- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:32:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:32:29
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2405.10357] RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2405.10357] RGB 引导的 ToF 成像系统：基于深度学习的方法综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.10357](https://ar5iv.labs.arxiv.org/html/2405.10357)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.10357](https://ar5iv.labs.arxiv.org/html/2405.10357)
- en: '[1]\fnmChenyang \surGe'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]\fnm陈阳 \sur葛'
- en: '[1]\orgdivInstitute of Artificial Intelligence and Robotics, \orgnameXi’an
    Jiaotong University, \orgaddress\streetNo.28, West Xianning Road, \cityXi’an,
    \postcode710049, \stateShaanxi, \countryChina'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]\orgdiv人工智能与机器人研究所，\orgname西安交通大学，\orgaddress\streetNo.28, 西南宁路，\city西安，\postcode710049，\state陕西，\country中国'
- en: 2]\orgdivDepartment of Computer Science and Engineering (DISI), \orgnameUniversity
    of Bologna, \orgaddress\streetViale Risorgimento, 2, \cityBologna, \postcode40136,
    \stateEmilia-Romagna, \countryItaly
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2]\orgdiv计算机科学与工程系（DISI），\orgname博洛尼亚大学，\orgaddress\streetViale Risorgimento,
    2，\city博洛尼亚，\postcode40136，\state艾米利亚-罗马涅，\country意大利
- en: ³\orgdivAdvanced Research Center on Electronic System (ARCES), \orgnameUniversity
    of Bologna, \orgaddress\streetVia Vincenzo Toffano, 2/2, \cityBologna, \postcode40125,
    \stateEmilia-Romagna, \countryItaly
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³\orgdiv电子系统先进研究中心（ARCES），\orgname博洛尼亚大学，\orgaddress\streetVia Vincenzo Toffano,
    2/2，\city博洛尼亚，\postcode40125，\state艾米利亚-罗马涅，\country意大利
- en: 'RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RGB 引导的 ToF 成像系统：基于深度学习的方法综述
- en: \fnmXin \surQiao    \fnmMatteo \surPoggi    \fnmPengchao \surDeng    \fnmHao
    \surWei    [cyge@mail.xjtu.edu.cn](mailto:cyge@mail.xjtu.edu.cn)    \fnmStefano
    \surMattoccia * [
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \fnm辛 \sur乔    \fnm马特奥 \sur波吉    \fnm彭超 \sur邓    \fnm浩 \sur魏    [cyge@mail.xjtu.edu.cn](mailto:cyge@mail.xjtu.edu.cn)
       \fnm斯特凡诺 \sur马托西亚 * [
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Integrating an RGB camera into a ToF imaging system has become a significant
    technique for perceiving the real world. The RGB guided ToF imaging system is
    crucial to several applications, including face anti-spoofing, saliency detection,
    and trajectory prediction. Depending on the distance of the working range, the
    implementation schemes of the RGB guided ToF imaging systems are different. Specifically,
    ToF sensors with a uniform field of illumination, which can output dense depth
    but have low resolution, are typically used for close-range measurements. In contrast,
    LiDARs, which emit laser pulses and can only capture sparse depth, are usually
    employed for long-range detection. In the two cases, depth quality improvement
    for RGB guided ToF imaging corresponds to two sub-tasks: guided depth super-resolution
    and guided depth completion. In light of the recent significant boost to the field
    provided by deep learning, this paper comprehensively reviews the works related
    to RGB guided ToF imaging, including network structures, learning strategies,
    evaluation metrics, benchmark datasets, and objective functions. Besides, we present
    quantitative comparisons of state-of-the-art methods on widely used benchmark
    datasets. Finally, we discuss future trends and the challenges in real applications
    for further research.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 将 RGB 相机集成到 ToF 成像系统中已成为感知现实世界的重要技术。RGB 引导的 ToF 成像系统在多个应用中至关重要，包括人脸防伪、显著性检测和轨迹预测。根据工作范围的距离，RGB
    引导的 ToF 成像系统的实现方案不同。具体来说，具有均匀照明场的 ToF 传感器可以输出密集的深度信息，但分辨率较低，通常用于近距离测量。相比之下，LiDAR
    发射激光脉冲，只能捕捉稀疏的深度信息，通常用于远距离检测。在这两种情况下，RGB 引导的 ToF 成像的深度质量改进对应于两个子任务：引导的深度超分辨率和引导的深度完成。鉴于深度学习对该领域的重大推动，本文全面回顾了与
    RGB 引导的 ToF 成像相关的工作，包括网络结构、学习策略、评估指标、基准数据集和目标函数。此外，我们对最先进方法在广泛使用的基准数据集上的定量比较进行了介绍。最后，我们讨论了未来的趋势以及在实际应用中面临的挑战，以便进一步研究。
- en: 'keywords:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: RGB-guided ToF imaging, Deep learning, Depth super-resolution, Depth completion,
    Multimodal fusion
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: RGB 引导的 ToF 成像，深度学习，深度超分辨率，深度完成，多模态融合
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The real world is three-dimensional, yet usually recorded with 2D vision sensors
    such as color cameras. To faithfully portray it, a diffuse trend is to employ
    a depth camera during collection, widely used nowadays in downstream tasks related
    to computer vision and pattern recognition, such as face anti-spoofing [[86](#bib.bib86),
    [19](#bib.bib19)], saliency detection [[16](#bib.bib16)], autonomous driving [[47](#bib.bib47),
    [162](#bib.bib162)] and virtual/augmented reality (VR/AR) [[54](#bib.bib54), [64](#bib.bib64)].
    Among depth cameras, time-of-flight (ToF) technology emerges as the preferred
    choice for many applications thanks to its compact structure, high precision,
    and low cost. Recently, the iPad Pro and iPhone Pro/Pro Max from Apple use LiDAR
    based on direct ToF (dToF) technology, often adopted in fields such as biometrics,
    photography, games, modeling, virtual reality, and augmented reality. The ToF-only
    imaging system is usually preferred when cost efficiency, energy efficiency, fast
    real-time, or privacy sensitivity are the primary considerations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界是三维的，但通常使用二维视觉传感器如彩色相机进行记录。为了忠实地表现现实世界，逐渐采用深度相机进行数据采集，这在计算机视觉和模式识别相关的下游任务中被广泛应用，如面部防伪 [[86](#bib.bib86),
    [19](#bib.bib19)]、显著性检测 [[16](#bib.bib16)]、自动驾驶 [[47](#bib.bib47), [162](#bib.bib162)]
    和虚拟/增强现实 (VR/AR) [[54](#bib.bib54), [64](#bib.bib64)]。在深度相机中，飞行时间 (ToF) 技术因其紧凑结构、高精度和低成本而成为许多应用的首选。最近，苹果的
    iPad Pro 和 iPhone Pro/Pro Max 使用基于直接 ToF (dToF) 技术的 LiDAR，这在生物识别、摄影、游戏、建模、虚拟现实和增强现实等领域被广泛采用。当成本效益、能源效率、快速实时性或隐私敏感性是主要考虑因素时，通常首选
    ToF-only 成像系统。
- en: While ToF-only cameras might be the preferred choice in scenarios where depth
    sensing is paramount, some drawbacks still limit their performance in applications
    such as scene understanding, mixed reality, human-computer interaction, and facial
    recognition, where accuracy, versatility, and image content cues play a crucial
    role. In particular, the resolution of existing consumer-grade ToF cameras is
    about 300K pixels, with the LiDAR sensors – again based on the time of flight
    principle – used, for instance, by autonomous vehicles capturing even sparser
    measurements. In contrast, the resolution of RGB cameras used in mobile phones
    (e.g., Huawei P60) can reach dozens of MegaPixels, i.e., orders of magnitude higher,
    yet at a much lower cost. One solution is to directly enhance the low-quality
    (LQ) ToF depth map, i.e., to upsample [[127](#bib.bib127)] or densify [[153](#bib.bib153)]
    it, relying on hand-crafted or neural network-learned priors. However, these priors
    are likely to cause the predicted High-Resolution (HR) depth to contain blurred
    edges and other undesired artifacts. To face these issues, RGBD cameras integrate
    an HR RGB imager and a ToF sensor. Then, under the guidance of HR color images,
    sparse or Low-Resolution (LR) depth maps can be densified or super-solved by exploiting
    the structural details available from the high-resolution image content.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在深度感知至关重要的场景中，ToF-only 相机可能是首选，但一些缺点仍然限制了它们在场景理解、混合现实、人机交互和面部识别等应用中的性能，其中准确性、多功能性和图像内容线索发挥着关键作用。特别是，现有消费级
    ToF 相机的分辨率约为 300K 像素，使用 LiDAR 传感器——同样基于飞行时间原理——的自动驾驶汽车甚至捕捉到更稀疏的测量结果。相比之下，手机中使用的
    RGB 相机（例如，华为 P60）的分辨率可以达到数十 MegaPixels，即数量级更高，但成本却低得多。一种解决方案是直接增强低质量 (LQ) ToF
    深度图，即对其进行上采样 [[127](#bib.bib127)] 或密集化 [[153](#bib.bib153)]，依赖于手工制作的或神经网络学习的先验。然而，这些先验可能会导致预测的高分辨率
    (HR) 深度图出现模糊边缘和其他不希望出现的伪影。为了解决这些问题，RGBD 相机集成了高分辨率 RGB 图像传感器和 ToF 传感器。在高分辨率彩色图像的指导下，可以通过利用高分辨率图像内容的结构细节来密集化或超分辨率地解决稀疏或低分辨率
    (LR) 深度图。
- en: 'In practical applications, ToF cameras based on different technical routes
    are deployed in different scenarios, mainly divided into the following three categories:
    1) for short-ranging acquisition in applications like face/gesture recognition,
    typically using an indirect ToF (iToF) camera with a uniform field of illumination,
    obtaining a dense depth map, yet at a low resolution; 2) for long-range measurements,
    as in the case of LiDAR sensors emitting laser pulses, which produces a scattered
    point cloud of the sensed scene: when coupled with a color camera, the sparsity
    of the depth map obtained by projecting the point cloud on the image plane depends
    on the resolution of the image, the higher, the sparser – i.e., containing only
    $5\%$ of pixels encoding valid depth measurements [[104](#bib.bib104)] on $\sim
    0.3$Mpx images; 3) for medium-ranging, as in the case of the ToF cameras used
    in augmented reality and automated guided vehicle robots, either iToF or dToF
    can be used. However, even when iToF is used for short-ranging, depth values around
    the corners or edges may be missing.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，基于不同技术路线的ToF相机被部署在不同场景中，主要分为以下三类：1）用于短距离获取的应用，如面部/手势识别，通常使用间接ToF (iToF)
    相机，具有均匀的照明场，获得密集的深度图，但分辨率较低；2）用于长距离测量，例如LiDAR传感器发射激光脉冲，产生散布的点云：当与彩色相机配合使用时，投影到图像平面的点云的深度图的稀疏性取决于图像的分辨率，分辨率越高，稀疏性越大——即，仅包含$5\%$的像素编码有效的深度测量[[104](#bib.bib104)]，在$\sim
    0.3$Mpx图像上；3）用于中等距离，例如在增强现实和自动引导车辆机器人中使用的ToF相机，可以使用iToF或dToF。然而，即使在短距离应用中使用iToF，角落或边缘附近的深度值也可能丢失。
- en: '| ![Refer to caption](img/afb9385ea4a1d7c0e8a12d566749cb47.png) |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/afb9385ea4a1d7c0e8a12d566749cb47.png) |'
- en: '| \begin{overpic}[width=203.80193pt]{x1.pdf} \put(62.0,18.0){\color[rgb]{1,0,0}\line(2,0){12.0}}
    \put(62.0,1.0){\color[rgb]{1,0,0}\line(2,0){12.0}} \put(62.0,1.0){\color[rgb]{1,0,0}\line(0,2){17.0}}
    \put(74.0,1.0){\color[rgb]{1,0,0}\line(0,2){17.0}} \end{overpic} |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| \begin{overpic}[width=203.80193pt]{x1.pdf} \put(62.0,18.0){\color[rgb]{1,0,0}\line(2,0){12.0}}
    \put(62.0,1.0){\color[rgb]{1,0,0}\line(2,0){12.0}} \put(62.0,1.0){\color[rgb]{1,0,0}\line(0,2){17.0}}
    \put(74.0,1.0){\color[rgb]{1,0,0}\line(0,2){17.0}} \end{overpic} |'
- en: '| 375$\times$1242 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 375$\times$1242 |'
- en: '| ![Refer to caption](img/013bc4fdcabf6596cfcb777209621a65.png) | ![Refer to
    caption](img/81595a0deffdaac750e1b352a825fe7d.png) | ![Refer to caption](img/ada06ad6eb9ad2efbcc35ea7e78d66e9.png)
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/013bc4fdcabf6596cfcb777209621a65.png) | ![参考说明](img/81595a0deffdaac750e1b352a825fe7d.png)
    | ![参考说明](img/ada06ad6eb9ad2efbcc35ea7e78d66e9.png) |'
- en: '| 375$\times$1242 | $\times 0.5$ | $\times 0.25$ |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 375$\times$1242 | $\times 0.5$ | $\times 0.25$ |'
- en: '| ![Refer to caption](img/dcc951b7651079dab30c042d1587ee69.png) | ![Refer to
    caption](img/a6c42ea4d02244353fc0249ec8e703da.png) | ![Refer to caption](img/ad08bec223554d03e671e7d7b5ac3ecc.png)
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/dcc951b7651079dab30c042d1587ee69.png) | ![参考说明](img/a6c42ea4d02244353fc0249ec8e703da.png)
    | ![参考说明](img/ad08bec223554d03e671e7d7b5ac3ecc.png) |'
- en: '| 375$\times$1242 | $\times 2$ | $\times 4$ |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 375$\times$1242 | $\times 2$ | $\times 4$ |'
- en: 'Figure 1: An example of LiDAR point cloud from the KITTI dataset reprojected
    on images at different resolutions.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：从KITTI数据集中重投影到不同分辨率图像上的LiDAR点云示例。
- en: 'In the three cases, improving the quality of original depth maps translates
    into 1) upsampling the original, spatial resolution through Guided Depth Super-Resolution
    (GDSR) [[33](#bib.bib33), [50](#bib.bib50)], or 2) densifying the sparse depth
    map through Guided Depth Completion (GDC) [[153](#bib.bib153), [13](#bib.bib13)].
    The goal of GDSR is to utilize RGB images to increase pixel density for low-resolution
    depth and provide richer visual details, while GDC aims to fill in missing depth
    values in sparse depth maps obtained when using laser pulses, as well as increase
    the overall depth map resolution – as the sparsity of the depth map depends on
    the resolution of the image over which the sensed point cloud is projected (the
    lower the image resolution, the denser the projected depth map; the higher the
    image resolution, the sparser the depth map). In Fig. [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), we show an example of LiDAR point cloud from the KITTI dataset reprojected
    at the original resolution used in the dataset (375$\times$1242) as well as both
    at lower and higher resolution (respectively $\times$0.5, $\times$0.25, $\times$2
    and $\times$4). We can observe how assuming different camera resolutions produces
    depth maps whose density changes sensibly, with fewer holes present at lower resolutions
    and much sparser depth points at higher resolutions. This evidence confirms that
    the density of a depth map obtained from LiDAR scans is a function of the resolution
    at which we wish to obtain a depth map and that, to some extent, the completion
    process has analogies with the super-resolution task. The key difference with
    GDSR is that the laser spot emitted by LiDAR in GDC is tiny and typically cannot
    cover the scene corresponding to one pixel in the sensor. Therefore, GDC usually
    requires perceiving a larger neighborhood (e.g., $7\times 7$) than GDSR in shallow
    feature extraction of the input to estimate depth values in areas where the information
    is missing. Generally, RGB guided ToF imaging systems involve one of the two sub-tasks,
    depending on the use case and the depth sensing technologies. Consequently, different
    solutions must be deployed depending on the specific sub-task. Nonetheless, both
    tasks aim to yield high-quality depth maps with the guidance of RGB information,
    that is, transfer high-frequency structural information and fine-grained features
    from HR color images to LR/sparse depth maps while avoiding texture-copying artifacts.
    Furthermore, it is worth noting that the model design of the two tasks exhibits
    intriguing similarities, which inspired us to analyze them together as one topic
    for investigation, and their specific classification will be elaborated upon in
    Sec. [2.4](#S2.SS4 "2.4 Taxonomy ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided ToF
    Imaging System: A Survey of Deep Learning-based Methods").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '在这三种情况下，提高原始深度图的质量可以通过1）通过引导深度超分辨率（GDSR）[[33](#bib.bib33), [50](#bib.bib50)]来上采样原始的空间分辨率，或2）通过引导深度补全（GDC）[[153](#bib.bib153),
    [13](#bib.bib13)]来稠密化稀疏深度图。GDSR的目标是利用RGB图像来增加低分辨率深度图的像素密度，并提供更丰富的视觉细节，而GDC的目标是填补使用激光脉冲获得的稀疏深度图中的缺失深度值，并提高整体深度图的分辨率——因为深度图的稀疏性取决于感测点云投影的图像分辨率（图像分辨率越低，投影的深度图越密集；图像分辨率越高，深度图越稀疏）。在图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods")中，我们展示了来自KITTI数据集的LiDAR点云的示例，重投影于数据集使用的原始分辨率（375$\times$1242）以及较低和较高分辨率（分别为$\times$0.5，$\times$0.25，$\times$2和$\times$4）。我们可以观察到，假设不同的相机分辨率会产生密度明显变化的深度图，在较低分辨率下孔洞较少，而在较高分辨率下深度点更稀疏。这些证据证实了从LiDAR扫描中获得的深度图的密度是我们希望获得的深度图分辨率的函数，并且在某种程度上，补全过程与超分辨率任务具有类似之处。与GDSR的主要区别在于，GDC中LiDAR发射的激光点非常小，通常无法覆盖与传感器中一个像素对应的场景。因此，GDC通常需要感知比GDSR更大的邻域（例如，$7\times
    7$），以在缺失信息的区域估计深度值。一般来说，RGB引导的ToF成像系统涉及两个子任务之一，具体取决于使用案例和深度传感技术。因此，必须根据具体的子任务部署不同的解决方案。然而，这两个任务的目标都是在RGB信息的指导下生成高质量的深度图，即从高分辨率彩色图像转移高频结构信息和细粒度特征到低分辨率/稀疏深度图，同时避免纹理复制伪影。此外，值得注意的是，这两个任务的模型设计展现出有趣的相似性，这启发我们将它们作为一个研究主题进行分析，它们的具体分类将在第[2.4](#S2.SS4
    "2.4 Taxonomy ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided ToF Imaging System:
    A Survey of Deep Learning-based Methods")节中详细说明。'
- en: '| ![Refer to caption](img/3bd63fa25c14f90e8856bc33a5e57e99.png) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/3bd63fa25c14f90e8856bc33a5e57e99.png) |'
- en: 'Figure 2: Timeline showing the advances in RGB guided ToF Imaging. Methods
    are marked in blue or purple, depending on the learning paradigm being unsupervised
    or supervised.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：时间轴展示了 RGB 引导的 ToF 成像的进展。方法根据学习范式（无监督或有监督）以蓝色或紫色标记。
- en: At first, traditional methods extract meaningful features from color images
    through guided filtering-based [[116](#bib.bib116), [49](#bib.bib49), [87](#bib.bib87)]
    or optimization-based [[23](#bib.bib23), [33](#bib.bib33), [102](#bib.bib102)]
    algorithms. These hand-crafted approaches rely on the assumption that depth discontinuities
    statistically co-occur in correspondence with RGB image edges. In real-world cases,
    however, this prior may fail to extract informative features and introduce artifacts.
    For instance, in the presence of rich texture, these methods convey essential
    structural details but also transfer the appearance of RGB images to depth maps
    as a side effect. Therefore, this task remains challenging as it requires properly
    distinguishing between high-frequency image information corresponding to discontinuities
    in the depth map and those not. Over the past decades, solutions for enhancing
    RGB guided ToF imaging have shifted from model-driven to data-driven paradigms [[20](#bib.bib20),
    [168](#bib.bib168)], in particular thanks to deep learning methods [[10](#bib.bib10),
    [208](#bib.bib208), [196](#bib.bib196)] that have rapidly advanced the field.
    Even with a simple network structure, such as a few stacked convolutional layers [[153](#bib.bib153)]
    or the vanilla encoder-decoder [[78](#bib.bib78)] architecture, it is straightforward
    to recover HR/dense depth maps with much higher accuracy compared to what obtained
    through hand-crafted methods. Consequently, RGB guided ToF imaging via deep learning
    received increasing research interest from academia and industry, motivating us
    to survey recent progress in this field.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，传统方法通过基于引导滤波的[[116](#bib.bib116)、[49](#bib.bib49)、[87](#bib.bib87)]或基于优化的[[23](#bib.bib23)、[33](#bib.bib33)、[102](#bib.bib102)]算法从彩色图像中提取有意义的特征。这些手工制作的方法依赖于一个假设，即深度不连续性在统计上与
    RGB 图像边缘对应。然而，在实际情况下，这种先验可能无法提取信息特征，并引入伪影。例如，在存在丰富纹理的情况下，这些方法传达了重要的结构细节，但也将 RGB
    图像的外观作为副作用转移到深度图中。因此，这项任务仍然具有挑战性，因为它需要正确区分与深度图中的不连续性对应的高频图像信息和那些不对应的高频信息。在过去的几十年中，RGB
    引导 ToF 成像的增强解决方案已经从模型驱动转变为数据驱动的范式[[20](#bib.bib20)、[168](#bib.bib168)]，特别是得益于深度学习方法[[10](#bib.bib10)、[208](#bib.bib208)、[196](#bib.bib196)]的快速进展。即使是简单的网络结构，例如几层堆叠的卷积层[[153](#bib.bib153)]或原始的编码器-解码器[[78](#bib.bib78)]架构，也能以比手工制作方法高得多的准确度恢复
    HR/密集深度图。因此，RGB 引导的 ToF 成像通过深度学习得到了学术界和工业界越来越多的研究兴趣，促使我们对这一领域的最新进展进行调查。
- en: Specifically, given the substantial development of deep-learning solutions in
    the last decade, this survey aims to provide a complete overview of solutions
    designed for enhancing RGB guided ToF imaging in both use cases. Although we are
    aware of previous review papers on guided depth super-resolution [[199](#bib.bib199)]
    and depth completion [[174](#bib.bib174), [55](#bib.bib55)] treated as standalone
    tasks, we highlight the lack of interconnection between the two, despite the shared
    insights they bring to the community of researchers working with RGB guided ToF
    sensors, which we aim to envelop in a comprehensive survey. Purposely, we will
    review the literature concerning deep learning frameworks developed to enhance
    the depth maps perceived from this family of devices. We will study and classify
    them according to several aspects, such as the framework design, the learning
    paradigm, and the objective functions minimized during training. Moreover, we
    introduce the reader to the standard datasets used as benchmarks for evaluating
    existing methodologies, reporting the performance of state-of-the-art methods
    as a reference.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，鉴于过去十年深度学习解决方案的显著发展，本调查旨在提供一个完整的概述，介绍旨在增强 RGB 引导 ToF 成像的解决方案，涵盖两个使用案例。尽管我们了解关于引导深度超分辨率[[199](#bib.bib199)]和深度完成[[174](#bib.bib174)、[55](#bib.bib55)]作为独立任务的以前的综述论文，但我们强调了这两者之间缺乏互联性，尽管它们为研究
    RGB 引导 ToF 传感器的社区带来了共享的见解，我们旨在在全面的综述中涵盖这些内容。特意地，我们将回顾旨在增强从这些设备系列感知的深度图的深度学习框架文献。我们将根据多个方面，如框架设计、学习范式和训练期间最小化的目标函数，对它们进行研究和分类。此外，我们将向读者介绍用于评估现有方法的标准数据集，并报告最先进方法的性能作为参考。
- en: 'We outline our major contributions as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们概述了我们的主要贡献如下：
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, we are the first to provide an in-depth investigation
    for RGB guided ToF imaging through deep learning, including guided depth super-resolution
    and guided depth completion as the main tasks to be dealt with according to the
    specific use case.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，我们是第一家通过深度学习对 RGB 引导 ToF 成像进行深入研究的机构，包括引导深度超分辨率和引导深度完成作为主要任务。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In each subtask, this paper reviews recent deep learning-based methods under
    several aspects to illustrate the motivations behind their design, contributions,
    and performance.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在每个子任务中，本文从多个方面审视了最近的基于深度学习的方法，以说明其设计动机、贡献和性能。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss the challenges and future trends of RGB guided ToF imaging.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了 RGB 引导 ToF 成像的挑战和未来趋势。
- en: 'The rest of this survey is structured as follows. In Section [2](#S2 "2 Preliminaries
    and Taxonomy ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), we describe preliminaries for RGB guided ToF imaging. Section. [3](#S3
    "3 Guided Depth Super-Resolution ‣ RGB Guided ToF Imaging System: A Survey of
    Deep Learning-based Methods") and [4](#S4 "4 Guided Depth Completion ‣ RGB Guided
    ToF Imaging System: A Survey of Deep Learning-based Methods") present deep learning-based
    methods for GDSR and GDC, respectively. In Section [5](#S5 "5 Benchmark Datasets
    and objective functions ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), we introduce the datasets and loss functions. Section [6](#S6 "6 Evaluation
    ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods") provides
    comparison results of some recent methods. Finally, we discuss future trends and
    challenges in Section [7](#S7 "7 Discussion and future trends ‣ RGB Guided ToF
    Imaging System: A Survey of Deep Learning-based Methods"). Fig. [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods") draws a timeline and reports for each year from 2017 to 2023, any method
    we will review in this paper, and the publication venue.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的其余部分结构如下。在第[2](#S2 "2 前言与分类 ‣ RGB 引导的 ToF 成像系统：深度学习方法综述")节中，我们描述了 RGB 引导
    ToF 成像的基础知识。第[3](#S3 "3 引导深度超分辨率 ‣ RGB 引导的 ToF 成像系统：深度学习方法综述")节和[4](#S4 "4 引导深度完成
    ‣ RGB 引导的 ToF 成像系统：深度学习方法综述")节分别介绍了基于深度学习的 GDSR 和 GDC 方法。在第[5](#S5 "5 基准数据集和目标函数
    ‣ RGB 引导的 ToF 成像系统：深度学习方法综述")节中，我们介绍了数据集和损失函数。第[6](#S6 "6 评估 ‣ RGB 引导的 ToF 成像系统：深度学习方法综述")节提供了一些最新方法的比较结果。最后，在第[7](#S7
    "7 讨论与未来趋势 ‣ RGB 引导的 ToF 成像系统：深度学习方法综述")节中，我们讨论了未来的趋势和挑战。图[2](#S1.F2 "图 2 ‣ 1
    引言 ‣ RGB 引导的 ToF 成像系统：深度学习方法综述")绘制了2017年到2023年的时间线和每年的报告，列出了本文将要评审的任何方法及其发表场所。
- en: 2 Preliminaries and Taxonomy
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 前言与分类
- en: In this section, we first introduce the foundation of RGB guided ToF imaging,
    then we present GDSR and GDC as the two main research trends aimed at enhancing
    ToF depth maps and the commonly used objective functions in both.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍了 RGB 引导 ToF 成像的基础知识，然后介绍了 GDSR 和 GDC 作为两个主要研究趋势，旨在增强 ToF 深度图以及两者中常用的目标函数。
- en: 2.1 Principles of ToF Imaging
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 ToF 成像原理
- en: 'Here, we describe the imaging principles of iToF and dToF, respectively. In
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Principles of ToF Imaging ‣ 2 Preliminaries and
    Taxonomy ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods"),
    we provide a schematic diagram of the principles commonly used by iToF and dToF.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们分别描述了 iToF 和 dToF 的成像原理。在图[3](#S2.F3 "图 3 ‣ 2.1 ToF 成像原理 ‣ 2 前言与分类 ‣ RGB
    引导的 ToF 成像系统：深度学习方法综述")中，我们提供了 iToF 和 dToF 常用原理的示意图。
- en: '| ![Refer to caption](img/aabebd51f0f9f392f453e2c43b2a431a.png) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/aabebd51f0f9f392f453e2c43b2a431a.png) |'
- en: 'Figure 3: iToF and dToF working principles. The former computes distances based
    on the phase shift between emitted and reflected light; the latter measures the
    time required for a laser light to bounce over an object.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：iToF 和 dToF 的工作原理。前者基于发射光和反射光之间的相位差计算距离；后者测量激光光线反射到物体上所需的时间。
- en: 2.1.1 Indirect ToF
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 间接 ToF
- en: 'Indirect ToF (iToF) determines distance by capturing reflected light and calculating
    the phase delay between emitted and reflected light. Currently, most of the off-the-shelf
    ToF cameras emit signals $s(t)$ using sinusoidal amplitude modulation:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 间接 ToF (iToF) 通过捕获反射光并计算发射光与反射光之间的相位延迟来确定距离。目前，大多数现成的 ToF 相机使用正弦幅度调制发射信号 $s(t)$：
- en: '|  | $s(t)=s_{1}\cos(\omega t)+s_{0}$ |  | (1) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $s(t)=s_{1}\cos(\omega t)+s_{0}$ |  | (1) |'
- en: 'Table 1: Characteristics of different types of LiDAR.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同类型 LiDAR 的特性。
- en: '| Type | Technology | Advantages | Disadvantages |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 技术 | 优点 | 缺点 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Mechanical | Rotating mirrors or prisms | High accuracy, long range | Large,
    expensive |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 机械 | 旋转镜或棱镜 | 高精度，长距离 | 大，昂贵 |'
- en: '| MEMS | Micromirrors | Small, cheap | Lower FoV, range |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| MEMS | 微镜 | 小巧，便宜 | 较低的视场，范围 |'
- en: '| Flash | Arrays of lasers firing simultaneously | Fast, high resolution |
    Short range |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 闪光 | 激光阵列同时发射 | 快速，高分辨率 | 短距离 |'
- en: '| OPA | Optical phased arrays | Small, cheap, versatile | Still in development
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| OPA | 光学相控阵列 | 小巧，便宜，多功能 | 仍在开发中 |'
- en: 'Since the baseline between the ToF transmitter and the receiver is so small
    (usually only a few millimeters), they can generally be considered coaxial. Assuming
    that the target is static, and the signal is only reflected once before being
    received by the sensor, the reflected signal of a single pixel measured at a specific
    fixed frequency can be expressed as a function of time $t$:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ToF 发射器和接收器之间的基线非常小（通常只有几毫米），它们通常可以视为共轴。假设目标是静态的，并且信号在被传感器接收之前只反射一次，那么在特定固定频率下测得的单个像素的反射信号可以表示为时间
    $t$ 的函数：
- en: '|  | $r(t)=\frac{\rho}{c^{2}\tau_{0}^{2}}[s_{1}\cos(\omega t-2\omega\tau_{0})+s_{0}]+e_{0}$
    |  | (2) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $r(t)=\frac{\rho}{c^{2}\tau_{0}^{2}}[s_{1}\cos(\omega t-2\omega\tau_{0})+s_{0}]+e_{0}$
    |  | (2) |'
- en: 'where $\rho$ is the surface reflectivity of the target, $\omega$ denotes angular
    frequency, $\tau_{0}$ is the delay of signal reflection from the target back to
    a pixel at the speed of light $c\approx 3\times 10^{8}$m/s, and $e_{0}$ is the
    offset caused by ambient light. According to [[51](#bib.bib51)], Eq. [1](#S2.E1
    "In 2.1.1 Indirect ToF ‣ 2.1 Principles of ToF Imaging ‣ 2 Preliminaries and Taxonomy
    ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods") can
    be rewritten as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\rho$ 是目标的表面反射率，$\omega$ 表示角频率，$\tau_{0}$ 是信号从目标反射回到像素的光速 $c\approx 3\times
    10^{8}$m/s 的延迟，而 $e_{0}$ 是由环境光引起的偏移。根据 [[51](#bib.bib51)]，方程 [1](#S2.E1 "In 2.1.1
    Indirect ToF ‣ 2.1 Principles of ToF Imaging ‣ 2 Preliminaries and Taxonomy ‣
    RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods") 可以重写为：'
- en: '|  | $\displaystyle A_{1}=\frac{\rho s_{1}}{c^{2}\tau_{0}^{2}},\quad A_{0}=\frac{\rho
    s_{0}}{c^{2}\tau_{0}^{2}}+e_{0}$ |  | (3) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A_{1}=\frac{\rho s_{1}}{c^{2}\tau_{0}^{2}},\quad A_{0}=\frac{\rho
    s_{0}}{c^{2}\tau_{0}^{2}}+e_{0}$ |  | (3) |'
- en: '|  | $\displaystyle r(t)=A_{1}\cos(\omega t-2\omega\tau_{0})+A_{0}$ |  | (4)
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r(t)=A_{1}\cos(\omega t-2\omega\tau_{0})+A_{0}$ |  | (4)
    |'
- en: 'The received signal remains sinusoidal, but the phase, which contains scene
    depth information, is changed. After a high-frequency periodic signal $B_{1}\cos(\omega
    t-\phi)$ modulates the incident signal, the modulated signal can be expressed
    as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接收到的信号保持正弦波形，但包含场景深度信息的相位发生了变化。在高频周期信号 $B_{1}\cos(\omega t-\phi)$ 调制入射信号后，调制后的信号可以表示为：
- en: '|  | $\displaystyle\tilde{I}_{\phi,\omega}(t)=$ | $\displaystyle r(t)B_{1}\cos(\omega
    t-\phi)$ |  | (5) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{I}_{\phi,\omega}(t)=$ | $\displaystyle r(t)B_{1}\cos(\omega
    t-\phi)$ |  | (5) |'
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{A_{1}B_{1}\cos(\phi-2\omega\tau_{0})}{2}+$
    |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\frac{A_{1}B_{1}\cos(\phi-2\omega\tau_{0})}{2}+$
    |  |'
- en: '|  |  | $\displaystyle\frac{A_{1}B_{1}\cos(2\omega t+\phi+2\omega\tau_{0})}{2}+$
    |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\frac{A_{1}B_{1}\cos(2\omega t+\phi+2\omega\tau_{0})}{2}+$
    |  |'
- en: '|  |  | $\displaystyle A_{0}B_{1}\cos(\omega t-\phi)$ |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle A_{0}B_{1}\cos(\omega t-\phi)$ |  |'
- en: 'Considering the quantum efficiency of ToF sensors, the sensor exposure time
    $T$ set to capture a sufficient number of photons is usually much larger than
    $\pi c/\omega$. During the exposure, the measurement is equivalent to integrating
    over the time range $T$, so the period terms in Eq. [5](#S2.E5 "In 2.1.1 Indirect
    ToF ‣ 2.1 Principles of ToF Imaging ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided
    ToF Imaging System: A Survey of Deep Learning-based Methods") all vanish in the
    calculation, and the result can be computed as:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 ToF 传感器的量子效率，设置捕获足够光子所需的传感器曝光时间 $T$ 通常远大于 $\pi c/\omega$。在曝光过程中，测量相当于对时间范围
    $T$ 进行积分，因此方程 [5](#S2.E5 "在 2.1.1 间接 ToF ‣ 2.1 ToF 成像原理 ‣ 2 前言与分类 ‣ RGB 引导 ToF
    成像系统：基于深度学习的方法概述") 中的周期项在计算中都消失，结果可以计算为：
- en: '|  | $\displaystyle I_{\phi,\omega}$ | $\displaystyle=\int_{-T/2}^{T/2}\tilde{i}_{\phi,\omega}(t)dt$
    |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle I_{\phi,\omega}$ | $\displaystyle=\int_{-T/2}^{T/2}\tilde{i}_{\phi,\omega}(t)dt$
    |  |'
- en: '|  |  | $\displaystyle\approx A_{1}B_{1}\cos(\phi-2\omega\tau_{0})=A_{1}g_{\phi,\omega}$
    |  | (6) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\approx A_{1}B_{1}\cos(\phi-2\omega\tau_{0})=A_{1}g_{\phi,\omega}$
    |  | (6) |'
- en: 'where $A_{1}$ is the scene response that represents the intensity of the encoded
    corresponding pixel at time $\tau_{0}$ after the laser transmitter sends a light
    pulse, $g_{\phi,\omega}=B_{1}\cos(\phi-2\omega\tau_{0})$ is the camera function,
    and $I_{\phi,\omega}$ is the raw measurement with correlation at phase $\phi$,
    angular frequency $\omega$, i.e. ToF raw data. If a single modulation frequency
    $f_{m}$ is used to collect multiple ($K\geq 2$) raw measurements in one cycle,
    the distance between the scene and camera can be calculated as:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A_{1}$ 是场景响应，代表激光发射器发送光脉冲后在时间 $\tau_{0}$ 的编码对应像素的强度，$g_{\phi,\omega}=B_{1}\cos(\phi-2\omega\tau_{0})$
    是相机函数，$I_{\phi,\omega}$ 是相位为 $\phi$、角频率为 $\omega$ 的原始测量，即 ToF 原始数据。如果在一个周期内使用单一调制频率
    $f_{m}$ 收集多个 ($K\geq 2$) 原始测量，可以计算场景与相机之间的距离为：
- en: '|  | $d=\frac{c}{2\omega}\arctan(\frac{\sin{\boldmath{\phi}}\cdot\boldmath{I}_{\phi,\omega}}{\cos{\boldmath{\phi}}\cdot\boldmath{I}_{\phi,\omega}})\\
    $ |  | (7) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $d=\frac{c}{2\omega}\arctan(\frac{\sin{\boldmath{\phi}}\cdot\boldmath{I}_{\phi,\omega}}{\cos{\boldmath{\phi}}\cdot\boldmath{I}_{\phi,\omega}})\\
    $ |  | (7) |'
- en: In practice, if the modulation frequency $f_{m}=\frac{\omega}{2\pi}$ is fixed,
    ToF cameras usually adopt the four-phase method to sample ToF raw data, i.e.,
    sampling four times in one cycle, with a phase step of $90^{\circ}$. For ease
    of reading, the four raw data can be abbreviated as $I_{i}=I_{\frac{\pi}{2\omega}i},i=0,1,2,3$.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果调制频率 $f_{m}=\frac{\omega}{2\pi}$ 固定，ToF 相机通常采用四相位法来采样 ToF 原始数据，即在一个周期内采样四次，相位步长为
    $90^{\circ}$。为了便于阅读，四个原始数据可以简写为 $I_{i}=I_{\frac{\pi}{2\omega}i},i=0,1,2,3$。
- en: '|  | $\displaystyle A=\frac{\sqrt{(I_{3}-I_{1})^{2}+(I_{0}-I_{2})^{2}}}{2}$
    |  | (8) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A=\frac{\sqrt{(I_{3}-I_{1})^{2}+(I_{0}-I_{2})^{2}}}{2}$
    |  | (8) |'
- en: '|  | $\displaystyle B=\frac{I_{0}+I_{1}+I_{2}+I_{3}}{4}$ |  | (9) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B=\frac{I_{0}+I_{1}+I_{2}+I_{3}}{4}$ |  | (9) |'
- en: '|  | $\displaystyle\phi=\arctan{\frac{I_{3}-I_{1}}{I_{0}-I_{2}}}$ |  | (10)
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi=\arctan{\frac{I_{3}-I_{1}}{I_{0}-I_{2}}}$ |  | (10)
    |'
- en: '|  | $\displaystyle d=\frac{c}{2\pi}(\frac{\phi}{2f}+N)$ |  | (11) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle d=\frac{c}{2\pi}(\frac{\phi}{2f}+N)$ |  | (11) |'
- en: where $N$ is an integer that represents the number of phase wraps. Based on
    the above principle, some works[[144](#bib.bib144), [15](#bib.bib15), [46](#bib.bib46),
    [76](#bib.bib76)] directly process the raw signal of the iToF, thus improving
    the depth quality.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 是表示相位包络数的整数。基于上述原理，一些研究[[144](#bib.bib144), [15](#bib.bib15), [46](#bib.bib46),
    [76](#bib.bib76)] 直接处理 iToF 的原始信号，从而提高深度质量。
- en: At present, most iToF for consumer use sensors based on complementary metal-oxide
    semiconductor (CMOS) or charge-coupled device (CCD), with a resolution of $320\times
    240$ (QVGA) or $640\times 480$ (VGA) pixels. However, the resolution is still
    significantly lower than that of an RGB image using the same technology.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数面向消费市场的 iToF 传感器基于互补金属氧化物半导体 (CMOS) 或电荷耦合器件 (CCD)，分辨率为 $320\times 240$
    (QVGA) 或 $640\times 480$ (VGA) 像素。然而，其分辨率仍显著低于使用相同技术的 RGB 图像。
- en: '![Refer to caption](img/3618d16c14ee7b2e61537fdf4366b67e.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3618d16c14ee7b2e61537fdf4366b67e.png)'
- en: (a) Principle of mechanical LiDAR
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 机械 LiDAR 原理
- en: '![Refer to caption](img/0bdfd02a168a418cad2fc027ae91d98d.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0bdfd02a168a418cad2fc027ae91d98d.png)'
- en: (b) Principle of MEMS LiDAR
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MEMS LiDAR 原理
- en: '![Refer to caption](img/c5baab47ef44cd04b5f6427f222d7fd6.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c5baab47ef44cd04b5f6427f222d7fd6.png)'
- en: (c) Principle of flash LiDAR
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 闪光 LiDAR 原理
- en: '![Refer to caption](img/b0e57a51a48429bd07644a532ba42422.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b0e57a51a48429bd07644a532ba42422.png)'
- en: (d) Principle of OPA LiDAR
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: (d) OPA LiDAR 的原理
- en: 'Figure 4: Schematic diagram of LiDAR systems with different imaging principles.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：具有不同成像原理的 LiDAR 系统示意图。
- en: 2.1.2 Direct ToF
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 直接 ToF
- en: DToF is a method that measures the time it takes for laser light to be fired
    directly at an object and returned to the sensor. The moment the laser is fired,
    the electronic clock is activated. Then, the pulse bounces off the target and
    is partially picked up by the photodetector. By measuring the time lapse $\Delta
    T$, the distance $d$ between the outgoing light pulse and the reflected light
    pulse is calculated with
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: DToF 是一种测量激光光线直接射向物体并返回传感器所需时间的方法。当激光发射的瞬间，电子时钟被激活。然后，脉冲从目标上反射回来，并被光探测器部分接收。通过测量时间间隔
    $\Delta T$，使用公式计算发射光脉冲与反射光脉冲之间的距离 $d$：
- en: '|  | $d=\frac{c\Delta T}{2}\\ $ |  | (12) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $d=\frac{c\Delta T}{2}\\ $ |  | (12) |'
- en: To achieve higher sensitivity and lower power consumption, dToF usually adopts
    single photon avalanche diodes (SPADs) instead of Avalanche Photodiodes (APDs)
    as photodetectors. Due to limitations in the size of the chip and its auxiliary
    circuits, the pixel count of SPAD arrays is much lower than that of iToF.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现更高的灵敏度和更低的功耗，dToF 通常采用单光子雪崩二极管（SPADs）而非雪崩光电二极管（APDs）作为光探测器。由于芯片及其辅助电路的尺寸限制，SPAD
    阵列的像素数量远低于 iToF。
- en: 'LiDAR, an acronym for Light Detection and Ranging, despite being based on the
    same working principle as dToF, is frequently listed as a distinct depth sensing
    technology due to some of its peculiar design and setup. For instance, conventional
    LiDAR continuously scans the environment utilizing mechanical components (e.g.,
    spinning mirrors). Another type is MEMS LiDAR, which uses micromirrors controlled
    by micro-electromechanical systems (MEMS) to scan the environment. This type of
    LiDAR is smaller and cheaper than mechanical LiDARs but has lower FoV and range.
    As a still developing technology, OPA LiDARs use optical phased arrays (OPAs)
    to steer the laser beam electronically, and they have the potential to be small,
    cheap, and versatile. Different from the techniques mentioned above, global shutter
    flash LiDARs, also referred to as flash LiDARs or ToF LiDARs, use arrays of lasers
    firing simultaneously to create a 3D image of the surrounding environment, just
    like iToF, thereby improving frame rate and imaging efficiency for applications
    such as autonomous driving. This technology enables the integration of dToF into
    consumer electronic devices. We provide a comparison summarizing the different
    types of LiDAR in Table. [1](#S2.T1 "Table 1 ‣ 2.1.1 Indirect ToF ‣ 2.1 Principles
    of ToF Imaging ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided ToF Imaging System:
    A Survey of Deep Learning-based Methods") and the schematic diagram of each method
    in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.1 Indirect ToF ‣ 2.1 Principles of ToF Imaging
    ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided ToF Imaging System: A Survey of Deep
    Learning-based Methods").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: LiDAR，即光学探测与测距的缩写，尽管基于与 dToF 相同的工作原理，但由于其独特的设计和设置，通常被列为一种独立的深度感知技术。例如，传统 LiDAR
    使用机械组件（如旋转镜）持续扫描环境。另一种类型是 MEMS LiDAR，它使用由微机电系统（MEMS）控制的微镜扫描环境。这种类型的 LiDAR 比机械
    LiDAR 更小、更便宜，但视场和范围较小。作为一种尚在发展中的技术，OPA LiDAR 使用光学相控阵列（OPA）电子引导激光束，具有潜在的小巧、便宜和多功能的优势。与上述技术不同的是，全局快门闪光
    LiDAR，也称为闪光 LiDAR 或 ToF LiDAR，使用激光阵列同时发射以创建周围环境的 3D 图像，类似于 iToF，从而提高了帧率和成像效率，适用于自动驾驶等应用。这项技术使
    dToF 能够集成到消费电子设备中。我们在表 [1](#S2.T1 "表 1 ‣ 2.1.1 间接 ToF ‣ 2.1 ToF 成像原理 ‣ 2 初步知识和分类
    ‣ RGB 引导 ToF 成像系统：基于深度学习的方法调查") 中总结了不同类型的 LiDAR 比较，并在图 [4](#S2.F4 "图 4 ‣ 2.1.1
    间接 ToF ‣ 2.1 ToF 成像原理 ‣ 2 初步知识和分类 ‣ RGB 引导 ToF 成像系统：基于深度学习的方法调查") 中展示了每种方法的示意图。
- en: In essence, LiDAR still employs the dToF principle, and the two terms of dToF
    and LiDAR are even interchangeable in many scenarios (e.g., Apple refers to the
    dToF sensor in iPad Pro and iPhone 12 Pro as a LiDAR scanner). However, LiDAR
    technology is rapidly evolving, and new state-of-the-art LiDARs are constantly
    being developed, thus potentially having more applications in various scenarios.
    For instance, the InnovizOne MEMS LiDAR, manufactured by EDOM technology, achieves
    an angular resolution of up to $0.1^{\circ}\times 0.1^{\circ}$, producing much
    denser point clouds than before.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，LiDAR 仍然采用 dToF 原理，dToF 和 LiDAR 这两个术语在许多场景中甚至可以互换（例如，苹果在 iPad Pro 和 iPhone
    12 Pro 中将 dToF 传感器称为 LiDAR 扫描仪）。然而，LiDAR 技术正在迅速发展，新的尖端 LiDAR 不断被开发出来，从而可能在各种场景中具有更多应用。例如，由
    EDOM 技术公司生产的 InnovizOne MEMS LiDAR 实现了高达 $0.1^{\circ}\times 0.1^{\circ}$ 的角分辨率，产生了比以前更密集的点云。
- en: 'Table 2: Characteristics of iToF and dToF.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：iToF 和 dToF 的特征。
- en: '| Parameter | iToF | dToF |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | iToF | dToF |'
- en: '| --- | --- | --- |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Principle |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 原理 |'
- en: '&#124; Using phase shift between emitted &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用发射光的相位变化 &#124;'
- en: '&#124; and reflected light to determine distance. &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和反射光来确定距离。 &#124;'
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Using stop watch method &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用秒表法 &#124;'
- en: '&#124; to calculate time lapse. &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算时间间隔。 &#124;'
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Detector type |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 探测器类型 |'
- en: '&#124; PMDs: $6\sim 100\mu m$ pixel size &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PMDs: $6\sim 100\mu m$ 像素大小 &#124;'
- en: '| SPADs / APDs |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| SPADs / APDs |'
- en: '| Depth calculation | In-pixel calculation | Histogram analysis |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 深度计算 | 像素内计算 | 直方图分析 |'
- en: '| Performance | Long integration time | Fast acquisition |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 长集成时间 | 快速获取 |'
- en: '| Range | Short-medium | Longer |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 范围 | 短中距离 | 更长 |'
- en: '| Range ambiguity | Yes | No |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 范围模糊 | 是 | 否 |'
- en: '| Pixel count | Large | Smaller |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 像素数量 | 多 | 少 |'
- en: '| Accuracy | Linearly related to distance | Higher |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 精度 | 与距离线性相关 | 更高 |'
- en: '| Power consumption | Low | Higher |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 功耗 | 低 | 更高 |'
- en: '| Portability | Generally compact | Traditionally larger |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 便携性 | 一般较紧凑 | 传统上较大 |'
- en: '| Cost | Medium | Higher |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 成本 | 中等 | 更高 |'
- en: 2.1.3 Comparative Analysis of dToF and iToF
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 dToF 和 iToF 的比较分析
- en: 'Here, we provide their characteristics in Table [2](#S2.T2 "Table 2 ‣ 2.1.2
    Direct ToF ‣ 2.1 Principles of ToF Imaging ‣ 2 Preliminaries and Taxonomy ‣ RGB
    Guided ToF Imaging System: A Survey of Deep Learning-based Methods"). Besides,
    we conduct a comparative analysis on the following aspects.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们在表 [2](#S2.T2 "Table 2 ‣ 2.1.2 Direct ToF ‣ 2.1 Principles of ToF Imaging
    ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided ToF Imaging System: A Survey of Deep
    Learning-based Methods") 中提供了它们的特征。此外，我们对以下方面进行了比较分析。'
- en: Accuracy. iToF cameras typically provide accurate distance measurements within
    a specific range. Its measurement error is theoretically positively correlated
    with the target distance. Currently, the accuracy of mass-produced iToF cameras
    can be controlled within $1\%$, e.g., Helios2 ToF 3D Camera with Sony’s IMX556
    DepthSense ToF has an accuracy of less than 5mm and a precision of less than 2mm
    at 1m camera distance. At the same time, dToF (LiDAR) can achieve millimeter-level
    accuracy even over longer distances. Therefore. for short-range applications,
    both iToF and dToF can achieve comparable accuracy, although they may fall short
    of structured light cameras. However, a key advantage of LiDAR is that its accuracy
    substantially remains constant regardless of distance, as discussed in Sec 2.1.3\.
    This attribute positions LiDAR measurements as reliably accurate in long-range
    applications, distinguishing them from other technologies.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 精度。iToF 相机通常在特定范围内提供准确的距离测量。其测量误差理论上与目标距离正相关。目前，大规模生产的 iToF 相机的精度可控制在 $1\%$
    以内，例如，搭载索尼 IMX556 DepthSense ToF 的 Helios2 ToF 3D 相机在 1 米相机距离下的精度小于 5mm，分辨率小于
    2mm。同时，dToF（LiDAR）即使在较长距离下也能实现毫米级精度。因此，对于短距离应用，iToF 和 dToF 的精度是相当的，尽管它们可能不及结构光相机。然而，LiDAR
    的一个关键优势是其精度在任何距离下都能保持相对恒定，如第 2.1.3 节所讨论。这一特性使得 LiDAR 测量在长距离应用中具有可靠的准确性，使其与其他技术区别开来。
- en: Range and Power consumption. iToF cameras are generally suitable for short to
    medium-range applications. The effective range may vary but typically within a
    few to tens of meters, depending on the specific design and implementation. Because
    of these characteristics, ToF cameras are often designed to be power-efficient
    and, therefore, suitable for consumer-grade devices. In contrast, LiDAR systems
    can achieve longer ranges, making them suitable for applications like autonomous
    vehicles and long-range mapping. Consequently, they are commonly employed in scenarios
    where power consumption is less critical.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 范围和功耗。iToF 相机通常适用于短至中等距离的应用。有效范围可能有所不同，但通常在几米到几十米之间，具体取决于设计和实现。这些特点使得 ToF 相机通常被设计为功耗高效，因此适合消费级设备。相比之下，LiDAR
    系统可以实现更长的测量范围，使其适用于如自动驾驶车辆和长距离测绘等应用。因此，它们通常用于对功耗要求不那么严格的场景。
- en: Portability. iToF cameras are inherently compact and lightweight, making them
    suitable for integration into portable devices such as smartphones, tablets, and
    wearable gadgets. Meanwhile, LiDAR, traditionally larger, is undergoing miniaturization
    trends, especially with the development of solid-state LiDAR, making it increasingly
    applicable in portable and handheld devices.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 便携性。iToF 相机本质上体积小巧且轻便，适合集成到智能手机、平板电脑和可穿戴设备等便携设备中。同时，传统上较大的 LiDAR 正在经历小型化趋势，特别是固态
    LiDAR 的发展，使其在便携和手持设备中的应用越来越广泛。
- en: Applications. Due to the properties peculiar to each, the two families of sensors
    are usually involved in different downstream applications. Specifically, iToF
    sensors are the best fit for AR/VR experiences, which mainly focus on narrow,
    close-range environments or, in general, for any applications running in a constrained
    space (e.g., bin picking with a robotic arm). On the contrary, dToF is preferable
    in contexts requiring long-range perception, e.g., the automotive one with ADAS
    or autonomous driving systems. It is worth noting that there is an overlap in
    the operating ranges of iToF and dToF, i.e., both can be used for applications
    such as AR/VR (e.g., Kinect v2 with iToF and iPad Pro with dToF) when performing
    medium-range sensing.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 应用。由于各自的特性，两类传感器通常涉及不同的下游应用。具体来说，iToF 传感器最适合用于 AR/VR 体验，主要关注狭窄、近距离环境，或者一般在受限空间内运行的应用（例如，机器人臂的箱体拾取）。相反，dToF
    更适合需要长距离感知的上下文，例如带有 ADAS 或自动驾驶系统的汽车。值得注意的是，iToF 和 dToF 的操作范围存在重叠，即在执行中等距离感知时，二者均可用于
    AR/VR 应用（例如，带有 iToF 的 Kinect v2 和带有 dToF 的 iPad Pro）。
- en: Though both imaging systems currently excel in their respective application
    domains, advancements in technology are steadily bridging the gap between their
    performance metrics, leading to a growing overlap in their practical uses.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两种成像系统在各自的应用领域中表现出色，但技术的进步正在稳步缩小它们性能指标之间的差距，导致它们在实际应用中的重叠逐渐增多。
- en: 2.2 Problem Formulation
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 问题表述
- en: Although ToF cameras have many advantages, they also bring some inherent limitations,
    such as multipath interference [[45](#bib.bib45), [46](#bib.bib46)], flying pixels [[119](#bib.bib119)]
    and wiggling errors[[59](#bib.bib59)]. Hand-crafted models can deal with some
    issues, e.g., depth correction, but others require additional cues, such as those
    available in RGB images.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 ToF 相机有许多优点，但它们也带来一些固有的限制，如多径干扰[[45](#bib.bib45)、[46](#bib.bib46)]、飞行像素[[119](#bib.bib119)]和抖动误差[[59](#bib.bib59)]。手工制作的模型可以解决一些问题，例如深度校正，但其他问题需要额外的线索，例如
    RGB 图像中提供的那些线索。
- en: The goal of RGB guided ToF imaging is to recover a high-quality depth map $Z_{hq}$
    from the acquired low-quality depth map $Z_{lq}:\Omega_{Z}\subset\Omega\mapsto\mathbb{R}_{+}$
    and high-resolution color image $I:\Omega\subset\mathbb{R}^{2}\mapsto\mathbb{R}^{3}_{+}$.
    Depending on the setup of ranging systems, low-quality depth maps can be either
    dense, yet at low resolution, or sparse. Besides, we assume all LQ depth maps
    and the corresponding color images are correctly aligned. This is possible if
    the RGB-D system is calibrated, i.e., the relative pose between the two color
    and depth cameras is known. If a deep neural network $\Phi_{\gamma}$ with parameters
    $\gamma$ is deployed for RGB guided ToF imaging, the task can be modeled as
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: RGB 引导的 ToF 成像的目标是从获取的低质量深度图 $Z_{lq}:\Omega_{Z}\subset\Omega\mapsto\mathbb{R}_{+}$
    和高分辨率彩色图像 $I:\Omega\subset\mathbb{R}^{2}\mapsto\mathbb{R}^{3}_{+}$ 中恢复出高质量深度图
    $Z_{hq}$。根据测距系统的设置，低质量深度图可以是稠密的但分辨率较低，或者是稀疏的。此外，我们假设所有 LQ 深度图和相应的彩色图像是正确对齐的。这是可能的，如果
    RGB-D 系统已校准，即两台彩色和深度摄像机之间的相对姿态是已知的。如果部署了具有参数 $\gamma$ 的深度神经网络 $\Phi_{\gamma}$
    用于 RGB 引导的 ToF 成像，则任务可以建模为
- en: '|  | $\hat{Z}_{hq}=\Phi_{\gamma}(Z_{lq},I)$ |  | (13) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{Z}_{hq}=\Phi_{\gamma}(Z_{lq},I)$ |  | (13) |'
- en: where $\hat{Z}_{hq}$ is the prediction of latent HR depth map.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{Z}_{hq}$ 是潜在 HR 深度图的预测值。
- en: 'The network parameters $\gamma$ are updated during training of $\Phi$, by solving
    the following optimization problem:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 网络参数 $\gamma$ 在训练 $\Phi$ 时进行更新，通过解决以下优化问题：
- en: '|  | $\hat{\gamma}=\mathop{\arg\min}\limits_{\gamma}\mathcal{L}(Z_{hq},\hat{Z}_{hq})$
    |  | (14) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\gamma}=\mathop{\arg\min}\limits_{\gamma}\mathcal{L}(Z_{hq},\hat{Z}_{hq})$
    |  | (14) |'
- en: where $\mathcal{L}$ is an objective function, usually minimizing the distance
    between the prediction and the ground truth depth.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}$ 是一个目标函数，通常是最小化预测与地面真值深度之间的距离。
- en: 2.3 Evaluation metrics
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 评估指标
- en: 'As for most computer vision tasks, for RGB guided ToF imaging, it is necessary
    to employ appropriate evaluation metrics. The most commonly used measures for
    both the two sub-tasks, i.e., GDSR and GDC, are root mean squared error (RMSE)
    and mean absolute error (MAE):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数计算机视觉任务而言，对于 RGB 引导的 ToF 成像，必须使用适当的评估指标。两项子任务，即 GDSR 和 GDC，最常用的度量是均方根误差（RMSE）和平均绝对误差（MAE）：
- en: '|  | $\displaystyle\mathrm{RMSE}(mm)=\sqrt{\frac{1}{N}\sum_{p\in N}(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2}}$
    |  | (15) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{RMSE}(mm)=\sqrt{\frac{1}{N}\sum_{p\in N}(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2}}$
    |  | (15) |'
- en: '|  | $\displaystyle N\mathrm{MAE}(mm)=\frac{1}{N}\sum_{p\in N}&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;$
    |  | (16) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle N\mathrm{MAE}(mm)=\frac{1}{N}\sum_{p\in N}&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;$
    |  | (16) |'
- en: with $p$ being a single pixel in the depth map.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p$ 是深度图中的一个像素。
- en: 'Metrics for GDSR. In addition to the above metrics, mean squared error (MSE)
    is frequently utilized, which has a similar role to RMSE:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: GDSR 的指标。除了上述指标，均方误差（MSE）也被频繁使用，其作用类似于 RMSE：
- en: '|  | $\mathrm{MSE}=\frac{1}{N}\sum_{p\in N}&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;^{2}$
    |  | (17) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{MSE}=\frac{1}{N}\sum_{p\in N}&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;^{2}$
    |  | (17) |'
- en: 'Furthermore, with a short-range ToF sensor, GDSR focuses on recovering depth
    maps with desirable details. As a result, peak signal-to-noise ratio (PNSR) and
    structural similarity index (SSIM) are also sometimes used to assess the quality
    of depth maps, which are defined as:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用短距离 ToF 传感器时，GDSR 侧重于恢复具有理想细节的深度图。因此，峰值信噪比（PNSR）和结构相似性指数（SSIM）有时也被用来评估深度图的质量，定义如下：
- en: '|  | $\displaystyle\mathrm{PSNR}=10log_{10}(\frac{Z_{max}^{2}}{\mathrm{MSE}})$
    |  | (18) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{PSNR}=10\log_{10}(\frac{Z_{max}^{2}}{\mathrm{MSE}})$
    |  | (18) |'
- en: '|  | $\displaystyle\mathrm{SSIM}(p,q)=\frac{(2\mu_{p}\mu_{q}+C_{1})(2\sigma_{xy}+C_{2})}{(\mu_{p}^{2}+\mu_{q}^{2}+C_{1})(\sigma_{p}^{2}+\sigma_{q}^{2}+C_{2})}$
    |  | (19) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{SSIM}(p,q)=\frac{(2\mu_{p}\mu_{q}+C_{1})(2\sigma_{xy}+C_{2})}{(\mu_{p}^{2}+\mu_{q}^{2}+C_{1})(\sigma_{p}^{2}+\sigma_{q}^{2}+C_{2})}$
    |  | (19) |'
- en: with $Z_{max}$ being the maximum depth value; $q$ is a neighbor of pixel $p$;
    $\mu_{p}$ and $\mu_{q}$ denote the mean values of HQ depth maps and the corresponding
    ground truth, respectively; $\sigma_{p}^{2}$ and $\sigma_{q}^{2}$ are the variance;
    $C_{1}$ and $C_{2}$ are constants used to maintain the stability of the division.
    The smaller the pixel value difference between the two depth maps, the higher
    the PSNR.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Z_{max}$ 是最大深度值；$q$ 是像素 $p$ 的邻居；$\mu_{p}$ 和 $\mu_{q}$ 分别表示 HQ 深度图和相应地面真值的均值；$\sigma_{p}^{2}$
    和 $\sigma_{q}^{2}$ 是方差；$C_{1}$ 和 $C_{2}$ 是用于保持分割稳定性的常数。两个深度图之间的像素值差异越小，PSNR 越高。
- en: Metrics for GDC. Sparse depth measurements are often captured from a long-range
    ToF LiDAR, so there are several task-specific metrics for GDC, including RMSE
    of the inverse depth (iRMSE), MAE of the inverse depth (iMAE), which are defined
    by
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: GDC的度量。稀疏深度测量通常是从长距离ToF LiDAR捕获的，因此GDC有几个特定任务的度量，包括逆深度的RMSE（iRMSE）、逆深度的MAE（iMAE），定义如下
- en: '|  | $\displaystyle\mathrm{iRMSE}(\frac{1}{km})=\sqrt{\frac{1}{n}\sum_{n\in
    N}(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2}}$ |  | (20) |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{iRMSE}(\frac{1}{km})=\sqrt{\frac{1}{n}\sum_{n\in
    N}(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2}}$ |  | (20) |'
- en: '|  | $\displaystyle\mathrm{iMAE}(\frac{1}{km})=\frac{1}{N}\sum_{p\in N}&#124;\frac{1}{Z_{hq}^{p}}-\frac{1}{\hat{Z}_{hq}^{p}}&#124;$
    |  | (21) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{iMAE}(\frac{1}{km})=\frac{1}{N}\sum_{p\in N}&#124;\frac{1}{Z_{hq}^{p}}-\frac{1}{\hat{Z}_{hq}^{p}}&#124;$
    |  | (21) |'
- en: When the evaluation of deep models is performed on indoor datasets, e.g., NYU-v2 [[138](#bib.bib138)],
    mean absolute relative error (REL) and thresholded accuracy are more popular
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当对室内数据集（例如NYU-v2 [[138](#bib.bib138)]）进行深度模型评估时，平均绝对相对误差（REL）和阈值准确率更为流行
- en: '|  | $\displaystyle\mathrm{REL}(mm)=\frac{1}{N}\sum_{n\in N}\frac{&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;}{Z_{hq}^{p}}$
    |  | (22) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{REL}(mm)=\frac{1}{N}\sum_{n\in N}\frac{&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;}{Z_{hq}^{p}}$
    |  | (22) |'
- en: '|  | $\displaystyle\sigma=\max(\frac{Z_{hq}^{p}}{\hat{Z}_{hq}^{p}},\frac{\hat{Z}_{hq}^{p}}{Z_{hq}^{p}})<th$
    |  | (23) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sigma=\max(\frac{Z_{hq}^{p}}{\hat{Z}_{hq}^{p}},\frac{\hat{Z}_{hq}^{p}}{Z_{hq}^{p}})<th$
    |  | (23) |'
- en: with $th$ being a given threshold.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$th$为给定的阈值。
- en: 2.4 Taxonomy
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 分类
- en: '| ![Refer to caption](img/268cb138edff4951bd613eecdc47604b.png) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/268cb138edff4951bd613eecdc47604b.png) |'
- en: 'Figure 5: Taxonomy of RGB guided ToF imaging techniques. We identify 5 and
    6 main categories for GDSR and GDC methods.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：RGB引导的ToF成像技术的分类。我们为GDSR和GDC方法识别了5类和6类主要类别。
- en: 'Figure [5](#S2.F5 "Figure 5 ‣ 2.4 Taxonomy ‣ 2 Preliminaries and Taxonomy ‣
    RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods") anticipates
    the proposed taxonomy for methods we will survey in this paper. On the one hand,
    from the figure, we can already highlight some common trends in GDSR and GDC tasks.
    Specifically, we can identify the category of Dual-branch Frameworks in both,
    i.e., those deploying two main branches for processing RGB and depth cues separately,
    as well as the category of Auxiliary-based Methods – those exploiting multi-task
    learning to improve the accuracy of the predicted depth map – Fusion-focused Methods
    which mainly study the fusion strategy for combining color and depth features,
    and the Unsupervised Methods not requiring ground-truth labels at training time.
    On the other hand, we identify some trends that are peculiar to one of the two
    tasks: specifically, for GDSR, we have Hybrid Methods, focusing on obtaining explainable
    models to run the super-resolution process; for GDC, Affinity-based Methods developed
    are characterized by the used of Spatial Propagation Networks (SPNs) which learn
    affinity matrices for propagating depth across neighboring pixels, and Uncertainty-guided
    Methods explicitly involve uncertainty modeling to improve the results. Each category
    will be introduced and discussed in detail in Sections[3](#S3 "3 Guided Depth
    Super-Resolution ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods") and [4](#S4 "4 Guided Depth Completion ‣ RGB Guided ToF Imaging System:
    A Survey of Deep Learning-based Methods"), for GDSR and GDC, respectively.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S2.F5 "Figure 5 ‣ 2.4 Taxonomy ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided
    ToF Imaging System: A Survey of Deep Learning-based Methods")展示了我们将在本文中调查的方法的分类。图中，我们可以突出显示GDSR和GDC任务的一些共同趋势。具体来说，我们可以识别两个类别：双分支框架，即那些分别处理RGB和深度线索的两个主要分支，以及辅助方法——那些利用多任务学习来提高预测深度图的准确性——融合方法主要研究将颜色和深度特征融合的策略，以及不需要在训练时使用真实标签的无监督方法。另一方面，我们识别了一些特定于这两个任务之一的趋势：特别是，对于GDSR，我们有混合方法，重点是获得可解释的模型来运行超分辨率过程；对于GDC，开发的亲和力方法的特点是使用空间传播网络（SPNs），它们学习亲和矩阵以在邻近像素之间传播深度，以及不确定性引导方法明确涉及不确定性建模以改善结果。每个类别将在[3](#S3
    "3 Guided Depth Super-Resolution ‣ RGB Guided ToF Imaging System: A Survey of
    Deep Learning-based Methods")和[4](#S4 "4 Guided Depth Completion ‣ RGB Guided
    ToF Imaging System: A Survey of Deep Learning-based Methods")节中详细介绍和讨论，分别针对GDSR和GDC。'
- en: 3 Guided Depth Super-Resolution
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 引导深度超分辨率
- en: 'In this section, we present and discuss recent deep learning-based approaches.
    Initially, researchers exploited general architectures with two branches to extract
    features from RGB and depth separately, which are fused in a naive manner, i.e.,
    by means of concatenation or summation. Numerous novel architectures and fusion
    schemes have been proposed to boost network accuracy. From the perspective of
    data requirements, depth super-resolution methods are divided into supervised
    and unsupervised learning. The difference between the two paradigms is that the
    former requires labeled input data, while the latter does not. For what concerns
    supervised learning, we classify existing works into three categories, depending
    on the main contribution they bring to the literature, in terms of (i) Dual-branch
    framework; (ii) optimization-inspired architecture; (iii) auxiliary learning;
    (iv) multi-modal fusion scheme. For unsupervised learning, the emphasis is on
    the fusion strategy and loss functions. These five categories are listed in the
    top branch of Fig. [5](#S2.F5 "Figure 5 ‣ 2.4 Taxonomy ‣ 2 Preliminaries and Taxonomy
    ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods").'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们介绍并讨论了基于深度学习的最新方法。最初，研究人员利用具有两个分支的通用架构分别从RGB和深度中提取特征，这些特征以简单的方式融合，即通过拼接或求和。提出了许多新颖的架构和融合方案以提高网络准确性。从数据需求的角度来看，深度超分辨率方法分为监督学习和无监督学习。两种范式的区别在于前者需要标记的输入数据，而后者则不需要。对于监督学习，我们将现有工作分类为三类，具体取决于它们对文献的主要贡献，包括（i）双分支框架；（ii）优化启发的架构；（iii）辅助学习；（iv）多模态融合方案。对于无监督学习，重点是融合策略和损失函数。这五类在图[5](#S2.F5
    "Figure 5 ‣ 2.4 Taxonomy ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided ToF Imaging
    System: A Survey of Deep Learning-based Methods")的顶部分支中列出。'
- en: Supervised methods are supplemented by labeled data so that the task can be
    viewed as a regression problem. Thus, deep models aim to learn depth predictions
    as close as possible to the ground truth depth labels.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 监督方法通过标记的数据进行补充，使得任务可以被视为回归问题。因此，深度模型旨在将深度预测尽可能接近真实的深度标签。
- en: 3.1 Dual-branch Framework
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 双分支框架
- en: 'Since the depth sensor captures the same scene as the RGB camera, the obtained
    two images are geometrically similar and complementary. Therefore, how to extract
    meaningful features from the two modalities through designed algorithms is the
    key to depth super-resolution. Intuitively, [[79](#bib.bib79)] design a deep joint
    filter (DJFR) which contains two sub-networks for extracting depth and RGB features,
    respectively. After concatenating the two streams, the fused information is decoded
    by another sub-network and outputs the predicted depth, as seen in Fig.[6](#S3.F6
    "Figure 6 ‣ 3.1 Dual-branch Framework ‣ 3 Guided Depth Super-Resolution ‣ RGB
    Guided ToF Imaging System: A Survey of Deep Learning-based Methods").'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '由于深度传感器捕捉到的场景与RGB相机相同，因此获得的两幅图像在几何上是相似且互补的。因此，如何通过设计的算法从这两种模态中提取有意义的特征是深度超分辨率的关键。直观上，[[79](#bib.bib79)]
    设计了一个深度联合滤波器（DJFR），该滤波器包含两个子网络，分别用于提取深度和RGB特征。将这两个流拼接后，融合的信息由另一个子网络解码，并输出预测的深度，如图[6](#S3.F6
    "Figure 6 ‣ 3.1 Dual-branch Framework ‣ 3 Guided Depth Super-Resolution ‣ RGB
    Guided ToF Imaging System: A Survey of Deep Learning-based Methods")所示。'
- en: Multi-scale information is crucial for many low-level vision tasks, including
    GDSR, which can recover images from different scales through rich hierarchical
    features. The multi-scale guided convolutional network (MSG-Net) [[58](#bib.bib58)]
    is the first work to super-solve depth maps using multi-scale guidance, which
    progressively enhances depth details from high- to low-level. Inspired by this
    idea, numerous methods utilizing multi-scale guidance have been proposed and attained
    significantly improved performance. For instance, a novel deep network for depth
    map super-resolution (DepthSR) proposed by[[44](#bib.bib44)] leverages a residual
    U-Net architecture[[129](#bib.bib129)], where hierarchical guidance is introduced
    at each scale, to recover HR depth maps. [[209](#bib.bib209)] design a multi-scale
    architecture with a guide similar to MSG-Net[[58](#bib.bib58)] where dense layers[[57](#bib.bib57)]
    are employed to revisit the features from all higher levels at a given scale.
    Also, guided by RGB images, the multi-scale fusion residual network for GDSR (MFR-SR)
    proposed by [[211](#bib.bib211)] progressively upsamples depth maps in multiple
    scales with global and local residual learning. [[77](#bib.bib77)] employ a multi-scale
    strategy with the proposed symmetric unit (SU) as the basic component of the network.
    With SU, it can effectively deal with textureless and edge features in depth images.
    Unlike previous works, [[210](#bib.bib210)] propose a two-stream multi-scale network,
    MIG-net, including three branches, i.e., intensity, depth, and gradient branches.
    At each scale, the depth and gradient features are iteratively refined by the
    guide. [[197](#bib.bib197)] introduce an attention-based hierarchical multi-modal
    fusion network (AHMF) where a bi-directional hierarchical feature collaboration
    module is proposed to make full use of multi-scale features. In this module, features
    from different levels are aggregated so that low and high level spatial information
    can collaborate to improve each other. [[160](#bib.bib160)] present a novel method
    that exploits pyramid structure to capture multi-scale features so that HR depth
    maps can be progressively recovered. [[187](#bib.bib187)] propose a structure
    flow-guided framework for GDSR, whose core is to learn a structure flow map to
    guide structure representation transfer. In this method, a flow-enhance pyramid
    edge attention network is introduced to learn multi-scale features for sharp edge
    reconstruction.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度信息对许多低层次视觉任务至关重要，包括 GDSR，它可以通过丰富的层次特征从不同尺度恢复图像。多尺度引导卷积网络（MSG-Net）[[58](#bib.bib58)]
    是首个利用多尺度引导来超分辨率深度图的工作，该方法逐步增强从高层到低层的深度细节。受到这一理念的启发，提出了众多利用多尺度引导的方法，并取得了显著的性能提升。例如，[[44](#bib.bib44)]
    提出的用于深度图超分辨率（DepthSR）的新型深度网络利用了残差 U-Net 结构[[129](#bib.bib129)]，在每个尺度引入层次引导，以恢复高分辨率深度图。[[209](#bib.bib209)]
    设计了一个类似于 MSG-Net[[58](#bib.bib58)] 的多尺度架构，其中采用密集层[[57](#bib.bib57)] 以在给定尺度上重新访问所有更高层次的特征。此外，[[211](#bib.bib211)]
    提出的 RGB 图像引导的多尺度融合残差网络用于 GDSR（MFR-SR），通过全局和局部残差学习在多个尺度上逐步上采样深度图。[[77](#bib.bib77)]
    采用了一种多尺度策略，提出了对称单元（SU）作为网络的基本组件。通过 SU，它可以有效处理深度图像中的无纹理和边缘特征。与之前的工作不同，[[210](#bib.bib210)]
    提出了一个包含三个分支（即强度、深度和梯度分支）的双流多尺度网络 MIG-net。在每个尺度上，深度和梯度特征通过引导进行迭代精炼。[[197](#bib.bib197)]
    引入了一种基于注意力的层次多模态融合网络（AHMF），其中提出了一个双向层次特征协作模块，以充分利用多尺度特征。在该模块中，不同层次的特征被聚合，从而低层次和高层次的空间信息可以协作以相互提升。[[160](#bib.bib160)]
    提出了一种新颖的方法，利用金字塔结构捕获多尺度特征，从而逐步恢复高分辨率深度图。[[187](#bib.bib187)] 提出了一个用于 GDSR 的结构流引导框架，其核心是学习一个结构流图来引导结构表示转换。在此方法中，引入了一个流增强金字塔边缘注意网络，以学习多尺度特征以实现清晰的边缘重建。
- en: '| ![Refer to caption](img/10637d920f41856ab3f49e4b9febaf0e.png) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/10637d920f41856ab3f49e4b9febaf0e.png) |'
- en: 'Figure 6: DJFR architecture [[79](#bib.bib79)] belonging to dual-branch framework.
    It employs a typical dual-branch auto-encoder framework, where the guide and source
    features are extracted from the two sub-networks in the encoder.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：DJFR 架构[[79](#bib.bib79)] 属于双分支框架。它采用了典型的双分支自编码器框架，其中引导和源特征从编码器中的两个子网络中提取。
- en: 'Using a coarse-to-fine scheme is another common solution for GDSR, consisting
    of building a multi-stage network to enhance depth maps progressively. The definitions
    of the multi-scale strategy and the coarse-to-fine scheme may overlap in some
    approaches. Thus, we distinguish them according to the focus of the works. [[167](#bib.bib167)]
    propose a coarse-to-fine neural network with two stages. In the coarse stage,
    the model learns large filter kernels to obtain less accurate results, further
    refined by smaller filter kernels in the fine stage. In [[208](#bib.bib208)],
    the filtering and refinement of depth-guided intensity features and intensity-guided
    depth features are iteratively conducted in multiple stages through the proposed
    depth-guided affine transformation. The network can reduce artifacts produced
    by distribution gaps between depth maps and the corresponding guide. [[183](#bib.bib183)]
    introduce a multi-branch aggregation network with multiple stages to reconstruct
    sharp depth boundaries. Specifically, they design three parallel branches – the
    reconstruction, the color, and the multi-scale branches – in each stage. [[141](#bib.bib141)]
    develop a coarse-to-fine framework consisting of several sub-modules that can
    gradually extract the high-frequency features from depth maps. Each sub-module
    employs a channel attention strategy to obtain informative features. Besides,
    the framework further improves the depth quality through Total Generalized Variation
    (TGV) term and input loss. In [[50](#bib.bib50)], a high-frequency guidance branch
    is developed to adaptively extract the HF information and reduce the LF components.
    The HF components are then fused with depth features by means of concatenation.
    [[186](#bib.bib186)] propose a recurrent structure attention guidance framework
    (RSAG) that utilizes both multi-scale and multi-stage strategies, shown in Fig.[7](#S3.F7
    "Figure 7 ‣ 3.1 Dual-branch Framework ‣ 3 Guided Depth Super-Resolution ‣ RGB
    Guided ToF Imaging System: A Survey of Deep Learning-based Methods"). In this
    framework, a deep contrastive network with multi-scale filters is designed to
    adaptively separate HF and LF features.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用粗到细方案是GDSR的另一种常见解决方案，包括建立一个多阶段网络以逐步增强深度图。多尺度策略和粗到细方案的定义在一些方法中可能会重叠。因此，我们根据工作的重点来区分它们。[[167](#bib.bib167)]提出了一种具有两个阶段的粗到细神经网络。在粗阶段，模型学习大滤波器核以获得较不精确的结果，在细阶段则通过较小的滤波器核进一步精炼。在[[208](#bib.bib208)]中，通过提出的深度引导仿射变换，在多个阶段迭代地进行深度引导强度特征和强度引导深度特征的过滤和精炼。该网络可以减少深度图与相应引导之间分布差距产生的伪影。[[183](#bib.bib183)]引入了一种具有多个阶段的多分支聚合网络，以重建锐利的深度边界。具体而言，他们在每个阶段设计了三个并行分支——重建分支、颜色分支和多尺度分支。[[141](#bib.bib141)]开发了一种粗到细框架，由若干子模块组成，能够逐步从深度图中提取高频特征。每个子模块采用通道注意策略以获取信息性特征。此外，该框架通过总广义变差（TGV）项和输入损失进一步提高深度质量。在[[50](#bib.bib50)]中，开发了一种高频引导分支，以自适应地提取高频信息并减少低频组件。然后通过连接将高频组件与深度特征融合。[[186](#bib.bib186)]提出了一种递归结构注意引导框架（RSAG），该框架利用了多尺度和多阶段策略，如图[7](#S3.F7
    "图7 ‣ 3.1 双分支框架 ‣ 3 引导深度超分辨率 ‣ RGB引导ToF成像系统：基于深度学习的方法综述")所示。在此框架中，设计了一个具有多尺度滤波器的深度对比网络，以自适应地分离高频和低频特征。
- en: '| ![Refer to caption](img/740fb5aabd80468f6a0f90245d520419.png) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| ![参考标题](img/740fb5aabd80468f6a0f90245d520419.png) |'
- en: 'Figure 7: RSAG architecture [[186](#bib.bib186)] belonging to dual-branch framework.
    It progressively refines the depth using a coarse-to-fine strategy.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：属于双分支框架的RSAG架构[[186](#bib.bib186)]。它使用粗到细的策略逐步细化深度。
- en: 'Instead of deploying spatially-invariant kernels in vanilla convolutional neural
    networks (CNNs), some methods design learnable kernels to boost the GDSR performance.
    [[68](#bib.bib68)] propose a deformable network that provides a custom convolution
    kernel for each pixel by computing neighborhood weights. The kernel weights can
    be obtained not only by calculating the pixels in regular positions (e.g., 8-neighborhood),
    but also by calculating its sub-pixels after interpolation. [[161](#bib.bib161)]
    introduce a continuous depth representation whose core is the proposed geometric
    spatial aggregator (GSA). The GSA comprises two parts: (1) the geometric encoder
    using the scale-modulated distance field to build the correlation between pixels
    and (2) the learnable kernel learning typical texture pattern processing prior.
    Thanks to the continuous representation, the model can super-solve depth at an
    arbitrary scale. [[198](#bib.bib198)] design a kernel generation network that
    can cope with inconsistent structures between RGB and depth.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法设计了可学习的卷积核以提升GDSR性能，而不是在普通卷积神经网络（CNNs）中部署空间不变核。[[68](#bib.bib68)] 提出了一种可变形网络，通过计算邻域权重为每个像素提供自定义卷积核。核权重不仅可以通过计算常规位置的像素（例如8邻域）来获得，还可以通过插值后的子像素来获得。[[161](#bib.bib161)]
    引入了一种连续深度表示，其核心是所提议的几何空间聚合器（GSA）。GSA包括两个部分：（1）使用尺度调制距离场建立像素之间相关性的几何编码器，（2）可学习核以学习典型的纹理模式处理先验。由于连续表示，该模型可以在任意尺度上超分辨深度。[[198](#bib.bib198)]
    设计了一种核生成网络，可以应对RGB和深度之间不一致的结构。
- en: To increase the interpretability, [[149](#bib.bib149)] regards GDSR as a neural
    implicit interpolation problem, which can map continuous coordinates of LR depth
    and HR color images into latent codes. Meanwhile, they learn the interpolation
    weights to establish a unified framework to yield the interpolation weights and
    values. To promote the deployment of dToF cameras in mobile devices, [[80](#bib.bib80)]
    perform depth estimation from a lightweight ToF sensor and RGB image (DELTAR).
    In this work, PointNet[[117](#bib.bib117)] and Efficient B5[[148](#bib.bib148)]
    are used to extract depth and RGB features, respectively, fused at the decoder
    stage through the proposed transformer-based fusion module.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加解释性，[[149](#bib.bib149)] 将GDSR视为一个神经隐式插值问题，该问题可以将低分辨率深度图和高分辨率彩色图像的连续坐标映射到潜在编码中。同时，他们学习插值权重以建立一个统一框架，从而生成插值权重和值。为了促进dToF相机在移动设备中的部署，[[80](#bib.bib80)]
    从轻量级的ToF传感器和RGB图像（DELTAR）中执行深度估计。在这项工作中，PointNet[[117](#bib.bib117)] 和 Efficient
    B5[[148](#bib.bib148)] 被用来提取深度和RGB特征，分别在解码阶段通过所提议的基于变压器的融合模块进行融合。
- en: 3.2 Hybrid Architecture
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 混合架构
- en: As neural networks are usually described as black-box models, some researchers
    combine optimization methods to obtain explainable frameworks. [[126](#bib.bib126)]
    present a novel network consisting of two sub-networks, i.e., a fully-convolutional
    network and a primal-dual network, with the first yielding HR depth maps and weights
    and the latter producing the final results with a non-local variational method.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络通常被描述为黑箱模型，一些研究人员结合优化方法以获得可解释的框架。[[126](#bib.bib126)] 提出了一种新颖的网络，由两个子网络组成，即一个全卷积网络和一个原始-对偶网络，第一个生成高分辨率深度图和权重，第二个使用非局部变分方法生成最终结果。
- en: As an optimization-inspired network, a weighted analysis sparse representation
    (WASR) model [[40](#bib.bib40)] is designed for GDSR. It leverages a neural network
    to learn filter parameterizations and non-linear functions to build more flexible
    stage-wise operations. Inspired by the multi-modal convolutional sparse coding
    model[[139](#bib.bib139)], [[21](#bib.bib21)] develop a network that can adaptively
    separate the shared features between different modalities from the unique features
    existing in a single modality. [[108](#bib.bib108)] propose a deep unfolding network
    to efficiently compute the convolutional sparse coding with the guide.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个受优化启发的网络，加权分析稀疏表示（WASR）模型[[40](#bib.bib40)] 专为GDSR设计。它利用神经网络来学习滤波器参数化和非线性函数，以构建更灵活的分阶段操作。受多模态卷积稀疏编码模型[[139](#bib.bib139)]
    的启发，[[21](#bib.bib21)] 开发了一个网络，可以自适应地分离不同模态之间的共享特征与单一模态中存在的独特特征。[[108](#bib.bib108)]
    提出了一个深度展开网络，以高效地计算卷积稀疏编码。
- en: '| ![Refer to caption](img/318f6ccc33ae80571167f0d5d15dfad5.png) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/318f6ccc33ae80571167f0d5d15dfad5.png) |'
- en: 'Figure 8: LGR architecture [[18](#bib.bib18)] belonging to optimization-inspired
    architecture. The optimization layer is plugged into the end of the network to
    predict the high-resolution depth.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：属于优化驱动架构的LGR架构[[18](#bib.bib18)]。优化层被插入到网络的末端，以预测高分辨率深度。
- en: '[[196](#bib.bib196)] introduce discrete cosine transform (DCT) into the proposed
    neural network to make the model explainable. This model utilizes DCT to tackle
    the optimization issue for GDSR by reconstructing the multi-channel HR depth features.
    Another advantage of using DCT is that it can make network design easier because
    it does not need to learn the mapping function between LR/HR image pairs, as it
    explicitly models it. [[18](#bib.bib18)] propose a novel architecture learning
    graph regularization (LGR) with a neural network to perform GDSR, as shown in
    Fig.[8](#S3.F8 "Figure 8 ‣ 3.2 Hybrid Architecture ‣ 3 Guided Depth Super-Resolution
    ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods"). First,
    a network extracts features from the source and the guide. A graph constructed
    using such features is then sent to a differentiable optimization layer to regularize
    it. Combining the advantages of model-based and learning-based methods, [[200](#bib.bib200)]
    propose a memory-augmented deep unfolding network in which local implicit prior
    and global implicit prior are introduced based on a maximal posterior view. To
    further prevent information loss, long short-term unit (LSTM) is employed in the
    persistent memory mechanism. [[110](#bib.bib110)] propose integrating guided anisotropic
    diffusion into a CNN to enhance depth discontinuities in the depth images. It
    has a fixed inference time and memory footprint, given a scale factor.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[[196](#bib.bib196)] 将离散余弦变换（DCT）引入到提议的神经网络中，以使模型具有可解释性。该模型利用DCT通过重建多通道高分辨率深度特征来解决GDSR的优化问题。使用DCT的另一个优点是它可以简化网络设计，因为它不需要学习低分辨率/高分辨率图像对之间的映射函数，因为它明确地对其建模。[[18](#bib.bib18)]
    提出了一个新颖的架构——学习图正则化（LGR），通过神经网络来执行GDSR，如图[8](#S3.F8 "Figure 8 ‣ 3.2 Hybrid Architecture
    ‣ 3 Guided Depth Super-Resolution ‣ RGB Guided ToF Imaging System: A Survey of
    Deep Learning-based Methods")所示。首先，网络从源图像和引导图像中提取特征。然后，利用这些特征构建的图被送入一个可微分的优化层进行正则化。结合了基于模型和基于学习方法的优点，[[200](#bib.bib200)]
    提出了一个记忆增强的深度展开网络，其中引入了基于最大后验视角的局部隐式先验和全局隐式先验。为了进一步防止信息丢失，持久记忆机制中使用了长短期记忆（LSTM）。[[110](#bib.bib110)]
    提出了将引导各向异性扩散集成到CNN中，以增强深度图像中的深度不连续性。给定一个缩放因子，它具有固定的推理时间和内存占用。'
- en: 3.3 Auxiliary-based Methods
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于辅助的方法
- en: 'Auxiliary learning, also known as multi-task learning, aims to find or design
    auxiliary tasks that can improve the performance of one or several primary tasks.
    Driven by its recent progress, several works attempt to exploit auxiliary task
    learning to upsample depth maps. [[145](#bib.bib145)] propose a knowledge distillation
    approach where depth estimation is employed as an auxiliary task during training
    to improve the results. As a representative method shown in Fig.[9](#S3.F9 "Figure
    9 ‣ 3.3 Auxiliary-based Methods ‣ 3 Guided Depth Super-Resolution ‣ RGB Guided
    ToF Imaging System: A Survey of Deep Learning-based Methods"), BridgeNet [[151](#bib.bib151)]
    explores a paradigm where the association between monocular depth estimate (MDE)
    and depth super-resolution (DSR) is exploited deeply. To make the two tasks work
    together, they design two auto-encoders, namely DSRNet and MDENet, with information
    interaction at the encoder stage. In MDENet, a high-frequency attention bridge
    is proposed to capture HF information which can guide depth upsampling in the
    other task. In contrast, the content guidance is provided by DSRNet to guide monocular
    depth estimates. Considering the distinct learning curves of the two sub-tasks,
    they optimize the two sub-networks separately.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '辅助学习，也称为多任务学习，旨在找到或设计可以提高一个或多个主要任务性能的辅助任务。受到最近进展的驱动，一些研究尝试利用辅助任务学习来放大深度图。[[145](#bib.bib145)]
    提出了一种知识蒸馏方法，其中在训练期间将深度估计作为辅助任务，以提高结果。作为一种代表性方法，如图[9](#S3.F9 "Figure 9 ‣ 3.3 Auxiliary-based
    Methods ‣ 3 Guided Depth Super-Resolution ‣ RGB Guided ToF Imaging System: A Survey
    of Deep Learning-based Methods")所示，BridgeNet [[151](#bib.bib151)] 探索了一种利用单目深度估计（MDE）和深度超分辨率（DSR）之间的关联的范式。为了使这两个任务协同工作，他们设计了两个自编码器，即DSRNet和MDENet，在编码器阶段进行信息交互。在MDENet中，提出了一种高频注意力桥来捕捉高频信息，从而指导另一任务中的深度上采样。相对而言，DSRNet提供内容指导，以指导单目深度估计。考虑到这两个子任务的不同学习曲线，他们分别优化了两个子网络。'
- en: In addition to auxiliary learning, some researchers consider using auxiliary
    guidance from color images to improve depth quality. Based on the importance of
    edge information in depth upsampling, [[165](#bib.bib165)] propose an edge-guided
    depth upsampling framework that leverages edge maps as the guide to recover HR
    depth maps. For training, the ground truth edge map is computed by the Canny operator
    on the corresponding HR depth map.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 除了辅助学习，一些研究者考虑利用颜色图像的辅助指导来提高深度质量。基于边缘信息在深度上采样中的重要性，[[165](#bib.bib165)] 提出了一个边缘引导的深度上采样框架，该框架利用边缘图作为指导来恢复高分辨率深度图。对于训练，地面真实边缘图通过Canny算子在相应的高分辨率深度图上计算得到。
- en: '| ![Refer to caption](img/6e53721dfc4da3ae9e8eda42ea86a096.png) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/6e53721dfc4da3ae9e8eda42ea86a096.png) |'
- en: 'Figure 9: BridgeNet architecture [[151](#bib.bib151)] belonging to auxiliary-based
    methods. The BridgeNet consists of a monocular depth estimation subnetwork and
    a guided depth upsampling subnetwork.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：属于辅助方法的BridgeNet架构 [[151](#bib.bib151)]。BridgeNet由一个单目深度估计子网络和一个引导深度上采样子网络组成。
- en: Differently from most existing GDSR methods, [[155](#bib.bib155)] attempt to
    gauge the upsampled depth quality through renderings of surface normal maps. More
    specifically, a visual appearance-based loss is proposed, which can assist a baseline
    network in yielding more visually pleasing results.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数现有的GDSR方法不同，[[155](#bib.bib155)] 尝试通过表面法线图的渲染来评估上采样深度的质量。更具体地说，提出了一种基于视觉外观的损失函数，这可以帮助基线网络生成更具视觉吸引力的结果。
- en: 3.4 Fusion-focused Methods
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 专注于融合的方法
- en: 'How to effectively fuse information from different modalities is critical to
    obtain high-quality depth maps. Intuitively, we can perform multi-modal fusion
    with a simple operation, such as concatenation [[165](#bib.bib165), [149](#bib.bib149),
    [18](#bib.bib18)]. However, these fusion schemes can not selectively transfer
    HF features from the guide to the target, and may even result in texture-copying
    artifacts. Therefore, researchers design different fusion schemes to alleviate
    the issue, mainly grouped into three categories: early, late, and multi-level
    fusion. [[77](#bib.bib77)] present a correlation-controlled color guidance block
    (block) to fuse the multi-modal information. [[197](#bib.bib197)] design a multi-modal
    attention-based fusion strategy, including a feature enhancement block and a feature
    recalibration block, with the former paying more attention to meaningful features
    extracted from depth maps and color images and the latter aiming at rescaling
    multi-modal features. This strategy can effectively avoid the texture-copying
    effect in the final prediction. [[196](#bib.bib196)] deploy an enhanced spatial
    attention block to transfer meaningful structural information from RGB to depth.
    [[186](#bib.bib186)] develop a recurrent structure attention block, where the
    latest depth estimate and its corresponding HR color image are taken as the input,
    to obtain useful HF features of the color image. [[160](#bib.bib160)] propose
    a multi-perspective cross-guided fusion filter block, to improve depth features
    gradually by fusing the structure details in RGB images. In this block, spatial
    representations from various views are learned to capture depth salient structures
    further. Moreover, a color-depth cross-attention module is employed to achieve
    edge preservation. [[198](#bib.bib198)] propose a multi-scale guided filtering
    module to refine the depth map in a coarse-to-fine manner. To obtain stable depth
    predictions, [[147](#bib.bib147)] conduct cross-modal fusion in a multi-frame
    manner, i.e., dToF depth video super-resolution (DVSR) and histogram video super-resolution
    (HVSR), to upsample LR depth maps. [[120](#bib.bib120)] propose an Adaptive Feature
    Fusion Module (AFFM), which enables the recovery of fine details from the HR guide
    and the LR depth maps. [[195](#bib.bib195)] propose a Spherical Space feature
    Decomposition network (SSDNet), which projects encoded features onto the spherical
    space. This strategy allows for separating and aligning domain-shared and domain-private
    features, respectively.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如何有效地融合来自不同模态的信息对于获取高质量的深度图至关重要。直观地，我们可以通过简单的操作如连接 [[165](#bib.bib165), [149](#bib.bib149),
    [18](#bib.bib18)] 来执行多模态融合。然而，这些融合方案不能选择性地将高频特征从引导图转移到目标图，并且可能会导致纹理复制伪影。因此，研究人员设计了不同的融合方案来缓解这个问题，主要分为三类：早期融合、晚期融合和多级融合。
    [[77](#bib.bib77)] 提出了一个关联控制的颜色引导块（block）来融合多模态信息。 [[197](#bib.bib197)] 设计了一种基于多模态注意力的融合策略，包括一个特征增强块和一个特征重新校准块，其中前者更关注从深度图和颜色图像中提取的有意义特征，后者旨在重新调整多模态特征。该策略可以有效避免最终预测中的纹理复制效果。
    [[196](#bib.bib196)] 部署了一个增强的空间注意力块，将有意义的结构信息从RGB传输到深度图。 [[186](#bib.bib186)]
    开发了一个递归结构注意力块，将最新的深度估计和其对应的HR颜色图像作为输入，以获得颜色图像的有用高频特征。 [[160](#bib.bib160)] 提出了一个多视角交叉引导融合滤波块，通过融合RGB图像中的结构细节逐步改善深度特征。在这个块中，来自不同视角的空间表示被学习，以进一步捕捉深度显著结构。此外，采用了一个颜色-深度交叉注意力模块来实现边缘保留。
    [[198](#bib.bib198)] 提出了一个多尺度引导滤波模块，以粗到细的方式细化深度图。为了获得稳定的深度预测， [[147](#bib.bib147)]
    以多帧方式进行跨模态融合，即dToF深度视频超分辨率（DVSR）和直方图视频超分辨率（HVSR），以放大低分辨率深度图。 [[120](#bib.bib120)]
    提出了一个自适应特征融合模块（AFFM），该模块使得从HR引导图和LR深度图中恢复细节成为可能。 [[195](#bib.bib195)] 提出了一个球面空间特征分解网络（SSDNet），该网络将编码特征投影到球面空间。这一策略允许分别分离和对齐领域共享特征和领域私有特征。
- en: 3.5 Unsupervised Methods
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 无监督方法
- en: 'Since collecting training data paired with annotations is labor-intensive and
    time-consuming, researchers seek to solve the issue by leveraging self-supervised
    learning, allowing training models without labels. In [[103](#bib.bib103)], GDSR
    is regarded as a pixel-wise mapping from the source to the target, implemented
    as a multi-layer perceptron. This formulation can be trained in a fully unsupervised
    manner with the constraint of having the LR source. [[135](#bib.bib135)] present
    a single-pair method that can upsample depth maps even when the input pairs are
    misaligned. During training, patches cropped from the input pairs are utilized
    as pseudo-label data to enable weakly supervised learning. To improve generalization,
    [[25](#bib.bib25)] propose a mutual modulation super-resolution model (MMSR),
    where a cross-modality modulation strategy using adaptive filters transfers meaningful
    features from one modality to the other, as shown in Fig.[10](#S3.F10 "Figure
    10 ‣ 3.5 Unsupervised Methods ‣ 3 Guided Depth Super-Resolution ‣ RGB Guided ToF
    Imaging System: A Survey of Deep Learning-based Methods"). In this mutual modulation,
    the spatial relationship between the corresponding pixels of the two modalities
    is fully exploited. Moreover, a cycle consistency loss is adopted to enforce the
    target faithful to the source. Differently from previous unsupervised methods
    for GDSR, [[136](#bib.bib136)] introduce a pretext task into this field. Through
    the proposed scene structure guidance network, explicit structure features of
    color images can be obtained, which, together with the corresponding LR depth
    maps, serve as the input of a baseline network to produce high-resolution depth
    maps. [[159](#bib.bib159)] construct an adversarial network where the dependency
    between RGB images and depth maps is fully exploited to enhance the depth. Further,
    the optimal transport theory is introduced into the framework to boost depth enhancement
    performance. [[121](#bib.bib121)] exploit a contrast learning scheme to extract
    unique features from the guidance, which can boost the GDSR performance based
    on a baseline network.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于收集带有注释的训练数据既费力又耗时，研究人员试图通过利用自监督学习来解决这个问题，从而允许在没有标签的情况下训练模型。在 [[103](#bib.bib103)]
    中，GDSR 被视为从源图像到目标图像的逐像素映射，实现为多层感知机。这种形式可以在完全无监督的方式下进行训练，只需约束有 LR 源即可。[[135](#bib.bib135)]
    提出了一个单对方法，即使在输入对不对齐时也能对深度图进行上采样。在训练过程中，利用从输入对中裁剪出的图像块作为伪标签数据以实现弱监督学习。为了提高泛化能力，[[25](#bib.bib25)]
    提出了一个互调制超分辨率模型（MMSR），其中使用自适应滤波器的跨模态调制策略将有意义的特征从一个模态转移到另一个模态，如图 [10](#S3.F10 "图
    10 ‣ 3.5 无监督方法 ‣ 3 引导深度超分辨率 ‣ RGB 引导 ToF 成像系统：基于深度学习的方法综述") 所示。在这种互调制中，充分利用了两个模态对应像素之间的空间关系。此外，采用了循环一致性损失来确保目标与源图像一致。与之前的
    GDSR 无监督方法不同，[[136](#bib.bib136)] 在该领域引入了一种预设任务。通过提出的场景结构引导网络，可以获得彩色图像的显式结构特征，这些特征与对应的
    LR 深度图一起作为基线网络的输入，以生成高分辨率深度图。[[159](#bib.bib159)] 构建了一个对抗网络，其中充分利用了 RGB 图像与深度图之间的依赖关系来增强深度。此外，将最优传输理论引入框架中，以提升深度增强性能。[[121](#bib.bib121)]
    利用对比学习方案从引导图像中提取独特特征，这些特征可以基于基线网络提升 GDSR 性能。
- en: 'To conclude this section, Tab. [3](#S3.T3 "Table 3 ‣ 3.5 Unsupervised Methods
    ‣ 3 Guided Depth Super-Resolution ‣ RGB Guided ToF Imaging System: A Survey of
    Deep Learning-based Methods") collects any of the methods discussed so far, divided
    into five categories. For each method, the venue and year are reported, together
    with a short description of the key idea behind it.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这一部分，表 [3](#S3.T3 "表 3 ‣ 3.5 无监督方法 ‣ 3 引导深度超分辨率 ‣ RGB 引导 ToF 成像系统：基于深度学习的方法综述")
    汇总了迄今为止讨论的所有方法，并分为五类。每种方法都报告了场所和年份，以及其背后的关键思想的简要描述。
- en: '| ![Refer to caption](img/db9b5829210e76e477be63807f8903b4.png) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/db9b5829210e76e477be63807f8903b4.png) |'
- en: 'Figure 10: MMSR framework [[25](#bib.bib25)] belonging to unsupervised methods.
    The MMSR adopts the cycle consistency loss to train the network in a self-supervised
    manner.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：属于无监督方法的 MMSR 框架 [[25](#bib.bib25)]。MMSR 采用循环一致性损失以自监督的方式训练网络。
- en: '| Paradigm | Method | Reference | Key Idea |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | 方法 | 参考文献 | 关键思想 |'
- en: '| Dual-branch Framework | DJFR [[79](#bib.bib79)] | TPAMI-2019 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 双分支框架 | DJFR [[79](#bib.bib79)] | TPAMI-2019 |'
- en: '&#124; Introduce two sub-networks for extracting features from the target and
    guidance images. Following the &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 引入两个子网络用于从目标图像和引导图像中提取特征。按照
- en: '&#124; concatenation operation, the fused information is fed into another sub-network
    for the predicted depth. &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 连接操作，将融合的信息送入另一个子网络以预测深度。&#124;'
- en: '|'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DMSG [[58](#bib.bib58)] | ECCV-2016 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| DMSG [[58](#bib.bib58)] | ECCV-2016 |'
- en: '&#124; First work to super-solve depth maps using multi-scale guidance, which
    progressively enhances depth details from &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 首次使用多尺度引导超分辨深度图，逐步增强深度细节&#124;'
- en: '&#124; high- to low-level. &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从高到低级。&#124;'
- en: '|'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DepthSR [[44](#bib.bib44)] | TIP-2019 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| DepthSR [[44](#bib.bib44)] | TIP-2019 |'
- en: '&#124; Exploit a residual U-Net structure, where hierarchical guidance is used
    at each scale, to recover HR depth maps. &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用残差U-Net结构，其中在每个尺度上使用层次引导，以恢复HR深度图。&#124;'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MGD [[209](#bib.bib209)] | TCSVT-2019 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| MGD [[209](#bib.bib209)] | TCSVT-2019 |'
- en: '&#124; Propose a multi-scale architecture based on dense layers that aim to
    revisit the features from all higher levels at a &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一个基于密集层的多尺度架构，旨在重新访问所有更高层的特征&#124;'
- en: '&#124; given scale. &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 给定尺度。&#124;'
- en: '|'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MFR-SR [[211](#bib.bib211)] | TCSVT-2019 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| MFR-SR [[211](#bib.bib211)] | TCSVT-2019 |'
- en: '&#124; Combine global and local residual learning to upsamples depth maps from
    coarse to fine via multi-scale frequency &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结合全局和局部残差学习，通过多尺度频率将深度图从粗到细地上采样&#124;'
- en: '&#124; synthesis. &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 合成。&#124;'
- en: '|'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MIG-net [[210](#bib.bib210)] | TMM-2021 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| MIG-net [[210](#bib.bib210)] | TMM-2021 |'
- en: '&#124; Introduce a two-stream multi-scale network to extract features in the
    image and gradient domains, where the depth &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引入一个双流多尺度网络来提取图像和梯度域中的特征，其中深度&#124;'
- en: '&#124; features and gradient features are alternatively complemented with each
    other. &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征和梯度特征交替互补。&#124;'
- en: '|'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SFG [[187](#bib.bib187)] | AAAI-2023 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| SFG [[187](#bib.bib187)] | AAAI-2023 |'
- en: '&#124; Propose a structure flow-guided framework for GDSR, whose key point
    is to learn a structure flow map to guide &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一个结构流引导框架用于GDSR，其关键点是学习一个结构流图来引导&#124;'
- en: '&#124; structure representation transferring. &#124;'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结构表示转换。&#124;'
- en: '|'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CCFN [[167](#bib.bib167)] | TIP-2019 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| CCFN [[167](#bib.bib167)] | TIP-2019 |'
- en: '&#124; Propose a coarse-to-fine neural network with two stages. In the first
    stage, the model learns large filter kernels to &#124;'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一个具有两个阶段的粗到细神经网络。在第一个阶段，模型学习大滤波器内核以&#124;'
- en: '&#124; obtain coarse results, which are further refined by smaller filter kernels
    in the second stage. &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 获得粗略结果，在第二阶段通过更小的滤波器内核进一步细化。&#124;'
- en: '|'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CGN [[208](#bib.bib208)] | TMM-2020 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| CGN [[208](#bib.bib208)] | TMM-2020 |'
- en: '&#124; With the proposed depth-guided affine transformation, the intensity-guided
    depth feature filtering and refinement &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过提出的深度引导仿射变换，强度引导的深度特征过滤和细化&#124;'
- en: '&#124; are carried out iteratively in multiple stages. &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以多阶段迭代进行。&#124;'
- en: '|'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| PMBANet [[183](#bib.bib183)] | TIP-2020 | Introduce a multi-branch aggregation
    network with multiple stages to reconstruct sharp depth boundaries. |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| PMBANet [[183](#bib.bib183)] | TIP-2020 | 引入一个多分支聚合网络，具有多个阶段，以重建清晰的深度边界。
    |'
- en: '| IRLF [[141](#bib.bib141)] | CVPR-2020 | Develop a coarse-to-fine framework
    consisting of several sub-modules with channel attention strategies. |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| IRLF [[141](#bib.bib141)] | CVPR-2020 | 开发一个粗到细的框架，由几个子模块组成，具有通道注意策略。 |'
- en: '| DKN [[68](#bib.bib68)] | IJCV-2021 | Design a deformable kernel network that
    obtains a kernel for each pixel based on the neighborhood weights. |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| DKN [[68](#bib.bib68)] | IJCV-2021 | 设计一个可变形核网络，为每个像素根据邻域权重获取一个核。 |'
- en: '| FDSR [[50](#bib.bib50)] | CVPR-2021 | Explore high-frequency (HF) information
    via an HF guidance branch, where the HF information is fused with depth features
    to improve the performance. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| FDSR [[50](#bib.bib50)] | CVPR-2021 | 通过HF引导分支探索高频（HF）信息，其中HF信息与深度特征融合以提高性能。
    |'
- en: '| GeoDSR [[161](#bib.bib161)] | AAAI-2023 | Introduce a continuous depth representation
    to effectively implement scale-continuous and spatial-continuous upsampling in
    guided depth super-resolution. |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| GeoDSR [[161](#bib.bib161)] | AAAI-2023 | 引入连续深度表示，以有效实施引导深度超分辨率中的尺度连续和空间连续上采样。
    |'
- en: '| DAGF [[198](#bib.bib198)] | TNNLS-2023 | Design a kernel generation network
    that can cope with inconsistent structures between RGB and depth, where a multi-scale
    guided filtering module is proposed to refine the depth map in a coarse-to-fine
    manner. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| DAGF [[198](#bib.bib198)] | TNNLS-2023 | 设计一个核生成网络，能够处理RGB和深度之间的不一致结构，其中提出了一个多尺度引导滤波模块，用于粗到细地细化深度图。
    |'
- en: '| DCSR [[164](#bib.bib164)] | Display-2023 | Proposes a depth map continuous
    SR framework that can achieve resolution adaptation at arbitrary SR ratios. |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| DCSR [[164](#bib.bib164)] | Display-2023 | 提出了一个深度图连续超分辨率框架，可以在任意超分辨率比下实现分辨率适应。
    |'
- en: '| PAC [[143](#bib.bib143)] | CVPR-2019 | Present a pixel-adaptive convolution
    operation for deep joint image upsampling. |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| PAC [[143](#bib.bib143)] | CVPR-2019 | 提出了一个像素自适应卷积操作，用于深度联合图像上采样。 |'
- en: '| DELTAR [[80](#bib.bib80)] | ECCV-2022 | Propose a transformer-based architecture
    with two branches, which can extract and fuse depth and RGB features efficiently.
    Besides, a cross-modal calibration is performed to align RGB and depth. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| DELTAR [[80](#bib.bib80)] | ECCV-2022 | 提出了一个基于变换器的架构，具有两个分支，可以高效地提取和融合深度和
    RGB 特征。此外，执行了跨模态校准以对齐 RGB 和深度。 |'
- en: '| Hybrid Model | DPDN [[126](#bib.bib126)] | BMVC-2016 | Use two sub-networks.
    one based on a fully-convolutional structure produces HR depth maps and weights.
    Another uses a non-local variational method to get final results. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Hybrid Model | DPDN [[126](#bib.bib126)] | BMVC-2016 | 使用两个子网络，其中一个基于全卷积结构生成高分辨率深度图和权重，另一个使用非局部变分方法获得最终结果。
    |'
- en: '| DG-CMM [[40](#bib.bib40)] | TPAMI-2019 | Propose a weighted analysis sparse
    representation (WASR) model and a neural network to learn filter parameterizations
    and non-linear functions to build flexible stage-wise operations. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| DG-CMM [[40](#bib.bib40)] | TPAMI-2019 | 提出了一个加权分析稀疏表示（WASR）模型和一个神经网络，以学习滤波器参数化和非线性函数来构建灵活的分阶段操作。
    |'
- en: '| CUNet [[21](#bib.bib21)] | TPAMI-2020 | Develop a multi-modal convolutional
    sparse coding (MCSC) models to solve the general multi-modal image restoration
    and multi-modal image fusion problems. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| CUNet [[21](#bib.bib21)] | TPAMI-2020 | 开发了一个多模态卷积稀疏编码（MCSC）模型，用于解决一般的多模态图像恢复和多模态图像融合问题。
    |'
- en: '| LMCSC [[108](#bib.bib108)] | TIP-2020 | Propose a deep unfolding network
    using guided image SR that exploits information from two modalities. |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| LMCSC [[108](#bib.bib108)] | TIP-2020 | 提出了一个深度展开网络，该网络使用引导图像超分辨率，利用两种模态的信息。
    |'
- en: '| LGR [[18](#bib.bib18)] | CVPR-2022 | Propose a novel architecture combining
    a neural network and graph-based optimization to perform GDSR. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| LGR [[18](#bib.bib18)] | CVPR-2022 | 提出了一个新颖的架构，结合神经网络和基于图的优化方法来执行 GDSR。
    |'
- en: '| DCTNet [[196](#bib.bib196)] | CVPR-2022 | Utilizes discrete cosine transform
    (DCT) to tackle the optimization issue for GDSR by reconstructing the multi-channel
    HR depth features. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| DCTNet [[196](#bib.bib196)] | CVPR-2022 | 利用离散余弦变换（DCT）通过重建多通道高分辨率深度特征来解决
    GDSR 的优化问题。'
- en: '| DaDa [[110](#bib.bib110)] | CVPR-2023 | Suggest incorporating guided anisotropic
    diffusion into a CNN to improve depth discontinuities in depth images. Given a
    scale factor, the inference time and memory demands of the method are fixed. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| DaDa [[110](#bib.bib110)] | CVPR-2023 | 建议将引导各向异性扩散整合到卷积神经网络中，以改善深度图像中的深度不连续性。给定一个比例因子，方法的推理时间和内存需求是固定的。
    |'
- en: '| MADUNet [[200](#bib.bib200)] | IJCV-2023 | Propose a memory-augmented deep
    unfolding network where the introduction of local and global implicit priors is
    based on a maximal a posterior view. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| MADUNet [[200](#bib.bib200)] | IJCV-2023 | 提出了一个记忆增强的深度展开网络，其中局部和全局隐式先验的引入基于最大后验视角。
    |'
- en: '| Auxiliary-based | CTKT [[145](#bib.bib145)] | CVPR-2021 | Propose a knowledge
    distillation method that uses the depth estimate auxiliary task during training
    for better results. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Auxiliary-based | CTKT [[145](#bib.bib145)] | CVPR-2021 | 提出了一个知识蒸馏方法，在训练过程中使用深度估计辅助任务以获得更好的结果。
    |'
- en: '| BridgeNet [[151](#bib.bib151)] | ACM MM-2021 | Explore a paradigm where the
    association between monocular depth estimate (MDE) and depth super-resolution
    are used to advance the performance of depth map super-resolution. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| BridgeNet [[151](#bib.bib151)] | ACM MM-2021 | 探索了一种范式，其中单目深度估计（MDE）与深度超分辨率之间的关联被用来提高深度图超分辨率的性能。
    |'
- en: '| DSR-N [[165](#bib.bib165)] | PR-2020 | Propose a depth upsampling framework
    that leverages edge information as the guide to recover HR depth maps. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| DSR-N [[165](#bib.bib165)] | PR-2020 | 提出了一个深度上采样框架，利用边缘信息作为指导来恢复高分辨率深度图。
    |'
- en: '| PDSR [[155](#bib.bib155)] | ICCV-2019 | Attempt to measure the upsampled
    depth quality using renderings of surface normal maps. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| PDSR [[155](#bib.bib155)] | ICCV-2019 | 尝试通过表面法线图的渲染来测量上采样深度的质量。 |'
- en: '| PDR-Net [[93](#bib.bib93)] | Neurocomputing-2022 | Proposes a progressive
    depth reconstruction network to further enhance the performance of DMSR. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| PDR-Net [[93](#bib.bib93)] | Neurocomputing-2022 | 提出了一个渐进深度重建网络，以进一步提高 DMSR
    的性能。 |'
- en: '| SVLRM [[24](#bib.bib24)] | TPAMI-2021 | Propose a new joint filtering method
    based on a spatially variant linear representation model where the target image
    is linearly represented by the guidance image. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| SVLRM [[24](#bib.bib24)] | TPAMI-2021 | 提出一种基于空间变换线性表示模型的新型联合滤波方法，其中目标图像由引导图像线性表示。
    |'
- en: '| Fusion-focused | JIIF [[149](#bib.bib149)] | MM-2021 | Considers GDSR to
    be a neural implicit interpolation problem that can convert latent codes from
    continuous coordinates in LR depth and HR color images. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 侧重融合 | JIIF [[149](#bib.bib149)] | MM-2021 | 将GDSR视为一种神经隐式插值问题，可以将LR深度图和HR彩色图像中的潜在编码转换。
    |'
- en: '| MSSN [[77](#bib.bib77)] | PR-2020 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| MSSN [[77](#bib.bib77)] | PR-2020 |'
- en: '&#124; Present a multi-scale strategy with the proposed symmetric unit as the
    basic component of the network, which can &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一种多尺度策略，将对称单元作为网络的基本组件，可以 &#124;'
- en: '&#124; effectively deal with textureless and edge features in depth images.
    &#124;'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 有效处理深度图像中的无纹理和边缘特征。 &#124;'
- en: '|'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| AHMF [[197](#bib.bib197)] | TIP-2021 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| AHMF [[197](#bib.bib197)] | TIP-2021 |'
- en: '&#124; Propose a hierarchical network based on a bi-directional hierarchical
    feature collaboration module, where low- and &#124;'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一种基于双向层次特征协作模块的层次网络，其中低级和 &#124;'
- en: '&#124; high-level spatial information can work together to improve each other.
    &#124;'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高级空间信息可以相互协作，以提高彼此的效果。 &#124;'
- en: '|'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| RSAG [[186](#bib.bib186)] | AAAI-2023 | Propose a recurrent structure attention
    guidance framework that employs both multi-scale and multi-stage strategies. A
    deep contrastive network with multi-scale filters is used to separate HF and LF
    features. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| RSAG [[186](#bib.bib186)] | AAAI-2023 | 提出一个递归结构注意力引导框架，采用多尺度和多阶段策略。使用具有多尺度滤波器的深度对比网络来分离HF和LF特征。
    |'
- en: '| JGF [[160](#bib.bib160)] | PR-2023 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| JGF [[160](#bib.bib160)] | PR-2023 |'
- en: '&#124; Present a novel method that uses pyramid structure to capture multi-scale
    features, allowing HR depth maps to be &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一种新方法，利用金字塔结构捕捉多尺度特征，使HR深度图像能够 &#124;'
- en: '&#124; recovered progressively. &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 逐步恢复。 &#124;'
- en: '|'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DVSR [[147](#bib.bib147)] | CVPR-2023 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| DVSR [[147](#bib.bib147)] | CVPR-2023 |'
- en: '&#124; Propose a multi-frame scheme to upsample LR dToF videos with corresponding
    HR RGB sequences. &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一种多帧方案，通过相应的HR RGB序列对LR dToF视频进行上采样。 &#124;'
- en: '|'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SSDNet [[195](#bib.bib195)] | ICCV-2023 | Propose a spherical contrast refinement
    module to enhance depth details from RGB features. |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| SSDNet [[195](#bib.bib195)] | ICCV-2023 | 提出一种球形对比度增强模块，以增强RGB特征中的深度细节。 |'
- en: '| DSR-EI [[120](#bib.bib120)] | CVIU-2023 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| DSR-EI [[120](#bib.bib120)] | CVIU-2023 |'
- en: '&#124; Propose AFFM to adaptively fuse discriminative cross-modality features.
    &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出AFFM以自适应融合具有区分性的跨模态特征。 &#124;'
- en: '|'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Unsupervised | P2P [[103](#bib.bib103)] | ICCV-2019 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 无监督 | P2P [[103](#bib.bib103)] | ICCV-2019 |'
- en: '&#124; GDSR is regarded as a pixel-wise mapping from source to target and implemented
    as a multi-layer perceptron. &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GDSR被视为从源到目标的逐像素映射，并实现为多层感知器。 &#124;'
- en: '|'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CMSR [[135](#bib.bib135)] | CVPR-2021 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| CMSR [[135](#bib.bib135)] | CVPR-2021 |'
- en: '&#124; Present a single-pair method that can upsample depth maps even when
    the input pairs are misaligned, where &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一种单对方法，即使在输入对未对齐的情况下也能上采样深度图，其中 &#124;'
- en: '&#124; patches cropped from the input pairs are utilized as pseudo-label data
    to enable weakly supervised learning. &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从输入对中裁剪的补丁作为伪标签数据，以实现弱监督学习。 &#124;'
- en: '|'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MMSR [[25](#bib.bib25)] | ECCV-2022 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| MMSR [[25](#bib.bib25)] | ECCV-2022 |'
- en: '&#124; A cross-modality modulation strategy using adaptive filters is developed
    to transfer significant features from one &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开发了一种使用自适应滤波器的跨模态调制策略，以从一个模态转移重要特征到另一个模态。 &#124;'
- en: '&#124; modality to the other in a proposed mutual modulation super-resolution
    model. &#124;'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模态到另一模态的互调超分辨率模型。 &#124;'
- en: '|'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DEDE [[159](#bib.bib159)] | TIP-2023 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| DEDE [[159](#bib.bib159)] | TIP-2023 |'
- en: '&#124; Design an adversarial network where the relationship between depth maps
    and RGB images are fully exploited &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设计一个对抗网络，充分利用深度图与RGB图像之间的关系 &#124;'
- en: '&#124; to improve the depth. &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提升深度。 &#124;'
- en: '|'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SSGnet [[136](#bib.bib136)] | AAAI-2023 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| SSGnet [[136](#bib.bib136)] | AAAI-2023 |'
- en: '&#124; Explicit structure features of color images can be extracted using the
    suggested scene structure guidance network, &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可以使用建议的场景结构引导网络提取彩色图像的显式结构特征， &#124;'
- en: '&#124; and these features—along with the corresponding LR depth maps—serve
    as the input of a base network. &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这些特征——以及相应的 LR 深度图——作为基础网络的输入。&#124;'
- en: '|'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CMPNet [[121](#bib.bib121)] | NN-2023 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| CMPNet [[121](#bib.bib121)] | NN-2023 |'
- en: '&#124; Develop an autoencoder-based framework with contrastive and reconstruction
    losses &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开发基于自编码器的框架，结合对比损失和重建损失 &#124;'
- en: '&#124; to reduce information redundancy. &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 减少信息冗余。&#124;'
- en: '|'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 3: List of Guided Depth Super-Resolution methods, divided according to
    the five outlined categories.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：引导深度超分辨率方法的列表，按照五个概述的类别进行划分。
- en: 4 Guided Depth Completion
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 引导深度完成
- en: 'Pivotal works on depth completion [[153](#bib.bib153), [13](#bib.bib13)] only
    take depth maps as input. However, due to the sparsity of depth maps obtained
    from long-range LiDAR measurements, a substantial amount of details, such as object
    boundaries, are lost. Thus, restoring these details is difficult on densified
    depth maps, even using deep learning. Therefore, researchers started exploiting
    additional modalities capturing the same scene to guide the completion of sparse
    depth maps. In practice, color images can provide rich texture, scene structure,
    and object details, thus allowing for the extraction of sufficient cues to enhance
    depth completion. Besides, color images can also offer various auxiliary cues,
    such as semantics, monocular depth, and surface normal. Similar to GDSR, we divide
    GDC into two categories: supervised and unsupervised methods. Supervised methods
    can be further classified into dual-branch architectures, spatial propagation
    networks, auxiliary-based methods, and uncertainty guidance. These six categories
    are listed in the bottom branch of Fig. [5](#S2.F5 "Figure 5 ‣ 2.4 Taxonomy ‣
    2 Preliminaries and Taxonomy ‣ RGB Guided ToF Imaging System: A Survey of Deep
    Learning-based Methods").'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '关键的深度完成工作[[153](#bib.bib153), [13](#bib.bib13)] 仅以深度图作为输入。然而，由于从长距离 LiDAR 测量中获得的深度图的稀疏性，许多细节，如物体边界，会丢失。因此，即使使用深度学习，恢复这些细节在稠密深度图上也很困难。因此，研究人员开始利用捕捉同一场景的额外模态来指导稀疏深度图的完成。在实际操作中，彩色图像可以提供丰富的纹理、场景结构和物体细节，从而提取足够的线索以增强深度完成。此外，彩色图像还可以提供各种辅助线索，如语义、单目深度和表面法线。类似于
    GDSR，我们将 GDC 分为两类：监督方法和无监督方法。监督方法可以进一步分类为双分支架构、空间传播网络、基于辅助的 方法和不确定性引导。这六类在图 [5](#S2.F5
    "Figure 5 ‣ 2.4 Taxonomy ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided ToF Imaging
    System: A Survey of Deep Learning-based Methods") 的底部分支中列出。'
- en: Most existing approaches employ supervised learning for this task, which typically
    constructs image pairs from dense maps and their sparse version for training.
    With dense depth information being used as annotation, deep neural networks are
    trained to solve a regression problem, as done for GDSR.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现有方法使用监督学习来完成这项任务，这通常从密集图和其稀疏版本中构建图像对进行训练。利用密集深度信息作为注释，训练深度神经网络来解决回归问题，就像
    GDSR 所做的那样。
- en: 4.1 Dual-branch Architecture
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 双分支架构
- en: 'The dual-branch architecture is common in GDC. It utilizes two encoders to
    extract features from RGB and sparse depth maps, and then the cross-modal information
    is sent to a single decoder after aggregation. In this latter, a designed information
    fusion strategy can also be incorporated. Thus, two folds must be focused on:
    effective representation learning and information fusion.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 双分支架构在 GDC 中很常见。它利用两个编码器从 RGB 和稀疏深度图中提取特征，然后将交叉模态信息在聚合后发送到单个解码器。在这个解码器中，还可以加入设计的信息融合策略。因此，必须重点关注两个方面：有效的表示学习和信息融合。
- en: '[[60](#bib.bib60)] employ a dual-branch architecture based on a modified version
    of the neural architecture search network (NASNet)[[206](#bib.bib206)]. First,
    the features from RGB images and depth maps are extracted using two NASNet encoders.
    Then, the intermediate features are fused in a late fusion scheme through channel-wise
    concatenation and fed into a decoder, yielding the final dense estimate. In this
    framework, validity masks are demonstrated to fail further to improve performance
    for GDC in large neural networks. To address the issue of model performance degradation
    caused by blurry guidance in the color images and unclear structures in the depth
    maps, [[180](#bib.bib180)] exploit a repetitive design based on the hourglass
    network[[150](#bib.bib150)]. To achieve multi-scale training, [[74](#bib.bib74)]
    introduce the cascaded architecture into the hourglass network, extended by [[30](#bib.bib30)]
    with denser interaction between the two modalities.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[[60](#bib.bib60)]采用了基于修改版神经架构搜索网络（NASNet）[[206](#bib.bib206)]的双分支架构。首先，使用两个NASNet编码器从RGB图像和深度图中提取特征。然后，通过通道级拼接的晚期融合方案将中间特征融合，并输入解码器，得到最终的密集估计。在该框架中，验证掩膜被证明无法进一步提升GDC在大型神经网络中的性能。为了解决由于彩色图像中的模糊引导和深度图中的不清晰结构导致的模型性能下降问题，[[180](#bib.bib180)]利用基于沙漏网络[[150](#bib.bib150)]的重复设计。为实现多尺度训练，[[74](#bib.bib74)]将级联架构引入沙漏网络，并由[[30](#bib.bib30)]扩展，使两种模态之间的交互更加密集。'
- en: 'In the two-branch framework proposed by [[154](#bib.bib154)], global and local
    information are extracted with two encoders based on ERFNet [[128](#bib.bib128)],
    respectively. Notably, the local branch uses two U-nets to reconstruct sharp edges
    and structural details. Finally, the predictions are weighted by the outputs of
    each branch. [[56](#bib.bib56)] propose a network that can perform precise and
    efficient depth completion (PENet), as depicted in Fig.[11](#S4.F11 "Figure 11
    ‣ 4.1 Dual-branch Architecture ‣ 4 Guided Depth Completion ‣ RGB Guided ToF Imaging
    System: A Survey of Deep Learning-based Methods"). It gradually enhances the accuracy
    of depth prediction in a coarse-to-fine manner. The first branch, named the color-dominant
    branch, takes a color image and a sparse depth map as input to produce a dense
    depth map that inherits sharp edges and structural details in the color image.
    The second branch, i.e., the depth dominant branch, employs a similar network,
    whose inputs are the previous dense and sparse depth map, to output another dense
    map. Finally, the two depth maps are fused using the strategy proposed by FusionNet [[154](#bib.bib154)].
    Notably, they concatenate the 3D position maps to the convolutional layer, forming
    a geometric convolutional layer, to lift the performance further. For indoor depth
    completion, [[63](#bib.bib63)] use a dual-branch structure to achieve an efficient
    solution with low power consumption.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[154](#bib.bib154)]提出的双分支框架中，通过基于ERFNet[[128](#bib.bib128)]的两个编码器提取全局和局部信息。值得注意的是，局部分支使用两个U-net来重建清晰的边缘和结构细节。最后，通过每个分支的输出对预测进行加权。[[56](#bib.bib56)]提出了一种可以进行精确和高效深度完成的网络（PENet），如图[11](#S4.F11
    "Figure 11 ‣ 4.1 Dual-branch Architecture ‣ 4 Guided Depth Completion ‣ RGB Guided
    ToF Imaging System: A Survey of Deep Learning-based Methods")所示。该网络以粗到细的方式逐渐提高深度预测的准确性。第一个分支，即色彩主导分支，接收彩色图像和稀疏深度图作为输入，生成继承了彩色图像中清晰边缘和结构细节的密集深度图。第二个分支，即深度主导分支，采用类似的网络，其输入为先前的密集和稀疏深度图，以输出另一张密集图。最后，使用FusionNet[[154](#bib.bib154)]提出的策略融合这两张深度图。特别地，他们将3D位置图拼接到卷积层上，形成几何卷积层，从而进一步提升性能。对于室内深度完成，[[63](#bib.bib63)]使用双分支结构实现低功耗的高效解决方案。'
- en: '| ![Refer to caption](img/5e1dd05cb0cfa07b316768aea00333dd.png) |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/5e1dd05cb0cfa07b316768aea00333dd.png) |'
- en: 'Figure 11: PENet [[56](#bib.bib56)] framework belonging to dual-branch architectures.
    It consists of two branches that can respectively produce depth estimates from
    color-dominant and depth-dominant information.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：PENet[[56](#bib.bib56)]框架属于双分支架构。它由两个分支组成，分别可以从色彩主导和深度主导信息中生成深度估计。
- en: Using a dense pseudo-depth map as a coarse reference for the final prediction,
    [[39](#bib.bib39)] can output stable and accurate dense depth maps. Different
    from the vanilla point-estimate methods with a dual-branch architecture, [[182](#bib.bib182)]
    develop a conditional prior network (CPN) to calculate a posterior probability
    over the depth of each pixel, combined with a likelihood term using sparse measurements.
    The standard convolution operation applies a kernel to the pixel grid in an image,
    which, however, cannot handle sparse depth maps with uneven distribution of depth
    values. To address this issue, [[194](#bib.bib194)] propose to leverage graph
    propagation to extract spatial contexts. Considering the variability of the graph
    structure, it applies the co-attention module[[100](#bib.bib100)] on the guided
    graph propagation for efficient multi-modal representation extraction. Combining
    traditional methods with a dual-branch deep network, [[175](#bib.bib175)] achieve
    promising results. In the most recent works, [[8](#bib.bib8)] employ two parallel
    Unet-shape networks to extract RGB and depth features at different scales. Then,
    feature fusion is performed in the decoding stage through the proposed attention-guided
    skip connection (AG-SC) module, yielding the desired results. In contrast, [[185](#bib.bib185)]
    propose a network named PointDC that leverages 2D and 3D information to conduct
    robust depth completion.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用密集伪深度图作为最终预测的粗略参考，[[39](#bib.bib39)]能够输出稳定且准确的密集深度图。与传统的双分支结构点估计方法不同，[[182](#bib.bib182)]开发了一种条件先验网络（CPN），用于计算每个像素深度的后验概率，并结合使用稀疏测量的似然项。标准卷积操作将内核应用于图像中的像素网格，但无法处理深度值分布不均的稀疏深度图。为了解决这个问题，[[194](#bib.bib194)]提出利用图传播来提取空间上下文。考虑到图结构的可变性，它在引导图传播中应用了共同注意力模块[[100](#bib.bib100)]，以实现高效的多模态表示提取。将传统方法与双分支深度网络相结合，[[175](#bib.bib175)]取得了令人满意的结果。在最近的研究中，[[8](#bib.bib8)]采用两个平行的Unet形状网络，以不同尺度提取RGB和深度特征。然后，通过提出的注意力引导跳跃连接（AG-SC）模块在解码阶段进行特征融合，得到了理想的结果。相比之下，[[185](#bib.bib185)]提出了一种名为PointDC的网络，该网络利用2D和3D信息进行稳健的深度补全。
- en: Guided image filtering. In GDC, deep-guided image filtering can be viewed as
    a type of dual-branch architecture that utilizes features from RGB images to predict
    a kernel. This kernel is often learned by a deep network and applied to depth
    for feature fusion or dense depth prediction.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 引导图像滤波。在GDC中，深度引导图像滤波可以视为一种双分支结构，利用RGB图像中的特征来预测一个内核。这个内核通常由深度网络学习并应用于深度图中，以进行特征融合或密集深度预测。
- en: Inspired by classical guided image filtering [[152](#bib.bib152), [49](#bib.bib49)],
    [[150](#bib.bib150)] propose a guided network consisting of a GuideNet and a DepthNet
    to recover dense depth maps. Inside the GuideNet, content-dependent and spatial-variant
    convolution kernels are predicted to capture geometric structural features in
    color images consistent with real-world scenes. Furthermore, a convolution factorization
    is introduced into the method to reduce computational overhead.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 受到经典引导图像滤波的启发[[152](#bib.bib152), [49](#bib.bib49)]，[[150](#bib.bib150)]提出了一种引导网络，包括GuideNet和DepthNet，用于恢复密集深度图。在GuideNet中，预测了内容相关和空间变异的卷积核，以捕捉颜色图像中与真实世界场景一致的几何结构特征。此外，引入了卷积因子分解，以减少计算开销。
- en: '[[89](#bib.bib89)] present a two-stage model where depth interpolation and
    refinement are performed sequentially. Specifically, they first interpolate the
    sparse depth map via the proposed differentiable kernel regression layer, proving
    more effective than hand-designed filters. Then, a residual network based on U-net
    is adopted to refine the coarse depth map.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[[89](#bib.bib89)]提出了一种两阶段模型，其中深度插值和细化按顺序进行。具体而言，他们首先通过提出的可微内核回归层插值稀疏深度图，比手工设计的滤波器更有效。然后，采用基于U-net的残差网络来细化粗略的深度图。'
- en: 4.2 Affinity-based Methods
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于亲和力的方法
- en: 'In the form of a generic matrix, the affinity matrix expresses the pairwise
    proximity information between a set of data points. Since the spatial propagation
    network (SPN) for learning the affinity matrix was first employed for depth completion [[95](#bib.bib95)],
    many significant efforts have been made to improve it, yielding impressive results.
    In this pioneering work, the affinity matrix, which can establish associations
    between any two pixels over a whole image, is composed of the spatial transformation
    matrix. Given a hidden representation $H^{s}$ at iteration $t$, the spatial propagation
    with affinity matrix $w$ can be expressed as:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 以通用矩阵的形式，亲和矩阵表示数据点集之间的成对接近信息。由于用于学习亲和矩阵的空间传播网络（SPN）首次被用于深度补全 [[95](#bib.bib95)]，因此做出了许多重要努力以改进它，并取得了令人印象深刻的成果。在这项开创性的工作中，亲和矩阵可以在整个图像中建立任意两个像素之间的关联，其由空间变换矩阵组成。给定在迭代$t$的隐藏表示
    $H^{s}$，利用亲和矩阵 $w$ 进行空间传播可以表示为：
- en: '|  | $\mathbf{H}_{p,q}^{t+1}=(I-d_{t})\mathbf{H}_{p,q}^{t}+\sum_{p,q\in N_{p,q}}w_{p,q}^{i,j}\mathbf{H}_{i,j}^{t}$
    |  | (24) |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}_{p,q}^{t+1}=(I-d_{t})\mathbf{H}_{p,q}^{t}+\sum_{p,q\in N_{p,q}}w_{p,q}^{i,j}\mathbf{H}_{i,j}^{t}$
    |  | (24) |'
- en: where $(p,q)$ and $(i,j)$ are the positions of reference and neighbor pixels,
    respectively, $N_{p,q}$ denote the neighborhood of $(p,q)$, and $d_{t}=1-\sum_{i,j\in
    N_{p,q}}w_{p,q}^{i,j}$.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(p,q)$ 和 $(i,j)$ 分别是参考像素和邻域像素的位置，$N_{p,q}$ 表示 $(p,q)$ 的邻域，而 $d_{t}=1-\sum_{i,j\in
    N_{p,q}}w_{p,q}^{i,j}$。
- en: '| ![Refer to caption](img/f86bf6dd18c9b6e5891ba34381be0bb3.png) |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/f86bf6dd18c9b6e5891ba34381be0bb3.png) |'
- en: 'Figure 12: CSPN framework [[11](#bib.bib11)] belonging to affinity-based methods.
    At each step, CSPN propagates a local region in all directions at the same time.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '图12: 属于亲和方法的CSPN框架 [[11](#bib.bib11)]。在每一步，CSPN同时在所有方向上传播局部区域。'
- en: 'Since spatial propagation is only performed along a certain direction at a
    time, the time cost of this method, which is run in sequence, is high. In contrast,
    [[11](#bib.bib11)] formulate the task as anisotropic diffusion filtering[[166](#bib.bib166),
    [94](#bib.bib94)], and implement it through a convolutional network named CSPN,
    depicted in Fig. [12](#S4.F12 "Figure 12 ‣ 4.2 Affinity-based Methods ‣ 4 Guided
    Depth Completion ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"). In detail, they first employ an Unet-based network that inputs an RGB
    image and a sparse depth map, and outputs a blurred depth map and an affinity
    matrix. Then, the propagation is carried out with recurrent convolutional operation.
    However, both approaches fix the receptive field in the propagation, which may
    yield suboptimal results for the task. To solve the problem, some methods utilizing
    flexible propagation schemes, such as CSPN++ [[10](#bib.bib10)], non-local SPN
    (NLSPN) [[113](#bib.bib113)], deformable SPN (DSPN) [[178](#bib.bib178)] and dynamic
    SPN (DySPN)[[84](#bib.bib84)], are proposed.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '由于空间传播一次只在某一方向上进行，因此这种按序列运行的方法的时间成本较高。相比之下，[[11](#bib.bib11)] 将任务表述为各向异性扩散滤波[[166](#bib.bib166),
    [94](#bib.bib94)]，并通过一个名为CSPN的卷积网络实现，如图[12](#S4.F12 "Figure 12 ‣ 4.2 Affinity-based
    Methods ‣ 4 Guided Depth Completion ‣ RGB Guided ToF Imaging System: A Survey
    of Deep Learning-based Methods")所示。具体来说，他们首先使用一个基于Unet的网络，该网络输入RGB图像和稀疏深度图，并输出模糊深度图和亲和矩阵。然后，使用递归卷积操作进行传播。然而，这两种方法都在传播中固定了接收场，这可能导致任务的结果不尽如人意。为了解决这个问题，一些采用灵活传播方案的方法被提出，例如CSPN++
    [[10](#bib.bib10)]，非局部SPN (NLSPN) [[113](#bib.bib113)]，可变形SPN (DSPN) [[178](#bib.bib178)]
    和动态SPN (DySPN) [[84](#bib.bib84)]。'
- en: To further improve the model performance for depth completion, [[10](#bib.bib10)]
    propose CSPN++, which includes two variants, i.e., context-aware CSPN (CA-CSPN)
    and resource-aware CSPN (RA-CSPN). By converting two constants in CSPN – the number
    of iterations and kernel size – into variables, CA-CSPN can adaptively assemble
    results from different steps in the propagation. However, in practice, the computational
    complexity is too high to be deployed in real applications. To speed up the algorithm,
    RA-CSPN picks the optimal kernel size and the number of iterations for each pixel
    depending on the learned hyper-parameters. With the improvements, the method can
    dynamically allocate the context and computing resources required by each pixel.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高深度补全模型的性能，[[10](#bib.bib10)] 提出了CSPN++，其中包括两个变体，即上下文感知CSPN（CA-CSPN）和资源感知CSPN（RA-CSPN）。通过将CSPN中的两个常数——迭代次数和卷积核大小——转化为变量，CA-CSPN可以自适应地组合传播中的不同步骤的结果。然而，实际上，计算复杂度过高，无法在实际应用中部署。为了加快算法速度，RA-CSPN根据学习到的超参数为每个像素选择最佳的卷积核大小和迭代次数。通过这些改进，该方法可以动态分配每个像素所需的上下文和计算资源。
- en: Unlike CSPN++, NLSPN [[113](#bib.bib113)] is proposed with a two-stage network
    based on non-local neighbors instead of a fixed receptive field. In the first
    stage, a U-net is employed to produce a coarse depth, a confidence map, and non-local
    neighbors prediction with their raw affinities. Then, non-local spatial propagation
    computed by deformable convolutions[[204](#bib.bib204)] is performed iteratively.
    In this manner, irrelevant features are avoided, while only relevant non-local
    neighborhood features are emphasized. Similar to NLSPN[[113](#bib.bib113)], DSPN [[178](#bib.bib178)]
    also conducts spatial propagation with deformable convolution, which can offer
    an adaptive receptive field according to the dependencies between pixels.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 与CSPN++不同，NLSPN[[113](#bib.bib113)] 提出了一个基于非局部邻域的两阶段网络，而不是固定感受野。在第一阶段，使用U-net生成粗略深度、置信度图和非局部邻域预测及其原始亲和度。然后，使用可变形卷积[[204](#bib.bib204)]进行迭代计算非局部空间传播。这样可以避免无关特征，同时突出显示相关的非局部邻域特征。与NLSPN[[113](#bib.bib113)]类似，DSPN[[178](#bib.bib178)]也使用可变形卷积进行空间传播，这可以根据像素之间的依赖关系提供自适应的感受野。
- en: Later, motivated by dynamic convolution [[9](#bib.bib9)], [[84](#bib.bib84)]
    develop a non-linear propagation model, named dynamic spatial propagation network
    (DySPN), where the affinity matrix is decoupled into parts depending on the distance.
    For neighborhoods with different distances, the affinity is assigned different
    weights to improve CSPN. Moreover, they propose a diffusion suppression operation,
    which adaptively stops iterations on specific pixels as directed by the attention
    matrix to avoid over-smoothing issues. According to different computational budgets,
    they design three variants of adaptive affinity matrix to achieve a balance between
    performance and complexity.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，受动态卷积的启发[[9](#bib.bib9)]，[[84](#bib.bib84)] 开发了一种非线性传播模型，称为动态空间传播网络（DySPN），其中亲和矩阵被解耦为根据距离划分的部分。对于不同距离的邻域，亲和度被赋予不同的权重以改善CSPN。此外，他们提出了一种扩散抑制操作，该操作根据注意力矩阵自适应地在特定像素上停止迭代，以避免过度平滑问题。根据不同的计算预算，他们设计了三种自适应亲和矩阵的变体，以在性能和复杂性之间取得平衡。
- en: '[[98](#bib.bib98)] integrate SPN and graph neural networks into one framework,
    allowing to learn both neighboring and long-range features. To adapt to practical
    applications, [[134](#bib.bib134), [17](#bib.bib17)] propose novel approaches
    that can achieve good generalization. [[191](#bib.bib191)] adopt the non-local
    spatial propagation network for improving the depth quality after obtaining an
    initial densified depth map. As for the backbone that produces an initial depth,
    they propose a novel single-branch network, which leverages both the advantages
    of CNNs and transformers. In the network, they design a joint convolutional attention
    and transformer (JCAT) block that contains two paths – the transformer layer and
    the convolutional attention layer – and achieves more efficient performance than
    pure transformer-based methods. To learn the neighborhood affinity, [[73](#bib.bib73)]
    propose a network that leverages multi-scale and local features. [[163](#bib.bib163)]
    propose a long-short range recurrent updating (LRRU) network that leverages an
    iterative scheme and employs fewer parameters to achieve state-of-the-art performance.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[[98](#bib.bib98)] 将SPN和图神经网络整合到一个框架中，从而能够学习邻域和远程特征。为了适应实际应用，[[134](#bib.bib134),
    [17](#bib.bib17)] 提出了可以实现良好泛化的新方法。[[191](#bib.bib191)] 采用非局部空间传播网络来提升初始深度图的质量。至于产生初始深度的骨干网络，他们提出了一种新型的单分支网络，融合了CNN和transformers的优点。在该网络中，他们设计了一个联合卷积注意力和transformer
    (JCAT) 模块，包含两个路径——transformer层和卷积注意力层——并且比纯transformer方法具有更高的效率。为了学习邻域亲和力，[[73](#bib.bib73)]
    提出了一个利用多尺度和局部特征的网络。[[163](#bib.bib163)] 提出了一个长短期范围递归更新 (LRRU) 网络，利用迭代方案，并采用更少的参数实现了最先进的性能。'
- en: 4.3 Auxiliary-based Methods
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 辅助方法
- en: 'As described in Sec. [3.3](#S3.SS3 "3.3 Auxiliary-based Methods ‣ 3 Guided
    Depth Super-Resolution ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), auxiliary learning employs several related tasks to improve the performance
    of the main task [[192](#bib.bib192)]. Many methods using auxiliary learning also
    emerge in GDC. [[2](#bib.bib2)] propose a multi-task framework, which allows conducting
    monocular depth estimation and sparse depth completion. This network is implemented
    as two cascaded sub-networks based on encoder-encoder architecture, with the first
    yielding a sparse depth map from a color image and the second estimating a dense
    depth map, taking the previous sparse depth as input. To obtain sharp depth boundaries
    in the final depth maps, they adopt adversarial training on both synthetic and
    real-world datasets.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '如Sec. [3.3](#S3.SS3 "3.3 Auxiliary-based Methods ‣ 3 Guided Depth Super-Resolution
    ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods")中所述，辅助学习利用几个相关任务来提升主要任务的性能[[192](#bib.bib192)]。许多使用辅助学习的方法也出现在GDC中。[[2](#bib.bib2)]
    提出了一个多任务框架，允许进行单目深度估计和稀疏深度补全。该网络实现为基于编码器-编码器架构的两个级联子网络，第一个从彩色图像生成稀疏深度图，第二个估计密集深度图，输入为前一个稀疏深度图。为了在最终深度图中获得清晰的深度边界，他们在合成和真实世界数据集上采用了对抗训练。'
- en: '| ![Refer to caption](img/8223cfc70cc3df5e0a359b9c49ea923e.png) |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/8223cfc70cc3df5e0a359b9c49ea923e.png) |'
- en: 'Figure 13: MNASnet framework [[60](#bib.bib60)] belonging to auxiliary-based
    methods. It can perform depth completion and semantic segmentation simultaneously
    with minor changes in the last layer.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '图13: MNASnet框架[[60](#bib.bib60)]属于辅助方法。它可以在最后一层进行少量更改的情况下，同时执行深度补全和语义分割。'
- en: 'In addition to monocular depth estimation, semantic segmentation also contributes
    to the performance improvement of depth completion. [[60](#bib.bib60)] perform
    multi-task learning, namely depth completion and semantic segmentation as shown
    in Fig. [13](#S4.F13 "Figure 13 ‣ 4.3 Auxiliary-based Methods ‣ 4 Guided Depth
    Completion ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods"),
    by designing different heads at the end of a common network, e.g., NASNet [[206](#bib.bib206)].
    When depth density varies, the method can still fuse features across modalities
    effectively. Unlike the previous method, which does not utilize results from the
    auxiliary task, [[188](#bib.bib188)] use semantic maps to improve the accuracy
    of depth completion. In order to integrate semantic segmentation and depth completion
    into a single framework, they build multi-task generative adversarial networks
    in combination with a semantic-guided smoothness loss, which is inspired by[[193](#bib.bib193)].
    In addition to semantic segmentation and depth completion, edge detection, which
    serves as a bridge between the two tasks, is introduced into the simultaneous
    semantic segmentation and depth completion multi-task network (SSDNet) [[207](#bib.bib207)].
    Based on the multi-task learning of semantic segmentation and depth completion,
    [[52](#bib.bib52)] utilize boundary class labeling to adjust the effects of adjacent
    pixels. In[[81](#bib.bib81)], the authors perform multiple perception tasks jointly,
    including 2D and 3D object detection, ground estimation and depth completion,
    to obtain better representations. Although this work is aimed at object detection,
    it demonstrates that the selected tasks are complementary, so helpful cues provided
    by the other tasks can also be applied to depth completion. Motivated by [[101](#bib.bib101)],
    [[125](#bib.bib125)] design an auxiliary task to generate depth contours while
    outputting dense depth maps at the end of the network.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '除了单眼深度估计，语义分割也有助于深度补全性能的提升。[[60](#bib.bib60)] 进行多任务学习，即深度补全和语义分割，如图[13](#S4.F13
    "Figure 13 ‣ 4.3 Auxiliary-based Methods ‣ 4 Guided Depth Completion ‣ RGB Guided
    ToF Imaging System: A Survey of Deep Learning-based Methods")所示，通过在公共网络的末端设计不同的头部，例如NASNet [[206](#bib.bib206)]。当深度密度变化时，该方法仍能有效地融合跨模态特征。不同于之前的方法，这些方法不利用辅助任务的结果，[[188](#bib.bib188)]
    使用语义图来提高深度补全的准确性。为了将语义分割和深度补全集成到一个框架中，他们结合语义引导的平滑损失，构建了多任务生成对抗网络，这一思路受到了[[193](#bib.bib193)]的启发。除了语义分割和深度补全，边缘检测作为这两项任务之间的桥梁，被引入到同时进行语义分割和深度补全的多任务网络（SSDNet）[[207](#bib.bib207)]中。基于语义分割和深度补全的多任务学习，[[52](#bib.bib52)]
    利用边界类别标记来调整相邻像素的效果。在[[81](#bib.bib81)]中，作者联合执行多个感知任务，包括2D和3D物体检测、地面估计和深度补全，以获得更好的表征。尽管这项工作旨在物体检测，但它表明所选任务是互补的，因此其他任务提供的有用线索也可以应用于深度补全。受到[[101](#bib.bib101)]的启发，[[125](#bib.bib125)]
    设计了一个辅助任务，以生成深度轮廓，同时在网络末端输出稠密深度图。'
- en: Researchers leverage some further auxiliary information to enhance the precision
    of densified depth maps. Through a modified auto-encoder framework, [[190](#bib.bib190),
    [123](#bib.bib123)] propose to estimate surface normal maps from color images
    to improve the depth accuracy. In[[123](#bib.bib123)], the surface normals are
    regarded as intermediate representations since they can result in higher errors
    in distance measures. Considering the limitation of surface normals, the dense
    depth map estimated by the color branch is also considered in the generation of
    the final dense depth. Instead of generating a surface normal from a color map,
    [[88](#bib.bib88)] take the normal map produced by sparse depth maps as an intermediate
    constraint during training. [[177](#bib.bib177)] introduce a unified two-stage
    framework based on the assumption that natural scenes can be represented by piecewise
    planes. They build a diffusion module that incorporates the geometric constraints
    between depth and surface normals. This module is differentiable, allowing the
    diffusion conductance to be adjusted flexibly depending on the similarity in the
    guide. Inspired by [[154](#bib.bib154), [56](#bib.bib56)], [[111](#bib.bib111)]
    added a semantic-guided branch to highlight depth discontinuities.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员利用一些额外的辅助信息来提高密集深度图的精度。通过修改的自编码器框架，[[190](#bib.bib190), [123](#bib.bib123)]
    提出了从彩色图像估计表面法线图以提高深度准确性。在 [[123](#bib.bib123)] 中，表面法线被视为中间表示，因为它们可能在距离测量中产生更高的误差。考虑到表面法线的局限性，颜色分支估计的密集深度图也被纳入最终密集深度的生成中。[[88](#bib.bib88)]
    不是从彩色图生成表面法线，而是将稀疏深度图生成的法线图作为训练过程中的中间约束。[[177](#bib.bib177)] 引入了一个统一的两阶段框架，基于自然场景可以由分段平面表示的假设。他们构建了一个扩散模块，该模块结合了深度和表面法线之间的几何约束。这个模块是可微分的，允许扩散导电率根据引导中的相似性进行灵活调整。受到
    [[154](#bib.bib154), [56](#bib.bib56)] 的启发，[[111](#bib.bib111)] 增加了一个语义引导的分支以突出深度不连续性。
- en: Instead of using surface normals as auxiliary information, some works utilize
    a point cloud to enhance model performance. In order to cope with unpredictable
    real-world environments, [[62](#bib.bib62)] propose a point-cloud-centric method
    based on the proposed attentive bilateral convolutional layer. This 3D convolution
    layer consists of four steps, i.e., splat, convolve, attention, and slice, which
    can directly perform convolution operations on point clouds and focus on capturing
    helpful features for GDC.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究利用点云来增强模型性能，而不是使用表面法线作为辅助信息。为了应对不可预测的现实世界环境，[[62](#bib.bib62)] 提出了基于所提出的注意双边卷积层的点云中心方法。这个3D卷积层包括四个步骤，即：splat、convolve、attention
    和 slice，可以直接对点云进行卷积操作，并专注于捕捉对 GDC 有帮助的特征。
- en: Sometimes, depth completion as auxiliary information can also improve the performance
    of other tasks. For instance, [[6](#bib.bib6), [172](#bib.bib172)] integrate depth
    completion networks into object detection models to achieve more accurate results.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，深度补全作为辅助信息也可以提升其他任务的性能。例如，[[6](#bib.bib6), [172](#bib.bib172)] 将深度补全网络集成到目标检测模型中，以实现更准确的结果。
- en: 4.4 Fusion-focused Methods
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 聚焦融合的方法
- en: Similar to GDSR, fusion strategies are also crucial for GDC. Compared with straightforward
    fusion methods, such as concatenation [[29](#bib.bib29), [205](#bib.bib205)] or
    summation[[137](#bib.bib137), [130](#bib.bib130)], researchers design various
    complicated and effective methods.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GDSR 类似，融合策略对 GDC 也至关重要。与简单的融合方法（如串联 [[29](#bib.bib29), [205](#bib.bib205)]
    或求和 [[137](#bib.bib137), [130](#bib.bib130)]）相比，研究人员设计了各种复杂且有效的方法。
- en: '[[72](#bib.bib72)] propose a multi-scale network that exchanges information
    with the attention mechanism at each encoder scale. [[179](#bib.bib179)] employ
    a similar strategy to fuse cross-modality features at multiple scales. [[194](#bib.bib194)]
    conduct multi-modal information fusion via the proposed symmetric gated fusion
    strategy consisting of two paths, each aiming to modulate the current modality
    features with adaptively extracted features from the other modality. Using this
    strategy, features can interact across modalities, thus allowing the model to
    obtain accurate depth estimates. [[90](#bib.bib90)] develop a novel coarse-to-fine
    framework that emphasizes the fusion of cross-modal information. It introduces
    a channel shuffle extraction operation to blend features from different modalities.
    Next, they propose an energy-based fusion operation, which selects feature values
    with higher regional energy to fuse the features effectively. [[66](#bib.bib66)]
    propose a multi-modal deep aggregation network (MDANet), containing multiple connection
    and aggregation pathways to implement a deeper fusion of the two modalities. [[91](#bib.bib91)]
    design a multi-feature channel shuffle extraction, which utilizes the channel
    shuffle operation to fuse features from color images and depth maps in the encoding
    stage. Then, a multi-level weighted combination leveraging multi-scale features
    is introduced into the decoding process to enhance the information fusion effectiveness
    further.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[[72](#bib.bib72)] 提出了一种多尺度网络，通过每个编码器尺度的注意机制交换信息。[[179](#bib.bib179)] 采用类似的策略，在多个尺度下融合跨模态特征。[[194](#bib.bib194)]
    通过提出的对称门控融合策略进行多模态信息融合，该策略由两个路径组成，每个路径旨在用从其他模态中自适应提取的特征调制当前模态特征。使用此策略，可以跨模态交互特征，从而使模型获得准确的深度估计。[[90](#bib.bib90)]
    开发了一种新颖的粗到细框架，强调跨模态信息的融合。它引入了通道混洗提取操作，以融合不同模态的特征。接下来，他们提出了一种基于能量的融合操作，选择具有较高区域能量的特征值，以有效地融合特征。[[66](#bib.bib66)]
    提出了一个多模态深度聚合网络（MDANet），包含多个连接和聚合路径，以实现两种模态的更深融合。[[91](#bib.bib91)] 设计了一种多特征通道混洗提取，利用通道混洗操作在编码阶段融合彩色图像和深度图的特征。然后，将多尺度特征加权组合引入解码过程，以进一步增强信息融合效果。'
- en: '| ![Refer to caption](img/18e0dfab6b405a94e97401113e0a7a16.png) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/18e0dfab6b405a94e97401113e0a7a16.png) |'
- en: 'Figure 14: The framework of FusionNet [[154](#bib.bib154)] belonging to the
    category of fusion strategy. This method fuses global and local features based
    on the confidence maps in a late fusion manner.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：FusionNet的框架[[154](#bib.bib154)]，属于融合策略类别。这种方法基于置信度图以晚期融合的方式融合全局和局部特征。
- en: 4.5 Uncertainty Guidance
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 不确定性引导
- en: Most of the works surveyed so far do not take into account the reliability of
    the sparse depth when designing algorithms. Recently, two categories of uncertainty,
    i.e., aleatoric and epistemic uncertainty, have been considered in GDC with Bayesian
    deep learning. Aleatoric or stochastic uncertainty represents inherent randomness
    in each observation of the same experiment. In the task of GDC, aleatoric uncertainty
    refers to the random noise in raw LiDAR data and the distribution of sparse pixels.
    Epistemic or systematic uncertainty is often caused by models ignoring certain
    practical factors. Hence, the models for GDC are usually focused on handling aleatoric
    uncertainty.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，大多数调研工作的算法设计并未考虑稀疏深度的可靠性。最近，两类不确定性，即**偶然不确定性**和**认识不确定性**，已在GDC中与贝叶斯深度学习一起考虑。偶然不确定性或随机不确定性表示同一实验中每次观察的固有随机性。在GDC任务中，偶然不确定性指的是原始LiDAR数据中的随机噪声和稀疏像素的分布。认识不确定性或系统性不确定性通常由模型忽略某些实际因素造成。因此，GDC的模型通常关注处理偶然不确定性。
- en: '[[28](#bib.bib28), [29](#bib.bib29)] introduce a novel framework with a proposed
    algebraically-constrained normalized convolution layer, where confidence can be
    computed and propagated to subsequent layers. Here, confidence is used as auxiliary
    information to improve the model performance. Furthermore, they propose a loss
    function by maximizing the final confidence to constrain the training. Later,
    [[27](#bib.bib27)] estimate the confidence of the input depth with the normalized
    convolution layer in an unsupervised manner, which can significantly improve the
    accuracy of depth prediction.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[[28](#bib.bib28), [29](#bib.bib29)] 引入了一种新颖的框架，并提出了代数约束的归一化卷积层，其中可以计算和传播置信度。这里，置信度作为辅助信息用于提升模型性能。此外，他们提出了一种通过最大化最终置信度来约束训练的损失函数。后来，[[27](#bib.bib27)]
    通过归一化卷积层以无监督方式估计输入深度的置信度，这可以显著提高深度预测的准确性。'
- en: 'As shown in Fig. [14](#S4.F14 "Figure 14 ‣ 4.4 Fusion-focused Methods ‣ 4 Guided
    Depth Completion ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), [[154](#bib.bib154)] estimate confidence maps in the global and local
    branches, respectively. Based on the learned confidence maps calculated by the
    softmax function in the last layer, the model can pay more attention to reliable
    pixels. Sharing a similar idea, [[123](#bib.bib123)] output confidence maps for
    occlusion handling and integrate depth predictions from color images and normal
    as well. [[177](#bib.bib177)] obtain confidence maps through a decoder to suppress
    noise propagation in depth maps.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示 [14](#S4.F14 "图 14 ‣ 4.4 聚焦融合方法 ‣ 4 引导深度完成 ‣ RGB 引导 ToF 成像系统：基于深度学习的方法概述")，[[154](#bib.bib154)]
    分别在全局和局部分支中估计置信度图。基于最后一层通过 softmax 函数计算的学习到的置信度图，模型可以更加关注可靠的像素。类似地，[[123](#bib.bib123)]
    输出用于遮挡处理的置信度图，并将来自彩色图像和法线的深度预测也进行整合。[[177](#bib.bib177)] 通过解码器获取置信度图，以抑制深度图中的噪声传播。
- en: '[[205](#bib.bib205)] provide a novel uncertainty formulation with Jeffrey’s
    prior, which can improve the robustness of the model to noise or invalid pixels.
    In the first stage, this proposed multi-scale joint prediction model outputs coarse
    depth and uncertainty maps, which are then sent to the uncertainty attention residual
    learning network for depth refinement. Through their architecture and loss function,
    outliers and uneven distribution of pixels are well handled. Additionally, [[115](#bib.bib115)]
    attempt to employ the proposed uncertainty-aware sampling algorithm to identify
    the reliability of pixels, which is beneficial for the subsequent adaptive prediction
    of the depth range for each pixel.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[[205](#bib.bib205)] 提供了一种基于 Jeffrey 先验的新颖不确定性公式，这可以提高模型对噪声或无效像素的鲁棒性。在第一阶段，该提出的多尺度联合预测模型输出粗略的深度和不确定性图，然后将其发送到不确定性关注残差学习网络进行深度细化。通过他们的架构和损失函数，可以很好地处理离群点和像素的不均匀分布。此外，[[115](#bib.bib115)]
    尝试采用提出的不确定性感知采样算法来识别像素的可靠性，这有利于随后对每个像素深度范围的自适应预测。'
- en: '| ![Refer to caption](img/4c6687c20aea14f2bb84be8db0272e6b.png) |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/4c6687c20aea14f2bb84be8db0272e6b.png) |'
- en: 'Figure 15: VIO [[169](#bib.bib169)] framework belonging to unsupervised methods.
    Combining the four losses of photometric consistency, sparse depth consistency,
    pose consistency and local smoothness enables training the network in a fully
    unsupervised manner.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15: VIO [[169](#bib.bib169)] 框架属于无监督方法。结合了光度一致性、稀疏深度一致性、姿态一致性和局部平滑性的四种损失，使得网络可以完全无监督地进行训练。'
- en: '| Paradigm | Method | Reference | Key Idea |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 范式 | 方法 | 参考文献 | 关键理念 |'
- en: '| Dual-branch | MNASNet [[60](#bib.bib60)] | 3DV-2018 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 双分支 | MNASNet [[60](#bib.bib60)] | 3DV-2018 |'
- en: '&#124; Leverage fused features from dual streams to yield the final dense estimation
    &#124;'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用来自双流的融合特征生成最终的密集估计 &#124;'
- en: '&#124; via a decoder. &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过解码器实现。 &#124;'
- en: '|'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| GuideNet [[150](#bib.bib150)] | TIP-2020 | Propose a hourglass network for
    depth completion. |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| GuideNet [[150](#bib.bib150)] | TIP-2020 | 提出了一种用于深度完成的沙漏网络。 |'
- en: '| KernelNet  [[89](#bib.bib89)] | TIP-2021 | Present a two-stage model where
    depth interpolation and refinement are performed sequentially. |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| KernelNet  [[89](#bib.bib89)] | TIP-2021 | 提出了一种两阶段模型，其中深度插值和细化依次进行。 |'
- en: '| RigNet [[180](#bib.bib180)] | ECCV-2022 | Exploit a repetitive design based
    on the hourglass network. |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| RigNet [[180](#bib.bib180)] | ECCV-2022 | 利用基于沙漏网络的重复设计。 |'
- en: '| MSG-CHN [[74](#bib.bib74)] | WACV-2020 | Introduce the cascaded architecture
    into the hourglass network. |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| MSG-CHN [[74](#bib.bib74)] | WACV-2020 | 将级联架构引入沙漏网络。 |'
- en: '| CDCNet  [[30](#bib.bib30)] | BMVC-2022 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| CDCNet  [[30](#bib.bib30)] | BMVC-2022 |'
- en: '| Penet [[56](#bib.bib56)] | ICRA-2021 | Enhance the accuracy of depth prediction
    in a coarse-to-fine manner. |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Penet [[56](#bib.bib56)] | ICRA-2021 | 以粗到细的方式提高深度预测的准确性。|'
- en: '| FusionNet [[154](#bib.bib154)] | MVA-2019 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| FusionNet [[154](#bib.bib154)] | MVA-2019 |'
- en: '&#124; Exploit confidence maps in the global and local branches to achieve
    a guided- &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用全局和局部分支中的置信图实现引导-&#124;'
- en: '&#124; base method. &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基础方法。&#124;'
- en: '|'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DenseLiDAR [[39](#bib.bib39)] | RAL-2021 | Use a dense pseudo-depth map as
    a coarse reference for the final prediction. |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| DenseLiDAR [[39](#bib.bib39)] | RAL-2021 | 使用密集伪深度图作为最终预测的粗略参考。|'
- en: '| SDDC [[175](#bib.bib175)] | Visual Computer-2023 | Propose the semantic-guided
    branch to achieve better prediction results. |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| SDDC [[175](#bib.bib175)] | Visual Computer-2023 | 提出语义引导分支，以获得更好的预测结果。|'
- en: '| QNN [[63](#bib.bib63)] | CVPR-2022 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| QNN [[63](#bib.bib63)] | CVPR-2022 |'
- en: '&#124; Adopt a dual-branch structure to achieve an efficient solution with
    low power &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 采用双分支结构，以低功耗实现高效解决方案&#124;'
- en: '&#124; consumption. &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 消耗。&#124;'
- en: '|'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ACMNet [[194](#bib.bib194)] | TIP-2021 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| ACMNet [[194](#bib.bib194)] | TIP-2021 |'
- en: '&#124; Conduct multi-modal information fusion via the proposed symmetric gated
    &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过提出的对称门控进行多模态信息融合&#124;'
- en: '&#124; fusion strategy. &#124;'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 融合策略。&#124;'
- en: '|'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| PointDC [[185](#bib.bib185)] | ICCV-2023 | Propose a dual-branch network
    to extract 2d and sparse 3D feature point cloud for depth completion. |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| PointDC [[185](#bib.bib185)] | ICCV-2023 | 提出了一种双分支网络，用于提取2D和稀疏3D特征点云以完成深度。|'
- en: '| AGGNet [[8](#bib.bib8)] | ICCV-2023 | Design a dual-branch autoencoder with
    attention mechenism to obtain dense depth maps. |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| AGGNet [[8](#bib.bib8)] | ICCV-2023 | 设计了一个带有注意力机制的双分支自编码器，以获得密集的深度图。|'
- en: '| Affinity-based | SPN [[95](#bib.bib95)] | NeurIPS-2017 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Affinity-based | SPN [[95](#bib.bib95)] | NeurIPS-2017 |'
- en: '&#124; Exploit the spatial propagation network to learning affinity matrix
    for depth &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用空间传播网络学习深度的亲和矩阵&#124;'
- en: '&#124; completion. &#124;'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完成。&#124;'
- en: '|'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CSPN [[11](#bib.bib11)] | ECCV-2018 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| CSPN [[11](#bib.bib11)] | ECCV-2018 |'
- en: '&#124; Formulate the task as anisotropic diffusion filtering, and implement
    it through a &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 将任务表述为各向异性扩散滤波，并通过&#124;'
- en: '&#124; deep network. &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度网络。&#124;'
- en: '|'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CSPN++ [[10](#bib.bib10)] | AAAI-2020 | Propose a new structure including
    context-aware and resource-aware CSPNs. |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| CSPN++ [[10](#bib.bib10)] | AAAI-2020 | 提出包括上下文感知和资源感知CSPNs的新结构。|'
- en: '| NLSPN [[113](#bib.bib113)] | ECCV-2020 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| NLSPN [[113](#bib.bib113)] | ECCV-2020 |'
- en: '&#124; Propose a two-stage network that is based on non-local neighbors instead
    of a &#124;'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出基于非局部邻居的两阶段网络，而不是&#124;'
- en: '&#124; fixed receptive field. &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 固定感受野。&#124;'
- en: '|'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DSPN [[178](#bib.bib178)] | ICIP-2020 | Conduct spatial propagation with
    deformable convolution. |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| DSPN [[178](#bib.bib178)] | ICIP-2020 | 通过可变形卷积进行空间传播。|'
- en: '| DySPN [[84](#bib.bib84)] | AAAI-2022 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| DySPN [[84](#bib.bib84)] | AAAI-2022 |'
- en: '&#124; Develop a non-linear propagation model, where the affinity matrix is
    decoupled &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开发一个非线性传播模型，其中亲和矩阵被解耦&#124;'
- en: '&#124; into parts depending on the distance. &#124;'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 根据距离将其划分为不同部分。&#124;'
- en: '|'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| GraphCSPN [[98](#bib.bib98)] | ECCV-2022 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| GraphCSPN [[98](#bib.bib98)] | ECCV-2022 |'
- en: '&#124; Integrate SPN and graph neural networks to learn both neighboring and
    long- &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 整合SPN和图神经网络，以学习邻近和长距离-&#124;'
- en: '&#124; range features. &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 范围特征。&#124;'
- en: '|'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Ssgp [[134](#bib.bib134)] | WACV-2021 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Ssgp [[134](#bib.bib134)] | WACV-2021 |'
- en: '&#124; Propose a unified architecture to perform sparse-to-dense interpolation
    in &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出统一架构以在&#124;'
- en: '&#124; different domains. &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不同领域。&#124;'
- en: '|'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SpAgNet [[17](#bib.bib17)] | WACV-2023 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| SpAgNet [[17](#bib.bib17)] | WACV-2023 |'
- en: '&#124; Propose a depth completion method that is independent of their distribution
    and &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出不依赖于分布和密度的深度完成方法&#124;'
- en: '&#124; density. &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 密度。&#124;'
- en: '|'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CompletionFormer [[191](#bib.bib191)] | CVPR-2023 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| CompletionFormer [[191](#bib.bib191)] | CVPR-2023 |'
- en: '&#124; Leverage the non-local spatial propagation network to improve the depth
    &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用非局部空间传播网络提高深度&#124;'
- en: '&#124; quality after obtaining initial depth with convolutions and transformers.
    &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 卷积和变换器获得初始深度后的质量。&#124;'
- en: '|'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DenseLConv [[73](#bib.bib73)] | IROS-2022 | Propose a depth completion method
    that is independent of their distribution and density. |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| DenseLConv [[73](#bib.bib73)] | IROS-2022 | 提出了一种不依赖于分布和密度的深度完成方法。|'
- en: '| LRRU [[163](#bib.bib163)] | ICCV-2023 | Propose a lightweight network to
    learn spatially-variant kernels and perform updates iteratively. |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| LRRU [[163](#bib.bib163)] | ICCV-2023 | 提出一种轻量级网络来学习空间变换核，并进行迭代更新。 |'
- en: '| Auxiliary-based | DCMDE [[2](#bib.bib2)] | 3DV-2019 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 基于辅助的 | DCMDE [[2](#bib.bib2)] | 3DV-2019 |'
- en: '&#124; Propose a multi-task framework conducting monocular depth estimation
    and &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一个多任务框架，进行单目深度估计和 &#124;'
- en: '&#124; sparse depth completion. &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 稀疏深度完成。 &#124;'
- en: '|'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Multitask GANs [[188](#bib.bib188)] | TNNLS-2021 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 多任务 GANs [[188](#bib.bib188)] | TNNLS-2021 |'
- en: '&#124; Present a multi-task network using semantic images to improve the accuracy
    &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一种多任务网络，使用语义图像提高精度 &#124;'
- en: '&#124; of depth completion. &#124;'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度完成的组成部分。 &#124;'
- en: '|'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SSDNet  [[207](#bib.bib207)] | Sensors-2020 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| SSDNet  [[207](#bib.bib207)] | Sensors-2020 |'
- en: '&#124; Introduce edge detection to serve as a bridge between semantic segmentation
    &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引入边缘检测，作为语义分割的桥梁 &#124;'
- en: '&#124; and depth completion. &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和深度完成。 &#124;'
- en: '|'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MMF  [[81](#bib.bib81)] | CVPR-2019 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| MMF  [[81](#bib.bib81)] | CVPR-2019 |'
- en: '&#124; Combine 2D and 3D object detection, ground estimation, and depth completion
    &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结合 2D 和 3D 目标检测、地面估计和深度完成 &#124;'
- en: '&#124; to obtain better representations. &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以获得更好的表示。 &#124;'
- en: '|'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DPS  [[52](#bib.bib52)] | RAL-2019 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| DPS  [[52](#bib.bib52)] | RAL-2019 |'
- en: '&#124; Present a method for estimating a dense depth from a sparse LIDAR point
    cloud &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一种从稀疏 LIDAR 点云估计密集深度的方法 &#124;'
- en: '&#124; and an image sequence. &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和图像序列。 &#124;'
- en: '|'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SIUNet  [[125](#bib.bib125)] | WACV-2023 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| SIUNet  [[125](#bib.bib125)] | WACV-2023 |'
- en: '&#124; Propose a network regarding depth contours as auxiliary information
    to obtain &#124;'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一个网络，将深度轮廓作为辅助信息来获得 &#124;'
- en: '&#124; dense depth maps. &#124;'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 密集深度图。 &#124;'
- en: '|'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DDC  [[190](#bib.bib190)] | CVPR-2018 | Propose to estimate surface normal
    maps from color images to improve the depth accuracy. |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| DDC  [[190](#bib.bib190)] | CVPR-2018 | 提出从彩色图像估计表面法线图，以提高深度精度。 |'
- en: '| Deeplidar  [[123](#bib.bib123)] | CVPR-2019 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| Deeplidar  [[123](#bib.bib123)] | CVPR-2019 |'
- en: '| NNNet  [[88](#bib.bib88)] | IEEE Access-2022 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| NNNet  [[88](#bib.bib88)] | IEEE Access-2022 |'
- en: '| DNC  [[177](#bib.bib177)] | ICCV-2019 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| DNC  [[177](#bib.bib177)] | ICCV-2019 |'
- en: '&#124; Confidence maps are obtained through a decoder to suppress noise propagation
    &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过解码器获得置信度图，以抑制噪声传播 &#124;'
- en: '&#124; in depth maps. &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度图中的 &#124;'
- en: '|'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SemAttNet  [[111](#bib.bib111)] | IEEE Access-2022 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| SemAttNet  [[111](#bib.bib111)] | IEEE Access-2022 |'
- en: '&#124; Add a semantic-guided branch for depth completion to highlight depth
    &#124;'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 添加一个语义引导分支用于深度完成，以突出深度 &#124;'
- en: '&#124; discontinuities. &#124;'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不连续性。 &#124;'
- en: '|'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ABCD  [[62](#bib.bib62)] | RAL-2021 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| ABCD  [[62](#bib.bib62)] | RAL-2021 |'
- en: '&#124; Propose a point-cloud-centric method that is based on the proposed attentive
    &#124;'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一种以点云为中心的方法，基于提出的注意力机制 &#124;'
- en: '&#124; bilateral convolutional layer. &#124;'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 双边卷积层。 &#124;'
- en: '|'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 4: List of Guided Depth Completion methods, divided according to the
    first three outlined categories.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：引导深度完成方法的列表，按前三种分类进行划分。
- en: '| Paradigm | Method | Reference | Key Idea |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | 方法 | 参考文献 | 关键思想 |'
- en: '| Fusion-focused | CGM  [[89](#bib.bib89)] | IEEE Access-2020 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 聚焦融合 | CGM  [[89](#bib.bib89)] | IEEE Access-2020 |'
- en: '&#124; Present a two-stage model where depth interpolation and refinement are
    &#124;'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一个两阶段模型，其中深度插值和细化是 &#124;'
- en: '&#124; performed sequentially. &#124;'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 依次执行。 &#124;'
- en: '|'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| RSIC  [[179](#bib.bib179)] | IEEE Access-2020 | Employ a similar strategy
    to fuse cross-modality features at multiple scales. |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| RSIC  [[179](#bib.bib179)] | IEEE Access-2020 | 采用类似策略在多个尺度下融合跨模态特征。 |'
- en: '| FCFR-Net  [[90](#bib.bib90)] | AAAI-2021 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| FCFR-Net  [[90](#bib.bib90)] | AAAI-2021 |'
- en: '&#124; Develop a novel coarse-to-fine framework that emphasizes the fusion
    of &#124;'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开发一种新颖的粗到细的框架，强调 &#124;'
- en: '&#124; cross-modal information. &#124;'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨模态信息。 &#124;'
- en: '|'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MDANet  [[66](#bib.bib66)] | ICRA-2021 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| MDANet  [[66](#bib.bib66)] | ICRA-2021 |'
- en: '&#124; Propose a multi-modal deep aggregation block that consists of multiple
    &#124;'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一种多模态深度聚合块，由多个 &#124;'
- en: '&#124; connection and aggregation pathways for deeper fusion. &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 连接和聚合路径以实现更深层次的融合。 &#124;'
- en: '|'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MFF-Net  [[91](#bib.bib91)] | RAL-2023 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| MFF-Net  [[91](#bib.bib91)] | RAL-2023 |'
- en: '&#124; Propose a multi-feature channel shuffle extraction and a decoding process
    with &#124;'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出一种多特征通道混洗提取和交叉模态信息解码过程的方案。 &#124;'
- en: '&#124; multi-scale features. &#124;'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多尺度特征的融合。 &#124;'
- en: '|'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Uncertainty-guided | MS-Net  [[29](#bib.bib29)] | TPAMI-2019 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 不确定性引导 | MS-Net  [[29](#bib.bib29)] | TPAMI-2019 |'
- en: '&#124; Introduce a novel framework with a proposed algebraically-constrained
    &#124;'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引入一种新颖的框架，提出代数约束&#124;'
- en: '&#124; normalized convolution layer. &#124;'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 归一化卷积层。&#124;'
- en: '|'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| pNCNN  [[27](#bib.bib27)] | CVPR-2020 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| pNCNN  [[27](#bib.bib27)] | CVPR-2020 |'
- en: '&#124; Estimate the confidence of the input depth with the normalized convolution
    &#124;'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用归一化卷积层估计输入深度的置信度&#124;'
- en: '&#124; layer in an unsupervised manner. &#124;'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以无监督的方式进行的层。&#124;'
- en: '|'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MJPM  [[205](#bib.bib205)] | AAAI-2022 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| MJPM  [[205](#bib.bib205)] | AAAI-2022 |'
- en: '&#124; Provide Jeffrey’s prior uncertainty formulation that can increase the
    model’s &#124;'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 提供杰弗瑞的先验不确定性公式，以提高模型的&#124;
- en: '&#124; resistance to noise and invalid pixel input. &#124;'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对噪声和无效像素输入的抵抗力。&#124;'
- en: '|'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| FusionNet [[168](#bib.bib168)] | RAL-2021 | Leverage synthetic datasets to
    alleviate the lack of real image pairs. |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| FusionNet [[168](#bib.bib168)] | RAL-2021 | 利用合成数据集缓解真实图像对的缺乏。 |'
- en: '| PADNet  [[115](#bib.bib115)] | ACM MM-2022 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| PADNet  [[115](#bib.bib115)] | ACM MM-2022 |'
- en: '&#124; Employ the proposed uncertainty-aware sampling algorithm to identify
    the &#124;'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 采用提出的不确定性感知采样算法来识别&#124;'
- en: '&#124; reliability of pixels, which is beneficial for prediction of the depth
    range. &#124;'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 像素的可靠性，这对深度范围预测有利。&#124;'
- en: '|'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Unsupervised | SfMLearner [[201](#bib.bib201)] | CVPR-2017 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| 无监督 | SfMLearner [[201](#bib.bib201)] | CVPR-2017 |'
- en: '&#124; Single-view depth prediction and multi-view camera pose estimation from
    two &#124;'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单视角深度预测和多视角相机姿态估计来自两个&#124;'
- en: '&#124; independent networks. &#124;'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 独立网络。&#124;'
- en: '|'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SSDC [[105](#bib.bib105)] | ICRA-2019 | Propose learning a mapping from the
    sparse input to the dense prediction. |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| SSDC [[105](#bib.bib105)] | ICRA-2019 | 提出从稀疏输入到密集预测的映射学习。 |'
- en: '| lsf [[124](#bib.bib124)] | WACV-2020 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| lsf [[124](#bib.bib124)] | WACV-2020 |'
- en: '&#124; Propose a least squares fitting model for the self-supervised learning
    benchmark &#124;'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出用于自监督学习基准的最小二乘拟合模型&#124;'
- en: '&#124; network. &#124;'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络。&#124;'
- en: '|'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| VIO [[169](#bib.bib169)] | ICRA-2020 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| VIO [[169](#bib.bib169)] | ICRA-2020 |'
- en: '&#124; Use a piecewise planar scaffolding of a scene as supervision to achieve
    self- &#124;'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 使用场景的分段平面支撑作为监督以实现自我&#124;
- en: '&#124; supervised learning. &#124;'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 监督学习。&#124;'
- en: '|'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| KBNet [[170](#bib.bib170)] | ICCV-2021 | Exploit the proposed calibrated
    backprojection layers to improve the baselines. |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| KBNet [[170](#bib.bib170)] | ICCV-2021 | 利用提出的标定反投影层来改善基准性能。 |'
- en: '| Monitored Distillation [[96](#bib.bib96)] | ECCV-2022 | Introduce knowledge
    distillation into depth completion. |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| Monitored Distillation [[96](#bib.bib96)] | ECCV-2022 | 将知识蒸馏引入深度补全。 |'
- en: '| CostDCNet [[65](#bib.bib65)] | ECCV-2022 | Propose a two-branch network based
    on the cost volume-based depth estimation. |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| CostDCNet [[65](#bib.bib65)] | ECCV-2022 | 提出基于代价体积的深度估计的双分支网络。 |'
- en: '| Struct-MDC [[61](#bib.bib61)] | RAL-2022 | First perform mesh depth construction
    by leveraging point and line features. |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| Struct-MDC [[61](#bib.bib61)] | RAL-2022 | 首先通过利用点和线特征执行网格深度构建。 |'
- en: '| DDP [[182](#bib.bib182)] | CVPR-2019 | Develop a conditional prior network
    to calculate a posterior probability over the depth of each pixel. |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| DDP [[182](#bib.bib182)] | CVPR-2019 | 开发一个条件先验网络来计算每个像素深度的后验概率。 |'
- en: '| AGG-CVCNet [[176](#bib.bib176)] | ACM MM-2022 | Propose a novel adjacent
    geometry guided training loss. |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| AGG-CVCNet [[176](#bib.bib176)] | ACM MM-2022 | 提出一种新颖的邻接几何引导训练损失。 |'
- en: '| MSC [[189](#bib.bib189)] | Remote Sensing-2022 | Introduce multi-modal spatio-temporal
    consistency constraints to train models. |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| MSC [[189](#bib.bib189)] | Remote Sensing-2022 | 引入多模态时空一致性约束以训练模型。 |'
- en: '| LeoVR [[75](#bib.bib75)] | MobiSys-2022 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| LeoVR [[75](#bib.bib75)] | MobiSys-2022 |'
- en: 'Table 5: List of Guided Depth Completion methods, divided according to the
    last three outlined categories.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：引导深度补全方法列表，按最后三种分类划分。
- en: 4.6 Unsupervised Methods
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 无监督方法
- en: In practice, dense ground truth depth maps are hard to obtain due to the limitations
    of LiDAR imaging principles. Even a high-end LiDAR can only generate at most $30\%$
    valid pixels of the whole depth map[[153](#bib.bib153)]. Thus, some methods attempt
    to address the problem with self-supervised learning, among which the representative
    classes build supervisory signals using monocular sequences or synchronized stereo
    pairs.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，由于LiDAR成像原理的限制，密集的真实深度图很难获得。即使是高端LiDAR，也只能生成最多$30\%$的有效像素[[153](#bib.bib153)]。因此，一些方法尝试通过自监督学习来解决这个问题，其中代表性的类别利用单目序列或同步立体对来构建监督信号。
- en: 'As depicted in Fig. [15](#S4.F15 "Figure 15 ‣ 4.5 Uncertainty Guidance ‣ 4
    Guided Depth Completion ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), [[201](#bib.bib201)] design a self-supervised framework in which only
    color and depth sequences must be provided instead of sparse/dense image pairs.
    The method produces single-view depth prediction and multi-view camera pose estimation
    from two independent networks. This principle is brought to GDC by [[105](#bib.bib105)]
    introducing three loss functions, i.e., sparse depth loss, photometric loss, and
    smoothness loss. Under this framework, the performance even outperforms some methods
    with semi-dense annotations. To increase the robustness to noise and outliers,
    [[124](#bib.bib124)] substituted the proposed least squares fitting model for
    the last convolutional layer of the self-supervised learning benchmark network[[38](#bib.bib38)].
    [[169](#bib.bib169)] propose to use a piecewise planar scaffolding of a scene
    as supervision to implement self-supervised learning. Using a similar structure,
    [[170](#bib.bib170)] leverage the proposed calibrated back-projection layers to
    improve the baselines, while [[96](#bib.bib96)] introduce knowledge distillation
    into depth completion. [[65](#bib.bib65)] propose a two-branch network inspired
    by cost volume-based depth estimation, which can obtain informative 2D and 3D
    representations from cross-modality input with the lightweight structure design.
    In [[61](#bib.bib61)], they first perform mesh depth construction by leveraging
    point and line features, which, together with the color image, are fed into a
    mesh depth refinement module. Finally, the depth prediction can be obtained through
    the calibrated backprojection network (KBNet) [[170](#bib.bib170)]. While most
    networks are trained on real datasets, [[168](#bib.bib168), [99](#bib.bib99)]
    train their models on synthetic datasets, alleviating the lack of real image pairs
    with dense labels.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[15](#S4.F15 "图 15 ‣ 4.5 不确定性指导 ‣ 4 深度补全 ‣ RGB引导的ToF成像系统：基于深度学习的方法综述")所示，[[201](#bib.bib201)]设计了一个自监督框架，其中只需要提供颜色和深度序列，而不是稀疏/密集的图像对。该方法通过两个独立的网络生成单视图深度预测和多视图相机姿态估计。[[105](#bib.bib105)]通过引入三种损失函数，即稀疏深度损失、光度损失和光滑性损失，将这一原理引入GDC。在这个框架下，性能甚至超过了一些具有半密集注释的方法。为了提高对噪声和离群点的鲁棒性，[[124](#bib.bib124)]用提出的最小二乘拟合模型替代了自监督学习基准网络[[38](#bib.bib38)]的最后卷积层。[[169](#bib.bib169)]提出使用场景的分段平面支架作为监督来实现自监督学习。使用类似结构的[[170](#bib.bib170)]利用提出的标定反投影层来改进基线，而[[96](#bib.bib96)]则将知识蒸馏引入深度补全。[[65](#bib.bib65)]提出了一种受成本体积深度估计启发的双分支网络，该网络可以从跨模态输入中获取信息丰富的2D和3D表示，且具有轻量级结构设计。在[[61](#bib.bib61)]中，他们首先通过利用点和线特征进行网格深度构建，然后将其与颜色图像一起输入网格深度优化模块。最后，可以通过标定反投影网络（KBNet）[[170](#bib.bib170)]获得深度预测。虽然大多数网络在真实数据集上训练，[[168](#bib.bib168),
    [99](#bib.bib99)]则在合成数据集上训练他们的模型，从而缓解了缺乏真实图像对和密集标签的问题。
- en: In unsupervised learning, stereo pairs also play an important role. Among the
    self-supervised stereo methods, most use photometric consistency [[37](#bib.bib37),
    [182](#bib.bib182), [137](#bib.bib137)] to infer geometry and obtain promising
    results. In contrast, [[176](#bib.bib176)] propose a novel adjacent geometry guided
    training loss to confine depth maps of low-confident regions by high-confident
    labels. [[189](#bib.bib189), [75](#bib.bib75)] introduce multi-modal spatiotemporal
    consistency constraints to train models, which enables the model to better adapt
    to real-world environments, such as dark and low-texture objects.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，立体对也发挥了重要作用。在自监督立体方法中，大多数使用光度一致性 [[37](#bib.bib37), [182](#bib.bib182),
    [137](#bib.bib137)] 来推断几何并获得有前景的结果。相比之下，[[176](#bib.bib176)] 提出了一个新颖的相邻几何引导训练损失，通过高置信度标签来限制低置信度区域的深度图。[[189](#bib.bib189),
    [75](#bib.bib75)] 引入了多模态时空一致性约束来训练模型，使模型更好地适应现实世界环境，如黑暗和低纹理物体。
- en: 'To conclude this section, Tabs. [4](#S4.T4 "Table 4 ‣ 4.5 Uncertainty Guidance
    ‣ 4 Guided Depth Completion ‣ RGB Guided ToF Imaging System: A Survey of Deep
    Learning-based Methods") and [5](#S4.T5 "Table 5 ‣ 4.5 Uncertainty Guidance ‣
    4 Guided Depth Completion ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods") collect the methods discussed so far, divided into six categories. For
    each method, the venue and year are reported, together with a short description
    of the key idea behind it.'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 总结本节内容，表格 [4](#S4.T4 "表格 4 ‣ 4.5 不确定性引导 ‣ 4 引导深度完成 ‣ RGB 引导 ToF 成像系统：基于深度学习的方法综述")
    和 [5](#S4.T5 "表格 5 ‣ 4.5 不确定性引导 ‣ 4 引导深度完成 ‣ RGB 引导 ToF 成像系统：基于深度学习的方法综述") 收集了迄今为止讨论的方法，分为六类。每种方法报告了其场地和年份，并简要描述了其核心思想。
- en: 5 Benchmark Datasets and objective functions
  id: totrans-498
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 基准数据集和目标函数
- en: We introduce existing datasets relevant to our survey and the most common loss
    functions used to train the methods we surveyed before.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了与我们的调查相关的现有数据集以及用于训练我们之前调查的方法的最常见损失函数。
- en: 5.1 Datasets
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 数据集
- en: This section introduces public datasets for RGB guided ToF imaging; they are
    divided into low-resolution and sparse depth according to the image characteristics
    collected by ToF cameras.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了用于 RGB 引导的 ToF 成像的公共数据集；它们根据 ToF 相机收集的图像特征分为低分辨率和稀疏深度。
- en: '|  | Dataset | Ref. | Year | Sensor Name | Capture condition | Modalities |
    Images |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据集 | 参考文献 | 年份 | 传感器名称 | 捕捉条件 | 模态 | 图片 |'
- en: '| LR Depth | ToFMark |  [[33](#bib.bib33)] | 2013 | PMD Nano | Indoor | Color,
    Depth | 3 Images |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| LR Depth | ToFMark |  [[33](#bib.bib33)] | 2013 | PMD Nano | 室内 | 彩色, 深度
    | 3 张图片 |'
- en: '| Lu |  [[102](#bib.bib102)] | 2014 | ASUS Xiton Pro | Indoor | Color, Depth
    | 6 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| Lu |  [[102](#bib.bib102)] | 2014 | ASUS Xiton Pro | 室内 | 彩色, 深度 | 6 |'
- en: '| SUN RGBD |  [[140](#bib.bib140)] | 2015 | Kinect v2 | Indoor | Color, Depth
    | 2860 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| SUN RGBD |  [[140](#bib.bib140)] | 2015 | Kinect v2 | 室内 | 彩色, 深度 | 2860
    |'
- en: '| DIML |  [[12](#bib.bib12)] | 2021 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| DIML |  [[12](#bib.bib12)] | 2021 |'
- en: '&#124; Kinect v2, &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Kinect v2, &#124;'
- en: '&#124; ZED Stereo &#124;'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ZED Stereo &#124;'
- en: '|'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Indoor, &#124;'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 室内, &#124;'
- en: '&#124; Outdoor &#124;'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 户外 &#124;'
- en: '| Color, Depth | $>$200 Scenes |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| 彩色, 深度 | $>$200 场景 |'
- en: '| RGBDD |  [[50](#bib.bib50)] | 2021 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| RGBDD |  [[50](#bib.bib50)] | 2021 |'
- en: '&#124; Huawei P30 Pro, &#124;'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Huawei P30 Pro, &#124;'
- en: '&#124; Helios &#124;'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Helios &#124;'
- en: '|'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Indoor, &#124;'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 室内, &#124;'
- en: '&#124; Outdoor &#124;'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 户外 &#124;'
- en: '| Color, Depth | 4811 Images |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| 彩色, 深度 | 4811 张图片 |'
- en: '| Middlebury |  [[133](#bib.bib133)]  [[53](#bib.bib53)]  [[131](#bib.bib131)]
    | 2003 2007 2014 | Structured Light Structured Light Stereo Camera | Indoor |
    Color,Depth | 32 Images |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| Middlebury |  [[133](#bib.bib133)]  [[53](#bib.bib53)]  [[131](#bib.bib131)]
    | 2003 2007 2014 | 结构光 结构光立体相机 | 室内 | 彩色, 深度 | 32 张图片 |'
- en: '| NYUv2 |  [[138](#bib.bib138)] | 2012 | Kinect V1 | Indoor | Color, Depth
    | 1449 Images |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| NYUv2 |  [[138](#bib.bib138)] | 2012 | Kinect V1 | 室内 | 彩色, 深度 | 1449 张图片
    |'
- en: '| MPI Sintel Depth |  [[5](#bib.bib5)] | 2012 | Blender |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| MPI Sintel Depth |  [[5](#bib.bib5)] | 2012 | Blender |'
- en: '&#124; Indoor, &#124;'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 室内, &#124;'
- en: '&#124; Outdoor &#124;'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 户外 &#124;'
- en: '|'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Color, Depth &#124;'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 彩色, 深度 &#124;'
- en: '&#124; Optical Flow &#124;'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 光流 &#124;'
- en: '| 1064 Images |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| 1064 张图片 |'
- en: '| ToF-FT3D |  [[158](#bib.bib158)] | 2022 | synthetic | – | Color, Depth |
    6250 Views |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| ToF-FT3D |  [[158](#bib.bib158)] | 2022 | 合成 | – | 彩色, 深度 | 6250 视图 |'
- en: '| Sparse Depth | KITTI |  [[153](#bib.bib153)] | 2017 | Velodyne, Stereo Camera
    | Outdoor | Color, Depth | 94k Frames |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏深度 | KITTI |  [[153](#bib.bib153)] | 2017 | Velodyne, 立体相机 | 户外 | 彩色, 深度
    | 94k 帧 |'
- en: '| DenseLivox |  [[184](#bib.bib184)] | 2021 |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| DenseLivox |  [[184](#bib.bib184)] | 2021 |'
- en: '&#124; Livox LiDAR, &#124;'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Livox LiDAR, &#124;'
- en: '&#124; RealSense d435i &#124;'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RealSense d435i &#124;'
- en: '|'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Indoor, &#124;'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 室内, &#124;'
- en: '&#124; Outdoor &#124;'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 户外 &#124;'
- en: '| Color, Depth | 6 Scenes |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| 彩色, 深度 | 6 场景 |'
- en: '| KITTI-360 |  [[82](#bib.bib82)] | 2022 | Velodyne, Stereo Camera | Outdoor
    |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| KITTI-360 |  [[82](#bib.bib82)] | 2022 | Velodyne, 立体相机 | 户外 |'
- en: '&#124; Color, Depth, &#124;'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 彩色, 深度, &#124;'
- en: '&#124; GPS, IMU &#124;'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GPS, IMU &#124;'
- en: '| 150 Scenes |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| 150 场景 |'
- en: '| Gibson |  [[173](#bib.bib173)] | 2018 |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| Gibson |  [[173](#bib.bib173)] | 2018 |'
- en: '&#124; NaVis, Matterport Camera, &#124;'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NaVis, Matterport 相机, &#124;'
- en: '&#124; DotProduct &#124;'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DotProduct &#124;'
- en: '| Outdoor | Color, Depth | 572 Scenes |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| 户外 | 彩色, 深度 | 572 场景 |'
- en: '| Virtual KITTI |  [[35](#bib.bib35)] | 2016 | Synthetic | Outdoor | Color,Depth
    | 50 Videos |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| Virtual KITTI |  [[35](#bib.bib35)] | 2016 | 合成 | 户外 | 彩色, 深度 | 50 视频 |'
- en: '| Leddar Pixset |  [[22](#bib.bib22)] | 2021 | Leddar Pixell LiDAR | Outdoor
    |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| Leddar Pixset |  [[22](#bib.bib22)] | 2021 | Leddar Pixell LiDAR | 户外 |'
- en: '&#124; Color, Depth &#124;'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 彩色, 深度 &#124;'
- en: '&#124; IMU, Radar &#124;'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; IMU, 雷达 &#124;'
- en: '| 29k Frames |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| 29k 帧 |'
- en: '| Waymo Perception |  [[146](#bib.bib146)] | 2020 | LiDAR | Outdoor | Color,
    Depth | 1150 Scenes |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| Waymo Perception |  [[146](#bib.bib146)] | 2020 | LiDAR | 户外 | 彩色, 深度 | 1150
    场景 |'
- en: '| DDAD |  [[42](#bib.bib42)] | 2020 | Luminar-H2 LiDAR | Outdoor | Color, Depth
    | 150 Scenes |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| DDAD |  [[42](#bib.bib42)] | 2020 | Luminar-H2 LiDAR | 户外 | 彩色, 深度 | 150
    场景 |'
- en: '| Near-Collision Set |  [[107](#bib.bib107)] | 2019 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| 近碰撞集 |  [[107](#bib.bib107)] | 2019 |'
- en: '&#124; LiDAR (N/A), &#124;'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LiDAR (N/A), &#124;'
- en: '&#124; Stereo Camera &#124;'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 立体相机 &#124;'
- en: '| Indoor | Color, Depth | 13658 Sequences |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| 室内 | 彩色, 深度 | 13658 序列 |'
- en: 'Table 6: RGB guided ToF Datasets, divided according to the depth map properties.'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: RGB 引导的 ToF 数据集，根据深度图属性进行划分。'
- en: 5.1.1 Low-resolution depth
  id: totrans-558
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 低分辨率深度
- en: Here, we mainly introduce datasets composed of low-resolution images collected
    by RGB guided ToF cameras, which can be employed for depth super-resolution and
    depth completion through certain sampling strategies. We also introduce datasets
    that are commonly used in this field, such as Middlebury [[133](#bib.bib133),
    [53](#bib.bib53), [132](#bib.bib132), [131](#bib.bib131)], NYU v2 [[138](#bib.bib138)],
    MPI Sintel Depth [[5](#bib.bib5)] and ToF-FT3D[[158](#bib.bib158)], although being
    collected by other sensors or synthesized.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们主要介绍由 RGB 引导的 ToF 相机收集的低分辨率图像组成的数据集，这些数据集可用于通过某些采样策略进行深度超分辨率和深度完成。我们还介绍了该领域常用的数据集，如
    Middlebury [[133](#bib.bib133), [53](#bib.bib53), [132](#bib.bib132), [131](#bib.bib131)],
    NYU v2 [[138](#bib.bib138)], MPI Sintel Depth [[5](#bib.bib5)] 和 ToF-FT3D[[158](#bib.bib158)]，尽管它们是由其他传感器采集或合成的。
- en: 'ToFMark [[33](#bib.bib33)]: This dataset captures 3 real-world scenes, i.e.,
    Books, Shark and Devil, by PMD Nano ToF camera.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 'ToFMark [[33](#bib.bib33)]: 该数据集通过 PMD Nano ToF 相机捕获了 3 个现实世界场景，即书籍、鲨鱼和魔鬼。'
- en: 'LU [[102](#bib.bib102)]: This dataset consists of 6 RGBD images captured by
    the ASUS Xtion Pro camera, which usually serves as validation or test set.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 'LU [[102](#bib.bib102)]: 该数据集包含 6 张由 ASUS Xtion Pro 相机拍摄的 RGBD 图像，通常用作验证或测试集。'
- en: 'SUN RGBD [[140](#bib.bib140)]: SUN RGBD dataset contains 10 335 indoor image
    pairs acquired by four different sensors. This dataset is often applied to scene
    understanding tasks. Additionally, this dataset can be used to investigate cross-sensor
    biases.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 'SUN RGBD [[140](#bib.bib140)]: SUN RGBD 数据集包含 10,335 对由四种不同传感器获取的室内图像。该数据集通常用于场景理解任务。此外，这个数据集还可以用于研究跨传感器的偏差。'
- en: 'DIML [[12](#bib.bib12)]: DIML is a large-scale RGBD database containing 2M
    RGBD frames. In order to obtain high-precision depth maps, they use Kinect v2
    to capture indoor scenes and ZED stereo camera to capture outdoor scenes. In outdoor
    scenes, confidence maps of disparity are also provided.'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 'DIML [[12](#bib.bib12)]: DIML 是一个大规模 RGBD 数据库，包含 200 万 RGBD 帧。为了获得高精度的深度图，他们使用
    Kinect v2 捕捉室内场景，并使用 ZED 立体相机捕捉户外场景。在户外场景中，还提供了视差的置信度图。'
- en: 'RGBDD [[50](#bib.bib50)]: RGBDD establishes the first depth map SR dataset,
    which can reflect the correspondence between real LR and HR depth maps. This dataset
    consists of 4811 images capturing various scenes, e.g., human body, stuffed dolls,
    toys and plants. Notably, LR and HR depth maps are collected by Huawei P30 Pro
    and Helios ToF camera, respectively.'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 'RGBDD [[50](#bib.bib50)]: RGBDD 建立了第一个深度图 SR 数据集，可以反映实际 LR 和 HR 深度图之间的对应关系。该数据集包含
    4811 张捕捉各种场景的图像，如人体、玩具娃娃、玩具和植物。值得注意的是，LR 和 HR 深度图分别由华为 P30 Pro 和 Helios ToF 相机采集。'
- en: 'Middlebury [[133](#bib.bib133), [53](#bib.bib53), [132](#bib.bib132), [131](#bib.bib131)]:
    The Middlebury dataset is a widely used dataset in the field of GDSR, and consists
    of four sub-datasets from different years. Middlebury 2003[[133](#bib.bib133)],
    Middlebury 2005 [[53](#bib.bib53)], Middlebury 2006 [[132](#bib.bib132)], and
    Middlebury 2012 [[131](#bib.bib131)] are acquired by the stereo camera under indoor
    condition. These datasets use stereo image pairs to generate depth maps.'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 'Middlebury [[133](#bib.bib133), [53](#bib.bib53), [132](#bib.bib132), [131](#bib.bib131)]:
    Middlebury数据集是GDSR领域广泛使用的数据集，由来自不同年份的四个子数据集组成。Middlebury 2003[[133](#bib.bib133)]、Middlebury
    2005 [[53](#bib.bib53)]、Middlebury 2006 [[132](#bib.bib132)]和Middlebury 2012 [[131](#bib.bib131)]是由立体摄像机在室内条件下获取的。这些数据集使用立体图像对生成深度图。'
- en: 'NYUv2 [[138](#bib.bib138)]: This dataset uses Microsoft Kinect v1 camera to
    capture 1449 images. Most previous works take the first 1000 RGBD images as the
    training set, and the rest of the 449 images as the test set. [[18](#bib.bib18),
    [110](#bib.bib110)] randomly selected 849 images for training, 300 for validation,
    and 300 for evaluation.'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 'NYUv2 [[138](#bib.bib138)]: 该数据集使用Microsoft Kinect v1摄像头捕获了1449张图像。大多数先前的工作将前1000张RGBD图像作为训练集，其余449张图像作为测试集。[[18](#bib.bib18),
    [110](#bib.bib110)]随机选择了849张图像用于训练，300张用于验证，300张用于评估。'
- en: 'MPI Sintel Depth [[5](#bib.bib5)]: This dataset originates from an animated
    short film for flow evaluation. It provides synthetic video sequences, including
    35 naturalistic scenes, for which depth maps are provided.'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 'MPI Sintel Depth [[5](#bib.bib5)]: 该数据集来源于用于光流评估的动画短片。它提供了合成的视频序列，包括35个自然场景，并提供了深度图。'
- en: 'ToF-FT3D [[158](#bib.bib158)]: ToF-FT3D, also known as ToF-FlyingThings3D,
    is a synthetic dataset which captures 6250 different views using Blender. Similar
    to the FlyingThings3D[[109](#bib.bib109)] for optical flow estimation, it consists
    of objects that fly along randomized 3D trajectories. Besides, this dataset provides
    ToF amplitude, ToF depth and RGB images with a resolution of $640\times 480$.'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 'ToF-FT3D [[158](#bib.bib158)]: ToF-FT3D，也称为ToF-FlyingThings3D，是一个合成数据集，通过Blender捕获了6250个不同的视角。类似于用于光流估计的FlyingThings3D
    [[109](#bib.bib109)]，它包括沿随机3D轨迹飞行的物体。此外，该数据集提供了分辨率为$640\times 480$的ToF幅度、ToF深度和RGB图像。'
- en: 5.1.2 Sparse depth
  id: totrans-569
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 稀疏深度
- en: In practical applications, sparse depth maps are usually produced by LiDAR and
    only contain about $4\%$ valid pixels[[153](#bib.bib153)]. Under some extreme
    conditions, even less than $1\%$ of the pixels are effectively measured[[43](#bib.bib43),
    [41](#bib.bib41)]. Although sparse depth maps can be obtained from dense depth
    maps with sampling strategies, here we focus on datasets collected by LiDAR.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，稀疏深度图通常由LiDAR生成，仅包含约$4\%$的有效像素[[153](#bib.bib153)]。在一些极端条件下，甚至不到$1\%$的像素被有效测量[[43](#bib.bib43),
    [41](#bib.bib41)]。虽然可以通过采样策略从密集深度图中获得稀疏深度图，但这里我们关注的是由LiDAR收集的数据集。
- en: 'KITTI [[153](#bib.bib153)]: KITTI is a dataset initially gathered by the Karlsruhe
    Institute of Technology (KIT) and Toyota Technological Institute at Chicago (TTI-C).
    As one of the most important datasets in the field of autonomous driving, it captures
    more than 93K depth maps with corresponding raw LiDAR scans and RGB images. The
    LiDAR and two color cameras used for data acquisition are Velodyne HDL-64E and
    Point Grey Flea 2 (FL2-14S3C-C), respectively. Based on the KITTI raw dataset,
    [[153](#bib.bib153)] remove noise and artifacts in the scenes and aggregates multiple
    LiDAR scans over time to obtain denser ground truth depths, making this dataset
    suitable for GDC. There are 86 000, 7 000, and 1 000 image pairs for training,
    validation, and evaluation, respectively.'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 'KITTI [[153](#bib.bib153)]: KITTI是由卡尔斯鲁厄理工学院（KIT）和芝加哥丰田技术研究所（TTI-C）最初收集的数据集。作为自动驾驶领域最重要的数据集之一，它捕获了超过93K的深度图，并附有相应的原始LiDAR扫描和RGB图像。用于数据采集的LiDAR和两台彩色摄像头分别是Velodyne
    HDL-64E和Point Grey Flea 2（FL2-14S3C）。基于KITTI原始数据集，[[153](#bib.bib153)]去除了场景中的噪声和伪影，并聚合了多个LiDAR扫描以获得更密集的真实深度，使该数据集适合用于GDC。该数据集中有86,000、7,000和1,000对图像分别用于训练、验证和评估。'
- en: 'Virtual KITTI [[35](#bib.bib35)]: This dataset is a photo-realistic synthetic
    video dataset that contains 50 videos with a total of 21 260 frames. Scenes in
    the dataset are generated with the Unity game engine and a real-to-virtual cloning
    method. It can be applied to a variety of tasks, such as object detection, object
    tracking, instance semantic segmentation, and more.'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 'Virtual KITTI [[35](#bib.bib35)]: 该数据集是一个逼真的合成视频数据集，包含50个视频，总共有21,260帧。数据集中的场景使用Unity游戏引擎和真实到虚拟的克隆方法生成。它可以应用于多种任务，例如物体检测、物体跟踪、实例语义分割等。'
- en: 'KITTI-360 [[82](#bib.bib82)]: KITTI-360 is a newly collected large-scale dataset
    with rich sensory information and full annotations. This dataset captures over
    320k images and 100k laser scans, including annotated static and dynamic 3D scene
    elements.'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 'KITTI-360 [[82](#bib.bib82)]: KITTI-360是一个新收集的大规模数据集，具有丰富的传感器信息和完整的标注。该数据集捕获了超过32万张图像和10万次激光扫描，包括标注的静态和动态3D场景元素。'
- en: 'Gibson [[173](#bib.bib173)]: Gibson contains 572 full buildings with 1447 floors
    covering a total area of 211k $m^{2}$. In addition to depth maps and the corresponding
    color maps, it provides normal maps and semantic object annotations.'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 'Gibson [[173](#bib.bib173)]: Gibson包含572栋完整建筑，共1447层，总面积为211,000 $m^{2}$。除了深度图和相应的彩色图外，它还提供了法线图和语义物体标注。'
- en: 'DenseLivox [[184](#bib.bib184)]: This dataset collects the images with a Livox
    Horizon LiDAR and Intel RalSense D435i camera. It contains dense, accurate depth
    as ground truth, with up to $88.3\%$ valid pixel coverage.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 'DenseLivox [[184](#bib.bib184)]: 该数据集使用Livox Horizon LiDAR和Intel RalSense D435i相机采集图像。它包含密集且准确的深度作为真实值，具有高达$88.3\%$的有效像素覆盖率。'
- en: 'Leddar Pixset [[22](#bib.bib22)]: Leddar Pixset is the first full-waveform
    flash LiDAR dataset for autonomous driving. It contains 29 000 frames in 97 sequences
    of various environments, weather conditions, and periods, annotated with more
    than 1.3 million 3D bounding boxes.'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 'Leddar Pixset [[22](#bib.bib22)]: Leddar Pixset是第一个用于自动驾驶的全波形闪光LiDAR数据集。它包含29,000帧，涵盖97个不同环境、天气条件和时间段的序列，并标注了超过130万的3D边界框。'
- en: 'Waymo Perception [[146](#bib.bib146)]: This dataset is collected by five LiDAR
    sensors and five high-resolution pinhole cameras, all synchronized and calibrated.
    It captures 1150 scenes, including urban and suburban areas, each lasting 20 seconds.
    Typically, 1000 scenes are used for training and 150 for testing.'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 'Waymo Perception [[146](#bib.bib146)]: 该数据集由五个LiDAR传感器和五个高分辨率针孔相机收集，所有传感器都经过同步和校准。它捕获了1150个场景，包括城市和郊区区域，每个场景持续20秒。通常，1000个场景用于训练，150个场景用于测试。'
- en: 'DDAD [[42](#bib.bib42)]: DDAD is a new benchmark dataset for autonomous driving
    from Toyota Research Institute, which consists of monocular videos and accurate
    ground-truth depth. It has a long measurable distance, which can reach up to 250m.
    There are 150 scenes with 12 650 individual image pairs in the training set, 50
    scenes with 3 950 image pairs in the validation set, and 3 080 images in the test
    set.'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 'DDAD [[42](#bib.bib42)]: DDAD是丰田研究所推出的自动驾驶新基准数据集，包含单目视频和准确的真实深度数据。它具有较长的可测量距离，最长可达250米。训练集中有150个场景，共12,650对图像，验证集中有50个场景，共3,950对图像，测试集中有3,080张图像。'
- en: 'Near-Collision Set [[107](#bib.bib107)]: Near-Collision Set is a large-scale,
    real-world dataset for near-collision prediction, collected with a stereo camera
    and LiDAR sensor. In this dataset, color images, accurate depth, and human pose
    annotations are provided. It contains 13 658 egocentric video snippets of humans
    navigating in indoor hallways.'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 'Near-Collision Set [[107](#bib.bib107)]: Near-Collision Set是一个大规模的真实世界数据集，用于近碰撞预测，使用立体相机和LiDAR传感器收集。在此数据集中，提供了彩色图像、准确的深度和人体姿态标注。它包含13,658段人在室内走廊导航的自视视频片段。'
- en: 'To conclude, Tab. [6](#S5.T6 "Table 6 ‣ 5.1 Datasets ‣ 5 Benchmark Datasets
    and objective functions ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods") lists the main datasets we discussed so far, categorizing them according
    to the input depth map being LR or sparse, as well as reporting the year of collection,
    the sensor used, the capture environment, the sensed modalities and the number
    of images.'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '总之，表[6](#S5.T6 "Table 6 ‣ 5.1 Datasets ‣ 5 Benchmark Datasets and objective
    functions ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods")列出了我们迄今讨论的主要数据集，根据输入深度图是否为LR或稀疏进行分类，同时报告了数据收集年份、使用的传感器、采集环境、感知的模态和图像数量。'
- en: 5.2 Objective Functions.
  id: totrans-581
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 目标函数。
- en: Notably, the performance of a trained model is directly related to the choice
    of loss functions, as they estimate the corrections to be applied to weights in
    the network during training. In this section, we discuss general loss functions
    for RGB guided ToF imaging, along with loss functions that are frequently employed
    for the two sub-tasks of GDSR and GDC, respectively.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，训练模型的性能与损失函数的选择直接相关，因为损失函数估计了训练过程中网络权重需要应用的修正。在本节中，我们将讨论 RGB 引导 ToF 成像的一般损失函数，以及
    GDSR 和 GDC 两个子任务中常用的损失函数。
- en: 5.2.1 General losses
  id: totrans-583
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 一般损失
- en: In RGB guided ToF imaging, the most frequently utilized losses are mean absolute
    error (MAE), mean square error (MSE), and root mean square error (RMSE). MAE is
    used in many works [[60](#bib.bib60), [137](#bib.bib137), [151](#bib.bib151),
    [149](#bib.bib149)], also referred to as $l_{1}$ loss
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RGB 引导的 ToF 成像中，最常用的损失函数是平均绝对误差 (MAE)、均方误差 (MSE) 和均方根误差 (RMSE)。MAE 在许多工作中使用[[60](#bib.bib60),
    [137](#bib.bib137), [151](#bib.bib151), [149](#bib.bib149)]，也称为 $l_{1}$ 损失
- en: '|  | $\mathrm{MAE}=\frac{1}{N}\sum_{p\in N}\parallel Z_{hq}^{p}-\hat{Z}_{hq}^{p}\parallel_{1}$
    |  | (25) |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{MAE}=\frac{1}{N}\sum_{p\in N}\parallel Z_{hq}^{p}-\hat{Z}_{hq}^{p}\parallel_{1}$
    |  | (25) |'
- en: where $\parallel\cdot\parallel_{1}$ means the $l_{1}$ norm and $N$ denotes the
    total number of valid pixels in depth maps. Some methods[[106](#bib.bib106), [196](#bib.bib196)]
    adopted MSE ($l_{2}$ loss) which is defined as
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\parallel\cdot\parallel_{1}$ 表示 $l_{1}$ 范数，$N$ 表示深度图中有效像素的总数。一些方法[[106](#bib.bib106),
    [196](#bib.bib196)] 采用 MSE ($l_{2}$ 损失)，其定义为
- en: '|  | $\mathrm{MSE}=\frac{1}{N}\sum_{p\in N}\parallel Z_{hq}^{p}-\hat{Z}_{hq}^{p}\parallel_{2}$
    |  | (26) |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{MSE}=\frac{1}{N}\sum_{p\in N}\parallel Z_{hq}^{p}-\hat{Z}_{hq}^{p}\parallel_{2}$
    |  | (26) |'
- en: 'where $\parallel\cdot\parallel_{2}$ represents the $l_{2}$ norm. Another common
    quantity is RMSE, i.e., the square root of MSE:'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\parallel\cdot\parallel_{2}$ 表示 $l_{2}$ 范数。另一个常见的量是 RMSE，即 MSE 的平方根：
- en: '|  | $\mathrm{RMSE}=\sqrt{\mathrm{MSE}}$ |  | (27) |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{RMSE}=\sqrt{\mathrm{MSE}}$ |  | (27) |'
- en: 'As a perceptual metric for image quality assessment, the Structural Similarity
    Index (SSIM) is based on the visible structures and is defined as shown in Eq.
    [19](#S2.E19 "In 2.3 Evaluation metrics ‣ 2 Preliminaries and Taxonomy ‣ RGB Guided
    ToF Imaging System: A Survey of Deep Learning-based Methods").'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 作为图像质量评估的感知度量，结构相似性指数 (SSIM) 基于可见结构，其定义如方程 [19](#S2.E19 "在 2.3 评估指标 ‣ 2 基础知识和分类
    ‣ RGB 引导 ToF 成像系统：基于深度学习的方法综述") 所示。
- en: 5.2.2 Losses for GDSR.
  id: totrans-591
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 GDSR 的损失函数。
- en: 'To achieve more robust training, a few approaches [[92](#bib.bib92)] use the
    Charbonnier loss[[7](#bib.bib7)]:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现更强的训练，某些方法[[92](#bib.bib92)] 使用 Charbonnier 损失[[7](#bib.bib7)]：
- en: '|  | $l_{cha}=\sqrt{\parallel Z_{hq}^{p}-\hat{Z}_{hq}^{p}\parallel^{2}+\epsilon^{2}}$
    |  | (28) |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '|  | $l_{cha}=\sqrt{\parallel Z_{hq}^{p}-\hat{Z}_{hq}^{p}\parallel^{2}+\epsilon^{2}}$
    |  | (28) |'
- en: 'where $\epsilon$ denotes a constant that ensures the penalty is non-zero. Sometimes,
    Peak Signal-to-Noise Ratio (PNSR) is also utilized for GDSR. PSNR can be formulated
    as follows:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\epsilon$ 表示一个常数，以确保惩罚项非零。有时，峰值信噪比 (PNSR) 也用于 GDSR。PSNR 可以表述为：
- en: '|  | $\mathrm{PSNR}=10log_{10}(\frac{Z_{max}^{2}}{\mathrm{MSE}})$ |  | (29)
    |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{PSNR}=10\log_{10}(\frac{Z_{max}^{2}}{\mathrm{MSE}})$ |  | (29)
    |'
- en: where $Z_{max}$ is the maximum depth value. The smaller the pixel value difference
    between the two depth maps, the higher the PSNR.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Z_{max}$ 是最大深度值。两个深度图之间的像素值差异越小，PSNR 越高。
- en: 5.2.3 Losses for GDC.
  id: totrans-597
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 GDC 的损失函数。
- en: 'Depth loss. During training, since MAE treats all errors equally and MSE emphasizes
    the outliers, Huber loss combines the advantages of both:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 深度损失。在训练过程中，由于 MAE 平等对待所有误差，而 MSE 强调异常值，Huber 损失结合了两者的优点：
- en: '|  | $l_{huber}=\begin{cases}\frac{1}{N}\sum_{p\in N}\frac{1}{2}(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2},&amp;&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;\leq
    1\\ \frac{1}{N}\sum_{p\in N}(&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;-\frac{1}{2}),&amp;&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;>1\end{cases}$
    |  | (30) |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '|  | $l_{huber}=\begin{cases}\frac{1}{N}\sum_{p\in N}\frac{1}{2}(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2},&amp;\vert
    Z_{hq}^{p}-\hat{Z}_{hq}^{p}\vert\leq 1\\ \frac{1}{N}\sum_{p\in N}(\vert Z_{hq}^{p}-\hat{Z}_{hq}^{p}\vert-\frac{1}{2}),&amp;\vert
    Z_{hq}^{p}-\hat{Z}_{hq}^{p}\vert>1\end{cases}$ |  | (30) |'
- en: 'where $|\cdot|$ is the operator for absolute value. In[[154](#bib.bib154)],
    another loss named Focal-MSE, inspired by [[83](#bib.bib83)], is proposed and
    proved to be better than MSE, which is formulated as:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $|\cdot|$ 是绝对值运算符。在[[154](#bib.bib154)] 中，提出了一种名为 Focal-MSE 的损失函数，受到[[83](#bib.bib83)]
    启发，并证明优于 MSE，其公式为：
- en: '|  | $l_{focal}=\frac{1}{N}\sum_{p\in N}(1+\frac{\mathrm{epoch}}{20}&#124;Z_{hq}^{p}-\hat{Z}_{hq}^{p}&#124;)\cdot(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2}$
    |  | (31) |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '|  | $l_{focal}=\frac{1}{N}\sum_{p\in N}(1+\frac{\mathrm{epoch}}{20}\,\vert
    Z_{hq}^{p}-\hat{Z}_{hq}^{p}\vert)\cdot(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2}$ |  |
    (31) |'
- en: Uncertainty-driven loss. Considering the uneven distribution in the captured
    depth maps from LiDAR, the uncertainty-driven loss is introduced[[177](#bib.bib177),
    [27](#bib.bib27), [205](#bib.bib205)] to focus on more reliable pixels. In their
    works, GDC is defined as maximizing the posterior probability. The final optimized
    loss with Jeffrey’s prior[[34](#bib.bib34)] can be written as
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性驱动的损失。考虑到从LiDAR捕获的深度图的分布不均，不确定性驱动的损失被引入[[177](#bib.bib177), [27](#bib.bib27),
    [205](#bib.bib205)]，以关注更可靠的像素。在他们的工作中，GDC 被定义为最大化后验概率。最终优化的损失与Jeffrey’s prior[[34](#bib.bib34)]
    可以写为：
- en: '|  | $l_{ud}=\frac{1}{N}\sum(e^{-s_{n}}(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2}+2s_{p})$
    |  | (32) |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '|  | $l_{ud}=\frac{1}{N}\sum(e^{-s_{n}}(Z_{hq}^{p}-\hat{Z}_{hq}^{p})^{2}+2s_{p})$
    |  | (32) |'
- en: where $s_{n}$ stands for prediction uncertainty at the $p^{th}$ pixel.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s_{n}$ 代表第 $p^{th}$ 像素的预测不确定性。
- en: 'Adversarial loss. For method adopting a generative adversarial learning strategy[[67](#bib.bib67),
    [157](#bib.bib157), [112](#bib.bib112)] to perform depth completion, the adversarial
    loss is needed to discriminate between real and fake images, which can be written
    as:'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗损失。对于采用生成对抗学习策略[[67](#bib.bib67), [157](#bib.bib157), [112](#bib.bib112)]
    来进行深度补全的方法，需要对抗损失来区分真实和虚假图像，其表达式为：
- en: '|  | $l_{adv}\!=\!\min_{G}\max_{D}\mathbb{E}[\log D(Z_{hq})]\!+\!\mathbb{E}[\log(1-D(G(Z_{lq},I)))]$
    |  | (33) |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '|  | $l_{adv}\!=\!\min_{G}\max_{D}\mathbb{E}[\log D(Z_{hq})]\!+\!\mathbb{E}[\log(1-D(G(Z_{lq},I)))]$
    |  | (33) |'
- en: where $G(\cdot)$ and $D(\cdot)$ denote the generator and discriminator, respectively.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G(\cdot)$ 和 $D(\cdot)$ 分别表示生成器和判别器。
- en: 'Normal constraint. To further improve depth accuracy, surface normal constraints
    have been exploited in several works[[190](#bib.bib190), [71](#bib.bib71), [177](#bib.bib177)].
    [[190](#bib.bib190), [71](#bib.bib71)] compute the depth-normal consistency loss
    between the predicted normal $R(p)$ at pixel $p$ and the estimated normal $v(p,q)$
    from the depth map, as follows:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 法线约束。为了进一步提高深度准确性，几项工作中已经利用了表面法线约束[[190](#bib.bib190), [71](#bib.bib71), [177](#bib.bib177)]。[[190](#bib.bib190),
    [71](#bib.bib71)] 计算了像素 $p$ 处的预测法线 $R(p)$ 与深度图中从深度图估计的法线 $v(p,q)$ 之间的深度-法线一致性损失，如下所示：
- en: '|  | $l_{n1}=\sum_{p,q\in N}\parallel<v(p,q),R(p)>\parallel^{2}$ |  | (34)
    |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '|  | $l_{n1}=\sum_{p,q\in N}\parallel<v(p,q),R(p)>\parallel^{2}$ |  | (34)
    |'
- en: 'where $q$ is a neighbour of pixel $p$, and $<\!\!\cdot\!\!>$ denotes the inner
    product. To enable models to be trained in an end-to-end fashion, [[177](#bib.bib177)]
    propose a negative cosine loss inspired by[[26](#bib.bib26)], which is defined
    as:'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q$ 是像素 $p$ 的邻居，$<\!\!\cdot\!\!>$ 表示内积。为了使模型能够以端到端的方式进行训练，[[177](#bib.bib177)]
    提出了一个受到[[26](#bib.bib26)]启发的负余弦损失，其定义为：
- en: '|  | $l_{n2}=-\frac{1}{N}\sum_{p\in N}R(p)\cdot\hat{R}(q)$ |  | (35) |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '|  | $l_{n2}=-\frac{1}{N}\sum_{p\in N}R(p)\cdot\hat{R}(q)$ |  | (35) |'
- en: where $\hat{R}(\cdot)$ is the computed normal vector.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{R}(\cdot)$ 是计算出的法向量。
- en: 'Smoothness term. Based on the assumption of local smoothness, the smoothness
    term is introduced in many works[[105](#bib.bib105), [137](#bib.bib137)] to avoid
    local optimal solutions, penalizing gradients along $x-$ and $y-$ directions.
    In order to maintain the discontinuities in the final depth maps, some methods[[169](#bib.bib169),
    [170](#bib.bib170), [14](#bib.bib14), [142](#bib.bib142), [130](#bib.bib130),
    [168](#bib.bib168)] weight the gradients maps according to the local properties
    in RGB images:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑项。基于局部平滑的假设，许多工作中引入了平滑项[[105](#bib.bib105), [137](#bib.bib137)]，以避免局部最优解，对
    $x-$ 和 $y-$ 方向上的梯度进行惩罚。为了在最终深度图中保持不连续性，一些方法[[169](#bib.bib169), [170](#bib.bib170),
    [14](#bib.bib14), [142](#bib.bib142), [130](#bib.bib130), [168](#bib.bib168)]根据RGB图像中的局部特性对梯度图进行加权：
- en: '|  | $l_{sm}=\frac{1}{N}\sum_{p\in N}\lambda_{x}(p)&#124;\partial_{x}\hat{Z}_{hq}^{p}&#124;+\lambda_{y}(p)&#124;\partial_{y}\hat{Z}_{hq}^{p}&#124;$
    |  | (36) |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '|  | $l_{sm}=\frac{1}{N}\sum_{p\in N}\lambda_{x}(p)\,\vert\partial_{x}\hat{Z}_{hq}^{p}\vert+\lambda_{y}(p)\,\vert\partial_{y}\hat{Z}_{hq}^{p}\vert$
    |  | (36) |'
- en: where $\lambda_{x}(p)=e^{-|\partial_{x}I^{p}|}$ and $\lambda_{y}(p)=e^{-|\partial_{y}I^{p}|}$.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{x}(p)=e^{-|\partial_{x}I^{p}|}$ 和 $\lambda_{y}(p)=e^{-|\partial_{y}I^{p}|}$。
- en: 'Photometric consistency. In unsupervised methods for GDC, photometric consistency
    based on the assumption of local smoothness is usually used to refine depth estimation.
    Given a reference image $I_{t}$ and its neighboring frame $I_{\tau}$ along the
    temporal dimension where $\tau\in T=\{t-1,t+1\}$, the goal is to estimate the
    difference between $I_{t}$ and its reconstructed version from $I_{\tau}$ as follows:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 光度一致性。在GDC的无监督方法中，基于局部平滑假设的光度一致性通常用于精炼深度估计。给定参考图像$I_{t}$及其在时间维度上的邻近帧$I_{\tau}$（其中$\tau\in
    T=\{t-1,t+1\}$），目标是估计$I_{t}$与其从$I_{\tau}$重建的版本之间的差异，如下所示：
- en: '|  | $\hat{I}_{\tau}(x,\hat{Z}_{hq})=I_{\tau}(\pi g_{\tau t}K^{-1}\overline{x}\hat{Z}_{hq}(x))$
    |  | (37) |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{I}_{\tau}(x,\hat{Z}_{hq})=I_{\tau}(\pi g_{\tau t}K^{-1}\overline{x}\hat{Z}_{hq}(x))$
    |  | (37) |'
- en: where $\overline{x}=[x^{T}1]^{T}$ are the homogeneous coordinates $x\subset\Omega$,
    $\pi$ denotes the perspective projection, $g_{\tau t}$ is the relative pose of
    the camera from time $t$ to $\tau$, $K$ represents the camera intrinsic matrix,
    and $\hat{Z}_{hq}(x)$ denotes the predicted depth of $x$. The relative pose can
    be either known or estimated by a network.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\overline{x}=[x^{T}1]^{T}$是齐次坐标$x\subset\Omega$，$\pi$表示透视投影，$g_{\tau t}$是从时间$t$到$\tau$的相对姿态，$K$表示相机内参矩阵，$\hat{Z}_{hq}(x)$表示$x$的预测深度。相对姿态可以是已知的，也可以由网络估计得到。
- en: 'The photometric consistency is constructed by the combination of $L1$ loss
    and SSIM on the photometric reprojection as:'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 光度一致性通过在光度重投影上结合$L1$损失和SSIM来构建，如下所示：
- en: '|  | $\displaystyle l_{ph}=$ | $\displaystyle\frac{1}{&#124;\Omega&#124;}\sum_{\tau\in
    T}\sum_{x\in\Omega}\omega_{co}&#124;I_{t}(x)-\hat{I}_{\tau}(x)&#124;+$ |  |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle l_{ph}=$ | $\displaystyle\frac{1}{\lvert\Omega\rvert}\sum_{\tau\in
    T}\sum_{x\in\Omega}\omega_{co}\lvert I_{t}(x)-\hat{I}_{\tau}(x)\lvert+$ |  |'
- en: '|  |  | $\displaystyle\omega_{st}(1-\mathrm{SSIM}(I_{t}(x),\hat{I}_{\tau}(x)))$
    |  | (38) |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\omega_{st}(1-\mathrm{SSIM}(I_{t}(x),\hat{I}_{\tau}(x)))$
    |  | (38) |'
- en: where $\omega_{co}$ and $\omega_{st}$ are weights to balance the two terms.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\omega_{co}$和$\omega_{st}$是用于平衡这两个项的权重。
- en: 'Pose consistency. Given a pair of images ${I_{t},I_{\tau}}$ as input, a pose
    network predicts the relative pose $g_{\tau t}\in SE(3)$. When the input of the
    image sequence is reversed, we expect to obtain $g_{t}\tau$. The pose consistency
    loss is formulated as:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态一致性。给定一对图像${I_{t},I_{\tau}}$作为输入，姿态网络预测相对姿态$g_{\tau t}\in SE(3)$。当图像序列的输入被反转时，我们期望获得$g_{t}\tau$。姿态一致性损失的公式为：
- en: '|  | $l_{pc}=\parallel\log(g_{\tau t}\cdot g_{t\tau})\parallel^{2}_{2}$ |  |
    (39) |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '|  | $l_{pc}=\parallel\log(g_{\tau t}\cdot g_{t\tau})\parallel^{2}_{2}$ |  |
    (39) |'
- en: where $\log:SE(3)\to se(3)$ is the logarithmic map.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\log:SE(3)\to se(3)$是对数映射。
- en: 'Others. Finally, we review other commonly used loss functions in Tab.[7](#S5.T7
    "Table 7 ‣ 5.2.3 Losses for GDC. ‣ 5.2 Objective Functions. ‣ 5 Benchmark Datasets
    and objective functions ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"), including depth loss, structural loss, and others.'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '其他。最后，我们回顾了表[7](#S5.T7 "Table 7 ‣ 5.2.3 Losses for GDC. ‣ 5.2 Objective Functions.
    ‣ 5 Benchmark Datasets and objective functions ‣ RGB Guided ToF Imaging System:
    A Survey of Deep Learning-based Methods")中其他常用的损失函数，包括深度损失、结构损失等。'
- en: '| Types | Notation | Purpose |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 符号 | 目的 |'
- en: '| --- | --- | --- |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Depth loss | $l_{berhu}$ | Berhu loss is a reversion of Huber loss $l_{huber}$.
    |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '| 深度损失 | $l_{berhu}$ | Berhu损失是Huber损失$l_{huber}$的变体。 |'
- en: '| $l_{ce}$ | Cross-entropy is to measure the validaty of pixels. |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| $l_{ce}$ | 交叉熵用于测量像素的有效性。 |'
- en: '| $l_{cyc}$ | Two $l_{1}$ losses for cycle consistency. |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| $l_{cyc}$ | 两个$l_{1}$损失用于循环一致性。 |'
- en: '| Structural loss | $l_{grad}$ | Gradient between the prediction and GT without
    weights. |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '| 结构损失 | $l_{grad}$ | 预测与GT之间的梯度，没有权重。 |'
- en: '| Others | $l_{cpn}$ | RMSE between the prediction and its reconstruction from
    CPN. |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | $l_{cpn}$ | 预测与其从CPN重建的RMSE。 |'
- en: '| $l_{cos}$ | Cosine similarity measures the similarity between the prediction
    and GT. |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| $l_{cos}$ | 余弦相似性测量预测与GT之间的相似性。 |'
- en: '| $l_{tp}$ | MAE between the initial depth and the prediction. |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| $l_{tp}$ | 初始深度与预测之间的MAE。 |'
- en: 'Table 7: Main loss terms deployed for GDSR and GDC.'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：用于GDSR和GDC的主要损失项。
- en: 6 Evaluation
  id: totrans-637
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 评估
- en: In this section, we compare the performance of the state-of-the-art GDSR and
    GDC approaches on widely used benchmarks. We first present the quantitative results
    for the two categories of RGB guided ToF imaging. Then, the pros and cons of the
    methods are analyzed. Notably, we focus more on the most recent methods for RGB
    guided ToF imaging.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们比较了最新的 GDSR 和 GDC 方法在广泛使用的基准上的性能。我们首先呈现了这两类 RGB 引导的 ToF 成像的定量结果。然后，分析这些方法的优缺点。特别地，我们更加关注
    RGB 引导的 ToF 成像的最新方法。
- en: 6.1 Comparison of GDSR methods
  id: totrans-639
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 GDSR 方法的比较
- en: We select representative GDSR methods for the quantitative comparison on Middlebury,
    NYUv2, RGBDD, and DIML datasets with scaling factors of $4\times,8\times$, and
    $16\times$. For all the datasets, we follow the recent literature [[149](#bib.bib149),
    [196](#bib.bib196), [186](#bib.bib186), [198](#bib.bib198)] to assess the depth
    quality according to the RMSE metric.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了代表性的 GDSR 方法，在 Middlebury、NYUv2、RGBDD 和 DIML 数据集上进行定量比较，缩放因子为 $4\times,8\times$
    和 $16\times$。对于所有数据集，我们遵循近期文献 [[149](#bib.bib149), [196](#bib.bib196), [186](#bib.bib186),
    [198](#bib.bib198)] 根据 RMSE 指标评估深度质量。
- en: 'Tab. [8](#S6.T8 "Table 8 ‣ 6.1 Comparison of GDSR methods ‣ 6 Evaluation ‣
    RGB Guided ToF Imaging System: A Survey of Deep Learning-based Methods") shows
    the comparison of the state-of-the-art GDSR methods. In this experiment, bicubic
    interpolation is set as the baseline method. From this table, we can see that
    although the early deep learning methods[[58](#bib.bib58), [78](#bib.bib78)] are
    designed with simple structures, they show more powerful performance than traditional
    methods. Using an encoder-decoder architecture, DJFR[[79](#bib.bib79)], DepthSR [[44](#bib.bib44)]
    and PAC[[143](#bib.bib143)] design two sub-networks that extract RGB and depth
    features respectively, and attain better performance compared to models built
    by several conventional CNN layers. In order to improve the representation capability
    of neural networks, PMBANet[[183](#bib.bib183)] design a complex structure and
    further boost the GDSR performance. Using a lightweight structure, FDSR[[50](#bib.bib50)]
    can still produce accurate results. As an alternative solution, the kernel prediction
    network (KPN) employed by DKN[[68](#bib.bib68)] and FDKN also achieves promising
    results. Further, [[198](#bib.bib198)] employ the KPN to handle inconsistent structures
    between RGB and depth, which improves generalization to real scenes. CUNet[[21](#bib.bib21)]
    develops an optimization-based method and yields results comparable to KPNs. Inspired
    by these optimization-based methods, DCTNet[[196](#bib.bib196)] and LGR[[18](#bib.bib18)]
    attain better performance. LGR[[18](#bib.bib18)], in particular, achieves the
    state-of-the-art on Middlebury and DIML datasets. Unlike most methods that can
    only perform upsampling at fixed integer scales, GeoDSR [[161](#bib.bib161)] learns
    a continuous representation and achieves strong performance in arbitrary scale
    depth super-resolution on the Middlebury dataset. With an effective multimodal
    feature fusion strategy, DSR-EI[[120](#bib.bib120)] attains the best results.
    However, the disadvantage is that it brings a large computational burden.'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [8](#S6.T8 "Table 8 ‣ 6.1 Comparison of GDSR methods ‣ 6 Evaluation ‣ RGB
    Guided ToF Imaging System: A Survey of Deep Learning-based Methods") 显示了最新的 GDSR
    方法的比较。在本实验中，双三次插值被设为基准方法。从该表中可以看出，尽管早期的深度学习方法[[58](#bib.bib58), [78](#bib.bib78)]
    设计简单，但它们的性能比传统方法更强大。通过使用编码器-解码器架构，DJFR[[79](#bib.bib79)]、DepthSR [[44](#bib.bib44)]
    和 PAC[[143](#bib.bib143)] 设计了两个子网络，分别提取 RGB 和深度特征，并且比由多个传统 CNN 层构建的模型表现更好。为了提高神经网络的表示能力，PMBANet[[183](#bib.bib183)]
    设计了复杂的结构，并进一步提升了 GDSR 性能。使用轻量级结构，FDSR[[50](#bib.bib50)] 仍能产生准确的结果。作为替代方案，DKN[[68](#bib.bib68)]
    和 FDKN 使用的核预测网络 (KPN) 也取得了有希望的结果。此外，[[198](#bib.bib198)] 使用 KPN 处理 RGB 和深度之间的不一致结构，从而提高了对真实场景的泛化能力。CUNet[[21](#bib.bib21)]
    发展了一种基于优化的方法，结果与 KPNs 相当。受这些基于优化的方法启发，DCTNet[[196](#bib.bib196)] 和 LGR[[18](#bib.bib18)]
    取得了更好的性能。特别是 LGR[[18](#bib.bib18)] 在 Middlebury 和 DIML 数据集上达到了最新的技术水平。与大多数只能在固定整数尺度上进行上采样的方法不同，GeoDSR
    [[161](#bib.bib161)] 学习了连续表示，并在 Middlebury 数据集上的任意尺度深度超分辨率中表现出色。凭借有效的多模态特征融合策略，DSR-EI[[120](#bib.bib120)]
    达到了最佳结果。然而，缺点是它带来了较大的计算负担。'
- en: From another perspective, most state-of-the-art methods adopt multi-scale or
    coarse-to-fine strategies, such as SFG [[187](#bib.bib187)], JGF [[160](#bib.bib160)],
    and RSAG [[186](#bib.bib186)]. On the one hand, these strategies have achieved
    impressive results in other tasks, showing their effectiveness in learning feature
    representations. On the other hand, some methods, such as DCTNet [[196](#bib.bib196)],
    LGR [[18](#bib.bib18)], DADA [[110](#bib.bib110)] and SSDNet[[195](#bib.bib195)],
    combine classic optimization algorithms with modern CNNs and demonstrate strong
    performance. Nonetheless, these models cannot be directly applied to many real
    scenarios, especially on mobile devices. Hence, efficient and effective approaches
    need to be further investigated.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一个角度来看，大多数最先进的方法采用了多尺度或粗到细的策略，如SFG [[187](#bib.bib187)]、JGF [[160](#bib.bib160)]和RSAG [[186](#bib.bib186)]。这些策略在其他任务中取得了显著的结果，展示了其在学习特征表示方面的有效性。另一方面，一些方法，如DCTNet [[196](#bib.bib196)]、LGR [[18](#bib.bib18)]、DADA [[110](#bib.bib110)]和SSDNet[[195](#bib.bib195)]，将经典的优化算法与现代CNN结合，展示了强劲的性能。然而，这些模型无法直接应用于许多实际场景，特别是在移动设备上。因此，需要进一步研究高效且有效的方法。
- en: '| Dataset | Middlebury | NYUv2 | RGBDD | DIML |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | Middlebury | NYUv2 | RGBDD | DIML |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Methods | $4\times$ | $8\times$ | $16\times$ | $4\times$ | $8\times$ | $16\times$
    | $4\times$ | $8\times$ | $16\times$ | $4\times$ | $8\times$ | $16\times$ |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | $4\times$ | $8\times$ | $16\times$ | $4\times$ | $8\times$ | $16\times$
    | $4\times$ | $8\times$ | $16\times$ | $4\times$ | $8\times$ | $16\times$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| Bicubic | 2.28 | 3.96 | 6.37 | 4.28 | 7.14 | 11.58 | 2.75 | 4.47 | 6.98 |
    1.92 | 3.20 | 5.14 |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| 双三次插值 | 2.28 | 3.96 | 6.37 | 4.28 | 7.14 | 11.58 | 2.75 | 4.47 | 6.98 | 1.92
    | 3.20 | 5.14 |'
- en: '| GF [[48](#bib.bib48)] | 2.49 | 3.98 | 6.08 | 5.05 | 6.97 | 11.1 | 2.72 |
    4.02 | 6.68 | 2.72 | 3.40 | 5.13 |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| GF [[48](#bib.bib48)] | 2.49 | 3.98 | 6.08 | 5.05 | 6.97 | 11.1 | 2.72 |
    4.02 | 6.68 | 2.72 | 3.40 | 5.13 |'
- en: '| DMSG [[58](#bib.bib58)] | 2.11 | 3.74 | 6.03 | 3.02 | 5.38 | 9.17 | 1.80
    | 3.04 | 5.10 | 1.39 | 2.34 | 4.02 |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| DMSG [[58](#bib.bib58)] | 2.11 | 3.74 | 6.03 | 3.02 | 5.38 | 9.17 | 1.80
    | 3.04 | 5.10 | 1.39 | 2.34 | 4.02 |'
- en: '| DJF [[78](#bib.bib78)] | 1.68 | 3.24 | 5.62 | 2.80 | 5.33 | 9.46 | 1.72 |
    2.96 | 5.26 | 1.39 | 2.49 | 4.27 |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '| DJF [[78](#bib.bib78)] | 1.68 | 3.24 | 5.62 | 2.80 | 5.33 | 9.46 | 1.72 |
    2.96 | 5.26 | 1.39 | 2.49 | 4.27 |'
- en: '| DJFR [[79](#bib.bib79)] | 1.32 | 3.19 | 5.57 | 2.38 | 4.94 | 9.18 | 1.50
    | 2.72 | 5.05 | 1.27 | 2.34 | 4.13 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '| DJFR [[79](#bib.bib79)] | 1.32 | 3.19 | 5.57 | 2.38 | 4.94 | 9.18 | 1.50
    | 2.72 | 5.05 | 1.27 | 2.34 | 4.13 |'
- en: '| DepthSR [[44](#bib.bib44)] | 2.08 | 3.26 | 5.78 | 3.49 | 5.70 | 9.76 | 1.82
    | 2.85 | 4.60 | 1.40 | 2.23 | 3.75 |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| DepthSR [[44](#bib.bib44)] | 2.08 | 3.26 | 5.78 | 3.49 | 5.70 | 9.76 | 1.82
    | 2.85 | 4.60 | 1.40 | 2.23 | 3.75 |'
- en: '| PAC [[143](#bib.bib143)] | 1.32 | 2.62 | 4.58 | 1.89 | 3.33 | 6.78 | 1.25
    | 1.98 | 3.49 | 1.27 | 2.03 | 3.45 |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '| PAC [[143](#bib.bib143)] | 1.32 | 2.62 | 4.58 | 1.89 | 3.33 | 6.78 | 1.25
    | 1.98 | 3.49 | 1.27 | 2.03 | 3.45 |'
- en: '| CUNet [[21](#bib.bib21)] | 1.10 | 2.17 | 4.33 | 1.92 | 3.70 | 6.78 | 1.18
    | 1.95 | 3.45 | 1.18 | 1.88 | 3.25 |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '| CUNet [[21](#bib.bib21)] | 1.10 | 2.17 | 4.33 | 1.92 | 3.70 | 6.78 | 1.18
    | 1.95 | 3.45 | 1.18 | 1.88 | 3.25 |'
- en: '| DKN [[68](#bib.bib68)] | 1.23 | 2.12 | 4.24 | 1.62 | 3.26 | 6.51 | 1.30 |
    1.96 | 3.42 | 1.27 | 1.86 | 3.22 |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| DKN [[68](#bib.bib68)] | 1.23 | 2.12 | 4.24 | 1.62 | 3.26 | 6.51 | 1.30 |
    1.96 | 3.42 | 1.27 | 1.86 | 3.22 |'
- en: '| FDKN [[68](#bib.bib68)] | 1.08 | 2.17 | 4.50 | 1.86 | 3.58 | 6.96 | 1.18
    | 1.91 | 3.41 | 1.13 | 1.84 | 3.29 |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| FDKN [[68](#bib.bib68)] | 1.08 | 2.17 | 4.50 | 1.86 | 3.58 | 6.96 | 1.18
    | 1.91 | 3.41 | 1.13 | 1.84 | 3.29 |'
- en: '| PMBANet [[183](#bib.bib183)] | 1.11 | 2.18 | 3.25 | 1.06 | 2.28 | 4.98 |
    1.21 | 1.90 | 3.33 | 1.10 | 1.72 | 3.11 |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| PMBANet [[183](#bib.bib183)] | 1.11 | 2.18 | 3.25 | 1.06 | 2.28 | 4.98 |
    1.21 | 1.90 | 3.33 | 1.10 | 1.72 | 3.11 |'
- en: '| FDSR [[50](#bib.bib50)] | 1.13 | 2.08 | 4.39 | 1.61 | 3.18 | 5.86 | 1.16
    | 1.82 | 3.06 | 1.10 | 1.71 | 2.87 |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| FDSR [[50](#bib.bib50)] | 1.13 | 2.08 | 4.39 | 1.61 | 3.18 | 5.86 | 1.16
    | 1.82 | 3.06 | 1.10 | 1.71 | 2.87 |'
- en: '| JIIF [[149](#bib.bib149)] | 1.09 | 1.82 | 3.31 | 1.37 | 2.76 | 5.27 | 1.15
    | 1.77 | 2.79 | 1.17 | 1.79 | 2.86 |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| JIIF [[149](#bib.bib149)] | 1.09 | 1.82 | 3.31 | 1.37 | 2.76 | 5.27 | 1.15
    | 1.77 | 2.79 | 1.17 | 1.79 | 2.86 |'
- en: '| SVLRM [[24](#bib.bib24)] | 1.11 | 2.13 | 4.34 | 1.51 | 3.21 | 6.98 | 1.22
    | 1.88 | 3.55 | 1.19 | 1.93 | 3.49 |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| SVLRM [[24](#bib.bib24)] | 1.11 | 2.13 | 4.34 | 1.51 | 3.21 | 6.98 | 1.22
    | 1.88 | 3.55 | 1.19 | 1.93 | 3.49 |'
- en: '| AHMF [[197](#bib.bib197)] | 1.07 | 1.63 | 3.14 | 1.40 | 2.89 | 5.64 | 1.10
    | 1.73 | 3.04 | 1.10 | 1.70 | 2.83 |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| AHMF [[197](#bib.bib197)] | 1.07 | 1.63 | 3.14 | 1.40 | 2.89 | 5.64 | 1.10
    | 1.73 | 3.04 | 1.10 | 1.70 | 2.83 |'
- en: '| DCTNet [[196](#bib.bib196)] | 1.10 | 2.05 | 4.19 | 1.59 | 3.16 | 5.84 | 1.08
    | 1.74 | 3.05 | 1.07 | 1.74 | 3.09 |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| DCTNet [[196](#bib.bib196)] | 1.10 | 2.05 | 4.19 | 1.59 | 3.16 | 5.84 | 1.08
    | 1.74 | 3.05 | 1.07 | 1.74 | 3.09 |'
- en: '| LGR [[18](#bib.bib18)] | 1.11 | 2.12 | 4.43 | 1.79 | 3.17 | 6.02 | 1.30 |
    1.83 | 3.12 | 1.25 | 1.79 | 3.03 |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '| LGR [[18](#bib.bib18)] | 1.11 | 2.12 | 4.43 | 1.79 | 3.17 | 6.02 | 1.30 |
    1.83 | 3.12 | 1.25 | 1.79 | 3.03 |'
- en: '| RSAG [[186](#bib.bib186)] | – | – | – | 1.23 | 2.51 | 5.27 | 1.14 | 1.75
    | 2.96 | – | – | – |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| RSAG [[186](#bib.bib186)] | – | – | – | 1.23 | 2.51 | 5.27 | 1.14 | 1.75
    | 2.96 | – | – | – |'
- en: '| SFG [[187](#bib.bib187)] | – | – | – | 1.45 | 2.84 | 5.56 | – | – | – | –
    | – | – |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| SFG [[187](#bib.bib187)] | – | – | – | 1.45 | 2.84 | 5.56 | – | – | – | –
    | – | – |'
- en: '| DAGF [[198](#bib.bib198)] | 1.15 | 1.80 | 3.70 | 1.36 | 2.87 | 6.06 | 1.17
    | 1.75 | 3.10 | 1.15 | 1.76 | 3.16 |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| DAGF [[198](#bib.bib198)] | 1.15 | 1.80 | 3.70 | 1.36 | 2.87 | 6.06 | 1.17
    | 1.75 | 3.10 | 1.15 | 1.76 | 3.16 |'
- en: '| JGF [[160](#bib.bib160)] | – | – | – | 1.57 | 2.48 | 4.11 | 1.25 | 1.85 |
    3.03 | – | – | – |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '| JGF [[160](#bib.bib160)] | – | – | – | 1.57 | 2.48 | 4.11 | 1.25 | 1.85 |
    3.03 | – | – | – |'
- en: '| DADA [[110](#bib.bib110)] | 1.02 | 1.72 | 3.16 | 1.55 | 2.88 | 5.34 | 1.12
    | 1.70 | 2.89 | 1.06 | 1.62 | 2.64 |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| DADA [[110](#bib.bib110)] | 1.02 | 1.72 | 3.16 | 1.55 | 2.88 | 5.34 | 1.12
    | 1.70 | 2.89 | 1.06 | 1.62 | 2.64 |'
- en: '| GeoDSR [[161](#bib.bib161)] | 1.04 | 1.68 | 3.10 | 1.42 | 2.62 | 4.86 | 1.10
    | 1.69 | 2.84 | 1.07 | 1.62 | 2.70 |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| GeoDSR [[161](#bib.bib161)] | 1.04 | 1.68 | 3.10 | 1.42 | 2.62 | 4.86 | 1.10
    | 1.69 | 2.84 | 1.07 | 1.62 | 2.70 |'
- en: '| SSDNet [[195](#bib.bib195)] | 1.02 | 1.91 | 4.02 | 1.60 | 3.14 | 5.86 | 1.04
    | 1.72 | 2.92 | – | – | – |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '| SSDNet [[195](#bib.bib195)] | 1.02 | 1.91 | 4.02 | 1.60 | 3.14 | 5.86 | 1.04
    | 1.72 | 2.92 | – | – | – |'
- en: '| DSR-EI [[120](#bib.bib120)] | 0.97 | 1.53 | 2.32 | 1.21 | 2.46 | 4.95 | 0.91
    | 1.37 | 2.10 | 0.69 | 1.19 | 1.96 |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| DSR-EI [[120](#bib.bib120)] | 0.97 | 1.53 | 2.32 | 1.21 | 2.46 | 4.95 | 0.91
    | 1.37 | 2.10 | 0.69 | 1.19 | 1.96 |'
- en: 'Table 8: GDSR – Results on Middlebury, NYUv2, RGBDD and DIML datasets. The
    lower the RMSE, the better. Best results in bold, second-best are underlined.'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：GDSR – 在 Middlebury、NYUv2、RGBDD 和 DIML 数据集上的结果。RMSE 越低越好。最佳结果用粗体标出，第二好的结果用下划线标出。
- en: 6.2 Comparison of GDC methods
  id: totrans-673
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 GDC 方法的比较
- en: 'This section gives the quantitative results achieved by several representative
    methods on the two benchmark datasets, i.e., KITTI driving scenes and NYUv2 indoor
    scenes datasets, as shown in Table [9](#S6.T9 "Table 9 ‣ 6.2 Comparison of GDC
    methods ‣ 6 Evaluation ‣ RGB Guided ToF Imaging System: A Survey of Deep Learning-based
    Methods"). For the KITTI dataset, we use RMSE, MAE, iRMSE, and iMAE metrics to
    evaluate model performance. The results on NYUv2 are measured using RMSE and REL
    metrics.'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '本节给出了几个具有代表性的方法在两个基准数据集上的定量结果，即 KITTI 驾驶场景和 NYUv2 室内场景数据集，如表[9](#S6.T9 "Table
    9 ‣ 6.2 Comparison of GDC methods ‣ 6 Evaluation ‣ RGB Guided ToF Imaging System:
    A Survey of Deep Learning-based Methods")所示。对于 KITTI 数据集，我们使用 RMSE、MAE、iRMSE 和
    iMAE 指标来评估模型性能。NYUv2 的结果使用 RMSE 和 REL 指标进行测量。'
- en: From the table, we can see that unsupervised models have been making continuous
    progress. Even so, their performance still lags significantly behind their supervised
    counterparts. By using uncertainty as auxiliary information for depth, MS-Net
     [[29](#bib.bib29)], pNCNN  [[27](#bib.bib27)], MJPM  [[205](#bib.bib205)] and
    PADNet[[115](#bib.bib115)] can filter out noisy input and invalid pixels, thus
    achieving better performance than versions without using uncertainty. As another
    auxiliary-based method, multi-task learning has been adopted by various methods
    to improve generalization performance. The results produced by Multitask GANs[[188](#bib.bib188)]
    and SIUNet[[125](#bib.bib125)] demonstrate the effectiveness of this kind of approach.
    Networks using dual-branch structure[[150](#bib.bib150), [89](#bib.bib89), [194](#bib.bib194),
    [39](#bib.bib39), [8](#bib.bib8), [185](#bib.bib185)] also attain good performance.
    These methods fully leverage multi-scale features to recover the details of depth
    maps progressively. Based on the framework, RigNet[[180](#bib.bib180)] obtains
    the best results on NYUv2.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 从表中可以看出，无监督模型正在不断进步。尽管如此，它们的性能仍然显著落后于监督模型。通过使用不确定性作为深度的辅助信息，MS-Net  [[29](#bib.bib29)]、pNCNN
     [[27](#bib.bib27)]、MJPM  [[205](#bib.bib205)] 和 PADNet[[115](#bib.bib115)] 可以过滤噪声输入和无效像素，从而取得比不使用不确定性的版本更好的性能。作为另一种基于辅助的方法，多任务学习已经被多种方法采用来提高泛化性能。Multitask
    GANs[[188](#bib.bib188)] 和 SIUNet[[125](#bib.bib125)] 产生的结果证明了这种方法的有效性。使用双分支结构的网络[[150](#bib.bib150),
    [89](#bib.bib89), [194](#bib.bib194), [39](#bib.bib39), [8](#bib.bib8), [185](#bib.bib185)]
    也达到了良好的性能。这些方法充分利用多尺度特征逐步恢复深度图的细节。基于该框架，RigNet[[180](#bib.bib180)] 在 NYUv2 上取得了最佳结果。
- en: We also analyze the performance of the affinity-based methods. The affinity
    matrix produced by these methods, such as NLSPN [[113](#bib.bib113)] and GraphCSPN[[98](#bib.bib98)],
    are shown to improve depth accuracy through spatial propagation consistently.
    In addition to spatial propagation, DySPN [[84](#bib.bib84)] employs the attention
    mechanism to achieve excellent results. Furthermore, CompletionFormer[[191](#bib.bib191)]
    combines CNN with a recently popular and dominant technique, Vision Transformer,
    to achieve better performance in depth completion. In the most recent work, LRRU[[163](#bib.bib163)]
    outperform other state-of-the-art approaches by learning spatially-variant kernels
    and updating iteratively.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: '-   我们还分析了基于亲和力的方法的性能。这些方法产生的亲和力矩阵，如 NLSPN [[113](#bib.bib113)] 和 GraphCSPN[[98](#bib.bib98)]，通过空间传播持续提高了深度准确性。除了空间传播，DySPN [[84](#bib.bib84)]
    还采用了注意力机制来取得优秀的结果。此外，CompletionFormer[[191](#bib.bib191)] 将 CNN 与最近流行且主导的技术 Vision
    Transformer 相结合，以实现更好的深度补全性能。在最新的研究中，LRRU[[163](#bib.bib163)] 通过学习空间变异的卷积核并进行迭代更新，超越了其他最先进的方法。'
- en: '| Method | Loss Function | Learning | KITTI Dataset | NYUv2 Dataset |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| Method | Loss Function | Learning | KITTI Dataset | NYUv2 Dataset |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| RMSE | MAE | iRMSE | iMAE | RMSE | REL |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| RMSE | MAE | iRMSE | iMAE | RMSE | REL |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| DDP [[182](#bib.bib182)] | $l_{1}+l_{cpn}+l_{SSIM}$ | Un | 1263.19 | 343.46
    | 3.58 | 1.32 | – | – |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| DDP [[182](#bib.bib182)] | $l_{1}+l_{cpn}+l_{SSIM}$ | Un | 1263.19 | 343.46
    | 3.58 | 1.32 | – | – |'
- en: '| VIO [[169](#bib.bib169)] | $l_{ph}+l_{1}+l_{pc}+l_{sm}$ | Un | 1169.97 |
    299.41 | 3.56 | 1.20 | – | – |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| VIO [[169](#bib.bib169)] | $l_{ph}+l_{1}+l_{pc}+l_{sm}$ | Un | 1169.97 |
    299.41 | 3.56 | 1.20 | – | – |'
- en: '| lsf [[124](#bib.bib124)] | $l_{1}+l_{SSIM}$ | Un | 885.00 | 225.20 | 3.40
    | – | 0.134 | – |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| lsf [[124](#bib.bib124)] | $l_{1}+l_{SSIM}$ | Un | 885.00 | 225.20 | 3.40
    | – | 0.134 | – |'
- en: '| KBNet [[170](#bib.bib170)] | $l_{ph}+l_{1}+l_{sm}$ | Un | 1069.47 | 256.76
    | 2.95 | 1.02 | – | – |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| KBNet [[170](#bib.bib170)] | $l_{ph}+l_{1}+l_{sm}$ | Un | 1069.47 | 256.76
    | 2.95 | 1.02 | – | – |'
- en: '| CSPN [[11](#bib.bib11)] | $l_{2}$ | Su | 1019.64 | 279.46 | 2.93 | 1.15 |
    0.117 | 0.016 |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '| CSPN [[11](#bib.bib11)] | $l_{2}$ | Su | 1019.64 | 279.46 | 2.93 | 1.15 |
    0.117 | 0.016 |'
- en: '| MS-Net  [[29](#bib.bib29)] | $l_{1}$ | Su | 859.22 | 207.77 | 2.52 | 0.92
    | 0.117 | 0.016 |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '| MS-Net  [[29](#bib.bib29)] | $l_{1}$ | Su | 859.22 | 207.77 | 2.52 | 0.92
    | 0.117 | 0.016 |'
- en: '| Deeplidar  [[123](#bib.bib123)] | $l_{2}+l_{n2}$ | Su | 758.38 | 226.50 |
    2.56 | 1.15 | – | – |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '| Deeplidar  [[123](#bib.bib123)] | $l_{2}+l_{n2}$ | Su | 758.38 | 226.50 |
    2.56 | 1.15 | – | – |'
- en: '| pNCNN  [[27](#bib.bib27)] | $l_{2}$ | Su | 988.57 | 228.53 | 2.71 | 1.00
    | - | - |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '| pNCNN  [[27](#bib.bib27)] | $l_{2}$ | Su | 988.57 | 228.53 | 2.71 | 1.00
    | - | - |'
- en: '| NLSPN [[113](#bib.bib113)] | $l_{1}$ or $l_{2}$ | Su | 741.68 | 199.59 |
    1.99 | 0.84 | 0.092 | 0.012 |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '| NLSPN [[113](#bib.bib113)] | $l_{1}$ 或 $l_{2}$ | Su | 741.68 | 199.59 | 1.99
    | 0.84 | 0.092 | 0.012 |'
- en: '| GuideNet [[150](#bib.bib150)] | $l_{2}$ | Su | 736.24 | 218.83 | 2.25 | 0.99
    | 0.101 | 0.015 |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| GuideNet [[150](#bib.bib150)] | $l_{2}$ | Su | 736.24 | 218.83 | 2.25 | 0.99
    | 0.101 | 0.015 |'
- en: '| KernelNet  [[89](#bib.bib89)] | $l_{1}+l_{ce}+l_{grad}$ | Su | 785.06 | 218.60
    | 2.11 | 0.92 | 0.111 | 0.015 |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| KernelNet  [[89](#bib.bib89)] | $l_{1}+l_{ce}+l_{grad}$ | Su | 785.06 | 218.60
    | 2.11 | 0.92 | 0.111 | 0.015 |'
- en: '| ACMNet [[194](#bib.bib194)] | $l_{2}+l_{sm}$ | Su | 744.91 | 206.09 | 2.08
    | 0.90 | 0.105 | 0.015 |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| ACMNet [[194](#bib.bib194)] | $l_{2}+l_{sm}$ | Su | 744.91 | 206.09 | 2.08
    | 0.90 | 0.105 | 0.015 |'
- en: '| DenseLiDAR [[39](#bib.bib39)] | $l_{2}+l_{grad}+l_{SSIM}$ | Su | 755.41 |
    214.13 | 2.25 | 0.96 | – | – |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| DenseLiDAR [[39](#bib.bib39)] | $l_{2}+l_{grad}+l_{SSIM}$ | Su | 755.41 |
    214.13 | 2.25 | 0.96 | – | – |'
- en: '| RigNet [[180](#bib.bib180)] | $l_{2}$ | Su | 712.66 | 203.35 | 2.08 | 0.90
    | 0.090 | 0.013 |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| RigNet [[180](#bib.bib180)] | $l_{2}$ | Su | 712.66 | 203.35 | 2.08 | 0.90
    | 0.090 | 0.013 |'
- en: '| Ssgp [[134](#bib.bib134)] | $l_{2}$ | Su | 833.00 | 204.00 | – | – | – |
    – |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '| Ssgp [[134](#bib.bib134)] | $l_{2}$ | Su | 833.00 | 204.00 | – | – | – |
    – |'
- en: '| DenseLConv [[73](#bib.bib73)] | $l_{2}$ | Su | 729.88 | 210.06 | 2.10 | 0.93
    | 0.099 | 0.015 |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '| DenseLConv [[73](#bib.bib73)] | $l_{2}$ | Su | 729.88 | 210.06 | 2.10 | 0.93
    | 0.099 | 0.015 |'
- en: '| GraphCSPN [[98](#bib.bib98)] | $l_{1}$ | Su | 738.41 | 199.31 | 1.96 | 0.84
    | 0.090 | 0.012 |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
  zh: '| GraphCSPN [[98](#bib.bib98)] | $l_{1}$ | Su | 738.41 | 199.31 | 1.96 | 0.84
    | 0.090 | 0.012 |'
- en: '| SpAgNet [[17](#bib.bib17)] | $l_{1}$ | Su | 844.79 | 218.39 | - | - | 0.114
    | 0.015 |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '| SpAgNet [[17](#bib.bib17)] | $l_{1}$ | Su | 844.79 | 218.39 | - | - | 0.114
    | 0.015 |'
- en: '| Multitask GANs [[188](#bib.bib188)] | $l_{adv}+l_{2}+l_{cyc}$ | Su | 746.96
    | 267.71 | 2.24 | 1.10 | - | - |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '| Multitask GANs [[188](#bib.bib188)] | $l_{adv}+l_{2}+l_{cyc}$ | Su | 746.96
    | 267.71 | 2.24 | 1.10 | - | - |'
- en: '| SIUNet  [[125](#bib.bib125)] | $l_{1}$ | Su | 1026.61 | 227.28 | 2.73 | 0.96
    | 0.138 | 0.015 |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
  zh: '| SIUNet  [[125](#bib.bib125)] | $l_{1}$ | Su | 1026.61 | 227.28 | 2.73 | 0.96
    | 0.138 | 0.015 |'
- en: '| FCFR-Net  [[90](#bib.bib90)] | $l_{1}+l_{2}$ | Su | 735.81 | 217.15 | 2.20
    | 0.98 | 0.106 | 0.015 |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '| FCFR-Net  [[90](#bib.bib90)] | $l_{1}+l_{2}$ | Su | 735.81 | 217.15 | 2.20
    | 0.98 | 0.106 | 0.015 |'
- en: '| MFF-Net  [[91](#bib.bib91)] | $l_{1}+l_{2}$ | Su | 719.85 | 208.11 | 2.21
    | 0.94 | 0.100 | 0.015 |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
  zh: '| MFF-Net  [[91](#bib.bib91)] | $l_{1}+l_{2}$ | Su | 719.85 | 208.11 | 2.21
    | 0.94 | 0.100 | 0.015 |'
- en: '| MJPM  [[205](#bib.bib205)] | $l_{1}+l_{2}$ | Su | 795.43 | 190.88 | 1.98
    | 0.83 | - | - |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '| MJPM  [[205](#bib.bib205)] | $l_{1}+l_{2}$ | Su | 795.43 | 190.88 | 1.98
    | 0.83 | - | - |'
- en: '| PADNet  [[115](#bib.bib115)] | $l_{1}+l_{focal}$ | Su | 746.19 | 197.99 |
    1.96 | 0.85 | 0.094 | 0.012 |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
  zh: '| PADNet  [[115](#bib.bib115)] | $l_{1}+l_{focal}$ | Su | 746.19 | 197.99 |
    1.96 | 0.85 | 0.094 | 0.012 |'
- en: '| DySPN [[84](#bib.bib84)] | $l_{1}+l_{2}$ | Su | 709.12 | 192.71 | 1.88 |
    0.82 | 0.090 | 0.012 |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '| DySPN [[84](#bib.bib84)] | $l_{1}+l_{2}$ | Su | 709.12 | 192.71 | 1.88 |
    0.82 | 0.090 | 0.012 |'
- en: '| CompletionFormer [[191](#bib.bib191)] | $l_{1}+l_{2}$ | Su | 708.87 | 203.45
    | 2.01 | 0.84 | 0.090 | 0.012 |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
  zh: '| CompletionFormer [[191](#bib.bib191)] | $l_{1}+l_{2}$ | Su | 708.87 | 203.45
    | 2.01 | 0.84 | 0.090 | 0.012 |'
- en: '| AGGNet [[8](#bib.bib8)] | $l_{2}+l_{huber}$ | Su | – | – | – | – | 0.092
    | 0.014 |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '| AGGNet [[8](#bib.bib8)] | $l_{2}+l_{huber}$ | Su | – | – | – | – | 0.092
    | 0.014 |'
- en: '| PointDC [[185](#bib.bib185)] | $l_{1}+l+2+l_{grad}$ | Su | 736.07 | 201.87
    | 1.97 | 0.87 | 0.089 | 0.012 |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
  zh: '| PointDC [[185](#bib.bib185)] | $l_{1}+l+2+l_{grad}$ | Su | 736.07 | 201.87
    | 1.97 | 0.87 | 0.089 | 0.012 |'
- en: '| LRRU [[163](#bib.bib163)] |  | Su | 696.51 | 189.96 | 1.87 | 0.81 | 0.091
    | 0.011 |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
  zh: '| LRRU [[163](#bib.bib163)] |  | Su | 696.51 | 189.96 | 1.87 | 0.81 | 0.091
    | 0.011 |'
- en: 'Table 9: GDC – Results on KITTI and NYU-v2 datasets. Un and Su indicate unsupervised
    and supervised models. The lower the RMSE, the better. Best results in bold, second-best
    are underlined.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: GDC – 在 KITTI 和 NYU-v2 数据集上的结果。Un 和 Su 表示无监督和有监督模型。RMSE 越低越好。最佳结果用**粗体**标出，第二好的用*下划线*标出。'
- en: 7 Discussion and future trends
  id: totrans-711
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论与未来趋势
- en: Although some reviewed methods have achieved promising results on RGB guided
    ToF imaging, some issues hinder their practical application. Here, we mainly describe
    the challenges and future trends from the following four aspects.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些回顾的方法在 RGB 引导的 ToF 成像上取得了令人鼓舞的结果，但一些问题阻碍了它们的实际应用。在这里，我们主要从以下四个方面描述挑战和未来趋势。
- en: 7.1 Unsupervised methods with lightweight structure
  id: totrans-713
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 具有轻量结构的无监督方法
- en: As described before, most methods perform GDSR and GDC after training with a
    supervised paradigm. However, labeled data is usually difficult to collect. In
    addition, the performance of mass-produced sensors generally fluctuates to a certain
    extent, and it is unrealistic to fine-tune the model on data collected from any
    existing sensor. Therefore, unsupervised learning methods need to be developed
    without ground truth depth. In particular, state-of-the-art unsupervised learning
    methods cannot achieve the performance of supervised learning methods, so there
    is still much room for improvement in this direction.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，大多数方法在使用有监督范式训练后进行 GDSR 和 GDC。然而，标记数据通常难以收集。此外，大量生产的传感器的性能一般会有一定程度的波动，在任何现有传感器上对数据进行微调是不现实的。因此，需要在没有地面真值深度的情况下开发无监督学习方法。特别是，最先进的无监督学习方法无法达到有监督学习方法的性能，因此这一方向仍有很大的改进空间。
- en: On the other hand, these methods often require many parameters and much computation
    to achieve better results, which makes the models unable to run on many devices
    at high speed, especially widely used mobile devices. While transferring the information
    to the cloud for processing is one solution to this problem, it raises thorny
    privacy issues. Thus, efficiently recovering high-quality depth is necessary and
    remains an open challenge. One recent example is SeaFormer [[156](#bib.bib156)],
    which proposes a versatile mobile-friendly backbone based on the vision transformer.
    Developing a simple yet efficient unsupervised model for RGB guided ToF imaging
    is challenging but highly desirable.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，这些方法通常需要许多参数和大量计算才能获得更好的结果，这使得模型无法在许多设备上高速运行，尤其是广泛使用的移动设备。虽然将信息传输到云端处理是解决此问题的一个方法，但它引发了棘手的隐私问题。因此，高效地恢复高质量的深度是必要的，并且仍然是一个未解的挑战。一个近期的例子是
    SeaFormer [[156](#bib.bib156)]，它提出了一种基于视觉变换器的多功能移动友好型骨干。开发一个简单而高效的 RGB 引导的 ToF
    成像的无监督模型具有挑战性，但非常值得追求。
- en: 7.2 Cross-domain generalization
  id: totrans-716
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 跨领域泛化
- en: Another issue concerns the ability of networks to generalize to new domains.
    This topic has been mostly overlooked in the literature despite its relevance
    for the widespread diffusion of depth estimation in practical applications. Purposely,
    [[3](#bib.bib3)] proposed a method to improve cross-domain generalization for
    depth completion by exploiting the virtual projection pattern paradigm proposed
    in [[4](#bib.bib4)] and casting depth completion as a correspondence problem using
    a virtual stereo setup. Processing properly hallucinated fictitious stereo pairs
    with a robust stereo network that possibly exploits the RGB image context, such
    as RAFT [[85](#bib.bib85)], leads to more robust cross-domain generalization than
    state-of-the-art depth completion networks.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是网络在新领域上的泛化能力。尽管这对深度估计在实际应用中的广泛推广具有相关性，但文献中对此话题的关注较少。为此，[[3](#bib.bib3)]
    提出了通过利用 [[4](#bib.bib4)] 中提出的虚拟投影模式范式来改进深度完成的跨领域泛化的方法，并将深度完成视为一个使用虚拟立体设置的对应问题。使用可能利用RGB图像上下文的强大立体网络（如
    RAFT [[85](#bib.bib85)]）来正确处理虚构的立体对，比现有的最先进的深度完成网络具有更强的跨领域泛化能力。
- en: 7.3 ToF with varifocal lens color camera
  id: totrans-718
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 使用变焦镜头的ToF彩色相机
- en: RGB cameras in many devices, such as mobile phones and drones, have variable
    focal lengths, while ToF cameras usually employ fixed focal length lenses. Different
    focal lengths correspond to different depths of field and angles of view. When
    a ToF camera captures a distant object, boundaries are usually blurred. If the
    color image autofocuses on the object, it can provide rich structural details
    and transfer to the depth map. Nonetheless, it also brings a problem that the
    scales of the images from the two modalities may differ. In this case, the RGB
    image and the depth map cannot be directly aligned by the registration technique,
    i.e., a new calibration is required. Hence, further investigation needs to be
    conducted on how to align two scale-inconsistent images, i.e., the RGB image and
    the depth map, and simultaneously transfer the scene geometry in the RGB image
    to the depth map.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 许多设备中的RGB相机，如手机和无人机，具有可变的焦距，而ToF相机通常使用固定焦距的镜头。不同的焦距对应不同的景深和视角。当ToF相机捕捉到远处的物体时，边界通常会模糊。如果彩色图像对物体进行自动对焦，它可以提供丰富的结构细节并转移到深度图。然而，这也带来了一个问题，即两个模态的图像的尺度可能不同。在这种情况下，RGB图像和深度图无法通过配准技术直接对齐，即需要新的校准。因此，需要进一步研究如何对齐两个尺度不一致的图像，即RGB图像和深度图，并同时将RGB图像中的场景几何信息转移到深度图中。
- en: 7.4 Spot-ToF
  id: totrans-720
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 点对ToF
- en: The main disadvantage of ToF cameras using floodlight projection, currently
    the most commonly used technique in mobile devices, is their close operating distance.
    The scheme proposed by [[1](#bib.bib1)] can significantly extend the measurable
    distance of the ToF camera, but the long acquisition time of each image limits
    the practical application. On the other hand, direct ToF (d-ToF) can capture distant
    objects, whereas the resolution of the sensors is much lower than that of indirect
    ToF (i-ToF) with tens of thousands of pixels, e.g., $8\times 8$ or $24\times 24$.
    LiDAR also faces the same problem, and its manufacturing cost is too expensive,
    making it currently unable to be used as a consumer-grade camera. [[36](#bib.bib36)]
    provide an alternative to alleviate the issue. Specifically, they replaced the
    optical diffuser in the ToF camera projector with a diffraction optical element
    (DOE), which is used to evenly distribute the incident light into thousands of
    laser beams. In this way, each laser beam can have a measurable distance greater
    than that of floodlight illumination. However, this approach faces two challenges.
    First, a laser beam forms a spot in the sensor, corresponding to several pixels.
    The uneven intensity distribution of the spot may lead to inconsistencies in the
    measurement, so it is necessary to develop new depth correction methods to obtain
    an accurate depth map. Second, the captured images are sparse (similar to LiDAR
    but at a lower cost), so effective and efficient algorithms need to be developed
    to estimate dense depth under this setting.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 使用泛光投影的ToF摄像头，目前在移动设备中最常用的技术，其主要缺点是操作距离较近。[[1](#bib.bib1)]提出的方案可以显著扩展ToF摄像头的可测量距离，但每张图像的长采集时间限制了其实用性。另一方面，直接ToF（d-ToF）可以捕捉远处的物体，但其传感器的分辨率远低于间接ToF（i-ToF），例如$8\times
    8$或$24\times 24$的几千像素。LiDAR也面临同样的问题，其制造成本过高，使其目前无法作为消费级摄像头使用。[[36](#bib.bib36)]提供了一个替代方案以缓解这一问题。具体而言，他们将ToF摄像头投影仪中的光学扩散器更换为衍射光学元件（DOE），用于将入射光均匀分布为数千束激光。通过这种方式，每束激光的可测量距离可以大于泛光照明。然而，这种方法面临两个挑战。首先，激光束在传感器上形成一个光斑，对应于多个像素。光斑的不均匀强度分布可能导致测量的不一致，因此需要开发新的深度修正方法以获得准确的深度图。其次，捕获的图像是稀疏的（类似于LiDAR但成本较低），因此需要开发有效的算法来估计这种情况下的密集深度。
- en: 7.5 Under-display RGBD camera
  id: totrans-722
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 屏下RGBD摄像头
- en: 'In recent years, in pursuit of a better visual experience, full-screen devices
    have attracted the attention of industry and academia. This fact also brings a
    major challenge: images captured by under-display cameras are severely degraded,
    requiring effective strategies for restoration. Most of the existing work is to
    study the image restoration of under-display RGB cameras (UDC), while few works
    focus on under-display ToF (UD-ToF) depth restoration.'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，追求更好的视觉体验，全屏设备吸引了业界和学术界的关注。这一事实也带来了一个重大挑战：屏下摄像头拍摄的图像严重退化，需要有效的恢复策略。现有的大部分工作研究的是屏下RGB摄像头（UDC）的图像恢复，而关注屏下ToF（UD-ToF）深度恢复的工作则较少。
- en: Under-display camera. Recently, smartphones with full-screen displays, eliminating
    the need for bezels, have become a new product trend and motivated manufacturers
    to design a brand-new imaging system, the Under-Display Camera. Placing a display
    in front of the camera lens can increase the screen-to-body ratio for a better
    user experience. However, the broad commercial manufacturing of UDC is prevented
    by poor imaging quality due to the inevitable degradations, such as blurs, noise,
    diffraction artifacts, color shifts, etc.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 屏下摄像头。最近，采用全屏显示的智能手机，取消了边框，成为了一种新的产品趋势，并促使制造商设计出全新的成像系统——屏下摄像头。将显示屏放置在摄像头镜头前面可以提高屏占比，从而提供更好的用户体验。然而，由于不可避免的图像质量退化，如模糊、噪声、衍射伪影、色彩偏移等，屏下摄像头的广泛商业生产受到限制。
- en: UDC image restoration has been studied in a few literary works. As the pioneered
    of this work, [[203](#bib.bib203)] devise a Monitor Camera Imaging System (MCIS)
    to collect paired data and offer two practical solutions for UDC image restoration,
    including a deconvolution-based Wierner Filter pipeline and the data-driven method.[[181](#bib.bib181)]
    redesign the display layout to improve the quality of recovered images. To address
    spatially variant blur and noise,[[70](#bib.bib70)] develop a controllable image
    restoration algorithm, which performs well on both a monitor-based aligned dataset[[202](#bib.bib202)]
    and a real-world dataset.[[32](#bib.bib32)] propose a domain-knowledge-based network
    to restore the UDC images. However, this work requires a point-spread function
    (PSF) as a prior, thus failing to work well when the PSF is unavailable. To remedy
    this,[[69](#bib.bib69)] design an effective two-branched network for UDC image
    restoration from the perspective of low-frequency and high-frequency information
    learning. The UDC image restoration methods need paired image datasets that perform
    poor generalization capability for real degradations. The reason behind this phenomenon
    is the domain discrepancy between synthesized data and real-captured data. To
    alleviate this problem,[[31](#bib.bib31)] employ collected non-aligned data for
    UDC image restoration and improve the robustness of the restoration network in
    real-world scenarios.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: UDC 图像恢复在一些文献中已有研究。作为这一工作的开创者，[[203](#bib.bib203)]设计了一种监视器相机成像系统（MCIS）来收集配对数据，并提供了两种实际的
    UDC 图像恢复解决方案，包括基于去卷积的 Wierner 滤波器管道和数据驱动方法。[[181](#bib.bib181)] 重新设计了显示布局，以提高恢复图像的质量。为了解决空间变化的模糊和噪声，[[70](#bib.bib70)]
    开发了一种可控的图像恢复算法，该算法在基于监视器的对齐数据集[[202](#bib.bib202)]和真实世界数据集上表现良好。[[32](#bib.bib32)]
    提出了基于领域知识的网络来恢复 UDC 图像。然而，这项工作需要点扩散函数（PSF）作为先验，因此在 PSF 不可用时效果不佳。为了解决这个问题，[[69](#bib.bib69)]
    设计了一种有效的双分支网络，从低频和高频信息学习的角度进行 UDC 图像恢复。UDC 图像恢复方法需要配对图像数据集，对于真实退化情况的泛化能力较差。这种现象背后的原因是合成数据和真实捕获数据之间的领域差异。为缓解这个问题，[[31](#bib.bib31)]
    采用了收集的非对齐数据进行 UDC 图像恢复，并提高了恢复网络在真实场景中的鲁棒性。
- en: Under-display ToF. Currently, only one work[[118](#bib.bib118)] has investigated
    image restoration with a ToF camera placed under a display. In this paper, they
    propose a cascaded network to restore the depth in a coarse-to-fine manner. Differently
    from previous methods, in the first stage, they design a complex-valued neural
    network to recover the ToF raw measurements. In the second stage, they refine
    the depth based on the proposed multi-scale depth enhancement block. Moreover,
    to enable the training, they introduce real and synthetic datasets for UD-ToF
    imaging, respectively. Specifically, the large-scale synthetic dataset is created
    by analyzing noise in magnitude and phase.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 屏下 ToF。目前，仅有一项研究[[118](#bib.bib118)]探讨了将 ToF 相机放置在显示器下进行图像恢复。在这篇论文中，他们提出了一种级联网络，以粗到精的方式恢复深度。与之前的方法不同，在第一阶段，他们设计了一个复杂值神经网络来恢复
    ToF 原始测量值。在第二阶段，他们基于所提出的多尺度深度增强模块来细化深度。此外，为了进行训练，他们分别引入了真实和合成的数据集用于 UD-ToF 成像。具体来说，大规模的合成数据集是通过分析幅度和相位中的噪声来创建的。
- en: If a color camera is located under a display with a ToF camera, there is greater
    potential for obtaining high-quality depth maps. However, the design of models
    to jointly compensate for multi-modal image degradation is still challenging.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 如果颜色相机位于显示器下方并配有 ToF 相机，则获取高质量深度图的潜力更大。然而，设计能够共同补偿多模态图像退化的模型仍然具有挑战性。
- en: 7.6 Hybrid models
  id: totrans-728
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6 混合模型
- en: Typically, RGBD-based imaging can be addressed by either conventional or deep-learning
    methods, each of which possesses advantages and disadvantages. A major advantage
    of conventional methods is that, through well-interpretable hand-crafted models,
    the predicted depth can be guaranteed to be loyal to the source. However, designing
    a model with excellent feature representation capabilities is usually extremely
    challenging. In contrast, deep-learning methods can perform very well on a specific
    task with sufficient and representative data for training. On the other hand,
    when there is insufficient data or a biased distribution, these trained models
    may not achieve the desired results during testing. Therefore, combining the advantages
    of both to design hybrid models is one of the future trends in RGB guided ToF
    camera imaging. In particular, the hand-crafted prior, complemented by the powerful
    feature representation capability of neural networks, can make it easier to achieve
    a lightweight approach.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，基于RGBD的成像可以通过传统方法或深度学习方法来解决，每种方法都有其优缺点。传统方法的主要优点是，通过可解释性强的手工设计模型，预测的深度可以保证忠实于源数据。然而，设计具有优秀特征表示能力的模型通常极具挑战性。相比之下，深度学习方法在具有充分且具有代表性的数据用于训练时，能够在特定任务上表现出色。另一方面，当数据不足或分布偏倚时，这些训练好的模型可能无法在测试过程中达到预期结果。因此，将两者的优点结合设计混合模型是RGB引导ToF相机成像未来的趋势之一。特别是，结合手工设计的先验知识和神经网络强大的特征表示能力，可以更容易实现轻量化的方法。
- en: 7.7 Online alignment of RGB and depth
  id: totrans-730
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7 RGB与深度的在线对齐
- en: In previous work, RGB and depth maps were usually assumed to have been well-aligned
    in spatial by calibration operations. However, in practice, the intrinsic ${c_{x},c_{y}}$
    and extrinsic parameters ${t_{x},t_{y}}$ change when the camera is deployed on
    a device or subjected to vibration, which requires online weakly calibration for
    the RGB guided ToF camera. On the other hand, cross-modal online alignment in
    the time dimension is also necessary for practical applications, as different
    sensors may not achieve completely accurate synchronization in sensing the environment.
    Especially when the acquisition is out of sync during motion, the imaging quality
    of the RGBD camera will be seriously degraded. Although some previous efforts[[122](#bib.bib122)]
    have been devoted to cross-modal correspondence matching, the algorithms consume
    a long inference time, so temporally and spatially efficient alignment methods
    remain to be further explored.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的工作中，RGB和深度图通常假设通过校准操作已在空间上对齐。然而，在实际应用中，当相机部署在设备上或受到振动时，内在参数${c_{x},c_{y}}$和外部参数${t_{x},t_{y}}$会发生变化，这需要对RGB引导ToF相机进行在线弱校准。另一方面，实际应用中在时间维度上的跨模态在线对齐也是必要的，因为不同传感器在感知环境时可能无法实现完全准确的同步。特别是在运动过程中采集数据不同步时，RGBD相机的成像质量会严重下降。尽管一些先前的工作[[122](#bib.bib122)]致力于跨模态对应匹配，但算法的推理时间较长，因此时间和空间高效的对齐方法仍需进一步探索。
- en: 7.8 Lightweight ToF
  id: totrans-732
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.8 轻量化ToF
- en: To promote the application of ToF cameras in various consumer electronic devices,
    lightweight ToF cameras [[80](#bib.bib80), [97](#bib.bib97)] were launched on
    the market and have received the favor of many manufacturers for their low power
    consumption, low cost, and compact structure. Unlike common ToFs, lightweight
    ToF cameras generate measurements as depth distributions rather than specific
    values. Specifically, this type of camera can be used in many fields, such as
    AR/VR and obstacle avoidance. Nonetheless, the lightweight design inevitably brings
    drawbacks such as extremely low resolution (e.g., $8\times 8$), which leads to
    a degradation in the depth quality and cannot support applications requiring high-quality
    depth. Therefore, using other modal information, such as RGB or IR, to improve
    the resolution of lightweight ToF depth becomes one of the future trends.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 为了促进ToF相机在各种消费电子设备中的应用，轻量化ToF相机[[80](#bib.bib80), [97](#bib.bib97)]已在市场上推出，并因其低功耗、低成本和紧凑结构而受到许多制造商的青睐。与常见的ToF相机不同，轻量化ToF相机生成的测量值是深度分布而非具体数值。具体而言，这种类型的相机可以应用于许多领域，如AR/VR和障碍物避免。然而，轻量化设计不可避免地带来了诸如极低分辨率（例如，$8\times
    8$）等缺点，这导致深度质量下降，无法支持需要高质量深度的应用。因此，利用其他模态信息，如RGB或IR，来提高轻量化ToF深度的分辨率成为未来的趋势之一。
- en: 7.9 Multi-frame processing
  id: totrans-734
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.9 多帧处理
- en: Currently, most algorithms deal with RGB guided ToF imaging in a single-frame
    fashion, using one degraded depth map as input to infer the corresponding high-quality
    output. However, the format we usually capture with cameras is a data stream (video)
    rather than a single image. Therefore, it is crucial to consider the temporal
    correlation of data and output stable depth maps. There have been some works [[171](#bib.bib171),
    [114](#bib.bib114)] employing multi-frame processing technology for image denoising,
    enhancement, and super-resolution, but only a few works[[147](#bib.bib147)] have
    applied it to ToF depth improvement. Based on the above analysis, fusing ToF depth
    and RGB images with multi-frame processing technology is a promising direction.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数算法以单帧的方式处理RGB引导的ToF成像，使用一张退化的深度图作为输入来推断对应的高质量输出。然而，我们通常使用相机捕获的格式是数据流（视频），而不是单一图像。因此，考虑数据的时间相关性和输出稳定的深度图是至关重要的。已有一些工作[[171](#bib.bib171),
    [114](#bib.bib114)]采用了多帧处理技术用于图像去噪、增强和超分辨率，但只有少数工作[[147](#bib.bib147)]将其应用于ToF深度改善。基于上述分析，将ToF深度与RGB图像融合的多帧处理技术是一个有前景的方向。
- en: 8 Conclusion
  id: totrans-736
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: 'In this paper, we have presented a survey of RGB guided ToF imaging methods
    based on deep learning. Our review covers preliminaries, evaluation metrics, network
    design, learning protocols, benchmark datasets, and objective functions. According
    to the measurable distance and the purpose of the ToF camera, we roughly divide
    the problem faced into two categories: guided depth super-resolution and guided
    depth completion. Moreover, we collect a quantitative comparison of the surveyed
    methods on widely used benchmarks and analyze their performance and respective
    characteristics. Finally, we summarize the current challenges in practical applications
    as well as promising future trends.'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们呈现了基于深度学习的RGB引导的ToF成像方法的综述。我们的回顾涵盖了基础知识、评估指标、网络设计、学习协议、基准数据集和目标函数。根据可测距离和ToF相机的目的，我们将面临的问题大致分为两个类别：引导深度超分辨率和引导深度完成。此外，我们收集了在广泛使用的基准上的方法的定量比较，并分析了它们的性能和各自的特点。最后，我们总结了实际应用中的当前挑战以及有前景的未来趋势。
- en: Acknowledgments
  id: totrans-738
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported by the National Natural Science Foundation of China (No.
    62088102, No. 62376208), Fundamental Research Funds for the Central Universities
    (No. xzy022023107), China Telecom Group Corporation-Xi’an Jiaotong University
    Jointly Established Intelligent Cloud Network Science and Education Integration
    Innovation Research Institute (No. 20221279-ZKT03).
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到中国国家自然科学基金（编号：62088102，62376208）、中央高校基础研究基金（编号：xzy022023107）、中国电信集团公司-西安交通大学联合建立的智能云网络科学与教育融合创新研究院（编号：20221279-ZKT03）的支持。
- en: References
  id: totrans-740
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: \bibcommenthead
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \bibcommenthead
- en: Achar \BOthers. [\APACyear2017] \APACinsertmetastarachar2017epipolar{APACrefauthors}Achar,
    S., Bartels, J.R., Whittaker, W.L., Kutulakos, K.N.\BCBL Narasimhan, S.G. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitleEpipolar time-of-flight imaging Epipolar time-of-flight imaging.\BBCQ
    \APACjournalVolNumPagesACM Transactions on Graphics (ToG)3641–8, \PrintBackRefs\CurrentBib
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achar \BOthers. [\APACyear2017] \APACinsertmetastarachar2017epipolar{APACrefauthors}Achar,
    S., Bartels, J.R., Whittaker, W.L., Kutulakos, K.N.\BCBL Narasimhan, S.G. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitleEpipolar time-of-flight imaging Epipolar time-of-flight imaging.\BBCQ
    \APACjournalVolNumPagesACM Transactions on Graphics (ToG)3641–8, \PrintBackRefs\CurrentBib
- en: 'Atapour-Abarghouei \BBA Breckon [\APACyear2019] \APACinsertmetastaratapour2019complete{APACrefauthors}Atapour-Abarghouei,
    A.\BCBT \BBA Breckon, T.P. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleTo complete
    or to estimate, that is the question: A multi-task approach to depth completion
    and monocular depth estimation To complete or to estimate, that is the question:
    A multi-task approach to depth completion and monocular depth estimation.\BBCQ
    \APACrefbtitle2019 International Conference on 3D Vision (3DV) 2019 international
    conference on 3d vision (3dv) (\BPGS183–193). \PrintBackRefs\CurrentBib'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Atapour-Abarghouei \BBA Breckon [\APACyear2019] \APACinsertmetastaratapour2019complete{APACrefauthors}Atapour-Abarghouei,
    A.\BCBT \BBA Breckon, T.P. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleTo complete
    or to estimate, that is the question: A multi-task approach to depth completion
    and monocular depth estimation To complete or to estimate, that is the question:
    A multi-task approach to depth completion and monocular depth estimation.\BBCQ
    \APACrefbtitle2019 International Conference on 3D Vision (3DV) 2019 international
    conference on 3d vision (3dv) (\BPGS183–193). \PrintBackRefs\CurrentBib'
- en: Bartolomei \BOthers. [\APACyear2024] \APACinsertmetastarBartolomei_2024_3DV{APACrefauthors}Bartolomei,
    L., Poggi, M., Conti, A., Tosi, F.\BCBL Mattoccia, S. \APACrefYearMonthDay2024March.
    \BBOQ\APACrefatitleRevisiting depth completion from a stereo matching perspective
    for cross-domain generalization Revisiting depth completion from a stereo matching
    perspective for cross-domain generalization.\BBCQ \APACrefbtitleInternational
    Conference on 3D Vision 2024 (3DV 2024). International conference on 3d vision
    2024 (3dv 2024). \PrintBackRefs\CurrentBib
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bartolomei \BOthers. [\APACyear2024] \APACinsertmetastarBartolomei_2024_3DV{APACrefauthors}Bartolomei,
    L., Poggi, M., Conti, A., Tosi, F.\BCBL Mattoccia, S. \APACrefYearMonthDay2024March.
    \BBOQ\APACrefatitle从立体匹配的角度重新审视深度补全 Revisiting depth completion from a stereo
    matching perspective for cross-domain generalization.\BBCQ \APACrefbtitle国际三维视觉会议2024
    (3DV 2024). International conference on 3d vision 2024 (3dv 2024). \PrintBackRefs\CurrentBib
- en: Bartolomei \BOthers. [\APACyear2023] \APACinsertmetastarBartolomei_2023_ICCV{APACrefauthors}Bartolomei,
    L., Poggi, M., Tosi, F., Conti, A.\BCBL Mattoccia, S. \APACrefYearMonthDay2023October.
    \BBOQ\APACrefatitleActive Stereo Without Pattern Projector Active stereo without
    pattern projector.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV) Proceedings of the ieee/cvf international
    conference on computer vision (iccv) (\BPG18470-18482). \PrintBackRefs\CurrentBib
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bartolomei \BOthers. [\APACyear2023] \APACinsertmetastarBartolomei_2023_ICCV{APACrefauthors}Bartolomei,
    L., Poggi, M., Tosi, F., Conti, A.\BCBL Mattoccia, S. \APACrefYearMonthDay2023October.
    \BBOQ\APACrefatitle无模式投影仪的主动立体视觉 Active stereo without pattern projector.\BBCQ
    \APACrefbtitleIEEE/CVF国际计算机视觉会议（ICCV）论文集 Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV) (\BPG18470-18482). \PrintBackRefs\CurrentBib
- en: 'Butler \BOthers. [\APACyear2012] \APACinsertmetastarbutler2012naturalistic{APACrefauthors}Butler,
    D.J., Wulff, J., Stanley, G.B.\BCBL Black, M.J. \APACrefYearMonthDay2012. \BBOQ\APACrefatitleA
    naturalistic open source movie for optical flow evaluation A naturalistic open
    source movie for optical flow evaluation.\BBCQ \APACrefbtitleComputer Vision–ECCV
    2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13,
    2012, Proceedings, Part VI 12 Computer vision–eccv 2012: 12th european conference
    on computer vision, florence, italy, october 7-13, 2012, proceedings, part vi
    12 (\BPGS611–625). \PrintBackRefs\CurrentBib'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Butler \BOthers. [\APACyear2012] \APACinsertmetastarbutler2012naturalistic{APACrefauthors}Butler,
    D.J., Wulff, J., Stanley, G.B.\BCBL Black, M.J. \APACrefYearMonthDay2012. \BBOQ\APACrefatitle用于光流评估的自然主义开源电影
    A naturalistic open source movie for optical flow evaluation.\BBCQ \APACrefbtitle计算机视觉–ECCV
    2012: 第十二届欧洲计算机视觉会议，意大利佛罗伦萨，2012年10月7-13日，论文集，第六部分 Computer Vision–ECCV 2012:
    12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012,
    Proceedings, Part VI 12 (\BPGS611–625). \PrintBackRefs\CurrentBib'
- en: Carranza-García \BOthers. [\APACyear2022] \APACinsertmetastarcarranza2022object{APACrefauthors}Carranza-García,
    M., Galán-Sales, F.J., Luna-Romera, J.M.\BCBL Riquelme, J.C. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleObject detection using depth completion and camera-LiDAR fusion
    for autonomous driving Object detection using depth completion and camera-lidar
    fusion for autonomous driving.\BBCQ \APACjournalVolNumPagesIntegrated Computer-Aided
    Engineering293241–258, \PrintBackRefs\CurrentBib
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carranza-García \BOthers. [\APACyear2022] \APACinsertmetastarcarranza2022object{APACrefauthors}Carranza-García,
    M., Galán-Sales, F.J., Luna-Romera, J.M.\BCBL Riquelme, J.C. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitle使用深度补全和相机-LiDAR融合进行自动驾驶的物体检测 Object detection using depth completion
    and camera-lidar fusion for autonomous driving.\BBCQ \APACjournalVolNumPagesIntegrated
    Computer-Aided Engineering293241–258, \PrintBackRefs\CurrentBib
- en: Charbonnier \BOthers. [\APACyear1994] \APACinsertmetastarcharbonnier1994two{APACrefauthors}Charbonnier,
    P., Blanc-Feraud, L., Aubert, G.\BCBL Barlaud, M. \APACrefYearMonthDay1994. \BBOQ\APACrefatitleTwo
    deterministic half-quadratic regularization algorithms for computed imaging Two
    deterministic half-quadratic regularization algorithms for computed imaging.\BBCQ
    \APACrefbtitleProceedings of 1st international conference on image processing
    Proceedings of 1st international conference on image processing (\BVOL2, \BPGS168–172).
    \PrintBackRefs\CurrentBib
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Charbonnier \BOthers. [\APACyear1994] \APACinsertmetastarcharbonnier1994two{APACrefauthors}Charbonnier,
    P., Blanc-Feraud, L., Aubert, G.\BCBL Barlaud, M. \APACrefYearMonthDay1994. \BBOQ\APACrefatitle用于计算成像的两种确定性半二次正则化算法
    Two deterministic half-quadratic regularization algorithms for computed imaging.\BBCQ
    \APACrefbtitle第一次国际图像处理会议论文集 Proceedings of 1st international conference on image
    processing (\BVOL2, \BPGS168–172). \PrintBackRefs\CurrentBib
- en: 'D. Chen \BOthers. [\APACyear2023] \APACinsertmetastarchen2023agg{APACrefauthors}Chen,
    D., Huang, T., Song, Z., Deng, S.\BCBL Jia, T. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleAGG-Net:
    Attention Guided Gated-convolutional Network for Depth Image Completion Agg-net:
    Attention guided gated-convolutional network for depth image completion.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF International Conference on Computer
    Vision Proceedings of the ieee/cvf international conference on computer vision (\BPGS8853–8862).
    \PrintBackRefs\CurrentBib'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈，D.\BOthers. [\APACyear2023] \APACinsertmetastarchen2023agg{APACrefauthors}陈，D.，黄，T.，宋，Z.，邓，S.\BCBL
    贾，T. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleAGG-Net：基于注意力的门控卷积网络用于深度图像补全
    AGG-Net：基于注意力的门控卷积网络用于深度图像补全。\BBCQ \APACrefbtitleIEEE/CVF国际计算机视觉会议论文集 IEEE/CVF国际计算机视觉会议论文集(\BPGS8853–8862).
    \PrintBackRefs\CurrentBib
- en: 'Y. Chen \BOthers. [\APACyear2020] \APACinsertmetastarchen2020dynamic{APACrefauthors}Chen,
    Y., Dai, X., Liu, M., Chen, D., Yuan, L.\BCBL Liu, Z. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleDynamic convolution: Attention over convolution kernels Dynamic
    convolution: Attention over convolution kernels.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF conference on computer vision and pattern recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS11030–11039).
    \PrintBackRefs\CurrentBib'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈，Y.\BOthers. [\APACyear2020] \APACinsertmetastarchen2020dynamic{APACrefauthors}陈，Y.，戴，X.，刘，M.，陈，D.，袁，L.\BCBL
    刘，Z. \APACrefYearMonthDay2020. \BBOQ\APACrefatitle动态卷积：对卷积核的关注 动态卷积：对卷积核的关注。\BBCQ
    \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集 IEEE/CVF计算机视觉与模式识别会议论文集(\BPGS11030–11039).
    \PrintBackRefs\CurrentBib
- en: 'Cheng \BOthers. [\APACyear2020] \APACinsertmetastarcheng2020cspn++{APACrefauthors}Cheng,
    X., Wang, P., Guan, C.\BCBL Yang, R. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleCspn++:
    Learning context and resource aware convolutional spatial propagation networks
    for depth completion Cspn++: Learning context and resource aware convolutional
    spatial propagation networks for depth completion.\BBCQ \APACrefbtitleProceedings
    of the AAAI Conference on Artificial Intelligence Proceedings of the aaai conference
    on artificial intelligence (\BVOL34, \BPGS10615–10622). \PrintBackRefs\CurrentBib'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成，\BOthers. [\APACyear2020] \APACinsertmetastarcheng2020cspn++{APACrefauthors}成，X.，王，P.，关，C.\BCBL
    杨，R. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleCSPN++：学习上下文和资源感知的卷积空间传播网络用于深度补全
    CSPN++：学习上下文和资源感知的卷积空间传播网络用于深度补全。\BBCQ \APACrefbtitleAAAI人工智能会议论文集 AAAI人工智能会议论文集(\BVOL34,
    \BPGS10615–10622). \PrintBackRefs\CurrentBib
- en: Cheng \BOthers. [\APACyear2018] \APACinsertmetastarcheng2018depth{APACrefauthors}Cheng,
    X., Wang, P.\BCBL Yang, R. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleDepth
    estimation via affinity learned with convolutional spatial propagation network
    Depth estimation via affinity learned with convolutional spatial propagation network.\BBCQ
    \APACrefbtitleProceedings of the European Conference on Computer Vision (ECCV)
    Proceedings of the european conference on computer vision (eccv) (\BPGS103–119).
    \PrintBackRefs\CurrentBib
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成，\BOthers. [\APACyear2018] \APACinsertmetastarcheng2018depth{APACrefauthors}成，X.，王，P.\BCBL
    杨，R. \APACrefYearMonthDay2018. \BBOQ\APACrefatitle通过卷积空间传播网络学习的亲和力进行深度估计 通过卷积空间传播网络学习的亲和力进行深度估计。\BBCQ
    \APACrefbtitle欧洲计算机视觉会议论文集 欧洲计算机视觉会议论文集(\BPGS103–119). \PrintBackRefs\CurrentBib
- en: Cho \BOthers. [\APACyear2021] \APACinsertmetastarcho2021deep{APACrefauthors}Cho,
    J., Min, D., Kim, Y.\BCBL Sohn, K. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleDeep
    monocular depth estimation leveraging a large-scale outdoor stereo dataset Deep
    monocular depth estimation leveraging a large-scale outdoor stereo dataset.\BBCQ
    \APACjournalVolNumPagesExpert Systems with Applications178114877, \PrintBackRefs\CurrentBib
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho，\BOthers. [\APACyear2021] \APACinsertmetastarcho2021deep{APACrefauthors}Cho，J.，Min，D.，Kim，Y.\BCBL
    Sohn，K. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle利用大规模户外立体数据集的深度单目深度估计 利用大规模户外立体数据集的深度单目深度估计。\BBCQ
    \APACjournalVolNumPages专家系统应用178114877， \PrintBackRefs\CurrentBib
- en: 'Chodosh \BOthers. [\APACyear2019] \APACinsertmetastarchodosh2019deep{APACrefauthors}Chodosh,
    N., Wang, C.\BCBL Lucey, S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDeep
    convolutional compressed sensing for lidar depth completion Deep convolutional
    compressed sensing for lidar depth completion.\BBCQ \APACrefbtitleComputer Vision–ACCV
    2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2–6,
    2018, Revised Selected Papers, Part I 14 Computer vision–accv 2018: 14th asian
    conference on computer vision, perth, australia, december 2–6, 2018, revised selected
    papers, part i 14 (\BPGS499–513). \PrintBackRefs\CurrentBib'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chodosh \BOthers. [\APACyear2019] \APACinsertmetastarchodosh2019deep{APACrefauthors}Chodosh,
    N., Wang, C.\BCBL Lucey, S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle深度卷积压缩感知用于激光雷达深度补全
    Deep convolutional compressed sensing for lidar depth completion.\BBCQ \APACrefbtitle计算机视觉–ACCV
    2018: 第十四届亚洲计算机视觉会议，澳大利亚珀斯，2018年12月2–6日，修订精选论文，第一部分 14 计算机视觉–accv 2018: 第十四届亚洲计算机视觉会议，澳大利亚珀斯，2018年12月2–6日，修订精选论文，第一部分
    14 (\BPGS499–513). \PrintBackRefs\CurrentBib'
- en: 'Choi \BOthers. [\APACyear2021] \APACinsertmetastarchoi2021selfdeco{APACrefauthors}Choi,
    J., Jung, D., Lee, Y., Kim, D., Manocha, D.\BCBL Lee, D. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleSelfdeco: Self-supervised monocular depth completion in challenging
    indoor environments Selfdeco: Self-supervised monocular depth completion in challenging
    indoor environments.\BBCQ \APACrefbtitle2021 IEEE International Conference on
    Robotics and Automation (ICRA) 2021 ieee international conference on robotics
    and automation (icra) (\BPGS467–474). \PrintBackRefs\CurrentBib'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Choi \BOthers. [\APACyear2021] \APACinsertmetastarchoi2021selfdeco{APACrefauthors}Choi,
    J., Jung, D., Lee, Y., Kim, D., Manocha, D.\BCBL Lee, D. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleSelfdeco: 自监督单目深度补全于挑战性室内环境 Selfdeco: Self-supervised monocular
    depth completion in challenging indoor environments.\BBCQ \APACrefbtitle2021 IEEE国际机器人与自动化会议
    (ICRA) 2021 ieee国际机器人与自动化会议 (icra) (\BPGS467–474). \PrintBackRefs\CurrentBib'
- en: 'Chugunov \BOthers. [\APACyear2021] \APACinsertmetastarchugunov2021mask{APACrefauthors}Chugunov,
    I., Baek, S\BHBIH., Fu, Q., Heidrich, W.\BCBL Heide, F. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleMask-tof: Learning microlens masks for flying pixel correction
    in time-of-flight imaging Mask-tof: Learning microlens masks for flying pixel
    correction in time-of-flight imaging.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf
    conference on computer vision and pattern recognition (\BPGS9116–9126). \PrintBackRefs\CurrentBib'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chugunov \BOthers. [\APACyear2021] \APACinsertmetastarchugunov2021mask{APACrefauthors}Chugunov,
    I., Baek, S\BHBIH., Fu, Q., Heidrich, W.\BCBL Heide, F. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleMask-tof: 学习微透镜掩模用于飞行像素校正的时间飞行成像 Mask-tof: Learning microlens
    masks for flying pixel correction in time-of-flight imaging.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集
    Proceedings of the ieee/cvf conference on computer vision and pattern recognition
    (\BPGS9116–9126). \PrintBackRefs\CurrentBib'
- en: 'Cong \BOthers. [\APACyear2018] \APACinsertmetastarcong2018hscs{APACrefauthors}Cong,
    R., Lei, J., Fu, H., Huang, Q., Cao, X.\BCBL Ling, N. \APACrefYearMonthDay2018.
    \BBOQ\APACrefatitleHSCS: Hierarchical sparsity based co-saliency detection for
    RGBD images Hscs: Hierarchical sparsity based co-saliency detection for rgbd images.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Multimedia2171660–1671, \PrintBackRefs\CurrentBib'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cong \BOthers. [\APACyear2018] \APACinsertmetastarcong2018hscs{APACrefauthors}Cong,
    R., Lei, J., Fu, H., Huang, Q., Cao, X.\BCBL Ling, N. \APACrefYearMonthDay2018.
    \BBOQ\APACrefatitleHSCS: 基于层次稀疏性的RGBD图像共同显著性检测 Hscs: Hierarchical sparsity based
    co-saliency detection for rgbd images.\BBCQ \APACjournalVolNumPagesIEEE多媒体事务2171660–1671,
    \PrintBackRefs\CurrentBib'
- en: Conti \BOthers. [\APACyear2023] \APACinsertmetastarconti2023sparsity{APACrefauthors}Conti,
    A., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleSparsity
    Agnostic Depth Completion Sparsity agnostic depth completion.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision Proceedings
    of the ieee/cvf winter conference on applications of computer vision (\BPGS5871–5880).
    \PrintBackRefs\CurrentBib
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conti \BOthers. [\APACyear2023] \APACinsertmetastarconti2023sparsity{APACrefauthors}Conti,
    A., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2023. \BBOQ\APACrefatitle稀疏性无关的深度补全
    Sparsity agnostic depth completion.\BBCQ \APACrefbtitleIEEE/CVF冬季计算机视觉应用会议论文集
    Proceedings of the ieee/cvf winter conference on applications of computer vision
    (\BPGS5871–5880). \PrintBackRefs\CurrentBib
- en: De Lutio \BOthers. [\APACyear2022] \APACinsertmetastarde2022learning{APACrefauthors}De Lutio,
    R., Becker, A., D’Aronco, S., Russo, S., Wegner, J.D.\BCBL Schindler, K. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleLearning graph regularisation for guided super-resolution Learning
    graph regularisation for guided super-resolution.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS1979–1988).
    \PrintBackRefs\CurrentBib
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Lutio \BOthers. [\APACyear2022] \APACinsertmetastarde2022learning{APACrefauthors}De
    Lutio, R., Becker, A., D’Aronco, S., Russo, S., Wegner, J.D.\BCBL Schindler, K.
    \APACrefYearMonthDay2022. \BBOQ\APACrefatitleLearning graph regularisation for
    guided super-resolution Learning graph regularisation for guided super-resolution.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS1979–1988). \PrintBackRefs\CurrentBib
- en: P. Deng \BOthers. [\APACyear2022] \APACinsertmetastardeng2022multi{APACrefauthors}Deng,
    P., Ge, C., Qiao, X.\BCBL Wei, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleMulti-stream
    Face Anti-spoofing System Using 3D Information Multi-stream face anti-spoofing
    system using 3d information.\BBCQ \APACrefbtitle2022 IEEE International Conference
    on Consumer Electronics (ICCE) 2022 ieee international conference on consumer
    electronics (icce) (\BPGS1–6). \PrintBackRefs\CurrentBib
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P. Deng \BOthers. [\APACyear2022] \APACinsertmetastardeng2022multi{APACrefauthors}Deng,
    P., Ge, C., Qiao, X.\BCBL Wei, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleMulti-stream
    Face Anti-spoofing System Using 3D Information Multi-stream face anti-spoofing
    system using 3d information.\BBCQ \APACrefbtitle2022 IEEE International Conference
    on Consumer Electronics (ICCE) 2022 ieee international conference on consumer
    electronics (icce) (\BPGS1–6). \PrintBackRefs\CurrentBib
- en: X. Deng \BBA Dragotti [\APACyear2019] \APACinsertmetastardeng2019coupled{APACrefauthors}Deng,
    X.\BCBT \BBA Dragotti, P.L. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleCoupled
    ista network for multi-modal image super-resolution Coupled ista network for multi-modal
    image super-resolution.\BBCQ \APACrefbtitleICASSP 2019-2019 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP) Icassp 2019-2019
    ieee international conference on acoustics, speech and signal processing (icassp) (\BPGS1862–1866).
    \PrintBackRefs\CurrentBib
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X. Deng \BBA Dragotti [\APACyear2019] \APACinsertmetastardeng2019coupled{APACrefauthors}Deng,
    X.\BCBT \BBA Dragotti, P.L. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleCoupled
    ista network for multi-modal image super-resolution Coupled ista network for multi-modal
    image super-resolution.\BBCQ \APACrefbtitleICASSP 2019-2019 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP) Icassp 2019-2019
    ieee international conference on acoustics, speech and signal processing (icassp)
    (\BPGS1862–1866). \PrintBackRefs\CurrentBib
- en: X. Deng \BBA Dragotti [\APACyear2020] \APACinsertmetastardeng2020deep{APACrefauthors}Deng,
    X.\BCBT \BBA Dragotti, P.L. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDeep
    convolutional neural network for multi-modal image restoration and fusion Deep
    convolutional neural network for multi-modal image restoration and fusion.\BBCQ
    \APACjournalVolNumPagesIEEE transactions on pattern analysis and machine intelligence43103333–3348,
    \PrintBackRefs\CurrentBib
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X. Deng \BBA Dragotti [\APACyear2020] \APACinsertmetastardeng2020deep{APACrefauthors}Deng,
    X.\BCBT \BBA Dragotti, P.L. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDeep
    convolutional neural network for multi-modal image restoration and fusion Deep
    convolutional neural network for multi-modal image restoration and fusion.\BBCQ
    \APACjournalVolNumPagesIEEE transactions on pattern analysis and machine intelligence43103333–3348,
    \PrintBackRefs\CurrentBib
- en: 'Déziel \BOthers. [\APACyear2021] \APACinsertmetastardeziel2021pixset{APACrefauthors}Déziel,
    J\BHBIL., Merriaux, P., Tremblay, F., Lessard, D., Plourde, D., Stanguennec, J.\BDBLOlivier,
    P. \APACrefYearMonthDay2021. \BBOQ\APACrefatitlePixset: An opportunity for 3D
    computer vision to go beyond point clouds with a full-waveform LiDAR dataset Pixset:
    An opportunity for 3d computer vision to go beyond point clouds with a full-waveform
    lidar dataset.\BBCQ \APACrefbtitle2021 IEEE International Intelligent Transportation
    Systems Conference (ITSC) 2021 ieee international intelligent transportation systems
    conference (itsc) (\BPGS2987–2993). \PrintBackRefs\CurrentBib'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Déziel \BOthers. [\APACyear2021] \APACinsertmetastardeziel2021pixset{APACrefauthors}Déziel,
    J\BHBIL., Merriaux, P., Tremblay, F., Lessard, D., Plourde, D., Stanguennec, J.\BDBLOlivier,
    P. \APACrefYearMonthDay2021. \BBOQ\APACrefatitlePixset: An opportunity for 3D
    computer vision to go beyond point clouds with a full-waveform LiDAR dataset Pixset:
    An opportunity for 3d computer vision to go beyond point clouds with a full-waveform
    lidar dataset.\BBCQ \APACrefbtitle2021 IEEE International Intelligent Transportation
    Systems Conference (ITSC) 2021 ieee international intelligent transportation systems
    conference (itsc) (\BPGS2987–2993). \PrintBackRefs\CurrentBib'
- en: Diebel \BBA Thrun [\APACyear2005] \APACinsertmetastardiebel2005application{APACrefauthors}Diebel,
    J.\BCBT \BBA Thrun, S. \APACrefYearMonthDay2005. \BBOQ\APACrefatitleAn application
    of markov random fields to range sensing An application of markov random fields
    to range sensing.\BBCQ \APACjournalVolNumPagesAdvances in neural information processing
    systems18, \PrintBackRefs\CurrentBib
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diebel \BBA Thrun [\APACyear2005] \APACinsertmetastardiebel2005application{APACrefauthors}Diebel,
    J.\BCBT \BBA Thrun, S. \APACrefYearMonthDay2005. \BBOQ\APACrefatitle将马尔可夫随机场应用于范围感知
    将马尔可夫随机场应用于范围感知。\BBCQ \APACjournalVolNumPages《神经信息处理系统进展》18卷，\PrintBackRefs\CurrentBib
- en: J. Dong \BOthers. [\APACyear2021] \APACinsertmetastardong2021learning{APACrefauthors}Dong,
    J., Pan, J., Ren, J.S., Lin, L., Tang, J.\BCBL Yang, M\BHBIH. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleLearning spatially variant linear representation models for
    joint filtering Learning spatially variant linear representation models for joint
    filtering.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Pattern Analysis and
    Machine Intelligence44118355–8370, \PrintBackRefs\CurrentBib
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. Dong \BOthers. [\APACyear2021] \APACinsertmetastardong2021learning{APACrefauthors}Dong,
    J., Pan, J., Ren, J.S., Lin, L., Tang, J.\BCBL Yang, M\BHBIH. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitle学习空间变异线性表示模型以进行联合滤波 学习空间变异线性表示模型以进行联合滤波。\BBCQ \APACjournalVolNumPages《IEEE模式分析与机器智能汇刊》44卷11835–8370，\PrintBackRefs\CurrentBib
- en: 'X. Dong \BOthers. [\APACyear2022] \APACinsertmetastardong2022learning{APACrefauthors}Dong,
    X., Yokoya, N., Wang, L.\BCBL Uezato, T. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleLearning
    Mutual Modulation for Self-supervised Cross-Modal Super-Resolution Learning mutual
    modulation for self-supervised cross-modal super-resolution.\BBCQ \APACrefbtitleComputer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part XIX Computer vision–eccv 2022: 17th european conference, tel
    aviv, israel, october 23–27, 2022, proceedings, part xix (\BPGS1–18). \PrintBackRefs\CurrentBib'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X. Dong \BOthers. [\APACyear2022] \APACinsertmetastardong2022learning{APACrefauthors}Dong,
    X., Yokoya, N., Wang, L.\BCBL Uezato, T. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle学习互调制以进行自监督的跨模态超分辨率
    学习互调制以进行自监督的跨模态超分辨率。\BBCQ \APACrefbtitle《计算机视觉–ECCV 2022：第17届欧洲会议，特拉维夫，以色列，2022年10月23–27日，论文集，第十九部分》
    《计算机视觉–ECCV 2022：第17届欧洲会议，特拉维夫，以色列，2022年10月23–27日，论文集，第十九部分》(\BPGS1–18)。 \PrintBackRefs\CurrentBib
- en: Eigen \BBA Fergus [\APACyear2015] \APACinsertmetastareigen2015predicting{APACrefauthors}Eigen,
    D.\BCBT \BBA Fergus, R. \APACrefYearMonthDay2015. \BBOQ\APACrefatitlePredicting
    depth, surface normals and semantic labels with a common multi-scale convolutional
    architecture Predicting depth, surface normals and semantic labels with a common
    multi-scale convolutional architecture.\BBCQ \APACrefbtitleProceedings of the
    IEEE International Conference on Computer Vision Proceedings of the ieee international
    conference on computer vision (\BPGS2650–2658). \PrintBackRefs\CurrentBib
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eigen \BBA Fergus [\APACyear2015] \APACinsertmetastareigen2015predicting{APACrefauthors}Eigen,
    D.\BCBT \BBA Fergus, R. \APACrefYearMonthDay2015. \BBOQ\APACrefatitle使用共同的多尺度卷积架构预测深度、表面法线和语义标签
    使用共同的多尺度卷积架构预测深度、表面法线和语义标签。\BBCQ \APACrefbtitle《IEEE国际计算机视觉大会论文集》 《IEEE国际计算机视觉大会论文集》(\BPGS2650–2658)。
    \PrintBackRefs\CurrentBib
- en: 'Eldesokey \BOthers. [\APACyear2020] \APACinsertmetastareldesokey2020uncertainty{APACrefauthors}Eldesokey,
    A., Felsberg, M., Holmquist, K.\BCBL Persson, M. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleUncertainty-aware
    cnns for depth completion: Uncertainty from beginning to end Uncertainty-aware
    cnns for depth completion: Uncertainty from beginning to end.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS12014–12023).
    \PrintBackRefs\CurrentBib'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eldesokey \BOthers. [\APACyear2020] \APACinsertmetastareldesokey2020uncertainty{APACrefauthors}Eldesokey,
    A., Felsberg, M., Holmquist, K.\BCBL Persson, M. \APACrefYearMonthDay2020. \BBOQ\APACrefatitle不确定性感知的卷积神经网络用于深度补全：从头到尾的不确定性
    不确定性感知的卷积神经网络用于深度补全：从头到尾的不确定性。\BBCQ \APACrefbtitle《IEEE/CVF计算机视觉与模式识别大会论文集》 《IEEE/CVF计算机视觉与模式识别大会论文集》(\BPGS12014–12023)。
    \PrintBackRefs\CurrentBib
- en: Eldesokey \BOthers. [\APACyear2018] \APACinsertmetastareldesokey2018propagating{APACrefauthors}Eldesokey,
    A., Felsberg, M.\BCBL Khan, F.S. \APACrefYearMonthDay2018. \BBOQ\APACrefatitlePropagating
    confidences through cnns for sparse data regression Propagating confidences through
    cnns for sparse data regression.\BBCQ \APACjournalVolNumPagesarXiv preprint arXiv:1805.11913,
    \PrintBackRefs\CurrentBib
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eldesokey \BOthers. [\APACyear2018] \APACinsertmetastareldesokey2018propagating{APACrefauthors}Eldesokey,
    A., Felsberg, M.\BCBL Khan, F.S. \APACrefYearMonthDay2018. \BBOQ\APACrefatitle通过卷积神经网络传播置信度以进行稀疏数据回归
    Propagating confidences through cnns for sparse data regression.\BBCQ \APACjournalVolNumPagesarXiv预印本
    arXiv:1805.11913, \PrintBackRefs\CurrentBib
- en: Eldesokey \BOthers. [\APACyear2019] \APACinsertmetastareldesokey2019confidence{APACrefauthors}Eldesokey,
    A., Felsberg, M.\BCBL Khan, F.S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleConfidence
    propagation through cnns for guided sparse depth regression Confidence propagation
    through cnns for guided sparse depth regression.\BBCQ \APACjournalVolNumPagesIEEE
    transactions on pattern analysis and machine intelligence42102423–2436, \PrintBackRefs\CurrentBib
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eldesokey \BOthers. [\APACyear2019] \APACinsertmetastareldesokey2019confidence{APACrefauthors}Eldesokey,
    A., Felsberg, M.\BCBL Khan, F.S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle通过卷积神经网络传播置信度以进行引导稀疏深度回归
    Confidence propagation through cnns for guided sparse depth regression.\BBCQ \APACjournalVolNumPagesIEEE模式分析与机器智能学报
    42102423–2436, \PrintBackRefs\CurrentBib
- en: Fan \BOthers. [\APACyear2022] \APACinsertmetastarFan_2022_BMVC{APACrefauthors}Fan,
    R., Li, Z., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleA
    Cascade Dense Connection Fusion Network for Depth Completion A cascade dense connection
    fusion network for depth completion.\BBCQ \APACrefbtitle33rd British Machine Vision
    Conference 2022, BMVC 2022, London, UK, November 21-24, 2022\. 33rd british machine
    vision conference 2022, BMVC 2022, london, uk, november 21-24, 2022. \PrintBackRefs\CurrentBib
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan \BOthers. [\APACyear2022] \APACinsertmetastarFan_2022_BMVC{APACrefauthors}Fan,
    R., Li, Z., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle一个级联密集连接融合网络用于深度补全
    A cascade dense connection fusion network for depth completion.\BBCQ \APACrefbtitle第33届英国机器视觉会议
    2022，BMVC 2022，伦敦，英国，2022年11月21-24日\. 第33届英国机器视觉会议 2022，BMVC 2022，伦敦，英国，2022年11月21-24日。
    \PrintBackRefs\CurrentBib
- en: Feng \BOthers. [\APACyear2023] \APACinsertmetastarfeng2023generating{APACrefauthors}Feng,
    R., Li, C., Chen, H., Li, S., Gu, J.\BCBL Loy, C.C. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleGenerating Aligned Pseudo-Supervision from Non-Aligned Data
    for Image Restoration in Under-Display Camera Generating aligned pseudo-supervision
    from non-aligned data for image restoration in under-display camera.\BBCQ \APACjournalVolNumPagesarXiv
    preprint arXiv:2304.06019, \PrintBackRefs\CurrentBib
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng \BOthers. [\APACyear2023] \APACinsertmetastarfeng2023generating{APACrefauthors}Feng,
    R., Li, C., Chen, H., Li, S., Gu, J.\BCBL Loy, C.C. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitle从非对齐数据生成对齐伪监督用于显示下摄像头的图像恢复 Generating aligned pseudo-supervision
    from non-aligned data for image restoration in under-display camera.\BBCQ \APACjournalVolNumPagesarXiv预印本
    arXiv:2304.06019, \PrintBackRefs\CurrentBib
- en: Feng \BOthers. [\APACyear2021] \APACinsertmetastarfeng2021removing{APACrefauthors}Feng,
    R., Li, C., Chen, H., Li, S., Loy, C.C.\BCBL Gu, J. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleRemoving diffraction image artifacts in under-display camera
    via dynamic skip connection network Removing diffraction image artifacts in under-display
    camera via dynamic skip connection network.\BBCQ \APACrefbtitleProceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS662–671).
    \PrintBackRefs\CurrentBib
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng \BOthers. [\APACyear2021] \APACinsertmetastarfeng2021removing{APACrefauthors}Feng,
    R., Li, C., Chen, H., Li, S., Loy, C.C.\BCBL Gu, J. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitle通过动态跳跃连接网络去除显示下摄像头中的衍射图像伪影 Removing diffraction image artifacts
    in under-display camera via dynamic skip connection network.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (\BPGS662–671). \PrintBackRefs\CurrentBib
- en: Ferstl \BOthers. [\APACyear2013] \APACinsertmetastarferstl2013image{APACrefauthors}Ferstl,
    D., Reinbacher, C., Ranftl, R., Rüther, M.\BCBL Bischof, H. \APACrefYearMonthDay2013.
    \BBOQ\APACrefatitleImage guided depth upsampling using anisotropic total generalized
    variation Image guided depth upsampling using anisotropic total generalized variation.\BBCQ
    \APACrefbtitleProceedings of the IEEE international conference on computer vision
    Proceedings of the ieee international conference on computer vision (\BPGS993–1000).
    \PrintBackRefs\CurrentBib
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferstl \BOthers. [\APACyear2013] \APACinsertmetastarferstl2013image{APACrefauthors}Ferstl,
    D., Reinbacher, C., Ranftl, R., Rüther, M.\BCBL Bischof, H. \APACrefYearMonthDay2013.
    \BBOQ\APACrefatitle图像引导深度上采样使用各向异性总广义变差 Image guided depth upsampling using anisotropic
    total generalized variation.\BBCQ \APACrefbtitle《IEEE国际计算机视觉会议论文集》(\BPGS993–1000).
    \PrintBackRefs\CurrentBib
- en: Figueiredo [\APACyear2001] \APACinsertmetastarfigueiredo2001adaptive{APACrefauthors}Figueiredo,
    M. \APACrefYearMonthDay2001. \BBOQ\APACrefatitleAdaptive sparseness using Jeffreys
    prior Adaptive sparseness using jeffreys prior.\BBCQ \APACjournalVolNumPagesAdvances
    in neural information processing systems14, \PrintBackRefs\CurrentBib
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Figueiredo [\APACyear2001] \APACinsertmetastarfigueiredo2001adaptive{APACrefauthors}Figueiredo,
    M. \APACrefYearMonthDay2001. \BBOQ\APACrefatitle使用Jeffreys先验的自适应稀疏性 Adaptive sparseness
    using jeffreys prior.\BBCQ \APACjournalVolNumPages《神经信息处理系统进展》14卷， \PrintBackRefs\CurrentBib
- en: Gaidon \BOthers. [\APACyear2016] \APACinsertmetastargaidon2016virtual{APACrefauthors}Gaidon,
    A., Wang, Q., Cabon, Y.\BCBL Vig, E. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleVirtual
    worlds as proxy for multi-object tracking analysis Virtual worlds as proxy for
    multi-object tracking analysis.\BBCQ \APACrefbtitleProceedings of the IEEE conference
    on computer vision and pattern recognition Proceedings of the ieee conference
    on computer vision and pattern recognition (\BPGS4340–4349). \PrintBackRefs\CurrentBib
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gaidon \BOthers. [\APACyear2016] \APACinsertmetastargaidon2016virtual{APACrefauthors}Gaidon,
    A., Wang, Q., Cabon, Y.\BCBL Vig, E. \APACrefYearMonthDay2016. \BBOQ\APACrefatitle虚拟世界作为多目标跟踪分析的代理
    Virtual worlds as proxy for multi-object tracking analysis.\BBCQ \APACrefbtitle《IEEE计算机视觉与模式识别会议论文集》(\BPGS4340–4349).
    \PrintBackRefs\CurrentBib
- en: Ge \BOthers. [\APACyear2021] \APACinsertmetastarge2021tof{APACrefauthors}Ge,
    C., Qiao, X., Huimin, Y., Zhou, Y.\BCBL Deng, P. \APACrefYearMonthDay2021\APACmonth10 12.
    \APACrefbtitleTOF depth sensor based on laser speckle projection and distance
    measuring method thereof. Tof depth sensor based on laser speckle projection and
    distance measuring method thereof. \APACaddressPublisherGoogle Patents. \APACrefnoteUS
    Patent 11,143,880 \PrintBackRefs\CurrentBib
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge \BOthers. [\APACyear2021] \APACinsertmetastarge2021tof{APACrefauthors}Ge,
    C., Qiao, X., Huimin, Y., Zhou, Y.\BCBL Deng, P. \APACrefYearMonthDay2021\APACmonth10
    12. \APACrefbtitle基于激光散斑投影的TOF深度传感器及其测距方法 Tof depth sensor based on laser speckle
    projection and distance measuring method thereof. \APACaddressPublisherGoogle
    Patents. \APACrefnote美国专利11,143,880 \PrintBackRefs\CurrentBib
- en: Godard \BOthers. [\APACyear2017] \APACinsertmetastargodard2017unsupervised{APACrefauthors}Godard,
    C., Mac Aodha, O.\BCBL Brostow, G.J. \APACrefYearMonthDay2017. \BBOQ\APACrefatitleUnsupervised
    monocular depth estimation with left-right consistency Unsupervised monocular
    depth estimation with left-right consistency.\BBCQ \APACrefbtitleProceedings of
    the IEEE conference on computer vision and pattern recognition Proceedings of
    the ieee conference on computer vision and pattern recognition (\BPGS270–279).
    \PrintBackRefs\CurrentBib
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Godard \BOthers. [\APACyear2017] \APACinsertmetastargodard2017unsupervised{APACrefauthors}Godard,
    C., Mac Aodha, O.\BCBL Brostow, G.J. \APACrefYearMonthDay2017. \BBOQ\APACrefatitle无监督单目深度估计与左右一致性
    Unsupervised monocular depth estimation with left-right consistency.\BBCQ \APACrefbtitle《IEEE计算机视觉与模式识别会议论文集》(\BPGS270–279).
    \PrintBackRefs\CurrentBib
- en: Godard \BOthers. [\APACyear2019] \APACinsertmetastargodard2019digging{APACrefauthors}Godard,
    C., Mac Aodha, O., Firman, M.\BCBL Brostow, G.J. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDigging
    into self-supervised monocular depth estimation Digging into self-supervised monocular
    depth estimation.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF international
    conference on computer vision Proceedings of the ieee/cvf international conference
    on computer vision (\BPGS3828–3838). \PrintBackRefs\CurrentBib
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Godard \BOthers. [\APACyear2019] \APACinsertmetastargodard2019digging{APACrefauthors}Godard,
    C., Mac Aodha, O., Firman, M.\BCBL Brostow, G.J. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle深入研究自监督单目深度估计
    Digging into self-supervised monocular depth estimation.\BBCQ \APACrefbtitle《IEEE/CVF国际计算机视觉会议论文集》(\BPGS3828–3838).
    \PrintBackRefs\CurrentBib
- en: 'J. Gu \BOthers. [\APACyear2021] \APACinsertmetastargu2021denselidar{APACrefauthors}Gu,
    J., Xiang, Z., Ye, Y.\BCBL Wang, L. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleDenseLiDAR:
    A real-time pseudo dense depth guided depth completion network Denselidar: A real-time
    pseudo dense depth guided depth completion network.\BBCQ \APACjournalVolNumPagesIEEE
    Robotics and Automation Letters621808–1815, \PrintBackRefs\CurrentBib'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'J. Gu \BOthers. [\APACyear2021] \APACinsertmetastargu2021denselidar{APACrefauthors}Gu,
    J., Xiang, Z., Ye, Y.\BCBL Wang, L. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleDenseLiDAR:
    A real-time pseudo dense depth guided depth completion network Denselidar: A real-time
    pseudo dense depth guided depth completion network.\BBCQ \APACjournalVolNumPagesIEEE
    Robotics and Automation Letters621808–1815, \PrintBackRefs\CurrentBib'
- en: S. Gu \BOthers. [\APACyear2019] \APACinsertmetastargu2019learned{APACrefauthors}Gu,
    S., Guo, S., Zuo, W., Chen, Y., Timofte, R., Van Gool, L.\BCBL Zhang, L. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleLearned dynamic guidance for depth image reconstruction Learned
    dynamic guidance for depth image reconstruction.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Pattern Analysis and Machine Intelligence42102437–2452, \PrintBackRefs\CurrentBib
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Gu \BOthers. [\APACyear2019] \APACinsertmetastargu2019learned{APACrefauthors}Gu,
    S., Guo, S., Zuo, W., Chen, Y., Timofte, R., Van Gool, L.\BCBL Zhang, L. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleLearned dynamic guidance for depth image reconstruction Learned
    dynamic guidance for depth image reconstruction.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Pattern Analysis and Machine Intelligence42102437–2452, \PrintBackRefs\CurrentBib
- en: Guizilini \BOthers. [\APACyear2021] \APACinsertmetastarguizilini2021sparse{APACrefauthors}Guizilini,
    V., Ambrus, R., Burgard, W.\BCBL Gaidon, A. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleSparse
    auxiliary networks for unified monocular depth prediction and completion Sparse
    auxiliary networks for unified monocular depth prediction and completion.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS11078–11088). \PrintBackRefs\CurrentBib
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guizilini \BOthers. [\APACyear2021] \APACinsertmetastarguizilini2021sparse{APACrefauthors}Guizilini,
    V., Ambrus, R., Burgard, W.\BCBL Gaidon, A. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleSparse
    auxiliary networks for unified monocular depth prediction and completion Sparse
    auxiliary networks for unified monocular depth prediction and completion.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS11078–11088). \PrintBackRefs\CurrentBib
- en: Guizilini, Ambrus\BCBL \BOthers. [\APACyear2020] \APACinsertmetastarguizilini20203d{APACrefauthors}Guizilini,
    V., Ambrus, R., Pillai, S., Raventos, A.\BCBL Gaidon, A. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitle3d packing for self-supervised monocular depth estimation 3d
    packing for self-supervised monocular depth estimation.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF conference on computer vision and pattern recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS2485–2494).
    \PrintBackRefs\CurrentBib
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guizilini, Ambrus\BCBL \BOthers. [\APACyear2020] \APACinsertmetastarguizilini20203d{APACrefauthors}Guizilini,
    V., Ambrus, R., Pillai, S., Raventos, A.\BCBL Gaidon, A. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitle3d packing for self-supervised monocular depth estimation 3d
    packing for self-supervised monocular depth estimation.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF conference on computer vision and pattern recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS2485–2494).
    \PrintBackRefs\CurrentBib
- en: Guizilini, Li\BCBL \BOthers. [\APACyear2020] \APACinsertmetastarguizilini2020robust{APACrefauthors}Guizilini,
    V., Li, J., Ambrus, R., Pillai, S.\BCBL Gaidon, A. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleRobust
    semi-supervised monocular depth estimation with reprojected distances Robust semi-supervised
    monocular depth estimation with reprojected distances.\BBCQ \APACrefbtitleConference
    on robot learning Conference on robot learning (\BPGS503–512). \PrintBackRefs\CurrentBib
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guizilini, Li\BCBL \BOthers. [\APACyear2020] \APACinsertmetastarguizilini2020robust{APACrefauthors}Guizilini,
    V., Li, J., Ambrus, R., Pillai, S.\BCBL Gaidon, A. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleRobust
    semi-supervised monocular depth estimation with reprojected distances Robust semi-supervised
    monocular depth estimation with reprojected distances.\BBCQ \APACrefbtitleConference
    on robot learning Conference on robot learning (\BPGS503–512). \PrintBackRefs\CurrentBib
- en: Guo \BOthers. [\APACyear2019] \APACinsertmetastarguo2018hierarchical{APACrefauthors}Guo,
    C., Li, C., Guo, J., Cong, R., Fu, H.\BCBL Han, P. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleHierarchical
    Features Driven Residual Learning for Depth Map Super-Resolution Hierarchical
    features driven residual learning for depth map super-resolution.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Image Processing2852545-2557, {APACrefDOI}  [https://doi.org/10.1109/TIP.2018.2887029](https://doi.org/10.1109/TIP.2018.2887029)
    \PrintBackRefs\CurrentBib
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo \BOthers. [\APACyear2019] \APACinsertmetastarguo2018hierarchical{APACrefauthors}Guo,
    C., Li, C., Guo, J., Cong, R., Fu, H.\BCBL Han, P. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle层次特征驱动的残差学习用于深度图像超分辨率
    层次特征驱动的残差学习用于深度图像超分辨率。\BBCQ \APACjournalVolNumPagesIEEE Transactions on Image
    Processing2852545-2557, {APACrefDOI}  [https://doi.org/10.1109/TIP.2018.2887029](https://doi.org/10.1109/TIP.2018.2887029)
    \PrintBackRefs\CurrentBib
- en: 'Gupta \BOthers. [\APACyear2015] \APACinsertmetastargupta2015phasor{APACrefauthors}Gupta,
    M., Nayar, S.K., Hullin, M.B.\BCBL Martin, J. \APACrefYearMonthDay2015. \BBOQ\APACrefatitlePhasor
    imaging: A generalization of correlation-based time-of-flight imaging Phasor imaging:
    A generalization of correlation-based time-of-flight imaging.\BBCQ \APACjournalVolNumPagesACM
    Transactions on Graphics (ToG)3451–18, \PrintBackRefs\CurrentBib'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta \BOthers. [\APACyear2015] \APACinsertmetastargupta2015phasor{APACrefauthors}Gupta,
    M., Nayar, S.K., Hullin, M.B.\BCBL Martin, J. \APACrefYearMonthDay2015. \BBOQ\APACrefatitle相位成像：基于相关的飞行时间成像的推广
    相位成像：基于相关的飞行时间成像的推广。\BBCQ \APACjournalVolNumPagesACM Transactions on Graphics
    (ToG)3451–18, \PrintBackRefs\CurrentBib
- en: 'Gutierrez-Barragan \BOthers. [\APACyear2021] \APACinsertmetastargutierrez2021itof2dtof{APACrefauthors}Gutierrez-Barragan,
    F., Chen, H., Gupta, M., Velten, A.\BCBL Gu, J. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleitof2dtof:
    A robust and flexible representation for data-driven time-of-flight imaging itof2dtof:
    A robust and flexible representation for data-driven time-of-flight imaging.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Computational Imaging71205–1214, \PrintBackRefs\CurrentBib'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gutierrez-Barragan \BOthers. [\APACyear2021] \APACinsertmetastargutierrez2021itof2dtof{APACrefauthors}Gutierrez-Barragan,
    F., Chen, H., Gupta, M., Velten, A.\BCBL Gu, J. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleitof2dtof：一种用于数据驱动飞行时间成像的鲁棒且灵活的表示
    itof2dtof：一种用于数据驱动飞行时间成像的鲁棒且灵活的表示。\BBCQ \APACjournalVolNumPagesIEEE Transactions
    on Computational Imaging71205–1214, \PrintBackRefs\CurrentBib
- en: 'Häne \BOthers. [\APACyear2017] \APACinsertmetastarhane20173d{APACrefauthors}Häne,
    C., Heng, L., Lee, G.H., Fraundorfer, F., Furgale, P., Sattler, T.\BCBL Pollefeys,
    M. \APACrefYearMonthDay2017. \BBOQ\APACrefatitle3D visual perception for self-driving
    cars using a multi-camera system: Calibration, mapping, localization, and obstacle
    detection 3d visual perception for self-driving cars using a multi-camera system:
    Calibration, mapping, localization, and obstacle detection.\BBCQ \APACjournalVolNumPagesImage
    and Vision Computing6814–27, \PrintBackRefs\CurrentBib'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Häne \BOthers. [\APACyear2017] \APACinsertmetastarhane20173d{APACrefauthors}Häne,
    C., Heng, L., Lee, G.H., Fraundorfer, F., Furgale, P., Sattler, T.\BCBL Pollefeys,
    M. \APACrefYearMonthDay2017. \BBOQ\APACrefatitle用于自动驾驶汽车的3D视觉感知，使用多摄像头系统：标定、映射、定位和障碍物检测
    用于自动驾驶汽车的3D视觉感知，使用多摄像头系统：标定、映射、定位和障碍物检测。\BBCQ \APACjournalVolNumPagesImage and
    Vision Computing6814–27, \PrintBackRefs\CurrentBib
- en: K. He \BOthers. [\APACyear2010] \APACinsertmetastarhe2010guided{APACrefauthors}He,
    K., Sun, J.\BCBL Tang, X. \APACrefYearMonthDay2010. \BBOQ\APACrefatitleGuided
    image filtering Guided image filtering.\BBCQ \APACrefbtitleEuropean conference
    on computer vision European conference on computer vision (\BPGS1–14). \PrintBackRefs\CurrentBib
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K. He \BOthers. [\APACyear2010] \APACinsertmetastarhe2010guided{APACrefauthors}He,
    K., Sun, J.\BCBL Tang, X. \APACrefYearMonthDay2010. \BBOQ\APACrefatitle引导图像滤波
    引导图像滤波。\BBCQ \APACrefbtitle欧洲计算机视觉大会 欧洲计算机视觉大会 (\BPGS1–14). \PrintBackRefs\CurrentBib
- en: K. He \BOthers. [\APACyear2012] \APACinsertmetastarhe2012guided{APACrefauthors}He,
    K., Sun, J.\BCBL Tang, X. \APACrefYearMonthDay2012. \BBOQ\APACrefatitleGuided
    image filtering Guided image filtering.\BBCQ \APACjournalVolNumPagesIEEE transactions
    on pattern analysis and machine intelligence3561397–1409, \PrintBackRefs\CurrentBib
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K. He \BOthers. [\APACyear2012] \APACinsertmetastarhe2012guided{APACrefauthors}He,
    K., Sun, J.\BCBL Tang, X. \APACrefYearMonthDay2012. \BBOQ\APACrefatitle引导图像滤波
    引导图像滤波。\BBCQ \APACjournalVolNumPagesIEEE Transactions on Pattern Analysis and
    Machine Intelligence3551397–1409, \PrintBackRefs\CurrentBib
- en: 'L. He \BOthers. [\APACyear2021] \APACinsertmetastarhe2021towards{APACrefauthors}He,
    L., Zhu, H., Li, F., Bai, H., Cong, R., Zhang, C.\BDBLZhao, Y. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleTowards fast and accurate real-world depth super-resolution:
    Benchmark dataset and baseline Towards fast and accurate real-world depth super-resolution:
    Benchmark dataset and baseline.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf
    conference on computer vision and pattern recognition (\BPGS9229–9238). \PrintBackRefs\CurrentBib'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L. He 等人. [2021年] \APACinsertmetastarhe2021towards{APACrefauthors}He, L., Zhu,
    H., Li, F., Bai, H., Cong, R., Zhang, C.\BDBLZhao, Y. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitle快速准确的真实世界深度超分辨率：基准数据集与基线 快速准确的真实世界深度超分辨率：基准数据集与基线.\BBCQ \APACrefbtitleIEEE/CVF
    计算机视觉与模式识别会议论文集 IEEE/CVF 计算机视觉与模式识别会议论文集 (\BPGS9229–9238). \PrintBackRefs\CurrentBib
- en: Heide \BOthers. [\APACyear2015] \APACinsertmetastarheide2015doppler{APACrefauthors}Heide,
    F., Heidrich, W., Hullin, M.\BCBL Wetzstein, G. \APACrefYearMonthDay2015. \BBOQ\APACrefatitleDoppler
    time-of-flight imaging Doppler time-of-flight imaging.\BBCQ \APACjournalVolNumPagesACM
    Transactions on Graphics (ToG)3441–11, \PrintBackRefs\CurrentBib
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heide 等人. [2015年] \APACinsertmetastarheide2015doppler{APACrefauthors}Heide,
    F., Heidrich, W., Hullin, M.\BCBL Wetzstein, G. \APACrefYearMonthDay2015. \BBOQ\APACrefatitle多普勒飞行时间成像
    多普勒飞行时间成像.\BBCQ \APACjournalVolNumPagesACM Transactions on Graphics (ToG)3441–11,
    \PrintBackRefs\CurrentBib
- en: Hirata \BOthers. [\APACyear2019] \APACinsertmetastarhirata2019real{APACrefauthors}Hirata,
    A., Ishikawa, R., Roxas, M.\BCBL Oishi, T. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleReal-time
    dense depth estimation using semantically-guided LIDAR data propagation and motion
    stereo Real-time dense depth estimation using semantically-guided lidar data propagation
    and motion stereo.\BBCQ \APACjournalVolNumPagesIEEE Robotics and Automation Letters443806–3811,
    \PrintBackRefs\CurrentBib
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hirata 等人. [2019年] \APACinsertmetastarhirata2019real{APACrefauthors}Hirata,
    A., Ishikawa, R., Roxas, M.\BCBL Oishi, T. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle实时密集深度估计：基于语义引导的激光雷达数据传播与运动立体视觉
    实时密集深度估计：基于语义引导的激光雷达数据传播与运动立体视觉.\BBCQ \APACjournalVolNumPagesIEEE Robotics and
    Automation Letters443806–3811, \PrintBackRefs\CurrentBib
- en: Hirschmuller \BBA Scharstein [\APACyear2007] \APACinsertmetastarhirschmuller2007evaluation{APACrefauthors}Hirschmuller,
    H.\BCBT \BBA Scharstein, D. \APACrefYearMonthDay2007. \BBOQ\APACrefatitleEvaluation
    of cost functions for stereo matching Evaluation of cost functions for stereo
    matching.\BBCQ \APACrefbtitle2007 IEEE Conference on Computer Vision and Pattern
    Recognition 2007 ieee conference on computer vision and pattern recognition (\BPGS1–8).
    \PrintBackRefs\CurrentBib
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hirschmuller 和 Scharstein [2007年] \APACinsertmetastarhirschmuller2007evaluation{APACrefauthors}Hirschmuller,
    H.\BCBT 和 Scharstein, D. \APACrefYearMonthDay2007. \BBOQ\APACrefatitle立体匹配的代价函数评估
    立体匹配的代价函数评估.\BBCQ \APACrefbtitle2007 IEEE Conference on Computer Vision and Pattern
    Recognition 2007 IEEE 计算机视觉与模式识别会议 (\BPGS1–8). \PrintBackRefs\CurrentBib
- en: Holynski \BBA Kopf [\APACyear2018] \APACinsertmetastarholynski2018fast{APACrefauthors}Holynski,
    A.\BCBT \BBA Kopf, J. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleFast depth
    densification for occlusion-aware augmented reality Fast depth densification for
    occlusion-aware augmented reality.\BBCQ \APACjournalVolNumPagesACM Transactions
    on Graphics (ToG)3761–11, \PrintBackRefs\CurrentBib
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holynski 和 Kopf [2018年] \APACinsertmetastarholynski2018fast{APACrefauthors}Holynski,
    A.\BCBT 和 Kopf, J. \APACrefYearMonthDay2018. \BBOQ\APACrefatitle快速深度密集化：面向遮挡的增强现实
    快速深度密集化：面向遮挡的增强现实.\BBCQ \APACjournalVolNumPagesACM Transactions on Graphics (ToG)3761–11,
    \PrintBackRefs\CurrentBib
- en: 'J. Hu \BOthers. [\APACyear2022] \APACinsertmetastarsurvey_completion_2{APACrefauthors}Hu,
    J., Bao, C., Ozay, M., Fan, C., Gao, Q., Liu, H.\BCBL Lam, T.L. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleDeep Depth Completion from Extremely Sparse Data: A Survey
    Deep depth completion from extremely sparse data: A survey.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Pattern Analysis and Machine Intelligence1-20, {APACrefDOI}  [https://doi.org/10.1109/TPAMI.2022.3229090](https://doi.org/10.1109/TPAMI.2022.3229090)
    \PrintBackRefs\CurrentBib'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. Hu 等人. [2022年] \APACinsertmetastarsurvey_completion_2{APACrefauthors}Hu,
    J., Bao, C., Ozay, M., Fan, C., Gao, Q., Liu, H.\BCBL Lam, T.L. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitle深度完成：来自极度稀疏数据的综述 深度完成：来自极度稀疏数据的综述.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Pattern Analysis and Machine Intelligence1-20, {APACrefDOI} [https://doi.org/10.1109/TPAMI.2022.3229090](https://doi.org/10.1109/TPAMI.2022.3229090)
    \PrintBackRefs\CurrentBib
- en: 'M. Hu \BOthers. [\APACyear2021] \APACinsertmetastarhu2021penet{APACrefauthors}Hu,
    M., Wang, S., Li, B., Ning, S., Fan, L.\BCBL Gong, X. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitlePenet: Towards precise and efficient image guided depth completion
    Penet: Towards precise and efficient image guided depth completion.\BBCQ \APACrefbtitle2021
    IEEE International Conference on Robotics and Automation (ICRA) 2021 ieee international
    conference on robotics and automation (icra) (\BPGS13656–13662). \PrintBackRefs\CurrentBib'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'M. Hu等人[\APACyear2021] \APACinsertmetastarhu2021penet{APACrefauthors}Hu, M.,
    Wang, S., Li, B., Ning, S., Fan, L.\BCBL Gong, X. \APACrefYearMonthDay2021. \BBOQ\APACrefatitlePenet：迈向精准高效的图像引导深度完成
    Penet: Towards precise and efficient image guided depth completion.\BBCQ \APACrefbtitle2021
    IEEE国际机器人与自动化会议 (ICRA) 2021 ieee international conference on robotics and automation
    (icra) (\BPGS13656–13662). \PrintBackRefs\CurrentBib'
- en: Huang \BOthers. [\APACyear2017] \APACinsertmetastarhuang2017densely{APACrefauthors}Huang,
    G., Liu, Z., Van Der Maaten, L.\BCBL Weinberger, K.Q. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitleDensely connected convolutional networks Densely connected
    convolutional networks.\BBCQ \APACrefbtitleProceedings of the IEEE conference
    on computer vision and pattern recognition Proceedings of the ieee conference
    on computer vision and pattern recognition (\BPGS4700–4708). \PrintBackRefs\CurrentBib
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等人[\APACyear2017] \APACinsertmetastarhuang2017densely{APACrefauthors}Huang,
    G., Liu, Z., Van Der Maaten, L.\BCBL Weinberger, K.Q. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitle密集连接卷积网络 Densely connected convolutional networks.\BBCQ \APACrefbtitleIEEE计算机视觉与模式识别会议论文集
    Proceedings of the ieee conference on computer vision and pattern recognition
    (\BPGS4700–4708). \PrintBackRefs\CurrentBib
- en: 'Hui \BOthers. [\APACyear2016] \APACinsertmetastarhui2016depth{APACrefauthors}Hui,
    T\BHBIW., Loy, C.C.\BCBL Tang, X. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleDepth
    map super-resolution by deep multi-scale guidance Depth map super-resolution by
    deep multi-scale guidance.\BBCQ \APACrefbtitleComputer Vision–ECCV 2016: 14th
    European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings,
    Part III 14 Computer vision–eccv 2016: 14th european conference, amsterdam, the
    netherlands, october 11-14, 2016, proceedings, part iii 14 (\BPGS353–369). \PrintBackRefs\CurrentBib'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hui等人[\APACyear2016] \APACinsertmetastarhui2016depth{APACrefauthors}Hui, T\BHBIW.,
    Loy, C.C.\BCBL Tang, X. \APACrefYearMonthDay2016. \BBOQ\APACrefatitle深度图超分辨率通过深度多尺度指导
    Depth map super-resolution by deep multi-scale guidance.\BBCQ \APACrefbtitle计算机视觉–ECCV
    2016：第十四届欧洲会议，荷兰阿姆斯特丹，2016年10月11-14日，会议论文集，第三部分 Computer vision–eccv 2016: 14th
    european conference, amsterdam, the netherlands, october 11-14, 2016, proceedings,
    part iii 14 (\BPGS353–369). \PrintBackRefs\CurrentBib'
- en: Hussmann \BOthers. [\APACyear2013] \APACinsertmetastarhussmann2013modulation{APACrefauthors}Hussmann,
    S., Knoll, F.\BCBL Edeler, T. \APACrefYearMonthDay2013. \BBOQ\APACrefatitleModulation
    method including noise model for minimizing the wiggling error of TOF cameras
    Modulation method including noise model for minimizing the wiggling error of tof
    cameras.\BBCQ \APACjournalVolNumPagesIEEE transactions on instrumentation and
    measurement6351127–1136, \PrintBackRefs\CurrentBib
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hussmann等人[\APACyear2013] \APACinsertmetastarhussmann2013modulation{APACrefauthors}Hussmann,
    S., Knoll, F.\BCBL Edeler, T. \APACrefYearMonthDay2013. \BBOQ\APACrefatitle包含噪声模型的调制方法以最小化TOF相机的摆动误差
    Modulation method including noise model for minimizing the wiggling error of tof
    cameras.\BBCQ \APACjournalVolNumPagesIEEE instrumentation and measurement期刊6351127–1136,
    \PrintBackRefs\CurrentBib
- en: 'Jaritz \BOthers. [\APACyear2018] \APACinsertmetastarjaritz2018sparse{APACrefauthors}Jaritz,
    M., De Charette, R., Wirbel, E., Perrotton, X.\BCBL Nashashibi, F. \APACrefYearMonthDay2018.
    \BBOQ\APACrefatitleSparse and dense data with cnns: Depth completion and semantic
    segmentation Sparse and dense data with cnns: Depth completion and semantic segmentation.\BBCQ
    \APACrefbtitle2018 International Conference on 3D Vision (3DV) 2018 international
    conference on 3d vision (3dv) (\BPGS52–60). \PrintBackRefs\CurrentBib'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jaritz等人[\APACyear2018] \APACinsertmetastarjaritz2018sparse{APACrefauthors}Jaritz,
    M., De Charette, R., Wirbel, E., Perrotton, X.\BCBL Nashashibi, F. \APACrefYearMonthDay2018.
    \BBOQ\APACrefatitle稀疏和密集数据与CNNs：深度完成和语义分割 Sparse and dense data with cnns: Depth
    completion and semantic segmentation.\BBCQ \APACrefbtitle2018年国际三维视觉会议 (3DV) 2018
    international conference on 3d vision (3dv) (\BPGS52–60). \PrintBackRefs\CurrentBib'
- en: 'J. Jeon \BOthers. [\APACyear2022] \APACinsertmetastarjeon2022struct{APACrefauthors}Jeon,
    J., Lim, H., Seo, D\BHBIU.\BCBL Myung, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleStruct-MDC:
    Mesh-refined unsupervised depth completion leveraging structural regularities
    from visual SLAM Struct-mdc: Mesh-refined unsupervised depth completion leveraging
    structural regularities from visual slam.\BBCQ \APACjournalVolNumPagesIEEE Robotics
    and Automation Letters736391–6398, \PrintBackRefs\CurrentBib'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'J. Jeon \BOthers. [\APACyear2022] \APACinsertmetastarjeon2022struct{APACrefauthors}Jeon,
    J., Lim, H., Seo, D\BHBIU.\BCBL Myung, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleStruct-MDC:
    利用视觉SLAM中的结构规律进行网格精细化无监督深度完成 Struct-mdc: Mesh-refined unsupervised depth completion
    leveraging structural regularities from visual slam.\BBCQ \APACjournalVolNumPagesIEEE
    Robotics and Automation Letters736391–6398, \PrintBackRefs\CurrentBib'
- en: 'Y. Jeon \BOthers. [\APACyear2021] \APACinsertmetastarjeon2021abcd{APACrefauthors}Jeon,
    Y., Kim, H.\BCBL Seo, S\BHBIW. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleABCD:
    Attentive bilateral convolutional network for robust depth completion Abcd: Attentive
    bilateral convolutional network for robust depth completion.\BBCQ \APACjournalVolNumPagesIEEE
    Robotics and Automation Letters7181–87, \PrintBackRefs\CurrentBib'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Y. Jeon \BOthers. [\APACyear2021] \APACinsertmetastarjeon2021abcd{APACrefauthors}Jeon,
    Y., Kim, H.\BCBL Seo, S\BHBIW. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleABCD:
    用于稳健深度完成的注意力双边卷积网络 Abcd: Attentive bilateral convolutional network for robust
    depth completion.\BBCQ \APACjournalVolNumPagesIEEE Robotics and Automation Letters7181–87,
    \PrintBackRefs\CurrentBib'
- en: Jiang \BOthers. [\APACyear2022] \APACinsertmetastarjiang2022low{APACrefauthors}Jiang,
    X., Cambareri, V., Agresti, G., Ugwu, C.I., Simonetto, A., Cardinaux, F.\BCBL
    Zanuttigh, P. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleA Low Memory Footprint
    Quantized Neural Network for Depth Completion of Very Sparse Time-of-Flight Depth
    Maps A low memory footprint quantized neural network for depth completion of very
    sparse time-of-flight depth maps.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf
    conference on computer vision and pattern recognition (\BPGS2687–2696). \PrintBackRefs\CurrentBib
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang \BOthers. [\APACyear2022] \APACinsertmetastarjiang2022low{APACrefauthors}Jiang,
    X., Cambareri, V., Agresti, G., Ugwu, C.I., Simonetto, A., Cardinaux, F.\BCBL
    Zanuttigh, P. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle一种低内存占用量化神经网络用于非常稀疏的飞行时间深度图的深度完成
    A low memory footprint quantized neural network for depth completion of very sparse
    time-of-flight depth maps.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集 Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS2687–2696).
    \PrintBackRefs\CurrentBib
- en: Kalia \BOthers. [\APACyear2019] \APACinsertmetastarkalia2019real{APACrefauthors}Kalia,
    M., Navab, N.\BCBL Salcudean, T. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleA
    real-time interactive augmented reality depth estimation technique for surgical
    robotics A real-time interactive augmented reality depth estimation technique
    for surgical robotics.\BBCQ \APACrefbtitle2019 International Conference on Robotics
    and Automation (ICRA) 2019 international conference on robotics and automation
    (icra) (\BPGS8291–8297). \PrintBackRefs\CurrentBib
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalia \BOthers. [\APACyear2019] \APACinsertmetastarkalia2019real{APACrefauthors}Kalia,
    M., Navab, N.\BCBL Salcudean, T. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle一种实时交互增强现实深度估计技术用于手术机器人
    A real-time interactive augmented reality depth estimation technique for surgical
    robotics.\BBCQ \APACrefbtitle2019年国际机器人与自动化大会 (ICRA) 2019 international conference
    on robotics and automation (icra) (\BPGS8291–8297). \PrintBackRefs\CurrentBib
- en: 'Kam \BOthers. [\APACyear2022] \APACinsertmetastarkam2022costdcnet{APACrefauthors}Kam,
    J., Kim, J., Kim, S., Park, J.\BCBL Lee, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleCostDCNet:
    Cost Volume Based Depth Completion for a Single RGB-D Image Costdcnet: Cost volume
    based depth completion for a single rgb-d image.\BBCQ \APACrefbtitleComputer Vision–ECCV
    2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part II Computer vision–eccv 2022: 17th european conference, tel aviv, israel,
    october 23–27, 2022, proceedings, part ii (\BPGS257–274). \PrintBackRefs\CurrentBib'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kam \BOthers. [\APACyear2022] \APACinsertmetastarkam2022costdcnet{APACrefauthors}Kam,
    J., Kim, J., Kim, S., Park, J.\BCBL Lee, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleCostDCNet:
    基于成本体积的单张RGB-D图像深度完成 Costdcnet: Cost volume based depth completion for a single
    rgb-d image.\BBCQ \APACrefbtitle计算机视觉–ECCV 2022: 第17届欧洲会议，特拉维夫，以色列，2022年10月23–27日，论文集，第二部分
    Computer vision–eccv 2022: 17th european conference, tel aviv, israel, october
    23–27, 2022, proceedings, part ii (\BPGS257–274). \PrintBackRefs\CurrentBib'
- en: 'Ke \BOthers. [\APACyear2021] \APACinsertmetastarke2021mdanet{APACrefauthors}Ke,
    Y., Li, K., Yang, W., Xu, Z., Hao, D., Huang, L.\BCBL Wang, G. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleMDANet: Multi-Modal Deep Aggregation Network for Depth Completion
    Mdanet: Multi-modal deep aggregation network for depth completion.\BBCQ \APACrefbtitle2021
    IEEE International Conference on Robotics and Automation (ICRA) 2021 ieee international
    conference on robotics and automation (icra) (\BPGS4288–4294). \PrintBackRefs\CurrentBib'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ke \BOthers. [\APACyear2021] \APACinsertmetastarke2021mdanet{APACrefauthors}Ke,
    Y., Li, K., Yang, W., Xu, Z., Hao, D., Huang, L.\BCBL Wang, G. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleMDANet: 多模态深度聚合网络用于深度完成 MDANet: Multi-modal deep aggregation
    network for depth completion.\BBCQ \APACrefbtitle2021年IEEE国际机器人与自动化大会论文集 2021
    IEEE International Conference on Robotics and Automation (ICRA) (\BPGS4288–4294).
    \PrintBackRefs\CurrentBib'
- en: Khan \BOthers. [\APACyear2021] \APACinsertmetastarkhan2021sparse{APACrefauthors}Khan,
    M.F.F., Troncoso Aldas, N.D., Kumar, A., Advani, S.\BCBL Narayanan, V. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleSparse to dense depth completion using a generative adversarial
    network with intelligent sampling strategies Sparse to dense depth completion
    using a generative adversarial network with intelligent sampling strategies.\BBCQ
    \APACrefbtitleProceedings of the 29th ACM International Conference on Multimedia
    Proceedings of the 29th acm international conference on multimedia (\BPGS5528–5536).
    \PrintBackRefs\CurrentBib
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan \BOthers. [\APACyear2021] \APACinsertmetastarkhan2021sparse{APACrefauthors}Khan,
    M.F.F., Troncoso Aldas, N.D., Kumar, A., Advani, S.\BCBL Narayanan, V. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitle稀疏到密集深度完成使用生成对抗网络及智能采样策略 Sparse to dense depth completion using
    a generative adversarial network with intelligent sampling strategies.\BBCQ \APACrefbtitle第29届ACM国际多媒体会议论文集
    Proceedings of the 29th ACM International Conference on Multimedia (\BPGS5528–5536).
    \PrintBackRefs\CurrentBib
- en: Kim \BOthers. [\APACyear2021] \APACinsertmetastarkim2021deformable{APACrefauthors}Kim,
    B., Ponce, J.\BCBL Ham, B. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleDeformable
    kernel networks for joint image filtering Deformable kernel networks for joint
    image filtering.\BBCQ \APACjournalVolNumPagesInternational Journal of Computer
    Vision1292579–600, \PrintBackRefs\CurrentBib
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim \BOthers. [\APACyear2021] \APACinsertmetastarkim2021deformable{APACrefauthors}Kim,
    B., Ponce, J.\BCBL Ham, B. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle可变形核网络用于联合图像滤波
    Deformable kernel networks for joint image filtering.\BBCQ \APACjournalVolNumPages国际计算机视觉期刊1292579–600,
    \PrintBackRefs\CurrentBib
- en: 'Koh \BOthers. [\APACyear2022] \APACinsertmetastarkoh2022bnudc{APACrefauthors}Koh,
    J., Lee, J.\BCBL Yoon, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleBnudc:
    A two-branched deep neural network for restoring images from under-display cameras
    Bnudc: A two-branched deep neural network for restoring images from under-display
    cameras.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Proceedings of the ieee/cvf conference on computer
    vision and pattern recognition (\BPGS1950–1959). \PrintBackRefs\CurrentBib'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Koh \BOthers. [\APACyear2022] \APACinsertmetastarkoh2022bnudc{APACrefauthors}Koh,
    J., Lee, J.\BCBL Yoon, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleBNUDC:
    用于恢复屏下摄像头图像的双分支深度神经网络 Bnudc: A two-branched deep neural network for restoring
    images from under-display cameras.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (\BPGS1950–1959).
    \PrintBackRefs\CurrentBib'
- en: Kwon \BOthers. [\APACyear2021] \APACinsertmetastarkwon2021controllable{APACrefauthors}Kwon,
    K., Kang, E., Lee, S., Lee, S\BHBIJ., Lee, H\BHBIE., Yoo, B.\BCBL Han, J\BHBIJ.
    \APACrefYearMonthDay2021. \BBOQ\APACrefatitleControllable image restoration for
    under-display camera in smartphones Controllable image restoration for under-display
    camera in smartphones.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf conference
    on computer vision and pattern recognition (\BPGS2073–2082). \PrintBackRefs\CurrentBib
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon \BOthers. [\APACyear2021] \APACinsertmetastarkwon2021controllable{APACrefauthors}Kwon,
    K., Kang, E., Lee, S., Lee, S\BHBIJ., Lee, H\BHBIE., Yoo, B.\BCBL Han, J\BHBIJ.
    \APACrefYearMonthDay2021. \BBOQ\APACrefatitle可控图像修复用于智能手机的屏下摄像头 Controllable image
    restoration for under-display camera in smartphones.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (\BPGS2073–2082).
    \PrintBackRefs\CurrentBib
- en: B\BHBIU. Lee \BOthers. [\APACyear2019] \APACinsertmetastarlee2019depth{APACrefauthors}Lee,
    B\BHBIU., Jeon, H\BHBIG., Im, S.\BCBL Kweon, I.S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDepth
    completion with deep geometry and context guidance Depth completion with deep
    geometry and context guidance.\BBCQ \APACrefbtitle2019 International Conference
    on Robotics and Automation (ICRA) 2019 international conference on robotics and
    automation (icra) (\BPGS3281–3287). \PrintBackRefs\CurrentBib
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B\BHBIU. Lee \BOthers. [\APACyear2019] \APACinsertmetastarlee2019depth{APACrefauthors}Lee,
    B\BHBIU., Jeon, H\BHBIG., Im, S.\BCBL Kweon, I.S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle基于深度几何和上下文引导的深度完成
    Depth completion with deep geometry and context guidance.\BBCQ \APACrefbtitle2019年国际机器人与自动化会议
    (ICRA) 2019 International Conference on Robotics and Automation (ICRA) (\BPGS3281–3287).
    \PrintBackRefs\CurrentBib
- en: S. Lee \BOthers. [\APACyear2020] \APACinsertmetastarlee2020deep{APACrefauthors}Lee,
    S., Lee, J., Kim, D.\BCBL Kim, J. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDeep
    architecture with cross guidance between single image and sparse lidar data for
    depth completion Deep architecture with cross guidance between single image and
    sparse lidar data for depth completion.\BBCQ \APACjournalVolNumPagesIEEE Access879801–79810,
    \PrintBackRefs\CurrentBib
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Lee \BOthers. [\APACyear2020] \APACinsertmetastarlee2020deep{APACrefauthors}Lee,
    S., Lee, J., Kim, D.\BCBL Kim, J. \APACrefYearMonthDay2020. \BBOQ\APACrefatitle单幅图像和稀疏激光雷达数据的交叉引导深度结构
    Deep architecture with cross guidance between single image and sparse lidar data
    for depth completion.\BBCQ \APACjournalVolNumPagesIEEE Access879801–79810, \PrintBackRefs\CurrentBib
- en: S. Lee \BOthers. [\APACyear2022] \APACinsertmetastarlee2022multi{APACrefauthors}Lee,
    S., Yi, E., Lee, J.\BCBL Kim, J. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleMulti-Scaled
    and Densely Connected Locally Convolutional Layers for Depth Completion Multi-scaled
    and densely connected locally convolutional layers for depth completion.\BBCQ
    \APACrefbtitle2022 IEEE/RSJ International Conference on Intelligent Robots and
    Systems (IROS) 2022 ieee/rsj international conference on intelligent robots and
    systems (iros) (\BPGS8360–8367). \PrintBackRefs\CurrentBib
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Lee \BOthers. [\APACyear2022] \APACinsertmetastarlee2022multi{APACrefauthors}Lee,
    S., Yi, E., Lee, J.\BCBL Kim, J. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle多尺度和密集连接的局部卷积层用于深度完成
    Multi-scaled and densely connected locally convolutional layers for depth completion.\BBCQ
    \APACrefbtitle2022 IEEE/RSJ国际智能机器人与系统会议论文集 2022 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS) (\BPGS8360–8367). \PrintBackRefs\CurrentBib
- en: A. Li \BOthers. [\APACyear2020] \APACinsertmetastarli2020multi{APACrefauthors}Li,
    A., Yuan, Z., Ling, Y., Chi, W., Zhang, C.\BCBL \BOthersPeriod. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleA multi-scale guided cascade hourglass network for depth completion
    A multi-scale guided cascade hourglass network for depth completion.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision Proceedings
    of the ieee/cvf winter conference on applications of computer vision (\BPGS32–40).
    \PrintBackRefs\CurrentBib
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A. Li \BOthers. [\APACyear2020] \APACinsertmetastarli2020multi{APACrefauthors}Li,
    A., Yuan, Z., Ling, Y., Chi, W., Zhang, C.\BCBL \BOthersPeriod. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitle一种多尺度引导级联Hourglass网络用于深度完成 A multi-scale guided cascade hourglass
    network for depth completion.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉应用冬季会议论文集 Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision (\BPGS32–40).
    \PrintBackRefs\CurrentBib
- en: 'D. Li \BOthers. [\APACyear2022] \APACinsertmetastarli2022motion{APACrefauthors}Li,
    D., Xu, J., Yang, Z., Zhang, Q., Ma, Q., Zhang, L.\BCBL Chen, P. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleMotion inspires notion: self-supervised visual-LiDAR fusion
    for environment depth estimation Motion inspires notion: self-supervised visual-lidar
    fusion for environment depth estimation.\BBCQ \APACrefbtitleProceedings of the
    20th Annual International Conference on Mobile Systems, Applications and Services
    Proceedings of the 20th annual international conference on mobile systems, applications
    and services (\BPGS114–127). \PrintBackRefs\CurrentBib'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'D. Li \BOthers. [\APACyear2022] \APACinsertmetastarli2022motion{APACrefauthors}Li,
    D., Xu, J., Yang, Z., Zhang, Q., Ma, Q., Zhang, L.\BCBL Chen, P. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitle运动启发的观念：自监督视觉-LiDAR融合用于环境深度估计 Motion inspires notion: self-supervised
    visual-lidar fusion for environment depth estimation.\BBCQ \APACrefbtitle第20届年度国际移动系统、应用和服务会议论文集
    Proceedings of the 20th Annual International Conference on Mobile Systems, Applications
    and Services (\BPGS114–127). \PrintBackRefs\CurrentBib'
- en: J. Li \BOthers. [\APACyear2022] \APACinsertmetastarli2022fisher{APACrefauthors}Li,
    J., Yue, T., Zhao, S.\BCBL Hu, X. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleFisher
    information guidance for learned time-of-flight imaging Fisher information guidance
    for learned time-of-flight imaging.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf
    conference on computer vision and pattern recognition (\BPGS16334–16343). \PrintBackRefs\CurrentBib
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. Li\BOthers. [\APACyear2022] \APACinsertmetastarli2022fisher{APACrefauthors}Li,
    J., Yue, T., Zhao, S.\BCBL Hu, X. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle费舍尔信息引导的学习飞行时间成像
    费舍尔信息引导的学习飞行时间成像.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集 IEEE/CVF计算机视觉与模式识别会议论文集
    (\BPGS16334–16343). \PrintBackRefs\CurrentBib
- en: T. Li \BOthers. [\APACyear2020] \APACinsertmetastarli2020depth{APACrefauthors}Li,
    T., Lin, H., Dong, X.\BCBL Zhang, X. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDepth
    image super-resolution using correlation-controlled color guidance and multi-scale
    symmetric network Depth image super-resolution using correlation-controlled color
    guidance and multi-scale symmetric network.\BBCQ \APACjournalVolNumPagesPattern
    Recognition107107513, \PrintBackRefs\CurrentBib
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T. Li\BOthers. [\APACyear2020] \APACinsertmetastarli2020depth{APACrefauthors}Li,
    T., Lin, H., Dong, X.\BCBL Zhang, X. \APACrefYearMonthDay2020. \BBOQ\APACrefatitle基于相关控制的颜色引导和多尺度对称网络的深度图像超分辨率
    基于相关控制的颜色引导和多尺度对称网络的深度图像超分辨率.\BBCQ \APACjournalVolNumPages模式识别 107107513, \PrintBackRefs\CurrentBib
- en: Y. Li \BOthers. [\APACyear2016] \APACinsertmetastarli2016deep{APACrefauthors}Li,
    Y., Huang, J\BHBIB., Ahuja, N.\BCBL Yang, M\BHBIH. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleDeep
    joint image filtering Deep joint image filtering.\BBCQ \APACrefbtitleEuropean
    conference on computer vision European conference on computer vision (\BPGS154–169).
    \PrintBackRefs\CurrentBib
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Li\BOthers. [\APACyear2016] \APACinsertmetastarli2016deep{APACrefauthors}Li,
    Y., Huang, J\BHBIB., Ahuja, N.\BCBL Yang, M\BHBIH. \APACrefYearMonthDay2016. \BBOQ\APACrefatitle深度联合图像滤波
    深度联合图像滤波.\BBCQ \APACrefbtitle欧洲计算机视觉会议 欧洲计算机视觉会议 (\BPGS154–169). \PrintBackRefs\CurrentBib
- en: Y. Li \BOthers. [\APACyear2019] \APACinsertmetastarli2019joint{APACrefauthors}Li,
    Y., Huang, J\BHBIB., Ahuja, N.\BCBL Yang, M\BHBIH. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleJoint
    image filtering with deep convolutional networks Joint image filtering with deep
    convolutional networks.\BBCQ \APACjournalVolNumPagesIEEE transactions on pattern
    analysis and machine intelligence4181909–1923, \PrintBackRefs\CurrentBib
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Li\BOthers. [\APACyear2019] \APACinsertmetastarli2019joint{APACrefauthors}Li,
    Y., Huang, J\BHBIB., Ahuja, N.\BCBL Yang, M\BHBIH. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle基于深度卷积网络的联合图像滤波
    基于深度卷积网络的联合图像滤波.\BBCQ \APACjournalVolNumPagesIEEE 论文集 4181909–1923, \PrintBackRefs\CurrentBib
- en: 'Y. Li \BOthers. [\APACyear2022] \APACinsertmetastarli2022deltar{APACrefauthors}Li,
    Y., Liu, X., Dong, W., Zhou, H., Bao, H., Zhang, G.\BDBLCui, Z. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleDeltar: Depth estimation from a light-weight tof sensor and
    rgb image Deltar: Depth estimation from a light-weight tof sensor and rgb image.\BBCQ
    \APACrefbtitleEuropean Conference on Computer Vision European conference on computer
    vision (\BPGS619–636). \PrintBackRefs\CurrentBib'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Y. Li\BOthers. [\APACyear2022] \APACinsertmetastarli2022deltar{APACrefauthors}Li,
    Y., Liu, X., Dong, W., Zhou, H., Bao, H., Zhang, G.\BDBLCui, Z. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleDeltar: 轻量级 tof 传感器和 rgb 图像的深度估计 Deltar: 轻量级 tof 传感器和 rgb 图像的深度估计.\BBCQ
    \APACrefbtitle欧洲计算机视觉会议 欧洲计算机视觉会议 (\BPGS619–636). \PrintBackRefs\CurrentBib'
- en: Liang \BOthers. [\APACyear2019] \APACinsertmetastarliang2019multi{APACrefauthors}Liang,
    M., Yang, B., Chen, Y., Hu, R.\BCBL Urtasun, R. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleMulti-task
    multi-sensor fusion for 3d object detection Multi-task multi-sensor fusion for
    3d object detection.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf conference
    on computer vision and pattern recognition (\BPGS7345–7353). \PrintBackRefs\CurrentBib
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang\BOthers. [\APACyear2019] \APACinsertmetastarliang2019multi{APACrefauthors}Liang,
    M., Yang, B., Chen, Y., Hu, R.\BCBL Urtasun, R. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle多任务多传感器融合用于
    3D 物体检测 多任务多传感器融合用于 3D 物体检测.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集 IEEE/CVF计算机视觉与模式识别会议论文集
    (\BPGS7345–7353). \PrintBackRefs\CurrentBib
- en: 'Liao \BOthers. [\APACyear2022] \APACinsertmetastarliao2022kitti{APACrefauthors}Liao,
    Y., Xie, J.\BCBL Geiger, A. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleKITTI-360:
    A novel dataset and benchmarks for urban scene understanding in 2d and 3d Kitti-360:
    A novel dataset and benchmarks for urban scene understanding in 2d and 3d.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Pattern Analysis and Machine Intelligence,
    \PrintBackRefs\CurrentBib'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liao \BOthers. [\APACyear2022] \APACinsertmetastarliao2022kitti{APACrefauthors}Liao,
    Y., Xie, J.\BCBL Geiger, A. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleKITTI-360:
    A novel dataset and benchmarks for urban scene understanding in 2d and 3d Kitti-360:
    A novel dataset and benchmarks for urban scene understanding in 2d and 3d.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Pattern Analysis and Machine Intelligence,
    \PrintBackRefs\CurrentBib'
- en: T\BHBIY. Lin \BOthers. [\APACyear2017] \APACinsertmetastarlin2017focal{APACrefauthors}Lin,
    T\BHBIY., Goyal, P., Girshick, R., He, K.\BCBL Dollár, P. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitleFocal loss for dense object detection Focal loss for dense
    object detection.\BBCQ \APACrefbtitleProceedings of the IEEE international conference
    on computer vision Proceedings of the ieee international conference on computer
    vision (\BPGS2980–2988). \PrintBackRefs\CurrentBib
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T\BHBIY. Lin \BOthers. [\APACyear2017] \APACinsertmetastarlin2017focal{APACrefauthors}Lin,
    T\BHBIY., Goyal, P., Girshick, R., He, K.\BCBL Dollár, P. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitleFocal loss for dense object detection Focal loss for dense
    object detection.\BBCQ \APACrefbtitleProceedings of the IEEE international conference
    on computer vision Proceedings of the ieee international conference on computer
    vision (\BPGS2980–2988). \PrintBackRefs\CurrentBib
- en: Y. Lin \BOthers. [\APACyear2022] \APACinsertmetastarlin2022dynamic{APACrefauthors}Lin,
    Y., Cheng, T., Zhong, Q., Zhou, W.\BCBL Yang, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleDynamic
    spatial propagation network for depth completion Dynamic spatial propagation network
    for depth completion.\BBCQ \APACrefbtitleProceedings of the AAAI Conference on
    Artificial Intelligence Proceedings of the aaai conference on artificial intelligence (\BVOL36,
    \BPGS1638–1646). \PrintBackRefs\CurrentBib
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Lin \BOthers. [\APACyear2022] \APACinsertmetastarlin2022dynamic{APACrefauthors}Lin,
    Y., Cheng, T., Zhong, Q., Zhou, W.\BCBL Yang, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleDynamic
    spatial propagation network for depth completion Dynamic spatial propagation network
    for depth completion.\BBCQ \APACrefbtitleProceedings of the AAAI Conference on
    Artificial Intelligence Proceedings of the aaai conference on artificial intelligence
    (\BVOL36, \BPGS1638–1646). \PrintBackRefs\CurrentBib
- en: 'Lipson \BOthers. [\APACyear2021] \APACinsertmetastarRAFT_STEREO{APACrefauthors}Lipson,
    L., Teed, Z.\BCBL Deng, J. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleRAFT-Stereo:
    Multilevel Recurrent Field Transforms for Stereo Matching Raft-stereo: Multilevel
    recurrent field transforms for stereo matching.\BBCQ \APACrefbtitleInternational
    Conference on 3D Vision (3DV). International conference on 3d vision (3dv). \PrintBackRefs\CurrentBib'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lipson \BOthers. [\APACyear2021] \APACinsertmetastarRAFT_STEREO{APACrefauthors}Lipson,
    L., Teed, Z.\BCBL Deng, J. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleRAFT-Stereo:
    Multilevel Recurrent Field Transforms for Stereo Matching Raft-stereo: Multilevel
    recurrent field transforms for stereo matching.\BBCQ \APACrefbtitleInternational
    Conference on 3D Vision (3DV). International conference on 3d vision (3dv). \PrintBackRefs\CurrentBib'
- en: 'A. Liu \BOthers. [\APACyear2021] \APACinsertmetastarliu2021casia{APACrefauthors}Liu,
    A., Tan, Z., Wan, J., Escalera, S., Guo, G.\BCBL Li, S.Z. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleCasia-surf cefa: A benchmark for multi-modal cross-ethnicity
    face anti-spoofing Casia-surf cefa: A benchmark for multi-modal cross-ethnicity
    face anti-spoofing.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision Proceedings of the ieee/cvf winter conference
    on applications of computer vision (\BPGS1179–1187). \PrintBackRefs\CurrentBib'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'A. Liu \BOthers. [\APACyear2021] \APACinsertmetastarliu2021casia{APACrefauthors}Liu,
    A., Tan, Z., Wan, J., Escalera, S., Guo, G.\BCBL Li, S.Z. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleCasia-surf cefa: A benchmark for multi-modal cross-ethnicity
    face anti-spoofing Casia-surf cefa: A benchmark for multi-modal cross-ethnicity
    face anti-spoofing.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision Proceedings of the ieee/cvf winter conference
    on applications of computer vision (\BPGS1179–1187). \PrintBackRefs\CurrentBib'
- en: J. Liu \BOthers. [\APACyear2012] \APACinsertmetastarliu2012guided{APACrefauthors}Liu,
    J., Gong, X.\BCBL Liu, J. \APACrefYearMonthDay2012. \BBOQ\APACrefatitleGuided
    inpainting and filtering for kinect depth maps Guided inpainting and filtering
    for kinect depth maps.\BBCQ \APACrefbtitleProceedings of the 21st International
    Conference on Pattern Recognition (ICPR2012) Proceedings of the 21st international
    conference on pattern recognition (icpr2012) (\BPGS2055–2058). \PrintBackRefs\CurrentBib
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. Liu \BOthers. [\APACyear2012] \APACinsertmetastarliu2012guided{APACrefauthors}Liu,
    J., Gong, X.\BCBL Liu, J. \APACrefYearMonthDay2012. \BBOQ\APACrefatitleGuided
    inpainting and filtering for kinect depth maps Guided inpainting and filtering
    for kinect depth maps.\BBCQ \APACrefbtitleProceedings of the 21st International
    Conference on Pattern Recognition (ICPR2012) Proceedings of the 21st international
    conference on pattern recognition (icpr2012) (\BPGS2055–2058). \PrintBackRefs\CurrentBib
- en: 'J. Liu \BBA Jung [\APACyear2022] \APACinsertmetastarliu2022nnnet{APACrefauthors}Liu,
    J.\BCBT \BBA Jung, C. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleNNNet: New
    Normal Guided Depth Completion From Sparse LiDAR Data and Single Color Image Nnnet:
    New normal guided depth completion from sparse lidar data and single color image.\BBCQ
    \APACjournalVolNumPagesIEEE Access10114252–114261, \PrintBackRefs\CurrentBib'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'J. Liu \BBA Jung [\APACyear2022] \APACinsertmetastarliu2022nnnet{APACrefauthors}Liu,
    J.\BCBT \BBA Jung, C. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleNNNet：从稀疏LiDAR数据和单一彩色图像进行的新常态引导深度完成
    NNNet: New Normal Guided Depth Completion From Sparse LiDAR Data and Single Color
    Image.\BBCQ \APACjournalVolNumPagesIEEE Access10114252–114261, \PrintBackRefs\CurrentBib'
- en: L. Liu, Liao\BCBL \BOthers. [\APACyear2021] \APACinsertmetastarliu2021learning{APACrefauthors}Liu,
    L., Liao, Y., Wang, Y., Geiger, A.\BCBL Liu, Y. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleLearning
    steering kernels for guided depth completion Learning steering kernels for guided
    depth completion.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Image Processing302850–2861,
    \PrintBackRefs\CurrentBib
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L. Liu, Liao\BCBL \BOthers. [\APACyear2021] \APACinsertmetastarliu2021learning{APACrefauthors}Liu,
    L., Liao, Y., Wang, Y., Geiger, A.\BCBL Liu, Y. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle用于引导深度完成的学习引导核
    Learning steering kernels for guided depth completion.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Image Processing302850–2861, \PrintBackRefs\CurrentBib
- en: 'L. Liu, Song\BCBL \BOthers. [\APACyear2021] \APACinsertmetastarliu2021fcfr{APACrefauthors}Liu,
    L., Song, X., Lyu, X., Diao, J., Wang, M., Liu, Y.\BCBL Zhang, L. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleFcfr-net: Feature fusion based coarse-to-fine residual learning
    for depth completion Fcfr-net: Feature fusion based coarse-to-fine residual learning
    for depth completion.\BBCQ \APACrefbtitleProceedings of the AAAI conference on
    artificial intelligence Proceedings of the aaai conference on artificial intelligence (\BVOL35,
    \BPGS2136–2144). \PrintBackRefs\CurrentBib'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L. Liu, Song\BCBL \BOthers. [\APACyear2021] \APACinsertmetastarliu2021fcfr{APACrefauthors}Liu,
    L., Song, X., Lyu, X., Diao, J., Wang, M., Liu, Y.\BCBL Zhang, L. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleFcfr-net：基于特征融合的粗到细残差学习用于深度完成 Fcfr-net: Feature Fusion Based
    Coarse-to-Fine Residual Learning for Depth Completion.\BBCQ \APACrefbtitleProceedings
    of the AAAI conference on artificial intelligence Proceedings of the AAAI Conference
    on Artificial Intelligence (\BVOL35, \BPGS2136–2144). \PrintBackRefs\CurrentBib'
- en: 'L. Liu \BOthers. [\APACyear2023] \APACinsertmetastarliu2023mff{APACrefauthors}Liu,
    L., Song, X., Sun, J., Lyu, X., Li, L., Liu, Y.\BCBL Zhang, L. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleMFF-Net: Towards Efficient Monocular Depth Completion with
    Multi-modal Feature Fusion Mff-net: Towards efficient monocular depth completion
    with multi-modal feature fusion.\BBCQ \APACjournalVolNumPagesIEEE Robotics and
    Automation Letters, \PrintBackRefs\CurrentBib'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L. Liu \BOthers. [\APACyear2023] \APACinsertmetastarliu2023mff{APACrefauthors}Liu,
    L., Song, X., Sun, J., Lyu, X., Li, L., Liu, Y.\BCBL Zhang, L. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleMFF-Net：基于多模态特征融合的高效单目深度完成 MFF-Net: Towards Efficient Monocular
    Depth Completion with Multi-modal Feature Fusion.\BBCQ \APACjournalVolNumPagesIEEE
    Robotics and Automation Letters, \PrintBackRefs\CurrentBib'
- en: P. Liu \BOthers. [\APACyear2021] \APACinsertmetastarliu2021deformable{APACrefauthors}Liu,
    P., Zhang, Z., Meng, Z.\BCBL Gao, N. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleDeformable
    Enhancement and Adaptive Fusion for Depth Map Super-Resolution Deformable enhancement
    and adaptive fusion for depth map super-resolution.\BBCQ \APACjournalVolNumPagesIEEE
    Signal Processing Letters29204–208, \PrintBackRefs\CurrentBib
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P. Liu \BOthers. [\APACyear2021] \APACinsertmetastarliu2021deformable{APACrefauthors}Liu,
    P., Zhang, Z., Meng, Z.\BCBL Gao, N. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle用于深度图超分辨率的可变形增强和自适应融合
    Deformable enhancement and adaptive fusion for depth map super-resolution.\BBCQ
    \APACjournalVolNumPagesIEEE Signal Processing Letters29204–208, \PrintBackRefs\CurrentBib
- en: 'P. Liu \BOthers. [\APACyear2022] \APACinsertmetastarliu2022pdr{APACrefauthors}Liu,
    P., Zhang, Z., Meng, Z., Gao, N.\BCBL Wang, C. \APACrefYearMonthDay2022. \BBOQ\APACrefatitlePDR-Net:
    Progressive depth reconstruction network for color guided depth map super-resolution
    Pdr-net: Progressive depth reconstruction network for color guided depth map super-resolution.\BBCQ
    \APACjournalVolNumPagesNeurocomputing47975–88, \PrintBackRefs\CurrentBib'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'P. Liu \BOthers. [\APACyear2022] \APACinsertmetastarliu2022pdr{APACrefauthors}Liu,
    P., Zhang, Z., Meng, Z., Gao, N.\BCBL Wang, C. \APACrefYearMonthDay2022. \BBOQ\APACrefatitlePDR-Net：用于彩色引导深度图超分辨率的渐进深度重建网络
    PDR-Net: Progressive Depth Reconstruction Network for Color Guided Depth Map Super-Resolution.\BBCQ
    \APACjournalVolNumPagesNeurocomputing47975–88, \PrintBackRefs\CurrentBib'
- en: 'R. Liu \BOthers. [\APACyear2016] \APACinsertmetastarliu2016learning{APACrefauthors}Liu,
    R., Zhong, G., Cao, J., Lin, Z., Shan, S.\BCBL Luo, Z. \APACrefYearMonthDay2016.
    \BBOQ\APACrefatitleLearning to diffuse: A new perspective to design pdes for visual
    analysis Learning to diffuse: A new perspective to design pdes for visual analysis.\BBCQ
    \APACjournalVolNumPagesIEEE transactions on pattern analysis and machine intelligence38122457–2471,
    \PrintBackRefs\CurrentBib'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'R. Liu \BOthers. [\APACyear2016] \APACinsertmetastarliu2016learning{APACrefauthors}Liu,
    R., Zhong, G., Cao, J., Lin, Z., Shan, S.\BCBL Luo, Z. \APACrefYearMonthDay2016.
    \BBOQ\APACrefatitle学习扩散：设计视觉分析 PDEs 的新视角 Learning to diffuse: A new perspective
    to design pdes for visual analysis.\BBCQ \APACjournalVolNumPagesIEEE 视觉分析与机器智能交易38122457–2471,
    \PrintBackRefs\CurrentBib'
- en: S. Liu \BOthers. [\APACyear2017] \APACinsertmetastarliu2017learning{APACrefauthors}Liu,
    S., De Mello, S., Gu, J., Zhong, G., Yang, M\BHBIH.\BCBL Kautz, J. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitleLearning affinity via spatial propagation networks Learning
    affinity via spatial propagation networks.\BBCQ \APACjournalVolNumPagesAdvances
    in Neural Information Processing Systems30, \PrintBackRefs\CurrentBib
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Liu \BOthers. [\APACyear2017] \APACinsertmetastarliu2017learning{APACrefauthors}Liu,
    S., De Mello, S., Gu, J., Zhong, G., Yang, M\BHBIH.\BCBL Kautz, J. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitle学习亲和力通过空间传播网络 Learning affinity via spatial propagation networks.\BBCQ
    \APACjournalVolNumPages神经信息处理系统进展30, \PrintBackRefs\CurrentBib
- en: 'T.Y. Liu \BOthers. [\APACyear2022] \APACinsertmetastarliu2022monitored{APACrefauthors}Liu,
    T.Y., Agrawal, P., Chen, A., Hong, B\BHBIW.\BCBL Wong, A. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleMonitored distillation for positive congruent depth completion
    Monitored distillation for positive congruent depth completion.\BBCQ \APACrefbtitleComputer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part II Computer vision–eccv 2022: 17th european conference, tel
    aviv, israel, october 23–27, 2022, proceedings, part ii (\BPGS35–53). \PrintBackRefs\CurrentBib'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'T.Y. Liu \BOthers. [\APACyear2022] \APACinsertmetastarliu2022monitored{APACrefauthors}Liu,
    T.Y., Agrawal, P., Chen, A., Hong, B\BHBIW.\BCBL Wong, A. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitle监测蒸馏用于正向一致深度补全 Monitored distillation for positive congruent
    depth completion.\BBCQ \APACrefbtitle计算机视觉–ECCV 2022: 第17届欧洲会议，特拉维夫，以色列，2022年10月23-27日，会议论文集，第二部分
    Computer vision–eccv 2022: 17th european conference, tel aviv, israel, october
    23–27, 2022, proceedings, part ii (\BPGS35–53). \PrintBackRefs\CurrentBib'
- en: X. Liu \BOthers. [\APACyear2023] \APACinsertmetastarliu2023multi{APACrefauthors}Liu,
    X., Li, Y., Teng, Y., Bao, H., Zhang, G., Zhang, Y.\BCBL Cui, Z. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleMulti-modal neural radiance field for monocular dense slam
    with a light-weight tof sensor Multi-modal neural radiance field for monocular
    dense slam with a light-weight tof sensor.\BBCQ \APACrefbtitleProceedings of the
    IEEE/CVF International Conference on Computer Vision Proceedings of the ieee/cvf
    international conference on computer vision (\BPGS1–11). \PrintBackRefs\CurrentBib
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X. Liu \BOthers. [\APACyear2023] \APACinsertmetastarliu2023multi{APACrefauthors}Liu,
    X., Li, Y., Teng, Y., Bao, H., Zhang, G., Zhang, Y.\BCBL Cui, Z. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitle多模态神经辐射场用于单目密集 SLAM 与轻量级 TOF 传感器 Multi-modal neural radiance
    field for monocular dense slam with a light-weight tof sensor.\BBCQ \APACrefbtitleIEEE/CVF
    计算机视觉国际会议论文集 Proceedings of the ieee/cvf international conference on computer
    vision (\BPGS1–11). \PrintBackRefs\CurrentBib
- en: 'X. Liu \BOthers. [\APACyear2022] \APACinsertmetastarliu2022graphcspn{APACrefauthors}Liu,
    X., Shao, X., Wang, B., Li, Y.\BCBL Wang, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleGraphCSPN:
    Geometry-Aware Depth Completion via Dynamic GCNs Graphcspn: Geometry-aware depth
    completion via dynamic gcns.\BBCQ \APACrefbtitleComputer Vision–ECCV 2022: 17th
    European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part
    XXXIII Computer vision–eccv 2022: 17th european conference, tel aviv, israel,
    october 23–27, 2022, proceedings, part xxxiii (\BPGS90–107). \PrintBackRefs\CurrentBib'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'X. Liu \BOthers. [\APACyear2022] \APACinsertmetastarliu2022graphcspn{APACrefauthors}Liu,
    X., Shao, X., Wang, B., Li, Y.\BCBL Wang, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleGraphCSPN:
    基于几何的深度补全通过动态 GCNs GraphCSPN: Geometry-aware depth completion via dynamic GCNs.\BBCQ
    \APACrefbtitle计算机视觉–ECCV 2022: 第17届欧洲会议，特拉维夫，以色列，2022年10月23-27日，会议论文集，第三十三部分 Computer
    vision–eccv 2022: 17th european conference, tel aviv, israel, october 23–27, 2022,
    proceedings, part xxxiii (\BPGS90–107). \PrintBackRefs\CurrentBib'
- en: 'Lopez-Rodriguez \BOthers. [\APACyear2020] \APACinsertmetastarlopez2020project{APACrefauthors}Lopez-Rodriguez,
    A., Busam, B.\BCBL Mikolajczyk, K. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleProject
    to adapt: Domain adaptation for depth completion from noisy and sparse sensor
    data Project to adapt: Domain adaptation for depth completion from noisy and sparse
    sensor data.\BBCQ \APACrefbtitleProceedings of the Asian Conference on Computer
    Vision. Proceedings of the asian conference on computer vision. \PrintBackRefs\CurrentBib'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lopez-Rodriguez \BOthers. [\APACyear2020] \APACinsertmetastarlopez2020project{APACrefauthors}Lopez-Rodriguez,
    A., Busam, B.\BCBL Mikolajczyk, K. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleProject
    to adapt: Domain adaptation for depth completion from noisy and sparse sensor
    data Project to adapt: Domain adaptation for depth completion from noisy and sparse
    sensor data.\BBCQ \APACrefbtitleProceedings of the Asian Conference on Computer
    Vision. Proceedings of the asian conference on computer vision. \PrintBackRefs\CurrentBib'
- en: J. Lu \BOthers. [\APACyear2016] \APACinsertmetastarlu2016hierarchical{APACrefauthors}Lu,
    J., Yang, J., Batra, D.\BCBL Parikh, D. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleHierarchical
    question-image co-attention for visual question answering Hierarchical question-image
    co-attention for visual question answering.\BBCQ \APACjournalVolNumPagesAdvances
    in neural information processing systems29, \PrintBackRefs\CurrentBib
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. Lu \BOthers. [\APACyear2016] \APACinsertmetastarlu2016hierarchical{APACrefauthors}Lu,
    J., Yang, J., Batra, D.\BCBL Parikh, D. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleHierarchical
    question-image co-attention for visual question answering Hierarchical question-image
    co-attention for visual question answering.\BBCQ \APACjournalVolNumPagesAdvances
    in neural information processing systems29, \PrintBackRefs\CurrentBib
- en: K. Lu \BOthers. [\APACyear2020] \APACinsertmetastarlu2020depth{APACrefauthors}Lu,
    K., Barnes, N., Anwar, S.\BCBL Zheng, L. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleFrom
    depth what can you see? Depth completion via auxiliary image reconstruction From
    depth what can you see? depth completion via auxiliary image reconstruction.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF conference on computer vision and pattern
    recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS11306–11315). \PrintBackRefs\CurrentBib
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K. Lu \BOthers. [\APACyear2020] \APACinsertmetastarlu2020depth{APACrefauthors}Lu,
    K., Barnes, N., Anwar, S.\BCBL Zheng, L. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleFrom
    depth what can you see? Depth completion via auxiliary image reconstruction From
    depth what can you see? depth completion via auxiliary image reconstruction.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF conference on computer vision and pattern
    recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS11306–11315). \PrintBackRefs\CurrentBib
- en: S. Lu \BOthers. [\APACyear2014] \APACinsertmetastarlu2014depth{APACrefauthors}Lu,
    S., Ren, X.\BCBL Liu, F. \APACrefYearMonthDay2014. \BBOQ\APACrefatitleDepth enhancement
    via low-rank matrix completion Depth enhancement via low-rank matrix completion.\BBCQ
    \APACrefbtitleProceedings of the IEEE conference on computer vision and pattern
    recognition Proceedings of the ieee conference on computer vision and pattern
    recognition (\BPGS3390–3397). \PrintBackRefs\CurrentBib
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Lu \BOthers. [\APACyear2014] \APACinsertmetastarlu2014depth{APACrefauthors}Lu,
    S., Ren, X.\BCBL Liu, F. \APACrefYearMonthDay2014. \BBOQ\APACrefatitleDepth enhancement
    via low-rank matrix completion Depth enhancement via low-rank matrix completion.\BBCQ
    \APACrefbtitleProceedings of the IEEE conference on computer vision and pattern
    recognition Proceedings of the ieee conference on computer vision and pattern
    recognition (\BPGS3390–3397). \PrintBackRefs\CurrentBib
- en: Lutio \BOthers. [\APACyear2019] \APACinsertmetastarlutio2019guided{APACrefauthors}Lutio,
    R.d., D’aronco, S., Wegner, J.D.\BCBL Schindler, K. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleGuided super-resolution as pixel-to-pixel transformation Guided
    super-resolution as pixel-to-pixel transformation.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF International Conference on Computer Vision Proceedings of the
    ieee/cvf international conference on computer vision (\BPGS8829–8837). \PrintBackRefs\CurrentBib
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lutio \BOthers. [\APACyear2019] \APACinsertmetastarlutio2019guided{APACrefauthors}Lutio,
    R.d., D’aronco, S., Wegner, J.D.\BCBL Schindler, K. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleGuided super-resolution as pixel-to-pixel transformation Guided
    super-resolution as pixel-to-pixel transformation.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF International Conference on Computer Vision Proceedings of the
    ieee/cvf international conference on computer vision (\BPGS8829–8837). \PrintBackRefs\CurrentBib
- en: 'Ma \BOthers. [\APACyear2018] \APACinsertmetastarMa2018SelfSupervisedSS{APACrefauthors}Ma,
    F., Cavalheiro, G.V.\BCBL Karaman, S. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleSelf-Supervised
    Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera
    Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and
    monocular camera.\BBCQ \APACjournalVolNumPages2019 International Conference on
    Robotics and Automation (ICRA)3288-3295, \PrintBackRefs\CurrentBib'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma \BOthers. [\APACyear2018] \APACinsertmetastarMa2018SelfSupervisedSS{APACrefauthors}Ma,
    F., Cavalheiro, G.V.\BCBL Karaman, S. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleSelf-Supervised
    Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera
    Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and
    monocular camera.\BBCQ \APACjournalVolNumPages2019 International Conference on
    Robotics and Automation (ICRA)3288-3295, \PrintBackRefs\CurrentBib'
- en: 'Ma \BOthers. [\APACyear2019] \APACinsertmetastarma2019self{APACrefauthors}Ma,
    F., Cavalheiro, G.V.\BCBL Karaman, S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleSelf-supervised
    sparse-to-dense: Self-supervised depth completion from lidar and monocular camera
    Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and
    monocular camera.\BBCQ \APACrefbtitle2019 International Conference on Robotics
    and Automation (ICRA) 2019 international conference on robotics and automation
    (icra) (\BPGS3288–3295). \PrintBackRefs\CurrentBib'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma \BOthers. [\APACyear2019] \APACinsertmetastarma2019self{APACrefauthors}Ma,
    F., Cavalheiro, G.V.\BCBL Karaman, S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle自监督稀疏到密集：从激光雷达和单目相机中自监督深度补全
    自监督稀疏到密集：从激光雷达和单目相机中自监督深度补全。\BBCQ \APACrefbtitle2019国际机器人与自动化会议 (ICRA) 2019 international
    conference on robotics and automation (icra) (\BPGS3288–3295). \PrintBackRefs\CurrentBib
- en: 'Ma \BBA Karaman [\APACyear2018] \APACinsertmetastarma2018sparse{APACrefauthors}Ma,
    F.\BCBT \BBA Karaman, S. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleSparse-to-dense:
    Depth prediction from sparse depth samples and a single image Sparse-to-dense:
    Depth prediction from sparse depth samples and a single image.\BBCQ \APACrefbtitle2018
    IEEE international conference on robotics and automation (ICRA) 2018 ieee international
    conference on robotics and automation (icra) (\BPGS4796–4803). \PrintBackRefs\CurrentBib'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma \BBA Karaman [\APACyear2018] \APACinsertmetastarma2018sparse{APACrefauthors}Ma,
    F.\BCBT \BBA Karaman, S. \APACrefYearMonthDay2018. \BBOQ\APACrefatitle稀疏到密集：从稀疏深度样本和单幅图像中预测深度
    稀疏到密集：从稀疏深度样本和单幅图像中预测深度。\BBCQ \APACrefbtitle2018 IEEE国际机器人与自动化会议 (ICRA) 2018 ieee国际机器人与自动化会议
    (icra) (\BPGS4796–4803). \PrintBackRefs\CurrentBib
- en: 'Manglik \BOthers. [\APACyear2019] \APACinsertmetastarmanglik2019forecasting{APACrefauthors}Manglik,
    A., Weng, X., Ohn-Bar, E.\BCBL Kitanil, K.M. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleForecasting
    time-to-collision from monocular video: Feasibility, dataset, and challenges Forecasting
    time-to-collision from monocular video: Feasibility, dataset, and challenges.\BBCQ
    \APACrefbtitle2019 IEEE/RSJ International Conference on Intelligent Robots and
    Systems (IROS) 2019 ieee/rsj international conference on intelligent robots and
    systems (iros) (\BPGS8081–8088). \PrintBackRefs\CurrentBib'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manglik \BOthers. [\APACyear2019] \APACinsertmetastarmanglik2019forecasting{APACrefauthors}Manglik,
    A., Weng, X., Ohn-Bar, E.\BCBL Kitanil, K.M. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle从单目视频中预测碰撞时间：可行性、数据集和挑战
    从单目视频中预测碰撞时间：可行性、数据集和挑战。\BBCQ \APACrefbtitle2019 IEEE/RSJ国际智能机器人与系统会议 (IROS) 2019
    ieee/rsj国际智能机器人与系统会议 (iros) (\BPGS8081–8088). \PrintBackRefs\CurrentBib
- en: Marivani \BOthers. [\APACyear2020] \APACinsertmetastarmarivani2020multimodal{APACrefauthors}Marivani,
    I., Tsiligianni, E., Cornelis, B.\BCBL Deligiannis, N. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleMultimodal deep unfolding for guided image super-resolution
    Multimodal deep unfolding for guided image super-resolution.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Image Processing298443–8456, \PrintBackRefs\CurrentBib
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marivani \BOthers. [\APACyear2020] \APACinsertmetastarmarivani2020multimodal{APACrefauthors}Marivani,
    I., Tsiligianni, E., Cornelis, B.\BCBL Deligiannis, N. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitle多模态深度展开用于引导图像超分辨率 多模态深度展开用于引导图像超分辨率。\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Image Processing298443–8456, \PrintBackRefs\CurrentBib
- en: Mayer \BOthers. [\APACyear2016] \APACinsertmetastarmayer2016large{APACrefauthors}Mayer,
    N., Ilg, E., Hausser, P., Fischer, P., Cremers, D., Dosovitskiy, A.\BCBL Brox,
    T. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleA large dataset to train convolutional
    networks for disparity, optical flow, and scene flow estimation A large dataset
    to train convolutional networks for disparity, optical flow, and scene flow estimation.\BBCQ
    \APACrefbtitleProceedings of the IEEE conference on computer vision and pattern
    recognition Proceedings of the ieee conference on computer vision and pattern
    recognition (\BPGS4040–4048). \PrintBackRefs\CurrentBib
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mayer \BOthers. [\APACyear2016] \APACinsertmetastarmayer2016large{APACrefauthors}Mayer,
    N., Ilg, E., Hausser, P., Fischer, P., Cremers, D., Dosovitskiy, A.\BCBL Brox,
    T. \APACrefYearMonthDay2016. \BBOQ\APACrefatitle一个大型数据集用于训练卷积网络以进行视差、光流和场景流估计
    一个大型数据集用于训练卷积网络以进行视差、光流和场景流估计。\BBCQ \APACrefbtitleIEEE计算机视觉与模式识别会议论文集 Proceedings
    of the ieee conference on computer vision and pattern recognition (\BPGS4040–4048).
    \PrintBackRefs\CurrentBib
- en: Metzger \BOthers. [\APACyear2022] \APACinsertmetastarmetzger2022guided{APACrefauthors}Metzger,
    N., Daudt, R.C.\BCBL Schindler, K. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleGuided
    Depth Super-Resolution by Deep Anisotropic Diffusion Guided depth super-resolution
    by deep anisotropic diffusion.\BBCQ \APACjournalVolNumPagesarXiv preprint arXiv:2211.11592,
    \PrintBackRefs\CurrentBib
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metzger \BOthers. [\APACyear2022] \APACinsertmetastarmetzger2022guided{APACrefauthors}Metzger，N.，Daudt，R.C.\BCBL
    Schindler，K. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle通过深度各向异性扩散的引导深度超分辨率 通过深度各向异性扩散的引导深度超分辨率。\BBCQ
    \APACjournalVolNumPagesarXiv预印本arXiv:2211.11592，\PrintBackRefs\CurrentBib
- en: 'Nazir \BOthers. [\APACyear2022] \APACinsertmetastarnazir2022semattnet{APACrefauthors}Nazir,
    D., Pagani, A., Liwicki, M., Stricker, D.\BCBL Afzal, M.Z. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleSemAttNet: Toward Attention-Based Semantic Aware Guided Depth
    Completion Semattnet: Toward attention-based semantic aware guided depth completion.\BBCQ
    \APACjournalVolNumPagesIEEE Access10120781–120791, \PrintBackRefs\CurrentBib'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nazir \BOthers. [\APACyear2022] \APACinsertmetastarnazir2022semattnet{APACrefauthors}Nazir，D.，Pagani，A.，Liwicki，M.，Stricker，D.\BCBL
    Afzal，M.Z. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleSemAttNet：面向基于注意力的语义感知引导深度完成Semattnet:
    Toward attention-based semantic aware guided depth completion。\BBCQ \APACjournalVolNumPagesIEEE
    Access10120781–120791，\PrintBackRefs\CurrentBib'
- en: Nguyen \BBA Yoo [\APACyear2022] \APACinsertmetastarnguyen2022patchgan{APACrefauthors}Nguyen,
    T.\BCBT \BBA Yoo, M. \APACrefYearMonthDay2022. \BBOQ\APACrefatitlePatchGAN-Based
    Depth Completion in Autonomous Vehicle Patchgan-based depth completion in autonomous
    vehicle.\BBCQ \APACrefbtitle2022 International Conference on Information Networking
    (ICOIN) 2022 international conference on information networking (icoin) (\BPGS498–501).
    \PrintBackRefs\CurrentBib
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen \BBA Yoo [\APACyear2022] \APACinsertmetastarnguyen2022patchgan{APACrefauthors}Nguyen，T.\BCBT \BBA
    Yoo，M. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle基于PatchGAN的自主车辆深度完成PatchGAN-based
    depth completion in autonomous vehicle。\BBCQ \APACrefbtitle2022国际信息网络会议2022 international
    conference on information networking (icoin) (\BPGS498–501)。\PrintBackRefs\CurrentBib
- en: 'Park \BOthers. [\APACyear2020] \APACinsertmetastarpark2020non{APACrefauthors}Park,
    J., Joo, K., Hu, Z., Liu, C\BHBIK.\BCBL So Kweon, I. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleNon-local spatial propagation network for depth completion
    Non-local spatial propagation network for depth completion.\BBCQ \APACrefbtitleComputer
    Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part XIII 16 Computer vision–eccv 2020: 16th european conference, glasgow, uk,
    august 23–28, 2020, proceedings, part xiii 16 (\BPGS120–136). \PrintBackRefs\CurrentBib'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park等人。[\APACyear2020] \APACinsertmetastarpark2020non{APACrefauthors}Park，J.，Joo，K.，Hu，Z.，刘，C\BHBIK.\BCBL
    So Kweon，I. \APACrefYearMonthDay2020. \BBOQ\APACrefatitle非局部空间传播网络用于深度完成的非局部空间传播网络。\BBCQ
    \APACrefbtitle计算机视觉–ECCV 2020：第16届欧洲会议，2020年8月23日–28日，英国格拉斯哥，第13部分16计算机视觉–eccv
    2020：第16届欧洲会议，格拉斯哥，英国，2020年8月23日–28日，第xiii部分16 (\BPGS120–136)。\PrintBackRefs\CurrentBib
- en: Patil \BOthers. [\APACyear2022] \APACinsertmetastarpatil2022multi{APACrefauthors}Patil,
    P.W., Dudhane, A., Chaudhary, S.\BCBL Murala, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleMulti-frame
    based adversarial learning approach for video surveillance Multi-frame based adversarial
    learning approach for video surveillance.\BBCQ \APACjournalVolNumPagesPattern
    Recognition122108350, \PrintBackRefs\CurrentBib
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patil \BOthers. [\APACyear2022] \APACinsertmetastarpatil2022multi{APACrefauthors}Patil，P.W.，Dudhane，A.，Chaudhary，S.\BCBL
    Murala，S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle视频监控的多帧基于对抗学习方法multi-frame
    based adversarial learning approach for video surveillance。\BBCQ \APACjournalVolNumPagesPattern
    Recognition122108350，\PrintBackRefs\CurrentBib
- en: Peng \BOthers. [\APACyear2022] \APACinsertmetastarpeng2022pixelwise{APACrefauthors}Peng,
    R., Zhang, T., Li, B.\BCBL Wang, Y. \APACrefYearMonthDay2022. \BBOQ\APACrefatitlePixelwise
    Adaptive Discretization with Uncertainty Sampling for Depth Completion Pixelwise
    adaptive discretization with uncertainty sampling for depth completion.\BBCQ \APACrefbtitleProceedings
    of the 30th ACM International Conference on Multimedia Proceedings of the 30th
    acm international conference on multimedia (\BPGS3926–3935). \PrintBackRefs\CurrentBib
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng \BOthers. [\APACyear2022] \APACinsertmetastarpeng2022pixelwise{APACrefauthors}Peng，R.，张，T.，李，B.\BCBL
    王，Y. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle像素级自适应离散化与不确定性采样用于深度完成的像素级自适应离散化与不确定性采样。\BBCQ
    \APACrefbtitle第30届ACM国际多媒体会议论文集第30届acm国际多媒体会议 (\BPGS3926–3935)。\PrintBackRefs\CurrentBib
- en: Petschnigg \BOthers. [\APACyear2004] \APACinsertmetastarpetschnigg2004digital{APACrefauthors}Petschnigg,
    G., Szeliski, R., Agrawala, M., Cohen, M., Hoppe, H.\BCBL Toyama, K. \APACrefYearMonthDay2004.
    \BBOQ\APACrefatitleDigital photography with flash and no-flash image pairs Digital
    photography with flash and no-flash image pairs.\BBCQ \APACjournalVolNumPagesACM
    transactions on graphics (TOG)233664–672, \PrintBackRefs\CurrentBib
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petschnigg \BOthers. [\APACyear2004] \APACinsertmetastarpetschnigg2004digital{APACrefauthors}Petschnigg,
    G., Szeliski, R., Agrawala, M., Cohen, M., Hoppe, H.\BCBL Toyama, K. \APACrefYearMonthDay2004.
    \BBOQ\APACrefatitle有闪光和无闪光图像对的数字摄影 有闪光和无闪光图像对的数字摄影。\BBCQ \APACjournalVolNumPagesACM
    transactions on graphics (TOG)233664–672, \PrintBackRefs\CurrentBib
- en: 'Qi \BOthers. [\APACyear2017] \APACinsertmetastarqi2017pointnet{APACrefauthors}Qi,
    C.R., Su, H., Mo, K.\BCBL Guibas, L.J. \APACrefYearMonthDay2017. \BBOQ\APACrefatitlePointnet:
    Deep learning on point sets for 3d classification and segmentation Pointnet: Deep
    learning on point sets for 3d classification and segmentation.\BBCQ \APACrefbtitleProceedings
    of the IEEE conference on computer vision and pattern recognition Proceedings
    of the ieee conference on computer vision and pattern recognition (\BPGS652–660).
    \PrintBackRefs\CurrentBib'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qi \BOthers. [\APACyear2017] \APACinsertmetastarqi2017pointnet{APACrefauthors}Qi,
    C.R., Su, H., Mo, K.\BCBL Guibas, L.J. \APACrefYearMonthDay2017. \BBOQ\APACrefatitlePointnet:
    基于点集的深度学习用于3D分类和分割 Pointnet: 基于点集的深度学习用于3D分类和分割。\BBCQ \APACrefbtitleIEEE计算机视觉与模式识别会议论文集
    Proceedings of the ieee conference on computer vision and pattern recognition
    (\BPGS652–660). \PrintBackRefs\CurrentBib'
- en: Qiao, Ge, Deng\BCBL \BOthers. [\APACyear2023] \APACinsertmetastarqiao2022depth{APACrefauthors}Qiao,
    X., Ge, C., Deng, P., Wei, H., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleDepth Restoration in Under-Display Time-of-Flight Imaging Depth
    restoration in under-display time-of-flight imaging.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Pattern Analysis and Machine Intelligence4555668-5683, {APACrefDOI}  [https://doi.org/10.1109/TPAMI.2022.3209905](https://doi.org/10.1109/TPAMI.2022.3209905)
    \PrintBackRefs\CurrentBib
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao, Ge, Deng\BCBL \BOthers. [\APACyear2023] \APACinsertmetastarqiao2022depth{APACrefauthors}Qiao,
    X., Ge, C., Deng, P., Wei, H., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitle显示下时间飞行成像中的深度恢复 显示下时间飞行成像中的深度恢复。\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Pattern Analysis and Machine Intelligence4555668-5683, {APACrefDOI}
    [https://doi.org/10.1109/TPAMI.2022.3209905](https://doi.org/10.1109/TPAMI.2022.3209905)
    \PrintBackRefs\CurrentBib
- en: Qiao \BOthers. [\APACyear2020] \APACinsertmetastarqiao2020valid{APACrefauthors}Qiao,
    X., Ge, C., Yao, H., Deng, P.\BCBL Zhou, Y. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleValid
    depth data extraction and correction for time-of-flight camera Valid depth data
    extraction and correction for time-of-flight camera.\BBCQ \APACrefbtitleTwelfth
    International Conference on Machine Vision (ICMV 2019) Twelfth international conference
    on machine vision (icmv 2019) (\BVOL11433, \BPGS696–703). \PrintBackRefs\CurrentBib
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao \BOthers. [\APACyear2020] \APACinsertmetastarqiao2020valid{APACrefauthors}Qiao,
    X., Ge, C., Yao, H., Deng, P.\BCBL Zhou, Y. \APACrefYearMonthDay2020. \BBOQ\APACrefatitle时间飞行相机的有效深度数据提取与修正
    时间飞行相机的有效深度数据提取与修正。\BBCQ \APACrefbtitle第十二届国际机器视觉会议（ICMV 2019） 第十二届国际机器视觉会议（ICMV
    2019） (\BVOL11433, \BPGS696–703). \PrintBackRefs\CurrentBib
- en: Qiao, Ge, Zhang\BCBL \BOthers. [\APACyear2023] \APACinsertmetastarqiao2023depth{APACrefauthors}Qiao,
    X., Ge, C., Zhang, Y., Zhou, Y., Tosi, F., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleDepth super-resolution from explicit and implicit high-frequency
    features Depth super-resolution from explicit and implicit high-frequency features.\BBCQ
    \APACjournalVolNumPagesComputer Vision and Image Understanding237103841, \PrintBackRefs\CurrentBib
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao, Ge, Zhang\BCBL \BOthers. [\APACyear2023] \APACinsertmetastarqiao2023depth{APACrefauthors}Qiao,
    X., Ge, C., Zhang, Y., Zhou, Y., Tosi, F., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitle从显式和隐式高频特征中提取深度超分辨率 从显式和隐式高频特征中提取深度超分辨率。\BBCQ \APACjournalVolNumPagesComputer
    Vision and Image Understanding237103841, \PrintBackRefs\CurrentBib
- en: Qiao, Ge, Zhao\BCBL \BOthers. [\APACyear2023] \APACinsertmetastarqiao2023self{APACrefauthors}Qiao,
    X., Ge, C., Zhao, C., Tosi, F., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleSelf-supervised depth super-resolution with contrastive multiview
    pre-training Self-supervised depth super-resolution with contrastive multiview
    pre-training.\BBCQ \APACjournalVolNumPagesNeural Networks168223–236, \PrintBackRefs\CurrentBib
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao, Ge, Zhao\BCBL \BOthers. [\APACyear2023] \APACinsertmetastarqiao2023self{APACrefauthors}Qiao,
    X., Ge, C., Zhao, C., Tosi, F., Poggi, M.\BCBL Mattoccia, S. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitle自监督深度超分辨率与对比多视角预训练 自监督深度超分辨率与对比多视角预训练。\BBCQ \APACjournalVolNumPagesNeural
    Networks168223–236, \PrintBackRefs\CurrentBib
- en: D. Qiu \BOthers. [\APACyear2019] \APACinsertmetastarqiu2019deep{APACrefauthors}Qiu,
    D., Pang, J., Sun, W.\BCBL Yang, C. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDeep
    end-to-end alignment and refinement for time-of-flight RGB-D module Deep end-to-end
    alignment and refinement for time-of-flight rgb-d module.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF International Conference on Computer Vision Proceedings of the
    ieee/cvf international conference on computer vision (\BPGS9994–10003). \PrintBackRefs\CurrentBib
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D. Qiu \BOthers. [\APACyear2019] \APACinsertmetastarqiu2019deep{APACrefauthors}Qiu,
    D., Pang, J., Sun, W.\BCBL Yang, C. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle深度端到端对齐与时间飞行RGB-D模块的优化
    Deep end-to-end alignment and refinement for time-of-flight rgb-d module.\BBCQ
    \APACrefbtitleIEEE/CVF国际计算机视觉会议论文集 Proceedings of the IEEE/CVF International Conference
    on Computer Vision (\BPGS9994–10003). \PrintBackRefs\CurrentBib
- en: 'J. Qiu \BOthers. [\APACyear2019] \APACinsertmetastarqiu2019deeplidar{APACrefauthors}Qiu,
    J., Cui, Z., Zhang, Y., Zhang, X., Liu, S., Zeng, B.\BCBL Pollefeys, M. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleDeeplidar: Deep surface normal guided depth prediction for
    outdoor scene from sparse lidar data and single color image Deeplidar: Deep surface
    normal guided depth prediction for outdoor scene from sparse lidar data and single
    color image.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Proceedings of the ieee/cvf conference on computer
    vision and pattern recognition (\BPGS3313–3322). \PrintBackRefs\CurrentBib'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'J. Qiu \BOthers. [\APACyear2019] \APACinsertmetastarqiu2019deeplidar{APACrefauthors}Qiu,
    J., Cui, Z., Zhang, Y., Zhang, X., Liu, S., Zeng, B.\BCBL Pollefeys, M. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleDeeplidar: 基于深度表面法向的稀疏激光雷达数据和单张彩色图像的户外场景深度预测 Deeplidar: Deep
    surface normal guided depth prediction for outdoor scene from sparse lidar data
    and single color image.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集 Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (\BPGS3313–3322).
    \PrintBackRefs\CurrentBib'
- en: Qu \BOthers. [\APACyear2020] \APACinsertmetastarqu2020depth{APACrefauthors}Qu,
    C., Nguyen, T.\BCBL Taylor, C. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDepth
    completion via deep basis fitting Depth completion via deep basis fitting.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision Proceedings of the ieee/cvf winter conference on applications
    of computer vision (\BPGS71–80). \PrintBackRefs\CurrentBib
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qu \BOthers. [\APACyear2020] \APACinsertmetastarqu2020depth{APACrefauthors}Qu,
    C., Nguyen, T.\BCBL Taylor, C. \APACrefYearMonthDay2020. \BBOQ\APACrefatitle通过深度基函数拟合实现深度补全
    Depth completion via deep basis fitting.\BBCQ \APACrefbtitleIEEE/CVF冬季计算机视觉应用会议论文集
    Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision
    (\BPGS71–80). \PrintBackRefs\CurrentBib
- en: 'Ramesh \BOthers. [\APACyear2023] \APACinsertmetastarramesh2023siunet{APACrefauthors}Ramesh,
    A.N., Giovanneschi, F.\BCBL González-Huici, M.A. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleSIUNet:
    Sparsity Invariant U-Net for Edge-Aware Depth Completion Siunet: Sparsity invariant
    u-net for edge-aware depth completion.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision Proceedings of the ieee/cvf
    winter conference on applications of computer vision (\BPGS5818–5827). \PrintBackRefs\CurrentBib'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ramesh \BOthers. [\APACyear2023] \APACinsertmetastarramesh2023siunet{APACrefauthors}Ramesh,
    A.N., Giovanneschi, F.\BCBL González-Huici, M.A. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleSIUNet:
    稀疏不变U-Net用于边缘感知深度补全 Siunet: Sparsity invariant u-net for edge-aware depth completion.\BBCQ
    \APACrefbtitleIEEE/CVF冬季计算机视觉应用会议论文集 Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision (\BPGS5818–5827). \PrintBackRefs\CurrentBib'
- en: Riegler, Ferstl\BCBL \BOthers. [\APACyear2016] \APACinsertmetastarriegler2016deep{APACrefauthors}Riegler,
    G., Ferstl, D., Rüther, M.\BCBL Horst, B. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleA
    Deep Primal-Dual Network for Guided Depth Super-Resolution A deep primal-dual
    network for guided depth super-resolution.\BBCQ \APACrefbtitleBritish Machine
    Vision Conference. British machine vision conference. \PrintBackRefs\CurrentBib
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riegler, Ferstl\BCBL \BOthers. [\APACyear2016] \APACinsertmetastarriegler2016deep{APACrefauthors}Riegler,
    G., Ferstl, D., Rüther, M.\BCBL Horst, B. \APACrefYearMonthDay2016. \BBOQ\APACrefatitle一种用于引导深度超分辨率的深度原始对偶网络
    A deep primal-dual network for guided depth super-resolution.\BBCQ \APACrefbtitle英国机器视觉会议论文集
    British Machine Vision Conference. \PrintBackRefs\CurrentBib
- en: 'Riegler, Rüther\BCBL \BBA Bischof [\APACyear2016] \APACinsertmetastarriegler2016atgv{APACrefauthors}Riegler,
    G., Rüther, M.\BCBL Bischof, H. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleAtgv-net:
    Accurate depth super-resolution Atgv-net: Accurate depth super-resolution.\BBCQ
    \APACrefbtitleComputer Vision–ECCV 2016: 14th European Conference, Amsterdam,
    The Netherlands, October 11-14, 2016, Proceedings, Part III 14 Computer vision–eccv
    2016: 14th european conference, amsterdam, the netherlands, october 11-14, 2016,
    proceedings, part iii 14 (\BPGS268–284). \PrintBackRefs\CurrentBib'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Riegler, Rüther\BCBL \BBA Bischof [\APACyear2016] \APACinsertmetastarriegler2016atgv{APACrefauthors}Riegler,
    G., Rüther, M.\BCBL Bischof, H. \APACrefYearMonthDay2016. \BBOQ\APACrefatitleAtgv-net：精准的深度超分辨率
    Atgv-net: Accurate depth super-resolution.\BBCQ \APACrefbtitle计算机视觉–ECCV 2016：第14届欧洲会议，荷兰阿姆斯特丹，2016年10月11-14日，会议录，第三部分
    14 Computer vision–eccv 2016: 14th european conference, amsterdam, the netherlands,
    october 11-14, 2016, proceedings, part iii 14 (\BPGS268–284). \PrintBackRefs\CurrentBib'
- en: 'Romera \BOthers. [\APACyear2017] \APACinsertmetastarromera2017erfnet{APACrefauthors}Romera,
    E., Alvarez, J.M., Bergasa, L.M.\BCBL Arroyo, R. \APACrefYearMonthDay2017. \BBOQ\APACrefatitleErfnet:
    Efficient residual factorized convnet for real-time semantic segmentation Erfnet:
    Efficient residual factorized convnet for real-time semantic segmentation.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Intelligent Transportation Systems191263–272,
    \PrintBackRefs\CurrentBib'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Romera \BOthers. [\APACyear2017] \APACinsertmetastarromera2017erfnet{APACrefauthors}Romera,
    E., Alvarez, J.M., Bergasa, L.M.\BCBL Arroyo, R. \APACrefYearMonthDay2017. \BBOQ\APACrefatitleErfnet：高效的残差因子分解卷积网络用于实时语义分割
    Erfnet: Efficient residual factorized convnet for real-time semantic segmentation.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Intelligent Transportation Systems191263–272,
    \PrintBackRefs\CurrentBib'
- en: 'Ronneberger \BOthers. [\APACyear2015] \APACinsertmetastarronneberger2015u{APACrefauthors}Ronneberger,
    O., Fischer, P.\BCBL Brox, T. \APACrefYearMonthDay2015. \BBOQ\APACrefatitleU-net:
    Convolutional networks for biomedical image segmentation U-net: Convolutional
    networks for biomedical image segmentation.\BBCQ \APACrefbtitleMedical Image Computing
    and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,
    Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 Medical image computing
    and computer-assisted intervention–miccai 2015: 18th international conference,
    munich, germany, october 5-9, 2015, proceedings, part iii 18 (\BPGS234–241). \PrintBackRefs\CurrentBib'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ronneberger \BOthers. [\APACyear2015] \APACinsertmetastarronneberger2015u{APACrefauthors}Ronneberger,
    O., Fischer, P.\BCBL Brox, T. \APACrefYearMonthDay2015. \BBOQ\APACrefatitleU-net：用于生物医学图像分割的卷积网络
    U-net: Convolutional networks for biomedical image segmentation.\BBCQ \APACrefbtitle医学图像计算与计算机辅助干预–MICCAI
    2015：第18届国际会议，德国慕尼黑，2015年10月5-9日，会议录，第三部分 18 Medical image computing and computer-assisted
    intervention–miccai 2015: 18th international conference, munich, germany, october
    5-9, 2015, proceedings, part iii 18 (\BPGS234–241). \PrintBackRefs\CurrentBib'
- en: Ryu \BOthers. [\APACyear2021] \APACinsertmetastarryu2021scanline{APACrefauthors}Ryu,
    K., Lee, K\BHBIi., Cho, J.\BCBL Yoon, K\BHBIJ. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleScanline
    resolution-invariant depth completion using a single image and sparse LiDAR point
    cloud Scanline resolution-invariant depth completion using a single image and
    sparse lidar point cloud.\BBCQ \APACjournalVolNumPagesIEEE Robotics and Automation
    Letters646961–6968, \PrintBackRefs\CurrentBib
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ryu \BOthers. [\APACyear2021] \APACinsertmetastarryu2021scanline{APACrefauthors}Ryu,
    K., Lee, K\BHBIi., Cho, J.\BCBL Yoon, K\BHBIJ. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle扫描线分辨率不变的深度补全使用单张图像和稀疏激光雷达点云
    Scanline resolution-invariant depth completion using a single image and sparse
    lidar point cloud.\BBCQ \APACjournalVolNumPagesIEEE Robotics and Automation Letters646961–6968,
    \PrintBackRefs\CurrentBib
- en: 'Scharstein \BOthers. [\APACyear2014] \APACinsertmetastarscharstein2014high{APACrefauthors}Scharstein,
    D., Hirschmüller, H., Kitajima, Y., Krathwohl, G., Nešić, N., Wang, X.\BCBL Westling,
    P. \APACrefYearMonthDay2014. \BBOQ\APACrefatitleHigh-resolution stereo datasets
    with subpixel-accurate ground truth High-resolution stereo datasets with subpixel-accurate
    ground truth.\BBCQ \APACrefbtitlePattern Recognition: 36th German Conference,
    GCPR 2014, Münster, Germany, September 2-5, 2014, Proceedings 36 Pattern recognition:
    36th german conference, gcpr 2014, münster, germany, september 2-5, 2014, proceedings
    36 (\BPGS31–42). \PrintBackRefs\CurrentBib'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scharstein \BOthers. [\APACyear2014] \APACinsertmetastarscharstein2014high{APACrefauthors}Scharstein,
    D., Hirschmüller, H., Kitajima, Y., Krathwohl, G., Nešić, N., Wang, X.\BCBL Westling,
    P. \APACrefYearMonthDay2014. \BBOQ\APACrefatitle高分辨率立体数据集及子像素级准确的地面真实值 High-resolution
    stereo datasets with subpixel-accurate ground truth.\BBCQ \APACrefbtitle模式识别：第36届德国会议，GCPR
    2014，德国明斯特，2014年9月2-5日，会议录 36 Pattern recognition: 36th german conference, gcpr
    2014, münster, germany, september 2-5, 2014, proceedings 36 (\BPGS31–42). \PrintBackRefs\CurrentBib'
- en: Scharstein \BBA Pal [\APACyear2007] \APACinsertmetastarscharstein2007learning{APACrefauthors}Scharstein,
    D.\BCBT \BBA Pal, C. \APACrefYearMonthDay2007. \BBOQ\APACrefatitleLearning conditional
    random fields for stereo Learning conditional random fields for stereo.\BBCQ \APACrefbtitle2007
    IEEE Conference on Computer Vision and Pattern Recognition 2007 ieee conference
    on computer vision and pattern recognition (\BPGS1–8). \PrintBackRefs\CurrentBib
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scharstein \BBA Pal [\APACyear2007] \APACinsertmetastarscharstein2007learning{APACrefauthors}Scharstein,
    D.\BCBT \BBA Pal, C. \APACrefYearMonthDay2007. \BBOQ\APACrefatitle学习立体图像的条件随机场
    Learning conditional random fields for stereo.\BBCQ \APACrefbtitle2007 IEEE计算机视觉与模式识别会议论文集
    2007 IEEE Conference on Computer Vision and Pattern Recognition (\BPGS1–8). \PrintBackRefs\CurrentBib
- en: Scharstein \BBA Szeliski [\APACyear2003] \APACinsertmetastarscharstein2003high{APACrefauthors}Scharstein,
    D.\BCBT \BBA Szeliski, R. \APACrefYearMonthDay2003. \BBOQ\APACrefatitleHigh-accuracy
    stereo depth maps using structured light High-accuracy stereo depth maps using
    structured light.\BBCQ \APACrefbtitle2003 IEEE Computer Society Conference on
    Computer Vision and Pattern Recognition, 2003\. Proceedings. 2003 ieee computer
    society conference on computer vision and pattern recognition, 2003\. proceedings. (\BVOL1,
    \BPGSI–I). \PrintBackRefs\CurrentBib
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scharstein \BBA Szeliski [\APACyear2003] \APACinsertmetastarscharstein2003high{APACrefauthors}Scharstein,
    D.\BCBT \BBA Szeliski, R. \APACrefYearMonthDay2003. \BBOQ\APACrefatitle高精度立体深度图的结构光方法
    High-accuracy stereo depth maps using structured light.\BBCQ \APACrefbtitle2003
    IEEE计算机协会计算机视觉与模式识别会议论文集 2003 IEEE Computer Society Conference on Computer Vision
    and Pattern Recognition, 2003\. Proceedings. (\BVOL1, \BPGSI–I). \PrintBackRefs\CurrentBib
- en: 'Schuster \BOthers. [\APACyear2021] \APACinsertmetastarschuster2021ssgp{APACrefauthors}Schuster,
    R., Wasenmuller, O., Unger, C.\BCBL Stricker, D. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleSsgp:
    Sparse spatial guided propagation for robust and generic interpolation Ssgp: Sparse
    spatial guided propagation for robust and generic interpolation.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision Proceedings
    of the ieee/cvf winter conference on applications of computer vision (\BPGS197–206).
    \PrintBackRefs\CurrentBib'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schuster \BOthers. [\APACyear2021] \APACinsertmetastarschuster2021ssgp{APACrefauthors}Schuster,
    R., Wasenmuller, O., Unger, C.\BCBL Stricker, D. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleSsgp：稀疏空间引导传播用于稳健和通用的插值
    Ssgp: Sparse spatial guided propagation for robust and generic interpolation.\BBCQ
    \APACrefbtitleIEEE/CVF冬季计算机视觉应用会议论文集 Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision (\BPGS197–206). \PrintBackRefs\CurrentBib'
- en: Shacht \BOthers. [\APACyear2021] \APACinsertmetastarshacht2021single{APACrefauthors}Shacht,
    G., Danon, D., Fogel, S.\BCBL Cohen-Or, D. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleSingle
    pair cross-modality super resolution Single pair cross-modality super resolution.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS6378–6387). \PrintBackRefs\CurrentBib
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shacht \BOthers. [\APACyear2021] \APACinsertmetastarshacht2021single{APACrefauthors}Shacht,
    G., Danon, D., Fogel, S.\BCBL Cohen-Or, D. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle单对跨模态超分辨率
    Single pair cross-modality super resolution.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (\BPGS6378–6387). \PrintBackRefs\CurrentBib
- en: Shin \BOthers. [\APACyear2023] \APACinsertmetastarshin2023task{APACrefauthors}Shin,
    J., Shin, S.\BCBL Jeon, H\BHBIG. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleTask-specific
    Scene Structure Representations Task-specific scene structure representations.\BBCQ
    \APACjournalVolNumPagesarXiv preprint arXiv:2301.00555, \PrintBackRefs\CurrentBib
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin \BOthers. [\APACyear2023] \APACinsertmetastarshin2023task{APACrefauthors}Shin,
    J., Shin, S.\BCBL Jeon, H\BHBIG. \APACrefYearMonthDay2023. \BBOQ\APACrefatitle任务特定的场景结构表示
    Task-specific scene structure representations.\BBCQ \APACjournalVolNumPagesarXiv
    preprint arXiv:2301.00555, \PrintBackRefs\CurrentBib
- en: 'Shivakumar \BOthers. [\APACyear2019] \APACinsertmetastarshivakumar2019dfusenet{APACrefauthors}Shivakumar,
    S.S., Nguyen, T., Miller, I.D., Chen, S.W., Kumar, V.\BCBL Taylor, C.J. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleDfusenet: Deep fusion of rgb and sparse depth information for
    image guided dense depth completion Dfusenet: Deep fusion of rgb and sparse depth
    information for image guided dense depth completion.\BBCQ \APACrefbtitle2019 IEEE
    Intelligent Transportation Systems Conference (ITSC) 2019 ieee intelligent transportation
    systems conference (itsc) (\BPGS13–20). \PrintBackRefs\CurrentBib'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shivakumar \BOthers. [\APACyear2019] \APACinsertmetastarshivakumar2019dfusenet{APACrefauthors}Shivakumar,
    S.S., Nguyen, T., Miller, I.D., Chen, S.W., Kumar, V.\BCBL Taylor, C.J. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleDfusenet: Deep fusion of rgb and sparse depth information for
    image guided dense depth completion Dfusenet: Deep fusion of rgb and sparse depth
    information for image guided dense depth completion.\BBCQ \APACrefbtitle2019 IEEE
    Intelligent Transportation Systems Conference (ITSC) 2019 ieee intelligent transportation
    systems conference (itsc) (\BPGS13–20). \PrintBackRefs\CurrentBib'
- en: Silberman \BOthers. [\APACyear2012] \APACinsertmetastarsilberman2012indoor{APACrefauthors}Silberman,
    N., Hoiem, D., Kohli, P.\BCBL Fergus, R. \APACrefYearMonthDay2012. \BBOQ\APACrefatitleIndoor
    segmentation and support inference from rgbd images Indoor segmentation and support
    inference from rgbd images.\BBCQ \APACrefbtitleEuropean conference on computer
    vision European conference on computer vision (\BPGS746–760). \PrintBackRefs\CurrentBib
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silberman \BOthers. [\APACyear2012] \APACinsertmetastarsilberman2012indoor{APACrefauthors}Silberman,
    N., Hoiem, D., Kohli, P.\BCBL Fergus, R. \APACrefYearMonthDay2012. \BBOQ\APACrefatitleIndoor
    segmentation and support inference from rgbd images Indoor segmentation and support
    inference from rgbd images.\BBCQ \APACrefbtitleEuropean conference on computer
    vision European conference on computer vision (\BPGS746–760). \PrintBackRefs\CurrentBib
- en: P. Song \BOthers. [\APACyear2019] \APACinsertmetastarsong2019multimodal{APACrefauthors}Song,
    P., Deng, X., Mota, J.F., Deligiannis, N., Dragotti, P.L.\BCBL Rodrigues, M.R.
    \APACrefYearMonthDay2019. \BBOQ\APACrefatitleMultimodal image super-resolution
    via joint sparse representations induced by coupled dictionaries Multimodal image
    super-resolution via joint sparse representations induced by coupled dictionaries.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Computational Imaging657–72, \PrintBackRefs\CurrentBib
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P. Song \BOthers. [\APACyear2019] \APACinsertmetastarsong2019multimodal{APACrefauthors}Song,
    P., Deng, X., Mota, J.F., Deligiannis, N., Dragotti, P.L.\BCBL Rodrigues, M.R.
    \APACrefYearMonthDay2019. \BBOQ\APACrefatitleMultimodal image super-resolution
    via joint sparse representations induced by coupled dictionaries Multimodal image
    super-resolution via joint sparse representations induced by coupled dictionaries.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Computational Imaging657–72, \PrintBackRefs\CurrentBib
- en: 'S. Song \BOthers. [\APACyear2015] \APACinsertmetastarsong2015sun{APACrefauthors}Song,
    S., Lichtenberg, S.P.\BCBL Xiao, J. \APACrefYearMonthDay2015. \BBOQ\APACrefatitleSun
    rgb-d: A rgb-d scene understanding benchmark suite Sun rgb-d: A rgb-d scene understanding
    benchmark suite.\BBCQ \APACrefbtitleProceedings of the IEEE conference on computer
    vision and pattern recognition Proceedings of the ieee conference on computer
    vision and pattern recognition (\BPGS567–576). \PrintBackRefs\CurrentBib'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'S. Song \BOthers. [\APACyear2015] \APACinsertmetastarsong2015sun{APACrefauthors}Song,
    S., Lichtenberg, S.P.\BCBL Xiao, J. \APACrefYearMonthDay2015. \BBOQ\APACrefatitleSun
    rgb-d: A rgb-d scene understanding benchmark suite Sun rgb-d: A rgb-d scene understanding
    benchmark suite.\BBCQ \APACrefbtitleProceedings of the IEEE conference on computer
    vision and pattern recognition Proceedings of the ieee conference on computer
    vision and pattern recognition (\BPGS567–576). \PrintBackRefs\CurrentBib'
- en: X. Song \BOthers. [\APACyear2020] \APACinsertmetastarsong2020channel{APACrefauthors}Song,
    X., Dai, Y., Zhou, D., Liu, L., Li, W., Li, H.\BCBL Yang, R. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleChannel attention based iterative residual learning for depth
    map super-resolution Channel attention based iterative residual learning for depth
    map super-resolution.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf conference
    on computer vision and pattern recognition (\BPGS5631–5640). \PrintBackRefs\CurrentBib
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X. Song \BOthers. [\APACyear2020] \APACinsertmetastarsong2020channel{APACrefauthors}Song,
    X., Dai, Y., Zhou, D., Liu, L., Li, W., Li, H.\BCBL Yang, R. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleChannel attention based iterative residual learning for depth
    map super-resolution Channel attention based iterative residual learning for depth
    map super-resolution.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf conference
    on computer vision and pattern recognition (\BPGS5631–5640). \PrintBackRefs\CurrentBib
- en: Z. Song \BOthers. [\APACyear2021] \APACinsertmetastarsong2021self{APACrefauthors}Song,
    Z., Lu, J., Yao, Y.\BCBL Zhang, J. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleSelf-supervised
    depth completion from direct visual-LiDAR odometry in autonomous driving Self-supervised
    depth completion from direct visual-lidar odometry in autonomous driving.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Intelligent Transportation Systems23811654–11665,
    \PrintBackRefs\CurrentBib
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Z. Song \BOthers. [\APACyear2021] \APACinsertmetastarsong2021self{APACrefauthors}Song,
    Z., Lu, J., Yao, Y.\BCBL Zhang, J. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle自监督深度补全来自直接视觉-LiDAR
    里程计在自动驾驶中的应用 自监督深度补全来自直接视觉-LiDAR 里程计在自动驾驶中的应用.\BBCQ \APACjournalVolNumPages《IEEE
    智能交通系统汇刊》23811654–11665, \PrintBackRefs\CurrentBib
- en: H. Su \BOthers. [\APACyear2019] \APACinsertmetastarsu2019pixel{APACrefauthors}Su,
    H., Jampani, V., Sun, D., Gallo, O., Learned-Miller, E.\BCBL Kautz, J. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitlePixel-adaptive convolutional neural networks Pixel-adaptive
    convolutional neural networks.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf
    conference on computer vision and pattern recognition (\BPGS11166–11175). \PrintBackRefs\CurrentBib
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H. Su \BOthers. [\APACyear2019] \APACinsertmetastarsu2019pixel{APACrefauthors}Su,
    H., Jampani, V., Sun, D., Gallo, O., Learned-Miller, E.\BCBL Kautz, J. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitle像素自适应卷积神经网络 像素自适应卷积神经网络.\BBCQ \APACrefbtitle《IEEE/CVF 计算机视觉与模式识别会议论文集》
    《IEEE/CVF 计算机视觉与模式识别会议论文集》 (\BPGS11166–11175). \PrintBackRefs\CurrentBib
- en: S. Su \BOthers. [\APACyear2018] \APACinsertmetastarsu2018deep{APACrefauthors}Su,
    S., Heide, F., Wetzstein, G.\BCBL Heidrich, W. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleDeep
    end-to-end time-of-flight imaging Deep end-to-end time-of-flight imaging.\BBCQ
    \APACrefbtitleProceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition Proceedings of the ieee conference on computer vision and pattern
    recognition (\BPGS6383–6392). \PrintBackRefs\CurrentBib
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Su \BOthers. [\APACyear2018] \APACinsertmetastarsu2018deep{APACrefauthors}Su,
    S., Heide, F., Wetzstein, G.\BCBL Heidrich, W. \APACrefYearMonthDay2018. \BBOQ\APACrefatitle深度端到端飞行时间成像
    深度端到端飞行时间成像.\BBCQ \APACrefbtitle《IEEE 计算机视觉与模式识别会议论文集》 《IEEE 计算机视觉与模式识别会议论文集》
    (\BPGS6383–6392). \PrintBackRefs\CurrentBib
- en: B. Sun \BOthers. [\APACyear2021] \APACinsertmetastarsun2021learning{APACrefauthors}Sun,
    B., Ye, X., Li, B., Li, H., Wang, Z.\BCBL Xu, R. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleLearning
    scene structure guidance via cross-task knowledge transfer for single depth super-resolution
    Learning scene structure guidance via cross-task knowledge transfer for single
    depth super-resolution.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Proceedings of the ieee/cvf conference
    on computer vision and pattern recognition (\BPGS7792–7801). \PrintBackRefs\CurrentBib
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B. Sun \BOthers. [\APACyear2021] \APACinsertmetastarsun2021learning{APACrefauthors}Sun,
    B., Ye, X., Li, B., Li, H., Wang, Z.\BCBL Xu, R. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle通过跨任务知识转移进行单景深超分辨率的场景结构引导
    通过跨任务知识转移进行单景深超分辨率的场景结构引导.\BBCQ \APACrefbtitle《IEEE/CVF 计算机视觉与模式识别会议论文集》 《IEEE/CVF
    计算机视觉与模式识别会议论文集》 (\BPGS7792–7801). \PrintBackRefs\CurrentBib
- en: 'P. Sun \BOthers. [\APACyear2020] \APACinsertmetastarsun2020scalability{APACrefauthors}Sun,
    P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P.\BDBLothers
    \APACrefYearMonthDay2020. \BBOQ\APACrefatitleScalability in perception for autonomous
    driving: Waymo open dataset Scalability in perception for autonomous driving:
    Waymo open dataset.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF conference
    on computer vision and pattern recognition Proceedings of the ieee/cvf conference
    on computer vision and pattern recognition (\BPGS2446–2454). \PrintBackRefs\CurrentBib'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P. Sun \BOthers. [\APACyear2020] \APACinsertmetastarsun2020scalability{APACrefauthors}Sun,
    P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P.\BDBLothers
    \APACrefYearMonthDay2020. \BBOQ\APACrefatitle自动驾驶中的感知可扩展性：Waymo 开放数据集 自动驾驶中的感知可扩展性：Waymo
    开放数据集.\BBCQ \APACrefbtitle《IEEE/CVF 计算机视觉与模式识别会议论文集》 《IEEE/CVF 计算机视觉与模式识别会议论文集》
    (\BPGS2446–2454). \PrintBackRefs\CurrentBib
- en: Z. Sun \BOthers. [\APACyear2023] \APACinsertmetastarsun2023consistent{APACrefauthors}Sun,
    Z., Ye, W., Xiong, J., Choe, G., Wang, J., Su, S.\BCBL Ranjan, R. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleConsistent Direct Time-of-Flight Video Depth Super-Resolution
    Consistent direct time-of-flight video depth super-resolution.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS5075–5085).
    \PrintBackRefs\CurrentBib
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Z. Sun \BOthers. [\APACyear2023] \APACinsertmetastarsun2023consistent{APACrefauthors}Sun,
    Z., Ye, W., Xiong, J., Choe, G., Wang, J., Su, S.\BCBL Ranjan, R. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitle一致的直接时间飞行视频深度超分辨率 Consistent direct time-of-flight video depth
    super-resolution.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集 Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (\BPGS5075–5085).
    \PrintBackRefs\CurrentBib
- en: 'Tan \BBA Le [\APACyear2019] \APACinsertmetastartan2019efficientnet{APACrefauthors}Tan,
    M.\BCBT \BBA Le, Q. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleEfficientnet:
    Rethinking model scaling for convolutional neural networks Efficientnet: Rethinking
    model scaling for convolutional neural networks.\BBCQ \APACrefbtitleInternational
    conference on machine learning International conference on machine learning (\BPGS6105–6114).
    \PrintBackRefs\CurrentBib'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan \BBA Le [\APACyear2019] \APACinsertmetastartan2019efficientnet{APACrefauthors}Tan,
    M.\BCBT \BBA Le, Q. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleEfficientnet:
    重新思考卷积神经网络的模型扩展 Efficientnet: Rethinking model scaling for convolutional neural
    networks.\BBCQ \APACrefbtitle国际机器学习会议论文集 International conference on machine learning (\BPGS6105–6114).
    \PrintBackRefs\CurrentBib'
- en: J. Tang \BOthers. [\APACyear2021] \APACinsertmetastartang2021joint{APACrefauthors}Tang,
    J., Chen, X.\BCBL Zeng, G. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleJoint
    implicit image function for guided depth super-resolution Joint implicit image
    function for guided depth super-resolution.\BBCQ \APACrefbtitleProceedings of
    the 29th ACM International Conference on Multimedia Proceedings of the 29th acm
    international conference on multimedia (\BPGS4390–4399). \PrintBackRefs\CurrentBib
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. Tang \BOthers. [\APACyear2021] \APACinsertmetastartang2021joint{APACrefauthors}Tang,
    J., Chen, X.\BCBL Zeng, G. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle联合隐式图像函数用于引导深度超分辨率
    Joint implicit image function for guided depth super-resolution.\BBCQ \APACrefbtitle第29届ACM国际多媒体会议论文集
    Proceedings of the 29th ACM International Conference on Multimedia (\BPGS4390–4399).
    \PrintBackRefs\CurrentBib
- en: J. Tang \BOthers. [\APACyear2020] \APACinsertmetastartang2020learning{APACrefauthors}Tang,
    J., Tian, F\BHBIP., Feng, W., Li, J.\BCBL Tan, P. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleLearning
    guided convolutional network for depth completion Learning guided convolutional
    network for depth completion.\BBCQ \APACjournalVolNumPagesIEEE Transactions on
    Image Processing301116–1129, \PrintBackRefs\CurrentBib
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. Tang \BOthers. [\APACyear2020] \APACinsertmetastartang2020learning{APACrefauthors}Tang,
    J., Tian, F\BHBIP., Feng, W., Li, J.\BCBL Tan, P. \APACrefYearMonthDay2020. \BBOQ\APACrefatitle学习引导的卷积网络用于深度补全
    Learning guided convolutional network for depth completion.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Image Processing301116–1129, \PrintBackRefs\CurrentBib
- en: 'Q. Tang \BOthers. [\APACyear2021] \APACinsertmetastartang2021bridgenet{APACrefauthors}Tang,
    Q., Cong, R., Sheng, R., He, L., Zhang, D., Zhao, Y.\BCBL Kwong, S. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleBridgenet: A joint learning network of depth map super-resolution
    and monocular depth estimation Bridgenet: A joint learning network of depth map
    super-resolution and monocular depth estimation.\BBCQ \APACrefbtitleProceedings
    of the 29th ACM International Conference on Multimedia Proceedings of the 29th
    acm international conference on multimedia (\BPGS2148–2157). \PrintBackRefs\CurrentBib'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Q. Tang \BOthers. [\APACyear2021] \APACinsertmetastartang2021bridgenet{APACrefauthors}Tang,
    Q., Cong, R., Sheng, R., He, L., Zhang, D., Zhao, Y.\BCBL Kwong, S. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleBridgenet: 一种深度图超分辨率和单目深度估计的联合学习网络 Bridgenet: A joint learning
    network of depth map super-resolution and monocular depth estimation.\BBCQ \APACrefbtitle第29届ACM国际多媒体会议论文集
    Proceedings of the 29th ACM International Conference on Multimedia (\BPGS2148–2157).
    \PrintBackRefs\CurrentBib'
- en: Tomasi \BBA Manduchi [\APACyear1998] \APACinsertmetastartomasi1998bilateral{APACrefauthors}Tomasi,
    C.\BCBT \BBA Manduchi, R. \APACrefYearMonthDay1998. \BBOQ\APACrefatitleBilateral
    filtering for gray and color images Bilateral filtering for gray and color images.\BBCQ
    \APACrefbtitleSixth international conference on computer vision (IEEE Cat. No.
    98CH36271) Sixth international conference on computer vision (ieee cat. no. 98ch36271) (\BPGS839–846).
    \PrintBackRefs\CurrentBib
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tomasi \BBA Manduchi [\APACyear1998] \APACinsertmetastartomasi1998bilateral{APACrefauthors}Tomasi,
    C.\BCBT \BBA Manduchi, R. \APACrefYearMonthDay1998. \BBOQ\APACrefatitle双边滤波用于灰度和彩色图像
    Bilateral filtering for gray and color images.\BBCQ \APACrefbtitle第六届国际计算机视觉会议论文集
    (IEEE Cat. No. 98CH36271) Sixth international conference on computer vision (ieee
    cat. no. 98ch36271) (\BPGS839–846). \PrintBackRefs\CurrentBib
- en: Uhrig \BOthers. [\APACyear2017] \APACinsertmetastaruhrig2017sparsity{APACrefauthors}Uhrig,
    J., Schneider, N., Schneider, L., Franke, U., Brox, T.\BCBL Geiger, A. \APACrefYearMonthDay2017.
    \BBOQ\APACrefatitleSparsity invariant cnns Sparsity invariant cnns.\BBCQ \APACrefbtitle2017
    international conference on 3D Vision (3DV) 2017 international conference on 3d
    vision (3dv) (\BPGS11–20). \PrintBackRefs\CurrentBib
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Uhrig 等人 [2017] 研究了《稀疏性不变的卷积神经网络》（Sparsity invariant cnns）。该论文在《2017年国际三维视觉会议》（2017
    international conference on 3D Vision (3DV)）上发表，页码为 11–20。
- en: Van Gansbeke \BOthers. [\APACyear2019] \APACinsertmetastarvan2019sparse{APACrefauthors}Van Gansbeke,
    W., Neven, D., De Brabandere, B.\BCBL Van Gool, L. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleSparse
    and noisy lidar completion with rgb guidance and uncertainty Sparse and noisy
    lidar completion with rgb guidance and uncertainty.\BBCQ \APACrefbtitle2019 16th
    international conference on machine vision applications (MVA) 2019 16th international
    conference on machine vision applications (mva) (\BPGS1–6). \PrintBackRefs\CurrentBib
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Gansbeke 等人 [2019] 提出了《稀疏且噪声的激光雷达补全与 RGB 指导及不确定性》（Sparse and noisy lidar
    completion with rgb guidance and uncertainty）。这篇文章发表于《2019年第16届国际机器视觉应用大会》（2019
    16th international conference on machine vision applications (mva)），页码为 1–6。
- en: Voynov \BOthers. [\APACyear2019] \APACinsertmetastarvoynov2019perceptual{APACrefauthors}Voynov,
    O., Artemov, A., Egiazarian, V., Notchenko, A., Bobrovskikh, G., Burnaev, E.\BCBL
    Zorin, D. \APACrefYearMonthDay2019. \BBOQ\APACrefatitlePerceptual deep depth super-resolution
    Perceptual deep depth super-resolution.\BBCQ \APACrefbtitleProceedings of the
    IEEE/CVF International Conference on Computer Vision Proceedings of the ieee/cvf
    international conference on computer vision (\BPGS5653–5663). \PrintBackRefs\CurrentBib
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voynov 等人 [2019] 发表了《感知深度超分辨率》（Perceptual deep depth super-resolution）。这篇文章在《IEEE/CVF国际计算机视觉会议论文集》（Proceedings
    of the IEEE/CVF International Conference on Computer Vision）上发表，页码为 5653–5663。
- en: 'Wan \BOthers. [\APACyear2023] \APACinsertmetastarwan2023seaformer{APACrefauthors}Wan,
    Q., Huang, Z., Lu, J., Yu, G.\BCBL Zhang, L. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleSeaFormer:
    Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation Seaformer:
    Squeeze-enhanced axial transformer for mobile semantic segmentation.\BBCQ \APACrefbtitleInternational
    Conference on Learning Representations (ICLR). International conference on learning
    representations (iclr). \PrintBackRefs\CurrentBib'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wan 等人 [2023] 介绍了《SeaFormer：用于移动语义分割的压缩增强轴向变换器》（SeaFormer: Squeeze-enhanced
    axial transformer for mobile semantic segmentation）。这篇论文发表于《国际学习表征会议》（International
    Conference on Learning Representations (ICLR)）。'
- en: H. Wang \BOthers. [\APACyear2022] \APACinsertmetastarwang2022rgb{APACrefauthors}Wang,
    H., Wang, M., Che, Z., Xu, Z., Qiao, X., Qi, M.\BDBLTang, J. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleRgb-depth fusion gan for indoor depth completion Rgb-depth
    fusion gan for indoor depth completion.\BBCQ \APACrefbtitleProceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings of
    the ieee/cvf conference on computer vision and pattern recognition (\BPGS6209–6218).
    \PrintBackRefs\CurrentBib
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022] 提出了《RGB-深度融合生成对抗网络用于室内深度补全》（Rgb-depth fusion gan for indoor depth
    completion）。这篇文章发表在《IEEE/CVF计算机视觉与模式识别会议论文集》（Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition），页码为 6209–6218。
- en: J. Wang \BOthers. [\APACyear2022] \APACinsertmetastarwang2022self{APACrefauthors}Wang,
    J., Liu, P.\BCBL Wen, F. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleSelf-Supervised
    Learning for RGB-Guided Depth Enhancement by Exploiting the Dependency Between
    RGB and Depth Self-supervised learning for rgb-guided depth enhancement by exploiting
    the dependency between rgb and depth.\BBCQ \APACjournalVolNumPagesIEEE Transactions
    on Image Processing32159–174, \PrintBackRefs\CurrentBib
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022] 研究了《利用 RGB 和深度之间的依赖关系进行 RGB 指导的深度增强的自监督学习》（Self-Supervised Learning
    for RGB-Guided Depth Enhancement by Exploiting the Dependency Between RGB and
    Depth）。该论文发表于《IEEE 图像处理汇刊》（IEEE Transactions on Image Processing），卷号 32，页码 159–174。
- en: J. Wang \BOthers. [\APACyear2023] \APACinsertmetastarwang2023self{APACrefauthors}Wang,
    J., Liu, P.\BCBL Wen, F. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleSelf-Supervised
    Learning for RGB-Guided Depth Enhancement by Exploiting the Dependency Between
    RGB and Depth Self-supervised learning for rgb-guided depth enhancement by exploiting
    the dependency between rgb and depth.\BBCQ \APACjournalVolNumPagesIEEE Transactions
    on Image Processing32159-174, {APACrefDOI}  [https://doi.org/10.1109/TIP.2022.3226419](https://doi.org/10.1109/TIP.2022.3226419)
    \PrintBackRefs\CurrentBib
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. Wang \BOthers. [\APACyear2023] \APACinsertmetastarwang2023self{APACrefauthors}Wang,
    J., Liu, P.\BCBL Wen, F. \APACrefYearMonthDay2023. \BBOQ\APACrefatitle通过利用RGB和深度之间的依赖关系进行RGB引导深度增强的自监督学习
    Self-supervised learning for rgb-guided depth enhancement by exploiting the dependency
    between rgb and depth.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Image
    Processing32159-174, {APACrefDOI} [https://doi.org/10.1109/TIP.2022.3226419](https://doi.org/10.1109/TIP.2022.3226419)
    \PrintBackRefs\CurrentBib
- en: K. Wang \BOthers. [\APACyear2023] \APACinsertmetastarwang2023joint{APACrefauthors}Wang,
    K., Zhao, L., Zhang, J., Zhang, J., Wang, A.\BCBL Bai, H. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleJoint depth map super-resolution method via deep hybrid-cross
    guidance filter Joint depth map super-resolution method via deep hybrid-cross
    guidance filter.\BBCQ \APACjournalVolNumPagesPattern Recognition136109260, \PrintBackRefs\CurrentBib
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K. Wang \BOthers. [\APACyear2023] \APACinsertmetastarwang2023joint{APACrefauthors}Wang,
    K., Zhao, L., Zhang, J., Zhang, J., Wang, A.\BCBL Bai, H. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitle通过深度混合交叉引导滤波的联合深度图超分辨率方法 Joint depth map super-resolution method
    via deep hybrid-cross guidance filter.\BBCQ \APACjournalVolNumPagesPattern Recognition136109260,
    \PrintBackRefs\CurrentBib
- en: X. Wang \BOthers. [\APACyear2022] \APACinsertmetastarwang2022learning{APACrefauthors}Wang,
    X., Chen, X., Ni, B., Tong, Z.\BCBL Wang, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleLearning
    Continuous Depth Representation via Geometric Spatial Aggregator Learning continuous
    depth representation via geometric spatial aggregator.\BBCQ \APACjournalVolNumPagesarXiv
    preprint arXiv:2212.03499, \PrintBackRefs\CurrentBib
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X. Wang \BOthers. [\APACyear2022] \APACinsertmetastarwang2022learning{APACrefauthors}Wang,
    X., Chen, X., Ni, B., Tong, Z.\BCBL Wang, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle通过几何空间聚合器学习连续深度表示
    Learning continuous depth representation via geometric spatial aggregator.\BBCQ
    \APACjournalVolNumPagesarXiv preprint arXiv:2212.03499, \PrintBackRefs\CurrentBib
- en: 'Y. Wang \BOthers. [\APACyear2019] \APACinsertmetastarwang2019pseudo{APACrefauthors}Wang,
    Y., Chao, W\BHBIL., Garg, D., Hariharan, B., Campbell, M.\BCBL Weinberger, K.Q.
    \APACrefYearMonthDay2019. \BBOQ\APACrefatitlePseudo-lidar from visual depth estimation:
    Bridging the gap in 3d object detection for autonomous driving Pseudo-lidar from
    visual depth estimation: Bridging the gap in 3d object detection for autonomous
    driving.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Proceedings of the ieee/cvf conference on computer
    vision and pattern recognition (\BPGS8445–8453). \PrintBackRefs\CurrentBib'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Y. Wang \BOthers. [\APACyear2019] \APACinsertmetastarwang2019pseudo{APACrefauthors}Wang,
    Y., Chao, W\BHBIL., Garg, D., Hariharan, B., Campbell, M.\BCBL Weinberger, K.Q.
    \APACrefYearMonthDay2019. \BBOQ\APACrefatitle伪激光雷达从视觉深度估计中提取：弥合自动驾驶3D物体检测中的差距
    Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection
    for autonomous driving.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集 Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (\BPGS8445–8453).
    \PrintBackRefs\CurrentBib'
- en: 'Y. Wang, Li\BCBL \BOthers. [\APACyear2023] \APACinsertmetastarLRRU_ICCV_2023{APACrefauthors}Wang,
    Y., Li, B., Zhang, G., Liu, Q., Tao, G.\BCBL Dai, Y. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleLRRU: Long-short Range Recurrent Updating Networks for Depth
    Completion Lrru: Long-short range recurrent updating networks for depth completion.\BBCQ
    \APACrefbtitleProceedings of the IEEE International Conference on Computer Vision
    (ICCV) Proceedings of the ieee international conference on computer vision (iccv) (\BPG9422-9432).
    \PrintBackRefs\CurrentBib'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Y. Wang, Li\BCBL \BOthers. [\APACyear2023] \APACinsertmetastarLRRU_ICCV_2023{APACrefauthors}Wang,
    Y., Li, B., Zhang, G., Liu, Q., Tao, G.\BCBL Dai, Y. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleLRRU: 长短期范围递归更新网络用于深度补全 Lrru: Long-short range recurrent updating
    networks for depth completion.\BBCQ \APACrefbtitleIEEE国际计算机视觉会议论文集 Proceedings
    of the IEEE International Conference on Computer Vision (ICCV) (\BPG9422-9432).
    \PrintBackRefs\CurrentBib'
- en: Y. Wang, Yang\BCBL \BBA Yue [\APACyear2023] \APACinsertmetastarwang2023depth{APACrefauthors}Wang,
    Y., Yang, J.\BCBL Yue, H. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleDepth map
    continuous super-resolution with local implicit guidance function Depth map continuous
    super-resolution with local implicit guidance function.\BBCQ \APACjournalVolNumPagesDisplays78102418,
    \PrintBackRefs\CurrentBib
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Wang, Yang\BCBL \BBA Yue [\APACyear2023] \APACinsertmetastarwang2023depth{APACrefauthors}Wang,
    Y., Yang, J.\BCBL Yue, H. \APACrefYearMonthDay2023. \BBOQ\APACrefatitle基于局部隐式引导函数的深度图连续超分辨率
    Depth map continuous super-resolution with local implicit guidance function.\BBCQ
    \APACjournalVolNumPagesDisplays78102418, \PrintBackRefs\CurrentBib
- en: Z. Wang \BOthers. [\APACyear2020] \APACinsertmetastarwang2020depth{APACrefauthors}Wang,
    Z., Ye, X., Sun, B., Yang, J., Xu, R.\BCBL Li, H. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDepth
    upsampling based on deep edge-aware learning Depth upsampling based on deep edge-aware
    learning.\BBCQ \APACjournalVolNumPagesPattern Recognition103107274, \PrintBackRefs\CurrentBib
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Z. Wang \BOthers. [\APACyear2020] \APACinsertmetastarwang2020depth{APACrefauthors}Wang,
    Z., Ye, X., Sun, B., Yang, J., Xu, R.\BCBL Li, H. \APACrefYearMonthDay2020. \BBOQ\APACrefatitle基于深度边缘感知学习的深度上采样
    Depth upsampling based on deep edge-aware learning.\BBCQ \APACjournalVolNumPagesPattern
    Recognition103107274, \PrintBackRefs\CurrentBib
- en: Weickert \BOthers. [\APACyear1998] \APACinsertmetastarweickert1998anisotropic{APACrefauthors}Weickert,
    J.\BCBT \BOthersPeriod. \APACrefYear1998. \APACrefbtitleAnisotropic diffusion
    in image processing Anisotropic diffusion in image processing (\BVOL1). \APACaddressPublisherTeubner
    Stuttgart. \PrintBackRefs\CurrentBib
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weickert \BOthers. [\APACyear1998] \APACinsertmetastarweickert1998anisotropic{APACrefauthors}Weickert,
    J.\BCBT \BOthersPeriod. \APACrefYear1998. \APACrefbtitle图像处理中的各向异性扩散 Anisotropic
    diffusion in image processing (\BVOL1). \APACaddressPublisherTeubner Stuttgart.
    \PrintBackRefs\CurrentBib
- en: Wen \BOthers. [\APACyear2019] \APACinsertmetastarwen2018deep{APACrefauthors}Wen,
    Y., Sheng, B., Li, P., Lin, W.\BCBL Feng, D.D. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDeep
    Color Guided Coarse-to-Fine Convolutional Network Cascade for Depth Image Super-Resolution
    Deep color guided coarse-to-fine convolutional network cascade for depth image
    super-resolution.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Image Processing282994-1006,
    {APACrefDOI}  [https://doi.org/10.1109/TIP.2018.2874285](https://doi.org/10.1109/TIP.2018.2874285)
    \PrintBackRefs\CurrentBib
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen \BOthers. [\APACyear2019] \APACinsertmetastarwen2018deep{APACrefauthors}Wen,
    Y., Sheng, B., Li, P., Lin, W.\BCBL Feng, D.D. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle深度颜色引导的粗到细卷积网络级联用于深度图像超分辨率
    Deep color guided coarse-to-fine convolutional network cascade for depth image
    super-resolution.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Image Processing282994-1006,
    {APACrefDOI}  [https://doi.org/10.1109/TIP.2018.2874285](https://doi.org/10.1109/TIP.2018.2874285)
    \PrintBackRefs\CurrentBib
- en: Wong \BOthers. [\APACyear2021] \APACinsertmetastarwong2021learning{APACrefauthors}Wong,
    A., Cicek, S.\BCBL Soatto, S. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleLearning
    topology from synthetic data for unsupervised depth completion Learning topology
    from synthetic data for unsupervised depth completion.\BBCQ \APACjournalVolNumPagesIEEE
    Robotics and Automation Letters621495–1502, \PrintBackRefs\CurrentBib
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong \BOthers. [\APACyear2021] \APACinsertmetastarwong2021learning{APACrefauthors}Wong,
    A., Cicek, S.\BCBL Soatto, S. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle从合成数据中学习拓扑以实现无监督深度补全
    Learning topology from synthetic data for unsupervised depth completion.\BBCQ
    \APACjournalVolNumPagesIEEE Robotics and Automation Letters621495–1502, \PrintBackRefs\CurrentBib
- en: Wong \BOthers. [\APACyear2020] \APACinsertmetastarwong2020unsupervised{APACrefauthors}Wong,
    A., Fei, X., Tsuei, S.\BCBL Soatto, S. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleUnsupervised
    depth completion from visual inertial odometry Unsupervised depth completion from
    visual inertial odometry.\BBCQ \APACjournalVolNumPagesIEEE Robotics and Automation
    Letters521899–1906, \PrintBackRefs\CurrentBib
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong \BOthers. [\APACyear2020] \APACinsertmetastarwong2020unsupervised{APACrefauthors}Wong,
    A., Fei, X., Tsuei, S.\BCBL Soatto, S. \APACrefYearMonthDay2020. \BBOQ\APACrefatitle无监督深度补全通过视觉惯性测距
    Unsupervised depth completion from visual inertial odometry.\BBCQ \APACjournalVolNumPagesIEEE
    Robotics and Automation Letters521899–1906, \PrintBackRefs\CurrentBib
- en: Wong \BBA Soatto [\APACyear2021] \APACinsertmetastarwong2021unsupervised{APACrefauthors}Wong,
    A.\BCBT \BBA Soatto, S. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleUnsupervised
    depth completion with calibrated backprojection layers Unsupervised depth completion
    with calibrated backprojection layers.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF
    International Conference on Computer Vision Proceedings of the ieee/cvf international
    conference on computer vision (\BPGS12747–12756). \PrintBackRefs\CurrentBib
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong \BBA Soatto [\APACyear2021] \APACinsertmetastarwong2021unsupervised{APACrefauthors}Wong,
    A.\BCBT \BBA Soatto, S. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle带有标定反投影层的无监督深度补全
    Unsupervised depth completion with calibrated backprojection layers.\BBCQ \APACrefbtitleIEEE/CVF
    国际计算机视觉大会论文集 Proceedings of the IEEE/CVF International Conference on Computer
    Vision (\BPGS12747–12756). \PrintBackRefs\CurrentBib
- en: Wronski \BOthers. [\APACyear2019] \APACinsertmetastarwronski2019handheld{APACrefauthors}Wronski,
    B., Garcia-Dorado, I., Ernst, M., Kelly, D., Krainin, M., Liang, C\BHBIK.\BDBLMilanfar,
    P. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleHandheld multi-frame super-resolution
    Handheld multi-frame super-resolution.\BBCQ \APACjournalVolNumPagesACM Transactions
    on Graphics (ToG)3841–18, \PrintBackRefs\CurrentBib
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wronski \BOthers. [\APACyear2019] \APACinsertmetastarwronski2019handheld{APACrefauthors}Wronski,
    B., Garcia-Dorado, I., Ernst, M., Kelly, D., Krainin, M., Liang, C\BHBIK.\BDBLMilanfar,
    P. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle手持多帧超分辨率 Handheld multi-frame
    super-resolution.\BBCQ \APACjournalVolNumPagesACM Transactions on Graphics (ToG)3841–18,
    \PrintBackRefs\CurrentBib
- en: 'Wu \BOthers. [\APACyear2022] \APACinsertmetastarwu2022sparse{APACrefauthors}Wu,
    X., Peng, L., Yang, H., Xie, L., Huang, C., Deng, C.\BDBLCai, D. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleSparse fuse dense: Towards high quality 3d detection with depth
    completion Sparse fuse dense: Towards high quality 3d detection with depth completion.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS5418–5427). \PrintBackRefs\CurrentBib'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu \BOthers. [\APACyear2022] \APACinsertmetastarwu2022sparse{APACrefauthors}Wu,
    X., Peng, L., Yang, H., Xie, L., Huang, C., Deng, C.\BDBLCai, D. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitle稀疏融合密集：迈向高质量3D检测与深度补全 Sparse fuse dense: Towards high quality
    3d detection with depth completion.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集
    Proceedings of the ieee/cvf conference on computer vision and pattern recognition
    (\BPGS5418–5427). \PrintBackRefs\CurrentBib'
- en: 'Xia \BOthers. [\APACyear2018] \APACinsertmetastarxia2018gibson{APACrefauthors}Xia,
    F., Zamir, A.R., He, Z., Sax, A., Malik, J.\BCBL Savarese, S. \APACrefYearMonthDay2018.
    \BBOQ\APACrefatitleGibson env: Real-world perception for embodied agents Gibson
    env: Real-world perception for embodied agents.\BBCQ \APACrefbtitleProceedings
    of the IEEE conference on computer vision and pattern recognition Proceedings
    of the ieee conference on computer vision and pattern recognition (\BPGS9068–9079).
    \PrintBackRefs\CurrentBib'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia \BOthers. [\APACyear2018] \APACinsertmetastarxia2018gibson{APACrefauthors}Xia,
    F., Zamir, A.R., He, Z., Sax, A., Malik, J.\BCBL Savarese, S. \APACrefYearMonthDay2018.
    \BBOQ\APACrefatitleGibson环境：具身体代理的现实感知 Gibson env: Real-world perception for embodied
    agents.\BBCQ \APACrefbtitleIEEE计算机视觉与模式识别会议论文集 Proceedings of the ieee conference
    on computer vision and pattern recognition (\BPGS9068–9079). \PrintBackRefs\CurrentBib'
- en: 'Xie \BOthers. [\APACyear2022] \APACinsertmetastarsurvey_completion_1{APACrefauthors}Xie,
    Z., Yu, X., Gao, X., Li, K.\BCBL Shen, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleRecent
    Advances in Conventional and Deep Learning-Based Depth Completion: A Survey Recent
    advances in conventional and deep learning-based depth completion: A survey.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Neural Networks and Learning Systems1-21,
    {APACrefDOI}  [https://doi.org/10.1109/TNNLS.2022.3201534](https://doi.org/10.1109/TNNLS.2022.3201534)
    \PrintBackRefs\CurrentBib'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie \BOthers. [\APACyear2022] \APACinsertmetastarsurvey_completion_1{APACrefauthors}Xie,
    Z., Yu, X., Gao, X., Li, K.\BCBL Shen, S. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle最近在传统和深度学习基础的深度补全方面的进展：一项调查
    Recent advances in conventional and deep learning-based depth completion: A survey.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Neural Networks and Learning Systems1-21,
    {APACrefDOI} [https://doi.org/10.1109/TNNLS.2022.3201534](https://doi.org/10.1109/TNNLS.2022.3201534)
    \PrintBackRefs\CurrentBib'
- en: J. Xu \BOthers. [\APACyear2023] \APACinsertmetastarxu2023real{APACrefauthors}Xu,
    J., Zhu, Y., Wang, W.\BCBL Liu, G. \APACrefYearMonthDay2023. \BBOQ\APACrefatitleA
    real-time semi-dense depth-guided depth completion network A real-time semi-dense
    depth-guided depth completion network.\BBCQ \APACjournalVolNumPagesThe Visual
    Computer1–11, \PrintBackRefs\CurrentBib
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. Xu \BOthers. [\APACyear2023] \APACinsertmetastarxu2023real{APACrefauthors}Xu,
    J., Zhu, Y., Wang, W.\BCBL Liu, G. \APACrefYearMonthDay2023. \BBOQ\APACrefatitle实时半稠密深度引导的深度补全网络
    A real-time semi-dense depth-guided depth completion network.\BBCQ \APACjournalVolNumPagesThe
    Visual Computer1–11, \PrintBackRefs\CurrentBib
- en: L. Xu \BOthers. [\APACyear2022] \APACinsertmetastarxu2022self{APACrefauthors}Xu,
    L., Guan, T., Wang, Y., Luo, Y., Chen, Z., Liu, W.\BCBL Yang, W. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleSelf-Supervised Multi-view Stereo via Adjacent Geometry Guided
    Volume Completion Self-supervised multi-view stereo via adjacent geometry guided
    volume completion.\BBCQ \APACrefbtitleProceedings of the 30th ACM International
    Conference on Multimedia Proceedings of the 30th acm international conference
    on multimedia (\BPGS2202–2210). \PrintBackRefs\CurrentBib
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L. Xu \BOthers. [\APACyear2022] \APACinsertmetastarxu2022self{APACrefauthors}Xu,
    L., Guan, T., Wang, Y., Luo, Y., Chen, Z., Liu, W.\BCBL Yang, W. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitle自监督多视角立体匹配通过相邻几何引导的体积补全 Self-supervised multi-view stereo via
    adjacent geometry guided volume completion.\BBCQ \APACrefbtitle第30届ACM国际多媒体会议论文集
    Proceedings of the 30th acm international conference on multimedia (\BPGS2202–2210).
    \PrintBackRefs\CurrentBib
- en: Y. Xu \BOthers. [\APACyear2019] \APACinsertmetastarxu2019depth{APACrefauthors}Xu,
    Y., Zhu, X., Shi, J., Zhang, G., Bao, H.\BCBL Li, H. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleDepth completion from sparse lidar data with depth-normal constraints
    Depth completion from sparse lidar data with depth-normal constraints.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF International Conference on Computer Vision Proceedings of the
    ieee/cvf international conference on computer vision (\BPGS2811–2820). \PrintBackRefs\CurrentBib
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Xu \BOthers. [\APACyear2019] \APACinsertmetastarxu2019depth{APACrefauthors}Xu,
    Y., Zhu, X., Shi, J., Zhang, G., Bao, H.\BCBL Li, H. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleDepth completion from sparse lidar data with depth-normal constraints
    Depth completion from sparse lidar data with depth-normal constraints.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF International Conference on Computer Vision Proceedings of the
    ieee/cvf international conference on computer vision (\BPGS2811–2820). \PrintBackRefs\CurrentBib
- en: Z. Xu \BOthers. [\APACyear2020] \APACinsertmetastarxu2020deformable{APACrefauthors}Xu,
    Z., Yin, H.\BCBL Yao, J. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDeformable
    spatial propagation networks for depth completion Deformable spatial propagation
    networks for depth completion.\BBCQ \APACrefbtitle2020 IEEE International Conference
    on Image Processing (ICIP) 2020 ieee international conference on image processing
    (icip) (\BPGS913–917). \PrintBackRefs\CurrentBib
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Z. Xu \BOthers. [\APACyear2020] \APACinsertmetastarxu2020deformable{APACrefauthors}Xu,
    Z., Yin, H.\BCBL Yao, J. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleDeformable
    spatial propagation networks for depth completion Deformable spatial propagation
    networks for depth completion.\BBCQ \APACrefbtitle2020 IEEE International Conference
    on Image Processing (ICIP) 2020 ieee international conference on image processing
    (icip) (\BPGS913–917). \PrintBackRefs\CurrentBib
- en: 'L. Yan \BOthers. [\APACyear2020] \APACinsertmetastaryan2020revisiting{APACrefauthors}Yan,
    L., Liu, K.\BCBL Belyaev, E. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleRevisiting
    sparsity invariant convolution: A network for image guided depth completion Revisiting
    sparsity invariant convolution: A network for image guided depth completion.\BBCQ
    \APACjournalVolNumPagesIEEE Access8126323–126332, \PrintBackRefs\CurrentBib'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L. Yan \BOthers. [\APACyear2020] \APACinsertmetastaryan2020revisiting{APACrefauthors}Yan,
    L., Liu, K.\BCBL Belyaev, E. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleRevisiting
    sparsity invariant convolution: A network for image guided depth completion Revisiting
    sparsity invariant convolution: A network for image guided depth completion.\BBCQ
    \APACjournalVolNumPagesIEEE Access8126323–126332, \PrintBackRefs\CurrentBib'
- en: 'Z. Yan \BOthers. [\APACyear2022] \APACinsertmetastaryan2022rignet{APACrefauthors}Yan,
    Z., Wang, K., Li, X., Zhang, Z., Li, J.\BCBL Yang, J. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleRigNet: Repetitive image guided network for depth completion
    Rignet: Repetitive image guided network for depth completion.\BBCQ \APACrefbtitleComputer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part XXVII Computer vision–eccv 2022: 17th european conference, tel
    aviv, israel, october 23–27, 2022, proceedings, part xxvii (\BPGS214–230). \PrintBackRefs\CurrentBib'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Z. Yan \BOthers. [\APACyear2022] \APACinsertmetastaryan2022rignet{APACrefauthors}Yan,
    Z., Wang, K., Li, X., Zhang, Z., Li, J.\BCBL Yang, J. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleRigNet: Repetitive image guided network for depth completion
    Rignet: Repetitive image guided network for depth completion.\BBCQ \APACrefbtitleComputer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part XXVII Computer vision–eccv 2022: 17th european conference, tel
    aviv, israel, october 23–27, 2022, proceedings, part xxvii (\BPGS214–230). \PrintBackRefs\CurrentBib'
- en: A. Yang \BBA Sankaranarayanan [\APACyear2021] \APACinsertmetastaryang2021designing{APACrefauthors}Yang,
    A.\BCBT \BBA Sankaranarayanan, A.C. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleDesigning
    display pixel layouts for under-panel cameras Designing display pixel layouts
    for under-panel cameras.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Pattern
    Analysis and Machine Intelligence4372245–2256, \PrintBackRefs\CurrentBib
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A. Yang \BBA Sankaranarayanan [\APACyear2021] \APACinsertmetastaryang2021designing{APACrefauthors}Yang,
    A.\BCBT \BBA Sankaranarayanan, A.C. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleDesigning
    display pixel layouts for under-panel cameras Designing display pixel layouts
    for under-panel cameras.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Pattern
    Analysis and Machine Intelligence4372245–2256, \PrintBackRefs\CurrentBib
- en: Y. Yang \BOthers. [\APACyear2019] \APACinsertmetastaryang2019dense{APACrefauthors}Yang,
    Y., Wong, A.\BCBL Soatto, S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDense
    depth posterior (ddp) from single image and sparse range Dense depth posterior
    (ddp) from single image and sparse range.\BBCQ \APACrefbtitleProceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings of
    the ieee/cvf conference on computer vision and pattern recognition (\BPGS3353–3362).
    \PrintBackRefs\CurrentBib
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Yang \BOthers. [\APACyear2019] \APACinsertmetastaryang2019dense{APACrefauthors}Yang,
    Y., Wong, A.\BCBL Soatto, S. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDense
    depth posterior (ddp) from single image and sparse range Dense depth posterior
    (ddp) from single image and sparse range.\BBCQ \APACrefbtitleProceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings of
    the ieee/cvf conference on computer vision and pattern recognition (\BPGS3353–3362).
    \PrintBackRefs\CurrentBib
- en: 'Ye \BOthers. [\APACyear2020] \APACinsertmetastarye2020pmbanet{APACrefauthors}Ye,
    X., Sun, B., Wang, Z., Yang, J., Xu, R., Li, H.\BCBL Li, B. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitlePMBANet: Progressive multi-branch aggregation network for scene
    depth super-resolution Pmbanet: Progressive multi-branch aggregation network for
    scene depth super-resolution.\BBCQ \APACjournalVolNumPagesIEEE Transactions on
    Image Processing297427–7442, \PrintBackRefs\CurrentBib'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye \BOthers. [\APACyear2020] \APACinsertmetastarye2020pmbanet{APACrefauthors}Ye,
    X., Sun, B., Wang, Z., Yang, J., Xu, R., Li, H.\BCBL Li, B. \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitlePMBANet: Progressive multi-branch aggregation network for scene
    depth super-resolution Pmbanet: Progressive multi-branch aggregation network for
    scene depth super-resolution.\BBCQ \APACjournalVolNumPagesIEEE Transactions on
    Image Processing297427–7442, \PrintBackRefs\CurrentBib'
- en: Q. Yu \BOthers. [\APACyear2021] \APACinsertmetastaryu2021grayscale{APACrefauthors}Yu,
    Q., Chu, L., Wu, Q.\BCBL Pei, L. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleGrayscale
    and normal guided depth completion with a low-cost lidar Grayscale and normal
    guided depth completion with a low-cost lidar.\BBCQ \APACrefbtitle2021 IEEE International
    Conference on Image Processing (ICIP) 2021 ieee international conference on image
    processing (icip) (\BPGS979–983). \PrintBackRefs\CurrentBib
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q. Yu \BOthers. [\APACyear2021] \APACinsertmetastaryu2021grayscale{APACrefauthors}Yu,
    Q., Chu, L., Wu, Q.\BCBL Pei, L. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleGrayscale
    and normal guided depth completion with a low-cost lidar Grayscale and normal
    guided depth completion with a low-cost lidar.\BBCQ \APACrefbtitle2021 IEEE International
    Conference on Image Processing (ICIP) 2021 ieee international conference on image
    processing (icip) (\BPGS979–983). \PrintBackRefs\CurrentBib
- en: Z. Yu \BOthers. [\APACyear2023] \APACinsertmetastaryu2023aggregating{APACrefauthors}Yu,
    Z., Sheng, Z., Zhou, Z., Luo, L., Cao, S\BHBIY., Gu, H.\BDBLShen, H\BHBIL. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleAggregating Feature Point Cloud for Depth Completion Aggregating
    feature point cloud for depth completion.\BBCQ \APACrefbtitleProceedings of the
    IEEE/CVF International Conference on Computer Vision Proceedings of the ieee/cvf
    international conference on computer vision (\BPGS8732–8743). \PrintBackRefs\CurrentBib
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Z. Yu \BOthers. [\APACyear2023] \APACinsertmetastaryu2023aggregating{APACrefauthors}Yu,
    Z., Sheng, Z., Zhou, Z., Luo, L., Cao, S\BHBIY., Gu, H.\BDBLShen, H\BHBIL. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleAggregating Feature Point Cloud for Depth Completion Aggregating
    feature point cloud for depth completion.\BBCQ \APACrefbtitleProceedings of the
    IEEE/CVF International Conference on Computer Vision Proceedings of the ieee/cvf
    international conference on computer vision (\BPGS8732–8743). \PrintBackRefs\CurrentBib
- en: Yuan \BOthers. [\APACyear2023\APACexlab\BCnt1] \APACinsertmetastaryuan2023recurrent{APACrefauthors}Yuan,
    J., Jiang, H., Li, X., Qian, J., Li, J.\BCBL Yang, J. \APACrefYearMonthDay2023\BCnt1.
    \BBOQ\APACrefatitleRecurrent Structure Attention Guidance for Depth Super-Resolution
    Recurrent structure attention guidance for depth super-resolution.\BBCQ \APACjournalVolNumPagesarXiv
    preprint arXiv:2301.13419, \PrintBackRefs\CurrentBib
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan \BOthers. [\APACyear2023\APACexlab\BCnt1] \APACinsertmetastaryuan2023recurrent{APACrefauthors}Yuan,
    J., Jiang, H., Li, X., Qian, J., Li, J.\BCBL Yang, J. \APACrefYearMonthDay2023\BCnt1.
    \BBOQ\APACrefatitleRecurrent Structure Attention Guidance for Depth Super-Resolution
    Recurrent structure attention guidance for depth super-resolution.\BBCQ \APACjournalVolNumPagesarXiv
    preprint arXiv:2301.13419, \PrintBackRefs\CurrentBib
- en: Yuan \BOthers. [\APACyear2023\APACexlab\BCnt2] \APACinsertmetastaryuan2023structure{APACrefauthors}Yuan,
    J., Jiang, H., Li, X., Qian, J., Li, J.\BCBL Yang, J. \APACrefYearMonthDay2023\BCnt2.
    \BBOQ\APACrefatitleStructure Flow-Guided Network for Real Depth Super-Resolution
    Structure flow-guided network for real depth super-resolution.\BBCQ \APACjournalVolNumPagesarXiv
    preprint arXiv:2301.13416, \PrintBackRefs\CurrentBib
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan \BOthers. [\APACyear2023\APACexlab\BCnt2] \APACinsertmetastaryuan2023structure{APACrefauthors}Yuan,
    J., Jiang, H., Li, X., Qian, J., Li, J.\BCBL Yang, J. \APACrefYearMonthDay2023\BCnt2.
    \BBOQ\APACrefatitleStructure Flow-Guided Network for Real Depth Super-Resolution
    Structure flow-guided network for real depth super-resolution.\BBCQ \APACjournalVolNumPagesarXiv
    preprint arXiv:2301.13416, \PrintBackRefs\CurrentBib
- en: C. Zhang \BOthers. [\APACyear2021] \APACinsertmetastarzhang2021multitask{APACrefauthors}Zhang,
    C., Tang, Y., Zhao, C., Sun, Q., Ye, Z.\BCBL Kurths, J. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleMultitask gans for semantic segmentation and depth completion
    with cycle consistency Multitask gans for semantic segmentation and depth completion
    with cycle consistency.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Neural
    Networks and Learning Systems32125404–5415, \PrintBackRefs\CurrentBib
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C. Zhang \BOthers. [\APACyear2021] \APACinsertmetastarzhang2021multitask{APACrefauthors}Zhang,
    C., Tang, Y., Zhao, C., Sun, Q., Ye, Z.\BCBL Kurths, J. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleMultitask gans for semantic segmentation and depth completion
    with cycle consistency Multitask gans for semantic segmentation and depth completion
    with cycle consistency.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Neural
    Networks and Learning Systems32125404–5415, \PrintBackRefs\CurrentBib
- en: Q. Zhang \BOthers. [\APACyear2022] \APACinsertmetastarzhang2022self{APACrefauthors}Zhang,
    Q., Chen, X., Wang, X., Han, J., Zhang, Y.\BCBL Yue, J. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitleSelf-Supervised Depth Completion Based on Multi-Modal Spatio-Temporal
    Consistency Self-supervised depth completion based on multi-modal spatio-temporal
    consistency.\BBCQ \APACjournalVolNumPagesRemote Sensing151135, \PrintBackRefs\CurrentBib
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q. Zhang \BOthers. [\APACyear2022] \APACinsertmetastarzhang2022self{APACrefauthors}Zhang,
    Q., Chen, X., Wang, X., Han, J., Zhang, Y.\BCBL Yue, J. \APACrefYearMonthDay2022.
    \BBOQ\APACrefatitle基于多模态时空一致性的自监督深度完成 Self-supervised depth completion based on
    multi-modal spatio-temporal consistency.\BBCQ \APACjournalVolNumPagesRemote Sensing151135,
    \PrintBackRefs\CurrentBib
- en: Y. Zhang \BBA Funkhouser [\APACyear2018] \APACinsertmetastarzhang2018deep{APACrefauthors}Zhang,
    Y.\BCBT \BBA Funkhouser, T. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleDeep
    depth completion of a single rgb-d image Deep depth completion of a single rgb-d
    image.\BBCQ \APACrefbtitleProceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition Proceedings of the ieee conference on computer vision
    and pattern recognition (\BPGS175–185). \PrintBackRefs\CurrentBib
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Zhang \BBA Funkhouser [\APACyear2018] \APACinsertmetastarzhang2018deep{APACrefauthors}Zhang,
    Y.\BCBT \BBA Funkhouser, T. \APACrefYearMonthDay2018. \BBOQ\APACrefatitle单幅 RGB-D
    图像的深度完成 Single RGB-D image depth completion.\BBCQ \APACrefbtitleIEEE 计算机视觉与模式识别会议论文集
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (\BPGS175–185). \PrintBackRefs\CurrentBib
- en: 'Y. Zhang \BOthers. [\APACyear2023] \APACinsertmetastarZhang2023CompletionFormer{APACrefauthors}Zhang,
    Y., Guo, X., Poggi, M., Zhu, Z., Huang, G.\BCBL Mattoccia, S. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleCompletionFormer: Depth Completion with Convolutions and Vision
    Transformers Completionformer: Depth completion with convolutions and vision transformers.\BBCQ
    \APACrefbtitleCVPR. Cvpr. \PrintBackRefs\CurrentBib'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Y. Zhang \BOthers. [\APACyear2023] \APACinsertmetastarZhang2023CompletionFormer{APACrefauthors}Zhang,
    Y., Guo, X., Poggi, M., Zhu, Z., Huang, G.\BCBL Mattoccia, S. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleCompletionFormer：卷积和视觉变换器进行深度完成 CompletionFormer: Depth completion
    with convolutions and vision transformers.\BBCQ \APACrefbtitleCVPR. Cvpr. \PrintBackRefs\CurrentBib'
- en: Y. Zhang \BBA Yang [\APACyear2021] \APACinsertmetastarzhang2021survey{APACrefauthors}Zhang,
    Y.\BCBT \BBA Yang, Q. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleA survey on
    multi-task learning A survey on multi-task learning.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Knowledge and Data Engineering34125586–5609, \PrintBackRefs\CurrentBib
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Zhang \BBA Yang [\APACyear2021] \APACinsertmetastarzhang2021survey{APACrefauthors}Zhang,
    Y.\BCBT \BBA Yang, Q. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle关于多任务学习的综述
    A survey on multi-task learning.\BBCQ \APACjournalVolNumPagesIEEE Transactions
    on Knowledge and Data Engineering34125586–5609, \PrintBackRefs\CurrentBib
- en: C. Zhao \BOthers. [\APACyear2020] \APACinsertmetastarzhao2020masked{APACrefauthors}Zhao,
    C., Yen, G.G., Sun, Q., Zhang, C.\BCBL Tang, Y. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleMasked
    GAN for unsupervised depth and pose prediction with scale consistency Masked gan
    for unsupervised depth and pose prediction with scale consistency.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Neural Networks and Learning Systems32125392–5403, \PrintBackRefs\CurrentBib
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C. Zhao \BOthers. [\APACyear2020] \APACinsertmetastarzhao2020masked{APACrefauthors}Zhao,
    C., Yen, G.G., Sun, Q., Zhang, C.\BCBL Tang, Y. \APACrefYearMonthDay2020. \BBOQ\APACrefatitle带尺度一致性的无监督深度和姿态预测的
    Masked GAN Masked GAN for unsupervised depth and pose prediction with scale consistency.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Neural Networks and Learning Systems32125392–5403,
    \PrintBackRefs\CurrentBib
- en: S. Zhao \BOthers. [\APACyear2021] \APACinsertmetastarzhao2021adaptive{APACrefauthors}Zhao,
    S., Gong, M., Fu, H.\BCBL Tao, D. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleAdaptive
    context-aware multi-modal network for depth completion Adaptive context-aware
    multi-modal network for depth completion.\BBCQ \APACjournalVolNumPagesIEEE Transactions
    on Image Processing305264–5276, \PrintBackRefs\CurrentBib
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S. Zhao \BOthers. [\APACyear2021] \APACinsertmetastarzhao2021adaptive{APACrefauthors}Zhao,
    S., Gong, M., Fu, H.\BCBL Tao, D. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle自适应上下文感知多模态网络用于深度完成
    Adaptive context-aware multi-modal network for depth completion.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Image Processing305264–5276, \PrintBackRefs\CurrentBib
- en: Z. Zhao \BOthers. [\APACyear2023] \APACinsertmetastarzhao2023spherical{APACrefauthors}Zhao,
    Z., Zhang, J., Gu, X., Tan, C., Xu, S., Zhang, Y.\BDBLVan Gool, L. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleSpherical space feature decomposition for guided depth map
    super-resolution Spherical space feature decomposition for guided depth map super-resolution.\BBCQ
    \APACjournalVolNumPagesProceedings of the IEEE International Conference on Computer
    Vision (ICCV)12547-12558, \PrintBackRefs\CurrentBib
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Z. Zhao \BOthers. [\APACyear2023] \APACinsertmetastarzhao2023spherical{APACrefauthors}赵志，张杰，顾旭，谭辰，徐顺，张颖。\BDBLVan
    Gool, L. \APACrefYearMonthDay2023. \BBOQ\APACrefatitle引导深度图超分辨率的球面空间特征分解 Spherical
    space feature decomposition for guided depth map super-resolution.\BBCQ \APACjournalVolNumPagesIEEE国际计算机视觉会议论文集
    Proceedings of the IEEE International Conference on Computer Vision (ICCV) 12547-12558,
    \PrintBackRefs\CurrentBib
- en: Z. Zhao \BOthers. [\APACyear2022] \APACinsertmetastarzhao2022discrete{APACrefauthors}Zhao,
    Z., Zhang, J., Xu, S., Lin, Z.\BCBL Pfister, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleDiscrete
    cosine transform network for guided depth map super-resolution Discrete cosine
    transform network for guided depth map super-resolution.\BBCQ \APACrefbtitleProceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Proceedings
    of the ieee/cvf conference on computer vision and pattern recognition (\BPGS5697–5707).
    \PrintBackRefs\CurrentBib
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Z. Zhao \BOthers. [\APACyear2022] \APACinsertmetastarzhao2022discrete{APACrefauthors}赵志，张杰，徐顺，林正。\BCBL
    Pfister, H. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle引导深度图超分辨率的离散余弦变换网络 Discrete
    cosine transform network for guided depth map super-resolution.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集
    Proceedings of the ieee/cvf conference on computer vision and pattern recognition
    (\BPGS5697–5707). \PrintBackRefs\CurrentBib
- en: Zhong \BOthers. [\APACyear2021] \APACinsertmetastarzhong2021high{APACrefauthors}Zhong,
    Z., Liu, X., Jiang, J., Zhao, D., Chen, Z.\BCBL Ji, X. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleHigh-resolution depth maps imaging via attention-based hierarchical
    multi-modal fusion High-resolution depth maps imaging via attention-based hierarchical
    multi-modal fusion.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Image Processing31648–663,
    \PrintBackRefs\CurrentBib
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong \BOthers. [\APACyear2021] \APACinsertmetastarzhong2021high{APACrefauthors}钟志，刘晓，姜静，赵东，陈哲。\BCBL
    Ji, X. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle基于注意力的层次化多模态融合高分辨率深度图像 High-resolution
    depth maps imaging via attention-based hierarchical multi-modal fusion.\BBCQ \APACjournalVolNumPagesIEEE图像处理汇刊
    IEEE Transactions on Image Processing 31648–663, \PrintBackRefs\CurrentBib
- en: Zhong \BOthers. [\APACyear2023\APACexlab\BCnt1] \APACinsertmetastarzhong2023deep{APACrefauthors}Zhong,
    Z., Liu, X., Jiang, J., Zhao, D.\BCBL Ji, X. \APACrefYearMonthDay2023\BCnt1. \BBOQ\APACrefatitleDeep
    attentional guided image filtering Deep attentional guided image filtering.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Neural Networks and Learning Systems,
    \PrintBackRefs\CurrentBib
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong \BOthers. [\APACyear2023\APACexlab\BCnt1] \APACinsertmetastarzhong2023deep{APACrefauthors}钟志，刘晓，姜静，赵东。\BCBL
    Ji, X. \APACrefYearMonthDay2023\BCnt1. \BBOQ\APACrefatitle深度注意力引导图像滤波 Deep attentional
    guided image filtering.\BBCQ \APACjournalVolNumPagesIEEE神经网络与学习系统汇刊 IEEE Transactions
    on Neural Networks and Learning Systems, \PrintBackRefs\CurrentBib
- en: 'Zhong \BOthers. [\APACyear2023\APACexlab\BCnt2] \APACinsertmetastarzhong2023guided{APACrefauthors}Zhong,
    Z., Liu, X., Jiang, J., Zhao, D.\BCBL Ji, X. \APACrefYearMonthDay2023\BCnt2. \BBOQ\APACrefatitleGuided
    Depth Map Super-resolution: A Survey Guided depth map super-resolution: A survey.\BBCQ
    \APACjournalVolNumPagesACM Computing Surveys, \PrintBackRefs\CurrentBib'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong \BOthers. [\APACyear2023\APACexlab\BCnt2] \APACinsertmetastarzhong2023guided{APACrefauthors}钟志，刘晓，姜静，赵东。\BCBL
    Ji, X. \APACrefYearMonthDay2023\BCnt2. \BBOQ\APACrefatitle引导深度图超分辨率综述 Guided depth
    map super-resolution: A survey.\BBCQ \APACjournalVolNumPagesACM计算机调查 ACM Computing
    Surveys, \PrintBackRefs\CurrentBib'
- en: M. Zhou \BOthers. [\APACyear2023] \APACinsertmetastarzhou2023memory{APACrefauthors}Zhou,
    M., Yan, K., Pan, J., Ren, W., Xie, Q.\BCBL Cao, X. \APACrefYearMonthDay2023.
    \BBOQ\APACrefatitleMemory-augmented deep unfolding network for guided image super-resolution
    Memory-augmented deep unfolding network for guided image super-resolution.\BBCQ
    \APACjournalVolNumPagesInternational Journal of Computer Vision1311215–242, \PrintBackRefs\CurrentBib
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M. Zhou \BOthers. [\APACyear2023] \APACinsertmetastarzhou2023memory{APACrefauthors}周明，颜凯，潘佳，任伟，谢强。\BCBL
    Cao, X. \APACrefYearMonthDay2023. \BBOQ\APACrefatitle记忆增强深度展开网络用于引导图像超分辨率 Memory-augmented
    deep unfolding network for guided image super-resolution.\BBCQ \APACjournalVolNumPages国际计算机视觉期刊
    International Journal of Computer Vision 1311215–242, \PrintBackRefs\CurrentBib
- en: T. Zhou \BOthers. [\APACyear2017] \APACinsertmetastarzhou2017unsupervised{APACrefauthors}Zhou,
    T., Brown, M., Snavely, N.\BCBL Lowe, D.G. \APACrefYearMonthDay2017. \BBOQ\APACrefatitleUnsupervised
    learning of depth and ego-motion from video Unsupervised learning of depth and
    ego-motion from video.\BBCQ \APACrefbtitleProceedings of the IEEE conference on
    computer vision and pattern recognition Proceedings of the ieee conference on
    computer vision and pattern recognition (\BPGS1851–1858). \PrintBackRefs\CurrentBib
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T. Zhou \BOthers. [\APACyear2017] \APACinsertmetastarzhou2017unsupervised{APACrefauthors}Zhou,
    T., Brown, M., Snavely, N.\BCBL Lowe, D.G. \APACrefYearMonthDay2017. \BBOQ\APACrefatitle从视频中无监督学习深度和自我运动
    Unsupervised learning of depth and ego-motion from video.\BBCQ \APACrefbtitleIEEE计算机视觉与模式识别会议论文集
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (\BPGS1851–1858). \PrintBackRefs\CurrentBib
- en: 'Y. Zhou \BOthers. [\APACyear2020] \APACinsertmetastarzhou2020udc{APACrefauthors}Zhou,
    Y., Kwan, M., Tolentino, K., Emerton, N., Lim, S., Large, T.\BDBLothers \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleUDC 2020 challenge on image restoration of under-display camera:
    Methods and results Udc 2020 challenge on image restoration of under-display camera:
    Methods and results.\BBCQ \APACrefbtitleComputer Vision–ECCV 2020 Workshops: Glasgow,
    UK, August 23–28, 2020, Proceedings, Part V 16 Computer vision–eccv 2020 workshops:
    Glasgow, uk, august 23–28, 2020, proceedings, part v 16 (\BPGS337–351). \PrintBackRefs\CurrentBib'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Y. Zhou \BOthers. [\APACyear2020] \APACinsertmetastarzhou2020udc{APACrefauthors}Zhou,
    Y., Kwan, M., Tolentino, K., Emerton, N., Lim, S., Large, T.\BDBLothers \APACrefYearMonthDay2020.
    \BBOQ\APACrefatitleUDC 2020挑战：显示下摄像头的图像修复：方法与结果 UDC 2020 challenge on image restoration
    of under-display camera: Methods and results.\BBCQ \APACrefbtitle计算机视觉–ECCV 2020研讨会论文集：英国格拉斯哥，2020年8月23–28日，会议记录，第五部分
    Computer Vision–ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings,
    Part V 16 (\BPGS337–351). \PrintBackRefs\CurrentBib'
- en: Y. Zhou \BOthers. [\APACyear2021] \APACinsertmetastarzhou2021image{APACrefauthors}Zhou,
    Y., Ren, D., Emerton, N., Lim, S.\BCBL Large, T. \APACrefYearMonthDay2021. \BBOQ\APACrefatitleImage
    restoration for under-display camera Image restoration for under-display camera.\BBCQ
    \APACrefbtitleProceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Proceedings of the ieee/cvf conference on computer vision and pattern
    recognition (\BPGS9179–9188). \PrintBackRefs\CurrentBib
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Zhou \BOthers. [\APACyear2021] \APACinsertmetastarzhou2021image{APACrefauthors}Zhou,
    Y., Ren, D., Emerton, N., Lim, S.\BCBL Large, T. \APACrefYearMonthDay2021. \BBOQ\APACrefatitle用于显示下摄像头的图像修复
    Image restoration for under-display camera.\BBCQ \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (\BPGS9179–9188). \PrintBackRefs\CurrentBib
- en: 'X. Zhu \BOthers. [\APACyear2019] \APACinsertmetastarzhu2019deformable{APACrefauthors}Zhu,
    X., Hu, H., Lin, S.\BCBL Dai, J. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDeformable
    convnets v2: More deformable, better results Deformable convnets v2: More deformable,
    better results.\BBCQ \APACrefbtitleProceedings of the IEEE/CVF conference on computer
    vision and pattern recognition Proceedings of the ieee/cvf conference on computer
    vision and pattern recognition (\BPGS9308–9316). \PrintBackRefs\CurrentBib'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'X. Zhu \BOthers. [\APACyear2019] \APACinsertmetastarzhu2019deformable{APACrefauthors}Zhu,
    X., Hu, H., Lin, S.\BCBL Dai, J. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDeformable
    convnets v2: 更加可变形，更好的结果 Deformable convnets v2: More deformable, better results.\BBCQ
    \APACrefbtitleIEEE/CVF计算机视觉与模式识别会议论文集 Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (\BPGS9308–9316). \PrintBackRefs\CurrentBib'
- en: Y. Zhu \BOthers. [\APACyear2022] \APACinsertmetastarzhu2022robust{APACrefauthors}Zhu,
    Y., Dong, W., Li, L., Wu, J., Li, X.\BCBL Shi, G. \APACrefYearMonthDay2022. \BBOQ\APACrefatitleRobust
    depth completion with uncertainty-driven loss functions Robust depth completion
    with uncertainty-driven loss functions.\BBCQ \APACrefbtitleProceedings of the
    AAAI Conference on Artificial Intelligence Proceedings of the aaai conference
    on artificial intelligence (\BVOL36, \BPGS3626–3634). \PrintBackRefs\CurrentBib
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y. Zhu \BOthers. [\APACyear2022] \APACinsertmetastarzhu2022robust{APACrefauthors}Zhu,
    Y., Dong, W., Li, L., Wu, J., Li, X.\BCBL Shi, G. \APACrefYearMonthDay2022. \BBOQ\APACrefatitle带有不确定性驱动损失函数的鲁棒深度补全
    Robust depth completion with uncertainty-driven loss functions.\BBCQ \APACrefbtitleAAAI人工智能会议论文集
    Proceedings of the AAAI Conference on Artificial Intelligence (\BVOL36, \BPGS3626–3634).
    \PrintBackRefs\CurrentBib
- en: Zoph \BOthers. [\APACyear2018] \APACinsertmetastarzoph2018learning{APACrefauthors}Zoph,
    B., Vasudevan, V., Shlens, J.\BCBL Le, Q.V. \APACrefYearMonthDay2018. \BBOQ\APACrefatitleLearning
    transferable architectures for scalable image recognition Learning transferable
    architectures for scalable image recognition.\BBCQ \APACrefbtitleProceedings of
    the IEEE conference on computer vision and pattern recognition Proceedings of
    the ieee conference on computer vision and pattern recognition (\BPGS8697–8710).
    \PrintBackRefs\CurrentBib
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoph\BOthers. [\APACyear2018] \APACinsertmetastarzoph2018learning{APACrefauthors}Zoph,
    B., Vasudevan, V., Shlens, J.\BCBL Le, Q.V. \APACrefYearMonthDay2018. \BBOQ\APACrefatitle可转移架构的学习用于可扩展图像识别
    Learning transferable architectures for scalable image recognition.\BBCQ \APACrefbtitle《IEEE计算机视觉与模式识别会议论文集》
    Proceedings of the IEEE conference on computer vision and pattern recognition
    (\BPGS8697–8710). \PrintBackRefs\CurrentBib
- en: Zou \BOthers. [\APACyear2020] \APACinsertmetastarzou2020simultaneous{APACrefauthors}Zou,
    N., Xiang, Z., Chen, Y., Chen, S.\BCBL Qiao, C. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleSimultaneous
    semantic segmentation and depth completion with constraint of boundary Simultaneous
    semantic segmentation and depth completion with constraint of boundary.\BBCQ \APACjournalVolNumPagesSensors203635,
    \PrintBackRefs\CurrentBib
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邹\BOthers. [\APACyear2020] \APACinsertmetastarzou2020simultaneous{APACrefauthors}邹楠、向志、陈源、陈爽\BCBL
    乔春 \APACrefYearMonthDay2020. \BBOQ\APACrefatitle带边界约束的语义分割和深度补全 Simultaneous semantic
    segmentation and depth completion with constraint of boundary.\BBCQ \APACjournalVolNumPages《传感器》203635,
    \PrintBackRefs\CurrentBib
- en: Zuo \BOthers. [\APACyear2020] \APACinsertmetastarzuo2020frequency{APACrefauthors}Zuo,
    Y., Fang, Y., An, P., Shang, X.\BCBL Yang, J. \APACrefYearMonthDay2020. \BBOQ\APACrefatitleFrequency-dependent
    depth map enhancement via iterative depth-guided affine transformation and intensity-guided
    refinement Frequency-dependent depth map enhancement via iterative depth-guided
    affine transformation and intensity-guided refinement.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Multimedia23772–783, \PrintBackRefs\CurrentBib
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左芳\BOthers. [\APACyear2020] \APACinsertmetastarzuo2020frequency{APACrefauthors}左颖、方燕、安平、尚旭\BCBL
    杨杰 \APACrefYearMonthDay2020. \BBOQ\APACrefatitle基于频率依赖的深度图增强：通过迭代深度引导的仿射变换和强度引导的细化
    Frequency-dependent depth map enhancement via iterative depth-guided affine transformation
    and intensity-guided refinement.\BBCQ \APACjournalVolNumPages《IEEE多媒体学报》23772–783,
    \PrintBackRefs\CurrentBib
- en: Zuo, Fang\BCBL \BOthers. [\APACyear2019] \APACinsertmetastarzuo2019depth{APACrefauthors}Zuo,
    Y., Fang, Y., Yang, Y., Shang, X.\BCBL Wu, Q. \APACrefYearMonthDay2019. \BBOQ\APACrefatitleDepth
    map enhancement by revisiting multi-scale intensity guidance within coarse-to-fine
    stages Depth map enhancement by revisiting multi-scale intensity guidance within
    coarse-to-fine stages.\BBCQ \APACjournalVolNumPagesIEEE Transactions on Circuits
    and Systems for Video Technology30124676–4687, \PrintBackRefs\CurrentBib
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左芳\BCBL \BOthers. [\APACyear2019] \APACinsertmetastarzuo2019depth{APACrefauthors}左颖、方燕、杨阳、尚旭\BCBL
    吴强 \APACrefYearMonthDay2019. \BBOQ\APACrefatitle通过在粗到细阶段重新审视多尺度强度引导的深度图增强 Depth
    map enhancement by revisiting multi-scale intensity guidance within coarse-to-fine
    stages.\BBCQ \APACjournalVolNumPages《IEEE视频技术电路与系统学报》30124676–4687, \PrintBackRefs\CurrentBib
- en: 'Zuo \BOthers. [\APACyear2021] \APACinsertmetastarzuo2021mig{APACrefauthors}Zuo,
    Y., Wang, H., Fang, Y., Huang, X., Shang, X.\BCBL Wu, Q. \APACrefYearMonthDay2021.
    \BBOQ\APACrefatitleMIG-net: Multi-scale network alternatively guided by intensity
    and gradient features for depth map super-resolution Mig-net: Multi-scale network
    alternatively guided by intensity and gradient features for depth map super-resolution.\BBCQ
    \APACjournalVolNumPagesIEEE Transactions on Multimedia243506–3519, \PrintBackRefs\CurrentBib'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '左芳\BOthers. [\APACyear2021] \APACinsertmetastarzuo2021mig{APACrefauthors}左颖、王浩、方燕、黄鑫、尚旭\BCBL
    吴强 \APACrefYearMonthDay2021. \BBOQ\APACrefatitleMIG-net：一种通过强度和梯度特征交替引导的多尺度网络用于深度图超分辨率
    MIG-net: Multi-scale network alternatively guided by intensity and gradient features
    for depth map super-resolution.\BBCQ \APACjournalVolNumPages《IEEE多媒体学报》243506–3519,
    \PrintBackRefs\CurrentBib'
- en: Zuo, Wu\BCBL \BOthers. [\APACyear2019] \APACinsertmetastarzuo2019multi{APACrefauthors}Zuo,
    Y., Wu, Q., Fang, Y., An, P., Huang, L.\BCBL Chen, Z. \APACrefYearMonthDay2019.
    \BBOQ\APACrefatitleMulti-scale frequency reconstruction for guided depth map super-resolution
    via deep residual network Multi-scale frequency reconstruction for guided depth
    map super-resolution via deep residual network.\BBCQ \APACjournalVolNumPagesIEEE
    Transactions on Circuits and Systems for Video Technology302297–306, \PrintBackRefs\CurrentBib
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左, 吴\BCBL \BOthers. [\APACyear2019] \APACinsertmetastarzuo2019multi{APACrefauthors}左,
    Y., 吴, Q., 方, Y., 安, P., 黄, L.\BCBL 陈, Z. \APACrefYearMonthDay2019. \BBOQ\APACrefatitle多尺度频率重建用于通过深度残差网络引导的深度图像超分辨率
    多尺度频率重建用于通过深度残差网络引导的深度图像超分辨率.\BBCQ \APACjournalVolNumPagesIEEE Transactions on
    Circuits and Systems for Video Technology302297–306, \PrintBackRefs\CurrentBib
