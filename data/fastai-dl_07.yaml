- en: 'Deep Learning 2: Part 1 Lesson 7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习2：第1部分第7课
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c](https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c)'
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我从* [*fast.ai 课程*](http://www.fast.ai/)* 中的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢给我这个学习机会的*
    [*Jeremy*](https://twitter.com/jeremyphoward) *和* [*Rachel*](https://twitter.com/math_rachel)。'
- en: '[Lesson 7](http://forums.fast.ai/t/wiki-lesson-7/9405)'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[第7课](http://forums.fast.ai/t/wiki-lesson-7/9405)'
- en: 'The theme of Part 1 is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分的主题是：
- en: classification and regression with deep learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习进行分类和回归
- en: identifying and learning best and established practices
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别和学习最佳和已建立的实践
- en: focus is on classification and regression which is predicting “a thing” (e.g.
    a number, a small number of labels)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重点是分类和回归，即预测“一件事”（例如一个数字，少量标签）
- en: 'Part 2 of the course:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 课程的第2部分：
- en: focus is on generative modeling which means predicting “lots of things” — for
    example, creating a sentence as in neural translation, image captioning, or question
    answering while creating an image such as in style transfer, super-resolution,
    segmentation and so forth.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重点是生成建模，这意味着预测“很多事情” — 例如，在神经翻译中创建句子，图像字幕或问题回答，同时创建图像，例如风格转移，超分辨率，分割等等。
- en: not as much best practices but a little more speculative from recent papers
    that may not be fully tested.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不是那么多的最佳实践，而是从最近的可能尚未完全测试的论文中更多的推测。
- en: Review of Char3Model [[02:49](https://youtu.be/H3g26EVADgY?t=2m49s)]
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Char3Model 的回顾
- en: 'Reminder: RNN is not in any way different or unusual or magical — just a standard
    fully connected network.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒：RNN 在任何方面都不是不同或不寻常或神奇的 — 只是一个标准的全连接网络。
- en: Standard fully connected network
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 标准全连接网络
- en: Arrows represent one or more layer operations — generally speaking a linear
    followed by a non-linear function, in this case matrix multiplications followed
    by `relu` or `tanh`
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 箭头代表一个或多个层操作 —— 一般来说是线性后跟一个非线性函数，本例中是矩阵乘法后跟 `relu` 或 `tanh`
- en: Arrows of the same color represent exactly the same weight matrix being used.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同颜色的箭头表示使用完全相同的权重矩阵。
- en: One slight difference from previous is that there are inputs coming in at the
    second and third layers. We tried two approaches — concatenating and adding these
    inputs to the current activations.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与以前的一个细微差别是第二层和第三层有输入进来。我们尝试了两种方法 —— 将这些输入连接或添加到当前激活中。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: By using `nn.Linear` we get both the weight matrix and the bias vector wrapped
    up for free for us.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用 `nn.Linear`，我们免费获得了权重矩阵和偏置向量。
- en: To deal with the fact that there is no orange arrow coming in for the first
    ellipse , we invented an empty matrix
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了解决第一个椭圆中没有橙色箭头的问题，我们发明了一个空矩阵
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Almost identical except for the `for` loop
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎相同，除了 `for` 循环
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: PyTorch version — `nn.RNN` will create the loop and keep track of `h` as it
    goes along.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch版本 — `nn.RNN` 将创建循环并跟踪 `h`。
- en: We are using white section to predict the green character — which seems wasteful
    as the next section mostly overlaps with the current section.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用白色部分来预测绿色字符 —— 这似乎是浪费的，因为下一部分与当前部分大部分重叠。
- en: 'We then tried splitting it into non-overlapping pieces in multi-output model:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们尝试在多输出模型中将其分割为不重叠的部分：
- en: In this approach, we are throwing away our `h` activation after processing each
    section and started a new one. In order to predict the second character using
    the first one in the next section, it has nothing to go on but a default activation.
    Let’s not throw away `h` .
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种方法中，我们在处理每个部分后丢弃了我们的 `h` 激活，并开始了一个新的激活。为了在下一部分中使用第一个字符来预测第二个字符，它除了默认激活外没有其他信息。让我们不要丢弃
    `h`。
- en: Stateful RNN [[08:52](https://youtu.be/H3g26EVADgY?t=8m52s)]
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有状态的RNN
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: One additional line in constructor. `self.init_hidden(bs)` sets `self.h` to
    bunch of zeros.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数中的一个额外行。`self.init_hidden(bs)` 将 `self.h` 设置为一堆零。
- en: '**Wrinkle #1** [[10:51](https://youtu.be/H3g26EVADgY?t=10m51s)] — if we were
    to simply do `self.h = h` , and we trained on a document that is a million characters
    long, then the size of unrolled version of the RNN has a million layers (ellipses).
    One million layer fully connected network is going to be very memory intensive
    because in order to do a chain rule, we have to multiply one million layers while
    remembering all one million gradients every batch.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题 #1** — 如果我们简单地执行 `self.h = h`，并在一个包含一百万个字符的文档上进行训练，那么 RNN 的展开版本的大小将有一百万层（椭圆）。一百万层全连接网络将非常占用内存，因为为了进行链式规则，我们必须在每个批次中乘以一百万层，同时记住所有一百万个梯度。'
- en: To avoid this, we tell it to forget its history from time to time. We can still
    remember the state (the values in our hidden matrix) without remembering everything
    about how we got there.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免这种情况，我们告诉它不时忘记它的历史。我们仍然可以记住状态（隐藏矩阵中的值）而不必记住如何到达那里的一切。
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Grab the tensor out of `Variable` `h` (remember, a tensor itself does not have
    any concept of history), and create a new `Variable` out of that. The new variable
    has the same value but no history of operations, therefore when it tries to back-propagate,
    it will stop there.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 `Variable` `h` 中取出张量（记住，张量本身没有任何历史概念），并从中创建一个新的 `Variable`。新变量具有相同的值，但没有操作历史，因此当它尝试反向传播时，它将在那里停止。
- en: '`forward` will process 8 characters, it then back propagate through eight layers,
    keep track of the values in out hidden state, but it will throw away its history
    of operations. This is called **back-prop through time (bptt)**.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward`将处理8个字符，然后通过8个层进行反向传播，跟踪隐藏状态中的值，但会丢弃其操作历史。这被称为**时间反向传播（bptt）**。'
- en: In other words, after the `for` loop, just throw away the history of operations
    and start afresh. So we are keeping our hidden state but we are not keeping our
    hidden state history.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 换句话说，在`for`循环之后，只需丢弃操作历史并重新开始。因此，我们保留了我们的隐藏状态，但没有保留我们的隐藏状态历史。
- en: Another good reason not to back-propagate through too many layers is that if
    you have any kind of gradient instability (e.g. gradient explosion or gradient
    banishing), the more layers you have, the harder the network gets to train (slower
    and less resilient).
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要通过太多层进行反向传播的另一个很好的理由是，如果您有任何梯度不稳定性（例如，梯度爆炸或梯度消失），您拥有的层数越多，网络训练就越困难（速度更慢，弹性更差）。
- en: On the other hand, the longer `bptt` means that you are able to explicitly capture
    a longer memory and more state.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，更长的`bptt`意味着您能够明确捕获更长的记忆和更多状态。
- en: '**Wrinkle #2** [[16:00](https://youtu.be/H3g26EVADgY?t=16m)] — how to create
    mini-batches. We do not want to process one section at a time, but a bunch in
    parallel at a time.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**皱纹＃2**[16:00] - 如何创建小批量。我们不想一次处理一个部分，而是一次并行处理一堆。'
- en: When we started looking at TorchText for the first time, we talked about how
    it creates these mini-batches.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们第一次开始研究TorchText时，我们谈到了它如何创建这些小批量。
- en: Jeremy said we take a whole long document consisting of the entire works of
    Nietzsche or all of the IMDB reviews concatenated together, we split this into
    64 equal sized chunks (NOT chunks of size 64).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeremy说我们拿一整个由尼采的全部作品或所有IMDB评论连接在一起的长文档，将其分成64个相等大小的块（不是大小为64的块）。
- en: For a document that is 64 million characters long, each “chunk” will be 1 million
    characters. We stack them together and now split them by `bptt` — 1 mini-bach
    consists of 64 by `bptt` matrix.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一个长度为6400万字符的文档，每个“块”将是100万个字符。我们将它们堆叠在一起，现在按`bptt`拆分它们 - 1个小批次由64个`bptt`矩阵组成。
- en: The first character of the second chunk(1,000,001th character) is likely be
    in the middle of a sentence. But it is okay since it only happens once every million
    characters.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二块（第100万个字符）的第一个字符可能在一个句子的中间。但没关系，因为这只会在每一百万个字符中发生一次。
- en: 'Question: Data augmentation for this kind of dataset? [[20:34](https://youtu.be/H3g26EVADgY?t=20m34s)]'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题：这种数据集的数据增强？[20:34]
- en: There is no known good way. Somebody recently won a Kaggle competition by doing
    data augmentation which randomly inserted parts of different rows — something
    like that may be useful here. But there has not been any recent state-of-the-art
    NLP papers that are doing this kind of data augmentation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 没有已知的好方法。最近有人通过进行数据增强赢得了一个Kaggle竞赛，随机插入不同行的部分 - 这样的方法可能在这里有用。但最近没有任何最先进的NLP论文在进行这种数据增强。
- en: 'Question: How do we choose the size of bptt? [[21:36](https://youtu.be/H3g26EVADgY?t=21m36s)]'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题：我们如何选择bptt的大小？[21:36]
- en: 'There are a couple things to think about:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有几件事需要考虑：
- en: the first is that mini-batch matrix has a size of `bs` (# of chunks) by `bptt`
    so your GPU RAM must be able to fit that by your embedding matrix. So if you get
    CUDA out of memory error, you need reduce one of these.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一点是小批量矩阵的大小为`bs`（块数）乘以`bptt`，因此您的GPU RAM必须能够容纳嵌入矩阵。因此，如果您遇到CUDA内存不足错误，您需要减少其中一个。
- en: If your training is unstable (e.g. your loss is shooting off to NaN suddenly),
    then you could try decreasing your `bptt` because you have less layers to gradient
    explode through.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的训练不稳定（例如，您的损失突然飙升到NaN），那么您可以尝试减少您的`bptt`，因为您的层较少，梯度不会爆炸。
- en: If it is too slow [[22:44](https://youtu.be/H3g26EVADgY?t=22m44s)], try decreasing
    your `bptt` because it will do one of those steps at a time. `for` loop cannot
    be parallelized (for the current version). There is a recent thing called QRNN
    (Quasi-Recurrent Neural Network) which does parallelize it and we hope to cover
    in part 2.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果速度太慢[22:44]，尝试减少你的`bptt`，因为它会一次执行一个步骤。`for`循环不能并行化（对于当前版本）。最近有一种叫做QRNN（准循环神经网络）的东西，它可以并行化，我们希望在第二部分中介绍。
- en: So pick the highest number that satisfies all these.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所以选择满足所有这些条件的最高数字。
- en: Stateful RNN & TorchText [[23:23](https://youtu.be/H3g26EVADgY?t=23m23s)]
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有状态的RNN和TorchText[23:23]
- en: When using an existing API which expects data to be certain format, you can
    either change your data to fit that format or you can write your own dataset sub-class
    to handle the format that your data is already in. Either is fine, but in this
    case, we will put our data in the format TorchText already support. Fast.ai wrapper
    around TorchText already has something where you can have a training path and
    validation path, and one or more text files in each path containing bunch of text
    that are concatenated together for your language model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用期望数据符合特定格式的现有API时，您可以将数据更改为符合该格式，也可以编写自己的数据集子类来处理您的数据已经存在的格式。两者都可以，但在这种情况下，我们将把我们的数据放在TorchText已经支持的格式中。Fast.ai对TorchText的包装器已经有了一些东西，您可以在每个路径中有一个训练路径和验证路径，并且每个路径中有一个或多个文本文件，其中包含一堆文本，这些文本被连接在一起用于您的语言模型。
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Made a copy of Nietzsche file, pasted into training and validation directory.
    Then deleted the last 20% of the rows from training set, and deleted everything
    but the last 20% from the validation set [[25:15](https://youtu.be/H3g26EVADgY?t=25m15s)].
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制了尼采文件，粘贴到训练和验证目录中。然后从训练集中删除最后20%的行，并删除验证集中除最后20%之外的所有内容[25:15]。
- en: The other benefit of doing it this way is that it seems like it is more realistic
    to have a validation set that was not a random shuffled set of rows of text, but
    it was totally separate part of the corpus.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这样做的另一个好处是，似乎更现实地拥有一个验证集，它不是文本行的随机洗牌集，而是完全独立于语料库的一部分。
- en: When you are doing a language model, you do not really need separate files.
    You can have multiple files but they just get concatenated together anyway.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您进行语言模型时，您实际上不需要单独的文件。您可以有多个文件，但它们最终会被连接在一起。
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In TorchText, we make this thing called `Field` and initially `Field` is just
    a description of how to go about pre-processing the text.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TorchText中，我们创建了一个叫做`Field`的东西，最初`Field`只是关于如何进行文本预处理的描述。
- en: '`lower` — we told it to lowercase the text'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lower` - 我们告诉它将文本转换为小写'
- en: '`tokenize` — Last time, we used a function that splits on whitespace that gave
    us a word model. This time, we want a character model, so use `list` function
    to tokenize strings. Remember, in Python, `list(''abc'')` will return `[''a'',
    ''b'', ''c'']` .'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize` - 上次，我们使用了一个在空格上分割的函数，给我们一个单词模型。这次，我们想要一个字符模型，所以使用`list`函数来对字符串进行标记化。记住，在Python中，`list(''abc'')`将返回`[''a''，''b''，''c'']`。'
- en: '`bs` : batch size, `bptt` : we renamed `cs` , `n_fac` : size of embedding,
    `n_hidden` : size of our hidden state'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bs`：批次大小，`bptt`：我们将其重命名为`cs`，`n_fac`：嵌入的大小，`n_hidden`：我们隐藏状态的大小'
- en: We do not have a separate test set, so we’ll just use validation set for testing
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有单独的测试集，所以我们将只使用验证集进行测试
- en: TorchText randomize the length of `bptt` a little bit each time. It does not
    always give us exactly 8 characters; 5% of the time, it will cut it in half and
    add on a small standard deviation to make it slightly bigger or smaller than 8\.
    We cannot shuffle the data since it needs to be contiguous, so this is a way to
    introduce some randomness.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TorchText每次都会稍微随机化`bptt`的长度。它并不总是给我们确切的8个字符；有5%的概率，它会将其减半并添加一个小的标准偏差，使其略大或略小于8。我们不能对数据进行洗牌，因为它需要是连续的，所以这是引入一些随机性的一种方式。
- en: 'Question [[31:46](https://youtu.be/H3g26EVADgY?t=31m46s)]: Does the size remain
    constant per mini-batch? Yes, we need to do matrix multiplication with `h` weight
    matrix, so mini-batch size must remain constant. But sequence length can change
    no problem.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：每个小批次的大小是否保持恒定？是的，我们需要用`h`权重矩阵进行矩阵乘法，因此小批次的大小必须保持恒定。但是序列长度可以改变，没有问题。
- en: '`len(md.trn_dl)` : length of data loader (i.e. how many mini-batches), `md.nt`
    : number of tokens (i.e. how many unique things are in the vocabulary)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`len(md.trn_dl)`：数据加载器的长度（即有多少个小批次），`md.nt`：标记的数量（即词汇表中有多少个唯一的东西）'
- en: Once you run `LanguageModelData.from_text_files` , `TEXT` will contain an extra
    attribute called `vocab`. `TEXT.vocab.itos` list of unique items in the vocabulary,
    and `TEXT.vocab.stoi` is a reverse mapping from each item to number.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦运行`LanguageModelData.from_text_files`，`TEXT`将包含一个名为`vocab`的额外属性。`TEXT.vocab.itos`是词汇表中唯一项目的列表，`TEXT.vocab.stoi`是从每个项目到数字的反向映射。
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Wrinkle #3** [[33:51](https://youtu.be/H3g26EVADgY?t=33m51s)]: Jeremy lied
    to us when he said that mini-batch size remains constant. It is very likely that
    the last mini-batch is shorter than the rest unless the dataset is exactly divisible
    by `bptt` times `bs` . That is why we check whether `self.h` ‘s second dimension
    is the same as `bs` of the input. If it is not the same, set it back to zero with
    the input’s `bs` . This happens at the end of the epoch and the beginning of the
    epoch (setting back to the full batch size).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题 #3**：Jeremy在说小批次大小保持恒定时对我们撒谎了。最后一个小批次很可能比其他小批次短，除非数据集恰好可以被`bptt`乘以`bs`整除。这就是为什么我们要检查`self.h`的第二维是否与输入的`bs`相同。如果不相同，将其设置回零，并使用输入的`bs`。这发生在周期结束和周期开始时（将其设置回完整的批次大小）。'
- en: '**Wrinkle #4** [[35:44](https://youtu.be/H3g26EVADgY?t=35m44s)]: The last wrinkle
    is something that slightly sucks about PyTorch and maybe somebody can be nice
    enough to try and fix it with a PR. Loss functions are not happy receiving a rank
    3 tensor (i.e. three dimensional array). There is no particular reason they ought
    to not be happy receiving a rank 3 tensor (sequence length by batch size by results
    — so you can just calculate loss for each of the two initial axis). Works for
    rank 2 or 4, but not 3.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题 #4**：最后一个问题是关于PyTorch的一个小问题，也许有人可以友好地尝试通过PR来修复它。损失函数不喜欢接收一个三维张量（即三维数组）。它们不应该不喜欢接收一个三维张量（按序列长度、批次大小和结果计算损失
    - 因此您可以为两个初始轴的每个计算损失）。对于二维或四维张量可以工作，但对于三维张量不行。'
- en: '`.view` will reshape rank 3 tensor into rank 2 of `-1` (however big as necessary)
    by `vocab_size`. TorchText automatically changes the **target** to be flattened
    out, so we do not need to do that for actual values (when we looked at a mini-batch
    in lesson 4, we noticed that it was flattened. Jeremy said we will learn about
    why later, so later is now.)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.view`将三维张量重塑为二维的`-1`（必要时尽可能大）乘以`vocab_size`。TorchText自动将**目标**展平，因此我们不需要为实际值这样做（当我们在第4课看到一个小批次时，我们注意到它被展平了。Jeremy说我们以后会了解原因，现在就是时候了）。'
- en: PyTorch (as of 0.3), `log_softmax` requires us to specify which axis we want
    to do the softmax over (i.e. which axis we want to sum to one). In this case we
    want to do it over the last axis `dim = -1`.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch（截至0.3版），`log_softmax`要求我们指定我们要对softmax进行的轴（即我们要将其求和为1的轴）。在这种情况下，我们希望在最后一个轴`dim
    = -1`上进行。
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let’s gain more insight by unpacking RNN [[42:48](https://youtu.be/H3g26EVADgY?t=42m48s)]
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们通过拆解RNN来获得更多见解
- en: 'We remove the use of `nn.RNN` and replace it with `nn.RNNCell` . PyTorch source
    code looks like the following. You should be able to read and understand (Note:
    they do not concatenate the input and the hidden state, but they sum them together
    — which was our first approach):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们移除了`nn.RNN`的使用，并用`nn.RNNCell`替换。PyTorch源代码如下。您应该能够阅读和理解（注意：它们不会连接输入和隐藏状态，而是将它们相加
    - 这是我们的第一种方法）：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Question about `tanh` [[44:06](https://youtu.be/H3g26EVADgY?t=44m6s)]: As we
    have seen last week, `tanh` is forcing the value to be between -1 and 1\. Since
    we are multiplying by this weight matrix again and again, we would worry that
    `relu` (since it is unbounded) might have more gradient explosion problem. Having
    said that, you can specify `RNNCell` to use different `nonlineality` whose default
    is `tanh` and ask it to use `relu` if you wanted to.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`tanh`的问题[[44:06](https://youtu.be/H3g26EVADgY?t=44m6s)]：正如我们上周所看到的，`tanh`强制值在-1和1之间。由于我们一遍又一遍地乘以这个权重矩阵，我们担心`relu`（因为它是无界的）可能会有更多的梯度爆炸问题。话虽如此，您可以指定`RNNCell`使用不同的`nonlineality`，其默认值为`tanh`，并要求其使用`relu`。
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`for` loop is back and append the result of linear function to a list — which
    in end gets stacked up together.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`for`循环回来并将线性函数的结果附加到列表中 - 最终将它们堆叠在一起。'
- en: fast.ai library actually does exactly this in order to use regularization approaches
    that are not supported by PyTorch.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际上，fast.ai库确实正是为了使用PyTorch不支持的正则化方法而这样做的。
- en: Gated Recurrent Unit (GRU) [[46:44](https://youtu.be/H3g26EVADgY?t=46m44s)]
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 门控循环单元（GRU）[[46:44](https://youtu.be/H3g26EVADgY?t=46m44s)]
- en: In practice, nobody really uses `RNNCell` since even with `tanh` , gradient
    explosions are still a problem and we need use low learning rate and small `bptt`
    to get them to train. So what we do is to replace `RNNCell` with something like
    `GRUCell` .
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，没有人真正使用`RNNCell`，因为即使使用`tanh`，梯度爆炸仍然是一个问题，我们需要使用较低的学习率和较小的`bptt`来训练它们。因此，我们所做的是用类似`GRUCell`替换`RNNCell`。
- en: '[http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)'
- en: Normally, the input gets multiplied by a weight matrix to create new activations
    `h` and get added to the existing activations straight away. That is not wha happens
    here.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，输入会乘以一个权重矩阵以创建新的激活`h`，并立即添加到现有的激活中。这里不是这样发生的。
- en: Input goes into `h˜` and it doesn’t just get added to the previous activations,
    but the previous activation gets multiplied by `r` (reset gate) which has a value
    of 0 or 1.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入进入`h˜`，它不仅仅被添加到先前的激活中，而是先前的激活被`r`（重置门）乘以，`r`的值为0或1。
- en: '`r` is calculated as below — matrix multiplication of some weight matrix and
    the concatenation of our previous hidden state and new input. In other words,
    this is a little one hidden layer neural net. It gets put through the sigmoid
    function as well. This mini neural net learns to determine how much of the hidden
    states to remember (maybe forget it all when it sees a full-stop character — beginning
    of a new sentence).'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r`的计算如下 - 一些权重矩阵的矩阵乘法和我们先前隐藏状态和新输入的连接。换句话说，这是一个小型的单隐藏层神经网络。它也通过sigmoid函数传递。这个小型神经网络学会了确定要记住隐藏状态的多少（也许在看到句号字符时全部忘记
    - 新句子的开始）。'
- en: '`z` gate (update gate) determines what degree to use `h˜` (the new input version
    of hidden states) and what degree to leave the hidden state the same as before.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`z`门（更新门）确定要使用`h˜`（隐藏状态的新输入版本）的程度，以及要保持隐藏状态与之前相同的程度。'
- en: '[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
- en: Linear interpolation
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性插值
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Above is what `GRUCell` code looks like, and our new model that utilize this
    is below:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 上面是`GRUCell`代码的样子，我们利用这个新模型如下：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As a result, we can lower the loss down to 1.36 (`RNNCell` one was 1.54). In
    practice, GRU and LSTM are what people uses.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们可以将损失降低到1.36（`RNNCell`为1.54）。在实践中，GRU和LSTM是人们使用的。
- en: 'Putting it all together: Long Short-Term Memory [[54:09](https://youtu.be/H3g26EVADgY?t=54m9s)]'
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容放在一起：长短期记忆[[54:09](https://youtu.be/H3g26EVADgY?t=54m9s)]
- en: 'LSTM has one more piece of state in it called “cell state” (not just hidden
    state), so if you do use a LSTM, you have to return a tuple of matrices in `init_hidden`
    (exactly the same size as hidden state):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM中还有一个称为“单元状态”的状态（不仅仅是隐藏状态），因此如果使用LSTM，必须在`init_hidden`中返回一个矩阵元组（与隐藏状态完全相同的大小）：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The code is identical to GRU one. The one thing that was added was `dropout`
    which does dropout after each time step and doubled the hidden layer — in a hope
    that it will be able to learn more and be resilient as it does so.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 代码与GRU相同。添加的一件事是`dropout`，它在每个时间步之后进行dropout并将隐藏层加倍 - 希望它能够学到更多并且在这样做时更具弹性。
- en: Callbacks (specifically SGDR) without Learner class [[55:23](https://youtu.be/H3g26EVADgY?t=55m23s)]
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回调（特别是SGDR）没有Learner类[[55:23](https://youtu.be/H3g26EVADgY?t=55m23s)]
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After creating a standard PyTorch model, we usually do something like `opt =
    optim.Adam(m.parameters(), 1e-3)`. Instead, we will use fast.ai `LayerOptimizer`
    which takes an optimizer `optim.Adam` , our model `m` , learning rate `1e-2` ,
    and optionally weight decay `1e-5` .
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建标准的PyTorch模型后，我们通常会做类似`opt = optim.Adam(m.parameters(), 1e-3)`的事情。相反，我们将使用fast.ai的`LayerOptimizer`，它接受一个优化器`optim.Adam`，我们的模型`m`，学习率`1e-2`，以及可选的权重衰减`1e-5`。
- en: A key reason `LayerOptimizer` exists is to do differential learning rates and
    differential weight decay. The reason we need to use it is that all of the mechanics
    inside fast.ai assumes that you have one of these. If you want to use callbacks
    or SGDR in code you are not using the Learner class, you need to use this.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LayerOptimizer`存在的一个关键原因是进行差分学习率和差分权重衰减。我们需要使用它的原因是fast.ai内部的所有机制都假定您有其中之一。如果要在不使用Learner类的代码中使用回调或SGDR，您需要使用这个。'
- en: '`lo.opt` returns the optimizer.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lo.opt`返回优化器。'
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When we call `fit`, we can now pass the `LayerOptimizer` and also `callbacks`.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们调用`fit`时，现在可以传递`LayerOptimizer`和`callbacks`。
- en: Here, we use cosine annealing callback — which requires a `LayerOptimizer` object.
    It does cosine annealing by changing learning rate in side the `lo` object.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，我们使用余弦退火回调 —— 需要一个`LayerOptimizer`对象。它通过更改`lo`对象内的学习率来进行余弦退火。
- en: 'Concept: Create a cosine annealing callback which is going to update the learning
    rates in the layer optimizer `lo` . The length of an epoch is equal to `len(md.trn_dl)`
    — how many mini-batches are there in an epoch is the length of the data loader.
    Since it is doing cosine annealing, it needs to know how often to reset. You can
    pass in `cycle_mult` in usual way. We can even save our model automatically just
    like we did with `cycle_save_name` in `Learner.fit`.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念：创建一个余弦退火回调，它将更新层优化器`lo`中的学习率。一个周期的长度等于`len(md.trn_dl)` —— 一个周期中有多少个小批次就是数据加载器的长度。由于它正在进行余弦退火，它需要知道多久重置一次。您可以以通常的方式传递`cycle_mult`。我们甚至可以自动保存我们的模型，就像我们在`Learner.fit`中使用`cycle_save_name`一样。
- en: We can do callback at a start of a training, epoch or a batch, or at the end
    of a training, an epoch, or a batch.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在训练、周期或批处理的开始时进行回调，也可以在训练、周期或批处理的结束时进行回调。
- en: It has been used for `CosAnneal` (SGDR), and decoupled weight decay (AdamW),
    loss-over-time graph, etc.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它已用于`CosAnneal`（SGDR），和解耦权重衰减（AdamW），随时间变化的损失图等。
- en: Testing [[59:55](https://youtu.be/H3g26EVADgY?t=59m55s)]
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试[[59:55](https://youtu.be/H3g26EVADgY?t=59m55s)]
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In lesson 6, when we were testing `CharRnn` model, we noticed that it repeated
    itself over and over. `torch.multinomial` used in this new version deals with
    this problem. `p[-1]` to get the final output (the triangle), `exp` to convert
    log probability to probability. We then use `torch.multinomial` function which
    will give us a sample using the given probabilities. If probability is [0, 1,
    0, 0] and ask it to give us a sample, it will always return the second item. If
    it was [0.5, 0, 0.5], it will give the first item 50% of the time, and second
    item . 50% of the time ([review of multinomial distribution](http://onlinestatbook.com/2/probability/multinomial.html))
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第6课中，当我们测试`CharRnn`模型时，我们注意到它一遍又一遍地重复。在这个新版本中使用的`torch.multinomial`处理了这个问题。`p[-1]`用于获取最终输出（三角形），`exp`用于将对数概率转换为概率。然后我们使用`torch.multinomial`函数，根据给定的概率给出一个样本。如果概率是[0,
    1, 0, 0]，并要求它给我们一个样本，它将始终返回第二个项目。如果是[0.5, 0, 0.5]，它将50%的时间给出第一个项目，50%的时间给出第二个项目（[多项分布的评论](http://onlinestatbook.com/2/probability/multinomial.html)）
- en: To play around with training character based language models like this, try
    running `get_next_n` at different levels of loss to get a sense of what it looks
    like. The example above is at 1.25, but at 1.3, it looks like a total junk.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要尝试训练基于字符的语言模型，可以尝试在不同损失水平上运行`get_next_n`，以了解其外观。上面的示例是1.25，但在1.3时，它看起来像一团垃圾。
- en: When you are playing around with NLP, particularly generative model like this,
    and the results are kind of okay but not great, do not be disheartened because
    that means you are actually very VERY nearly there!
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您在玩弄NLP时，特别是像这样的生成模型，并且结果还可以但不是很好时，请不要灰心，因为这意味着您实际上非常非常接近成功！
- en: '[Back to computer vision: CIFAR 10](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson7-cifar10.ipynb)
    [[1:01:58](https://youtu.be/H3g26EVADgY?t=1h1m58s)]'
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[返回计算机视觉：CIFAR 10](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson7-cifar10.ipynb)
    [[1:01:58](https://youtu.be/H3g26EVADgY?t=1h1m58s)]'
- en: CIFAR 10 is an old and well known dataset in academia — well before ImageNet,
    there was CIFAR 10\. It is small both in terms of number of images and size of
    images which makes it interesting and challenging. You will likely be working
    with thousands of images rather than one and a half million images. Also a lot
    of the things we are looking at like in medical imaging, we are looking at a specific
    area where there is a lung nodule, you are probably looking at 32 by 32 pixels
    at most.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR 10是学术界中一个古老而著名的数据集 —— 在ImageNet之前，有CIFAR 10。它在图像数量和大小方面都很小，这使得它既有趣又具有挑战性。您可能会处理成千上万张图像，而不是一百五十万张图像。此外，我们正在研究的许多内容，比如在医学成像中，我们正在查看一个肺结节的特定区域，您可能最多查看32x32像素。
- en: It also runs quickly, so it is much better to test our your algorithms. As Ali
    Rahini mentioned in NIPS 2017, Jeremy has the concern that many people are not
    doing carefully tuned and throught-about experiments in deep learning, but instead,
    they throw lots of GPUs and TPUs or lots of data and consider that a day. It is
    important to test many versions of your algorithm on dataset like CIFAR 10 rather
    than ImageNet that takes weeks. MNIST is also good for studies and experiments
    even though people tend to complain about it.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 它也运行得很快，因此最好测试一下您的算法。正如Ali Rahini在NIPS 2017中提到的，Jeremy担心许多人在深度学习中没有进行精心调整和深思熟虑的实验，而是他们投入大量的GPU和TPU或大量的数据，然后认为一天就够了。在像CIFAR
    10这样的数据集上测试您的算法的许多版本是很重要的，而不是像ImageNet那样需要几周的时间。尽管人们倾向于抱怨MNIST，但它也适用于研究和实验。
- en: CIFAR 10 data in image format is available [here](http://pjreddie.com/media/files/cifar.tgz)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR 10数据以图像格式可在[此处](http://pjreddie.com/media/files/cifar.tgz)获取
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`classes` — image labels'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classes` — 图像标签'
- en: '`stats` —When we use pre-trained models, you can call `tfms_from_model` which
    creates the necessary transforms to convert our data set into a normalized dataset
    based on the means and standard deviations of each channel in the original model
    that was trained in. Since we are training a model from scratch, we ned to tell
    it the mean and standard deviation of our data to normalize it. Make sure you
    can calculate the mean and the standard deviation for each channel.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stats` — 当我们使用预训练模型时，可以调用`tfms_from_model`，它会创建必要的转换，将我们的数据集转换为基于原始模型中每个通道的均值和标准差的归一化数据集。由于我们正在从头开始训练模型，因此需要告诉它我们数据的均值和标准差以进行归一化。确保您可以计算每个通道的均值和标准差。'
- en: '`tfms` — For CIFAR 10 data augmentation, people typically do horizontal flip
    and black padding around the edge and randomly select 32 by 32 area within the
    padded image.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tfms` — 对于CIFAR 10数据增强，人们通常会进行水平翻转和在边缘周围添加黑色填充，并在填充图像内随机选择32x32区域。'
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'From [this notebook](https://github.com/KeremTurgutlu/deeplearning/blob/master/Exploring%20Optimizers.ipynb)
    by our student Kerem Turgutlu:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 来自我们的学生Kerem Turgutlu的[这个笔记本](https://github.com/KeremTurgutlu/deeplearning/blob/master/Exploring%20Optimizers.ipynb)：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`nn.ModuleList` — whenever you create a list of layers in PyTorch, you have
    to wrap it in `ModuleList` to register these as attributes.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.ModuleList` - 每当您在PyTorch中创建一组层时，您必须将其包装在 `ModuleList` 中以将这些注册为属性。'
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now we step up one level of API higher — rather than calling `fit` function,
    we create a `learn` object *from a custom model*. `ConfLearner.from_model_data`
    takes standard PyTorch model and model data object.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们提高一个API级别 - 而不是调用 `fit` 函数，我们从一个自定义模型创建一个 `learn` 对象。`ConfLearner.from_model_data`
    接受标准的PyTorch模型和模型数据对象。
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With a simple one hidden layer model with 122,880 parameters, we achieved 46.9%
    accuracy. Let’s improve this and gradually build up to a basic ResNet architecture.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个具有122,880个参数的简单单隐藏层模型，我们实现了46.9%的准确率。让我们改进这一点，并逐渐构建一个基本的ResNet架构。
- en: CNN [[01:12:30](https://youtu.be/H3g26EVADgY?t=1h12m30s)]
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN [[01:12:30](https://youtu.be/H3g26EVADgY?t=1h12m30s)]
- en: Let’s replace a fully connected model with a convolutional model. Fully connected
    layer is simply doing a dot product. That is why the weight matrix is big (3072
    input * 40 = 122880). We are not using the parameters very efficiently because
    every single pixel in the input has a different weight. What we want to do is
    a group of 3 by 3 pixels that have particular patterns to them (i.e. convolution).
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们用一个卷积模型替换一个全连接模型。全连接层只是做一个点积。这就是为什么权重矩阵很大（3072个输入 * 40 = 122880）。我们没有有效地使用参数，因为输入中的每个像素都有不同的权重。我们想要做的是一组具有特定模式的3x3像素（即卷积）。
- en: We will use a filter with three by three kernel. When there are multiple filters,
    the output will have additional dimension.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用一个3x3核的滤波器。当有多个滤波器时，输出将具有额外的维度。
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Replace `nn.Linear` with `nn.Conv2d`
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用 `nn.Conv2d` 替换 `nn.Linear`
- en: First two parameters are exactly the same as `nn.Linear` — the number of features
    coming in, and the number of features coming out
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前两个参数与 `nn.Linear` 完全相同 - 输入特征的数量和输出特征的数量
- en: '`kernel_size=3` , the size of the filter'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel_size=3`，滤波器的大小'
- en: '`stride=2` will use every other 3 by 3 area which will halve the output resolution
    in each dimension (i.e. it has the same effect as 2 by 2 max-pooling)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride=2` 将使用每隔一个3x3区域，这将使每个维度的输出分辨率减半（即具有与2x2最大池化相同的效果）'
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`ConvNet([3, 20, 40, 80], 10)` — It start with 3 RGB channels, 20, 40, 80 features,
    then 10 classes to predict.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConvNet([3, 20, 40, 80], 10)` - 从3个RGB通道开始，20、40、80个特征，然后预测10个类别。'
- en: '`AdaptiveMaxPool2d` — This followed by a linear layer is how you get from 3
    by 3 down to a prediction of one of 10 classes and is now a standard for state-of-the-art
    algorithms. The very last layer, we do a special kind of max-pooling for which
    you specify the output activation resolution rather than how big of an area to
    poll. In other words, here we do 3 by 3 max-pool which is equivalent of 1 by 1
    *adaptive* max-pool.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AdaptiveMaxPool2d` - 这是一个线性层后面的内容，通过这种方式，你可以从3x3降到10个类别中的一个预测，并且现在已经成为最先进算法的标准。在最后一层，我们进行一种特殊类型的最大池化，您需要指定输出激活分辨率，而不是要池化的区域有多大。换句话说，在这里我们进行3x3最大池化，相当于1x1的*自适应*最大池化。'
- en: '`x = x.view(x.size(0), -1)` — `x` has a shape of # of the features by 1 by
    1, so it will remove the last two layers.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x = x.view(x.size(0), -1)` - `x` 的形状是特征的数量乘以1乘以1，因此它将删除最后两层。'
- en: This model is called “fully convolutional network” — where every layer is convolutional
    except for the very last.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个模型被称为“完全卷积网络” - 每一层都是卷积的，除了最后一层。
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The default final learning rate `lr_find` tries is 10\. If the loss is still
    getting better at that point, you can overwrite by specifying `end_lr` .
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr_find` 尝试的默认最终学习率是10。如果在那一点上损失仍在变好，您可以通过指定 `end_lr` 来覆盖。'
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It flattened out around 60% accuracy. Considering it uses about 30,000 parameters
    (compared to 47% with 122k parameters)
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率在60%左右稳定下来。考虑到它使用约30,000个参数（与122k参数的47%相比）
- en: Time per epoch is about the same since their architectures are both simple and
    most of time is spent doing memory transfer.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个时期的时间大约相同，因为它们的架构都很简单，大部分时间都花在内存传输上。
- en: Refactored [[01:21:57](https://youtu.be/H3g26EVADgY?t=1h21m57s)]
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重构 [[01:21:57](https://youtu.be/H3g26EVADgY?t=1h21m57s)]
- en: Simplify `forward` function by creating `ConvLayer` (our first custom layer!).
    In PyTorch, layer definition and neural network definitions are identical. Anytime
    you have a layer, you can use it as a neural net, when you have a neural net,
    you can use it as a layer.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建 `ConvLayer`（我们的第一个自定义层）简化 `forward` 函数。在PyTorch中，层定义和神经网络定义是相同的。每当您有一个层时，您可以将其用作神经网络，当您有一个神经网络时，您可以将其用作层。
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`padding=1` — When you do convolution the image shrink by 1 pixel on each side.
    So it does not go from 32 by 32 to 16 by 16 but actually 15 by 15\. `padding`
    will add a border so we can keep the edge pixel information. It is not as big
    of a deal for a big image, but when it’s down to 4 by 4, you really don’t want
    to throw away a whole piece.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding=1` - 当进行卷积时，图像的每一侧都会缩小1个像素。因此，它不是从32x32到16x16，而实际上是15x15。`padding`
    将添加一个边框，以便我们可以保留边缘像素信息。对于大图像来说，这不是一个大问题，但当缩小到4x4时，您真的不想丢弃整个部分。'
- en: '[PRE28]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Another difference from the last model is that `nn.AdaptiveMaxPool2d` does not
    have any state (i.e. no weights). So we can just call it as a function `F.adaptive_max_pool2d`
    .
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与上一个模型的另一个不同之处是 `nn.AdaptiveMaxPool2d` 没有任何状态（即没有权重）。因此，我们可以将其作为一个函数 `F.adaptive_max_pool2d`
    调用。
- en: BatchNorm [[1:25:10](https://youtu.be/H3g26EVADgY?t=1h25m10s)]
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BatchNorm [[1:25:10](https://youtu.be/H3g26EVADgY?t=1h25m10s)]
- en: The last model, when we tried to add more layers, we had trouble training. The
    reason we had trouble training was that if we used larger learning rates, it would
    go off to NaN and if we used smaller learning rate, it would take forever and
    doesn’t have a chance to explore properly — so it was not resilient.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个模型，当我们尝试添加更多层时，我们遇到了训练困难。我们遇到训练困难的原因是，如果使用更大的学习率，它会变成NaN，如果使用更小的学习率，它将花费很长时间，无法正确探索
    - 因此它不具有弹性。
- en: To make it resilient, we will use something called batch normalization. BatchNorm
    came out about two years ago and it has been quite transformative since it suddenly
    makes it really easy to train deeper networks.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了使其具有弹性，我们将使用一种称为批量归一化的东西。 BatchNorm大约两年前出现，自那时以来，它已经发生了很大变化，因为它突然使训练更深的网络变得非常容易。
- en: We can simply use `nn.BatchNorm` but to learn about it, we will write it from
    scratch.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以简单地使用`nn.BatchNorm`，但为了了解它，我们将从头开始编写。
- en: It is unlikely that the weight matrices on average are not going to cause your
    activations to keep getting smaller and smaller or keep getting bigger and bigger.
    It is important to keep them at reasonable scale. So we start things off with
    zero-mean standard deviation one by normalizing the input. What we really want
    to do is to do this for all layers, not just the inputs.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均来看，权重矩阵不太可能导致激活不断变小或不断变大。保持它们在合理的范围内很重要。因此，我们从零均值标准差为1开始通过对输入进行归一化。我们真正想要做的是对所有层进行这样的操作，而不仅仅是对输入。
- en: '[PRE29]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Calculate the mean of each channel or each filter and standard deviation of
    each channel or each filter. Then subtract the means and divide by the standard
    deviations.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每个通道或每个滤波器的均值和每个通道或每个滤波器的标准差。然后减去均值并除以标准差。
- en: We no longer need to normalize our input because it is normalizing it per channel
    or for later layers it is normalizing per filter.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不再需要归一化我们的输入，因为它是按通道归一化的，或者对于后续层，它是按滤波器归一化的。
- en: 'Turns out this is not enough since SGD is bloody-minded [[01:29:20](https://youtu.be/H3g26EVADgY?t=1h29m20s)].
    If SGD decided that it wants matrix to be bigger/smaller overall, doing `(x=self.means)
    / self.stds` is not enough because SGD will undo it and try to do it again in
    the next mini-batch. So we will add two parameters: `a` — adder (initial value
    zeros) and `m` — multiplier (initial value ones) for each channel.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事实证明这还不够，因为SGD是固执的。如果SGD决定要使矩阵整体变大/变小，那么做`(x=self.means) / self.stds`是不够的，因为SGD会撤消它，并尝试在下一个小批次中再次执行。因此，我们将添加两个参数：`a`
    - 加法器（初始值为零）和`m` - 乘法器（初始值为1）用于每个通道。
- en: '`Parameter` tells PyTorch that it is allowed to learn these as weights.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Parameter`告诉PyTorch可以将这些作为权重进行学习。'
- en: 'Why does this work? If it wants to scale the layer up, it does not have to
    scale up every single value in the matrix. It can just scale up this single trio
    of numbers `self.m` , if it wants to shift it all up or down a bit, it does not
    have to shift the entire weight matrix, they can just shift this trio of numbers
    `self.a`. Intuition: We are normalizing the data and then we are saying you can
    then shift it and scale it using far fewer parameters than would have been necessary
    if it were to actually shift and scale the entire set of convolutional filters.
    In practice, it allows us to increase our learning rates, it increase the resilience
    of training, and it allows us to add more layers and still train effectively.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么这样做？如果要扩展该层，它不必扩展矩阵中的每个值。如果要将其全部上移或下移一点，它不必移动整个权重矩阵，它们只需移动这三个数字`self.m`。直觉：我们正在对数据进行归一化，然后我们说您可以使用远少于实际需要的参数来移动和缩放它，而不是移动和缩放整套卷积滤波器。在实践中，它允许我们增加学习速率，增加训练的弹性，并且允许我们添加更多层并仍然有效地进行训练。
- en: The other thing batch norm does is that it regularizes, in other words, you
    can often decrease or remove dropout or weight decay. The reason why is each mini-batch
    is going to have a different mean and a different standard deviation to the previous
    mini-batch. So they keep changing and it is changing the meaning of the filters
    in a subtle way acting as a noise (i.e. regularization).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量归一化的另一件事是正则化，换句话说，您通常可以减少或删除辍学或权重衰减。原因是每个小批次将具有不同的均值和不同的标准差与上一个小批次不同。因此它们不断变化，以微妙的方式改变滤波器的含义，起到噪声（即正则化）的作用。
- en: In real version, it does not use this batch’s mean and standard deviation but
    takes an exponentially weighted moving average standard deviation and mean.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在真实版本中，它不使用这个批次的均值和标准差，而是采用指数加权移动平均标准差和均值。
- en: '`**if** self.training` — this is important because when you are going through
    the validation set, you do not want to be changing the meaning of the model. There
    are some types of layer that are actually sensitive to what the mode of the network
    is whether it is in training mode or evaluation/test mode. There was a bug when
    we implemented mini net for MovieLens that dropout was applied during the validation
    — which was fixed. In PyTorch, there are two such layer: dropout and batch norm.
    `nn.Dropout` already does the check.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**if** self.training` - 这很重要，因为当您通过验证集时，您不希望更改模型的含义。有一些类型的层实际上对网络的模式敏感，无论它是处于训练模式还是评估/测试模式。当我们为MovieLens实现迷你网络时，存在一个错误，即在验证期间应用了辍学
    - 这已经得到修复。在PyTorch中，有两种这样的层：辍学和批量归一化。`nn.Dropout`已经进行了检查。'
- en: '[[01:37:01](https://youtu.be/H3g26EVADgY?t=1h37m1s)] The key difference in
    fast.ai which no other library does is that these means and standard deviations
    get updated in training mode in every other library as soon as you basically say
    I am training, regardless of whether that layer is set to trainable or not. With
    a pre-trained network, that is a terrible idea. If you have a pre-trained network
    for specific values of those means and standard deviations in batch norm, if you
    change them, it changes the meaning of those pre-trained layers. In fast.ai, always
    by default, it will not touch those means and standard deviations if your layer
    is frozen. As soon as you un-freeze it, it will start updating them unless you
    set `learn.bn_freeze=True`. In practice, this often seems to work a lot better
    for pre-trained models particularly if you are working with data that is quite
    similar to what the pre-trained model was trained with.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在fast.ai中的关键区别是，这些均值和标准差在训练模式下会得到更新，而在其他库中，只要您说“我在训练”，无论该层是否可训练，这些均值和标准差就会立即得到更新。对于预训练网络来说，这是一个糟糕的主意。如果您有一个针对批量归一化中这些均值和标准差的特定值进行预训练的网络，如果更改它们，就会改变这些预训练层的含义。在fast.ai中，默认情况下，如果您的层被冻结，它将不会触及这些均值和标准差。一旦您解冻它，它将开始更新它们，除非您设置`learn.bn_freeze=True`。实际上，这在处理与预训练模型非常相似的数据时似乎经常效果更好。
- en: Where do you put batch-norm layer? We will talk more in a moment, but for now,
    after `relu`
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您应该在哪里放置批量归一化层？我们稍后会详细讨论，但现在，在`relu`之后
- en: Ablation Study [[01:39:41](https://youtu.be/H3g26EVADgY?t=1h39m41s)]
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消融研究
- en: It is something where you try turning on and off different pieces of your model
    to see which bits make which impacts, and one of the things that wasn’t done in
    the original batch norm paper was any kind of effective ablation. And one of the
    things therefore that was missing was this question which was just asked — where
    to put the batch norm. That oversight caused a lot of problems because it turned
    out the original paper did not actually put it in the best spot. Other people
    since then have now figured that out and when Jeremy show people code where it
    is actually in the spot that is better, people say his batch norm is in the wrong
    spot.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个尝试打开和关闭模型不同部分以查看哪些部分产生哪些影响的过程，原始批量归一化论文中没有进行任何有效的消融。因此，缺失的一点是刚刚提出的这个问题——批量归一化放在哪里。这个疏忽导致了很多问题，因为原始论文实际上没有将其放在最佳位置。自那时以来，其他人已经弄清楚了这一点，当Jeremy向人们展示代码时，实际上放在更好位置的人们会说他的批量归一化放错了位置。
- en: Try and always use batch norm on every layer if you can
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽量在每一层上都使用批量归一化。
- en: Don’t stop normalizing your data so that people using your data will know how
    you normalized your data. Other libraries might not deal with batch norm for pre-trained
    models correctly, so when people start re-training, it might cause problems.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要停止对数据进行归一化，这样使用您的数据的人就会知道您是如何对数据进行归一化的。其他库可能无法正确处理预训练模型的批量归一化，因此当人们开始重新训练时可能会出现问题。
- en: '[PRE30]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Rest of the code is similar — Using `BnLayer` instead of `ConvLayer`
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码的其余部分类似——使用`BnLayer`而不是`ConvLayer`
- en: A single convolutional layer was added at the start trying to get closer to
    the modern approaches. It has a bigger kernel size and a stride of 1\. The basic
    idea is that we want the first layer to have a richer input. It does convolution
    using the 5 by 5 area which allows it to try and find more interesting richer
    features in that 5 by 5 area, then spit out bigger output (in this case, it’s
    10 by 5 by 5 filters). Typically it is 5 by 5 or 7 by 7, or even 11 by 11 convolution
    with quite a few filters coming out (e.g. 32 filters).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始时添加了一个单个卷积层，试图接近现代方法。它具有更大的内核大小和步幅为1。基本思想是我们希望第一层具有更丰富的输入。它使用5x5区域进行卷积，这使它可以尝试在该5x5区域中找到更有趣更丰富的特征，然后输出更大的输出（在这种情况下，是10x5x5个滤波器）。通常是5x5或7x7，甚至是11x11卷积，输出相当多的滤波器（例如32个滤波器）。
- en: Since `padding = kernel_size — 1 / 2` and `stride=1` , the input size is the
    same as the output size — just more filters.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于`padding = kernel_size — 1 / 2`和`stride=1`，输入大小与输出大小相同——只是有更多的滤波器。
- en: It is a good way of trying to create a richer starting point.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是尝试创建更丰富的起点的好方法。
- en: Deep BatchNorm [[01:50:52](https://youtu.be/H3g26EVADgY?t=1h50m52s)]
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度批量归一化
- en: Let’s increase the depth of the model. We cannot just add more of stride 2 layers
    since it halves the size of the image each time. Instead, after each stride 2
    layer, we insert a stride 1 layer.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们增加模型的深度。我们不能只添加更多的步幅为2的层，因为每次都会将图像的大小减半。相反，在每个步幅为2的层之后，我们插入一个步幅为1的层。
- en: '[PRE31]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The accuracy remained the same as before. This is now 12 layers deep, and it
    is too deep even for batch norm to handle. It is possible to train 12 layer deep
    conv net but it starts to get difficult. And it does not seem to be helping much
    if at all.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率与之前相同。现在深度为12层，即使对于批量归一化来说也太深了。可以训练12层深的卷积网络，但开始变得困难。而且似乎并没有太多帮助。
- en: ResNet [[01:52:43](https://youtu.be/H3g26EVADgY?t=1h52m43s)]
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ResNet
- en: '[PRE32]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`ResnetLayer` inherit from `BnLayer` and override `forward`.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ResnetLayer`继承自`BnLayer`并覆盖`forward`。'
- en: Then add bunch of layers and make it 3 times deeper, ad it still trains beautifully
    just because of `x + super().forward(x)` .
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后添加一堆层，使其深度增加3倍，仍然可以很好地训练，只是因为`x + super().forward(x)`。
- en: '[PRE33]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**ResNet block** [[01:53:18](https://youtu.be/H3g26EVADgY?t=1h53m18s)]'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet块
- en: '`**return** **x + super().forward(x)**`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`**return** **x + super().forward(x)**`'
- en: '*y = x + f(x)*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = x + f(x)*'
- en: Where *x* is prediction from the previous layer, *y* is prediction from the
    current layer.Shuffle around the formula and we get:formula shuffle
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*x*是来自上一层的预测，*y*是来自当前层的预测。重新排列公式，我们得到：公式重新排列
- en: '*f(x) = y − x*'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*f(x) = y − x*'
- en: The difference *y − x* is **residual**. The residual is the error in terms of
    what we have calculated so far. What this is saying is that try to find a set
    of convolutional weights that attempts to fill in the amount we were off by. So
    in other words, we have an input, and we have a function which tries to predict
    the error (i.e. how much we are off by). Then we add a prediction of how much
    we were wrong by to the input, then add another prediction of how much we were
    wrong by that time, and repeat that layer after layer — zooming into the correct
    answer. This is based on a theory called **boosting**.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 差异*y − x*是**残差**。残差是迄今为止我们计算的错误。这意味着尝试找到一组卷积权重，试图填补我们偏离的量。换句话说，我们有一个输入，我们有一个函数试图预测错误（即我们偏离的量）。然后我们将输入的错误预测量相加，然后再添加另一个错误预测量，然后重复这个过程，逐层放大到正确答案。这基于一种称为**boosting**的理论。
- en: The full ResNet does two convolutions before it gets added back to the original
    input (we did just one here).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的ResNet在将其添加回原始输入之前进行了两次卷积（我们这里只做了一次）。
- en: In every block `x = l3(l2(l(x)))` , one of the layers is not a `ResnetLayer`
    but a standard convolution with `stride=2` — this is called a “bottleneck layer”.
    ResNet does not convolutional layer but a different form of bottleneck block which
    we will cover in Part 2.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个块`x = l3(l2(l(x)))`中，其中一层不是`ResnetLayer`而是一个带有`stride=2`的标准卷积——这被称为“瓶颈层”。ResNet不是卷积层，而是我们将在第2部分中介绍的不同形式的瓶颈块。
- en: ResNet 2 [[01:59:33](https://youtu.be/H3g26EVADgY?t=1h59m33s)]
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ResNet 2 [[01:59:33](https://youtu.be/H3g26EVADgY?t=1h59m33s)]
- en: Here, we increased the size of features and added dropout.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们增加了特征的大小并添加了dropout。
- en: '[PRE34]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '85% was a state-of-the-art back in 2012 or 2013 for CIFAR 10\. Nowadays, it
    is up to 97% so there is a room for improvement but all based on these tecniques:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 85%是2012年或2013年CIFAR 10的最新技术。如今，它已经达到了97%，因此还有改进的空间，但所有都基于这些技术：
- en: Better approaches to data augmentation
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的数据增强方法
- en: Better approaches to regularization
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的正则化方法
- en: Some tweaks on ResNet
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对ResNet进行一些调整
- en: 'Question [[02:01:07](https://youtu.be/H3g26EVADgY?t=2h1m7s)]: Can we apply
    “training on the residual” approach for non-image problem? Yes! But it has been
    ignored everywhere else. In NLP, “transformer architecture” recently appeared
    and was shown to be the state of the art for translation, and it has a simple
    ResNet structure in it. This general approach is called “skip connection” (i.e.
    the idea of skipping over a layer) and appears a lot in computer vision, but nobody
    else much seems to be using it even through there is nothing computer vision specific
    about it. Good opportunity!'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 问题[[02:01:07](https://youtu.be/H3g26EVADgY?t=2h1m7s)]:我们可以将“训练残差”方法应用于非图像问题吗？是的！但是它已经被其他地方忽略了。在NLP中，“transformer架构”最近出现，并被证明是翻译的最新技术，并且其中有一个简单的ResNet结构。这种一般方法称为“跳过连接”（即跳过一层的想法），在计算机视觉中经常出现，但似乎没有其他人多使用，尽管它与计算机视觉无关。好机会！
- en: '[Dogs vs. Cats](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson7-CAM.ipynb)
    [[02:02:03](https://youtu.be/H3g26EVADgY?t=2h2m3s)]'
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[狗与猫](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson7-CAM.ipynb)
    [[02:02:03](https://youtu.be/H3g26EVADgY?t=2h2m3s)]'
- en: Going back dogs and cats. We will create resnet34 (if you are interested in
    what the trailing number means, [see here](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)
    — just different parameters).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 回到狗和猫。我们将创建resnet34（如果您对尾随数字的含义感兴趣，请参阅[这里](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)——只是不同的参数）。
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Our ResNet model had Relu → BatchNorm. TorchVision does BatchNorm →Relu. There
    are three different versions of ResNet floating around, and the best one is PreAct
    ([https://arxiv.org/pdf/1603.05027.pdf](https://arxiv.org/pdf/1603.05027.pdf)).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的ResNet模型具有Relu → BatchNorm。TorchVision使用BatchNorm → Relu。有三个不同版本的ResNet在流传，最好的是PreAct
    ([https://arxiv.org/pdf/1603.05027.pdf](https://arxiv.org/pdf/1603.05027.pdf))。
- en: Currently, the final layer has a thousands features because ImageNet has 1000
    features, so we need to get rid of it.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前，最后一层有数千个特征，因为ImageNet有1000个特征，所以我们需要摆脱它。
- en: When you use fast.ai’s `ConvLearner` , it deletes the last two layers for you.
    fast.ai replaces `AvgPool2d` with Adaptive Average Pooling and Adaptive Max Pooling
    and concatenate the two together.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您使用fast.ai的`ConvLearner`时，它会为您删除最后两层。fast.ai用自适应平均池化和自适应最大池化替换`AvgPool2d`，并将两者连接在一起。
- en: For this exercise, we will do a simple version.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于这个练习，我们将做一个简单版本。
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Remove the last two layers
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除最后两层
- en: Add a convolution which just has 2 outputs.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个只有2个输出的卷积。
- en: Do average pooling then softmax
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行平均池化然后进行softmax
- en: There is no linear layer at the end. This is a different way of producing just
    two numbers — which allows us to do CAM!
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后没有线性层。这是产生两个数字的不同方式——这使我们能够进行CAM！
- en: '[PRE37]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`ConvLearner.from_model` is what we learned about earlier — allows us to create
    a Learner object with custom model.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConvLearner.from_model`是我们之前学到的——允许我们使用自定义模型创建Learner对象。'
- en: Then freeze the layer except the ones we just added.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后冻结除了我们刚刚添加的层之外的所有层。
- en: Class Activation Maps (CAM) [[02:08:55](https://youtu.be/H3g26EVADgY?t=2h8m55s)]
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类激活图（CAM）[[02:08:55](https://youtu.be/H3g26EVADgY?t=2h8m55s)]
- en: We pick a specific image, and use a technique called CAM where we take a model
    and we ask it which parts of the image turned out to be important.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择一个特定的图像，并使用一种称为CAM的技术，询问模型哪些部分的图像被证明是重要的。
- en: 'How did it do this? Let’s work backwards. The way it did it was by producing
    this matrix:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何做到的？让我们逆向工作。它是通过生成这个矩阵来做到的：
- en: 'Big numbers correspond to the cat. So what is this matrix? This matrix simply
    equals to the value of feature matrix `feat` times `py` vector:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 大数字对应于猫。那么这个矩阵是什么？这个矩阵简单地等于特征矩阵`feat`乘以`py`向量的值：
- en: '[PRE38]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`py` vector is the predictions that says “I am 100% confident it’s a cat.”
    `feat` is the values (2×7×7) coming out of the final convolutional layer (the
    `Conv2d` layer we added). If we multiply `feat` by `py` , we get all of the first
    channel and none of the second channel. Therefore, it is going to return the value
    of the last convolutional layers for the section which lines up with being a cat.
    In other words, if we multiply `feat` by `[0, 1]` , it will line up with being
    a dog.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`py` 向量是预测，表示“我对这是一只猫有100%的信心”。`feat` 是最终卷积层（我们添加的`Conv2d`层）输出的值（2×7×7）。如果我们将`feat`乘以`py`，我们会得到所有第一个通道的值，而第二个通道的值为零。因此，它将返回与猫对应的部分的最后一个卷积层的值。换句话说，如果我们将`feat`乘以`[0,
    1]`，它将与狗对应。'
- en: '[PRE39]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Put it in another way, in the model, the only thing that happened after the
    convolutional layer was an average pooling layer. The average pooling layer took
    took the 7 by 7 grid and averaged out how much each part is “cat-like”. We then
    took the “cattyness” matrix, resized it to be the same size as the original cat
    image, and overlaid it on top, then you get the heat map.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在模型中，卷积层之后唯一发生的事情是平均池化层。平均池化层将7×7的网格平均化，计算出每个部分有多少“像猫”。然后，我们将“猫样”矩阵调整大小为与原始猫图像相同的大小，并叠加在顶部，然后你就得到了热图。
- en: The way you can use this technique at home is
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在家中使用这种技术的方法是：
- en: when you have a large image, you can calculate this matrix on a quick small
    little convolutional net
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您有一幅大图像时，您可以在一个快速小的卷积网络上计算这个矩阵。
- en: zoom into the area that has the highest value
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 放大具有最高值的区域
- en: re-run it just on that part
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅在该部分重新运行
- en: We skipped this over quickly as we ran out of time, but we will learn more about
    these kind of approaches in Part 2.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 由于时间不够，我们很快跳过了这部分，但我们将在第2部分中学习更多关于这种方法的内容。
- en: '“Hook” is the mechanism that lets us ask the model to return the matrix. `register_forward_hook`
    asks PyTorch that every time it calculates a layer it runs the function given
    — sort of like a callback that happens every time it calculates a layer. In the
    following case, it saves the value of the particular layer we were interested
    in:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: “Hook”是让我们要求模型返回矩阵的机制。`register_forward_hook`要求PyTorch每次计算一个层时运行给定的函数 - 类似于每次计算一个层时发生的回调。在以下情况下，它保存了我们感兴趣的特定层的值：
- en: '[PRE40]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Questions to Jeremy [[02:14:27](https://youtu.be/H3g26EVADgY?t=2h14m27s)]:
    “Your journey into Deep Learning” and “How to keep up with important research
    for practitioners”'
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Jeremy的问题[[02:14:27](https://youtu.be/H3g26EVADgY?t=2h14m27s)]：“您对深度学习的探索”和“如何跟上从业者的重要研究”
- en: '“If you intend to come to Part 2, you are expected to master all the techniques
    er have learned in Part 1”. Here are something you can do:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: “如果您打算参加第2部分，您应该掌握我们在第1部分学到的所有技术”。以下是您可以做的一些事情：
- en: Watch each of the video at least 3 times.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 至少观看每个视频3次。
- en: Make sure you can re-create the notebooks without watching the videos — maybe
    do so with different datasets to make it more interesting.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您可以重新创建笔记本而无需观看视频 - 可能使用不同的数据集来使其更有趣。
- en: Keep an eye on the forum for recent papers, recent advances.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 密切关注论坛上的最新论文和最新进展。
- en: Be tenacious and keep working at it!
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 坚持不懈，继续努力！
