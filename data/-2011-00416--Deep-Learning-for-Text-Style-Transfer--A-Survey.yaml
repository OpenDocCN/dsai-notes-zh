- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:58:44'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:58:44
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2011.00416] Deep Learning for Text Style Transfer: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2011.00416] 深度学习与文本风格转移：调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2011.00416](https://ar5iv.labs.arxiv.org/html/2011.00416)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2011.00416](https://ar5iv.labs.arxiv.org/html/2011.00416)
- en: \historydates
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \historydates
- en: 'Submission received: 25 April 2021, Revised version received: 30 August 2021,
    Accepted for publication: 4 December 2021. yy2021 \dochead'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 提交收到：2021年4月25日，修订版收到：2021年8月30日，接受出版：2021年12月4日。yy2021 \dochead
- en: 'Deep Learning for Text Style Transfer:'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习与文本风格转移：
- en: A Survey
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 调查
- en: Di Jin Equal contribution. MIT CSAIL
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Di Jin 同等贡献。MIT CSAIL
- en: jindi15@mit.edu    Zhijing Jin^* Max Planck Institute & ETH Zürich
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: jindi15@mit.edu    Zhijing Jin^* 马克斯·普朗克研究所 & ETH Zürich
- en: zjin@tue.mpg.de    Zhiting Hu UC San Diego
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: zjin@tue.mpg.de    Zhiting Hu UC San Diego
- en: zhh019@ucsd.edu    Olga Vechtomova University of Waterloo
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: zhh019@ucsd.edu    Olga Vechtomova 滑铁卢大学
- en: ovechtom@uwaterloo.ca    Rada Mihalcea University of Michigan
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ovechtom@uwaterloo.ca    Rada Mihalcea 密歇根大学
- en: mihalcea@umich.edu
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: mihalcea@umich.edu
- en: Abstract
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Text style transfer is an important task in natural language generation, which
    aims to control certain attributes in the generated text, such as politeness,
    emotion, humor, and many others. It has a long history in the field of natural
    language processing, and recently has re-gained significant attention thanks to
    the promising performance brought by deep neural models. In this paper, we present
    a systematic survey of the research on neural text style transfer, spanning over
    100 representative articles since the first neural text style transfer work in
    2017\. We discuss the task formulation, existing datasets and subtasks, evaluation,
    as well as the rich methodologies in the presence of parallel and non-parallel
    data. We also provide discussions on a variety of important topics regarding the
    future development of this task.¹¹1Our curated paper list is at [https://github.com/zhijing-jin/Text_Style_Transfer_Survey](https://github.com/zhijing-jin/Text_Style_Transfer_Survey).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 文本风格转移是自然语言生成中的一项重要任务，旨在控制生成文本中的某些属性，如礼貌、情感、幽默等。这在自然语言处理领域有着悠久的历史，最近由于深度神经模型带来的良好表现，再次获得了显著关注。在本文中，我们系统地综述了神经文本风格转移的研究，涵盖了自2017年首个神经文本风格转移工作以来的100多篇具有代表性的文章。我们讨论了任务的表述、现有数据集和子任务、评估以及在有并行和非并行数据情况下的丰富方法论。我们还提供了关于此任务未来发展的各种重要话题的讨论。¹¹1我们整理的论文列表在[https://github.com/zhijing-jin/Text_Style_Transfer_Survey](https://github.com/zhijing-jin/Text_Style_Transfer_Survey)。
- en: '^†^†issue: xx'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†问题：xx
- en: 1 Introduction
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Language is situational. Every utterance fits in a specific time, place, and
    scenario, conveys specific characteristics of the speaker, and typically has a
    well-defined intent. For example, someone who is uncertain is more likely to use
    tag questions (e.g., “This is true, isn’t it?”) than declarative sentences (e.g.,
    “This is definitely true.”). Similarly, a professional setting is more likely
    to include formal statements (e.g., “Please consider taking a seat.”) as compared
    to an informal situation (e.g., “Come and sit!”). For artificial intelligence
    systems to accurately understand and generate language, it is necessary to model
    language with style/attribute,²²2Note that we interchangeably use the terms style
    and attribute in this survey. Attribute is a broader terminology that can include
    content preferences, e.g., sentiment, topic, and so on. This survey uses style
    in the same broad way, following the common practice in recent papers (see Section [2.1](#S2.SS1
    "2.1 How to Define Style? ‣ 2 What Is Text Style Transfer? ‣ Deep Learning for
    Text Style Transfer: A Survey")). which goes beyond merely verbalizing the semantics
    in a non-stylized way. The values of the attributes can be drawn from a wide range
    of choices depending on pragmatics, such as the extent of formality, politeness,
    simplicity, personality, emotion, partner effect (e.g., reader awareness), genre
    of writing (e.g., fiction or non-fiction), and so on.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 语言是情境性的。每一句话都适用于特定的时间、地点和情境，传达说话者的特定特征，并通常具有明确的意图。例如，与陈述句（如“这是肯定的。”）相比，表达不确定的人更可能使用附加疑问句（如“这是真的，不是吗？”）。类似地，专业场合更可能包括正式的陈述（如“请考虑坐下。”），而非正式场合则更多使用非正式的表达（如“来坐下吧！”）。为了使人工智能系统能够准确理解和生成语言，有必要对语言进行风格/属性建模²²2注意，我们在本调查中交替使用风格和属性这两个术语。属性是一个更广泛的术语，可以包括内容偏好，例如情感、话题等。本调查以相同的广泛方式使用风格，遵循近期论文中的常见做法（见第[2.1节](#S2.SS1
    "2.1 如何定义风格？ ‣ 2 文本风格迁移是什么？ ‣ 深度学习文本风格迁移综述")），这超越了仅仅以非风格化的方式表达语义。属性的值可以根据语用学从各种选择中提取，例如正式程度、礼貌、简洁、个性、情感、合作伙伴效应（如读者意识）、写作体裁（如虚构或非虚构）等。
- en: The goal of text style transfer (TST) is to automatically control the style
    attributes of text while preserving the content. TST has a wide range of applications,
    as outlined by McDonald and Pustejovsky ([1985](#bib.bib127)) and Hovy ([1987](#bib.bib69)).
    The style of language is crucial because it makes natural language processing
    more user-centered. TST has many immediate applications. For instance, one such
    application is intelligent bots for which users prefer distinct and consistent
    persona (e.g., empathetic) instead of emotionless or inconsistent persona. Another
    application is the development of intelligent writing assistants; for example,
    non-expert writers often need to polish their writings to better fit their purpose,
    e.g., more professional, polite, objective, humorous, or other advanced writing
    requirements, which may take years of experience to master. Other applications
    include automatic text simplification (where the target style is “simple”), debiasing
    online text (where the target style is “objective”), fighting against offensive
    language (where the target style is “non-offensive”), and so on.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 文本风格迁移（TST）的目标是自动控制文本的风格属性，同时保持内容。TST有广泛的应用，如McDonald和Pustejovsky ([1985](#bib.bib127))
    以及Hovy ([1987](#bib.bib69)) 所述。语言风格至关重要，因为它使自然语言处理更加以用户为中心。TST有许多直接应用。例如，一个应用是智能聊天机器人，其中用户更喜欢具有独特和一致性的人格（如富有同情心的），而不是没有情感或不一致的人格。另一个应用是开发智能写作助手；例如，非专业的作者通常需要润色他们的写作，以更好地符合他们的目的，例如更专业、礼貌、客观、幽默或其他高级写作要求，这可能需要多年的经验来掌握。其他应用包括自动文本简化（目标风格为“简单”）、去偏见在线文本（目标风格为“客观”）、对抗冒犯性语言（目标风格为“非冒犯性”）等。
- en: 'To formally define text style transfer, let us denote the target utterance
    as $\bm{x}^{\prime}$ and the target discourse style attribute as $a^{\prime}$.
    TST aims to model $p(\bm{x}^{\prime}|a,\bm{x})$, where $\bm{x}$ is a given text
    carrying a source attribute value $a$. Consider the previous example of text expressed
    by two different extents of formality:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正式定义文本风格迁移，我们用$\bm{x}^{\prime}$表示目标话语，以及$a^{\prime}$表示目标话语风格属性。TST旨在建模$p(\bm{x}^{\prime}|a,\bm{x})$，其中$\bm{x}$是携带源属性值$a$的给定文本。考虑之前的例子，即以两种不同正式程度表达的文本：
- en: '| Source sentence $\bm{x}$: | “Come and sit!” | Source attribute $a$: | Informal
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 源句子 $\bm{x}$: | “Come and sit!” | 源属性 $a$: | 非正式 |'
- en: '| Target sentence $\bm{x}^{\prime}$: | “Please consider taking a seat.” | Target
    attribute $a^{\prime}$: | Formal |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 目标句子 $\bm{x}^{\prime}$: | “Please consider taking a seat.” | 目标属性 $a^{\prime}$:
    | 正式 |'
- en: In this case, a TST model should be able to modify the formality and generate
    the formal sentence $\bm{x}^{\prime}=$“Please consider taking a seat.” given the
    informal input $\bm{x}=$“Come and sit!”. Note that the key difference of TST from
    another NLP task, style-conditioned language modeling, is that the latter is conditioned
    on only a style token, whereas TST takes as input both the target style attribute
    $a^{\prime}$ and a source sentence $\bm{x}$ that constrains the content.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个TST模型应该能够修改正式度，并根据非正式输入 $\bm{x}=$“Come and sit!” 生成正式句子 $\bm{x}^{\prime}=$“Please
    consider taking a seat.”。请注意，TST与另一种NLP任务风格条件语言建模的关键区别在于，后者仅以风格标记为条件，而TST的输入包括目标风格属性
    $a^{\prime}$ 和约束内容的源句子 $\bm{x}$。
- en: Crucial to the definition of style transfer is the distinction of “style” and
    “content,” for which there are two common practices. The first one is by linguistic
    definition, where non-functional linguistic features are classified into the style
    (e.g., formality), and the semantics are classified into the content. In contrast,
    the second practice is data-driven, – given two corpora (e.g., a positive review
    set and a negative review set), the invariance between the two corpora is the
    content, whereas the variance is the style (e.g., sentiment, topic) Mou and Vechtomova
    ([2020](#bib.bib131)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 风格转移定义的关键在于“风格”和“内容”的区分，这里有两种常见做法。第一种是通过语言学定义，将非功能性语言特征分类为风格（例如，正式度），将语义分类为内容。相反，第二种做法是数据驱动的——给定两个语料库（例如，积极评价集和负面评价集），两个语料库之间的不变性是内容，而变异性是风格（例如，情感、主题）
    Mou 和 Vechtomova ([2020](#bib.bib131))。
- en: Driven by the growing needs for TST, active research in this field has emerged,
    from the traditional linguistic approaches, to the more recent neural network-based
    approaches. Traditional approaches rely on term replacement and templates. For
    example, early work in NLG for weather forecasts builds domain-specific templates
    to express different types of weather with different levels of uncertainty for
    different users Sripada et al. ([2004](#bib.bib187)); Reiter et al. ([2005](#bib.bib160));
    Belz ([2008](#bib.bib11)); Gkatzia, Lemon, and Rieser ([2017](#bib.bib49)). Research
    that more explicitly focuses on TST starts from the frame language-based systems
    McDonald and Pustejovsky ([1985](#bib.bib127)), and schema-based NLG systems Hovy
    ([1987](#bib.bib69), [1990](#bib.bib70)) which generate text with pragmatic constraints
    such as formality under small-scale well-defined schema. Most of this earlier
    work required domain-specific templates, hand-featured phrase sets that express
    a certain attribute (e.g., friendly), and sometimes a look-up table of expressions
    with the same meaning but multiple different attributes Bateman and Paris ([1989](#bib.bib9));
    Stamatatos et al. ([1997](#bib.bib188)); Power, Scott, and Bouayad-Agha ([2003](#bib.bib148));
    Reiter, Robertson, and Osman ([2003](#bib.bib159)); Sheikha and Inkpen ([2011](#bib.bib181));
    Mairesse and Walker ([2011](#bib.bib121)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对TST的需求不断增长，活跃的研究已经从传统的语言学方法转向近年来的神经网络方法。传统方法依赖于术语替换和模板。例如，早期的NLG工作在天气预报中构建了领域特定的模板，以不同的方式表达不同类型的天气，并针对不同用户的不确定性程度
    Sripada 等 ([2004](#bib.bib187)); Reiter 等 ([2005](#bib.bib160)); Belz ([2008](#bib.bib11));
    Gkatzia, Lemon, 和 Rieser ([2017](#bib.bib49))。更明确地关注TST的研究从基于框架语言的系统 McDonald
    和 Pustejovsky ([1985](#bib.bib127)) 开始，以及基于模式的NLG系统 Hovy ([1987](#bib.bib69),
    [1990](#bib.bib70))，这些系统在小规模明确的模式下生成具有语用约束（如正式度）的文本。这些早期工作大多需要领域特定的模板、表达某种属性的手工特征短语集（例如，友好），有时还需要一个包含相同意义但具有不同属性的表达的查找表
    Bateman 和 Paris ([1989](#bib.bib9)); Stamatatos 等 ([1997](#bib.bib188)); Power,
    Scott 和 Bouayad-Agha ([2003](#bib.bib148)); Reiter, Robertson 和 Osman ([2003](#bib.bib159));
    Sheikha 和 Inkpen ([2011](#bib.bib181)); Mairesse 和 Walker ([2011](#bib.bib121))。
- en: 'With the success of deep learning in the last decade, a variety of neural methods
    have been recently proposed for TST. If parallel data are provided, standard sequence-to-sequence
    models are often directly applied Rao and Tetreault ([2018](#bib.bib156)) (see
    Section [4](#S4 "4 Methods on Parallel Data ‣ Deep Learning for Text Style Transfer:
    A Survey")). However, most use cases do not have parallel data, so TST on non-parallel
    corpora has become a prolific research area (see Section [5](#S5 "5 Methods on
    Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey")). The first
    line of approaches disentangle text into its content and attribute in the latent
    space, and apply generative modeling Hu et al. ([2017](#bib.bib72)); Shen et al.
    ([2017](#bib.bib182)). This trend was then joined by another distinctive line
    of approach, prototype editing Li et al. ([2018](#bib.bib107)) which extracts
    a sentence template and its attribute markers to generate the text. Another paradigm
    soon followed, i.e., pseudo-parallel corpus construction to train the model as
    if in a supervised way with the pseudo-parallel data Zhang et al. ([2018d](#bib.bib234));
    Jin et al. ([2019](#bib.bib82)). These three directions, (1) disentanglement,
    (2) prototype editing, and (3) pseudo-parallel corpus construction, are further
    advanced with the emergence of Transformer-based models Sudhakar, Upadhyay, and
    Maheswaran ([2019](#bib.bib190)); Malmi, Severyn, and Rothe ([2020a](#bib.bib122)).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '随着深度学习在过去十年的成功，最近提出了多种神经方法用于TST。如果提供了平行数据，标准的序列到序列模型通常会被直接应用 Rao 和 Tetreault
    ([2018](#bib.bib156))（见第[4](#S4 "4 Methods on Parallel Data ‣ Deep Learning for
    Text Style Transfer: A Survey")节）。然而，大多数使用场景没有平行数据，因此非平行语料上的TST已成为一个丰硕的研究领域（见第[5](#S5
    "5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey")节）。第一类方法将文本在潜在空间中解构为内容和属性，并应用生成建模
    Hu 等 ([2017](#bib.bib72))；Shen 等 ([2017](#bib.bib182))。随后，另一类独特的方法应运而生，即原型编辑 Li
    等 ([2018](#bib.bib107))，它提取句子模板及其属性标记来生成文本。另一种范式紧随其后，即伪平行语料库构建，通过伪平行数据训练模型，仿佛是在监督的方式下
    Zhang 等 ([2018d](#bib.bib234))；Jin 等 ([2019](#bib.bib82))。这三种方向，（1）解构，（2）原型编辑，和（3）伪平行语料库构建，随着Transformer模型的出现进一步发展
    Sudhakar、Upadhyay 和 Maheswaran ([2019](#bib.bib190))；Malmi、Severyn 和 Rothe ([2020a](#bib.bib122))。'
- en: Given the advances in TST methodologies, it now starts to expand its impact
    to downstream applications, such as persona-based dialog generation Niu and Bansal
    ([2018](#bib.bib135)); Huang et al. ([2018](#bib.bib73)), stylistic summarization
    Jin et al. ([2020a](#bib.bib80)), stylized language modeling to imitate specific
    authors Syed et al. ([2020](#bib.bib192)), online text debiasing Pryzant et al.
    ([2020](#bib.bib150)); Ma et al. ([2020](#bib.bib118)), simile generation Chakrabarty,
    Muresan, and Peng ([2020](#bib.bib28)), and many others.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于TST方法学的进展，它现在开始扩展其对下游应用的影响，例如基于人物的对话生成 Niu 和 Bansal ([2018](#bib.bib135))；Huang
    等 ([2018](#bib.bib73))，风格化摘要 Jin 等 ([2020a](#bib.bib80))，模仿特定作者的风格化语言建模 Syed 等
    ([2020](#bib.bib192))，在线文本去偏见 Pryzant 等 ([2020](#bib.bib150))；Ma 等 ([2020](#bib.bib118))，比喻生成
    Chakrabarty、Muresan 和 Peng ([2020](#bib.bib28))，以及其他许多应用。
- en: 'Table 1: Overview of the survey.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：调查概览。
- en: '| Motivation | Data | Method | Extended Applications |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 动机 | 数据 | 方法 | 扩展应用 |'
- en: '| • Artistic writing • Communication • Mitigating social issues | Tasks • Formality
    • Politeness • Gender • Humor • Romance • Biasedness Key Properties • Parallel
    vs. non-parallel • Uni- vs. bi-directional • Dataset size • Large vs. small word
    overlap | • Toxicity • Authorship • Simplicity • Sentiment • Topic • Political
    slant | On Parallel Data • Multi-tasking • Inference techniques • Data augmentation
    On Non-Parallel Data • Disentanglement • Prototype editing • Pseudo data construction
    | Helping Other NLP Tasks • Paraphrasing • Data augmentation • Adversarial robustness
    • Persona-consistent dialog • Anonymization • Summarization • Style-specific MT
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| • 艺术写作 • 交流 • 缓解社会问题 | 任务 • 正式性 • 礼貌 • 性别 • 幽默 • 浪漫 • 偏见 主要特性 • 平行与非平行 •
    单向与双向 • 数据集规模 • 大词汇重叠与小词汇重叠 | • 毒性 • 作者身份 • 简单性 • 情感 • 话题 • 政治倾向 | 平行数据上的 • 多任务
    • 推理技术 • 数据增强 非平行数据上的 • 解构 • 原型编辑 • 伪数据构建 | 帮助其他NLP任务 • 释义 • 数据增强 • 对抗鲁棒性 • 人物一致对话
    • 匿名化 • 摘要 • 特定风格的机器翻译 |'
- en: 1.0.0.0.1 Motivation of a Survey on TST.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 1.0.0.0.1 TST调查的动机。
- en: 'The increasing interest in modeling the style of text can be regarded as a
    trend reflecting the fact that NLP researchers start to focus more on user-centeredness
    and personalization. However, despite the growing interest in TST, the existing
    literature shows a large diversity in the selection of benchmark datasets, methodological
    frameworks, and evaluation metrics. Thus, the aim of this survey is to provide
    summaries and potential standardizations on some important aspects of TST, such
    as the terminology, problem definition, benchmark datasets, and evaluation metrics.
    We also aim to provide different perspectives on the methodology of TST, and suggest
    some potential cross-cutting research questions for our proposed research agenda
    of the field. As shown in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Deep Learning
    for Text Style Transfer: A Survey"), the key contributions targeted by this survey
    are as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '对文本风格建模日益增长的兴趣可以视为一种趋势，反映了NLP研究人员开始更加关注用户中心和个性化。然而，尽管对TST的兴趣不断增加，现有文献在基准数据集、方法框架和评价指标的选择上表现出很大的多样性。因此，本调查的目的是对TST的一些重要方面（如术语、问题定义、基准数据集和评价指标）提供总结和潜在的标准化。我们还旨在提供关于TST方法论的不同视角，并提出一些潜在的跨领域研究问题，以供我们提出的研究议程使用。如表[1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ Deep Learning for Text Style Transfer: A Survey")所示，本调查的关键贡献如下：'
- en: '1.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We conduct the first comprehensive review that covers most existing works (more
    than 100 papers) on deep learning-based TST.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进行了一项首次全面的回顾，涵盖了大多数现有的基于深度学习的TST工作（超过100篇论文）。
- en: '2.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'We provide an overview of the task setting, terminology definition, benchmark
    datasets (Section [2](#S2 "2 What Is Text Style Transfer? ‣ Deep Learning for
    Text Style Transfer: A Survey")), and evaluation metrics for which we proposed
    standard practices that can be helpful for future works (Section [3](#S3 "3 How
    to Evaluate Style Transfer? ‣ Deep Learning for Text Style Transfer: A Survey")).'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们提供了任务设置、术语定义、基准数据集（第[2](#S2 "2 What Is Text Style Transfer? ‣ Deep Learning
    for Text Style Transfer: A Survey")节）和评价指标的概述，我们提出了可以对未来工作有帮助的标准化实践（第[3](#S3 "3
    How to Evaluate Style Transfer? ‣ Deep Learning for Text Style Transfer: A Survey")节）。'
- en: '3.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'We categorize the existing approaches on parallel data (Section [4](#S4 "4
    Methods on Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey"))
    and non-parallel data (Section [5](#S5 "5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey")) for which we distill some unified
    methodological frameworks.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们对现有的平行数据（第[4](#S4 "4 Methods on Parallel Data ‣ Deep Learning for Text Style
    Transfer: A Survey")节）和非平行数据（第[5](#S5 "5 Methods on Non-Parallel Data ‣ Deep Learning
    for Text Style Transfer: A Survey")节）的方法进行了分类，并提炼出一些统一的方法框架。'
- en: '4.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'We discuss a potential research agenda for TST (Section [6](#S6 "6 Research
    Agenda ‣ Deep Learning for Text Style Transfer: A Survey")), including expanding
    the scope of styles, improving the methodology, loosening dataset assumptions,
    and improving evaluation metrics.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们讨论了TST的潜在研究议程（第[6](#S6 "6 Research Agenda ‣ Deep Learning for Text Style
    Transfer: A Survey")节），包括扩大风格范围、改进方法论、放宽数据集假设以及改进评价指标。'
- en: '5.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'We provide a vision for how to broaden the impact of TST (Section [7](#S7 "7
    Expanding the Impact of TST ‣ Deep Learning for Text Style Transfer: A Survey")),
    including connecting to more NLP tasks, and more specialized downstream applications,
    as well as considering some important ethical impacts.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们提供了如何扩大TST影响的愿景（第[7](#S7 "7 Expanding the Impact of TST ‣ Deep Learning for
    Text Style Transfer: A Survey")节），包括与更多NLP任务和更专业的下游应用连接，以及考虑一些重要的伦理影响。'
- en: 1.0.0.0.2 Paper Selection.
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 1.0.0.0.2 论文选择。
- en: The neural TST papers reviewed in this survey are mainly from top conferences
    in NLP and artificial intelligence (AI), including ACL, EMNLP, NAACL, COLING,
    CoNLL, NeurIPS, ICML, ICLR, AAAI, and IJCAI. Other than conference papers, we
    also include some non-peer-reviewed preprint papers that can offer some insightful
    information about the field. The major factors for selecting non-peer-reviewed
    preprint papers include novelty and completeness, among others.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查中回顾的神经TST论文主要来自NLP和人工智能（AI）领域的顶级会议，包括ACL、EMNLP、NAACL、COLING、CoNLL、NeurIPS、ICML、ICLR、AAAI和IJCAI。除了会议论文外，我们还包括了一些未经过同行评审的预印本论文，这些论文可以提供一些对该领域有见解的信息。选择未经过同行评审的预印本论文的主要因素包括新颖性和完整性等。
- en: 2 What Is Text Style Transfer?
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 什么是文本风格迁移？
- en: 'This section provides an overview of the style transfer task. Section [2.1](#S2.SS1
    "2.1 How to Define Style? ‣ 2 What Is Text Style Transfer? ‣ Deep Learning for
    Text Style Transfer: A Survey") goes through the definition of styles and the
    scope of this survey. Section [2.2](#S2.SS2 "2.2 Task Formulation ‣ 2 What Is
    Text Style Transfer? ‣ Deep Learning for Text Style Transfer: A Survey") gives
    a task formulation and introduces the notations that will be used across the survey.
    Finally, Section [2.3](#S2.SS3 "2.3 Existing Subtasks with Datasets ‣ 2 What Is
    Text Style Transfer? ‣ Deep Learning for Text Style Transfer: A Survey") lists
    all the common subtasks for neural text style transfer which can save the literature
    review efforts for future researchers.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了风格转换任务。第[2.1](#S2.SS1 "2.1 如何定义风格？ ‣ 2 什么是文本风格转换？ ‣ 深度学习用于文本风格转换：综述")节讨论了风格的定义和本次调查的范围。第[2.2](#S2.SS2
    "2.2 任务表述 ‣ 2 什么是文本风格转换？ ‣ 深度学习用于文本风格转换：综述")节给出了任务表述，并介绍了本次调查中将使用的符号。最后，第[2.3](#S2.SS3
    "2.3 现有子任务及数据集 ‣ 2 什么是文本风格转换？ ‣ 深度学习用于文本风格转换：综述")节列出了所有常见的神经文本风格转换子任务，这可以节省未来研究者的文献综述工作。
- en: 2.1 How to Define Style?
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 如何定义风格？
- en: 2.1.0.0.1 Linguistic Definition of Style.
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.1.0.0.1 风格的语言学定义。
- en: An intuitive notion of style refers to the manner in which the semantics is
    expressed McDonald and Pustejovsky ([1985](#bib.bib127)). Just as everyone has
    their own signatures, style originates as the characteristics inherent to every
    person’s utterance, which can be expressed through the use of certain stylistic
    devices such as metaphors, as well as choice of words, syntactic structures, and
    so on. Style can also go beyond the sentence level to the discourse level, such
    as the stylistic structure of the entire piece of the work, e.g., stream of consciousness,
    or flashbacks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 风格的直观概念指的是语义表达的方式（McDonald 和 Pustejovsky [1985](#bib.bib127)）。就像每个人都有自己的签名一样，风格起源于每个人话语中固有的特征，这些特征可以通过使用某些修辞手法（如隐喻）、词汇选择、句法结构等来表达。风格还可以超越句子层面，达到话语层面，比如整个作品的风格结构，例如意识流或闪回。
- en: Beyond the intrinsic personal styles, for pragmatic uses, style further becomes
    a protocol to regularize the manner of communication. For example, for academic
    writing, the protocol requires formality and professionalism. Hovy ([1987](#bib.bib69))
    defines style by its pragmatic aspects, including both personal (e.g., personality,
    gender) and interpersonal (e.g., humor, romance) aspects. Most existing literature
    also takes these well-defined categories of styles.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了固有的个人风格外，在实用场景中，风格进一步成为规范沟通方式的协议。例如，对于学术写作，规范要求正式性和专业性。Hovy ([1987](#bib.bib69))
    从实用的角度定义风格，包括个人（如，个性、性别）和人际（如，幽默、浪漫）方面。现有文献大多也采用这些明确的风格类别。
- en: '![Refer to caption](img/302b20cf72aabaa1b6d63c604cf0e78e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/302b20cf72aabaa1b6d63c604cf0e78e.png)'
- en: 'Figure 1: Venn diagram of the linguistic definition of style and data-driven
    definition of style.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：风格的语言学定义与数据驱动的风格定义的维恩图。
- en: 2.1.0.0.2 Data-Driven Definition of Style as the Scope of this Survey.
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.1.0.0.2 数据驱动的风格定义作为本次调查的范围。
- en: 'This survey aims to provide an overview of existing neural text style transfer
    approaches. To be concise, we will limit the scope to the most common settings
    of existing literature. Specifically, most deep learning work on TST adopts a
    data-driven definition of style, and the scope of this survey covers the styles
    in currently available TST datasets. The data-driven definition of style is different
    from the linguistic or rule-based definition of style, which theoretically constrains
    what constitutes a style and what not, such as a style guide (e.g., American Psychological
    Association, [1983](#bib.bib1)) that requires that formal text not include any
    contraction, e.g., “isn’t.” The distinction of the two defintions of style is
    shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.1.0.0.1 Linguistic Definition of Style.
    ‣ 2.1 How to Define Style? ‣ 2 What Is Text Style Transfer? ‣ Deep Learning for
    Text Style Transfer: A Survey").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查旨在提供现有神经文本风格转换方法的概述。为了简明，我们将范围限制在现有文献中最常见的设置上。具体而言，大多数关于TST的深度学习工作采用了数据驱动的风格定义，本调查的范围涵盖了当前可用TST数据集中的风格。数据驱动的风格定义不同于语言学或规则基础的风格定义，后者理论上限制了什么构成风格，什么不构成风格，例如风格指南（例如，美国心理学会，[1983](#bib.bib1)）要求正式文本中不包含任何缩写词，如“isn’t”。这两种风格定义的区别见图[1](#S2.F1
    "图1 ‣ 2.1.0.0.1 语言学风格定义 ‣ 2.1 如何定义风格？ ‣ 2 什么是文本风格转换？ ‣ 深度学习文本风格转换：综述")。
- en: With the rise of deep learning methods of TST, the data-driven definition of
    style extends the linguistic style to a broader concept – the general attributes
    in text. It regards “style” as the attributes that vary across datasets, as opposed
    to the characteristics that stay invariant Mou and Vechtomova ([2020](#bib.bib131)).
    The reason is that deep learning models, which are the focus of this survey, need
    large corpora to learn the style from, but not all styles have well-matched large
    corpora. Therefore, apart from the very few manually-annotated datasets with linguistic
    style definitions, such as formality Rao and Tetreault ([2018](#bib.bib156)) and
    humor & romance Gan et al. ([2017](#bib.bib42)), many recent dataset collection
    works automatically look for meta-information to link a corpus to a certain attribute.
    A typical example is the widely used Yelp review dataset Shen et al. ([2017](#bib.bib182)),
    where reviews with low ratings are put into the negative corpus, and reviews with
    high ratings are put into the positive corpus, although the negative vs. positive
    opinion is not a style that belongs to the linguistic definition, but more of
    a content-related attribute.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 随着TST深度学习方法的兴起，数据驱动的风格定义将语言风格扩展到更广泛的概念——文本中的一般属性。它将“风格”视为在数据集之间变化的属性，而不是保持不变的特征Mou和Vechtomova
    ([2020](#bib.bib131))。原因在于，深度学习模型（本调查的重点）需要大量语料库来学习风格，但并非所有风格都有匹配的语料库。因此，除了极少数手动标注的具有语言风格定义的数据集，如正式性Rao和Tetreault
    ([2018](#bib.bib156))和幽默与浪漫Gan等人 ([2017](#bib.bib42))，许多最近的数据集收集工作自动寻找元信息以将语料库与某一属性关联。一个典型的例子是广泛使用的Yelp评论数据集Shen等人
    ([2017](#bib.bib182))，其中低评分的评论被放入负面语料库，而高评分的评论被放入正面语料库，尽管负面与正面意见并不是属于语言学定义中的风格，而更像是与内容相关的属性。
- en: 'Most methods mentioned in this survey can be applied to scenarios that follow
    this data-driven definition of style. As a double-sided sword, the prerequisite
    for most methods is that there exist style-specific corpora for each style of
    interest, either parallel or non-parallel. Note that there can be future works
    that do not take such an assumption, which will be discussed in Section [6.3](#S6.SS3
    "6.3 Loosening the Style-Specific Dataset Assumptions ‣ 6 Research Agenda ‣ Deep
    Learning for Text Style Transfer: A Survey").'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查中提到的大多数方法可以应用于遵循这种数据驱动风格定义的场景。作为一把双刃剑，大多数方法的前提条件是每种感兴趣的风格都有特定风格的语料库，无论是平行还是非平行。请注意，未来可能会有不做这种假设的研究工作，这将在第[6.3](#S6.SS3
    "6.3 放松对特定风格数据集的假设 ‣ 6 研究议程 ‣ 深度学习文本风格转换：综述")节中讨论。
- en: 2.1.0.0.3 Comparison of the Two Definitions.
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.1.0.0.3 两种定义的比较。
- en: There are two phenomena rising from the data-driven definition of style as opposed
    to the linguistic style. One is that the data-driven definition of style can include
    a broader range of attributes including content and topic preferences of the text.
    The other is that data-driven styles, if collected through automatic classification
    by meta-information such as ratings, user information, and source of text, can
    be more ambiguous than the linguistically defined styles. As shown in Jin et al.
    ([2019](#bib.bib82), Section 4.1.1), some automatically collected datasets have
    a concerningly high undecideable rate and inter-annotator disagreement rate when
    the annotators are asked to associate the dataset with human-defined styles such
    as political slant and gender-specific tones.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动的风格定义与语言学风格相比，有两个现象。一是数据驱动的风格定义可以包括更广泛的属性，包括文本的内容和主题偏好。另一点是，如果数据驱动的风格是通过自动分类，如评分、用户信息和文本来源等元信息收集的，则可能比语言学定义的风格更模糊。正如Jin等人（[2019](#bib.bib82)，第4.1.1节）所示，当要求标注者将数据集与人类定义的风格（如政治倾向和性别特定的语气）关联时，一些自动收集的数据集具有令人担忧的高不可决率和标注者间分歧率。
- en: 'The advantage of the data-driven style is that it can marry well with deep
    learning methods because most neural models learn the concept of style by learning
    to distinguish the multiple style corpora. For the (non-data-driven) linguistic
    style, although it is under-explored in the existing deep learning works of TST,
    we provide in Section [6.3](#S6.SS3 "6.3 Loosening the Style-Specific Dataset
    Assumptions ‣ 6 Research Agenda ‣ Deep Learning for Text Style Transfer: A Survey")
    a discussion of how potential future works can learn TST of linguistics styles
    with no matched data.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动风格的优势在于它可以很好地与深度学习方法结合，因为大多数神经模型通过学习区分多个风格语料库来学习风格的概念。对于（非数据驱动的）语言学风格，尽管在现有的TST深度学习工作中尚未深入探讨，我们在第[6.3](#S6.SS3
    "6.3 放宽风格特定数据集假设 ‣ 6 研究议程 ‣ 深度学习文本风格转换综述")节中讨论了潜在未来工作如何在没有匹配数据的情况下学习语言学风格的TST。
- en: 2.2 Task Formulation
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 任务定义
- en: 'We define the main notations used in this survey in Table [2](#S2.T2 "Table
    2 ‣ 2.2 Task Formulation ‣ 2 What Is Text Style Transfer? ‣ Deep Learning for
    Text Style Transfer: A Survey").'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[2](#S2.T2 "表 2 ‣ 2.2 任务定义 ‣ 2 文本风格转换是什么？ ‣ 深度学习文本风格转换综述")中定义了本调查中使用的主要符号。
- en: 'Table 2: Notation of each variable and its corresponding meaning.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：每个变量的符号及其对应的意义。
- en: '| Category | Notation | Meaning |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 符号 | 意义 |'
- en: '| Attribute | $a$ | An attribute value, e.g., the formal style |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | $a$ | 属性值，例如，正式风格 |'
- en: '| ${a}^{\prime}$ | An attribute value different from $a$ |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| ${a}^{\prime}$ | 与$a$不同的属性值 |'
- en: '| $\mathbb{A}$ | A predefined set of attribute values |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbb{A}$ | 预定义的属性值集合 |'
- en: '| $a_{i}$ | The $i$-th attribute value in $\mathbb{A}$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| $a_{i}$ | $\mathbb{A}$中的第$i$个属性值 |'
- en: '| Sentence | $\bm{x}$ | A sentence with attribute value $a$ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 句子 | $\bm{x}$ | 带有属性值$a$的句子 |'
- en: '| $\bm{x}^{\prime}$ | A sentence with attribute value ${a}^{\prime}$ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| $\bm{x}^{\prime}$ | 带有属性值${a}^{\prime}$的句子 |'
- en: '| $\bm{X}_{i}$ | A corpus of sentences with attribute value $a_{i}$ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| $\bm{X}_{i}$ | 具有属性值$a_{i}$的句子语料库 |'
- en: '| $\bm{x}_{i}$ | A sentence from the corpus $\bm{X}_{i}$ |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| $\bm{x}_{i}$ | 语料库$\bm{X}_{i}$中的一个句子 |'
- en: '| $\widehat{\bm{x}^{\prime}}$ | Attribute-transferred sentence of $\bm{x}$
    learned by the model |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| $\widehat{\bm{x}^{\prime}}$ | 模型学习的$\bm{x}$的属性转移句子 |'
- en: '| Model | $E$ | Encoder of a TST model |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | $E$ | TST模型的编码器 |'
- en: '| $G$ | Generator of a TST model |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| $G$ | TST模型的生成器 |'
- en: '| $f_{c}$ | Attribute classifier |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| $f_{c}$ | 属性分类器 |'
- en: '| $\bm{\theta}_{\mathrm{E}}$ | Parameters of the encoder |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| $\bm{\theta}_{\mathrm{E}}$ | 编码器的参数 |'
- en: '| $\bm{\theta}_{\mathrm{G}}$ | Parameters of the generator |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| $\bm{\theta}_{\mathrm{G}}$ | 生成器的参数 |'
- en: '| $\bm{\theta}_{f_{c}}$ | Parameters of the attribute classifier |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| $\bm{\theta}_{f_{c}}$ | 属性分类器的参数 |'
- en: '| Embedding | $\bm{z}$ | Latent representation of text, i.e., $\bm{z}\overset{\Delta}{=}E(\bm{x})$
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入 | $\bm{z}$ | 文本的潜在表示，即$\bm{z}\overset{\Delta}{=}E(\bm{x})$ |'
- en: '| $\bm{a}$ | Latent representation of the attribute value in text |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| $\bm{a}$ | 文本中属性值的潜在表示 |'
- en: 'As mentioned previously in Section [2.1.0.0.2](#S2.SS1.SSS0.P2 "2.1.0.0.2 Data-Driven
    Definition of Style as the Scope of this Survey. ‣ 2.1 How to Define Style? ‣
    2 What Is Text Style Transfer? ‣ Deep Learning for Text Style Transfer: A Survey"),
    most neural approaches assume a given set of attribute values $\mathbb{A}$, and
    each attribute value has its own corpus. For example, if the task is about formality
    transfer, then for the attribute of text formality, there are two attribute values,
    $a$ = “formal” and $a^{\prime}$ = “informal,” corresponding to a corpus $\bm{X}_{1}$
    of formal sentences and another corpus $\bm{X}_{2}$ of informal sentences. The
    style corpora can be parallel or non-parallel. Parallel data means that each sentence
    with the attribute $a$ is paired with a counterpart sentence with another attribute
    ${a}^{\prime}$. In contrast, non-parallel data only assumes mono-style corpora.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述于第[2.1.0.0.2](#S2.SS1.SSS0.P2 "2.1.0.0.2 数据驱动的风格定义作为本调查的范围。 ‣ 2.1 如何定义风格？
    ‣ 2 什么是文本风格转移？ ‣ 深度学习在文本风格转移中的应用：综述")节，大多数神经方法假设一组给定的属性值 $\mathbb{A}$，每个属性值都有自己的语料库。例如，如果任务涉及正式性转移，则对于文本正式性的属性，有两个属性值，$a$
    = “正式”和 $a^{\prime}$ = “非正式”，对应于一个正式句子的语料库 $\bm{X}_{1}$ 和另一个非正式句子的语料库 $\bm{X}_{2}$。风格语料库可以是平行的或非平行的。平行数据意味着每个具有属性
    $a$ 的句子都与另一个属性 ${a}^{\prime}$ 的对应句子配对。相反，非平行数据仅假设单一风格的语料库。
- en: 2.3 Existing Subtasks with Datasets
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 现有的子任务与数据集
- en: 'Table 3: List of common subtasks of TST and their corresponding attribute values
    and datasets. For datasets with multiple attribute-specific corpora, we report
    their sizes by the number of sentences of the smallest of all corpora. We also
    report whether the dataset is parallel (Pa?).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: TST 的常见子任务及其对应的属性值和数据集列表。对于具有多个属性特定语料库的数据集，我们报告其大小以最小语料库的句子数量为准。我们还报告数据集是否为平行的
    (Pa?)。'
- en: '| Task | Attribute Values | Datasets | Size | Pa? |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 属性值 | 数据集 | 大小 | Pa? |'
- en: '| Style Features |  |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 风格特征 |  |  |'
- en: '| Formality | Informal$\leftrightarrow$Formal | GYAFC³³3GYAFC data: [https://github.com/raosudha89/GYAFC-corpus](https://github.com/raosudha89/GYAFC-corpus)  (Rao
    and Tetreault, [2018](#bib.bib156)) | 50K | ✓ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 正式性 | 非正式$\leftrightarrow$正式 | GYAFC³³3GYAFC 数据: [https://github.com/raosudha89/GYAFC-corpus](https://github.com/raosudha89/GYAFC-corpus)  (Rao
    和 Tetreault, [2018](#bib.bib156)) | 50K | ✓ |'
- en: '| XFORMAL⁴⁴4GYAFC data: [https://github.com/Elbria/xformal-FoST](https://github.com/Elbria/xformal-FoST)  (Briakou
    et al., [2021b](#bib.bib18)) | 1K | ✓ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| XFORMAL⁴⁴4GYAFC 数据: [https://github.com/Elbria/xformal-FoST](https://github.com/Elbria/xformal-FoST)  (Briakou
    等人, [2021b](#bib.bib18)) | 1K | ✓ |'
- en: '| Politeness | Impolite$\rightarrow$Polite | Politeness⁵⁵5Politeness data:
    [https://github.com/tag-and-generate/politeness-dataset](https://github.com/tag-and-generate/politeness-dataset)  (Madaan
    et al., [2020](#bib.bib119)) | 1M | ✗ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 礼貌性 | 不礼貌$\rightarrow$礼貌 | 礼貌性⁵⁵5礼貌性数据: [https://github.com/tag-and-generate/politeness-dataset](https://github.com/tag-and-generate/politeness-dataset)  (Madaan
    等人, [2020](#bib.bib119)) | 1M | ✗ |'
- en: '| Gender | Masculine$\leftrightarrow$Feminine | Yelp Gender⁶⁶6The Yelp Gender
    dataset is from the Yelp Challenge [https://www.yelp.com/dataset](https://www.yelp.com/dataset)
    and its preprocessing needs to follow Prabhumoye et al. ([2018](#bib.bib149)).  (Prabhumoye
    et al., [2018](#bib.bib149)) | 2.5M | ✗ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 性别 | 男性$\leftrightarrow$女性 | Yelp 性别⁶⁶6Yelp 性别数据来自 Yelp Challenge [https://www.yelp.com/dataset](https://www.yelp.com/dataset)，其预处理需遵循
    Prabhumoye 等人 ([2018](#bib.bib149)) 的要求。 (Prabhumoye 等人, [2018](#bib.bib149))
    | 2.5M | ✗ |'
- en: '| Humor& Romance | Factual$\leftrightarrow$Humorous$\leftrightarrow$ Romantic
    | FlickrStyle⁷⁷7FlickrStyle data: [https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/imagecaption](https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/imagecaption)  (Gan
    et al., [2017](#bib.bib42)) | 5K | ✓ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 幽默与浪漫 | 事实性$\leftrightarrow$幽默性$\leftrightarrow$ 浪漫 | FlickrStyle⁷⁷7FlickrStyle
    数据: [https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/imagecaption](https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/imagecaption)  (Gan
    等人, [2017](#bib.bib42)) | 5K | ✓ |'
- en: '| Biasedness | Biased$\rightarrow$Neutral | Wiki Neutrality⁸⁸8Wiki Neutrality
    data: [http://bit.ly/bias-corpus](http://bit.ly/bias-corpus)  (Pryzant et al.,
    [2020](#bib.bib150)) | 181K | ✓ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 偏见 | 偏见$\rightarrow$中立 | Wiki 中立⁸⁸8Wiki 中立数据: [http://bit.ly/bias-corpus](http://bit.ly/bias-corpus)  (Pryzant
    等人, [2020](#bib.bib150)) | 181K | ✓ |'
- en: '| Toxicity | Offensive$\rightarrow$Non-offensive | Twitter dos Santos, Melnyk,
    and Padhi ([2018](#bib.bib172)) | 58K | ✗ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 毒性 | 攻击性$\rightarrow$非攻击性 | Twitter dos Santos, Melnyk 和 Padhi ([2018](#bib.bib172))
    | 58K | ✗ |'
- en: '| Reddit dos Santos, Melnyk, and Padhi ([2018](#bib.bib172)) | 224K |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Reddit dos Santos, Melnyk 和 Padhi ([2018](#bib.bib172)) | 224K |'
- en: '| Reddit Politics Tran, Zhang, and Soleymani ([2020](#bib.bib198)) | 350K |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Reddit Politics Tran, Zhang, 和 Soleymani ([2020](#bib.bib198)) | 350K |'
- en: '| Authorship | Shakespearean$\leftrightarrow$Modern | Shakespeare Xu et al.
    ([2012](#bib.bib221)) | 18K | ✓ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 作者身份 | 莎士比亚风格$\leftrightarrow$现代风格 | Shakespeare Xu 等人 ([2012](#bib.bib221))
    | 18K | ✓ |'
- en: '| Different Bible translators | Bible⁹⁹9Bible data: [https://github.com/keithecarlson/StyleTransferBibleData](https://github.com/keithecarlson/StyleTransferBibleData)  (Carlson,
    Riddell, and Rockmore, [2018](#bib.bib25)) | 28M |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 不同的圣经翻译者 | 圣经⁹⁹9圣经数据: [https://github.com/keithecarlson/StyleTransferBibleData](https://github.com/keithecarlson/StyleTransferBibleData)  (Carlson,
    Riddell, 和 Rockmore, [2018](#bib.bib25)) | 28M |  |'
- en: '| Simplicity | Complicated$\rightarrow$Simple | PWKP Zhu, Bernhard, and Gurevych
    ([2010](#bib.bib238)) | 108K | ✓ |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 简单性 | 复杂$\rightarrow$简单 | PWKP Zhu, Bernhard, 和 Gurevych ([2010](#bib.bib238))
    | 108K | ✓ |'
- en: '| Expert (den Bercken, Sips, and Lofi, [2019](#bib.bib13)) | 2.2K | ✓ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 专家 (den Bercken, Sips, 和 Lofi, [2019](#bib.bib13)) | 2.2K | ✓ |'
- en: '| MIMIC-III^(10)^(10)10MIMIC-III data: Request access at [https://mimic.physionet.org/gettingstarted/access/](https://mimic.physionet.org/gettingstarted/access/)
    and follow the preprocessing of Weng, Chung, and Szolovits ([2019](#bib.bib209))  Weng,
    Chung, and Szolovits ([2019](#bib.bib209)) | 59K | ✗ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| MIMIC-III^(10)^(10)10MIMIC-III 数据: 请求访问 [https://mimic.physionet.org/gettingstarted/access/](https://mimic.physionet.org/gettingstarted/access/)
    并按照 Weng, Chung, 和 Szolovits ([2019](#bib.bib209)) 的预处理操作  Weng, Chung, 和 Szolovits
    ([2019](#bib.bib209)) | 59K | ✗ |'
- en: '| MSD^(11)^(11)11MSD data: [https://srhthu.github.io/expertise-style-transfer/](https://srhthu.github.io/expertise-style-transfer/)  (Cao
    et al., [2020](#bib.bib23)) | 114K | ✓ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| MSD^(11)^(11)11MSD 数据: [https://srhthu.github.io/expertise-style-transfer/](https://srhthu.github.io/expertise-style-transfer/)  (Cao
    等人, [2020](#bib.bib23)) | 114K | ✓ |'
- en: '| Engagingness | Plain$\rightarrow$Attractive | Math^(12)^(12)12Math data:
    [https://gitlab.cs.washington.edu/kedzior/Rewriter/](https://gitlab.cs.washington.edu/kedzior/Rewriter/)  Koncel-Kedziorski
    et al. ([2016](#bib.bib93)) | $<$1K | ✓ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 吸引力 | 平淡$\rightarrow$吸引人 | Math^(12)^(12)12Math 数据: [https://gitlab.cs.washington.edu/kedzior/Rewriter/](https://gitlab.cs.washington.edu/kedzior/Rewriter/)  Koncel-Kedziorski
    等人 ([2016](#bib.bib93)) | $<$1K | ✓ |'
- en: '| TitleStylist^(13)^(13)13TitleStylist data: [https://github.com/jind11/TitleStylist](https://github.com/jind11/TitleStylist)  (Jin
    et al., [2020a](#bib.bib80)) | 146K | ✗ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| TitleStylist^(13)^(13)13TitleStylist 数据: [https://github.com/jind11/TitleStylist](https://github.com/jind11/TitleStylist)  (Jin
    等人, [2020a](#bib.bib80)) | 146K | ✗ |'
- en: '| Content Preferences |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 内容偏好 |  |'
- en: '| Sentiment | Positive$\leftrightarrow$Negative | Yelp^(14)^(14)14Yelp data:
    [https://github.com/shentianxiao/language-style-transfer](https://github.com/shentianxiao/language-style-transfer)  (Shen
    et al., [2017](#bib.bib182)) | 250K | ✗ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 情感 | 积极$\leftrightarrow$消极 | Yelp^(14)^(14)14Yelp 数据: [https://github.com/shentianxiao/language-style-transfer](https://github.com/shentianxiao/language-style-transfer)  (Shen
    等人, [2017](#bib.bib182)) | 250K | ✗ |'
- en: '| Amazon^(15)^(15)15Amazon data: [https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/amazon](https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/amazon)  (He
    and McAuley, [2016](#bib.bib63)) | 277K |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Amazon^(15)^(15)15Amazon 数据: [https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/amazon](https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/amazon)  (He
    和 McAuley, [2016](#bib.bib63)) | 277K |  |'
- en: '| Topic | Entertainment$\leftrightarrow$Politics | Yahoo! Answers^(16)^(16)16Yahoo!
    Answers data: [https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&did=11](https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&did=11)  Huang
    et al. ([2020](#bib.bib74)) | 153K | ✗ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 话题 | 娱乐$\leftrightarrow$政治 | Yahoo! Answers^(16)^(16)16Yahoo! Answers 数据:
    [https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&did=11](https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&did=11)  Huang
    等人 ([2020](#bib.bib74)) | 153K | ✗ |'
- en: '| Politics | Democratic$\leftrightarrow$Republican | Political^(17)^(17)17Political
    data: [https://nlp.stanford.edu/robvoigt/rtgender/](https://nlp.stanford.edu/robvoigt/rtgender/)  (Voigt
    et al., [2018](#bib.bib204)) | 540K | ✗ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 政治 | 民主党$\leftrightarrow$共和党 | Political^(17)^(17)17Political 数据: [https://nlp.stanford.edu/robvoigt/rtgender/](https://nlp.stanford.edu/robvoigt/rtgender/)  (Voigt
    等人, [2018](#bib.bib204)) | 540K | ✗ |'
- en: 'We list the common subtasks and corresponding datasets for neural TST in Table [3](#S2.T3
    "Table 3 ‣ 2.3 Existing Subtasks with Datasets ‣ 2 What Is Text Style Transfer?
    ‣ Deep Learning for Text Style Transfer: A Survey"). The attributes of interest
    vary from style features (e.g., formality and politeness) to content preferences
    (e.g., sentiment and topics). Each task of which will be elaborated below.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格 [3](#S2.T3 "Table 3 ‣ 2.3 Existing Subtasks with Datasets ‣ 2 What Is
    Text Style Transfer? ‣ Deep Learning for Text Style Transfer: A Survey") 中列出了神经
    TST 的常见子任务及其对应的数据集。感兴趣的属性从风格特征（例如，正式性和礼貌）到内容偏好（例如，情感和话题）各不相同。每个任务将在下面详细阐述。'
- en: 2.3.0.0.1 Formality.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.0.0.1 正式性。
- en: Adjusting the extent of formality in text was first proposed by Hovy ([1987](#bib.bib69)).
    It is one of the most distinctive stylistic aspects that can be observed through
    many linguistic phenomena, such as more full names (e.g., “television”) instead
    of abbreviations (e.g., “TV”), and more nouns (e.g., “solicitation”) instead of
    verbs (e.g., “request”). The formality dataset, Grammarly’s Yahoo Answers Formality
    Corpus (GYAFC) Rao and Tetreault ([2018](#bib.bib156)), contains 50K formal-informal
    pairs retrieved by first getting 50K informal sentences from the Yahoo Answers
    corpus, and then recruiting crowdsource workers to rewrite them in a formal way.
    Briakou et al. ([2021b](#bib.bib18)) extend the formality dataset to a multilingual
    version with three more languages, Brazilian Portuguese, French, and Italian.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 调整文本中的正式程度最早由Hovy ([1987](#bib.bib69)) 提出。这是通过许多语言现象可以观察到的最显著的文体特征之一，例如使用完整的名称（例如，“television”）而不是缩写（例如，“TV”），以及更多名词（例如，“solicitation”）而不是动词（例如，“request”）。正式度数据集，Grammarly的Yahoo
    Answers Formality Corpus (GYAFC) Rao 和 Tetreault ([2018](#bib.bib156))，包含50K对正式与非正式的对比，这些对比通过首先从Yahoo
    Answers语料库获取50K个非正式句子，然后招募众包工作者将其重写为正式的方式获得。Briakou 等人 ([2021b](#bib.bib18)) 将正式度数据集扩展到一个多语言版本，加入了巴西葡萄牙语、法语和意大利语。
- en: 2.3.0.0.2 Politeness.
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.0.0.2 礼貌。
- en: Politeness transfer Madaan et al. ([2020](#bib.bib119)) aims to control the
    politeness in text. For example, “Could you please send me the data?” is a more
    polite expression than “send me the data!”. Madaan et al. ([2020](#bib.bib119))
    compiled a dataset of 1.39 million automatically labeled instances from the raw
    Enron corpus Shetty and Adibi ([2004](#bib.bib183)). As politeness is culture-dependent,
    this dataset mainly focuses on politeness in North American English.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 礼貌转换 Madaan 等人 ([2020](#bib.bib119)) 旨在控制文本中的礼貌程度。例如，“Could you please send
    me the data?” 比“send me the data!” 更为礼貌。Madaan 等人 ([2020](#bib.bib119)) 编制了一个包含1.39百万个自动标记实例的数据集，来源于原始的Enron语料库
    Shetty 和 Adibi ([2004](#bib.bib183))。由于礼貌受文化影响，该数据集主要关注北美英语中的礼貌。
- en: 2.3.0.0.3 Gender.
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.0.0.3 性别。
- en: Linguistic phenomena related to gender is a heated research area Trudgill ([1972](#bib.bib199));
    Lakoff ([1973](#bib.bib99)); Tannen ([1990](#bib.bib194)); Argamon et al. ([2003](#bib.bib3));
    Boulis and Ostendorf ([2005](#bib.bib16)). The gender-related TST dataset is proposed
    by Prabhumoye et al. ([2018](#bib.bib149)) who compiled 2.5M reviews from Yelp
    Dataset Challenge that are labeled with the gender of the user.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 与性别相关的语言现象是一个热门的研究领域 Trudgill ([1972](#bib.bib199)); Lakoff ([1973](#bib.bib99));
    Tannen ([1990](#bib.bib194)); Argamon 等人 ([2003](#bib.bib3)); Boulis 和 Ostendorf
    ([2005](#bib.bib16))。性别相关的TST数据集由 Prabhumoye 等人 ([2018](#bib.bib149)) 提出，他们从Yelp数据集挑战中编制了250万条评论，并标注了用户的性别。
- en: 2.3.0.0.4 Humor&Romance.
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.0.0.4 幽默与浪漫。
- en: Humor and romance are some artistic attributes that can provide readers with
    joy. Li et al. ([2018](#bib.bib107)) first propose to borrow the FlickrStyle stylized
    caption dataset Gan et al. ([2017](#bib.bib42)) from the computer vision domain.
    In the FlickrStyle image caption dataset, each image has three captions, with
    a factual, a humorous, and a romantic style, respectively. By keeping only the
    captions of the three styles, Li et al. ([2018](#bib.bib107)) created a subset
    of the FlickrStyle dataset of 5K parallel (factual, humorous, romantic) triplets.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 幽默和浪漫是一些艺术特质，可以为读者带来欢乐。Li 等人 ([2018](#bib.bib107)) 首次提议从计算机视觉领域借用FlickrStyle风格化字幕数据集
    Gan 等人 ([2017](#bib.bib42))。在FlickrStyle图像字幕数据集中，每个图像有三种字幕，分别是事实的、幽默的和浪漫的。通过只保留这三种风格的字幕，Li
    等人 ([2018](#bib.bib107)) 创建了一个包含5K个平行（事实、幽默、浪漫）三重奏的FlickrStyle数据集子集。
- en: 2.3.0.0.5 Biasedness.
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.0.0.5 偏见度。
- en: Wiki Neutrality Corpus Pryzant et al. ([2020](#bib.bib150)) is the first corpus
    of biased and neutralized sentence pairs. It is collected from Wikipedia revisions
    that adjusted the tone of existing sentences to a more neutral voice. The types
    of bias in the biased corpus include framing bias, epistemological bias, and demographic
    bias.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Wiki中立语料库 Pryzant 等人 ([2020](#bib.bib150)) 是第一个包含偏见和中立句子对的语料库。它从Wikipedia修订中收集，这些修订将现有句子的语调调整为更中立的声音。偏见语料库中的偏见类型包括框架偏见、认识论偏见和人口统计学偏见。
- en: 2.3.0.0.6 Toxicity.
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.0.0.6 毒性。
- en: Another important use of TST is to fight against offensive language. Tran, Zhang,
    and Soleymani ([2020](#bib.bib198)) collect 350K offensive sentences and 7M non-offensive
    sentences by crawling sentences from Reddits using a list of restricted words.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: TST的另一个重要用途是对抗冒犯性语言。Tran, Zhang 和 Soleymani ([2020](#bib.bib198)) 通过使用限制词列表从Reddit上爬取句子，收集了350K个冒犯性句子和7M个非冒犯性句子。
- en: 2.3.0.0.7 Authorship.
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.0.0.7 作者身份。
- en: Changing the tone of the author is an artistic use of text style transfer. Xu
    et al. ([2012](#bib.bib221)) created an aligned corpus of 18K pairs of Shakespearean
    English and its modern English translation. Carlson, Riddell, and Rockmore ([2018](#bib.bib25))
    collected 28M parallel data from English versions of the Bible by different translators.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 改变作者语气是文本风格迁移的艺术性用途。Xu等人（[2012](#bib.bib221)）创建了一个包含18K对莎士比亚英文及其现代英文翻译的对齐语料库。Carlson、Riddell和Rockmore（[2018](#bib.bib25)）收集了来自不同翻译者的英文版圣经的28M平行数据。
- en: 2.3.0.0.8 Simplicity.
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.0.0.8 简单性。
- en: Another important use of TST is to lower the language barrier for readers, such
    as translating legalese, medical jargon, or other professional text into simple
    English, to avoid discrepancies between expert wordings and laymen’s understanding
    Tan and Goonawardene ([2017](#bib.bib193)). Common tasks include converting standard
    English Wikipedia into Simple Wikipedia whose dataset contains 108K samples Zhu,
    Bernhard, and Gurevych ([2010](#bib.bib238)). Another task is to simplify medical
    descriptions to patient-friendly text, including a dataset with 2.2K samples (den
    Bercken, Sips, and Lofi, [2019](#bib.bib13)), another non-parallel dataset with
    59K free-text discharge summaries compiled from MIMIC-III Weng, Chung, and Szolovits
    ([2019](#bib.bib209)), and a more recent parallel dataset with 114K samples compiled
    from from the health reference Merck Manuals (MSD) where discussions on each medical
    topic has one version for professionals, and the other for consumers Cao et al.
    ([2020](#bib.bib23)).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: TST的另一个重要用途是降低读者的语言障碍，例如将法律术语、医学行话或其他专业文本翻译成简单英语，以避免专家用词与普通人理解之间的差异（Tan和Goonawardene，[2017](#bib.bib193)）。常见的任务包括将标准英文维基百科转换为简单维基百科，其数据集包含108K样本（Zhu、Bernhard和Gurevych，[2010](#bib.bib238)）。另一个任务是将医学描述简化为患者友好的文本，包括一个包含2.2K样本的数据集（den
    Bercken、Sips和Lofi，[2019](#bib.bib13)），另一个非平行数据集包含59K自由文本出院总结，编译自MIMIC-III（Weng、Chung和Szolovits，[2019](#bib.bib209)），以及一个较新的平行数据集，包含114K样本，编译自健康参考Merck
    Manuals（MSD），其中每个医学主题都有一个专业版和一个消费者版（Cao等人，[2020](#bib.bib23)）。
- en: 2.3.0.0.9 Sentiment.
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.0.0.9 情感。
- en: Sentiment modification is the most popular task in previous work on TST. It
    aims to change the sentiment polarity in reviews, for example from a negative
    review to a positive review, or vice versa. There is also work on transferring
    sentiments on fine-grained review ratings, e.g., 1-5 scores. Commonly used datasets
    include Yelp reviews Shen et al. ([2017](#bib.bib182)), and Amazon product reviews
    He and McAuley ([2016](#bib.bib63)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 情感修改是TST（文本风格迁移）中最受欢迎的任务。其目的是改变评论中的情感倾向，例如将负面评论转变为正面评论，或反之亦然。还有一些研究关注在细粒度的评价评分上转移情感，例如1-5分。常用的数据集包括Yelp评论（Shen等人，[2017](#bib.bib182)）和亚马逊产品评论（He和McAuley，[2016](#bib.bib63)）。
- en: 2.3.0.0.10 Topic.
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.0.0.10 主题。
- en: There are a few works that cover topic transfer. For example, Huang et al. ([2020](#bib.bib74))
    form a two-topic corpus by compiling Yahoo! Answers under two topics, entertainment
    and politics, respectively. There is also a recent dataset with 21 text styles
    such as Sciences, Sport, Politics, and others Zeng, Shoeybi, and Liu ([2020](#bib.bib227)).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些研究涉及主题迁移。例如，黄等人（[2020](#bib.bib74)）通过编译Yahoo! Answers中关于娱乐和政治两个主题的问答，形成了一个两个主题的语料库。最近还有一个包含21种文本风格的数据集，如科学、体育、政治等，由Zeng、Shoeybi和Liu（[2020](#bib.bib227)）提供。
- en: 2.3.0.0.11 Political Slant.
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.0.0.11 政治倾向。
- en: Political slant transfer proposed by Prabhumoye et al. ([2018](#bib.bib149))
    aims to transfer the political view in text. For example, a republican’s comment
    can be “defund all illegal immigrants,” while democrats are more likely to support
    humanistic actions towards immigrants. The political slant dataset Voigt et al.
    ([2018](#bib.bib204)) is collected from comments on Facebook posts of the United
    States Senate and House members. The dataset uses top-level comments directly
    responding to the posts of a democratic or republican Congressperson. There are
    540K training, 4K development, and 56K test instances in the dataset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Prabhumoye等人（[2018](#bib.bib149)）提出的政治倾向迁移旨在转移文本中的政治观点。例如，一名共和党人的评论可能是“取消所有非法移民的资金”，而民主党人则更倾向于支持对移民采取人道行动。政治倾向数据集Voigt等人（[2018](#bib.bib204)）收集自美国参议院和众议院成员Facebook帖子上的评论。该数据集使用直接回应民主党或共和党国会议员帖子中的顶级评论。数据集中包含540K训练、4K开发和56K测试实例。
- en: 2.3.0.0.12 Combined Attributes.
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.0.0.12 综合属性。
- en: 'Lample et al. ([2019](#bib.bib102)) propose a more challenging setting of text
    attribute transfer – multi-attribute transfer. For example, the source sentence
    can be a positive review on an Asian restaurant written by a male reviewer, and
    the target sentence is a negative review on an American restaurant written by
    a female. Each of their datasets has 1-3 independent categories of attributes.
    Their first dataset is FYelp, which is compiled from the Yelp Dataset Challenge,
    labeled with sentiment (positive or negative), gender (male or female), and eatery
    category (American, Asian, bar, dessert, or Mexican). Their second dataset, Amazon,
    which is based on the Amazon product review dataset Li et al. ([2018](#bib.bib107)),
    contains the following attributes: sentiment (positive or negative), and product
    category (book, clothing, electronics, movies, or music). Their third dataset,
    Social Media Content dataset, collected from internal Facebook data which is private,
    contains gender (male or female), age group (18-24 or 65+), and writer-annotated
    feeling (relaxed or annoyed).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Lample 等人 ([2019](#bib.bib102)) 提出了更具挑战性的文本属性转移设置——多属性转移。例如，源句子可以是男性评论者对一家亚洲餐馆的正面评价，而目标句子则是女性评论者对一家美国餐馆的负面评价。他们的每个数据集包含
    1-3 个独立的属性类别。他们的第一个数据集是 FYelp，这个数据集从 Yelp 数据集挑战中编制而成，标注了情感（正面或负面）、性别（男性或女性）和餐馆类别（美国、亚洲、酒吧、甜点或墨西哥）。他们的第二个数据集是
    Amazon，该数据集基于 Li 等人 ([2018](#bib.bib107)) 的亚马逊产品评论数据集，包含以下属性：情感（正面或负面）和产品类别（书籍、服装、电子产品、电影或音乐）。他们的第三个数据集是社交媒体内容数据集，该数据集收集自私人
    Facebook 数据，包含性别（男性或女性）、年龄组（18-24 或 65+）和作者标注的情感（放松或烦恼）。
- en: 3 How to Evaluate Style Transfer?
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 如何评估风格转移？
- en: 'A successful style-transferred output not only needs to demonstrate the correct
    target style, but also, due to the uncontrollability of neural networks, we need
    to verify that it preserves the original semantics, and maintains natural language
    fluency. Therefore, the commonly used practice of evaluation considers the following
    three criteria: (1) transferred style strength, (2) semantic preservation, and
    (3) fluency.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的风格转移输出不仅需要展示正确的目标风格，而且由于神经网络的不可控性，我们还需要验证其是否保留了原始语义，并保持自然语言流畅。因此，常用的评估实践考虑以下三个标准：（1）转移风格强度，（2）语义保留，和（3）流畅度。
- en: 'We will first introduce the practice of automatic evaluation on the three criteria,
    discuss the benefits and caveats of automatic evaluation, and then introduce human
    evaluation as a remedy for some of the intrinsic weaknesses of automatic evaluation.
    Finally, we will suggest some standard practice of TST evaluation for future work.
    The overview of evaluation methods regarding each criterion is listed in Table [4](#S3.T4
    "Table 4 ‣ 3 How to Evaluate Style Transfer? ‣ Deep Learning for Text Style Transfer:
    A Survey").'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍在这三种标准下自动评估的实践，讨论自动评估的优点和注意事项，然后介绍人类评估作为解决自动评估一些固有弱点的补救措施。最后，我们将为未来的工作建议一些
    TST 评估的标准实践。有关每个标准的评估方法概述列在表 [4](#S3.T4 "表 4 ‣ 3 如何评估风格转移？ ‣ 深度学习文本风格转移综述") 中。
- en: 'Table 4: Overview of evaluation methods for each criterion.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：各标准评估方法概述。
- en: '| Criterion | Automatic Evaluation | Human Evaluation |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | 自动评估 | 人工评估 |'
- en: '| Overall | BLEU with gold references | Rating or ranking |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | 带有黄金参考的 BLEU | 评分或排名 |'
- en: '|    - Transferred Style Strength | Accuracy by a separately trained style
    classifier | Rating or ranking |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|   - 转移风格强度 | 由单独训练的风格分类器的准确度 | 评分或排名 |'
- en: '|    - Semantic Preservation | BLEU/ROUGE/etc. with (modified) inputs | Rating
    or ranking |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|   - 语义保留 | 带有（修改）输入的 BLEU/ROUGE 等 | 评分或排名 |'
- en: '|    - Fluency | Perplexity by a separately trained LM | Rating or ranking
    |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|   - 流畅度 | 由单独训练的语言模型困惑度 | 评分或排名 |'
- en: 3.1 Automatic Evaluation
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 自动评估
- en: Automatic evaluation provides an economic, reproducible, and scalable way to
    assess the quality of generation results. However, due to the complexities of
    natural language, each metric introduced below can address certain aspects, but
    also has intrinsic blind spots.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 自动评估提供了一种经济、可重复且可扩展的方式来评估生成结果的质量。然而，由于自然语言的复杂性，下面介绍的每个指标可以解决某些方面的问题，但也有固有的盲点。
- en: 3.1.0.0.1 BLEU with Gold References.
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.0.0.1 带有黄金参考的 BLEU。
- en: Similar to many text generation tasks, text style transfer also has human-written
    references on several datasets (e.g., Yelp, Captions, etc.), so it is common to
    use the BLEU score Papineni et al. ([2002](#bib.bib142)) between the gold references
    and model outputs. Using BLEU to evaluate TST models has been seen across pre-deep
    learning works Xu et al. ([2012](#bib.bib221)); Jhamtani et al. ([2017](#bib.bib78))
    and deep learning approaches Rao and Tetreault ([2018](#bib.bib156)); Li et al.
    ([2018](#bib.bib107)); Jin et al. ([2019](#bib.bib82)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于许多文本生成任务，文本风格迁移也在人类编写的多个数据集中（如 Yelp、Captions 等）有参考，因此通常使用**BLEU**分数 Papineni
    等 ([2002](#bib.bib142)) 来评估黄金参考与模型输出之间的匹配。在深度学习之前的研究中，评估 TST 模型时使用 BLEU 已被看到 Xu
    等 ([2012](#bib.bib221)); Jhamtani 等 ([2017](#bib.bib78))，以及深度学习方法 Rao 和 Tetreault
    ([2018](#bib.bib156)); Li 等 ([2018](#bib.bib107)); Jin 等 ([2019](#bib.bib82))。
- en: 'There are three problems with using BLEU between the gold references and model
    outputs:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **BLEU** 评估黄金参考和模型输出之间存在三个问题：
- en: Problem 1)
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题 1)
- en: It mainly evaluates content and simply copying the input can result in high
    BLEU scores
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它主要评估内容，简单地复制输入可能会导致高 **BLEU** 分数。
- en: Problem 2)
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题 2)
- en: BLEU is shown to have low correlation with human evaluation
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 研究显示 **BLEU** 与人类评估的相关性较低。
- en: Problem 3)
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题 3)
- en: Some datasets do not have human-written references
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些数据集没有人类编写的参考。
- en: 'Problem [1](#S3.I1.i1 "item 1 ‣ 3.1.0.0.1 BLEU with Gold References. ‣ 3.1
    Automatic Evaluation ‣ 3 How to Evaluate Style Transfer? ‣ Deep Learning for Text
    Style Transfer: A Survey"): Different from machine translation, where using BLEU
    only is sufficient, TST has to consider the caveat that simply copying the input
    sentence can achieve high BLEU scores with the gold references on many datasets
    (e.g., $\sim$40 on Yelp, $\sim$20 on Humor&Romance, $\sim$50 for informal-to-formal
    style transfer, and $\sim$30 for formal-to-informal style transfer). It is because
    most text rewrites have a large extent of n-gram overlap with the source sentence.
    In contrast, machine translation does not have this concern, because the vocabulary
    of its input and output are different, and copying the input sequence does not
    give high BLEU scores. A possible fix to consider is to combine BLEU with PINC
    Chen and Dolan ([2011](#bib.bib29)) as in paraphrasing Xu et al. ([2012](#bib.bib221));
    Jhamtani et al. ([2017](#bib.bib78)). By using PINC and BLEU as a 2-dimensional
    metric, we can minimize the n-gram overlap with the source sentence but maximize
    the n-gram overlap with the reference sentences.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '问题 [1](#S3.I1.i1 "item 1 ‣ 3.1.0.0.1 BLEU with Gold References. ‣ 3.1 Automatic
    Evaluation ‣ 3 How to Evaluate Style Transfer? ‣ Deep Learning for Text Style
    Transfer: A Survey")：与机器翻译不同，使用 **BLEU** 仅是足够的，TST 必须考虑到这样一个警告，即简单地复制输入句子可以在许多数据集上（例如，Yelp
    上约 40，Humor&Romance 上约 20，非正式到正式风格转变约 50，以及正式到非正式风格转变约 30）实现高 **BLEU** 分数。这是因为大多数文本重写与源句子有大量的
    n-gram 重叠。相比之下，机器翻译没有这个问题，因为其输入和输出的词汇不同，复制输入序列不会得到高 **BLEU** 分数。一个可能的解决办法是考虑将
    **BLEU** 与 PINC Chen 和 Dolan ([2011](#bib.bib29)) 结合使用，如在意图改述中 Xu 等 ([2012](#bib.bib221));
    Jhamtani 等 ([2017](#bib.bib78)) 所述。通过使用 **PINC** 和 **BLEU** 作为二维指标，我们可以最小化与源句子的
    n-gram 重叠，同时最大化与参考句子的 n-gram 重叠。'
- en: 'Problem [2](#S3.I1.i2 "item 2 ‣ 3.1.0.0.1 BLEU with Gold References. ‣ 3.1
    Automatic Evaluation ‣ 3 How to Evaluate Style Transfer? ‣ Deep Learning for Text
    Style Transfer: A Survey")&[3](#S3.I1.i3 "item 3 ‣ 3.1.0.0.1 BLEU with Gold References.
    ‣ 3.1 Automatic Evaluation ‣ 3 How to Evaluate Style Transfer? ‣ Deep Learning
    for Text Style Transfer: A Survey"): Other problems include insufficient correlation
    of BLEU with human evaluations (e.g., $\leq$0.30 w.r.t. human-rated grammaticality
    shown in Li et al. ([2018](#bib.bib107)) and $\leq$0.45 w.r.t. human evaluations
    shown in Mir et al. ([2019](#bib.bib130))), and the unavailability of human-written
    references for some datasets (e.g., gender and political datasets Prabhumoye et al.
    ([2018](#bib.bib149)), and the politeness dataset Madaan et al. ([2020](#bib.bib119))).
    A commonly used fix is to make the evaluation more fine-grained using three different
    independent aspects, namely transferred style strength, semantic preservation,
    and fluency, which will be detailed below.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '问题 [2](#S3.I1.i2 "item 2 ‣ 3.1.0.0.1 BLEU with Gold References. ‣ 3.1 Automatic
    Evaluation ‣ 3 How to Evaluate Style Transfer? ‣ Deep Learning for Text Style
    Transfer: A Survey")&[3](#S3.I1.i3 "item 3 ‣ 3.1.0.0.1 BLEU with Gold References.
    ‣ 3.1 Automatic Evaluation ‣ 3 How to Evaluate Style Transfer? ‣ Deep Learning
    for Text Style Transfer: A Survey")：其他问题包括BLEU与人工评估的相关性不足（例如，相对于Li et al. ([2018](#bib.bib107))显示的人类评分语法正确性$\leq$0.30，和相对于Mir
    et al. ([2019](#bib.bib130))显示的人工评估$\leq$0.45），以及某些数据集（例如，性别和政治数据集 Prabhumoye
    et al. ([2018](#bib.bib149))，以及礼貌数据集 Madaan et al. ([2020](#bib.bib119))）缺乏人工编写的参考文献。一个常用的解决方法是使用三个不同的独立方面来使评估更加细致，这些方面分别是转移风格强度、语义保留和流畅度，以下将详细介绍。'
- en: 3.1.0.0.2 Transferred Style Strength.
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.0.0.2 转移风格强度。
- en: To automatically evaluate the transferred style strength, most works separately
    train a style classifier to distinguish the attributes Hu et al. ([2017](#bib.bib72));
    Shen et al. ([2017](#bib.bib182)); Fu et al. ([2018](#bib.bib41)); Li et al. ([2018](#bib.bib107));
    Prabhumoye et al. ([2018](#bib.bib149)).^(18)^(18)18Note that this style classifier
    usually report 80+% or 90+% accuracy, and we will discuss the the problem of false
    positives and false negatives in last paragraph of automatic evaluation. This
    classifier is used to judge whether each sample generated by the model conforms
    to the target attribute. The transferred style strength is calculated as $\frac{\text{\#
    test samples correctly classified}}{\text{\# all test samples}}$. Li et al. ([2018](#bib.bib107))
    shows that the attribute classifier correlates well with human evaluation on some
    datasets (e.g., Yelp and Captions), but has almost no correlation with others
    (e.g., Amazon). The reason is that some product genres has a dominant number of
    positive or negative reviews.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了自动评估转移风格强度，大多数研究分别训练一个风格分类器来区分属性 Hu et al. ([2017](#bib.bib72))；Shen et al.
    ([2017](#bib.bib182))；Fu et al. ([2018](#bib.bib41))；Li et al. ([2018](#bib.bib107))；Prabhumoye
    et al. ([2018](#bib.bib149)).^(18)^(18)18注意，这种风格分类器通常报告80%以上或90%以上的准确率，我们将在自动评估的最后一段讨论假阳性和假阴性的问题。该分类器用于判断模型生成的每个样本是否符合目标属性。转移风格强度计算为$\frac{\text{\#
    test samples correctly classified}}{\text{\# all test samples}}$。Li et al. ([2018](#bib.bib107))表明，该属性分类器与某些数据集（例如，Yelp和Captions）的人工评估相关性较好，但与其他数据集（例如，Amazon）几乎没有相关性。原因是某些产品类别有大量的正面或负面评论。
- en: 3.1.0.0.3 Semantic Preservation.
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.0.0.3 语义保留。
- en: Many metrics can be applied to measure the similarity between the input and
    output sentence pairs, including BLEU (Papineni et al., [2002](#bib.bib142)),
    ROUGE (Lin and Och, [2004](#bib.bib112)), METEOR (Banerjee and Lavie, [2005](#bib.bib7)),
    chrF (Popović, [2015](#bib.bib146)), Word Mover Distance (WMD) (Kusner et al.,
    [2015](#bib.bib97)). Recently, some additional deep-learning-based metrics are
    proposed, such as cosine similarity based on sentence embeddings (Fu et al., [2018](#bib.bib41)),
    and BERTScore (Zhang et al., [2020](#bib.bib230)). There are also evaluation metrics
    that are specific for TST such as the Part-of-Speech distance (Tian, Hu, and Yu,
    [2018](#bib.bib195)). Another newly proposed metric is to first delete all attribute-related
    expressions in the text, and then apply the above similarity evaluations Mir et al.
    ([2019](#bib.bib130)). Among all the metrics, Mir et al. ([2019](#bib.bib130));
    Yamshchikov et al. ([2021](#bib.bib222)) showed that METEOR and WMD have better
    correlation with human evaluation than BLEU, although, in practice, BLEU is the
    most widely used metric to evaluate the semantic similarity between the source
    sentence and style-transferred output Yang et al. ([2018](#bib.bib224)); Madaan
    et al. ([2020](#bib.bib119)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 可以应用许多指标来测量输入和输出句子对之间的相似性，包括 BLEU（Papineni 等，[2002](#bib.bib142)）、ROUGE（Lin
    和 Och，[2004](#bib.bib112)）、METEOR（Banerjee 和 Lavie，[2005](#bib.bib7)）、chrF（Popović，[2015](#bib.bib146)）、词移动距离（WMD）（Kusner
    等，[2015](#bib.bib97)）。最近，提出了一些额外的基于深度学习的指标，例如基于句子嵌入的余弦相似度（Fu 等，[2018](#bib.bib41)）和
    BERTScore（Zhang 等，[2020](#bib.bib230)）。还有一些特定于 TST 的评估指标，如词性距离（Tian, Hu 和 Yu，[2018](#bib.bib195)）。另一种新提出的指标是首先删除文本中所有与属性相关的表达，然后应用上述相似性评估（Mir
    等，[2019](#bib.bib130)）。在所有指标中，Mir 等 ([2019](#bib.bib130)) 和 Yamshchikov 等 ([2021](#bib.bib222))
    表明 METEOR 和 WMD 与人类评估的相关性优于 BLEU，尽管在实践中，BLEU 是评估源句子与风格转移输出之间语义相似性的最广泛使用的指标（Yang
    等，[2018](#bib.bib224)；Madaan 等，[2020](#bib.bib119)）。
- en: 3.1.0.0.4 Fluency.
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.0.0.4 流畅性。
- en: 'Fluency is a basic requirement for natural language outputs. To automate this
    evaluation, perplexity is calculated via a language model (LM) pretrained on the
    training data of all attributes (Yang et al., [2018](#bib.bib224)). However, the
    effectiveness of perplexity remains debateable, as Pang and Gimpel ([2019](#bib.bib141))
    showed its high correlation with human ratings of fluency, whereas Mir et al.
    ([2019](#bib.bib130)) suggested no significant correlation between perplexity
    and human scores. We note that perplexity by LM can suffer from the following
    undesired properties:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 流畅性是自然语言输出的基本要求。为了自动化评估，困惑度通过在所有属性的训练数据上预训练的语言模型（LM）进行计算（Yang 等，[2018](#bib.bib224)）。然而，困惑度的有效性仍然存在争议，因为
    Pang 和 Gimpel ([2019](#bib.bib141)) 表示其与流畅性的人类评分有很高的相关性，而 Mir 等 ([2019](#bib.bib130))
    则认为困惑度与人类评分之间没有显著相关性。我们注意到，LM 产生的困惑度可能会受到以下不希望出现的属性的影响：
- en: '1.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Biased towards shorter sentences than longer sentences.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 偏向于较短的句子，而不是较长的句子。
- en: '2.'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: For the same meaning, less frequent words will have worse perplexity (e.g.,
    agreeable) than frequent words (e.g., good).
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于相同的意思，使用不那么频繁的词汇（例如 agreeable）其困惑度会比常用词汇（例如 good）更差。
- en: '3.'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: A sentence’s own perplexity will change if the sentence prior to it changes.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果前面的句子发生变化，句子的困惑度也会改变。
- en: '4.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: LMs are not good enough yet.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LMs 目前还不够好。
- en: '5.'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: LMs do not necessarily handle well the domain shift between their training corpus
    and the style-transferred text.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言模型（LM）可能无法很好地处理训练语料库与风格转移文本之间的领域转换。
- en: '6.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Perplexity scores produced by LMs are sensitive to the training corpora, LM
    architecture and configuration, as well as optimization configuration. Therefore,
    different models’ outputs must be evaluated by exactly the same LM for fair comparison,
    which adds more difficulty to benchmarking.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LMs 产生的困惑度分数对训练语料库、LM 架构和配置以及优化配置非常敏感。因此，不同模型的输出必须由完全相同的 LM 进行评估，以确保公平比较，这增加了基准测试的难度。
- en: Such properties will bias against certain models, which is not desired for an
    evaluation metric. As a potential remedy, future researchers can try grammaticality
    checker to score the generated text.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属性会对某些模型产生偏见，这对于评估指标来说是不希望出现的。作为潜在的解决办法，未来的研究人员可以尝试使用语法检查器来评分生成的文本。
- en: 3.1.0.0.5 Task-Specific Criteria.
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.0.0.5 任务特定标准。
- en: As TST can serve as a component for other downstream applications, some task-specific
    criteria are also proposed to evaluate the quality of generated text. For example,
    Reiter, Robertson, and Osman ([2003](#bib.bib159)) evaluated the effect of their
    tailored text on reducing smokers’ intent to smoke through clinical trials. Jin
    et al. ([2020a](#bib.bib80)) applied TST to generate eye-catchy headlines so they
    have an attractive score, and future works in this direction can also test the
    click-through rates. Hu et al. ([2017](#bib.bib72)) evaluated how the generated
    text as augmented data can improve the downstream attribute classification accuracy.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TST可以作为其他下游应用的一个组件，因此还提出了一些任务特定的标准来评估生成文本的质量。例如，Reiter、Robertson和Osman ([2003](#bib.bib159))
    通过临床试验评估了其定制文本在减少吸烟者吸烟意图方面的效果。Jin等 ([2020a](#bib.bib80)) 应用TST生成引人注目的标题，使其具有较高的吸引力分数，未来在这一方向的研究也可以测试点击率。Hu等
    ([2017](#bib.bib72)) 评估了生成的文本作为扩充数据如何提高下游属性分类的准确性。
- en: 3.1.0.0.6 Tips for Automatic Metrics.
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.1.0.0.6 自动评估指标提示。
- en: 'For the evaluation metrics that rely on the pretrained models, namely the style
    classifier and LM, we need to beware of the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于依赖于预训练模型的评估指标，即风格分类器和语言模型，我们需要注意以下几点：
- en: '1.'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The pretrained models for automatic evaluation should be separate from the proposed
    TST model
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动评估的预训练模型应与提出的TST模型分开。
- en: '2.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Machine learning models can be imperfect, so we should be aware of the potential
    false positives and false negatives
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机器学习模型可能不完美，因此我们应当意识到潜在的假阳性和假阴性。
- en: '3.'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The pretrained models are imperfect in the sense that they will favor towards
    a certain type of methods
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预训练模型并不完美，因为它们倾向于某种特定类型的方法。
- en: For the first point, it is important to not use the same style classifier or
    LM in the proposed TST approach, otherwise it can overfit or hack the metrics.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一点，重要的是不要在提出的TST方法中使用相同的风格分类器或语言模型，否则可能会导致过拟合或操控指标。
- en: For the second point, we need to understand what can be the false positives
    and false negatives of the generated outputs. An illustrative example is that
    if the style classifier only reports 80+% performance (e.g., on the gender dataset
    Prabhumoye et al. ([2018](#bib.bib149)) and Amazon dataset Li et al. ([2018](#bib.bib107))),
    even perfect style rewrites can only score 80+%, but maybe an imperfect model
    can score 90% because it can resemble the imperfect style classification model
    more and makes advantage of the false positives. Other reasons for false positives
    can be adversarial attacks. Jin et al. ([2020b](#bib.bib81)) showed that merely
    paraphrasing by synonyms can drop the performance of high-accuracy classification
    models from TextCNN Kim ([2014](#bib.bib91)) to BERT Devlin et al. ([2019](#bib.bib36))
    by 90+%. Therefore, higher scores by the style classifier does not necessarily
    indicate more successful transfer. Moreover, the style classifier can produce
    false negatives if there is a distribution shift between the training data and
    style-transferred outputs. For example, in the training corpus, a product may
    appear often with the positive attribute, and in the style-transferred outputs,
    this product co-occurs with the opposite, negative attribute. Such false negatives
    are observed on the Amazon product review dataset Li et al. ([2018](#bib.bib107)).
    On the other hand, the biases of the LM correlate with sentence length, synonym
    replacement, and prior context.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二点，我们需要了解生成输出的假阳性和假阴性是什么。一个说明性的例子是，如果风格分类器仅报告80%以上的性能（例如，在性别数据集Prabhumoye等
    ([2018](#bib.bib149)) 和亚马逊数据集Li等 ([2018](#bib.bib107)) 上），即使是完美的风格重写也只能得到80%以上的分数，但一个不完美的模型可能会得到90%的分数，因为它可能更类似于不完美的风格分类模型，从而利用假阳性。其他导致假阳性的原因可能是对抗性攻击。Jin等
    ([2020b](#bib.bib81)) 显示，仅通过同义词进行释义可能会使高准确率的分类模型从TextCNN Kim ([2014](#bib.bib91))
    降低到BERT Devlin等 ([2019](#bib.bib36)) 90%以上。因此，风格分类器的高分数并不一定表示更成功的转移。此外，如果训练数据与风格转换后的输出之间存在分布变化，风格分类器可能会产生假阴性。例如，在训练语料中，某产品可能经常与正面属性一起出现，而在风格转换后的输出中，该产品则与相反的负面属性共同出现。这种假阴性在亚马逊产品评论数据集Li等
    ([2018](#bib.bib107)) 中有所观察。另一方面，语言模型的偏差与句子长度、同义词替换和先前上下文相关。
- en: The third point is a direct result implied by the second point, so in practice,
    we need to keep in mind and check whether the proposed model takes advantage of
    the evaluation metrics or makes improvements that are generalizable.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 第三点是第二点所暗示的直接结果，因此在实际操作中，我们需要牢记并检查所提出的模型是否利用了评估指标或在通用性方面做出了改进。
- en: 3.2 Human Evaluation
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 人工评估
- en: Compared to the pros and cons of the automatic evaluation metrics mentioned
    above, human evaluation stands out for its flexibility and comprehensiveness.
    For example, when asking humans to evaluate the fluency, we do not need to worry
    for the bias towards shorter sentences as in the LM. We can also design criteria
    that are not computationally easy such as comparing and ranking the outputs of
    multiple models. There are several ways to conduct human evaluation. In terms
    of evaluation types, there are pointwise scoring, namely asking humans to provide
    absolute scores of the model outputs, and pairwise comparison, namely asking humans
    to judge which of the two outputs is better, or providing a ranking for multiple
    outputs. In terms of the criteria, humans can provide overall evaluation, or separate
    scores for transferred style strength, semantic preservation, and fluency.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述自动评估指标的优缺点相比，人工评估以其灵活性和全面性而脱颖而出。例如，在要求人类评估流畅性时，我们无需担心对短句子的偏见，如同在语言模型中那样。我们还可以设计一些计算上不容易实现的标准，如比较和排名多个模型的输出。进行人工评估有几种方法。在评估类型方面，有逐点评分，即要求人类提供模型输出的绝对分数，以及对比评分，即要求人类判断两个输出中哪个更好，或者为多个输出提供排名。在标准方面，人类可以提供总体评估，或对转移风格的强度、语义保留和流畅性进行单独评分。
- en: However, the well-known limitations of human evaluation are cost and irreproducibility.
    Performing human evaluations can be time consuming, which may result in significant
    time and financial costs. Moreover, the human evaluation results in two studies
    are often not directly comparable, because human evaluation results tend to be
    subjective and not easily irreproducible Belz et al. ([2020](#bib.bib12)). Moreover,
    some styles are very difficult to evaluate without expertise and extensive reading
    experience.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，人工评估的著名限制是成本和不可重复性。进行人工评估可能非常耗时，从而导致显著的时间和财务成本。此外，两项研究中的人工评估结果通常不可直接比较，因为人工评估结果往往是主观的且不容易重复（Belz
    等，[2020](#bib.bib12)）。此外，一些风格在没有专业知识和广泛阅读经验的情况下非常难以评估。
- en: As a remedy, we encourage future researchers to report inter-rater agreement
    scores such as the Cohen’s kappa Cohen ([1960](#bib.bib33)) and Krippendorff’s
    alpha Krippendorff ([2018](#bib.bib95)). Briakou et al. ([2021a](#bib.bib17))
    also recommends to standardize and describe evaluation protocols (e.g., linguistic
    background of the annotators, compensation, detailed annotation instructions for
    each evaluation aspect), and release annotations.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 作为补救措施，我们鼓励未来的研究人员报告如Cohen’s kappa Cohen（[1960](#bib.bib33)）和Krippendorff’s
    alpha Krippendorff（[2018](#bib.bib95)）等的评估者间一致性得分。Briakou 等（[2021a](#bib.bib17)）也建议标准化和描述评估协议（例如，标注员的语言背景、报酬、每个评估方面的详细标注说明），并发布标注结果。
- en: 3.2.0.0.1 Tips for Human Evaluation.
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3.2.0.0.1 人工评估的提示
- en: As common practice, most works use 100 outputs for each style transfer direction
    (e.g., 100 outputs for formal$\rightarrow$informal, and 100 outputs for informal$\rightarrow$formal),
    and two human annotators for each task (Shen et al., [2017](#bib.bib182); Fu et al.,
    [2018](#bib.bib41); Li et al., [2018](#bib.bib107)).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种常见做法，大多数工作使用每种风格转移方向的`100`个输出（例如，从正式$\rightarrow$非正式的`100`个输出，以及从非正式$\rightarrow$正式的`100`个输出），并为每项任务安排两名人工标注员（Shen
    等，[2017](#bib.bib182)；Fu 等，[2018](#bib.bib41)；Li 等，[2018](#bib.bib107)）。
- en: 3.3 Suggested Evaluation Settings for Future Work
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 未来工作的建议评估设置
- en: Currently, the experiments of various TST works do not adopt the same setting,
    making it difficult to do head-to-head comparison among the empirical results
    of multiple works. Although it is reasonable to customize the experimental settings
    according to the needs of a certain work, it is suggested to at least use the
    standard setting in at least one of the many reported experiments, to make it
    easy to compare with previous and future works. For example, at least (1) experiment
    on at least one commonly used dataset, (2) list up-to-date best-performing previous
    models as baselines, (3) report on a superset of the most commonly used metrics,
    and (4) release system outputs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，各种 TST 研究的实验设置不一致，这使得在多个工作的经验结果之间进行面对面的比较变得困难。虽然根据某项工作的需要定制实验设置是合理的，但建议至少在许多报告的实验中使用标准设置之一，以便与以前和未来的工作进行比较。例如，至少（1）在至少一个常用数据集上进行实验，（2）列出最新的最佳表现的先前模型作为基线，（3）报告一个最常用度量的超集，（4）发布系统输出。
- en: For (1), we suggest that future works use at least one of the most commonly
    used benchmark datasets, such as the Yelp data prepreocessed by Shen et al. ([2017](#bib.bib182))
    and its five human references provided by Jin et al. ([2019](#bib.bib82)), Amazon
    data preprocessed by Li et al. ([2018](#bib.bib107)), and formality data provided
    by Rao and Tetreault ([2018](#bib.bib156)).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于（1），我们建议未来的研究使用至少一个最常用的基准数据集，例如，Shen 等人（[2017](#bib.bib182)）预处理的 Yelp 数据及其
    Jin 等人（[2019](#bib.bib82)）提供的五个人工参考，Li 等人（[2018](#bib.bib107)）预处理的 Amazon 数据，以及
    Rao 和 Tetreault（[2018](#bib.bib156)）提供的正式性数据。
- en: For (2), we suggest that future works actively check the latest style transfer
    papers curated at [https://github.com/fuzhenxin/Style-Transfer-in-Text](https://github.com/fuzhenxin/Style-Transfer-in-Text)
    and our repository [https://github.com/zhijing-jin/Text_Style_Transfer_Survey](https://github.com/zhijing-jin/Text_Style_Transfer_Survey),
    and compare with the state-of-the-art performances instead of older ones. We also
    call for more reproducibility in the community, including source codes and evaluation
    codes, because, for example, there are several different scripts to evaluate the
    BLEU scores.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于（2），我们建议未来的工作积极查看最新的风格迁移论文，这些论文可以在 [https://github.com/fuzhenxin/Style-Transfer-in-Text](https://github.com/fuzhenxin/Style-Transfer-in-Text)
    和我们的仓库 [https://github.com/zhijing-jin/Text_Style_Transfer_Survey](https://github.com/zhijing-jin/Text_Style_Transfer_Survey)
    中找到，并与最先进的表现进行比较，而不是与较旧的结果进行比较。我们还呼吁社区增加可复现性，包括源代码和评估代码，因为例如，用于评估 BLEU 分数的脚本有几种不同的版本。
- en: 'For (3), since no single evaluation metric is perfect and comprehensive enough
    for TST, it is strongly suggested to use both human and automatic evaluation on
    three criteria. In evaluation, apart from customized use of metrics, we suggest
    that most future works to include at least the following evaluation practices:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于（3），由于没有单一的评估指标能够完美且全面地评估 TST，强烈建议同时使用人工评估和自动评估这三项标准。在评估中，除了定制使用的指标外，我们建议大多数未来的工作至少包括以下评估实践：
- en: •
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Human evaluation: rate at least two state-of-the-art models according to the
    curated paper lists'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人工评估：根据精心策划的论文列表对至少两个最先进的模型进行评分。
- en: •
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Automatic evaluation: at least report the BLEU score with all available references
    if there exist human-written references (e.g., the five references for the Yelp
    dataset provided by Jin et al. ([2019](#bib.bib82))), and BLEU with the input
    only when there are no human-written references.'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动评估：如果存在人工编写的参考文献（例如，Jin 等人（[2019](#bib.bib82)）为 Yelp 数据集提供的五个参考），至少报告所有可用参考文献的
    BLEU 分数；如果没有人工编写的参考文献，则仅报告输入的 BLEU 分数。
- en: For (4), it will also be very helpful to provide system outputs for each TST
    paper, so that future works can better reproduce both human and automatic evaluation
    results. Note that releasing system outputs can help future works’ comparison
    of automatic evaluation results, because there can be different scripts to evaluate
    the BLEU scores, as well as different style classifiers and LM. It will be a great
    addition to the TST community if future work can establish an online leaderboard,
    let existing works upload their output files, and automatically evaluate the model
    outputs using a standard set of automatic evaluation scripts.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于（4），提供每个TST论文的系统输出也将非常有帮助，以便未来的工作可以更好地重现人类和自动评估结果。请注意，发布系统输出可以帮助未来工作的自动评估结果比较，因为可以有不同的脚本来评估BLEU分数，以及不同的风格分类器和语言模型。如果未来的工作能够建立一个在线排行榜，让现有的工作上传其输出文件，并使用一套标准的自动评估脚本自动评估模型输出，这将是TST社区的一个重大补充。
- en: 4 Methods on Parallel Data
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基于平行数据的方法
- en: 'Over the last several years, various methods have been proposed for text style
    transfer. In general, they can be categorized based on whether the dataset has
    parallel text with different styles or several non-parallel mono-style corpora.
    The rightmost column “Pa?” in Table [3](#S2.T3 "Table 3 ‣ 2.3 Existing Subtasks
    with Datasets ‣ 2 What Is Text Style Transfer? ‣ Deep Learning for Text Style
    Transfer: A Survey") shows whether there exist parallel data for each TST subtask.
    In this section, we will cover TST methods on parallel datasets, and in Section [5](#S5
    "5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey")
    we will detail the approaches on non-parallel datasets. To ease the understanding
    for the readers, we will in most cases explain TST on one attribute between two
    values, such as transferring the formality between informal and formal tones,
    which can potentially be extended to multiple attributes.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，已经提出了各种文本风格转换方法。通常，这些方法可以根据数据集是否具有不同风格的平行文本或几个非平行的单一风格语料库来分类。表格[3](#S2.T3
    "表 3 ‣ 2.3 现有子任务及数据集 ‣ 2 什么是文本风格转换？ ‣ 深度学习文本风格转换综述")中的最右侧一列“Pa？”显示了每个TST子任务是否存在平行数据。在本节中，我们将介绍基于平行数据集的TST方法，而在第[5](#S5
    "5 非平行数据的方法 ‣ 深度学习文本风格转换综述")节中，我们将详细讨论基于非平行数据集的方法。为了便于读者理解，我们大多数情况下将解释在两个值之间的一个属性的TST，如在非正式和正式语调之间转换形式，这可以潜在地扩展到多个属性。
- en: 'Most methods adopt the standard neural sequence-to-sequence (seq2seq) model
    with the encoder-decoder architecture, which was initially developed for neural
    machine translation (NMT) (Sutskever, Vinyals, and Le, [2014](#bib.bib191); Bahdanau,
    Cho, and Bengio, [2015](#bib.bib6); Cho et al., [2014](#bib.bib32)) and extensively
    seen on text generation tasks such as summarization Rush, Chopra, and Weston ([2015](#bib.bib169))
    and many others Song et al. ([2019](#bib.bib186)). The encoder-decoder seq2seq
    model can be implemented by either LSTM as in  Rao and Tetreault ([2018](#bib.bib156));
    Shang et al. ([2019](#bib.bib178)) or Transformer Vaswani et al. ([2017](#bib.bib202))
    as in Xu, Ge, and Wei ([2019](#bib.bib220)). Copy mechanism Gülçehre et al. ([2016](#bib.bib57));
    See, Liu, and Manning ([2017](#bib.bib175)) is also added to better handle stretches
    of text which should not be changed (e.g., some proper nouns and rare words) (Gu
    et al., [2016](#bib.bib55); Merity et al., [2017](#bib.bib129)). Based on this
    architecture, recent works have developed multiple directions of improvement:
    multi-tasking, inference techniques, and data augmentation, which will be introduced
    below.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数方法采用标准的神经序列到序列（seq2seq）模型，具有编码器-解码器架构，这最初是为神经机器翻译（NMT）开发的 (Sutskever, Vinyals,
    and Le, [2014](#bib.bib191); Bahdanau, Cho, and Bengio, [2015](#bib.bib6); Cho
    et al., [2014](#bib.bib32))，并广泛应用于文本生成任务，如摘要生成 Rush, Chopra, and Weston ([2015](#bib.bib169))
    和其他许多任务 Song et al. ([2019](#bib.bib186))。编码器-解码器 seq2seq 模型可以通过 LSTM 实现，如 Rao
    和 Tetreault ([2018](#bib.bib156)); Shang et al. ([2019](#bib.bib178))，或者通过 Transformer
    Vaswani et al. ([2017](#bib.bib202)) 实现，如 Xu, Ge, and Wei ([2019](#bib.bib220))。还加入了复制机制
    Gülçehre et al. ([2016](#bib.bib57)); See, Liu, and Manning ([2017](#bib.bib175))，以更好地处理不应更改的文本部分（例如，某些专有名词和稀有词汇）
    (Gu et al., [2016](#bib.bib55); Merity et al., [2017](#bib.bib129))。基于这一架构，最近的工作已经开发了多个改进方向：多任务处理、推断技术和数据增强，下面将介绍这些内容。
- en: 4.0.0.0.1 Multi-Tasking.
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.0.0.0.1 多任务处理。
- en: 'In addition to the seq2seq learning on paired attributed-text, Xu, Ge, and
    Wei ([2019](#bib.bib220)) propose adding three other loss functions: (1) classifier-guided
    loss, which is calculated using a well-trained attribute classifier and encourages
    the model to generate sentences conforming to the target attribute, (2) self-reconstruction
    loss, which encourages the seq2seq model to reconstruct the input itself by specifying
    the desired style the same as the input style, and (3) cycle loss, which first
    transfers the input sentence to the target attribute and then transfers the output
    back to its original attribute. Each of the three losses can gain performance
    improvement of 1-5 BLEU points with the human references Xu, Ge, and Wei ([2019](#bib.bib220)).
    Another type of multi-tasking is to jointly learn TST and machine translation
    from French to English, which improves the performance by 1 BLEU score with human-written
    references Niu, Rao, and Carpuat ([2018](#bib.bib137)). Specific for formality
    transfer, Zhang, Ge, and Sun ([2020](#bib.bib232)) multi-task TST and grammar
    error correction (GEC) so that knowledge from GEC data can be transferred to the
    informal-to-formal style transfer task.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在配对的属性文本上进行seq2seq学习外，Xu、Ge和Wei（[2019](#bib.bib220)）还提出了增加三种其他损失函数：（1）分类器指导损失，它使用经过良好训练的属性分类器进行计算，鼓励模型生成符合目标属性的句子；（2）自重建损失，它鼓励seq2seq模型通过指定与输入样式相同的期望样式来重建输入；（3）循环损失，它首先将输入句子转换为目标属性，然后将输出转换回其原始属性。这三种损失中的每一种都可以在使用人类参考的情况下提高1-5
    BLEU分数（Xu、Ge和Wei（[2019](#bib.bib220)））。另一种多任务学习方式是联合学习TST和法语到英语的机器翻译，这种方法在使用人类书写的参考文献的情况下提高了1
    BLEU分数（Niu、Rao和Carpuat（[2018](#bib.bib137)））。针对正式性转换，Zhang、Ge和Sun（[2020](#bib.bib232)）将TST和语法错误纠正（GEC）多任务结合，使得来自GEC数据的知识可以转移到非正式到正式的风格转换任务中。
- en: Apart from the additional loss designs, using the pretrained language model
    GPT-2 (Radford et al., [2019](#bib.bib154)) can lead to improvement by at least
    7 BLEU scores with human references Wang et al. ([2019](#bib.bib207)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 除了额外的损失设计外，使用预训练的语言模型GPT-2（Radford等，[2019](#bib.bib154)）可以使人类参考下的BLEU分数至少提高7分（Wang等，[2019](#bib.bib207)）。
- en: 4.0.0.0.2 Inference Techniques.
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.0.0.0.2 推理技术。
- en: To avoid the model copying too many parts of the input sentence and not performing
    sufficient edits to flip the attribute, Kajiwara ([2019](#bib.bib84)) first identify
    words in the source sentence requiring replacement, and then change the words
    by negative lexically constrained decoding (Post and Vilar, [2018](#bib.bib147))
    that avoids naive copying. Since this method only changes the beam search process
    for model inference, it can be applied to any text style transfer model without
    model re-training.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免模型复制过多的输入句子部分而未能充分编辑以翻转属性，Kajiwara（[2019](#bib.bib84)）首先识别源句子中需要替换的单词，然后通过负词汇约束解码（Post和Vilar，[2018](#bib.bib147)）来改变这些单词，以避免简单的复制。由于这种方法仅改变了模型推理的束搜索过程，因此可以应用于任何文本风格转换模型，而无需重新训练模型。
- en: 4.0.0.0.3 Data Augmentation.
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 4.0.0.0.3 数据增强。
- en: Since style transfer data is expensive to annotate, there are not as many parallel
    datasets as in machine translation. Hence, various methods have been proposed
    for data augmentation to enrich the data. For example, Rao and Tetreault ([2018](#bib.bib156))
    first train a phrase-based machine translation (PBMT) model on a given parallel
    dataset and then use back-translation (Sennrich, Haddow, and Birch, [2016b](#bib.bib177))
    to construct a pseudo-parallel dataset as additional training data, which leads
    to an improvement of around 9.7 BLEU scores with respect to human written references.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 由于风格转换数据的标注成本较高，因此没有像机器翻译中那样多的平行数据集。因此，提出了各种数据增强方法来丰富数据。例如，Rao和Tetreault（[2018](#bib.bib156)）首先在给定的平行数据集上训练一个基于短语的机器翻译（PBMT）模型，然后使用回译（Sennrich、Haddow和Birch，[2016b](#bib.bib177)）构建一个伪平行数据集作为额外的训练数据，这导致相对于人工书写的参考文献，BLEU分数提高了约9.7分。
- en: Most recently, Zhang, Ge, and Sun ([2020](#bib.bib232)) use a data augmentation
    technique by making use of largely available online text. They scrape informal
    text from online forums and generate back-translations, i.e., informal English
    $\rightarrow$ a pivot language such as French $\rightarrow$ formal English, where
    the formality of the back-translated English text is ensured with a formality
    classifier that is used to only keep text that is classified as formal text.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，张、葛和孙 ([2020](#bib.bib232)) 使用了一种通过利用大量可用在线文本的数据增强技术。他们从在线论坛抓取非正式文本，并生成反向翻译，即，非正式英语
    $\rightarrow$ 一个中介语言如法语 $\rightarrow$ 正式英语，其中通过形式分类器确保反向翻译的英语文本的正式性，该分类器只保留被分类为正式文本的内容。
- en: 5 Methods on Non-Parallel Data
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 种非平行数据方法
- en: 'Parallel data for TST is difficult to obtain, and for some styles impossible
    to crowd-source (e.g., Mark Twain novels rewritten in Hemmingway’s style). Hence,
    the majority TST methods assume only non-parallel mono-style corpora, and investigate
    how to build deep learning models based on this constraint. In this section, we
    will introduce three main branches of TST methods: disentanglement (Section [5.1](#S5.SS1
    "5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey")), prototype editing (Section [5.2](#S5.SS2 "5.2 Prototype
    Editing ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer:
    A Survey")), and pseudo-parallel corpus construction (Section [5.3](#S5.SS3 "5.3
    Pseudo-Parallel Corpus Construction ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning
    for Text Style Transfer: A Survey")).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 'TST 的平行数据难以获取，对于某些风格更是无法众包（例如，马克·吐温的小说改写成海明威风格）。因此，大多数 TST 方法假设只有非平行的单风格语料，并研究如何在这个约束下构建深度学习模型。在这一部分，我们将介绍
    TST 方法的三个主要分支：解耦（第[5.1节](#S5.SS1 "5.1 Disentanglement ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey")）、原型编辑（第[5.2节](#S5.SS2
    "5.2 Prototype Editing ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey")）和伪平行语料库构建（第[5.3节](#S5.SS3 "5.3 Pseudo-Parallel Corpus
    Construction ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer:
    A Survey")）。'
- en: 5.1 Disentanglement
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 解耦
- en: 'Disentanglement-based models usually perform the following three actions:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 基于解耦的模型通常执行以下三个操作：
- en: •
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Encode the text $\bm{x}$ with attribute $a$ into a latent representation $\bm{z}$
    (i.e., $\bm{x}\rightarrow\bm{z}$)
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将属性 $a$ 的文本 $\bm{x}$ 编码为潜在表示 $\bm{z}$（即，$\bm{x}\rightarrow\bm{z}$）
- en: •
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Manipulate the latent representation $\bm{z}$ to remove the source attribute
    (i.e., $\bm{z}\rightarrow\bm{z}^{\prime}$)
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 操作潜在表示 $\bm{z}$ 以去除源属性（即，$\bm{z}\rightarrow\bm{z}^{\prime}$）
- en: •
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Decode into text $\bm{x}^{\prime}$ with the target attribute $a^{\prime}$ (i.e.,
    $\bm{z}^{\prime}\rightarrow\bm{x}^{\prime}$)
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将文本 $\bm{x}^{\prime}$ 解码为目标属性 $a^{\prime}$（即，$\bm{z}^{\prime}\rightarrow\bm{x}^{\prime}$）
- en: 'To build such models, the common workflow in disentanglement papers consists
    of the following three steps:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建这样的模型，解耦论文中的常见工作流程包括以下三个步骤：
- en: Step 1)
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 1)
- en: 'Select a model as the backbone for the encoder-decoder learning (Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 Encoder-Decoder Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey"))'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '选择一个模型作为编码器-解码器学习的主干（第[5.1.1节](#S5.SS1.SSS1 "5.1.1 Encoder-Decoder Training
    Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning
    for Text Style Transfer: A Survey")）'
- en: Step 2)
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 2)
- en: 'Select a manipulation method of the latent representation (Section [5.1.2](#S5.SS1.SSS2
    "5.1.2 Latent Representation Manipulation ‣ 5.1 Disentanglement ‣ 5 Methods on
    Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey"))'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '选择潜在表示的操作方法（第[5.1.2节](#S5.SS1.SSS2 "5.1.2 Latent Representation Manipulation
    ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey")）'
- en: Step 3)
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 3)
- en: 'For the manipulation method chosen above, select (multiple) appropriate loss
    functions (Section [5.1.3](#S5.SS1.SSS3 "5.1.3 Training Objectives ‣ 5.1 Disentanglement
    ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey"))'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '对于上述选择的操作方法，选择（多个）适当的损失函数（第[5.1.3节](#S5.SS1.SSS3 "5.1.3 Training Objectives
    ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey")）'
- en: 'The organization of this section starts with Section [5.1.1](#S5.SS1.SSS1 "5.1.1
    Encoder-Decoder Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey") which introduces the
    encoder-decoder training objectives that is used for Step [1](#S5.I2.i1 "item
    1 ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey"). Next, Section [5.1.2](#S5.SS1.SSS2 "5.1.2 Latent Representation
    Manipulation ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning
    for Text Style Transfer: A Survey") overviews three main approaches to manipulate
    the latent representation for Step [2](#S5.I2.i2 "item 2 ‣ 5.1 Disentanglement
    ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey"),
    and Section [5.1.3](#S5.SS1.SSS3 "5.1.3 Training Objectives ‣ 5.1 Disentanglement
    ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey")
    goes through a plethora of training objectives for Step [3](#S5.I2.i3 "item 3
    ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey"). Table [5](#S5.T5 "Table 5 ‣ 5.1 Disentanglement ‣
    5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey")
    provides an overview of existing models and their corresponding configurations.
    To give a rough idea of the effectiveness of each model, we show their performance
    on the Yelp dataset.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的组织从第[5.1.1节](#S5.SS1.SSS1 "5.1.1 编码器-解码器训练方法 ‣ 5.1 解缠结 ‣ 5 种非平行数据方法 ‣ 文本风格转换的深度学习：综述")开始，介绍了用于第[1步](#S5.I2.i1
    "第 1 项 ‣ 5.1 解缠结 ‣ 5 种非平行数据方法 ‣ 文本风格转换的深度学习：综述")的编码器-解码器训练目标。接着，第[5.1.2节](#S5.SS1.SSS2
    "5.1.2 潜在表示操控 ‣ 5.1 解缠结 ‣ 5 种非平行数据方法 ‣ 文本风格转换的深度学习：综述")概述了操控潜在表示的三种主要方法，用于第[2步](#S5.I2.i2
    "第 2 项 ‣ 5.1 解缠结 ‣ 5 种非平行数据方法 ‣ 文本风格转换的深度学习：综述")，而第[5.1.3节](#S5.SS1.SSS3 "5.1.3
    训练目标 ‣ 5.1 解缠结 ‣ 5 种非平行数据方法 ‣ 文本风格转换的深度学习：综述")则讲解了第[3步](#S5.I2.i3 "第 3 项 ‣ 5.1
    解缠结 ‣ 5 种非平行数据方法 ‣ 文本风格转换的深度学习：综述")的众多训练目标。表[5](#S5.T5 "表 5 ‣ 5.1 解缠结 ‣ 5 种非平行数据方法
    ‣ 文本风格转换的深度学习：综述")提供了现有模型及其对应配置的概述。为了大致了解每个模型的有效性，我们展示了它们在Yelp数据集上的性能。
- en: 'Table 5: Summary of existing disentanglement-based methods and the setting
    they adopted, with a reference of their performance on the Yelp dataset. For the
    settings, we include the encoder-decoder training method (Enc-Dec) in Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 Encoder-Decoder Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey"), the disentanglement
    method (Disen.) in Section [5.1.2](#S5.SS1.SSS2 "5.1.2 Latent Representation Manipulation
    ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey"), and the loss types used to control style (Style Control)
    and content (Content Control) in Section [5.1.3](#S5.SS1.SSS3 "5.1.3 Training
    Objectives ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning
    for Text Style Transfer: A Survey"). For the model performance, we report automatic
    evaluation scores including BLEU with the one human reference (BL-Ref) provided
    by Li et al. ([2018](#bib.bib107)), accuracy (Acc.), BLEU with the input (BL-Inp)
    and perplexity (PPL). ^∗ marks numbers reported by Liu et al. ([2020](#bib.bib113)).
    Readers can refer to Hu, Lee, and Aggarwal ([2020](#bib.bib71)) for more complete
    performance results on Yelp.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：现有解缠结方法的总结及其采用的设置，并参考了它们在Yelp数据集上的表现。对于设置，我们包括第[5.1.1节](#S5.SS1.SSS1 "5.1.1
    编码器-解码器训练方法 ‣ 5.1 解缠结 ‣ 5 种非平行数据方法 ‣ 文本风格转换的深度学习：综述")中的编码器-解码器训练方法（Enc-Dec），第[5.1.2节](#S5.SS1.SSS2
    "5.1.2 潜在表示操控 ‣ 5.1 解缠结 ‣ 5 种非平行数据方法 ‣ 文本风格转换的深度学习：综述")中的解缠结方法（Disen.），以及第[5.1.3节](#S5.SS1.SSS3
    "5.1.3 训练目标 ‣ 5.1 解缠结 ‣ 5 种非平行数据方法 ‣ 文本风格转换的深度学习：综述")中用于控制风格（风格控制）和内容（内容控制）的损失类型。对于模型性能，我们报告了自动评估分数，包括BLEU与一个人工参考（BLEU-参考），由Li等人（[2018](#bib.bib107)）提供，准确率（准确率），BLEU与输入（BLEU-输入）和困惑度（困惑度）。^∗标记了Liu等人（[2020](#bib.bib113)）报告的数字。读者可以参考Hu,
    Lee和Aggarwal（[2020](#bib.bib71)）了解关于Yelp的更完整性能结果。
- en: '|  | Settings | Performance on Yelp |  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | 设置 | Yelp上的性能 |  |'
- en: '|  | Enc-Dec | Disen. | Style Control | Content Control | BL-Ref | Acc. (%)
    | BL-Inp | PPL$\downarrow$ |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | 编码器-解码器 | 解缠结 | 风格控制 | 内容控制 | BLEU-参考 | 准确率 (%) | BLEU-输入 | 困惑度$\downarrow$
    |'
- en: '| Mueller, Gifford, and Jaakkola ([2017](#bib.bib132)) | VAE | LRE | – | –
    | – | – | – | – |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Mueller, Gifford 和 Jaakkola ([2017](#bib.bib132)) | VAE | LRE | – | – | –
    | – | – | – |'
- en: '| Hu et al. ([2017](#bib.bib72)) | VAE | ACC | ACO | – | 22.3 | 86.7 | 58.4
    | – |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Hu 等人 ([2017](#bib.bib72)) | VAE | ACC | ACO | – | 22.3 | 86.7 | 58.4 | –
    |'
- en: '| Shen et al. ([2017](#bib.bib182)) | AE&GAN | ACC | AdvR$\parallel$AdvO |
    – | 7.8 | 73.9 | 20.7 | 72^∗ |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Shen 等人 ([2017](#bib.bib182)) | AE&GAN | ACC | AdvR$\parallel$AdvO | – |
    7.8 | 73.9 | 20.7 | 72^∗ |'
- en: '| Fu et al. ([2018](#bib.bib41)) | AE | ACC | AdvR | – | 12.9 | 46.9 | 40.1
    | 166.5^∗ |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Fu 等人 ([2018](#bib.bib41)) | AE | ACC | AdvR | – | 12.9 | 46.9 | 40.1 | 166.5^∗
    |'
- en: '| Prabhumoye et al. ([2018](#bib.bib149)) | AE | ACC | ACO | – | 6.8 | 87.2
    | – | 32.8^∗ |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| Prabhumoye 等人 ([2018](#bib.bib149)) | AE | ACC | ACO | – | 6.8 | 87.2 | –
    | 32.8^∗ |'
- en: '| Zhao et al. ([2018](#bib.bib235)) | GAN | ACC | AdvR | – | – | 73.4 | 31.2
    | 29.7 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Zhao 等人 ([2018](#bib.bib235)) | GAN | ACC | AdvR | – | – | 73.4 | 31.2 |
    29.7 |'
- en: '| Yang et al. ([2018](#bib.bib224)) | AE | ACC | LMO | – | – | 91.2 | 57.8
    | 47.0&60.9 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Yang 等人 ([2018](#bib.bib224)) | AE | ACC | LMO | – | – | 91.2 | 57.8 | 47.0&60.9
    |'
- en: '| Logeswaran, Lee, and Bengio ([2018](#bib.bib116)) | AE | ACC | AdvO | Cycle
    | – | 90.5 | – | 133 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Logeswaran, Lee 和 Bengio ([2018](#bib.bib116)) | AE | ACC | AdvO | Cycle
    | – | 90.5 | – | 133 |'
- en: '| Tian, Hu, and Yu ([2018](#bib.bib195)) | AE | ACC | AdvO | Noun | 24.9 |
    92.7 | 63.3 | – |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| Tian, Hu 和 Yu ([2018](#bib.bib195)) | AE | ACC | AdvO | Noun | 24.9 | 92.7
    | 63.3 | – |'
- en: '| Liao et al. ([2018](#bib.bib111)) | VAE | LRE | – | – | – | 88.3 | – | –
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| Liao 等人 ([2018](#bib.bib111)) | VAE | LRE | – | – | – | 88.3 | – | – |'
- en: '| Romanov et al. ([2019](#bib.bib167)) | AE | LRS | ACR&AdvR | – | – | – |
    – | – |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Romanov 等人 ([2019](#bib.bib167)) | AE | LRS | ACR&AdvR | – | – | – | – |
    – |'
- en: '| John et al. ([2019](#bib.bib83)) | AE&VAE | LRS | ACR&AdvR | BoW&AdvBoW |
    – | 93.4 | – | – |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| John 等人 ([2019](#bib.bib83)) | AE&VAE | LRS | ACR&AdvR | BoW&AdvBoW | – |
    93.4 | – | – |'
- en: '| Bao et al. ([2019](#bib.bib8)) | VAE | LRS | ACR&AdvR | BoW&AdvBoW | – |
    – | – | – |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Bao 等人 ([2019](#bib.bib8)) | VAE | LRS | ACR&AdvR | BoW&AdvBoW | – | – |
    – | – |'
- en: '| Dai et al. ([2019](#bib.bib34)) | AE | ACC | ACO | Cycle | 20.3 | 87.7 |
    54.9 | 73 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Dai 等人 ([2019](#bib.bib34)) | AE | ACC | ACO | Cycle | 20.3 | 87.7 | 54.9
    | 73 |'
- en: '| Wang, Hua, and Wan ([2019](#bib.bib206)) | AE | LRE | – | – | 24.6 | 95.4
    | – | 46.2 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Wang, Hua 和 Wan ([2019](#bib.bib206)) | AE | LRE | – | – | 24.6 | 95.4 |
    – | 46.2 |'
- en: '| Li et al. ([2020](#bib.bib110)) | GAN | ACC | ACO&AdvR | – | – | 95.5 | 53.3
    | – |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Li 等人 ([2020](#bib.bib110)) | GAN | ACC | ACO&AdvR | – | – | 95.5 | 53.3
    | – |'
- en: '| Liu et al. ([2020](#bib.bib113)) | VAE | LRE | – | – | 18.8 | 92.3 | – |
    18.3 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 ([2020](#bib.bib113)) | VAE | LRE | – | – | 18.8 | 92.3 | – | 18.3
    |'
- en: '| Yi et al. ([2020](#bib.bib225)) | VAE | ACC | ACO | Cycle | 26.0 | 90.8 |
    – | 109 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Yi 等人 ([2020](#bib.bib225)) | VAE | ACC | ACO | Cycle | 26.0 | 90.8 | – |
    109 |'
- en: '| Jin et al. ([2020a](#bib.bib80)) | AE | LRE | – | – | – | – | – | – |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Jin 等人 ([2020a](#bib.bib80)) | AE | LRE | – | – | – | – | – | – |'
- en: 5.1.1 Encoder-Decoder Training Method
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 编码器-解码器训练方法
- en: 'There are three model choices to obtain the latent representation $\bm{z}$
    from the discrete text $\bm{x}$ and then decode it into the new text $\bm{x}^{\prime}$
    via reconstruction training: auto-encoder (AE), variational auto-encoder (VAE),
    and generative adversarial networks (GANs).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种模型选择来获取从离散文本 $\bm{x}$ 到潜在表示 $\bm{z}$，然后通过重建训练将其解码为新文本 $\bm{x}^{\prime}$：自编码器（AE）、变分自编码器（VAE）和生成对抗网络（GANs）。
- en: 5.1.1.0.1 Auto-Encoder (AE).
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.1.0.1 自编码器（AE）。
- en: Auto-encoding is a commonly used method to learn the latent representation $\bm{z}$,
    which first encodes the input sentence $\bm{x}$ into a latent vector $\bm{z}$
    and then reconstructs a sentence as similar to the input sentence as possible.
    AE is used in many TST works (e.g., Shen et al., [2017](#bib.bib182); Hu et al.,
    [2017](#bib.bib72); Fu et al., [2018](#bib.bib41); Zhao et al., [2018](#bib.bib235);
    Prabhumoye et al., [2018](#bib.bib149); Yang et al., [2018](#bib.bib224)). To
    avoid auto-encoding from blindly copying all the elements from the input, Hill,
    Cho, and Korhonen ([2016](#bib.bib65)) adopt denoising auto-encoding (DAE) Vincent
    et al. ([2010](#bib.bib203)) to replace AE in NLP tasks. Specifically, DAE first
    passes the input sentence $\bm{x}$ through a noise model to randomly drop, shuffle,
    or mask some words, and then reconstructs the original sentence from this corrupted
    sentence. This idea is used in later TST works, e.g., Lample et al. ([2019](#bib.bib102));
    Jin et al. ([2020a](#bib.bib80)). As pre-trained models became prevalent in recent
    years, the DAE training method has increased in popularity relative to its counterparts
    such as GAN and VAE, because pre-training over large corpora can grant models
    better performance in terms of semantic preservation and fluency (Lai, Toral,
    and Nissim, [2021](#bib.bib98); Riley et al., [2021b](#bib.bib165)).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码（Auto-encoding）是一种常用的方法，用于学习潜在表示 $\bm{z}$，该方法首先将输入句子 $\bm{x}$ 编码成潜在向量 $\bm{z}$，然后重建一个尽可能与输入句子相似的句子。AE
    被许多 TST 工作（例如，Shen 等人，[2017](#bib.bib182)；Hu 等人，[2017](#bib.bib72)；Fu 等人，[2018](#bib.bib41)；Zhao
    等人，[2018](#bib.bib235)；Prabhumoye 等人，[2018](#bib.bib149)；Yang 等人，[2018](#bib.bib224)）中使用。为了避免自编码盲目地复制输入中的所有元素，Hill、Cho
    和 Korhonen（[2016](#bib.bib65)）采用了去噪自编码（DAE）Vincent 等人（[2010](#bib.bib203)）来替代
    NLP 任务中的 AE。具体来说，DAE 首先通过噪声模型对输入句子 $\bm{x}$ 进行处理，随机丢弃、打乱或屏蔽一些词，然后从这个被破坏的句子中重建原始句子。这一思想被后来的
    TST 工作所采用，例如 Lample 等人（[2019](#bib.bib102)）；Jin 等人（[2020a](#bib.bib80)）。随着近年来预训练模型的普及，DAE
    训练方法的受欢迎程度相较于 GAN 和 VAE 等其他方法有所增加，因为在大语料库上进行预训练可以提高模型在语义保持和流畅性方面的表现（Lai、Toral
    和 Nissim，[2021](#bib.bib98)；Riley 等人，[2021b](#bib.bib165)）。
- en: 5.1.1.0.2 Variational Auto-Encoder (VAE).
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.1.0.2 变分自编码器（VAE）。
- en: Instead of reconstructing data based on the deterministic latent representations
    by AE, a variational auto-encoder (VAE) (Kingma and Welling, [2014](#bib.bib92);
    Rezende, Mohamed, and Wierstra, [2014](#bib.bib162)) reconstructs data based on
    the sampled latent vector from its posterior, and use the regularization by Kullback–Leibler
    divergence. VAE is also commonly used in TST works Mueller, Gifford, and Jaakkola
    ([2017](#bib.bib132)); Hu et al. ([2017](#bib.bib72)); Liu et al. ([2020](#bib.bib113));
    Liao et al. ([2018](#bib.bib111)); Yi et al. ([2020](#bib.bib225)); Tikhonov et al.
    ([2019](#bib.bib196)). The VAE loss is formulated as
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AE 基于确定性潜在表示重建数据不同，变分自编码器（VAE）（Kingma 和 Welling，[2014](#bib.bib92)；Rezende、Mohamed
    和 Wierstra，[2014](#bib.bib162)）基于从其后验分布中采样的潜在向量重建数据，并使用 Kullback–Leibler 散度进行正则化。VAE
    也在许多 TST 工作中广泛使用，如 Mueller、Gifford 和 Jaakkola（[2017](#bib.bib132)）；Hu 等人（[2017](#bib.bib72)）；Liu
    等人（[2020](#bib.bib113)）；Liao 等人（[2018](#bib.bib111)）；Yi 等人（[2020](#bib.bib225)）；Tikhonov
    等人（[2019](#bib.bib196)）。VAE 的损失函数被表述为
- en: '|  | $\begin{split}\mathcal{L}_{\mathrm{VAE}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}})=-\mathbb{E}_{q_{\mathrm{E}}(\bm{z}&#124;\bm{x})}\log
    p_{\mathrm{G}}(\bm{x}&#124;\bm{z})+\lambda\mathrm{KL}\Big{[}q_{\mathrm{E}}(\bm{z}&#124;\bm{x})&#124;&#124;p(\bm{z})\Big{]},\end{split}$
    |  | (1) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}_{\mathrm{VAE}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}})=-\mathbb{E}_{q_{\mathrm{E}}(\bm{z}&#124;\bm{x})}\log
    p_{\mathrm{G}}(\bm{x}&#124;\bm{z})+\lambda\mathrm{KL}\Big{[}q_{\mathrm{E}}(\bm{z}&#124;\bm{x})&#124;&#124;p(\bm{z})\Big{]},\end{split}$
    |  | (1) |'
- en: where $\lambda$ is the hyper-parameter to balance the reconstruction loss and
    the KL term, $p(\bm{z})$ is the prior drawn from the standard normal distribution
    of $\mathcal{N}(\bm{0},\bm{I})$, and $q_{\mathrm{E}}(\bm{z}|\bm{x})$ is the posterior
    in the form of $\mathcal{N}(\bm{\mu},\bm{\sigma})$, where $\bm{\mu}$ and $\bm{\sigma}$
    are predicted by the encoder.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是平衡重建损失和KL项的超参数，$p(\bm{z})$ 是从标准正态分布 $\mathcal{N}(\bm{0},\bm{I})$
    中抽取的先验分布，$q_{\mathrm{E}}(\bm{z}|\bm{x})$ 是以 $\mathcal{N}(\bm{\mu},\bm{\sigma})$
    形式的后验分布，其中 $\bm{\mu}$ 和 $\bm{\sigma}$ 由编码器预测。
- en: '![Refer to caption](img/57fc4a457226570cf3be64e93189883d.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/57fc4a457226570cf3be64e93189883d.png)'
- en: (a) Latent Representation Editing
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 潜在表示编辑
- en: '![Refer to caption](img/d1ac029aaefbf8792d06e1b352d85a0c.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d1ac029aaefbf8792d06e1b352d85a0c.png)'
- en: (b) Attribute Code Control
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 属性代码控制
- en: '![Refer to caption](img/af47a039b056cb9015eac611b468a721.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/af47a039b056cb9015eac611b468a721.png)'
- en: (c) Latent Representation Splitting
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 潜在表示分裂
- en: 'Figure 2: Three methods to manipulate the latent space based on disentanglement
    for text style transfer.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：基于解耦的文本风格迁移的潜在空间操作三种方法。
- en: 5.1.1.0.3 Generative Adversarial Networks (GANs).
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.1.0.3 生成对抗网络（GANs）。
- en: 'GANs (Goodfellow et al., [2014](#bib.bib51)) can also be applied to TST Shen
    et al. ([2017](#bib.bib182)); Zhao et al. ([2018](#bib.bib235)); Li et al. ([2020](#bib.bib110)).
    The way GANs work is to first approximate the samples drawn from a true distribution
    $\bm{z}$ by employing a noise sample $\bm{s}$ and a generator function $G$ to
    produce $\widehat{\bm{z}}=G(\bm{s})$. Next, a critic/discriminator $f_{c}(\bm{z})$
    is used to distinguish real data and generated samples. The critic is trained
    to distinguish the real samples from generated samples, and the generator is trained
    to fool the critic. Formally, the training process is expressed as a min-max game
    played among the encoder $E$, generator $G$, and the critic $f_{c}$:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: GANs（Goodfellow 等人，[2014](#bib.bib51)）也可以应用于 TST Shen 等人 ([2017](#bib.bib182))；Zhao
    等人 ([2018](#bib.bib235))；Li 等人 ([2020](#bib.bib110))。GANs 的工作方式是首先通过使用噪声样本 $\bm{s}$
    和生成函数 $G$ 来逼近从真实分布 $\bm{z}$ 中提取的样本，生成 $\widehat{\bm{z}}=G(\bm{s})$。接下来，使用一个判别器/鉴别器
    $f_{c}(\bm{z})$ 来区分真实数据和生成样本。判别器被训练来区分真实样本和生成样本，而生成器则被训练来欺骗判别器。形式上，训练过程表示为在编码器
    $E$、生成器 $G$ 和判别器 $f_{c}$ 之间进行的最小化-最大化游戏：
- en: '|  | $\max_{c}\min_{E,G}\mathcal{L}_{\mathrm{GAN}}=-\mathbb{E}_{p(\bm{z})}\log
    p_{\mathrm{G}}(\bm{x}&#124;\bm{z})+\mathbb{E}_{p(\bm{z})}f_{c}(\bm{z})-\mathbb{E}_{p(\widehat{\bm{z}})}f_{c}(\widehat{\bm{z}}).$
    |  | (2) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{c}\min_{E,G}\mathcal{L}_{\mathrm{GAN}}=-\mathbb{E}_{p(\bm{z})}\log
    p_{\mathrm{G}}(\bm{x}&#124;\bm{z})+\mathbb{E}_{p(\bm{z})}f_{c}(\bm{z})-\mathbb{E}_{p(\widehat{\bm{z}})}f_{c}(\widehat{\bm{z}}).$
    |  | (2) |'
- en: 5.1.2 Latent Representation Manipulation
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 潜在表示操作
- en: 'Based on the general encoder and decoder training method, the core element
    of disentanglement is the manipulation of latent representation $\bm{z}$. Figure
    [2](#S5.F2 "Figure 2 ‣ 5.1.1.0.2 Variational Auto-Encoder (VAE). ‣ 5.1.1 Encoder-Decoder
    Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey") illustrates three main methods: latent
    representation editing, attribute code control, and latent representation splitting.
    In addition, the “Disen.” column of Table [5](#S5.T5 "Table 5 ‣ 5.1 Disentanglement
    ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey")
    shows the type of latent representation manipulation for each work in disentanglement.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '基于通用编码器和解码器训练方法，解耦的核心元素是潜在表示 $\bm{z}$ 的操作。图 [2](#S5.F2 "Figure 2 ‣ 5.1.1.0.2
    Variational Auto-Encoder (VAE). ‣ 5.1.1 Encoder-Decoder Training Method ‣ 5.1
    Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style
    Transfer: A Survey") 说明了三种主要方法：潜在表示编辑、属性编码控制和潜在表示分裂。此外，表 [5](#S5.T5 "Table 5 ‣
    5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey") 的“Disen.” 列显示了每项工作在解耦中的潜在表示操作类型。'
- en: 'The first approach, Latent Representation Editing (LRE), shown in Figure [2(a)](#S5.F2.sf1
    "In Figure 2 ‣ 5.1.1.0.2 Variational Auto-Encoder (VAE). ‣ 5.1.1 Encoder-Decoder
    Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey"), is achieved by ensuring two properties
    of the latent representation $\bm{z}$. The first property is that $\bm{z}$ should
    be able to serve as the latent representation for auto-encoding, namely aligning
    $f_{c}(\bm{z})$ with the input $\bm{x}$, where $\bm{z}\overset{\Delta}{=}E(\bm{x})$.
    The second property is that $\bm{z}$ should be learned such that it incorporates
    the new attribute value of interest $a^{\prime}$. To achieve this, the common
    practice is to first learn an attribute classifier $f_{c}$, e.g., a multilayer
    perceptron (MLP) that takes the latent representation $\bm{z}$ as input, and then
    iteratively update $\bm{z}$ within the constrained space by the first property
    and in the same time maximize the prediction confidence score regarding $a^{\prime}$
    by this attribute classifier  (Mueller, Gifford, and Jaakkola, [2017](#bib.bib132);
    Liao et al., [2018](#bib.bib111); Wang, Hua, and Wan, [2019](#bib.bib206); Liu
    et al., [2020](#bib.bib113)). An alternative way to achieve the second property
    is to multi-task by another auto-encoding task on the corpus with the attribute
    $a^{\prime}$ and share most layers of the transformer except the query transformation
    and layer normalization layers Jin et al. ([2020a](#bib.bib80)).'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法，潜在表示编辑（LRE），如图 [2(a)](#S5.F2.sf1 "图2 ‣ 5.1.1.0.2 变分自编码器 (VAE) ‣ 5.1.1
    编码器-解码器训练方法 ‣ 5.1 解缠结 ‣ 5 非平行数据方法 ‣ 深度学习文本风格迁移：综述") 所示，通过确保潜在表示 $\bm{z}$ 的两个属性来实现。第一个属性是
    $\bm{z}$ 应该能够作为自编码的潜在表示，即将 $f_{c}(\bm{z})$ 与输入 $\bm{x}$ 对齐，其中 $\bm{z}\overset{\Delta}{=}E(\bm{x})$。第二个属性是
    $\bm{z}$ 应该被学习以包含感兴趣的新属性值 $a^{\prime}$。为实现这一点，常见做法是首先学习一个属性分类器 $f_{c}$，例如，一个将潜在表示
    $\bm{z}$ 作为输入的多层感知器（MLP），然后在第一个属性的约束空间内迭代更新 $\bm{z}$，同时通过该属性分类器最大化关于 $a^{\prime}$
    的预测置信度得分  (Mueller, Gifford, 和 Jaakkola, [2017](#bib.bib132)；Liao 等, [2018](#bib.bib111)；Wang,
    Hua, 和 Wan, [2019](#bib.bib206)；Liu 等, [2020](#bib.bib113))。实现第二个属性的另一种方式是通过另一个自编码任务进行多任务学习，任务涉及具有属性
    $a^{\prime}$ 的语料库，并且共享大部分转换器层，除了查询转换和层归一化层 Jin 等 ([2020a](#bib.bib80))。
- en: 'The second approach, Attribute Code Control (ACC), as shown in Figure [2(b)](#S5.F2.sf2
    "In Figure 2 ‣ 5.1.1.0.2 Variational Auto-Encoder (VAE). ‣ 5.1.1 Encoder-Decoder
    Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey"), first enforces the latent representation
    $\bm{z}$ of the sentence $\bm{x}$ to contain all information except its attribute
    value $a$ via adversarial learning, and then the transferred output is decoded
    based on the combination of $\bm{z}$ and a structured attribute code $\bm{a}$
    corresponding to the attribute value $a$. During the decoding process, the attribute
    code vector $\bm{a}$ controls the attribute of generated text by acting as either
    the initial state (Shen et al., [2017](#bib.bib182); Yi et al., [2020](#bib.bib225))
    or the embedding (Fu et al., [2018](#bib.bib41); Dai et al., [2019](#bib.bib34)).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法，属性代码控制（ACC），如图 [2(b)](#S5.F2.sf2 "图2 ‣ 5.1.1.0.2 变分自编码器 (VAE) ‣ 5.1.1
    编码器-解码器训练方法 ‣ 5.1 解缠结 ‣ 5 非平行数据方法 ‣ 深度学习文本风格迁移：综述") 所示，首先通过对抗学习强制句子 $\bm{x}$ 的潜在表示
    $\bm{z}$ 包含所有信息，除了其属性值 $a$，然后基于 $\bm{z}$ 和对应于属性值 $a$ 的结构化属性代码 $\bm{a}$ 的组合解码转移后的输出。在解码过程中，属性代码向量
    $\bm{a}$ 通过作为初始状态 (Shen 等, [2017](#bib.bib182)；Yi 等, [2020](#bib.bib225)) 或嵌入
    (Fu 等, [2018](#bib.bib41)；Dai 等, [2019](#bib.bib34)) 控制生成文本的属性。
- en: 'The third approach, Latent Representation Splitting (LRS), as illustrated in
    Figure [2(c)](#S5.F2.sf3 "In Figure 2 ‣ 5.1.1.0.2 Variational Auto-Encoder (VAE).
    ‣ 5.1.1 Encoder-Decoder Training Method ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey"), first disentangles the
    input text into two parts: the latent attribute representation $\bm{a}$, and semantic
    representation $\bm{z}$ that captures attribute-independent information. We then
    replace the source attribute $\bm{a}$ with the target attribute $\bm{a}^{\prime}$,
    and the final transferred text is generated using the combination of $\bm{z}$
    and ${a}^{\prime}$ John et al. ([2019](#bib.bib83)); Romanov et al. ([2019](#bib.bib167)).'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法，潜在表示分割（Latent Representation Splitting, LRS），如图 [2(c)](#S5.F2.sf3 "图 2
    ‣ 5.1.1.0.2 变分自编码器（VAE）。 ‣ 5.1.1 编码器-解码器训练方法 ‣ 5.1 解开 ‣ 5 非平行数据方法 ‣ 深度学习用于文本风格转换：综述")
    所示，首先将输入文本分解为两个部分：潜在属性表示 $\bm{a}$ 和捕获属性无关信息的语义表示 $\bm{z}$。然后，我们将源属性 $\bm{a}$ 替换为目标属性
    $\bm{a}^{\prime}$，最终的转换文本是通过 $\bm{z}$ 和 ${a}^{\prime}$ 的组合生成的（John 等人，[2019](#bib.bib83)；Romanov
    等人，[2019](#bib.bib167)）。
- en: 5.1.3 Training Objectives
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 训练目标
- en: 'When disentangling the attribute information $a$ and the attribute-independent
    semantic information $\bm{z}$, we need to achieve two aims:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在解开属性信息 $a$ 和与属性无关的语义信息 $\bm{z}$ 时，我们需要实现两个目标：
- en: Aim 1)
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标 1)
- en: 'The target attribute is fully and exclusively controlled by $\bm{a}$ (and not
    $\bm{z}$). We typically use style-oriented losses to achieve this aim (Section [5.1.3](#S5.SS1.SSS3
    "5.1.3 Training Objectives ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data
    ‣ Deep Learning for Text Style Transfer: A Survey")).'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标属性完全且独占地由 $\bm{a}$ 控制（而不是 $\bm{z}$）。我们通常使用风格导向损失来实现这一目标（第 [5.1.3](#S5.SS1.SSS3
    "5.1.3 训练目标 ‣ 5.1 解开 ‣ 5 非平行数据方法 ‣ 深度学习用于文本风格转换：综述") 节）。
- en: Aim 2)
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标 2)
- en: 'The attribute-independent information is fully and exclusively captured by
    $\bm{z}$ (and not $\bm{a}$). Content-oriented losses are more often used for this
    aim (Section [5.1.3.0.5](#S5.SS1.SSS3.P5 "5.1.3.0.5 Language Modeling on Outputs
    (LMO). ‣ 5.1.3 Training Objectives ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey")).'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 属性无关的信息完全且独占地由 $\bm{z}$ 捕获（而不是 $\bm{a}$）。内容导向的损失更常用于实现这一目标（第 [5.1.3.0.5](#S5.SS1.SSS3.P5
    "5.1.3.0.5 输出上的语言建模（LMO）。 ‣ 5.1.3 训练目标 ‣ 5.1 解开 ‣ 5 非平行数据方法 ‣ 深度学习用于文本风格转换：综述")
    节）。
- en: We describe the various style-oriented and content-oriented losses below. \subsubsubsectionStyle-Oriented
    Losses
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下面描述了各种风格导向的损失和内容导向的损失。\subsubsubsection风格导向的损失
- en: 'To achieve [Aim 1](#S5.I3.i1 "item Aim 1 ‣ 5.1.3 Training Objectives ‣ 5.1
    Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style
    Transfer: A Survey"), many different style-oriented losses have been proposed,
    to nudge the model to learn a more clearly disentangled $\bm{a}$ and exclude the
    attribute information from $\bm{z}$.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为实现 [目标 1](#S5.I3.i1 "项目 目标 1 ‣ 5.1.3 训练目标 ‣ 5.1 解开 ‣ 5 非平行数据方法 ‣ 深度学习用于文本风格转换：综述")，已经提出了许多不同的风格导向损失，以促使模型学习更清晰地解开的
    $\bm{a}$ 并将属性信息从 $\bm{z}$ 中排除。
- en: 5.1.3.0.1 Attribute Classifier on Outputs (ACO).
  id: totrans-287
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.3.0.1 输出上的属性分类器（ACO）。
- en: 'ACO aims to make sentences generated by the generator ${G}$ carry the target
    attribute $a^{\prime}$ according to a pre-trained attribute classifier $f_{c}$ (Hu
    et al., [2017](#bib.bib72); Prabhumoye et al., [2018](#bib.bib149); Yamshchikov
    et al., [2019](#bib.bib223)). The generator $G$ takes as input the learned attribute
    vector $\widehat{\bm{a}^{\prime}}$, which can be either an attribute code vector
    trained from scratch (as in the ACC approach) or the attribute representation
    disentangled from text (by the LRS approach). We denote the generation process
    to obtain the transferred sentence $\widehat{\bm{x}^{\prime}}$ as $\widehat{\bm{x}^{\prime}}\overset{\Delta}{=}G(E(\bm{x});\bm{a}^{\prime})$.
    Correspondingly, ACO minimizes the following learning objective:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ACO 旨在使生成器 ${G}$ 生成的句子根据预训练的属性分类器 $f_{c}$ 带有目标属性 $a^{\prime}$（Hu 等人，[2017](#bib.bib72)；Prabhumoye
    等人，[2018](#bib.bib149)；Yamshchikov 等人，[2019](#bib.bib223)）。生成器 $G$ 以学习到的属性向量 $\widehat{\bm{a}^{\prime}}$
    作为输入，该向量可以是从头训练的属性编码向量（如 ACC 方法所示）或从文本中解开的属性表示（如 LRS 方法所示）。我们将生成过程表示为 $\widehat{\bm{x}^{\prime}}\overset{\Delta}{=}G(E(\bm{x});\bm{a}^{\prime})$。相应地，ACO
    最小化以下学习目标：
- en: '|  | $\mathcal{L}_{\mathrm{ACO}}(\bm{\theta}_{\mathrm{G}},\bm{a}^{\prime})=-\mathbb{E}_{p(\bm{x})}\log
    f_{c}(\bm{x}^{\prime})~{}.$ |  | (3) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\mathrm{ACO}}(\bm{\theta}_{\mathrm{G}},\bm{a}^{\prime})=-\mathbb{E}_{p(\bm{x})}\log
    f_{c}(\bm{x}^{\prime})~{}.$ |  | (3) |'
- en: 'In training, ACO can be trained in two ways: either a normal loss function
    trained by Gumbel-softmax distribution to approximate the discrete training (Jang,
    Gu, and Poole, [2017](#bib.bib77)), or a negative reward for reinforcement learning
    by policy gradient training Williams ([1992](#bib.bib211)) as in Luo et al. ([2019](#bib.bib117)).'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中，ACO可以通过两种方式进行训练：一种是通过Gumbel-softmax分布训练的普通损失函数，以近似离散训练 (Jang, Gu, and Poole,
    [2017](#bib.bib77))，另一种是通过策略梯度训练的负奖励进行强化学习 (Williams ([1992](#bib.bib211)))，如Luo等人
    ([2019](#bib.bib117))所示。
- en: 5.1.3.0.2 Attribute Classifier on Representations (ACR).
  id: totrans-291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.3.0.2 属性分类器在表示上的应用 (ACR)。
- en: 'Different from the previous ACO objective whose training signal is from the
    the output sentence $\widehat{\bm{x}^{\prime}}$, ACR directly enforces the disentangled
    attribute representation $\bm{a}$ to be correctly classified by the attribute
    classifier, by the following objective John et al. ([2019](#bib.bib83)); Romanov
    et al. ([2019](#bib.bib167)):'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的ACO目标不同，ACR直接通过以下目标强制解耦的属性表示$\bm{a}$被属性分类器正确分类，John等人 ([2019](#bib.bib83))；Romanov等人
    ([2019](#bib.bib167))：
- en: '|  | $\mathcal{L}_{\mathrm{ACR}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{f_{c}})=-\mathbb{E}_{p(\bm{a})}\log
    f_{c}(\bm{a})~{}.$ |  | (4) |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\mathrm{ACR}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{f_{c}})=-\mathbb{E}_{p(\bm{a})}\log
    f_{c}(\bm{a})~{}.$ |  | (4) |'
- en: 5.1.3.0.3 Adversarial Learning on Representations (AdvR).
  id: totrans-294
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.3.0.3 表示上的对抗学习 (AdvR)。
- en: 'As the previous ACR explicitly requires the latent $\bm{a}$ to be classified
    by $f_{c}$, AdvR trains from another perspective – enforcing that no attribute-related
    information is contained in $\bm{z}$ Fu et al. ([2018](#bib.bib41)); Zhao et al.
    ([2018](#bib.bib235)); Romanov et al. ([2019](#bib.bib167)); John et al. ([2019](#bib.bib83));
    Tikhonov et al. ([2019](#bib.bib196)); Li et al. ([2020](#bib.bib110)). Note that
    by combining ACR and AdvR, we can make attribute information captured fully and
    exclusively in $\bm{a}$. To achieve AdvR, the encoder $E$ is trained to generate
    the latent representation $\bm{z}\overset{\Delta}{=}E(\bm{x})$ so that $\bm{z}$
    cannot be discriminated by the attribute classifier $f_{c}$, which is expressed
    by the following learning objective:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 由于之前的ACR明确要求潜在的$\bm{a}$由$f_{c}$进行分类，AdvR从另一个角度进行训练——强制$\bm{z}$中不包含属性相关的信息 (Fu
    et al. ([2018](#bib.bib41))；Zhao et al. ([2018](#bib.bib235))；Romanov et al. ([2019](#bib.bib167))；John
    et al. ([2019](#bib.bib83))；Tikhonov et al. ([2019](#bib.bib196))；Li et al. ([2020](#bib.bib110))。请注意，通过结合ACR和AdvR，我们可以使属性信息完全且专属地捕获在$\bm{a}$中。为了实现AdvR，编码器$E$被训练生成潜在表示$\bm{z}\overset{\Delta}{=}E(\bm{x})$，使得$\bm{z}$不能被属性分类器$f_{c}$区分，这由以下学习目标表示：
- en: '|  | $\max_{E}\min_{f_{c}}\mathcal{L}_{\mathrm{AdvR}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{f_{c}})=-\mathbb{E}_{p(\bm{x})}\log
    f_{c}(E(\bm{x}))~{}.$ |  | (5) |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{E}\min_{f_{c}}\mathcal{L}_{\mathrm{AdvR}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{f_{c}})=-\mathbb{E}_{p(\bm{x})}\log
    f_{c}(E(\bm{x}))~{}.$ |  | (5) |'
- en: 'Since AdvR can be imbalanced if the number of samples of each attribute value
    differs largely, an extension of AdvR is to treat different attribute values with
    equal weight Shen et al. ([2017](#bib.bib182)):'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 由于如果每个属性值的样本数量差异很大，AdvR可能会不平衡，因此AdvR的扩展是对不同属性值给予相等的权重 (Shen et al. ([2017](#bib.bib182)))：
- en: '|  | $\begin{split}\max_{E}\min_{f_{c}}\mathcal{L}_{\mathrm{AAE}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{f_{c}})&amp;=-\mathbb{E}_{p(\bm{x})}\Big{[}\log
    f_{c}(E(\bm{x}))\Big{]}\\ &amp;-\mathbb{E}_{p(\bm{x}^{\prime})}\Big{[}\log(1-f_{c}(E(\bm{x}^{\prime})))\Big{]}~{}.\end{split}$
    |  | (6) |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\max_{E}\min_{f_{c}}\mathcal{L}_{\mathrm{AAE}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{f_{c}})&amp;=-\mathbb{E}_{p(\bm{x})}\Big{[}\log
    f_{c}(E(\bm{x}))\Big{]}\\ &amp;-\mathbb{E}_{p(\bm{x}^{\prime})}\Big{[}\log(1-f_{c}(E(\bm{x}^{\prime})))\Big{]}~{}.\end{split}$
    |  | (6) |'
- en: Note that $p(\bm{x})$ is the distribution of sentences of one attribute, and
    $p(\bm{x}^{\prime})$ is the distribution of sentences of the other attribute.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，$p(\bm{x})$是一个属性的句子分布，$p(\bm{x}^{\prime})$是另一个属性的句子分布。
- en: 5.1.3.0.4 Adversarial Learning on Outputs (AdvO).
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.3.0.4 输出上的对抗学习 (AdvO)。
- en: Apart from AdvR that adversarially learn the latent representations, we can
    also use AdvO to perform adversarial training on the outputs, to make them undistinguishable
    from the real data Shen et al. ([2017](#bib.bib182)); Logeswaran, Lee, and Bengio
    ([2018](#bib.bib116)); Tian, Hu, and Yu ([2018](#bib.bib195)). Specifically, for
    each attribute $a_{i}$, we train a classifier $f_{c}^{(i)}$ to distinguish between
    true $\bm{x}_{i}$ from the mono-style corpus of attribute $a_{i}$, and the generated
    sentence $\widehat{\bm{x}_{i}}\overset{\Delta}{=}G(E(\bm{x}_{k});\bm{a}_{i})$,
    where $k\neq i$, which aims to have the attribute $a_{i}$. The loss function is
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对潜在表示进行对抗学习的AdvR，我们还可以使用AdvO对输出进行对抗训练，使其无法与真实数据区分 Shen et al. ([2017](#bib.bib182))；Logeswaran,
    Lee, 和 Bengio ([2018](#bib.bib116))；Tian, Hu, 和 Yu ([2018](#bib.bib195))。具体而言，对于每个属性$a_{i}$，我们训练一个分类器$f_{c}^{(i)}$来区分真实的$\bm{x}_{i}$与具有属性$a_{i}$的单一风格语料库中的生成句子$\widehat{\bm{x}_{i}}\overset{\Delta}{=}G(E(\bm{x}_{k});\bm{a}_{i})$，其中$k\neq
    i$，目标是使得该句子具有属性$a_{i}$。损失函数为
- en: '|  | $\begin{split}\max_{E,G}\min_{f_{c}^{(i)}}\mathcal{L}_{\mathrm{AdvO}}^{(i)}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}},\bm{\theta}_{f_{c}^{(i)}})&amp;=-\mathbb{E}_{p(\bm{x}_{i})}\Big{[}\log
    f_{c}^{(i)}(\bm{x}_{i})\Big{]}\\ &amp;-\mathbb{E}_{p(\bm{x}_{k})}\Big{[}\log(1-f_{c}^{(i)}(G(E(\bm{x}_{k});a_{i})))\Big{]}~{}.\end{split}$
    |  | (7) |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\max_{E,G}\min_{f_{c}^{(i)}}\mathcal{L}_{\mathrm{AdvO}}^{(i)}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}},\bm{\theta}_{f_{c}^{(i)}})&amp;=-\mathbb{E}_{p(\bm{x}_{i})}\Big{[}\log
    f_{c}^{(i)}(\bm{x}_{i})\Big{]}\\ &amp;-\mathbb{E}_{p(\bm{x}_{k})}\Big{[}\log(1-f_{c}^{(i)}(G(E(\bm{x}_{k});a_{i})))\Big{]}~{}.\end{split}$
    |  | (7) |'
- en: 'In the training process, usually we first optimize all attribute classifiers
    $f_{c}^{(i)}$, and then train the encoder, generator, and the attribute classifiers
    together by optimizing the sum the all AdvO training losses:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，通常我们首先优化所有属性分类器$f_{c}^{(i)}$，然后通过优化所有AdvO训练损失的总和来一起训练编码器、生成器和属性分类器：
- en: '|  | $\max_{E,G}\sum_{i}^{&#124;\mathbb{A}&#124;}\min_{f_{c}^{(i)}}\mathcal{L}_{\mathrm{AdvO}}^{(i)}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}},\bm{\theta}_{f_{c}^{(i)}})~{}.$
    |  | (8) |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{E,G}\sum_{i}^{&#124;\mathbb{A}&#124;}\min_{f_{c}^{(i)}}\mathcal{L}_{\mathrm{AdvO}}^{(i)}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}},\bm{\theta}_{f_{c}^{(i)}})~{}.$
    |  | (8) |'
- en: Note that in order to propagate the gradients, it is feasible to use the sequence
    of hidden states in the generator instead of discrete text for $G(E(\bm{x}_{k});a_{i})$
    Shen et al. ([2017](#bib.bib182)).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了传播梯度，可以使用生成器中的隐藏状态序列，而不是离散文本来进行$G(E(\bm{x}_{k});a_{i})$ Shen et al. ([2017](#bib.bib182))。
- en: 5.1.3.0.5 Language Modeling on Outputs (LMO).
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.3.0.5 输出的语言建模 (LMO)。
- en: The above AdvO learns classifiers to distinguish between true samples and generated
    samples. Such discriminative classification can be alternatively achieved by generative
    language modeling, namely $\mathrm{LM}_{i}$ for each mono-style corpus with the
    attribute $a_{i}$ (Yang et al., [2018](#bib.bib224)). Specifically, the training
    objective for each attribute is
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 上述AdvO学习分类器以区分真实样本和生成样本。这种判别分类还可以通过生成语言建模来实现，即$\mathrm{LM}_{i}$用于每个具有属性$a_{i}$的单一风格语料库（Yang
    et al., [2018](#bib.bib224)）。具体而言，每个属性的训练目标是
- en: '|  | $\begin{split}\mathcal{L}_{\mathrm{LMO}}^{(i)}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}},\bm{\theta}_{\mathrm{LM}_{i}})=-\mathbb{E}_{p(\bm{x_{i}})}\Big{[}\log
    p_{\mathrm{LM}_{i}}(\bm{x}_{i})\Big{]}+\gamma\mathbb{E}_{p(\bm{z}_{k})}\Big{[}\log
    p_{\mathrm{LM}_{i}}(G(E(\bm{x}_{k});a_{i}))\Big{]}~{},\end{split}$ |  | (9) |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}_{\mathrm{LMO}}^{(i)}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}},\bm{\theta}_{\mathrm{LM}_{i}})=-\mathbb{E}_{p(\bm{x_{i}})}\Big{[}\log
    p_{\mathrm{LM}_{i}}(\bm{x}_{i})\Big{]}+\gamma\mathbb{E}_{p(\bm{z}_{k})}\Big{[}\log
    p_{\mathrm{LM}_{i}}(G(E(\bm{x}_{k});a_{i}))\Big{]}~{},\end{split}$ |  | (9) |'
- en: 'where $\gamma$ is a hyperparameter to weight the two terms. The total training
    objective sums over the losses of all attributes:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\gamma$是一个超参数，用于权衡两个项。总训练目标是所有属性损失的总和：
- en: '|  | $\max_{E,G}\sum_{i}^{&#124;\mathbb{A}&#124;}\min_{\mathrm{LM}^{(i)}}\mathcal{L}_{\mathrm{LMO}}^{(i)}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}},\bm{\theta}_{\mathrm{LM}_{i}})~{}.$
    |  | (10) |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{E,G}\sum_{i}^{&#124;\mathbb{A}&#124;}\min_{\mathrm{LM}^{(i)}}\mathcal{L}_{\mathrm{LMO}}^{(i)}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{\mathrm{G}},\bm{\theta}_{\mathrm{LM}_{i}})~{}.$
    |  | (10) |'
- en: \subsubsubsection
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: \subsubsubsection
- en: 'Content-Oriented Losses The style-oriented losses introduced above ensures
    the attribute information to be contained in $\bm{a}$, but not necessarily putting
    constraints on the style-independent semantics $\bm{z}$. To learn the attribute-independent
    information fully and exclusively in $\bm{z}$, the following content-oriented
    losses are proposed:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 内容导向的损失 上述引入的风格导向损失确保了属性信息被包含在 $\bm{a}$ 中，但不一定对风格无关的语义 $\bm{z}$ 施加约束。为了在 $\bm{z}$
    中充分且独占地学习与属性无关的信息，提出了以下内容导向的损失：
- en: 5.1.3.0.6 Cycle Reconstruction (Cycle).
  id: totrans-313
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.3.0.6 循环重建（Cycle）。
- en: 'The cycle reconstruction loss  (dos Santos, Melnyk, and Padhi, [2018](#bib.bib172);
    Logeswaran, Lee, and Bengio, [2018](#bib.bib116); Luo et al., [2019](#bib.bib117);
    Dai et al., [2019](#bib.bib34); Yi et al., [2020](#bib.bib225); Huang et al.,
    [2020](#bib.bib74)) first encodes a sentence $\bm{x}$ to its latent representation
    $\bm{z}\overset{\Delta}{=}E(\bm{x})$, and then feed $\bm{z}$ to the generator
    $G$ to obtain the generated sentence $G(\bm{z})$. Since the alignment of the input
    and the generated sentence is to preserve attribute-independent semantic information,
    the generator can be conditioned on any attribute, namely $\bm{a}$ or $\bm{a}^{\prime}$.
    The cycle loss constrains the output $\widehat{\bm{x}^{\prime}}$ to align with
    the input $\bm{x}$ (and, similarly, the output $\widehat{\bm{x}}$ to align with
    the input $\bm{x}^{\prime}$) so that the content information can be preserved:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 循环重建损失（dos Santos, Melnyk, 和 Padhi，[2018](#bib.bib172)；Logeswaran, Lee, 和 Bengio，[2018](#bib.bib116)；Luo
    等人，[2019](#bib.bib117)；Dai 等人，[2019](#bib.bib34)；Yi 等人，[2020](#bib.bib225)；Huang
    等人，[2020](#bib.bib74)）首先将句子 $\bm{x}$ 编码为其潜在表示 $\bm{z}\overset{\Delta}{=}E(\bm{x})$，然后将
    $\bm{z}$ 输入生成器 $G$ 以获得生成的句子 $G(\bm{z})$。由于输入句子与生成句子的对齐是为了保留与属性无关的语义信息，生成器可以根据任何属性进行条件设置，即
    $\bm{a}$ 或 $\bm{a}^{\prime}$。循环损失限制输出 $\widehat{\bm{x}^{\prime}}$ 与输入 $\bm{x}$
    对齐（同样，输出 $\widehat{\bm{x}}$ 与输入 $\bm{x}^{\prime}$ 对齐），以便保留内容信息：
- en: '|  | $\mathcal{L}_{\mathrm{Cycle}}(\bm{\theta}_{E},\bm{\theta}_{G})=-\mathbb{E}_{p(\bm{x})}\Big{[}\log
    p_{\mathrm{G}}(\bm{x}&#124;E(\bm{x}))\Big{]}-\mathbb{E}_{p(\bm{x}^{\prime})}\Big{[}\log
    p_{\mathrm{G}}(\bm{x}^{\prime}&#124;E(\bm{x}^{\prime}))\Big{]}~{}.$ |  | (11)
    |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\mathrm{Cycle}}(\bm{\theta}_{E},\bm{\theta}_{G})=-\mathbb{E}_{p(\bm{x})}\Big{[}\log
    p_{\mathrm{G}}(\bm{x} \mid E(\bm{x}))\Big{]}-\mathbb{E}_{p(\bm{x}^{\prime})}\Big{[}\log
    p_{\mathrm{G}}(\bm{x}^{\prime} \mid E(\bm{x}^{\prime}))\Big{]}~{}.$ |  | (11)
    |'
- en: One way to train the above cycle loss is by reinforcement learning as done by
    Luo et al. ([2019](#bib.bib117)) who use the loss function as a negative for content
    preservation.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 训练上述循环损失的一种方法是通过强化学习，如 Luo 等人（[2019](#bib.bib117)）所做的，他们将损失函数用作内容保留的负值。
- en: 5.1.3.0.7 Bag-of-Words Overlap (BoW).
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.3.0.7 词袋重叠（BoW）。
- en: To approximately measure content preservation, bag-of-words (BoW) features are
    used by John et al. ([2019](#bib.bib83)); Bao et al. ([2019](#bib.bib8)). To focus
    on content information only, John et al. ([2019](#bib.bib83)) exclude stopwords
    and style-specific words.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 为了大致测量内容保留情况，John 等人（[2019](#bib.bib83)）；Bao 等人（[2019](#bib.bib8)）使用了词袋（BoW）特征。为了仅关注内容信息，John
    等人（[2019](#bib.bib83)）排除了停用词和特定风格的词汇。
- en: 'Let us denote the vocabulary set as $\mathbb{V}$. We first predict the distribution
    of BoW features $q_{\mathrm{BoW}}(\bm{z})$ of the latent representation $\bm{z}$
    using softmax on the $1\times|\mathbb{V}|$ BoW features. We then calculate the
    cross entropy loss of this BoW distribution $q_{\mathrm{BoW}}(\bm{z})$ against
    the ground-truth BoW distribution $p_{\mathrm{BoW}}(\bm{x})$ in the input sentence
    $\bm{x}$. The BoW loss is formulated as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 设词汇集为 $\mathbb{V}$。我们首先使用 $1\times|\mathbb{V}|$ 词袋特征对潜在表示 $\bm{z}$ 进行 softmax
    操作，从而预测词袋特征的分布 $q_{\mathrm{BoW}}(\bm{z})$。然后我们计算该词袋分布 $q_{\mathrm{BoW}}(\bm{z})$
    与输入句子 $\bm{x}$ 中真实词袋分布 $p_{\mathrm{BoW}}(\bm{x})$ 之间的交叉熵损失。词袋损失的公式如下：
- en: '|  | $\mathcal{L}_{\mathrm{BoW}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{q_{\mathrm{BoW}}})=-p_{\mathrm{BoW}}(\bm{x})\log
    q_{\mathrm{BoW}}(\bm{z})~{}.$ |  | (12) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\mathrm{BoW}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{q_{\mathrm{BoW}}})=-p_{\mathrm{BoW}}(\bm{x})\log
    q_{\mathrm{BoW}}(\bm{z})~{}.$ |  | (12) |'
- en: 5.1.3.0.8 Adversarial BoW Overlap (AdvBoW).
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.3.0.8 对抗性词袋重叠（AdvBoW）。
- en: BoW ensures the content to be fully captured in $\bm{z}$. As a further step,
    we want to ensure that the content information is exclusively captured in $\bm{z}$,
    namely not contained in $\bm{a}$ at all, via the following AdvBow loss on $\bm{a}$
    John et al. ([2019](#bib.bib83)); Bao et al. ([2019](#bib.bib8)).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: BoW 确保内容在 $\bm{z}$ 中被完全捕获。作为进一步的步骤，我们希望确保内容信息完全被捕获在 $\bm{z}$ 中，即完全不包含在 $\bm{a}$
    中，通过以下 AdvBow 损失在 $\bm{a}$ 上 John 等 ([2019](#bib.bib83))；Bao 等 ([2019](#bib.bib8))。
- en: When disentangling $\bm{z}$ and $\bm{a}$ in the LRS framework, we train an adversarial
    classifier $q_{\mathrm{BoW}}(\bm{a})$ to predict the BoW features given $\bm{a}$
    by aligning it with the ground-truth BoW distribution $p_{\mathrm{BoW}}(\bm{x})$,
    namely minimizing
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LRS 框架中解缠 $\bm{z}$ 和 $\bm{a}$ 时，我们训练一个对抗分类器 $q_{\mathrm{BoW}}(\bm{a})$，以预测给定
    $\bm{a}$ 的 BoW 特征，通过将其与真实 BoW 分布 $p_{\mathrm{BoW}}(\bm{x})$ 对齐，即最小化
- en: '|  | $\mathcal{L}_{\mathrm{AdvBoW}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{q_{\mathrm{BoW}}})=-p_{\mathrm{BoW}}(\bm{x})\log
    q_{\mathrm{BoW}}(\bm{z})~{}.$ |  | (13) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\mathrm{AdvBoW}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{q_{\mathrm{BoW}}})=-p_{\mathrm{BoW}}(\bm{x})\log
    q_{\mathrm{BoW}}(\bm{z})~{}.$ |  | (13) |'
- en: The final min-max objective is
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的最小-最大目标是
- en: '|  | $\max_{E}\min_{q_{\mathrm{BoW}}}\mathcal{L}_{\mathrm{AdvBoW}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{q_{\mathrm{BoW}}}).$
    |  | (14) |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{E}\min_{q_{\mathrm{BoW}}}\mathcal{L}_{\mathrm{AdvBoW}}(\bm{\theta}_{\mathrm{E}},\bm{\theta}_{q_{\mathrm{BoW}}}).$
    |  | (14) |'
- en: 5.1.3.0.9 Other Losses/Rewards.
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.3.0.9 其他损失/奖励。
- en: There are also other losses/rewards in recent work such as the noun overlap
    loss (Noun) Tian, Hu, and Yu ([2018](#bib.bib195)), as well as rewards for semantics
    and fluency (Xu et al., [2018](#bib.bib217); Gong et al., [2019](#bib.bib50);
    Sancheti et al., [2020](#bib.bib171)). We do not discuss them in much detail because
    they do not directly operate on the disentanglement of latent representations.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的工作中还有其他损失/奖励，例如名词重叠损失（Noun） Tian、Hu 和 Yu ([2018](#bib.bib195))，以及语义和流畅性的奖励（Xu
    等，[2018](#bib.bib217)；Gong 等，[2019](#bib.bib50)；Sancheti 等，[2020](#bib.bib171)）。我们不会详细讨论这些内容，因为它们并未直接作用于潜在表示的解缠结。
- en: 5.2 Prototype Editing
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 原型编辑
- en: Despite a plethora of models that use end-to-end training of neural networks,
    the prototype-based text editing approach still attracts lots of attention, since
    the proposal of a pipeline method called delete, retrieve, and generate Li et al.
    ([2018](#bib.bib107)).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有大量使用端到端神经网络训练的模型，基于原型的文本编辑方法仍然受到广泛关注，特别是因为 Li 等 ([2018](#bib.bib107)) 提出的删除、检索和生成的管道方法。
- en: Prototype editing is reminiscent of early word replacement methods used for
    TST, such as synonym matching using a style dictionary Sheikha and Inkpen ([2011](#bib.bib181)),
    WordNet Khosmood and Levinson ([2010](#bib.bib89)); Mansoorizadeh et al. ([2016](#bib.bib125)),
    hand-crafted rules Khosmood and Levinson ([2008](#bib.bib90)); Castro, Ortega,
    and Muñoz ([2017](#bib.bib26)), or using hypernyms and definitions to replace
    the style-carrying words Karadzhov et al. ([2017](#bib.bib87)).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 原型编辑让人联想到早期用于文本风格转移的词汇替换方法，例如使用风格词典 Sheikha 和 Inkpen ([2011](#bib.bib181)) 进行的同义词匹配，WordNet
    Khosmood 和 Levinson ([2010](#bib.bib89))；Mansoorizadeh 等 ([2016](#bib.bib125))，手工规则
    Khosmood 和 Levinson ([2008](#bib.bib90))；Castro、Ortega 和 Muñoz ([2017](#bib.bib26))，或使用上位词和定义来替换携带风格的词
    Karadzhov 等 ([2017](#bib.bib87))。
- en: 'Featuring more controllability and interpretability, prototype editing builds
    an explicit pipeline for text style transfer from $\bm{x}$ with attribute $a$
    to its counterpart $\bm{x}^{\prime}$ with attribute ${a}^{\prime}$:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 原型编辑具有更强的可控性和可解释性，它为从属性 $a$ 的 $\bm{x}$ 到属性 ${a}^{\prime}$ 的其对应 $\bm{x}^{\prime}$
    的文本风格转移构建了一个明确的管道。
- en: Step 1)
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 1)
- en: 'Detect attribute markers of $a$ in the input sentence $\bm{x}$, and delete
    them, resulting in a content-only sentence (Section [5.2.1](#S5.SS2.SSS1 "5.2.1
    Attribute Marker Detection ‣ 5.2 Prototype Editing ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey"));'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检测输入句子 $\bm{x}$ 中的属性标记 $a$，并删除它们， resulting in a content-only sentence (第[5.2.1](#S5.SS2.SSS1
    "5.2.1 属性标记检测 ‣ 5.2 原型编辑 ‣ 5 非平行数据的方法 ‣ 文本风格转移的深度学习：综述")节)；
- en: Step 2)
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 2)
- en: 'Retrieve candidate attribute markers carrying the desired attribute ${a}^{\prime}$
    (Section [5.2.2](#S5.SS2.SSS2 "5.2.2 Target Attribute Retriever ‣ 5.2 Prototype
    Editing ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer:
    A Survey"));'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检索携带所需属性 ${a}^{\prime}$ 的候选属性标记（第[5.2.2](#S5.SS2.SSS2 "5.2.2 目标属性检索器 ‣ 5.2 原型编辑
    ‣ 5 非平行数据的方法 ‣ 文本风格转移的深度学习：综述")节）；
- en: Step 3)
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 3)
- en: 'Infill the sentence by adding new attribute markers and make sure the generated
    sentence is fluent (Section [5.2.3](#S5.SS2.SSS3 "5.2.3 Generation from Prototypes
    ‣ 5.2 Prototype Editing ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey")).'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过添加新的属性标记来填充句子，并确保生成的句子流畅（见[5.2.3](#S5.SS2.SSS3 "5.2.3 从原型生成 ‣ 5.2 原型编辑 ‣ 5
    非平行数据方法 ‣ 文本风格转移的深度学习：综述")）。
- en: 5.2.1 Attribute Marker Detection
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 属性标记检测
- en: 'Extracting attribute markers is a non-trivial NLP task. Traditional ways to
    do it involve first using tagging, parsing and morphological analysis to select
    features, and then filtering by mutual information and Chi-square testing. In
    recent deep learning pipelines, there are three major types of approaches to identify
    attribute markers: frequency-ratio methods, attention-based methods, and fusion
    methods.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 提取属性标记是一个复杂的NLP任务。传统方法首先使用标记、解析和形态分析来选择特征，然后通过互信息和卡方检验进行筛选。在近期的深度学习流程中，识别属性标记主要有三种方法：频率比方法、基于注意力的方法和融合方法。
- en: Frequency-ratio methods calculate some statistics for each n-gram in the corpora.
    For example, Li et al. ([2018](#bib.bib107)) detect the attribute markers by calculating
    its relative frequency of co-occurrence with attribute $a$ versus ${a}^{\prime}$,
    and those with frequencies higher than a threshold are considered the markers
    of $a$. Using a similar approach, Madaan et al. ([2020](#bib.bib119)) first calculate
    the ratio of mean TF-IDF between the two attribute corpora for each n-gram, then
    normalize this ratio across all possible n-grams, and finally mark those n-grams
    with a normalized ratio $p$ higher than a pre-set threshold as attribute markers.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 频率比方法计算语料库中每个n-gram的一些统计数据。例如，Li et al. ([2018](#bib.bib107)）通过计算与属性$a$相对比属性${a}^{\prime}$的共现相对频率来检测属性标记，频率高于阈值的被视为$a$的标记。使用类似的方法，Madaan
    et al. ([2020](#bib.bib119)）首先计算每个n-gram在两个属性语料库中的平均TF-IDF比率，然后对所有可能的n-gram进行归一化，最后将归一化比率$p$高于预设阈值的n-gram标记为属性标记。
- en: Attention-based methods train an attribute classifier using the attention mechanism
    Bahdanau, Cho, and Bengio ([2015](#bib.bib6)), and consider words with attention
    weights higher than average as markers Xu et al. ([2018](#bib.bib217)). For the
    architecture of the classifier, Zhang et al. ([2018c](#bib.bib233)) use LSTM,
    and Sudhakar, Upadhyay, and Maheswaran ([2019](#bib.bib190)) use a BERT classifier,
    where the BERT classifier has shown higher detection accuracy for the attribute
    markers.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的方法使用注意力机制（Bahdanau, Cho, 和 Bengio ([2015](#bib.bib6)））训练属性分类器，并将注意力权重高于平均水平的词汇视为标记（Xu
    et al. ([2018](#bib.bib217)））。在分类器的架构中，Zhang et al. ([2018c](#bib.bib233)）使用LSTM，而Sudhakar,
    Upadhyay, 和 Maheswaran ([2019](#bib.bib190)）使用BERT分类器，其中BERT分类器在属性标记的检测准确率上表现更高。
- en: Fusion methods combine the advantages of the above two methods. For example,
    Wu et al. ([2019](#bib.bib214)) prioritize the attribute markers predicted by
    frequency-ratio methods, and use attention-based methods as an auxiliary back
    up. One use case is when frequency-ratio methods fail to identify any attribute
    markers in a given sentence, they will use the attention-based methods as a secondary
    choice to generate attribute markers. Another case is to reduce false positives.
    To reduce the number of attribute markers that are wrongly recognized, Wu et al.
    ([2019](#bib.bib214)) set a threshold to filter out low-quality attribute markers
    by frequency-ratio methods, and in cases where all attribute markers are deleted,
    they use the markers predicted by attention-based methods.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 融合方法结合了上述两种方法的优点。例如，Wu et al. ([2019](#bib.bib214)）优先考虑由频率比方法预测的属性标记，并将基于注意力的方法作为辅助备份。一种用例是当频率比方法无法识别给定句子中的任何属性标记时，它们将使用基于注意力的方法作为次选来生成属性标记。另一种情况是减少假阳性。为减少被错误识别的属性标记的数量，Wu
    et al. ([2019](#bib.bib214)）设置了一个阈值，通过频率比方法过滤掉低质量的属性标记，并在所有属性标记被删除的情况下，使用基于注意力的方法预测的标记。
- en: There are still remaining limitations of the previous methods, such as imperfect
    accuracy of the attribute classifier, and unclear relation between attribute and
    attention scores. Hence, Lee ([2020](#bib.bib103)) propose word importance scoring,
    similar to what is used by Jin et al. ([2020b](#bib.bib81)) for adversarial paraphrasing,
    to measure how important a token is to the attribute by the difference in the
    attribute probability of the original sentence and that after deleting a token.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的方法仍存在一些限制，例如属性分类器的准确性不完善，以及属性与注意力分数之间的关系不明确。因此，Lee（[2020](#bib.bib103)）提出了词重要性评分，类似于Jin等人（[2020b](#bib.bib81)）用于对抗性释义的方法，通过比较原始句子和删除一个标记后的句子在属性概率上的差异来衡量一个标记对属性的重要性。
- en: 5.2.2 Target Attribute Retriever
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 目标属性检索器
- en: After deleting the attribute markers $\mathrm{Marker}_{a}(\bm{x})$ of the sentence
    $\bm{x}$ with attribute $a$, we need to find a counterpart attribute marker $\mathrm{Marker}_{a^{\prime}}(\bm{x}^{\prime})$
    from another sentence $\bm{x}^{\prime}$ carrying a different attribute ${a}^{\prime}$.
    Denote the sentence template with all attribute markers deleted as $\mathrm{Template}(\bm{x})\overset{\Delta}{=}\bm{x}\backslash\mathrm{Marker}_{a}(\bm{x})$.
    Similarly, the template of the sentence $\bm{x}^{\prime}$ is $\mathrm{Template}(\bm{x}^{\prime})\overset{\Delta}{=}\bm{x}^{\prime}\backslash\mathrm{Marker}_{a^{\prime}}(\bm{x}^{\prime})$.
    A common approach is to find the counterpart attribute marker by its context,
    because the templates of the original attribute and its counter attribute marker
    should be similar. Specifically, we first match a template $\mathrm{Template}(\bm{x})$
    with the most similar template $\mathrm{Template}(\bm{x}^{\prime})$ in the opposite
    attribute corpus, and then identify the attribute markers $\mathrm{Marker}_{a}(\bm{x})$
    and $\mathrm{Marker}_{a^{\prime}}(\bm{x}^{\prime})$ as counterparts of each other.
    To match templates with their counterparts, most previous works find the nearest
    neighbors by the cosine similarity of sentence embeddings. Commonly used sentence
    embeddings include TF-IDF as used in Li et al. ([2018](#bib.bib107)); Sudhakar,
    Upadhyay, and Maheswaran ([2019](#bib.bib190)), averaged GloVe embedding distance
    used in Li et al. ([2018](#bib.bib107)); Sudhakar, Upadhyay, and Maheswaran ([2019](#bib.bib190)),
    and Universal Sentence Encoder Cer et al. ([2018](#bib.bib27)) used in Sudhakar,
    Upadhyay, and Maheswaran ([2019](#bib.bib190)). Apart from sentence embeddings,
    Tran, Zhang, and Soleymani ([2020](#bib.bib198)) use Part-of-Speech templates
    to match several candidates in the opposite corpus, and conduct an exhaustive
    search to fill parts of the candidate sentences into the masked positions of the
    original attribute markers.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 删除句子$\bm{x}$中属性$a$的属性标记$\mathrm{Marker}_{a}(\bm{x})$后，我们需要从另一个句子$\bm{x}^{\prime}$中找到一个具有不同属性${a}^{\prime}$的对应属性标记$\mathrm{Marker}_{a^{\prime}}(\bm{x}^{\prime})$。将删除所有属性标记后的句子模板表示为$\mathrm{Template}(\bm{x})\overset{\Delta}{=}\bm{x}\backslash\mathrm{Marker}_{a}(\bm{x})$。同样，句子$\bm{x}^{\prime}$的模板为$\mathrm{Template}(\bm{x}^{\prime})\overset{\Delta}{=}\bm{x}^{\prime}\backslash\mathrm{Marker}_{a^{\prime}}(\bm{x}^{\prime})$。一种常见的方法是通过上下文找到对应的属性标记，因为原始属性及其对立属性标记的模板应该相似。具体而言，我们首先将模板$\mathrm{Template}(\bm{x})$与对立属性语料库中最相似的模板$\mathrm{Template}(\bm{x}^{\prime})$进行匹配，然后将属性标记$\mathrm{Marker}_{a}(\bm{x})$和$\mathrm{Marker}_{a^{\prime}}(\bm{x}^{\prime})$识别为彼此的对应项。为了匹配模板及其对应项，大多数以前的工作通过句子嵌入的余弦相似度找到最近邻。常用的句子嵌入包括Li等人（[2018](#bib.bib107)）使用的TF-IDF；Sudhakar、Upadhyay和Maheswaran（[2019](#bib.bib190)）使用的平均GloVe嵌入距离；以及Cer等人（[2018](#bib.bib27)）在Sudhakar、Upadhyay和Maheswaran（[2019](#bib.bib190)）中使用的Universal
    Sentence Encoder。除了句子嵌入外，Tran、Zhang和Soleymani（[2020](#bib.bib198)）使用词性模板在对立语料库中匹配多个候选项，并进行穷尽搜索，将候选句子的部分填入原始属性标记的掩码位置。
- en: 5.2.3 Generation from Prototypes
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 从原型生成
- en: Li et al. ([2018](#bib.bib107)) and Sudhakar, Upadhyay, and Maheswaran ([2019](#bib.bib190))
    feed the content-only sentence template and new attribute markers into a pretrained
    language model that rearranges them into a natural sentence. This infilling process
    can naturally be achieved by a masked language model (MLM) (Malmi, Severyn, and
    Rothe, [2020b](#bib.bib123)). For example, Wu et al. ([2019](#bib.bib214)) use
    MLM of the template conditioned on the target attribute, and this MLM is trained
    on an additional attribute classification loss using the model output and a fixed
    pre-trained attribute classifier. Since these generation practices are complicated,
    Madaan et al. ([2020](#bib.bib119)) propose a simpler way. They skip Step 2 that
    explicitly retrieves attribute candidates, and, instead, directly learn a generation
    model that only takes attribute-masked sentences as inputs. This generation model
    is trained on data where the attribute-carrying sentences $\bm{x}$ are paired
    with their templates $\mathrm{Template}(\bm{x})$. Training on the pairs of $(\mathrm{Template}(\bm{x}),\bm{x})$
    constructed in the above way can make the model learn how to fill the masked sentence
    template with the target attribute $a$.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人（[2018](#bib.bib107)）和 Sudhakar、Upadhyay 及 Maheswaran（[2019](#bib.bib190)）将仅包含内容的句子模板和新的属性标记输入到预训练的语言模型中，该模型将其重新排列成自然句子。这种填充过程可以通过掩码语言模型（MLM）（Malmi、Severyn
    和 Rothe，[2020b](#bib.bib123)）自然地实现。例如，Wu 等人（[2019](#bib.bib214)）使用条件于目标属性的模板 MLM，并且该
    MLM 在使用模型输出和固定的预训练属性分类器的额外属性分类损失上进行训练。由于这些生成实践比较复杂，Madaan 等人（[2020](#bib.bib119)）提出了一种更简单的方法。他们跳过明确检索属性候选的第
    2 步，而是直接学习一个生成模型，该模型仅以属性掩码句子作为输入。这个生成模型是在属性携带句子 $\bm{x}$ 和其模板 $\mathrm{Template}(\bm{x})$
    组成的数据上进行训练的。对上述方式构建的 $(\mathrm{Template}(\bm{x}),\bm{x})$ 对进行训练可以使模型学会如何用目标属性
    $a$ 填充掩码句子模板。
- en: 5.3 Pseudo-Parallel Corpus Construction
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 伪平行语料库构建
- en: To provide more signals for training, it is also helpful to generate pseudo-parallel
    data for TST. Two major approaches are retrieval-based and generation-based methods.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更多的训练信号，生成伪平行数据对 TST 也很有帮助。主要有两种方法：基于检索的方法和基于生成的方法。
- en: 5.3.0.0.1 Retrieval-Based Corpora Construction.
  id: totrans-351
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.3.0.0.1 基于检索的语料库构建。
- en: One common way to construct pseudo-parallel data is through retrieval, namely
    extracting aligned sentence pairs from two mono-style corpora. Jin et al. ([2019](#bib.bib82))
    empirically observe that semantically similar sentences in the two mono-style
    corpora tend to be the attribute-transferred counterparts of each other. Hence,
    they construct the initial pseudo corpora by matching sentence pairs in the two
    attributed corpora according to the cosine similarity of pretrained sentence embeddings.
    Formally, for each sentence $\bm{x}$, its pseudo counterpart $\widehat{\bm{x}^{\prime}}$
    is its most similar sentence in the other attribute corpus $\bm{X}^{\prime}$,
    namely $\widehat{\bm{x}^{\prime}}=\operatorname*{argmax}_{\bm{x}^{\prime}\in\bm{X}^{\prime}}\mathrm{Similarity}(\bm{x},\bm{x}^{\prime})$.
    This approach is extended by Nikolov and Hahnloser ([2019](#bib.bib134)) who use
    large-scale hierarchical alignment to extract pseudo-parallel style transfer pairs.
    Such retrieval-based pseudo-parallel data construction is also useful for machine
    translation Munteanu and Marcu ([2005](#bib.bib133)); Uszkoreit et al. ([2010](#bib.bib201));
    Marie and Fujita ([2017](#bib.bib126)); Grégoire and Langlais ([2018](#bib.bib53));
    Ren et al. ([2020](#bib.bib161)).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 构建伪平行数据的一种常见方法是通过检索，即从两个单风格语料库中提取对齐的句子对。Jin 等人（[2019](#bib.bib82)）通过实证观察到，在两个单风格语料库中，语义相似的句子往往是彼此的属性转移对应句。因此，他们通过根据预训练句子嵌入的余弦相似度在两个属性语料库中匹配句子对来构建初始伪语料库。正式地，对于每个句子
    $\bm{x}$，它的伪对应句 $\widehat{\bm{x}^{\prime}}$ 是在另一个属性语料库 $\bm{X}^{\prime}$ 中最相似的句子，即
    $\widehat{\bm{x}^{\prime}}=\operatorname*{argmax}_{\bm{x}^{\prime}\in\bm{X}^{\prime}}\mathrm{Similarity}(\bm{x},\bm{x}^{\prime})$。Nikolov
    和 Hahnloser（[2019](#bib.bib134)）扩展了这种方法，使用大规模的层次对齐来提取伪平行风格转移对。这种基于检索的伪平行数据构建对机器翻译也有用（Munteanu
    和 Marcu [2005](#bib.bib133)；Uszkoreit 等人 [2010](#bib.bib201)；Marie 和 Fujita [2017](#bib.bib126)；Grégoire
    和 Langlais [2018](#bib.bib53)；Ren 等人 [2020](#bib.bib161)）。
- en: 5.3.0.0.2 Generation-Based Corpora Construction.
  id: totrans-353
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.3.0.0.2 基于生成的语料库构建。
- en: Another way is through generation, such as iterative back-translation (IBT)
    Hoang et al. ([2018](#bib.bib66)). IBT is a widely used method in machine translation
    Artetxe et al. ([2018](#bib.bib4)); Lample et al. ([2018a](#bib.bib100), [b](#bib.bib101));
    Dou, Anastasopoulos, and Neubig ([2020](#bib.bib37)) which adopts an iterative
    process to generate pseudo-parallel corpora.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是通过生成，例如迭代回译（IBT）Hoang等人（[2018](#bib.bib66)）。IBT是机器翻译中广泛使用的方法，Artetxe等人（[2018](#bib.bib4)）；Lample等人（[2018a](#bib.bib100),
    [b](#bib.bib101)）；Dou, Anastasopoulos和Neubig（[2020](#bib.bib37)）采用了迭代过程来生成伪平行语料库。
- en: 'Before starting the iterative process, IBT needs to first initialize two style
    transfer models, $M_{a\rightarrow a^{\prime}}$ which transfers from the attribute
    $a$ to the other attribute $a^{\prime}$ and $M_{a^{\prime}\rightarrow a}$ which
    transfers from $a^{\prime}$ to $a$. Then, in each iteration, it executes the following
    steps:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始迭代过程之前，IBT需要首先初始化两个风格迁移模型，$M_{a\rightarrow a^{\prime}}$将属性$a$转移到另一属性$a^{\prime}$，$M_{a^{\prime}\rightarrow
    a}$将$a^{\prime}$转移到$a$。然后，在每次迭代中，执行以下步骤：
- en: Step 1)
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤1)
- en: Use the models to generate pseudo-parallel corpora. Specifically, $M_{a\rightarrow
    a^{\prime}}(\bm{x})$ generates pseudo pairs $(\bm{x},\widehat{\bm{x}^{\prime}})$
    for all $\bm{x}\in\bm{X}$, and $M_{a^{\prime}\rightarrow a}(\bm{x}^{\prime})$
    generates pairs of $(\widehat{\bm{x}},\bm{x}^{\prime})$ for all $\bm{x}^{\prime}\in\bm{X}^{\prime}$;
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用模型生成伪平行语料库。具体来说，$M_{a\rightarrow a^{\prime}}(\bm{x})$为所有$\bm{x}\in\bm{X}$生成伪对$(\bm{x},\widehat{\bm{x}^{\prime}})$，$M_{a^{\prime}\rightarrow
    a}(\bm{x}^{\prime})$为所有$\bm{x}^{\prime}\in\bm{X}^{\prime}$生成对$(\widehat{\bm{x}},\bm{x}^{\prime})$；
- en: Step 2)
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤2)
- en: 'Re-train these two style transfer models on the datasets generated by [1](#S5.I5.i1
    "item 1 ‣ 5.3.0.0.2 Generation-Based Corpora Construction. ‣ 5.3 Pseudo-Parallel
    Corpus Construction ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey"), i.e., re-train $M_{a\rightarrow a^{\prime}}(\bm{x})$
    on $(\widehat{\bm{x}},\bm{x}^{\prime})$ pairs and $M_{a^{\prime}\rightarrow a}(\bm{x}^{\prime})$
    on $(\widehat{\bm{x}^{\prime}},\bm{x})$ pairs.'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '重新训练这两个风格迁移模型，使用由[1](#S5.I5.i1 "item 1 ‣ 5.3.0.0.2 Generation-Based Corpora
    Construction. ‣ 5.3 Pseudo-Parallel Corpus Construction ‣ 5 Methods on Non-Parallel
    Data ‣ Deep Learning for Text Style Transfer: A Survey")生成的数据集，即在$(\widehat{\bm{x}},\bm{x}^{\prime})$对上重新训练$M_{a\rightarrow
    a^{\prime}}(\bm{x})$，在$(\widehat{\bm{x}^{\prime}},\bm{x})$对上重新训练$M_{a^{\prime}\rightarrow
    a}(\bm{x}^{\prime})$。'
- en: 'For Step [1](#S5.I5.i1 "item 1 ‣ 5.3.0.0.2 Generation-Based Corpora Construction.
    ‣ 5.3 Pseudo-Parallel Corpus Construction ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey"), in order to generate the initial
    pseudo-parallel corpora, a simple baseline is to randomly initialize the two models
    $M_{a\rightarrow a^{\prime}}$ and $M_{a^{\prime}\rightarrow a}$, and use them
    to translate the attribute of each sentence in $\bm{x}\in\bm{X}$ and $\bm{x}^{\prime}\in\bm{X}^{\prime}$.
    However, this simple initialization is subject to randomness and may not bootstrap
    well. Another way adopted by Zhang et al. ([2018d](#bib.bib234)) borrows the idea
    from unsupervised machine translation Lample et al. ([2018a](#bib.bib100)) that
    first learns an unsupervised word-to-word translation table between attribute
    $a$ and $a^{\prime}$, and uses it to generate an initial pseudo-parallel corpora.
    Based on such initial corpora, they train initial style transfer models and bootstrap
    the IBT process. Another model, Iterative Matching and Translation (IMaT) Jin
    et al. ([2019](#bib.bib82)), does not learn the word translation table, and instead
    trains the initial style transfer models on a retrieval-based pseudo-parallel
    corpora introduced in the retrieval-based corpora construction above.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '对于步骤[1](#S5.I5.i1 "item 1 ‣ 5.3.0.0.2 Generation-Based Corpora Construction.
    ‣ 5.3 Pseudo-Parallel Corpus Construction ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey")，为了生成初始伪平行语料库，一个简单的基线是随机初始化两个模型$M_{a\rightarrow
    a^{\prime}}$和$M_{a^{\prime}\rightarrow a}$，并使用它们翻译每个句子的属性$\bm{x}\in\bm{X}$和$\bm{x}^{\prime}\in\bm{X}^{\prime}$。然而，这种简单的初始化是随机的，可能无法很好地引导过程。张等人（[2018d](#bib.bib234)）采用了从无监督机器翻译中借用的思路，Lample等人（[2018a](#bib.bib100)）首先学习了属性$a$和$a^{\prime}$之间的无监督词对词翻译表，并用它来生成初始伪平行语料库。基于这些初始语料库，他们训练了初始风格迁移模型并引导了IBT过程。另一种模型，迭代匹配和翻译（IMaT）Jin等人（[2019](#bib.bib82)），没有学习词翻译表，而是基于上述检索式语料库构建方法在检索式伪平行语料库上训练了初始风格迁移模型。'
- en: 'For Step [2](#S5.I5.i2 "item 2 ‣ 5.3.0.0.2 Generation-Based Corpora Construction.
    ‣ 5.3 Pseudo-Parallel Corpus Construction ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey"), during the iterative process, it
    is possible to encounter divergence, as there is no constraint to ensure that
    each iteration will produce better pseudo-parallel corpora than the previous iteration.
    One way to enhance the convergence of IBT is to add additional losses. For example,
    Zhang et al. ([2018d](#bib.bib234)) use the attribute classification loss ACO,
    as in Eq. ([3](#S5.E3 "In 5.1.3.0.1 Attribute Classifier on Outputs (ACO). ‣ 5.1.3
    Training Objectives ‣ 5.1 Disentanglement ‣ 5 Methods on Non-Parallel Data ‣ Deep
    Learning for Text Style Transfer: A Survey")), to check whether the generated
    sentence by back-translation fits the desired attribute according to a pre-trained
    style classifier. Alternatively, IMaT Jin et al. ([2019](#bib.bib82)) uses a checking
    mechanism instead of additional losses. At the end of each iteration, IMaT looks
    at all candidate pseudo-pairs of an original sentence, and uses Word Mover Distance
    Kusner et al. ([2015](#bib.bib97)) to select the sentence that has the desired
    attribute and is the closest to the original sentence.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 对于步骤 [2](#S5.I5.i2 "项目 2 ‣ 5.3.0.0.2 基于生成的语料库构建 ‣ 5.3 伪平行语料库构建 ‣ 5 在非平行数据上的方法
    ‣ 文本风格转换的深度学习：综述")，在迭代过程中可能会遇到发散现象，因为没有约束条件来确保每次迭代都能产生比前一次更好的伪平行语料库。一种增强 IBT 收敛性的方法是添加额外的损失。例如，Zhang
    等人 ([2018d](#bib.bib234)) 使用属性分类损失 ACO，如 Eq. ([3](#S5.E3 "在 5.1.3.0.1 输出上的属性分类器
    (ACO) ‣ 5.1.3 训练目标 ‣ 5.1 解缠结 ‣ 5 在非平行数据上的方法 ‣ 文本风格转换的深度学习：综述"))，来检查反向翻译生成的句子是否符合预训练风格分类器所期望的属性。或者，IMaT
    Jin 等人 ([2019](#bib.bib82)) 使用检查机制而不是额外的损失。在每次迭代结束时，IMaT 查看原始句子的所有候选伪对，并使用 Word
    Mover Distance Kusner 等人 ([2015](#bib.bib97)) 选择具有期望属性且最接近原始句子的句子。
- en: 6 Research Agenda
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 研究议程
- en: 'In this section, we will propose some potential directions for future TST research,
    including expanding the scope of styles (Section [6.1](#S6.SS1 "6.1 Expanding
    the Scope of Styles ‣ 6 Research Agenda ‣ Deep Learning for Text Style Transfer:
    A Survey")), improving the methodology (Section [6.2](#S6.SS2 "6.2 Improving the
    Methodology on Non-Parallel Data ‣ 6 Research Agenda ‣ Deep Learning for Text
    Style Transfer: A Survey")), loosening the style-specific data assumptions (Section [6.3](#S6.SS3
    "6.3 Loosening the Style-Specific Dataset Assumptions ‣ 6 Research Agenda ‣ Deep
    Learning for Text Style Transfer: A Survey")), and improving evaluation metrics
    (Section [6.4](#S6.SS4 "6.4 Improving Evaluation Metrics ‣ 6 Research Agenda ‣
    Deep Learning for Text Style Transfer: A Survey")).'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提出一些未来 TST 研究的潜在方向，包括扩展风格范围（第 [6.1](#S6.SS1 "6.1 扩展风格范围 ‣ 6 研究议程 ‣ 文本风格转换的深度学习：综述")
    节）、改进方法（第 [6.2](#S6.SS2 "6.2 改进非平行数据的方法 ‣ 6 研究议程 ‣ 文本风格转换的深度学习：综述") 节）、放宽风格特定数据假设（第
    [6.3](#S6.SS3 "6.3 放宽风格特定数据集假设 ‣ 6 研究议程 ‣ 文本风格转换的深度学习：综述") 节）和改进评估指标（第 [6.4](#S6.SS4
    "6.4 改进评估指标 ‣ 6 研究议程 ‣ 文本风格转换的深度学习：综述") 节）。
- en: 6.1 Expanding the Scope of Styles
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 扩展风格范围
- en: 6.1.0.0.1 More Styles.
  id: totrans-365
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.1.0.0.1 更多风格
- en: Extending the list of styles for TST is one popular research direction. Existing
    research originally focused on styles such as simplification Zhu, Bernhard, and
    Gurevych ([2010](#bib.bib238)), formality Sheikha and Inkpen ([2011](#bib.bib181)),
    and sentiment transfer Shen et al. ([2017](#bib.bib182)), while the recent two
    years have seen a richer set of styles such as politeness Madaan et al. ([2020](#bib.bib119)),
    biasedness Pryzant et al. ([2020](#bib.bib150)), medical text simplification Cao
    et al. ([2020](#bib.bib23)), and so on.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展 TST 的风格列表是一个热门的研究方向。现有研究最初关注于简化 Zhu, Bernhard, 和 Gurevych ([2010](#bib.bib238))、正式性
    Sheikha 和 Inkpen ([2011](#bib.bib181)) 和情感转换 Shen 等人 ([2017](#bib.bib182)) 等风格，而近年来则出现了更多丰富的风格，如礼貌
    Madaan 等人 ([2020](#bib.bib119))、偏见 Pryzant 等人 ([2020](#bib.bib150))、医学文本简化 Cao
    等人 ([2020](#bib.bib23)) 等。
- en: 'Such extension of styles is driven by the advancement of TST methods, and also
    various downstream needs, such as persona-based dialog generation, customized
    text rewriting applications, and moderation of online text. Apart from the styles
    that have been researched as listed in Table [3](#S2.T3 "Table 3 ‣ 2.3 Existing
    Subtasks with Datasets ‣ 2 What Is Text Style Transfer? ‣ Deep Learning for Text
    Style Transfer: A Survey"), there are also many other new styles that can be interesting
    to conduct new research on, including but not limited to the following:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '风格的这种扩展受到 TST 方法的进步以及各种下游需求的推动，例如基于角色的对话生成、自定义文本重写应用程序和在线文本的审查。除了表 [3](#S2.T3
    "Table 3 ‣ 2.3 Existing Subtasks with Datasets ‣ 2 What Is Text Style Transfer?
    ‣ Deep Learning for Text Style Transfer: A Survey") 中列出的已研究的风格，还有许多其他新风格值得进行新研究，包括但不限于以下内容：'
- en: •
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Factual-to-empathetic transfer, to improve counseling dialogs (after the first
    version of this survey in 2020, we gladly found that this direction has now a
    preliminary exploration by Sharma et al. ([2021](#bib.bib180)));
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从事实到同情的转移，以改善咨询对话（在本调查的第一个版本之后，我们高兴地发现 Sharma 等人 ([2021](#bib.bib180)) 现在对这一方向进行了初步探索）；
- en: •
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Non-native-to-native transfer (i.e., reformulating grammatical error correction
    with TST);
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从非母语到母语的转移（即，用 TST 改写语法错误修正）；
- en: •
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sentence disambiguation, to resolve nuance in text.
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子消歧，解决文本中的细微差别。
- en: 6.1.0.0.2 More Difficult Forms of Style.
  id: totrans-374
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.1.0.0.2 更困难的风格形式。
- en: Another direction is to explore more complicated forms of styles. As covered
    by this survey, the early work on deep learning-based TST explores relatively
    simple styles, such as verb tenses Hu et al. ([2017](#bib.bib72)) and positive-vs-negative
    Yelp reviews Shen et al. ([2017](#bib.bib182)). In these tasks, each data point
    is one sentence with a clear, categorized style, and the entire dataset is in
    the same domain. Moreover, the existing datasets can decouple style and style-independent
    contents relatively well.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个方向是探索更复杂的风格形式。如本调查所述，早期的深度学习基础 TST 研究探索了相对简单的风格，例如动词时态 Hu 等人 ([2017](#bib.bib72))
    和正面与负面 Yelp 评论 Shen 等人 ([2017](#bib.bib182))。在这些任务中，每个数据点是一个具有明确分类风格的句子，整个数据集在同一领域。此外，现有的数据集可以相对较好地解耦风格与风格独立的内容。
- en: 'We propose that TST can potentially be extended into the following settings:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出 TST 可以潜在地扩展到以下设置：
- en: •
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Aspect-based style transfer (e.g., transferring the sentiment on one aspect
    but not the other aspects on aspect-based sentiment analysis data)
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于方面的风格迁移（例如，在基于方面的情感分析数据中，将某一方面的情感转移而非其他方面）
- en: •
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Authorship transfer (which has tightly coupled style and content)
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者身份转移（其风格与内容紧密结合）
- en: •
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Document-level style transfer (which includes discourse planning)
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文档级风格迁移（包括话语规划）
- en: •
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Domain adaptive style transfer (which is preceded by Li et al. ([2019](#bib.bib105)))
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 域适应风格迁移（前述由 Li 等人 ([2019](#bib.bib105)) 提出）
- en: 6.1.0.0.3 Style Interwoven with Semantics.
  id: totrans-385
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.1.0.0.3 风格与语义交织。
- en: In some cases, it can be difficult or impossible to separate attributes from
    meaning, namely the subject matter or the argument that the author wants to convey.
    One reason is that the subject that the author is going to write about can influence
    the choice of writing style. For example, science fiction writing can use the
    first person voice and fancy, flowery tone when describing a place. Another reason
    is that many stylistic devices such as allusion depend on content words.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，将属性与意义分离可能困难或不可能，即作者想要传达的主题或论点。一个原因是作者将要写作的主题可能会影响写作风格的选择。例如，科幻写作在描述一个地方时可以使用第一人称视角和华丽的语调。另一个原因是许多修辞手法，如暗示，依赖于内容词。
- en: Currently, it is a simplification of the problem setting to limit it to scenarios
    where the attribute and semantics can be approximately separated. For evaluation,
    so far researchers have allowed the human judges decide the scores of transferred
    style strength and the content preservation.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，将问题设置简化为限制在属性和语义大致可分离的场景中。对于评估，到目前为止，研究人员允许人工评审员决定迁移风格强度和内容保留的评分。
- en: In future work, it will be an interesting direction to address the more challenging
    scenarios where the style and semantics are interwoven.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的工作中，解决风格与语义交织的更具挑战性的场景将是一个有趣的方向。
- en: 6.2 Improving the Methodology on Non-Parallel Data
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 改进非平行数据上的方法
- en: Since the majority of TST research focuses on non-parallel data, we discuss
    below its strengths and limitations.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数 TST 研究集中在非平行数据上，我们下面讨论其优点和局限性。
- en: 6.2.1 Understanding the Strengths and Limitations of Existing Methods
  id: totrans-391
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 理解现有方法的优点和局限性
- en: 'To come up with improvement directions for TST methods, it is important to
    first investigate the strengths and limitations of existing methods. We analyze
    the three major streams of approaches for unsupervised TST in Table [6](#S6.T6
    "Table 6 ‣ 6.2.1 Understanding the Strengths and Limitations of Existing Methods
    ‣ 6.2 Improving the Methodology on Non-Parallel Data ‣ 6 Research Agenda ‣ Deep
    Learning for Text Style Transfer: A Survey"), including their strengths, weaknesses,
    and future directions.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提出 TST 方法的改进方向，首先需要调查现有方法的优点和局限性。我们在表 [6](#S6.T6 "Table 6 ‣ 6.2.1 Understanding
    the Strengths and Limitations of Existing Methods ‣ 6.2 Improving the Methodology
    on Non-Parallel Data ‣ 6 Research Agenda ‣ Deep Learning for Text Style Transfer:
    A Survey") 中分析了三种主要的无监督 TST 方法，包括它们的优点、缺点和未来方向。'
- en: 'Table 6: The strengths ($+$), weaknesses ($-$), and improvement directions
    ($?$) of the three mainstreams of TST methods on non-parallel data.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 三种主流 TST 方法在非平行数据上的优点 ($+$)、缺点 ($-$) 和改进方向 ($?$)。'
- en: '| Method | Strengths & Weaknesses |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 优点与缺点 |'
- en: '| Disentanglement | $+$ More profound in theoretical analysis, e.g., disentangled
    representation learning |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 解缠结 | $+$ 理论分析更深入，例如解缠结表示学习 |'
- en: '| $-$ Difficulties of training deep generative models (VAEs, GANs) for text
    |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| $-$ 训练深度生成模型（VAEs、GANs）以生成文本的困难 |'
- en: '| $-$ Hard to represent all styles as latent code |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| $-$ 难以将所有风格表示为潜在代码 |'
- en: '| $-$ Computational cost rises with the number of styles to model |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| $-$ 随着要建模的风格数量增加，计算成本上升 |'
- en: '| Prototype Editing | $+$ High BLEU scores due to large word preservation |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 原型编辑 | $+$ 由于保留了大量单词，BLEU 分数较高 |'
- en: '| $-$ Attribute marker detection step can fail if the style and semantics are
    confounded |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| $-$ 如果风格和语义混淆，则属性标记检测步骤可能会失败 |'
- en: '| $-$ The step target attribute retrieval by templates can fail if there are
    large rewrites for styles, e.g., Shakespearean English vs. modern English |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| $-$ 通过模板的步骤目标属性检索可能会失败，如果风格有较大改写，例如莎士比亚英语与现代英语 |'
- en: '| $-$ Target attribute retrieval step has large complexity (quadratic to the
    number of sentences) |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| $-$ 目标属性检索步骤复杂性大（与句子数量的平方成正比） |'
- en: '| $-$ Large computational cost if there are many styles, each of which needs
    a pre-trained LM for the generation step |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| $-$ 如果风格多且每种风格都需要预训练语言模型进行生成步骤，则计算成本较高 |'
- en: '| $?$ Future work can enable matchings for syntactic variation |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| $?$ 未来的工作可以实现句法变化的匹配 |'
- en: '| $?$ Future work can use grammatical error correction to post-edit the output
    |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| $?$ 未来的工作可以使用语法错误纠正来后处理输出 |'
- en: '| Pseudo-Parallel Corpus Construction | $+$ Performance can approximate supervised
    model performance, if the pseudo-parallel data are of good quality |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 伪平行语料构建 | $+$ 如果伪平行数据质量良好，性能可以接近监督模型的性能 |'
- en: '| $-$ May fail for small corpora |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| $-$ 对小规模语料可能会失败 |'
- en: '| $-$ May fail if the mono-style corpora do not have many samples with similar
    contents |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| $-$ 如果单风格语料中没有许多内容相似的样本，可能会失败 |'
- en: '| $-$ For IBT, divergence is possible, and sometimes needs special designs
    to prevent it |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| $-$ 对于 IBT，可能会出现发散，有时需要特殊设计来防止这种情况 |'
- en: '| $-$ For IBT, time complexity is high (due to iterative pseudo data generation)
    |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| $-$ 对于 IBT，时间复杂度高（由于迭代伪数据生成） |'
- en: '| $?$ Improve the convergence of the IBT |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| $?$ 改进 IBT 的收敛性 |'
- en: 6.2.1.0.1 Challenges for Disentanglement.
  id: totrans-412
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.1.0.1 解缠结的挑战。
- en: Theoretically, although disentanglement is impossible without inductive biases
    or other forms of supervision Locatello et al. ([2019](#bib.bib114)), disentanglement
    is achievable with some weak signals, such as only knowing how many factors have
    changed, but not which ones Locatello et al. ([2020](#bib.bib115)).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，尽管没有归纳偏置或其他形式的监督 Locatello et al. ([2019](#bib.bib114)) 时解缠结是不可能的，但通过一些弱信号，如仅知道多少因素发生了变化，但不清楚哪些因素
    Locatello et al. ([2020](#bib.bib115))，解缠结是可以实现的。
- en: In practice, some big challenges for disentanglement-based methods include,
    for example, the difficulty to train deep text generative models such as VAEs
    and GANs. Also, it is not easy to represent all styles as latent code. Moreover,
    if targeting multiple styles, the computational complexity linearly increases
    with the number of styles to model.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，基于解耦的方法面临的一些重大挑战包括，例如，训练深度文本生成模型（如VAE和GAN）的难度。同时，表示所有风格作为潜在代码也不容易。此外，如果目标是多个风格，计算复杂度会随着模型化的风格数量线性增加。
- en: 6.2.1.0.2 Challenges for Prototype Editing.
  id: totrans-415
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.1.0.2 原型编辑的挑战。
- en: Prototype-editing approaches usually result in relatively high BLEU scores,
    partly because the output text largely overlaps with the input text. This line
    of methods is likely to perform well on tasks such as sentiment modification,
    for which it is easy to identify “attribute markers,” and the input and output
    sentences share an attribute-independent template.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 原型编辑方法通常会产生相对较高的BLEU分数，部分原因是输出文本与输入文本大部分重叠。这类方法在情感修改等任务中表现良好，因为很容易识别“属性标记”，输入和输出句子共享一个属性无关的模板。
- en: However, prototype editing cannot be applied to all types of style transfer
    tasks. The first step, attribute marker retrieval, might not work if the datasets
    have confounded style and contents, because they may lead to wrong extraction
    of attribute markers, such as some content words or artifacts which can also be
    used to distinguish the style-specific data.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，原型编辑并不适用于所有类型的风格迁移任务。如果数据集混淆了风格和内容，第一步属性标记检索可能不起作用，因为这可能导致属性标记的错误提取，如某些内容词或伪影，这些也可以用于区分风格特定的数据。
- en: The second step, target attribute retrieval by templates, will fail if there
    is too little word overlap between a sentence and its counterpart carrying another
    style. An example is the TST task to “Shakespearize” modern English. There is
    little lexical overlap between a Shakespearean sentence written in early modern
    English and its corresponding modern English expression. In such cases, the retrieval
    step is likely to fail, because there is a large number of rewrites between the
    two styles, and the template might be almost hollow. Moreover, this step is also
    computationally expensive, if there are a large number of sentences in the data
    (e.g., all Wikipedia text), since this step needs to calculate the pair-wise similarity
    among all available sentences across style-specific corpora.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步，通过模板检索目标属性，如果句子与另一种风格的对应句子之间的词重叠过少，则会失败。一个例子是将现代英语“莎士比亚化”的TST任务。早期现代英语的莎士比亚句子与其对应的现代英语表达之间几乎没有词汇重叠。在这种情况下，检索步骤可能会失败，因为两种风格之间有大量的重写，模板可能几乎为空。此外，如果数据中有大量句子（例如，所有的维基百科文本），此步骤也会计算开销较大，因为该步骤需要计算所有可用句子之间的风格特定语料库的配对相似性。
- en: The third step, generation from prototype, requires a separate pretrained LM
    for each style corpus. When there are multiple styles of interest (e.g., multiple
    persona), this will induce a large computational cost.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步，从原型生成，需要为每种风格语料库使用一个单独的预训练语言模型。当涉及多个感兴趣的风格（例如，多种个性）时，这将导致大量的计算开销。
- en: 'The last limitation of prototype editing is that it amplifies the intrinsic
    problem of using BLEU to evaluate TST (Problem [1](#S3.I1.i1 "item 1 ‣ 3.1.0.0.1
    BLEU with Gold References. ‣ 3.1 Automatic Evaluation ‣ 3 How to Evaluate Style
    Transfer? ‣ Deep Learning for Text Style Transfer: A Survey"), namely the fact
    that simply copying the input can result in a high BLEU score) as introduced in
    Section [3.1](#S3.SS1 "3.1 Automatic Evaluation ‣ 3 How to Evaluate Style Transfer?
    ‣ Deep Learning for Text Style Transfer: A Survey")). For the retrieval-based
    method, some can argue that there is some performance gain because this method
    in practice copies more expressions in the input sentence than other lines of
    methods.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 原型编辑的最后一个限制是，它放大了使用BLEU评估TST的固有问题（问题 [1](#S3.I1.i1 "第1项 ‣ 3.1.0.0.1 BLEU与黄金参考。
    ‣ 3.1 自动评估 ‣ 3 如何评估风格迁移？ ‣ 深度学习文本风格迁移：一项调查")，即简单复制输入可能会导致高BLEU分数），如第[3.1节](#S3.SS1
    "3.1 自动评估 ‣ 3 如何评估风格迁移？ ‣ 深度学习文本风格迁移：一项调查")所述。对于基于检索的方法，有人可能会争辩说，这种方法在实践中复制了比其他方法更多的输入句子中的表达，从而获得了一些性能提升。
- en: As future study, there can be many interesting directions to explore, for example,
    investigating the performance of existing prototype editing models under a challenging
    dataset that reveals the above shortcomings, proposing new models to improve this
    line of approaches, and better evaluation methods for prototype editing models.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 作为未来的研究，可以探索许多有趣的方向，例如，调查现有原型编辑模型在揭示上述不足的挑战数据集下的表现，提出新模型以改进这一方法线，并改进原型编辑模型的评估方法。
- en: 6.2.1.0.3 Challenges for Pseudo-Parallel Corpus Construction.
  id: totrans-422
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.1.0.3 伪平行语料库构建的挑战。
- en: The method to construct pseudo-parallel data can be effective, especially when
    the pseudo-parallel corpora resemble supervised data. The challenge is that this
    approach may not work if the non-parallel corpora do not have enough samples that
    can be matched to create the pseudo-parallel corpora, or when the IBT cannot bootstrap
    well or fails to converge. The time complexity for training IBT is also very high
    because it needs to iteratively generate pseudo-parallel corpus and re-train models.
    Interesting future directions can be reducing the computational cost, designing
    more effective bootstrapping, and improving the convergence of IBT.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 构建伪平行数据的方法可以是有效的，尤其是当伪平行语料库类似于有监督数据时。挑战在于，如果非平行语料库中没有足够的样本可以匹配以创建伪平行语料库，或者当
    IBT 无法很好地自举或未能收敛时，这种方法可能不起作用。训练 IBT 的时间复杂度也非常高，因为它需要迭代生成伪平行语料库并重新训练模型。未来有趣的方向包括减少计算成本、设计更有效的自举方法和改善
    IBT 的收敛性。
- en: 6.2.2 Understanding the Evolution from Traditional NLG to Deep Learning Methods
  id: totrans-424
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 理解从传统 NLG 到深度学习方法的演变
- en: Despite the exciting methodological revolution led by deep learning recently,
    we are also interested in the merging point of traditional computational linguistics
    and the deep learning techniques Henderson ([2020](#bib.bib64)). Specific to the
    context of TST, we will introduce the traditional NLG framework, and its impact
    on the current TST approaches, especially the prototype editing method.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习最近带来了令人兴奋的方法论革命，我们仍然对传统计算语言学和深度学习技术的融合点感兴趣 Henderson ([2020](#bib.bib64))。针对
    TST 的背景，我们将介绍传统 NLG 框架及其对当前 TST 方法，特别是原型编辑方法的影响。
- en: 6.2.2.0.1 Traditional NLG Framework.
  id: totrans-426
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.2.0.1 传统 NLG 框架。
- en: 'The traditional NLG framework stages sentence generation into the following
    steps Reiter and Dale ([1997](#bib.bib158)):'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 传统 NLG 框架将句子生成分为以下几个步骤 Reiter 和 Dale ([1997](#bib.bib158))：
- en: '1.'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Content determination (not applicable)
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内容确定（不适用）
- en: '2.'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Discourse planning (not applicable)
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 话语规划（不适用）
- en: '3.'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Sentence aggregation
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子聚合
- en: '4.'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Lexicalization
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 词汇化
- en: '5.'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Referring expression generation
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指代表达生成
- en: '6.'
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Linguistic realization
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言实现
- en: The first two steps, content determination and discourse planning are not applicable
    to most datasets because the current focus of TST is sentence-level and not discourse-level.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个步骤，内容确定和话语规划，对于大多数数据集不适用，因为目前 TST 的重点是句子级别而非话语级别。
- en: Among Steps 3 to 6, sentence aggregation groups necessary information into a
    single sentence, lexicalization chooses the right word to express the concepts
    generated by sentence aggregation, referring expression generation produces surface
    linguistic forms for domain entities, and linguistic realization edits the text
    so that it conforms to grammar, including syntax, morphology, and orthography.
    This framework is widely applied to NLG tasks (e.g., Zue and Glass, [2000](#bib.bib239);
    Mani, [2001](#bib.bib124); McTear, [2002](#bib.bib128); Gatt and Reiter, [2009](#bib.bib46);
    Androutsopoulos and Malakasiotis, [2010](#bib.bib2)).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步到第6步中，句子聚合将必要的信息整合到一个句子中，词汇化选择合适的词来表达句子聚合生成的概念，指代表达生成为领域实体生成表面语言形式，而语言实现则编辑文本，使其符合语法，包括句法、形态和正字法。该框架广泛应用于自然语言生成（NLG）任务（例如，Zue
    和 Glass, [2000](#bib.bib239); Mani, [2001](#bib.bib124); McTear, [2002](#bib.bib128);
    Gatt 和 Reiter, [2009](#bib.bib46); Androutsopoulos 和 Malakasiotis, [2010](#bib.bib2)）。
- en: 6.2.2.0.2 Re-Viewing Prototype-Based TST.
  id: totrans-442
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.2.0.2 重新审视基于原型的 TST。
- en: 'Among the approaches introduced so far, the most relevant for the traditional
    NLG is the prototype-based text editing, which has been introduced in Section [5.2](#S5.SS2
    "5.2 Prototype Editing ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text
    Style Transfer: A Survey").'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '在目前介绍的方法中，最相关的传统 NLG 方法是基于原型的文本编辑，该方法在第[5.2](#S5.SS2 "5.2 Prototype Editing
    ‣ 5 Methods on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey")节中介绍。'
- en: Using the language of the traditional NLG framework, the prototype-based techniques
    can be viewed as a combination of sentence aggregation, lexicalization, and linguistic
    realization. Specifically, prototype-based techniques first prepare an attribute-free
    sentence template, and supply it with candidate attribute markers that carry the
    desired attribute, both of which are sentence aggregation. Then, using language
    models to infill the prototype with the correct expressions corresponds to lexicalization
    and linguistic realization. Note that the existing TST systems do not explicitly
    deal with referring expression generation (e.g., generating co-references), leaving
    it to be handled by language models.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 使用传统NLG框架的语言，基于原型的技术可以被视为句子聚合、词汇化和语言实现的组合。具体而言，基于原型的技术首先准备一个无属性的句子模板，并用带有期望属性的候选属性标记填充它，这两个步骤都属于句子聚合。然后，使用语言模型将原型填充上正确的表达对应于词汇化和语言实现。请注意，现有的TST系统没有明确处理指代表达生成（例如，生成共指），这留给语言模型处理。
- en: 6.2.2.0.3 Meeting Point of Traditional and New Methods.
  id: totrans-445
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.2.0.3 传统方法与新方法的交汇点。
- en: Viewing prototype-based editing as a merging point where traditional, controllable
    framework meets deep learning models, we can see that it takes advantage of the
    powerful deep learning models and the interpretable pipeline of the traditional
    NLG. There are several advantages in merging the traditional NLG with the deep
    learning models. First, sentence planning-like steps make the generated contents
    more controllable. For example, the template of the original sentence is saved,
    and the counterpart attributes can also be explicitly retrieved, as a preparation
    for the final rewriting. Such a controllable, white-box approach can be easy to
    tune, debug, and improve. The accuracy of attribute marker extraction, for example,
    is constantly improving across literature Sudhakar, Upadhyay, and Maheswaran ([2019](#bib.bib190))
    and different ways to extract attribute markers can be easily fused Wu et al.
    ([2019](#bib.bib214)). Second, sentence planning-like steps ensure the truthfulness
    of information. As most content words are kept and no additional information is
    hallucinated by the black-box neural networks, we can better ensure that the information
    of the attribute-transferred output is consistent with the original input.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 将基于原型的编辑视为传统的、可控的框架与深度学习模型相交汇的点，我们可以看到它充分利用了强大的深度学习模型和传统NLG的可解释性流程。将传统NLG与深度学习模型结合有几个优势。首先，类似于句子规划的步骤使得生成的内容更加可控。例如，保存原句的模板，并且可以明确检索到对应的属性，为最终的重写做准备。这样的可控、白盒方法便于调整、调试和改进。例如，属性标记提取的准确性在Sudhakar、Upadhyay和Maheswaran（[2019](#bib.bib190)）的文献中不断提高，且不同的属性标记提取方法可以轻松融合Wu等（[2019](#bib.bib214)）。其次，类似于句子规划的步骤确保了信息的真实性。由于大多数内容词被保留，并且黑箱神经网络不会产生额外信息，我们可以更好地确保属性转移输出的信息与原始输入一致。
- en: 6.2.3 Inspiration from Tasks with Similar Nature
  id: totrans-447
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3 类似任务的启示
- en: An additional perspective that can inspire new methodological innovation is
    insights from other tasks that share a similar nature as TST. We will introduce
    in this section several closely-related tasks, including machine translation,
    image style transfer, style-conditioned language modeling, counterfactual story
    rewriting, contrastive text generation, and prototype-based text editing.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 一个能够启发新方法创新的额外视角是来自与TST具有相似性质的其他任务的见解。本节将介绍几个紧密相关的任务，包括机器翻译、图像风格迁移、风格条件语言建模、反事实故事重写、对比文本生成和基于原型的文本编辑。
- en: 6.2.3.0.1 Machine Translation.
  id: totrans-449
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.3.0.1 机器翻译。
- en: 'The problem settings of machine translation and text style transfer share much
    in common: the source and target language in machine translation is analogous
    to the original and desired attribute, $a$ and $a^{\prime}$, respectively. The
    major difference is that in NMT, the source and target corpora are in completely
    different languages, which have almost disjoint word vocabulary, whereas in text
    style transfer, the input and output are in the same language, and the model is
    usually encouraged to copy most content words from input such as the BoW loss
    introduced in Section [5.1.3.0.5](#S5.SS1.SSS3.P5 "5.1.3.0.5 Language Modeling
    on Outputs (LMO). ‣ 5.1.3 Training Objectives ‣ 5.1 Disentanglement ‣ 5 Methods
    on Non-Parallel Data ‣ Deep Learning for Text Style Transfer: A Survey"). Some
    TST works have been inspired by MT, such as the pseudo-parallel construction Nikolov
    and Hahnloser ([2019](#bib.bib134)); Zhang et al. ([2018d](#bib.bib234)), and
    in the future there may be more interesting intersections.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译和文本风格迁移的问题设置有许多相似之处：机器翻译中的源语言和目标语言类似于原始属性和期望属性 $a$ 和 $a^{\prime}$。主要的区别在于，在
    NMT 中，源语言和目标语言是完全不同的语言，几乎没有重叠的词汇，而在文本风格迁移中，输入和输出是在同一种语言中，模型通常被鼓励从输入中复制大部分内容词，例如第
    [5.1.3.0.5](#S5.SS1.SSS3.P5 "5.1.3.0.5 语言建模 (LMO)。 ‣ 5.1.3 训练目标 ‣ 5.1 解缠结 ‣ 5
    方法论 ‣ 深度学习文本风格迁移的调查") 节中介绍的 BoW 损失。一些 TST 工作受到了 MT 的启发，例如伪平行构造 Nikolov 和 Hahnloser
    ([2019](#bib.bib134)）；Zhang 等 ([2018d](#bib.bib234)），未来可能会有更多有趣的交集。
- en: 6.2.3.0.2 Data-to-Text Generation.
  id: totrans-451
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.3.0.2 数据到文本生成。
- en: Data-to-text generation is another potential domain that can draw inspiration
    from and to TST. The data-to-text generation task is to generate textual descriptions
    from structured data such as tables Wiseman, Shieber, and Rush ([2017](#bib.bib212));
    Parikh et al. ([2020](#bib.bib143)), meaning representations Novikova, Dusek,
    and Rieser ([2017](#bib.bib138)), or Resource Description Framework (RDF) triples
    Gardent et al. ([2017](#bib.bib44)); Ferreira et al. ([2020](#bib.bib38)). With
    the recent rise of pretrained seq2seq models for transfer learning Raffel et al.
    ([2020](#bib.bib155)), it is common to formulate data-to-text as a seq2seq task
    by serializing the structured data into a sequence Kale and Rastogi ([2020](#bib.bib85));
    Ribeiro et al. ([2020](#bib.bib163)); Guo et al. ([2020](#bib.bib58)). Then data-to-text
    generation can be seen as a special form of TST from structured information to
    text. This potential connection has not yet been investigated but worth exploring.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 数据到文本生成是另一个可以从 TST 中获得灵感的潜在领域。数据到文本生成任务是从结构化数据（例如表格 Wiseman、Shieber 和 Rush ([2017](#bib.bib212)）；Parikh
    等 ([2020](#bib.bib143)），语义表示 Novikova、Dusek 和 Rieser ([2017](#bib.bib138)），或资源描述框架（RDF）三元组
    Gardent 等 ([2017](#bib.bib44)）；Ferreira 等 ([2020](#bib.bib38)）生成文本描述。随着最近预训练 seq2seq
    模型在迁移学习中的崛起 Raffel 等 ([2020](#bib.bib155)），将数据到文本任务作为 seq2seq 任务来处理已经变得很常见，通过将结构化数据序列化为序列
    Kale 和 Rastogi ([2020](#bib.bib85)）；Ribeiro 等 ([2020](#bib.bib163)）；Guo 等 ([2020](#bib.bib58)）。然后数据到文本生成可以看作是从结构化信息到文本的
    TST 的一种特殊形式。这种潜在的联系尚未被研究，但值得探索。
- en: 6.2.3.0.3 Neural Style Transfer.
  id: totrans-453
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.3.0.3 神经风格迁移。
- en: Neural style transfer first originates in image style transfer Gatys, Ecker,
    and Bethge ([2016](#bib.bib47)), and its disentanglement ideas inspired some early
    TST researcg Shen et al. ([2017](#bib.bib182)). The difference between image style
    transfer and TST is that, for images, it is feasible to disentangle the explicit
    representation of the image texture as the gram matrix of image neural feature
    vectors, but for text, styles do not have such an explicit representation, but
    more abstract attributes. Besides this difference, many other aspects of style
    transfer research can have shared nature. Note that there are style transfer works
    across different modalities, including images Gatys, Ecker, and Bethge ([2016](#bib.bib47));
    Zhu et al. ([2017](#bib.bib237)); Chen et al. ([2017b](#bib.bib31)), text, voice
    Gao, Singh, and Raj ([2018](#bib.bib43)); Qian et al. ([2019](#bib.bib151)); Yuan
    et al. ([2021](#bib.bib226)), handwriting Azadi et al. ([2018](#bib.bib5)); Zhang
    and Liu ([2013](#bib.bib231)), and videos Ruder, Dosovitskiy, and Brox ([2016](#bib.bib168));
    Chen et al. ([2017a](#bib.bib30)). Many new advances in one style transfer field
    can inspire another style transfer field. For example, image style transfer has
    been used as a way for data augmentation Zheng et al. ([2019](#bib.bib236)); Jackson
    et al. ([2019](#bib.bib75)) and adversarial attack Xu et al. ([2020](#bib.bib219)),
    but TST has not yet been applied for such usage.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 神经风格迁移最初起源于图像风格迁移 Gatys, Ecker, and Bethge ([2016](#bib.bib47))，其解耦思想启发了早期的
    TST 研究 Shen et al. ([2017](#bib.bib182))。图像风格迁移和 TST 之间的区别在于，对于图像，可以将图像纹理的显式表示作为图像神经特征向量的
    Gram 矩阵进行解耦，但对于文本，风格没有这种显式表示，而是更抽象的属性。除了这一差异之外，风格迁移研究的许多其他方面可以具有共享的特性。请注意，不同模态之间的风格迁移研究，包括图像
    Gatys, Ecker, and Bethge ([2016](#bib.bib47))；Zhu et al. ([2017](#bib.bib237))；Chen
    et al. ([2017b](#bib.bib31))、文本、语音 Gao, Singh, and Raj ([2018](#bib.bib43))；Qian
    et al. ([2019](#bib.bib151))；Yuan et al. ([2021](#bib.bib226))、手写 Azadi et al.
    ([2018](#bib.bib5))；Zhang and Liu ([2013](#bib.bib231)) 和视频 Ruder, Dosovitskiy,
    and Brox ([2016](#bib.bib168))；Chen et al. ([2017a](#bib.bib30))。一个风格迁移领域中的许多新进展可以激发另一个风格迁移领域。例如，图像风格迁移已被用作数据增强
    Zheng et al. ([2019](#bib.bib236))；Jackson et al. ([2019](#bib.bib75)) 和对抗攻击 Xu
    et al. ([2020](#bib.bib219)) 的方法，但 TST 尚未用于这种用途。
- en: 6.2.3.0.4 Style-Conditioned Language Modeling.
  id: totrans-455
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.3.0.4 风格条件语言建模。
- en: Different from language modeling that learns how to generate general natural
    language text, conditional language modeling learns how to generate text given
    a condition, such as some context, or a control code Pfaff ([1979](#bib.bib144));
    Poplack ([2000](#bib.bib145)). Recent advances of conditional language models
    Keskar et al. ([2019](#bib.bib88)); Dathathri et al. ([2020](#bib.bib35)) also
    include text generation conditioned on a style token, such as positive or negative.
    Possible conditions include author style Syed et al. ([2020](#bib.bib192)), speaker
    identity, persona and emotion Li et al. ([2016](#bib.bib106)), genre, attributes
    derived from text, topics, and sentiment Ficler and Goldberg ([2017](#bib.bib39)).
    They are currently limited to a small set of pre-defined “condition” tokens and
    can only generate from scratch a sentence, but not yet able to be conditioned
    on an original sentence for style rewriting. The interesting finding in this research
    direction is that it can make good use of a pretrained LM and just do some light-weight
    inference techniques to generate style-conditioned text, so perhaps such approaches
    can inspire future TST methods and reduce the carbon footprints of training TST
    models from scratch.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 与学习如何生成一般自然语言文本的语言建模不同，有条件语言建模学习如何在给定条件下生成文本，例如某些上下文或控制代码 Pfaff ([1979](#bib.bib144))；Poplack
    ([2000](#bib.bib145))。最近的条件语言模型进展 Keskar et al. ([2019](#bib.bib88))；Dathathri
    et al. ([2020](#bib.bib35)) 还包括基于风格标记的文本生成，例如积极或消极。可能的条件包括作者风格 Syed et al. ([2020](#bib.bib192))、说话者身份、角色和情感
    Li et al. ([2016](#bib.bib106))、体裁、文本衍生的属性、主题和情感 Ficler and Goldberg ([2017](#bib.bib39))。目前，这些模型仅限于一小组预定义的“条件”标记，并且只能从头生成一个句子，但尚未能够以原始句子为条件进行风格重写。这一研究方向的有趣发现是，它可以很好地利用预训练的语言模型，并仅需一些轻量级的推理技术来生成风格条件的文本，因此这种方法可能会激发未来的
    TST 方法，并减少从头训练 TST 模型的碳足迹。
- en: 6.2.3.0.5 Counterfactual Story Rewriting.
  id: totrans-457
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.3.0.5 反事实故事重写。
- en: Counterfactual story rewriting aims to learn a new event sequence in the presence
    of a perturbation of a previous event (i.e., counterfactual condition) Goodman
    ([1947](#bib.bib52)); Starr ([2019](#bib.bib189)). Qin et al. ([2019](#bib.bib153))
    propose the first dataset, each sample of which takes an originally five-sentence
    story, and changes the event in the second sentence to a new, counterfactual event.
    The task is to generate the last three sentences of the story based on the newly
    altered second sentence that initiates the story. The criteria of the counterfactual
    story rewriting include relevance with the first two sentences, and minimal edits
    from the original story ending. This line of research is relatively difficult
    to directly apply to TST, because its motivation and dataset nature is different
    from the general text style transfer, and more importantly, this task is not conditioned
    on a predefined categorized style token, but the free-form textual story beginning.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实故事重写旨在学习在前一个事件的扰动（即反事实条件）下的新事件序列 Goodman ([1947](#bib.bib52)); Starr ([2019](#bib.bib189))。Qin
    等人 ([2019](#bib.bib153)) 提出了第一个数据集，每个样本取自原本五句话的故事，并将第二句话中的事件更改为新的反事实事件。任务是根据新更改的第二句话生成故事的最后三句话，该句子启动了故事。反事实故事重写的标准包括与前两句话的相关性，以及与原始故事结尾的最小编辑。这一研究方向相对较难直接应用于
    TST，因为其动机和数据集性质与一般文本风格迁移不同，更重要的是，这项任务不是以预定义的分类样式标记为条件，而是以自由形式的文本故事开头为条件。
- en: 6.2.3.0.6 Contrastive Text Generation.
  id: totrans-459
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.3.0.6 对比文本生成。
- en: 'As neural network-based NLP models more easily learn spurious statistical correlations
    in the data rather than achieve robust understanding Jia and Liang ([2017](#bib.bib79)),
    there are recent works to construct auxillary datasets composed of near-misses
    of the original data. For example, Gardner et al. ([2020](#bib.bib45)) ask crowdsource
    workers to rewrite the input of the task with minimal changes but matching a different
    target label. To alleviate expensive human labor, Xing et al. ([2020](#bib.bib216))
    develop an automatic text editing approach to generate contrast set for aspect-based
    sentiment analysis. The difference between contrastive text generation and text
    style transfer is that the former does not require content preservation but mainly
    aims to construct a slightly textually different input that can result in a change
    of the ground-truth output, to test the model robustness. So the two tasks are
    not completely the same, although they have some intersections that might inspire
    future work, such as aspect-based style transfer suggested in Section [6.1](#S6.SS1
    "6.1 Expanding the Scope of Styles ‣ 6 Research Agenda ‣ Deep Learning for Text
    Style Transfer: A Survey").'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '由于基于神经网络的 NLP 模型更容易学习数据中的虚假统计关联而不是实现稳健的理解 Jia 和 Liang ([2017](#bib.bib79))，近期有工作构建了由原始数据的近似错误组成的辅助数据集。例如，Gardner
    等人 ([2020](#bib.bib45)) 让众包工人对任务输入进行最小更改但匹配不同的目标标签。为了减轻昂贵的人力劳动，Xing 等人 ([2020](#bib.bib216))
    开发了一种自动文本编辑方法来生成对比集，以进行基于方面的情感分析。对比文本生成与文本风格迁移的区别在于，前者不要求内容保持不变，而主要旨在构建略微文本不同的输入，这可以导致地面真实输出的变化，以测试模型的鲁棒性。因此，尽管这两个任务有一些交集可能会启发未来的工作，例如第
    [6.1](#S6.SS1 "6.1 Expanding the Scope of Styles ‣ 6 Research Agenda ‣ Deep Learning
    for Text Style Transfer: A Survey") 节建议的基于方面的风格迁移，但它们并不完全相同。'
- en: 6.2.3.0.7 Prototype-Based Text Editing.
  id: totrans-461
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.2.3.0.7 原型基础的文本编辑。
- en: Prototype editing is not unique in TST, but also widely used in other NLP tasks.
    Knowing the new advances in prototype editing for other tasks can potentially
    inspire new method innovations in TST. Guu et al. ([2018](#bib.bib60)) first proposes
    the protype editing approach to improve LM by first sampling a lexically similar
    sentence prototype and then editing it using variational encoder and decoders.
    This prototype-and-then-edit approach can also be seen in summarization Wang,
    Quan, and Wang ([2019](#bib.bib205)), machine translation Cao and Xiong ([2018](#bib.bib22));
    Wu, Wang, and Wang ([2019](#bib.bib213)); Gu et al. ([2018](#bib.bib56)); Zhang
    et al. ([2018a](#bib.bib228)); Bulté and Tezcan ([2019](#bib.bib20)), conversation
    generation Weston, Dinan, and Miller ([2018](#bib.bib210)); Cai et al. ([2019](#bib.bib21)),
    code generation Hashimoto et al. ([2018](#bib.bib62)), and question answering
    Lewis et al. ([2020](#bib.bib104)). As an extension to the retrieve and edit steps,
    Hossain, Ghazvininejad, and Zettlemoyer ([2020](#bib.bib67)) use an ensemble approach
    to retrieve a set of relevant prototypes, edit, and finally rerank to pick the
    best output for machine translation. Such extension can also be potentially applied
    to text style transfer.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 原型编辑在TST中并不是独特的，还广泛应用于其他NLP任务。了解其他任务中原型编辑的新进展可能会激发TST领域的新方法创新。Guu 等人（[2018](#bib.bib60)）首次提出了原型编辑方法，通过先采样一个词汇上类似的句子原型，然后使用变分编码器和解码器进行编辑，以改进语言模型。这种“先原型然后编辑”的方法也可以在摘要生成
    Wang, Quan 和 Wang（[2019](#bib.bib205)）、机器翻译 Cao 和 Xiong（[2018](#bib.bib22)）；Wu,
    Wang 和 Wang（[2019](#bib.bib213)）；Gu 等人（[2018](#bib.bib56)）；Zhang 等人（[2018a](#bib.bib228)）；Bulté
    和 Tezcan（[2019](#bib.bib20)）、对话生成 Weston, Dinan 和 Miller（[2018](#bib.bib210)）；Cai
    等人（[2019](#bib.bib21)）、代码生成 Hashimoto 等人（[2018](#bib.bib62)）以及问答系统 Lewis 等人（[2020](#bib.bib104)）中看到。作为检索和编辑步骤的扩展，Hossain,
    Ghazvininejad 和 Zettlemoyer（[2020](#bib.bib67)）使用集成方法来检索一组相关的原型，进行编辑，并最终重新排序以选择最佳输出用于机器翻译。这种扩展也可能应用于文本风格转换。
- en: 6.3 Loosening the Style-Specific Dataset Assumptions
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 放宽风格特定数据集假设
- en: 'A common assumption for most deep learning-based TST works, as mentioned in
    Section [2.1.0.0.2](#S2.SS1.SSS0.P2 "2.1.0.0.2 Data-Driven Definition of Style
    as the Scope of this Survey. ‣ 2.1 How to Define Style? ‣ 2 What Is Text Style
    Transfer? ‣ Deep Learning for Text Style Transfer: A Survey"), is the availability
    of style-specific corpora for each style of interest, either parallel or non-parallel.
    This assumption can potentially be loosened in two ways.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 正如第[2.1.0.0.2](#S2.SS1.SSS0.P2 "2.1.0.0.2 数据驱动的风格定义作为本调查范围。 ‣ 2.1 如何定义风格？ ‣
    2 什么是文本风格转换？ ‣ 深度学习文本风格转换综述")节中提到的，大多数基于深度学习的TST工作通常假设每种感兴趣的风格都有风格特定的语料库，无论是平行的还是非平行的。这一假设可以通过两种方式得到放宽。
- en: 6.3.0.0.1 Linguistic Styles with No Matched Data.
  id: totrans-465
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.0.0.1 无匹配数据的语言风格
- en: 'Since there are various concerns raised by the data-driven definition of style
    as described in Section [2.1](#S2.SS1 "2.1 How to Define Style? ‣ 2 What Is Text
    Style Transfer? ‣ Deep Learning for Text Style Transfer: A Survey"), a potentially
    good research direction is to bring back the linguistic definition of style, and
    thus remove some of the concerns associated with large datasets. Several methods
    can be a potential fit for this: prompt design Li and Liang ([2021](#bib.bib109));
    Qin and Eisner ([2021](#bib.bib152)); Scao and Rush ([2021](#bib.bib173)) that
    passes a prompt to GPT Radford et al. ([2019](#bib.bib154)); Brown et al. ([2020](#bib.bib19))
    to obtain a style-transferred text; style-specific template design; or use templates
    to first generate synthetic data and make models learn from the synthetic data.
    Prompt design is not yet investigated as a direction for TST research, but it
    is an interesting direction to explore.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第[2.1](#S2.SS1 "2.1 如何定义风格？ ‣ 2 什么是文本风格转换？ ‣ 深度学习文本风格转换综述")节中描述的数据驱动风格定义提出了各种担忧，一个潜在的良好研究方向是恢复语言学风格定义，从而消除与大型数据集相关的一些问题。几个方法可能适合这一方向：提示设计
    Li 和 Liang（[2021](#bib.bib109)）；Qin 和 Eisner（[2021](#bib.bib152)）；Scao 和 Rush（[2021](#bib.bib173)），这些方法将提示传递给
    GPT Radford 等人（[2019](#bib.bib154)）；Brown 等人（[2020](#bib.bib19)）以获得风格转换文本；风格特定模板设计；或使用模板首先生成合成数据，并使模型从合成数据中学习。提示设计尚未被研究作为TST研究的方向，但这是一个值得探索的有趣方向。
- en: 6.3.0.0.2 Distinguishing Styles from a Mixed Corpus.
  id: totrans-467
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 6.3.0.0.2 从混合语料库中区分风格
- en: It might also be possible to distinguish styles direction from a mixed corpus
    with no style labels. For example, Riley et al. ([2021a](#bib.bib164)) learn a
    style vector space from text; Xu, Cheung, and Cao ([2020](#bib.bib218)) use unsupervised
    representation learning to separate the style and contents from a mixed corpus
    of unspecified styles; Guo et al. ([2021](#bib.bib59)) use cycle training with
    a conditional variational auto-encoder to unsupervisedly learn to express the
    same semantics through different styles. Theoretically, although disentanglement
    is impossible without inductive biases or other forms of supervision Locatello
    et al. ([2019](#bib.bib114)), disentanglement is achievable with some weak signals,
    such as only knowing how many factors have changed, but not which ones Locatello
    et al. ([2020](#bib.bib115)). A more advanced direction can be emergent styles
    Kang, Wang, and de Melo ([2020](#bib.bib86)), since styles can be evolving, for
    example across dialog turns.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有风格标签的混合语料中区分风格方向也是可能的。例如，Riley 等人 ([2021a](#bib.bib164)) 从文本中学习了风格向量空间；Xu、Cheung
    和 Cao ([2020](#bib.bib218)) 使用无监督表示学习将风格与内容从未指定风格的混合语料中分离；Guo 等人 ([2021](#bib.bib59))
    使用条件变分自编码器的循环训练无监督地学习通过不同风格表达相同语义。理论上，虽然没有归纳偏差或其他形式的监督，**解缠结**是不可能的 Locatello
    等人 ([2019](#bib.bib114))，但通过一些弱信号，例如仅知道多少因素发生了变化但不知道具体哪些因素 Locatello 等人 ([2020](#bib.bib115))，**解缠结**是可以实现的。一个更先进的方向是新兴风格
    Kang、Wang 和 de Melo ([2020](#bib.bib86))，因为风格可能会不断演变，例如在对话轮次中。
- en: 6.4 Improving Evaluation Metrics
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 改进评估指标
- en: There has been a lot of attention to the problems of evaluation metrics of TST
    and potential improvements Pang and Gimpel ([2019](#bib.bib141)); Tikhonov and
    Yamshchikov ([2018](#bib.bib197)); Mir et al. ([2019](#bib.bib130)); Fu et al.
    ([2019](#bib.bib40)); Pang ([2019](#bib.bib140)); Yamshchikov et al. ([2021](#bib.bib222));
    Jafaritazehjani et al. ([2020](#bib.bib76)). Recently, Gehrmann et al. ([2021](#bib.bib48))
    has proposed a new framework which is a live environment to evaluate NLG in a
    principled and reproducible manner. Apart from the existing scoring methods, future
    works can also make use of linguistic rules such as a checklist to evaluate what
    capabilities the TST model has achieved. For example, there can be a checklist
    for formality transfer according to existing style guidelines such as the APA
    style guide American Psychological Association ([1983](#bib.bib1)). Such a checklist-based
    evaluation can make the performance of black-box deep learning models more interpretable,
    and also allow for more insightful error analysis.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 对 TST 评估指标的问题和潜在改进已有大量关注 Pang 和 Gimpel ([2019](#bib.bib141))；Tikhonov 和 Yamshchikov
    ([2018](#bib.bib197))；Mir 等人 ([2019](#bib.bib130))；Fu 等人 ([2019](#bib.bib40))；Pang
    ([2019](#bib.bib140))；Yamshchikov 等人 ([2021](#bib.bib222))；Jafaritazehjani 等人
    ([2020](#bib.bib76))。最近，Gehrmann 等人 ([2021](#bib.bib48)) 提出了一个新框架，这是一种在原则性和可重复的方式下评估
    NLG 的实时环境。除了现有的评分方法外，未来的工作还可以利用语言规则，例如清单，来评估 TST 模型已实现的能力。例如，可以根据现有的风格指南（如 APA
    风格指南）制定形式转移的清单 American Psychological Association ([1983](#bib.bib1))。这种基于清单的评估可以使黑箱深度学习模型的性能更具可解释性，同时也允许进行更深入的错误分析。
- en: 7 Expanding the Impact of TST
  id: totrans-471
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 扩大 TST 的影响
- en: 'In this last section of this survey, we highlight several directions to expand
    the impact of TST. First, TST can be used to help other NLP tasks such as paraphrasing,
    data augmentation, and adversarial robustness probing (Section [7.1](#S7.SS1 "7.1
    Connecting TST to More NLP Tasks ‣ 7 Expanding the Impact of TST ‣ Deep Learning
    for Text Style Transfer: A Survey")). Moreover, many specialized downstream tasks
    can be achieved with the help of TST, such as persona-consistent dialog generation,
    attractive headline generation, style-specific machine translation, and anonymization
    (Section [7.2](#S7.SS2 "7.2 Connecting TST to More Specialized Applications ‣
    7 Expanding the Impact of TST ‣ Deep Learning for Text Style Transfer: A Survey")).
    Last but not the least, we overview the ethical impacts that are important to
    take into consideration for future development of TST (Section [7.3](#S7.SS3 "7.3
    Considering Ethical Impacts of TST ‣ 7 Expanding the Impact of TST ‣ Deep Learning
    for Text Style Transfer: A Survey")).'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '在本调查的最后一部分，我们突出了几种扩展TST影响力的方向。首先，TST可以用于帮助其他NLP任务，如**释义生成**、数据增强和对抗鲁棒性探测（第[7.1节](#S7.SS1
    "7.1 Connecting TST to More NLP Tasks ‣ 7 Expanding the Impact of TST ‣ Deep Learning
    for Text Style Transfer: A Survey")）。此外，许多专业的下游任务可以在TST的帮助下实现，如个性一致的对话生成、吸引人的标题生成、风格特定的机器翻译和匿名化（第[7.2节](#S7.SS2
    "7.2 Connecting TST to More Specialized Applications ‣ 7 Expanding the Impact
    of TST ‣ Deep Learning for Text Style Transfer: A Survey")）。最后但同样重要的是，我们概述了对于TST未来发展的伦理影响（第[7.3节](#S7.SS3
    "7.3 Considering Ethical Impacts of TST ‣ 7 Expanding the Impact of TST ‣ Deep
    Learning for Text Style Transfer: A Survey")）。'
- en: 7.1 Connecting TST to More NLP Tasks
  id: totrans-473
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 将TST与更多NLP任务连接起来
- en: Text style transfer can be applied to other important NLP tasks, such as paraphrase
    generation, data augmentation, and adversarial robustness probing.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 文本风格转换可以应用于其他重要的NLP任务，如**释义生成**、数据增强和对抗鲁棒性探测。
- en: 7.1.0.0.1 Paraphrase Generation.
  id: totrans-475
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 7.1.0.0.1 **释义生成**。
- en: 'Paraphrase generation is to express the same information in alternative ways
    Madnani and Dorr ([2010](#bib.bib120)). The nature of paraphrasing shares a lot
    in common with TST, which is to transfer the style of text while preserving the
    content. One of the common ways of paraphrasing is syntactic variation, such as
    “X wrote Y.”, “Y was written by X.” and “X is the writer of Y.” Androutsopoulos
    and Malakasiotis ([2010](#bib.bib2)). Besides syntactic variation, it also makes
    sense to include stylistic variation as a form of paraphrases, which means that
    the linguistic style transfer (not the content preference transfer in Table [3](#S2.T3
    "Table 3 ‣ 2.3 Existing Subtasks with Datasets ‣ 2 What Is Text Style Transfer?
    ‣ Deep Learning for Text Style Transfer: A Survey")) can be regarded as a subset
    of paraphrasing. The caution here is that if the paraphrasing is for a downstream
    task, researchers should first check if the downstream task is compatible with
    the used styles. For example, dialog generation may be sensitive to all linguistic
    styles, whereas summarization can allow linguistic style-varied paraphrases in
    the dataset.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '释义生成是用不同的方式表达相同的信息，Madnani和Dorr ([2010](#bib.bib120))。释义的性质与TST有很多相似之处，即在保持内容的同时转移文本的风格。常见的释义方式之一是句法变体，如“X
    写了 Y.”、“Y 被 X 写。”和“X 是 Y 的作者。”Androutsopoulos和Malakasiotis ([2010](#bib.bib2))。除了句法变体，还可以将风格变体作为释义的一种形式，这意味着语言风格转移（而不是表格[3](#S2.T3
    "Table 3 ‣ 2.3 Existing Subtasks with Datasets ‣ 2 What Is Text Style Transfer?
    ‣ Deep Learning for Text Style Transfer: A Survey")中的内容偏好转移）可以看作是释义的一个子集。这里需要注意的是，如果释义是用于下游任务，研究人员应首先检查下游任务是否与所使用的风格兼容。例如，对话生成可能对所有语言风格都很敏感，而摘要生成可以允许数据集中存在语言风格不同的释义。'
- en: There are three implications of this connection of TST and paraphrase generation.
    First, many trained TST models can be borrowed for paraphrasing, such as formality
    transfer and simplification. A second connection is that the method innovations
    proposed in the two fields can inspire each other. For example, Krishna, Wieting,
    and Iyyer ([2020](#bib.bib96)) formulate style transfer as a paraphrasing task.
    Thirdly, the evaluation metrics of the two tasks can also inspire each other.
    For example, Yamshchikov et al. ([2021](#bib.bib222)) associate the semantic similarity
    metrics for two tasks.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: TST和**释义生成**的这种关联有三个含义。首先，许多训练好的TST模型可以借用来进行释义生成，如正式性转移和简化。第二个关联是这两个领域中提出的方法创新可以相互启发。例如，Krishna、Wieting和Iyyer
    ([2020](#bib.bib96)) 将风格转移公式化为释义任务。第三，两个任务的评估指标也可以相互启发。例如，Yamshchikov等 ([2021](#bib.bib222))
    将两个任务的语义相似性指标相关联。
- en: 7.1.0.0.2 Data Augmentation.
  id: totrans-478
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 7.1.0.0.2 数据增强。
- en: Data augmentation generates text similar to the existing training data so that
    the model can have larger training data. TST is a good method for data augmentation
    because TST can produce text with different styles but the same meaning. Image
    style transfer has already been used for data augmentation Zheng et al. ([2019](#bib.bib236));
    Jackson et al. ([2019](#bib.bib75)), so it can be interesting to see future works
    to also apply text style transfer for data augmentation.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强生成与现有训练数据相似的文本，以便模型可以获得更多的训练数据。TST是数据增强的好方法，因为TST可以生成具有不同风格但含义相同的文本。图像风格迁移已经用于数据增强Zheng
    et al. ([2019](#bib.bib236))；Jackson et al. ([2019](#bib.bib75))，因此，看到未来的工作也将文本风格迁移应用于数据增强将是非常有趣的。
- en: 7.1.0.0.3 Adversarial Robustness Probing.
  id: totrans-480
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 7.1.0.0.3 对抗性鲁棒性检测。
- en: Another use of style transferred text is adversarial robustness probing. For
    example, styles that are task-agnostic can be used for general adversarial attack
    (e.g., politeness transfer to probe sentiment classification robustness) Jin et al.
    ([2020b](#bib.bib81)), while the styles that can change the task output can be
    used to construct contrast sets (e.g., sentiment transfer to probe sentiment classification
    robustness) Xing et al. ([2020](#bib.bib216)). Xu et al. ([2020](#bib.bib219))
    applies image style transfer to adversarial attack, and future research can also
    explore the use of TST in the two ways suggested above.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 风格迁移文本的另一种用途是对抗性鲁棒性检测。例如，可以使用与任务无关的风格进行一般对抗性攻击（例如，通过礼貌转移来检测情感分类的鲁棒性）Jin et al.
    ([2020b](#bib.bib81))，而可以改变任务输出的风格则可用于构建对比集（例如，通过情感转移来检测情感分类的鲁棒性）Xing et al. ([2020](#bib.bib216))。Xu
    et al. ([2020](#bib.bib219)) 将图像风格迁移应用于对抗性攻击，未来的研究还可以探索TST在上述两种方式中的应用。
- en: 7.2 Connecting TST to More Specialized Applications
  id: totrans-482
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 将TST连接到更多专业应用
- en: TST can be applied not only to other NLP tasks as introduced in the previous
    section, but also be helpful for specialized downstream applications. In practice,
    when applying NLP models, it is important to customize for some specific needs,
    such as generating dialog with a consistent persona, writing headlines that are
    attractive and engaging, making machine translation models adapt to different
    styles, and anonymizing the user identity by obfuscating the style.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: TST不仅可以应用于前一节介绍的其他NLP任务，还可以对专业下游应用有所帮助。在实际应用NLP模型时，定制一些特定需求是重要的，例如生成具有一致个性的对话、编写引人注目的标题、使机器翻译模型适应不同风格，以及通过模糊风格来匿名用户身份。
- en: 7.2.0.0.1 Persona-Consistent Dialog Generation.
  id: totrans-484
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 7.2.0.0.1 个性一致的对话生成。
- en: A useful downstream application of TST is persona-consistent dialog generation
    Li et al. ([2016](#bib.bib106)); Zhang et al. ([2018b](#bib.bib229)); Shuster
    et al. ([2020](#bib.bib185)). Since conversational agents directly interact with
    users, there is a strong demand for human-like dialog generation. Previously,
    this is done by encoding speaker traits into a vector and the conversation is
    then conditioned on this vector Li et al. ([2016](#bib.bib106)). As future work,
    text style transfer can also be used as part of the pipeline of persona-based
    dialog generation, where the persona can be categorized into distinctive style
    types, and then the generated text can be post-processed by a style transfer model.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: TST的一个有用下游应用是具有个性一致性的对话生成Li et al. ([2016](#bib.bib106))；Zhang et al. ([2018b](#bib.bib229))；Shuster
    et al. ([2020](#bib.bib185))。由于对话代理直接与用户互动，因此对类人对话生成的需求很强烈。以前，这通常是通过将说话者特征编码为向量，然后对话基于该向量进行调节Li
    et al. ([2016](#bib.bib106))。作为未来的工作，文本风格迁移也可以作为基于个性的对话生成管道的一部分，其中个性可以被分类为不同的风格类型，然后生成的文本可以由风格迁移模型进行后处理。
- en: 7.2.0.0.2 Attractive Headline Generation.
  id: totrans-486
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 7.2.0.0.2 吸引人的标题生成。
- en: In journalism writing, it is crucial to generate engaging headlines. Jin et al.
    ([2020a](#bib.bib80)) first use TST to generate eye-catchy headlines with three
    different styles, humorous, romantic, and clickbaity styles. Li et al. ([2021](#bib.bib108))
    follow this direction and propose a disentanglement-based model to generate attractive
    headlines for Chinese news.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 在新闻写作中，生成引人注目的标题至关重要。Jin et al. ([2020a](#bib.bib80)) 首次使用TST生成三种不同风格的引人注目的标题，包括幽默、浪漫和吸引点击的风格。Li
    et al. ([2021](#bib.bib108)) 跟随这一方向，并提出了一种基于解缠结的模型来为中文新闻生成吸引人的标题。
- en: 7.2.0.0.3 Style-Specific Machine Translation.
  id: totrans-488
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 7.2.0.0.3 风格特定的机器翻译。
- en: In machine translation, it is useful to have an additional control of the style
    for the translated text. Commonly used styles for TST in machine translation are
    politeness Sennrich, Haddow, and Birch ([2016a](#bib.bib176)) and formality Niu,
    Martindale, and Carpuat ([2017](#bib.bib136)); Wu, Wang, and Liu ([2020](#bib.bib215)).
    For example, Wu, Wang, and Liu ([2020](#bib.bib215)) translates from informal
    Chinese to formal English.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器翻译中，为翻译后的文本增加风格控制是很有用的。机器翻译中常用的 TST 风格包括礼貌 Sennrich、Haddow 和 Birch ([2016a](#bib.bib176))
    和正式性 Niu、Martindale 和 Carpuat ([2017](#bib.bib136)); Wu、Wang 和 Liu ([2020](#bib.bib215))。例如，Wu、Wang
    和 Liu ([2020](#bib.bib215)) 将非正式的中文翻译为正式的英语。
- en: 7.2.0.0.4 Anonymization.
  id: totrans-490
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 7.2.0.0.4 匿名化。
- en: TST can also be used for anonymization, which is an important way to protect
    user privacy, especially since there are ongoing heated discussions of ethics
    in the artificial intelligence community. Many concerns have been raised towards
    the discriminative task of author profiling, which can mine the demographic identities
    of the author of a writing, even including privacy-invading properties such as
    gender and age Schler et al. ([2006](#bib.bib174)). As a potential solution, TST
    can be applied to alter the text and obfuscate the real identity of the users
    Reddy and Knight ([2016](#bib.bib157)); Gröndahl and Asokan ([2020](#bib.bib54)).
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: TST 还可以用于匿名化，这是一种保护用户隐私的重要方法，特别是在人工智能社区对伦理的热烈讨论仍在继续的情况下。对作者画像的歧视性任务引发了许多担忧，这可以挖掘出作者的群体身份，甚至包括如性别和年龄等侵犯隐私的属性
    Schler 等 ([2006](#bib.bib174))。作为一种潜在的解决方案，TST 可以用于改变文本并模糊用户的真实身份 Reddy 和 Knight
    ([2016](#bib.bib157)); Gröndahl 和 Asokan ([2020](#bib.bib54))。
- en: 7.3 Considering Ethical Impacts of TST
  id: totrans-492
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 考虑 TST 的伦理影响
- en: 'Recently, there is more and more attention being paid to the ethical concerns
    associated with AI research. We discuss in the following two ethical considerations:
    (1) social impact of TST applications, and (2) data privacy problem of text style
    transfer.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，越来越多的关注集中在与人工智能研究相关的伦理问题上。我们讨论以下两个伦理考量：（1）TST 应用的社会影响，以及（2）文本风格转换的数据隐私问题。
- en: Fields that involve human subjects or direct application to humans work under
    a set of core principles and guidelines Beauchamp, Childress et al. ([2001](#bib.bib10)).
    Before initiating a research project, responsible research bodies use these principles
    as a ruler to judge whether the research is ethically correct to start. NLP reserch
    and applications, including TST, that directly involve human users, is regulated
    under a central regulatory board, Institutional Review Board (IRB). We also provide
    several guidelines below to avoid ethical misconduct in future publications on
    text style transfer.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及人类受试者或直接应用于人类的领域在一套核心原则和指南下工作 Beauchamp、Childress 等 ([2001](#bib.bib10))。在启动研究项目之前，负责的研究机构使用这些原则作为标准来判断研究是否伦理上正确。直接涉及人类用户的
    NLP 研究和应用，包括 TST，受到中央监管机构，即机构审查委员会（IRB）的监管。我们还提供了若干指南，以避免未来关于文本风格转换的出版物中的伦理失当。
- en: 7.3.1 Social Impact of TST Applications
  id: totrans-495
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1 TST 应用的社会影响
- en: Technologies can have unintended negative consequences Hovy and Spruit ([2016](#bib.bib68)).
    For example, TST can facilitate the automation of intelligent assistants with
    designed attributes, but can also be used to create fake text or fraud.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 技术可能会产生意想不到的负面后果 Hovy 和 Spruit ([2016](#bib.bib68))。例如，TST 可以促进具有设计属性的智能助手的自动化，但也可能被用于创建虚假文本或欺诈。
- en: 'Thus, inventors of a technology should beware how other people very probably
    adopt this technology for their own incentives. For TST, since it has a wide range
    of subtasks and applications, we examine each of them with the following two questions:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，技术发明者应当警惕其他人可能如何将这项技术用于自己的激励。对于 TST，由于其具有广泛的子任务和应用，我们用以下两个问题来检查每一个任务：
- en: •
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Who will benefit from such a technology?
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 谁将从这种技术中受益？
- en: •
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Who will be harmed by such a technology?
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 谁将受到这种技术的伤害？
- en: 'Although many ethical issues are debatable, we try to categorize the text attribute
    tasks into three ethical levels: beneficial, neutral, and tasks that can be obvious
    double-sided swords.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多伦理问题值得讨论，我们尝试将文本属性任务分类为三种伦理级别：有益的、中立的以及可能明显存在双面剑的任务。
- en: 7.3.1.0.1 Beneficial.
  id: totrans-503
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 7.3.1.0.1 有益的。
- en: An important direction of NLP for social good is to fight against abusive online
    text. Text style transfer can serve as a very helpful tool as it can be used to
    transfer malicious text to normal language. Shades of abusive language include
    hate speech, offensive language, sexist and racist language, aggression, profanity,
    cyberbullying, harassment, trolling, and toxic language Waseem et al. ([2017](#bib.bib208)).
    There are also other negative text such as propaganda Bernays ([2005](#bib.bib14));
    Carey ([1997](#bib.bib24)), and others. It is widely known that malicious text
    is harmful to people. For example, research shows that cyberbullying victims tend
    to have more stress and suicidal ideation Kowalski et al. ([2014](#bib.bib94)),
    and also detachment from family and offline victimization Oksanen et al. ([2014](#bib.bib139)).
    There are more and more efforts put into combating toxic language, such as 30K
    content moderators that Facebook and Instagram employ Harrison ([2019](#bib.bib61)).
    Therefore, the automatic malicious-to-normal language transfer can be a helpful
    intelligent assistant to address such needs. Apart from purifying malicious text
    on social media, it can also be used on social chatbots to make sure there are
    no bad contents in language they generate (Roller et al., [2021](#bib.bib166)).
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 在社会公益方面的一个重要方向是对抗恶意在线文本。文本风格转换可以作为一个非常有用的工具，因为它可以用于将恶意文本转换为正常语言。恶意语言的阴影包括仇恨言论、冒犯性语言、性别歧视和种族歧视语言、侵略性、亵渎、网络欺凌、骚扰、恶搞和有毒语言
    Waseem 等人 ([2017](#bib.bib208))。还有其他负面文本，如宣传 Bernays ([2005](#bib.bib14))；Carey
    ([1997](#bib.bib24)) 等。众所周知，恶意文本对人们有害。例如，研究表明，网络欺凌受害者往往有更多的压力和自杀念头 Kowalski 等人
    ([2014](#bib.bib94))，以及与家庭的脱节和离线受害 Oksanen 等人 ([2014](#bib.bib139))。对抗有毒语言的努力越来越多，例如
    Facebook 和 Instagram 雇佣的 30K 内容审核员 Harrison ([2019](#bib.bib61))。因此，自动的恶意到正常语言转换可以作为一个有用的智能助手来应对这些需求。除了净化社交媒体上的恶意文本外，它还可以用于社交聊天机器人，以确保其生成的语言中没有不良内容
    (Roller 等人, [2021](#bib.bib166))。
- en: 7.3.1.0.2 Neutral.
  id: totrans-505
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 7.3.1.0.2 中性。
- en: Most text style transfer tasks are neutral. For example, informal-to-formal
    transfer can be used as a writing assistant to help make writings more professional,
    and formal-to-informal transfer can tune the tone of bots to be more casual. Most
    applications to customized the persona of bots are also neutral with regard to
    their societal impact.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数文本风格转换任务是中性的。例如，非正式到正式的转换可以用作写作助手，帮助使写作更加专业，而正式到非正式的转换可以调整机器人的语气，使其更加随意。大多数定制化机器人个性的应用在其社会影响方面也属于中性。
- en: 7.3.1.0.3 Double-Sided Sword.
  id: totrans-507
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 7.3.1.0.3 双刃剑。
- en: Besides positive and neutral applications, there are, unfortunately, several
    text style transfer tasks that are double-sided swords. For example, one of the
    most popular TST tasks, sentiment modification, although it can be used to change
    intelligent assistants or robots from a negative to positive mood (which is unlikely
    to harm any parties), the vast majority of papers applies this technology to manipulate
    the polarity of reviews, such as Yelp Shen et al. ([2017](#bib.bib182)) and Amazon
    reviews He and McAuley ([2016](#bib.bib63)). This leads to a setting where a negative
    restaurant review is changed to a positive comment, or vice versa, with debatable
    ethics. Such a technique can be used as a cheating method for the commercial body
    to polish its reviews, or harm the reputation of their competitors. Once this
    technology is used, it will automatically manipulate the online text to contain
    polarity that the model owner desires. Hence, we suggest the research community
    raise serious concern against the review sentiment modification task.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 除了积极和中性的应用外，不幸的是，有些文本风格转换任务确实是双刃剑。例如，最流行的 TST 任务之一——情感修改，虽然可以将智能助手或机器人从负面情绪转换为积极情绪（这不太可能对任何一方造成伤害），但绝大多数论文将这项技术应用于操控评论的极性，例如
    Yelp Shen 等人 ([2017](#bib.bib182)) 和 Amazon 评论 He 和 McAuley ([2016](#bib.bib63))。这导致了一个设置，即将负面的餐馆评论更改为正面的评论，或反之，这种做法在伦理上有争议。这样的技术可以被商业体用作作弊手段来润色其评论，或损害竞争对手的声誉。一旦使用了这项技术，它会自动操控在线文本以包含模型所有者期望的极性。因此，我们建议研究界对评论情感修改任务提出严肃的关注。
- en: Another task, political slant transfer, may induce concerns within some specific
    context. For example, social bots (i.e., autonomous bots on social media, such
    as Twitter bots and Facebook bots) are a big problem in the U.S., even playing
    a significant role in the 2016 United States presidential election Bessi and Ferrara
    ([2016](#bib.bib15)); Shao et al. ([2018](#bib.bib179)). It is reported that at
    least 400,000 bots were responsible for about 19% of the total Tweets. Social
    bots usually target to advocate certain ideas, supporting campaigns, or aggregating
    other sources either by acting as a "follower" and/or gathering followers itself.
    So the political slant transfer task, which transfers the tone and content between
    republican comments and democratic ones, are highly sensitive and may face the
    risk of being used on social bots to manipulate political views of the mass.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个任务，政治倾向转换，可能在特定背景下引发担忧。例如，社交机器人（即社交媒体上的自动机器人，如Twitter机器人和Facebook机器人）在美国是一个大问题，甚至在2016年美国总统选举中扮演了重要角色
    Bessi 和 Ferrara ([2016](#bib.bib15))；Shao 等人 ([2018](#bib.bib179))。有报告称，至少有40万个机器人负责了约19%的总推文。社交机器人通常旨在倡导某些思想、支持运动或聚合其他来源，既可以作为“跟随者”，也可以自行聚集追随者。因此，政治倾向转换任务，即在共和党和民主党的评论之间转换语气和内容，非常敏感，可能面临被社交机器人用来操控大众政治观点的风险。
- en: Some more arguable ones are male-to-female tone transfer, which can be potentially
    used for identity deception. The cheater can create an online account and pretend
    to be an attractive young lady. There is also the reversed direction (female-to-male
    tone transfer) which can be used for applications such as authorship obfuscation
    Shetty, Schiele, and Fritz ([2018](#bib.bib184)) which anonymizes the author attributes
    by hiding the gender of a female author by re-synthesizing the text to use male.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 一些更具争议的情况是男性到女性的语气转换，这可能会被用来进行身份欺骗。欺骗者可以创建一个在线账户，假装成一个有吸引力的年轻女性。还有反向的方向（女性到男性的语气转换），可以用于如作者身份模糊化等应用
    Shetty、Schiele 和 Fritz ([2018](#bib.bib184))，通过重新合成文本以使用男性语气来隐藏女性作者的性别。
- en: 7.3.2 Data Privacy Issues for TST
  id: totrans-511
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.2 文本风格转换的数据隐私问题
- en: Another ethical concern is the use of data in the research practice. Researchers
    should not overmine user data, such as the demographic identities. Such data privacy
    widely exists in the data science community as a whole, and there have been many
    ethical discussions Tse et al. ([2015](#bib.bib200)); Russell, Dewey, and Tegmark
    ([2015](#bib.bib170)).
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个伦理问题是研究实践中的数据使用。研究人员不应过度挖掘用户数据，例如人口统计信息。这种数据隐私问题在数据科学社区中普遍存在，并且已经有许多伦理讨论，Tse等人
    ([2015](#bib.bib200))；Russell、Dewey 和 Tegmark ([2015](#bib.bib170))。
- en: Although the TST task needs data containing some attributes along with the text
    content. While it is acceptable to use ratings of reviews that are classified
    as positive or negative, but the user attributes are sensitive, including the
    gender of the user’s account Prabhumoye et al. ([2018](#bib.bib149)), and age
    Lample et al. ([2019](#bib.bib102)). The collection and potential use of such
    sensitive user attributes can have implications that need to be carefully considered.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文本风格转换任务需要包含一些属性的数据以及文本内容。虽然使用被分类为正面或负面的评论评分是可以接受的，但用户属性是敏感的，包括用户账户的性别 Prabhumoye
    等人 ([2018](#bib.bib149))，以及年龄 Lample 等人 ([2019](#bib.bib102))。收集和潜在使用这些敏感用户属性可能会产生需要仔细考虑的影响。
- en: 8 Conclusion
  id: totrans-514
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: This paper presented a comprehensive review of text style transfer with deep
    learning methods. We have surveyed recent research efforts in TST and developed
    schemes to categorize and distill the existing literature. This survey has covered
    the task formulation, evaluation metrics, and methods on parallel and non-parallel
    data. We also discussed several important topics in the research agenda of TST,
    and how to expand the impact of TST to other tasks and applications, including
    ethical considerations. This survey provides a reference for future researchers
    working on TST.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对深度学习方法在文本风格转换中的应用进行了全面综述。我们调查了近期在文本风格转换领域的研究工作，并制定了分类和提炼现有文献的方案。该综述涵盖了任务定义、评价指标以及在平行数据和非平行数据上的方法。我们还讨论了文本风格转换研究议程中的几个重要话题，以及如何将文本风格转换的影响扩展到其他任务和应用，包括伦理考虑。该综述为未来从事文本风格转换研究的学者提供了参考。
- en: Acknowledgement
  id: totrans-516
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Qipeng Guo for his insightful discussions and the anonymous reviewers
    for their constructive suggestions.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢郭启鹏的深刻讨论和匿名评审者的建设性建议。
- en: \starttwocolumn
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: \starttwocolumn
- en: References
  id: totrans-519
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: American Psychological Association (1983) American Psychological Association.
    1983. *Publication manual*. American Psychological Association Washington, DC.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国心理学会 (1983) 美国心理学会. 1983. *出版手册*。美国心理学会，华盛顿特区。
- en: Androutsopoulos and Malakasiotis (2010) Androutsopoulos, Ion and Prodromos Malakasiotis.
    2010. A survey of paraphrasing and textual entailment methods. *Journal of Artificial
    Intelligence Research*, 38:135–187.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Androutsopoulos 和 Malakasiotis (2010) Androutsopoulos, Ion 和 Prodromos Malakasiotis.
    2010. 改写和文本蕴涵方法的调查。*人工智能研究期刊*，38:135–187。
- en: Argamon et al. (2003) Argamon, Shlomo, Moshe Koppel, Jonathan Fine, and Anat Rachel
    Shimoni. 2003. Gender, genre, and writing style in formal written texts. *Text
    & Talk*, 23(3):321–346.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argamon 等 (2003) Argamon, Shlomo, Moshe Koppel, Jonathan Fine, 和 Anat Rachel
    Shimoni. 2003. 正式书面文本中的性别、体裁和写作风格。*Text & Talk*，23(3):321–346。
- en: Artetxe et al. (2018) Artetxe, Mikel, Gorka Labaka, Eneko Agirre, and Kyunghyun
    Cho. 2018. Unsupervised neural machine translation. In *6th International Conference
    on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
    3, 2018, Conference Track Proceedings*, OpenReview.net.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Artetxe 等 (2018) Artetxe, Mikel, Gorka Labaka, Eneko Agirre, 和 Kyunghyun Cho.
    2018. 无监督神经机器翻译。收录于 *第六届国际学习表征会议，ICLR 2018，温哥华，BC，加拿大，2018年4月30日-5月3日，会议论文集*，OpenReview.net。
- en: Azadi et al. (2018) Azadi, Samaneh, Matthew Fisher, Vladimir G. Kim, Zhaowen
    Wang, Eli Shechtman, and Trevor Darrell. 2018. Multi-content GAN for few-shot
    font style transfer. In *2018 IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*, pages 7564–7573, IEEE Computer
    Society.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azadi 等 (2018) Azadi, Samaneh, Matthew Fisher, Vladimir G. Kim, Zhaowen Wang,
    Eli Shechtman, 和 Trevor Darrell. 2018. 用于少样本字体风格迁移的多内容 GAN。收录于 *2018 IEEE 计算机视觉与模式识别会议，CVPR
    2018，盐湖城，犹他州，美国，2018年6月18-22日*，第 7564–7573 页，IEEE 计算机学会。
- en: Bahdanau, Cho, and Bengio (2015) Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua
    Bengio. 2015. Neural machine translation by jointly learning to align and translate.
    In *3rd International Conference on Learning Representations, ICLR 2015, San Diego,
    CA, USA, May 7-9, 2015, Conference Track Proceedings*.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau, Cho 和 Bengio (2015) Bahdanau, Dzmitry, Kyunghyun Cho, 和 Yoshua Bengio.
    2015. 通过联合学习对齐和翻译的神经机器翻译。收录于 *第三届国际学习表征会议，ICLR 2015，圣地亚哥，加州，美国，2015年5月7-9日，会议论文集*。
- en: 'Banerjee and Lavie (2005) Banerjee, Satanjeev and Alon Lavie. 2005. METEOR:
    An automatic metric for MT evaluation with improved correlation with human judgments.
    In *Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures
    for Machine Translation and/or Summarization*, pages 65–72, Association for Computational
    Linguistics, Ann Arbor, Michigan.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Banerjee 和 Lavie (2005) Banerjee, Satanjeev 和 Alon Lavie. 2005. METEOR: 一种改进了与人类判断相关性的自动评估指标。收录于
    *ACL 机器翻译与/或摘要的内在和外在评估措施研讨会论文集*，第 65–72 页，计算语言学协会，安娜堡，密歇根州。'
- en: Bao et al. (2019) Bao, Yu, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova,
    Xin-yu Dai, and Jiajun Chen. 2019. Generating sentences from disentangled syntactic
    and semantic spaces. In *Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics*, pages 6008–6019, Association for Computational
    Linguistics, Florence, Italy.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao 等 (2019) Bao, Yu, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova,
    Xin-yu Dai, 和 Jiajun Chen. 2019. 从解缠的句法和语义空间生成句子。收录于 *第57届计算语言学协会年会论文集*，第 6008–6019
    页，计算语言学协会，佛罗伦萨，意大利。
- en: Bateman and Paris (1989) Bateman, John A and Cecile Paris. 1989. Phrasing a
    text in terms the user can understand. In *IJCAI*, pages 1511–1517.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bateman 和 Paris (1989) Bateman, John A 和 Cecile Paris. 1989. 用用户能理解的术语表达文本。收录于
    *IJCAI*，第 1511–1517 页。
- en: Beauchamp, Childress et al. (2001) Beauchamp, Tom L, James F Childress, et al.
    2001. *Principles of biomedical ethics*. Oxford University Press, USA.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beauchamp, Childress 等 (2001) Beauchamp, Tom L, James F Childress 等. 2001. *生物医学伦理原则*。牛津大学出版社，美国。
- en: Belz (2008) Belz, Anja. 2008. Automatic generation of weather forecast texts
    using comprehensive probabilistic generation-space models. *Natural Language Engineering*,
    14(4):431–455.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belz (2008) Belz, Anja. 2008. 使用综合概率生成空间模型自动生成天气预报文本。*自然语言工程*，14(4):431–455。
- en: 'Belz et al. (2020) Belz, Anya, Shubham Agarwal, Anastasia Shimorina, and Ehud
    Reiter. 2020. ReproGen: Proposal for a shared task on reproducibility of human
    evaluations in NLG. In *Proceedings of the 13th International Conference on Natural
    Language Generation*, pages 232–236, Association for Computational Linguistics,
    Dublin, Ireland.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Belz et al. (2020) Belz, Anya, Shubham Agarwal, Anastasia Shimorina 和 Ehud
    Reiter. 2020. ReproGen: 关于NLG中人类评估可重复性的共享任务提案。在 *第13届国际自然语言生成会议论文集*，第232–236页，计算语言学协会，爱尔兰都柏林。'
- en: den Bercken, Sips, and Lofi (2019) den Bercken, Laurens Van, Robert-Jan Sips,
    and Christoph Lofi. 2019. Evaluating neural text simplification in the medical
    domain. In *The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May
    13-17, 2019*, pages 3286–3292, ACM.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: den Bercken, Sips, and Lofi (2019) den Bercken, Laurens Van, Robert-Jan Sips
    和 Christoph Lofi. 2019. 评估医学领域的神经文本简化。在 *全球网络会议，WWW 2019，旧金山，加州，美国，2019年5月13-17日*，第3286–3292页，ACM。
- en: Bernays (2005) Bernays, Edward L. 2005. *Propaganda*. Ig publishing.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bernays (2005) Bernays, Edward L. 2005. *宣传*. Ig publishing.
- en: Bessi and Ferrara (2016) Bessi, Alessandro and Emilio Ferrara. 2016. Social
    bots distort the 2016 us presidential election online discussion. *First Monday*,
    21(11-7).
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bessi and Ferrara (2016) Bessi, Alessandro 和 Emilio Ferrara. 2016. 社会机器人扭曲了2016年美国总统选举的在线讨论。*First
    Monday*，21(11-7)。
- en: Boulis and Ostendorf (2005) Boulis, Constantinos and Mari Ostendorf. 2005. A
    quantitative analysis of lexical differences between genders in telephone conversations.
    In *ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics,
    Proceedings of the Conference, 25-30 June 2005, University of Michigan, USA*,
    pages 435–442, The Association for Computer Linguistics.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boulis and Ostendorf (2005) Boulis, Constantinos 和 Mari Ostendorf. 2005. 电话对话中性别词汇差异的定量分析。在
    *ACL 2005，第43届计算语言学协会年会，会议论文集，2005年6月25-30日，密歇根大学，美国*，第435–442页，计算语言学协会。
- en: Briakou et al. (2021a) Briakou, Eleftheria, Sweta Agrawal, Ke Zhang, Joel Tetreault,
    and Marine Carpuat. 2021a. A review of human evaluation for style transfer. In
    *Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and
    Metrics (GEM 2021)*, pages 58–67, Association for Computational Linguistics, Online.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Briakou et al. (2021a) Briakou, Eleftheria, Sweta Agrawal, Ke Zhang, Joel Tetreault
    和 Marine Carpuat. 2021a. 风格迁移的人类评估回顾。在 *第1届自然语言生成、评估和度量研讨会（GEM 2021）论文集*，第58–67页，计算语言学协会，在线。
- en: 'Briakou et al. (2021b) Briakou, Eleftheria, Di Lu, Ke Zhang, and Joel Tetreault.
    2021b. Olá, bonjour, salve! XFORMAL: A benchmark for multilingual formality style
    transfer. In *Proceedings of the 2021 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies*,
    pages 3199–3216, Association for Computational Linguistics, Online.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Briakou et al. (2021b) Briakou, Eleftheria, Di Lu, Ke Zhang 和 Joel Tetreault.
    2021b. Olá, bonjour, salve! XFORMAL: 一个多语言正式风格迁移的基准测试。在 *2021年北美计算语言学协会会议：人类语言技术论文集*，第3199–3216页，计算语言学协会，在线。'
- en: 'Brown et al. (2020) Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. Language models are few-shot learners. In *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever 和 Dario
    Amodei. 2020. 语言模型是少样本学习者。在 *神经信息处理系统进展33：2020年神经信息处理系统年会，NeurIPS 2020，2020年12月6-12日，虚拟*。
- en: 'Bulté and Tezcan (2019) Bulté, Bram and Arda Tezcan. 2019. Neural fuzzy repair:
    Integrating fuzzy matches into neural machine translation. In *Proceedings of
    the 57th Conference of the Association for Computational Linguistics, ACL 2019,
    Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers*, pages 1800–1809,
    Association for Computational Linguistics.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bulté and Tezcan (2019) Bulté, Bram 和 Arda Tezcan. 2019. 神经模糊修复：将模糊匹配集成到神经机器翻译中。在
    *第57届计算语言学协会会议，ACL 2019，意大利佛罗伦萨，2019年7月28日-8月2日，第1卷：长篇论文*，第1800–1809页，计算语言学协会。
- en: 'Cai et al. (2019) Cai, Deng, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu,
    Wai Lam, and Shuming Shi. 2019. Skeleton-to-response: Dialogue generation guided
    by retrieval memory. In *Proceedings of the 2019 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers)*, pages 1219–1228, Association for Computational Linguistics.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai et al. (2019) Cai, Deng, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu,
    Wai Lam, 和 Shuming Shi. 2019. Skeleton-to-response: 对话生成由检索记忆指导。在 *2019年北美计算语言学协会会议：人类语言技术，NAACL-HLT
    2019，明尼阿波利斯，美国，2019年6月2-7日，第1卷（长短论文）*，第1219–1228页，计算语言学协会。'
- en: Cao and Xiong (2018) Cao, Qian and Deyi Xiong. 2018. Encoding gated translation
    memory into neural machine translation. In *Proceedings of the 2018 Conference
    on Empirical Methods in Natural Language Processing, Brussels, Belgium, October
    31 - November 4, 2018*, pages 3042–3047, Association for Computational Linguistics.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao and Xiong (2018) Cao, Qian 和 Deyi Xiong. 2018. 将门控翻译记忆编码到神经机器翻译中。在 *2018年自然语言处理经验方法会议，比利时布鲁塞尔，2018年10月31日-11月4日*，第3042–3047页，计算语言学协会。
- en: 'Cao et al. (2020) Cao, Yixin, Ruihao Shui, Liangming Pan, Min-Yen Kan, Zhiyuan
    Liu, and Tat-Seng Chua. 2020. Expertise style transfer: A new task towards better
    communication between experts and laymen. In *Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics*, pages 1061–1071, Association
    for Computational Linguistics, Online.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao et al. (2020) Cao, Yixin, Ruihao Shui, Liangming Pan, Min-Yen Kan, Zhiyuan
    Liu, 和 Tat-Seng Chua. 2020. 专业风格迁移：向专家与非专家之间更好沟通的新任务。在 *第58届计算语言学协会年会*，第1061–1071页，计算语言学协会，在线。
- en: 'Carey (1997) Carey, Alex. 1997. *Taking the risk out of democracy: Corporate
    propaganda versus freedom and liberty*. University of Illinois Press.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carey (1997) Carey, Alex. 1997. *让民主摆脱风险：企业宣传与自由和权利*。伊利诺伊大学出版社。
- en: Carlson, Riddell, and Rockmore (2018) Carlson, Keith, Allen Riddell, and Daniel
    Rockmore. 2018. Evaluating prose style transfer with the bible. *Royal Society
    open science*, 5(10):171920.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlson, Riddell, and Rockmore (2018) Carlson, Keith, Allen Riddell, 和 Daniel
    Rockmore. 2018. 使用圣经评估散文风格迁移。*皇家学会开放科学*，5(10):171920。
- en: Castro, Ortega, and Muñoz (2017) Castro, Daniel, Reynier Ortega, and Rafael
    Muñoz. 2017. Author masking by sentence transformation—notebook for pan at clef
    2017. In *CLEF 2017 Evaluation Labs and Workshop–Working Notes Papers*, pages
    11–14.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Castro, Ortega, and Muñoz (2017) Castro, Daniel, Reynier Ortega, 和 Rafael Muñoz.
    2017. 通过句子转换进行作者遮蔽——PAN 2017的笔记本。在 *CLEF 2017评估实验室和工作坊——工作笔记论文*，第11–14页。
- en: 'Cer et al. (2018) Cer, Daniel, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole
    Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan,
    Chris Tar, Brian Strope, and Ray Kurzweil. 2018. Universal sentence encoder for
    english. In *Proceedings of the 2018 Conference on Empirical Methods in Natural
    Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October
    31 - November 4, 2018*, pages 169–174, Association for Computational Linguistics.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cer et al. (2018) Cer, Daniel, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco,
    Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
    Brian Strope, 和 Ray Kurzweil. 2018. 英语通用句子编码器。在 *2018年自然语言处理经验方法会议，EMNLP 2018：系统演示，比利时布鲁塞尔，2018年10月31日-11月4日*，第169–174页，计算语言学协会。
- en: 'Chakrabarty, Muresan, and Peng (2020) Chakrabarty, Tuhin, Smaranda Muresan,
    and Nanyun Peng. 2020. Generating similes effortlessly like a pro: A style transfer
    approach for simile generation. In *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020*,
    pages 6455–6469, Association for Computational Linguistics.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakrabarty, Muresan, and Peng (2020) Chakrabarty, Tuhin, Smaranda Muresan,
    和 Nanyun Peng. 2020. 像专家一样轻松生成比喻：一种比喻生成的风格迁移方法。在 *2020年自然语言处理经验方法会议，EMNLP 2020，在线，2020年11月16-20日*，第6455–6469页，计算语言学协会。
- en: 'Chen and Dolan (2011) Chen, David L. and William B. Dolan. 2011. Collecting
    highly parallel data for paraphrase evaluation. In *The 49th Annual Meeting of
    the Association for Computational Linguistics: Human Language Technologies, Proceedings
    of the Conference, 19-24 June, 2011, Portland, Oregon, USA*, pages 190–200, The
    Association for Computer Linguistics.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen and Dolan (2011) Chen, David L. and William B. Dolan. 2011. 收集高度并行的数据用于释义评价。在*第49届计算语言学协会年会：人类语言技术，会议论文集，2011年6月19-24日，俄勒冈州，波特兰*，第190–200页，计算语言学协会。
- en: Chen et al. (2017a) Chen, Dongdong, Jing Liao, Lu Yuan, Nenghai Yu, and Gang
    Hua. 2017a. Coherent online video style transfer. In *IEEE International Conference
    on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017*, pages 1114–1123,
    IEEE Computer Society.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2017a) Chen, Dongdong, Jing Liao, Lu Yuan, Nenghai Yu, and Gang
    Hua. 2017a. 连贯的在线视频风格迁移。在*IEEE 国际计算机视觉会议，ICCV 2017，威尼斯，意大利，2017年10月22-29日*，第1114–1123页，IEEE
    计算机学会。
- en: 'Chen et al. (2017b) Chen, Dongdong, Lu Yuan, Jing Liao, Nenghai Yu, and Gang
    Hua. 2017b. StyleBank: An explicit representation for neural image style transfer.
    In *2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017,
    Honolulu, HI, USA, July 21-26, 2017*, pages 2770–2779, IEEE Computer Society.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2017b) Chen, Dongdong, Lu Yuan, Jing Liao, Nenghai Yu, and Gang
    Hua. 2017b. StyleBank: 神经图像风格迁移的显式表示。在*2017 IEEE 计算机视觉与模式识别大会，CVPR 2017，夏威夷，HI，美国，2017年7月21-26日*，第2770–2779页，IEEE
    计算机学会。'
- en: 'Cho et al. (2014) Cho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and
    Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder
    approaches. In *Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics
    and Structure in Statistical Translation, Doha, Qatar, 25 October 2014*, pages
    103–111, Association for Computational Linguistics.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho et al. (2014) Cho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and
    Yoshua Bengio. 2014. 神经机器翻译的属性：编码器-解码器方法。在*SSST@EMNLP 2014，第八届统计翻译中的句法、语义和结构研讨会，多哈，卡塔尔，2014年10月25日*，第103–111页，计算语言学协会。
- en: Cohen (1960) Cohen, Jacob. 1960. A coefficient of agreement for nominal scales.
    *Educational and psychological measurement*, 20(1):37–46.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen (1960) Cohen, Jacob. 1960. 名义尺度的一致性系数。*教育与心理测量*，20(1):37–46。
- en: 'Dai et al. (2019) Dai, Ning, Jianze Liang, Xipeng Qiu, and Xuanjing Huang.
    2019. Style transformer: Unpaired text style transfer without disentangled latent
    representation. In *Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics*, pages 5997–6007, Association for Computational
    Linguistics, Florence, Italy.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai et al. (2019) Dai, Ning, Jianze Liang, Xipeng Qiu, and Xuanjing Huang. 2019.
    风格变换器：无需解缠结潜在表示的无配对文本风格迁移。在*第57届计算语言学协会年会论文集*，第5997–6007页，计算语言学协会，佛罗伦萨，意大利。
- en: 'Dathathri et al. (2020) Dathathri, Sumanth, Andrea Madotto, Janice Lan, Jane
    Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and
    play language models: A simple approach to controlled text generation. In *8th
    International Conference on Learning Representations, ICLR 2020, Addis Ababa,
    Ethiopia, April 26-30, 2020*, OpenReview.net.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dathathri et al. (2020) Dathathri, Sumanth, Andrea Madotto, Janice Lan, Jane
    Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. 插件式语言模型：一种简单的受控文本生成方法。在*第8届国际学习表征会议，ICLR
    2020，亚的斯亚贝巴，埃塞俄比亚，2020年4月26-30日*，OpenReview.net。
- en: 'Devlin et al. (2019) Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers)*, pages 4171–4186, Association for Computational Linguistics.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2019) Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT：用于语言理解的深度双向变换器的预训练。在*2019年北美计算语言学协会年会：人类语言技术会议论文集，NAACL-HLT
    2019，明尼阿波利斯，MN，美国，2019年6月2-7日，第1卷（长篇与短篇论文）*，第4171–4186页，计算语言学协会。
- en: Dou, Anastasopoulos, and Neubig (2020) Dou, Zi-Yi, Antonios Anastasopoulos,
    and Graham Neubig. 2020. Dynamic data selection and weighting for iterative back-translation.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2020, Online, November 16-20, 2020*, pages 5894–5904, Association
    for Computational Linguistics.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dou, Anastasopoulos, 和 Neubig (2020) Dou, Zi-Yi, Antonios Anastasopoulos, 和
    Graham Neubig. 2020. 动态数据选择与加权用于迭代回译。在 *2020年自然语言处理经验方法会议论文集，EMNLP 2020，在线，2020年11月16-20日*，第5894–5904页，计算语言学协会。
- en: Ferreira et al. (2020) Ferreira, Thiago, Claire Gardent, Nikolai Ilinykh, Chris
    van der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. 2020. The
    2020 bilingual, bi-directional WebNLG+ shared task overview and evaluation results
    (WebNLG+ 2020). In *Proceedings of the 3rd International Workshop on Natural Language
    Generation from the Semantic Web (WebNLG+)*.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferreira et al. (2020) Ferreira, Thiago, Claire Gardent, Nikolai Ilinykh, Chris
    van der Lee, Simon Mille, Diego Moussallem, 和 Anastasia Shimorina. 2020. 2020年双语双向
    WebNLG+ 共享任务概述及评估结果 (WebNLG+ 2020)。在 *第三届从语义网生成自然语言的国际研讨会论文集 (WebNLG+)*。
- en: Ficler and Goldberg (2017) Ficler, Jessica and Yoav Goldberg. 2017. Controlling
    linguistic style aspects in neural language generation. *CoRR*, abs/1707.02633.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ficler 和 Goldberg (2017) Ficler, Jessica 和 Yoav Goldberg. 2017. 控制神经语言生成中的语言风格方面。*CoRR*，abs/1707.02633。
- en: 'Fu et al. (2019) Fu, Yao, Hao Zhou, Jiaze Chen, and Lei Li. 2019. Rethinking
    text attribute transfer: A lexical analysis. In *Proceedings of the 12th International
    Conference on Natural Language Generation, INLG 2019, Tokyo, Japan, October 29
    - November 1, 2019*, pages 24–33, Association for Computational Linguistics.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2019) Fu, Yao, Hao Zhou, Jiaze Chen, 和 Lei Li. 2019. 重新思考文本属性迁移：词汇分析。在
    *第十二届国际自然语言生成会议论文集，INLG 2019，日本东京，2019年10月29日-11月1日*，第24–33页，计算语言学协会。
- en: 'Fu et al. (2018) Fu, Zhenxin, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui
    Yan. 2018. Style transfer in text: Exploration and evaluation. In *Proceedings
    of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the
    30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th
    AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New
    Orleans, Louisiana, USA, February 2-7, 2018*, pages 663–670, AAAI Press.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2018) Fu, Zhenxin, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, 和 Rui Yan.
    2018. 文本中的风格迁移：探索与评估。在 *第三十二届AAAI人工智能会议论文集，(AAAI-18)，第30届人工智能创新应用会议 (IAAI-18)，和第八届AAAI教育进展研讨会
    (EAAI-18)，美国路易斯安那州新奥尔良，2018年2月2-7日*，第663–670页，AAAI出版社。
- en: 'Gan et al. (2017) Gan, Chuang, Zhe Gan, Xiaodong He, Jianfeng Gao, and Li Deng.
    2017. StyleNet: Generating attractive visual captions with styles. In *2017 IEEE
    Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI,
    USA, July 21-26, 2017*, pages 955–964, IEEE Computer Society.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gan et al. (2017) Gan, Chuang, Zhe Gan, Xiaodong He, Jianfeng Gao, 和 Li Deng.
    2017. StyleNet: 生成具有风格的吸引人视觉字幕。在 *2017年IEEE计算机视觉与模式识别大会，CVPR 2017，夏威夷，HI，美国，2017年7月21-26日*，第955–964页，IEEE计算机学会。'
- en: Gao, Singh, and Raj (2018) Gao, Yang, Rita Singh, and Bhiksha Raj. 2018. Voice
    impersonation using generative adversarial networks. In *2018 IEEE International
    Conference on Acoustics, Speech and Signal Processing, ICASSP 2018, Calgary, AB,
    Canada, April 15-20, 2018*, pages 2506–2510, IEEE.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao, Singh, 和 Raj (2018) Gao, Yang, Rita Singh, 和 Bhiksha Raj. 2018. 使用生成对抗网络的语音模仿。在
    *2018年IEEE国际声学、语音和信号处理会议，ICASSP 2018，加拿大卡尔加里，2018年4月15-20日*，第2506–2510页，IEEE。
- en: 'Gardent et al. (2017) Gardent, Claire, Anastasia Shimorina, Shashi Narayan,
    and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from
    RDF data. In *Proceedings of the 10th International Conference on Natural Language
    Generation, INLG 2017, Santiago de Compostela, Spain, September 4-7, 2017*, pages
    124–133, Association for Computational Linguistics.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gardent et al. (2017) Gardent, Claire, Anastasia Shimorina, Shashi Narayan,
    和 Laura Perez-Beltrachini. 2017. The WebNLG challenge: 从 RDF 数据生成文本。在 *第十届国际自然语言生成会议论文集，INLG
    2017，西班牙圣地亚哥，2017年9月4-7日*，第124–133页，计算语言学协会。'
- en: 'Gardner et al. (2020) Gardner, Matt, Yoav Artzi, Victoria Basmova, Jonathan
    Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth
    Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi,
    Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh,
    Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and
    Ben Zhou. 2020. Evaluating models’ local decision boundaries via contrast sets.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020*, pages 1307–1323,
    Association for Computational Linguistics.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gardner 等 (2020) Gardner, Matt、Yoav Artzi、Victoria Basmova、Jonathan Berant、Ben
    Bogin、Sihao Chen、Pradeep Dasigi、Dheeru Dua、Yanai Elazar、Ananth Gottumukkala、Nitish
    Gupta、Hannaneh Hajishirzi、Gabriel Ilharco、Daniel Khashabi、Kevin Lin、Jiangming
    Liu、Nelson F. Liu、Phoebe Mulcaire、Qiang Ning、Sameer Singh、Noah A. Smith、Sanjay
    Subramanian、Reut Tsarfaty、Eric Wallace、Ally Zhang 和 Ben Zhou. 2020. 通过对比集评估模型的局部决策边界。见于*2020年自然语言处理实证方法会议：发现，EMNLP
    2020，在线活动，2020年11月16-20日*，第1307-1323页，计算语言学协会。
- en: 'Gatt and Reiter (2009) Gatt, Albert and Ehud Reiter. 2009. SimpleNLG: A realisation
    engine for practical applications. In *ENLG 2009 - Proceedings of the 12th European
    Workshop on Natural Language Generation, March 30-31, 2009, Athens, Greece*, pages
    90–93, The Association for Computer Linguistics.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gatt 和 Reiter (2009) Gatt, Albert 和 Ehud Reiter. 2009. SimpleNLG: 实际应用的实现引擎。见于*ENLG
    2009 - 第12届欧洲自然语言生成研讨会论文集，2009年3月30-31日，雅典，希腊*，第90-93页，计算语言学协会。'
- en: Gatys, Ecker, and Bethge (2016) Gatys, Leon A., Alexander S. Ecker, and Matthias
    Bethge. 2016. Image style transfer using convolutional neural networks. In *2016
    IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas,
    NV, USA, June 27-30, 2016*, pages 2414–2423, IEEE Computer Society.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gatys, Ecker, 和 Bethge (2016) Gatys, Leon A.、Alexander S. Ecker 和 Matthias Bethge.
    2016. 使用卷积神经网络的图像风格迁移。见于*2016 IEEE 计算机视觉与模式识别大会，CVPR 2016，拉斯维加斯，美国，2016年6月27-30日*，第2414-2423页，IEEE计算机学会。
- en: 'Gehrmann et al. (2021) Gehrmann, Sebastian, Tosin P. Adewumi, Karmanya Aggarwal,
    Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi
    Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh D. Dhole, Wanyu Du, Esin
    Durmus, Ondrej Dusek, Chris Emezue, Varun Gangal, Cristina Garbacea, Tatsunori
    Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly,
    Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad
    Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major,
    Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev,
    Rubungo Andre Niyongabo, Salomey Osei, Ankur P. Parikh, Laura Perez-Beltrachini,
    Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João
    Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla
    Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola,
    and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation
    and metrics. *CoRR*, abs/2102.01672.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehrmann 等 (2021) Gehrmann, Sebastian、Tosin P. Adewumi、Karmanya Aggarwal、Pawan
    Sasanka Ammanamanchi、Aremu Anuoluwapo、Antoine Bosselut、Khyathi Raghavi Chandu、Miruna-Adriana
    Clinciu、Dipanjan Das、Kaustubh D. Dhole、Wanyu Du、Esin Durmus、Ondrej Dusek、Chris
    Emezue、Varun Gangal、Cristina Garbacea、Tatsunori Hashimoto、Yufang Hou、Yacine Jernite、Harsh
    Jhamtani、Yangfeng Ji、Shailza Jolly、Dhruv Kumar、Faisal Ladhak、Aman Madaan、Mounica
    Maddela、Khyati Mahajan、Saad Mahamood、Bodhisattwa Prasad Majumder、Pedro Henrique
    Martins、Angelina McMillan-Major、Simon Mille、Emiel van Miltenburg、Moin Nadeem、Shashi
    Narayan、Vitaly Nikolaev、Rubungo Andre Niyongabo、Salomey Osei、Ankur P. Parikh、Laura
    Perez-Beltrachini、Niranjan Ramesh Rao、Vikas Raunak、Juan Diego Rodriguez、Sashank
    Santhanam、João Sedoc、Thibault Sellam、Samira Shaikh、Anastasia Shimorina、Marco Antonio
    Sobrevilla Cabezudo、Hendrik Strobelt、Nishant Subramani、Wei Xu、Diyi Yang、Akhila
    Yerukola 和 Jiawei Zhou. 2021. GEM 基准测试：自然语言生成、其评估与指标。*CoRR*，abs/2102.01672。
- en: Gkatzia, Lemon, and Rieser (2017) Gkatzia, Dimitra, Oliver Lemon, and Verena
    Rieser. 2017. Data-to-text generation improves decision-making under uncertainty.
    *IEEE Computational Intelligence Magazine*, 12(3):10–17.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gkatzia, Lemon 和 Rieser (2017) Gkatzia, Dimitra、Oliver Lemon 和 Verena Rieser.
    2017. 数据到文本生成改善不确定性下的决策。*IEEE 计算智能杂志*，12(3):10-17。
- en: 'Gong et al. (2019) Gong, Hongyu, Suma Bhat, Lingfei Wu, JinJun Xiong, and Wen-mei
    Hwu. 2019. Reinforcement learning based text style transfer without parallel training
    corpus. In *Proceedings of the 2019 Conference of the North American Chapter of
    the Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long and Short Papers)*, pages 3168–3180, Association for Computational Linguistics,
    Minneapolis, Minnesota.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong 等（2019）Gong, Hongyu, Suma Bhat, Lingfei Wu, JinJun Xiong, 和 Wen-mei Hwu.
    2019. 基于强化学习的文本风格转换，无需平行训练语料库。见 *2019年北美计算语言学协会年会：人类语言技术会议论文集，第1卷（长篇和短篇论文）*，第3168–3180页，计算语言学协会，明尼阿波利斯，明尼苏达州。
- en: Goodfellow et al. (2014) Goodfellow, Ian J., Jean Pouget-Abadie, M. Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio.
    2014. Generative adversarial nets. In *NIPS*.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等（2014）Goodfellow, Ian J., Jean Pouget-Abadie, M. Mirza, Bing Xu,
    David Warde-Farley, Sherjil Ozair, Aaron C. Courville, 和 Yoshua Bengio. 2014.
    生成对抗网络。见 *NIPS*。
- en: Goodman (1947) Goodman, Nelson. 1947. The problem of counterfactual conditionals.
    *The Journal of Philosophy*, 44(5):113–128.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodman（1947）Goodman, Nelson. 1947. 反事实条件句的问题。*哲学杂志*，44(5)：113–128。
- en: Grégoire and Langlais (2018) Grégoire, Francis and Philippe Langlais. 2018.
    Extracting parallel sentences with bidirectional recurrent neural networks to
    improve machine translation. In *Proceedings of the 27th International Conference
    on Computational Linguistics, COLING 2018, Santa Fe, New Mexico, USA, August 20-26,
    2018*, pages 1442–1453, Association for Computational Linguistics.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grégoire 和 Langlais（2018）Grégoire, Francis 和 Philippe Langlais. 2018. 使用双向递归神经网络提取平行句子以改进机器翻译。见
    *第27届国际计算语言学会议会议论文集，COLING 2018，美国新墨西哥州圣菲，2018年8月20-26日*，第1442–1453页，计算语言学协会。
- en: Gröndahl and Asokan (2020) Gröndahl, Tommi and N. Asokan. 2020. Effective writing
    style transfer via combinatorial paraphrasing. *Proceedings on Privacy Enhancing
    Technologies*, 2020(4):175–195.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gröndahl 和 Asokan（2020）Gröndahl, Tommi 和 N. Asokan. 2020. 通过组合性释义进行有效的写作风格转换。*隐私增强技术会议论文集*，2020年第4期：175–195。
- en: 'Gu et al. (2016) Gu, Jiatao, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016.
    Incorporating copying mechanism in sequence-to-sequence learning. In *Proceedings
    of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 1631–1640, Association for Computational Linguistics,
    Berlin, Germany.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2016）Gu, Jiatao, Zhengdong Lu, Hang Li, 和 Victor O.K. Li. 2016. 在序列到序列学习中结合复制机制。见
    *第54届计算语言学协会年会会议论文集（第1卷：长篇论文）*，第1631–1640页，计算语言学协会，德国柏林。
- en: Gu et al. (2018) Gu, Jiatao, Yong Wang, Kyunghyun Cho, and Victor O. K. Li.
    2018. Search engine guided neural machine translation. In *Proceedings of the
    Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th
    innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI
    Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans,
    Louisiana, USA, February 2-7, 2018*, pages 5133–5140, AAAI Press.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2018）Gu, Jiatao, Yong Wang, Kyunghyun Cho, 和 Victor O. K. Li. 2018. 搜索引擎引导的神经机器翻译。见
    *第32届AAAI人工智能会议（AAAI-18）、第30届人工智能创新应用（IAAI-18）和第8届AAAI人工智能教育进展研讨会（EAAI-18），美国路易斯安那州新奥尔良，2018年2月2-7日*，第5133–5140页，AAAI出版社。
- en: 'Gülçehre et al. (2016) Gülçehre, Çaglar, Sungjin Ahn, Ramesh Nallapati, Bowen
    Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. In *Proceedings of
    the 54th Annual Meeting of the Association for Computational Linguistics, ACL
    2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers*, The Association
    for Computer Linguistics.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gülçehre 等（2016）Gülçehre, Çaglar, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou,
    和 Yoshua Bengio. 2016. 指向未知词汇。见 *第54届计算语言学协会年会会议论文集，ACL 2016，2016年8月7-12日，德国柏林，第1卷：长篇论文*，计算语言学协会。
- en: 'Guo et al. (2020) Guo, Qipeng, Zhijing Jin, Ning Dai, Xipeng Qiu, Xiangyang
    Xue, David Wipf, and Zheng Zhang. 2020. P2: A plan-and-pretrain approach for knowledge
    graph-to-text generation. In *Proceedings of the 3rd WebNLG Workshop on Natural
    Language Generation from the Semantic Web (WebNLG+ 2020)*, Association for Computational
    Linguistics, Dublin, Ireland (Virtual).'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2020）Guo, Qipeng, Zhijing Jin, Ning Dai, Xipeng Qiu, Xiangyang Xue, David
    Wipf, 和 Zheng Zhang. 2020. P2：一种计划与预训练相结合的知识图谱到文本生成方法。见 *第3届WebNLG自然语言生成研讨会（WebNLG+
    2020）*，计算语言学协会，爱尔兰都柏林（虚拟）。
- en: 'Guo et al. (2021) Guo, Qipeng, Zhijing Jin, Ziyu Wang, Xipeng Qiu, Weinan Zhang,
    Jun Zhu, Zheng Zhang, and David Wipf. 2021. Fork or fail: Cycle-consistent training
    with many-to-one mappings. In *The 24th International Conference on Artificial
    Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event*,
    volume 130 of *Proceedings of Machine Learning Research*, pages 1828–1836, PMLR.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人（2021）Guo, Qipeng, Zhijing Jin, Ziyu Wang, Xipeng Qiu, Weinan Zhang, Jun
    Zhu, Zheng Zhang 和 David Wipf. 2021. 分叉还是失败：具有多对一映射的循环一致性训练。在*第24届国际人工智能与统计学会议,
    AISTATS 2021, 2021年4月13-15日, 虚拟会议*，*机器学习研究论文集*第130卷，第1828–1836页，PMLR。
- en: Guu et al. (2018) Guu, Kelvin, Tatsunori B. Hashimoto, Yonatan Oren, and Percy
    Liang. 2018. Generating sentences by editing prototypes. *Transactions of the
    Association for Computational Linguistics*, 6:437–450.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu 等人（2018）Guu, Kelvin, Tatsunori B. Hashimoto, Yonatan Oren 和 Percy Liang.
    2018. 通过编辑原型生成句子。*计算语言学协会会刊*，6:437–450。
- en: Harrison (2019) Harrison, Sara. 2019. Twitter and instagram unveil new ways
    to combat hate–again.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harrison（2019）Harrison, Sara. 2019. 推特和 Instagram 再次推出新的对抗仇恨的方法。
- en: 'Hashimoto et al. (2018) Hashimoto, Tatsunori B., Kelvin Guu, Yonatan Oren,
    and Percy Liang. 2018. A retrieve-and-edit framework for predicting structured
    outputs. In *Advances in Neural Information Processing Systems 31: Annual Conference
    on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018,
    Montréal, Canada*, pages 10073–10083.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hashimoto 等人（2018）Hashimoto, Tatsunori B., Kelvin Guu, Yonatan Oren 和 Percy
    Liang. 2018. 用于预测结构化输出的检索-编辑框架。在*神经信息处理系统进展 31: 神经信息处理系统2018年年会, NeurIPS 2018,
    2018年12月3-8日, 加拿大蒙特利尔*，第10073–10083页。'
- en: 'He and McAuley (2016) He, Ruining and Julian J. McAuley. 2016. Ups and downs:
    Modeling the visual evolution of fashion trends with one-class collaborative filtering.
    In *Proceedings of the 25th International Conference on World Wide Web, WWW 2016,
    Montreal, Canada, April 11 - 15, 2016*, pages 507–517, ACM.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 和 McAuley（2016）He, Ruining 和 Julian J. McAuley. 2016. 起伏：使用单类协同过滤建模时尚趋势的视觉演变。在*第25届国际万维网会议,
    WWW 2016, 加拿大蒙特利尔, 2016年4月11-15日*，第507–517页，ACM。
- en: Henderson (2020) Henderson, James. 2020. The unstoppable rise of computational
    linguistics in deep learning. In *Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020*,
    pages 6294–6306, Association for Computational Linguistics.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson（2020）Henderson, James. 2020. 计算语言学在深度学习中的不可阻挡的崛起。在*第58届计算语言学协会年会,
    ACL 2020, 在线, 2020年7月5-10日*，第6294–6306页，计算语言学协会。
- en: 'Hill, Cho, and Korhonen (2016) Hill, Felix, Kyunghyun Cho, and Anna Korhonen.
    2016. Learning distributed representations of sentences from unlabelled data.
    In *NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, San Diego California,
    USA, June 12-17, 2016*, pages 1367–1377, The Association for Computational Linguistics.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hill, Cho 和 Korhonen（2016）Hill, Felix, Kyunghyun Cho 和 Anna Korhonen. 2016.
    从未标记数据中学习句子的分布式表示。在*NAACL HLT 2016, 北美计算语言学协会2016年年会：人类语言技术, 美国加利福尼亚州圣地亚哥, 2016年6月12-17日*，第1367–1377页，计算语言学协会。
- en: Hoang et al. (2018) Hoang, Vu Cong Duy, Philipp Koehn, Gholamreza Haffari, and
    Trevor Cohn. 2018. Iterative back-translation for neural machine translation.
    In *Proceedings of the 2nd Workshop on Neural Machine Translation and Generation*,
    pages 18–24, Association for Computational Linguistics, Melbourne, Australia.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoang 等人（2018）Hoang, Vu Cong Duy, Philipp Koehn, Gholamreza Haffari 和 Trevor
    Cohn. 2018. 神经机器翻译的迭代回译。在*第2届神经机器翻译与生成工作坊论文集*，第18–24页，计算语言学协会，澳大利亚墨尔本。
- en: Hossain, Ghazvininejad, and Zettlemoyer (2020) Hossain, Nabil, Marjan Ghazvininejad,
    and Luke Zettlemoyer. 2020. Simple and effective retrieve-edit-rerank text generation.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, ACL 2020, Online, July 5-10, 2020*, pages 2532–2538, Association
    for Computational Linguistics.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hossain, Ghazvininejad 和 Zettlemoyer（2020）Hossain, Nabil, Marjan Ghazvininejad
    和 Luke Zettlemoyer. 2020. 简单有效的检索-编辑-重新排序文本生成。在*第58届计算语言学协会年会, ACL 2020, 在线, 2020年7月5-10日*，第2532–2538页，计算语言学协会。
- en: 'Hovy and Spruit (2016) Hovy, Dirk and Shannon L. Spruit. 2016. The social impact
    of natural language processing. In *Proceedings of the 54th Annual Meeting of
    the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin,
    Germany, Volume 2: Short Papers*, The Association for Computer Linguistics.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hovy 和 Spruit（2016）Hovy, Dirk 和 Shannon L. Spruit. 2016. 自然语言处理的社会影响。见于 *第54届计算语言学协会年会会议论文集，ACL
    2016，2016年8月7-12日，德国柏林，第2卷：短文集*，计算机语言学协会。
- en: Hovy (1987) Hovy, Eduard. 1987. Generating natural language under pragmatic
    constraints. *Journal of Pragmatics*, 11(6):689–719.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hovy（1987）Hovy, Eduard. 1987. 在语用约束下生成自然语言。 *语用学期刊*，11(6)：689–719。
- en: Hovy (1990) Hovy, Eduard H. 1990. Pragmatics and natural language generation.
    *Artificial Intelligence*, 43(2):153–197.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hovy（1990）Hovy, Eduard H. 1990. 语用学与自然语言生成。 *人工智能*，43(2)：153–197。
- en: 'Hu, Lee, and Aggarwal (2020) Hu, Zhiqiang, R. K. Lee, and C. Aggarwal. 2020.
    Text style transfer: A review and experiment evaluation. *ArXiv*, abs/2010.12742.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu, Lee 和 Aggarwal（2020）Hu, Zhiqiang, R. K. Lee, 和 C. Aggarwal. 2020. 文本风格迁移：综述与实验评估。
    *ArXiv*，abs/2010.12742。
- en: Hu et al. (2017) Hu, Zhiting, Zichao Yang, Xiaodan Liang, R. Salakhutdinov,
    and E. Xing. 2017. Toward controlled generation of text. In *ICML*.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2017）Hu, Zhiting, Zichao Yang, Xiaodan Liang, R. Salakhutdinov, 和 E. Xing.
    2017. 朝向受控文本生成。见于 *ICML*。
- en: 'Huang et al. (2018) Huang, Chenyang, Osmar Zaïane, Amine Trabelsi, and Nouha
    Dziri. 2018. Automatic dialogue generation with expressed emotions. In *Proceedings
    of the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 2 (Short Papers)*, pages 49–54,
    Association for Computational Linguistics, New Orleans, Louisiana.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2018）Huang, Chenyang, Osmar Zaïane, Amine Trabelsi, 和 Nouha Dziri. 2018.
    自动对话生成与表达情感。见于 *2018年北美计算语言学协会会议论文集：人类语言技术，第2卷（短文集）*，第49–54页，计算语言学协会，新奥尔良，路易斯安那州。
- en: Huang et al. (2020) Huang, Yufang, Wentao Zhu, Deyi Xiong, Yiye Zhang, Changjian
    Hu, and Feiyu Xu. 2020. Cycle-consistent adversarial autoencoders for unsupervised
    text style transfer. In *Proceedings of the 28th International Conference on Computational
    Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020*, pages
    2213–2223, International Committee on Computational Linguistics.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2020）Huang, Yufang, Wentao Zhu, Deyi Xiong, Yiye Zhang, Changjian Hu,
    和 Feiyu Xu. 2020. 循环一致对抗自编码器用于无监督文本风格迁移。见于 *第28届国际计算语言学大会会议论文集，COLING 2020，西班牙巴塞罗那（在线），2020年12月8-13日*，第2213–2223页，国际计算语言学委员会。
- en: 'Jackson et al. (2019) Jackson, Philip T. G., Amir Atapour Abarghouei, Stephen
    Bonner, Toby P. Breckon, and Boguslaw Obara. 2019. Style augmentation: Data augmentation
    via style randomization. In *IEEE Conference on Computer Vision and Pattern Recognition
    Workshops, CVPR Workshops 2019, Long Beach, CA, USA, June 16-20, 2019*, pages
    83–92, Computer Vision Foundation / IEEE.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jackson 等（2019）Jackson, Philip T. G., Amir Atapour Abarghouei, Stephen Bonner,
    Toby P. Breckon, 和 Boguslaw Obara. 2019. 风格增强：通过风格随机化的数据增强。见于 *IEEE计算机视觉与模式识别大会研讨会，CVPR研讨会
    2019，美国加利福尼亚州长滩，2019年6月16-20日*，第83–92页，计算机视觉基金会/IEEE。
- en: 'Jafaritazehjani et al. (2020) Jafaritazehjani, Somayeh, Gwénolé Lecorvé, Damien
    Lolive, and John Kelleher. 2020. Style versus content: A distinction without a
    (learnable) difference? In *Proceedings of the 28th International Conference on
    Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13,
    2020*, pages 2169–2180, International Committee on Computational Linguistics.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jafaritazehjani 等（2020）Jafaritazehjani, Somayeh, Gwénolé Lecorvé, Damien Lolive,
    和 John Kelleher. 2020. 风格与内容：一种无（可学习）差异的区分？见于 *第28届国际计算语言学大会会议论文集，COLING 2020，西班牙巴塞罗那（在线），2020年12月8-13日*，第2169–2180页，国际计算语言学委员会。
- en: Jang, Gu, and Poole (2017) Jang, Eric, Shixiang Gu, and Ben Poole. 2017. Categorical
    reparameterization with gumbel-softmax. In *5th International Conference on Learning
    Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
    Proceedings*, OpenReview.net.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang, Gu 和 Poole（2017）Jang, Eric, Shixiang Gu, 和 Ben Poole. 2017. 使用 Gumbel-Softmax
    的分类重新参数化。见于 *第五届国际学习表征会议，ICLR 2017，法国土伦，2017年4月24-26日，会议论文集*，OpenReview.net。
- en: Jhamtani et al. (2017) Jhamtani, Harsh, Varun Gangal, Eduard H. Hovy, and Eric
    Nyberg. 2017. Shakespearizing modern language using copy-enriched sequence-to-sequence
    models. *CoRR*, abs/1707.01161.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jhamtani 等（2017）Jhamtani, Harsh, Varun Gangal, Eduard H. Hovy, 和 Eric Nyberg.
    2017. 使用复制增强的序列到序列模型进行现代语言的莎士比亚化。 *CoRR*，abs/1707.01161。
- en: Jia and Liang (2017) Jia, Robin and Percy Liang. 2017. Adversarial examples
    for evaluating reading comprehension systems. In *Proceedings of the 2017 Conference
    on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark,
    September 9-11, 2017*, pages 2021–2031, Association for Computational Linguistics.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 和 Liang (2017) Jia, Robin 和 Percy Liang. 2017. 用于评估阅读理解系统的对抗性示例。在 *2017年自然语言处理实证方法会议论文集，EMNLP
    2017，丹麦哥本哈根，2017年9月9-11日*，第2021-2031页，计算语言学协会。
- en: 'Jin et al. (2020a) Jin, Di, Zhijing Jin, Joey Tianyi Zhou, Lisa Orii, and Peter
    Szolovits. 2020a. Hooks in the headline: Learning to generate headlines with controlled
    styles. In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 5082–5093, Association for Computational Linguistics, Online.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等 (2020a) Jin, Di, Zhijing Jin, Joey Tianyi Zhou, Lisa Orii 和 Peter Szolovits.
    2020a. 头条中的钩子：学习生成具有控制风格的头条。在 *第58届计算语言学协会年会论文集*，第5082-5093页，计算语言学协会，在线。
- en: Jin et al. (2020b) Jin, Di, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits.
    2020b. Is BERT really robust? A strong baseline for natural language attack on
    text classification and entailment. In *The Thirty-Fourth AAAI Conference on Artificial
    Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial
    Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances
    in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020*,
    pages 8018–8025, AAAI Press.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等 (2020b) Jin, Di, Zhijing Jin, Joey Tianyi Zhou 和 Peter Szolovits. 2020b.
    BERT真的强健吗？文本分类和蕴含的自然语言攻击的强基线。在 *第34届AAAI人工智能会议，AAAI 2020，第32届人工智能创新应用会议，IAAI 2020，第10届AAAI教育进展研讨会，EAAI
    2020，美国纽约，2020年2月7-12日*，第8018-8025页，AAAI出版社。
- en: 'Jin et al. (2019) Jin, Zhijing, Di Jin, Jonas Mueller, Nicholas Matthews, and
    Enrico Santus. 2019. IMaT: Unsupervised text attribute transfer via iterative
    matching and translation. In *Proceedings of the 2019 Conference on Empirical
    Methods in Natural Language Processing and the 9th International Joint Conference
    on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November
    3-7, 2019*, pages 3095–3107, Association for Computational Linguistics.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等 (2019) Jin, Zhijing, Di Jin, Jonas Mueller, Nicholas Matthews 和 Enrico
    Santus. 2019. IMaT：通过迭代匹配和翻译进行无监督文本属性转换。在 *2019年自然语言处理实证方法会议暨第9届国际联合自然语言处理会议，EMNLP-IJCNLP
    2019，中国香港，2019年11月3-7日*，第3095-3107页，计算语言学协会。
- en: John et al. (2019) John, Vineet, Lili Mou, Hareesh Bahuleyan, and Olga Vechtomova.
    2019. Disentangled representation learning for non-parallel text style transfer.
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 424–434, Association for Computational Linguistics, Florence,
    Italy.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: John 等 (2019) John, Vineet, Lili Mou, Hareesh Bahuleyan 和 Olga Vechtomova. 2019.
    用于非平行文本风格转换的解缠结表示学习。在 *第57届计算语言学协会年会论文集*，第424-434页，计算语言学协会，意大利佛罗伦萨。
- en: Kajiwara (2019) Kajiwara, Tomoyuki. 2019. Negative lexically constrained decoding
    for paraphrase generation. In *Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics*, pages 6047–6052, Association for Computational
    Linguistics, Florence, Italy.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kajiwara (2019) Kajiwara, Tomoyuki. 2019. 用于同义句生成的负向词汇约束解码。在 *第57届计算语言学协会年会论文集*，第6047-6052页，计算语言学协会，意大利佛罗伦萨。
- en: Kale and Rastogi (2020) Kale, Mihir and Abhinav Rastogi. 2020. Text-to-text
    pre-training for data-to-text tasks. In *Proceedings of the 13th International
    Conference on Natural Language Generation, INLG 2020, Dublin, Ireland, December
    15-18, 2020*, pages 97–102, Association for Computational Linguistics.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kale 和 Rastogi (2020) Kale, Mihir 和 Abhinav Rastogi. 2020. 文本到文本的预训练用于数据到文本的任务。在
    *第13届国际自然语言生成会议论文集，INLG 2020，爱尔兰都柏林，2020年12月15-18日*，第97-102页，计算语言学协会。
- en: 'Kang, Wang, and de Melo (2020) Kang, Yipeng, Tonghan Wang, and Gerard de Melo.
    2020. Incorporating pragmatic reasoning communication into emergent language.
    In *Advances in Neural Information Processing Systems 33: Annual Conference on
    Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual*.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang, Wang 和 de Melo (2020) Kang, Yipeng, Tonghan Wang 和 Gerard de Melo. 2020.
    将实用推理通信融入新兴语言。在 *神经信息处理系统进展33：2020年神经信息处理系统年度会议，NeurIPS 2020，2020年12月6-12日，虚拟会议*。
- en: 'Karadzhov et al. (2017) Karadzhov, Georgi, Tsvetomila Mihaylova, Yasen Kiprov,
    Georgi Georgiev, Ivan Koychev, and Preslav Nakov. 2017. The case for being average:
    A mediocrity approach to style masking and author obfuscation - (best of the labs
    track at CLEF-2017). In *Experimental IR Meets Multilinguality, Multimodality,
    and Interaction - 8th International Conference of the CLEF Association, CLEF 2017,
    Dublin, Ireland, September 11-14, 2017, Proceedings*, volume 10456 of *Lecture
    Notes in Computer Science*, pages 173–185, Springer.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karadzhov et al. (2017) Karadzhov, Georgi, Tsvetomila Mihaylova, Yasen Kiprov,
    Georgi Georgiev, Ivan Koychev, 和 Preslav Nakov. 2017. 平庸的例子：一种平庸方法用于风格掩盖和作者混淆
    - （CLEF-2017最佳实验室跟踪）。在*实验信息检索与多语言性、多模态性和互动 - 第八届CLEF协会国际会议（CLEF 2017），2017年9月11-14日，爱尔兰都柏林，会议录*，第10456卷*计算机科学讲义笔记*，第173–185页，Springer。
- en: 'Keskar et al. (2019) Keskar, Nitish Shirish, Bryan McCann, Lav R. Varshney,
    Caiming Xiong, and Richard Socher. 2019. CTRL: A conditional transformer language
    model for controllable generation. *CoRR*, abs/1909.05858.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keskar et al. (2019) Keskar, Nitish Shirish, Bryan McCann, Lav R. Varshney,
    Caiming Xiong, 和 Richard Socher. 2019. CTRL：一种用于可控生成的条件变换器语言模型。*CoRR*，abs/1909.05858。
- en: Khosmood and Levinson (2010) Khosmood, Foaad and Robert Levinson. 2010. Automatic
    synonym and phrase replacement show promise for style transformation. In *The
    Ninth International Conference on Machine Learning and Applications, ICMLA 2010,
    Washington, DC, USA, 12-14 December 2010*, pages 958–961, IEEE Computer Society.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khosmood and Levinson (2010) Khosmood, Foaad和Robert Levinson. 2010. 自动同义词和短语替换在风格转换中表现出潜力。在*第九届国际机器学习与应用大会（ICMLA
    2010），2010年12月12-14日，美国华盛顿特区*，第958–961页，IEEE计算机学会。
- en: Khosmood and Levinson (2008) Khosmood, Foaad and Robert A Levinson. 2008. Automatic
    natural language style classification and transformation. In *BCS-IRSG Workshop
    on Corpus Profiling*, pages 1–11.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khosmood and Levinson (2008) Khosmood, Foaad 和 Robert A Levinson. 2008. 自动自然语言风格分类与转换。在*BCS-IRSG语料库分析研讨会*，第1–11页。
- en: Kim (2014) Kim, Yoon. 2014. Convolutional neural networks for sentence classification.
    In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT,
    a Special Interest Group of the ACL*, pages 1746–1751, ACL.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim (2014) Kim, Yoon. 2014. 卷积神经网络用于句子分类。在*2014年自然语言处理实证方法会议（EMNLP 2014），2014年10月25-29日，卡塔尔多哈，SIGDAT会议，ACL的特别兴趣小组*，第1746–1751页，ACL。
- en: Kingma and Welling (2014) Kingma, Diederik P. and M. Welling. 2014. Auto-encoding
    variational bayes. *CoRR*, abs/1312.6114.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma and Welling (2014) Kingma, Diederik P. 和 M. Welling. 2014. 自编码变分贝叶斯。*CoRR*，abs/1312.6114。
- en: Koncel-Kedziorski et al. (2016) Koncel-Kedziorski, Rik, Ioannis Konstas, Luke
    Zettlemoyer, and Hannaneh Hajishirzi. 2016. A theme-rewriting approach for generating
    algebra word problems. In *Proceedings of the 2016 Conference on Empirical Methods
    in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4,
    2016*, pages 1617–1628, The Association for Computational Linguistics.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koncel-Kedziorski et al. (2016) Koncel-Kedziorski, Rik, Ioannis Konstas, Luke
    Zettlemoyer, 和 Hannaneh Hajishirzi. 2016. 一种主题重写方法用于生成代数词题。在*2016年自然语言处理实证方法会议（EMNLP
    2016），2016年11月1-4日，美国德克萨斯州奥斯汀*，第1617–1628页，计算语言学协会。
- en: 'Kowalski et al. (2014) Kowalski, Robin M, Gary W Giumetti, Amber N Schroeder,
    and Micah R Lattanner. 2014. Bullying in the digital age: A critical review and
    meta-analysis of cyberbullying research among youth. *Psychological bulletin*,
    140(4):1073.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kowalski et al. (2014) Kowalski, Robin M, Gary W Giumetti, Amber N Schroeder,
    和Micah R Lattanner. 2014. 数字时代的欺凌：青少年网络欺凌研究的批判性回顾与元分析。*心理学公报*，140(4):1073。
- en: 'Krippendorff (2018) Krippendorff, Klaus. 2018. *Content analysis: An introduction
    to its methodology*. Sage publications.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krippendorff (2018) Krippendorff, Klaus. 2018. *内容分析：方法论导论*。Sage出版。
- en: Krishna, Wieting, and Iyyer (2020) Krishna, Kalpesh, John Wieting, and Mohit
    Iyyer. 2020. Reformulating unsupervised style transfer as paraphrase generation.
    *CoRR*, abs/2010.05700.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna, Wieting, and Iyyer (2020) Krishna, Kalpesh, John Wieting, 和Mohit Iyyer.
    2020. 将无监督风格迁移重新表述为释义生成。*CoRR*，abs/2010.05700。
- en: Kusner et al. (2015) Kusner, Matt, Yu Sun, Nicholas Kolkin, and Kilian Weinberger.
    2015. From word embeddings to document distances. In *International Conference
    on Machine Learning*, pages 957–966.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kusner et al. (2015) Kusner, Matt, Yu Sun, Nicholas Kolkin, 和Kilian Weinberger.
    2015. 从词嵌入到文档距离。在*国际机器学习大会*，第957–966页。
- en: Lai, Toral, and Nissim (2021) Lai, Huiyuan, Antonio Toral, and M. Nissim. 2021.
    Thank you bart! rewarding pre-trained models improves formality style transfer.
    In *ACL/IJCNLP*.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai, Toral, and Nissim (2021) Lai, Huiyuan, Antonio Toral, and M. Nissim. 2021.
    谢谢你，Bart！奖励预训练模型改进了正式风格迁移。见 *ACL/IJCNLP*。
- en: Lakoff (1973) Lakoff, Robin. 1973. Language and woman’s place. *Language in
    society*, 2(1):45–79.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakoff (1973) Lakoff, Robin. 1973. 《语言与女性的地位》。*语言与社会*，2(1):45–79。
- en: Lample et al. (2018a) Lample, Guillaume, Alexis Conneau, Ludovic Denoyer, and
    Marc’Aurelio Ranzato. 2018a. Unsupervised machine translation using monolingual
    corpora only. In *6th International Conference on Learning Representations, ICLR
    2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*,
    OpenReview.net.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lample et al. (2018a) Lample, Guillaume, Alexis Conneau, Ludovic Denoyer, and
    Marc’Aurelio Ranzato. 2018a. 使用单语语料库的无监督机器翻译。见 *第六届国际学习表征会议，ICLR 2018，加拿大温哥华，2018年4月30日
    - 5月3日，会议论文集*，OpenReview.net。
- en: Lample et al. (2018b) Lample, Guillaume, Myle Ott, Alexis Conneau, Ludovic Denoyer,
    and Marc’Aurelio Ranzato. 2018b. Phrase-based & neural unsupervised machine translation.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing, Brussels, Belgium, October 31 - November 4, 2018*, pages 5039–5049,
    Association for Computational Linguistics.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lample et al. (2018b) Lample, Guillaume, Myle Ott, Alexis Conneau, Ludovic Denoyer,
    and Marc’Aurelio Ranzato. 2018b. 基于短语和神经的无监督机器翻译。见 *2018年自然语言处理实证方法会议论文集，比利时布鲁塞尔，2018年10月31日
    - 11月4日*，第5039–5049页，计算语言学协会。
- en: Lample et al. (2019) Lample, Guillaume, Sandeep Subramanian, Eric Michael Smith,
    Ludovic Denoyer, Marc’Aurelio Ranzato, and Y-Lan Boureau. 2019. Multiple-attribute
    text rewriting. In *ICLR*.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lample et al. (2019) Lample, Guillaume, Sandeep Subramanian, Eric Michael Smith,
    Ludovic Denoyer, Marc’Aurelio Ranzato, and Y-Lan Boureau. 2019. 多属性文本重写。见 *ICLR*。
- en: 'Lee (2020) Lee, Joosung. 2020. Stable style transformer: Delete and generate
    approach with encoder-decoder for text style transfer. *CoRR*, abs/2005.12086.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee (2020) Lee, Joosung. 2020. 稳定风格变换器：用于文本风格迁移的删除与生成方法的编码器-解码器。*CoRR*，abs/2005.12086。
- en: Lewis et al. (2020) Lewis, Patrick S. H., Ethan Perez, Aleksandra Piktus, Fabio
    Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau
    Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented
    generation for knowledge-intensive NLP tasks. *CoRR*, abs/2005.11401.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis et al. (2020) Lewis, Patrick S. H., Ethan Perez, Aleksandra Piktus, Fabio
    Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau
    Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. 用于知识密集型 NLP 任务的检索增强生成。*CoRR*，abs/2005.11401。
- en: Li et al. (2019) Li, Dianqi, Yizhe Zhang, Zhe Gan, Yu Cheng, Chris Brockett,
    Bill Dolan, and Ming-Ting Sun. 2019. Domain adaptive text style transfer. In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP
    2019, Hong Kong, China, November 3-7, 2019*, pages 3302–3311, Association for
    Computational Linguistics.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019) Li, Dianqi, Yizhe Zhang, Zhe Gan, Yu Cheng, Chris Brockett,
    Bill Dolan, and Ming-Ting Sun. 2019. 域自适应文本风格迁移。见 *2019年自然语言处理实证方法会议和第九届国际自然语言处理联合会议，EMNLP-IJCNLP
    2019，中国香港，2019年11月3-7日*，第3302–3311页，计算语言学协会。
- en: 'Li et al. (2016) Li, Jiwei, Michel Galley, Chris Brockett, Georgios P. Spithourakis,
    Jianfeng Gao, and William B. Dolan. 2016. A persona-based neural conversation
    model. In *Proceedings of the 54th Annual Meeting of the Association for Computational
    Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers*,
    The Association for Computer Linguistics.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2016) Li, Jiwei, Michel Galley, Chris Brockett, Georgios P. Spithourakis,
    Jianfeng Gao, and William B. Dolan. 2016. 基于角色的神经对话模型。见 *第54届计算语言学协会年会论文集，ACL
    2016，2016年8月7-12日，德国柏林，第1卷：长篇论文*，计算语言学协会。
- en: 'Li et al. (2018) Li, Juncen, Robin Jia, He He, and Percy Liang. 2018. Delete,
    retrieve, generate: A simple approach to sentiment and style transfer. In *Proceedings
    of the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 1865–1874,
    Association for Computational Linguistics, New Orleans, Louisiana.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2018) Li, Juncen, Robin Jia, He He, and Percy Liang. 2018. 删除、检索、生成：一种简单的情感与风格迁移方法。见
    *2018年北美计算语言学协会人类语言技术会议论文集：长篇论文*，第1865–1874页，计算语言学协会，新奥尔良，路易斯安那州。
- en: 'Li et al. (2021) Li, Mingzhe, Xiuying Chen, Min Yang, Shen Gao, Dongyan Zhao,
    and Rui Yan. 2021. The style-content duality of attractiveness: Learning to write
    eye-catching headlines via disentanglement. In *Thirty-Fifth AAAI Conference on
    Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications
    of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances
    in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021*, pages
    13252–13260, AAAI Press.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2021）Li, Mingzhe, Xiuying Chen, Min Yang, Shen Gao, Dongyan Zhao, 和 Rui
    Yan。2021年。《吸引力的风格-内容二重性：通过解耦学习撰写引人注目的标题》。发表于*第35届人工智能AAAI会议，AAAI 2021，第33届创新应用人工智能会议，IAAI
    2021，第11届人工智能教育进展研讨会，EAAI 2021，虚拟会议，2021年2月2-9日*，第13252-13260页，AAAI出版社。
- en: 'Li and Liang (2021) Li, Xiang Lisa and Percy Liang. 2021. Prefix-tuning: Optimizing
    continuous prompts for generation. In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers),
    Virtual Event, August 1-6, 2021*, pages 4582–4597, Association for Computational
    Linguistics.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Liang（2021）Li, Xiang Lisa 和 Percy Liang。2021年。《前缀调优：优化生成的连续提示》。发表于*第59届计算语言学协会年会暨第11届国际自然语言处理联合会议，ACL/IJCNLP
    2021，（卷1：长篇论文），虚拟会议，2021年8月1-6日*，第4582-4597页，计算语言学协会。
- en: Li et al. (2020) Li, Yuan, Chunyuan Li, Yizhe Zhang, Xiujun Li, Guoqing Zheng,
    Lawrence Carin, and Jianfeng Gao. 2020. Complementary auxiliary classifiers for
    label-conditional text generation. In *The Thirty-Fourth AAAI Conference on Artificial
    Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial
    Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances
    in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020*,
    pages 8303–8310, AAAI Press.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020）Li, Yuan, Chunyuan Li, Yizhe Zhang, Xiujun Li, Guoqing Zheng, Lawrence
    Carin, 和 Jianfeng Gao。2020年。《用于标签条件文本生成的互补辅助分类器》。发表于*第34届人工智能AAAI会议，AAAI 2020，第32届创新应用人工智能会议，IAAI
    2020，第10届人工智能教育进展研讨会，EAAI 2020，美国纽约，2020年2月7-12日*，第8303-8310页，AAAI出版社。
- en: 'Liao et al. (2018) Liao, Yi, Lidong Bing, Piji Li, Shuming Shi, Wai Lam, and
    Tong Zhang. 2018. QuaSE: Sequence editing under quantifiable guidance. In *Proceedings
    of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages
    3855–3864, Association for Computational Linguistics, Brussels, Belgium.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao 等人（2018）Liao, Yi, Lidong Bing, Piji Li, Shuming Shi, Wai Lam, 和 Tong Zhang。2018年。《QuaSE：在可量化指导下的序列编辑》。发表于*2018年自然语言处理实证方法会议*，第3855-3864页，计算语言学协会，比利时布鲁塞尔。
- en: Lin and Och (2004) Lin, Chin-Yew and Franz Josef Och. 2004. Automatic evaluation
    of machine translation quality using longest common subsequence and skip-bigram
    statistics. In *Proceedings of the 42nd Annual Meeting of the Association for
    Computational Linguistics (ACL-04)*, pages 605–612, Barcelona, Spain.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 和 Och（2004）Lin, Chin-Yew 和 Franz Josef Och。2004年。《使用最长公共子序列和跳字组统计的机器翻译质量自动评估》。发表于*第42届计算语言学协会年会（ACL-04）*，第605-612页，西班牙巴塞罗那。
- en: 'Liu et al. (2020) Liu, Dayiheng, Jie Fu, Yidan Zhang, Chris Pal, and Jiancheng
    Lv. 2020. Revision in continuous space: Unsupervised text style transfer without
    adversarial learning. In *The Thirty-Fourth AAAI Conference on Artificial Intelligence,
    AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
    Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial
    Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020*, pages 8376–8383,
    AAAI Press.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2020）Liu, Dayiheng, Jie Fu, Yidan Zhang, Chris Pal, 和 Jiancheng Lv。2020年。《在连续空间中的修正：无监督文本风格转换无需对抗学习》。发表于*第34届人工智能AAAI会议，AAAI
    2020，第32届创新应用人工智能会议，IAAI 2020，第10届人工智能教育进展研讨会，EAAI 2020，美国纽约，2020年2月7-12日*，第8376-8383页，AAAI出版社。
- en: Locatello et al. (2019) Locatello, Francesco, Stefan Bauer, Mario Lucic, Gunnar
    Rätsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. 2019. Challenging
    common assumptions in the unsupervised learning of disentangled representations.
    In *Proceedings of the 36th International Conference on Machine Learning, ICML
    2019, 9-15 June 2019, Long Beach, California, USA*, volume 97 of *Proceedings
    of Machine Learning Research*, pages 4114–4124, PMLR.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Locatello 等（2019）Locatello, Francesco, Stefan Bauer, Mario Lucic, Gunnar Rätsch,
    Sylvain Gelly, Bernhard Schölkopf 和 Olivier Bachem. 2019. 挑战无监督学习中对解缠表示的常见假设。发表于
    *第36届国际机器学习大会论文集，ICML 2019，2019年6月9-15日，美国加利福尼亚州长滩*，*机器学习研究论文集*第97卷，第4114–4124页，PMLR。
- en: Locatello et al. (2020) Locatello, Francesco, Ben Poole, Gunnar Rätsch, Bernhard
    Schölkopf, Olivier Bachem, and Michael Tschannen. 2020. Weakly-supervised disentanglement
    without compromises. In *Proceedings of the 37th International Conference on Machine
    Learning, ICML 2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings
    of Machine Learning Research*, pages 6348–6359, PMLR.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Locatello 等（2020）Locatello, Francesco, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf,
    Olivier Bachem 和 Michael Tschannen. 2020. 无妥协的弱监督解缠。发表于 *第37届国际机器学习大会论文集，ICML
    2020，2020年7月13-18日，虚拟会议*，*机器学习研究论文集*第119卷，第6348–6359页，PMLR。
- en: 'Logeswaran, Lee, and Bengio (2018) Logeswaran, Lajanugen, Honglak Lee, and
    Samy Bengio. 2018. Content preserving text generation with attribute controls.
    In *Advances in Neural Information Processing Systems 31: Annual Conference on
    Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, pages 5108–5118.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logeswaran、Lee 和 Bengio（2018）Logeswaran, Lajanugen, Honglak Lee 和 Samy Bengio.
    2018. 具有属性控制的内容保持文本生成。发表于 *神经信息处理系统年会进展 31：2018年神经信息处理系统年会，NeurIPS 2018，2018年12月3-8日，加拿大蒙特利尔*，第5108–5118页。
- en: Luo et al. (2019) Luo, Fuli, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang,
    Zhifang Sui, and Xu Sun. 2019. A dual reinforcement learning framework for unsupervised
    text style transfer. In *Proceedings of the 28th International Joint Conference
    on Artificial Intelligence, IJCAI 2019*.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等（2019）Luo, Fuli, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang, Zhifang
    Sui 和 Xu Sun. 2019. 用于无监督文本风格转移的双重强化学习框架。发表于 *第28届国际人工智能联合会议论文集，IJCAI 2019*。
- en: 'Ma et al. (2020) Ma, Xinyao, Maarten Sap, Hannah Rashkin, and Yejin Choi. 2020.
    PowerTransformer: Unsupervised controllable revision for biased language correction.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2020, Online, November 16-20, 2020*, pages 7426–7441, Association
    for Computational Linguistics.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2020）Ma, Xinyao, Maarten Sap, Hannah Rashkin 和 Yejin Choi. 2020. PowerTransformer：用于偏见语言修正的无监督可控修订。发表于
    *2020年自然语言处理实证方法会议论文集，EMNLP 2020，在线，2020年11月16-20日*，第7426–7441页，计算语言学协会。
- en: 'Madaan et al. (2020) Madaan, Aman, Amrith Setlur, Tanmay Parekh, Barnabás Póczos,
    Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W. Black, and Shrimai Prabhumoye.
    2020. Politeness transfer: A tag and generate approach. In *Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,
    Online, July 5-10, 2020*, pages 1869–1881, Association for Computational Linguistics.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madaan 等（2020）Madaan, Aman, Amrith Setlur, Tanmay Parekh, Barnabás Póczos, Graham
    Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W. Black 和 Shrimai Prabhumoye.
    2020. 礼貌性转移：标记与生成方法。发表于 *第58届计算语言学协会年会论文集，ACL 2020，在线，2020年7月5-10日*，第1869–1881页，计算语言学协会。
- en: 'Madnani and Dorr (2010) Madnani, Nitin and Bonnie J. Dorr. 2010. Generating
    phrasal and sentential paraphrases: A survey of data-driven methods. *Compututational
    Linguistics*, 36(3):341–387.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madnani 和 Dorr（2010）Madnani, Nitin 和 Bonnie J. Dorr. 2010. 生成短语和句子的释义：数据驱动方法的综述。*计算语言学*，36(3)：341–387。
- en: 'Mairesse and Walker (2011) Mairesse, François and Marilyn A. Walker. 2011.
    Controlling user perceptions of linguistic style: Trainable generation of personality
    traits. *Computational Linguistics*, 37(3):455–488.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mairesse 和 Walker（2011）Mairesse, François 和 Marilyn A. Walker. 2011. 控制用户对语言风格的感知：可训练的个性特征生成。*计算语言学*，37(3)：455–488。
- en: Malmi, Severyn, and Rothe (2020a) Malmi, Eric, Aliaksei Severyn, and Sascha
    Rothe. 2020a. Unsupervised text style transfer with padded masked language models.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2020, Online, November 16-20, 2020*, pages 8671–8680, Association
    for Computational Linguistics.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malmi, Severyn 和 Rothe (2020a) Malmi, Eric, Aliaksei Severyn 和 Sascha Rothe.
    2020a. 使用填充掩码语言模型的无监督文本风格转换。见于 *2020年自然语言处理实证方法会议，EMNLP 2020，在线，2020年11月16-20日*，第8671–8680页，计算语言学协会。
- en: Malmi, Severyn, and Rothe (2020b) Malmi, Eric, Aliaksei Severyn, and Sascha
    Rothe. 2020b. Unsupervised text style transfer with padded masked language models.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 8671–8680, Association for Computational Linguistics,
    Online.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malmi, Severyn 和 Rothe (2020b) Malmi, Eric, Aliaksei Severyn 和 Sascha Rothe.
    2020b. 使用填充掩码语言模型的无监督文本风格转换。见于 *2020年自然语言处理实证方法会议（EMNLP）论文集*，第8671–8680页，计算语言学协会，在线。
- en: Mani (2001) Mani, Inderjeet. 2001. *Automatic summarization*, volume 3. John
    Benjamins Publishing.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mani (2001) Mani, Inderjeet. 2001. *自动摘要*，第3卷。John Benjamins Publishing。
- en: Mansoorizadeh et al. (2016) Mansoorizadeh, Muharram, Taher Rahgooy, Mohammad
    Aminiyan, and Mahdy Eskandari. 2016. Author obfuscation using wordnet and language
    models—notebook for pan at clef 2016. In *CLEF 2016 Evaluation Labs and Workshop–Working
    Notes Papers*, pages 5–8.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mansoorizadeh 等 (2016) Mansoorizadeh, Muharram, Taher Rahgooy, Mohammad Aminiyan,
    和 Mahdy Eskandari. 2016. 使用WordNet和语言模型的作者模糊化——CLEF 2016的笔记本。见于 *CLEF 2016评估实验室和研讨会–工作笔记论文*，第5–8页。
- en: 'Marie and Fujita (2017) Marie, Benjamin and Atsushi Fujita. 2017. Efficient
    extraction of pseudo-parallel sentences from raw monolingual data using word embeddings.
    In *Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 2: Short
    Papers*, pages 392–398, Association for Computational Linguistics.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marie 和 Fujita (2017) Marie, Benjamin 和 Atsushi Fujita. 2017. 使用词嵌入从原始单语数据中高效提取伪平行句子。见于
    *第55届计算语言学协会年会论文集，ACL 2017，加拿大温哥华，7月30日 - 8月4日，第2卷：短文*，第392–398页，计算语言学协会。
- en: McDonald and Pustejovsky (1985) McDonald, David D. and James Pustejovsky. 1985.
    A computational theory of prose style for natural language generation. In *EACL
    1985, 2nd Conference of the European Chapter of the Association for Computational
    Linguistics, March 27-29, 1985, University of Geneva, Geneva, Switzerland*, pages
    187–193, The Association for Computer Linguistics.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McDonald 和 Pustejovsky (1985) McDonald, David D. 和 James Pustejovsky. 1985.
    自然语言生成的散文风格计算理论。见于 *EACL 1985，第2届计算语言学协会欧洲分会会议，1985年3月27-29日，瑞士日内瓦大学*，第187–193页，计算语言学协会。
- en: 'McTear (2002) McTear, Michael F. 2002. Spoken dialogue technology: Enabling
    the conversational user interface. *ACM Computing Surveys*, 34(1):90–169.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McTear (2002) McTear, Michael F. 2002. 口语对话技术：支持对话用户界面。*ACM计算机调查*，34(1):90–169。
- en: Merity et al. (2017) Merity, Stephen, Caiming Xiong, James Bradbury, and R. Socher.
    2017. Pointer sentinel mixture models. *ArXiv*, abs/1609.07843.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等 (2017) Merity, Stephen, Caiming Xiong, James Bradbury, 和 R. Socher.
    2017. 指针守卫混合模型。*ArXiv*，abs/1609.07843。
- en: 'Mir et al. (2019) Mir, Remi, Bjarke Felbo, Nick Obradovich, and Iyad Rahwan.
    2019. Evaluating style transfer for text. In *Proceedings of the 2019 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 495–504,
    Association for Computational Linguistics, Minneapolis, Minnesota.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mir 等 (2019) Mir, Remi, Bjarke Felbo, Nick Obradovich 和 Iyad Rahwan. 2019. 文本风格转换的评估。见于
    *2019年北美计算语言学协会会议：人类语言技术，第1卷（长文和短文）*，第495–504页，计算语言学协会，美国明尼阿波利斯。
- en: 'Mou and Vechtomova (2020) Mou, Lili and Olga Vechtomova. 2020. Stylized text
    generation: Approaches and applications. In *Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics: Tutorial Abstracts*, pages 19–22,
    Association for Computational Linguistics, Online.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mou 和 Vechtomova (2020) Mou, Lili 和 Olga Vechtomova. 2020. 风格化文本生成：方法与应用。见于
    *第58届计算语言学协会年会：教程摘要*，第19–22页，计算语言学协会，在线。
- en: 'Mueller, Gifford, and Jaakkola (2017) Mueller, Jonas, David K. Gifford, and
    Tommi S. Jaakkola. 2017. Sequence to better sequence: Continuous revision of combinatorial
    structures. In *Proceedings of the 34th International Conference on Machine Learning,
    ICML 2017, Sydney, NSW, Australia, 6-11 August 2017*, volume 70 of *Proceedings
    of Machine Learning Research*, pages 2536–2544, PMLR.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mueller, Gifford, 和 Jaakkola (2017) Mueller, Jonas, David K. Gifford, 和 Tommi
    S. Jaakkola. 2017. 更好的序列：组合结构的持续修正. 收录于 *第34届国际机器学习会议论文集, ICML 2017, 悉尼, NSW,
    澳大利亚, 2017年8月6-11日*，第70卷 *机器学习研究论文集*，页码 2536–2544, PMLR.
- en: Munteanu and Marcu (2005) Munteanu, Dragos Stefan and Daniel Marcu. 2005. Improving
    machine translation performance by exploiting non-parallel corpora. *Computational
    Linguistics*, 31(4):477–504.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Munteanu 和 Marcu (2005) Munteanu, Dragos Stefan 和 Daniel Marcu. 2005. 通过利用非平行语料库提高机器翻译性能.
    *计算语言学*, 31(4):477–504.
- en: Nikolov and Hahnloser (2019) Nikolov, Nikola I. and Richard H. R. Hahnloser.
    2019. Large-scale hierarchical alignment for data-driven text rewriting. In *Proceedings
    of the International Conference on Recent Advances in Natural Language Processing,
    RANLP 2019, Varna, Bulgaria, September 2-4, 2019*, pages 844–853, INCOMA Ltd.
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nikolov 和 Hahnloser (2019) Nikolov, Nikola I. 和 Richard H. R. Hahnloser. 2019.
    大规模层次对齐用于数据驱动的文本重写. 收录于 *国际自然语言处理近期进展会议论文集, RANLP 2019, 瓦尔纳, 保加利亚, 2019年9月2-4日*，页码
    844–853, INCOMA Ltd.
- en: Niu and Bansal (2018) Niu, Tong and Mohit Bansal. 2018. Polite dialogue generation
    without parallel data. *Transactions of the Association for Computational Linguistics*,
    6:373–389.
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu 和 Bansal (2018) Niu, Tong 和 Mohit Bansal. 2018. 无需平行数据的礼貌对话生成. *计算语言学协会会刊*,
    6:373–389.
- en: 'Niu, Martindale, and Carpuat (2017) Niu, Xing, Marianna J. Martindale, and
    Marine Carpuat. 2017. A study of style in machine translation: Controlling the
    formality of machine translation output. In *Proceedings of the 2017 Conference
    on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark,
    September 9-11, 2017*, pages 2814–2819, Association for Computational Linguistics.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu, Martindale, 和 Carpuat (2017) Niu, Xing, Marianna J. Martindale, 和 Marine
    Carpuat. 2017. 机器翻译中的风格研究：控制机器翻译输出的正式性. 收录于 *2017年自然语言处理实证方法会议论文集, EMNLP 2017,
    哥本哈根, 丹麦, 2017年9月9-11日*，页码 2814–2819, 计算语言学协会.
- en: Niu, Rao, and Carpuat (2018) Niu, Xing, Sudha Rao, and Marine Carpuat. 2018.
    Multi-task neural models for translating between styles within and across languages.
    In *Proceedings of the 27th International Conference on Computational Linguistics*,
    pages 1008–1021, Association for Computational Linguistics, Santa Fe, New Mexico,
    USA.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu, Rao, 和 Carpuat (2018) Niu, Xing, Sudha Rao, 和 Marine Carpuat. 2018. 跨语言和语言内风格翻译的多任务神经模型.
    收录于 *第27届计算语言学国际会议论文集*，页码 1008–1021, 计算语言学协会, 圣菲, 新墨西哥州, 美国.
- en: 'Novikova, Dusek, and Rieser (2017) Novikova, Jekaterina, Ondrej Dusek, and
    Verena Rieser. 2017. The E2E dataset: New challenges for end-to-end generation.
    In *Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue,
    Saarbrücken, Germany, August 15-17, 2017*, pages 201–206, Association for Computational
    Linguistics.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Novikova, Dusek, 和 Rieser (2017) Novikova, Jekaterina, Ondrej Dusek, 和 Verena
    Rieser. 2017. E2E 数据集：端到端生成的新挑战. 收录于 *第18届SIGdial话语与对话年会会议论文集, 萨尔布吕肯, 德国, 2017年8月15-17日*，页码
    201–206, 计算语言学协会.
- en: 'Oksanen et al. (2014) Oksanen, Atte, James Hawdon, Emma Holkeri, Matti Näsi,
    and Pekka Räsänen. 2014. Exposure to online hate among young social media users.
    In *Soul of society: A focus on the lives of children & youth*. Emerald Group
    Publishing Limited.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oksanen 等人 (2014) Oksanen, Atte, James Hawdon, Emma Holkeri, Matti Näsi, 和 Pekka
    Räsänen. 2014. 年轻社交媒体用户的在线仇恨暴露. 收录于 *社会的灵魂：聚焦于儿童和青少年的生活*。Emerald Group Publishing
    Limited.
- en: Pang (2019) Pang, Richard Yuanzhe. 2019. The daunting task of real-world textual
    style transfer auto-evaluation. *CoRR*, abs/1910.03747.
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang (2019) Pang, Richard Yuanzhe. 2019. 现实世界文本风格转移自动评估的艰巨任务. *CoRR*, abs/1910.03747.
- en: Pang and Gimpel (2019) Pang, Richard Yuanzhe and Kevin Gimpel. 2019. Unsupervised
    evaluation metrics and learning criteria for non-parallel textual transfer. In
    *Proceedings of the 3rd Workshop on Neural Generation and Translation@EMNLP-IJCNLP
    2019, Hong Kong, November 4, 2019*, pages 138–147, Association for Computational
    Linguistics.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang 和 Gimpel (2019) Pang, Richard Yuanzhe 和 Kevin Gimpel. 2019. 无监督评估指标和非平行文本转移的学习标准.
    收录于 *第3届神经生成与翻译研讨会@EMNLP-IJCNLP 2019, 香港, 2019年11月4日*，页码 138–147, 计算语言学协会.
- en: 'Papineni et al. (2002) Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In
    *Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics*,
    pages 311–318, Association for Computational Linguistics, Philadelphia, Pennsylvania,
    USA.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni 等（2002）Papineni, Kishore, Salim Roukos, Todd Ward 和 Wei-Jing Zhu. 2002.
    BLEU：一种机器翻译自动评估方法。发表于*第40届计算语言学协会年会论文集*，第311–318页，计算语言学协会，宾夕法尼亚州费城，美国。
- en: 'Parikh et al. (2020) Parikh, Ankur P., Xuezhi Wang, Sebastian Gehrmann, Manaal
    Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A controlled
    table-to-text generation dataset. In *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020*,
    pages 1173–1186, Association for Computational Linguistics.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parikh 等（2020）Parikh, Ankur P., Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui,
    Bhuwan Dhingra, Diyi Yang 和 Dipanjan Das. 2020. ToTTo：一个受控的表格到文本生成数据集。发表于*2020年自然语言处理实证方法会议论文集，EMNLP
    2020，线上，2020年11月16-20日*，第1173–1186页，计算语言学协会。
- en: 'Pfaff (1979) Pfaff, Carol W. 1979. Constraints on language mixing: Intrasentential
    code-switching and borrowing in Spanish/English. *Language*, pages 291–318.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pfaff（1979）Pfaff, Carol W. 1979. 语言混合的约束：西班牙语/英语中的句内代码切换和借用。*语言*，第291–318页。
- en: 'Poplack (2000) Poplack, Shana. 2000. Sometimes I’ll start a sentence in Spanish
    y termino en español: Toward a typology of code-switching. *The bilingualism reader*,
    18(2):221–256.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poplack（2000）Poplack, Shana. 2000. 有时我会用西班牙语开始一个句子 y termino en español：朝向代码切换的类型学。*双语主义读本*，18(2):221–256。
- en: 'Popović (2015) Popović, Maja. 2015. chrF: Character n-gram F-score for automatic
    MT evaluation. In *Proceedings of the Tenth Workshop on Statistical Machine Translation*,
    pages 392–395, Association for Computational Linguistics, Lisbon, Portugal.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Popović（2015）Popović, Maja. 2015. chrF：用于自动机器翻译评估的字符n-gram F-score。发表于*第十届统计机器翻译研讨会论文集*，第392–395页，计算语言学协会，葡萄牙里斯本。
- en: 'Post and Vilar (2018) Post, Matt and David Vilar. 2018. Fast lexically constrained
    decoding with dynamic beam allocation for neural machine translation. In *Proceedings
    of the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 1314–1324,
    Association for Computational Linguistics, New Orleans, Louisiana.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Post 和 Vilar（2018）Post, Matt 和 David Vilar. 2018. 通过动态光束分配实现快速词汇约束解码用于神经机器翻译。发表于*2018年北美计算语言学协会：人类语言技术会议论文集，第1卷（长篇论文）*，第1314–1324页，计算语言学协会，路易斯安那州新奥尔良。
- en: Power, Scott, and Bouayad-Agha (2003) Power, Richard, Donia Scott, and Nadjet
    Bouayad-Agha. 2003. Generating texts with style. In *International Conference
    on Intelligent Text Processing and Computational Linguistics*, pages 444–452,
    Springer.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Power, Scott 和 Bouayad-Agha（2003）Power, Richard, Donia Scott 和 Nadjet Bouayad-Agha.
    2003. 生成风格文本。发表于*国际智能文本处理与计算语言学会议*，第444–452页，Springer。
- en: 'Prabhumoye et al. (2018) Prabhumoye, Shrimai, Yulia Tsvetkov, Ruslan Salakhutdinov,
    and Alan W Black. 2018. Style transfer through back-translation. In *Proceedings
    of the 56th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 866–876, Association for Computational Linguistics, Melbourne,
    Australia.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prabhumoye 等（2018）Prabhumoye, Shrimai, Yulia Tsvetkov, Ruslan Salakhutdinov
    和 Alan W Black. 2018. 通过反向翻译进行风格迁移。发表于*第56届计算语言学协会年会（第1卷：长篇论文）*，第866–876页，计算语言学协会，澳大利亚墨尔本。
- en: Pryzant et al. (2020) Pryzant, Reid, Richard Diehl Martinez, Nathan Dass, Sadao
    Kurohashi, Dan Jurafsky, and Diyi Yang. 2020. Automatically neutralizing subjective
    bias in text. In *The Thirty-Fourth AAAI Conference on Artificial Intelligence,
    AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
    Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial
    Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020*, pages 480–489,
    AAAI Press.
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pryzant 等（2020）Pryzant, Reid, Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi,
    Dan Jurafsky 和 Diyi Yang. 2020. 自动中和文本中的主观偏见。发表于*第三十四届AAAI人工智能大会，AAAI 2020，第三十二届创新人工智能应用大会，IAAI
    2020，第十届AAAI教育进展研讨会，EAAI 2020，纽约，NY，USA，2020年2月7-12日*，第480–489页，AAAI出版社。
- en: 'Qian et al. (2019) Qian, Kaizhi, Yang Zhang, Shiyu Chang, Xuesong Yang, and
    Mark Hasegawa-Johnson. 2019. AutoVC: Zero-shot voice style transfer with only
    autoencoder loss. In *Proceedings of the 36th International Conference on Machine
    Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA*, volume 97 of
    *Proceedings of Machine Learning Research*, pages 5210–5219, PMLR.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qian 等人 (2019) Qian, Kaizhi, Yang Zhang, Shiyu Chang, Xuesong Yang 和 Mark Hasegawa-Johnson.
    2019. AutoVC: 仅通过自编码器损失进行零样本语音风格转换。发表于 *第36届国际机器学习大会，ICML 2019，2019年6月9-15日，美国加利福尼亚州长滩*，第97卷
    *机器学习研究会议论文集*，第5210–5219页，PMLR。'
- en: 'Qin and Eisner (2021) Qin, Guanghui and Jason Eisner. 2021. Learning how to
    ask: Querying LMs with mixtures of soft prompts. In *Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021*, pages 5203–5212,
    Association for Computational Linguistics.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 和 Eisner (2021) Qin, Guanghui 和 Jason Eisner. 2021. 学会如何提问：用混合软提示查询语言模型。发表于
    *2021年北美计算语言学协会：人类语言技术会议，NAACL-HLT 2021，在线，2021年6月6-11日*，第5203–5212页，计算语言学协会。
- en: Qin et al. (2019) Qin, Lianhui, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula,
    Elizabeth Clark, and Yejin Choi. 2019. Counterfactual story reasoning and generation.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing,
    EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019*, pages 5042–5052, Association
    for Computational Linguistics.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人 (2019) Qin, Lianhui, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula,
    Elizabeth Clark 和 Yejin Choi. 2019. 反事实故事推理与生成。发表于 *2019年自然语言处理经验方法会议暨第九届国际联合自然语言处理会议，EMNLP-IJCNLP
    2019，中国香港，2019年11月3-7日*，第5042–5052页，计算语言学协会。
- en: Radford et al. (2019) Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
    *OpenAI Blog*, 1(8):9.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 (2019) Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei 和 Ilya Sutskever. 2019. 语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8):9。
- en: Raffel et al. (2020) Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of Machine Learning Research*, 21:140:1–140:67.
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 (2020) Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li 和 Peter J. Liu. 2020. 探索统一文本到文本变换器的迁移学习极限。*机器学习研究期刊*，21:140:1–140:67。
- en: 'Rao and Tetreault (2018) Rao, Sudha and Joel Tetreault. 2018. Dear sir or madam,
    may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality
    style transfer. In *Proceedings of the 2018 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)*, pages 129–140, Association for Computational Linguistics,
    New Orleans, Louisiana.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rao 和 Tetreault (2018) Rao, Sudha 和 Joel Tetreault. 2018. 亲爱的先生或女士，我介绍一下GYAFC数据集：形式风格转换的语料库、基准和指标。发表于
    *2018年北美计算语言学协会：人类语言技术会议第1卷（长论文）*，第129–140页，计算语言学协会，新奥尔良，路易斯安那州。
- en: Reddy and Knight (2016) Reddy, Sravana and Kevin Knight. 2016. Obfuscating gender
    in social media writing. In *Proceedings of the First Workshop on NLP and Computational
    Social Science, NLP+CSS@EMNLP 2016, Austin, TX, USA, November 5, 2016*, pages
    17–26, Association for Computational Linguistics.
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reddy 和 Knight (2016) Reddy, Sravana 和 Kevin Knight. 2016. 在社交媒体写作中模糊性别。发表于
    *第一次自然语言处理与计算社会科学研讨会，NLP+CSS@EMNLP 2016，美国德克萨斯州奥斯汀，2016年11月5日*，第17–26页，计算语言学协会。
- en: Reiter and Dale (1997) Reiter, Ehud and Robert Dale. 1997. Building applied
    natural language generation systems. *Natural Language Engineering*, 3(1):57–87.
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reiter 和 Dale (1997) Reiter, Ehud 和 Robert Dale. 1997. 构建应用自然语言生成系统。*自然语言工程*，3(1):57–87。
- en: 'Reiter, Robertson, and Osman (2003) Reiter, Ehud, Roma Robertson, and Liesl M
    Osman. 2003. Lessons from a failure: Generating tailored smoking cessation letters.
    *Artificial Intelligence*, 144(1-2):41–58.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reiter, Robertson 和 Osman (2003) Reiter, Ehud, Roma Robertson 和 Liesl M Osman.
    2003. 从失败中获得的教训：生成量身定制的戒烟信。*人工智能*，144(1-2):41–58。
- en: Reiter et al. (2005) Reiter, Ehud, Somayajulu Sripada, Jim Hunter, Jin Yu, and
    Ian Davy. 2005. Choosing words in computer-generated weather forecasts. *Artificial
    Intelligence*, 167(1-2):137–169.
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reiter 等人 (2005) Reiter, Ehud, Somayajulu Sripada, Jim Hunter, Jin Yu 和 Ian
    Davy. 2005. 在计算机生成天气预报中选择词汇。*人工智能*，167(1-2):137–169。
- en: Ren et al. (2020) Ren, Shuo, Yu Wu, Shujie Liu, Ming Zhou, and Shuai Ma. 2020.
    A retrieve-and-rewrite initialization method for unsupervised machine translation.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, ACL 2020, Online, July 5-10, 2020*, pages 3498–3504, Association
    for Computational Linguistics.
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人 (2020) Ren, Shuo, Yu Wu, Shujie Liu, Ming Zhou, 和 Shuai Ma. 2020. 一种用于无监督机器翻译的检索和重写初始化方法。在*第58届计算语言学协会年会，ACL
    2020，在线，2020年7月5-10日*，第3498–3504页，计算语言学协会。
- en: Rezende, Mohamed, and Wierstra (2014) Rezende, Danilo Jimenez, Shakir Mohamed,
    and Daan Wierstra. 2014. Stochastic backpropagation and approximate inference
    in deep generative models. In *Proceedings of the 31th International Conference
    on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014*, volume 32 of
    *JMLR Workshop and Conference Proceedings*, pages 1278–1286, JMLR.org.
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rezende, Mohamed 和 Wierstra (2014) Rezende, Danilo Jimenez, Shakir Mohamed,
    和 Daan Wierstra. 2014. 深度生成模型中的随机反向传播和近似推理。在*第31届国际机器学习会议，ICML 2014，北京，中国，2014年6月21-26日*，第32卷，*JMLR研讨会与会议论文集*，第1278–1286页，JMLR.org。
- en: Ribeiro et al. (2020) Ribeiro, Leonardo F. R., Martin Schmitt, Hinrich Schütze,
    and Iryna Gurevych. 2020. Investigating pretrained language models for graph-to-text
    generation. *CoRR*, abs/2007.08426.
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro 等人 (2020) Ribeiro, Leonardo F. R., Martin Schmitt, Hinrich Schütze,
    和 Iryna Gurevych. 2020. 调查预训练语言模型在图到文本生成中的应用。*CoRR*，abs/2007.08426。
- en: 'Riley et al. (2021a) Riley, Parker, Noah Constant, Mandy Guo, Girish Kumar,
    David Uthus, and Zarana Parekh. 2021a. TextSETTR: Few-shot text style extraction
    and tunable targeted restyling. In *Proceedings of the 59th Annual Meeting of
    the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 3786–3800,
    Association for Computational Linguistics, Online.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riley 等人 (2021a) Riley, Parker, Noah Constant, Mandy Guo, Girish Kumar, David
    Uthus, 和 Zarana Parekh. 2021a. TextSETTR：少样本文本风格提取与可调目标重塑。在*第59届计算语言学协会年会和第11届国际自然语言处理联合会议（第一卷：长篇论文）*，第3786–3800页，计算语言学协会，在线。
- en: 'Riley et al. (2021b) Riley, Parker, Noah Constant, Mandy Guo, Girish Kumar,
    David C. Uthus, and Zarana Parekh. 2021b. TextSettr: Few-shot text style extraction
    and tunable targeted restyling. In *ACL/IJCNLP*.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riley 等人 (2021b) Riley, Parker, Noah Constant, Mandy Guo, Girish Kumar, David
    C. Uthus, 和 Zarana Parekh. 2021b. TextSettr：少样本文本风格提取与可调目标重塑。在*ACL/IJCNLP*。
- en: 'Roller et al. (2021) Roller, Stephen, Emily Dinan, Naman Goyal, Da Ju, Mary
    Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau,
    and Jason Weston. 2021. Recipes for building an open-domain chatbot. In *Proceedings
    of the 16th Conference of the European Chapter of the Association for Computational
    Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021*, pages 300–325,
    Association for Computational Linguistics.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roller 等人 (2021) Roller, Stephen, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson,
    Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, 和 Jason Weston.
    2021. 构建开放领域聊天机器人的方法。在*第16届欧洲计算语言学协会会议：主卷，EACL 2021，在线，2021年4月19日至23日*，第300–325页，计算语言学协会。
- en: 'Romanov et al. (2019) Romanov, Alexey, Anna Rumshisky, Anna Rogers, and David
    Donahue. 2019. Adversarial decomposition of text representation. In *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    815–825, Association for Computational Linguistics, Minneapolis, Minnesota.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Romanov 等人 (2019) Romanov, Alexey, Anna Rumshisky, Anna Rogers, 和 David Donahue.
    2019. 文本表示的对抗性分解。在*第2019届北美计算语言学协会会议：人类语言技术，第一卷（长篇和短篇论文）*，第815–825页，计算语言学协会，明尼阿波利斯，明尼苏达州。
- en: Ruder, Dosovitskiy, and Brox (2016) Ruder, Manuel, Alexey Dosovitskiy, and Thomas
    Brox. 2016. Artistic style transfer for videos. In *Pattern Recognition - 38th
    German Conference, GCPR 2016, Hannover, Germany, September 12-15, 2016, Proceedings*,
    volume 9796 of *Lecture Notes in Computer Science*, pages 26–36, Springer.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruder, Dosovitskiy 和 Brox (2016) Ruder, Manuel, Alexey Dosovitskiy, 和 Thomas
    Brox. 2016. 视频的艺术风格迁移。在*模式识别 - 第38届德国会议，GCPR 2016，汉诺威，德国，2016年9月12-15日，会议论文集*，第9796卷，*计算机科学讲义笔记*，第26–36页，Springer。
- en: Rush, Chopra, and Weston (2015) Rush, Alexander M., Sumit Chopra, and Jason
    Weston. 2015. A neural attention model for abstractive sentence summarization.
    In *Proceedings of the 2015 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015*, pages 379–389,
    The Association for Computational Linguistics.
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rush, Chopra 和 Weston (2015) Rush, Alexander M., Sumit Chopra 和 Jason Weston.
    2015. 用于抽象句子总结的神经注意力模型。见于*2015年自然语言处理经验方法会议，EMNLP 2015，葡萄牙里斯本，2015年9月17-21日*，第379–389页，计算语言学协会。
- en: Russell, Dewey, and Tegmark (2015) Russell, Stuart J., Daniel Dewey, and Max
    Tegmark. 2015. Research priorities for robust and beneficial artificial intelligence.
    *AI Magazine*, 36(4):105–114.
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russell, Dewey 和 Tegmark (2015) Russell, Stuart J., Daniel Dewey 和 Max Tegmark.
    2015. 对稳健和有益人工智能的研究优先级。*AI Magazine*，36(4):105–114。
- en: Sancheti et al. (2020) Sancheti, Abhilasha, Kundan Krishna, Balaji Vasan Srinivasan,
    and Anandhavelu Natarajan. 2020. Reinforced rewards framework for text style transfer.
    In *European Conference on Information Retrieval*, pages 545–560, Springer.
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sancheti 等 (2020) Sancheti, Abhilasha, Kundan Krishna, Balaji Vasan Srinivasan
    和 Anandhavelu Natarajan. 2020. 用于文本风格转换的强化奖励框架。见于*欧洲信息检索会议*，第545–560页，Springer。
- en: 'dos Santos, Melnyk, and Padhi (2018) dos Santos, Cícero Nogueira, Igor Melnyk,
    and Inkit Padhi. 2018. Fighting offensive language on social media with unsupervised
    text style transfer. In *Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018,
    Volume 2: Short Papers*, pages 189–194, Association for Computational Linguistics.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dos Santos, Melnyk 和 Padhi (2018) dos Santos, Cícero Nogueira, Igor Melnyk 和
    Inkit Padhi. 2018. 通过无监督文本风格转换对抗社交媒体上的攻击性语言。见于*第56届计算语言学协会年会，ACL 2018，澳大利亚墨尔本，2018年7月15-20日，第2卷：短论文*，第189–194页，计算语言学协会。
- en: 'Scao and Rush (2021) Scao, Teven Le and Alexander M. Rush. 2021. How many data
    points is a prompt worth? In *Proceedings of the 2021 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, NAACL-HLT 2021, Online, June 6-11, 2021*, pages 2627–2636, Association
    for Computational Linguistics.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scao 和 Rush (2021) Scao, Teven Le 和 Alexander M. Rush. 2021. 一个提示值多少数据点？见于*2021年北美计算语言学协会年会：人类语言技术，NAACL-HLT
    2021，在线，2021年6月6-11日*，第2627–2636页，计算语言学协会。
- en: Schler et al. (2006) Schler, Jonathan, Moshe Koppel, Shlomo Argamon, and James W.
    Pennebaker. 2006. Effects of age and gender on blogging. In *Computational Approaches
    to Analyzing Weblogs, Papers from the 2006 AAAI Spring Symposium, Technical Report
    SS-06-03, Stanford, California, USA, March 27-29, 2006*, pages 199–205, AAAI.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schler 等 (2006) Schler, Jonathan, Moshe Koppel, Shlomo Argamon 和 James W. Pennebaker.
    2006. 年龄和性别对博客的影响。见于*分析博客的计算方法，2006年AAAI春季研讨会论文，技术报告 SS-06-03，加州斯坦福，2006年3月27-29日*，第199–205页，AAAI。
- en: 'See, Liu, and Manning (2017) See, Abigail, Peter J. Liu, and Christopher D.
    Manning. 2017. Get to the point: Summarization with pointer-generator networks.
    In *Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers*,
    pages 1073–1083, Association for Computational Linguistics.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: See, Liu 和 Manning (2017) See, Abigail, Peter J. Liu 和 Christopher D. Manning.
    2017. 直击要点：使用指针生成网络进行总结。见于*第55届计算语言学协会年会，ACL 2017，加拿大温哥华，2017年7月30日 - 8月4日，第1卷：长论文*，第1073–1083页，计算语言学协会。
- en: 'Sennrich, Haddow, and Birch (2016a) Sennrich, Rico, Barry Haddow, and Alexandra
    Birch. 2016a. Controlling politeness in neural machine translation via side constraints.
    In *NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, San Diego California,
    USA, June 12-17, 2016*, pages 35–40, The Association for Computational Linguistics.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sennrich, Haddow 和 Birch (2016a) Sennrich, Rico, Barry Haddow 和 Alexandra Birch.
    2016a. 通过侧面约束控制神经机器翻译中的礼貌性。见于*NAACL HLT 2016，2016年北美计算语言学协会年会：人类语言技术，加州圣地亚哥，美国，2016年6月12-17日*，第35–40页，计算语言学协会。
- en: 'Sennrich, Haddow, and Birch (2016b) Sennrich, Rico, Barry Haddow, and Alexandra
    Birch. 2016b. Edinburgh neural machine translation systems for WMT 16. In *Proceedings
    of the First Conference on Machine Translation: Volume 2, Shared Task Papers*,
    pages 371–376, Association for Computational Linguistics, Berlin, Germany.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sennrich, Haddow 和 Birch (2016b) Sennrich, Rico, Barry Haddow 和 Alexandra Birch.
    2016b. 爱丁堡神经机器翻译系统用于WMT 16。见于 *首次机器翻译会议论文集：第二卷，共享任务论文*，第371–376页，计算语言学协会，德国柏林。
- en: 'Shang et al. (2019) Shang, Mingyue, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan
    Zhao, Shuming Shi, and Rui Yan. 2019. Semi-supervised text style transfer: Cross
    projection in latent space. In *Proceedings of the 2019 Conference on Empirical
    Methods in Natural Language Processing and the 9th International Joint Conference
    on Natural Language Processing (EMNLP-IJCNLP)*, pages 4937–4946, Association for
    Computational Linguistics, Hong Kong, China.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang 等 (2019) Shang, Mingyue, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan Zhao,
    Shuming Shi 和 Rui Yan. 2019. 半监督文本风格迁移：潜在空间中的交叉投影。见于 *2019年自然语言处理经验方法会议及第9届国际自然语言处理联合会议（EMNLP-IJCNLP）论文集*，第4937–4946页，计算语言学协会，中国香港。
- en: Shao et al. (2018) Shao, Chengcheng, Giovanni Luca Ciampaglia, Onur Varol, Kai-Cheng
    Yang, Alessandro Flammini, and Filippo Menczer. 2018. The spread of low-credibility
    content by social bots. *Nature communications*, 9(1):1–9.
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等 (2018) Shao, Chengcheng, Giovanni Luca Ciampaglia, Onur Varol, Kai-Cheng
    Yang, Alessandro Flammini 和 Filippo Menczer. 2018. 社会机器人传播低可信度内容的研究。*自然通讯*，9(1):1–9。
- en: 'Sharma et al. (2021) Sharma, Ashish, Inna W. Lin, Adam S. Miner, David C. Atkins,
    and Tim Althoff. 2021. Towards facilitating empathic conversations in online mental
    health support: A reinforcement learning approach. *CoRR*, abs/2101.07714.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等 (2021) Sharma, Ashish, Inna W. Lin, Adam S. Miner, David C. Atkins
    和 Tim Althoff. 2021. 促进在线心理健康支持中的共情对话：一种强化学习方法。*CoRR*，abs/2101.07714。
- en: Sheikha and Inkpen (2011) Sheikha, Fadi Abu and Diana Inkpen. 2011. Generation
    of formal and informal sentences. In *ENLG 2011 - Proceedings of the 13th European
    Workshop on Natural Language Generation, 28-30 September 2011, Nancy, France*,
    pages 187–193, The Association for Computer Linguistics.
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheikha 和 Inkpen (2011) Sheikha, Fadi Abu 和 Diana Inkpen. 2011. 形式化和非形式化句子的生成。见于
    *ENLG 2011 - 第13届欧洲自然语言生成研讨会论文集, 2011年9月28-30日, 法国南希*，第187–193页，计算语言学协会。
- en: Shen et al. (2017) Shen, Tianxiao, Tao Lei, Regina Barzilay, and Tommi Jaakkola.
    2017. Style transfer from non-parallel text by cross-alignment. In *Advances in
    neural information processing systems*, pages 6830–6841.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等 (2017) Shen, Tianxiao, Tao Lei, Regina Barzilay 和 Tommi Jaakkola. 2017.
    通过交叉对齐从非平行文本中进行风格迁移。见于 *神经信息处理系统进展*，第6830–6841页。
- en: Shetty and Adibi (2004) Shetty, Jitesh and Jafar Adibi. 2004. The enron email
    dataset database schema and brief statistical report. *Information sciences institute
    technical report, University of Southern California*, 4(1):120–128.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shetty 和 Adibi (2004) Shetty, Jitesh 和 Jafar Adibi. 2004. Enron 邮件数据集数据库模式及简要统计报告。*信息科学研究所技术报告,
    南加州大学*，4(1):120–128。
- en: 'Shetty, Schiele, and Fritz (2018) Shetty, Rakshith, Bernt Schiele, and Mario
    Fritz. 2018. A4NT: author attribute anonymity by adversarial training of neural
    machine translation. In *27th USENIX Security Symposium, USENIX Security 2018,
    Baltimore, MD, USA, August 15-17, 2018*, pages 1633–1650, USENIX Association.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shetty, Schiele 和 Fritz (2018) Shetty, Rakshith, Bernt Schiele 和 Mario Fritz.
    2018. A4NT：通过对抗训练的神经机器翻译实现作者属性匿名化。见于 *第27届USENIX安全研讨会, USENIX Security 2018, 美国马里兰州巴尔的摩,
    2018年8月15-17日*，第1633–1650页，USENIX协会。
- en: 'Shuster et al. (2020) Shuster, Kurt, Samuel Humeau, Antoine Bordes, and Jason
    Weston. 2020. Image-Chat: Engaging grounded conversations. In *Proceedings of
    the 58th Annual Meeting of the Association for Computational Linguistics, ACL
    2020, Online, July 5-10, 2020*, pages 2414–2429, Association for Computational
    Linguistics.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shuster 等 (2020) Shuster, Kurt, Samuel Humeau, Antoine Bordes 和 Jason Weston.
    2020. Image-Chat: 引发有根有据的对话。见于 *第58届计算语言学协会年会论文集, ACL 2020, 在线, 2020年7月5-10日*，第2414–2429页，计算语言学协会。'
- en: 'Song et al. (2019) Song, Kaitao, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan
    Liu. 2019. MASS: masked sequence to sequence pre-training for language generation.
    In *Proceedings of the 36th International Conference on Machine Learning, ICML
    2019, 9-15 June 2019, Long Beach, California, USA*, volume 97 of *Proceedings
    of Machine Learning Research*, pages 5926–5936, PMLR.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等 (2019) Song, Kaitao, Xu Tan, Tao Qin, Jianfeng Lu 和 Tie-Yan Liu. 2019.
    MASS：用于语言生成的掩码序列到序列预训练。见于 *第36届国际机器学习大会论文集，ICML 2019, 2019年6月9-15日, 美国加利福尼亚州长滩*，第97卷
    *机器学习研究论文集*，第5926–5936页，PMLR。
- en: Sripada et al. (2004) Sripada, Somayajulu, Ehud Reiter, Ian Davy, and Kristian
    Nilssen. 2004. Lessons from deploying NLG technology for marine weather forecast
    text generation. In *Proceedings of the 16th Eureopean Conference on Artificial
    Intelligence, ECAI’2004, including Prestigious Applicants of Intelligent Systems,
    PAIS 2004, Valencia, Spain, August 22-27, 2004*, pages 760–764, IOS Press.
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sripada et al. (2004) Sripada, Somayajulu, Ehud Reiter, Ian Davy, 和 Kristian
    Nilssen. 2004. 部署NLG技术用于海洋天气预报文本生成的经验教训。发表于 *第16届欧洲人工智能会议论文集，ECAI’2004，包括智能系统的杰出应用，PAIS
    2004，西班牙瓦伦西亚，2004年8月22-27日*，第760–764页，IOS出版社。
- en: Stamatatos et al. (1997) Stamatatos, Efstathios, S Michos, Nikos Fakotakis,
    and George Kokkinakis. 1997. A user-assisted business letter generator dealing
    with text’s stylistic variations. In *Proceedings Ninth IEEE International Conference
    on Tools with Artificial Intelligence*, pages 182–189, IEEE.
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stamatatos et al. (1997) Stamatatos, Efstathios, S Michos, Nikos Fakotakis,
    和 George Kokkinakis. 1997. 处理文本风格变化的用户辅助商务信函生成器。发表于 *第九届IEEE国际人工智能工具会议论文集*，第182–189页，IEEE。
- en: Starr (2019) Starr, William. 2019. Counterfactuals. *The Stanford Encyclopedia
    of Philosophy*.
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Starr (2019) Starr, William. 2019. 反事实。*斯坦福哲学百科全书*。
- en: Sudhakar, Upadhyay, and Maheswaran (2019) Sudhakar, Akhilesh, Bhargav Upadhyay,
    and Arjun Maheswaran. 2019. "transforming" delete, retrieve, generate approach
    for controlled text style transfer. In *Proceedings of the 2019 Conference on
    Empirical Methods in Natural Language Processing and the 9th International Joint
    Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China,
    November 3-7, 2019*, pages 3267–3277, Association for Computational Linguistics.
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sudhakar, Upadhyay, and Maheswaran (2019) Sudhakar, Akhilesh, Bhargav Upadhyay,
    和 Arjun Maheswaran. 2019. “转换”删除、检索、生成方法用于受控文本风格转换。发表于 *2019年自然语言处理实证方法会议暨第九届国际联合自然语言处理会议，EMNLP-IJCNLP
    2019，香港，中国，2019年11月3-7日*，第3267–3277页，计算语言学协会。
- en: Sutskever, Vinyals, and Le (2014) Sutskever, Ilya, Oriol Vinyals, and Quoc V
    Le. 2014. Sequence to sequence learning with neural networks. In *Advances in
    neural information processing systems*, pages 3104–3112.
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever, Vinyals, and Le (2014) Sutskever, Ilya, Oriol Vinyals, 和 Quoc V Le.
    2014. 基于神经网络的序列到序列学习。发表于 *神经信息处理系统进展*，第3104–3112页。
- en: Syed et al. (2020) Syed, Bakhtiyar, Gaurav Verma, Balaji Vasan Srinivasan, Anandhavelu
    Natarajan, and Vasudeva Varma. 2020. Adapting language models for non-parallel
    author-stylized rewriting. In *AAAI*.
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Syed et al. (2020) Syed, Bakhtiyar, Gaurav Verma, Balaji Vasan Srinivasan, Anandhavelu
    Natarajan, 和 Vasudeva Varma. 2020. 针对非平行作者风格化重写的语言模型适应。发表于 *AAAI*。
- en: 'Tan and Goonawardene (2017) Tan, Sharon Swee-Lin and Nadee Goonawardene. 2017.
    Internet health information seeking and the patient-physician relationship: A
    systematic review. *Journal of medical Internet research*, 19(1):e9.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan and Goonawardene (2017) Tan, Sharon Swee-Lin 和 Nadee Goonawardene. 2017.
    互联网健康信息获取与患者-医生关系：系统综述。*医学互联网研究期刊*, 19(1):e9。
- en: 'Tannen (1990) Tannen, Deborah. 1990. Gender differences in topical coherence:
    Creating involvement in best friends’ talk. *Discourse Processes*, 13(1):73–90.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tannen (1990) Tannen, Deborah. 1990. 话题连贯中的性别差异：在最好的朋友对话中创造参与感。*话语过程*, 13(1):73–90。
- en: Tian, Hu, and Yu (2018) Tian, Youzhi, Zhiting Hu, and Zhou Yu. 2018. Structured
    content preservation for unsupervised text style transfer. *CoRR*, abs/1810.06526.
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian, Hu, and Yu (2018) Tian, Youzhi, Zhiting Hu, and Zhou Yu. 2018. 无监督文本风格转换的结构化内容保留。*CoRR*,
    abs/1810.06526。
- en: 'Tikhonov et al. (2019) Tikhonov, Alexey, Viacheslav Shibaev, Aleksander Nagaev,
    Aigul Nugmanova, and Ivan P. Yamshchikov. 2019. Style transfer for texts: Retrain,
    report errors, compare with rewrites. In *Proceedings of the 2019 Conference on
    Empirical Methods in Natural Language Processing and the 9th International Joint
    Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 3936–3945, Association
    for Computational Linguistics, Hong Kong, China.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tikhonov et al. (2019) Tikhonov, Alexey, Viacheslav Shibaev, Aleksander Nagaev,
    Aigul Nugmanova, 和 Ivan P. Yamshchikov. 2019. 文本风格转换：重新训练、报告错误、与改写进行比较。发表于 *2019年自然语言处理实证方法会议暨第九届国际联合自然语言处理会议
    (EMNLP-IJCNLP)*，第3936–3945页，计算语言学协会，香港，中国。
- en: Tikhonov and Yamshchikov (2018) Tikhonov, Alexey and Ivan P. Yamshchikov. 2018.
    What is wrong with style transfer for texts? *CoRR*, abs/1808.04365.
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tikhonov and Yamshchikov (2018) Tikhonov, Alexey 和 Ivan P. Yamshchikov. 2018.
    文本风格转换的弊端是什么？*CoRR*, abs/1808.04365。
- en: 'Tran, Zhang, and Soleymani (2020) Tran, Minh, Yipeng Zhang, and Mohammad Soleymani.
    2020. Towards a friendly online community: An unsupervised style transfer framework
    for profanity redaction. *CoRR*, abs/2011.00403.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran, Zhang, 和 Soleymani (2020) Tran, Minh, Yipeng Zhang, 和 Mohammad Soleymani.
    2020. 面向友好的在线社区：一种用于亵渎词处理的无监督风格转换框架。*CoRR*, abs/2011.00403。
- en: Trudgill (1972) Trudgill, Peter. 1972. Sex, covert prestige and linguistic change
    in the urban british english of norwich. *Language in society*, 1(2):179–195.
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trudgill (1972) Trudgill, Peter. 1972. 性别、隐性声望和诺里奇都市英国英语的语言变化。*语言与社会*，1(2):179–195。
- en: Tse et al. (2015) Tse, Jonathan, Dawn E. Schrader, Dipayan P. Ghosh, Tony C.
    Liao, and David Lundie. 2015. A bibliometric analysis of privacy and ethics in
    *IEEE Security and Privacy*. *Ethics and Information Technology*, 17(2):153–163.
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tse 等 (2015) Tse, Jonathan, Dawn E. Schrader, Dipayan P. Ghosh, Tony C. Liao,
    和 David Lundie. 2015. *IEEE Security and Privacy*中的隐私和伦理的文献计量分析。*伦理与信息技术*，17(2):153–163。
- en: Uszkoreit et al. (2010) Uszkoreit, Jakob, Jay Ponte, Ashok C. Popat, and Moshe
    Dubiner. 2010. Large scale parallel document mining for machine translation. In
    *COLING 2010, 23rd International Conference on Computational Linguistics, Proceedings
    of the Conference, 23-27 August 2010, Beijing, China*, pages 1101–1109, Tsinghua
    University Press.
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Uszkoreit 等 (2010) Uszkoreit, Jakob, Jay Ponte, Ashok C. Popat, 和 Moshe Dubiner.
    2010. 大规模并行文档挖掘用于机器翻译。发表于*COLING 2010，第23届国际计算语言学会议，会议论文集，2010年8月23日至27日，中国北京*，页码1101–1109，清华大学出版社。
- en: 'Vaswani et al. (2017) Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *Advances in Neural Information Processing Systems 30: Annual
    Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
    Long Beach, CA, USA*, pages 5998–6008.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 (2017) Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, 和 Illia Polosukhin. 2017. 注意力机制是你所需的一切。发表于*神经信息处理系统进展
    30：2017年神经信息处理系统年会，2017年12月4日至9日，美国长滩*，页码5998–6008。
- en: 'Vincent et al. (2010) Vincent, Pascal, Hugo Larochelle, Isabelle Lajoie, Yoshua
    Bengio, and Pierre-Antoine Manzagol. 2010. Stacked denoising autoencoders: Learning
    useful representations in a deep network with a local denoising criterion. *Journal
    of Machine Learning Research*, 11:3371–3408.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent 等 (2010) Vincent, Pascal, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio,
    和 Pierre-Antoine Manzagol. 2010. 堆叠去噪自编码器：在具有局部去噪准则的深度网络中学习有用的表示。*机器学习研究杂志*，11:3371–3408。
- en: 'Voigt et al. (2018) Voigt, Rob, David Jurgens, Vinodkumar Prabhakaran, Dan
    Jurafsky, and Yulia Tsvetkov. 2018. RtGender: A corpus for studying differential
    responses to gender. In *Proceedings of the Eleventh International Conference
    on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018*,
    European Language Resources Association (ELRA).'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voigt 等 (2018) Voigt, Rob, David Jurgens, Vinodkumar Prabhakaran, Dan Jurafsky,
    和 Yulia Tsvetkov. 2018. RtGender：一个用于研究对性别不同反应的语料库。发表于*第十一届国际语言资源与评估会议，LREC 2018，日本宫崎，2018年5月7日至12日*，欧洲语言资源协会（ELRA）。
- en: 'Wang, Quan, and Wang (2019) Wang, Kai, Xiaojun Quan, and Rui Wang. 2019. BiSET:
    Bi-directional selective encoding with template for abstractive summarization.
    In *Proceedings of the 57th Conference of the Association for Computational Linguistics,
    ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers*, pages
    2153–2162, Association for Computational Linguistics.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang, Quan, 和 Wang (2019) Wang, Kai, Xiaojun Quan, 和 Rui Wang. 2019. BiSET：用于抽象总结的双向选择编码与模板。发表于*第57届计算语言学协会年会论文集，ACL
    2019，意大利佛罗伦萨，2019年7月28日至8月2日，第1卷：长篇论文*，页码2153–2162，计算语言学协会。
- en: 'Wang, Hua, and Wan (2019) Wang, Ke, Hang Hua, and Xiaojun Wan. 2019. Controllable
    unsupervised text attribute transfer via editing entangled latent representation.
    In *Advances in Neural Information Processing Systems 32: Annual Conference on
    Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
    Vancouver, BC, Canada*, pages 11034–11044.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang, Hua, 和 Wan (2019) Wang, Ke, Hang Hua, 和 Xiaojun Wan. 2019. 通过编辑纠缠的潜在表示进行可控的无监督文本属性转移。发表于*神经信息处理系统进展
    32：2019年神经信息处理系统年会，NeurIPS 2019，2019年12月8日至14日，加拿大温哥华*，页码11034–11044。
- en: Wang et al. (2019) Wang, Yunli, Yu Wu, Lili Mou, Zhoujun Li, and Wenhan Chao.
    2019. Harnessing pre-trained neural networks with rules for formality style transfer.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 3573–3578, Association for Computational Linguistics, Hong
    Kong, China.
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019) Wang, Yunli, Yu Wu, Lili Mou, Zhoujun Li, 和 Wenhan Chao.
    2019. 利用规则与预训练神经网络进行正式风格转换。发表于*2019年自然语言处理实证方法会议及第9届国际联合自然语言处理会议（EMNLP-IJCNLP）论文集*，第3573–3578页，计算语言学协会，中国香港。
- en: 'Waseem et al. (2017) Waseem, Zeerak, Thomas Davidson, Dana Warmsley, and Ingmar
    Weber. 2017. Understanding abuse: A typology of abusive language detection subtasks.
    In *Proceedings of the First Workshop on Abusive Language Online, ALW@ACL 2017,
    Vancouver, BC, Canada, August 4, 2017*, pages 78–84, Association for Computational
    Linguistics.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Waseem et al. (2017) Waseem, Zeerak, Thomas Davidson, Dana Warmsley, 和 Ingmar
    Weber. 2017. 理解滥用：滥用语言检测子任务的类型学。发表于*首次网络滥用语言研讨会论文集（ALW@ACL 2017），加拿大不列颠哥伦比亚省温哥华，2017年8月4日*，第78–84页，计算语言学协会。
- en: Weng, Chung, and Szolovits (2019) Weng, Wei-Hung, Yu-An Chung, and Peter Szolovits.
    2019. Unsupervised clinical language translation. In *Proceedings of the 25th
    ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD
    2019, Anchorage, AK, USA, August 4-8, 2019*, pages 3121–3131, ACM.
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng, Chung, 和 Szolovits (2019) Weng, Wei-Hung, Yu-An Chung, 和 Peter Szolovits.
    2019. 无监督临床语言翻译。发表于*第25届ACM SIGKDD国际知识发现与数据挖掘大会论文集（KDD 2019），美国阿拉斯加州安克雷奇，2019年8月4-8日*，第3121–3131页，ACM。
- en: 'Weston, Dinan, and Miller (2018) Weston, Jason, Emily Dinan, and Alexander H.
    Miller. 2018. Retrieve and refine: Improved sequence generation models for dialogue.
    In *Proceedings of the 2nd International Workshop on Search-Oriented Conversational
    AI, SCAI@EMNLP 2018, Brussels, Belgium, October 31, 2018*, pages 87–92, Association
    for Computational Linguistics.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weston, Dinan, 和 Miller (2018) Weston, Jason, Emily Dinan, 和 Alexander H. Miller.
    2018. 检索与精炼：改进的对话生成模型。发表于*第二届面向搜索的对话AI国际研讨会论文集（SCAI@EMNLP 2018），比利时布鲁塞尔，2018年10月31日*，第87–92页，计算语言学协会。
- en: Williams (1992) Williams, Ronald J. 1992. Simple statistical gradient-following
    algorithms for connectionist reinforcement learning. *Machine learning*, 8(3-4):229–256.
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams (1992) Williams, Ronald J. 1992. 连接主义强化学习的简单统计梯度跟踪算法。*机器学习*，8(3-4)：229–256。
- en: Wiseman, Shieber, and Rush (2017) Wiseman, Sam, Stuart M. Shieber, and Alexander M.
    Rush. 2017. Challenges in data-to-document generation. In *Proceedings of the
    2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017,
    Copenhagen, Denmark, September 9-11, 2017*, pages 2253–2263, Association for Computational
    Linguistics.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wiseman, Shieber, 和 Rush (2017) Wiseman, Sam, Stuart M. Shieber, 和 Alexander
    M. Rush. 2017. 数据到文档生成中的挑战。发表于*2017年自然语言处理实证方法会议论文集（EMNLP 2017），丹麦哥本哈根，2017年9月9-11日*，第2253–2263页，计算语言学协会。
- en: 'Wu, Wang, and Wang (2019) Wu, Jiawei, Xin Wang, and William Yang Wang. 2019.
    Extract and edit: An alternative to back-translation for unsupervised neural machine
    translation. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers)*, pages 1173–1183, Association for Computational Linguistics.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu, Wang, 和 Wang (2019) Wu, Jiawei, Xin Wang, 和 William Yang Wang. 2019. 提取与编辑：一种替代反向翻译的无监督神经机器翻译方法。发表于*2019年北美计算语言学协会人类语言技术会议论文集（NAACL-HLT
    2019），美国明尼阿波利斯，2019年6月2-7日，第1卷（长短篇论文）*，第1173–1183页，计算语言学协会。
- en: 'Wu et al. (2019) Wu, Xing, Tao Zhang, Liangjun Zang, Jizhong Han, and Songlin
    Hu. 2019. "mask and infill" : Applying masked language model to sentiment transfer.
    *CoRR*, abs/1908.08039.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2019) Wu, Xing, Tao Zhang, Liangjun Zang, Jizhong Han, 和 Songlin
    Hu. 2019. “mask and infill” ：应用掩码语言模型进行情感转换。*CoRR*，abs/1908.08039。
- en: Wu, Wang, and Liu (2020) Wu, Yu, Yunli Wang, and Shujie Liu. 2020. A dataset
    for low-resource stylized sequence-to-sequence generation. In *The Thirty-Fourth
    AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative
    Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
    NY, USA, February 7-12, 2020*, pages 9290–9297, AAAI Press.
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu, Wang, 和 Liu（2020）Wu, Yu, Yunli Wang, 和 Shujie Liu. 2020. 一个低资源风格化序列到序列生成的数据集。在
    *第三十四届 AAAI 人工智能会议，AAAI 2020，第三十二届创新应用人工智能会议，IAAI 2020，第十届 AAAI 人工智能教育进展研讨会，EAAI
    2020，纽约，美国，2020年2月7-12日*，第9290–9297页，AAAI出版社。
- en: 'Xing et al. (2020) Xing, Xiaoyu, Zhijing Jin, Di Jin, Bingning Wang, Qi Zhang,
    and Xuanjing Huang. 2020. Tasty burgers, soggy fries: Probing aspect robustness
    in aspect-based sentiment analysis. In *Proceedings of the 2020 Conference on
    Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November
    16-20, 2020*, pages 3594–3605, Association for Computational Linguistics.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xing 等（2020）Xing, Xiaoyu, Zhijing Jin, Di Jin, Bingning Wang, Qi Zhang, 和 Xuanjing
    Huang. 2020. 美味的汉堡，湿透的薯条：探讨基于方面的情感分析中的方面鲁棒性。在 *2020年自然语言处理经验方法会议，EMNLP 2020，在线，2020年11月16-20日*，第3594–3605页，计算语言学协会。
- en: 'Xu et al. (2018) Xu, Jingjing, Xu Sun, Qi Zeng, Xiaodong Zhang, Xuancheng Ren,
    Houfeng Wang, and Wenjie Li. 2018. Unpaired sentiment-to-sentiment translation:
    A cycled reinforcement learning approach. In *Proceedings of the 56th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages
    979–988, Association for Computational Linguistics, Melbourne, Australia.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2018）Xu, Jingjing, Xu Sun, Qi Zeng, Xiaodong Zhang, Xuancheng Ren, Houfeng
    Wang, 和 Wenjie Li. 2018. 无配对情感到情感翻译：一种循环强化学习方法。在 *第56届计算语言学协会年会（第1卷：长篇论文）*，第979–988页，计算语言学协会，澳大利亚墨尔本。
- en: Xu, Cheung, and Cao (2020) Xu, Peng, Jackie Chi Kit Cheung, and Yanshuai Cao.
    2020. On variational learning of controllable representations for text without
    supervision. In *Proceedings of the 37th International Conference on Machine Learning,
    ICML 2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings of Machine
    Learning Research*, pages 10534–10543, PMLR.
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu, Cheung, 和 Cao（2020）Xu, Peng, Jackie Chi Kit Cheung, 和 Yanshuai Cao. 2020.
    关于无监督文本可控表示的变分学习。在 *第37届国际机器学习大会，ICML 2020，2020年7月13-18日，虚拟活动*，第119卷 *机器学习研究论文集*，第10534–10543页，PMLR。
- en: Xu et al. (2020) Xu, Qiuling, Guanhong Tao, Siyuan Cheng, Lin Tan, and Xiangyu
    Zhang. 2020. Towards feature space adversarial attack. *CoRR*, abs/2004.12385.
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2020）Xu, Qiuling, Guanhong Tao, Siyuan Cheng, Lin Tan, 和 Xiangyu Zhang.
    2020. 朝着特征空间对抗攻击。*CoRR*，abs/2004.12385。
- en: Xu, Ge, and Wei (2019) Xu, Ruochen, Tao Ge, and Furu Wei. 2019. Formality style
    transfer with hybrid textual annotations. *CoRR*, abs/1903.06353.
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu, Ge, 和 Wei（2019）Xu, Ruochen, Tao Ge, 和 Furu Wei. 2019. 具有混合文本注释的正式风格迁移。*CoRR*，abs/1903.06353。
- en: 'Xu et al. (2012) Xu, Wei, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin
    Cherry. 2012. Paraphrasing for style. In *COLING 2012, 24th International Conference
    on Computational Linguistics, Proceedings of the Conference: Technical Papers,
    8-15 December 2012, Mumbai, India*, pages 2899–2914, Indian Institute of Technology
    Bombay.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2012）Xu, Wei, Alan Ritter, Bill Dolan, Ralph Grishman, 和 Colin Cherry.
    2012. 用于风格的释义。在 *COLING 2012，第24届国际计算语言学大会，会议论文集：技术论文，2012年12月8-15日，印度孟买*，第2899–2914页，印度理工学院孟买分校。
- en: 'Yamshchikov et al. (2021) Yamshchikov, Ivan P., Viacheslav Shibaev, Nikolay
    Khlebnikov, and Alexey Tikhonov. 2021. Style-transfer and paraphrase: Looking
    for a sensible semantic similarity metric. In *Thirty-Fifth AAAI Conference on
    Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications
    of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances
    in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021*, pages
    14213–14220, AAAI Press.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yamshchikov 等（2021）Yamshchikov, Ivan P., Viacheslav Shibaev, Nikolay Khlebnikov,
    和 Alexey Tikhonov. 2021. 风格迁移与释义：寻找合理的语义相似性度量。在 *第三十五届 AAAI 人工智能会议，AAAI 2021，第三十三届创新应用人工智能会议，IAAI
    2021，第十一届人工智能教育进展研讨会，EAAI 2021，虚拟活动，2021年2月2-9日*，第14213–14220页，AAAI出版社。
- en: Yamshchikov et al. (2019) Yamshchikov, Ivan P., Viacheslav Shibaev, Aleksander
    Nagaev, Jürgen Jost, and Alexey Tikhonov. 2019. Decomposing textual information
    for style transfer. In *Proceedings of the 3rd Workshop on Neural Generation and
    Translation*, pages 128–137, Association for Computational Linguistics, Hong Kong.
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yamshchikov et al. (2019) Yamshchikov, Ivan P., Viacheslav Shibaev, Aleksander
    Nagaev, Jürgen Jost, and Alexey Tikhonov. 2019. 分解文本信息以进行风格转换。载于*第三届神经生成与翻译研讨会论文集*，第128–137页，计算语言学协会，香港。
- en: 'Yang et al. (2018) Yang, Zichao, Zhiting Hu, Chris Dyer, Eric P. Xing, and
    Taylor Berg-Kirkpatrick. 2018. Unsupervised text style transfer using language
    models as discriminators. In *Advances in Neural Information Processing Systems
    31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
    3-8 December 2018, Montréal, Canada*, pages 7298–7309.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2018) Yang, Zichao, Zhiting Hu, Chris Dyer, Eric P. Xing, and Taylor
    Berg-Kirkpatrick. 2018. 使用语言模型作为鉴别器的无监督文本风格转换。载于*神经信息处理系统年会论文集 31：2018年神经信息处理系统会议，NeurIPS
    2018，2018年12月3-8日，加拿大蒙特利尔*，第7298–7309页。
- en: Yi et al. (2020) Yi, Xiaoyuan, Zhenghao Liu, Wenhao Li, and Maosong Sun. 2020.
    Text style transfer via learning style instance supported latent space. In *Proceedings
    of the Twenty-Ninth International Joint Conference on Artificial Intelligence,
    IJCAI 2020*, pages 3801–3807, ijcai.org.
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi et al. (2020) Yi, Xiaoyuan, Zhenghao Liu, Wenhao Li, and Maosong Sun. 2020.
    通过学习风格实例支持的潜在空间进行文本风格转换。载于*第二十九届国际人工智能联合会议论文集，IJCAI 2020*，第3801–3807页，ijcai.org。
- en: Yuan et al. (2021) Yuan, Siyang, Pengyu Cheng, Ruiyi Zhang, Weituo Hao, Zhe
    Gan, and Lawrence Carin. 2021. Improving zero-shot voice style transfer via disentangled
    representation learning. *CoRR*, abs/2103.09420.
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2021) Yuan, Siyang, Pengyu Cheng, Ruiyi Zhang, Weituo Hao, Zhe
    Gan, and Lawrence Carin. 2021. 通过解耦表示学习改进零样本语音风格转换。*CoRR*，abs/2103.09420。
- en: Zeng, Shoeybi, and Liu (2020) Zeng, Kuo-Hao, Mohammad Shoeybi, and Ming-Yu Liu.
    2020. Style example-guided text generation using generative adversarial transformers.
    *CoRR*, abs/2003.00674.
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng, Shoeybi, and Liu (2020) Zeng, Kuo-Hao, Mohammad Shoeybi, and Ming-Yu Liu.
    2020. 使用生成对抗变换器的风格示例引导文本生成。*CoRR*，abs/2003.00674。
- en: 'Zhang et al. (2018a) Zhang, Jingyi, Masao Utiyama, Eiichiro Sumita, Graham
    Neubig, and Satoshi Nakamura. 2018a. Guiding neural machine translation with retrieved
    translation pieces. In *Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)*,
    pages 1325–1335, Association for Computational Linguistics.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018a) Zhang, Jingyi, Masao Utiyama, Eiichiro Sumita, Graham Neubig,
    and Satoshi Nakamura. 2018a. 通过检索的翻译片段指导神经机器翻译。载于*2018年北美计算语言学协会会议论文集：人类语言技术，NAACL-HLT
    2018，美国路易斯安那州新奥尔良，2018年6月1-6日，第1卷（长篇论文）*，第1325–1335页，计算语言学协会。
- en: 'Zhang et al. (2018b) Zhang, Saizheng, Emily Dinan, Jack Urbanek, Arthur Szlam,
    Douwe Kiela, and Jason Weston. 2018b. Personalizing dialogue agents: I have a
    dog, do you have pets too? In *Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018,
    Volume 1: Long Papers*, pages 2204–2213, Association for Computational Linguistics.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018b) Zhang, Saizheng, Emily Dinan, Jack Urbanek, Arthur Szlam,
    Douwe Kiela, and Jason Weston. 2018b. 个性化对话代理：我有一只狗，你也有宠物吗？载于*第56届计算语言学协会年会论文集，ACL
    2018，澳大利亚墨尔本，2018年7月15-20日，第1卷：长篇论文*，第2204–2213页，计算语言学协会。
- en: 'Zhang et al. (2020) Zhang, Tianyi, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In *8th
    International Conference on Learning Representations, ICLR 2020, Addis Ababa,
    Ethiopia, April 26-30, 2020*, OpenReview.net.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020) Zhang, Tianyi, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020. BERTScore：使用BERT评估文本生成。载于*第八届国际表示学习会议，ICLR 2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日*，OpenReview.net。
- en: Zhang and Liu (2013) Zhang, Xu-Yao and Cheng-Lin Liu. 2013. Writer adaptation
    with style transfer mapping. *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, 35(7):1773–1787.
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang and Liu (2013) Zhang, Xu-Yao and Cheng-Lin Liu. 2013. 通过风格转换映射进行作者适应。*IEEE模式分析与机器智能学报*，35(7)：1773–1787。
- en: Zhang, Ge, and Sun (2020) Zhang, Yi, Tao Ge, and Xu Sun. 2020. Parallel data
    augmentation for formality style transfer. In *Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics*, pages 3221–3228, Association
    for Computational Linguistics, Online.
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang, Ge, 和 Sun (2020) Zhang, Yi, Tao Ge, 和 Xu Sun. 2020. 形式风格迁移的并行数据增强。在 *第58届计算语言学协会年会会议录*，第3221–3228页，计算语言学协会，在线.
- en: Zhang et al. (2018c) Zhang, Yi, Jingjing Xu, Pengcheng Yang, and Xu Sun. 2018c.
    Learning sentiment memories for sentiment modification without parallel data.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing, Brussels, Belgium, October 31 - November 4, 2018*, pages 1103–1108,
    Association for Computational Linguistics.
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2018c) Zhang, Yi, Jingjing Xu, Pengcheng Yang, 和 Xu Sun. 2018c. 为情感修改学习情感记忆，无需并行数据。在
    *2018年自然语言处理实证方法会议，布鲁塞尔，比利时，2018年10月31日 - 11月4日*，第1103–1108页，计算语言学协会.
- en: Zhang et al. (2018d) Zhang, Zhirui, Shuo Ren, Shujie Liu, Jianyong Wang, Peng
    Chen, Mu Li, Ming Zhou, and Enhong Chen. 2018d. Style transfer as unsupervised
    machine translation. *CoRR*, abs/1808.07894.
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2018d) Zhang, Zhirui, Shuo Ren, Shujie Liu, Jianyong Wang, Peng Chen,
    Mu Li, Ming Zhou, 和 Enhong Chen. 2018d. 作为无监督机器翻译的风格迁移。*CoRR*, abs/1808.07894.
- en: Zhao et al. (2018) Zhao, Junbo Jake, Yoon Kim, Kelly Zhang, Alexander M. Rush,
    and Yann LeCun. 2018. Adversarially regularized autoencoders. In *Proceedings
    of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
    Stockholm, Sweden, July 10-15, 2018*, volume 80 of *Proceedings of Machine Learning
    Research*, pages 5897–5906, PMLR.
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2018) Zhao, Junbo Jake, Yoon Kim, Kelly Zhang, Alexander M. Rush, 和
    Yann LeCun. 2018. 对抗性正则化自编码器。在 *第35届国际机器学习会议，ICML 2018，斯德哥尔摩，瑞典，2018年7月10-15日*，第80卷
    *机器学习研究会议录*，第5897–5906页，PMLR.
- en: 'Zheng et al. (2019) Zheng, Xu, Tejo Chalasani, Koustav Ghosal, Sebastian Lutz,
    and Aljosa Smolic. 2019. STaDA: Style transfer as data augmentation. In *Proceedings
    of the 14th International Joint Conference on Computer Vision, Imaging and Computer
    Graphics Theory and Applications, VISIGRAPP 2019, Volume 4: VISAPP, Prague, Czech
    Republic, February 25-27, 2019*, pages 107–114, SciTePress.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等 (2019) Zheng, Xu, Tejo Chalasani, Koustav Ghosal, Sebastian Lutz, 和
    Aljosa Smolic. 2019. STaDA：作为数据增强的风格迁移。在 *第14届国际计算机视觉、图像与计算机图形理论与应用联合会议，VISIGRAPP
    2019，第4卷：VISAPP，布拉格，捷克共和国，2019年2月25-27日*，第107–114页，SciTePress.
- en: Zhu et al. (2017) Zhu, Jun-Yan, Taesung Park, Phillip Isola, and Alexei A. Efros.
    2017. Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In *IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy,
    October 22-29, 2017*, pages 2242–2251, IEEE Computer Society.
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 (2017) Zhu, Jun-Yan, Taesung Park, Phillip Isola, 和 Alexei A. Efros. 2017.
    使用循环一致性对抗网络进行无配对图像到图像的转换。在 *IEEE 国际计算机视觉会议，ICCV 2017，威尼斯，意大利，2017年10月22-29日*，第2242–2251页，IEEE
    计算机学会.
- en: Zhu, Bernhard, and Gurevych (2010) Zhu, Zhemin, Delphine Bernhard, and Iryna
    Gurevych. 2010. A monolingual tree-based translation model for sentence simplification.
    In *COLING 2010, 23rd International Conference on Computational Linguistics, Proceedings
    of the Conference, 23-27 August 2010, Beijing, China*, pages 1353–1361, Tsinghua
    University Press.
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu, Bernhard, 和 Gurevych (2010) Zhu, Zhemin, Delphine Bernhard, 和 Iryna Gurevych.
    2010. 用于句子简化的单语树形翻译模型。在 *COLING 2010，第23届计算语言学国际会议，会议论文集，北京，中国，2010年8月23-27日*，第1353–1361页，清华大学出版社.
- en: 'Zue and Glass (2000) Zue, Victor W and James R Glass. 2000. Conversational
    interfaces: Advances and challenges. *Proceedings of the IEEE*, 88(8):1166–1180.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zue 和 Glass (2000) Zue, Victor W 和 James R Glass. 2000. 对话接口：进展与挑战。*IEEE 会议录*,
    88(8):1166–1180.
