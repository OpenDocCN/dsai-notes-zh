- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:01:22'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:01:22
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2004.13408] Time Series Forecasting With Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2004.13408] 时间序列预测与深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2004.13408](https://ar5iv.labs.arxiv.org/html/2004.13408)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2004.13408](https://ar5iv.labs.arxiv.org/html/2004.13408)
- en: \subject
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \subject
- en: Deep learning, time series modelling
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，时间序列建模
- en: \corres
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \corres
- en: Bryan Lim
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Bryan Lim
- en: 'Time Series Forecasting With Deep Learning: A Survey'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列预测与深度学习：综述
- en: Bryan Lim¹ and Stefan Zohren¹ ¹Department of Engineering Science, University
    of Oxford, Oxford, UK [blim@robots.ox.ac.uk](mailto:blim@robots.ox.ac.uk)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Bryan Lim¹ 和 Stefan Zohren¹ ¹牛津大学工程科学系，牛津，英国 [blim@robots.ox.ac.uk](mailto:blim@robots.ox.ac.uk)
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Numerous deep learning architectures have been developed to accommodate the
    diversity of time series datasets across different domains. In this article, we
    survey common encoder and decoder designs used in both one-step-ahead and multi-horizon
    time series forecasting – describing how temporal information is incorporated
    into predictions by each model. Next, we highlight recent developments in hybrid
    deep learning models, which combine well-studied statistical models with neural
    network components to improve pure methods in either category. Lastly, we outline
    some ways in which deep learning can also facilitate decision support with time
    series data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 许多深度学习架构已经被开发出来，以适应不同领域时间序列数据集的多样性。在这篇文章中，我们调查了在一步预测和多步预测中使用的常见编码器和解码器设计——描述了每个模型如何将时间信息纳入预测中。接下来，我们强调了混合深度学习模型的最新发展，这些模型结合了成熟的统计模型与神经网络组件，以提高纯方法在任一类别中的性能。最后，我们概述了深度学习如何利用时间序列数据促进决策支持的一些方式。
- en: 'keywords:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep neural networks, time series forecasting, uncertainty estimation, hybrid
    models, interpretability, counterfactual prediction{fmtext}
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络，时间序列预测，不确定性估计，混合模型，可解释性，对抗预测{fmtext}
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Time series modelling has historically been a key area of academic research
    – forming an integral part of applications in topics such as climate modelling
    [[1](#bib.bib1)], biological sciences [[2](#bib.bib2)] and medicine [[3](#bib.bib3)],
    as well as commercial decision making in retail [[4](#bib.bib4)] and finance [[5](#bib.bib5)]
    to name a few. While traditional methods have focused on parametric models informed
    by domain expertise – such as autoregressive (AR) [[6](#bib.bib6)], exponential
    smoothing [[7](#bib.bib7), [8](#bib.bib8)] or structural time series models [[9](#bib.bib9)]
    – modern machine learning methods provide a means to learn temporal dynamics in
    a purely data-driven manner [[10](#bib.bib10)]. With the increasing data availability
    and computing power in recent times, machine learning has become a vital part
    of the next generation of time series forecasting models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列建模历来是学术研究的一个重要领域——在气候建模 [[1](#bib.bib1)]、生物科学 [[2](#bib.bib2)] 和医学 [[3](#bib.bib3)]
    等主题的应用中，商业决策在零售 [[4](#bib.bib4)] 和金融 [[5](#bib.bib5)] 等领域中也发挥了重要作用。虽然传统方法侧重于由领域知识提供的信息的参数模型——如自回归
    (AR) [[6](#bib.bib6)]、指数平滑 [[7](#bib.bib7), [8](#bib.bib8)] 或结构时间序列模型 [[9](#bib.bib9)]——但现代机器学习方法提供了一种完全数据驱动的方式来学习时间动态
    [[10](#bib.bib10)]。随着数据可用性和计算能力的增加，机器学习已经成为下一代时间序列预测模型的重要组成部分。
- en: Deep learning in particular has gained popularity in recent times, inspired
    by notable achievements in image classification [[11](#bib.bib11)], natural language
    processing [[12](#bib.bib12)] and reinforcement learning [[13](#bib.bib13)]. By
    incorporating bespoke architectural assumptions – or inductive biases [[14](#bib.bib14)]
    – that reflect the nuances of underlying datasets, deep neural networks are able
    to learn complex data representations [[15](#bib.bib15)], which alleviates the
    need for manual feature engineering and model design. The availability of open-source
    backpropagation frameworks [[16](#bib.bib16), [17](#bib.bib17)] has also simplified
    the network training, allowing for the customisation for network components and
    loss functions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习尤其在最近获得了流行，受到图像分类 [[11](#bib.bib11)]、自然语言处理 [[12](#bib.bib12)] 和强化学习 [[13](#bib.bib13)]
    的显著成就的启发。通过引入反映数据集细微差别的定制架构假设——即归纳偏差 [[14](#bib.bib14)]——深度神经网络能够学习复杂的数据表示 [[15](#bib.bib15)]，从而减轻了手动特征工程和模型设计的需求。开放源代码的反向传播框架
    [[16](#bib.bib16), [17](#bib.bib17)] 也简化了网络训练，使得网络组件和损失函数的定制成为可能。
- en: Given the diversity of time-series problems across various domains, numerous
    neural network design choices have emerged. In this article, we summarise the
    common approaches to time series prediction using deep neural networks. Firstly,
    we describe the state-of-the-art techniques available for common forecasting problems
    – such as multi-horizon forecasting and uncertainty estimation. Secondly, we analyse
    the emergence of a new trend in hybrid models, which combine both domain-specific
    quantitative models with deep learning components to improve forecasting performance.
    Next, we outline two key approaches in which neural networks can be used to facilitate
    decision support, specifically through methods in interpretability and counterfactual
    prediction. Finally, we conclude with some promising future research directions
    in deep learning for time series prediction – specifically in the form of continuous-time
    and hierarchical models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于各个领域时间序列问题的多样性，出现了许多神经网络设计选择。在本文中，我们总结了使用深度神经网络进行时间序列预测的常见方法。首先，我们描述了用于常见预测问题的最新技术——例如多视角预测和不确定性估计。其次，我们分析了混合模型新趋势的出现，这些模型将领域特定的定量模型与深度学习组件相结合，以提高预测性能。接下来，我们概述了两种关键方法，通过解释性和反事实预测的方法，神经网络可以用于促进决策支持。最后，我们总结了一些有前景的未来研究方向，特别是在时间序列预测中的深度学习形式，如连续时间模型和层次模型。
- en: While we endeavour to provide a comprehensive overview of modern methods in
    deep learning, we note that our survey is by no means all-encompassing. Indeed,
    a rich body of literature exists for automated approaches to time series forecasting
    - including automatic parametric model selection [[18](#bib.bib18)], and traditional
    machine learning methods such as kernel regression [[19](#bib.bib19)] and support
    vector regression [[20](#bib.bib20)]. In addition, Gaussian processes [[21](#bib.bib21)]
    have been extensively used for time series prediction – with recent extensions
    including deep Gaussian processes [[22](#bib.bib22)], and parallels in deep learning
    via neural processes [[23](#bib.bib23)]. Furthermore, older models of neural networks
    have been used historically in time series applications, as seen in [[24](#bib.bib24)]
    and [[25](#bib.bib25)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们努力提供现代深度学习方法的全面概述，但我们指出我们的综述绝不是包罗万象的。确实，关于时间序列预测的自动化方法存在丰富的文献——包括自动参数模型选择[[18](#bib.bib18)]，以及传统的机器学习方法，如核回归[[19](#bib.bib19)]和支持向量回归[[20](#bib.bib20)]。此外，高斯过程[[21](#bib.bib21)]已经被广泛用于时间序列预测——最近的扩展包括深度高斯过程[[22](#bib.bib22)]，以及通过神经过程[[23](#bib.bib23)]在深度学习中的类比。此外，历史上，旧的神经网络模型也曾用于时间序列应用，如[[24](#bib.bib24)]和[[25](#bib.bib25)]所示。
- en: 2 Deep Learning Architectures for Time Series Forecasting
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度学习架构在时间序列预测中的应用
- en: 'Time series forecasting models predict future values of a target $y_{i,t}$
    for a given entity $i$ at time $t$. Each entity represents a logical grouping
    of temporal information – such as measurements from individual weather stations
    in climatology, or vital signs from different patients in medicine – and can be
    observed at the same time. In the simplest case, one-step-ahead forecasting models
    take the form:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列预测模型用于预测给定实体$i$在时间$t$的目标$y_{i,t}$的未来值。每个实体代表时间信息的逻辑分组——例如气候学中来自个别气象站的测量数据，或医学中来自不同患者的生命体征——这些信息可以在相同时间进行观察。在最简单的情况下，一步前预测模型的形式如下：
- en: '|  | $\hat{y}_{i,t+1}=f(y_{i,t-k:t},\bm{x}_{i,t-k:t},\bm{s}_{i}),$ |  | (1)
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}_{i,t+1}=f(y_{i,t-k:t},\bm{x}_{i,t-k:t},\bm{s}_{i}),$ |  | (1)
    |'
- en: where $\hat{y}_{i,t+1}$ is the model forecast, $y_{i,t-k:t}=\{y_{i,t-k},\dots,y_{i,t}\}$,
    $\bm{x}_{i,t-k:t}=\{\bm{x}_{i,t-k},\dots,\bm{x}_{i,t}\}$ are observations of the
    target and exogenous inputs respectively over a look-back window $k$, $s_{i}$
    is static metadata associated with the entity (e.g. sensor location), and $f(.)$
    is the prediction function learnt by the model. While we focus on univariate forecasting
    in this survey (i.e. 1-D targets), we note that the same components can be extended
    to multivariate models without loss of generality [[26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]. For notational simplicity,
    we omit the entity index $i$ in subsequent sections unless explicitly required.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\hat{y}_{i,t+1}$是模型预测，$y_{i,t-k:t}=\{y_{i,t-k},\dots,y_{i,t}\}$，$\bm{x}_{i,t-k:t}=\{\bm{x}_{i,t-k},\dots,\bm{x}_{i,t}\}$分别是目标和外生输入在回溯窗口$k$上的观测值，$s_{i}$是与实体（例如传感器位置）相关的静态元数据，$f(.)$是模型学习的预测函数。虽然我们在本调查中专注于单变量预测（即1-D目标），但我们注意到这些组件可以扩展到多变量模型而不会丧失一般性[[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]。为了简化符号表示，后续部分将省略实体索引$i$，除非明确要求。
- en: 2.1 Basic Building Blocks
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基本构建块
- en: 'Deep neural networks learn predictive relationships by using a series of non-linear
    layers to construct intermediate feature representations [[15](#bib.bib15)]. In
    time series settings, this can be viewed as encoding relevant historical information
    into a latent variable $\bm{z}_{t}$, with the final forecast produced using $\bm{z}_{t}$
    alone:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络通过使用一系列非线性层来构建中间特征表示，从而学习预测关系[[15](#bib.bib15)]。在时间序列环境中，这可以看作是将相关历史信息编码到潜在变量$\bm{z}_{t}$中，最终预测仅使用$\bm{z}_{t}$生成：
- en: '|  | $\displaystyle f(y_{t-k:t},\bm{x}_{t-k:t},\bm{s})=g_{\mathrm{dec}}(\bm{z}_{t}),$
    |  | (2) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f(y_{t-k:t},\bm{x}_{t-k:t},\bm{s})=g_{\mathrm{dec}}(\bm{z}_{t}),$
    |  | (2) |'
- en: '|  | $\displaystyle\bm{z}_{t}=g_{\mathrm{enc}}(y_{t-k:t},\bm{x}_{t-k:t},\bm{s}),$
    |  | (3) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{z}_{t}=g_{\mathrm{enc}}(y_{t-k:t},\bm{x}_{t-k:t},\bm{s}),$
    |  | (3) |'
- en: 'where $g_{\mathrm{enc}}(.)$, $g_{\mathrm{dec}}(.)$ are encoder and decoder
    functions respectively, and recalling that that subscript $i$ from Equation ([1](#S2.E1
    "In 2 Deep Learning Architectures for Time Series Forecasting ‣ Time Series Forecasting
    With Deep Learning: A Survey")) been removed to simplify notation (e.g. $y_{i,t}$
    replaced by $y_{t}$). These encoders and decoders hence form the basic building
    blocks of deep learning architectures, with the choice of network determining
    the types of relationships that can be learnt by our model. In this section, we
    examine modern design choices for encoders, as overviewed in Figure [1](#S2.F1
    "Figure 1 ‣ 2.1 Basic Building Blocks ‣ 2 Deep Learning Architectures for Time
    Series Forecasting ‣ Time Series Forecasting With Deep Learning: A Survey"), and
    their relationship to traditional temporal models. In addition, we explore common
    network outputs and loss functions used in time series forecasting applications.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$g_{\mathrm{enc}}(.)$、$g_{\mathrm{dec}}(.)$分别是编码器和解码器函数，并且回顾到方程（[1](#S2.E1
    "在2深度学习架构用于时间序列预测 ‣ 时间序列预测与深度学习：综述")）中的下标$i$已被移除以简化符号（例如$y_{i,t}$被替换为$y_{t}$）。这些编码器和解码器因此形成了深度学习架构的基本构建块，网络的选择决定了模型可以学习的关系类型。在本节中，我们将考察现代编码器的设计选择，如图[1](#S2.F1
    "图1 ‣ 2.1 基本构建块 ‣ 2 深度学习架构用于时间序列预测 ‣ 时间序列预测与深度学习：综述")所概述，并探讨其与传统时间模型的关系。此外，我们还将探索时间序列预测应用中常用的网络输出和损失函数。
- en: '![Refer to caption](img/c90b719a626015a8d711197e66130be6.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/c90b719a626015a8d711197e66130be6.png)'
- en: (a) CNN Model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (a) CNN模型。
- en: '![Refer to caption](img/7a38388a9591de23da2b9f5f4b7ca4c3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/7a38388a9591de23da2b9f5f4b7ca4c3.png)'
- en: (b) RNN Model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (b) RNN模型。
- en: '![Refer to caption](img/2849388f6a085cfce1db71ff7b7244cb.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/2849388f6a085cfce1db71ff7b7244cb.png)'
- en: (c) Attention-based Model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 基于注意力的模型。
- en: 'Figure 1: Incorporating temporal information using different encoder architectures.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：使用不同编码器架构整合时间信息。
- en: 2.1.1 Convolutional Neural Networks
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 卷积神经网络
- en: 'Traditionally designed for image datasets, convolutional neural networks (CNNs)
    extract local relationships that are invariant across spatial dimensions [[31](#bib.bib31),
    [11](#bib.bib11)]. To adapt CNNs to time series datasets, researchers utilise
    multiple layers of causal convolutions [[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)]
    – i.e. convolutional filters designed to ensure only past information is used
    for forecasting. For an intermediate feature at hidden layer $l$, each causal
    convolutional filter takes the form below:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）传统上是为图像数据集设计的，提取在空间维度上不变的局部关系 [[31](#bib.bib31), [11](#bib.bib11)]。为了将
    CNN 适应于时间序列数据集，研究人员利用多个层的因果卷积 [[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)]
    —— 即设计为仅使用过去信息进行预测的卷积滤波器。对于隐藏层 $l$ 的中间特征，每个因果卷积滤波器的形式如下：
- en: '|  | $\displaystyle\bm{h}_{t}^{l+1}$ | $\displaystyle=A\bigg{(}\left(\bm{W}*\bm{h}\right)(l,t)\bigg{)},$
    |  | (4) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{h}_{t}^{l+1}$ | $\displaystyle=A\bigg{(}\left(\bm{W}*\bm{h}\right)(l,t)\bigg{)},$
    |  | (4) |'
- en: '|  | $\displaystyle\left(\bm{W}*\bm{h}\right)(l,t)$ | $\displaystyle=\sum_{\tau=0}^{k}\bm{W}(l,\tau)\bm{h}_{t-\tau}^{l},$
    |  | (5) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left(\bm{W}*\bm{h}\right)(l,t)$ | $\displaystyle=\sum_{\tau=0}^{k}\bm{W}(l,\tau)\bm{h}_{t-\tau}^{l},$
    |  | (5) |'
- en: where $\bm{h}_{t}^{l}\in\mathbb{R}^{\mathcal{H}_{in}}$ is an intermediate state
    at layer $l$ at time $t$, $*$ is the convolution operator, $\bm{W}(l,\tau)\in\mathbb{R}^{\mathcal{H}_{out}\times\mathcal{H}_{in}}$
    is a fixed filter weight at layer $l$, and $A(.)$ is an activation function, such
    as a sigmoid function, representing any architecture-specific non-linear processing.
    For CNNs that use a total of L convolutional layers, we note that the encoder
    output is then $\bm{z}_{t}=\bm{h}^{L}_{t}$.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{h}_{t}^{l}\in\mathbb{R}^{\mathcal{H}_{in}}$ 是时间 $t$ 时第 $l$ 层的中间状态，$*$
    是卷积算子，$\bm{W}(l,\tau)\in\mathbb{R}^{\mathcal{H}_{out}\times\mathcal{H}_{in}}$
    是第 $l$ 层的固定滤波器权重，而 $A(.)$ 是激活函数，例如 sigmoid 函数，表示任何架构特定的非线性处理。对于使用总共 $L$ 层卷积的 CNN，我们注意到编码器的输出为
    $\bm{z}_{t}=\bm{h}^{L}_{t}$。
- en: 'Considering the 1-D case, we can see that Equation ([5](#S2.E5 "In 2.1.1 Convolutional
    Neural Networks ‣ 2.1 Basic Building Blocks ‣ 2 Deep Learning Architectures for
    Time Series Forecasting ‣ Time Series Forecasting With Deep Learning: A Survey"))
    bears a strong resemblance to finite impulse response (FIR) filters in digital
    signal processing [[35](#bib.bib35)]. This leads to two key implications for temporal
    relationships learnt by CNNs. Firstly, in line with the spatial invariance assumptions
    for standard CNNs, temporal CNNs assume that relationships are time-invariant
    – using the same set of filter weights at each time step and across all time.
    In addition, CNNs are only able to use inputs within its defined lookback window,
    or receptive field, to make forecasts. As such, the receptive field size $k$ needs
    to be tuned carefully to ensure that the model can make use of all relevant historical
    information. It is worth noting that a single causal CNN layer with a linear activation
    function is equivalent to an auto-regressive (AR) model.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到一维情况，我们可以看到方程式 ([5](#S2.E5 "在 2.1.1 卷积神经网络 ‣ 2.1 基本构建模块 ‣ 2 深度学习架构用于时间序列预测
    ‣ 使用深度学习进行时间序列预测：综述")) 与数字信号处理中的有限冲激响应 (FIR) 滤波器非常相似 [[35](#bib.bib35)]。这对于 CNN
    学习的时间关系有两个关键含义。首先，与标准 CNN 的空间不变性假设一致，时间 CNN 假设关系是时间不变的 —— 在每个时间步和所有时间上使用相同的一组滤波器权重。此外，CNN
    只能使用其定义的回顾窗口或感受野内的输入来进行预测。因此，感受野大小 $k$ 需要仔细调整，以确保模型可以利用所有相关的历史信息。值得注意的是，具有线性激活函数的单层因果
    CNN 等同于自回归（AR）模型。
- en: Dilated Convolutions
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 膨胀卷积
- en: 'Using standard convolutional layers can be computationally challenging where
    long-term dependencies are significant, as the number of parameters scales directly
    with the size of the receptive field. To alleviate this, modern architectures
    frequently make use of dilated covolutional layers [[32](#bib.bib32), [33](#bib.bib33)],
    which extend Equation ([5](#S2.E5 "In 2.1.1 Convolutional Neural Networks ‣ 2.1
    Basic Building Blocks ‣ 2 Deep Learning Architectures for Time Series Forecasting
    ‣ Time Series Forecasting With Deep Learning: A Survey")) as below:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准卷积层在长期依赖关系显著的情况下可能计算上具有挑战性，因为参数数量直接随着感受野的大小增长。为了解决这个问题，现代架构常常使用膨胀卷积层 [[32](#bib.bib32),
    [33](#bib.bib33)]，这扩展了方程式 ([5](#S2.E5 "在 2.1.1 卷积神经网络 ‣ 2.1 基本构建模块 ‣ 2 深度学习架构用于时间序列预测
    ‣ 使用深度学习进行时间序列预测：综述")) 如下：
- en: '|  | $\displaystyle\left(\bm{W}*\bm{h}\right)(l,t,d_{l})$ | $\displaystyle=\sum_{\tau=0}^{\lfloor
    k/d_{l}\rfloor}\bm{W}(l,\tau)\bm{h}_{t-d_{l}\tau}^{l},$ |  | (6) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left(\bm{W}*\bm{h}\right)(l,t,d_{l})$ | $\displaystyle=\sum_{\tau=0}^{\lfloor
    k/d_{l}\rfloor}\bm{W}(l,\tau)\bm{h}_{t-d_{l}\tau}^{l},$ |  | (6) |'
- en: 'where $\lfloor.\rfloor$ is the floor operator and $d_{l}$ is a layer-specific
    dilation rate. Dilated convolutions can hence be interpreted as convolutions of
    a down-sampled version of the lower layer features – reducing resolution to incorporate
    information from the distant past. As such, by increasing the dilation rate with
    each layer, dilated convolutions can gradually aggregate information at different
    time blocks, allowing for more history to be used in an efficient manner. With
    the WaveNet architecture of [[32](#bib.bib32)] for instance, dilation rates are
    increased in powers of 2 with adjacent time blocks aggregated in each layer –
    allowing for $2^{l}$ time steps to be used at layer $l$ as shown in Figure [1a](#S2.F1.sf1
    "In Figure 1 ‣ 2.1 Basic Building Blocks ‣ 2 Deep Learning Architectures for Time
    Series Forecasting ‣ Time Series Forecasting With Deep Learning: A Survey").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lfloor.\rfloor$ 是下取整运算符，$d_{l}$ 是特定层的膨胀率。因此，膨胀卷积可以被解释为对下层特征的下采样版本进行卷积——降低分辨率以整合来自远古过去的信息。因此，通过在每一层中增加膨胀率，膨胀卷积可以逐渐聚合不同时间块的信息，从而以高效的方式利用更多的历史信息。例如，在
    [[32](#bib.bib32)] 的 WaveNet 架构中，膨胀率按 2 的幂次增加，相邻时间块在每一层中被聚合——允许在第 $l$ 层使用 $2^{l}$
    个时间步，如图 [1a](#S2.F1.sf1 "在图 1 ‣ 2.1 基本构建块 ‣ 2 深度学习架构用于时间序列预测 ‣ 使用深度学习进行时间序列预测：综述")
    所示。
- en: 2.1.2 Recurrent Neural Networks
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 循环神经网络
- en: 'Recurrent neural networks (RNNs) have historically been used in sequence modelling
    [[31](#bib.bib31)], with strong results on a variety of natural language processing
    tasks [[36](#bib.bib36)]. Given the natural interpretation of time series data
    as sequences of inputs and targets, many RNN-based architectures have been developed
    for temporal forecasting applications [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40)]. At its core, RNN cells contain an internal memory state which
    acts as a compact summary of past information. The memory state is recursively
    updated with new observations at each time step as shown in Figure [1b](#S2.F1.sf2
    "In Figure 1 ‣ 2.1 Basic Building Blocks ‣ 2 Deep Learning Architectures for Time
    Series Forecasting ‣ Time Series Forecasting With Deep Learning: A Survey"), i.e.:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络 (RNNs) 在序列建模中历史上已经被使用 [[31](#bib.bib31)]，在各种自然语言处理任务中取得了强有力的成果 [[36](#bib.bib36)]。鉴于时间序列数据自然地被解释为输入和目标的序列，许多基于
    RNN 的架构已被开发用于时间预测应用 [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)]。RNN
    单元的核心包含一个内部记忆状态，作为过去信息的紧凑总结。记忆状态随着每个时间步的新观察被递归更新，如图 [1b](#S2.F1.sf2 "在图 1 ‣ 2.1
    基本构建块 ‣ 2 深度学习架构用于时间序列预测 ‣ 使用深度学习进行时间序列预测：综述") 所示，即：
- en: '|  | $\displaystyle\bm{z}_{t}$ | $\displaystyle=\nu\left(\bm{z}_{t-1},y_{t},\bm{x}_{t},\bm{s}\right),$
    |  | (7) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{z}_{t}$ | $\displaystyle=\nu\left(\bm{z}_{t-1},y_{t},\bm{x}_{t},\bm{s}\right),$
    |  | (7) |'
- en: 'Where $\bm{z}_{t}\in\mathbb{R}^{\mathcal{H}}$ here is the hidden internal state
    of the RNN, and $\nu(.)$ is the learnt memory update function. For instance, the
    Elman RNN [[41](#bib.bib41)], one of the simplest RNN variants, would take the
    form below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{z}_{t}\in\mathbb{R}^{\mathcal{H}}$ 是 RNN 的隐藏内部状态，而 $\nu(.)$ 是学习到的记忆更新函数。例如，Elman
    RNN [[41](#bib.bib41)]，作为最简单的 RNN 变体之一，其形式如下：
- en: '|  | $\displaystyle y_{t+1}$ | $\displaystyle=\gamma_{y}(\bm{W}_{y}\bm{z}_{t}+\bm{b}_{y}),$
    |  | (8) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y_{t+1}$ | $\displaystyle=\gamma_{y}(\bm{W}_{y}\bm{z}_{t}+\bm{b}_{y}),$
    |  | (8) |'
- en: '|  | $\displaystyle\bm{z}_{t}$ | $\displaystyle=\gamma_{z}(\bm{W}_{z_{1}}\bm{z}_{t-1}+\bm{W}_{z_{2}}y_{t}+\bm{W}_{z_{3}}\bm{x}_{t}+\bm{W}_{z_{4}}\bm{s}+\bm{b}_{z}),$
    |  | (9) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{z}_{t}$ | $\displaystyle=\gamma_{z}(\bm{W}_{z_{1}}\bm{z}_{t-1}+\bm{W}_{z_{2}}y_{t}+\bm{W}_{z_{3}}\bm{x}_{t}+\bm{W}_{z_{4}}\bm{s}+\bm{b}_{z}),$
    |  | (9) |'
- en: 'Where $\bm{W}_{.},\bm{b}_{.}$ are the linear weights and biases of the network
    respectively, and $\gamma_{y}(.),\gamma_{z}(.)$ are network activation functions.
    Note that RNNs do not require the explicit specification of a lookback window
    as per the CNN case. From a signal processing perspective, the main recurrent
    layer – i.e. Equation ([9](#S2.E9 "In 2.1.2 Recurrent Neural Networks ‣ 2.1 Basic
    Building Blocks ‣ 2 Deep Learning Architectures for Time Series Forecasting ‣
    Time Series Forecasting With Deep Learning: A Survey")) – thus resembles a non-linear
    version of infinite impulse response (IIR) filters.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\bm{W}_{.},\bm{b}_{.}$ 分别是网络的线性权重和偏置，而 $\gamma_{y}(.),\gamma_{z}(.)$ 是网络激活函数。注意，RNN
    不需要像 CNN 那样显式指定回溯窗口。从信号处理的角度来看，主要的递归层 —— 即公式 ([9](#S2.E9 "In 2.1.2 Recurrent Neural
    Networks ‣ 2.1 Basic Building Blocks ‣ 2 Deep Learning Architectures for Time
    Series Forecasting ‣ Time Series Forecasting With Deep Learning: A Survey")) ——
    因此类似于无限脉冲响应（IIR）滤波器的非线性版本。'
- en: Long Short-term Memory
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 长短期记忆
- en: 'Due to the infinite lookback window, older variants of RNNs can suffer from
    limitations in learning long-range dependencies in the data [[42](#bib.bib42),
    [43](#bib.bib43)] – due to issues with exploding and vanishing gradients [[31](#bib.bib31)].
    Intuitively, this can be seen as a form of resonance in the memory state. Long
    Short-Term Memory networks (LSTMs) [[44](#bib.bib44)] were hence developed to
    address these limitations, by improving gradient flow within the network. This
    is achieved through the use of a cell state $\bm{c}_{t}$ which stores long-term
    information, modulated through a series of gates as below:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无限回溯窗口，RNN 的旧变体在学习数据中的长期依赖性时可能会受到限制 [[42](#bib.bib42), [43](#bib.bib43)] ——
    这与梯度爆炸和消失的问题有关 [[31](#bib.bib31)]。直观地，这可以被视为记忆状态中的一种共鸣。长短期记忆网络（LSTMs）[[44](#bib.bib44)]
    因此被开发出来以解决这些限制，通过改善网络内部的梯度流。这是通过使用一个存储长期信息的单元状态 $\bm{c}_{t}$ 实现的，该状态通过一系列门控机制进行调节，如下所示：
- en: '|  | Input gate: | $\displaystyle\bm{i}_{t}$ | $\displaystyle=\sigma(\bm{W}_{i_{1}}\bm{z}_{t-1}+\bm{W}_{i_{2}}y_{t}+\bm{W}_{i_{3}}\bm{x}_{t}+\bm{W}_{i_{4}}\bm{s}+\bm{b}_{i}),$
    |  | (10) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入门： | $\displaystyle\bm{i}_{t}$ | $\displaystyle=\sigma(\bm{W}_{i_{1}}\bm{z}_{t-1}+\bm{W}_{i_{2}}y_{t}+\bm{W}_{i_{3}}\bm{x}_{t}+\bm{W}_{i_{4}}\bm{s}+\bm{b}_{i}),$
    |  | (10) |'
- en: '|  | Output gate: | $\displaystyle\bm{o}_{t}$ | $\displaystyle=\sigma(\bm{W}_{o_{1}}\bm{z}_{t-1}+\bm{W}_{o_{2}}y_{t}+\bm{W}_{o_{3}}\bm{x}_{t}+\bm{W}_{o_{4}}\bm{s}+\bm{b}_{o}),$
    |  | (11) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | 输出门： | $\displaystyle\bm{o}_{t}$ | $\displaystyle=\sigma(\bm{W}_{o_{1}}\bm{z}_{t-1}+\bm{W}_{o_{2}}y_{t}+\bm{W}_{o_{3}}\bm{x}_{t}+\bm{W}_{o_{4}}\bm{s}+\bm{b}_{o}),$
    |  | (11) |'
- en: '|  | Forget gate: | $\displaystyle\bm{f}_{t}$ | $\displaystyle=\sigma(\bm{W}_{f_{1}}\bm{z}_{t-1}+\bm{W}_{f_{2}}y_{t}+\bm{W}_{f_{3}}\bm{x}_{t}+\bm{W}_{f_{4}}\bm{s}+\bm{b}_{f}),$
    |  | (12) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | 遗忘门： | $\displaystyle\bm{f}_{t}$ | $\displaystyle=\sigma(\bm{W}_{f_{1}}\bm{z}_{t-1}+\bm{W}_{f_{2}}y_{t}+\bm{W}_{f_{3}}\bm{x}_{t}+\bm{W}_{f_{4}}\bm{s}+\bm{b}_{f}),$
    |  | (12) |'
- en: 'where $\bm{z}_{t-1}$ is the hidden state of the LSTM, and $\sigma(.)$ is the
    sigmoid activation function. The gates modify the hidden and cell states of the
    LSTM as below:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{z}_{t-1}$ 是 LSTM 的隐藏状态，$\sigma(.)$ 是 sigmoid 激活函数。门控机制修改 LSTM 的隐藏状态和单元状态如下：
- en: '|  | Hidden state: | $\displaystyle\bm{z}_{t}$ | $\displaystyle=\bm{o}_{t}\odot\text{tanh}(\bm{c}_{t}),$
    |  | (13) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | 隐藏状态： | $\displaystyle\bm{z}_{t}$ | $\displaystyle=\bm{o}_{t}\odot\text{tanh}(\bm{c}_{t}),$
    |  | (13) |'
- en: '|  | Cell state: | $\displaystyle\bm{c}_{t}$ | $\displaystyle=\bm{f}_{t}\odot\bm{c}_{t-1}$
    |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | 单元状态： | $\displaystyle\bm{c}_{t}$ | $\displaystyle=\bm{f}_{t}\odot\bm{c}_{t-1}$
    |  |'
- en: '|  | $\displaystyle+\bm{i}_{t}\odot\text{tanh}(\bm{W}_{c_{1}}\bm{z}_{t-1}+\bm{W}_{c_{2}}y_{t}+\bm{W}_{c_{3}}\bm{x}_{t}+\bm{W}_{c_{4}}\bm{s}+\bm{b}_{c}),$
    |  | (14) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+\bm{i}_{t}\odot\text{tanh}(\bm{W}_{c_{1}}\bm{z}_{t-1}+\bm{W}_{c_{2}}y_{t}+\bm{W}_{c_{3}}\bm{x}_{t}+\bm{W}_{c_{4}}\bm{s}+\bm{b}_{c}),$
    |  | (14) |'
- en: Where $\odot$ is the element-wise (Hadamard) product, and $\text{tanh}(.)$ is
    the tanh activation function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\odot$ 是逐元素（Hadamard）乘积，$\text{tanh}(.)$ 是 tanh 激活函数。
- en: Relationship to Bayesian Filtering
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与贝叶斯过滤的关系
- en: As examined in [[39](#bib.bib39)], Bayesian filters [[45](#bib.bib45)] and RNNs
    are both similar in their maintenance of a hidden state which is recursively updated
    over time. For Bayesian filters, such as the Kalman filter [[46](#bib.bib46)],
    inference is performed by updating the sufficient statistics of the latent state
    – using a series of state transition and error correction steps. As the Bayesian
    filtering steps use deterministic equations to modify sufficient statistics, the
    RNN can be viewed as a simultaneous approximation of both steps – with the memory
    vector containing all relevant information required for prediction.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [[39](#bib.bib39)] 中所述，贝叶斯滤波器[[45](#bib.bib45)] 和 RNN 在维护一个被递归更新的隐藏状态方面是相似的。对于贝叶斯滤波器，如卡尔曼滤波器
    [[46](#bib.bib46)]，推断通过更新潜在状态的充分统计来执行——使用一系列状态转移和误差修正步骤。由于贝叶斯滤波步骤使用确定性方程来修改充分统计，RNN
    可以被视为对这两个步骤的同时近似——其中内存向量包含了进行预测所需的所有相关信息。
- en: 2.1.3 Attention Mechanisms
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 注意力机制
- en: 'The development of attention mechanisms [[47](#bib.bib47), [48](#bib.bib48)]
    has also lead to improvements in long-term dependency learning – with Transformer
    architectures achieving state-of-the-art performance in multiple natural language
    processing applications [[49](#bib.bib49), [50](#bib.bib50), [12](#bib.bib12)].
    Attention layers aggregate temporal features using dynamically generated weights
    (see Figure [1c](#S2.F1.sf3 "In Figure 1 ‣ 2.1 Basic Building Blocks ‣ 2 Deep
    Learning Architectures for Time Series Forecasting ‣ Time Series Forecasting With
    Deep Learning: A Survey")), allowing the network to directly focus on significant
    time steps in the past – even if they are very far back in the lookback window.
    Conceptually, attention is a mechanism for a key-value lookup based on a given
    query [[51](#bib.bib51)], taking the form below:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的开发[[47](#bib.bib47), [48](#bib.bib48)]也促进了长期依赖学习的改进——变换器架构在多个自然语言处理应用中实现了最先进的性能[[49](#bib.bib49),
    [50](#bib.bib50), [12](#bib.bib12)]。注意力层使用动态生成的权重来聚合时间特征（见图 [1c](#S2.F1.sf3 "图1
    ‣ 2.1 基本构建块 ‣ 2 深度学习架构用于时间序列预测 ‣ 深度学习时间序列预测：综述")），使得网络能够直接关注过去的关键时间步——即使这些时间步在回溯窗口中距离很远。从概念上讲，注意力机制是一种基于给定查询的键值查找机制[[51](#bib.bib51)]，其形式如下：
- en: '|  | $\displaystyle\bm{h}_{t}=\sum_{\tau=0}^{k}\alpha(\bm{\kappa}_{t},\bm{q}_{\tau})\bm{v}_{t-\tau},$
    |  | (15) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{h}_{t}=\sum_{\tau=0}^{k}\alpha(\bm{\kappa}_{t},\bm{q}_{\tau})\bm{v}_{t-\tau},$
    |  | (15) |'
- en: Where the key $\bm{\kappa}_{t}$, query $\bm{q}_{\tau}$ and value $\bm{v}_{t-\tau}$
    are intermediate features produced at different time steps by lower levels of
    the network. Furthermore, $\alpha(\bm{\kappa}_{t},\bm{q}_{\tau})\in[0,1]$ is the
    attention weight for $t-\tau$ generated at time $t$, and $\bm{h}_{t}$ is the context
    vector output of the attention layer. Note that multiple attention layers can
    also be used together as per the CNN case, with the output from the final layer
    forming the encoded latent variable $\bm{z}_{t}$.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中键 $\bm{\kappa}_{t}$、查询 $\bm{q}_{\tau}$ 和值 $\bm{v}_{t-\tau}$ 是由网络的较低层在不同时间步生成的中间特征。此外，$\alpha(\bm{\kappa}_{t},\bm{q}_{\tau})\in[0,1]$
    是在时间 $t$ 生成的 $t-\tau$ 的注意力权重，而 $\bm{h}_{t}$ 是注意力层的上下文向量输出。请注意，多个注意力层也可以像 CNN 的情况一样一起使用，最终层的输出形成编码的潜变量
    $\bm{z}_{t}$。
- en: 'Recent work has also demonstrated the benefits of using attention mechanisms
    in time series forecasting applications, with improved performance over comparable
    recurrent networks [[52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54)]. For
    instance, [[52](#bib.bib52)] use attention to aggregate features extracted by
    RNN encoders, with attention weights produced as below:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究还展示了在时间序列预测应用中使用注意力机制的好处，相比于类似的递归网络有了改进的性能[[52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54)]。例如，[[52](#bib.bib52)] 使用注意力来聚合由 RNN 编码器提取的特征，注意力权重的生成如下：
- en: '|  | $\displaystyle\bm{\alpha}(t)$ | $\displaystyle=\text{softmax}(\bm{\eta}_{t}),$
    |  | (16) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{\alpha}(t)$ | $\displaystyle=\text{softmax}(\bm{\eta}_{t}),$
    |  | (16) |'
- en: '|  | $\displaystyle\bm{\eta}_{t}$ | $\displaystyle=\mathbf{W}_{\eta_{1}}\text{tanh}(\mathbf{W}_{\eta_{2}}\bm{\kappa}_{t-1}+\mathbf{W}_{\eta_{3}}\bm{q}_{\tau}+\bm{b}_{\eta}),$
    |  | (17) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{\eta}_{t}$ | $\displaystyle=\mathbf{W}_{\eta_{1}}\text{tanh}(\mathbf{W}_{\eta_{2}}\bm{\kappa}_{t-1}+\mathbf{W}_{\eta_{3}}\bm{q}_{\tau}+\bm{b}_{\eta}),$
    |  | (17) |'
- en: where $\bm{\alpha}(t)=[\alpha(t,0),\dots\alpha(t,k)]$ is a vector of attention
    weights, $\bm{\kappa}_{t-1},\bm{q}_{t}$ are outputs from LSTM encoders used for
    feature extraction, and $\text{softmax}(.)$ is the softmax activation function.
    More recently, Transformer architectures have also been considered in [[53](#bib.bib53),
    [54](#bib.bib54)], which apply scalar-dot product self-attention [[49](#bib.bib49)]
    to features extracted within the lookback window. From a time series modelling
    perspective, attention provides two key benefits. Firstly, networks with attention
    are able to directly attend to any significant events that occur. In retail forecasting
    applications, for example, this includes holiday or promotional periods which
    can have a positive effect on sales. Secondly, as shown in [[54](#bib.bib54)],
    attention-based networks can also learn regime-specific temporal dynamics – by
    using distinct attention weight patterns for each regime.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{\alpha}(t)=[\alpha(t,0),\dots\alpha(t,k)]$ 是注意力权重向量，$\bm{\kappa}_{t-1},\bm{q}_{t}$
    是用于特征提取的 LSTM 编码器的输出，$\text{softmax}(.)$ 是 softmax 激活函数。最近，Transformer 架构也被考虑在内
    [[53](#bib.bib53), [54](#bib.bib54)]，它将标量点积自注意力 [[49](#bib.bib49)] 应用到回顾窗口内提取的特征上。从时间序列建模的角度来看，注意力提供了两个关键好处。首先，具有注意力的网络能够直接关注任何重要事件的发生。例如，在零售预测应用中，这包括可能对销售产生积极影响的节假日或促销期。其次，如
    [[54](#bib.bib54)] 所示，基于注意力的网络还可以学习特定阶段的时间动态——通过对每个阶段使用不同的注意力权重模式。
- en: 2.1.4 Outputs and Loss Functions
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 输出和损失函数
- en: 'Given the flexibility of neural networks, deep neural networks have been used
    to model both discrete [[55](#bib.bib55)] and continuous [[37](#bib.bib37), [56](#bib.bib56)]
    targets – by customising of decoder and output layer of the neural network to
    match the desired target type. In one-step-ahead prediction problems, this can
    be as simple as combining a linear transformation of encoder outputs (i.e. Equation
    ([2](#S2.E2 "In 2.1 Basic Building Blocks ‣ 2 Deep Learning Architectures for
    Time Series Forecasting ‣ Time Series Forecasting With Deep Learning: A Survey")))
    together with an appropriate output activation for the target. Regardless of the
    form of the target, predictions can be further divided into two different categories
    – point estimates and probabilistic forecasts.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于神经网络的灵活性，深度神经网络已被用于建模离散 [[55](#bib.bib55)] 和连续 [[37](#bib.bib37), [56](#bib.bib56)]
    目标——通过自定义神经网络的解码器和输出层以匹配所需的目标类型。在一步前预测问题中，这可以简单地将编码器输出的线性变换（即公式 ([2](#S2.E2 "在
    2.1 基本构建模块 ‣ 2 时间序列预测的深度学习架构 ‣ 基于深度学习的时间序列预测：综述"))) 与适当的目标输出激活结合起来。不论目标形式如何，预测可以进一步分为两种不同的类别——点估计和概率预测。
- en: Point Estimates
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 点估计
- en: 'A common approach to forecasting is to determine the expected value of a future
    target. This essentially involves reformulating the problem to a classification
    task for discrete outputs (e.g. forecasting future events), and regression task
    for continuous outputs – using the encoders described above. For the binary classification
    case, the final layer of the decoder then features a linear layer with a sigmoid
    activation function – allowing the network to predict the probability of event
    occurrence at a given time step. For one-step-ahead forecasts of binary and continuous
    targets, networks are trained using binary cross-entropy and mean square error
    loss functions respectively:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的预测方法是确定未来目标的期望值。这本质上涉及将问题重新表述为离散输出的分类任务（例如预测未来事件），以及连续输出的回归任务——使用上述描述的编码器。对于二分类情况，解码器的最终层通常具有一个线性层和一个
    sigmoid 激活函数——使网络能够预测在给定时间步上事件发生的概率。对于二分类和连续目标的一步前预测，网络分别使用二元交叉熵和均方误差损失函数进行训练：
- en: '|  | $\displaystyle\mathcal{L}_{classification}$ | $\displaystyle=-\frac{1}{T}\sum_{t=1}^{T}y_{t}\log(\hat{y}_{t})+(1-y_{t})\log(1-\hat{y}_{t})$
    |  | (18) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{classification}$ | $\displaystyle=-\frac{1}{T}\sum_{t=1}^{T}y_{t}\log(\hat{y}_{t})+(1-y_{t})\log(1-\hat{y}_{t})$
    |  | (18) |'
- en: '|  | $\displaystyle\mathcal{L}_{regression}$ | $\displaystyle=\frac{1}{T}\sum_{t=1}^{T}\left(y_{t}-\hat{y}_{t}\right)^{2}$
    |  | (19) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{regression}$ | $\displaystyle=\frac{1}{T}\sum_{t=1}^{T}\left(y_{t}-\hat{y}_{t}\right)^{2}$
    |  | (19) |'
- en: While the loss functions above are the most common across applications, we note
    that the flexibility of neural networks also allows for more complex losses to
    be adopted - e.g. losses for quantile regression [[56](#bib.bib56)] and multinomial
    classification [[32](#bib.bib32)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述损失函数是应用中最常见的，但我们注意到神经网络的灵活性也允许采用更复杂的损失——例如，分位数回归的损失 [[56](#bib.bib56)] 和多项式分类的损失
    [[32](#bib.bib32)]。
- en: Probabilistic Outputs
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 概率输出
- en: While point estimates are crucial to predicting the future value of a target,
    understanding the uncertainty of a model’s forecast can be useful for decision
    makers in different domains. When forecast uncertainties are wide, for instance,
    model users can exercise more caution when incorporating predictions into their
    decision making, or alternatively rely on other sources of information. In some
    applications, such as financial risk management, having access to the full predictive
    distribution will allow decision makers to optimise their actions in the presence
    of rare events – e.g. allowing risk managers to insulate portfolios against market
    crashes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然点估计对预测目标的未来值至关重要，但理解模型预测的不确定性对于不同领域的决策者也是有用的。例如，当预测不确定性很大时，模型用户可以在将预测纳入决策时更加谨慎，或依赖其他信息来源。在一些应用中，如金融风险管理，能够获取完整的预测分布将允许决策者在面对稀有事件时优化其行动——例如，允许风险管理者保护投资组合免受市场崩盘的影响。
- en: 'A common way to model uncertainties is to use deep neural networks to generate
    parameters of known distributions [[37](#bib.bib37), [27](#bib.bib27), [38](#bib.bib38)].
    For example, Gaussian distributions are typically used for forecasting problems
    with continuous targets, with the networks outputting means and variance parameters
    for the predictive distributions at each step as below:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的不确定性建模方法是使用深度神经网络生成已知分布的参数 [[37](#bib.bib37), [27](#bib.bib27), [38](#bib.bib38)]。例如，高斯分布通常用于具有连续目标的预测问题，网络输出每一步预测分布的均值和方差参数，如下所示：
- en: '|  | $\displaystyle y_{t+\tau}$ | $\displaystyle\sim N(\mu(t,\tau),\zeta(t,\tau)^{2}),$
    |  | (20) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y_{t+\tau}$ | $\displaystyle\sim N(\mu(t,\tau),\zeta(t,\tau)^{2}),$
    |  | (20) |'
- en: '|  | $\displaystyle\mu(t,\tau)$ | $\displaystyle=\bm{W}_{\mu}\bm{h}^{L}_{t}+\bm{b}_{\mu},$
    |  | (21) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mu(t,\tau)$ | $\displaystyle=\bm{W}_{\mu}\bm{h}^{L}_{t}+\bm{b}_{\mu},$
    |  | (21) |'
- en: '|  | $\displaystyle\zeta(t,\tau)$ | $\displaystyle=\text{softplus}(\bm{W}_{\Sigma}\bm{h}^{L}_{t}+\bm{b}_{\Sigma}),$
    |  | (22) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\zeta(t,\tau)$ | $\displaystyle=\text{softplus}(\bm{W}_{\Sigma}\bm{h}^{L}_{t}+\bm{b}_{\Sigma}),$
    |  | (22) |'
- en: where $\bm{h}^{L}_{t}$ is the final layer of the network, and $\text{softplus}(.)$
    is the softplus activation function to ensure that standard deviations take only
    positive values.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{h}^{L}_{t}$ 是网络的最后一层，$\text{softplus}(.)$ 是softplus激活函数，用于确保标准差仅取正值。
- en: 2.2 Multi-horizon Forecasting Models
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 多时间段预测模型
- en: 'In many applications, it is often beneficial to have access to predictive estimates
    at multiple points in the future – allowing decision makers to visualise trends
    over a future horizon, and optimise their actions across the entire path. From
    a statistical perspective, multi-horizon forecasting can be viewed as a slight
    modification of one-step-ahead prediction problem (i.e. Equation ([1](#S2.E1 "In
    2 Deep Learning Architectures for Time Series Forecasting ‣ Time Series Forecasting
    With Deep Learning: A Survey"))) as below:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用中，能够在未来多个时间点获取预测估计值通常是有益的——这允许决策者可视化未来视野中的趋势，并优化其整个路径上的行动。从统计学角度来看，多时间段预测可以被视为对一步预测问题的轻微修改（即公式
    ([1](#S2.E1 "在2深度学习架构用于时间序列预测 ‣ 使用深度学习的时间序列预测：调查"))）如下：
- en: '|  | $\hat{y}_{t+\tau}=f(y_{t-k:t},\bm{x}_{t-k:t},\bm{u}_{t-k:t+\tau},\bm{s},\tau),$
    |  | (23) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}_{t+\tau}=f(y_{t-k:t},\bm{x}_{t-k:t},\bm{u}_{t-k:t+\tau},\bm{s},\tau),$
    |  | (23) |'
- en: 'where $\tau\in\{1,\dots,\tau_{max}\}$ is a discrete forecast horizon, $\bm{u}_{t}$
    are known future inputs (e.g. date information, such as the day-of-week or month)
    across the entire horizon, and $\bm{x}_{t}$ are inputs that can only be observed
    historically. In line with traditional econometric approaches [[57](#bib.bib57),
    [58](#bib.bib58)], deep learning architectures for multi-horizon forecasting can
    be divided into iterative and direct methods – as shown in Figure [2](#S2.F2 "Figure
    2 ‣ 2.2 Multi-horizon Forecasting Models ‣ 2 Deep Learning Architectures for Time
    Series Forecasting ‣ Time Series Forecasting With Deep Learning: A Survey") and
    described in detail below.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\tau\in\{1,\dots,\tau_{max}\}$是离散预测视角，$\bm{u}_{t}$是已知的未来输入（例如日期信息，如星期几或月份）在整个视角内，而$\bm{x}_{t}$是只能从历史上观察到的输入。根据传统计量经济学方法[[57](#bib.bib57),
    [58](#bib.bib58)]，用于多视角预测的深度学习架构可以分为迭代方法和直接方法 – 如图 [2](#S2.F2 "图 2 ‣ 2.2 多视角预测模型
    ‣ 2 深度学习架构用于时间序列预测 ‣ 使用深度学习进行时间序列预测：综述")所示，并在下文中详细描述。
- en: '![Refer to caption](img/1a613dea6663e3ad33ef0a84a246b51b.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1a613dea6663e3ad33ef0a84a246b51b.png)'
- en: (a) Iterative Methods
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 迭代方法
- en: '![Refer to caption](img/83682e95608997deed2b716f2b759a8f.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/83682e95608997deed2b716f2b759a8f.png)'
- en: (b) Direct Methods
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 直接方法
- en: 'Figure 2: Main types of multi-horizon forecasting models. Colours used to distinguish
    between model weights – with iterative models using a common model across the
    entire horizon and direct methods taking a sequence-to-sequence approach.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：主要的多视角预测模型类型。颜色用于区分模型权重 – 迭代模型在整个预测范围内使用一个共同模型，而直接方法采用序列到序列的方法。
- en: 2.2.1 Iterative Methods
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 迭代方法
- en: 'Iterative approaches to multi-horizon forecasting typically make use of autoregressive
    deep learning architectures [[37](#bib.bib37), [39](#bib.bib39), [53](#bib.bib53),
    [40](#bib.bib40)] – producing multi-horizon forecasts by recursively feeding samples
    of the target into future time steps (see Figure [2a](#S2.F2.sf1 "In Figure 2
    ‣ 2.2 Multi-horizon Forecasting Models ‣ 2 Deep Learning Architectures for Time
    Series Forecasting ‣ Time Series Forecasting With Deep Learning: A Survey")).
    By repeating the procedure to generate multiple trajectories, forecasts are then
    produced using the sampling distributions for target values at each step. For
    instance, predictive means can be obtained using the Monte Carlo estimate $\hat{y}_{t+\tau}=\sum_{j=1}^{J}\tilde{y}_{t+\tau}^{(j)}/J$,
    where $\tilde{y}_{t+\tau}^{(j)}$ is a sample taken based on the model of Equation
    ([20](#S2.E20 "In Probabilistic Outputs ‣ 2.1.4 Outputs and Loss Functions ‣ 2.1
    Basic Building Blocks ‣ 2 Deep Learning Architectures for Time Series Forecasting
    ‣ Time Series Forecasting With Deep Learning: A Survey")). As autoregressive models
    are trained in the exact same fashion as one-step-ahead prediction models (i.e.
    via backpropagation through time), the iterative approach allows for the easy
    generalisation of standard models to multi-step forecasting. However, as a small
    amount of error is produced at each time step, the recursive structure of iterative
    methods can potentially lead to large error accumulations over longer forecasting
    horizons. In addition, iterative methods assume that all inputs but the target
    are known at run-time – requiring only samples of the target to be fed into future
    time steps. This can be a limitation in many practical scenarios where observed
    inputs exist, motivating the need for more flexible methods.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角预测的迭代方法通常使用自回归深度学习架构[[37](#bib.bib37), [39](#bib.bib39), [53](#bib.bib53),
    [40](#bib.bib40)] – 通过递归地将目标样本输入未来时间步来生成多视角预测（见图 [2a](#S2.F2.sf1 "图 2 ‣ 2.2 多视角预测模型
    ‣ 2 深度学习架构用于时间序列预测 ‣ 使用深度学习进行时间序列预测：综述")）。通过重复这一过程生成多个轨迹，预测是使用每一步目标值的采样分布来产生的。例如，预测均值可以通过蒙特卡罗估计获得$\hat{y}_{t+\tau}=\sum_{j=1}^{J}\tilde{y}_{t+\tau}^{(j)}/J$，其中$\tilde{y}_{t+\tau}^{(j)}$是基于方程([20](#S2.E20
    "在概率输出 ‣ 2.1.4 输出和损失函数 ‣ 2.1 基本构建块 ‣ 2 深度学习架构用于时间序列预测 ‣ 使用深度学习进行时间序列预测：综述"))的模型取样。由于自回归模型与一步预测模型（即通过时间反向传播）以完全相同的方式进行训练，因此迭代方法允许将标准模型轻松地推广到多步预测。然而，由于每个时间步都会产生少量的误差，迭代方法的递归结构可能会导致较长预测视角上的误差积累。此外，迭代方法假设除了目标外的所有输入在运行时都是已知的
    – 只需将目标的样本输入未来时间步。这在许多实际场景中可能是一个限制，因为存在观察到的输入，这促使了对更灵活方法的需求。
- en: 2.2.2 Direct Methods
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 直接方法
- en: 'Direct methods alleviate the issues with iterative methods by producing forecasts
    directly using all available inputs. They typically make use of sequence-to-sequence
    architectures [[56](#bib.bib56), [52](#bib.bib52), [54](#bib.bib54)], using an
    encoder to summarise past information (i.e. targets, observed inputs and a priori
    known inputs), and a decoder to combine them with known future inputs – as depicted
    in Figure [2b](#S2.F2.sf2 "In Figure 2 ‣ 2.2 Multi-horizon Forecasting Models
    ‣ 2 Deep Learning Architectures for Time Series Forecasting ‣ Time Series Forecasting
    With Deep Learning: A Survey"). As described in [[59](#bib.bib59)], alternative
    approach is to use simpler models to directly produce a fixed-length vector matching
    the desired forecast horizon. This, however, does require the specification of
    a maximum forecast horizon (i.e. $\tau_{max}$), with predictions made only at
    the predefined discrete intervals.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 直接方法通过直接利用所有可用输入来产生预测，从而缓解了迭代方法的问题。它们通常使用序列到序列的架构 [[56](#bib.bib56), [52](#bib.bib52),
    [54](#bib.bib54)]，利用编码器总结过去的信息（即目标、观测输入和先验已知输入），以及解码器将其与已知的未来输入结合起来——如图[2b](#S2.F2.sf2
    "在图 2 ‣ 2.2 多视角预测模型 ‣ 2 深度学习架构用于时间序列预测 ‣ 基于深度学习的时间序列预测：综述")所示。正如[[59](#bib.bib59)]中所描述的，另一种方法是使用更简单的模型直接生成一个固定长度的向量，以匹配所需的预测视角。然而，这确实要求指定一个最大预测视角（即$\tau_{max}$），并且仅在预定义的离散时间点进行预测。
- en: 3 Incorporating Domain Knowledge with Hybrid Models
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 融合领域知识与混合模型
- en: Despite its popularity, the efficacy of machine learning for time series prediction
    has historically been questioned – as evidenced by forecasting competitions such
    as the M-competitions [[60](#bib.bib60)]. Prior to the M4 competition of 2018
    [[61](#bib.bib61)], the prevailing wisdom was that sophisticated methods do not
    produce more accurate forecasts, and simple models with ensembling had a tendency
    to do better [[62](#bib.bib62), [63](#bib.bib63), [59](#bib.bib59)]. Two key reasons
    have been identified to explain the underperformance of machine learning methods.
    Firstly, the flexibility of machine learning methods can be a double-edged sword
    – making them prone to overfitting [[59](#bib.bib59)]. Hence, simpler models may
    potentially do better in low data regimes, which are particularly common in forecasting
    problems with a small number of historical observations (e.g. quarterly macroeconomic
    forecasts). Secondly, similar to stationarity requirements of statistical models,
    machine learning models can be sensitive to how inputs are pre-processed [[37](#bib.bib37),
    [26](#bib.bib26), [59](#bib.bib59)], which ensure that data distributions at training
    and test time are similar.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其受欢迎程度很高，但历史上对机器学习在时间序列预测中的有效性提出了质疑——如M竞赛 [[60](#bib.bib60)] 所示。在2018年的M4竞赛
    [[61](#bib.bib61)] 之前，普遍认为复杂的方法并不能产生更准确的预测，而使用集成的简单模型往往效果更佳 [[62](#bib.bib62),
    [63](#bib.bib63), [59](#bib.bib59)]。已确定两个关键原因来解释机器学习方法表现不佳的原因。首先，机器学习方法的灵活性可能是一把双刃剑——使其容易过拟合
    [[59](#bib.bib59)]。因此，在数据量少的情况下，简单模型可能会表现更好，这在具有少量历史观测的预测问题中尤其常见（例如季度宏观经济预测）。其次，与统计模型的平稳性要求类似，机器学习模型对输入数据的预处理非常敏感
    [[37](#bib.bib37), [26](#bib.bib26), [59](#bib.bib59)]，确保训练和测试时的数据分布相似。
- en: 'A recent trend in deep learning has been in developing hybrid models which
    address these limitations, demonstrating improved performance over pure statistical
    or machine learning models in a variety of applications [[64](#bib.bib64), [38](#bib.bib38),
    [65](#bib.bib65), [66](#bib.bib66)]. Hybrid methods combine well-studied quantitative
    time series models together with deep learning – using deep neural networks to
    generate model parameters at each time step. On the one hand, hybrid models allow
    domain experts to inform neural network training using prior information – reducing
    the hypothesis space of the network and improving generalisation. This is especially
    useful for small datasets [[38](#bib.bib38)], where there is a greater risk of
    overfitting for deep learning models. Furthermore, hybrid models allow for the
    separation of stationary and non-stationary components, and avoid the need for
    custom input pre-processing. An example of this is the Exponential Smoothing RNN
    (ES-RNN) [[64](#bib.bib64)], winner of the M4 competition, which uses exponential
    smoothing to capture non-stationary trends and learns additional effects with
    the RNN. In general, hybrid models utilise deep neural networks in two manners:
    a) to encode time-varying parameters for non-probabilistic parametric models [[64](#bib.bib64),
    [65](#bib.bib65), [67](#bib.bib67)], and b) to produce parameters of distributions
    used by probabilistic models [[38](#bib.bib38), [40](#bib.bib40), [66](#bib.bib66)].'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习中的一个趋势是开发混合模型来解决这些限制，在各种应用中表现出优于纯统计或机器学习模型的改进性能 [[64](#bib.bib64), [38](#bib.bib38),
    [65](#bib.bib65), [66](#bib.bib66)]。混合方法将经过充分研究的定量时间序列模型与深度学习结合——使用深度神经网络在每个时间步生成模型参数。一方面，混合模型允许领域专家使用先前信息来指导神经网络训练——减少网络的假设空间并提高泛化能力。这对于小数据集尤为有用
    [[38](#bib.bib38)]，因为在深度学习模型中有更大的过拟合风险。此外，混合模型允许将平稳和非平稳分量分开，并避免了自定义输入预处理的需求。一个例子是获胜于
    M4 比赛的指数平滑 RNN（ES-RNN） [[64](#bib.bib64)]，它使用指数平滑捕捉非平稳趋势，并与 RNN 学习附加效应。一般而言，混合模型以两种方式利用深度神经网络：a）对非概率参数模型进行时间变化参数编码
    [[64](#bib.bib64), [65](#bib.bib65), [67](#bib.bib67)]，和 b）为概率模型生成分布参数 [[38](#bib.bib38),
    [40](#bib.bib40), [66](#bib.bib66)]。
- en: 3.1 Non-probabilistic Hybrid Models
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 非概率混合模型
- en: 'With parametric time series models, forecasting equations are typically defined
    analytically and provide point forecasts for future targets. Non-probabilistic
    hybrid models hence modify these forecasting equations to combine statistical
    and deep learning components. The ES-RNN for example, utilises the update equations
    of the Holt-Winters exponential smoothing model [[8](#bib.bib8)] – combining multiplicative
    level and seasonality components with deep learning outputs as below:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于参数时间序列模型，预测方程通常是通过分析定义的，并为未来目标提供点预测。因此，非概率混合模型修改这些预测方程，以结合统计和深度学习组件。例如，ES-RNN
    利用 Holt-Winters 指数平滑模型 [[8](#bib.bib8)] 的更新方程——将乘法水平和季节性分量与深度学习输出结合，如下所示：
- en: '|  | $\displaystyle\hat{y}_{i,t+\tau}$ | $\displaystyle=\exp(\bm{W}_{ES}\bm{h}^{L}_{i,t+\tau}+\bm{b}_{ES})\times
    l_{i,t}\times\gamma_{i,t+\tau},$ |  | (24) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{y}_{i,t+\tau}$ | $\displaystyle=\exp(\bm{W}_{ES}\bm{h}^{L}_{i,t+\tau}+\bm{b}_{ES})\times
    l_{i,t}\times\gamma_{i,t+\tau},$ |  | (24) |'
- en: '|  | $\displaystyle l_{i,t}$ | $\displaystyle=\beta_{1}^{(i)}y_{i,t}/\gamma_{i,t}+(1-\beta_{1}^{(i)})l_{i,t-1},$
    |  | (25) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle l_{i,t}$ | $\displaystyle=\beta_{1}^{(i)}y_{i,t}/\gamma_{i,t}+(1-\beta_{1}^{(i)})l_{i,t-1},$
    |  | (25) |'
- en: '|  | $\displaystyle\gamma_{i,t}$ | $\displaystyle=\beta_{2}^{(i)}y_{i,t}/l_{i,t}+(1-\beta_{2}^{(i)})\gamma_{i,t-\kappa},$
    |  | (26) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\gamma_{i,t}$ | $\displaystyle=\beta_{2}^{(i)}y_{i,t}/l_{i,t}+(1-\beta_{2}^{(i)})\gamma_{i,t-\kappa},$
    |  | (26) |'
- en: where $\bm{h}^{L}_{i,t+\tau}$ is the final layer of the network for the $\tau$th-step-ahead
    forecast, $l_{i,t}$ is a level component, $\gamma_{i,t}$ is a seasonality component
    with period $\kappa$, and $\beta_{1}^{(i)},\beta_{2}^{(i)}$ are entity-specific
    static coefficients. From the above equations, we can see that the exponential
    smoothing components ($l_{i,t},\gamma_{i,t}$) handle the broader (e.g. exponential)
    trends within the datasets, reducing the need for additional input scaling.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{h}^{L}_{i,t+\tau}$ 是 $\tau$ 步预测的网络最终层，$l_{i,t}$ 是水平分量，$\gamma_{i,t}$
    是周期为 $\kappa$ 的季节性分量，而 $\beta_{1}^{(i)},\beta_{2}^{(i)}$ 是特定实体的静态系数。从上述方程中，我们可以看到指数平滑分量（$l_{i,t},\gamma_{i,t}$）处理数据集中的更广泛（例如指数）趋势，减少了对额外输入缩放的需求。
- en: 3.2 Probabilistic Hybrid Models
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 概率混合模型
- en: 'Probabilistic hybrid models can also be used in applications where distribution
    modelling is important – utilising probabilistic generative models for temporal
    dynamics such as Gaussian processes [[40](#bib.bib40)] and linear state space
    models [[38](#bib.bib38)]. Rather than modifying forecasting equations, probabilistic
    hybrid models use neural networks to produce parameters for predictive distributions
    at each step. For instance, Deep State Space Models [[38](#bib.bib38)] encode
    time-varying parameters for linear state space models as below – performing inference
    via the Kalman filtering equations [[46](#bib.bib46)]:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 概率混合模型也可以用于分布建模重要的应用场景——利用概率生成模型处理时间动态，如高斯过程 [[40](#bib.bib40)] 和线性状态空间模型 [[38](#bib.bib38)]。与其修改预测方程，概率混合模型使用神经网络在每一步生成预测分布的参数。例如，深度状态空间模型
    [[38](#bib.bib38)] 将线性状态空间模型的时间变化参数编码如下——通过卡尔曼滤波方程 [[46](#bib.bib46)] 进行推断：
- en: '|  | $\displaystyle y_{t}$ | $\displaystyle=\bm{a}(\bm{h}^{L}_{i,t+\tau})^{T}\bm{l}_{t}+\phi(\bm{h}^{L}_{i,t+\tau})\epsilon_{t},$
    |  | (27) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y_{t}$ | $\displaystyle=\bm{a}(\bm{h}^{L}_{i,t+\tau})^{T}\bm{l}_{t}+\phi(\bm{h}^{L}_{i,t+\tau})\epsilon_{t},$
    |  | (27) |'
- en: '|  | $\displaystyle\bm{l}_{t}$ | $\displaystyle=\bm{F}(\bm{h}^{L}_{i,t+\tau})\bm{l}_{t-1}+\bm{q}(\bm{h}^{L}_{i,t+\tau})+\bm{\Sigma}(\bm{h}^{L}_{i,t+\tau})\odot\bm{\Sigma}_{t},$
    |  | (28) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{l}_{t}$ | $\displaystyle=\bm{F}(\bm{h}^{L}_{i,t+\tau})\bm{l}_{t-1}+\bm{q}(\bm{h}^{L}_{i,t+\tau})+\bm{\Sigma}(\bm{h}^{L}_{i,t+\tau})\odot\bm{\Sigma}_{t},$
    |  | (28) |'
- en: where $\bm{l}_{t}$ is the hidden latent state, $\bm{a}(.)$, $\bm{F}(.)$, $\bm{q}(.)$
    are linear transformations of $\bm{h}^{L}_{i,t+\tau}$, $\phi(.)$, $\bm{\Sigma}(.)$
    are linear transformations with softmax activations, $\epsilon_{t}\sim N(0,1)$
    is a univariate residual and $\bm{\Sigma}_{t}\sim N(0,\mathbb{I})$ is a multivariate
    normal random variable.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{l}_{t}$ 是隐藏的潜在状态，$\bm{a}(.)$、$\bm{F}(.)$、$\bm{q}(.)$ 是 $\bm{h}^{L}_{i,t+\tau}$
    的线性变换，$\phi(.)$、$\bm{\Sigma}(.)$ 是带有 softmax 激活的线性变换，$\epsilon_{t}\sim N(0,1)$
    是单变量残差，而 $\bm{\Sigma}_{t}\sim N(0,\mathbb{I})$ 是多变量正态随机变量。
- en: 4 Facilitating Decision Support Using Deep Neural Networks
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 使用深度神经网络促进决策支持
- en: Although model builders are mainly concerned with the accuracy of their forecasts,
    end-users typically use predictions to guide their future actions. For instance,
    doctors can make use of clinical forecasts (e.g. probabilities of disease onset
    and mortality) to help them prioritise tests to order, formulate a diagnosis and
    determine a course of treatment. As such, while time series forecasting is a crucial
    preliminary step, a better understanding of both temporal dynamics and the motivations
    behind a model’s forecast can help users further optimise their actions. In this
    section, we explore two directions in which neural networks have been extended
    to facilitate decision support with time series data – focusing on methods in
    interpretability and causal inference.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型构建者主要关注预测的准确性，但终端用户通常使用预测来指导他们的未来行动。例如，医生可以利用临床预测（例如疾病发作和死亡率的概率）来帮助他们优先排序测试、制定诊断并确定治疗方案。因此，虽然时间序列预测是一个至关重要的初步步骤，但对时间动态和模型预测背后的动机的更好理解可以帮助用户进一步优化他们的行动。在本节中，我们探讨了神经网络在时间序列数据上促进决策支持的两个方向——重点关注可解释性和因果推断的方法。
- en: 4.1 Interpretability With Time Series Data
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 时间序列数据的可解释性
- en: With the deployment of neural networks in mission-critical applications [[68](#bib.bib68)],
    there is a increasing need to understand both how and why a model makes a certain
    prediction. Moreover, end-users can have little prior knowledge with regards to
    the relationships present in their data, with datasets growing in size and complexity
    in recent times. Given the black-box nature of standard neural network architectures,
    a new body of research has emerged in methods for interpreting deep learning models.
    We present a summary below – referring the reader to dedicated surveys for more
    in-depth analyses [[69](#bib.bib69), [70](#bib.bib70)].
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 随着神经网络在关键任务应用中的部署 [[68](#bib.bib68)]，对模型如何以及为何做出某种预测的理解需求日益增加。此外，终端用户可能对其数据中的关系了解甚少，而数据集的规模和复杂性近年来也在增长。鉴于标准神经网络架构的黑箱性质，出现了一系列新的研究方向，致力于解释深度学习模型。我们在下面总结了相关内容——并参考专门的调查以获取更深入的分析
    [[69](#bib.bib69), [70](#bib.bib70)]。
- en: Techniques for Post-hoc Interpretability
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 事后可解释性技术
- en: Post-hoc interpretable models are developed to interpret trained networks, and
    helping to identify important features or examples without modifying the original
    weights. Methods can mainly be divided into two main categories. Firstly, one
    possible approach is to apply simpler interpretable surrogate models between the
    inputs and outputs of the neural network, and rely on the approximate model to
    provide explanations. For instance, Local Interpretable Model-Agnostic Explanations
    (LIME) [[71](#bib.bib71)] identify relevant features by fitting instance-specific
    linear models to perturbations of the input, with the linear coefficients providing
    a measure of importance. Shapley additive explanations (SHAP) [[72](#bib.bib72)]
    provide another surrogate approach, which utilises Shapley values from cooperative
    game theory to identify important features across the dataset. Next, gradient-based
    method – such as saliency maps [[73](#bib.bib73), [74](#bib.bib74)] and influence
    functions [[75](#bib.bib75)] – have been proposed, which analyse network gradients
    to determine which input features have the greatest impact on loss functions.
    While post-hoc interpretability methods can help with feature attributions, they
    typically ignore any sequential dependencies between inputs – making it difficult
    to apply them to complex time series datasets.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 后验可解释模型旨在解释训练好的网络，并帮助识别重要特征或示例，而无需修改原始权重。方法主要可以分为两大类。首先，一种可能的方法是在神经网络的输入和输出之间应用更简单的可解释替代模型，并依赖于近似模型提供解释。例如，局部可解释模型无关解释
    (LIME) [[71](#bib.bib71)] 通过将特定实例的线性模型拟合到输入的扰动上来识别相关特征，线性系数提供了重要性度量。Shapley 加性解释
    (SHAP) [[72](#bib.bib72)] 提供了另一种替代方法，利用合作博弈论中的 Shapley 值来识别数据集中的重要特征。接下来，梯度基方法——如显著性图
    [[73](#bib.bib73)、[74](#bib.bib74)] 和影响函数 [[75](#bib.bib75)]——也已被提出，这些方法分析网络梯度以确定哪些输入特征对损失函数的影响最大。虽然后验可解释性方法可以帮助进行特征归因，但它们通常忽略输入之间的任何顺序依赖性——使其难以应用于复杂的时间序列数据集。
- en: Inherent Interpretability with Attention Weights
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用注意力权重的固有可解释性
- en: 'An alternative approach is to directly design architectures with explainable
    components, typically in the form of strategically placed attention layers. As
    attention weights are produced as outputs from a softmax layer, the weights are
    constrained to sum to 1, i.e. $\sum_{\tau=0}^{k}\alpha(t,\tau)=1$. For time series
    models, the outputs of Equation ([15](#S2.E15 "In 2.1.3 Attention Mechanisms ‣
    2.1 Basic Building Blocks ‣ 2 Deep Learning Architectures for Time Series Forecasting
    ‣ Time Series Forecasting With Deep Learning: A Survey")) can hence also be interpreted
    as a weighted average over temporal features, using the weights supplied by the
    attention layer at each step. An analysis of attention weights can then be used
    to understand the relative importance of features at each time step. Instance-wise
    interpretability studies have been performed in [[76](#bib.bib76), [55](#bib.bib55),
    [53](#bib.bib53)], where the authors used specific examples to show how the magnitudes
    of $\alpha(t,\tau)$ can indicate which time points were most significant for predictions.
    By analysing distributions of attention vectors across time, [[54](#bib.bib54)]
    also shows how attention mechanisms can be used to identify persistent temporal
    relationships – such as seasonal patterns – in the dataset.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种方法是直接设计具有可解释组件的架构，通常以战略性放置的注意力层的形式存在。由于注意力权重是从 softmax 层输出的，权重被约束为总和为 1，即
    $\sum_{\tau=0}^{k}\alpha(t,\tau)=1$。对于时间序列模型，方程 ([15](#S2.E15 "In 2.1.3 Attention
    Mechanisms ‣ 2.1 Basic Building Blocks ‣ 2 Deep Learning Architectures for Time
    Series Forecasting ‣ Time Series Forecasting With Deep Learning: A Survey")) 的输出也可以解释为在每一步使用注意力层提供的权重对时间特征进行加权平均。然后，可以分析注意力权重以理解每个时间步特征的相对重要性。[[76](#bib.bib76)、[55](#bib.bib55)、[53](#bib.bib53)]
    的实例级可解释性研究展示了如何通过具体示例说明 $\alpha(t,\tau)$ 的大小可以指示哪些时间点对预测最为重要。通过分析跨时间的注意力向量分布，[[54](#bib.bib54)]
    还展示了注意力机制如何用于识别数据集中的持续时间关系——例如季节模式。'
- en: 4.2 Counterfactual Predictions & Causal Inference Over Time
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 反事实预测与随时间变化的因果推断
- en: In addition to understanding the relationships learnt by the networks, deep
    learning can also help to facilitate decision support by producing predictions
    outside of their observational datasets, or counterfactual forecasts. Counterfactual
    predictions are particularly useful for scenario analysis applications – allowing
    users to evaluate how different sets of actions can impact target trajectories.
    This can be useful both from a historical angle, i.e. determining what would have
    happened if a different set of circumstances had occurred, and from a forecasting
    perspective, i.e. determining which actions to take to optimise future outcomes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除了理解网络学习到的关系外，深度学习还可以通过产生观察数据集之外的预测或反事实预测来帮助促进决策支持。反事实预测在情景分析应用中尤为有用——允许用户评估不同操作集合对目标轨迹的影响。这在历史角度上非常有用，即确定如果发生了不同的情况会发生什么，以及从预测角度出发，即确定采取哪些措施以优化未来的结果。
- en: While a large class of deep learning methods exists for estimating causal effects
    in static settings [[77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79)], the
    key challenge in time series datasets is the presence of time-dependent confounding
    effects. This arises due to circular dependencies when actions that can affect
    the target are also conditional on observations of the target. Without any adjusting
    for time-dependent confounders, straightforward estimations techniques can results
    in biased results, as shown in [[80](#bib.bib80)]. Recently, several methods have
    emerged to train deep neural networks while adjusting for time-dependent confounding,
    based on extensions of statistical techniques and the design of new loss functions.
    With statistical methods, [[81](#bib.bib81)] extends the inverse-probability-of-treatment-weighting
    (IPTW) approach of marginal structural models in epidemiology – using one set
    of networks to estimate treatment application probabilities, and a sequence-to-sequence
    model to learn unbiased predictions. Another approach in [[82](#bib.bib82)] extends
    the G-computation framework, jointly modelling distributions of the target and
    actions using deep learning. In addition, new loss functions have been proposed
    in [[83](#bib.bib83)], which adopts domain adversarial training to learn balanced
    representations of patient history.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在大量用于估计静态设置中因果效应的深度学习方法[[77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79)]，但时间序列数据集中的主要挑战是时间依赖的混杂效应。这是由于当可以影响目标的操作也取决于目标的观察时，出现循环依赖的问题。如果不对时间依赖的混杂因素进行调整，简单的估计技术可能会导致偏倚结果，如[[80](#bib.bib80)]所示。最近，出现了几种方法来训练深度神经网络，同时调整时间依赖的混杂因素，这些方法基于统计技术的扩展和新的损失函数设计。通过统计方法，[[81](#bib.bib81)]扩展了流行病学中的边际结构模型的逆概率加权（IPTW）方法——使用一组网络来估计治疗应用概率，并使用序列到序列模型来学习无偏预测。[[82](#bib.bib82)]中的另一种方法扩展了G计算框架，使用深度学习联合建模目标和操作的分布。此外，[[83](#bib.bib83)]中提出了新的损失函数，该函数采用领域对抗训练来学习患者历史的平衡表示。
- en: 5 Conclusions and Future Directions
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来方向
- en: With the growth in data availability and computing power in recent times, deep
    neural networks architectures have achieved much success in forecasting problems
    across multiple domains. In this article, we survey the main architectures used
    for time series forecasting – highlighting the key building blocks used in neural
    network design. We examine how they incorporate temporal information for one-step-ahead
    predictions, and describe how they can be extended for use in multi-horizon forecasting.
    Furthermore, we outline the recent trend of hybrid deep learning models, which
    combine statistical and deep learning components to outperform pure methods in
    either category. Finally, we summarise two ways in which deep learning can be
    extended to improve decision support over time, focusing on methods in interpretability
    and counterfactual prediction.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 随着最近数据可用性和计算能力的增长，深度神经网络架构在多个领域的预测问题上取得了很大成功。本文回顾了用于时间序列预测的主要架构——突出神经网络设计中使用的关键构建块。我们研究了它们如何将时间信息用于一步预测，并描述了它们如何扩展以用于多时间段预测。此外，我们概述了近期混合深度学习模型的趋势，这些模型结合了统计和深度学习组件，超越了任何一种方法的纯粹效果。最后，我们总结了深度学习可以如何扩展以改善决策支持的方法，重点关注解释性和反事实预测的方法。
- en: Although a large number of deep learning models have been developed for time
    series forecasting, some limitations still exist. Firstly, deep neural networks
    typically require time series to be discretised at regular intervals, making it
    difficult to forecast datasets where observations can be missing or arrive at
    random intervals. While some preliminary research on continuous-time models has
    been done via Neural Ordinary Differential Equations [[84](#bib.bib84)], additional
    work needs to be done to extend this work for datasets with complex inputs (e.g.
    static variables) and to benchmark them against existing models. In addition,
    as mentioned in [[85](#bib.bib85)], time series often have a hierarchical structure
    with logical groupings between trajectories – e.g. in retail forecasting, where
    product sales in the same geography can be affected by common trends. As such,
    the development of architectures which explicit account for such hierarchies could
    be an interesting research direction, and potentially improve forecasting performance
    over existing univariate or multivariate models.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经开发了大量用于时间序列预测的深度学习模型，但仍然存在一些局限性。首先，深度神经网络通常要求时间序列在规律的间隔上进行离散化，这使得预测数据集时难以处理缺失或随机间隔的观察值。尽管已经通过神经常微分方程[[84](#bib.bib84)]进行了关于连续时间模型的一些初步研究，但仍需要进一步的工作来扩展这一工作，以处理具有复杂输入（例如静态变量）的数据集，并将其与现有模型进行基准测试。此外，如[[85](#bib.bib85)]所述，时间序列通常具有层次结构，在轨迹之间存在逻辑分组——例如在零售预测中，相同地理位置的产品销售可能受共同趋势的影响。因此，开发明确考虑这些层次结构的架构可能是一个有趣的研究方向，并可能提高现有单变量或多变量模型的预测性能。
- en: \competing
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: \competing
- en: The author(s) declare that they have no competing interests.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作者声明他们没有竞争利益。
- en: References
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Mudelsee M. Trend analysis of climate time series: A review of methods.
    Earth-Science Reviews. 2019;190:310 – 322.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Mudelsee M. 气候时间序列的趋势分析：方法回顾。地球科学评论。2019；190：310 – 322。'
- en: '[2] Stoffer DS, Ombao H. Editorial: Special issue on time series analysis in
    the biological sciences. Journal of Time Series Analysis. 2012;33(5):701–703.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Stoffer DS, Ombao H. 编辑：生物科学中的时间序列分析特刊。时间序列分析杂志。2012；33(5)：701–703。'
- en: '[3] Topol EJ. High-performance medicine: the convergence of human and artificial
    intelligence. Nature Medicine. 2019 Jan;25(1):44–56.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Topol EJ. 高性能医学：人类与人工智能的融合。自然医学。2019年1月；25(1)：44–56。'
- en: '[4] Böse JH, Flunkert V, Gasthaus J, Januschowski T, Lange D, Salinas D, et al.
    Probabilistic Demand Forecasting at Scale. Proc VLDB Endow. 2017 Aug;10(12):1694–1705.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Böse JH, Flunkert V, Gasthaus J, Januschowski T, Lange D, Salinas D, 等。大规模概率需求预测。Proc
    VLDB Endow. 2017年8月；10(12)：1694–1705。'
- en: '[5] Andersen TG, Bollerslev T, Christoffersen PF, Diebold FX. Volatility Forecasting.
    National Bureau of Economic Research; 2005\. 11188.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Andersen TG, Bollerslev T, Christoffersen PF, Diebold FX. 波动率预测。国家经济研究局；2005。11188。'
- en: '[6] Box GEP, Jenkins GM. Time Series Analysis: Forecasting and Control. Holden-Day;
    1976.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Box GEP, Jenkins GM. 时间序列分析：预测与控制。Holden-Day；1976。'
- en: '[7] Gardner Jr ES. Exponential smoothing: The state of the art. Journal of
    Forecasting. 1985;4(1):1–28.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Gardner Jr ES. 指数平滑：现状。预测杂志。1985；4(1)：1–28。'
- en: '[8] Winters PR. Forecasting Sales by Exponentially Weighted Moving Averages.
    Management Science. 1960;6(3):324–342.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Winters PR. 通过指数加权移动平均法预测销售。管理科学。1960；6(3)：324–342。'
- en: '[9] Harvey AC. Forecasting, Structural Time Series Models and the Kalman Filter.
    Cambridge University Press; 1990.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Harvey AC. 预测、结构时间序列模型与卡尔曼滤波器。剑桥大学出版社；1990。'
- en: '[10] Ahmed NK, Atiya AF, Gayar NE, El-Shishiny H. An Empirical Comparison of
    Machine Learning Models for Time Series Forecasting. Econometric Reviews. 2010;29(5-6):594–621.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Ahmed NK, Atiya AF, Gayar NE, El-Shishiny H. 机器学习模型在时间序列预测中的实证比较。计量经济学评论。2010；29(5-6)：594–621。'
- en: '[11] Krizhevsky A, Sutskever I, Hinton GE. ImageNet Classification with Deep
    Convolutional Neural Networks. In: Pereira F, Burges CJC, Bottou L, Weinberger
    KQ, editors. Advances in Neural Information Processing Systems 25 (NIPS); 2012\.
    p. 1097–1105.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Krizhevsky A, Sutskever I, Hinton GE. 使用深度卷积神经网络进行ImageNet分类。在：Pereira
    F, Burges CJC, Bottou L, Weinberger KQ，编辑。神经信息处理系统进展25（NIPS）；2012年。第1097–1105页。'
- en: '[12] Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional
    Transformers for Language Understanding. In: Proceedings of the 2019 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 1 (Long and Short Papers); 2019\. p. 4171–4186.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Devlin J, Chang MW, Lee K, Toutanova K. BERT：深度双向转换器的预训练用于语言理解。在：2019年北美计算语言学协会：人类语言技术会议论文集，第1卷（长篇和短篇论文）；2019年。第4171–4186页。'
- en: '[13] Silver D, Huang A, Maddison CJ, Guez A, Sifre L, van den Driessche G,
    et al. Mastering the game of Go with deep neural networks and tree search. Nature.
    2016;529:484–503.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Silver D, Huang A, Maddison CJ, Guez A, Sifre L, van den Driessche G 等.
    使用深度神经网络和树搜索掌握围棋游戏。《自然》。2016年；529：484–503。'
- en: '[14] Baxter J. A Model of Inductive Bias Learning. J Artif Int Res. 2000;12(1):149–198.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Baxter J. 归纳偏置学习模型。《人工智能研究期刊》。2000年；12(1)：149–198。'
- en: '[15] Bengio Y, Courville A, Vincent P. Representation Learning: A Review and
    New Perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence.
    2013;35(8):1798–1828.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Bengio Y, Courville A, Vincent P. 表示学习：综述与新视角。《IEEE模式分析与机器智能汇刊》。2013年；35(8)：1798–1828。'
- en: '[16] Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al.. TensorFlow:
    Large-Scale Machine Learning on Heterogeneous Systems; 2015. Software available
    from tensorflow.org. Available from: [http://tensorflow.org/](http://tensorflow.org/).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C 等. TensorFlow:
    大规模机器学习在异构系统上的应用；2015年。软件可从tensorflow.org获取。访问网址：[http://tensorflow.org/](http://tensorflow.org/)。'
- en: '[17] Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. PyTorch:
    An Imperative Style, High-Performance Deep Learning Library. In: Advances in Neural
    Information Processing Systems 32; 2019\. p. 8024–8035.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G 等. PyTorch：一种命令式风格的高性能深度学习库。在：神经信息处理系统进展32；2019年。第8024–8035页。'
- en: '[18] Hyndman RJ, Khandakar Y. Automatic time series forecasting: the forecast
    package for R. Journal of Statistical Software. 2008;26(3):1–22.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Hyndman RJ, Khandakar Y. 自动时间序列预测：R的forecast包。《统计软件期刊》。2008年；26(3)：1–22。'
- en: '[19] Nadaraya EA. On Estimating Regression. Theory of Probability and Its Applications.
    1964;9(1):141–142.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Nadaraya EA. 关于回归估计。《概率论与其应用》。1964年；9(1)：141–142。'
- en: '[20] Smola AJ, Schölkopf B. A Tutorial on Support Vector Regression. Statistics
    and Computing. 2004;14(3):199–222.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Smola AJ, Schölkopf B. 支持向量回归教程。《统计与计算》。2004年；14(3)：199–222。'
- en: '[21] Williams CKI, Rasmussen CE. Gaussian Processes for Regression. In: Advances
    in Neural Information Processing Systems (NIPS); 1996. .'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Williams CKI, Rasmussen CE. 回归的高斯过程。在：神经信息处理系统进展（NIPS）；1996年。'
- en: '[22] Damianou A, Lawrence N. Deep Gaussian Processes. In: Proceedings of the
    Conference on Artificial Intelligence and Statistics (AISTATS); 2013\. .'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Damianou A, Lawrence N. 深度高斯过程。在：人工智能与统计学会议论文集（AISTATS）；2013年。'
- en: '[23] Garnelo M, Rosenbaum D, Maddison C, Ramalho T, Saxton D, Shanahan M, et al.
    Conditional Neural Processes. In: Proceedings of the International Conference
    on Machine Learning (ICML); 2018\. .'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Garnelo M, Rosenbaum D, Maddison C, Ramalho T, Saxton D, Shanahan M 等.
    条件神经过程。在：国际机器学习会议论文集（ICML）；2018年。'
- en: '[24] Waibel A. Modular Construction of Time-Delay Neural Networks for Speech
    Recognition. Neural Comput. 1989;1(1):39–46.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Waibel A. 用于语音识别的时间延迟神经网络的模块化构建。《神经计算》。1989年；1(1)：39–46。'
- en: '[25] Wan E. Time Series Prediction by Using a Connectionist Network with Internal
    Delay Lines. In: Time Series Prediction. Addison-Wesley; 1994\. p. 195–217.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Wan E. 使用具有内部延迟线的连接主义网络进行时间序列预测。在：时间序列预测。Addison-Wesley；1994年。第195–217页。'
- en: '[26] Sen R, Yu HF, Dhillon I. Think Globally, Act Locally: A Deep Neural Network
    Approach to High-Dimensional Time Series Forecasting. In: Advances in Neural Information
    Processing Systems (NeurIPS); 2019\. .'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Sen R, Yu HF, Dhillon I. 全球思考，本地行动：一种深度神经网络方法用于高维时间序列预测。在：神经信息处理系统（NeurIPS）进展；2019年。'
- en: '[27] Wen R, Torkkola K. Deep Generative Quantile-Copula Models for Probabilistic
    Forecasting. In: ICML Time Series Workshop; 2019\. .'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Wen R, Torkkola K. 深度生成量化-耦合模型用于概率预测。在：ICML时间序列研讨会；2019年。'
- en: '[28] Li Y, Yu R, Shahabi C, Liu Y. Diffusion Convolutional Recurrent Neural
    Network: Data-Driven Traffic Forecasting. In: (Proceedings of the International
    Conference on Learning Representations ICLR); 2018\. .'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Li Y, Yu R, Shahabi C, Liu Y. 扩散卷积递归神经网络：数据驱动的交通预测。在：（国际学习表示会议ICLR论文集）；2018年。'
- en: '[29] Ghaderi A, Sanandaji BM, Ghaderi F. Deep Forecast: Deep Learning-based
    Spatio-Temporal Forecasting. In: ICML Time Series Workshop; 2017\. .'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Ghaderi A, Sanandaji BM, Ghaderi F. Deep Forecast：基于深度学习的时空预测。见：ICML时间序列研讨会；2017年。'
- en: '[30] Salinas D, Bohlke-Schneider M, Callot L, Medico R, Gasthaus J. High-dimensional
    multivariate forecasting with low-rank Gaussian Copula Processes. In: Advances
    in Neural Information Processing Systems (NeurIPS); 2019\. .'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Salinas D, Bohlke-Schneider M, Callot L, Medico R, Gasthaus J. 使用低秩高斯Copula过程的高维多变量预测。见：神经信息处理系统会议（NeurIPS）论文集；2019年。'
- en: '[31] Goodfellow I, Bengio Y, Courville A. Deep Learning. MIT Press; 2016. [http://www.deeplearningbook.org](http://www.deeplearningbook.org).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Goodfellow I, Bengio Y, Courville A. 深度学习。MIT出版社；2016年。[http://www.deeplearningbook.org](http://www.deeplearningbook.org)。'
- en: '[32] van den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, et al.
    WaveNet: A Generative Model for Raw Audio. arXiv e-prints. 2016 Sep;p. arXiv:1609.03499.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] van den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, 等.
    WaveNet：用于原始音频的生成模型。arXiv电子预印本。2016年9月；第arXiv:1609.03499页。'
- en: '[33] Bai S, Zico Kolter J, Koltun V. An Empirical Evaluation of Generic Convolutional
    and Recurrent Networks for Sequence Modeling. arXiv e-prints. 2018;p. arXiv:1803.01271.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Bai S, Zico Kolter J, Koltun V. 对通用卷积和递归网络进行序列建模的经验评估。arXiv电子预印本。2018年；第arXiv:1803.01271页。'
- en: '[34] Borovykh A, Bohte S, Oosterlee CW. Conditional Time Series Forecasting
    with Convolutional Neural Networks. arXiv e-prints. 2017;p. arXiv:1703.04691.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Borovykh A, Bohte S, Oosterlee CW. 使用卷积神经网络的条件时间序列预测。arXiv电子预印本。2017年；第arXiv:1703.04691页。'
- en: '[35] Lyons RG. Understanding Digital Signal Processing (2nd Edition). USA:
    Prentice Hall PTR; 2004.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Lyons RG. 理解数字信号处理（第二版）。美国：Prentice Hall PTR；2004年。'
- en: '[36] Young T, Hazarika D, Poria S, Cambria E. Recent Trends in Deep Learning
    Based Natural Language Processing [Review Article]. IEEE Computational Intelligence
    Magazine. 2018;13(3):55–75.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Young T, Hazarika D, Poria S, Cambria E. 基于深度学习的自然语言处理的最新趋势[综述文章]。IEEE计算智能杂志。2018年；13(3):55–75。'
- en: '[37] Salinas D, Flunkert V, Gasthaus J. DeepAR: Probabilistic Forecasting with
    Autoregressive Recurrent Networks. arXiv e-prints. 2017;p. arXiv:1704.04110.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Salinas D, Flunkert V, Gasthaus J. DeepAR：具有自回归递归网络的概率预测。arXiv电子预印本。2017年；第arXiv:1704.04110页。'
- en: '[38] Rangapuram SS, Seeger MW, Gasthaus J, Stella L, Wang Y, Januschowski T.
    Deep State Space Models for Time Series Forecasting. In: Advances in Neural Information
    Processing Systems (NIPS); 2018. .'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Rangapuram SS, Seeger MW, Gasthaus J, Stella L, Wang Y, Januschowski T.
    用于时间序列预测的深度状态空间模型。见：神经信息处理系统会议（NIPS）论文集；2018年。'
- en: '[39] Lim B, Zohren S, Roberts S. Recurrent Neural Filters: Learning Independent
    Bayesian Filtering Steps for Time Series Prediction. In: International Joint Conference
    on Neural Networks (IJCNN); 2020\. .'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Lim B, Zohren S, Roberts S. 递归神经滤波器：学习用于时间序列预测的独立贝叶斯滤波步骤。见：国际神经网络联合会议（IJCNN）；2020年。'
- en: '[40] Wang Y, Smola A, Maddix D, Gasthaus J, Foster D, Januschowski T. Deep
    Factors for Forecasting. In: Proceedings of the International Conference on Machine
    Learning (ICML); 2019\. .'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Wang Y, Smola A, Maddix D, Gasthaus J, Foster D, Januschowski T. 用于预测的深度因子。见：国际机器学习会议（ICML）论文集；2019年。'
- en: '[41] Elman JL. Finding structure in time. Cognitive Science. 1990;14(2):179
    – 211.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Elman JL. 在时间中寻找结构。认知科学。1990年；14(2):179 – 211。'
- en: '[42] Bengio Y, Simard P, Frasconi P. Learning long-term dependencies with gradient
    descent is difficult. IEEE Transactions on Neural Networks. 1994;5(2):157–166.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Bengio Y, Simard P, Frasconi P. 用梯度下降学习长期依赖是困难的。IEEE神经网络学报。1994年；5(2):157–166。'
- en: '[43] Kolen JF, Kremer SC. In: Gradient Flow in Recurrent Nets: The Difficulty
    of Learning LongTerm Dependencies; 2001\. p. 237–243.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Kolen JF, Kremer SC. 见：递归网络中的梯度流：学习长期依赖的困难；2001年。第237–243页。'
- en: '[44] Hochreiter S, Schmidhuber J. Long Short-Term Memory. Neural Computation.
    1997 Nov;9(8):1735–1780.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Hochreiter S, Schmidhuber J. 长短期记忆。神经计算。1997年11月；9(8):1735–1780。'
- en: '[45] Srkk S. Bayesian Filtering and Smoothing. Cambridge University Press;
    2013.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Srkk S. 贝叶斯滤波与平滑。剑桥大学出版社；2013年。'
- en: '[46] Kalman RE. A New Approach to Linear Filtering and Prediction Problems.
    Journal of Basic Engineering. 1960;82(1):35.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Kalman RE. 线性滤波和预测问题的新方法。基础工程杂志。1960年；82(1):35。'
- en: '[47] Bahdanau D, Cho K, Bengio Y. Neural Machine Translation by Jointly Learning
    to Align and Translate. In: Proceedings of the International Conference on Learning
    Representations (ICLR); 2015\. .'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Bahdanau D, Cho K, Bengio Y. 通过联合学习对齐和翻译进行神经机器翻译。见：国际学习表示会议（ICLR）论文集；2015年。'
- en: '[48] Cho K, van Merriënboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk
    H, et al. Learning Phrase Representations using RNN Encoder–Decoder for Statistical
    Machine Translation. In: Proceedings of the 2014 Conference on Empirical Methods
    in Natural Language Processing (EMNLP); 2014\. .'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Cho K, van Merriënboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk
    H, 等人。使用RNN编码器-解码器进行统计机器翻译的短语表示学习。在：2014年自然语言处理经验方法会议（EMNLP）论文集；2014年。'
- en: '[49] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al.
    Attention is All you Need. In: Advances in Neural Information Processing Systems
    (NIPS); 2017. .'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, 等人。注意力即一切。在：神经信息处理系统进展（NIPS）；2017年。'
- en: '[50] Dai Z, Yang Z, Yang Y, Carbonell J, Le Q, Salakhutdinov R. Transformer-XL:
    Attentive Language Models beyond a Fixed-Length Context. In: Proceedings of the
    57th Annual Meeting of the Association for Computational Linguistics (ACL); 2019\.
    .'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Dai Z, Yang Z, Yang Y, Carbonell J, Le Q, Salakhutdinov R。Transformer-XL：超越固定长度上下文的注意力语言模型。在：第57届计算语言学协会年会（ACL）论文集；2019年。'
- en: '[51] Graves A, Wayne G, Danihelka I. Neural Turing Machines. CoRR. 2014;abs/1410.5401.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Graves A, Wayne G, Danihelka I。神经图灵机。CoRR。2014年；abs/1410.5401。'
- en: '[52] Fan C, Zhang Y, Pan Y, Li X, Zhang C, Yuan R, et al. Multi-Horizon Time
    Series Forecasting with Temporal Attention Learning. In: Proceedings of the ACM
    SIGKDD international conference on Knowledge discovery and data mining (KDD);
    2019\. .'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Fan C, Zhang Y, Pan Y, Li X, Zhang C, Yuan R, 等人。基于时间注意学习的多时间尺度时间序列预测。在：ACM
    SIGKDD 国际知识发现与数据挖掘会议（KDD）论文集；2019年。'
- en: '[53] Li S, Jin X, Xuan Y, Zhou X, Chen W, Wang YX, et al. Enhancing the Locality
    and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting.
    In: Advances in Neural Information Processing Systems (NeurIPS); 2019\. .'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Li S, Jin X, Xuan Y, Zhou X, Chen W, Wang YX, 等人。增强变压器在时间序列预测中的局部性并突破记忆瓶颈。在：神经信息处理系统进展（NeurIPS）；2019年。'
- en: '[54] Lim B, Arik SO, Loeff N, Pfister T. Temporal Fusion Transformers for Interpretable
    Multi-horizon Time Series Forecasting. arXiv e-prints. 2019;p. arXiv:1912.09363.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Lim B, Arik SO, Loeff N, Pfister T。用于可解释的多时间尺度时间序列预测的时间融合变压器。arXiv 预印本。2019年；页码
    arXiv:1912.09363。'
- en: '[55] Choi E, Bahadori MT, Sun J, Kulas JA, Schuetz A, Stewart WF. RETAIN: An
    Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism.
    In: Advances in Neural Information Processing Systems (NIPS); 2016. .'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Choi E, Bahadori MT, Sun J, Kulas JA, Schuetz A, Stewart WF。RETAIN：一种使用逆时间注意机制的可解释预测模型用于医疗保健。在：神经信息处理系统进展（NIPS）；2016年。'
- en: '[56] Wen R, et al. A Multi-Horizon Quantile Recurrent Forecaster. In: NIPS
    2017 Time Series Workshop; 2017\. .'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Wen R, 等人。多时间尺度分位数递归预测模型。在：NIPS 2017 时间序列研讨会；2017年。'
- en: '[57] Taieb SB, Sorjamaa A, Bontempi G. Multiple-output modeling for multi-step-ahead
    time series forecasting. Neurocomputing. 2010;73(10):1950 – 1957.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Taieb SB, Sorjamaa A, Bontempi G。多输出建模用于多步预测时间序列。《神经计算》。2010年；73(10):1950
    – 1957。'
- en: '[58] Marcellino M, Stock J, Watson M. A Comparison of Direct and Iterated Multistep
    AR Methods for Forecasting Macroeconomic Time Series. Journal of Econometrics.
    2006;135:499–526.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Marcellino M, Stock J, Watson M。直接和迭代多步自回归方法在宏观经济时间序列预测中的比较。《计量经济学杂志》。2006年；135:499–526。'
- en: '[59] Makridakis S, Spiliotis E, Assimakopoulos V. Statistical and Machine Learning
    forecasting methods: Concerns and ways forward. PLOS ONE. 2018 03;13(3):1–26.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Makridakis S, Spiliotis E, Assimakopoulos V。统计和机器学习预测方法：问题与前进方向。《PLOS
    ONE》。2018年3月；13(3):1–26。'
- en: '[60] Hyndman R. A brief history of forecasting competitions. International
    Journal of Forecasting. 2020;36(1):7–14.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Hyndman R。预测竞赛的简要历史。《国际预测杂志》。2020年；36(1):7–14。'
- en: '[61] The M4 Competition: 100,000 time series and 61 forecasting methods. International
    Journal of Forecasting. 2020;36(1):54 – 74.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] M4 竞赛：100,000个时间序列和61种预测方法。《国际预测杂志》。2020年；36(1):54 – 74。'
- en: '[62] Fildes R, Hibon M, Makridakis S, Meade N. Generalising about univariate
    forecasting methods: further empirical evidence. International Journal of Forecasting.
    1998;14(3):339 – 358.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Fildes R, Hibon M, Makridakis S, Meade N。关于单变量预测方法的泛化：进一步的实证证据。《国际预测杂志》。1998年；14(3):339
    – 358。'
- en: '[63] Makridakis S, Hibon M. The M3-Competition: results, conclusions and implications.
    International Journal of Forecasting. 2000;16(4):451 – 476. The M3- Competition.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Makridakis S, Hibon M。M3-竞赛：结果、结论及其影响。《国际预测杂志》。2000年；16(4):451 – 476。M3-竞赛。'
- en: '[64] Smyl S. A hybrid method of exponential smoothing and recurrent neural
    networks for time series forecasting. International Journal of Forecasting. 2020;36(1):75
    – 85. M4 Competition.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Smyl S. 指数平滑与递归神经网络的混合方法用于时间序列预测。国际预测期刊。2020年；36(1)：75 – 85。M4竞赛。'
- en: '[65] Lim B, Zohren S, Roberts S. Enhancing Time-Series Momentum Strategies
    Using Deep Neural Networks. The Journal of Financial Data Science. 2019;.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Lim B, Zohren S, Roberts S. 利用深度神经网络增强时间序列动量策略。《金融数据科学期刊》。2019年。'
- en: '[66] Grover A, Kapoor A, Horvitz E. A Deep Hybrid Model for Weather Forecasting.
    In: Proceedings of the ACM SIGKDD international conference on knowledge discovery
    and data mining (KDD); 2015\. .'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Grover A, Kapoor A, Horvitz E. 用于天气预测的深度混合模型。载于：ACM SIGKDD国际知识发现与数据挖掘会议（KDD）会议录；2015年。'
- en: '[67] Binkowski M, Marti G, Donnat P. Autoregressive Convolutional Neural Networks
    for Asynchronous Time Series. In: Proceedings of the International Conference
    on Machine Learning (ICML); 2018\. .'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Binkowski M, Marti G, Donnat P. 自回归卷积神经网络用于异步时间序列。载于：国际机器学习会议（ICML）会议录；2018年。'
- en: '[68] Moraffah R, Karami M, Guo R, Raglin A, Liu H. Causal Interpretability
    for Machine Learning – Problems, Methods and Evaluation. arXiv e-prints. 2020;p.
    arXiv:2003.03934.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Moraffah R, Karami M, Guo R, Raglin A, Liu H. 机器学习的因果可解释性——问题、方法与评估。arXiv预印本。2020年；第arXiv:2003.03934页。'
- en: '[69] Chakraborty S, Tomsett R, Raghavendra R, Harborne D, Alzantot M, Cerutti
    F, et al. Interpretability of deep learning models: A survey of results. In: 2017
    IEEE SmartWorld Conference Proceedings); 2017\. p. 1–6.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Chakraborty S, Tomsett R, Raghavendra R, Harborne D, Alzantot M, Cerutti
    F, 等。深度学习模型的可解释性：结果调查。载于：2017年IEEE SmartWorld会议录；2017年。第1–6页。'
- en: '[70] Rudin C. Stop explaining black box machine learning models for high stakes
    decisions and use interpretable models instead. Nature Machine Intelligence. 2019
    May;1(5):206–215.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Rudin C. 停止为高风险决策解释黑箱机器学习模型，而改用可解释模型。自然机器智能。2019年5月；1(5)：206–215。'
- en: '[71] Ribeio M, Singh S, Guestrin C. "Why Should I Trust You?" Explaining the
    Predictions of Any Classifier. In: KDD; 2016\. .'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Ribeio M, Singh S, Guestrin C. “我为什么要相信你？”解释任何分类器的预测。载于：KDD；2016年。'
- en: '[72] Lundberg S, Lee SI. A Unified Approach to Interpreting Model Predictions.
    In: Advances in Neural Information Processing Systems (NIPS); 2017. .'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Lundberg S, Lee SI. 统一模型预测解释方法。载于：神经信息处理系统进展（NIPS）；2017年。'
- en: '[73] Simonyan K, Vedaldi A, Zisserman A. Deep Inside Convolutional Networks:
    Visualising Image Classification Models and Saliency Maps. arXiv e-prints. 2013;p.
    arXiv:1312.6034.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Simonyan K, Vedaldi A, Zisserman A. 深入卷积网络：可视化图像分类模型和显著性图。arXiv预印本。2013年；第arXiv:1312.6034页。'
- en: '[74] Siddiqui SA, Mercier D, Munir M, Dengel A, Ahmed S. TSViz: Demystification
    of Deep Learning Models for Time-Series Analysis. IEEE Access. 2019;7:67027–67040.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Siddiqui SA, Mercier D, Munir M, Dengel A, Ahmed S. TSViz：时间序列分析中深度学习模型的揭示。IEEE
    Access。2019年；7：67027–67040。'
- en: '[75] Koh PW, Liang P. Understanding Black-box Predictions via Influence Functions.
    In: Proceedings of the International Conference on Machine Learning(ICML; 2017\.
    .'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Koh PW, Liang P. 通过影响函数理解黑箱预测。载于：国际机器学习会议（ICML）会议录；2017年。'
- en: '[76] Bai T, Zhang S, Egleston BL, Vucetic S. Interpretable Representation Learning
    for Healthcare via Capturing Disease Progression through Time. In: Proceedings
    of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining
    (KDD); 2018\. .'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Bai T, Zhang S, Egleston BL, Vucetic S. 通过捕捉疾病进程进行医疗保健的可解释表征学习。载于：ACM
    SIGKDD国际知识发现与数据挖掘会议（KDD）会议录；2018年。'
- en: '[77] Yoon J, Jordon J, van der Schaar M. GANITE: Estimation of Individualized
    Treatment Effects using Generative Adversarial Nets. In: International Conference
    on Learning Representations (ICLR); 2018\. .'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Yoon J, Jordon J, van der Schaar M. GANITE: 使用生成对抗网络估计个性化治疗效果。载于：国际学习表征会议（ICLR）；2018年。'
- en: '[78] Hartford J, Lewis G, Leyton-Brown K, Taddy M. Deep IV: A Flexible Approach
    for Counterfactual Prediction. In: Proceedings of the 34th International Conference
    on Machine Learning (ICML); 2017\. .'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Hartford J, Lewis G, Leyton-Brown K, Taddy M. 深度IV：一种用于反事实预测的灵活方法。载于：第34届国际机器学习会议（ICML）会议录；2017年。'
- en: '[79] Alaa AM, Weisz M, van der Schaar M. Deep Counterfactual Networks with
    Propensity Dropout. In: Proceedings of the 34th International Conference on Machine
    Learning (ICML); 2017\. .'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Alaa AM, Weisz M, van der Schaar M. 深度反事实网络与倾向性丢弃。载于：第34届国际机器学习会议（ICML）会议录；2017年。'
- en: '[80] Mansournia MA, Etminan M, Danaei G, Kaufman JS, Collins G. Handling time
    varying confounding in observational research. BMJ. 2017;359.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Mansournia MA, Etminan M, Danaei G, Kaufman JS, Collins G. 处理观察研究中的时间变化混杂因素。BMJ。2017年；359。'
- en: '[81] Lim B, Alaa A, van der Schaar M. Forecasting Treatment Responses Over
    Time Using Recurrent Marginal Structural Networks. In: NeurIPS; 2018\. .'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Lim B, Alaa A, van der Schaar M. 使用递归边际结构网络预测时间上的治疗反应。见：NeurIPS；2018年。'
- en: '[82] Li R, Shahn Z, Li J, Lu M, Chakraborty P, Sow D, et al. G-Net: A Deep
    Learning Approach to G-computation for Counterfactual Outcome Prediction Under
    Dynamic Treatment Regimes. arXiv e-prints. 2020;p. arXiv:2003.10551.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Li R, Shahn Z, Li J, Lu M, Chakraborty P, Sow D, 等. G-Net：一种用于动态治疗方案下反事实结果预测的深度学习方法。arXiv
    电子预印本。2020年；p. arXiv:2003.10551。'
- en: '[83] Bica I, Alaa AM, Jordon J, van der Schaar M. Estimating counterfactual
    treatment outcomes over time through adversarially balanced representations. In:
    International Conference on Learning Representations(ICLR); 2020\. .'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Bica I, Alaa AM, Jordon J, van der Schaar M. 通过对抗性平衡表示估计时间上的反事实治疗结果。见：国际学习表征会议（ICLR）；2020年。'
- en: '[84] Chen RTQ, Rubanova Y, Bettencourt J, Duvenaud D. Neural Ordinary Differential
    Equations. In: Proceedings of the International Conference on Neural Information
    Processing Systems (NIPS); 2018\. .'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Chen RTQ, Rubanova Y, Bettencourt J, Duvenaud D. 神经常微分方程。见：国际神经信息处理系统会议（NIPS）论文集；2018年。'
- en: '[85] Fry C, Brundage M. The M4 Forecasting Competition – A Practitioner’s View.
    International Journal of Forecasting. 2019;.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Fry C, Brundage M. M4预测竞赛——从实践者的角度。国际预测期刊。2019年。'
