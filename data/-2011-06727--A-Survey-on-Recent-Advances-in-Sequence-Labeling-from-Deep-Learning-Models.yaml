- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:58:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:58:34
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2011.06727] A Survey on Recent Advances in Sequence Labeling from Deep Learning
    Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2011.06727] 深度学习模型在序列标注中的最新进展调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2011.06727](https://ar5iv.labs.arxiv.org/html/2011.06727)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2011.06727](https://ar5iv.labs.arxiv.org/html/2011.06727)
- en: A Survey on Recent Advances in Sequence Labeling from Deep Learning Models
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型在序列标注中的最新进展调查
- en: 'Zhiyong He, Zanbo Wang, Wei Wei1, Shanshan Feng, Xianling Mao, and Sheng Jiang
    This work was supported in part by the National Natural Science Foundation of
    China under Grant No. 61602197 and Grant No. 61772076, Grant No. 61972448, Grant
    No. L1924068 and in part by Equipment Pre-Research Fund for The 13th Five-year
    Plan under Grant No. 41412050801.E-mail addresses: weiw@hust.edu.cn (W. Wei)Z.
    He is with the School of Electronic Engineering, Naval University of Engineering.Z.
    Wang, W. Wei and S. Jiang are with the School of Computer Science and Technology,
    Huazhong University of Science and Technology.S. Feng is with the Inception Institute
    of Artificial Intelligence Abu Dhabi, UAE.X. Mao is with the School of Computer,
    Beijing Institute of Technology.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 何志勇、王赞博、魏伟、冯珊珊、毛先凌、姜晟 本工作部分由中国国家自然科学基金资助，基金编号：61602197、61772076、61972448、L1924068，部分由“十三五”设备预研基金资助，基金编号：41412050801。电子邮件地址：weiw@hust.edu.cn（魏伟）何志勇在海军工程大学电子工程学院工作。王赞博、魏伟和姜晟在华中科技大学计算机科学与技术学院工作。冯珊珊在阿布扎比的启迪人工智能研究所工作。毛先凌在北京理工大学计算机学院工作。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Sequence labeling (SL) is a fundamental research problem encompassing a variety
    of tasks, *e.g.,* part-of-speech (POS) tagging, named entity recognition (NER),
    text chunking *etc.* Though prevalent and effective in many downstream applications
    (*e.g.,* information retrieval, question answering and knowledge graph embedding),
    conventional sequence labeling approaches heavily rely on hand-crafted or language-specific
    features. Recently, deep learning has been employed for sequence labeling task
    due to its powerful capability in automatically learning complex features of instances
    and effectively yielding the stat-of-the-art performances. In this paper, we aim
    to present a comprehensive review of existing deep learning-based sequence labeling
    models, which consists of three related tasks, *e.g.,* part-of-speech tagging,
    named entity recognition and text chunking. Then, we systematically present the
    existing approaches base on a scientific taxonomy, as well as the widely-used
    experimental datasets and popularly-adopted evaluation metrics in SL domain. Furthermore,
    we also present an in-depth analysis of different SL models on the factors that
    may affect the performance, and the future directions in SL domain.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 序列标注（SL）是一个基础研究问题，涵盖了多种任务，*例如*，词性标注（POS）、命名实体识别（NER）、文本切块*等*。尽管在许多下游应用（*例如*，信息检索、问答系统和知识图谱嵌入）中，传统的序列标注方法有效且普遍，但它们严重依赖于手工设计或特定语言的特征。最近，由于其自动学习复杂实例特征的强大能力，并且有效地产生了最先进的性能，深度学习已被用于序列标注任务。本文旨在对现有的基于深度学习的序列标注模型进行全面回顾，涵盖了三个相关任务，*例如*，词性标注、命名实体识别和文本切块。接着，我们基于科学分类法系统地介绍现有的方法，以及序列标注领域广泛使用的实验数据集和常用的评估指标。此外，我们还对不同序列标注模型在可能影响性能的因素以及序列标注领域的未来方向进行了深入分析。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Sequence labeling, deep learning, natural language processing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 序列标注、深度学习、自然语言处理。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Sequence labeling is a type of pattern recognition task in the important branch
    of natural language processing (NLP). From the perspective of linguistics, the
    smallest meaningful unit in a language is typically regarded as morpheme, and
    each sentence can thus be viewed as a sequence composed of morphemes. Accordingly,
    the sequence labeling problem in NLP domain can be formulate it as a task that
    aims at assigning labels to a category of morphemes that generally have similar
    roles within the grammatical structure of sentences and have similar grammatical
    properties, and the meanings of the assigned labels usually depend on the types
    of specific tasks, examples of classical tasks include part-of-speech (POS) tagging [[71](#bib.bib71)],
    named entity recognition (NER) [[52](#bib.bib52)], text chunking [[65](#bib.bib65)]
    and *etc.*, which play a pivotal role in natural language understanding and can
    benefit a variety of downstream applications such as syntactic parsing [[81](#bib.bib81)],
    relation extraction [[64](#bib.bib64)] and entity coreference resolution [[78](#bib.bib78)]
    and *etc.*, and hence has quickly gained massive attention.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 序列标注是自然语言处理（NLP）这一重要领域中的一种模式识别任务。从语言学的角度来看，语言中最小的有意义单位通常被认为是语素，因此每个句子可以被视为由语素组成的序列。因此，NLP领域中的序列标注问题可以被表述为一个任务，旨在为一类在句子的语法结构中通常具有相似角色且具有相似语法属性的语素分配标签，而分配的标签的含义通常依赖于具体任务的类型，经典任务的例子包括词性标注（POS）[[71](#bib.bib71)]、命名实体识别（NER）[[52](#bib.bib52)]、文本切分[[65](#bib.bib65)]以及*等等*，这些任务在自然语言理解中扮演着关键角色，并且可以为各种下游应用提供帮助，例如句法解析[[81](#bib.bib81)]、关系抽取[[64](#bib.bib64)]和实体共指解析[[78](#bib.bib78)]以及*等等*，因此迅速获得了大量关注。
- en: Generally, conventional sequence labeling approaches are usually on the basis
    of classical machine learning technologies, *e.g.,* Hidden Markov Models (HMM) [[3](#bib.bib3)]
    and Conditional Random Fields (CRFs) [[51](#bib.bib51)], which often heavily rely
    on hand-crafted features (*e.g.,* whether a word is capitalized) or language-specific
    resources (*e.g.,* gazetteers). Despite superior performance achieved, the requirement
    of the considerable amount of domain knowledge and efforts on feature engineering
    make them extremely difficult to extend to new areas. Over the past decade, the
    great success has been achieved by deep learning (DL) due to its powerful capability
    in automatically learning complex features of data. Hence, there already exist
    many efforts dedicated to research on how to exploit the representation learning
    capability of deep neural network for enhancing sequence labeling tasks, and many
    of these methods have successively advanced the state-of-the-art performances [[8](#bib.bib8),
    [1](#bib.bib1), [19](#bib.bib19)]. This trend motivates us to conduct a comprehensive
    survey to summarize the current status of deep learning techniques in the filed
    of sequence labeling. By comparing the choices of different deep learning architectures,
    we aim to identify the impacts on the model performance, making it convenient
    for subsequent researchers to better understand the advantages/disadvantages of
    such models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，传统的序列标注方法通常基于经典的机器学习技术，如隐马尔可夫模型（HMM）[[3](#bib.bib3)]和条件随机场（CRFs）[[51](#bib.bib51)]，这些方法通常依赖于手工构造的特征（*例如*，单词是否大写）或特定语言的资源（*例如*，地名词典）。尽管取得了优越的性能，但对大量领域知识和特征工程的需求使得这些方法在扩展到新领域时极为困难。在过去十年中，由于其在自动学习数据复杂特征方面的强大能力，深度学习（DL）取得了巨大成功。因此，已经有许多研究致力于如何利用深度神经网络的表示学习能力来提升序列标注任务，许多这些方法相继推动了最先进的性能[[8](#bib.bib8),
    [1](#bib.bib1), [19](#bib.bib19)]。这一趋势促使我们进行全面的调查，以总结深度学习技术在序列标注领域的现状。通过比较不同深度学习架构的选择，我们旨在识别对模型性能的影响，使后续研究人员能够更好地理解这些模型的优缺点。
- en: Differences with former surveys. In literature, there have been numerous attempts
    to improve the performance of sequence labeling tasks using deep learning models.
    However, to the best of our knowledge, there are nearly none comprehensive surveys
    that provide an in-depth summary of existing neural network based methods or developments
    on part of this topic so far. Actually, in the past few years, several surveys
    have been presented on traditional approaches for sequence labeling. For example,
    Nguyen *et al.* [[79](#bib.bib79)] propose a systematic survey on machine learning
    based sequence labeling problems. Nadeau *et al.* [[76](#bib.bib76)] survey on
    the problem of *named entity recognition* and present an overview of the trend
    from hand-crafted rule-based algorithms to machine learning techniques. Kumar
    and Josan [[49](#bib.bib49)] conduct a short review on *part-of-speech tagging*
    for Indian language. In summary, the most reviews for sequence labeling mainly
    cover papers on traditional machine learning methods, rather than the recent applied
    techniques of deep learning (DL). Recently, two work [[117](#bib.bib117), [54](#bib.bib54)]
    present a good literature survey of the deep learning based models for named entity
    recognition (NER) problem, however it is solely a sub-task for sequence labeling.
    To the best of our knowledge, there has been so far no survey that can provide
    an exhaustive summary of recent research on DL-based sequence labeling methods.
    Given the increasing popularity of deep learning models in sequence labeling,
    a systematic survey will be of high academic and practical significance. We summarize
    and analyze these related works and over $100$ studies are covered in this survey.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往调查的不同。在文献中，已经有很多尝试通过深度学习模型提高序列标注任务的性能。然而，据我们所知，几乎没有全面的调查提供现有神经网络方法或相关发展领域的深入总结。实际上，在过去几年中，已经有几项关于传统序列标注方法的调查。例如，Nguyen
    *et al.* [[79](#bib.bib79)] 提出了一个关于基于机器学习的序列标注问题的系统调查。Nadeau *et al.* [[76](#bib.bib76)]
    调查了*命名实体识别*问题，并概述了从手工规则算法到机器学习技术的发展趋势。Kumar 和 Josan [[49](#bib.bib49)] 对印度语言的*词性标注*进行了简要回顾。总的来说，大多数序列标注的评论主要涵盖传统机器学习方法的论文，而非最近应用的深度学习（DL）技术。最近，有两项工作
    [[117](#bib.bib117), [54](#bib.bib54)] 提供了关于命名实体识别（NER）问题的深度学习模型的良好文献调查，但这仅仅是序列标注的一个子任务。据我们所知，迄今为止还没有调查能提供关于基于深度学习的序列标注方法的详尽总结。鉴于深度学习模型在序列标注中的日益流行，系统调查具有较高的学术和实际意义。我们总结并分析了这些相关工作，并涵盖了本调查中的$100$多项研究。
- en: Contributions of this survey. The goal of this survey is to thoroughly review
    the recent applied techniques of deep learning in the filed of sequence labeling (SL),
    and provides a panoramic view to enlighten and guide the researchers and practitioners
    in SL research community for quickly understanding and stepping into this area.
    Specifically, we present a comprehensive survey on deep learning-based SL techniques
    to systematically summarize the state-of-the-arts with a scientific taxonomy along
    three axes, *i.e.,* *embedding module*, *context encoder module*, and *inference
    module*. In addition, we also present an overview on the experiment settings (*i.e.,*
    *dataset* or *evaluation metric*) for commonly studied tasks in sequence labeling
    domain. Besides, we have discussed and compared the results give by the most representative
    models for analyzing the effects of different factors and architectures. Finally,
    we present readers with the challenges and open issues faced by current DL-based
    sequence labeling methods and outline future directions in this area.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的贡献。本调查的目标是全面回顾深度学习在序列标注（SL）领域的最新应用技术，并提供全景视图，以启发和指导SL研究社区的研究人员和从业者，快速理解并进入这一领域。具体而言，我们呈现了基于深度学习的SL技术的综合调查，以系统地总结最前沿技术，并通过科学分类法沿三个轴线进行概述，即*嵌入模块*、*上下文编码模块*和*推理模块*。此外，我们还概述了序列标注领域常见任务的实验设置（即*数据集*或*评估指标*）。此外，我们还讨论并比较了最具代表性的模型给出的结果，以分析不同因素和架构的效果。最后，我们向读者呈现了当前基于深度学习的序列标注方法面临的挑战和未解决的问题，并概述了这一领域的未来方向。
- en: 'Roadmap. The remaining of this paper is organized as follows: Section [II](#S2
    "II Background ‣ A Survey on Recent Advances in Sequence Labeling from Deep Learning
    Models") introduces background of sequence labeling, consisting of several related
    tasks and traditional machine learning approaches. Section [III](#S3 "III Deep
    Learning Based Models ‣ A Survey on Recent Advances in Sequence Labeling from
    Deep Learning Models") presents deep learning models for sequence labeling based
    on our proposed taxonomy. Section [IV](#S4 "IV Evaluation Metrics and DataSets
    ‣ A Survey on Recent Advances in Sequence Labeling from Deep Learning Models")
    summarizes the experimental settings (*i.e.,* dataset and evaluation metric) for
    related tasks. Section [V](#S5 "V Comparisons on Experimental Results of Various
    Techniques ‣ A Survey on Recent Advances in Sequence Labeling from Deep Learning
    Models") lists the results of different methods, followed by the discussion of
    the promising future directions. Finally, Section [VII](#S7 "VII Conclusions ‣
    A Survey on Recent Advances in Sequence Labeling from Deep Learning Models") concludes
    this survey.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 路线图。本文的其余部分组织如下：第[II](#S2 "II Background ‣ A Survey on Recent Advances in Sequence
    Labeling from Deep Learning Models")节介绍了序列标注的背景，包括几个相关任务和传统机器学习方法。第[III](#S3 "III
    Deep Learning Based Models ‣ A Survey on Recent Advances in Sequence Labeling
    from Deep Learning Models")节展示了基于我们提出的分类法的深度学习模型。第[IV](#S4 "IV Evaluation Metrics
    and DataSets ‣ A Survey on Recent Advances in Sequence Labeling from Deep Learning
    Models")节总结了相关任务的实验设置（*即，*数据集和评估指标）。第[V](#S5 "V Comparisons on Experimental Results
    of Various Techniques ‣ A Survey on Recent Advances in Sequence Labeling from
    Deep Learning Models")节列出了不同方法的结果，并讨论了有前景的未来方向。最后，第[VII](#S7 "VII Conclusions
    ‣ A Survey on Recent Advances in Sequence Labeling from Deep Learning Models")节总结了本次调查。
- en: II Background
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: In this section, we first give an introduction of three widely-studied classical
    *sequence labeling* tasks, *i.e.,* *part-of-speech (POS) tagging*, *named entity
    recognition* (NER) and *text chunking*. Then, we briefly introduce the traditional
    machine learning based techniques in sequence labeling domain.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍三个广泛研究的经典*序列标注*任务，即*词性标注*（POS）、*命名实体识别*（NER）和*文本切分*。然后，我们简要介绍序列标注领域中基于传统机器学习的技术。
- en: II-A Classical Sequence Labeling Task
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 经典序列标注任务
- en: II-A1 Part-of-speech Tagging (POS)
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 词性标注（POS）
- en: POS receives a high degree of acceptance from both academia and industry, which
    is a standard sequence labeling task that aims at assigning a correct part-of-speech
    tag to each lexical item (*a.k.a.,* word) such as noun (NN), verb (VB) and adjective
    (JJ). In general, part-of-speech (POS) tagging can also be viewed as a subclass
    division of all words in a language, which is thus also called a word class. The
    tagging system of part-of-speech tags is not usually uniform under different data
    set, *e.g.,* PTB (Penn Treebank) [[72](#bib.bib72)], which includes $45$ different
    types of POS tags for word classification, such as for sentence “Mr. Jones is
    editor of the Journal”, it will be labeled with a sequence like ”NNP NNP VBZ NN
    IN DT NN”.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注在学术界和工业界都得到了高度认可，这是一项标准的序列标注任务，旨在为每个词汇项（*即，*单词）分配正确的词性标签，如名词（NN）、动词（VB）和形容词（JJ）。一般来说，词性标注（POS）也可以视为对语言中所有单词的子类划分，因此也称为词类。在不同的数据集下，词性标签的标注系统通常不是统一的，例如
    PTB（Penn Treebank）[[72](#bib.bib72)]，它包括$45$种不同类型的词性标签用于词汇分类，例如对于句子“Mr. Jones
    is editor of the Journal”，它将被标注为像“NNP NNP VBZ NN IN DT NN”的序列。
- en: In fact, Part-of-speech can be regarded as a coarse-grained word cluster task,
    the goal of which is to label the form and syntactic information of words in a
    sentence, which is benefit for alleviating the sparseness of word-level features,
    and servers as an important pre-processing step in natural language processing
    domain for various subsequent tasks like semantic role labeling or syntax analysis.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，词性标注可以视为一种粗粒度的词汇聚类任务，其目标是标注句子中单词的形式和句法信息，这有助于缓解词级特征的稀疏性，并且作为自然语言处理领域中多种后续任务如语义角色标注或句法分析的重要预处理步骤。
- en: II-A2 Named Entity Recognition (NER)
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 命名实体识别（NER）
- en: Name entity recognition (NER, *a.k.a.,* named entity identification or entity
    chunking), is a well-known classical sequence labeling task, the goal of which
    is to identify named entities from text belonging to pre-defined categories, which
    generally consists of three major categories (*i.e.,* entity, time, and numeric)
    and seven sub-categories (*i.e.,* person name, organization, location, time, date,
    currency, and percentage). Particularly, in this paper we mainly focus on the
    NER problem in English language, and a widely-adopted English taxonomy is CoNLL2003
    NER corpus [[97](#bib.bib97)], which is collected from Reuters News Corpus that
    includes four different types of named entities, *i.e.,* person (PER), location
    (LOC), organization (ORG) and proper nouns (MISC).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER，*即*命名实体识别或实体切分）是一个著名的经典序列标注任务，其目标是从文本中识别出属于预定义类别的命名实体，这些类别一般包括三个主要类别（*即*
    实体、时间和数字）以及七个子类别（*即* 人名、组织、地点、时间、日期、货币和百分比）。特别地，在本文中，我们主要关注英语中的NER问题，广泛采用的英语分类法是CoNLL2003
    NER语料库[[97](#bib.bib97)]，该语料库来自路透社新闻语料库，包括四种不同类型的命名实体，*即* 人物（PER）、地点（LOC）、组织（ORG）和专有名词（MISC）。
- en: Generally, the label of a word in NER is composed of two parts, *i.e.,* “X-Y”,
    where “X” indicates the position of the labeled word and “Y” refers to the corresponding
    category within a pre-defined taxonomy. In particular, it may be labeled with
    a special label (*e.g.,* “none”), if a word cannot be classified into any pre-defined
    category. Generally, the widely-adopted tagging scheme in the industry is BIOES
    system, that is, the word labeled “B” (Begin), “I” (Inside) and “E” (End) means
    that it is the first, middle or last word of a named entity phrase, respectively.
    The word labeled “0-” (Outside) means it does not belong to any named entity phrase
    and “S-” (Single) indicates it is the only word that represent an entity.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，NER中一个单词的标签由两个部分组成，*即* “X-Y”，其中“X”表示标记单词的位置，“Y”指的是预定义分类中的对应类别。特别地，如果一个单词不能被分类到任何预定义类别中，它可能会被标记为特殊标签（*例如*，“none”）。通常，行业中广泛采用的标记方案是BIOES系统，即被标记为“B”（开始）、“I”（内部）和“E”（结束）的单词分别表示它是命名实体短语的第一个、中间或最后一个单词。被标记为“0-”（外部）的单词表示它不属于任何命名实体短语，而“S-”（单独）表示它是唯一一个表示实体的单词。
- en: Named entity recognition is a very important task in natural language processing
    and is a basic technology for many high-level applications, such as search engine,
    question and answer systems, recommendation systems, translation systems, *etc.*
    Without loss of generality, we take machine translation as example to illustrate
    the importance of NER for various downstream tasks. In the process of translation,
    if the text contains named entity with a specific meaning, the translation system
    usually tends to translate multiple words that make up the named entity separately,
    resulting in blunt or even erroneous translation results. But if the named entity
    is identified first, the translation algorithm will have a better understanding
    of the word order and semantics of the text thus can output a better translation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别是自然语言处理中的一个非常重要的任务，是许多高级应用的基础技术，例如搜索引擎、问答系统、推荐系统、翻译系统，*等等*。一般来说，我们以机器翻译为例来阐明NER对各种下游任务的重要性。在翻译过程中，如果文本中包含具有特定含义的命名实体，翻译系统通常倾向于将组成命名实体的多个单词分开翻译，导致翻译结果生硬甚至错误。但如果首先识别出命名实体，翻译算法将更好地理解文本的词序和语义，从而能够输出更好的翻译结果。
- en: II-A3 Text Chunking
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 文本切分
- en: 'The goal of the text chunking task is to divide text into syntactically related
    non-overlapping groups of words, *i.e.,* phrase, such as noun phrase, verb phrase,
    *etc.* The task can be essentially regarded as a sequence labeling problem that
    assign specific labels to words in sentences. Similar with NER, it can also adopt
    the BIOES tagging system. For example, the sentence “The little dog barked at
    the cat.” can be divided into the following phrases: “(The little dog) (barked
    at) (the cat)”. Therefore, with the BIOES tagging system, the label sequence corresponding
    to this sentence is “B-NP I-NP E-NP B-VP E-VP B-NP E-NP”, which means that “The
    little dog” and “the cat” are noun phrases and “barted at” is a verb phrase.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 文本块任务的目标是将文本划分为在句法上相关且不重叠的词组，即短语，如名词短语、动词短语等。该任务本质上可以看作是一个序列标注问题，即为句子中的词分配特定标签。类似于NER，它也可以采用BIOES标记系统。例如，句子“The
    little dog barked at the cat.” 可以被划分为以下短语：“(The little dog) (barked at) (the cat)”。因此，使用BIOES标记系统，该句子的标签序列是“B-NP
    I-NP E-NP B-VP E-VP B-NP E-NP”，这意味着“The little dog”和“the cat”是名词短语，而“barked at”是动词短语。
- en: II-A4 Others
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A4 其他
- en: There have been many explorations into applying the sequence labeling framework
    to address other problems such as dependency parsing [[105](#bib.bib105), [60](#bib.bib60)],
    semantic role labeling [[82](#bib.bib82), [107](#bib.bib107)], answer selection [[132](#bib.bib132),
    [56](#bib.bib56)], text error detection [[93](#bib.bib93), [92](#bib.bib92)],
    document summarization [[77](#bib.bib77)], constituent parsing [[24](#bib.bib24)],
    sub-event detection [[4](#bib.bib4)], emotion detection in dialogues [[102](#bib.bib102)]
    and complex word identification [[25](#bib.bib25)].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 已有许多探索将序列标注框架应用于解决其他问题，例如依存句法分析 [[105](#bib.bib105), [60](#bib.bib60)], 语义角色标注 [[82](#bib.bib82),
    [107](#bib.bib107)], 答案选择 [[132](#bib.bib132), [56](#bib.bib56)], 文本错误检测 [[93](#bib.bib93),
    [92](#bib.bib92)], 文档摘要 [[77](#bib.bib77)], 成分句法分析 [[24](#bib.bib24)], 子事件检测 [[4](#bib.bib4)],
    对话中的情感检测 [[102](#bib.bib102)] 和复杂词汇识别 [[25](#bib.bib25)]。
- en: II-B Traditional Machine Learning Based Approaches
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 传统机器学习方法
- en: The traditional statistical machine learning techniques are the primary method
    for early sequence labeling problems. Based on the carefully designed features
    to represent each training data, the machine learning algorithms are utilized
    to train the model from example inputs and their expected outputs, learning to
    make predictions for unseen samples. Common statistical machine learning techniques
    include Hidden Markov Models (HMM) [[21](#bib.bib21)], Support Vector Machines
    (SVM) [[32](#bib.bib32)], Maximum Entropy Models [[41](#bib.bib41)] and Conditional
    Random Fields (CRF) [[51](#bib.bib51)]. HMM is a statistical model used to describe
    a Markov process with implicit unknown states. Bikel et al. [[7](#bib.bib7)] propose
    the first HMM-based model for NER system, named IdentiFinder. This model is extended
    by Zhou and Su [[131](#bib.bib131)] and achieves better performance by assumimg
    mutual information independence rather than conditional probability independence
    of HMM.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的统计机器学习技术是早期序列标注问题的主要方法。基于精心设计的特征来表示每个训练数据，机器学习算法被用于从示例输入及其期望输出中训练模型，学习对未见样本进行预测。常见的统计机器学习技术包括隐马尔可夫模型（HMM） [[21](#bib.bib21)],
    支持向量机（SVM） [[32](#bib.bib32)], 最大熵模型 [[41](#bib.bib41)] 和条件随机场（CRF） [[51](#bib.bib51)]。HMM是一种用于描述具有隐含未知状态的马尔可夫过程的统计模型。Bikel等人 [[7](#bib.bib7)]
    提出了第一个基于HMM的NER系统模型，名为IdentiFinder。该模型由Zhou和Su [[131](#bib.bib131)] 扩展，并通过假设互信息独立性而非HMM的条件概率独立性，取得了更好的性能。
- en: SVM, which is alleged large margin classifier, is well-known for the good generalization
    capabilities and has been successfully applied to many pattern recognition problems.
    In the field of sequence labeling, Kudoh and Matsumoto [[48](#bib.bib48)] first
    propose to apply SVM classifier to the phrase chunking task and achieve the best
    performance at the time. Several subsequent studies using SVM for NER tasks are
    successively proposed [[36](#bib.bib36), [59](#bib.bib59)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为大间隔分类器的支持向量机（SVM）因其良好的泛化能力而广受欢迎，并成功应用于许多模式识别问题。在序列标注领域，Kudoh和Matsumoto [[48](#bib.bib48)]
    首次提出将SVM分类器应用于短语块任务，并在当时取得了最佳表现。随后的几项研究也相继提出了使用SVM进行NER任务的应用 [[36](#bib.bib36),
    [59](#bib.bib59)]。
- en: Ratnaparkhi [[90](#bib.bib90)] proposes the first maximum entropy model for
    part-of-speech tagging, and achieves great results. Some work for NER also adopt
    the maximum entry model [[12](#bib.bib12), [5](#bib.bib5)]. The maximum entropy
    markov model is further proposed [[73](#bib.bib73)], which obtains a certain degree
    of improvement compared with the original maximum entropy model. Lafferty et al. [[51](#bib.bib51)]
    point out that utilizing the maximum entropy model for sequence labeling may suffer
    from a label bias problem. The proposed CRF model has achieved significant improvement
    in part-of-speech tagging and named entity recognition tasks and has gradually
    become the mainstream method of sequence labeling tasks [[74](#bib.bib74), [47](#bib.bib47)].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Ratnaparkhi [[90](#bib.bib90)] 提出了第一个最大熵模型用于词性标注，并取得了很好的结果。一些NER工作也采用了最大熵模型[[12](#bib.bib12),
    [5](#bib.bib5)]。最大熵马尔可夫模型进一步提出[[73](#bib.bib73)]，相比原始最大熵模型获得了一定程度的改进。Lafferty
    等人 [[51](#bib.bib51)] 指出，使用最大熵模型进行序列标注可能会遇到标签偏差问题。所提出的 CRF 模型在词性标注和命名实体识别任务中取得了显著改进，并逐渐成为序列标注任务的主流方法
    [[74](#bib.bib74), [47](#bib.bib47)]。
- en: III Deep Learning Based Models
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 基于深度学习的模型
- en: 'TABLE I: An overview of the deep learning based models for sequence lableing
    (LM: language model, pre LM emb: pretrained language model embedding, gaz: gazetteer,
    cap: capitalization, InNet: a funnel-shaped wide CNN architecture [[116](#bib.bib116)],
    AE: autoencoder, MO-BiLSTM: multi-order Bi-LSTM [[126](#bib.bib126)], INN: implicitly-defined
    neural network [[42](#bib.bib42)], EL-CRF: embedded-state latent CRF [[108](#bib.bib108)],
    SRL: semantic role labeling), SA: self-attention'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：基于深度学习的序列标注模型概述（LM：语言模型，pre LM emb：预训练语言模型嵌入，gaz：词典，cap：大写化，InNet：漏斗形宽 CNN
    架构[[116](#bib.bib116)]，AE：自编码器，MO-BiLSTM：多阶 Bi-LSTM [[126](#bib.bib126)]，INN：隐式定义的神经网络[[42](#bib.bib42)]，EL-CRF：嵌入状态潜在
    CRF[[108](#bib.bib108)]，SRL：语义角色标注），SA：自注意力
- en: '| Ref | Embedding Module | Context Encoder | Inference Module | Tasks |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 嵌入模块 | 上下文编码器 | 推理模块 | 任务 |'
- en: '| external input | word embedding | character-level |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 外部输入 | 词嵌入 | 字符级别 |'
- en: '|  [[71](#bib.bib71)] | $\backslash$ | Glove | CNN | Bi-LSTM | CRF | POS, NER
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  [[71](#bib.bib71)] | $\backslash$ | Glove | CNN | Bi-LSTM | CRF | POS, NER
    |'
- en: '|  [[8](#bib.bib8)] | $\backslash$ | Word2vec | Bi-LSTM | Bi-LSTM | Softmax
    | POS |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  [[8](#bib.bib8)] | $\backslash$ | Word2vec | Bi-LSTM | Bi-LSTM | Softmax
    | POS |'
- en: '|  [[121](#bib.bib121)] | $\backslash$ | Glove | Bi-LSTM | Bi-LSTM | CRF |
    POS |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  [[121](#bib.bib121)] | $\backslash$ | Glove | Bi-LSTM | Bi-LSTM | CRF |
    POS |'
- en: '|  [[65](#bib.bib65)] | $\backslash$ | Glove | Bi-LSTM+LM | Bi-LSTM | CRF |
    POS, NER, chunking |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  [[65](#bib.bib65)] | $\backslash$ | Glove | Bi-LSTM+LM | Bi-LSTM | CRF |
    POS, NER, chunking |'
- en: '|  [[88](#bib.bib88)] | $\backslash$ | Polyglot | Bi-LSTM | Bi-LSTM | CRF |
    POS |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  [[88](#bib.bib88)] | $\backslash$ | Polyglot | Bi-LSTM | Bi-LSTM | CRF |
    POS |'
- en: '|  [[91](#bib.bib91)] | $\backslash$ | Word2vec | Bi-LSTM | Bi-LSTM+LM | CRF
    | POS, NER, chunking |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  [[91](#bib.bib91)] | $\backslash$ | Word2vec | Bi-LSTM | Bi-LSTM+LM | CRF
    | POS, NER, chunking |'
- en: '|  [[85](#bib.bib85)] | $\backslash$ | Senna | CNN | Bi-LSTM+ pre LM | CRF
    | NER, chunking |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  [[85](#bib.bib85)] | $\backslash$ | Senna | CNN | Bi-LSTM+预训练 LM | CRF |
    NER, chunking |'
- en: '|  [[1](#bib.bib1)] | Pre LM emb | Glove | Bi-LSTM | Bi-LSTM | CRF | POS, NER,
    chunking |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  [[1](#bib.bib1)] | 预训练 LM 嵌入 | Glove | Bi-LSTM | Bi-LSTM | CRF | POS, NER,
    chunking |'
- en: '|  [[127](#bib.bib127)] | $\backslash$ | - | Bi-LSTM | Bi-LSTM | LSTM+Softmax
    | POS, NER |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  [[127](#bib.bib127)] | $\backslash$ | - | Bi-LSTM | Bi-LSTM | LSTM+Softmax
    | POS, NER |'
- en: '|  [[122](#bib.bib122)] | $\backslash$ | Glove | Bi-LSTM+LM | Bi-LSTM | CRF+Semi-CRF
    | NER |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  [[122](#bib.bib122)] | $\backslash$ | Glove | Bi-LSTM+LM | Bi-LSTM | CRF+Semi-CRF
    | NER |'
- en: '|  [[126](#bib.bib126)] | Spelling, gaz | Senna | $\backslash$ | MO-BiLSTM
    | Softmax | NER, chunking |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  [[126](#bib.bib126)] | 拼写，词典 | Senna | $\backslash$ | MO-BiLSTM | Softmax
    | NER, chunking |'
- en: '|  [[26](#bib.bib26)] | $\backslash$ | Word2vec | Bi-LSTM | Parallel Bi-LSTM
    | Softmax | NER |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  [[26](#bib.bib26)] | $\backslash$ | Word2vec | Bi-LSTM | 并行 Bi-LSTM | Softmax
    | NER |'
- en: '|  [[120](#bib.bib120)] | $\backslash$ | Senna, Glove | Bi-GRU | Bi-GRU | CRF
    | POS, NER, chunking |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  [[120](#bib.bib120)] | $\backslash$ | Senna, Glove | Bi-GRU | Bi-GRU | CRF
    | POS, NER, chunking |'
- en: '|  [[62](#bib.bib62)] | $\backslash$ | Trained on wikipedia | Bi-LSTM | Bi-LSTM
    | Softmax | POS |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  [[62](#bib.bib62)] | $\backslash$ | 在维基百科上训练 | Bi-LSTM | Bi-LSTM | Softmax
    | POS |'
- en: '|  [[13](#bib.bib13)] | Cap, lexicon | Senna | CNN | Bi-LSTM | CRF | NER |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  [[13](#bib.bib13)] | 大写化，词典 | Senna | CNN | Bi-LSTM | CRF | NER |'
- en: '|  [[92](#bib.bib92)] | $\backslash$ | Word2vec | Bi-LSTM | Bi-LSTM | CRF |
    POS, NER, chunking |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  [[92](#bib.bib92)] | $\backslash$ | Word2vec | Bi-LSTM | Bi-LSTM | CRF |
    POS, NER, chunking |'
- en: '|  [[116](#bib.bib116)] | $\backslash$ | Glove | InNet | Bi-LSTM | CRF | POS,
    NER, chunking |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  [[116](#bib.bib116)] | $\backslash$ | Glove | InNet | Bi-LSTM | CRF | POS,
    NER, chunking |'
- en: '|  [[42](#bib.bib42)] | Spelling, gaz | Senna | $\backslash$ | INN | Softmax
    | POS |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  [[42](#bib.bib42)] | 拼写，gaze | Senna | $\backslash$ | INN | Softmax | POS
    |'
- en: '|  [[108](#bib.bib108)] | $\backslash$ | Glove | $\backslash$ | Bi-LSTM | EL-CRF
    | Citation field extraction |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  [[108](#bib.bib108)] | $\backslash$ | Glove | $\backslash$ | Bi-LSTM | EL-CRF
    | 引文字段提取 |'
- en: '|  [[37](#bib.bib37)] | $\backslash$ | Trained with skip-gram | $\backslash$
    | Bi-LSTM | Skip-chain CRF | Clinical entities detection |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  [[37](#bib.bib37)] | $\backslash$ | 经过 skip-gram 训练 | $\backslash$ | Bi-LSTM
    | Skip-chain CRF | 临床实体检测'
- en: '|  [[115](#bib.bib115)] | Word shapes, gaz | Glove | CNN | Bi-LSTM | CRF |
    NER |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  [[115](#bib.bib115)] | 词形，gaze | Glove | CNN | Bi-LSTM | CRF | NER |'
- en: '|  [[17](#bib.bib17)] | Gaz, cap | Senna | $\backslash$ | CNN | CRF | POS,
    NER, chunking, SRL |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  [[17](#bib.bib17)] | gaze, cap | Senna | $\backslash$ | CNN | CRF | POS,
    NER, 分块, SRL |'
- en: '|  [[113](#bib.bib113)] | $\backslash$ | Glove | CNN | Gated-CNN | CRF | NER
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  [[113](#bib.bib113)] | $\backslash$ | Glove | CNN | Gated-CNN | CRF | NER
    |'
- en: '|  [[104](#bib.bib104)] | $\backslash$ | Word2vec | $\backslash$ | ID-CNN |
    CRF | NER |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  [[104](#bib.bib104)] | $\backslash$ | Word2vec | $\backslash$ | ID-CNN |
    CRF | NER |'
- en: '|  [[52](#bib.bib52)] | $\backslash$ | Word2vec | Bi-LSTM | Bi-LSTM | CRF |
    NER |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  [[52](#bib.bib52)] | $\backslash$ | Word2vec | Bi-LSTM | Bi-LSTM | CRF |
    NER |'
- en: '|  [[35](#bib.bib35)] | Spelling, gaz | Senna | $\backslash$ | Bi-LSTM | CRF
    | POS, NER, chunking |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  [[35](#bib.bib35)] | 拼写，gaze | Senna | $\backslash$ | Bi-LSTM | CRF | POS,
    NER, 分块 |'
- en: '|  [[99](#bib.bib99)] | $\backslash$ | Word2vec | CNN | CNN | CRF | POS |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  [[99](#bib.bib99)] | $\backslash$ | Word2vec | CNN | CNN | CRF | POS |'
- en: '|  [[124](#bib.bib124)] | $\backslash$ | Senna | CNN | Bi-LSTM | Pointer network
    | Chunking, slot filling |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  [[124](#bib.bib124)] | $\backslash$ | Senna | CNN | Bi-LSTM | 指针网络 | 分块,
    插槽填充 |'
- en: '|  [[130](#bib.bib130)] | $\backslash$ | Word2vec | $\backslash$ | Bi-LSTM
    | LSTM | Entity relation extraction |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  [[130](#bib.bib130)] | $\backslash$ | Word2vec | $\backslash$ | Bi-LSTM
    | LSTM | 实体关系提取 |'
- en: '|  [[23](#bib.bib23)] | LS vector, cap | SSKIP | Bi-LSTM | LSTM | CRF | NER
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  [[23](#bib.bib23)] | LS 向量, cap | SSKIP | Bi-LSTM | LSTM | CRF | NER |'
- en: '|  [[103](#bib.bib103)] | $\backslash$ | Word2vec | CNN | CNN | LSTM | NER
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  [[103](#bib.bib103)] | $\backslash$ | Word2vec | CNN | CNN | LSTM | NER
    |'
- en: '|  [[55](#bib.bib55)] | $\backslash$ | Glove | $\backslash$ | Bi-GRU | Pointer
    network | Text segmentation |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  [[55](#bib.bib55)] | $\backslash$ | Glove | $\backslash$ | Bi-GRU | 指针网络
    | 文本分割 |'
- en: '|  [[27](#bib.bib27)] | $\backslash$ | - | CNN | Bi-LSTM | Softmax | POS |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  [[27](#bib.bib27)] | $\backslash$ | - | CNN | Bi-LSTM | Softmax | POS |'
- en: '|  [[20](#bib.bib20)] | $\backslash$ | Word2vec, FastText | LSTM+attention
    | Bi-LSTM | Softmax | POS |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  [[20](#bib.bib20)] | $\backslash$ | Word2vec, FastText | LSTM+attention
    | Bi-LSTM | Softmax | POS |'
- en: '|  [[34](#bib.bib34)] | $\backslash$ | Glove | CNN | Bi-LSTM | NCRF transducers
    | POS, NER, chunking |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  [[34](#bib.bib34)] | $\backslash$ | Glove | CNN | Bi-LSTM | NCRF 变换器 | POS,
    NER, 分块 |'
- en: '|  [[40](#bib.bib40)] | $\backslash$ | - | Bi-LSTM+AE | Bi-LSTM | softmax |
    POS |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  [[40](#bib.bib40)] | $\backslash$ | - | Bi-LSTM+AE | Bi-LSTM | softmax |
    POS |'
- en: '|  [[101](#bib.bib101)] | Lexicons | Glove | CNN | Bi-LSTM | Segment-level
    CRF | NER |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  [[101](#bib.bib101)] | 词典 | Glove | CNN | Bi-LSTM | 分段级 CRF | NER |'
- en: '|  [[10](#bib.bib10)] | $\backslash$ | Glove | CNN | GRN+CNN | CRF | NER |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  [[10](#bib.bib10)] | $\backslash$ | Glove | CNN | GRN+CNN | CRF | NER |'
- en: '|  [[114](#bib.bib114)] | $\backslash$ | Glove | CNN | Bi-LSTM+SA | CRF | POS,
    NER, chunking |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  [[114](#bib.bib114)] | $\backslash$ | Glove | CNN | Bi-LSTM+SA | CRF | POS,
    NER, 分块 |'
- en: 'In this section, we survey deep learning based approaches for sequence labeling.
    We present the review with a scientific taxonomy that categorize existing works
    along three axes: embedding module, context encoder module, and inference module,
    of which three stages neural sequence labeling models often consists. The embedding
    module is the first stage that maps words into their distributed representations.
    The context encoder module extracts contextual features and the inference module
    predict labels and generate optimal label sequence as output of the model. In
    Table [I](#S3.T1 "TABLE I ‣ III Deep Learning Based Models ‣ A Survey on Recent
    Advances in Sequence Labeling from Deep Learning Models"), we make a brief overview
    of the deep learning based sequence labeling models with the aforementioned taxonomy.
    We list the different architectures that these work adopt in the three stages
    and the final column give the focused tasks.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了基于深度学习的序列标注方法。我们通过一种科学分类法呈现评述，将现有工作按嵌入模块、上下文编码模块和推理模块三个维度分类，这三个阶段通常构成神经序列标注模型。嵌入模块是第一个阶段，将词映射到其分布式表示。上下文编码模块提取上下文特征，而推理模块预测标签并生成模型输出的最佳标签序列。在表格
    [I](#S3.T1 "TABLE I ‣ III Deep Learning Based Models ‣ A Survey on Recent Advances
    in Sequence Labeling from Deep Learning Models") 中，我们简要概述了基于深度学习的序列标注模型，采用上述分类法。我们列出了这些工作在三个阶段采用的不同架构，最后一列给出了关注的任务。
- en: III-A Embedding Module
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 嵌入模块
- en: The embedding module maps words into their distributed representations as the
    initial input of model. An embedding lookup table is usually required to convert
    the one-hot encoding of each word to a low dimensional real-valued dense vector,
    where each dimension represents a latent feature. In addition to pretrained word
    embeddings, character-level representations, hand-crafted features and sentence-level
    representations can also be part of the embedding module, supplementing features
    for the initial input from different perspectives.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模块将词映射到其分布式表示作为模型的初始输入。通常需要一个嵌入查找表，将每个词的独热编码转换为一个低维的实值稠密向量，其中每个维度代表一个潜在特征。除了预训练词嵌入，字符级表示、手工设计特征和句子级表示也可以成为嵌入模块的一部分，从不同角度补充初始输入的特征。
- en: III-A1 Pretrained Word Embeddings
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 预训练词嵌入
- en: Pretrained word embeddings that learned on a large corpus of unlabeled data
    has become a key component in many neural NLP models. Adopting it to initialize
    the embedding lookup table can achieve significant improvements over randomly
    initialized ones, since the syntactic and semantic information within language
    are captured during the pretraining process. There are many published pretrained
    word embeddings that have been widely used, such as Word2Vec, Senna, GloVe and
    *etc.*
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在大量未标记数据上学习的预训练词嵌入已成为许多神经网络自然语言处理模型的关键组成部分。采用它来初始化嵌入查找表，可以显著改善与随机初始化相比的效果，因为在预训练过程中捕捉到了语言中的句法和语义信息。许多已发布的预训练词嵌入被广泛使用，例如
    Word2Vec、Senna、GloVe 等。
- en: Word2vec [[75](#bib.bib75)] is a popular method to compute vector representations
    of words, which provides two model architectures including the continuous bag-of-words
    and skip-gram. Santos et al. [[99](#bib.bib99)] use word2vec’s skip-gram method
    to train word embeddings and added to their sequence labeling model. Similarly,
    some work [[91](#bib.bib91), [92](#bib.bib92), [20](#bib.bib20)] initialize the
    word embeddings in their model with publicly available pretrained vectors that
    created using word2vec. Lample et al. [[52](#bib.bib52)] apply skip-n-gram [[63](#bib.bib63)]
    to pretrain their word embeddings, which is a variation of word2vec that accounts
    for word order. Gregoric et al. [[26](#bib.bib26)] follow their work and also
    used the same embeddings.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec [[75](#bib.bib75)] 是一种流行的词向量表示方法，提供了包括连续词袋模型和跳字模型在内的两种模型架构。Santos 等人 [[99](#bib.bib99)]
    使用 word2vec 的跳字模型来训练词嵌入，并将其添加到他们的序列标注模型中。类似地，一些工作 [[91](#bib.bib91), [92](#bib.bib92),
    [20](#bib.bib20)] 用公开的预训练词向量初始化他们的模型，这些向量是使用 word2vec 创建的。Lample 等人 [[52](#bib.bib52)]
    应用 skip-n-gram [[63](#bib.bib63)] 预训练他们的词嵌入，这是一种考虑词序的 word2vec 变体。Gregoric 等人 [[26](#bib.bib26)]
    跟随他们的工作，也使用了相同的词嵌入。
- en: Collobert et al. [[17](#bib.bib17)] propose the “SENNA” architecture in 2011,
    which pioneers the idea of solving natural language processing tasks from the
    perspective of neural language model, and it also includes a construction method
    of pretrained word embeddings. Many subsequent work [[85](#bib.bib85), [126](#bib.bib126),
    [13](#bib.bib13), [35](#bib.bib35), [119](#bib.bib119), [70](#bib.bib70)] adopt
    SENNA word embeddings as the initial input of their sequence labeling models.
    Besides, Stanford’s publicly available GloVe embeddings [[84](#bib.bib84)] that
    trained on $6$ billion token corpus from Wikipedia and web text are also widely
    used and adopted by many work  [[71](#bib.bib71), [121](#bib.bib121), [65](#bib.bib65),
    [1](#bib.bib1), [122](#bib.bib122), [116](#bib.bib116), [55](#bib.bib55), [34](#bib.bib34),
    [10](#bib.bib10)] to initilize their word embeddings.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Collobert 等人 [[17](#bib.bib17)] 在2011年提出了“SENNA”架构，开创了从神经语言模型的角度解决自然语言处理任务的思路，并且包括了预训练词嵌入的构建方法。许多后续的工作 [[85](#bib.bib85),
    [126](#bib.bib126), [13](#bib.bib13), [35](#bib.bib35), [119](#bib.bib119), [70](#bib.bib70)]
    将SENNA词嵌入作为其序列标注模型的初始输入。此外，斯坦福公开的GloVe词嵌入 [[84](#bib.bib84)] 是在来自维基百科和网页文本的60亿标记语料库上训练的，也被许多工作 [[71](#bib.bib71),
    [121](#bib.bib121), [65](#bib.bib65), [1](#bib.bib1), [122](#bib.bib122), [116](#bib.bib116),
    [55](#bib.bib55), [34](#bib.bib34), [10](#bib.bib10)] 广泛使用和采纳，用于初始化其词嵌入。
- en: The above pretrained word embedding methods only generate a single context-independent
    vector for each word, ignoring the modeling of polysemy problem. Recently, many
    approaches for learning contextual word representations [[85](#bib.bib85), [86](#bib.bib86),
    [1](#bib.bib1)] have been proposed, where bidirectional language models (LM) are
    trained on a large unlabeled corpus and the corresponding internal states are
    utilized to produce a word representation. And the representation of each word
    is dependent on its context. For instance, the generated embedding of the word
    “present” in “How many people were present at the meeting?” is different from
    that in “I’m not at all satisfied with the present situation”.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 上述预训练词嵌入方法仅生成每个词的单一上下文无关向量，忽略了多义性问题的建模。最近，许多学习上下文词表示的方法 [[85](#bib.bib85), [86](#bib.bib86),
    [1](#bib.bib1)] 已被提出，其中双向语言模型（LM）在大规模未标注语料上训练，利用相应的内部状态生成词表示。每个词的表示依赖于其上下文。例如，“How
    many people were present at the meeting?” 中“present”的生成嵌入与 “I’m not at all satisfied
    with the present situation” 中的不同。
- en: Peters et al. [[85](#bib.bib85)] propose pretrained contextual embeddings from
    bidirectional language models and added them to sequence labeling model, achieving
    pretty excellent performance on the task of NER and chunking. The method first
    pretrains the forward and backward neural language model separately with Bi-LSTM
    architecture on a large, unlabeled corpus. Then it removes the top softmax layer
    and concatenates the forward and backward LM embeddings to form bidirectional
    LM embeddings for every token in a given input sequence. Peters et al. extend
    their method [[85](#bib.bib85)] in  [[86](#bib.bib86)] by introducing ELMo (Embeddings
    from Language Models) representations. Unlike previous approaches that just utilize
    the top LSTM layer, the ELMo representations are a linear combination of internal
    states of all bidirectional LM layers, where the weight of each layer is task-specific.
    By adding these representations to existing models, the method significantly improves
    the performance across a broad range of diverse NLP tasks [[10](#bib.bib10), [34](#bib.bib34),
    [15](#bib.bib15)].
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Peters 等人 [[85](#bib.bib85)] 提出了来自双向语言模型的预训练上下文嵌入，并将其添加到序列标注模型中，在命名实体识别（NER）和块分割任务上取得了非常优异的性能。该方法首先使用
    Bi-LSTM 结构在大规模未标注语料上分别预训练前向和后向神经语言模型。然后，移除顶部 softmax 层，并将前向和后向语言模型嵌入连接起来，形成给定输入序列中每个
    token 的双向语言模型嵌入。Peters 等人通过引入 ELMo（来自语言模型的嵌入）在 [[86](#bib.bib86)] 中扩展了他们的方法 [[85](#bib.bib85)]。与仅使用顶部
    LSTM 层的先前方法不同，ELMo 表示是所有双向语言模型层的内部状态的线性组合，其中每一层的权重是特定任务的。通过将这些表示添加到现有模型中，该方法显著提高了各种
    NLP 任务的性能 [[10](#bib.bib10), [34](#bib.bib34), [15](#bib.bib15)]。
- en: Akbik et al. [[1](#bib.bib1)] propose a similar method to generate pretrained
    contextual word embeddings by adopting a bidirectional character-aware language
    model, which learns to predict the next and previous character instead of word.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Akbik 等人 [[1](#bib.bib1)] 提出了类似的方法，通过采用双向字符感知语言模型生成预训练的上下文词嵌入，该模型学习预测下一个和上一个字符，而不是词。
- en: Devlin et al. [[19](#bib.bib19)] propose a pretraining language representation
    model called BERT, which stands for Bidirectional Encoder Representations from
    Transformers. It obtains new state-of-the-art results on eleven tasks and causes
    a sensation in the NLP communities. The core idea of BERT is to pretrain deep
    bidirectional representations by jointly conditioning on both left and right context
    in all layers. Although the sequence labeling tasks can be addressed by fine-tuning
    the existing pre-trained BERT model, the output hidden states of BERT can also
    be taken as additional word embeddings to promote the performance of sequence
    labeling models [[67](#bib.bib67), luo2020hierarchica].
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Devlin 等人 [[19](#bib.bib19)] 提出了一个名为 BERT 的预训练语言表示模型，BERT 代表双向编码器表示来自变换器。它在十一项任务上取得了新的最先进结果，并在
    NLP 社区引起了轰动。BERT 的核心思想是通过在所有层中共同条件化左右上下文来预训练深度双向表示。虽然序列标注任务可以通过微调现有的预训练 BERT 模型来解决，但
    BERT 的输出隐藏状态也可以作为额外的词嵌入，以提升序列标注模型的性能 [[67](#bib.bib67), luo2020hierarchica]。
- en: By modeling the context information, the word representations produced by ELMo
    and BERT can encode rich semantic information. In addition to such context modeling,
    a recent work proposed by He et al. [[31](#bib.bib31)] provide a new kind of word
    embedding that is both context-aware and knowledge-aware, which encode the prior
    knowledge of entities from an external knowledge base. The proposed knowledge-graph
    augmented word representations significantly promotes the performance of NER in
    various domains.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过建模上下文信息，ELMo 和 BERT 生成的词表示可以编码丰富的语义信息。除了这种上下文建模，最近 He 等人 [[31](#bib.bib31)]
    提出的工作提供了一种新的词嵌入，这种嵌入既关注上下文又关注知识，编码了来自外部知识库的实体先验知识。所提出的知识图谱增强词表示显著提升了各种领域命名实体识别（NER）的性能。
- en: III-A2 Character-level Representations
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 字符级表示
- en: Although syntactic and semantic information are captured inside the pretrained
    word embeddings, the word morphological and shape information is normally ignored,
    which is extremely useful for many sequence labeling tasks like part-of-speech
    tagging. Recently, many researches learn the character-level representations of
    words through neural networks and incorporate them into the embedding module of
    models to exploit useful intra-word information, which can also tackle the out-of-vocabulary
    word problem effectively and has been verified to be helpful in numerous sequence
    labeling tasks. The two most common architectures to capture character-to-word
    representations are Convolutional Neural Networks(CNNs) and Recurrent Neural Networks(RNNs).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管预训练的词嵌入中捕获了句法和语义信息，但通常忽略了词的形态和形状信息，这对于许多序列标注任务，如词性标注，极为重要。最近，许多研究通过神经网络学习单词的字符级表示，并将其纳入模型的嵌入模块，以利用有用的词内信息，这也能有效解决词汇表外词汇问题，并已被证明对众多序列标注任务有帮助。捕捉字符到词的表示的两种最常见的架构是卷积神经网络（CNNs）和递归神经网络（RNNs）。
- en: Convolutional Neural Networks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络。
- en: Santos and Zadrozny [[99](#bib.bib99)] initially propose the approach that using
    CNNs to learn character-level representations of words for sequence labeling,
    which is followed by many subsequent work [[71](#bib.bib71), [13](#bib.bib13),
    [103](#bib.bib103), [10](#bib.bib10), [115](#bib.bib115), [15](#bib.bib15), [129](#bib.bib129)].
    The approach applies a convolutional operation to the sequence of character embeddings
    and produces local features of each character. Then a fixed-sized character-level
    embedding of the word is extracted by using the max over all character windows.
    The process is depicted in Fig [1](#S3.F1 "Figure 1 ‣ III-A2 Character-level Representations
    ‣ III-A Embedding Module ‣ III Deep Learning Based Models ‣ A Survey on Recent
    Advances in Sequence Labeling from Deep Learning Models").
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Santos 和 Zadrozny [[99](#bib.bib99)] 最初提出了一种方法，使用 CNN 来学习单词的字符级表示用于序列标注，这种方法随后被许多后续工作所采用
    [[71](#bib.bib71), [13](#bib.bib13), [103](#bib.bib103), [10](#bib.bib10), [115](#bib.bib115),
    [15](#bib.bib15), [129](#bib.bib129)]。该方法对字符嵌入序列应用卷积操作，并生成每个字符的局部特征。然后，通过对所有字符窗口进行最大池化，提取单词的固定大小字符级嵌入。该过程在图
    [1](#S3.F1 "图 1 ‣ III-A2 字符级表示 ‣ III-A 嵌入模块 ‣ III 基于深度学习的模型 ‣ 对深度学习模型在序列标注中的最新进展的综述")
    中进行了描述。
- en: '![Refer to caption](img/782c8246a9f50f41477ffb0cdb9d4b78.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/782c8246a9f50f41477ffb0cdb9d4b78.png)'
- en: 'Figure 1: Convolutional approach to character-level feature extraction [[99](#bib.bib99)].'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：卷积方法进行字符级特征提取 [[99](#bib.bib99)]。
- en: Xin et al. [[116](#bib.bib116)] propose IntNet, a funnel-shaped wide convolutional
    neural network for learning character-level representations for sequence labeling.
    Unlike previous CNN-based character embedding approaches, this method delicately
    designs the convolutional block that comprises of several consecutive operations,
    and utilizes multiple convolutional layers in which feature maps are concatenated
    in every other ones. It helps the network to capture different levels of features
    and explore the full potential of CNNs to learn better internal structure of words.
    The proposed model achieves significant improvements over other character embedding
    models and obtains state-of-the-art performance on various sequence labeling datasets.
    Its main architecture can be shown in Fig [2](#S3.F2 "Figure 2 ‣ III-A2 Character-level
    Representations ‣ III-A Embedding Module ‣ III Deep Learning Based Models ‣ A
    Survey on Recent Advances in Sequence Labeling from Deep Learning Models").
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Xin 等人 [[116](#bib.bib116)] 提出了 IntNet，这是一种漏斗形宽卷积神经网络，用于学习字符级表示以进行序列标注。与之前基于
    CNN 的字符嵌入方法不同，这种方法精心设计了包含多个连续操作的卷积块，并利用多个卷积层，其中特征图在每隔一层中连接。这有助于网络捕获不同级别的特征，并充分发挥
    CNN 学习单词内部结构的潜力。该模型在其他字符嵌入模型中取得了显著的改进，并在各种序列标注数据集上获得了最先进的性能。其主要架构见图 [2](#S3.F2
    "Figure 2 ‣ III-A2 Character-level Representations ‣ III-A Embedding Module ‣
    III Deep Learning Based Models ‣ A Survey on Recent Advances in Sequence Labeling
    from Deep Learning Models")。
- en: '![Refer to caption](img/5a56b454c3408b31fb4c18f4649ba1b7.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5a56b454c3408b31fb4c18f4649ba1b7.png)'
- en: 'Figure 2: The main architecture of IntNet [[116](#bib.bib116)].'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：IntNet 的主要架构 [[116](#bib.bib116)]。
- en: Recurrent Neural Networks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络。
- en: Ling et al. [[62](#bib.bib62)] propose a compositional character to word (C2W)
    model that uses bidirectional LSTMs (Bi-LSTM) to build word embeddings by taking
    the characters as atomic units. A forward and a backward LSTM processes the character
    embeddings sequence of a word in direct and reverse order. And the representaion
    for a word derived from its characters is obtained by combining the final states
    of the bidirectional LSTM. Illustration of the proposed method is shown in Fig
    [3](#S3.F3 "Figure 3 ‣ III-A2 Character-level Representations ‣ III-A Embedding
    Module ‣ III Deep Learning Based Models ‣ A Survey on Recent Advances in Sequence
    Labeling from Deep Learning Models"). By exploiting the features in language effectively,
    the C2W model yields excellent results in language modeling and part-of-speech
    tagging. And many work [[52](#bib.bib52), [127](#bib.bib127), [26](#bib.bib26),
    [121](#bib.bib121), [88](#bib.bib88)] follow them to apply Bi-LSTM for obtaining
    character-level representations for sequence labeling. Similarly, Yang et al. [[120](#bib.bib120)]
    employ GRUs for the character embedding model instead of LSTM units.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Ling 等人 [[62](#bib.bib62)] 提出了一个组成字符到单词（C2W）模型，该模型使用双向 LSTM（Bi-LSTM）通过将字符作为基本单位来构建单词嵌入。一个前向
    LSTM 和一个反向 LSTM 以直接和反向的顺序处理单词的字符嵌入序列。从其字符获得的单词表示是通过结合双向 LSTM 的最终状态获得的。所提出方法的示意图见图
    [3](#S3.F3 "Figure 3 ‣ III-A2 Character-level Representations ‣ III-A Embedding
    Module ‣ III Deep Learning Based Models ‣ A Survey on Recent Advances in Sequence
    Labeling from Deep Learning Models")。通过有效利用语言中的特征，C2W 模型在语言建模和词性标注中取得了优异的结果。许多工作 [[52](#bib.bib52),
    [127](#bib.bib127), [26](#bib.bib26), [121](#bib.bib121), [88](#bib.bib88)] 继续使用
    Bi-LSTM 获取字符级表示用于序列标注。类似地，Yang 等人 [[120](#bib.bib120)] 使用 GRU 替代 LSTM 单元进行字符嵌入模型。
- en: '![Refer to caption](img/3839f9f418a080fdfcbb12173f76fdaa.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3839f9f418a080fdfcbb12173f76fdaa.png)'
- en: 'Figure 3: Illustration of the lexical Composition Model [[62](#bib.bib62)].'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：词汇组成模型的示意图 [[62](#bib.bib62)]。
- en: Dozat et al. [[20](#bib.bib20)] propose a RNN based character-level model in
    which the character embeddings sequence of each word is fed into a unidirectional
    LSTM followed by an attention mechanism. The method first extracts the hidden
    state and cell state of each character from the LSTM and then computes linear
    attention over the hidden states. The output of attention is concatenated with
    cell state of the final character to form the character-level word embedding for
    their POS tagging model.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Dozat 等人 [[20](#bib.bib20)] 提出了基于 RNN 的字符级模型，其中每个单词的字符嵌入序列输入到一个单向 LSTM 后，再经过一个注意力机制。该方法首先从
    LSTM 中提取每个字符的隐藏状态和单元状态，然后对隐藏状态进行线性注意力计算。注意力的输出与最终字符的单元状态连接，形成字符级单词嵌入，用于他们的词性标注模型。
- en: Kann et al. [[40](#bib.bib40)] propose a character-based recurrent sequence-to-sequence
    architecture, which connects the Bi-LSTM character encoding model to a LSTM based
    decoder that associated with an auxiliary objective (random string autoencoding,
    word autoencoding or lemmatization). The multi-task architecture introduces additional
    character-level supervision into the model, which helps them build a more robust
    neural POS taggers for low-resource languages.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Kann 等人 [[40](#bib.bib40)] 提出了一个基于字符的递归序列到序列架构，该架构将 Bi-LSTM 字符编码模型与 LSTM 基于解码器连接，并与一个辅助目标（随机字符串自编码、词自编码或词形还原）相关联。该多任务架构为模型引入了额外的字符级监督，有助于构建更强大的低资源语言神经
    POS 标注器。
- en: Bohnet et al. [[8](#bib.bib8)] propose a novel sentence-level character model
    for learning context sensitive character-based representations of words. Unlike
    all aforementioned token-level character model, this method feeds all characters
    of a sentence into a Bi-LSTM layer and concatenates the forward and backward output
    vector of the first and last character in the word to form its final character-level
    representation. This strategy allows context information to be incorporated in
    the initial word embeddings before flowing into the context encoder module. Similarly,
    Liu et al. [[65](#bib.bib65)] also adopt the character-level Bi-LSTM that processes
    all characters of a sentence instead of a word. However, their proposed model
    focuses on extracting knowledge from raw texts by leveraging the neural language
    model to effectively extract character-level information. In particular, the forward
    and backward character-level LSTM would predict the next and previous word at
    word boundaries. In order to mediate the primary sequence labeling task and the
    auxiliary language model task, highway networks are further employed, which transform
    the output of the shared character-level layer into two different representations.
    One is used for language model and the other can be viewed as character-level
    representation that combined with the word embedding for sequence labeling model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Bohnet 等人 [[8](#bib.bib8)] 提出了一个新颖的句子级字符模型，用于学习上下文敏感的字符级词表示。与所有上述的标记级字符模型不同，该方法将句子的所有字符输入到
    Bi-LSTM 层，并将词中第一个和最后一个字符的前向和后向输出向量拼接在一起，形成其最终的字符级表示。这一策略允许在流入上下文编码器模块之前将上下文信息纳入初始词嵌入。类似地，Liu
    等人 [[65](#bib.bib65)] 也采用了处理句子中所有字符的字符级 Bi-LSTM，而不是单一词的字符级 Bi-LSTM。然而，他们提出的模型重点在于通过利用神经语言模型从原始文本中有效提取字符级信息。特别地，前向和后向字符级
    LSTM 将预测词边界的下一个和上一个词。为了调解主要的序列标注任务和辅助的语言模型任务，进一步采用了高速公路网络，将共享字符级层的输出转换为两种不同的表示。一种用于语言模型，另一种可以视为字符级表示，并与词嵌入结合用于序列标注模型。
- en: III-A3 Hand-crafted features
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 手工特征
- en: As aforementioned, enabled by the powerful capacity to extract features automatically,
    deep neural network based models have the advantage of not requiring complex feature
    engineering. However, before fully end-to-end deep learning models [[71](#bib.bib71),
    [52](#bib.bib52)] are proposed for sequence labeling tasks, feature engineering
    is typically utilized in neural models [[17](#bib.bib17), [35](#bib.bib35), [13](#bib.bib13)],
    where hand-crafted features such as word spelling features that can greatly benefits
    POS tagging and gazetteer features that are widly used in NER are represented
    as discrete vectors and then integrated to the embedding module. For example,
    Collobert et al. [[17](#bib.bib17)] utilize word suffix, gazetteer and capitalization
    features as well as cascading features that include tags from related tasks. Huang
    et al. [[35](#bib.bib35)] adopt designed spelling features (include word prefix
    and suffix features, capitalization feature *etc.*), context features (unigram,
    bi-gram and tri-gram features) and gazetteer features. Chiu and Nichols [[13](#bib.bib13)]
    use character-type, capitalization, and lexicon features.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，借助自动提取特征的强大能力，基于深度神经网络的模型具有不需要复杂特征工程的优势。然而，在完全端到端的深度学习模型[[71](#bib.bib71),
    [52](#bib.bib52)]用于序列标注任务之前，特征工程通常在神经模型[[17](#bib.bib17), [35](#bib.bib35), [13](#bib.bib13)]中使用，其中手工特征，如对POS标注大有裨益的单词拼写特征和广泛用于NER的词典特征，被表示为离散向量，然后集成到嵌入模块中。例如，Collobert
    等人[[17](#bib.bib17)]利用词后缀、词典和大写特征以及包含相关任务标签的级联特征。Huang 等人[[35](#bib.bib35)]采用了设计的拼写特征（包括词前缀和后缀特征、大写特征*等*），上下文特征（单字、双字和三字特征）和词典特征。Chiu
    和 Nichols[[13](#bib.bib13)]使用了字符类型、大写和词汇特征。
- en: In recent two years, there have been some work [[115](#bib.bib115), [23](#bib.bib23),
    [94](#bib.bib94), [61](#bib.bib61), [66](#bib.bib66)] that focus on incorporating
    manual features into neural models in a more effective manner and obtain significant
    further improvements for sequence labeling. Wu et al. [[115](#bib.bib115)] propose
    a hybrid neural model which combines a feature auto-encoder loss component to
    utilize hand-crafted features, and significantly outperforms existing competitive
    models on the task of NER. Exploited manual features include part-of-speech tags,
    word shapes and gazetteers. In particular, the auto-encoder auxiliary component
    takes hand-crafted features as input and learns to re-construct them into output,
    which helps the model to preserve important information stored in these features
    and thus enhances the primary sequence labeling task. Their proposed method has
    demonstrated the utility of hand-crafted features for named entity recognition
    on English data. However, designing such features for low-resource languages is
    challenging, because gazetteers in these languages are absent. To address this
    proplem, Rijhwani et al. [[94](#bib.bib94)] propose a method of ‘”soft gazetteers”
    that incorporates information from English knowledge bases through cross-lingual
    entity linking and create continuous-valued gazetteer features for low-resource
    languages.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，有一些工作[[115](#bib.bib115), [23](#bib.bib23), [94](#bib.bib94), [61](#bib.bib61),
    [66](#bib.bib66)]专注于将手工特征更有效地融入神经模型中，并为序列标注取得了显著的进一步改进。Wu 等人[[115](#bib.bib115)]提出了一种混合神经模型，该模型结合了特征自编码器损失组件来利用手工特征，并在命名实体识别任务上显著优于现有的竞争模型。利用的手工特征包括词性标签、单词形状和词典。特别是，自编码器辅助组件将手工特征作为输入，并学习将其重构为输出，这有助于模型保留这些特征中存储的重要信息，从而增强主要的序列标注任务。他们提出的方法展示了手工特征在英文数据上进行命名实体识别的实用性。然而，为低资源语言设计这样的特征是具有挑战性的，因为这些语言中缺乏词典。为解决这个问题，Rijhwani
    等人[[94](#bib.bib94)]提出了一种“软词典”的方法，通过跨语言实体链接从英文知识库中获取信息，为低资源语言创建连续值的词典特征。
- en: Ghaddar et al. [[23](#bib.bib23)] propose a novel lexical representation (called
    Lexical Similarity *i.e.,* (LS) vector) for NER, indicating that robust lexical
    features are quiet useful and can greatly benefit deep neural network architectures.
    The method first embeds words and named entity types into a joint low-dimensional
    vector space, which is trained from a Wikipedia corpus annotated with 120 fine-grained
    entity types. Then a 120-dimensional feature vector (*i.e.,* LS vector) for each
    word is computed offline, where each dimension encodes the similarity of the word
    embedding with the embedding of an entity type. The LS vectors are finally incorporated
    into the embedding module of their neural NER model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Ghaddar 等人[[23](#bib.bib23)]提出了一种新颖的词汇表示（称为词汇相似度 *即，*（LS）向量）用于命名实体识别，表明强大的词汇特征非常有用，并且可以大大提升深度神经网络架构。该方法首先将词汇和命名实体类型嵌入到一个联合的低维向量空间，该空间从一个注释了120个细粒度实体类型的维基百科语料库中训练而来。然后，计算每个词的120维特征向量（*即，*
    LS 向量），其中每个维度编码词嵌入与实体类型嵌入的相似度。最后，将LS向量纳入他们的神经NER模型的嵌入模块中。
- en: III-A4 Sentence-level Representations
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 句子级别的表示
- en: '![Refer to caption](img/c2272efaac21689c8b05095c1a677c67.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c2272efaac21689c8b05095c1a677c67.png)'
- en: 'Figure 4: The global contextual encoder (on the right) outputs sentence-level
    representation that serves as an enhancement of token representation. [[67](#bib.bib67)].'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：全局上下文编码器（右侧）输出句子级别的表示，作为词表示的增强[[67](#bib.bib67)]。
- en: Existing research [[128](#bib.bib128)] has proved that the global contextual
    information from the entire sentence is useful for modeling sequence, which is
    insufficiently captured at each token position in context encoder like Bi-LSTM.
    To solve this problem, some recent work [[67](#bib.bib67), [68](#bib.bib68)] have
    introduced sentence-level representations into the embedded module, that is, in
    addition to pretrained word embeddings and character-level representations, they
    also assign every word with a global representation learned from the entire sentence,
    which can be shown in Fig [4](#S3.F4 "Figure 4 ‣ III-A4 Sentence-level Representations
    ‣ III-A Embedding Module ‣ III Deep Learning Based Models ‣ A Survey on Recent
    Advances in Sequence Labeling from Deep Learning Models"). Though these work propose
    different ways to get sentence representation, they all prove the superiority
    of adding it in the embedding module for the final performance of sequence labeling
    tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现有研究[[128](#bib.bib128)]已证明，从整个句子获取的全局上下文信息对于建模序列非常有用，这在像Bi-LSTM这样的上下文编码器中每个词的位置上未被充分捕捉。为了解决这个问题，一些近期的工作[[67](#bib.bib67),
    [68](#bib.bib68)]将句子级别的表示引入了嵌入模块，即除了预训练的词嵌入和字符级表示之外，他们还为每个词分配了从整个句子中学习到的全局表示，这可以在图[4](#S3.F4
    "Figure 4 ‣ III-A4 Sentence-level Representations ‣ III-A Embedding Module ‣ III
    Deep Learning Based Models ‣ A Survey on Recent Advances in Sequence Labeling
    from Deep Learning Models")中展示。尽管这些工作提出了不同的句子表示方法，但它们都证明了在嵌入模块中添加句子表示对于序列标注任务的最终性能的优越性。
- en: III-B Context Encoder Module
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 上下文编码模块
- en: Context dependency plays a significant role in sequence labeling tasks. The
    context encoder module extracts contextual features of each token and capture
    the context dependencies of given input sequence. Learned contextual representations
    will be passed into inference module for label prediction. There are three commonly
    used model architectures for context encoder module, *i.e.,* RNN, CNN and Transformers.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文依赖在序列标注任务中扮演着重要角色。上下文编码模块提取每个词的上下文特征，并捕捉给定输入序列的上下文依赖。学习到的上下文表示将传递到推理模块进行标签预测。上下文编码模块有三种常用的模型架构，*即，*
    RNN、CNN 和 Transformers。
- en: III-B1 Recurrent Neural Network
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 循环神经网络
- en: Bi-LSTM is almost the most widely used context encoder architecture today. Concretely,
    it incorporates past/future contexts from both directions (forward/backward) to
    generate the hidden states of each token, and then jointly concatenate them to
    represent the global information of the entire sequence. While Hammerton [[29](#bib.bib29)]
    has studied utilizing LSTMs for NER tasks in the past, the lack of computing power
    limits the effectiveness of their model. With recent advances in deep learning,
    much research effort has been dedicated to using Bi-LSTM architecture and achieve
    excellent performance. Huang et al. [[35](#bib.bib35)] initially adopt Bi-LSTM
    to generate contextual representations of every word in their sequence labeling
    model, and produce state-of-the-art accuracy on POS tagging, chunking and NER
    data sets. Similarly,  [[127](#bib.bib127), [62](#bib.bib62), [71](#bib.bib71),
    [52](#bib.bib52), [13](#bib.bib13), [8](#bib.bib8), [121](#bib.bib121), [65](#bib.bib65),
    [88](#bib.bib88)] also choose the same Bi-LSTM architecture for context encoding.
    Gated Recurrent Unit(GRU) is a variant of LSTM which also addresses long-dependency
    issues in RNN networks, and several work utilize Bi-GRU as their context encoder
    architecture [[120](#bib.bib120), [55](#bib.bib55), [133](#bib.bib133)].
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Bi-LSTM 几乎是目前最广泛使用的上下文编码器架构。具体来说，它结合了来自两个方向（前向/后向）的过去/未来上下文，以生成每个标记的隐藏状态，然后将这些状态联合连接以表示整个序列的全局信息。虽然
    Hammerton [[29](#bib.bib29)] 曾研究过利用 LSTM 进行 NER 任务，但计算能力的不足限制了其模型的效果。随着深度学习的最新进展，许多研究工作致力于使用
    Bi-LSTM 架构并取得了优异的表现。Huang 等人 [[35](#bib.bib35)] 最初采用 Bi-LSTM 生成序列标注模型中每个词的上下文表示，并在
    POS 标注、chunking 和 NER 数据集上取得了最先进的准确性。类似地，[[127](#bib.bib127), [62](#bib.bib62),
    [71](#bib.bib71), [52](#bib.bib52), [13](#bib.bib13), [8](#bib.bib8), [121](#bib.bib121),
    [65](#bib.bib65), [88](#bib.bib88)] 也选择了相同的 Bi-LSTM 架构进行上下文编码。门控递归单元（GRU）是 LSTM
    的一种变体，也解决了 RNN 网络中的长期依赖问题，一些工作使用 Bi-GRU 作为其上下文编码器架构 [[120](#bib.bib120), [55](#bib.bib55),
    [133](#bib.bib133)]。
- en: Rei [[91](#bib.bib91)] propose a multitask learning method that equips the Bi-LSTM
    context encoder module with a auxiliary training objective, which learns to predict
    surrounding words for every word in the sentence. It shows that the language modeling
    objective provides consistent performance improvements on several sequence labeling
    benchmark, because it motivates the model to learn more general semantic and syntactic
    composition patterns of the language.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Rei [[91](#bib.bib91)] 提出了一个多任务学习方法，该方法为 Bi-LSTM 上下文编码器模块配备了辅助训练目标，学习为句子中的每个词预测周围的词。研究表明，语言建模目标在多个序列标注基准测试中提供了一致的性能提升，因为它激励模型学习更通用的语言语义和句法组成模式。
- en: Zhang et al. [[126](#bib.bib126)] propose a new method called Multi-Order BiLSTM
    which combines low order and high order LSTMs together in order to learn more
    tag dependencies. The high order LSTMs predict multiple tags for the current token
    which contains not only the current tag but also the previous several tags. The
    model keeps the scalability to high order models with a pruning technique, and
    achieves the state-of-the-art result in chunking and highly competitive results
    in two NER datasets.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等人 [[126](#bib.bib126)] 提出了一个新方法称为 Multi-Order BiLSTM，它将低阶和高阶 LSTM 结合在一起，以学习更多的标签依赖关系。高阶
    LSTM 为当前标记预测多个标签，这些标签不仅包括当前标签，还包括之前的几个标签。该模型通过修剪技术保持了对高阶模型的可扩展性，并在 chunking 上取得了最先进的结果，并在两个
    NER 数据集中取得了高度竞争的结果。
- en: Ma et al. [[69](#bib.bib69)] propose a LSTM-based model for jointly training
    sentence-level classification and sequence labeling tasks, in which a modified
    LSTM structure is adopted as their context encoder module. In particular, the
    method employs a convolutional neural network before LSTM to extract features
    from both the context and previous tags of each word. Therefore, the input for
    LSTM is changed to include meaningful contextual and label information.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Ma 等人 [[69](#bib.bib69)] 提出了一个基于 LSTM 的模型，用于联合训练句子级分类和序列标注任务，其中采用了修改的 LSTM 结构作为其上下文编码器模块。特别地，该方法在
    LSTM 之前使用卷积神经网络提取每个词的上下文和之前标签的特征。因此，LSTM 的输入被更改为包括有意义的上下文和标签信息。
- en: Most of the existing LSTM based methods use one or more stacked LSTM layers
    to extract context features of words. However, Gregoric et al. [[26](#bib.bib26)]
    present a different architecture which employs multiple parallel independent Bi-LSTM
    units across the same input and promotes diversity among them by employing an
    inter-model regularization term. It shows that the method reduces the total number
    of parameters in the model and achieves significant improvements on the CoNLL
    2003 NER dataset compared to other previous methods.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的大多数基于 LSTM 的方法使用一个或多个堆叠的 LSTM 层来提取单词的上下文特征。然而，Gregoric 等人 [[26](#bib.bib26)]
    提出了不同的架构，该架构在相同输入上采用多个并行的独立 Bi-LSTM 单元，并通过采用模型间正则化项来促进它们之间的多样性。研究表明，该方法减少了模型中的总参数数量，并在
    CoNLL 2003 NER 数据集上相比于其他方法取得了显著改进。
- en: 'Kazi et al. [[42](#bib.bib42)] propose a novel implicitly-defined neural network
    architecture for sequence labeling. In contrast to traditional recurrent neural
    networks, this work provides a different mechanism that each state is able to
    consider information in both directions. The method extends RNN by changing the
    definition of implicit hidden layer function:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Kazi 等人 [[42](#bib.bib42)] 提出了用于序列标注的全新隐式定义神经网络架构。与传统的递归神经网络相比，这项工作提供了一种不同的机制，使得每个状态能够考虑双向的信息。该方法通过改变隐层函数的定义来扩展
    RNN：
- en: '|  | $h_{t}=f(\xi_{t},h_{t-1},h_{t+1})$ |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{t}=f(\xi_{t},h_{t-1},h_{t+1})$ |  |'
- en: where $\xi_{t}$ denotes the input of hidden layer, $h_{t-1}$ and $h_{t+1}$ is
    the hidden state of last and next time step, respectively. It forgoes the causality
    assumption used to formulate RNN and leads to an implicit set of equations for
    the entire sequence of hidden states. They compute them via an approximate Newton
    solve and apply the Krylov Subspace method [[45](#bib.bib45)]. The implicitly-defined
    neural network architecture helps to achieve improvements on problems with complex,
    long-distance dependencies.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\xi_{t}$ 表示隐层的输入，$h_{t-1}$ 和 $h_{t+1}$ 分别是前一时间步和下一时间步的隐藏状态。它摒弃了用于构造 RNN
    的因果性假设，并为整个隐藏状态序列推导出一组隐式方程。它们通过近似的牛顿法求解，并应用 Krylov 子空间方法 [[45](#bib.bib45)]。这种隐式定义的神经网络架构有助于在具有复杂长距离依赖的问题上取得改进。
- en: Although Bi-LSTM has been widely adopted as context encoder architecture, there
    are still several natural limitations, such as the shallow connections between
    consecutive hidden states of RNNs. At each time step, BiLSTMs consume an incoming
    word and construct a new summary of the past subsequence. This process should
    be highly non-linear so that the hidden states can quickly adapt to variable inputs
    while still retaining useful summaries of the past [[83](#bib.bib83)]. Deep transition
    RNNs extend conventional RNNs by increasing the transition depth of consecutive
    hidden states [[83](#bib.bib83)]. Recently, Liu et al. [[67](#bib.bib67)] introduce
    the deep transition architecture for sequence labeling and achieve a significant
    performance improvement on the tasks of text chunking and NER. Besides, the way
    of sequentially processing inputs of RNN might limit the ability to capture the
    non-continuous relations over tokens within a sentence. To tackle the problem,
    a recent work proposed by Wei et al. [[114](#bib.bib114)] employs self-attention
    to provide complementary context information on the basis of Bi-LSTM. They propose
    a position-aware self-attention as well as a well-designed self-attentional context
    fusion network, aiming to explore the relative positional information of an input
    sequence for capturing the latent relations among tokens. It shows that the method
    achieves significant improvements on the tasks of POS, NER and chunking.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Bi-LSTM 已被广泛应用于上下文编码器架构，但仍然存在一些自然限制，比如 RNN 中连续隐藏状态之间的浅连接。在每个时间步，BiLSTM 消耗一个输入单词，并构建过去子序列的新摘要。这个过程应高度非线性，以便隐藏状态能够迅速适应变量输入，同时保留过去的有用摘要 [[83](#bib.bib83)]。深度过渡
    RNN 通过增加连续隐藏状态的过渡深度来扩展传统 RNN [[83](#bib.bib83)]。最近，Liu 等人 [[67](#bib.bib67)] 引入了用于序列标注的深度过渡架构，并在文本切分和命名实体识别任务上取得了显著的性能提升。此外，RNN
    顺序处理输入的方式可能限制了捕捉句子中标记间非连续关系的能力。为了解决这个问题，Wei 等人 [[114](#bib.bib114)] 提出的最新工作采用了自注意力机制，基于
    Bi-LSTM 提供补充的上下文信息。他们提出了位置感知自注意力以及精心设计的自注意力上下文融合网络，旨在探索输入序列的相对位置信息，以捕捉标记间的潜在关系。结果表明，该方法在
    POS、NER 和切分任务上取得了显著改进。
- en: III-B2 Convolutional Neural Networks
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 卷积神经网络
- en: Convolutional Neural Networks (CNNs) are another popular architecture for encoding
    context information in sequence labeling models. Compared to RNN, CNN based methods
    are considerably faster since it can fully leverage the GPU parallelism through
    the feed-forward structure. An initial work in this area is proposed by Collobert
    et al. [[17](#bib.bib17)]. The method employs a simple feed-forward neural network
    with a fixed-size sliding window over the input sequence embedding, which can
    be viewed as a simplified CNN without pooling layer. And this window approach
    is based on the assumption that the label of a word depends mainly on its neighbors.
    Santos et al. [[99](#bib.bib99)] follow their work and use similar structure for
    context feature extraction.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）是另一种在序列标注模型中编码上下文信息的流行架构。与 RNN 相比，基于 CNN 的方法速度显著更快，因为它可以通过前馈结构充分利用
    GPU 并行性。该领域的初步工作由 Collobert 等人提出 [[17](#bib.bib17)]。该方法使用一个固定大小滑动窗口的简单前馈神经网络来处理输入序列嵌入，这可以被视为没有池化层的简化
    CNN。这个窗口方法基于一个假设，即一个词的标签主要依赖于它的邻居。Santos 等人 [[99](#bib.bib99)] 跟随他们的工作，并使用类似的结构进行上下文特征提取。
- en: Shen et al. [[103](#bib.bib103)] propose a deep active learning based model
    for NER tasks. Their tagging model extracts context representations for each word
    using a CNN due to its strong efficiency, which is crucial for their iterative
    retraining scheme. The structure has two convolutional layers with kernels of
    width three, and it concatenates the representation at the last convolutional
    layer with the input embedding to form the output.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Shen 等人 [[103](#bib.bib103)] 提出了一个基于深度主动学习的模型用于命名实体识别任务。他们的标注模型使用 CNN 提取每个词的上下文表示，因其强大的效率对于他们的迭代再训练方案至关重要。该结构包含两个卷积层，卷积核宽度为三，并将最后一个卷积层的表示与输入嵌入连接形成输出。
- en: Wang et al. [[113](#bib.bib113)] employ stacked Gated Convolutional Neural Networks(GCNN)
    for named entity recognition, which extend the convolutional layer with gating
    mechanism. In particular, a gated convolutional layer can be written as
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[113](#bib.bib113)] 使用堆叠的门控卷积神经网络（GCNN）进行命名实体识别，这种网络通过门控机制扩展了卷积层。特别地，一个门控卷积层可以表示为
- en: '|  | $F_{gating}(\mathbf{X})=(\mathbf{X}*\mathbf{W}+\hat{\mathbf{b}})\odot\sigma(\mathbf{X}*\mathbf{V}+\hat{\mathbf{c}})$
    |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{gating}(\mathbf{X})=(\mathbf{X}*\mathbf{W}+\hat{\mathbf{b}})\odot\sigma(\mathbf{X}*\mathbf{V}+\hat{\mathbf{c}})$
    |  |'
- en: where $*$ denotes row convolution, $\mathbf{X}$ is the input of this layer,
    $\mathbf{W},\hat{\mathbf{b}},\mathbf{V},\hat{\mathbf{c}}$ are the parameters to
    be learned, $\sigma$ is the sigmoid function and represents element-wise product.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $*$ 表示行卷积，$\mathbf{X}$ 是该层的输入，$\mathbf{W},\hat{\mathbf{b}},\mathbf{V},\hat{\mathbf{c}}$
    是需要学习的参数，$\sigma$ 是 sigmoid 函数，表示逐元素乘积。
- en: Though relatively high efficiency, a major disadvantage of CNNs is that it has
    difficulties in capturing long-range dependencies in sequences due to the limited
    receptive fields, which makes fewer methods to perform sequence labeling tasks
    with CNNs than RNNs. In recent year, some CNN-based models modify traditional
    CNNs to better capture global context information and achieve excellent results
    for sequence labeling.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管效率较高，但 CNN 的一个主要缺点是由于有限的感受野，它在捕捉序列中的长程依赖关系方面存在困难，这使得使用 CNN 进行序列标注任务的方法少于 RNN。近年来，一些基于
    CNN 的模型对传统 CNN 进行了修改，以更好地捕捉全局上下文信息，并在序列标注任务中取得了优异的结果。
- en: Strubell et al. [[104](#bib.bib104)] propose a Iterated Dilated Convolutional
    Neural Networks (ID-CNNs) method for the task of NER, which enables significant
    speed improvements while maintaining accuracy comparable to the state-of-the-arts.
    Dilated convolutions [[123](#bib.bib123)] operate on a sliding window of context
    like typical CNN layers, but the context need not be consecutive. The convolution
    is defined over a wider effective input width by skipping over several inputs
    at a time, and the effective input width can grow exponentially with the depth.
    Thus it can incorporate broader context into the representation of a token than
    typical CNN. Fig [5](#S3.F5 "Figure 5 ‣ III-B2 Convolutional Neural Networks ‣
    III-B Context Encoder Module ‣ III Deep Learning Based Models ‣ A Survey on Recent
    Advances in Sequence Labeling from Deep Learning Models") shows the structure.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Strubell 等人 [[104](#bib.bib104)] 提出了用于命名实体识别（NER）任务的迭代扩张卷积神经网络（ID-CNNs）方法，该方法在保持与最先进技术相当的准确性的同时，实现了显著的速度提升。扩张卷积
    [[123](#bib.bib123)] 像典型的 CNN 层一样在滑动窗口的上下文上操作，但上下文不必是连续的。卷积通过跳过多个输入同时定义在更宽的有效输入宽度上，并且有效输入宽度可以随着深度呈指数增长。因此，它能够将比典型
    CNN 更广泛的上下文纳入到标记的表示中。图 [5](#S3.F5 "Figure 5 ‣ III-B2 Convolutional Neural Networks
    ‣ III-B Context Encoder Module ‣ III Deep Learning Based Models ‣ A Survey on
    Recent Advances in Sequence Labeling from Deep Learning Models") 展示了这一结构。
- en: '![Refer to caption](img/c462fa16eeb0d3d92e72be1e199527ee.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c462fa16eeb0d3d92e72be1e199527ee.png)'
- en: 'Figure 5: A dilated CNN block with maximum dilation width 4 and filter width
    3\. Neurons contributing to a single highlighted neuron in the last layer are
    also highlighted [[104](#bib.bib104)].'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：一个扩张卷积块，最大扩张宽度为 4，滤波器宽度为 3。对最后一层中单个高亮神经元有贡献的神经元也被高亮显示 [[104](#bib.bib104)]。
- en: The proposed iterated dilated CNN architecture repeatedly applies the same block
    of dilated convolutions to token-wise representations. Repeatedly employing the
    same parameters prevents overfitting problem and provides the model desirable
    generalization capabilities.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的迭代扩张 CNN 架构将相同的扩张卷积块重复应用于逐标记表示。重复使用相同的参数可以防止过拟合问题，并为模型提供理想的泛化能力。
- en: Chen et al. [[10](#bib.bib10)] propose gated relation network(GRN) for NER,
    in which a gated relation layer that models the relationship between any two words
    is built on top of CNNs for capturing long-range context information. Specifically,
    it firstly computes the relation score vector between any two words,
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 等人 [[10](#bib.bib10)] 提出了用于 NER 的门控关系网络（GRN），其中在 CNNs 之上构建了一个建模任意两个词之间关系的门控关系层，以捕获长距离上下文信息。具体来说，它首先计算任意两个词之间的关系得分向量，
- en: '|  | $r_{ij}=\mathbf{W}_{rx}[x_{i};x_{j}]+b_{rx}$ |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{ij}=\mathbf{W}_{rx}[x_{i};x_{j}]+b_{rx}$ |  |'
- en: where $x_{i}$ and $x_{j}$ denote the local context features from the CNN layer
    for the i-th and j-th word in the sequence, $\mathbf{W}_{rx}$ is the weight matrix
    and $b_{rx}$ is the bias vector. Note that the relation score vector $r_{ij}$
    is of the same dimension as $x_{i}$ and $x_{j}$. Then the corresponding global
    contextual representation $r_{i}$ for the ith word is obtained by performing a
    weighted-summing up operation, in which a gating mechanism is adopted for adaptively
    selecting other dependent words.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{i}$ 和 $x_{j}$ 表示 CNN 层中序列中第 i 个和第 j 个词的局部上下文特征，$\mathbf{W}_{rx}$ 是权重矩阵，$b_{rx}$
    是偏置向量。请注意，关系得分向量 $r_{ij}$ 的维度与 $x_{i}$ 和 $x_{j}$ 相同。然后，通过执行加权求和操作获得第 i 个词的相应全局上下文表示
    $r_{i}$，其中采用了门控机制以自适应选择其他依赖词。
- en: '|  | $r_{i}=\frac{1}{T}\sum_{j=1}^{T}\sigma(r_{ij})\odot x_{j}$ |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{i}=\frac{1}{T}\sum_{j=1}^{T}\sigma(r_{ij})\odot x_{j}$ |  |'
- en: where $\sigma$ is a gate using sigmoid function, and $\odot$ denotes element-wise
    multiplication. The proposed GRN model achieves significantly better performance
    than ID-CNN [[104](#bib.bib104)], owing to its stronger capacity to capture global
    context dependencies.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma$ 是一个使用 sigmoid 函数的门控，而 $\odot$ 表示逐元素乘法。由于其更强的捕捉全局上下文依赖的能力，提出的 GRN
    模型相比于 ID-CNN [[104](#bib.bib104)] 实现了显著更好的性能。
- en: III-B3 Transformers
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 Transformer
- en: The Transformer model is proposed by Vaswani et al. [[111](#bib.bib111)] in
    2017 and achieves excellent performance for Neural Machine Translation (NMT) tasks.
    The overall architecture is based solely on attention mechanisms to draw global
    dependencies between input, dispensing with recurrence and convolutions entirely.
    The initial proposed Transformer employs a sequence to sequence structure that
    comprises the encoder and decoder. But the subsequent research work often adopt
    the encoder part to serve as the feature extractor, thus our introduction here
    is limited to it.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型由Vaswani等人[[111](#bib.bib111)]于2017年提出，并在神经机器翻译（NMT）任务中取得了优异的性能。整体架构完全基于注意力机制，以在输入之间建立全局依赖，完全摆脱了递归和卷积。最初提出的Transformer采用了一个由编码器和解码器组成的序列到序列结构。但后续的研究工作通常采用编码器部分作为特征提取器，因此我们这里的介绍仅限于此。
- en: The encoder is composed of a stack of several identical layers, which includes
    a multi-head self-attention mechanism and a position-wise fully connected feed-forward
    network. It employs a residual connection [[30](#bib.bib30)] around each of the
    two sub-layers to ease the training of deep neural network. And layer normalization [[53](#bib.bib53)]
    is applied after the residual connection to stabilize the activations of model.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器由几个相同的层堆叠而成，这些层包括多头自注意力机制和逐位置全连接前馈网络。它在每个两个子层周围使用残差连接[[30](#bib.bib30)]，以简化深度神经网络的训练。层归一化[[53](#bib.bib53)]在残差连接之后应用，以稳定模型的激活。
- en: Due to its superior performance, the Transformer is widely used in various NLP
    tasks and has achieved excellent results. However, in sequence labeling tasks,
    the Transformer encoder has been reported to perform poorly [[28](#bib.bib28)].
    Recently, Yan et al. [[118](#bib.bib118)] analyze the properties of Transformer
    for exploring the reason why Transformer does not work well in sequence labeling
    tasks especially NER. Both the direction and relative distance information are
    important in the NER, but these information will lose when the sinusoidal position
    embedding is used in the vanilla Transformer. To address the problem, they propose
    TENER, an architecture adopting adapted Transformer Encoder by incorporating the
    direction and relative distance aware attention and the un-scaled attention, which
    can greatly boost the performance of Transformer encoder for NER. Star-Transformer
    is a lightweight alternative of Transformer proposed by Shao et al. [[28](#bib.bib28)].
    It replaces the fully-connected structure with a star-shaped topology, in which
    every two non-adjacent nodes are connected through a shared relay node. The model
    complexity is reduced significantly, and it also achieved great improvements against
    the standard Transformer on various tasks including sequence labeling tasks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其优越的性能，Transformer在各种NLP任务中被广泛使用，并取得了卓越的结果。然而，在序列标注任务中，Transformer编码器的表现被报道为较差[[28](#bib.bib28)]。最近，Yan等人[[118](#bib.bib118)]分析了Transformer的属性，探讨了Transformer在序列标注任务（特别是NER）中表现不佳的原因。NER中，方向和相对距离信息都很重要，但在原始Transformer中使用的正弦位置嵌入会丢失这些信息。为了解决这个问题，他们提出了TENER，这是一种通过结合方向和相对距离感知注意力以及非缩放注意力来调整Transformer编码器的架构，这可以大大提升Transformer编码器在NER任务中的表现。Star-Transformer是Shao等人[[28](#bib.bib28)]提出的Transformer的轻量级替代方案。它用星形拓扑结构替代了全连接结构，其中每两个非相邻节点通过一个共享的中继节点连接。模型复杂性显著降低，并且在包括序列标注任务在内的各种任务上相较于标准Transformer取得了很大改进。
- en: III-C Inference Module
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 推理模块
- en: The inference module takes the representations from context encoder module as
    input, and generate the optimal label sequence.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 推理模块将上下文编码器模块的表示作为输入，并生成最佳标签序列。
- en: III-C1 Softmax
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 Softmax
- en: The softmax function that also called normalized exponential function, is a
    generalization of logic functions and has been widely used in a variety of probability-based
    multi-classification methods. It maps a $K$-dimensional vector $z$ into another
    $K$-dimensional real vector $\sigma(z)$ such that each element has a range between
    $0$ and $1$ and the sum of all elements equals $1$. The form of the function is
    usually given by the following formula
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax函数，也称为归一化指数函数，是逻辑函数的一个推广，已广泛应用于各种基于概率的多分类方法。它将一个$K$维向量$z$映射到另一个$K$维实数向量$\sigma(z)$，使得每个元素的范围在$0$到$1$之间，并且所有元素的总和等于$1$。该函数的形式通常由以下公式给出
- en: '|  | $\sigma(z)_{j}=\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}}$ |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma(z)_{j}=\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}}$ |  |'
- en: where $j=1,\dots,K$.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $j=1,\dots,K$。
- en: Many models for sequence labeling treat the problem as a set of independent
    classification tasks, and utilize a softmax layer as a linear classifier to assign
    optimal label for each word in a sequence [[8](#bib.bib8), [26](#bib.bib26), [62](#bib.bib62),
    [42](#bib.bib42), [27](#bib.bib27), [69](#bib.bib69), [43](#bib.bib43), [44](#bib.bib44)].
    Specifically, given the output representation $h_{t}$ of the context encoder at
    time step $t$, the probability distribution of the t-th word’s label can be obtained
    by a fully connected layer and a final softmax function
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 许多序列标注模型将问题视为一组独立的分类任务，并利用 softmax 层作为线性分类器，为序列中的每个词分配最优标签 [[8](#bib.bib8),
    [26](#bib.bib26), [62](#bib.bib62), [42](#bib.bib42), [27](#bib.bib27), [69](#bib.bib69),
    [43](#bib.bib43), [44](#bib.bib44)]。具体来说，给定时间步 $t$ 上上下文编码器的输出表示 $h_{t}$，t-th 词标签的概率分布可以通过全连接层和最终的
    softmax 函数获得。
- en: '|  | $o_{t}=softmax(\mathbf{W}h_{t}+b)$ |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | $o_{t}=softmax(\mathbf{W}h_{t}+b)$ |  |'
- en: where the weight matrix $\mathbf{W}\in{R^{d\times|T|}}$ maps $h_{t}$ to the
    space of labels, $d$ is the dimension of $h_{t}$ and $|T|$ is the number of all
    possible labels.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 其中权重矩阵 $\mathbf{W}\in{R^{d\times|T|}}$ 将 $h_{t}$ 映射到标签空间，$d$ 是 $h_{t}$ 的维度，$|T|$
    是所有可能标签的数量。
- en: III-C2 Conditional Random Fields
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 条件随机场
- en: The above methods of independently inferring word labels in a given sequence
    ignore the dependencies between labels. Typically, the correct label to each word
    often depends on the choices of nearby elements. Therefore, it is necessary to
    consider the correlation between labels of adjacent neighborhoods to jointly decode
    the optimal label chain of the entire sequence. CRF model [[45](#bib.bib45)] has
    been proven to be powerful in learning the strong dependencies across output labels,
    thus most of the neural network-based models for sequence labeling employ CRF
    as the inference module [[71](#bib.bib71), [121](#bib.bib121), [65](#bib.bib65),
    [88](#bib.bib88), [91](#bib.bib91), [85](#bib.bib85), [1](#bib.bib1), [125](#bib.bib125),
    [9](#bib.bib9), [22](#bib.bib22)].
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 上述独立推断给定序列中词标签的方法忽略了标签之间的依赖关系。通常，每个词的正确标签常常依赖于附近元素的选择。因此，有必要考虑邻近区域标签之间的相关性，以共同解码整个序列的最优标签链。CRF
    模型 [[45](#bib.bib45)] 已被证明在学习输出标签之间的强依赖关系方面非常有效，因此大多数基于神经网络的序列标注模型都将 CRF 作为推断模块 [[71](#bib.bib71),
    [121](#bib.bib121), [65](#bib.bib65), [88](#bib.bib88), [91](#bib.bib91), [85](#bib.bib85),
    [1](#bib.bib1), [125](#bib.bib125), [9](#bib.bib9), [22](#bib.bib22)]。
- en: Specifically, let $\mathbf{Z}=[\hat{\mathbf{z}}_{1},\hat{\mathbf{z}}_{2},\ldots,\hat{\mathbf{z}}_{n}]^{\top}$
    be the output of context encoder of the given sequence $\hat{\mathbf{x}}$, the
    probability $\Pr(\hat{\mathbf{y}}|\hat{\mathbf{x}})$ of generating the whole label
    sequence $y_{i}\in\hat{\mathbf{y}}$ with regard to $\mathbf{Z}$ is
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 具体地，设 $\mathbf{Z}=[\hat{\mathbf{z}}_{1},\hat{\mathbf{z}}_{2},\ldots,\hat{\mathbf{z}}_{n}]^{\top}$
    为给定序列 $\hat{\mathbf{x}}$ 的上下文编码器输出，生成整个标签序列 $y_{i}\in\hat{\mathbf{y}}$ 的概率 $\Pr(\hat{\mathbf{y}}|\hat{\mathbf{x}})$
    相对于 $\mathbf{Z}$ 为
- en: '|  | $\Pr(\hat{\mathbf{y}}&#124;\hat{\mathbf{x}})=\frac{\prod_{j=1}^{n}\phi(y_{j-1},y_{j},\hat{\mathbf{z}}_{j})}{\sum_{y^{{}^{\prime}}\in{\mathbf{Y}(\mathbf{Z})}}\prod_{j=1}^{n}\phi(y^{{}^{\prime}}_{j-1},y^{{}^{\prime}}_{j},\hat{\mathbf{z}}_{j})},$
    |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Pr(\hat{\mathbf{y}}&#124;\hat{\mathbf{x}})=\frac{\prod_{j=1}^{n}\phi(y_{j-1},y_{j},\hat{\mathbf{z}}_{j})}{\sum_{y^{{}^{\prime}}\in{\mathbf{Y}(\mathbf{Z})}}\prod_{j=1}^{n}\phi(y^{{}^{\prime}}_{j-1},y^{{}^{\prime}}_{j},\hat{\mathbf{z}}_{j})},$
    |  |'
- en: where $\mathbf{Y}(\mathbf{Z})$ is the set of possible label sequences for $\mathbf{Z}$;
    $\phi(y_{j-1},y_{j},\hat{\mathbf{z}}_{j})\!=\!\exp(\mathbf{W}_{y_{j-1},y_{j}}\hat{\mathbf{z}}_{j}+b_{y_{j-1},y_{j}}),$
    $\mathbf{W}_{y_{j-1},y_{j}}$ and $b_{y_{j-1},y_{j}}$ indicate the weighted matrix
    and bias parameters corresponding to the label pair $(y_{j-1},y_{j})$, respectively.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{Y}(\mathbf{Z})$ 是 $\mathbf{Z}$ 的可能标签序列集合；$\phi(y_{j-1},y_{j},\hat{\mathbf{z}}_{j})\!=\!\exp(\mathbf{W}_{y_{j-1},y_{j}}\hat{\mathbf{z}}_{j}+b_{y_{j-1},y_{j}})$，$\mathbf{W}_{y_{j-1},y_{j}}$
    和 $b_{y_{j-1},y_{j}}$ 分别表示与标签对 $(y_{j-1},y_{j})$ 对应的加权矩阵和偏置参数。
- en: Semi-CRF.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 半条件随机场。
- en: Semi-Markov conditional random fields (semi-CRFs) [[100](#bib.bib100)] is an
    extension of conventional CRFs, in which labels are assigned to the segments of
    input sequence rather than to individual words. It extracts features of segments
    and models the transition between them, suitable for segment-level sequence labeling
    tasks such as named entity recognition and phrase chunking. Compared to CRFs,
    the advantage of semi-CRFs is that it can make full use of segment-level information
    to capture the internal properties of segments, and higher-order label dependencies
    can be taken into account. However, since it jointly learns to determine the length
    of each segment and the corresponding label, the time complexity becomes higher.
    Besides, more features is required for modeling segments with different lengths
    and automatically extracting meaningful segment-level features is an important
    issue for Semi-CRFs. With advances in deep learning, some models combining neural
    networks and Semi-CRFs for sequence labeling have been studied.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 半马尔可夫条件随机场（semi-CRFs）[[100](#bib.bib100)] 是对传统CRFs的扩展，其中标签分配给输入序列的片段，而不是单个单词。它提取片段的特征并建模片段之间的过渡，适用于如命名实体识别和短语分块等片段级序列标注任务。与CRFs相比，semi-CRFs的优势在于可以充分利用片段级信息来捕捉片段的内部属性，并考虑更高阶的标签依赖。然而，由于它需要联合学习确定每个片段的长度和相应标签，时间复杂度较高。此外，建模具有不同长度的片段需要更多特征，自动提取有意义的片段级特征是Semi-CRFs的重要问题。随着深度学习的进步，一些结合神经网络和Semi-CRFs进行序列标注的模型已经被研究。
- en: Kong et al. [[46](#bib.bib46)] propose Segmental Recurrent Neural Networks (SRNNs)
    for segment-level sequence labeling problems, which adopts a semi-CRF as the inference
    module and learns representations of segments through Bi-LSTM. Based on the recurrent
    nature of RNN, this method further designs a dynamic programming algorithm to
    reduce the time complexity. A parallel work Gated Recursive Semi-CRFs (grSemi-CRFs)
    proposed by Zhuo et al. [[134](#bib.bib134)] employs a Gated Recursive Convolutional
    Neural Network (grConv) [[14](#bib.bib14)] to extract segment features for semi-CRF.
    The grConv is a variant of recursive neural network that learns segment-level
    representations by constructing a pyramid-like structure and recursively combining
    adjacent segment vectors. The follow-up work proposed by Kemos et al. [[43](#bib.bib43)]
    utilize the same grConv architecture for extracting segment features in their
    neural semi-CRF model. It takes characters as the basic input unit but does not
    require any correct token boundaries, which is different from existing character-level
    models. The model is based on semi-CRF to jointly segment (tokenize) and label
    characters, being robust for languages with difficult or noisy tokenization. Sato
    et al. [[101](#bib.bib101)] design Segment-level Neural CRF for segment-level
    sequence labeling tasks. The method applies a CNN to obtain segment-level representations
    and constructs segment lattice to reduce search space.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Kong等人[[46](#bib.bib46)] 提出了用于片段级序列标注问题的片段递归神经网络（SRNNs），该方法采用semi-CRF作为推断模块，并通过Bi-LSTM学习片段的表示。基于RNN的递归特性，该方法进一步设计了一种动态规划算法来降低时间复杂度。Zhuo等人[[134](#bib.bib134)]
    提出的一个并行工作Gated Recursive Semi-CRFs（grSemi-CRFs）使用Gated Recursive Convolutional
    Neural Network（grConv）[[14](#bib.bib14)] 提取semi-CRF的片段特征。grConv是一种递归神经网络变体，通过构建金字塔结构并递归地组合相邻片段向量来学习片段级表示。Kemos等人[[43](#bib.bib43)]
    提出的后续工作利用相同的grConv架构在他们的神经semi-CRF模型中提取片段特征。该模型以字符为基本输入单位，但不需要任何正确的标记边界，这与现有的字符级模型不同。该模型基于semi-CRF，联合进行片段（标记）和标签字符，对于具有困难或噪声标记的语言具有鲁棒性。Sato等人[[101](#bib.bib101)]
    设计了用于片段级序列标注任务的Segment-level Neural CRF。该方法应用CNN来获得片段级表示，并构建片段格栅以减少搜索空间。
- en: The aforementioned models only adopt segment-level labels for segment score
    calculation and model training. An extension [[122](#bib.bib122)] proposed by
    Ye et al. demonstrates that incorporating word-level labels information can be
    beneficial for building semi-CRFs. The proposed Hybrid Semi-CRFs(HSCRF) model
    utilizes word-level and segment-level labels simultaneously to derive the segment
    scores. Besides, the methods of integrating CRF and HSCRF output layers into an
    unified network for jointly training and decoding are further presented. The Hybrid
    Semi-CRFs model is also adopted as baseline for subsequent work  [[66](#bib.bib66)].
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 上述模型仅采用段级标签进行段分数计算和模型训练。Ye 等人[[122](#bib.bib122)]提出的扩展表明，结合词级标签信息对于构建半 CRF 是有益的。所提出的混合半
    CRF (HSCRF) 模型同时利用词级和段级标签来推导段分数。此外，还进一步提出了将 CRF 和 HSCRF 输出层整合到统一网络中以进行联合训练和解码的方法。混合半
    CRF 模型也被用作后续工作的基准[[66](#bib.bib66)]。
- en: Skip-chain CRF. The Skip-chain CRF [[106](#bib.bib106)] is a variant of conventional
    linear chain CRF that captures long-range label dependencies by means of skip
    edges, which basically refers to edges between the label positions not adjacent
    to each other. However, the skip-chain CRF contains loop in graph structure, making
    the process of model training and inference intractable. Loop belief propagation
    that requires multiple iterations of messaging can be one of the approximate solutions,
    but is fairly time consuming for large neural network based models. In order to
    mitigate the problem, Jagannatha et al. [[37](#bib.bib37)] propose an approximate
    approach for computation of marginals which adopts recurrent units to model the
    messages. The proposed approximate neural skip-chain CRF model is used for enhancing
    the exact phrase detection of clinical entities.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 跳链 CRF。跳链 CRF[[106](#bib.bib106)] 是传统线性链 CRF 的一种变体，通过跳跃边捕捉长程标签依赖，跳跃边基本上是指标签位置之间不相邻的边。然而，跳链
    CRF 在图结构中包含循环，使得模型训练和推断过程变得不可处理。需要多次迭代消息的循环信念传播可以是其中一种近似解决方案，但对于大型神经网络模型来说相当耗时。为了缓解这一问题，Jagannatha
    等人[[37](#bib.bib37)]提出了一种近似计算边际的方案，该方案采用递归单元来建模消息。所提出的近似神经跳链 CRF 模型用于提高临床实体的精确短语检测。
- en: Embedded-State Latent CRF. Thai et al. [[108](#bib.bib108)] design a novel embedded-state
    latent CRF for neural sequence labeling, which has more capacities in modeling
    non-local label dependencies that often neglected by conventional CRF. This method
    incorporates latent variables into the CRF model for capturing global constraints
    between labels and applies representation learning to the output space. In order
    to reduce the numbers of parameters and prevent overfitting, a parsimonious factorized
    parameter strategy to learn low-rank embedding matrices are further adopted.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入状态潜在 CRF。Thai 等人[[108](#bib.bib108)]设计了一种新颖的嵌入状态潜在 CRF 用于神经序列标注，该模型在建模传统 CRF
    常常忽略的非本地标签依赖方面具有更强的能力。这种方法将潜在变量引入 CRF 模型中，以捕捉标签之间的全局约束，并将表示学习应用于输出空间。为了减少参数数量并防止过拟合，还进一步采用了一种简约的因子化参数策略来学习低秩嵌入矩阵。
- en: NCRF transducers. Based on the similar motivation of modeling long-range dependencies
    between labels, Hu et al. [[34](#bib.bib34)] present a further extension and propose
    neural CRF transducers (NCRF transducers), which introduces RNN transducers to
    implement the edge potential in CRF model. The edge potential represents the score
    for current label by considering dependencies from all previous labels. Thus the
    proposed model can capture long-range label dependencies from the beginning up
    to each current position.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: NCRF 传输器。基于对标签之间长程依赖建模的相似动机，Hu 等人[[34](#bib.bib34)]提出了进一步的扩展，提出了神经 CRF 传输器（NCRF
    传输器），它引入了 RNN 传输器以实现 CRF 模型中的边缘潜力。边缘潜力通过考虑来自所有之前标签的依赖来表示当前标签的分数。因此，所提出的模型可以从一开始就捕捉到每个当前位置的长程标签依赖。
- en: III-C3 Recurrent Neural Network
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C3 递归神经网络
- en: '![Refer to caption](img/ce9fe88d90a8695d124d2832e856d59d.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ce9fe88d90a8695d124d2832e856d59d.png)'
- en: 'Figure 6: LSTM architecture for inferring label [[103](#bib.bib103)].'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：用于推断标签的 LSTM 架构[[103](#bib.bib103)]。
- en: RNN is extremely suitable for feature extraction of sequential data, so it is
    widely used for encoding contextual information in sequence labeling models. Some
    studies demonstrate that RNN structure can also be adopted in the inference module
    for producing optimal label sequence. In addition to the learned representations
    output from context encoder, the information of former predicted labels also serves
    as an input. Thus the corresponding label of each word is generated based on both
    the features of input sequence and the previous predicted labels, making long-range
    label dependencies captured. However, unlike the global normalized CRF model,
    the RNN-based reasoning method greedily decodes the label from left to right,
    so it’s a local normalized model that might suffer from label bias and exposure
    bias problems [[2](#bib.bib2)].
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 非常适合于序列数据的特征提取，因此在序列标注模型中广泛用于编码上下文信息。一些研究表明，RNN 结构也可以用于推理模块，以产生最佳标签序列。除了上下文编码器输出的学习表示，前一个预测标签的信息也作为输入。因此，每个词的相应标签是基于输入序列的特征和之前的预测标签生成的，使得长期标签依赖被捕捉。然而，与全局归一化的
    CRF 模型不同，基于 RNN 的推理方法从左到右贪婪解码标签，因此它是一个局部归一化模型，可能会遭遇标签偏倚和曝光偏倚问题 [[2](#bib.bib2)]。
- en: Shen et al. [[103](#bib.bib103)] employ a LSTM layer on top of the context encoder
    for label decoding. As dipicted in Fig [6](#S3.F6 "Figure 6 ‣ III-C3 Recurrent
    Neural Network ‣ III-C Inference Module ‣ III Deep Learning Based Models ‣ A Survey
    on Recent Advances in Sequence Labeling from Deep Learning Models"), the decoder
    LSTM takes the last generated label as well as the contextual representation of
    current word as inputs, and computes the hidden state which will be passed through
    softmax function to finally decode the label. Zheng et al. [[130](#bib.bib130)]
    adopt a similar LSTM structure as the inference module of their sequence labeling
    model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Shen 等人 [[103](#bib.bib103)] 在上下文编码器上方使用 LSTM 层进行标签解码。如图 [6](#S3.F6 "Figure
    6 ‣ III-C3 Recurrent Neural Network ‣ III-C Inference Module ‣ III Deep Learning
    Based Models ‣ A Survey on Recent Advances in Sequence Labeling from Deep Learning
    Models") 所示，解码器 LSTM 将最后生成的标签以及当前词的上下文表示作为输入，计算出隐藏状态，然后通过 softmax 函数传递以最终解码标签。Zheng
    等人 [[130](#bib.bib130)] 采用类似的 LSTM 结构作为其序列标注模型的推理模块。
- en: Unlike the above two studies, Vaswani et al. [[110](#bib.bib110)] utilize a
    LSTM decoder that can be considered as parallel with the context encoder module.
    The LSTM only accepts the last label as input to produce a hidden state, which
    will be combined with the word context representation for label decoding. Zhang
    et al. [[127](#bib.bib127)] introduce a novel joint labeling strategy based on
    LSTM decoder. The output hidden state and contextual representation are not integrated
    before the labeling decision is made but independently estimate the labeling probability.
    Those two probabilities are then merged by weighted averaging to produce the final
    result. Specifically, a parameter is dynamically computed by a gate mechanism
    to adaptively balance the involvement of the two parts. Experiments show that
    the proposed label LSTM could significantly improve the performance.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述两项研究不同，Vaswani 等人 [[110](#bib.bib110)] 利用可以与上下文编码器模块并行的 LSTM 解码器。LSTM 仅接受最后一个标签作为输入以生成隐藏状态，该状态将与词上下文表示结合用于标签解码。Zhang
    等人 [[127](#bib.bib127)] 引入了一种基于 LSTM 解码器的新颖联合标注策略。在做出标注决策之前，输出的隐藏状态和上下文表示并没有整合，而是独立估计标注概率。这两个概率通过加权平均合并，生成最终结果。具体而言，通过门控机制动态计算一个参数，以自适应地平衡两个部分的参与。实验表明，提出的标签
    LSTM 能显著提高性能。
- en: Encoder-Decoder-Pointer Framework.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器-指针框架。
- en: '![Refer to caption](img/200b031411dad5e8eacc3c6f5f960f58.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/200b031411dad5e8eacc3c6f5f960f58.png)'
- en: 'Figure 7: The encoder-decoder-pointer framework [[124](#bib.bib124)].'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：编码器-解码器-指针框架 [[124](#bib.bib124)]。
- en: 'Zhai et al. [[124](#bib.bib124)] propose a neural sequence chunking model based
    on an encoder-decoder-pointer framework, which is suitable for tasks that need
    assign labels to meaningful chunks in sentences, such as phrase chunking and semantic
    role labeling. The architecture is illustrated in Fig [7](#S3.F7 "Figure 7 ‣ III-C3
    Recurrent Neural Network ‣ III-C Inference Module ‣ III Deep Learning Based Models
    ‣ A Survey on Recent Advances in Sequence Labeling from Deep Learning Models").
    The proposed model divides original sequence labeling task into two steps: (1)
    Segmentation, identifying the scope of each chunk; (2) Labeling, treating each
    chunk as a complete unit to label. It adopts a pointer network [[112](#bib.bib112)]
    to process the segmentation by determining the ending point of each chunk and
    the LSTM decoder is utilized for labeling based on the segmentation results. The
    model proposed by Li et al. [[55](#bib.bib55)] also employs the similar architecture
    for their text segmentation model, where a seq2seq model equipped with pointer
    network is designed to infer the segment boundaries.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Zhai 等人[[124](#bib.bib124)]提出了一种基于编码器-解码器-指针框架的神经序列块分析模型，适用于需要为句子中的有意义块分配标签的任务，如短语块分析和语义角色标注。该架构如图[7](#S3.F7
    "Figure 7 ‣ III-C3 循环神经网络 ‣ III-C 推理模块 ‣ III 深度学习模型 ‣ 深度学习模型序列标注的最新进展调查")所示。该模型将原始序列标注任务分为两个步骤：(1)
    分割，识别每个块的范围；(2) 标注，将每个块视为一个完整的单元进行标注。它采用指针网络[[112](#bib.bib112)]来处理分割，通过确定每个块的结束点，而
    LSTM 解码器则用于基于分割结果进行标注。Li 等人[[55](#bib.bib55)]提出的模型也采用了类似的架构用于他们的文本分割模型，其中设计了一个配备指针网络的
    seq2seq 模型来推断段边界。
- en: IV Evaluation Metrics and DataSets
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 评估指标和数据集
- en: As mentioned in Section [II](#S2 "II Background ‣ A Survey on Recent Advances
    in Sequence Labeling from Deep Learning Models"), three common related tasks of
    sequence labeling problems include POS tagging, NER, and chunking. In this section,
    we list some widely used datasets in Table [II](#S4.T2 "TABLE II ‣ IV-A Datasets
    ‣ IV Evaluation Metrics and DataSets ‣ A Survey on Recent Advances in Sequence
    Labeling from Deep Learning Models") and will describe several most commonly used
    datasets of these three tasks, and introduce the corresponding evaluation metrics
    as well.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[II](#S2 "II 背景 ‣ 深度学习模型序列标注的最新进展调查")节中提到的，序列标注问题的三个常见相关任务包括 POS 标注、NER 和块分析。在本节中，我们在表[II](#S4.T2
    "TABLE II ‣ IV-A 数据集 ‣ IV 评估指标和数据集 ‣ 深度学习模型序列标注的最新进展调查")中列出了一些广泛使用的数据集，并将描述这三个任务中最常用的几个数据集，同时介绍相应的评估指标。
- en: IV-A Datasets
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 数据集
- en: 'TABLE II: List of annotated datasets for POS and NER.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: POS 和 NER 的注释数据集列表。'
- en: '| Task | Corpus | Year | URL |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 语料库 | 年份 | 网址 |'
- en: '| POS | Wall Street Journal(WSJ) | 2000 | [https://catalog.ldc.upenn.edu/LDC2000T43/](https://catalog.ldc.upenn.edu/LDC2000T43/)
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| POS | 华尔街日报（WSJ） | 2000 | [https://catalog.ldc.upenn.edu/LDC2000T43/](https://catalog.ldc.upenn.edu/LDC2000T43/)
    |'
- en: '| NEGRA German Corpus | 2006 | [http://www.coli.uni-saarland.de/projects/sfb378/negra-corpus/](http://www.coli.uni-saarland.de/projects/sfb378/negra-corpus/)
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| NEGRA 德语语料库 | 2006 | [http://www.coli.uni-saarland.de/projects/sfb378/negra-corpus/](http://www.coli.uni-saarland.de/projects/sfb378/negra-corpus/)
    |'
- en: '| Rit-Twitter | 2011 | [https://github.com/aritter/twitter_nlp](https://github.com/aritter/twitter_nlp)
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Rit-Twitter | 2011 | [https://github.com/aritter/twitter_nlp](https://github.com/aritter/twitter_nlp)
    |'
- en: '| Prague Dependency Treebank | 2012 - 2013 | [http://ufal.mff.cuni.cz/pdt2.0/](http://ufal.mff.cuni.cz/pdt2.0/)
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 布拉格依赖树库 | 2012 - 2013 | [http://ufal.mff.cuni.cz/pdt2.0/](http://ufal.mff.cuni.cz/pdt2.0/)
    |'
- en: '| Universal Dependency(UD) | 2015 - 2020 | [https://universaldependencies.org](https://universaldependencies.org)
    |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 通用依赖（UD） | 2015 - 2020 | [https://universaldependencies.org](https://universaldependencies.org)
    |'
- en: '| NER | ACE | 2000 - 2008 | [https://www.ldc.upenn.edu/collaborations/past-projects/ace](https://www.ldc.upenn.edu/collaborations/past-projects/ace)
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| NER | ACE | 2000 - 2008 | [https://www.ldc.upenn.edu/collaborations/past-projects/ace](https://www.ldc.upenn.edu/collaborations/past-projects/ace)
    |'
- en: '| CoNLL02 | 2002 | [https://www.clips.uantwerpen.be/conll2002/ner/](https://www.clips.uantwerpen.be/conll2002/ner/)
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| CoNLL02 | 2002 | [https://www.clips.uantwerpen.be/conll2002/ner/](https://www.clips.uantwerpen.be/conll2002/ner/)
    |'
- en: '| CoNLL03 | 2003 | [https://www.clips.uantwerpen.be/conll2003/ner/](https://www.clips.uantwerpen.be/conll2003/ner/)
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| CoNLL03 | 2003 | [https://www.clips.uantwerpen.be/conll2003/ner/](https://www.clips.uantwerpen.be/conll2003/ner/)
    |'
- en: '| GENIA | 2004 | [http://www.geniaproject.org/home](http://www.geniaproject.org/home)
    |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| GENIA | 2004 | [http://www.geniaproject.org/home](http://www.geniaproject.org/home)
    |'
- en: '| OntoNotes | 2007 - 2012 | [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| OntoNotes | 2007 - 2012 | [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)
    |'
- en: '| WiNER | 2012 | [http://rali.iro.umontreal.ca/rali/en/winer-wikipedia-for-ner](http://rali.iro.umontreal.ca/rali/en/winer-wikipedia-for-ner)
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| WiNER | 2012 | [http://rali.iro.umontreal.ca/rali/en/winer-wikipedia-for-ner](http://rali.iro.umontreal.ca/rali/en/winer-wikipedia-for-ner)
    |'
- en: '| W-NUT | 2015 - 2018 | [http://noisy-text.github.io](http://noisy-text.github.io)
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| W-NUT | 2015 - 2018 | [http://noisy-text.github.io](http://noisy-text.github.io)
    |'
- en: IV-A1 POS tagging
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 词性标注
- en: 'We will introduce three widely used datasets for part-of-speech tagging: WSJ,
    UD and Rit-Twitter.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍三种广泛使用的词性标注数据集：WSJ、UD 和 Rit-Twitter。
- en: WSJ. A standard dataset for POS tagging is the Wall Street Journal (WSJ) portion
    of the Penn Treebank [[72](#bib.bib72)] and a large number of work use it in their
    experiments. The dataset contains $25$ sections and classifies each word into
    $45$ different types of POS tags. A data split method used in  [[16](#bib.bib16)]
    has become popular, in which sections $0$-$18$ as training data, $19$-$21$ as
    development data, and sections $22$-$24$ as test data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: WSJ。一个标准的词性标注数据集是《华尔街日报》（WSJ）部分的 Penn Treebank [[72](#bib.bib72)]，大量研究在实验中使用了该数据集。该数据集包含
    $25$ 个部分，将每个词分类为 $45$ 种不同的 POS 标签。[[16](#bib.bib16)] 中使用的一种数据拆分方法已经变得流行，其中第 $0$
    到 $18$ 部分作为训练数据，第 $19$ 到 $21$ 部分作为开发数据，第 $22$ 到 $24$ 部分作为测试数据。
- en: UD. Universal Dependencies (UD) is a project that is developing cross-linguistic
    grammatical annotation, which contains more than $100$ treebanks in over $60$
    languages. Its original annotation scheme for part-of-speech tagging take the
    form of Google universal POS tag sets [[87](#bib.bib87)] that include $12$ language-independent
    tags. A recent version of UD  [[80](#bib.bib80)] proposed a POS tag set that has
    17 categories which partially overlap with those defined in [[87](#bib.bib87)],
    and annotations from it have been used by many recent work [[88](#bib.bib88),
    [6](#bib.bib6), [121](#bib.bib121), [43](#bib.bib43)] to evaluate their models.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: UD。Universal Dependencies（UD）是一个开发跨语言语法标注的项目，包含了超过 $100$ 个树库，覆盖 $60$ 多种语言。其原始的词性标注方案采用
    Google 通用 POS 标签集 [[87](#bib.bib87)]，包括 $12$ 种语言无关的标签。UD 的最新版本 [[80](#bib.bib80)]
    提出了一个包含 $17$ 个类别的 POS 标签集，这些类别与 [[87](#bib.bib87)] 中定义的标签部分重叠，其标注结果被许多近期的研究 [[88](#bib.bib88),
    [6](#bib.bib6), [121](#bib.bib121), [43](#bib.bib43)] 用于模型评估。
- en: 'Rit-Twitter. The Rit-Twitter dataset [[95](#bib.bib95)] is a benchmark for
    social media part-of-speech tagging which is comprised of $16K$ tokens from Twitter.
    It adopts an extended version of the PTB tagset with several Twitter-specific
    tags includes: retweets, @usernames, #hashtags, and urls.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Rit-Twitter。Rit-Twitter 数据集 [[95](#bib.bib95)] 是一个社交媒体词性标注的基准数据集，包含来自 Twitter
    的 $16K$ 个标记。它采用了扩展版本的 PTB 标签集，包括一些 Twitter 特有的标签：转发、@用户名、#标签和网址。
- en: IV-A2 NER
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 NER
- en: 'We will introduce three widely used datasets for NER : CoNLL 2002, CoNLL 2003
    and OntoNotes.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍三种广泛使用的 NER 数据集：CoNLL 2002、CoNLL 2003 和 OntoNotes。
- en: 'CoNLL 2002 $\&amp;$ CoNLL 2003. CoNLL 2002 [[98](#bib.bib98)] and CoNLL 2003 [[97](#bib.bib97)]
    are two shared tasks created for NER. Both of these datasets contains annotations
    from newswire text and are tagged with four different entities - PER (person),
    LOC (location), ORG (organization) and MISC (miscellaneous including all other
    types of entities). CoNLL02 focuses on two languages: Dutch and Spanish, while
    CoNLL03 on English and German. Among them, the English dataset of CoNLL03 is the
    most widely used for NER and lots of recent work report their performance on it.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: CoNLL 2002 $\&amp;$ CoNLL 2003。CoNLL 2002 [[98](#bib.bib98)] 和 CoNLL 2003 [[97](#bib.bib97)]
    是两个为命名实体识别（NER）创建的共享任务。这两个数据集都包含来自新闻文本的标注，并标记了四种不同的实体类型——PER（人名）、LOC（地点）、ORG（组织）和MISC（杂项，包括所有其他类型的实体）。CoNLL02
    主要集中于荷兰语和西班牙语，而 CoNLL03 则集中于英语和德语。其中，CoNLL03 的英语数据集是最广泛使用的 NER 数据集，许多近期的研究报告了在该数据集上的性能。
- en: OntoNotes. The OntoNotes project [[33](#bib.bib33)] was developed to annotate
    a large corpus from various genres in three languages (English, Chinese, and Arabic)
    with several layers of annotation, including named entities, coreference, part
    of speech, word sense, propositions, and syntactic parse trees. Regarding the
    NER dataset, the tag set consists of $18$ coarse entity types, containing $89$
    subtypes and the whole dataset contains $2$ million tokens. There have been 5
    versions so far, and the English dataset of the latest Release $5.0$ version [[89](#bib.bib89)]
    has been utilized by many recent NER work in their experiments.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: OntoNotes。OntoNotes 项目 [[33](#bib.bib33)] 旨在对来自三种语言（英语、中文和阿拉伯语）的大型语料库进行注释，注释层次包括命名实体、共指、词性、词义、命题和句法解析树。关于
    NER 数据集，标签集包含 $18$ 种粗略的实体类型，包含 $89$ 个子类型，整个数据集包含 $2$ 百万标记。目前已经有 5 个版本，最新的 Release
    $5.0$ 版本 [[89](#bib.bib89)] 被许多最近的 NER 工作在实验中使用。
- en: IV-A3 Chunking
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 切分
- en: CoNLL 2000. The CoNLL 2000 shared task [[96](#bib.bib96)] dataset is widely
    used for text chunking. The dataset is based on the WSJ part of the Penn Treebank
    as corpus and the annotation consists of $12$ different labels including $11$
    syntactic chunk types in addition to Other. Since it only includes training and
    test sets, many researchers [[65](#bib.bib65), [85](#bib.bib85), [120](#bib.bib120)]
    randomly sampled a part of sentences from training set as the development set.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: CoNLL 2000。CoNLL 2000 共享任务 [[96](#bib.bib96)] 数据集广泛用于文本切分。该数据集基于 Penn Treebank
    的 WSJ 部分作为语料库，注释包括 $12$ 种不同的标签，其中 $11$ 种为句法块类型，另有 Other。由于数据集仅包括训练集和测试集，许多研究者 [[65](#bib.bib65),
    [85](#bib.bib85), [120](#bib.bib120)] 随机抽取了一部分训练集中的句子作为开发集。
- en: IV-B Evaluation Metrics
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 评估指标
- en: Part-of-speech tagging systems are usually evaluated according to the token
    accuracy. And F1-score, the harmonic mean of precision and recall, is usually
    adopted as the evaluation metric of NER and chunking.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注系统通常根据标记准确性进行评估。F1-score，即精确度和召回率的调和均值，通常作为 NER 和切分的评估指标。
- en: IV-B1 Accuracy
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 准确率
- en: Accuracy depicts the ratio of the number of correctly classified instances and
    the total number of instances, which can be computed using the following equation
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率表示正确分类实例的数量与总实例数量的比率，可以使用以下公式计算：
- en: '|  | $ACC=\frac{TP+TN}{TP+TN+FP+FN},$ |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $ACC=\frac{TP+TN}{TP+TN+FP+FN},$ |  |'
- en: where $TP,TN,FP,FN$ denote True positive, True negative, False positive, False
    negative, respectively.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $TP, TN, FP, FN$ 分别表示真正例、真负例、假正例和假负例。
- en: IV-B2 F1-score
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 F1-score
- en: F1-score indicates the fraction of correctly classified instances for each class
    within the dataset, which can be computed as follows
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: F1-score 表示数据集中每个类别的正确分类实例的比例，可以按如下公式计算：
- en: '|  | $F1=\frac{\sum_{i=1}^{C}{2*PERC_{i}*REC_{i}/(PREC_{i}+REC_{i})}}{C},$
    |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $F1=\frac{\sum_{i=1}^{C}{2*PERC_{i}*REC_{i}/(PREC_{i}+REC_{i})}}{C},$
    |  |'
- en: where $PREC$ is precision that is computed by $PREC=\frac{TP}{TP+FP}$, $REC$
    is recall that can be computed by $REC=\frac{TP}{TP+FN}$, and $C$ denotes the
    total number of classes.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $PREC$ 是通过 $PREC=\frac{TP}{TP+FP}$ 计算的精确度，$REC$ 是通过 $REC=\frac{TP}{TP+FN}$
    计算的召回率，$C$ 表示类别总数。
- en: V Comparisons on Experimental Results of Various Techniques
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 各种技术的实验结果比较
- en: 'TABLE III: POS tagging accuracy of different models on test data from WSJ proportion
    of PTB.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：不同模型在 WSJ 数据上测试数据的 POS 标注准确率，PTB 的比例。
- en: '| External resources | Method | Accuracy |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 外部资源 | 方法 | 准确率 |'
- en: '| --- | --- | --- |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| None | Collobert et al. 2011  [[17](#bib.bib17)] | 97.29% |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| None | Collobert et al. 2011  [[17](#bib.bib17)] | 97.29% |'
- en: '| Santos et al. 2014 [[99](#bib.bib99)] | 97.32% |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Santos et al. 2014 [[99](#bib.bib99)] | 97.32% |'
- en: '| Huang et al. 2015 [[35](#bib.bib35)] | 97.55% |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Huang et al. 2015 [[35](#bib.bib35)] | 97.55% |'
- en: '| Ling et al. 2015 [[62](#bib.bib62)] | 97.78% |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Ling et al. 2015 [[62](#bib.bib62)] | 97.78% |'
- en: '| Plank et al.2016 [[88](#bib.bib88)] | 97.22% |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Plank et al.2016 [[88](#bib.bib88)] | 97.22% |'
- en: '| Rei et al. 2016 [[92](#bib.bib92)] | 97.27% |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Rei et al. 2016 [[92](#bib.bib92)] | 97.27% |'
- en: '| Vaswani et al. 2016  [[110](#bib.bib110)] | 97.40% |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Vaswani et al. 2016  [[110](#bib.bib110)] | 97.40% |'
- en: '| Andor et al. 2016 [[2](#bib.bib2)] | 97.44% |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Andor et al. 2016 [[2](#bib.bib2)] | 97.44% |'
- en: '| Ma and Hovy 2016 [[71](#bib.bib71)] | 97.55% |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Ma and Hovy 2016 [[71](#bib.bib71)] | 97.55% |'
- en: '| Ma and Sun 2016 [[70](#bib.bib70)] | 97.56% |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| Ma and Sun 2016 [[70](#bib.bib70)] | 97.56% |'
- en: '| Rei 2017 [[91](#bib.bib91)] | 97.43% |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Rei 2017 [[91](#bib.bib91)] | 97.43% |'
- en: '| Yang et al. 2017 [[120](#bib.bib120)] | 97.55% |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al. 2017 [[120](#bib.bib120)] | 97.55% |'
- en: '| Kazi and Thompson 2017  [[42](#bib.bib42)] | 97.37% |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Kazi and Thompson 2017  [[42](#bib.bib42)] | 97.37% |'
- en: '| Bohnet et al. 2018  [[8](#bib.bib8)] | 97.96% |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Bohnet et al. 2018  [[8](#bib.bib8)] | 97.96% |'
- en: '| Yasunaga et al. 2018 [[121](#bib.bib121)] | 97.55% |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Yasunaga et al. 2018 [[121](#bib.bib121)] | 97.55% |'
- en: '| Liu et al. 2018 [[65](#bib.bib65)] | 97.53% |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. 2018 [[65](#bib.bib65)] | 97.53% |'
- en: '| Zhang et al. 2018 [[127](#bib.bib127)] | 97.59% |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. 2018 [[127](#bib.bib127)] | 97.59% |'
- en: '| Xin et al. 2018 [[116](#bib.bib116)] | 97.58% |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Xin et al. 2018 [[116](#bib.bib116)] | 97.58% |'
- en: '| Zhang et al. 2018 [[128](#bib.bib128)] | 97.55% |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. 2018 [[128](#bib.bib128)] | 97.55% |'
- en: '| Hu et al. 2019 [[34](#bib.bib34)] | 97.52% |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Hu et al. 2019 [[34](#bib.bib34)] | 97.52% |'
- en: '| Cui et al. 2019 [[18](#bib.bib18)] | 97.65% |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Cui et al. 2019 [[18](#bib.bib18)] | 97.65% |'
- en: '| Jiang et al. 2020 [[39](#bib.bib39)] | 97.7% |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| Jiang et al. 2020 [[39](#bib.bib39)] | 97.7% |'
- en: '| Unlabeled Word Corpus | Akbik et al. 2018  [[1](#bib.bib1)] | 97.85% |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 未标注词语语料库 | Akbik et al. 2018  [[1](#bib.bib1)] | 97.85% |'
- en: '| Clark et al. 2018  [[15](#bib.bib15)] | 97.7% |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Clark et al. 2018  [[15](#bib.bib15)] | 97.7% |'
- en: 'TABLE IV: F1-score of different models on test data from CoNLL 2003 NER(English).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 不同模型在 CoNLL 2003 NER（英语）测试数据上的 F1-score。'
- en: '| External resources | Method | F1-score |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 外部资源 | 方法 | F1-score |'
- en: '| None | Collobert et al. 2011 [[17](#bib.bib17)] | 88.67% |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 无 | Collobert et al. 2011 [[17](#bib.bib17)] | 88.67% |'
- en: '| Kuru et al. 2016 [[50](#bib.bib50)] | 84.52% |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Kuru et al. 2016 [[50](#bib.bib50)] | 84.52% |'
- en: '| Chiu and Nichols 2016 [[13](#bib.bib13)] | 90.91% |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Chiu and Nichols 2016 [[13](#bib.bib13)] | 90.91% |'
- en: '| Lample et al. 2016 [[52](#bib.bib52)] | 90.94% |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Lample et al. 2016 [[52](#bib.bib52)] | 90.94% |'
- en: '| Ma and Hovy 2016 [[71](#bib.bib71)] | 91.21% |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Ma and Hovy 2016 [[71](#bib.bib71)] | 91.21% |'
- en: '| Rei 2017  [[91](#bib.bib91)] | 86.26% |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Rei 2017  [[91](#bib.bib91)] | 86.26% |'
- en: '| Strubell et al. 2017  [[104](#bib.bib104)] | 90.54% |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Strubell et al. 2017  [[104](#bib.bib104)] | 90.54% |'
- en: '| Zhang et al. 2017  [[126](#bib.bib126)] | 90.70% |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. 2017  [[126](#bib.bib126)] | 90.70% |'
- en: '| Tran et al. 2017 [[109](#bib.bib109)] | 91.23% |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Tran et al. 2017 [[109](#bib.bib109)] | 91.23% |'
- en: '| Wang et al. 2017  [[113](#bib.bib113)] | 91.24% |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. 2017  [[113](#bib.bib113)] | 91.24% |'
- en: '| Sato et al. 2017 [[101](#bib.bib101)] | 91.28% |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Sato et al. 2017 [[101](#bib.bib101)] | 91.28% |'
- en: '| Shen et al. 2018  [[103](#bib.bib103)] | 90.69% |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Shen et al. 2018  [[103](#bib.bib103)] | 90.69% |'
- en: '| Zhang et al. 2018 [[127](#bib.bib127)] | 91.22% |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. 2018 [[127](#bib.bib127)] | 91.22% |'
- en: '| Liu et al. 2018  [[65](#bib.bib65)] | 91.24% |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. 2018  [[65](#bib.bib65)] | 91.24% |'
- en: '| Ye and Ling 2018 [[122](#bib.bib122)] | 91.38% |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Ye and Ling 2018 [[122](#bib.bib122)] | 91.38% |'
- en: '| Gregoric et al. 2018  [[26](#bib.bib26)] | 91.48% |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Gregoric et al. 2018  [[26](#bib.bib26)] | 91.48% |'
- en: '| Zhang et al. 2018 [[128](#bib.bib128)] | 91.57% |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. 2018 [[128](#bib.bib128)] | 91.57% |'
- en: '| Xin et al. 2018 [[116](#bib.bib116)] | 91.64% |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Xin et al. 2018 [[116](#bib.bib116)] | 91.64% |'
- en: '| Hu et al. 2019 [[34](#bib.bib34)] | 91.40% |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Hu et al. 2019 [[34](#bib.bib34)] | 91.40% |'
- en: '| Chen et al. 2019 [[10](#bib.bib10)] | 91.44% |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al. 2019 [[10](#bib.bib10)] | 91.44% |'
- en: '| Yan et al. 2019 [[118](#bib.bib118)] | 91.45% |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Yan et al. 2019 [[118](#bib.bib118)] | 91.45% |'
- en: '| Liu et al. 2019 [[67](#bib.bib67)] | 91.96% |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. 2019 [[67](#bib.bib67)] | 91.96% |'
- en: '| Luo et al. 2020 [[68](#bib.bib68)] | 91.96% |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Luo et al. 2020 [[68](#bib.bib68)] | 91.96% |'
- en: '| Jiang et al. 2020 [[39](#bib.bib39)] | 92.2% |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Jiang et al. 2020 [[39](#bib.bib39)] | 92.2% |'
- en: '| Li et al. 2020 [[58](#bib.bib58)] | 92.67% |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. 2020 [[58](#bib.bib58)] | 92.67% |'
- en: '| CoNLL00,WSJ | Yang et al. 2017  [[120](#bib.bib120)] | 91.26% |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| CoNLL00,WSJ | Yang et al. 2017  [[120](#bib.bib120)] | 91.26% |'
- en: '| Gazetteers | Collobert et al. 2011 [[17](#bib.bib17)] | 89.59% |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 地名词典 | Collobert et al. 2011 [[17](#bib.bib17)] | 89.59% |'
- en: '| Huang et al. 2015 [[35](#bib.bib35)] | 90.10% |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| Huang et al. 2015 [[35](#bib.bib35)] | 90.10% |'
- en: '| Wu et al. 2018 [[115](#bib.bib115)] | 91.89% |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al. 2018 [[115](#bib.bib115)] | 91.89% |'
- en: '| Liu et al. 2019 [[66](#bib.bib66)] | 92.75% |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. 2019 [[66](#bib.bib66)] | 92.75% |'
- en: '| Chen et al. 2020 [[11](#bib.bib11)] | 91.76% |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al. 2020 [[11](#bib.bib11)] | 91.76% |'
- en: '| Lexicons | Chiu and Nichols 2016 [[13](#bib.bib13)] | 91.62% |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 词汇表 | Chiu and Nichols 2016 [[13](#bib.bib13)] | 91.62% |'
- en: '| Sato et al. 2017 [[101](#bib.bib101)] | 91.55% |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| Sato et al. 2017 [[101](#bib.bib101)] | 91.55% |'
- en: '| Ghaddar and Langlais 2018 [[23](#bib.bib23)] | 91.73% |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Ghaddar and Langlais 2018 [[23](#bib.bib23)] | 91.73% |'
- en: '| Unlabeled Word Corpus | Peters et al. 2017 [[85](#bib.bib85)] | 91.93% |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 未标注词语语料库 | Peters et al. 2017 [[85](#bib.bib85)] | 91.93% |'
- en: '| Peters et al. 2018 [[86](#bib.bib86)] | 92.22% |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Peters et al. 2018 [[86](#bib.bib86)] | 92.22% |'
- en: '| Devlin et al. 2018 [[19](#bib.bib19)] | 92.80% |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Devlin et al. 2018 [[19](#bib.bib19)] | 92.80% |'
- en: '| Akbik et al. 2018 [[1](#bib.bib1)] | 93.09% |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Akbik et al. 2018 [[1](#bib.bib1)] | 93.09% |'
- en: '| Clark et al. 2018 [[15](#bib.bib15)] | 92.6% |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| Clark et al. 2018 [[15](#bib.bib15)] | 92.6% |'
- en: '| Li et al. 2020 [[57](#bib.bib57)] | 93.04% |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. 2020 [[57](#bib.bib57)] | 93.04% |'
- en: '| LM emb | Tran et al. 2017 [[109](#bib.bib109)] | 91.69% |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| LM emb | Tran et al. 2017 [[109](#bib.bib109)] | 91.69% |'
- en: '| Chen et al. 2019 [[10](#bib.bib10)] | 92.34% |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al. 2019 [[10](#bib.bib10)] | 92.34% |'
- en: '| Hu et al. 2019 [[34](#bib.bib34)] | 92.36% |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Hu et al. 2019 [[34](#bib.bib34)] | 92.36% |'
- en: '| Liu et al. 2019 [[67](#bib.bib67)] | 93.47% |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. 2019 [[67](#bib.bib67)] | 93.47% |'
- en: '| Jiang et al. 2019 [[38](#bib.bib38)] | 93.47% |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Jiang et al. 2019 [[38](#bib.bib38)] | 93.47% |'
- en: '| Luo et al. 2019 [[68](#bib.bib68)] | 93.37% |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Luo et al. 2019 [[68](#bib.bib68)] | 93.37% |'
- en: '| Knowledge Graph | He et al. 2020  [[31](#bib.bib31)] | 91.8% |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 知识图谱 | He et al. 2020 [[31](#bib.bib31)] | 91.8% |'
- en: 'TABLE V: F1-score of different models on test data from CoNLL 2000 Chunking.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：不同模型在 CoNLL 2000 Chunking 测试数据上的 F1 分数。
- en: '| External resources | Method | F1-score |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 外部资源 | 方法 | F1分数 |'
- en: '| --- | --- | --- |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| None | Collobert et al. 2011 [[17](#bib.bib17)] | 94.32% |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 无 | Collobert et al. 2011 [[17](#bib.bib17)] | 94.32% |'
- en: '| Huang et al. 2015 [[35](#bib.bib35)] | 94.46% |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Huang et al. 2015 [[35](#bib.bib35)] | 94.46% |'
- en: '| Rei et al. 2016 [[92](#bib.bib92)] | 92.67% |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| Rei et al. 2016 [[92](#bib.bib92)] | 92.67% |'
- en: '| Rei 2017 [[91](#bib.bib91)] | 93.88% |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Rei 2017 [[91](#bib.bib91)] | 93.88% |'
- en: '| Zhai et al. 2017 [[124](#bib.bib124)] | 94.72% |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| Zhai et al. 2017 [[124](#bib.bib124)] | 94.72% |'
- en: '| Sato et al. 2017 [[101](#bib.bib101)] | 94.84% |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Sato et al. 2017 [[101](#bib.bib101)] | 94.84% |'
- en: '| Zhang et al. 2017 [[126](#bib.bib126)] | 95.01% |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. 2017 [[126](#bib.bib126)] | 95.01% |'
- en: '| Xin et al. 2018 [[116](#bib.bib116)] | 95.29% |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Xin et al. 2018 [[116](#bib.bib116)] | 95.29% |'
- en: '| Hu et al. 2019 [[34](#bib.bib34)] | 95.14% |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| Hu et al. 2019 [[34](#bib.bib34)] | 95.14% |'
- en: '| Liu et al. 2019 [[67](#bib.bib67)] | 95.43% |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. 2019 [[67](#bib.bib67)] | 95.43% |'
- en: '|  | Chen et al. 2020 [[11](#bib.bib11)] | 95.45% |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | Chen et al. 2020 [[11](#bib.bib11)] | 95.45% |'
- en: '| Unlabeled Word Corpus | Peters et al. 2017 [[85](#bib.bib85)] | 96.37% |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 未标记词汇语料库 | Peters et al. 2017 [[85](#bib.bib85)] | 96.37% |'
- en: '| Akbik et al. 2018 [[1](#bib.bib1)] | 96.72% |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Akbik et al. 2018 [[1](#bib.bib1)] | 96.72% |'
- en: '| Clark et al. 2018 [[15](#bib.bib15)] | 97% |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Clark et al. 2018 [[15](#bib.bib15)] | 97% |'
- en: '| LM emb | Liu et al. 2019 [[67](#bib.bib67)] | 97.3% |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| LM emb | Liu et al. 2019 [[67](#bib.bib67)] | 97.3% |'
- en: '| CoNLL03,WSJ | Yang et al. 2017 [[120](#bib.bib120)] | 95.41% |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| CoNLL03,WSJ | Yang et al. 2017 [[120](#bib.bib120)] | 95.41% |'
- en: While formal experimental evaluation is left out of the scope of this paper,
    we present a brief analysis of the experimental results of various techniques.
    For each of these three tasks, we choose one widely used dataset and report the
    performance of various models on the benchmark. The three datasets includes WSJ
    for POS, CoNLL 2003 NER and CoNLL 2000 chunking, and the results for these three
    tasks are given in Table [III](#S5.T3 "TABLE III ‣ V Comparisons on Experimental
    Results of Various Techniques ‣ A Survey on Recent Advances in Sequence Labeling
    from Deep Learning Models"), Table [IV](#S5.T4 "TABLE IV ‣ V Comparisons on Experimental
    Results of Various Techniques ‣ A Survey on Recent Advances in Sequence Labeling
    from Deep Learning Models") and Table [V](#S5.T5 "TABLE V ‣ V Comparisons on Experimental
    Results of Various Techniques ‣ A Survey on Recent Advances in Sequence Labeling
    from Deep Learning Models"), respectively. We also indicate whether the model
    makes use of external knowledge or resource in these tables.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然正式的实验评估超出了本文的范围，我们对各种技术的实验结果进行了简要分析。对于这三项任务中的每一项，我们选择了一个广泛使用的数据集，并报告了各种模型在基准测试上的性能。这三种数据集包括用于词性标注的
    WSJ、用于命名实体识别的 CoNLL 2003 和用于块标记的 CoNLL 2000，结果分别列在表 [III](#S5.T3 "TABLE III ‣
    V Comparisons on Experimental Results of Various Techniques ‣ A Survey on Recent
    Advances in Sequence Labeling from Deep Learning Models")、表 [IV](#S5.T4 "TABLE
    IV ‣ V Comparisons on Experimental Results of Various Techniques ‣ A Survey on
    Recent Advances in Sequence Labeling from Deep Learning Models") 和表 [V](#S5.T5
    "TABLE V ‣ V Comparisons on Experimental Results of Various Techniques ‣ A Survey
    on Recent Advances in Sequence Labeling from Deep Learning Models") 中。我们还在这些表中指明模型是否利用了外部知识或资源。
- en: As shown in Table [III](#S5.T3 "TABLE III ‣ V Comparisons on Experimental Results
    of Various Techniques ‣ A Survey on Recent Advances in Sequence Labeling from
    Deep Learning Models"), different models have achieved relatively high performance
    (more than $97\%$) in terms of the accuracy of POS tagging. Among these work listed
    in the table, the Bi-LSTM-CNN-CRF model proposed by Ma and Hovy [[71](#bib.bib71)]
    has become a popular baseline for most subsequent work in this field, which is
    also the first end-to-end model for sequence labeling requiring no feature engineering
    or data preprocessing. The reported accuracy of Ma and Hovy is $97.55\%$, and
    several studies in recent two years slightly outperform it by exploring different
    issues and building new models. For example, the model proposed by Zhang et al. [[127](#bib.bib127)]
    performs better with a improvement of $0.04\%$, which takes the long range tag
    dependencies into consideration by incorporating a tag LSTM in their model. Besides,
    Bohnet et al. [[8](#bib.bib8)] achieves the state-of-the-art performance with
    $97.96\%$ accuracy by modeling the sentence-level context for initial character
    and word-based representations.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如表格 [III](#S5.T3 "TABLE III ‣ V Comparisons on Experimental Results of Various
    Techniques ‣ A Survey on Recent Advances in Sequence Labeling from Deep Learning
    Models") 所示，不同的模型在词性标注的准确性方面已取得了相对较高的表现（超过 $97\%$）。在表中列出的这些工作中，Ma 和 Hovy 提出的 Bi-LSTM-CNN-CRF
    模型 [[71](#bib.bib71)] 已成为该领域后续大多数工作的流行基线，它也是第一个无需特征工程或数据预处理的端到端序列标注模型。Ma 和 Hovy
    报告的准确率为 $97.55\%$，而近年来一些研究通过探索不同问题和建立新模型稍微超越了它。例如，Zhang 等人提出的模型 [[127](#bib.bib127)]
    在改进 $0.04\%$ 的基础上表现更好，该模型通过在其模型中引入标签 LSTM 来考虑长范围标签依赖。此外，Bohnet 等人 [[8](#bib.bib8)]
    通过建模句子级上下文来获得初始字符和基于词的表示，取得了 $97.96\%$ 的最新性能。
- en: Table [IV](#S5.T4 "TABLE IV ‣ V Comparisons on Experimental Results of Various
    Techniques ‣ A Survey on Recent Advances in Sequence Labeling from Deep Learning
    Models") shows the results of different models on CoNLL 2003 NER datatsets. Compared
    with the POS tagging task, the overall score of NER task is lower, with most work
    between $91\%$ and $92\%$, which indicates NER is more difficult than POS tagging.
    Among the work that utilize no external resources, Li et al. [[58](#bib.bib58)]
    performs best, with a average F1-score of $92.67\%$. Their proposed model focuses
    on rare entities and applied novel techniques including local context reconstruction
    and delexicalized entity identification. We can observe that models which utilize
    external resources can generally achieve higher performance on all these three
    tasks, especially pretraining language models that using large unlabeled word
    corpus. But these models require a larger neural network that need huge computing
    resources and longer time for training.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [IV](#S5.T4 "TABLE IV ‣ V Comparisons on Experimental Results of Various
    Techniques ‣ A Survey on Recent Advances in Sequence Labeling from Deep Learning
    Models") 显示了不同模型在 CoNLL 2003 NER 数据集上的结果。与词性标注任务相比，NER 任务的总体得分较低，大多数工作的得分在 $91\%$
    和 $92\%$ 之间，这表明 NER 比词性标注更为困难。在不使用外部资源的工作中，Li 等人 [[58](#bib.bib58)] 的表现最好，平均 F1-score
    达到 $92.67\%$。他们提出的模型专注于稀有实体，并应用了包括局部上下文重建和去词汇化实体识别在内的新技术。我们可以观察到，利用外部资源的模型在所有这三项任务上通常可以实现更高的性能，特别是使用大规模未标记词语语料库的预训练语言模型。然而，这些模型需要更大的神经网络，需耗费大量计算资源和更长的训练时间。
- en: VI The Promising Paths for Future Research
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 未来研究的有前途的路径
- en: Although much success has been achieved in this filed, challenges still exist
    from different perspectives. In this section, we provide the following directions
    for further research in deep learning based sequence labeling.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这一领域取得了许多成功，但从不同的角度来看，仍然存在挑战。在本节中，我们提供了以下针对基于深度学习的序列标注的进一步研究方向。
- en: 'Sequence labeling for low-resource data. Supervised learning algorithms including
    deep learning based models, rely on large annotated data for training. However,
    data annotations are expensive and often take a lot of time, leaving a big challenge
    in sequence labeling for many low-resource languages and specific resource-poor
    domains. Although some work have explored methods for this problem, there still
    exists a large scope for improvement. Future efforts could be dedicated on enhancing
    performance of sequence labeling on low-resource data by focusing on the following
    three research directions: (1) training a LM like BERT with the unlabeled corpus
    and finetune it with limited labeled corpus in a low-resource data; (2) providing
    more effective deep transfer learning models to transfer knowledge from one language
    or domain to another; (3) exploring appropriate data augmentation techniques to
    enlarge the available data for sequence labeling.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 针对低资源数据的序列标注。监督学习算法，包括基于深度学习的模型，依赖大量标注数据进行训练。然而，数据标注成本高昂且常常耗时，这在许多低资源语言和特定资源匮乏领域的序列标注中带来了巨大挑战。尽管一些研究已经探索了该问题的方法，但仍有很大的改进空间。未来的工作可以集中在以下三个研究方向，以提升低资源数据上的序列标注性能：（1）使用未标注语料库训练类似于`BERT`的语言模型，并用有限的标注语料进行微调；（2）提供更有效的深度迁移学习模型，以将知识从一种语言或领域转移到另一种；（3）探索合适的数据增强技术，以扩大用于序列标注的数据量。
- en: Scalability of deep learning based sequence labeling. Most neural models for
    sequence labeling do not scale well for large data, making it a challenge to build
    more scalable deep learning based sequence labeling models. The main reason for
    this is when the size of data grows, the parameters of models increase exponentially,
    leading to the high complexity of back propagation. While several models have
    achieved excellent performance with huge computing power, there exists need for
    developing approaches to balance model complexity and scalability. In addition,
    for pratical usage, its necessary to develop scalable methods for real-world applications.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的序列标注的可扩展性。大多数序列标注的神经模型在处理大规模数据时扩展性较差，这使得构建更具可扩展性的基于深度学习的序列标注模型成为一个挑战。主要原因是随着数据规模的增长，模型的参数呈指数级增长，导致反向传播的复杂度增加。尽管一些模型凭借巨大的计算能力取得了出色的性能，但仍需要开发平衡模型复杂性和可扩展性的方法。此外，为了实际应用，有必要开发适用于现实世界的可扩展方法。
- en: Utilization of external resources. As discussed in Section [V](#S5 "V Comparisons
    on Experimental Results of Various Techniques ‣ A Survey on Recent Advances in
    Sequence Labeling from Deep Learning Models"), the performance of neural sequence
    labeling models benefits significantly from external resources, including gazetteers,
    lexicons, large unlabeled word corpus, and etc. Though some research effort have
    been dedicated on this issue, how to effectively incorporate external resources
    in neural sequence labeling models remains to be explored.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 外部资源的利用。如在第[V](#S5 "V Comparisons on Experimental Results of Various Techniques
    ‣ A Survey on Recent Advances in Sequence Labeling from Deep Learning Models")节中讨论的那样，神经序列标注模型的性能在很大程度上受益于外部资源，包括地名词典、词汇表、大规模未标注词语语料库等。虽然一些研究已经致力于这一问题，但如何有效地将外部资源整合到神经序列标注模型中仍需进一步探索。
- en: VII Conclusions
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: This survey aims to thoroughly review applications of deep learning techniques
    in sequence labeling, and provides a panoramic view so that readers can build
    a comprehensive understanding of this area. We present a summary for the literature
    with a scientific taxonomy. In addition, we provide an overview of the datasets
    and evaluation metrics of the commonly studied tasks of sequence labeling problems.
    Besides, we also discuss and compare the results of different models and analyze
    the factors and different architectures that affect the performance. Finally,
    we present readers with the challenges and open issues faced by current methods
    and identify the future directions in this area. We hope that this survey can
    help to enlighten and guide the researchers, practitioners, and educators who
    are interested in sequence labeling.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述旨在深入审查深度学习技术在序列标注中的应用，并提供一个全景视角，以便读者可以建立对这一领域的全面理解。我们呈现了带有科学分类法的文献总结。此外，我们概述了常见序列标注问题的任务数据集和评估指标。除此之外，我们还讨论和比较了不同模型的结果，并分析了影响性能的因素和不同架构。最后，我们向读者展示了当前方法面临的挑战和未解问题，并确定了该领域的未来方向。我们希望本综述能够帮助启发和指导对序列标注感兴趣的研究人员、从业者和教育工作者。
- en: References
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Alan Akbik, Duncan Blythe, and Roland Vollgraf. Contextual string embeddings
    for sequence labeling. In COLING, pages 1638–1649, 2018.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Alan Akbik, Duncan Blythe, 和 Roland Vollgraf. 用于序列标注的上下文字符串嵌入. 载于COLING,
    第1638–1649页, 2018年。'
- en: '[2] Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro
    Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. Globally normalized
    transition-based neural networks. arXiv preprint arXiv:1603.06042, 2016.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro
    Presta, Kuzman Ganchev, Slav Petrov, 和 Michael Collins. 全局归一化的基于转移的神经网络. arXiv预印本
    arXiv:1603.06042, 2016年。'
- en: '[3] Leonard E Baum and Ted Petrie. Statistical inference for probabilistic
    functions of finite state markov chains. The annals of mathematical statistics,
    37(6):1554–1563, 1966.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Leonard E Baum 和 Ted Petrie. 有限状态马尔可夫链的概率函数的统计推断. 数学统计年鉴, 37(6):1554–1563,
    1966年。'
- en: '[4] Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder.
    Sub-event detection from twitter streams as a sequence labeling problem. NAACL,
    2019.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Giannis Bekoulis, Johannes Deleu, Thomas Demeester, 和 Chris Develder. 从Twitter流中检测子事件作为序列标注问题.
    NAACL, 2019年。'
- en: '[5] Oliver Bender, Franz Josef Och, and Hermann Ney. Maximum entropy models
    for named entity recognition. In Proceedings of the seventh conference on Natural
    language learning at HLT-NAACL 2003-Volume 4, pages 148–151\. Association for
    Computational Linguistics, 2003.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Oliver Bender, Franz Josef Och, 和 Hermann Ney. 用于命名实体识别的最大熵模型. 载于第七届自然语言学习会议HLT-NAACL
    2003年卷4，第148–151页. 计算语言学协会, 2003年。'
- en: '[6] Gáabor Berend. Sparse coding of neural word embeddings for multilingual
    sequence labeling. Transactions of the Association for Computational Linguistics,
    5:247–261, 2017.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Gáabor Berend. 多语言序列标注的神经词嵌入稀疏编码. 计算语言学协会会刊, 5:247–261, 2017年。'
- en: '[7] Daniel M Bikel, Richard Schwartz, and Ralph M Weischedel. An algorithm
    that learns what’s in a name. Machine learning, 34(1-3):211–231, 1999.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Daniel M Bikel, Richard Schwartz, 和 Ralph M Weischedel. 一个学习名字内容的算法. 机器学习,
    34(1-3):211–231, 1999年。'
- en: '[8] Bernd Bohnet, Ryan McDonald, Goncalo Simoes, Daniel Andor, Emily Pitler,
    and Joshua Maynez. Morphosyntactic tagging with a meta-bilstm model over context
    sensitive token encodings. ACL, 2018.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Bernd Bohnet, Ryan McDonald, Goncalo Simoes, Daniel Andor, Emily Pitler,
    和 Joshua Maynez. 使用上下文敏感标记编码的meta-bilstm模型进行形态句法标注. ACL, 2018年。'
- en: '[9] Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, and Shengping Liu. Adversarial
    transfer learning for chinese named entity recognition with self-attention mechanism.
    In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing, pages 182–192, 2018.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, 和 Shengping Liu. 使用自注意力机制的中文命名实体识别对抗转移学习.
    载于2018年自然语言处理经验方法会议论文集，第182–192页, 2018年。'
- en: '[10] Hui Chen, Zijia Lin, Guiguang Ding, Jianguang Lou, Yusen Zhang, and Borje
    Karlsson. Grn: Gated relation network to enhance convolutional neural network
    for named entity recognition. AAAI, 2019.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Hui Chen, Zijia Lin, Guiguang Ding, Jianguang Lou, Yusen Zhang, 和 Borje
    Karlsson. GRN：门控关系网络，增强卷积神经网络的命名实体识别. AAAI, 2019年。'
- en: '[11] Luoxin Chen, Weitong Ruan, Xinyue Liu, and Jianhua Lu. Seqvat: Virtual
    adversarial training for semi-supervised sequence labeling. In Proceedings of
    the 58th Annual Meeting of the Association for Computational Linguistics, pages
    8801–8811, 2020.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Luoxin Chen, Weitong Ruan, Xinyue Liu, 和 Jianhua Lu. Seqvat：用于半监督序列标注的虚拟对抗训练。在第58届计算语言学协会年会上，页8801–8811,
    2020。'
- en: '[12] Hai Leong Chieu and Hwee Tou Ng. Named entity recognition: a maximum entropy
    approach using global information. In Proceedings of the 19th international conference
    on Computational linguistics-Volume 1, pages 1–7\. Association for Computational
    Linguistics, 2002.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Hai Leong Chieu 和 Hwee Tou Ng. 命名实体识别：一种使用全局信息的最大熵方法。在第19届国际计算语言学会议论文集中，第1卷，页1–7。计算语言学协会，2002。'
- en: '[13] Jason PC Chiu and Eric Nichols. Named entity recognition with bidirectional
    lstm-cnns. Transactions of the Association for Computational Linguistics, 4:357–370,
    2016.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Jason PC Chiu 和 Eric Nichols. 使用双向 LSTM-CNNs 的命名实体识别。《计算语言学协会会刊》，4:357–370,
    2016。'
- en: '[14] Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio.
    On the properties of neural machine translation: Encoder-decoder approaches. arXiv
    preprint arXiv:1409.1259, 2014.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, 和 Yoshua Bengio.
    神经机器翻译的性质：编码器-解码器方法。arXiv 预印本 arXiv:1409.1259, 2014。'
- en: '[15] Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc V Le. Semi-supervised
    sequence modeling with cross-view training. EMNLP, 2018.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Kevin Clark, Minh-Thang Luong, Christopher D Manning, 和 Quoc V Le. 通过跨视角训练的半监督序列建模。EMNLP,
    2018。'
- en: '[16] Michael Collins. Discriminative training methods for hidden markov models:
    Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02
    conference on Empirical methods in natural language processing-Volume 10, pages
    1–8\. Association for Computational Linguistics, 2002.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Michael Collins. 隐马尔可夫模型的判别训练方法：理论和感知机算法的实验。在 ACL-02 会议的《自然语言处理的经验方法》论文集中，第10卷，页1–8。计算语言学协会，2002。'
- en: '[17] Ronan Collobert, Koray Kavukcuoglu, Jason Weston, Leon Bottou, Pavel Kuksa,
    and Michael Karlen. Natural language processing (almost) from scratch. Journal
    of Machine Learning Research, 12(1):2493–2537, 2011.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Ronan Collobert, Koray Kavukcuoglu, Jason Weston, Leon Bottou, Pavel Kuksa,
    和 Michael Karlen. 自然语言处理（几乎）从零开始。《机器学习研究期刊》，12(1):2493–2537, 2011。'
- en: '[18] Leyang Cui and Yue Zhang. Hierarchically-refined label attention network
    for sequence labeling. EMNLP, 2019.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Leyang Cui 和 Yue Zhang. 层次化精细化标签注意力网络用于序列标注。EMNLP, 2019。'
- en: '[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova. Bert：深度双向变换器的预训练以理解语言。arXiv
    预印本 arXiv:1810.04805, 2018。'
- en: '[20] Timothy Dozat, Peng Qi, and Christopher D Manning. Stanford’s graph-based
    neural dependency parser at the conll 2017 shared task. In Proceedings of the
    CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,
    pages 20–30, 2017.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Timothy Dozat, Peng Qi, 和 Christopher D Manning. 斯坦福图基神经依赖解析器在 CoNLL 2017
    共享任务中的表现。在 CoNLL 2017 共享任务：从原始文本到通用依赖的多语言解析的论文集中，页20–30, 2017。'
- en: '[21] Sean R Eddy. Hidden markov models. Current opinion in structural biology,
    6(3):361–365, 1996.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Sean R Eddy. 隐马尔可夫模型。《结构生物学当前观点》，6(3):361–365, 1996。'
- en: '[22] Xiaocheng Feng, Xiachong Feng, Bing Qin, Zhangyin Feng, and Ting Liu.
    Improving low resource named entity recognition using cross-lingual knowledge
    transfer. In IJCAI, pages 4071–4077, 2018.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Xiaocheng Feng, Xiachong Feng, Bing Qin, Zhangyin Feng, 和 Ting Liu. 使用跨语言知识转移改进低资源命名实体识别。在
    IJCAI, 页4071–4077, 2018。'
- en: '[23] Abbas Ghaddar and Philippe Langlais. Robust lexical features for improved
    neural network named-entity recognition. arXiv preprint arXiv:1806.03489, 2018.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Abbas Ghaddar 和 Philippe Langlais. 为改进神经网络命名实体识别而采用的鲁棒词汇特征。arXiv 预印本 arXiv:1806.03489,
    2018。'
- en: '[24] Carlos Gómez-Rodríguez and David Vilares. Constituent parsing as sequence
    labeling. EMNLP, 2018.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Carlos Gómez-Rodríguez 和 David Vilares. 作为序列标注的成分解析。EMNLP, 2018。'
- en: '[25] Sian Gooding and Ekaterina Kochmar. Complex word identification as a sequence
    labelling task. ACL, 2019.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Sian Gooding 和 Ekaterina Kochmar. 复杂词汇识别作为序列标注任务。ACL, 2019。'
- en: '[26] Andrej Zukov Gregoric, Yoram Bachrach, and Sam Coope. Named entity recognition
    with parallel recurrent neural networks. In Proceedings of the 56th Annual Meeting
    of the Association for Computational Linguistics (Volume 2: Short Papers), pages
    69–74, 2018.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Andrej Zukov Gregoric, Yoram Bachrach, 和 Sam Coope. 使用并行递归神经网络进行命名实体识别。载于《第56届计算语言学协会年会论文集（第2卷：短论文）》，页码69–74，2018年。'
- en: '[27] Tao Gui, Qi Zhang, Haoran Huang, Minlong Peng, and Xuanjing Huang. Part-of-speech
    tagging for twitter with adversarial neural networks. In Proceedings of the 2017
    Conference on Empirical Methods in Natural Language Processing, pages 2411–2420,
    2017.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Tao Gui, Qi Zhang, Haoran Huang, Minlong Peng, 和 Xuanjing Huang. 使用对抗神经网络进行Twitter的词性标注。载于《2017年自然语言处理实证方法会议论文集》，页码2411–2420，2017年。'
- en: '[28] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng
    Zhang. Star-transformer. NAACL, 2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, 和 Zheng
    Zhang. Star-transformer。NAACL，2019年。'
- en: '[29] James Hammerton. Named entity recognition with long short-term memory.
    In Proceedings of the seventh conference on Natural language learning at HLT-NAACL
    2003-Volume 4, pages 172–175\. Association for Computational Linguistics, 2003.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] James Hammerton. 使用长短期记忆网络进行命名实体识别。载于《第七届自然语言学习会议论文集 HLT-NAACL 2003》第4卷，页码172–175。计算语言学协会，2003年。'
- en: '[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 770–778, 2016.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 用于图像识别的深度残差学习。载于《IEEE计算机视觉与模式识别会议论文集》，页码770–778，2016年。'
- en: '[31] Qizhen He, Liang Wu, Yida Yin, and Heming Cai. Knowledge-graph augmented
    word representations for named entity recognition. AAAI, 2020.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Qizhen He, Liang Wu, Yida Yin, 和 Heming Cai. 知识图谱增强的词表示用于命名实体识别。AAAI，2020年。'
- en: '[32] Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard
    Scholkopf. Support vector machines. IEEE Intelligent Systems and their applications,
    13(4):18–28, 1998.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, 和 Bernhard Scholkopf.
    支持向量机。IEEE智能系统及其应用，13(4):18–28，1998年。'
- en: '[33] Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph
    Weischedel. Ontonotes: The 90$\backslash$% solution. In Proceedings of the human
    language technology conference of the NAACL, Companion Volume: Short Papers, 2006.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, 和 Ralph Weischedel.
    Ontonotes: 90$\backslash$% 解决方案。载于NAACL人类语言技术会议论文集，附录卷：短论文，2006年。'
- en: '[34] Kai Hu, Zhijian Ou, Min Hu, and Junlan Feng. Neural crf transducers for
    sequence labeling. In ICASSP 2019-2019 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), pages 2997–3001\. IEEE, 2019.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Kai Hu, Zhijian Ou, Min Hu, 和 Junlan Feng. 用于序列标注的神经CRF变换器。载于《ICASSP 2019-2019
    IEEE国际声学、语音与信号处理会议（ICASSP）论文集》，页码2997–3001。IEEE，2019年。'
- en: '[35] Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for sequence
    tagging. Computer Science, 2015.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Zhiheng Huang, Wei Xu, 和 Kai Yu. 双向LSTM-CRF模型用于序列标注。计算机科学，2015年。'
- en: '[36] Hideki Isozaki and Hideto Kazawa. Efficient support vector classifiers
    for named entity recognition. In Proceedings of the 19th international conference
    on Computational linguistics-Volume 1, pages 1–7\. Association for Computational
    Linguistics, 2002.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Hideki Isozaki 和 Hideto Kazawa. 用于命名实体识别的高效支持向量分类器。载于《第19届国际计算语言学会议论文集第1卷》，页码1–7。计算语言学协会，2002年。'
- en: '[37] Abhyuday N Jagannatha and Hong Yu. Structured prediction models for rnn
    based sequence labeling in clinical text. In Proceedings of the conference on
    empirical methods in natural language processing. conference on empirical methods
    in natural language processing, volume 2016, page 856\. NIH Public Access, 2016.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Abhyuday N Jagannatha 和 Hong Yu. 针对临床文本的RNN基础序列标注的结构化预测模型。载于《自然语言处理实证方法会议论文集》。自然语言处理实证方法会议，卷2016，页码856。NIH公共访问，2016年。'
- en: '[38] Yufan Jiang, Chi Hu, Tong Xiao, Chunliang Zhang, and Jingbo Zhu. Improved
    differentiable architecture search for language modeling and named entity recognition.
    In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP), pages 3576–3581, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Yufan Jiang, Chi Hu, Tong Xiao, Chunliang Zhang, 和 Jingbo Zhu. 改进的可微架构搜索用于语言建模和命名实体识别。载于《2019年自然语言处理实证方法会议及第9届国际联合自然语言处理会议（EMNLP-IJCNLP）论文集》，页码3576–3581，2019年。'
- en: '[39] Zhengbao Jiang, Wei Xu, Jun Araki, and Graham Neubig. Generalizing natural
    language analysis through span-relation representations. ACL, 2020.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Zhengbao Jiang、Wei Xu、Jun Araki 和 Graham Neubig。通过跨度关系表示来推广自然语言分析。ACL，2020年。'
- en: '[40] Katharina Kann, Johannes Bjerva, Isabelle Augenstein, Barbara Plank, and
    Anders Søgaard. Character-level supervision for low-resource pos tagging. In Proceedings
    of the Workshop on Deep Learning Approaches for Low-Resource NLP, pages 1–11,
    2018.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Katharina Kann、Johannes Bjerva、Isabelle Augenstein、Barbara Plank 和 Anders
    Søgaard。低资源词性标注的字符级监督。发表于低资源自然语言处理深度学习方法研讨会，第1–11页，2018年。'
- en: '[41] Jagat Narain Kapur. Maximum-entropy models in science and engineering.
    John Wiley & Sons, 1989.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Jagat Narain Kapur。科学和工程中的最大熵模型。John Wiley & Sons，1989年。'
- en: '[42] Michaeel Kazi and Brian Thompson. Implicitly-defined neural networks for
    sequence labeling. In Proceedings of the 55th Annual Meeting of the Association
    for Computational Linguistics (Volume 2: Short Papers), pages 172–177, 2017.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Michaeel Kazi 和 Brian Thompson。用于序列标注的隐式定义神经网络。发表于第55届计算语言学协会年会（第2卷：短文集），第172–177页，2017年。'
- en: '[43] Apostolos Kemos, Heike Adel, and Hinrich Schütze. Neural semi-markov conditional
    random fields for robust character-based part-of-speech tagging. arXiv preprint
    arXiv:1808.04208, 2018.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Apostolos Kemos、Heike Adel 和 Hinrich Schütze。用于稳健字符级词性标注的神经半马尔可夫条件随机场。arXiv
    预印本 arXiv:1808.04208，2018年。'
- en: '[44] Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and Eric Fosler-Lussier.
    Cross-lingual transfer learning for pos tagging without cross-lingual resources.
    In Proceedings of the 2017 Conference on Empirical Methods in Natural Language
    Processing, pages 2832–2838, 2017.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Joo-Kyung Kim、Young-Bum Kim、Ruhi Sarikaya 和 Eric Fosler-Lussier。无需跨语言资源的词性标注跨语言迁移学习。发表于2017年自然语言处理实证方法会议，第2832–2838页，2017年。'
- en: '[45] Dana A Knoll and David E Keyes. Jacobian-free newton–krylov methods: a
    survey of approaches and applications. Journal of Computational Physics, 193(2):357–397,
    2004.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Dana A Knoll 和 David E Keyes。无雅可比自由牛顿–克雷洛夫方法：方法和应用的调查。《计算物理学杂志》，193(2):357–397，2004年。'
- en: '[46] Lingpeng Kong, Chris Dyer, and Noah A Smith. Segmental recurrent neural
    networks. arXiv preprint arXiv:1511.06018, 2015.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Lingpeng Kong、Chris Dyer 和 Noah A Smith。分段递归神经网络。arXiv 预印本 arXiv:1511.06018，2015年。'
- en: '[47] Vijay Krishnan and Christopher D Manning. An effective two-stage model
    for exploiting non-local dependencies in named entity recognition. In Proceedings
    of the 21st International Conference on Computational Linguistics and the 44th
    annual meeting of the Association for Computational Linguistics, pages 1121–1128\.
    Association for Computational Linguistics, 2006.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Vijay Krishnan 和 Christopher D Manning。用于利用命名实体识别中的非本地依赖的有效双阶段模型。发表于第21届国际计算语言学大会和第44届计算语言学协会年会论文集，第1121–1128页。计算语言学协会，2006年。'
- en: '[48] Taku Kudoh and Yuji Matsumoto. Use of support vector learning for chunk
    identification. In Fourth Conference on Computational Natural Language Learning
    and the Second Learning Language in Logic Workshop, 2000.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Taku Kudoh 和 Yuji Matsumoto。使用支持向量学习进行块识别。发表于第四届计算自然语言学习会议和第二届逻辑语言学习研讨会，2000年。'
- en: '[49] Dinesh Kumar and Gurpreet Singh Josan. Part of speech taggers for morphologically
    rich indian languages: a survey. International Journal of Computer Applications,
    6(5):32–41, 2010.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Dinesh Kumar 和 Gurpreet Singh Josan。形态丰富的印度语言的词性标注器：调查。《国际计算机应用杂志》，6(5):32–41，2010年。'
- en: '[50] Onur Kuru, Ozan Arkan Can, and Deniz Yuret. Charner: Character-level named
    entity recognition. In Proceedings of COLING 2016, the 26th International Conference
    on Computational Linguistics: Technical Papers, pages 911–921, 2016.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Onur Kuru、Ozan Arkan Can 和 Deniz Yuret。Charner：字符级命名实体识别。发表于COLING 2016，第26届国际计算语言学大会技术论文集，第911–921页，2016年。'
- en: '[51] John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random
    fields: Probabilistic models for segmenting and labeling sequence data. In ICML,
    2001.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] John Lafferty、Andrew McCallum 和 Fernando CN Pereira。条件随机场：用于分割和标注序列数据的概率模型。发表于ICML，2001年。'
- en: '[52] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami,
    and Chris Dyer. Neural architectures for named entity recognition. In NAACL, 2016.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Guillaume Lample、Miguel Ballesteros、Sandeep Subramanian、Kazuya Kawakami
    和 Chris Dyer。用于命名实体识别的神经架构。发表于 NAACL，2016年。'
- en: '[53] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.
    arXiv preprint arXiv:1607.06450, 2016.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Jimmy Lei Ba、Jamie Ryan Kiros 和 Geoffrey E Hinton。层归一化。arXiv 预印本 arXiv:1607.06450，2016年。'
- en: '[54] Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. A survey on deep learning
    for named entity recognition. arXiv preprint arXiv:1812.09449, 2018.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Jing Li, Aixin Sun, Jianglei Han 和 Chenliang Li。深度学习在命名实体识别中的综述。arXiv
    预印本 arXiv:1812.09449，2018年。'
- en: '[55] Jing Li, Aixin Sun, and Shafiq Joty. Segbot: A generic neural text segmentation
    model with pointer network. In IJCAI, pages 4166–4172, 2018.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Jing Li, Aixin Sun 和 Shafiq Joty。Segbot：一种通用的神经文本分割模型，采用指针网络。在 IJCAI 上，页码
    4166–4172，2018年。'
- en: '[56] Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, and Wei
    Xu. Dataset and neural recurrent sequence labeling model for open-domain factoid
    question answering. arXiv preprint arXiv:1607.06275, 2016.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou 和 Wei Xu。开放域事实性问答的数据集和神经递归序列标注模型。arXiv
    预印本 arXiv:1607.06275，2016年。'
- en: '[57] Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, and Jiwei
    Li. A unified mrc framework for named entity recognition. ACL, 2020.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu 和 Jiwei Li。一个统一的
    MRC 框架用于命名实体识别。ACL，2020。'
- en: '[58] Yangming Li, Han Li, Kaisheng Yao, and Xiaolong Li. Handling rare entities
    for neural sequence labeling. In Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics, pages 6441–6451, 2020.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Yangming Li, Han Li, Kaisheng Yao 和 Xiaolong Li。处理神经序列标注中的稀有实体。在第58届计算语言学协会年会上，页码
    6441–6451，2020年。'
- en: '[59] Yaoyong Li, Kalina Bontcheva, and Hamish Cunningham. Svm based learning
    system for information extraction. In International Workshop on Deterministic
    and Statistical Methods in Machine Learning, pages 319–339\. Springer, 2004.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Yaoyong Li, Kalina Bontcheva 和 Hamish Cunningham。基于 SVM 的信息抽取学习系统。在机器学习中的确定性和统计方法国际研讨会上，页码
    319–339。Springer，2004年。'
- en: '[60] Zuchao Li, Jiaxun Cai, Shexia He, and Hai Zhao. Seq2seq dependency parsing.
    In Proceedings of the 27th International Conference on Computational Linguistics,
    pages 3203–3214, 2018.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Zuchao Li, Jiaxun Cai, Shexia He 和 Hai Zhao。Seq2seq 依赖解析。在第27届计算语言学国际会议上，页码
    3203–3214，2018年。'
- en: '[61] Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun, Bin Dong, and Shanshan Jiang.
    Gazetteer-enhanced attentive neural networks for named entity recognition. In
    Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
    and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
    pages 6233–6238, 2019.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun, Bin Dong 和 Shanshan Jiang。基于地名库增强的注意力神经网络用于命名实体识别。在2019年自然语言处理实证方法会议和第9届国际自然语言处理联合会议（EMNLP-IJCNLP）上，页码
    6233–6238，2019年。'
- en: '[62] Wang Ling, Tiago Luís, Luís Marujo, Ramón Fernandez Astudillo, Silvio
    Amir, Chris Dyer, Alan W Black, and Isabel Trancoso. Finding function in form:
    Compositional character models for open vocabulary word representation. arXiv
    preprint arXiv:1508.02096, 2015.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Wang Ling, Tiago Luís, Luís Marujo, Ramón Fernandez Astudillo, Silvio
    Amir, Chris Dyer, Alan W Black 和 Isabel Trancoso。形式中的功能：用于开放词汇词表示的组合字符模型。arXiv
    预印本 arXiv:1508.02096，2015年。'
- en: '[63] Wang Ling, Yulia Tsvetkov, Silvio Amir, Ramon Fermandez, Chris Dyer, Alan W
    Black, Isabel Trancoso, and Chu-Cheng Lin. Not all contexts are created equal:
    Better word representations with variable attention. In Proceedings of the 2015
    Conference on Empirical Methods in Natural Language Processing, pages 1367–1372,
    2015.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Wang Ling, Yulia Tsvetkov, Silvio Amir, Ramon Fermandez, Chris Dyer, Alan
    W Black, Isabel Trancoso 和 Chu-Cheng Lin。并非所有上下文都一样：通过可变注意力实现更好的词表示。在2015年自然语言处理实证方法会议上，页码
    1367–1372，2015年。'
- en: '[64] Liyuan Liu, Xiang Ren, Qi Zhu, Shi Zhi, Huan Gui, Heng Ji, and Jiawei
    Han. Heterogeneous supervision for relation extraction: A representation learning
    approach. In EMNLP, 2017.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Liyuan Liu, Xiang Ren, Qi Zhu, Shi Zhi, Huan Gui, Heng Ji 和 Jiawei Han。用于关系提取的异构监督：一种表征学习方法。在
    EMNLP，2017年。'
- en: '[65] Liyuan Liu, Jingbo Shang, Xiang Ren, Frank Fangzheng Xu, Huan Gui, Jian
    Peng, and Jiawei Han. Empower sequence labeling with task-aware neural language
    model. In AAAI, 2018.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Liyuan Liu, Jingbo Shang, Xiang Ren, Frank Fangzheng Xu, Huan Gui, Jian
    Peng 和 Jiawei Han。通过任务感知神经语言模型增强序列标注。在 AAAI，2018年。'
- en: '[66] Tianyu Liu, Jin-Ge Yao, and Chin-Yew Lin. Towards improving neural named
    entity recognition with gazetteers. In Proceedings of the 57th Annual Meeting
    of the Association for Computational Linguistics, pages 5301–5307, 2019.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Tianyu Liu, Jin-Ge Yao 和 Chin-Yew Lin。利用地名库提高神经命名实体识别。在第57届计算语言学协会年会上，页码
    5301–5307，2019年。'
- en: '[67] Yijin Liu, Fandong Meng, Jinchao Zhang, Jinan Xu, Yufeng Chen, and Jie
    Zhou. Gcdt: A global context enhanced deep transition architecture for sequence
    labeling. ACL, 2019.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Yijin Liu, Fandong Meng, Jinchao Zhang, Jinan Xu, Yufeng Chen 和 Jie Zhou.
    GCDT: 一种全球上下文增强的深度过渡架构用于序列标注。ACL, 2019年。'
- en: '[68] Ying Luo, Fengshun Xiao, and Hai Zhao. Hierarchical contextualized representation
    for named entity recognition. In AAAI, pages 8441–8448, 2020.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Ying Luo, Fengshun Xiao 和 Hai Zhao. 用于命名实体识别的层次化上下文化表示。发表于 AAAI, 页码 8441–8448,
    2020年。'
- en: '[69] Mingbo Ma, Kai Zhao, Liang Huang, Bing Xiang, and Bowen Zhou. Jointly
    trained sequential labeling and classification by sparse attention neural networks.
    arXiv preprint arXiv:1709.10191, 2017.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Mingbo Ma, Kai Zhao, Liang Huang, Bing Xiang 和 Bowen Zhou. 通过稀疏注意力神经网络联合训练的序列标注和分类。arXiv
    预印本 arXiv:1709.10191, 2017年。'
- en: '[70] Shuming Ma and Xu Sun. A new recurrent neural crf for learning non-linear
    edge features. arXiv preprint arXiv:1611.04233, 2016.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Shuming Ma 和 Xu Sun. 一种用于学习非线性边缘特征的新型递归神经 CRF。arXiv 预印本 arXiv:1611.04233,
    2016年。'
- en: '[71] Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional
    lstm-cnns-crf. In ACL, 2016.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Xuezhe Ma 和 Eduard Hovy. 通过双向 LSTM-CNNs-CRF 的端到端序列标注。发表于 ACL, 2016年。'
- en: '[72] Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building
    a large annotated corpus of english: The penn treebank. 1993.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Mitchell Marcus, Beatrice Santorini 和 Mary Ann Marcinkiewicz. 构建大规模标注语料库：宾州树库。1993年。'
- en: '[73] Andrew McCallum, Dayne Freitag, and Fernando CN Pereira. Maximum entropy
    markov models for information extraction and segmentation. In Icml, volume 17,
    pages 591–598, 2000.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Andrew McCallum, Dayne Freitag 和 Fernando CN Pereira. 用于信息提取和分段的最大熵马尔可夫模型。发表于
    ICML, 卷 17, 页码 591–598, 2000年。'
- en: '[74] Andrew McCallum and Wei Li. Early results for named entity recognition
    with conditional random fields, feature induction and web-enhanced lexicons. In
    Proceedings of the seventh conference on Natural language learning at HLT-NAACL
    2003-Volume 4, pages 188–191\. Association for Computational Linguistics, 2003.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Andrew McCallum 和 Wei Li. 条件随机场、特征诱导和网络增强词典在命名实体识别中的早期结果。发表于第七届自然语言学习会议
    HLT-NAACL 2003-卷4, 页码 188–191. 计算语言学协会, 2003年。'
- en: '[75] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation
    of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Tomas Mikolov, Kai Chen, Greg Corrado 和 Jeffrey Dean. 高效估计词向量表示。arXiv
    预印本 arXiv:1301.3781, 2013年。'
- en: '[76] David Nadeau and Satoshi Sekine. A survey of named entity recognition
    and classification. Lingvisticae Investigationes, 30(1):3–26, 2007.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] David Nadeau 和 Satoshi Sekine. 命名实体识别与分类的综述。语言学研究, 30(1):3–26, 2007年。'
- en: '[77] Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent
    neural network based sequence model for extractive summarization of documents.
    In Thirty-First AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Ramesh Nallapati, Feifei Zhai 和 Bowen Zhou. Summarunner: 基于递归神经网络的文档提取摘要序列模型。发表于第三十一届
    AAAI 人工智能会议, 2017年。'
- en: '[78] V. Ng. Supervised noun phrase coreference research: The first fifteen
    years. In ACL, 2010.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] V. Ng. 监督名词短语共指研究：前十五年。发表于 ACL, 2010年。'
- en: '[79] Nam Nguyen and Yunsong Guo. Comparisons of sequence labeling algorithms
    and extensions. In Proceedings of the 24th international conference on Machine
    learning, pages 681–688\. ACM, 2007.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Nam Nguyen 和 Yunsong Guo. 序列标注算法及其扩展的比较。发表于第24届国际机器学习会议, 页码 681–688. ACM,
    2007年。'
- en: '[80] Joakim Nivre, Željko Agić, Maria Jesus Aranzabe, Masayuki Asahara, Aitziber
    Atutxa, Miguel Ballesteros, John Bauer, Kepa Bengoetxea, Riyaz Ahmad Bhat, Cristina
    Bosco, et al. Universal dependencies 1.2. 2015.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Joakim Nivre, Željko Agić, Maria Jesus Aranzabe, Masayuki Asahara, Aitziber
    Atutxa, Miguel Ballesteros, John Bauer, Kepa Bengoetxea, Riyaz Ahmad Bhat, Cristina
    Bosco 等. Universal Dependencies 1.2. 2015年。'
- en: '[81] Joakim Nivre and Mario Scholz. Deterministic dependency parsing of english
    text. In COLING, page 64\. Association for Computational Linguistics, 2004.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Joakim Nivre 和 Mario Scholz. 英文文本的确定性依赖句法分析。发表于 COLING, 页码 64. 计算语言学协会,
    2004年。'
- en: '[82] Jaehui Park. Selectively connected self-attentions for semantic role labeling.
    Applied Sciences, 9(8):1716, 2019.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Jaehui Park. 用于语义角色标注的选择性连接自注意力机制。应用科学, 9(8):1716, 2019年。'
- en: '[83] Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How
    to construct deep recurrent neural networks. ICLR, 2014.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho 和 Yoshua Bengio. 如何构建深度递归神经网络。ICLR,
    2014年。'
- en: '[84] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global
    vectors for word representation. In Proceedings of the 2014 conference on empirical
    methods in natural language processing (EMNLP), pages 1532–1543, 2014.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Jeffrey Pennington、Richard Socher 和 Christopher Manning。GloVe：用于词表示的全局向量。发表于2014年自然语言处理经验方法会议（EMNLP）论文集中，页码1532–1543，2014年。'
- en: '[85] Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power.
    Semi-supervised sequence tagging with bidirectional language models. In ACL, 2017.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Matthew E Peters、Waleed Ammar、Chandra Bhagavatula 和 Russell Power。使用双向语言模型的半监督序列标注。发表于ACL，2017年。'
- en: '[86] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher
    Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations.
    NAACL, 2018.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Matthew E. Peters、Mark Neumann、Mohit Iyyer、Matt Gardner、Christopher Clark、Kenton
    Lee 和 Luke Zettlemoyer。深度上下文化词表示。NAACL，2018年。'
- en: '[87] Slav Petrov, Dipanjan Das, and Ryan McDonald. A universal part-of-speech
    tagset. arXiv preprint arXiv:1104.2086, 2011.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Slav Petrov、Dipanjan Das 和 Ryan McDonald。一个通用的词性标注集。arXiv 预印本 arXiv:1104.2086，2011年。'
- en: '[88] Barbara Plank, Anders Søgaard, and Yoav Goldberg. Multilingual part-of-speech
    tagging with bidirectional long short-term memory models and auxiliary loss. ACL,
    2016.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Barbara Plank、Anders Søgaard 和 Yoav Goldberg。使用双向长短期记忆模型和辅助损失的多语言词性标注。ACL，2016年。'
- en: '[89] Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders
    Björkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. Towards robust linguistic
    analysis using ontonotes. In Proceedings of the Seventeenth Conference on Computational
    Natural Language Learning, pages 143–152, 2013.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Sameer Pradhan、Alessandro Moschitti、Nianwen Xue、Hwee Tou Ng、Anders Björkelund、Olga
    Uryupina、Yuchen Zhang 和 Zhi Zhong。利用 OntoNotes 实现稳健的语言分析。发表于第十七届计算自然语言学习会议论文集中，页码143–152，2013年。'
- en: '[90] Adwait Ratnaparkhi. A maximum entropy model for part-of-speech tagging.
    In Conference on Empirical Methods in Natural Language Processing, 1996.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Adwait Ratnaparkhi。用于词性标注的最大熵模型。发表于自然语言处理经验方法会议，1996年。'
- en: '[91] Marek Rei. Semi-supervised multitask learning for sequence labeling. In
    ACL, 2017.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Marek Rei。用于序列标注的半监督多任务学习。发表于ACL，2017年。'
- en: '[92] Marek Rei, Gamal KO Crichton, and Sampo Pyysalo. Attending to characters
    in neural sequence labeling models. COLING, 2016.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Marek Rei、Gamal KO Crichton 和 Sampo Pyysalo。在神经序列标注模型中关注字符。COLING，2016年。'
- en: '[93] Marek Rei and Helen Yannakoudakis. Compositional sequence labeling models
    for error detection in learner writing. arXiv preprint arXiv:1607.06153, 2016.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Marek Rei 和 Helen Yannakoudakis。用于学习者写作中的错误检测的组合序列标注模型。arXiv 预印本 arXiv:1607.06153，2016年。'
- en: '[94] Shruti Rijhwani, Shuyan Zhou, Graham Neubig, and Jaime Carbonell. Soft
    gazetteers for low-resource named entity recognition. ACL, 2020.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Shruti Rijhwani、Shuyan Zhou、Graham Neubig 和 Jaime Carbonell。用于低资源命名实体识别的软地名库。ACL，2020年。'
- en: '[95] Alan Ritter, Sam Clark, Oren Etzioni, et al. Named entity recognition
    in tweets: an experimental study. In Proceedings of the conference on empirical
    methods in natural language processing, pages 1524–1534\. Association for Computational
    Linguistics, 2011.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Alan Ritter、Sam Clark、Oren Etzioni 等人。推文中的命名实体识别：一项实验研究。发表于自然语言处理经验方法会议论文集中，页码1524–1534。计算语言学协会，2011年。'
- en: '[96] Erik F Sang and Sabine Buchholz. Introduction to the conll-2000 shared
    task: Chunking. arXiv preprint cs/0009008, 2000.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Erik F Sang 和 Sabine Buchholz。CONLL-2000共享任务介绍：块标注。arXiv 预印本 cs/0009008，2000年。'
- en: '[97] Erik F Sang and Fien De Meulder. Introduction to the conll-2003 shared
    task: Language-independent named entity recognition. arXiv preprint cs/0306050,
    2003.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Erik F Sang 和 Fien De Meulder。CONLL-2003共享任务介绍：语言无关的命名实体识别。arXiv 预印本 cs/0306050，2003年。'
- en: '[98] Erik F. Tjong Kim Sang. Introduction to the conll-2002 shared task: Language-independent
    named entity recognition. Computer Science, pages 142–147, 2002.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Erik F. Tjong Kim Sang。CONLL-2002共享任务介绍：语言无关的命名实体识别。计算机科学，页码142–147，2002年。'
- en: '[99] Cicero Nogueira Dos Santos and Bianca Zadrozny. Learning character-level
    representations for part-of-speech tagging. In ICML, 2014.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Cicero Nogueira Dos Santos 和 Bianca Zadrozny。为词性标注学习字符级表示。发表于ICML，2014年。'
- en: '[100] Sunita Sarawagi and William W Cohen. Semi-markov conditional random fields
    for information extraction. In Advances in neural information processing systems,
    pages 1185–1192, 2005.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Sunita Sarawagi 和 William W Cohen。用于信息提取的半马尔可夫条件随机场。发表于神经信息处理系统进展，页码1185–1192，2005年。'
- en: '[101] Motoki Sato, Hiroyuki Shindo, Ikuya Yamada, and Yuji Matsumoto. Segment-level
    neural conditional random fields for named entity recognition. In Proceedings
    of the Eighth International Joint Conference on Natural Language Processing (Volume
    2: Short Papers), pages 97–102, 2017.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Motoki Sato、Hiroyuki Shindo、Ikuya Yamada 和 Yuji Matsumoto。用于命名实体识别的段级神经条件随机场。在第八届国际自然语言处理联合会议（卷
    2：短篇论文）上，第 97–102 页，2017。'
- en: '[102] Rohit Saxena, Savita Bhat, and Niranjan Pedanekar. Emotionx-area66: Predicting
    emotions in dialogues using hierarchical attention network with sequence labeling.
    In Proceedings of the Sixth International Workshop on Natural Language Processing
    for Social Media, pages 50–55, 2018.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Rohit Saxena、Savita Bhat 和 Niranjan Pedanekar。Emotionx-area66：使用层次注意力网络进行对话情感预测的序列标注。在第六届国际社交媒体自然语言处理研讨会的论文集中，第
    50–55 页，2018。'
- en: '[103] Yanyao Shen, Hyokun Yun, Zachary C Lipton, Yakov Kronrod, and Animashree
    Anandkumar. Deep active learning for named entity recognition. ICLR, 2018.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Yanyao Shen、Hyokun Yun、Zachary C Lipton、Yakov Kronrod 和 Animashree Anandkumar。用于命名实体识别的深度主动学习。ICLR，2018。'
- en: '[104] Emma Strubell, Patrick Verga, David Belanger, and Andrew McCallum. Fast
    and accurate entity recognition with iterated dilated convolutions. EMNLP, 2017.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Emma Strubell、Patrick Verga、David Belanger 和 Andrew McCallum。利用迭代膨胀卷积实现快速准确的实体识别。EMNLP，2017。'
- en: '[105] Michalina Strzyz, David Vilares, and Carlos Gómez-Rodríguez. Viable dependency
    parsing as sequence labeling. NAACL, 2019.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Michalina Strzyz、David Vilares 和 Carlos Gómez-Rodríguez。作为序列标注的可行依存解析。NAACL，2019。'
- en: '[106] Charles Sutton, Andrew McCallum, et al. An introduction to conditional
    random fields. Foundations and Trends® in Machine Learning, 4(4):267–373, 2012.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Charles Sutton、Andrew McCallum 等。条件随机场简介。机器学习基础与趋势®，4(4):267–373，2012。'
- en: '[107] Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen, and Xiaodong Shi. Deep
    semantic role labeling with self-attention. In Thirty-Second AAAI Conference on
    Artificial Intelligence, 2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Zhixing Tan、Mingxuan Wang、Jun Xie、Yidong Chen 和 Xiaodong Shi。使用自注意力的深度语义角色标注。在第三十二届
    AAAI 人工智能会议上，2018。'
- en: '[108] Dung Thai, Sree Harsha Ramesh, Shikhar Murty, Luke Vilnis, and Andrew
    McCallum. Embedded-state latent conditional random fields for sequence labeling.
    arXiv preprint arXiv:1809.10835, 2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Dung Thai、Sree Harsha Ramesh、Shikhar Murty、Luke Vilnis 和 Andrew McCallum。用于序列标注的嵌入状态潜在条件随机场。arXiv
    预印本 arXiv:1809.10835，2018。'
- en: '[109] Quan Tran, Andrew MacKinlay, and Antonio Jimeno Yepes. Named entity recognition
    with stack residual lstm and trainable bias decoding. arXiv preprint arXiv:1706.07598,
    2017.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Quan Tran、Andrew MacKinlay 和 Antonio Jimeno Yepes。使用堆叠残差 LSTM 和可训练偏差解码的命名实体识别。arXiv
    预印本 arXiv:1706.07598，2017。'
- en: '[110] Ashish Vaswani, Yonatan Bisk, Kenji Sagae, and Ryan Musa. Supertagging
    with lstms. In Proceedings of the 2016 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    pages 232–237, 2016.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Ashish Vaswani、Yonatan Bisk、Kenji Sagae 和 Ryan Musa。使用 LSTM 进行超标注。在第
    2016 年北美计算语言学协会：人类语言技术会议的论文集中，第 232–237 页，2016。'
- en: '[111] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    In NIPS, pages 5998–6008, 2017.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan
    N Gomez、Łukasz Kaiser 和 Illia Polosukhin。注意力即全部。NIPS， 第 5998–6008 页，2017。'
- en: '[112] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks.
    In Advances in Neural Information Processing Systems, pages 2692–2700, 2015.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Oriol Vinyals、Meire Fortunato 和 Navdeep Jaitly。指针网络。在神经信息处理系统进展中，第 2692–2700
    页，2015。'
- en: '[113] Chunqi Wang, Wei Chen, and Bo Xu. Named entity recognition with gated
    convolutional neural networks. In Chinese Computational Linguistics and Natural
    Language Processing Based on Naturally Annotated Big Data, pages 110–121\. Springer,
    2017.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Chunqi Wang、Wei Chen 和 Bo Xu。使用门控卷积神经网络的命名实体识别。在《基于自然注释大数据的中文计算语言学与自然语言处理》中，第
    110–121 页。Springer，2017。'
- en: '[114] Wei Wei, Zanbo Wang, Xianling Mao, Guangyou Zhou, Pan Zhou, and Sheng
    Jiang. Position-aware self-attention based neural sequence labeling. Pattern Recognition,
    page 107636, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Wei Wei、Zanbo Wang、Xianling Mao、Guangyou Zhou、Pan Zhou 和 Sheng Jiang。基于位置感知自注意力的神经序列标注。模式识别，第
    107636 页，2020。'
- en: '[115] Minghao Wu, Fei Liu, and Trevor Cohn. Evaluating the utility of hand-crafted
    features in sequence labelling. arXiv preprint arXiv:1808.09075, 2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Minghao Wu、Fei Liu 和 Trevor Cohn。评估手工特征在序列标注中的效用。arXiv 预印本 arXiv:1808.09075，2018。'
- en: '[116] Yingwei Xin, Ethan Hart, Vibhuti Mahajan, and Jean-David Ruvini. Learning
    better internal structure of words for sequence labeling. EMNLP, 2018.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Yingwei Xin、Ethan Hart、Vibhuti Mahajan 和 Jean-David Ruvini。学习更好的词内部结构用于序列标注。EMNLP，2018年。'
- en: '[117] Vikas Yadav and Steven Bethard. A survey on recent advances in named
    entity recognition from deep learning models. In Proceedings of the 27th International
    Conference on Computational Linguistics, pages 2145–2158, 2018.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Vikas Yadav 和 Steven Bethard。深度学习模型在命名实体识别中的近期进展调查。载于第27届国际计算语言学大会论文集，第2145–2158页，2018年。'
- en: '[118] Hang Yan, Bocao Deng, Xiaonan Li, and Xipeng Qiu. Tener: Adapting transformer
    encoder for name entity recognition. arXiv preprint arXiv:1911.04474, 2019.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Hang Yan、Bocao Deng、Xiaonan Li 和 Xipeng Qiu。Tener：为命名实体识别调整 transformer
    编码器。arXiv 预印本 arXiv:1911.04474，2019年。'
- en: '[119] Zhilin Yang, Ruslan Salakhutdinov, and William Cohen. Multi-task cross-lingual
    sequence tagging from scratch. arXiv preprint arXiv:1603.06270, 2016.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Zhilin Yang、Ruslan Salakhutdinov 和 William Cohen。从头开始的多任务跨语言序列标注。arXiv
    预印本 arXiv:1603.06270，2016年。'
- en: '[120] Zhilin Yang, Ruslan Salakhutdinov, and William W Cohen. Transfer learning
    for sequence tagging with hierarchical recurrent networks. In ICLR, 2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Zhilin Yang、Ruslan Salakhutdinov 和 William W Cohen。使用层次递归网络的序列标注迁移学习。载于
    ICLR，2017年。'
- en: '[121] Michihiro Yasunaga, Jungo Kasai, and Dragomir Radev. Robust multilingual
    part-of-speech tagging via adversarial training. In NAACL, 2018.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Michihiro Yasunaga、Jungo Kasai 和 Dragomir Radev。通过对抗训练实现鲁棒的多语言词性标注。载于
    NAACL，2018年。'
- en: '[122] Zhi-Xiu Ye and Zhen-Hua Ling. Hybrid semi-markov crf for neural sequence
    labeling. ACL, 2018.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Zhi-Xiu Ye 和 Zhen-Hua Ling。用于神经序列标注的混合半马尔可夫 CRF。ACL，2018年。'
- en: '[123] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated
    convolutions. arXiv preprint arXiv:1511.07122, 2015.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Fisher Yu 和 Vladlen Koltun。通过膨胀卷积进行多尺度上下文聚合。arXiv 预印本 arXiv:1511.07122，2015年。'
- en: '[124] Feifei Zhai, Saloni Potdar, Bing Xiang, and Bowen Zhou. Neural models
    for sequence chunking. In Thirty-First AAAI Conference on Artificial Intelligence,
    2017.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Feifei Zhai、Saloni Potdar、Bing Xiang 和 Bowen Zhou。用于序列块划分的神经模型。载于第31届
    AAAI 人工智能会议，2017年。'
- en: '[125] Boliang Zhang, Di Lu, Xiaoman Pan, Ying Lin, Halidanmu Abudukelimu, Heng
    Ji, and Kevin Knight. Embracing non-traditional linguistic resources for low-resource
    language name tagging. In Proceedings of the Eighth International Joint Conference
    on Natural Language Processing (Volume 1: Long Papers), pages 362–372, 2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Boliang Zhang、Di Lu、Xiaoman Pan、Ying Lin、Halidanmu Abudukelimu、Heng Ji
    和 Kevin Knight。拥抱非传统语言资源进行低资源语言名称标注。载于第八届国际联合自然语言处理会议（卷1：长篇论文），第362–372页，2017年。'
- en: '[126] Yi Zhang, Xu Sun, Shuming Ma, Yang Yang, and Xuancheng Ren. Does higher
    order lstm have better accuracy for segmenting and labeling sequence data? In
    COLING, 2017.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Yi Zhang、Xu Sun、Shuming Ma、Yang Yang 和 Xuancheng Ren。高阶 LSTM 是否在分段和标注序列数据方面有更好准确性？载于
    COLING，2017年。'
- en: '[127] Yuan Zhang, Hongshen Chen, Yihong Zhao, Qun Liu, and Dawei Yin. Learning
    tag dependencies for sequence tagging. In IJCAI, 2018.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Yuan Zhang、Hongshen Chen、Yihong Zhao、Qun Liu 和 Dawei Yin。序列标注中的标签依赖学习。载于
    IJCAI，2018年。'
- en: '[128] Yue Zhang, Qi Liu, and Linfeng Song. Sentence-state lstm for text representation.
    ACL, 2018.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Yue Zhang、Qi Liu 和 Linfeng Song。用于文本表示的句子状态 LSTM。ACL，2018年。'
- en: '[129] Sendong Zhao, Ting Liu, Sicheng Zhao, and Fei Wang. A neural multi-task
    learning framework to jointly model medical named entity recognition and normalization.
    CoRR, abs/1812.06081, 2019.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Sendong Zhao、Ting Liu、Sicheng Zhao 和 Fei Wang。用于医学命名实体识别和归一化的神经多任务学习框架。CoRR，abs/1812.06081，2019年。'
- en: '[130] Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu.
    Joint extraction of entities and relations based on a novel tagging scheme. arXiv
    preprint arXiv:1706.05075, 2017.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Suncong Zheng、Feng Wang、Hongyun Bao、Yuexing Hao、Peng Zhou 和 Bo Xu。基于新型标记方案的实体和关系联合提取。arXiv
    预印本 arXiv:1706.05075，2017年。'
- en: '[131] GuoDong Zhou and Jian Su. Named entity recognition using an hmm-based
    chunk tagger. In proceedings of the 40th Annual Meeting on Association for Computational
    Linguistics, pages 473–480\. Association for Computational Linguistics, 2002.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] GuoDong Zhou 和 Jian Su。基于 HMM 的块标记器进行命名实体识别。载于第40届计算语言学协会年会论文集，第473–480页。计算语言学协会，2002年。'
- en: '[132] Xiaoqiang Zhou, Baotian Hu, Qingcai Chen, Buzhou Tang, and Xiaolong Wang.
    Answer sequence learning with neural networks for answer selection in community
    question answering. arXiv preprint arXiv:1506.06490, 2015.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Xiaoqiang Zhou、Baotian Hu、Qingcai Chen、Buzhou Tang 和 Xiaolong Wang。用于社区问答中答案选择的神经网络答案序列学习。arXiv
    预印本 arXiv:1506.06490，2015年。'
- en: '[133] Yuying Zhu, Guoxin Wang, and Börje F Karlsson. Can-ner: Convolutional
    attention network for chinese named entity recognition. NAACL, 2019.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] 朱玉英、王国新和Börje F Karlsson。Can-ner：用于中文命名实体识别的卷积注意力网络。NAACL，2019年。'
- en: '[134] Jingwei Zhuo, Yong Cao, Jun Zhu, Bo Zhang, and Zaiqing Nie. Segment-level
    sequence modeling using gated recursive semi-markov conditional random fields.
    In Proceedings of the 54th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), volume 1, pages 1413–1423, 2016.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] 卓京伟、曹勇、朱俊、张博和聂在青。使用门控递归半马尔可夫条件随机场进行分段级序列建模。载于《第54届计算语言学协会年会论文集（第一卷：长篇论文）》中，第一卷，第1413–1423页，2016年。'
