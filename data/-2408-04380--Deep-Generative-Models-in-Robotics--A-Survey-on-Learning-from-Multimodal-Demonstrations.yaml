- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:30:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:30:46
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2408.04380] Deep Generative Models in Robotics: A Survey on Learning from
    Multimodal Demonstrations'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2408.04380] 机器人中的深度生成模型：关于多模态示范学习的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.04380](https://ar5iv.labs.arxiv.org/html/2408.04380)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.04380](https://ar5iv.labs.arxiv.org/html/2408.04380)
- en: 'Deep Generative Models in Robotics:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器人中的深度生成模型：
- en: A Survey on Learning from Multimodal Demonstrations
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 关于多模态示范学习的调查
- en: Julen Urain¹, Ajay Mandlekar², Yilun Du³, Nur Muhammad Mahi Shafiullah⁴, Danfei
    Xu^(52), Katerina Fragkiadaki⁶, Georgia Chalvatzaki⁷, Jan Peters^(718) ¹German
    Research Center for AI, ²Nvidia, ³Massachusetts Institute of Technology, ⁴New
    York University, ⁵Georgia Institute of Technology, ⁶Carnegie Mellon University,
    ⁷Technische Universität Darmstadt, ⁸Hessian.AI
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Julen Urain¹, Ajay Mandlekar², Yilun Du³, Nur Muhammad Mahi Shafiullah⁴, Danfei
    Xu^(52), Katerina Fragkiadaki⁶, Georgia Chalvatzaki⁷, Jan Peters^(718) ¹德国人工智能研究中心，²Nvidia，³麻省理工学院，⁴纽约大学，⁵乔治亚理工学院，⁶卡内基梅隆大学，⁷达姆施塔特工业大学，⁸Hessian.AI
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Learning from Demonstrations, the field that proposes to learn robot behavior
    models from data, is gaining popularity with the emergence of deep generative
    models. Although the problem has been studied for years under names such as Imitation
    Learning, Behavioral Cloning, or Inverse Reinforcement Learning, classical methods
    have relied on models that don’t capture complex data distributions well or don’t
    scale well to large numbers of demonstrations. In recent years, the robot learning
    community has shown increasing interest in using deep generative models to capture
    the complexity of large datasets. In this survey, we aim to provide a unified
    and comprehensive review of the last year’s progress in the use of deep generative
    models in robotics. We present the different types of models that the community
    has explored, such as energy-based models, diffusion models, action value maps,
    or generative adversarial networks. We also present the different types of applications
    in which deep generative models have been used, from grasp generation to trajectory
    generation or cost learning. One of the most important elements of generative
    models is the generalization out of distributions. In our survey, we review the
    different decisions the community has made to improve the generalization of the
    learned models. Finally, we highlight the research challenges and propose a number
    of future directions for learning deep generative models in robotics.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从示范中学习，即通过数据学习机器人行为模型的领域，随着深度生成模型的出现而日益受到关注。尽管这一问题已经被研究了多年，以模仿学习、行为克隆或逆向强化学习等名义存在，但传统方法依赖的模型无法很好地捕捉复杂的数据分布或在大量示范中表现不佳。近年来，机器人学习社区对使用深度生成模型来捕捉大数据集的复杂性表现出了越来越大的兴趣。在这项调查中，我们旨在提供对过去一年深度生成模型在机器人领域应用进展的统一和全面的回顾。我们介绍了社区探索的不同类型的模型，例如基于能量的模型、扩散模型、动作价值图或生成对抗网络。我们还展示了深度生成模型在不同应用中的使用情况，从抓取生成到轨迹生成或成本学习。生成模型的一个重要元素是超出分布的泛化能力。在我们的调查中，我们回顾了社区为提高学习模型的泛化能力所做出的不同决策。最后，我们突出了研究挑战，并提出了一些关于机器人深度生成模型学习的未来方向。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: 'robotics, generative models, decision making, control, imitation learning,
    behavioral cloning, learning from demonstrations^†^†publicationid: pubid:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '机器人技术，生成模型，决策制定，控制，模仿学习，行为克隆，从示范中学习^†^†publicationid: pubid:'
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: '[Learning from Demonstration (LfD)](#glo.main.lfd) [[1](#bib.bib1), [2](#bib.bib2)],
    also known as Imitation Learning [[3](#bib.bib3), [4](#bib.bib4)], is the field
    that proposes to learn the desired robot behavior by observing and imitating a
    set of expert demonstrations. Conditioned on observations of the scene and the
    desired task to be solved, the model, commonly known as policy, is trained to
    generate actions that emulate the behavior in the expert demonstrations. Depending
    on the task, these actions may represent desirable end-effector poses [[5](#bib.bib5),
    [6](#bib.bib6)], robot trajectories [[7](#bib.bib7), [8](#bib.bib8)] or desirable
    scene arrangements [[9](#bib.bib9), [10](#bib.bib10)], to name a few.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[从演示中学习 (LfD)](#glo.main.lfd) [[1](#bib.bib1), [2](#bib.bib2)]，也称为模仿学习 [[3](#bib.bib3),
    [4](#bib.bib4)]，是一个领域，旨在通过观察和模仿一组专家演示来学习期望的机器人行为。基于对场景和需要解决的任务的观察，模型（通常称为策略）被训练生成模拟专家演示中行为的动作。这些动作可能代表期望的末端执行器姿态
    [[5](#bib.bib5), [6](#bib.bib6)]、机器人轨迹 [[7](#bib.bib7), [8](#bib.bib8)] 或期望的场景布置
    [[9](#bib.bib9), [10](#bib.bib10)] 等。'
- en: '[LfD](#glo.main.lfd) includes several approaches to tackle this problem. [Behavioural
    Cloning (BC)](#glo.main.bc) methods [[1](#bib.bib1)] fit a conditional generative
    model to the actions conditioned on the observations. Despite its shortcomings
    in sequential decision-making problems (e.g., compounding errors leading to covariate
    shift [[11](#bib.bib11)]), in practice, it has shown some of the most impressive
    results [[6](#bib.bib6), [12](#bib.bib12), [7](#bib.bib7), [13](#bib.bib13)] in
    part due to its stable and efficient training algorithms.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[从演示中学习 (LfD)](#glo.main.lfd) 包括几种解决此问题的方法。[行为克隆 (BC)](#glo.main.bc) 方法 [[1](#bib.bib1)]
    通过基于观察的数据来拟合条件生成模型。尽管它在顺序决策问题中存在缺陷（例如，误差累积导致协变量偏移 [[11](#bib.bib11)]），但在实践中，它展示了一些最令人印象深刻的结果
    [[6](#bib.bib6), [12](#bib.bib12), [7](#bib.bib7), [13](#bib.bib13)]，部分原因是其稳定和高效的训练算法。'
- en: Alternatively, [Inverse Reinforcement Learning (IRL)](#glo.main.irl) [[14](#bib.bib14),
    [15](#bib.bib15)] or variations such as [[16](#bib.bib16), [17](#bib.bib17)] combine
    the demonstrations with trial-and-error in the environment (i.e. [Reinforcement
    Learning (RL)](#glo.main.rl)), resulting in policies that are more robust than
    [BC](#glo.main.bc), but limited by less stable training algorithms. Unlike [BC](#glo.main.bc),
    which directly mimics the actions from the demonstrations, [IRL](#glo.main.irl)
    focuses on inferring the underlying reward functions that the demonstrated behaviors
    aim to optimize, and applies [RL](#glo.main.rl) to infer the policy. A key advantage
    of [IRL](#glo.main.irl) is its ability to learn from mere observations [[18](#bib.bib18),
    [19](#bib.bib19)], without explicit information about the actions taken during
    the demonstrations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，[逆向强化学习 (IRL)](#glo.main.irl) [[14](#bib.bib14), [15](#bib.bib15)] 或诸如 [[16](#bib.bib16),
    [17](#bib.bib17)] 等变体将演示与环境中的试错结合起来（即 [强化学习 (RL)](#glo.main.rl)），生成的策略比 [行为克隆
    (BC)](#glo.main.bc) 更加稳健，但受限于不够稳定的训练算法。与 [行为克隆 (BC)](#glo.main.bc) 直接模仿演示中的动作不同，[逆向强化学习
    (IRL)](#glo.main.irl) 侧重于推断演示行为旨在优化的潜在奖励函数，并应用 [强化学习 (RL)](#glo.main.rl) 推断策略。[逆向强化学习
    (IRL)](#glo.main.irl) 的一个关键优势是能够仅从观察中学习 [[18](#bib.bib18), [19](#bib.bib19)]，而无需明确的演示过程中所采取的动作的信息。
- en: In [LfD](#glo.main.lfd), the inherent characteristics of the demonstrations
    pose significant challenges. Typically, the collected data is suboptimal, noisy,
    conditioned on high-dimensional observations, and includes multiple modes of behavior [[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)]. This diversity can be observed in the multiple
    ways to grasp a given object, the preferences of the experts in providing the
    demonstrations, or the divergences between experts. These inherent properties
    of the data lead the researchers to find models that can properly capture its
    distribution.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [从演示中学习 (LfD)](#glo.main.lfd) 中，演示的固有特性带来了重大挑战。通常，收集的数据是次优的、嘈杂的、基于高维观察，并且包含多种行为模式
    [[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)]。这种多样性可以在多种抓取给定物体的方式、专家提供演示的偏好或专家之间的差异中观察到。这些数据的固有特性导致研究人员寻找能够正确捕捉其分布的模型。
- en: Traditionally, before deep learning became standard, [LfD](#glo.main.lfd) methods
    often used [Gaussian Process (GP)](#glo.main.gp) [[23](#bib.bib23), [24](#bib.bib24)],
    [Hidden Markov Model (HMM)](#glo.main.hmm) [[25](#bib.bib25), [26](#bib.bib26)],
    or [Gaussian Mixture Models (GMM)](#glo.main.gmm) [[27](#bib.bib27)] to represent
    the generative models. However, these models were not scalable to large datasets
    and were unable to represent conditioned distributions in high-dimensional contexts
    such as images. Neural network-based models allowed for conditioning in high-dimensional
    variables such as images [[28](#bib.bib28), [29](#bib.bib29)] or text [[30](#bib.bib30),
    [31](#bib.bib31)], but they were typically trained as unimodal models. These types
    of models are at odds with the nature of the collected demonstrations. The inability
    of these models to capture the inherent diversity and multiple modes in the data
    led researchers to limit themselves to smaller [[32](#bib.bib32)] or highly curated
    datasets to ensure unimodality and thus simplify the modeling process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，在深度学习成为标准之前，[LfD](#glo.main.lfd)方法通常使用[Gaussian Process (GP)](#glo.main.gp)
    [[23](#bib.bib23), [24](#bib.bib24)], [Hidden Markov Model (HMM)](#glo.main.hmm)
    [[25](#bib.bib25), [26](#bib.bib26)], 或[Gaussian Mixture Models (GMM)](#glo.main.gmm)
    [[27](#bib.bib27)]来表示生成模型。然而，这些模型无法扩展到大数据集，并且无法在高维上下文中如图像中表示条件分布。基于神经网络的模型允许在高维变量如图像[[28](#bib.bib28),
    [29](#bib.bib29)]或文本[[30](#bib.bib30), [31](#bib.bib31)]中进行条件设置，但通常被训练为单模态模型。这些模型类型与收集到的演示的本质相悖。这些模型未能捕捉到数据的固有多样性和多个模式，使研究人员不得不限制在较小的[[32](#bib.bib32)]或高度策划的数据集上，以确保单模态性，从而简化建模过程。
- en: 'Recent successes of [Deep Generative Models (DGM)](#glo.main.dgm) in image [[33](#bib.bib33)]
    and text generation [[34](#bib.bib34)] have demonstrated their ability to capture
    highly multimodal data distributions. In recent years, these expressive models
    have garnered attention in the field of robotics for Imitation Learning applications
    (see [Figure 1](#S1.F1 "In I Introduction ‣ Deep Generative Models in Robotics:
    A Survey on Learning from Multimodal Demonstrations")). For example, [Diffusion
    Models (DM)](#glo.main.dm) [[35](#bib.bib35), [33](#bib.bib33)] have been effectively
    used to learn high-dimensional trajectory distributions [[36](#bib.bib36), [7](#bib.bib7),
    [8](#bib.bib8)]; Language and image-based policies have been developed using GPT-style
    models representing categorical distributions in the action space [[37](#bib.bib37)];
    and [Variational Autoencoders (VAE)](#glo.main.vae) [[38](#bib.bib38)] were applied
    to generate 6-DoF grasping poses for arbitrary objects [[5](#bib.bib5)].'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[深度生成模型 (DGM)](#glo.main.dgm)在图像[[33](#bib.bib33)]和文本生成[[34](#bib.bib34)]中的近期成功展示了其捕捉高度多模态数据分布的能力。近年来，这些富有表现力的模型在机器人领域的模仿学习应用中引起了关注（见[Figure 1](#S1.F1
    "在 I 介绍 ‣ 机器人中的深度生成模型：关于从多模态演示学习的调查")）。例如，[扩散模型 (DM)](#glo.main.dm) [[35](#bib.bib35),
    [33](#bib.bib33)] 已有效地用于学习高维轨迹分布 [[36](#bib.bib36), [7](#bib.bib7), [8](#bib.bib8)]；基于语言和图像的策略使用GPT风格模型开发，表示动作空间中的分类分布
    [[37](#bib.bib37)]；[变分自编码器 (VAE)](#glo.main.vae) [[38](#bib.bib38)] 被应用于生成用于任意物体的6自由度抓取姿势
    [[5](#bib.bib5)]。'
- en: This article presents a unified and comprehensive review of the various approaches
    explored by the robotics community to learn [DGM](#glo.main.dgm) from demonstrations
    to capture the inherent multimodality of the data. While some of these models
    are borrowed from other areas of machine learning, such as [DM](#glo.main.dm),
    we also highlight approaches that have been particularly influential in representing
    action distributions in robotics, such as Action Value Maps [[39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文呈现了机器人社区探索的各种方法的统一和全面的综述，这些方法通过演示学习[DGM](#glo.main.dgm)以捕捉数据的固有多模态性。虽然其中一些模型借鉴了机器学习的其他领域，例如[DM](#glo.main.dm)，但我们也强调了一些在机器人中表示动作分布特别有影响力的方法，如Action
    Value Maps [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)]。
- en: 'The survey focuses mainly on approaches that consider offline data, i.e., no
    additional data collected online or interactively, and offline supervision, i.e.,
    no additional supervision other than expert actions. Although learning [DGM](#glo.main.dgm)
    from offline datasets has been widely studied in various fields from vision to
    text generation, there are inherent challenges in robotics that require careful
    design choices. To motivate the specific design choices for robotics applications,
    in [Section I-A](#S1.SS1 "I-A Challenges in Learning from Offline Demonstrations
    ‣ I Introduction ‣ Deep Generative Models in Robotics: A Survey on Learning from
    Multimodal Demonstrations"), we present the fundamental challenges of learning
    policies from demonstrations in robotics.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该综述主要集中于考虑离线数据的方法，即不收集额外的在线或互动数据，和离线监督，即除了专家动作之外没有额外的监督。虽然从离线数据集中学习[DGM](#glo.main.dgm)已在从视觉到文本生成的各种领域广泛研究，但机器人中的固有挑战需要仔细的设计选择。为了激励机器人应用的具体设计选择，在[第
    I-A 节](#S1.SS1 "I-A 从离线演示中学习的挑战 ‣ I 引言 ‣ 机器人中的深度生成模型：多模态演示学习的综述")中，我们介绍了从机器人演示中学习策略的基本挑战。
- en: '![Refer to caption](img/8ae2332a7cefbe9f160b87105ae05fbc.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8ae2332a7cefbe9f160b87105ae05fbc.png)'
- en: 'Figure 1: Selected publications for this survey per year. Different colors
    indicate different types of [DGM](#glo.main.dgm). We categorize [DGM](#glo.main.dgm)
    into five classes.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 本综述每年的精选出版物。不同的颜色表示不同类型的[DGM](#glo.main.dgm)。我们将[DGM](#glo.main.dgm)分为五类。'
- en: 'We divide the survey into six sections (see LABEL:fig:main):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将综述分为六个部分（见 LABEL:fig:main）：
- en: 'In [Section II](#S2 "II Problem Formulation ‣ Deep Generative Models in Robotics:
    A Survey on Learning from Multimodal Demonstrations"), we formalize the problem
    and provide the nomenclature that we will use throughout the survey.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 II 节](#S2 "II 问题表述 ‣ 机器人中的深度生成模型：多模态演示学习的综述")中，我们对问题进行了形式化，并提供了在整个综述中使用的术语表。
- en: 'In [Section III](#S3 "III Density Estimation Models ‣ Deep Generative Models
    in Robotics: A Survey on Learning from Multimodal Demonstrations"), we introduce
    the most commonly used [DGM](#glo.main.dgm) in robotics, present their inherent
    properties, briefly list various works that have applied these methods to robotics,
    and present the training and sampling algorithms for each model.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 III 节](#S3 "III 密度估计模型 ‣ 机器人中的深度生成模型：多模态演示学习的综述")中，我们介绍了机器人中最常用的[DGM](#glo.main.dgm)，展示了它们的固有特性，简要列出了应用这些方法到机器人中的各种工作，并介绍了每个模型的训练和采样算法。
- en: 'In [Section IV](#S4 "IV Integrating Generative Models into Robotics ‣ Deep
    Generative Models in Robotics: A Survey on Learning from Multimodal Demonstrations"),
    we present the different types of applications in which deep generative models
    have been applied highlighting the type of data that the models generate and the
    type of conditioning variables that are considered.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 IV 节](#S4 "IV 将生成模型融入机器人技术 ‣ 机器人中的深度生成模型：多模态演示学习的综述")中，我们展示了深度生成模型应用的不同类型，突出模型生成的数据类型以及考虑的条件变量类型。
- en: 'In [Section V](#S5 "V Generalizing outside data distributions ‣ Deep Generative
    Models in Robotics: A Survey on Learning from Multimodal Demonstrations"), we
    present a range of design and algorithmic inductive biases to improve the generalization
    out of the data distribution of the learned models. How can we guarantee the generation
    of useful actions given as context observations that were not in the demonstrations?
    Among the options we present are the modular composition of generative models,
    the extraction of informative features from the observations, and the exploitation
    of symmetries between the observations and the actions.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 V 节](#S5 "V 泛化超出数据分布 ‣ 机器人中的深度生成模型：多模态演示学习的综述")中，我们展示了一系列设计和算法上的归纳偏置，以提高从学习模型的数据分布之外的泛化能力。我们如何保证在给定的上下文观察中生成有用的动作，而这些观察未出现在演示中？我们提出的选项包括生成模型的模块化组合，从观察中提取信息特征，以及利用观察和动作之间的对称性。
- en: 'Finally, in [Section VI](#S6 "VI Future Research Directions ‣ Deep Generative
    Models in Robotics: A Survey on Learning from Multimodal Demonstrations"), we
    highlight the current research challenges in the field and suggest future directions.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在[第 VI 节](#S6 "VI 未来研究方向 ‣ 机器人中的深度生成模型：多模态演示学习的综述")中，我们强调了该领域当前的研究挑战，并提出了未来的研究方向。
- en: I-A Challenges in Learning from Offline Demonstrations
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从离线演示中学习的I-A挑战
- en: Learning robot policies from offline demonstrations presents several challenges.
    While many of these challenges (e.g., multiple modes in the demonstrations) are
    shared with other research areas, such as image generation or text generation,
    there are robotics-specific challenges that we should consider. Below, we present
    the main challenges in learning robot policies from offline data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从离线演示中学习机器人策略面临着几个挑战。尽管这些挑战中的许多（例如演示中的多种模式）与其他研究领域（如图像生成或文本生成）共享，但我们还需要考虑特定于机器人技术的挑战。以下是从离线数据中学习机器人策略的主要挑战。
- en: Demonstration Diversity. One of the main challenges is the inherent variability
    within the demonstrations themselves [[42](#bib.bib42)]. Different demonstrators
    may have different skill levels, preferences, and strategies for accomplishing
    the same task, resulting in a wide range of approaches encapsulated in the dataset.
    Unimodal distributions lack the expressiveness to capture this variability in
    the demonstrations, resulting in poor performance. [DGM](#glo.main.dgm) are a
    promising approach to address this challenge. Being able to capture complex multimodal
    distributions, these models can learn to represent the different strategies and
    behaviors exhibited in the demonstrations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 演示多样性。主要挑战之一是演示本身的固有变异性[[42](#bib.bib42)]。不同的演示者可能具有不同的技能水平、偏好和完成同一任务的策略，导致数据集中封装了各种方法。单模态分布缺乏捕捉这些演示变异性的表达能力，导致表现不佳。[DGM](#glo.main.dgm)是应对这一挑战的有前景的方法。这些模型能够捕捉复杂的多模态分布，从而学习表示演示中表现出的不同策略和行为。
- en: Heterogeneous Action and State Spaces. Unlike computer vision, where the data
    space is well defined, in robotics, there is no a single state-action space. Robot
    actions can range from torque commands, to desired target positions or desired
    trajectories. In addition, robot behavior can be modeled in both the robot’s configuration
    space and the task space. This variability leads to heterogeneous datasets and
    heterogeneous solutions for learning robot policies.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 异质的动作和状态空间。与计算机视觉不同，在计算机视觉中数据空间定义明确，在机器人技术中不存在单一的状态-动作空间。机器人动作可以包括从扭矩命令到期望目标位置或期望轨迹等各种形式。此外，机器人行为可以在机器人配置空间和任务空间中建模。这种变异性导致了异质的数据集和学习机器人策略的异质解决方案。
- en: Partially Observable Demonstrations. When a human performs a demonstration,
    his actions are not based solely on observable elements; they are driven by internal
    states influenced by the demonstrator’s knowledge of the task and a history of
    observations. In addition, humans can incorporate information from the environment
    that may not be readily available or observable by a robot’s sensors, such as
    peripheral details captured by human vision but missed by the robot’s cameras.
    This mismatch often results in demonstrations that only partially represent the
    context of the task, leading to ambiguities in the policies learned by the robot.
    The issue of partial observability has been studied extensively in the literature [[43](#bib.bib43)].
    A common practical approach is to encode the history of observations as contexts
    rather than a single observation, allowing the model to extract internal states
    that could reduce the ambiguity [[44](#bib.bib44)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 部分可观察的演示。当人类进行演示时，他的行为不仅仅基于可观察的元素；它们还受到演示者对任务的知识和观察历史影响的内部状态的驱动。此外，人类可以融入环境中的信息，这些信息可能不是机器人传感器容易获取或观察到的，例如人类视觉捕捉到的边缘细节，但机器人摄像头却遗漏了。这种不匹配通常导致演示仅部分代表任务的背景，从而导致机器人学习到的策略中的模糊性。部分可观察性的问题在文献中已经被广泛研究[[43](#bib.bib43)]。一种常见的实际方法是将观察历史编码为上下文，而不是单一观察，使模型能够提取内部状态，从而减少模糊性[[44](#bib.bib44)]。
- en: Temporal Dependencies and Long-Horizon Planning. Robotic tasks often involve
    sequential decision-making, where actions are interrelated over time. This sequential
    nature can result in compounding errors that lead the robot into situations not
    encountered in the training demonstrations. This problem has been addressed in
    several ways. Some works propose learning short-horizon skills that can then be
    concatenated with a high-level planner. In another direction, a number of works [[36](#bib.bib36),
    [13](#bib.bib13)] propose learning policies that generate trajectories of actions
    rather than single-step actions, thus reducing the sequentially compounded errors.
    In addition, other options are to inject noise while generating the demonstrations [[45](#bib.bib45)]
    or to interactively grow the dataset [[11](#bib.bib11)].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 时间依赖性和长期规划。机器人任务通常涉及顺序决策，其中行动在时间上是相互关联的。这种顺序特性可能导致累积错误，使机器人进入训练演示中未遇到的情况。这个问题已经通过几种方式得到解决。一些工作提出了学习短期技能，然后将其与高级规划器连接。另一些工作[[36](#bib.bib36),
    [13](#bib.bib13)]则提出学习生成行动轨迹的策略，而不是单步行动，从而减少顺序累积的错误。此外，其他选项包括在生成演示时注入噪声[[45](#bib.bib45)]或交互式地扩展数据集[[11](#bib.bib11)]。
- en: Mismatch between training and evaluation objectives. Learning from offline demonstrations
    is typically framed as a density estimation problem. The learned model is trained
    to produce samples that resemble the training dataset. However, the learned models
    are used to solve a given task, where the metric to be maximized is the task success
    rate. This mismatch between the training objective and the evaluation objective
    can lead to poor performance when the robot is used to solve a particular task.
    One possible direction to address this problem is to combine a behavioral cloning
    phase with a posterior reinforcement learning fine-tuning [[46](#bib.bib46)].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 训练目标和评估目标之间的不匹配。离线演示的学习通常被框定为密度估计问题。学习到的模型被训练以生成类似于训练数据集的样本。然而，学习到的模型被用于解决特定任务，其中需要最大化的指标是任务成功率。这种训练目标与评估目标之间的不匹配可能导致机器人在解决特定任务时表现不佳。解决此问题的一个可能方向是将行为克隆阶段与后续强化学习微调相结合[[46](#bib.bib46)]。
- en: 'Distribution Shifts and Generalization. A fundamental challenge in learning
    from offline demonstrations is the distribution shift between the demonstration
    data and the real-world scenarios in which the learned policies are deployed.
    Demonstrations are typically collected in controlled environments or specific
    contexts, but the robot must operate in potentially novel situations not covered
    by the demonstrations. This mismatch can lead to generalization failures and performance
    degradation when the learned policies are applied outside the scope of the training
    data. Addressing this challenge requires techniques that can extrapolate from
    the given demonstrations and adapt to new, unseen environments. We dedicate [Section V](#S5
    "V Generalizing outside data distributions ‣ Deep Generative Models in Robotics:
    A Survey on Learning from Multimodal Demonstrations") to explore different approaches
    to improve generalization in robotics applications.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '分布转移和泛化。从离线演示中学习的一个基本挑战是演示数据与实际应用中学习策略的分布转移。演示通常在受控环境或特定背景下收集，但机器人必须在演示未覆盖的潜在新情况中操作。这种不匹配可能导致泛化失败和性能下降。当学习策略应用于超出训练数据范围的情况时，需要采用能够从给定演示中推断并适应新环境的技术。我们将[第V节](#S5
    "V Generalizing outside data distributions ‣ Deep Generative Models in Robotics:
    A Survey on Learning from Multimodal Demonstrations")专门探讨改进机器人应用中泛化的不同方法。'
- en: '![Refer to caption](img/0e423cf566fd083e5a268c3d3cf7494b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0e423cf566fd083e5a268c3d3cf7494b.png)'
- en: 'Figure 2: Left: A visual representation of Sampling Models. Given a latent
    sample ${\bm{z}}$, usually sampled from a normal distribution, Sampling Models
    generate an action sample through a learned decoder ${\bm{a}}={\bm{D}}_{{\bm{\theta}}}({\bm{z}},{\bm{c}})$.
    Right: A representation of common applications for Sampling Models: as sampling
    distribution [[47](#bib.bib47)], as behavior prior [[48](#bib.bib48)] and, as
    generative model [[5](#bib.bib5)].'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：左侧：采样模型的视觉表示。给定一个潜在样本${\bm{z}}$，通常从正态分布中采样，采样模型通过学习到的解码器生成一个行动样本${\bm{a}}={\bm{D}}_{{\bm{\theta}}}({\bm{z}},{\bm{c}})$。右侧：采样模型的常见应用表示：作为采样分布[[47](#bib.bib47)]，作为行为先验[[48](#bib.bib48)]，以及作为生成模型[[5](#bib.bib5)]。
- en: I-B Related Surveys
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 相关调查
- en: The field of [LfD](#glo.main.lfd) has a long history that has been explored
    in several surveys.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[LfD](#glo.main.lfd)领域有着悠久的历史，已在多个调查中得到探讨。'
- en: Before deep learning-based approaches became standard, several surveys [[49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52)] explored the basic problem
    of Imitation Learning. These surveys address questions such as How should we acquire
    data?, What model should we learn?, or How should we learn a policy?.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习方法成为标准之前，几个调查[[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52)]探讨了模仿学习的基本问题。这些调查解答了诸如我们应如何获取数据？我们应学习什么模型？或者我们应如何学习策略？等问题。
- en: More recent works [[53](#bib.bib53), [3](#bib.bib3), [54](#bib.bib54)] updated
    the reviews to the new state of the art where deep learning-based models were
    beginning to be integrated into [LfD](#glo.main.lfd) problems. In particular,
    [[3](#bib.bib3)] presented an algorithmic perspective on Imitation Learning, allowing
    the comparison of different algorithms from an information-theoretic point of
    view.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 更近期的工作[[53](#bib.bib53), [3](#bib.bib3), [54](#bib.bib54)]更新了对新技术状态的评审，其中深度学习模型开始被集成到[广义的模仿学习](#glo.main.lfd)问题中。特别是，[[3](#bib.bib3)]从信息论的角度呈现了模仿学习的算法视角，允许比较不同算法。
- en: The current stage of the robot learning community, with the increasing availability
    of large-scale robot demonstrations both in simulation and in the real world,
    the growing importance of imitation-based approaches, and the increasing availability
    of cheap robot hardware, makes it timely to provide a survey covering the research
    of the last years and focusing on the challenges the field is currently facing
    (multimodality, generalization, heterogeneous datasets …).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 目前机器人学习社区的阶段，随着大规模机器人演示在模拟和现实世界中的日益普及，模仿基础方法的重要性日益增加，以及便宜的机器人硬件的日益可用，使得提供一个涵盖过去几年研究并专注于该领域当前面临的挑战（多模态性、泛化、异质数据集……）的调查变得及时。
- en: Recently, a few surveys [[55](#bib.bib55), [56](#bib.bib56)] have explored the
    problem of learning foundational models for robotics, which mainly focused on
    integrating Internet-scale vision and language foundation models into robotics
    problems. Despite the potential of applying vision-language foundational models
    to robotics problems, our survey focuses on a different problem. The interest
    in this survey is in exploring approaches for learning policies directly from
    embodied robotics data (in part, due to the growing availability of large datasets [[22](#bib.bib22),
    [57](#bib.bib57)]), rather than adapting vision-language models to robotics.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，几个调查[[55](#bib.bib55), [56](#bib.bib56)]探讨了机器人学基础模型学习的问题，主要集中在将互联网规模的视觉和语言基础模型整合到机器人学问题中。尽管将视觉-语言基础模型应用于机器人学问题具有潜力，但我们的调查关注的是另一个问题。本调查的兴趣在于探索直接从具身机器人数据中学习策略的方法（部分原因是大型数据集的日益可用[[22](#bib.bib22),
    [57](#bib.bib57)]），而不是将视觉-语言模型适配到机器人学中。
- en: II Problem Formulation
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 问题表述
- en: The primary goal of [BC](#glo.main.bc) is to learn a conditioned probability
    density model (generative model) $\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$, that
    accurately captures the underlying probability distribution of the data, denoted
    as $\rho_{\mathcal{{\mathcal{D}}}}({\bm{a}}|{\bm{c}})$, where ${\bm{a}}$ is the
    data variable we want to generate and ${\bm{c}}$ is the conditioning variable.
    The central idea is to ensure that the samples generated by the model ${\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$
    are indistinguishable from the real data samples ${\bm{a}}\sim\rho_{\mathcal{{\mathcal{D}}}}({\bm{a}}|{\bm{c}})$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[BC](#glo.main.bc)的主要目标是学习一个条件概率密度模型（生成模型）$\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$，准确捕捉数据的潜在概率分布，记作$\rho_{\mathcal{{\mathcal{D}}}}({\bm{a}}|{\bm{c}})$，其中${\bm{a}}$是我们想要生成的数据变量，${\bm{c}}$是条件变量。核心思想是确保模型生成的样本${\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$与真实数据样本${\bm{a}}\sim\rho_{\mathcal{{\mathcal{D}}}}({\bm{a}}|{\bm{c}})$不可区分。'
- en: In the context of decision-making and control, ${\bm{a}}$ represents the action,
    which range from end-effector poses [[58](#bib.bib58)], displacements [[59](#bib.bib59)],
    trajectories [[36](#bib.bib36)], desired scene arrangement [[60](#bib.bib60)],
    to robot configurations [[61](#bib.bib61)]. The conditioning variable ${\bm{c}}:({\bm{o}},{\bm{g}})$
    is usually decoupled between ${\bm{o}}$ the observations of the scene and ${\bm{g}}$
    the goal definition. Observations may include visual data [[62](#bib.bib62)],
    3D spatial data [[63](#bib.bib63)], or robot proprioception, providing information
    about the state of the environment. Depending on the task, it is also common to
    provide a history of the last $t$ observations rather than a single-step observation.
    The goal variable ${\bm{g}}$ defines the desired behavior or task that the robot
    should accomplish. This goal can be specified in a variety of ways, including
    language commands [[64](#bib.bib64)], desired goal states [[65](#bib.bib65)],
    or goal images [[66](#bib.bib66)]; each provides a different approach to directing
    the robot’s actions toward achieving specific outcomes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策和控制的背景下，${\bm{a}}$ 代表动作，其范围包括末端执行器姿态 [[58](#bib.bib58)]、位移 [[59](#bib.bib59)]、轨迹
    [[36](#bib.bib36)]、期望场景排列 [[60](#bib.bib60)] 以及机器人配置 [[61](#bib.bib61)]。条件变量 ${\bm{c}}:({\bm{o}},{\bm{g}})$
    通常在场景的观察 ${\bm{o}}$ 和目标定义 ${\bm{g}}$ 之间解耦。观察可以包括视觉数据 [[62](#bib.bib62)]、3D 空间数据
    [[63](#bib.bib63)] 或机器人自感知，提供有关环境状态的信息。根据任务的不同，常常会提供最近 $t$ 次观察的历史，而不是单步观察。目标变量
    ${\bm{g}}$ 定义了机器人应完成的期望行为或任务。这个目标可以通过多种方式指定，包括语言命令 [[64](#bib.bib64)]、期望目标状态 [[65](#bib.bib65)]
    或目标图像 [[66](#bib.bib66)]；每种方式都为引导机器人行动以实现特定结果提供了不同的方法。
- en: To learn the model $\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$, we operate under
    the assumption that the true data distribution $\rho_{\mathcal{{\mathcal{D}}}}({\bm{a}}|{\bm{c}})$
    is unknown and that we only have access to a finite set of samples drawn from
    that distribution. These samples form a dataset ${\mathcal{D}}:\{{\bm{a}}_{n},{\bm{c}}_{n}\}_{n=1}^{N}$
    where $N$ is the number of samples. The task of learning the generative model
    is then formulated as an optimization problem, where the objective is to minimize
    the divergence between the learned distribution $\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$
    and the true data distribution $\rho_{{\mathcal{D}}}({\bm{a}}|{\bm{c}})$
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习模型 $\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$，我们假设真实的数据分布 $\rho_{\mathcal{{\mathcal{D}}}}({\bm{a}}|{\bm{c}})$
    是未知的，并且我们仅能访问从该分布中抽取的有限样本。这些样本形成了一个数据集 ${\mathcal{D}}:\{{\bm{a}}_{n},{\bm{c}}_{n}\}_{n=1}^{N}$，其中
    $N$ 是样本数量。学习生成模型的任务因此被表述为一个优化问题，其目标是最小化学习到的分布 $\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$
    和真实数据分布 $\rho_{{\mathcal{D}}}({\bm{a}}|{\bm{c}})$ 之间的差异
- en: '|  | $\displaystyle{\bm{\theta}}^{*}=\operatorname*{arg\,min}_{{\bm{\theta}}}{\mathbb{E}}_{{\bm{a}},{\bm{c}}\sim{\mathcal{D}}}\left[{\mathbb{D}}(\rho_{{\mathcal{D}}}({\bm{a}}&#124;{\bm{c}}),\rho_{{\bm{\theta}}}({\bm{a}}&#124;{\bm{c}}))\right],$
    |  | (1) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{\theta}}^{*}=\operatorname*{arg\,min}_{{\bm{\theta}}}{\mathbb{E}}_{{\bm{a}},{\bm{c}}\sim{\mathcal{D}}}\left[{\mathbb{D}}(\rho_{{\mathcal{D}}}({\bm{a}}&#124;{\bm{c}}),\rho_{{\bm{\theta}}}({\bm{a}}&#124;{\bm{c}}))\right],$
    |  | (1) |'
- en: 'where ${\mathbb{D}}$ is the divergence distance. Despite the general representation
    in ([1](#S2.E1 "Equation 1 ‣ II Problem Formulation ‣ Deep Generative Models in
    Robotics: A Survey on Learning from Multimodal Demonstrations")), the training
    algorithm is modified depending on the selected model $\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$
    (Gaussian, [Energy Based Models (EBM)](#glo.main.ebm) [[67](#bib.bib67), [68](#bib.bib68)],
    [DM](#glo.main.dm) [[69](#bib.bib69), [33](#bib.bib33)]).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\mathbb{D}}$ 是差异距离。尽管在 ([1](#S2.E1 "方程 1 ‣ II 问题表述 ‣ 机器人中的深度生成模型：多模态示范学习综述"))
    中有通用表示，但训练算法会根据所选模型 $\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$（高斯，[能量模型 (EBM)](#glo.main.ebm)
    [[67](#bib.bib67), [68](#bib.bib68)]，[DM](#glo.main.dm) [[69](#bib.bib69), [33](#bib.bib33)])
    进行调整。
- en: III Density Estimation Models
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 密度估计模型
- en: 'The central idea of this survey is to present in a unified way the different
    types of models that have been used in robotics to properly capture the multimodality
    in the demonstrations. Thus, this survey does not include works that have used
    unimodal models to represent the policies, and focuses on models that are able
    to generate samples from multimodal distributions. We categorize these models
    into five groups:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的核心思想是以统一的方式呈现在机器人领域中用于适当捕捉演示的多模态性的不同类型模型。因此，本调查不包括使用单模态模型表示策略的工作，而是关注那些能够从多模态分布中生成样本的模型。我们将这些模型分为五类：
- en: Sampling Models. Given a noise sample, these models generate the action directly.
    They tend to have very fast inference times. [DGM](#glo.main.dgm) like [VAE](#glo.main.vae),
    [Generative Adversarial Networks (GAN)](#glo.main.gan), or [Normalizing Flows
    (NFlow)](#glo.main.nf) fall into this category.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 采样模型。给定一个噪声样本，这些模型直接生成动作。它们通常具有非常快的推理时间。[DGM](#glo.main.dgm) 如 [VAE](#glo.main.vae)、[生成对抗网络
    (GAN)](#glo.main.gan) 或 [规范流 (NFlow)](#glo.main.nf) 都属于这一类别。
- en: Energy-based Models. Given an action candidate as input, [EBM](#glo.main.ebm)
    returns a scalar value representing the energy of that action candidate. Sampling
    from a [EBM](#glo.main.ebm) usually requires [Markov Chain Monte Carlo (MCMC)](#glo.main.mcmc)
    strategies. We also consider as [EBM](#glo.main.ebm), models that define the energy
    as the distance between feature descriptors [[70](#bib.bib70)].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 基于能量的模型。给定一个动作候选作为输入，[EBM](#glo.main.ebm) 返回一个标量值，表示该动作候选的能量。通常，从 [EBM](#glo.main.ebm)
    采样需要 [马尔可夫链蒙特卡洛 (MCMC)](#glo.main.mcmc) 策略。我们还将一些将能量定义为特征描述符之间距离的模型 [[70](#bib.bib70)]
    视为 [EBM](#glo.main.ebm)。
- en: Diffusion Models. [DM](#glo.main.dm) are a type of generative model that learns
    to generate data by reversing a gradual corruption process. These types of models
    are able to generate high quality samples due to the iterative denoising process.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型。[DM](#glo.main.dm)是一种生成模型，通过逆向逐步损坏过程来学习生成数据。这些类型的模型能够生成高质量的样本，因为它们采用了迭代去噪过程。
- en: Categorical Models. Given a context variable, categorical models represent the
    action distribution as a discrete distribution of $k$ bins. We group both GPT-inspired
    action models [[37](#bib.bib37)] and action value maps [[62](#bib.bib62)] into
    this category. Note that despite the categorical distributions represent both
    types of models, action value maps directly inpaint the categorical distribution
    in the visual observations. In contrast, in GPT-inspired models, observations
    and action distributions are represented in different spaces.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型。给定一个上下文变量，分类模型将动作分布表示为 $k$ 个离散分布。我们将 GPT 风格的动作模型 [[37](#bib.bib37)] 和动作值映射
    [[62](#bib.bib62)] 都归入这一类别。需要注意的是，尽管分类分布代表了这两种模型，动作值映射直接在视觉观察中对分类分布进行填补。相比之下，在
    GPT 风格的模型中，观察和动作分布在不同的空间中表示。
- en: Mixture Density Models. Given a context variable, [Mixture Density Models (MDM)](#glo.main.mdm)
    returns the parameters of a mixture density function representing the action distribution.
    Common choices are models that return the means, standard deviations, and weights
    of a [GMM](#glo.main.gmm) or a mixture of logistic distributions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 混合密度模型。给定一个上下文变量，[混合密度模型 (MDM)](#glo.main.mdm) 返回表示动作分布的混合密度函数的参数。常见选择包括返回 [GMM](#glo.main.gmm)
    的均值、标准差和权重的模型，或逻辑分布的混合模型。
- en: The classification presented is not rigid or definitive. For example, Normalizing
    Flows [[71](#bib.bib71)] operates as a sampling model in the generation, but it
    also facilitates the calculation of the likelihood of a sample in a manner akin
    to [EBM](#glo.main.ebm). Furthermore, we cluster inside Categorical models to
    both GPT-style autoregressive models [[37](#bib.bib37), [72](#bib.bib72)] and
    Action Value Maps [[73](#bib.bib73), [74](#bib.bib74)]. While both models express
    the distribution via a categorical distribution, they diverge conceptually.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所呈现的分类并不是严格或决定性的。例如，规范流 [[71](#bib.bib71)] 在生成过程中作为采样模型操作，但它也以类似于 [EBM](#glo.main.ebm)
    的方式来计算样本的似然性。此外，我们将 GPT 风格的自回归模型 [[37](#bib.bib37), [72](#bib.bib72)] 和动作值映射 [[73](#bib.bib73),
    [74](#bib.bib74)] 都归入分类模型中。虽然这两种模型都通过分类分布来表达分布，但它们在概念上存在差异。
- en: In the following, we present in five distinct subsection each model type, its
    inherent properties and the type of problems in which it has been applied.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下部分中，我们将分五个不同的小节介绍每种模型类型、其固有属性以及它们应用的具体问题类型。
- en: III-A Sampling Models
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 采样模型
- en: 'We call sampling models to the set of deep generative models that allow explicit
    sample generation. Given a context variable ${\bm{c}}\in\mathbb{R}^{c}$ and a
    latent variable ${\bm{z}}\in\mathbb{R}^{z}$, the network decodes the latent variable
    into a sample ${\bm{a}}={\bm{D}}_{{\bm{\theta}}}({\bm{z}},{\bm{c}})$. To generate
    an action sample from our model ${\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$,
    we first sample a latent variable from an easy-to-sample from distribution ${\bm{z}}\sim{\mathcal{N}}({\bm{0}},{\bm{I}})$
    (e.g., a normal distribution) and decode it into an action ${\bm{a}}={\bm{D}}_{{\bm{\theta}}}({\bm{z}},{\bm{c}})$
    (see [Figure 2](#S1.F2 "In I-A Challenges in Learning from Offline Demonstrations
    ‣ I Introduction ‣ Deep Generative Models in Robotics: A Survey on Learning from
    Multimodal Demonstrations")). There are several generative models that fall into
    this category: [GAN](#glo.main.gan) [[75](#bib.bib75)], [VAE](#glo.main.vae) [[38](#bib.bib38)],
    or [NFlow](#glo.main.nf) [[71](#bib.bib71)].'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采样模型称为允许显式生成样本的深度生成模型的集合。给定一个上下文变量${\bm{c}}\in\mathbb{R}^{c}$和一个潜在变量${\bm{z}}\in\mathbb{R}^{z}$，网络将潜在变量解码为样本${\bm{a}}={\bm{D}}_{{\bm{\theta}}}({\bm{z}},{\bm{c}})$。为了从我们的模型中生成一个动作样本${\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$，我们首先从一个易于采样的分布中采样一个潜在变量${\bm{z}}\sim{\mathcal{N}}({\bm{0}},{\bm{I}})$（例如，正态分布），然后将其解码为一个动作${\bm{a}}={\bm{D}}_{{\bm{\theta}}}({\bm{z}},{\bm{c}})$（见[图
    2](#S1.F2 "在 I-A 从离线演示中学习的挑战 ‣ I 引言 ‣ 机器人中的深度生成模型：关于从多模态演示中学习的调查")）。有几种生成模型属于这个类别：[GAN](#glo.main.gan)
    [[75](#bib.bib75)]，[VAE](#glo.main.vae) [[38](#bib.bib38)]，或者[NFlow](#glo.main.nf)
    [[71](#bib.bib71)]。
- en: III-A1 Main applications
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 主要应用
- en: 'In the field of robotics, these types of models have been used in several contexts
    and applications (see [Figure 2](#S1.F2 "In I-A Challenges in Learning from Offline
    Demonstrations ‣ I Introduction ‣ Deep Generative Models in Robotics: A Survey
    on Learning from Multimodal Demonstrations")).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人领域，这些类型的模型已经在多个上下文和应用中使用（见[图 2](#S1.F2 "在 I-A 从离线演示中学习的挑战 ‣ I 引言 ‣ 机器人中的深度生成模型：关于从多模态演示中学习的调查")）。
- en: As an Initial Sampling Distribution. Due to their fast sampling time, they have
    been used as initial sampling distributions for motion planning and optimization
    problems [[47](#bib.bib47), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79)]. In [[47](#bib.bib47)], conditioned [VAE](#glo.main.vae) were
    used to sample initial collision-free guided states for sampling-based motion
    planning problems [[80](#bib.bib80), [81](#bib.bib81)]. In [[77](#bib.bib77)],
    [GAN](#glo.main.gan) were used to generate initial states for long-horizon tasks
    and motion planning problems. The output of the [GAN](#glo.main.gan) was later
    optimized to satisfy a set of constraints.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作为初始采样分布。由于其快速的采样时间，它们已被用作运动规划和优化问题的初始采样分布[[47](#bib.bib47), [76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79)]。在[[47](#bib.bib47)]中，条件[VAE](#glo.main.vae)被用于采样初始的无碰撞引导状态，适用于基于采样的运动规划问题[[80](#bib.bib80),
    [81](#bib.bib81)]。在[[77](#bib.bib77)]中，[GAN](#glo.main.gan)被用于生成长时间范围任务和运动规划问题的初始状态。[GAN](#glo.main.gan)的输出随后被优化以满足一组约束条件。
- en: As Exploration Guiding Models. A common problem in [RL](#glo.main.rl) is the
    exploration. Given the large state-action space, deciding which regions are meaningful
    to explore is usually a hard problem. To guide this exploration, several works [[82](#bib.bib82),
    [48](#bib.bib48), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85)] have explored
    learning a sampling model that encodes all the possible behaviors in a dataset.
    This model can be used integrated into a [RL](#glo.main.rl) problem, by running
    a policy in the latent space. Given that the model will generate solutions from
    the dataset, the policy learns to search in the latent space to maximize a given
    reward.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 作为探索指导模型。在[RL](#glo.main.rl)中，一个常见的问题是探索。由于状态-动作空间很大，决定哪些区域是有意义的进行探索通常是一个困难的问题。为了指导这种探索，已有几项工作[[82](#bib.bib82),
    [48](#bib.bib48), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85)]探讨了学习一个采样模型，该模型编码数据集中所有可能的行为。这个模型可以集成到[RL](#glo.main.rl)问题中，通过在潜在空间中运行策略。由于模型将从数据集中生成解决方案，策略学习在潜在空间中搜索以最大化给定的奖励。
- en: As Explicit Sampling Models. The most straightforward application is to use
    the model as a generative model. In this context, sampling models have been used
    to generate grasp poses [[5](#bib.bib5), [86](#bib.bib86)], inverse kinematic
    solutions [[61](#bib.bib61), [65](#bib.bib65)], or directly sample actions in
    a policy [[87](#bib.bib87), [88](#bib.bib88)].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 作为显式采样模型。最直接的应用是将模型用作生成模型。在这种情况下，采样模型被用于生成抓取姿态 [[5](#bib.bib5)、[86](#bib.bib86)]、逆运动学解
    [[61](#bib.bib61)、[65](#bib.bib65)] 或直接在策略中采样动作 [[87](#bib.bib87)、[88](#bib.bib88)]。
- en: III-A2 Training Sampling Model
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 训练采样模型
- en: '[GAN](#glo.main.gan), [VAE](#glo.main.vae), and [NFlow](#glo.main.nf) share
    the same sampling process. However, each model is trained using a different algorithm.
    In the following, we briefly present the training pipelines for the three models.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[GAN](#glo.main.gan)、[VAE](#glo.main.vae) 和 [NFlow](#glo.main.nf) 共享相同的采样过程。然而，每个模型使用不同的算法进行训练。接下来，我们简要介绍这三种模型的训练流程。'
- en: 'Variational Autoencoders. The [VAE](#glo.main.vae) model, introduced in [[38](#bib.bib38)],
    consists of two networks: an encoder and a decoder. Given an action ${\bm{a}}$,
    the encoder maps it to the parameters of a latent normal distribution ${\bm{\mu}}_{z},{\bm{\sigma}}_{z}={\mathcal{E}}_{{\bm{\psi}}}({\bm{a}})$.
    Given a sample from the latent space ${\bm{z}}\sim{\mathcal{N}}({\bm{\mu}}_{z},{\bm{\sigma}}_{z}{\bm{I}})$,
    the decoder maps the latent variable to the action space $\hat{{\bm{a}}}={\bm{D}}_{{\bm{\theta}}}({\bm{z}},{\bm{c}})$,
    conditioned on the context variable ${\bm{c}}$.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器。[VAE](#glo.main.vae) 模型由 [[38](#bib.bib38)] 提出，包含两个网络：编码器和解码器。给定一个动作
    ${\bm{a}}$，编码器将其映射到潜在正态分布的参数 ${\bm{\mu}}_{z},{\bm{\sigma}}_{z}={\mathcal{E}}_{{\bm{\psi}}}({\bm{a}})$。给定潜在空间中的样本
    ${\bm{z}}\sim{\mathcal{N}}({\bm{\mu}}_{z},{\bm{\sigma}}_{z}{\bm{I}})$，解码器将潜在变量映射到动作空间
    $\hat{{\bm{a}}}={\bm{D}}_{{\bm{\theta}}}({\bm{z}},{\bm{c}})$，条件是上下文变量 ${\bm{c}}$。
- en: 'The training loss consists of two parts: a reconstruction loss and a KL divergence.
    Given a dataset ${\mathcal{D}}:\{{\bm{a}}_{n},{\bm{c}}_{n}\}_{n=1}^{N}$, the [VAE](#glo.main.vae)
    loss is given by'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失由两部分组成：重建损失和 KL 散度。给定数据集 ${\mathcal{D}}:\{{\bm{a}}_{n},{\bm{c}}_{n}\}_{n=1}^{N}$，[VAE](#glo.main.vae)
    损失由下式给出
- en: '|  | $\displaystyle{\mathcal{L}}({\bm{\theta}},{\bm{\psi}})=$ | $\displaystyle{\mathbb{E}}_{{\bm{a}},{\bm{c}}\sim{\mathcal{D}}}[{\mathbb{D}}_{\text{KL}}(\rho({\bm{z}}&#124;{\bm{a}}),\rho({\bm{z}}))$
    |  | (2) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{L}}({\bm{\theta}},{\bm{\psi}})=$ | $\displaystyle{\mathbb{E}}_{{\bm{a}},{\bm{c}}\sim{\mathcal{D}}}[{\mathbb{D}}_{\text{KL}}(\rho({\bm{z}}&#124;{\bm{a}}),\rho({\bm{z}}))$
    |  | (2) |'
- en: '|  |  | $\displaystyle+{\mathbb{E}}_{{\bm{z}}\sim\rho({\bm{z}}&#124;{\bm{a}})}[&#124;&#124;{\bm{D}}_{{\bm{\theta}}}({\bm{z}},{\bm{c}})-{\bm{a}}&#124;&#124;_{2}^{2}]]$
    |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+{\mathbb{E}}_{{\bm{z}}\sim\rho({\bm{z}}&#124;{\bm{a}})}[&#124;&#124;{\bm{D}}_{{\bm{\theta}}}({\bm{z}},{\bm{c}})-{\bm{a}}&#124;&#124;_{2}^{2}]]$
    |  |'
- en: where $\rho({\bm{z}}|{\bm{a}})={\mathcal{N}}({\bm{z}}|{\mathcal{E}}({\bm{a}}))$
    is a Gaussian whose parameters are the encoder outputs. $\rho({\bm{z}})={\mathcal{N}}({\bm{0}},{\bm{I}})$
    is a Gaussian around zero. While the KL divergence term encourages the encoder
    to generate distributions close to $\rho({\bm{z}})$, the reconstruction loss aims
    to decode a latent sample to look as similar as possible to the input ${\bm{a}}$.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\rho({\bm{z}}|{\bm{a}})={\mathcal{N}}({\bm{z}}|{\mathcal{E}}({\bm{a}}))$
    是一个高斯，其参数是编码器的输出。$\rho({\bm{z}})={\mathcal{N}}({\bm{0}},{\bm{I}})$ 是一个围绕零的高斯。尽管
    KL 散度项鼓励编码器生成接近 $\rho({\bm{z}})$ 的分布，但重建损失的目标是将潜在样本解码得尽可能类似于输入 ${\bm{a}}$。
- en: Generative Adversarial Networks. Unlike [VAE](#glo.main.vae), [GAN](#glo.main.gan)
    [[75](#bib.bib75)] suggests having a discriminator $p=C_{{\bm{\psi}}}({\bm{a}},{\bm{c}})$
    instead of an encoder. Given a sample generated by the model ${\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$,
    the discriminator is trained to discriminate between samples generated by our
    model and samples coming from the dataset, while the generator is trained to make
    the generated samples as similar as possible to the dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络。与 [VAE](#glo.main.vae) 不同，[GAN](#glo.main.gan) [[75](#bib.bib75)] 建议使用一个鉴别器
    $p=C_{{\bm{\psi}}}({\bm{a}},{\bm{c}})$ 代替编码器。给定模型生成的样本 ${\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$，鉴别器被训练以区分我们模型生成的样本和来自数据集的样本，而生成器被训练使生成的样本尽可能类似于数据集。
- en: Given a dataset ${\mathcal{D}}:\{{\bm{a}}_{n},{\bm{c}}_{n}\}_{n=1}^{N}$, the
    [GAN](#glo.main.gan) objective is represented by the binary cross-entropy loss
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据集 ${\mathcal{D}}:\{{\bm{a}}_{n},{\bm{c}}_{n}\}_{n=1}^{N}$，[GAN](#glo.main.gan)
    的目标由二元交叉熵损失表示
- en: '|  | $\displaystyle{\mathcal{J}}({\bm{\theta}},{\bm{\psi}})=$ | $\displaystyle{\mathbb{E}}_{{\bm{a}},{\bm{c}}\sim{\mathcal{D}}}[\log
    C_{{\bm{\psi}}}({\bm{a}},{\bm{c}})+$ |  | (3) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{J}}({\bm{\theta}},{\bm{\psi}})=$ | $\displaystyle{\mathbb{E}}_{{\bm{a}},{\bm{c}}\sim{\mathcal{D}}}[\log
    C_{{\bm{\psi}}}({\bm{a}},{\bm{c}})+$ |  | (3) |'
- en: '|  |  | $\displaystyle{\mathbb{E}}_{{\bm{z}}\sim{\mathcal{N}}({\bm{0}},{\bm{I}})}\left[\log(1-C_{{\bm{\psi}}}({\bm{D}}_{{\bm{\theta}}}({\bm{z}},{\bm{c}})),{\bm{c}})\right]].$
    |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\mathbb{E}}_{{\bm{z}}\sim{\mathcal{N}}({\bm{0}},{\bm{I}})}\left[\log(1-C_{{\bm{\psi}}}({\bm{D}}_{{\bm{\theta}}}({\bm{z}},{\bm{c}})),{\bm{c}})\right]].$
    |  |'
- en: Then, the optimization problem is solved by a minimization-maximization problem,
    where we aim to minimize the objective with respect to ${\bm{\psi}}$ (the discriminator)
    and maximize it with respect to ${\bm{\theta}}$ (the generator). The discriminator
    aims to discriminate between real data samples and the fake samples produced by
    the generator, while the generator aims to produce samples that are indistinguishable
    from real data to the discriminator.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，优化问题通过最小化-最大化问题来解决，我们的目标是最小化相对于 ${\bm{\psi}}$（判别器）的目标，并最大化相对于 ${\bm{\theta}}$（生成器）的目标。判别器旨在区分真实数据样本和生成器生成的虚假样本，而生成器则旨在生成与真实数据无异的样本。
- en: Normalizing Flows. The generator ${\bm{D}}_{{\bm{\theta}}}$ in [NFlow](#glo.main.nf)
    is different from those in [VAE](#glo.main.vae) or [GAN](#glo.main.gan). While
    in [GAN](#glo.main.gan) or [VAE](#glo.main.vae) it is represented by an arbitrary
    network, in [NFlow](#glo.main.nf) we are required to have an invertible network [[71](#bib.bib71),
    [89](#bib.bib89), [90](#bib.bib90)] as generator.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化流。在[NFlow](#glo.main.nf)中的生成器 ${\bm{D}}_{{\bm{\theta}}}$ 与[VAE](#glo.main.vae)
    或[GAN](#glo.main.gan)中的生成器不同。虽然在[GAN](#glo.main.gan)或[VAE](#glo.main.vae)中它由任意网络表示，但在[NFlow](#glo.main.nf)中，我们需要一个可逆网络[[71](#bib.bib71)，[89](#bib.bib89)，[90](#bib.bib90)]作为生成器。
- en: Since the generator ${\bm{D}}_{{\bm{\theta}}}$ is invertible, [NFlow](#glo.main.nf)
    allows the exact calculation of the likelihood [[71](#bib.bib71)]
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成器 ${\bm{D}}_{{\bm{\theta}}}$ 是可逆的，[NFlow](#glo.main.nf) 允许精确计算似然[[71](#bib.bib71)]。
- en: '|  | $\displaystyle\log\rho_{{\bm{\theta}}}({\bm{a}}&#124;{\bm{c}})=\log\rho_{z}\left({\bm{D}}_{{\bm{\theta}}}^{-1}({\bm{a}},{\bm{c}})\right)+\log&#124;\det{\bm{J}}_{{\bm{D}}_{{\bm{\theta}}}}({\bm{a}},{\bm{c}})&#124;,$
    |  | (4) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\log\rho_{{\bm{\theta}}}({\bm{a}}&#124;{\bm{c}})=\log\rho_{z}\left({\bm{D}}_{{\bm{\theta}}}^{-1}({\bm{a}},{\bm{c}})\right)+\log&#124;\det{\bm{J}}_{{\bm{D}}_{{\bm{\theta}}}}({\bm{a}},{\bm{c}})&#124;,$
    |  | (4) |'
- en: where $\rho_{z}={\mathcal{N}}({\bm{0}},{\bm{I}})$ is the latent space normal
    distribution and ${\bm{J}}_{{\bm{D}}_{{\bm{\theta}}}}$ is the Jacobian of the
    decoder.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\rho_{z}={\mathcal{N}}({\bm{0}},{\bm{I}})$ 是潜在空间的正态分布，${\bm{J}}_{{\bm{D}}_{{\bm{\theta}}}}$
    是解码器的雅可比矩阵。
- en: Then, given a dataset ${\mathcal{D}}:\{{\bm{a}}_{n},{\bm{c}}_{n}\}_{n=1}^{N}$,
    [NFlow](#glo.main.nf) are trained by minimizing the negative log-likelihood
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，给定一个数据集 ${\mathcal{D}}:\{{\bm{a}}_{n},{\bm{c}}_{n}\}_{n=1}^{N}$，[NFlow](#glo.main.nf)
    通过最小化负对数似然来训练。
- en: '|  | $\displaystyle{\mathcal{L}}({\bm{\theta}})$ | $\displaystyle=-{\mathbb{E}}_{{\bm{a}},{\bm{c}}\sim{\mathcal{D}}}\left[\log\rho_{{\bm{\theta}}}({\bm{a}}&#124;{\bm{c}})\right].$
    |  | (5) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{L}}({\bm{\theta}})$ | $\displaystyle=-{\mathbb{E}}_{{\bm{a}},{\bm{c}}\sim{\mathcal{D}}}\left[\log\rho_{{\bm{\theta}}}({\bm{a}}&#124;{\bm{c}})\right].$
    |  | (5) |'
- en: Note that unlike [VAE](#glo.main.vae) and [GAN](#glo.main.gan), [NFlow](#glo.main.nf)
    does not require to training an additional model. Additionally, since the generator
    is invertible, we can compute the likelihood of a sample in our model, similar
    to [EBM](#glo.main.ebm).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与[VAE](#glo.main.vae)和[GAN](#glo.main.gan)不同，[NFlow](#glo.main.nf) 不需要训练额外的模型。此外，由于生成器是可逆的，我们可以计算模型中样本的似然，类似于[EBM](#glo.main.ebm)。
- en: III-B Energy-Based Models.
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 能量基础模型。
- en: 'We call [EBM](#glo.main.ebm) to the set of deep generative models that, given
    an action ${\bm{a}}$, output a scalar value ${\bm{e}}\in\mathbb{R}$, ${\bm{e}}=E_{{\bm{\theta}}}({\bm{a}},{\bm{c}})$,
    where ${\bm{c}}$ denotes the conditioning context variable ([Figure 3](#S3.F3
    "In III-B Energy-Based Models. ‣ III Density Estimation Models ‣ Deep Generative
    Models in Robotics: A Survey on Learning from Multimodal Demonstrations")). In
    [EBM](#glo.main.ebm), the probability density model $\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$
    is represented by a Boltzmann distribution'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称[EBM](#glo.main.ebm)为一组深度生成模型，这些模型在给定一个动作 ${\bm{a}}$ 时输出一个标量值 ${\bm{e}}\in\mathbb{R}$，${\bm{e}}=E_{{\bm{\theta}}}({\bm{a}},{\bm{c}})$，其中
    ${\bm{c}}$ 表示条件上下文变量（[图3](#S3.F3 "在 III-B 能量基础模型中。 ‣ III 密度估计模型 ‣ 机器人中的深度生成模型：关于从多模态演示中学习的调查")）。在[EBM](#glo.main.ebm)中，概率密度模型
    $\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$ 由玻尔兹曼分布表示。
- en: '|  | $\displaystyle\rho_{{\bm{\theta}}}({\bm{a}}&#124;{\bm{c}})\propto\exp\left(-E_{{\bm{\theta}}}({\bm{a}},{\bm{c}})\right),$
    |  | (6) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\rho_{{\bm{\theta}}}({\bm{a}}&#124;{\bm{c}})\propto\exp\left(-E_{{\bm{\theta}}}({\bm{a}},{\bm{c}})\right),$
    |  | (6) |'
- en: where $E_{{\bm{\theta}}}({\bm{a}},{\bm{c}})$ is the energy of the distribution,
    i.e., the unnormalized log-likelihood.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $E_{{\bm{\theta}}}({\bm{a}},{\bm{c}})$ 是分布的能量，即未标准化的对数似然。
- en: '![Refer to caption](img/45cb2783312e6ba81fc2a3611bd41cdd.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/45cb2783312e6ba81fc2a3611bd41cdd.png)'
- en: 'Figure 3: Left: A visual representation of an [EBM](#glo.main.ebm). Given as
    input an action variable ${\bm{a}}$, [EBM](#glo.main.ebm) output the unnormalized
    log probability of the input action $e={\bm{E}}_{{\bm{\theta}}}({\bm{a}},{\bm{c}})$.
    Right: A visual representation of the different strategies to train or represent
    an [EBM](#glo.main.ebm): Contrastive Divergence [[91](#bib.bib91)], Supervised
    Learning [[92](#bib.bib92)] and, Neural Descriptor Fields [[70](#bib.bib70)].'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：左：一个[EBM](#glo.main.ebm)的可视化表示。以动作变量 ${\bm{a}}$ 作为输入，[EBM](#glo.main.ebm)
    输出输入动作的未标准化对数概率 $e={\bm{E}}_{{\bm{\theta}}}({\bm{a}},{\bm{c}})$。右：训练或表示[EBM](#glo.main.ebm)的不同策略的可视化表示：对比散度[[91](#bib.bib91)]、监督学习[[92](#bib.bib92)]和神经描述场[[70](#bib.bib70)]。
- en: Sampling from an [EBM](#glo.main.ebm) ${\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$
    is not direct due to the implicit nature of the model. [EBM](#glo.main.ebm) define
    a probability distribution over the data by an energy function, and sampling requires
    methods like [MCMC](#glo.main.mcmc) to approximate the distribution. A common
    sampling algorithm is Langevin Monte Carlo. Given an initial sample ${\bm{a}}_{0}\sim\rho_{0}({\bm{a}}_{0})$
    generated from a simple prior distribution, the samples are generated by iteratively
    updating the sample by
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从[EBM](#glo.main.ebm)中采样 ${\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$
    由于模型的隐含性质而不直接。[EBM](#glo.main.ebm)通过能量函数定义数据的概率分布，采样需要像[MCMC](#glo.main.mcmc)这样的近似分布的方法。常见的采样算法是Langevin
    Monte Carlo。给定从简单先验分布生成的初始样本 ${\bm{a}}_{0}\sim\rho_{0}({\bm{a}}_{0})$，样本通过迭代更新生成。
- en: '|  | $\displaystyle{\bm{a}}_{k+1}={\bm{a}}_{k}-\frac{\epsilon}{2}\nabla_{{\bm{a}}}E_{{\bm{\theta}}}({\bm{a}}_{k},{\bm{c}})+\sqrt{\epsilon}{\mathcal{N}}({\bm{0}},{\bm{I}}),$
    |  | (7) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{a}}_{k+1}={\bm{a}}_{k}-\frac{\epsilon}{2}\nabla_{{\bm{a}}}E_{{\bm{\theta}}}({\bm{a}}_{k},{\bm{c}})+\sqrt{\epsilon}{\mathcal{N}}({\bm{0}},{\bm{I}}),$
    |  | (7) |'
- en: where $\epsilon>0$ is a small constant. This process can be computationally
    intensive and slower than direct sampling methods used in models such as [VAE](#glo.main.vae)
    or [GAN](#glo.main.gan). Alternatively, given the implicit nature of the [EBM](#glo.main.ebm),
    some works [[93](#bib.bib93), [94](#bib.bib94)] search for the most likely sample
    by solving an optimization problem
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\epsilon>0$ 是一个小常数。这个过程可能计算密集且比直接采样方法（如[VAE](#glo.main.vae)或[GAN](#glo.main.gan)）更慢。作为替代方案，鉴于[EBM](#glo.main.ebm)的隐含性质，一些工作[[93](#bib.bib93)，[94](#bib.bib94)]通过解决优化问题来寻找最可能的样本。
- en: '|  | $\displaystyle{\bm{a}}^{*}=\arg\min_{{\bm{a}}}E_{{\bm{\theta}}}({\bm{a}},{\bm{c}}).$
    |  | (8) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{a}}^{*}=\arg\min_{{\bm{a}}}E_{{\bm{\theta}}}({\bm{a}},{\bm{c}}).$
    |  | (8) |'
- en: Because of their implicit nature, [EBM](#glo.main.ebm) contain several interesting
    properties. As explored in [[95](#bib.bib95), [93](#bib.bib93), [96](#bib.bib96)],
    [EBM](#glo.main.ebm) allow a modular composition of different [EBM](#glo.main.ebm).
    This modular approach allows separate [EBM](#glo.main.ebm) to be trained to represent
    different behaviors or aspects of the data, and then these models can be combined.
    The result is a composite model in which the variable ${\bm{a}}$ has a high probability
    under all the component models, effectively integrating different features or
    patterns captured by each individual [EBM](#glo.main.ebm).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其隐含性质，[EBM](#glo.main.ebm)具有几个有趣的特性。如在[[95](#bib.bib95)，[93](#bib.bib93)，[96](#bib.bib96)]中探讨的，[EBM](#glo.main.ebm)允许不同[EBM](#glo.main.ebm)的模块化组合。这种模块化方法允许单独的[EBM](#glo.main.ebm)被训练以表示数据的不同行为或方面，然后这些模型可以组合。结果是一个复合模型，其中变量
    ${\bm{a}}$ 在所有组件模型下都有较高的概率，从而有效地整合了每个单独[EBM](#glo.main.ebm)所捕捉到的不同特征或模式。
- en: Another interesting property is the energy mapping [[58](#bib.bib58), [96](#bib.bib96)].
    [EBM](#glo.main.ebm), due to its implicit nature, allows to generate samples in
    a space different from their training space. This is especially useful in robotics.
    For example, [EBM](#glo.main.ebm) trained in the task space ${\bm{E}}_{{\bm{\theta}}}({\bm{x}}|{\bm{c}})$
    can effectively guide the selection of robot joint configurations ${\bm{q}}$,
    if we have access to the forward kinematics mapping ${\bm{x}}=\phi_{\text{FK}}({\bm{q}})$.
    By composing the map and the energy ${\bm{E}}_{{\bm{\theta}}}({\bm{q}}|{\bm{c}})={\bm{E}}_{{\bm{\theta}}}(\phi_{\text{FK}}({\bm{q}})|{\bm{c}})$,
    we can represent an [EBM](#glo.main.ebm) in the configuration space that sets
    low energy to those configurations that lead to low energy in the task space.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的特性是能量映射[[58](#bib.bib58), [96](#bib.bib96)]。由于其隐式特性，[EBM](#glo.main.ebm)
    允许在与训练空间不同的空间中生成样本。这在机器人技术中尤其有用。例如，[EBM](#glo.main.ebm) 在任务空间 ${\bm{E}}_{{\bm{\theta}}}({\bm{x}}|{\bm{c}})$
    上进行训练时，如果我们可以访问正向运动学映射 ${\bm{x}}=\phi_{\text{FK}}({\bm{q}})$，它可以有效地指导机器人关节配置 ${\bm{q}}$
    的选择。通过组合映射和能量 ${\bm{E}}_{{\bm{\theta}}}({\bm{q}}|{\bm{c}})={\bm{E}}_{{\bm{\theta}}}(\phi_{\text{FK}}({\bm{q}})|{\bm{c}})$，我们可以在配置空间中表示一个[EBM](#glo.main.ebm)，将低能量设置为那些在任务空间中导致低能量的配置。
- en: III-B1 Main Application
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 主要应用
- en: The main applications of [EBM](#glo.main.ebm) in robotics range from cost/reward
    functions for sequential decision making problems to direct generative models.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[EBM](#glo.main.ebm) 在机器人技术中的主要应用范围包括顺序决策问题的成本/奖励函数到直接生成模型。'
- en: As a cost/reward function. Learning [EBM](#glo.main.ebm) to represent cost or
    reward functions has been widely studied in [Inverse Optimal Control (IOC)](#glo.main.ioc) [[97](#bib.bib97),
    [98](#bib.bib98)] or [IRL](#glo.main.irl) [[14](#bib.bib14), [15](#bib.bib15)].
    Some training algorithms such as [Contrastive Divergence (CD)](#glo.main.cd) loss,
    require the generation of samples from the learned [EBM](#glo.main.ebm) during
    the training process. Different [IRL](#glo.main.irl) and [IOC](#glo.main.ioc)
    methods propose different approaches to sample from the learned [EBM](#glo.main.ebm).
    [[98](#bib.bib98)] proposes solving a maximum entropy trajectory optimization
    to generate the samples, [[99](#bib.bib99)] proposes generating the samples using
    Langevin dynamics, while [[14](#bib.bib14), [15](#bib.bib15)], propose generating
    the samples from a policy trained with [RL](#glo.main.rl), given the learned [EBM](#glo.main.ebm)
    is the reward function.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作为成本/奖励函数。学习[EBM](#glo.main.ebm) 以表示成本或奖励函数在[逆最优控制 (IOC)](#glo.main.ioc) [[97](#bib.bib97),
    [98](#bib.bib98)]或[逆强化学习 (IRL)](#glo.main.irl) [[14](#bib.bib14), [15](#bib.bib15)]中已被广泛研究。一些训练算法，如[对比散度
    (CD)](#glo.main.cd) 损失，需要在训练过程中从学习到的[EBM](#glo.main.ebm)中生成样本。不同的[IRL](#glo.main.irl)
    和[IOC](#glo.main.ioc) 方法提出了不同的从学习到的[EBM](#glo.main.ebm) 中采样的方法。[[98](#bib.bib98)]
    提出了解决最大熵轨迹优化以生成样本，[[99](#bib.bib99)] 提出使用 Langevin 动力学生成样本，而[[14](#bib.bib14),
    [15](#bib.bib15)] 提出了从使用[RL](#glo.main.rl) 训练的策略中生成样本，前提是学习到的[EBM](#glo.main.ebm)
    是奖励函数。
- en: As a generative model. Besides [IRL](#glo.main.irl) and [IOC](#glo.main.ioc)
    approaches, which focus on sequential decision-making problems, several works
    have explored learning [EBM](#glo.main.ebm) for direct action generation. In [[92](#bib.bib92)],
    an [EBM](#glo.main.ebm) is learned to generate grasping poses for arbitrary objects.
    Unlike most of the approaches to learning [EBM](#glo.main.ebm) s, the model is
    learned to fit a 6-DoF [Signed Distance Field (SDF)](#glo.main.sdf). Similarly,
    [[100](#bib.bib100)] generates end-effector poses. However, their work focuses
    on a broader set of tasks beyond grasping, such as opening drawers or pressing
    buttons. In [[94](#bib.bib94)], an [EBM](#glo.main.ebm) is trained as a visuomotor
    policy. The authors claim that a [EBM](#glo.main.ebm)-based policy captures the
    discontinuities in the data better than a deterministic or Gaussian policy. In
    [[101](#bib.bib101)], an [EBM](#glo.main.ebm) is trained as a transition dynamics
    model. In [[102](#bib.bib102)], a cost function for state estimation is learned.
    The learned [EBM](#glo.main.ebm) represents the joint probability of the state
    given an observation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 作为生成模型。除了[IRL](#glo.main.irl)和[IOC](#glo.main.ioc)方法，这些方法专注于顺序决策问题，还有一些工作探讨了直接生成动作的[EBM](#glo.main.ebm)学习。在[[92](#bib.bib92)]中，一个[EBM](#glo.main.ebm)被用来生成对任意对象的抓取姿势。与大多数[EBM](#glo.main.ebm)学习方法不同，该模型被学习以拟合6自由度[签名距离场（SDF）](#glo.main.sdf)。类似地，[[100](#bib.bib100)]生成末端执行器姿势。然而，他们的工作关注于超越抓取的更广泛任务，如打开抽屉或按按钮。在[[94](#bib.bib94)]中，一个[EBM](#glo.main.ebm)被训练作为视觉运动策略。作者声称，基于[EBM](#glo.main.ebm)的策略比确定性或高斯策略更好地捕捉数据中的不连续性。在[[101](#bib.bib101)]中，一个[EBM](#glo.main.ebm)被训练作为过渡动态模型。在[[102](#bib.bib102)]中，学习了一个用于状态估计的成本函数。学习到的[EBM](#glo.main.ebm)表示给定观察的状态的联合概率。
- en: III-B2 Training Energy-Based Model
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 训练基于能量的模型
- en: One of the most popular algorithms for training [EBM](#glo.main.ebm) is [CD](#glo.main.cd)
    [[103](#bib.bib103), [99](#bib.bib99)]. This method involves a contrastive game
    between negative samples, generated from a given distribution and positive samples,
    obtained from the dataset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练[EBM](#glo.main.ebm)的最流行算法之一是[CD](#glo.main.cd) [[103](#bib.bib103), [99](#bib.bib99)]。该方法涉及负样本和从数据集中获得的正样本之间的对比游戏。
- en: 'In contrast to [CD](#glo.main.cd), it has been popular in robotics to train
    models with supervised learning losses such as occupancy loss or [SDF](#glo.main.sdf).
    Despite not being common, given their popularity in robotics, we introduce them
    as additional approaches to fitting [EBM](#glo.main.ebm) (see [Figure 3](#S3.F3
    "In III-B Energy-Based Models. ‣ III Density Estimation Models ‣ Deep Generative
    Models in Robotics: A Survey on Learning from Multimodal Demonstrations")).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于[CD](#glo.main.cd)，在机器人学中，使用监督学习损失，如占据损失或[SDF](#glo.main.sdf)来训练模型已经变得非常流行。尽管这些方法并不常见，但鉴于它们在机器人学中的流行性，我们将其作为拟合[EBM](#glo.main.ebm)的额外方法进行介绍（见[图3](#S3.F3
    "在III-B节 能量模型。‣ III 密度估计模型 ‣ 机器人学中的深度生成模型：多模态示范学习的调查")）。
- en: Contrastive Divergence. A common approach to learning density models is to minimize
    the negative log-likelihood of the data. However, computing the log-likelihood
    requires access to the model’s normalization constant, which is intractable in
    [EBM](#glo.main.ebm). To adapt the negative log-likelihood loss to [EBM](#glo.main.ebm),
    [[103](#bib.bib103)] approximates the calculation by samples. Given $\nabla_{{\bm{\theta}}}\log
    Z_{{\bm{\theta}}}=-{\mathbb{E}}_{{\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}})}\left[\nabla_{{\bm{\theta}}}E_{{\bm{\theta}}}({\bm{a}})\right]$,
    we can approximate the gradient of the negative log-likelihood
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对比散度。学习密度模型的一种常见方法是最小化数据的负对数似然。然而，计算对数似然需要访问模型的归一化常数，这在[EBM](#glo.main.ebm)中是不可计算的。为了将负对数似然损失适应于[EBM](#glo.main.ebm)，[[103](#bib.bib103)]通过样本近似计算。给定$\nabla_{{\bm{\theta}}}\log
    Z_{{\bm{\theta}}}=-{\mathbb{E}}_{{\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}})}\left[\nabla_{{\bm{\theta}}}E_{{\bm{\theta}}}({\bm{a}})\right]$，我们可以近似负对数似然的梯度。
- en: '|  | $\displaystyle\nabla_{{\bm{\theta}}}{\mathcal{L}}({\bm{\theta}})={\mathbb{E}}_{{\color[rgb]{0.5234375,0.6484375,0.06640625}{\bm{a}}}\sim{\mathcal{D}}}\left[\nabla_{{\bm{\theta}}}E_{{\bm{\theta}}}({\color[rgb]{0.5234375,0.6484375,0.06640625}{\bm{a}}})\right]-{\mathbb{E}}_{{\color[rgb]{0.94921875,0.55859375,0.171875}{\bm{a}}}\sim\rho_{{\bm{\theta}}}}\left[\nabla_{{\bm{\theta}}}E_{{\bm{\theta}}}({\color[rgb]{0.94921875,0.55859375,0.171875}{\bm{a}}})\right].$
    |  | (9) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{{\bm{\theta}}}{\mathcal{L}}({\bm{\theta}})={\mathbb{E}}_{{\color[rgb]{0.5234375,0.6484375,0.06640625}{\bm{a}}}\sim{\mathcal{D}}}\left[\nabla_{{\bm{\theta}}}E_{{\bm{\theta}}}({\color[rgb]{0.5234375,0.6484375,0.06640625}{\bm{a}}})\right]-{\mathbb{E}}_{{\color[rgb]{0.94921875,0.55859375,0.171875}{\bm{a}}}\sim\rho_{{\bm{\theta}}}}\left[\nabla_{{\bm{\theta}}}E_{{\bm{\theta}}}({\color[rgb]{0.94921875,0.55859375,0.171875}{\bm{a}}})\right].$
    |  | (9) |'
- en: 'In [Equation 9](#S3.E9 "In III-B2 Training Energy-Based Model ‣ III-B Energy-Based
    Models. ‣ III Density Estimation Models ‣ Deep Generative Models in Robotics:
    A Survey on Learning from Multimodal Demonstrations"), the energy is pushed down
    for the samples in the dataset (positive samples) and pushed up for the rest of
    the samples that are not part of the dataset. On each iteration, we sample a set
    of points from the current energy model ${\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}})$
    (negative samples) to evaluate the model. However, if the dimension of ${\bm{a}}$
    is large, it may be difficult to sample ${\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}})$
    properly. To mitigate this difficulty, techniques such as using the gradient of
    the energy function  [[99](#bib.bib99)] or minimizing the KL divergence between
    samples  [[104](#bib.bib104)] are common approaches.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在[公式 9](#S3.E9 "在 III-B2 训练基于能量的模型 ‣ III-B 基于能量的模型。 ‣ III 密度估计模型 ‣ 机器人学中的深度生成模型：关于多模态演示学习的综述")中，数据集中的样本（正样本）的能量被压低，而数据集中未包含的其他样本的能量被压高。在每次迭代中，我们从当前的能量模型${\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}})$（负样本）中采样一组点以评估模型。然而，如果${\bm{a}}$的维度较大，可能难以正确采样${\bm{a}}\sim\rho_{{\bm{\theta}}}({\bm{a}})$。为了缓解这一困难，常用的技术包括使用能量函数的梯度[[99](#bib.bib99)]或最小化样本之间的KL散度[[104](#bib.bib104)]。
- en: '![Refer to caption](img/18d1264f2cafb6b3b6f832f2c8e90d99.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/18d1264f2cafb6b3b6f832f2c8e90d99.png)'
- en: 'Figure 4: Left: A visual representation of a [DM](#glo.main.dm). Given an action
    ${\bm{a}}$ and a scalar $k$ informing on the diffusion step, the model ${\bm{s}}={\bm{S}}_{{\bm{\theta}}}({\bm{a}},{\bm{c}},k)$
    outputs a vector ${\bm{s}}$ conditioned on ${\bm{c}}$. The output ${\bm{s}}$ is
    related to the score of a distribution $\rho({\bm{a}}_{k})$. Right: A visualization
    of the denoising process [[36](#bib.bib36)].'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：左：一个[DM](#glo.main.dm)的视觉表示。给定一个动作${\bm{a}}$和一个标量$k$，用于说明扩散步骤，模型${\bm{s}}={\bm{S}}_{{\bm{\theta}}}({\bm{a}},{\bm{c}},k)$输出一个条件于${\bm{c}}$的向量${\bm{s}}$。输出的${\bm{s}}$与分布$\rho({\bm{a}}_{k})$的评分有关。右：去噪过程的可视化[[36](#bib.bib36)]。
- en: Supervised Learning. A few works in robotics [[92](#bib.bib92), [105](#bib.bib105)]
    have explored training [SDF](#glo.main.sdf), occupancy fields, or binary classifiers
    to represent scalar fields. Although not explicitly trained as a generative model,
    the learned model is architecturally equivalent to an [EBM](#glo.main.ebm). The
    model takes an action variable ${\bm{a}}$ as input and outputs a scalar value
    ${\bm{e}}$ that informs about the quality of the sample. After the training, this
    model can be used to generate samples. For example, in [[92](#bib.bib92)] a [SDF](#glo.main.sdf)
    model is trained to generate 6 DoF grasp poses, while in [[105](#bib.bib105)]
    an occupancy network is trained to generate grasp poses.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习。一些机器人学领域的研究[[92](#bib.bib92), [105](#bib.bib105)]探讨了训练[SDF](#glo.main.sdf)、占用场或二分类器来表示标量场。虽然这些模型并未被明确训练为生成模型，但学习到的模型在结构上等价于[EBM](#glo.main.ebm)。该模型将一个动作变量${\bm{a}}$作为输入，输出一个标量值${\bm{e}}$，该值提供关于样本质量的信息。训练完成后，该模型可以用来生成样本。例如，在[[92](#bib.bib92)]中，训练了一个[SDF](#glo.main.sdf)模型以生成6自由度抓取姿势，而在[[105](#bib.bib105)]中，训练了一个占用网络以生成抓取姿势。
- en: Neural Descriptor Fields. A set of works [[70](#bib.bib70), [106](#bib.bib106)]
    represent an [EBM](#glo.main.ebm) as the Euclidean distance to a target action
    ${\bm{a}}^{*}$ in a learned latent space $E({\bm{a}}|{\bm{a}}^{*})=||{\bm{\phi}}_{{\bm{\theta}}}({\bm{a}}^{*})-{\bm{\phi}}_{{\bm{\theta}}}({\bm{a}})||$,
    where ${\bm{z}}={\bm{\phi}}_{{\bm{\theta}}}({\bm{a}})$, maps an action to a latent
    vector ${\bm{z}}\in\mathbb{R}^{d}$ of dimension $d$. In contrast to learning the
    [EBM](#glo.main.ebm) directly, these methods propose learning a feature encoder
    ${\bm{\phi}}_{{\bm{\theta}}}$, which computes a latent vector for a given input
    ${\bm{a}}$. In [[70](#bib.bib70), [106](#bib.bib106)], the feature encoder is
    trained by reconstructing the [SDF](#glo.main.sdf) of an object. The feature encoder
    is conditioned on the pointcloud of the object ${\bm{\phi}}_{{\bm{\theta}}}({\bm{a}},{\bm{c}})$,
    where ${\bm{c}}$ is the pointcloud. In [[107](#bib.bib107)], the CLIP [[108](#bib.bib108)]
    features are used to learn the feature encoder.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 神经描述符场。一组工作 [[70](#bib.bib70), [106](#bib.bib106)] 将 [EBM](#glo.main.ebm) 表示为在学习的潜在空间中到目标动作
    ${\bm{a}}^{*}$ 的欧几里得距离 $E({\bm{a}}|{\bm{a}}^{*})=||{\bm{\phi}}_{{\bm{\theta}}}({\bm{a}}^{*})-{\bm{\phi}}_{{\bm{\theta}}}({\bm{a}})||$，其中
    ${\bm{z}}={\bm{\phi}}_{{\bm{\theta}}}({\bm{a}})$ 将一个动作映射到一个维度为 $d$ 的潜在向量 ${\bm{z}}\in\mathbb{R}^{d}$。与直接学习
    [EBM](#glo.main.ebm) 相对，这些方法提出了学习特征编码器 ${\bm{\phi}}_{{\bm{\theta}}}$，该编码器为给定的输入
    ${\bm{a}}$ 计算潜在向量。在 [[70](#bib.bib70), [106](#bib.bib106)] 中，特征编码器通过重建对象的 [SDF](#glo.main.sdf)
    进行训练。特征编码器以对象的点云 ${\bm{\phi}}_{{\bm{\theta}}}({\bm{a}},{\bm{c}})$ 为条件，其中 ${\bm{c}}$
    是点云。在 [[107](#bib.bib107)] 中，CLIP [[108](#bib.bib108)] 特征被用于学习特征编码器。
- en: III-C Diffusion Models
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 扩散模型
- en: '[DM](#glo.main.dm) [[109](#bib.bib109), [33](#bib.bib33)] frame the data generation
    process as an iterative denoising process. Given a prior sampling distribution
    ${\bm{a}}_{N}\sim\rho({\bm{a}}_{N})$, typically a Gaussian distribution, an iterative
    denoising process $\rho_{{\bm{\theta}}}({\bm{a}}_{k-1}|{\bm{a}}_{k})$ moves the
    noisy samples from the prior to the data distribution'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[DM](#glo.main.dm) [[109](#bib.bib109), [33](#bib.bib33)] 将数据生成过程框定为迭代去噪过程。给定先验采样分布
    ${\bm{a}}_{N}\sim\rho({\bm{a}}_{N})$，通常为高斯分布，迭代去噪过程 $\rho_{{\bm{\theta}}}({\bm{a}}_{k-1}|{\bm{a}}_{k})$
    将噪声样本从先验分布移动到数据分布。'
- en: '|  | $\displaystyle\rho({\bm{a}}_{0})=\int\rho({\bm{a}}_{N})\prod_{k=1}^{N}\rho_{{\bm{\theta}}}({\bm{a}}_{k-1}&#124;{\bm{a}}_{k}){\textrm{d}}{\bm{a}}_{1:N},$
    |  | (10) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\rho({\bm{a}}_{0})=\int\rho({\bm{a}}_{N})\prod_{k=1}^{N}\rho_{{\bm{\theta}}}({\bm{a}}_{k-1}&#124;{\bm{a}}_{k}){\textrm{d}}{\bm{a}}_{1:N},$
    |  | (10) |'
- en: where $\rho({\bm{a}}_{0})\equiv\rho_{{\mathcal{D}}}({\bm{a}}_{0})$ is equivalent
    to the data distribution. The denoising process $\rho_{{\bm{\theta}}}({\bm{a}}_{k-1}|{\bm{a}}_{k})$
    is the inverse of a forward diffusion process $q({\bm{a}}_{k+1}|{\bm{a}}_{k})$
    that gradually adds noise to the dataset samples.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\rho({\bm{a}}_{0})\equiv\rho_{{\mathcal{D}}}({\bm{a}}_{0})$ 等价于数据分布。去噪过程
    $\rho_{{\bm{\theta}}}({\bm{a}}_{k-1}|{\bm{a}}_{k})$ 是一个前向扩散过程 $q({\bm{a}}_{k+1}|{\bm{a}}_{k})$
    的逆过程，该过程逐渐向数据集样本添加噪声。
- en: In practice, [DM](#glo.main.dm) are closely related to [EBM](#glo.main.ebm),
    where the denoising prediction estimates the gradient field of an energy function
     [[110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112)]. Given as input
    an action ${\bm{a}}$, outputs a vector ${\bm{s}}$, ${\bm{s}}={\bm{S}}_{{\bm{\theta}}}({\bm{a}},{\bm{c}},k)$,
    where ${\bm{c}}$ denotes the context variable and $k$ is a scalar value informing
    about the diffusion step, each step of the diffusion process can be seen as a
    step of Langevin dynamics sampling using an [EBM](#glo.main.ebm). Due to the iterative
    sampling process, [DM](#glo.main.dm) have slower inference times compared to other
    [DGM](#glo.main.dgm). Recent research, such as Consistency Policies [[113](#bib.bib113)],
    explores how to make [DM](#glo.main.dm) sampling faster.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，[DM](#glo.main.dm) 与 [EBM](#glo.main.ebm) 密切相关，其中去噪预测估计能量函数的梯度场 [[110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112)]。输入一个动作 ${\bm{a}}$，输出一个向量 ${\bm{s}}$，${\bm{s}}={\bm{S}}_{{\bm{\theta}}}({\bm{a}},{\bm{c}},k)$，其中
    ${\bm{c}}$ 表示上下文变量，$k$ 是一个标量值，指示扩散步骤的每一步都可以看作是使用 [EBM](#glo.main.ebm) 的朗之万动力学采样步骤。由于迭代采样过程，[DM](#glo.main.dm)
    的推理时间比其他 [DGM](#glo.main.dgm) 更慢。最近的研究，如一致性策略 [[113](#bib.bib113)]，探讨了如何加速 [DM](#glo.main.dm)
    采样。
- en: '[DM](#glo.main.dm) have become particularly popular in the last year because
    of several important properties for generative modeling. [DM](#glo.main.dm) are
    capable of representing high-dimensional continuous space distributions and have
    a stable training pipeline. Due to its connection to [EBM](#glo.main.ebm), [DM](#glo.main.dm)
    implicitly parameterizes the actions, allowing the composition of diffusion models.
    Several works have explored the modular composition of [DM](#glo.main.dm) with
    additional objectives [[36](#bib.bib36), [58](#bib.bib58), [114](#bib.bib114),
    [115](#bib.bib115)] or other [DM](#glo.main.dm) [[116](#bib.bib116), [117](#bib.bib117)].
    Although not applied to robotics, composition is particularly popular in image
    generation. Classifier Guidance [[118](#bib.bib118)] proposes to combine the output
    of an unconditional [DM](#glo.main.dm) with the gradient of a classifier in the
    generation process. Classifier-free guidance [[119](#bib.bib119)] proposes instead
    to combine an unconditional [DM](#glo.main.dm) with a conditioned [DM](#glo.main.dm).
    In [[111](#bib.bib111), [120](#bib.bib120)], several conditioned [DM](#glo.main.dm)
    were combined for modular generation.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[DM](#glo.main.dm) 在过去一年中特别受欢迎，因为它具有若干重要的生成建模属性。[DM](#glo.main.dm) 能够表示高维连续空间分布，并且具有稳定的训练流程。由于其与[EBM](#glo.main.ebm)的联系，[DM](#glo.main.dm)
    隐式地参数化了动作，从而允许扩散模型的组合。若干工作探索了[DM](#glo.main.dm)的模块化组合与附加目标[[36](#bib.bib36)、[58](#bib.bib58)、[114](#bib.bib114)、[115](#bib.bib115)]或其他[DM](#glo.main.dm)[[116](#bib.bib116)、[117](#bib.bib117)]。尽管尚未应用于机器人领域，组合在图像生成中尤其受欢迎。分类器引导[[118](#bib.bib118)]
    提出了将无条件[DM](#glo.main.dm)的输出与分类器的梯度结合在生成过程中。无分类器引导[[119](#bib.bib119)]则提议将无条件[DM](#glo.main.dm)与有条件[DM](#glo.main.dm)结合。在[[111](#bib.bib111)、[120](#bib.bib120)]中，多个有条件[DM](#glo.main.dm)被组合用于模块化生成。'
- en: III-C1 Main Applications
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 主要应用
- en: 'Due to their high expressiveness and flexibility, [DM](#glo.main.dm) have been
    widely integrated into many robotics tasks. We decouple the applications into
    three main clusters: papers that aim to directly learn robot trajectories, papers
    that explore the modularity and composability of [DM](#glo.main.dm), and papers
    that generate other types of variables, beyond trajectories.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其高表达性和灵活性，[DM](#glo.main.dm) 已被广泛集成到许多机器人任务中。我们将应用分解为三个主要类别：直接学习机器人轨迹的论文、探讨[DM](#glo.main.dm)的模块化和可组合性的论文，以及生成其他类型变量（超越轨迹）的论文。
- en: Trajectory Generation. The generation of robot trajectories is an essential
    element for solving any robot task. Traditionally, these trajectory generators
    have been represented with simpler models, such as policies that generate the
    trajectories autoregressively, or structured models, such as task and motion planning
    algorithms. However, the expressiveness of [DM](#glo.main.dm) has allowed the
    robotics community to directly generate the trajectories without the need for
    these models. In [[36](#bib.bib36), [114](#bib.bib114)], [Denoising Diffusion
    Probabilistic Models (DDPM)](#glo.main.ddpm) was used to learn trajectory generators
    from demonstrations. The generation of the robot trajectories is similar to a
    receding horizon control loop [[121](#bib.bib121)], which allows reactive generation.
    In [[8](#bib.bib8), [7](#bib.bib7)] the trajectory [DM](#glo.main.dm) was introduced
    conditioned on visual input and in [[64](#bib.bib64), [122](#bib.bib122)] it was
    conditioned on both language and vision. [[115](#bib.bib115), [123](#bib.bib123)]
    explored the integration of [DM](#glo.main.dm) for motion planning problems, both
    for collision-free trajectory generation and navigation tasks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹生成。生成机器人轨迹是解决任何机器人任务的核心要素。传统上，这些轨迹生成器使用较简单的模型，如自回归生成轨迹的策略，或结构化模型，如任务和运动规划算法。然而，[DM](#glo.main.dm)的表达能力使机器人社区能够直接生成轨迹，而无需这些模型。在[[36](#bib.bib36)、[114](#bib.bib114)]中，[去噪扩散概率模型（DDPM）](#glo.main.ddpm)被用于从演示中学习轨迹生成器。机器人轨迹的生成类似于递归视界控制循环[[121](#bib.bib121)]，这允许反应生成。在[[8](#bib.bib8)、[7](#bib.bib7)]中，轨迹[DM](#glo.main.dm)
    在视觉输入条件下引入，而在[[64](#bib.bib64)、[122](#bib.bib122)]中，则在语言和视觉条件下引入。[[115](#bib.bib115)、[123](#bib.bib123)]
    探索了[DM](#glo.main.dm)在运动规划问题中的集成，包括无碰撞轨迹生成和导航任务。
- en: Generation beyond trajectories. Besides trajectories, [DM](#glo.main.dm) have
    been used to generate several types of data for robotics tasks. Several works [[58](#bib.bib58),
    [10](#bib.bib10), [124](#bib.bib124)] have explored using [DM](#glo.main.dm) to
    generate SE(3) poses, both for generating robot grasp poses [[58](#bib.bib58)]
    or object placement poses [[10](#bib.bib10), [116](#bib.bib116)] for pick and
    place tasks. Some works [[9](#bib.bib9), [125](#bib.bib125), [117](#bib.bib117)]
    have used [DM](#glo.main.dm) to generate scene arrangements. This information
    is used to define a high-level goal plan for a motion planner to rearrange a scene.
    Some works [[126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128)] have explored
    learning video [DM](#glo.main.dm), which are used to have a general representation
    plan of the desired behavior. Then an inverse dynamics model generates actions
    in the robot to match the video behavior. While [[129](#bib.bib129)], [DM](#glo.main.dm)
    were used to generate realistic tactile images from images; in [[130](#bib.bib130)]
    movement primitive weights are generated.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 超越轨迹的生成。除了轨迹，[DM](#glo.main.dm) 还被用于生成各种类型的机器人任务数据。几项工作 [[58](#bib.bib58), [10](#bib.bib10),
    [124](#bib.bib124)] 探索了使用 [DM](#glo.main.dm) 生成 SE(3) 姿态，无论是生成机器人抓取姿态 [[58](#bib.bib58)]
    还是对象放置姿态 [[10](#bib.bib10), [116](#glo.main.dm)] 用于抓取和放置任务。一些工作 [[9](#bib.bib9),
    [125](#bib.bib125), [117](#bib.bib117)] 使用 [DM](#glo.main.dm) 生成场景安排。这些信息用于定义运动规划器的高级目标计划，以重新安排场景。一些工作
    [[126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128)] 探索了学习视频 [DM](#glo.main.dm)，用于对所需行为进行一般表示计划。然后，逆动力学模型在机器人中生成与视频行为匹配的动作。而
    [[129](#bib.bib129)] 中，[DM](#glo.main.dm) 被用于从图像中生成逼真的触觉图像；在 [[130](#bib.bib130)]
    中生成了运动原语权重。
- en: Modular Composition. Several of the papers cited above have explored modular
    composition of [DM](#glo.main.dm) in various forms. In [[36](#bib.bib36), [114](#bib.bib114)],
    the learned [DM](#glo.main.dm) is combined with a reward function to condition
    the generation towards high-reward regions. In [[115](#bib.bib115)], a trajectory
    [DM](#glo.main.dm) is combined with the [SDF](#glo.main.sdf) of the scene to perform
    collision free trajectory generation. In [[58](#bib.bib58)], a generative grasping
    model was integrated into a motion planning problem to generate trajectories for
    pick and place tasks. [[117](#bib.bib117)] composes multiple object relations
    to generate scene arrangements with local models. In [[124](#bib.bib124)], the
    placement generative model is combined with a feasibility score to adapt to feasible
    placement poses. In [[116](#bib.bib116)], instead, a set of [DM](#glo.main.dm)
    are temporally composed to solve long horizon planning tasks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 模块化组合。上述引用的几篇论文探讨了 [DM](#glo.main.dm) 在各种形式下的模块化组合。在 [[36](#bib.bib36), [114](#bib.bib114)]
    中，学习到的 [DM](#glo.main.dm) 与奖励函数结合，以使生成过程朝向高奖励区域。在 [[115](#bib.bib115)] 中，一个轨迹
    [DM](#glo.main.dm) 与场景的 [SDF](#glo.main.sdf) 结合，以进行无碰撞轨迹生成。在 [[58](#bib.bib58)]
    中，将生成抓取模型集成到运动规划问题中，以生成用于抓取和放置任务的轨迹。在 [[117](#bib.bib117)] 中，多个对象关系被组合生成具有局部模型的场景安排。在
    [[124](#bib.bib124)] 中，放置生成模型与可行性评分结合，以适应可行的放置姿态。在 [[116](#bib.bib116)] 中，一组 [DM](#glo.main.dm)
    被时间上组合以解决长时间范围的规划任务。
- en: III-C2 Training Diffusion Models
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 训练扩散模型
- en: An interesting property of [DM](#glo.main.dm) is their stable training process.
    Unlike [CD](#glo.main.cd) for training [EBM](#glo.main.ebm), [DM](#glo.main.dm)
    are learned with stable target signals, which leads to more robust training, but
    can still be seen as a parameterization of an implicit energy landscape. Below
    we describe the training of the diffusion models and their connection to [EBM](#glo.main.ebm).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[DM](#glo.main.dm) 的一个有趣特性是其稳定的训练过程。与用于训练 [EBM](#glo.main.ebm) 的 [CD](#glo.main.cd)
    不同，[DM](#glo.main.dm) 是通过稳定的目标信号进行学习的，这导致了更强的鲁棒性，但仍然可以看作是隐式能量景观的一种参数化。下面我们描述了扩散模型的训练及其与
    [EBM](#glo.main.ebm) 的联系。'
- en: To train a diffusion model, we learn to model the inverse transition function
    $\rho_{{\bm{\theta}}}({\bm{a}}_{k-1}|{\bm{a}}_{k})$ at each time step $k$. This
    transition kernel is parameterized by a Gaussian distribution corresponding to
    the sampling process
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练扩散模型，我们在每个时间步 $k$ 学习建模逆过渡函数 $\rho_{{\bm{\theta}}}({\bm{a}}_{k-1}|{\bm{a}}_{k})$。这个过渡核由与采样过程对应的高斯分布参数化。
- en: '|  | ${\bm{a}}_{k-1}=B_{k}({\bm{a}}_{k}-C_{k}{\bm{S}}_{{\bm{\theta}}}({\bm{a}}_{k},k)+D_{k}\mathbf{\xi}),\quad\mathbf{\xi}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$
    |  | (11) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{a}}_{k-1}=B_{k}({\bm{a}}_{k}-C_{k}{\bm{S}}_{{\bm{\theta}}}({\bm{a}}_{k},k)+D_{k}\mathbf{\xi}),\quad\mathbf{\xi}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$
    |  | (11) |'
- en: where $B_{k}$, $C_{k}$, and $D_{k}$ are fixed (unlearned) constants in the diffusion
    process. To learn the above transition distributions, we only need to learn and
    model the score function ${\bm{S}}_{{\bm{\theta}}}({\bm{a}},k)$.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$B_{k}$、$C_{k}$和$D_{k}$是扩散过程中固定（未学习的）常数。为了学习上述过渡分布，我们只需学习和建模得分函数${\bm{S}}_{{\bm{\theta}}}({\bm{a}},k)$。
- en: The score function ${\bm{S}}_{{\bm{\theta}}}({\bm{a}}_{k},k)$ corresponds to
    the gradient of an [EBM](#glo.main.ebm) ${\bm{S}}_{{\bm{\theta}}}({\bm{a}},k)=\nabla_{{\bm{a}}}E_{\theta}({\bm{a}},k)$,
    where the [EBM](#glo.main.ebm) models the noise convolved action distribution
    $p_{k}({\bm{a}})\;\propto\;e^{-E_{\theta}({\bm{a}},k)}$, where
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 得分函数${\bm{S}}_{{\bm{\theta}}}({\bm{a}}_{k},k)$对应于[EBM](#glo.main.ebm) ${\bm{S}}_{{\bm{\theta}}}({\bm{a}},k)=\nabla_{{\bm{a}}}E_{\theta}({\bm{a}},k)$的梯度，其中[EBM](#glo.main.ebm)建模了噪声卷积动作分布$p_{k}({\bm{a}})\;\propto\;e^{-E_{\theta}({\bm{a}},k)}$，其中
- en: '|  | $p_{k}({\bm{a}})=\int_{{\bm{a}}^{*}}p({\bm{a}}^{*})\cdot\mathcal{N}({\bm{a}};\sqrt{1-\sigma_{k}^{2}}{\bm{a}}^{*},\sigma^{2}_{k}\mathbf{I}).$
    |  | (12) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{k}({\bm{a}})=\int_{{\bm{a}}^{*}}p({\bm{a}}^{*})\cdot\mathcal{N}({\bm{a}};\sqrt{1-\sigma_{k}^{2}}{\bm{a}}^{*},\sigma^{2}_{k}\mathbf{I}).$
    |  | (12) |'
- en: We can directly learn this [EBM](#glo.main.ebm) $E_{\theta}({\bm{a}},k)$ implicitly
    by directly training the ${\bm{S}}_{{\bm{\theta}}}({\bm{a}},k)$ to denoise actions
    through denoising score matching  [[110](#bib.bib110)].
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过直接训练${\bm{S}}_{{\bm{\theta}}}({\bm{a}},k)$来隐式地学习这个[EBM](#glo.main.ebm)
    $E_{\theta}({\bm{a}},k)$，以通过去噪得分匹配来去噪动作[[110](#bib.bib110)]。
- en: '|  | $\mathcal{L}_{\text{MSE}}(\theta)=\&#124;{\bm{S}}_{{\bm{\theta}}}({\bm{a}}_{k}+\epsilon,k)-\epsilon\&#124;^{2},\quad\epsilon\sim\mathcal{N}(0,1).$
    |  | (13) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{MSE}}(\theta)=\&#124;{\bm{S}}_{{\bm{\theta}}}({\bm{a}}_{k}+\epsilon,k)-\epsilon\&#124;^{2},\quad\epsilon\sim\mathcal{N}(0,1).$
    |  | (13) |'
- en: In comparison to methods for training [EBM](#glo.main.ebm), this objective to
    learn the score is both faster and more stable.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练[EBM](#glo.main.ebm)的方法相比，这个学习得分的目标更快且更稳定。
- en: 'By learning this score function ${\bm{S}}_{{\bm{\theta}}}({\bm{a}}_{k},k)$
    and corresponding implicit landscape $E_{\theta}({\bm{a}},k)$ in a diffusion model,
    each reverse transition kernel in the diffusion process in [Equation 11](#S3.E11
    "In III-C2 Training Diffusion Models ‣ III-C Diffusion Models ‣ III Density Estimation
    Models ‣ Deep Generative Models in Robotics: A Survey on Learning from Multimodal
    Demonstrations") corresponds to Langevin sampling on a sequence of noise convolved
    [EBM](#glo.main.ebm) $E_{\theta}({\bm{a}},k)$, where an added contraction term
    $B_{k}$ is used to transition between separate successive energy landscape. This
    implicit view of sampling in diffusion models allows us to combine multiple diffusion
    models together [[112](#bib.bib112)].'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在扩散模型中学习这个得分函数${\bm{S}}_{{\bm{\theta}}}({\bm{a}}_{k},k)$及其对应的隐式景观$E_{\theta}({\bm{a}},k)$，扩散过程中的每个反向过渡核在[方程11](#S3.E11
    "在III-C2训练扩散模型 ‣ III-C 扩散模型 ‣ III 密度估计模型 ‣ 机器人中的深度生成模型：从多模态示例学习的综述")中对应于在噪声卷积[EBM](#glo.main.ebm)
    $E_{\theta}({\bm{a}},k)$上进行Langevin采样，其中一个附加的收缩项$B_{k}$用于在不同的连续能量景观之间进行过渡。这种在扩散模型中的隐式采样视图使我们能够将多个扩散模型结合在一起[[112](#bib.bib112)]。
- en: III-D Categorical Models
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 分类模型
- en: We refer to Categorical Models as the set of generative models that, given a
    context variable ${\bm{c}}$ as input, output the probability of K different categories,
    where K is finite. In practice, it is common for the network to output K logits.
    Given the logits, a Softmax function converts the logits into probability values
    for each action.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分类模型称为一组生成模型，该模型在给定上下文变量${\bm{c}}$作为输入时，输出K个不同类别的概率，其中K是有限的。在实际应用中，网络通常输出K个logits。给定logits，Softmax函数将logits转换为每个动作的概率值。
- en: '![Refer to caption](img/6135dbb53bdd3434ba0f7ac638aa768b.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6135dbb53bdd3434ba0f7ac638aa768b.png)'
- en: 'Figure 5: A visual representation of Action Value Maps and Autoregressive models.
    Given a visual observation ${\bm{o}}$ as input, Action Value Maps ${\bm{H}}=Q_{{\bm{\theta}}}({\bm{o}})$
    output an observation shape probability map ${\bm{H}}$, where each pixel is a
    projection of a possible action. Autoregressive models output K'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：动作价值图和自回归模型的视觉表示。给定一个视觉观察${\bm{o}}$作为输入，动作价值图${\bm{H}}=Q_{{\bm{\theta}}}({\bm{o}})$输出一个观察形状概率图${\bm{H}}$，其中每个像素都是可能动作的投影。自回归模型输出K。
- en: 'In robotics, categorical models are mainly used to represent policies. There
    are two main types of architectures in which categorical distributions have been
    used: Action Value Maps and Autoregressive Models.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人技术中，分类模型主要用于表示策略。分类分布有两种主要的架构：动作价值图和自回归模型。
- en: Action Value Maps
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 动作价值图
- en: Action Value Maps are a set of generative models that, given a visual observation
    ${\bm{o}}$ as input, output a value map ${\bm{H}}$ of the same form as the input,
    ${\bm{H}}=Q_{{\bm{\theta}}}({\bm{o}})$, similar to attention maps. The visual
    observation ${\bm{o}}$ can be images [[40](#bib.bib40), [131](#bib.bib131)], point
    clouds [[63](#bib.bib63)], or voxels [[132](#bib.bib132), [6](#bib.bib6)]. The
    generated value map ${\bm{H}}$ assigns a probabilistic value to each distinct
    location or ‘pixel’ within the visual observation ${\bm{o}}$.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 动作价值图是一组生成模型，给定一个视觉观察${\bm{o}}$作为输入，输出一个与输入形式相同的价值图${\bm{H}}$，${\bm{H}}=Q_{{\bm{\theta}}}({\bm{o}})，类似于注意力图。视觉观察${\bm{o}}$可以是图像[[40](#bib.bib40),
    [131](#bib.bib131)]、点云[[63](#bib.bib63)]或体素[[132](#bib.bib132), [6](#bib.bib6)]。生成的价值图${\bm{H}}$为视觉观察${\bm{o}}$中的每个独特位置或‘像素’分配一个概率值。
- en: The core principle behind action value maps is the interpretation of each ‘pixel’
    location in the visual input as representative of a potential action that a robot
    could perform. Consider, for example, a robot picking task. In this scenario,
    each individual pixel in an image could represent a potential target point for
    moving the robot arm to perform a pick. The resulting value map is then analyzed
    as a categorical distribution encompassing the full range of potential actions.
    Interestingly, the approach is easily extended to multiple primitives, where each
    pixel represents the geometrically grounded parameters of each primitive. For
    example, in [[73](#bib.bib73)] two value maps are generated, one for the grasping
    primitive and another for the pushing primitive. Then the selected action is the
    one with the highest energy among all possible pixels.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 动作价值图背后的核心原理是将视觉输入中每个‘像素’位置解释为代表机器人可能执行的潜在动作。例如，考虑一个机器人抓取任务。在这种情况下，图像中的每个单独像素可以表示一个潜在的目标点，用于移动机器人手臂进行抓取。然后，分析得到的价值图作为涵盖所有潜在动作的分类分布。有趣的是，这种方法可以很容易地扩展到多个原语，其中每个像素代表每个原语的几何基础参数。例如，在[[73](#bib.bib73)]中生成了两个价值图，一个用于抓取原语，另一个用于推送原语。然后，选择的动作是所有可能像素中能量最高的那个。
- en: A number of works [[133](#bib.bib133), [6](#bib.bib6), [131](#bib.bib131)] have
    extended action value maps to also be conditioned on language commands, defining
    the goal ${\bm{g}}$ of the learned policy $Q_{{\bm{\theta}}}({\bm{o}},{\bm{g}})$.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究[[133](#bib.bib133), [6](#bib.bib6), [131](#bib.bib131)]将动作价值图扩展到也依赖于语言指令，定义了学习到的策略$Q_{{\bm{\theta}}}({\bm{o}},{\bm{g}})$的目标${\bm{g}}$。
- en: Autoregressive Models
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型
- en: Autoregressive models are a set of generative models that generate long-horizon
    data by iteratively invoking the model while conditioning on past data. For example,
    given an action trajectory ${\bm{\tau}}=({\bm{a}}_{0},{\bm{a}}_{1},\dots,{\bm{a}}_{T})$,
    an autoregressive model represents the distribution over the trajectory
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型是一组生成模型，通过迭代调用模型并以过去的数据为条件来生成长时间跨度的数据。例如，给定一个动作轨迹${\bm{\tau}}=({\bm{a}}_{0},{\bm{a}}_{1},\dots,{\bm{a}}_{T})，自回归模型表示轨迹的分布。
- en: '|  | $\displaystyle\rho({\bm{\tau}})=\rho({\bm{a}}_{0})\prod_{i=1}^{T}\rho({\bm{a}}_{i}&#124;{\bm{a}}_{0:i-1}),$
    |  | (14) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\rho({\bm{\tau}})=\rho({\bm{a}}_{0})\prod_{i=1}^{T}\rho({\bm{a}}_{i}\mid{\bm{a}}_{0:i-1}),$
    |  | (14) |'
- en: as the product of the conditioned action distribution on the previous data.
    To generate a trajectory, we first sample an initial sample ${\bm{a}}_{0}\sim\rho({\bm{a}}_{0})$
    and then iteratively call the generative model conditioned on the previously generated
    actions. This type of model has become particularly popular for language generation.
    Models such as BERT [[34](#bib.bib34)] or GPT-3 [[134](#bib.bib134)] build on
    Transformer models [[135](#bib.bib135)] to generate autoregressive long text data.
    Some generative models in image generation are also based on autoregressive generative
    models, where the pixels of the image are generated iteratively [[136](#bib.bib136)].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对先前数据的条件化动作分布的乘积。为了生成一个轨迹，我们首先采样一个初始样本${\bm{a}}_{0}\sim\rho({\bm{a}}_{0})$，然后迭代地调用条件化于先前生成动作的生成模型。这种类型的模型在语言生成中尤其流行。像BERT
    [[34](#bib.bib34)]或GPT-3 [[134](#bib.bib134)]这样的模型基于Transformer模型[[135](#bib.bib135)]生成自回归长文本数据。一些图像生成中的生成模型也基于自回归生成模型，其中图像的像素是迭代生成的[[136](#bib.bib136)]。
- en: In robotics, it has been common to refer to autoregressive models as policies
    that use GPT-like structures to generate robot actions [[137](#bib.bib137), [37](#bib.bib37),
    [72](#bib.bib72), [12](#bib.bib12), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140)].
    Several of these models represent the action distribution (i.e., policy) as a
    categorical distribution and generate action trajectories autoregressively, similar
    to language models. However, in certain cases, the action distribution is represented
    by [DM](#glo.main.dm) [[141](#bib.bib141), [142](#bib.bib142)] or by adapting
    the means of the categorical distribution [[59](#bib.bib59), [143](#bib.bib143),
    [144](#bib.bib144)].
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人领域，通常将自回归模型称为使用类似GPT结构生成机器人动作的策略[[137](#bib.bib137)、[37](#bib.bib37)、[72](#bib.bib72)、[12](#bib.bib12)、[138](#bib.bib138)、[139](#bib.bib139)、[140](#bib.bib140)]。这些模型中的一些将动作分布（即策略）表示为分类分布，并以自回归方式生成动作轨迹，类似于语言模型。然而，在某些情况下，动作分布由[DM](#glo.main.dm)表示[[141](#bib.bib141)、[142](#bib.bib142)]，或通过调整分类分布的均值表示[[59](#bib.bib59)、[143](#bib.bib143)、[144](#bib.bib144)]。
- en: III-D1 Training Categorical Model
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-D1 训练分类模型
- en: 'To fit a Categorical distribution to a dataset, two factors are important:
    finding the right prediction space and the right training loss.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将分类分布拟合到数据集中，有两个重要因素：找到正确的预测空间和正确的训练损失。
- en: Discrete Tokenization in Action Modeling. Frequently, robotic actions are continuous
    while categorical distributions predict distribution over discrete values. As
    a result, models with categorical distributions require “tokenizing” the continuous
    actions into discrete “action tokens”. Often, this is done with an ad-hoc tokenizer
    binning every axis values into a certain number of bins, such as in [[140](#bib.bib140),
    [37](#bib.bib37), [72](#bib.bib72)]. Some models use a non-parametric algorithms
    such as k-means to tokenize the actions [[59](#bib.bib59), [143](#bib.bib143),
    [145](#bib.bib145)]. Later models [[144](#bib.bib144)] use more advanced generative
    methods, such as a VQ-VAE [[146](#bib.bib146)], to tokenize the actions into discrete
    tokens.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 动作建模中的离散化。在动作建模中，机器人动作通常是连续的，而分类分布预测离散值的分布。因此，具有分类分布的模型需要将连续动作“离散化”成离散的“动作标记”。通常，这通过一个特定的分箱器将每个轴的值分成一定数量的箱子来完成，例如在[[140](#bib.bib140)、[37](#bib.bib37)、[72](#bib.bib72)]中。一些模型使用如k-means这样的非参数算法来离散化动作[[59](#bib.bib59)、[143](#bib.bib143)、[145](#bib.bib145)]。后来的一些模型[[144](#bib.bib144)]使用更先进的生成方法，如VQ-VAE[[146](#bib.bib146)]，将动作离散化为离散的标记。
- en: Losses for Training. Quite frequently, Categorical models are trained with the
    [Cross-Entropy (CE)](#glo.main.ce) loss. Given a dataset ${\mathcal{D}}:({\bm{a}}_{i},{\bm{c}}_{i})_{i=0}^{N}$,
    the [CE](#glo.main.ce) loss
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失。相当多的情况下，分类模型通过[交叉熵（CE）](#glo.main.ce)损失进行训练。给定一个数据集${\mathcal{D}}:({\bm{a}}_{i},{\bm{c}}_{i})_{i=0}^{N}$，[CE](#glo.main.ce)损失
- en: '|  | $\displaystyle{\mathcal{L}}({\bm{\theta}})=-{\mathbb{E}}_{j,{\bm{c}}}\left[\log{\bm{C}}_{{\bm{\theta}}}({\bm{c}})_{j}\right],$
    |  | (15) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{L}}({\bm{\theta}})=-{\mathbb{E}}_{j,{\bm{c}}}\left[\log{\bm{C}}_{{\bm{\theta}}}({\bm{c}})_{j}\right],$
    |  | (15) |'
- en: is represented as the negative log-likelihood, where $j$ is class related the
    action in the dataset. However, given the imbalance between different classes,
    some recent models [[59](#bib.bib59), [143](#bib.bib143), [144](#bib.bib144)]
    use Focal loss [[147](#bib.bib147)] instead
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表示为负对数似然，其中 $j$ 是数据集中与动作相关的类别。然而，考虑到不同类别之间的不平衡，一些近期模型[[59](#bib.bib59)、[143](#bib.bib143)、[144](#bib.bib144)]使用Focal
    loss[[147](#bib.bib147)]。
- en: '|  | $\displaystyle{\mathcal{L}}({\bm{\theta}})=-{\mathbb{E}}_{j,{\bm{c}}}\left[(1-{\bm{C}}_{{\bm{\theta}}}({\bm{c}})_{j})^{\gamma}\log{\bm{C}}_{{\bm{\theta}}}({\bm{c}})_{j}\right],$
    |  | (16) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{L}}({\bm{\theta}})=-{\mathbb{E}}_{j,{\bm{c}}}\left[(1-{\bm{C}}_{{\bm{\theta}}}({\bm{c}})_{j})^{\gamma}\log{\bm{C}}_{{\bm{\theta}}}({\bm{c}})_{j}\right],$
    |  | (16) |'
- en: where $\gamma$ is a balancing hyperparameter. Focal loss is less sensitive for
    outliers and class imbalance, which can be important for tokenized action datasets.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\gamma$ 是一个平衡超参数。Focal loss 对于离群点和类别不平衡的敏感性较低，这对离散化的动作数据集可能很重要。
- en: On Action Value Maps. In the particular case of Action Value Maps, the action
    classes are projected to the pixel space of the input observation. The actions
    in the dataset ${\bm{a}}_{j}$ are first projected to a one-hot pixel map $H_{j}$
    of the same shape of the visual observation ${\bm{o}}_{j}$. The one-hot pixel
    map $H_{j}$ will set to one the pixel that relates to the action and zero otherwise.
    Then, the [CE](#glo.main.ce) loss is represented as
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 关于动作值映射。在动作值映射的特定情况下，动作类别被投影到输入观察的像素空间。数据集中的动作 ${\bm{a}}_{j}$ 首先被投影到与视觉观察 ${\bm{o}}_{j}$
    形状相同的一个热编码像素图 $H_{j}$。热编码像素图 $H_{j}$ 会将与动作相关的像素设为一，其余像素设为零。然后，[CE](#glo.main.ce)
    损失表示为
- en: '|  | $\displaystyle{\mathcal{L}}({\bm{\theta}})=-{\mathbb{E}}_{h,{\bm{o}}\in{\mathcal{D}}}[\log
    Q_{{\bm{\theta}}}(h&#124;{\bm{o}})],$ |  | (17) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{L}}({\bm{\theta}})=-{\mathbb{E}}_{h,{\bm{o}}\in{\mathcal{D}}}[\log
    Q_{{\bm{\theta}}}(h&#124;{\bm{o}})],$ |  | (17) |'
- en: where $h$ denotes the positive pixel in the pixel map $H$. Despite using only
    the positive samples, the Softmax propagates the gradients to all the pixel-space,
    pushing the probability down to any pixel not being the positive one.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h$ 表示像素图 $H$ 中的正像素。尽管只使用正样本，Softmax 仍将梯度传播到所有像素空间，将概率压低到任何不是正像素的像素上。
- en: '![Refer to caption](img/ceda7d85faf7d2386f2ee8ff92f055cb.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ceda7d85faf7d2386f2ee8ff92f055cb.png)'
- en: 'Figure 6: A visual representation of [MDM](#glo.main.mdm). Given the context
    ${\bm{c}}$ as input, [MDM](#glo.main.mdm) $H,\Omega={\bm{M}}_{{\bm{\theta}}}({\bm{c}})$
    outputs the parameters of a mixture density model, with $H:({\bm{\mu}}_{0},{\bm{\sigma}}_{0}\dots,{\bm{\mu}}_{N},{\bm{\sigma}}_{N})$
    the parameters of the models and $\Omega:(\omega_{0},\dots,\omega_{N})$ the weights
    for each model in the mixture.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: [MDM](#glo.main.mdm) 的视觉表示。给定输入上下文 ${\bm{c}}$，[MDM](#glo.main.mdm) $H,\Omega={\bm{M}}_{{\bm{\theta}}}({\bm{c}})$
    输出混合密度模型的参数，其中 $H:({\bm{\mu}}_{0},{\bm{\sigma}}_{0}\dots,{\bm{\mu}}_{N},{\bm{\sigma}}_{N})$
    为模型参数，$\Omega:(\omega_{0},\dots,\omega_{N})$ 为混合中每个模型的权重。'
- en: III-E Mixture Density Models
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 混合密度模型
- en: We call [MDM](#glo.main.mdm) [[148](#bib.bib148), [149](#bib.bib149)] to the
    set of generative models $H,\Omega={\bm{M}}_{{\bm{\theta}}}({\bm{c}})$ that take
    as input a context variable ${\bm{c}}$ and output the parameters $H:({\bm{\eta}}_{0},\dots,{\bm{\eta}}_{N})$
    and the weights $\omega:(\omega_{0},\dots,\omega_{N})$ of a mixture density function.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 [MDM](#glo.main.mdm) [[148](#bib.bib148), [149](#bib.bib149)] 称为生成模型集合 $H,\Omega={\bm{M}}_{{\bm{\theta}}}({\bm{c}})$，该模型以上下文变量
    ${\bm{c}}$ 为输入，输出混合密度函数的参数 $H:({\bm{\eta}}_{0},\dots,{\bm{\eta}}_{N})$ 和权重 $\omega:(\omega_{0},\dots,\omega_{N})$。
- en: '|  | $\displaystyle\rho_{{\bm{\theta}}}({\bm{a}}&#124;{\bm{c}})=\sum_{k=0}^{N}\omega_{k}\rho({\bm{a}};{\bm{\eta}}_{k}),$
    |  | (18) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\rho_{{\bm{\theta}}}({\bm{a}}&#124;{\bm{c}})=\sum_{k=0}^{N}\omega_{k}\rho({\bm{a}};{\bm{\eta}}_{k}),$
    |  | (18) |'
- en: where each vector ${\bm{\eta}}_{k}$ and scalar value $\omega_{k}>0$ represent
    the parameters and weight of the $k$ density model. In practice, it is common
    to use [GMM](#glo.main.gmm) [[66](#bib.bib66), [44](#bib.bib44), [150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152)] or Mixture of Logistic Models [[149](#bib.bib149),
    [84](#bib.bib84), [153](#bib.bib153)] to represent the mixture model. Then the
    parameters ${\bm{\eta}}_{k}=({\bm{\mu}}_{k},{\bm{\sigma}}_{k})$ typically represent
    the mean ${\bm{\mu}}_{k}$, the standard deviation ${\bm{\sigma}}_{k}$, and $\omega_{k}$,
    the weight of each mode in the mixture model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中每个向量 ${\bm{\eta}}_{k}$ 和标量值 $\omega_{k}>0$ 表示第 $k$ 个密度模型的参数和权重。在实践中，常用 [GMM](#glo.main.gmm)
    [[66](#bib.bib66), [44](#bib.bib44), [150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152)]
    或逻辑模型的混合 [[149](#bib.bib149), [84](#bib.bib84), [153](#bib.bib153)] 来表示混合模型。然后，参数
    ${\bm{\eta}}_{k}=({\bm{\mu}}_{k},{\bm{\sigma}}_{k})$ 通常表示均值 ${\bm{\mu}}_{k}$、标准差
    ${\bm{\sigma}}_{k}$ 和 $\omega_{k}$，即每个模式在混合模型中的权重。
- en: In the robotics literature, it is common to combine [MDM](#glo.main.mdm) with
    additional generative models. In [[84](#bib.bib84), [44](#bib.bib44), [153](#bib.bib153)],
    [VAE](#glo.main.vae) is combined with [MDM](#glo.main.mdm). Given a latent variable
    ${\bm{z}}$ representing the high-level plan, a [VAE](#glo.main.vae) decoder is
    trained to generate the parameters of a [MDM](#glo.main.mdm) for the action space
    distribution. This differs from the classical use of [VAE](#glo.main.vae), where
    the output directly generates the action.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人学文献中，常常将 [MDM](#glo.main.mdm) 与额外的生成模型结合。在 [[84](#bib.bib84), [44](#bib.bib44),
    [153](#bib.bib153)] 中，[VAE](#glo.main.vae) 与 [MDM](#glo.main.mdm) 结合使用。给定一个表示高层次计划的潜在变量
    ${\bm{z}}$，训练一个 [VAE](#glo.main.vae) 解码器来生成动作空间分布的 [MDM](#glo.main.mdm) 参数。这与经典的
    [VAE](#glo.main.vae) 使用方法不同，后者的输出直接生成动作。
- en: III-E1 Main Application
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-E1 主要应用
- en: A few works [[44](#bib.bib44), [150](#bib.bib150), [151](#bib.bib151), [153](#bib.bib153),
    [84](#bib.bib84)] proposed representing visuomotor policies with [MDM](#glo.main.mdm),
    usually representing the action space as displacements in the end-effector. Several
    of these works motivate the use of a mixture density model rather than an unimodal
    model in the representation of the policy to better capture the inherent multimodality
    in the demonstrations.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究 [[44](#bib.bib44), [150](#bib.bib150), [151](#bib.bib151), [153](#bib.bib153),
    [84](#bib.bib84)] 提出了用 [MDM](#glo.main.mdm) 表示视觉-运动策略，通常将动作空间表示为末端执行器的位移。其中一些研究动机是使用混合密度模型而不是单峰模型来表示策略，以更好地捕捉演示中的固有多模态性。
- en: III-E2 Training Mixture Density Model
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-E2 训练混合密度模型
- en: Given a dataset ${\mathcal{D}}:({\bm{a}}_{i},{\bm{c}}_{i})_{i=0}^{N}$, [MDM](#glo.main.mdm)
    are trained by minimizing the negative log-likelihood of the learned model
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据集 ${\mathcal{D}}:({\bm{a}}_{i},{\bm{c}}_{i})_{i=0}^{N}$，[MDM](#glo.main.mdm)
    通过最小化学习模型的负对数似然进行训练
- en: '|  | $\displaystyle{\mathcal{L}}({\bm{\theta}})=-{\mathbb{E}}_{{\bm{a}},{\bm{c}}\sim{\mathcal{D}}}\left[\log\rho_{{\bm{\theta}}}({\bm{a}}&#124;{\bm{c}})\right]$
    |  | (19) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{L}}({\bm{\theta}})=-{\mathbb{E}}_{{\bm{a}},{\bm{c}}\sim{\mathcal{D}}}\left[\log\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})\right]$
    |  | (19) |'
- en: 'with $\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$ (See [Equation 18](#S3.E18 "In
    III-E Mixture Density Models ‣ III Density Estimation Models ‣ Deep Generative
    Models in Robotics: A Survey on Learning from Multimodal Demonstrations")) being
    the density model parameterized with the output of the learned model $H,\Omega={\bm{M}}_{{\bm{\theta}}}({\bm{c}})$.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 $\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$（见 [公式 18](#S3.E18 "在 III-E 混合密度模型
    ‣ III 密度估计模型 ‣ 机器人技术中的深度生成模型：多模态演示学习的综述")），它是以学习模型输出 $H,\Omega={\bm{M}}_{{\bm{\theta}}}({\bm{c}})$
    为参数的密度模型。
- en: IV Integrating Generative Models into Robotics
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 将生成模型集成到机器人技术中
- en: '![Refer to caption](img/9221624bfb11780273f3913a4c1ad76e.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9221624bfb11780273f3913a4c1ad76e.png)'
- en: 'Figure 7: Visual representation of different approaches to apply [DGM](#glo.main.dgm)
    in robotics tasks. Colored: Learned models, Grey: Predefined models. (a) Cloth
    Manipulation. Given a set of motion primitives, an Action Value Map selects the
    primitive and the parameters of the primitive [[41](#bib.bib41)]. (b) Object Picking.
    An SE(3) pose generative model generates a target pose to grasp an object and
    a motion planner generates the path to reach the grasp [[5](#bib.bib5)]. (c) Visuo-Motor
    Policy. Given an image as input, a visuomotor policy generates end-effector actions.
    Then, an Operational Space Controller maps the action to the configuration space [[150](#bib.bib150)].
    (d) Video Planning. A [LLM](#glo.main.llm) generates a plan in text. The text
    generates a video of the substeps. Then, a goal-conditioned policy generates robot
    actions conditioned on generated images [[128](#bib.bib128)].'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：将 [DGM](#glo.main.dgm) 应用于机器人任务的不同方法的视觉表示。彩色：学习的模型，灰色：预定义的模型。（a）布料操控。给定一组动作原语，动作值图选择原语及其参数
    [[41](#bib.bib41)]。（b）物体抓取。一个 SE(3) 姿态生成模型生成一个目标姿态来抓取物体，运动规划器生成达到抓取的路径 [[5](#bib.bib5)]。（c）视觉-运动策略。给定图像作为输入，视觉-运动策略生成末端执行器的动作。然后，操作空间控制器将动作映射到配置空间
    [[150](#bib.bib150)]。（d）视频规划。一个 [LLM](#glo.main.llm) 生成文本中的计划。该文本生成子步骤的视频。然后，一个目标条件策略基于生成的图像生成机器人动作
    [[128](#bib.bib128)]。
- en: In this section, we look at different design strategies for integrating [DGM](#glo.main.dgm)
    into robotics. The number of robotic tasks in which [DGM](#glo.main.dgm) has been
    applied is vast; pick and place tasks [[154](#bib.bib154), [92](#bib.bib92), [58](#bib.bib58)],
    cloth manipulation [[155](#bib.bib155), [41](#bib.bib41)], scene rearrangement [[60](#bib.bib60),
    [9](#bib.bib9)], food preparation [[7](#bib.bib7), [150](#bib.bib150), [13](#bib.bib13)].
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了将 [DGM](#glo.main.dgm) 集成到机器人技术中的不同设计策略。[DGM](#glo.main.dgm) 已在广泛的机器人任务中应用；例如，抓取和放置任务
    [[154](#bib.bib154), [92](#bib.bib92), [58](#bib.bib58)]，布料操控 [[155](#bib.bib155),
    [41](#bib.bib41)]，场景重排 [[60](#bib.bib60), [9](#bib.bib9)]，食品准备 [[7](#bib.bib7),
    [150](#bib.bib150), [13](#bib.bib13)]。
- en: A common strategy for using [DGM](#glo.main.dgm) in this wide range of tasks
    is to integrate the learned [DGM](#glo.main.dgm) into a larger framework. The
    learned model is typically combined with other predefined or learned components,
    such as perception modules, motion primitives, task and motion planners, or controllers,
    creating a synergy that exploits the strengths of both learned and predefined
    components. The [DGM](#glo.main.dgm) s are tasked with addressing the components
    of the problem that are difficult to model conventionally, while the predefined
    elements handle aspects that are easier to define. This combination enhances the
    system’s ability to tackle complex tasks by leveraging the predictive power and
    adaptability of the [DGM](#glo.main.dgm) s.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这广泛的任务中使用[DGM](#glo.main.dgm)的一个常见策略是将学习到的[DGM](#glo.main.dgm)集成到更大的框架中。学习到的模型通常与其他预定义或学习的组件，如感知模块、运动原语、任务和运动规划器或控制器，结合起来，形成利用学习和预定义组件优势的协同效应。[DGM](#glo.main.dgm)负责处理那些难以用传统方法建模的问题组件，而预定义的元素处理那些易于定义的方面。这种组合通过利用[DGM](#glo.main.dgm)的预测能力和适应性，增强了系统解决复杂任务的能力。
- en: 'Due to the variety of tasks and the flexibility in which [DGM](#glo.main.dgm)
    can be combined with other components, there are a wide variety of ways in which
    [DGM](#glo.main.dgm) have been integrated into robotics problems. We present some
    possible combinations in [Figure 7](#S4.F7 "In IV Integrating Generative Models
    into Robotics ‣ Deep Generative Models in Robotics: A Survey on Learning from
    Multimodal Demonstrations"). In the following, we present some of the most common
    strategies that the robotics community has used to integrate [DGM](#glo.main.dgm)
    to solve robotics tasks. We classify the strategies based on the element that
    [DGM](#glo.main.dgm) generates.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 由于任务的多样性以及[DGM](#glo.main.dgm)与其他组件结合的灵活性，[DGM](#glo.main.dgm)在机器人问题中的集成方式也非常多样。我们在[图7](#S4.F7
    "在 IV 章节中将生成模型集成到机器人技术中 ‣ 深度生成模型在机器人中的应用：多模态演示学习综述")中展示了一些可能的组合。接下来，我们展示了机器人社区常用的一些策略，以将[DGM](#glo.main.dgm)集成到解决机器人任务中。我们根据[DGM](#glo.main.dgm)生成的元素对这些策略进行分类。
- en: IV-A Generating End Effector Target Poses
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 生成末端执行器目标姿态
- en: End-effector 6D (position and orientation) poses are among the most common elements
    generated with [DGM](#glo.main.dgm) [[5](#bib.bib5), [6](#bib.bib6), [100](#bib.bib100),
    [58](#bib.bib58), [154](#bib.bib154), [132](#bib.bib132), [131](#bib.bib131),
    [70](#bib.bib70), [10](#bib.bib10)]. The generated poses were used as target grasping
    poses of an object in the scene [[5](#bib.bib5), [154](#bib.bib154), [70](#bib.bib70),
    [58](#bib.bib58)], as target placing poses for the objects [[10](#bib.bib10)],
    or as an action of a policy [[6](#bib.bib6), [100](#bib.bib100), [131](#bib.bib131)].
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 末端执行器的6D（位置和方向）姿态是使用[DGM](#glo.main.dgm)生成的最常见元素之一[[5](#bib.bib5), [6](#bib.bib6),
    [100](#bib.bib100), [58](#bib.bib58), [154](#bib.bib154), [132](#bib.bib132),
    [131](#bib.bib131), [70](#bib.bib70), [10](#bib.bib10)]。生成的姿态被用作场景中物体的目标抓取姿态[[5](#bib.bib5),
    [154](#bib.bib154), [70](#bib.bib70), [58](#bib.bib58)]，作为物体的目标放置姿态[[10](#bib.bib10)]，或作为策略的动作[[6](#bib.bib6),
    [100](#bib.bib100), [131](#bib.bib131)]。
- en: When to use. SE(3) pose-based [DGM](#glo.main.dgm) have been particularly successful
    in capturing particularly relevant target locations in robot tasks. For example,
    in a pick and place task, the SE(3) pose model will inform the desirable grasping
    pose and the desirable placing pose. In a drawer opening task, the SE(3) pose
    might inform the desirable pose for grasping the drawer handle and where to move
    to properly open the drawer.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 何时使用。基于SE(3)姿态的[DGM](#glo.main.dgm)在捕捉机器人任务中的关键目标位置方面尤其成功。例如，在取放任务中，SE(3)姿态模型将提供所需的抓取姿态和放置姿态。在抽屉开关任务中，SE(3)姿态可能会提供有关抓取抽屉把手的理想姿态以及如何移动以正确打开抽屉的信息。
- en: 'How to use. The SE(3) pose generative models typically exploit the symmetries
    between the scene and the poses, leading to improved generalization in novel scenarios.
    More details can be found in [Section V](#S5 "V Generalizing outside data distributions
    ‣ Deep Generative Models in Robotics: A Survey on Learning from Multimodal Demonstrations").
    The integration of the output of the [DGM](#glo.main.dgm) in robotics applications
    depends on the task.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用。SE(3)姿态生成模型通常利用场景和姿态之间的对称性，从而在新场景中提高了泛化能力。更多细节可以在[第V节](#S5 "V 章 在数据分布外进行泛化
    ‣ 深度生成模型在机器人中的应用：多模态演示学习综述")中找到。[DGM](#glo.main.dgm)输出在机器人应用中的集成取决于任务。
- en: In offline tasks, where the robot behavior is computed offline and executed
    in open-loop, it is common to combine the [DGM](#glo.main.dgm) with motion planning
    modules. In [[5](#bib.bib5), [154](#bib.bib154), [105](#bib.bib105), [156](#bib.bib156)],
    the generated SE(3) pose is used as the target pose in a motion planning problem.
    Then, sampling-based [[81](#bib.bib81), [80](#bib.bib80)] or optimization-based [[157](#bib.bib157),
    [158](#bib.bib158)] motion planners are used to generate the configuration space
    trajectories. In [[58](#bib.bib58), [92](#bib.bib92)], the learned [DGM](#glo.main.dgm)
    is integrated directly into the motion planning problem. Instead of sampling a
    pose, since the learned model is a [EBM](#glo.main.ebm), the model is integrated
    as an additional cost function.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 离线任务中，机器人行为在离线计算后以开环方式执行，通常会将[DGM](#glo.main.dgm)与运动规划模块结合。在[[5](#bib.bib5),
    [154](#bib.bib154), [105](#bib.bib105), [156](#bib.bib156)]中，生成的SE(3)姿态被用作运动规划问题中的目标姿态。然后，基于采样的[[81](#bib.bib81),
    [80](#bib.bib80)]或基于优化的[[157](#bib.bib157), [158](#bib.bib158)]运动规划器被用来生成配置空间轨迹。在[[58](#bib.bib58),
    [92](#bib.bib92)]中，学习到的[DGM](#glo.main.dgm)被直接集成到运动规划问题中。由于学习模型是[EBM](#glo.main.ebm)，因此模型被集成作为额外的成本函数，而不是采样姿态。
- en: In online tasks, where the generative model is used as a policy, the proposed
    solutions depend on the horizon of the output. In [[6](#bib.bib6), [100](#bib.bib100),
    [131](#bib.bib131), [159](#bib.bib159), [160](#bib.bib160)], the generated SE(3)
    pose is used as a target pose in a motion planning problem. Then, since the problem
    to be solved is sequential, the system sequentially generates new trajectories
    as the robot reaches the previous target pose. Some recent works [[161](#bib.bib161),
    [162](#bib.bib162)], have proposed to replace the motion planner with a trajectory
    diffusion model conditioned on the generated SE(3) pose. To have a higher control
    frequency, some works [[163](#bib.bib163)] have proposed using operational space
    controllers [[164](#bib.bib164), [165](#bib.bib165)] instead of a motion planner,
    usually considering a shorter horizon SE(3) target pose.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在线任务中，当生成模型用作策略时，所提议的解决方案取决于输出的范围。在[[6](#bib.bib6), [100](#bib.bib100), [131](#bib.bib131),
    [159](#bib.bib159), [160](#bib.bib160)]中，生成的SE(3)姿态被用作运动规划问题中的目标姿态。由于要解决的问题是序列性的，因此系统在机器人达到前一个目标姿态时，顺序生成新的轨迹。一些最近的研究[[161](#bib.bib161),
    [162](#bib.bib162)]提议用基于生成的SE(3)姿态的轨迹扩散模型来替代运动规划器。为了实现更高的控制频率，一些研究[[163](#bib.bib163)]提议使用操作空间控制器[[164](#bib.bib164),
    [165](#bib.bib165)]代替运动规划器，通常考虑较短的SE(3)目标姿态。
- en: IV-B Trajectories
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 轨迹
- en: In recent years, [DGM](#glo.main.dgm) have shown the ability to generate high-dimensional
    data. This has led the research community to learn [DGM](#glo.main.dgm) that generate
    complete trajectories [[36](#bib.bib36), [7](#bib.bib7), [115](#bib.bib115), [8](#bib.bib8),
    [13](#bib.bib13), [166](#bib.bib166), [123](#bib.bib123)], rather than single-step
    actions.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，[DGM](#glo.main.dgm)显示了生成高维数据的能力。这促使研究界学习生成完整轨迹的[DGM](#glo.main.dgm) [[36](#bib.bib36),
    [7](#bib.bib7), [115](#bib.bib115), [8](#bib.bib8), [13](#bib.bib13), [166](#bib.bib166),
    [123](#bib.bib123)]，而不是单步动作。
- en: When to use. Directly generating the entire trajectory can have several advantages
    over generating single target poses. First, trajectory generation allows the generation
    of complex dynamic motions (pouring tomato into a pizza [[7](#bib.bib7)], inserting
    a battery [[13](#bib.bib13)]) in contrast to motion planning based methods that
    are usually limited to reaching tasks. Second, trajectory generation may lead
    to smaller covariate shift errors in contrast to single step generation. Because
    we generate trajectories, the execution of a long-horizon task might require fewer
    sequential decision steps, leading to smaller accumulated errors [[13](#bib.bib13)].
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 何时使用。直接生成整个轨迹相较于生成单个目标姿态有几个优势。首先，轨迹生成允许生成复杂的动态动作（例如将番茄倒入比萨中[[7](#bib.bib7)]，插入电池[[13](#bib.bib13)]），与通常限制于到达任务的运动规划方法相比。其次，轨迹生成可能导致较小的协变量转移误差，相较于单步生成。因为我们生成轨迹，长时间任务的执行可能需要较少的顺序决策步骤，从而导致较小的累积误差[[13](#bib.bib13)]。
- en: How to use. The most common applications of trajectory [DGM](#glo.main.dgm)
    are integrated into offline motion planners [[157](#bib.bib157), [167](#bib.bib167)]
    or used as receding horizon controllers [[168](#bib.bib168), [169](#bib.bib169),
    [170](#bib.bib170)].
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用。轨迹[DGM](#glo.main.dgm)的最常见应用是集成到离线运动规划器[[157](#bib.bib157), [167](#bib.bib167)]中或作为递归范围控制器[[168](#bib.bib168),
    [169](#bib.bib169), [170](#bib.bib170)]使用。
- en: In offline motion planning, a number of works [[36](#bib.bib36), [115](#bib.bib115),
    [123](#bib.bib123)] compose the trajectory [DGM](#glo.main.dgm) with heuristic
    cost or reward functions. Since the generative model is a [DM](#glo.main.dm),
    the sampling process is conditioned on additional reward/cost functions to generate
    trajectories that satisfy additional objectives such as reaching a goal [[36](#bib.bib36)]
    or avoiding obstacles [[115](#bib.bib115)]. This approach is similar to classifier-guided
    generation [[118](#bib.bib118)], where a diffusion model for image generation
    is conditioned with additional classifiers. Other works have composed multiple
    trajectory [DGM](#glo.main.dgm) to generate long horizon trajectories [[116](#bib.bib116)]
    by sequential composition.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在离线运动规划中，一些工作[[36](#bib.bib36)、[115](#bib.bib115)、[123](#bib.bib123)]将轨迹[DGM](#glo.main.dgm)与启发式成本或奖励函数结合。由于生成模型是[DM](#glo.main.dm)，采样过程以附加的奖励/成本函数为条件，以生成满足附加目标的轨迹，例如到达目标[[36](#bib.bib36)]或避免障碍物[[115](#bib.bib115)]。这种方法类似于分类器指导生成[[118](#bib.bib118)]，其中图像生成的扩散模型以附加的分类器为条件。其他工作通过顺序组合将多个轨迹[DGM](#glo.main.dgm)组合生成长时间范围的轨迹[[116](#bib.bib116)]。
- en: In receding horizon control problems, a trajectory [DGM](#glo.main.dgm) iteratively
    generates new trajectories adapted to changes in the scene. In [[7](#bib.bib7)],
    a [DM](#glo.main.dm) is used to generate future action trajectories. The authors
    assert the importance of predicting a sequence of actions to overcome possible
    latency gaps caused by image processing, policy inference, and network delays.
    Instead of using [DM](#glo.main.dm), [[13](#bib.bib13)] uses a [VAE](#glo.main.vae)
    to generate action trajectories. To generate the whole trajectory, they use an
    autoregressive generation approach, in which the action for a single step is generated
    at each step. To ensure smooth behavior, the authors propose a temporal ensemble
    of the generated actions with weighted averaging.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾性视野控制问题中，一个轨迹[DGM](#glo.main.dgm)会迭代生成适应场景变化的新轨迹。在[[7](#bib.bib7)]中，使用[DM](#glo.main.dm)生成未来的动作轨迹。作者强调预测一系列动作的重要性，以克服由图像处理、策略推断和网络延迟可能造成的延迟间隙。与使用[DM](#glo.main.dm)不同，[[13](#bib.bib13)]使用[VAE](#glo.main.vae)生成动作轨迹。为了生成整个轨迹，他们采用自回归生成方法，其中每一步的动作在每一步生成。为了确保平滑的行为，作者建议使用生成动作的时间集成加权平均。
- en: Once the desired trajectories are computed, a trajectory tracking controller
    is applied to move the robot along the generated path.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出期望的轨迹，就会应用轨迹跟踪控制器将机器人沿着生成的路径移动。
- en: IV-C Generating End-Effector Displacement
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 生成末端执行器位移
- en: One of the most common action spaces for policy learning is end-effector displacement [[44](#bib.bib44),
    [153](#bib.bib153), [37](#bib.bib37), [150](#bib.bib150), [141](#bib.bib141)].
    End Effector Displacements refer to 6 DoF changes in both the position and orientation
    of the robot’s end effector that are directly related to the velocity in the end
    effector.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 策略学习中最常见的动作空间之一是末端执行器位移[[44](#bib.bib44)、[153](#bib.bib153)、[37](#bib.bib37)、[150](#bib.bib150)、[141](#bib.bib141)]。末端执行器位移指的是机器人末端执行器位置和方向的6自由度变化，这些变化与末端执行器的速度直接相关。
- en: When to use. Displacement [DGM](#glo.main.dgm) have been commonly used as control
    policies. The low dimension of the generated variable allows a high frequency
    generation in contrast to trajectory [DGM](#glo.main.dgm). This allows the robot
    to adapt quickly to changes in the environment, leading to highly reactive policies.
    Additionally, in combination with an in-hand camera, we can build policies that
    act locally, providing a high degree of generalization. For example, if the in-hand
    camera observes an apple to be picked, given the actions are generated wrt. the
    end-effector, the robot uses only relative information and adapts its behavior
    to novel situations as long as the relative position of the end-effector and the
    apple remains the same.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机。位移[DGM](#glo.main.dgm)通常用作控制策略。生成变量的低维度允许与轨迹[DGM](#glo.main.dgm)相比的高频生成。这使得机器人能够快速适应环境变化，从而产生高度反应的策略。此外，结合手持摄像头，我们可以建立局部动作策略，提供高程度的泛化。例如，如果手持摄像头观察到一个苹果需要摘取，考虑到动作是相对于末端执行器生成的，只要末端执行器和苹果的相对位置保持不变，机器人就会使用相对信息并适应新情况。
- en: How to use. End-effector displacement [DGM](#glo.main.dgm) are integrated into
    feedback controllers. The output of the [DGM](#glo.main.dgm) can be integrated
    into impedance controllers informing on the desirable motion direction. In visuomotor
    policies, it is common to represent the displacement in the robot’s wrist camera
    frame. Given as observation the camera images, this allows representing local
    policies that could generalize its performance to scenes in which the scene looks
    similar locally from the camera view [[153](#bib.bib153)]. Rather than conditioning
    the policy on the last observation, several works [[44](#bib.bib44), [59](#bib.bib59),
    [153](#bib.bib153)] have found it useful to condition the model on a history of
    observations by using LSTM or RNN networks. This allows the policy to encode relevant
    features that might not be possible to extract from the last observation. Different
    [DGM](#glo.main.dgm) have been applied to capture end-effector displacements,
    from [MDM](#glo.main.mdm) [[44](#bib.bib44), [150](#bib.bib150), [153](#bib.bib153)],
    categorical distributions [[37](#bib.bib37), [72](#bib.bib72)], [EBM](#glo.main.ebm) [[94](#bib.bib94)]
    to [DM](#glo.main.dm) [[141](#bib.bib141)].
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用。末端执行器位移 [DGM](#glo.main.dgm) 被集成到反馈控制器中。[DGM](#glo.main.dgm) 的输出可以集成到阻抗控制器中，以告知理想的运动方向。在视觉运动策略中，通常在机器人手腕相机框架中表示位移。给定作为观察的相机图像，这允许表示可能在本地从相机视图看起来相似的场景的局部策略[[153](#bib.bib153)]。与其根据最后一次观察来调整策略，一些工作[[44](#bib.bib44),
    [59](#bib.bib59), [153](#bib.bib153)]发现利用 LSTM 或 RNN 网络基于观察历史来调整模型是有用的。这允许策略编码可能无法从最后一次观察中提取的相关特征。不同的
    [DGM](#glo.main.dgm) 已被应用于捕捉末端执行器位移，从 [MDM](#glo.main.mdm) [[44](#bib.bib44),
    [150](#bib.bib150), [153](#bib.bib153)]，分类分布[[37](#bib.bib37), [72](#bib.bib72)]，到
    [EBM](#glo.main.ebm) [[94](#bib.bib94)] 和 [DM](#glo.main.dm) [[141](#bib.bib141)]。
- en: IV-D Scene Arrangements
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 场景布置
- en: A set of works proposes generating desirable target scenes [[60](#bib.bib60),
    [9](#bib.bib9), [124](#bib.bib124), [125](#bib.bib125), [10](#bib.bib10), [117](#bib.bib117),
    [96](#bib.bib96)]. The target scene is represented as a set of SE(3) poses for
    different objects.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作建议生成理想的目标场景[[60](#bib.bib60), [9](#bib.bib9), [124](#bib.bib124), [125](#bib.bib125),
    [10](#bib.bib10), [117](#bib.bib117), [96](#bib.bib96)]。目标场景表示为不同对象的一组 SE(3) 姿势。
- en: When to use. Scene arrangement [DGM](#glo.main.dgm) are commonly applied to
    generate desirable placing poses for a set of objects in a scene. Given, we know
    the different objects in the scene, a set of works considers the generation of
    the placing poses of those objects given text commands informing on the desirable
    scene arrangement [[60](#bib.bib60), [96](#bib.bib96), [125](#bib.bib125), [9](#bib.bib9),
    [124](#bib.bib124)]. For example, given a text command ”Set the table for dinner”,
    the scene arrangement [DGM](#glo.main.dgm) will generate a set of placing poses
    for dishes, glasses, and cutlery.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 何时使用。场景布置 [DGM](#glo.main.dgm) 通常用于生成场景中一组对象的理想放置姿势。考虑到我们知道场景中的不同对象，一些工作考虑了根据文本命令生成这些对象的放置姿势，以通知理想的场景布置[[60](#bib.bib60),
    [96](#bib.bib96), [125](#bib.bib125), [9](#bib.bib9), [124](#bib.bib124)]。例如，给定文本命令“为晚餐摆放餐具”，场景布置
    [DGM](#glo.main.dgm) 将生成餐盘、玻璃杯和餐具的放置姿势。
- en: How to use. Scene arrangement [DGM](#glo.main.dgm) are commonly integrated into
    robot tasks with task and motion planners. Given a generated set of placing poses
    for a set of objects in the scene, the task and motion planner decides the order
    in which each object should be placed and the robot motion to pick and place each
    object. This type of model assumes access to some form of object representation.
    In [[96](#bib.bib96)], the bounding box of the different object’s of interest
    is extracted from an image. In [[125](#bib.bib125)], semantic masking is applied
    with Mask R-CNN [[171](#bib.bib171)]. In [[9](#bib.bib9)], the pointcloud of the
    object’s in the scene is cropped to represent the different objects. Several works
    have explored composing multiple scene arranging models to generate complex arrangements [[96](#bib.bib96),
    [117](#bib.bib117)]. In [[96](#bib.bib96)] multiple [EBM](#glo.main.ebm) are composed
    with different objectives (such as place the fruit in circle [EBM](#glo.main.ebm)
    and place the fruits on the plate [EBM](#glo.main.ebm)). In [[117](#bib.bib117)],
    the arrangement of a set of objects in the scene is framed as a constraint satisfaction
    problem. The work composes a set of [DM](#glo.main.dm) representing the relative
    pose of the objects between each other.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用。场景安排[模型](#glo.main.dgm)通常与任务和运动规划器集成在机器人任务中。给定一组为场景中的物体生成的放置姿势，任务和运动规划器决定每个物体应放置的顺序以及机器人拾取和放置每个物体的运动。这种类型的模型假设可以访问某种形式的物体表示。在[[96](#bib.bib96)]中，从图像中提取了不同物体的边界框。在[[125](#bib.bib125)]中，使用了Mask
    R-CNN进行语义遮蔽[[171](#bib.bib171)]。在[[9](#bib.bib9)]中，场景中物体的点云被裁剪以表示不同的物体。一些工作探讨了组合多个场景安排模型以生成复杂的排列[[96](#bib.bib96)，[117](#bib.bib117)]。在[[96](#bib.bib96)]中，多个[EBM](#glo.main.ebm)被组合以实现不同的目标（例如，将水果放在圆圈中[EBM](#glo.main.ebm)和将水果放在盘子上[EBM](#glo.main.ebm)）。在[[117](#bib.bib117)]中，场景中一组物体的排列被框定为一个约束满足问题。该工作组合了一组[DM](#glo.main.dm)来表示物体之间的相对姿势。
- en: V Generalizing outside data distributions
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 泛化到外部数据分布
- en: The problem of generalization in generative modeling refers to the ability of
    a [DGM](#glo.main.dgm) to produce high-quality, meaningful samples beyond the
    training dataset. In the particular case of robotics, given a generative model
    $\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$ trained on a dataset ${\mathcal{D}}:\{{\bm{a}}_{n},{\bm{c}}_{n}\}_{n=1}^{N}$,
    the goal is to generate useful actions ${\bm{a}}$ for contexts ${\bm{c}}\notin{\mathcal{D}}$
    that are not part of the dataset.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 生成建模中的泛化问题指的是[模型](#glo.main.dgm)在训练数据集之外生成高质量、有意义样本的能力。在机器人领域的特定情况下，给定一个在数据集${\mathcal{D}}:\{{\bm{a}}_{n},{\bm{c}}_{n}\}_{n=1}^{N}$上训练的生成模型$\rho_{{\bm{\theta}}}({\bm{a}}|{\bm{c}})$，目标是为数据集中未包含的上下文${\bm{c}}\notin{\mathcal{D}}$生成有用的动作${\bm{a}}$。
- en: 'Achieving high generalization capabilities requires the selection of smart
    architectural choices that enhance the agent’s generalization capabilities. We
    group the strategies in three main categories:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 实现高泛化能力需要选择智能的架构设计，这些设计能够增强智能体的泛化能力。我们将这些策略分为三大类：
- en: Composition. Rather than learning a monolithic policy, several researchers have
    explored learning individual behavior modules that can later be composed to generate
    complex behaviors. This composition can be both parallel (combining the behaviors
    together) and sequential (for generating long-horizon tasks). The composition
    of simple modules allows the formation of complex models that can generalize to
    new tasks not seen in the demonstrations [[172](#bib.bib172)].
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 组合。与其学习一个整体的策略，几个研究人员探讨了学习可以后来组合以生成复杂行为的单独行为模块。这种组合可以是并行的（将行为组合在一起）和顺序的（用于生成长时间范围的任务）。简单模块的组合允许形成可以对演示中未见过的新任务进行泛化的复杂模型[[172](#bib.bib172)]。
- en: Feature Selection from Observations. With a small dataset, the robot is expected
    to learn spurious correlations between observations and actions, in part due to
    the high dimensionality of the observations (images, tactile signals). To alleviate
    this problem, several researchers have explored the problem of reducing the observations
    to informative features, thus improving the system’s ability to learn meaningful
    correlations.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 从观察数据中选择特征。对于小型数据集，机器人可能会学习到观察与行动之间的虚假关联，这部分是由于观察数据（图像、触觉信号）的高维度。为了缓解这个问题，一些研究人员探讨了将观察数据减少到信息特征的问题，从而提高系统学习有意义关联的能力。
- en: Observation-Action Symmetries. In visuomotor policies, visual observations and
    actions are typically represented in different spaces. Given this mismatch, it
    is difficult for the policy to learn the correct relations between observations
    and actions, leading to poor generalization when observations are changed. To
    reduce this mismatch, a large line of research explores the problem of representing
    both visual observations and actions in a common space. In this context, some
    approaches propose mapping actions to pixel space or representing both visual
    observations and actions in 3D space.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 观察-动作对称性。在视觉运动策略中，视觉观察和动作通常表示在不同的空间中。由于这种不匹配，策略很难学习观察和动作之间的正确关系，导致在观察发生变化时泛化能力差。为减少这种不匹配，大量研究探索了在一个共同空间中表示视觉观察和动作的问题。在这种背景下，一些方法提出将动作映射到像素空间或在3D空间中表示视觉观察和动作。
- en: In the following, we elaborate on these three approaches and show how they have
    been useful for robotics problems.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们详细阐述这三种方法，并展示它们在机器人问题中的实用性。
- en: '![Refer to caption](img/51b376474a32e29c84d0a22efae8caf2.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/51b376474a32e29c84d0a22efae8caf2.png)'
- en: 'Figure 8: Visual representation of energy composition. Due to the implicit
    nature of [EBM](#glo.main.ebm), the distribution of the models can be composed
    to satisfy multiple objectives.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：能量组合的视觉表示。由于[EBM](#glo.main.ebm)的隐式特性，模型的分布可以组合以满足多个目标。
- en: '![Refer to caption](img/8a72ddfdd4603797951d14423d95323c.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8a72ddfdd4603797951d14423d95323c.png)'
- en: 'Figure 9: Given the high-dimensionality of the visual contexts, different works
    explore how to extract informative features from the context by an information
    bottleneck. Among the options, we can extract the pose of the relevant objects,
    extract relevant keypoints or extract the regions of interest in the image.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：鉴于视觉上下文的高维性，不同的研究探索了如何通过信息瓶颈从上下文中提取信息特征。在这些选项中，我们可以提取相关物体的姿态、提取相关关键点或提取图像中的兴趣区域。
- en: V-A Composition
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 组合
- en: 'Many robot tasks can be decomposed into a composition of simpler subtasks.
    Consider, for instance, the problem of selecting a grasping pose to pick an object.
    The selection of the pose can be decomposed into the satisfaction of multiple
    simpler objectives: satisfying the robot’s joint limits, avoiding collisions in
    the scene, and constructing a geometrically consistent grasp pose on an object.
    While learning a monolithic [DGM](#glo.main.dgm) to take into consideration all
    the objectives would require gathering a set of demonstrations satisfying these
    constraints, we can solve this task in a zero-shot manner by directly combining
    simpler models that capture each simpler objective. This modular approach allows
    us to learn a set of simple objectives that can combined at test-time to solve
    complex new unseen tasks [[172](#bib.bib172)].'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器人任务可以分解为更简单的子任务的组合。例如，考虑选择一个抓取姿态以拾取物体的问题。姿态的选择可以分解为满足多个更简单目标：满足机器人的关节限制、避免场景中的碰撞，以及在物体上构建几何一致的抓取姿态。虽然学习一个单一的[DGM](#glo.main.dgm)以考虑所有目标需要收集满足这些约束的一组演示，但我们可以通过直接组合捕捉每个简单目标的模型来以零样本方式解决这个任务。这种模块化方法允许我们学习一组简单的目标，这些目标可以在测试时组合以解决复杂的新任务
    [[172](#bib.bib172)]。
- en: Formally, given a generative model $\rho({\bm{a}}|{\bm{c}}_{1})$ which generates
    actions conditioned on a objective ${\bm{c}}_{1}$ and another generative model
    $\rho({\bm{a}}|{\bm{c}}_{2})$ which generates actions conditioned on a objective
    ${\bm{c}}_{2}$, the composition
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，给定一个生成模型 $\rho({\bm{a}}|{\bm{c}}_{1})$，它在目标 ${\bm{c}}_{1}$ 的条件下生成动作，以及另一个生成模型
    $\rho({\bm{a}}|{\bm{c}}_{2})$，它在目标 ${\bm{c}}_{2}$ 的条件下生成动作，组合
- en: '|  | $\displaystyle\rho({\bm{a}}&#124;{\bm{c}}_{1},{\bm{c}}_{2})\propto\rho({\bm{a}}&#124;{\bm{c}}_{1})\rho({\bm{a}}&#124;{\bm{c}}_{2}),$
    |  | (20) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\rho({\bm{a}}|{\bm{c}}_{1},{\bm{c}}_{2})\propto\rho({\bm{a}}|{\bm{c}}_{1})\rho({\bm{a}}|{\bm{c}}_{2}),$
    |  | (20) |'
- en: leads to a generative model $\rho({\bm{a}}|{\bm{c}}_{1},{\bm{c}}_{2})$ that
    will generate actions that are likely for both ${\bm{c}}_{1}$ and ${\bm{c}}_{2}$
    objective [[173](#bib.bib173)]. Alternatively, given a discriminative model $\rho({\bm{c}}_{2}|{\bm{a}})$,
    the composition
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 生成一个模型 $\rho({\bm{a}}|{\bm{c}}_{1},{\bm{c}}_{2})$，它将生成对目标 ${\bm{c}}_{1}$ 和 ${\bm{c}}_{2}$
    都可能的动作 [[173](#bib.bib173)]。或者，给定一个判别模型 $\rho({\bm{c}}_{2}|{\bm{a}})$，组合
- en: '|  | $\displaystyle\rho({\bm{a}}&#124;{\bm{c}}_{1},{\bm{c}}_{2})\propto\rho({\bm{a}}&#124;{\bm{c}}_{1})\rho({\bm{c}}_{2}&#124;{\bm{a}}),$
    |  | (21) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\rho({\bm{a}}|{\bm{c}}_{1},{\bm{c}}_{2})\propto\rho({\bm{a}}|{\bm{c}}_{1})\rho({\bm{c}}_{2}|{\bm{a}}),$
    |  | (21) |'
- en: also allows us to construct a generative model $\rho({\bm{a}}|{\bm{c}}_{1},{\bm{c}}_{2})$
    through Bayes rule. Thus, composing multiple probabilistic models allows for the
    construction of samples that satisfy multiple objectives.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 还允许我们通过贝叶斯规则构建生成模型 $\rho({\bm{a}}|{\bm{c}}_{1},{\bm{c}}_{2})$。因此，组合多个概率模型允许构建满足多个目标的样本。
- en: In [[154](#bib.bib154)], two models are composed to generate grasp poses that
    are both valid to grasp any object and also collision-free. First, a [VAE](#glo.main.vae)-based
    grasp pose generative model [[5](#bib.bib5)] generates a set of grasp candidates.
    Then, a discriminative model evaluates collisions in the scene for the grasp candidates.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[154](#bib.bib154)]中，两个模型被组合以生成既有效于抓取任何物体又不发生碰撞的抓取姿势。首先，一个基于[VAE](#glo.main.vae)的抓取姿势生成模型[[5](#bib.bib5)]生成一组抓取候选姿势。然后，一个判别模型评估抓取候选姿势在场景中的碰撞情况。
- en: Due to their implicit nature, [EBM](#glo.main.ebm) s and [DM](#glo.main.dm)
    s have been extensively integrated for composable sampling. One early example
    in [EBM](#glo.main.ebm) s is in [[101](#bib.bib101)] which illustrates how combining
    a trajectory-level EBM with a reward function can implement model-based planning
    and generate high-reward trajectories. In [[36](#bib.bib36)], this approach is
    applied to diffusion models, where a generalistic trajectory-level diffusion model
    is combined with a reward function to generate high-reward samples among the demonstrations.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其隐式特性，[EBM](#glo.main.ebm)和[DM](#glo.main.dm)已被广泛整合用于可组合采样。一个早期的例子是在[EBM](#glo.main.ebm)中，如[[101](#bib.bib101)]所示，如何将轨迹级EBM与奖励函数结合以实现基于模型的规划并生成高奖励轨迹。在[[36](#bib.bib36)]中，这种方法被应用于扩散模型，其中一个通用的轨迹级扩散模型与奖励函数相结合，以生成在演示中获得高奖励的样本。
- en: A set of works [[58](#bib.bib58), [174](#bib.bib174), [115](#bib.bib115), [117](#bib.bib117),
    [175](#bib.bib175)] have illustrated in various forms how multiple cost or constraint
    functions are combined to define an optimization problem over trajectories. For
    example, in [[58](#bib.bib58)], a diffusion in SE(3) representing valid grasp
    poses is combined with collision-avoidance, trajectory smoothness, or robot joint
    limits cost to generate collision-free pick and place trajectories.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列工作[[58](#bib.bib58), [174](#bib.bib174), [115](#bib.bib115), [117](#bib.bib117),
    [175](#bib.bib175)]以各种形式展示了如何将多个成本或约束函数组合以定义轨迹上的优化问题。例如，在[[58](#bib.bib58)]中，一个表示有效抓取姿势的SE(3)中的扩散与避免碰撞、轨迹平滑或机器人关节限制成本结合，以生成无碰撞的取放轨迹。
- en: Compositionality can also be applied sequentially along a temporal axis to generate
    trajectories for solving long-horizon tasks. In [[116](#bib.bib116), [176](#bib.bib176),
    [126](#bib.bib126)], diffusion models are composed sequentially to solve long-horizon
    manipulation tasks. The authors propose learning diffusion models for individual
    skills and then sample trajectories by chaining skills and sampling from the joint
    distribution.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 可组合性还可以沿时间轴顺序应用，以生成解决长期任务的轨迹。在[[116](#bib.bib116), [176](#bib.bib176), [126](#bib.bib126)]中，扩散模型被顺序组合以解决长期操作任务。作者提出学习个体技能的扩散模型，然后通过链接技能并从联合分布中采样来生成轨迹。
- en: Composability have been also explored to induce generalization in the object
    class [[177](#bib.bib177)] or among sensor modalities [[178](#bib.bib178)]. In
    [[177](#bib.bib177)], the motion of the tools to solve a given task is generated
    by diffusion models. The work proposed segmenting the tool into different sections (handle,
    body, rim) and composing the diffusion models defined for the different parts.
    This decoupling induces a generalization to novel objects with different shapes
    and sizes. In [[178](#bib.bib178)], several learned models are combined in which
    each model might depend on a different sensor modality from images, tactile or
    pointcloud. This composition allows the policy to learn sensor-specialized skills
    and combine them afterwards.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 可组合性也被探索以诱导对象类别的泛化[[177](#bib.bib177)]或传感器模态之间的泛化[[178](#bib.bib178)]。在[[177](#bib.bib177)]中，通过扩散模型生成解决给定任务的工具运动。该研究提出将工具分割为不同部分（手柄、主体、边缘）并组合为不同部分定义的扩散模型。这种解耦引导了对不同形状和大小的新物体的泛化。在[[178](#bib.bib178)]中，多个学习的模型被组合在一起，每个模型可能依赖于来自图像、触觉或点云的不同传感器模态。这种组合使策略能够学习传感器专用技能，并在之后进行组合。
- en: Finally, composability can be applied to foundation models trained on separate
    sources of Internet knowledge [[128](#bib.bib128), [126](#bib.bib126)] to combine
    information across each model. In  [[128](#bib.bib128)], a large language model
    capturing high-level information is combined with a video model capturing low-level
    information and an egocentric action model capturing action information. By composing
    all three models together, the model can in a zero-shot manner solve long-horizon
    tasks by integrating the knowledge across all three models. This composition is
    extended in [[126](#bib.bib126)], where planning between a vision-language and
    video model is used to construct long horizon plans.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，组合性可以应用于在不同来源的互联网知识上训练的基础模型 [[128](#bib.bib128), [126](#bib.bib126)]，以结合各模型之间的信息。在 [[128](#bib.bib128)]
    中，一个捕捉高级信息的大型语言模型与一个捕捉低级信息的视频模型以及一个捕捉动作信息的自我中心动作模型相结合。通过将这三个模型组合在一起，模型可以以零样本的方式通过整合所有三个模型的知识来解决长期任务。这种组合在 [[126](#bib.bib126)]
    中得到了扩展，其中通过视觉-语言模型和视频模型之间的规划来构建长期计划。
- en: V-B Extracting the informative features from the perception
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 从感知中提取信息特征
- en: Given the high amount of information in the visual observations ${\bm{c}}$,
    to properly solve a robotics task, we might require to apply some form of representation
    learning to focus on the meaningful features to solve the tasks. For example,
    due to the limited training data, end-to-end visuomotor policies are likely to
    falsely associate actions with task-irrelevant visual factors, leading to poor
    generalization in new situations [[150](#bib.bib150), [179](#bib.bib179)]. In
    contrast, with a proper representation learning approach, the robot might learn
    meaningful features for generalization beyond the demonstrations.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于视觉观察中的信息量很大，为了正确解决机器人任务，我们可能需要应用某种形式的表示学习，以关注解决任务所需的有意义的特征。例如，由于训练数据有限，端到端的视觉运动策略可能会错误地将动作与任务无关的视觉因素关联起来，从而导致在新情况中泛化效果差 [[150](#bib.bib150),
    [179](#bib.bib179)]。相比之下，通过适当的表示学习方法，机器人可能会学习到有意义的特征，从而在演示之外进行泛化。
- en: Consider a language-conditioned policy trained on demonstrations that include
    specific text commands, such as ”open a drawer”. Crucially, the ability of these
    models to generalize to semantically similar but lexically distinct commands,
    such as ”pull-out a drawer” without direct training on such commands, represents
    a significant advancement in generalization. Another example might be in an image-conditioned
    policy. Given a learned model, the robot should be able to generalize its behavior
    to scenes in which distractors might appear or objects are located in novel places.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个基于语言条件的策略，该策略在包含特定文本命令的演示上进行训练，例如“打开抽屉”。至关重要的是，这些模型在没有直接训练的情况下，对语义上类似但在词汇上不同的命令（例如“拉出抽屉”）的泛化能力，代表了泛化的重要进展。另一个例子可能是在图像条件的策略中。给定一个学习到的模型，机器人应该能够将其行为泛化到干扰物可能出现或物体位于新位置的场景中。
- en: This generalization is facilitated by learning an encoder ${\bm{z}}={\mathcal{E}}({\bm{c}})$
    that is capable of producing latent representations ${\bm{z}}$ that capture the
    relevant features to solve the robotics tasks.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这种泛化通过学习一个编码器 ${\bm{z}}={\mathcal{E}}({\bm{c}})$ 得以实现，该编码器能够生成捕捉解决机器人任务所需相关特征的潜在表示
    ${\bm{z}}$。
- en: Related to visual contexts, a common approach is to extract some form of object-centric
    features from the images, usually related to the location of the objects.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 与视觉上下文相关的一个常见方法是从图像中提取某种形式的以物体为中心的特征，通常与物体的位置相关。
- en: A classical approach is to pre-train a pose estimation model, that will transform
    a visual input into the position ${\bm{p}}\in\mathbb{R}^{3}$ and orientation $R\in
    SO(3)$ of the object of interest [[180](#bib.bib180), [181](#bib.bib181), [182](#bib.bib182)].
    Nevertheless, as pointed out in [[183](#bib.bib183)], a category-level pose estimation
    can be ambiguous under large intra-category shape variations. For example, knowing
    the pose of a coffee mug might not be enough to successfully hang it on a rack,
    as different coffee mugs might have different handles shapes or handle locations.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的方法是预训练一个姿态估计模型，该模型将视觉输入转换为感兴趣物体的位置 ${\bm{p}}\in\mathbb{R}^{3}$ 和方向 $R\in
    SO(3)$ [[180](#bib.bib180), [181](#bib.bib181), [182](#bib.bib182)]。然而，如 [[183](#bib.bib183)]
    所指出的那样，类别级别的姿态估计在大范围类别内形状变化下可能会模糊。例如，知道一个咖啡杯的姿态可能不足以成功地将其挂在架子上，因为不同的咖啡杯可能有不同的把手形状或把手位置。
- en: Alternatively, a set of works has proposed extracting a set of key points from
    the image [[28](#bib.bib28), [183](#bib.bib183), [184](#bib.bib184), [185](#bib.bib185),
    [186](#bib.bib186), [187](#bib.bib187), [179](#bib.bib179)]. For example, in [[183](#bib.bib183)],
    a 3D keypoint detection network transforms an RGB-D image into a set of 3D keypoints
    ${\bm{P}}=\{{\bm{p}}_{i}\}_{i=1}^{N}\in\mathbb{R}^{N\times 3}$, where $N$ is the
    number of keypoints. In contrast with only extracting the pose, several key points
    could inform about the shape of the object of interest.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是从图像中提取一组关键点 [[28](#bib.bib28), [183](#bib.bib183), [184](#bib.bib184),
    [185](#bib.bib185), [186](#bib.bib186), [187](#bib.bib187), [179](#bib.bib179)]。例如，在[[183](#bib.bib183)]中，一个3D关键点检测网络将RGB-D图像转换为一组3D关键点
    ${\bm{P}}=\{{\bm{p}}_{i}\}_{i=1}^{N}\in\mathbb{R}^{N\times 3}$，其中 $N$ 是关键点的数量。与仅提取姿势不同，几个关键点可以提供关于感兴趣物体形状的信息。
- en: A more general approach is to extract a set of cropped images through bounding
    boxes [[188](#bib.bib188), [189](#bib.bib189), [150](#bib.bib150)]. Given an RGB
    image as input, the encoder outputs a set of Regions of Interest (RoI) represented
    by bounding box locations ${\bm{p}}_{i}\in\mathbb{R}^{4}$ (pixel locations to
    construct the bounding box) and the cropped images ${\bm{I}}_{i}^{\text{crop}}$
    by the given pixel locations. While [[188](#bib.bib188), [189](#bib.bib189)] consider
    a category-level training to extract the bounding boxes, in [[150](#bib.bib150)],
    a general pre-trained Region Proposal Network (RPN) [[190](#bib.bib190)] is used
    to extract the cropped images. Then, a Transformed policy, sets the attention
    on task-relevant cropped images.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 更通用的方法是通过边界框 [[188](#bib.bib188), [189](#bib.bib189), [150](#bib.bib150)]提取一组裁剪图像。给定一个RGB图像作为输入，编码器输出一组由边界框位置
    ${\bm{p}}_{i}\in\mathbb{R}^{4}$（构造边界框的像素位置）表示的兴趣区域（RoI）以及由给定像素位置裁剪出的图像 ${\bm{I}}_{i}^{\text{crop}}$。虽然[[188](#bib.bib188),
    [189](#bib.bib189)]考虑了提取边界框的类别级训练，但在[[150](#bib.bib150)]中，使用了一般的预训练区域建议网络（RPN） [[190](#bib.bib190)]来提取裁剪图像。然后，经过变换的策略将注意力集中在任务相关的裁剪图像上。
- en: An alternative approach to get the cropper images is through segmentation masks [[191](#bib.bib191),
    [192](#bib.bib192), [193](#bib.bib193)]. For example, in [[191](#bib.bib191)],
    Slot Attention [[194](#bib.bib194)] is applied to extract the different segmentation
    masks of the objects in the scene unsupervised. In [[192](#bib.bib192)], it is
    proposed to provide both demonstrations and scribbles on the important objects
    to pay attention to. Then, an interactive segmentation model [[195](#bib.bib195)]
    generates the segmentation mask of the desirable objects.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 获取裁剪图像的另一种方法是通过分割掩码 [[191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193)]。例如，在[[191](#bib.bib191)]中，**Slot
    Attention** [[194](#bib.bib194)]被应用于无监督地提取场景中物体的不同分割掩码。在[[192](#bib.bib192)]中，建议提供对重要物体的演示和标记，然后，一个交互式分割模型 [[195](#bib.bib195)]生成所需物体的分割掩码。
- en: A recent line of research explores using language conditioned semantic features
    from images [[107](#bib.bib107), [196](#bib.bib196), [133](#bib.bib133), [197](#bib.bib197)].
    Given a language command, the model highlights the semantically most aligned features
    allowing the robot’s behavior to focus mostly on them. This relation between language
    and vision inputs is commonly obtained by computing the cosine distance between
    the CLIP features [[108](#bib.bib108)]. This approach is particularly relevant
    for robotics as it allows to exploit the pre-trained vision language models in
    an efficient way.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究方向探索了使用来自图像的语言条件语义特征 [[107](#bib.bib107), [196](#bib.bib196), [133](#bib.bib133),
    [197](#bib.bib197)]。给定语言命令，模型突出最符合语义的特征，使机器人的行为主要集中在这些特征上。这种语言与视觉输入之间的关系通常通过计算CLIP特征之间的余弦距离获得
    [[108](#bib.bib108)]。这种方法对机器人技术尤其相关，因为它允许以高效的方式利用预训练的视觉语言模型。
- en: In a different direction, a few works have explored how to integrate tactile
    information for robot manipulation. A common strategy has been to reconstruct
    3D shapes from tactile [[198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200),
    [201](#bib.bib201), [202](#bib.bib202), [203](#bib.bib203)]. In [[198](#bib.bib198)],
    a vision-based predicted 3D shape, represented by a voxel-grid is updated with
    multiple touches on the object. The tactile information in combination with the
    location of the sensor is converted into occupied voxel information to ground
    it the 3D space. In [[199](#bib.bib199)], the shape of the object is reconstructed
    into a Neural SDF, while manipulating the object. Given the object’s orientation
    and pose is changed, the work combines a pose estimation with a shape reconstruction
    objective. In [[203](#bib.bib203)], the tactile signals are represented as a 3D
    pointcloud. Given binary sensors, the authors transform the signal into a 3D pointcloud
    if the sensor is in contact with an object.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个方向上，一些研究探讨了如何将触觉信息整合到机器人操作中。一种常见的策略是从触觉[[198](#bib.bib198), [199](#bib.bib199),
    [200](#bib.bib200), [201](#bib.bib201), [202](#bib.bib202), [203](#bib.bib203)]中重建3D形状。在[[198](#bib.bib198)]中，基于视觉预测的3D形状，以体素网格表示，通过多次触摸对象进行更新。触觉信息与传感器的位置结合，转换为占用体素信息，从而将其固定在3D空间中。在[[199](#bib.bib199)]中，物体的形状被重建为神经SDF，同时操控物体。由于物体的方向和姿势发生变化，该工作结合了姿势估计和形状重建目标。在[[203](#bib.bib203)]中，触觉信号被表示为3D点云。给定二进制传感器，作者将信号转换为3D点云，前提是传感器与物体接触。
- en: V-C Exploiting Symmetries between Perception and Action
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 充分利用感知和动作之间的对称性
- en: Multiple robot tasks have inherent symmetries. Consider, for instance, a top-view
    picking problem. Given a demonstration of the desired grasp pose to pick an apple;
    if the apple is moved 10 centimeters, the desired grasp pose should similarly
    move 10 centimeters. Thus, building policies that exploit this symmetry will induce
    important generalization.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 多机器人任务具有固有的对称性。例如，考虑一个俯视拾取问题。给定一个拾取苹果的期望抓取姿势示例；如果苹果移动了10厘米，则期望的抓取姿势也应移动10厘米。因此，建立利用这种对称性的策略将引入重要的泛化能力。
- en: Representing both perception and action in a shared space has shown important
    results in this direction [[204](#bib.bib204), [40](#bib.bib40)]. Given both (action
    and observation) are represented in the same space, the generative model exploits
    the spatial structure and allows building architectures that contain spatial symmetries,
    such as translation equivariance [[39](#bib.bib39), [62](#bib.bib62)].
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享空间中表示感知和动作在这一方向上取得了重要成果[[204](#bib.bib204), [40](#bib.bib40)]。由于动作和观察都表示在相同的空间中，生成模型利用空间结构并允许构建包含空间对称性的架构，如平移等变性[[39](#bib.bib39),
    [62](#bib.bib62)]。
- en: 'A common policy architecture to ground the actions into the perception is known
    as Action Value Map [[39](#bib.bib39), [41](#bib.bib41)] or Affordance Map [[40](#bib.bib40),
    [133](#bib.bib133)] (See [Figure 5](#S3.F5 "In III-D Categorical Models ‣ III
    Density Estimation Models ‣ Deep Generative Models in Robotics: A Survey on Learning
    from Multimodal Demonstrations")). Consider, the top-view picking problem. Given
    a visual observation ${\bm{o}}$ of the apple to pick, an Action Value Map $\rho({\bm{a}}|{\bm{o}})$
    will learn to place a high probability on the pixels around the apple (given the
    action is grounded in the pixel space) and a low probability on the rest of the
    space. Then, in inference time, even if the apple is translated, the action distribution
    will similarly translate to the region where the apple is. We visualize it [Figure 10](#S5.F10
    "In V-C Exploiting Symmetries between Perception and Action ‣ V Generalizing outside
    data distributions ‣ Deep Generative Models in Robotics: A Survey on Learning
    from Multimodal Demonstrations").'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的政策架构，用于将动作与感知结合，称为动作价值图[[39](#bib.bib39), [41](#bib.bib41)]或赋能图[[40](#bib.bib40),
    [133](#bib.bib133)]（见[图5](#S3.F5 "在III-D 类别模型 ‣ III 密度估计模型 ‣ 机器人中的深度生成模型：从多模态示例中学习的综述")）。考虑俯视拾取问题。给定一个苹果的视觉观察${\bm{o}}$，一个动作价值图$\rho({\bm{a}}|{\bm{o}})$将学习在苹果周围的像素上分配较高的概率（假设动作在像素空间中固定），而在其余空间上分配较低的概率。然后，在推断时，即使苹果被平移，动作分布也将相应地平移到苹果所在的区域。我们在[图10](#S5.F10
    "在V-C 充分利用感知和动作之间的对称性 ‣ V 泛化到数据分布外 ‣ 机器人中的深度生成模型：从多模态示例中学习的综述")中可视化。
- en: '![Refer to caption](img/9080776569f97248dd5bfa4e02a5b084.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9080776569f97248dd5bfa4e02a5b084.png)'
- en: 'Figure 10: Action Value Maps contain spatial symmetries by projecting the action
    to the pixel space. Given the value map ${\bm{H}}$ is computed by local features
    in the input image ${\bm{o}}$, a translation of the pixels, leads to a translation
    in the value map. Figure inspired by [[62](#bib.bib62)].'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：动作值图通过将动作投影到像素空间来包含空间对称性。给定值图 ${\bm{H}}$ 是通过输入图像 ${\bm{o}}$ 中的局部特征计算的，像素的平移会导致值图的平移。图灵感来自
    [[62](#bib.bib62)]。
- en: This model type has been particularly successful on top-view manipulation tasks.
    One of the first applications of Affordance Models was for grasp pose generation
    in bin-picking problems [[40](#bib.bib40)]. Given an image as input, the model
    outputs a value map in the pixel space, representing the quality of all 2D locations
    to pick an object via suction. To consider the orientation of a parallel gripper,
    [[40](#bib.bib40)] rotates the observation image by 16 different angles and generate
    16 value maps, one per rotated image. Each rotated value map is used as a possible
    orientation candidate for the grasp. [[204](#bib.bib204)] instead, generates an
    additional value map informing about the optimal orientation per pixel. In [[205](#bib.bib205),
    [62](#bib.bib62), [133](#bib.bib133)], Affordance models are extended to pick
    and place problems. In [[205](#bib.bib205)], the correlation between the picking
    and the placing actions is induced through a matching module that infers the correspondence
    between possible picking objects and possible placing locations. In [[62](#bib.bib62)],
    the placing value map is conditioned on a crop image along a selected picking
    pixel. In [[133](#bib.bib133)], Transporter Networks [[62](#bib.bib62)] are extended
    to consider language goal ${\bm{g}}$ in addition to the visual observations $Q_{{\bm{\theta}}}({\bm{o}},{\bm{g}})$.
    In [[206](#bib.bib206)], Transporter Networks are extended with Equivariant networks.
    This leads to not only translation equivariance but also rotation equivariance.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型类型在顶视操作任务上特别成功。Affordance Models 的首次应用之一是用于在 bin-picking 问题中生成抓取姿态[[40](#bib.bib40)]。给定一张图像作为输入，模型输出一个像素空间中的值图，表示通过吸力抓取物体的所有
    2D 位置的质量。为了考虑平行夹具的方向，[[40](#bib.bib40)] 通过 16 个不同的角度旋转观察图像，并生成 16 个值图，每个旋转图像对应一个值图。每个旋转后的值图作为抓取的可能方向候选。[[204](#bib.bib204)]
    则生成一个额外的值图，提供每个像素的最佳方向。在 [[205](#bib.bib205), [62](#bib.bib62), [133](#bib.bib133)]
    中，Affordance models 扩展到拾取和放置问题。在 [[205](#bib.bib205)] 中，通过一个匹配模块来推断可能的拾取物体与可能的放置位置之间的对应关系，从而引入了拾取和放置动作之间的相关性。在
    [[62](#bib.bib62)] 中，放置值图以选定的拾取像素的裁剪图像为条件。在 [[133](#bib.bib133)] 中，Transporter
    Networks [[62](#bib.bib62)] 扩展到考虑语言目标 ${\bm{g}}$，除了视觉观察 $Q_{{\bm{\theta}}}({\bm{o}},{\bm{g}})$。在
    [[206](#bib.bib206)] 中，Transporter Networks 扩展为等变网络。这不仅导致平移等变，还导致旋转等变。
- en: This type of models have been particularly useful for deformable objects [[207](#bib.bib207),
    [41](#bib.bib41), [208](#bib.bib208), [155](#bib.bib155)]. In [[207](#bib.bib207)]
    the problem of rearranging a deformable object is solved as a sequence of pick
    and place actions. In their work, Transporter Networks are extended to learn a
    goal-conditioned pick and place policy. In [[41](#bib.bib41)], a bimanual robot
    is trained for cloth manipulation. The Affordance Model is trained to select the
    parameters of a Flinging policy. Similarly to [[40](#bib.bib40)] the image is
    rotated to consider different possible grasping orientations. Addtionally, the
    image is scaled to different sizes to parameterize the distance between both manipulators
    when flinging.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的模型在可变形物体 [[207](#bib.bib207), [41](#bib.bib41), [208](#bib.bib208), [155](#bib.bib155)]
    上特别有用。在 [[207](#bib.bib207)] 中，可变形物体的重新排列问题被解决为一系列的拾取和放置动作。在他们的工作中，Transporter
    Networks 扩展到学习一个目标条件的拾取和放置策略。在 [[41](#bib.bib41)] 中，训练了一个双手机器人进行布料操作。Affordance
    Model 被训练以选择 Flinging 策略的参数。与 [[40](#bib.bib40)] 类似，图像被旋转以考虑不同的抓取方向。此外，图像还被缩放到不同的尺寸，以参数化在
    Flinging 时两个操作器之间的距离。
- en: Beyond Pick and Place or deformable object manipulation, in [[74](#bib.bib74)],
    an Affordance model is trained to throw objects. An Affordance model first selects
    the place to pick an object, then a throwing velocity module assign a desirable
    throwing velocity to that pixel. In [[73](#bib.bib73)], a policy is learned to
    tidy up a table. Given two primitives (push and pick), an Affordance Model is
    trained per each primitive and the most likely action is selected among all value
    maps. Finally, in [[39](#bib.bib39)], Affordance models were applied in a mobile
    navigation task, in which the robot needs to manipulate a set of objects.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Pick and Place或可变形物体操作，在[[74](#bib.bib74)]中，一个Affordance模型被训练用于投掷物体。Affordance模型首先选择一个放置物体的位置，然后投掷速度模块为该像素分配一个理想的投掷速度。在[[73](#bib.bib73)]中，一个策略被学习用于整理桌面。给定两个原语（推和取），每个原语都训练一个Affordance模型，并在所有值映射中选择最可能的动作。最后，在[[39](#bib.bib39)]中，Affordance模型被应用于一个移动导航任务，其中机器人需要操作一组物体。
- en: Grounding perception and action have been also explored for 6-DoF manipulation.
    In [[209](#bib.bib209), [6](#bib.bib6), [210](#bib.bib210)], Action Value Maps
    are extended to a voxel grid space. Given as input a voxel representing a 3D space,
    the action space is defined as a categorical distribution along the voxels, where
    each voxel represents a target 3D location to move the end-effector. To generate
    the voxel-grid with meaningful semantic information, [[159](#bib.bib159)] propose
    constructing the voxel-grid combining Neural Radiance Fields [[211](#bib.bib211)]
    and Stable Diffusion [[212](#bib.bib212)]. Voxel-based network are usually computationally
    demanding. To tackle it, RVT [[131](#bib.bib131), [213](#bib.bib213)] instead
    proposes projecting the problem into multiple image-level Action Value Maps. Given
    multiple viewpoints, RVT proposes generating an Action Value Map for each view
    and then, sample the action through an optimization over all views. Similarly
    [[214](#bib.bib214)] also projects the 6-DoF manipulation problem to an image
    Action Value Map. In this case, the authors solve an optimization problem among
    multiple viewpoints to select the one that provides the best top view for solving
    the task.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 地面感知和动作也被探索用于6-DoF操作。在[[209](#bib.bib209), [6](#bib.bib6), [210](#bib.bib210)]中，动作值图被扩展到体素网格空间。输入一个表示3D空间的体素，动作空间被定义为体素上的类别分布，其中每个体素表示一个目标3D位置，用于移动末端执行器。为了生成具有有意义语义信息的体素网格，[[159](#bib.bib159)]提议结合神经辐射场[[211](#bib.bib211)]和稳定扩散[[212](#bib.bib212)]来构建体素网格。基于体素的网络通常计算要求高。为了解决这个问题，RVT[[131](#bib.bib131),
    [213](#bib.bib213)]提出将问题投影到多个图像级别的动作值图上。给定多个视角，RVT建议为每个视角生成一个动作值图，然后通过对所有视角进行优化来采样动作。同样，[[214](#bib.bib214)]也将6-DoF操作问题投影到图像动作值图中。在这种情况下，作者在多个视角之间解决优化问题，以选择提供最佳顶视图来解决任务的视角。
- en: Instead of using a voxel grid, a set of works have explored using point cloud
    representations in which the action is directly projected in the point cloud itself [[63](#bib.bib63),
    [215](#bib.bib215), [216](#bib.bib216), [217](#bib.bib217), [218](#bib.bib218),
    [219](#bib.bib219)]. Similar to image-based value maps, these approaches represent
    a categorical distribution along the points in the pointcloud, where each point
    is a possible action and the model outputs a probability over all the points.
    A particular case can be found in [[100](#bib.bib100)], where the action can be
    represented in any point in the 3D space, given a pointcloud as observation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用体素网格，一些研究探讨了使用点云表示，其中动作直接投影到点云本身[[63](#bib.bib63), [215](#bib.bib215), [216](#bib.bib216),
    [217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219)]。类似于基于图像的值映射，这些方法表示了点云中各点的类别分布，其中每个点是一个可能的动作，模型输出所有点的概率。在[[100](#bib.bib100)]中可以找到一个特例，其中给定点云作为观察，动作可以在3D空间中的任何点表示。
- en: A limitation of Action Value Maps is that do not scale to large action spaces
    such as trajectories. To represent higher dimensional action spaces, several works
    have explored integrating observation-action symmetries in [DM](#glo.main.dm).
    [[220](#bib.bib220), [221](#bib.bib221)] compute the denoising step by projecting
    the action candidate to a set of camera views. Alternatively, [[161](#bib.bib161),
    [160](#bib.bib160)] first build a featurized 3D pointcloud scene and denoise the
    actions directly in the 3D space by computing the relative distance between observations
    and actions.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 行动价值图的一个限制是它们无法扩展到如轨迹这样的大型行动空间。为了表示更高维度的行动空间，一些研究探索了在[DM](#glo.main.dm)中整合观察-行动对称性。[[220](#bib.bib220),
    [221](#bib.bib221)]通过将行动候选投影到一组相机视图来计算去噪步骤。另一种方法是[[161](#bib.bib161), [160](#bib.bib160)]首先构建一个特征化的3D点云场景，然后通过计算观察和行动之间的相对距离直接在3D空间中去噪行动。
- en: VI Future Research Directions
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 未来研究方向
- en: 'Despite the successful deployment of [LfD](#glo.main.lfd) in several robot
    tasks, there are several open research challenges. We consider three main pillars
    will drive the future research in [LfD](#glo.main.lfd):'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管[LfD](#glo.main.lfd)在几个机器人任务中的应用已取得成功，但仍存在若干未解的研究挑战。我们认为有三个主要支柱将推动未来在[LfD](#glo.main.lfd)领域的研究：
- en: •
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How do we solve long-horizon tasks?
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们如何解决长期任务？
- en: •
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How do we obtain large amounts of data to train [DGM](#glo.main.dgm)?, and how
    do we learn from them?
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们如何获取大量数据来训练[DGM](#glo.main.dgm)？我们又如何从中学习？
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How do we guarantee policies to generalize to novel goals and novel scenes?.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们如何确保策略能够泛化到新的目标和新场景？
- en: In the following, we present a set of future research directions to apply [LfD](#glo.main.lfd)
    methods to solve robotics tasks.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一组未来研究方向，以应用[LfD](#glo.main.lfd)方法解决机器人任务。
- en: Robot policies for long horizon tasks. Long-horizon tasks are usually solved
    through task and motion planning algorithms. These approaches are usually crafted
    for specific applications and do not generalize to any possible task. On the other
    hand, learning-based policies are usually limited to short-horizon skills. Learning
    policies that are able to solve any type of long-horizon task is an open research
    question. A promising direction is the combination of [LLM](#glo.main.llm) for
    high-level task planning with low-level, short-horizon robot skills [[128](#bib.bib128),
    [137](#bib.bib137)]. Nevertheless, properly exploiting the outputs of the [LLM](#glo.main.llm)
    for task generation will require a proper grounding of the language commands with
    robot actions.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 长期任务的机器人策略。长期任务通常通过任务和运动规划算法来解决。这些方法通常针对特定应用进行设计，不能普遍适用于任何可能的任务。另一方面，基于学习的策略通常仅限于短期技能。能够解决任何类型的长期任务的学习策略仍是一个未解的研究问题。一个有前景的方向是将[LLM](#glo.main.llm)用于高层任务规划与低层、短期机器人技能[[128](#bib.bib128),
    [137](#bib.bib137)]结合起来。然而，正确利用[LLM](#glo.main.llm)的输出进行任务生成将需要将语言命令与机器人动作正确结合。
- en: Learning from video demonstrations. Teleoperation data is one of the most common
    approaches to demonstrate to the robots how to behave. Nevertheless, collecting
    large amounts of teleoperated data is costly. On the contrary, the internet is
    full of videos of humans performing all sorts of tasks. These videos are an important
    source of data to teach robots the desirable behavior to solve any sort of task.
    Several strategies have been explored, from extracting informative features from
    video [[222](#bib.bib222), [223](#bib.bib223)], learning directly rewards from
    the videos [[224](#bib.bib224), [225](#bib.bib225)], or learning video generative
    models [[126](#bib.bib126), [127](#bib.bib127)]. Among the different challenges
    to properly learn from videos are solving the embodiment mismatch between the
    human and the robot, lack of direct action data, or the mismatch between training
    and testing environments.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从视频演示中学习。远程操作数据是向机器人演示行为的最常见方法之一。然而，收集大量远程操作数据的成本很高。相反，互联网充满了人们执行各种任务的视频。这些视频是教机器人如何完成任务的重要数据来源。已探索了几种策略，包括从视频中提取信息特征[[222](#bib.bib222),
    [223](#bib.bib223)]，直接从视频中学习奖励[[224](#bib.bib224), [225](#bib.bib225)]，或学习视频生成模型[[126](#bib.bib126),
    [127](#bib.bib127)]。从视频中正确学习的不同挑战包括解决人类与机器人之间的体现不匹配、缺乏直接行动数据或训练与测试环境之间的不匹配。
- en: Learning from synthetic data. Given the difficulty of collecting real robot
    data, physics simulators emerge as a possible approach to generate large amounts
    of data. In this direction, there have been several works [[226](#bib.bib226),
    [227](#bib.bib227), [228](#bib.bib228), [229](#bib.bib229), [230](#bib.bib230)]
    that built benchmarks in simulation and provide pipelines for synthetic data generation.
    Nevertheless, deploying real robot policies trained on synthetic data requires
    properly addressing the sim-to-real gap.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 从合成数据中学习。由于收集真实机器人数据的难度，物理模拟器成为生成大量数据的可能方法。在这个方向上，已经有若干工作[[226](#bib.bib226),
    [227](#bib.bib227), [228](#bib.bib228), [229](#bib.bib229), [230](#bib.bib230)]在模拟中建立了基准，并提供了合成数据生成的流程。然而，将在合成数据上训练的真实机器人策略投入实际使用需要妥善解决模拟到现实的差距。
- en: Learning from online interaction. Given the high variability of the possible
    scenes a robot could encounter, learning a generalist single policy from an offline
    dataset for all possible tasks is unfeasible. Instead, an important research direction
    proposes training policies for new tasks by allowing the robot to interact with
    the environment in which it will be deployed [[231](#bib.bib231)]. This requires
    the robot to explore different possible behaviors to find those that are most
    suitable for the task in deployment. However, the way the robot explores and learns
    to solve new tasks is critical for efficient learning of new policies and is an
    important direction of future research.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 从在线交互中学习。鉴于机器人可能遇到的场景的高变异性，从离线数据集中学习一个通用的单一策略用于所有可能的任务是不可行的。相反，一个重要的研究方向是通过允许机器人与将要部署的环境进行交互来训练新的任务策略[[231](#bib.bib231)]。这要求机器人探索不同的可能行为，以找到最适合部署任务的行为。然而，机器人探索和学习解决新任务的方式对高效学习新策略至关重要，是未来研究的重要方向。
- en: 'Generalization. Even if the models are trained on large amounts of data, the
    robot will likely encounter situations that were not in the dataset. Thus, the
    generative model should be capable of generalization, generating good actions
    in unseen situations. As shown in [Section V](#S5 "V Generalizing outside data
    distributions ‣ Deep Generative Models in Robotics: A Survey on Learning from
    Multimodal Demonstrations"), a proper selection of inductive biases can promote
    generalization capabilities. Despite some interesting properties, current generative
    models have not shown yet powerful generalization capabilities and additional
    exploration on structured priors for generalization is an important direction
    of future work. In addition, integrating internet knowledge can be additional
    source of generalization performance. Existing foundation models capture rich
    sources of information from the internet that a robot policy can exploit to generalize
    to new settings. Finally, structure in terms of 3D geometry can further help in
    the grounding and aggregating of semantic information for robot policies, leading
    to better generalization. In this regard, 3D Feature fields [[100](#bib.bib100),
    [107](#bib.bib107), [196](#bib.bib196)] are a direction to represent semantic
    information and the robot actions in common space.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '泛化。即使模型在大量数据上进行训练，机器人仍可能遇到数据集中未出现的情况。因此，生成模型应该具备泛化能力，能够在未见过的情况中产生良好的动作。如[第
    V 节](#S5 "V Generalizing outside data distributions ‣ Deep Generative Models in
    Robotics: A Survey on Learning from Multimodal Demonstrations")所示，适当选择归纳偏差可以促进泛化能力。尽管当前生成模型具有一些有趣的特性，但尚未展现出强大的泛化能力，进一步探索结构先验以实现泛化是未来工作的重要方向。此外，整合互联网知识可以作为提高泛化性能的额外来源。现有的基础模型捕捉了来自互联网的丰富信息，机器人策略可以利用这些信息来泛化到新设置。最后，从
    3D 几何结构的角度来看，进一步帮助机器人策略在语义信息的基础上进行有效的搭建和聚合，从而实现更好的泛化。在这方面，3D 特征场[[100](#bib.bib100),
    [107](#bib.bib107), [196](#bib.bib196)]是表示语义信息和机器人动作在共同空间中的一种方向。'
- en: References
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural network,”
    *Advances in Neural Information Processing Systems*, 1988.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural network,”
    *Advances in Neural Information Processing Systems*, 1988.'
- en: '[2] S. Schaal, “Learning from demonstration,” in *Advances in Neural Information
    Processing Systems*, 1997.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] S. Schaal, “Learning from demonstration,” in *Advances in Neural Information
    Processing Systems*, 1997.'
- en: '[3] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, J. Peters *et al.*,
    “An algorithmic perspective on imitation learning,” *Foundations and Trends® in
    Robotics*, vol. 7, no. 1-2, pp. 1–179, 2018.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, J. Peters *等*，“对模仿学习的算法视角，”
    *《机器人学基础与趋势®》*，第7卷，第1-2期，页码1–179，2018年。'
- en: '[4] S. Schaal, “Is imitation learning the route to humanoid robots,” *Trends
    in cognitive sciences*, 1999.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. Schaal，“模仿学习是否是类人机器人之路，” *《认知科学趋势》*，1999年。'
- en: '[5] A. Mousavian, C. Eppner, and D. Fox, “6-dof graspnet: Variational grasp
    generation for object manipulation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 2901–2910.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. Mousavian, C. Eppner, 和 D. Fox，“6-dof graspnet：用于物体操作的变分抓取生成，” 见 *IEEE/CVF国际计算机视觉会议论文集*，2019年，页码2901–2910。'
- en: '[6] M. Shridhar, L. Manuelli, and D. Fox, “Perceiver-actor: A multi-task transformer
    for robotic manipulation,” in *Conference on Robot Learning*, 2023.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. Shridhar, L. Manuelli, 和 D. Fox，“Perceiver-actor：一种用于机器人操作的多任务变换器，”
    见 *机器人学习会议*，2023年。'
- en: '[7] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song,
    “Diffusion policy: Visuomotor policy learning via action diffusion,” *Proceedings
    of Robotics: Science and Systems (R:SS)*, 2023.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, 和 S. Song，“扩散策略：通过动作扩散的视觉运动策略学习，”
    *《机器人学：科学与系统会议（R:SS）》*，2023年。'
- en: '[8] M. Reuss, M. Li, X. Jia, and R. Lioutikov, “Goal-conditioned imitation
    learning using score-based diffusion policies,” *arXiv preprint arXiv:2304.02532*,
    2023.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] M. Reuss, M. Li, X. Jia, 和 R. Lioutikov，“基于分数的扩散策略的目标条件模仿学习，” *arXiv预印本
    arXiv:2304.02532*，2023年。'
- en: '[9] W. Liu, T. Hermans, S. Chernova, and C. Paxton, “Structdiffusion: Object-centric
    diffusion for semantic rearrangement of novel objects,” *Proceedings of Robotics:
    Science and Systems (R:SS)*, 2023.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] W. Liu, T. Hermans, S. Chernova, 和 C. Paxton，“Structdiffusion：面向新颖物体的语义重排的对象中心扩散，”
    *《机器人学：科学与系统会议（R:SS）》*，2023年。'
- en: '[10] A. Simeonov, A. Goyal, L. Manuelli, Y.-C. Lin, A. Sarmiento, A. R. Garcia,
    P. Agrawal, and D. Fox, “Shelving, stacking, hanging: Relational pose diffusion
    for multi-modal rearrangement,” in *Conference on Robot Learning*, 2023.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. Simeonov, A. Goyal, L. Manuelli, Y.-C. Lin, A. Sarmiento, A. R. Garcia,
    P. Agrawal, 和 D. Fox，“货架、堆叠、悬挂：用于多模态重排的关系姿态扩散，” 见 *机器人学习会议*，2023年。'
- en: '[11] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning
    and structured prediction to no-regret online learning,” in *Proceedings of the
    fourteenth international conference on artificial intelligence and statistics*.   JMLR
    Workshop and Conference Proceedings, 2011, pp. 627–635.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Ross, G. Gordon, 和 D. Bagnell，“将模仿学习和结构预测简化为无悔在线学习，” 见 *第十四届国际人工智能与统计会议论文集*。
    JMLR研讨会与会议论文集，2011年，页码627–635。'
- en: '[12] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid,
    J. Tompson, Q. Vuong, T. Yu *et al.*, “Palm-e: An embodied multimodal language
    model,” *arXiv preprint arXiv:2303.03378*, 2023.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A.
    Wahid, J. Tompson, Q. Vuong, T. Yu *等*，“Palm-e：一种具身的多模态语言模型，” *arXiv预印本 arXiv:2303.03378*，2023年。'
- en: '[13] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, “Learning fine-grained bimanual
    manipulation with low-cost hardware,” *arXiv preprint arXiv:2304.13705*, 2023.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] T. Z. Zhao, V. Kumar, S. Levine, 和 C. Finn，“使用低成本硬件学习细粒度双手操作，” *arXiv预印本
    arXiv:2304.13705*，2023年。'
- en: '[14] B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey *et al.*, “Maximum
    entropy inverse reinforcement learning.” in *AAAI*, 2008.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey *等*，“最大熵逆强化学习。” 见
    *AAAI*，2008年。'
- en: '[15] J. Fu, K. Luo, and S. Levine, “Learning robust rewards with adverserial
    inverse reinforcement learning,” in *International Conference on Learning Representations*,
    2018.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] J. Fu, K. Luo, 和 S. Levine，“通过对抗逆强化学习学习鲁棒奖励，” 见 *学习表征国际会议*，2018年。'
- en: '[16] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in *Advances
    in Neural Information Processing Systems*, 2016.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Ho 和 S. Ermon，“生成对抗模仿学习，” 见 *神经信息处理系统进展*，2016年。'
- en: '[17] F. Torabi, G. Warnell, and P. Stone, “Generative adversarial imitation
    from observation,” *arXiv preprint arXiv:1807.06158*, 2018.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] F. Torabi, G. Warnell, 和 P. Stone，“从观察中生成对抗模仿学习，” *arXiv预印本 arXiv:1807.06158*，2018年。'
- en: '[18] P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, S. Levine,
    and G. Brain, “Time-contrastive networks: Self-supervised learning from video,”
    in *2018 IEEE international conference on robotics and automation (ICRA)*.   IEEE,
    2018, pp. 1134–1141.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, S. Levine,
    和 G. Brain，“时间对比网络：从视频中进行自监督学习，” 见 *2018年IEEE国际机器人与自动化会议（ICRA）*。 IEEE，2018年，页码1134–1141。'
- en: '[19] F. Torabi, G. Warnell, and P. Stone, “Recent advances in imitation learning
    from observation,” *arXiv preprint arXiv:1905.13566*, 2019.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] F. Torabi, G. Warnell, 和 P. Stone，“从观察中模仿学习的最新进展，” *arXiv预印本 arXiv:1905.13566*，2019年。'
- en: '[20] A. Mandlekar, Y. Zhu, A. Garg, J. Booher, M. Spero, A. Tung, J. Gao, J. Emmons,
    A. Gupta, E. Orbay *et al.*, “Roboturk: A crowdsourcing platform for robotic skill
    learning through imitation,” in *Conference on Robot Learning*.   PMLR, 2018,
    pp. 879–893.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Mandlekar, Y. Zhu, A. Garg, J. Booher, M. Spero, A. Tung, J. Gao, J.
    Emmons, A. Gupta, E. Orbay *等*，“Roboturk：通过模仿进行机器人技能学习的众包平台，” 见于 *机器人学习会议*。 PMLR，2018年，第879–893页。'
- en: '[21] A. Mandlekar, J. Booher, M. Spero, A. Tung, A. Gupta, Y. Zhu, A. Garg,
    S. Savarese, and L. Fei-Fei, “Scaling robot supervision to hundreds of hours with
    roboturk: Robotic manipulation dataset through human reasoning and dexterity,”
    in *2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2019, pp. 1048–1055.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] A. Mandlekar, J. Booher, M. Spero, A. Tung, A. Gupta, Y. Zhu, A. Garg,
    S. Savarese, 和 L. Fei-Fei，“通过Roboturk将机器人监督扩展到数百小时：通过人类推理和灵巧的机器人操作数据集，” 见于 *2019年IEEE/RSJ国际智能机器人与系统会议（IROS）*。
    IEEE，2019年，第1048–1055页。'
- en: '[22] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky,
    A. Rai, A. Singh, A. Brohan *et al.*, “Open x-embodiment: Robotic learning datasets
    and rt-x models,” *arXiv preprint arXiv:2310.08864*, 2023.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky,
    A. Rai, A. Singh, A. Brohan *等*，“开放x-embodiment：机器人学习数据集和rt-x模型，” *arXiv预印本 arXiv:2310.08864*，2023年。'
- en: '[23] K. Grochow, S. L. Martin, A. Hertzmann, and Z. Popović, “Style-based inverse
    kinematics,” in *ACM SIGGRAPH 2004 Papers*, 2004, pp. 522–531.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] K. Grochow, S. L. Martin, A. Hertzmann, 和 Z. Popović，“基于风格的逆运动学，” 见于 *ACM
    SIGGRAPH 2004论文集*，2004年，第522–531页。'
- en: '[24] A. P. Shon, K. Grochow, and R. P. Rao, “Robotic imitation from human motion
    capture using gaussian processes,” in *5th IEEE-RAS International Conference on
    Humanoid Robots, 2005.*   IEEE, 2005, pp. 129–134.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. P. Shon, K. Grochow, 和 R. P. Rao，“使用高斯过程从人类运动捕捉中进行机器人模仿，” 见于 *第五届IEEE-RAS国际类人机器人会议，2005年*。
    IEEE，2005年，第129–134页。'
- en: '[25] S. Tso and K. Liu, “Hidden markov model for intelligent extraction of
    robot trajectory command from demonstrated trajectories,” in *Proceedings of the
    IEEE International Conference on Industrial Technology (ICIT’96)*.   IEEE, 1996,
    pp. 294–298.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. Tso 和 K. Liu，“用于智能提取机器人轨迹命令的隐马尔可夫模型，” 见于 *IEEE国际工业技术会议（ICIT’96）*。 IEEE，1996年，第294–298页。'
- en: '[26] J. Yang, Y. Xu, and C. S. Chen, “Human action learning via hidden markov
    model,” *IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and
    Humans*, vol. 27, no. 1, pp. 34–44, 1997.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. Yang, Y. Xu, 和 C. S. Chen，“通过隐马尔可夫模型学习人类动作，” *IEEE系统、人类和控制论学报-A部分：系统和人类*，第27卷，第1期，第34–44页，1997年。'
- en: '[27] S. Calinon, F. Guenter, and A. Billard, “On learning, representing, and
    generalizing a task in a humanoid robot,” *IEEE Transactions on Systems, Man,
    and Cybernetics, Part B (Cybernetics)*, vol. 37, no. 2, pp. 286–298, 2007.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Calinon, F. Guenter, 和 A. Billard，“在类人机器人中学习、表示和推广任务，” *IEEE系统、人类和控制论学报，B部分（控制论）*，第37卷，第2期，第286–298页，2007年。'
- en: '[28] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of
    deep visuomotor policies,” *The Journal of Machine Learning Research*, vol. 17,
    no. 1, pp. 1334–1373, 2016.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] S. Levine, C. Finn, T. Darrell, 和 P. Abbeel，“深度视觉运动策略的端到端训练，” *机器学习研究期刊*，第17卷，第1期，第1334–1373页，2016年。'
- en: '[29] S. James, M. Bloesch, and A. J. Davison, “Task-embedded control networks
    for few-shot imitation learning,” in *Conference on robot learning*.   PMLR, 2018,
    pp. 783–795.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. James, M. Bloesch, 和 A. J. Davison，“面向少样本模仿学习的任务嵌入控制网络，” 见于 *机器人学习会议*。
    PMLR，2018年，第783–795页。'
- en: '[30] C. Lynch and P. Sermanet, “Grounding language in play,” *arXiv preprint
    arXiv:2005.07648*, vol. 3, 2020.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] C. Lynch 和 P. Sermanet，“在游戏中赋予语言意义，” *arXiv预印本 arXiv:2005.07648*，第3卷，2020年。'
- en: '[31] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine,
    and C. Finn, “Bc-z: Zero-shot task generalization with robotic imitation learning,”
    in *Conference on Robot Learning*.   PMLR, 2022, pp. 991–1002.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine,
    和 C. Finn，“Bc-z：通过机器人模仿学习实现零-shot任务泛化，” 见于 *机器人学习会议*。 PMLR，2022年，第991–1002页。'
- en: '[32] S. M. Khansari-Zadeh and A. Billard, “Learning stable nonlinear dynamical
    systems with gaussian mixture models,” *IEEE Transactions on Robotics*, vol. 27,
    no. 5, pp. 943–957, 2011.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. M. Khansari-Zadeh 和 A. Billard，“使用高斯混合模型学习稳定的非线性动态系统，” *IEEE机器人学报*，第27卷，第5期，第943–957页，2011年。'
- en: '[33] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    *Advances in Neural Information Processing Systems*, 2020.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. Ho, A. Jain 和 P. Abbeel，“去噪扩散概率模型”，*神经信息处理系统进展*，2020年。'
- en: '[34] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep
    bidirectional transformers for language understanding,” in *Proceedings of NAACL-HLT*,
    2019.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] J. D. M.-W. C. Kenton 和 L. K. Toutanova，“Bert：深度双向变换器的预训练用于语言理解”，见 *NAACL-HLT
    会议论文集*，2019年。'
- en: '[35] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole,
    “Score-based generative modeling through stochastic differential equations,” *arXiv
    preprint arXiv:2011.13456*, 2020.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon 和 B. Poole，“通过随机微分方程的基于分数的生成建模”，*arXiv
    预印本 arXiv:2011.13456*，2020年。'
- en: '[36] M. Janner, Y. Du, J. Tenenbaum, and S. Levine, “Planning with diffusion
    for flexible behavior synthesis,” in *International Conference on Machine Learning*,
    2022.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] M. Janner, Y. Du, J. Tenenbaum 和 S. Levine，“通过扩散规划灵活的行为合成”，见 *国际机器学习会议*，2022年。'
- en: '[37] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan,
    K. Hausman, A. Herzog, J. Hsu *et al.*, “Rt-1: Robotics transformer for real-world
    control at scale,” *arXiv preprint arXiv:2212.06817*, 2022.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan,
    K. Hausman, A. Herzog, J. Hsu *等*，“Rt-1：用于大规模现实世界控制的机器人变换器”，*arXiv 预印本 arXiv:2212.06817*，2022年。'
- en: '[38] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in *Conference
    on Learning Representations, ICLR 2014*, Y. Bengio and Y. LeCun, Eds., 2014.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] D. P. Kingma 和 M. Welling，“自编码变分贝叶斯”，见 *学习表征会议，ICLR 2014*，Y. Bengio 和
    Y. LeCun 主编，2014年。'
- en: '[39] J. Wu, X. Sun, A. Zeng, S. Song, J. Lee, S. Rusinkiewicz, and T. Funkhouser,
    “Spatial action maps for mobile manipulation,” in *16th Robotics: Science and
    Systems, RSS 2020*.   MIT Press Journals, 2020.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. Wu, X. Sun, A. Zeng, S. Song, J. Lee, S. Rusinkiewicz 和 T. Funkhouser，“用于移动操控的空间动作图”，见
    *第16届机器人：科学与系统会议，RSS 2020*。MIT Press Journals，2020年。'
- en: '[40] A. Zeng, S. Song, K.-T. Yu, E. Donlon, F. R. Hogan, M. Bauza, D. Ma, O. Taylor,
    M. Liu, E. Romo *et al.*, “Robotic pick-and-place of novel objects in clutter
    with multi-affordance grasping and cross-domain image matching,” *The International
    Journal of Robotics Research*, 2022.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. Zeng, S. Song, K.-T. Yu, E. Donlon, F. R. Hogan, M. Bauza, D. Ma, O.
    Taylor, M. Liu, E. Romo *等*，“在混乱中通过多功能抓取和跨领域图像匹配的机器人取放新物体”，*国际机器人研究期刊*，2022年。'
- en: '[41] H. Ha and S. Song, “Flingbot: The unreasonable effectiveness of dynamic
    manipulation for cloth unfolding,” in *Conference on Robot Learning*.   PMLR,
    2022, pp. 24–33.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H. Ha 和 S. Song，“Flingbot：动态操控在布料展开中的非凡效果”，见 *机器人学习会议*。PMLR，2022年，第24–33页。'
- en: '[42] X. Jia, D. Blessing, X. Jiang, M. Reuss, A. Donat, R. Lioutikov, and G. Neumann,
    “Towards diverse behaviors: A benchmark for imitation learning with human demonstrations,”
    *International Conference on Learning Representations (ICLR)*, 2024.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] X. Jia, D. Blessing, X. Jiang, M. Reuss, A. Donat, R. Lioutikov 和 G. Neumann，“朝向多样行为：带有人类演示的模仿学习基准”，*国际学习表征会议（ICLR）*，2024年。'
- en: '[43] M. T. Spaan, “Partially observable markov decision processes,” in *Reinforcement
    learning: State-of-the-art*.   Springer, 2012, pp. 387–414.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] M. T. Spaan，“部分可观察的马尔可夫决策过程”，见 *强化学习：最前沿*。Springer，2012年，第387–414页。'
- en: '[44] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei,
    S. Savarese, Y. Zhu, and R. Martín-Martín, “What matters in learning from offline
    human demonstrations for robot manipulation,” in *Conference on Robot Learning*.   PMLR,
    2022, pp. 1678–1690.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei,
    S. Savarese, Y. Zhu 和 R. Martín-Martín，“从离线人类演示中学习对于机器人操作的重要性”，见 *机器人学习会议*。PMLR，2022年，第1678–1690页。'
- en: '[45] M. Laskey, J. Lee, R. Fox, A. Dragan, and K. Goldberg, “Dart: Noise injection
    for robust imitation learning,” in *Conference on robot learning*.   PMLR, 2017,
    pp. 143–156.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] M. Laskey, J. Lee, R. Fox, A. Dragan 和 K. Goldberg，“Dart：用于鲁棒模仿学习的噪声注入”，见
    *机器人学习会议*。PMLR，2017年，第143–156页。'
- en: '[46] J. Kober, B. Mohler, and J. Peters, “Imitation and reinforcement learning
    for motor primitives with perceptual coupling,” in *From motor learning to interaction
    learning in robots*.   Springer, 2010, pp. 209–225.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Kober, B. Mohler 和 J. Peters，“用于运动原语的模仿与强化学习与感知耦合”，见 *从运动学习到机器人互动学习*。Springer，2010年，第209–225页。'
- en: '[47] B. Ichter, J. Harrison, and M. Pavone, “Learning sampling distributions
    for robot motion planning,” in *2018 IEEE International Conference on Robotics
    and Automation (ICRA)*.   IEEE, 2018, pp. 7087–7094.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] B. Ichter, J. Harrison, 和 M. Pavone, “学习机器人运动规划的采样分布，” 见于 *2018 IEEE 国际机器人与自动化会议
    (ICRA)*。 IEEE, 2018, 页码 7087–7094。'
- en: '[48] A. Singh, H. Liu, G. Zhou, A. Yu, N. Rhinehart, and S. Levine, “Parrot:
    Data-driven behavioral priors for reinforcement learning,” *International Conference
    on Learning Representations*, 2021.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. Singh, H. Liu, G. Zhou, A. Yu, N. Rhinehart, 和 S. Levine, “Parrot:
    数据驱动的行为先验用于强化学习，” *学习表征国际会议*, 2021。'
- en: '[49] A. Billard, S. Calinon, R. Dillmann, and S. Schaal, “Survey: Robot programming
    by demonstration,” Springrer, Tech. Rep., 2008.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] A. Billard, S. Calinon, R. Dillmann, 和 S. Schaal, “综述: 基于示范的机器人编程，” Springrer,
    技术报告, 2008。'
- en: '[50] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey of robot
    learning from demonstration,” *Robotics and autonomous systems*, vol. 57, no. 5,
    pp. 469–483, 2009.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] B. D. Argall, S. Chernova, M. Veloso, 和 B. Browning, “基于示范的机器人学习综述，” *机器人与自主系统*,
    卷 57, 期 5, 页码 469–483, 2009。'
- en: '[51] S. Chernova and A. L. Thomaz, *Robot learning from human teachers*.   Morgan
    & Claypool Publishers, 2014.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] S. Chernova 和 A. L. Thomaz, *从人类教师学习的机器人*。 Morgan & Claypool Publishers,
    2014。'
- en: '[52] A. G. Billard, S. Calinon, and R. Dillmann, “Learning from humans,” *Springer
    handbook of robotics*, pp. 1995–2014, 2016.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] A. G. Billard, S. Calinon, 和 R. Dillmann, “向人类学习，” *Springer 机器人手册*, 页码
    1995–2014, 2016。'
- en: '[53] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne, “Imitation learning:
    A survey of learning methods,” *ACM Computing Surveys (CSUR)*, vol. 50, no. 2,
    pp. 1–35, 2017.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] A. Hussein, M. M. Gaber, E. Elyan, 和 C. Jayne, “模仿学习: 学习方法综述，” *ACM 计算调查
    (CSUR)*, 卷 50, 期 2, 页码 1–35, 2017。'
- en: '[54] H. Ravichandar, A. S. Polydoros, S. Chernova, and A. Billard, “Recent
    advances in robot learning from demonstration,” *Annual review of control, robotics,
    and autonomous systems*, vol. 3, pp. 297–330, 2020.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] H. Ravichandar, A. S. Polydoros, S. Chernova, 和 A. Billard, “基于示范的机器人学习的最新进展，”
    *控制、机器人与自主系统年度回顾*, 卷 3, 页码 297–330, 2020。'
- en: '[55] R. Firoozi, J. Tucker, S. Tian, A. Majumdar, J. Sun, W. Liu, Y. Zhu, S. Song,
    A. Kapoor, K. Hausman *et al.*, “Foundation models in robotics: Applications,
    challenges, and the future,” *arXiv preprint arXiv:2312.07843*, 2023.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] R. Firoozi, J. Tucker, S. Tian, A. Majumdar, J. Sun, W. Liu, Y. Zhu, S.
    Song, A. Kapoor, K. Hausman *等*, “机器人中的基础模型: 应用、挑战与未来，” *arXiv 预印本 arXiv:2312.07843*,
    2023。'
- en: '[56] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y. Xie,
    T. Zhang, Z. Zhao *et al.*, “Toward general-purpose robots via foundation models:
    A survey and meta-analysis,” *arXiv preprint arXiv:2312.08782*, 2023.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y.
    Xie, T. Zhang, Z. Zhao *等*, “通过基础模型迈向通用机器人: 综述与元分析，” *arXiv 预印本 arXiv:2312.08782*,
    2023。'
- en: '[57] C. Eppner, A. Mousavian, and D. Fox, “Acronym: A large-scale grasp dataset
    based on simulation,” in *2021 IEEE International Conference on Robotics and Automation
    (ICRA)*.   IEEE, 2021, pp. 6222–6227.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] C. Eppner, A. Mousavian, 和 D. Fox, “Acronym: 基于仿真的大规模抓取数据集，” 见于 *2021
    IEEE 国际机器人与自动化会议 (ICRA)*。 IEEE, 2021, 页码 6222–6227。'
- en: '[58] J. Urain, N. Funk, G. Chalvatzaki, and J. Peters, “Se(3)-diffusionfields:
    Learning cost functions for joint grasp and motion optimization through diffusion,”
    *IEEE International Conference on Robotics and Automation (ICRA)*, 2023.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] J. Urain, N. Funk, G. Chalvatzaki, 和 J. Peters, “Se(3)-diffusionfields:
    通过扩散学习关节抓取和运动优化的成本函数，” *IEEE 国际机器人与自动化会议 (ICRA)*, 2023。'
- en: '[59] N. M. Shafiullah, Z. Cui, A. A. Altanzaya, and L. Pinto, “Behavior transformers:
    Cloning $k$ modes with one stone,” *Advances in neural information processing
    systems*, vol. 35, pp. 22 955–22 968, 2022.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] N. M. Shafiullah, Z. Cui, A. A. Altanzaya, 和 L. Pinto, “行为变换器: 一石二鸟地克隆
    $k$ 模式，” *神经信息处理系统进展*, 卷 35, 页码 22 955–22 968, 2022。'
- en: '[60] W. Liu, C. Paxton, T. Hermans, and D. Fox, “Structformer: Learning spatial
    structure for language-guided semantic rearrangement of novel objects,” in *2022
    International Conference on Robotics and Automation (ICRA)*.   IEEE, 2022, pp.
    6322–6329.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] W. Liu, C. Paxton, T. Hermans, 和 D. Fox, “Structformer: 为语言指导的新颖物体的语义重排学习空间结构，”
    见于 *2022 国际机器人与自动化会议 (ICRA)*。 IEEE, 2022, 页码 6322–6329。'
- en: '[61] T. S. Lembono, E. Pignat, J. Jankowski, and S. Calinon, “Generative adversarial
    network to learn valid distributions of robot configurations for inverse kinematics
    and constrained motion planning,” *CoRR, abs/2011.05717*, 2020.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] T. S. Lembono, E. Pignat, J. Jankowski, 和 S. Calinon, “生成对抗网络学习机器人配置的有效分布，用于逆向运动学和受限运动规划，”
    *CoRR, abs/2011.05717*, 2020。'
- en: '[62] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong,
    I. Krasin, D. Duong, V. Sindhwani *et al.*, “Transporter networks: Rearranging
    the visual world for robotic manipulation,” in *Conference on Robot Learning*,
    2021.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T.
    Armstrong, I. Krasin, D. Duong, V. Sindhwani *等*，“运输网络：为机器人操作重新排列视觉世界，” 见于 *机器人学习会议*，2021年。'
- en: '[63] K. Mo, L. J. Guibas, M. Mukadam, A. Gupta, and S. Tulsiani, “Where2act:
    From pixels to actions for articulated 3d objects,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 6813–6823.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] K. Mo, L. J. Guibas, M. Mukadam, A. Gupta, 和 S. Tulsiani, “Where2act:
    从像素到动作的关节3D对象，” 见于 *IEEE/CVF 国际计算机视觉会议论文集*，2021年，第6813–6823页。'
- en: '[64] H. Ha, P. Florence, and S. Song, “Scaling up and distilling down: Language-guided
    robot skill acquisition,” in *Conference on Robot Learning*, 2023.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] H. Ha, P. Florence, 和 S. Song, “扩展与提炼：语言指导的机器人技能获取，” 见于 *机器人学习会议*，2023年。'
- en: '[65] B. Ames, J. Morgan, and G. Konidaris, “Ikflow: Generating diverse inverse
    kinematics solutions,” *IEEE Robotics and Automation Letters*, 2022.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] B. Ames, J. Morgan, 和 G. Konidaris, “Ikflow: 生成多样的逆运动学解，” *IEEE 机器人与自动化快报*，2022年。'
- en: '[66] A. Mandlekar, D. Xu, R. Martín-Martín, S. Savarese, and L. Fei-Fei, “Learning
    to generalize across long-horizon tasks from human demonstrations,” *arXiv preprint
    arXiv:2003.06085*, 2020.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] A. Mandlekar, D. Xu, R. Martín-Martín, S. Savarese, 和 L. Fei-Fei, “从人类演示中学习在长时间任务中的泛化能力，”
    *arXiv 预印本 arXiv:2003.06085*，2020年。'
- en: '[67] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang, “A tutorial
    on energy-based learning,” *Predicting structured data*, vol. 1, no. 0, 2006.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, 和 F. Huang, “关于基于能量的学习的教程，”
    *预测结构化数据*，第1卷，第0期，2006年。'
- en: '[68] Y. Song and D. P. Kingma, “How to train your energy-based models,” *arXiv
    preprint arXiv:2101.03288*, 2021.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Y. Song 和 D. P. Kingma, “如何训练你的基于能量的模型，” *arXiv 预印本 arXiv:2101.03288*，2021年。'
- en: '[69] Y. Song and S. Ermon, “Generative modeling by estimating gradients of
    the data distribution,” *Advances in Neural Information Processing Systems*, 2019.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Y. Song 和 S. Ermon, “通过估计数据分布的梯度进行生成建模，” *神经信息处理系统进展*，2019年。'
- en: '[70] A. Simeonov, Y. Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez, P. Agrawal,
    and V. Sitzmann, “Neural descriptor fields: Se(3)-equivariant object representations
    for manipulation,” in *International Conference on Robotics and Automation*.   IEEE,
    2022.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] A. Simeonov, Y. Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez, P.
    Agrawal, 和 V. Sitzmann, “神经描述符场：用于操作的Se(3)-等变对象表示，” 见于 *国际机器人与自动化会议*。 IEEE，2022年。'
- en: '[71] D. Rezende and S. Mohamed, “Variational inference with normalizing flows,”
    in *International Conference on Machine Learning*, 2015.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] D. Rezende 和 S. Mohamed, “利用归一化流的变分推断，” 见于 *国际机器学习会议*，2015年。'
- en: '[72] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski,
    T. Ding, D. Driess, A. Dubey, C. Finn *et al.*, “Rt-2: Vision-language-action
    models transfer web knowledge to robotic control,” *arXiv preprint arXiv:2307.15818*,
    2023.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski,
    T. Ding, D. Driess, A. Dubey, C. Finn *等*，“Rt-2: 视觉-语言-行动模型将网络知识转移到机器人控制，” *arXiv
    预印本 arXiv:2307.15818*，2023年。'
- en: '[73] A. Zeng, S. Song, S. Welker, J. Lee, A. Rodriguez, and T. Funkhouser,
    “Learning synergies between pushing and grasping with self-supervised deep reinforcement
    learning,” in *2018 IEEE/RSJ International Conference on Intelligent Robots and
    Systems (IROS)*.   IEEE, 2018, pp. 4238–4245.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] A. Zeng, S. Song, S. Welker, J. Lee, A. Rodriguez, 和 T. Funkhouser, “通过自监督深度强化学习学习推挤与抓取之间的协同作用，”
    见于 *2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*。
    IEEE，2018年，第4238–4245页。'
- en: '[74] A. Zeng, S. Song, J. Lee, A. Rodriguez, and T. Funkhouser, “Tossingbot:
    Learning to throw arbitrary objects with residual physics,” *IEEE Transactions
    on Robotics*, vol. 36, no. 4, pp. 1307–1319, 2020.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] A. Zeng, S. Song, J. Lee, A. Rodriguez, 和 T. Funkhouser, “Tossingbot:
    学习用残差物理学抛掷任意物体，” *IEEE 机器人学杂志*，第36卷，第4期，第1307–1319页，2020年。'
- en: '[75] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems*, 2014.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio, “生成对抗网络，” 见于 *神经信息处理系统进展*，2014年。'
- en: '[76] M. Mohammadi, A. Al-Fuqaha, and J.-S. Oh, “Path planning in support of
    smart mobility applications using generative adversarial networks,” in *IEEE International
    Conference on Internet of Things*, 2018.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] M. Mohammadi, A. Al-Fuqaha, 和 J.-S. Oh, “使用生成对抗网络支持智能移动应用的路径规划，” 见于 *IEEE
    国际物联网会议*，2018年。'
- en: '[77] J. Ortiz-Haro, J.-S. Ha, D. Driess, and M. Toussaint, “Structured deep
    generative models for sampling on constraint manifolds in sequential manipulation,”
    in *Conference on Robot Learning*, 2022.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] J. Ortiz-Haro, J.-S. Ha, D. Driess, 和 M. Toussaint，“用于约束流形上采样的结构化深度生成模型”，发表于*机器人学习会议*，2022年。'
- en: '[78] T. Lai and F. Ramos, “Plannerflows: Learning motion samplers with normalising
    flows,” in *IEEE/RSJ International Conference on Intelligent Robots and Systems
    (IROS)*, 2021.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] T. Lai 和 F. Ramos，“Plannerflows：通过归一化流学习运动采样器”，发表于*IEEE/RSJ 国际智能机器人与系统会议
    (IROS)*，2021年。'
- en: '[79] T. Lai, W. Zhi, T. Hermans, and F. Ramos, “Parallelised diffeomorphic
    sampling-based motion planning,” in *Conference on Robot Learning*, 2022.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] T. Lai, W. Zhi, T. Hermans, 和 F. Ramos，“并行化的流形采样基础运动规划”，发表于*机器人学习会议*，2022年。'
- en: '[80] S. LaValle, “Rapidly-exploring random trees: A new tool for path planning,”
    *Research Report 9811*, 1998.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] S. LaValle，“快速探索随机树：一种新的路径规划工具”，*研究报告9811*，1998年。'
- en: '[81] L. E. Kavraki, P. Svestka, J.-C. Latombe, and M. H. Overmars, “Probabilistic
    roadmaps for path planning in high-dimensional configuration spaces,” *IEEE transactions
    on Robotics and Automation*, vol. 12, no. 4, pp. 566–580, 1996.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] L. E. Kavraki, P. Svestka, J.-C. Latombe, 和 M. H. Overmars，“高维配置空间路径规划的概率道路图”，*IEEE
    机器人与自动化学报*，第12卷，第4期，页码566–580，1996年。'
- en: '[82] H. Van Hoof, N. Chen, M. Karl, P. van der Smagt, and J. Peters, “Stable
    reinforcement learning with autoencoders for tactile and visual data,” in *2016
    IEEE/RSJ international conference on intelligent robots and systems (IROS)*.   IEEE,
    2016, pp. 3928–3934.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] H. Van Hoof, N. Chen, M. Karl, P. van der Smagt, 和 J. Peters，“使用自编码器进行稳定的强化学习，以处理触觉和视觉数据”，发表于*2016
    IEEE/RSJ 国际智能机器人与系统会议 (IROS)*。IEEE，2016年，页码3928–3934。'
- en: '[83] K. Pertsch, Y. Lee, and J. Lim, “Accelerating reinforcement learning with
    learned skill priors,” in *Conference on Robot Learning*, 2021.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] K. Pertsch, Y. Lee, 和 J. Lim，“通过学习的技能先验加速强化学习”，发表于*机器人学习会议*，2021年。'
- en: '[84] C. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine, and P. Sermanet,
    “Learning latent plans from play,” in *Conference on robot learning*, 2020.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] C. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine, 和 P.
    Sermanet，“从游戏中学习潜在计划”，发表于*机器人学习会议*，2020年。'
- en: '[85] A. Mandlekar, F. Ramos, B. Boots, S. Savarese, L. Fei-Fei, A. Garg, and
    D. Fox, “Iris: Implicit reinforcement without interaction at scale for learning
    control from offline robot manipulation data,” in *2020 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2020, pp. 4414–4420.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] A. Mandlekar, F. Ramos, B. Boots, S. Savarese, L. Fei-Fei, A. Garg, 和
    D. Fox，“Iris：在大规模下无交互的隐式强化学习，以便从离线机器人操作数据中学习控制”，发表于*2020 IEEE 国际机器人与自动化会议 (ICRA)*。IEEE，2020年，页码4414–4420。'
- en: '[86] M. Yan, A. Li, M. Kalakrishnan, and P. Pastor, “Learning probabilistic
    multi-modal actor models for vision-based robotic grasping,” in *2019 International
    Conference on Robotics and Automation (ICRA)*.   IEEE, 2019, pp. 4804–4810.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] M. Yan, A. Li, M. Kalakrishnan, 和 P. Pastor，“学习基于视觉的机器人抓取的概率多模态演员模型”，发表于*2019
    国际机器人与自动化会议 (ICRA)*。IEEE，2019年，页码4804–4810。'
- en: '[87] J. Urain, M. Ginesi, D. Tateo, and J. Peters, “Imitationflows: Learning
    deep stable stochastic dynamic systems by normalizing flows,” in *IEEE/RSJ International
    Conference on Intelligent Robots and Systems*, 2020.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] J. Urain, M. Ginesi, D. Tateo, 和 J. Peters，“Imitationflows：通过归一化流学习深度稳定的随机动态系统”，发表于*IEEE/RSJ
    国际智能机器人与系统会议*，2020年。'
- en: '[88] M. A. Rana, A. Li, D. Fox, B. Boots, F. Ramos, and N. Ratliff, “Euclideanizing
    flows: Diffeomorphic reduction for learning stable dynamical systems,” in *Learning
    for Dynamics and Control*.   PMLR, 2020, pp. 630–639.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] M. A. Rana, A. Li, D. Fox, B. Boots, F. Ramos, 和 N. Ratliff，“欧几里得化流：通过流形缩减学习稳定的动态系统”，发表于*动态与控制学习*。PMLR，2020年，页码630–639。'
- en: '[89] G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan,
    “Normalizing flows for probabilistic modeling and inference,” *arXiv preprint
    arXiv:1912.02762*, 2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, 和 B. Lakshminarayanan，“用于概率建模和推断的归一化流”，*arXiv
    预印本 arXiv:1912.02762*，2019年。'
- en: '[90] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud, “Neural ordinary
    differential equations,” in *Advances in Neural Information Processing Systems*,
    2018.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] R. T. Chen, Y. Rubanova, J. Bettencourt, 和 D. K. Duvenaud，“神经普通微分方程”，发表于*神经信息处理系统进展*，2018年。'
- en: '[91] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, and P. Abbeel, “Deep
    spatial autoencoders for visuomotor learning,” in *2016 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2016, pp. 512–519.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, 和 P. Abbeel, “用于视觉运动学习的深度空间自编码器，”
    收录于 *2016 IEEE国际机器人与自动化会议（ICRA）*。IEEE，2016年，第512–519页。'
- en: '[92] T. Weng, D. Held, F. Meier, and M. Mukadam, “Neural grasp distance fields
    for robot manipulation,” in *2023 IEEE International Conference on Robotics and
    Automation (ICRA)*.   IEEE, 2023, pp. 1814–1821.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] T. Weng, D. Held, F. Meier, 和 M. Mukadam, “用于机器人操控的神经抓取距离场，” 收录于 *2023
    IEEE国际机器人与自动化会议（ICRA）*。IEEE，2023年，第1814–1821页。'
- en: '[93] J. Urain, A. Li, P. Liu, C. D’Eramo, and J. Peters, “Composable energy
    policies for reactive motion generation and reinforcement learning,” *The International
    Journal of Robotics Research (IJRR)*, 2023.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Urain, A. Li, P. Liu, C. D’Eramo, 和 J. Peters, “可组合的能源政策用于反应式运动生成和强化学习，”
    *国际机器人研究期刊（IJRR）*，2023年。'
- en: '[94] P. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A. Wong,
    J. Lee, I. Mordatch, and J. Tompson, “Implicit behavioral cloning,” in *Conference
    on Robot Learning*, 2022.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] P. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A.
    Wong, J. Lee, I. Mordatch, 和 J. Tompson, “隐式行为克隆，” 收录于 *机器人学习会议*，2022年。'
- en: '[95] Y. Du, S. Li, and I. Mordatch, “Compositional visual generation and inference
    with energy based models,” in *Advances in Neural Information Processing Systems*,
    2020.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Y. Du, S. Li, 和 I. Mordatch, “基于能量的模型的组合视觉生成与推理，” 收录于 *神经信息处理系统进展*，2020年。'
- en: '[96] N. Gkanatsios, A. Jain, Z. Xian, Y. Zhang, C. G. Atkeson, and K. Fragkiadaki,
    “Energy-based models are zero-shot planners for compositional scene rearrangement,”
    in *RSS 2023 Workshop on Learning for Task and Motion Planning*, 2023.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] N. Gkanatsios, A. Jain, Z. Xian, Y. Zhang, C. G. Atkeson, 和 K. Fragkiadaki,
    “基于能量的模型是组合场景重排的零样本规划者，” 收录于 *RSS 2023任务与运动规划学习研讨会*，2023年。'
- en: '[97] M. Kalakrishnan, P. Pastor, L. Righetti, and S. Schaal, “Learning objective
    functions for manipulation,” in *2013 IEEE International Conference on Robotics
    and Automation*.   IEEE, 2013, pp. 1331–1336.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] M. Kalakrishnan, P. Pastor, L. Righetti, 和 S. Schaal, “用于操控的目标函数学习，” 收录于
    *2013 IEEE国际机器人与自动化会议*。IEEE，2013年，第1331–1336页。'
- en: '[98] C. Finn, S. Levine, and P. Abbeel, “Guided cost learning: Deep inverse
    optimal control via policy optimization,” in *International Conference on Machine
    Learning*, 2016.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] C. Finn, S. Levine, 和 P. Abbeel, “引导成本学习：通过策略优化进行深度逆最优控制，” 收录于 *国际机器学习会议*，2016年。'
- en: '[99] Y. Du and I. Mordatch, “Implicit generation and modeling with energy based
    models,” *Advances in Neural Information Processing Systems*, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Y. Du 和 I. Mordatch, “基于能量的模型的隐式生成与建模，” *神经信息处理系统进展*，2019年。'
- en: '[100] T. Gervet, Z. Xian, N. Gkanatsios, and K. Fragkiadaki, “Act3d: Infinite
    resolution action detection transformer for robotic manipulation,” *arXiv preprint
    arXiv:2306.17817*, 2023.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] T. Gervet, Z. Xian, N. Gkanatsios, 和 K. Fragkiadaki, “Act3d: 用于机器人操控的无限分辨率动作检测变换器，”
    *arXiv预印本arXiv:2306.17817*，2023年。'
- en: '[101] Y. Du, T. Lin, and I. Mordatch, “Model based planning with energy based
    models,” *CORL*, 2019.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Y. Du, T. Lin, 和 I. Mordatch, “基于模型的规划与基于能量的模型，” *CORL*，2019年。'
- en: '[102] P. Sodhi, E. Dexheimer, M. Mukadam, S. Anderson, and M. Kaess, “Leo:
    Learning energy-based models in factor graph optimization,” in *Conference on
    Robot Learning*.   PMLR, 2022, pp. 234–244.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] P. Sodhi, E. Dexheimer, M. Mukadam, S. Anderson, 和 M. Kaess, “Leo: 在因子图优化中学习基于能量的模型，”
    收录于 *机器人学习会议*。PMLR，2022年，第234–244页。'
- en: '[103] G. E. Hinton, “Training products of experts by minimizing contrastive
    divergence,” *Neural computation*, vol. 14, no. 8, pp. 1771–1800, 2002.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] G. E. Hinton, “通过最小化对比散度训练专家产品，” *神经计算*，第14卷，第8期，第1771–1800页，2002年。'
- en: '[104] Y. Du, S. Li, J. Tenenbaum, and I. Mordatch, “Improved contrastive divergence
    training of energy based models,” *arXiv preprint arXiv:2012.01316*, 2020.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Y. Du, S. Li, J. Tenenbaum, 和 I. Mordatch, “改进的基于能量的模型对比散度训练，” *arXiv预印本arXiv:2012.01316*，2020年。'
- en: '[105] Z. Jiang, Y. Zhu, M. Svetlik, K. Fang, and Y. Zhu, “Synergies between
    affordance and geometry: 6-dof grasp detection via implicit representations,”
    *Proceedings of Robotics: Science and Systems (R:SS)*, 2021.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Z. Jiang, Y. Zhu, M. Svetlik, K. Fang, 和 Y. Zhu, “可用性与几何之间的协同：通过隐式表示进行6自由度抓取检测，”
    *机器人科学与系统会议（R:SS）*，2021年。'
- en: '[106] A. Simeonov, Y. Du, Y.-C. Lin, A. R. Garcia, L. P. Kaelbling, T. Lozano-Pérez,
    and P. Agrawal, “Se (3)-equivariant relational rearrangement with neural descriptor
    fields,” in *Conference on Robot Learning*.   PMLR, 2023, pp. 835–846.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] A. Simeonov, Y. Du, Y.-C. Lin, A. R. Garcia, L. P. Kaelbling, T. Lozano-Pérez,
    和 P. Agrawal，“具有神经描述符场的 SE (3)-等变关系重新排列，”在 *机器人学习会议*。 PMLR，2023年，第835–846页。'
- en: '[107] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam,
    “Clip-fields: Weakly supervised semantic fields for robotic memory,” *arXiv preprint
    arXiv:2210.05663*, 2022.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, 和 A. Szlam，“Clip-fields：用于机器人记忆的弱监督语义场，”
    *arXiv 预印本 arXiv:2210.05663*，2022年。'
- en: '[108] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *et al.*, “Learning transferable visual models
    from natural language supervision,” in *International Conference on Machine Learning*,
    2021.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G.
    Sastry, A. Askell, P. Mishkin, J. Clark *等*，“从自然语言监督中学习可转移的视觉模型，”在 *国际机器学习会议*，2021年。'
- en: '[109] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep
    unsupervised learning using nonequilibrium thermodynamics,” in *International
    Conference on Machine Learning*, 2015.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, 和 S. Ganguli，“利用非平衡热力学的深度无监督学习，”在
    *国际机器学习会议*，2015年。'
- en: '[110] P. Vincent, “A connection between score matching and denoising autoencoders,”
    *Neural computation*, vol. 23, no. 7, pp. 1661–1674, 2011.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] P. Vincent，“分数匹配与去噪自编码器之间的联系，” *神经计算*，第23卷，第7期，第1661–1674页，2011年。'
- en: '[111] N. Liu, S. Li, Y. Du, A. Torralba, and J. B. Tenenbaum, “Compositional
    visual generation with composable diffusion models,” in *European Conference on
    Computer Vision*.   Springer, 2022, pp. 423–439.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] N. Liu, S. Li, Y. Du, A. Torralba, 和 J. B. Tenenbaum，“使用可组合扩散模型的组合视觉生成，”在
    *欧洲计算机视觉会议*。 Springer，2022年，第423–439页。'
- en: '[112] Y. Du, C. Durkan, R. Strudel, J. B. Tenenbaum, S. Dieleman, R. Fergus,
    J. Sohl-Dickstein, A. Doucet, and W. S. Grathwohl, “Reduce, reuse, recycle: Compositional
    generation with energy-based diffusion models and mcmc,” in *International Conference
    on Machine Learning*.   PMLR, 2023, pp. 8489–8510.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Y. Du, C. Durkan, R. Strudel, J. B. Tenenbaum, S. Dieleman, R. Fergus,
    J. Sohl-Dickstein, A. Doucet, 和 W. S. Grathwohl，“减少、重复使用、回收：基于能量的扩散模型和马尔科夫链蒙特卡罗的组合生成，”在
    *国际机器学习会议*。 PMLR，2023年，第8489–8510页。'
- en: '[113] A. Prasad, K. Lin, J. Wu, L. Zhou, and J. Bohg, “Consistency policy:
    Accelerated visuomotor policies via consistency distillation,” *arXiv preprint
    arXiv:2405.07503*, 2024.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Prasad, K. Lin, J. Wu, L. Zhou, 和 J. Bohg，“一致性策略：通过一致性蒸馏加速视觉运动策略，”
    *arXiv 预印本 arXiv:2405.07503*，2024年。'
- en: '[114] A. Ajay, Y. Du, A. Gupta, J. B. Tenenbaum, T. S. Jaakkola, and P. Agrawal,
    “Is conditional generative modeling all you need for decision making?” in *International
    Conference on Learning Representations*, 2022.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] A. Ajay, Y. Du, A. Gupta, J. B. Tenenbaum, T. S. Jaakkola, 和 P. Agrawal，“条件生成建模是否是决策制定所需的一切？”在
    *国际学习表征会议*，2022年。'
- en: '[115] J. Carvalho, A. T. Le, M. Baierl, D. Koert, and J. Peters, “Motion planning
    diffusion: Learning and planning of robot motions with diffusion models,” *arXiv
    preprint arXiv:2308.01557*, 2023.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J. Carvalho, A. T. Le, M. Baierl, D. Koert, 和 J. Peters，“运动规划扩散：使用扩散模型的机器人运动学习与规划，”
    *arXiv 预印本 arXiv:2308.01557*，2023年。'
- en: '[116] U. Mishra, S. Xue, Y. Chen, and D. Xu, “Generative skill chaining: Long-horizon
    skill planning with diffusion models,” in *CoRL 2023 Workshop on Learning Effective
    Abstractions for Planning (LEAP)*, 2023.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] U. Mishra, S. Xue, Y. Chen, 和 D. Xu，“生成技能链：使用扩散模型的长期技能规划，”在 *CoRL 2023
    规划有效抽象学习（LEAP）工作坊*，2023年。'
- en: '[117] Z. Yang, J. Mao, Y. Du, J. Wu, J. B. Tenenbaum, T. Lozano-Pérez, and
    L. P. Kaelbling, “Compositional diffusion-based continuous constraint solvers,”
    *arXiv preprint arXiv:2309.00966*, 2023.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Z. Yang, J. Mao, Y. Du, J. Wu, J. B. Tenenbaum, T. Lozano-Pérez, 和 L.
    P. Kaelbling，“基于组合扩散的连续约束求解器，” *arXiv 预印本 arXiv:2309.00966*，2023年。'
- en: '[118] P. Dhariwal and A. Nichol, “Diffusion models beat gans on image synthesis,”
    *Advances in neural information processing systems*, vol. 34, pp. 8780–8794, 2021.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] P. Dhariwal 和 A. Nichol，“扩散模型在图像合成上超越 GANs，” *神经信息处理系统进展*，第34卷，第8780–8794页，2021年。'
- en: '[119] J. Ho and T. Salimans, “Classifier-free diffusion guidance,” in *NeurIPS
    2021 Workshop on Deep Generative Models and Downstream Applications*, 2021.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] J. Ho 和 T. Salimans，“无分类器的扩散指导，”在 *NeurIPS 2021 深度生成模型与下游应用工作坊*，2021年。'
- en: '[120] L. Huang, D. Chen, Y. Liu, Y. Shen, D. Zhao, and J. Zhou, “Composer:
    Creative and controllable image synthesis with composable conditions,” *arXiv
    preprint arXiv:2302.09778*, 2023.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] L. Huang, D. Chen, Y. Liu, Y. Shen, D. Zhao, 和 J. Zhou，“Composer：具有可组合条件的创意和可控图像合成”，*arXiv预印本
    arXiv:2302.09778*，2023年。'
- en: '[121] W. H. Kwon and S. H. Han, *Receding horizon control: model predictive
    control for state models*.   Springer Science & Business Media, 2005.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] W. H. Kwon 和 S. H. Han，*递归视界控制：用于状态模型的模型预测控制*。Springer科学与商业媒体，2005年。'
- en: '[122] M. Reuss and R. Lioutikov, “Multimodal diffusion transformer for learning
    from play,” in *2nd Workshop on Language and Robot Learning: Language as Grounding*,
    2023.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] M. Reuss 和 R. Lioutikov，“用于游戏学习的多模态扩散变换器”，在*第二届语言与机器人学习研讨会：语言作为基础*，2023年。'
- en: '[123] S. Huang, Z. Wang, P. Li, B. Jia, T. Liu, Y. Zhu, W. Liang, and S.-C.
    Zhu, “Diffusion-based generation, optimization, and planning in 3d scenes,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2023, pp. 16 750–16 761.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] S. Huang, Z. Wang, P. Li, B. Jia, T. Liu, Y. Zhu, W. Liang, 和 S.-C. Zhu，“基于扩散的3D场景生成、优化与规划”，在*IEEE/CVF计算机视觉与模式识别会议论文集*，2023年，第16,750–16,761页。'
- en: '[124] U. A. Mishra and Y. Chen, “Reorientdiff: Diffusion model based reorientation
    for object manipulation,” *arXiv preprint arXiv:2303.12700*, 2023.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] U. A. Mishra 和 Y. Chen，“Reorientdiff：基于扩散模型的物体操作再定向”，*arXiv预印本 arXiv:2303.12700*，2023年。'
- en: '[125] I. Kapelyukh, V. Vosylius, and E. Johns, “Dall-e-bot: Introducing web-scale
    diffusion models to robotics,” *IEEE Robotics and Automation Letters*, 2023.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] I. Kapelyukh, V. Vosylius, 和 E. Johns，“Dall-e-bot：将网络规模扩散模型引入机器人技术”，*IEEE机器人与自动化快报*，2023年。'
- en: '[126] Y. Du, M. Yang, P. Florence, F. Xia, A. Wahid, B. Ichter, P. Sermanet,
    T. Yu, P. Abbeel, J. B. Tenenbaum *et al.*, “Video language planning,” *arXiv
    preprint arXiv:2310.10625*, 2023.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Y. Du, M. Yang, P. Florence, F. Xia, A. Wahid, B. Ichter, P. Sermanet,
    T. Yu, P. Abbeel, J. B. Tenenbaum *等*，“视频语言规划”，*arXiv预印本 arXiv:2310.10625*，2023年。'
- en: '[127] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. B. Tenenbaum, D. Schuurmans,
    and P. Abbeel, “Learning universal policies via text-guided video generation,”
    in *Thirty-seventh Conference on Neural Information Processing Systems*, 2023.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. B. Tenenbaum, D. Schuurmans,
    和 P. Abbeel，“通过文本引导的视频生成学习通用策略”，在*第37届神经信息处理系统会议*，2023年。'
- en: '[128] A. Ajay, S. Han, Y. Du, S. Li, A. Gupta, T. Jaakkola, J. Tenenbaum, L. Kaelbling,
    A. Srivastava, and P. Agrawal, “Compositional foundation models for hierarchical
    planning,” *arXiv preprint arXiv:2309.08587*, 2023.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. Ajay, S. Han, Y. Du, S. Li, A. Gupta, T. Jaakkola, J. Tenenbaum, L.
    Kaelbling, A. Srivastava, 和 P. Agrawal，“用于分层规划的组合基础模型”，*arXiv预印本 arXiv:2309.08587*，2023年。'
- en: '[129] C. Higuera, B. Boots, and M. Mukadam, “Learning to read braille: Bridging
    the tactile reality gap with diffusion models,” *arXiv preprint arXiv:2304.01182*,
    2023.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] C. Higuera, B. Boots, 和 M. Mukadam，“学习阅读盲文：利用扩散模型弥合触觉现实差距”，*arXiv预印本
    arXiv:2304.01182*，2023年。'
- en: '[130] P. M. Scheikl, N. Schreiber, C. Haas, N. Freymuth, G. Neumann, R. Lioutikov,
    and F. Mathis-Ullrich, “Movement primitive diffusion: Learning gentle robotic
    manipulation of deformable objects,” *IEEE Robotics and Automation Letters*, 2024.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] P. M. Scheikl, N. Schreiber, C. Haas, N. Freymuth, G. Neumann, R. Lioutikov,
    和 F. Mathis-Ullrich，“运动原型扩散：学习柔性物体的温和机器人操作”，*IEEE机器人与自动化快报*，2024年。'
- en: '[131] A. Goyal, J. Xu, Y. Guo, V. Blukis, Y.-W. Chao, and D. Fox, “Rvt: Robotic
    view transformer for 3d object manipulation,” *arXiv preprint arXiv:2306.14896*,
    2023.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] A. Goyal, J. Xu, Y. Guo, V. Blukis, Y.-W. Chao, 和 D. Fox，“Rvt：用于3D物体操作的机器人视图变换器”，*arXiv预印本
    arXiv:2306.14896*，2023年。'
- en: '[132] M. Breyer, J. J. Chung, L. Ott, R. Siegwart, and J. Nieto, “Volumetric
    grasping network: Real-time 6 dof grasp detection in clutter,” *arXiv preprint
    arXiv:2101.01132*, 2021.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] M. Breyer, J. J. Chung, L. Ott, R. Siegwart, 和 J. Nieto，“体积抓取网络：在杂乱环境中的实时6自由度抓取检测”，*arXiv预印本
    arXiv:2101.01132*，2021年。'
- en: '[133] M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What and where pathways
    for robotic manipulation,” in *Conference on Robot Learning*, 2022.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] M. Shridhar, L. Manuelli, 和 D. Fox，“Cliport：机器人操作的‘是什么’与‘在哪里’路径”，在*机器人学习会议*，2022年。'
- en: '[134] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *Advances in Neural Information Processing Systems*, 2020.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell *等*，“语言模型是少样本学习者”，*神经信息处理系统进展*，2020年。'
- en: '[135] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in Neural
    Information Processing Systems*, 2017.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] A. Vaswani、N. Shazeer、N. Parmar、J. Uszkoreit、L. Jones、A. N. Gomez、Ł.
    Kaiser 和 I. Polosukhin，“注意力机制才是你需要的，” *神经信息处理系统进展*，2017 年。'
- en: '[136] A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves
    *et al.*, “Conditional image generation with pixelcnn decoders,” *Advances in
    neural information processing systems*, vol. 29, 2016.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] A. Van den Oord、N. Kalchbrenner、L. Espeholt、O. Vinyals、A. Graves *等*，“使用
    pixelcnn 解码器的条件图像生成，” *神经信息处理系统进展*，第 29 卷，2016 年。'
- en: '[137] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn,
    C. Fu, K. Gopalakrishnan, K. Hausman *et al.*, “Do as i can, not as i say: Grounding
    language in robotic affordances,” *arXiv preprint arXiv:2204.01691*, 2022.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] M. Ahn、A. Brohan、N. Brown、Y. Chebotar、O. Cortes、B. David、C. Finn、C. Fu、K.
    Gopalakrishnan、K. Hausman *等*，“言行一致：将语言基础化为机器人功能，” *arXiv 预印本 arXiv:2204.01691*，2022
    年。'
- en: '[138] K.-H. Lee, O. Nachum, M. S. Yang, L. Lee, D. Freeman, S. Guadarrama,
    I. Fischer, W. Xu, E. Jang, H. Michalewski *et al.*, “Multi-game decision transformers,”
    *Advances in Neural Information Processing Systems*, vol. 35, pp. 27 921–27 936,
    2022.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] K.-H. Lee、O. Nachum、M. S. Yang、L. Lee、D. Freeman、S. Guadarrama、I. Fischer、W.
    Xu、E. Jang、H. Michalewski *等*，“多游戏决策变换器，” *神经信息处理系统进展*，第 35 卷，页码 27 921–27 936，2022
    年。'
- en: '[139] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron,
    M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg *et al.*, “A generalist agent,”
    *arXiv preprint arXiv:2205.06175*, 2022.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] S. Reed、K. Zolna、E. Parisotto、S. G. Colmenarejo、A. Novikov、G. Barth-Maron、M.
    Gimenez、Y. Sulsky、J. Kay、J. T. Springenberg *等*，“一个通用智能体，” *arXiv 预印本 arXiv:2205.06175*，2022
    年。'
- en: '[140] M. Janner, Q. Li, and S. Levine, “Offline reinforcement learning as one
    big sequence modeling problem,” *Advances in neural information processing systems*,
    vol. 34, pp. 1273–1286, 2021.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] M. Janner、Q. Li 和 S. Levine，“离线强化学习作为一个大的序列建模问题，” *神经信息处理系统进展*，第 34 卷，页码
    1273–1286，2021 年。'
- en: '[141] O. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari,
    J. Hejna, C. Xu, J. Luo *et al.*, “Octo: An open-source generalist robot policy,”
    2023.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] O. M. Team、D. Ghosh、H. Walke、K. Pertsch、K. Black、O. Mees、S. Dasari、J.
    Hejna、C. Xu、J. Luo *等*，“Octo：一个开源通用机器人策略，” 2023 年。'
- en: '[142] X. Jia, Q. Wang, A. Donat, B. Xing, G. Li, H. Zhou, O. Celik, D. Blessing,
    R. Lioutikov, and G. Neumann, “Mail: Improving imitation learning with mamba,”
    *arXiv preprint arXiv:2406.08234*, 2024.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] X. Jia、Q. Wang、A. Donat、B. Xing、G. Li、H. Zhou、O. Celik、D. Blessing、R.
    Lioutikov 和 G. Neumann，“Mail：通过 mamba 改进模仿学习，” *arXiv 预印本 arXiv:2406.08234*，2024
    年。'
- en: '[143] Z. J. Cui, Y. Wang, N. M. M. Shafiullah, and L. Pinto, “From play to
    policy: Conditional behavior generation from uncurated robot data,” *arXiv preprint
    arXiv:2210.10047*, 2022.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Z. J. Cui、Y. Wang、N. M. M. Shafiullah 和 L. Pinto，“从游戏到策略：从未整理的机器人数据中生成条件行为，”
    *arXiv 预印本 arXiv:2210.10047*，2022 年。'
- en: '[144] S. Lee, Y. Wang, H. Etukuru, H. J. Kim, N. M. M. Shafiullah, and L. Pinto,
    “Behavior generation with latent actions,” *arXiv preprint arXiv:2403.03181*,
    2024.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] S. Lee、Y. Wang、H. Etukuru、H. J. Kim、N. M. M. Shafiullah 和 L. Pinto，“具有潜在动作的行为生成，”
    *arXiv 预印本 arXiv:2403.03181*，2024 年。'
- en: '[145] R. Dadashi, L. Hussenot, D. Vincent, S. Girgin, A. Raichuk, M. Geist,
    and O. Pietquin, “Continuous control with action quantization from demonstrations,”
    *arXiv preprint arXiv:2110.10149*, 2021.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] R. Dadashi、L. Hussenot、D. Vincent、S. Girgin、A. Raichuk、M. Geist 和 O.
    Pietquin，“通过示范进行动作量化的连续控制，” *arXiv 预印本 arXiv:2110.10149*，2021 年。'
- en: '[146] A. Van Den Oord, O. Vinyals *et al.*, “Neural discrete representation
    learning,” *Advances in neural information processing systems*, vol. 30, 2017.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] A. Van Den Oord、O. Vinyals *等*，“神经离散表示学习，” *神经信息处理系统进展*，第 30 卷，2017 年。'
- en: '[147] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
    dense object detection,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 2980–2988.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] T.-Y. Lin、P. Goyal、R. Girshick、K. He 和 P. Dollár，“用于密集物体检测的焦点损失，” *IEEE
    国际计算机视觉会议论文集*，2017 年，页码 2980–2988。'
- en: '[148] C. M. Bishop, “Mixture density networks,” 1994.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] C. M. Bishop，“混合密度网络，” 1994 年。'
- en: '[149] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma, “Pixelcnn++: Improving
    the pixelcnn with discretized logistic mixture likelihood and other modifications,”
    *arXiv preprint arXiv:1701.05517*, 2017.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] T. Salimans、A. Karpathy、X. Chen 和 D. P. Kingma，“Pixelcnn++：通过离散化的逻辑混合似然和其他修改改进
    pixelcnn，” *arXiv 预印本 arXiv:1701.05517*，2017 年。'
- en: '[150] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, “Viola: Imitation learning for
    vision-based manipulation with object proposal priors,” in *Proceedings of The
    6th Conference on Robot Learning*, ser. Proceedings of Machine Learning Research,
    K. Liu, D. Kulic, and J. Ichnowski, Eds., vol. 205.   PMLR, 14–18 Dec 2023, pp.
    1199–1210\. [Online]. Available: [https://proceedings.mlr.press/v205/zhu23a.html](https://proceedings.mlr.press/v205/zhu23a.html)'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Y. Zhu, A. Joshi, P. Stone, 和 Y. Zhu，“Viola: 基于视觉的操控的模仿学习与对象提议先验，” 在
    *第六届机器人学习会议论文集*，系列：机器学习研究论文集，K. Liu, D. Kulic, 和 J. Ichnowski 编，卷205。   PMLR，2023年12月14–18日，第1199–1210页。
    [在线]. 可用：[https://proceedings.mlr.press/v205/zhu23a.html](https://proceedings.mlr.press/v205/zhu23a.html)'
- en: '[151] W. Wan, Y. Zhu, R. Shah, and Y. Zhu, “Lotus: Continual imitation learning
    for robot manipulation through unsupervised skill discovery,” *arXiv preprint
    arXiv:2311.02058*, 2023.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] W. Wan, Y. Zhu, R. Shah, 和 Y. Zhu，“Lotus: 通过无监督技能发现实现机器人操控的持续模仿学习，” *arXiv
    预印本 arXiv:2311.02058*，2023。'
- en: '[152] C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu, Y. Zhu, and A. Anandkumar,
    “Mimicplay: Long-horizon imitation learning by watching human play,” *arXiv preprint
    arXiv:2302.12422*, 2023.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu, Y. Zhu, 和 A. Anandkumar，“Mimicplay:
    通过观察人类游戏进行长期模仿学习，” *arXiv 预印本 arXiv:2302.12422*，2023。'
- en: '[153] O. Mees, L. Hermann, and W. Burgard, “What matters in language conditioned
    robotic imitation learning over unstructured data,” *IEEE Robotics and Automation
    Letters*, vol. 7, no. 4, pp. 11 205–11 212, 2022.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] O. Mees, L. Hermann, 和 W. Burgard，“在非结构化数据上的语言条件机器人模仿学习中，什么是关键，” *IEEE机器人与自动化信函*，第7卷，第4期，第11 205–11 212页，2022年。'
- en: '[154] A. Murali, A. Mousavian, C. Eppner, C. Paxton, and D. Fox, “6-dof grasping
    for target-driven object manipulation in clutter,” in *2020 IEEE International
    Conference on Robotics and Automation (ICRA)*.   IEEE, 2020, pp. 6232–6238.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] A. Murali, A. Mousavian, C. Eppner, C. Paxton, 和 D. Fox，“在杂乱环境中针对目标驱动的对象操控进行6自由度抓取，”
    在 *2020 IEEE国际机器人与自动化会议（ICRA）*。   IEEE，2020年，第6232–6238页。'
- en: '[155] T. Weng, S. M. Bajracharya, Y. Wang, K. Agrawal, and D. Held, “Fabricflownet:
    Bimanual cloth manipulation with a flow-based policy,” in *Conference on Robot
    Learning*.   PMLR, 2022, pp. 192–202.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] T. Weng, S. M. Bajracharya, Y. Wang, K. Agrawal, 和 D. Held，“Fabricflownet:
    基于流的策略进行双手布料操控，” 在 *机器人学习会议*。   PMLR，2022年，第192–202页。'
- en: '[156] S. Jauhri, I. Lunawat, and G. Chalvatzaki, “Learning any-view 6dof robotic
    grasping in cluttered scenes via neural surface rendering,” *arXiv preprint arXiv:2306.07392*,
    2023.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] S. Jauhri, I. Lunawat, 和 G. Chalvatzaki，“通过神经表面渲染学习任何视角6自由度机器人抓取在杂乱场景中，”
    *arXiv 预印本 arXiv:2306.07392*，2023。'
- en: '[157] N. Ratliff, M. Zucker, J. A. Bagnell, and S. Srinivasa, “Chomp: Gradient
    optimization techniques for efficient motion planning,” in *2009 IEEE International
    Conference on Robotics and Automation*.   IEEE, 2009, pp. 489–494.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] N. Ratliff, M. Zucker, J. A. Bagnell, 和 S. Srinivasa，“Chomp: 用于高效运动规划的梯度优化技术，”
    在 *2009 IEEE国际机器人与自动化会议*。   IEEE，2009年，第489–494页。'
- en: '[158] M. Kalakrishnan, S. Chitta, E. Theodorou, P. Pastor, and S. Schaal, “Stomp:
    Stochastic trajectory optimization for motion planning,” in *IEEE international
    conference on robotics and automation*.   IEEE, 2011, pp. 4569–4574.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] M. Kalakrishnan, S. Chitta, E. Theodorou, P. Pastor, 和 S. Schaal，“Stomp:
    用于运动规划的随机轨迹优化，” 在 *IEEE国际机器人与自动化会议*。   IEEE，2011年，第4569–4574页。'
- en: '[159] Y. Ze, G. Yan, Y.-H. Wu, A. Macaluso, Y. Ge, J. Ye, N. Hansen, L. E.
    Li, and X. Wang, “Gnfactor: Multi-task real robot learning with generalizable
    neural feature fields,” in *Conference on Robot Learning*.   PMLR, 2023, pp. 284–301.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Y. Ze, G. Yan, Y.-H. Wu, A. Macaluso, Y. Ge, J. Ye, N. Hansen, L. E.
    Li, 和 X. Wang，“Gnfactor: 具有可泛化神经特征场的多任务真实机器人学习，” 在 *机器人学习会议*。   PMLR，2023年，第284–301页。'
- en: '[160] T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki, “3d diffuser actor: Policy
    diffusion with 3d scene representations,” *arXiv preprint arXiv:2402.10885*, 2024.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] T.-W. Ke, N. Gkanatsios, 和 K. Fragkiadaki，“3d diffuser actor: 通过3d场景表示进行策略扩散，”
    *arXiv 预印本 arXiv:2402.10885*，2024。'
- en: '[161] Z. Xian, N. Gkanatsios, T. Gervet, T.-W. Ke, and K. Fragkiadaki, “Chaineddiffuser:
    Unifying trajectory diffusion and keypose prediction for robotic manipulation,”
    in *7th Annual Conference on Robot Learning*, 2023.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Z. Xian, N. Gkanatsios, T. Gervet, T.-W. Ke, 和 K. Fragkiadaki，“Chaineddiffuser:
    统一轨迹扩散和关键姿态预测用于机器人操控，” 在 *第七届年度机器人学习会议*，2023年。'
- en: '[162] X. Ma, S. Patidar, I. Haughton, and S. James, “Hierarchical diffusion
    policy for kinematics-aware multi-task robotic manipulation,” *arXiv preprint
    arXiv:2403.03890*, 2024.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] X. Ma, S. Patidar, I. Haughton, 和 S. James，“层次扩散策略用于运动学感知的多任务机器人操控，”
    *arXiv 预印本 arXiv:2403.03890*，2024。'
- en: '[163] J. Urain, D. Tateo, and J. Peters, “Learning stable vector fields on
    lie groups,” in *Robotics and Automation Letters (RA-L)*, 2022.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] J. Urain, D. Tateo, 和 J. Peters, “在李群上学习稳定的向量场，” 在 *《机器人与自动化信函（RA-L）》*，2022年。'
- en: '[164] J. Nakanishi, R. Cory, M. Mistry, J. Peters, and S. Schaal, “Operational
    space control: A theoretical and empirical comparison,” *The International Journal
    of Robotics Research*, vol. 27, no. 6, pp. 737–757, 2008.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] J. Nakanishi, R. Cory, M. Mistry, J. Peters, 和 S. Schaal, “操作空间控制：理论与实证比较，”
    *《国际机器人研究杂志》*，第27卷，第6期，页码737–757，2008年。'
- en: '[165] O. Khatib, “A unified approach for motion and force control of robot
    manipulators: The operational space formulation,” *IEEE Journal on Robotics and
    Automation*, vol. 3, no. 1, pp. 43–53, 1987.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] O. Khatib, “机器人操控器的运动与力控制统一方法：操作空间公式，” *《IEEE机器人与自动化杂志》*，第3卷，第1期，页码43–53，1987年。'
- en: '[166] C. Jiang, A. Cornman, C. Park, B. Sapp, Y. Zhou, D. Anguelov *et al.*,
    “Motiondiffuser: Controllable multi-agent motion prediction using diffusion,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2023, pp. 9644–9653.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] C. Jiang, A. Cornman, C. Park, B. Sapp, Y. Zhou, D. Anguelov *等*，“Motiondiffuser：使用扩散的可控多智能体运动预测，”
    在 *《IEEE/CVF计算机视觉与模式识别会议论文集》*，2023年，页码9644–9653。'
- en: '[167] M. Mukadam, X. Yan, and B. Boots, “Gaussian process motion planning,”
    in *2016 IEEE international conference on robotics and automation (ICRA)*.   IEEE,
    2016, pp. 9–15.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] M. Mukadam, X. Yan, 和 B. Boots, “高斯过程运动规划，” 在 *2016年IEEE国际机器人与自动化会议（ICRA）*。
    IEEE，2016年，页码9–15。'
- en: '[168] D. Q. Mayne and H. Michalska, “Receding horizon control of nonlinear
    systems,” in *Proceedings of the 27th IEEE Conference on Decision and Control*.   IEEE,
    1988, pp. 464–465.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] D. Q. Mayne 和 H. Michalska, “非线性系统的回退控制，” 在 *第27届IEEE决策与控制会议论文集*。 IEEE，1988年，页码464–465。'
- en: '[169] G. Williams, A. Aldrich, and E. A. Theodorou, “Model predictive path
    integral control: From theory to parallel computation,” *Journal of Guidance,
    Control, and Dynamics*, vol. 40, no. 2, pp. 344–357, 2017.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] G. Williams, A. Aldrich, 和 E. A. Theodorou, “模型预测路径积分控制：从理论到并行计算，” *《引导、控制与动态杂志》*，第40卷，第2期，页码344–357，2017年。'
- en: '[170] K. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep reinforcement
    learning in a handful of trials using probabilistic dynamics models,” *Advances
    in Neural Information Processing Systems*, 2018.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] K. Chua, R. Calandra, R. McAllister, 和 S. Levine, “使用概率动力学模型在少数试验中进行深度强化学习，”
    *《神经信息处理系统进展》*，2018年。'
- en: '[171] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *Proceedings
    of the IEEE international conference on computer vision*, 2017, pp. 2961–2969.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] K. He, G. Gkioxari, P. Dollár, 和 R. Girshick, “Mask r-cnn，” 在 *IEEE国际计算机视觉会议论文集*，2017年，页码2961–2969。'
- en: '[172] Y. Du and L. Kaelbling, “Compositional generative modeling: A single
    model is not all you need,” *arXiv preprint arXiv:2402.01103*, 2024.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Y. Du 和 L. Kaelbling, “组合生成建模：一个模型并不全是你需要的，” *arXiv预印本arXiv:2402.01103*，2024年。'
- en: '[173] Y. Du, S. Li, and I. Mordatch, “Compositional visual generation with
    energy based models,” *Advances in Neural Information Processing Systems*, 2020.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Y. Du, S. Li, 和 I. Mordatch, “基于能量模型的组合视觉生成，” *《神经信息处理系统进展》*，2020年。'
- en: '[174] K. Saha, V. Mandadi, J. Reddy, A. Srikanth, A. Agarwal, B. Sen, A. Singh,
    and M. Krishna, “Edmp: Ensemble-of-costs-guided diffusion for motion planning,”
    *arXiv preprint arXiv:2309.11414*, 2023.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] K. Saha, V. Mandadi, J. Reddy, A. Srikanth, A. Agarwal, B. Sen, A. Singh,
    和 M. Krishna, “Edmp：基于代价的扩散集成运动规划，” *arXiv预印本arXiv:2309.11414*，2023年。'
- en: '[175] Y. Luo, C. Sun, J. B. Tenenbaum, and Y. Du, “Potential based diffusion
    motion planning,” *arXiv preprint arXiv:2407.06169*, 2024.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Y. Luo, C. Sun, J. B. Tenenbaum, 和 Y. Du, “基于潜力的扩散运动规划，” *arXiv预印本arXiv:2407.06169*，2024年。'
- en: '[176] U. A. Mishra, Y. Chen, and D. Xu, “Generative factor chaining: Coordinated
    manipulation with diffusion-based factor graph,” in *ICRA Workshop $\{$$\backslash$textemdash$\}$
    Back to the Future: Robot Learning Going Probabilistic*, 2024.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] U. A. Mishra, Y. Chen, 和 D. Xu, “生成因子链：基于扩散的因子图协调操作，” 在 *ICRA研讨会$\{$$\backslash$textemdash$\}$
    回到未来：机器人学习的概率性*，2024年。'
- en: '[177] W. Liu, J. Mao, J. Hsu, T. Hermans, A. Garg, and J. Wu, “Composable part-based
    manipulation,” in *7th Annual Conference on Robot Learning*, 2023.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] W. Liu, J. Mao, J. Hsu, T. Hermans, A. Garg, 和 J. Wu, “可组合的部件基础操作，” 在
    *第7届机器人学习年会*，2023年。'
- en: '[178] L. Wang, J. Zhao, Y. Du, E. H. Adelson, and R. Tedrake, “Poco: Policy
    composition from and for heterogeneous robot learning,” *arXiv preprint arXiv:2402.02511*,
    2024.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] L. Wang, J. Zhao, Y. Du, E. H. Adelson, 和 R. Tedrake, “Poco：来自异质机器人学习的策略组合，”
    *arXiv预印本arXiv:2402.02511*，2024年。'
- en: '[179] C. Wang, R. Wang, A. Mandlekar, L. Fei-Fei, S. Savarese, and D. Xu, “Generalization
    through hand-eye coordination: An action space for learning spatially-invariant
    visuomotor control,” in *2021 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS)*.   IEEE, 2021, pp. 8913–8920.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] C. Wang, R. Wang, A. Mandlekar, L. Fei-Fei, S. Savarese, 和 D. Xu, “通过手眼协调实现泛化：学习空间不变视觉运动控制的动作空间，”
    发表在*2021 IEEE/RSJ 国际智能机器人与系统会议 (IROS)*。IEEE, 2021, 第8913–8920页。'
- en: '[180] Y. Yoon, G. N. DeSouza, and A. C. Kak, “Real-time tracking and pose estimation
    for industrial objects using geometric features,” in *2003 IEEE International
    conference on robotics and automation (cat. no. 03CH37422)*, vol. 3.   IEEE, 2003,
    pp. 3473–3478.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Y. Yoon, G. N. DeSouza, 和 A. C. Kak, “利用几何特征进行工业物体的实时跟踪与姿态估计，” 发表在*2003
    IEEE 国际机器人与自动化会议 (cat. no. 03CH37422)*，第3卷。IEEE, 2003, 第3473–3478页。'
- en: '[181] C. Sahin and T.-K. Kim, “Category-level 6d object pose recovery in depth
    images,” in *Proceedings of the European Conference on Computer Vision (ECCV)
    Workshops*, 2018, pp. 0–0.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] C. Sahin 和 T.-K. Kim, “深度图像中的分类级6d物体姿态恢复，” 发表在*欧洲计算机视觉会议 (ECCV) 研讨会*，2018年，第0–0页。'
- en: '[182] X. Deng, Y. Xiang, A. Mousavian, C. Eppner, T. Bretl, and D. Fox, “Self-supervised
    6d object pose estimation for robot manipulation,” in *2020 IEEE International
    Conference on Robotics and Automation (ICRA)*.   IEEE, 2020, pp. 3665–3671.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] X. Deng, Y. Xiang, A. Mousavian, C. Eppner, T. Bretl, 和 D. Fox, “自监督6d物体姿态估计用于机器人操作，”
    发表在*2020 IEEE 国际机器人与自动化会议 (ICRA)*。IEEE, 2020, 第3665–3671页。'
- en: '[183] L. Manuelli, W. Gao, P. Florence, and R. Tedrake, “kpam: Keypoint affordances
    for category-level robotic manipulation,” in *The International Symposium of Robotics
    Research*.   Springer, 2019, pp. 132–157.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] L. Manuelli, W. Gao, P. Florence, 和 R. Tedrake, “kpam: 分类级机器人操作的关键点适应性，”
    发表在*国际机器人研究研讨会*。Springer, 2019, 第132–157页。'
- en: '[184] L. Manuelli, Y. Li, P. Florence, and R. Tedrake, “Keypoints into the
    future: Self-supervised correspondence in model-based reinforcement learning,”
    *arXiv preprint arXiv:2009.05085*, 2020.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] L. Manuelli, Y. Li, P. Florence, 和 R. Tedrake, “关键点未来：基于模型的强化学习中的自监督对应，”
    *arXiv 预印本 arXiv:2009.05085*，2020年。'
- en: '[185] M. Sieb, Z. Xian, A. Huang, O. Kroemer, and K. Fragkiadaki, “Graph-structured
    visual imitation,” in *Conference on Robot Learning*.   PMLR, 2020, pp. 979–989.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] M. Sieb, Z. Xian, A. Huang, O. Kroemer, 和 K. Fragkiadaki, “图结构视觉模仿，”
    发表在*机器人学习会议*。PMLR, 2020, 第979–989页。'
- en: '[186] T. D. Kulkarni, A. Gupta, C. Ionescu, S. Borgeaud, M. Reynolds, A. Zisserman,
    and V. Mnih, “Unsupervised learning of object keypoints for perception and control,”
    *Advances in neural information processing systems*, vol. 32, 2019.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] T. D. Kulkarni, A. Gupta, C. Ionescu, S. Borgeaud, M. Reynolds, A. Zisserman,
    和 V. Mnih, “无监督物体关键点学习用于感知和控制，” *神经信息处理系统进展*，第32卷，2019年。'
- en: '[187] Z. Qin, K. Fang, Y. Zhu, L. Fei-Fei, and S. Savarese, “Keto: Learning
    keypoint representations for tool manipulation,” in *2020 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2020, pp. 7278–7285.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Z. Qin, K. Fang, Y. Zhu, L. Fei-Fei, 和 S. Savarese, “Keto: 学习用于工具操作的关键点表示，”
    发表在*2020 IEEE 国际机器人与自动化会议 (ICRA)*。IEEE, 2020, 第7278–7285页。'
- en: '[188] C. Devin, P. Abbeel, T. Darrell, and S. Levine, “Deep object-centric
    representations for generalizable robot learning,” in *2018 IEEE International
    Conference on Robotics and Automation (ICRA)*.   IEEE, 2018, pp. 7111–7118.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] C. Devin, P. Abbeel, T. Darrell, 和 S. Levine, “用于可泛化机器人学习的深度物体中心表示，”
    发表在*2018 IEEE 国际机器人与自动化会议 (ICRA)*。IEEE, 2018, 第7111–7118页。'
- en: '[189] D. Wang, C. Devin, Q.-Z. Cai, F. Yu, and T. Darrell, “Deep object-centric
    policies for autonomous driving,” in *2019 International Conference on Robotics
    and Automation (ICRA)*.   IEEE, 2019, pp. 8853–8859.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] D. Wang, C. Devin, Q.-Z. Cai, F. Yu, 和 T. Darrell, “用于自动驾驶的深度物体中心策略，”
    发表在*2019 国际机器人与自动化会议 (ICRA)*。IEEE, 2019, 第8853–8859页。'
- en: '[190] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” *Advances in neural information
    processing systems*, vol. 28, 2015.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] S. Ren, K. He, R. Girshick, 和 J. Sun, “Faster R-CNN: 面向实时物体检测的区域建议网络，”
    *神经信息处理系统进展*，第28卷，2015年。'
- en: '[191] N. Heravi, A. Wahid, C. Lynch, P. Florence, T. Armstrong, J. Tompson,
    P. Sermanet, J. Bohg, and D. Dwibedi, “Visuomotor control in multi-object scenes
    using object-aware representations,” in *2023 IEEE International Conference on
    Robotics and Automation (ICRA)*.   IEEE, 2023, pp. 9515–9522.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] N. Heravi, A. Wahid, C. Lynch, P. Florence, T. Armstrong, J. Tompson,
    P. Sermanet, J. Bohg, 和 D. Dwibedi, “在多物体场景中使用物体感知表示进行视觉运动控制，” 发表在*2023 IEEE 国际机器人与自动化会议
    (ICRA)*。IEEE, 2023, 第9515–9522页。'
- en: '[192] Y. Zhu, Z. Jiang, P. Stone, and Y. Zhu, “Learning generalizable manipulation
    policies with object-centric 3d representations,” *arXiv preprint arXiv:2310.14386*,
    2023.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Y. Zhu, Z. Jiang, P. Stone, 和 Y. Zhu，“通过以物体为中心的3d表示学习通用操控策略”，*arXiv预印本
    arXiv:2310.14386*，2023。'
- en: '[193] Y. Wu, O. P. Jones, M. Engelcke, and I. Posner, “Apex: Unsupervised,
    object-centric scene segmentation and tracking for robot manipulation,” in *2021
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2021, pp. 3375–3382.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Y. Wu, O. P. Jones, M. Engelcke, 和 I. Posner，“Apex：无监督的以物体为中心的场景分割与跟踪用于机器人操控”，见于
    *2021 IEEE/RSJ国际智能机器人与系统大会（IROS）*。 IEEE, 2021, 第3375–3382页。'
- en: '[194] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold,
    J. Uszkoreit, A. Dosovitskiy, and T. Kipf, “Object-centric learning with slot
    attention,” *Advances in Neural Information Processing Systems*, vol. 33, pp.
    11 525–11 538, 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold,
    J. Uszkoreit, A. Dosovitskiy, 和 T. Kipf，“基于槽注意力的以物体为中心的学习”，*神经信息处理系统进展*，第33卷，第11,525–11,538页，2020。'
- en: '[195] H. K. Cheng, Y.-W. Tai, and C.-K. Tang, “Modular interactive video object
    segmentation: Interaction-to-mask, propagation and difference-aware fusion,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2021, pp. 5559–5568.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] H. K. Cheng, Y.-W. Tai, 和 C.-K. Tang，“模块化交互视频对象分割：交互到掩膜、传播和差异感知融合”，见于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2021，第5559–5568页。'
- en: '[196] W. Shen, G. Yang, A. Yu, J. Wong, L. P. Kaelbling, and P. Isola, “Distilled
    feature fields enable few-shot language-guided manipulation,” *arXiv preprint
    arXiv:2308.07931*, 2023.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] W. Shen, G. Yang, A. Yu, J. Wong, L. P. Kaelbling, 和 P. Isola，“蒸馏特征场使得少量样本语言引导操控成为可能”，*arXiv预印本
    arXiv:2308.07931*，2023。'
- en: '[197] C. Huang, O. Mees, A. Zeng, and W. Burgard, “Visual language maps for
    robot navigation,” in *2023 IEEE International Conference on Robotics and Automation
    (ICRA)*.   IEEE, 2023, pp. 10 608–10 615.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] C. Huang, O. Mees, A. Zeng, 和 W. Burgard，“用于机器人导航的视觉语言地图”，见于 *2023 IEEE国际机器人与自动化大会（ICRA）*。
    IEEE, 2023, 第10,608–10,615页。'
- en: '[198] S. Wang, J. Wu, X. Sun, W. Yuan, W. T. Freeman, J. B. Tenenbaum, and
    E. H. Adelson, “3d shape perception from monocular vision, touch, and shape priors,”
    in *2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2018, pp. 1606–1613.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] S. Wang, J. Wu, X. Sun, W. Yuan, W. T. Freeman, J. B. Tenenbaum, 和 E.
    H. Adelson，“从单眼视觉、触摸和形状先验中感知3d形状”，见于 *2018 IEEE/RSJ国际智能机器人与系统大会（IROS）*。 IEEE,
    2018, 第1606–1613页。'
- en: '[199] S. Suresh, H. Qi, T. Wu, T. Fan, L. Pineda, M. Lambeta, J. Malik, M. Kalakrishnan,
    R. Calandra, M. Kaess *et al.*, “Neural feels with neural fields: Visuo-tactile
    perception for in-hand manipulation,” *arXiv preprint arXiv:2312.13469*, 2023.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] S. Suresh, H. Qi, T. Wu, T. Fan, L. Pineda, M. Lambeta, J. Malik, M.
    Kalakrishnan, R. Calandra, M. Kaess *等*，“神经场与神经感受：用于手内操控的视触觉感知”，*arXiv预印本 arXiv:2312.13469*，2023。'
- en: '[200] E. Smith, R. Calandra, A. Romero, G. Gkioxari, D. Meger, J. Malik, and
    M. Drozdzal, “3d shape reconstruction from vision and touch,” *Advances in Neural
    Information Processing Systems*, vol. 33, pp. 14 193–14 206, 2020.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] E. Smith, R. Calandra, A. Romero, G. Gkioxari, D. Meger, J. Malik, 和
    M. Drozdzal，“从视觉和触觉中重建3d形状”，*神经信息处理系统进展*，第33卷，第14,193–14,206页，2020。'
- en: '[201] S. Suresh, Z. Si, J. G. Mangelson, W. Yuan, and M. Kaess, “Shapemap 3-d:
    Efficient shape mapping through dense touch and vision,” in *2022 International
    Conference on Robotics and Automation (ICRA)*.   IEEE, 2022, pp. 7073–7080.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] S. Suresh, Z. Si, J. G. Mangelson, W. Yuan, 和 M. Kaess，“Shapemap 3-d:
    通过密集触摸和视觉实现高效形状映射”，见于 *2022年国际机器人与自动化大会（ICRA）*。 IEEE, 2022, 第7073–7080页。'
- en: '[202] Y. Chen, A. E. Tekden, M. P. Deisenroth, and Y. Bekiroglu, “Sliding touch-based
    exploration for modeling unknown object shape with multi-fingered hands,” in *2023
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2023, pp. 8943–8950.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] Y. Chen, A. E. Tekden, M. P. Deisenroth, 和 Y. Bekiroglu，“基于滑动触摸的探索，用于用多指手建模未知物体形状”，见于
    *2023 IEEE/RSJ国际智能机器人与系统大会（IROS）*。 IEEE, 2023, 第8943–8950页。'
- en: '[203] Y. Yuan, H. Che, Y. Qin, B. Huang, Z.-H. Yin, K.-W. Lee, Y. Wu, S.-C.
    Lim, and X. Wang, “Robot synesthesia: In-hand manipulation with visuotactile sensing,”
    *arXiv preprint arXiv:2312.01853*, 2023.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Y. Yuan, H. Che, Y. Qin, B. Huang, Z.-H. Yin, K.-W. Lee, Y. Wu, S.-C.
    Lim, 和 X. Wang，“机器人联觉：通过视触觉感知进行手内操控”，*arXiv预印本 arXiv:2312.01853*，2023。'
- en: '[204] D. Morrison, P. Corke, and J. Leitner, “Closing the loop for robotic
    grasping: A real-time, generative grasp synthesis approach,” *arXiv preprint arXiv:1804.05172*,
    2018.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] D. Morrison, P. Corke, 和 J. Leitner，“闭环机器人抓取：一种实时生成的抓取合成方法”，*arXiv预印本
    arXiv:1804.05172*，2018。'
- en: '[205] K. Zakka, A. Zeng, J. Lee, and S. Song, “Form2fit: Learning shape priors
    for generalizable assembly from disassembly,” in *2020 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2020, pp. 9404–9410.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] K. Zakka, A. Zeng, J. Lee, 和 S. Song，“Form2fit: 从拆解中学习通用的形状先验用于装配，”在
    *2020 IEEE 国际机器人与自动化会议 (ICRA)* 上。IEEE，2020年，第9404–9410页。'
- en: '[206] H. Huang, D. Wang, R. Walters, and R. Platt, “Equivariant transporter
    network,” *arXiv preprint arXiv:2202.09400*, 2022.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] H. Huang, D. Wang, R. Walters, 和 R. Platt，“对称传输网络，” *arXiv 预印本 arXiv:2202.09400*，2022年。'
- en: '[207] D. Seita, P. Florence, J. Tompson, E. Coumans, V. Sindhwani, K. Goldberg,
    and A. Zeng, “Learning to rearrange deformable cables, fabrics, and bags with
    goal-conditioned transporter networks,” in *2021 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2021, pp. 4568–4575.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] D. Seita, P. Florence, J. Tompson, E. Coumans, V. Sindhwani, K. Goldberg,
    和 A. Zeng，“学习重新排列可变形电缆、织物和袋子，使用目标条件传输网络，”在 *2021 IEEE 国际机器人与自动化会议 (ICRA)* 上。IEEE，2021年，第4568–4575页。'
- en: '[208] Y. Avigal, L. Berscheid, T. Asfour, T. Kröger, and K. Goldberg, “Speedfolding:
    Learning efficient bimanual folding of garments,” in *2022 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*.   IEEE, 2022, pp. 1–8.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Y. Avigal, L. Berscheid, T. Asfour, T. Kröger, 和 K. Goldberg，“Speedfolding:
    学习高效的双手折叠衣物，”在 *2022 IEEE/RSJ 智能机器人与系统国际会议 (IROS)* 上。IEEE，2022年，第1–8页。'
- en: '[209] S. James, K. Wada, T. Laidlow, and A. J. Davison, “Coarse-to-fine q-attention:
    Efficient learning for visual robotic manipulation via discretisation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022,
    pp. 13 739–13 748.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] S. James, K. Wada, T. Laidlow, 和 A. J. Davison，“粗到细的q-attention: 通过离散化进行视觉机器人操作的高效学习，”在
    *IEEE/CVF 计算机视觉与模式识别会议论文集* 中，2022年，第13 739–13 748页。'
- en: '[210] M. Grotz, M. Shridhar, T. Asfour, and D. Fox, “Peract2: A perceiver actor
    framework for bimanual manipulation tasks,” *arXiv preprint arXiv:2407.00278*,
    2024.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] M. Grotz, M. Shridhar, T. Asfour, 和 D. Fox，“Peract2: 一个用于双手操作任务的感知者-执行者框架，”
    *arXiv 预印本 arXiv:2407.00278*，2024年。'
- en: '[211] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,”
    *Communications of the ACM*, vol. 65, no. 1, pp. 99–106, 2021.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    和 R. Ng，“Nerf: 将场景表示为神经辐射场以进行视图合成，” *ACM 通讯*，第65卷，第1期，第99–106页，2021年。'
- en: '[212] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution
    image synthesis with latent diffusion models,” in *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, 2022, pp. 10 684–10 695.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, 和 B. Ommer，“利用潜在扩散模型进行高分辨率图像合成，”在
    *IEEE/CVF 计算机视觉与模式识别会议论文集* 中，2022年，第10 684–10 695页。'
- en: '[213] A. Goyal, V. Blukis, J. Xu, Y. Guo, Y.-W. Chao, and D. Fox, “Rvt-2: Learning
    precise manipulation from few demonstrations,” *arXiv preprint arXiv:2406.08545*,
    2024.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] A. Goyal, V. Blukis, J. Xu, Y. Guo, Y.-W. Chao, 和 D. Fox，“Rvt-2: 从少量演示中学习精确操作，”
    *arXiv 预印本 arXiv:2406.08545*，2024年。'
- en: '[214] Y.-C. Lin, P. Florence, A. Zeng, J. T. Barron, Y. Du, W.-C. Ma, A. Simeonov,
    A. R. Garcia, and P. Isola, “Mira: Mental imagery for robotic affordances,” in
    *Conference on Robot Learning*.   PMLR, 2023, pp. 1916–1927.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] Y.-C. Lin, P. Florence, A. Zeng, J. T. Barron, Y. Du, W.-C. Ma, A. Simeonov,
    A. R. Garcia, 和 P. Isola，“Mira: 机器人可操作性的心理意象，”在 *机器人学习会议* 上。PMLR，2023年，第1916–1927页。'
- en: '[215] R. Wu, Y. Zhao, K. Mo, Z. Guo, Y. Wang, T. Wu, Q. Fan, X. Chen, L. Guibas,
    and H. Dong, “Vat-mart: Learning visual action trajectory proposals for manipulating
    3d articulated objects,” *arXiv preprint arXiv:2106.14440*, 2021.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] R. Wu, Y. Zhao, K. Mo, Z. Guo, Y. Wang, T. Wu, Q. Fan, X. Chen, L. Guibas,
    和 H. Dong，“Vat-mart: 学习操控3D关节物体的视觉动作轨迹建议，” *arXiv 预印本 arXiv:2106.14440*，2021年。'
- en: '[216] Y. Geng, B. An, H. Geng, Y. Chen, Y. Yang, and H. Dong, “Rlafford: End-to-end
    affordance learning for robotic manipulation,” in *2023 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2023, pp. 5880–5886.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] Y. Geng, B. An, H. Geng, Y. Chen, Y. Yang, 和 H. Dong，“Rlafford: 机器人操作的端到端可操作性学习，”在
    *2023 IEEE 国际机器人与自动化会议 (ICRA)* 上。IEEE，2023年，第5880–5886页。'
- en: '[217] Y. Wang, R. Wu, K. Mo, J. Ke, Q. Fan, L. J. Guibas, and H. Dong, “Adaafford:
    Learning to adapt manipulation affordance for 3d articulated objects via few-shot
    interactions,” in *European Conference on Computer Vision*.   Springer, 2022,
    pp. 90–107.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Y. Wang, R. Wu, K. Mo, J. Ke, Q. Fan, L. J. Guibas, 和 H. Dong，“Adaafford:
    通过少量交互学习适应3D关节物体的操作能力，”在 *欧洲计算机视觉会议* 上。Springer，2022年，第90–107页。'
- en: '[218] Y. Zhao, R. Wu, Z. Chen, Y. Zhang, Q. Fan, K. Mo, and H. Dong, “Dualafford:
    Learning collaborative visual affordance for dual-gripper manipulation,” in *The
    Eleventh International Conference on Learning Representations*, 2022.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] Y. Zhao, R. Wu, Z. Chen, Y. Zhang, Q. Fan, K. Mo, 和 H. Dong, “Dualafford:
    学习双夹持操作的协作视觉可用性，” 在 *第十一届国际学习表征会议*，2022年。'
- en: '[219] K. Mo, Y. Qin, F. Xiang, H. Su, and L. Guibas, “O2o-afford: Annotation-free
    large-scale object-object affordance learning,” in *Conference on Robot Learning*.   PMLR,
    2022, pp. 1666–1677.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] K. Mo, Y. Qin, F. Xiang, H. Su, 和 L. Guibas, “O2o-afford: 无需注释的大规模物体-物体可用性学习，”
    在 *机器人学习会议*。PMLR，2022年，页码1666–1677。'
- en: '[220] V. Vosylius, Y. Seo, J. Uruç, and S. James, “Render and diffuse: Aligning
    image and action spaces for diffusion-based behaviour cloning,” *arXiv preprint
    arXiv:2405.18196*, 2024.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] V. Vosylius, Y. Seo, J. Uruç, 和 S. James, “渲染与扩散：对齐图像和动作空间以进行基于扩散的行为克隆，”
    *arXiv预印本 arXiv:2405.18196*，2024年。'
- en: '[221] M. Shridhar, Y. L. Lo, and S. James, “Generative image as action models,”
    *arXiv preprint arXiv:2407.07875*, 2024.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] M. Shridhar, Y. L. Lo, 和 S. James, “生成图像作为动作模型，” *arXiv预印本 arXiv:2407.07875*，2024年。'
- en: '[222] X. B. Peng, E. Coumans, T. Zhang, T.-W. Lee, J. Tan, and S. Levine, “Learning
    agile robotic locomotion skills by imitating animals,” *arXiv preprint arXiv:2004.00784*,
    2020.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] X. B. Peng, E. Coumans, T. Zhang, T.-W. Lee, J. Tan, 和 S. Levine, “通过模仿动物学习灵活的机器人运动技能，”
    *arXiv预印本 arXiv:2004.00784*，2020年。'
- en: '[223] C. Qian, J. Urain, K. Zakka, and J. Peters, “Pianomime: Learning a generalist,
    dexterous piano player from internet demonstrations,” *arXiv preprint arXiv:2407.18178*,
    2024.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] C. Qian, J. Urain, K. Zakka, 和 J. Peters, “Pianomime: 从互联网演示中学习通用的灵活钢琴演奏者，”
    *arXiv预印本 arXiv:2407.18178*，2024年。'
- en: '[224] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A.
    Huang, Y. Zhu, and A. Anandkumar, “Minedojo: Building open-ended embodied agents
    with internet-scale knowledge,” *Advances in Neural Information Processing Systems*,
    vol. 35, pp. 18 343–18 362, 2022.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A.
    Huang, Y. Zhu, 和 A. Anandkumar, “Minedojo: 构建具有互联网规模知识的开放式体现代理，” *神经信息处理系统进展*，第35卷，页码18 343–18 362，2022年。'
- en: '[225] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang,
    “Vip: Towards universal visual reward and representation via value-implicit pre-training,”
    *arXiv preprint arXiv:2210.00030*, 2022.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, 和 A. Zhang,
    “Vip: 朝向通过价值隐含预训练的通用视觉奖励和表示，” *arXiv预印本 arXiv:2210.00030*，2022年。'
- en: '[226] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, “Rlbench: The robot
    learning benchmark & learning environment,” *IEEE Robotics and Automation Letters*,
    vol. 5, no. 2, pp. 3019–3026, 2020.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] S. James, Z. Ma, D. R. Arrojo, 和 A. J. Davison, “Rlbench: 机器人学习基准与学习环境，”
    *IEEE机器人与自动化通讯*，第5卷，第2期，页码3019–3026，2020年。'
- en: '[227] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone, “Libero:
    Benchmarking knowledge transfer for lifelong robot learning,” *Advances in Neural
    Information Processing Systems*, vol. 36, 2024.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, 和 P. Stone, “Libero:
    基于终身机器人的知识转移基准测试，” *神经信息处理系统进展*，第36卷，2024年。'
- en: '[228] J. Gu, F. Xiang, X. Li, Z. Ling, X. Liu, T. Mu, Y. Tang, S. Tao, X. Wei,
    Y. Yao *et al.*, “Maniskill2: A unified benchmark for generalizable manipulation
    skills,” *arXiv preprint arXiv:2302.04659*, 2023.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] J. Gu, F. Xiang, X. Li, Z. Ling, X. Liu, T. Mu, Y. Tang, S. Tao, X. Wei,
    Y. Yao *等*，“Maniskill2: 一种通用操作技能的统一基准，” *arXiv预印本 arXiv:2302.04659*，2023年。'
- en: '[229] A. Mandlekar, S. Nasiriany, B. Wen, I. Akinola, Y. Narang, L. Fan, Y. Zhu,
    and D. Fox, “Mimicgen: A data generation system for scalable robot learning using
    human demonstrations,” *arXiv preprint arXiv:2310.17596*, 2023.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] A. Mandlekar, S. Nasiriany, B. Wen, I. Akinola, Y. Narang, L. Fan, Y.
    Zhu, 和 D. Fox, “Mimicgen: 一种用于可扩展机器人学习的数据生成系统，基于人类演示，” *arXiv预印本 arXiv:2310.17596*，2023年。'
- en: '[230] S. Nasiriany, A. Maddukuri, L. Zhang, A. Parikh, A. Lo, A. Joshi, A. Mandlekar,
    and Y. Zhu, “Robocasa: Large-scale simulation of everyday tasks for generalist
    robots,” *arXiv preprint arXiv:2406.02523*, 2024.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] S. Nasiriany, A. Maddukuri, L. Zhang, A. Parikh, A. Lo, A. Joshi, A.
    Mandlekar, 和 Y. Zhu, “Robocasa: 大规模的日常任务模拟用于通用机器人，” *arXiv预印本 arXiv:2406.02523*，2024年。'
- en: '[231] C. Celemin, R. Pérez-Dattari, E. Chisari, G. Franzese, L. de Souza Rosa,
    R. Prakash, Z. Ajanović, M. Ferraz, A. Valada, J. Kober *et al.*, “Interactive
    imitation learning in robotics: A survey,” *Foundations and Trends® in Robotics*,
    vol. 10, no. 1-2, pp. 1–197, 2022.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] C. Celemin, R. Pérez-Dattari, E. Chisari, G. Franzese, L. de Souza Rosa,
    R. Prakash, Z. Ajanović, M. Ferraz, A. Valada, J. Kober *等*，“机器人中的互动模仿学习：综述，”
    *机器人学基础与趋势®*，第10卷，第1-2期，页码1–197，2022年。'
- en: Glossary
  id: totrans-493
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词汇表
- en: BC
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: BC
- en: Behavioural Cloning
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 行为克隆
- en: CD
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: CD
- en: Contrastive Divergence
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 对比发散
- en: CE
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: CE
- en: Cross-Entropy
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵
- en: DDPM
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM
- en: Denoising Diffusion Probabilistic Models
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪扩散概率模型
- en: DGM
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 深度生成模型
- en: Deep Generative Models
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 深度生成模型
- en: DM
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型
- en: Diffusion Models
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型
- en: EBM
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 能量基模型
- en: Energy Based Models
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 基于能量的模型
- en: GAN
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: Generative Adversarial Networks
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: GMM
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: Gaussian Mixture Models
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: GP
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程
- en: Gaussian Process
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程
- en: HMM
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型
- en: Hidden Markov Model
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型
- en: IOC
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 内部最优控制
- en: Inverse Optimal Control
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 逆最优控制
- en: IRL
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 逆强化学习
- en: Inverse Reinforcement Learning
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 逆强化学习
- en: LfD
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 从示范中学习
- en: Learning from Demonstration
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 从示范中学习
- en: LLM
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: Large Language Models
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: MCMC
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链蒙特卡洛
- en: Markov Chain Monte Carlo
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链蒙特卡洛
- en: MDM
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 多维模型
- en: Mixture Density Models
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 混合密度模型
- en: NFlow
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 流量模型
- en: Normalizing Flows
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化流
- en: RL
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement Learning
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习
- en: SDF
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 符号距离场
- en: Signed Distance Field
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 符号距离场
- en: VAE
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: Variational Autoencoders
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器
