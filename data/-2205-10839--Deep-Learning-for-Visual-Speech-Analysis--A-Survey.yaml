- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:46:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:46:19
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2205.10839] Deep Learning for Visual Speech Analysis: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2205.10839] 深度学习在视觉语音分析中的应用：一项综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2205.10839](https://ar5iv.labs.arxiv.org/html/2205.10839)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2205.10839](https://ar5iv.labs.arxiv.org/html/2205.10839)
- en: 'Deep Learning for Visual Speech Analysis: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在视觉语音分析中的应用：一项综述
- en: Changchong Sheng, Gangyao Kuang, Liang Bai, Chenping Hou, Yulan Guo, Xin Xu,
    Matti Pietikäinen, and Li Liu^∗
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Changchong Sheng, Gangyao Kuang, Liang Bai, Chenping Hou, Yulan Guo, Xin Xu,
    Matti Pietikäinen, 和 Li Liu^∗
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Visual speech, referring to the visual domain of speech, has attracted increasing
    attention due to its wide applications, such as public security, medical treatment,
    military defense, and film entertainment. As a powerful AI strategy, deep learning
    techniques have extensively promoted the development of visual speech learning.
    Over the past five years, numerous deep learning based methods have been proposed
    to address various problems in this area, especially automatic visual speech recognition
    and generation. To push forward future research on visual speech, this paper aims
    to present a comprehensive review of recent progress in deep learning methods
    on visual speech analysis. We cover different aspects of visual speech, including
    fundamental problems, challenges, benchmark datasets, a taxonomy of existing methods,
    and state-of-the-art performance. Besides, we also identify gaps in current research
    and discuss inspiring future research directions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉语音，作为语音的视觉领域，由于其广泛应用，如公共安全、医疗治疗、军事防御和影视娱乐，吸引了越来越多的关注。作为一种强大的人工智能策略，深度学习技术极大地推动了视觉语音学习的发展。在过去五年中，已经提出了许多基于深度学习的方法来解决这一领域的各种问题，特别是自动视觉语音识别和生成。为了推动未来视觉语音研究的发展，本文旨在全面回顾深度学习在视觉语音分析中的最新进展。我们涵盖了视觉语音的不同方面，包括基本问题、挑战、基准数据集、现有方法的分类以及最新的性能表现。此外，我们还识别了当前研究中的空白，并讨论了未来研究的有趣方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep Learning, Visual Speech, Lip Reading, Speech Perception, Computer Vision,
    Computer Graphics
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、视觉语音、唇读、语音感知、计算机视觉、计算机图形学
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Human speech is by nature bimodal: visual and audio. Visual speech refers to
    the visual domain of speech, i.e., the movements of the lips, tongue, teeth, jaw, etc.,
    and other facial muscles of the lower face that are naturally produced during
    talking [[1](#bib.bib1)], while audio speech refers to the acoustic waveform pronounced
    by the speaker. Speech perception is intrinsically bimodal, as shown several decades
    ago by the famous McGurk effect [[2](#bib.bib2)] that human speech perception
    depends not only on auditory information, but also on visual cues like lip movements.
    Therefore, undoubtedly, visual speech contributes to human speech perception,
    especially for people who are hearing-impaired or hard of hearing or when acoustic
    information is corrupted.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人类的语音本质上是双模态的：视觉和听觉。视觉语音指的是语音的视觉领域，即在说话时自然产生的嘴唇、舌头、牙齿、下颌等及下半脸其他面部肌肉的运动[[1](#bib.bib1)]，而听觉语音指的是说话者发出的声学波形。语音感知本质上是双模态的，这一点在几十年前著名的麦戈克效应[[2](#bib.bib2)]中得到了证明，表明人类的语音感知不仅依赖于听觉信息，还依赖于如唇部运动等视觉线索。因此，毫无疑问，视觉语音对人类语音感知有贡献，尤其是对于听力受损或听力困难的人群，或当声学信息受到干扰时。
- en: 'As a fundamental and challenging topic in computer vision and multimedia, automatic
    Visual Speech Analysis (VSA) has received increasing attention in recent years,
    due to the important role it plays in a wide variety of applications many of which
    are newly emerging. VSA embraces two fundamental closely-related and formal-dual
    problems: Visual Speech Recognition (VSR) or Lip Reading, Visual Speech Generation
    (VSG) or Lip Sequence Generation. Significant progress has been witnessed in this
    field due to the recent boom of deep learning. Typical academia and practical
    applications of VSA include multimodal speech recognition and enhancement, speaker
    recognition and verification [[3](#bib.bib3)], medical assistance, security, forensic,
    video compression, entertainment, human-computer interaction, emotion understanding [[4](#bib.bib4),
    [5](#bib.bib5)], etc.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为计算机视觉和多媒体领域的一个基础而具有挑战性的课题，自动视觉语音分析（VSA）近年来受到了越来越多的关注，因为它在许多新兴应用中扮演着重要角色。VSA
    包含两个基本的、紧密相关且形式对偶的问题：视觉语音识别（VSR）或唇读，视觉语音生成（VSG）或唇部序列生成。由于深度学习的迅猛发展，该领域已经取得了显著的进展。VSA
    的典型学术和实际应用包括多模态语音识别和增强、说话人识别和验证 [[3](#bib.bib3)]、医疗辅助、安全、法医学、视频压缩、娱乐、人机交互、情感理解 [[4](#bib.bib4),
    [5](#bib.bib5)] 等。
- en: To give some application examples, in speech recognition and enhancement, visual
    speech can be treated as a complementary signal to increase the accuracy and robustness
    of current audio speech recognition and separation under various unfavorable acoustic
    conditions [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]. In
    the medical domain, solving the VSR task can also help the hearing impaired [[10](#bib.bib10)]
    and people with vocal cord lesions. In public security, VSA can be applied to
    face forgery detection [[11](#bib.bib11)] and liveness detection [[12](#bib.bib12)].
    In human-computer interaction, visual speech can serve as a new type of interactive
    information, improving the diversity and robustness of interactions [[13](#bib.bib13),
    [14](#bib.bib14)]. In the entertainment domain, VSG technology plays a crucial
    role in personalized 3D talking avatars generation [[15](#bib.bib15)] in virtual
    gaming and realizing high-fidelity photo-realistic talking videos generation for
    movie post-production like visual dubbing [[16](#bib.bib16)]. In addition, VSR
    can be used to transcribe archival silent films.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以一些应用实例为例，在语音识别和增强中，视觉语音可以作为一种补充信号，用于提高当前音频语音识别和分离在各种不利声学条件下的准确性和鲁棒性 [[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]。在医疗领域，解决 VSR 任务还可以帮助听力障碍者 [[10](#bib.bib10)]
    和有声带病变的人。在公共安全中，VSA 可以应用于人脸伪造检测 [[11](#bib.bib11)] 和活体检测 [[12](#bib.bib12)]。在人机交互中，视觉语音可以作为一种新型的交互信息，提高交互的多样性和鲁棒性 [[13](#bib.bib13),
    [14](#bib.bib14)]。在娱乐领域，VSG 技术在虚拟游戏中个性化 3D 对话头像生成 [[15](#bib.bib15)] 和实现高保真度的照片级真实对话视频生成（如视觉配音）在电影后期制作中扮演着关键角色 [[16](#bib.bib16)]。此外，VSR
    还可以用于转录档案静默电影。
- en: 'The core of VSA lies in visual speech representation learning and sequence
    modeling. In the era dominated by traditional VSA methods, shallow representations
    of visual speech such as visemes [[17](#bib.bib17), [18](#bib.bib18)], mouth geometry
    descriptors [[19](#bib.bib19)], linear transformation features [[20](#bib.bib20)],
    statistical representations [[21](#bib.bib21)], and sequence modeling like Gaussian
    process dynamical models [[22](#bib.bib22)], hidden Markov models (HMMs) [[23](#bib.bib23)],
    decision tree models [[24](#bib.bib24)] were widely used in solving VSA tasks.
    Since the significant breakthroughs [[25](#bib.bib25)] of deep neural networks
    (DNNs) in the image classification task, most computer vision and natural language
    problems have focused explicitly on deep learning methods, including VSA. In 2016,
    deep learning based VSA methods [[26](#bib.bib26), [27](#bib.bib27)] have vastly
    outperformed traditional approaches, bringing the VSA into the deep learning era.
    Meanwhile, the emergence of large-scale VSA datasets [[27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)] promoted the further development
    of deep learning based VSA research. In this paper, we mainly focus on the deep
    learning based VSA approaches. The milestones of VSA technologies from 2016 to
    the present are shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning
    for Visual Speech Analysis: A Survey"), including representative deep VSR and
    VSG methods and related audio-visual datasets.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: VSA 的核心在于视觉语音表示学习和序列建模。在传统 VSA 方法主导的时代，视觉语音的浅层表示，如 visemes [[17](#bib.bib17),
    [18](#bib.bib18)]，口部几何描述符 [[19](#bib.bib19)]，线性变换特征 [[20](#bib.bib20)]，统计表示 [[21](#bib.bib21)]，以及序列建模，如高斯过程动态模型
    [[22](#bib.bib22)]，隐马尔可夫模型（HMMs） [[23](#bib.bib23)]，决策树模型 [[24](#bib.bib24)]，在解决
    VSA 任务中被广泛使用。自从深度神经网络（DNNs）在图像分类任务中的重大突破 [[25](#bib.bib25)] 以来，大多数计算机视觉和自然语言问题都明确关注深度学习方法，包括
    VSA。2016 年以来，基于深度学习的 VSA 方法 [[26](#bib.bib26), [27](#bib.bib27)] 远远超越了传统方法，使 VSA
    进入了深度学习时代。同时，大规模 VSA 数据集 [[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31)] 的出现促进了基于深度学习的 VSA 研究的进一步发展。本文主要关注基于深度学习的 VSA
    方法。图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 深度学习在视觉语音分析中的应用：综述") 展示了从 2016 年至今 VSA 技术的里程碑，包括代表性的深度
    VSR 和 VSG 方法以及相关视听数据集。
- en: '![Refer to caption](img/e075f495bb3e6a3db26a930d62a245ad.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/e075f495bb3e6a3db26a930d62a245ad.png)'
- en: 'Figure 1: Chronological milestones on visual speech analysis from 2016 to the
    present, including representative VSR and VSG methods, and audio-visual datasets.
    Handcrafted feature engineering methods dominated VSA until a transition took
    place in 2016 with the introduction of related deep networks.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：从 2016 年至今视觉语音分析的时间节点，包括代表性的 VSR 和 VSG 方法，以及视听数据集。在 2016 年引入相关深度网络之前，手工特征工程方法主导了
    VSA 的发展。
- en: Although the recent promising progress brought by deep learning in the past
    several years, the VSA technology is still in its early stages and incapable of
    performing at a level sufficient for real-world applications. This is certainly
    not because of a lack of effort by researchers, as there have been many excellent
    works on VSA [[6](#bib.bib6), [32](#bib.bib32), [28](#bib.bib28), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)]. Therefore, it is of great importance to systematically
    review the recent developments in the field, identify the main challenges and
    open problems preventing its advancement, and define promising future directions.
    However, the large part of VSA research remains rather scattered, and no such
    systematic surveys exist. This motivates this survey which will fill this gap.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近年来深度学习取得了令人鼓舞的进展，VSA 技术仍处于早期阶段，无法达到足以满足实际应用的水平。这当然不是因为研究人员缺乏努力，因为已有许多优秀的
    VSA 工作 [[6](#bib.bib6), [32](#bib.bib32), [28](#bib.bib28), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)]。因此，系统回顾该领域的最新发展，识别主要挑战和阻碍进展的开放问题，并定义有前景的未来方向至关重要。然而，VSA
    研究的很大一部分仍然相当分散，并且没有这样的系统性综述存在。这激励了本综述，以填补这一空白。
- en: 1.1 The Scope of this Survey
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 本调查的范围
- en: The main objective of this survey is to provide a comprehensive overview of
    current deep learning based VSA methods, in particular, VSR and VSG and related
    applications, main challenges, benchmark datasets, methods, and State of the Art
    (SOTA) results, together with existing gaps and promising future research directions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查的主要目标是提供对当前基于深度学习的VSA方法的全面概述，特别是VSR和VSG及相关应用、主要挑战、基准数据集、方法和前沿成果，同时指出现有的差距和有前景的未来研究方向。
- en: There are mainly three reasons that we comprehensively overview VSR and VSG
    together. First, as the most fundamental problems in VSA, VSR and VSG cover most
    aspects of visual speech analysis. Other VSA-related tasks, such as speech enhancement,
    speaker verification, face forgery detection, etc., can be seen as extended applications
    of VSR and VSG. Second, because VSR and VSG are formal-dual and mutually promoted,
    the dual learning [[36](#bib.bib36)] and generative adversarial learning mechanisms [[37](#bib.bib37)]
    are widely adopted in many existing VSA works [[32](#bib.bib32), [38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)]. Therefore, we intend to
    provide a side-by-side perspective for readers to know the evolution of VSR and
    VSG. Third, VSR and VSG have common core technical points, such as visual speech
    representation learning approaches and contextual sequence modeling approaches.
    We hope it would be helpful for readers to have an accessible understanding of
    the cross-task transferability of these methods.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们综合概述VSR和VSG主要有三个原因。首先，作为VSA中最基础的问题，VSR和VSG涵盖了视觉语音分析的大部分方面。其他与VSA相关的任务，如语音增强、说话人验证、面部伪造检测等，可以视为VSR和VSG的扩展应用。其次，由于VSR和VSG是形式上的对偶且互相促进，双重学习[[36](#bib.bib36)]和生成对抗学习机制[[37](#bib.bib37)]在许多现有的VSA工作中被广泛采用[[32](#bib.bib32),
    [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)]。因此，我们打算为读者提供一个并列视角，以了解VSR和VSG的演变。第三，VSR和VSG有共同的核心技术点，如视觉语音表示学习方法和上下文序列建模方法。我们希望这对读者理解这些方法的跨任务可迁移性有所帮助。
- en: 1.2 Differences with Related Surveys
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 与相关综述的不同之处
- en: Several surveys on VSA [[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46)] have been published. However, they have only
    partially reviewed specific VSA tasks. For example, [[43](#bib.bib43), [44](#bib.bib44),
    [46](#bib.bib46)] conducted reviews on VSR, and [[42](#bib.bib42), [45](#bib.bib45)]
    focused on VSG. We give a brief conclusion related surveys and then emphasize
    our new contributions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 已经发布了几篇关于VSA的综述[[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46)]。然而，它们仅部分地回顾了特定的VSA任务。例如，[[43](#bib.bib43), [44](#bib.bib44),
    [46](#bib.bib46)]对VSR进行了综述，而[[42](#bib.bib42), [45](#bib.bib45)]则关注于VSG。我们将简要总结相关综述，然后强调我们的新贡献。
- en: 'In 2014, Zhou et al. [[46](#bib.bib46)] summarized three central questions
    of visual feature extraction for VSR: speaker dependency, head pose variation,
    and efficiently encoding of spatio-temporal information. Then they reviewed mainstream
    visual features extraction and dynamic audio-visual speech fusion methods of VSR
    from the view of question-solving, which brought a new perspective for readers
    to know the developments of VSR. In 2015, Mattheyses et al. [[45](#bib.bib45)]
    gave an extensive and comprehensive overview of audio-visual speech synthesis
    with great effort. We advocate readers to refer to [[45](#bib.bib45)] for the
    development history of VSG before 2015. As [[45](#bib.bib45), [46](#bib.bib46)]
    provided comprehensive surveys on traditional VSR and VSG methods, in this paper,
    we mainly focus on the recent advances caused by deep learning technologies. Adriana et
    al. [[44](#bib.bib44)] summarized VSR datasets according to the differences in
    recognition tasks and reviewed traditional and deep learning based methods of
    VSR. They mainly focused on the existing datasets, and the analysis of VSR methods
    for different recognition tasks on each dataset. However, the research reviewed
    in [[44](#bib.bib44)] is mainly pre-2018, prior to the recent striking success.
    Recently, Fenghour et al. [[43](#bib.bib43)] conducted a survey reviewing deep
    learning driven VSR methods, including audio-visual datasets, feature extraction,
    classification networks and classification schemes. However, some essential advances
    of VSR were omitted, such as self-supervised learning methods [[47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)], cross-modal knowledge distillation
    methods [[51](#bib.bib51), [52](#bib.bib52), [34](#bib.bib34)], graph neural networks
    backbone architectures [[53](#bib.bib53), [54](#bib.bib54)], etc. Chen et al. [[42](#bib.bib42)]
    conducted a thoughtful analysis across several representative identity-independent
    VSG methods and designed a performance evaluation benchmark for VSG. However,
    their core contributions are well-defined standards of evaluation metrics rather
    than the comprehensive discussion and overview of VSG methods.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在2014年，Zhou等人[[46](#bib.bib46)]总结了视觉特征提取在视觉语音识别（VSR）中的三个核心问题：说话者依赖性、头部姿势变化以及时空信息的有效编码。随后，他们从问题解决的角度回顾了主流的视觉特征提取和动态视听语音融合方法，这为读者了解VSR的发展带来了新的视角。2015年，Mattheyses等人[[45](#bib.bib45)]付出了巨大努力，对视听语音合成进行了广泛而全面的概述。我们建议读者参考[[45](#bib.bib45)]了解2015年前VSG的发展历史。由于[[45](#bib.bib45)]和[[46](#bib.bib46)]对传统VSR和VSG方法进行了全面的调查，因此本文主要关注由深度学习技术引起的最新进展。Adriana等人[[44](#bib.bib44)]根据识别任务的差异总结了VSR数据集，并回顾了传统和基于深度学习的VSR方法。他们主要关注现有数据集，以及对每个数据集上不同识别任务的VSR方法的分析。然而，[[44](#bib.bib44)]中的研究主要集中在2018年之前的工作，而最近的显著成功并未涵盖。最近，Fenghour等人[[43](#bib.bib43)]进行了一项调查，回顾了深度学习驱动的VSR方法，包括视听数据集、特征提取、分类网络和分类方案。然而，一些重要的VSR进展被遗漏了，例如自监督学习方法[[47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)]，跨模态知识蒸馏方法[[51](#bib.bib51),
    [52](#bib.bib52), [34](#bib.bib34)]，图神经网络骨干架构[[53](#bib.bib53), [54](#bib.bib54)]，等等。Chen等人[[42](#bib.bib42)]对几种具有代表性的独立于身份的VSG方法进行了深思熟虑的分析，并设计了一个VSG性能评估基准。然而，他们的核心贡献是明确的评估标准，而非对VSG方法的全面讨论和概述。
- en: Now we are in a place to summarize our main contributions in this paper.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来总结本文的主要贡献。
- en: $\bullet$
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: To the best of our knowledge, this is the *first* survey paper to systematically
    and comprehensively review deep learning methods for visual speech analysis, covering
    two fundamental problems, i.e., visual speech recognition and visual speech generation.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这篇文章是*首篇*系统性和全面评审深度学习方法在视觉语音分析中的应用的调查论文，涵盖了两个基本问题，即视觉语音识别和视觉语音生成。
- en: $\bullet$
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Problem definition, main challenges, benchmark datasets and testing protocols
    are summarized for each problem, and notably, the relationship among different
    VSA problems is also identified.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对每个问题进行了问题定义、主要挑战、基准数据集和测试协议的总结，并且特别地，还确定了不同VSA问题之间的关系。
- en: $\bullet$
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We propose a taxonomy to group the prominent methods. In addition, performance
    comparisons, merits, and demerits of representative approaches and their underlying
    connections are also analyzed.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种分类法来对突出的方法进行分组。此外，还分析了代表性方法的性能比较、优点和缺点及其潜在联系。
- en: $\bullet$
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Open issues and promising directions in this field are provided.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本领域的开放问题和有前景的方向已被提供。
- en: 'The remainder of this paper is structured as follows. The problem definitions
    and main challenges of VSA are summarized in Section [2](#S2 "2 Background ‣ Deep
    Learning for Visual Speech Analysis: A Survey"). In Section [3](#S3 "3 Datasets
    and Evaluation Metrics ‣ Deep Learning for Visual Speech Analysis: A Survey"),
    we review the audio-visual datasets and evaluation metrics and compare dataset
    attributes from multiple perspectives. Section [4](#S4 "4 Visual Speech Recognition
    ‣ Deep Learning for Visual Speech Analysis: A Survey") illustrates the general
    framework and representative methods for VSR. Section [5](#S5 "5 Visual Speech
    Generation ‣ Deep Learning for Visual Speech Analysis: A Survey") provides a comprehensive
    survey of existing methods for VSG. A taxonomy of VSR and VSG methods is illustrated
    in Fig. [2](#S1.F2 "Figure 2 ‣ 1.2 Differences with Related Surveys ‣ 1 Introduction
    ‣ Deep Learning for Visual Speech Analysis: A Survey"). In Section [6](#S6 "6
    Conclusion and Outlooks ‣ Deep Learning for Visual Speech Analysis: A Survey"),
    we conclude the paper and discuss the possible promising future research directions.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的剩余部分结构如下。VSA的问题定义和主要挑战在第[2](#S2 "2 Background ‣ Deep Learning for Visual
    Speech Analysis: A Survey")节中总结。在第[3](#S3 "3 Datasets and Evaluation Metrics ‣
    Deep Learning for Visual Speech Analysis: A Survey")节中，我们回顾了音频-视觉数据集和评价指标，并从多个角度比较数据集属性。第[4](#S4
    "4 Visual Speech Recognition ‣ Deep Learning for Visual Speech Analysis: A Survey")节阐述了VSR的一般框架和代表性方法。第[5](#S5
    "5 Visual Speech Generation ‣ Deep Learning for Visual Speech Analysis: A Survey")节提供了现有VSG方法的全面调查。第[2](#S1.F2
    "Figure 2 ‣ 1.2 Differences with Related Surveys ‣ 1 Introduction ‣ Deep Learning
    for Visual Speech Analysis: A Survey")节中的图示了VSR和VSG方法的分类。在第[6](#S6 "6 Conclusion
    and Outlooks ‣ Deep Learning for Visual Speech Analysis: A Survey")节中，我们总结了本文并讨论了未来可能有前景的研究方向。'
- en: '<svg   height="372.16" overflow="visible" version="1.1" width="763.86"><g transform="translate(0,372.16)
    matrix(1 0 0 -1 0 0) translate(15.47,0) translate(0,122.61)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -11.78 238.18)" fill="#000000"
    stroke="#000000"><foreignobject width="156.91" height="9.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Deep Learning on Visual Speech</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 19.71 224)" fill="#000000" stroke="#000000"><foreignobject
    width="187.78" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Visual
    Speech Recognition (Section [4](#S4 "4 Visual Speech Recognition ‣ Deep Learning
    for Visual Speech Analysis: A Survey"))</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 51.21 209.83)" fill="#000000" stroke="#000000"><foreignobject width="172.47"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Backbone
    architectures (Section [4.2](#S4.SS2 "4.2 Backbone Architectures ‣ 4 Visual Speech
    Recognition ‣ Deep Learning for Visual Speech Analysis: A Survey"))</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 195.66)" fill="#000000" stroke="#000000"><foreignobject
    width="177.23" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Visual
    frontend network (Section [4.2.1](#S4.SS2.SSS1 "4.2.1 Visual frontend network
    ‣ 4.2 Backbone Architectures ‣ 4 Visual Speech Recognition ‣ Deep Learning for
    Visual Speech Analysis: A Survey"))</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 114.2 185.53)" fill="#000000" stroke="#000000"><foreignobject width="630.8"
    height="20.32" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CNN based:
    VGG [[27](#bib.bib27)], ResNet [[55](#bib.bib55)], STCNN [[56](#bib.bib56)]…</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 114.2 167.31)" fill="#000000" stroke="#000000"><foreignobject
    width="431.51" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GCN
    based: STGCN [[53](#bib.bib53)],ASST-GCN [[54](#bib.bib54)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 114.2 153.14)" fill="#000000" stroke="#000000"><foreignobject
    width="260.38" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Transformer
    based: VTP [[57](#bib.bib57)]</foreignobject></g><g transform="matrix(1.0 0.0
    0.0 1.0 82.71 138.97)" fill="#000000" stroke="#000000"><foreignobject width="191.01"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Temporal
    backend network (Section [4.2.2](#S4.SS2.SSS2 "4.2.2 Temporal backend network
    ‣ 4.2 Backbone Architectures ‣ 4 Visual Speech Recognition ‣ Deep Learning for
    Visual Speech Analysis: A Survey"))</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 114.2 124.79)" fill="#000000" stroke="#000000"><foreignobject width="359.77"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RNN based:
    BiLSTM, BiGRU, BiConvLSTM [[58](#bib.bib58)]</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 114.2 110.62)" fill="#000000" stroke="#000000"><foreignobject width="501.32"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TCN based:
    MST-TCN [[59](#bib.bib59)], DS-TCN [[60](#bib.bib60)]</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 114.2 96.45)" fill="#000000" stroke="#000000"><foreignobject width="512.15"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Transformer
    based: Transformer [[6](#bib.bib6)], TF-blocks [[61](#bib.bib61)]…</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 51.21 82.27)" fill="#000000" stroke="#000000"><foreignobject
    width="157.24" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Learning
    Paradigms (Section [4.3](#S4.SS3 "4.3 Learning Paradigms ‣ 4 Visual Speech Recognition
    ‣ Deep Learning for Visual Speech Analysis: A Survey"))</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 68.1)" fill="#000000" stroke="#000000"><foreignobject
    width="154.17" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Supervised
    learning (Section [4.3.1](#S4.SS3.SSS1 "4.3.1 Supervised learning for VSR ‣ 4.3
    Learning Paradigms ‣ 4 Visual Speech Recognition ‣ Deep Learning for Visual Speech
    Analysis: A Survey"))</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0
    114.2 53.93)" fill="#000000" stroke="#000000"><foreignobject width="429.31" height="11.07"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CTC based: TM-CTC [[6](#bib.bib6)],
    LCANet [[62](#bib.bib62)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0
    1.0 114.2 39.75)" fill="#000000" stroke="#000000"><foreignobject width="437.7"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">seq2seq
    based: WAS [[28](#bib.bib28)], TM-seq2seq [[6](#bib.bib6)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 114.2 25.58)" fill="#000000" stroke="#000000"><foreignobject
    width="459.07" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">KD
    based: self-KD [[52](#bib.bib52)], cross-modal KD [[63](#bib.bib63)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 11.41)" fill="#000000" stroke="#000000"><foreignobject
    width="173.6" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Self-supervised
    learning (Section [4.3.2](#S4.SS3.SSS2 "4.3.2 Unsupervised learning for VSR ‣
    4.3 Learning Paradigms ‣ 4 Visual Speech Recognition ‣ Deep Learning for Visual
    Speech Analysis: A Survey"))</foreignobject></g><g transform="matrix(1.0 0.0 0.0
    1.0 114.2 -2.77)" fill="#000000" stroke="#000000"><foreignobject width="533.83"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Contrastive
    learning: AVE-Net [[47](#bib.bib47)], ADC-SSL [[50](#bib.bib50)]…</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 19.71 -16.94)" fill="#000000" stroke="#000000"><foreignobject
    width="184.66" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Visual
    Speech Generation (Section [5](#S5 "5 Visual Speech Generation ‣ Deep Learning
    for Visual Speech Analysis: A Survey"))</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 51.21 -31.11)" fill="#000000" stroke="#000000"><foreignobject width="167.55"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Two-stage
    Framework (Section [5.2](#S5.SS2 "5.2 Two-Stage VSG Framework ‣ 5 Visual Speech
    Generation ‣ Deep Learning for Visual Speech Analysis: A Survey"))</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 -45.29)" fill="#000000" stroke="#000000"><foreignobject
    width="517.97" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Landmark
    based: ObamaNet [[64](#bib.bib64)],ATVG [[65](#bib.bib65)]…</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 -56.69)" fill="#000000" stroke="#000000"><foreignobject
    width="635.6" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Coefficient
    based: AAM [[66](#bib.bib66)], Blendshape [[67](#bib.bib67)], 3DMM [[68](#bib.bib68)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 -73.63)" fill="#000000" stroke="#000000"><foreignobject
    width="490.19" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Vertex
    based: VOCA [[69](#bib.bib69)], LipsyNc3D [[70](#bib.bib70)]…</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 51.21 -87.81)" fill="#000000" stroke="#000000"><foreignobject
    width="166.01" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">One-stage
    Framework (Section [5.3](#S5.SS3 "5.3 One-Stage VSG Frameworks ‣ 5 Visual Speech
    Generation ‣ Deep Learning for Visual Speech Analysis: A Survey"))</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 -97.94)" fill="#000000" stroke="#000000"><foreignobject
    width="634.57" height="20.32" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GAN
    based: DAVS [[71](#bib.bib71)], Wav2Lip [[38](#bib.bib38)], S2TF [[72](#bib.bib72)]…</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 -116.15)" fill="#000000" stroke="#000000"><foreignobject
    width="376.27" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Others:
    AD-NeRF [[73](#bib.bib73)], DCKs [[74](#bib.bib74)]…</foreignobject></g></g></g></svg>'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="372.16" overflow="visible" version="1.1" width="763.86"><g transform="translate(0,372.16)
    matrix(1 0 0 -1 0 0) translate(15.47,0) translate(0,122.61)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -11.78 238.18)" fill="#000000"
    stroke="#000000"><foreignobject width="156.91" height="9.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">视觉语音中的深度学习</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 19.71 224)" fill="#000000" stroke="#000000"><foreignobject width="187.78"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">视觉语音识别（第[4](#S4
    "4 视觉语音识别 ‣ 深度学习在视觉语音分析中的应用：调查")节）</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 51.21 209.83)" fill="#000000" stroke="#000000"><foreignobject width="172.47"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">主干网络架构（第[4.2](#S4.SS2
    "4.2 主干网络架构 ‣ 4 视觉语音识别 ‣ 深度学习在视觉语音分析中的应用：调查")节）</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 82.71 195.66)" fill="#000000" stroke="#000000"><foreignobject width="177.23"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">视觉前端网络（第[4.2.1](#S4.SS2.SSS1
    "4.2.1 视觉前端网络 ‣ 4.2 主干网络架构 ‣ 4 视觉语音识别 ‣ 深度学习在视觉语音分析中的应用：调查")节）</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 114.2 185.53)" fill="#000000" stroke="#000000"><foreignobject
    width="630.8" height="20.32" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">基于CNN的：VGG
    [[27](#bib.bib27)]，ResNet [[55](#bib.bib55)]，STCNN [[56](#bib.bib56)]…</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 114.2 167.31)" fill="#000000" stroke="#000000"><foreignobject
    width="431.51" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">基于GCN的：STGCN
    [[53](#bib.bib53)]，ASST-GCN [[54](#bib.bib54)]</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 114.2 153.14)" fill="#000000" stroke="#000000"><foreignobject width="260.38"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">基于Transformer的：VTP
    [[57](#bib.bib57)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 82.71
    138.97)" fill="#000000" stroke="#000000"><foreignobject width="191.01" height="11.07"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">时间后端网络（第[4.2.2](#S4.SS2.SSS2
    "4.2.2 时间后端网络 ‣ 4.2 主干网络架构 ‣ 4 视觉语音识别 ‣ 深度学习在视觉语音分析中的应用：调查")节）</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 114.2 124.79)" fill="#000000" stroke="#000000"><foreignobject
    width="359.77" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">基于RNN的：BiLSTM，BiGRU，BiConvLSTM
    [[58](#bib.bib58)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 114.2
    110.62)" fill="#000000" stroke="#000000"><foreignobject width="501.32" height="11.07"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">基于TCN的：MST-TCN [[59](#bib.bib59)]，DS-TCN
    [[60](#bib.bib60)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 114.2
    96.45)" fill="#000000" stroke="#000000"><foreignobject width="512.15" height="11.07"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">基于Transformer的：Transformer
    [[6](#bib.bib6)]，TF-blocks [[61](#bib.bib61)]…</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 51.21 82.27)" fill="#000000" stroke="#000000"><foreignobject width="157.24"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">学习范式（第[4.3](#S4.SS3
    "4.3 学习范式 ‣ 4 视觉语音识别 ‣ 深度学习在视觉语音分析中的应用：调查")节）</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 82.71 68.1)" fill="#000000" stroke="#000000"><foreignobject width="154.17"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">监督学习（第[4.3.1](#S4.SS3.SSS1
    "4.3.1 监督学习用于VSR ‣ 4.3 学习范式 ‣ 4 视觉语音识别 ‣ 深度学习在视觉语音分析中的应用：调查")节）</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 114.2 53.93)" fill="#000000" stroke="#000000"><foreignobject
    width="429.31" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">基于CTC的：TM-CTC
    [[6](#bib.bib6)]，LCANet [[62](#bib.bib62)]</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 114.2 39.75)" fill="#000000" stroke="#000000"><foreignobject width="437.7"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">基于seq2seq的：WAS
    [[28](#bib.bib28)]，TM-seq2seq [[6](#bib.bib6)]</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 114.2 25.58)" fill="#000000" stroke="#000000"><foreignobject width="459.07"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">基于KD的：self-KD
    [[52](#bib.bib52)]，cross-modal KD [[63](#bib.bib63)]</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 82.71 11.41)" fill="#000000" stroke
- en: 'Figure 2: A taxonomy of representative visual speech recognition and generation
    methods.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：代表性的视觉语音识别和生成方法的分类。
- en: 2 Background
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: '![Refer to caption](img/29b69f82a0b6056f07e06b063bb66075.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/29b69f82a0b6056f07e06b063bb66075.png)'
- en: 'Figure 3: The two formal-dual fundamental problems of visual speech analysis.
    Top part: Visual speech recognition or lip reading; Bottom part: Visual speech
    generation or lip sequence generation.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：视觉语音分析的两个形式对偶的基本问题。上部：视觉语音识别或唇读；下部：视觉语音生成或唇部序列生成。
- en: 2.1 The Problems
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题
- en: 'Visual speech analysis can be divided into two fundamental problems: recognition
    and generation. As shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2 Background ‣ Deep Learning
    for Visual Speech Analysis: A Survey"), the two problems are formal-dual and have
    a reverse pipeline.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉语音分析可以分为两个基本问题：识别和生成。如图[3](#S2.F3 "图 3 ‣ 2 背景 ‣ 视觉语音分析的深度学习：综述")所示，这两个问题是形式上的对偶，并具有相反的处理流程。
- en: 'Visual speech recognition (VSR), also known as automatic lip reading, involves
    designing algorithms to infer the text content according to the speaker’s mouth
    movements. Given a talking face video, a VSR system first crops the video and
    gets the mouth-centered cropped video. And then, it decodes the cropped video
    into a specific type of text (words, phrases or sentences). According to recognition
    targets, VSR mainly includes two types: word-level and sentence-lvel. The word-level
    VSR aims to classify the input video into one of a set of predefined word categories,
    while the sentence-level VSR tries to predict consecutive sentences from the input
    video. More specifically, a VSR system mainly consists of two sub-problems: visual
    speech representation learning and recognition. The extraction of discriminative
    visual speech features plays a relatively more important role since even the best
    recognizer will fail to achieve good results on poor visual speech features.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉语音识别（VSR），也称为自动唇读，涉及设计算法以根据说话者的口型推断文本内容。给定一个说话面部视频，VSR 系统首先裁剪视频并获得以嘴部为中心的裁剪视频。然后，它将裁剪的视频解码成特定类型的文本（词语、短语或句子）。根据识别目标，VSR
    主要包括两种类型：词级和句子级。词级 VSR 旨在将输入视频分类到预定义词汇类别之一，而句子级 VSR 尝试从输入视频中预测连续的句子。更具体地说，VSR
    系统主要由两个子问题组成：视觉语音表示学习和识别。由于即使是最好的识别器在差的视觉语音特征上也会表现不佳，因此提取具有辨别性的视觉语音特征显得相对更为重要。
- en: As a dual task of VSR, the goal of visual speech generation (VSG) is to synthesize
    a photo-realistic, high-quality talking video that corresponds to the driving
    source (e.g., a piece of reference audio or text) and the target identity. Specifically,
    a VSG system first extracts speech representations from the driving source and
    then fuses the learned speech representations with the target identity to output
    continual talking frames. From the perspective of the learning target, the goal
    of VSG is more subjective and diverse than that of VSR, making VSG a more challenging
    problem than VSR.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 VSR 的对偶任务，视觉语音生成（VSG）的目标是合成一个与驱动源（例如一段参考音频或文本）和目标身份对应的真实感、高质量的对话视频。具体而言，VSG
    系统首先从驱动源中提取语音表示，然后将学习到的语音表示与目标身份融合，以输出连续的对话帧。从学习目标的角度来看，VSG 的目标比 VSR 更具主观性和多样性，使得
    VSG 成为比 VSR 更具挑战性的问题。
- en: '![Refer to caption](img/bbb467053fbdfc046e713211a0dc1503.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/bbb467053fbdfc046e713211a0dc1503.png)'
- en: 'Figure 4: Main Challenges of visual speech analysis. (a) A taxonomy of main
    challenges. (b) Some practical examples of different challenges. (b1) The upper
    and lower lines are the respectively different visual dynamics of the word “wind”
    under different contexts; (b2) The upper video instance refers to the word “place”,
    while the lower video refers to the word ”please”. However, their visual dynamics
    are very similar; (b3) Two people speak the word ”after” respectively, with a
    noticeable difference in their lip motions; (b4) An example of real-time changes
    in the head pose of a speaker during talking.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：视觉语音分析的主要挑战。（a）主要挑战的分类。（b）不同挑战的一些实际示例。（b1）上面和下面的线分别是“wind”这个词在不同语境下的不同视觉动态；（b2）上面的视频实例指的是“place”这个词，而下面的视频指的是“please”这个词。然而，它们的视觉动态非常相似；（b3）两个人分别说“after”这个词，嘴唇动作有明显差异；（b4）说话者在说话过程中头部姿态的实时变化示例。
- en: 2.2 Main Challenges
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 主要挑战
- en: 'TABLE I: Statistics of commonly used audio-visual datasets.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：常用视听数据集的统计信息。
- en: '| Dataset Name | #Hours | #Vocab. | #Utter. | #Subj. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 数据集名称 | #小时 | #词汇 | #发言 | #受试者 |'
- en: '&#124; Image size &#124;'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像大小 &#124;'
- en: '&#124;  FPS &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FPS &#124;'
- en: '| Environment | Data Type | Year | Highlight | Download Link |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | 数据类型 | 年份 | 亮点 | 下载链接 |'
- en: '| AVICAR [[75](#bib.bib75)] | $\sim$33 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| AVICAR [[75](#bib.bib75)] | $\sim$33 |'
- en: '&#124; $26^{\dagger}$ &#124;'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $26^{\dagger}$ &#124;'
- en: '&#124; $13^{\ddagger}$ &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $13^{\ddagger}$ &#124;'
- en: '&#124; 1317 &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1317 &#124;'
- en: '| 59k | 86 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 59k | 86 |'
- en: '&#124; 720×480 &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 720×480 &#124;'
- en: '&#124; 30 &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 30 &#124;'
- en: '| Car-driving |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 汽车驾驶 |'
- en: '&#124; 4-view face- &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4视角面部- &#124;'
- en: '&#124; centered videos &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 居中视频 &#124;'
- en: '| 2004 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 2004 |'
- en: '&#124; Recorded in a car environment with various noise &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在车载环境中记录，含各种噪声 &#124;'
- en: '&#124; conditions; Consists of four scripts: isolated digits, &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 条件；包含四种脚本：孤立数字， &#124;'
- en: '&#124; isolated letters, phone numbers, and sentences &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 孤立字母、电话号码和句子 &#124;'
- en: '| [[76](#bib.bib76)] |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| [[76](#bib.bib76)] |'
- en: '| GRID [[77](#bib.bib77)] | $\sim$28 | 51 | 33k | 33 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| GRID [[77](#bib.bib77)] | $\sim$28 | 51 | 33k | 33 |'
- en: '&#124; 720×576 &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 720×576 &#124;'
- en: '&#124; 25 &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 25 &#124;'
- en: '| Lab-controlled |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 实验室控制 |'
- en: '&#124; 3-second face- &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3秒面部- &#124;'
- en: '&#124; centered videos &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 居中视频 &#124;'
- en: '| 2006 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 2006 |'
- en: '&#124; Each sentence consists of a six-word sequence &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 每句包含六词序列 &#124;'
- en: '&#124; of the specific form &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特定形式 &#124;'
- en: '| [[78](#bib.bib78)] |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| [[78](#bib.bib78)] |'
- en: '| MODALITY [[79](#bib.bib79)] | $\sim$31 | 182 | 5880 | 35 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 模态 [[79](#bib.bib79)] | $\sim$31 | 182 | 5880 | 35 |'
- en: '&#124; 1920×1080 &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1920×1080 &#124;'
- en: '&#124; 100 &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 100 &#124;'
- en: '| Lab-controlled |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 实验室控制 |'
- en: '&#124; Stereoscopic &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 立体感 &#124;'
- en: '&#124; RGB-D face- &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB-D 面部- &#124;'
- en: '&#124; centered videos &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 居中视频 &#124;'
- en: '| 2015 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 2015 |'
- en: '&#124; Command-like sentences; High resolution with &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 命令类句子；高分辨率，含 &#124;'
- en: '&#124; varying noise conditions &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 噪声条件变化 &#124;'
- en: '| [[80](#bib.bib80)] |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| [[80](#bib.bib80)] |'
- en: '| OuluVS2 [[81](#bib.bib81)] | $\sim$2 | N/A | 2120 | 53 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| OuluVS2 [[81](#bib.bib81)] | $\sim$2 | 不适用 | 2120 | 53 |'
- en: '&#124; 1920×1080 &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1920×1080 &#124;'
- en: '&#124; 30 &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 30 &#124;'
- en: '| Lab-controlled |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 实验室控制 |'
- en: '&#124; 5-view face- &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5视角面部- &#124;'
- en: '&#124; centered videos &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 居中视频 &#124;'
- en: '| 2015 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 2015 |'
- en: '&#124; Three types of utterances; High recording quality; &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三种发言类型；高录音质量； &#124;'
- en: '&#124; Five views from 0-90 &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从0到90度的五个视角 &#124;'
- en: '| [[82](#bib.bib82)] |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| [[82](#bib.bib82)] |'
- en: '| IBM AV-ASR [[83](#bib.bib83)] | $\sim$40 | $\sim$10.4k | N/A | 262 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| IBM AV-ASR [[83](#bib.bib83)] | $\sim$40 | $\sim$10.4k | 不适用 | 262 |'
- en: '&#124; 704×480 &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 704×480 &#124;'
- en: '&#124; 30 &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 30 &#124;'
- en: '| Lab-controlled |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 实验室控制 |'
- en: '&#124; Face-centered &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部中心 &#124;'
- en: '&#124; videos &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频 &#124;'
- en: '| 2015 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 2015 |'
- en: '&#124; Large vocabulary size; Recorded in clean, studio &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大词汇量；在干净、录音室环境中记录 &#124;'
- en: '&#124; conditions &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 条件 &#124;'
- en: '| Unpublic |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 未公开 |'
- en: '| LRW [[27](#bib.bib27)] | $\sim$111 | 500 | $\sim$539K | 1k+ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| LRW [[27](#bib.bib27)] | $\sim$111 | 500 | $\sim$539K | 1k+ |'
- en: '&#124; 256×256 &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 256×256 &#124;'
- en: '&#124; 25 &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 25 &#124;'
- en: '| In-the-wild |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 在野外 |'
- en: '&#124; 1.2-second face- &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1.2秒面部- &#124;'
- en: '&#124; centered videos &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 居中视频 &#124;'
- en: '| 2016 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 2016 |'
- en: '&#124; Collected from British television programs; &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从英国电视节目中收集； &#124;'
- en: '&#124; Each video corresponds to a word category &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 每个视频对应一个词汇类别 &#124;'
- en: '| [[84](#bib.bib84)] |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| [[84](#bib.bib84)] |'
- en: '| LRS2-BBC [[6](#bib.bib6)] | $\sim$225 | $\sim$62.8k | $\sim$144.5k | 1k+
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| LRS2-BBC [[6](#bib.bib6)] | $\sim$225 | $\sim$62.8k | $\sim$144.5k | 1k+
    |'
- en: '&#124; 160×160 &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 160×160 &#124;'
- en: '&#124; 25 &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 25 &#124;'
- en: '| In-the-wild |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 在野外 |'
- en: '&#124; Face-centered &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部中心 &#124;'
- en: '&#124; videos &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频 &#124;'
- en: '| 2017 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 2017 |'
- en: '&#124; Collected from British television; Large-scale; &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从英国电视中收集；大规模； &#124;'
- en: '&#124; Open-world; Sentence-level lip reading &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开放世界；句子级唇读 &#124;'
- en: '| [[85](#bib.bib85)] |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| [[85](#bib.bib85)] |'
- en: '| VoxCeleb1 [[30](#bib.bib30)] | $\sim$352 | N/A | $\sim$153.5k | 1k+ | N/A
    | In-the-wild |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| VoxCeleb1 [[30](#bib.bib30)] | $\sim$352 | 不适用 | $\sim$153.5k | 1k+ | 不适用
    | 在野外 |'
- en: '&#124; Public YouTube &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 公共YouTube &#124;'
- en: '&#124; videos &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频 &#124;'
- en: '| 2017 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 2017 |'
- en: '&#124; Large-scale; In-the-wild; Mainly for speaker &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大规模；在野外；主要用于发言人 &#124;'
- en: '&#124; identification and verification &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 识别与验证 &#124;'
- en: '| [[86](#bib.bib86)] |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| [[86](#bib.bib86)] |'
- en: '| ObamaSet [[87](#bib.bib87)] | $\sim$14 | N/A | N/A | 1 | N/A | In-the-wild
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| ObamaSet [[87](#bib.bib87)] | $\sim$14 | 不适用 | 不适用 | 1 | 不适用 | 在野外 |'
- en: '&#124; Public YouTube &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 公共YouTube &#124;'
- en: '&#124; videos &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频 &#124;'
- en: '| 2017 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 2017 |'
- en: '&#124; Focuses on Barack Obama; Collected from Obama’s &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重点关注奥巴马；从奥巴马的 &#124;'
- en: '&#124; weekly presidential addresses; High quality &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 每周总统讲话；高质量 &#124;'
- en: '| [[88](#bib.bib88)] |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [[88](#bib.bib88)] |'
- en: '| LRS3-TED [[89](#bib.bib89)] | $\sim$475 | $\sim$71.1k | $\sim$151.8k | 5k+
    |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| LRS3-TED [[89](#bib.bib89)] | $\sim$475 | $\sim$71.1k | $\sim$151.8k | 5k+
    |'
- en: '&#124; 224×224 &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 224×224 &#124;'
- en: '&#124; 25 &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 25 &#124;'
- en: '| In-the-wild |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 野外场景 |'
- en: '&#124; Face-centered &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部中心 &#124;'
- en: '&#124; videos &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频 &#124;'
- en: '| 2018 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 2018 |'
- en: '&#124; Larger-scale; Along with the corresponding &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 更大规模；以及相应的 &#124;'
- en: '&#124; subtitles and word alignment boundaries &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 字幕和词对齐边界 &#124;'
- en: '| [[90](#bib.bib90)] |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| [[90](#bib.bib90)] |'
- en: '| VoxCeleb2 [[29](#bib.bib29)] | $\sim$2.4k | N/A | $\sim$1.1m | 6k+ | N/A
    | In-the-wild |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| VoxCeleb2 [[29](#bib.bib29)] | $\sim$2.4k | 不适用 | $\sim$1.1m | 6k+ | 不适用
    | 野外场景 |'
- en: '&#124; Public YouTube &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 公共YouTube &#124;'
- en: '&#124; videos &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频 &#124;'
- en: '| 2018 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 2018 |'
- en: '&#124; Significantly larger scale; A wide range of different &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 显著更大规模；广泛的不同 &#124;'
- en: '&#124; ethnicities, accents, professions, and ages &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 民族、口音、职业和年龄 &#124;'
- en: '| [[91](#bib.bib91)] |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| [[91](#bib.bib91)] |'
- en: '| LSVSR [[92](#bib.bib92)] | $\sim$3.9k | $\sim$127k | $\sim$2.9m | N/A | N/A
    | In-the-wild |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| LSVSR [[92](#bib.bib92)] | $\sim$3.9k | $\sim$127k | $\sim$2.9m | 不适用 | 不适用
    | 野外场景 |'
- en: '&#124; Face-centered &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部中心 &#124;'
- en: '&#124; videos &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频 &#124;'
- en: '| 2018 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 2018 |'
- en: '&#124; The largest existing visual speech recognition &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 现有最大规模的视觉语音识别 &#124;'
- en: '&#124; dataset; Extracted from public YouTube videos &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集；从公共YouTube视频中提取 &#124;'
- en: '| Unpublic |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 不公开 |'
- en: '| LRW-1000 [[31](#bib.bib31)] | $\sim$57 | 1k | $\sim$718K | 2k+ | N/A | In-the-wild
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| LRW-1000 [[31](#bib.bib31)] | $\sim$57 | 1k | $\sim$718K | 2k+ | 不适用 | 野外场景
    |'
- en: '&#124; Mouth-centered &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 口部中心 &#124;'
- en: '&#124; videos &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频 &#124;'
- en: '| 2019 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '&#124; The first large-scale Mandarin audio-visual &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 第一个大规模普通话音视频数据集 &#124;'
- en: '&#124; speech dataset; collected from TV programs &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语音数据集；从电视节目中收集 &#124;'
- en: '| [[93](#bib.bib93)] |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| [[93](#bib.bib93)] |'
- en: '| Faceforensics++ [[94](#bib.bib94)] | $\sim$5.7 | N/A | $\sim$1k | 1k | N/A
    | In-the-wild |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Faceforensics++ [[94](#bib.bib94)] | $\sim$5.7 | 不适用 | $\sim$1k | 1k | 不适用
    | 野外场景 |'
- en: '&#124; Manipulated &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 操作过的 &#124;'
- en: '&#124; talking videos &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对话视频 &#124;'
- en: '| 2019 | Commonly used for facial forgery detection. | [[95](#bib.bib95)] |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | 常用于面部伪造检测。 | [[95](#bib.bib95)] |'
- en: '| VOCASET [[69](#bib.bib69)] | N/A | N/A | 255 | 12 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| VOCASET [[69](#bib.bib69)] | 不适用 | 不适用 | 255 | 12 |'
- en: '&#124; 5023 vertices &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5023个顶点 &#124;'
- en: '&#124; 60 &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 60 &#124;'
- en: '| Lab-controlled |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 实验室控制 |'
- en: '&#124; 3d face &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D面部 &#124;'
- en: '&#124; mesh &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网格 &#124;'
- en: '| 2019 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '&#124; Higher quality 3D scans as well as alignments &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 更高质量的3D扫描以及对齐 &#124;'
- en: '&#124; of the entire head &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 整个头部 &#124;'
- en: '| [[96](#bib.bib96)] |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] |'
- en: '| MEAD [[97](#bib.bib97)] | $\sim$39 | N/A | N/A | 60 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| MEAD [[97](#bib.bib97)] | $\sim$39 | 不适用 | 不适用 | 60 |'
- en: '&#124; 1920×1080 &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1920×1080 &#124;'
- en: '&#124; 30 &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 30 &#124;'
- en: '| Lab-controlled |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 实验室控制 |'
- en: '&#124; 7-view face- &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 7视角面部- &#124;'
- en: '&#124; centered videos &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 中心视频 &#124;'
- en: '| 2020 | Multi-view Emotional Audio-visual Dataset | [[98](#bib.bib98)] |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | 多视角情感音视频数据集 | [[98](#bib.bib98)] |'
- en: '| HDTF [[68](#bib.bib68)] | $\sim$15.8 | N/A | 10k+ | 300+ | N/A | In-the-wild
    |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| HDTF [[68](#bib.bib68)] | $\sim$15.8 | 不适用 | 10k+ | 300+ | 不适用 | 野外场景 |'
- en: '&#124; Face-centered &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部中心 &#124;'
- en: '&#124; videos &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频 &#124;'
- en: '| 2021 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 2021 |'
- en: '&#124; Higher video resolution than previous in-the-wild &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 比之前野外场景中的视频分辨率更高 &#124;'
- en: '&#124; datasets &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集 &#124;'
- en: '| [[99](#bib.bib99)] |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| [[99](#bib.bib99)] |'
- en: '1'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: '$\dagger$: Alphabets, $\ddagger$: Digits.'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\dagger$: 字母，$\ddagger$: 数字。'
- en: 'Despite several years of development, most VSA methods have not been capable
    of meeting real-world requirements due to various challenges. As illustrated in
    Fig. [4](#S2.F4 "Figure 4 ‣ 2.1 The Problems ‣ 2 Background ‣ Deep Learning for
    Visual Speech Analysis: A Survey")(a), to systematically present the challenges
    in VSA, we classify the main difficulties from recognition-related, and generation-related
    and discuss the challenges of audio-visual datasets. In Fig. [4](#S2.F4 "Figure
    4 ‣ 2.1 The Problems ‣ 2 Background ‣ Deep Learning for Visual Speech Analysis:
    A Survey")(b)&(c), we provide some instances of typical challenges for intuitively
    understanding.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管经过多年的发展，大多数视觉语音分析（VSA）方法由于各种挑战未能满足实际需求。如图[4](#S2.F4 "图 4 ‣ 2.1 问题 ‣ 2 背景 ‣
    深度学习在视觉语音分析中的应用：一项综述")（a）所示，为了系统地呈现VSA中的挑战，我们将主要困难分类为与识别相关的和与生成相关的，并讨论音视频数据集的挑战。在图[4](#S2.F4
    "图 4 ‣ 2.1 问题 ‣ 2 背景 ‣ 深度学习在视觉语音分析中的应用：一项综述")（b）&（c）中，我们提供了一些典型挑战的实例，以便直观理解。
- en: 2.2.1 Recognition-related Challenges
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 识别相关挑战
- en: From the perspective of representation learning, the ideal of visual speech
    recognition is to extract speech-related features with strong distinctiveness
    and robustness. However, both of the two goals suffer from severe practical challenges.
    Recognition-related challenges mainly stem from (1) the vast range of intra-class
    variations and (2) the inter-class similarities.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 从表示学习的角度来看，视觉语音识别的理想是提取具有强区分性和鲁棒性的语音相关特征。然而，这两个目标都面临着严重的实际挑战。识别相关挑战主要源于（1）类内变化的广泛范围和（2）类间相似性。
- en: 'Intra-class variations can be organized into two types: visual speech intrinsic
    factors and other recognition-irrelevant factors. In terms of visual speech intrinsic
    factors, as shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1 The Problems ‣ 2 Background
    ‣ Deep Learning for Visual Speech Analysis: A Survey")(b1), a word can produce
    quite different visual dynamics due to different contexts, continuous reading,
    speech emotion changing, speaking rate, etc. Meanwhile, many types of speech-irrelevant
    interference information dramatically impact visual speech recognition, such as
    speaker difference, head pose movement, facial expression, imaging condition,
    and so on. As illustrated in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1 The Problems ‣ 2
    Background ‣ Deep Learning for Visual Speech Analysis: A Survey")(b3), although
    the two speakers say the same word “after”, their lip motions look different.
    Early research on visual speech recognition only considered frontal talking face
    videos in laboratory environments, limiting its availability in practical applications.
    Nowadays, visual speech recognition in-the-wild has attracted more and more attention,
    many audio-visual datasets were collected from various realistic scenes (TV News,
    public speech video…). Fig. [4](#S2.F4 "Figure 4 ‣ 2.1 The Problems ‣ 2 Background
    ‣ Deep Learning for Visual Speech Analysis: A Survey")(b4) shows an example video
    from the LRS3 dataset, a speaker is talking with dramatic head pose changes. it
    is hard to eliminate interference of these irrelevant motion information under
    the unconstrained environment. In addition to head pose changes, illumination,
    noise corruption, poor resolution etc also bring great difficulties to visual
    speech recognition.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '类内变化可以分为两种类型：视觉语音固有因素和其他与识别无关的因素。就视觉语音固有因素而言，如图[4](#S2.F4 "Figure 4 ‣ 2.1 The
    Problems ‣ 2 Background ‣ Deep Learning for Visual Speech Analysis: A Survey")(b1)所示，由于不同的语境、连续阅读、语音情感变化、语速等，一个词可以产生非常不同的视觉动态。同时，许多与语音无关的干扰信息对视觉语音识别产生了显著影响，如说话者差异、头部姿势变化、面部表情、成像条件等。如图[4](#S2.F4
    "Figure 4 ‣ 2.1 The Problems ‣ 2 Background ‣ Deep Learning for Visual Speech
    Analysis: A Survey")(b3)所示，尽管两位说话者说的是相同的词“after”，他们的唇部动作看起来却不同。早期的视觉语音识别研究仅考虑了实验室环境中的正面讲述面部视频，限制了其在实际应用中的可用性。如今，野外的视觉语音识别引起了越来越多的关注，许多音视频数据集来自各种现实场景（如电视新闻、公共演讲视频等）。图[4](#S2.F4
    "Figure 4 ‣ 2.1 The Problems ‣ 2 Background ‣ Deep Learning for Visual Speech
    Analysis: A Survey")(b4)展示了来自LRS3数据集的一个示例视频，其中一位说话者的头部姿势变化非常剧烈。在不受约束的环境下，排除这些无关运动信息的干扰非常困难。除了头部姿势变化，光照、噪声干扰、分辨率低等也给视觉语音识别带来了很大的困难。'
- en: 'Besides intra-class variations, visual speech recognition also suffers from
    inter-class similarities. As we know, the phoneme is the smallest recognizable
    unit of sound in a language that serves to distinguish words from one another.
    Similarly, viseme is the smallest recognizable unit of visual speech. The number
    of visemes is much smaller than that of phonemes. There are about three times
    as many phonemes as visemes in English. Therefore, several phonemes map onto a
    few visemes. Some phonemes, such as [p] and [b] [k] and [g], [t] and [d], etc have
    almost the same visual characteristics, so they are almost indistinguishable without
    considering the context in visual domain. We define this phenomenon as visual
    ambiguity, the leading cause of inter-class similarities. Fig. [4](#S2.F4 "Figure
    4 ‣ 2.1 The Problems ‣ 2 Background ‣ Deep Learning for Visual Speech Analysis:
    A Survey")(b2) demonstrates an instance of word-level visual ambiguity. The other
    challenge of inter-class similarities stems from thousands of word classes. Subtle
    differences (e.g. various forms of words) in different word classes make the problem
    more difficult.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '除了类内变化，视觉语音识别还受到类间相似性的影响。正如我们所知，音位是语言中最小的可识别声音单位，用于区分单词。类似地，视音位是视觉语音中最小的可识别单位。视音位的数量远小于音位。在英语中，视音位的数量约为音位的三分之一。因此，几个音位对应到少数几个视音位。一些音位，如[p]和[b]，[k]和[g]，[t]和[d]等，具有几乎相同的视觉特征，因此在视觉领域不考虑上下文的情况下几乎无法区分。我们将这种现象定义为视觉模糊，这是类间相似性的主要原因。图[4](#S2.F4
    "Figure 4 ‣ 2.1 The Problems ‣ 2 Background ‣ Deep Learning for Visual Speech
    Analysis: A Survey")(b2)展示了一个单词级视觉模糊的实例。类间相似性的另一个挑战来自于成千上万的词类。不同词类中的细微差别（例如词的不同形式）使得问题更加复杂。'
- en: 2.2.2 Generation-Related Challenges
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 生成相关的挑战
- en: 'Different from visual speech recognition, visual speech generation requires
    not only speech-related information but also identity-related information. As
    shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1 The Problems ‣ 2 Background ‣ Deep Learning
    for Visual Speech Analysis: A Survey")(a), generation-related challenges mainly
    come from (1) information coupling, (2) diversity targets, and (3) evaluation
    validity.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '与视觉语音识别不同，视觉语音生成不仅需要与语音相关的信息，还需要与身份相关的信息。如图[4](#S2.F4 "Figure 4 ‣ 2.1 The Problems
    ‣ 2 Background ‣ Deep Learning for Visual Speech Analysis: A Survey")(a)所示，生成相关的挑战主要来自于（1）信息耦合，（2）多样性目标，以及（3）评估有效性。'
- en: 'A talking face video contains many types of coupled information, such as various
    motion-related information and identity-related information. For motion coupling,
    motions that occur on a talking face video can be categorized into two types:
    intrinsic motions (head pose, facial expression, lip motion, etc.) and extrinsic
    motions (camera motion, background motion, etc.). All of these various motions
    are highly coupled. The motion coupling challenge stems not only from disentangling
    lip motion from all of these speech-irrelevant motions but also from integrating
    the synthesized lip sequence into a given identity image. The other coupling issue
    of visual speech generation is identity coupling. As illustrated in Fig. [4](#S2.F4
    "Figure 4 ‣ 2.1 The Problems ‣ 2 Background ‣ Deep Learning for Visual Speech
    Analysis: A Survey")(c1), people may feel eerie and uncomfortable while observing
    these images due to the identity of generated face subtlely changed. This phenomenon,
    also known as the “uncanny valley effect” [[100](#bib.bib100)], occurs when people
    observe a synthetic face that’s almost human-like but not quite perfect. Generally,
    the driving source contains rich information about the source identity. Therefore,
    the critical challenge is how to remove the identity information from the driving
    source to avoid corruption in the process of the target identity synthesis. Besides,
    most existing methods are only adaptive to specific target identity since different
    speakers have significant differences in appearance, speaking habits, etc. So,
    the lack of identity generalization is also an important challenge.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '一段带有对话的面部视频包含多种耦合信息，例如各种与运动相关的信息和与身份相关的信息。对于运动耦合来说，在对话面部视频中发生的运动可以分为两种类型：内在运动（头部姿势、面部表情、嘴唇运动等）和外在运动（相机运动、背景运动等）。所有这些不同的运动都是高度耦合的。运动耦合的挑战不仅在于将嘴唇运动与所有这些与语音无关的运动分离开来，还在于将合成的嘴唇序列与给定的身份图像进行整合。视觉语音生成的另一个耦合问题是身份耦合。如图 [4](#S2.F4
    "Figure 4 ‣ 2.1 The Problems ‣ 2 Background ‣ Deep Learning for Visual Speech
    Analysis: A Survey")(c1) 所示，人们在观察这些图像时可能会感到毛骨悚然和不适，因为生成的面部身份被微妙地改变了。这种现象，也被称为“恐怖谷效应” [[100](#bib.bib100)]，发生在当人们观察一个几乎像人的合成面孔时，但却不完美。通常，驱动源包含了丰富的源身份信息。因此，关键挑战在于如何从驱动源中去除身份信息，以避免在目标身份合成过程中出现污染。此外，大多数现有的方法只能适应特定的目标身份，因为不同的发言者在外貌、说话习惯等方面存在显著差异。因此，身份泛化的缺乏也是一个重要的挑战。'
- en: 'Semantic consistency and visual quality are the most desired properties of
    an excellent VSG method. Semantic consistency represents that the synthesized
    lip sequence should be synchronized and speech-consistent with the driving source.
    As shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1 The Problems ‣ 2 Background ‣ Deep
    Learning for Visual Speech Analysis: A Survey")(c2), semantic consistency mainly
    involves two demands: temporal alignment and speech matching. However, the synchronous
    speech mapping between the driving source and the generated talking video is difficult
    to realize due to intrinsic differences in temporal resolution and speech characteristics
    of different data modalities. As for visual quality, There are two difficulties:
    (1) There is a lack of explicit training objectives since the fidelity and visual
    quality of the generated lip motion sequences are difficult to define quantitatively.
    (2) Because humans are sensitive to subtle artifacts, integrating the generated
    lip sequence into the whole face without people-oriented perceptual errors is
    a complicated issue.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '语义一致性和视觉质量是优秀视觉语音生成（VSG）方法最期望的特性。语义一致性表示合成的嘴唇序列应与驱动源在同步和语音一致性方面保持一致。如图 [4](#S2.F4
    "Figure 4 ‣ 2.1 The Problems ‣ 2 Background ‣ Deep Learning for Visual Speech
    Analysis: A Survey")(c2) 所示，语义一致性主要涉及两个要求：时间对齐和语音匹配。然而，由于不同数据模态在时间分辨率和语音特征上的固有差异，实现驱动源与生成对话视频之间的同步语音映射是困难的。至于视觉质量，有两个难点：（1）由于生成的嘴唇运动序列的保真度和视觉质量难以定量定义，因此缺乏明确的训练目标。（2）由于人类对细微的伪影非常敏感，将生成的嘴唇序列无误地整合到整个面部中而不产生以人为主的感知错误是一个复杂的问题。'
- en: Besides the aforementioned difficulties, efficiently evaluating visual speech
    generation methods is another challenge. Existing evaluations on this problem,
    including qualitative and quantitative metrics, have many limitations. For example,
    qualitative metrics like user study are unreproducible and unstable. As for quantitative
    metrics, although there are a dozen of evaluation metrics, some of them are not
    appropriate and even mutually contradictory.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述困难之外，高效评估视觉语音生成方法是另一个挑战。现有的评估，包括定性和定量指标，存在许多局限性。例如，像用户研究这样的定性指标是不可重复的和不稳定的。至于定量指标，尽管有十几种评估指标，但其中一些并不合适，甚至相互矛盾。
- en: 2.2.3 Dataset-Related Challenges
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 数据集相关挑战
- en: In addition to the above problem-oriented difficulties and challenges, audio-visual
    dataset-related issues also significantly impact the progress of VSA. Since that
    most of the current deep learning methods are data-driven, the importance of datasets
    is self-evident. However, existing audio-visual datasets suffer from small scale
    and weak annotations due to privacy protection and high labor cost. A potential
    research direction is to realize cross-modal self-supervised visual speech learning [[50](#bib.bib50),
    [101](#bib.bib101), [49](#bib.bib49)] based on unlabeled audio-visual data. Despite
    this, the issue of dataset scale limitation remains to be resolved.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述以问题为导向的困难和挑战之外，音视频数据集相关的问题也对视觉语音分析（VSA）的进展产生了显著影响。由于目前大多数深度学习方法都是数据驱动的，因此数据集的重要性不言而喻。然而，现有的音视频数据集由于隐私保护和高劳动成本而遭受规模小和标注薄弱的问题。一个潜在的研究方向是基于未标注的音视频数据实现跨模态自监督视觉语音学习[[50](#bib.bib50),
    [101](#bib.bib101), [49](#bib.bib49)]。尽管如此，数据集规模限制的问题仍然有待解决。
- en: '![Refer to caption](img/e75cef180aa0cc80765216b4b4c7c812.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e75cef180aa0cc80765216b4b4c7c812.png)'
- en: 'Figure 5: Some example images from AVICAR, OuluVS2, Faceforensics++, GRID,
    LRW, LRS2-BBC, VoxCeleb1, VoxCeleb2, MODALITY, ObamaSet, LRS3-TED, LRW-1000, VOCASET,
    HDTF, MEAD. See Table. [I](#S2.T1 "TABLE I ‣ 2.2 Main Challenges ‣ 2 Background
    ‣ Deep Learning for Visual Speech Analysis: A Survey") for a summary of these
    datasets.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '图5：来自AVICAR、OuluVS2、Faceforensics++、GRID、LRW、LRS2-BBC、VoxCeleb1、VoxCeleb2、MODALITY、ObamaSet、LRS3-TED、LRW-1000、VOCASET、HDTF、MEAD的一些示例图像。有关这些数据集的总结，请参见表[I](#S2.T1
    "TABLE I ‣ 2.2 Main Challenges ‣ 2 Background ‣ Deep Learning for Visual Speech
    Analysis: A Survey")。'
- en: 3 Datasets and Evaluation Metrics
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据集与评估指标
- en: Datasets have played an important role throughout the history of visual speech
    research, especially in the big data era. First, benchmark datasets serve as a
    common platform for measuring and comparing performances of competing VSA algorithms;
    Second, as a typical data-driven learning strategy, deep learning technologies
    have made significant progress in many audio-visual learning tasks. It is worth
    noting that the large amounts of annotated data play a crucial role in their success;
    Third, datasets also further push the field towards increasingly complicated and
    challenging problems. Therefore, in this section, we first review the existing
    commonly used datasets for VSA with motivations, statistics, highlights, and the
    download links, then introduce evaluation metrics of different tasks, and finally
    discuss the future trends in audio-visual datasets.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在视觉语音研究的历史中扮演了重要角色，特别是在大数据时代。首先，基准数据集作为一个共同的平台，用于测量和比较竞争的VSA算法的性能；其次，作为典型的数据驱动学习策略，深度学习技术在许多音视频学习任务中取得了显著进展。值得注意的是，大量标注数据在其成功中发挥了至关重要的作用；第三，数据集还进一步推动了该领域向越来越复杂和具有挑战性的问题发展。因此，在本节中，我们首先回顾现有的常用VSA数据集，包括其动机、统计信息、亮点以及下载链接，然后介绍不同任务的评估指标，最后讨论音视频数据集的未来趋势。
- en: 3.1 Datasets
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集
- en: 'There are dozens of commonly used audio-visual datasets built for VSA. The
    statistics, highlights and download links are summarized in Table [I](#S2.T1 "TABLE
    I ‣ 2.2 Main Challenges ‣ 2 Background ‣ Deep Learning for Visual Speech Analysis:
    A Survey"), and some selected sample images are shown in Fig. [5](#S2.F5 "Figure
    5 ‣ 2.2.3 Dataset-Related Challenges ‣ 2.2 Main Challenges ‣ 2 Background ‣ Deep
    Learning for Visual Speech Analysis: A Survey"). We divide these datasets into
    two types: controlled and uncontrolled environments. We introduce them briefly
    in the following.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '有几十个常用的视听数据集是为VSA（视觉语音分析）构建的。统计数据、亮点和下载链接汇总在表格[I](#S2.T1 "TABLE I ‣ 2.2 Main
    Challenges ‣ 2 Background ‣ Deep Learning for Visual Speech Analysis: A Survey")中，一些选定的样本图像展示在图[5](#S2.F5
    "Figure 5 ‣ 2.2.3 Dataset-Related Challenges ‣ 2.2 Main Challenges ‣ 2 Background
    ‣ Deep Learning for Visual Speech Analysis: A Survey")中。我们将这些数据集分为两种类型：受控环境和非受控环境。接下来，我们将简要介绍它们。'
- en: 3.1.1 Datasets under controlled environments
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 受控环境下的数据集
- en: 'As we can see from Table [I](#S2.T1 "TABLE I ‣ 2.2 Main Challenges ‣ 2 Background
    ‣ Deep Learning for Visual Speech Analysis: A Survey"), before 2015, visual speech
    research mainly focused on controlled environments. Controllable factors include
    recording conditions, equipment, data types, scripts, etc. These datasets provide
    an excellent foundation for visual speech research. Next, we review some representative
    audio-visual datasets collected under controlled environments.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '从表格[I](#S2.T1 "TABLE I ‣ 2.2 Main Challenges ‣ 2 Background ‣ Deep Learning
    for Visual Speech Analysis: A Survey")中可以看出，在2015年之前，视觉语音研究主要集中在受控环境中。可控因素包括录音条件、设备、数据类型、脚本等。这些数据集为视觉语音研究提供了极好的基础。接下来，我们将回顾一些在受控环境下收集的代表性视听数据集。'
- en: AVICAR [[75](#bib.bib75)] is the most representative public audio-visual dataset
    recorded in a car-driving environment. As mentioned above, visual speech can contribute
    to audio-based speech recognition, especially in noisy environments. Motivated
    by this, AVICAR is collected for modeling bimodal speech in a driving car, as
    car-driving is a typical acoustic noisy environment.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: AVICAR [[75](#bib.bib75)] 是在汽车驾驶环境中录制的最具代表性的公共视听数据集。如前所述，视觉语音可以有助于基于音频的语音识别，特别是在嘈杂环境中。受到此激励，AVICAR被收集用于建模驾驶汽车中的双模态语音，因为汽车驾驶是典型的噪声环境。
- en: GRID [[77](#bib.bib77)], consisting of high-quality audio and video recordings
    of 1,000 syntactically identical phrases spoken by 34 talkers, is built for comprehensive
    audio-visual perceptual analysis and microscopic modeling. Besides speech recognition,
    it can also support audio-visual speech separation tasks.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: GRID [[77](#bib.bib77)] 包含34位说话者说出1,000个语法上相同的短语的高质量音频和视频录音，旨在进行全面的视听感知分析和微观建模。除了语音识别外，它还可以支持视听语音分离任务。
- en: MODALITY [[79](#bib.bib79)] contains 31 hours of recordings was created to test
    the robustness of audio-visual speech recognition (AVSR) systems. As for the difference
    from other datasets, its corpus includes high-resolution, high-framerate stereoscopic
    video streams from RGB-D cameras.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: MODALITY [[79](#bib.bib79)] 包含31小时的录音，旨在测试视听语音识别（AVSR）系统的鲁棒性。与其他数据集的不同之处在于，其语料库包括来自RGB-D摄像头的高分辨率、高帧率立体视频流。
- en: OuluVS2 [[81](#bib.bib81)] is a multi-view audio-visual dataset built for non-rigid
    mouth motion analysis. It includes 53 speakers uttering three types of utterances.
    Moreover, it is recorded from five different views spanned between the frontal
    and profile views. Multiple views of talking mouths simulate a real-world situation,
    as users may not face the video camera all the time while talking.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: OuluVS2 [[81](#bib.bib81)] 是一个用于非刚性口腔运动分析的多视角视听数据集。它包括53位说话者发出三种类型的语句。此外，它是从五个不同视角记录的，视角跨度从正面到侧面。多个视角的说话口腔模拟了真实世界的情况，因为用户在讲话时可能并不总是面对摄像机。
- en: IBM AV-ASR [[83](#bib.bib83)] is a large corpus containing 40 hours of audio-visual
    recordings from 262 speakers in clean, studio conditions. Compared to previous
    datasets under controlled environments, it has significant advantages in vocabulary
    and speaker number. However, this dataset is not publicly available.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: IBM AV-ASR [[83](#bib.bib83)] 是一个包含262位说话者在清洁、录音室环境中录制的40小时视听记录的大型语料库。与之前的受控环境数据集相比，它在词汇量和说话者数量上具有显著优势。然而，该数据集并未公开提供。
- en: VOCASET [[69](#bib.bib69)] is a 4D face dataset with about 29 minutes of 4D
    face scans with synchronized audio from 12 speakers (6 females and 6 males), and
    the 4D face scans are recorded at 60fps. As a representative high-quality 4D face
    audio-visual dataset, VOCASET greatly promoted the research on 3D VSG.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: VOCASET [[69](#bib.bib69)] 是一个带有约29分钟4D人脸扫描和同步音频的人脸数据集，其中包括12位演讲者（6位女性和6位男性）的数据，这些4D人脸扫描是以60fps的速度录制的。作为高质量的4D人脸音视频数据集的代表，VOCASET极大地推进了3D
    VSG的研究。
- en: MEAD [[97](#bib.bib97)], namely Multi-view Emotional Audio-visual Dataset, is
    a large-scale, high-quality emotional audio-visual dataset. Unlike previous datasets,
    it focuses on natural emotional talking face generation and takes multiple emotion
    states (eight different emotions at three intensity levels) into consideration.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: MEAD [[97](#bib.bib97)]，即Multi-view Emotional Audio-visual Dataset，是一个大规模、高质量的情感音视频数据集。与之前的数据集不同，它关注的是自然情感对话人脸生成，并考虑了多种情感状态（三个强度水平的八种不同情感）。
- en: 3.1.2 Datasets under uncontrolled environments
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 无控制环境下的数据集
- en: Recently, researchers are gradually shifting their focus to in-the-wild visual
    speech learning. As a result, many large-scale in-the-wild audio-visual datasets
    are constructed to promote the research. We introduce some of the audio-visual
    datasets collected under in-the-wild environments in the following.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究人员逐渐将重点转向野外环境下的视觉语音学习。为了推动研究，人们构建了许多大规模的野外音视频数据集。下面介绍一些在野外环境下收集的音视频数据集。
- en: LRW [[27](#bib.bib27)] is a word-level audio-visual dataset constructed by a
    multi-stage data automatic collection pipeline. It revolutionary enlarged the
    dataset scale and speaker number based on the rich data volume of BBC television
    programs. It contains over 1,000k word instances spoken by over a thousand people.
    The main objective of LRW is to test speaker-independent word-level lip reading
    methods.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: LRW [[27](#bib.bib27)] 是一个由多阶段数据自动采集流程构建的字级别音视频数据集。它从BBC电视节目的丰富数据量中革命性地扩大了数据规模和说话人数量。该数据集包含了超过一千人讲述的一百万个词语实例。LRW的主要目标是测试无关说话人的字级别唇读方法。
- en: LRS2-BBC [[6](#bib.bib6)] is a sentence-level audio-visual dataset with a similar
    data collection pipeline and data source as that LRW dataset. It is built for
    sentence-level lip reading, a more challenging VSR problem than word-level lip
    reading. All videos in LRS2-BBC are collected from the BBC program, and it contains
    over 144.5k utterances with a vocabulary size of about 62.8k.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: LRS2-BBC [[6](#bib.bib6)] 是一个句子级别的音视频数据集，具有与LRW数据集相似的数据收集流程和数据来源。它是为了句子级别的唇读而构建的，这是一个比字级别唇读更具挑战性的VSR问题。LRS2-BBC中的所有视频都是从BBC节目中收集的，它包含了约14.45万个话语，词汇量约为6.28万。
- en: VoxCeleb1 [[30](#bib.bib30)] is a large-scale text-independent audio-visual
    dataset collected from open-source YouTube media. It contains over 100k utterances
    from 1,251 celebrities. Although it is mainly built for speaker identification,
    it also can be used for VSG.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: VoxCeleb1 [[30](#bib.bib30)]是一个从开源YouTube媒体中收集的大规模无文本依赖的音视频数据集。它包含了1251位名人的超过10万个话语。虽然它主要用于说话人识别，但也可以用于VSG。
- en: ObamaSet [[87](#bib.bib87)] is a specific audio-visual dataset focused on the
    visual speech analysis of former US President Barack Obama. All video samples
    are collected from his weekly address footage. Unlike previous datasets, it focuses
    on Barack Obama only and does not provide any human annotations. Therefore, it
    is only used for Obama-oriented VSG.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ObamaSet [[87](#bib.bib87)] 是一个专注于对美国前总统巴拉克·奥巴马进行视觉演讲分析的特定音视频数据集。所有视频样本均从他的周播视频中收集而来。与之前的数据集不同，它仅关注巴拉克·奥巴马，并不提供任何人类注释。因此，它只用于面向奥巴马的VSG。
- en: LRS3-TED [[89](#bib.bib89)] is a large-scale sentence-level audio-visual dataset.
    Compared to LRS2-TED, it has a larger scale in terms of duration, vocabulary,
    and number of speakers. It consists of talking face videos from over 400 hours
    of TED and TEDx videos, the corresponding subtitles, and word alignment boundaries.
    Besides, it is the largest among existing public available annotated English audio-visual
    datasets.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: LRS3-TED [[89](#bib.bib89)]是一个大规模的句子级音视频数据集。与LRS2-TED相比，它在持续时间、词汇量和演讲者数量方面规模更大。它由来自于400多小时TED和TEDx视频的说话面部视频、相应的字幕和单词对齐边界组成。此外，在现有公开可用的英文音视频数据集中，它是最大的。
- en: VoxCeleb2 [[29](#bib.bib29)] is a high-level version of VoxCeleb1 extended on
    ethnic diversity. In VoxCeleb2, the VoxCeleb1 dataset is re-purposed to serve
    as a test set for speaker verification. Furthermore, it is currently the largest
    public available audio-visual dataset.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: VoxCeleb2 [[29](#bib.bib29)] 是VoxCeleb1的高级版本，扩展了民族多样性。在VoxCeleb2中，VoxCeleb1数据集被重新用于作为说话人验证的测试集。此外，它目前是最大的公开音视频数据集。
- en: LSVSR [[92](#bib.bib92)] is the largest existing visual speech recognition dataset,
    consisting of pairs of text and video clips of faces speaking (3,886 hours of
    video). It is collected from public YouTube videos. But unfortunately, this dataset
    is not publicly available due to the restricted license.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: LSVSR [[92](#bib.bib92)] 是现存最大的视频语音识别数据集，由文本和面孔说话的视频片段对组成（3886小时的视频）。这些视频来自公共YouTube视频。但遗憾的是，由于许可限制，该数据集不可公开获取。
- en: LRW-1000 [[31](#bib.bib31)] is the largest word-level Chinese Mandarin audio-visual
    dataset. It contains 1k word classes with 718,018 samples from more than 2k individual
    speakers. Each class corresponds to the syllables of a Mandarin word composed
    of one or several Chinese characters. All videos are collected from television
    programs on Chinese TV stations.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: LRW-1000 [[31](#bib.bib31)] 是最大的字级中文普通话音视频数据集。它包含1k个词汇类别，共有718,018个样本，来自2000多位个体发言者。每个类别对应一个普通话词汇的音节，该词汇由一个或多个汉字组成。所有视频均来自中国电视台的电视节目。
- en: Faceforensics++ [[94](#bib.bib94)] is an automated benchmark for facial manipulation
    detection. Different from existing audio-visual datasets, all videos have been
    manipulated based on DeepFakes [[102](#bib.bib102)], Face2Face [[103](#bib.bib103)],
    FaceSwap [[104](#bib.bib104)], NeuralTextures [[105](#bib.bib105)] as main methods
    for facial manipulations. It is commonly used to test forgery video detection
    methods.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Faceforensics++ [[94](#bib.bib94)] 是一个用于面部操控检测的自动化基准。不同于现有的音视频数据集，所有视频都基于DeepFakes [[102](#bib.bib102)]、Face2Face [[103](#bib.bib103)]、FaceSwap [[104](#bib.bib104)]、NeuralTextures [[105](#bib.bib105)]
    这些面部操控方法进行过操作。它通常用于测试伪造视频检测方法。
- en: HDTF [[68](#bib.bib68)] is a large-scale in-the-wild audio-visual dataset built
    for talking face generation. It consists of about 362 different high-resolution
    videos collected online. Due to the high quality of origin videos, the cropped
    face-centered videos also have higher visual quality than that of previous datasets
    like LRW and LRS2-BBC.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: HDTF [[68](#bib.bib68)] 是一个大规模的真实环境音视频数据集，用于生成对话面孔。它包含约362个不同的高分辨率视频，均从网上收集。由于原始视频的高质量，裁剪后的以面孔为中心的视频视觉质量也比之前的数据集如LRW和LRS2-BBC要高。
- en: 'In addition to the datasets listed in table [I](#S2.T1 "TABLE I ‣ 2.2 Main
    Challenges ‣ 2 Background ‣ Deep Learning for Visual Speech Analysis: A Survey"),
    there are several audio-visual datasets recorded in different languages. For example,
    Spanish language dataset VLRF [[106](#bib.bib106)], Australian English dataset [[107](#bib.bib107)],
    Russian language dataset HAVRUS [[108](#bib.bib108)] etc. also promoted the research
    of VSA on various languages.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '除了表格 [I](#S2.T1 "TABLE I ‣ 2.2 Main Challenges ‣ 2 Background ‣ Deep Learning
    for Visual Speech Analysis: A Survey")中列出的数据集外，还有几个录制了不同语言的音视频数据集。例如，西班牙语数据集VLRF [[106](#bib.bib106)]，澳大利亚英语数据集 [[107](#bib.bib107)]，俄语数据集HAVRUS [[108](#bib.bib108)]等也推动了对各种语言VSA的研究。'
- en: Considering that datasets play a crucial role in VSA, we would like to give
    a summary and discussion of datasets to help readers to know the development of
    VSA. Compared with early audio-visual datasets, recent ones have been improved
    in the number of subjects, dataset scale, recording conditions and script diversity,
    data quality, etc. Due to the privacy protection laws (e.g., General Data Protection
    Regulation (GDPR) in Europe Union), some of existing large-scale datasets [[83](#bib.bib83),
    [92](#bib.bib92)] are not public available. An intuitive solution is to automatically
    collect available data from online media (e.g., Youtube, BBC, or other online
    television programs). However, existing audio-visual data auto-collection algorithms
    may cause a large amount of low-quality data. Therefore, an optimized auto-collection
    algorithm is crucial for VSA datasets in the future.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到数据集在VSA中的关键作用，我们将总结和讨论数据集，以帮助读者了解VSA的发展。与早期的视听数据集相比，近期的数据集在受试者数量、数据集规模、录制条件和脚本多样性、数据质量等方面都有所改进。由于隐私保护法律（例如欧洲联盟的一般数据保护条例（GDPR）），一些现有的大规模数据集[[83](#bib.bib83)，[92](#bib.bib92)]无法公开获取。一个直观的解决方案是自动从在线媒体（如Youtube、BBC或其他在线电视节目）收集可用数据。然而，现有的视听数据自动收集算法可能会导致大量低质量数据。因此，优化的自动收集算法对未来的VSA数据集至关重要。
- en: '![Refer to caption](img/a6b9b2252148430d53356572cb830198.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a6b9b2252148430d53356572cb830198.png)'
- en: 'Figure 6: The overall diagram of VSR and various visual frontend networks,
    temporal backbone networks, and trainging paradigms.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：VSR的整体图以及各种视觉前端网络、时间骨干网络和训练范式。
- en: 3.2 Evaluation Metrics
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 评估指标
- en: 3.2.1 Evaluation Metrics on VSR
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 VSR上的评估指标
- en: The word-level VSR task is essentially a multi-class classification problem.
    Therefore, classification accuracy is the most common evaluation metric for classification
    models because of its simplicity and efficiency. Besides, $Top-k$ accuracy, namely
    the standard accuracy of the actual class being equal to any of the $k$ most probable
    classes predicted by the classification model, is also widely used in VSR.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 词级VSR任务本质上是一个多类分类问题。因此，由于其简单性和效率，分类准确率是分类模型最常用的评估指标。此外，$Top-k$准确率，即实际类别与分类模型预测的$k$个最可能类别中的任何一个相等的标准准确率，也在VSR中广泛使用。
- en: As for the sentence-level task, Character Error Rate (CER) and Word Error Rate
    (WER) [[109](#bib.bib109)], also known as average character-level and word-level
    edit distances, are the most commonly used evaluation metrics. CER is defined
    as ${\rm CER}=(S+D+I)/N$, where $S$, $D$ and $I$ are the numbers of substitutions,
    deletions, and insertions respectively to get from the reference to the hypothesis,
    and $N$ is the number of characters in the reference. This metric imposes smaller
    penalties where the predicted string is similar to the ground truth. For example,
    if the ground truth is “about” and the model prediction is “above”, then ${\rm
    CER}=0.4$. WER and CER are calculated in the same way. The difference lies in
    whether the formula is applied to character-level or word-level. Besides, BLEU [[110](#bib.bib110)],
    a modified form of n-gram precision to compare a candidate sentence to one or
    more reference sentences, is sometimes adopted.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于句子级任务，字符错误率（CER）和词错误率（WER）[[109](#bib.bib109)]，也称为平均字符级和词级编辑距离，是最常用的评估指标。CER的定义为${\rm
    CER}=(S+D+I)/N$，其中$S$、$D$和$I$分别是将参考文本转化为假设文本所需的替换、删除和插入的数量，$N$是参考文本中的字符数。这个指标在预测字符串与实际值相似时会赋予较小的惩罚。例如，如果实际值是“about”，模型预测是“above”，则${\rm
    CER}=0.4$。WER和CER的计算方法相同，区别在于公式应用于字符级还是词级。此外，BLEU[[110](#bib.bib110)]，一种修改后的n-gram精度，用于将候选句子与一个或多个参考句子进行比较，有时也会被采用。
- en: 3.2.2 Evaluation Metrics on VSG
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 VSG上的评估指标
- en: Appropriate evaluation for VSG continues to be an open problem, and many recent
    works have explored various evaluation metrics on VSG. We categorize those metrics
    based on three learning targets, i.e., identity preservation, visual quality,
    audio-visual semantic consistency.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对VSG的适当评估仍然是一个未解的问题，许多最近的研究探讨了VSG上的各种评估指标。我们根据三个学习目标对这些指标进行了分类，即身份保留、视觉质量、视听语义一致性。
- en: Identity Preservation. One of the most important goals of VSG is to preserve
    the target identity as much as possible during video generation, as humans are
    quite sensitive to subtle appearance changes in synthesized videos. Since identity
    is a semantic concept, direct evaluation is not feasible. To evaluate how well
    the generated video preserves the target identity, existing works usually use
    the embedding distance of the generated video frames and the ground truth image
    to measure the identity-preserving performance. For example, Vougioukas et al. [[40](#bib.bib40)]
    adopted the average content distance (ACD) [[111](#bib.bib111)] to measure the
    average Euclidean distance of target image representation, obtained using OpenFace [[112](#bib.bib112)],
    and the representations of generated frames. Besides, Zakharov et al. [[113](#bib.bib113)]
    used the cosine similarity between embedding vectors of the ArcFace network [[114](#bib.bib114)]
    for measuring identity mismatch.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 身份保留。VSG的一个重要目标是尽可能地保留目标身份，因为人类对合成视频中细微的外观变化非常敏感。由于身份是一个语义概念，直接评估是不切实际的。为了评估生成的视频在多大程度上保留了目标身份，现有工作通常使用生成的视频帧与真实图像之间的嵌入距离来衡量身份保留性能。例如，Vougioukas等人[[40](#bib.bib40)]采用了平均内容距离（ACD）[[111](#bib.bib111)]来测量目标图像表示的平均欧几里得距离，该表示是使用OpenFace[[112](#bib.bib112)]获得的，并与生成帧的表示进行比较。此外，Zakharov等人[[113](#bib.bib113)]使用ArcFace网络[[114](#bib.bib114)]的嵌入向量之间的余弦相似度来测量身份不匹配。
- en: Visual Quality. To evaluate the quality of the synthesized video frames, reconstruction
    error measurement (e.g., Mean Squared Error) is a natural evaluation way. However,
    reconstruction error only focuses on pixel-wise alignments and ignores global
    visual quality. Therefore, existing works usually adopt Peak Signal-to-Noise Ratio
    (PSNR) and Structure Similarity Index Measure (SSIM) to evaluate the global visual
    quality of generated frames. More recently, Prajwal et al. [[38](#bib.bib38)]
    introduced Fr${\rm\acute{e}}$chet Inception Distance (FID) to measure the distance
    between synthetic and real data distributions, as FID is more consistent with
    human perception evaluation. Besides, Cumulative Probability Blur Detection (CPBD) [[115](#bib.bib115)],
    a non-reference measure, is also widely used to evaluate the loss of sharpness
    during video generation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉质量。要评估合成视频帧的质量，重建误差测量（例如，均方误差）是一种自然的评估方式。然而，重建误差仅关注像素级对齐，而忽略了整体视觉质量。因此，现有工作通常采用峰值信噪比（PSNR）和结构相似性指数测量（SSIM）来评估生成帧的整体视觉质量。最近，Prajwal等人[[38](#bib.bib38)]引入了Fr${\rm\acute{e}}$chet
    Inception Distance（FID）来测量合成数据和真实数据分布之间的距离，因为FID与人类感知评估更加一致。此外，累积概率模糊检测（CPBD）[[115](#bib.bib115)]，一种无参考度量，也被广泛用于评估视频生成过程中的清晰度损失。
- en: Audio-visual Semantic Consistency. Semantic consistency of the generated video
    and the driving source mainly contains audio-visual synchronization and speech
    consistency. For, audio-visual synchronization, Landmark Distance (LMD) [[116](#bib.bib116)]
    computes the Euclidean distance of the lip region landmarks between the synthesized
    video frames and ground truth frames. The other synchronization evaluation metric
    is to use a pre-trained audio-to-video synchronisation network [[48](#bib.bib48)]
    to predict the offset of generated frames and the ground truth. For the speech
    consistency, Chen et al. [[42](#bib.bib42)] proposed a lip-synchronization evaluation
    metric, i.e., Lip-Reading Similarity Distance (LRSD), which measures the Euclidean
    distance of semantic-level speech embeddings obtained by lip reading networks.
    For better evaluation of speech consistency, lip reading results (accuracy, CER,
    or WER) comparisons of the generated frames and ground truth are also used as
    consistency evaluation metrics.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 视听语义一致性。生成视频和驱动源的语义一致性主要包含视听同步和语音一致性。对于视听同步，Landmark Distance（LMD）[[116](#bib.bib116)]计算合成视频帧与真实帧之间的唇部区域地标的欧几里得距离。另一个同步评估指标是使用预训练的音频到视频同步网络[[48](#bib.bib48)]来预测生成帧与真实帧之间的偏移量。对于语音一致性，Chen等人[[42](#bib.bib42)]提出了一种唇同步评估指标，即唇读相似度距离（LRSD），它测量由唇读网络获得的语义级别语音嵌入的欧几里得距离。为了更好地评估语音一致性，还使用生成帧与真实帧的唇读结果（准确率、CER或WER）比较作为一致性评估指标。
- en: In addition to the above objective metrics, subjective metrics like user study
    are also widely used in VSG.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述客观指标，主观指标如用户研究在VSG中也被广泛使用。
- en: 4 Visual Speech Recognition
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉语音识别
- en: 4.1 The Overall Framework
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 总体框架
- en: Visual Speech Recognition (VSR), also known as lip reading, aims to decode speech
    from speakers’ mouth movements. An essential preprocessing of VSR is mouth-centered
    region of interest (ROI) cropping. A talking face video contains a large amount
    of redundant information (such as pose, illumination, gender, skin color, etc.)
    unrelated to the VSR task. To reduce redundant information, it is necessary to
    crop mouth-centered videos from the raw input video. However, defining the size
    of mouth-centered ROI is still an open problem. Koumparoulis et al. [[117](#bib.bib117)]
    proved that the selection of ROI will significantly affect the final recognition
    performance, but it is still unable to determine the optimal ROI.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉语音识别（VSR），也称为唇读，旨在从发言者的嘴部动作中解码语音。VSR的一个重要预处理步骤是嘴部中心感兴趣区域（ROI）的裁剪。一个讲话面孔视频包含大量与VSR任务无关的冗余信息（如姿势、光照、性别、肤色等）。为了减少冗余信息，有必要从原始输入视频中裁剪出嘴部中心的视频。然而，定义嘴部中心ROI的大小仍然是一个未解问题。Koumparoulis等人[[117](#bib.bib117)]证明了ROI的选择会显著影响最终的识别性能，但仍无法确定最佳ROI。
- en: 'As shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.2 Datasets under uncontrolled environments
    ‣ 3.1 Datasets ‣ 3 Datasets and Evaluation Metrics ‣ Deep Learning for Visual
    Speech Analysis: A Survey"), a VSR system usually contains three sub-modules.
    The first sub-module is visual feature extraction, intending to extract compact
    and effective visual feature vectors from mouth-cropped videos. Then, the second
    sub-module is temporal context aggregation, aiming to aggregate temporal context
    information for better text script decoding and recognition. The above two sub-modules
    are also the cores of deep learning based VSR methods. This paper will summarize
    and discuss existing deep networks for visual feature extraction and temporal
    context aggregation in Section. [4.2](#S4.SS2 "4.2 Backbone Architectures ‣ 4
    Visual Speech Recognition ‣ Deep Learning for Visual Speech Analysis: A Survey").
    The last sub-module is text decoding, i.e., converting the feature representations
    to text.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[6](#S3.F6 "Figure 6 ‣ 3.1.2 Datasets under uncontrolled environments ‣ 3.1
    Datasets ‣ 3 Datasets and Evaluation Metrics ‣ Deep Learning for Visual Speech
    Analysis: A Survey")所示，一个VSR系统通常包含三个子模块。第一个子模块是视觉特征提取，旨在从嘴部裁剪的视频中提取紧凑而有效的视觉特征向量。然后，第二个子模块是时间上下文聚合，旨在聚合时间上下文信息，以便更好地解码和识别文本脚本。上述两个子模块也是基于深度学习的VSR方法的核心。本文将在[4.2](#S4.SS2
    "4.2 Backbone Architectures ‣ 4 Visual Speech Recognition ‣ Deep Learning for
    Visual Speech Analysis: A Survey")节中总结和讨论现有的深度网络用于视觉特征提取和时间上下文聚合。最后一个子模块是文本解码，即将特征表示转换为文本。'
- en: 'The rest of this section is organized as follows. Section. [4.2](#S4.SS2 "4.2
    Backbone Architectures ‣ 4 Visual Speech Recognition ‣ Deep Learning for Visual
    Speech Analysis: A Survey") presents our taxonomy of deep representation learning
    networks for VSR. And then, we review and discuss various visual speech representation
    learning paradigms for VSR (supervised learning and unsupervised learning) in
    Section. [4.3](#S4.SS3 "4.3 Learning Paradigms ‣ 4 Visual Speech Recognition ‣
    Deep Learning for Visual Speech Analysis: A Survey"). Section. [4.4](#S4.SS4 "4.4
    Summary and performance comparison ‣ 4 Visual Speech Recognition ‣ Deep Learning
    for Visual Speech Analysis: A Survey") provides a comprehensive summary for readers
    to know the progress and limitations of existing VSR methods.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '本节的其余部分组织如下。[4.2](#S4.SS2 "4.2 Backbone Architectures ‣ 4 Visual Speech Recognition
    ‣ Deep Learning for Visual Speech Analysis: A Survey")节展示了我们对VSR的深度表示学习网络的分类法。接着，我们在[4.3](#S4.SS3
    "4.3 Learning Paradigms ‣ 4 Visual Speech Recognition ‣ Deep Learning for Visual
    Speech Analysis: A Survey")节中回顾并讨论了各种VSR（监督学习和无监督学习）的视觉语音表示学习范式。[4.4](#S4.SS4
    "4.4 Summary and performance comparison ‣ 4 Visual Speech Recognition ‣ Deep Learning
    for Visual Speech Analysis: A Survey")节为读者提供了一个全面的总结，以了解现有VSR方法的进展和局限性。'
- en: 4.2 Backbone Architectures
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主干架构
- en: 'Before the era of deep learning, representation learning for VSR had already
    been explored for a long time. From the feature engineering perspective, traditional
    feature extraction methods can be categorized into three types: appearance-based,
    shape-based, and motion-based [[8](#bib.bib8)]. Although simple and explainable,
    traditional representation learning methods usually do not work well, especially
    in uncontrolled environments. This paper mainly focuses on summarizing and discussing
    representation learning methods driven by deep learning technologies. Considering
    the significant difference between deep representation learning and traditional
    feature extraction, we introduce a novel taxonomy strategy based on two independent
    parts: visual frontend network and temporal backend network.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习时代之前，VSR的表示学习已经被探索了很长时间。从特征工程的角度来看，传统的特征提取方法可以分为三类：基于外观的、基于形状的和基于运动的[[8](#bib.bib8)]。尽管简单且易于解释，但传统的表示学习方法通常效果不好，特别是在非受控环境下。本文主要总结和讨论了深度学习技术驱动的表示学习方法。考虑到深度表示学习与传统特征提取之间的显著差异，我们引入了一种基于两个独立部分的新分类策略：视觉前端网络和时间后端网络。
- en: 4.2.1 Visual frontend network
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 视觉前端网络
- en: 'As shown in Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.2 Datasets under uncontrolled environments
    ‣ 3.1 Datasets ‣ 3 Datasets and Evaluation Metrics ‣ Deep Learning for Visual
    Speech Analysis: A Survey"), there are mainly three types of input data: mouth-centered
    videos, dense optical flow, and landmark points. Among them, mouth-centered videos
    and dense optical flow are regular grid data, so CNNs are the most suitable and
    commonly used backbone architectures for them. On the other hand, as landmark
    points are irregular data, some existing works [[118](#bib.bib118), [53](#bib.bib53),
    [54](#bib.bib54)] adopted Graph Convolution Networks (GCNs) to extract visual
    features from landmark points. Next, we review these backbone architectures.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [6](#S3.F6 "Figure 6 ‣ 3.1.2 Datasets under uncontrolled environments ‣
    3.1 Datasets ‣ 3 Datasets and Evaluation Metrics ‣ Deep Learning for Visual Speech
    Analysis: A Survey")所示，主要有三种输入数据类型：以嘴部为中心的视频、密集光流和标志点。其中，嘴部为中心的视频和密集光流是规则网格数据，因此CNN是最适合且最常用的骨干架构。另一方面，由于标志点是非规则数据，一些现有的工作[[118](#bib.bib118)、[53](#bib.bib53)、[54](#bib.bib54)]采用了图卷积网络（GCNs）来从标志点中提取视觉特征。接下来，我们回顾这些骨干架构。'
- en: 'TABLE II: The pros and cons of various visual frontend network architectures
    and temporal backend network architectures.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：各种视觉前端网络架构和时间后端网络架构的优缺点。
- en: '| Architectures | Available input | Pros | Cons |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 可用输入 | 优势 | 劣势 |'
- en: '| 2D CNNs | mouth video | high memory/time efficiency | poor at capturing temporal
    correlation |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 2D CNNs | 嘴部视频 | 高内存/时间效率 | 捕捉时间关联性差 |'
- en: '| 3D CNNs | mouth video / optical flow | powerful at short-term spatio-temporal
    modeling | high memory/time cost |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 3D CNNs | 嘴部视频 / 光流 | 强大的短期时空建模能力 | 高内存/时间成本 |'
- en: '| 3D + 2D CNNs | mouth video / optical flow | high memory/time efficiency;
    strong discrimination | not good at capturing subtle lip dynamics |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 3D + 2D CNNs | 嘴部视频 / 光流 | 高内存/时间效率；强区分能力 | 不擅长捕捉细微的唇部动态 |'
- en: '| Visual transformers | mouth video | high robustness; strong discrimination
    | high memory/time cost |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Visual transformers | 嘴部视频 | 高鲁棒性；强区分能力 | 高内存/时间成本 |'
- en: '| GCNs | lip landmark points | high computation efficiency; semantic preserving
    | low accuracy ; low robustness |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| GCNs | 嘴唇标志点 | 高计算效率；语义保留 | 精度低；鲁棒性差 |'
- en: '| RNNs | visual features | relatively good generalization | short-term dependency;
    serial computing |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| RNNs | 视觉特征 | 相对较好的泛化能力 | 短期依赖；串行计算 |'
- en: '| Transformers | visual features | long-term dependency; parallel computation
    | overfitting on small datasets; hard to converage |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Transformers | 视觉特征 | 长期依赖；并行计算 | 小数据集上的过拟合；难以收敛 |'
- en: '| TCNs | visual features | adaptive to multi-scale patterns; high memory efficiency
    | short-term dependency |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| TCNs | 视觉特征 | 适应多尺度模式；高内存效率 | 短期依赖 |'
- en: CNN-based Architectures. CNNs have been becoming one of the most common architectures
    in the field of deep learning. Since AlexNet [[119](#bib.bib119)] was proposed
    in 2012, researchers have invented a variety of deeper, wider, and lighter CNN
    models [[120](#bib.bib120)]. Representative CNN architectures, such as VGG [[121](#bib.bib121)],
    ResNet [[122](#bib.bib122)], MobileNet [[123](#bib.bib123)], DenseNet [[124](#bib.bib124)],
    ShuffleNet [[125](#bib.bib125)] etc, have been widely used in learning visual
    representation for VSR.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 CNN 的架构。CNN 已成为深度学习领域中最常见的架构之一。自从 **AlexNet**[[119](#bib.bib119)] 在 2012
    年提出以来，研究人员发明了多种更深、更宽、更轻的 CNN 模型[[120](#bib.bib120)]。代表性的 CNN 架构，如 VGG[[121](#bib.bib121)],
    ResNet[[122](#bib.bib122)], MobileNet[[123](#bib.bib123)], DenseNet[[124](#bib.bib124)],
    ShuffleNet[[125](#bib.bib125)] 等，已被广泛应用于 VSR 的视觉表示学习。
- en: The first end-to-end deep visual representation learning for word-level VSR
    was proposed by Chung et al. [[27](#bib.bib27)]. Based on the VGG-M backbone network,
    they compared different image sequence input (Multiple Towers vs. Early Fusion)
    and temporal fusion (2D CNNs vs. 3D CNNs) architectures and discussed their pros
    and cons. The experimental results showed that the 2D CNNs are superior to their
    3D counterparts by a large margin. However, the above conclusion was not rigorous
    enough, as the ablation study is insufficient, and word-level VSR datasets have
    a very short-term dependency. In 2017, Assael et al. [[56](#bib.bib56)] proposed
    LipNet, the first end-to-end sentence-level VSR model. LipNet extracts visual
    features using a 3-layer Spatio-temporal Convolution Neural Network (STCNN, also
    known as 3D CNN). The experimental results confirm the intuition that extracting
    spatio-temporal features using STCNN is better than aggregating spatial-only features.
    Considering 3D CNNs are more capable of capturing the dynamics of the mouth region
    while 2D CNNs are more efficient in time and memory, Stafylakis et al. [[55](#bib.bib55)]
    proposed to combine 3D CNNs and 2D CNNs for visual feature extraction. In specific,
    the proposed visual backbone network consists of a shadow 3D CNN and 2D ResNet.
    The 3D CNN has just one layer to aggregate short-term temporal information on
    lip movements. Due to the considerable performance of the model, plenty of VSR
    models [[126](#bib.bib126), [61](#bib.bib61), [59](#bib.bib59), [52](#bib.bib52),
    [50](#bib.bib50)] adopted it as the backbone network for visual features extraction.
    Recently, Feng et al. [[127](#bib.bib127)] improved this architecture by integrating
    the Squeeze-and-Extract [[128](#bib.bib128)] module. Besides VGG and ResNet, researchers
    have also adopted other representative 2D CNN architectures, including DenseNet [[58](#bib.bib58)],
    ShuffleNet [[52](#bib.bib52)], MobileNet [[129](#bib.bib129)], etc.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**Chung** 等人首次提出了用于单词级 VSR 的端到端深度视觉表示学习[[27](#bib.bib27)]。他们基于 VGG-M 主干网络，比较了不同的图像序列输入（Multiple
    Towers vs. Early Fusion）和时间融合（2D CNNs vs. 3D CNNs）架构，并讨论了它们的优缺点。实验结果显示，2D CNNs
    相较于 3D CNNs 显著优越。然而，以上结论并不够严谨，因为消融研究不足，并且单词级 VSR 数据集具有非常短期的依赖性。2017 年，**Assael**
    等人[[56](#bib.bib56)] 提出了 LipNet，这是第一个端到端句子级 VSR 模型。LipNet 使用三层时空卷积神经网络（STCNN，也称为
    3D CNN）提取视觉特征。实验结果确认了使用 STCNN 提取时空特征优于仅聚合空间特征的直觉。考虑到 3D CNNs 更能捕捉口部区域的动态，而 2D
    CNNs 在时间和内存上更高效，**Stafylakis** 等人[[55](#bib.bib55)] 提出了将 3D CNNs 和 2D CNNs 结合用于视觉特征提取。具体来说，提出的视觉主干网络由一个影子
    3D CNN 和 2D ResNet 组成。3D CNN 仅有一层用于聚合口部运动的短期时间信息。由于模型的卓越性能，许多 VSR 模型[[126](#bib.bib126),
    [61](#bib.bib61), [59](#bib.bib59), [52](#bib.bib52), [50](#bib.bib50)] 采用了它作为视觉特征提取的主干网络。最近，**Feng**
    等人[[127](#bib.bib127)] 通过整合 Squeeze-and-Extract[[128](#bib.bib128)] 模块改进了这一架构。除了
    VGG 和 ResNet，研究人员还采用了其他代表性的 2D CNN 架构，包括 DenseNet[[58](#bib.bib58)], ShuffleNet[[52](#bib.bib52)],
    MobileNet[[129](#bib.bib129)] 等。'
- en: GCN-based Architectures. Considering CNNs are not suitable for irregular grid
    data, researchers proposed to utilize Graph Convolution Networks (GCNs) to extract
    visual features from the facial landmark points [[130](#bib.bib130)]. Liu et al. [[53](#bib.bib53)]
    proposed the first end-to-end GCN model (ST-GCN) that extracts shape-based visual
    features by learning the lip landmark points and their relationships. They firstly
    proposed lip graph connection relations and defined the graph adjacency matrices
    based on the manifold distance of nodes. Then, they combined the image features
    and shape features to extract more discriminative visual features. However, the
    lip graph connection relations do not naturally exist, and the intuition-guided
    predefined lip graph restricts the representation ability of shape-based features.
    Motivated by this, Sheng et al. [[54](#bib.bib54)] proposed an Adaptive Semantic-Spatial-Temporal
    Graph Convolution Network (ASST-GCN). Unlike [[53](#bib.bib53)], the ASST-GCN
    parameterizes graph connections and automatically learns adaptive graph connections.
    Besides, they introduced two graph structures, i.e., semantic graph and spatial-temporal
    graph, making graph parameters can be adaptively learned with other parameters
    in the network training. Existing works show that image-based features are more
    discriminative than landmark-based features. Sheng et al. [[54](#bib.bib54)] concluded
    the reason for this. The accuracy and coordinates resolution of landmark point
    detection significantly influence its feature discrimination. However, facial
    landmark detection is challenging, especially in uncontrolled environments. Since
    the complementarity between image and landmark features, the combination of CNNs
    and GCNs is often widely adopted [[118](#bib.bib118), [53](#bib.bib53), [54](#bib.bib54)].
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 基于GCN的架构。考虑到CNN不适用于不规则网格数据，研究人员提出利用图卷积网络（GCNs）从面部标志点中提取视觉特征[[130](#bib.bib130)]。刘
    等[[53](#bib.bib53)] 提出了第一个端到端的GCN模型（ST-GCN），通过学习唇部标志点及其关系来提取基于形状的视觉特征。他们首先提出了唇部图连接关系，并基于节点的流形距离定义了图邻接矩阵。然后，他们结合图像特征和形状特征，以提取更具判别性的视觉特征。然而，唇部图连接关系并不存在于自然界中，直观引导的预定义唇部图限制了基于形状特征的表示能力。受此启发，Sheng
    等[[54](#bib.bib54)] 提出了自适应语义-空间-时间图卷积网络（ASST-GCN）。与[[53](#bib.bib53)]不同，ASST-GCN参数化图连接并自动学习自适应图连接。此外，他们引入了两种图结构，即语义图和空间-时间图，使图参数可以与网络训练中的其他参数一起自适应地学习。现有研究表明，基于图像的特征比基于标志点的特征更具判别性。Sheng
    等[[54](#bib.bib54)] 总结了这一原因。标志点检测的准确性和坐标分辨率显著影响其特征判别能力。然而，面部标志点检测具有挑战性，尤其是在不受控环境中。由于图像和标志特征之间的互补性，CNN和GCN的结合通常被广泛采用[[118](#bib.bib118),
    [53](#bib.bib53), [54](#bib.bib54)]。
- en: Visual Transformer-based Architectures. Inspired by the significant success
    of transformer architectures in the field of NLP, researchers have recently applied
    transformers to computer vision (CV) tasks [[131](#bib.bib131)]. Recently, transformers
    have been showing they are potential alternatives to CNNs. Afouras et al. [[57](#bib.bib57)]
    designed an end-to-end visual transformer-based pooling mechanism that learns
    to track and aggregate the lip movement representations. The proposed visual backbone
    network can reduce the need for complicated preprocessing, improving the robustness
    of visual representation. The ablation study clearly shows that the visual transformer-based
    pooling mechanism significantly boosts the performance of VSR.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 基于视觉变换器的架构。受到变换器架构在自然语言处理（NLP）领域显著成功的启发，研究人员最近将变换器应用于计算机视觉（CV）任务[[131](#bib.bib131)]。近期，变换器表现出作为CNN的潜在替代品。Afouras
    等[[57](#bib.bib57)] 设计了一种端到端的基于视觉变换器的池化机制，该机制学习跟踪和聚合唇部运动表示。所提出的视觉主干网络可以减少对复杂预处理的需求，提高视觉表示的鲁棒性。消融研究清楚地表明，基于视觉变换器的池化机制显著提升了VSR的性能。
- en: Based on the above backbone architectures, some works further improved visual
    representation by utilizing two-stream networks. For example, Weng et al. [[132](#bib.bib132)]
    successfully migrated the two-stream (the raw grayscale video stream and the dense
    optical flow stream) I3D model to VSR, and achieve comparable performance on word-level
    VSR. However, dense optical flow and 3D convolution calculation is very time consuming,
    resulting in low feature extraction efficiency. Wang et al. [[58](#bib.bib58)]
    utilized 2D CNNs and 3D CNNs to extract both frame-wise spatial features and short-term
    spatio-temporal features, and then fused the features with an adaptive mask to
    obtain strong, multi-grained visual features.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述骨干架构，一些研究通过利用双流网络进一步提高视觉表示。例如，Weng等人[[132](#bib.bib132)]成功将双流（原始灰度视频流和稠密光流流）I3D模型迁移到VSR，并在词级VSR上取得了可比性能。然而，稠密光流和3D卷积计算非常耗时，导致特征提取效率低。Wang等人[[58](#bib.bib58)]利用2D
    CNN和3D CNN提取帧级空间特征和短期时空特征，然后用自适应掩码融合特征，以获得强大、多层次的视觉特征。
- en: 4.2.2 Temporal backend network
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 时序后端网络
- en: The temporal backend network built upon visual features aims to further aggregate
    context information. In traditional VSR, classical statistical models (e.g., Hidden
    Markov Model, HMM) are commonly used for temporal information aggregation.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 基于视觉特征构建的时序后端网络旨在进一步聚合上下文信息。在传统的视觉语音识别（VSR）中，常用经典统计模型（如隐马尔可夫模型，HMM）进行时序信息聚合。
- en: 'RNN-based Architectures. In the field of deep learning, Recurrent Neural Networks
    (RNNs) are representative network structures used to learn sequence data. The
    typical RNN structures (e.g., LSTM and GRU) are shown in Fig. [6](#S3.F6 "Figure
    6 ‣ 3.1.2 Datasets under uncontrolled environments ‣ 3.1 Datasets ‣ 3 Datasets
    and Evaluation Metrics ‣ Deep Learning for Visual Speech Analysis: A Survey"),
    and their basic structures are similar to that of HMM, in which the dependencies
    between the observed state sequences are described by the transformation of the
    hidden state sequence. Compared to HMM, RNNs have a more powerful representation
    ability due to the nonlinear transformation during hidden state transitions. Bidirectional
    RNNs (BiRNNs) are variations of basic RNNs, which attempt to aggregate context
    information from previous timesteps as well as future timesteps. Many works [[56](#bib.bib56),
    [28](#bib.bib28), [55](#bib.bib55), [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134),
    [135](#bib.bib135), [127](#bib.bib127), [50](#bib.bib50)] have adopted RNN-based
    (BiLSTM or BiGRU) network architectures as the temporal backend network in VSR.
    Beyond above fundamental RNN structures, various modifications [[58](#bib.bib58),
    [136](#bib.bib136)] have been made to improve feature learning for VSR. For example,
    Wang et al. [[58](#bib.bib58)] utilized BiConvLSTM [[137](#bib.bib137)] as temporal
    backend network. ConvLSTM is a convolutional counterpart of conventional fully
    connected LSTM, which models temporal dependency while preserving spatial information.
    Wang et al.integrated the attention mechanism into the model to further improve
    the BiConvLSTM architecture.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '基于RNN的架构。在深度学习领域，递归神经网络（RNNs）是用于学习序列数据的代表性网络结构。典型的RNN结构（如LSTM和GRU）如图[6](#S3.F6
    "Figure 6 ‣ 3.1.2 Datasets under uncontrolled environments ‣ 3.1 Datasets ‣ 3
    Datasets and Evaluation Metrics ‣ Deep Learning for Visual Speech Analysis: A
    Survey")所示，其基本结构类似于HMM，其中观察状态序列之间的依赖关系通过隐藏状态序列的转换进行描述。与HMM相比，RNN由于在隐藏状态转移过程中的非线性转换具有更强的表示能力。双向RNN（BiRNNs）是基本RNN的变体，旨在从先前和未来的时间步聚合上下文信息。许多研究[[56](#bib.bib56),
    [28](#bib.bib28), [55](#bib.bib55), [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134),
    [135](#bib.bib135), [127](#bib.bib127), [50](#bib.bib50)]采用基于RNN（BiLSTM或BiGRU）的网络架构作为VSR中的时序后端网络。除了上述基本RNN结构外，还进行了各种修改[[58](#bib.bib58),
    [136](#bib.bib136)]以改善VSR的特征学习。例如，Wang等人[[58](#bib.bib58)]利用BiConvLSTM[[137](#bib.bib137)]作为时序后端网络。ConvLSTM是传统全连接LSTM的卷积对偶，能够在保留空间信息的同时建模时序依赖。Wang等人将注意力机制集成到模型中，以进一步改进BiConvLSTM架构。'
- en: 'Transformer-based Architectures. Compared to RNN-based architectures, Transformers [[138](#bib.bib138)]
    have significant advantages in long-term dependency and parallel computation.
    However, transformers usually suffer from some drawbacks. First, transformers
    are more prone to overfitting than RNNs and TCNs in small-scale datasets. Second,
    transformers are limited in some specific tasks (e.g., word-level VSR tasks) with
    short-term context. Therefore, transformers are more suitable for sentence-level
    VSR tasks, rather than word-level VSR tasks.  [[6](#bib.bib6)] is the first work
    introducing transformers to VSR. Based on the basic transformer architecture,
    the authors proposed two types of backend models: TM-seq2seq and TM-CTC. The difference
    between the two models lies in the training target. The experiments clearly showed
    that the transformer-based backend network performs much better than the RNN-based
    backend network in the sentence-level VSR task. Since the basic transformer pays
    no extra attention to short-term dependency, Zhang et al. [[61](#bib.bib61)] proposed
    multiple Temporal Focal blocks (TF-blocks), helping features to look around their
    neighbors and capturing more short-term temporal dependencies. The results demonstrated
    that the short-term dependency is as crucial as the long-term dependency in sentence-level
    VSR.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的架构。与基于RNN的架构相比，Transformers[[138](#bib.bib138)]在长期依赖和并行计算方面具有显著优势。然而，transformers通常也存在一些缺陷。首先，transformers在小规模数据集上比RNNs和TCNs更容易过拟合。其次，transformers在一些特定任务（例如，单词级VSR任务）中的短期上下文有限。因此，transformers更适合于句子级VSR任务，而非单词级VSR任务。[[6](#bib.bib6)]是首个将transformers引入VSR的工作。基于基本的transformer架构，作者提出了两种后端模型：TM-seq2seq和TM-CTC。这两种模型的区别在于训练目标。实验清楚地显示，基于transformer的后端网络在句子级VSR任务中的表现远优于基于RNN的后端网络。由于基本的transformer对短期依赖没有额外关注，Zhang等人[[61](#bib.bib61)]提出了多个时间焦点块（TF-blocks），帮助特征关注其邻域并捕捉更多短期时间依赖。结果表明，短期依赖在句子级VSR中与长期依赖同样重要。
- en: TCN-based Architectures. In the context of deep sequence models, RNNs and Transformers
    have a high demand for memory and computation ability. Temporal Convolutional
    Networks (TCNs) are another type of deep sequence model, and various improvements
    have been applied to the basic TCN to make them more appropriate for VSR. For
    example, Afouras et al. [[60](#bib.bib60)] used depth-wise separable convolution
    (DS-TCN) for sentence-level VSR. However, the performance of DS-TCN does not work
    as well as transformers, as TCN-based models have poor ability on capturing long-term
    dependency. To enable temporal backend network capturing multi-scale temporal
    patterns, Martinez [[59](#bib.bib59)] proposed to utilize multi-scale TCN (MS-TCN)
    structure, which achieved SOTA results (87.9% Acc) on the word-level LRW dataset.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 基于TCN的架构。在深度序列模型的背景下，RNNs和Transformers对内存和计算能力的需求较高。时间卷积网络（TCNs）是另一种深度序列模型，已经对基本的TCN进行了各种改进，使其更适合于VSR。例如，Afouras等人[[60](#bib.bib60)]使用了深度可分离卷积（DS-TCN）用于句子级VSR。然而，DS-TCN的性能不如transformers，因为基于TCN的模型在捕捉长期依赖方面表现较差。为了使时间后端网络能够捕捉多尺度时间模式，Martinez[[59](#bib.bib59)]提出了利用多尺度TCN（MS-TCN）结构，该结构在单词级LRW数据集上取得了SOTA结果（87.9%
    Acc）。
- en: 'Table. [II](#S4.T2 "TABLE II ‣ 4.2.1 Visual frontend network ‣ 4.2 Backbone
    Architectures ‣ 4 Visual Speech Recognition ‣ Deep Learning for Visual Speech
    Analysis: A Survey") summarizes the general pros and cons of various visual frontend
    networks and temporal backend networks, and the available inputs of the corresponding
    visual frontend network. As we know, most of the existing VSR models are derived
    from general backbone models used in other fields (e.g., action recognition [[139](#bib.bib139),
    [140](#bib.bib140)], audio speech recognition [[137](#bib.bib137)], etc.), and
    few are designed explicitly for VSR. Therefore, more attention should be paid
    to the particular structure adaptive to the properties of VSR in the future.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '表格。[II](#S4.T2 "TABLE II ‣ 4.2.1 Visual frontend network ‣ 4.2 Backbone Architectures
    ‣ 4 Visual Speech Recognition ‣ Deep Learning for Visual Speech Analysis: A Survey")总结了各种视觉前端网络和时间后端网络的一般优缺点，以及相应视觉前端网络的可用输入。如我们所知，大多数现有的VSR模型都源于其他领域使用的一般骨干模型（例如，动作识别[[139](#bib.bib139),
    [140](#bib.bib140)]、音频语音识别[[137](#bib.bib137)]等），而很少有模型是专门为VSR设计的。因此，未来应更多关注适应VSR特性的特定结构。'
- en: 4.3 Learning Paradigms
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 学习范式
- en: 4.3.1 Supervised learning for VSR
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 监督学习用于VSR
- en: 'There are two mainstream VSR tasks: word-level and sentence-level. With a limited
    number of word categories, the former is to recognize isolated words from the
    input videos (i.e., talking face video classification), usually trained with multi-classification
    cross-entropy loss. The latter is to make unconstrained sentence-level sequence
    prediction. However, due to the unconstrained word categories and video frame
    length, it is much more complicated than the word-level VSR task.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 主流的VSR任务有两种：词级别和句子级别。前者是从输入视频中识别孤立的词汇（即，谈话面部视频分类），通常使用多分类交叉熵损失进行训练，因为词汇类别数量有限。后者是进行不受约束的句子级序列预测。然而，由于不受约束的词汇类别和视频帧长度，这比词级VSR任务复杂得多。
- en: Supervised learning of end-to-end sentence-level VSR tasks (sentence prediction)
    can be divided into two types. Given the input sequence, the first type uses a
    neural network as an emission model, which outputs the likelihood of each output
    symbol (e.g., phonemes, characters, words). These methods generally employ a second
    phase of decoding using HMM. A popular version of this variant is the Contortionist
    Temporal Classification (CTC) [[141](#bib.bib141)], where the model predicts frame-wise
    labels and then looks for the optimal alignment between the frame-wise predictions
    and the output sequence. The main weakness of CTC is that the output labels are
    not conditioned on each other (it assumes each unit is conditional independent),
    and hence a language model is needed as a post-processing step. Different from
    the basic CTC, Xu et al. [[62](#bib.bib62)] proposed LCANet that feeds the encoded
    spatio-temporal features into a cascaded attention CTC decoder. The introduction
    of an attention mechanism improves the defect of the conditional independence
    assumption CTC in hidden neural layers. Another assumption of this approach is
    that it assumes a monotonic ordering between input and output sequences, which
    is suitable for VSR but not for machine translation.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端句子级VSR任务（句子预测）的有监督学习可以分为两种类型。给定输入序列，第一种类型使用神经网络作为发射模型，该模型输出每个输出符号的可能性（例如，音素、字符、词汇）。这些方法通常使用HMM进行第二阶段的解码。这个变体的一个流行版本是扭曲时间分类（CTC）[[141](#bib.bib141)]，其中模型预测帧级标签，然后寻找帧级预测与输出序列之间的最佳对齐。CTC的主要缺点是输出标签之间没有条件关系（它假设每个单元是条件独立的），因此需要作为后处理步骤的语言模型。与基本的CTC不同，Xu等人[[62](#bib.bib62)]提出了LCANet，它将编码的时空特征输入到级联注意力CTC解码器中。引入注意力机制改善了CTC在隐藏神经层中的条件独立假设缺陷。该方法的另一个假设是它假设输入和输出序列之间的单调排序，这适用于VSR但不适用于机器翻译。
- en: The second type is sequence-to-sequence (seq2seq) models that first read the
    whole input sequence before predicting the output sentence. A number of works
    adopted this approach for speech recognition [[142](#bib.bib142)]. Chan et al. [[143](#bib.bib143)]
    proposed an elegant seq2seq method to transcribe audio signal to characters. Seq2seq
    models decode an output symbol at time $t$ (e.g., phonemes, characters, words)
    conditioned on previous outputs $1,...,t-1$. Thus, unlike CTC-based models, the
    model implicitly learns a language model over output symbols, and no further processing
    is required. However, it has been shown [[144](#bib.bib144)] that it is beneficial
    to incorporate an external language model in the decoding of seq2seq models as
    well. Chung et al. [[28](#bib.bib28)] proposed the WAS (Watch, Attend and Spell)
    model, which is a classical seq2seq VSR model. With the help of attention mechanism,
    WAS model is more capable of capturing long-term dependency.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种类型是序列到序列（seq2seq）模型，这些模型首先读取整个输入序列，然后预测输出句子。一些研究采用了这种方法进行语音识别[[142](#bib.bib142)]。Chan等人[[143](#bib.bib143)]提出了一种优雅的seq2seq方法，用于将音频信号转录为字符。Seq2seq模型在时间$t$解码一个输出符号（例如，音素、字符、词汇），该符号基于之前的输出$1,...,t-1$。因此，与基于CTC的模型不同，该模型隐式地学习了一个输出符号的语言模型，不需要进一步处理。然而，已有研究[[144](#bib.bib144)]表明，在解码seq2seq模型时，结合外部语言模型也是有益的。Chung等人[[28](#bib.bib28)]提出了WAS（Watch,
    Attend and Spell）模型，这是一种经典的seq2seq VSR模型。借助注意力机制，WAS模型更能捕捉长期依赖关系。
- en: Based on the transformer backbone architecture, Afouras et al. [[6](#bib.bib6)]
    have deeply analyzed the pros and cons of the CTC model and the seq2seq model
    for VSR. Generally, the seq2seq model performs well than the CTC model in the
    sentence-level VSR task. But, the seq2seq model needs more training time and inference
    time. Besides, the CTC model generalizes better and adapts faster as the sequence
    lengths are increased.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 基于transformer主干架构，Afouras 等 [[6](#bib.bib6)] 深入分析了CTC模型和seq2seq模型在VSR中的优缺点。通常，在句子级别的VSR任务中，seq2seq模型的表现优于CTC模型。但seq2seq模型需要更多的训练时间和推理时间。此外，CTC模型在序列长度增加时具有更好的泛化能力和更快的适应速度。
- en: Besides the above label-level supervised learning paradigms, feature-level supervised
    learning is also widely explored in VSR. Knowledge distillation [[145](#bib.bib145)]
    (KD) technology is the key to feature-level supervised learning. For example,
    Ma et al. [[52](#bib.bib52)] proposed a multi-stage self-KD training framework
    for the word-level VSR task. Like label smoothing, KD can provide an extra supervisory
    signal with inter-class similarity information. Some works [[51](#bib.bib51),
    [63](#bib.bib63), [146](#bib.bib146)] utilized cross-modal KD to train a robust
    VSR model by distilling from a well-trained ASR model. With the help of the ASR
    model, KD technology can significantly speed up the training of the VSR model.
    Meanwhile, combining CTC loss and KD loss can further improve the performance
    of VSR.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述标签级监督学习范式，特征级监督学习在VSR中也得到了广泛探索。知识蒸馏 [[145](#bib.bib145)] 技术是特征级监督学习的关键。例如，Ma
    等 [[52](#bib.bib52)] 提出了一个多阶段自蒸馏训练框架用于词级VSR任务。类似于标签平滑，KD可以提供具有类间相似性信息的额外监督信号。一些工作
    [[51](#bib.bib51), [63](#bib.bib63), [146](#bib.bib146)] 利用跨模态KD通过从经过良好训练的ASR模型中蒸馏来训练一个强大的VSR模型。在ASR模型的帮助下，KD技术可以显著加快VSR模型的训练速度。同时，将CTC损失和KD损失结合起来，可以进一步提高VSR的性能。
- en: '![Refer to caption](img/180067fefb8e36861258fa9ab204820a.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/180067fefb8e36861258fa9ab204820a.png)'
- en: 'Figure 7: The general motivation and available downstream tasks of self-supervised
    learning on visual speech.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：自监督学习在视觉语音中的一般动机和可用下游任务。
- en: 4.3.2 Unsupervised learning for VSR
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 无监督学习用于VSR
- en: Unsupervised learning for VSR aims to learn discriminative visual representations
    without access to manual annotation. Among multiple unsupervised learning frameworks,
    cross-modal self-supervised learning is dominated in VSR. Despite the remarkable
    progress witnessed in the past decade, the successes of supervised deep learning
    rely heavily on vast manually annotated training data, which has severe limitations
    in many real-world applications, including the VSR task. Firstly, supervised learning
    is restricted to relatively narrow domains primarily defined by the labeled training
    data and thus leads to limited generalization ability. Secondly, a large amount
    of accurately labeled data like a large-scale annotated dataset for VSR is costly
    to gather. Recently, self-supervised learning has received growing attention due
    to its high label efficiency and good generalization.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习在视频超级分辨率（VSR）中的目标是学习具有区分性的视觉表征，而无需人工标注。尽管过去十年取得了显著进展，监督深度学习的成功仍然严重依赖于大量人工标注的训练数据，这在许多现实世界应用中，包括VSR任务，存在严重的局限性。首先，监督学习受限于主要由标记训练数据定义的相对狭窄的领域，因此导致有限的泛化能力。其次，像大规模注释数据集这样的准确标注数据收集成本高昂。最近，因其高标签效率和良好的泛化能力，自监督学习受到了越来越多的关注。
- en: 'TABLE III: The comparison of representative VSR methods.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：代表性VSR方法的比较。
- en: '| Task type | Method | Frontend network | Backend network | Experimental settings
    | Performance (Dataset) | Highlights |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 任务类型 | 方法 | 前端网络 | 后端网络 | 实验设置 | 性能（数据集） | 亮点 |'
- en: '| Learning paradigm | Extra datasets | Extra LM | output symbol |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 学习范式 | 额外数据集 | 额外语言模型 | 输出符号 |'
- en: '| Word-level | Chung et al. [[27](#bib.bib27)] | VGG-M | / | / | / | / | word
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 词级别 | Chung 等 [[27](#bib.bib27)] | VGG-M | / | / | / | / | word |'
- en: '&#124; 61.1% (LRW) &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 61.1%（LRW） &#124;'
- en: '&#124; 25.7%(LRW-1000) &#124;'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 25.7%（LRW-1000） &#124;'
- en: '|'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Discussed various temporal fusion ways for &#124;'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 讨论了各种时间融合方式 &#124;'
- en: '&#124; word-level VSR networks &#124;'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 词级VSR网络 &#124;'
- en: '|'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Stafylakis et al. [[55](#bib.bib55)] | C3D-ResNet34 | BiLSTM | / | / | /
    | word |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Stafylakis 等 [[55](#bib.bib55)] | C3D-ResNet34 | BiLSTM | / | / | / | word
    |'
- en: '&#124; 83.5% (LRW) &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 83.5%（LRW） &#124;'
- en: '&#124; 38.2% (LRW-1000) &#124;'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 38.2%（LRW-1000） &#124;'
- en: '|'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Proposed the most widely used visual &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了最广泛使用的视觉 &#124;'
- en: '&#124; frontend network, i.e., C3D_ResNet &#124;'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 前端网络，即C3D_ResNet &#124;'
- en: '|'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Wang et al. [[58](#bib.bib58)] |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| Wang等人[[58](#bib.bib58)] |'
- en: '&#124; ResNet34 &#124;'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet34 &#124;'
- en: '&#124; 3D-DenseNet52 &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D-DenseNet52 &#124;'
- en: '| BiConvLSTM | / | / | / | word |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| BiConvLSTM | / | / | / | 单词 |'
- en: '&#124; 83.3% (LRW) &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 83.3% (LRW) &#124;'
- en: '&#124; 36.9% (LRW-1000) &#124;'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 36.9% (LRW-1000) &#124;'
- en: '|'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Introduced the BiConvLSTM architecture &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引入了BiConvLSTM架构 &#124;'
- en: '&#124; for VSR &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用于VSR &#124;'
- en: '|'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Liu et al. [[53](#bib.bib53)] |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| Liu等人[[53](#bib.bib53)] |'
- en: '&#124; C3D-ResNet34 &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; C3D-ResNet34 &#124;'
- en: '&#124; ST-GCN &#124;'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ST-GCN &#124;'
- en: '| BiGRU | / | / | / | word | 84.25% (LRW) |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| BiGRU | / | / | / | 单词 | 84.25% (LRW) |'
- en: '&#124; Firstly utilized the GCN-based network &#124;'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 首次利用基于GCN的网络 &#124;'
- en: '&#124; in VSR &#124;'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在VSR中 &#124;'
- en: '|'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Martinez et al. [[59](#bib.bib59)] | C3D-ResNet18 | MS-TCN | / | / | / |
    word |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| Martinez等人[[59](#bib.bib59)] | C3D-ResNet18 | MS-TCN | / | / | / | 单词 |'
- en: '&#124; 85.3% (LRW) &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 85.3% (LRW) &#124;'
- en: '&#124; 41.4% (LRW-1000) &#124;'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 41.4% (LRW-1000) &#124;'
- en: '|'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Improved performance by multi-scale TCN; &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过多尺度TCN提高了性能; &#124;'
- en: '&#124; Adaptive to varying input length &#124;'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 适应不同的输入长度 &#124;'
- en: '|'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sheng et al. [[54](#bib.bib54)] |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Sheng等人[[54](#bib.bib54)] |'
- en: '&#124; C3D-ResNet18 &#124;'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; C3D-ResNet18 &#124;'
- en: '&#124; ASST-GCN &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ASST-GCN &#124;'
- en: '| MSTCN | / | / | / | word | 85.7% (LRW) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| MSTCN | / | / | / | 单词 | 85.7% (LRW) |'
- en: '&#124; Introduced lip semantic encoding; &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引入了唇部语义编码; &#124;'
- en: '&#124; No need for predefined lip graph &#124;'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不需要预定义的唇部图 &#124;'
- en: '|'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Ma et al. [[52](#bib.bib52)] | C3D-ResNet18 | MS-TCN |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| Ma等人[[52](#bib.bib52)] | C3D-ResNet18 | MS-TCN |'
- en: '&#124; Multi-stage &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多阶段 &#124;'
- en: '&#124; KD &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; KD &#124;'
- en: '| / | / | word |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| / | / | 单词 |'
- en: '&#124; 87.7% (LRW) &#124;'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 87.7% (LRW) &#124;'
- en: '&#124; 43.2% (LRW-1000) &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 43.2% (LRW-1000) &#124;'
- en: '|'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Improved generalization with the help of KD &#124;'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 借助KD提高了泛化能力 &#124;'
- en: '&#124; and achieved SOTA results on LRW &#124;'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并在LRW上取得了SOTA结果 &#124;'
- en: '|'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Feng et al. [[127](#bib.bib127)] | SE-C3D-ResNet18 | BiGRU | / | / | / |
    word |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Feng等人[[127](#bib.bib127)] | SE-C3D-ResNet18 | BiGRU | / | / | / | 单词 |'
- en: '&#124; 85.0% (LRW) &#124;'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 85.0% (LRW) &#124;'
- en: '&#124; 48.0% (LRW-1000) &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 48.0% (LRW-1000) &#124;'
- en: '|'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Introduced Squeeze-and-Extract module; &#124;'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引入了Squeeze-and-Extract模块; &#124;'
- en: '&#124; Achieved SOTA results on LRW-1000 &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在LRW-1000上取得了SOTA结果 &#124;'
- en: '|'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Yang et al. [[34](#bib.bib34)] | C3D-ResNet18 | ResNet18 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| Yang等人[[34](#bib.bib34)] | C3D-ResNet18 | ResNet18 |'
- en: '&#124; Cross-modal &#124;'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨模态 &#124;'
- en: '&#124; mutual learning &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 互学习 &#124;'
- en: '| / | / | word |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| / | / | 单词 |'
- en: '&#124; $\mathbf{88.5\%^{\dagger}}$ (LRW) &#124;'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathbf{88.5\%^{\dagger}}$ (LRW) &#124;'
- en: '&#124; $\mathbf{50.5\%^{\dagger}}$ (LRW-1000) &#124;'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathbf{50.5\%^{\dagger}}$ (LRW-1000) &#124;'
- en: '|'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Proposed a unified framework for audio- &#124;'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了音频的统一框架 &#124;'
- en: '&#124; visual speech recognition and synthesis &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视觉语音识别和合成 &#124;'
- en: '|'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sentence-level | Assael et al. [[56](#bib.bib56)] | ST-CNN | BiGRU | CTC
    loss | / | $\surd$ | character |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 句子级别 | Assael等人[[56](#bib.bib56)] | ST-CNN | BiGRU | CTC损失 | / | $\surd$
    | 字符 |'
- en: '&#124; 1.9% CER (GRID) &#124;'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1.9% CER (GRID) &#124;'
- en: '&#124; 4.8% WER (GRID) &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4.8% WER (GRID) &#124;'
- en: '|'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The first end-to-end sentence-level VSR model &#124;'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 第一个端到端的句子级别VSR模型 &#124;'
- en: '|'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Xu et al. [[62](#bib.bib62)] | C3D + HighwayNet | BiGRU | CTC loss | / |
    / | character |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| Xu等人[[62](#bib.bib62)] | C3D + HighwayNet | BiGRU | CTC损失 | / | / | 字符 |'
- en: '&#124; 1.3% CER (GRID) &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1.3% CER (GRID) &#124;'
- en: '&#124; 2.9% WER (GRID) &#124;'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2.9% WER (GRID) &#124;'
- en: '|'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Compensated the defect of the CTC approach &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 弥补了CTC方法的缺陷 &#124;'
- en: '|'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Afouras et al. [[6](#bib.bib6)] | C3D-ResNet18 | Transformer | CTC loss |
    LRW, MVLRS, LRS2 | $\surd$ | character |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| Afouras等人[[6](#bib.bib6)] | C3D-ResNet18 | Transformer | CTC损失 | LRW, MVLRS,
    LRS2 | $\surd$ | 字符 |'
- en: '&#124; 54.7% CER (LRS2) &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 54.7% CER (LRS2) &#124;'
- en: '&#124; 66.3% WER (LRS3) &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 66.3% WER (LRS3) &#124;'
- en: '| Deeply analyzed the pros and cons of the CTC model and the seq2seq model
    |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 深入分析了CTC模型和seq2seq模型的优缺点 |'
- en: '| seq2seq loss |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| seq2seq损失 |'
- en: '&#124; 48.3% CER (LRS2) &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 48.3% CER (LRS2) &#124;'
- en: '&#124; 58.9% WER (LRS3) &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 58.9% WER (LRS3) &#124;'
- en: '|'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Shillingford et al. [[92](#bib.bib92)] | ST-CNN | BiLSTM | CTC loss | LSVSR
    | $\surd$ | phoneme |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| Shillingford等人[[92](#bib.bib92)] | ST-CNN | BiLSTM | CTC损失 | LSVSR | $\surd$
    | 音素 |'
- en: '&#124; 28.3% CER (LSVSR) &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 28.3% CER (LSVSR) &#124;'
- en: '&#124; 40.9% WER (LSVSR) &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 40.9% WER (LSVSR) &#124;'
- en: '&#124; 55.1 WER (LRS3) &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 55.1 WER (LRS3) &#124;'
- en: '|'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Adopted phoneme as the output symbol &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 采用了音素作为输出符号 &#124;'
- en: '&#124; and proposed the largest dataset LSVSR &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并提出了最大的LSVSR数据集 &#124;'
- en: '|'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Zhang et al. [[61](#bib.bib61)] | C3D-ResNet18 | TF-blocks | seq2seq loss
    | LRW, LRS2 | / | character |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等 [[61](#bib.bib61)] | C3D-ResNet18 | TF-blocks | seq2seq 损失 | LRW,
    LRS2 | / | 字符 |'
- en: '&#124; 1.3% WER (GRID) &#124;'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1.3% WER（GRID） &#124;'
- en: '&#124; 51.7% WER (LRS2) &#124;'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 51.7% WER（LRS2） &#124;'
- en: '&#124; 60.1% WER (LRS3) &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 60.1% WER（LRS3） &#124;'
- en: '|'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Integrated causal convolution into transformer &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 将因果卷积集成到变换器中 &#124;'
- en: '&#124; for VSR &#124;'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用于 VSR &#124;'
- en: '|'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Makino et al. [[147](#bib.bib147)] | ST-CNN | RNN-T (BiLSTM) | seq2seq loss
    | YT-31khrs | $\surd$ | character | 33.6% WER (LRS3) |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| Makino 等 [[147](#bib.bib147)] | ST-CNN | RNN-T（BiLSTM） | seq2seq 损失 | YT-31khrs
    | $\surd$ | 字符 | 33.6% WER（LRS3） |'
- en: '&#124; Proposed an RNN-T based VSR system and &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了基于 RNN-T 的 VSR 系统，并 &#124;'
- en: '&#124; collectd a large dataset from YouTube &#124;'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从 YouTube 收集了一个大型数据集 &#124;'
- en: '|'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Ma et al. [[148](#bib.bib148)] | C3D-ResNet18 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| Ma 等 [[148](#bib.bib148)] | C3D-ResNet18 |'
- en: '&#124; Conformer + &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Conformer + &#124;'
- en: '&#124; Transformer &#124;'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 变换器 &#124;'
- en: '|'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CTC loss + &#124;'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CTC 损失 + &#124;'
- en: '&#124; seq2seq loss &#124;'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; seq2seq 损失 &#124;'
- en: '|'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LRW, LRS2, &#124;'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRW, LRS2, &#124;'
- en: '&#124; LRS3 &#124;'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRS3 &#124;'
- en: '| $\surd$ | character |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| $\surd$ | 字符 |'
- en: '&#124; 37.9% WER (LRS2) &#124;'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 37.9% WER（LRS2） &#124;'
- en: '&#124; 43.3% WER (LRS3) &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 43.3% WER（LRS3） &#124;'
- en: '|'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Proposed a hybrid CTC/Attention model &#124;'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了混合 CTC/Attention 模型 &#124;'
- en: '&#124; and achieved SOTA results on LRS3 &#124;'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并在 LRS3 上取得了 SOTA 结果 &#124;'
- en: '|'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Afouras et al. [[57](#bib.bib57)] | 3DCNN+VTP | Transformer | seq2seq loss
    |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| Afouras 等 [[57](#bib.bib57)] | 3DCNN+VTP | 变换器 | seq2seq 损失 |'
- en: '&#124; LRS2, LRS3, &#124;'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRS2, LRS3, &#124;'
- en: '&#124; MVLRS, TEDx &#124;'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MVLRS, TEDx &#124;'
- en: '| $\surd$ | sub-word |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| $\surd$ | 子词 |'
- en: '&#124; 22.6% WER (LRS2) &#124;'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 22.6% WER（LRS2） &#124;'
- en: '&#124; 30.7% WER (LRS3) &#124;'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 30.7% WER（LRS3） &#124;'
- en: '|'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Introduced sub-word as the output symbol and &#124;'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引入子词作为输出符号，并 &#124;'
- en: '&#124; replaced the 2DCNN with the visual transformer &#124;'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 将 2DCNN 替换为视觉变换器 &#124;'
- en: '|'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '1'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: '$\dagger$: audio data is used in the training.'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\dagger$: 训练中使用了音频数据。'
- en: 'Recent advances in cross-modal self-supervised learning have shown that the
    corresponding audio can serve as a supervisory signal to learn effective visual
    representations for VSR. As shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3.1 Supervised
    learning for VSR ‣ 4.3 Learning Paradigms ‣ 4 Visual Speech Recognition ‣ Deep
    Learning for Visual Speech Analysis: A Survey"), audio-visual self-supervised
    learning aims to extract efficient representations from the co-occurring A-V data
    pairs without any extra annotation. Based on the natural synchronization property
    of audio and video, existing methods mainly adopt contrastive learning to achieve
    this goal. Chung et al. [[149](#bib.bib149)] are the first to train an A-V synchronization
    model in an end-to-end manner with margin-based [[150](#bib.bib150)] pairwise
    contrastive loss. Besides VSR, they have proved that the trained network work
    can effectively be finetuned to other tasks like speaker detection. With the same
    training strategy, Korbar et al. [[151](#bib.bib151)] broadened the scope of the
    study to encompass arbitrary human activities rather than lip movements. Except
    for margin-based loss, L1 loss and binary classification loss [[47](#bib.bib47),
    [152](#bib.bib152), [153](#bib.bib153), [154](#bib.bib154)] are also widely used
    for A-V representations learning. Those works have proved the learned A-V representations
    can be further transferred to more downstream tasks, such as visualizing the locations
    of sound sources, action recognition, audio-visual source separation, etcRecently,
    Chung et al. [[101](#bib.bib101)] reformulated the contrastive task as a multi-way
    matching task and demonstrated that using multiple negative samples can improve
    the performance. Considering existing methods only exploit the natural synchronization
    of the video and the corresponding audio, Sheng et al. [[50](#bib.bib50)] proposed
    a novel self-supervised learning framework called Adversarial Dual-Contrast Self-Supervised
    Learning (ADC-SSL), to go beyond previous methods by explicitly forcing the visual
    representations disentangled from speech-unrelated information. To achieve this
    goal, they combine contrastive learning and adversarial training by three pretext
    tasks: A-V synchronization, identity discrimination, and modality classification.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '近期在跨模态自监督学习中的进展表明，相应的音频可以作为监督信号来学习有效的视觉表示。正如图 [7](#S4.F7 "Figure 7 ‣ 4.3.1
    Supervised learning for VSR ‣ 4.3 Learning Paradigms ‣ 4 Visual Speech Recognition
    ‣ Deep Learning for Visual Speech Analysis: A Survey") 所示，视听自监督学习旨在从共现的音视频数据对中提取有效的表示，而无需额外的标注。基于音频和视频的自然同步特性，现有方法主要采用对比学习来实现这一目标。Chung
    等人 [[149](#bib.bib149)] 是首个以端到端方式训练音视频同步模型的研究者，他们使用了基于边际的 [[150](#bib.bib150)]
    成对对比损失。除了VSR，他们还证明了训练出的网络可以有效地微调到其他任务，如说话人检测。在相同的训练策略下，Korbar 等人 [[151](#bib.bib151)]
    扩展了研究范围，涵盖了任意的人类活动，而不仅仅是嘴唇动作。除了边际损失，L1损失和二分类损失 [[47](#bib.bib47), [152](#bib.bib152),
    [153](#bib.bib153), [154](#bib.bib154)] 也被广泛用于音视频表示学习。这些研究证明了学习到的音视频表示可以进一步转移到更多下游任务中，如声音源位置可视化、动作识别、音视频源分离等。最近，Chung
    等人 [[101](#bib.bib101)] 将对比任务重新表述为多重匹配任务，并证明使用多个负样本可以提高性能。考虑到现有方法仅利用视频和对应音频的自然同步，Sheng
    等人 [[50](#bib.bib50)] 提出了一个新颖的自监督学习框架，称为对抗双对比自监督学习（ADC-SSL），通过明确地将视觉表示与与语音无关的信息分离，超越了之前的方法。为实现这一目标，他们通过三个前置任务结合了对比学习和对抗训练：音视频同步、身份辨别和模态分类。'
- en: 4.4 Summary and performance comparison
  id: totrans-442
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 总结与性能比较
- en: We have witnessed significant progress in various aspects of visual speech recognition.
    In this subsection, we will compare existing VSR methods on representative datasets
    and summarize the main issues of VSR.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在视觉语音识别的各个方面都见证了显著的进展。在本小节中，我们将对现有的视觉语音识别（VSR）方法在代表性数据集上的表现进行比较，并总结VSR的主要问题。
- en: 4.4.1 Performance Comparison
  id: totrans-444
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 性能比较
- en: 'In this section, we compare the existing deep learning-based VSR methods. Due
    to a large number of methods proposed for VSR, it is not possible to list and
    compare all of them. Thus, we select representative works and several milestone
    methods. Table. [III](#S4.T3 "TABLE III ‣ 4.3.2 Unsupervised learning for VSR
    ‣ 4.3 Learning Paradigms ‣ 4 Visual Speech Recognition ‣ Deep Learning for Visual
    Speech Analysis: A Survey") summarizes the performance and some experimental settings
    of some representative VSR methods on large-scale commonly used benchmark datasets,
    including LRW [[27](#bib.bib27)], LRW-1000 [[31](#bib.bib31)], GRID [[77](#bib.bib77)],
    LRS2 [[6](#bib.bib6)] and LRS3 [[89](#bib.bib89)].'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们比较了现有的基于深度学习的VSR方法。由于提出的VSR方法数量庞大，无法列出并比较所有方法。因此，我们选择了具有代表性的工作和几个里程碑式的方法。表[III](#S4.T3
    "TABLE III ‣ 4.3.2 Unsupervised learning for VSR ‣ 4.3 Learning Paradigms ‣ 4
    Visual Speech Recognition ‣ Deep Learning for Visual Speech Analysis: A Survey")
    总结了一些代表性VSR方法在大规模常用基准数据集上的性能和一些实验设置，包括LRW[[27](#bib.bib27)]、LRW-1000[[31](#bib.bib31)]、GRID[[77](#bib.bib77)]、LRS2[[6](#bib.bib6)]和LRS3[[89](#bib.bib89)]。'
- en: As for the word-level VSR task, various visual frontend networks have been designed
    to boost the performance, such as VGG-M, C3D-ResNet, ST-GCN, ASST-GCN,etc. Among
    them, the C3D-ResNet architecture is the most widely used. [[55](#bib.bib55)]
    provided the baseline (C3D-ResNet34 + BiLSTM, 83.5%) on the LRW dataset. Many
    subsequent works inherited this structure and further improved the performance
    by introducing some tricks, such as label smoothing, weight decay, dropout, Squeeze-and-Extract
    module, two-stream, multi-stage KD, etc. As for temporal backend networks, RNN-based
    models and TCN-based models have similar performance. Based on C3D-ResNet18 +
    MSTCN, Ma et al. [[52](#bib.bib52)] improved the SOTA to 87.7% on LRW. Recently,
    more and more works [[149](#bib.bib149), [47](#bib.bib47), [101](#bib.bib101),
    [49](#bib.bib49), [50](#bib.bib50), [34](#bib.bib34)] tried to improve visual
    representations by utilizing extra audio information in the training stage rather
    than the design of network architectures, as audio signals can provide more fine-grained
    supervision than text annotations. The SOTA results (88.5% on LRW and 50.5% on
    LRW-1000) was realized based on cross-modal audio-visual mutual learning [[34](#bib.bib34)].
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 对于词级VSR任务，已经设计了各种视觉前端网络以提升性能，如VGG-M、C3D-ResNet、ST-GCN、ASST-GCN等。其中，C3D-ResNet架构是最广泛使用的。[[55](#bib.bib55)]
    提供了LRW数据集上的基准（C3D-ResNet34 + BiLSTM，83.5%）。许多后续工作继承了这个结构，并通过引入一些技巧进一步提高了性能，如标签平滑、权重衰减、丢弃、Squeeze-and-Extract模块、双流、多阶段KD等。至于时间后端网络，基于RNN的模型和基于TCN的模型性能相似。基于C3D-ResNet18
    + MSTCN，Ma等人[[52](#bib.bib52)] 将LRW上的SOTA提升到了87.7%。最近，越来越多的工作[[149](#bib.bib149)、[47](#bib.bib47)、[101](#bib.bib101)、[49](#bib.bib49)、[50](#bib.bib50)、[34](#bib.bib34)]
    尝试通过在训练阶段利用额外的音频信息来改进视觉表征，而不是通过网络架构的设计，因为音频信号可以提供比文本注释更精细的监督。基于跨模态音视互学的SOTA结果（LRW上的88.5%和LRW-1000上的50.5%）[[34](#bib.bib34)]
    已经实现。
- en: 'As for the sentence-level VSR task, deep learning-based VSR methods have vastly
    outperformed human lip-readers [[56](#bib.bib56)]. As shown in Table. [III](#S4.T3
    "TABLE III ‣ 4.3.2 Unsupervised learning for VSR ‣ 4.3 Learning Paradigms ‣ 4
    Visual Speech Recognition ‣ Deep Learning for Visual Speech Analysis: A Survey"),
    deep VSR models almost reach performance saturation (SOTA result: 1.3% WER) on
    the simple (constrained recording environment and limited corpus) GRID dataset.
    Therefore, researchers pay more attention to VSR in unconstrained environments.
    Motivated by the practical need, we focus more on the large-scale in-the-wild
    datasets (e.g., LRS2 and LRS3). The fair performance comparison of sentence-level
    VSR methods is quite hard, as there are too many extra influencing factors. For
    example, some methods trained the model with extra datasets (even some of them
    are not public available). Besides, the outputs of the model are generally optimized
    by extra language models, while language models are trained with existing large-scale
    text corpus. The introduction of language models can significantly improve the
    performance, and it is not fair to compare these methods optimized by different
    language models. Therefore, to make it clearer for readers, we list some representative
    sentence-level VSR models as well as their experimental settings in Table. [III](#S4.T3
    "TABLE III ‣ 4.3.2 Unsupervised learning for VSR ‣ 4.3 Learning Paradigms ‣ 4
    Visual Speech Recognition ‣ Deep Learning for Visual Speech Analysis: A Survey").'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '对于句子级VSR任务，基于深度学习的VSR方法已经远远超越了人类读唇者[[56](#bib.bib56)]。如表[III](#S4.T3 "TABLE
    III ‣ 4.3.2 Unsupervised learning for VSR ‣ 4.3 Learning Paradigms ‣ 4 Visual
    Speech Recognition ‣ Deep Learning for Visual Speech Analysis: A Survey")所示，深度VSR模型在简单的（受限录制环境和有限语料库）GRID数据集上几乎达到了性能饱和（SOTA结果：1.3%
    WER）。因此，研究人员更多地关注于不受约束环境下的VSR。由于实际需求的驱动，我们更加关注大规模野外数据集（如LRS2和LRS3）。句子级VSR方法的公平性能比较相当困难，因为存在太多额外的影响因素。例如，一些方法使用了额外的数据集来训练模型（其中一些数据集甚至不可公开获取）。此外，模型的输出通常通过额外的语言模型进行优化，而语言模型是基于现有的大规模文本语料库进行训练的。语言模型的引入可以显著提高性能，因此比较这些由不同语言模型优化的方法是不公平的。因此，为了让读者更清楚，我们在表[III](#S4.T3
    "TABLE III ‣ 4.3.2 Unsupervised learning for VSR ‣ 4.3 Learning Paradigms ‣ 4
    Visual Speech Recognition ‣ Deep Learning for Visual Speech Analysis: A Survey")中列出了一些具有代表性的句子级VSR模型及其实验设置。'
- en: 4.4.2 Main issues and facts
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 主要问题和事实
- en: 'Over the last decade, deep learning-based VSR techniques have been significantly
    developed. However, some issues remain to be solved. We conclude them as follows:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，基于深度学习的VSR技术得到了显著发展。然而，仍有一些问题亟待解决。我们总结如下：
- en: $\bullet$
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: The cropping preprocessing of raw talking face videos have a significant impact
    on the recognition results, and how to define the optimal lip ROI for the VSR
    task is worthy of further exploration.
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 原始谈话面部视频的裁剪预处理对识别结果有显著影响，如何定义VSR任务的最佳嘴唇ROI值得进一步探索。
- en: $\bullet$
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: In practical applications, real-time is another substantial demand for VSR.
    However, most existing VSR methods only focus on recognition accuracy but ignore
    real-time. Therefore, the trade-off between accuracy and real-time should be considered
    in the future.
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在实际应用中，实时性是VSR的另一个重要需求。然而，大多数现有的VSR方法只关注识别准确性，而忽视了实时性。因此，未来应考虑准确性和实时性之间的权衡。
- en: $\bullet$
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'There is no formal robustness analysis of existing VSR methods. As we have
    mentioned in Section. [2.2.1](#S2.SS2.SSS1 "2.2.1 Recognition-related Challenges
    ‣ 2.2 Main Challenges ‣ 2 Background ‣ Deep Learning for Visual Speech Analysis:
    A Survey"), VSR faces many challenges, such as speaker differences and unconstrained
    environments. Existing deep learning-based VSR networks are rarely targeted to
    solve these problems. Therefore, robustness analysis of VSR methods needs more
    attention in the future.'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '现有VSR方法缺乏正式的鲁棒性分析。如我们在第[2.2.1](#S2.SS2.SSS1 "2.2.1 Recognition-related Challenges
    ‣ 2.2 Main Challenges ‣ 2 Background ‣ Deep Learning for Visual Speech Analysis:
    A Survey")节中提到，VSR面临许多挑战，如说话者差异和不受约束的环境。现有的基于深度学习的VSR网络很少针对这些问题。因此，VSR方法的鲁棒性分析在未来需要更多关注。'
- en: $\bullet$
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Another serious problem of VSR research is the lack of fair benchmarks for algorithm
    comparison, especially for the sentence-level VSR task. The performance of VSR
    is affected by many factors, such as extra language models, multiple training
    datasets, audio signals, and implementation details. Due to the lack of a unified
    test platform, a fair comparison of VSR algorithms is not easy to achieve.
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VSR研究的另一个严重问题是缺乏公平的基准用于算法比较，尤其是句子级别的VSR任务。VSR的性能受多种因素影响，如额外的语言模型、多训练数据集、音频信号和实现细节。由于缺乏统一的测试平台，公正比较VSR算法并不容易实现。
- en: 5 Visual Speech Generation
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 可视语音生成
- en: Visual Speech Generation (VSG), also known as lip sequence generation, aims
    to synthesize a lip sequence corresponding to the driving source (a clip of audio
    or a piece of text).
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 可视语音生成（VSG），也称为唇部序列生成，旨在合成与驱动源（音频片段或一段文本）相对应的唇部序列。
- en: Traditional VSG methods suffer from severe practical challenges [[45](#bib.bib45)],
    such as complex generation pipelines, constrained applicable environments, over-reliance
    on fine-grained viseme (phoneme) annotations, etc. To realize mapping driving
    sources to lip dynamics, representative traditional VSG methods mainly adopted
    cross-modal retrieval approaches [[155](#bib.bib155), [156](#bib.bib156), [16](#bib.bib16),
    [103](#bib.bib103)] and HMM-based approaches [[157](#bib.bib157), [158](#bib.bib158)].
    For example, Thies et al. [[103](#bib.bib103)] introduced a typical image-based
    mouth synthesis approach that generates a realistic mouth interior by retrieving
    and warping best-matching mouth shapes from offline samples. However, retrieval-based
    methods are static text-phoneme-viseme mappings and do not really consider the
    contextual information of the speech. Meanwhile, retrieval-based methods are pretty
    sensitive to head pose changes. HMM-based methods also suffer from some drawbacks,
    such as the limitation of the prior assumptions (e.g., Gaussian Mixture Model
    (GMM) and its diagonal covariance). As deep learning technologies have extensively
    promoted the developments of VSG, we focus on reviewing deep learning based VSG
    methods in this section.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的VSG方法面临严重的实际挑战[[45](#bib.bib45)]，如复杂的生成流程、适用环境受限、过度依赖细粒度的视觉语音（音素）注释等。为了实现驱动源与唇部动态的映射，具有代表性的传统VSG方法主要采用跨模态检索方法[[155](#bib.bib155),
    [156](#bib.bib156), [16](#bib.bib16), [103](#bib.bib103)]和基于HMM的方法[[157](#bib.bib157),
    [158](#bib.bib158)]。例如，Thies等人[[103](#bib.bib103)]介绍了一种典型的基于图像的嘴部合成方法，该方法通过从离线样本中检索和变形最佳匹配的嘴部形状来生成逼真的嘴部内部。然而，基于检索的方法是静态的文本-音素-视觉语音映射，并没有真正考虑讲话的上下文信息。同时，基于检索的方法对头部姿势变化非常敏感。基于HMM的方法也存在一些缺点，如先验假设的限制（例如，高斯混合模型（GMM）及其对角协方差）。随着深度学习技术的广泛推广，我们在本节中主要回顾基于深度学习的VSG方法。
- en: To make the scope of VSG clear for readers, we first explain the relationship
    and difference between VSG and another hot topic, i.e., Talking Face Generation
    (TFG) ¹¹1Talking Face Generation is also called talking face synthesis, talking
    head generation, or talking portraits generation. These concepts are interchangeable,
    and to be consistent, the expression “Talking Face Generation (TFG)” is adopted
    in this paper. [[159](#bib.bib159), [71](#bib.bib71)].
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使读者清楚VSG的范围，我们首先解释VSG与另一个热门话题——即说话面部生成（TFG）的关系和区别¹¹1说话面部生成也被称为说话面合成、说话头生成或说话肖像生成。这些概念可以互换，为了保持一致，本文采用了“说话面部生成（TFG）”这一表达。[[159](#bib.bib159),
    [71](#bib.bib71)]。
- en: TFG aims to synthesize a realistic, high-quality talking face video corresponding
    to the driving source and the target identity. According to the modality of driving
    sources, TFG can be divided into audio-driven, text-driven, and video-driven.
    Among them, video-driven TFG mainly focuses on video-oriented face-to-face facial
    expression transferring rather than visual speech generation. Therefore, video-driven
    TFG methods will not appear in this paper.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: TFG旨在合成与驱动源和目标身份相对应的现实高质量说话面部视频。根据驱动源的模式，TFG可以分为音频驱动、文本驱动和视频驱动。其中，视频驱动的TFG主要集中在视频导向的面部表情转移，而不是可视语音生成。因此，视频驱动的TFG方法不会出现在本文中。
- en: Traditionally, VSG can be viewed as a key sub-component of text-driven (audio-driven)
    TFG. The other component is video editing, following a specific editing pipeline
    to output the final synthesized talking face video based on the generated lip
    sequence. Recently, to reduce manual intervention and simplify the complexity
    of the overall pipeline, more and more researchers have tried to synthesize full
    talking face in an end-to-end manner instead of lip sequence. Consequently, the
    definition boundary between VSG and text-driven (audio-driven) TFG is getting
    blurred, which means some text-driven (audio-driven) TFG methods are also in our
    review scope. Therefore, to give a comprehensive survey on VSG, we also review
    some TFG methods driven by text and audio, as these works implicitly or explicitly
    involve VSG modules.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，VSG可以视为文本驱动（音频驱动）TFG的一个关键子组件。另一个组件是视频编辑，按照特定的编辑流程生成最终合成的发言面部视频，基于生成的嘴唇序列。最近，为了减少人工干预和简化整体流程的复杂性，越来越多的研究者尝试以端到端的方式合成完整的发言面部，而不是仅仅合成嘴唇序列。因此，VSG与文本驱动（音频驱动）TFG之间的定义边界变得模糊，这意味着一些文本驱动（音频驱动）TFG方法也在我们的综述范围之内。因此，为了全面调查VSG，我们还回顾了一些由文本和音频驱动的TFG方法，因为这些工作隐含或显式地涉及VSG模块。
- en: 5.1 The Overall Pipeline
  id: totrans-464
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 总体流程
- en: Given a reference identity (an image or a 3D facial model of the target speaker)
    and a driving source (a piece of audio or text), the objective of VSG is to generate
    the final synthesized talking lip (face) videos. Existing VSG approaches have
    various properties, such as input modalities (text-driven or audio-driven), synthesizing
    strategies (computer graphics based, image reconstruction based, or hybrid based),
    speaker generalization (speaker-independent or speaker-dependent), learning paradigms
    (supervised learning or unsupervised learning), classifying these approaches is
    not an easy task.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个参考身份（目标说话者的图像或3D面部模型）和一个驱动源（音频或文本），VSG的目标是生成最终合成的发言嘴唇（面部）视频。现有的VSG方法具有各种属性，例如输入模态（文本驱动或音频驱动）、合成策略（基于计算机图形学、基于图像重建或混合）、说话者泛化（说话者独立或说话者依赖）、学习范式（监督学习或无监督学习），对这些方法进行分类并非易事。
- en: 'This section provides a novel taxonomy for VSG methods, as shown in Fig. [8](#S5.F8
    "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning
    for Visual Speech Analysis: A Survey")(a). In specific, we organize VSG approaches
    into two frameworks: a) Two-stage frameworks, which include two mapping steps, i.e.,
    driving source to facial parameters and facial parameters to videos; and b) One-stage
    (Unified) frameworks, having single generation process which is intermediate facial
    parameters free. Next, we review and analyze current two-stage and one-stage VSG
    methods as well as their advantages and disadvantages in detail in Section. [5.2](#S5.SS2
    "5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning for
    Visual Speech Analysis: A Survey") and [5.3](#S5.SS3 "5.3 One-Stage VSG Frameworks
    ‣ 5 Visual Speech Generation ‣ Deep Learning for Visual Speech Analysis: A Survey"),
    respectively.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了一种新颖的VSG方法分类，如图[8](#S5.F8 "图 8 ‣ 5.2 两阶段VSG框架 ‣ 5 视觉语音生成 ‣ 视觉语音分析的深度学习：综述")(a)所示。具体而言，我们将VSG方法组织为两种框架：a)
    两阶段框架，包括两个映射步骤，即从驱动源到面部参数和从面部参数到视频；b) 单阶段（统一）框架，具有单一生成过程，其中不涉及中间面部参数。接下来，我们将在第[5.2节](#S5.SS2
    "5.2 两阶段VSG框架 ‣ 5 视觉语音生成 ‣ 视觉语音分析的深度学习：综述")和第[5.3节](#S5.SS3 "5.3 单阶段VSG框架 ‣ 5
    视觉语音生成 ‣ 视觉语音分析的深度学习：综述")详细回顾和分析当前的两阶段和单阶段VSG方法及其优缺点。
- en: 5.2 Two-Stage VSG Framework
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 两阶段VSG框架
- en: 'The two-stage VSG frameworks mainly consist of two steps: a) mapping the driving
    source to facial parameters using DNNs and b) transforming the learned facial
    parameters to output videos based on GPU rendering, video editing, or Generative
    Adversarial Networks (GANs) [[37](#bib.bib37)]. According to the data type of
    facial parameters, existing two-stage VSG approaches can be divided into Landmarks
    based, Coefficients based, Vertex based and others.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 两阶段VSG框架主要包括两个步骤：a) 使用DNN将驱动源映射到面部参数；b) 基于GPU渲染、视频编辑或生成对抗网络（GANs）将学习到的面部参数转换为输出视频[[37](#bib.bib37)]。根据面部参数的数据类型，现有的两阶段VSG方法可以分为基于地标的、基于系数的、基于顶点的及其他。
- en: '![Refer to caption](img/8b109f4ce641f5d523c07915ee94ce44.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8b109f4ce641f5d523c07915ee94ce44.png)'
- en: 'Figure 8: (a): The overall framework of visual speech generation. (b)-(h):
    Representative Two-stage VSG methods. (i)-(l): Representative One-stage VSG methods.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '图8: (a): 视觉语音生成的总体框架。 (b)-(h): 代表性的两阶段VSG方法。 (i)-(l): 代表性的一阶段VSG方法。'
- en: 5.2.1 Landmark based Methods
  id: totrans-471
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 基于标志点的方法
- en: 'Facial landmark points around facial components capture the rigid and non-rigid
    facial deformations due to head movements and facial expressions [[160](#bib.bib160)].
    Facial landmark points are widely used in various facial analysis tasks, including
    VSG. As a pioneering work, Suwajanakorn et al. [[87](#bib.bib87)] adopted a simple
    single-layer LSTM with the time delay mechanism to learn a nonlinear mapping from
    audio coefficients to lip landmark points. As shown in Fig. [8](#S5.F8 "Figure
    8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning for
    Visual Speech Analysis: A Survey")(b), the model outputs the synthesized talking
    face video of former US President Barack Obama, following the pipeline of facial
    texture synthesis, video re-timing, and target video compositing. Beyond computer
    graphic video generation methods, as shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5.2
    Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning for Visual
    Speech Analysis: A Survey")(c), Kumar et al. [[64](#bib.bib64)] proposed the LSTM
    + UNet architecture, improving the model by replacing the complex video generation
    pipeline with a pix2pix framework [[161](#bib.bib161)]. In this way, there is
    no need to get involved with details of a face, e.g., synthesizing realistic teeth.
    However, as the above methods are trained on only Barack Obama with many hours
    of his weekly address footage, they cannot generalize to new identities or voices.
    The LSTM + UNet VSG backbone architecture is widely adopted in many subsequent
    works [[162](#bib.bib162), [97](#bib.bib97), [163](#bib.bib163), [15](#bib.bib15)].
    Unlike previous methods using audio MFCC features as input, Sinha et al. [[162](#bib.bib162),
    [163](#bib.bib163)] introduced DeepSpeech [[164](#bib.bib164)] features instead,
    as DeepSpeech features are more robust for the variation of speakers.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '面部标志点围绕面部组件捕捉因头部运动和面部表情引起的刚性和非刚性面部变形[[160](#bib.bib160)]。面部标志点广泛应用于各种面部分析任务，包括VSG。作为开创性工作，Suwajanakorn等人[[87](#bib.bib87)]采用了一个简单的单层LSTM与时间延迟机制来学习从音频系数到唇部标志点的非线性映射。如图[8](#S5.F8
    "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning
    for Visual Speech Analysis: A Survey")(b)所示，模型输出了前美国总统奥巴马的合成说话面部视频，遵循了面部纹理合成、视频重新定时和目标视频合成的流程。超越计算机图形视频生成方法，如图[8](#S5.F8
    "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning
    for Visual Speech Analysis: A Survey")(c)所示，Kumar等人[[64](#bib.bib64)]提出了LSTM +
    UNet架构，通过用pix2pix框架[[161](#bib.bib161)]替代复杂的视频生成管道来改进模型。这样，就不需要涉及面部的细节，例如合成逼真的牙齿。然而，由于上述方法仅在奥巴马的每周讲话视频上进行了训练，它们不能推广到新的身份或声音。LSTM
    + UNet VSG骨干架构在许多后续工作中得到了广泛采用[[162](#bib.bib162), [97](#bib.bib97), [163](#bib.bib163),
    [15](#bib.bib15)]。与之前使用音频MFCC特征作为输入的方法不同，Sinha等人[[162](#bib.bib162), [163](#bib.bib163)]引入了DeepSpeech[[164](#bib.bib164)]特征，因为DeepSpeech特征对于说话者变化更为鲁棒。'
- en: In 2018, Jalalifar et al. [[165](#bib.bib165)] proposed LSTM + C-GAN VSG backbone
    architecture, using the basic conditional generative adversarial network (C-GAN) [[166](#bib.bib166)]
    for generating talking faces given the learned landmarks. As the LSTM network
    and C-GAN network are mutual independence, this model can reanimate the target
    face with audio from another person. In 2019, Chen et al. [[65](#bib.bib65)] proposed
    a novel LSTM + Convolutional-RNN structure, further considering the correlation
    between adjacent video frames during generation. Besides, they also propose a
    novel dynamic pixel-wise Loss to solve the pixel jittering problem in correlated
    audio-visual regions. Wang et al. [[167](#bib.bib167)] proposed a three-stage
    VSG framework. Firstly, they use the 3D Hourglass Network as a motion field generator
    to predict landmark points based on the input audio, head motions, and the reference
    image. And then convert the predicted landmark points to dense motion fields.
    Finally, the synthesized talking video is obtained using a first-order motion
    model [[168](#bib.bib168)]. Recently, they further updated the motion field generator
    by replacing the 3D Hourglass Network with a self-attention architecture [[41](#bib.bib41)].
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2018 年，Jalalifar 等人 [[165](#bib.bib165)] 提出了 LSTM + C-GAN VSG 主干架构，使用基本的条件生成对抗网络
    (C-GAN) [[166](#bib.bib166)] 生成给定学习到的地标的对话面部。由于 LSTM 网络和 C-GAN 网络是互相独立的，该模型可以用来自另一人的音频重新激活目标面部。在
    2019 年，Chen 等人 [[65](#bib.bib65)] 提出了新颖的 LSTM + 卷积 RNN 结构，进一步考虑了生成过程中相邻视频帧之间的相关性。此外，他们还提出了一种新颖的动态像素级损失来解决相关音频-视觉区域中的像素抖动问题。Wang
    等人 [[167](#bib.bib167)] 提出了一个三阶段 VSG 框架。首先，他们使用 3D Hourglass 网络作为运动场生成器，根据输入音频、头部运动和参考图像预测地标点。然后将预测的地标点转换为稠密的运动场。最后，使用一阶运动模型
    [[168](#bib.bib168)] 获得合成的对话视频。最近，他们通过用自注意力架构 [[41](#bib.bib41)] 替代 3D Hourglass
    网络，进一步更新了运动场生成器。
- en: 'Besides 2D landmark based approaches, mapping driving source to 3D landmarks
    is also widely explored. Audio signals contain sich semantic-level information,
    including speech content, speaker’s speaking style, emotion, etc. Zhou et al. [[15](#bib.bib15)]
    utilized a voice conversion neural network to learn disentangled speech content
    and identity features. Then, an LSTM-based network is introduced to predict 3D
    landmarks based on speech content features. Finally, the final synthesized talking
    face video is realized using a UNet-style generator network. The key insight is
    to predict 3D landmarks from disentangled audio content features and speaker-aware
    features, such that they capture controllable lip synchronization and head motion
    dynamics. As shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5.2 Two-Stage VSG Framework
    ‣ 5 Visual Speech Generation ‣ Deep Learning for Visual Speech Analysis: A Survey")(d),
    Lu et al. [[169](#bib.bib169)] introduced extracting the high-level speech information
    using an autoregressive predictive coding (APC) model [[170](#bib.bib170)] and
    manifold projection for better generalization. Then, an audio to lip-related motion
    module is designed to predict 3D lip landmarks. Finally, an image-to-image translation
    network (UNet) is introduced to synthesize video frames.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '除了二维地标方法，映射驾驶源到三维地标也得到了广泛的探索。音频信号包含诸如语音内容、说话风格、情感等语义层次的信息。Zhou 等人 [[15](#bib.bib15)]
    使用语音转换神经网络来学习解耦的语音内容和身份特征。然后，引入基于 LSTM 的网络来根据语音内容特征预测三维地标。最后，使用 UNet 风格的生成网络实现最终合成的对话面部视频。关键在于从解耦的音频内容特征和说话者感知特征中预测三维地标，以便捕捉可控的唇部同步和头部运动动态。如图
    [8](#S5.F8 "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation
    ‣ Deep Learning for Visual Speech Analysis: A Survey")(d) 所示，Lu 等人 [[169](#bib.bib169)]
    引入了使用自回归预测编码 (APC) 模型 [[170](#bib.bib170)] 和流形投影来提取高级语音信息，以获得更好的泛化能力。然后，设计了一个音频到唇部相关运动的模块来预测三维唇部地标。最后，引入了一个图像到图像转换网络
    (UNet) 来合成视频帧。'
- en: 5.2.2 Coefficient based Methods
  id: totrans-475
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 基于系数的方法
- en: '2D Coefficient based. Active Appearance Model (AAM) is one of the most commonly
    used facial coefficient models, representing both the shape and texture variations
    and their correlation. Fan et al. [[26](#bib.bib26)] utilized a two-layer BiLSTM
    network to estimate AAM coefficients of the mouth area based on the overlapped
    triphone input, which then is transferred to a face image to produce a photo-realistic
    talking head. The experiments show that the BiLSTM network has superior performance
    to previous HMM-based approaches. Similarly, as shown in Fig. [8](#S5.F8 "Figure
    8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning for
    Visual Speech Analysis: A Survey")(e), Taylor et al. [[66](#bib.bib66)] introduced
    a simple and effective DNN as a sliding window predictor to automatically learn
    AAM coefficients based on the fixed-length phoneme sequence. Furthermore, the
    model can be retargeted to drive other face models with the help of an effective
    retargeting approach. The main practical limitation of AAM coefficients is that
    the reference face AAM parameterization may cause potential errors when retargeting
    to a new subject.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '基于2D系数。主动外观模型（AAM）是最常用的面部系数模型之一，表示形状和纹理变化及其相关性。Fan等[[26](#bib.bib26)]利用两层BiLSTM网络根据重叠的三音素输入估计嘴部区域的AAM系数，然后将其转移到面部图像上以生成照片级逼真的说话头。实验表明，BiLSTM网络的性能优于之前的HMM方法。类似地，如图[8](#S5.F8
    "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning
    for Visual Speech Analysis: A Survey")(e)所示，Taylor等[[66](#bib.bib66)]引入了一种简单有效的DNN作为滑动窗口预测器，以自动学习基于固定长度音素序列的AAM系数。此外，该模型可以通过有效的重新定向方法驱动其他面部模型。AAM系数的主要实际限制是参考面部AAM参数化可能在重新定向到新对象时造成潜在错误。'
- en: '3D Coefficient based. Besides 2D facial coefficient models, 3D facial coefficients
    via principal component analysis (PCA) are more commonly used in VSG [[171](#bib.bib171),
    [172](#bib.bib172), [173](#bib.bib173), [67](#bib.bib67), [174](#bib.bib174),
    [70](#bib.bib70), [175](#bib.bib175)]. Pham et al. [[171](#bib.bib171), [172](#bib.bib172),
    [176](#bib.bib176)] proposed utilizing CNN + RNN based backbone architectures
    to map audio signals to blendshape coefficients [[177](#bib.bib177)] of a 3D face.
    However, these methods rely heavily on the prior 3D facial models of target speakers.
    Hussen et al. [[173](#bib.bib173)] finetuned a pretrained DNN-based acoustic model
    to map driving audios to 3D blendshape coefficients, as they hold the idea that
    a pretrained acoustic model has better generalization on speaker-independent VSG
    tasks than a randomly initialized model. As shown in Fig. [8](#S5.F8 "Figure 8
    ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning for
    Visual Speech Analysis: A Survey")(f), Thies et al. [[67](#bib.bib67)] proposed
    a generalized Audio2Expression network and a specialized UNet-based neural face
    rendering network for audio-driven VSG. The proposed Audio2Expression network
    aims to estimate temporally stable 3D blenshape coefficients based on the DeepSpeech
    audio features, using a CNN based backbone architecture and a content-aware filtering
    network. In this way, the model is able to synthesize talking face videos from
    an audio sequence from another person.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '基于3D系数。除了2D面部系数模型外，通过主成分分析（PCA）得到的3D面部系数在VSG中使用更为广泛[[171](#bib.bib171), [172](#bib.bib172),
    [173](#bib.bib173), [67](#bib.bib67), [174](#bib.bib174), [70](#bib.bib70), [175](#bib.bib175)]。Pham等[[171](#bib.bib171),
    [172](#bib.bib172), [176](#bib.bib176)]提出利用基于CNN + RNN的骨干架构将音频信号映射到3D面部的混合形状系数[[177](#bib.bib177)]。然而，这些方法在很大程度上依赖于目标说话者的预训练3D面部模型。Hussen等[[173](#bib.bib173)]微调了一个预训练的基于DNN的声学模型，将驱动音频映射到3D混合形状系数，他们认为预训练的声学模型在说话者无关的VSG任务上比随机初始化的模型具有更好的泛化能力。如图[8](#S5.F8
    "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning
    for Visual Speech Analysis: A Survey")(f)所示，Thies等[[67](#bib.bib67)]提出了一种通用的Audio2Expression网络和一个专门的基于UNet的神经面部渲染网络，用于音频驱动的VSG。所提出的Audio2Expression网络旨在基于DeepSpeech音频特征估计时间上稳定的3D混合形状系数，使用CNN基础架构和内容感知过滤网络。通过这种方式，该模型能够从另一个人的音频序列合成说话面部视频。'
- en: 'Besides the 3D blendshape model, Kim et al. [[178](#bib.bib178), [179](#bib.bib179)]
    introduced 3D Morphable Model (3DMM) [[180](#bib.bib180)], a more dense 3d face
    parametric representation, for video-oriented face2face translation. The 3DMM
    coefficients contain the rigid head pose parameters, facial identity coefficients,
    expression coefficients, gaze direction parameters for both eyes, and spherical
    harmonics illumination coefficients. Referring to the 3DMM based face2face translation
    pipeline mentioned above, [[181](#bib.bib181), [174](#bib.bib174), [175](#bib.bib175),
    [68](#bib.bib68), [182](#bib.bib182), [4](#bib.bib4), [183](#bib.bib183)] converted
    the driving source from video to audio clips (text scripts) and migrated this
    pipeline to VSG tasks. These methods have an approximate framework, as shown in
    Fig. [8](#S5.F8 "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation
    ‣ Deep Learning for Visual Speech Analysis: A Survey")(g). The flowchart of this
    framework generally follows four steps: 1) Train a network to map the driving
    source to the facial expression coefficients, as visual speech information is
    implicit in facial expression coefficients. 2) Use a pretrained deep face reconstruction
    model to get the 3DMM coefficients of the reference identity image. 3) Combine
    the 3DMM coefficients from the reference identity image and the predicted facial
    expression coefficients to get hybrid 3DMM coefficients. 4) Synthesize talking
    videos using GPU rendering or a generation network.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '除了 3D blendshape 模型外，Kim 等人 [[178](#bib.bib178), [179](#bib.bib179)] 还介绍了 3D
    Morphable Model (3DMM) [[180](#bib.bib180)]，一种更密集的 3D 面部参数表示，用于视频导向的 face2face
    翻译。3DMM 系数包含了刚性头部姿态参数、面部身份系数、表情系数、双眼注视方向参数和球面谐波照明系数。参考上述基于 3DMM 的 face2face 翻译流程
    [[181](#bib.bib181), [174](#bib.bib174), [175](#bib.bib175), [68](#bib.bib68),
    [182](#bib.bib182), [4](#bib.bib4), [183](#bib.bib183)]，将驱动源从视频转换为音频片段（文本脚本），并将这一流程迁移到
    VSG 任务中。这些方法具有类似的框架，如图 [8](#S5.F8 "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5
    Visual Speech Generation ‣ Deep Learning for Visual Speech Analysis: A Survey")(g)
    所示。该框架的流程图大致遵循以下四个步骤：1）训练一个网络，将驱动源映射到面部表情系数，因为面部表情系数中隐含了视觉语音信息。2）使用预训练的深度面部重建模型获取参考身份图像的
    3DMM 系数。3）将参考身份图像的 3DMM 系数和预测的面部表情系数结合，得到混合 3DMM 系数。4）使用 GPU 渲染或生成网络合成说话视频。'
- en: Following the above flowchart, Song et al. [[181](#bib.bib181)] designed a novel
    Audio2Expression network. They empirically find that source identity information
    embedded in speech features will degrade the performance of mapping speech to
    mouth movement. Therefore, they explicitly add an ID-Removing sub-network to remove
    the identity information from the driving audio. Meanwhile, a UNet-style generation
    network is introduced to complete the mouth region guided by mouth landmarks.
    Yi et al. [[174](#bib.bib174)] proposed an LSTM-based network to map audio MFCC
    features to facial expression and head pose, as they argue that audio and head
    pose are correlated in a short period. Besides, they propose a memory-augmented
    GAN to refine these synthesized video frames into real ones. Wu et al. [[182](#bib.bib182)]
    proposed an arbitrary talking style imitation VSG method. During the mapping stage,
    they introduced an extra style reference video as input and used a deep 3D reconstruction
    model to get the style code of the reference video. Next, they concatenate audio
    features with the reconstructed style code to predict the stylized 3DMM coefficients.
    However, the above 3DMM based models are not able to disentangle visual speech
    information from other facial expressions like eyebrow and head pose. Therefore,
    Zhang et al. [[68](#bib.bib68)] proposed a novel flow-guided VSG framework, including
    one style-specific animation generator and one flow-guided video generator, to
    synthesize high visual quality videos. Moreover, the style-specific animation
    generator successfully disentangles lip dynamics with eyebrow and head pose. Li et
    al. [[184](#bib.bib184)] employed a similar framework for text-driven VSG. Ji et
    al. [[4](#bib.bib4)] proposed an emotional video portrait (EVP) to achieve audio-driven
    emotional control for talking face synthesis. Unlike previous methods, they adopt
    the cross-reconstruction [[185](#bib.bib185)] technique in the audio2expression
    stage to decompose the input audio into disentangled content and emotion embeddings.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述流程图，Song 等人[[181](#bib.bib181)] 设计了一种新颖的 Audio2Expression 网络。他们通过实验证明，嵌入语音特征中的源身份信息会降低将语音映射到嘴部动作的性能。因此，他们明确添加了一个
    ID-Removing 子网络，以去除驱动音频中的身份信息。同时，引入了一个 UNet 风格的生成网络，以嘴部标记引导完成嘴部区域。Yi 等人[[174](#bib.bib174)]
    提出了一个基于 LSTM 的网络，将音频 MFCC 特征映射到面部表情和头部姿势，他们认为音频和头部姿势在短时间内是相关的。此外，他们提出了一种记忆增强的
    GAN，将这些合成的视频帧细化为真实的帧。Wu 等人[[182](#bib.bib182)] 提出了一个任意谈话风格模仿的 VSG 方法。在映射阶段，他们引入了额外的风格参考视频作为输入，并使用深度
    3D 重建模型获取参考视频的风格代码。接下来，他们将音频特征与重建的风格代码连接，以预测风格化的 3DMM 系数。然而，以上基于 3DMM 的模型无法将视觉语音信息与其他面部表情如眉毛和头部姿势分离。因此，Zhang
    等人[[68](#bib.bib68)] 提出了一个新颖的流引导 VSG 框架，包括一个风格特定的动画生成器和一个流引导的视频生成器，以合成高视觉质量的视频。此外，风格特定的动画生成器成功地将嘴唇动态与眉毛和头部姿势分离。Li
    等人[[184](#bib.bib184)] 采用了类似的框架用于文本驱动的 VSG。Ji 等人[[4](#bib.bib4)] 提出了一个情感视频肖像（EVP），以实现音频驱动的情感控制，用于合成谈话面孔。与以前的方法不同，他们在
    audio2expression 阶段采用了 cross-reconstruction[[185](#bib.bib185)] 技术，将输入音频分解为解耦的内容和情感嵌入。
- en: 5.2.3 Vertex based Methods
  id: totrans-480
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 基于顶点的方法
- en: '3D facial vertices are another popularly used 3D face model in VSG. For example,
    Karras et al. [[5](#bib.bib5)] used a simple CNN-based architecture to learn a
    nonlinear mapping from input audios to the 3D vertex coordinates (totally 15,066
    vertices) of the target face. To make the synthesized video more natural, they
    introduce an extra emotion code as an intuitive control for the emotional state
    of the face puppet. However, the proposed model is specialized for a particular
    speaker. To overcome this issue, as shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5.2 Two-Stage
    VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning for Visual Speech Analysis:
    A Survey")(h), Cudeiro et al. [[69](#bib.bib69)] extended the model to multiple
    subjects. The proposed VOCA model concatenates the Deepspeech audio features and
    one-hot vector of a speaker and outputs 3D vertex (totally 5023 vertices) displacements
    instead of vertex coordinates. The critical contribution of VOCA is that the additional
    identity control parameters can vary the identity-dependent visual dynamics. Based
    on VOCA, Liu et al. [[186](#bib.bib186)] proposed a geometry-guided dense perspective
    network (GDPnet) with two constraints from different perspectives to achieve a
    more robust generation. Fan et al. [[187](#bib.bib187)] proposed a Transformer-based
    autoregressive VSG model named FaceFormer to encode the long-term audio context
    information and predict a sequence of 3D face vertices.'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '3D 面部顶点是 VSG 中另一个常用的 3D 面部模型。例如，Karras 等人[[5](#bib.bib5)] 使用了一个简单的基于 CNN 的架构来学习从输入音频到目标面部的
    3D 顶点坐标（共 15,066 个顶点）的非线性映射。为了使合成的视频更自然，他们引入了一个额外的情感代码，作为面部玩偶情感状态的直观控制。然而，所提出的模型专门针对特定的说话人。为了解决这个问题，如图
    [8](#S5.F8 "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation
    ‣ Deep Learning for Visual Speech Analysis: A Survey")(h) 所示，Cudeiro 等人[[69](#bib.bib69)]
    将模型扩展到多个主体。所提出的 VOCA 模型将 Deepspeech 音频特征和一个说话人的 one-hot 向量串联，并输出 3D 顶点（共 5023
    个顶点）位移，而不是顶点坐标。VOCA 的关键贡献在于额外的身份控制参数可以改变依赖于身份的视觉动态。基于 VOCA，Liu 等人[[186](#bib.bib186)]
    提出了一个几何引导的密集透视网络（GDPnet），通过来自不同视角的两个约束来实现更强健的生成。Fan 等人[[187](#bib.bib187)] 提出了一个基于
    Transformer 的自回归 VSG 模型 FaceFormer，用于编码长期音频上下文信息并预测一系列 3D 面部顶点。'
- en: Richard et al. [[188](#bib.bib188)] proposed a categorical latent space for
    VSG that disentangles audio-correlated and audio-uncorrelated (facial expressions
    like eye blinks, eyebrow) information based on a cross-modality loss. Then, a
    UNet-style architecture with skip connections is used to predict 3D vertex coordinates.
    Since the modalities disentanglement mechanism, the plausible motion of uncorrelated
    regions of the face is controllable, making the synthesized video more photo-realistic.
    Lahiri et al. [[70](#bib.bib70)] proposed a speaker-dependent VSR method, which
    decomposes the audio to talking face mapping problem into the prediction of the
    3D face shape and the regressions over the 2D texture atlas. To do so, they first
    introduced a normalization preprocessing stage to eliminate the effects of head
    movement and lighting variations. Then, a geometry decoder and an auto-regressive
    texture synthesis network were trained to learn vertex displacements and the corresponding
    lip-centered texture, respectively. Finally, a computer graphics based video rendering
    pipeline is used to generate talking videos for the target speaker.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: Richard 等人[[188](#bib.bib188)] 提出了一个用于 VSG 的分类潜在空间，该空间通过交叉模态损失将音频相关和音频无关（如眼睛眨动、眉毛）信息解耦。然后，使用具有跳跃连接的
    UNet 风格架构来预测 3D 顶点坐标。由于模态解耦机制，面部无关区域的合理运动是可控的，使得合成的视频更加逼真。Lahiri 等人[[70](#bib.bib70)]
    提出了一个依赖于说话人的 VSR 方法，该方法将音频到说话面部映射问题分解为 3D 面部形状的预测和对 2D 纹理图集的回归。为此，他们首先引入了一个归一化预处理阶段，以消除头部运动和光照变化的影响。然后，训练了一个几何解码器和一个自回归纹理合成网络，分别学习顶点位移和相应的以嘴唇为中心的纹理。最后，使用基于计算机图形的渲染管线生成目标说话人的说话视频。
- en: 5.3 One-Stage VSG Frameworks
  id: totrans-483
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 单阶段 VSG 框架
- en: The two-stage VSG frameworks have been dominated before 2018\. Nevertheless,
    two-stage VSG frameworks suffer from the complex processing pipeline, expensive
    and time-consuming facial parameter annotations, extra aid technologies like facial
    landmark detection and monocular 3D face reconstruction etc. Therefore, instead
    of optimizing the individual components of a complex two-stage VSG pipeline, researchers
    have paid more attention to exploring one-stage (end-to-end) VSG approaches. One-stage
    VSG pipelines refer to architectures that directly generate talking lip (face)
    videos from driving source with an end-to-end learning strategy that does not
    involve any intermediate facial parameters.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 两阶段VSG框架在2018年之前一直占主导地位。然而，两阶段VSG框架存在处理流程复杂、面部参数标注昂贵且耗时、需要额外的辅助技术如面部关键点检测和单目3D人脸重建等问题。因此，研究人员更加关注探索一阶段（端到端）VSG方法。一阶段VSG流程是指直接从驱动源生成说话嘴唇（面部）视频的架构，采用不涉及任何中间面部参数的端到端学习策略。
- en: 'Speech2Vid [[189](#bib.bib189)] was among the first to explore one-stage VSG
    frameworks. As shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5.2 Two-Stage VSG Framework
    ‣ 5 Visual Speech Generation ‣ Deep Learning for Visual Speech Analysis: A Survey")(i),
    it consists of four sub-networks. An audio encoder aims to extract speech features
    based on the driving audio; An identity encoder aims to extract identity features
    based on the reference image; And an image decoder tries to output synthesized
    images based on the fused speech and identity features. The above sub-networks
    form an autoencoder architecture, and L1 reconstruction loss is used for training.
    Besides, a separate pretrained deblurring CNN is introduced as a post-processing
    module to improve image quality. As a pioneer work, Speech2Vid provides a baseline
    for speaker-independent VSG and greatly motivates the research on one-stage VSG.
    However, Speech2Vid only uses the L1 reconstruction loss during training, which
    is not efficient for VSG for the following reasons. 1) The L1 reconstruction loss
    is operated on the whole face, and spontaneous motion of the face mainly occurs
    on the upper part of the face, leading to discouraging the visual speech generation.
    2) As Speech2Vid is temporal-independent (no knowledge of its previous outputs),
    it usually produces less coherent video sequences. 3) No consideration of consistency
    of the generated video with the driving audio.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 'Speech2Vid [[189](#bib.bib189)] 是最早探索一阶段VSG框架的工作之一。如图 [8](#S5.F8 "Figure 8
    ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning for
    Visual Speech Analysis: A Survey")(i)所示，它由四个子网络组成。音频编码器旨在基于驱动音频提取语音特征；身份编码器旨在基于参考图像提取身份特征；图像解码器尝试基于融合的语音和身份特征输出合成图像。上述子网络形成了一个自编码器架构，并使用L1重建损失进行训练。此外，引入了一个单独的预训练去模糊CNN作为后处理模块，以提高图像质量。作为开创性工作，Speech2Vid为说话者独立VSG提供了基准，并极大地激发了一阶段VSG的研究。然而，Speech2Vid在训练过程中仅使用L1重建损失，这对VSG的效率不高，原因如下：1）L1重建损失作用于整个面部，面部的自发运动主要发生在面部的上部，从而抑制了视觉语音生成。2）由于Speech2Vid是时间独立的（没有对其先前输出的知识），它通常生成的序列视频连贯性较差。3）没有考虑生成视频与驱动音频的一致性。'
- en: 5.3.1 GAN based Methods
  id: totrans-486
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 基于GAN的方法
- en: 'To overcome the above limitations of Speech2Vid, many researchers try to improve
    VSG performance by utilizing generative adversarial training [[37](#bib.bib37)]
    strategies. As shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5.2 Two-Stage VSG Framework
    ‣ 5 Visual Speech Generation ‣ Deep Learning for Visual Speech Analysis: A Survey")(j),
    GAN based VSG methods usually consist of three sub-architectures, i.e., encoders,
    generators, and discriminators.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '为了克服Speech2Vid的上述限制，许多研究人员尝试通过利用生成对抗训练 [[37](#bib.bib37)]策略来提升VSG的性能。如图 [8](#S5.F8
    "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning
    for Visual Speech Analysis: A Survey")(j)所示，基于GAN的VSG方法通常包括三个子架构，即编码器、生成器和鉴别器。'
- en: 'Taking audio-driven VSG as an example, a piece of audio naturally entangles
    various information, such as speech, emotion, speaking style, etc. As we have
    emphasized in Section. [2.2.2](#S2.SS2.SSS2 "2.2.2 Generation-Related Challenges
    ‣ 2.2 Main Challenges ‣ 2 Background ‣ Deep Learning for Visual Speech Analysis:
    A Survey"), information coupling brings enormous challenges to VSR. To ameliorate
    this issue, Zhou et al. [[71](#bib.bib71)] proposed a novel VSG framework called
    Disentangled Audio-Visual System (DAVS). Compared with previous VSG approaches,
    they focus more on the disentangled speech and identity feature extraction, which
    is realized based on supervised adversarial training. However, DAVS relies on
    extra Word-ID labels and Person-ID labels in the training stage. Sun et al. [[72](#bib.bib72)]
    improved the model by learning speech and identity features within a self-supervised
    contrastive learning framework, with no need for extra annotations. Si et al. [[190](#bib.bib190)]
    utilized knowledge distillation to disentangle emotion features, identity features,
    and speech features from the audio input with the help of a pretrained emotion
    recognition teacher network and a pretrained face recognition teacher network.
    Recently, some works have tried to encode additional facial controllable dynamics
    like emotion and head pose into the generation pipeline to generate a more natural-spontaneous
    talking face. For example, [[191](#bib.bib191), [192](#bib.bib192)] introduce
    additional emotion encoders, and [[193](#bib.bib193)] devise implicit pose encodings
    into the generation pipeline.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '以音频驱动的视觉语音生成（VSG）为例，一段音频自然地包含了各种信息，如语音、情感、说话风格等。正如我们在第[2.2.2](#S2.SS2.SSS2
    "2.2.2 Generation-Related Challenges ‣ 2.2 Main Challenges ‣ 2 Background ‣ Deep
    Learning for Visual Speech Analysis: A Survey")节中强调的，信息耦合给视觉语音识别（VSR）带来了巨大的挑战。为了解决这个问题，Zhou等人[[71](#bib.bib71)]提出了一种新型的VSG框架，称为解耦音频-视觉系统（DAVS）。与之前的VSG方法相比，他们更关注解耦语音和身份特征提取，这依赖于监督对抗训练。然而，DAVS在训练阶段需要额外的词汇-ID标签和人物-ID标签。Sun等人[[72](#bib.bib72)]通过在自监督对比学习框架内学习语音和身份特征来改进模型，无需额外的标注。Si等人[[190](#bib.bib190)]利用知识蒸馏从音频输入中解耦情感特征、身份特征和语音特征，借助于预训练的情感识别教师网络和预训练的面部识别教师网络。最近，一些研究尝试将额外的可控面部动态，如情感和头部姿态，编码到生成管道中，以生成更加自然和自发的说话面孔。例如，[[191](#bib.bib191),
    [192](#bib.bib192)]引入了额外的情感编码器，而[[193](#bib.bib193)]则在生成管道中设计了隐式姿态编码。'
- en: Considering the drawbacks of only using image reconstruction loss, GAN based
    methods focus on customizing more effective learning goals for VSG. For example,
    Prajwal et al. [[194](#bib.bib194), [38](#bib.bib38)] introduced a simple audio-visual
    synchronization discriminator for lip-syncing VSG. In addition, Chen et al. [[116](#bib.bib116)]
    proposed an audio-visual derivative correlation loss to optimize the consistency
    of the two modalities in feature spaces and a three-stream GAN discriminator to
    force talking mouth videos generation depending on the input audio signal.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到仅使用图像重建损失的缺陷，基于生成对抗网络（GAN）的方法专注于为VSG定制更有效的学习目标。例如，Prajwal等人[[194](#bib.bib194),
    [38](#bib.bib38)]为唇动同步的VSG引入了一个简单的音频-视觉同步判别器。此外，Chen等人[[116](#bib.bib116)]提出了一种音频-视觉导数相关损失，以优化特征空间中两种模态的一致性，并提出了一个三流GAN判别器，根据输入音频信号强制生成说话嘴部视频。
- en: For temporal-dependent video generation, [[40](#bib.bib40), [195](#bib.bib195),
    [196](#bib.bib196)] utilized autoregression style VSG generator networks for talking
    face generation. Two discriminators, i.e., a frame and sequence discriminator,
    are used to optimize the generated facial dynamics. Based on [[40](#bib.bib40)],
    Song et al. [[39](#bib.bib39)] introduced a VSR discriminator further to improve
    the lip movement accuracy of generated talking videos. The ablation study demonstrated
    that the additional VSR discriminator helps achieve more obvious lip movement,
    proving our motivation that VSR and VSG are dual and mutually promoted. Furthermore,
    Chen et al. [[32](#bib.bib32)] developed the DualLip system to jointly improve
    VSR and VSG by leveraging the task duality and demonstrating that both VSR and
    VSG models can be enhanced with the help of extra unlabeled data. Besides the
    above learning goals, the optical flow discriminator [[197](#bib.bib197)], speech-related
    facial action units [[198](#bib.bib198)], and cross-modal mutual information estimator [[199](#bib.bib199)]
    are also utilized to optimize lip motion and cross-modal consistency of generated
    talking videos with the driving source.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 对于时间相关的视频生成，[[40](#bib.bib40), [195](#bib.bib195), [196](#bib.bib196)] 利用自回归风格的
    VSG 生成网络进行说话脸部生成。使用了两个判别器，即帧判别器和序列判别器，用于优化生成的面部动态。基于 [[40](#bib.bib40)]，Song et
    al. [[39](#bib.bib39)] 进一步引入了 VSR 判别器，以提高生成视频的唇部运动准确性。消融研究表明，额外的 VSR 判别器有助于实现更明显的唇部运动，证明了我们认为
    VSR 和 VSG 是相互促进的动机。此外，Chen et al. [[32](#bib.bib32)] 开发了 DualLip 系统，通过利用任务的双重性共同提高
    VSR 和 VSG，并证明了在额外无标注数据的帮助下，VSR 和 VSG 模型都可以得到增强。除了上述学习目标外，还利用了光流判别器 [[197](#bib.bib197)]、与语音相关的面部动作单元
    [[198](#bib.bib198)] 和跨模态互信息估计器 [[199](#bib.bib199)] 来优化生成视频的唇部运动和跨模态一致性。
- en: 'TABLE IV: The comparison of representative VSG methods.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 代表性 VSG 方法的比较。'
- en: '| Framework & Method | Settings | GRID | LRW | Highlights |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| 框架 & 方法 | 设置 | GRID | LRW | 亮点 |'
- en: '| Input^† | Training Set | Extra Requriment | By-product^‡ | PSNR | SSIM |
    LMD | PSNR | SSIM | LMD |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| 输入^† | 训练集 | 额外要求 | 副产品^‡ | PSNR | SSIM | LMD | PSNR | SSIM | LMD |'
- en: '| Two-stage | Landmark based | Chen et al. [[65](#bib.bib65)] | A+I |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| 两阶段 | 基于地标 | Chen et al. [[65](#bib.bib65)] | A+I |'
- en: '&#124; LRW &#124;'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRW &#124;'
- en: '&#124; GRID &#124;'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GRID &#124;'
- en: '| Landmark detector | / | 32.15 | 0.83 | 1.29 | 30.91 | 0.81 | 1.37 |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| 地标检测器 | / | 32.15 | 0.83 | 1.29 | 30.91 | 0.81 | 1.37 |'
- en: '&#124; Proposed a Convolutional-RNN structure, &#124;'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了一个卷积-RNN 结构 &#124;'
- en: '&#124; which utilizes correlation between adjacent &#124;'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用相邻之间的相关性 &#124;'
- en: '&#124; frames in the generation stage &#124;'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成阶段的帧 &#124;'
- en: '|'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Das et al. [[163](#bib.bib163)] | A+I | TCD-TIMIT |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| Das et al. [[163](#bib.bib163)] | A+I | TCD-TIMIT |'
- en: '&#124; Landmark detector; &#124;'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 地标检测器; &#124;'
- en: '&#124; DeepSpeech model &#124;'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DeepSpeech 模型 &#124;'
- en: '| Blink motion | 29.9 | 0.83 | 1.22 | / | / | / |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| 眨眼动作 | 29.9 | 0.83 | 1.22 | / | / | / |'
- en: '&#124; Proposed two GAN based networks to &#124;'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了两个基于 GAN 的网络来 &#124;'
- en: '&#124; learn the motion and texture separately &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分别学习运动和纹理 &#124;'
- en: '|'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Wang et al. [[167](#bib.bib167)] | A+I |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[167](#bib.bib167)] | A+I |'
- en: '&#124; GRID &#124;'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GRID &#124;'
- en: '&#124; LRW &#124;'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRW &#124;'
- en: '&#124; VoxCeleb &#124;'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VoxCeleb &#124;'
- en: '|'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Landmark detector; &#124;'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 地标检测器; &#124;'
- en: '&#124; Pretrained image generator &#124;'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预训练图像生成器 &#124;'
- en: '&#124; and face encoder &#124;'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和面部编码器 &#124;'
- en: '| Head Motion | 30.93 | 0.91 | / | 19.53* | 0.63* | / |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 头部运动 | 30.93 | 0.91 | / | 19.53* | 0.63* | / |'
- en: '&#124; Regressed the head motions &#124;'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回归头部运动 &#124;'
- en: '&#124; in accordance with audio dynamics &#124;'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 根据音频动态 &#124;'
- en: '|'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Coefficient based | Song et al. [[181](#bib.bib181)] | A+V |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| 基于系数 | Song et al. [[181](#bib.bib181)] | A+V |'
- en: '&#124; GRID & &#124;'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GRID & &#124;'
- en: '&#124; a novel dataset &#124;'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一个新颖的数据集 &#124;'
- en: '| Face reconstruction | Head Motion | 32.23 | 0.97 | / | / | / | / |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| 面部重建 | 头部运动 | 32.23 | 0.97 | / | / | / | / |'
- en: '&#124; Proposed an Audio ID-Removing Network &#124;'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了一个音频 ID 移除网络 &#124;'
- en: '&#124; for pure speech feature learning &#124;'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用于纯语音特征学习 &#124;'
- en: '|'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Yi et al. [[174](#bib.bib174)] | A+I | LRW | Face reconstruction | Head Motion
    | / | / | / | 30.94 | 0.75 | 1.58 |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| Yi et al. [[174](#bib.bib174)] | A+I | LRW | 面部重建 | 头部运动 | / | / | / | 30.94
    | 0.75 | 1.58 |'
- en: '&#124;    Proposed a memory-augmented GAN &#124;'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;    提出了一个增强记忆的 GAN &#124;'
- en: '&#124; module for rendered frames refining &#124;'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用于渲染帧的精炼模块 &#124;'
- en: '|'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| One-stage |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| 单阶段 |'
- en: '&#124; Auto &#124;'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动 &#124;'
- en: '&#124; Encoder &#124;'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编码器 &#124;'
- en: '| Chung et al. [[189](#bib.bib189)] | A+I |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| Chung et al. [[189](#bib.bib189)] | A+I |'
- en: '&#124; VoxCeleb &#124;'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VoxCeleb &#124;'
- en: '&#124; LRW &#124;'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRW &#124;'
- en: '| Pretrained face encoder | / | 29.36 | 0.74 | 1.35 | 28.06 | 0.46 | 2.25 |
    promoted the research on one-stage VSG |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| 预训练人脸编码器 | / | 29.36 | 0.74 | 1.35 | 28.06 | 0.46 | 2.25 | 推动了单阶段VSG的研究 |'
- en: '| GAN Based | Vougioukas et al. [[195](#bib.bib195)] | A+I | GRID | Pretrained
    VSR model | / | 27.10 | 0.82 | / | 23.08 | 0.76 | / |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 基于GAN的 | Vougioukas等人 [[195](#bib.bib195)] | A+I | GRID | 预训练VSR模型 | / |
    27.10 | 0.82 | / | 23.08 | 0.76 | / |'
- en: '&#124; Utilized an autoregressive temporal GAN &#124;'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用自回归时间GAN &#124;'
- en: '&#124; for more coherent sequences generation &#124;'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用于生成更连贯的序列 &#124;'
- en: '|'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Chen et al. [[116](#bib.bib116)] | A+I |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| 陈等人 [[116](#bib.bib116)] | A+I |'
- en: '&#124; GRID &#124;'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GRID &#124;'
- en: '&#124; LRW &#124;'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRW &#124;'
- en: '| Pretrained FlowNet | / | 29.89 | 0.73 | 1.18 | 28.65 | 0.53 | 1.92 |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| 预训练FlowNet | / | 29.89 | 0.73 | 1.18 | 28.65 | 0.53 | 1.92 |'
- en: '&#124; Proposed a novel correlation loss to &#124;'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了新颖的相关性损失来 &#124;'
- en: '&#124; synchronize lip movements and input audio &#124;'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 同步唇部动作和输入音频 &#124;'
- en: '|'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Prajwal et al. [[194](#bib.bib194)] | A+V | LRS2 | / | / | / | / | / | 33.4
    | 0.96 | 0.60 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| Prajwal等人 [[194](#bib.bib194)] | A+V | LRS2 | / | / | / | / | / | 33.4 |
    0.96 | 0.60 |'
- en: '&#124; Proved that a lip synchronization &#124;'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 证明了唇部同步 &#124;'
- en: '&#124; discriminator is quite useful for VSG &#124;'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 鉴别器改进了跨模态一致性，对VSG非常有用 &#124;'
- en: '|'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Song et al. [[39](#bib.bib39)] | A+I |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| 宋等人 [[39](#bib.bib39)] | A+I |'
- en: '&#124; TCD-TIMIT &#124;'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TCD-TIMIT &#124;'
- en: '&#124; LRW &#124;'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRW &#124;'
- en: '&#124; VoxCeleb &#124;'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VoxCeleb &#124;'
- en: '| Pretrained VSR model | / | / | / | / | 27.43 | 0.92 | 3.14 |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| 预训练VSR模型 | / | / | / | / | 27.43 | 0.92 | 3.14 |'
- en: '&#124; Introduced a lip-reading discriminator &#124;'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引入了唇读鉴别器 &#124;'
- en: '&#124; to guide lip motion generation &#124;'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引导唇部动作生成 &#124;'
- en: '|'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Zhou et al. [[71](#bib.bib71)] | A+V | LRW | Word and Identity labels | /
    | / | / | / | 26.7 | 0.88 | / |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| 周等人 [[71](#bib.bib71)] | A+V | LRW | 词汇和身份标签 | / | / | / | / | 26.7 | 0.88
    | / |'
- en: '&#124; Improved visual quality by using disentangled &#124;'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过使用解耦的 &#124;'
- en: '&#124;  audio-visual representation learning &#124;'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  音频-视觉表示学习 &#124;'
- en: '|'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Zhu et al. [[199](#bib.bib199)] | A+I |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| 朱等人 [[199](#bib.bib199)] | A+I |'
- en: '&#124; GRID &#124;'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GRID &#124;'
- en: '&#124; LRW &#124;'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LRW &#124;'
- en: '| / | / | 31.01 | 0.97 | 0.78 | 32.08 | 0.92 | 1.21 |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| / | / | 31.01 | 0.97 | 0.78 | 32.08 | 0.92 | 1.21 |'
- en: '&#124; Improved cross-modality coherence with a novel &#124;'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过一种新颖的 &#124;'
- en: '&#124; Asymmetric Mutual Information Estimator (AMIE) &#124;'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 非对称互信息估计器（AMIE） &#124;'
- en: '|'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Chen et al. [[198](#bib.bib198)] | A+I |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| 陈等人 [[198](#bib.bib198)] | A+I |'
- en: '&#124; GRID &#124;'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GRID &#124;'
- en: '&#124; TCD-TIMIT &#124;'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TCD-TIMIT &#124;'
- en: '| AU Classifier | / | 29.84 | 0.77 | / | / | / | / |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| AU分类器 | / | 29.84 | 0.77 | / | / | / | / |'
- en: '&#124; Used both audio and speech-related facial &#124;'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用了音频和语音相关的面部 &#124;'
- en: '&#124; action units (AUs) as driving information &#124;'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 作为驱动信息的动作单元（AUs） &#124;'
- en: '|'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Others | Ye et al. [[74](#bib.bib74)] | A+V | a mixed dataset | Pretrained
    AudioNet | / | / | / | / | 31.98 | 0.81 | 1.44 |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | Ye等人 [[74](#bib.bib74)] | A+V | 混合数据集 | 预训练AudioNet | / | / | / | /
    | 31.98 | 0.81 | 1.44 |'
- en: '&#124; Proposed a novel one-stage VSG paradigm with &#124;'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提出了一个新颖的单阶段VSG范式 &#124;'
- en: '&#124; the introduction of dynamic convolution kernels &#124;'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引入了动态卷积核 &#124;'
- en: '|'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '1'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: '$*$: Including background regions.'
  id: totrans-585
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$*$: 包括背景区域。'
- en: '2'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2'
- en: '^†: A-Audio, I-Image, V-Video.'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '^†: A-音频, I-图像, V-视频。'
- en: '3'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3'
- en: '^‡: Additional effects besides VSG.'
  id: totrans-589
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '^‡: 除了VSG的附加效果。'
- en: 5.3.2 Other Methods
  id: totrans-590
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 其他方法
- en: 'In addition, some other one-stage VSG schemes have also been proposed. Inspired
    by the success of the neural radiance field (NeRF) [[200](#bib.bib200)], Guo et
    al. [[73](#bib.bib73)] proposed the audio-driven neural radiance fields (AD-NeRF)
    model for VSG. As shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5.2 Two-Stage VSG Framework
    ‣ 5 Visual Speech Generation ‣ Deep Learning for Visual Speech Analysis: A Survey")(k),
    AD-NeRF takes DeepSpeech audio features as conditional input, learning an implicit
    neural scene representation function to map audio features to dynamic neural radiance
    fields for talking face rendering. Furthermore, AD-NeRF models not only the head
    region but also the upper body via learning two individual neural radiance fields.
    However, AD-NeRF does not generalize well on mismatched driving audios and speakers.
    As shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual
    Speech Generation ‣ Deep Learning for Visual Speech Analysis: A Survey")(l), unlike
    the previous concatenation-based feature fusion strategy, Ye et al. [[74](#bib.bib74)]
    presented a full convolutional neural network with dynamic convolution kernels
    (DCKs) for cross-modal feature fusion, which extracts features from audio and
    reshapes features as DCKs of the fully convolutional network. Due to the simple
    yet effective network architecture, the real-time performance of VSG is significantly
    improved.'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，还有一些其他的一阶段VSG方案被提出。受神经辐射场（NeRF）[[200](#bib.bib200)]成功的启发，Guo等人[[73](#bib.bib73)]提出了用于VSG的音频驱动神经辐射场（AD-NeRF）模型。如图[8](#S5.F8
    "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning
    for Visual Speech Analysis: A Survey")(k)所示，AD-NeRF将DeepSpeech音频特征作为条件输入，学习一个隐式的神经场景表示函数，将音频特征映射到动态神经辐射场以进行说话面部渲染。此外，AD-NeRF不仅建模头部区域，还通过学习两个独立的神经辐射场来建模上半身。然而，AD-NeRF在不匹配的驱动音频和说话者上表现不佳。如图[8](#S5.F8
    "Figure 8 ‣ 5.2 Two-Stage VSG Framework ‣ 5 Visual Speech Generation ‣ Deep Learning
    for Visual Speech Analysis: A Survey")(l)所示，与之前基于连接的特征融合策略不同，Ye等人[[74](#bib.bib74)]提出了一种具有动态卷积核（DCKs）的全卷积神经网络，用于跨模态特征融合，该网络从音频中提取特征，并将特征重塑为全卷积网络的DCKs。由于网络架构简单却有效，VSG的实时性能显著提高。'
- en: 5.4 Summary and Performance Comparison
  id: totrans-592
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 总结与性能比较
- en: Visual speech generation is an important and challenging problem in the cross-field
    of computer vision, computer graphics, and natural language analysis and has received
    considerable attention in recent five years. Moreover, thanks to remarkable developments
    in deep learning techniques, the field of VSG has dramatically evolved. In this
    subsection, we will discuss representative VSG methods on large-scale datasets
    and summarize the main issues of VSG.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 可视语音生成是计算机视觉、计算机图形学和自然语言分析交叉领域中的一个重要且具有挑战性的问题，近年来受到了相当多的关注。此外，得益于深度学习技术的显著进展，VSG领域已经发生了剧变。在本小节中，我们将讨论大规模数据集上的代表性VSG方法，并总结VSG的主要问题。
- en: Because VSG approaches have various implementation requirements (driving sources,
    extra technologies, diverse annotation needs, specific datasets, etc.) and configurations
    (training sets, learning paradigms, lip or whole face generation, background,
    pose and emotion control, etc.), it may be impractical to compare every recently
    proposed VSG method in a unified and fair manner.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 由于VSG方法具有多种实现要求（驱动源、额外技术、多样化的注释需求、特定数据集等）和配置（训练集、学习范式、嘴唇或整个面部生成、背景、姿势和情感控制等），在统一和公平的方式下比较每种最近提出的VSG方法可能是不切实际的。
- en: 'It is nevertheless valuable to integrate some representative VSG methods and
    their requirements, configurations, and highlights into a table. Therefore, as
    shown in Table. [IV](#S5.T4 "TABLE IV ‣ 5.3.1 GAN based Methods ‣ 5.3 One-Stage
    VSG Frameworks ‣ 5 Visual Speech Generation ‣ Deep Learning for Visual Speech
    Analysis: A Survey"), we summarize the performance and experimental settings of
    some representative VSG methods tested on large-scale, commonly used benchmark
    datasets, including GRID and LRW.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，将一些代表性VSG方法及其要求、配置和亮点整合到一个表格中仍然是有价值的。因此，如表[IV](#S5.T4 "TABLE IV ‣ 5.3.1
    GAN based Methods ‣ 5.3 One-Stage VSG Frameworks ‣ 5 Visual Speech Generation
    ‣ Deep Learning for Visual Speech Analysis: A Survey")所示，我们总结了在大规模、常用基准数据集上测试的一些代表性VSG方法的性能和实验设置，包括GRID和LRW。'
- en: 'To give readers a general understanding of the performance of the VSG method
    in different frameworks, three commonly used quantitative evaluation metrics, i.e.,
    PSNR, SSIM, and LMD, are listed in Table. [IV](#S5.T4 "TABLE IV ‣ 5.3.1 GAN based
    Methods ‣ 5.3 One-Stage VSG Frameworks ‣ 5 Visual Speech Generation ‣ Deep Learning
    for Visual Speech Analysis: A Survey"). It is worth noting that the above three
    metrics are most widely used for VSG, even though they are not yet effective and
    perfect enough. Although many quantitative metrics for VSG were proposed recently,
    the following issues need further investigation.'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '为了让读者对VSG方法在不同框架中的表现有一个总体了解，表格中列出了三种常用的定量评估指标，即PSNR、SSIM和LMD，[IV](#S5.T4 "TABLE
    IV ‣ 5.3.1 GAN based Methods ‣ 5.3 One-Stage VSG Frameworks ‣ 5 Visual Speech
    Generation ‣ Deep Learning for Visual Speech Analysis: A Survey")。值得注意的是，尽管这三种指标在VSG中最为广泛使用，但它们尚未足够有效和完善。尽管最近提出了许多用于VSG的定量指标，但以下问题仍需进一步研究。'
- en: $\bullet$
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: In the early stage of VSG, qualitative evaluations are primarily utilized, such
    as visualization and user preference studies [[66](#bib.bib66), [16](#bib.bib16),
    [87](#bib.bib87)]. However, qualitative evaluations are unstable and unreproducible.
  id: totrans-598
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在VSG的早期阶段，主要使用定性评估方法，例如可视化和用户偏好研究[[66](#bib.bib66), [16](#bib.bib16), [87](#bib.bib87)]。然而，定性评估不稳定且不可重复。
- en: $\bullet$
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Many works have attempted to establish VSG evaluation benchmarks, and more than
    a dozen evaluation metrics have been proposed for VSG. Consequently, existing
    VSG evaluation benchmarks are not unified. Chen et al. [[42](#bib.bib42)] have
    conducted a survey of VSG evaluation and designed a unified benchmark for VSG
    according to desired properties. To promote VSG development, researchers should
    pay more effort to VSG evaluation benchmarks.
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 许多研究尝试建立VSG评估基准，并提出了十多种评估指标。因此，现有的VSG评估基准尚未统一。陈等人[[42](#bib.bib42)]对VSG评估进行了调查，并根据期望的特性设计了统一的VSG评估基准。为了推动VSG的发展，研究人员应更加关注VSG评估基准。
- en: $\bullet$
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: The results of quantitative evaluation and qualitative evaluation are sometimes
    in mutual conflict. For example, some works [[39](#bib.bib39), [195](#bib.bib195),
    [167](#bib.bib167)] have observed that both the PSNR and SSIM are negatively affected
    by introducing image or video discriminators. Nevertheless, these discriminators
    significantly improve the video realism and visual quality in the user study experiments.
  id: totrans-602
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定量评估和定性评估的结果有时会相互冲突。例如，一些研究[[39](#bib.bib39), [195](#bib.bib195), [167](#bib.bib167)]观察到，引入图像或视频判别器会对PSNR和SSIM产生负面影响。然而，这些判别器在用户研究实验中显著提高了视频的真实性和视觉质量。
- en: $\bullet$
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: In practical applications, real-time is another substantial demand for VSG.
    However, most of the current VSG methods ignore real-time. Therefore, real-time
    performance is also an important evaluation metric that needs to be considered
    in the future.
  id: totrans-604
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在实际应用中，实时性是VSG的另一个重要需求。然而，目前大多数VSG方法忽略了实时性。因此，实时性能也是未来需要考虑的重要评估指标。
- en: 6 Conclusion and Outlooks
  id: totrans-605
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与展望
- en: 'In this paper, we have presented a comprehensive review on the deep learning
    based VSA. We focus on two fundamental questions, i.e., visual speech recognition
    and visual speech generation, and summarize realistic challenges and current developments,
    including datasets, evaluation protocols, representative methods, SOTA performance,
    practical issues, etc. We presented a systemic overview of VSR and VSG approaches
    and discussed their underlying connections, contributions, and shortcomings. Considering
    that many practical issues discussed in Section. [4.4](#S4.SS4 "4.4 Summary and
    performance comparison ‣ 4 Visual Speech Recognition ‣ Deep Learning for Visual
    Speech Analysis: A Survey") and Section. [5.4](#S5.SS4 "5.4 Summary and Performance
    Comparison ‣ 5 Visual Speech Generation ‣ Deep Learning for Visual Speech Analysis:
    A Survey") remain unresolved, there are still enough opportunities for VSA research
    and application. We attempt to provide some ideas and discuss potential future
    research directions in the following.'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们对基于深度学习的VSA进行了全面的综述。我们关注两个基本问题，即视觉语音识别和视觉语音生成，并总结了现实挑战和当前进展，包括数据集、评估协议、代表性方法、SOTA性能、实际问题等。我们提供了VSR和VSG方法的系统概述，并讨论了它们的基本联系、贡献和不足。考虑到第[4.4节](#S4.SS4
    "4.4 Summary and performance comparison ‣ 4 Visual Speech Recognition ‣ Deep Learning
    for Visual Speech Analysis: A Survey")和第[5.4节](#S5.SS4 "5.4 Summary and Performance
    Comparison ‣ 5 Visual Speech Generation ‣ Deep Learning for Visual Speech Analysis:
    A Survey")中讨论的许多实际问题仍未解决，VSA研究和应用仍有足够的机会。我们尝试提供一些想法，并讨论以下潜在的未来研究方向。'
- en: Advanced sensors for visual speech. There are at least three reasons for developing
    more advanced sensors for acquiring visual speech data. (1) It is estimated that
    only 30% to 40% of speech sounds can be noncontact lip read. (2) Noncontact visual
    speech data suffer from Coupling with expression and head movement, etc. (3) Existing
    VSA systems are developed based on talking face videos. However, these systems
    fail when the speaker’s mouth is covered by a mask (ordinary in the coronavirus
    disease 2019 (COVID-19) pandemic). As diverse visual speech data provides a promising
    way to boost the development of VSA, therefore, a potential solution is to develop
    contact visual speech sensors by capturing the motion of speech-related muscles.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 高级视觉语音传感器。开发更先进的传感器以获取视觉语音数据有至少三个原因。(1) 估计只有30%到40%的语音声音可以通过非接触式唇读。 (2) 非接触式视觉语音数据受表情和头部运动等因素的影响。
    (3) 现有的VSA系统是基于说话者的面部视频开发的。然而，这些系统在说话者的嘴巴被口罩遮盖时（在2019冠状病毒病（COVID-19）疫情中很常见）会失效。由于多样的视觉语音数据为VSA的发展提供了有希望的途径，因此，一个潜在的解决方案是通过捕捉与语音相关的肌肉运动来开发接触式视觉语音传感器。
- en: Learning with fewer labels. As we have mentioned above, collecting a large-scale
    audio-visual dataset is quite costly, and manually labeling is even more consuming.
    Existing deep learning based VSA approaches usually rely heavily on labelled data,
    which is a current limitation for VSA research. Recently, some works have explored
    cross-modal self-supervised learning, knowledge distillation for VSA. However,
    it is valuable to explore other label-efficient learning paradigms like domain
    adaptation, active learning, few-shot learning, etc.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 少标签学习。如前所述，收集大规模视听数据集的成本相当高，而人工标注的成本更高。现有的基于深度学习的VSA方法通常严重依赖标注数据，这是VSA研究的当前限制。最近，一些研究探索了跨模态自监督学习、知识蒸馏等。然而，探索其他高效标注学习范式，如领域适应、主动学习、少样本学习等，是有价值的。
- en: Multilingual VSA. Existing audio-visual datasets are mostly monolingual. In
    general, English is the most universal language. However, in some practical scenes
    like air traffic control (ATC) and international conference, multilingual communication
    is needed. Although multilingual audio speech recognition has been widely explored,
    multilingual visual speech recognition has received little attention yet.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 多语言VSA。现有的视听数据集大多是单语的。一般来说，英语是最普遍的语言。然而，在一些实际场景中，如空中交通管制（ATC）和国际会议，需要多语言交流。尽管多语言语音识别已经得到广泛研究，但多语言视觉语音识别仍然鲜有关注。
- en: Extended applications of VSA. Besides VSR and VSG, there are also some hot topics
    that VSA can be helpful. One of the most common tasks is audio-visual speech recognition
    (AVSR), a speech recognition technology that uses visual and audio information.
    Another typical extended task is audio-visual speech enhancement (AVSE), aiming
    to separate a speaker’s voice given lip regions in the corresponding video by
    predicting both the magnitude and the phase of the target signal. Besides, for
    DeepFake detection, VSA can serve as an effective technology for counterfeit talking
    video detection.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: VSA 的扩展应用。除了 VSR 和 VSG，还有一些热门话题，VSA 也可能发挥作用。其中一个最常见的任务是音视频语音识别（AVSR），这是一种利用视觉和音频信息的语音识别技术。另一个典型的扩展任务是音视频语音增强（AVSE），其目的是通过预测目标信号的幅度和相位来分离给定视频中的说话者声音。此外，在
    DeepFake 检测中，VSA 可以作为有效的伪造对话视频检测技术。
- en: VSA technologies for virtual characters. As an emerging type of internet application
    and social platform, Metaverse has recently gained a lot of attention. Virtual
    avatar modeling is a crucial technology in the field of Metaverse. With the rapid
    development of Metaverse technology, virtual characters-oriented VSA technologies
    also came into being. Considering that existing VSA methods mostly focus on realistic
    speakers, virtual characters-oriented VSA research is a potential direction in
    the future.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: VSA 技术在虚拟角色中的应用。作为一种新兴的互联网应用和社交平台，元宇宙最近引起了广泛关注。虚拟头像建模是元宇宙领域中的关键技术。随着元宇宙技术的快速发展，面向虚拟角色的
    VSA 技术也应运而生。考虑到现有的 VSA 方法主要集中在现实说话者上，面向虚拟角色的 VSA 研究是未来的潜在方向。
- en: Security & robustness for VSA. Security and robustness are important requirements
    in the public safety field of VSA technology. Recent research has demonstrated
    that deep learning-based AI systems are vulnerable to different types of attacks,
    such as adversarial attacks , and spoofing attacks. This raises serious concerns
    in the field of security. However, security and robustness are not taken seriously
    in existing VSA approaches.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: VSA 的安全性和鲁棒性。安全性和鲁棒性是 VSA 技术在公共安全领域中的重要要求。最近的研究表明，基于深度学习的 AI 系统易受各种攻击，例如对抗攻击和欺骗攻击。这在安全领域引发了严重关切。然而，现有的
    VSA 方法并没有充分重视安全性和鲁棒性。
- en: Privacy preserving for VSA. As VSA involves face-related private information,
    it is hard to construct a public large-scale audio-visual dataset, which also
    hinders the development of VSA. To address this issue, available privacy-preserving
    techniques such as Federated Learning, Homomorphic Encryption, and Secure Multi-Party
    Computation can be helpful. However, to the best of our knowledge, privacy preserving
    VSA research has not yet started.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: VSA 的隐私保护。由于 VSA 涉及与面部相关的私人信息，构建一个公开的大规模音视频数据集非常困难，这也阻碍了 VSA 的发展。为了解决这个问题，可用的隐私保护技术，如联邦学习、同态加密和安全多方计算，可以发挥作用。然而，据我们所知，隐私保护的
    VSA 研究尚未开始。
- en: References
  id: totrans-614
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] T. Chen, “Audiovisual speech processing,” *IEEE signal processing magazine*,
    vol. 18, no. 1, pp. 9–21, 2001.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] T. Chen, “视听语音处理，” *IEEE 信号处理杂志*，第18卷，第1期，页码 9–21，2001年。'
- en: '[2] H. McGurk and J. MacDonald, “Hearing lips and seeing voices,” *Nature*,
    vol. 264, no. 5588, pp. 746–748, 1976.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] H. McGurk 和 J. MacDonald, “听嘴唇和看声音，” *自然*，第264卷，第5588期，页码 746–748，1976年。'
- en: '[3] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, P. Nguyen, R. Pang,
    I. Lopez Moreno, Y. Wu *et al.*, “Transfer learning from speaker verification
    to multispeaker text-to-speech synthesis,” *NeruIPS*, vol. 31, 2018.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, P. Nguyen, R. Pang,
    I. Lopez Moreno, Y. Wu *等*，“从说话人验证到多说话人文本到语音合成的迁移学习，” *NeruIPS*，第31卷，2018年。'
- en: '[4] X. Ji, H. Zhou, K. Wang, W. Wu, C. C. Loy, X. Cao, and F. Xu, “Audio-driven
    emotional video portraits,” in *CVPR*, 2021, pp. 14 080–14 089.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] X. Ji, H. Zhou, K. Wang, W. Wu, C. C. Loy, X. Cao, 和 F. Xu, “音频驱动的情感视频肖像，”
    见 *CVPR*，2021年，页码 14 080–14 089。'
- en: '[5] T. Karras, T. Aila, S. Laine, A. Herva, and J. Lehtinen, “Audio-driven
    facial animation by joint end-to-end learning of pose and emotion,” *ACM TOG*,
    vol. 36, no. 4, pp. 1–12, 2017.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] T. Karras, T. Aila, S. Laine, A. Herva, 和 J. Lehtinen, “通过姿态和情感的联合端到端学习驱动的音频面部动画，”
    *ACM TOG*，第36卷，第4期，页码 1–12，2017年。'
- en: '[6] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, “Deep
    audio-visual speech recognition,” *IEEE TPAMI*, 2018.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, 和 A. Zisserman, “深度音视频语音识别，”
    *IEEE TPAMI*，2018年。'
- en: '[7] T. Afouras, J. S. Chung, and A. Zisserman, “The conversation: Deep audio-visual
    speech enhancement,” *arXiv:1804.04121*, 2018.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] T. Afouras, J. S. Chung, 和 A. Zisserman, “对话：深度视听语音增强，” *arXiv:1804.04121*，2018年。'
- en: '[8] S. Dupont and J. Luettin, “Audio-visual speech modeling for continuous
    speech recognition,” *IEEE TMM*, vol. 2, no. 3, pp. 141–151, 2000.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] S. Dupont 和 J. Luettin, “连续语音识别的视听语音建模，” *IEEE TMM*，第2卷，第3期，第141–151页，2000年。'
- en: '[9] D. Michelsanti, Z.-H. Tan, S.-X. Zhang, Y. Xu, M. Yu, D. Yu, and J. Jensen,
    “An overview of deep-learning-based audio-visual speech enhancement and separation,”
    *IEEE/ACM Transactions on Audio, Speech, and Language Processing*, 2021.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] D. Michelsanti, Z.-H. Tan, S.-X. Zhang, Y. Xu, M. Yu, D. Yu, 和 J. Jensen,
    “基于深度学习的视听语音增强与分离概述，” *IEEE/ACM语音、音频与语言处理汇刊*，2021年。'
- en: '[10] N. Tye-Murray, M. S. Sommers, and B. Spehar, “Audiovisual integration
    and lipreading abilities of older adults with normal and impaired hearing,” *Ear
    and hearing*, vol. 28, no. 5, pp. 656–668, 2007.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] N. Tye-Murray, M. S. Sommers, 和 B. Spehar, “正常与受损听力的老年人视听整合与唇读能力，” *耳与听觉*，第28卷，第5期，第656–668页，2007年。'
- en: '[11] A. Haliassos, K. Vougioukas, S. Petridis, and M. Pantic, “Lips don’t lie:
    A generalisable and robust approach to face forgery detection,” in *CVPR*, 2021,
    pp. 5039–5049.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Haliassos, K. Vougioukas, S. Petridis, 和 M. Pantic, “嘴唇不会撒谎：一种通用且稳健的面部伪造检测方法，”
    *CVPR*，2021年，第5039–5049页。'
- en: '[12] Z. Akhtar, C. Micheloni, and G. L. Foresti, “Biometric liveness detection:
    Challenges and research opportunities,” *IEEE Security & Privacy*, vol. 13, no. 5,
    pp. 63–72, 2015.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Z. Akhtar, C. Micheloni, 和 G. L. Foresti, “生物识别活体检测：挑战与研究机会，” *IEEE安全与隐私*，第13卷，第5期，第63–72页，2015年。'
- en: '[13] A. Rekik, A. Ben-Hamadou, and W. Mahdi, “Human machine interaction via
    visual speech spotting,” in *ACIVS*, 2015, pp. 566–574.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] A. Rekik, A. Ben-Hamadou, 和 W. Mahdi, “通过视觉语音定位的人机交互，” *ACIVS*，2015年，第566–574页。'
- en: '[14] K. Sun, C. Yu, W. Shi, L. Liu, and Y. Shi, “Lip-interact: Improving mobile
    device interaction with silent speech commands,” in *ACM UIST*, 2018, pp. 581–593.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] K. Sun, C. Yu, W. Shi, L. Liu, 和 Y. Shi, “Lip-interact：通过无声语音命令改善移动设备交互，”
    *ACM UIST*，2018年，第581–593页。'
- en: '[15] Y. Zhou, X. Han, E. Shechtman, J. Echevarria, E. Kalogerakis, and D. Li,
    “Makelttalk: speaker-aware talking-head animation,” *ACM TOG*, vol. 39, no. 6,
    pp. 1–15, 2020.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Zhou, X. Han, E. Shechtman, J. Echevarria, E. Kalogerakis, 和 D. Li,
    “Makelttalk：面向说话者的动画头像，” *ACM TOG*，第39卷，第6期，第1–15页，2020年。'
- en: '[16] P. Garrido, L. Valgaerts, H. Sarmadi, I. Steiner, K. Varanasi, P. Perez,
    and C. Theobalt, “Vdub: Modifying face video of actors for plausible visual alignment
    to a dubbed audio track,” in *Computer graphics forum*, vol. 34, no. 2.   Wiley
    Online Library, 2015, pp. 193–204.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] P. Garrido, L. Valgaerts, H. Sarmadi, I. Steiner, K. Varanasi, P. Perez,
    和 C. Theobalt, “Vdub：修改演员面部视频以实现与配音音轨的可信视觉对齐，” *计算机图形学论坛*，第34卷，第2期。Wiley在线图书馆，2015年，第193–204页。'
- en: '[17] L. Cappelletta and N. Harte, “Viseme definitions comparison for visual-only
    speech recognition,” in *EUSIPCO*, 2011, pp. 2109–2113.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] L. Cappelletta 和 N. Harte, “用于仅视觉语音识别的发音定义比较，” *EUSIPCO*，2011年，第2109–2113页。'
- en: '[18] G. Potamianos, C. Neti, G. Gravier, A. Garg, and A. W. Senior, “Recent
    advances in the automatic recognition of audiovisual speech,” *Proceedings of
    the IEEE*, vol. 91, no. 9, pp. 1306–1326, 2003.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] G. Potamianos, C. Neti, G. Gravier, A. Garg, 和 A. W. Senior, “自动识别视听语音的最新进展，”
    *IEEE学报*，第91卷，第9期，第1306–1326页，2003年。'
- en: '[19] K. Kirchho, “Robust speech recognition using articulatory information,”
    Ph.D. dissertation, Citeseer, 1999.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] K. Kirchho, “使用发音信息进行稳健的语音识别，” 博士论文，Citeseer，1999年。'
- en: '[20] G. Potamianos, H. P. Graf, and E. Cosatto, “An image transform approach
    for hmm based automatic lipreading,” in *ICIP*, 1998, pp. 173–177.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] G. Potamianos, H. P. Graf, 和 E. Cosatto, “基于图像变换的HMM自动唇读方法，” *ICIP*，1998年，第173–177页。'
- en: '[21] I. Matthews, T. F. Cootes, J. A. Bangham, S. Cox, and R. Harvey, “Extraction
    of visual features for lipreading,” *IEEE TPAMI*, vol. 24, no. 2, pp. 198–213,
    2002.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] I. Matthews, T. F. Cootes, J. A. Bangham, S. Cox, 和 R. Harvey, “唇读的视觉特征提取，”
    *IEEE TPAMI*，第24卷，第2期，第198–213页，2002年。'
- en: '[22] S. Deena, S. Hou, and A. Galata, “Visual speech synthesis by modelling
    coarticulation dynamics using a non-parametric switching state-space model,” in
    *ICMI Workshop*, 2010, pp. 1–8.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] S. Deena, S. Hou, 和 A. Galata, “通过建模连贯性动态的非参数切换状态空间模型进行视觉语音合成，” *ICMI研讨会*，2010年，第1–8页。'
- en: '[23] R. Anderson, B. Stenger, V. Wan, and R. Cipolla, “Expressive visual text-to-speech
    using active appearance models,” in *CVPR*, 2013, pp. 3382–3389.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] R. Anderson, B. Stenger, V. Wan, 和 R. Cipolla, “使用主动外观模型的富有表现力的视觉文本转语音，”
    *CVPR*，2013年，第3382–3389页。'
- en: '[24] T. Kim, Y. Yue, S. Taylor, and I. Matthews, “A decision tree framework
    for spatiotemporal sequence prediction,” in *ACM SIGKDD KDD*, 2015, pp. 577–586.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. Kim, Y. Yue, S. Taylor, 和 I. Matthews，“一个用于时空序列预测的决策树框架，”发表于*ACM SIGKDD
    KDD*，2015年，第577–586页。'
- en: '[25] A. Krizhevsky, I. Sutskever, and G. Hinton, “ImageNet classification with
    deep convolutional neural networks,” in *NeurIPS*, 2012, pp. 1097–1105.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Krizhevsky, I. Sutskever, 和 G. Hinton，“使用深度卷积神经网络的ImageNet分类，”发表于*NeurIPS*，2012年，第1097–1105页。'
- en: '[26] B. Fan, L. Wang, F. K. Soong, and L. Xie, “Photo-real talking head with
    deep bidirectional lstm,” in *ICASSP*, 2015, pp. 4884–4888.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] B. Fan, L. Wang, F. K. Soong, 和 L. Xie，“使用深度双向LSTM的照片级真实谈话头，”发表于*ICASSP*，2015年，第4884–4888页。'
- en: '[27] J. S. Chung and A. Zisserman, “Lip reading in the wild,” in *ACCV*, 2016,
    pp. 87–103.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] J. S. Chung 和 A. Zisserman，“野外的唇读，”发表于*ACCV*，2016年，第87–103页。'
- en: '[28] J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, “Lip reading sentences
    in the wild,” in *CVPR*, 2017, pp. 3444–3453.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. S. Chung, A. Senior, O. Vinyals, 和 A. Zisserman，“自然环境中的唇读句子，”发表于*CVPR*，2017年，第3444–3453页。'
- en: '[29] J. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep speaker recognition,”
    *arXiv:1806.05622*, 2018.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. S. Chung, A. Nagrani, 和 A. Zisserman，“Voxceleb2：深度说话人识别，”*arXiv:1806.05622*，2018年。'
- en: '[30] A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: a large-scale speaker
    identification dataset,” *arXiv:1706.08612*, 2017.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] A. Nagrani, J. S. Chung, 和 A. Zisserman，“Voxceleb：大规模说话人识别数据集，”*arXiv:1706.08612*，2017年。'
- en: '[31] S. Yang, Y. Zhang, D. Feng, M. Yang, C. Wang, J. Xiao, K. Long, S. Shan,
    and X. Chen, “Lrw-1000: A naturally-distributed large-scale benchmark for lip
    reading in the wild,” in *FG*, 2019, pp. 1–8.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Yang, Y. Zhang, D. Feng, M. Yang, C. Wang, J. Xiao, K. Long, S. Shan,
    和 X. Chen，“Lrw-1000：一个自然分布的大规模唇读基准，”发表于*FG*，2019年，第1–8页。'
- en: '[32] W. Chen, X. Tan, Y. Xia, T. Qin, Y. Wang, and T.-Y. Liu, “Duallip: A system
    for joint lip reading and generation,” in *ACM MM*, 2020, pp. 1985–1993.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] W. Chen, X. Tan, Y. Xia, T. Qin, Y. Wang, 和 T.-Y. Liu，“Duallip：一个联合唇读和生成的系统，”发表于*ACM
    MM*，2020年，第1985–1993页。'
- en: '[33] S. Ren, Y. Du, J. Lv, G. Han, and S. He, “Learning from the master: Distilling
    cross-modal advanced knowledge for lip reading,” in *CVPR*, 2021, pp. 13 325–13 333.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. Ren, Y. Du, J. Lv, G. Han, 和 S. He，“向大师学习：从跨模态高级知识中提取唇读信息，”发表于*CVPR*，2021年，第13 325–13 333页。'
- en: '[34] C.-C. Yang, W.-C. Fan, C.-F. Yang, and Y.-C. F. Wang, “Cross-modal mutual
    learning for audio-visual speech recognition and manipulation,” in *AAAI*, 2022.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] C.-C. Yang, W.-C. Fan, C.-F. Yang, 和 Y.-C. F. Wang，“音视频语音识别与操控的跨模态互学习，”发表于*AAAI*，2022年。'
- en: '[35] Y. Zhao, R. Xu, X. Wang, P. Hou, H. Tang, and M. Song, “Hearing lips:
    Improving lip reading by distilling speech recognizers,” in *AAAI*, vol. 34, no. 04,
    2020, pp. 6917–6924.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Zhao, R. Xu, X. Wang, P. Hou, H. Tang, 和 M. Song，“听唇：通过提炼语音识别器改进唇读，”发表于*AAAI*，第34卷，第04期，2020年，第6917–6924页。'
- en: '[36] D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T.-Y. Liu, and W.-Y. Ma, “Dual
    learning for machine translation,” *NeurIPS*, vol. 29, 2016.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T.-Y. Liu, 和 W.-Y. Ma，“机器翻译的双重学习，”*NeurIPS*，第29卷，2016年。'
- en: '[37] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” *NeurIPS*, vol. 27,
    2014.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络，”*NeurIPS*，第27卷，2014年。'
- en: '[38] K. Prajwal, R. Mukhopadhyay, V. P. Namboodiri, and C. Jawahar, “A lip
    sync expert is all you need for speech to lip generation in the wild,” in *ACM
    MM*, 2020, pp. 484–492.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] K. Prajwal, R. Mukhopadhyay, V. P. Namboodiri, 和 C. Jawahar，“一个唇同步专家是你在野外语音到唇生成所需的一切，”发表于*ACM
    MM*，2020年，第484–492页。'
- en: '[39] Y. Song, J. Zhu, D. Li, A. Wang, and H. Qi, “Talking face generation by
    conditional recurrent adversarial network,” in *IJCAI*, 2019, pp. 919–925.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Y. Song, J. Zhu, D. Li, A. Wang, 和 H. Qi，“通过条件递归对抗网络生成谈话面，”发表于*IJCAI*，2019年，第919–925页。'
- en: '[40] K. Vougioukas, S. Petridis, and M. Pantic, “End-to-end speech-driven facial
    animation with temporal gans,” *arXiv:1805.09313*, 2018.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] K. Vougioukas, S. Petridis, 和 M. Pantic，“基于时间生成对抗网络的端到端语音驱动面部动画，”*arXiv:1805.09313*，2018年。'
- en: '[41] S. Wang, L. Li, Y. Ding, and X. Yu, “One-shot talking face generation
    from single-speaker audio-visual correlation learning,” *arXiv:2112.02749*, 2021.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Wang, L. Li, Y. Ding, 和 X. Yu，“基于单一说话者音视频相关学习的一次性谈话面生成，”*arXiv:2112.02749*，2021年。'
- en: '[42] L. Chen, G. Cui, Z. Kou, H. Zheng, and C. Xu, “What comprises a good talking-head
    video generation?: A survey and benchmark,” *arXiv:2005.03201*, 2020.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] L. Chen, G. Cui, Z. Kou, H. Zheng, 和 C. Xu，“什么构成了一个好的谈话头视频生成？：调查和基准，”*arXiv:2005.03201*，2020年。'
- en: '[43] S. Fenghour, D. Chen, K. Guo, B. Li, and P. Xiao, “Deep learning-based
    automated lip-reading: A survey,” *IEEE Access*, 2021.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] S. Fenghour, D. Chen, K. Guo, B. Li, 和 P. Xiao，“基于深度学习的自动化唇读：调查，”*IEEE
    Access*，2021年。'
- en: '[44] A. Fernandez-Lopez and F. M. Sukno, “Survey on automatic lip-reading in
    the era of deep learning,” *Image and Vision Computing*, vol. 78, pp. 53–72, 2018.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] A. Fernandez-Lopez 和 F. M. Sukno, “深度学习时代自动唇读的综述，” *Image and Vision Computing*，第78卷，第53–72页，2018。'
- en: '[45] W. Mattheyses and W. Verhelst, “Audiovisual speech synthesis: An overview
    of the state-of-the-art,” *Speech Communication*, vol. 66, pp. 182–217, 2015.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] W. Mattheyses 和 W. Verhelst, “音视频语音合成：最先进技术的概述，” *Speech Communication*，第66卷，第182–217页，2015。'
- en: '[46] Z. Zhou, G. Zhao, X. Hong, and M. Pietikäinen, “A review of recent advances
    in visual speech decoding,” *Image and vision computing*, vol. 32, no. 9, pp.
    590–605, 2014.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Z. Zhou, G. Zhao, X. Hong, 和 M. Pietikäinen, “视觉语音解码的近期进展综述，” *Image and
    Vision Computing*，第32卷，第9期，第590–605页，2014。'
- en: '[47] R. Arandjelovic and A. Zisserman, “Objects that sound,” in *ECCV*, 2018,
    pp. 435–451.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] R. Arandjelovic 和 A. Zisserman, “会发声的物体，” 见 *ECCV*，2018，第435–451页。'
- en: '[48] S.-W. Chung, J. S. Chung, and H.-G. Kang, “Perfect match: Improved cross-modal
    embeddings for audio-visual synchronisation,” in *ICASSP*, 2019, pp. 3965–3969.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] S.-W. Chung, J. S. Chung, 和 H.-G. Kang, “完美匹配：改进的跨模态嵌入用于音视频同步，” 见 *ICASSP*，2019，第3965–3969页。'
- en: '[49] S.-W. Chung, H. G. Kang, and J. S. Chung, “Seeing voices and hearing voices:
    learning discriminative embeddings using cross-modal self-supervision,” *arXiv:2004.14326*,
    2020.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] S.-W. Chung, H. G. Kang, 和 J. S. Chung, “看见声音和听见声音：使用跨模态自监督学习辨别嵌入，” *arXiv:2004.14326*，2020。'
- en: '[50] C. Sheng, M. Pietikäinen, Q. Tian, and L. Liu, “Cross-modal self-supervised
    learning for lip reading: When contrastive learning meets adversarial training,”
    in *ACM MM*, 2021, pp. 2456–2464.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] C. Sheng, M. Pietikäinen, Q. Tian, 和 L. Liu, “用于唇读的跨模态自监督学习：当对比学习遇到对抗训练，”
    见 *ACM MM*，2021，第2456–2464页。'
- en: '[51] T. Afouras, J. S. Chung, and A. Zisserman, “Asr is all you need: Cross-modal
    distillation for lip reading,” in *ICASSP*, 2020, pp. 2143–2147.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] T. Afouras, J. S. Chung, 和 A. Zisserman, “Asr 是你所需要的一切：用于唇读的跨模态蒸馏，” 见
    *ICASSP*，2020，第2143–2147页。'
- en: '[52] P. Ma, B. Martinez, S. Petridis, and M. Pantic, “Towards practical lipreading
    with distilled and efficient models,” in *ICASSP*, 2021, pp. 7608–7612.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] P. Ma, B. Martinez, S. Petridis, 和 M. Pantic, “面向实用唇读的蒸馏和高效模型，” 见 *ICASSP*，2021，第7608–7612页。'
- en: '[53] H. Liu, Z. Chen, and B. Yang, “Lip graph assisted audio-visual speech
    recognition using bidirectional synchronous fusion.” in *INTERSPEECH*, 2020, pp.
    3520–3524.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] H. Liu, Z. Chen, 和 B. Yang, “使用双向同步融合的唇图辅助音视频语音识别。” 见 *INTERSPEECH*，2020，第3520–3524页。'
- en: '[54] C. Sheng, X. Zhu, H. Xu, M. Pietikainen, and L. Liu, “Adaptive semantic-spatio-temporal
    graph convolutional network for lip reading,” *IEEE TMM*, 2021.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] C. Sheng, X. Zhu, H. Xu, M. Pietikainen, 和 L. Liu, “用于唇读的自适应语义-时空图卷积网络，”
    *IEEE TMM*，2021。'
- en: '[55] T. Stafylakis and G. Tzimiropoulos, “Combining residual networks with
    LSTMs for lipreading,” in *Interspeech*, 2017.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] T. Stafylakis 和 G. Tzimiropoulos, “将残差网络与LSTM结合用于唇读，” 见 *Interspeech*，2017。'
- en: '[56] Y. M. Assael, B. Shillingford, S. Whiteson, and N. De Freitas, “Lipnet:
    End-to-end sentence-level lipreading,” *arXiv:1611.01599*, 2016.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. M. Assael, B. Shillingford, S. Whiteson, 和 N. De Freitas, “Lipnet：端到端的句子级唇读，”
    *arXiv:1611.01599*，2016。'
- en: '[57] T. Afouras, A. Zisserman *et al.*, “Sub-word level lip reading with visual
    attention,” *arXiv:2110.07603*, 2021.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] T. Afouras, A. Zisserman *等*，“带有视觉注意力的子词级别唇读，” *arXiv:2110.07603*，2021。'
- en: '[58] C. Wang, “Multi-grained spatio-temporal modeling for lip-reading,” *arXiv:1908.11618*,
    2019.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] C. Wang, “用于唇读的多粒度时空建模，” *arXiv:1908.11618*，2019。'
- en: '[59] B. Martinez, P. Ma, S. Petridis, and M. Pantic, “Lipreading using temporal
    convolutional networks,” in *ICASSP*, 2020, pp. 6319–6323.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] B. Martinez, P. Ma, S. Petridis, 和 M. Pantic, “使用时间卷积网络的唇读，” 见 *ICASSP*，2020，第6319–6323页。'
- en: '[60] T. Afouras, J. S. Chung, and A. Zisserman, “Deep lip reading: a comparison
    of models and an online application,” *arXiv:1806.06053*, 2018.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] T. Afouras, J. S. Chung, 和 A. Zisserman, “深度唇读：模型比较与在线应用，” *arXiv:1806.06053*，2018。'
- en: '[61] X. Zhang, F. Cheng, and S. Wang, “Spatio-temporal fusion based convolutional
    sequence learning for lip reading,” in *ICCV*, 2019, pp. 713–722.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] X. Zhang, F. Cheng, 和 S. Wang, “基于时空融合的卷积序列学习用于唇读，” 见 *ICCV*，2019，第713–722页。'
- en: '[62] K. Xu, D. Li, N. Cassimatis, and X. Wang, “Lcanet: End-to-end lipreading
    with cascaded attention-ctc,” in *FG*, 2018, pp. 548–555.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] K. Xu, D. Li, N. Cassimatis, 和 X. Wang, “Lcanet：级联注意力-CTC的端到端唇读，” 见 *FG*，2018，第548–555页。'
- en: '[63] W. Li, S. Wang, M. Lei, S. M. Siniscalchi, and C.-H. Lee, “Improving audio-visual
    speech recognition performance with cross-modal student-teacher training,” in
    *ICASSP*, 2019, pp. 6560–6564.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] W. Li, S. Wang, M. Lei, S. M. Siniscalchi, 和 C.-H. Lee, “通过跨模态师生训练提高音视频语音识别性能，”
    见 *ICASSP*，2019，第6560–6564页。'
- en: '[64] R. Kumar, J. Sotelo, K. Kumar, A. de Brébisson, and Y. Bengio, “Obamanet:
    Photo-realistic lip-sync from text,” *arXiv:1801.01442*, 2017.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] R. Kumar, J. Sotelo, K. Kumar, A. de Brébisson, 和 Y. Bengio，“Obamanet:
    从文本生成照片级真实感的唇同步”，*arXiv:1801.01442*，2017年。'
- en: '[65] L. Chen, R. K. Maddox, Z. Duan, and C. Xu, “Hierarchical cross-modal talking
    face generation with dynamic pixel-wise loss,” in *CVPR*, 2019, pp. 7832–7841.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] L. Chen, R. K. Maddox, Z. Duan, 和 C. Xu，“具有动态像素级损失的层次化跨模态谈话面生成”，发表于 *CVPR*，2019年，第7832–7841页。'
- en: '[66] S. Taylor, T. Kim, Y. Yue, M. Mahler, J. Krahe, A. G. Rodriguez, J. Hodgins,
    and I. Matthews, “A deep learning approach for generalized speech animation,”
    *ACM TOG*, vol. 36, no. 4, pp. 1–11, 2017.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S. Taylor, T. Kim, Y. Yue, M. Mahler, J. Krahe, A. G. Rodriguez, J. Hodgins,
    和 I. Matthews，“一种用于通用语音动画的深度学习方法”，*ACM TOG*，第36卷，第4期，第1–11页，2017年。'
- en: '[67] J. Thies, M. Elgharib, A. Tewari, C. Theobalt, and M. Nießner, “Neural
    voice puppetry: Audio-driven facial reenactment,” in *ECCV*, 2020, pp. 716–731.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. Thies, M. Elgharib, A. Tewari, C. Theobalt, 和 M. Nießner，“神经语音布偶戏:
    音频驱动的面部重现”，发表于 *ECCV*，2020年，第716–731页。'
- en: '[68] Z. Zhang, L. Li, Y. Ding, and C. Fan, “Flow-guided one-shot talking face
    generation with a high-resolution audio-visual dataset,” in *CVPR*, 2021, pp.
    3661–3670.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Z. Zhang, L. Li, Y. Ding, 和 C. Fan，“基于流引导的一次性谈话面生成与高分辨率音频-视觉数据集”，发表于 *CVPR*，2021年，第3661–3670页。'
- en: '[69] D. Cudeiro, T. Bolkart, C. Laidlaw, A. Ranjan, and M. J. Black, “Capture,
    learning, and synthesis of 3d speaking styles,” in *CVPR*, 2019, pp. 10 101–10 111.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] D. Cudeiro, T. Bolkart, C. Laidlaw, A. Ranjan, 和 M. J. Black，“捕捉、学习和合成三维说话风格”，发表于
    *CVPR*，2019年，第10 101–10 111页。'
- en: '[70] A. Lahiri, V. Kwatra, C. Frueh, J. Lewis, and C. Bregler, “Lipsync3d:
    Data-efficient learning of personalized 3d talking faces from video using pose
    and lighting normalization,” in *CVPR*, 2021, pp. 2755–2764.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] A. Lahiri, V. Kwatra, C. Frueh, J. Lewis, 和 C. Bregler，“Lipsync3d: 从视频中高效学习个性化三维谈话面，使用姿态和光照标准化”，发表于
    *CVPR*，2021年，第2755–2764页。'
- en: '[71] H. Zhou, Y. Liu, Z. Liu, P. Luo, and X. Wang, “Talking face generation
    by adversarially disentangled audio-visual representation,” in *AAAI*, vol. 33,
    no. 01, 2019, pp. 9299–9306.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] H. Zhou, Y. Liu, Z. Liu, P. Luo, 和 X. Wang，“通过对抗性解耦的音频-视觉表示生成谈话面”，发表于
    *AAAI*，第33卷，第01期，2019年，第9299–9306页。'
- en: '[72] Y. Sun, H. Zhou, Z. Liu, and H. Koike, “Speech2talking-face: Inferring
    and driving a face with synchronized audio-visual representation,” in *IJCAI*,
    2021.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Y. Sun, H. Zhou, Z. Liu, 和 H. Koike，“Speech2talking-face: 推断并驱动具有同步音视频表示的面部”，发表于
    *IJCAI*，2021年。'
- en: '[73] Y. Guo, K. Chen, S. Liang, Y.-J. Liu, H. Bao, and J. Zhang, “Ad-nerf:
    Audio driven neural radiance fields for talking head synthesis,” in *ICCV*, 2021,
    pp. 5784–5794.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. Guo, K. Chen, S. Liang, Y.-J. Liu, H. Bao, 和 J. Zhang，“Ad-nerf: 基于音频驱动的神经辐射场用于谈话头像合成”，发表于
    *ICCV*，2021年，第5784–5794页。'
- en: '[74] Z. Ye, M. Xia, R. Yi, J. Zhang, Y.-K. Lai, X. Huang, G. Zhang, and Y.-j.
    Liu, “Audio-driven talking face video generation with dynamic convolution kernels,”
    *IEEE TMM*, 2022.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Z. Ye, M. Xia, R. Yi, J. Zhang, Y.-K. Lai, X. Huang, G. Zhang, 和 Y.-j.
    Liu，“基于音频驱动的谈话面视频生成与动态卷积核”，*IEEE TMM*，2022年。'
- en: '[75] B. Lee, M. Hasegawa-Johnson, C. Goudeseune, S. Kamdar, S. Borys, M. Liu,
    and T. Huang, “Avicar: Audio-visual speech corpus in a car environment,” in *Eighth
    International Conference on Spoken Language Processing*, 2004.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] B. Lee, M. Hasegawa-Johnson, C. Goudeseune, S. Kamdar, S. Borys, M. Liu,
    和 T. Huang，“Avicar: 汽车环境中的音频-视觉语料库”，发表于 *第八届国际语音语言处理会议*，2004年。'
- en: '[76] http://www.isle.illinois.edu/sst/AVICAR/\#information  .'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] http://www.isle.illinois.edu/sst/AVICAR/#information。'
- en: '[77] M. Cooke, J. Barker, S. Cunningham, and X. Shao, “An audio-visual corpus
    for speech perception and automatic speech recognition,” *The Journal of the Acoustical
    Society of America*, vol. 120, no. 5, pp. 2421–2424, 2006.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] M. Cooke, J. Barker, S. Cunningham, 和 X. Shao，“用于语音感知和自动语音识别的音频-视觉语料库”，*The
    Journal of the Acoustical Society of America*，第120卷，第5期，第2421–2424页，2006年。'
- en: '[78] http://spandh.dcs.shef.ac.uk/gridcorpus/  .'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] http://spandh.dcs.shef.ac.uk/gridcorpus/。'
- en: '[79] A. Czyzewski, B. Kostek, P. Bratoszewski, J. Kotus, and M. Szykulski,
    “An audio-visual corpus for multimodal automatic speech recognition,” *Journal
    of Intelligent Information Systems*, vol. 49, no. 2, pp. 167–192, 2017.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] A. Czyzewski, B. Kostek, P. Bratoszewski, J. Kotus, 和 M. Szykulski，“用于多模态自动语音识别的音频-视觉语料库”，*Journal
    of Intelligent Information Systems*，第49卷，第2期，第167–192页，2017年。'
- en: '[80] http://www.modality-corpus.org/  .'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] http://www.modality-corpus.org/。'
- en: '[81] I. Anina, Z. Zhou, G. Zhao, and M. Pietikäinen, “Ouluvs2: A multi-view
    audiovisual database for non-rigid mouth motion analysis,” in *FG*, vol. 1, 2015,
    pp. 1–5.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] I. Anina, Z. Zhou, G. Zhao, 和 M. Pietikäinen，“Ouluvs2: 用于非刚性嘴部动作分析的多视角音视频数据库”，发表于
    *FG*，第1卷，2015年，第1–5页。'
- en: '[82] http://www.cse.oulu.fi/CMV/Downloads  .'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] http://www.cse.oulu.fi/CMV/Downloads。'
- en: '[83] Y. Mroueh, E. Marcheret, and V. Goel, “Deep multimodal learning for audio-visual
    speech recognition,” in *ICASSP*, 2015, pp. 2130–2134.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. Mroueh, E. Marcheret, 和 V. Goel, “深度多模态学习用于音视频语音识别，” 见*ICASSP*，2015年，第2130–2134页。'
- en: '[84] https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html  .'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html 。'
- en: '[85] https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html  .'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html 。'
- en: '[86] https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html  .'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html 。'
- en: '[87] S. Suwajanakorn, S. M. Seitz, and I. Kemelmacher-Shlizerman, “Synthesizing
    obama: learning lip sync from audio,” *ACM ToG*, vol. 36, no. 4, pp. 1–13, 2017.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] S. Suwajanakorn, S. M. Seitz, 和 I. Kemelmacher-Shlizerman, “合成奥巴马：从音频学习嘴型同步，”
    *ACM ToG*，第36卷，第4期，第1–13页，2017年。'
- en: '[88] https://github.com/supasorn/synthesizing_obama_network_training  .'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] https://github.com/supasorn/synthesizing_obama_network_training  。'
- en: '[89] T. Afouras, J. S. Chung, and A. Zisserman, “Lrs3-ted: a large-scale dataset
    for visual speech recognition,” *arXiv:1809.00496*, 2018.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] T. Afouras, J. S. Chung, 和 A. Zisserman, “Lrs3-ted：用于视觉语音识别的大规模数据集，” *arXiv:1809.00496*，2018年。'
- en: '[90] https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs3.html  .'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs3.html 。'
- en: '[91] https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html  .'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html 。'
- en: '[92] B. Shillingford, Y. Assael, M. W. Hoffman, T. Paine, C. Hughes, U. Prabhu,
    H. Liao, H. Sak, K. Rao, L. Bennett *et al.*, “Large-scale visual speech recognition,”
    *arXiv:1807.05162*, 2018.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] B. Shillingford, Y. Assael, M. W. Hoffman, T. Paine, C. Hughes, U. Prabhu,
    H. Liao, H. Sak, K. Rao, L. Bennett *等*，“大规模视觉语音识别，” *arXiv:1807.05162*，2018年。'
- en: '[93] http://vipl.ict.ac.cn/view_database.php?id=14  .'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] http://vipl.ict.ac.cn/view_database.php?id=14 。'
- en: '[94] A. Rossler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Nießner,
    “Faceforensics++: Learning to detect manipulated facial images,” in *ICCV*, 2019,
    pp. 1–11.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] A. Rossler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, 和 M. Nießner,
    “Faceforensics++：学习检测篡改面部图像，” 见*ICCV*，2019年，第1–11页。'
- en: '[95] https://github.com/ondyari/FaceForensics  .'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] https://github.com/ondyari/FaceForensics 。'
- en: '[96] https://voca.is.tue.mpg.de/  .'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] https://voca.is.tue.mpg.de/ 。'
- en: '[97] K. Wang, Q. Wu, L. Song, Z. Yang, W. Wu, C. Qian, R. He, Y. Qiao, and
    C. C. Loy, “Mead: A large-scale audio-visual dataset for emotional talking-face
    generation,” in *ECCV*, 2020, pp. 700–717.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] K. Wang, Q. Wu, L. Song, Z. Yang, W. Wu, C. Qian, R. He, Y. Qiao, 和 C.
    C. Loy, “Mead: 大规模音视频数据集用于情感化人脸生成，” 见*ECCV*，2020年，第700–717页。'
- en: '[98] https://wywu.github.io/projects/MEAD/MEAD.html  .'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] https://wywu.github.io/projects/MEAD/MEAD.html 。'
- en: '[99] https://github.com/MRzzm/HDTF  .'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] https://github.com/MRzzm/HDTF 。'
- en: '[100] M. Mori, K. F. MacDorman, and N. Kageki, “The uncanny valley [from the
    field],” *IEEE Robotics & Automation Magazine*, vol. 19, no. 2, pp. 98–100, 2012.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] M. Mori, K. F. MacDorman, 和 N. Kageki, “恐怖谷[来自现场]，” *IEEE Robotics &
    Automation Magazine*，第19卷，第2期，第98–100页，2012年。'
- en: '[101] S.-W. Chung, J. S. Chung, and H.-G. Kang, “Perfect match: Self-supervised
    embeddings for cross-modal retrieval,” *IEEE Journal of Selected Topics in Signal
    Processing*, vol. 14, no. 3, pp. 568–576, 2020.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S.-W. Chung, J. S. Chung, 和 H.-G. Kang, “完美匹配：自监督嵌入用于跨模态检索，” *IEEE Journal
    of Selected Topics in Signal Processing*，第14卷，第3期，第568–576页，2020年。'
- en: '[102] P. Korshunov and S. Marcel, “Deepfakes: a new threat to face recognition?
    assessment and detection,” *arXiv:1812.08685*, 2018.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] P. Korshunov 和 S. Marcel, “Deepfakes: 人脸识别的新威胁？评估与检测，” *arXiv:1812.08685*，2018年。'
- en: '[103] J. Thies, M. Zollhofer, M. Stamminger, C. Theobalt, and M. Nießner, “Face2face:
    Real-time face capture and reenactment of rgb videos,” in *CVPR*, 2016, pp. 2387–2395.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] J. Thies, M. Zollhofer, M. Stamminger, C. Theobalt, 和 M. Nießner, “Face2face：实时面部捕捉与RGB视频重演，”
    见*CVPR*，2016年，第2387–2395页。'
- en: '[104] https://github.com/MarekKowalski/FaceSwap/  .'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] https://github.com/MarekKowalski/FaceSwap/ 。'
- en: '[105] J. Thies, M. Zollhöfer, and M. Nießner, “Deferred neural rendering: Image
    synthesis using neural textures,” *ACM TOG*, vol. 38, no. 4, pp. 1–12, 2019.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] J. Thies, M. Zollhöfer, 和 M. Nießner, “延迟神经渲染：使用神经纹理的图像合成，” *ACM TOG*，第38卷，第4期，第1–12页，2019年。'
- en: '[106] A. Fernandez-Lopez, O. Martinez, and F. M. Sukno, “Towards estimating
    the upper bound of visual-speech recognition: The visual lip-reading feasibility
    database,” in *FG*, 2017, pp. 208–215.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] A. Fernandez-Lopez, O. Martinez, 和 F. M. Sukno, “朝着估计视觉语音识别上限的方向：视觉唇读可行性数据库，”
    见*FG*，2017年，第208–215页。'
- en: '[107] https://austalk.edu.au/  .'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] https://austalk.edu.au/ 。'
- en: '[108] V. Verkhodanova, A. Ronzhin, I. Kipyatkova, D. Ivanko, A. Karpov, and
    M. Železnỳ, “Havrus corpus: high-speed recordings of audio-visual russian speech,”
    in *International Conference on Speech and Computer*, 2016, pp. 338–345.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] V. Verkhodanova, A. Ronzhin, I. Kipyatkova, D. Ivanko, A. Karpov, 和 M.
    Železnỳ, “Havrus语料库：高速录制的音视频俄语语音，” 见*International Conference on Speech and Computer*，2016年，第338–345页。'
- en: '[109] E. S. Ristad and P. N. Yianilos, “Learning string-edit distance,” *IEEE
    TPAMI*, vol. 20, no. 5, pp. 522–532, 1998.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] E. S. Ristad 和 P. N. Yianilos，"学习字符串编辑距离"，*IEEE TPAMI*，卷20，第5期，页码522–532，1998年。'
- en: '[110] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic
    evaluation of machine translation,” in *ACL*, 2002, pp. 311–318.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] K. Papineni, S. Roukos, T. Ward 和 W.-J. Zhu，"Bleu: 一种自动评估机器翻译的方法"，发表于
    *ACL*，2002年，页码311–318。'
- en: '[111] S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz, “Mocogan: Decomposing
    motion and content for video generation,” in *CVPR*, 2018, pp. 1526–1535.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] S. Tulyakov, M.-Y. Liu, X. Yang 和 J. Kautz，"Mocogan: 将运动与内容分解用于视频生成"，发表于
    *CVPR*，2018年，页码1526–1535。'
- en: '[112] B. Amos, B. Ludwiczuk, M. Satyanarayanan *et al.*, “Openface: A general-purpose
    face recognition library with mobile applications,” *CMU School of Computer Science*,
    vol. 6, no. 2, p. 20, 2016.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] B. Amos, B. Ludwiczuk, M. Satyanarayanan *等*，"Openface: 一款通用面部识别库，具有移动应用"，*CMU计算机科学学院*，卷6，第2期，页码20，2016年。'
- en: '[113] E. Zakharov, A. Shysheya, E. Burkov, and V. Lempitsky, “Few-shot adversarial
    learning of realistic neural talking head models,” in *ICCV*, 2019, pp. 9459–9468.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] E. Zakharov, A. Shysheya, E. Burkov 和 V. Lempitsky，"现实主义神经说话头模型的少样本对抗学习"，发表于
    *ICCV*，2019年，页码9459–9468。'
- en: '[114] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular
    margin loss for deep face recognition,” in *CVPR*, 2019, pp. 4690–4699.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] J. Deng, J. Guo, N. Xue 和 S. Zafeiriou，"Arcface: 用于深度人脸识别的加性角度边际损失"，发表于
    *CVPR*，2019年，页码4690–4699。'
- en: '[115] N. D. Narvekar and L. J. Karam, “A no-reference perceptual image sharpness
    metric based on a cumulative probability of blur detection,” in *2009 International
    Workshop on Quality of Multimedia Experience*, 2009, pp. 87–91.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] N. D. Narvekar 和 L. J. Karam，"一种基于模糊检测累积概率的无参考感知图像清晰度度量"，发表于 *2009年国际多媒体体验质量研讨会*，2009年，页码87–91。'
- en: '[116] L. Chen, Z. Li, R. K. Maddox, Z. Duan, and C. Xu, “Lip movements generation
    at a glance,” in *ECCV*, 2018, pp. 520–535.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] L. Chen, Z. Li, R. K. Maddox, Z. Duan 和 C. Xu，"一瞥即知的唇动生成"，发表于 *ECCV*，2018年，页码520–535。'
- en: '[117] A. Koumparoulis, G. Potamianos, Y. Mroueh, and S. J. Rennie, “Exploring
    roi size in deep learning based lipreading.” in *AVSP*, 2017, pp. 64–69.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] A. Koumparoulis, G. Potamianos, Y. Mroueh 和 S. J. Rennie，"探讨深度学习中ROI尺寸的影响"，发表于
    *AVSP*，2017年，页码64–69。'
- en: '[118] C. Zhang and H. Zhao, “Lip reading using local-adjacent feature extractor
    and multi-level feature fusion,” in *Journal of Physics: Conference Series*, vol.
    1883, no. 1, 2021, p. 012083.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] C. Zhang 和 H. Zhao，"使用局部邻接特征提取器和多层特征融合的唇读"，发表于 *Journal of Physics: Conference
    Series*，卷1883，第1期，2021年，页码012083。'
- en: '[119] A. Krizhevsky, I. Sutskever, and G. Hinton, “ImageNet classification
    with deep convolutional neural networks,” in *NeurIPS*, 2012, pp. 1097–1105.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] A. Krizhevsky, I. Sutskever 和 G. Hinton，"使用深度卷积神经网络进行ImageNet分类"，发表于
    *NeurIPS*，2012年，页码1097–1105。'
- en: '[120] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, “A survey of convolutional
    neural networks: analysis, applications, and prospects,” *IEEE TNNLS*, 2021.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Z. Li, F. Liu, W. Yang, S. Peng 和 J. Zhou，"卷积神经网络综述：分析、应用及前景"，*IEEE TNNLS*，2021年。'
- en: '[121] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large
    scale image recognition,” in *ICLR*, 2015.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] K. Simonyan 和 A. Zisserman，"用于大规模图像识别的非常深的卷积网络"，发表于 *ICLR*，2015年。'
- en: '[122] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] K. He, X. Zhang, S. Ren 和 J. Sun，"深度残差学习用于图像识别"，发表于 *CVPR*，2016年，页码770–778。'
- en: '[123] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision
    applications,” in *CVPR*, 2017.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto
    和 H. Adam，"Mobilenets: 针对移动视觉应用的高效卷积神经网络"，发表于 *CVPR*，2017年。'
- en: '[124] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten, “Densely connected
    convolutional networks,” in *CVPR*, 2017.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] G. Huang, Z. Liu, K. Q. Weinberger 和 L. van der Maaten，"密集连接卷积网络"，发表于
    *CVPR*，2017年。'
- en: '[125] X. Zhang, X. Zhou, M. Lin, and J. Sun, “ShuffleNet: an extremely efficient
    convolutional neural network for mobile devices,” in *CVPR*, 2018.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] X. Zhang, X. Zhou, M. Lin 和 J. Sun，"ShuffleNet: 一种针对移动设备的极其高效的卷积神经网络"，发表于
    *CVPR*，2018年。'
- en: '[126] S. Petridis, T. Stafylakis, P. Ma, F. Cai, G. Tzimiropoulos, and M. Pantic,
    “End-to-end audiovisual speech recognition,” in *ICASSP*, 2018, pp. 6548–6552.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] S. Petridis, T. Stafylakis, P. Ma, F. Cai, G. Tzimiropoulos 和 M. Pantic，"端到端的视听语音识别"，发表于
    *ICASSP*，2018年，页码6548–6552。'
- en: '[127] D. Feng, S. Yang, S. Shan, and X. Chen, “Learn an effective lip reading
    model without pains,” *arXiv:2011.07557*, 2020.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] D. Feng, S. Yang, S. Shan 和 X. Chen，"学习一个有效的唇读模型，毫不费力"，*arXiv:2011.07557*，2020年。'
- en: '[128] J. Hu, L. Shen, and G. Sun, “Squeeze and excitation networks,” in *CVPR*,
    2018.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] J. Hu、L. Shen 和 G. Sun，“压缩和激励网络”，发表于*CVPR*，2018年。'
- en: '[129] A. Koumparoulis and G. Potamianos, “Mobilipnet: Resource-efficient deep
    learning based lipreading.” in *Interspeech*, 2019, pp. 2763–2767.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] A. Koumparoulis 和 G. Potamianos，“Mobilipnet：资源高效的基于深度学习的唇读”，发表于*Interspeech*，2019年，第2763–2767页。'
- en: '[130] V. Kazemi and J. Sullivan, “One millisecond face alignment with an ensemble
    of regression trees,” in *CVPR*, 2014, pp. 1867–1874.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] V. Kazemi 和 J. Sullivan，“用回归树集成进行一毫秒人脸对齐”，发表于*CVPR*，2014年，第1867–1874页。'
- en: '[131] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,
    C. Xu, Y. Xu *et al.*, “A survey on vision transformer,” *IEEE TPAMI*, 2022.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] K. Han、Y. Wang、H. Chen、X. Chen、J. Guo、Z. Liu、Y. Tang、A. Xiao、C. Xu、Y.
    Xu *等*，“视觉变换器调查”，发表于*IEEE TPAMI*，2022年。'
- en: '[132] X. Weng and K. Kitani, “Learning spatio temporal features with two stream
    deep 3D CNNs for lipreading,” in *BMVC*, 2019.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] X. Weng 和 K. Kitani，“利用两流深度3D CNNs学习时空特征用于唇读”，发表于*BMVC*，2019年。'
- en: '[133] M. Luo, S. Yang, S. Shan, and X. Chen, “Pseudo-convolutional policy gradient
    for sequence-to-sequence lip-reading,” in *FG*, 2020, pp. 273–280.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] M. Luo、S. Yang、S. Shan 和 X. Chen，“序列到序列唇读的伪卷积策略梯度”，发表于*FG*，2020年，第273–280页。'
- en: '[134] X. Zhao, S. Yang, S. Shan, and X. Chen, “Mutual information maximization
    for effective lip reading,” in *FG*, 2020, pp. 420–427.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] X. Zhao、S. Yang、S. Shan 和 X. Chen，“有效唇读的互信息最大化”，发表于*FG*，2020年，第420–427页。'
- en: '[135] J. Xiao, S. Yang, Y. Zhang, S. Shan, and X. Chen, “Deformation flow based
    two-stream network for lip reading,” in *FG*, 2020, pp. 364–370.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] J. Xiao、S. Yang、Y. Zhang、S. Shan 和 X. Chen，“基于变形流的双流网络用于唇读”，发表于*FG*，2020年，第364–370页。'
- en: '[136] X. Chen, J. Du, and H. Zhang, “Lipreading with densenet and resbi-lstm,”
    *Signal, Image and Video Processing*, vol. 14, no. 5, pp. 981–989, 2020.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] X. Chen、J. Du 和 H. Zhang，“利用DenseNet和ResBi-LSTM进行唇读”，*Signal, Image and
    Video Processing*，第14卷，第5期，第981–989页，2020年。'
- en: '[137] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo, “Convolutional
    lstm network: A machine learning approach for precipitation nowcasting,” *NeurIPS*,
    vol. 28, 2015.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] X. Shi、Z. Chen、H. Wang、D.-Y. Yeung、W.-K. Wong 和 W.-c. Woo，“卷积LSTM网络：用于降水现在casting的机器学习方法”，发表于*NeurIPS*，第28卷，2015年。'
- en: '[138] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NeurIPS*, 2017,
    pp. 5998–6008.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] A. Vaswani、N. Shazeer、N. Parmar、J. Uszkoreit、L. Jones、A. N. Gomez、Ł.
    Kaiser 和 I. Polosukhin，“注意力即是你所需”，发表于*NeurIPS*，2017年，第5998–6008页。'
- en: '[139] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model
    and the kinetics dataset,” in *CVPR*, 2017, pp. 6299–6308.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] J. Carreira 和 A. Zisserman，“你要去哪里，动作识别？一种新模型和动力学数据集”，发表于*CVPR*，2017年，第6299–6308页。'
- en: '[140] S. Yan, Y. Xiong, and D. Lin, “Spatial temporal graph convolutional networks
    for skeleton-based action recognition,” in *AAAI*, 2018.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] S. Yan、Y. Xiong 和 D. Lin，“基于骨架的动作识别的空间时间图卷积网络”，发表于*AAAI*，2018年。'
- en: '[141] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, “Connectionist
    temporal classification: labelling unsegmented sequence data with recurrent neural
    networks,” in *ICML*, 2006, pp. 369–376.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] A. Graves、S. Fernández、F. Gomez 和 J. Schmidhuber，“连接主义时间分类：用递归神经网络标记未分段的序列数据”，发表于*ICML*，2006年，第369–376页。'
- en: '[142] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based
    models for speech recognition,” *NeurIPS*, vol. 28, 2015.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] J. K. Chorowski、D. Bahdanau、D. Serdyuk、K. Cho 和 Y. Bengio，“基于注意力的语音识别模型”，发表于*NeurIPS*，第28卷，2015年。'
- en: '[143] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, “Listen, attend and spell,”
    *arXiv:1508.01211*, 2015.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] W. Chan、N. Jaitly、Q. V. Le 和 O. Vinyals，“听，注意和拼写”，*arXiv:1508.01211*，2015年。'
- en: '[144] A. Kannan, Y. Wu, P. Nguyen, T. N. Sainath, Z. Chen, and R. Prabhavalkar,
    “An analysis of incorporating an external language model into a sequence-to-sequence
    model,” in *ICASSP*, 2018, pp. 1–5828.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] A. Kannan、Y. Wu、P. Nguyen、T. N. Sainath、Z. Chen 和 R. Prabhavalkar，“将外部语言模型融入序列到序列模型的分析”，发表于*ICASSP*，2018年，第1–5828页。'
- en: '[145] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A
    survey,” *IJCV*, vol. 129, no. 6, pp. 1789–1819, 2021.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] J. Gou、B. Yu、S. J. Maybank 和 D. Tao，“知识蒸馏：综述”，*IJCV*，第129卷，第6期，第1789–1819页，2021年。'
- en: '[146] J. Yu, S.-X. Zhang, J. Wu, S. Ghorbani, B. Wu, S. Kang, S. Liu, X. Liu,
    H. Meng, and D. Yu, “Audio-visual recognition of overlapped speech for the lrs2
    dataset,” in *ICASSP*, 2020, pp. 6984–6988.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] J. Yu、S.-X. Zhang、J. Wu、S. Ghorbani、B. Wu、S. Kang、S. Liu、X. Liu、H. Meng
    和 D. Yu，“LRS2数据集的重叠语音的视听识别”，发表于*ICASSP*，2020年，第6984–6988页。'
- en: '[147] T. Makino, H. Liao, Y. Assael, B. Shillingford, B. Garcia, O. Braga,
    and O. Siohan, “Recurrent neural network transducer for audio-visual speech recognition,”
    in *IEEE ASRU workshop*, 2019, pp. 905–912.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] T. Makino, H. Liao, Y. Assael, B. Shillingford, B. Garcia, O. Braga 和
    O. Siohan，“用于音频-视觉语音识别的递归神经网络转导器”，发表于 *IEEE ASRU workshop*，2019年，第905–912页。'
- en: '[148] P. Ma, S. Petridis, and M. Pantic, “End-to-end audio-visual speech recognition
    with conformers,” in *ICASSP*, 2021, pp. 7613–7617.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] P. Ma, S. Petridis 和 M. Pantic，“端到端音频-视觉语音识别与变换器”，发表于 *ICASSP*，2021年，第7613–7617页。'
- en: '[149] J. S. Chung and A. Zisserman, “Out of time: automated lip sync in the
    wild,” in *ACCV*, 2016, pp. 251–263.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] J. S. Chung 和 A. Zisserman，“超越时间：自动化的野外嘴唇同步”，发表于 *ACCV*，2016年，第251–263页。'
- en: '[150] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction by learning
    an invariant mapping,” in *CVPR*, vol. 2, 2006, pp. 1735–1742.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] R. Hadsell, S. Chopra 和 Y. LeCun，“通过学习不变映射进行降维”，发表于 *CVPR*，第2卷，2006年，第1735–1742页。'
- en: '[151] B. Korbar, D. Tran, and L. Torresani, “Cooperative learning of audio
    and video models from self-supervised synchronization,” in *NeurIPS*, 2018, pp.
    7763–7774.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] B. Korbar, D. Tran 和 L. Torresani，“从自监督同步中学习音频和视频模型的协作学习”，发表于 *NeurIPS*，2018年，第7763–7774页。'
- en: '[152] A. Owens and A. A. Efros, “Audio-visual scene analysis with self-supervised
    multisensory features,” in *ECCV*, 2018, pp. 631–648.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] A. Owens 和 A. A. Efros，“使用自监督多感官特征的音频-视觉场景分析”，发表于 *ECCV*，2018年，第631–648页。'
- en: '[153] A. Senocak, T.-H. Oh, J. Kim, M.-H. Yang, and I. So Kweon, “Learning
    to localize sound source in visual scenes,” in *CVPR*, 2018, pp. 4358–4366.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] A. Senocak, T.-H. Oh, J. Kim, M.-H. Yang 和 I. So Kweon，“学习在视觉场景中定位声音源”，发表于
    *CVPR*，2018年，第4358–4366页。'
- en: '[154] P. Ma, R. Mira, S. Petridis, B. W. Schuller, and M. Pantic, “Lira: Learning
    visual speech representations from audio through self-supervision,” *arXiv:2106.09171*,
    2021.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] P. Ma, R. Mira, S. Petridis, B. W. Schuller 和 M. Pantic，“Lira：通过自监督学习音频的视觉语音表示”，*arXiv:2106.09171*，2021年。'
- en: '[155] C. Bregler, M. Covell, and M. Slaney, “Video rewrite: Driving visual
    speech with audio,” in *SIGGRAPH*, 1997, pp. 353–360.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] C. Bregler, M. Covell 和 M. Slaney，“视频重写：通过音频驱动视觉语音”，发表于 *SIGGRAPH*，1997年，第353–360页。'
- en: '[156] P. Garrido, L. Valgaerts, O. Rehmsen, T. Thormahlen, P. Perez, and C. Theobalt,
    “Automatic face reenactment,” in *CVPR*, 2014, pp. 4217–4224.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] P. Garrido, L. Valgaerts, O. Rehmsen, T. Thormahlen, P. Perez 和 C. Theobalt，“自动面部重现”，发表于
    *CVPR*，2014年，第4217–4224页。'
- en: '[157] S. Fu, R. Gutierrez-Osuna, A. Esposito, P. K. Kakumanu, and O. N. Garcia,
    “Audio/visual mapping with cross-modal hidden markov models,” *IEEE TMM*, vol. 7,
    no. 2, pp. 243–252, 2005.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] S. Fu, R. Gutierrez-Osuna, A. Esposito, P. K. Kakumanu 和 O. N. Garcia，“使用跨模态隐马尔可夫模型的音频/视觉映射”，*IEEE
    TMM*，第7卷，第2期，第243–252页，2005年。'
- en: '[158] L. Xie and Z.-Q. Liu, “Realistic mouth-synching for speech-driven talking
    face using articulatory modelling,” *IEEE TMM*, vol. 9, no. 3, pp. 500–510, 2007.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] L. Xie 和 Z.-Q. Liu，“使用发音建模的现实嘴部同步以实现语音驱动的说话面孔”，*IEEE TMM*，第9卷，第3期，第500–510页，2007年。'
- en: '[159] A. Jamaludin, J. S. Chung, and A. Zisserman, “You said that?: Synthesising
    talking faces from audio,” *IJCV*, vol. 127, no. 11, pp. 1767–1779, 2019.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] A. Jamaludin, J. S. Chung 和 A. Zisserman，“你说了什么？：从音频合成说话的面孔”，*IJCV*，第127卷，第11期，第1767–1779页，2019年。'
- en: '[160] Y. Wu and Q. Ji, “Facial landmark detection: A literature survey,” *IJCV*,
    vol. 127, no. 2, pp. 115–142, 2019.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Y. Wu 和 Q. Ji，“面部标志检测：文献综述”，*IJCV*，第127卷，第2期，第115–142页，2019年。'
- en: '[161] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
    with conditional adversarial networks,” in *CVPR*, 2017, pp. 1125–1134.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] P. Isola, J.-Y. Zhu, T. Zhou 和 A. A. Efros，“使用条件对抗网络的图像到图像翻译”，发表于 *CVPR*，2017年，第1125–1134页。'
- en: '[162] S. Sinha, S. Biswas, and B. Bhowmick, “Identity-preserving realistic
    talking face generation,” in *IJCNN*, 2020, pp. 1–10.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] S. Sinha, S. Biswas 和 B. Bhowmick，“保持身份的现实说话面孔生成”，发表于 *IJCNN*，2020年，第1–10页。'
- en: '[163] D. Das, S. Biswas, S. Sinha, and B. Bhowmick, “Speech-driven facial animation
    using cascaded gans for learning of motion and texture,” in *ECCV*, 2020, pp.
    408–424.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] D. Das, S. Biswas, S. Sinha 和 B. Bhowmick，“使用级联生成对抗网络的语音驱动面部动画以学习运动和纹理”，发表于
    *ECCV*，2020年，第408–424页。'
- en: '[164] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger,
    S. Satheesh, S. Sengupta, A. Coates *et al.*, “Deep speech: Scaling up end-to-end
    speech recognition,” *arXiv:1412.5567*, 2014.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R.
    Prenger, S. Satheesh, S. Sengupta 和 A. Coates *等*，“深度语音：扩展端到端语音识别”，*arXiv:1412.5567*，2014年。'
- en: '[165] S. A. Jalalifar, H. Hasani, and H. Aghajan, “Speech-driven facial reenactment
    using conditional generative adversarial networks,” *arXiv:1803.07461*, 2018.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] S. A. Jalalifar, H. Hasani 和 H. Aghajan，“使用条件生成对抗网络的语音驱动面部重现”，*arXiv:1803.07461*，2018年。'
- en: '[166] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”
    *arXiv:1411.1784*, 2014.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] M. Mirza 和 S. Osindero，“条件生成对抗网络”，*arXiv:1411.1784*，2014年。'
- en: '[167] S. Wang, L. Li, Y. Ding, C. Fan, and X. Yu, “Audio2head: Audio-driven
    one-shot talking-head generation with natural head motion,” *arXiv:2107.09293*,
    2021.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] S. Wang, L. Li, Y. Ding, C. Fan, 和 X. Yu，“Audio2head：基于音频驱动的单次谈话头生成与自然头部运动”，*arXiv:2107.09293*，2021年。'
- en: '[168] A. Siarohin, S. Lathuilière, S. Tulyakov, E. Ricci, and N. Sebe, “First
    order motion model for image animation,” *NeurIPS*, vol. 32, 2019.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] A. Siarohin, S. Lathuilière, S. Tulyakov, E. Ricci, 和 N. Sebe，“用于图像动画的第一阶运动模型”，*NeurIPS*，第32卷，2019年。'
- en: '[169] Y. Lu, J. Chai, and X. Cao, “Live speech portraits: real-time photorealistic
    talking-head animation,” *ACM TOG*, vol. 40, no. 6, pp. 1–17, 2021.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Y. Lu, J. Chai, 和 X. Cao，“实时语音肖像：实时逼真的谈话头动画”，*ACM TOG*，第40卷，第6期，第1–17页，2021年。'
- en: '[170] Y.-A. Chung and J. Glass, “Generative pre-training for speech with autoregressive
    predictive coding,” in *ICASSP*, 2020, pp. 3497–3501.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Y.-A. Chung 和 J. Glass，“用于语音的生成预训练与自回归预测编码”，发表于*ICASSP*，2020年，第3497–3501页。'
- en: '[171] H. X. Pham, S. Cheung, and V. Pavlovic, “Speech-driven 3d facial animation
    with implicit emotional awareness: a deep learning approach,” in *CVPR Workshops*,
    2017, pp. 80–88.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] H. X. Pham, S. Cheung, 和 V. Pavlovic，“隐式情感感知的语音驱动 3D 面部动画：一种深度学习方法”，发表于*CVPR
    Workshops*，2017年，第80–88页。'
- en: '[172] H. X. Pham, Y. Wang, and V. Pavlovic, “End-to-end learning for 3d facial
    animation from speech,” in *ICMI*, 2018, pp. 361–365.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] H. X. Pham, Y. Wang, 和 V. Pavlovic，“从语音到 3D 面部动画的端到端学习”，发表于*ICMI*，2018年，第361–365页。'
- en: '[173] A. Hussen Abdelaziz, B.-J. Theobald, J. Binder, G. Fanelli, P. Dixon,
    N. Apostoloff, T. Weise, and S. Kajareker, “Speaker-independent speech-driven
    visual speech synthesis using domain-adapted acoustic models,” in *ICMI*, 2019,
    pp. 220–225.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] A. Hussen Abdelaziz, B.-J. Theobald, J. Binder, G. Fanelli, P. Dixon,
    N. Apostoloff, T. Weise, 和 S. Kajareker，“使用领域适应的声学模型进行说话人无关的语音驱动视觉语音合成”，发表于*ICMI*，2019年，第220–225页。'
- en: '[174] R. Yi, Z. Ye, J. Zhang, H. Bao, and Y.-J. Liu, “Audio-driven talking
    face video generation with learning-based personalized head pose,” *arXiv:2002.10137*,
    2020.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] R. Yi, Z. Ye, J. Zhang, H. Bao, 和 Y.-J. Liu，“基于音频驱动的谈话面部视频生成与学习基础的个性化头部姿势”，*arXiv:2002.10137*，2020年。'
- en: '[175] X. Yao, O. Fried, K. Fatahalian, and M. Agrawala, “Iterative text-based
    editing of talking-heads using neural retargeting,” *ACM ToG*, vol. 40, no. 3,
    pp. 1–14, 2021.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] X. Yao, O. Fried, K. Fatahalian, 和 M. Agrawala，“基于神经重定向的谈话头迭代文本编辑”，*ACM
    ToG*，第40卷，第3期，第1–14页，2021年。'
- en: '[176] P. Tzirakis, A. Papaioannou, A. Lattas, M. Tarasiou, B. Schuller, and
    S. Zafeiriou, “Synthesising 3d facial motion from “in-the-wild” speech,” in *FG*,
    2020, pp. 265–272.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] P. Tzirakis, A. Papaioannou, A. Lattas, M. Tarasiou, B. Schuller, 和 S.
    Zafeiriou，“从“野外”语音合成 3D 面部运动”，发表于*FG*，2020年，第265–272页。'
- en: '[177] C. Cao, Y. Weng, S. Zhou, Y. Tong, and K. Zhou, “Facewarehouse: A 3d
    facial expression database for visual computing,” *IEEE TVCG*, vol. 20, no. 3,
    pp. 413–425, 2013.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] C. Cao, Y. Weng, S. Zhou, Y. Tong, 和 K. Zhou，“Facewarehouse：一个用于视觉计算的
    3D 面部表情数据库”，*IEEE TVCG*，第20卷，第3期，第413–425页，2013年。'
- en: '[178] H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Niessner, P. Pérez,
    C. Richardt, M. Zollhöfer, and C. Theobalt, “Deep video portraits,” *ACM TOG*,
    vol. 37, no. 4, pp. 1–14, 2018.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Niessner, P. Pérez,
    C. Richardt, M. Zollhöfer, 和 C. Theobalt，“深度视频肖像”，*ACM TOG*，第37卷，第4期，第1–14页，2018年。'
- en: '[179] H. Kim, M. Elgharib, M. Zollhöfer, H.-P. Seidel, T. Beeler, C. Richardt,
    and C. Theobalt, “Neural style-preserving visual dubbing,” *ACM TOG*, vol. 38,
    no. 6, pp. 1–13, 2019.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] H. Kim, M. Elgharib, M. Zollhöfer, H.-P. Seidel, T. Beeler, C. Richardt,
    和 C. Theobalt，“神经风格保留视觉配音”，*ACM TOG*，第38卷，第6期，第1–13页，2019年。'
- en: '[180] Y. Deng, J. Yang, S. Xu, D. Chen, Y. Jia, and X. Tong, “Accurate 3d face
    reconstruction with weakly-supervised learning: From single image to image set,”
    in *CVPR Workshops*, 2019, pp. 0–0.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Y. Deng, J. Yang, S. Xu, D. Chen, Y. Jia, 和 X. Tong，“基于弱监督学习的精确 3D 面部重建：从单张图像到图像集”，发表于*CVPR
    Workshops*，2019年，第0–0页。'
- en: '[181] L. Song, W. Wu, C. Qian, R. He, and C. C. Loy, “Everybody’s talkin’:
    Let me talk as you want,” *IEEE TIFS*, 2022.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] L. Song, W. Wu, C. Qian, R. He, 和 C. C. Loy，“每个人都在说话：让我按你想要的方式说话”，*IEEE
    TIFS*，2022年。'
- en: '[182] H. Wu, J. Jia, H. Wang, Y. Dou, C. Duan, and Q. Deng, “Imitating arbitrary
    talking style for realistic audio-driven talking face synthesis,” in *ACM MM*,
    2021, pp. 1478–1486.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] H. Wu, J. Jia, H. Wang, Y. Dou, C. Duan, 和 Q. Deng，“模仿任意谈话风格进行逼真的音频驱动谈话面部合成”，发表于*ACM
    MM*，2021年，第1478–1486页。'
- en: '[183] C. Zhang, Y. Zhao, Y. Huang, M. Zeng, S. Ni, M. Budagavi, and X. Guo,
    “Facial: Synthesizing dynamic talking face with implicit attribute learning,”
    in *ICCV*, 2021, pp. 3867–3876.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] C. Zhang, Y. Zhao, Y. Huang, M. Zeng, S. Ni, M. Budagavi, 和 X. Guo, “Facial:
    通过隐式属性学习合成动态说话面孔，”发表于 *ICCV*，2021年，第3867–3876页。'
- en: '[184] L. Li, S. Wang, Z. Zhang, Y. Ding, Y. Zheng, X. Yu, and C. Fan, “Write-a-speaker:
    Text-based emotional and rhythmic talking-head generation,” in *AAAI*, vol. 35,
    no. 3, 2021, pp. 1911–1920.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] L. Li, S. Wang, Z. Zhang, Y. Ding, Y. Zheng, X. Yu, 和 C. Fan, “Write-a-speaker:
    基于文本的情感和节奏说话头生成，”发表于 *AAAI*，第35卷，第3期，2021年，第1911–1920页。'
- en: '[185] K. Aberman, R. Wu, D. Lischinski, B. Chen, and D. Cohen-Or, “Learning
    character-agnostic motion for motion retargeting in 2d,” *arXiv:1905.01680*, 2019.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] K. Aberman, R. Wu, D. Lischinski, B. Chen, 和 D. Cohen-Or, “学习字符无关的运动以进行2D运动重定向，”
    *arXiv:1905.01680*，2019年。'
- en: '[186] J. Liu, B. Hui, K. Li, Y. Liu, Y.-K. Lai, Y. Zhang, Y. Liu, and J. Yang,
    “Geometry-guided dense perspective network for speech-driven facial animation,”
    *IEEE TVCG*, 2021.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] J. Liu, B. Hui, K. Li, Y. Liu, Y.-K. Lai, Y. Zhang, Y. Liu, 和 J. Yang,
    “基于几何引导的密集透视网络用于语音驱动的面部动画，” *IEEE TVCG*，2021年。'
- en: '[187] Y. Fan, Z. Lin, J. Saito, W. Wang, and T. Komura, “Faceformer: Speech-driven
    3d facial animation with transformers,” *arXiv:2112.05329*, 2021.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Y. Fan, Z. Lin, J. Saito, W. Wang, 和 T. Komura, “Faceformer: 基于变压器的语音驱动3D面部动画，”
    *arXiv:2112.05329*，2021年。'
- en: '[188] A. Richard, M. Zollhöfer, Y. Wen, F. De la Torre, and Y. Sheikh, “Meshtalk:
    3d face animation from speech using cross-modality disentanglement,” in *ICCV*,
    2021, pp. 1173–1182.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] A. Richard, M. Zollhöfer, Y. Wen, F. De la Torre, 和 Y. Sheikh, “Meshtalk:
    使用跨模态解耦的语音生成3D面部动画，”发表于 *ICCV*，2021年，第1173–1182页。'
- en: '[189] J. S. Chung, A. Jamaludin, and A. Zisserman, “You said that?” in *BMVC*,
    2017.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] J. S. Chung, A. Jamaludin, 和 A. Zisserman, “你刚才说了什么？”发表于 *BMVC*，2017年。'
- en: '[190] S. Si, J. Wang, X. Qu, N. Cheng, W. Wei, X. Zhu, and J. Xiao, “Speech2video:
    Cross-modal distillation for speech to video generation,” *arXiv:2107.04806*,
    2021.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] S. Si, J. Wang, X. Qu, N. Cheng, W. Wei, X. Zhu, 和 J. Xiao, “Speech2video:
    语音到视频生成的跨模态蒸馏，” *arXiv:2107.04806*，2021年。'
- en: '[191] N. Sadoughi and C. Busso, “Speech-driven expressive talking lips with
    conditional sequential generative adversarial networks,” *IEEE TAC*, vol. 12,
    no. 4, pp. 1031–1044, 2019.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] N. Sadoughi 和 C. Busso, “基于语音的表情丰富的说话嘴唇与条件序列生成对抗网络，” *IEEE TAC*，第12卷，第4期，第1031–1044页，2019年。'
- en: '[192] S. E. Eskimez, Y. Zhang, and Z. Duan, “Speech driven talking face generation
    from a single image and an emotion condition,” *IEEE TMM*, 2021.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] S. E. Eskimez, Y. Zhang, 和 Z. Duan, “从单一图像和情感条件生成语音驱动的说话面孔，” *IEEE TMM*，2021年。'
- en: '[193] H. Zhou, Y. Sun, W. Wu, C. C. Loy, X. Wang, and Z. Liu, “Pose-controllable
    talking face generation by implicitly modularized audio-visual representation,”
    in *CVPR*, 2021, pp. 4176–4186.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] H. Zhou, Y. Sun, W. Wu, C. C. Loy, X. Wang, 和 Z. Liu, “通过隐式模块化音视频表示生成可控姿势的说话面孔，”发表于
    *CVPR*，2021年，第4176–4186页。'
- en: '[194] P. KR, R. Mukhopadhyay, J. Philip, A. Jha, V. Namboodiri, and C. Jawahar,
    “Towards automatic face-to-face translation,” in *ACM MM*, 2019, pp. 1428–1436.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] P. KR, R. Mukhopadhyay, J. Philip, A. Jha, V. Namboodiri, 和 C. Jawahar,
    “走向自动面对面翻译，”发表于 *ACM MM*，2019年，第1428–1436页。'
- en: '[195] K. Vougioukas, S. Petridis, and M. Pantic, “Realistic speech-driven facial
    animation with gans,” *IJCV*, vol. 128, no. 5, pp. 1398–1413, 2020.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] K. Vougioukas, S. Petridis, 和 M. Pantic, “基于GAN的现实语音驱动面部动画，” *IJCV*，第128卷，第5期，第1398–1413页，2020年。'
- en: '[196] S. E. Eskimez, R. K. Maddox, C. Xu, and Z. Duan, “End-to-end generation
    of talking faces from noisy speech,” in *ICASSP*, 2020, pp. 1948–1952.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] S. E. Eskimez, R. K. Maddox, C. Xu, 和 Z. Duan, “从噪声语音生成端到端说话面孔，”发表于 *ICASSP*，2020年，第1948–1952页。'
- en: '[197] D. Zeng, H. Liu, H. Lin, and S. Ge, “Talking face generation with expression-tailored
    generative adversarial network,” in *ACM MM*, 2020, pp. 1716–1724.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] D. Zeng, H. Liu, H. Lin, 和 S. Ge, “具有表情定制生成对抗网络的说话面孔生成，”发表于 *ACM MM*，2020年，第1716–1724页。'
- en: '[198] S. Chen, Z. Liu, J. Liu, Z. Yan, and L. Wang, “Talking head generation
    with audio and speech related facial action units,” in *BMVC*, 2021.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] S. Chen, Z. Liu, J. Liu, Z. Yan, 和 L. Wang, “基于音频和语音相关面部动作单元的说话头生成，”发表于
    *BMVC*，2021年。'
- en: '[199] H. Zhu, H. Huang, Y. Li, A. Zheng, and R. He, “Arbitrary talking face
    generation via attentional audio-visual coherence learning,” in *IJCAI*, 2021,
    pp. 2362–2368.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] H. Zhu, H. Huang, Y. Li, A. Zheng, 和 R. He, “通过注意力音视频一致性学习生成任意说话面孔，”发表于
    *IJCAI*，2021年，第2362–2368页。'
- en: '[200] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    and R. Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,”
    in *ECCV*, 2020, pp. 405–421.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi,
    和 R. Ng, “Nerf: 通过神经辐射场表示场景以进行视图合成，”发表于 *ECCV*，2020年，第405–421页。'
