- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:47:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:47:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2203.13450] A Comparative Survey of Deep Active Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2203.13450] 深度主动学习的比较调研'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2203.13450](https://ar5iv.labs.arxiv.org/html/2203.13450)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2203.13450](https://ar5iv.labs.arxiv.org/html/2203.13450)
- en: A Comparative Survey of Deep Active Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度主动学习的比较调研
- en: Xueying Zhan
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**詹雪莹**'
- en: City University of Hong Kong
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 香港城市大学
- en: xyzhan2-c@my.cityu.edu.hk &Qingzhong Wang
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: xyzhan2-c@my.cityu.edu.hk &**Qingzhong Wang**
- en: Baidu Research
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 百度研究院
- en: wangqingzhong@baidu.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: wangqingzhong@baidu.com
- en: '&Kuan-Hao Huang'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**黄宽豪**'
- en: University of California
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 加州大学
- en: khhuang@cs.ucla.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: khhuang@cs.ucla.edu
- en: \ANDHaoyi Xiong
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \AND**熊浩毅**
- en: Baidu Research
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 百度研究院
- en: xionghaoyi@baidu.com &Dejing Dou
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: xionghaoyi@baidu.com &**Dejing Dou**
- en: Baidu Research
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 百度研究院
- en: doudejing@baidu.com &Antoni B. Chan
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: doudejing@baidu.com &**Antoni B. Chan**
- en: City University of Hong Kong
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 香港城市大学
- en: abchan@cityu.edu.hk Work was completed while the first author was at Baidu Research.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: abchan@cityu.edu.hk 该工作在第一作者在百度研究院期间完成。
- en: Abstract
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: While deep learning (DL) is data-hungry and usually relies on extensive labeled
    data to deliver good performance, Active Learning (AL) reduces labeling costs
    by selecting a small proportion of samples from unlabeled data for labeling and
    training. Therefore, Deep Active Learning (DAL) has risen as a feasible solution
    for maximizing model performance under a limited labeling cost/budget in recent
    years. Although abundant methods of DAL have been developed and various literature
    reviews conducted, the performance evaluation of DAL methods under fair comparison
    settings is not yet available. Our work intends to fill this gap. In this work,
    We construct a DAL toolkit, *$\text{DeepAL}^{+}$*, by re-implementing 19 highly-cited
    DAL methods. We survey and categorize DAL-related works and construct comparative
    experiments across frequently used datasets and DAL algorithms. Additionally,
    we explore some factors (e.g., batch size, number of epochs in the training process)
    that influence the efficacy of DAL, which provides better references for researchers
    to design their DAL experiments or carry out DAL-related applications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习（DL）需要大量数据并且通常依赖于广泛的标注数据来实现良好性能，但主动学习（AL）通过从未标注数据中选择一小部分样本进行标注和训练，从而减少标注成本。因此，深度主动学习（DAL）近年来作为在有限标注成本/预算下最大化模型性能的可行解决方案而兴起。尽管已经开发了大量的DAL方法并进行过各种文献综述，但在公平比较设置下对DAL方法的性能评估仍然不可用。我们的工作旨在填补这一空白。在这项工作中，我们通过重新实现19种高度引用的DAL方法构建了一个DAL工具包，*$\text{DeepAL}^{+}$*。我们调研并分类了与DAL相关的工作，并在常用数据集和DAL算法之间构建了比较实验。此外，我们探讨了一些影响DAL效果的因素（例如，批量大小、训练过程中的轮次），为研究人员设计DAL实验或进行DAL相关应用提供了更好的参考。
- en: 1 Introduction
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Blessed by the capacity of representation learning in an over-parameterized
    architecture, Deep Neural Networks (DNNs) have been used as significant workhorses
    in various machine learning (ML) tasks. While DNNs can work with extensive training
    datasets and deliver decent performance, collecting and annotating data to feed
    DNNs training becomes extremely expensive and time-consuming. On the other hand,
    given a large pool of unlabeled data, AL improves learning efficiency by selecting
    small subsets of samples for annotating and training [[76](#bib.bib76)]. In this
    way, a sweet spot appears at the intersection of DNNs and AL, where representation
    learning can be achieved with reduced labeling costs. Deep Active Learning (DAL)
    has been employed in various tasks, e.g., named entity recognition [[9](#bib.bib9),
    [62](#bib.bib62)], semantic parsing [[16](#bib.bib16)], object detection [[53](#bib.bib53),
    [23](#bib.bib23)], image segmentation [[6](#bib.bib6), [55](#bib.bib55)], counting
    [[81](#bib.bib81)], etc. Besides these applications, multiple unified DAL frameworks
    have been designed and perform well on various tasks [[57](#bib.bib57), [2](#bib.bib2),
    [50](#bib.bib50), [63](#bib.bib63)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 得益于在超参数化架构中的表示学习能力，深度神经网络（DNNs）已被广泛应用于各种机器学习（ML）任务。虽然DNNs可以处理大量训练数据并提供相当不错的性能，但收集和注释数据以供DNNs训练变得极其昂贵且耗时。另一方面，给定大量未标记的数据，主动学习（AL）通过选择小部分样本进行标注和训练来提高学习效率[[76](#bib.bib76)]。这样，DNNs与AL的交集出现了一个*甜蜜点*，在这里可以实现表示学习并降低标注成本。深度主动学习（DAL）已在多个任务中得到应用，例如命名实体识别[[9](#bib.bib9),
    [62](#bib.bib62)]，语义解析[[16](#bib.bib16)]，目标检测[[53](#bib.bib53), [23](#bib.bib23)]，图像分割[[6](#bib.bib6),
    [55](#bib.bib55)]，计数[[81](#bib.bib81)]等。除了这些应用外，还设计了多个统一的DAL框架，并在各种任务上表现良好[[57](#bib.bib57),
    [2](#bib.bib2), [50](#bib.bib50), [63](#bib.bib63)]。
- en: 'DAL originated from AL for classical ML tasks, which has been well studied
    in past years. The application of AL to classical ML tasks appear in a wealth
    of literature surveys [[59](#bib.bib59), [71](#bib.bib71), [19](#bib.bib19), [1](#bib.bib1),
    [17](#bib.bib17), [39](#bib.bib39), [74](#bib.bib74)] and comparative studies
    [[36](#bib.bib36), [56](#bib.bib56), [60](#bib.bib60), [68](#bib.bib68), [7](#bib.bib7),
    [65](#bib.bib65), [51](#bib.bib51), [49](#bib.bib49), [44](#bib.bib44), [80](#bib.bib80)].
    Some traditional AL methods for classical ML have been generalized to DL tasks
    [[69](#bib.bib69), [20](#bib.bib20), [3](#bib.bib3)]. Adapting AL methods to work
    well on classical ML tasks has several issues to overcome [[52](#bib.bib52)]:
    1) different from traditional AL methods that use fixed pre-processed features
    to calculate uncertainty/representativeness, in DL tasks, feature representations
    are jointly learned with DNNs. Therefore, feature representations are dynamically
    changing during DAL processes, and thus pairwise distances/similarities used by
    representativeness-based measures need to be re-computed in every stage, whereas
    for AL with classical ML tasks, these pairwise terms can be pre-computed. 2) DNNs
    are typically over-confident with their predictions and thus evaluating the uncertainty
    of unlabeled data might be unreliable. Ren et al. [[52](#bib.bib52)] conducted
    a comprehensive review of DAL, which systematically summarizes and categorizes
    $189$ existing works. Indeed it is a comprehensive study of DAL and guides new
    and experienced researchers who want to use it. However, due to the lack of experimental
    comparisons among various branches of DAL algorithms across different datasets/tasks,
    it is difficult for researchers to distinguish which DAL algorithms are suitable
    for which task. Our work aims to fill this gap.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: DAL 起源于用于经典 ML 任务的 AL，这在过去几年中已被广泛研究。AL 在经典 ML 任务中的应用出现在大量文献综述中 [[59](#bib.bib59),
    [71](#bib.bib71), [19](#bib.bib19), [1](#bib.bib1), [17](#bib.bib17), [39](#bib.bib39),
    [74](#bib.bib74)] 和比较研究中 [[36](#bib.bib36), [56](#bib.bib56), [60](#bib.bib60),
    [68](#bib.bib68), [7](#bib.bib7), [65](#bib.bib65), [51](#bib.bib51), [49](#bib.bib49),
    [44](#bib.bib44), [80](#bib.bib80)]。一些用于经典 ML 的传统 AL 方法已经推广到 DL 任务中 [[69](#bib.bib69),
    [20](#bib.bib20), [3](#bib.bib3)]。将 AL 方法调整以适用于经典 ML 任务面临几个需要克服的问题 [[52](#bib.bib52)]：1)
    与使用固定预处理特征来计算不确定性/代表性的传统 AL 方法不同，在 DL 任务中，特征表示是与 DNN 共同学习的。因此，在 DAL 过程中，特征表示会动态变化，因此基于代表性的度量所使用的成对距离/相似度需要在每个阶段重新计算，而在经典
    ML 任务中的 AL，这些成对项可以预先计算。2) DNN 通常对其预测过于自信，因此评估未标记数据的不确定性可能不可靠。Ren 等人 [[52](#bib.bib52)]
    对 DAL 进行了全面的综述，系统地总结和分类了 $189$ 项现有工作。确实这是对 DAL 的全面研究，并指导了希望使用它的新研究人员和有经验的研究人员。然而，由于缺乏不同数据集/任务中各种
    DAL 算法的实验比较，研究人员很难区分哪些 DAL 算法适合哪些任务。我们的工作旨在填补这一空白。
- en: In this work, we construct a DAL toolkit, called *$\text{DeepAL}^{+}$*, by re-implementing
    19 DAL methods surveyed in this paper. *$\text{DeepAL}^{+}$* is sequel to our
    previous work *DeepAL* [[27](#bib.bib27)]. Compared to *DeepAL*, which includes
    11 highly-cited DAL approaches prior to 2018, in *$\text{DeepAL}^{+}$*, 1) we
    upgraded and optimized some algorithms that already were implemented in *DeepAL*;
    2) we re-implemented more highly-cited DAL algorithms, most of which are proposed
    after 2018; 3) besides well-studied datasets adopted in *DeepAL* like *MNIST*
    [[14](#bib.bib14)], *CIFAR* [[38](#bib.bib38)] and *SVHN* [[45](#bib.bib45)],
    we integrated more complicated tasks in *$\text{DeepAL}^{+}$* like medical image
    analysis [[66](#bib.bib66), [31](#bib.bib31)] and object recognition with correlated
    backgrounds (containing spurious correlations) [[54](#bib.bib54)]. We conduct
    comparative experiments between a variety of DAL approaches based on *$\text{DeepAL}^{+}$*
    on multiple tasks and also explore factors of interest to researchers, such as
    the influence of batch size and the number of training epochs in each AL iteration,
    and timing-cost comparison. More descriptions of *$\text{DeepAL}^{+}$* are in
    Section B in Appendix.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们通过重新实现本文中调查的19种DAL方法，构建了一个名为*$\text{DeepAL}^{+}$*的DAL工具包。*$\text{DeepAL}^{+}$*是我们之前工作的续集*DeepAL*
    [[27](#bib.bib27)]。与包含2018年前11种高引用DAL方法的*DeepAL*相比，在*$\text{DeepAL}^{+}$*中，1）我们升级和优化了在*DeepAL*中已经实现的一些算法；2）我们重新实现了更多的高引用DAL算法，其中大多数是在2018年后提出的；3）除了在*DeepAL*中采用的如*MNIST*
    [[14](#bib.bib14)]、*CIFAR* [[38](#bib.bib38)]和*SVHN* [[45](#bib.bib45)]等研究较多的数据集外，我们在*$\text{DeepAL}^{+}$*中整合了更多复杂的任务，如医学图像分析
    [[66](#bib.bib66), [31](#bib.bib31)] 和具有相关背景（包含虚假关联）的物体识别 [[54](#bib.bib54)]。我们在多个任务上基于*$\text{DeepAL}^{+}$*对各种DAL方法进行了比较实验，并探讨了研究人员感兴趣的因素，如每次AL迭代中的批量大小和训练周期数的影响，以及时间成本比较。有关*$\text{DeepAL}^{+}$*的更多描述请参见附录B部分。
- en: We hope that our comparative study/benchmarking test brings authentic comparative
    evaluation for DAL, provides a quick look at which DAL models are more effective
    and what are the challenges and possible research directions in DAL, as well as
    offering guidelines for conducting fair comparative experiments for future DAL
    methods. More importantly, we expect that our *$\text{DeepAL}^{+}$* will contribute
    to the development of DAL since *$\text{DeepAL}^{+}$* is extensible, allowing
    easy incorporation of new basic tasks/datasets, new DAL algorithms, and new basic
    learned models. This makes the application of DAL to downstream tasks, and designing
    new DAL algorithms becomes easier. *$\text{DeepAL}^{+}$* is an ongoing process.
    We will keep expanding it by incorporating more basic tasks, models, and DAL algorithms.
    Our *$\text{DeepAL}^{+}$* is available on [https://github.com/SineZHAN/deepALplus](https://github.com/SineZHAN/deepALplus).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的比较研究/基准测试能为DAL提供真实的比较评估，快速了解哪些DAL模型更有效，以及DAL中的挑战和可能的研究方向，并为未来DAL方法的公平比较实验提供指导。更重要的是，我们期待我们的*$\text{DeepAL}^{+}$*能推动DAL的发展，因为*$\text{DeepAL}^{+}$*是可扩展的，允许轻松地整合新的基础任务/数据集、新的DAL算法和新的基础学习模型。这使得将DAL应用于下游任务以及设计新的DAL算法变得更加容易。*$\text{DeepAL}^{+}$*是一个持续的过程。我们将不断扩展它，加入更多的基础任务、模型和DAL算法。我们的*$\text{DeepAL}^{+}$*可在[https://github.com/SineZHAN/deepALplus](https://github.com/SineZHAN/deepALplus)上获取。
- en: 2 DAL Approaches
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 DAL方法
- en: This section provides an overview of highly-cited DAL methods in recent years,
    including the perspectives of querying strategies and techniques for enhancing
    DAL methods.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了近年来高引用的DAL方法，包括查询策略和增强DAL方法的技术视角。
- en: Problem Definition.
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题定义。
- en: We only discuss pool-based AL, since most DAL approaches belong to this category.
    Pool-based AL selects most informative data iteratively from a large pool of unlabeled
    $i.i.d.$ data samples until either the basic learner(s) reaches a certain level
    of performance or a fixed budget is exhausted [[11](#bib.bib11)]. We consider
    a general process of DAL, taking classification tasks as example, where other
    tasks (e.g., image segmentation) follow the common definition of their tasks domain.
    We have an initial labeled set ${\mathcal{D}}_{l}=\{(\mathbf{x}_{j},y_{j})\}_{j=1}^{M}$
    and a large unlabeled data pool ${\mathcal{D}}_{u}=\{\mathbf{x}_{i},\}_{i=1}^{N}$,
    where $M\ll N$, $y_{i}\in\{0,1\}$ is the class label of $\mathbf{x}_{i}$ for binary
    classification, or $y_{i}\in\{1,...,k\}$ for multi-class classification. In each
    iteration, we select batch of samples ${\mathcal{D}}_{q}$ with batch size $b$
    from ${\mathcal{D}}_{u}$ based on basic learned model $\mathcal{M}$ and an acquisition
    function $\alpha(\mathbf{x},\mathcal{M})$, and query their labels from the oracle.
    Data samples are selected by ${\mathcal{D}}_{q}^{*}=\arg\max\nolimits_{\mathbf{x}\in{\mathcal{D}}_{u}}^{b}\alpha(\mathbf{x},\mathcal{M})$,
    where the superscript $b$ indicates selection of the top $b$ points. ${\mathcal{D}}_{l}$
    and ${\mathcal{D}}_{u}$ are then updated, and $\mathcal{M}$ is retrained on ${\mathcal{D}}_{l}$.
    DAL terminates when the budget $Q$ is exhausted or a desired model performance
    is reached.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅讨论池式 AL，因为大多数 DAL 方法属于此类别。池式 AL 从大量未标记的 $i.i.d.$ 数据样本中迭代选择最有信息的数据，直到基本学习器达到一定性能水平或固定预算耗尽
    [[11](#bib.bib11)]。我们考虑 DAL 的一般过程，以分类任务为例，其他任务（例如图像分割）遵循其任务领域的共同定义。我们有一个初始标记集
    ${\mathcal{D}}_{l}=\{(\mathbf{x}_{j},y_{j})\}_{j=1}^{M}$ 和一个大型未标记数据池 ${\mathcal{D}}_{u}=\{\mathbf{x}_{i},\}_{i=1}^{N}$，其中
    $M\ll N$，$y_{i}\in\{0,1\}$ 是 $\mathbf{x}_{i}$ 的类别标签（用于二分类），或 $y_{i}\in\{1,...,k\}$
    用于多分类。在每次迭代中，我们从 ${\mathcal{D}}_{u}$ 中基于基本学习模型 $\mathcal{M}$ 和获取函数 $\alpha(\mathbf{x},\mathcal{M})$
    选择批量样本 ${\mathcal{D}}_{q}$，并从 oracle 查询它们的标签。数据样本由 ${\mathcal{D}}_{q}^{*}=\arg\max\nolimits_{\mathbf{x}\in{\mathcal{D}}_{u}}^{b}\alpha(\mathbf{x},\mathcal{M})$
    选择，其中上标 $b$ 表示选择前 $b$ 个点。然后更新 ${\mathcal{D}}_{l}$ 和 ${\mathcal{D}}_{u}$，并在 ${\mathcal{D}}_{l}$
    上重新训练 $\mathcal{M}$。当预算 $Q$ 耗尽或达到所需模型性能时，DAL 终止。
- en: 2.1 Querying Strategies
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 查询策略
- en: 'DAL can be categorized into 3 branches from the perspective of querying strategy:
    uncertainty-based, representativeness/diversity-based and combined strategies,
    as shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Querying Strategies ‣ 2 DAL Approaches
    ‣ A Comparative Survey of Deep Active Learning").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从查询策略的角度来看，DAL 可以分为 3 个分支：基于不确定性、基于代表性/多样性和组合策略，如图 [1](#S2.F1 "图 1 ‣ 2.1 查询策略
    ‣ 2 DAL 方法 ‣ 深度主动学习的比较调研") 所示。
- en: '![Refer to caption](img/ad1ddfea7826a1bc9afa29b1437faf4f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ad1ddfea7826a1bc9afa29b1437faf4f.png)'
- en: 'Figure 1: Categorization of DAL sampling/querying strategies.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：DAL 采样/查询策略的分类。
- en: 2.1.1 Uncertainty-based Querying Strategies
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 基于不确定性的查询策略
- en: 'Uncertainty-based DAL selects data samples with high aleatoric uncertainty
    or epistemic uncertainty, where aleatoric uncertainty refers to the natural uncertainty
    in data due to influences on data generation processes that are inherently random.
    Epistemic uncertainty comes from the modeling/learning process and is caused by
    a lack of knowledge [[59](#bib.bib59), [58](#bib.bib58), [46](#bib.bib46), [30](#bib.bib30)].
    Many uncertainty-based DAL measures are adapted from pool-based AL techniques
    for classical ML tasks. Typical methods include:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基于不确定性的 DAL 选择具有高内在不确定性或认知不确定性的数据样本，其中内在不确定性指数据生成过程中的自然随机影响导致的不确定性。认知不确定性来源于建模/学习过程，并由知识不足引起
    [[59](#bib.bib59), [58](#bib.bib58), [46](#bib.bib46), [30](#bib.bib30)]。许多基于不确定性的
    DAL 测度是从经典 ML 任务的池式 AL 技术中改编过来的。典型方法包括：
- en: '1.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Maximum Entropy (Entropy) [[61](#bib.bib61)] selects data $\mathbf{x}$ that
    maximize the predictive entropy $H_{\mathcal{M}}[y|\mathbf{x}]$: $\alpha_{\textbf{entropy}}(\mathbf{x},\mathcal{M})=H_{\mathcal{M}}[y|\mathbf{x}]=-\sum\nolimits_{k}p_{\mathcal{M}}(y=k|\mathbf{x})\log
    p_{\mathcal{M}}(y=k|\mathbf{x})$, where $p_{\mathcal{M}}(y|\mathbf{x})$ is the
    posterior label probability from the classifier $\mathcal{M}$.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最大熵（Entropy）[[61](#bib.bib61)] 选择数据 $\mathbf{x}$ 以最大化预测熵 $H_{\mathcal{M}}[y|\mathbf{x}]$：$\alpha_{\textbf{entropy}}(\mathbf{x},\mathcal{M})=H_{\mathcal{M}}[y|\mathbf{x}]=-\sum\nolimits_{k}p_{\mathcal{M}}(y=k|\mathbf{x})\log
    p_{\mathcal{M}}(y=k|\mathbf{x})$，其中 $p_{\mathcal{M}}(y|\mathbf{x})$ 是来自分类器 $\mathcal{M}$
    的后验标签概率。
- en: '2.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Margin [[45](#bib.bib45)] selects data $\mathbf{x}$ whose two most likely labels
    $(\hat{y}_{1},\hat{y}_{2})$ have smallest difference in posterior probabilities:
    $\alpha_{\textbf{margin}}(\mathbf{x},\mathcal{M})=-[p_{\mathcal{M}}(\hat{y}_{1}|\mathbf{x})-p_{\mathcal{M}}(\hat{y}_{2}|\mathbf{x})]$.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 边际（Margin）[[45](#bib.bib45)] 选择其两个最可能标签 $(\hat{y}_{1},\hat{y}_{2})$ 后验概率差异最小的数据
    $\mathbf{x}$：$\alpha_{\textbf{margin}}(\mathbf{x},\mathcal{M})=-[p_{\mathcal{M}}(\hat{y}_{1}|\mathbf{x})-p_{\mathcal{M}}(\hat{y}_{2}|\mathbf{x})]$。
- en: '3.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Least Confidence (LeastConf) [[69](#bib.bib69)] selects data $\mathbf{x}$ whose
    most likely label $\hat{y}$ has lowest posterior probability: $\alpha_{\textbf{LeastConf}}(\mathbf{x},\mathcal{M})=-p_{\mathcal{M}}(\hat{y}|\mathbf{x})$.
    A similar method is Variation Ratios (VarRatio) [[18](#bib.bib18)], which measures
    the lack of confidence like LeastConf: $\alpha_{\textbf{VarRatio}}(\mathbf{x},\mathcal{M})=1-p_{\mathcal{M}}(\hat{y}|\mathbf{x})$.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最小置信度（LeastConf）[[69](#bib.bib69)] 选择最可能标签 $\hat{y}$ 后验概率最低的数据 $\mathbf{x}$：$\alpha_{\textbf{LeastConf}}(\mathbf{x},\mathcal{M})=-p_{\mathcal{M}}(\hat{y}|\mathbf{x})$。类似的方法是变异率（VarRatio）[[18](#bib.bib18)]，它测量与
    LeastConf 类似的置信度缺乏：$\alpha_{\textbf{VarRatio}}(\mathbf{x},\mathcal{M})=1-p_{\mathcal{M}}(\hat{y}|\mathbf{x})$。
- en: '4.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Bayesian Active Learning by Disagreements (BALD) [[26](#bib.bib26), [20](#bib.bib20)]
    chooses data points that are expected to maximize the information gained from
    the model parameters $\mathbf{\omega}$, i.e. the mutual information between predictions
    and model posterior: $\alpha_{\textbf{BALD}}(\mathbf{x},\mathcal{M})=H_{\mathcal{M}}[y|\mathbf{x}]-\mathbb{E}_{p(\mathbf{\omega}|D_{l})}[H_{\mathcal{M}}[y|\mathbf{x},\mathbf{\omega}]]$.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贝叶斯主动学习中的分歧（BALD）[[26](#bib.bib26), [20](#bib.bib20)] 选择那些预计能够最大化从模型参数 $\mathbf{\omega}$
    中获得信息的数据点，即预测与模型后验之间的互信息：$\alpha_{\textbf{BALD}}(\mathbf{x},\mathcal{M})=H_{\mathcal{M}}[y|\mathbf{x}]-\mathbb{E}_{p(\mathbf{\omega}|D_{l})}[H_{\mathcal{M}}[y|\mathbf{x},\mathbf{\omega}]]$。
- en: '5.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Mean Standard Deviation (MeanSTD) [[29](#bib.bib29)] maximizes the mean standard
    deviation of the predicted probabilities over all $k$ classes: $\alpha_{\textbf{MeanSTD}}(\mathbf{x},\mathcal{M})=\frac{1}{k}\sum\nolimits_{k}\sqrt{\text{Var}_{q(\omega)}[p(y=k|\mathbf{x},\omega)]}$.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平均标准差（MeanSTD）[[29](#bib.bib29)] 最大化所有 $k$ 类别预测概率的平均标准差：$\alpha_{\textbf{MeanSTD}}(\mathbf{x},\mathcal{M})=\frac{1}{k}\sum\nolimits_{k}\sqrt{\text{Var}_{q(\omega)}[p(y=k|\mathbf{x},\omega)]}$。
- en: Inspired by recent advances in generating adversarial examples, some DAL methods
    utilize adversarial attacks to rank the uncertainty/informativeness of each unlabeled
    data sample. The DeepFool Active Learning method (AdvDeepFool) [[15](#bib.bib15)]
    queries the unlabeled samples that are closest to their adversarial attacks (DeepFool).
    Specifically, $\alpha_{\textbf{AdvDeepFool}}(\mathbf{x},\mathcal{M})=\mathbf{r}_{\mathbf{x}}$,
    where $\mathbf{r}_{\mathbf{x}}$ is the minimal perturbation that causes the changing
    of labels, e.g., for binary classification, $\mathbf{r}_{\mathbf{x}}=\mathop{\mathrm{argmin}}_{\mathbf{r},~{}\mathcal{M}(\mathbf{x})\neq\mathcal{M}(\mathbf{x}+\mathbf{r})}-\tfrac{\mathcal{M}(\mathbf{x}+\mathbf{r})}{||\nabla\mathcal{M}(\mathbf{x}+\mathbf{r})||^{2}_{2}}\nabla\mathcal{M}(\mathbf{x}+\mathbf{r})$.
    DeepFool attack can be replaced by other attack methods, e.g., Basic Interactive
    Method (BIM) [[40](#bib.bib40)], called AdvBIM.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 受生成对抗样本的最新进展启发，一些深度主动学习（DAL）方法利用对抗攻击来排名每个未标记数据样本的不确定性/信息量。DeepFool 主动学习方法（AdvDeepFool）[[15](#bib.bib15)]
    查询最接近其对抗攻击（DeepFool）的未标记样本。具体地，$\alpha_{\textbf{AdvDeepFool}}(\mathbf{x},\mathcal{M})=\mathbf{r}_{\mathbf{x}}$，其中
    $\mathbf{r}_{\mathbf{x}}$ 是导致标签改变的最小扰动，例如，对于二分类问题，$\mathbf{r}_{\mathbf{x}}=\mathop{\mathrm{argmin}}_{\mathbf{r},~{}\mathcal{M}(\mathbf{x})\neq\mathcal{M}(\mathbf{x}+\mathbf{r})}-\tfrac{\mathcal{M}(\mathbf{x}+\mathbf{r})}{||\nabla\mathcal{M}(\mathbf{x}+\mathbf{r})||^{2}_{2}}\nabla\mathcal{M}(\mathbf{x}+\mathbf{r})$。DeepFool
    攻击可以被其他攻击方法替代，例如，基本交互方法（BIM）[[40](#bib.bib40)]，称为 AdvBIM。
- en: 'Generative Adversarial Active Learning (GAAL) [[83](#bib.bib83)] synthesizes
    queries via Generative Adversarial Networks (GANs). In contrast to regular AL
    that selects points from the unlabeled data pool, GAAL generates images from GAN
    for querying human annotators. However, the generated data very close to the classifier
    decision boundary may be meaningless, and even human annotators could not distinguish
    its category. An improved approach called Bayesian Generative Active Deep Learning
    (BGADL) [[67](#bib.bib67)] combines active learning with data augmentation. BGADL
    utilizes typical Bayesian DAL approaches for its acquisition function (e.g., $\alpha_{\textbf{BALD}}$)
    and then trains a VAE-ACGAN to generate synthetic data samples to enlarge the
    training set. Other practical uncertainty-based measures include i) utilizing
    gradient: Wang et al. [[72](#bib.bib72)] found that gradient norm can effectively
    guide unlabeled data selection; that is, selecting unlabeled data of higher gradient
    norm can reduce the upper bound of the test loss. Another work that utilizes gradient
    is Batch Active learning by Diverse Gradient Embeddings (BADGE)[[2](#bib.bib2)]
    measures uncertainty as the gradient magnitude with respect to parameters in the
    output layer since DNNs are optimized using gradient-based methods like SGD. ii)
    Loss Prediction Loss (LPL) [[78](#bib.bib78)] uses a loss prediction strategy
    by attaching a small parametric module that is trained to predict the loss of
    unlabeled inputs with respect to the target model, by minimizing the loss prediction
    loss between predicted loss and target loss. LPL picks the top $b$ data samples
    with the highest predicted loss.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗主动学习（GAAL）[[83](#bib.bib83)]通过生成对抗网络（GANs）合成查询。与从未标记的数据池中选择点的常规主动学习不同，GAAL通过GAN生成图像以便查询人工标注者。然而，生成的数据如果非常接近分类器的决策边界可能没有意义，甚至人工标注者也可能无法区分其类别。一种改进的方法称为贝叶斯生成主动深度学习（BGADL）[[67](#bib.bib67)]，将主动学习与数据增强结合。BGADL利用典型的贝叶斯DAL方法来处理其获取函数（例如，$\alpha_{\textbf{BALD}}$），然后训练一个VAE-ACGAN生成合成数据样本以扩大训练集。其他实际的不确定性度量方法包括：i）利用梯度：Wang等人[[72](#bib.bib72)]发现梯度范数可以有效地指导未标记数据的选择；即，选择具有较高梯度范数的未标记数据可以降低测试损失的上界。另一项利用梯度的工作是基于多样化梯度嵌入的批量主动学习（BADGE）[[2](#bib.bib2)]，将不确定性度量为相对于输出层参数的梯度大小，因为深度神经网络是通过如SGD等基于梯度的方法进行优化的。ii）损失预测损失（LPL）[[78](#bib.bib78)]使用损失预测策略，通过附加一个小的参数模块来训练预测未标记输入相对于目标模型的损失，通过最小化预测损失与目标损失之间的损失预测损失来进行训练。LPL选择预测损失最高的前$b$个数据样本。
- en: 2.1.2 Representative/Diversity-based Querying Strategies
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 代表性/基于多样性的查询策略
- en: Representative/diversity-based strategies select batches of samples representative
    of the unlabeled set and are based on the intuition that the selected representative
    examples, once labeled, can act as a surrogate for the entire dataset [[2](#bib.bib2)].
    Clustering methods are widely used in representative-based strategies. A typical
    method is KMeans, which selects centroids by iteratively sampling points in proportion
    to their squared distances from the nearest previously selected centroid. Another
    widely adopted approach [[21](#bib.bib21), [57](#bib.bib57)] selects a batch of
    representative points based on a core set, which is a sub-sample of a dataset
    that can be used as a proxy for the full set. CoreSet is measured in the penultimate
    layer space $h(\mathbf{x})$ of the current model. Firstly, given ${\mathcal{D}}_{l}$,
    an example $\mathbf{x}_{u}$ is selected with the greatest distance to its nearest
    neighbor in the hidden space $u=\arg\max\nolimits_{\mathbf{x}_{i}\in{\mathcal{D}}_{u}}\min\nolimits_{\mathbf{x}_{j}\in{\mathcal{D}}_{l}}\Delta(h(\mathbf{x}_{i},\mathbf{x}_{j}))$.
    Sampling is then repeated until batch size $b$ is reached. In another method,
    Cluster-Margin [[12](#bib.bib12)] selects a diverse set of examples on which the
    model is least confident. It first runs hierarchical agglomerate clustering with
    average-linkage as pre-processing, and then selects an unlabeled subset with lowest
    margin scores (Margin), which is then filtered down to a diverse set with $b$
    samples. In contrast to CoreSet, Cluster-Margin only runs clustering once as pre-processing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 代表性/多样性策略选择代表未标记集的样本批次，基于这样的直觉：一旦标记，所选的代表性示例可以作为整个数据集的替代品 [[2](#bib.bib2)]。聚类方法在基于代表性的策略中被广泛使用。一个典型的方法是KMeans，它通过按比例抽样距离最近的已选质心的平方距离来选择质心。另一种广泛采用的方法
    [[21](#bib.bib21), [57](#bib.bib57)] 基于核心集选择一批代表性点，核心集是数据集的一个子样本，可以用作全体数据集的代理。CoreSet在当前模型的倒数第二层空间
    $h(\mathbf{x})$ 中进行测量。首先，给定 ${\mathcal{D}}_{l}$，选择与其在隐藏空间中最近邻的距离最大的示例 $\mathbf{x}_{u}$，其中
    $u=\arg\max\nolimits_{\mathbf{x}_{i}\in{\mathcal{D}}_{u}}\min\nolimits_{\mathbf{x}_{j}\in{\mathcal{D}}_{l}}\Delta(h(\mathbf{x}_{i},\mathbf{x}_{j}))$。然后重复抽样直到达到批次大小
    $b$。另一种方法，Cluster-Margin [[12](#bib.bib12)] 选择模型最不自信的一组多样化示例。它首先进行层次聚合聚类，使用平均链接作为预处理，然后选择具有最低边际分数（Margin）的未标记子集，再筛选为包含
    $b$ 个样本的多样化集合。与 CoreSet 相比，Cluster-Margin 仅在预处理中运行一次聚类。
- en: Point Processes are also adopted in representative-based DAL, e.g., Active-DPP
    [[4](#bib.bib4)]. A determinantal point process (DPP) captures diversity by constructing
    a pair-wise (dis)similarity matrix and calculating its determinant. BADGE also
    utilizes DPPs as a representative measure. Discriminative AL (DiscAL) [[22](#bib.bib22)]
    is a representative measure that, reminiscent of GANs, attempts to fool a discriminator
    that tries to distinguish between data coming from two different distributions
    (unlabeled/labeled). Variational Adversarial AL (VAAL) [[64](#bib.bib64)] learns
    a distribution of labeled data in latent space using a VAE and an adversarial
    network trained to discriminate between unlabeled and labeled data. The network
    is optimized using both reconstruction and adversarial losses. $\alpha_{\textbf{VAAL}}$
    is formed with the discriminator that estimates the probability that the data
    comes from the unlabeled data. Wasserstein Adversarial AL (WAAL) [[63](#bib.bib63)]
    searches the diverse unlabeled batch that also has larger diversity than the labeled
    samples through adversarial training by $\mathcal{H}$-divergence.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 点过程也被应用于基于代表性的 DAL，例如 Active-DPP [[4](#bib.bib4)]。确定性点过程（DPP）通过构建成对的（非）相似性矩阵并计算其行列式来捕捉多样性。BADGE
    也利用 DPP 作为代表性度量。判别性 AL（DiscAL） [[22](#bib.bib22)] 是一种代表性度量，类似于 GANs，试图欺骗一个试图区分来自两个不同分布（未标记/标记）的数据的判别器。变分对抗
    AL（VAAL） [[64](#bib.bib64)] 使用 VAE 和训练来区分未标记数据和标记数据的对抗网络来学习标签数据在潜在空间中的分布。该网络通过重建和对抗损失进行优化。$\alpha_{\textbf{VAAL}}$
    是通过估计数据来自未标记数据的概率来形成的判别器。Wasserstein 对抗 AL（WAAL） [[63](#bib.bib63)] 通过 $\mathcal{H}$-散度在对抗训练中搜索多样化的未标记批次，这些批次也具有比标记样本更大的多样性。
- en: 2.1.3 Combined Querying Strategies
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 组合查询策略
- en: Due to the demand for larger batch size (representative/diversity) and more
    precise decision boundaries for higher model performance (uncertainty) in DAL,
    combined strategies have become the dominant approaches to DAL. It aims to achieve
    a trade-off between uncertainty and representativeness/diversity in query selection.
    We mainly discuss the optimization methods with respect to multiple objectives
    (uncertainty, diversity, etc.) in this paper, including weighted-sum and two-stage
    optimization.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 DAL 对于更大的批量大小（代表性/多样性）和更精确的决策边界（不确定性）的需求，组合策略已成为 DAL 的主流方法。其目标是实现查询选择中不确定性和代表性/多样性之间的权衡。我们在本文中主要讨论与多个目标（不确定性、多样性等）相关的优化方法，包括加权和优化和两阶段优化。
- en: 'Weighted-sum optimization is both simple and flexible, where the objective
    functions are summed up with weight $\beta$: $\alpha_{\textbf{weighted-sum}}=\alpha_{\textbf{uncertainty}}+\beta\alpha_{\textbf{representative}}$.
    However, two factors limit its usage in Combined DAL: 1) it introduces extra hyper-parameter
    $\beta$ for tuning; 2) unlike uncertainty-based measure that provide a single
    score per sample, representativeness is usually expressed in matrix form, which
    is not straightforward to convert into a single per-sample score. A example of
    weighted-sum optimization is Exploitation-Exploration [[77](#bib.bib77)] selects
    samples that are most uncertain and least redundant (exploitation), as well as
    most diverse (exploration). Specifically, in the exploitation step, $\alpha_{\textbf{exploitation}}=\alpha_{\textbf{entropy}}({\mathcal{D}}_{q},\mathcal{M})-\tfrac{\beta}{|{\mathcal{D}}_{q}|}\alpha_{\textbf{similarity}}({\mathcal{D}}_{q})$.
    Using DPPs is a natural way to balance uncertainty score and pairwise diversity
    well without introducing additional hyper-parameters [[4](#bib.bib4), [2](#bib.bib2),
    [79](#bib.bib79)]. However, sampling from DPPs in DAL is not trivial since DPPs
    have a time complexity of $O(N^{3})$.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 加权和优化既简单又灵活，其中目标函数的和用权重 $\beta$ 加权：$\alpha_{\textbf{weighted-sum}}=\alpha_{\textbf{uncertainty}}+\beta\alpha_{\textbf{representative}}$。然而，两个因素限制了它在组合
    DAL 中的使用：1）它引入了额外的超参数 $\beta$ 进行调整；2）与基于不确定性的度量提供每个样本的单一得分不同，代表性通常以矩阵形式表达，这不容易转换为单个每样本得分。加权和优化的一个例子是开发-探索
    [[77](#bib.bib77)] 选择最不确定且冗余最少的样本（开发），以及最具多样性的样本（探索）。具体来说，在开发步骤中，$\alpha_{\textbf{exploitation}}=\alpha_{\textbf{entropy}}({\mathcal{D}}_{q},\mathcal{M})-\tfrac{\beta}{|{\mathcal{D}}_{q}|}\alpha_{\textbf{similarity}}({\mathcal{D}}_{q})$。使用
    DPPs 是在不引入额外超参数的情况下很好地平衡不确定性得分和成对多样性的方法 [[4](#bib.bib4), [2](#bib.bib2), [79](#bib.bib79)]。然而，DAL
    中的 DPPs 采样并不简单，因为 DPPs 的时间复杂度为 $O(N^{3})$。
- en: Two-stage (multi-stage) optimization is a popular combined strategy, Each stage
    refines the previous stage’s selections using different criteria. E.g., stage
    1 selects an informative subset with a size larger than $b$, and then stage 2
    selects $b$ samples with maximum diversity. WAAL uses two stage optimization for
    implementing discriminative learning via training a DNN for discriminative features
    in stage 1, and making batch selections in stage 2 [[63](#bib.bib63)]. BADGE computes
    gradient embeddings for each unlabeled data samples in stage 1, then clusters
    by KMeans++ in stage 2 [[2](#bib.bib2)]. Diverse mini-Batch Active Learning (DBAL)
    [[82](#bib.bib82)] first pre-filters unlabeled data pool to the top $\rho b$ most
    informative/uncertain examples ($\rho$ is pre-filter factor), then clusters these
    samples to $b$ clusters with (weighted) KMeans and selects $b$ samples closest
    to the cluster centers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 两阶段（多阶段）优化是一种流行的组合策略，每个阶段都使用不同的标准来优化前一个阶段的选择。例如，阶段 1 选择一个比 $b$ 更大的信息子集，然后阶段
    2 选择具有最大多样性的 $b$ 个样本。WAAL 使用两阶段优化，通过在阶段 1 训练 DNN 以实现判别学习，并在阶段 2 进行批量选择 [[63](#bib.bib63)]。BADGE
    在阶段 1 为每个未标记的数据样本计算梯度嵌入，然后在阶段 2 使用 KMeans++ 进行聚类 [[2](#bib.bib2)]。多样化小批量主动学习（DBAL）
    [[82](#bib.bib82)] 首先将未标记的数据池预过滤到前 $\rho b$ 个最具信息量/不确定性的样本（$\rho$ 是预过滤因子），然后用（加权）KMeans
    将这些样本聚成 $b$ 个簇，并选择最接近簇中心的 $b$ 个样本。
- en: 2.2 Enhancing of DAL Methods
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 DAL 方法的增强
- en: 'In Section [2.1](#S2.SS1 "2.1 Querying Strategies ‣ 2 DAL Approaches ‣ A Comparative
    Survey of Deep Active Learning"), many highly-cited DAL methods have designed
    acquisition functions, e.g., Entropy, CoreSet and BADGE. These methods are easily
    adapted to various tasks since they only involve the data selection process, not
    the training process of the backbone. However, how well these DAL methods can
    perform is limited, e.g., one might not exceed the performance of training on
    full data. Some DAL models are proposed for enhancing DAL methods that can break
    the limitation, which can be categorized into two branches: data and model aspect.
    The data aspect includes data augmentation and pseudo labeling, while the model
    aspect includes attaching extra networks, modifying loss functions, and ensemble.
    Due to limited space, we exclude related joint tasks that modify DAL methods like
    semi-/ self-/un-/supervised, transfer, or reinforcement learning.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[2.1](#S2.SS1 "2.1 查询策略 ‣ 2 DAL 方法 ‣ 深度主动学习的比较调查")节中，许多被高度引用的DAL方法设计了获取函数，例如**熵**、**CoreSet**和**BADGE**。这些方法由于仅涉及数据选择过程，而不是主干网络的训练过程，因此容易适应各种任务。然而，这些DAL方法的性能表现有限，例如，某些方法可能无法超过在全数据上训练的表现。一些DAL模型被提出用于增强DAL方法，突破这些限制，这些方法可以分为两个方面：数据方面和模型方面。数据方面包括数据增强和伪标签，而模型方面包括附加额外网络、修改损失函数和集成。由于篇幅限制，我们排除了那些修改DAL方法的相关联合任务，如半监督/自监督/无监督/监督学习、迁移学习或强化学习。
- en: Data aspect.
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据方面。
- en: Pseudo-labeling utilizes large-scale unlabeled data for training. Cost-Effective
    AL (CEAL) [[70](#bib.bib70)] assigns high-confident (low entropy $H_{\mathcal{M}}[y|\mathbf{x}]$)
    pseudo labels predicted by $\mathcal{M}$ for training in the next iteration. However,
    this introduces new hyperparameters to threshold the prediction confidence, which,
    if badly tuned, can corrupt the training set with wrong labels [[15](#bib.bib15)].
    Data augmentation uses labeled samples for enlarging the training set. However,
    data augmentation might waste computational resources because it indiscriminately
    generates samples that are not guaranteed to be informative. AdvDeepFool adds
    adversarial samples to the training set [[15](#bib.bib15)], while BGADL employs
    ACGAN and Bayesian Data Augmentation for producing new artificial samples that
    are as informative as the selected samples [[67](#bib.bib67)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 伪标签利用大规模未标记数据进行训练。成本效益主动学习（CEAL）[[70](#bib.bib70)]为下一个迭代的训练分配由$\mathcal{M}$预测的高置信度（低熵$H_{\mathcal{M}}[y|\mathbf{x}]$）伪标签。然而，这引入了新的超参数来阈值预测置信度，如果调整不好，可能会用错误标签破坏训练集[[15](#bib.bib15)]。数据增强使用标记样本来扩大训练集。然而，数据增强可能浪费计算资源，因为它无差别地生成样本，这些样本不一定具有信息性。AdvDeepFool将对抗样本添加到训练集中[[15](#bib.bib15)]，而BGADL则采用ACGAN和贝叶斯数据增强生成与选定样本一样有信息量的新人工样本[[67](#bib.bib67)]。
- en: Model aspect.
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型方面。
- en: Some researchers utilize extra modules to improve target model performance and
    make selections in DAL. For instance, LPL jointly learns the target backbone model
    and loss prediction model, which can predict when the target model is likely to
    produce a wrong prediction. Choi et al. [[10](#bib.bib10)] constructs mixture
    density networks to estimate a probability distribution for each localization
    and classification head’s output for the object detection task. Revising the loss
    function of the target model is also promising. WAAL adopts min-max loss by leveraging
    the unlabeled data for better distinguishing labeled and unlabeled samples [[63](#bib.bib63)].
    Another approach is ensemble learning. DNNs use a softmax layer to obtain the
    label’s posterior probability and tend to be overconfident when calculating the
    uncertainty. To increase uncertainty, Gal et al. [[20](#bib.bib20)] leverages
    Monte-Carlo (MC) Dropout, where uncertainty in the weights $\omega$ induces prediction
    uncertainty by marginalizing over the approximate posterior using MC integration.
    It can be viewed as an ensemble of models sampled with dropouts. Beluch et al.
    [[3](#bib.bib3)] found that ensembles of multiple classifiers perform better than
    MC Dropout for calculating uncertainty scores.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员利用额外的模块来提高目标模型的性能，并在DAL中进行选择。例如，LPL联合学习目标骨干模型和损失预测模型，这可以预测目标模型何时可能产生错误的预测。Choi等人[[10](#bib.bib10)]构建了混合密度网络来估计每个定位和分类头输出的概率分布，以用于目标检测任务。修订目标模型的损失函数也是一种有前途的方法。WAAL通过利用未标记的数据来采用最小-最大损失，以更好地区分标记样本和未标记样本[[63](#bib.bib63)]。另一种方法是集成学习。DNN使用softmax层来获得标签的后验概率，并在计算不确定性时倾向于过度自信。为了增加不确定性，Gal等人[[20](#bib.bib20)]利用Monte-Carlo（MC）Dropout，其中权重$\omega$中的不确定性通过使用MC积分对近似后验进行边际化来引起预测不确定性。这可以视为一个通过dropout采样的模型集合。Beluch等人[[3](#bib.bib3)]发现，多个分类器的集成在计算不确定性分数时表现优于MC
    Dropout。
- en: 3 Comparative Experiments of DAL
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 DAL的比较实验
- en: We conduct comparisons on $19$ methods across $10$ public available datasets,
    in which these datasets are selected with reference of [[52](#bib.bib52)] (see
    Table 2 in [[52](#bib.bib52)]) and highly-cited DAL papers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在$19$种方法上进行比较，这些方法在$10$个公开数据集上进行，其中这些数据集的选择参考了[[52](#bib.bib52)]（见[[52](#bib.bib52)]中的表2）和引用率高的DAL论文。
- en: 3.1 Experimental Settings
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: Datasets.
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集。
- en: 'Considering some DAL approaches currently only support computer vision tasks
    like VAAL, for consistency and fairness of our experiments, we adopt the image
    classification tasks, similar to most DAL papers. We use: *MNIST* [[14](#bib.bib14)],
    *FashionMNIST* [[75](#bib.bib75)], *EMNIST* [[13](#bib.bib13)], *SVHN* [[45](#bib.bib45)],
    *CIFAR10*, *CIFAR100* [[38](#bib.bib38)] and *TinyImageNet* [[41](#bib.bib41)].
    We construct an imbalanced dataset based on *CIFAR10*, called *CIFAR10-imb*, which
    sub-samples the training set with ratios of 1:2:$\cdots$:10 for classes 0 through
    9\. We also consider medical imaging analysis tasks, including Breast Cancer Histopathological
    Image Classification (*BreakHis*) [[66](#bib.bib66)] and Chest X-Ray Pneuomonia
    classification (Pneumonia-MNIST) [[31](#bib.bib31)]. Additionally, we adopted
    an object recognition dataset with correlated backgrounds (*Waterbird*) [[54](#bib.bib54),
    [35](#bib.bib35)]. This dataset contains waterbird and landbird classes, which
    are manually mixed to water and land backgrounds. It is challenging since DNNs
    might spuriously rely on the background instead of learning to recognize the object
    semantics.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到一些DAL方法目前仅支持计算机视觉任务，如VAAL，为了保持实验的一致性和公平性，我们采用了与大多数DAL论文类似的图像分类任务。我们使用：*MNIST*
    [[14](#bib.bib14)]、*FashionMNIST* [[75](#bib.bib75)]、*EMNIST* [[13](#bib.bib13)]、*SVHN*
    [[45](#bib.bib45)]、*CIFAR10*、*CIFAR100* [[38](#bib.bib38)]和*TinyImageNet* [[41](#bib.bib41)]。我们基于*CIFAR10*构建了一个不平衡的数据集，称为*CIFAR10-imb*，它对训练集进行子采样，类别0至9的比例为1:2:$\cdots$:10。我们还考虑了医学影像分析任务，包括乳腺癌组织病理图像分类（*BreakHis*）[[66](#bib.bib66)]和胸部X光肺炎分类（Pneumonia-MNIST）[[31](#bib.bib31)]。此外，我们还采用了一个具有相关背景的目标识别数据集（*Waterbird*）[[54](#bib.bib54),
    [35](#bib.bib35)]。该数据集包含水鸟和陆鸟类别，这些类别被手动混合到水域和陆地背景中。这具有挑战性，因为DNN可能会虚假地依赖背景，而不是学习识别对象语义。
- en: DAL methods.
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DAL方法。
- en: We test Random Sampling (Random), Entropy, Margin, LeastConf and their MC Dropout
    versions [[3](#bib.bib3)] (denoted as EntropyD, MarginD, LeastConfD, respectively),
    BALD, MeanSTD, VarRatio, CEAL(Entropy), KMeans, the greedy version of CoreSet
    (denoted as KCenter), BADGE, AdversarialBIM, WAAL, VAAL, and LPL. For KMeans,
    considering that we need to cluster large amounts of data, the original KMeans
    implementation based on the scikit-learn library [[48](#bib.bib48)] will be too
    time-consuming on large-scale unlabeled data pools. Thus, to save the time cost
    and let our *$\text{DeepAL}^{+}$* be more adaptable to DL tasks, we implemented
    KMeans with GPU (KMeans (GPU)) based on the faiss library [[28](#bib.bib28)].
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了随机采样（Random）、熵（Entropy）、边际（Margin）、LeastConf及其MC Dropout版本 [[3](#bib.bib3)]（分别表示为EntropyD、MarginD、LeastConfD）、BALD、MeanSTD、VarRatio、CEAL（Entropy）、KMeans、CoreSet的贪婪版本（表示为KCenter）、BADGE、对抗BIM、WAAL、VAAL和LPL。对于KMeans，考虑到我们需要对大量数据进行聚类，基于scikit-learn库的原始KMeans实现[[48](#bib.bib48)]在大规模未标记数据池上会耗时过长。因此，为了节省时间成本并使我们的*$\text{DeepAL}^{+}$*
    更适应DL任务，我们基于faiss库[[28](#bib.bib28)]实现了GPU版本的KMeans（KMeans（GPU））。
- en: For all AL methods, we employed ResNet18 (w/o pre-training) [[24](#bib.bib24)]
    as the basic learner. For a fair comparison, consistent experimental settings
    of the basic classifier are used across all DAL methods. We run these experiments
    using *$\text{DeepAL}^{+}$* toolkit.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有AL方法，我们采用了ResNet18（无预训练）[[24](#bib.bib24)]作为基本学习器。为了公平比较，在所有DAL方法中使用了一致的基本分类器实验设置。我们使用*$\text{DeepAL}^{+}$*工具包运行这些实验。
- en: Experimental protocol.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实验协议。
- en: We repeat each experiment for $3$ trials with random splits of the initial labeled
    and unlabeled pool (using the same random seed) and report average testing performance.
    For evaluation metrics, for brevity, we report *overall performance* using *area
    under the budget curve (AUBC)* [[80](#bib.bib80), [79](#bib.bib79)], where the
    performance-budget curve is generated by evaluating the DAL method for varying
    budgets (e.g., accuracy vs. budget). Higher AUBC values indicate better overall
    performance. We also report the final accuracy (F-acc), which is the accuracy
    after the budget $Q$ is exhausted. More details of experimental settings (i.e.,
    datasets, implementations) are in Section D in Appendix.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个实验重复进行$3$次试验，随机分割初始标记和未标记池（使用相同的随机种子），并报告平均测试性能。为了简洁起见，我们报告了*总体性能*，使用*预算曲线下面积（AUBC）*
    [[80](#bib.bib80), [79](#bib.bib79)]，其中性能-预算曲线是通过评估DAL方法在不同预算下生成的（例如，准确率与预算）。较高的AUBC值表示更好的总体性能。我们还报告了最终准确率（F-acc），即预算$Q$耗尽后的准确率。实验设置的更多细节（即数据集、实现）见附录的D节。
- en: '|  |  | *MNIST* | *FashionMNIST* | *EMNIST* | *SVHN* | *PneumoniaMNIST* |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |  | *MNIST* | *FashionMNIST* | *EMNIST* | *SVHN* | *PneumoniaMNIST* |'
- en: '|  | Model | AUBC | F-acc | AUBC | F-acc | AUBC | F-acc | AUBC | F-acc | AUBC
    | F-acc |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | AUBC | F-acc | AUBC | F-acc | AUBC | F-acc | AUBC | F-acc | AUBC
    | F-acc |'
- en: '|  | Full | $-$ | $0.9916$ | $-$ | $0.9120$ | $-$ | $0.8684$ | $-$ | $0.9190$
    | $-$ | $0.9039$ |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整 | $-$ | $0.9916$ | $-$ | $0.9120$ | $-$ | $0.8684$ | $-$ | $0.9190$
    | $-$ | $0.9039$ |'
- en: '|  | Random | $0.9570$ | $0.9738$ | $0.8313$ | $0.8434$ | $0.8057$ | $0.8377$
    | $0.8110$ | $0.8806$ | $0.8283$ | $\mathbf{0.9077}$ |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | 随机 | $0.9570$ | $0.9738$ | $0.8313$ | $0.8434$ | $0.8057$ | $0.8377$ |
    $0.8110$ | $0.8806$ | $0.8283$ | $\mathbf{0.9077}$ |'
- en: '|   Unc | LeastConf | $0.9677$ | $0.9892$ | $0.8377$ | $0.8820$ | $0.8113$
    | $0.8479$ | $0.8350$ | $0.9094$ | $0.8520$ | $\mathbf{0.9097}$ |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|   Unc | LeastConf | $0.9677$ | $0.9892$ | $0.8377$ | $0.8820$ | $0.8113$
    | $0.8479$ | $0.8350$ | $0.9094$ | $0.8520$ | $\mathbf{0.9097}$ |'
- en: '| LeastConfD | $0.9750$ | $0.9915$ | $0.8450$ | $0.8744$ | $0.8117$ | $0.8483$
    | $0.8320$ | $0.9083$ | $0.8243$ | $0.8654$ |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| LeastConfD | $0.9750$ | $0.9915$ | $0.8450$ | $0.8744$ | $0.8117$ | $0.8483$
    | $0.8320$ | $0.9083$ | $0.8243$ | $0.8654$ |'
- en: '| Margin | $0.9733$ | $0.9881$ | $0.8427$ | $0.8772$ | $0.8103$ | $0.8468$
    | $0.8373$ | $0.9138$ | $0.8580$ | $0.8859$ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Margin | $0.9733$ | $0.9881$ | $0.8427$ | $0.8772$ | $0.8103$ | $0.8468$
    | $0.8373$ | $0.9138$ | $0.8580$ | $0.8859$ |'
- en: '| MarginD | $0.9703$ | $0.9899$ | $0.8417$ | $0.8756$ | $0.8197$ | $0.8472$
    | $0.8357$ | $0.9104$ | $0.8230$ | $\mathbf{0.9149}$ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| MarginD | $0.9703$ | $0.9899$ | $0.8417$ | $0.8756$ | $0.8197$ | $0.8472$
    | $0.8357$ | $0.9104$ | $0.8230$ | $\mathbf{0.9149}$ |'
- en: '| Entropy | $0.9723$ | $0.9883$ | $0.8397$ | $0.8660$ | $0.8090$ | $0.8458$
    | $0.8297$ | $0.9099$ | $0.8570$ | $\mathbf{0.9132}$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Entropy | $0.9723$ | $0.9883$ | $0.8397$ | $0.8660$ | $0.8090$ | $0.8458$
    | $0.8297$ | $0.9099$ | $0.8570$ | $\mathbf{0.9132}$ |'
- en: '| EntropyD | $0.9683$ | $0.9887$ | $0.8417$ | $0.8784$ | $0.8167$ | $0.8507$
    | $0.8290$ | $0.9091$ | $0.8177$ | $0.8710$ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| EntropyD | $0.9683$ | $0.9887$ | $0.8417$ | $0.8784$ | $0.8167$ | $0.8507$
    | $0.8290$ | $0.9091$ | $0.8177$ | $0.8710$ |'
- en: '| BALD | $0.9697$ | $0.9885$ | $0.8423$ | $0.8888$ | $0.8197$ | $0.8448$ |
    $0.8333$ | $0.9020$ | $0.8270$ | $\mathbf{0.9204}$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| BALD | $0.9697$ | $0.9885$ | $0.8423$ | $0.8888$ | $0.8197$ | $0.8448$ |
    $0.8333$ | $0.9020$ | $0.8270$ | $\mathbf{0.9204}$ |'
- en: '| MeanSTD | $0.9713$ | $0.9735$ | $0.8457$ | $0.8766$ | $0.8110$ | $0.8426$
    | $0.8323$ | $0.9087$ | $0.7827$ | $0.8802$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| MeanSTD | $0.9713$ | $0.9735$ | $0.8457$ | $0.8766$ | $0.8110$ | $0.8426$
    | $0.8323$ | $0.9087$ | $0.7827$ | $0.8802$ |'
- en: '| VarRatio | $0.9717$ | $0.9841$ | $0.8410$ | $0.8754$ | $0.8107$ | $0.8497$
    | $0.8357$ | $0.9079$ | $0.8530$ | $0.8672$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| VarRatio | $0.9717$ | $0.9841$ | $0.8410$ | $0.8754$ | $0.8107$ | $0.8497$
    | $0.8357$ | $0.9079$ | $0.8530$ | $0.8672$ |'
- en: '| CEAL(Entropy) | $0.9787$ | $0.9889$ | $0.8477$ | $0.8826$ | $0.8163$ | $0.8459$
    | $0.8430$ | $0.9142$ | $0.8543$ | $\mathbf{0.9179}$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| CEAL(Entropy) | $0.9787$ | $0.9889$ | $0.8477$ | $0.8826$ | $0.8163$ | $0.8459$
    | $0.8430$ | $0.9142$ | $0.8543$ | $\mathbf{0.9179}$ |'
- en: '| Repr/Div | KMeans | $0.9640$ | $0.9813$ | $0.8260$ | $0.8525$ | $0.7903$
    | $0.8264$ | $0.8027$ | $0.8671$ | $0.8243$ | $\mathbf{0.9044}$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Repr/Div | KMeans | $0.9640$ | $0.9813$ | $0.8260$ | $0.8525$ | $0.7903$
    | $0.8264$ | $0.8027$ | $0.8671$ | $0.8243$ | $\mathbf{0.9044}$ |'
- en: '| KMeans (GPU) | $0.9637$ | $0.9747$ | $0.8343$ | $0.8657$ | $0.7990$ | $0.8362$
    | $0.8120$ | $0.8688$ | $0.8333$ | $\mathbf{0.9155}$ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| KMeans (GPU) | $0.9637$ | $0.9747$ | $0.8343$ | $0.8657$ | $0.7990$ | $0.8362$
    | $0.8120$ | $0.8688$ | $0.8333$ | $\mathbf{0.9155}$ |'
- en: '| KCenter | $0.9740$ | $0.9877$ | $0.8353$ | $0.8466$ | $*$ | $*$ | $0.8283$
    | $0.9000$ | $0.8130$ | $\mathbf{0.9189}$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| KCenter | $0.9740$ | $0.9877$ | $0.8353$ | $0.8466$ | $*$ | $*$ | $0.8283$
    | $0.9000$ | $0.8130$ | $\mathbf{0.9189}$ |'
- en: '| VAAL | $0.9623$ | $0.9573$ | $0.8297$ | $0.8535$ | $0.8027$ | $0.8363$ |
    $0.8117$ | $0.8813$ | $0.8393$ | $0.9064$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| VAAL | $0.9623$ | $0.9573$ | $0.8297$ | $0.8535$ | $0.8027$ | $0.8363$ |
    $0.8117$ | $0.8813$ | $0.8393$ | $0.9064$ |'
- en: '|  | BADGE(KMeans++) | $0.9707$ | $0.9904$ | $0.8437$ | $0.8662$ | $*$ | $*$
    | $0.8377$ | $0.9057$ | $0.8340$ | $\mathbf{0.9066}$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | BADGE(KMeans++) | $0.9707$ | $0.9904$ | $0.8437$ | $0.8662$ | $*$ | $*$
    | $0.8377$ | $0.9057$ | $0.8340$ | $\mathbf{0.9066}$ |'
- en: '| Enhance | AdvBIM | $0.9680$ | $0.9840$ | $0.8437$ | $0.8729$ | # | # | #
    | # | $0.8297$ | $0.9197$ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Enhance | AdvBIM | $0.9680$ | $0.9840$ | $0.8437$ | $0.8729$ | # | # | #
    | # | $0.8297$ | $0.9197$ |'
- en: '| LPL | $0.8913$ | $0.9732$ | $0.7600$ | $0.8471$ | $0.5640$ | $0.6474$ | $0.8737$
    | $\mathbf{0.9452}$ | $0.8593$ | $\mathbf{0.9346}$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| LPL | $0.8913$ | $0.9732$ | $0.7600$ | $0.8471$ | $0.5640$ | $0.6474$ | $0.8737$
    | $\mathbf{0.9452}$ | $0.8593$ | $\mathbf{0.9346}$ |'
- en: '| WAAL | $0.9890$ | $\mathbf{0.9946}$ | $0.8703$ | $0.8984$ | $0.8293$ | $0.8423$
    | $0.8603$ | $0.9135$ | $0.9663$ | $\mathbf{0.9564}$ |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| WAAL | $0.9890$ | $\mathbf{0.9946}$ | $0.8703$ | $0.8984$ | $0.8293$ | $0.8423$
    | $0.8603$ | $0.9135$ | $0.9663$ | $\mathbf{0.9564}$ |'
- en: '|  |  | *CIFAR10* | *CIFAR100* | *CIFAR10-imb* | *Tiny ImageNet* | *BreakHis*
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  |  | *CIFAR10* | *CIFAR100* | *CIFAR10-imb* | *Tiny ImageNet* | *BreakHis*
    |'
- en: '|  | Full | $-$ | $0.8793$ | $-$ | $0.6062$ | $-$ | $0.8036$ | $-$ | $0.4583$
    | $-$ | $0.8306$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | Full | $-$ | $0.8793$ | $-$ | $0.6062$ | $-$ | $0.8036$ | $-$ | $0.4583$
    | $-$ | $0.8306$ |'
- en: '|  | Random | $0.7967$ | $0.8679$ | $0.4667$ | $0.5903$ | $0.7103$ | $\mathbf{0.8105}$
    | $0.2577$ | $0.3544$ | $0.8010$ | $0.8150$ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | Random | $0.7967$ | $0.8679$ | $0.4667$ | $0.5903$ | $0.7103$ | $\mathbf{0.8105}$
    | $0.2577$ | $0.3544$ | $0.8010$ | $0.8150$ |'
- en: '|   Unc | LeastConf | $0.8150$ | $0.8785$ | $0.4747$ | $\mathbf{0.6072}$ |
    $0.7330$ | $0.8022$ | $0.2417$ | $0.3470$ | $0.8213$ | $0.8302$ |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|   Unc | LeastConf | $0.8150$ | $0.8785$ | $0.4747$ | $\mathbf{0.6072}$ |
    $0.7330$ | $0.8022$ | $0.2417$ | $0.3470$ | $0.8213$ | $0.8302$ |'
- en: '| LeastConfD | $0.8137$ | $\mathbf{0.8825}$ | $0.4730$ | $0.5997$ | $0.7323$
    | $\mathbf{0.8065}$ | $0.2620$ | $0.3698$ | $0.8140$ | $\mathbf{0.8313}$ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| LeastConfD | $0.8137$ | $\mathbf{0.8825}$ | $0.4730$ | $0.5997$ | $0.7323$
    | $\mathbf{0.8065}$ | $0.2620$ | $0.3698$ | $0.8140$ | $\mathbf{0.8313}$'
- en: '| Margin | $0.8153$ | $\mathbf{0.8834}$ | $0.4790$ | $0.6010$ | $0.7367$ |
    $0.8029$ | $0.2557$ | $0.3611$ | $0.8217$ | $0.8289$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Margin | $0.8153$ | $\mathbf{0.8834}$ | $0.4790$ | $0.6010$ | $0.7367$ |
    $0.8029$ | $0.2557$ | $0.3611$ | $0.8217$ | $0.8289$ |'
- en: '| MarginD | $0.8140$ | $\mathbf{0.8837}$ | $0.4777$ | $0.6000$ | $0.7260$ |
    $\mathbf{0.8128}$ | $0.2607$ | $0.3541$ | $0.8253$ | $\mathbf{0.8364}$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| MarginD | $0.8140$ | $\mathbf{0.8837}$ | $0.4777$ | $0.6000$ | $0.7260$ |
    $\mathbf{0.8128}$ | $0.2607$ | $0.3541$ | $0.8253$ | $\mathbf{0.8364}$ |'
- en: '| Entropy | $0.8130$ | $0.8784$ | $0.4693$ | $0.6048$ | $0.7320$ | $\mathbf{0.8187}$
    | $0.2343$ | $0.3346$ | $0.8213$ | $0.8251$ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Entropy | $0.8130$ | $0.8784$ | $0.4693$ | $0.6048$ | $0.7320$ | $\mathbf{0.8187}$
    | $0.2343$ | $0.3346$ | $0.8213$ | $0.8251$ |'
- en: '| EntropyD | $0.8140$ | $0.8787$ | $0.4677$ | $0.6004$ | $0.7317$ | $0.7963$
    | $0.2627$ | $0.3716$ | $0.8017$ | $0.8115$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| EntropyD | $0.8140$ | $0.8787$ | $0.4677$ | $0.6004$ | $0.7317$ | $0.7963$
    | $0.2627$ | $0.3716$ | $0.8017$ | $0.8115$ |'
- en: '| BALD | $0.8103$ | $0.8762$ | $0.4760$ | $0.5942$ | $0.7210$ | $0.7927$ |
    $0.2623$ | $0.3648$ | $0.8147$ | $0.8296$ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| BALD | $0.8103$ | $0.8762$ | $0.4760$ | $0.5942$ | $0.7210$ | $0.7927$ |
    $0.2623$ | $0.3648$ | $0.8147$ | $0.8296$ |'
- en: '| MeanSTD | $0.8087$ | $\mathbf{0.8821}$ | $0.4717$ | $0.5963$ | $0.7203$ |
    $0.7996$ | $0.2510$ | $0.3551$ | $0.8053$ | $0.8202$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| MeanSTD | $0.8087$ | $\mathbf{0.8821}$ | $0.4717$ | $0.5963$ | $0.7203$ |
    $0.7996$ | $0.2510$ | $0.3551$ | $0.8053$ | $0.8202$ |'
- en: '| VarRatio | $0.8150$ | $08780$ | $0.4747$ | $0.5959$ | $0.7353$ | $\mathbf{0.8165}$
    | $0.2407$ | $0.3426$ | $0.8197$ | $0.8264$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| VarRatio | $0.8150$ | $0.8780$ | $0.4747$ | $0.5959$ | $0.7353$ | $\mathbf{0.8165}$
    | $0.2407$ | $0.3426$ | $0.8197$ | $0.8264$ |'
- en: '| CEAL(Entropy) | $0.8150$ | $\mathbf{0.8794}$ | $0.4693$ | $0.6043$ | $0.7327$
    | $\mathbf{0.8187}$ | $0.2347$ | $0.3400$ | $0.8163$ | $0.8181$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| CEAL(Entropy) | $0.8150$ | $\mathbf{0.8794}$ | $0.4693$ | $0.6043$ | $0.7327$
    | $\mathbf{0.8187}$ | $0.2347$ | $0.3400$ | $0.8163$ | $0.8181$ |'
- en: '|   Repr/Div | KMeans | $0.7910$ | $0.8713$ | $0.4570$ | $0.5834$ | $0.7070$
    | $0.7908$ | $0.2447$ | $0.3385$ | $0.8203$ | $\mathbf{0.8394}$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|   Repr/Div | KMeans | $0.7910$ | $0.8713$ | $0.4570$ | $0.5834$ | $0.7070$
    | $0.7908$ | $0.2447$ | $0.3385$ | $0.8203$ | $\mathbf{0.8394}$ |'
- en: '| KMeans (GPU) | $0.7977$ | $0.8718$ | $0.4687$ | $0.5842$ | $0.7140$ | $0.7921$
    | $0.1340$ | $0.2288$ | $0.8140$ | $\mathbf{0.8323}$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| KMeans (GPU) | $0.7977$ | $0.8718$ | $0.4687$ | $0.5842$ | $0.7140$ | $0.7921$
    | $0.1340$ | $0.2288$ | $0.8140$ | $\mathbf{0.8323}$ |'
- en: '| KCenter | $0.8047$ | $0.8741$ | $0.4770$ | $0.5993$ | $0.7233$ | $0.7826$
    | $0.2540$ | $0.346$ | $0.8027$ | $0.8289$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| KCenter | $0.8047$ | $0.8741$ | $0.4770$ | $0.5993$ | $0.7233$ | $0.7826$
    | $0.2540$ | $0.346$ | $0.8027$ | $0.8289$ |'
- en: '| VAAL | $0.7973$ | $0.8679$ | $0.4693$ | $0.5870$ | $0.7113$ | $0.7950$ |
    $0.1313$ | $0.2191$ | $0.8197$ | $\mathbf{0.8344}$ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| VAAL | $0.7973$ | $0.8679$ | $0.4693$ | $0.5870$ | $0.7113$ | $0.7950$ |
    $0.1313$ | $0.2191$ | $0.8197$ | $\mathbf{0.8344}$ |'
- en: '|  | BADGE(KMeans++) | $0.8143$ | $\mathbf{0.8794}$ | $0.4803$ | $0.6034$ |
    $0.7347$ | $\mathbf{0.8126}$ | # | # | $0.8343$ | $\mathbf{0.8470}$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | BADGE(KMeans++) | $0.8143$ | $\mathbf{0.8794}$ | $0.4803$ | $0.6034$ |
    $0.7347$ | $\mathbf{0.8126}$ | # | # | $0.8343$ | $\mathbf{0.8470}$ |'
- en: '| Enhance | AdvBIM | $0.7997$ | $0.8750$ | $0.4713$ | $0.5855$ | # | # | #
    | # | $0.8240$ | $\mathbf{0.8337}$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Enhance | AdvBIM | $0.7997$ | $0.8750$ | $0.4713$ | $0.5855$ | # | # | #
    | # | $0.8240$ | $\mathbf{0.8337}$ |'
- en: '| LPL | $0.8220$ | $\mathbf{0.9028}$ | $0.4640$ | $\mathbf{0.6369}$ | $0.7477$
    | $\mathbf{0.8478}$ | $0.0090$ | $0.0051$ | $0.8277$ | $\mathbf{0.8316}$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| LPL | $0.8220$ | $\mathbf{0.9028}$ | $0.4640$ | $\mathbf{0.6369}$ | $0.7477$
    | $\mathbf{0.8478}$ | $0.0090$ | $0.0051$ | $0.8277$ | $\mathbf{0.8316}$ |'
- en: '| WAAL | $0.8253$ | $0.8717$ | $0.4277$ | $0.5560$ | $0.7523$ | $0.7993$ |
    $0.0157$ | $0.0050$ | $0.8620$ | $\mathbf{0.8698}$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| WAAL | $0.8253$ | $0.8717$ | $0.4277$ | $0.5560$ | $0.7523$ | $0.7993$ |
    $0.0157$ | $0.0050$ | $0.8620$ | $\mathbf{0.8698}$ |'
- en: 'Table 1: Overall results of DAL comparative experiments. We bold F-acc values
    that are higher than full performance. We rank F-acc and AUBC of each task with
    top 1st, 2nd and 3rd with red, teal and blue respectively. “$*$” refers to the
    experiment needed too much memory, e.g., KCenter on *EMNIST*. “#” refers to the
    experiment that has not been completed yet. Completed tables of all tasks are
    shown in Tables 4, 5, 6, 7, and 8 in Appendix.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：DAL比较实验的总体结果。我们将F-acc值中高于完整性能的值用粗体表示。我们用红色、青色和蓝色分别标记了每个任务的F-acc和AUBC的前1、2和3名。“$*$”表示实验需要过多内存，例如
    *EMNIST* 上的 KCenter。“#”表示尚未完成的实验。所有任务的完整表格见附录中的表4、5、6、7和8。
- en: 3.2 Analysis of Comparative Experiments
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 比较实验分析
- en: 'For analyzing the experiment results, we roughly divided our tasks into three
    groups: 1) standard image classification, including *MNIST*, *FashionMNIST*, *EMNIST*,
    *SVHN*, *CIFAR10*, *CIFAR10-imb*, *CIFAR100* and *TinyImageNet*; 2) medical image
    analysis, including *BreakHis* and *PneumoniaMNIST*; 3) comparative studies, including
    *MNIST* and *Waterbird*, which would be introduced in Section [3.4](#S3.SS4 "3.4
    How pre-training influence DAL performance? ‣ 3 Comparative Experiments of DAL
    ‣ A Comparative Survey of Deep Active Learning"). We report *AUBC (acc)* and F-acc
    performance in Table [1](#S3.T1 "Table 1 ‣ Experimental protocol. ‣ 3.1 Experimental
    Settings ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey of Deep Active
    Learning"). We provided overall accuracy-budget curves and summarizing tables,
    as shown in Figure 5 and Tables 4 to 9 in Appendix.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析实验结果，我们大致将任务分为三组：1) 标准图像分类，包括 *MNIST*、*FashionMNIST*、*EMNIST*、*SVHN*、*CIFAR10*、*CIFAR10-imb*、*CIFAR100*
    和 *TinyImageNet*；2) 医学图像分析，包括 *BreakHis* 和 *PneumoniaMNIST*；3) 比较研究，包括 *MNIST*
    和 *Waterbird*，将在第[3.4](#S3.SS4 "3.4 预训练如何影响DAL性能？ ‣ 3 DAL的比较实验 ‣ 深度主动学习的比较调查")节中介绍。我们在表[1](#S3.T1
    "表 1 ‣ 实验协议 ‣ 3.1 实验设置 ‣ 3 DAL的比较实验 ‣ 深度主动学习的比较调查")中报告了 *AUBC (acc)* 和 F-acc 性能。我们提供了整体准确度-预算曲线和总结表格，详见图5和附录中的表4至表9。
- en: The typical uncertainty-based strategies on group 1, standard image classification
    tasks (in Table [1](#S3.T1 "Table 1 ‣ Experimental protocol. ‣ 3.1 Experimental
    Settings ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey of Deep Active
    Learning"), from LeastConf to CEAL) are generally $1\%\sim 3\%$ higher than Random
    on average performance across the whole AL process (*AUBC*). Among these uncertainty-based
    methods, we have conducted paired t-tests of each method with the other methods
    comparing AUBCs across group 1, standard image classification tasks, and no method
    performs significantly better than the others (all $p$-value are larger than $0.05$).
    Considering dropout, there are only negligible effects (or even counter-intuitive)
    compared with the original versions (e.g., EntropyD vs. Entropy) on the normal
    image classification task, which is consistent with the observations in [[3](#bib.bib3)],
    except for *TinyImageNet*. On *TinyImageNet*, dropout versions are generally $1\%\sim
    3\%$ higher than the original versions. One possible explanation is that it is
    not accurate to use the feature representations provided by a single backbone
    model for calculating uncertainty score, while the dropout technique could help
    increase the uncertainty, and the differences among the uncertainty scores of
    unlabeled data samples will be increased. Therefore dropout versions provide better
    acquisition functions on *TinyImageNet*. Another comparison group is CEAL(Entropy)
    and Entropy, CEAL improved Entropy by an average of $0.5\%$ on $8$ datasets with
    threshold of confidence/entropy 1e-5\. This idea seems effective, but the threshold
    must be carefully tuned to get better performance. On medical image analysis tasks
    (e.g., *PneumoniaMNIST*), performances are slightly different; VarRatio is even
    $4.5\%$ lower than Random. Additionally, we observed the F-acc of many DAL algorithms
    are higher than full performance (e.g., F-acc is $0.9189$ on KCenter and $0.9039$
    on full data), and the performances of dropout versions are worse than normal
    methods, e.g., $0.857$ on Entropy and $0.8177$ on EntropyD. These abnormal phenomena
    could be caused by the distribution shift between training/test sets and data
    redundancy in AL processes. The detailed explanations are in Section E.1 in Appendix.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在组1的典型基于不确定性的策略中，标准图像分类任务（见表格 [1](#S3.T1 "Table 1 ‣ Experimental protocol. ‣
    3.1 Experimental Settings ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey
    of Deep Active Learning")），从 LeastConf 到 CEAL 的平均性能通常比 Random 高 $1\%\sim 3\%$（*AUBC*）。在这些基于不确定性的方法中，我们进行了各方法与其他方法的配对
    t 检验，比较了组1标准图像分类任务中的 AUBC，结果没有方法表现显著优于其他方法（所有 $p$-值均大于 $0.05$）。考虑到 dropout，与原始版本（例如
    EntropyD 与 Entropy）相比，正常图像分类任务中的影响微乎其微（或甚至与直觉相反），这与 [[3](#bib.bib3)] 中的观察一致，*TinyImageNet*
    除外。在 *TinyImageNet* 上，dropout 版本的性能通常比原始版本高 $1\%\sim 3\%$。一个可能的解释是，使用单个骨干模型提供的特征表示来计算不确定性评分并不准确，而
    dropout 技术可以帮助增加不确定性，增加无标签数据样本之间的不确定性评分差异。因此，dropout 版本在 *TinyImageNet* 上提供了更好的获取函数。另一个比较组是
    CEAL(Entropy) 和 Entropy，CEAL 在 $8$ 个数据集上的平均性能比 Entropy 高 $0.5\%$，阈值为置信度/熵 1e-5。这一思路似乎有效，但必须仔细调整阈值以获得更好的性能。在医学图像分析任务（例如
    *PneumoniaMNIST*）中，性能略有不同；VarRatio 甚至比 Random 低 $4.5\%$。此外，我们观察到许多 DAL 算法的 F-acc
    高于完整数据的性能（例如 KCenter 的 F-acc 为 $0.9189$ 和完整数据为 $0.9039$），且 dropout 版本的表现比正常方法差，例如
    Entropy 的 $0.857$ 和 EntropyD 的 $0.8177$。这些异常现象可能是由于训练/测试集之间的分布偏移和 AL 过程中的数据冗余造成的。详细解释见附录
    E.1 节。
- en: Compared with uncertainty-based measures, the performances of representativeness/diversity-based
    methods (KMeans, KCenter and VAAL) do not show much advantage. Furthermore, they
    have relatively high time and memory costs since the pairwise distance matrix
    used by KMeans and KCenter need to be re-computed in each iteration with current
    feature representations, while VAAL requires re-training a VAE. Also, a high memory
    load is needed for storing the pairwise distance matrix for large unlabeled data
    pools like *EMNIST*).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于基于不确定性的度量，基于代表性/多样性的方法（KMeans、KCenter 和 VAAL）的表现并没有明显的优势。此外，它们的时间和内存成本相对较高，因为
    KMeans 和 KCenter 使用的成对距离矩阵在每次迭代中都需要重新计算当前特征表示，而 VAAL 需要重新训练一个 VAE。此外，对于像*EMNIST*这样的大型无标签数据池，需要高内存负载来存储成对距离矩阵。
- en: Compared with the performance of representativeness-based AL strategies on classical
    ML tasks [[80](#bib.bib80)], we believe that good representativeness-based DAL
    performance is based on good feature representations. Our analysis is consistent
    with the implicit analyses in [[43](#bib.bib43)]. Compared with the CPU-version
    KMeans, KMeans (GPU) is more time-efficient (see Section E.1 in Appendix) and
    performs better (see Table [1](#S3.T1 "Table 1 ‣ Experimental protocol. ‣ 3.1
    Experimental Settings ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey
    of Deep Active Learning")).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与经典机器学习任务上的基于代表性的主动学习策略的表现相比[[80](#bib.bib80)]，我们认为良好的基于代表性的深度主动学习性能是基于良好的特征表示。我们的分析与[[43](#bib.bib43)]中的隐式分析一致。与CPU版KMeans相比，KMeans
    (GPU)在时间效率上更高（见附录中的E.1节），并且表现更好（见表[1](#S3.T1 "Table 1 ‣ Experimental protocol.
    ‣ 3.1 Experimental Settings ‣ 3 Comparative Experiments of DAL ‣ A Comparative
    Survey of Deep Active Learning")）。
- en: Combined strategy BADGE shows its advantage on multiple datasets, where it consistently
    has relatively better performance. BADGE has $1\%\sim 3\%$ higher AUBC performance
    compared with single KMeans and achieves comparable performance with uncertainty-based
    strategies and higher AUBC on *CIFAR100* dataset.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 结合策略BADGE在多个数据集上显示出其优势，在这些数据集中，它的表现始终相对较好。与单独的KMeans相比，BADGE的AUBC性能提高了$1\%\sim
    3\%$，并且在*CIFAR100*数据集上，其表现与基于不确定性的策略相当，并且AUBC更高。
- en: For enhanced DAL methods like LPL, WAAL, AdvBIM and CEAL, we are delighted to
    see their potential over typical DAL methods. For instance, LPL improves F-acc
    over full training on *SVHN* ($0.9452$ vs. $0.8793$), *CIFAR10* ($0.9028$ vs.
    $0.8793$), *CIFAR100* ($0.6369$ vs. $0.6062$), and *CIFAR10-imb* ($0.8478$ vs.
    $0.8036$). However, LPL is sensitive to hyper-parameters in the LossNet used to
    predict the target loss, e.g., the feature size determined by FC layer in LossNet.
    The LPL results on *EMNIST* and *TinyImageNet* indicate that it does not work
    on all datasets (we have tried many hyper-parameter settings on LossNet but did
    not work). A similar phenomenon occurs with WAAL. A potential explanation is that
    both *EMNIST* and *TinyImageNet* contain too many categories, which brings difficulty
    to the loss prediction in LPL and extracting diversified features in WAAL. This
    explanation is further verified in Section [3.4](#S3.SS4 "3.4 How pre-training
    influence DAL performance? ‣ 3 Comparative Experiments of DAL ‣ A Comparative
    Survey of Deep Active Learning") – we adopt a pre-trained ResNet18 as the basic
    classifier, which obtains better feature representations for loss prediction,
    yielding better performance of LPL compared to the non-pre-trained version ($0.9923$
    vs. $0.8913$). The performance comparison on CIFAR100 also supported this explanation.
    AdvBIM, which adds adversarial samples for training, does not achieve ground-breaking
    performances like LPL. These adversarial samples are learned by the current backbone
    model, thus the improvement provided by AdvBIM is marginal. Moreover, AdvBIM is
    extremely time-consuming since it requires calculating adversarial distance $\mathbf{r}$
    for every unlabeled data sample in every iteration. AdvBIM on *EMNIST* and *TinyImageNet*
    could not be completed due to the prohibitive computing requirements.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像LPL、WAAL、AdvBIM和CEAL这样的增强型深度主动学习方法，我们很高兴看到它们相对于典型的深度主动学习方法的潜力。例如，LPL在完整训练上提高了在*SVHN*上的F-acc（$0.9452$对比$0.8793$）、*CIFAR10*（$0.9028$对比$0.8793$）、*CIFAR100*（$0.6369$对比$0.6062$）和*CIFAR10-imb*（$0.8478$对比$0.8036$）。然而，LPL对用于预测目标损失的LossNet中的超参数非常敏感，例如，由LossNet中的FC层确定的特征大小。LPL在*EMNIST*和*TinyImageNet*上的结果表明，它并不适用于所有数据集（我们尝试了许多LossNet的超参数设置，但都没有成功）。WAAL也出现了类似的现象。一个可能的解释是，*EMNIST*和*TinyImageNet*包含的类别过多，这给LPL中的损失预测和WAAL中的多样化特征提取带来了困难。在第[3.4](#S3.SS4
    "3.4 How pre-training influence DAL performance? ‣ 3 Comparative Experiments of
    DAL ‣ A Comparative Survey of Deep Active Learning")节中，这一解释得到了进一步验证——我们采用了预训练的ResNet18作为基本分类器，这为损失预测获得了更好的特征表示，从而使LPL的性能优于未预训练的版本（$0.9923$对比$0.8913$）。对CIFAR100的性能比较也支持了这一解释。AdvBIM通过增加对抗样本来进行训练，但并没有像LPL那样取得突破性的表现。这些对抗样本被当前的主干模型学习到，因此AdvBIM提供的改进是边际性的。此外，AdvBIM非常耗时，因为它需要在每次迭代中计算每个未标记数据样本的对抗距离$\mathbf{r}$。由于计算要求过高，AdvBIM在*EMNIST*和*TinyImageNet*上的运行无法完成。
- en: 3.3 Ablation Study – numbers of training epochs and batch size
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 消融研究 – 训练周期和批量大小
- en: Compared to classical ML tasks [[80](#bib.bib80)] that are typically convex
    optimization problems and have globally optimal solutions, DL typically involves
    non-convex optimization problems with local minima. Different hyper-parameters
    like learning rate, optimizer, number of training epochs, and AL batch size lead
    to other solutions with various performances. Here we conduct ablation studies
    on the effect of the number of training epochs in each AL iteration and batch
    size $b$. Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Ablation Study – numbers of training
    epochs and batch size ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey
    of Deep Active Learning") presents the results (see Figure 6 in the Appendix for
    more figures). Compared with Random, Entropy achieves better performance when
    the model is trained using more epochs, e.g., Entropy boosts AUBC by around $1.5$
    when the model is trained $30$ epochs. We also see that using more training epochs
    results in better performance, e.g., AUBC gradually increases from around $0.66$
    to $0.80$ for Random. It is worth noting that the improvement of AUBC brought
    by increasing the number of epochs has diminishing returns. Some researchers prefer
    to use a vast number of epochs during DAL training processes, e.g., Yoo and Kweon
    [[78](#bib.bib78)] used $200$ epochs. However, others like [[32](#bib.bib32)]
    suggest that increasing the number of training epochs will not effectively improve
    testing performance due to generalization problems. Therefore, selecting an optimal
    number of training epochs is vital for reducing computation costs while maintaining
    good model performance. Interestingly, AL batch size has less impact on the performance,
    e.g., Entropy achieves similar performance using different AL batch sizes of $500$,
    $1000$, and $2000$, which is important for DAL since we can use a relatively large
    batch size to reduce the number of training cycles.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于通常是凸优化问题并具有全局最优解的经典机器学习任务[[80](#bib.bib80)]，深度学习通常涉及具有局部最小值的非凸优化问题。不同的超参数，如学习率、优化器、训练周期数量和主动学习（AL）批次大小，会导致不同的解和各种性能。在这里，我们对每次主动学习迭代中的训练周期数量和批次大小
    $b$ 的影响进行了消融研究。图[3](#S3.F3 "Figure 3 ‣ 3.3 Ablation Study – numbers of training
    epochs and batch size ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey
    of Deep Active Learning")展示了结果（更多图见附录中的图6）。与随机方法相比，当模型使用更多的训练周期时，熵方法（Entropy）表现更好，例如，当模型训练
    $30$ 个周期时，熵方法将 AUBC 提高了约 $1.5$。我们还发现，使用更多的训练周期会带来更好的性能，例如，对于随机方法，AUBC 从约 $0.66$
    逐渐增加到 $0.80$。值得注意的是，增加训练周期数所带来的 AUBC 改进具有递减回报。一些研究人员在深度主动学习（DAL）训练过程中喜欢使用大量的周期，例如，Yoo
    和 Kweon [[78](#bib.bib78)] 使用了 $200$ 个周期。然而，其他研究人员如[[32](#bib.bib32)]则建议，由于泛化问题，增加训练周期数不会有效提高测试性能。因此，选择一个最优的训练周期数量对于减少计算成本同时保持良好的模型性能至关重要。有趣的是，AL
    批次大小对性能的影响较小，例如，熵方法在使用不同的 AL 批次大小 $500$、$1000$ 和 $2000$ 时，表现相似，这对于 DAL 来说很重要，因为我们可以使用相对较大的批次大小来减少训练周期的数量。
- en: WAAL performs consistently better than Entropy and BADGE and the number of training
    epochs has less impact on its performance, e.g., training with $5$ epochs, WAAL
    achieves AUBC $0.825$ when training with 5 epochs and the AUBC remains consistent
    when increasing to 30 epochs. The possible reason is that WAAL considers diversity
    among samples and constructs a good representation through leveraging the unlabeled
    data information, thus reducing data bias. Moreover, we present the accuracy-budget
    curves using different batch sizes and training epochs in Figure A3\. We also
    conclude that training epochs have less impact on WAAL, which is important for
    active learning approaches since fewer training epochs will save training time.
    In addition, WAAL outperforms its counterparts (i.e., Badge and Entropy).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: WAAL 的表现始终优于熵方法（Entropy）和 BADGE，并且训练周期数量对其性能影响较小，例如，当训练 $5$ 个周期时，WAAL 的 AUBC
    为 $0.825$，当训练周期增加到 30 个时，AUBC 保持不变。可能的原因是 WAAL 考虑了样本之间的多样性，并通过利用未标记数据的信息构建了良好的表示，从而减少了数据偏差。此外，我们在图
    A3 中展示了使用不同批次大小和训练周期的准确度-预算曲线。我们还总结了训练周期对 WAAL 的影响较小，这对主动学习方法非常重要，因为较少的训练周期可以节省训练时间。此外，WAAL
    超越了其对手（即 Badge 和 Entropy）。
- en: '![Refer to caption](img/2ac10b04c4ff49755db71663f1ef9126.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2ac10b04c4ff49755db71663f1ef9126.png)'
- en: 'Figure 2: Ablation studies on varying number of epochs and batch size on *CIFAR10*.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在 *CIFAR10* 上的不同训练周期数量和批次大小的消融研究。
- en: '![Refer to caption](img/d5539a9a187490995294fe01fcc4f1a4.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d5539a9a187490995294fe01fcc4f1a4.png)'
- en: 'Figure 3: Activation maps of ResNet18 w/ and w/o ImageNet1K pre-training.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：ResNet18在有无ImageNet1K预训练下的激活图。
- en: '![Refer to caption](img/b836abd2f05cc40f8f0346959d531f7b.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b836abd2f05cc40f8f0346959d531f7b.png)'
- en: (a)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/55f09371a0f79c68e6a2fb655a247992.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/55f09371a0f79c68e6a2fb655a247992.png)'
- en: (b)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/1c574e6a7f3a2d95e9f4140995e74bbe.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1c574e6a7f3a2d95e9f4140995e74bbe.png)'
- en: (c)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: '![Refer to caption](img/f0a5ef9be7c1a35fff5b43a966611f58.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f0a5ef9be7c1a35fff5b43a966611f58.png)'
- en: (d)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (d)
- en: '![Refer to caption](img/4b5fc3a6eab859590a86f7196a887c53.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4b5fc3a6eab859590a86f7196a887c53.png)'
- en: (e)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (e)
- en: '![Refer to caption](img/ef6847d6b4529a0cc13eb7e4b53f16d2.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ef6847d6b4529a0cc13eb7e4b53f16d2.png)'
- en: (f)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (f)
- en: '![Refer to caption](img/bbfe707a6ad0f4598df51f937a4d750a.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bbfe707a6ad0f4598df51f937a4d750a.png)'
- en: (g)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: (g)
- en: '![Refer to caption](img/970c96c4069823f3db75ca93cea03d4f.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/970c96c4069823f3db75ca93cea03d4f.png)'
- en: (h)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: (h)
- en: 'Figure 4: Overall (mismatch, worst group) accuracy vs. budget curves on *MNIST*
    and *Waterbird* datasets.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：*MNIST*和*Waterbird*数据集上的总体（不匹配，最差组）准确率与预算曲线。
- en: 3.4 How pre-training influence DAL performance?
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 预训练如何影响DAL性能？
- en: 'Pre-training has become a central technique in research and applications of
    DNNs in recent years [[25](#bib.bib25)]. In our work, we selected *Waterbird*
    and *MNIST* datasets to conduct comparative experiments, with non pre-trained
    ResNet18 and pre-trained ResNet18 (pre-trained on ImageNet-1K data, ResNet18P
    for short). *Waterbird* and *MNIST* have completely different natures. *Waterbird*
    dataset contains spurious correlations, and non pre-trained model will focus more
    on backgrounds, e.g., the classifier will wrongly classify a landbird as a waterbird
    when the background is water. Pre-trained models provide better feature representations
    and allow better object semantics (see Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Ablation
    Study – numbers of training epochs and batch size ‣ 3 Comparative Experiments
    of DAL ‣ A Comparative Survey of Deep Active Learning")). *Waterbird* is separated
    to four groups based on object and background: $\{(\text{waterbird, water}),\text{(waterbird,
    land}),\text{(landbird, land}),\text{(landbird, land})\}$. Besides overall accuracy,
    we also report mismatch and worst group accuracy [[54](#bib.bib54), [42](#bib.bib42)],
    which refers to the accuracy of groups $\{(\text{waterbird, land}),\text{(landbird,
    water})\}$ and the lowest accuracy among four groups respectively. On *MNIST*,
    there is no valid background information. Therefore both models w/ and w/o pre-training
    would focus on semantic itself. The goal of this experiment is to observe how
    the pre-training technique influences DAL on hard tasks (i.e., *Waterbird*) and
    easy/well-studied tasks (i.e., *MNIST*) and how feature representations generated
    by basic learners influence DAL methods.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，预训练已成为DNN研究和应用的核心技术[[25](#bib.bib25)]。在我们的工作中，我们选择了*Waterbird*和*MNIST*数据集进行对比实验，分别使用未预训练的ResNet18和预训练的ResNet18（简称ResNet18P，预训练数据为ImageNet-1K）。*Waterbird*和*MNIST*具有完全不同的性质。*Waterbird*数据集包含虚假关联，未预训练模型会更多地关注背景，例如，当背景是水时，分类器会错误地将陆地鸟分类为水鸟。预训练模型提供了更好的特征表示，并允许更好的对象语义（见图[3](#S3.F3
    "图 3 ‣ 3.3 消融研究 – 训练周期数和批量大小 ‣ 3 DAL的对比实验 ‣ 深度主动学习的对比调查")）。*Waterbird*数据集基于对象和背景被分为四组：$\{(\text{水鸟,
    水}),\text{(水鸟, 陆地}),\text{(陆地鸟, 陆地}),\text{(陆地鸟, 陆地})\}$。除了整体准确率外，我们还报告了不匹配和最差组的准确率[[54](#bib.bib54),
    [42](#bib.bib42)]，它们分别指代组$\{(\text{水鸟, 陆地}),\text{(陆地鸟, 水})\}$的准确率和四组中的最低准确率。在*MNIST*上，没有有效的背景信息。因此，无论是否预训练，两种模型都会关注语义本身。此实验的目标是观察预训练技术如何影响在困难任务（即*Waterbird*）和简单/研究充分的任务（即*MNIST*）上的DAL，以及基本学习者生成的特征表示如何影响DAL方法。
- en: Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Ablation Study – numbers of training epochs
    and batch size ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey of Deep
    Active Learning") presents overall (also mismatch and worst group in *Waterbird*)
    accuracy vs. budget curves for *MNIST* and *Waterbird*. On *MNIST*, pre-training
    does enhance overall DAL performance, but the ranking across these methods is
    not change (except for LPL), e.g., Entropy $>$ EntropyD $>$ KMeans in both *MNIST*
    with ResNet18 and ResNet18P. LPL performs far better based on ResNet18P, since
    loss prediction is more accurate with better feature representations. In *Waterbird*,
    considering ResNet18 w/o pre-training, normal DAL methods like Entropy, EntropyD,
    CEAL and KMeans are affected by the quality of backbone, which influences the
    DAL selections. Moreover, selecting more data even induces more biases (*Waterbird*
    is imbalanced among four groups) and cause performance reduction. These DAL methods
    return to normal when using the pre-trained ResNet18P, since it helps generate
    predictions with accurate directions (i.e., focus on the object itself, as shown
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Ablation Study – numbers of training epochs
    and batch size ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey of Deep
    Active Learning")). On *Waterbird*, LPL and WAAL w/o pre-training has better performance,
    possibly because they acquire more information with help of enhancing techniques
    (i.e., loss prediction and collecting unlabeled data information). However, LPL
    and WAAL could not well learn worst group under both w/ and w/o pre-training situations.
    A possible explanation is that they are affected by the imbalance problems of
    these groups, which induces bias problems in loss prediction and collecting unlabeled
    data information, and results in poor performance of the worst group.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S3.F4 "图4 ‣ 3.3 消融研究 – 训练周期数和批量大小 ‣ 3 DAL的比较实验 ‣ 深度主动学习的比较调查")展示了*MNIST*和*Waterbird*的总体（以及*Waterbird*中的不匹配和最差组）准确度与预算曲线。在*MNIST*中，预训练确实增强了总体DAL性能，但这些方法的排名没有变化（除了LPL），例如，Entropy
    $>$ EntropyD $>$ KMeans，在*MNIST*的ResNet18和ResNet18P中均是如此。基于ResNet18P，LPL表现远超其他方法，因为损失预测更准确，特征表示更好。在*Waterbird*中，考虑到ResNet18没有预训练，正常的DAL方法如Entropy、EntropyD、CEAL和KMeans受限于主干网络的质量，这影响了DAL的选择。此外，选择更多数据甚至会引入更多的偏差（*Waterbird*在四个组中是不平衡的），并导致性能下降。这些DAL方法在使用预训练的ResNet18P时恢复正常，因为它有助于生成具有准确方向的预测（即，专注于目标本身，如图[3](#S3.F3
    "图3 ‣ 3.3 消融研究 – 训练周期数和批量大小 ‣ 3 DAL的比较实验 ‣ 深度主动学习的比较调查")所示）。在*Waterbird*中，LPL和没有预训练的WAAL表现更好，可能是因为它们借助增强技术（即，损失预测和收集未标记数据的信息）获取了更多信息。然而，LPL和WAAL在有无预训练的情况下都无法很好地学习最差组。一个可能的解释是，它们受到了这些组的不平衡问题的影响，这导致了损失预测和收集未标记数据的信息中的偏差问题，从而导致最差组的表现不佳。
- en: 4 Challenges and Future Directions of DAL
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DAL的挑战与未来方向
- en: Since there is little room for improving DAL by only designing acquisition functions
    as shown in Sections [2.2](#S2.SS2 "2.2 Enhancing of DAL Methods ‣ 2 DAL Approaches
    ‣ A Comparative Survey of Deep Active Learning") and [3.2](#S3.SS2 "3.2 Analysis
    of Comparative Experiments ‣ 3 Comparative Experiments of DAL ‣ A Comparative
    Survey of Deep Active Learning"), researchers focus on proposing effective ways
    to enhance DAL methods like LPL and enlarging batch size in each round to reduce
    the time and computation cost. However, enhancing methods might not work well
    on all tasks (as shown in Table [1](#S3.T1 "Table 1 ‣ Experimental protocol. ‣
    3.1 Experimental Settings ‣ 3 Comparative Experiments of DAL ‣ A Comparative Survey
    of Deep Active Learning")). Better and more universal enhancement methods are
    needed in DAL. Cluster-Margin have scaled to batch sizes (100K-1M) several orders
    of magnitude larger than previous studies [[12](#bib.bib12)], it is hard to be
    transcended.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于仅通过设计获取函数（如第[2.2](#S2.SS2 "2.2 增强DAL方法 ‣ 2 DAL方法 ‣ 深度主动学习的比较调查")节和第[3.2](#S3.SS2
    "3.2 比较实验分析 ‣ 3 DAL的比较实验 ‣ 深度主动学习的比较调查")节所示）来改进DAL的空间有限，研究人员专注于提出有效的方法来增强DAL方法，如LPL，以及在每轮中增加批量大小，以减少时间和计算成本。然而，增强方法可能并不适用于所有任务（如表[1](#S3.T1
    "表1 ‣ 实验方案 ‣ 3.1 实验设置 ‣ 3 DAL的比较实验 ‣ 深度主动学习的比较调查")所示）。在DAL中需要更好且更通用的增强方法。Cluster-Margin已扩展到比之前研究大几个数量级的批量大小（100K-1M）[[12](#bib.bib12)]，这很难被超越。
- en: Another notable situation is the research trend shifting towards developing
    new methodologies to utilize better unlabeled data like semi- and self-supervised
    learning (S4L). Chan et al. [[8](#bib.bib8)] integrated S4L and DAL and conducted
    extensive experiments with contemporary methods, demonstrating that much of the
    benefit of DAL is subsumed by S4L techniques. A clear direction is to better leverage
    the unlabeled data during training in the DAL process under the very few label
    regimes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个显著的情况是研究趋势转向开发新方法，以更好地利用未标记的数据，如半监督学习和自监督学习（S4L）。Chan等人[[8](#bib.bib8)]将S4L与DAL结合，并进行了广泛的实验，证明了DAL的大部分好处被S4L技术所涵盖。一个明确的方向是在DAL过程中，在标签非常稀少的情况下更好地利用未标记的数据。
- en: Recently, some researchers employed DAL on more complex tasks like Visual Question
    Answering (VQA) [[30](#bib.bib30)] and observed that DAL methods might not perform
    well.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一些研究者在更复杂的任务上使用了DAL，例如视觉问答（VQA）[[30](#bib.bib30)]，并观察到DAL方法可能表现不佳。
- en: 'Many potential reasons limit DAL performance: i) task-specific DAL may be needed
    for those specific tasks; ii) better feature representations are needed; and iii)
    various dataset properties need to be considered, like collective outliers in
    VQA tasks [[30](#bib.bib30)]. These are possible research directions that demand
    prompt solutions. Therefore, with the increasing demand for dealing with larger
    and more complex data in realistic scenarios, e.g., out-of-distribution (OOD),
    rare classes, imbalanced data, and larger unlabeled data pool [[37](#bib.bib37),
    [12](#bib.bib12)], DAL under different data types are becoming more popular. E.g.,
    Kothawade et al. [[37](#bib.bib37)] let DAL work on rare classes, redundancy,
    imbalanced, and OOD data scenarios.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 许多潜在原因限制了DAL的性能：i) 可能需要特定任务的DAL；ii) 需要更好的特征表示；iii) 需要考虑各种数据集属性，例如VQA任务中的集体离群点[[30](#bib.bib30)]。这些都是需要及时解决的可能研究方向。因此，随着处理现实场景中更大更复杂数据（例如，分布外（OOD）、稀有类别、不平衡数据和更大未标记数据池[[37](#bib.bib37),
    [12](#bib.bib12)]）的需求增加，不同数据类型下的DAL变得越来越受欢迎。例如，Kothawade等人[[37](#bib.bib37)]让DAL在稀有类别、冗余、不平衡和OOD数据场景中发挥作用。
- en: Another possible research direction is to apply DAL techniques with new data-insufficient
    tasks like automatic driving, medical image analysis, etc. Haussmann et al. [[23](#bib.bib23)]
    have applied AL in an autonomous driving setting of nighttime detection of pedestrians
    and bicycles to improve nighttime detection of pedestrians and bicycles. It has
    shown improved detection accuracy of self-driving DNNs over manual curation –
    data selected with AL yields relative improvements in mean average precision of
    $3\times$ on pedestrian detection and $4.4\times$ on detection of bicycles over
    manually-selected data. Budd et al. [[5](#bib.bib5)] presents a survey on AL adoption
    in the medical domain. Different tasks have different concerns when integrating
    DAL techniques. For instance, In medical imaging, there are many rare yet important
    diseases (e.g., various forms of cancers), while non-cancerous images are much
    more than compared to the cancerous ones [[37](#bib.bib37)]. Therefore, rare classes
    must be considered when designing AL strategies in medical image analysis. More
    importantly, labeling medical images require expertise, and annotation costs and
    effort remain significant. Task-specific DAL is also worthy of research in recent
    years.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能的研究方向是将DAL技术应用于数据不足的新任务，如自动驾驶、医学图像分析等。Haussmann等人[[23](#bib.bib23)]在夜间行人和自行车检测的自动驾驶环境中应用了AL，以改善夜间对行人和自行车的检测。结果显示，与人工筛选的数据相比，使用AL选出的数据在行人检测的平均精度上有$3\times$的相对提升，自行车检测上有$4.4\times$的提升。Budd等人[[5](#bib.bib5)]展示了AL在医学领域采用的调研。不同任务在整合DAL技术时有不同的关注点。例如，在医学影像中，有许多罕见但重要的疾病（如各种癌症），而非癌症图像相比癌症图像要多得多[[37](#bib.bib37)]。因此，在医学图像分析中设计AL策略时必须考虑罕见类别。更重要的是，标记医学图像需要专业知识，标注成本和工作量仍然很大。近年来，任务特定的DAL也值得研究。
- en: Acknowledgments and Disclosure of Funding
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢和资金披露
- en: Parts of experiments (including experiments on *PneumoniaMNIST* and *BreakHis*
    datasets) in this paper were carried out on Baidu Data Federation Platform (Baidu
    FedCube). For usages, please contact us via [{fedcube,shubang}@baidu.com](%7Bfedcube,shubang%7D@baidu.com).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的部分实验（包括在*PneumoniaMNIST*和*BreakHis*数据集上的实验）是在百度数据联邦平台（Baidu FedCube）上进行的。有关使用，请通过[{fedcube,shubang}@baidu.com](%7Bfedcube,shubang%7D@baidu.com)与我们联系。
- en: References
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Aggarwal et al. [2014] Charu C Aggarwal, Xiangnan Kong, Quanquan Gu, Jiawei
    Han, and S Yu Philip. Active learning: A survey. In *Data Classification*, pages
    599–634\. Chapman and Hall/CRC, 2014.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aggarwal等人[2014] Charu C Aggarwal, Xiangnan Kong, Quanquan Gu, Jiawei Han, 和
    S Yu Philip. 主动学习：一项综述。见于*数据分类*，第599–634页。Chapman and Hall/CRC，2014年。
- en: Ash et al. [2020] Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John
    Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain
    gradient lower bounds. In *8th International Conference on Learning Representations,
    ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ash等人[2020] Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford,
    和 Alekh Agarwal. 通过多样化、不确定的梯度下界进行深度批量主动学习。见于*第8届国际学习表征会议，ICLR 2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日*。OpenReview.net，2020年。
- en: Beluch et al. [2018] William H Beluch, Tim Genewein, Andreas Nürnberger, and
    Jan M Köhler. The power of ensembles for active learning in image classification.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pages 9368–9377, 2018.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beluch等人[2018] William H Beluch, Tim Genewein, Andreas Nürnberger, 和 Jan M Köhler.
    集成学习在图像分类中的强大作用。见于*IEEE计算机视觉与模式识别会议论文集*，第9368–9377页，2018年。
- en: Bıyık et al. [2019] Erdem Bıyık, Kenneth Wang, Nima Anari, and Dorsa Sadigh.
    Batch active learning using determinantal point processes. *arXiv preprint arXiv:1906.07975*,
    2019.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bıyık等人[2019] Erdem Bıyık, Kenneth Wang, Nima Anari, 和 Dorsa Sadigh. 使用行列式点过程的批量主动学习。*arXiv预印本
    arXiv:1906.07975*，2019年。
- en: Budd et al. [2021] Samuel Budd, Emma C Robinson, and Bernhard Kainz. A survey
    on active learning and human-in-the-loop deep learning for medical image analysis.
    *Medical Image Analysis*, 71:102062, 2021.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Budd等人[2021] Samuel Budd, Emma C Robinson, 和 Bernhard Kainz. 主动学习和人机协作深度学习在医学图像分析中的调查。*医学图像分析*，71:102062，2021年。
- en: Casanova et al. [2020] Arantxa Casanova, Pedro O Pinheiro, Negar Rostamzadeh,
    and Christopher J Pal. Reinforced active learning for image segmentation. *arXiv
    preprint arXiv:2002.06583*, 2020.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Casanova等人[2020] Arantxa Casanova, Pedro O Pinheiro, Negar Rostamzadeh, 和 Christopher
    J Pal. 强化主动学习用于图像分割。*arXiv预印本 arXiv:2002.06583*，2020年。
- en: Cawley [2011] Gavin C Cawley. Baseline methods for active learning. In *Active
    Learning and Experimental Design workshop In conjunction with AISTATS 2010*, pages
    47–57\. JMLR Workshop and Conference Proceedings, 2011.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cawley[2011] Gavin C Cawley. 主动学习的基准方法。见于*主动学习与实验设计研讨会（与AISTATS 2010同时举行）*，第47–57页。JMLR研讨会与会议论文集，2011年。
- en: 'Chan et al. [2021] Yao-Chun Chan, Mingchen Li, and Samet Oymak. On the marginal
    benefit of active learning: Does self-supervision eat its cake? In *ICASSP 2021-2021
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    pages 3455–3459\. IEEE, 2021.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan等人[2021] Yao-Chun Chan, Mingchen Li, 和 Samet Oymak. 主动学习的边际效益：自监督是否会吃掉它的蛋糕？见于*ICASSP
    2021-2021 IEEE国际声学、语音与信号处理会议（ICASSP）*，第3455–3459页。IEEE，2021年。
- en: Chen et al. [2015] Yukun Chen, Thomas A Lasko, Qiaozhu Mei, Joshua C Denny,
    and Hua Xu. A study of active learning methods for named entity recognition in
    clinical text. *Journal of biomedical informatics*, 58:11–18, 2015.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人[2015] Yukun Chen, Thomas A Lasko, Qiaozhu Mei, Joshua C Denny, 和 Hua
    Xu. 一项关于临床文本中命名实体识别的主动学习方法的研究。*生物医学信息学杂志*，58:11–18，2015年。
- en: Choi et al. [2021] Jiwoong Choi, Ismail Elezi, Hyuk-Jae Lee, Clement Farabet,
    and Jose M Alvarez. Active learning for deep object detection via probabilistic
    modeling. *arXiv preprint arXiv:2103.16130*, 2021.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi等人[2021] Jiwoong Choi, Ismail Elezi, Hyuk-Jae Lee, Clement Farabet, 和 Jose
    M Alvarez. 通过概率建模进行深度对象检测的主动学习。*arXiv预印本 arXiv:2103.16130*，2021年。
- en: Chu et al. [2011] Wei Chu, Martin Zinkevich, Lihong Li, Achint Thomas, and Belle
    Tseng. Unbiased online active learning in data streams. In *Proceedings of the
    17th ACM SIGKDD international conference on Knowledge discovery and data mining*,
    pages 195–203, 2011.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu等人[2011] Wei Chu, Martin Zinkevich, Lihong Li, Achint Thomas, 和 Belle Tseng.
    数据流中的无偏在线主动学习。见于*第17届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，第195–203页，2011年。
- en: Citovsky et al. [2021] Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros
    Karydas, Anand Rajagopalan, Afshin Rostamizadeh, and Sanjiv Kumar. Batch active
    learning at scale. *Advances in Neural Information Processing Systems*, 34, 2021.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Citovsky等人[2021] Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas,
    Anand Rajagopalan, Afshin Rostamizadeh, 和 Sanjiv Kumar. 大规模批量主动学习。*神经信息处理系统进展*，34，2021年。
- en: 'Cohen et al. [2017] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre
    Van Schaik. Emnist: Extending mnist to handwritten letters. In *2017 International
    Joint Conference on Neural Networks (IJCNN)*, pages 2921–2926\. IEEE, 2017.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen 等 [2017] Gregory Cohen, Saeed Afshar, Jonathan Tapson 和 Andre Van Schaik。Emnist：将
    mnist 扩展到手写字母。在 *2017 国际联合神经网络会议 (IJCNN)*，第 2921–2926 页。IEEE，2017年。
- en: Deng [2012] Li Deng. The mnist database of handwritten digit images for machine
    learning research. *IEEE Signal Processing Magazine*, 29(6):141–142, 2012.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng [2012] Li Deng。用于机器学习研究的手写数字图像的 mnist 数据库。*IEEE 信号处理杂志*，29(6):141–142，2012年。
- en: 'Ducoffe and Precioso [2018] Melanie Ducoffe and Frederic Precioso. Adversarial
    active learning for deep networks: a margin based approach. *arXiv preprint arXiv:1802.09841*,
    2018.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ducoffe 和 Precioso [2018] Melanie Ducoffe 和 Frederic Precioso。针对深度网络的对抗性主动学习：一种基于边际的方法。*arXiv
    预印本 arXiv:1802.09841*，2018年。
- en: 'Duong et al. [2018] Long Duong, Hadi Afshar, Dominique Estival, Glen Pink,
    Philip R Cohen, and Mark Johnson. Active learning for deep semantic parsing. In
    *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
    (Volume 2: Short Papers)*, pages 43–48, 2018.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duong 等 [2018] Long Duong, Hadi Afshar, Dominique Estival, Glen Pink, Philip
    R Cohen 和 Mark Johnson。深度语义解析的主动学习。在 *第 56 届计算语言学协会年会论文集 (第 2 卷：短论文)*，第 43–48
    页，2018年。
- en: Elahi et al. [2016] Mehdi Elahi, Francesco Ricci, and Neil Rubens. A survey
    of active learning in collaborative filtering recommender systems. *Computer Science
    Review*, 20:29–50, 2016.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elahi 等 [2016] Mehdi Elahi, Francesco Ricci 和 Neil Rubens。协同过滤推荐系统中的主动学习调查。*计算机科学评论*，20:29–50，2016年。
- en: 'Freeman and Freeman [1965] Linton C Freeman and Linton C Freeman. *Elementary
    applied statistics: for students in behavioral science*. New York: Wiley, 1965.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freeman 和 Freeman [1965] Linton C Freeman 和 Linton C Freeman. *初级应用统计学：行为科学学生的指南*。纽约：Wiley，1965年。
- en: Fu et al. [2013] Yifan Fu, Xingquan Zhu, and Bin Li. A survey on instance selection
    for active learning. *Knowledge and information systems*, 35(2):249–283, 2013.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 [2013] Yifan Fu, Xingquan Zhu 和 Bin Li。关于主动学习的实例选择的调查。*知识与信息系统*，35(2):249–283，2013年。
- en: Gal et al. [2017] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian
    active learning with image data. In *International Conference on Machine Learning*,
    pages 1183–1192\. PMLR, 2017.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal 等 [2017] Yarin Gal, Riashat Islam 和 Zoubin Ghahramani。使用图像数据的深度贝叶斯主动学习。在
    *国际机器学习会议*，第 1183–1192 页。PMLR，2017年。
- en: Geifman and El-Yaniv [2017] Yonatan Geifman and Ran El-Yaniv. Deep active learning
    over the long tail. *arXiv preprint arXiv:1711.00941*, 2017.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geifman 和 El-Yaniv [2017] Yonatan Geifman 和 Ran El-Yaniv。长尾上的深度主动学习。*arXiv 预印本
    arXiv:1711.00941*，2017年。
- en: Gissin and Shalev-Shwartz [2019] Daniel Gissin and Shai Shalev-Shwartz. Discriminative
    active learning. *arXiv preprint arXiv:1907.06347*, 2019.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gissin 和 Shalev-Shwartz [2019] Daniel Gissin 和 Shai Shalev-Shwartz。区分性主动学习。*arXiv
    预印本 arXiv:1907.06347*，2019年。
- en: Haussmann et al. [2020] Elmar Haussmann, Michele Fenzi, Kashyap Chitta, Jan
    Ivanecky, Hanson Xu, Donna Roy, Akshita Mittel, Nicolas Koumchatzky, Clement Farabet,
    and Jose M Alvarez. Scalable active learning for object detection. In *2020 IEEE
    Intelligent Vehicles Symposium (IV)*, pages 1430–1435\. IEEE, 2020.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haussmann 等 [2020] Elmar Haussmann, Michele Fenzi, Kashyap Chitta, Jan Ivanecky,
    Hanson Xu, Donna Roy, Akshita Mittel, Nicolas Koumchatzky, Clement Farabet 和 Jose
    M Alvarez。用于目标检测的可扩展主动学习。在 *2020 IEEE 智能车辆研讨会 (IV)*，第 1430–1435 页。IEEE，2020年。
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pages 770–778, 2016.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren 和 Jian Sun。深度残差学习用于图像识别。在
    *IEEE 计算机视觉与模式识别会议论文集*，第 770–778 页，2016年。
- en: Hendrycks et al. [2019] Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using
    pre-training can improve model robustness and uncertainty. In *International Conference
    on Machine Learning*, pages 2712–2721\. PMLR, 2019.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 [2019] Dan Hendrycks, Kimin Lee 和 Mantas Mazeika。使用预训练可以提高模型的鲁棒性和不确定性。在
    *国际机器学习会议*，第 2712–2721 页。PMLR，2019年。
- en: Houlsby et al. [2011] Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté
    Lengyel. Bayesian active learning for classification and preference learning.
    *arXiv preprint arXiv:1112.5745*, 2011.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby 等 [2011] Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani 和 Máté Lengyel。用于分类和偏好学习的贝叶斯主动学习。*arXiv
    预印本 arXiv:1112.5745*，2011年。
- en: 'Huang [2021] Kuan-Hao Huang. Deepal: Deep active learning in python. *arXiv
    preprint arXiv:2111.15258*, 2021.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang [2021] Kuan-Hao Huang。Deepal：Python 中的深度主动学习。*arXiv 预印本 arXiv:2111.15258*，2021年。
- en: Johnson et al. [2019] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale
    similarity search with GPUs. *IEEE Transactions on Big Data*, 7(3):535–547, 2019.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等 [2019] Jeff Johnson, Matthijs Douze, 和 Hervé Jégou。使用 GPU 进行大规模相似性搜索。*IEEE
    大数据事务*，7(3)：535–547，2019。
- en: Kampffmeyer et al. [2016] Michael Kampffmeyer, Arnt-Borre Salberg, and Robert
    Jenssen. Semantic segmentation of small objects and modeling of uncertainty in
    urban remote sensing images using deep convolutional neural networks. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition workshops*,
    pages 1–9, 2016.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kampffmeyer 等 [2016] Michael Kampffmeyer, Arnt-Borre Salberg, 和 Robert Jenssen。使用深度卷积神经网络对城市遥感图像中的小物体进行语义分割和不确定性建模。在
    *IEEE计算机视觉与模式识别会议论文集*，页码 1–9，2016。
- en: Karamcheti et al. [2021] Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, and
    Christopher D Manning. Mind your outliers! investigating the negative impact of
    outliers on active learning for visual question answering. *arXiv preprint arXiv:2107.02331*,
    2021.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karamcheti 等 [2021] Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, 和 Christopher
    D Manning。注意你的异常值！研究异常值对视觉问答主动学习的负面影响。*arXiv 预印本 arXiv:2107.02331*，2021。
- en: Kermany et al. [2018] Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS
    Valentim, Huiying Liang, Sally L Baxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing
    Yan, et al. Identifying medical diagnoses and treatable diseases by image-based
    deep learning. *Cell*, 172(5):1122–1131, 2018.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kermany 等 [2018] Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS
    Valentim, Huiying Liang, Sally L Baxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing
    Yan 等。通过基于图像的深度学习识别医疗诊断和可治疗疾病。*Cell*，172(5)：1122–1131，2018。
- en: 'Keskar et al. [2016] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal,
    Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep
    learning: Generalization gap and sharp minima. *arXiv preprint arXiv:1609.04836*,
    2016.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keskar 等 [2016] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail
    Smelyanskiy, 和 Ping Tak Peter Tang。关于深度学习的大批量训练：泛化差距和尖锐极小值。*arXiv 预印本 arXiv:1609.04836*，2016。
- en: 'Killamsetty et al. [2021] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh
    Ramakrishnan, and Rishabh Iyer. Glister: Generalization based data subset selection
    for efficient and robust learning. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, volume 35, pages 8110–8118, 2021.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Killamsetty 等 [2021] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh
    Ramakrishnan, 和 Rishabh Iyer。Glister: 基于泛化的数据子集选择用于高效和鲁棒的学习。在 *AAAI 人工智能会议论文集*，第
    35 卷，页码 8110–8118，2021。'
- en: 'Kirsch et al. [2019] Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald:
    Efficient and diverse batch acquisition for deep bayesian active learning. *Advances
    in neural information processing systems*, 32, 2019.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kirsch 等 [2019] Andreas Kirsch, Joost Van Amersfoort, 和 Yarin Gal. Batchbald:
    高效且多样化的批量获取用于深度贝叶斯主动学习。*神经信息处理系统进展*，32，2019。'
- en: 'Koh et al. [2021] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael
    Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas
    Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts.
    In *International Conference on Machine Learning*, pages 5637–5664\. PMLR, 2021.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Koh 等 [2021] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie,
    Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas
    Phillips, Irena Gao 等。Wilds: 真实环境下分布转移的基准。在 *国际机器学习大会*，页码 5637–5664。PMLR，2021。'
- en: Körner and Wrobel [2006] Christine Körner and Stefan Wrobel. Multi-class ensemble-based
    active learning. In *European conference on machine learning*, pages 687–694.
    Springer, 2006.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Körner 和 Wrobel [2006] Christine Körner 和 Stefan Wrobel。基于多类集成的主动学习。在 *欧洲机器学习会议*，页码
    687–694。Springer，2006。
- en: 'Kothawade et al. [2021] Suraj Kothawade, Nathan Beck, Krishnateja Killamsetty,
    and Rishabh Iyer. Similar: Submodular information measures based active learning
    in realistic scenarios. *Advances in Neural Information Processing Systems*, 34,
    2021.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kothawade 等 [2021] Suraj Kothawade, Nathan Beck, Krishnateja Killamsetty, 和
    Rishabh Iyer。Similar: 基于子模信息度量的主动学习在现实场景中的应用。*神经信息处理系统进展*，34，2021。'
- en: Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
    layers of features from tiny images. 2009.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 [2009] Alex Krizhevsky, Geoffrey Hinton 等。学习来自小图像的多层特征。2009。
- en: 'Kumar and Gupta [2020] Punit Kumar and Atul Gupta. Active learning query strategies
    for classification, regression, and clustering: a survey. *Journal of Computer
    Science and Technology*, 35(4):913–945, 2020.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 和 Gupta [2020] Punit Kumar 和 Atul Gupta。分类、回归和聚类的主动学习查询策略：综述。*计算机科学与技术期刊*，35(4)：913–945，2020。
- en: Kurakin et al. [2016] Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial
    examples in the physical world, 2016.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurakin 等人 [2016] 阿列克谢·库拉金、伊恩·古德费洛、萨米·本吉奥 等。物理世界中的对抗样本，2016。
- en: Le and Yang [2015] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge.
    *CS 231N*, 7(7):3, 2015.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 和 Yang [2015] 雅·勒 和 宣·杨。Tiny imagenet 视觉识别挑战。*CS 231N*，7(7)：3，2015。
- en: Liu et al. [2022] Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin,
    Xiangyang Ji, and Antoni B Chan. An empirical study on distribution shift robustness
    from the perspective of pre-training and data augmentation. *arXiv preprint arXiv:2205.12753*,
    2022.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2022] 子全·刘、易·徐、元洪·徐、齐·钱、浩·李、荣·金、向阳·季 和 安东尼·B·陈。从预训练和数据增强的角度对分布偏移鲁棒性的实证研究。*arXiv
    预印本 arXiv:2205.12753*，2022。
- en: Munjal et al. [2020] Prateek Munjal, Nasir Hayat, Munawar Hayat, Jamshid Sourati,
    and Shadab Khan. Towards robust and reproducible active learning using neural
    networks. *arXiv preprint arXiv:2002.09564*, 2020.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Munjal 等人 [2020] 普拉提克·穆贾尔、纳西尔·哈亚特、穆纳瓦尔·哈亚特、贾姆希德·苏拉蒂 和 沙达布·汗。利用神经网络进行鲁棒和可重复的主动学习。*arXiv
    预印本 arXiv:2002.09564*，2020。
- en: Naseem et al. [2021] Usman Naseem, Matloob Khushi, Shah Khalid Khan, Kamran
    Shaukat, and Mohammad Ali Moni. A comparative analysis of active learning for
    biomedical text mining. *Applied System Innovation*, 4(1):23, 2021.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naseem 等人 [2021] 乌斯曼·纳西姆、马图布·库希、沙赫·卡利德·汗、卡姆兰·肖卡特 和 穆罕默德·阿里·莫尼。生物医学文本挖掘的主动学习比较分析。*应用系统创新*，4(1)：23，2021。
- en: Netzer et al. [2011] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco,
    Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature
    learning. 2011.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Netzer 等人 [2011] 尤瓦尔·内策尔、陶·王、亚当·科茨、亚历山德罗·比萨科、博·吴 和 安德鲁·Y·吴。在自然图像中读取数字的无监督特征学习。2011。
- en: Nguyen et al. [2019] Vu-Linh Nguyen, Sébastien Destercke, and Eyke Hüllermeier.
    Epistemic uncertainty sampling. In *International Conference on Discovery Science*,
    pages 72–86\. Springer, 2019.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人 [2019] 武林·阮、塞巴斯蒂安·德斯特克 和 艾克·赫勒梅耶。认知不确定性采样。在 *国际发现科学会议*，第 72–86 页。斯普林格，2019。
- en: 'Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.
    *Advances in neural information processing systems*, 32, 2019.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke 等人 [2019] 亚当·帕兹克、萨姆·格罗斯、弗朗西斯科·马萨、亚当·勒雷、詹姆斯·布拉德伯里、格雷戈里·查南、特雷弗·基林、泽明·林、娜塔利亚·吉梅尔斯赫因、卢卡·安蒂加
    等。PyTorch：一种命令式风格的高性能深度学习库。*神经信息处理系统进展*，32，2019。
- en: 'Pedregosa et al. [2011] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort,
    Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,
    Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. *the
    Journal of machine Learning research*, 12:2825–2830, 2011.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pedregosa 等人 [2011] 法比安·佩德雷戈萨、盖尔·瓦罗克斯、亚历山大·格朗福、文森特·米歇尔、贝特朗·蒂里翁、奥利维耶·格里塞尔、马修·布隆德尔、彼得·普雷滕霍费、罗恩·维斯、文森特·迪布尔
    和 其他人。Scikit-learn：Python 中的机器学习。*机器学习研究期刊*，12：2825–2830，2011。
- en: Pereira-Santos et al. [2019] Davi Pereira-Santos, Ricardo Bastos Cavalcante
    Prudêncio, and André CPLF de Carvalho. Empirical investigation of active learning
    strategies. *Neurocomputing*, 326:15–27, 2019.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pereira-Santos 等人 [2019] 达维·佩雷拉-桑托斯、里卡多·巴斯托斯·卡瓦尔坎特·普鲁登西奥 和 安德烈·CPLF·德·卡瓦略。主动学习策略的实证研究。*神经计算*，326：15–27，2019。
- en: Pinsler et al. [2019] Robert Pinsler, Jonathan Gordon, Eric Nalisnick, and José Miguel
    Hernández-Lobato. Bayesian batch active learning as sparse subset approximation.
    *Advances in neural information processing systems*, 32:6359–6370, 2019.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinsler 等人 [2019] 罗伯特·平斯勒、乔纳森·戈登、埃里克·纳利斯尼克 和 霍塞·米格尔·埃尔南德斯-洛巴托。贝叶斯批量主动学习作为稀疏子集近似。*神经信息处理系统进展*，32：6359–6370，2019。
- en: 'Ramirez-Loaiza et al. [2017] Maria E Ramirez-Loaiza, Manali Sharma, Geet Kumar,
    and Mustafa Bilgic. Active learning: an empirical study of common baselines. *Data
    mining and knowledge discovery*, 31(2):287–313, 2017.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramirez-Loaiza 等人 [2017] 玛利亚·E·拉米雷斯-洛阿萨、马纳利·香玛、吉特·库马尔 和 穆斯塔法·比尔吉克。主动学习：常见基线的实证研究。*数据挖掘与知识发现*，31(2)：287–313，2017。
- en: Ren et al. [2021] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui
    Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning.
    *ACM Computing Surveys (CSUR)*, 54(9):1–40, 2021.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人 [2021] 彭臻·任、云·肖、肖军·常、博耀·黄、智辉·李、布里吉·B·古普塔、小江·陈 和 欣·王。深度主动学习的调查。*ACM计算机调查（CSUR）*，54(9)：1–40，2021。
- en: Roy et al. [2018] Soumya Roy, Asim Unmesh, and Vinay P Namboodiri. Deep active
    learning for object detection. In *BMVC*, volume 362, page 91, 2018.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy 等人 [2018] 苏摩亚·罗伊、阿西姆·安梅什 和 维奈·P·南布迪里。用于目标检测的深度主动学习。在 *BMVC*，第 362 卷，第 91
    页，2018。
- en: Sagawa et al. [2019] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and
    Percy Liang. Distributionally robust neural networks. In *International Conference
    on Learning Representations*, 2019.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sagawa 等 [2019] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto 和 Percy Liang.
    分布鲁棒神经网络。在 *国际学习表征会议*，2019。
- en: Saidu and Csató [2021] Isah Charles Saidu and Lehel Csató. Active learning with
    bayesian unet for efficient semantic image segmentation. *Journal of Imaging*,
    7(2):37, 2021.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saidu 和 Csató [2021] Isah Charles Saidu 和 Lehel Csató. 使用贝叶斯 Unet 进行高效语义图像分割的主动学习。*成像期刊*，7(2):37，2021。
- en: 'Schein and Ungar [2007] Andrew I Schein and Lyle H Ungar. Active learning for
    logistic regression: an evaluation. *Machine Learning*, 68(3):235–265, 2007.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schein 和 Ungar [2007] Andrew I Schein 和 Lyle H Ungar. 针对逻辑回归的主动学习：评估。*机器学习*，68(3):235–265，2007。
- en: 'Sener and Savarese [2017] Ozan Sener and Silvio Savarese. Active learning for
    convolutional neural networks: A core-set approach. *arXiv preprint arXiv:1708.00489*,
    2017.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sener 和 Savarese [2017] Ozan Sener 和 Silvio Savarese. 卷积神经网络的主动学习：核心集方法。*arXiv
    预印本 arXiv:1708.00489*，2017。
- en: 'Senge et al. [2014] Robin Senge, Stefan Bösner, Krzysztof Dembczyński, Jörg
    Haasenritter, Oliver Hirsch, Norbert Donner-Banzhoff, and Eyke Hüllermeier. Reliable
    classification: Learning classifiers that distinguish aleatoric and epistemic
    uncertainty. *Information Sciences*, 255:16–29, 2014.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Senge 等 [2014] Robin Senge, Stefan Bösner, Krzysztof Dembczyński, Jörg Haasenritter,
    Oliver Hirsch, Norbert Donner-Banzhoff 和 Eyke Hüllermeier. 可靠分类：学习能够区分随机不确定性和知识性不确定性的分类器。*信息科学*，255:16–29，2014。
- en: Settles [2009] Burr Settles. Active learning literature survey. 2009.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Settles [2009] Burr Settles. 主动学习文献综述。2009。
- en: Settles and Craven [2008] Burr Settles and Mark Craven. An analysis of active
    learning strategies for sequence labeling tasks. In *proceedings of the 2008 conference
    on empirical methods in natural language processing*, pages 1070–1079, 2008.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Settles 和 Craven [2008] Burr Settles 和 Mark Craven. 针对序列标注任务的主动学习策略分析。在 *2008年自然语言处理经验方法会议论文集*，页1070–1079，2008。
- en: Shannon [2001] Claude Elwood Shannon. A mathematical theory of communication.
    *ACM SIGMOBILE mobile computing and communications review*, 5(1):3–55, 2001.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shannon [2001] Claude Elwood Shannon. 通信的数学理论。*ACM SIGMOBILE 移动计算与通信评论*，5(1):3–55，2001。
- en: Shen et al. [2017] Yanyao Shen, Hyokun Yun, Zachary C Lipton, Yakov Kronrod,
    and Animashree Anandkumar. Deep active learning for named entity recognition.
    *arXiv preprint arXiv:1707.05928*, 2017.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等 [2017] Yanyao Shen, Hyokun Yun, Zachary C Lipton, Yakov Kronrod 和 Animashree
    Anandkumar. 用于命名实体识别的深度主动学习。*arXiv 预印本 arXiv:1707.05928*，2017。
- en: 'Shui et al. [2020] Changjian Shui, Fan Zhou, Christian Gagné, and Boyu Wang.
    Deep active learning: Unified and principled method for query and training. In
    *International Conference on Artificial Intelligence and Statistics*, pages 1308–1318\.
    PMLR, 2020.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shui 等 [2020] Changjian Shui, Fan Zhou, Christian Gagné 和 Boyu Wang. 深度主动学习：统一和原则化的查询和训练方法。在
    *国际人工智能与统计学会议*，页1308–1318。PMLR，2020。
- en: Sinha et al. [2019] Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational
    adversarial active learning. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 5972–5981, 2019.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinha 等 [2019] Samarth Sinha, Sayna Ebrahimi 和 Trevor Darrell. 变分对抗主动学习。在 *IEEE/CVF
    国际计算机视觉会议论文集*，页5972–5981，2019。
- en: 'Sivaraman and Trivedi [2014] Sayanan Sivaraman and Mohan M Trivedi. Active
    learning for on-road vehicle detection: A comparative study. *Machine vision and
    applications*, 25(3):599–611, 2014.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sivaraman 和 Trivedi [2014] Sayanan Sivaraman 和 Mohan M Trivedi. 路面车辆检测的主动学习：比较研究。*机器视觉与应用*，25(3):599–611，2014。
- en: Spanhol et al. [2015] Fabio A Spanhol, Luiz S Oliveira, Caroline Petitjean,
    and Laurent Heutte. A dataset for breast cancer histopathological image classification.
    *Ieee transactions on biomedical engineering*, 63(7):1455–1462, 2015.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spanhol 等 [2015] Fabio A Spanhol, Luiz S Oliveira, Caroline Petitjean 和 Laurent
    Heutte. 用于乳腺癌组织病理图像分类的数据集。*IEEE 生物医学工程学报*，63(7):1455–1462，2015。
- en: Tran et al. [2019] Toan Tran, Thanh-Toan Do, Ian Reid, and Gustavo Carneiro.
    Bayesian generative active deep learning. In *International Conference on Machine
    Learning*, pages 6295–6304\. PMLR, 2019.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 等 [2019] Toan Tran, Thanh-Toan Do, Ian Reid 和 Gustavo Carneiro. 贝叶斯生成主动深度学习。在
    *国际机器学习会议*，页6295–6304。PMLR，2019。
- en: Tuia et al. [2011] Devis Tuia, Michele Volpi, Loris Copa, Mikhail Kanevski,
    and Jordi Munoz-Mari. A survey of active learning algorithms for supervised remote
    sensing image classification. *IEEE Journal of Selected Topics in Signal Processing*,
    5(3):606–617, 2011.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tuia 等 [2011] Devis Tuia, Michele Volpi, Loris Copa, Mikhail Kanevski 和 Jordi
    Munoz-Mari. 监督遥感图像分类的主动学习算法综述。*IEEE 选择信号处理期刊*，5(3):606–617，2011年。
- en: Wang and Shang [2014] Dan Wang and Yi Shang. A new active labeling method for
    deep learning. In *2014 International joint conference on neural networks (IJCNN)*,
    pages 112–119\. IEEE, 2014.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Shang [2014] Dan Wang 和 Yi Shang. 一种用于深度学习的新型主动标注方法。在*2014 国际神经网络联合会议
    (IJCNN)*，第112–119页。IEEE，2014年。
- en: Wang et al. [2016] Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, and Liang Lin.
    Cost-effective active learning for deep image classification. *IEEE Transactions
    on Circuits and Systems for Video Technology*, 27(12):2591–2600, 2016.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2016] Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang 和 Liang Lin. 针对深度图像分类的成本效益主动学习。*IEEE
    视频技术电路与系统汇刊*，27(12):2591–2600，2016年。
- en: 'Wang and Hua [2011] Meng Wang and Xian-Sheng Hua. Active learning in multimedia
    annotation and retrieval: A survey. *ACM Transactions on Intelligent Systems and
    Technology (TIST)*, 2(2):1–21, 2011.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 和 Hua [2011] Meng Wang 和 Xian-Sheng Hua. 多媒体注释和检索中的主动学习: 一项综述。*ACM 智能系统与技术事务
    (TIST)*，2(2):1–21，2011年。'
- en: Wang et al. [2021] Tianyang Wang, Xingjian Li, Pengkun Yang, Guosheng Hu, Xiangrui
    Zeng, Siyu Huang, Cheng-Zhong Xu, and Min Xu. Boosting active learning via improving
    test performance. *arXiv preprint arXiv:2112.05683*, 2021.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2021] Tianyang Wang, Xingjian Li, Pengkun Yang, Guosheng Hu, Xiangrui
    Zeng, Siyu Huang, Cheng-Zhong Xu 和 Min Xu. 通过提高测试性能来增强主动学习。*arXiv 预印本 arXiv:2112.05683*，2021年。
- en: Wei et al. [2015] Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data
    subset selection and active learning. In *International conference on machine
    learning*, pages 1954–1963\. PMLR, 2015.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 [2015] Kai Wei, Rishabh Iyer 和 Jeff Bilmes. 数据子集选择和主动学习中的次模性。在*国际机器学习会议*，第1954–1963页。PMLR，2015年。
- en: 'Wu et al. [2020] Jian Wu, Victor S Sheng, Jing Zhang, Hua Li, Tetiana Dadakova,
    Christine Leon Swisher, Zhiming Cui, and Pengpeng Zhao. Multi-label active learning
    algorithms for image classification: Overview and future promise. *ACM Computing
    Surveys (CSUR)*, 53(2):1–35, 2020.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等 [2020] Jian Wu, Victor S Sheng, Jing Zhang, Hua Li, Tetiana Dadakova,
    Christine Leon Swisher, Zhiming Cui 和 Pengpeng Zhao. 图像分类的多标签主动学习算法: 概述与未来展望。*ACM
    计算机调查 (CSUR)*，53(2):1–35，2020年。'
- en: 'Xiao et al. [2017] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist:
    a novel image dataset for benchmarking machine learning algorithms. *arXiv preprint
    arXiv:1708.07747*, 2017.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等 [2017] Han Xiao, Kashif Rasul 和 Roland Vollgraf. Fashion-mnist: 一个用于机器学习算法基准测试的新图像数据集。*arXiv
    预印本 arXiv:1708.07747*，2017年。'
- en: Xie et al. [2021] Yichen Xie, Masayoshi Tomizuka, and Wei Zhan. Towards general
    and efficient active learning. *arXiv preprint arXiv:2112.07963*, 2021.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等 [2021] Yichen Xie, Masayoshi Tomizuka 和 Wei Zhan. 朝着通用且高效的主动学习迈进。*arXiv
    预印本 arXiv:2112.07963*，2021年。
- en: Yin et al. [2017] Changchang Yin, Buyue Qian, Shilei Cao, Xiaoyu Li, Jishang
    Wei, Qinghua Zheng, and Ian Davidson. Deep similarity-based batch mode active
    learning with exploration-exploitation. In *2017 IEEE International Conference
    on Data Mining (ICDM)*, pages 575–584\. IEEE, 2017.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等 [2017] Changchang Yin, Buyue Qian, Shilei Cao, Xiaoyu Li, Jishang Wei,
    Qinghua Zheng 和 Ian Davidson. 基于深度相似性的批量模式主动学习与探索-利用。 在*2017 IEEE 国际数据挖掘会议 (ICDM)*，第575–584页。IEEE，2017年。
- en: Yoo and Kweon [2019] Donggeun Yoo and In So Kweon. Learning loss for active
    learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 93–102, 2019.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoo 和 Kweon [2019] Donggeun Yoo 和 In So Kweon. 主动学习的学习损失。在*IEEE/CVF 计算机视觉与模式识别会议论文集*，第93–102页，2019年。
- en: Zhan et al. [2021a] Xueying Zhan, Qing Li, and Antoni B Chan. Multiple-criteria
    based active learning with fixed-size determinantal point processes. *arXiv preprint
    arXiv:2107.01622*, 2021a.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhan 等 [2021a] Xueying Zhan, Qing Li 和 Antoni B Chan. 基于多个标准的主动学习与固定大小的行列式点过程。*arXiv
    预印本 arXiv:2107.01622*，2021年a。
- en: 'Zhan et al. [2021b] Xueying Zhan, Huan Liu, Qing Li, and Antoni B. Chan. A
    comparative survey: Benchmarking for pool-based active learning. In Zhi-Hua Zhou,
    editor, *Proceedings of the Thirtieth International Joint Conference on Artificial
    Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021*,
    pages 4679–4686. ijcai.org, 2021b.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhan 等 [2021b] Xueying Zhan, Huan Liu, Qing Li 和 Antoni B. Chan. 一项比较综述: 基于池的主动学习基准测试。在
    Zhi-Hua Zhou 编辑的*第三十届国际人工智能联合会议 IJCAI 2021 论文集，虚拟会议 / 加拿大蒙特利尔，2021年8月19-27日*，第4679–4686页。ijcai.org，2021年b。'
- en: 'Zhao et al. [2020] Zhen Zhao, Miaojing Shi, Xiaoxiao Zhao, and Li Li. Active
    crowd counting with limited supervision. In *Computer Vision–ECCV 2020: 16th European
    Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XX 16*, pages 565–581.
    Springer, 2020.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '赵振等 [2020] 赵振、苗静、赵晓晓和李李。有限监督下的主动人群计数。在*计算机视觉–ECCV 2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23-28日，会议录，第XX部分
    16*，第565-581页。施普林格，2020年。'
- en: Zhdanov [2019] Fedor Zhdanov. Diverse mini-batch active learning. *arXiv preprint
    arXiv:1901.05954*, 2019.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 兹赫达诺夫 [2019] 费多尔·兹赫达诺夫。多样化小批量主动学习。*arXiv 预印本 arXiv:1901.05954*，2019年。
- en: Zhu and Bento [2017] Jia-Jie Zhu and José Bento. Generative adversarial active
    learning. *arXiv preprint arXiv:1702.07956*, 2017.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱佳杰和何塞·本托 [2017]。生成对抗主动学习。*arXiv 预印本 arXiv:1702.07956*，2017年。
- en: 'Appendix A Related Work: comparison with other existing DAL toolkits'
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 相关工作：与其他现有DAL工具包的比较
- en: 'The existing major DAL toolkits/libraries include:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现有主要的DAL工具包/库包括：
- en: •
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our previous work *DeepAL*¹¹1[https://github.com/ej0cl6/deep-active-learning](https://github.com/ej0cl6/deep-active-learning)
    [[27](#bib.bib27)].
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们之前的工作*DeepAL*¹¹1[https://github.com/ej0cl6/deep-active-learning](https://github.com/ej0cl6/deep-active-learning)
    [[27](#bib.bib27)]。
- en: •
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Pytorch Active Learning (PAL) Library ²²2[https://github.com/rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning).
    Accompanied with the book Human-in-the-Loop Machine Learning.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pytorch Active Learning (PAL)库 ²²2[https://github.com/rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning)。附有《人机协作机器学习》一书。
- en: •
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DISTIL library³³3[https://github.com/decile-team/distil](https://github.com/decile-team/distil).
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DISTIL库³³3[https://github.com/decile-team/distil](https://github.com/decile-team/distil)。
- en: Compared with our previous work *DeepAL*, we 1) add more built-in support datasets/tasks,
    where *DeepAL* only support *CIFAR10*, *MNIST*, *FashionMNIST* and *SVHN*, while
    in *$\text{DeepAL}^{+}$*, we add *EMNIST*, *TinyImageNet*, *PneumoniaMNIST*, *BreakHis*
    and wilds-series tasks [[35](#bib.bib35)] like *waterbirds*. 2) We then optimize
    part of existing algorithms to make it better to be adopted on Deep Active Learning
    tasks like KMeans, we re-implemented it by using faiss-gpu libirary [[4](#bib.bib4)],
    it is much faster and perform better than scikit-learn library [[48](#bib.bib48)]
    based KMeans implementation. We conduct Principal Component Analysis (PCA) to
    reduce dimension on representativeness-based approach, i.e., KCenter since it
    costs too much memory for storing pair-wise similarity matrix on DAL tasks. Note
    that both *DeepAL* and *$\text{DeepAL}^{+}$* remove CoreSet approach [[57](#bib.bib57)]
    since Coreset uses the greedy 2-OPT solution for the k-Center problem as an initialization
    and checks the feasibility of a mixed integer program (MIP). They adopted Gourbi
    optimizer⁴⁴4[https://www.gurobi.com/](https://www.gurobi.com/) to solve MIP and
    it is not a free optimizer. The users can use KCenter, a greedy optimization of
    CoreSet. 3) We Add more independent DAL methods implementations like MeanSTD,
    VarRatio, BADGE, LPL, VAAL, WAAL and CEAL.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前的工作*DeepAL*相比，我们 1) 增加了更多的内置支持数据集/任务，其中*DeepAL*仅支持*CIFAR10*、*MNIST*、*FashionMNIST*和*SVHN*，而在*$\text{DeepAL}^{+}$*中，我们新增了*EMNIST*、*TinyImageNet*、*PneumoniaMNIST*、*BreakHis*以及像*waterbirds*这样的wilds系列任务
    [[35](#bib.bib35)]。2) 然后我们优化了部分现有算法，使其更适用于深度主动学习任务，比如KMeans，我们通过使用faiss-gpu库 [[4](#bib.bib4)]
    重新实现了它，比基于scikit-learn库 [[48](#bib.bib48)] 的KMeans实现要快得多，性能也更好。我们对代表性基础的方法，如KCenter，进行了主成分分析（PCA）以减少维度，因为在DAL任务中存储成对相似度矩阵需要太多内存。注意，*DeepAL*和*$\text{DeepAL}^{+}$*都移除了CoreSet方法
    [[57](#bib.bib57)]，因为CoreSet使用贪婪的2-OPT解决k-Center问题作为初始化，并检查混合整数程序（MIP）的可行性。他们采用了Gourbi优化器⁴⁴4[https://www.gurobi.com/](https://www.gurobi.com/)
    来解决MIP，这不是一个免费的优化器。用户可以使用KCenter，它是CoreSet的贪婪优化。3) 我们增加了更多独立的DAL方法实现，如MeanSTD、VarRatio、BADGE、LPL、VAAL、WAAL和CEAL。
- en: PAL library is a fundamental human-in-loop framework. Users need to interact
    with computers by inputting the ground truth labels of the instance asked by the
    computer. Besides typical uncertainty-/representativeness-/diversity-/adaptive-based
    AL approaches like Least Confidence, PAL also includes AL with transfer learning
    (ALTL). PAL is more likely to provide a template and tell people how to apply
    AL to different human-in-loop tasks. If someone is new to the AL research field
    and he could try to use this library to understand how AL works.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: PAL库是一个基础的人机协作框架。用户需要通过输入计算机要求的实例的真实标签来与计算机互动。除了典型的基于不确定性/代表性/多样性/自适应的主动学习方法，如Least
    Confidence，PAL还包括转移学习的主动学习（ALTL）。PAL更倾向于提供一个模板，并告诉人们如何将主动学习应用于不同的人机协作任务。如果有人对主动学习研究领域不熟悉，他可以尝试使用这个库来了解主动学习是如何工作的。
- en: DISTIL library majorly serves for the submodular functions proposed by the authors
    group [[37](#bib.bib37)], besides the implementations that are already implemented
    in *DeepAL* and *$\text{DeepAL}^{+}$*, they have own implementations like FASS
    [[73](#bib.bib73)], BatchBALD [[34](#bib.bib34)] (we have also implemented this
    method, it is based on BALD, but it is really memory consuming so we finally deleted
    it.), Glister [[33](#bib.bib33)] (for robust learning) and Submodular (Conditional)
    Mutual Information (S(C)MI) [[37](#bib.bib37)] for AL. It is a easy-to-use library
    especially if someone want to use their submodular functions. They have no implementations
    of AL with enhanced techniques like LPL and WAAL.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: DISTIL库主要用于作者组提出的子模函数[[37](#bib.bib37)]，除了*DeepAL*和*$\text{DeepAL}^{+}$*中已经实现的功能外，他们还有如FASS
    [[73](#bib.bib73)]、BatchBALD [[34](#bib.bib34)]（我们也实现了这种方法，它基于BALD，但真的很占用内存，所以我们最后删除了它）、Glister
    [[33](#bib.bib33)]（用于鲁棒学习）和子模（条件）互信息（S(C)MI）[[37](#bib.bib37)]用于AL。他们的库使用起来很方便，尤其是如果有人想使用他们的子模函数。他们没有LPL和WAAL等增强技术的AL实现。
- en: 'We made a brief comparison between our *$\text{DeepAL}^{+}$* and existing DAL
    libraries, see Table [2](#A1.T2 "Table 2 ‣ Appendix A Related Work: comparison
    with other existing DAL toolkits ‣ A Comparative Survey of Deep Active Learning").'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对我们的*$\text{DeepAL}^{+}$*与现有的DAL库进行了简要比较，见表[2](#A1.T2 "Table 2 ‣ Appendix
    A Related Work: comparison with other existing DAL toolkits ‣ A Comparative Survey
    of Deep Active Learning")。'
- en: '| Toolkit | # of implemetations | Comparison with *$\text{DeepAL}^{+}$* |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 工具包 | 实现数量 | 与*$\text{DeepAL}^{+}$*的比较 |'
- en: '| *DeepAL* | 11 | Our previous work, we updated it. |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| *DeepAL* | 11 | 我们之前的工作，我们进行了更新。 |'
- en: '| *PAL* | 11 | It contains ALTL methods, *$\text{DeepAL}^{+}$* have more concrete
    algorithm re-implementations |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| *PAL* | 11 | 包含ALTL方法，*$\text{DeepAL}^{+}$*具有更多具体的算法重新实现 |'
- en: '| *DISTIL* | 20 | Majorly for submodular related functions implementations,
    no AL with enhanced techniques implementations. |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| *DISTIL* | 20 | 主要用于子模函数的实现，没有采用增强技术的AL实现。 |'
- en: '| *$\text{DeepAL}^{+}$* | $19$ | $-$ |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| *$\text{DeepAL}^{+}$* | $19$ | $-$ |'
- en: 'Table 2: Comparison between our *$\text{DeepAL}^{+}$* and existing DAL libraries.
    We excluded Random since strictly speaking, it does not belong to AL approaches.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：我们*$\text{DeepAL}^{+}$*与现有DAL库的比较。我们排除了Random，因为严格来说，它不属于AL方法。
- en: Appendix B More introduction of DeepAL$+$ toolkit
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 对DeepAL$+$工具包的更多介绍
- en: 'We listed some introductions of our *$\text{DeepAL}^{+}$* in previous Section
     [A](#A1 "Appendix A Related Work: comparison with other existing DAL toolkits
    ‣ A Comparative Survey of Deep Active Learning"). *$\text{DeepAL}^{+}$* is user-friendly,
    using a single command can run experiments, we construct the framework/benchmark
    by easy-to-separate mode, we split the basic networks, querying strategies, dataset/task
    design, and parameters add-in (e.g., set numbers of training epochs, optimizer
    parameters). It is simple to add new AL sampling strategies, new basic backbones,
    and new datasets/tasks in these benchmarks. It makes the users propose new AL
    sampling strategies easier, test new methods on multiple basic tasks, and compare
    them with most SOTA DAL methods. We sincerely hope our *$\text{DeepAL}^{+}$* would
    help researchers in the DAL research field reduce unnecessary workload and focus
    on designing new DAL approaches more. This work is ongoing; we would continually
    add the latest and well-perform DAL approaches and incorporate more datasets/tasks.
    Moreover, if newly proposed DAL methods are designed based on *DeepAL* or *$\text{DeepAL}^{+}$*,
    it would be easier to be further incorporated into our toolkit like BADGE and
    WAAL.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在之前的[A](#A1 "Appendix A Related Work: comparison with other existing DAL
    toolkits ‣ A Comparative Survey of Deep Active Learning")部分中列出了一些对我们*$\text{DeepAL}^{+}$*的介绍。*$\text{DeepAL}^{+}$*使用方便，使用单个命令即可运行实验，我们通过易于分离的模式构建框架/基准，我们分割了基础网络、查询策略、数据集/任务设计和参数添加（例如，设置训练轮次、优化器参数）。在这些基准中添加新的AL采样策略、新的基础骨干网络以及新的数据集/任务都很简单。这使得用户更容易提出新的AL采样策略，在多个基础任务上测试新方法，并与大多数SOTA
    DAL方法进行比较。我们真诚希望我们的*$\text{DeepAL}^{+}$*能帮助DAL研究领域的研究人员减少不必要的工作量，更多地专注于设计新的DAL方法。该工作仍在进行中；我们将继续添加最新和表现良好的DAL方法，并纳入更多的数据集/任务。此外，如果新提出的DAL方法是基于*DeepAL*或*$\text{DeepAL}^{+}$*设计的，它将更容易进一步纳入我们的工具包，如BADGE和WAAL。'
- en: Appendix C Licences
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 许可证
- en: Datasets.
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集。
- en: 'We listed the licence of datasets we used in our experiments, all datasets
    employed in our comparative experiments are public datasets:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了我们实验中使用的数据集的许可证，所有在比较实验中使用的数据集都是公开数据集：
- en: •
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CIFAR10 and CIFAR100 [[38](#bib.bib38)]: MIT Licence.'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CIFAR10 和 CIFAR100 [[38](#bib.bib38)]: MIT 许可证。'
- en: •
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MNIST [[14](#bib.bib14)], EMNIST [[13](#bib.bib13)]: Creative Commons Attribution-Share
    Alike 3.0 license.'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MNIST [[14](#bib.bib14)], EMNIST [[13](#bib.bib13)]: 创用CC署名-相同方式共享 3.0 许可证。'
- en: •
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FashionMNIST [[75](#bib.bib75)]: MIT Licence.'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'FashionMNIST [[75](#bib.bib75)]: MIT 许可证。'
- en: •
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'PneumoniaMNIST [[31](#bib.bib31)]: CC BY 4.0 License.'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'PneumoniaMNIST [[31](#bib.bib31)]: CC BY 4.0 许可证。'
- en: •
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BreakHis [[66](#bib.bib66)]: Creative Commons Attribution 4.0 International
    License.'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BreakHis [[66](#bib.bib66)]: 创用CC署名 4.0 国际许可证。'
- en: •
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Waterbird [[54](#bib.bib54), [35](#bib.bib35)]: MIT License.'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Waterbird [[54](#bib.bib54), [35](#bib.bib35)]: MIT 许可证。'
- en: Methods.
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方法。
- en: 'We listed all related license of the original implementations of DAL methods
    that we re-implemented and basic backbone models in our *DeepAL$+$* toolkit:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了所有相关的原始 DAL 方法实现的许可证，以及我们在 *DeepAL$+$* 工具包中重新实现的基础骨干模型：
- en: •
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'PyTorch [[47](#bib.bib47)]: Modified BSD.'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'PyTorch [[47](#bib.bib47)]: 修改版 BSD。'
- en: •
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scikit-Learn [[48](#bib.bib48)]: BSD License.'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Scikit-Learn [[48](#bib.bib48)]: BSD 许可证。'
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BADGE [[2](#bib.bib2)]: Not listed.'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BADGE [[2](#bib.bib2)]: 未列出。'
- en: •
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LPL [[78](#bib.bib78)]: Not listed.'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'LPL [[78](#bib.bib78)]: 未列出。'
- en: •
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VAAL [[64](#bib.bib64)]: BSD 2-Clause “Simplified” License.'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'VAAL [[64](#bib.bib64)]: BSD 2-Clause “简化版”许可证。'
- en: •
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'WAAL [[63](#bib.bib63)]: Not listed.'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'WAAL [[63](#bib.bib63)]: 未列出。'
- en: •
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CEAL [[70](#bib.bib70)]: Not listed.'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CEAL [[70](#bib.bib70)]: 未列出。'
- en: •
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Methods originated from DeepAL [[27](#bib.bib27)] library implementation: MIT
    Licence.'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 起源于 DeepAL [[27](#bib.bib27)] 库实现的方法：MIT 许可证。
- en: •
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'KMeans (faiss library [[28](#bib.bib28)] implementation): MIT Licence.'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'KMeans (faiss 库 [[28](#bib.bib28)] 实现): MIT 许可证。'
- en: •
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ResNet18 [[24](#bib.bib24)]: MIT License.'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ResNet18 [[24](#bib.bib24)]: MIT 许可证。'
- en: Appendix D Experimental Settings
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 实验设置
- en: D.1 Datasets
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 数据集
- en: 'Considering some DAL approaches currently only support computer vision tasks
    like VAAL, for consistency and fairness of our experiments, we adopt 1) the image
    classification tasks, similar to most DAL papers. We use the following datasets
    (details in Table [3](#A4.T3 "Table 3 ‣ D.1 Datasets ‣ Appendix D Experimental
    Settings ‣ A Comparative Survey of Deep Active Learning")): *MNIST* [[14](#bib.bib14)],
    *FashionMNIST* [[75](#bib.bib75)], *EMNIST* [[13](#bib.bib13)], *SVHN* [[45](#bib.bib45)],
    *CIFAR10* and *CIFAR100* [[38](#bib.bib38)] and *Tiny ImageNet* [[41](#bib.bib41)].
    Additionally, to explore DAL performance on imbalanced data, we construct an imbalanced
    dataset based on *CIFAR10*, called *CIFAR10-imb*, which sub-samples the training
    set with ratios of 1:2:$\cdots$:10 for classes 0 through 9\. 2) The medical image
    analysis tasks, including Breast Cancer Histopathological Image Classification
    (*BreakHis*) [[66](#bib.bib66)] and Pneumonia-MNIST (pediatric chest X-ray) (*PneumoniaMNIST*)
    [[31](#bib.bib31)]. Additionally, we adopted an object recognition with correlated
    backgrounds dataset (*Waterbird*) [[54](#bib.bib54)], it considers two classes:
    waterbird and landbird. These objected were manually mixed to water and land background,
    and waterbirds (landbirds) more frequently appearing against a water (land) background.
    It is a challenging task since DNNs might spuriously rely on background instead
    of learning to recognize semantic/object.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到一些 DAL 方法目前仅支持计算机视觉任务，如 VAAL，为了实验的一致性和公平性，我们采用了 1) 图像分类任务，类似于大多数 DAL 论文。我们使用了以下数据集（详细信息见表
    [3](#A4.T3 "Table 3 ‣ D.1 Datasets ‣ Appendix D Experimental Settings ‣ A Comparative
    Survey of Deep Active Learning")）：*MNIST* [[14](#bib.bib14)], *FashionMNIST* [[75](#bib.bib75)],
    *EMNIST* [[13](#bib.bib13)], *SVHN* [[45](#bib.bib45)], *CIFAR10* 和 *CIFAR100*
    [[38](#bib.bib38)] 以及 *Tiny ImageNet* [[41](#bib.bib41)]。此外，为了探索 DAL 在不平衡数据上的表现，我们基于
    *CIFAR10* 构建了一个不平衡数据集，称为 *CIFAR10-imb*，它对 0 到 9 类的训练集进行 1:2:$\cdots$:10 的子采样。2)
    医学图像分析任务，包括乳腺癌组织病理图像分类（*BreakHis*） [[66](#bib.bib66)] 和肺炎-MNIST（儿科胸部 X 射线）（*PneumoniaMNIST*）
    [[31](#bib.bib31)]。此外，我们还采用了一个具有相关背景的物体识别数据集（*Waterbird*） [[54](#bib.bib54)]，它考虑了两个类别：水鸟和陆鸟。这些物体被手动混合到水和陆地背景中，水鸟（陆鸟）在水（陆地）背景下更频繁出现。这是一个具有挑战性的任务，因为深度神经网络可能会虚假地依赖背景，而不是学习识别语义/物体。
- en: '| Dataset | $\#i$ | $\#u$ | $\#t$ | $b$ | $Q$ | $\#k$ | $\#e$ |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | $\#i$ | $\#u$ | $\#t$ | $b$ | $Q$ | $\#k$ | $\#e$ |'
- en: '| *MNIST* | $500$ | $59,500$ | $10,000$ | $250$ | $10,000$ | $10$ | $20$ |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| *MNIST* | $500$ | $59,500$ | $10,000$ | $250$ | $10,000$ | $10$ | $20$ |'
- en: '| *FashionMNIST* | $500$ | $59,500$ | $10,000$ | $250$ | $10,000$ | $10$ |
    $20$ |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| *FashionMNIST* | $500$ | $59,500$ | $10,000$ | $250$ | $10,000$ | $10$ |
    $20$ |'
- en: '| *EMNIST* | $1,000$ | $696,932$ | $116,323$ | $500$ | $50,000$ | $62$ | $20$
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| *EMNIST* | $1,000$ | $696,932$ | $116,323$ | $500$ | $50,000$ | $62$ | $20$
    |'
- en: '| *SVHN* | $500$ | $72,757$ | $26,032$ | $250$ | $20,000$ | $10$ | $20$ |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| *SVHN* | $500$ | $72,757$ | $26,032$ | $250$ | $20,000$ | $10$ | $20$ |'
- en: '| *CIFAR10* | $1,000$ | $49,000$ | $10,000$ | $500$ | $40,000$ | $10$ | $30$
    |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| *CIFAR10* | $1,000$ | $49,000$ | $10,000$ | $500$ | $40,000$ | $10$ | $30$
    |'
- en: '| *CIFAR100* | $1,000$ | $49,000$ | $10,000$ | $500$ | $40,000$ | $100$ | $40$
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| *CIFAR100* | $1,000$ | $49,000$ | $10,000$ | $500$ | $40,000$ | $100$ | $40$
    |'
- en: '| *Tiny ImageNet* | $1,000$ | $99,000$ | $10,000$ | $500$ | $40,000$ | $200$
    | $40$ |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| *Tiny ImageNet* | $1,000$ | $99,000$ | $10,000$ | $500$ | $40,000$ | $200$
    | $40$ |'
- en: '| *CIFAR10-imb* | $1,000$ | $26,499$ | $10,000$ | $500$ | $20,000$ | $10$ |
    $30$ |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| *CIFAR10-imb* | $1,000$ | $26,499$ | $10,000$ | $500$ | $20,000$ | $10$ |
    $30$ |'
- en: '| *BreakHis* | $100$ | $5,436$ | $2,373$ | $100$ | $5,000$ | $2$ | $10$ |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| *BreakHis* | $100$ | $5,436$ | $2,373$ | $100$ | $5,000$ | $2$ | $10$ |'
- en: '| *PneumoniaMNIST* | $100$ | $5,132$ | $5,232$ | $100$ | $5,000$ | $2$ | $10$
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| *PneumoniaMNIST* | $100$ | $5,132$ | $5,232$ | $100$ | $5,000$ | $2$ | $10$
    |'
- en: '| *Waterbird* | $100$ | $4,695$ | $5,794$ | $100$ | $4,000$ | $2$ | $10$ |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| *Waterbird* | $100$ | $4,695$ | $5,794$ | $100$ | $4,000$ | $2$ | $10$ |'
- en: 'Table 3: Datasets used in comparative experiments. $\#i$ is the size of initial
    labeled pool, $\#u$ is the size of unlabeled data pool, $\#t$ is the size of testing
    set, $\#k$ is number of categories and $\#e$ is number of epochs used to train
    the basic classifier in each AL round.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：用于对比实验的数据集。$\#i$ 是初始标记池的大小，$\#u$ 是未标记数据池的大小，$\#t$ 是测试集的大小，$\#k$ 是类别数，$\#e$
    是每个 AL 轮次中用于训练基本分类器的轮次数。
- en: D.2 Implementation details
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 实现细节
- en: 'We employed ResNet18⁵⁵5[https://pytorch.org/vision/stable/models.html#id10](https://pytorch.org/vision/stable/models.html#id10)
    [[24](#bib.bib24)] as the basic learner. On *MNIST*, *EMNIST*, *FashionMNIST*,
    *TinyImagenet*, *CIFAR10* and *CIFAR100*, we adopted *Adam* optimizer (learning
    rate: $1e-3$) . On *PneumoniaMNIST*, *BreakHis* and *Waterbird*, since Adam would
    cause overfitting, we use SGD optimizer with learning rate: 1e-2 on *BreakHis*
    and *PneumoniaMNIST*, learning rate: $0.0005$, weight decay: 1e-5, momentum: $0.9$
    on *Waterbird*.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了 ResNet18⁵⁵5[https://pytorch.org/vision/stable/models.html#id10](https://pytorch.org/vision/stable/models.html#id10)
    [[24](#bib.bib24)] 作为基本学习器。在 *MNIST*、*EMNIST*、*FashionMNIST*、*TinyImagenet*、*CIFAR10*
    和 *CIFAR100* 上，我们使用了 *Adam* 优化器（学习率：$1e-3$）。在 *PneumoniaMNIST*、*BreakHis* 和 *Waterbird*
    上，由于 Adam 会导致过拟合，我们使用了 SGD 优化器，在 *BreakHis* 和 *PneumoniaMNIST* 上学习率为：1e-2，在 *Waterbird*
    上学习率为：$0.0005$，权重衰减：1e-5，动量：$0.9$。
- en: For a fair comparison, consistent experimental settings of the basic classifier
    are used across all DAL methods. The dataset-specific implementation details are
    discussed as follows.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平比较，所有 DAL 方法中使用了一致的基本分类器实验设置。数据集特定的实现细节如下讨论。
- en: •
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*MNIST*, *FashionMNIST* and *EMNIST*: number of training epochs is $20$, the
    kernel size of the first convolutional layer in ResNet18 is $7\times 7$ (consistent
    with the original PyTorch implementation), input pre-processing step include normalization.'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*MNIST*、*FashionMNIST* 和 *EMNIST*：训练轮次为 $20$，ResNet18 中第一卷积层的卷积核大小为 $7\times
    7$（与原始 PyTorch 实现一致），输入预处理步骤包括归一化。'
- en: •
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*CIFAR10*, *CIFAR100*: number of training epochs is $30$, the kernel size of
    the first convolutional layer in ResNet18 is $3\times 3$ (consistent with PyTorch-CIFAR
    implementation⁶⁶6[https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py](https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py)),
    input pre-processing steps include random crop (pad=4), random horizontal flip
    ($p=0.5$) and normalization.'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*CIFAR10*、*CIFAR100*：训练轮次为 $30$，ResNet18 中第一卷积层的卷积核大小为 $3\times 3$（与 PyTorch-CIFAR
    实现⁶⁶6[https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py](https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py)
    一致），输入预处理步骤包括随机裁剪（pad=4）、随机水平翻转（$p=0.5$）和归一化。'
- en: •
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*TinyImageNet*: number of training epochs is $40$, the same implementation
    of ResNet18 as *CIFAR*, input pre-processing steps include random rotation (degree=20),
    random horizontal flip ($p=0.5$) and normalization.'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*TinyImageNet*：训练轮次为 $40$，与 *CIFAR* 使用相同的 ResNet18 实现，输入预处理步骤包括随机旋转（角度=20）、随机水平翻转（$p=0.5$）和归一化。'
- en: •
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*SVHN*: number of training epochs is $20$, the same implementation of ResNet18
    as *MNIST*, input pre-processing steps include normalization.'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*SVHN*：训练轮次为 $20$，与 *MNIST* 使用相同的 ResNet18 实现，输入预处理步骤包括归一化。'
- en: •
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*BreakHis*: number of training epochs is $10$, the same implementation of ResNet10
    as *CIFAR*, input pre-proccessing steps include random rotation (degree=90), random
    horizontal flip ($p=0.8$), random resize crop (scale=224), randomly change the
    brightness, contrast, saturation and hue of image – ColorJitter (brightness=0.4,
    contrast=0.4, saturation=0.4, hue=0.1) and normalization.'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*BreakHis*: 训练轮次为$10$，与*CIFAR*中的ResNet10实现相同，输入预处理步骤包括随机旋转（角度=90），随机水平翻转（$p=0.8$），随机调整裁剪（scale=224），随机改变图像的亮度、对比度、饱和度和色调—ColorJitter（亮度=0.4，对比度=0.4，饱和度=0.4，色调=0.1）以及归一化。'
- en: •
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*PneumoniaMNIST*: number of training epochs is $10$, the same implementation
    o ResNet18 as *CIFAR*, input pre-processing steps include resize (shape=255),
    center crop (shape = 224), random horizontal flip ($p=0.5$), random rotation (degree=10),
    random gray scale, random affine (translate=(0.05, 0.05), degree=0).'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*肺炎MNIST*: 训练轮次为$10$，与*CIFAR*中的ResNet18实现相同，输入预处理步骤包括调整大小（shape=255），中心裁剪（shape=224），随机水平翻转（$p=0.5$），随机旋转（角度=10），随机灰度，随机仿射（平移=(0.05,
    0.05)，角度=0）。'
- en: •
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Waterbirds*: number of training epochs is $30$, the same implementation of
    ResNet18 as *MNIST*, input pre-processing steps include random horizontal flip
    ($p=0.5$).'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*水鸟*: 训练轮次为$30$，与*MNIST*中的ResNet18实现相同，输入预处理步骤包括随机水平翻转（$p=0.5$）。'
- en: 'The model-specific implementation details are discussed as follows. For MC
    Dropout implementation, we employed $10$ forward passes. For CEAL(Entropy), we
    set threshold of confidence/entropy score for assigning pseudo labels as $1e-5$.
    For KCenter, since using the full feature vector would take too much memory in
    pair-wise distance calculation, we employ Principal components analysis (PCA)
    to reduce feature dimension to $32$ according to [[30](#bib.bib30)]. For VAE in
    VAAL, we followed the same architecture in [[64](#bib.bib64)] and train VAE with
    $30$ epochs with *Adam* optimizer (learning rate: $1e-3$). For LPL, we train LossNet
    with *Adam* optimizer (learning rate: $1e-2$); since LossNet is co-trained with
    basic classifier, we firstly co-trained LossNet and basic classifier followed
    by normal training processes, then we detached feature updating (the same as stop
    training basic classifier) and assign $20$ extra epochs for training LossNet.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 模型特定的实现细节如下讨论。对于MC Dropout实现，我们采用了$10$次前向传递。对于CEAL（熵），我们设置了伪标签分配的置信度/熵分数阈值为$1e-5$。对于KCenter，由于使用完整的特征向量在配对距离计算中需要太多内存，我们采用主成分分析（PCA）将特征维度减少到$32$，参考[[30](#bib.bib30)]。对于VAAL中的VAE，我们遵循了[[64](#bib.bib64)]中的相同架构，并用*Adam*优化器（学习率：$1e-3$）训练VAE
    $30$轮。对于LPL，我们用*Adam*优化器（学习率：$1e-2$）训练LossNet；由于LossNet与基本分类器共同训练，我们首先共同训练LossNet和基本分类器，然后进行正常的训练过程，接着我们分离特征更新（与停止训练基本分类器相同），并为训练LossNet分配$20$个额外的轮次。
- en: D.3 Experimental environments in our comparative experiments
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 我们比较实验中的实验环境
- en: We conduct experiments on a single Tesla V100-SXM2 GPU with 16GB memory except
    for running experiments on *PneumoniaMNIST* and *BreakHis*, since running them
    need $>16GB$ and $<32GB$ memories. We run experiments of *PneumoniaMNIST* and
    *BreakHis* on another single Tesla V100-SXM2 GPU with 32GB memory. We only use
    a single GPU for each experiment.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一台16GB内存的Tesla V100-SXM2 GPU上进行实验，除了在*肺炎MNIST*和*BreakHis*上运行实验，因为运行这些实验需要$>16GB$和$<32GB$内存。我们在另一台32GB内存的Tesla
    V100-SXM2 GPU上运行*肺炎MNIST*和*BreakHis*的实验。每个实验仅使用一个GPU。
- en: Appendix E Completed Experimental Results
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 完整实验结果
- en: E.1 Overall experiments
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 总体实验
- en: E.1.1 Performance of Standard Image Classification tasks.
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.1.1 标准图像分类任务的性能
- en: Tables [4](#A5.T4 "Table 4 ‣ E.1.2 Performance of Medical Image Analysis tasks.
    ‣ E.1 Overall experiments ‣ Appendix E Completed Experimental Results ‣ A Comparative
    Survey of Deep Active Learning"), [5](#A5.T5 "Table 5 ‣ E.1.2 Performance of Medical
    Image Analysis tasks. ‣ E.1 Overall experiments ‣ Appendix E Completed Experimental
    Results ‣ A Comparative Survey of Deep Active Learning"), [6](#A5.T6 "Table 6
    ‣ E.1.2 Performance of Medical Image Analysis tasks. ‣ E.1 Overall experiments
    ‣ Appendix E Completed Experimental Results ‣ A Comparative Survey of Deep Active
    Learning") record the overall performances of standard image classification tasks
    group, including *MNIST*, *FashionMNIST*, *EMNIST*, *SVHN*, *CIFAR10*, *CIFAR1O-imb*,
    *CIFAR100* and *TinyImageNet* datasets. Including AUBC (acc) performance with
    mean and standard deviation over 3 trials, the average running time that takes
    the running time of Random as unit and the F-acc score.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [4](#A5.T4 "Table 4 ‣ E.1.2 Performance of Medical Image Analysis tasks.
    ‣ E.1 Overall experiments ‣ Appendix E Completed Experimental Results ‣ A Comparative
    Survey of Deep Active Learning")、[5](#A5.T5 "Table 5 ‣ E.1.2 Performance of Medical
    Image Analysis tasks. ‣ E.1 Overall experiments ‣ Appendix E Completed Experimental
    Results ‣ A Comparative Survey of Deep Active Learning") 和 [6](#A5.T6 "Table 6
    ‣ E.1.2 Performance of Medical Image Analysis tasks. ‣ E.1 Overall experiments
    ‣ Appendix E Completed Experimental Results ‣ A Comparative Survey of Deep Active
    Learning") 记录了标准图像分类任务组的整体表现，包括 *MNIST*、*FashionMNIST*、*EMNIST*、*SVHN*、*CIFAR10*、*CIFAR1O-imb*、*CIFAR100*
    和 *TinyImageNet* 数据集。包括 AUBC (acc) 的表现，均值和标准差经过 3 次试验，平均运行时间（以随机运行时间为单位）和 F-acc
    分数。
- en: Note that KMeans(GPU) performs better than KMeans on major tasks, i.e., Table [5](#A5.T5
    "Table 5 ‣ E.1.2 Performance of Medical Image Analysis tasks. ‣ E.1 Overall experiments
    ‣ Appendix E Completed Experimental Results ‣ A Comparative Survey of Deep Active
    Learning"). However, from the average running time, KMeans(GPU) seems to have
    more time than KMeans, it does not mean KMeans(GPU) run slower than KMeans, since
    the running time calculation does not count the waiting time, e.g., wait for memory
    allocation, the time for data load from GPU to CPU or from CPU to GPU. In KMeans,
    in every AL iteration, we need to load data (feature embedding) from GPU to CPU
    and use scikit-learn library to calculate. At this step, the program must waste
    time waiting for the operating system to allocate memory for calculation. Nevertheless,
    these waiting times could be saved in KMeans(GPU). So actually KMeans(GPU) run
    faster than KMeans on DAL tasks that use GPU for calculation.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，KMeans(GPU) 在主要任务中表现优于 KMeans，即表格 [5](#A5.T5 "Table 5 ‣ E.1.2 Performance
    of Medical Image Analysis tasks. ‣ E.1 Overall experiments ‣ Appendix E Completed
    Experimental Results ‣ A Comparative Survey of Deep Active Learning")。然而，从平均运行时间来看，KMeans(GPU)
    的时间似乎比 KMeans 更长，这并不意味着 KMeans(GPU) 的运行速度比 KMeans 慢，因为运行时间计算没有包括等待时间，例如等待内存分配、数据从
    GPU 到 CPU 或从 CPU 到 GPU 的加载时间。在 KMeans 中，每次 AL 迭代时，我们需要从 GPU 加载数据（特征嵌入）到 CPU，并使用
    scikit-learn 库进行计算。在这个步骤中，程序必须浪费时间等待操作系统分配计算所需的内存。然而，这些等待时间在 KMeans(GPU) 中可以节省。因此，实际上，KMeans(GPU)
    在使用 GPU 进行计算的 DAL 任务中运行速度比 KMeans 更快。
- en: E.1.2 Performance of Medical Image Analysis tasks.
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.1.2 医学图像分析任务的性能。
- en: 'Table [7](#A5.T7 "Table 7 ‣ E.1.2 Performance of Medical Image Analysis tasks.
    ‣ E.1 Overall experiments ‣ Appendix E Completed Experimental Results ‣ A Comparative
    Survey of Deep Active Learning") records the overall performances of medical image
    analysis group, including *PneumoniaMNIST* and *BreakHis* datasets. Both LPL,
    WAAL and BADGE perform well on these medical image analysis tasks. Another thing
    that worth to pay attention is: all MC dropout based versions (i.e., LeastConfD,
    MarginD. EntropyD, as well as BALD), perform worse than original versions (i.e.,
    LeastConf, Margin and Entropy), especially on *PneumonialMNIST*. For example,
    on *PneumonialMNIST*, the AUBC value of LeastConf is 0.852, while LeastConfD only
    have 0.8243 AUBC value. A potential reason is in *PneumoniaMNIST*, to justify
    whether an image – a chest X-ray report pneumonia, one needs to check the local
    lesions and observe the lung’s overall condition. The basic learner needs both
    local and global features to make an accurate prediction. While MC dropout reduces
    the model capacity and might ignore some feature information, making less convincing
    predictions [[3](#bib.bib3)] and hurt DAL performance. Another phenomenon is,
    considering F-acc, we noticed that many DAL approaches’ F-acc are higher than
    the accuracy trained on full dataset (0.9039), e.g., 0.9149 on MarginD, 0.9204
    on BALD, 0.9179 on CEAL, 0.9197 on AdvBIM. These results can be summarized as
    one phenomenon: the subset selected from the full subset would contribute to better
    performance. This is because *PheumoniaMNIST* contains distribution/dataset shift
    between training and testing set. Also, this dataset might contain redundant data
    samples and confusing information. That is, these medical images are not one-to-one.
    They are many-to-one. One patient would correspond to several chest X-ray images,
    which causes redundancy. Additionally, some patients may have more than one disease
    e.g., we can see on some X-ray images that there is a posterior spinal fixator
    that the patient used to fix his spine. These features also might influence the
    predictions.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [7](#A5.T7 "Table 7 ‣ E.1.2 Performance of Medical Image Analysis tasks. ‣
    E.1 Overall experiments ‣ Appendix E Completed Experimental Results ‣ A Comparative
    Survey of Deep Active Learning") 记录了医学图像分析组的整体表现，包括*PneumoniaMNIST*和*BreakHis*数据集。LPL、WAAL和BADGE在这些医学图像分析任务上表现良好。另一个值得关注的现象是：所有基于MC
    dropout的版本（即LeastConfD、MarginD、EntropyD以及BALD），表现都不如原始版本（即LeastConf、Margin和Entropy），尤其是在*PneumoniaMNIST*上。例如，在*PneumoniaMNIST*上，LeastConf的AUBC值为0.852，而LeastConfD仅为0.8243
    AUBC值。一个潜在的原因是，在*PneumoniaMNIST*中，要判断一张胸部X光片是否存在肺炎，需要检查局部病变并观察肺部的整体状况。基本学习者需要局部和全局特征才能做出准确预测。然而，MC
    dropout减少了模型的能力，可能忽略了一些特征信息，使得预测不够令人信服[[3](#bib.bib3)]，并影响了DAL性能。另一个现象是，考虑到F-acc，我们注意到许多DAL方法的F-acc高于在完整数据集上训练的准确率（0.9039），例如，MarginD为0.9149，BALD为0.9204，CEAL为0.9179，AdvBIM为0.9197。这些结果可以总结为一个现象：从完整子集选择的子集会贡献于更好的性能。这是因为*PneumoniaMNIST*在训练集和测试集之间存在分布/数据集的变化。此外，这个数据集可能包含冗余的数据样本和混淆信息。也就是说，这些医学图像不是一对一的，它们是多对一的。一名患者可能对应多张胸部X光片，这造成了冗余。此外，一些患者可能有多种疾病，例如，我们可以在一些X光片上看到患者使用的后脊椎固定器。这些特征也可能影响预测结果。
- en: Compared with standard tasks (i.e., standard image classification tasks in our
    comparative survey), real-life applications would encounter more unexpected problems
    like we discussed aforementioned *PneumoniaMNIST* dataset. That is why we are
    working on adding more different kinds of tasks for testing DAL approaches. We
    also encourage DAL researchers to try DAL approaches on various data scenarios
    and tasks.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准任务（即我们比较调查中的标准图像分类任务）相比，现实生活中的应用会遇到更多意外问题，如我们之前讨论的*PneumoniaMNIST*数据集。这就是为什么我们致力于添加更多不同类型的任务来测试DAL方法。我们也鼓励DAL研究人员在各种数据场景和任务上尝试DAL方法。
- en: '|  | *MNIST* | *MNIST (w/ pre-train)* | *Waterbird* | *Waterbird (w/ pre-train)*
    |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  | *MNIST* | *MNIST (w/ pre-train)* | *Waterbird* | *Waterbird (w/ pre-train)*
    |'
- en: '| Model | AUBC | F-acc | Time | AUBC | F-acc | Time | AUBC | F-acc | Time |
    AUBC | F-acc | Time |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | AUBC | F-acc | 时间 | AUBC | F-acc | 时间 | AUBC | F-acc | 时间 | AUBC | F-acc
    | 时间 |'
- en: '| Full | $-$ | $0.9916$ | $-$ | $-$ | $0.9931$ | $-$ | $-$ | $0.5678$ | $-$
    | $-$ | $0.8459$ | $-$ |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 完整 | $-$ | $0.9916$ | $-$ | $-$ | $0.9931$ | $-$ | $-$ | $0.5678$ | $-$ |
    $-$ | $0.8459$ | $-$ |'
- en: '| Random | 0.9570 $\pm$ 0.0036 | 0.9738 | 1.00 | 0.9767 $\pm$ 0.0005 | 0.9822
    | 1.00 | 0.5950 $\pm$ 0.0092 | 0.5657 | 1.00 | 0.8070 $\pm$ 0.0043 | 0.8511 |
    Time |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 0.9570 $\pm$ 0.0036 | 0.9738 | 1.00 | 0.9767 $\pm$ 0.0005 | 0.9822 |
    1.00 | 0.5950 $\pm$ 0.0092 | 0.5657 | 1.00 | 0.8070 $\pm$ 0.0043 | 0.8511 | 时间
    |'
- en: '| LeastConf | 0.9677 $\pm$ 0.0041 | 0.9892 | 1.14 | 0.9833 $\pm$ 0.0012 | 0.9794
    | 1.38 | 0.5843 $\pm$ 0.0054 | 0.5612 | 2.78 | 0.8473 $\pm$ 0.0021 | 0.8605 |
    1.00 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| LeastConf | 0.9677 $\pm$ 0.0041 | 0.9892 | 1.14 | 0.9833 $\pm$ 0.0012 | 0.9794
    | 1.38 | 0.5843 $\pm$ 0.0054 | 0.5612 | 2.78 | 0.8473 $\pm$ 0.0021 | 0.8605 |
    1.00 |'
- en: '| LeastConfD | 0.9745 $\pm$ 0.0008 | 0.9915 | 2.17 | 0.9840 $\pm$ 0.0029 |
    0.9926 | 2.37 | 0.5847 $\pm$ 0.0009 | 0.5947 | 2.87 | 0.7947 $\pm$ 0.0173 | 0.8516
    | 1.00 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| LeastConfD | 0.9745 $\pm$ 0.0008 | 0.9915 | 2.17 | 0.9840 $\pm$ 0.0029 |
    0.9926 | 2.37 | 0.5847 $\pm$ 0.0009 | 0.5947 | 2.87 | 0.7947 $\pm$ 0.0173 | 0.8516
    | 1.00 |'
- en: '| Margin | 0.9733 $\pm$ 0.0012 | 0.9881 | 1.25 | 0.9813 $\pm$ 0.0029 | 0.9875
    | 1.35 | 0.5840 $\pm$ 0.0029 | 0.6064 | 0.99 | 0.8460 $\pm$ 0.0029 | 0.8629 |
    2.01 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| Margin | 0.9733 $\pm$ 0.0012 | 0.9881 | 1.25 | 0.9813 $\pm$ 0.0029 | 0.9875
    | 1.35 | 0.5840 $\pm$ 0.0029 | 0.6064 | 0.99 | 0.8460 $\pm$ 0.0029 | 0.8629 |
    2.01 |'
- en: '| MarginD | 0.9703 $\pm$ 0.0025 | 0.9899 | 2.84 | 0.9843 $\pm$ 0.0005 | 0.9883
    | 2.41 | 0.5960 $\pm$ 0.0049 | 0.5629 | 1.19 | 0.7983 $\pm$ 0.0166 | 0.8458 |
    1.00 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| MarginD | 0.9703 $\pm$ 0.0025 | 0.9899 | 2.84 | 0.9843 $\pm$ 0.0005 | 0.9883
    | 2.41 | 0.5960 $\pm$ 0.0049 | 0.5629 | 1.19 | 0.7983 $\pm$ 0.0166 | 0.8458 |
    1.00 |'
- en: '| Entropy | 0.9723 $\pm$ 0.0052 | 0.9883 | 1.54 | 0.9830 $\pm$ 0.0024 | 0.9907
    | 1.09 | 0.5823 $\pm$ 0.0074 | 0.6204 | 1.02 | 0.8473 $\pm$ 0.0019 | 0.8557 |
    1.19 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Entropy | 0.9723 $\pm$ 0.0052 | 0.9883 | 1.54 | 0.9830 $\pm$ 0.0024 | 0.9907
    | 1.09 | 0.5823 $\pm$ 0.0074 | 0.6204 | 1.02 | 0.8473 $\pm$ 0.0019 | 0.8557 |
    1.19 |'
- en: '| EntropyD | 0.9643 $\pm$ 0.0045 | 0.9887 | 3.14 | 0.9840 $\pm$ 0.0022 | 0.9912
    | 2.08 | 0.5817 $\pm$ 0.0101 | 0.6321 | 1.19 | 0.8000 $\pm$ 0.0222 | 0.8477 |
    1.03 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| EntropyD | 0.9643 $\pm$ 0.0045 | 0.9887 | 3.14 | 0.9840 $\pm$ 0.0022 | 0.9912
    | 2.08 | 0.5817 $\pm$ 0.0101 | 0.6321 | 1.19 | 0.8000 $\pm$ 0.0222 | 0.8477 |
    1.03 |'
- en: '| BALD | 0.9697 $\pm$ 0.0034 | 0.9885 | 3.12 | 0.9807 $\pm$ 0.0009 | 0.9834
    | 2.16 | 0.5970 $\pm$ 0.0070 | 0.6136 | 2.01 | 0.7773 $\pm$ 0.0012 | 0.8452 |
    2.32 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| BALD | 0.9697 $\pm$ 0.0034 | 0.9885 | 3.12 | 0.9807 $\pm$ 0.0009 | 0.9834
    | 2.16 | 0.5970 $\pm$ 0.0070 | 0.6136 | 2.01 | 0.7773 $\pm$ 0.0012 | 0.8452 |
    2.32 |'
- en: '| MeanSTD | 0.9713 $\pm$ 0.0034 | 0.9735 | 2.50 | 0.9847 $\pm$ 0.0005 | 0.9907
    | 2.20 | 0.5890 $\pm$ 0.0063 | 0.5758 | 2.20 | 0.7890 $\pm$ 0.0107 | 0.8401 |
    1.64 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| MeanSTD | 0.9713 $\pm$ 0.0034 | 0.9735 | 2.50 | 0.9847 $\pm$ 0.0005 | 0.9907
    | 2.20 | 0.5890 $\pm$ 0.0063 | 0.5758 | 2.20 | 0.7890 $\pm$ 0.0107 | 0.8401 |
    1.64 |'
- en: '| VarRatio | 0.9717 $\pm$ 0.0083 | 0.9841 | 1.77 | 0.9847 $\pm$ 0.0005 | 0.9902
    | 1.26 | 0.5803 $\pm$ 0.0026 | 0.5570 | 1.87 | 0.8460 $\pm$ 0.0029 | 0.8577 |
    2.32 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| VarRatio | 0.9717 $\pm$ 0.0083 | 0.9841 | 1.77 | 0.9847 $\pm$ 0.0005 | 0.9902
    | 1.26 | 0.5803 $\pm$ 0.0026 | 0.5570 | 1.87 | 0.8460 $\pm$ 0.0029 | 0.8577 |
    2.32 |'
- en: '| CEAL(Entropy) | 0.9787 $\pm$ 0.0019 | 0.9889 | 3.33 | 0.9863 $\pm$ 0.0005
    | 0.9872 | 2.27 | 0.5943 $\pm$ 0.0071 | 0.5811 | 1.93 | 0.8460 $\pm$ 0.0022 |
    0.8518 | 1.06 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| CEAL(Entropy) | 0.9787 $\pm$ 0.0019 | 0.9889 | 3.33 | 0.9863 $\pm$ 0.0005
    | 0.9872 | 2.27 | 0.5943 $\pm$ 0.0071 | 0.5811 | 1.93 | 0.8460 $\pm$ 0.0022 |
    0.8518 | 1.06 |'
- en: '| KMeans | 0.9640 $\pm$ 0.0016 | 0.9813 | 8.78 | # | # | # | 0.5920 $\pm$ 0.0022
    | 0.5846 | 2.31 | 0.7823 $\pm$ 0.0066 | 0.8410 | 1.43 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| KMeans | 0.9640 $\pm$ 0.0016 | 0.9813 | 8.78 | # | # | # | 0.5920 $\pm$ 0.0022
    | 0.5846 | 2.31 | 0.7823 $\pm$ 0.0066 | 0.8410 | 1.43 |'
- en: '| Kmeans(GPU) | 0.9637 $\pm$ 0.0021 | 0.9747 | 10.14 | 0.9743 $\pm$ 0.0005
    | 0.98 | 3.34 | 0.5663 $\pm$ 0.0054 | 0.5987 | 1.30 | 0.7937 $\pm$ 0.0041 | 0.8365
    | 2.33 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Kmeans(GPU) | 0.9637 $\pm$ 0.0021 | 0.9747 | 10.14 | 0.9743 $\pm$ 0.0005
    | 0.98 | 3.34 | 0.5663 $\pm$ 0.0054 | 0.5987 | 1.30 | 0.7937 $\pm$ 0.0041 | 0.8365
    | 2.33 |'
- en: '| KCenter | 0.9740 $\pm$ 0.0014 | 0.9877 | 7.10 | 0.9523 $\pm$ 0.0039 | 0.9659
    | 8.60 | 0.6097 $\pm$ 0.0108 | 0.6373 | 2.13 | 0.8297 $\pm$ 0.0031 | 0.8555 |
    1.31 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| KCenter | 0.9740 $\pm$ 0.0014 | 0.9877 | 7.10 | 0.9523 $\pm$ 0.0039 | 0.9659
    | 8.60 | 0.6097 $\pm$ 0.0108 | 0.6373 | 2.13 | 0.8297 $\pm$ 0.0031 | 0.8555 |
    1.31 |'
- en: '| VAAL | 0.9623 $\pm$ 0.0024 | 0.9573 | 19.20 | 0.9737 $\pm$ 0.0005 | 0.9718
    | 6.93 | 0.6217 $\pm$ 0.0025 | 0.5758 | 9.78 | 0.8070 $\pm$ 0.0029 | 0.8546 |
    9.69 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| VAAL | 0.9623 $\pm$ 0.0024 | 0.9573 | 19.20 | 0.9737 $\pm$ 0.0005 | 0.9718
    | 6.93 | 0.6217 $\pm$ 0.0025 | 0.5758 | 9.78 | 0.8070 $\pm$ 0.0029 | 0.8546 |
    9.69 |'
- en: '| BADGE(KMeans++) | 0.9707 $\pm$ 0.0062 | 0.9904 | 32.51 | 0.9647 $\pm$ 0.0026
    | 0.9841 | 7.04 | 0.5837 $\pm$ 0.0074 | 0.6194 | 2.47 | 0.8460 $\pm$ 0.0014 |
    0.8538 | 1.94 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| BADGE(KMeans++) | 0.9707 $\pm$ 0.0062 | 0.9904 | 32.51 | 0.9647 $\pm$ 0.0026
    | 0.9841 | 7.04 | 0.5837 $\pm$ 0.0074 | 0.6194 | 2.47 | 0.8460 $\pm$ 0.0014 |
    0.8538 | 1.94 |'
- en: '| AdvBIM | 0.9680 $\pm$ 0.0037 | 0.9840 | 20.74 | # | # | # | # | # | # | 0.8033
    $\pm$ 0.0082 | 0.8380 | 10.64 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| AdvBIM | 0.9680 $\pm$ 0.0037 | 0.9840 | 20.74 | # | # | # | # | # | # | 0.8033
    $\pm$ 0.0082 | 0.8380 | 10.64 |'
- en: '| LPL | 0.8913 $\pm$ 0.0062 | 0.9732 | 5.44 | 0.9923 $\pm$ 0.0005 | 0.9955
    | 2.29 | 0.7277 $\pm$ 0.0017 | 0.7783 | 4.50 | 0.7803 $\pm$ 0.0073 | 0.7817 |
    4.51 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| LPL | 0.8913 $\pm$ 0.0062 | 0.9732 | 5.44 | 0.9923 $\pm$ 0.0005 | 0.9955
    | 2.29 | 0.7277 $\pm$ 0.0017 | 0.7783 | 4.50 | 0.7803 $\pm$ 0.0073 | 0.7817 |
    4.51 |'
- en: '| WAAL | 0.9890 $\pm$ 0.0014 | 0.9946 | 36.10 | 0.9780 $\pm$ 0.0008 | 0.9943
    | 2.39 | 0.6837 $\pm$ 0.0073 | 0.7784 | 6.87 | 0.7009 $\pm$ 0.0067 | 0.7783 |
    6.81 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| WAAL | 0.9890 $\pm$ 0.0014 | 0.9946 | 36.10 | 0.9780 $\pm$ 0.0008 | 0.9943
    | 2.39 | 0.6837 $\pm$ 0.0073 | 0.7784 | 6.87 | 0.7009 $\pm$ 0.0067 | 0.7783 |
    6.81 |'
- en: 'Table 4: Results of DAL comparative experiments with *MNIST* w/ and w/o pre-train
    and *Waterbird* w/ and w/o pre-train. We report the AUBC for overall accuracy,
    final accuracy (F-acc) after quota $Q$ is exhausted, and the average running time
    of the whole AL processes (including training and querying processes) relative
    to Random. We rank F-acc and AUBC of each task with top 1st, 2nd and 3rd with
    red, teal and blue respectively. “#” indicates that the experiment has not been
    completed yet.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：*MNIST* 的带预训练与不带预训练、*Waterbird* 的带预训练与不带预训练的DAL比较实验结果。我们报告了整体准确度的AUBC、配额$Q$耗尽后的最终准确度（F-acc）以及整个AL过程（包括训练和查询过程）的平均运行时间，相对于随机方法。我们将每个任务的F-acc和AUBC按红色、青色和蓝色分别标记为第一、第二和第三名。“#”表示实验尚未完成。
- en: '|  | *CIFAR10* | *CIFAR10-imb* | *CIFAR100* | *SVHN* |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  | *CIFAR10* | *CIFAR10-imb* | *CIFAR100* | *SVHN* |'
- en: '| Model | AUBC | F-acc | Time | AUBC | F-acc | Time | AUBC | F-acc | Time |
    AUBC | F-acc | Time |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| Model | AUBC | F-acc | Time | AUBC | F-acc | Time | AUBC | F-acc | Time |
    AUBC | F-acc | Time |'
- en: '| Full | $-$ | $0.8793$ | $-$ | $-$ | $0.8036$ | $-$ | $-$ | $0.6062$ | $-$
    | $-$ | $0.9190$ | $-$ |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| Full | $-$ | $0.8793$ | $-$ | $-$ | $0.8036$ | $-$ | $-$ | $0.6062$ | $-$
    | $-$ | $0.9190$ | $-$ |'
- en: '| Random | 0.7967 $\pm$ 0.0005 | 0.8679 | 1.00 | 0.7103 $\pm$ 0.0017 | 0.8015
    | 1.00 | 0.4667 $\pm$ 0.0009 | 0.5903 | 1.00 | 0.8110 $\pm$ 0.0008 | 0.8806 |
    1.00 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| Random | 0.7967 $\pm$ 0.0005 | 0.8679 | 1.00 | 0.7103 $\pm$ 0.0017 | 0.8015
    | 1.00 | 0.4667 $\pm$ 0.0009 | 0.5903 | 1.00 | 0.8110 $\pm$ 0.0008 | 0.8806 |
    1.00 |'
- en: '| LeastConf | 0.8150 $\pm$ 0.0000 | 0.8785 | 1.04 | 0.7330 $\pm$ 0.0022 | 0.8022
    | 1.04 | 0.4747 $\pm$ 0.0009 | 0.6072 | 1.02 | 0.8350 $\pm$ 0.0028 | 0.9094 |
    1.05 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| LeastConf | 0.8150 $\pm$ 0.0000 | 0.8785 | 1.04 | 0.7330 $\pm$ 0.0022 | 0.8022
    | 1.04 | 0.4747 $\pm$ 0.0009 | 0.6072 | 1.02 | 0.8350 $\pm$ 0.0028 | 0.9094 |
    1.05 |'
- en: '| LeastConfD | 0.8137 $\pm$ 0.0012 | 0.8825 | 1.10 | 0.7323 $\pm$ 0.0033 |
    0.8065 | 1.18 | 0.4730 $\pm$ 0.0008 | 0.5997 | 1.13 | 0.8320 $\pm$ 0.0008 | 0.9083
    | 1.68 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| LeastConfD | 0.8137 $\pm$ 0.0012 | 0.8825 | 1.10 | 0.7323 $\pm$ 0.0033 |
    0.8065 | 1.18 | 0.4730 $\pm$ 0.0008 | 0.5997 | 1.13 | 0.8320 $\pm$ 0.0008 | 0.9083
    | 1.68 |'
- en: '| Margin | 0.8153 $\pm$ 0.0005 | 0.8834 | 1.01 | 0.7367 $\pm$ 0.0033 | 0.8029
    | 0.80 | 0.4790 $\pm$ 0.0008 | 0.6010 | 0.93 | 0.8373 $\pm$ 0.0005 | 0.9138 |
    1.35 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| Margin | 0.8153 $\pm$ 0.0005 | 0.8834 | 1.01 | 0.7367 $\pm$ 0.0033 | 0.8029
    | 0.80 | 0.4790 $\pm$ 0.0008 | 0.6010 | 0.93 | 0.8373 $\pm$ 0.0005 | 0.9138 |
    1.35 |'
- en: '| MarginD | 0.8140 $\pm$ 0.0008 | 0.8837 | 1.17 | 0.7260 $\pm$ 0.0014 | 0.8128
    | 0.86 | 0.4777 $\pm$ 0.0005 | 0.6000 | 1.06 | 0.8357 $\pm$ 0.0034 | 0.9104 |
    1.46 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| MarginD | 0.8140 $\pm$ 0.0008 | 0.8837 | 1.17 | 0.7260 $\pm$ 0.0014 | 0.8128
    | 0.86 | 0.4777 $\pm$ 0.0005 | 0.6000 | 1.06 | 0.8357 $\pm$ 0.0034 | 0.9104 |
    1.46 |'
- en: '| Entropy | 0.8130 $\pm$ 0.0008 | 0.8784 | 1.07 | 0.7320 $\pm$ 0.0019 | 0.8187
    | 0.73 | 0.4693 $\pm$ 0.0017 | 0.6048 | 0.78 | 0.8297 $\pm$ 0.0009 | 0.9099 |
    1.33 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| Entropy | 0.8130 $\pm$ 0.0008 | 0.8784 | 1.07 | 0.7320 $\pm$ 0.0019 | 0.8187
    | 0.73 | 0.4693 $\pm$ 0.0017 | 0.6048 | 0.78 | 0.8297 $\pm$ 0.0009 | 0.9099 |
    1.33 |'
- en: '| EntropyD | 0.8140 $\pm$ 0.0000 | 0.8787 | 1.12 | 0.7317 $\pm$ 0.0021 | 0.7963
    | 0.78 | 0.4677 $\pm$ 0.0005 | 0.6004 | 0.87 | 0.8290 $\pm$ 0.0008 | 0.9091 |
    1.43 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| EntropyD | 0.8140 $\pm$ 0.0000 | 0.8787 | 1.12 | 0.7317 $\pm$ 0.0021 | 0.7963
    | 0.78 | 0.4677 $\pm$ 0.0005 | 0.6004 | 0.87 | 0.8290 $\pm$ 0.0008 | 0.9091 |
    1.43 |'
- en: '| BALD | 0.8103 $\pm$ 0.0009 | 0.8762 | 1.18 | 0.7210 $\pm$ 0.0024 | 0.7927
    | 1.20 | 0.4760 $\pm$ 0.0008 | 0.5942 | 1.03 | 0.8333 $\pm$ 0.0005 | 0.9020 |
    1.51 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| BALD | 0.8103 $\pm$ 0.0009 | 0.8762 | 1.18 | 0.7210 $\pm$ 0.0024 | 0.7927
    | 1.20 | 0.4760 $\pm$ 0.0008 | 0.5942 | 1.03 | 0.8333 $\pm$ 0.0005 | 0.9020 |
    1.51 |'
- en: '| MeanSTD | 0.8087 $\pm$ 0.0009 | 0.8821 | 1.11 | 0.7203 $\pm$ 0.0017 | 0.7996
    | 0.78 | 0.4717 $\pm$ 0.0012 | 0.5963 | 1.11 | 0.8323 $\pm$ 0.0026 | 0.9087 |
    2.52 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| MeanSTD | 0.8087 $\pm$ 0.0009 | 0.8821 | 1.11 | 0.7203 $\pm$ 0.0017 | 0.7996
    | 0.78 | 0.4717 $\pm$ 0.0012 | 0.5963 | 1.11 | 0.8323 $\pm$ 0.0026 | 0.9087 |
    2.52 |'
- en: '| VarRatio | 0.8150 $\pm$ 0.0008 | 0.8780 | 1.00 | 0.7353 $\pm$ 0.0024 | 0.8165
    | 1.03 | 0.4747 $\pm$ 0.0012 | 0.5959 | 0.97 | 0.8357 $\pm$ 0.0009 | 0.9079 |
    1.45 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| VarRatio | 0.8150 $\pm$ 0.0008 | 0.8780 | 1.00 | 0.7353 $\pm$ 0.0024 | 0.8165
    | 1.03 | 0.4747 $\pm$ 0.0012 | 0.5959 | 0.97 | 0.8357 $\pm$ 0.0009 | 0.9079 |
    1.45 |'
- en: '| CEAL(Entropy) | 0.8150 $\pm$ 0.0016 | 0.8794 | 1.00 | 0.7327 $\pm$ 0.0050
    | 0.8187 | 0.75 | 0.4693 $\pm$ 0.0005 | 0.6043 | 0.94 | 0.8430 $\pm$ 0.0028 |
    0.9142 | 1.16 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| CEAL(Entropy) | 0.8150 $\pm$ 0.0016 | 0.8794 | 1.00 | 0.7327 $\pm$ 0.0050
    | 0.8187 | 0.75 | 0.4693 $\pm$ 0.0005 | 0.6043 | 0.94 | 0.8430 $\pm$ 0.0028 |
    0.9142 | 1.16 |'
- en: '| KMeans | 0.7910 $\pm$ 0.0016 | 0.8713 | 0.50 | 0.7070 $\pm$ 0.0029 | 0.7908
    | 3.06 | 0.4570 $\pm$ 0.0008 | 0.5834 | 1.01 | 0.8027 $\pm$ 0.0012 | 0.8671 |
    5.22 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| KMeans | 0.7910 $\pm$ 0.0016 | 0.8713 | 0.50 | 0.7070 $\pm$ 0.0029 | 0.7908
    | 3.06 | 0.4570 $\pm$ 0.0008 | 0.5834 | 1.01 | 0.8027 $\pm$ 0.0012 | 0.8671 |
    5.22 |'
- en: '| KMeans(GPU) | 0.7977 $\pm$ 0.0009 | 0.8718 | 1.64 | 0.7140 $\pm$ 0.0008 |
    0.7921 | 1.54 | 0.4687 $\pm$ 0.0005 | 0.5842 | 1.28 | 0.8120 $\pm$ 0.0008 | 0.8688
    | 5.76 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| KMeans(GPU) | 0.7977 $\pm$ 0.0009 | 0.8718 | 1.64 | 0.7140 $\pm$ 0.0008 |
    0.7921 | 1.54 | 0.4687 $\pm$ 0.0005 | 0.5842 | 1.28 | 0.8120 $\pm$ 0.0008 | 0.8688
    | 5.76 |'
- en: '| KCenter | 0.8047 $\pm$ 0.0012 | 0.8741 | 0.98 | 0.7233 $\pm$ 0.0009 | 0.7826
    | 2.87 | 0.4770 $\pm$ 0.0016 | 0.5993 | 1.02 | 0.8283 $\pm$ 0.0017 | 0.9000 |
    5.66 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| KCenter | 0.8047 $\pm$ 0.0012 | 0.8741 | 0.98 | 0.7233 $\pm$ 0.0009 | 0.7826
    | 2.87 | 0.4770 $\pm$ 0.0016 | 0.5993 | 1.02 | 0.8283 $\pm$ 0.0017 | 0.9000 |
    5.66 |'
- en: '| VAAL | 0.7973 $\pm$ 0.0009 | 0.8679 | 1.26 | 0.7113 $\pm$ 0.0012 | 0.7950
    | 4.58 | 0.4693 $\pm$ 0.0005 | 0.5870 | 1.20 | 0.8117 $\pm$ 0.0012 | 0.8813 |
    9.84 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| VAAL | 0.7973 $\pm$ 0.0009 | 0.8679 | 1.26 | 0.7113 $\pm$ 0.0012 | 0.7950
    | 4.58 | 0.4693 $\pm$ 0.0005 | 0.5870 | 1.20 | 0.8117 $\pm$ 0.0012 | 0.8813 |
    9.84 |'
- en: '| BADGE(KMeans++) | 0.8143 $\pm$ 0.0005 | 0.8794 | 2.08 | 0.7347 $\pm$ 0.0019
    | 0.8126 | 5.91 | 0.4803 $\pm$ 0.0005 | 0.6028 | 1.12 | 0.8377 $\pm$ 0.0017 |
    0.9057 | 10.27 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| BADGE(KMeans++) | 0.8143 $\pm$ 0.0005 | 0.8794 | 2.08 | 0.7347 $\pm$ 0.0019
    | 0.8126 | 5.91 | 0.4803 $\pm$ 0.0005 | 0.6028 | 1.12 | 0.8377 $\pm$ 0.0017 |
    0.9057 | 10.27 |'
- en: '| AdvBIM | 0.7997 $\pm$ 0.0005 | 0.8750 | 2.59 | # | # | # | # | # | # | #
    | # | # |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| AdvBIM | 0.7997 $\pm$ 0.0005 | 0.8750 | 2.59 | # | # | # | # | # | # | #
    | # | # |'
- en: '| LPL | 0.8220 $\pm$ 0.0014 | 0.9028 | 2.19 | 0.7477 $\pm$ 0.0060 | 0.8478
    | 3.14 | 0.4640 $\pm$ 0.0024 | 0.6369 | 0.71 | 0.8737 $\pm$ 0.0061 | 0.9452 |
    2.26 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| LPL | 0.8220 $\pm$ 0.0014 | 0.9028 | 2.19 | 0.7477 $\pm$ 0.0060 | 0.8478
    | 3.14 | 0.4640 $\pm$ 0.0024 | 0.6369 | 0.71 | 0.8737 $\pm$ 0.0061 | 0.9452 |
    2.26 |'
- en: '| WAAL | 0.8253 $\pm$ 0.0005 | 0.8717 | 1.65 | 0.7523 $\pm$ 0.0021 | 0.7993
    | 4.00 | 0.4277 $\pm$ 0.0005 | 0.5560 | 1.13 | 0.8603 $\pm$ 0.0017 | 0.9139 |
    9.88 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| WAAL | 0.8253 $\pm$ 0.0005 | 0.8717 | 1.65 | 0.7523 $\pm$ 0.0021 | 0.7993
    | 4.00 | 0.4277 $\pm$ 0.0005 | 0.5560 | 1.13 | 0.8603 $\pm$ 0.0017 | 0.9139 |
    9.88 |'
- en: 'Table 5: Results of DAL comparative experiments, including *CIFAR10*, *CIFAR10-imb*,
    *CIFAR100* and *SVHN*. We report the AUBC for overall accuracy, final accuracy
    (F-acc) after quota $Q$ is exhausted, and the average running time of the whole
    AL processes (including training and querying processes) relative to Random. We
    rank F-acc and AUBC of each task with top 1st, 2nd and 3rd with red, teal and
    blue respectively. “#” indicates that the experiment has not been completed yet.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: DAL 比较实验的结果，包括*CIFAR10*、*CIFAR10-imb*、*CIFAR100* 和 *SVHN*。我们报告了总体准确率的
    AUBC、配额 $Q$ 耗尽后的最终准确率（F-acc），以及与随机方法相比的整个主动学习过程（包括训练和查询过程）的平均运行时间。我们用红色、青色和蓝色分别标出每个任务的
    F-acc 和 AUBC 的前 1、2 和 3 名。“#”表示实验尚未完成。'
- en: '|  | *EMNIST* | *FashionMNIST* | *TinyImageNet* |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  | *EMNIST* | *FashionMNIST* | *TinyImageNet* |'
- en: '| Model | AUBC | F-acc | Time | AUBC | F-acc | Time | AUBC | F-acc | Time |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | AUBC | F-acc | 时间 | AUBC | F-acc | 时间 | AUBC | F-acc | 时间 |'
- en: '| Full | $-$ | $0.8684$ | $-$ | $-$ | $0.9120$ | $-$ | $-$ | $0.4583$ | $-$
    |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 完整 | $-$ | $0.8684$ | $-$ | $-$ | $0.9120$ | $-$ | $-$ | $0.4583$ | $-$ |'
- en: '| Random | 0.8057 $\pm$ 0.0026 | 0.8377 | 1.00 | 0.8313 $\pm$ 0.0034 | 0.8434
    | 1.00 | 0.2577 $\pm$ 0.0017 | 0.3544 | 1.00 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 0.8057 $\pm$ 0.0026 | 0.8377 | 1.00 | 0.8313 $\pm$ 0.0034 | 0.8434 |
    1.00 | 0.2577 $\pm$ 0.0017 | 0.3544 | 1.00 |'
- en: '| LeastConf | 0.8113 $\pm$ 0.0068 | 0.8479 | 1.11 | 0.8377 $\pm$ 0.0029 | 0.8820
    | 1.10 | 0.2417 $\pm$ 0.0009 | 0.3470 | 1.00 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| LeastConf | 0.8113 $\pm$ 0.0068 | 0.8479 | 1.11 | 0.8377 $\pm$ 0.0029 | 0.8820
    | 1.10 | 0.2417 $\pm$ 0.0009 | 0.3470 | 1.00 |'
- en: '| LeastConfD | 0.8177 $\pm$ 0.0005 | 0.8483 | 1.90 | 0.8450 $\pm$ 0.0036 |
    0.8744 | 1.80 | 0.2620 $\pm$ 0.0000 | 0.3698 | 0.93 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| LeastConfD | 0.8177 $\pm$ 0.0005 | 0.8483 | 1.90 | 0.8450 $\pm$ 0.0036 |
    0.8744 | 1.80 | 0.2620 $\pm$ 0.0000 | 0.3698 | 0.93 |'
- en: '| Margin | 0.8103 $\pm$ 0.0041 | 0.8468 | 0.85 | 0.8427 $\pm$ 0.0040 | 0.8772
    | 1.21 | 0.2557 $\pm$ 0.0012 | 0.3611 | 1.02 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| Margin | 0.8103 $\pm$ 0.0041 | 0.8468 | 0.85 | 0.8427 $\pm$ 0.0040 | 0.8772
    | 1.21 | 0.2557 $\pm$ 0.0012 | 0.3611 | 1.02 |'
- en: '| MarginD | 0.8197 $\pm$ 0.0017 | 0.8472 | 0.72 | 0.8417 $\pm$ 0.0012 | 0.8756
    | 2.07 | 0.2607 $\pm$ 0.0017 | 0.3541 | 1.32 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| MarginD | 0.8197 $\pm$ 0.0017 | 0.8472 | 0.72 | 0.8417 $\pm$ 0.0012 | 0.8756
    | 2.07 | 0.2607 $\pm$ 0.0017 | 0.3541 | 1.32 |'
- en: '| Entropy | 0.8090 $\pm$ 0.0057 | 0.8458 | 0.97 | 0.8397 $\pm$ 0.0029 | 0.8660
    | 1.22 | 0.2343 $\pm$ 0.0005 | 0.3346 | 1.02 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 熵 | 0.8090 $\pm$ 0.0057 | 0.8458 | 0.97 | 0.8397 $\pm$ 0.0029 | 0.8660 |
    1.22 | 0.2343 $\pm$ 0.0005 | 0.3346 | 1.02 |'
- en: '| EntropyD | 0.8167 $\pm$ 0.0019 | 0.8507 | 0.15 | 0.8417 $\pm$ 0.0033 | 0.8784
    | 2.02 | 0.2627 $\pm$ 0.0005 | 0.3716 | 0.09 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| EntropyD | 0.8167 $\pm$ 0.0019 | 0.8507 | 0.15 | 0.8417 $\pm$ 0.0033 | 0.8784
    | 2.02 | 0.2627 $\pm$ 0.0005 | 0.3716 | 0.09 |'
- en: '| BALD | 0.8197 $\pm$ 0.0024 | 0.8448 | 1.87 | 0.8423 $\pm$ 0.0095 | 0.8888
    | 2.07 | 0.2623 $\pm$ 0.0017 | 0.3648 | 0.91 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| BALD | 0.8197 $\pm$ 0.0024 | 0.8448 | 1.87 | 0.8423 $\pm$ 0.0095 | 0.8888
    | 2.07 | 0.2623 $\pm$ 0.0017 | 0.3648 | 0.91 |'
- en: '| MeanSTD | 0.8110 $\pm$ 0.0014 | 0.8426 | 0.68 | 0.8457 $\pm$ 0.0017 | 0.8766
    | 2.18 | 0.2510 $\pm$ 0.0008 | 0.3551 | 0.17 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| MeanSTD | 0.8110 $\pm$ 0.0014 | 0.8426 | 0.68 | 0.8457 $\pm$ 0.0017 | 0.8766
    | 2.18 | 0.2510 $\pm$ 0.0008 | 0.3551 | 0.17 |'
- en: '| VarRatio | 0.8107 $\pm$ 0.0060 | 0.8497 | 1.14 | 0.8410 $\pm$ 0.0037 | 0.8754
    | 1.27 | 0.2407 $\pm$ 0.0005 | 0.3426 | 1.06 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| VarRatio | 0.8107 $\pm$ 0.0060 | 0.8497 | 1.14 | 0.8410 $\pm$ 0.0037 | 0.8754
    | 1.27 | 0.2407 $\pm$ 0.0005 | 0.3426 | 1.06 |'
- en: '| CEAL(Entropy) | 0.8167 $\pm$ 0.0039 | 0.8459 | 2.05 | 0.8477 $\pm$ 0.0026
    | 0.8826 | 1.29 | 0.2347 $\pm$ 0.0009 | 0.3400 | 1.03 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| CEAL(熵) | 0.8167 $\pm$ 0.0039 | 0.8459 | 2.05 | 0.8477 $\pm$ 0.0026 | 0.8826
    | 1.29 | 0.2347 $\pm$ 0.0009 | 0.3400 | 1.03 |'
- en: '| KMeans | 0.7863 $\pm$ 0.0068 | 0.8222 | 1.56 | 0.8260 $\pm$ 0.0036 | 0.8525
    | 5.93 | 0.2447 $\pm$ 0.0009 | 0.3385 | 0.55 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| KMeans | 0.7863 $\pm$ 0.0068 | 0.8222 | 1.56 | 0.8260 $\pm$ 0.0036 | 0.8525
    | 5.93 | 0.2447 $\pm$ 0.0009 | 0.3385 | 0.55 |'
- en: '| KMeans(GPU) | 0.7990 $\pm$ 0.0022 | 0.8362 | 1.90 | 0.8343 $\pm$ 0.0012 |
    0.8657 | 7.58 | 0.1340 $\pm$ 0.0008 | 0.2288 | 0.79 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| KMeans（GPU） | 0.7990 $\pm$ 0.0022 | 0.8362 | 1.90 | 0.8343 $\pm$ 0.0012 |
    0.8657 | 7.58 | 0.1340 $\pm$ 0.0008 | 0.2288 | 0.79 |'
- en: '| KCenter | * | * | * | 0.8353 $\pm$ 0.0019 | 0.8466 | 0.76 | 0.2540 $\pm$
    0.0000 | 0.3460 | 0.43 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| KCenter | * | * | * | 0.8353 $\pm$ 0.0019 | 0.8466 | 0.76 | 0.2540 $\pm$
    0.0000 | 0.3460 | 0.43 |'
- en: '| VAAL | 0.8027 $\pm$ 0.0019 | 0.8363 | 1.78 | 0.8297 $\pm$ 0.0012 | 0.8535
    | 12.78 | 0.1313 $\pm$ 0.0005 | 0.2191 | 0.15 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| VAAL | 0.8027 $\pm$ 0.0019 | 0.8363 | 1.78 | 0.8297 $\pm$ 0.0012 | 0.8535
    | 12.78 | 0.1313 $\pm$ 0.0005 | 0.2191 | 0.15 |'
- en: '| BADGE(KMeans++) | * | * | * | 0.8437 $\pm$ 0.0019 | 0.8662 | 19.36 | # |
    # | # |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| BADGE（KMeans++） | * | * | * | 0.8437 $\pm$ 0.0019 | 0.8662 | 19.36 | # |
    # | # |'
- en: '| AdvBIM | # | # | # | 0.8390 $\pm$ 0.0016 | 0.8737 | 22.86 | # | # | # |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| AdvBIM | # | # | # | 0.8390 $\pm$ 0.0016 | 0.8737 | 22.86 | # | # | # |'
- en: '| LPL | 0.5447 $\pm$ 0.0023 | 0.6555 | 1.09 | 0.7600 $\pm$ 0.0094 | 0.8471
    | 4.00 | 0.0090 $\pm$ 0.0000 | 0.0051 | 0.40 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| LPL | 0.5447 $\pm$ 0.0023 | 0.6555 | 1.09 | 0.7600 $\pm$ 0.0094 | 0.8471
    | 4.00 | 0.0090 $\pm$ 0.0000 | 0.0051 | 0.40 |'
- en: '| WAAL | 0.8293 $\pm$ 0.0005 | 0.8423 | 1.59 | 0.8703 $\pm$ 0.0012 | 0.8984
    | 18.42 | 0.0157 $\pm$ 0.0005 | 0.0050 | 0.58 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| WAAL | 0.8293 $\pm$ 0.0005 | 0.8423 | 1.59 | 0.8703 $\pm$ 0.0012 | 0.8984
    | 18.42 | 0.0157 $\pm$ 0.0005 | 0.0050 | 0.58 |'
- en: 'Table 6: Results of DAL comparative experiments, including *EMNIST*, *FashionMNIST*
    and *TinyImageNet*. We report the AUBC for accuracy, final accuracy (F-acc) after
    quota $Q$ is exhausted, and the average running time of the whole AL processes
    (including training and querying processes) relative to Random. We rank F-acc
    and AUBC of each task with top 1st, 2nd and 3rd with red, teal and blue respectively.
    “$*$” indicates that the experiment needed too much memory, e.g., KCenter on *EMNIST*,
    while “#” indicates that the experiment has not been completed yet.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：DAL 比较实验的结果，包括 *EMNIST*、*FashionMNIST* 和 *TinyImageNet*。我们报告了准确率的 AUBC、配额
    $Q$ 耗尽后的最终准确率（F-acc）以及相对于随机方法的整个 AL 过程（包括训练和查询过程）的平均运行时间。我们对每个任务的 F-acc 和 AUBC
    进行排名，分别用红色、青色和蓝色表示第 1、2 和 3 名。"$*$" 表示实验需要过多内存，例如 *EMNIST* 上的 KCenter，而 "#" 表示实验尚未完成。
- en: '|  | *PneumoniaMNIST* | *BreakHis* |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | *PneumoniaMNIST* | *BreakHis* |'
- en: '| Model | AUBC | F-acc | Time | AUBC | F-acc | Time |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | AUBC | F-acc | 时间 | AUBC | F-acc | 时间 |'
- en: '| Full | $-$ | $0.9039$ | $-$ | $-$ | $0.8306$ | $-$ |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| Full | $-$ | $0.9039$ | $-$ | $-$ | $0.8306$ | $-$ |'
- en: '| Random | 0.8283 $\pm$ 0.0073 | 0.9077 | 1.00 | 0.8010 $\pm$ 0.0014 | 0.8150
    | 1.00 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 0.8283 $\pm$ 0.0073 | 0.9077 | 1.00 | 0.8010 $\pm$ 0.0014 | 0.8150 |
    1.00 |'
- en: '| LeastConf | 0.8520 $\pm$ 0.0022 | 0.9097 | 0.62 | 0.8213 $\pm$ 0.0017 | 0.8302
    | 0.91 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| LeastConf | 0.8520 $\pm$ 0.0022 | 0.9097 | 0.62 | 0.8213 $\pm$ 0.0017 | 0.8302
    | 0.91 |'
- en: '| LeastConfD | 0.8243 $\pm$ 0.0127 | 0.8654 | 1.10 | 0.8130 $\pm$ 0.0022 |
    0.8269 | 1.02 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| LeastConfD | 0.8243 $\pm$ 0.0127 | 0.8654 | 1.10 | 0.8130 $\pm$ 0.0022 |
    0.8269 | 1.02 |'
- en: '| Margin | 0.8580 $\pm$ 0.0045 | 0.8859 | 0.96 | 0.8217 $\pm$ 0.0009 | 0.8289
    | 1.12 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| Margin | 0.8580 $\pm$ 0.0045 | 0.8859 | 0.96 | 0.8217 $\pm$ 0.0009 | 0.8289
    | 1.12 |'
- en: '| MarginD | 0.8230 $\pm$ 0.0054 | 0.9149 | 1.27 | 0.8257 $\pm$ 0.0012 | 0.8364
    | 1.16 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| MarginD | 0.8230 $\pm$ 0.0054 | 0.9149 | 1.27 | 0.8257 $\pm$ 0.0012 | 0.8364
    | 1.16 |'
- en: '| Entropy | 0.8570 $\pm$ 0.0028 | 0.9132 | 0.95 | 0.8213 $\pm$ 0.0005 | 0.8251
    | 1.30 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| Entropy | 0.8570 $\pm$ 0.0028 | 0.9132 | 0.95 | 0.8213 $\pm$ 0.0005 | 0.8251
    | 1.30 |'
- en: '| EntropyD | 0.8177 $\pm$ 0.0045 | 0.8710 | 1.26 | 0.8017 $\pm$ 0.0009 | 0.8115
    | 1.49 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| EntropyD | 0.8177 $\pm$ 0.0045 | 0.8710 | 1.26 | 0.8017 $\pm$ 0.0009 | 0.8115
    | 1.49 |'
- en: '| BALD | 0.8270 $\pm$ 0.0014 | 0.9204 | 0.87 | 0.8150 $\pm$ 0.0016 | 0.8334
    | 0.89 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| BALD | 0.8270 $\pm$ 0.0014 | 0.9204 | 0.87 | 0.8150 $\pm$ 0.0016 | 0.8334
    | 0.89 |'
- en: '| MeanSTD | 0.7827 $\pm$ 0.0041 | 0.8802 | 1.18 | 0.7960 $\pm$ 0.0016 | 0.8076
    | 0.98 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| MeanSTD | 0.7827 $\pm$ 0.0041 | 0.8802 | 1.18 | 0.7960 $\pm$ 0.0016 | 0.8076
    | 0.98 |'
- en: '| VarRatio | 0.8530 $\pm$ 0.0065 | 0.8672 | 0.72 | 0.8270 $\pm$ 0.0008 | 0.8365
    | 0.74 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| VarRatio | 0.8530 $\pm$ 0.0065 | 0.8672 | 0.72 | 0.8270 $\pm$ 0.0008 | 0.8365
    | 0.74 |'
- en: '| CEAL(Entropy) | 0.8543 $\pm$ 0.0102 | 0.9179 | 0.75 | 0.8143 $\pm$ 0.0025
    | 0.8206 | 0.80 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| CEAL（熵） | 0.8543 $\pm$ 0.0102 | 0.9179 | 0.75 | 0.8143 $\pm$ 0.0025 | 0.8206
    | 0.80 |'
- en: '| KMeans | 0.8243 $\pm$ 0.0042 | 0.9044 | 0.64 | 0.8203 $\pm$ 0.0024 | 0.8394
    | 0.68 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| KMeans | 0.8243 $\pm$ 0.0042 | 0.9044 | 0.64 | 0.8203 $\pm$ 0.0024 | 0.8394
    | 0.68 |'
- en: '| KMeans(GPU) | 0.8333 $\pm$ 0.0053 | 0.9155 | 1.04 | 0.8140 $\pm$ 0.0016 |
    0.8323 | 1.38 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| KMeans（GPU） | 0.8333 $\pm$ 0.0053 | 0.9155 | 1.04 | 0.8140 $\pm$ 0.0016 |
    0.8323 | 1.38 |'
- en: '| KCenter | 0.8130 $\pm$ 0.0057 | 0.9189 | 1.01 | 0.8027 $\pm$ 0.0012 | 0.8289
    | 1.45 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| KCenter | 0.8130 $\pm$ 0.0057 | 0.9189 | 1.01 | 0.8027 $\pm$ 0.0012 | 0.8289
    | 1.45 |'
- en: '| VAAL | 0.8393 $\pm$ 0.0063 | 0.9064 | 5.33 | 0.8197 $\pm$ 0.0021 | 0.8344
    | 2.81 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| VAAL | 0.8393 $\pm$ 0.0063 | 0.9064 | 5.33 | 0.8197 $\pm$ 0.0021 | 0.8344
    | 2.81 |'
- en: '| BADGE(KMeans++) | 0.8340 $\pm$ 0.0022 | 0.9066 | 0.56 | 0.8343 $\pm$ 0.0012
    | 0.8470 | 0.68 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| BADGE(KMeans++) | 0.8340 $\pm$ 0.0022 | 0.9066 | 0.56 | 0.8343 $\pm$ 0.0012
    | 0.8470 | 0.68 |'
- en: '| AdvBIM | 0.8297 $\pm$ 0.0087 | 0.9197 | 3.61 | 0.8240 $\pm$ 0.0008 | 0.8337
    | 2.52 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| AdvBIM | 0.8297 $\pm$ 0.0087 | 0.9197 | 3.61 | 0.8240 $\pm$ 0.0008 | 0.8337
    | 2.52 |'
- en: '| LPL | 0.8593 $\pm$ 0.0087 | 0.9346 | 4.53 | 0.8277 $\pm$ 0.0009 | 0.8316
    | 2.66 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| LPL | 0.8593 $\pm$ 0.0087 | 0.9346 | 4.53 | 0.8277 $\pm$ 0.0009 | 0.8316
    | 2.66 |'
- en: '| WAAL | 0.9663 $\pm$ 0.0012 | 0.9564 | 5.24 | 0.8620 $\pm$ 0.0036 | 0.8698
    | 2.78 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| WAAL | 0.9663 $\pm$ 0.0012 | 0.9564 | 5.24 | 0.8620 $\pm$ 0.0036 | 0.8698
    | 2.78 |'
- en: 'Table 7: Results of DAL comparative experiments, including *PneumoniaMNIST*
    and *BreakHis*. We report the AUBC for overall accuracy, final accuracy (F-acc)
    after quota $Q$ is exhausted, and the average running time of the whole AL processes
    (including training and querying processes) relative to Random.We rank F-acc and
    AUBC of each task with top 1st, 2nd and 3rd with red, teal and blue respectively.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：DAL 比较实验的结果，包括 *PneumoniaMNIST* 和 *BreakHis*。我们报告了整体准确率的 AUBC，配额 $Q$ 用尽后的最终准确率（F-acc），以及相对于
    Random 的整个 AL 过程（包括训练和查询过程）的平均运行时间。我们用红色、青色和蓝色分别标记每个任务的 F-acc 和 AUBC 排名前 1、2 和
    3。
- en: '|  | *Waterbird (mismatch)* | *Waterbird (mismatch w/ pre-train)* | *Waterbird
    (worst group)* | *Waterbird (worst group w/ pre-train)* |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  | *Waterbird (mismatch)* | *Waterbird (mismatch w/ pre-train)* | *Waterbird
    (worst group)* | *Waterbird (worst group w/ pre-train)* |'
- en: '| Model | AUBC | F-acc | Time | AUBC | F-acc | Time | AUBC | F-acc | Time |
    AUBC | F-acc | Time |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| Model | AUBC | F-acc | Time | AUBC | F-acc | Time | AUBC | F-acc | Time |
    AUBC | F-acc | Time |'
- en: '| Random | 0.2892 $\pm$ 0.1309 | 0.2713 | 1.00 | 0.6378 $\pm$ 0.0720 | 0.7209
    | 1.00 | 0.0405 $\pm$ 0.0321 | 0.0535 | 1.00 | 0.3893 $\pm$ 0.1329 | 0.5177 |
    1.00 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| Random | 0.2892 $\pm$ 0.1309 | 0.2713 | 1.00 | 0.6378 $\pm$ 0.0720 | 0.7209
    | 1.00 | 0.0405 $\pm$ 0.0321 | 0.0535 | 1.00 | 0.3893 $\pm$ 0.1329 | 0.5177 |
    1.00 |'
- en: '| LeastConf | 0.2587 $\pm$ 0.1158 | 0.2224 | 2.78 | 0.7139 $\pm$ 0.0418 | 0.7402
    | 1.00 | 0.0452 $\pm$ 0.0310 | 0.0467 | 2.78 | 0.5508 $\pm$ 0.0872 | 0.5348 |
    1.00 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| LeastConf | 0.2587 $\pm$ 0.1158 | 0.2224 | 2.78 | 0.7139 $\pm$ 0.0418 | 0.7402
    | 1.00 | 0.0452 $\pm$ 0.0310 | 0.0467 | 2.78 | 0.5508 $\pm$ 0.0872 | 0.5348 |
    1.00 |'
- en: '| LeastConfD | 0.2691 $\pm$ 0.1525 | 0.2635 | 2.88 | 0.6115 $\pm$ 0.0795 |
    0.7229 | 2.01 | 0.0458 $\pm$ 0.0334 | 0.0621 | 2.88 | 0.3869 $\pm$ 0.1383 | 0.5576
    | 2.01 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| LeastConfD | 0.2691 $\pm$ 0.1525 | 0.2635 | 2.88 | 0.6115 $\pm$ 0.0795 |
    0.7229 | 2.01 | 0.0458 $\pm$ 0.0334 | 0.0621 | 2.88 | 0.3869 $\pm$ 0.1383 | 0.5576
    | 2.01 |'
- en: '| Margin | 0.2592 $\pm$ 0.1167 | 0.3034 | 0.99 | 0.7117 $\pm$ 0.0408 | 0.7450
    | 1.00 | 0.0449 $\pm$ 0.0308 | 0.0389 | 0.99 | 0.5509 $\pm$ 0.0851 | 0.5475 |
    1.00 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| Margin | 0.2592 $\pm$ 0.1167 | 0.3034 | 0.99 | 0.7117 $\pm$ 0.0408 | 0.7450
    | 1.00 | 0.0449 $\pm$ 0.0308 | 0.0389 | 0.99 | 0.5509 $\pm$ 0.0851 | 0.5475 |
    1.00 |'
- en: '| MarginD | 0.2923 $\pm$ 0.1512 | 0.3089 | 1.19 | 0.6190 $\pm$ 0.0779 | 0.7109
    | 1.19 | 0.0381 $\pm$ 0.0303 | 0.0581 | 1.19 | 0.3943 $\pm$ 0.1358 | 0.5675 |
    1.19 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| MarginD | 0.2923 $\pm$ 0.1512 | 0.3089 | 1.19 | 0.6190 $\pm$ 0.0779 | 0.7109
    | 1.19 | 0.0381 $\pm$ 0.0303 | 0.0581 | 1.19 | 0.3943 $\pm$ 0.1358 | 0.5675 |
    1.19 |'
- en: '| Entropy | 0.2578 $\pm$ 0.1173 | 0.3255 | 1.03 | 0.7133 $\pm$ 0.0396 | 0.7289
    | 1.03 | 0.0475 $\pm$ 0.0337 | 0.0498 | 1.03 | 0.5516 $\pm$ 0.0825 | 0.5540 |
    1.03 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| Entropy | 0.2578 $\pm$ 0.1173 | 0.3255 | 1.03 | 0.7133 $\pm$ 0.0396 | 0.7289
    | 1.03 | 0.0475 $\pm$ 0.0337 | 0.0498 | 1.03 | 0.5516 $\pm$ 0.0825 | 0.5540 |
    1.03 |'
- en: '| EntropyD | 0.2612 $\pm$ 0.1463 | 0.3608 | 1.19 | 0.6221 $\pm$ 0.0842 | 0.7163
    | 2.32 | 0.0466 $\pm$ 0.0342 | 0.1158 | 1.19 | 0.3910 $\pm$ 0.1393 | 0.5582 |
    2.32 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| EntropyD | 0.2612 $\pm$ 0.1463 | 0.3608 | 1.19 | 0.6221 $\pm$ 0.0842 | 0.7163
    | 2.32 | 0.0466 $\pm$ 0.0342 | 0.1158 | 1.19 | 0.3910 $\pm$ 0.1393 | 0.5582 |
    2.32 |'
- en: '| BALD | 0.2929 $\pm$ 0.1333 | 0.3421 | 2.02 | 0.5798 $\pm$ 0.0943 | 0.7107
    | 1.64 | 0.0405 $\pm$ 0.0287 | 0.1022 | 2.02 | 0.3788 $\pm$ 0.1488 | 0.5680 |
    1.64 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| BALD | 0.2929 $\pm$ 0.1333 | 0.3421 | 2.02 | 0.5798 $\pm$ 0.0943 | 0.7107
    | 1.64 | 0.0405 $\pm$ 0.0287 | 0.1022 | 2.02 | 0.3788 $\pm$ 0.1488 | 0.5680 |
    1.64 |'
- en: '| MeanSTD | 0.2789 $\pm$ 0.1386 | 0.2237 | 2.20 | 0.5999 $\pm$ 0.0925 | 0.7003
    | 2.32 | 0.0461 $\pm$ 0.0298 | 0.0472 | 2.20 | 0.3782 $\pm$ 0.1473 | 0.5665 |
    2.32 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| MeanSTD | 0.2789 $\pm$ 0.1386 | 0.2237 | 2.20 | 0.5999 $\pm$ 0.0925 | 0.7003
    | 2.32 | 0.0461 $\pm$ 0.0298 | 0.0472 | 2.20 | 0.3782 $\pm$ 0.1473 | 0.5665 |
    2.32 |'
- en: '| VarRatio | 0.2512 $\pm$ 0.1063 | 0.2307 | 1.88 | 0.7111 $\pm$ 0.0413 | 0.7333
    | 1.06 | 0.0465 $\pm$ 0.0305 | 0.0696 | 1.88 | 0.5509 $\pm$ 0.0854 | 0.5488 |
    1.06 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| VarRatio | 0.2512 $\pm$ 0.1063 | 0.2307 | 1.88 | 0.7111 $\pm$ 0.0413 | 0.7333
    | 1.06 | 0.0465 $\pm$ 0.0305 | 0.0696 | 1.88 | 0.5509 $\pm$ 0.0854 | 0.5488 |
    1.06 |'
- en: '| CEAL(Entropy) | 0.2863 $\pm$ 0.1510 | 0.3436 | 1.93 | 0.7119 $\pm$ 0.0432
    | 0.7224 | 1.43 | 0.0440 $\pm$ 0.0333 | 0.0363 | 1.93 | 0.5458 $\pm$ 0.0852 |
    0.5680 | 1.43 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| CEAL(Entropy) | 0.2863 $\pm$ 0.1510 | 0.3436 | 1.93 | 0.7119 $\pm$ 0.0432
    | 0.7224 | 1.43 | 0.0440 $\pm$ 0.0333 | 0.0363 | 1.93 | 0.5458 $\pm$ 0.0852 |
    0.5680 | 1.43 |'
- en: '| KMeans | 0.2809 $\pm$ 0.1130 | 0.2984 | 2.31 | 0.5867 $\pm$ 0.0992 | 0.7005
    | 2.33 | 0.0428 $\pm$ 0.0275 | 0.0457 | 2.31 | 0.3499 $\pm$ 0.1335 | 0.4943 |
    2.33 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| KMeans | 0.2809 $\pm$ 0.1130 | 0.2984 | 2.31 | 0.5867 $\pm$ 0.0992 | 0.7005
    | 2.33 | 0.0428 $\pm$ 0.0275 | 0.0457 | 2.31 | 0.3499 $\pm$ 0.1335 | 0.4943 |
    2.33 |'
- en: '| KMeans(GPU) | 0.2361 $\pm$ 0.1354 | 0.2831 | 1.31 | 0.6131 $\pm$ 0.0811 |
    0.6922 | 1.31 | 0.0544 $\pm$ 0.0384 | 0.0524 | 1.31 | 0.4669 $\pm$ 0.1173 | 0.5774
    | 1.31 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| KMeans(GPU) | 0.2361 $\pm$ 0.1354 | 0.2831 | 1.31 | 0.6131 $\pm$ 0.0811 |
    0.6922 | 1.31 | 0.0544 $\pm$ 0.0384 | 0.0524 | 1.31 | 0.4669 $\pm$ 0.1173 | 0.5774
    | 1.31 |'
- en: '| KCenter | 0.3272 $\pm$ 0.1520 | 0.3870 | 2.13 | 0.6810 $\pm$ 0.0665 | 0.7156
    | 2.12 | 0.0315 $\pm$ 0.0257 | 0.0363 | 2.13 | 0.4872 $\pm$ 0.1188 | 0.5696 |
    2.12 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| KCenter | 0.3272 $\pm$ 0.1520 | 0.3870 | 2.13 | 0.6810 $\pm$ 0.0665 | 0.7156
    | 2.12 | 0.0315 $\pm$ 0.0257 | 0.0363 | 2.13 | 0.4872 $\pm$ 0.1188 | 0.5696 |
    2.12 |'
- en: '| VAAL | 0.3521 $\pm$ 0.1716 | 0.3500 | 9.79 | 0.6469 $\pm$ 0.0517 | 0.7301
    | 9.69 | 0.0323 $\pm$ 0.0306 | 0.0633 | 9.79 | 0.3688 $\pm$ 0.1656 | 0.5135 |
    9.69 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| VAAL | 0.3521 $\pm$ 0.1716 | 0.3500 | 9.79 | 0.6469 $\pm$ 0.0517 | 0.7301
    | 9.69 | 0.0323 $\pm$ 0.0306 | 0.0633 | 9.79 | 0.3688 $\pm$ 0.1656 | 0.5135 |
    9.69 |'
- en: '| BADGE(KMeans++) | 0.2637 $\pm$ 0.1347 | 0.3215 | 2.48 | 0.7118 $\pm$ 0.0449
    | 0.7260 | 1.94 | 0.0458 $\pm$ 0.0328 | 0.0639 | 2.48 | 0.5391 $\pm$ 0.1005 |
    0.5587 | 1.94 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| BADGE(KMeans++) | 0.2637 $\pm$ 0.1347 | 0.3215 | 2.48 | 0.7118 $\pm$ 0.0449
    | 0.7260 | 1.94 | 0.0458 $\pm$ 0.0328 | 0.0639 | 2.48 | 0.5391 $\pm$ 0.1005 |
    0.5587 | 1.94 |'
- en: '| AdvBIM | # | # | # | 0.6309 $\pm$ 0.0836 | 0.6965 | 10.64 | # | # | # | 0.4164
    $\pm$ 0.1499 | 0.5992 | 10.64 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| AdvBIM | # | # | # | 0.6309 $\pm$ 0.0836 | 0.6965 | 10.64 | # | # | # | 0.4164
    $\pm$ 0.1499 | 0.5992 | 10.64 |'
- en: '| LPL | 0.6332 $\pm$ 0.1979 | 0.7783 | 4.50 | 0.5494 $\pm$ 0.2175 | 0.7786
    | 4.51 | 0.0187 $\pm$ 0.0035 | 0.0005 | 4.50 | 0.1094 $\pm$ 0.0112 | 0.0099 |
    4.51 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| LPL | 0.6332 $\pm$ 0.1979 | 0.7783 | 4.50 | 0.5494 $\pm$ 0.2175 | 0.7786
    | 4.51 | 0.0187 $\pm$ 0.0035 | 0.0005 | 4.50 | 0.1094 $\pm$ 0.0112 | 0.0099 |
    4.51 |'
- en: '| WAAL | 0.5543 $\pm$ 0.2544 | 0.7784 | 6.88 | 0.6036 $\pm$ 0.2138 | 0.7782
    | 6.81 | 0.0382 $\pm$ 0.0054 | 0.0005 | 6.88 | 0.0390 $\pm$ 0.0055 | 0.0010 |
    6.81 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| WAAL | 0.5543 $\pm$ 0.2544 | 0.7784 | 6.88 | 0.6036 $\pm$ 0.2138 | 0.7782
    | 6.81 | 0.0382 $\pm$ 0.0054 | 0.0005 | 6.88 | 0.0390 $\pm$ 0.0055 | 0.0010 |
    6.81 |'
- en: 'Table 8: Results of *Waterbird*. We report the AUBC for mismatch and worst
    group accuracy, final accuracy (F-acc) after quota $Q$ is exhausted, and the average
    running time of the whole AL processes (including training and querying processes)
    relative to Random. We bold F-acc values that are higher than full performance.
    We did not rank the top three methods since labeling them is not of great reference
    value. “#” indicates that the experiment has not been completed yet.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: *Waterbird* 的结果。我们报告了不匹配和最差组准确度的 AUBC、配额 $Q$ 用尽后的最终准确度（F-acc）以及相对于随机方法的整个
    AL 过程的平均运行时间（包括训练和查询过程）。我们将高于全面性能的 F-acc 值加粗。由于标记前三种方法的参考价值不大，因此我们没有对它们进行排名。“#”
    表示实验尚未完成。'
- en: E.2 DAL method ranking and summarizing
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 DAL 方法排名和总结
- en: We next consider the overall performance of DAL methods on the eight standard
    image classification datasets and two medical image analysis datasets using win-tie-loss
    counts, respectively, as shown in Table [9](#A5.T9 "Table 9 ‣ E.2 DAL method ranking
    and summarizing ‣ Appendix E Completed Experimental Results ‣ A Comparative Survey
    of Deep Active Learning"). We use a margin of $0.5\%$, e.g., a “win” is counted
    for method A if it outperforms method B by $0.5\%$ in pairwise comparison. Table
    [9](#A5.T9 "Table 9 ‣ E.2 DAL method ranking and summarizing ‣ Appendix E Completed
    Experimental Results ‣ A Comparative Survey of Deep Active Learning") shows the
    advantage of uncertainty-based DAL methods likeLeastConfD (3rd), and pseudo labeling
    for enhancing uncertainty-based DAL methods, i.e., CEAL (2nd). WAAL perform the
    best. Additionally, Dropout method also can improve DAL methods, e.g., LeastConfD
    ranks 3nd while LeastConf only ranks 9th. LPL only 13th. Although they achieve
    the best performances on some datasets (e.g., *SVHN*, *CIFAR10*) and have high
    win counts, they also perform extremely poorly on other datasets (e.g., *Tiny
    ImageNet*), which contributes to their low ranking. VAAL and KMeans perform even
    worse than Random. AdvBIM ranks far behind due to many incomplete tasks. This
    is a drawback of AdvBIM (also of AdvDeepFool), that is, these methods spend too
    much for re-calculating adversarial distance $\mathbf{r}$ for each unlabeled data
    sample per AL round.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来考虑 DAL 方法在八个标准图像分类数据集和两个医学图像分析数据集上的总体表现，分别使用胜-平-负计数，如表 [9](#A5.T9 "表 9
    ‣ E.2 DAL 方法排名和总结 ‣ 附录 E 完成的实验结果 ‣ 深度主动学习的比较调查")所示。我们使用 $0.5\%$ 的边际，例如，如果方法 A
    在成对比较中优于方法 B $0.5\%$，则计算为“胜”。表 [9](#A5.T9 "表 9 ‣ E.2 DAL 方法排名和总结 ‣ 附录 E 完成的实验结果
    ‣ 深度主动学习的比较调查") 显示了不确定性基于 DAL 方法的优势，如 LeastConfD（第3位），以及伪标签用于增强不确定性基于 DAL 方法，即
    CEAL（第2位）。WAAL 表现最好。此外，Dropout 方法也可以改善 DAL 方法，例如 LeastConfD 排名第3，而 LeastConf 仅排第9。LPL
    仅排第13。尽管它们在一些数据集（例如，*SVHN*、*CIFAR10*）上表现最佳，并且赢得了高计数，但在其他数据集（例如，*Tiny ImageNet*）上的表现极差，这导致了它们的低排名。VAAL
    和 KMeans 的表现甚至不如 Random。AdvBIM 排名远远落后，因为许多任务不完整。这是 AdvBIM 的一个缺陷（AdvDeepF 也是如此），即这些方法在每轮
    AL 中为每个未标记的数据样本重新计算对抗距离 $\mathbf{r}$ 的成本过高。
- en: On medical image analysis tasks, both WAAL and LPL outperform other DAL approaches,
    which constantly shows the advantage of DAL with enhancing techniques. Combined
    strategy, BADGE also obtained a good ranking (4th). More noticeably, BADGE always
    obtains comparable performances on various tasks. Therefore, for new/unseen tasks/data,
    we recommend first trying combined DAL approaches. In medical image analysis tasks,
    VAAL perform better than standard image classification tasks.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学图像分析任务中，WAAL 和 LPL 优于其他 DAL 方法，这不断展示了 DAL 在增强技术下的优势。结合策略，BADGE 也获得了不错的排名（第4位）。更明显的是，BADGE
    在各种任务上总是能取得可比的表现。因此，对于新任务/未见数据，我们建议首先尝试结合的 DAL 方法。在医学图像分析任务中，VAAL 比标准图像分类任务表现更好。
- en: '|  | *Standard Image Classification (8 datasets)* | *Medical Image Analysis
    (2 datasets)* |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '|  | *标准图像分类（8个数据集）* | *医学图像分析（2个数据集）* |'
- en: '| Rank | Method | $\text{win}-\text{tie}-\text{loss}$ | Method | $\text{win}-\text{tie}-\text{loss}$
    |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| 排名 | 方法 | $\text{win}-\text{tie}-\text{loss}$ | 方法 | $\text{win}-\text{tie}-\text{loss}$
    |'
- en: '| 1 | WAAL | $103-2-31$ | WAAL | $34-0-0$ |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| 1 | WAAL | $103-2-31$ | WAAL | $34-0-0$ |'
- en: '| 2 | CEAL | $74-35-27$ | LPL | $25-6-3$ |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| 2 | CEAL | $74-35-27$ | LPL | $25-6-3$ |'
- en: '| 3 | LeastConfD | $63-59-14$ | VarRatio | $23-7-4$ |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 3 | LeastConfD | $63-59-14$ | VarRatio | $23-7-4$ |'
- en: '| 4 | MarginD | $61-55-20$ | BADGE | $24-1-9$ |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| 4 | MarginD | $61-55-20$ | BADGE | $24-1-9$ |'
- en: '| 5 | Margin | $60-57-19$ | Margin | $19-10-5$ |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Margin | $60-57-19$ | Margin | $19-10-5$ |'
- en: '| 6 | BALD | $56-59-21$ | Entropy | $18-11-5$ |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 6 | BALD | $56-59-21$ | 熵 | $18-11-5$ |'
- en: '| 7 | EntropyD | $54-55-27$ | LeastConf | $18-9-7$ |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 7 | EntropyD | $54-55-27$ | LeastConf | $18-9-7$ |'
- en: '| 8 | VarRatio | $52-58-26$ | VAAL | $16-9-12$ |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| 8 | VarRatio | $52-58-26$ | VAAL | $16-9-12$ |'
- en: '| 9 | LeastConf | $51-53-32$ | CEAL | $15-7-12$ |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 9 | LeastConf | $51-53-32$ | CEAL | $15-7-12$ |'
- en: '| 10 | Badge | $46-49-41$ | AdvBIM | $13-11-10$ |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| 10 | Badge | $46-49-41$ | AdvBIM | $13-11-10$ |'
- en: '| 11 | MeanSTD | $44-50-42$ | MarginD | $12-9-13$ |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| 11 | MeanSTD | $44-50-42$ | MarginD | $12-9-13$ |'
- en: '| 12 | Entropy | $40-54-42$ | KMeans | $10-9-15$ |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 熵 | $40-54-42$ | KMeans | $10-9-15$ |'
- en: '| 13 | LPL | $57-4-75$ | BALD | $7-8-19$ |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| 13 | LPL | $57-4-75$ | BALD | $7-8-19$ |'
- en: '| 14 | KCenter | $41-34-61$ | LeastConfD | $7-6-21$ |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 14 | KCenter | $41-34-61$ | LeastConfD | $7-6-21$ |'
- en: '| 15 | Random | $26-23-87$ | Random | $4-7-23$ |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 随机 | $26-23-87$ | 随机 | $4-7-23$ |'
- en: '| 16 | VAAL | $20-15-101$ | EntropyD | $2-3-29$ |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| 16 | VAAL | $20-15-101$ | EntropyD | $2-3-29$ |'
- en: '| 17 | KMeans | $18-9-109$ | KCenter | $2-3-29$ |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| 17 | KMeans | $18-9-109$ | KCenter | $2-3-29$ |'
- en: '| 18 | AdvBIM | $10-25-101$ | MeanSTD | $0-1-33$ |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| 18 | AdvBIM | $10-25-101$ | MeanSTD | $0-1-33$ |'
- en: 'Table 9: Comparison of DAL methods using win-tie-loss across 8 datasets on
    standard image classification tasks and 2 medical image analysis tasks with AUBC
    (acc). Methods are ranked by $2\times win+tie$.'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：在标准图像分类任务和 2 个医学图像分析任务上，使用胜-平-负比较DAL方法的结果，基于AUBC（准确率）。方法按$2\times win+tie$排序。
- en: To observe the performance differences on various DAL methods in varying AL
    stages, we provide overall accuracy-budget curves on multiple datasets, as shown
    in Figure [5](#A5.F5 "Figure 5 ‣ E.2 DAL method ranking and summarizing ‣ Appendix
    E Completed Experimental Results ‣ A Comparative Survey of Deep Active Learning").
    From this figure, it could be observed that LPL is weak in the early stage of
    AL processes due to the inaccurate loss prediction trained on insufficient labeled
    data. In later stages, by co-training LossNet and the basic classifier on more
    labeled data, LossNet has demonstrated its ability to enhance the basic classifier.
    In contrast, WAAL performs better in the early stage of AL processes due to the
    design of the loss function that is more suitable for AL. It helps distinguish
    labeled and unlabeled samples and select more representative data samples in the
    early stage. Therefore, WAAL brings more benefits when limiting the budget for
    labeling costs.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察不同DAL方法在不同AL阶段的性能差异，我们提供了多个数据集上的整体准确率-预算曲线，如图 [5](#A5.F5 "图 5 ‣ E.2 DAL
    方法排名和总结 ‣ 附录 E 完成的实验结果 ‣ 深度主动学习的比较调查")所示。从该图中可以观察到，由于在标记数据不足的情况下训练的损失预测不准确，LPL在AL过程的早期阶段较弱。在后期，通过在更多标记数据上共同训练LossNet和基本分类器，LossNet显示了增强基本分类器的能力。相比之下，WAAL在AL过程的早期阶段表现更好，因为其设计的损失函数更适合AL。它有助于区分标记和未标记样本，并在早期阶段选择更具代表性的数据样本。因此，当限制标记成本的预算时，WAAL带来了更多的好处。
- en: '![Refer to caption](img/6a262f3a12aa7e2787701c35c257c3db.png)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6a262f3a12aa7e2787701c35c257c3db.png)'
- en: (a) MNIST
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MNIST
- en: '![Refer to caption](img/3b076e9f3271f9c8a92e1e69c1d76b11.png)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3b076e9f3271f9c8a92e1e69c1d76b11.png)'
- en: (b) FashionMNIST
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: (b) FashionMNIST
- en: '![Refer to caption](img/4fc1155b1075c5548f6bd6e2567e7347.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4fc1155b1075c5548f6bd6e2567e7347.png)'
- en: (c) SVHN
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: (c) SVHN
- en: '![Refer to caption](img/7de4d6df4057d5d18a332df33d7456e2.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7de4d6df4057d5d18a332df33d7456e2.png)'
- en: (d) CIFAR10
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: (d) CIFAR10
- en: '![Refer to caption](img/5b1a031f7616d96746ceb45f8f3a8bbc.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5b1a031f7616d96746ceb45f8f3a8bbc.png)'
- en: (e) CIFAR10-imb
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: (e) CIFAR10-imb
- en: '![Refer to caption](img/73511e7976b9ec8046cc7e7626806027.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/73511e7976b9ec8046cc7e7626806027.png)'
- en: (f) CIFAR100
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: (f) CIFAR100
- en: 'Figure 5: Overall accuracy-budget curve of *MNIST*, *FashionMNIST*, *CIFAR10*,
    *CIFAR10 (imb)* and *CIFAR100* datasets. The mean and standard deviation of the
    AUBC (acc) performance over $3$ trials is shown in parentheses in the legend.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：*MNIST*、*FashionMNIST*、*CIFAR10*、*CIFAR10 (imb)* 和 *CIFAR100* 数据集的整体准确率-预算曲线。AUBC（准确率）的均值和标准差在图例中以括号形式显示。
- en: 'E.3 Ablation study: numbers of training epochs and batch size'
  id: totrans-497
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.3 消融研究：训练轮次和批量大小
- en: 'We present the accuracy-budget curves using different batch sizes and training
    epochs, as shown in Figure [6](#A5.F6 "Figure 6 ‣ E.3 Ablation study: numbers
    of training epochs and batch size ‣ Appendix E Completed Experimental Results
    ‣ A Comparative Survey of Deep Active Learning"). A detailed analysis of this
    ablation study is in the main paper.'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了使用不同批量大小和训练轮次的准确率-预算曲线，如图 [6](#A5.F6 "图 6 ‣ E.3 消融研究：训练轮次和批量大小 ‣ 附录 E 完成的实验结果
    ‣ 深度主动学习的比较调查")所示。对该消融研究的详细分析见主文献。
- en: '![Refer to caption](img/4b660fbd811402351c67f4e889d6be3d.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4b660fbd811402351c67f4e889d6be3d.png)'
- en: (a) $b=1000$, $\#e=5$
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: (a) $b=1000$, $\#e=5$
- en: '![Refer to caption](img/d9f9bc149d15b6cc7c7dca771974139f.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d9f9bc149d15b6cc7c7dca771974139f.png)'
- en: (b) $b=2000$, $\#e=5$
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: (b) $b=2000$, $\#e=5$
- en: '![Refer to caption](img/19ddccf130e52180ac51ae2a8f2a5158.png)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/19ddccf130e52180ac51ae2a8f2a5158.png)'
- en: (c) $b=4000$, $\#e=5$
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: (c) $b=4000$, $\#e=5$
- en: '![Refer to caption](img/596612cfddf21c51b0c11bebc3025eff.png)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/596612cfddf21c51b0c11bebc3025eff.png)'
- en: (d) $b=10000$, $\#e=5$
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: (d) $b=10000$, $\#e=5$
- en: '![Refer to caption](img/6198a95b9cd9e8e2d5a7cc668c15ae64.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6198a95b9cd9e8e2d5a7cc668c15ae64.png)'
- en: (e) $b=1000$, $\#e=10$
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: (e) $b=1000$, $\#e=10$
- en: '![Refer to caption](img/033192d9dcf3561b8fc4f6cdd4c9317f.png)'
  id: totrans-509
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/033192d9dcf3561b8fc4f6cdd4c9317f.png)'
- en: (f) $b=2000$, $\#e=10$
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: (f) $b=2000$, $\#e=10$
- en: '![Refer to caption](img/ad7d1630b8edb5ab7148dd5032796940.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ad7d1630b8edb5ab7148dd5032796940.png)'
- en: (g) $b=4000$, $\#e=10$
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: (g) $b=4000$, $\#e=10$
- en: '![Refer to caption](img/0cf093ceb14ce548ecb362f1bac20e66.png)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0cf093ceb14ce548ecb362f1bac20e66.png)'
- en: (h) $b=10000$, $\#e=10$
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: (h) $b=10000$，$\#e=10$
- en: '![Refer to caption](img/f5a25f38deac5e4ada158cfcffcbf064.png)'
  id: totrans-515
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f5a25f38deac5e4ada158cfcffcbf064.png)'
- en: (i) $b=1000$, $\#e=15$
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: (i) $b=1000$，$\#e=15$
- en: '![Refer to caption](img/69c82c7a00419cdaf7e8c236b96953e6.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/69c82c7a00419cdaf7e8c236b96953e6.png)'
- en: (j) $b=2000$, $\#e=15$
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: (j) $b=2000$，$\#e=15$
- en: '![Refer to caption](img/b1af6eed1564f7e8ad6cbda5f614cfd8.png)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b1af6eed1564f7e8ad6cbda5f614cfd8.png)'
- en: (k) $b=4000$, $\#e=15$
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: (k) $b=4000$，$\#e=15$
- en: '![Refer to caption](img/9be919672968d59e527c64b1ba87b3b3.png)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9be919672968d59e527c64b1ba87b3b3.png)'
- en: (l) $b=10000$, $\#e=15$
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: (l) $b=10000$，$\#e=15$
- en: '![Refer to caption](img/58cc7e809e3ba2f3ca9793f49d730141.png)'
  id: totrans-523
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/58cc7e809e3ba2f3ca9793f49d730141.png)'
- en: (m) $b=1000$, $\#e=20$
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: (m) $b=1000$，$\#e=20$
- en: '![Refer to caption](img/7a349a4b455daa94cffeb3ed53e19320.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7a349a4b455daa94cffeb3ed53e19320.png)'
- en: (n) $b=2000$, $\#e=20$
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: (n) $b=2000$，$\#e=20$
- en: '![Refer to caption](img/b5d420d4f3f979d080e20211fffc35db.png)'
  id: totrans-527
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b5d420d4f3f979d080e20211fffc35db.png)'
- en: (o) $b=4000$, $\#e=20$
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: (o) $b=4000$，$\#e=20$
- en: '![Refer to caption](img/f4e7ae94904cd930226c56e1b2bc31e9.png)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f4e7ae94904cd930226c56e1b2bc31e9.png)'
- en: (p) $b=10000$, $\#e=20$
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: (p) $b=10000$，$\#e=20$
- en: '![Refer to caption](img/f2f7ee06c799358ac48fa394b616c92d.png)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f2f7ee06c799358ac48fa394b616c92d.png)'
- en: (q) $b=1000$, $\#e=25$
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: (q) $b=1000$，$\#e=25$
- en: '![Refer to caption](img/4aecf3bd27a0f21fc23d66b1c90a3f3c.png)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4aecf3bd27a0f21fc23d66b1c90a3f3c.png)'
- en: (r) $b=2000$, $\#e=25$
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: (r) $b=2000$，$\#e=25$
- en: '![Refer to caption](img/249cc7e265a86ed660bf714c88f7cfb6.png)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/249cc7e265a86ed660bf714c88f7cfb6.png)'
- en: (s) $b=4000$, $\#e=25$
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: (s) $b=4000$，$\#e=25$
- en: '![Refer to caption](img/74f77db68b88434273a83e14665f1014.png)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/74f77db68b88434273a83e14665f1014.png)'
- en: (t) $b=10000$, $\#e=25$
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: (t) $b=10000$，$\#e=25$
- en: '![Refer to caption](img/8187aa3cbb0fe268c3fd373398fec636.png)'
  id: totrans-539
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8187aa3cbb0fe268c3fd373398fec636.png)'
- en: (u) $b=1000$, $\#e=30$
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: (u) $b=1000$，$\#e=30$
- en: '![Refer to caption](img/a7b70e2f24b6e4d9fba2c177533e02fc.png)'
  id: totrans-541
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a7b70e2f24b6e4d9fba2c177533e02fc.png)'
- en: (v) $b=2000$, $\#e=30$
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: (v) $b=2000$，$\#e=30$
- en: '![Refer to caption](img/da57b746beb8627cd52cce9278a03265.png)'
  id: totrans-543
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/da57b746beb8627cd52cce9278a03265.png)'
- en: (w) $b=4000$, $\#e=30$
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: (w) $b=4000$，$\#e=30$
- en: '![Refer to caption](img/1ee7cf8eeed9273d12190afc2d3f4361.png)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1ee7cf8eeed9273d12190afc2d3f4361.png)'
- en: (x) $b=10000$, $\#e=30$
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: (x) $b=10000$，$\#e=30$
- en: 'Figure 6: The accuracy-budget curve of different DAL methods on *CIFAR10* dataset
    with different batch sizes and numbers of training epochs in each active learning
    round. From left to right, the value of $b$ equals to $1,000$, $2,000$, $4,000$
    and $10,000$. From top to bottom, the value of training epoch equals to $5$, $10$,
    $15$, $20$, $25$ and $30$.'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：不同DAL方法在*CIFAR10*数据集上，不同批量大小和每次主动学习轮次中的训练周期数的准确性-预算曲线。从左到右，$b$的值分别为$1,000$、$2,000$、$4,000$和$10,000$。从上到下，训练周期的值为$5$、$10$、$15$、$20$、$25$和$30$。
- en: 'E.4 Ablation study: w/ and w/o pre-training techniques'
  id: totrans-548
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.4 消融研究：有无预训练技术
- en: The detailed results of MNIST, Waterbird w/ and w/o pre-training techniques
    are shown in Tables [4](#A5.T4 "Table 4 ‣ E.1.2 Performance of Medical Image Analysis
    tasks. ‣ E.1 Overall experiments ‣ Appendix E Completed Experimental Results ‣
    A Comparative Survey of Deep Active Learning") and [8](#A5.T8 "Table 8 ‣ E.1.2
    Performance of Medical Image Analysis tasks. ‣ E.1 Overall experiments ‣ Appendix
    E Completed Experimental Results ‣ A Comparative Survey of Deep Active Learning"),
    including the overall accuracy, mismatch group accuracy and worst group accuracy.
    We record AUBC (acc) with mean and standard deviation over 3 trials, F-acc and
    average running time. We have detailed analysed this experiment in main paper.
    From Tables [4](#A5.T4 "Table 4 ‣ E.1.2 Performance of Medical Image Analysis
    tasks. ‣ E.1 Overall experiments ‣ Appendix E Completed Experimental Results ‣
    A Comparative Survey of Deep Active Learning") and [8](#A5.T8 "Table 8 ‣ E.1.2
    Performance of Medical Image Analysis tasks. ‣ E.1 Overall experiments ‣ Appendix
    E Completed Experimental Results ‣ A Comparative Survey of Deep Active Learning"),
    we could observe that on *Waterbird* w/o pre-train, most typical DAL sampling
    methods like LeastConf, Margin, Entropy, KCenter, etc., perform even worse than
    Random.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST、Waterbird 以及使用或不使用预训练技术的详细结果见表格 [4](#A5.T4 "Table 4 ‣ E.1.2 Performance
    of Medical Image Analysis tasks. ‣ E.1 Overall experiments ‣ Appendix E Completed
    Experimental Results ‣ A Comparative Survey of Deep Active Learning") 和 [8](#A5.T8
    "Table 8 ‣ E.1.2 Performance of Medical Image Analysis tasks. ‣ E.1 Overall experiments
    ‣ Appendix E Completed Experimental Results ‣ A Comparative Survey of Deep Active
    Learning")，包括整体准确率、误差组准确率和最差组准确率。我们记录了 AUBC (acc) 的均值和标准差（基于 3 次实验）、F-acc 和平均运行时间。我们在主要论文中对这个实验进行了详细分析。从表格
    [4](#A5.T4 "Table 4 ‣ E.1.2 Performance of Medical Image Analysis tasks. ‣ E.1
    Overall experiments ‣ Appendix E Completed Experimental Results ‣ A Comparative
    Survey of Deep Active Learning") 和 [8](#A5.T8 "Table 8 ‣ E.1.2 Performance of
    Medical Image Analysis tasks. ‣ E.1 Overall experiments ‣ Appendix E Completed
    Experimental Results ‣ A Comparative Survey of Deep Active Learning") 中可以观察到，在
    *Waterbird* 无预训练的情况下，大多数典型的 DAL 采样方法，如 LeastConf、Margin、Entropy、KCenter 等，表现甚至比
    Random 更差。
