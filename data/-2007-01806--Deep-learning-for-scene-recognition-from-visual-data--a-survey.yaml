- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:00:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:00:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2007.01806] Deep learning for scene recognition from visual data: a survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2007.01806] 从视觉数据进行场景识别的深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2007.01806](https://ar5iv.labs.arxiv.org/html/2007.01806)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2007.01806](https://ar5iv.labs.arxiv.org/html/2007.01806)
- en: '¹¹institutetext: Bernoulli Institute for Mathematics, Computer Science and
    Artificial Intelligence, University of Groningen, Nijenborgh 9, 9747 AG, Groningen,
    The Netherlands'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构：荷兰格罗宁根大学伯努利数学、计算机科学与人工智能研究所，Nijenborgh 9, 9747 AG, Groningen, The Netherlands
- en: '¹¹email: e.talavera.martinez@rug.nl'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹电子邮件：e.talavera.martinez@rug.nl
- en: 'Deep learning for scene recognition from visual data: a survey'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从视觉数据进行场景识别的深度学习：综述
- en: Alina Matei 11Both authors contributed equally to this studyBoth authors contributed
    equally to this study    Andreea Glavan 11**    Estefanía Talavera 11 [0000-0001-5918-8990](https://orcid.org/0000-0001-5918-8990
    "ORCID identifier")
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 阿丽娜·马泰**  安德莉亚·格拉万**    斯特法尼亚·塔拉维拉 [0000-0001-5918-8990](https://orcid.org/0000-0001-5918-8990
    "ORCID identifier")
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The use of deep learning techniques has exploded during the last few years,
    resulting in a direct contribution to the field of artificial intelligence. This
    work aims to be a review of the state-of-the-art in scene recognition with deep
    learning models from visual data. Scene recognition is still an emerging field
    in computer vision, which has been addressed from a single image and dynamic image
    perspective. We first give an overview of available datasets for image and video
    scene recognition. Later, we describe ensemble techniques introduced by research
    papers in the field. Finally, we give some remarks on our findings and discuss
    what we consider challenges in the field and future lines of research. This paper
    aims to be a future guide for model selection for the task of scene recognition.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术在过去几年中得到了迅猛发展，直接促进了人工智能领域的进步。本研究旨在回顾使用深度学习模型进行场景识别的最新进展。场景识别仍然是计算机视觉中的一个新兴领域，主要从单张图像和动态图像的角度进行了探讨。我们首先概述了用于图像和视频场景识别的可用数据集。随后，我们描述了该领域研究论文中提出的集成技术。最后，我们对我们的发现做出一些评论，并讨论我们认为该领域的挑战及未来的研究方向。本文旨在为场景识别任务的模型选择提供未来的指南。
- en: 'Keywords:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Scene Recognition Ensemble Techniques Deep Learning Computer Vision
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 场景识别 集成技术 深度学习 计算机视觉
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Recognizing scenes is a task that humans do on a daily basis. When walking
    down the street and going from one location to the other, tends to be easy for
    a human to identify where s/he is located. During the past years, deep learning
    architectures, such as Convolutional Neural Networks (CNNs) have outperformed
    traditional methods in many classification tasks. These models have shown to achieve
    high classification performance when large and variety datasets are available
    for training. Nowadays, the available visual data is not only presented in a static
    format, as an image, but also in a dynamic format, as video recordings. The analysis
    of videos adds an additional level of complexity since the inherent temporal aspect
    of video recordings must be considered: a video can capture scenes which suffer
    temporal alterations. Scene recognition with deep learning has been addressed
    by ensemble techniques that combine different levels of semantics extracted from
    the images, e.g. recognized objects, global information, and context at different
    scales.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 识别场景是人类每天都会进行的任务。当在人行道上走动并从一个位置移动到另一个位置时，人类通常很容易识别自己所在的位置。在过去几年中，深度学习架构，如卷积神经网络（CNNs），在许多分类任务中超越了传统方法。这些模型在大型和多样化的数据集可用于训练时表现出高分类性能。如今，现有的视觉数据不仅以静态格式（如图像）呈现，还以动态格式（如视频录制）呈现。视频分析增加了额外的复杂性，因为必须考虑视频录制的固有时间方面：视频可以捕捉到经历时间变化的场景。深度学习中的场景识别已经通过集成技术得到处理，这些技术结合了从图像中提取的不同层次的语义，例如识别的对象、全局信息和不同尺度的上下文。
- en: Developing robust and reliable models for the automatic recognition of scenes
    is of importance in the field of intelligent systems and artificial intelligence
    since it directly supports real-life applications. For instance, Scene and event
    recognition has been previously addressed in the literature [[1](#bib.bib1), [29](#bib.bib29)].
    Scene recognition for robot localization with indoor localization for mobile robots
    is one of the emerging application scopes of scene recognition [[2](#bib.bib2),
    [5](#bib.bib5), [21](#bib.bib21)]. According to the authors of [[21](#bib.bib21)],
    in the following two decades, every household could own a social robot employed
    for housekeeping, surveillance or companionship tasks. In the field of lifelogging,
    collections of photo-sequences have proven to be a rich tool for the understanding
    of the behaviour of people. In [[9](#bib.bib9), [19](#bib.bib19)] methods were
    develop for the analysis of egocentric image collected by wearable cameras. The
    above-mentioned approaches address the recognition of scenes either following
    an image-based approach or a video or photo-sequence based approach.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 发展稳健且可靠的自动场景识别模型对于智能系统和人工智能领域至关重要，因为它直接支持现实生活中的应用。例如，场景和事件识别在文献中已有探讨[[1](#bib.bib1),
    [29](#bib.bib29)]。室内定位中的机器人本地化的场景识别是场景识别的一个新兴应用范围[[2](#bib.bib2), [5](#bib.bib5),
    [21](#bib.bib21)]。根据[[21](#bib.bib21)]的作者，在接下来的二十年里，每个家庭可能都会拥有用于家务、监控或陪伴任务的社交机器人。在lifelogging领域，照片序列的集合已被证明是理解人类行为的丰富工具。在[[9](#bib.bib9),
    [19](#bib.bib19)]中，开发了用于分析由可穿戴相机收集的自我中心图像的方法。上述方法要么采用基于图像的方法，要么采用基于视频或照片序列的方法来解决场景识别问题。
- en: As contributions, (i) to the best of our knowledge, this is the first survey
    that collects works that address the task of scene recognition with deep deep
    learning from visual data, both from images and videos. Moreover, (ii) we describe
    available datasets which assisted the fast advancement in the field.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为贡献，(i) 据我们所知，这是第一个收集处理视觉数据（包括图像和视频）场景识别任务的深度学习工作的调查。此外，(ii) 我们描述了那些助力于该领域快速发展的可用数据集。
- en: 'This paper is structured as follows: in Section [2](#S2 "2 Datasets for scene
    recognition ‣ Deep learning for scene recognition from visual data: a survey")
    we discuss the available datasets supporting scene and object focused recognition.
    Section [3](#S3 "3 Frameworks for scene recognition ‣ Deep learning for scene
    recognition from visual data: a survey") addresses the methodology of the state-of-the-art
    techniques and approaches discussed in the paper at hand. Furthermore, in Section
    [4](#S4 "4 Discussion ‣ Deep learning for scene recognition from visual data:
    a survey") we discuss the presented approaches. Finally, in Section [5](#S5 "5
    Conclusions ‣ Deep learning for scene recognition from visual data: a survey")
    we draw some conclusions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '本文结构如下：在第[2](#S2 "2 Datasets for scene recognition ‣ Deep learning for scene
    recognition from visual data: a survey")节中，我们讨论了支持场景和对象集中识别的可用数据集。第[3](#S3 "3
    Frameworks for scene recognition ‣ Deep learning for scene recognition from visual
    data: a survey")节讨论了论文中讨论的最先进技术和方法论。此外，在第[4](#S4 "4 Discussion ‣ Deep learning
    for scene recognition from visual data: a survey")节中，我们讨论了提出的方法。最后，在第[5](#S5 "5
    Conclusions ‣ Deep learning for scene recognition from visual data: a survey")节中，我们总结了一些结论。'
- en: 2 Datasets for scene recognition
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 个场景识别数据集
- en: The latest advancements in deep learning methods for scene recognition are motivated
    by the availability of large and exhaustive datasets and hardware that allows
    the training of deep networks. Thus, deep learning CNNs are applied to tackle
    the complexity and high variance of the task of scene recognition.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法在场景识别中的最新进展得益于大型和详尽的数据集及支持深度网络训练的硬件。因此，深度学习CNN被应用于应对场景识别任务的复杂性和高差异性。
- en: 'The inherent difficulty of scene recognition is related to the nature of the
    images depicting a scene context. Two major challenges were described in [[30](#bib.bib30)]:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 场景识别的固有困难与描绘场景上下文的图像的特性有关。[[30](#bib.bib30)]中描述了两个主要挑战：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Visual inconsistency refers to low inter-class variance. Some scene categories
    can share similar visual appearances which create the issue of class overlaps.
    Since images belonging to two different classes can be easily confused with one
    another, the class overlap cannot be neglected.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视觉不一致性指的是低的类间差异。一些场景类别可能具有相似的视觉外观，这会导致类别重叠的问题。由于属于两个不同类别的图像可能容易互相混淆，因此类别重叠不可忽视。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Annotation ambiguity describes a high intra-class variance of scene categories.
    Demarcation of the categories is a subjective process which is highly dependent
    on the experience of the annotators, therefore images from the same category can
    showcase significant differences in appearance.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标注模糊性描述了场景类别的高类内方差。类别的划分是一个主观过程，极度依赖标注员的经验，因此来自同一类别的图像可能在外观上存在显著差异。
- en: The majority of the available datasets are focused on object categories providing
    labels [[6](#bib.bib6), [10](#bib.bib10), [13](#bib.bib13), [20](#bib.bib20)],
    bounding boxes [[15](#bib.bib15)] or segmentations [[15](#bib.bib15), [18](#bib.bib18)].
    ImageNet [[6](#bib.bib6)], COCO (Common Objects in Context)[[18](#bib.bib18)],
    and Open Images [[15](#bib.bib15)] are well known in the field of object recognition.
    Even though these dataset were built for object recognition, transfer learning
    has shown to be an effective approach when aiming to apply them for scene recognition.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数可用数据集集中于对象类别，提供标签 [[6](#bib.bib6), [10](#bib.bib10), [13](#bib.bib13), [20](#bib.bib20)]、边界框
    [[15](#bib.bib15)] 或分割 [[15](#bib.bib15), [18](#bib.bib18)]。ImageNet [[6](#bib.bib6)]、COCO（上下文中的常见对象）[[18](#bib.bib18)]
    和 Open Images [[15](#bib.bib15)] 在对象识别领域广为人知。尽管这些数据集是为对象识别构建的，但迁移学习已被证明是应用于场景识别的有效方法。
- en: 'Figure 1: Example of samples of the publicly available datasets as described
    in Table [1](#S2.T1 "Table 1 ‣ 2 Datasets for scene recognition ‣ Deep learning
    for scene recognition from visual data: a survey"). Samples are presented from
    the same classes amongst similar datasets (i.e. scene, video and object centric)
    in order to emphasize the diversity of the image and video data. For the video-centric
    datasets (i.e. Maryland ”in-the-wild”, YUPENN, YUP++) representative video frames
    are presented.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：表 [1](#S2.T1 "表 1 ‣ 2 数据集用于场景识别 ‣ 深度学习用于从视觉数据中识别场景的调查") 中描述的公开可用数据集的样本示例。样本来自相似数据集（即场景、视频和物体中心），以突出图像和视频数据的多样性。对于视频中心的数据集（即马里兰州“野外”、YUPENN、YUP++），展示了代表性的视频帧。
- en: '![Refer to caption](img/fe248ee7b3df1e8b169b82cd3fd222ea.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fe248ee7b3df1e8b169b82cd3fd222ea.png)'
- en: In the literature we can find the 15-scenes [[16](#bib.bib16)], MIT Indoor67
    [[23](#bib.bib23)], SUN397 [[32](#bib.bib32)], and Places365 [[35](#bib.bib35)]
    as scene-centered datasets. More specifically, the Places project introduced Places365
    as a reference dataset, which is composed of 434 scenes which account for 98%
    of the type of scenes a person can encounter in the natural and man-made world.
    A total of 10 million images were gathered, out of which 365 scene categories
    were chosen to be part of the dataset. Several annotators were asked to label
    every image and images with contradicting labels were discarded. Currently, the
    dataset is available in the Places365-standard format (i.e. 365 categories, roughly
    1 million images training set, validation set with 50 images per class and test
    with 900 images per class) and the Places365-challenge format which extends the
    training set to 8 million image samples in total. With a dataset of this magnitude,
    the training of CNNs exclusively on data describing scenes becomes feasible.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中我们可以找到15-scenes [[16](#bib.bib16)]、MIT Indoor67 [[23](#bib.bib23)]、SUN397
    [[32](#bib.bib32)] 和 Places365 [[35](#bib.bib35)] 作为以场景为中心的数据集。更具体地，Places项目引入了Places365作为参考数据集，该数据集包含434个场景，占自然和人造世界中人们可能遇到的场景类型的98%。共收集了1000万张图像，从中选择了365个场景类别作为数据集的一部分。多个标注员被要求标注每张图像，标注不一致的图像被丢弃。目前，数据集提供了Places365标准格式（即365个类别，约100万张图像训练集，每个类别50张图像的验证集和每个类别900张图像的测试集）和Places365挑战格式，后者将训练集扩展到总共800万张图像样本。凭借这样规模的数据集，仅用描述场景的数据进行CNN训练变得可行。
- en: 'Table 1: An overview of publicly available datasets for the task of scene recognition.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：公开可用的数据集概览，用于场景识别任务。
- en: '| Dataset | Data | #Classes | Classification of | Labelled as |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 数据 | 类别数量 | 分类为 | 标注为 |'
- en: '|  Images |  Streams |  Object |  Scenes |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  图像 |  流 |  物体 |  场景 |'
- en: '| Places365 [[35](#bib.bib35)] | 1M images | 365 |  |  |  |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Places365 [[35](#bib.bib35)] | 100 万张图像 | 365 |  |  |  |  |'
- en: '| MIT Indoor67 [[23](#bib.bib23)] | 15620 images | 67 |  |  |  |  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| MIT Indoor67 [[23](#bib.bib23)] | 15620 张图像 | 67 |  |  |  |  |'
- en: '| SUN397 [[32](#bib.bib32)] | 108754 images | 397 |  |  |  |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| SUN397 [[32](#bib.bib32)] | 108754 张图像 | 397 |  |  |  |  |'
- en: '| 15 scene [[32](#bib.bib32)] | 4000 images | 15 |  |  |  |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 15 场景 [[32](#bib.bib32)] | 4000 张图像 | 15 |  |  |  |  |'
- en: '| Maryland ‘in-the-wild’ [[24](#bib.bib24)] | 10 videos | 13 |  |  |  |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 马里兰州“野外” [[24](#bib.bib24)] | 10 视频 | 13 |  |  |  |  |'
- en: '| YUPENN [[7](#bib.bib7)] | 410 videos | 14 |  |  |  |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| YUPENN [[7](#bib.bib7)] | 410 视频 | 14 |  |  |  |  |'
- en: '| YUP++ [[8](#bib.bib8)] | 1200 videos | 20 |  |  |  |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| YUP++ [[8](#bib.bib8)] | 1200 视频 | 20 |  |  |  |  |'
- en: '| Imagenet [[6](#bib.bib6)] | 3.2M images | 1000 |  |  |  |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Imagenet [[6](#bib.bib6)] | 3.2M 图像 | 1000 |  |  |  |  |'
- en: '| COCO [[18](#bib.bib18)] | 1.5M images | 80 |  |  |  |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| COCO [[18](#bib.bib18)] | 1.5M 图像 | 80 |  |  |  |  |'
- en: '| Open Images [[15](#bib.bib15)] | 1.7M images | 600 |  |  |  |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Open Images [[15](#bib.bib15)] | 1.7M 图像 | 600 |  |  |  |  |'
- en: Scene recognition also encloses dynamic scene data; due to the limited amount
    of available datasets which include such data, most of the research efforts in
    this sub-field also include gathering suitable experimental data. Here we highlight
    the Maryland ‘in-the-wild’ [[24](#bib.bib24)], YUPENN [[7](#bib.bib7)] , YUP++
    [[8](#bib.bib8)] datasets. The dataset in [[8](#bib.bib8)] poses new challenges
    by introducing more complex data, i.e. videos with camera motion. The scope of
    the categories that are being recorded amongst the three datasets presented is
    not nearly as exhaustive as in the case of the objects and scenes datasets mentioned
    above. This is an indicator of the incipient status of research in this particular
    area of scene recognition.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 场景识别还包括动态场景数据；由于包含此类数据的可用数据集数量有限，因此该子领域的大多数研究工作还包括收集适当的实验数据。在这里，我们重点介绍了马里兰州“野外”[[24](#bib.bib24)]、YUPENN
    [[7](#bib.bib7)] 和 YUP++ [[8](#bib.bib8)] 数据集。数据集 [[8](#bib.bib8)] 通过引入更复杂的数据，即带有摄像机运动的视频，带来了新的挑战。三种数据集中记录的类别范围远没有上述物体和场景数据集那样详尽。这反映了这一特定场景识别领域研究的初期状态。
- en: The original models proposed by the authors of the [[24](#bib.bib24)] and [[7](#bib.bib7)]
    datasets were not based on deep learning techniques. The authors of the the Maryland
    ‘in-the-wild’ [[24](#bib.bib24)], introduced a chaotic system framework for describing
    the videos. The authors’ proposed pipeline extracts a 960-dimensional Gist descriptor
    per videoframe. Each dimension is considered a time-series, from which the chaotic
    invariants are computed. Traditional classifiers, such as KNN and SVM, are used
    for the final classification. In [[7](#bib.bib7)], the authors introduced the
    YUPENN dataset and for its analysis, they proposed a spatiotemporal oriented energy
    feature representation of the videos which they classify using KNN.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[[24](#bib.bib24)] 和 [[7](#bib.bib7)] 数据集的原始模型并未基于深度学习技术。马里兰州“野外”[[24](#bib.bib24)]
    的作者引入了一种混沌系统框架来描述视频。作者提出的管道为每个视频帧提取一个 960 维的 Gist 描述符。每一维被视为时间序列，从中计算出混沌不变量。最终分类使用传统分类器，如
    KNN 和 SVM。在 [[7](#bib.bib7)] 中，作者引入了 YUPENN 数据集，并为其分析提出了一种时空导向的能量特征表示，使用 KNN 进行分类。'
- en: 'An overview of the described datasets is provided in Table [1](#S2.T1 "Table
    1 ‣ 2 Datasets for scene recognition ‣ Deep learning for scene recognition from
    visual data: a survey"). In Figure [1](#S2.F1 "Figure 1 ‣ 2 Datasets for scene
    recognition ‣ Deep learning for scene recognition from visual data: a survey")
    we complete the quantitative overview of the datasets by presenting representative
    image samples for each of the datasets described.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [1](#S2.T1 "Table 1 ‣ 2 Datasets for scene recognition ‣ Deep learning for
    scene recognition from visual data: a survey") 中提供了所描述数据集的概述。在图 [1](#S2.F1 "Figure
    1 ‣ 2 Datasets for scene recognition ‣ Deep learning for scene recognition from
    visual data: a survey") 中，我们通过呈现每个数据集的代表性图像样本来完成数据集的定量概述。'
- en: 3 Frameworks for scene recognition
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 种场景识别框架
- en: 'In this section, we describe relevant aspects of the state-of-the-art methods
    on scene recognition with deep learning. The choice for deep architectures is
    motivated by the complexity of the task: since the images are not described semantically
    the models used are aimed at learning generic contextual features of the scenes,
    which are captured by the high-level convolutional layers.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了有关场景识别的最先进深度学习方法的相关方面。选择深度架构是因为任务的复杂性：由于图像未进行语义描述，所使用的模型旨在学习场景的一般上下文特征，这些特征由高层卷积层捕捉。
- en: Previous to deep learning, visual recognition techniques have made extensive
    use of object recognition when faced with such problems [[4](#bib.bib4), [17](#bib.bib17)].
    The scenes would be recognized based on exhaustive lists of objects identified
    in the scene. However, other challenges appear such as object detection and their
    high appearance variability. The combination of object detection and overall context
    recognition [[28](#bib.bib28)] showed promising results.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习之前，视觉识别技术在面对这类问题时广泛使用了对象识别[[4](#bib.bib4)、[17](#bib.bib17)]。场景将基于场景中识别出的对象的详尽列表进行识别。然而，还出现了其他挑战，如对象检测及其高变化性。对象检测和整体上下文识别的结合[[28](#bib.bib28)]
    显示出了有希望的结果。
- en: Focusing on deep learning research papers, we group them based on the type of
    the analysed datasets, images or videos. We present their performances and limitations
    in the context of the evaluated datasets.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 聚焦于深度学习研究论文，我们根据分析的数据集类型（图像或视频）对其进行分组。我们在评估的数据集的背景下展示它们的表现和局限性。
- en: 3.1 Static scene recognition
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 静态场景识别
- en: Several works have addressed the recognition of scenes based on single image
    analysis. The best well-known work on scene recognition was introduced in [[35](#bib.bib35)],
    which relied on the Places365 dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究已经解决了基于单张图像分析的场景识别问题。最著名的场景识别工作在 [[35](#bib.bib35)] 中介绍，它依赖于 Places365 数据集。
- en: 'Table 2: Top-5 classification accuracy of the trained networks on the validation
    and test splits of the Places365 dataset. Apart from the ResNet architecture which
    has been fine-tuned over Places365, the other architectures are trained from scratch.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 Places365 数据集的验证集和测试集上的训练网络的 Top-5 分类准确率。除了在 Places365 上进行微调的 ResNet 架构外，其他架构都是从头开始训练的。
- en: '| Architectures | Top-5 accuracy |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | Top-5 准确率 |'
- en: '| --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| trained on Places365 |  Validation set |  Test set |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 训练于 Places365 |  验证集 |  测试集 |'
- en: '| --- | --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Places365 AlexNet [[35](#bib.bib35)] | 82.89% | 82.75% |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Places365 AlexNet [[35](#bib.bib35)] | 82.89% | 82.75% |'
- en: '| Places365 GoogleNet[[35](#bib.bib35)] | 83.88% | 84.01% |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Places365 GoogleNet[[35](#bib.bib35)] | 83.88% | 84.01% |'
- en: '| Places365 VGG [[35](#bib.bib35)] | 84.91% | 85.01% |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Places365 VGG [[35](#bib.bib35)] | 84.91% | 85.01% |'
- en: '| Places365 ResNet [[35](#bib.bib35)] | 85.08% | 85.07% |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Places365 ResNet [[35](#bib.bib35)] | 85.08% | 85.07% |'
- en: 'Deep learning architectures have been trained over the Places365 dataset. The
    approach proposed by the authors of literature [[35](#bib.bib35)] is to exploit
    the vast dataset at hand by training three popular CNNs architectures (i.e. AlexNet
    [[14](#bib.bib14)], GoogLeNet [[26](#bib.bib26)], VGG16 [[25](#bib.bib25)]) on
    the Places dataset. The performance of these architectures over the validation
    and test splits of the Places365 dataset are presented in Table [2](#S3.T2 "Table
    2 ‣ 3.1 Static scene recognition ‣ 3 Frameworks for scene recognition ‣ Deep learning
    for scene recognition from visual data: a survey"). When introducing a new dataset,
    it became a ritual to test the generalization capabilities of weights trained
    over Places365\. Thus, authors fine-tune these specialised networks trained on
    Places365 over newly available datasets. For instance, the VGG16[[25](#bib.bib25)],
    pre-trained on the Places365 dataset, achieved a 92.99% accuracy on the SUN Attribute
    dataset [[31](#bib.bib31)]. To compare the performance of the above approaches
    for static scene recognition, the following datasets are considered: 15 scenes
    dataset [[16](#bib.bib16)], MIT Indoor 67 [[23](#bib.bib23)] and SUN 397 [[32](#bib.bib32)].
    An overview of the comparison of the quantitative results is presented in Table
    [3](#S3.T3 "Table 3 ‣ 3.1 Static scene recognition ‣ 3 Frameworks for scene recognition
    ‣ Deep learning for scene recognition from visual data: a survey").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习架构已在 Places365 数据集上进行了训练。文献[[35](#bib.bib35)]的作者提出的方法是利用手头的庞大数据集，通过在 Places
    数据集上训练三种流行的 CNN 架构（即 AlexNet [[14](#bib.bib14]、GoogLeNet [[26](#bib.bib26]、VGG16
    [[25](#bib.bib25]）来发挥其作用。这些架构在 Places365 数据集的验证集和测试集上的表现见表 [2](#S3.T2 "Table 2
    ‣ 3.1 Static scene recognition ‣ 3 Frameworks for scene recognition ‣ Deep learning
    for scene recognition from visual data: a survey")。引入新的数据集时，测试在 Places365 上训练的权重的泛化能力成为了一种惯例。因此，作者在新数据集上微调这些专门训练在
    Places365 上的网络。例如，预训练于 Places365 数据集的 VGG16[[25](#bib.bib25] 在 SUN 属性数据集 [[31](#bib.bib31]
    上达到了 92.99% 的准确率。为了比较上述静态场景识别方法的表现，考虑了以下数据集：15 scenes 数据集 [[16](#bib.bib16]、MIT
    Indoor 67 [[23](#bib.bib23] 和 SUN 397 [[32](#bib.bib32]。定量结果的比较概述见表 [3](#S3.T3
    "Table 3 ‣ 3.1 Static scene recognition ‣ 3 Frameworks for scene recognition ‣
    Deep learning for scene recognition from visual data: a survey")。'
- en: 'Table 3: An overview of the quantitative comparison in terms of accuracy between
    methods for single image classification for the 15 scenes, MIT Indoor, SUN 397
    datasets.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：针对 15 个场景、MIT 室内、SUN 397 数据集的单图像分类方法在准确度上的定量比较概述。
- en: '|  | 15 scenes | MIT Indoor | SUN 397 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | 15 个场景 | MIT 室内 | SUN 397 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Places365 AlexNet [[35](#bib.bib35)] | 89.25% | 70.72% | 56.12% |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Places365 AlexNet [[35](#bib.bib35)] | 89.25% | 70.72% | 56.12% |'
- en: '| Places365 GoogleNet[[35](#bib.bib35)] | 91.25% | 73.20% | 58.37% |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Places365 GoogleNet[[35](#bib.bib35)] | 91.25% | 73.20% | 58.37% |'
- en: '| Places365 VGG [[35](#bib.bib35)] | 91.97% | 76.53% | 63.24% |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Places365 VGG [[35](#bib.bib35)] | 91.97% | 76.53% | 63.24% |'
- en: '| Hybrid1365 VGG [[35](#bib.bib35)] | 92.15% | 79.49% | 61.77% |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Hybrid1365 VGG [[35](#bib.bib35)] | 92.15% | 79.49% | 61.77% |'
- en: '| 7-scale Hybrid VGG [[12](#bib.bib12)] | 94.08% | 80.22% | 63.19%* |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 7-scale Hybrid VGG [[12](#bib.bib12)] | 94.08% | 80.22% | 63.19%* |'
- en: '| 7-scale Hybrid AlexNet [[12](#bib.bib12)] | 93.90% | 80.97% | 65.38% |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 7-scale Hybrid AlexNet [[12](#bib.bib12)] | 93.90% | 80.97% | 65.38% |'
- en: 'Furthermore, in [[11](#bib.bib11)] the authors experimented with the ResNet152
    residual network architecture, fine-tuned over the Places365\. This work achieved
    a top-5 accuracy of 85.08% and 85.07% on the validation and, respectively, the
    test set of the Places365 dataset, as shown in Table [2](#S3.T2 "Table 2 ‣ 3.1
    Static scene recognition ‣ 3 Frameworks for scene recognition ‣ Deep learning
    for scene recognition from visual data: a survey").'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，在 [[11](#bib.bib11)] 中，作者对 ResNet152 残差网络架构进行了实验，并在 Places365 数据集上进行了微调。这项工作在
    Places365 数据集的验证集和测试集上分别达到了 85.08% 和 85.07% 的 top-5 准确率，如表 [2](#S3.T2 "Table 2
    ‣ 3.1 Static scene recognition ‣ 3 Frameworks for scene recognition ‣ Deep learning
    for scene recognition from visual data: a survey") 所示。'
- en: 'The use of the semantic and contextual composition of the image has been proposed
    by various approaches. For instance, in [[29](#bib.bib29)], the authors proposed
    the Hybrid1365 VGG architecture, a combination of deep learning techniques trained
    for object and scene recognition. The method uses different scales at which objects
    appear in a scene can facilitate the classification process by targeting distinct
    regions of interest within the image. Objects usually appear at lower scales.
    Therefore, the object classifier should target local scopes of the image. In contrast,
    the scene classifier should be aimed at the global scale, in order to capture
    contextual information. They concluded that it is possible to extend the performance
    obtained individually by each method. The Hybrid1365 VGG architecture [[29](#bib.bib29)]
    scores the highest average accuracy of 81.48% over all the experiments conducted
    for the place-centric CNN approach (has the highest performance for 2 out of 3
    comparison datasets as shown in Table [3](#S3.T3 "Table 3 ‣ 3.1 Static scene recognition
    ‣ 3 Frameworks for scene recognition ‣ Deep learning for scene recognition from
    visual data: a survey")).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '各种方法提出了图像的语义和上下文组合。例如，在 [[29](#bib.bib29)] 中，作者提出了 Hybrid1365 VGG 架构，这是一种为物体和场景识别训练的深度学习技术组合。该方法利用物体在场景中出现的不同尺度，通过针对图像中的不同兴趣区域来促进分类过程。物体通常出现在较低的尺度上，因此，物体分类器应针对图像的局部范围。相比之下，场景分类器应面向全局尺度，以捕捉上下文信息。他们总结道，可以扩展每种方法单独获得的性能。Hybrid1365
    VGG 架构 [[29](#bib.bib29)] 在所有针对地点中心 CNN 方法的实验中得到了最高的平均准确率 81.48%（在表 [3](#S3.T3
    "Table 3 ‣ 3.1 Static scene recognition ‣ 3 Frameworks for scene recognition ‣
    Deep learning for scene recognition from visual data: a survey") 中显示的 3 个比较数据集中的
    2 个具有最高性能）。'
- en: The dataset biases which arise under different scaling conditions of the images
    is addressed in [[12](#bib.bib12)], by involving a multi-scale model which combines
    various CNNs specialized either on object or place knowledge. The authors combined
    the training data available in the Places and ImageNet datasets. The knowledge
    learned from the two datasets is coupled in a scale-adaptive way. In order to
    aggregate the extracted features over the architectures used, simple max pooling¹¹1Max
    pooling is a pooling operation which computes the maximum value in each patch
    of a feature map; it is employed for down-sampling input representations. is adopted
    in order to down-sample the feature space. If the scaling operation is significant,
    the features of the data can drastically change from describing scene data to
    object data. The architectures are employed to extract features in parallel from
    patches, which represent the input image at increasingly larger scale versions.
    The multi-scale model combines several AlexNet architectures [[14](#bib.bib14)].
    The hybrid multi-scale architecture uses distinctive models for different scale
    ranges; depending on the scale range, the most suitable model is chosen from object-centric
    CNN (pre-trained on ImageNet), scene-centric CNN (pre-trained on Places365) or
    a fine-tuned CNN (adapted to the corresponding scale based on the dataset at hand).
    In total, seven scales were considered; the scales were obtained by scaling the
    original images between $227\times 227$ and $1827\times 1827$ pixels. For the
    final classification given by the multi-scale hybrid approach, the concatenation
    of the fc7 features (i.e. features extracted by the 7th fully connected layer
    of the CNN) from the seven networks are considered. Principal Component Analysis
    (PCA) is used to reduce the feature space. This model obtained the highest accuracy
    of 95.18% on the 15 scenes dataset [[16](#bib.bib16)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集偏差在图像不同缩放条件下的问题在[[12](#bib.bib12)]中得到了解决，涉及一种多尺度模型，该模型结合了专注于物体或场景知识的多种CNN。作者将Places和ImageNet数据集中可用的训练数据进行了结合。两种数据集中学到的知识以适应缩放的方式进行耦合。为了聚合在所使用架构上提取的特征，采用了简单的最大池化¹¹1最大池化是一种池化操作，它计算特征图中每个区域的最大值；用于下采样输入表示。方法以对特征空间进行下采样。如果缩放操作显著，数据的特征可能会从描述场景数据变化为物体数据。架构被用于从补丁中并行提取特征，这些补丁表示输入图像在越来越大尺度的版本下。多尺度模型结合了几种AlexNet架构[[14](#bib.bib14)]。混合多尺度架构使用不同尺度范围的独特模型；根据尺度范围，从物体中心CNN（在ImageNet上预训练）、场景中心CNN（在Places365上预训练）或精细调整的CNN（根据手头的数据集调整到相应尺度）中选择最合适的模型。总共考虑了七个尺度；这些尺度通过将原始图像缩放到$227\times
    227$和$1827\times 1827$像素之间获得。对于多尺度混合方法给出的最终分类，考虑了来自七个网络的fc7特征（即由CNN的第七个全连接层提取的特征）的串联。主成分分析（PCA）用于减少特征空间。该模型在15个场景数据集上获得了最高的95.18%准确率[[16](#bib.bib16)]。
- en: The hybrid approaches presented in [[29](#bib.bib29)] and [[12](#bib.bib12)]
    achieve higher accuracy than a human expert, which was quantified as 70.60%. This
    indicates that the combination of object-centric and scene-centric knowledge can
    potentially establish a new performance standard for scene recognition.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[29](#bib.bib29)]和[[12](#bib.bib12)]中提出的混合方法实现了比人类专家更高的准确率，该准确率定量为70.60%。这表明，物体中心和场景中心知识的结合可能为场景识别建立了新的性能标准。
- en: 3.2 Dynamic scene recognition
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 动态场景识别
- en: While early research in the field of scene recognition has been directed at
    single images, lately attention has been naturally drawn towards scene recognition
    from videos. CNNs have shown promising results for the general task of scene recognition
    in single images and have the potential to be also generalized to video data[[34](#bib.bib34),
    [33](#bib.bib33)]. To achieve this generalization, the spatio-temporal nature
    of dynamic scenes must be considered. While static scenes (depicted as single
    images) only present spatial features, videos also capture temporal transformations
    which affect the spatial aspect of the scene. Therefore, one challenge related
    to the task of scene classification from videos is creating a model which is powerful
    enough to capture both the spatial and temporal information of the scene. However,
    there are few works on video analysis for scene recognition.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管早期的场景识别研究主要集中在单幅图像上，但最近自然地转向了视频中的场景识别。卷积神经网络（CNN）在单幅图像的场景识别任务中表现出了有希望的结果，并有潜力推广到视频数据[[34](#bib.bib34),
    [33](#bib.bib33)]。为了实现这种泛化，必须考虑动态场景的时空特性。虽然静态场景（表现为单幅图像）仅呈现空间特征，但视频还捕捉了时间上的变化，这会影响场景的空间方面。因此，与视频中的场景分类任务相关的一个挑战是创建一个足够强大的模型，以捕捉场景的空间和时间信息。然而，目前关于视频分析用于场景识别的研究较少。
- en: In the works introduced in [[3](#bib.bib3), [22](#bib.bib22)], the authors relied
    on Long Short Term Memory networks (LSTMs) for video description. However, they
    did not focus on recognizing the scenes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[3](#bib.bib3), [22](#bib.bib22)]中介绍的工作中，作者依赖于长短期记忆网络（LSTM）进行视频描述。然而，他们并未专注于场景识别。
- en: 'Table 4: Overview of the results achieved by the spatio-temporal residual network
    (T-ResNet) proposed in [[8](#bib.bib8)] over the YUP++ dataset.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：[[8](#bib.bib8)]中提出的时空残差网络（T-ResNet）在YUP++数据集上取得的结果概述。
- en: '|  | YUP++ static | YUP++ moving | YUP++ complete |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | YUP++ 静态 | YUP++ 移动 | YUP++ 完整 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ResNet | 86.50% | 73.50% | 85.90% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ResNet | 86.50% | 73.50% | 85.90% |'
- en: '| T-ResNet | 92.41% | 81.50% | 89.00% |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| T-ResNet | 92.41% | 81.50% | 89.00% |'
- en: 'In [[8](#bib.bib8)], the authors introduced the T-ResNet architecture, alongside
    the YUP++ dataset, which established a new benchmark in the sub-field of dynamic
    scene recognition. The T-ResNet is based on a residual network [[27](#bib.bib27)]
    that was pre-trained on the ImageNet dataset [[6](#bib.bib6)]. It employs transfer
    learning to adapt the spatial-centric residual architecture to a spatio-temporal-centric
    network. The results achieved by the architecture were only compared with the
    classical ResNet architecture as shown in Table [4](#S3.T4 "Table 4 ‣ 3.2 Dynamic
    scene recognition ‣ 3 Frameworks for scene recognition ‣ Deep learning for scene
    recognition from visual data: a survey"). The superiority of the T-ResNet is evident:
    it achieves an accuracy of 92.41% on the YUP++ static camera partition, 81.50%
    on the YUP++ moving camera partition and finally 89.00% on the entire YUP++ dataset.
    This demonstrates the superiority of the spatio-temporal approach. The T-ResNet
    model exhibits strong performance for classes with linear motion patterns, e.g.
    classes ‘elevator’, ‘ocean’, ‘windmill farm’. However, for scene categories presenting
    irregular or mixed defining motion patterns the performance is negatively impacted,
    e.g. classes ‘snowing’ and ‘fireworks’. The authors of [[8](#bib.bib8)] observed
    that T-ResNet exhibits difficulties distinguishing intrinsic scene dynamics from
    the additional motion of the camera. Further research is required to account for
    this difference.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[8](#bib.bib8)]中，作者介绍了T-ResNet架构，并提供了YUP++数据集，这在动态场景识别的子领域建立了新的基准。T-ResNet基于一个在ImageNet数据集[[6](#bib.bib6)]上预训练的残差网络[[27](#bib.bib27)]。它采用迁移学习将以空间为中心的残差架构调整为以时空为中心的网络。该架构取得的结果仅与经典的ResNet架构进行比较，如表[4](#S3.T4
    "Table 4 ‣ 3.2 Dynamic scene recognition ‣ 3 Frameworks for scene recognition
    ‣ Deep learning for scene recognition from visual data: a survey")所示。T-ResNet的优越性显而易见：它在YUP++静态相机分区上的准确率为92.41%，在YUP++移动相机分区上的准确率为81.50%，在整个YUP++数据集上的准确率为89.00%。这表明了时空方法的优越性。T-ResNet模型在具有线性运动模式的类别上表现出强大的性能，例如‘电梯’、‘海洋’、‘风车农场’。然而，对于呈现不规则或混合定义运动模式的场景类别，性能受到负面影响，例如‘下雪’和‘烟花’。[[8](#bib.bib8)]的作者观察到T-ResNet在区分内在场景动态与相机额外运动方面存在困难。需要进一步研究来考虑这一差异。'
- en: 4 Discussion
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论
- en: The novel availability of large, exhaustive datasets, such as the Places Database,
    is offering significant support for further research for the challenge of scene
    recognition. The combination of scene-centric and object-centric knowledge has
    proven superior to only considering the scene context. Dynamic scene recognition
    reached new state-of-the-art performance through the approach of adapting spatial
    networks to the task, transforming the network to also consider the temporal aspect
    of the scenes. These emerging spatio-temporal networks are suitable for video
    data captured with a static camera. However, it still faces difficulties in the
    case of added camera motion.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 大型、详尽数据集的最新可用性，如 Places 数据库，为场景识别挑战的进一步研究提供了重要支持。场景中心和对象中心知识的结合已被证明优于仅考虑场景背景的做法。通过将空间网络适应任务、使网络同时考虑场景的时间方面，动态场景识别达到了新的最先进性能。这些新兴的时空网络适用于用静态相机捕获的视频数据。然而，在相机运动添加的情况下仍面临困难。
- en: One observation arising from methods addressing single image analysis scene
    recognition is that deeper CNN architectures such as GoogLeNet [[26](#bib.bib26)]
    or VGG [[25](#bib.bib25)] are not superior in all cases. For the hybrid multi-scale
    model combining scene-centric and object-centric networks in [[12](#bib.bib12)],
    experiments using VGG architecture for more than two-scales (two VGG networks)
    obtained disappointing results, inferior to the baseline performance achieved
    with one single scale (one network). Since the multi-scale hybrid model entails
    seven different scales, it can be inferred that VGG becomes noisy when applied
    on small input image patches.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从单图像分析场景识别方法中得到的一个观察结果是，更深层的 CNN 架构，如 GoogLeNet [[26](#bib.bib26)] 或 VGG [[25](#bib.bib25)]
    在所有情况下并不优越。对于结合场景中心和对象中心网络的混合多尺度模型 [[12](#bib.bib12)]，使用 VGG 架构进行超过两尺度的实验（两个 VGG
    网络）得到了令人失望的结果，性能逊色于仅使用单一尺度（一个网络）所取得的基准性能。由于多尺度混合模型涉及七种不同尺度，可以推断出 VGG 在应用于小输入图像补丁时会变得嘈杂。
- en: Addressing the task of scene recognition from the global features that describe
    an image, the CNNs are expected to learn deep features that are relevant for the
    contextual clues present in the image. Literature [[35](#bib.bib35)] observers
    that the low-level convolutional layers detect low-level visual concepts such
    as object edges and textures, while the high-level layers activate on entire objects
    and scene parts. Even though the model has been previously trained on an exclusively
    places-centric dataset, the network still identifies semantic clues in the image
    by detecting objects alongside contextual clues. Therefore, CNNs trained on the
    Places Database (which does not contain object labels) could still be employed
    for object detection.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 针对描述图像的全局特征进行场景识别任务时，CNNs 被期望学习与图像中存在的上下文线索相关的深层特征。文献 [[35](#bib.bib35)] 观察到低级卷积层检测低级视觉概念，如物体边缘和纹理，而高级层则激活于整个物体和场景部分。尽管模型之前已在专门针对场景的数据集上训练，但网络仍通过检测物体以及上下文线索来识别图像中的语义线索。因此，虽然
    Places 数据库中不包含对象标签，但在该数据库上训练的 CNNs 仍可用于对象检测。
- en: 'Another aspect arising from training the same architecture on datasets with
    a different number of scene categories (i.e. and Places365) proves that having
    more categories leads to better results as well as more predicted categories.
    We can observe that the architecture AlexNet trained on Places205 (version prior
    to Places365) obtains 57.2% accuracy, while the same architecture trained on Places365
    obtains 57.7% accuracy. For the places CNN approach two main types of miss-classifications
    occur: on one hand, less-typical activities happening in a scene context (e.g.
    taking a photo at a construction site) and on the other hand, images depicting
    multiple scene parts. A possible solution, as proposed by [[35](#bib.bib35)],
    would be assigned multiple ground-truth labels in order to capture the content
    of an image more precisely.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从在不同数量场景类别（即 Places365）数据集上训练相同架构得到的另一个方面证明，拥有更多类别会导致更好的结果以及更多预测类别。我们可以观察到，AlexNet
    在 Places205（Places365 之前的版本）上训练获得 57.2% 的准确率，而相同架构在 Places365 上训练获得 57.7% 的准确率。对于
    places CNN 方法，主要存在两种类型的错误分类：一方面，场景背景中发生的不典型活动（例如在建筑工地拍照）；另一方面，描绘多个场景部分的图像。[[35](#bib.bib35)]
    提出的一个可能解决方案是分配多个真实标签，以更准确地捕捉图像内容。
- en: The results achieved by the T-ResNet model illustrate the potential of spatio-temporal
    networks for video analysis. The transformation from a purely spatial network
    to a spatio-temporal one can succeed on the basis of a very small training set
    (i.e. only 10% of the YUP++ dataset introduced) as proven by [[8](#bib.bib8)].
    Well-initialized spatial networks can be efficiently transformed to extract spatio-temporal
    features, therefore, in theory, most networks that perform well on single image
    analysis could be easily adapted to video analysis.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: T-ResNet模型取得的结果展示了时空网络在视频分析中的潜力。根据[[8](#bib.bib8)]的证明，从纯粹的空间网络到时空网络的转化可以在非常小的训练集（即仅10%的YUP++数据集）基础上成功。经过良好初始化的空间网络可以高效地转化以提取时空特征，因此，理论上，大多数在单图像分析中表现良好的网络可以轻松适应视频分析。
- en: 5 Conclusions
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we describe the state-of-the-art on deep learning for scene recognition.
    Furthermore, we presented some of the applications of scene recognition to emphasize
    the importance of this topic. We argue that the main factor to consider is the
    type of data on which recognition and classification are applied. Since the task
    of scene recognition is not entirely subjective due to the nature of the scene
    images and the scene categories overlap, no one particular method can be generalized
    to all scene recognition tasks. This paper will aid professionals in making an
    informed decision about which approach best fits their scene recognition challenge.
    We have found room for research in the field of video analysis and expect that
    numerous works will emerge in the coming years.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们描述了深度学习在场景识别中的最新进展。此外，我们展示了一些场景识别的应用，以强调这一主题的重要性。我们认为，主要需要考虑的因素是应用于识别和分类的数据类型。由于场景识别任务的主观性不完全由于场景图像的性质以及场景类别的重叠，因此没有一种特定的方法可以概括所有的场景识别任务。本文将帮助专业人员做出明智的决策，选择最适合他们场景识别挑战的方法。我们发现视频分析领域有进一步研究的空间，并预计未来几年会出现大量相关的研究工作。
- en: References
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Bacha, S., Allili, M.S., Benblidia, N.: Event recognition in photo albums
    using probabilistic graphical models and feature relevance. Journal of Visual
    Communication and Image Representation 40, 546–558 (2016)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Bacha, S., Allili, M.S., Benblidia, N.：使用概率图模型和特征相关性进行照片集中的事件识别。视觉通信与图像表示杂志
    40，546–558（2016年）'
- en: '[2] Baumgartl, H., Buettner, R.: Development of a highly precise place recognition
    module for effective human-robot interactions in changing lighting and viewpoint
    conditions. In: Proceedings of the 53rd Hawaii International Conference on System
    Sciences (2020)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Baumgartl, H., Buettner, R.：在变化的光照和视点条件下，为有效的人机交互开发一个高精度的地点识别模块。在：第53届夏威夷国际系统科学会议论文集（2020年）'
- en: '[3] Bin, Y., Yang, Y., Shen, F., Xie, N., Shen, H.T., Li, X.: Describing video
    with attention-based bidirectional lstm. IEEE transactions on cybernetics 49(7),
    2631–2641 (2018)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Bin, Y., Yang, Y., Shen, F., Xie, N., Shen, H.T., Li, X.：使用基于注意力的双向lstm描述视频。IEEE网络学报
    49(7)，2631–2641（2018年）'
- en: '[4] Bosch, A., Muñoz, X., Martí, R.: Which is the best way to organize/classify
    images by content? Image and vision computing 25(6), 778–791 (2007)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Bosch, A., Muñoz, X., Martí, R.：按内容组织/分类图像的最佳方法是什么？图像与视觉计算 25(6)，778–791（2007年）'
- en: '[5] Chaves, D., Ruiz-Sarmiento, J., Petkov, N., Gonzalez-Jimenez, J.: Integration
    of cnn into a robotic architecture to build semantic maps of indoor environments.
    In: International Work-Conference on Artificial Neural Networks. pp. 313–324\.
    Springer (2019)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Chaves, D., Ruiz-Sarmiento, J., Petkov, N., Gonzalez-Jimenez, J.：将cnn集成到机器人架构中以构建室内环境的语义地图。在：国际人工神经网络工作会议。第313–324页。Springer（2019年）'
- en: '[6] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet:
    A large-scale hierarchical image database. In: 2009 IEEE conference on computer
    vision and pattern recognition. pp. 248–255\. Ieee (2009)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.：Imagenet：一个大规模层次化图像数据库。在：2009年IEEE计算机视觉与模式识别会议。第248–255页。IEEE（2009年）'
- en: '[7] Derpanis, K.G., Lecce, M., Daniilidis, K., Wildes, R.P.: Dynamic scene
    understanding: The role of orientation features in space and time in scene classification.
    In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 1306–1313
    (2012)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Derpanis, K.G., Lecce, M., Daniilidis, K., Wildes, R.P.：动态场景理解：在场景分类中，方向特征在空间和时间中的作用。在：IEEE计算机视觉与模式识别会议。第1306–1313页（2012年）'
- en: '[8] Feichtenhofer, C., Pinz, A., Wildes, R.P.: Temporal residual networks for
    dynamic scene recognition. In: Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition. pp. 4728–4737 (2017)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Feichtenhofer, C., Pinz, A., Wildes, R.P.: 动态场景识别的时间残差网络。发表于：IEEE计算机视觉与模式识别会议论文集。第4728–4737页（2017）'
- en: '[9] Furnari, A., Farinella, G.M., Battiato, S.: Temporal segmentation of egocentric
    videos to highlight personal locations of interest. In: European Conference on
    Computer Vision. pp. 474–489\. Springer (2016)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Furnari, A., Farinella, G.M., Battiato, S.: 自中心视频的时间分割以突出个人兴趣地点。发表于：欧洲计算机视觉会议。第474–489页。Springer（2016）'
- en: '[10] Griffin, G., Holub, A., Perona, P.: Caltech-256 object category dataset
    (2007)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Griffin, G., Holub, A., Perona, P.: Caltech-256 物体类别数据集（2007）'
- en: '[11] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
    recognition. In: Proceedings of the IEEE conference on computer vision and pattern
    recognition. pp. 770–778 (2016)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] He, K., Zhang, X., Ren, S., Sun, J.: 用于图像识别的深度残差学习。发表于：IEEE计算机视觉与模式识别会议论文集。第770–778页（2016）'
- en: '[12] Herranz, L., Jiang, S., Li, X.: Scene recognition with cnns: objects,
    scales and dataset bias. In: Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition. pp. 571–579 (2016)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Herranz, L., Jiang, S., Li, X.: 使用 CNN 进行场景识别：物体、尺度和数据集偏差。发表于：IEEE计算机视觉与模式识别会议论文集。第571–579页（2016）'
- en: '[13] Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features
    from tiny images (2009)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Krizhevsky, A., Hinton, G., 等：从微小图像中学习多个特征层（2009）'
- en: '[14] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with
    deep convolutional neural networks. In: Advances in neural information processing
    systems. pp. 1097–1105 (2012)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Krizhevsky, A., Sutskever, I., Hinton, G.E.: 使用深度卷积神经网络的 ImageNet 分类。发表于：神经信息处理系统进展。第1097–1105页（2012）'
- en: '[15] Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset,
    J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., Duerig, T., Ferrari, V.:
    The open images dataset v4: Unified image classification, object detection, and
    visual relationship detection at scale. IJCV (2020)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset,
    J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., Duerig, T., Ferrari, V.:
    开放图像数据集 v4：统一的图像分类、物体检测和视觉关系检测。IJCV（2020）'
- en: '[16] Lazebnik, S., Schmid, C., Ponce, J.: Beyond bags of features: Spatial
    pyramid matching for recognizing natural scene categories. In: 2006 IEEE Computer
    Society Conference on Computer Vision and Pattern Recognition (CVPR’06). vol. 2,
    pp. 2169–2178\. IEEE (2006)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Lazebnik, S., Schmid, C., Ponce, J.: 超越特征袋：用于识别自然场景类别的空间金字塔匹配。发表于：2006年IEEE计算机视觉与模式识别会议（CVPR''06）。第2卷，第2169–2178页。IEEE（2006）'
- en: '[17] Li, L.J., Su, H., Lim, Y., Fei-Fei, L.: Objects as attributes for scene
    classification. In: European conference on computer vision. pp. 57–69. Springer
    (2010)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Li, L.J., Su, H., Lim, Y., Fei-Fei, L.: 作为场景分类属性的物体。发表于：欧洲计算机视觉会议。第57–69页。Springer（2010）'
- en: '[18] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
    Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European
    conference on computer vision. pp. 740–755\. Springer (2014)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
    Dollár, P., Zitnick, C.L.: 微软 COCO：上下文中的常见物体。发表于：欧洲计算机视觉会议。第740–755页。Springer（2014）'
- en: '[19] Martinez, E.T., Leyva-Vallina, M., Sarker, M.K., Puig, D., Petkov, N.,
    Radeva, P.: Hierarchical approach to classify food scenes in egocentric photo-streams.
    IEEE journal of biomedical and health informatics (2019)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Martinez, E.T., Leyva-Vallina, M., Sarker, M.K., Puig, D., Petkov, N.,
    Radeva, P.: 层次方法用于分类自中心照片流中的食物场景。IEEE 生物医学与健康信息学杂志（2019）'
- en: '[20] Nene, S.A., Nayar, S.K., Murase, H., et al.: Columbia object image library
    (1996)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Nene, S.A., Nayar, S.K., Murase, H., 等：哥伦比亚物体图像库（1996）'
- en: '[21] Othman, K.M., Rad, A.B.: An indoor room classification system for social
    robots via integration of cnn and ecoc. Applied Sciences 9(3),  470 (2019)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Othman, K.M., Rad, A.B.: 通过整合 CNN 和 ECOC 的室内房间分类系统用于社交机器人。应用科学 9(3)，470（2019）'
- en: '[22] Peris, Á., Bolaños, M., Radeva, P., Casacuberta, F.: Video description
    using bidirectional recurrent neural networks. In: International Conference on
    Artificial Neural Networks. pp. 3–11\. Springer (2016)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Peris, Á., Bolaños, M., Radeva, P., Casacuberta, F.: 使用双向递归神经网络的视频描述。发表于：国际人工神经网络会议。第3–11页。Springer（2016）'
- en: '[23] Quattoni, A., Torralba, A.: Recognizing indoor scenes. In: 2009 IEEE Conference
    on Computer Vision and Pattern Recognition. pp. 413–420\. IEEE (2009)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Quattoni, A., Torralba, A.: 识别室内场景。发表于：2009年IEEE计算机视觉与模式识别会议。第413–420页。IEEE（2009）'
- en: '[24] Shroff, N., Turaga, P., Chellappa, R.: Moving vistas: Exploiting motion
    for describing scenes. In: 2010 IEEE Computer Society Conference on Computer Vision
    and Pattern Recognition. pp. 1911–1918\. IEEE (2010)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Shroff, N., Turaga, P., Chellappa, R.: 移动景观：利用运动描述场景。见：2010年IEEE计算机视觉与模式识别会议。第1911–1918页。IEEE（2010年）'
- en: '[25] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
    image recognition. arXiv preprint arXiv:1409.1556 (2014)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Simonyan, K., Zisserman, A.: 用于大规模图像识别的非常深的卷积网络。arXiv预印本arXiv:1409.1556（2014年）'
- en: '[26] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan,
    D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings
    of the IEEE conference on computer vision and pattern recognition. pp. 1–9 (2015)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan,
    D., Vanhoucke, V., Rabinovich, A.: 深入探讨卷积网络。见：IEEE计算机视觉与模式识别会议论文集。第1–9页（2015年）'
- en: '[27] Thorpe, M., van Gennip, Y.: Deep limits of residual neural networks. arXiv
    preprint arXiv:1810.11741 (2018)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Thorpe, M., van Gennip, Y.: 残差神经网络的深层极限。arXiv预印本arXiv:1810.11741（2018年）'
- en: '[28] Viswanathan, P., Southey, T., Little, J., Mackworth, A.: Place classification
    using visual object categorization and global information. In: 2011 Canadian Conference
    on Computer and Robot Vision. pp. 1–7\. IEEE (2011)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Viswanathan, P., Southey, T., Little, J., Mackworth, A.: 利用视觉物体分类和全局信息进行场所分类。见：2011年加拿大计算机与机器人视觉会议。第1–7页。IEEE（2011年）'
- en: '[29] Wang, L., Wang, Z., Du, W., Qiao, Y.: Object-scene convolutional neural
    networks for event recognition in images. CVPR, ChaLearn Looking at People (LAP)
    challenge (2015)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Wang, L., Wang, Z., Du, W., Qiao, Y.: 用于图像事件识别的对象-场景卷积神经网络。CVPR, ChaLearn注视人（LAP）挑战赛（2015年）'
- en: '[30] Wang, L., Guo, S., Huang, W., Xiong, Y., Qiao, Y.: Knowledge guided disambiguation
    for large-scale scene classification with multi-resolution cnns. IEEE Transactions
    on Image Processing 26(4), 2055–2068 (2017)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Wang, L., Guo, S., Huang, W., Xiong, Y., Qiao, Y.: 知识引导的歧义消解用于多分辨率cnn的大规模场景分类。IEEE图像处理汇刊26(4)，第2055–2068页（2017年）'
- en: '[31] Xiao, J., Ehinger, K.A., Hays, J., Torralba, A., Oliva, A.: Sun database:
    Exploring a large collection of scene categories. International Journal of Computer
    Vision 119(1), 3–22 (2016)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Xiao, J., Ehinger, K.A., Hays, J., Torralba, A., Oliva, A.: Sun数据库：探索大量场景类别。国际计算机视觉杂志119(1)，第3–22页（2016年）'
- en: '[32] Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun database:
    Large-scale scene recognition from abbey to zoo. In: 2010 IEEE Computer Society
    Conference on Computer Vision and Pattern Recognition. pp. 3485–3492\. IEEE (2010)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun数据库：从修道院到动物园的大规模场景识别。见：2010年IEEE计算机视觉与模式识别会议。第3485–3492页。IEEE（2010年）'
- en: '[33] Xu, Z., Yang, Y., Hauptmann, A.G.: A discriminative cnn video representation
    for event detection. In: Proceedings of the IEEE conference on computer vision
    and pattern recognition. pp. 1798–1807 (2015)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Xu, Z., Yang, Y., Hauptmann, A.G.: 用于事件检测的判别性cnn视频表示。见：IEEE计算机视觉与模式识别会议论文集。第1798–1807页（2015年）'
- en: '[34] Yue-Hei Ng, J., Hausknecht, M., Vijayanarasimhan, S., Vinyals, O., Monga,
    R., Toderici, G.: Beyond short snippets: Deep networks for video classification.
    In: Proceedings of the IEEE conference on computer vision and pattern recognition.
    pp. 4694–4702 (2015)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Yue-Hei Ng, J., Hausknecht, M., Vijayanarasimhan, S., Vinyals, O., Monga,
    R., Toderici, G.: 超越短片段：用于视频分类的深度网络。见：IEEE计算机视觉与模式识别会议论文集。第4694–4702页（2015年）'
- en: '[35] Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places:
    A 10 million image database for scene recognition. IEEE Transactions on Pattern
    Analysis and Machine Intelligence (2017)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places:
    一个用于场景识别的1000万图像数据库。IEEE模式分析与机器智能汇刊（2017年）'
