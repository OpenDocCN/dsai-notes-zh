- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:37:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:37:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2307.05826] Detection, Instance Segmentation, and Classification for Astronomical
    Surveys with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration
    with Hyper Suprime-Cam Data'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2307.05826] 基于深度学习的天文调查中的检测、实例分割和分类（DeepDISC）：Detectron2 实现及 Hyper Suprime-Cam
    数据演示'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.05826](https://ar5iv.labs.arxiv.org/html/2307.05826)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2307.05826](https://ar5iv.labs.arxiv.org/html/2307.05826)
- en: 'Detection, Instance Segmentation, and Classification for Astronomical Surveys
    with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration with
    Hyper Suprime-Cam Data'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的天文调查中的检测、实例分割和分类（DeepDISC）：Detectron2 实现及 Hyper Suprime-Cam 数据演示
- en: Grant Merz,¹ Yichen Liu,¹ Colin J. Burke,¹ Patrick D. Aleo,¹ Xin Liu,^(1,2,3)
    Matias Carrasco Kind,^(1,2) Volodymyr Kindratenko,^(2,3,4,5) Yufeng Liu⁶
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Grant Merz,¹ Yichen Liu,¹ Colin J. Burke,¹ Patrick D. Aleo,¹ Xin Liu,^(1,2,3)
    Matias Carrasco Kind,^(1,2) Volodymyr Kindratenko,^(2,3,4,5) Yufeng Liu⁶
- en: ¹Department of Astronomy, University of Illinois at Urbana-Champaign, 1002 West
    Green Street, Urbana, IL 61801, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹天文学系，伊利诺伊大学厄本那-香槟分校，1002 West Green Street, Urbana, IL 61801, USA
- en: ²National Center for Supercomputing Applications, University of Illinois at
    Urbana-Champaign, 1205 West Clark Street, Urbana, IL 61801, USA
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²国家超级计算应用中心，伊利诺伊大学厄本那-香槟分校，1205 West Clark Street, Urbana, IL 61801, USA
- en: ³Center for Artificial Intelligence Innovation, University of Illinois at Urbana-Champaign,
    1205 West Clark Street, Urbana, IL 61801, USA
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³人工智能创新中心，伊利诺伊大学厄本那-香槟分校，1205 West Clark Street, Urbana, IL 61801, USA
- en: ⁴Department of Computer Science, University of Illinois at Urbana-Champaign,
    201 North Goodwin Avenue, Urbana, IL 61801, USA
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴计算机科学系，伊利诺伊大学厄本那-香槟分校，201 North Goodwin Avenue, Urbana, IL 61801, USA
- en: ⁵Department of Electrical and Computer Engineering, University of Illinois at
    Urbana-Champaign, 306 North Wright Street, Urbana, IL 61801, USA
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵电气与计算机工程系，伊利诺伊大学厄本那-香槟分校，306 North Wright Street, Urbana, IL 61801, USA
- en: '⁶Department of Physics, University of Illinois at Urbana-Champaign, 1110 West
    Green Street, Urbana, IL 61801, USA E-mail: gmerz3@illinois.edu(Accepted XXX.
    Received YYY; in original form ZZZ)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ⁶物理学系，伊利诺伊大学厄本那-香槟分校，1110 West Green Street, Urbana, IL 61801, USA 电子邮件：gmerz3@illinois.edu（接受时间
    XXX。收到时间 YYY；原稿形式 ZZZ）
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The next generation of wide-field deep astronomical surveys will deliver unprecedented
    amounts of images through the 2020s and beyond. As both the sensitivity and depth
    of observations increase, more blended sources will be detected. This reality
    can lead to measurement biases that contaminate key astronomical inferences. We
    implement new deep learning models available through Facebook AI Research’s Detectron2
    repository to perform the simultaneous tasks of object identification, deblending,
    and classification on large multi-band coadds from the Hyper Suprime-Cam (HSC).
    We use existing detection/deblending codes and classification methods to train
    a suite of deep neural networks, including state-of-the-art transformers. Once
    trained, we find that transformers outperform traditional convolutional neural
    networks and are more robust to different contrast scalings. Transformers are
    able to detect and deblend objects closely matching the ground truth, achieving
    a median bounding box Intersection over Union of 0.99\. Using high quality class
    labels from the Hubble Space Telescope, we find that the best-performing networks
    can classify galaxies with near 100% completeness and purity across the whole
    test sample and classify stars above 60% completeness and 80% purity out to HSC
    i-band magnitudes of 25 mag. This framework can be extended to other upcoming
    deep surveys such as the Legacy Survey of Space and Time and those with the Roman
    Space Telescope to enable fast source detection and measurement. Our code, DeepDISC,
    is publicly available at [https://github.com/grantmerz/deepdisc](https://github.com/grantmerz/deepdisc).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下一代广视场深空天文调查将在2020年代及以后提供前所未有的图像数据。随着观察的灵敏度和深度的提高，将会检测到更多的混合源。这种现象可能导致测量偏差，从而污染关键的天文推断。我们采用Facebook
    AI Research的Detectron2库中的新深度学习模型，在Hyper Suprime-Cam (HSC)的大规模多波段合成图像上执行物体识别、去混叠和分类这三个任务。我们使用现有的检测/去混叠代码和分类方法来训练一系列深度神经网络，包括最先进的transformers。训练后，我们发现transformers在不同对比度缩放下的表现优于传统的卷积神经网络，并且更为稳健。Transformers能够检测并去混叠与实际情况紧密匹配的物体，实现了0.99的中位数边界框交并比（Intersection
    over Union）。利用来自哈勃太空望远镜的高质量类别标签，我们发现表现最好的网络能够以接近100%的完整性和纯度对整个测试样本进行星系分类，并且在HSC
    i带幅度达到25 mag的情况下，星星分类的完整性超过60%，纯度超过80%。这个框架可以扩展到其他即将进行的深空调查，例如Legacy Survey of
    Space and Time和罗马太空望远镜的数据，以实现快速的源检测和测量。我们的代码DeepDISC可以在[https://github.com/grantmerz/deepdisc](https://github.com/grantmerz/deepdisc)上公开获取。
- en: 'keywords:'
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'techniques: image processing – methods: data analysis – galaxies: general –
    Sky Surveys^†^†pubyear: 2023^†^†pagerange: Detection, Instance Segmentation, and
    Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data–[6](#A1.T6 "Table
    6 ‣ Appendix A DECam results ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data")'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 技术：图像处理 – 方法：数据分析 – 星系：一般 – 天空调查^†^†出版年份：2023^†^†页码范围：利用深度学习（DeepDISC）进行天文调查的检测、实例分割和分类：Detectron2在Hyper
    Suprime-Cam数据上的实现和演示–[6](#A1.T6 "表 6 ‣ 附录 A DECam 结果 ‣ 利用深度学习（DeepDISC）进行天文调查的检测、实例分割和分类：Detectron2在Hyper
    Suprime-Cam数据上的实现和演示")
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The rise of machine learning/artificial intelligence has allowed for rapid advancement
    in many image analysis tasks to the benefit of researchers who wish to work with
    large sets of imaging data. This active field of study, known as computer vision,
    has led to developments in many disciplines including medical imaging (Zhou et al.,
    [2021](#bib.bib74)), urban planning (Ibrahim et al., [2020](#bib.bib37)), autonomous
    systems (Pavel et al., [2022](#bib.bib62)) and more.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习/人工智能的崛起使得许多图像分析任务得以快速发展，这对希望处理大量成像数据的研究人员来说大有裨益。这个活跃的研究领域被称为计算机视觉，已在许多学科中取得了进展，包括医学成像（Zhou
    et al., [2021](#bib.bib74)）、城市规划（Ibrahim et al., [2020](#bib.bib37)）、自主系统（Pavel
    et al., [2022](#bib.bib62)）等。
- en: Tasks such as image compression, inpainting, object classification and detection,
    and many others have been extensively studied. Astronomy is no exception, and
    many methods that utilize deep learning have been applied to simulations and real
    survey data for tasks such as object detection, star/galaxy classification, photometric
    redshift estimation, image generation, deblending and more (see Huertas-Company
    & Lanusse [2023](#bib.bib35) for a comprehensive review). Machine learning methods
    are already becoming instrumental in handling the large volume of data processed
    every day in survey pipelines (e.g., Bosch et al., [2018](#bib.bib11); Russeil
    et al., [2022](#bib.bib67); Tachibana & Miller, [2018](#bib.bib70); Malanchev
    et al., [2021](#bib.bib54); Mahabal et al., [2019](#bib.bib53))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图像压缩、修复、物体分类与检测等任务已经得到了广泛研究。天文学也不例外，许多利用深度学习的方法已被应用于模拟和实际观测数据中，进行如物体检测、星体/星系分类、光度红移估计、图像生成、去混叠等任务（详见
    Huertas-Company & Lanusse [2023](#bib.bib35)）。机器学习方法已经在处理每天在调查管道中处理的大量数据方面发挥了重要作用（例如，Bosch
    et al., [2018](#bib.bib11); Russeil et al., [2022](#bib.bib67); Tachibana & Miller,
    [2018](#bib.bib70); Malanchev et al., [2021](#bib.bib54); Mahabal et al., [2019](#bib.bib53)）。
- en: The next generation of astronomical surveys such as the upcoming Legacy Survey
    of Space and Time (LSST; Ivezić et al., [2019](#bib.bib38)) at the Vera C. Rubin
    Observatory, the Wide-Field Imaging Survey at the Nancy Grace Roman Space Telescope
    (Roman; Spergel et al., [2013](#bib.bib69)), and Euclid (Amiaux et al., [2012](#bib.bib5))
    will produce unprecedented amounts of imaging data throughout the 2020s and beyond.
    LSST will provide incredibly deep ground-based observations of the sky, revealing
    a map of the universe including objects as faint as $\sim$25-27 mag at a 5$\sigma$
    detection for 10 year observing runs. Ground-based surveys such as the Hyper Suprime-Cam
    Subaru Strategic Program (HSC SSP; Aihara et al., [2018a](#bib.bib1)) and the
    Dark Energy Survey (DES; Dark Energy Survey Collaboration et al., [2016](#bib.bib20))
    have already mapped large swaths of the sky and produced catalogs of tens of millions
    of objects, with HSC depths being comparable to LSST. The astronomical research
    community is now in an era that demands robust and efficient techniques to detect
    and analyze sources in images.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下一代天文调查，如即将开展的太空与时间遗产调查（LSST；Ivezić et al., [2019](#bib.bib38)）于 Vera C. Rubin
    天文台、Nancy Grace Roman 太空望远镜的广域成像调查（Roman；Spergel et al., [2013](#bib.bib69)）以及欧几里得（Amiaux
    et al., [2012](#bib.bib5)），将在2020年代及以后产生前所未有的成像数据。LSST 将提供极为深度的地基观测，揭示一个包括暗弱天体（如$\sim$25-27
    mag，5$\sigma$ 检测限）的宇宙地图，为期10年的观测运行。地基调查，如 Hyper Suprime-Cam Subaru 战略计划（HSC SSP；Aihara
    et al., [2018a](#bib.bib1)）和暗能量调查（DES；暗能量调查合作组 et al., [2016](#bib.bib20)）已经绘制了大面积的天空图，并产生了数千万个天体的目录，HSC
    深度与 LSST 相当。天文学研究界正处于一个需要强大而高效技术来检测和分析图像中天体的时代。
- en: Current surveys such as HSC already report large fractions of blended (overlapping)
    objects. For instance, 58% of objects in the the shallowest field (Wide) of the
    HSC survey are blended, i.e., detected in a region of sky above the 5$\sigma$
    threshold (26.2 mag) containing multiple significant peaks in surface brightness.
    As depths increase, line-of-sight projections and physical mergers cause the overall
    number of blends to increase. This fraction rises to 66% for the Deep and 74%
    for the UltraDeep layers, which are comparable to LSST depths (Bosch et al., [2018](#bib.bib11)).
    If blends are not identified, they will bias results from pipelines that assume
    object isolation. For example, Boucaud et al. ([2020](#bib.bib12)) show that the
    traditional detection/deblending methods can lead to a photometric error of >0.75
    mag for $\sim$12% of their sample of artificially blended galaxies from the Cosmic
    Assembly Near-infrared Deep Extragalactic Legacy survey (CANDELS Grogin et al.,
    [2011](#bib.bib29); Koekemoer et al., [2011](#bib.bib42)). Unrecognized blends
    can cause an increase in the noise of galaxy shear measurements by $\sim$14% for
    deep observations (Dawson et al., [2016](#bib.bib21)). Deblending, or source separation,
    has been recognized as a high priority in survey science, especially as LSST begins
    preparations for first light.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的调查，如HSC，已经报告了大量的混叠（重叠）对象。例如，HSC调查中最浅的区域（宽场）的58%对象是混叠的，即，在包含多个显著表面亮度峰值的天空区域中被检测到，超出了5$\sigma$阈值（26.2
    mag）。随着深度的增加，视线投影和物理合并导致混叠对象的总数增加。这一比例在Deep层上升至66%，在UltraDeep层上升至74%，这与LSST的深度相当（Bosch
    et al., [2018](#bib.bib11)）。如果混叠对象未被识别，它们将使假设对象孤立的管道结果产生偏差。例如，Boucaud et al. ([2020](#bib.bib12))
    显示，传统的检测/去混叠方法可能会导致来自宇宙组装近红外深外星系遗产调查（CANDELS Grogin et al., [2011](#bib.bib29);
    Koekemoer et al., [2011](#bib.bib42)）的人工混叠星系样本中约12%的光度误差超过0.75 mag。未识别的混叠可能导致深度观测中星系剪切测量的噪声增加约14%（Dawson
    et al., [2016](#bib.bib21)）。去混叠或源分离已被认定为调查科学中的高优先级任务，特别是当LSST开始准备首次观测时。
- en: Despite rigorous efforts to deblend objects, the problem of deblending remains,
    and in some sense will always remain in astronomical studies. Deblending involves
    separating a mixture of signals in order to independently measure properties of
    each individual object. This an imaging problem analogous to the “cocktail party
    problem”, in which an attempt is made to isolate individual voices from a mixture
    of conversations. However, since it is impossible to trace a photon back to an
    individual source, astronomical deblending is characterized as an under-constrained
    inverse problem. Deblending methods must rely on assumptions about source properties
    and models of signal mixing (Melchior et al., [2021](#bib.bib56)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管经过严格的去混叠对象的努力，去混叠问题仍然存在，在某种意义上，它在天文学研究中将始终存在。去混叠涉及分离混合的信号，以独立测量每个单独对象的属性。这是一个成像问题，类似于“鸡尾酒会问题”，即尝试从混合的对话中隔离出个体声音。然而，由于无法追踪光子回到单一来源，天文学中的去混叠被称为一个约束不足的逆问题。去混叠方法必须依赖于关于源属性的假设和信号混合的模型（Melchior
    et al., [2021](#bib.bib56)）。
- en: A first step in deblending is object detection. Many codes have been developed
    for source detection and classification, including FOCAS (Jarvis & Tyson, [1981](#bib.bib39)),
    NEXT (Andreon et al., [2000](#bib.bib6)) and SExtractor (Bertin & Arnouts, [1996](#bib.bib9)).
    SExtractor is widely used in survey pipelines including HSC (Bosch et al., [2018](#bib.bib11))
    and DES (Morganson et al., [2018](#bib.bib59)), but can be sensitive to configuration
    parameters. While SExtractor also deblends by segmenting, or identifying pixels
    belonging to unique sources, modern deblenders have been developed such as Morpheus
    (Hausen & Robertson, [2020](#bib.bib30)) and Scarlet, (Melchior et al., [2018](#bib.bib55))
    with the latter implemented in HSC and LSST pipelines. With hopes for real-time
    object detection and deblending algorithms in surveys such as LSST, machine learning
    applications to crowded fields offer a promising avenue. The use of deep neural
    networks, or deep learning has seen particular success in image processing. In
    addition to efficiency and flexibility, neural networks may be able to overcome
    limitations of traditional peak-finding algorithms due to their fundamentally
    different detection mechanism.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 去混叠的第一步是目标检测。已经开发了许多源检测和分类的代码，包括FOCAS（Jarvis & Tyson, [1981](#bib.bib39)）、NEXT（Andreon
    et al., [2000](#bib.bib6)）和SExtractor（Bertin & Arnouts, [1996](#bib.bib9)）。SExtractor在包括HSC（Bosch
    et al., [2018](#bib.bib11)）和DES（Morganson et al., [2018](#bib.bib59)）的调查管道中被广泛使用，但可能对配置参数敏感。虽然SExtractor通过分割或识别属于唯一源的像素来去混叠，但现代去混叠器如Morpheus（Hausen
    & Robertson, [2020](#bib.bib30)）和Scarlet（Melchior et al., [2018](#bib.bib55)）已经开发，其中后者已在HSC和LSST管道中实施。随着对LSST等调查中实时目标检测和去混叠算法的期望，机器学习在拥挤领域的应用提供了一个有前途的方向。深度神经网络或深度学习在图像处理方面取得了特别成功。除了效率和灵活性，神经网络还可能克服传统峰值查找算法的局限性，因为它们具有根本不同的检测机制。
- en: There is a growing body of deep learning deblending methods in astronomy. Reiman
    & Göhre ([2019](#bib.bib65)) use a Generative Adversarial Network (GAN) to deblend
    small cutouts of Sloan Digital Sky Survey (SDSS Alam et al., [2015](#bib.bib4))
    galaxies from Galaxy Zoo (Lintott et al., [2011](#bib.bib49)). Arcelin et al.
    ([2021](#bib.bib7)) use a variational autoencoder to deblend small cutouts of
    simulated LSST galaxies. Hemmati et al. ([2022](#bib.bib34)) use GANs to deblend
    images with HSC resolution and recover Hubble Space Telescope resolution. On larger
    scales, Bretonnière et al. ([2021](#bib.bib13)) use a probabilistic U-net model
    to deblend large simulated scenes of galaxies.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 天文学中深度学习去混叠方法的研究不断增加。Reiman & Göhre ([2019](#bib.bib65)) 使用生成对抗网络（GAN）从银河动物园（Lintott
    et al., [2011](#bib.bib49)）中去混叠斯隆数字天空调查（SDSS Alam et al., [2015](#bib.bib4)）的银河系小切片。Arcelin
    et al. ([2021](#bib.bib7)) 使用变分自编码器去混叠模拟的LSST银河系小切片。Hemmati et al. ([2022](#bib.bib34))
    使用GAN去混叠HSC分辨率的图像并恢复哈勃太空望远镜分辨率。在更大尺度上，Bretonnière et al. ([2021](#bib.bib13))
    使用概率U-net模型去混叠大型模拟银河系场景。
- en: In addition to blending, another pressing issue with increased depth is the
    presence of many unresolved galaxies in the deep samples of smaller and fainter
    objects. This will prove difficult for star-galaxy classification schemes that
    rely on morphological features to distinguish between a point source star or a
    point source galaxy, although machine learning methods have been employed to combat
    this problem (Tachibana & Miller, [2018](#bib.bib70); Miller & Hall, [2021](#bib.bib57)).
    Muyskens et al. ([2022](#bib.bib60)) use a Gaussian process classifier to perform
    star/galaxy classification on HSC images. This is an important area of study,
    as misclassifications can introduce biases in studies that require careful measurement
    of galaxy properties. For instance, it has been shown that stellar contamination
    can be a significant source of bias in galaxy clustering measurements (Ross et al.,
    [2011](#bib.bib66)). Precise constraints of cosmological models require a correction
    of this systematic bias in measurements of clustering at high photometric redshifts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了混叠问题，深度样本中许多未解决的银河系问题也是一个紧迫问题，尤其是对于较小和较暗的物体。这将对依赖形态特征来区分点源星星和点源银河系的星银河分类方案构成挑战，尽管机器学习方法已被用于应对这一问题（Tachibana
    & Miller, [2018](#bib.bib70); Miller & Hall, [2021](#bib.bib57)）。Muyskens et al.
    ([2022](#bib.bib60)) 使用高斯过程分类器对HSC图像进行星星/银河系分类。这是一个重要的研究领域，因为错误分类可能会在需要仔细测量银河系特性的研究中引入偏差。例如，已显示恒星污染可能是银河系聚类测量中的一个重要偏差来源（Ross
    et al., [2011](#bib.bib66)）。精确的宇宙学模型约束需要在高光度红移的聚类测量中纠正这种系统性偏差。
- en: The broader field of computer vision has seen a large growth in object detection,
    classification, and semantic segmentation models. Object detection and classification
    consist of identifying the presence of an object in an image and categorizing
    it from a list of possible classes. Semantic segmentation involves identifying
    the portion of an image which belongs to a specific class, i.e. deblending. Put
    together, these tasks amount to instance segmentation. This pixel-level masking
    can be used to deblend objects by selecting the pixels associated with each individual
    object by class. The benchmark leader in deep learning instance segmentation models
    has been the Mask-RCNN framework (He et al., [2017](#bib.bib32)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的更广泛领域在目标检测、分类和语义分割模型方面取得了巨大增长。目标检测和分类包括识别图像中物体的存在并将其从可能的类别列表中进行分类。语义分割涉及识别图像中属于特定类别的部分，即去混叠。综合来看，这些任务构成了实例分割。这种像素级的掩膜可以用于通过选择与每个单独对象类别相关的像素来去混叠对象。深度学习实例分割模型中的基准领导者一直是
    Mask-RCNN 框架（He 等人，[2017](#bib.bib32)）。
- en: The Mask R-CNN architecture was implemented in Burke et al. ([2019](#bib.bib14))
    to detect, deblend, and classify large scenes of simulated stars and galaxies.
    Other architectures have been tested in astronomical contexts, including You Only
    Look Once (YOLO Bochkovskiy et al., [2020](#bib.bib10)). He et al. ([2021](#bib.bib33))
    use a combination of the instance segmentation model YOLOv4 and a separate classification
    network to perform source detection and classification on SDSS images, and González
    et al. ([2018](#bib.bib28)) use a YOLO model to detect and morphologically classify
    SDSS galaxies. However, these models do not perform segmentation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN 架构在 Burke 等人（[2019](#bib.bib14)）的研究中被实现，用于检测、去混叠和分类模拟星星和星系的大场景。在天文学背景下，其他架构也被测试过，包括
    You Only Look Once (YOLO Bochkovskiy 等人，[2020](#bib.bib10))。He 等人（[2021](#bib.bib33)）使用了实例分割模型
    YOLOv4 和一个独立的分类网络来在 SDSS 图像上进行源检测和分类，而 González 等人（[2018](#bib.bib28)）使用 YOLO
    模型来检测和形态学分类 SDSS 星系。然而，这些模型并不执行分割。
- en: The rapid pace of research has led to many new variations and methods that can
    outperform benchmark architectures. To the benefit of computer vision researchers,
    Facebook AI Research (FAIR) has compiled a library of next-gen object detection
    and segmentation models under the framework titled Detectron2 (Wu et al., [2019](#bib.bib72)).
    This modular, fast, and well-documented library makes a fertile testing ground
    for astronomical survey data. In addition to a variety of architectures, pre-trained
    models are also provided. By leveraging transfer learning, i.e., the transfer
    of a neural network’s knowledge from one domain to another, we can cut back on
    training time and costs with these pre-trained models. It is also possible to
    interface new models with detectron2, e.g., Li et al. ([2022](#bib.bib46)); Cheng
    et al. ([2022](#bib.bib18)), taking advantage of its modular nature and flexibility¹¹1See
    https://github.com/facebookresearch/detectron2/tree/main/projects for a comprehensive
    list of projects..
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 研究的快速进展导致了许多新变体和方法，这些方法可以超越基准架构。为了有利于计算机视觉研究人员，Facebook AI Research (FAIR) 编制了一个下一代目标检测和分割模型的库，框架名为
    Detectron2（Wu 等人，[2019](#bib.bib72)）。这个模块化、快速且文档齐全的库为天文调查数据提供了肥沃的测试场地。除了各种架构，库中还提供了预训练模型。通过利用迁移学习，即从一个领域到另一个领域的神经网络知识转移，我们可以减少这些预训练模型的训练时间和成本。此外，也可以将新模型与
    Detectron2 接口，例如 Li 等人（[2022](#bib.bib46)）；Cheng 等人（[2022](#bib.bib18)），利用其模块化特性和灵活性¹¹1查看
    https://github.com/facebookresearch/detectron2/tree/main/projects 获取项目的全面列表..
- en: In this work, we leverage the resources of the detectron2 library by testing
    state-of-the-art instance segmentation models on large scenes, each containing
    hundreds of objects. We perform object detection, segmentation, and classification
    simultaneously on large multi-band HSC coadds. Many deep learning applications
    have been tested on simulated images, but methods applied to real data are often
    limited by a lack of ground truth. Here, we construct a methodology for using
    instance segmentation models on real astronomical data, and demonstrate the potential
    and challenges of this framework when applied to deep images. The HSC data is
    ideal for testing this framework, as it represents the state-of-the-art among
    wide/deep surveys, and is closest in quality to upcoming LSST data. By interfacing
    with detectron2, we are able to test new models as the repository is updated.
    We compare models with different performance metrics, and test how robust they
    are to contrast scalings that alter the dynamic range of the data, which will
    be important to consider for application to other datasets.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们利用了detectron2库的资源，通过在包含数百个物体的大场景上测试最新的实例分割模型。我们在大规模多波段HSC拼接图像上同时执行物体检测、分割和分类。许多深度学习应用已在模拟图像上进行测试，但应用于真实数据的方法通常受限于缺乏真实标注。这里，我们构建了一种在真实天文数据上使用实例分割模型的方法，并展示了该框架在应用于深度图像时的潜力和挑战。HSC数据非常适合测试这一框架，因为它代表了广域/深度调查中的最新技术，并且质量最接近即将发布的LSST数据。通过与detectron2接口，我们能够在库更新时测试新模型。我们比较了不同性能指标的模型，并测试它们在对比度缩放影响数据动态范围的情况下的稳健性，这在应用于其他数据集时将需要考虑。
- en: The major contributions of this work can be summarized as 1) Using instance
    segmentation models to deblend and classify objects in real images from HSC. This
    demonstrates the feasibility for future integration with wide/deep survey pipelines.
    We will show that the models can learn inherent features in the data that lead
    to classification performance gains above traditional morphological methods. 2)
    Comparing the performances of different models when the input data undergoes different
    contrast scalings. There is no standard method for scaling image data in astronomical
    studies that use deep neural networks, so we apply a variety of pre-processing
    scalings to the data for each model. Dynamic ranges can vary significantly across
    datasets, and raw data may not be ideal for feature extraction. We test sensitivity
    to contrast scalings to identify models that will be more easily adapted to different
    datasets. 3) Interfacing our pipeline with the detectron2 framework to test state-of-the-art
    models. Of particular note are our tests using transformer-based architectures,
    an emerging framework in computer vision studies. We will show that these architectures
    are more robust and accurate than traditional convolutional neural networks in
    both deblending and classifying objects in large scenes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的主要贡献可以总结为：1) 使用实例分割模型对HSC的真实图像中的物体进行去混合和分类。这表明了未来与广域/深度调查管道集成的可行性。我们将展示这些模型能够学习数据中的固有特征，从而在分类性能上超越传统的形态学方法。2)
    比较不同模型在输入数据经过不同对比度缩放时的表现。天文研究中使用深度神经网络的图像数据缩放没有标准方法，因此我们对每个模型的数据应用了各种预处理缩放。动态范围在数据集中可能有显著差异，原始数据可能不适合特征提取。我们测试了对比度缩放的敏感性，以识别那些更容易适应不同数据集的模型。3)
    将我们的管道与detectron2框架接口，以测试最新的模型。特别值得注意的是我们使用基于变换器的架构进行的测试，这是计算机视觉研究中的新兴框架。我们将展示这些架构在大场景中的去混合和分类物体方面比传统的卷积神经网络更强大和准确。
- en: 'This paper is organized as follows. In §[2](#S2 "2 Detectron2 Framework ‣ Detection,
    Instance Segmentation, and Classification for Astronomical Surveys with Deep Learning
    (DeepDISC): Detectron2 Implementation and Demonstration with Hyper Suprime-Cam
    Data"), we present an overview of detectron2 in which we highlight the flexibility
    of its modular nature and describe the portion of the available deep learning
    models we implemented. In §[3](#S3 "3 Implementation ‣ Detection, Instance Segmentation,
    and Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data"), we describe the
    curation of our datasets, production of ground truth labels, data preparation
    and our training procedure. In §[4](#S4 "4 HSC Results ‣ Detection, Instance Segmentation,
    and Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data") we present the
    results of training our suite of models and assess performance with different
    metrics. §[5](#S5 "5 Discussion ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data"), we discuss the differences in
    model capabilities, compare the performance of our pipeline to existing results,
    and discuss the benefits and drawbacks of our method. In §[6](#S6 "6 Conclusions
    ‣ Detection, Instance Segmentation, and Classification for Astronomical Surveys
    with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration with
    Hyper Suprime-Cam Data"), we contextualize our findings and conclude.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '本文组织如下。在 §[2](#S2 "2 Detectron2 Framework ‣ Detection, Instance Segmentation,
    and Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data")，我们展示了 detectron2
    的概述，其中突出其模块化特性的灵活性，并描述了我们实现的部分深度学习模型。在 §[3](#S3 "3 Implementation ‣ Detection,
    Instance Segmentation, and Classification for Astronomical Surveys with Deep Learning
    (DeepDISC): Detectron2 Implementation and Demonstration with Hyper Suprime-Cam
    Data")，我们描述了数据集的整理、地面真值标签的生成、数据准备和我们的训练过程。在 §[4](#S4 "4 HSC Results ‣ Detection,
    Instance Segmentation, and Classification for Astronomical Surveys with Deep Learning
    (DeepDISC): Detectron2 Implementation and Demonstration with Hyper Suprime-Cam
    Data") 中，我们展示了训练模型集的结果，并通过不同的指标评估性能。在 §[5](#S5 "5 Discussion ‣ Detection, Instance
    Segmentation, and Classification for Astronomical Surveys with Deep Learning (DeepDISC):
    Detectron2 Implementation and Demonstration with Hyper Suprime-Cam Data")，我们讨论了模型能力的差异，将我们的方法与现有结果进行比较，并讨论了我们方法的优缺点。在
    §[6](#S6 "6 Conclusions ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data")，我们将我们的发现置于背景中并得出结论。'
- en: 2 Detectron2 Framework
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 Detectron2 框架
- en: 'We leverage the modular power of detectron2 by implementing models with varying
    architectures. The pre-trained models we test in Detectron2’s Model Zoo have a
    structure that follows the GeneralizedRCNN meta-architecture provided by the codebase.
    This architecture is a flexible overarching structure that allows for a variety
    of changes, provided they support the following components: (1) a per-image feature
    extraction backbone, (2) region-proposal generation, (3) per-region feature extraction/prediction.
    The schematic of this meta-architecture is shown in Figure [1](#S2.F1 "Figure
    1 ‣ 2 Detectron2 Framework ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过实现具有不同架构的模型来利用 Detectron2 的模块化能力。我们在 Detectron2 的模型库中测试的预训练模型遵循代码库提供的 GeneralizedRCNN
    元架构。这个架构是一个灵活的总体结构，允许各种更改，只要这些更改支持以下组件：（1）每图像特征提取骨干网络，（2）区域提议生成，（3）每区域特征提取/预测。该元架构的示意图见图
    [1](#S2.F1 "Figure 1 ‣ 2 Detectron2 Framework ‣ Detection, Instance Segmentation,
    and Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data")。'
- en: '![Refer to caption](img/3f80b405eed3f6388a0e463cfce15f04.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3f80b405eed3f6388a0e463cfce15f04.png)'
- en: 'Figure 1: Generalized RCNN meta-architecture. A multi-channel image along with
    ground truth object annotations is fed to the backbone feature extractor. These
    features are passed to the RPN and ROI heads to predict object locations and annotations.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：广义 RCNN 元架构。将多通道图像以及地面真值对象注释输入骨干特征提取器。这些特征被传递给 RPN 和 ROI 头，以预测对象位置和注释。
- en: 'The feature extraction backbone takes an input image and outputs “feature maps”
    by running the input through a neural network, often composed of convolutional
    layers. In our tests, we use ResNet backbones and transformer-based backbones.
    ResNets are convolutional neural networks that utilize skip connections that allow
    for deep architectures with many layers without suffering from the degrading accuracy
    problem known to plague deep neural networks (He et al., [2016](#bib.bib31)).
    In this paper we explore a few different ResNet backbones: ResNet50, ResNet101
    and ResNeXt. A ResNet50 network consists of 50 total layers, with two at the head
    or ”stem” of the network and then four stages consisting of 3, 4, 6 and 3 convolutional
    layers, respectively. Each stage includes a skip connection. A ResNet101 network
    is similar to a ResNet50 setup, but with each stage consisting of 3, 4, 23 and
    3 convolutional layers, respectively. Subsequent layers undergo a pooling operation
    that reduces the input resolution. We refer the reader to He et al. ([2016](#bib.bib31))
    for details regarding these layers. ResNeXt layers work similar to ResNet layers,
    but include grouped convolutions which add an extra parallel set of transforms
    (Xie et al., [2017](#bib.bib73)). We also test a network with deformable convolutions,
    in which the regularly spaced convolutional kernel is deformed by a pixel-wise
    offset that is learned by the network (Dai et al., [2017](#bib.bib19)).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取主干接收输入图像，并通过神经网络（通常由卷积层组成）输出“特征图”。在我们的测试中，我们使用 ResNet 主干和基于 Transformer
    的主干。ResNet 是利用跳跃连接的卷积神经网络，这允许拥有许多层的深层架构，而不会遭遇困扰深层神经网络的准确性下降问题（He et al., [2016](#bib.bib31)）。在本文中，我们探讨了几种不同的
    ResNet 主干：ResNet50、ResNet101 和 ResNeXt。ResNet50 网络由总共 50 层组成，其中有两层位于网络的头部或“茎部”，然后四个阶段分别包含
    3、4、6 和 3 层卷积层。每个阶段都包括一个跳跃连接。ResNet101 网络类似于 ResNet50 设置，但每个阶段分别包含 3、4、23 和 3
    层卷积层。后续层经过池化操作，以减少输入分辨率。有关这些层的详细信息，请参阅 He et al. ([2016](#bib.bib31))。ResNeXt
    层与 ResNet 层类似，但包含分组卷积，增加了一组额外的并行变换（Xie et al., [2017](#bib.bib73)）。我们还测试了一种具有可变形卷积的网络，其中规则间隔的卷积核通过网络学习到的逐像素偏移进行变形（Dai
    et al., [2017](#bib.bib19)）。
- en: The stages of a ResNet backbone produce feature maps, representing higher level
    image aspects such as edges and corners. While one can simply take the feature
    map outputted by the last layer of the backbone, this can pose a challenge in
    detecting objects of different scales. This motivates the extraction of features
    at different backbones stages (and thus scale sizes). A hierarchical feature extractor
    known as a feature pyramid network (FPN Lin et al., [2017](#bib.bib48)) has seen
    great success in object detection benchmarks. The FPN allows each feature map
    extracted by a ResNet stage to share information with other feature maps of different
    scales before ultimately passing on to the Region Proposal Network (RPN).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 主干的各个阶段生成特征图，代表图像的高层次特征，如边缘和角落。虽然可以直接使用主干最后一层输出的特征图，但这在检测不同尺度的物体时可能会带来挑战。这促使我们在不同的主干阶段（从而不同的尺度）提取特征。一个被称为特征金字塔网络（FPN
    Lin et al., [2017](#bib.bib48)）的层次特征提取器在物体检测基准测试中取得了巨大成功。FPN 允许由 ResNet 阶段提取的每个特征图与其他不同尺度的特征图共享信息，然后最终传递给区域提议网络（RPN）。
- en: After the image features have been extracted, the next stage of Generalized-RCNN
    networks involves region proposal. This stage involves placing bounding boxes
    at points in the feature maps and sampling from the proposed boxes to curate a
    selection of possible objects. After this sampling has been done, bounding boxes
    are once again proposed and sent to the Region of Interest (ROI) heads, where
    they are compared to the ground truth annotations. The annotations consist of
    bounding box coordinates, segmentation masks, and other information such as class
    labels. Ultimately, many tasks can be done on the objects inside these regions
    of interest, including classification, and with the advent of Mask-RCNN frameworks,
    semantic segmentation. We do not include the details of the RPN and ROI heads,
    as these structures largely remain the same in our tests. We do test architectures
    with a cascade structure (Cai & Vasconcelos, [2018](#bib.bib15)) which involves
    iterating the RPN at successively higher detection thresholds to produce better
    guesses for object locations. For specifics, we refer the reader to Girshick ([2015](#bib.bib27)),
    He et al. ([2017](#bib.bib32)) and the detectron2 codebase.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像特征提取完成后，广义-RCNN网络的下一阶段涉及区域提议。此阶段包括在特征图上的点处放置边界框，并从提议的框中进行采样，以策划可能的对象选择。在完成此采样后，边界框将再次被提议并发送到感兴趣区域（ROI）头部，在那里与真实标注进行比较。标注包括边界框坐标、分割掩膜以及其他信息，如类别标签。最终，可以对这些感兴趣区域中的对象执行许多任务，包括分类，并且随着Mask-RCNN框架的出现，还包括语义分割。我们不包括RPN和ROI头部的详细信息，因为这些结构在我们的测试中大致保持不变。我们确实测试了具有级联结构的架构（Cai
    & Vasconcelos, [2018](#bib.bib15)），该结构涉及在逐渐更高的检测阈值下迭代RPN，以产生对对象位置的更好猜测。有关详细信息，我们参考Girshick（[2015](#bib.bib27)）、He等人（[2017](#bib.bib32)）以及detectron2代码库。
- en: We train a suite of networks to allow for several comparisons. We use a shorthand
    to denote network configurations as follows.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了一套网络以便进行多方面的比较。我们使用简写来表示网络配置，如下所示。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'R101c4: A ResNet50 backbone that uses features from the last residual stage'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: R101c4：一个使用来自最后一个残差阶段特征的ResNet50骨干网络
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'R101fpn: A ResNet101 backbone that uses a FPN'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: R101fpn：一个使用FPN的ResNet101骨干网络
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'R101dc5: A ResNet101 backbone that uses a FPN with the stride of the last block
    layer reduced by a factor of two and the dilation increased by a factor of two'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: R101dc5：一个使用FPN的ResNet101骨干网络，其中最后一个块层的步幅减少了两倍，膨胀系数增加了两倍
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'R50def: A ResNet50 backbone that uses a FPN and deformable convolutions'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: R50def：一个使用FPN和可变形卷积的ResNet50骨干网络
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'R50cas: A ResNet50 backbone that uses a cascaded FPN'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: R50cas：一个使用级联FPN的ResNet50骨干网络
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'X101fpn: A ResNeXt101 backbone that uses a FPN'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: X101fpn：一个使用FPN的ResNeXt101骨干网络
- en: In addition to these ResNet based models, we also test transformer based architectures.
    A transformer is a encoder-decoder model that employs self-attention. Briefly,
    self-attention consists of applying linear operations to an encoded sequence to
    produce intermediate “query, key and value” tensors. A further series of linear
    operations and scalings are done to these intermediate tensors to produce an output
    sequence, and then a final linear operation is performed on the entire output
    sequence. Transformer models have exploded in popularity in the domain of natural
    language processing due to their scalability and generalizability on sequences,
    which translates well to language structure. Recently, transformers have been
    used in computer vision tasks such as image classification and object detection.
    These models been shown to be competitive with the dominant convolutional neural
    networks, and are seeing rapid advances in performance measures (Dosovitskiy et al.,
    [2020](#bib.bib24); Caron et al., [2021](#bib.bib16); Oquab et al., [2023](#bib.bib61);
    Liu et al., [2021](#bib.bib50); Li et al., [2022](#bib.bib46)). For example, MViTv2
    utilizes multi-head pooling attention (MHPA Fan et al., [2021](#bib.bib25)) to
    apply self-attention at different image scales, allowing for the detection of
    features of varying sizes. To obtain the input encoded sequences, an image is
    first divided into patches which are flattened and sent through a linear layer.
    MHPA is applied to the sequences to produce the image features. In an object detection
    context, these features are input to an FPN in the same way as features obtained
    from a ResNet in RCNN models. Another modern transformer model, the Swin Transformer
    (Liu et al., [2021](#bib.bib50)), applies multi-head attention to image patches,
    but rather than a pooling operation, use patch merging to combine features of
    different image patches. Swin models also use shifted window attention to allow
    for efficient computation and information propagation across the image. We test
    both MViTv2 and Swin backbones in our implementation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基于 ResNet 的模型，我们还测试了基于变换器的架构。变换器是一个采用自注意力机制的编码器-解码器模型。简而言之，自注意力机制包括对编码序列应用线性操作，以生成中间的“查询、键和值”张量。对这些中间张量进行一系列线性操作和缩放，以生成输出序列，然后对整个输出序列进行最终的线性操作。由于其在序列上的可扩展性和通用性，变换器模型在自然语言处理领域迅速流行，这与语言结构的契合度很好。近年来，变换器已被用于计算机视觉任务，如图像分类和目标检测。这些模型已被证明与主流卷积神经网络竞争，并在性能指标上迅速取得进展（Dosovitskiy
    等，[2020](#bib.bib24)；Caron 等，[2021](#bib.bib16)；Oquab 等，[2023](#bib.bib61)；Liu
    等，[2021](#bib.bib50)；Li 等，[2022](#bib.bib46)）。例如，MViTv2 使用多头池化注意力（MHPA Fan 等，[2021](#bib.bib25)）在不同的图像尺度上应用自注意力，从而允许检测不同大小的特征。为了获得输入的编码序列，首先将图像划分为多个块，这些块被展平并通过一个线性层。将
    MHPA 应用于这些序列以生成图像特征。在目标检测的背景下，这些特征以与从 RCNN 模型中的 ResNet 获得的特征相同的方式输入到 FPN 中。另一种现代变换器模型
    Swin Transformer（Liu 等，[2021](#bib.bib50)）对图像块应用多头注意力，但采用了块合并而不是池化操作来结合不同图像块的特征。Swin
    模型还使用了移动窗口注意力，以实现高效的计算和信息传播。我们在实现中测试了 MViTv2 和 Swin 背景模型。
- en: 3 Implementation
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实现
- en: 3.1 HSC coadds
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 HSC 合成
- en: 'In this work, the data we use consist of multi-band image coadds of roughly
    4000 pixels² from the Deep and Ultra-Deep fields of the Hyper Suprime Cam (HSC)
    Subaru Strategic Program (SSP; Aihara et al., [2018b](#bib.bib2)) Data Release
    3 (Aihara et al., [2022](#bib.bib3)). The HSC SSP is a three-tiered imaging survey
    using the wide-field imaging camera HSC. The HSC instrument (Miyazaki et al.,
    [2017](#bib.bib58)) consists of a 1.77 deg² camera with a pixel scale of 0.168”,
    attached to the prime focus of the Subaru 8.2 m telescope in Mauna Kea. The Deep+UltraDeep
    component of the HSC survey covers $\sim$36 deg² of the sky in five broad optical
    bands ($grizy$; (Kawanomoto et al., [2018](#bib.bib40))) up to a full 5$\sigma$
    depth of $\sim$27 mag (depending on the filter). Despite limitations (e.g., sky
    subtraction and crowded field issues), the HSC DR3 data provides the closest match
    among all currently available deep-wide surveys to the expected data quality of
    LSST wide fields. The Deep/Ultra-Deep field properties are listed in Table [1](#S3.T1
    "Table 1 ‣ 3.1 HSC coadds ‣ 3 Implementation ‣ Detection, Instance Segmentation,
    and Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data"). We use the g,
    r and i bands.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们使用的数据包括来自 Hyper Suprime Cam (HSC) Subaru Strategic Program (SSP; Aihara
    et al., [2018b](#bib.bib2)) 数据发布 3 (Aihara et al., [2022](#bib.bib3)) 的大约 4000
    像素² 的多波段图像合成。HSC SSP 是一个三级成像调查，使用广视场成像相机 HSC。HSC 仪器 (Miyazaki et al., [2017](#bib.bib58))
    由一个 1.77 度² 的相机组成，像素尺度为 0.168”，附加在位于 Mauna Kea 的 Subaru 8.2 m 望远镜的主焦点上。HSC 调查的
    Deep+UltraDeep 部分覆盖了$\sim$36 度² 的天空，涵盖五个宽光谱带 ($grizy$; (Kawanomoto et al., [2018](#bib.bib40)))，达到约
    27 mag 的完整 5$\sigma$ 深度（取决于滤镜）。尽管存在一些限制（例如，天空减法和拥挤的场景问题），HSC DR3 数据在所有当前可用的深宽调查中提供了最接近
    LSST 宽场期望数据质量的匹配。Deep/Ultra-Deep 场的属性列在表 [1](#S3.T1 "Table 1 ‣ 3.1 HSC coadds
    ‣ 3 Implementation ‣ Detection, Instance Segmentation, and Classification for
    Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data") 中。我们使用 g、r 和 i 波段。'
- en: '|  | median exposure (min) | seeing (“) | depth (mag) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | 中位曝光（分钟） | 视见（”） | 深度（mag） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| g | 70 | 0.83 | 27.4 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| g | 70 | 0.83 | 27.4 |'
- en: '| r | 66 | 0.77 | 27.1 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| r | 66 | 0.77 | 27.1 |'
- en: '| i | 98 | 0.66 | 26.9 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| i | 98 | 0.66 | 26.9 |'
- en: 'Table 1: Properties of the HSC Deep/UltraDeep images'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: HSC Deep/UltraDeep 图像的属性'
- en: 'Given the large depth of the survey, a significant portion of objects are blended
    in comparison to other ground-based surveys such as the Dark Energy Survey (Dark
    Energy Survey Collaboration et al., [2016](#bib.bib20)). For reference, 58% of
    objects in the the shallowest field (Wide) of the HSC survey are blended. While
    a significant challenge, this lends the HSC fields to be an excellent set of data
    for testing deblending algorithms, particularly those suited for crowded fields.
    The pipeline to produce the image coadds is described in detail in Bosch et al.
    ([2018](#bib.bib11)). There are two sets of sky-subtracted coadds. The first set
    consists of global sky-subtracted coadds. The second set also uses the global
    sky-subtracted images, but an additional local sky subtraction algorithm is applied.
    This is to remove the wings of bright objects, artifacts that can cause problems
    in object detection algorithms. However, this process creates a trade-off with
    removing flux from extended objects, and Aihara et al. ([2018a](#bib.bib1)) empirically
    find a local sky subtraction scale of 21.5 arcseconds to be a good balance. Ultimately,
    we use these local sky-subtracted images, as bright wings and artifacts can introduce
    problems of over-deblending or “shredding” and we want our “ground truth” detections
    to be as clean and accurate as possible. To further ensure a clean training set,
    we apply a few quality cuts to the sample. Some images suffer from missing data
    in one or more bands, especially at the edge of the imaging fields. We use the
    bitmasks provided in the coadd FITS files to exclude images with >30% of the pixels
    assigned a NO$\_$DATA flag. Given that the neural network takes multi-band images,
    if one of the g, r or i band images is flagged in this way, we exclude the other
    bands as well. There remain some imaging artifacts and issues, such as saturated
    regions around bright stars, and we discuss how these affect network performance
    in Section [4.2](#S4.SS2 "4.2 Missing and Extra Label Bias Mitigation ‣ 4 HSC
    Results ‣ Detection, Instance Segmentation, and Classification for Astronomical
    Surveys with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration
    with Hyper Suprime-Cam Data").'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于调查的深度较大，与其他地面调查如暗能量调查（Dark Energy Survey Collaboration 等，[2016](#bib.bib20)）相比，许多对象被混合在一起。作为参考，HSC调查中最浅的领域（Wide）中有58%的对象被混合。虽然这是一个重大挑战，但这使得HSC领域成为测试去混合算法的优秀数据集，特别是适用于拥挤领域的算法。生成图像合成的管道在Bosch等人（[2018](#bib.bib11)）中有详细描述。有两组天空减除的合成图像。第一组包括全球天空减除的合成图像。第二组也使用全球天空减除图像，但应用了额外的局部天空减除算法。这是为了去除明亮对象的边缘，这些伪影可能会在对象检测算法中造成问题。然而，这一过程也会在去除扩展对象的光通量方面造成折衷，Aihara等人（[2018a](#bib.bib1)）经验发现局部天空减除尺度为21.5角秒是一个良好的平衡。最终，我们使用这些局部天空减除图像，因为明亮的边缘和伪影可能会引入过度去混合或“撕裂”的问题，我们希望我们的“基准真相”检测尽可能干净和准确。为了进一步确保训练集的清洁，我们对样本应用了一些质量筛选。一些图像在一个或多个波段上缺少数据，尤其是在成像领域的边缘。我们使用合成FITS文件中提供的位掩码来排除像素标记为NO$\_$DATA标志的图像。如果g、r或i波段中的任何一个图像被标记为这种方式，我们也排除其他波段。仍然存在一些成像伪影和问题，如明亮恒星周围的饱和区域，我们将在第[4.2](#S4.SS2
    "4.2 缺失和额外标签偏差缓解 ‣ 4 HSC结果 ‣ 使用深度学习（DeepDISC）进行天文调查的检测、实例分割和分类：Detectron2实施和Hyper
    Suprime-Cam数据演示")节中讨论这些如何影响网络性能。
- en: 3.2 Ground Truth Generation
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基准真相生成
- en: 'We must provide ground-truth object locations and masks to the network to perform
    pixel-level segmentation. We utilize the multi-band deblending code scarlet (Melchior
    et al., [2018](#bib.bib55)) to produce a model for each individual source from
    which we create an object mask. scarlet utilizes constrained matrix factorization
    to produce a spectral decomposition of an object. It is a non-parametric model
    that has been demonstrated to work well on individual galaxies and blended scenes.
    Before we run scarlet, we extract an object catalog using sep, the python wrapper
    for SExtractor. Then, each identified source is modelled and the “blend” or composition
    of sources is fit to the coadd image data. Once the final blend model is computed,
    the mask is determined by running sep on each individual model source and setting
    a mask threshold of 5$\sigma$ above the background. Both the scarlet modelling
    and mask thresholding are done on the detection image, i.e., the sum over all
    bands. The run time of this process increases with the number objects in an image.
    In order to reduce run-time, we divide the 4k stitched coadd images into 16 images
    of $\sim$1000$\times$1000 pixels². While scarlet on its own is a powerful deblender,
    the fits can take up to $\sim$30 minutes depending on the number of objects in
    the image, which motivates the use of efficient neural networks. After this process
    is complete, we compile a training set of 1000 1k$\times$1k pixels² images. The
    distribution of the number of sources per image is shown in Figure [2](#S3.F2
    "Figure 2 ‣ 3.2 Ground Truth Generation ‣ 3 Implementation ‣ Detection, Instance
    Segmentation, and Classification for Astronomical Surveys with Deep Learning (DeepDISC):
    Detectron2 Implementation and Demonstration with Hyper Suprime-Cam Data").'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须提供真实的物体位置和掩膜给网络，以执行像素级分割。我们利用多波段去混叠代码scarlet（Melchior et al., [2018](#bib.bib55)）为每个单独的源生成模型，并从中创建物体掩膜。scarlet利用受限矩阵分解生成物体的光谱分解。这是一个非参数模型，已被证明在单独的星系和混合场景上效果良好。在运行scarlet之前，我们使用sep提取物体目录，sep是SExtractor的Python封装器。然后，每个识别出的源都被建模，“混合”或源的组合被拟合到合成图像数据中。一旦最终的混合模型计算完成，掩膜通过在每个单独的模型源上运行sep并设置5$\sigma$的掩膜阈值来确定。scarlet建模和掩膜阈值处理都是在检测图像上进行的，即所有波段的总和。这个过程的运行时间随着图像中物体数量的增加而增加。为了减少运行时间，我们将4k拼接的合成图像分成16张大约1000$\times$1000像素²的图像。尽管scarlet本身是一个强大的去混叠工具，但根据图像中的物体数量，拟合可能需要长达30分钟，这促使我们使用高效的神经网络。在这个过程完成后，我们编制了1000张1k$\times$1k像素²图像的训练集。每张图像中的源数量分布如图[2](#S3.F2
    "图2 ‣ 3.2 真实值生成 ‣ 3 实现 ‣ 使用深度学习（DeepDISC）进行天文调查的检测、实例分割和分类（Detectron2 实现和 Hyper
    Suprime-Cam 数据演示")所示。
- en: '![Refer to caption](img/4890a7c599be4f735c389cf4e6f3bb7b.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4890a7c599be4f735c389cf4e6f3bb7b.png)'
- en: 'Figure 2: Histogram of the number of objects detected at >5$\sigma$ above the
    background for HSC images in the training set. The images are taken from both
    the Deep and UltraDeep fields.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：训练集中HSC图像中>5$\sigma$背景的物体数量直方图。这些图像来自Deep和UltraDeep领域。
- en: 'The trade-off in using real over simulated data is that in supervised tasks,
    there is a lack of predetermined labels. For the classification task, we produce
    object labels with a catalog match to the HSC DR3 catalogs. We convert each detected
    source center to RA and DEC coordinates and then run the match_to_catalog_sky
    algorithm in astropy to find objects in the HSC catalog within 1 arcsecond. Then,
    we compare the i-band magnitude of the deblended source to the “cmodel” magnitude
    of the catalog objects and pick the object with the smallest magnitude difference.
    If no objects are within 1 arcsecond or no objects have a magnitude difference
    smaller than 1, we discard the object from our labelled set. Once an object is
    matched, we use the HSC catalog “extendedness value” to determine classes, which
    is based on differences in PSF magnitudes and extended model magnitudes. While
    yielding high accuracy at bright magnitudes, this metric becomes unreliable for
    star classification around a limiting magnitude of 24 mag in the i band (Bosch
    et al., [2018](#bib.bib11)). We additionally discard objects with NaN values in
    the DR3 catalog, as the class is indeterminate. We show an example image and the
    results of our labelling methodology in Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Ground
    Truth Generation ‣ 3 Implementation ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data"), with color-coded classes.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '使用真实数据而非模拟数据的权衡在于，在监督任务中缺乏预先确定的标签。对于分类任务，我们通过与HSC DR3目录的匹配来生成物体标签。我们将每个检测到的源中心转换为RA和DEC坐标，然后在astropy中运行match_to_catalog_sky算法，以找到HSC目录中1角秒内的物体。接着，我们将去混叠源的i波段星等与目录物体的“cmodel”星等进行比较，并选择星等差异最小的物体。如果1角秒内没有物体或没有物体的星等差异小于1，我们将该物体从标记集中丢弃。一旦物体匹配成功，我们使用HSC目录中的“扩展值”来确定类别，该值基于PSF星等和扩展模型星等之间的差异。虽然在亮星等下能获得高精度，但对于i波段接近24等限星等的恒星分类，这个指标变得不可靠（Bosch等，[2018](#bib.bib11)）。我们还会丢弃DR3目录中NaN值的物体，因为其类别不确定。我们在图[3](#S3.F3
    "Figure 3 ‣ 3.2 Ground Truth Generation ‣ 3 Implementation ‣ Detection, Instance
    Segmentation, and Classification for Astronomical Surveys with Deep Learning (DeepDISC):
    Detectron2 Implementation and Demonstration with Hyper Suprime-Cam Data")中展示了一个示例图像及我们的标记方法的结果，并标注了颜色编码的类别。'
- en: '![Refer to caption](img/3feccd4d9b3a2078ce1430c8a6259e87.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/3feccd4d9b3a2078ce1430c8a6259e87.png)'
- en: 'Figure 3: The ground truth masks and bounding boxes on an example image in
    the test set of our HSC Deep/UltraDeep field data. As this set is class-agnostic,
    we use white markings for every object. The image without overlaid masks/boxes
    in shown below for clarity. A Lupton contrast scaling is used in this visualization.
    Galaxies are colored green, and stars are colored red.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：测试集中一个示例图像的真实掩膜和边界框。在这个数据集上，由于类别无关，我们对每个物体使用白色标记。为了清晰起见，下面展示了未叠加掩膜/框的图像。该可视化使用了Lupton对比度缩放。星系被标记为绿色，而恒星被标记为红色。
- en: 3.3 Data Preparation
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据准备
- en: '![Refer to caption](img/30a96ae5129731222d433a76e8fd93b2.png)![Refer to caption](img/a651920ffc6b1ea513aa2c8ef0dfd00d.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/30a96ae5129731222d433a76e8fd93b2.png)![参考图注](img/a651920ffc6b1ea513aa2c8ef0dfd00d.png)'
- en: 'Figure 4: Top row: RGB images in the HSC DR3 dataset with different contrast
    scalings. The scalings are, from left to right: Lupton, Lupton high contrast,
    and z-scale. Bottom: Histograms of pixel values to the corresponding image in
    the top row. Red, green, and blue represent values in the i, r, and g filters,
    respectively.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：顶行：HSC DR3数据集中不同对比度缩放的RGB图像。缩放方式，从左到右为：Lupton、Lupton高对比度和z-scale。底行：对应顶行图像的像素值直方图。红色、绿色和蓝色分别表示i、r和g滤光片中的值。
- en: 'We employ three common methods for scaling the raw data from the coadd FITS
    files to RGB values. These are: a z-scale, a Lupton scale, and a high-contrast
    Lupton scale. The z-scale transformations are commonly employed in computer vision
    tasks and are given by'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用三种常见方法将coadd FITS文件中的原始数据缩放到RGB值。这些方法是：z-scale、Lupton缩放和高对比度Lupton缩放。z-scale变换在计算机视觉任务中常用，其公式为
- en: '|  | $\displaystyle R=A(i-\bar{I})/\sigma_{I}$ |  | (1) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R=A(i-\bar{I})/\sigma_{I}$ |  | (1) |'
- en: '|  | $\displaystyle G=A(r-\bar{I})/\sigma_{I}$ |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G=A(r-\bar{I})/\sigma_{I}$ |  |'
- en: '|  | $\displaystyle B=A(g-\bar{I})/\sigma_{I}$ |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B=A(g-\bar{I})/\sigma_{I}$ |  |'
- en: where $I=(i+r+g)/3$ with a mean $\bar{I}$ and standard deviation $\sigma_{I}$,
    $R$ is pixel values in the red channel (and similarly for the green $G$ and blue
    $B$ channels using the $r$ and $g$ -bands respectively). We set $A=10^{3}$ for
    the training and cast the images to 16-bit integers. In addition to z-scaling,
    we also apply a Lupton scaling from Lupton et al. ([2004](#bib.bib51)). This is
    an asinh scaling with
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I=(i+r+g)/3$，具有均值 $\bar{I}$ 和标准差 $\sigma_{I}$，$R$ 是红色通道的像素值（绿色 $G$ 和蓝色 $B$
    通道分别使用 $r$ 和 $g$ 波段）。我们将 $A=10^{3}$ 用于训练，并将图像转换为 16 位整数。除了 z 缩放，我们还应用了 Lupton
    et al.（[2004](#bib.bib51)）的 Lupton 缩放。这是一种 asinh 缩放方法。
- en: '|  | $\displaystyle R=i(\textrm{asinh}(Q(I-\textrm{minimum})/\textrm{stretch})/Q$
    |  | (2) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R=i(\textrm{asinh}(Q(I-\textrm{minimum})/\textrm{stretch})/Q$
    |  | (2) |'
- en: '|  | $\displaystyle G=r(\textrm{asinh}(Q(I-\textrm{minimum})/\textrm{stretch})/Q$
    |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G=r(\textrm{asinh}(Q(I-\textrm{minimum})/\textrm{stretch})/Q$
    |  |'
- en: '|  | $\displaystyle B=g(\textrm{asinh}(Q(I-\textrm{minimum})/\textrm{stretch})/Q.$
    |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B=g(\textrm{asinh}(Q(I-\textrm{minimum})/\textrm{stretch})/Q.$
    |  |'
- en: 'We use a stretch of 0.5 and $Q=10$ and set the minimum to zero and cast the
    images to unsigned 8-bit integers. Lupton scaling brings out the fainter extended
    parts of galaxies while avoiding saturation in the bright central regions. These
    augmentations preserve the color information of objects to aid in classification.
    Lastly, we also use a high-contrast Lupton scaling, in which image brightness
    and contrast is doubled after applying the Lupton scaling. We test all of these
    scalings for each network architecture. In Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Data
    Preparation ‣ 3 Implementation ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data"), we show an example image and
    a histogram of pixel values in i, r and g bands (corresponding to RGB colors)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用 0.5 的拉伸比例和 $Q=10$，将最小值设置为零，并将图像转换为无符号 8 位整数。Lupton 缩放可以突显星系中较暗的延展部分，同时避免在明亮的中央区域饱和。这些增强保持了物体的颜色信息，以帮助分类。最后，我们还使用了高对比度的
    Lupton 缩放，其中图像的亮度和对比度在应用 Lupton 缩放后翻倍。我们测试了每种网络架构的所有这些缩放。在图 [4](#S3.F4 "Figure
    4 ‣ 3.3 Data Preparation ‣ 3 Implementation ‣ Detection, Instance Segmentation,
    and Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data") 中，我们展示了一个示例图像和
    i、r 和 g 波段的像素值直方图（对应于 RGB 颜色）'
- en: We apply data augmentation to the training and test sets. Data augmentation
    has become a staple of many deep learning methods. It allows the network to “see”
    more information without needing to store extra images in memory. We employ spatial
    augmentations of random flips and 90^∘ rotations. We do not employ blurring or
    noise addition, as the real data we train on is already convolved with a PSF and
    contains noise. For future generalizations of this framework to different datasets,
    then blur/noise augmentations may be useful, but for inference purposes on test
    data taken under the same conditions as the training data, spatial augmentations
    are sufficient. We also employ a random 50% crop on each image during training
    so that the data can fit into GPU memory. We considered applying all contrast
    scalings as a data augmentation, but did not find a significant improvement in
    network performance. However, this could be used in future work to reduce the
    training costs, as results were on par with networks trained with only one contrast
    scaling.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对训练集和测试集应用了数据增强。数据增强已成为许多深度学习方法的基础。它使网络能够“看到”更多的信息，而无需在内存中存储额外的图像。我们采用了随机翻转和
    90^∘ 旋转的空间增强方法。我们没有使用模糊或噪声添加，因为我们训练的数据已经被 PSF 卷积且包含噪声。对于将来将该框架推广到不同数据集的工作，模糊/噪声增强可能会有用，但对于在与训练数据相同条件下获得的测试数据的推理目的，空间增强已经足够。我们还在训练期间对每张图像进行随机
    50% 裁剪，以便数据能够适应 GPU 内存。我们考虑了将所有对比度缩放作为数据增强，但没有发现网络性能有显著提升。然而，这可能在未来的工作中用来减少训练成本，因为结果与只用一个对比度缩放训练的网络表现相当。
- en: 3.4 Training
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 训练
- en: Training is done using stochastic gradient descent to update the network weights
    by minimizing a loss function. The loss functions of these Mask-RCNN models is
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 训练是通过随机梯度下降来更新网络权重，目标是最小化损失函数。这些 Mask-RCNN 模型的损失函数是
- en: '|  | $L=L_{\text{cls}}+L_{\text{box}}+L_{\text{mask}}$ |  | (3) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $L=L_{\text{cls}}+L_{\text{box}}+L_{\text{mask}}$ |  | (3) |'
- en: where the classification loss $L_{\text{cls}}$ is $-\log p_{u}$ or the log of
    the estimated probability of an object belonging to its true class $u$. Discrete
    probability distributions are calculated per class (plus a background class) for
    each ROI. $L_{\text{box}}$ is a smoothed L1 loss calculated over the predicted
    and true bounding box coordinates as given in Girshick ([2015](#bib.bib27)). Finally,
    the mask loss $L_{\text{mask}}$ is the per-pixel average binary cross-entropy
    loss between the ground truth and predicted masks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中分类损失 $L_{\text{cls}}$ 是 $-\log p_{u}$ 或对象属于其真实类别 $u$ 的估计概率的对数。离散概率分布是每个 ROI
    中每个类别（加上一个背景类别）计算的。$L_{\text{box}}$ 是一个平滑的 L1 损失，计算预测的和真实的边界框坐标，如 Girshick ([2015](#bib.bib27))
    所给出。最后，掩码损失 $L_{\text{mask}}$ 是真实掩码和预测掩码之间每像素的平均二元交叉熵损失。
- en: All networks are pre-trained on either the MS-COCO (Lin et al., [2014](#bib.bib47))
    or ImageNet-1k (Deng et al., [2009](#bib.bib22)) datasets of terrestrial images,
    and so we use transfer learning to apply these models to the our astronomical
    datasets. Transfer learning is a technique in deep learning where networks can
    generalize knowledge of one task to complete a different but related task (See
    Tan et al. [2018](#bib.bib71) for an overview of deep transfer learning). It is
    often used when applying a pre-trained deep learning model to a different domain
    than the one seen during training. By using pre-trained weights as initial conditions,
    training is likely to converge faster and be less prone to over-fitting. We use
    weights provided by Detectron2 as the starting point for our training procedure.
    We then train the networks for 50 total epochs, i.e. the entire training set is
    seen 50 total times by the network. In order to facilitate the transfer of knowledge,
    we first freeze the feature extraction backbones of the models and only train
    the head layers in the ROI and RPN networks for 15 epochs. We use a learning rate
    of 0.001 for this step. Then, we unfreeze the feature extraction backbone and
    train the entire network for 35 epochs. We begin this step with a learning rate
    of 0.0001 and decrease by a factor of 10 every 10 epochs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所有网络都是在MS-COCO (Lin et al., [2014](#bib.bib47)) 或 ImageNet-1k (Deng et al.,
    [2009](#bib.bib22)) 数据集上的地球图像上进行预训练的，因此我们使用迁移学习将这些模型应用于我们的天文数据集。迁移学习是一种深度学习技术，其中网络可以将一个任务的知识泛化到完成一个不同但相关的任务（参见
    Tan et al. [2018](#bib.bib71) 以了解深度迁移学习的概述）。当将预训练的深度学习模型应用于与训练期间不同的领域时，它通常会被使用。通过使用预训练的权重作为初始条件，训练可能会更快收敛，并且不易过拟合。我们使用
    Detectron2 提供的权重作为我们训练过程的起点。然后，我们将网络训练 50 个完整的 epoch，即整个训练集被网络看到 50 次。为了促进知识的迁移，我们首先冻结模型的特征提取骨干，仅训练
    ROI 和 RPN 网络中的头部层 15 个 epoch。我们为这一步使用 0.001 的学习率。然后，我们解冻特征提取骨干，并训练整个网络 35 个 epoch。我们以
    0.0001 的学习率开始这一步，并在每 10 个 epoch 下降一个 10 倍的因子。
- en: We use two NVIDIA Tesla V100 GPUs in HAL system (Kindratenko et al., [2020](#bib.bib41))
    to train on 1,000 images of size 500 pixels² paired with object annotations. When
    trained in parallel on each GPU, our models take roughly $\sim$3 hours to complete.
    Transformer architectures tend to use more memory, and thus are trained on 4 GPUs
    for roughly 4 hours.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 HAL 系统 (Kindratenko et al., [2020](#bib.bib41)) 中使用两块 NVIDIA Tesla V100
    GPU 对 1,000 张 500 像素² 的图像及其对象注释进行训练。当在每个 GPU 上并行训练时，我们的模型大约需要 $\sim$3 小时完成。Transformer
    架构通常使用更多的内存，因此在 4 块 GPU 上训练大约需要 4 小时。
- en: 4 HSC Results
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 HSC 结果
- en: After training, we evaluate network performance on the test set of HSC images.
    The test set is taken from the patches in the UltraDeep COSMOS (Scoville et al.,
    [2007](#bib.bib68)) field and consists of 95 images of 1000 pixels². No test set
    images were seen during training. A benefit of the instance segmentation models
    used in this work is their ability to infer on images of variable size. Thus,
    despite the need to crop images during training, we are still able to utilize
    the full size of the images in the test set.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们在 HSC 图像的测试集上评估网络性能。测试集来自 UltraDeep COSMOS (Scoville et al., [2007](#bib.bib68))
    领域中的补丁，并由 95 张 1000 像素² 的图像组成。测试集图像在训练期间没有见过。该工作的实例分割模型的一个优点是它们能够对大小可变的图像进行推断。因此，尽管在训练期间需要裁剪图像，我们仍然能够利用测试集中图像的全部大小。
- en: We evaluate classification performance with precision and recall, given by
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用精度和召回率来评估分类性能，计算公式为
- en: '|  | $p=\frac{\text{TP}}{\text{TP}+\text{FP}},$ |  | (4) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $p=\frac{\text{TP}}{\text{TP}+\text{FP}},$ |  | (4) |'
- en: '|  | $r=\frac{\text{TP}}{\text{TP}+\text{FN}}.$ |  | (5) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $r=\frac{\text{TP}}{\text{TP}+\text{FN}}.$ |  | (5) |'
- en: True positives (TP) are counted as a detection that has a confidence score outputted
    by the network above a certain threshold and additionally can be matched to a
    ground truth object by having an Intersection over Union (IOU) above another threshold.
    False negatives (FN) are those ground truth objects that do not have a corresponding
    detection. False Positives (FP) are those detections with a high confidence score
    but do not have a matching ground truth. The IOU is defined as
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 真实正例（TP）指的是网络输出的置信度分数超过某个阈值的检测，并且可以通过交并比（IOU）匹配到真实对象。假负例（FN）是指没有对应检测的真实对象。假正例（FP）是指具有高置信度分数但没有匹配真实对象的检测。IOU定义为
- en: '|  | $\text{IOU}=\frac{\text{\emph{area}}(\text{box}_{\text{predicted}}\cap\text{box}_{\text{truth}})}{\text{\emph{area}}(\text{box}_{\text{predicted}}\cup\text{box}_{\text{truth}})}.$
    |  | (6) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{IOU}=\frac{\text{\emph{area}}(\text{box}_{\text{predicted}}\cap\text{box}_{\text{truth}})}{\text{\emph{area}}(\text{box}_{\text{predicted}}\cup\text{box}_{\text{truth}})}.$
    |  | (6) |'
- en: or the area of the intersection over the area of the union of the predicted
    and ground truth bounding boxes. Precision and recall are often broken down by
    class, or combined into one value, the AP score,
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 或预测框与真实框的交集面积与并集面积之比。精度和召回率通常按类别分解，或者合并为一个值，即AP分数，
- en: '|  | $\text{AP}=\frac{1}{51}\sum_{r\in\{0,0.02,...,1.0\}}p(r)$ |  | (7) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{AP}=\frac{1}{51}\sum_{r\in\{0,0.02,...,1.0\}}p(r)$ |  | (7) |'
- en: where $p(r)$ is maximum the precision in a recall bin of width $\Delta r$. AP
    scores are computed for IOU thresholds of {0.5,0.55…0.95} and averaged.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: where $p(r)$ is maximum the precision in a recall bin of width $\Delta r$. AP
    scores are computed for IOU thresholds of {0.5,0.55…0.95} and averaged.
- en: '|  | ResNets | Transformers |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | ResNets | Transformers |'
- en: '| --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  |  | R101C4 | R101dc5 | R101fpn | R50cas | R50def | X101fpn | MViTv2 | Swin
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  |  | R101C4 | R101dc5 | R101fpn | R50cas | R50def | X101fpn | MViTv2 | Swin
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Galaxies | Lupton | 23.7 | 24.6 | 40.9 | 46.3 | 41.7 | 41.4 | 51.7 | 50.8
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Galaxies | Lupton | 23.7 | 24.6 | 40.9 | 46.3 | 41.7 | 41.4 | 51.7 | 50.8
    |'
- en: '| LuptonHC | 26.1 | 28.0 | 43.6 | 46.0 | 43.2 | 43.1 | 50.9 | 50.3 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 26.1 | 28.0 | 43.6 | 46.0 | 43.2 | 43.1 | 50.9 | 50.3 |'
- en: '| zscale | 22.9 | 30.7 | 40.2 | 39.6 | 21.8 | 34.1 | 52.7 | 52.5 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 22.9 | 30.7 | 40.2 | 39.6 | 21.8 | 34.1 | 52.7 | 52.5 |'
- en: '| Stars | Lupton | 10.3 | 9.6 | 7.3 | 7.4 | 4.3 | 2.5 | 34.1 | 33.9 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Stars | Lupton | 10.3 | 9.6 | 7.3 | 7.4 | 4.3 | 2.5 | 34.1 | 33.9 |'
- en: '| LuptonHC | 2.4 | 5.1 | 6.1 | 8.1 | 5.5 | 8.3 | 28.0 | 25.0 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 2.4 | 5.1 | 6.1 | 8.1 | 5.5 | 8.3 | 28.0 | 25.0 |'
- en: '| zscale | 15.6 | 10.5 | 17.9 | 25.5 | 12.7 | 17.2 | 35.8 | 33.9 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 15.6 | 10.5 | 17.9 | 25.5 | 12.7 | 17.2 | 35.8 | 33.9 |'
- en: '| Small | Lupton | 17.6 | 18.0 | 26.1 | 28.0 | 24.6 | 23.7 | 43.7 | 43.1 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Small | Lupton | 17.6 | 18.0 | 26.1 | 28.0 | 24.6 | 23.7 | 43.7 | 43.1 |'
- en: '| LuptonHC | 14.8 | 17.2 | 25.9 | 27.7 | 25.4 | 26.9 | 40.1 | 38.4 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 14.8 | 17.2 | 25.9 | 27.7 | 25.4 | 26.9 | 40.1 | 38.4 |'
- en: '| zscale | 19.7 | 21.5 | 30.2 | 33.2 | 18.1 | 26.8 | 44.8 | 43.8 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 19.7 | 21.5 | 30.2 | 33.2 | 18.1 | 26.8 | 44.8 | 43.8 |'
- en: '| Medium | Lupton | 8.7 | 11.9 | 14.4 | 11.5 | 13.7 | 11.7 | 17.4 | 16.1 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Medium | Lupton | 8.7 | 11.9 | 14.4 | 11.5 | 13.7 | 11.7 | 17.4 | 16.1 |'
- en: '| LuptonHC | 7.8 | 11.1 | 13.4 | 12.7 | 10.3 | 12.6 | 16.3 | 15.5 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 7.8 | 11.1 | 13.4 | 12.7 | 10.3 | 12.6 | 16.3 | 15.5 |'
- en: '| zscale | 3.8 | 9.0 | 7.2 | 7.3 | 1.6 | 3.6 | 15.1 | 14.9 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 3.8 | 9.0 | 7.2 | 7.3 | 1.6 | 3.6 | 15.1 | 14.9 |'
- en: '| Large | Lupton | 16.4 | 30.9 | 18.9 | 14.3 | 19.6 | 9.3 | 43.1 | 41.5 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Large | Lupton | 16.4 | 30.9 | 18.9 | 14.3 | 19.6 | 9.3 | 43.1 | 41.5 |'
- en: '| LuptonHC | 15.3 | 22.8 | 14.9 | 15.0 | 11.6 | 13.0 | 38.6 | 39.7 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 15.3 | 22.8 | 14.9 | 15.0 | 11.6 | 13.0 | 38.6 | 39.7 |'
- en: '| zscale | 0.7 | 3.6 | 3.8 | 5.2 | 0.1 | 0.9 | 37.8 | 37.0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 0.7 | 3.6 | 3.8 | 5.2 | 0.1 | 0.9 | 37.8 | 37.0 |'
- en: 'Table 2: AP scores on COSMOS HSC set for all network configurations (larger
    is better). Galaxy and Star AP scores are calculated separately, whereas Small
    (0-32 pixels²), Medium (32-96 pixels²) and Large (>96 pixels²) object AP scores
    are averaged across both classes. The best result for each row is emphasized in
    bold. The MViTv2 backbone gives the best results in all cases except for one.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：所有网络配置在COSMOS HSC数据集上的AP分数（分数越大越好）。银河系和恒星的AP分数是分别计算的，而小物体（0-32像素²）、中等物体（32-96像素²）和大物体（>96像素²）的AP分数则是两个类别的平均值。每行的最佳结果以**粗体**显示。除了一个例外，MViTv2骨干网在所有情况下都提供了最佳结果。
- en: 'AP scores on the HSC COSMOS test set are reported for all network configurations
    in Table [2](#S4.T2 "Table 2 ‣ 4 HSC Results ‣ Detection, Instance Segmentation,
    and Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data"). We report the
    per-class AP score for stars and galaxies separately, as well as the Small, Medium,
    and Large AP scores, defined by the object bounding box size of 0-32 pixels²,
    32-96 pixels² and >96 pixels², respectively. For galaxies and stars, AP score
    can vary significantly across network configurations. For ResNet-based architectures,
    AP for galaxies is consistently higher than stars, which may be due to the higher
    sample size of galaxies and morphological features that make galaxies easier to
    distinguish than compact stars. Among ResNet-based networks, a Lupton high-contrast
    scaling generally gives the highest galaxy AP score, while a z-scaling always
    gives the highest star AP score. It appears that these networks are very sensitive
    to the contrast scaling used, which is not desirable for application to other
    datasets with different dynamic ranges. However, transformer-based architectures
    perform more robustly with varying contrast scalings, and outperform ResNet architectures
    in almost all cases. For these networks, galaxy AP scores all lie within $\sim$50-52,
    showing a gain of about 5 over the highest performing ResNet configuration. Stellar
    AP scores for Lupton and z-scalings lie within $\sim$33-35, with high-contrast
    Lupton scalings performing worse by an AP of $\sim$8. Among the Small, Medium,
    and Large AP metrics, transformer-based networks also outperform ResNet-based
    networks, in some cases seeing massive gains in AP score. The networks generally
    perform better on Small and Large object categories over Medium objects, again
    likely due to sample size.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 'HSC COSMOS测试集上的AP分数在表[2](# S4.T2 "Table 2 ‣ 4 HSC Results ‣ Detection, Instance
    Segmentation, and Classification for Astronomical Surveys with Deep Learning (DeepDISC):
    Detectron2 Implementation and Demonstration with Hyper Suprime-Cam Data")中报告了所有网络配置。我们分别报告了星星和星系的每类AP分数，以及小型、中型和大型AP分数，分别由0-32像素²、32-96像素²和>96像素²的对象边界框大小定义。对于星系和星星，AP分数在不同的网络配置中可能会有显著变化。对于基于ResNet的架构，星系的AP始终高于星星，这可能是由于星系的样本量较大，且形态特征使得星系比紧凑的星星更容易区分。在基于ResNet的网络中，Lupton高对比度缩放通常会给出最高的星系AP分数，而z缩放总是会给出最高的星星AP分数。看起来这些网络对所使用的对比度缩放非常敏感，这对应用于具有不同动态范围的其他数据集并不理想。但是，基于transformer的架构在使用不同对比度缩放时表现更加鲁棒，并在几乎所有情况下都优于ResNet架构。对于这些网络，星系AP分数都在$\sim$50-52之间，比最佳的ResNet配置提高了约5个百分点。Lupton和z缩放的星体AP分数在$\sim$33-35之间，高对比度的Lupton缩放表现更差，AP约为$\sim$8。在小型、中型和大型AP指标中，transformer-based网络在某些情况下也优于ResNet-based网络，在AP分数上取得了巨大的增长。这些网络通常在小型和大型对象类别上表现更好，这很可能是由于样本量造成的。'
- en: 'Many studies of instance segmentation models use the MS-COCO or ImageNet-1k
    datasets as a benchmark to judge performance through the AP score. These data
    consist of terrestrial images with many object classes, so it can not necessarily
    be used as a comparison for our AP scores calculated on astronomical survey images
    with only 2 classes. However, to give a reader a sense of the range of typical
    values, the AP scores for models trained on terrestrial data typically range from
    $\sim$35-45 for convolutional backbones and push to $\sim$55 for transformer backbones
    (see the detectron2 repo for results). For a more fair comparison, we look to
    Burke et al. ([2019](#bib.bib14)) in which instance segmentation models were tested
    on the simulated observations from the Dark Energy Camera (DECam Flaugher et al.,
    [2015](#bib.bib26)). The authors report an AP score for galaxies of 49.6 and score
    of 48.6 for stars, averaged to a combined score of 49.0\. We also train our suite
    of models on the DECam dataset and report the results in Appendix [A](#A1 "Appendix
    A DECam results ‣ Detection, Instance Segmentation, and Classification for Astronomical
    Surveys with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration
    with Hyper Suprime-Cam Data"). More recently, He et al. ([2021](#bib.bib33)) use
    a combination of the instance segmentation model YOLOv4 (Bochkovskiy et al., [2020](#bib.bib10))
    and a separate classification network to perform source detection and classification
    on SDSS images. They report an AP score of 52.81 for their single-class detection
    network.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '许多实例分割模型的研究使用 MS-COCO 或 ImageNet-1k 数据集作为基准，通过 AP 分数评估性能。这些数据集包含了多种物体类别的地面图像，因此不能直接用于与我们在仅有
    2 类的天文调查图像上计算的 AP 分数进行比较。然而，为了让读者了解典型值的范围，训练于地面数据的模型的 AP 分数通常在 $\sim$35-45 之间，对于卷积主干网络，Transformer
    主干网络则推高到 $\sim$55（参见 detectron2 仓库中的结果）。为了更公平的比较，我们参考了 Burke 等人（[2019](#bib.bib14)），他们在
    Dark Energy Camera（DECam Flaugher 等，[2015](#bib.bib26)）的模拟观测数据上测试了实例分割模型。作者报告了一个星系的
    AP 分数为 49.6，星星的分数为 48.6，平均得到一个 49.0 的综合分数。我们也在 DECam 数据集上训练了我们的模型，并在附录 [A](#A1
    "Appendix A DECam results ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data") 中报告了结果。最近，He 等人（[2021](#bib.bib33)）结合了实例分割模型
    YOLOv4（Bochkovskiy 等，[2020](#bib.bib10)）和一个独立的分类网络，在 SDSS 图像上进行源检测和分类。他们报告了单类检测网络的
    AP 分数为 52.81。'
- en: 4.1 Incorrect Label Bias Mitigation
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 错误标签偏差缓解
- en: There is an inherent bias in our measure of AP scores due to incorrect object
    class labels. In measurements described above, we test the network abilities to
    infer classes based on labels generated from HSC catalogs. However, these labels
    are known to become unreliable, especially for stars, around i-band magnitudes
    of $\sim$24 mag (Bosch et al., [2018](#bib.bib11)). We use HSC coadds in the COSMOS
    field for our test dataset, and attempt to mitigate this mislabelling bias by
    exploiting the overlap of this field with space-based observations using the Advanced
    Camera for Surveys (ACS) on the Hubble Space Telescope (HST). Because of the lack
    of atmospheric seeing, morphological classification of stars/galaxies using the
    HST COSMOS catalog data is much more precise for faint objects, and can be used
    as ground truth instead of HSC labels. This will test how much poor classification
    behaviour is due to label generation as opposed to limitations of the models.
    We generate HST labels by cross-matching detected sources to the catalog of Leauthaud
    et al. ([2007](#bib.bib45)) within 1 arcsecond. If there is no object within 1
    arcsecond, we discard the object. There is not necessarily a one-to-one match
    of HSC versus HST labels, as we are cross-matching to different catalogs, but
    the number of objects per image remains roughly the same for either labelling
    scheme. We will refer to this as the HST COSMOS test set.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对象类别标签的不正确，我们的AP分数测量存在固有偏差。在上述测量中，我们测试了网络根据HSC目录生成的标签推断类别的能力。然而，这些标签已知在i带星等约为$\sim$24
    mag时变得不可靠（Bosch et al., [2018](#bib.bib11)）。我们使用COSMOS领域的HSC共加数据集，并尝试通过利用该领域与使用哈勃空间望远镜（HST）上的高级相机（ACS）的空间观测的重叠来减轻这种错误标记偏差。由于缺乏大气视差，使用HST
    COSMOS目录数据对星星/星系的形态分类在微弱对象中要精确得多，可以作为地面真相而不是HSC标签。这将测试较差的分类行为是由于标签生成还是模型的局限性。我们通过将检测到的源与Leauthaud
    et al. ([2007](#bib.bib45)) 的目录在1角秒内交叉匹配来生成HST标签。如果在1角秒内没有对象，我们将丢弃该对象。HSC与HST标签之间不一定存在一对一的匹配，因为我们正在与不同的目录交叉匹配，但每个图像中的对象数量对于任何标记方案基本保持不变。我们将其称为HST
    COSMOS测试集。
- en: 'This small set is not sufficient to train a network, so instead of training
    on HST-labelled data, we take the models trained on HSC-labelled data and test
    their evaluation performance on the HST COSMOS test set. To highlight the differences
    in class label generation, in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Missing and Extra
    Label Bias Mitigation ‣ 4 HSC Results ‣ Detection, Instance Segmentation, and
    Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data") we show the number
    of stars and galaxies as a function of HSC i-band magnitude for the COSMOS set
    for both HSC and HST class labels. The unreliable quality of HSC labels at faint
    magnitudes is reflected in the increased counts of stars, especially the bump
    in stellar counts beginning at i$\sim$25 mag. Also of note is the fewer amounts
    of star counts in the HSC COSMOS set at bright magnitudes. This is likely due
    to our HSC label generating procedure of discarding objects with NaN values in
    the HSC catalog. Bright stars are likely to have saturated pixels in their centers,
    causing these error flags to appear. With HST labels. we can test with a more
    astrophysically accurate baseline.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '这个小数据集不足以训练网络，因此我们不在HST标记的数据上进行训练，而是使用在HSC标记的数据上训练的模型，并在HST COSMOS测试集上测试它们的评估性能。为了突出类别标签生成的差异，在图
    [5](#S4.F5 "Figure 5 ‣ 4.2 Missing and Extra Label Bias Mitigation ‣ 4 HSC Results
    ‣ Detection, Instance Segmentation, and Classification for Astronomical Surveys
    with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration with
    Hyper Suprime-Cam Data") 中，我们展示了COSMOS数据集中HSC和HST类别标签的HSC i带星等函数的星星和星系数量。HSC标签在微弱星等下的不可靠质量体现在星星的计数增加上，尤其是i$\sim$25
    mag开始的星星计数峰值。另一个值得注意的是，在HSC COSMOS数据集中，明亮星等的星星计数较少。这可能是由于我们在HSC目录中生成标签时丢弃了NaN值的对象。明亮的星星很可能在其中心有饱和像素，导致这些错误标志的出现。使用HST标签，我们可以用更天体物理学上准确的基线进行测试。'
- en: 'Using this new test set, we present AP scores in Table [3](#S4.T3 "Table 3
    ‣ 4.1 Incorrect Label Bias Mitigation ‣ 4 HSC Results ‣ Detection, Instance Segmentation,
    and Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data").'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '使用这个新的测试集，我们在表 [3](#S4.T3 "Table 3 ‣ 4.1 Incorrect Label Bias Mitigation ‣
    4 HSC Results ‣ Detection, Instance Segmentation, and Classification for Astronomical
    Surveys with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration
    with Hyper Suprime-Cam Data") 中展示了AP分数。'
- en: '|  | ResNets | Transformers |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | ResNets | Transformers |'
- en: '| --- | --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  |  | R101C4 | R101dc5 | R101fpn | R50cas | R50def | X101fpn | MViTv2 | Swin
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  |  | R101C4 | R101dc5 | R101fpn | R50cas | R50def | X101fpn | MViTv2 | Swin
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Galaxies | Lupton | 25.9 | 26.8 | 42.9 | 49.4 | 43.5 | 42.8 | 51.8 | 52.4
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 银河系 | Lupton | 25.9 | 26.8 | 42.9 | 49.4 | 43.5 | 42.8 | 51.8 | 52.4 |'
- en: '| LuptonHC | 27.4 | 30.0 | 46.2 | 50.2 | 46.7 | 44.3 | 51.5 | 51.6 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 27.4 | 30.0 | 46.2 | 50.2 | 46.7 | 44.3 | 51.5 | 51.6 |'
- en: '| zscale | 25.5 | 32.5 | 42.7 | 41.5 | 23.0 | 35.6 | 52.2 | 52.9 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 25.5 | 32.5 | 42.7 | 41.5 | 23.0 | 35.6 | 52.2 | 52.9 |'
- en: '| Stars | Lupton | 16.2 | 15.0 | 10.9 | 10.9 | 7.1 | 3.8 | 52.9 | 53.7 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 星体 | Lupton | 16.2 | 15.0 | 10.9 | 10.9 | 7.1 | 3.8 | 52.9 | 53.7 |'
- en: '| LuptonHC | 4.2 | 7.9 | 11.2 | 14.2 | 9.4 | 13.9 | 42.1 | 37.7 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 4.2 | 7.9 | 11.2 | 14.2 | 9.4 | 13.9 | 42.1 | 37.7 |'
- en: '| zscale | 28.3 | 19.1 | 29.3 | 41.6 | 23.8 | 29.0 | 53.9 | 52.6 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 28.3 | 19.1 | 29.3 | 41.6 | 23.8 | 29.0 | 53.9 | 52.6 |'
- en: '| Small | Lupton | 22.0 | 22.1 | 29.3 | 31.4 | 27.0 | 25.2 | 54.0 | 54.7 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 小型 | Lupton | 22.0 | 22.1 | 29.3 | 31.4 | 27.0 | 25.2 | 54.0 | 54.7 |'
- en: '| LuptonHC | 16.4 | 19.9 | 30.0 | 33.3 | 29.4 | 30.7 | 48.2 | 46.0 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 16.4 | 19.9 | 30.0 | 33.3 | 29.4 | 30.7 | 48.2 | 46.0 |'
- en: '| zscale | 28.0 | 27.1 | 37.8 | 42.9 | 24.8 | 34.1 | 54.7 | 54.3 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 28.0 | 27.1 | 37.8 | 42.9 | 24.8 | 34.1 | 54.7 | 54.3 |'
- en: '| Medium | Lupton | 8.3 | 11.7 | 13.8 | 11.0 | 13.1 | 11.1 | 16.3 | 15.2 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 中型 | Lupton | 8.3 | 11.7 | 13.8 | 11.0 | 13.1 | 11.1 | 16.3 | 15.2 |'
- en: '| LuptonHC | 7.5 | 10.8 | 12.7 | 12.2 | 9.9 | 12.0 | 15.4 | 14.6 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 7.5 | 10.8 | 12.7 | 12.2 | 9.9 | 12.0 | 15.4 | 14.6 |'
- en: '| zscale | 3.7 | 8.5 | 7.3 | 7.4 | 1.7 | 3.6 | 14.1 | 14.1 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 3.7 | 8.5 | 7.3 | 7.4 | 1.7 | 3.6 | 14.1 | 14.1 |'
- en: '| Large | Lupton | 6.2 | 11.1 | 7.2 | 5.9 | 7.2 | 3.6 | 15.1 | 15.0 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 大型 | Lupton | 6.2 | 11.1 | 7.2 | 5.9 | 7.2 | 3.6 | 15.1 | 15.0 |'
- en: '| LuptonHC | 5.4 | 7.9 | 5.3 | 4.8 | 4.4 | 4.8 | 13.7 | 14.0 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 5.4 | 7.9 | 5.3 | 4.8 | 4.4 | 4.8 | 13.7 | 14.0 |'
- en: '| zscale | 0.3 | 1.2 | 1.3 | 1.9 | 0.1 | 0.2 | 13.6 | 13.5 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 0.3 | 1.2 | 1.3 | 1.9 | 0.1 | 0.2 | 13.6 | 13.5 |'
- en: 'Table 3: Same as Table [2](#S4.T2 "Table 2 ‣ 4 HSC Results ‣ Detection, Instance
    Segmentation, and Classification for Astronomical Surveys with Deep Learning (DeepDISC):
    Detectron2 Implementation and Demonstration with Hyper Suprime-Cam Data"), but
    with the COSMOS HST test set.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 与表 [2](#S4.T2 "表 2 ‣ 4 HSC 结果 ‣ 使用深度学习（DeepDISC）对天文调查的检测、实例分割和分类：Detectron2
    实现和 Hyper Suprime-Cam 数据演示") 相同，但使用了 COSMOS HST 测试集。'
- en: The results for galaxy/star AP scores are in line with the previous results
    on the HSC COSMOS test set. In all cases, transformer architectures outperform
    ResNet architectures and are more robust to different contrast scalings. AP scores
    for Small bounding box objects improves for all network configurations, Medium
    bounding box AP score roughly remains the same, and Large bounding box AP score
    worsens. The decrease in Large bounding-box AP scores is likely due to the initial
    label generation step with sep that over-deblends or “shreds” large extended galaxies
    and saturated regions around stars. With our HSC label generation, we exclude
    many of the shredded regions by enforcing the i-band $\Delta$1 mag criterion and
    discarding labels matched to saturated catalog objects with NaN values. However,
    our HST label generation is solely based on a distance matching criterion, and
    so some of these shredded regions are included in the ground truth labels in the
    HST COSMOS test set. These spurious extra labels can lead to lower AP scores if
    the networks avoid shredding these regions at inference. In the next section,
    we examine metrics other than AP score that are less susceptible to this effect.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于银河系/星体 AP 分数的结果与之前在 HSC COSMOS 测试集上的结果一致。在所有情况下，Transformer 架构的表现优于 ResNet
    架构，并且对不同的对比度缩放具有更好的鲁棒性。小型边界框对象的 AP 分数在所有网络配置中都有所改善，中型边界框的 AP 分数大致保持不变，而大型边界框的
    AP 分数则变差。大型边界框 AP 分数的下降可能是由于初始标签生成步骤中使用的 sep 过度融合或“撕裂”了大型扩展的银河系和星体周围的饱和区域。通过我们的
    HSC 标签生成，我们通过强制 i 带 $\Delta$1 mag 标准并丢弃与饱和目录对象（NaN 值）匹配的标签来排除许多被撕裂的区域。然而，我们的 HST
    标签生成完全基于距离匹配标准，因此这些撕裂的区域在 HST COSMOS 测试集中被包含在实际标签中。如果网络在推断时避免撕裂这些区域，这些虚假的额外标签可能会导致更低的
    AP 分数。在下一节中，我们将考察其他不易受此影响的指标。
- en: 4.2 Missing and Extra Label Bias Mitigation
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 缺失和额外标签偏差的缓解
- en: '![Refer to caption](img/93d29cf90a5406c5b7f1497bc33fd771.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/93d29cf90a5406c5b7f1497bc33fd771.png)'
- en: 'Figure 5: Galaxy and star counts for our COSMOS set, with labels generated
    from HSC and HST catalogs. The extra counts of HSC stars at faint magnitudes is
    due to galaxy contamination when classification is based on the extendedness metric.
    The low sample of bright HSC stars follows from our catalog matching procedure
    of excluding objects with NaN values.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：我们的 COSMOS 数据集中星系和恒星的计数，标签来源于 HSC 和 HST 目录。HSC 在微弱星等处的额外恒星计数是由于当分类基于扩展度指标时星系的污染。亮恒星样本的不足是由于我们的目录匹配程序中排除了
    NaN 值的对象。
- en: '![Refer to caption](img/8aee1fe61c74c3279868323b94dfb596.png)![Refer to caption](img/d5620a48e8975d6c3e7f1eb26fa822e2.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8aee1fe61c74c3279868323b94dfb596.png)![参考说明](img/d5620a48e8975d6c3e7f1eb26fa822e2.png)'
- en: 'Figure 6: Top: Galaxy precision/recall metrics as a function of object magnitude
    in the HST i-band. The colors correspond to individual backbone architectures
    and are shown in the legend. Linestyles represent different network architectures
    following the legend, and colors indicate which contrast scaling was used (red
    for Lupton, blue for LuptonHC and black for z-scale). The black vertical line
    indicates the Deep/UltraDeep i-band 5$\sigma$ magnitude of 26.9 mag. The y-axis
    is truncated to better show the differences across the models. Bottom: Stellar
    precision/recall metrics as a function of object magnitude in the HST i-band.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：顶部：HST i 波段中星系的精确度/召回率指标随对象星等的变化。颜色对应于各个主干网络架构，并在图例中显示。线型表示不同的网络架构，颜色则指示使用了哪种对比度缩放（红色为
    Lupton，蓝色为 LuptonHC，黑色为 z-scale）。黑色垂直线表示 Deep/UltraDeep i 波段 5$\sigma$ 星等为 26.9
    mag。y 轴被截断以更好地显示模型之间的差异。底部：HST i 波段中恒星的精确度/召回率指标随对象星等的变化。
- en: '|  | ResNets | Transformers |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | ResNets | Transformers |'
- en: '| --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  |  | R101C4 | R101dc5 | R101fpn | R50cas | R50def | X101fpn | MViTv2 | Swin
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  |  | R101C4 | R101dc5 | R101fpn | R50cas | R50def | X101fpn | MViTv2 | Swin
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Galaxies | Lupton | 0.96 | 0.98 | 0.98 | 0.98 | 0.98 | 0.98 | 0.99 | 0.99
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 星系 | Lupton | 0.96 | 0.98 | 0.98 | 0.98 | 0.98 | 0.98 | 0.99 | 0.99 |'
- en: '| LuptonHC | 0.97 | 0.98 | 0.98 | 0.98 | 0.98 | 0.98 | 0.99 | 0.99 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 0.97 | 0.98 | 0.98 | 0.98 | 0.98 | 0.98 | 0.99 | 0.99 |'
- en: '| zscale | 0.98 | 0.98 | 0.98 | 0.99 | 0.97 | 0.98 | 0.99 | 0.99 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 0.98 | 0.98 | 0.98 | 0.99 | 0.97 | 0.98 | 0.99 | 0.99 |'
- en: '| Stars | Lupton | 0.46 | 0.47 | 0.33 | 0.33 | 0.21 | 0.15 | 0.88 | 0.88 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 恒星 | Lupton | 0.46 | 0.47 | 0.33 | 0.33 | 0.21 | 0.15 | 0.88 | 0.88 |'
- en: '| LuptonHC | 0.23 | 0.33 | 0.32 | 0.40 | 0.29 | 0.37 | 0.80 | 0.75 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 0.23 | 0.33 | 0.32 | 0.40 | 0.29 | 0.37 | 0.80 | 0.75 |'
- en: '| zscale | 0.69 | 0.57 | 0.61 | 0.76 | 0.60 | 0.64 | 0.87 | 0.87 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 0.69 | 0.57 | 0.61 | 0.76 | 0.60 | 0.64 | 0.87 | 0.87 |'
- en: 'Table 4: F1 scores for star and galaxy classes in the HST COSMOS test set,
    computed for all network configurations. Transformer networks outperform convolutional
    networks in all cases, especially for stars.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在 HST COSMOS 测试集中针对所有网络配置计算的恒星和星系类别的 F1 分数。Transformer 网络在所有情况下都优于卷积网络，尤其是在恒星检测方面。
- en: Since we have done the labelling ourselves using sep, scarlet and catalog matching
    to produce ground truth detections, masks and classes, traditional metrics of
    network performance may not be the best choice in characterizing efficacy. Consider
    the precision/recall and AP metric. An implicit assumption in these metrics is
    the completeness and purity of the ground truth labels. This assumption holds
    for large annotated sets of terrestrial images such as the MS-COCO set (Lin et al.,
    [2014](#bib.bib47)) commonly used as a benchmark in object detection/segmentation
    studies. It also holds for simulated datasets of astronomical images (Burke et al.,
    [2019](#bib.bib14)) as the ground truth object locations, masks, and classes are
    all known a priori when constructing the training and test set labels. However,
    real data of large astronomical scenes presents a challenge. Given that we must
    generate labels without a known underlying truth, any comparisons to this “ground
    truth” are really comparisons to the methods used to generate these labels. Issues
    in the label generating procedures will propagate to the performance metrics.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用sep、scarlet和目录匹配自己标记了地面真实检测、掩模和类别，传统的网络性能指标可能不是表征效能的最佳选择。考虑精确度/召回率和AP指标。这些指标的一个隐含假设是地面真实标签的完整性和纯度。这个假设适用于大型标注的地面图像数据集，例如MS-COCO数据集（Lin
    et al., [2014](#bib.bib47)），该数据集通常用作目标检测/分割研究的基准。它也适用于天文图像的模拟数据集（Burke et al.,
    [2019](#bib.bib14)），因为在构建训练和测试集标签时，地面真实对象的位置、掩模和类别都是已知的。然而，真实的大型天文场景数据提出了挑战。由于我们必须在没有已知真实情况的情况下生成标签，与这一“地面真实”进行的任何比较实际上是与生成这些标签的方法的比较。标签生成过程中的问题将会传递到性能指标中。
- en: First, the ground truth detections are produced from running sep using a detection
    threshold of 5$\sigma$ above the background. This causes a lack of complete labels,
    as some objects are missed. We could lower this threshold, but then run the risk
    of further over-deblending extended/saturated objects. This leads to the second
    issue in that there will still remain some level of shredding that will cause
    spurious extra objects to appear in the ground truth set, i.e, a lack of pure
    labels. If the networks do not shred extended/saturated objects as much as sep,
    (which is a desirable feature of the networks) then the AP metric will be lower
    due to less spurious network detections than the ground truth. Finally, the object
    detection mechanisms of the neural networks used in this work are fundamentally
    different from the peak-finding detection used in sep.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用检测阈值为背景以上5$\sigma$的sep运行生成地面真实检测。这会导致标签不完整，因为一些对象会被遗漏。我们可以降低这个阈值，但这样会有进一步过度融合扩展/饱和对象的风险。这导致第二个问题，即仍然会存在一些撕裂现象，这会导致地面真实集出现虚假的额外对象，即标签的不纯。
    如果网络没有像sep那样撕裂扩展/饱和对象（这是网络的一个理想特征），那么AP指标将会较低，因为网络检测到的虚假对象较少。最后，本文中使用的神经网络的目标检测机制与sep中使用的峰值检测方法从根本上不同。
- en: 'These issues lead to cases in which the neural networks detect objects that
    are not labelled in our ground truth catalog, despite being actual objects, or
    cases in which the networks do not detect unphysical objects that are in the ground
    truth. Any metric that considers true/false detections is subject to this effect.
    We do not wish to count these cases of fake true/false positives, as this would
    lead to a reduction in performance metrics that does not reflect network classification/detection
    accuracy, but rather the limitations of our label generation. Therefore, we construct
    a set of metrics similar to the canonical precision and recall, but slightly alter
    our definitions of positive and negative detections. We use equations [4](#S4.E4
    "In 4 HSC Results ‣ Detection, Instance Segmentation, and Classification for Astronomical
    Surveys with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration
    with Hyper Suprime-Cam Data") and [5](#S4.E5 "In 4 HSC Results ‣ Detection, Instance
    Segmentation, and Classification for Astronomical Surveys with Deep Learning (DeepDISC):
    Detectron2 Implementation and Demonstration with Hyper Suprime-Cam Data"), but
    we limit our metrics to the set of objects D that are matched to a ground truth
    detection. The set of matched detections D is determined by selecting the inferred
    bounding box with the highest IOU to a ground truth bounding box, above a threshold
    of 0.5\. Then for a given class C, true positives are the objects in D that are
    correctly classified, false positives are objects that are incorrectly assigned
    class C, and false negatives are matched objects with a ground truth class C that
    the network assigns to a different class. With these metrics, precision and recall
    measure purely the classification power of the network, without bias from missing
    labels or extra false labels. If we assume that the network’s ability to classify
    remains consistent for objects outside of the matched set, we can generalize these
    metrics to overall classification performance.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题导致神经网络检测到我们地面真值目录中未标记的对象，尽管它们是真实存在的对象，或者神经网络未检测到地面真值中存在的非物理对象。任何考虑真假检测的指标都受到这种影响。我们不希望计算这些虚假的真假正例，因为这会导致性能指标下降，这并不反映网络的分类/检测准确性，而是我们标签生成的局限性。因此，我们构建了一组类似于经典精确率和召回率的指标，但稍微调整了我们对正例和负例的定义。我们使用方程
    [4](#S4.E4 "在 4 HSC 结果 ‣ 深度学习的天文调查中的检测、实例分割和分类（DeepDISC）：Detectron2 实现及 Hyper
    Suprime-Cam 数据演示") 和 [5](#S4.E5 "在 4 HSC 结果 ‣ 深度学习的天文调查中的检测、实例分割和分类（DeepDISC）：Detectron2
    实现及 Hyper Suprime-Cam 数据演示")，但我们将我们的指标限制在与地面真值检测匹配的对象集合 D 上。匹配检测集合 D 是通过选择与地面真值边界框的
    IOU 最高的推断边界框来确定的，阈值为 0.5。然后，对于给定的类别 C，真正例是集合 D 中正确分类的对象，假正例是被错误分配为类别 C 的对象，假负例是与地面真值类别
    C 匹配的对象，但网络将其分配为其他类别。通过这些指标，精确率和召回率纯粹测量网络的分类能力，不受丢失标签或额外虚假标签的偏倚。如果我们假设网络对匹配集外的对象的分类能力保持一致，我们可以将这些指标推广到整体分类性能。
- en: We combine precision and recall into one metric to judge classification power,
    the F1 score, which is given by the harmonic mean between precision and recall,
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将精确率和召回率结合成一个指标来评估分类能力，即 F1 分数，它是精确率和召回率之间的调和平均数，
- en: '|  | $\textrm{F}1=2\times\frac{p*r}{p+r}.$ |  | (8) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textrm{F}1=2\times\frac{p*r}{p+r}.$ |  | (8) |'
- en: 'The F1 score balances the trade-off between precision and recall, with a value
    close to unity being desirable. We report the F1 scores for the networks on the
    HST COSMOS test set in Table [4](#S4.T4 "Table 4 ‣ 4.2 Missing and Extra Label
    Bias Mitigation ‣ 4 HSC Results ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data"). The best performing configuration
    among ResNet architectures is the R50cas network with a z-scale scaling. A Swin
    network with a Lupton scaling achieves the highest overall galaxy and star F1
    scores, although the MViTv2 architecture remains competitive. Nearly all transformer
    networks configurations perform better on star/galaxy classification than ResNet-based
    networks. Classification power of transformer-based networks is again more robust
    to contrast scalings than ResNet-based networks.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 'F1分数在精度和召回率之间平衡权衡，接近1的值是理想的。我们在表[4](#S4.T4 "Table 4 ‣ 4.2 Missing and Extra
    Label Bias Mitigation ‣ 4 HSC Results ‣ Detection, Instance Segmentation, and
    Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data")中报告了HST COSMOS测试集上的网络F1分数。ResNet架构中表现最佳的配置是具有z-scale缩放的R50cas网络。采用Lupton缩放的Swin网络在整体星系和恒星F1分数上取得了最高成绩，尽管MViTv2架构仍然具有竞争力。几乎所有的transformer网络配置在星系/恒星分类上表现优于基于ResNet的网络。基于transformer的网络在对比度缩放的鲁棒性方面再次优于基于ResNet的网络。'
- en: 'To examine network performance on faint objects, we show precision and recall
    as a function of i band magnitude for the HST COSMOS test set in Figure [6](#S4.F6
    "Figure 6 ‣ 4.2 Missing and Extra Label Bias Mitigation ‣ 4 HSC Results ‣ Detection,
    Instance Segmentation, and Classification for Astronomical Surveys with Deep Learning
    (DeepDISC): Detectron2 Implementation and Demonstration with Hyper Suprime-Cam
    Data"). Galaxy recall maintains a value close to one for all objects regardless
    of magnitude, with some fluctuations of a few percent for some models. Galaxy
    precision dips for some models at bright magnitudes, which may be due to compact
    galaxies with bright cores resembling stars. However, these dips are more likely
    due to inherent limitations of the models rather than label generation, as transformer
    architectures produce high galaxy precision and recall across magnitude bins compared
    to ResNet architectures. Most ResNet architectures suffer with stellar recall,
    with many showing poor performance even at bright magnitudes. Stellar precision
    reaches near unity at bright magnitudes for all architectures, but many networks
    configurations begin to drop in performance around i band magnitudes of 21 mag.
    The best performing networks maintain a stellar precision above 0.8 out to $\sim$25
    mag in the i band. The transformer models we trained are able to achieve a 99.6
    percent galaxy recall, 99.2 percent galaxy precision, 85.4 percent stellar recall
    and 91.5 percent star precision on our HST COSMOS test set, averaged over the
    whole magnitude range. For comparison, He et al. ([2021](#bib.bib33)) perform
    deep neural network object detection and classification of stars, galaxies, and
    quasars in large SDSS images. With their sample of objects that covers an r band
    magnitude range of 14-25 mag, they report a galaxy recall of 95.1 percent, galaxy
    precision of 95.8 percent, stellar recall of 84.6 percent and stellar precision
    of 94.5 percent.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '为了检查在微弱物体上的网络性能，我们展示了HST COSMOS测试集在i带光度下的精度和召回率，如图[6](#S4.F6 "Figure 6 ‣ 4.2
    Missing and Extra Label Bias Mitigation ‣ 4 HSC Results ‣ Detection, Instance
    Segmentation, and Classification for Astronomical Surveys with Deep Learning (DeepDISC):
    Detectron2 Implementation and Demonstration with Hyper Suprime-Cam Data")所示。无论光度如何，星系的召回率对所有物体保持接近1的值，虽然对于某些模型会有几个百分点的波动。对于某些模型，星系的精度在明亮的光度下会下降，这可能是由于紧凑的星系的亮核心类似于恒星。然而，这些下降更可能是模型固有限制的结果，而非标签生成问题，因为与ResNet架构相比，transformer架构在所有光度区间内产生了高的星系精度和召回率。大多数ResNet架构在恒星召回率上表现较差，许多在明亮的光度下表现不佳。对于所有架构，恒星精度在明亮的光度下接近1，但许多网络配置在i带光度为21
    mag左右时开始出现性能下降。表现最佳的网络在i带光度为$\sim$25 mag时保持恒星精度在0.8以上。我们训练的transformer模型在HST COSMOS测试集中能够实现99.6%的星系召回率、99.2%的星系精度、85.4%的恒星召回率和91.5%的恒星精度，这些指标是对整个光度范围的平均值。相比之下，He等人（[2021](#bib.bib33)）在大型SDSS图像中进行深度神经网络对象检测和分类，包括恒星、星系和类星体。他们的样本覆盖了14-25
    mag的r带光度范围，报告了95.1%的星系召回率、95.8%的星系精度、84.6%的恒星召回率和94.5%的恒星精度。'
- en: 4.3 Deblending
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 去混叠
- en: 'In order to quantify deblending performance of the networks, we compute IOU
    scores for matched objects. The process is similar to the matching done in computing
    classification precision/recall. We first set a detection confidence threshold
    of 0.5 and then compute the bounding box IOUs for all detected and ground truth
    objects. For each ground truth object, we take the corresponding detected object
    with the highest IOU above a threshold of 0.5\. We employ this threshold to avoid
    the biases discussed in Section [4.2](#S4.SS2 "4.2 Missing and Extra Label Bias
    Mitigation ‣ 4 HSC Results ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data"). An IOU of one indicates a perfect
    match between the ground truth box and the inferred box. In addition to bounding
    box IOU, we also compute the segmentation mask IOU, which follows from Equation
    [6](#S4.E6 "In 4 HSC Results ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data"), but uses the area of the true
    and predicted segmentation masks. We report the median IOU for all matched objects
    in Table [5](#S4.T5 "Table 5 ‣ 4.3 Deblending ‣ 4 HSC Results ‣ Detection, Instance
    Segmentation, and Classification for Astronomical Surveys with Deep Learning (DeepDISC):
    Detectron2 Implementation and Demonstration with Hyper Suprime-Cam Data"), and
    show the distributions in Figure and [7](#S4.F7 "Figure 7 ‣ 4.3 Deblending ‣ 4
    HSC Results ‣ Detection, Instance Segmentation, and Classification for Astronomical
    Surveys with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration
    with Hyper Suprime-Cam Data"). Transformer-based networks generally produce a
    higher bounding box IOU than ResNet-based networks, although the R50cas, R101fpn
    and X101fpn networks remain competitive. Segmentation mask IOUs are lower than
    bounding box IOUs in all cases. This indicates that while the networks are able
    to identify overall object sizes quite well, the finer details of object shapes
    within the bounding boxes are not as well inferred.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '为了量化网络的去混叠性能，我们计算了匹配对象的IOU分数。这个过程类似于计算分类精度/召回率时的匹配。我们首先设置检测置信度阈值为0.5，然后计算所有检测到的和真实对象的边界框IOU。对于每个真实对象，我们取IOU高于0.5阈值的对应检测对象。我们采用这个阈值以避免第[4.2](#S4.SS2
    "4.2 Missing and Extra Label Bias Mitigation ‣ 4 HSC Results ‣ Detection, Instance
    Segmentation, and Classification for Astronomical Surveys with Deep Learning (DeepDISC):
    Detectron2 Implementation and Demonstration with Hyper Suprime-Cam Data")节讨论的偏差。IOU为一表示真实框与推测框的完美匹配。除了边界框IOU外，我们还计算了分割掩码IOU，这从[6](#S4.E6
    "In 4 HSC Results ‣ Detection, Instance Segmentation, and Classification for Astronomical
    Surveys with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration
    with Hyper Suprime-Cam Data")方程得出，但使用了真实和预测分割掩码的面积。我们在[5](#S4.T5 "Table 5 ‣ 4.3
    Deblending ‣ 4 HSC Results ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data")表中报告了所有匹配对象的中位数IOU，并在图[7](#S4.F7
    "Figure 7 ‣ 4.3 Deblending ‣ 4 HSC Results ‣ Detection, Instance Segmentation,
    and Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data")中展示了分布。基于变压器的网络通常产生比基于ResNet的网络更高的边界框IOU，尽管R50cas、R101fpn和X101fpn网络仍然具有竞争力。分割掩码IOU在所有情况下都低于边界框IOU。这表明，虽然网络能够较好地识别整体对象尺寸，但在边界框内对象形状的细节推断较差。'
- en: '![Refer to caption](img/1ef82f2267df1caa0a416b5dfc86f425.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1ef82f2267df1caa0a416b5dfc86f425.png)'
- en: 'Figure 7: Bounding box IOUs of each detected object that is matched a a ground
    truth object. Rows show the results for different transformer backbones. Top:
    results for ResNet backbones. Bottom: results for transformer backbones. The left
    column represents Lupton scaling, the middle Lupton high-contrast and the right
    z-scaling.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：每个检测到的对象与真实对象匹配的边界框IOU。行显示了不同变压器骨干网络的结果。顶部：ResNet骨干网络的结果。底部：变压器骨干网络的结果。左列表示Lupton缩放，中间列为Lupton高对比度，右列为z缩放。
- en: '|  | ResNets | Transformers |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | ResNets | Transformers |'
- en: '| --- | --- | --- |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | R101C4 | R101dc5 | R101fpn | R50cas | R50def | X101fpn | MViTv2 | Swin
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | R101C4 | R101dc5 | R101fpn | R50cas | R50def | X101fpn | MViTv2 | Swin
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Lup | 0.75 (0.61) | 0.78 (0.57) | 0.93 (0.63) | 0.94 (0.62) | 0.93 (0.64)
    | 0.93 (0.64) | 0.94 (0.64) | 0.94 (0.64) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Lup | 0.75 (0.61) | 0.78 (0.57) | 0.93 (0.63) | 0.94 (0.62) | 0.93 (0.64)
    | 0.93 (0.64) | 0.94 (0.64) | 0.94 (0.64) |'
- en: '| LupHC | 0.76 (0.61) | 0.79 (0.58) | 0.93 (0.64) | 0.94 (0.64) | 0.93 (0.64)
    | 0.93 (0.64) | 0.94 (0.64) | 0.94 (0.64) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| LupHC | 0.76 (0.61) | 0.79 (0.58) | 0.93 (0.64) | 0.94 (0.64) | 0.93 (0.64)
    | 0.93 (0.64) | 0.94 (0.64) | 0.94 (0.64) |'
- en: '| Zscale | 0.78 (0.61) | 0.81 (0.59) | 0.92 (0.62) | 0.93 (0.63) | 0.82 (0.65)
    | 0.91 (0.64) | 0.94 (0.65) | 0.94 (0.65) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Zscale | 0.78 (0.61) | 0.81 (0.59) | 0.92 (0.62) | 0.93 (0.63) | 0.82 (0.65)
    | 0.91 (0.64) | 0.94 (0.65) | 0.94 (0.65) |'
- en: 'Table 5: Median bounding box IOUs for matched objects in the COSMOS HST test.
    The best bounding box IOU for each row is emphasized in bold. Also shown in parentheses
    are the median segmentation mask IOUs. An IOU above 0.5 is considered to be a
    good match, and a score of 1.0 is a perfect overlap of ground truth and inference.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：COSMOS HST 测试中匹配物体的中位数边界框 IOU。每行中最佳的边界框 IOU 用**粗体**突出显示。括号中还显示了中位数分割掩码 IOU。IOU
    值高于 0.5 被认为是匹配良好，1.0 的得分表示真实值与推断的完美重叠。
- en: 'The median IOUs measure the ability of the network to detect and segment objects,
    but it does not fully capture the deblending power of the networks. We examine
    the cases of a few close blends to get a sense of the ability of the networks
    to distinguish large overlapping objects. We demonstrate the deblending capabilities
    of the different networks in Figure [8](#S4.F8 "Figure 8 ‣ 4.3 Deblending ‣ 4
    HSC Results ‣ Detection, Instance Segmentation, and Classification for Astronomical
    Surveys with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration
    with Hyper Suprime-Cam Data"). In very crowded scenes, the networks are able to
    distinguish the individual sources, and even pick up objects that are not present
    in the labelled set, which may present an advantage for studies of low surface-brightness
    galaxies. As discussed in Section [4.2](#S4.SS2 "4.2 Missing and Extra Label Bias
    Mitigation ‣ 4 HSC Results ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data"), this is likely due to the difference
    in object detection abilities of the Region Proposal Networks compared to peak-finding
    methods, and highlights that the models are not limited by the training data,
    but are able to extrapolate beyond it. It is also possible to alter inference
    hyperparameters such as IOU or detection confidence thresholds, which could allow
    for more or less detections or overlap between detections. In Figure [9](#S4.F9
    "Figure 9 ‣ 4.3 Deblending ‣ 4 HSC Results ‣ Detection, Instance Segmentation,
    and Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data") we demonstrate
    the effect of lowering the confidence threshold hyperparameter, allowing for more
    low-confidence detections. While not equivalent, this is similar to lowering the
    detection threshold in peak-finding algorithms. There are cases in which deblending
    is poor, and these are typically very large galaxies with one or more very large
    and very close companions. In such instances, it may be better to use a different
    contrast scaling. In Figure [10](#S4.F10 "Figure 10 ‣ 4.3 Deblending ‣ 4 HSC Results
    ‣ Detection, Instance Segmentation, and Classification for Astronomical Surveys
    with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration with
    Hyper Suprime-Cam Data"), a Lupton contrast scaling prevents the network from
    deblending multiple large sources. With the same IOU/confidence score thresholds,
    a z-scaling works to better isolate the two sources. This is likely due to much
    larger dynamic range of our z-scaling, which allows for less smearing of the sources
    and more distinguishing power in this case. Overall, there does not seem to be
    a one-size-fits-all network configuration for the cases of very large and very
    close blends. Training on more data would likely improve the ability to detect
    and segment these objects.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '中位数 IOU 衡量了网络检测和分割物体的能力，但并未完全捕捉网络的去混叠能力。我们检查了几个紧密混合的案例，以了解网络区分大范围重叠物体的能力。我们在图
    [8](#S4.F8 "Figure 8 ‣ 4.3 Deblending ‣ 4 HSC Results ‣ Detection, Instance Segmentation,
    and Classification for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2
    Implementation and Demonstration with Hyper Suprime-Cam Data") 中展示了不同网络的去混叠能力。在非常拥挤的场景中，网络能够区分单独的来源，甚至识别出标签集中不存在的物体，这可能对低表面亮度星系的研究具有优势。如在第
    [4.2](#S4.SS2 "4.2 Missing and Extra Label Bias Mitigation ‣ 4 HSC Results ‣ Detection,
    Instance Segmentation, and Classification for Astronomical Surveys with Deep Learning
    (DeepDISC): Detectron2 Implementation and Demonstration with Hyper Suprime-Cam
    Data") 节中讨论的，这可能是由于区域提议网络与峰值检测方法在物体检测能力上的差异，突出了模型并不受训练数据的限制，而是能够进行超出训练数据的推断。还可以改变推断超参数，例如
    IOU 或检测置信度阈值，这可能会允许更多或更少的检测或检测之间的重叠。在图 [9](#S4.F9 "Figure 9 ‣ 4.3 Deblending ‣
    4 HSC Results ‣ Detection, Instance Segmentation, and Classification for Astronomical
    Surveys with Deep Learning (DeepDISC): Detectron2 Implementation and Demonstration
    with Hyper Suprime-Cam Data") 中，我们展示了降低置信度阈值超参数的效果，从而允许更多低置信度的检测。虽然不完全等同，但这类似于在峰值检测算法中降低检测阈值。有些情况下去混叠效果较差，这通常是非常大的星系与一个或多个非常大且非常接近的伴星。在这种情况下，使用不同的对比度缩放可能更好。在图
    [10](#S4.F10 "Figure 10 ‣ 4.3 Deblending ‣ 4 HSC Results ‣ Detection, Instance
    Segmentation, and Classification for Astronomical Surveys with Deep Learning (DeepDISC):
    Detectron2 Implementation and Demonstration with Hyper Suprime-Cam Data") 中，Lupton
    对比度缩放防止了网络去混叠多个大型源。使用相同的 IOU/置信度评分阈值，z-scaling 更好地隔离了这两个来源。这可能是由于我们 z-scaling
    的动态范围更大，这减少了源的模糊，并在这种情况下提供了更多的区分能力。总体而言，对于非常大且非常接近的混合情况，似乎没有一种通用的网络配置。使用更多的数据进行训练可能会提高检测和分割这些物体的能力。'
- en: '![Refer to caption](img/3863e6c58362fe235b3f855783601221.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3863e6c58362fe235b3f855783601221.png)'
- en: 'Figure 8: Inference on a close blend. The ground truth is shown on the left.
    RGB images are created with a Lupton contrast scaling. Other panels show model
    inference of segmentation maps and classes. Top row, left to right: R101C4, R101dc5,
    R101fpn. Bottom row, left to right: R50cas, R50def, X101fpn. The colors indicate
    classes, green for galaxy and red for star. Differences in detections are solely
    due to the different backbones. While the networks do not pick up every ground
    truth object, they are also able to detect real objects that were missed by our
    ground truth labelling.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：对近距离混合的推断。地面真相显示在左侧。RGB 图像使用 Lupton 对比度缩放创建。其他面板展示了模型的分割图和类别推断。顶行，从左到右：R101C4、R101dc5、R101fpn。底行，从左到右：R50cas、R50def、X101fpn。颜色表示类别，绿色表示星系，红色表示恒星。检测差异完全由于不同的骨干网络。虽然网络未能识别所有地面真相物体，但它们也能够检测到地面真相标注遗漏的真实物体。
- en: '![Refer to caption](img/ae319a0c1da00900e34833c98de373ff.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ae319a0c1da00900e34833c98de373ff.png)'
- en: 'Figure 9: Inference on the same close blend as Figure [8](#S4.F8 "Figure 8
    ‣ 4.3 Deblending ‣ 4 HSC Results ‣ Detection, Instance Segmentation, and Classification
    for Astronomical Surveys with Deep Learning (DeepDISC): Detectron2 Implementation
    and Demonstration with Hyper Suprime-Cam Data"), but only with a Swin architecture.
    The ground truth is shown on the left most panel, and the effect of lowering the
    detection confidence threshold to 0.5, 0.4, 0.3 is shown in left to right, respectively.
    As the threshold is lowered, objects within a larger footprint are detected.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9：与图 [8](#S4.F8 "Figure 8 ‣ 4.3 Deblending ‣ 4 HSC Results ‣ Detection, Instance
    Segmentation, and Classification for Astronomical Surveys with Deep Learning (DeepDISC):
    Detectron2 Implementation and Demonstration with Hyper Suprime-Cam Data") 中的相同近距离混合进行的推断，但仅使用了
    Swin 架构。地面真相显示在最左侧面板中，将检测置信度阈值降低到 0.5、0.4 和 0.3 的效果分别从左到右显示。随着阈值的降低，能够检测到更大区域内的物体。'
- en: '![Refer to caption](img/4026065197dabf4c352bba0b445b0185.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4026065197dabf4c352bba0b445b0185.png)'
- en: 'Figure 10: The effect of using a different contrast scaling on a close blend.
    We show inference of a R50cas network when trained on Lupton scaled images (left)
    and z-scaled images (right). The objects are more easily distinguished with a
    z-scaling.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：使用不同对比度缩放对近距离混合的影响。我们展示了 R50cas 网络在 Lupton 缩放图像（左）和 z 缩放图像（右）上训练时的推断。通过
    z 缩放，物体更容易被区分。
- en: 5 Discussion
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: The effectiveness of instance segmentation models has been proven in many domains,
    boosted by the ability of networks to work “out-of-the-box” and without much fine-tuning.
    It has been shown that an object detection model based on the Mask R-CNN framework
    performs well in the classification and detection/segmentation of simulated astronomical
    survey images (Burke et al., [2019](#bib.bib14)). In this work, we have trained
    and tested a broad range of state-of-the-art instance segmentation models on real
    data taken from the HSC SSP Data Release 3 to push the direction of deep learning
    based galaxy detection, classification, and deblending towards real applications.
    Network training and evaluation performance is limited by the efficacy of our
    label generation methodology, a task not easily formulated when the ground truth
    is not completely known. This limitation also affects the choices of metrics we
    use to measure network performance. Often, classification and detection power
    are combined into the AP score, used throughout instance segmentation literature.
    However, this may not the best choice of metric for comparisons, as it implicitly
    assumes the completeness and correctness of the ground truth labels. To attempt
    to mitigate the effects of incorrect labels on performance metrics, we construct
    a test set of objects with class labels determined from more accurate space-based
    HST observations. However, since the AP metric artificially suffers from the detections
    of “false positives” that are true objects simply missing from the labelled set
    and/or the presence of spurious ground truth detections, we further attempt to
    mitigate this bias by constraining performance metrics to detected objects that
    have a matched ground truth label.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 实例分割模型的有效性已在许多领域得到证实，这得益于网络能够“开箱即用”且无需进行大量微调的能力。据显示，基于Mask R-CNN框架的目标检测模型在模拟天文调查图像的分类和检测/分割方面表现良好（Burke
    et al.，[2019](#bib.bib14)）。在这项工作中，我们对来自HSC SSP数据第3版发布的真实数据进行了广泛的最新实例分割模型的训练和测试，以推动基于深度学习的星系检测、分类和去混合的方向朝着真实应用。网络的训练和评估性能受我们的标签生成方法的有效性限制，这是一项任务，在地面真实情况并非完全已知时不容易制定。这一限制还影响了我们用来衡量网络性能的指标选择。通常，分类和检测能力被合并为AP得分，这在实例分割文献中被广泛使用。然而，这可能不是用于比较的最佳指标，因为它暗含了地面真相标签的完整性和正确性。为了试图减轻不正确标签对性能指标的影响，我们构建了一个测试集，其中的对象的类别标签是根据更准确的空间观测到的HST观测确定的。然而，由于AP指标在“假阳性”检测方面人为受到影响，这些真实目标仅仅是缺失于标记集的，或者存在虚假的地面真实检测存在的情况，我们进一步尝试通过限制性能指标来缓解这种偏差，以检测到具有匹配地面真实标签的对象。
- en: We find that all networks perform well at classifying galaxies, even out to
    the faintest in the sample. Despite the wide variety of colors, sizes, and morphologies
    in the real imaging data, our models can identify these objects. Stellar classification
    is worse, likely due to the smaller sample size in the training and test set.
    Transformer based networks generally outperform ResNet based networks in classification
    power of both stars and galaxies. They also appear to be more robust classifiers
    as magnitudes become fainter. Transformer based models maintain near 100% completeness
    (recall) and purity (precision) of galaxy selection across the whole sample and
    above 60% completeness and 80% purity of stars out to i-band magnitudes of 25
    mag. These models are able to outperform the extendedness classifier used in the
    HSC catalogs, which depending on cuts yields near 100% galaxy purity, roughly
    90% galaxy completeness, stellar completeness slightly above 50% and stellar purity
    slightly above 40% at i-band magnitudes of 25 mag (Bosch et al., [2018](#bib.bib11)).
    The performance increase of our models is especially noteworthy because they are
    able to surpass the HSC class labelling despite being trained with it. Transformer
    models are also more robust to different contrast scalings than traditional convolutional
    neural networks, indicating that they may be more applicable to a wide range of
    images across surveys with different dynamic ranges.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现所有网络在对星系进行分类时表现良好，即使是对样本中最微弱的星系也能进行有效分类。尽管真实成像数据中颜色、大小和形态的种类繁多，我们的模型仍能识别这些对象。恒星分类的效果较差，可能是由于训练和测试集中样本量较小。基于
    Transformer 的网络在星星和星系的分类能力上通常优于基于 ResNet 的网络。随着星等变得更暗，Transformer 基于的模型似乎也更为稳健。Transformer
    基于的模型在整个样本中保持了近 100% 的星系选择完整性（召回率）和纯度（准确率），以及在 i 带星等 25 mag 时超过 60% 的恒星完整性和 80%
    的纯度。这些模型能够超越 HSC 目录中使用的扩展性分类器，后者根据切割条件产生接近 100% 的星系纯度、约 90% 的星系完整性、略高于 50% 的恒星完整性和略高于
    40% 的恒星纯度，i 带星等为 25 mag（Bosch et al., [2018](#bib.bib11)）。我们模型性能的提升尤其值得注意，因为它们能够超越
    HSC 类别标注，尽管它们是在 HSC 数据上进行训练的。与传统卷积神经网络相比，Transformer 模型对不同对比度缩放的鲁棒性更强，这表明它们可能更适用于具有不同动态范围的广泛图像。
- en: The detection/deblending capabilities are measured by the median bounding box
    IOUs of the networks. Again, transformer based networks generally outperform convolutional
    ResNet based networks. The improved performance of transformer networks over convolutional
    based ones may be attributable to the ability of different attention heads to
    encode information at different image scale sizes (Dosovitskiy et al., [2020](#bib.bib24)),
    allowing for more overall global information propagation than CNNs. While a convolutional
    neural network is able to learn spatial features through sliding a kernel across
    an image, a transformer learns features over the entire input at once, removing
    any limitations due to kernel sizes. It is possible that the transformer backbones
    are implicitly utilizing large scale features in the images such as the spatial
    clustering of objects, background noise or seeing and using these bulk properties
    to inform the network.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 检测/去混叠能力通过网络的中位数边界框 IOU 进行测量。同样，基于 Transformer 的网络通常优于基于卷积 ResNet 的网络。Transformer
    网络相对于基于卷积的网络性能提升，可能归因于不同注意力头能够在不同图像尺度上编码信息（Dosovitskiy et al., [2020](#bib.bib24)），从而比
    CNN 允许更多的全局信息传播。虽然卷积神经网络能够通过在图像上滑动内核来学习空间特征，但 Transformer 一次性学习整个输入的特征，从而消除了内核大小带来的限制。可能
    Transformer 主干隐含地利用了图像中的大尺度特征，如对象的空间聚类、背景噪声或视场，并利用这些整体属性来为网络提供信息。
- en: We examine a few cases of close blends to qualitatively see how the networks
    distinguish objects. There are cases in which the networks do not detect close
    objects, but these can sometimes be mitigated by altering the confidence and NMS
    IOU threshold hyperparmeters (which can be done after training). In other cases,
    using a different contrast scaling helps to isolate closely blended objects.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查了一些近距离混合的案例，以定性地观察网络如何区分对象。在一些情况下，网络未能检测到近距离的对象，但这些问题有时可以通过调整置信度和 NMS IOU
    阈值超参数（可以在训练后进行调整）来缓解。在其他情况下，使用不同的对比度缩放有助于隔离紧密混合的对象。
- en: There is room to improve both classification and segmentation of these models
    in future work. One possibility is constructing a larger training set with more
    accurate labels. With better and larger samples of stars/galaxies, networks may
    perform better on classification. The more close blends of large galaxies seen
    during training, the more likely the networks will be able to distinguish these
    scenes. There could be more fine-tuning of hyperparmeters done to the architectures
    before training, rather than running them out-of-the-box. Additionally, the use
    of more photometric information could help in all tasks. We use the i, r and g
    bands on the HSC instrument in this work, corresponding to RGB color images, but
    could further investigate the performance if we include the z and y bands.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的工作中，这些模型的分类和分割仍有改进的空间。一种可能性是构建一个包含更准确标签的更大训练集。通过更好和更多的星星/星系样本，网络可能在分类上表现得更好。在训练过程中看到的大型星系的更多近距离混合场景，网络将更有可能区分这些场景。在训练之前，对架构进行更多超参数的微调可能会有所帮助，而不是直接使用默认设置。此外，使用更多光度信息可能有助于所有任务。我们在这项工作中使用了HSC仪器上的i、r和g波段，相当于RGB彩色图像，但如果我们加入z和y波段，可能会进一步调查性能。
- en: It is possible that these networks need to be trained longer, or that the fundamentally
    different properties of astronomical images over terrestrial ones limits the abilities
    of these architectures in extracting useful features for classification. Despite
    our attempts to mitigate measurement biases arising from label generation, classification
    remains a challenge for these models at faint magnitudes. A machine learning model
    has already been used to classify HSC data using photometry information with better
    accuracy than morphological methods, but relies on the upstream task of detection
    (Bosch et al., [2018](#bib.bib11)). The instance segmentation models presented
    in this work are able to identify and assign classes after training using only
    an image as input.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络可能需要更长时间的训练，或者由于天文图像与地面图像在基本属性上的差异，限制了这些架构在提取用于分类的有用特征的能力。尽管我们尝试减轻由标签生成引起的测量偏差，但在微弱亮度下，分类仍然是这些模型面临的挑战。已经有一种机器学习模型被用来利用光度信息对HSC数据进行分类，其准确性优于形态学方法，但依赖于上游的检测任务（Bosch
    等人，[2018](#bib.bib11)）。本文展示的实例分割模型能够在训练后仅使用图像作为输入来识别和分配类别。
- en: 6 Conclusions
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: It is already a necessary consequence of the current epoch of astronomical research
    for machine learning algorithms to parse through massive sets of images. A first
    step in catalog construction is detecting these objects from imaging data. Advancements
    in the broader computer vision community have given rise to a large ecosystem
    of models that perform many necessary tasks at once, including detection, segmentation,
    and classification. While tried and tested on terrestrial data and shown to work
    on simulated astronomical data, the application on real survey images remains
    a work in progress. Many methods rely on the object detection stage to produce
    measurements of individual objects. In this work, we employ a variety of instance
    segmentation models available through Detectron2 to perform the detection task
    as well as deblending and object classification simultaneously on images taken
    from the HSC-SSP Data Release 3\. We carefully construct ground truth labels with
    existing frameworks and catalog matching, and caution that real data gives no
    straightforward way of producing labels. We find that the best networks perform
    well at classifying the faintest galaxies in the sample, and perform better than
    traditional methods at classifying stars up to i-band magnitudes of $\sim$25 mag.
    We find that even if trained on less accurate class labels, the neural networks
    still pick up on useful features that allow inference of the true underlying class.
    We expect more data with accurate labels to improve performance. The best performing
    models are able to detect and deblend by matching ground truth object locations
    and bounding boxes. Transformer networks appear to be a promising avenue of exploration
    in further studies.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的天文研究时代已使得机器学习算法解析大规模图像集成为必然结果。目录构建的第一步是从成像数据中检测这些对象。计算机视觉领域的进展催生了一个庞大的模型生态系统，这些模型可以同时执行许多必要的任务，包括检测、分割和分类。虽然这些方法在地球数据上经过测试并在模拟天文数据中显示有效，但在真实调查图像中的应用仍在进行中。许多方法依赖于对象检测阶段来生成单个对象的测量。在这项工作中，我们利用Detectron2提供的各种实例分割模型来同时进行检测任务、去混叠和对象分类，图像取自HSC-SSP数据发布3。我们通过现有框架和目录匹配精心构建真实标签，并警告说真实数据没有简单的方式来生成标签。我们发现，最佳网络在分类样本中最微弱的星系时表现良好，比传统方法在分类高达i-band
    $ \sim $25 mag的恒星时表现更好。即使在训练时使用不准确的类别标签，神经网络仍能捕捉到有用的特征，从而推断真实的基础类别。我们预计，更多准确标签的数据将改善性能。表现最好的模型能够通过匹配真实对象位置和边界框来检测和去混叠。Transformer网络在进一步研究中似乎是一个有前途的探索方向。
- en: There are many other areas for future study. While we tested a variety of models,
    there are many within Detectron2 that we did not implement. Some architectures
    are quite large and require significant resources to train. For example, we attempted
    to implement ViT backbones (Dosovitskiy et al., [2020](#bib.bib24)) among our
    set of transformer-based architectures, but were limited by the available GPU
    memory. Many models, especially transformers, are trained with state-of-the-art
    computing resources at FAIR or other organizations, and subsequently retraining
    them demands significant resources. Tests could be done on other sets of real
    data, with other downstream tasks in mind. For example, González et al. ([2018](#bib.bib28))
    investigate the application of instance segmentation models on SDSS data to classify
    galaxy morphologies. It would be straightforward to add additional classes, or
    implement a redshift estimation network using the modular nature of detectron2.
    In future work we plan to add a photo-z estimator branch to the Mask R-CNN/transformer
    networks and interface with the LSST software RAIL (Redshift Assessment Infrastructure
    Layers)²²2https://github.com/LSSTDESC/RAIL. The availability of realistic LSST-like
    simulations (LSST Dark Energy Science Collaboration (LSST DESC) et al., [2021](#bib.bib44))
    for training will allow us to avoid biases from label generation. The efficiency
    of neural networks and the ability to perform multiple tasks at once is now a
    necessity with the amount of survey data pouring into pipelines.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 未来还有许多其他研究领域。虽然我们测试了各种模型，但在Detectron2中还有许多模型未被实现。一些架构非常庞大，需要大量资源进行训练。例如，我们尝试在我们的变换器架构集合中实现ViT骨干（Dosovitskiy等，[2020](#bib.bib24)），但受到可用GPU内存的限制。许多模型，尤其是变换器，使用FAIR或其他组织的最先进计算资源进行训练，随后的重新训练需要大量资源。可以在其他真实数据集上进行测试，考虑其他下游任务。例如，González等人（[2018](#bib.bib28)）研究了在SDSS数据上应用实例分割模型以分类星系形态。使用Detectron2的模块化特性，添加额外类别或实现红移估计网络将变得简单。在未来的工作中，我们计划在Mask
    R-CNN/变换器网络中添加一个photo-z估计器分支，并与LSST软件RAIL（红移评估基础设施层）²²2https://github.com/LSSTDESC/RAIL接口。可用的逼真LSST-like模拟（LSST
    Dark Energy Science Collaboration (LSST DESC)等，[2021](#bib.bib44)）将使我们能够避免标签生成中的偏差。神经网络的效率和一次执行多项任务的能力现在是应对大量调查数据涌入管道的必要条件。
- en: As surveys push deeper into the sky, they will produce unprecedented amounts
    of objects that will be necessary to process. LSST will provide the deepest ground-based
    observations ever, and survey terrabytes of data every night, highlighting a need
    for accurate and precise object detection and classification, potentially in real-time.
    Correctly classifying and and deblending sources will be necessary for a wide
    range of studies, and deep instance segmentation models will be a valuable tool
    in handling these tasks.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 随着调查深入天空，它们将产生前所未有的大量对象，这些对象需要处理。LSST将提供最深的地基观测，每晚调查数TB的数据，突显了对准确和精确的对象检测和分类的需求，可能还需要实时进行。正确分类和分离源将对广泛的研究至关重要，深度实例分割模型将在处理这些任务中发挥重要作用。
- en: Acknowledgements
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Dr. S. Luo and Dr. D. Mu at the National Center for Supercomputing
    Applications (NCSA) for their assistance with the GPU cluster used in this work.
    We thank Y. Shen for helpful discussion on the HST observations of the COSMOS
    field. G.M., Y.L., Y.L. and X.L. acknowledge support from the NCSA Faculty Fellowship
    and the NCSA SPIN programs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢国家超级计算应用中心（NCSA）的S. Luo博士和D. Mu博士对本工作中使用的GPU集群的支持。我们感谢Y. Shen对COSMOS领域HST观测的有益讨论。G.M.、Y.L.、Y.L.和X.L.感谢NCSA教师奖学金和NCSA
    SPIN项目的支持。
- en: 'This work utilizes resources supported by the National Science Foundation’s
    Major Research Instrumentation program, grant #1725729, as well as the University
    of Illinois at Urbana-Champaign.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作利用了由国家科学基金会重大研究仪器项目（拨款号#1725729）以及伊利诺伊大学厄本那-香槟分校支持的资源。
- en: We acknowledge use of Matplotlib (Hunter, [2007](#bib.bib36)), a community-developed
    Python library for plotting. This research made use of Astropy,³³3http://www.astropy.org
    a community-developed core Python package for Astronomy (Astropy Collaboration
    et al., [2013](#bib.bib8); Price-Whelan et al., [2018](#bib.bib64)). This research
    has made use of NASA’s Astrophysics Data System.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确认使用了 Matplotlib（Hunter, [2007](#bib.bib36)），这是一个由社区开发的 Python 绘图库。本研究使用了
    Astropy，³³3http://www.astropy.org，这是一个由社区开发的核心 Python 天文学包（Astropy Collaboration
    et al., [2013](#bib.bib8); Price-Whelan et al., [2018](#bib.bib64)）。本研究还利用了 NASA
    的天体物理数据系统。
- en: The Hyper Suprime-Cam (HSC) collaboration includes the astronomical communities
    of Japan and Taiwan, and Princeton University. The HSC instrumentation and software
    were developed by the National Astronomical Observatory of Japan (NAOJ), the Kavli
    Institute for the Physics and Mathematics of the Universe (Kavli IPMU), the University
    of Tokyo, the High Energy Accelerator Research Organization (KEK), the Academia
    Sinica Institute for Astronomy and Astrophysics in Taiwan (ASIAA), and Princeton
    University. Funding was contributed by the FIRST program from Japanese Cabinet
    Office, the Ministry of Education, Culture, Sports, Science and Technology (MEXT),
    the Japan Society for the Promotion of Science (JSPS), Japan Science and Technology
    Agency (JST), the Toray Science Foundation, NAOJ, Kavli IPMU, KEK, ASIAA, and
    Princeton University.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Hyper Suprime-Cam（HSC）合作项目包括日本和台湾的天文学社区以及普林斯顿大学。HSC 的仪器和软件由日本国家天文台（NAOJ）、宇宙物理与数学研究所（Kavli
    IPMU）、东京大学、高能加速器研究组织（KEK）、台湾中央研究院天文学与天体物理学研究所（ASIAA）以及普林斯顿大学开发。资助来自日本内阁府的 FIRST
    计划、日本文部科学省（MEXT）、日本学术振兴会（JSPS）、日本科学技术振兴机构（JST）、东丽科学基金会、NAOJ、Kavli IPMU、KEK、ASIAA
    和普林斯顿大学。
- en: This paper makes use of software developed for the Large Synoptic Survey Telescope.
    We thank the LSST Project for making their code available as free software at
    http://dm.lsst.org
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文使用了为大型广域巡天望远镜开发的软件。我们感谢 LSST 项目将其代码作为免费软件提供，网址为 http://dm.lsst.org
- en: The Pan-STARRS1 Surveys (PS1) have been made possible through contributions
    of the Institute for Astronomy, the University of Hawaii, the Pan-STARRS Project
    Office, the Max-Planck Society and its participating institutes, the Max Planck
    Institute for Astronomy, Heidelberg and the Max Planck Institute for Extraterrestrial
    Physics, Garching, The Johns Hopkins University, Durham University, the University
    of Edinburgh, Queen’s University Belfast, the Harvard-Smithsonian Center for Astrophysics,
    the Las Cumbres Observatory Global Telescope Network Incorporated, the National
    Central University of Taiwan, the Space Telescope Science Institute, the National
    Aeronautics and Space Administration under Grant No. NNX08AR22G issued through
    the Planetary Science Division of the NASA Science Mission Directorate, the National
    Science Foundation under Grant No. AST-1238877, the University of Maryland, and
    Eotvos Lorand University (ELTE) and the Los Alamos National Laboratory.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Pan-STARRS1 调查（PS1）得以实现，得益于天文学研究所、夏威夷大学、Pan-STARRS 项目办公室、马克斯·普朗克学会及其参与机构、海德堡的马克斯·普朗克天文学研究所、加尔兴的马克斯·普朗克外星物理研究所、约翰斯·霍普金斯大学、达勒姆大学、爱丁堡大学、贝尔法斯特女王大学、哈佛-史密松天体物理中心、全球望远镜网络
    Las Cumbres 观察台、台湾中央大学、空间望远镜科学研究所、国家航空航天局（NASA）根据 NNX08AR22G 资助的计划科学部门、国家科学基金会（NSF）资助的
    AST-1238877 计划、马里兰大学、厄尔特大学（ELTE）以及洛斯阿拉莫斯国家实验室。
- en: Based [in part] on data collected at the Subaru Telescope and retrieved from
    the HSC data archive system, which is operated by Subaru Telescope and Astronomy
    Data Center at National Astronomical Observatory of Japan.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[部分]基于在 Subaru Telescope 收集的数据，并从由 Subaru Telescope 和日本国家天文台的天文学数据中心操作的 HSC
    数据存档系统中检索的数据。'
- en: This research has made use of the NASA/IPAC Infrared Science Archive, which
    is funded by the National Aeronautics and Space Administration and operated by
    the California Institute of Technology.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究使用了 NASA/IPAC 红外科学档案馆，该档案馆由国家航空航天局资助，并由加州理工学院运营。
- en: References
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Aihara et al. (2018a) Aihara H., et al., 2018a, [Publications of the Astronomical
    Society of Japan](http://dx.doi.org/10.1093/pasj/psx066), 70, S4
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aihara 等（2018a）Aihara H., et al., 2018a, [日本天文学会出版物](http://dx.doi.org/10.1093/pasj/psx066),
    70, S4
- en: Aihara et al. (2018b) Aihara H., et al., 2018b, [PASJ](http://dx.doi.org/10.1093/pasj/psx081),
    [70, S8](https://ui.adsabs.harvard.edu/abs/2018PASJ...70S...8A)
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aihara 等人 (2018b) Aihara H., 等人, 2018b, [PASJ](http://dx.doi.org/10.1093/pasj/psx081),
    [70, S8](https://ui.adsabs.harvard.edu/abs/2018PASJ...70S...8A)
- en: Aihara et al. (2022) Aihara H., et al., 2022, [Publications of the Astronomical
    Society of Japan](http://dx.doi.org/10.1093/pasj/psab122), 74, 247
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aihara 等人 (2022) Aihara H., 等人, 2022, [日本天文学会出版物](http://dx.doi.org/10.1093/pasj/psab122),
    74, 247
- en: Alam et al. (2015) Alam S., et al., 2015, [ApJS](http://dx.doi.org/10.1088/0067-0049/219/1/12),
    [219, 12](https://ui.adsabs.harvard.edu/abs/2015ApJS..219...12A)
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alam 等人 (2015) Alam S., 等人, 2015, [ApJS](http://dx.doi.org/10.1088/0067-0049/219/1/12),
    [219, 12](https://ui.adsabs.harvard.edu/abs/2015ApJS..219...12A)
- en: 'Amiaux et al. (2012) Amiaux J., et al., 2012, Euclid Mission: building of a
    reference survey, [doi:10.1117/12.926513](http://dx.doi.org/10.1117/12.926513),
    [https://doi.org/10.1117/12.926513](https://doi.org/10.1117/12.926513)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amiaux 等人 (2012) Amiaux J., 等人, 2012, 欧几里得任务：参考调查的建立, [doi:10.1117/12.926513](http://dx.doi.org/10.1117/12.926513),
    [https://doi.org/10.1117/12.926513](https://doi.org/10.1117/12.926513)
- en: Andreon et al. (2000) Andreon S., Gargiulo G., Longo G., Tagliaferri R., Capuano
    N., 2000, [MNRAS](http://dx.doi.org/10.1046/j.1365-8711.2000.03700.x), [319, 700](https://ui.adsabs.harvard.edu/abs/2000MNRAS.319..700A)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andreon 等人 (2000) Andreon S., Gargiulo G., Longo G., Tagliaferri R., Capuano
    N., 2000, [MNRAS](http://dx.doi.org/10.1046/j.1365-8711.2000.03700.x), [319, 700](https://ui.adsabs.harvard.edu/abs/2000MNRAS.319..700A)
- en: Arcelin et al. (2021) Arcelin B., Doux C., Aubourg E., Roucelle C., LSST Dark
    Energy Science Collaboration 2021, [Monthly Notices of the Royal Astronomical
    Society](http://dx.doi.org/10.1093/mnras/staa3062), 500, 531
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arcelin 等人 (2021) Arcelin B., Doux C., Aubourg E., Roucelle C., LSST Dark Energy
    Science Collaboration 2021, [皇家天文学会月刊](http://dx.doi.org/10.1093/mnras/staa3062),
    500, 531
- en: Astropy Collaboration et al. (2013) Astropy Collaboration et al., 2013, [A&A](http://dx.doi.org/10.1051/0004-6361/201322068),
    [558, A33](http://adsabs.harvard.edu/abs/2013A%26A...558A..33A)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Astropy Collaboration 等人 (2013) Astropy Collaboration 等人, 2013, [A&A](http://dx.doi.org/10.1051/0004-6361/201322068),
    [558, A33](http://adsabs.harvard.edu/abs/2013A%26A...558A..33A)
- en: Bertin & Arnouts (1996) Bertin E., Arnouts S., 1996, [A&AS](http://dx.doi.org/10.1051/aas:1996164),
    [117, 393](https://ui.adsabs.harvard.edu/abs/1996A&AS..117..393B)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertin & Arnouts (1996) Bertin E., Arnouts S., 1996, [A&AS](http://dx.doi.org/10.1051/aas:1996164),
    [117, 393](https://ui.adsabs.harvard.edu/abs/1996A&AS..117..393B)
- en: Bochkovskiy et al. (2020) Bochkovskiy A., Wang C.-Y., Liao H.-Y. M., 2020, [arXiv
    e-prints](http://dx.doi.org/10.48550/arXiv.2004.10934), [p. arXiv:2004.10934](https://ui.adsabs.harvard.edu/abs/2020arXiv200410934B)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bochkovskiy 等人 (2020) Bochkovskiy A., Wang C.-Y., Liao H.-Y. M., 2020, [arXiv
    预印本](http://dx.doi.org/10.48550/arXiv.2004.10934), [p. arXiv:2004.10934](https://ui.adsabs.harvard.edu/abs/2020arXiv200410934B)
- en: Bosch et al. (2018) Bosch J., et al., 2018, [Publications of the Astronomical
    Society of Japan](http://dx.doi.org/10.1093/pasj/psx080), 70, S5
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bosch 等人 (2018) Bosch J., 等人, 2018, [日本天文学会出版物](http://dx.doi.org/10.1093/pasj/psx080),
    70, S5
- en: Boucaud et al. (2020) Boucaud A., et al., 2020, [MNRAS](http://dx.doi.org/10.1093/mnras/stz3056),
    [491, 2481](https://ui.adsabs.harvard.edu/abs/2020MNRAS.491.2481B)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boucaud 等人 (2020) Boucaud A., 等人, 2020, [MNRAS](http://dx.doi.org/10.1093/mnras/stz3056),
    [491, 2481](https://ui.adsabs.harvard.edu/abs/2020MNRAS.491.2481B)
- en: Bretonnière et al. (2021) Bretonnière H., Boucaud A., Huertas-Company M., 2021,
    Probabilistic segmentation of overlapping galaxies for large cosmological surveys,
    [doi:10.48550/arXiv.2111.15455](http://dx.doi.org/10.48550/arXiv.2111.15455),
    [http://arxiv.org/abs/2111.15455](http://arxiv.org/abs/2111.15455)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bretonnière 等人 (2021) Bretonnière H., Boucaud A., Huertas-Company M., 2021,
    大规模宇宙学调查中重叠星系的概率分割, [doi:10.48550/arXiv.2111.15455](http://dx.doi.org/10.48550/arXiv.2111.15455),
    [http://arxiv.org/abs/2111.15455](http://arxiv.org/abs/2111.15455)
- en: Burke et al. (2019) Burke C. J., Aleo P. D., Chen Y.-C., Liu X., Peterson J. R.,
    Sembroski G. H., Lin J. Y.-Y., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz2845),
    [490, 3952](https://ui.adsabs.harvard.edu/abs/2019MNRAS.490.3952B)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burke 等人 (2019) Burke C. J., Aleo P. D., Chen Y.-C., Liu X., Peterson J. R.,
    Sembroski G. H., Lin J. Y.-Y., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz2845),
    [490, 3952](https://ui.adsabs.harvard.edu/abs/2019MNRAS.490.3952B)
- en: Cai & Vasconcelos (2018) Cai Z., Vasconcelos N., 2018, in Proceedings of the
    IEEE conference on computer vision and pattern recognition. pp 6154–6162
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai & Vasconcelos (2018) Cai Z., Vasconcelos N., 2018, 见于 IEEE 计算机视觉与模式识别会议论文集.
    pp 6154–6162
- en: Caron et al. (2021) Caron M., Touvron H., Misra I., Jégou H., Mairal J., Bojanowski
    P., Joulin A., 2021, in Proceedings of the IEEE/CVF international conference on
    computer vision. pp 9650–9660
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caron 等人 (2021) Caron M., Touvron H., Misra I., Jégou H., Mairal J., Bojanowski
    P., Joulin A., 2021, 见于 IEEE/CVF 国际计算机视觉会议论文集. pp 9650–9660
- en: Cheng (2017) Cheng J., 2017, PhD thesis, Purdue University
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng (2017) Cheng J., 2017, 博士学位论文, 普渡大学
- en: Cheng et al. (2022) Cheng B., Parkhi O., Kirillov A., 2022, in Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp 2617–2626
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等（2022）Cheng B.，Parkhi O.，Kirillov A.，2022，在 IEEE/CVF 计算机视觉与模式识别会议论文集中，页
    2617–2626
- en: Dai et al. (2017) Dai J., Qi H., Xiong Y., Li Y., Zhang G., Hu H., Wei Y., 2017,
    [arXiv e-prints](http://dx.doi.org/10.48550/arXiv.1703.06211), [p. arXiv:1703.06211](https://ui.adsabs.harvard.edu/abs/2017arXiv170306211D)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2017）Dai J.，Qi H.，Xiong Y.，Li Y.，Zhang G.，Hu H.，Wei Y.，2017，[arXiv 预印本](http://dx.doi.org/10.48550/arXiv.1703.06211)，[p.
    arXiv:1703.06211](https://ui.adsabs.harvard.edu/abs/2017arXiv170306211D)
- en: Dark Energy Survey Collaboration et al. (2016) Dark Energy Survey Collaboration
    et al., 2016, [MNRAS](http://dx.doi.org/10.1093/mnras/stw641), [460, 1270](https://ui.adsabs.harvard.edu/abs/2016MNRAS.460.1270D)
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dark Energy Survey Collaboration 等（2016）Dark Energy Survey Collaboration 等，2016，[MNRAS](http://dx.doi.org/10.1093/mnras/stw641)，[460,
    1270](https://ui.adsabs.harvard.edu/abs/2016MNRAS.460.1270D)
- en: Dawson et al. (2016) Dawson W. A., Schneider M. D., Tyson J. A., Jee M. J.,
    2016, [ApJ](http://dx.doi.org/10.3847/0004-637X/816/1/11), [816, 11](http://adsabs.harvard.edu/abs/2016ApJ...816...11D)
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dawson 等（2016）Dawson W. A.，Schneider M. D.，Tyson J. A.，Jee M. J.，2016，[ApJ](http://dx.doi.org/10.3847/0004-637X/816/1/11)，[816,
    11](http://adsabs.harvard.edu/abs/2016ApJ...816...11D)
- en: Deng et al. (2009) Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L.,
    2009, in 2009 IEEE Conference on Computer Vision and Pattern Recognition. pp 248–255,
    [doi:10.1109/CVPR.2009.5206848](http://dx.doi.org/10.1109/CVPR.2009.5206848)
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等（2009）Deng J.，Dong W.，Socher R.，Li L.-J.，Li K.，Fei-Fei L.，2009，在 2009
    年 IEEE 计算机视觉与模式识别大会上，页 248–255，[doi:10.1109/CVPR.2009.5206848](http://dx.doi.org/10.1109/CVPR.2009.5206848)
- en: Dey et al. (2019) Dey A., et al., 2019, [AJ](http://dx.doi.org/10.3847/1538-3881/ab089d),
    [157, 168](https://ui.adsabs.harvard.edu/abs/2019AJ....157..168D)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dey 等（2019）Dey A. 等，2019，[AJ](http://dx.doi.org/10.3847/1538-3881/ab089d)，[157,
    168](https://ui.adsabs.harvard.edu/abs/2019AJ....157..168D)
- en: Dosovitskiy et al. (2020) Dosovitskiy A., et al., 2020, [arXiv e-prints](http://dx.doi.org/10.48550/arXiv.2010.11929),
    [p. arXiv:2010.11929](https://ui.adsabs.harvard.edu/abs/2020arXiv201011929D)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等（2020）Dosovitskiy A. 等，2020，[arXiv 预印本](http://dx.doi.org/10.48550/arXiv.2010.11929)，[p.
    arXiv:2010.11929](https://ui.adsabs.harvard.edu/abs/2020arXiv201011929D)
- en: Fan et al. (2021) Fan H., Xiong B., Mangalam K., Li Y., Yan Z., Malik J., Feichtenhofer
    C., 2021, in Proceedings of the IEEE/CVF international conference on computer
    vision. pp 6824–6835
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等（2021）Fan H.，Xiong B.，Mangalam K.，Li Y.，Yan Z.，Malik J.，Feichtenhofer C.，2021，在
    IEEE/CVF 国际计算机视觉大会论文集中，页 6824–6835
- en: Flaugher et al. (2015) Flaugher B., et al., 2015, [AJ](http://dx.doi.org/10.1088/0004-6256/150/5/150),
    [150, 150](https://ui.adsabs.harvard.edu/abs/2015AJ....150..150F)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flaugher 等（2015）Flaugher B. 等，2015，[AJ](http://dx.doi.org/10.1088/0004-6256/150/5/150)，[150,
    150](https://ui.adsabs.harvard.edu/abs/2015AJ....150..150F)
- en: Girshick (2015) Girshick R., 2015, in 2015 IEEE International Conference on
    Computer Vision (ICCV). pp 1440–1448, [doi:10.1109/ICCV.2015.169](http://dx.doi.org/10.1109/ICCV.2015.169)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girshick（2015）Girshick R.，2015，在 2015 年 IEEE 国际计算机视觉大会（ICCV）上，页 1440–1448，[doi:10.1109/ICCV.2015.169](http://dx.doi.org/10.1109/ICCV.2015.169)
- en: González et al. (2018) González R. E., Muñoz R. P., Hernández C. A., 2018, [Astronomy
    and Computing](http://dx.doi.org/10.1016/j.ascom.2018.09.004), [25, 103](https://ui.adsabs.harvard.edu/abs/2018A&C....25..103G)
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: González 等（2018）González R. E.，Muñoz R. P.，Hernández C. A.，2018，[天文学与计算](http://dx.doi.org/10.1016/j.ascom.2018.09.004)，[25,
    103](https://ui.adsabs.harvard.edu/abs/2018A&C....25..103G)
- en: Grogin et al. (2011) Grogin N. A., et al., 2011, [ApJS](http://dx.doi.org/10.1088/0067-0049/197/2/35),
    [197, 35](https://ui.adsabs.harvard.edu/abs/2011ApJS..197...35G)
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grogin 等（2011）Grogin N. A. 等，2011，[ApJS](http://dx.doi.org/10.1088/0067-0049/197/2/35)，[197,
    35](https://ui.adsabs.harvard.edu/abs/2011ApJS..197...35G)
- en: Hausen & Robertson (2020) Hausen R., Robertson B., 2020, [ApJS](http://dx.doi.org/10.3847/1538-4365/ab8868),
    248, 20
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hausen & Robertson（2020）Hausen R.，Robertson B.，2020，[ApJS](http://dx.doi.org/10.3847/1538-4365/ab8868)，248，20
- en: He et al. (2016) He K., Zhang X., Ren S., Sun J., 2016, 2016 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pp 770–778
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2016）He K.，Zhang X.，Ren S.，Sun J.，2016，2016 年 IEEE 计算机视觉与模式识别大会（CVPR），页
    770–778
- en: He et al. (2017) He K., Gkioxari G., Dollár P., Girshick R., 2017, in Proceedings
    of the IEEE international conference on computer vision. pp 2961–2969
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2017）He K.，Gkioxari G.，Dollár P.，Girshick R.，2017，在 IEEE 国际计算机视觉大会论文集中，页
    2961–2969
- en: He et al. (2021) He Z., Qiu B., Luo A.-L., Shi J., Kong X., Jiang X., 2021,
    [Monthly Notices of the Royal Astronomical Society](http://dx.doi.org/10.1093/mnras/stab2243),
    508, 2039
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2021）He Z.，Qiu B.，Luo A.-L.，Shi J.，Kong X.，Jiang X.，2021，[皇家天文学会月刊](http://dx.doi.org/10.1093/mnras/stab2243)，508，2039
- en: Hemmati et al. (2022) Hemmati S., et al., 2022, [ApJ](http://dx.doi.org/10.3847/1538-4357/aca1b8),
    [941, 141](https://ui.adsabs.harvard.edu/abs/2022ApJ...941..141H)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hemmati等（2022）Hemmati S.等，2022年，[ApJ](http://dx.doi.org/10.3847/1538-4357/aca1b8)，[941,
    141](https://ui.adsabs.harvard.edu/abs/2022ApJ...941..141H)
- en: Huertas-Company & Lanusse (2023) Huertas-Company M., Lanusse F., 2023, [Publ.
    Astron. Soc. Australia](http://dx.doi.org/10.1017/pasa.2022.55), [40, e001](https://ui.adsabs.harvard.edu/abs/2023PASA...40....1H)
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huertas-Company & Lanusse（2023）Huertas-Company M.，Lanusse F.，2023年，[Publ. Astron.
    Soc. Australia](http://dx.doi.org/10.1017/pasa.2022.55)，[40, e001](https://ui.adsabs.harvard.edu/abs/2023PASA...40....1H)
- en: Hunter (2007) Hunter J. D., 2007, [Computing in Science & Engineering](http://dx.doi.org/10.1109/MCSE.2007.55),
    9, 90
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hunter（2007）Hunter J. D.，2007年，[Computing in Science & Engineering](http://dx.doi.org/10.1109/MCSE.2007.55)，9，90
- en: Ibrahim et al. (2020) Ibrahim M. R., Haworth J., Cheng T., 2020, [Cities](http://dx.doi.org/https://doi.org/10.1016/j.cities.2019.102481),
    96, 102481
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ibrahim等（2020）Ibrahim M. R.，Haworth J.，Cheng T.，2020年，[Cities](http://dx.doi.org/https://doi.org/10.1016/j.cities.2019.102481)，96，102481
- en: Ivezić et al. (2019) Ivezić Ž., et al., 2019, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c),
    [873, 111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivezić等（2019）Ivezić Ž.等，2019年，[ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c)，[873,
    111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
- en: Jarvis & Tyson (1981) Jarvis J. F., Tyson J. A., 1981, [AJ](http://dx.doi.org/10.1086/112907),
    [86, 476](https://ui.adsabs.harvard.edu/abs/1981AJ.....86..476J)
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jarvis & Tyson（1981）Jarvis J. F.，Tyson J. A.，1981年，[AJ](http://dx.doi.org/10.1086/112907)，[86,
    476](https://ui.adsabs.harvard.edu/abs/1981AJ.....86..476J)
- en: Kawanomoto et al. (2018) Kawanomoto S., et al., 2018, [PASJ](http://dx.doi.org/10.1093/pasj/psy056),
    70, 66
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kawanomoto等（2018）Kawanomoto S.等，2018年，[PASJ](http://dx.doi.org/10.1093/pasj/psy056)，70，66
- en: Kindratenko et al. (2020) Kindratenko V., et al., 2020, in Practice and Experience
    in Advanced Research Computing. PEARC ’20. Association for Computing Machinery,
    New York, NY, USA, p. 41–48, [doi:10.1145/3311790.3396649](http://dx.doi.org/10.1145/3311790.3396649)
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kindratenko等（2020）Kindratenko V.等，2020年，在《高级研究计算实践与经验》中，PEARC ’20，计算机协会，纽约，NY，美国，第41–48页，[doi:10.1145/3311790.3396649](http://dx.doi.org/10.1145/3311790.3396649)
- en: Koekemoer et al. (2011) Koekemoer A. M., et al., 2011, [ApJS](http://dx.doi.org/10.1088/0067-0049/197/2/36),
    [197, 36](https://ui.adsabs.harvard.edu/abs/2011ApJS..197...36K)
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koekemoer等（2011）Koekemoer A. M.等，2011年，[ApJS](http://dx.doi.org/10.1088/0067-0049/197/2/36)，[197,
    36](https://ui.adsabs.harvard.edu/abs/2011ApJS..197...36K)
- en: Kroupa (2001) Kroupa P., 2001, [MNRAS](http://dx.doi.org/10.1046/j.1365-8711.2001.04022.x),
    [322, 231](https://ui.adsabs.harvard.edu/abs/2001MNRAS.322..231K)
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kroupa（2001）Kroupa P.，2001年，[MNRAS](http://dx.doi.org/10.1046/j.1365-8711.2001.04022.x)，[322,
    231](https://ui.adsabs.harvard.edu/abs/2001MNRAS.322..231K)
- en: LSST Dark Energy Science Collaboration (LSST DESC) et al. (2021) LSST Dark Energy
    Science Collaboration (LSST DESC) et al., 2021, [ApJS](http://dx.doi.org/10.3847/1538-4365/abd62c),
    [253, 31](https://ui.adsabs.harvard.edu/abs/2021ApJS..253...31L)
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSST暗能量科学合作组（LSST DESC）等（2021）LSST暗能量科学合作组（LSST DESC）等，2021年，[ApJS](http://dx.doi.org/10.3847/1538-4365/abd62c)，[253,
    31](https://ui.adsabs.harvard.edu/abs/2021ApJS..253...31L)
- en: Leauthaud et al. (2007) Leauthaud A., et al., 2007, [ApJS](http://dx.doi.org/10.1086/516598),
    [172, 219](https://ui.adsabs.harvard.edu/abs/2007ApJS..172..219L)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leauthaud等（2007）Leauthaud A.等，2007年，[ApJS](http://dx.doi.org/10.1086/516598)，[172,
    219](https://ui.adsabs.harvard.edu/abs/2007ApJS..172..219L)
- en: Li et al. (2022) Li Y., Wu C.-Y., Fan H., Mangalam K., Xiong B., Malik J., Feichtenhofer
    C., 2022, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition. pp 4804–4814
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2022）Li Y.，Wu C.-Y.，Fan H.，Mangalam K.，Xiong B.，Malik J.，Feichtenhofer C.，2022年，在IEEE/CVF计算机视觉与模式识别会议论文集中，第4804–4814页
- en: Lin et al. (2014) Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan
    D., Dollár P., Zitnick C. L., 2014, in European Conference on Computer Vision
    (ECCV). Zürich
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2014）Lin T.-Y.，Maire M.，Belongie S.，Hays J.，Perona P.，Ramanan D.，Dollár
    P.，Zitnick C. L.，2014年，在欧洲计算机视觉会议（ECCV）中。苏黎世
- en: Lin et al. (2017) Lin T.-Y., Dollár P., Girshick R. B., He K., Hariharan B.,
    Belongie S. J., 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pp 936–944
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2017）Lin T.-Y.，Dollár P.，Girshick R. B.，He K.，Hariharan B.，Belongie S.
    J.，2017年，2017 IEEE计算机视觉与模式识别会议（CVPR），第936–944页
- en: Lintott et al. (2011) Lintott C., et al., 2011, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2010.17432.x),
    [410, 166](https://ui.adsabs.harvard.edu/abs/2011MNRAS.410..166L)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lintott等（2011）Lintott C.等，2011年，[MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2010.17432.x)，[410,
    166](https://ui.adsabs.harvard.edu/abs/2011MNRAS.410..166L)
- en: Liu et al. (2021) Liu Z., Lin Y., Cao Y., Hu H., Wei Y., Zhang Z., Lin S., Guo
    B., 2021, in Proceedings of the IEEE/CVF international conference on computer
    vision. pp 10012–10022
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2021）Liu Z.，Lin Y.，Cao Y.，Hu H.，Wei Y.，Zhang Z.，Lin S.，Guo B.，2021年，在IEEE/CVF国际计算机视觉会议论文集中，第10012–10022页
- en: Lupton et al. (2004) Lupton R., Blanton M. R., Fekete G., Hogg D. W., O’Mullane
    W., Szalay A., Wherry N., 2004, [PASP](http://dx.doi.org/10.1086/382245), [116,
    133](https://ui.adsabs.harvard.edu/abs/2004PASP..116..133L)
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lupton et al. (2004) Lupton R., Blanton M. R., Fekete G., Hogg D. W., O’Mullane
    W., Szalay A., Wherry N., 2004, [PASP](http://dx.doi.org/10.1086/382245), [116,
    133](https://ui.adsabs.harvard.edu/abs/2004PASP..116..133L)
- en: Madau & Dickinson (2014) Madau P., Dickinson M., 2014, [ARA&A](http://dx.doi.org/10.1146/annurev-astro-081811-125615),
    [52, 415](https://ui.adsabs.harvard.edu/abs/2014ARA&A..52..415M)
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madau & Dickinson (2014) Madau P., Dickinson M., 2014, [ARA&A](http://dx.doi.org/10.1146/annurev-astro-081811-125615),
    [52, 415](https://ui.adsabs.harvard.edu/abs/2014ARA&A..52..415M)
- en: Mahabal et al. (2019) Mahabal A., et al., 2019, [PASP](http://dx.doi.org/10.1088/1538-3873/aaf3fa),
    [131, 038002](https://ui.adsabs.harvard.edu/abs/2019PASP..131c8002M)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahabal et al. (2019) Mahabal A., et al., 2019, [PASP](http://dx.doi.org/10.1088/1538-3873/aaf3fa),
    [131, 038002](https://ui.adsabs.harvard.edu/abs/2019PASP..131c8002M)
- en: Malanchev et al. (2021) Malanchev K. L., et al., 2021, [MNRAS](http://dx.doi.org/10.1093/mnras/stab316),
    [502, 5147](https://ui.adsabs.harvard.edu/abs/2021MNRAS.502.5147M)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malanchev et al. (2021) Malanchev K. L., et al., 2021, [MNRAS](http://dx.doi.org/10.1093/mnras/stab316),
    [502, 5147](https://ui.adsabs.harvard.edu/abs/2021MNRAS.502.5147M)
- en: Melchior et al. (2018) Melchior P., Moolekamp F., Jerdee M., Armstrong R., Sun
    A.-L., Bosch J., Lupton R., 2018, [Astronomy and Computing](http://dx.doi.org/10.1016/j.ascom.2018.07.001),
    24, 129
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Melchior et al. (2018) Melchior P., Moolekamp F., Jerdee M., Armstrong R., Sun
    A.-L., Bosch J., Lupton R., 2018, [天文学与计算](http://dx.doi.org/10.1016/j.ascom.2018.07.001),
    24, 129
- en: Melchior et al. (2021) Melchior P., Joseph R., Sanchez J., MacCrann N., Gruen
    D., 2021, [Nat Rev Phys](http://dx.doi.org/10.1038/s42254-021-00353-y), 3, 712
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Melchior et al. (2021) Melchior P., Joseph R., Sanchez J., MacCrann N., Gruen
    D., 2021, [自然综述物理](http://dx.doi.org/10.1038/s42254-021-00353-y), 3, 712
- en: Miller & Hall (2021) Miller A. A., Hall X. J., 2021, [PASP](http://dx.doi.org/10.1088/1538-3873/abf038),
    [133, 054502](https://ui.adsabs.harvard.edu/abs/2021PASP..133e4502M)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miller & Hall (2021) Miller A. A., Hall X. J., 2021, [PASP](http://dx.doi.org/10.1088/1538-3873/abf038),
    [133, 054502](https://ui.adsabs.harvard.edu/abs/2021PASP..133e4502M)
- en: Miyazaki et al. (2017) Miyazaki S., et al., 2017, [Publications of the Astronomical
    Society of Japan](http://dx.doi.org/10.1093/pasj/psx063), 70
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miyazaki et al. (2017) Miyazaki S., et al., 2017, [日本天文学会出版物](http://dx.doi.org/10.1093/pasj/psx063),
    70
- en: Morganson et al. (2018) Morganson E., et al., 2018, [PASP](http://dx.doi.org/10.1088/1538-3873/aab4ef),
    [130, 074501](https://ui.adsabs.harvard.edu/abs/2018PASP..130g4501M)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Morganson et al. (2018) Morganson E., et al., 2018, [PASP](http://dx.doi.org/10.1088/1538-3873/aab4ef),
    [130, 074501](https://ui.adsabs.harvard.edu/abs/2018PASP..130g4501M)
- en: Muyskens et al. (2022) Muyskens A. L., Goumiri I. R., Priest B. W., Schneider
    M. D., Armstrong R. E., Bernstein J., Dana R., 2022, [AJ](http://dx.doi.org/10.3847/1538-3881/ac4e93),
    [163, 148](https://ui.adsabs.harvard.edu/abs/2022AJ....163..148M)
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muyskens et al. (2022) Muyskens A. L., Goumiri I. R., Priest B. W., Schneider
    M. D., Armstrong R. E., Bernstein J., Dana R., 2022, [AJ](http://dx.doi.org/10.3847/1538-3881/ac4e93),
    [163, 148](https://ui.adsabs.harvard.edu/abs/2022AJ....163..148M)
- en: Oquab et al. (2023) Oquab M., et al., 2023, [arXiv e-prints](http://dx.doi.org/10.48550/arXiv.2304.07193),
    [p. arXiv:2304.07193](https://ui.adsabs.harvard.edu/abs/2023arXiv230407193O)
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oquab et al. (2023) Oquab M., et al., 2023, [arXiv e-prints](http://dx.doi.org/10.48550/arXiv.2304.07193),
    [p. arXiv:2304.07193](https://ui.adsabs.harvard.edu/abs/2023arXiv230407193O)
- en: Pavel et al. (2022) Pavel M. I., Tan S. Y., Abdullah A., 2022, [Applied Sciences](http://dx.doi.org/10.3390/app12146831),
    12
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavel et al. (2022) Pavel M. I., Tan S. Y., Abdullah A., 2022, [应用科学](http://dx.doi.org/10.3390/app12146831),
    12
- en: Peterson et al. (2015) Peterson J. R., et al., 2015, [ApJS](http://dx.doi.org/10.1088/0067-0049/218/1/14),
    [218, 14](https://ui.adsabs.harvard.edu/abs/2015ApJS..218...14P)
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peterson et al. (2015) Peterson J. R., et al., 2015, [ApJS](http://dx.doi.org/10.1088/0067-0049/218/1/14),
    [218, 14](https://ui.adsabs.harvard.edu/abs/2015ApJS..218...14P)
- en: Price-Whelan et al. (2018) Price-Whelan A. M., et al., 2018, [AJ](http://dx.doi.org/10.3847/1538-3881/aabc4f),
    [156, 123](https://ui.adsabs.harvard.edu/#abs/2018AJ....156..123T)
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Price-Whelan et al. (2018) Price-Whelan A. M., et al., 2018, [AJ](http://dx.doi.org/10.3847/1538-3881/aabc4f),
    [156, 123](https://ui.adsabs.harvard.edu/#abs/2018AJ....156..123T)
- en: Reiman & Göhre (2019) Reiman D. M., Göhre B. E., 2019, [Monthly Notices of the
    Royal Astronomical Society](http://dx.doi.org/10.1093/mnras/stz575), 485, 2617
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reiman & Göhre (2019) Reiman D. M., Göhre B. E., 2019, [皇家天文学会月报](http://dx.doi.org/10.1093/mnras/stz575),
    485, 2617
- en: Ross et al. (2011) Ross A. J., et al., 2011, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2011.19351.x),
    [417, 1350](https://ui.adsabs.harvard.edu/abs/2011MNRAS.417.1350R)
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross et al. (2011) Ross A. J., et al., 2011, [MNRAS](http://dx.doi.org/10.1111/j.1365-2966.2011.19351.x),
    [417, 1350](https://ui.adsabs.harvard.edu/abs/2011MNRAS.417.1350R)
- en: Russeil et al. (2022) Russeil E., Ishida E. E. O., Le Montagner R., Peloton
    J., Moller A., 2022, [arXiv e-prints](http://dx.doi.org/10.48550/arXiv.2211.10987),
    [p. arXiv:2211.10987](https://ui.adsabs.harvard.edu/abs/2022arXiv221110987R)
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russeil et al. (2022) Russeil E., Ishida E. E. O., Le Montagner R., Peloton
    J., Moller A., 2022, [arXiv 电子预印本](http://dx.doi.org/10.48550/arXiv.2211.10987),
    [p. arXiv:2211.10987](https://ui.adsabs.harvard.edu/abs/2022arXiv221110987R)
- en: Scoville et al. (2007) Scoville N., et al., 2007, [ApJS](http://dx.doi.org/10.1086/516585),
    [172, 1](https://ui.adsabs.harvard.edu/abs/2007ApJS..172....1S)
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scoville et al. (2007) Scoville N., et al., 2007, [ApJS](http://dx.doi.org/10.1086/516585),
    [172, 1](https://ui.adsabs.harvard.edu/abs/2007ApJS..172....1S)
- en: Spergel et al. (2013) Spergel D., et al., 2013, arXiv e-prints, [p. arXiv:1305.5422](https://ui.adsabs.harvard.edu/abs/2013arXiv1305.5422S)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spergel et al. (2013) Spergel D., et al., 2013, arXiv 电子预印本, [p. arXiv:1305.5422](https://ui.adsabs.harvard.edu/abs/2013arXiv1305.5422S)
- en: Tachibana & Miller (2018) Tachibana Y., Miller A. A., 2018, [PASP](http://dx.doi.org/10.1088/1538-3873/aae3d9),
    [130, 128001](https://ui.adsabs.harvard.edu/abs/2018PASP..130l8001T)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tachibana & Miller (2018) Tachibana Y., Miller A. A., 2018, [PASP](http://dx.doi.org/10.1088/1538-3873/aae3d9),
    [130, 128001](https://ui.adsabs.harvard.edu/abs/2018PASP..130l8001T)
- en: 'Tan et al. (2018) Tan C., Sun F., Kong T., Zhang W., Yang C., Liu C., 2018,
    A Survey on Deep Transfer Learning: 27th International Conference on Artificial
    Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part III. pp
    270–279, [doi:10.1007/978-3-030-01424-7_27](http://dx.doi.org/10.1007/978-3-030-01424-7_27)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan et al. (2018) Tan C., Sun F., Kong T., Zhang W., Yang C., Liu C., 2018,
    深度迁移学习综述: 第27届国际人工神经网络会议, 希腊罗德岛, 2018年10月4-7日, 会议录, 第三部分, 页270–279, [doi:10.1007/978-3-030-01424-7_27](http://dx.doi.org/10.1007/978-3-030-01424-7_27)'
- en: Wu et al. (2019) Wu Y., Kirillov A., Massa F., Lo W.-Y., Girshick R., 2019,
    Detectron2, [https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2)
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2019) Wu Y., Kirillov A., Massa F., Lo W.-Y., Girshick R., 2019,
    Detectron2, [https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2)
- en: Xie et al. (2017) Xie S., Girshick R., Dollár P., Tu Z., He K., 2017, in Proceedings
    of the IEEE conference on computer vision and pattern recognition. pp 1492–1500
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2017) Xie S., Girshick R., Dollár P., Tu Z., He K., 2017, 在 IEEE
    计算机视觉与模式识别会议录中, 页1492–1500
- en: Zhou et al. (2021) Zhou S. K., et al., 2021, Proceedings of the IEEE, 109, 820
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2021) Zhou S. K., et al., 2021, IEEE 会议录, 109, 820
- en: Appendix A DECam results
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A DECam 结果
- en: For a baseline comparison of network performances, we utilize the PhoSim dataset
    created and used by Burke et al. ([2019](#bib.bib14)). We refer to the earlier
    work for a full description, but provide a brief summary here. Crowded fields
    as taken with DECam are produced using the Photon Simulator code (Peterson et al.,
    [2015](#bib.bib63)). Simulations account for equipment optics (Cheng, [2017](#bib.bib17)),
    telescope options (Flaugher et al., [2015](#bib.bib26)) and atmospheric conditions.
    Spiral, elliptical and irregular galaxies are produced by sampling three-dimensional
    sersic profiles with additional parameters for extra morphological features. Stars
    are modeled as point sources and created following an initial mass function from
    (Kroupa, [2001](#bib.bib43)). For both stars and galaxies, SEDs and metallicities
    are assigned based on physical models. Cosmic star formation history (Madau &
    Dickinson, [2014](#bib.bib52)) is used to assign galaxy number density and population,
    while the distribution of stars is based on galactic latitude. To simulated crowded
    fields, the galactic overdensity is boosted by a factor of 4\. A 512x512 pixel²
    image is produced with g,r, and z DECam bands. Integration time and magnitude
    ranges are assigned to roughly correspond to DECaLs DR7 coadds (Dey et al., [2019](#bib.bib23)).
    In order to assign object masks, a g-band image without background is produced
    for every object in the field. The PSF is configured to $\sim$1 arcsec. In total,
    1000 images are produced for our training set, while an additional 250 are used
    for validation and another 50 as our test set for evaluation. Each image contains
    roughly 150 objects.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对网络性能进行基准比较，我们利用了Burke等人创建和使用的PhoSim数据集（[2019](#bib.bib14)）。我们参考了早期的工作以获得完整描述，但在此提供简要总结。使用Photon
    Simulator代码（Peterson等，[2015](#bib.bib63)）生成了由DECam拍摄的拥挤场景。模拟考虑了设备光学（Cheng，[2017](#bib.bib17)）、望远镜选项（Flaugher等，[2015](#bib.bib26)）和大气条件。螺旋、椭圆和不规则星系通过采样三维Sersic模型，并添加额外参数以展示更多形态特征来生成。恒星被建模为点源，并按照（Kroupa，[2001](#bib.bib43)）的初始质量函数生成。对于恒星和星系，SEDs和金属丰度基于物理模型分配。使用宇宙星形成历史（Madau
    & Dickinson，[2014](#bib.bib52)）来分配星系的数量密度和种群，而恒星的分布则基于银河纬度。为了模拟拥挤的场景，银河超密度被提高了4倍。生成了一个512x512像素²的图像，包含g、r和z
    DECam波段。积分时间和亮度范围大致对应于DECaLs DR7合成图（Dey等，[2019](#bib.bib23)）。为了分配对象掩码，为场中的每个对象生成了一张没有背景的g波段图像。PSF配置为$\sim$1角秒。总共生成了1000张图像作为我们的训练集，另外250张用于验证，还有50张作为测试集进行评估。每张图像包含大约150个对象。
- en: 'Here we present the results of two runs on the simulated DECam data, using
    the R101fpn and MViTv2 backbones. These backbones are chosen to compare the performance
    of convolutional versus transformer-based architectures. We use the same contrast
    scalings that were applied to the HSC data, but change the stretch parameter to
    100 and Q to 10 for the Lupton and Lupton high-contrast scalings. The dynamic
    range of the simulated data is different from the HSC data, so the adjustment
    is done to make galaxy features more distinguishable. AP scores for each configuration
    are shown in Table [6](#A1.T6 "Table 6 ‣ Appendix A DECam results ‣ Detection,
    Instance Segmentation, and Classification for Astronomical Surveys with Deep Learning
    (DeepDISC): Detectron2 Implementation and Demonstration with Hyper Suprime-Cam
    Data"). We adapt the ranges for Small, Medium, and Large bounding box sizes to
    match those used in Burke et al. ([2019](#bib.bib14)). Overall, we find that a
    Lupton scaling with a ResNet backbone works best for this dataset, giving the
    highest AP scores for almost all categories. This is in contrast to the results
    on HSC data, however we note that a transformer backbone is again more robust
    to contrast scalings. Although Burke et al. ([2019](#bib.bib14)) use a z-scale
    with a R101fpn backbone, our results are different as we use a slightly altered
    z-scale formula in that we rescale each band by a constant $\sigma_{I}$ rather
    than a per-band scale factor. This alteration makes galaxy classification performance
    worse (AP=29.80 compared to AP=49.6) but star classification performance better
    (AP=54.32 compared to AP=48.6). The large drop in galaxy AP suggests that the
    R101fpn backbone is very sensitive to the contrast scaling. All other configurations
    result in better galaxy and star AP scores than the Burke et al. ([2019](#bib.bib14))
    results. Our AP scores for Small objects are lower, but Medium and Large are much
    higher. For size categories, we use the same size definitions as in Burke et al.
    ([2019](#bib.bib14)), but compute an average AP for all IOU thresholds, rather
    than the AP at only the lowest threshold IOU=0.5\. Thus, our results can be thought
    of as a kind of lower bound, as AP score tends to increase with a lower IOU threshold.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了使用 R101fpn 和 MViTv2 主干网络对模拟 DECam 数据的两次运行结果。这些主干网络的选择是为了比较卷积架构与基于变换器的架构的性能。我们使用了与
    HSC 数据相同的对比度缩放，但将伸缩参数更改为 100，并将 Q 更改为 10 以适用于 Lupton 和 Lupton 高对比度缩放。模拟数据的动态范围与
    HSC 数据不同，因此进行了调整以使星系特征更具辨识度。每个配置的 AP 分数见表格 [6](#A1.T6 "表 6 ‣ 附录 A DECam 结果 ‣ 使用深度学习
    (DeepDISC) 的天文调查检测、实例分割和分类：Detectron2 实现和 Hyper Suprime-Cam 数据演示")。我们将 Small、Medium
    和 Large 边界框大小的范围调整为与 Burke 等人 ([2019](#bib.bib14)) 使用的范围匹配。总体而言，我们发现使用 ResNet
    主干网络的 Lupton 缩放对这个数据集效果最佳，几乎所有类别的 AP 分数都最高。这与 HSC 数据的结果形成对比，然而我们注意到变换器主干网络对对比度缩放的鲁棒性更强。虽然
    Burke 等人 ([2019](#bib.bib14)) 使用了带有 R101fpn 主干网络的 z-scale，我们的结果有所不同，因为我们使用了稍微修改的
    z-scale 公式，即通过常数 $\sigma_{I}$ 重新缩放每个波段，而不是每波段缩放因子。这种改变使得星系分类性能变差（AP=29.80 对比 AP=49.6），但星类分类性能提升（AP=54.32
    对比 AP=48.6）。星系 AP 的大幅下降表明 R101fpn 主干网络对对比度缩放非常敏感。所有其他配置的星系和星类 AP 分数均优于 Burke 等人
    ([2019](#bib.bib14)) 的结果。我们对 Small 物体的 AP 分数较低，但 Medium 和 Large 的分数则高得多。对于尺寸类别，我们使用与
    Burke 等人 ([2019](#bib.bib14)) 相同的尺寸定义，但计算所有 IOU 阈值的平均 AP，而不是仅计算最低阈值 IOU=0.5 的
    AP。因此，我们的结果可以视为一种下限，因为 AP 分数通常随着 IOU 阈值的降低而增加。
- en: '|  |  | R101fpn | MViTv2 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  |  | R101fpn | MViTv2 |'
- en: '| Galaxies | Lupton | 65.8 | 62.5 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Galaxies | Lupton | 65.8 | 62.5 |'
- en: '| LuptonHC | 58.3 | 62.2 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 58.3 | 62.2 |'
- en: '| zscale | 29.8 | 60.4 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 29.8 | 60.4 |'
- en: '| Stars | Lupton | 70.1 | 68.0 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Stars | Lupton | 70.1 | 68.0 |'
- en: '| LuptonHC | 64.3 | 68.6 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 64.3 | 68.6 |'
- en: '| zscale | 54.3 | 66.4 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 54.3 | 66.4 |'
- en: '| Small | Lupton | 68.3 | 65.7 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Small | Lupton | 68.3 | 65.7 |'
- en: '| LuptonHC | 61.8 | 65.7 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 61.8 | 65.7 |'
- en: '| zscale | 42.3 | 63.8 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 42.3 | 63.8 |'
- en: '| Medium | Lupton | 36.1 | 31.6 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Medium | Lupton | 36.1 | 31.6 |'
- en: '| LuptonHC | 29.0 | 46.7 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 29.0 | 46.7 |'
- en: '| zscale | 16.3 | 31.7 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 16.3 | 31.7 |'
- en: '| Large | Lupton | 72.6 | 54.9 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| Large | Lupton | 72.6 | 54.9 |'
- en: '| LuptonHC | 49.1 | 65.2 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| LuptonHC | 49.1 | 65.2 |'
- en: '| zscale | 38.0 | 68.0 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| zscale | 38.0 | 68.0 |'
- en: 'Table 6: AP scores for DECam runs. Galaxy and star AP scores improve over the
    results of Burke et al. ([2019](#bib.bib14)) when different contrast scalings
    and backbones are applied. Transformer-based models are more robust to contrast
    scalings, consistent with results on real HSC data.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：DECam 运行的 AP 分数。当应用不同的对比度缩放和骨干网络时，星系和恒星的 AP 分数相较于 Burke 等人（[2019](#bib.bib14)）的结果有所提升。基于
    Transformer 的模型对对比度缩放更具鲁棒性，这与真实 HSC 数据上的结果一致。
