- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:38:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:38:38'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2306.15552] A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2306.15552] 《异构 HPC 平台上深度学习硬件加速器的调研》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.15552](https://ar5iv.labs.arxiv.org/html/2306.15552)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2306.15552](https://ar5iv.labs.arxiv.org/html/2306.15552)
- en: \SetWatermarkText
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \SetWatermarkText
- en: Preprint version \SetWatermarkScale0.6 \SetWatermarkColor[gray]0.8 \forestsetqtree/.style=for
    tree=parent anchor=south, child anchor=north,align=center,inner sep=0pt
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 预印本版本 \SetWatermarkScale0.6 \SetWatermarkColor[gray]0.8 \forestsetqtree/.style=for
    tree=parent anchor=south, child anchor=north,align=center,inner sep=0pt
- en: A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《异构 HPC 平台上深度学习硬件加速器的调研》
- en: Cristina Silvano [cristina.silvano@polimi.it](mailto:cristina.silvano@polimi.it)
    ,  Daniele Ielmini [daniele.ielmini@polimi.it](mailto:daniele.ielmini@polimi.it)
    ,  Fabrizio Ferrandi [fabrizio.ferrandi@polimi.it](mailto:fabrizio.ferrandi@polimi.it)
    ,  Leandro Fiorin [leandro.fiorin@polimi.it](mailto:leandro.fiorin@polimi.it)
    ,  Serena Curzel [serena.curzel@polimi.it](mailto:serena.curzel@polimi.it) Politecnico
    di MilanoItaly ,  Luca Benini [luca.benini@unibo.it](mailto:luca.benini@unibo.it)
    ,  Francesco Conti [f.conti@unibo.it](mailto:f.conti@unibo.it) ,  Angelo Garofalo
    [angelo.garofalo@unibo.it](mailto:angelo.garofalo@unibo.it) Università di BolognaItaly
    ,  Cristian Zambelli [cristian.zambelli@unife.it](mailto:cristian.zambelli@unife.it)
    ,  Enrico Calore [enrico.calore@fe.infn.it](mailto:enrico.calore@fe.infn.it) , 
    Sebastiano Fabio Schifano [sebastiano.fabio.schifano@unife.it](mailto:sebastiano.fabio.schifano@unife.it)
    Università degli Studi di FerraraItaly ,  Maurizio Palesi [maurizio.palesi@unict.it](mailto:maurizio.palesi@unict.it)
    ,  Giuseppe Ascia [giuseppe.ascia@unict.it](mailto:giuseppe.ascia@unict.it) , 
    Davide Patti [davide.patti@unict.it](mailto:davide.patti@unict.it) Università
    degli Studi di CataniaItaly ,  Stefania Perri [s.perri@unical.it](mailto:s.perri@unical.it)
    Università degli Studi della CalabriaItaly ,  Nicola Petra [nicola.petra@unina.it](mailto:nicola.petra@unina.it)
    ,  Davide De Caro [dadecaro@unina.it](mailto:dadecaro@unina.it) Università degli
    Studi di Napoli Federico IIItaly ,  Luciano Lavagno [luciano.lavagno@polito.it](mailto:luciano.lavagno@polito.it)
    ,  Teodoro Urso [teodoro.urso@polito.it](mailto:teodoro.urso@polito.it) Politecnico
    di TorinoItaly ,  Valeria Cardellini [cardellini@ing.uniroma2.it](mailto:cardellini@ing.uniroma2.it)
    ,  Gian Carlo Cardarilli [g.cardarilli@uniroma2.it](mailto:g.cardarilli@uniroma2.it)
    Università di Roma “Tor Vergata”Italy  and  Robert Birke [robert.birke@unito.it](mailto:robert.birke@unito.it)
    Università degli Studi di TorinoItaly(2023)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Cristina Silvano [cristina.silvano@polimi.it](mailto:cristina.silvano@polimi.it)
    ,  Daniele Ielmini [daniele.ielmini@polimi.it](mailto:daniele.ielmini@polimi.it)
    ,  Fabrizio Ferrandi [fabrizio.ferrandi@polimi.it](mailto:fabrizio.ferrandi@polimi.it)
    ,  Leandro Fiorin [leandro.fiorin@polimi.it](mailto:leandro.fiorin@polimi.it)
    ,  Serena Curzel [serena.curzel@polimi.it](mailto:serena.curzel@polimi.it) Politecnico
    di MilanoItaly ,  Luca Benini [luca.benini@unibo.it](mailto:luca.benini@unibo.it)
    ,  Francesco Conti [f.conti@unibo.it](mailto:f.conti@unibo.it) ,  Angelo Garofalo
    [angelo.garofalo@unibo.it](mailto:angelo.garofalo@unibo.it) Università di BolognaItaly
    ,  Cristian Zambelli [cristian.zambelli@unife.it](mailto:cristian.zambelli@unife.it)
    ,  Enrico Calore [enrico.calore@fe.infn.it](mailto:enrico.calore@fe.infn.it) ,  Sebastiano
    Fabio Schifano [sebastiano.fabio.schifano@unife.it](mailto:sebastiano.fabio.schifano@unife.it)
    Università degli Studi di FerraraItaly ,  Maurizio Palesi [maurizio.palesi@unict.it](mailto:maurizio.palesi@unict.it)
    ,  Giuseppe Ascia [giuseppe.ascia@unict.it](mailto:giuseppe.ascia@unict.it) ,  Davide
    Patti [davide.patti@unict.it](mailto:davide.patti@unict.it) Università degli Studi
    di CataniaItaly ,  Stefania Perri [s.perri@unical.it](mailto:s.perri@unical.it)
    Università degli Studi della CalabriaItaly ,  Nicola Petra [nicola.petra@unina.it](mailto:nicola.petra@unina.it)
    ,  Davide De Caro [dadecaro@unina.it](mailto:dadecaro@unina.it) Università degli
    Studi di Napoli Federico IIItaly ,  Luciano Lavagno [luciano.lavagno@polito.it](mailto:luciano.lavagno@polito.it)
    ,  Teodoro Urso [teodoro.urso@polito.it](mailto:teodoro.urso@polito.it) Politecnico
    di TorinoItaly ,  Valeria Cardellini [cardellini@ing.uniroma2.it](mailto:cardellini@ing.uniroma2.it)
    ,  Gian Carlo Cardarilli [g.cardarilli@uniroma2.it](mailto:g.cardarilli@uniroma2.it)
    Università di Roma “Tor Vergata”Italy  和  Robert Birke [robert.birke@unito.it](mailto:robert.birke@unito.it)
    Università degli Studi di TorinoItaly (2023)
- en: Abstract.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Recent trends in deep learning (DL) imposed hardware accelerators as the most
    viable solution for several classes of high-performance computing (HPC) applications
    such as image classification, computer vision, and speech recognition. This survey
    summarizes and classifies the most recent advances in designing DL accelerators
    suitable to reach the performance requirements of HPC applications. In particular,
    it highlights the most advanced approaches to support deep learning accelerations
    including not only GPU and TPU-based accelerators but also design-specific hardware
    accelerators such as FPGA-based and ASIC-based accelerators, Neural Processing
    Units, open hardware RISC-V-based accelerators and co-processors. The survey also
    describes accelerators based on emerging memory technologies and computing paradigms,
    such as 3D-stacked Processor-In-Memory, non-volatile memories (mainly, Resistive
    RAM and Phase Change Memories) to implement in-memory computing, Neuromorphic
    Processing Units, and accelerators based on Multi-Chip Modules. The survey classifies
    the most influential architectures and technologies proposed in the last years,
    with the purpose of offering the reader a comprehensive perspective in the rapidly
    evolving field of deep learning. Finally, it provides some insights into future
    challenges in DL accelerators such as quantum accelerators and photonics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）的最新趋势将硬件加速器视为多类高性能计算（HPC）应用的最可行解决方案，例如图像分类、计算机视觉和语音识别。本文综述并分类了设计适合满足HPC应用性能要求的DL加速器的最新进展。特别是，它强调了支持深度学习加速的最先进方法，包括不仅是基于GPU和TPU的加速器，还包括设计专用的硬件加速器，如基于FPGA和ASIC的加速器、神经处理单元、开放硬件RISC-V基加速器和协处理器。该综述还描述了基于新兴存储技术和计算范式的加速器，如3D堆叠的处理器内存、非易失性存储器（主要是电阻RAM和相变存储器）以实现内存计算、类脑处理单元以及基于多芯片模块的加速器。该综述对近年来提出的最具影响力的架构和技术进行了分类，旨在为读者提供一个关于快速发展的深度学习领域的全面视角。最后，它还对深度学习加速器未来的挑战提供了一些见解，如量子加速器和光子学。
- en: 'Hardware Accelerators, High-Performance Computing, Deep Learning, Deep Neural
    Networks, Emerging Memory Technologies.^†^†copyright: acmcopyright^†^†journalyear:
    2023^†^†doi: XXXXXXX.XXXXXXX^†^†journal: CSUR^†^†ccs: Computer systems organization Architectures^†^†ccs:
    Hardware Reconfigurable logic and FPGAs^†^†ccs: Hardware Emerging technologies^†^†ccs:
    Hardware Very large scale integration design^†^†ccs: Hardware Power and energy^†^†ccs:
    Computing methodologies Machine learning'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件加速器、高性能计算、深度学习、深度神经网络、新兴存储技术。^†^†版权：acmcopyright^†^†期刊年份：2023^†^†doi：XXXXXXX.XXXXXXX^†^†期刊：CSUR^†^†ccs：计算机系统组织
    架构^†^†ccs：硬件 可重构逻辑和FPGA^†^†ccs：硬件 新兴技术^†^†ccs：硬件 超大规模集成设计^†^†ccs：硬件 电力和能源^†^†ccs：计算方法
    机器学习
- en: 1\. Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Since the advent of the Exascale era, we have witnessed the convergence between
    High-Performance Computing (HPC) and Artificial Intelligence (AI). The ever-increasing
    computing power of HPC systems and their ability to manage large amounts of data
    made the development of more and more sophisticated machine learning (ML) techniques
    possible. Deep Learning (DL) is a subset of ML and uses artificial Deep Neural
    Networks (DNNs) with multiple layers of artificial neurons to attempt to mimic
    human brain behavior by learning from large amounts of data. Thanks to technological
    and architectural improvements, not only an increasing number of parallel high-end
    processors but also co-processors such as graphics processing units (GPUs) and
    vector/tensor computing units have been integrated into the nodes of HPC systems.
    This supercomputing power enabled the speed up of the automatic training phase
    of DNN models and their subsequent inference phase in the target application scenarios.
    The introduction of the pioneering AlexNet ([Krizhevsky2012,](#bib.bib151) ) at
    the ImageNet challenge in 2012 was enabled by GPU computing, outlining the value
    of acceleration during the training and inference phase. Since then, a multitude
    of DNN models have been developed for various tasks including image recognition
    and classification, Natural Language Processing (NLP), and Generative AI. These
    applications require specialized *hardware accelerators*, to efficiently handle
    the heavy computational demands of DNN algorithms. DL accelerators are currently
    in use in several types of computing systems spanning from ultra-low-power and
    resource-constrained devices on the edge up to servers, HPC infrastructures, and
    data centers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自从Exascale时代的到来，我们见证了高性能计算（HPC）与人工智能（AI）之间的融合。HPC系统计算能力的不断增强及其管理大量数据的能力使得越来越复杂的机器学习（ML）技术的开发成为可能。深度学习（DL）是ML的一个子集，它使用具有多层人工神经元的人工深度神经网络（DNNs）试图通过从大量数据中学习来模拟人脑行为。由于技术和架构的改进，不仅越来越多的高端处理器并行集成，还包括图形处理单元（GPUs）和向量/张量计算单元等协处理器已集成到HPC系统的节点中。这种超级计算能力加速了DNN模型的自动训练阶段以及在目标应用场景中的后续推理阶段。2012年在ImageNet挑战赛上，开创性的AlexNet
    ([Krizhevsky2012,](#bib.bib151)) 的出现得益于GPU计算，凸显了在训练和推理阶段加速的价值。此后，开发了大量用于图像识别和分类、自然语言处理（NLP）以及生成式AI等各种任务的DNN模型。这些应用需要专业的*硬件加速器*，以高效处理DNN算法的重计算需求。目前，DL加速器已在从超低功耗和资源受限的边缘设备到服务器、高性能计算基础设施和数据中心等各种计算系统中使用。
- en: Scope of the survey. This survey is an attempt to provide an extensive overview
    of the most influential architectures to accelerate DL for high-performance applications.
    The survey highlights various approaches that support DL acceleration including
    GPU-based accelerators, Tensor Processor Units, FPGA-based accelerators, and ASIC-based
    accelerators, such as Neural Processing Units and co-processors on the open-hardware
    RISC-V architecture. The survey also includes accelerators based on emerging technologies
    and computing paradigms, such as 3D-stacked PIM, emerging non-volatile memories
    such as the Resistive switching Random Access Memory (RRAM) and the Phase Change
    Memory (PCM), Neuromorphic Processing Units, and Multi-Chip Modules.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 调查范围。本调查尝试提供对加速深度学习（DL）在高性能应用中最具影响力的架构的广泛概述。调查重点介绍了支持DL加速的各种方法，包括基于GPU的加速器、张量处理单元、基于FPGA的加速器和基于ASIC的加速器，如神经处理单元和开源硬件RISC-V架构上的协处理器。调查还包括基于新兴技术和计算范式的加速器，如3D堆叠PIM、新兴的非易失性存储器如电阻式随机存取存储器（RRAM）和相变存储器（PCM）、神经形态处理单元以及多芯片模块。
- en: Overall, we have reviewed the research on DL accelerators from the past two
    decades, covering a significant time span of literature in this field. We have
    described and referenced about 230 works proposed for DL acceleration. Being DL
    acceleration such a prolific and rapidly evolving field, we do not claim to cover
    exhaustively all the research works appeared so far, but we focused on the most
    influential contributions. Moreover, this survey can be leveraged as a connecting
    point for some previous surveys on accelerators on the AI and DL field  ([chen2020engineering,](#bib.bib40)
    ; [Hassanpour2022,](#bib.bib108) ; [gao2023acm,](#bib.bib82) ; [rathi2023acm,](#bib.bib231)
    ) and other surveys focused on some more specific aspects of DL, such as the architecture-oriented
    optimization of sparse matrices  ([reuther_hpec22,](#bib.bib232) ) and the Neural
    Architecture Search  ([Chitty2022ACMSUR,](#bib.bib46) ).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们回顾了过去二十年中关于深度学习加速器的研究，涵盖了该领域文献中的一个重要时间跨度。我们描述并引用了大约230项深度学习加速工作。由于深度学习加速是一个丰富且迅速发展的领域，我们并不声称全面覆盖所有迄今为止出现的研究工作，但我们专注于最具影响力的贡献。此外，本调查可以作为某些先前关于人工智能和深度学习领域加速器的调查（[chen2020engineering,](#bib.bib40)
    ; [Hassanpour2022,](#bib.bib108) ; [gao2023acm,](#bib.bib82) ; [rathi2023acm,](#bib.bib231)
    ）和其他专注于深度学习的更具体方面的调查的连接点，如稀疏矩阵的架构优化（[reuther_hpec22,](#bib.bib232) ）和神经架构搜索（[Chitty2022ACMSUR,](#bib.bib46)
    ）。
- en: Organization of the survey. The survey is structured in different categories
    and sub-categories belonging to the areas of computer architecture and hardware
    design. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms"), the
    proposed classification is based on several representative features of the accelerators,
    in order to highlight their similarities and differences. To this aim, we organized
    the material in a way that all research papers corresponding to multiple types
    of classifications are cited under each classification. For example, let us consider
    the work $W$, which primarily belongs to the sub-category $X$ where it makes its
    primary contribution. According to our classification policy, this work could
    be cited again in another sub-category $Y$, where it makes its secondary contribution.
    Moreover, under each classification, we have selectively chosen the most notable
    and influential works and, for each work, we focused on its innovative contributions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 调查的组织。该调查按计算机架构和硬件设计领域中的不同类别和子类别进行结构化。如图[1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")所示，提出的分类基于加速器的几个代表性特征，以突出它们的相似性和差异。为此，我们以一种方式组织材料，使得所有对应于多种分类类型的研究论文都在每个分类下进行引用。例如，假设我们考虑工作$W$，它主要属于子类别$X$，并在其中做出主要贡献。根据我们的分类政策，该工作也可以在另一个子类别$Y$中被引用，其中它做出次要贡献。此外，在每个分类下，我们选择了最具代表性和影响力的工作，对于每项工作，我们重点关注其创新贡献。
- en: 'The main paper is structured as follows: Section [2](#S2 "2\. Deep Learning
    Background ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms") provides a brief background on dominant DL topologies, while Section [3](#S3
    "3\. GPU- and TPU-based accelerators ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms") reviews the most significant acceleration solutions
    based on GPUs and TPUs. Section [4](#S4 "4\. Hardware Accelerators ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms") introduces
    three types of hardware-based accelerators: FPGA-based, ASIC-based, and accelerators
    based on the open-hardware RISC-V Instruction Set Architecture (ISA). Section [5](#S5
    "5\. Accelerators based on Emerging Paradigms and Technologies ‣ A Survey on Deep
    Learning Hardware Accelerators for Heterogeneous HPC Platforms") describes DL
    accelerators based on emerging computing paradigms and technologies. A final discussion
    on future trends in DL accelerators can be found in Section $6$. The survey is
    complemented by a supplementary document (referenced as Appendix A) including
    some basic definitions and in-depth technological aspects. To conclude, we hope
    this survey could be useful for a wide range of readers, including computer architects,
    hardware developers, HPC engineers, researchers, and technical professionals.
    A major effort was spent to use a clear and concise technical writing style: we
    hope this effort could be useful in particular to the young generations of master’s
    and Ph.D. students. To facilitate the reading, a list of acronyms is reported
    in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ A Survey on Deep Learning Hardware
    Accelerators for Heterogeneous HPC Platforms").'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 主要论文的结构如下：第[2](#S2 "2\. 深度学习背景 ‣ 针对异构HPC平台的深度学习硬件加速器调研")节提供了关于主流DL拓扑的简要背景，第[3](#S3
    "3\. 基于GPU和TPU的加速器 ‣ 针对异构HPC平台的深度学习硬件加速器调研")节回顾了基于GPU和TPU的最重要的加速解决方案。第[4](#S4
    "4\. 硬件加速器 ‣ 针对异构HPC平台的深度学习硬件加速器调研")节介绍了三种类型的基于硬件的加速器：基于FPGA的、基于ASIC的以及基于开源硬件RISC-V指令集架构（ISA）的加速器。第[5](#S5
    "5\. 基于新兴范式和技术的加速器 ‣ 针对异构HPC平台的深度学习硬件加速器调研")节描述了基于新兴计算范式和技术的DL加速器。第$6$节对DL加速器的未来趋势进行了最终讨论。该调研还附有一个补充文档（参见附录A），包括一些基本定义和深入的技术方面。最后，我们希望这项调研对包括计算机架构师、硬件开发人员、HPC工程师、研究人员和技术专家在内的广泛读者有所帮助。我们投入了大量精力以清晰简洁的技术写作风格进行撰写：我们希望这一努力对年轻的硕士和博士生特别有用。为了方便阅读，缩略语列表已在表[1](#S1.T1
    "表1 ‣ 1\. 引言 ‣ 针对异构HPC平台的深度学习硬件加速器调研")中列出。
- en: \forestset
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \forestset
- en: dir tree/.style= for tree= parent anchor=south west, child anchor=west, anchor=mid
    west, inner ysep=-3.5pt, grow’=0, align=left, edge path= [draw, \forestoptionedge]
    (!u.parent anchor) ++(1em,0) —- (.child anchor)\forestoptionedge label; , if n
    children=0 delay= prepend=[,phantom, calign with current] , fit=rectangle, before
    computing xy= l=2em ,
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: dir tree/.style= 用于树形结构的样式：parent anchor=south west, child anchor=west, anchor=mid
    west, inner ysep=-3.5pt, grow’=0, align=left, edge path= [draw, \forestoptionedge]
    (!u.parent anchor) ++(1em,0) —- (.child anchor)\forestoptionedge label; , if n
    children=0 delay= prepend=[,phantom, calign with current] , fit=rectangle, before
    computing xy= l=2em ,
- en: '{forest}'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: dir tree [Survey Organization [§ [2](#S2 "2\. Deep Learning Background ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms") DL Background
    ] [§ [3](#S3 "3\. GPU- and TPU-based accelerators ‣ A Survey on Deep Learning
    Hardware Accelerators for Heterogeneous HPC Platforms") GPU- & TPU-based Accelerators
    [GPU-based Accelerators ] [TPU-based Accelerators ] ] [§ [4](#S4 "4\. Hardware
    Accelerators ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms") Hardware Accelerators [Reconfigurable Hardware Accelerators ]
    [ASIC-based Accelerators [Accelerating Arithmetic Data-paths] [Neural Processing
    Units] [Single-chip NPU] ] [Accelerators based on RISC-V [ISA extensions for (Deep)
    Learning] [Vector Co-processors] [Memory-coupled Neural Processing Units (NPUs)]
    ] ] ]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: dir tree [调查组织 [§ [2](#S2 "2\. 深度学习背景 ‣ 针对异构HPC平台的深度学习硬件加速器调研") 深度学习背景 ] [§ [3](#S3
    "3\. 基于GPU和TPU的加速器 ‣ 针对异构HPC平台的深度学习硬件加速器调研") 基于GPU和TPU的加速器 [基于GPU的加速器 ] [基于TPU的加速器
    ] ] [§ [4](#S4 "4\. 硬件加速器 ‣ 针对异构HPC平台的深度学习硬件加速器调研") 硬件加速器 [可重构硬件加速器 ] [基于ASIC的加速器
    [加速算术数据通路] [神经处理单元] [单芯片NPU] ] [基于RISC-V的加速器 [（深度）学习的ISA扩展] [向量协处理器] [与内存耦合的神经处理单元（NPU）]
    ] ] ]
- en: (a)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '{forest}'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: dir tree [ [§ [5](#S5 "5\. Accelerators based on Emerging Paradigms and Technologies
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
    Emerging Paradigms and Technologies [Accelerators for Sparse Matrices ] [3D-stacked
    Processing-in-memory [3-D Stacked PIM Solutions] ] [In-Memory computing][Full-digital
    Neuromorphic Accelerators] [Multi-Chip Modules ] ] [§ [6](#S6 "6\. Conclusions
    and Open Challenges ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms") Conclusions & Open Challenges] ]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 目录树 [ [§ [5](#S5 "5\. Accelerators based on Emerging Paradigms and Technologies
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
    新兴范式和技术 [稀疏矩阵加速器] [3D堆叠内存处理 [3-D 堆叠 PIM 解决方案] ] [内存计算][全数字神经形态加速器] [多芯片模块] ] [§ [6](#S6
    "6\. Conclusions and Open Challenges ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms") 结论与挑战] ]
- en: (b)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: Figure 1\. Organization of the survey
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 调查组织结构
- en: Table 1. List of acronyms
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 缩略语列表
- en: '| Acronym | Acronym | Acronym |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 缩略语 | 缩略语 | 缩略语 |'
- en: '| AI: Artificial Intelligence | ASIC: Application Specific Integrated Circuit
    | BRAM: Block Random Access Memory |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| AI: 人工智能 | ASIC: 专用集成电路 | BRAM: 块随机存取内存 |'
- en: '| CMOS: Complementary Metal Oxide Semiconductor | CNN: Convolutional Neural
    Network | CPU: Central Processing Unit |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| CMOS: 互补金属氧化物半导体 | CNN: 卷积神经网络 | CPU: 中央处理单元 |'
- en: '| DL: Deep Learning | DP: Double Precision | DNN: Deep Neural Network |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| DL: 深度学习 | DP: 双精度 | DNN: 深度神经网络 |'
- en: '| DRAM: Dynamic Random Access Memory | EDA: Electronic Design Automation |
    FLOPS: Floating Point Operations per Second |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| DRAM: 动态随机存取内存 | EDA: 电子设计自动化 | FLOPS: 每秒浮点运算次数 |'
- en: '| FMA: Fused Multiply-Add | FPGA: Field-Programmable Gate Array | GEMM: General
    Matrix Multiply |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| FMA: 融合乘加 | FPGA: 现场可编程门阵列 | GEMM: 广义矩阵乘法 |'
- en: '| GP-GPU: General-Purpose Graphics Processing Unit | GPU: Graphics Processing
    Unit | HBM: High Bandwidth Memory |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| GP-GPU: 通用图形处理单元 | GPU: 图形处理单元 | HBM: 高带宽内存 |'
- en: '| HDL: Hardware Description Language | HLS: High Level Synthesis | HMC: Hybrid
    Memory Cube |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| HDL: 硬件描述语言 | HLS: 高级综合 | HMC: 混合内存立方体 |'
- en: '| HPC: High-Performance Computing | MLP: Multi-Layer Perceptron | NPU: Neural
    Processing Unit |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| HPC: 高性能计算 | MLP: 多层感知机 | NPU: 神经处理单元 |'
- en: '| IMC: In-Memory Computing | IoT: Internet of Things | ISA: Instruction Set
    Architecture |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| IMC: 内存计算 | IoT: 物联网 | ISA: 指令集架构 |'
- en: '| MCM: Multi-Chip Module | ML: Machine Learning | NDP: Near Data Processing
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| MCM: 多芯片模块 | ML: 机器学习 | NDP: 接近数据处理 |'
- en: '| NN: Neural Network | NoC: Network on Chip | PCM: Phase Change Memory |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| NN: 神经网络 | NoC: 芯片网络 | PCM: 相变内存 |'
- en: '| PCU: Programmable Computing Unit | PIM: Processing In-Memory | PULP: Parallel
    Ultra Low Power |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| PCU: 可编程计算单元 | PIM: 内存中处理 | PULP: 并行超低功耗 |'
- en: '| QC: Quantum Computing | QML: Quantum Machine Learning | QNN: Quantized Neural
    Network |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| QC: 量子计算 | QML: 量子机器学习 | QNN: 量化神经网络 |'
- en: '| QPU: Quantum Processing Unit | RAM: Random Access Memory | RRAM: Resistive
    RAM |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| QPU: 量子处理单元 | RAM: 随机存取内存 | RRAM: 阻变内存 |'
- en: '| RISC: Reduced Instruction Set Computer | RNN: Recurrent Neural Network |
    SoC: System on Chip |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| RISC: 精简指令集计算机 | RNN: 循环神经网络 | SoC: 系统单芯片 |'
- en: '| SP: Single Precision | SIMD: Single Instruction Multiple Data | SIMT: Single
    Instruction Multiple Thread |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| SP: 单精度 | SIMD: 单指令多数据 | SIMT: 单指令多线程 |'
- en: '| SNN: Spiking Neural Network | SRAM: Static Random Access Memory | TPU: Tensor
    Processing Unit |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| SNN: 脉冲神经网络 | SRAM: 静态随机存取内存 | TPU: 张量处理单元 |'
- en: '| TNN: Ternary Neural Network | VPU: Vector Processing Unit | VRAM: Video Random
    Access Memory |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| TNN: 三值神经网络 | VPU: 向量处理单元 | VRAM: 视频随机存取内存 |'
- en: 2\. Deep Learning Background
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 深度学习背景
- en: 'Deep Learning ([lecun2015,](#bib.bib158) ; [Schmidhuber_2015,](#bib.bib238)
    ) is a subset of ML methods that can automatically discover the representations
    needed for feature detection or classification from large data sets, by employing
    multiple layers of processing to extract progressively higher-level features.
    The most recent works in literature clearly show that two main DL topologies have
    emerged as dominant: Deep Neural Networks and Transformers.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习 ([lecun2015,](#bib.bib158) ; [Schmidhuber_2015,](#bib.bib238) ) 是一种机器学习方法的子集，能够自动发现从大数据集中进行特征检测或分类所需的表示，通过使用多个处理层来逐步提取更高层次的特征。文献中最新的研究清楚地表明，两个主要的深度学习拓扑结构已成为主流：深度神经网络和变换器。
- en: 'Concerning DNNs, there are three types of DNNs mostly used today: Multi-Layer
    Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Recurrent Neural
    Networks (RNNs). MLPs ([Rosenblatt1957,](#bib.bib234) ) are feed-forward ANNs
    composed of a series of fully connected layers, where each layer is a set of nonlinear
    functions of a weighted sum of all outputs of the previous one. On the contrary,
    in a CNN ([Lecun1998,](#bib.bib159) ), a convolutional layer extracts the simple
    features from the inputs by executing convolution operations. Each layer is a
    set of nonlinear functions of weighted sums of different subsets of outputs from
    the previous layer, with each subset sharing the same weights. Each convolutional
    layer in the model can capture a different high-level representation of input
    data, allowing the system to automatically extract the features of the inputs
    to complete a specific task, e.g., image classification, face authentication,
    and image semantic segmentation. Finally, RNNs ([Schmidhuber_2015,](#bib.bib238)
    ) address the time-series problem of sequential input data. Each RNN layer is
    a collection of nonlinear functions of weighted sums of the outputs of the previous
    layer and the previous state, calculated when processing the previous samples
    and stored in the RNN’s internal memory. RNN models are widely used in NLP for
    natural language modeling, word embedding, and machine translation. More details
    on concepts and terminology related to DNNs are provided in Sec. A.1 of Appendix
    A.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 关于DNN，目前主要使用三种类型的DNN：多层感知器（MLPs）、卷积神经网络（CNNs）和递归神经网络（RNNs）。MLPs ([Rosenblatt1957,](#bib.bib234)
    ) 是前馈人工神经网络（ANNs），由一系列完全连接的层组成，每一层都是前一层所有输出的加权和的非线性函数。相反，在CNN ([Lecun1998,](#bib.bib159)
    ) 中，卷积层通过执行卷积操作从输入中提取简单特征。每一层都是前一层不同子集的加权和的非线性函数，其中每个子集共享相同的权重。模型中的每个卷积层可以捕获输入数据的不同高级表示，使系统能够自动提取输入的特征以完成特定任务，例如图像分类、人脸认证和图像语义分割。最后，RNNs
    ([Schmidhuber_2015,](#bib.bib238) ) 解决了序列输入数据的时间序列问题。每个RNN层是前一层和前一状态的加权和的非线性函数的集合，这些函数在处理前一样本时计算并存储在RNN的内部记忆中。RNN模型广泛应用于自然语言处理（NLP）中的自然语言建模、词嵌入和机器翻译。有关DNN的概念和术语的更多详细信息，请参见附录A的第A.1节。
- en: Each type of DNN is especially effective for a specific subset of cognitive
    applications. Depending on the target application, and the resource constraints
    of the computing system, different DNN models have been deployed. Besides DNNs,
    Transformer-based models ([vaswaniAttentionAllYou2017,](#bib.bib276) ) recently
    captured a great deal of attention. Transformers were originally proposed for
    NLP ([vaswaniAttentionAllYou2017,](#bib.bib276) ), and are designed to recognize
    long-distance dependencies between data by *attention layers*, where the weights
    used to linearly transform input data are computed dynamically based on the input
    data itself. While DNNs use convolutional layers to perform “local” operations
    on small portions of the input, Transformers use attention layers to perform “global”
    operations on the whole input. Although quite different, DNNs and Transformers
    share many underlying principles (such as gradient descent training, and reliance
    on linear algebra), and many of the DL-dedicated architectures described in this
    survey address both types of topologies.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每种类型的深度神经网络（DNN）在特定的认知应用子集上都特别有效。根据目标应用和计算系统的资源限制，已经部署了不同的DNN模型。除了DNN之外，基于Transformer的模型
    ([vaswaniAttentionAllYou2017,](#bib.bib276) ) 最近引起了广泛关注。Transformers最初是为自然语言处理（NLP）
    ([vaswaniAttentionAllYou2017,](#bib.bib276) ) 提出的，旨在通过*注意力层*来识别数据之间的长距离依赖关系，其中用于线性变换输入数据的权重是根据输入数据本身动态计算的。虽然DNN使用卷积层对输入的局部区域执行“局部”操作，但Transformers使用注意力层对整个输入执行“全局”操作。虽然两者差异很大，但DNN和Transformers共享许多基本原理（如梯度下降训练和对线性代数的依赖），并且本调查中描述的许多专门用于深度学习（DL）的架构都涉及这两种拓扑结构。
- en: 3\. GPU- and TPU-based accelerators
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. GPU和TPU基于的加速器
- en: 3.1\. GPU-based accelerators
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. 基于GPU的加速器
- en: When designing a DL architecture, the decision to include GPUs relies on expected
    benefits in terms of memory bandwidth, datasets size, and optimization of long-running
    tasks. The performance of GPU accelerators could be compared in different ways.
    As a first approximation, their theoretical peak performance and memory bandwidth
    could be used. Anyhow several other architectural characteristics could affect
    the final performance of actual algorithm implementation. To get a better overview
    of their expected performance, running a specific workload, it could be preferable
    to use reference benchmarks, possibly made of representative sets of commonly
    used algorithms implementations. For this reason, different benchmarks have been
    developed, each of them able to test the obtainable performance concerning a given
    workload characteristic, or a given set of application kernels. In the context
    of ML, one of the most used benchmarks is MLPerf ([mlperf,](#bib.bib182) ), which
    has a specific set of training phase tasks ([mlperf-training,](#bib.bib181) ).
    Its results on two different systems, embedding the latest GPU architecture and
    its predecessor (i.e., NVIDIA Hopper and Ampere) are shown in Table [2](#S3.T2
    "Table 2 ‣ 3.1\. GPU-based accelerators ‣ 3\. GPU- and TPU-based accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms"),
    highlighting on average an approximate $2\times$ factor of performance improvement.
    Different vendors, like AMD and Intel, have also developed GP-GPU architectures
    mostly oriented to HPC and more recently to AI computing. Yet the terminology
    used by different vendors is not the same, they share most of the hardware details.
    For example, AMD names Compute Unit which NVIDIA calls Streaming Multiprocessor,
    and Intel calls Compute Slice or Execution-Unite (EU). Further, NVIDIA names Warp
    the set of instructions scheduled and executed at each cycle, while AMD uses the
    term Wavefront, and Intel uses the term EU-Thread. Concerning the execution model,
    NVIDIA uses the Single Instruction Multiple Thread (SIMT), while AMD and Intel
    use the Single Instruction Multiple Data (SIMD) ([khairy2019,](#bib.bib141) ).
    In Table [3](#S3.T3 "Table 3 ‣ 3.1\. GPU-based accelerators ‣ 3\. GPU- and TPU-based
    accelerators ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms"), we report the main hardware features of the three most recent
    GP-GPU architectures developed by NVIDIA H100 ([h100,](#bib.bib12) ), AMD ([mi250x,](#bib.bib9)
    ) and Intel ([arc770,](#bib.bib119) ). We compare the peak performance related
    to the 32-bit single- and 64-bit double-precision, and the peak performance achieved
    using half-precision.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计深度学习架构时，是否包括 GPU 取决于内存带宽、数据集大小以及长时间运行任务的优化预期效益。可以通过不同方式比较 GPU 加速器的性能。作为初步估计，可以使用它们的理论峰值性能和内存带宽。不过，其他一些架构特征也可能影响实际算法实现的最终性能。为了更好地了解它们的预期性能，运行特定的工作负载时，使用参考基准测试可能更为合适，这些基准测试通常由代表性算法实现集合构成。因此，已经开发了不同的基准测试，每个基准测试都能测试特定工作负载特征或特定应用内核集的可获得性能。在
    ML 的背景下，最常用的基准测试之一是 MLPerf ([mlperf](#bib.bib182) )，它有一组特定的训练阶段任务 ([mlperf-training](#bib.bib181)
    )。它在两种不同系统上的结果（分别嵌入了最新 GPU 架构和其前身，即 NVIDIA Hopper 和 Ampere）显示在表 [2](#S3.T2 "Table
    2 ‣ 3.1\. GPU-based accelerators ‣ 3\. GPU- and TPU-based accelerators ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")中，平均突出显示了大约
    $2\times$ 的性能提升因素。不同的供应商，如 AMD 和 Intel，也开发了主要面向 HPC 和最近面向 AI 计算的 GP-GPU 架构。尽管不同供应商使用的术语不同，但它们共享大多数硬件细节。例如，AMD
    称之为计算单元，而 NVIDIA 称之为流式多处理器，Intel 称之为计算片段或执行单元（EU）。此外，NVIDIA 称每个周期调度和执行的指令集合为 Warp，而
    AMD 使用术语 Wavefront，Intel 使用术语 EU-Thread。关于执行模型，NVIDIA 使用单指令多线程（SIMT），而 AMD 和 Intel
    使用单指令多数据（SIMD） ([khairy2019](#bib.bib141) )。在表 [3](#S3.T3 "Table 3 ‣ 3.1\. GPU-based
    accelerators ‣ 3\. GPU- and TPU-based accelerators ‣ A Survey on Deep Learning
    Hardware Accelerators for Heterogeneous HPC Platforms")中，我们报告了 NVIDIA H100 ([h100](#bib.bib12)
    )、AMD ([mi250x](#bib.bib9) ) 和 Intel ([arc770](#bib.bib119) ) 开发的三种最新 GP-GPU 架构的主要硬件特征。我们比较了与
    32 位单精度和 64 位双精度相关的峰值性能，以及使用半精度时实现的峰值性能。
- en: Table 2. MLPerf Training v2.1 Benchmark Results (minutes)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2. MLPerf Training v2.1 基准测试结果（分钟）
- en: '|  | ImageNet | KiTS19 | OpenImages | COCO | LibriSpeech | Wikipedia | Go |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | ImageNet | KiTS19 | OpenImages | COCO | LibriSpeech | Wikipedia | Go |'
- en: '|  | ResNet | 3D U-Net | RetinaNet | Mask R-CNN | RNN-T | BERT | Minigo |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | ResNet | 3D U-Net | RetinaNet | Mask R-CNN | RNN-T | BERT | Minigo |'
- en: '| 8 $\times$ A100 | 30.8 | 25.6 | 89.1 | 43.1 | 32.5 | 24.2 | 161.6 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 8 $\times$ A100 | 30.8 | 25.6 | 89.1 | 43.1 | 32.5 | 24.2 | 161.6 |'
- en: '| 8 $\times$ H100 | 14.7 | 13.1 | 38.0 | 20.3 | 18.2 | 6.4 | 174.6 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 8 $\times$ H100 | 14.7 | 13.1 | 38.0 | 20.3 | 18.2 | 6.4 | 174.6 |'
- en: Table 3. Selected hardware features of most recent GP-GPU systems developed
    by NVIDIA, AMD, and Intel
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表3. 最近开发的NVIDIA、AMD和Intel GP-GPU系统的选定硬件特性
- en: '| Model (Vendor) | H100 (NVIDIA) | Instinct MI250X (AMD) | Arc 770 (Intel)
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 型号 (厂商) | H100 (NVIDIA) | Instinct MI250X (AMD) | Arc 770 (Intel) |'
- en: '| #physical-cores | 132 | 220 | 32 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 物理核心数 | 132 | 220 | 32 |'
- en: '| #logical-cores | 16896 | 14080 | 4096 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑核心数 | 16896 | 14080 | 4096 |'
- en: '| Clock (GHz) | 1.6 | 1.7 | 2.4 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 时钟 (GHz) | 1.6 | 1.7 | 2.4 |'
- en: '| Peak perf. DP (TF) | 30 | 47.9 | 4.9 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 峰值性能 DP (TF) | 30 | 47.9 | 4.9 |'
- en: '| Peak perf. SP (TF) | 60 | 95.8 | 19.7 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 峰值性能 SP (TF) | 60 | 95.8 | 19.7 |'
- en: '| Peak perf. FP16 (TF) | 120 | 383 | 39.3 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 峰值性能 FP16 (TF) | 120 | 383 | 39.3 |'
- en: '| Max Memory (GB) | 80 HBM2e | 128GB HBM2e | 16GB GDDR6 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 最大内存 (GB) | 80 HBM2e | 128GB HBM2e | 16GB GDDR6 |'
- en: '| Mem BW (TB/s) | 2.0 | 3.2 | 0.56 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 内存带宽 (TB/s) | 2.0 | 3.2 | 0.56 |'
- en: '| TDP Power (Watt) | 350 | 560 | 225 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| TDP 功耗 (瓦特) | 350 | 560 | 225 |'
- en: Over the last years, NVIDIA deployed the DGX ([dgx,](#bib.bib200) ) line of
    server and workstation platforms specialized in using GPUs to accelerate DL applications.
    The DGX systems are based on high-performance commodity CPUs, and a set of GPUs
    interconnected using a motherboard-integrated network based on high-speed NVLink ([nvlink,](#bib.bib201)
    ) technology developed by NVIDIA. The hardware features of such architectures
    are described in detail in Sec. A.2 of Appendix A.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，NVIDIA推出了DGX ([dgx,](#bib.bib200) ) 服务器和工作站平台系列，专门使用GPU来加速深度学习应用。DGX系统基于高性能的商用CPU和一组通过主板集成网络互联的GPU，该网络基于NVIDIA开发的高速NVLink
    ([nvlink,](#bib.bib201) ) 技术。此类架构的硬件特性在附录A的第A.2节中有详细描述。
- en: 3.2\. TPU-based accelerators
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 基于TPU的加速器
- en: Tensor Processing Units (TPUs) dedicated to training and inference have been
    proposed very early after the emergence of the first large CNN-based applications.
    This is due to the observation that these workloads are dominated by linear algebra
    kernels that can be refactored as matrix multiplications (particularly if performed
    in batches) and that their acceleration is particularly desirable for high-margin
    applications in data centers. More recently, the emergence of exponentially larger
    models with each passing year (e.g., the GPT-2, GPT-3, GPT-4 Transformer-based
    large language models) required a continuous investment in higher-performance
    training architectures.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 专用于训练和推理的张量处理单元（TPU）在首批大型CNN应用出现后不久就被提出。这是因为观察到这些工作负载由可以重构为矩阵乘法的线性代数内核主导（特别是在批处理时），并且它们的加速对于数据中心的高利润应用特别有利。最近，每年出现的模型规模不断扩大（例如，基于Transformer的大型语言模型GPT-2、GPT-3、GPT-4）需要持续投资于更高性能的训练架构。
- en: Google showcased the first TPU ([Jouppi_2017,](#bib.bib132) ; [jouppiMotivationEvaluationFirst2018,](#bib.bib134)
    ) at ISCA in 2017, but according to the original paper the first deployment occurred
    in 2015 – just three years after the “AlexNet revolution”. Their last TPU v4 implementation
    outperforms the previous TPU v3 by 2.1x and improves performance/Watt by 2.7x
    ([jouppi2023isca,](#bib.bib133) ). In 2019, Habana Labs and Intel proposed Goya
    and Gaudi as microarchitectures for the acceleration of inference ([medina2019hotchips,](#bib.bib183)
    ). While Google and Intel rely on a mixture of in-house designs and GPUs, the
    other main data center providers typically relied on NVIDIA GPUs to serve DL workloads.
    Starting from the Volta architecture ([choquetteVoltaPerformanceProgrammability2018,](#bib.bib48)
    ) and continuing with Ampere ([choquetteA100DatacenterGPU2021,](#bib.bib49) )
    and Hopper ([choquetteNVIDIAHopperH1002023,](#bib.bib47) ; [elsterNvidiaHopperGPU2022,](#bib.bib70)
    ), NVIDIA has embedded inside the GPU Streaming Multiprocessors the counterpart
    of smaller TPUs, i.e., TensorCores.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Google在2017年ISCA大会上展示了首款TPU ([Jouppi_2017,](#bib.bib132) ; [jouppiMotivationEvaluationFirst2018,](#bib.bib134)
    )，但根据原始论文，首个部署发生在2015年——就在“AlexNet革命”之后的三年。他们的最新TPU v4实现性能比之前的TPU v3高出2.1倍，并且每瓦性能提升了2.7倍
    ([jouppi2023isca,](#bib.bib133) )。2019年，Habana Labs和Intel提出了Goya和Gaudi作为推理加速的微架构
    ([medina2019hotchips,](#bib.bib183) )。虽然Google和Intel依赖于自家设计和GPU的混合，其他主要数据中心提供商通常依赖于NVIDIA
    GPU来处理深度学习工作负载。从Volta架构 ([choquetteVoltaPerformanceProgrammability2018,](#bib.bib48)
    ) 到Ampere ([choquetteA100DatacenterGPU2021,](#bib.bib49) ) 和Hopper ([choquetteNVIDIAHopperH1002023,](#bib.bib47)
    ; [elsterNvidiaHopperGPU2022,](#bib.bib70) )，NVIDIA在GPU流式多处理器内部嵌入了小型TPU的对应物，即TensorCores。
- en: GraphCore Colossus Mk1 and Mk2 IPUs ([knowlesGraphcore2021,](#bib.bib148) ;
    [jiaDissectingGraphcoreIPU2019,](#bib.bib126) ) target the GNNs, DNNs, and Transformers
    training employing a tiled many-core architecture of relatively simple processors.
    GraphCore focuses on a high power- and cost-efficient memory hierarchy that does
    not rely on high-bandwidth off-chip HBM, but on cheaper DRAM chips combined with
    a large amount of on-chip SRAM (in the order of 1 GiB per chip). According to
    GraphCore, this design achieves $\sim$2$\times$ the energy efficiency of an NVIDIA
    Ampere GPU and $\sim$3$\times$ that of a Google TPUv3 on sustained workloads.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: GraphCore Colossus Mk1 和 Mk2 IPU（[knowlesGraphcore2021](#bib.bib148)；[jiaDissectingGraphcoreIPU2019](#bib.bib126)）针对GNNs、DNNs和Transformers训练，采用了相对简单处理器的平铺多核架构。GraphCore专注于高功率和成本效率的内存层次结构，不依赖于高带宽的离芯HBM，而是使用更便宜的DRAM芯片，并结合大量的片上SRAM（每个芯片约1
    GiB）。根据GraphCore的说法，这种设计在持续工作负载下能实现大约2倍于NVIDIA Ampere GPU的能效和大约3倍于Google TPUv3的能效。
- en: Concerning academic and research-proposed architectures, IBM Research focused
    on introducing techniques to reduce the precision of data formats used for training ([agrawal20219,](#bib.bib2)
    ; [venkataramaniRaPiDAIAccelerator2021,](#bib.bib279) ), introducing Hybrid-FP8
    formats in training ASICs and tensor processors. A similar effort is performed
    by the authors of Cambricon-Q ([zhaoCambriconQHybridArchitecture2021,](#bib.bib307)
    ), which also introduce further improvements to exploit the statistical properties
    of tensors to minimize bandwidth consumption and maximize efficiency. Finally,
    Gemmini ([gonzalez16mm106GOPS2021,](#bib.bib92) ; [gencGemminiEnablingSystematic2021,](#bib.bib87)
    ) and RedMulE ([tortorellaRedMulECompactFP162022a,](#bib.bib266) ; [tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265)
    ) are efforts to introduce tensor processor hardware IPs (respectively, generated
    from a template and hand-tuned) that can be integrated inside System-on-Chips,
    similarly to what NVIDIA does with TensorCores.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 关于学术和研究提出的架构，IBM Research 专注于引入技术以降低用于训练的数据格式的精度（[agrawal20219](#bib.bib2)；[venkataramaniRaPiDAIAccelerator2021](#bib.bib279)），在训练ASIC和张量处理器中引入了Hybrid-FP8格式。Cambricon-Q的作者也进行类似的工作（[zhaoCambriconQHybridArchitecture2021](#bib.bib307)），同样引入了进一步改进，以利用张量的统计特性来最小化带宽消耗并最大化效率。最后，Gemmini（[gonzalez16mm106GOPS2021](#bib.bib92)；[gencGemminiEnablingSystematic2021](#bib.bib87)）和RedMulE（[tortorellaRedMulECompactFP162022a](#bib.bib266)；[tortorellaRedMuleMixedPrecisionMatrixMatrix2023](#bib.bib265)）是旨在引入张量处理器硬件IP（分别是从模板生成和手动调优）的努力，这些IP可以集成在片上系统中，类似于NVIDIA对TensorCores的做法。
- en: 4\. Hardware Accelerators
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件加速器
- en: Typical HPC workloads, like genomics, astrophysics, finance, and cyber security,
    require the elaboration of massive amounts of data and they can take advantage
    of DL methods with results that can surpass human ability ([Bengio_2009,](#bib.bib16)
    ; [Schmidhuber_2015,](#bib.bib238) ; [Goodfellow_2016,](#bib.bib93) ; [SPAGNOLO20201,](#bib.bib252)
    ). However, an ever-increasing computing power, a rapid change of the data analysis
    approaches, and the introduction of novel computational paradigms are needed.
    DL models rely on remarkable computational complexities that can be efficiently
    supported, without renouncing a good trade-off between speed, energy efficiency,
    design effort, and cost, by optimized hardware platforms that are able to provide
    high levels of parallelism and a considerable amount of memory resources. These
    platforms can be developed using CPUs, GPUs, FPGAs, CGRAs, and ASICs ([Goodfellow_2016,](#bib.bib93)
    ; [DHILLESWARARAO_2022,](#bib.bib63) ; [Sze_2017,](#bib.bib261) ; [Machupalli_2022,](#bib.bib176)
    ; [Du_2018,](#bib.bib67) ; [Wang_2019,](#bib.bib289) ; [Leibo2019ACMSUR,](#bib.bib171)
    ). CPUs may have higher cache size and higher on-chip bandwidth than GPUs and
    reconfigurable architectures, but they show a limited ability to process large
    amounts of data in parallel. On the other hand, with their high throughput and
    parallelism, GPUs are extremely efficient in terms of performance, but, as a drawback,
    they consume a lot of power and are much more expensive than their counterparts.
    Heterogeneous computing platforms based on modern FPGAs achieve moderate speed
    and consume less energy compared to GPUs, despite limited computing and memory
    resources ([Qadeer2015,](#bib.bib225) ; [Sze_2017,](#bib.bib261) ; [DHILLESWARARAO_2022,](#bib.bib63)
    ). Conversely, ASICs take longer design times, require higher design efforts,
    and do not offer flexibility, but they provide optimum computational speed and
    power consumption. A good trade-off between speed, power consumption, and design
    effort is offered by CGRAs that exhibit near-ASIC energy efficiency and performances
    with near-FPGA reconfigurability levels.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的 HPC 工作负载，如基因组学、天体物理学、金融和网络安全，需要处理大量数据，并且可以利用 DL 方法，其结果可以超越人类能力（[Bengio_2009,](#bib.bib16)
    ; [Schmidhuber_2015,](#bib.bib238) ; [Goodfellow_2016,](#bib.bib93) ; [SPAGNOLO20201,](#bib.bib252)
    ）。然而，计算能力的不断提升、数据分析方法的快速变化以及新计算范式的引入是必需的。 DL 模型依赖于显著的计算复杂性，这些复杂性可以通过优化的硬件平台高效支持，这些平台能够提供高水平的并行性和大量的内存资源，同时不牺牲速度、能效、设计努力和成本之间的良好平衡。这些平台可以通过使用
    CPUs、GPUs、FPGAs、CGRAs 和 ASICs 开发（[Goodfellow_2016,](#bib.bib93) ; [DHILLESWARARAO_2022,](#bib.bib63)
    ; [Sze_2017,](#bib.bib261) ; [Machupalli_2022,](#bib.bib176) ; [Du_2018,](#bib.bib67)
    ; [Wang_2019,](#bib.bib289) ; [Leibo2019ACMSUR,](#bib.bib171) ）。CPU 可能具有比 GPU
    和可重构架构更高的缓存大小和更高的片上带宽，但它们在并行处理大量数据方面的能力有限。另一方面，凭借其高吞吐量和并行性，GPU 在性能方面极为高效，但作为一个缺点，它们消耗大量电力，并且比其对应物更贵。基于现代
    FPGA 的异构计算平台实现了适中的速度，并且与 GPU 相比，能耗更低，尽管计算和内存资源有限（[Qadeer2015,](#bib.bib225) ;
    [Sze_2017,](#bib.bib261) ; [DHILLESWARARAO_2022,](#bib.bib63) ）。相反，ASIC 需要更长的设计时间，需要更高的设计努力，并且不提供灵活性，但它们提供了最佳的计算速度和功耗。CGRAs
    提供了速度、功耗和设计努力之间的良好平衡，它们展现了接近 ASIC 的能效和性能，并且具有接近 FPGA 的可重构性水平。
- en: '![Refer to caption](img/608526b17cd73d3a5f950458d9c127c2.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/608526b17cd73d3a5f950458d9c127c2.png)'
- en: (a)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/681b55d873e3b349a21a3a8cc52037b7.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/681b55d873e3b349a21a3a8cc52037b7.png)'
- en: (b)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/049e980a46ac167faa6b2894bc30b5da.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/049e980a46ac167faa6b2894bc30b5da.png)'
- en: (c)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 2. Dataflows in DL accelerators: (a) Weights stationary; (b) Output
    stationary; (c) Input stationary.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. DL 加速器中的数据流： (a) 权重固定； (b) 输出固定； (c) 输入固定。
- en: 'Independently on the technology, a common problem in the design of accelerators
    is the energy and latency cost of accessing the off-chip DRAM memory, in particular
    considering the significant amount of data that the target HPC applications need
    to process. As sketched in Figure [2](#S4.F2 "Figure 2 ‣ 4\. Hardware Accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms"),
    different data reuse and stationary strategies can be exploited to reduce the
    number of accesses, each strategy offers a certain benefit ([Park2015,](#bib.bib213)
    ; [Peemen2013,](#bib.bib217) ; [Sankaradas2009,](#bib.bib236) ; [Sriam2011,](#bib.bib256)
    ; [Cavigelli2017,](#bib.bib34) ; [Gupta2015,](#bib.bib99) ). In weight stationary
    dataflow, convolutional weights are fixed and stored in the local memory of the
    Processing Elements (PEs) and reused on input activations uploaded step-by-step
    from the external DRAM. Conversely, in output stationary dataflow, partial outputs
    from the PEs are stored locally and reused step-by-step until the computation
    is completed. Then, just the final results are moved to the external DRAM. An
    efficient alternative is input stationary dataflow: in this case, input activations
    are stored in the PEs’ local memory, while weights are uploaded from the external
    DRAM and sent to the PEs. Another approach common to many accelerators is the
    introduction of *quantization* to reduce the data type width. Quantization represents
    an open challenge in designing DL models and many recent studies addressed this
    topic ([quant2021,](#bib.bib88) ; [Liu2022,](#bib.bib173) ). Integer or fixed-point
    data formats are generally preferred over the more computationally-intensive floating-point
    ones. This guarantees better memory occupation, lower computational cost, and
    higher robustness of the model ([jin2022fnet,](#bib.bib129) ). Extreme quantization
    techniques that use only one bit for the data stored (Binary Neural Networks ([BNN2020,](#bib.bib227)
    )) are widely used for very large networks but to get a comparable accuracy they
    require 2-11$\times$ the number of parameters and operations ([Umuroglu2017,](#bib.bib270)
    ), making them not suitable for complex problems.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 无论技术如何，设计加速器时一个常见的问题是访问外部 DRAM 存储器的能量和延迟成本，尤其是考虑到目标 HPC 应用需要处理的大量数据。如图[2](#S4.F2
    "图 2 ‣ 4. 硬件加速器 ‣ 关于异构 HPC 平台深度学习硬件加速器的调查")所示，可以利用不同的数据重用和静态策略来减少访问次数，每种策略都有其特定的好处（[Park2015,](#bib.bib213)；[Peemen2013,](#bib.bib217)；[Sankaradas2009,](#bib.bib236)；[Sriam2011,](#bib.bib256)；[Cavigelli2017,](#bib.bib34)；[Gupta2015,](#bib.bib99)）。在权重静态数据流中，卷积权重固定并存储在处理单元（PEs）的本地内存中，并在从外部
    DRAM 上传的输入激活上逐步重用。相反，在输出静态数据流中，来自 PEs 的部分输出被本地存储并逐步重用，直到计算完成。然后，只有最终结果会被移动到外部
    DRAM。一个有效的替代方案是输入静态数据流：在这种情况下，输入激活存储在 PEs 的本地内存中，而权重则从外部 DRAM 上传并发送到 PEs。另一种许多加速器常用的方法是引入*量化*以减少数据类型宽度。量化在设计
    DL 模型中代表了一个开放挑战，许多近期研究已经涉及这一话题（[quant2021,](#bib.bib88)；[Liu2022,](#bib.bib173)）。通常，整数或定点数据格式较浮点数据格式更受青睐，因为后者计算复杂度更高。这可以保证更好的内存占用、更低的计算成本和更高的模型鲁棒性（[jin2022fnet,](#bib.bib129)）。极端量化技术仅使用一位数据存储（如二值神经网络（[BNN2020,](#bib.bib227)））在非常大的网络中被广泛使用，但为了获得相当的准确性，它们需要2-11倍的参数和操作数量（[Umuroglu2017,](#bib.bib270)），使其不适合复杂问题。
- en: 4.1\. Reconfigurable Hardware Accelerators
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1. 可重构硬件加速器
- en: FPGAs and CGRAs are highly sought-after solutions to hardware accelerate a wide
    range of applications, including DL. The main feature of such reconfigurable platforms
    is the ability to support different computational requirements by repurposing
    the underlying hardware accelerators at deploy-time and also at runtime. More
    details on FPGA technologies and related EDA frameworks are respectively provided
    in Sections A.3 and A.4 of Appendix A. Several FPGA-based hardware accelerators
    for DL are structured as heterogeneous embedded systems ([Ma2018,](#bib.bib174)
    ; [Yazdanbakhsh2018,](#bib.bib298) ; [Aimar2019,](#bib.bib4) ; [Perri2020,](#bib.bib219)
    ; [Li2021,](#bib.bib164) ) that mainly consist of a general-purpose processor,
    for running the software workload; a computational module, designed to speed up
    common DL operators, like convolutions ([Qiu2016,](#bib.bib229) ; [Venieris2019,](#bib.bib277)
    ), de-convolutions ([Chang2020,](#bib.bib35) ; [Sestito2021,](#bib.bib240) ),
    pooling, fully connected operations, activation, and softmax functions ([Spagnolo2022_1,](#bib.bib253)
    ; [Spagnolo2022_2,](#bib.bib254) ); and a memory hierarchy to optimize data movement
    to/from the external DRAM to store data to be processed and computational results.
    A typical approach to accelerate convolutions consists of a systolic array architecture
    (SA), a regular pattern that can be easily replicated ([Xuechao2017,](#bib.bib292)
    ). Each PE in the array is a SIMD vector accumulation module where inputs and
    weights are supplied at each cycle by shifting them from the horizontally and
    vertically adjacent PEs (Figure [3(a)](#S4.F3.sf1 "In Figure 3 ‣ 4.1\. Reconfigurable
    Hardware Accelerators ‣ 4\. Hardware Accelerators ‣ A Survey on Deep Learning
    Hardware Accelerators for Heterogeneous HPC Platforms")). The use of pipelined
    groups of PEs with short local communication and regular architecture enables
    a high clock frequency and limited global data transfer (Figure [3(b)](#S4.F3.sf2
    "In Figure 3 ‣ 4.1\. Reconfigurable Hardware Accelerators ‣ 4\. Hardware Accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: FPGA 和 CGRA 是高度需求的解决方案，用于硬件加速广泛的应用，包括 DL。这些可重构平台的主要特性是能够通过在部署时和运行时重新利用底层硬件加速器来支持不同的计算需求。有关
    FPGA 技术和相关 EDA 框架的更多细节分别提供在附录 A 的 A.3 和 A.4 节中。几种基于 FPGA 的 DL 硬件加速器被构建为异构嵌入式系统（[Ma2018,](#bib.bib174)
    ; [Yazdanbakhsh2018,](#bib.bib298) ; [Aimar2019,](#bib.bib4) ; [Perri2020,](#bib.bib219)
    ; [Li2021,](#bib.bib164)），主要包括一个通用处理器，用于运行软件负载；一个计算模块，设计用于加速常见的 DL 操作，如卷积（[Qiu2016,](#bib.bib229)
    ; [Venieris2019,](#bib.bib277)），反卷积（[Chang2020,](#bib.bib35) ; [Sestito2021,](#bib.bib240)），池化，完全连接操作，激活和
    softmax 函数（[Spagnolo2022_1,](#bib.bib253) ; [Spagnolo2022_2,](#bib.bib254)）；以及一个内存层次结构，以优化数据在外部
    DRAM 和计算结果之间的移动。加速卷积的典型方法包括一种纵向阵列架构 (SA)，这是一种可以轻松复制的规则模式 ([Xuechao2017,](#bib.bib292))。阵列中的每个
    PE 是一个 SIMD 向量累加模块，其中输入和权重通过水平和垂直相邻的 PEs 在每个周期提供（图 [3(a)](#S4.F3.sf1 "在图 3 ‣ 4.1.
    可重构硬件加速器 ‣ 4. 硬件加速器 ‣ 关于异构 HPC 平台的深度学习硬件加速器的调查")）。使用具有短局部通信和规则架构的流水线组 PEs 可以实现高时钟频率和有限的全局数据传输（图
    [3(b)](#S4.F3.sf2 "在图 3 ‣ 4.1. 可重构硬件加速器 ‣ 4. 硬件加速器 ‣ 关于异构 HPC 平台的深度学习硬件加速器的调查")）。
- en: '![Refer to caption](img/800c4452f6812b62722462a610689803.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/800c4452f6812b62722462a610689803.png)'
- en: (a)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/2046a6a03193da9f7f71e2233bc9ed13.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2046a6a03193da9f7f71e2233bc9ed13.png)'
- en: (b)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 3. FPGA accelerators: (a) Systolic array accelerator; (b) Pipelined
    dataflow accelerator.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3. FPGA 加速器: (a) 纵向阵列加速器; (b) 流水线数据流加速器。'
- en: Although FPGAs have traditionally been proposed as accelerators for edge applications,
    they are starting to be adopted also in data centers. Microsoft’s Project Brainwave
    ([Brainwave,](#bib.bib76) ) uses several FPGA boards to accelerate the execution
    of RNNs in the cloud, exploiting the reconfigurability to adapt the platform to
    different DL models. One way to face the limitations imposed by the capability
    of FPGAs to effectively map very large DL models is to use a deeply pipelined
    multi-FPGA design. Recent studies focus on optimizing this type of architecture
    and maximizing the overall throughput ([Zhang2016,](#bib.bib303) ; [Rahman2017,](#bib.bib230)
    ; [Junnan2019,](#bib.bib242) ). In these application contexts, CGRAs represent
    an alternative to FPGAs, providing reconfigurability with coarser-grained functional
    units. They are based on an array of PEs, performing the basic arithmetic, logic,
    and memory operations at the word level and using a small register file as temporary
    data storage. Neighboring PEs are connected through reconfigurable routing that
    allows transferring intermediate results of the computations towards the proper
    neighbors for the next computational step. CGRAs can represent a powerful solution
    to accelerate dense linear algebra applications, such as ML, image processing,
    and computer vision ([amber2022vlsi,](#bib.bib30) ; [tangram,](#bib.bib84) ).
    Thanks to parallel computing and time-multiplexing, CGRAs can efficiently support
    and combine spatial and temporal computational models. Furthermore, they are flexible
    enough for specific domains, and their interconnections, being not as complex
    as those present on FPGAs, provide remarkable advantages in terms of speed, energy
    efficiency, and resource utilization.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管传统上 FPGA 被提议作为边缘应用的加速器，但它们开始在数据中心中得到采用。微软的 Project Brainwave（[Brainwave,](#bib.bib76)）使用多个
    FPGA 板来加速在云中执行 RNNs，利用可重新配置性来适应不同的 DL 模型平台。面对 FPGA 有效地映射非常大的 DL 模型的能力所施加的限制之一是使用深度流水线化的多
    FPGA 设计。最近的研究集中于优化这种类型的架构，并最大化整体吞吐量（[Zhang2016,](#bib.bib303)；[Rahman2017,](#bib.bib230)；[Junnan2019,](#bib.bib242)）。在这些应用上下文中，CGRAs
    是 FPGA 的替代方案，提供了与更粗粒度的功能单元重新配置。它们基于一组 PE，执行单词级别的基本算术、逻辑和内存操作，并使用小的寄存器文件作为临时数据存储。相邻的
    PE 通过可重构路由连接，允许将计算的中间结果传输到适当的邻居，用于下一步计算。CGRAs 可以提供强大的解决方案，加速密集的线性代数应用，如 ML、图像处理和计算机视觉（[amber2022vlsi,](#bib.bib30)；[tangram,](#bib.bib84)）。由于并行计算和时间复用，CGRAs
    可以有效支持和结合空间和时间计算模型。此外，它们对于特定领域足够灵活，它们的互连，因为不像 FPGA 上存在的那样复杂，提供了速度、能量效率和资源利用方面的显著优势。
- en: 4.2\. ASIC-based Accelerators
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 基于 ASIC 的加速器
- en: 4.2.1\. Accelerating Arithmetic Data-paths
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 加速算术数据通路
- en: Performance achievable with ASIC accelerators for the inference of DL models
    is mainly dependent on the structure of the arithmetic data path. At its core,
    DL systems perform several finite impulse response operations over a large set
    of data. Hence the accelerator can be optimized by exploiting techniques used
    for the efficient implementation of the underlying arithmetic operations. As shown
    in Figure  [4](#S4.F4 "Figure 4 ‣ 4.2.1\. Accelerating Arithmetic Data-paths ‣
    4.2\. ASIC-based Accelerators ‣ 4\. Hardware Accelerators ‣ A Survey on Deep Learning
    Hardware Accelerators for Heterogeneous HPC Platforms"), three main types of optimization
    can be performed on the arithmetic data path. The first optimization approach
    takes into account that convolution is one of the main operations performed in
    a DL system. As demonstrated in ([Cheng_2004,](#bib.bib43) ; [Tsao_2012,](#bib.bib269)
    ), mono-dimensional convolutions can be efficiently computed by performing a reduced
    number of multiplications, thus improving the trade-off between the throughput
    of the circuit and the number of hardware resources needed for its implementation.
    This technique has been further developed in ([Wang_2018,](#bib.bib287) ; [Cheng_2020,](#bib.bib44)
    ; [Wang_2022,](#bib.bib285) ), where it is applied to multi-dimensional convolutions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ASIC 加速器进行深度学习模型推理时的性能主要取决于算术数据路径的结构。深度学习系统核心上执行多个有限脉冲响应操作，对大量数据进行处理。因此，加速器可以通过利用高效实现底层算术操作的技术来进行优化。如图
    [4](#S4.F4 "Figure 4 ‣ 4.2.1\. Accelerating Arithmetic Data-paths ‣ 4.2\. ASIC-based
    Accelerators ‣ 4\. Hardware Accelerators ‣ A Survey on Deep Learning Hardware
    Accelerators for Heterogeneous HPC Platforms") 所示，可以对算术数据路径进行三种主要类型的优化。第一种优化方法考虑到卷积是深度学习系统中执行的主要操作之一。如
    ([Cheng_2004,](#bib.bib43) ; [Tsao_2012,](#bib.bib269) ) 所示，单维卷积可以通过减少乘法次数来高效计算，从而改善电路吞吐量和实现所需硬件资源之间的权衡。这种技术在
    ([Wang_2018,](#bib.bib287) ; [Cheng_2020,](#bib.bib44) ; [Wang_2022,](#bib.bib285)
    ) 中得到了进一步发展，应用于多维卷积。
- en: The multiplication itself can be implemented with optimized circuits. In ([MinJou_1999,](#bib.bib131)
    ; [Petra_2011,](#bib.bib220) ; [Frustaci2020,](#bib.bib80) ) the area and power
    dissipation of the multiplier circuit is reduced by discarding part of the partial
    products used to compute the result. These circuits trade off precision and circuit
    complexity to improve speed and power consumption. This approach is often referred
    to as *approximate computing paradigm*, providing a way to approximate the design
    at the cost of an acceptable accuracy loss. Approximate computing techniques proposed
    in ([Kulkarni_2011,](#bib.bib152) ; [Zervakis_2016,](#bib.bib302) ) provide a
    reduced complexity multiplier by modifying the way the partial products are computed.
    In ([Zacharelos_2022,](#bib.bib300) ), a recursive approach is proposed, in which
    the multiplier is decomposed into small approximate units. In the approach proposed
    in ([Esposito_2017,](#bib.bib72) ), the approximation is implemented in the way
    the partial products are summed. Finally, the approximate computing paradigm can
    also be implemented in the 4-2 compressors ([Ahmadinejad_2019,](#bib.bib3) ; [Yang_2015,](#bib.bib297)
    ; [Ha_2018,](#bib.bib101) ; [Strollo_2020,](#bib.bib259) ; [Park_2021,](#bib.bib208)
    ; [Kong_2021,](#bib.bib149) ) that represent the atomic blocks used for the compression
    of the partial products.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法本身可以通过优化电路来实现。在 ([MinJou_1999,](#bib.bib131) ; [Petra_2011,](#bib.bib220)
    ; [Frustaci2020,](#bib.bib80) ) 中，通过丢弃计算结果所需的部分部分积来减少乘法器电路的面积和功耗。这些电路在提高速度和功耗方面权衡了精度和电路复杂性。这种方法通常被称为*近似计算范式*，提供了一种在可接受的精度损失代价下近似设计的方法。
    ([Kulkarni_2011,](#bib.bib152) ; [Zervakis_2016,](#bib.bib302) ) 中提出的近似计算技术通过修改部分积的计算方式提供了一个降低复杂性的乘法器。在
    ([Zacharelos_2022,](#bib.bib300) ) 中，提出了一种递归方法，将乘法器分解为小的近似单元。在 ([Esposito_2017,](#bib.bib72)
    ) 中提出的方法中，近似是通过部分积求和的方式实现的。最后，近似计算范式也可以在 4-2 压缩器中实现 ([Ahmadinejad_2019,](#bib.bib3)
    ; [Yang_2015,](#bib.bib297) ; [Ha_2018,](#bib.bib101) ; [Strollo_2020,](#bib.bib259)
    ; [Park_2021,](#bib.bib208) ; [Kong_2021,](#bib.bib149) )，这些压缩器代表了用于部分积压缩的原子块。
- en: Different from the previous works, the segmentation method aims at reducing
    the bit-width of the multiplicands. The approaches in ([Hashemi_2015,](#bib.bib107)
    ; [Vahdat_2019,](#bib.bib273) ) describe a dynamic segmentation method in which
    the segment is selected starting from the leading one of the multiplicand binary
    representation. On the contrary, the paper in ([Narayanamoorthy_2015,](#bib.bib193)
    ) proposes a static segmentation method, which reduces the complexity of the selection
    mechanism by choosing between two segments with a fixed number of bits. The paper
    in ([Strollo_2022,](#bib.bib260) ) improves the accuracy of the static segmentation
    method multipliers by reducing the maximum approximation error, whereas in ([Li_2021,](#bib.bib166)
    ) the authors propose a hybrid approach in which a static stage is cascaded to
    a dynamic one.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往的工作不同，分段方法旨在减少乘数的位宽。([Hashemi_2015,](#bib.bib107) ; [Vahdat_2019,](#bib.bib273)
    ) 中描述了一种动态分段方法，其中分段是从乘数二进制表示的领先位开始选择的。相反，([Narayanamoorthy_2015,](#bib.bib193)
    ) 中的论文提出了一种静态分段方法，通过在两个具有固定位数的分段之间选择来减少选择机制的复杂性。([Strollo_2022,](#bib.bib260)
    ) 中的论文通过减少最大近似误差来提高静态分段方法乘法器的准确性，而 ([Li_2021,](#bib.bib166) ) 中的作者提出了一种混合方法，其中静态阶段级联到动态阶段。
- en: '{forest}'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [Arithmetic
    Data-path [Convolution Optimization [ Cheng ([Cheng_2004,](#bib.bib43) )
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [算术数据路径 [卷积优化
    [ Cheng ([Cheng_2004,](#bib.bib43) )
- en: Tsao ([Tsao_2012,](#bib.bib269) )
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Tsao ([Tsao_2012,](#bib.bib269) )
- en: Wang ([Wang_2018,](#bib.bib287) )
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Wang ([Wang_2018,](#bib.bib287) )
- en: Cheng ([Cheng_2020,](#bib.bib44) )
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Cheng ([Cheng_2020,](#bib.bib44) )
- en: Wang ([Wang_2022,](#bib.bib285) )
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Wang ([Wang_2022,](#bib.bib285) )
- en: '] ], [Approximate Computing [Multiplication [ Jou ([MinJou_1999,](#bib.bib131)
    ), Petra ([Petra_2011,](#bib.bib220) )'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '] ], [近似计算 [乘法 [ Jou ([MinJou_1999,](#bib.bib131) ), Petra ([Petra_2011,](#bib.bib220)
    )'
- en: Kulkarni ([Kulkarni_2011,](#bib.bib152) ), Zervakis ([Zervakis_2016,](#bib.bib302)
    )
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Kulkarni ([Kulkarni_2011,](#bib.bib152) ), Zervakis ([Zervakis_2016,](#bib.bib302)
    )
- en: Zacharelos ([Zacharelos_2022,](#bib.bib300) ), Esposito ([Esposito_2017,](#bib.bib72)
    )
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Zacharelos ([Zacharelos_2022,](#bib.bib300) ), Esposito ([Esposito_2017,](#bib.bib72)
    )
- en: '] ], [4-2 compressors [ Ahmadinejad ([Ahmadinejad_2019,](#bib.bib3) ), Yang
    ([Yang_2015,](#bib.bib297) )'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '] ], [4-2 压缩器 [ Ahmadinejad ([Ahmadinejad_2019,](#bib.bib3) ), Yang ([Yang_2015,](#bib.bib297)
    )'
- en: Ha ([Ha_2018,](#bib.bib101) ), Strollo ([Strollo_2020,](#bib.bib259) )
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Ha ([Ha_2018,](#bib.bib101) ), Strollo ([Strollo_2020,](#bib.bib259) )
- en: Park ([Park_2021,](#bib.bib208) ), Kong ([Kong_2021,](#bib.bib149) )
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Park ([Park_2021,](#bib.bib208) ), Kong ([Kong_2021,](#bib.bib149) )
- en: '] ] ], [Segmentation Methods [ Hashemi ([Hashemi_2015,](#bib.bib107) )'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '] ] ], [分段方法 [ Hashemi ([Hashemi_2015,](#bib.bib107) )'
- en: Vahdat ([Vahdat_2019,](#bib.bib273) )
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Vahdat ([Vahdat_2019,](#bib.bib273) )
- en: Narayanamoorthy ([Narayanamoorthy_2015,](#bib.bib193) )
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Narayanamoorthy ([Narayanamoorthy_2015,](#bib.bib193) )
- en: Strollo ([Strollo_2022,](#bib.bib260) )
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Strollo ([Strollo_2022,](#bib.bib260) )
- en: Li ([Li_2021,](#bib.bib166) )
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Li ([Li_2021,](#bib.bib166) )
- en: '] ] ]'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '] ] ]'
- en: Figure 4. Taxonomy of the data-path architectures described in Section [4.2.1](#S4.SS2.SSS1
    "4.2.1\. Accelerating Arithmetic Data-paths ‣ 4.2\. ASIC-based Accelerators ‣
    4\. Hardware Accelerators ‣ A Survey on Deep Learning Hardware Accelerators for
    Heterogeneous HPC Platforms")
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. 第 [4.2.1](#S4.SS2.SSS1 "4.2.1\. 加速算术数据路径 ‣ 4.2\. 基于 ASIC 的加速器 ‣ 4\. 硬件加速器
    ‣ 深度学习硬件加速器在异构 HPC 平台上的调查") 节中描述的数据路径架构分类
- en: 4.2.2\. Neural Processing Units (NPUs)
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 神经处理单元 (NPUs)
- en: The purpose of an NPU is to accelerate the performance and improve the energy
    efficiency of specific tasks offloaded from the CPU ([song2019isscc,](#bib.bib248)
    ) ([hung2021jsscc,](#bib.bib115) ). In particular, NPUs are designed to accommodate
    a reasonable amount of multiply/accumulate (MAC) units, which are the PEs devised
    in the convolutional and fully-connected layers of DNNs ([Chen_2017,](#bib.bib41)
    ; [desoli17isscc,](#bib.bib62) ).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: NPU 的目的是加速性能并提高从 CPU ([song2019isscc,](#bib.bib248) ) ([hung2021jsscc,](#bib.bib115)
    ) 转移过来的特定任务的能效。特别是，NPU 设计用于容纳合理数量的乘法/累加 (MAC) 单元，这些单元是 DNN 中卷积和全连接层中的 PEs ([Chen_2017,](#bib.bib41)
    ; [desoli17isscc,](#bib.bib62) )。
- en: Table 4. Summary of NPU accelerators.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4. NPU 加速器总结。
- en: '| NPU | Process | Area [mm²] | Supply voltage [V] | Max. Freq. [MHz] | PP [TOPS]
    | Max EE [TOPS/W] | Max AE [TOPS/mm²] |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| NPU | 制程 | 面积 [mm²] | 供电电压 [V] | 最大频率 [MHz] | PP [TOPS] | 最大能效 [TOPS/W] |
    最大面积效能 [TOPS/mm²] |'
- en: '| Samsung ([song2019isscc,](#bib.bib248) ) | 8 nm | 5.5 | 0.8 | 933 | 6.9 |
    3.4 | 1.25 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Samsung ([song2019isscc,](#bib.bib248) ) | 8 nm | 5.5 | 0.8 | 933 | 6.9 |
    3.4 | 1.25 |'
- en: '| UM+NVIDIA ([zhang2019vlsi,](#bib.bib304) ) | 16 nm | 2.4 | 0.8 | 480 | -
    | 3.6 | - |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| UM+NVIDIA ([zhang2019vlsi,](#bib.bib304) ) | 16 nm | 2.4 | 0.8 | 480 | -
    | 3.6 | - |'
- en: '| MediaTek ([lin2020isscc,](#bib.bib168) ) | 7 nm | 3.04 | 0.825 | 880 | 3.6
    | 6.55 | 1.18 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| MediaTek ([lin2020isscc,](#bib.bib168) ) | 7 nm | 3.04 | 0.825 | 880 | 3.6
    | 6.55 | 1.18 |'
- en: '| Alibaba ([jiao2020isscc,](#bib.bib128) ) | 12 nm | 709 | - | 700 | 825 |
    499 | 1.16 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Alibaba ([jiao2020isscc,](#bib.bib128) ) | 12 nm | 709 | - | 700 | 825 |
    499 | 1.16 |'
- en: '| Samsung ([park2021isscc,](#bib.bib211) ) | 5 nm | 5.46 | 0.9 | 1196 | 29.4
    | 13.6 | 2.69 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Samsung ([park2021isscc,](#bib.bib211) ) | 5 nm | 5.46 | 0.9 | 1196 | 29.4
    | 13.6 | 2.69 |'
- en: '| Samsung ([park2022isscc,](#bib.bib212) ) | 4 nm | 4.74 | 1 | 1197 | 39.3
    | 11.59 | 6.9 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Samsung ([park2022isscc,](#bib.bib212) ) | 4 nm | 4.74 | 1 | 1197 | 39.3
    | 11.59 | 6.9 |'
- en: Each PE contains a synaptic weight buffer and the MAC units to perform the computation
    of a neuron, namely, multiplication, accumulation, and an activation function
    (e.g., sigmoid). A PE can be realized entirely with a full-CMOS design or by using
    emerging non-volatile memories such as RRAM and PCM to perform in situ matrix-vector
    multiplication as in the RENO chip ([liu2015dac,](#bib.bib172) ) or as in the
    MAC units proposed in ([xue2019isscc,](#bib.bib296) ; [narayanan2021ted,](#bib.bib194)
    ). The advantage of these architectures is that only the input and final output
    are digital; the intermediate results are all analog and are coordinated by analog
    routers. Data converters (DACs and ADCs) are required only when transferring data
    between the NPU and the CPU with an advantage in terms of energy efficiency (the
    work in ([xue2019isscc,](#bib.bib296) ) reports an energy efficiency of 53.17
    TOPS/W), although there are insufficient experimental data to support this evidence
    in comparison with full-digital NPUs. In Table [4](#S4.T4 "Table 4 ‣ 4.2.2\. Neural
    Processing Units (NPUs) ‣ 4.2\. ASIC-based Accelerators ‣ 4\. Hardware Accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms"),
    we reported the main features of several full-digital NPUs designs by also highlighting
    their Peak Performance (PP), Energy Efficiency (EE), and Area Efficiency (AE).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 每个PE包含一个突触权重缓冲区和用于执行神经元计算的MAC单元，即乘法、累加和激活函数（例如，sigmoid）。一个PE可以完全通过全CMOS设计实现，或使用新兴的非易失性存储器如RRAM和PCM在现场执行矩阵-向量乘法，例如RENO芯片（[liu2015dac,](#bib.bib172)）或在（[xue2019isscc,](#bib.bib296)
    ; [narayanan2021ted,](#bib.bib194)）中提出的MAC单元。这些架构的优点在于只有输入和最终输出是数字的；中间结果都是模拟的，并由模拟路由器协调。数据转换器（DACs和ADCs）仅在NPU和CPU之间传输数据时需要，这在能源效率方面具有优势（[xue2019isscc,](#bib.bib296)中的研究报告了53.17
    TOPS/W的能源效率），尽管与全数字NPU相比，支持这一证据的实验数据不足。在表[4](#S4.T4 "Table 4 ‣ 4.2.2\. Neural
    Processing Units (NPUs) ‣ 4.2\. ASIC-based Accelerators ‣ 4\. Hardware Accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")中，我们报告了几种全数字NPU设计的主要特性，同时突出了它们的峰值性能（PP）、能源效率（EE）和面积效率（AE）。
- en: 4.2.3\. Single-chip NPUs
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 单芯片NPU
- en: In the DNN landscape, single-chip domain-specific accelerators achieved great
    success in both cloud and edge scenarios. These custom architectures offer better
    performance and energy efficiency concerning CPUs/GPUs thanks to an optimized
    data flow (or data reuse pattern) that reduces off-chip memory accesses, while
    improving the system efficiency ([chen2020engineering,](#bib.bib40) ). The DianNao
    series represents a full digital stand-alone DNN accelerator that introduces a
    customized design to minimize the memory transfer latency and enhance the system
    efficiency. DaDianNao ([chen2016commacm,](#bib.bib39) ) targets the datacenter
    scenario and integrates a large on-chip embedded dynamic random access memory
    (eDRAM) to avoid the long main memory access time. The same principle applies
    to the embedded scenario. ShiDianNao ([chen2016commacm,](#bib.bib39) ) is a DNN
    accelerator dedicated to CNN applications. Using a weight-sharing strategy, its
    footprint is much smaller than the previous design. It is possible to map all
    of the CNN parameters onto a small on-chip static random access memory (SRAM)
    when the CNN model is small. In this way, ShiDianNao avoids expensive off-chip
    DRAM access time and achieves 60 times more energy efficiency compared to DianNao.
    Furthermore, domain-specific instruction set architectures (ISAs) have been proposed
    to support a wide range of NN applications. Cambricon ([zhang2016micro,](#bib.bib305)
    ) and EIE ([han2016isca,](#bib.bib103) ) are examples of architectures that integrate
    scalar, vector, matrix, logical, data transfer, and control instructions. Their
    ISA considers data parallelism and the use of customized vector/matrix instructions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在DNN领域，单芯片领域特定加速器在云端和边缘场景中取得了巨大成功。这些定制架构提供了比CPU/GPUs更好的性能和能效，得益于优化的数据流（或数据重用模式），这减少了芯片外内存访问，同时提高了系统效率（[chen2020engineering](#bib.bib40)）。DianNao系列代表了一种全数字独立DNN加速器，它引入了定制设计以最小化内存传输延迟并提高系统效率。DaDianNao（[chen2016commacm](#bib.bib39)）面向数据中心场景，并集成了大量片上嵌入式动态随机访问内存（eDRAM），以避免长时间的主内存访问。相同的原则也适用于嵌入式场景。ShiDianNao（[chen2016commacm](#bib.bib39)）是一个专门用于CNN应用的DNN加速器。通过使用权重共享策略，它的占用空间比之前的设计小得多。当CNN模型较小时，可以将所有CNN参数映射到一个小的片上静态随机访问内存（SRAM）中。这样，ShiDianNao避免了昂贵的芯片外DRAM访问时间，并实现了比DianNao高60倍的能效。此外，已经提出了领域特定的指令集架构（ISA）以支持广泛的NN应用。Cambricon（[zhang2016micro](#bib.bib305)）和EIE（[han2016isca](#bib.bib103)）是集成了标量、向量、矩阵、逻辑、数据传输和控制指令的架构示例。它们的ISA考虑了数据并行性以及自定义向量/矩阵指令的使用。
- en: Eyeriss is another notable accelerator ([Chen_2017,](#bib.bib41) ) that can
    support high throughput inference and optimize system-level energy efficiency,
    also including off-chip DRAMs. The main features of Eyeriss are a spatial architecture
    based on an array of 168 PEs that creates a four-level memory hierarchy, a dataflow
    that reconfigures the spatial architecture to map the computation of a given CNN
    and optimize towards the best energy efficiency, a network-on-chip (NoC) architecture
    that uses both multi-cast and point-to-point single-cycle data delivery, and run-length
    compression (RLC) and PE data gating that exploit the statistics of zero data
    in CNNs to further improve EE.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Eyeriss是另一个值得注意的加速器（[Chen_2017](#bib.bib41)），它可以支持高吞吐量的推理并优化系统级能效，还包括芯片外DRAM。Eyeriss的主要特点是基于168个PE阵列的空间架构，创建了四级内存层次结构，重新配置空间架构的数据流以映射给定CNN的计算并优化到最佳能效，采用多播和点对点单周期数据传输的网络芯片（NoC）架构，以及利用CNN中零数据统计的运行长度压缩（RLC）和PE数据门控进一步提高能效。
- en: In ([desoli17isscc,](#bib.bib62) ), STMicroelectronics introduced the Orlando
    system-on-chip, a 28nm FDSOI-based CNN accelerator integrating an SRAM-based architecture
    with low-power features and adaptive circuitry to support a wide voltage range.
    Such a DNN processor provides an energy-efficient set of convolutional accelerators
    supporting kernel compression, an on-chip reconfigurable data-transfer fabric,
    a power-efficient array of DSPs to support complete real-world computer vision
    applications, an ARM-based host subsystem with peripherals, a range of high-speed
    I/O interfaces, and a chip-to-chip multilink to pair multiple accelerators together.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ([desoli17isscc,](#bib.bib62) ) 中，意法半导体公司介绍了Orlando系统芯片，这是一种基于28nm FDSOI的CNN加速器，集成了具有低功耗特性的SRAM架构和自适应电路，以支持广泛的电压范围。这样的DNN处理器提供了一组高效能的卷积加速器，支持内核压缩、片上可重构数据传输结构、功耗高效的DSP阵列以支持完整的现实世界计算机视觉应用、基于ARM的主机子系统及外设、一系列高速I/O接口，以及用于将多个加速器配对的芯片间多链路。
- en: Table 5. Summary of single-chip digital DNN accelerators
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表5. 单芯片数字DNN加速器的总结
- en: '| Accelerator | Technology | Application | Area [mm²] | Power [mW] | Performance
    [GOPS] | EE [GOPS/W] |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 加速器 | 技术 | 应用 | 面积 [mm²] | 功耗 [mW] | 性能 [GOPS] | 能效 [GOPS/W] |'
- en: '| DaDianNao ([chen2016commacm,](#bib.bib39) ) | 28 nm | DNN | 67.7 | 15970
    | 5585 | - |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| DaDianNao ([chen2016commacm,](#bib.bib39) ) | 28 nm | DNN | 67.7 | 15970
    | 5585 | - |'
- en: '| ShiDianNao ([chen2016commacm,](#bib.bib39) ) | 65 nm | CNN | 4.86 | 320 |
    194 | - |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| ShiDianNao ([chen2016commacm,](#bib.bib39) ) | 65 nm | CNN | 4.86 | 320 |
    194 | - |'
- en: '| Cambricon ([zhang2016micro,](#bib.bib305) ) | 65 nm | CNN | 6.38 | 954 |
    1111.92 | - |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Cambricon ([zhang2016micro,](#bib.bib305) ) | 65 nm | CNN | 6.38 | 954 |
    1111.92 | - |'
- en: '| EIE ([han2016isca,](#bib.bib103) ) | 28 nm | CNN+LSTM | 63.8 | 2360 | 1.6
    | 177.78 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| EIE ([han2016isca,](#bib.bib103) ) | 28 nm | CNN+LSTM | 63.8 | 2360 | 1.6
    | 177.78 |'
- en: '| Eyeriss ([Chen_2017,](#bib.bib41) ) | 65 nm | CNN | 16 | 450 | 33.6 | 74.7
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Eyeriss ([Chen_2017,](#bib.bib41) ) | 65 nm | CNN | 16 | 450 | 33.6 | 74.7
    |'
- en: '| STM ([desoli17isscc,](#bib.bib62) ) | 28 nm | CNN | 34.8 | 39 | 750 | 2900
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| STM ([desoli17isscc,](#bib.bib62) ) | 28 nm | CNN | 34.8 | 39 | 750 | 2900
    |'
- en: '| IBM ([oh2020vlsic,](#bib.bib202) ) | 14 nm | CNN+LSTM+RNN | 9.84 | - | 3000
    | 1100 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| IBM ([oh2020vlsic,](#bib.bib202) ) | 14 nm | CNN+LSTM+RNN | 9.84 | - | 3000
    | 1100 |'
- en: '| IBM ([lee2022jsscc,](#bib.bib160) ) | 7 nm | CNN+RNN | 19.6 | - | 16300 |
    3580 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| IBM ([lee2022jsscc,](#bib.bib160) ) | 7 nm | CNN+RNN | 19.6 | - | 16300 |
    3580 |'
- en: IBM presented in ([oh2020vlsic,](#bib.bib202) ) a processor core for AI training
    and inference tasks applicable to a broad range of neural networks (such as CNN,
    LSTM, and RNN). High compute efficiency is achieved for robust FP16 training via
    efficient heterogeneous 2-D systolic array-SIMD compute engines that leverage
    DLFloat16 FPUs. A modular dual-corelet architecture with a shared scratchpad memory
    and a software-controlled network/memory interface enables scalability to many-core
    SoCs for scale-out paradigms. In 2022, IBM also presented a 7-nm four-core mixed-precision
    AI chip ([lee2022jsscc,](#bib.bib160) ) that demonstrates leading-edge power efficiency
    for low-precision training and inference without model accuracy degradation. The
    chip is based on a high-bandwidth ring interconnect to enable efficient data transfers,
    while workload-aware power management with clock frequency throttling maximizes
    the application performance within a given power envelope.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: IBM在 ([oh2020vlsic,](#bib.bib202) ) 中展示了一个用于AI训练和推理任务的处理器核心，适用于广泛的神经网络（如CNN、LSTM和RNN）。通过利用DLFloat16
    FPUs的高效异构2D系统阵列-SIMD计算引擎，实现了对稳健FP16训练的高计算效率。一个模块化双核架构，配有共享的临时存储器和软件控制的网络/内存接口，支持多核SoC的可扩展性以适应扩展范式。2022年，IBM还展示了一款7nm四核混合精度AI芯片
    ([lee2022jsscc,](#bib.bib160) )，在低精度训练和推理中展示了领先的功耗效率，且没有模型准确性降级。该芯片基于高带宽环形互连，以实现高效的数据传输，同时，工作负载感知的功耗管理和时钟频率限制最大限度地提高了给定功率范围内的应用性能。
- en: Qualcomm presented an AI core that is a scalar 4-way VLIW architecture that
    includes vector/tensor units and lower precision to enable high-performance inference
    ([karam2021hotchip,](#bib.bib36) ). The design uses a 7 nm technology and is sought
    to be integrated into the AI 100 SoC to reach up to 149 TOPS with a power efficiency
    of 12.37 TOPS/W.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 高通展示了一个AI核心，这是一个标量4路VLIW架构，包括向量/张量单元和较低精度以实现高性能推理 ([karam2021hotchip,](#bib.bib36)
    )。该设计采用7nm技术，旨在集成到AI 100 SoC中，达到高达149 TOPS的性能，功耗效率为12.37 TOPS/W。
- en: 4.3\. Accelerators based on open-hardware RISC-V
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 基于开源硬件RISC-V的加速器
- en: RISC-V is an open-source, modular instruction set architecture (ISA) that is
    gaining popularity in computer architecture research due to its flexibility and
    suitability for integration with acceleration capabilities for DL. The RISC-V
    ISA is designed with a small, simple core that can be extended with optional instruction
    set extensions (ISEs) to support various application domains. RISC-V offers several
    advantages for DL acceleration research. First, the modular nature of the ISA
    allows researchers to easily integrate acceleration capabilities as ISEs, which
    can be customized to suit the specific needs of different DL models. Second, RISC-V
    supports a set of standard interfaces, such as AXI4, that can be used to interface
    with external acceleration units integrated on the same System-on-Chip at various
    levels of coupling. This makes it easy to integrate specialized DL hardware accelerators
    into RISC-V-based systems. Moreover, the defining feature of the RISC-V ISA is
    its openness, meaning that anybody can design a RISC-V implementation without
    paying royalties or needing a particular license. Thanks to this non-technical
    advantage against other ISAs (such. as ARM, x86), RISC-V has gained significant
    attention from academia and emerging startups.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: RISC-V 是一种开源的、模块化的指令集架构（ISA），由于其灵活性和适合与深度学习加速能力集成的特性，正在计算机架构研究中获得越来越多的关注。RISC-V
    ISA 设计有一个小而简单的核心，可以通过可选的指令集扩展（ISEs）进行扩展，以支持各种应用领域。RISC-V 为深度学习加速研究提供了若干优势。首先，ISA
    的模块化特性使研究人员可以轻松地将加速能力集成到 ISEs 中，这些加速能力可以根据不同深度学习模型的特定需求进行定制。其次，RISC-V 支持一组标准接口，如
    AXI4，可以用于与集成在同一芯片上的外部加速单元进行接口，支持各种耦合级别。这使得将专用的深度学习硬件加速器集成到基于 RISC-V 的系统中变得非常容易。此外，RISC-V
    ISA 的定义特征是其开放性，这意味着任何人都可以设计 RISC-V 实现，而无需支付版权费用或特定许可。由于这一非技术性优势，相比其他 ISA（如 ARM、x86），RISC-V
    获得了学术界和新兴创业公司的显著关注。
- en: '{forest}'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [RISC-V for
    Deep Learning [Specialized ISA & SIMD [ Dustin ([ottaviDustin16CoresParallel2023,](#bib.bib205)
    ), Marsellus ([conti22124TOPS2023,](#bib.bib53) )
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [RISC-V for
    Deep Learning [Specialized ISA & SIMD [ Dustin ([ottaviDustin16CoresParallel2023,](#bib.bib205)
    ), Marsellus ([conti22124TOPS2023,](#bib.bib53) )
- en: Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64) ), Manticore ([zarubaManticore4096CoreRISCV2021,](#bib.bib301)
    )
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64) ), Manticore ([zarubaManticore4096CoreRISCV2021,](#bib.bib301)
    )
- en: Celerity ([davidsonCelerityOpenSource511Core2018,](#bib.bib56) ), Tenstorrent ([vasiljevicComputeSubstrateSoftware2021,](#bib.bib275)
    )
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Celerity ([davidsonCelerityOpenSource511Core2018,](#bib.bib56) ), Tenstorrent ([vasiljevicComputeSubstrateSoftware2021,](#bib.bib275)
    )
- en: exSDOTP ([bertacciniMiniFloatNNExSdotpISA2022,](#bib.bib17) ), Esperanto ([ditzelAcceleratingMLRecommendation2022,](#bib.bib65)
    )
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: exSDOTP ([bertacciniMiniFloatNNExSdotpISA2022,](#bib.bib17) ), Esperanto ([ditzelAcceleratingMLRecommendation2022,](#bib.bib65)
    )
- en: CNC ([chenEightCoreRISCVProcessor2022,](#bib.bib38) ), Cococcioni et al. ([cococcioniLightweightPositPorcessing2021,](#bib.bib51)
    )
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: CNC ([chenEightCoreRISCVProcessor2022,](#bib.bib38) ), Cococcioni et al. ([cococcioniLightweightPositPorcessing2021,](#bib.bib51)
    )
- en: PERCIVAL ([mallasenPERCIVAL2022,](#bib.bib178) ), Wang et al. ([wangWinogradBasedConvolution2021,](#bib.bib288)
    )
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: PERCIVAL ([mallasenPERCIVAL2022,](#bib.bib178) ), Wang et al. ([wangWinogradBasedConvolution2021,](#bib.bib288)
    )
- en: Tang et al. ([tangGeneralPurposeGraphConvolutionNeuralAccelerator2022,](#bib.bib264)
    ), RISC-VTF ([JiaoRISCVExtensionTransformer2021,](#bib.bib127) )
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Tang et al. ([tangGeneralPurposeGraphConvolutionNeuralAccelerator2022,](#bib.bib264)
    ), RISC-VTF ([JiaoRISCVExtensionTransformer2021,](#bib.bib127) )
- en: Paulin et al. ([paulinRNNBasedRadioResource2021,](#bib.bib215) )
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Paulin et al. ([paulinRNNBasedRadioResource2021,](#bib.bib215) )
- en: '] ], [Vector Co-Processor [ Lee et al. ([lee201445nm,](#bib.bib162) ), AVA ([lazoAdaptableRegisterFile2022,](#bib.bib157)
    )'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '] ], [Vector Co-Processor [ Lee et al. ([lee201445nm,](#bib.bib162) ), AVA ([lazoAdaptableRegisterFile2022,](#bib.bib157)
    )'
- en: Spatz ([cavalcanteSpatzCompactVector2022,](#bib.bib33) ), Vitruvius+ ([minerviniVitruviusAreaEfficientRISCV2023,](#bib.bib186)
    )
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Spatz ([cavalcanteSpatzCompactVector2022,](#bib.bib33) ), Vitruvius+ ([minerviniVitruviusAreaEfficientRISCV2023,](#bib.bib186)
    )
- en: Ara ([cavalcanteAra1GHzScalable2020,](#bib.bib32) ), Perotti et al. ([perottiNewAraVector2022,](#bib.bib218)
    )
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Ara ([cavalcanteAra1GHzScalable2020,](#bib.bib32) ), Perotti et al. ([perottiNewAraVector2022,](#bib.bib218)
    )
- en: Xuantie ([chenXuantie910CommercialMultiCore2020,](#bib.bib37) ), Arrow ([assirArrowRISCVVector2021,](#bib.bib15)
    )
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Xuantie ([chenXuantie910CommercialMultiCore2020,](#bib.bib37) ), Arrow ([assirArrowRISCVVector2021,](#bib.bib15)
    )
- en: Ventana ([ventanaProduct,](#bib.bib280) )
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Ventana ([ventanaProduct,](#bib.bib280) )
- en: '] ], [Memory-Coupled NPUs [L1-Coupled [ Darkside ([garofaloDARKSIDEHeterogeneousRISCV2022,](#bib.bib86)
    ), Marsellus ([conti22124TOPS2023,](#bib.bib53) )'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '] ], [内存耦合的 NPU [L1-耦合 [ Darkside ([garofaloDARKSIDEHeterogeneousRISCV2022,](#bib.bib86)
    )，Marsellus ([conti22124TOPS2023,](#bib.bib53) )'
- en: Garofalo et al. ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85) ),
    Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235) )
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Garofalo 等 ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85) )，Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235)
    )
- en: RedMulE ([tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265) ; [tortorellaRedMulECompactFP162022a,](#bib.bib266)
    ), Archimedes ([prasadSpecializationMeetsFlexibility,](#bib.bib222) )
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: RedMulE ([tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265) ; [tortorellaRedMulECompactFP162022a,](#bib.bib266)
    )，Archimedes ([prasadSpecializationMeetsFlexibility,](#bib.bib222) )
- en: Bruschi et al. ([bruschiEndtoEndDNNInference2022,](#bib.bib26) ), GAP9 ([gap9Product,](#bib.bib94)
    )
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Bruschi 等 ([bruschiEndtoEndDNNInference2022,](#bib.bib26) )，GAP9 ([gap9Product,](#bib.bib94)
    )
- en: '] ], [L2-Coupled [ SNCPU ([juSystolicNeuralCPU2023,](#bib.bib136) ), SamurAI ([miro-panadesSamurAIVersatileIoT2022,](#bib.bib187)
    )'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '] ], [L2-耦合 [ SNCPU ([juSystolicNeuralCPU2023,](#bib.bib136) )，SamurAI ([miro-panadesSamurAIVersatileIoT2022,](#bib.bib187)
    )'
- en: Gemmini ([gencGemminiEnablingSystematic2021,](#bib.bib87) ), DIANA ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112)
    )
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Gemmini ([gencGemminiEnablingSystematic2021,](#bib.bib87) )，DIANA ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112)
    )
- en: TinyVers ([jainTinyVersTinyVersatile2023,](#bib.bib121) ), Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64)
    )
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: TinyVers ([jainTinyVersTinyVersatile2023,](#bib.bib121) )，Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64)
    )
- en: Simba ([shao_micro19,](#bib.bib243) ), Lee et al. ([lee64TOPSEnergyEfficientTensor2022,](#bib.bib161)
    )
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Simba ([shao_micro19,](#bib.bib243) )，Lee 等 ([lee64TOPSEnergyEfficientTensor2022,](#bib.bib161)
    )
- en: Tambe et al. ([tambe2212nm182023,](#bib.bib263) )
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Tambe 等 ([tambe2212nm182023,](#bib.bib263) )
- en: '] ], [L3-Coupled [ Gonzalez et al. ([gonzalez16mm106GOPS2021,](#bib.bib92)
    )'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '] ], [L3-耦合 [ Gonzalez 等 ([gonzalez16mm106GOPS2021,](#bib.bib92) )'
- en: ESP ([jia12nmAgileDesignedSoC2022,](#bib.bib124) )
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ESP ([jia12nmAgileDesignedSoC2022,](#bib.bib124) )
- en: '] ], ] ]'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '] ], ] ]'
- en: Figure 5. Taxonomy of RISC-V based acceleration units discussed in Section [4.3](#S4.SS3
    "4.3\. Accelerators based on open-hardware RISC-V ‣ 4\. Hardware Accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. 讨论于第[4.3](#S4.SS3 "4.3\. 基于开源硬件 RISC-V 的加速器 ‣ 4\. 硬件加速器 ‣ 关于异构 HPC 平台深度学习硬件加速器的调查")节的
    RISC-V 基础加速单元分类
- en: Figure [5](#S4.F5 "Figure 5 ‣ 4.3\. Accelerators based on open-hardware RISC-V
    ‣ 4\. Hardware Accelerators ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms") reports a synthetic taxonomy of the RISC-V-based
    architectures for DL acceleration discussed in this survey.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](#S4.F5 "图 5 ‣ 4.3\. 基于开源硬件 RISC-V 的加速器 ‣ 4\. 硬件加速器 ‣ 关于异构 HPC 平台深度学习硬件加速器的调查")展示了本调查中讨论的基于
    RISC-V 的 DL 加速架构的综合分类。
- en: 4.3.1\. RISC-V ISA extensions for (Deep) Learning
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. RISC-V ISA 扩展用于（深度）学习
- en: (Deep) neural networks are often limited by the computing and memory resources
    used by their large number of weights. Weight compression via alternative or quantized
    number representations is often adopted to speed up and optimize the performance
    of neural network models. Works in ([cococcioniLightweightPositPorcessing2021,](#bib.bib51)
    ; [mallasenPERCIVAL2022,](#bib.bib178) ) propose ISA extensions for *posit* numbers
    which can be used to do weight compression. Posit is an alternative representation
    to the standard IEEE floating point format for real numbers. Posit numbers need
    fewer bits to obtain the same precision or dynamic range of IEEE floats allowing
    them to store more weights in a same-sized memory. For example, the work in ([cococcioniLightweightPositPorcessing2021,](#bib.bib51)
    ) provides an efficient conversion between 8- or 16-bit posits and 32-bit IEEE
    floats or fixed point formats with little loss in precision leading to a 10x speedup
    in inference time. Other works directly address the compute-intensive parts of
    different neural networks, in particular CNNs, GCNs, and transformers. ([wangWinogradBasedConvolution2021,](#bib.bib288)
    ) proposes a new Winograd-based convolution instruction to speed up the time-consuming
    convolutional layers in CNNs. The matrix convolution between the CNN kernel and
    the input data cannot be executed efficiently using the standard RISC-V instructions.
    The proposed extension enables to compute a convolution producing a $2\times 2$
    output using a $3\times 3$ kernel on a $4\times 4$ input in a single instruction
    using 19 clock cycles instead of multiple instructions totaling 140 cycles using
    the standard RISC-V ISA. ([tangGeneralPurposeGraphConvolutionNeuralAccelerator2022,](#bib.bib264)
    ) addresses the computational bottlenecks of GCNs by designing a set of general-purpose
    instructions for GCNs that mitigate the compute inefficiencies in aggregating
    and combining feature vectors. As such the authors combine the software programmability
    given by the RISC-V ISA with the compute efficiency of GCN accelerators. Similarly,
    ([JiaoRISCVExtensionTransformer2021,](#bib.bib127) ) focuses on transformer models.
    Notably, the extension comprises instructions to accelerate the well-known ReLU
    activation and softmax functions. Paulin et al. ([paulinRNNBasedRadioResource2021,](#bib.bib215)
    ) performs a similar task but focuses on RNNs.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: （深度）神经网络常常受到其大量权重所占用的计算和内存资源的限制。通过替代或量化数字表示进行权重压缩常被采用，以加快和优化神经网络模型的性能。([cococcioniLightweightPositPorcessing2021,](#bib.bib51)
    ; [mallasenPERCIVAL2022,](#bib.bib178) )的工作提出了针对*posit* 数字的ISA扩展，可用于进行权重压缩。Posit
    是一种替代标准 IEEE 浮点格式的实数表示方法。Posit 数字需要更少的位数来获得与 IEEE 浮点相同的精度或动态范围，从而使它们能在相同大小的内存中存储更多的权重。例如，([cococcioniLightweightPositPorcessing2021,](#bib.bib51)
    )的工作提供了在 8 位或 16 位 posit 和 32 位 IEEE 浮点或定点格式之间的高效转换，几乎没有精度损失，从而实现了推理时间的 10 倍加速。其他工作直接解决了不同神经网络中的计算密集型部分，特别是
    CNN、GCN 和变换器。([wangWinogradBasedConvolution2021,](#bib.bib288) ) 提出了一个基于 Winograd
    的卷积指令，以加快 CNN 中时间消耗大的卷积层。CNN 核心与输入数据之间的矩阵卷积不能使用标准 RISC-V 指令高效执行。所提出的扩展使得在 19 个时钟周期内通过单条指令在
    $4\times 4$ 输入上使用 $3\times 3$ 核心计算 $2\times 2$ 输出，而不是使用标准 RISC-V ISA 需要总计 140
    个周期的多条指令。([tangGeneralPurposeGraphConvolutionNeuralAccelerator2022,](#bib.bib264)
    ) 通过设计一组通用指令来应对 GCN 的计算瓶颈，这些指令缓解了在聚合和组合特征向量时的计算低效。因此，作者将 RISC-V ISA 提供的软件可编程性与
    GCN 加速器的计算效率相结合。类似地，([JiaoRISCVExtensionTransformer2021,](#bib.bib127) ) 关注于变换器模型。值得注意的是，该扩展包括加速著名的
    ReLU 激活函数和 softmax 函数的指令。Paulin 等人 ([paulinRNNBasedRadioResource2021,](#bib.bib215)
    ) 执行了类似的任务，但重点关注 RNN。
- en: Following the trend to use aggressive quantization schemes to accelerate inference
    of DNNs, many ISA extensions focus on low-bit-width arithmetics to implement Quantized
    Neural Networks (QNNs), often combined with multi-core parallel execution to further
    boost performance and efficiency. Several developments augment the PULP RI5CY
    core used in Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235) ) to improve its energy
    efficiency on QNNs. Marsellus ([conti22124TOPS2023,](#bib.bib53) ) (16 cores)
    and Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64) ) (8 cores) use Xpulpnn,
    an ISA extension for low-bitwidth (2/4-bit) integer dot-products used to accelerate
    symmetric precision QNNs. Dustin ([ottaviDustin16CoresParallel2023,](#bib.bib205)
    ) (16 cores) also exploits a similar concept, but it also introduces a lockstep
    mechanism to operate all the cores in a SIMD fashion, further increasing their
    efficiency. Manticore ([zarubaManticore4096CoreRISCV2021,](#bib.bib301) ), Celerity ([davidsonCelerityOpenSource511Core2018,](#bib.bib56)
    ), Esperanto ([ditzelAcceleratingMLRecommendation2022,](#bib.bib65) ) and Tenstorrent ([vasiljevicComputeSubstrateSoftware2021,](#bib.bib275)
    ) exploit ISA extensions for faster RISC-V based DL workload execution in the
    context of many-core architectures where a large number of very simple cores cooperate.
    As these architectures are targeted at server-based training as well as inference,
    they typically focus on floating point multiply-accumulate and dot-product operations,
    such as exSDOTP ([bertacciniMiniFloatNNExSdotpISA2022,](#bib.bib17) ).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 随着使用激进量化方案加速 DNN 推理的趋势，许多 ISA 扩展专注于低位宽算术操作以实现量化神经网络（QNNs），通常结合多核并行执行以进一步提升性能和效率。几项发展增强了用于
    Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235)) 的 PULP RI5CY 核心，以提高其在 QNNs 上的能源效率。Marsellus
    ([conti22124TOPS2023,](#bib.bib53))（16 核）和 Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64))（8
    核）使用 Xpulpnn，这是一种用于加速对称精度 QNNs 的低位宽（2/4 位）整数点积的 ISA 扩展。Dustin ([ottaviDustin16CoresParallel2023,](#bib.bib205))（16
    核）也利用了类似的概念，但它还引入了一种锁步机制，以 SIMD 方式操作所有核心，进一步提高其效率。Manticore ([zarubaManticore4096CoreRISCV2021,](#bib.bib301))、Celerity
    ([davidsonCelerityOpenSource511Core2018,](#bib.bib56))、Esperanto ([ditzelAcceleratingMLRecommendation2022,](#bib.bib65))
    和 Tenstorrent ([vasiljevicComputeSubstrateSoftware2021,](#bib.bib275)) 利用 ISA
    扩展，在多核架构的上下文中加速 RISC-V 基于 DL 的工作负载，其中大量非常简单的核心协作。由于这些架构旨在用于基于服务器的训练和推理，因此它们通常专注于浮点乘加和点积操作，如
    exSDOTP ([bertacciniMiniFloatNNExSdotpISA2022,](#bib.bib17))。
- en: 4.3.2\. RISC-V Vector Co-processors
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. RISC-V 矢量协处理器
- en: Vector co-processors represent a sort of natural architectural target for Deep
    Learning-oriented RISC-V acceleration. Commercial RISC-V vector processors mainly
    targeted at HPC markets, such as Xuantie ([chenXuantie910CommercialMultiCore2020,](#bib.bib37)
    ), and Ventana ([ventanaProduct,](#bib.bib280) ), have recently started appearing.
    In addition, vector co-processors explicitly tailored for DL, like Spatz ([cavalcanteSpatzCompactVector2022,](#bib.bib33)
    ) and Arrow ([assirArrowRISCVVector2021,](#bib.bib15) ), have been developed.
    The former in particular focuses only on a subset of the V extension and on 32-bit
    data, capturing better opportunities for energy efficiency.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 矢量协处理器代表了一种自然的架构目标，用于深度学习导向的 RISC-V 加速。商业 RISC-V 矢量处理器主要针对 HPC 市场，例如 Xuantie
    ([chenXuantie910CommercialMultiCore2020,](#bib.bib37)) 和 Ventana ([ventanaProduct,](#bib.bib280))，最近开始出现。此外，专门针对
    DL 的矢量协处理器，如 Spatz ([cavalcanteSpatzCompactVector2022,](#bib.bib33)) 和 Arrow ([assirArrowRISCVVector2021,](#bib.bib15))，也已开发。其中，前者特别专注于
    V 扩展的一个子集和 32 位数据，捕捉到更好的能源效率机会。
- en: 4.3.3\. RISC-V Memory-coupled Neural Processing Units (NPUs)
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. RISC-V 内存耦合神经处理单元（NPUs）
- en: Concerning the tightest kind of memory coupling, at L1, most proposals in the
    state-of-the-art are based on the Parallel Ultra-Low Power (PULP) template, and
    devote significant effort to enabling fast communication between RISC-V cores
    and accelerators. Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235) ) is a prototype
    system coupling 9 RI5CY cores with a quantized DNN convolutional NPU sharing directly
    L1 scratchpad memory with the cores. GreenWaves Technologies GAP9 ([gap9Product,](#bib.bib94)
    ) is a commercial product, targeted at the wearables market, that follows the
    same line with many architectural improvements and a redesigned AI NPU for QNNs
    – leading to the product achieving the best performance and energy efficiency
    in the public TinyMLPerf Tiny 1.0 challenge as of this writing ([tinyMLPerf2023,](#bib.bib189)
    ). Archimedes ([prasadSpecializationMeetsFlexibility,](#bib.bib222) ) proposes
    a large AI hardware accelerator for performance-hungry Extended Reality applications.
    RedMulE ([tortorellaRedMulECompactFP162022a,](#bib.bib266) ; [tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265)
    ), integrated into the Darkside prototype ([garofaloDARKSIDEHeterogeneousRISCV2022,](#bib.bib86)
    ), follows the same principles but focusing on floating-point computation to support
    training as well as inference. Garofalo et al. ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85)
    ) and Bruschi et al. ([bruschiEndtoEndDNNInference2022,](#bib.bib26) ) integrate
    in-memory-computing PCM-based NPUs, the latter simulating a system scaled up to
    match the size of server-class hardware.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 关于最紧密的内存耦合，在L1层，大多数最先进的提案基于Parallel Ultra-Low Power (PULP)模板，并投入大量精力以实现RISC-V核心与加速器之间的快速通信。Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235))
    是一个原型系统，将9个RI5CY核心与一个量化的DNN卷积NPU耦合在一起，NPU直接与核心共享L1片内存。GreenWaves Technologies
    GAP9 ([gap9Product,](#bib.bib94)) 是一款针对可穿戴设备市场的商业产品，它遵循相同的思路，并进行了许多架构改进和重新设计的AI
    NPU用于QNNs——使得该产品在撰写本文时在公共TinyMLPerf Tiny 1.0挑战中实现了最佳性能和能效([tinyMLPerf2023,](#bib.bib189))。Archimedes ([prasadSpecializationMeetsFlexibility,](#bib.bib222))
    提出了一个大型AI硬件加速器，针对对性能要求高的扩展现实应用。RedMulE ([tortorellaRedMulECompactFP162022a,](#bib.bib266);
    [tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265))，集成在Darkside原型中 ([garofaloDARKSIDEHeterogeneousRISCV2022,](#bib.bib86))，遵循相同的原则，但重点关注浮点计算，以支持训练和推理。Garofalo 等 ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85))
    和 Bruschi 等 ([bruschiEndtoEndDNNInference2022,](#bib.bib26)) 集成了基于PCM的内存计算NPU，后者模拟了一个扩大到匹配服务器级硬件规模的系统。
- en: <svg   height="604.86" overflow="visible" version="1.1" width="1746.04"><g transform="translate(0,604.86)
    matrix(1 0 0 -1 0 0) translate(236.2,0) translate(0,4.86)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g clip-path="url(#pgfcp1)"><g stroke="#262626" fill="#262626"
    transform="matrix(1.0 0.0 0.0 1.0 99.52 40.47)"><foreignobject width="17.71" height="11.41"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{1}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 369.08 40.47)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{2}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 638.64 40.47)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{3}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 908.2 40.47)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{4}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1177.76 40.47)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{5}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 610.1 22.44)"><foreignobject
    width="75.18" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Power
    [mW]</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 73.37 71.19)"><foreignobject width="17.71" height="11.41" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{1}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 73.37 188.73)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{2}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 73.37 306.27)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{3}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 73.37 423.8)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{4}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 73.37 541.34)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{5}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(0.0 1.0 -1.0 0.0 64.36 285.61)"><foreignobject
    width="75.85" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Perf
    [GOPS]</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 24.6 188.26)"><foreignobject width="381.86" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SNCPU ([juSystolicNeuralCPU2023,](#bib.bib136)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 441.15 146.66)"><foreignobject width="415.69" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Dustin ([ottaviDustin16CoresParallel2023,](#bib.bib205)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 375.58 96.6)"><foreignobject width="511.55" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Darkside ([garofaloDARKSIDEHeterogeneousRISCV2022,](#bib.bib86)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 -86.21 150.18)"><foreignobject width="469.31" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SamurAI ([miro-panadesSamurAIVersatileIoT2022,](#bib.bib187)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 413.33 283.01)"><foreignobject width="364.84" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Marsellus ([conti22124TOPS2023,](#bib.bib53)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 436.56 317.68)"><foreignobject width="558.5" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Garofalo et al. ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 421.59 244.85)"><foreignobject width="472.19" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">DIANA ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 200.68 113.65)"><foreignobject width="418.22" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">TinyVers ([jainTinyVersTinyVersatile2023,](#bib.bib121)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 309.84 187.65)"><foreignobject width="416.11" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 306.53 130.65)"><foreignobject width="366.6" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 -236.2 241.25)"><foreignobject width="702.27" height="44.74" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">RedMulE ([tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265)
    ; [tortorellaRedMulECompactFP162022a,](#bib.bib266) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 666.57 255.79)"><foreignobject
    width="428.68" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Spatz
    ([cavalcanteSpatzCompactVector2022,](#bib.bib33) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 470.24 124.65)"><foreignobject
    width="467.42" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Manticore
    ([zarubaManticore4096CoreRISCV2021,](#bib.bib301) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 567.49 110.51)"><foreignobject
    width="510.05" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Vitruvius+
    ([minerviniVitruviusAreaEfficientRISCV2023,](#bib.bib186) )</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 631.65 144.81)"><foreignobject
    width="396.14" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Ara
    ([cavalcanteAra1GHzScalable2020,](#bib.bib32) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 509.63 86.8)"><foreignobject
    width="412.46" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Perotti
    et al. ([perottiNewAraVector2022,](#bib.bib218) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 -66.37 329.09)"><foreignobject
    width="468.73" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Archimedes
    ([prasadSpecializationMeetsFlexibility,](#bib.bib222) )</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 674.59 532.16)"><foreignobject
    width="463.39" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Lee
    et al. ([lee64TOPSEnergyEfficientTensor2022,](#bib.bib161) )</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 778.65 507.27)"><foreignobject
    width="418.26" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Axelera
    AI ([ward-foxtonAxeleraDemosAI,](#bib.bib290) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 19.79 303.78)"><foreignobject
    width="379.83" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Tambe
    et al. ([tambe2212nm182023,](#bib.bib263) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 508.79 226.33)"><foreignobject
    width="482.87" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">exSDOTP
    ([bertacciniMiniFloatNNExSdotpISA2022,](#bib.bib17) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1009.36 557.91)"><foreignobject
    width="500.48" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Esperanto
    ([ditzelAcceleratingMLRecommendation2022,](#bib.bib65) )</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 789.97 472.79)"><foreignobject
    width="477.72" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Bruschi
    et al. ([bruschiEndtoEndDNNInference2022,](#bib.bib26) )</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 579.83 188.19)"><foreignobject
    width="425.26" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CNC
    ([chenEightCoreRISCVProcessor2022,](#bib.bib38) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1210.69 445.45)"><foreignobject
    width="54.23" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Maturity</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 423.87)"><foreignobject
    width="39.97" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Silicon</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 402.58)"><foreignobject
    width="63.34" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Pre-silicon</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 381.29)"><foreignobject
    width="64.96" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Simulation</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1210.69 359.12)"><foreignobject
    width="65.53" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Data
    Type</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 1241.25 337.55)"><foreignobject width="32.29" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">FP64</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 316.26)"><foreignobject
    width="32.29" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FP16</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 294.97)"><foreignobject
    width="25.37" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FP8</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 273.68)"><foreignobject
    width="25.37" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FP4</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 252.39)"><foreignobject
    width="39.2" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">INT32</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 231.1)"><foreignobject
    width="32.29" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">INT8</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 209.81)"><foreignobject
    width="81.1" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">INT2
    x INT8</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 1241.25 188.52)"><foreignobject width="81.1" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">INT2 x INT4</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 167.23)"><foreignobject
    width="32.29" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">INT2</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 145.93)"><foreignobject
    width="42.66" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Analog</foreignobject></g></g></g></svg>
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="604.86" overflow="visible" version="1.1" width="1746.04"><g transform="translate(0,604.86)
    matrix(1 0 0 -1 0 0) translate(236.2,0) translate(0,4.86)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g clip-path="url(#pgfcp1)"><g stroke="#262626" fill="#262626"
    transform="matrix(1.0 0.0 0.0 1.0 99.52 40.47)"><foreignobject width="17.71" height="11.41"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{1}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 369.08 40.47)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{2}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 638.64 40.47)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{3}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 908.2 40.47)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{4}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1177.76 40.47)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{5}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 610.1 22.44)"><foreignobject
    width="75.18" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">功率
    [mW]</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 73.37 71.19)"><foreignobject width="17.71" height="11.41" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{1}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 73.37 188.73)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{2}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 73.37 306.27)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{3}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 73.37 423.8)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{4}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 73.37 541.34)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{5}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(0.0 1.0 -1.0 0.0 64.36 285.61)"><foreignobject
    width="75.85" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">性能
    [GOPS]</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 24.6 188.26)"><foreignobject width="381.86" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SNCPU ([juSystolicNeuralCPU2023,](#bib.bib136))</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 441.15 146.66)"><foreignobject
    width="415.69" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Dustin
    ([ottaviDustin16CoresParallel2023,](#bib.bib205))</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 375.58 96.6)"><foreignobject
    width="511.55" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Darkside
    ([garofaloDARKSIDEHeterogeneousRISCV2022,](#bib.bib86))</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 -86.21 150.18)"><foreignobject
    width="469.31" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SamurAI
    ([miro-panadesSamurAIVersatileIoT2022,](#bib.bib187))</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 413.33 283.01)"><foreignobject
    width="364.84" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Marsellus
    ([conti22124TOPS2023,](#bib.bib53))</foreignobject></g><g stroke="#262626" fill="#262626"
    transform="matrix(1.0 0.0 0.0 1.0 436.56 317.68)"><foreignobject width="558.5"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Garofalo
    et al. ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85))</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 421.59 244.85)"><foreignobject
    width="472.19" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DIANA
    ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112))</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 200.68 113.65)"><foreignobject
    width="418.22" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TinyVers
    ([jainTinyVersTinyVersatile2023,](#bib.bib121))</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 309.84 187.65)
- en: Figure 6. Performance and power consumption of prototypes of several State-of-the-Art
    Deep Learning acceleration architectures discussed in Section [4.3](#S4.SS3 "4.3\.
    Accelerators based on open-hardware RISC-V ‣ 4\. Hardware Accelerators ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms").
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6. 讨论的几种最先进深度学习加速架构的性能和功耗，详见第 [4.3](#S4.SS3 "4.3\. 基于开放硬件 RISC-V 的加速器 ‣ 4\.
    硬件加速器 ‣ 深度学习硬件加速器的综述") 节
- en: Table 6. Summary of RISC-V Deep Learning acceleration architectures
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6. RISC-V 深度学习加速架构汇总
- en: '| Category | Accelerator | Tech [nm] | Area [mm2] | Freq [MHz] | Voltage [V]
    | Power [mW] | Perf [GOPS] | Eff [GOPS/W] | # MAC units | Data Type | Maturity
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 加速器 | 技术 [nm] | 面积 [mm2] | 频率 [MHz] | 电压 [V] | 功耗 [mW] | 性能 [GOPS] |
    效率 [GOPS/W] | # MAC 单元 | 数据类型 | 成熟度 |'
- en: '| ISA | Dustin ([ottaviDustin16CoresParallel2023,](#bib.bib205) ) | 65nm |
    10 | 205 | 1.2 | 156 | 33.6 | 215 | 128 | INT2 x INT4 | Silicon |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| ISA | Dustin ([ottaviDustin16CoresParallel2023,](#bib.bib205) ) | 65nm |
    10 | 205 | 1.2 | 156 | 33.6 | 215 | 128 | INT2 x INT4 | 硅 |'
- en: '| Kraken (RISC-V cores) ([dimauroKrakenDirectEvent2022,](#bib.bib64) ) | 22nm
    | 9 | 330 | 0.8 | 300 | 75 | 750 | 128 | INT2 | Silicon |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Kraken (RISC-V 核心) ([dimauroKrakenDirectEvent2022,](#bib.bib64) ) | 22nm
    | 9 | 330 | 0.8 | 300 | 75 | 750 | 128 | INT2 | 硅 |'
- en: '| Manticore ([zarubaManticore4096CoreRISCV2021,](#bib.bib301) ) | 22nm | 888
    | 500 | 0.6 | 200 | 25 | 188 | 24 | FP64 | Silicon |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Manticore ([zarubaManticore4096CoreRISCV2021,](#bib.bib301) ) | 22nm | 888
    | 500 | 0.6 | 200 | 25 | 188 | 24 | FP64 | 硅 |'
- en: '| Celerity ([davidsonCelerityOpenSource511Core2018,](#bib.bib56) ) | 16nm |
    25 | 1050 | - | 1900 | - | - | 496 | INT32 | Silicon |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Celerity ([davidsonCelerityOpenSource511Core2018,](#bib.bib56) ) | 16nm |
    25 | 1050 | - | 1900 | - | - | 496 | INT32 | 硅 |'
- en: '| Tenstorrent ([vasiljevicComputeSubstrateSoftware2021,](#bib.bib275) ) | 12nm
    | 477 | - | - | - | 92000 | - | - | FP16 | Silicon |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Tenstorrent ([vasiljevicComputeSubstrateSoftware2021,](#bib.bib275) ) | 12nm
    | 477 | - | - | - | 92000 | - | - | FP16 | 硅 |'
- en: '| exSDOTP ([bertacciniMiniFloatNNExSdotpISA2022,](#bib.bib17) ) | 12nm | 0.52
    | 1260 | 0.8 | 278 | 160 | 575 | 16 | FP8 | Pre-silicon |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| exSDOTP ([bertacciniMiniFloatNNExSdotpISA2022,](#bib.bib17) ) | 12nm | 0.52
    | 1260 | 0.8 | 278 | 160 | 575 | 16 | FP8 | 前硅 |'
- en: '| Esperanto ([ditzelAcceleratingMLRecommendation2022,](#bib.bib65) ) | 7nm
    | 570 | 1000 | - | 20000 | 139000 | 6.95 | 69632 | INT8 | Silicon |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 世界语 ([ditzelAcceleratingMLRecommendation2022,](#bib.bib65) ) | 7nm | 570
    | 1000 | - | 20000 | 139000 | 6.95 | 69632 | INT8 | 硅 |'
- en: '| CNC ([chenEightCoreRISCVProcessor2022,](#bib.bib38) ) | 4nm | 1.92 | 1150
    | 0.85 | 510 | 75.8 | 149 | 512 | INT8 | Silicon |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| CNC ([chenEightCoreRISCVProcessor2022,](#bib.bib38) ) | 4nm | 1.92 | 1150
    | 0.85 | 510 | 75.8 | 149 | 512 | INT8 | 硅 |'
- en: '| Vector | Lee et al. ([lee64TOPSEnergyEfficientTensor2022,](#bib.bib161) )
    | 14nm | 181 | 2000 | 0.8 | 60000 | 64000 | 1450 | 16384 | INT8 | Silicon |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Vector | Lee 等 ([lee64TOPSEnergyEfficientTensor2022,](#bib.bib161) ) | 14nm
    | 181 | 2000 | 0.8 | 60000 | 64000 | 1450 | 16384 | INT8 | 硅 |'
- en: '| AVA ([lazoAdaptableRegisterFile2022,](#bib.bib157) ) | 22nm | 3.9 | - | -
    | - | - | - | - | FP64 | Pre-silicon |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| AVA ([lazoAdaptableRegisterFile2022,](#bib.bib157) ) | 22nm | 3.9 | - | -
    | - | - | - | - | FP64 | 前硅 |'
- en: '| Spatz ([cavalcanteSpatzCompactVector2022,](#bib.bib33) ) | 22nm | 20 | 594
    | 0.8 | 1070 | 285 | 266 | 256 | INT32 | Pre-silicon |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Spatz ([cavalcanteSpatzCompactVector2022,](#bib.bib33) ) | 22nm | 20 | 594
    | 0.8 | 1070 | 285 | 266 | 256 | INT32 | 前硅 |'
- en: '| Vitruvius+ ([minerviniVitruviusAreaEfficientRISCV2023,](#bib.bib186) ) |
    22nm | 1.3 | 1400 | 0.8 | 459 | 21.7 | 47.3 | 8 | FP64 | Pre-silicon |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Vitruvius+ ([minerviniVitruviusAreaEfficientRISCV2023,](#bib.bib186) ) |
    22nm | 1.3 | 1400 | 0.8 | 459 | 21.7 | 47.3 | 8 | FP64 | 前硅 |'
- en: '| Ara ([cavalcanteAra1GHzScalable2020,](#bib.bib32) ) | 22nm | 10735 kGE |
    1040 | 0.8 | 794 | 32.4 | 40.8 | 16 | FP64 | Pre-silicon |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Ara ([cavalcanteAra1GHzScalable2020,](#bib.bib32) ) | 22nm | 10735 kGE |
    1040 | 0.8 | 794 | 32.4 | 40.8 | 16 | FP64 | 前硅 |'
- en: '| Perotti et al. ([perottiNewAraVector2022,](#bib.bib218) ) | 22nm | 0.81 |
    1340 | 0.8 | 280 | 10.4 | 37.1 | 4 | FP64 | Pre-silicon |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Perotti 等 ([perottiNewAraVector2022,](#bib.bib218) ) | 22nm | 0.81 | 1340
    | 0.8 | 280 | 10.4 | 37.1 | 4 | FP64 | 前硅 |'
- en: '| L1 NPU | Darkside ([garofaloDARKSIDEHeterogeneousRISCV2022,](#bib.bib86)
    ) | 65nm | 3.85 | 200 | 1.2 | 89.1 | 12.6 | 152 | 32 | FP16 | Silicon |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| L1 NPU | Darkside ([garofaloDARKSIDEHeterogeneousRISCV2022,](#bib.bib86)
    ) | 65nm | 3.85 | 200 | 1.2 | 89.1 | 12.6 | 152 | 32 | FP16 | 硅 |'
- en: '| Marsellus (NPU) ([conti22124TOPS2023,](#bib.bib53) ) | 22nm | 18.7 | 420
    | 0.8 | 123 | 637 | 7600 | 10368 1–bit | INT2 | Silicon |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Marsellus (NPU) ([conti22124TOPS2023,](#bib.bib53) ) | 22nm | 18.7 | 420
    | 0.8 | 123 | 637 | 7600 | 10368 1–bit | INT2 | 硅 |'
- en: '| Garofalo et al. ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85)
    ) | 22nm | 30 | 500 | 0.8 | 150 | 958 | 6390 | 36 (DW) | INT8 | Pre-silicon |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Garofalo 等 ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85) ) |
    22nm | 30 | 500 | 0.8 | 150 | 958 | 6390 | 36 (DW) | INT8 | 前硅 |'
- en: '| Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235) ) | 22nm | 12 | 450 | 0.8 |
    49.4 | 32.2 | 651 | 27 | INT8 | Silicon |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235) ) | 22nm | 12 | 450 | 0.8 |
    49.4 | 32.2 | 651 | 27 | INT8 | 硅 |'
- en: '| RedMulE ([tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265)
    ; [tortorellaRedMulECompactFP162022a,](#bib.bib266) ) | 22nm | 0.73 | 613 | 0.8
    | 193 | 117 | 608 | 96 | FP16 | Pre-silicon |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| RedMulE ([tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265)
    ; [tortorellaRedMulECompactFP162022a,](#bib.bib266) ) | 22nm | 0.73 | 613 | 0.8
    | 193 | 117 | 608 | 96 | FP16 | 预硅 |'
- en: '| Archimedes ([prasadSpecializationMeetsFlexibility,](#bib.bib222) ) | 22nm
    | 3.38 | 270 | 0.65 | 112 | 1198 | 10.6 | 5184 | INT2 x INT8 | Pre-silicon |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Archimedes ([prasadSpecializationMeetsFlexibility,](#bib.bib222) ) | 22nm
    | 3.38 | 270 | 0.65 | 112 | 1198 | 10.6 | 5184 | INT2 x INT8 | 预硅 |'
- en: '| Bruschi et al. ([bruschiEndtoEndDNNInference2022,](#bib.bib26) ) | 5nm |
    480 | - | - | 3070 | 20000 | 6500 | 3.35$\times$10⁷ | Analog | Simulation |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Bruschi 等 ([bruschiEndtoEndDNNInference2022,](#bib.bib26) ) | 5nm | 480 |
    - | - | 3070 | 20000 | 6500 | 3.35$\times$10⁷ | 模拟 | 模拟 |'
- en: '| L2 NPU | SNCPU ([juSystolicNeuralCPU2023,](#bib.bib136) ) | 65nm | 4.47 |
    400 | 1 | 116 | 75.9 | 655 | 100 | INT8 | Silicon |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| L2 NPU | SNCPU ([juSystolicNeuralCPU2023,](#bib.bib136) ) | 65nm | 4.47 |
    400 | 1 | 116 | 75.9 | 655 | 100 | INT8 | 硅 |'
- en: '| SamurAI ([miro-panadesSamurAIVersatileIoT2022,](#bib.bib187) ) | 28nm | 4.52
    | 350 | 0.9 | 94.7 | 36 | 380 | 64 | INT8 | Silicon |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| SamurAI ([miro-panadesSamurAIVersatileIoT2022,](#bib.bib187) ) | 28nm | 4.52
    | 350 | 0.9 | 94.7 | 36 | 380 | 64 | INT8 | 硅 |'
- en: '| Gemmini ([gencGemminiEnablingSystematic2021,](#bib.bib87) ) | 22nm | 1.03
    | 1000 | - | - | - | - | 256 | INT8 | Pre-silicon |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Gemmini ([gencGemminiEnablingSystematic2021,](#bib.bib87) ) | 22nm | 1.03
    | 1000 | - | - | - | - | 256 | INT8 | 预硅 |'
- en: '| DIANA (digital) ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112) ) | 22nm
    | 10.24 | 280 | 0.8 | 132 | 230 | 1740 | 256 | INT8 | Silicon |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| DIANA (数字) ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112) ) | 22nm | 10.24
    | 280 | 0.8 | 132 | 230 | 1740 | 256 | INT8 | 硅 |'
- en: '| DIANA (analog) ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112) ) | 22nm
    | 10.24 | 350 | 0.8 | 132 | 18100 | 176000 | 256 | INT8 | Silicon |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| DIANA (模拟) ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112) ) | 22nm | 10.24
    | 350 | 0.8 | 132 | 18100 | 176000 | 256 | INT8 | 硅 |'
- en: '| TinyVers ([jainTinyVersTinyVersatile2023,](#bib.bib121) ) | 22nm | 6.25 |
    150 | 0.8 | 20 | 17.6 | 863 | 64 | INT8 | Silicon |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| TinyVers ([jainTinyVersTinyVersatile2023,](#bib.bib121) ) | 22nm | 6.25 |
    150 | 0.8 | 20 | 17.6 | 863 | 64 | INT8 | 硅 |'
- en: '| Simba ([shao_micro19,](#bib.bib243) ) | 16nm | 6 | 161 | 0.42 | - | - | 9100
    | 1024 | INT8 | Silicon |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Simba ([shao_micro19,](#bib.bib243) ) | 16nm | 6 | 161 | 0.42 | - | - | 9100
    | 1024 | INT8 | 硅 |'
- en: '| Axelera AI ([ward-foxtonAxeleraDemosAI,](#bib.bib290) ) | 12nm | 9 | 800
    | - | 2787 | 39300 | 14100 | - | INT8 | Silicon |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Axelera AI ([ward-foxtonAxeleraDemosAI,](#bib.bib290) ) | 12nm | 9 | 800
    | - | 2787 | 39300 | 14100 | - | INT8 | 硅 |'
- en: '| Tambe et al. ([tambe2212nm182023,](#bib.bib263) ) | 12nm | 4.59 | 717 | 1
    | 111 | 734 | 6612 | - | FP4 | Silicon |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Tambe 等 ([tambe2212nm182023,](#bib.bib263) ) | 12nm | 4.59 | 717 | 1 | 111
    | 734 | 6612 | - | FP4 | 硅 |'
- en: '| L3 NPU | Gonzalez et al. ([gonzalez16mm106GOPS2021,](#bib.bib92) ) | 22nm
    | 16 | 961 | - | - | - | 106.1 | 256 | INT8 | Silicon |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| L3 NPU | Gonzalez 等 ([gonzalez16mm106GOPS2021,](#bib.bib92) ) | 22nm | 16
    | 961 | - | - | - | 106.1 | 256 | INT8 | 硅 |'
- en: '| ESP ([jia12nmAgileDesignedSoC2022,](#bib.bib124) ) | 12nm | 21.6 | 1520 |
    1 | 1830 | - | - | 3x NVDLA | INT8 | Silicon |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| ESP ([jia12nmAgileDesignedSoC2022,](#bib.bib124) ) | 12nm | 21.6 | 1520 |
    1 | 1830 | - | - | 3x NVDLA | INT8 | 硅 |'
- en: Moving the shared memory from L1 to L2/L3, there are other NPU solutions in
    the state-of-the-art. In these architectures, RISC-V open-source cores are flexible
    and easy to be integrated. Systems exploiting this template include, for example,
    SNCPU ([juSystolicNeuralCPU2023,](#bib.bib136) ), which builds a hybrid system
    that can act as either a set of 10 RISC-V cores or be reconfigured in a systolic
    NPU. Gonzalez et al. ([gonzalez16mm106GOPS2021,](#bib.bib92) ) and Genc et al. ([gencGemminiEnablingSystematic2021,](#bib.bib87)
    ) exploit a systolic array generation tool, Gemmini, to generate systolic arrays
    to accelerate DNNs and coupled with a RISC-V core by exploiting a shared L3 or
    L2 memory, respectively. Simba ([shao_micro19,](#bib.bib243) ) follows a similar
    template and is also meant to be scaled towards server-grade performance using
    the integration of chiplets on multi-chip modules. ESP ([giriESP4MLPlatformBasedDesign2020,](#bib.bib89)
    ; [giriAcceleratorIntegrationOpenSource2021,](#bib.bib90) ) and Tambe et al. ([tambe2212nm182023,](#bib.bib263)
    ) also focus on integrating hardware accelerators and NPUs in large-scale Network-on-Chips
    using RISC-V cores as computing engines. On the other end of the spectrum, SamurAI ([miro-panadesSamurAIVersatileIoT2022,](#bib.bib187)
    ), TinyVers ([jainTinyVersTinyVersatile2023,](#bib.bib121) ), and DIANA ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112)
    ) build up AI-IoT systems composed of a microcontroller and L2-coupled NPUs (in
    the case of DIANA both an analog SRAM-IMC-based accelerator and a conventional
    digital one). Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64) ) couples the
    aforementioned RISC-V ISA-extended cluster with specialized L2-coupled Spiking
    Neural Network (SNN) and Ternary Neural Network (TNN) accelerators.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 将共享内存从L1移动到L2/L3，当前最先进的NPU解决方案还有其他选择。在这些架构中，RISC-V开源核心具有灵活性，易于集成。例如，SNCPU ([juSystolicNeuralCPU2023,](#bib.bib136)
    )构建了一个混合系统，可以作为10个RISC-V核心的集合使用，或者重新配置为一个流脉动NPU。Gonzalez 等 ([gonzalez16mm106GOPS2021,](#bib.bib92)
    )和Genc 等 ([gencGemminiEnablingSystematic2021,](#bib.bib87) )利用流脉动阵列生成工具Gemmini生成流脉动阵列，以加速DNN，并通过分别利用共享的L3或L2内存与RISC-V核心耦合。Simba ([shao_micro19,](#bib.bib243)
    )遵循类似的模板，也旨在通过在多芯片模块上集成芯片来实现服务器级别的性能。ESP ([giriESP4MLPlatformBasedDesign2020,](#bib.bib89)
    ; [giriAcceleratorIntegrationOpenSource2021,](#bib.bib90) )和Tambe 等 ([tambe2212nm182023,](#bib.bib263)
    )也专注于在大型网络芯片中集成硬件加速器和NPU，使用RISC-V核心作为计算引擎。另一方面，SamurAI ([miro-panadesSamurAIVersatileIoT2022,](#bib.bib187)
    )、TinyVers ([jainTinyVersTinyVersatile2023,](#bib.bib121) )和DIANA ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112)
    )构建了由微控制器和L2耦合NPU（DIANA的情况下既有基于模拟SRAM-IMC的加速器，也有传统数字加速器）组成的AI-IoT系统。Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64)
    )将上述RISC-V ISA扩展集群与专门的L2耦合尖峰神经网络（SNN）和三元神经网络（TNN）加速器配对。
- en: Figure [6](#S4.F6 "Figure 6 ‣ 4.3.3\. RISC-V Memory-coupled Neural Processing
    Units (NPUs) ‣ 4.3\. Accelerators based on open-hardware RISC-V ‣ 4\. Hardware
    Accelerators ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms") summarizes performance, power, technological maturity, and data
    types used in the architectures discussed in this section. RISC-V-based solutions
    occupy essentially the full spectrum of DL architectures ranging from 10 mW microcontrollers
    up to 100 W SoCs meant to be integrated as part of HPC systems. So far, most of
    the research has focused on the lower end of this spectrum, striving for the best
    energy efficiency. We can observe how efficiency is strongly correlated with architectural
    techniques yielding accuracy (e.g., data bit-width reduction & quantization).
    Table [6](#S4.T6 "Table 6 ‣ 4.3.3\. RISC-V Memory-coupled Neural Processing Units
    (NPUs) ‣ 4.3\. Accelerators based on open-hardware RISC-V ‣ 4\. Hardware Accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
    summarizes all quantitative information available on the discussed architectures;
    whenever multiple operating points have been reported for a single architecture,
    the table reports always the highest performance (for consistency, this applies
    to both performance and energy efficiency values).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6](#S4.F6 "图 6 ‣ 4.3.3\. RISC-V 内存耦合神经处理单元 (NPUs) ‣ 4.3\. 基于开放硬件RISC-V的加速器
    ‣ 4\. 硬件加速器 ‣ 深度学习硬件加速器在异构HPC平台上的调查") 总结了本节讨论的架构在性能、功耗、技术成熟度和数据类型方面的信息。基于RISC-V的解决方案几乎覆盖了DL架构的整个范围，从10
    mW的微控制器到100 W的系统级芯片（SoCs），这些SoCs旨在作为HPC系统的一部分进行集成。到目前为止，大多数研究集中在该范围的低端，力求最佳能效。我们可以观察到效率与架构技术（如数据位宽减少和量化）对准确性的强相关性。表 [6](#S4.T6
    "表 6 ‣ 4.3.3\. RISC-V 内存耦合神经处理单元 (NPUs) ‣ 4.3\. 基于开放硬件RISC-V的加速器 ‣ 4\. 硬件加速器 ‣
    深度学习硬件加速器在异构HPC平台上的调查") 总结了讨论的架构中的所有定量信息；每当报告了单个架构的多个操作点时，表格总是报告最高性能（为了一致性，这适用于性能和能效值）。
- en: 5\. Accelerators based on Emerging Paradigms and Technologies
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 基于新兴范式和技术的加速器
- en: 5.1\. Accelerators for Sparse Matrices
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 稀疏矩阵加速器
- en: This section overviews accelerator architectures purposely designed to take
    advantage of computations suitable to sparse matrices. Definitions, storage formats
    appropriate for sparse matrices, and their impacts on the computational complexity
    of DL models are discussed in Appendix A.5.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了专门设计以利用适合稀疏矩阵计算的加速器架构。稀疏矩阵的定义、适用的存储格式以及它们对DL模型计算复杂度的影响在附录A.5中讨论。
- en: Some accelerators (e.g., Cnvlutin, Cambricon-X, Eyeriss) handle only one-sided
    sparsity, which stems either from zero-valued activations or network pruning,
    thus achieving only a partial reduction in compute and data reduction. On the
    other hand, other accelerators (e.g., SCNN, SparTen, Eyeriss v2) target two-sided
    sparsity, which originates from network pruning and zero-valued activations. In
    addition to the different approaches to exploiting sparsity, these architectures
    also employ distinct dataflows to execute the DNN layers. Due to the complexity
    of the logic, existing hardware accelerators for sparse processing are typically
    limited to a specific layer type (e.g., fully-connected layers, convolutional
    layers).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一些加速器（例如，Cnvlutin、Cambricon-X、Eyeriss）仅处理单边稀疏性，这种稀疏性源自零值激活或网络剪枝，因此仅实现了计算和数据减少的部分效果。另一方面，其他加速器（例如，SCNN、SparTen、Eyeriss
    v2）针对双边稀疏性，这种稀疏性源自网络剪枝和零值激活。除了利用稀疏性的不同方法外，这些架构还采用了不同的数据流来执行DNN层。由于逻辑的复杂性，现有的稀疏处理硬件加速器通常限于特定层类型（例如，全连接层、卷积层）。
- en: Eyeriss ([Chen_2017,](#bib.bib41) ) targets CNN acceleration by storing in DRAM
    only nonzero-valued activations in Compressed Sparse Columns (CSC) format and
    by skipping zero-valued activations (by means of gating the datapath switching
    and memory accesses) to save energy. Eyeriss v2 ([eyerissv2:2019,](#bib.bib42)
    ), which targets DNNs on mobile devices, also supports sparse network models.
    It utilizes the CSC format to store weights and activations, which are kept compressed
    not only in memory but also during processing. To improve flexibility, it uses
    a hierarchical mesh for the PEs interconnections. By means of these optimizations,
    Eyeriss v2 is significantly faster and more energy-efficient than the original
    Eyeriss.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Eyeriss ([Chen_2017,](#bib.bib41)) 通过仅在 DRAM 中以压缩稀疏列（CSC）格式存储非零值激活，并跳过零值激活（通过控制数据路径切换和内存访问）来节省能源，从而针对
    CNN 加速。Eyeriss v2 ([eyerissv2:2019,](#bib.bib42))，针对移动设备上的 DNN，也支持稀疏网络模型。它利用 CSC
    格式来存储权重和激活，这些数据在内存中和处理过程中都保持压缩。为了提高灵活性，它使用了层次化的网格结构来连接处理元素（PEs）。通过这些优化，Eyeriss
    v2 比原始的 Eyeriss 更加快速和节能。
- en: Cnvlutin ([cnvlutin:isca2016,](#bib.bib6) ), which also targets CNN acceleration,
    uses hierarchical data-parallel units, skips computation cycles for zero-valued
    activations and employs a co-designed data storage format based on Compressed
    Sparse Rows (CSR) to compress the activations in DRAM. However, it does not consider
    the sparsity of the weights. On the contrary, Cambricon-X architecture ([zhang2016micro,](#bib.bib305)
    ) exploits the sparsity of CNNs by enabling the PEs to store the compressed weights
    in CSR format for asynchronous computation. However, it does not exploit activation
    sparsity. EIE ([han2016isca,](#bib.bib103) ), besides compressing the weights
    through a variant of CSC sparse matrix representation and skipping zero-valued
    activations, employs a scalable array of PEs, each storing a partition of the
    DNN in SRAM that allows obtaining significant energy savings with respect to DRAM.
    NullHop ([Aimar2019,](#bib.bib4) ) is a CNN accelerator architecture that applies
    the Compressed Image Size (CIS) format to the weights and skips the null activations,
    similarly to EIE. Sparse CNN (SCNN) ([SCNN:isca2017,](#bib.bib207) ) is an accelerator
    architecture for inference in CNNs. It employs a cluster of asynchronous PEs comprising
    several multipliers and accumulators. SCNN exploits sparsity in both weights and
    activations, which are stored in the classic CSR representation. It employs a
    Cartesian product-based computation architecture that maximizes the reuse of weights
    and activations within the cluster of PEs; the values are delivered to an array
    of multipliers, and the resulting scattered products are summed using a dedicated
    interconnection mesh. By exploiting two-sided sparsity, SCNN improves performance
    and energy over dense architectures. SparTen ([sparten:micro2019,](#bib.bib91)
    ) is based on SCNN ([SCNN:isca2017,](#bib.bib207) ). It addresses some considerable
    overheads of SCNN in performing the sparse vector-vector dot product by improving
    the distribution of the operations to the multipliers and allows using any convolutional
    stride (not being limited to unit-stride convolutions as SCNN). It also addresses
    unbalanced sparsity distribution across the PEs employing an offline software
    scheme. The PermDNN architecture ([perm:micro2018,](#bib.bib60) ) addresses the
    generation and execution of hardware-friendly structured sparse DNN models using
    permuted diagonal matrices. In this way, it does not incur load imbalance which
    is caused by the irregularity of unstructured sparse DNN models.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: Cnvlutin ([cnvlutin:isca2016,](#bib.bib6) )，同样针对CNN加速，使用了分层的数据并行单元，跳过零值激活的计算周期，并采用了基于压缩稀疏行（CSR）的协同设计数据存储格式来压缩DRAM中的激活。然而，它没有考虑权重的稀疏性。相反，Cambricon-X架构 ([zhang2016micro,](#bib.bib305)
    ) 通过使处理单元（PEs）能够以CSR格式存储压缩权重来利用CNN的稀疏性，从而实现异步计算。然而，它没有利用激活的稀疏性。EIE ([han2016isca,](#bib.bib103)
    ) 除了通过CSC稀疏矩阵表示的变体压缩权重并跳过零值激活外，还采用了一个可扩展的PE阵列，每个PE在SRAM中存储DNN的一部分，这样可以相较于DRAM获得显著的能源节省。NullHop ([Aimar2019,](#bib.bib4)
    ) 是一种CNN加速器架构，应用了压缩图像大小（CIS）格式于权重，并跳过空激活，类似于EIE。稀疏CNN（SCNN） ([SCNN:isca2017,](#bib.bib207)
    ) 是一种用于CNN推理的加速器架构。它采用了一个异步PE集群，包含多个乘法器和累加器。SCNN利用权重和激活的稀疏性，这些数据以经典的CSR表示方式存储。它采用了基于笛卡尔积的计算架构，最大化了PE集群内权重和激活的重用；这些值被传送到一个乘法器阵列，最终通过专用的互连网格将散布的乘积求和。通过利用双侧稀疏性，SCNN在性能和能源方面优于密集架构。SparTen ([sparten:micro2019,](#bib.bib91)
    ) 基于SCNN ([SCNN:isca2017,](#bib.bib207) )。它通过改进操作分配到乘法器上，解决了SCNN在执行稀疏向量-向量点积时的一些显著开销，并允许使用任何卷积步幅（不再局限于SCNN的单位步幅卷积）。它还通过采用离线软件方案解决了PE之间稀疏性分布不均的问题。PermDNN架构 ([perm:micro2018,](#bib.bib60)
    ) 通过使用置换对角矩阵解决了生成和执行硬件友好的结构化稀疏DNN模型的问题。这样，它不会因为不规则的非结构化稀疏DNN模型而导致负载不平衡。
- en: SqueezeFlow ([squeezeflow:2019,](#bib.bib165) ) is an accelerator architecture
    that exploits the sparsity of CNN models. To reduce hardware complexity, it exploits
    concise convolution rules to benefit from the reduction of computation and memory
    accesses as well as the acceleration of existing dense CNN architectures without
    intrusive PE modifications. The Run Length Compression (RLC) format is used to
    compress activations and weights. A different strategy is pursued by the Unique
    Weight CNN (UCNN) accelerator ([ucnn:isca2018,](#bib.bib110) ), which proposes
    a generalization of the sparsity problem. Rather than considering only the repetition
    of zero-valued weights, UCNN exploits repeated weights with any value by reusing
    CNN sub-computations and reducing the model size in memory. SIGMA ([sigma:2020,](#bib.bib226)
    ) is an accelerator for DNN training, which is characterized by a flexible and
    scalable architecture that offers high utilization of its PEs regardless of kernel
    shape (i.e., matrices of arbitrary dimensions) and sparsity pattern. It targets
    the acceleration of GEMMs with unstructured sparsity. Bit-Tactical ([bittactical:2019,](#bib.bib59)
    ) is a DNN accelerator where the responsibility for exploiting weight sparsity
    is shared between a static scheduling middleware and a co-designed hardware front-end,
    with a lightweight sparse shuffling network which comprises two multiplexers per
    activation input. Unlike SIGMA and other accelerators, Bit-tactical leverages
    scheduling in software to align inputs and weights.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: SqueezeFlow ([squeezeflow:2019,](#bib.bib165) ) 是一种利用CNN模型稀疏性的加速器架构。为了减少硬件复杂性，它利用简洁的卷积规则，以减少计算和内存访问，并加速现有的密集型CNN架构，而无需侵入性的PE修改。Run
    Length Compression (RLC) 格式用于压缩激活值和权重。Unique Weight CNN (UCNN) 加速器 ([ucnn:isca2018,](#bib.bib110)
    ) 采用了稀疏性问题的一般化策略。UCNN 不仅考虑零值权重的重复，而是通过重用CNN子计算和减少内存中的模型大小，利用任意值的重复权重。SIGMA ([sigma:2020,](#bib.bib226)
    ) 是一种用于DNN训练的加速器，具有灵活且可扩展的架构，无论核的形状（即任意维度的矩阵）和稀疏性模式如何，都能提供高效的PE利用率。它旨在加速具有非结构化稀疏性的GEMM。Bit-Tactical ([bittactical:2019,](#bib.bib59)
    ) 是一种DNN加速器，其中利用权重稀疏性的责任由静态调度中间件和共同设计的硬件前端共享，后者具有一个轻量级的稀疏洗牌网络，每个激活输入包含两个多路复用器。与SIGMA和其他加速器不同，Bit-Tactical
    利用软件中的调度来对齐输入和权重。
- en: Flexagon ([flexagon:aplos2023,](#bib.bib191) ) is a reconfigurable accelerator
    capable of performing sparse-sparse matrix multiplication computation by using
    the particular data flow that best matches each case.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Flexagon ([flexagon:aplos2023,](#bib.bib191) ) 是一种可重配置的加速器，能够通过使用最匹配每种情况的数据流来执行稀疏-稀疏矩阵乘法计算。
- en: Besides the design of specialized hardware accelerators to exploit model sparsity,
    a parallel trend is to use GPU architectures. Pruned sparse models with unstructured
    sparse patterns introduce irregular memory accesses that are unfriendly on commodity
    GPU architectures. The first direction to tackle this issue on commodity DNN accelerators
    is at the software layer, using pruning algorithms that enforce a particular sparsity
    pattern, such as tile sparsity ([guo:sc2020,](#bib.bib97) ), on the model that
    allows leveraging existing GEMM accelerators. A second direction to leverage the
    sparsity in DNN models on GPUs is to introduce new architectural support, such
    as Sparse Tensor Cores ([sparsetensorcore:2021,](#bib.bib188) ). The NVIDIA Ampere
    architecture introduces this Sparse Tensor Core design with a fixed 50% weight
    pruning target and achieves a better accuracy and performance trade-off. However,
    sparsity from activations, which are dynamic and unpredictable, is challenging
    to leverage on GPUs. Indeed, the current Sparse Tensor Core is only able to take
    advantage of weight sparsity but not activation sparsity. Reconfigurability appears
    to be a keyword for the design of new sparse accelerators because some network
    models exhibit dynamic sparsity ([fedus:jmlr2022,](#bib.bib73) ), where the position
    of non-zero elements changes over time.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 除了设计专用的硬件加速器来利用模型稀疏性外，另一个并行趋势是使用GPU架构。具有非结构化稀疏模式的修剪稀疏模型引入了不适合通用GPU架构的不规则内存访问。解决这一问题的第一个方向是在软件层面，使用强制特定稀疏模式的修剪算法，例如瓦片稀疏性（[guo:sc2020,](#bib.bib97)），在模型上进行处理，以便利用现有的GEMM加速器。第二个方向是在GPU上利用DNN模型的稀疏性是引入新的架构支持，例如稀疏张量核心（[sparsetensorcore:2021,](#bib.bib188)）。NVIDIA
    Ampere架构引入了这一稀疏张量核心设计，设定了50%的权重修剪目标，并实现了更好的准确性和性能折衷。然而，来自激活的稀疏性（动态且不可预测）在GPU上利用起来具有挑战性。实际上，目前的稀疏张量核心只能利用权重稀疏性，而不能利用激活稀疏性。可重构性似乎是新型稀疏加速器设计的关键词，因为一些网络模型表现出动态稀疏性（[fedus:jmlr2022,](#bib.bib73)），即非零元素的位置随时间变化。
- en: 5.2\. Emerging 3D-stacked Processing-in-memory Technologies
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 新兴的3D堆叠处理内存技术
- en: 'Two main 3D stacked memory standards have been recently proposed: the Hybrid
    Memory Cube (HMC) and the High Bandwidth Memory (HBM) that provide highly parallel
    access to the memory which is well suited to the highly parallel architecture
    of the DNN accelerators. The processing elements of 3D stacked DNN accelerators
    can be embedded in the logic die or in the memory dies, reducing significantly
    the latency of accessing data in main memory, and improving the energy efficiency
    of the system. However, as detailed in Sec. A.6 of Appendix A, there are some
    challenges and limitations to be taken into account when using this technology
    ([Kim2022,](#bib.bib143) ).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最近提出了两种主要的3D堆叠存储标准：混合内存立方体（**HMC**）和高带宽内存（**HBM**），它们提供了对内存的高度并行访问，非常适合深度神经网络加速器的高度并行架构。3D堆叠深度神经网络加速器的处理单元可以嵌入在逻辑芯片或存储芯片中，这大大减少了访问主存数据的延迟，并提高了系统的能源效率。然而，如附录A的第A.6节所述，在使用这一技术时需要考虑一些挑战和限制（[Kim2022,](#bib.bib143)）。
- en: '![Refer to caption](img/d069eb841a255166d30cd9e7fba1914e.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d069eb841a255166d30cd9e7fba1914e.png)'
- en: (a)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: （a）
- en: '![Refer to caption](img/0b5ba532313e955c6955f772ba8be642.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0b5ba532313e955c6955f772ba8be642.png)'
- en: (b)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: （b）
- en: Figure 7. (a) Neurocube architecture. (b) HBM-PIM architecture.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图7。 （a）神经立方体架构。 （b）HBM-PIM架构。
- en: 5.2.1\. State-of-the-art on 3D-stacked Processor-in-memory solutions
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 3D堆叠处理内存解决方案的最新技术
- en: Table 7. Summary of 3D-staked Processing-in-memory DNN accelerators.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表7。3D堆叠处理内存DNN加速器的总结。
- en: '| PIM | Year | Integration Level | 3D Mem. Tech. | Functions | Data Type |
    Tech. Node | Performance [GOPs/s] | Power [W] | Maturity |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| PIM | 年份 | 集成等级 | 3D存储技术 | 功能 | 数据类型 | 技术节点 | 性能 [GOPs/s] | 功耗 [W] | 成熟度
    |'
- en: '| Neurocube([Kim2016,](#bib.bib142) ) | 2016 | Logic die | HMC | MAC | 16-bit
    fixed point | 15nm | 132 | 3.4 + HMC | Layout |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 神经立方体([Kim2016,](#bib.bib142) ) | 2016 | 逻辑芯片 | HMC | MAC | 16位定点 | 15nm
    | 132 | 3.4 + HMC | 布局 |'
- en: '| Tetris([Gao2017,](#bib.bib83) ) | 2017 | Logic die | HMC | ALU/MAC | 16-bit
    fixed point | 45nm | - | 8.42 | Simulation |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| Tetris([Gao2017,](#bib.bib83) ) | 2017 | 逻辑芯片 | HMC | ALU/MAC | 16位定点 | 45nm
    | - | 8.42 | 仿真 |'
- en: '| NeuralHMC([Min2019,](#bib.bib185) ) | 2019 | Logic die | HMC | MAC | 32-bit
    floating point | - | - | - | Simulation |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| NeuralHMC([Min2019,](#bib.bib185) ) | 2019 | 逻辑芯片 | HMC | MAC | 32位浮点 | -
    | - | - | 仿真 |'
- en: '| VIMA([Cordeiro2021,](#bib.bib54) ) | 2021 | Logic die | HMC | ALU/MULT/DIV
    | 32-bit integer/floating point | - | - | 3.2 + HMC | Simulation |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| VIMA([Cordeiro2021,](#bib.bib54) ) | 2021 | 逻辑芯片 | HMC | ALU/MULT/DIV | 32位整数/浮点数
    | - | - | 3.2 + HMC | 仿真 |'
- en: '| Newton([He2020,](#bib.bib109) ) | 2020 | Bank | HBM | MAC | bfloat16 | -
    | - | - | Simulation |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Newton([He2020,](#bib.bib109) ) | 2020 | 银行 | HBM | MAC | bfloat16 | - |
    - | - | 仿真 |'
- en: '| HBM-PIM([Kwon2021,](#bib.bib154) ) | 2020 | Bank | HBM | ALU/MAC | 16-bit
    floating point | 20nm | 1200 | - | Silicon |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| HBM-PIM([Kwon2021,](#bib.bib154) ) | 2020 | 银行 | HBM | ALU/MAC | 16位浮点数 |
    20nm | 1200 | - | 硅 |'
- en: From Table [7](#S5.T7 "Table 7 ‣ 5.2.1\. State-of-the-art on 3D-stacked Processor-in-memory
    solutions ‣ 5.2\. Emerging 3D-stacked Processing-in-memory Technologies ‣ 5\.
    Accelerators based on Emerging Paradigms and Technologies ‣ A Survey on Deep Learning
    Hardware Accelerators for Heterogeneous HPC Platforms"), we can distinguish two
    different approaches when integrating digital PEs in a 3D stacked memory architecture
    ([Kim2022,](#bib.bib143) ). The first approach, most commonly found in the literature,
    embeds the computing into the logic die of the memory block (logic die-level PIM).
    In the second approach (bank-level PIM), the processing logic is integrated into
    each DRAM die at the level of the memory banks, after the column decoder and selector
    blocks.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 从表 [7](#S5.T7 "表7 ‣ 5.2.1\. 3D堆叠内存计算解决方案的最新进展 ‣ 5.2\. 新兴3D堆叠内存计算技术 ‣ 5\. 基于新兴范式和技术的加速器
    ‣ 关于异构HPC平台的深度学习硬件加速器的调查")中，我们可以区分在3D堆叠内存架构中集成数字PE的两种不同方法 ([Kim2022,](#bib.bib143)
    )。第一种方法，在文献中最为常见，是将计算嵌入到内存块的逻辑芯片中（逻辑芯片级PIM）。第二种方法（银行级PIM）则是在每个DRAM芯片的内存银行级别集成处理逻辑，位于列解码器和选择块之后。
- en: A first example of a 3D PIM implementation is Neurocube ([Kim2016,](#bib.bib142)
    ) that, as shown in Figure [7(a)](#S5.F7.sf1 "In Figure 7 ‣ 5.2\. Emerging 3D-stacked
    Processing-in-memory Technologies ‣ 5\. Accelerators based on Emerging Paradigms
    and Technologies ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms"), is embedded into the logic die of an HMC, and consists of a cluster
    of PEs connected by a 2D mesh Network-on-Chip (NoC). The PE is composed of a row
    of multiply accumulator (MAC) units, a cache memory, a temporal buffer, and a
    memory module for storing shared synaptic weights. Each PE is associated with
    a single memory vault and can operate independently and communicate through the
    TSVs and the vault controller. A host communicates with the Neurocube through
    the external links of the HMC to configure the Neurocube for different neural
    network architectures. Each vault controller in the HMC has an associated programmable
    neuro sequence generator (PNG), i.e., a programmable state machine that controls
    the data movements required for neural computation. Neurocube implements an output
    stationary dataflow, meaning that each MAC from a PE is responsible for the computations
    of a different output neuron at a time.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一个3D PIM实现的初步示例是Neurocube ([Kim2016,](#bib.bib142) )，如图 [7(a)](#S5.F7.sf1 "图7
    ‣ 5.2\. 新兴3D堆叠内存计算技术 ‣ 5\. 基于新兴范式和技术的加速器 ‣ 关于异构HPC平台的深度学习硬件加速器的调查")所示，它嵌入在HMC的逻辑芯片中，并由通过2D网格网络连接的PE集群组成。每个PE由一排乘法累加器（MAC）单元、一个缓存存储器、一个临时缓冲区和一个用于存储共享突触权重的内存模块组成。每个PE与单个内存金库关联，可以独立操作并通过TSV和金库控制器进行通信。主机通过HMC的外部链接与Neurocube进行通信，以配置Neurocube以适应不同的神经网络架构。HMC中的每个金库控制器都有一个相关的可编程神经序列生成器（PNG），即一个可编程状态机，用于控制神经计算所需的数据移动。Neurocube实现了一种输出静态数据流，意味着每个PE的MAC负责一次计算一个不同的输出神经元。
- en: Similarly to Neurocube, Tetris ([Gao2017,](#bib.bib83) ) uses an HMC memory
    stack organized into 16 vaults. Each vault is associated with a PE, connected
    to the vault controller, and composed of a systolic array of 14 $\times$14 PEs
    and a small SRAM buffer, shared among the PEs. A 2D mesh NoC connects all the
    PEs. Differently from previous accelerator approaches, the dimension of the buffers
    in the logic layer is reduced and optimized to take into account the lower cost
    of accessing the DRAM layers, as well as the area constraints of the 3D package.
    Each PE has a register file and a MAC to locally store the inputs/weights and
    perform computations. Tetris implements a row stationary dataflow that maps 1D
    convolutions onto a single PE and utilizes the PE register file for local data
    reuse. A 2D convolution is orchestrated on the 2D array interconnect so that the
    data propagation among PEs remains local. In ([Gao2017,](#bib.bib83) ), an optimal
    scheduling is discussed to maximize on-chip reuse of weights and/or activations,
    and resource utilization. However, a programming model is not presented.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 与Neurocube类似，Tetris ([Gao2017,](#bib.bib83)) 使用了一个组织成16个金库的HMC内存栈。每个金库都与一个PE相关联，连接到金库控制器，并由一个14
    $\times$ 14 PEs的膨胀阵列和一个小型SRAM缓冲区组成，该缓冲区在PE之间共享。一个2D网格NoC连接所有PE。与之前的加速器方法不同，逻辑层中的缓冲区尺寸已被减少和优化，以考虑到访问DRAM层的较低成本以及3D封装的面积限制。每个PE都有一个寄存器文件和一个MAC，用于本地存储输入/权重并执行计算。Tetris实现了一个行固定数据流，将1D卷积映射到单个PE，并利用PE寄存器文件进行本地数据重用。一个2D卷积在2D阵列互连上协调，以使PE之间的数据传播保持本地化。在
    ([Gao2017,](#bib.bib83)) 中，讨论了一个最佳调度方案，以最大化芯片内权重和/或激活的重用，以及资源利用。然而，并未提出编程模型。
- en: NeuralHMC ([Min2019,](#bib.bib185) ) adopts the same systolic architecture and
    row-stationary dataflow discussed in Tetris. However, the authors introduce the
    use of a weight-sharing pipelined MAC design to lower the cost of accessing weight
    data, by reducing the original 32 bits floating points weights to a 5 or 8 bits
    cluster index, saving memory consumption. Moreover, they discuss a series of mechanisms
    to reduce and optimize packet scheduling and on-chip communication in multi-HMC
    architectures.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: NeuralHMC ([Min2019,](#bib.bib185)) 采用了与Tetris中讨论的相同的膨胀体系结构和行固定数据流。然而，作者引入了权重共享流水线MAC设计，以通过将原始32位浮点权重减少为5位或8位的簇索引，从而降低访问权重数据的成本，节省内存消耗。此外，他们讨论了一系列机制，以减少和优化多HMC体系结构中的数据包调度和芯片内通信。
- en: The authors in ([Cordeiro2021,](#bib.bib54) ) study the benefits of migrating
    ML kernels on near-data processing (NDP) architecture capable of large-vector
    operations. The work derives from previous work of the same authors ([Alves2016,](#bib.bib7)
    ), where they introduced the HIVE architecture, which extends the HMC ISA for
    performing common vector operations directly inside the HMC, avoiding contention
    on the interconnections as well as cache pollution. The newly introduced Vector-In-Memory
    Architecture (VIMA) supports all ARM NEON Integer and Floating-point instructions
    and operates over vectors of 8 KB of data by fetching data over the 32 channels
    (vaults) of the HMC in parallel. The authors extend and use an NDP intrinsics
    library that supports the validation of NDP architectures based on large vectors,
    provides insights, and show the benefits of migrating ML algorithms to vector-based
    NDP architectures. Their simulated results show a significant speed-up and energy
    reduction with respect to an x86 baseline. Several accelerators employing the
    second PIM approach can be found in the literature. Newton ([He2020,](#bib.bib109)
    ) proposes a fixed data flow accelerator that computes matrix-vector multiplication
    effectively. It employs only MAC units, buffers, and a DRAM-like command interface
    for the host CPU to issue commands to the PIM compute, avoiding the overhead and
    granularity issues of offloading-based accelerators, e.g., the delay in the launch
    of the kernel and the switching between the PIM/non-PIM operational modes. To
    reduce the output vector write traffic with minimal output buffering, Newton employs
    an unusually-wide interleaved layout (DRAM row-wide). In Newton, input/output
    vectors have high reuse while the matrix has no reuse.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在（[Cordeiro2021,](#bib.bib54)）的研究中探讨了将机器学习内核迁移到支持大向量操作的近数据处理（NDP）架构的好处。该工作源自同一作者的早期工作（[Alves2016,](#bib.bib7)），他们介绍了HIVE架构，该架构扩展了HMC
    ISA，以直接在HMC内部执行常见的向量操作，从而避免了互连争用以及缓存污染。新引入的Vector-In-Memory Architecture（VIMA）支持所有ARM
    NEON整数和浮点指令，并通过并行从HMC的32个通道（vaults）中提取数据，处理8 KB数据的向量。作者扩展并使用了一个NDP内建库，该库支持基于大向量的NDP架构的验证，提供了见解，并展示了将机器学习算法迁移到基于向量的NDP架构的好处。他们的模拟结果显示，与x86基准相比，速度显著提升且能耗降低。在文献中可以找到几种采用第二种PIM方法的加速器。Newton（[He2020,](#bib.bib109)）提出了一种固定数据流加速器，能够有效地计算矩阵-向量乘法。它仅使用MAC单元、缓冲区和类似DRAM的命令接口来使主CPU向PIM计算发出命令，从而避免了基于离线加速器的开销和粒度问题，例如内核启动延迟和PIM/非PIM操作模式之间的切换。为了减少输出向量写入流量并最小化输出缓冲，Newton采用了异常宽的交错布局（DRAM行宽）。在Newton中，输入/输出向量具有高重用性，而矩阵没有重用。
- en: HBM-PIM ([Kwon2021,](#bib.bib154) ) implements a function-in-memory DRAM (FIMDRAM)
    that integrates a 16-wide SIMD engine within the memory banks exploits bank-level
    parallelism to provide 4 $\times$ higher processing bandwidth than an off-chip
    memory solution (Figure [7(b)](#S5.F7.sf2 "In Figure 7 ‣ 5.2\. Emerging 3D-stacked
    Processing-in-memory Technologies ‣ 5\. Accelerators based on Emerging Paradigms
    and Technologies ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms")). In their design, half of the cell array in each bank of the
    HBM was removed and replaced by a programmable computing unit (PCU), placed adjacent
    to the cell array to utilize bank-level parallelism. Each PCU is shared among
    two banks, and there are 8 PCUs per pseudo-channel. The PCU is divided into a
    register group, an execution unit, a decoding unit for parsing instructions needed
    to perform operations, and interface units to control data flow. The register
    group consists of a command-register file for instruction memory (CRF), a general-purpose
    register file for weight and accumulation (GRF), and a scalar register file to
    store constants for MAC operations (SRF). The PIM controller is integrated to
    support the programmability of the PCU and, similarly to Newton, the seamless
    integration with the host by determining the switching between the PIM/non-PIM
    operational modes. If the PIM mode is asserted, the PCUs execute the instructions
    pre-stored in the CRF, incrementing the program counter every time a DRAM’s read
    command is issued. 3D-stacked PIM has been also proposed for accelerating applications
    loosely related to DNNs. We present a brief overview of these accelerators in
    Sec. A.6 of the Appendix A.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: HBM-PIM ([Kwon2021,](#bib.bib154)) 实现了一种功能内存DRAM（FIMDRAM），在内存银行中集成了一个16宽的SIMD引擎，利用银行级并行性提供了比离芯片内存解决方案高4倍的处理带宽（图
    [7(b)](#S5.F7.sf2 "图 7 ‣ 5.2\. 新兴3D堆叠处理内存技术 ‣ 5\. 基于新兴范式和技术的加速器 ‣ 关于异构HPC平台深度学习硬件加速器的调查")）。在其设计中，每个HBM银行中的一半单元阵列被移除，并替换为一个可编程计算单元（PCU），该单元被放置在靠近单元阵列的位置，以利用银行级并行性。每个PCU在两个银行之间共享，每个伪通道有8个PCU。PCU被划分为一个寄存器组、一个执行单元、一个用于解析执行所需指令的解码单元，以及控制数据流的接口单元。寄存器组包括一个指令存储器的命令寄存器文件（CRF）、一个用于权重和累加的通用寄存器文件（GRF）以及一个用于存储MAC操作常量的标量寄存器文件（SRF）。PIM控制器集成以支持PCU的可编程性，并类似于Newton，通过确定PIM/非PIM操作模式之间的切换实现与主机的无缝集成。如果PIM模式被启用，PCU将执行预先存储在CRF中的指令，每次发出DRAM的读取命令时递增程序计数器。3D堆叠PIM也被提出用于加速与DNN松散相关的应用。我们在附录A的第A.6节中简要概述了这些加速器。
- en: 5.3\. In-memory computing accelerators based on emerging memories
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 基于新兴存储器的内存计算加速器
- en: 'In-memory computing (IMC) has been proposed to break both the memory and the
    compute wall in data-driven AI workloads, using either SRAM or emerging memory
    technologies (such as PCM and RRAM described in Sec. A.7 of Appendix A) offering
    different trade-offs when used as an integrated computing device at the system
    level. Full-digital IMC designs offer a fast path for the integration of the next
    generation of neural processing systems like in NPUs. An example of IMC architecture
    has recently been proposed by STMicroelectronics in ([desoli2023isscc,](#bib.bib61)
    ), where a scalable and design time parametric NPU for edge AI relying on digital
    SRAM IMC has been manufactured in 18 nm FDSOI technology achieving an end-to-end
    system-level energy efficiency of 77 TOPS/W and an area efficiency of 13.6 TOPS/mm².
    This IMC-NPU is the evolution of the Orlando system-on-chip proposed in ([desoli17isscc,](#bib.bib62)
    ). Another digital IMC design is NeuroCIM ([kim2022vlsi,](#bib.bib145) ), an energy-efficient
    processor with four key features achieving 310.4 TOPS/W: Most significant bit
    (MSB) Word Skipping to reduce the BL activity; early stopping to enable lower
    bitline activity; mixed-mode firing for multi-macro aggregation; voltage folding
    to extend the dynamic range. A first example of an RRAM-based accelerator is the
    ISAAC ([shafiee2016isca,](#bib.bib241) ) tile-based architecture that proposes
    a pipeline design for CNN processing, which combines the data encoding and the
    processing steps within in situ multiply and accumulate units (IMA). In the first
    pipeline step, data are fetched from a chip eDRAM to the computation tile. The
    data format in ISAAC is fixed 16-bit. In computation, in each cycle, 1 bit is
    input to the IMA, and the computation result from the IMA is converted to digital
    format, thus requiring 16 clock cycles to process the input. The nonlinear activation
    is then applied, and the results are written back to eDRAM. Tiled computation
    is widely used from RRAM to improve the throughput. The PipeLayer ([song2017hpca,](#bib.bib249)
    ) architecture introduces intra-layer parallelism and an inter-layer pipeline
    for tiled architecture, using duplicates of processing units featuring the same
    weights to process multiple data in parallel. RRAM-based accelerators have been
    also designed for RNNs. In ([wan2022nature,](#bib.bib284) ), all the decomposed
    operations were formulated into in-situ MACs to provide high throughput. Further,
    designs like PRIME ([chi2016isca,](#bib.bib45) ) takes part of the RRAM memory
    arrays to serve as the accelerator instead of adding an extra processing unit
    for computation. This can be considered as an architecture that is borderline
    between NPUs and stand-alone. However, ([cao2022tcomp,](#bib.bib29) ) noted that
    existing PIM RRAM accelerators suffer from frequent and energy-intensive analog-to-digital
    (A/D) conversions, severely limiting their performance. To this extent, they presented
    a new architecture to efficiently accelerate DL tasks by minimizing the required
    A/D conversions with analog accumulation and neural-approximated peripheral circuits.
    Using a new dataflow they remarkably reduced the required A/D conversions for
    matrix-vector multiplications by extending shift-and-add (S+A) operations into
    the analog domain before the final quantizations. A summary of the technological
    features in major RRAM accelerators can be found in ([smagulova2023pieee,](#bib.bib246)
    ).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 内存计算（IMC）已被提出用来突破数据驱动AI工作负载中的内存和计算壁垒，使用SRAM或新兴的存储技术（如附录A中的第A.7节所述的PCM和RRAM），这些技术在作为系统级集成计算设备时提供了不同的权衡。全数字IMC设计为下一代神经处理系统（如NPUs）的集成提供了快速路径。STMicroelectronics最近在([desoli2023isscc,](#bib.bib61))中提出了IMC架构的一个示例，其中一个可扩展且设计时参数化的边缘AI
    NPU依赖于数字SRAM IMC，在18 nm FDSOI技术下制造，达到了77 TOPS/W的端到端系统级能效和13.6 TOPS/mm²的面积效率。这个IMC-NPU是([desoli17isscc,](#bib.bib62))中提出的Orlando系统单芯片的演变。另一个数字IMC设计是NeuroCIM
    ([kim2022vlsi,](#bib.bib145))，这是一种节能处理器，具有四个关键特性，达到310.4 TOPS/W：最显著位（MSB）字跳过以减少BL活动；早期停止以降低位线活动；混合模式发射以进行多宏聚合；电压折叠以扩展动态范围。RRAM基础的加速器的第一个示例是ISAAC
    ([shafiee2016isca,](#bib.bib241))，这是一种基于瓦片的架构，提出了一种用于CNN处理的流水线设计，将数据编码和处理步骤结合在原位乘法和累加单元（IMA）中。在第一个流水线步骤中，数据从芯片eDRAM获取到计算瓦片中。ISAAC中的数据格式为固定的16位。在计算中，每个周期输入1位到IMA中，IMA的计算结果被转换为数字格式，因此需要16个时钟周期来处理输入。然后应用非线性激活，将结果写回eDRAM。瓦片计算在从RRAM中广泛使用以提高吞吐量。PipeLayer
    ([song2017hpca,](#bib.bib249))架构引入了层内并行性和用于瓦片架构的层间流水线，使用相同权重的处理单元副本以并行处理多个数据。RRAM基础的加速器也已被设计用于RNNs。在([wan2022nature,](#bib.bib284))中，所有分解的操作都被形式化为原位MACs以提供高吞吐量。此外，像PRIME
    ([chi2016isca,](#bib.bib45))这样的设计利用部分RRAM存储阵列作为加速器，而不是添加额外的处理单元进行计算。这可以视为NPUs和独立加速器之间的边界架构。然而，[（cao2022tcomp,](#bib.bib29)）指出现有的PIM
    RRAM加速器存在频繁且能耗高的模拟到数字（A/D）转换，严重限制了它们的性能。为此，他们提出了一种新架构，通过最小化所需的A/D转换，结合模拟累积和神经逼近外围电路来高效加速DL任务。使用一种新的数据流，他们显著减少了矩阵-向量乘法所需的A/D转换，通过将移位加法（S+A）操作扩展到模拟域中，直到最终量化。在([smagulova2023pieee,](#bib.bib246))中可以找到主要RRAM加速器的技术特性总结。
- en: The first PCM-based silicon demonstrator for DNN inference is Hermes, which
    appeared in 2021 ([khaddam2022hermes,](#bib.bib140) ). The IMC accelerator consists
    of a 256x256 PCM cross-bar and optimized ADC circuitry to reduce the read-out
    latency and energy penalty. The SoC is implemented in 14nm technology, showing
    10.5 TOPS/W energy efficiency and performance density of 1.59 TOPS/mm2 on inference
    tasks of multi-layer perceptrons and ResNet-9 models trained on MNIST and CIFAR-10
    datasets, with comparable accuracy as the software baseline. The same 256x256
    PCM cross-bar has been integrated into a scaled-up mixed-signal architecture that
    targets the inference of long short-term memory (LSTM) and ResNet-based neural
    networks ([gallo202264,](#bib.bib81) ). The chip, implemented in the same 14nm
    technology, consists of 64 analog cores interconnected via an on-chip communication
    network and complemented with digital logic to execute activation functions, normalization,
    and other kernels than Matrix-Vector Multiplications (MVMs). The accelerator achieves
    a peak throughput of 63.1 TOPS with an energy efficiency of 9.76 TOPS for 8-bit
    input/8-bit output MVM operations.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 首个基于PCM的硅演示器用于DNN推理的是Hermes，该设备于2021年出现（[khaddam2022hermes,](#bib.bib140)）。IMC加速器包含一个256x256
    PCM交叉开关和优化的ADC电路，以减少读出延迟和能量损失。该SoC采用14nm技术实现，展示了10.5 TOPS/W的能效和1.59 TOPS/mm²的性能密度，用于MNIST和CIFAR-10数据集上训练的多层感知机和ResNet-9模型的推理任务，其准确度与软件基准相当。相同的256x256
    PCM交叉开关已集成到一个扩展的混合信号架构中，目标是推理长短期记忆（LSTM）和基于ResNet的神经网络（[gallo202264,](#bib.bib81)）。该芯片采用相同的14nm技术实现，由64个模拟核心通过片上通信网络互联，并辅以数字逻辑以执行激活函数、归一化以及其他矩阵-向量乘法（MVM）以外的内核。该加速器实现了63.1
    TOPS的峰值吞吐量，对于8位输入/8位输出的MVM操作，其能效为9.76 TOPS。
- en: Besides silicon stand-alone demonstrators, the PCM technology is evaluated from
    a broader perspective in heterogeneous architectures that target different classes
    of devices, from IoT end nodes to many-core HPC systems. Such studies aim to highlight
    and overcome the system-level challenges that arise when PCM technology is integrated
    into more complex mixed-signal systems. For example, Garofalo et al. ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85)
    ) analyze the limited flexibility of AIMC cores that can only sustain MVM-oriented
    workloads, but they are inefficient to execute low-reuse kernels and other ancillary
    functions such as batch-normalization and activation functions. To better balance
    Amdahl’s effects that show up on the execution of end-to-end DNN inference workloads,
    they propose as a solution an analog-digital edge system that complements the
    computing capabilities of PCM-based accelerators with the flexibility of general-purpose
    cores. The architecture, benchmarked on a real-world MobileNetV2 model, demonstrates
    significant advantages over purely digital solutions. Bruschi et al. ([bruschiEndtoEndDNNInference2022,](#bib.bib26)
    ) leave the edge domain to study the potentiality of PCM-based AIMC in much more
    powerful HPC many-core systems. The work presents a general-purpose chiplet-oriented
    architecture of 512 processing clusters, each composed of RISC-V cores for digital
    computations and nvAIMC cores for analog-amenable operations, such as 2D convolutions.
    This system is benchmarked on a ResNet18 DNN model, achieving 20.2 TOPS and 6.5
    TOPS/W.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 除了硅单独演示器外，PCM技术还从更广泛的角度在异构架构中进行了评估，目标是不同类别的设备，从物联网终端节点到多核高性能计算系统。这些研究旨在突出并克服PCM技术集成到更复杂的混合信号系统中时出现的系统级挑战。例如，Garofalo等人（[garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85)）分析了AIMC核心的有限灵活性，这些核心只能支持以MVM为导向的工作负载，但在执行低重用内核和其他辅助功能（如批量归一化和激活函数）时效率较低。为了更好地平衡在端到端DNN推理工作负载执行中出现的Amdahl效应，他们提出了一种模拟-数字边缘系统的解决方案，该系统通过通用核心的灵活性来补充基于PCM的加速器的计算能力。该架构在真实世界的MobileNetV2模型上进行了基准测试，显示出相较于纯数字解决方案的显著优势。Bruschi等人（[bruschiEndtoEndDNNInference2022,](#bib.bib26)）离开边缘领域，研究PCM基于AIMC在更强大的HPC多核系统中的潜力。该研究展示了一种面向通用芯片的512个处理集群的架构，每个集群由用于数字计算的RISC-V核心和用于模拟适应操作（如2D卷积）的nvAIMC核心组成。该系统在ResNet18
    DNN模型上进行了基准测试，实现了20.2 TOPS和6.5 TOPS/W。
- en: Table 8. Summary of IMC accelerators based on RRAM and PCM memories
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表8. 基于RRAM和PCM存储器的IMC加速器总结
- en: '| Accelerator | Technology | Process | Application | Area [mm²] | Power [mW]
    | Performance [GOPS] | EE [GOPS/W] | AE [GOPS/mm²] |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 加速器 | 技术 | 过程 | 应用 | 面积 [mm²] | 功率 [mW] | 性能 [GOPS] | 能效 [GOPS/W] | 面积效率
    [GOPS/mm²] |'
- en: '| ISAAC ([shafiee2016isca,](#bib.bib241) ) | RRAM+CMOS | 32 nm | CNN | 85.4
    | 65800 | - | 380.7 | 466.8 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| ISAAC ([shafiee2016isca,](#bib.bib241) ) | RRAM+CMOS | 32 nm | CNN | 85.4
    | 65800 | - | 380.7 | 466.8 |'
- en: '| PipeLayer ([song2017hpca,](#bib.bib249) ) | RRAM+CMOS | - | CNN | 82.63 |
    - | - | 140 | 1485 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| PipeLayer ([song2017hpca,](#bib.bib249) ) | RRAM+CMOS | - | CNN | 82.63 |
    - | - | 140 | 1485 |'
- en: '| NeuralPIM ([cao2022tcomp,](#bib.bib29) ) | RRAM+CMOS | 32 nm | CNN+RNN |
    86.4 | 67700 | - | 2040.6 | 1904 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| NeuralPIM ([cao2022tcomp,](#bib.bib29) ) | RRAM+CMOS | 32 nm | CNN+RNN |
    86.4 | 67700 | - | 2040.6 | 1904 |'
- en: '| PRIME ([chi2016isca,](#bib.bib45) ) | RRAM+CMOS | 65 nm | MLP+CNN | - | -
    | - | 2100 | 1230 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| PRIME ([chi2016isca,](#bib.bib45) ) | RRAM+CMOS | 65 nm | MLP+CNN | - | -
    | - | 2100 | 1230 |'
- en: '| NeuRRAM ([wan2022nature,](#bib.bib284) ) | RRAM+CMOS | 130 nm | CNN+RNN+RBN
    | 159 | 49.7 | 2135 | 43000 | - |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| NeuRRAM ([wan2022nature,](#bib.bib284) ) | RRAM+CMOS | 130 nm | CNN+RNN+RBN
    | 159 | 49.7 | 2135 | 43000 | - |'
- en: '| Hermes ([khaddam2022hermes,](#bib.bib140) ) | PCM+CMOS | 14 nm | MLP+CNN+LSTM
    | - | - | - | 10500 | 1590 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Hermes ([khaddam2022hermes,](#bib.bib140) ) | PCM+CMOS | 14 nm | MLP+CNN+LSTM
    | - | - | - | 10500 | 1590 |'
- en: 5.4\. Full-digital Neuromorphic Accelerators
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 完全数字化的神经形态加速器
- en: Neuromorphic computing aims at a paradigm shift from Von Neumann-based architectures
    to distributed and co-integrated memory and PEs, the granularity at which this
    paradigm shift is achieved in digital implementations strongly varies between
    a distributed Von Neumann or full-custom approach, from high to low processing
    and memory separation ([frenkel2021arxiv,](#bib.bib77) ). Neuromorphic chip architectures
    enable the hardware implementation of spiking neural networks (SNNs) ([rathi2023acm,](#bib.bib231)
    ) and advanced bio-inspired computing systems that have the potential to achieve
    even higher energy efficiency with respect to DNN stand-alone accelerators described
    so far ([akopyan2015tcad,](#bib.bib5) ).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 神经形态计算旨在将范诺依曼架构转变为分布式和协同集成的存储器与处理单元（PEs），这一范式转变在数字实现中的粒度在分布式范诺依曼或全定制方法之间差异很大，从高到低处理和存储分离（[frenkel2021arxiv,](#bib.bib77)）。神经形态芯片架构使得脉冲神经网络（SNNs）（[rathi2023acm,](#bib.bib231)）和先进的生物启发计算系统的硬件实现成为可能，这些系统有潜力在与当前所描述的深度神经网络（DNN）独立加速器相比，达到更高的能效（[akopyan2015tcad,](#bib.bib5)）。
- en: A first example of a digital architecture for SNN and neuroscience simulation
    acceleration is the SpiNNaker chip ([painkras2013jsscc,](#bib.bib206) ). It follows
    a distributed von-Neumann approach using a globally asynchronous locally synchronous
    (GALS) design for efficient handling of asynchronous spike data and is based on
    a 130 nm technology. SpiNNaker has been optimized for large-scale SNN experiments
    while keeping a high degree of flexibility. The evolution of the architecture
    using 22 nm technology embedding 4 ARM Cortex M4F cores out of the 152 per chip
    is planned for the final SpiNNaker 2 system ([liu2018frontiers,](#bib.bib170)
    ). The objective is to simulate two orders of magnitude more neurons per chip
    compared to ([painkras2013jsscc,](#bib.bib206) ). However, it has been demonstrated
    that GPU-based accelerators compare favorably to a SpiNNaker system when it comes
    to large SNN and cortical-scale simulations ([knight2018frontiers,](#bib.bib147)
    ).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数字架构的首个例子，用于SNN和神经科学仿真加速是SpiNNaker芯片（[painkras2013jsscc,](#bib.bib206)）。它采用了分布式的范诺依曼方法，使用全局异步局部同步（GALS）设计来高效处理异步脉冲数据，并基于130
    nm技术。SpiNNaker已针对大规模SNN实验进行优化，同时保持了较高的灵活性。计划在最终的SpiNNaker 2系统中使用22 nm技术，嵌入152个ARM
    Cortex M4F核心中的4个（[liu2018frontiers,](#bib.bib170)）。目标是每个芯片模拟比（[painkras2013jsscc,](#bib.bib206)）多两个数量级的神经元。然而，已有证明表明，与大规模SNN和皮层尺度仿真相比，基于GPU的加速器在性能上优于SpiNNaker系统（[knight2018frontiers,](#bib.bib147)）。
- en: Full-custom digital hardware leads to higher-density and more energy-efficient
    neuron and synapse integration for spiking neural networks (SNN) compared to the
    two formerly described accelerators ([frenkel2021arxiv,](#bib.bib77) ). All the
    accelerators reported in this paper benefit from moving the memory (generally
    SRAM elements) closer to computation. The 45 nm design in ([seo2011cicc,](#bib.bib239)
    ) is a small-scale architecture for SNN acceleration embedding 256 Leaky-Integration-Fire
    (LIF) neurons and up to 64k synapses based on the Stochastic Synaptic Time Dependant
    Plasticity (S-STDP) concept. It achieves reasonably high neuron and synapse densities,
    despite the use of a custom SRAM and given its energy-efficiency figures is an
    ideal choice, especially for edge computing scenarios. At the same integration
    scale, the ODIN chip embeds 256 neurons and 64k Spike Driven Synaptic Plasticity
    (SDSP)-based 4-bit synapses in a 28 nm CMOS process ([frenkel2018tbiocas,](#bib.bib78)
    ). A first attempt to scale up the NPU for SNN applications is represented by
    the 65 nm MorphIC chip, which is based on the ODIN core integrated into a quadcore
    design ([frenkel2019tbiocas,](#bib.bib79) ).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 全定制数字硬件比前面描述的两个加速器 ([frenkel2021arxiv,](#bib.bib77)) 更能实现高密度和更节能的神经元及突触集成，用于尖峰神经网络（SNN）。本文报告的所有加速器都通过将内存（通常是SRAM元件）移近计算来获益。([seo2011cicc,](#bib.bib239))
    中的45纳米设计是一个小规模的SNN加速架构，嵌入256个泄漏积分放电（LIF）神经元和高达64k个基于随机突触时间依赖性塑性（S-STDP）概念的突触。尽管使用了定制SRAM，并且考虑到其能效数据，它仍然是一种理想的选择，特别是对于边缘计算场景。在相同的集成规模下，ODIN芯片在28纳米CMOS工艺中嵌入256个神经元和64k个基于尖峰驱动突触塑性的4位突触
    ([frenkel2018tbiocas,](#bib.bib78))。65纳米的MorphIC芯片代表了将NPU扩展到SNN应用的第一次尝试，它基于ODIN核心集成到四核设计中
    ([frenkel2019tbiocas,](#bib.bib79))。
- en: 'Concerning large-scale neuromorphic platforms required for cognitive computing
    applications, there are currently two designs offered: the 28 nm IBM TrueNorth
    ([akopyan2015tcad,](#bib.bib5) ) and the 14 nm Intel Loihi ([davies2018micro,](#bib.bib57)
    ) neuromorphic chips. TrueNorth is a GALS design embedding as high as 1M neurons
    and 256M binary non-plastic synapses per chip, where neurons rely on a custom
    model that allows modifying their behaviors by combining up to three neurons ([cassidy2013ijcnn,](#bib.bib31)
    ). Loihi is a fully asynchronous design embedding up to 180k neurons and 114k
    (9- bit) to 1M (binary) synapses per chip. Neurons rely on a LIF model with a
    configurable number of compartments to which several functionalities such as axonal
    and refractory delays, spike latency, and threshold adaptation have been added.
    The spike-based plasticity rule used for synapses is programmable.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 关于认知计算应用所需的大规模神经形态平台，目前有两种设计方案：28纳米的IBM TrueNorth ([akopyan2015tcad,](#bib.bib5))
    和14纳米的Intel Loihi ([davies2018micro,](#bib.bib57)) 神经形态芯片。TrueNorth 是一种GALS设计，每个芯片嵌入高达1M个神经元和256M个二进制非塑性突触，神经元依赖于一种定制模型，该模型允许通过结合多达三个神经元来修改其行为
    ([cassidy2013ijcnn,](#bib.bib31))。Loihi 是一种完全异步的设计，每个芯片嵌入高达180k个神经元和114k（9位）到1M（二进制）突触。神经元依赖于具有可配置数量的隔室的LIF模型，并添加了轴突和折返延迟、尖峰延迟和阈值适应等多个功能。用于突触的尖峰基塑性规则是可编程的。
- en: In digital designs for neuromorphic chips, versatility can be obtained with
    a joint optimization comprising power and area efficiencies. This flexibility
    in optimizing between versatility and efficiency in digital designs is highlighted
    with platforms going from versatility-driven (e.g., SpiNNaker) to efficiency-driven
    (e.g., ODIN and MorphIC), through platforms aiming at a well-balanced trade-off
    on both sides (e.g., Loihi). Table [9](#S5.T9 "Table 9 ‣ 5.4\. Full-digital Neuromorphic
    Accelerators ‣ 5\. Accelerators based on Emerging Paradigms and Technologies ‣
    A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
    summarizes the main characteristics of the neuromorphic chips described so far
    with particular insight on the Energy per spike operation (SOP) that is seen as
    a primary benchmarking factor for these architectures.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经形态芯片的数字设计中，可以通过联合优化功耗和面积效率来获得多样性。这种在数字设计中优化多样性和效率的灵活性通过从以多样性为驱动（例如SpiNNaker）到以效率为驱动（例如ODIN和MorphIC）的平台，以及旨在两者之间实现良好平衡的平台（例如Loihi）得到了突出展示。表[9](#S5.T9
    "Table 9 ‣ 5.4\. Full-digital Neuromorphic Accelerators ‣ 5\. Accelerators based
    on Emerging Paradigms and Technologies ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms")总结了迄今为止描述的神经形态芯片的主要特征，特别是尖峰操作能量（SOP），这是这些架构的主要基准因素。
- en: Table 9. Summary of Neuromorphic chip characteristics based on ([frenkel2021arxiv,](#bib.bib77)
    )
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9. 基于 ([frenkel2021arxiv,](#bib.bib77) ) 的神经形态芯片特性总结
- en: '| Chip name | Technology | Cores | Core Area [mm²] | Neurons per core | Synapses
    per core | Weights storage | Supply Voltage [V] | Energy per SOP [J] |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 芯片名称 | 技术 | 核心数量 | 核心面积 [mm²] | 每核心神经元数量 | 每核心突触数量 | 权重存储 | 供电电压 [V] | 每次操作能量
    [J] |'
- en: '| SpiNNaker ([painkras2013jsscc,](#bib.bib206) ) | 0.13 $\mu$m | 18 | 3.75
    | 1000 | - | Off-chip | 1.2 | $>$11.3n/26.6n |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| SpiNNaker ([painkras2013jsscc,](#bib.bib206) ) | 0.13 $\mu$m | 18 | 3.75
    | 1000 | - | 离芯片 | 1.2 | $>$11.3n/26.6n |'
- en: '| ([seo2011cicc,](#bib.bib239) ) | 45 nm SOI | 1 | 0.8 | 256 | 64k | 1-bit
    SRAM | 0.53 - 1.0 | - |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| ([seo2011cicc,](#bib.bib239) ) | 45 nm SOI | 1 | 0.8 | 256 | 64k | 1-bit
    SRAM | 0.53 - 1.0 | - |'
- en: '| ODIN ([frenkel2018tbiocas,](#bib.bib78) ) | 28 nm FDSOI | 1 | 0.086 | 256
    | 64k | (3+1)-bits (SRAM) | 0.55 - 1.0 | 8.4p/12.7p |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| ODIN ([frenkel2018tbiocas,](#bib.bib78) ) | 28 nm FDSOI | 1 | 0.086 | 256
    | 64k | (3+1)-bits (SRAM) | 0.55 - 1.0 | 8.4p/12.7p |'
- en: '| MorphIC ([frenkel2019tbiocas,](#bib.bib79) ) | 65 nm LP | 4 | 0.715 | 512
    | 528k | 1-bit (SRAM) | 0.8 - 1.2 | 30p/51p |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| MorphIC ([frenkel2019tbiocas,](#bib.bib79) ) | 65 nm LP | 4 | 0.715 | 512
    | 528k | 1-bit (SRAM) | 0.8 - 1.2 | 30p/51p |'
- en: '| TrueNorth ([akopyan2015tcad,](#bib.bib5) ) | 28 nm | 4096 | 0.095 | 256 |
    64k | 1-bit (SRAM) | 0.7 - 1.05 | 26p |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| TrueNorth ([akopyan2015tcad,](#bib.bib5) ) | 28 nm | 4096 | 0.095 | 256 |
    64k | 1-bit (SRAM) | 0.7 - 1.05 | 26p |'
- en: '| Loihi ([davies2018micro,](#bib.bib57) ) | 14 nm FinFET | 128 | 0.4 | 1024
    | 1M | 1- to 9 bits (SRAM) | 0.5 - 1.25 | $>$23.6p | {forest}'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '| Loihi ([davies2018micro,](#bib.bib57) ) | 14 nm FinFET | 128 | 0.4 | 1024
    | 1M | 1-到9位 (SRAM) | 0.5 - 1.25 | $>$23.6p | {forest}'
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [NPU and
    Neuromorphic computing accelerators [MAC acceleration [Full-digital [Samsung ([song2019isscc,](#bib.bib248)
    ), UM+NVIDIA ([zhang2019vlsi,](#bib.bib304) )
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [NPU and
    Neuromorphic computing accelerators [MAC acceleration [Full-digital [Samsung ([song2019isscc,](#bib.bib248)
    ), UM+NVIDIA ([zhang2019vlsi,](#bib.bib304) )
- en: MediaTek ([lin2020isscc,](#bib.bib168) ), Alibaba ([jiao2020isscc,](#bib.bib128)
    )
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: MediaTek ([lin2020isscc,](#bib.bib168) ), Alibaba ([jiao2020isscc,](#bib.bib128)
    )
- en: Samsung ([park2021isscc,](#bib.bib211) ), Samsung ([park2022isscc,](#bib.bib212)
    )
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: Samsung ([park2021isscc,](#bib.bib211) ), Samsung ([park2022isscc,](#bib.bib212)
    )
- en: '] ] [RRAM/PCM-based [ RENO ([liu2015dac,](#bib.bib172) )'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '] ] [RRAM/PCM-based [ RENO ([liu2015dac,](#bib.bib172) )'
- en: NTHU ([xue2019isscc,](#bib.bib296) )
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: NTHU ([xue2019isscc,](#bib.bib296) )
- en: IBM ([narayanan2021ted,](#bib.bib194) )
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: IBM ([narayanan2021ted,](#bib.bib194) )
- en: '] ] ], [Single chip NPUs [Full-digital [DaDianNao ([chen2016commacm,](#bib.bib39)
    ), ShiDianNao ([chen2016commacm,](#bib.bib39) )'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '] ] ], [单芯片 NPU [全数字 [DaDianNao ([chen2016commacm,](#bib.bib39) ), ShiDianNao
    ([chen2016commacm,](#bib.bib39) )'
- en: Cambricon ([zhang2016micro,](#bib.bib305) ), EIE ([han2016isca,](#bib.bib103)
    )
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Cambricon ([zhang2016micro,](#bib.bib305) ), EIE ([han2016isca,](#bib.bib103)
    )
- en: STM ([desoli17isscc,](#bib.bib62) ), IBM ([oh2020vlsic,](#bib.bib202) )
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: STM ([desoli17isscc,](#bib.bib62) ), IBM ([oh2020vlsic,](#bib.bib202) )
- en: IBM ([lee2022jsscc,](#bib.bib160) )
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: IBM ([lee2022jsscc,](#bib.bib160) )
- en: '] ] [In-memory computing [STM ([desoli2023isscc,](#bib.bib61) ), ISAAC ([shafiee2016isca,](#bib.bib241)
    )'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '] ] [内存计算 [STM ([desoli2023isscc,](#bib.bib61) ), ISAAC ([shafiee2016isca,](#bib.bib241)
    )'
- en: PipeLayer ([song2017hpca,](#bib.bib249) ), NeuralPIM ([cao2022tcomp,](#bib.bib29)
    )
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: PipeLayer ([song2017hpca,](#bib.bib249) ), NeuralPIM ([cao2022tcomp,](#bib.bib29)
    )
- en: PRIME ([chi2016isca,](#bib.bib45) ), NeuRRAM ([wan2022nature,](#bib.bib284)
    )
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: PRIME ([chi2016isca,](#bib.bib45) ), NeuRRAM ([wan2022nature,](#bib.bib284)
    )
- en: Hermes ([khaddam2022hermes,](#bib.bib140) )
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: Hermes ([khaddam2022hermes,](#bib.bib140) )
- en: '] ] ], [Neuromorphic chips [SpiNNaker ([painkras2013jsscc,](#bib.bib206) ),
    Seo et al. ([seo2011cicc,](#bib.bib239) )'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '] ] ], [神经形态芯片 [SpiNNaker ([painkras2013jsscc,](#bib.bib206) ), Seo 等 ([seo2011cicc,](#bib.bib239)
    )'
- en: ODIN ([frenkel2018tbiocas,](#bib.bib78) ), MorphIC ([frenkel2019tbiocas,](#bib.bib79)
    )
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ODIN ([frenkel2018tbiocas,](#bib.bib78) ), MorphIC ([frenkel2019tbiocas,](#bib.bib79)
    )
- en: TrueNorth ([akopyan2015tcad,](#bib.bib5) ), Loihi ([davies2018micro,](#bib.bib57)
    )
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: TrueNorth ([akopyan2015tcad,](#bib.bib5) ), Loihi ([davies2018micro,](#bib.bib57)
    )
- en: '] ], ]'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '] ], ]'
- en: Figure 8. Taxonomy of neural accelerators discussed in Sections [4.2.2](#S4.SS2.SSS2
    "4.2.2\. Neural Processing Units (NPUs) ‣ 4.2\. ASIC-based Accelerators ‣ 4\.
    Hardware Accelerators ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms"), [4.2.3](#S4.SS2.SSS3 "4.2.3\. Single-chip NPUs ‣ 4.2\. ASIC-based
    Accelerators ‣ 4\. Hardware Accelerators ‣ A Survey on Deep Learning Hardware
    Accelerators for Heterogeneous HPC Platforms"), [5.3](#S5.SS3 "5.3\. In-memory
    computing accelerators based on emerging memories ‣ 5\. Accelerators based on
    Emerging Paradigms and Technologies ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms"), [5.4](#S5.SS4 "5.4\. Full-digital Neuromorphic
    Accelerators ‣ 5\. Accelerators based on Emerging Paradigms and Technologies ‣
    A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8. 神经加速器的分类，如第[4.2.2](#S4.SS2.SSS2 "4.2.2. 神经处理单元 (NPU) ‣ 4.2. 基于 ASIC 的加速器
    ‣ 4. 硬件加速器 ‣ 关于异构 HPC 平台深度学习硬件加速器的调查")、[4.2.3](#S4.SS2.SSS3 "4.2.3. 单芯片 NPU ‣
    4.2. 基于 ASIC 的加速器 ‣ 4. 硬件加速器 ‣ 关于异构 HPC 平台深度学习硬件加速器的调查")、[5.3](#S5.SS3 "5.3. 基于新兴存储器的内存计算加速器
    ‣ 5. 基于新兴范式和技术的加速器 ‣ 关于异构 HPC 平台深度学习硬件加速器的调查")、[5.4](#S5.SS4 "5.4. 全数字神经形态加速器
    ‣ 5. 基于新兴范式和技术的加速器 ‣ 关于异构 HPC 平台深度学习硬件加速器的调查")
- en: 5.5\. Accelerators based on Multi-Chip Modules
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5. 基于多芯片模块的加速器
- en: The alternate multichip-module (MCM) silicon interposer-based integration technology,
    described in Section A.8 of Appendix A, offers several advantages over single-chip
    designs. These advantages include increased functionality, reduced power consumption,
    higher performance, improved reliability, and cost savings.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 附录 A 的第 A.8 节中描述的替代多芯片模块 (MCM) 硅互连集成技术相较于单芯片设计提供了若干优势。这些优势包括功能增加、功耗减少、性能提升、可靠性提高和成本节省。
- en: By utilizing MCM, designers can combine multiple chips and functionalities into
    a single package, resulting in a reduced overall footprint and cost. MCM-based
    designs can also be optimized for different power requirements, achieving higher
    power efficiency compared to single-chip designs. Additionally, MCM-based designs
    can be fine-tuned for different tasks, offering superior performance compared
    to single-chip counterparts.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 MCM，设计师可以将多个芯片和功能集成到一个包中，从而减少整体体积和成本。基于 MCM 的设计还可以针对不同的功率需求进行优化，与单芯片设计相比，实现更高的功率效率。此外，基于
    MCM 的设计可以针对不同任务进行微调，提供优于单芯片设计的性能。
- en: The use of redundancy in MCM-based designs improves fault tolerance, leading
    to improved reliability compared to single-chip designs. Furthermore, MCM-based
    designs can utilize off-the-shelf components and existing manufacturing processes
    and technology, contributing to cost savings in overall manufacturing.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: MCM 设计中使用的冗余提高了容错能力，从而提升了相较于单芯片设计的可靠性。此外，MCM 设计可以利用现成的组件和现有的制造工艺和技术，降低整体制造成本。
- en: Figure [9](#S5.F9 "Figure 9 ‣ 5.5\. Accelerators based on Multi-Chip Modules
    ‣ 5\. Accelerators based on Emerging Paradigms and Technologies ‣ A Survey on
    Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms") illustrates
    a comprehensive taxonomy of the MCM-based designs explored in this survey. Specifically,
    Sec. A.8.2 of Appendix A discusses a collection of representative general-purpose
    MCM-based designs, while this section concentrates on MCM-based DNN accelerators.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#S5.F9 "图 9 ‣ 5.5. 基于多芯片模块的加速器 ‣ 5. 基于新兴范式和技术的加速器 ‣ 关于异构 HPC 平台深度学习硬件加速器的调查")
    说明了本调查中探讨的基于 MCM 的设计的全面分类法。具体而言，附录 A 的第 A.8.2 节讨论了一系列具有代表性的通用 MCM 设计，而本节则集中在基于
    MCM 的 DNN 加速器上。
- en: '{forest}'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '{forest}'
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [Accelerators
    based on Multi-Chip Modules [General Purpose [Passive Interposer [ Mounce et al.([mounce_ac16,](#bib.bib190)
    )
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree，[基于多芯片模块的加速器
    [通用 [被动互连 [ Mounce 等人（[mounce_ac16](#bib.bib190)）
- en: Vijayaraghavan et al. ([vijayaraghavan_hpca17,](#bib.bib282) )
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Vijayaraghavan 等人（[vijayaraghavan_hpca17](#bib.bib282)）
- en: Arunkumar et al. ([arunkumar_isca17,](#bib.bib14) ), Lin et al. ([lin_vlsi19,](#bib.bib169)
    ) ] ], [Active Interposer [ Vivet et al.([vivet_jssc21,](#bib.bib283) )
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Arunkumar 等人（[arunkumar_isca17](#bib.bib14)），Lin 等人（[lin_vlsi19](#bib.bib169)），[Active
    Interposer [ Vivet 等人（[vivet_jssc21](#bib.bib283)）
- en: Martinez et al. ([martinez_vlsi20,](#bib.bib179) ) ] ] ], [Domain Specific [
    Kwon et al.([kwon_iceic23,](#bib.bib153) ), Nurvitadhi et al. ([nurvitadhi_fccm19,](#bib.bib196)
    )
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Martinez 等 ([martinez_vlsi20,](#bib.bib179) ) ] ] ], [领域特定 [ Kwon 等 ([kwon_iceic23,](#bib.bib153)
    ), Nurvitadhi 等 ([nurvitadhi_fccm19,](#bib.bib196) )
- en: Verhelst et al. ([verhelst2022ml,](#bib.bib281) ), Centaur ([hwang_isca20,](#bib.bib116)
    )
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Verhelst 等 ([verhelst2022ml,](#bib.bib281) ), Centaur ([hwang_isca20,](#bib.bib116)
    )
- en: Lan et al. ([lan_eptc21,](#bib.bib155) ), Lego ([xuan_isocc22,](#bib.bib295)
    )
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Lan 等 ([lan_eptc21,](#bib.bib155) ), Lego ([xuan_isocc22,](#bib.bib295) )
- en: Chimera ([prabhu2022chimera,](#bib.bib221) ), SWAP ([sharma2022swap,](#bib.bib244)
    )
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Chimera ([prabhu2022chimera,](#bib.bib221) ), SWAP ([sharma2022swap,](#bib.bib244)
    )
- en: SPRINT ([li2021sprint,](#bib.bib167) ), Simba ([zimmer_jssc20,](#bib.bib308)
    ; [shao_micro19,](#bib.bib243) ) ] ] ]
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: SPRINT ([li2021sprint,](#bib.bib167) ), Simba ([zimmer_jssc20,](#bib.bib308)
    ; [shao_micro19,](#bib.bib243) ) ] ] ]
- en: Figure 9. Taxonomy of MCM based accelerators discussed in Section [5.5](#S5.SS5
    "5.5\. Accelerators based on Multi-Chip Modules ‣ 5\. Accelerators based on Emerging
    Paradigms and Technologies ‣ A Survey on Deep Learning Hardware Accelerators for
    Heterogeneous HPC Platforms")
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9. 在第[5.5](#S5.SS5 "5.5\. 基于多芯片模块的加速器 ‣ 5\. 基于新兴范式和技术的加速器 ‣ 深度学习硬件加速器在异构 HPC
    平台上的调查")节中讨论的基于 MCM 的加速器分类。
- en: In the realm of DL, chiplet-based design is utilized to create hardware accelerator
    platforms that are both efficient and scalable. Designing AI processors for data
    explosion computing due to the physical limitations of semiconductors and high
    costs is challenging. In ([kwon_iceic23,](#bib.bib153) ) authors propose chiplet-based
    design as a viable solution to this problem. They outline various aspects of designing
    a chiplet AI processor, including incorporating NPU chiplets, HBM chiplets, and
    2.5D interposers, ensuring signal integrity for high-speed interconnections, power
    delivery network for chiplets, bonding reliability, thermal stability, and interchiplet
    data transfer on heterogeneous integration architecture. They conclude that chiplet-based
    design provides higher performance at a lower cost compared to IP-based design.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习领域，chiplet 基设计被用于创建既高效又可扩展的硬件加速器平台。由于半导体的物理限制和高成本，设计用于数据爆炸计算的 AI 处理器是具有挑战性的。在 ([kwon_iceic23,](#bib.bib153)
    )中，作者提出 chiplet 基设计作为解决此问题的可行方案。他们概述了设计 chiplet AI 处理器的各个方面，包括集成 NPU chiplets、HBM
    chiplets 和 2.5D 中介层，确保高速度互连的信号完整性、chiplets 的电源供应网络、粘合可靠性、热稳定性和异构集成架构上的 chiplet
    数据传输。他们总结道，chiplet 基设计在成本更低的情况下提供了更高的性能，相较于基于 IP 的设计。
- en: Data-intensive DL algorithms with strict latency constraints require balancing
    both data movement and compute capabilities. Thus, persistent approaches that
    keep the entire DL model on-chip are becoming the new norm for real-time services
    to avoid expensive off-chip memory accesses. In ([nurvitadhi_fccm19,](#bib.bib196)
    ) it is shown how the integration of FPGA with ASIC chiplets outperforms GPU-based
    platforms (NVIDIA Volta) in terms of latency by enhancing on-chip memory capacity
    and bandwidth and provides compute throughput matching the required bandwidth.
    Specifically, it is reported that the GPU and chiplet-based FPGA computing capabilities
    are 6% and 57% of their peak, respectively. In terms of delay and energy efficiency,
    the FPGA outperforms the GPU with a delay that is 1/16 and energy efficiency that
    is 34x better than the GPU’s peak performance.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 数据密集型深度学习算法具有严格的延迟要求，需要平衡数据移动和计算能力。因此，保持整个深度学习模型在芯片上的持久方法正在成为实时服务的新常态，以避免昂贵的离芯片内存访问。在 ([nurvitadhi_fccm19,](#bib.bib196)
    )中展示了如何通过提升芯片内存容量和带宽，使集成 FPGA 与 ASIC chiplets 在延迟方面超越 GPU 平台（NVIDIA Volta），并提供与所需带宽匹配的计算吞吐量。具体来说，据报告
    GPU 和 chiplet 基 FPGA 计算能力分别为其峰值的 6% 和 57%。在延迟和能效方面，FPGA 在延迟上优于 GPU，延迟为 GPU 的 1/16，能效是
    GPU 峰值性能的 34 倍。
- en: In ([verhelst2022ml,](#bib.bib281) ) the authors investigate the recent multi-core
    trend in deep-learning accelerators evolution as a solution to further increase
    throughput and match the ever-growing computational demands. Chiplet integration
    is considered a promising implementation strategy for both homogeneous and heterogeneous
    multi-core accelerators. In ([hwang_isca20,](#bib.bib116) ) authors present a
    new chiplet-based hybrid sparse-dense accelerator called Centaur, which addresses
    memory-intensive embedding layers and compute-intensive multi-layer perceptron
    layers. The proposed accelerator demonstrates significant performance speedup
    and energy efficiency improvement compared to conventional approaches monolithic
    approaches. The trend towards developing high throughput and energy-efficient
    neural network hardware accelerators due to the growing complexity and dimension
    of neural network algorithms is analyzed in ([lan_eptc21,](#bib.bib155) ). The
    authors propose a chiplet-based architecture for a multi-core neuromorphic processor
    with a chip-package co-design flow. It is shown how the proposed design is reusable
    for different neuromorphic computing applications by scaling the number of chips
    in a package and by reusing existing IPs from different technology nodes with
    2.5D integration technology. The challenges of using modern DNN accelerators in
    multi-tenant DNN data centers are investigated in ([xuan_isocc22,](#bib.bib295)
    ). The MCM architecture is proposed as a promising approach to address this issue
    but highlights the challenge of distributing DNN model layers with different parameters
    across chiplets. Thus the authors present Lego MCM architecture with a dynamic
    scheduler that adapts to the size of DNN model layers and increases chiplet utilization.
    The results show that Lego MCM achieves a 1.51x speedup over a monolithic DNN
    accelerator. Chimera ([prabhu2022chimera,](#bib.bib221) ) is a non-volatile chip
    for DNN training and inference that does not require off-chip memory. Multiple
    Chimera accelerator chiplets can be combined in a multi-chip system to enable
    inference on models larger than the single-chip memory with only $5\%$ energy
    overhead. Chiplet-based PIM DNN hardware accelerators have also been proposed.
    SWAP ([sharma2022swap,](#bib.bib244) ) is a DNN inference accelerator based on
    the 2.5D integration of multiple resistive RAM chiplets that allows fabrication
    cost reductions. The authors also propose a design space exploration flow to optimize
    the interconnection Network-on-Package, minimizing inter-chiplet communications
    and enabling link pruning. Inter-chiplet communication remains one of the main
    challenges in multi-chiplet architectures. Authors in ([li2021sprint,](#bib.bib167)
    ) investigate photonic-based interconnects as an alternative to metallic-based
    inter-chiplet networks and propose a DNN inference accelerator namely SPRINT.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在([verhelst2022ml,](#bib.bib281) )中，作者研究了深度学习加速器演化中的近期多核趋势，以解决提高吞吐量和匹配不断增长的计算需求。Chiplet集成被认为是一种有前途的实现策略，适用于同质和异质多核加速器。在([hwang_isca20,](#bib.bib116)
    )中，作者提出了一种新的基于Chiplet的混合稀疏-密集加速器，称为Centaur，旨在解决内存密集型的嵌入层和计算密集型的多层感知机层。与传统的整体化方法相比，所提加速器在性能加速和能源效率方面显示了显著的提升。由于神经网络算法的复杂性和维度的增长，开发高吞吐量和能源高效的神经网络硬件加速器的趋势在([lan_eptc21,](#bib.bib155)
    )中进行了分析。作者提出了一种基于Chiplet的多核神经形态处理器架构，采用了芯片封装协同设计流程。展示了如何通过扩展封装中的芯片数量和使用不同技术节点的现有IP，利用2.5D集成技术使所提设计适用于不同的神经形态计算应用。([xuan_isocc22,](#bib.bib295)
    )中研究了在多租户DNN数据中心使用现代DNN加速器的挑战。MCM架构被提议作为一种有前途的方法来解决这一问题，但突出了在Chiplet之间分配具有不同参数的DNN模型层的挑战。因此，作者提出了Lego
    MCM架构，配有适应DNN模型层大小的动态调度器，以提高Chiplet的利用率。结果表明，Lego MCM相较于整体化DNN加速器实现了1.51倍的加速。Chimera
    ([prabhu2022chimera,](#bib.bib221) )是一个用于DNN训练和推理的非易失性芯片，不需要外部内存。多个Chimera加速器Chiplet可以组合成一个多芯片系统，从而在单芯片内存之外的模型上进行推理，能耗增加仅为$5\%$。也有基于Chiplet的PIM
    DNN硬件加速器被提出。SWAP ([sharma2022swap,](#bib.bib244) )是一个基于多个电阻RAM Chiplet的2.5D集成的DNN推理加速器，允许降低制造成本。作者还提出了一种设计空间探索流程，以优化封装上的互连网络，最小化芯片间通信，并实现链路裁剪。芯片间通信仍然是多Chiplet架构的主要挑战之一。在([li2021sprint,](#bib.bib167)
    )中，作者研究了光子基互连作为金属基芯片间网络的替代方案，并提出了名为SPRINT的DNN推理加速器。
- en: '![Refer to caption](img/22f709a77d6b9b2905a308d5d4e8b0e7.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/22f709a77d6b9b2905a308d5d4e8b0e7.png)'
- en: 'Figure 10. Simba architecture ([shao_micro19,](#bib.bib243) ) from left to
    right: package with 36 chiplets, chiplet, and processing element.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10. Simba 架构 ([shao_micro19,](#bib.bib243) ) 从左到右：包含 36 个芯片小块的封装、芯片小块和处理单元。
- en: Finally, as a representative chiplet-based DNN hardware accelerator, we report
    Simba ([zimmer_jssc20,](#bib.bib308) ; [shao_micro19,](#bib.bib243) ). Simba is
    a scalable DNN accelerator consisting of 36 chiplets connected in a mesh network
    on a multi-chip module using ground-referenced signaling. Simba enables flexible
    scaling for efficient inference on a wide range of DNNs, from mobile to data center
    domains. The prototype achieves high area efficiency, energy efficiency, and peak
    performance for both one-chiplet and 36-chiplet systems. Simba architecture is
    shown in Figure [10](#S5.F10 "Figure 10 ‣ 5.5\. Accelerators based on Multi-Chip
    Modules ‣ 5\. Accelerators based on Emerging Paradigms and Technologies ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms"). It implements
    a tile-based architecture and adopts a hierarchical interconnect to efficiently
    connect different PEs. This hierarchical interconnect consists of a network-on-chip
    (NoC) that connects PEs on the same chiplet and a network-on-package (NoP) that
    connects chiplets together on the same package. Each Simba chiplet contains an
    array of PEs, a global PE, an NoP router, and a controller, all connected by a
    chiplet-level interconnect. Table [10](#S5.T10 "Table 10 ‣ 5.5\. Accelerators
    based on Multi-Chip Modules ‣ 5\. Accelerators based on Emerging Paradigms and
    Technologies ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms") presents a summary of the key characteristics of a representative
    subset of chiplet-based DNN accelerators that were reviewed earlier.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，作为一种代表性的基于芯片小块的 DNN 硬件加速器，我们报告了 Simba ([zimmer_jssc20,](#bib.bib308) ; [shao_micro19,](#bib.bib243)
    )。Simba 是一个可扩展的 DNN 加速器，由 36 个芯片小块组成，这些小块通过基于地参考信号的网格网络连接在一个多芯片模块上。Simba 使得在从移动设备到数据中心领域的广泛
    DNN 上进行高效推理的灵活扩展成为可能。该原型在单芯片小块和 36 芯片小块系统中都实现了高区域效率、能源效率和峰值性能。Simba 架构如图 [10](#S5.F10
    "Figure 10 ‣ 5.5\. Accelerators based on Multi-Chip Modules ‣ 5\. Accelerators
    based on Emerging Paradigms and Technologies ‣ A Survey on Deep Learning Hardware
    Accelerators for Heterogeneous HPC Platforms") 所示。它实现了基于块的架构，并采用了分层互连来高效连接不同的处理单元（PE）。这种分层互连包括一个连接同一芯片小块上
    PE 的片上网络（NoC）和一个连接同一封装上芯片小块的封装网络（NoP）。每个 Simba 芯片小块包含一个 PE 阵列、一个全球 PE、一个 NoP 路由器和一个控制器，所有这些都通过芯片小块级互连连接。表
    [10](#S5.T10 "Table 10 ‣ 5.5\. Accelerators based on Multi-Chip Modules ‣ 5\.
    Accelerators based on Emerging Paradigms and Technologies ‣ A Survey on Deep Learning
    Hardware Accelerators for Heterogeneous HPC Platforms") 总结了早前审查的代表性芯片小块 DNN 加速器的关键特性。
- en: Table 10. Summary of Chiplet-based DNN Accelerators.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10. 基于芯片小块的 DNN 加速器总结。
- en: '|  | Simba ([shao_micro19,](#bib.bib243) ) | Centaur ([hwang_isca20,](#bib.bib116)
    ) | Lego ([xuan_isocc22,](#bib.bib295) ) | Chimera ([prabhu2022chimera,](#bib.bib221)
    ) | SWAP ([sharma2022swap,](#bib.bib244) ) | SPRINT ([li2021sprint,](#bib.bib167)
    ) |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  | Simba ([shao_micro19,](#bib.bib243) ) | Centaur ([hwang_isca20,](#bib.bib116)
    ) | Lego ([xuan_isocc22,](#bib.bib295) ) | Chimera ([prabhu2022chimera,](#bib.bib221)
    ) | SWAP ([sharma2022swap,](#bib.bib244) ) | SPRINT ([li2021sprint,](#bib.bib167)
    ) |'
- en: '| Technology | 16nm | FPGA | FPGA | 40nm | - | 28nm |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 技术 | 16nm | FPGA | FPGA | 40nm | - | 28nm |'
- en: '| Area | 6 mm²* | - | 19016 LUT, 16916 FF 0.5 BRAM, 28 DSP† | 29.2 mm² | -
    | - |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 面积 | 6 mm²* | - | 19016 LUT, 16916 FF 0.5 BRAM, 28 DSP† | 29.2 mm² | - |
    - |'
- en: '| Power Efficiency | 9.1 TOPS/W** | - | - | 2.2 TOPS/W | - | - |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 能效 | 9.1 TOPS/W** | - | - | 2.2 TOPS/W | - | - |'
- en: '| Throughput | 4–128 TOPS | 0.313 TOPS | - | 0.92 TOPS | - | - |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 吞吐量 | 4–128 TOPS | 0.313 TOPS | - | 0.92 TOPS | - | - |'
- en: '| Frequency | 161 MHz–1.8 GHz | 200 MHz | - | 200 MHz | - | - |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 频率 | 161 MHz–1.8 GHz | 200 MHz | - | 200 MHz | - | - |'
- en: '| Precisions | int8 | - | - | int8, fp16 | - | - |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 精度 | int8 | - | - | int8, fp16 | - | - |'
- en: '| On-chip Memory | 752 KiB* | - | - | 2.5 MB‡ | - | 128 KiB* |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 片上内存 | 752 KiB* | - | - | 2.5 MB‡ | - | 128 KiB* |'
- en: '| Chiplet Bandwidth | 100 GB/s | - | - | 1.9 Gbps | - | 180 Gbps |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 芯片小块带宽 | 100 GB/s | - | - | 1.9 Gbps | - | 180 Gbps |'
- en: '| Processing Type | Near-Memory | Near-Memory | Near-Memory | Near-Memory |
    In-Memory | Near-Memory |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 处理类型 | 内存附近 | 内存附近 | 内存附近 | 内存附近 | 内存中 | 内存附近 |'
- en: '| Sparsity | ✗ | ✓ | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏性 | ✗ | ✓ | ✗ | ✗ | ✗ | ✗ |'
- en: '| Interconnect | Wired Mesh (GRS) | - | - | Wired App. specific‡‡ | Wired Pruned
    | Optical§ |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 互连 | 有线网格（GRS） | - | - | 有线应用特定‡‡ | 有线剪枝 | 光学§ |'
- en: '| Applications | CNN Inference | Recommendation Inference | Multi-Tenant Inference
    | Inference Training | Multiple Applications | CNN Inference |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | CNN 推理 | 推荐推理 | 多租户推理 | 推理训练 | 多种应用 | CNN 推理 |'
- en: '| *One chiplet, **When operating at a minimum voltage of 0.42 V with a 161
    MHz PE frequency |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| *一个芯片，小于0.42 V的最低电压下运行，161 MHz的PE频率 |'
- en: '| †16$\times$16 chiplet, ‡2 MB RRAM, 0.5 MB SRAM, ‡‡C2C links (77 pJ/bit, 1.9
    Gbits/s), §0.77 pJ/bit |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| †16$\times$16芯片，小型2 MB RRAM，0.5 MB SRAM，C2C链路（77 pJ/bit，1.9 Gbits/s），0.77
    pJ/bit |'
- en: 6\. Conclusions and Open Challenges
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6. 结论与开放挑战
- en: The Deep Learning ecosystem based on advanced computer architectures and memory
    technologies spans from edge computing solutions to high-performance servers,
    supercomputers, and up to large data centers for data analytics. In this context,
    the main objective of this survey is to provide an overview of the leading computing
    platforms utilized for accelerating the execution and enhancing the efficiency
    of high-performance Deep Learning applications. More in detail, this survey includes
    GPU-based accelerators, Tensor Processor Units, FPGA-based accelerators, Neural
    Processing Units, and co-processors based on the open-hardware RISC-V architecture.
    The survey also describes accelerators based on emerging computing paradigms and
    technologies, such as 3D-stacked PIM, emerging non-volatile memories, NPUs, and
    Multi-Chip Modules.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 基于先进计算架构和内存技术的深度学习生态系统涵盖了从边缘计算解决方案到高性能服务器、超级计算机以及大型数据中心的数据分析。在这种背景下，本调查的主要目标是提供一个关于加速高性能深度学习应用执行和提高效率的领先计算平台的概述。更详细地说，本调查包括基于GPU的加速器、张量处理单元、基于FPGA的加速器、神经处理单元，以及基于开放硬件RISC-V架构的协处理器。调查还描述了基于新兴计算范式和技术的加速器，例如3D堆叠PIM、新兴非易失性存储器、NPU和多芯片模块。
- en: 'Before concluding this survey, we would like to introduce some open challenges
    on two promising technologies to further speed up DL workloads: Quantum Computing
    and Photonic Computing. There is a general agreement that quantum computers will
    not replace classical computing systems, but they will be used in combination
    with supercomputers to accelerate some hard-to-compute problems. Quantum computers
    will play the role of unconventional accelerators with the goal to outperform
    conventional supercomputers, thanks to the improved parallelism which enables
    the so-called *quantum speedup*. Governments, supercomputing centers, and companies
    around the world have also started to investigate $How$/$When$/$Where$ quantum
    processing units (QPUs) could fit into HPC infrastructures to speed up some heavy
    tasks, such as Deep Learning workloads. Emerging trends and commercial solutions
    related to *hybrid* quantum-classical supercomputers are described in  ([HPCWIRE2022,](#bib.bib113)
    ). To address this challenging trend, in October 2022, the EuroHPC Joint Undertaking
    initiative selected six supercomputing centers across the European Union to host
    quantum computers and simulators. IBM Research was the first provider to offer
    a cloud-based QC service. IBM Qiskit  ([QISKIT2023,](#bib.bib228) ) is an open-source
    SDK based on a library of quantum gates/circuits: Remote users can develop quantum
    programs and execute them on quantum simulators and cloud-based quantum processors.
    Cloud providers have also jumped into the quantum race. As an example, Amazon
    Braket ([BRAKET2023,](#bib.bib23) ) is a QC service based on different types of
    quantum systems and simulators, including the quantum annealer from D-Wave. On
    this trend, there is a general agreement that GPUs will play a key role in hybrid
    quantum-classical computing systems. GPU company NVIDIA offers CuQuantum DGX hardware
    appliance integrating a software container on a full-stack quantum circuit simulator:
    The system uses NVIDIA’s A100 GPUs to accelerate quantum simulation workloads.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本次调查之前，我们想介绍两项有前景的技术上的一些开放挑战，以进一步加快DL工作负载的速度：量子计算和光子计算。普遍认为，量子计算机不会取代经典计算系统，但它们将与超级计算机结合使用，以加速一些难以计算的问题。量子计算机将作为非常规加速器，目标是超越传统超级计算机，这得益于改进的并行性，从而实现所谓的*量子加速*。各国政府、超级计算中心和公司也开始研究量子处理单元（QPU）如何适应高性能计算（HPC）基础设施，以加速一些繁重的任务，如深度学习工作负载。有关*混合*量子-经典超级计算机的新兴趋势和商业解决方案描述见 ([HPCWIRE2022,](#bib.bib113)
    )。为应对这一挑战趋势，2022年10月，EuroHPC联合实施机构选择了欧盟内的六个超级计算中心来部署量子计算机和模拟器。IBM Research是首个提供基于云的量子计算服务的供应商。IBM
    Qiskit ([QISKIT2023,](#bib.bib228) )是一个基于量子门/电路库的开源SDK：远程用户可以开发量子程序，并在量子模拟器和基于云的量子处理器上执行这些程序。云服务提供商也纷纷加入了量子竞赛。例如，Amazon
    Braket ([BRAKET2023,](#bib.bib23) )是一个基于不同类型的量子系统和模拟器的量子计算服务，包括来自D-Wave的量子退火器。在这一趋势下，普遍认为GPU将在混合量子-经典计算系统中发挥关键作用。GPU公司NVIDIA提供了CuQuantum
    DGX硬件设备，集成了一个全栈量子电路模拟器的软件容器：该系统使用NVIDIA的A100 GPU来加速量子模拟工作负载。
- en: Recently, a survey on QC technologies appeared in  ([GYONGYOSI201951,](#bib.bib100)
    ), while another survey on QC frameworks appeared in  ([upama2022,](#bib.bib271)
    ). More specifically, there is a promising research trend on *Quantum Machine
    Learning*  ([Biamonte2017,](#bib.bib18) ) which aims at developing quantum algorithms
    that outperform classical computing algorithms on machine learning tasks such
    as recommendation systems. More in detail, classical deep neural networks inspired
    the development of *Deep Quantum Learning* methods. The main advantage of these
    methods is that they do not require a large, general-purpose quantum computer.
    Quantum annealers, such as the D-Wave commercial solutions ([D-WAVE2023,](#bib.bib55)
    ), are well-suited for implementing deep quantum learners. Quantum annealers are
    special-purpose quantum processors that are significantly easier to construct
    and scale up than general-purpose quantum computers. Following this research trend,
    Google proposed TensorFlow Quantum (TFQ)  ([broughton2021,](#bib.bib24) ), an
    open-source quantum machine learning library for prototyping hybrid quantum-classical
    ML models.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一项关于QC技术的调查出现在([GYONGYOSI201951,](#bib.bib100))，而另一项关于QC框架的调查则出现在([upama2022,](#bib.bib271))。更具体地说，有一个有前景的研究趋势，即*量子机器学习*([Biamonte2017,](#bib.bib18))，旨在开发在机器学习任务（如推荐系统）中超越经典计算算法的量子算法。更详细地说，经典深度神经网络启发了*深度量子学习*方法的发展。这些方法的主要优势是它们不需要大型通用量子计算机。量子退火器，如D-Wave商业解决方案([D-WAVE2023,](#bib.bib55))，非常适合用于实现深度量子学习器。量子退火器是特殊用途的量子处理器，比通用量子计算机更容易构建和扩展。继这一研究趋势之后，谷歌提出了TensorFlow
    Quantum（TFQ）([broughton2021,](#bib.bib24)），这是一个用于原型设计混合量子-经典机器学习模型的开源量子机器学习库。
- en: The second challenging and promising research direction is represented by the
    use of Photonic Computing to further accelerate DL tasks on HPC infrastructures.
    Photonic Computing relies on the computation of electromagnetic waves typically
    via non-linear modulation and interference effects. It was originally introduced
    in the 1980s to address optical pattern recognition and optical Fourier transform
    processing ([ambs_2010,](#bib.bib8) ). Despite the potential advantages of processing
    parallelism and speed, optical computing has never translated into a commercial
    technology. Only recently, due to the emergence of data-intensive computing tasks,
    such as AI, optical computing has seen a renewed interest. There are two main
    advantages of optical computing, namely (i) the inherent speed of signal transmission,
    where light pulses can be transferred without the typical RC delays and IR drop
    of electrical interconnects, and (ii) the inherent parallelism, where multiple
    wavelengths, polarizations, and modes can be processed by the same hardware (e.g.,
    waveguides, interferometers, etc.), without interfering with each other. These
    properties can provide strong benefits to data-intensive computing tasks such
    as DL. Photonic computing represents a promising platform for accelerating AI.
    For instance, it has been estimated that photonic multiply-accumulate operations
    can show significant improvements over digital electronics in terms of energy
    efficiency ($>10^{2}$), speed ($>10^{3}$), and compute density ($>10^{2}$) ([nahmias_2020,](#bib.bib192)
    ). However, there are still many challenges to developing an industrially feasible
    photonic system. The main challenge is the area/energy inefficiency of processing
    across the mixed optical/electronic domain. Optical-electrical conversion and
    vice versa result in considerable overhead in terms of area and power consumption.
    To bridge this gap, the trend is developing silicon photonic integrated circuits
    (PICs) with increasing robustness, manufacturability, and scalability. Photonic
    computing essentially operates in the analog domain, thus accuracy is deeply affected
    by accumulated noise and imprecision of optical devices, such as electro-optic
    and phase change modulators. These challenges, similar to those arising in analog
    IMC, might be mitigated by hardware-aware training and system-level calibration
    techniques.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个具有挑战性且前景广阔的研究方向是利用光子计算进一步加速高性能计算（HPC）基础设施上的深度学习（DL）任务。光子计算依赖于电磁波的计算，通常通过非线性调制和干涉效应实现。它最早在1980年代被引入，旨在解决光学模式识别和光学傅里叶变换处理（[ambs_2010,](#bib.bib8)）。尽管光学计算在处理并行性和速度方面具有潜在优势，但它从未转化为商业技术。直到最近，由于数据密集型计算任务（如人工智能）的出现，光学计算才重新引起关注。光学计算主要有两个优势，即（i）信号传输的固有速度，其中光脉冲可以在没有电气互连典型RC延迟和IR降落的情况下传输，以及（ii）固有的并行性，其中多个波长、偏振和模式可以由同一硬件（例如，光波导、干涉仪等）处理，而不会相互干扰。这些特性可以为数据密集型计算任务（如深度学习）提供强大的好处。光子计算代表了加速人工智能的一个有前景的平台。例如，估计光子乘加运算在能效（$>10^{2}$）、速度（$>10^{3}$）和计算密度（$>10^{2}$）方面比数字电子器件具有显著改进（[nahmias_2020,](#bib.bib192)）。然而，开发工业可行的光子系统仍面临许多挑战。主要挑战是光电混合领域的面积/能量低效。光电转换及其反向过程在面积和功耗方面产生了相当大的开销。为了解决这一问题，趋势是开发具有更高鲁棒性、可制造性和可扩展性的硅光子集成电路（PICs）。光子计算本质上在模拟领域中运行，因此准确性受到光学器件（如电光和相变调制器）的噪声和不精确度的严重影响。这些挑战，类似于模拟IMC中出现的挑战，可能通过硬件感知训练和系统级校准技术得到缓解。
- en: Appendix A Appendix
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: 'A.1\. Deep Learning Background: Concepts and Terminology'
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1\. 深度学习背景：概念与术语
- en: 'Deep Learning ([Schmidhuber14,](#bib.bib237) ; [lecun2015,](#bib.bib158) )
    is a subset of ML methods that use artificial DNNs for automatically discover
    the representations needed for feature detection or classification from large
    data sets, by employing multiple layers of processing to extract progressively
    higher-level features. DNNs mimic human brain functionalities, in which neurons
    are interconnected with each other to receive information, process it, and pass
    it to other neurons. As shown in Figure [11(a)](#A1.F11.sf1 "In Figure 11 ‣ A.1\.
    Deep Learning Background: Concepts and Terminology ‣ Appendix A Appendix ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms"), in a
    way similar to the brain’s neuron, the simple model of a perceptron (artificial
    neuron) receives information from a set of inputs and applies a nonlinear function
    F (activation function) on a weighted (W) sum of the inputs (X) ([Rosenblatt1957,](#bib.bib234)
    ). DNNs are composed of a number of layers of artificial neurons (hidden layers),
    organized between the input layer, which brings the initial data into the system,
    and the output layer, in which the desired predictions are obtained (see Figure [11(b)](#A1.F11.sf2
    "In Figure 11 ‣ A.1\. Deep Learning Background: Concepts and Terminology ‣ Appendix
    A Appendix ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms")). In feed-forward networks, the outputs of one layer become the
    inputs of the next layer in the model, while in recurrent networks, the output
    of a neuron can be the input of neurons in the same or previous layers. The term
    “deep” in DNNs refers to the use of a large number of layers, which results in
    more accurate models that capture complex patterns and concepts.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（[Schmidhuber14,](#bib.bib237) ; [lecun2015,](#bib.bib158) ）是机器学习方法中的一个子集，它使用人工
    DNN 自动发现从大数据集中进行特征检测或分类所需的表示，通过采用多层处理来逐步提取更高层次的特征。DNN 模仿人脑的功能，其中神经元相互连接以接收信息、处理信息并将其传递给其他神经元。如图
    [11(a)](#A1.F11.sf1 "图 11 ‣ A.1\. 深度学习背景：概念与术语 ‣ 附录 A 附录 ‣ 深度学习硬件加速器的调查") 所示，类似于大脑的神经元，感知器（人工神经元）的简单模型从一组输入中接收信息，并在输入（X）的加权（W）总和上应用一个非线性函数
    F（激活函数）（[Rosenblatt1957,](#bib.bib234) ）。DNN 由多个人工神经元层（隐藏层）组成，这些层在输入层（将初始数据输入系统）和输出层（获得所需预测）之间组织（参见图
    [11(b)](#A1.F11.sf2 "图 11 ‣ A.1\. 深度学习背景：概念与术语 ‣ 附录 A 附录 ‣ 深度学习硬件加速器的调查")）。在前馈网络中，一个层的输出成为模型中下一个层的输入，而在递归网络中，一个神经元的输出可以是同一层或前面层中神经元的输入。“深度”一词指的是
    DNN 中使用的大量层，这导致更准确的模型，可以捕捉复杂的模式和概念。
- en: '![Refer to caption](img/4421338c1045af6ff67b67dd029821b6.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4421338c1045af6ff67b67dd029821b6.png)'
- en: (a)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/cf10d8a53a9bd2cf5a47a820eb6b68c2.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cf10d8a53a9bd2cf5a47a820eb6b68c2.png)'
- en: (b)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: Figure 11. Model of a perceptron (artificial neuron) (a) and of a multi-layer
    DNN (b).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11. 感知器（人工神经元）的模型（a）和多层 DNN 的模型（b）。
- en: 'There are two phases in a DNN’s operation: training, and inference. In the
    *training* phase, the neural network model is fed on a curated data set so that
    it can “learn” everything it needs to about the type of data it will analyze.
    In the case of *supervised* learning, a large set of examples and their corresponding
    labels indicating the correct classification are passed as input to the DNN. A
    forward pass is executed, and the error against the correct labels is measured.
    Then, the error is used in the DNN’s backward pass to update the weights. This
    loop is performed repeatedly until the DNN model achieves the desired accuracy.
    In *unsupervised* learning, the DNN uses unlabeled data to create an encoded self-organization
    of weights and activations that captures patterns as probability densities. With
    *semi-supervised* learning, during training a small amount of labeled data is
    combined with a large amount of unlabeled data. In the *inference* phase, the
    trained DNN model is used to make predictions on unseen data. When it comes to
    deployment, the trained model is often modified and simplified to meet real-world
    power and performance requirements. The two phases present different computational
    characteristics. On the one hand, the training phase of a model is computationally
    expensive but usually performed only once. On the other hand, the trained model
    is used for predictions on multiple input data, often under strict latency and/or
    energy constraints.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: DNN的操作分为两个阶段：训练和推理。在*训练*阶段，神经网络模型会接受一个精心准备的数据集，以便“学习”它将要分析的数据类型的一切。在*监督*学习的情况下，大量的示例及其对应的正确分类标签会作为输入传递给DNN。执行前向传递，并测量与正确标签的误差。然后，误差用于DNN的反向传递中更新权重。这个过程会重复进行，直到DNN模型达到期望的准确性。在*无监督*学习中，DNN使用未标记的数据创建一个编码的自组织权重和激活的结构，以捕捉作为概率密度的模式。在*半监督*学习中，训练期间将少量标记数据与大量未标记数据结合起来。在*推理*阶段，训练好的DNN模型用于对未见过的数据进行预测。对于部署，训练好的模型通常会被修改和简化，以满足实际的功耗和性能要求。这两个阶段具有不同的计算特性。一方面，模型的训练阶段计算开销大，但通常只执行一次。另一方面，训练好的模型会在多个输入数据上进行预测，通常要满足严格的延迟和/或能量约束。
- en: 'Three general types of DNN are mostly used today: Multi-Layer Perceptrons (MLPs),
    Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs). MLPs
    ([Rosenblatt1957,](#bib.bib234) ) are feed-forward ANNs composed of a series of
    fully connected layers, where each layer is a set of nonlinear functions of a
    weighted sum of all outputs of the previous one. On the contrary, in a CNN ([Lecun1998,](#bib.bib159)
    ), a convolutional layer extracts the simple features from the inputs by executing
    convolution operations. Each layer is a set of nonlinear functions of weighted
    sums of different subsets of outputs from the previous layer, with each subset
    sharing the same weights. Each convolutional layer in the model can capture a
    different high-level representation of input data, allowing the system to automatically
    extract the features of the inputs to complete a specific task, e.g., image classification,
    face authentication, and image semantic segmentation. Finally, RNNs ([Schmidhuber14,](#bib.bib237)
    ) address the time-series problem of sequential input data. Each RNN layer is
    a collection of nonlinear functions of weighted sums of the outputs of the previous
    layer and the previous state, calculated when processing the previous samples
    and stored in the RNN’s internal memory. RNN models are widely used in Natural
    Language Processing (NLP) for natural language modeling, word embedding, and machine
    translation.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 目前主要使用三种类型的深度神经网络（DNN）：多层感知机（MLPs）、卷积神经网络（CNNs）和递归神经网络（RNNs）。MLPs（[Rosenblatt1957,](#bib.bib234)
    ）是前馈人工神经网络（ANNs），由一系列全连接层组成，其中每一层都是前一层所有输出的加权和的非线性函数集合。相反，在CNN（[Lecun1998,](#bib.bib159)
    ）中，卷积层通过执行卷积操作从输入中提取简单特征。每一层都是前一层输出的不同子集的加权和的非线性函数集合，其中每个子集共享相同的权重。模型中的每个卷积层可以捕捉输入数据的不同高级表示，从而使系统能够自动提取输入特征以完成特定任务，例如图像分类、人脸认证和图像语义分割。最后，RNNs（[Schmidhuber14,](#bib.bib237)
    ）解决了序列输入数据的时间序列问题。每个RNN层是前一层输出和前一状态加权和的非线性函数的集合，这些函数在处理前一个样本时计算，并存储在RNN的内部记忆中。RNN模型广泛应用于自然语言处理（NLP），用于自然语言建模、词嵌入和机器翻译。
- en: On the one hand, DNNs such as AlexNet([Krizhevsky2012,](#bib.bib151) ) and the
    more recent GoogLeNet([Szegedy2015,](#bib.bib262) ) are composed of tens of layers,
    with millions of weights to be trained and used in every prediction, requiring
    tens to hundreds of megabytes (or even gigabytes) of memory for their storage.
    The calculation of the weighted sums requires a large number of data movements
    between the different levels of the memory hierarchy and the processing units,
    often posing a challenge to the available energy, memory bandwidth, and memory
    storage of the computing architecture. On the other hand, Tiny machine learning
    (TinyML) DNN models ([Warden2019,](#bib.bib291) ) have been investigated to run
    on small, battery-operated devices like microcontrollers, trading off prediction
    accuracy with respect to low-latency, low-power, and low-bandwidth model inference
    of sensor data on edge devices.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，像 AlexNet ([Krizhevsky2012,](#bib.bib151)) 和更近期的 GoogLeNet ([Szegedy2015,](#bib.bib262))
    等 DNNs 由数十层组成，每个预测中都需要训练和使用数百万个权重，这要求存储几十到几百兆字节（甚至是吉字节）的内存。加权和的计算需要在内存层次结构的不同级别和处理单元之间进行大量的数据移动，这往往对计算架构的可用能源、内存带宽和内存存储构成挑战。另一方面，Tiny
    机器学习 (TinyML) DNN 模型 ([Warden2019,](#bib.bib291)) 已被研究用于运行在小型电池驱动设备如微控制器上，以低延迟、低功耗和低带宽的模型推理来权衡预测准确性。
- en: 'Besides DNNs, the other major category of DL algorithms is that of Transformer-based
    models ([vaswaniAttentionAllYou2017,](#bib.bib276) ), which have recently captured
    great attention. Transformers were originally proposed for NLP ([vaswaniAttentionAllYou2017,](#bib.bib276)
    ), and are designed to recognize long-distance dependencies between data by means
    of *attention* layers. In attention layers, the weights used to linearly transform
    the input data are computed dynamically based on the input data itself. Transformer
    models are flexible (e.g., they can also be used for vision tasks ([dosovitskiyImageWorth16x162021,](#bib.bib66)
    )) and, most importantly, empirical scaling laws ([kaplanScalingLawsNeural2020,](#bib.bib139)
    ) govern their expressiveness: larger transformers trained for more time on larger
    datasets deliver better performance in consistent and predictable way capabilities.
    This makes it possible to construct and pre-train larger and larger models (e.g.,
    GPT-4 ([openaiGPT4TechnicalReport2023,](#bib.bib204) ), PaLM ([anilPaLMTechnicalReport2023,](#bib.bib13)
    ), LLaMA ([touvronLLaMAOpenEfficient2023,](#bib.bib267) )) with hundreds of billions
    of trillions of parameters, which can be then used as *Foundation Models* ([bommasaniOpportunitiesRisksFoundation2022,](#bib.bib22)
    ) to be fine-tuned for more focused applications.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 DNNs 之外，深度学习算法的另一个主要类别是基于 Transformer 的模型 ([vaswaniAttentionAllYou2017,](#bib.bib276))，这些模型最近引起了极大的关注。Transformers
    最初是为自然语言处理 (NLP) 提出的 ([vaswaniAttentionAllYou2017,](#bib.bib276))，设计目的是通过 *注意力*
    层来识别数据之间的长距离依赖。在注意力层中，用于线性转换输入数据的权重是基于输入数据本身动态计算的。Transformer 模型具有灵活性（例如，它们也可以用于视觉任务
    ([dosovitskiyImageWorth16x162021,](#bib.bib66))），最重要的是，经验性的扩展规律 ([kaplanScalingLawsNeural2020,](#bib.bib139))
    统治了它们的表现力：训练时间更长、数据集更大的大型 Transformers 提供了更好的一致性和可预测的性能。这使得可以构建和预训练越来越大的模型（例如
    GPT-4 ([openaiGPT4TechnicalReport2023,](#bib.bib204))、PaLM ([anilPaLMTechnicalReport2023,](#bib.bib13))、LLaMA
    ([touvronLLaMAOpenEfficient2023,](#bib.bib267)）），这些模型具有数百亿至万亿个参数，然后可以作为 *基础模型*
    ([bommasaniOpportunitiesRisksFoundation2022,](#bib.bib22)) 进行微调以适应更专注的应用。
- en: A.2\. Technology for GPU and TPU Architectures
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2\. GPU 和 TPU 架构的技术
- en: In this sub-section, we review the basic features of NVIDIA GPU architectures
    to boost the performance of HPC and Deep Learning applications. GPUs are specific-purpose
    processors introduced to compute efficiently graphics-related tasks, such as 3D
    rendering. They became widely used since the nineties as co-processors, working
    alongside CPUs, to offload graphics-related computations. The introduction of
    programmable shaders into GPU architectures, increased their flexibility paving
    the way for their adoption to perform general-purpose computations. Despite being
    specifically designed for computer graphics, their highly-parallel architecture
    is well suited to tackle a wide range of applications. Consequently, in the early
    2000s, GPUs started to be used to accelerate data-parallel computations not necessarily
    related to graphics, which could benefit from their architecture as well. This
    practice is commonly referred to as General-Purpose computing on GPUs (GP-GPU)
    started to be increasingly popular in the early 2010s with the advent of the CUDA
    language. The technological development of the last ten years significantly increased
    the computing power of GPU devices, which, due to their highly parallel nature,
    are incidentally very well suited to accelerate neural network training algorithms.
    The availability of such computing power allowed more complex neural network models
    to become practically usable, fostering the development of DNNs.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们回顾了NVIDIA GPU架构的基本特性，以提升HPC和深度学习应用的性能。GPU是专用处理器，用于高效计算图形相关任务，如3D渲染。自90年代起，GPU作为协处理器与CPU一起工作，以减轻图形相关计算的负担，变得广泛使用。可编程着色器的引入增加了GPU架构的灵活性，为其执行通用计算铺平了道路。尽管GPU专门设计用于计算机图形，其高度并行的架构非常适合处理各种应用。因此，在2000年代初，GPU开始被用来加速与图形不直接相关的数据并行计算，这也可以从其架构中获益。这种做法通常被称为GPU上的通用计算（GP-GPU），并在2010年代初随着CUDA语言的出现变得越来越流行。在过去十年的技术发展显著提高了GPU设备的计算能力，这些设备由于其高度并行的特性，意外地非常适合加速神经网络训练算法。这种计算能力的可用性使得更复杂的神经网络模型变得实用，促进了DNN的发展。
- en: The impressive results obtainable with DNNs in the context of AI, followed by
    significant investments in this market sector, induced hardware manufacturers
    to modify GPU architectures in order to be even more optimized to compute such
    workloads, as an example implementing the support for lower-precision computations.
    This led to a de-facto co-design of GPU architectures and neural network algorithms
    implementations, which is nowadays significantly boosting the performance, accuracy,
    and energy efficiency of AI applications. The hardware architecture of a GPU is
    based on a multicore design of processing elements called Streaming Multiprocessors
    (SM). Each SM, in turn, includes a number of compute units, called CUDA-cores
    in NVIDIA jargon, to execute at each clock-cycle multiple warps, i.e. groups of
    32 operations called CUDA-threads processed by the Single Instruction Multiple
    Thread (SIMT) fashion. SIMT execution enables different threads of a group to
    take different branches (with a performance penalty). By varying CPU threads,
    context switches among active CUDA threads are very fast. Typically one CUDA-thread
    processes one element of the target data set. This helps to exploit the available
    parallelism of the algorithm and to hide the latency by swapping among threads
    waiting for data coming from memory and threads ready to run. This structure remained
    stable across generations, with several enhancements implemented in the most recent
    architectures making available more registers addressable to each CUDA thread.
    Considering each generation of NVIDIA architecture, some minor differences occurred.
    The C2050 and C2070 boards based on the Fermi processor architecture differ in
    the amount of available global memory. Both cards have a peak performance of $\approx
    1$ Tflops in single-precision (SP), and $\approx 500$ Gflops in double-precision
    (DP), and the peak memory bandwidth is $144$ GB/s.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能领域，使用深度神经网络（DNN）可以取得显著的结果，加上对该市场领域的重大投资，促使硬件制造商修改 GPU 架构，以便更好地计算这些工作负载，例如实现对低精度计算的支持。这导致了
    GPU 架构与神经网络算法实现的实际协同设计，如今显著提升了 AI 应用的性能、准确性和能效。GPU 的硬件架构基于一个多核设计的处理单元，称为流处理多处理器（SM）。每个
    SM 包含多个计算单元，即 NVIDIA 行话中的 CUDA 核心，用于在每个时钟周期执行多个 warps，即 32 个操作组，称为 CUDA 线程，由单指令多线程（SIMT）方式处理。SIMT
    执行使得一个组的不同线程可以选择不同的分支（带来性能惩罚）。通过调整 CPU 线程，活跃的 CUDA 线程之间的上下文切换非常快速。通常，一个 CUDA 线程处理一个目标数据集的元素。这有助于利用算法的并行性，并通过在等待来自内存的数据和准备运行的线程之间交换来隐藏延迟。这种结构在各代之间保持稳定，最近的架构中实现了若干增强，使每个
    CUDA 线程可寻址的寄存器更多。考虑到 NVIDIA 架构的每一代，存在一些小的差异。基于 Fermi 处理器架构的 C2050 和 C2070 显卡在可用的全局内存量上有所不同。这两款显卡在单精度（SP）下的峰值性能为
    $\approx 1$ Tflops，在双精度（DP）下为 $\approx 500$ Gflops，峰值内存带宽为 $144$ GB/s。
- en: The K20, K40, and K80 are boards based on the Kepler architecture. The K40 processor
    has more global memory than the K20 and slightly improves memory bandwidth and
    floating-point throughput, while the K80 has two enhanced Kepler GPUs with more
    registers and shared memory than K20/K40 and extended GPUBoost features. On the
    Kepler K20 and K40, the peak SP (DP) performance is $\approx 5$ Tflops ($\approx
    1.5$ Tflops), while on the K80 the aggregate performance of the two GPUs delivers
    a peak SP (DP) of $\approx 5.6$ Tflops ($\approx 1.9$ Tflops). The peak memory
    bandwidth is $250$ and $288$ GB/s respectively for the K20X and the K40 while
    on the K80 the aggregate peak is $480$ GB/s.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: K20、K40 和 K80 是基于 Kepler 架构的显卡。K40 处理器的全局内存比 K20 更多，并且略微提高了内存带宽和浮点运算吞吐量，而 K80
    具有两个增强的 Kepler GPU，相较于 K20/K40 具有更多的寄存器和共享内存，并扩展了 GPUBoost 功能。在 Kepler K20 和 K40
    上，峰值 SP (DP) 性能为 $\approx 5$ Tflops ($\approx 1.5$ Tflops)，而在 K80 上，两个 GPU 的总和提供了峰值
    SP (DP) 性能为 $\approx 5.6$ Tflops ($\approx 1.9$ Tflops)。K20X 和 K40 的峰值内存带宽分别为
    $250$ 和 $288$ GB/s，而 K80 的总峰值为 $480$ GB/s。
- en: The P100 board is based on the Pascal architecture, engineered to tackle memory
    challenges using stacked memory, a technology that enables multiple layers of
    DRAM components to be integrated vertically on the package along with the GPU.
    The P100 is the first GPU accelerator to use High Bandwidth Memory 2 (HBM2) to
    provide greater bandwidth, more than twice the capacity, and higher energy efficiency,
    compared to off-package GDDR5 used in previous generations. The SXM-2 version
    of P100 board also integrates the NVLinks, NVIDIA’s new high-speed interconnect
    technology for GPU-accelerated computing significantly increasing performance
    for both GPU-to-GPU communications and for GPU access to system memory. The P100
    delivers a peak performance of $\approx 10.5$ Tflops SP and $\approx 5.3$ in DP,
    while the peak memory bandwidth has been increased to $732$ GB/s.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: P100 板卡基于 Pascal 架构，旨在通过堆叠内存解决内存挑战，这是一种将多个 DRAM 层垂直集成到封装内与 GPU 一起使用的技术。P100
    是首款使用高带宽内存 2（HBM2）的 GPU 加速器，提供比前几代使用的离封装 GDDR5 更高的带宽、更大的容量和更高的能效。P100 板卡的 SXM-2
    版本还集成了 NVLinks，NVIDIA 新的高速互连技术，大幅提升了 GPU 与 GPU 之间以及 GPU 访问系统内存的性能。P100 提供约 10.5
    Tflops SP 和约 5.3 Tflops DP 的峰值性能，同时峰值内存带宽提升至 732 GB/s。
- en: The Volta architecture has been developed and engineered for the convergence
    of HPC and AI. Key compute features of Tesla V100 include a new SM Architecture
    Optimized for Deep Learning, integrating Tensor Cores designed specifically for
    deep learning. Also, the Tesla V100 board integrates a second-generation NVLink
    supporting up to 6 links at 25 GB/s for a total of 300 GB/s, and1 6GB of HBM2
    memory subsystem delivering 900 GB/sec peak memory bandwidth provides 1.5x delivered
    memory bandwidth versus Pascal GP100\. Volta increases the computing throughput
    to 7.5 Tflops DP, and the memory bandwidth to 900 GB/s, respectively a factor
    1.4X and 1.2X w.r.t. the Pascal architecture.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: Volta 架构旨在推动 HPC 和 AI 的融合。Tesla V100 的关键计算特性包括优化深度学习的新 SM 架构，集成了专门为深度学习设计的 Tensor
    Cores。同时，Tesla V100 板卡集成了第二代 NVLink，支持最多 6 条链接，每条 25 GB/s，总带宽达到 300 GB/s，及 6GB
    的 HBM2 内存子系统，提供 900 GB/sec 的峰值内存带宽，相比 Pascal GP100 提升了 1.5 倍。Volta 将计算吞吐量提高至 7.5
    Tflops DP，内存带宽提升至 900 GB/s，相较于 Pascal 架构分别增加了 1.4 倍和 1.2 倍。
- en: The Ampere architecture adds a powerful new generation of Tensor Core that boosts
    throughput over V100 for Deep Learning applications running 10x faster. The peak
    performance in DP has been increased to 9.7 TFlops, and to 19.5 TFlops using Tensor
    Core or single precision FP32 operations. The A100 40GB of high-speed HBM2 memory
    with a peak bandwidth of 1555 GB/sec, corresponding to a 73% increase compared
    to the Tesla V100\. It also supports a third-generation of NVIDIA NVLink with
    a data rate of 50 Gbit/sec per signal pair, nearly doubling the 25.78 Gbits/sec
    rate in V100.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: Ampere 架构引入了一代强大的 Tensor Core，这使得深度学习应用的吞吐量比 V100 提升了 10 倍。DP 的峰值性能提升至 9.7 TFlops，使用
    Tensor Core 或单精度 FP32 操作时为 19.5 TFlops。A100 配备了 40GB 高速 HBM2 内存，峰值带宽为 1555 GB/sec，相较于
    Tesla V100 提升了 73%。它还支持第三代 NVIDIA NVLink，每对信号的传输速率为 50 Gbit/sec，几乎是 V100 中 25.78
    Gbits/sec 速率的两倍。
- en: The Hopper is the latest architecture developed by NVidia providing a new generation
    of streaming multiprocessors with several new features. Tensor Cores are up to
    6x faster chip-to-chip compared to A100, the memory subsystem is based on HBM3
    modules providing nearly a 2x bandwidth increase over the previous generation,
    and integrate a fourth-generation of NVlinks providing a 3x bandwidth increase.
    The peak performance is boosted up to 24 TFlops in DP, and 48 TFlops using FP64
    tensor core and FP32 operations. The H100 SXM5 GPU raises the bar considerably
    by supporting 80 GB (five stacks) of fast HBM3 memory, delivering over 3 TB/sec
    of memory bandwidth, effectively a 2x increase over the memory bandwidth of the
    A100 that was launched just two years ago. The PCIe H100 provides 80 GB of fast
    HBM2e with over 2 TB/sec of memory bandwidth. The H100 also introduces DPX instructions
    to accelerate the performance of Dynamic Programming algorithms. These new instructions
    provide support for advanced fused operands for the inner loop of many dynamic
    programming algorithms. This leads to dramatically faster times-to-solution in
    disease diagnosis, logistics routing optimizations, and even graph analytics.
    For a more complete description, we can refer to ([fermi,](#bib.bib197) ; [kepler,](#bib.bib198)
    ; [pascal,](#bib.bib106) ; [volta,](#bib.bib69) ; [Ampere,](#bib.bib150) ; [h100,](#bib.bib12)
    ), while the work in [Table 11](#A1.T11 "Table 11 ‣ A.2\. Technology for GPU and
    TPU Architectures ‣ Appendix A Appendix ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms") summarizes just a few relevant parameters of
    NVIDIA GPU architectures.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: Hopper 是由 NVidia 开发的最新架构，提供了新一代流处理器，具有多个新特性。Tensor 核心在芯片间的速度比 A100 快最多 6 倍，内存子系统基于
    HBM3 模块，带宽比上一代提高了近 2 倍，并且集成了第四代 NVlinks，带宽提升了 3 倍。峰值性能在 DP 下提高到 24 TFlops，而使用
    FP64 tensor 核心和 FP32 操作时可达 48 TFlops。H100 SXM5 GPU 显著提高了标准，支持 80 GB（五个堆栈）的快速 HBM3
    内存，提供超过 3 TB/sec 的内存带宽，相较于仅两年前发布的 A100 的内存带宽有效提高了 2 倍。PCIe H100 提供了 80 GB 的快速
    HBM2e 和超过 2 TB/sec 的内存带宽。H100 还引入了 DPX 指令，以加速动态编程算法的性能。这些新指令支持高级融合操作数，用于许多动态编程算法的内部循环。这在疾病诊断、物流路线优化甚至图形分析中大幅度加快了解决方案的时间。有关更完整的描述，可以参考
    ([fermi,](#bib.bib197) ; [kepler,](#bib.bib198) ; [pascal,](#bib.bib106) ; [volta,](#bib.bib69)
    ; [Ampere,](#bib.bib150) ; [h100,](#bib.bib12) )，而 [Table 11](#A1.T11 "Table 11
    ‣ A.2\. Technology for GPU and TPU Architectures ‣ Appendix A Appendix ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms") 中的工作总结了
    NVIDIA GPU 架构的一些相关参数。
- en: Table 11. Summary of hardware features of NVIDIA GPU architectures.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11. NVIDIA GPU 架构硬件特性总结。
- en: '| Architecture | Fermi | Kepler | Kepler | Kepler |  | Pascal | Volta | Ampere
    | Hopper |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | Fermi | Kepler | Kepler | Kepler |  | Pascal | Volta | Ampere | Hopper
    |'
- en: '| GPU | GF100 | GK110 | GK110B | GK210 | $\times$ 2 | P100 | V100 | A100 |
    H100 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| GPU | GF100 | GK110 | GK110B | GK210 | $\times$ 2 | P100 | V100 | A100 |
    H100 |'
- en: '| Year | 2011 | 2012 | 2013 | 2014 |  | 2016 | 2017 | 2021 | 2022 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 2011 | 2012 | 2013 | 2014 |  | 2016 | 2017 | 2021 | 2022 |'
- en: '| #SMs | 16 | 14 | 15 | 13 | $\times$ 2 | 56 | 80 | 108 | 132 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| #SMs | 16 | 14 | 15 | 13 | $\times$ 2 | 56 | 80 | 108 | 132 |'
- en: '| #CUDA-cores | 448 | 2688 | 2880 | 2496 | $\times$ 2 | 3584 | 5120 | 6912
    | 16896 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| #CUDA 核心 | 448 | 2688 | 2880 | 2496 | $\times$ 2 | 3584 | 5120 | 6912 | 16896
    |'
- en: '| Base clock (MHz) | 1.15 | 735 | 745 | 562 |  | 1328 | 1370 | 1700 | 1600
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 基础时钟 (MHz) | 1.15 | 735 | 745 | 562 |  | 1328 | 1370 | 1700 | 1600 |'
- en: '| Base DP (Gflops) | 515 | 1310 | 1430 | 935 | $\times$ 2 | 4755 | 7000 | 9700
    | 30000 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 基础 DP (Gflops) | 515 | 1310 | 1430 | 935 | $\times$ 2 | 4755 | 7000 | 9700
    | 30000 |'
- en: '| Total available memory (GB) | 3 | 6 | 12 | 12 | $\times$ 2 | 16 | 16 | 40
    | 80 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 总可用内存 (GB) | 3 | 6 | 12 | 12 | $\times$ 2 | 16 | 16 | 40 | 80 |'
- en: '| Memory bus width (bit) | 384 | 384 | 384 | 384 | $\times$ 2 | 4096 | 4096
    | 5120 | 5120 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 内存总线宽度 (位) | 384 | 384 | 384 | 384 | $\times$ 2 | 4096 | 4096 | 5120 | 5120
    |'
- en: '| Peak mem. BW (GB/s) | 144 | 250 | 288 | 240 | $\times$ 2 | 732 | 900 | 1555
    | 3072 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 峰值内存带宽 (GB/s) | 144 | 250 | 288 | 240 | $\times$ 2 | 732 | 900 | 1555 | 3072
    |'
- en: The number of GPU modules available within the DGX ([dgx,](#bib.bib200) ) line
    of server and workstation platforms varies from 4 to 16 Tesla daughter cards integrated
    into the system using a version of the high-bandwidth SMX([smx,](#bib.bib224)
    ) socket solution. The DGX-1 server, the first of DGX line, was announced in 2016,
    and it was first based on 8 Pascal cards, after upgraded to Volta, interconneced
    by an NVLink mesh network. The Pascal-based DGX-1 delivered 170 TFlops using FP16
    half-precision processing, while the Volta-based upgrade increased this to 960
    TFlops using FP16 tensor computing. The DGX-2, the successor of DGX-1, was announced
    in 2018; it is based on 16 V100 32 GB GPU cards in a single unit interconnected
    by a NVSwitch ([nvlink,](#bib.bib201) ) for high-bandwidth GPU-to-GPU communications,
    and delivers nearly 2 PFlops using FP16 tensor processing, ans assemble a total
    of 512 GB of HBM2 memory. The DGX Station is a workstation designed as a deskside
    AI system that can operate completely independent without the typical infrastructure
    of a datacenter. The DGX Station is a tower chassis, and the first available was
    including four Testa V100 accelerators each with 16 GB of HBM2 memory, delivering
    an aggregate computing performance of nearly 500 TFops using FP16 tensor computing.
    The Ampere version of the DGX Station includes four A100 accelerators configured
    with either 40 or 80 GB of memory each, resulting either in 160 GB or 320 GB variants,
    and a peak FP16-tensor computing performance of approximately 1 PFlops. The DGX
    A100 server is the 3rd generation of DGX servers announced in 2002\. It includes
    8 A100 accelerators, and it is the first DGX server replacing the Intel Xeon CPUs
    with the AMD EPYC CPUs, delivering a peak FP16-tensor computing performance of
    approximately 2.5 PFlops. The DGX H100 Server has been announced in 2022, and
    it is the 4th generation of DGX servers. It includes 8 Hopper H100 cards delivering
    a total of 16 PFlops of FP16-tensor AI computing, and assembling a total of 640
    GB of HBM3 memory. The DGX SuperPod is a high-performance turnkey supercomputer
    solution based on DGX hardware, combining high-performance DGX compute nodes with
    fast storage and high bandwidth networking, that can be used as building-block
    to assemble large supercomputer systems. The Selene Supercomputer, installed at
    the Argonne National Laboratory, is one example of a DGX SuperPod-based system,
    built from 280 DGX A100 nodes. The new version of SuperPod based on H100 DGX can
    scale up to 32 nodes, for a total of 256 H100 GPUs and 64 x86 CPUs. This gives
    the complete SuperPod a total 20TB of HBM3 memory, 70.4 TB/s of bisection bandwidth,
    and up to 1 EFlop of FP8 and 500 PFlops of FP16 tensor AI computing. The Eos([eos,](#bib.bib199)
    ) supercomputer announced in March 2022, designed, built, and operated by Nvidia,
    is based on 18 H100 SuperPods, for a total of 576 DGX H100 systems. This allows
    Eos to deliver approximately 18 EFlops of FP8 and 9 EFLOPs of FP16 computing,
    making Eos the fastest AI supercomputer in the world. Table [Table 12](#A1.T12
    "Table 12 ‣ A.2\. Technology for GPU and TPU Architectures ‣ Appendix A Appendix
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
    summarizes the computing performance of a few DGX systems. We report the peak
    computing performance using tensor FP16 operations relevant for AI applications,
    and the standard FP32 and FP64 relevant for many scientific applications.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在DGX（[dgx,](#bib.bib200)）系列的服务器和工作站平台中，可用的GPU模块数量从4到16个特斯拉子卡不等，这些模块通过高带宽的SMX（[smx,](#bib.bib224)）插槽解决方案集成到系统中。DGX-1服务器是DGX系列的首款产品，于2016年发布，最初基于8张Pascal卡，后来升级到Volta，通过NVLink网状网络互联。基于Pascal的DGX-1在使用FP16半精度处理时提供了170
    TFlops的计算性能，而升级到Volta后，这一性能提升至960 TFlops，使用FP16张量计算。DGX-2是DGX-1的继任者，于2018年发布；它基于一个单元内的16张V100
    32 GB GPU卡，通过NVSwitch（[nvlink,](#bib.bib201)）实现高带宽GPU间通信，并提供近2 PFlops的FP16张量处理性能，并配备总计512
    GB的HBM2内存。DGX Station是一种设计为桌边AI系统的工作站，可以在没有数据中心典型基础设施的情况下完全独立运行。DGX Station是一个塔式机箱，首款产品包括四张每张16
    GB HBM2内存的Testa V100加速卡，提供近500 TFops的FP16张量计算性能。Ampere版本的DGX Station包括四张A100加速卡，每张配置40
    GB或80 GB内存，分别提供160 GB或320 GB的变体，并具有约1 PFlops的FP16张量计算性能。DGX A100服务器是2002年发布的第三代DGX服务器。它包括8张A100加速卡，并且是第一款将Intel
    Xeon CPUs替换为AMD EPYC CPUs的DGX服务器，提供约2.5 PFlops的FP16张量计算性能。DGX H100服务器于2022年发布，是第四代DGX服务器。它包括8张Hopper
    H100卡，提供总计16 PFlops的FP16张量AI计算，并配备总计640 GB的HBM3内存。DGX SuperPod是一种基于DGX硬件的高性能交钥匙超级计算机解决方案，将高性能DGX计算节点与快速存储和高带宽网络结合在一起，可以作为组装大型超级计算机系统的构建块。位于阿贡国家实验室的Selene超级计算机是基于DGX
    SuperPod的系统的一个例子，由280个DGX A100节点构建。基于H100 DGX的新版本SuperPod可以扩展到32个节点，总计256个H100
    GPU和64个x86 CPU。这使得整个SuperPod拥有总共20TB的HBM3内存，70.4 TB/s的分段带宽，以及高达1 EFlop的FP8和500
    PFlops的FP16张量AI计算性能。Eos（[eos,](#bib.bib199)）超级计算机于2022年3月宣布，由Nvidia设计、构建和运营，基于18个H100
    SuperPods，总计576个DGX H100系统。这使得Eos能够提供约18 EFlops的FP8和9 EFLOPs的FP16计算，使其成为世界上最快的AI超级计算机。表[Table
    12](#A1.T12 "Table 12 ‣ A.2. Technology for GPU and TPU Architectures ‣ Appendix
    A Appendix ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms")总结了一些DGX系统的计算性能。我们报告了针对AI应用的张量FP16操作的峰值计算性能，以及许多科学应用相关的标准FP32和FP64性能。
- en: Table 12. Performance in TFlops of DGX based platforms; for H100 platforms,
    sparsity features are used.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12. DGX 平台的 TFlops 性能；对于 H100 平台，使用了稀疏性特征。
- en: '| Platform | #GPUs | FP16 Tensor | F32 | FP64 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 平台 | #GPUs | FP16 张量 | F32 | FP64 |'
- en: '| DGX1-P100 | 8x P100 | – | 85 | 42 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| DGX1-P100 | 8x P100 | – | 85 | 42 |'
- en: '| DGX1-V100 | 8x V100 | 1000 | 124 | 62 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| DGX1-V100 | 8x V100 | 1000 | 124 | 62 |'
- en: '| DGX2 | 16x V100 | 2000 | 248 | 124 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| DGX2 | 16x V100 | 2000 | 248 | 124 |'
- en: '| DGX-A100 Server | 8x A100 | 2496 | 154 | 77 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| DGX-A100 服务器 | 8x A100 | 2496 | 154 | 77 |'
- en: '| DGX-H100 Server | 8x H100 | 16000 | 544 | 272 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| DGX-H100 服务器 | 8x H100 | 16000 | 544 | 272 |'
- en: '| Supercomputer |  |  |  |  |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 超级计算机 |  |  |  |  |'
- en: '| Selene | 2240x A100 | 698880 | 43120 | 21560 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| Selene | 2240x A100 | 698880 | 43120 | 21560 |'
- en: '| Eos | 4608x H100 | 9216000 | 313344 | 156672 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| Eos | 4608x H100 | 9216000 | 313344 | 156672 |'
- en: 'The TPU presented in TPU ([Jouppi_2017,](#bib.bib132) ; [jouppiMotivationEvaluationFirst2018,](#bib.bib134)
    ) is centered on a large (256$\times$256) systolic array operating on signed or
    unsigned 8-bit integers and targeting exclusively data center inference applications;
    this is coupled with a large amount of on-chip SRAM for activations (24 MiB) and
    a high-bandwidth (30 GiB/s) dedicated path to off-chip L3 DRAM for weights. The
    next design iterations (TPUv2, TPUv3) ([jouppiDomainspecificSupercomputerTraining2020,](#bib.bib135)
    ) forced to move from an inference-oriented design to a more general engine tuned
    for both inference and training, employing the 16-bit BF16 floating-point format,
    more cores (2 per chip) using each one or two 4$\times$ smaller arrays than TPUv1
    (128$\times$128, to reduce under-usage inefficiencies). TPUv2/v3 also introduced
    high-bandwidth memory support, which results in more than 20$\times$ increase
    in the available off-chip memory bandwidth. Conversely, Goya ([medina2019hotchips,](#bib.bib183)
    ) relies on PCIe 4.0 to interface to a host processor and exploits a design that
    uses a heterogenous approach comprising of a large General Matrix Multiply (GMM)
    engine, TPUs, and a large shared DDR4 memory pool. Each TPU also incorporates
    its own local memory that can be either hardware-managed or fully software-managed,
    allowing the compiler to optimize the residency of data and reduce movement. Each
    of the individual TPUs is a VLIW design that has been optimized for AI applications.
    The TPU supports mixed-precision operations including 8-bit, 16-bit, and 32-bit
    SIMD vector operations for both integer and floating-point. Gaudi has an enhanced
    version of the TPUs and uses HBM global memories rather than the DDR used in Goya,
    increasing the support towards bfloat16 data types and by including more operations
    and functionalities dedicated to training operations. As a counterpart of smaller
    TPUs, NVIDIA TensorCores are small units, designed to perform a 4$\times$4$\times$4
    FP16 GEMM operation per cycle in Volta (doubled in Ampere and quadrupled in Hopper,
    adding also support for other data types). Performance is then obtained by parallelization:
    each Streaming Multiprocessor includes eight TensorCores controlled by 32 threads;
    and, depending on the specific chip, GPUs can contain tens of Streaming Multiprocessors.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: TPU（见 TPU ([Jouppi_2017,](#bib.bib132) ; [jouppiMotivationEvaluationFirst2018,](#bib.bib134)
    )）的设计集中在一个大型（256$\times$256）系统阵列上，该阵列操作有符号或无符号 8 位整数，专门针对数据中心推理应用；并且配备了大量的片上 SRAM
    用于激活（24 MiB），以及一个高带宽（30 GiB/s）的专用通道连接到片外的 L3 DRAM 用于权重。后续设计迭代（TPUv2, TPUv3）([jouppiDomainspecificSupercomputerTraining2020,](#bib.bib135)
    ) 强调从推理导向设计转向一个更通用的引擎，针对推理和训练进行优化，采用 16 位 BF16 浮点格式，增加了更多核心（每芯片 2 个），每个核心或两个 4$\times$
    更小的阵列（128$\times$128），以减少低使用效率。TPUv2/v3 还引入了高带宽内存支持，使片外内存带宽增加了 20$\times$ 以上。相比之下，Goya
    ([medina2019hotchips,](#bib.bib183) ) 依靠 PCIe 4.0 接口连接主处理器，并利用了异构设计，包括一个大型通用矩阵乘法（GMM）引擎、TPUs
    和一个大型共享 DDR4 内存池。每个 TPU 还包括自己的本地内存，这些内存可以是硬件管理的或完全软件管理的，允许编译器优化数据的驻留并减少移动。每个 TPU
    是一个 VLIW 设计，针对 AI 应用进行了优化。TPU 支持混合精度操作，包括 8 位、16 位和 32 位 SIMD 向量操作，用于整数和浮点。Gaudi
    是 TPU 的增强版本，使用 HBM 全局内存而不是 Goya 使用的 DDR，增加了对 bfloat16 数据类型的支持，并包括更多专门用于训练操作的功能。作为较小
    TPU 的对照，NVIDIA TensorCores 是小型单元，设计用于每个周期在 Volta 中执行 4$\times$4$\times$4 FP16
    GEMM 操作（在 Ampere 中加倍，在 Hopper 中四倍增加，同时支持其他数据类型）。性能通过并行化获得：每个流处理器包括八个 TensorCores，由
    32 个线程控制；并且，根据具体芯片的不同，GPU 可以包含数十个流处理器。
- en: A.3\. FPGA Technology
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3\. FPGA 技术
- en: 'FPGAs are semiconductor devices that provide a unique combination of flexibility
    and performance thanks to their fundamental building blocks, known as Configurable
    Logic Blocks (CLBs) or simply Logic Elements (LEs). They consist of look-up tables
    (LUTs) and flip-flops that can be used to implement arbitrary combinational and
    sequential bit-level operations, on the basis of user-defined tasks. Programmable
    interconnects provide the necessary routing resources to establish connections
    between different elements within the device and to facilitate the seamless flow
    of data and control signals. FPGAs provide various types of memory resources that
    can be utilized for different purposes:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: FPGA是半导体器件，凭借其基础构建模块，即可配置逻辑块（CLBs）或简单逻辑元素（LEs），提供了独特的灵活性和性能组合。它们由查找表（LUTs）和触发器组成，可以用于实施任意组合和顺序位级操作，基于用户定义的任务。可编程互连提供了必要的路由资源，以建立设备内不同元素之间的连接，并促进数据和控制信号的无缝流动。FPGA提供了各种类型的内存资源，可用于不同的目的：
- en: •
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Block RAM (BRAM): specialized on-chip memory resource that offers dedicated
    storage for data or program code. BRAMs are characterized by their dual-port or
    true dual-port design, providing high bandwidth and low latency. Organized into
    fixed-sized blocks, BRAMs serve a variety of purposes including data buffering,
    cache memory, FIFO implementation, and storage for coefficients or tables utilized
    in digital signal processing applications. BRAMs play a crucial role in optimizing
    data access and manipulation within FPGA designs.'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 块RAM（BRAM）：专用的片上内存资源，提供数据或程序代码的专用存储。BRAM的特点是双端口或真正的双端口设计，提供高带宽和低延迟。组织为固定大小的块，BRAM用于数据缓冲、缓存内存、FIFO实现以及数字信号处理应用中使用的系数或表的存储。BRAM在优化FPGA设计中的数据访问和操作中起着至关重要的作用。
- en: •
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Distributed RAM: memory elements that are distributed across the logic fabric
    of an FPGA. Unlike BRAM, which is dedicated memory, distributed RAM is implemented
    using the LUTs present in the CLBs. These LUT-based memories offer smaller storage
    capacity compared to BRAM but are more flexible and can be used for smaller data
    sets or temporary storage within the FPGA.'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分布式RAM：分布在FPGA逻辑结构中的内存元素。与专用内存BRAM不同，分布式RAM使用CLBs中的LUTs实现。这些基于LUT的内存相比BRAM存储容量较小，但更灵活，可用于较小的数据集或FPGA中的临时存储。
- en: •
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Configuration Memory: a specialized form of memory dedicated to storing the
    configuration data necessary to define the desired behavior of the FPGA. This
    memory contains the bitstream or configuration file, which is loaded into the
    FPGA upon startup. Configuration memory can be implemented using diverse technologies,
    such as SRAM-based or flash-based configurations, offering flexibility in how
    the FPGA is programmed and initialized.'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配置内存：一种专门用于存储定义FPGA期望行为所需配置数据的内存。此内存包含比特流或配置文件，在启动时加载到FPGA中。配置内存可以通过多种技术实现，例如基于SRAM或基于闪存的配置，提供了灵活的FPGA编程和初始化方式。
- en: 'Moreover, in order to meet the demands of emerging technologies and applications,
    FPGAs provide the designers with specialized macros, such as Digital Signal Processors
    (DSPs) and embedded multipliers, that can be exploited to enhance processing capabilities,
    improve power efficiency, and increase the flexibility of hardware accelerators
    for DL. The latter exploit FPGAs mostly to accelerate inference, while training
    is delegated to GPUs: this reflects the differences between the two phases, as
    training is only executed once and requires high throughput, while for inference,
    especially on edge devices, latency, and power consumption become critical ([guo2019dl,](#bib.bib98)
    ; [blaiech2019survey,](#bib.bib19) ). FPGAs are also often used as a prototyping
    platform to explore different architectures before committing to ASIC manufacturing
    ([Spagnolo2022_3,](#bib.bib251) ).'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了满足新兴技术和应用的需求，FPGA为设计人员提供了专用宏，例如数字信号处理器（DSPs）和嵌入式乘法器，这些可以用来增强处理能力，提高功率效率，并增加硬件加速器的灵活性以适应DL。后者主要利用FPGA加速推理，而训练则交给GPU：这反映了两者阶段之间的差异，因为训练仅执行一次且需要高吞吐量，而推理，尤其是在边缘设备上，延迟和功耗变得至关重要（[guo2019dl,](#bib.bib98)；[blaiech2019survey,](#bib.bib19)）。FPGA还常用于作为原型平台，以探索不同架构后再决定是否生产ASIC（[Spagnolo2022_3,](#bib.bib251)）。
- en: A.4\. EDA Frameworks
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4. EDA框架
- en: Implementing hardware accelerators for ML algorithms, particularly DNNs, is
    a complex task that is rarely addressed through manual coding in low-level Hardware
    Description Languages (HDL). When Register Transfer Level (RTL) design is required
    to achieve high performance, templated components may be used ([Suda2016,](#bib.bib175)
    ). Instead, there are several electronic design automation (EDA) tools that bridge
    the gap between ML models and FPGAs/ASICs, allowing researchers to focus on developing
    the algorithms at a high level of abstraction ([venieris2018toolflows,](#bib.bib278)
    ).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 为机器学习算法，特别是深度神经网络（DNNs）实现硬件加速器是一项复杂的任务，这通常不会通过在低级硬件描述语言（HDL）中手动编码来解决。当需要寄存器传输级（RTL）设计以实现高性能时，可以使用模板化组件（[Suda2016,](#bib.bib175)）。相反，有几个电子设计自动化（EDA）工具可以弥合机器学习模型与FPGA/ASIC之间的差距，使研究人员能够专注于在更高抽象层次上开发算法（[venieris2018toolflows,](#bib.bib278)）。
- en: Vitis AI, Xilinx’s development environment for AI inference ([VitisAI,](#bib.bib10)
    ), supports models developed in major frameworks such as PyTorch ([torch,](#bib.bib214)
    ), TensorFlow ([TF,](#bib.bib1) ) and Caffe ([Caffe,](#bib.bib125) ), and maps
    them on deep learning processor unit (DPU) cores present on modern Xilinx boards
    alongside the standard FPGA logic. The work in ([Ajili2022,](#bib.bib268) ) describes
    the implementation of DeepSense, a framework that includes CNN and RNN, with a
    focus on the choice of parameters to define DPUs used by Vitis AI; ([Vandendriessche2022,](#bib.bib274)
    ) performs a parametric study of the DPU architecture used by Vitis AI and examines
    the tradeoffs between the resources used and the clock frequency, as well as their
    impact on power consumption; ([Wang2021,](#bib.bib286) ) compares the FPGA implementation
    of YOLOv3 provided by Vitis AI with its GPU counterpart, showing higher throughput
    and lower power consumption; ([Ushiroyama2022,](#bib.bib272) ) evaluates the implementation
    of three different CNNs in terms of precision, power consumption, throughput,
    and design man-hours, and compares these figures with their GPU counterparts.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: Vitis AI，赛灵思用于AI推理的开发环境（[VitisAI,](#bib.bib10)），支持在主要框架中开发的模型，如PyTorch（[torch,](#bib.bib214)）、TensorFlow（[TF,](#bib.bib1)）和Caffe（[Caffe,](#bib.bib125)），并将它们映射到现代赛灵思板上的深度学习处理单元（DPU）核心以及标准FPGA逻辑中。
    ([Ajili2022,](#bib.bib268) )的工作描述了DeepSense框架的实现，该框架包括CNN和RNN，并关注定义Vitis AI使用的DPU的参数选择；([Vandendriessche2022,](#bib.bib274)
    )对Vitis AI使用的DPU架构进行了参数研究，考察了所用资源与时钟频率之间的权衡及其对功耗的影响；([Wang2021,](#bib.bib286)
    )将Vitis AI提供的YOLOv3的FPGA实现与其GPU版本进行了比较，显示了更高的吞吐量和更低的功耗；([Ushiroyama2022,](#bib.bib272)
    )评估了三种不同CNN的实现，考虑了精度、功耗、吞吐量和设计工时，并将这些数据与其GPU版本进行了比较。
- en: High-Level Synthesis (HLS) plays a crucial role to automate the design of ML
    accelerators. HLS tools such as Vitis HLS ([vitishls,](#bib.bib294) ), Bambu ([Bambu,](#bib.bib74)
    ), Intel HLS Compiler ([IntelHLS2022,](#bib.bib120) ), Catapult ([CatapultHLS2022,](#bib.bib245)
    ), Stratus HLS ([StratusHLS2022,](#bib.bib27) ), or LegUp ([LegUpHLS2013,](#bib.bib28)
    ) provide users with a high level of abstraction where they can describe the desired
    functionality with a software programming language (C/C++/SystemC) and automatically
    obtain a corresponding high-performance HDL implementation. HLS thus boosts the
    productivity of hardware designers, who can benefit from faster design changes
    and functional verification. In fact, HLS allows generating accelerators for different
    platforms (e.g., larger or smaller FPGAs) without altering the C/C++ source code
    apart from a few design directives. This makes it possible to explore the design
    space and find the best implementation much faster than with HDL design. Note
    that code must be written with hardware knowledge in mind in order to meet given
    performance and resource usage results. Arbitrary software code, written for a
    CPU target, could achieve very low performance since it typically does not expose
    enough parallelism to exploit the spatial concurrency available on FPGA or ASIC.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 高层次综合（HLS）在自动化设计ML加速器中扮演着至关重要的角色。HLS工具如Vitis HLS（[vitishls,](#bib.bib294)）、Bambu（[Bambu,](#bib.bib74)）、Intel
    HLS编译器（[IntelHLS2022,](#bib.bib120)）、Catapult（[CatapultHLS2022,](#bib.bib245)）、Stratus
    HLS（[StratusHLS2022,](#bib.bib27)）或LegUp（[LegUpHLS2013,](#bib.bib28)）为用户提供了一个高抽象级别的平台，在这里，他们可以用软件编程语言（C/C++/SystemC）描述所需功能，并自动获得相应的高性能HDL实现。HLS因此提高了硬件设计师的生产力，他们可以从更快的设计更改和功能验证中受益。实际上，HLS允许为不同平台（例如更大或更小的FPGA）生成加速器，而无需修改C/C++源代码，只需少量设计指令。这使得探索设计空间并找到最佳实现的速度比HDL设计快得多。需要注意的是，代码必须考虑硬件知识，以满足给定的性能和资源使用结果。任意为CPU目标编写的软件代码可能会实现非常低的性能，因为它通常未能充分暴露可用于FPGA或ASIC的空间并发。
- en: 'There are also academic open-source HLS tools available, such as Bambu ([Bambu,](#bib.bib74)
    ). Bambu aids designers in the high-level synthesis of complex applications. It
    supports various C/C++ constructs and follows a software compilation-like flow.
    The tool consists of three phases: front-end, middle-end, and back-end. In the
    front end, the input code is parsed and translated into an intermediate representation.
    The middle-end performs target-independent analyses and optimizations. Lastly,
    the back-end synthesizes Verilog/VHDL code for simulation, logic synthesis, and
    implementation using external tools. Bambu offers a command-line interface and
    is particularly useful for designers seeking assistance in HLS and optimizing
    hardware designs efficiently.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些学术开放源代码的HLS工具，如Bambu（[Bambu,](#bib.bib74)）。Bambu帮助设计师进行复杂应用的高层次综合。它支持各种C/C++构造，并遵循类似软件编译的流程。该工具包括三个阶段：前端、中端和后端。在前端，输入代码被解析并转换为中间表示。中端进行与目标无关的分析和优化。最后，后端合成Verilog/VHDL代码，用于仿真、逻辑综合和使用外部工具进行实现。Bambu提供了命令行界面，特别适用于寻求HLS帮助并高效优化硬件设计的设计师。
- en: 'In order to explore the acceleration of DNN inference on FPGAs, several frameworks
    and packages have been developed based on HLS. They can be divided into two categories:
    tools based on libraries of HLS templates, such as FINN ([FINN,](#bib.bib20) )
    and hls4ml ([hls4ml,](#bib.bib68) ), and tools that use a compiler-based approach,
    such as SODA ([sodaMICRO,](#bib.bib21) ) and ScaleHLS ([scalehls2022dac,](#bib.bib299)
    ). In ([Machura2022,](#bib.bib177) ), a comparison between a custom implementation
    of two DNNs written in SystemVerilog and an implementation using the Xilinx tools
    FINN and Vitis AI is presented; a comparison between FINN and Vitis AI is reported
    in ([Hamanaka2023,](#bib.bib102) ), where a ResNet model is implemented using
    a widely used set of configurations of FINN and Vitis AI. Both FINN and hls4ml
    use Vitis HLS as a backend; they parse a model exported from high-level ML frameworks
    and replace operators with C/C++ functions taken from a library of templates that
    already contains Vitis optimization directives. The HLS tool processes the C/C++
    code and produces a corresponding accelerator design. The library of templates
    is necessarily tied to a specific HLS tool, and it requires expert HLS developers
    to implement in advance the best version of all necessary ML operators for a pre-determined
    backend tool. On the other hand, SODA and ScaleHLS use a compiler infrastructure
    (MLIR, the Multi-Level Intermediate Representation from the LLVM project ([lattner2021mlir,](#bib.bib156)
    )) to progressively translate the input model through representations at different
    levels of abstraction, until they can be passed to the HLS tool as a C++ representation
    or an LLVM IR. This second approach exploits the existing MLIR infrastructure
    for machine learning, without requiring to create and maintain a library of operators.
    A hybrid RTL–HLS approach has been proposed in ([Guan2017,](#bib.bib96) ) to improve
    performance and development time for various DL algorithms.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探讨DNN推理在FPGAs上的加速，基于HLS开发了若干框架和包。这些工具可以分为两类：基于HLS模板库的工具，例如FINN ([FINN,](#bib.bib20))
    和hls4ml ([hls4ml,](#bib.bib68))，以及使用编译器基础方法的工具，例如SODA ([sodaMICRO,](#bib.bib21))
    和ScaleHLS ([scalehls2022dac,](#bib.bib299))。在 ([Machura2022,](#bib.bib177)) 中，比较了用SystemVerilog编写的两个DNN的定制实现与使用Xilinx工具FINN和Vitis
    AI的实现；在 ([Hamanaka2023,](#bib.bib102)) 中报告了FINN和Vitis AI的比较，其中使用了FINN和Vitis AI广泛使用的配置集实现了一个ResNet模型。FINN和hls4ml都使用Vitis
    HLS作为后端；它们解析从高级ML框架导出的模型，并用从包含Vitis优化指令的模板库中提取的C/C++函数替换操作符。HLS工具处理C/C++代码并生成相应的加速器设计。模板库必然与特定HLS工具绑定，并且需要专家HLS开发人员预先实现所有必要ML操作符的最佳版本以适应预定的后端工具。另一方面，SODA和ScaleHLS使用编译器基础设施（MLIR，来自LLVM项目的多级中间表示
    ([lattner2021mlir,](#bib.bib156) )）来逐步将输入模型转换为不同抽象级别的表示，直到可以将其作为C++表示或LLVM IR传递给HLS工具。这第二种方法利用了现有的MLIR机器学习基础设施，无需创建和维护操作符库。([Guan2017,](#bib.bib96))
    中提出了一种混合RTL-HLS方法，以提高各种DL算法的性能和开发时间。
- en: A.5\. Sparse Matrices
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5\. 稀疏矩阵
- en: 'The most famous definition of *sparse matrix* is attributed to James Wilkinson
    and dates back to more than 50 years ago ([davis2007,](#bib.bib58) ): any matrix
    with enough zeros that it pays to take advantage of them. A more recent and quantitative
    definition by Filippone et al. ([filippone2017sparse,](#bib.bib75) ) states that
    a matrix is *sparse* if its number of non-zero coefficients is $O(n)$, where $n$
    is the number of rows (columns) of the matrix.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '*稀疏矩阵*的最著名定义归功于詹姆斯·威尔金森，距今已有50多年历史 ([davis2007,](#bib.bib58))：任何有足够多零元素的矩阵，都值得利用这些零元素。Filippone等人提出了一个更近期的定量定义 ([filippone2017sparse,](#bib.bib75))，如果一个矩阵的非零系数数量是$O(n)$，其中$n$是矩阵的行（列）数，那么这个矩阵就是*稀疏的*。'
- en: Sparse matrices are usually stored in a compressed format to avoid redundant
    storage as well as a lot of useless calculations. That is, the storage format
    attempts to take advantage of the zeros by avoiding their explicit storage. The
    counterpart is that the traditional simple mapping between the index pair of each
    matrix coefficient and the position of the coefficient in memory is destroyed.
    Therefore, all sparse matrix storage formats are devised around rebuilding this
    mapping by using some auxiliary index information. This rebuilding has a non-negligible
    cost and impact on the matrix operations to be performed. Therefore, the performance
    of sparse matrix computations depends on the selected storage format.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵通常以压缩格式存储，以避免冗余存储以及大量无用计算。也就是说，存储格式尝试利用零值，避免它们的显式存储。相对的，传统的矩阵系数索引对与系数在内存中位置的简单映射被破坏。因此，所有稀疏矩阵存储格式都是围绕使用一些辅助索引信息来重建这种映射而设计的。这种重建具有不可忽视的成本和对矩阵操作的影响。因此，稀疏矩阵计算的性能取决于所选的存储格式。
- en: 'Widely used sparse matrix storage formats include COOrdinate (COO), Compressed
    Sparse Rows (CSR), and Compressed Sparse Columns (CSC) ([filippone2017sparse,](#bib.bib75)
    ). CSR is perhaps the most popular sparse matrix representation: It compresses
    the sparse matrix into three different arrays. The first array represents the
    non-zero values, the second contains the column indexes, and the third marks the
    boundaries of each row in the matrix. The above formats can be considered general-purpose,
    meaning that they can be used on most hardware with little or no changes. However,
    hardware-oriented formats become attractive when moving to special computing architectures
    such as accelerators. Many storage formats, such as ELLPACK, are specifically
    developed for vector machines to introduce a certain degree of regularity in the
    data structure to enable the efficient exploitation of vector instructions.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛使用的稀疏矩阵存储格式包括 COOrdinate (COO)、Compressed Sparse Rows (CSR) 和 Compressed Sparse
    Columns (CSC) ([filippone2017sparse,](#bib.bib75) )。CSR 可能是最流行的稀疏矩阵表示形式：它将稀疏矩阵压缩成三个不同的数组。第一个数组表示非零值，第二个包含列索引，第三个标记矩阵中每一行的边界。上述格式可以被视为通用的，这意味着它们可以在大多数硬件上使用，几乎无需更改。然而，当转向特殊计算架构如加速器时，面向硬件的格式变得有吸引力。许多存储格式，如
    ELLPACK，专门为矢量计算机开发，以在数据结构中引入一定程度的规律性，从而有效利用矢量指令。
- en: A factor that can drive the choice of the storage format is the sparsity pattern
    of the matrix that is, the pattern of non-zero entries contained in the matrix.
    Common sparsity patterns include unstructured (where nonzeros are randomly and
    irregularly scattered), diagonal (where nonzeros are restricted to a small number
    of matrix diagonals), and block sparse (either coarse-grain or fine-grain). Each
    of these sparsity patterns is best addressed using different formats. For instance,
    the diagonal format (DIA) is an appropriate representation for diagonal matrices.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动存储格式选择的一个因素是矩阵的稀疏性模式，即矩阵中非零条目的模式。常见的稀疏性模式包括无结构的（非零值随机且不规则地分布）、对角的（非零值限制在少量矩阵对角线）和块稀疏的（粗粒度或细粒度）。每种稀疏性模式最适合使用不同的格式。例如，对角格式
    (DIA) 是对角矩阵的适当表示。
- en: DNN models are composed of large, dense matrices which are typically used in
    matrix multiplication and convolutions. In the last years, state-of-the-art DL
    models have dramatically increased in size, with hundreds of billions of parameters
    (e.g., large language models as GPT-3 require 175B parameters ([brown:2020,](#bib.bib25)
    )) and trillions of compute operations per input sample. In order to reduce DNN
    model sizes and computation requirements (including the energy footprint), pruning
    (i.e., setting to zero) of DNN weights has emerged as a particularly effective
    and promising technique. Pruning entails identifying unnecessary redundancy in
    DNN-trained model weights and zero out these nonessential weights ([han:nips2015,](#bib.bib105)
    ; [srinivas:2015,](#bib.bib255) ), thus allowing to discard of zero values from
    storage and computations. Therefore, pruning induces sparsity in the DL model,
    in which a large proportion (typically between 50%([park:arxiv2016,](#bib.bib209)
    ) to 90% ([hao:arxiv2015,](#bib.bib104) )) of the weights are zero. Pruning methods
    allow keeping model accuracy with little loss in model quality, thus achieving
    the same expressive power as dense model counterparts, while leading to models
    that are more efficient in terms of computing and storage resources demand.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 模型由大而密集的矩阵组成，这些矩阵通常用于矩阵乘法和卷积。在过去几年中，最先进的深度学习（DL）模型的规模显著增加，拥有数百亿个参数（例如，大型语言模型如GPT-3需要175B参数 ([brown:2020,](#bib.bib25)
    )）以及每个输入样本的万亿次计算操作。为了减少 DNN 模型的规模和计算需求（包括能源消耗），剪枝（即将权重设为零）已经成为一种特别有效且有前景的技术。剪枝包括识别
    DNN 训练模型权重中的不必要冗余，并将这些非必要的权重置为零 ([han:nips2015,](#bib.bib105) ; [srinivas:2015,](#bib.bib255)
    )，从而可以将零值从存储和计算中丢弃。因此，剪枝在 DL 模型中引入了稀疏性，其中大比例（通常在50%([park:arxiv2016,](#bib.bib209)
    )至90% ([hao:arxiv2015,](#bib.bib104) )）的权重为零。剪枝方法能够保持模型的准确性，模型质量损失很小，从而实现与密集模型相同的表达能力，同时使得模型在计算和存储资源需求方面更高效。
- en: The second factor that induces sparsity in DNN models is the ReLU (rectified
    linear unit) operator, which is frequently used as an activation function. Indeed,
    ReLU resets all the negative values in the matrices of the activations¹¹1The activations
    are the output values of an individual layer that are passed as inputs to the
    next layer. to zero.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 促使 DNN 模型稀疏性的第二个因素是 ReLU（修正线性单元）运算符，它常作为激活函数使用。事实上，ReLU 将激活矩阵中的所有负值重置为零¹¹1激活值是个别层的输出值，这些值被传递到下一层。
- en: Because of network pruning and zero-valued activations, sparsity has become
    an active area of research in DNN models. These two techniques allow to reduce
    both the memory size and the memory accesses, the latter thanks to the removal
    of useless operations (i.e., multiply by zero), which also save processing power
    and energy consumption. As regards the memory size, the number of non-zero entries
    in the resulting sparse matrices can be reduced to 20-80% and 50-70% for weights
    and activations, respectively ([han:nips2015,](#bib.bib105) ; [SCNN:isca2017,](#bib.bib207)
    ). Sparse matrices can thus be stored using a compressed representation, thus
    leading to at least 2-3x memory size reduction. However, the main disadvantage
    of a sparse matrix is that the indexes become relative, which adds extra levels
    of indirection that add complexity and need to be carefully managed to avoid inefficiency.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 由于网络剪枝和零值激活，稀疏性已经成为深度神经网络（DNN）模型中的一个活跃研究领域。这两种技术不仅能减少内存大小，还能减少内存访问，后者通过移除无用操作（即，乘以零）来实现，这也节省了处理能力和能耗。关于内存大小，结果稀疏矩阵中非零项的数量可以减少到20-80%（权重）和50-70%（激活值），例如 ([han:nips2015,](#bib.bib105)
    ; [SCNN:isca2017,](#bib.bib207) )。因此，稀疏矩阵可以使用压缩表示来存储，从而至少实现2-3倍的内存大小缩减。然而，稀疏矩阵的主要缺点是索引变得相对，这增加了额外的间接层次，增加了复杂性，需要仔细管理以避免低效。
- en: As regards the sparsity pattern of DNN models, it can range from unstructured
    as a result of fine-grain pruning, which maintains model accuracy, to structured
    when coarse-grain pruning is applied to improve execution efficiency at the cost
    of downgrading the model accuracy ([han:nips2015,](#bib.bib105) ; [wen:nisp2016,](#bib.bib293)
    ). Randomly distributed nonzeros can lead to irregular memory accesses, that are
    unfriendly on commodity architectures, e.g., GPU, as well as to irregular computations
    that introduce conditional branches to utilize the sparsity. The latter are hardly
    applicable for accelerators, which are designed for fine-grained data or thread
    parallelism rather than flexible data path control. On the other hand, hardware-friendly
    structured sparsity can efficiently accelerate the DNN evaluation at the cost
    of model accuracy degradation.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 关于深度神经网络（DNN）模型的稀疏模式，它可以从由于细粒度剪枝而产生的非结构化（这保持了模型准确性），到应用粗粒度剪枝以提高执行效率但会降低模型准确性的结构化模式（[han:nips2015,](#bib.bib105)
    ; [wen:nisp2016,](#bib.bib293)）。随机分布的非零元素可能导致不规则的内存访问，这对商品架构（如GPU）不友好，也会导致引入条件分支以利用稀疏性的不规则计算。这些对于专为细粒度数据或线程并行设计的加速器几乎不适用，而不是灵活的数据路径控制。另一方面，硬件友好的结构化稀疏性可以在模型准确性降低的代价下有效加速DNN评估。
- en: Moreover, sparsity is becoming ubiquitous in modern deep learning workloads
    (i.e., not only because of the application of compression techniques such as network
    pruning and zero-valued activations) due to the application of deep learning to
    graphs for modeling relations (in social networks, proteins, etc.) using highly
    sparse matrices, such as in Graph Neural Networks (GNNs).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于将深度学习应用于图形（用于建模关系，如社交网络、蛋白质等），并使用高度稀疏的矩阵（例如图神经网络（GNNs）），稀疏性在现代深度学习工作负载中变得越来越普遍（不仅仅是因为应用了压缩技术，如网络剪枝和零值激活）。
- en: The key computational kernel within most DL workloads is general matrix-matrix
    multiplications (GEMM) ([gao2023acm,](#bib.bib82) ). It appears frequently during
    both the forward pass (inference and training) and backward pass (training); for
    instance, experiments reported in ([sigma:2020,](#bib.bib226) ) show that GEMM
    comprises around 70% of the total compute cycles during training for Transformer
    and Google Neural Machine Translation workloads. Therefore, GEMM represents a
    primary target for hardware acceleration in order to speed up training and inference.
    However, GEMM in DL is characterized by sparsity of matrices, which arises from
    pruning as explained above, and non-square matrix dimensions, which arise from
    mini-batches and weight factorization ([park-facebook:arxiv2018,](#bib.bib210)
    ). A popular computational kernel for convolutional neural networks (CNNs) is
    a sparse vector-vector dot product. Sparse-dense matrix multiplication (SpMM)
    and sampled-dense matrix multiplication (SDDMM) are two of the most generic kernels
    in GNNs.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习（DL）工作负载中的关键计算核心是通用矩阵-矩阵乘法（GEMM）（[gao2023acm,](#bib.bib82)）。它在前向传播（推理和训练）和反向传播（训练）中经常出现；例如，在（[sigma:2020,](#bib.bib226)）报告的实验中，GEMM占据了Transformer和Google神经机器翻译工作负载训练期间约70%的计算周期。因此，GEMM是硬件加速的主要目标，以加快训练和推理。然而，DL中的GEMM的特点是矩阵的稀疏性，这种稀疏性来源于上述剪枝，和非方阵尺寸，这些来自于小批量和权重因式分解（[park-facebook:arxiv2018,](#bib.bib210)）。卷积神经网络（CNNs）中一个流行的计算核心是稀疏向量-向量点积。稀疏-密集矩阵乘法（SpMM）和采样-密集矩阵乘法（SDDMM）是图神经网络（GNNs）中最通用的两个核心。
- en: Spatial-architecture-based hardware accelerators that exploit sparsity have
    different architectures that allow to adapt the computation to sparse matrices.
    In the following, we review their main features.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 利用稀疏性的基于空间架构的硬件加速器具有不同的架构，可以使计算适应稀疏矩阵。接下来，我们将深入探讨它们的主要特征。
- en: A.6\. Emerging 3D-stacked Processing-in-memory Technologies
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.6. 新兴的3D堆叠内存处理技术
- en: 3D integration technologies ([Kada2015,](#bib.bib137) ) enable to stack as many
    as 16 or more 2D integrated circuits and interconnect them vertically using, for
    instance, through-silicon vias (TSVs), micro bumps, or Cu-Cu connections. In this
    way, a 3D circuit behaves as a single device achieving a smaller area footprint
    than conventional 2D circuits, while reducing power and latency in data transfer.
    In general, 3D integration is a term that includes such technologies as 3D wafer-level
    packaging (3DWLP) ([Soussan2008,](#bib.bib250) ), 2.5D and 3D interposer-based
    integration ([Lau2011,](#bib.bib52) ), 3D stacked ICs (3D-SICs), 3D heterogeneous
    integration, and 3D systems integration, as well as true monolithic 3D ICs ([Knechtel2017,](#bib.bib146)
    ; [Mathur2021,](#bib.bib180) ).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 3D集成技术 ([Kada2015,](#bib.bib137) ) 使得可以将多达16个或更多的2D集成电路堆叠起来，并使用例如硅通孔（TSV）、微凸点或Cu-Cu连接进行垂直互连。通过这种方式，3D电路表现得像一个单一设备，具有比传统2D电路更小的占地面积，同时减少数据传输中的功耗和延迟。一般而言，3D集成是一个涵盖诸如3D晶圆级封装（3DWLP）
    ([Soussan2008,](#bib.bib250) ), 2.5D和3D中介基板集成 ([Lau2011,](#bib.bib52) ), 3D堆叠IC（3D-SICs）、3D异构集成和3D系统集成等技术的术语，以及真正的单片3D
    IC ([Knechtel2017,](#bib.bib146) ; [Mathur2021,](#bib.bib180) )。
- en: These technologies have been employed in the development of new memory devices,
    which stack layers of conventional 2D DRAM or other memory types (for instance,
    Non-Volatile Memory (NVM) based on ReRAM ([Clarke2015,](#bib.bib50) )) together
    with one or more optional layers of logic circuits. These logic layers are often
    implemented with different process technology and can include buffer circuitry,
    test logic, and processing elements. Compared to 2D memories, 3D stacking increases
    memory capacity and bandwidth, reduces access latency due to the shorter on-chip
    wiring interconnection and the use of wider buses, and potentially improves the
    performance and power efficiency of the system. In fact, 3D stacking of DRAM memory
    provides an order of magnitude higher bandwidth and up to $5\times$ better energy
    efficiency than conventional 2D solutions, making the technology an excellent
    option for meeting the requirements in terms of high throughput and low energy
    of DNN accelerators ([Hassanpour2022,](#bib.bib108) ).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术已被用于开发新型内存设备，这些设备堆叠了传统的2D DRAM或其他内存类型（例如基于ReRAM的非易失性内存（NVM） ([Clarke2015,](#bib.bib50)
    )），以及一个或多个可选的逻辑电路层。这些逻辑层通常采用不同的工艺技术实现，可以包括缓冲电路、测试逻辑和处理元素。与2D内存相比，3D堆叠提高了内存容量和带宽，减少了由于更短的芯片内布线互连和使用更宽的总线而导致的访问延迟，并可能提高系统的性能和能效。实际上，DRAM内存的3D堆叠提供了比传统2D解决方案高一个数量级的带宽和高达$5\times$的能效提升，使这项技术成为满足DNN加速器在高吞吐量和低能耗方面要求的优良选择
    ([Hassanpour2022,](#bib.bib108) )。
- en: 'Two main 3D stacked memory standards have been recently proposed: the Hybrid
    Memory Cube (HMC) and the High Bandwidth Memory (HBM). 3D stacked processing-in-memory
    accelerator proposals modify the architecture of the 3D memory block inserting
    processing logic near the memory elements. Two approaches can be commonly found.
    In the first approach, the computing logic is embedded into the logic die (logic
    die-level processing-in-memory); in the second approach, the processing logic
    is integrated into each DRAM die at the level of memory banks, after the column
    decoder and selector blocks (bank-level processing-in-memory). We present in the
    following subsections the main characteristics of the two 3D stacked memory standards
    and overview the existing accelerators adopting them.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 最近提出了两个主要的3D堆叠内存标准：混合内存立方体（HMC）和高带宽内存（HBM）。3D堆叠的处理内存加速器方案通过在内存元素附近插入处理逻辑来修改3D内存块的架构。通常可以找到两种方法。第一种方法是将计算逻辑嵌入到逻辑芯片中（逻辑芯片级处理内存）；第二种方法是在每个DRAM芯片的内存银行级别集成处理逻辑，在列解码器和选择器块之后（银行级处理内存）。我们在接下来的子节中介绍这两种3D堆叠内存标准的主要特征，并概述采用它们的现有加速器。
- en: A.6.1\. Hybrid Memory Cube
  id: totrans-413
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.6.1\. 混合内存立方体
- en: The Hybrid Memory Cube is a single package containing four to eight DRAM die
    and one logic die, all stacked together using thousands of TSVs, achieving a much
    desired high memory bandwidth ([Jeddeloh2012,](#bib.bib122) ; [HMC2014,](#bib.bib117)
    ). As shown in Figure [12(a)](#A1.F12.sf1 "In Figure 12 ‣ A.6.1\. Hybrid Memory
    Cube ‣ A.6\. Emerging 3D-stacked Processing-in-memory Technologies ‣ Appendix
    A Appendix ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms"), in a HMC, memory is organized vertically, and portions of each
    memory die are combined with the corresponding portions of the other memory dies
    and the logic die. This creates a 2D grid of vertical partitions, referred as
    vaults ([Pawlowski2011,](#bib.bib216) ). Each vault is functionally and operationally
    independent and includes in the logic layer a memory controller that manages all
    memory reference operations within that vault, as well as determining timing requirements
    and dealing with refresh operations, eliminating these functions from the host
    memory controller. The independence of each vault allows to exploit memory level
    parallelism, as multiple partitions in the DRAM dies can be accessed simultaneously.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 混合内存立方体（Hybrid Memory Cube）是一个单一封装，其中包含四到八个DRAM芯片和一个逻辑芯片，这些芯片通过数千个TSV（穿透硅通孔）堆叠在一起，达到非常期望的高内存带宽（[Jeddeloh2012,](#bib.bib122)
    ; [HMC2014,](#bib.bib117)）。如图[12(a)](#A1.F12.sf1 "在图12 ‣ A.6.1\. 混合内存立方体 ‣ A.6\.
    新兴的3D堆叠内存处理技术 ‣ 附录A 附录 ‣ 关于异构HPC平台的深度学习硬件加速器调查")所示，在HMC中，内存是垂直组织的，每个内存芯片的部分区域与其他内存芯片和逻辑芯片的相应部分结合在一起。这创造了一个垂直分区的二维网格，称为保险库（[Pawlowski2011,](#bib.bib216)）。每个保险库在功能和操作上都是独立的，并且在逻辑层中包含一个内存控制器，该控制器管理该保险库内的所有内存引用操作，同时确定时序要求并处理刷新操作，将这些功能从主内存控制器中剥离。每个保险库的独立性使得可以利用内存级别的并行性，因为DRAM芯片中的多个分区可以同时访问。
- en: Commands and data are transmitted from and to the host across external I/O links
    consisting of up to four serial links, each with a default of 16 input lanes and
    16 output lanes for full duplex operation (HMC2 specifications ([HMC2014,](#bib.bib117)
    )). All in-band communication across a link is packetized. According to specifications,
    up to 320 GB/s effective bandwidth can be achieved by considering 30 Gb/s SerDes
    I/O interfaces, with a storage capacity, depending on the number of stacked layers,
    of 4GB and 8GB ([Micron2018,](#bib.bib184) ; [HMC2014,](#bib.bib117) ).
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 命令和数据通过外部I/O链路从主机传输到主机，每个链路由最多四个串行链路组成，每个链路默认具有16个输入通道和16个输出通道，以实现全双工操作（HMC2规格（[HMC2014,](#bib.bib117)））。所有链路中的带内通信都被分组。根据规格，通过考虑30
    Gb/s SerDes I/O接口，可以实现高达320 GB/s的有效带宽，存储容量则取决于堆叠层数，分别为4GB和8GB（[Micron2018,](#bib.bib184)
    ; [HMC2014,](#bib.bib117)）。
- en: '![Refer to caption](img/5ef05c738b121efc555df73560b4756a.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5ef05c738b121efc555df73560b4756a.png)'
- en: (a)
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/d4d072174d468e295c811693bb3b8feb.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d4d072174d468e295c811693bb3b8feb.png)'
- en: (b)
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: Figure 12. (a) High-level architecture of the Hybrid Memory Cube. (b) Cut through
    image of a computing system with HBM.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 图12. (a) 混合内存立方体的高层架构。 (b) 具有HBM的计算系统的截面图。
- en: A.6.2\. High Bandwidth Memory
  id: totrans-421
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.6.2\. 高带宽内存
- en: The High Bandwidth Memory is a high-speed computer memory interface for 3D-stacked
    synchronous dynamic random-access memory (SDRAM) ([Joonyoung2014,](#bib.bib144)
    ; [Sohn2017,](#bib.bib247) ). Each memory module is composed by stacking up to
    eight DRAM dies and an optional base die including buffer circuitry and test logic.
    Dies are vertically interconnected by TSVs and microbumps, in a way similar to
    the HMC. As shown in Figure [12(b)](#A1.F12.sf2 "In Figure 12 ‣ A.6.1\. Hybrid
    Memory Cube ‣ A.6\. Emerging 3D-stacked Processing-in-memory Technologies ‣ Appendix
    A Appendix ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms"), the memory stack is often connected to the memory controller
    on the host (e.g., GPU or CPU) through purpose-built silicon chip, called interposer
    ([Lau2011,](#bib.bib52) ), which is effectively a miniature PCB that goes inside
    the package and decreases the memory paths by allowing the host and the memory
    to be physically close. However, as semiconductor device fabrication is significantly
    more expensive than printed circuit board manufacture, this adds cost to the final
    product. Alternatively, the memory die can be stacked directly on the host processor
    chip.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 高带宽内存是一种用于 3D 堆叠同步动态随机存取内存（SDRAM）的高速计算机内存接口 ([Joonyoung2014,](#bib.bib144) ;
    [Sohn2017,](#bib.bib247) )。每个内存模块由最多八个 DRAM 芯片和一个可选的基础芯片（包括缓冲电路和测试逻辑）堆叠组成。芯片通过
    TSV 和微凸点垂直互连，方式类似于 HMC。如图 [12(b)](#A1.F12.sf2 "在图 12 ‣ A.6.1\. 混合内存立方体 ‣ A.6\.
    新兴 3D 堆叠内存处理技术 ‣ 附录 A 附录 ‣ 关于异构 HPC 平台的深度学习硬件加速器调查") 所示，内存堆叠通常通过专用硅芯片（称为中介层）连接到主机（例如
    GPU 或 CPU）的内存控制器 ([Lau2011,](#bib.bib52) )，这实际上是一个迷你 PCB，安装在封装内部，通过使主机和内存物理上接近来减少内存路径。然而，由于半导体器件制造的成本显著高于印刷电路板制造，这增加了最终产品的成本。另一种选择是将内存芯片直接堆叠在主处理器芯片上。
- en: The HBM DRAM is tightly coupled to the host computer through a distributed interface,
    which is divided into independent channels. The HBM DRAM uses a wide-interface
    architecture to achieve high-speed, low-power operation. Each channel interface
    maintains a 128-bit (HMB2) or 64-bit (HMB3) data bus operating at double data
    rate (DDR). The latest version (2022) of the HBM (HBM3) supports up to 16 channels
    of 64 bits, with a total number of data pins equal to 1024, and with an overall
    package bandwidth of 600 GB/s ([JEDEC2023,](#bib.bib123) ; [Prickett2022,](#bib.bib223)
    ). Depending on the producer, the HBM stack consists of 8 or 12 16Gb DRAMs, with
    a total maximum memory capacity of 24 GB ([Robinson2022,](#bib.bib233) ).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: HBM DRAM 通过分布式接口紧密耦合到主计算机，该接口被分为独立的通道。HBM DRAM 使用宽接口架构实现高速、低功耗操作。每个通道接口维护一个
    128 位（HMB2）或 64 位（HMB3）数据总线，以双数据速率（DDR）运行。最新版本（2022年）的 HBM（HBM3）支持最多 16 个 64 位通道，总数据引脚数为
    1024，整体封装带宽为 600 GB/s ([JEDEC2023,](#bib.bib123) ; [Prickett2022,](#bib.bib223)
    )。根据生产商的不同，HBM 堆叠由 8 或 12 个 16Gb DRAM 组成，总最大内存容量为 24 GB ([Robinson2022,](#bib.bib233)
    )。
- en: 'A.6.3\. 3D stacked PIM: Accelerating applications loosely related to DNNs'
  id: totrans-424
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.6.3\. 3D 堆叠 PIM：加速与 DNN 松散相关的应用
- en: In addition to the DNN accelerators described in Section 5.2, we briefly overview
    3D stacked accelerators for applications loosely related to DNNs. The use of PEs
    in the logic layer of an HMC is discussed in ([Oliveira2017,](#bib.bib203) ),
    to support the simulation of large networks on neurons. The proposed Neuron In-Memory
    (NIM) architecture is composed of 2,048 functional units, operating on integer
    and floating-point data, and a small register file with 8 × 16 registers of 32
    bits each per vault. Fast vector elements operation is also supported. When compared
    with traditional multi-core environments, NIM provides overall system acceleration
    and reduces overall energy consumption, taking advantages of the broad bandwidth
    available in 3D-stacked memory devices.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 除了第 5.2 节中描述的 DNN 加速器外，我们简要概述了用于与 DNN 松散相关应用的 3D 堆叠加速器。在 ([Oliveira2017,](#bib.bib203)
    ) 中讨论了 HMC 逻辑层中 PE 的使用，以支持大规模神经网络的模拟。提出的 Neuron In-Memory（NIM）架构由 2,048 个功能单元组成，处理整数和浮点数据，以及一个小型寄存器文件，每个保险库中有
    8 × 16 个 32 位寄存器。还支持快速向量元素操作。与传统的多核环境相比，NIM 提供了整体系统加速并降低了整体能耗，利用了 3D 堆叠内存设备中广泛的带宽。
- en: Millipede ([Nitin2018,](#bib.bib195) ) is an NDP architecture for Big data Machine
    Learning Analytics (BMLA) that implements its processors in the logic layer of
    3D-stacked memories. These processors have a local memory, register file, pipeline,
    cache, and prefetch buffers.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: Millipede（[Nitin2018,](#bib.bib195)）是一种用于大数据机器学习分析（BMLA）的 NDP 架构，将其处理器实现于 3D
    堆叠内存的逻辑层中。这些处理器具有本地内存、寄存器文件、流水线、缓存和预取缓冲区。
- en: The authors in ([Mathur2021,](#bib.bib180) ) explore the design trade-offs and
    thermal implications of 3D stacking in different configurations layers of SRAM
    buffers and systolic accelerators composed of MAC elements, while targeting Deep
    learning applications. The main memory (DRAM) is however not necessarily stacked
    with the rest of the system. Their simulations show that stacking PE array on
    top of the SRAM stack in a logic-over-memory fashion can not only achieve low
    energy but also mitigate the thermal impact of 3-D.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: （[Mathur2021,](#bib.bib180)）中的作者探讨了在不同配置的 SRAM 缓冲区和由 MAC 元素组成的流线型加速器中的 3D 堆叠的设计权衡和热影响，同时针对深度学习应用。主内存（DRAM）与系统的其余部分不一定堆叠。他们的模拟表明，将
    PE 数组堆叠在 SRAM 堆栈顶部，采用逻辑覆盖内存的方式，不仅可以实现低能耗，还可以减轻 3D 的热影响。
- en: iPIM ([Gu2020,](#bib.bib95) ) uses a near-bank architecture for image processing.
    The control and the execution are decoupled to obtain a higher bank-level bandwidth
    and maximize the parallel execution of processing engines on the memory dies.
    Each vault contains a control core in the logic die, while the execution logic
    is placed in the memory die of each vault. Each control core manages intra/inter-vault
    data communication and executes instruction decoding with the support of the single-instruction-multiple-bank
    (SIMB) instruction set architecture (ISA).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: iPIM（[Gu2020,](#bib.bib95)）采用近银行架构进行图像处理。控制和执行解耦，以获得更高的银行级带宽，并最大化内存芯片上处理引擎的并行执行。每个金库包含一个逻辑芯片中的控制核心，而执行逻辑则放置在每个金库的内存芯片中。每个控制核心管理内/跨金库的数据通信，并在单指令多银行（SIMB）指令集架构（ISA）的支持下执行指令解码。
- en: Neurosensor ([Amir2018,](#bib.bib11) ) is a 3D CMOS image sensor system with
    an integrated convolutional neural network computation. The image sensor, read-out
    circuits, memory, and neural computation logic layers are integrated in a single
    stack. The DNN computation platform is an adaptation from Neurocube ([Kim2016,](#bib.bib142)
    ), and consists of a global controller, processing elements, a 2D mesh NoC connecting
    the PEs, and a programmable neurosequence generator for DRAM. The DNN computation
    is split between the sensor and the host, and the optimal task distribution depends
    on the processing capabilities of the sensor, the available amount of in-sensor
    memory for storing the synaptic weights, and the available bandwidth between the
    sensor and the host.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: Neurosensor（[Amir2018,](#bib.bib11)）是一个集成卷积神经网络计算的 3D CMOS 图像传感器系统。图像传感器、读出电路、内存和神经计算逻辑层集成在一个堆叠中。DNN
    计算平台是从 Neurocube（[Kim2016,](#bib.bib142)）改编而来的，包括一个全局控制器、处理元素、连接 PEs 的 2D 网格 NoC
    和一个用于 DRAM 的可编程神经序列生成器。DNN 计算在传感器和主机之间分配，最佳任务分配取决于传感器的处理能力、用于存储突触权重的传感器内存量，以及传感器和主机之间的带宽。
- en: 'A.6.4\. 3D stacked PIM: Some considerations'
  id: totrans-430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.6.4\. 3D 堆叠 PIM：一些考虑因素
- en: First, the amount of processing elements that can be integrated into 3D stacked
    memories is limited by the size of the package. Moreover, the overall power dissipation
    of these elements is limited by thermal issues of 3D stacking, as an increase
    in the operation temperature would result in performance degradation from overheating
    ([Heyman2022,](#bib.bib111) ). Second, the stacking of multiple IC layers has
    a high manufacturing complexity, which leads to lower yield and problematic testability.
    Therefore, in order to support the adoption of this technology, proper cooling
    methods and better manufacturing solutions are required.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，能够集成到 3D 堆叠内存中的处理元素数量受到封装尺寸的限制。此外，这些元素的整体功耗受限于 3D 堆叠的热问题，因为操作温度的升高会导致过热从而性能下降（[Heyman2022,](#bib.bib111)）。其次，多个
    IC 层的堆叠具有较高的制造复杂性，这导致了较低的良率和测试难度。因此，为了支持这一技术的应用，需要适当的冷却方法和更好的制造解决方案。
- en: Apart from the above-mentioned technological challenges, embedding processing
    elements into the memory and moving the computation closer to it requires to rethink
    of the optimization of the system design in order to take into account the proximity
    of the processing logic to the main memory. Depending on the use case, this might
    involve the redesign of the on-chip buffers in the logic die, to support the lower
    latency and energy cost of the accesses to main memory, as well as the use of
    new approaches for representing, partitioning, and mapping the dataflow of the
    application in order to exploit the highly parallel system supported by the availability
    of multiple channels ([Hassanpour2022,](#bib.bib108) ).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述提到的技术挑战之外，将处理单元嵌入内存并将计算移动到更靠近内存的位置需要重新思考系统设计的优化，以考虑处理逻辑与主内存的接近性。根据使用情况，这可能涉及重新设计逻辑芯片中的片上缓冲区，以支持更低延迟和能量成本的主内存访问，以及采用新的方法来表示、划分和映射应用的数据流，以利用由多个通道支持的高度并行系统
    ([Hassanpour2022,](#bib.bib108) )。
- en: A.7\. RRAM and PCM Technologies for IMC
  id: totrans-433
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.7\. RRAM 和 PCM 技术用于 IMC
- en: Besides conventional CMOS designs, emerging non-volatile memories such as the
    RRAM and the PCM have been recently explored for integration in stand-alone DNN
    accelerators. The RRAM device structure (see Figure [13](#A1.F13 "Figure 13 ‣
    A.7\. RRAM and PCM Technologies for IMC ‣ Appendix A Appendix ‣ A Survey on Deep
    Learning Hardware Accelerators for Heterogeneous HPC Platforms")a) is a metal-insulator-metal
    (MIM) structure that consists of a top electrode (TE), a bottom electrode (BE),
    and a metal-oxide layer sandwiched between them. By applying a proper voltage
    across the electrodes and setting the maximum current flowing in the MIM stack
    (through a series transistor), an RRAM cell can modulate the shape of a conductive
    filament created in the metal-oxide layer. In PCM the active material is a chalcogenide
    phase change material, which can remain in either crystalline or amorphous states
    for long periods of time at moderately high temperatures. Starting from the amorphous
    state, the application of voltage pulses with relatively low amplitude causes
    the crystallization induced by Joule heating, whereas the application of pulses
    at higher amplitudes can lead to local melting and consequent amorphization. A
    typical PCM cell has a mushroom shape shown in Figure [13](#A1.F13 "Figure 13
    ‣ A.7\. RRAM and PCM Technologies for IMC ‣ Appendix A Appendix ‣ A Survey on
    Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")a, where
    the pillar-like bottom electrode confines heat and current, thus resulting in
    a hemispherical shape of the molten material. In both technologies, their resistance
    state can be tuned not only as a digital memory but also as a continuous analog
    memory with multiple states to perform in-memory computing ([ielmini2018nature,](#bib.bib118)
    ). This characteristic allows efficient matrix-vector multiplication when RRAM
    and PCM are arranged in crossbar structures (see Figure [13](#A1.F13 "Figure 13
    ‣ A.7\. RRAM and PCM Technologies for IMC ‣ Appendix A Appendix ‣ A Survey on
    Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")b).
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 除了传统的 CMOS 设计之外，诸如 RRAM 和 PCM 之类的新兴非易失性存储器最近已被探索用于独立的 DNN 加速器中。RRAM 器件结构（见图 [13](#A1.F13
    "图 13 ‣ A.7\. RRAM 和 PCM 技术用于 IMC ‣ 附录 A 附录 ‣ 关于异构 HPC 平台的深度学习硬件加速器的调查")a）是一种金属-绝缘体-金属
    (MIM) 结构，由上电极 (TE)、下电极 (BE) 和夹在它们之间的金属-氧化物层组成。通过在电极间施加适当的电压并设置 MIM 堆叠中的最大电流（通过串联晶体管），RRAM
    单元可以调节金属-氧化物层中导电丝的形状。在 PCM 中，活动材料是硫化物相变材料，它可以在中等高温下保持晶体或非晶态的长时间稳定。从非晶态开始，施加相对低幅度的电压脉冲会导致由于焦耳加热而引起的结晶，而施加更高幅度的脉冲会导致局部熔化和随后的非晶化。典型的
    PCM 单元具有图 [13](#A1.F13 "图 13 ‣ A.7\. RRAM 和 PCM 技术用于 IMC ‣ 附录 A 附录 ‣ 关于异构 HPC
    平台的深度学习硬件加速器的调查")a 所示的蘑菇状结构，其中类似柱状的下电极限制了热量和电流，从而形成熔融材料的半球形状。在这两种技术中，其电阻状态不仅可以调节为数字存储器，还可以作为具有多个状态的连续模拟存储器进行内存计算
    ([ielmini2018nature,](#bib.bib118) )。这种特性允许在 RRAM 和 PCM 布置在交叉条形结构中时高效地进行矩阵-向量乘法（见图 [13](#A1.F13
    "图 13 ‣ A.7\. RRAM 和 PCM 技术用于 IMC ‣ 附录 A 附录 ‣ 关于异构 HPC 平台的深度学习硬件加速器的调查")b）。
- en: '![Refer to caption](img/24f13a977ce342577b541310eb32cadf.png)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/24f13a977ce342577b541310eb32cadf.png)'
- en: Figure 13. (a) RRAM and PCM devices structure and (b) their arrangement in a
    crossbar structure for matrix-vector multiplication. (c) Example of a stand-alone
    DNN accelerator (i.e., PRIME ([chi2016isca,](#bib.bib45) )) using RRAM crossbars
    for in situ MAC operations. Reprinted from ([lepri2023jeds,](#bib.bib163) ) and
    ([chen2020engineering,](#bib.bib40) ) under Creative Commons License.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13. (a) RRAM和PCM设备的结构，以及 (b) 它们在交叉条结构中的排列，用于矩阵-向量乘法。 (c) 一个独立的DNN加速器示例（即，PRIME
    ([chi2016isca,](#bib.bib45) )）使用RRAM交叉条进行原位MAC操作。转载自 ([lepri2023jeds,](#bib.bib163)
    ) 和 ([chen2020engineering,](#bib.bib40) )，并依据创作共享许可证发布。
- en: A.8\. Alternative Integration Technologies
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.8\. 替代集成技术
- en: The semiconductor industry has grown significantly as a result of increased
    integration complexity, resulting in improved performance and cost-effectiveness
    of transistors. Unfortunately, the trend of increasing the number of transistors
    per die is slowing down, leading to a power-efficiency-driven design era known
    as *dark silicon* ([esmaeilzadeh_isca11,](#bib.bib71) ). While the number of transistors
    per die continues to increase, many foundries are struggling to achieve the targeted
    area scaling per transistor, and new process technologies are expected to slow
    down. The cost per transistor may no longer hold, resulting in yield challenges
    and additional wafer costs. Circuit designers and computer architects can no longer
    rely on the free availability of additional transistors and integration opportunities
    with each new process node, and non-recurring engineering costs have also increased
    due to fabrication and system complexity challenges ([shalf_ptrs19,](#bib.bib130)
    ).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 半导体行业由于集成复杂性的增加而显著增长，从而提高了晶体管的性能和成本效益。不幸的是，增加每片晶圆晶体管数量的趋势正在减缓，进入了一个以*黑硅*（[esmaeilzadeh_isca11,](#bib.bib71)）为驱动的功率效率设计时代。尽管每片晶圆的晶体管数量持续增加，但许多铸造厂仍在努力实现每个晶体管的目标面积缩放，并且预计新工艺技术将放缓。每个晶体管的成本可能不再稳定，导致产量挑战和额外的晶圆成本。电路设计师和计算机架构师不再可以依赖每个新工艺节点提供的额外晶体管和集成机会，并且由于制造和系统复杂性挑战，非重复工程成本也有所增加（[shalf_ptrs19,](#bib.bib130)）。
- en: 'Alternative integration technologies can provide cost reductions and increase
    the number of transistors per circuit. These technologies include die-level integration
    such as 3D die stacking with connections through micro-bumps or Through-Silicon
    Vias (TSVs) ([hu_micro18,](#bib.bib114) ), or through interposer-based 2.5D integration ([zhang_apr15,](#bib.bib306)
    ). By partitioning a monolithic SoC across multiple small dies, namely *chiplets*,
    (see Figure [14(a)](#A1.F14.sf1 "In Figure 14 ‣ A.8\. Alternative Integration
    Technologies ‣ Appendix A Appendix ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms")), yield per die can be improved and metal layer
    count can be reduced, which can lead to a lower total IC cost ([stow_iccad16,](#bib.bib257)
    ). In fact, larger chips cost more due to two main factors: geometry and manufacturing
    defects. Fewer larger chips can fit in a wafer, while defects in larger chips
    waste more silicon than defects in smaller chips ([ajaykumar_micro15,](#bib.bib138)
    ). Smaller chips can be packed more tightly, resulting in more chips that work.
    In general, making smaller chips results in a higher yield of functioning chips
    (see Figure [14(b)](#A1.F14.sf2 "In Figure 14 ‣ A.8\. Alternative Integration
    Technologies ‣ Appendix A Appendix ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms")).'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 替代集成技术可以降低成本并增加每个电路的晶体管数量。这些技术包括通过微凸点或硅通孔（TSV）进行连接的3D芯片堆叠等晶圆级集成（[hu_micro18,](#bib.bib114)），或通过基板互连的2.5D集成（[zhang_apr15,](#bib.bib306)）。通过将单片SoC分割成多个小芯片，即*芯片组*（参见图 [14(a)](#A1.F14.sf1
    "在图 14 ‣ A.8\. 替代集成技术 ‣ 附录 A 附录 ‣ 异构HPC平台深度学习硬件加速器调查")），可以提高每片芯片的产量并减少金属层数，从而降低整体IC成本（[stow_iccad16,](#bib.bib257)）。事实上，由于两个主要因素：几何形状和制造缺陷，较大的芯片成本更高。较少的大芯片能够适应一个晶圆，而较大的芯片中的缺陷浪费的硅比小芯片中的缺陷更多（[ajaykumar_micro15,](#bib.bib138)）。较小的芯片可以更紧密地排列，从而实现更多的工作芯片。一般来说，制造较小的芯片可以提高功能芯片的产量（参见图 [14(b)](#A1.F14.sf2
    "在图 14 ‣ A.8\. 替代集成技术 ‣ 附录 A 附录 ‣ 异构HPC平台深度学习硬件加速器调查")）。
- en: '![Refer to caption](img/154ef7af12776ea2eda5d522df665c2a.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/154ef7af12776ea2eda5d522df665c2a.png)'
- en: (a)
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/1fabdd942b755fc2afde5fae758184a3.png)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1fabdd942b755fc2afde5fae758184a3.png)'
- en: (b)
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: Figure 14. Die-level integration through TSV-based 3D and interposer-based 2.5D
    technologies [14(a)](#A1.F14.sf1 "In Figure 14 ‣ A.8\. Alternative Integration
    Technologies ‣ Appendix A Appendix ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms") ([stow_iccad16,](#bib.bib257) ) and overall
    number of chips and impact on yield of an example defect distribution for two
    different chip sizes ([ajaykumar_micro15,](#bib.bib138) )
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 图14. 通过基于TSV的3D和基于中介层的2.5D技术进行的芯片级集成 [14(a)](#A1.F14.sf1 "在图14中 ‣ A.8\. 替代集成技术
    ‣ 附录A 附录 ‣ 异构HPC平台的深度学习硬件加速器调查") ([stow_iccad16,](#bib.bib257) ) 和两个不同芯片尺寸的示例缺陷分布对芯片数量和良品率的整体影响 ([ajaykumar_micro15,](#bib.bib138)
    )
- en: Die-level integration provides innovative integration strategies, like heterogeneous
    process integration between dies that can improve performance and reduce costs ([zhang_apr15,](#bib.bib306)
    ). Additionally, this technology can be exploited for the reuse of intellectual
    property to configure SoCs with different die combinations and reduce non-recurring
    overheads.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 芯片级集成提供了创新的集成策略，如芯片之间的异质工艺集成，这可以提高性能并降低成本 ([zhang_apr15,](#bib.bib306) )。此外，这项技术可以用于重用知识产权，以配置具有不同芯片组合的SoC，并减少一次性开支。
- en: In multichip-module (MCM) silicon interposer-based integration, the interposer
    uses micro-bumps to connect the smaller chips, which have a higher density than
    traditional C4 bumps. The impedance across the interposer is the same as conventional
    on-chip interconnects. The only downside is the additional cost of the interposer.
    Vertical 3D chip stacking involves combining multiple chips with through-silicon
    vias (TSVs) for vertical interconnects. This technique has the potential to offer
    the highest bandwidth but it requires significant cost and overall process complexity
    as each die must be thinned and processed for TSVs. Overall, as 3D stacking is
    more expensive and complex, while also potentially causing thermal issues, we
    focus on MCM silicon interposer-based design in the following.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于硅中介层的多芯片模块（MCM）集成中，中介层使用微凸点连接较小的芯片，这些微凸点的密度高于传统的C4凸点。中介层的阻抗与传统的芯片内部互连相同。唯一的缺点是中介层的额外成本。垂直3D芯片堆叠涉及将多个芯片通过硅通孔（TSV）进行垂直互连。这种技术具有提供最高带宽的潜力，但需要显著的成本和整体工艺复杂性，因为每个芯片必须进行减薄和TSV处理。总体而言，由于3D堆叠更昂贵且复杂，同时可能引发热问题，我们在接下来的内容中将重点关注基于MCM硅中介层的设计。
- en: A.8.1\. MCM Silicon Interposer-based Design
  id: totrans-447
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.8.1\. 基于MCM硅中介层的设计
- en: 'In 2.5D integration technology, an interposer is a substrate that connects
    multiple dies (chiplets) together. There are two types of interposers: passive
    interposers and active interposers ([stow_iccad17,](#bib.bib258) ). Passive interposers
    are simple substrates that connect multiple dies together without adding any active
    components. They mainly provide electrical connections, signal routing, and thermal
    management between the dies. On the other hand, active interposers contain active
    components such as transistors, capacitors, and inductors, in addition to the
    electrical connections and signal routing provided by passive interposers. Active
    interposers can perform some processing and signal conditioning functions between
    the dies.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在2.5D集成技术中，中介层是连接多个芯片（芯片块）的基板。有两种类型的中介层：被动中介层和主动中介层 ([stow_iccad17,](#bib.bib258)
    )。被动中介层是简单的基板，仅用于连接多个芯片，而不增加任何主动组件。它们主要提供芯片之间的电气连接、信号路由和热管理。另一方面，主动中介层除了提供电气连接和信号路由外，还包含晶体管、电容器和电感器等主动组件。主动中介层可以在芯片之间执行一些处理和信号调理功能。
- en: A.8.2\. General Purpose Chiplet-based Architectures
  id: totrans-449
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.8.2\. 通用芯片块架构
- en: Chiplet-based designs are being utilized across a wide range of platforms, tailored
    to support various application contexts. The challenges of creating integrated
    SoCs for aerospace platforms in advanced semiconductor nodes are reported in ([mounce_ac16,](#bib.bib190)
    ) in which authors highlight the possibility of creating heterogeneous mixtures
    of chiplets, including different embodiments of processors, ultradense memory
    servers, field-programmable gate array clusters, and configurable analog and radiofrequency
    functional blocks. Further, some of the features necessary to support scalability
    and heterogeneity with multi-domain, hybrid architectures involving a mixture
    of semiconductor technologies and advanced packaging approaches are also outlined.
    In ([vijayaraghavan_hpca17,](#bib.bib282) ) a chiplet-based computing system for
    climate prediction is presented. It integrates a high-throughput and energy-efficient
    GPU chiplet, high-performance multi-core CPU chiplet, and large-capacity 3D memory.
    The system can achieve a bandwidth of 3 TB/s and power consumption of 160 W at
    1 GHz.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 基于芯片片段的设计正在广泛应用于各种平台，以支持不同的应用场景。在（[mounce_ac16](#bib.bib190)）中，报告了为航空航天平台创建集成SoC的挑战，作者强调了创建异构芯片片段混合体的可能性，包括不同形式的处理器、超密集内存服务器、现场可编程门阵列集群以及可配置的模拟和射频功能模块。此外，还概述了一些支持可扩展性和异质性的特性，这些特性涉及到多领域、混合架构，结合了多种半导体技术和先进封装方法。在（[vijayaraghavan_hpca17](#bib.bib282)）中，提出了一种用于气候预测的基于芯片片段的计算系统。该系统集成了一个高吞吐量和高能效的GPU芯片片段、高性能的多核CPU芯片片段和大容量的3D内存。该系统能够在1
    GHz时实现3 TB/s的带宽和160 W的功耗。
- en: GPU platforms are also benefiting from chiplet-based integration. In ([arunkumar_isca17,](#bib.bib14)
    ) a single-chip multi-core GPU is broken down into multiple GPU chiplets to improve
    both performance and energy efficiency by increasing hardware resource utilization
    for both the GPU and DRAM chiplets, while also mitigating the dark silicon effect.
    Additionally, breaking the larger GPU into multiple smaller chiplets has resulted
    in improved wafer yield.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: GPU平台也从基于芯片片段的集成中获益。在（[arunkumar_isca17](#bib.bib14)）中，一个单芯片多核GPU被拆分成多个GPU芯片片段，以提高性能和能源效率，通过增加GPU和DRAM芯片片段的硬件资源利用率，同时减轻暗硅效应。此外，将较大的GPU拆分为多个较小的芯片片段也改善了晶圆的良率。
- en: The design and implementation of a dual-chiplet Chip-on-Wafer-on-Substrate are
    presented in ([lin_vlsi19,](#bib.bib169) ), where each chiplet has four ARM Cortex-A72
    processors operating at 4 GHz. The on-die interconnect mesh bus operates above
    4 GHz at a 2 mm distance and the inter-chiplet connection features a scalable,
    power-efficient, high-bandwidth interface achieving 8 Gb/s/pin and 320 GB/s bandwidth.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 双芯片封装的设计与实现见于（[lin_vlsi19](#bib.bib169)），其中每个芯片片段配备了四个工作频率为4 GHz的ARM Cortex-A72处理器。芯片内部互连网格总线在2毫米距离下的工作频率超过4
    GHz，而芯片片段间的连接则具备一个可扩展、节能、高带宽的接口，实现了8 Gb/s/引脚和320 GB/s的带宽。
- en: The above work uses 2.5D integration technology based on a passive interposer.
    In ([vivet_jssc21,](#bib.bib283) ), the authors observe that current passive interposer
    solutions still lack flexibility and efficiency when it comes to long-distance
    communication, smooth integration of chiplets with incompatible interfaces, and
    easy integration of less-scalable analog functions, such as power management and
    system input/output signals (IOs). Thus, they present a CMOS Active Interposer
    that integrates power management and distributed interconnects, enabling a scalable
    cache-coherent memory hierarchy. The proposed platform integrates six chiplets
    onto the active interposer, offering a total of 96 cores.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 上述工作使用了基于被动中介层的2.5D集成技术。在（[vivet_jssc21](#bib.bib283)）中，作者观察到当前的被动中介层解决方案在长距离通信、不兼容接口的芯片片段的平滑集成以及不易扩展的模拟功能（如电源管理和系统输入/输出信号(IOs)）的集成方面仍然缺乏灵活性和效率。因此，他们提出了一种CMOS主动中介层，集成了电源管理和分布式互连，实现了可扩展的缓存一致性内存层级。所提平台将六个芯片片段集成到主动中介层上，提供总共96个核心。
- en: The exploitation of active interposers as a way to address energy-efficiency
    and computing density issues in high-performance computing (HPC) for Exascale
    architectures is discussed in ([martinez_vlsi20,](#bib.bib179) ). The authors
    suggest that the integration of chiplets, active interposers, and FPGA can lead
    to dense, efficient, and modular computing nodes. They detail the ExaNoDe multi-chip-module
    which combines various components and demonstrate that multi-level integration
    allows for tight integration of hardware accelerators in a heterogeneous HPC compute
    node.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在高性能计算（HPC）中利用主动中介层作为解决能源效率和计算密度问题的方法在 ([martinez_vlsi20,](#bib.bib179) )中进行了讨论。作者建议集成芯片、小型中介层和FPGA可以形成紧凑、高效且模块化的计算节点。他们详细描述了ExaNoDe多芯片模块，展示了多级集成如何实现异构HPC计算节点中硬件加速器的紧密集成。
- en: Acknowledgements.
  id: totrans-455
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: This work has been supported by the Spoke 1 on *Future HPC* of the Italian Research
    Center on High-Performance Computing, Big Data and Quantum Computing (ICSC) funded
    by MUR Mission 4 - Next Generation EU.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了意大利高性能计算、大数据和量子计算研究中心（ICSC）Spoke 1 on *Future HPC*的资助，该中心由MUR Mission
    4 - Next Generation EU资助。
- en: References
  id: totrans-457
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '(1) Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
    G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,
    Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg,
    J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens,
    J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan,
    V., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and
    Zheng, X. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, 2015.
    Software available from tensorflow.org.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(1) Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
    G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,
    Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg,
    J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens,
    J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan,
    V., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., 和
    Zheng, X. TensorFlow: 在异构系统上的大规模机器学习，2015。软件可从tensorflow.org获取。'
- en: (2) Agrawal, A., Lee, S. K., Silberman, J., Ziegler, M., Kang, M., Venkataramani,
    S., Cao, N., Fleischer, B., Guillorn, M., Cohen, M., et al. 9.1 A 7nm 4-core AI
    chip with 25.6 TFLOPS hybrid FP8 training, 102.4 TOPS INT4 inference and workload-aware
    throttling. In 2021 IEEE International Solid-State Circuits Conference (ISSCC)
    (Feb. 2021), vol. 64, IEEE, pp. 144–146.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) Agrawal, A., Lee, S. K., Silberman, J., Ziegler, M., Kang, M., Venkataramani,
    S., Cao, N., Fleischer, B., Guillorn, M., Cohen, M., 等人。9.1 一款 7nm 4核 AI 芯片，具有
    25.6 TFLOPS 混合 FP8 训练、102.4 TOPS INT4 推理和工作负载感知的节流功能。见 2021 IEEE 国际固态电路会议（ISSCC）
    (2021年2月)，第64卷，IEEE，第144–146页。
- en: (3) Ahmadinejad, M., Moaiyeri, M. H., and Sabetzadeh, F. Energy and area efficient
    imprecise compressors for approximate multiplication at nanoscale. AEU - International
    Journal of Electronics and Communications 110 (2019), 152859.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) Ahmadinejad, M., Moaiyeri, M. H., 和 Sabetzadeh, F. 能量和面积高效的近似压缩器用于纳米尺度近似乘法。AEU
    - 国际电子与通信杂志 110 (2019), 152859。
- en: '(4) Aimar, A., Mostafa, H., Calabrese, E., Rios-Navarro, A., Tapiador-Morales,
    R., Lungu, I.-A., Milde, M. B., Corradi, F., Linares-Barranco, A., Liu, S.-C.,
    and Delbruck, T. NullHop: A Flexible Convolutional Neural Network Accelerator
    Based on Sparse Representations of Feature Maps. IEEE Transactions on Neural Networks
    and Learning Systems 30, 3 (2019), 644–656.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(4) Aimar, A., Mostafa, H., Calabrese, E., Rios-Navarro, A., Tapiador-Morales,
    R., Lungu, I.-A., Milde, M. B., Corradi, F., Linares-Barranco, A., Liu, S.-C.,
    和 Delbruck, T. NullHop: 基于特征图稀疏表示的灵活卷积神经网络加速器。IEEE 神经网络与学习系统汇刊 30, 3 (2019), 644–656。'
- en: '(5) Akopyan, F., Sawada, J., Cassidy, A., Alvarez-Icaza, R., Arthur, J., Merolla,
    P., Imam, N., Nakamura, Y., Datta, P., Nam, G.-J., Taba, B., Beakes, M., Brezzo,
    B., Kuang, J. B., Manohar, R., Risk, W. P., Jackson, B., and Modha, D. S. TrueNorth:
    Design and Tool Flow of a 65 mW 1 Million Neuron Programmable Neurosynaptic Chip.
    IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
    34, 10 (2015), 1537–1557.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(5) Akopyan, F., Sawada, J., Cassidy, A., Alvarez-Icaza, R., Arthur, J., Merolla,
    P., Imam, N., Nakamura, Y., Datta, P., Nam, G.-J., Taba, B., Beakes, M., Brezzo,
    B., Kuang, J. B., Manohar, R., Risk, W. P., Jackson, B., 和 Modha, D. S. TrueNorth:
    设计与工具流程的 65 mW 100 万神经元可编程神经突触芯片。IEEE 集成电路与系统计算机辅助设计汇刊 34, 10 (2015), 1537–1557。'
- en: '(6) Albericio, J., Judd, P., Hetherington, T., Aamodt, T., Jerger, N. E., and
    Moshovos, A. Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing.
    In Proceedings of the 43rd International Symposium on Computer Architecture (2016),
    ISCA’16.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) Albericio, J., Judd, P., Hetherington, T., Aamodt, T., Jerger, N. E., 和
    Moshovos, A. Cnvlutin：无效神经元的深度神经网络计算. 见于第43届国际计算机架构研讨会论文集 (2016), ISCA’16。
- en: (7) Alves, M. A. Z., Diener, M., Santos, P. C., and Carro, L. Large vector extensions
    inside the HMC. In 2016 Design, Automation & Test in Europe Conference & Exhibition
    (DATE) (2016), pp. 1249–1254.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7) Alves, M. A. Z., Diener, M., Santos, P. C., 和 Carro, L. HMC 内的大型向量扩展. 见于2016年设计、自动化与欧洲测试会议与展览会
    (DATE) (2016), 第1249–1254页。
- en: '(8) Ambs, P. Optical Computing: A 60-Year Adventure. Advances in Optical Technologies
    2010, 372652 (May 2010).'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8) Ambs, P. 光学计算：60年的冒险. 光学技术进展 2010, 372652 (2010年5月)。
- en: (9) AMD. AMD Instinct MI200 series accelerator, Jan 2021.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9) AMD. AMD Instinct MI200 系列加速器, 2021年1月。
- en: (10) AMD-Xilinx. VitisAI develop environment, 2023.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (10) AMD-Xilinx. VitisAI 开发环境, 2023年。
- en: (11) Amir, M. F., Ko, J. H., Na, T., Kim, D., and Mukhopadhyay, S. 3-D Stacked
    Image Sensor With Deep Neural Network Computation. IEEE Sensors Journal 18, 10
    (2018), 4187–4199.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (11) Amir, M. F., Ko, J. H., Na, T., Kim, D., 和 Mukhopadhyay, S. 3-D 堆叠图像传感器与深度神经网络计算.
    IEEE 传感器期刊 18, 10 (2018), 4187–4199。
- en: (12) Andersch, M., Palmer, G., Krashinsky, R., Stam, N., Mehta, V., Brito, G.,
    and Ramaswamy, S. NVIDIA Hopper Architecture In-Depth, Mar 2022.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (12) Andersch, M., Palmer, G., Krashinsky, R., Stam, N., Mehta, V., Brito, G.,
    和 Ramaswamy, S. NVIDIA Hopper 架构深入分析, 2022年3月。
- en: (13) Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A.,
    Shakeri, S., Taropa, E., Bailey, P., Chen, Z., Chu, E., Clark, J. H., Shafey,
    L. E., Huang, Y., Meier-Hellstern, K., Mishra, G., Moreira, E., Omernick, M.,
    Robinson, K., Ruder, S., Tay, Y., Xiao, K., Xu, Y., Zhang, Y., Abrego, G. H.,
    Ahn, J., Austin, J., Barham, P., Botha, J., Bradbury, J., Brahma, S., Brooks,
    K., Catasta, M., Cheng, Y., Cherry, C., Choquette-Choo, C. A., Chowdhery, A.,
    Crepy, C., Dave, S., Dehghani, M., Dev, S., Devlin, J., Díaz, M., Du, N., Dyer,
    E., Feinberg, V., Feng, F., Fienber, V., Freitag, M., Garcia, X., Gehrmann, S.,
    Gonzalez, L., Gur-Ari, G., Hand, S., Hashemi, H., Hou, L., Howland, J., Hu, A.,
    Hui, J., Hurwitz, J., Isard, M., Ittycheriah, A., Jagielski, M., Jia, W., Kenealy,
    K., Krikun, M., Kudugunta, S., Lan, C., Lee, K., Lee, B., Li, E., Li, M., Li,
    W., Li, Y., Li, J., Lim, H., Lin, H., Liu, Z., Liu, F., Maggioni, M., Mahendru,
    A., Maynez, J., Misra, V., Moussalem, M., Nado, Z., Nham, J., Ni, E., Nystrom,
    A., Parrish, A., Pellat, M., Polacek, M., Polozov, A., Pope, R., Qiao, S., Reif,
    E., Richter, B., Riley, P., Ros, A. C., Roy, A., Saeta, B., Samuel, R., Shelby,
    R., Slone, A., Smilkov, D., So, D. R., Sohn, D., Tokumine, S., Valter, D., Vasudevan,
    V., Vodrahalli, K., Wang, X., Wang, P., Wang, Z., Wang, T., Wieting, J., Wu, Y.,
    Xu, K., Xu, Y., Xue, L., Yin, P., Yu, J., Zhang, Q., Zheng, S., Zheng, C., Zhou,
    W., Zhou, D., Petrov, S., and Wu, Y. PaLM 2 Technical Report, May 2023.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (13) Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A.,
    Shakeri, S., Taropa, E., Bailey, P., Chen, Z., Chu, E., Clark, J. H., Shafey,
    L. E., Huang, Y., Meier-Hellstern, K., Mishra, G., Moreira, E., Omernick, M.,
    Robinson, K., Ruder, S., Tay, Y., Xiao, K., Xu, Y., Zhang, Y., Abrego, G. H.,
    Ahn, J., Austin, J., Barham, P., Botha, J., Bradbury, J., Brahma, S., Brooks,
    K., Catasta, M., Cheng, Y., Cherry, C., Choquette-Choo, C. A., Chowdhery, A.,
    Crepy, C., Dave, S., Dehghani, M., Dev, S., Devlin, J., Díaz, M., Du, N., Dyer,
    E., Feinberg, V., Feng, F., Fienber, V., Freitag, M., Garcia, X., Gehrmann, S.,
    Gonzalez, L., Gur-Ari, G., Hand, S., Hashemi, H., Hou, L., Howland, J., Hu, A.,
    Hui, J., Hurwitz, J., Isard, M., Ittycheriah, A., Jagielski, M., Jia, W., Kenealy,
    K., Krikun, M., Kudugunta, S., Lan, C., Lee, K., Lee, B., Li, E., Li, M., Li,
    W., Li, Y., Li, J., Lim, H., Lin, H., Liu, Z., Liu, F., Maggioni, M., Mahendru,
    A., Maynez, J., Misra, V., Moussalem, M., Nado, Z., Nham, J., Ni, E., Nystrom,
    A., Parrish, A., Pellat, M., Polacek, M., Polozov, A., Pope, R., Qiao, S., Reif,
    E., Richter, B., Riley, P., Ros, A. C., Roy, A., Saeta, B., Samuel, R., Shelby,
    R., Slone, A., Smilkov, D., So, D. R., Sohn, D., Tokumine, S., Valter, D., Vasudevan,
    V., Vodrahalli, K., Wang, X., Wang, P., Wang, Z., Wang, T., Wieting, J., Wu, Y.,
    Xu, K., Xu, Y., Xue, L., Yin, P., Yu, J., Zhang, Q., Zheng, S., Zheng, C., Zhou,
    W., Zhou, D., Petrov, S., 和 Wu, Y. PaLM 2 技术报告, 2023年5月。
- en: '(14) Arunkumar, A., Bolotin, E., Cho, B., Milic, U., Ebrahimi, E., Villa, O.,
    Jaleel, A., Wu, C.-J., and Nellans, D. MCM-GPU: Multi-chip-module GPUs for continued
    performance scalability. In 2017 ACM/IEEE 44th Annual International Symposium
    on Computer Architecture (ISCA) (2017), pp. 320–332.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (14) Arunkumar, A., Bolotin, E., Cho, B., Milic, U., Ebrahimi, E., Villa, O.,
    Jaleel, A., Wu, C.-J., 和 Nellans, D. MCM-GPU：用于持续性能扩展的多芯片模块 GPU. 见于2017年 ACM/IEEE
    第44届国际计算机架构年会 (ISCA) (2017), 第320–332页。
- en: '(15) Assir, I. A., Iskandarani, M. E., Sandid, H. R. A., and Saghir, M. A. R.
    Arrow: A RISC-V Vector Accelerator for Machine Learning Inference. CoRR abs/2107.07169,
    arXiv:2107.07169 (July 2021).'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (15) Assir, I. A., Iskandarani, M. E., Sandid, H. R. A., 和 Saghir, M. A. R.
    Arrow：一种用于机器学习推理的 RISC-V 向量加速器. CoRR abs/2107.07169, arXiv:2107.07169 (2021年7月)。
- en: (16) Bengio, Y. Learning Deep Architectures for AI. Foundations and Trends in
    Machine Learning 2, 1 (2009), 1–127.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) Bengio, Y. 深度架构学习的基础。机器学习基础与趋势 2, 1 (2009), 1–127。
- en: '(17) Bertaccini, L., Paulin, G., Fischer, T., Mach, S., and Benini, L. MiniFloat-NN
    and ExSdotp: An ISA Extension and a Modular Open Hardware Unit for Low-Precision
    Training on RISC-V Cores. In 2022 IEEE 29th Symposium on Computer Arithmetic (ARITH)
    (Sept. 2022), pp. 1–8.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(17) Bertaccini, L., Paulin, G., Fischer, T., Mach, S., and Benini, L. MiniFloat-NN
    和 ExSdotp: 用于 RISC-V 核心低精度训练的 ISA 扩展和模块化开放硬件单元。在2022年IEEE第29届计算机算术研讨会（ARITH）（2022年9月），第1–8页。'
- en: (18) Biamonte, J., Wittek, P., Pancotti, N., Rebentrost, P., Wiebe, N., and
    Lloyd, S. Quantum machine learning. Nature 549, 7671 (sep 2017), 195–202.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (18) Biamonte, J., Wittek, P., Pancotti, N., Rebentrost, P., Wiebe, N., and
    Lloyd, S. 量子机器学习。自然 549, 7671 (2017年9月), 195–202。
- en: (19) Blaiech, A. G., Khalifa, K. B., Valderrama, C., Fernandes, M. A., and Bedoui,
    M. H. A survey and taxonomy of FPGA-based deep learning accelerators. Journal
    of Systems Architecture 98 (2019), 331–345.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (19) Blaiech, A. G., Khalifa, K. B., Valderrama, C., Fernandes, M. A., and Bedoui,
    M. H. 基于 FPGA 的深度学习加速器的调查与分类。系统架构期刊 98 (2019), 331–345。
- en: '(20) Blott, M., Preußer, T. B., Fraser, N. J., Gambardella, G., O’brien, K.,
    Umuroglu, Y., et al. FINN-R: An end-to-end deep-learning framework for fast exploration
    of quantized neural networks. ACM Transactions on Reconfigurable Technology and
    Systems 11, 3 (2018), 1–23.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(20) Blott, M., Preußer, T. B., Fraser, N. J., Gambardella, G., O’brien, K.,
    Umuroglu, Y., 等。FINN-R: 一种端到端的深度学习框架，用于快速探索量化神经网络。ACM可重构技术与系统期刊 11, 3 (2018),
    1–23。'
- en: '(21) Bohm Agostini, N., Curzel, S., Zhang, J. J., Limaye, A., Tan, C., Amatya,
    V., Minutoli, M., Castellana, V. G., Manzano, J., Brooks, D., Wei, G.-Y., and
    Tumeo, A. Bridging Python to Silicon: The SODA Toolchain. IEEE Micro 42, 5 (2022),
    78–88.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (21) Bohm Agostini, N., Curzel, S., Zhang, J. J., Limaye, A., Tan, C., Amatya,
    V., Minutoli, M., Castellana, V. G., Manzano, J., Brooks, D., Wei, G.-Y., and
    Tumeo, A. 将 Python 与硅连接起来：SODA 工具链。IEEE 微型计算机 42, 5 (2022), 78–88。
- en: (22) Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx,
    S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E.,
    Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis,
    J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy,
    J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K.,
    Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J.,
    Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri,
    P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M.,
    Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec,
    J., Levent, I., Li, X. L., Li, X., Ma, T., Malik, A., Manning, C. D., Mirchandani,
    S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman,
    B., Nie, A., Niebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou,
    I., Park, J. S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich,
    R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa,
    S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A. W.,
    Tramèr, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M., Yasunaga,
    M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L.,
    Zhou, K., and Liang, P. On the Opportunities and Risks of Foundation Models, July
    2022.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (22) Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx,
    S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E.,
    Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis,
    J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy,
    J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K.,
    Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J.,
    Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri,
    P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M.,
    Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec,
    J., Levent, I., Li, X. L., Li, X., Ma, T., Malik, A., Manning, C. D., Mirchandani,
    S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman,
    B., Nie, A., Niebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou,
    I., Park, J. S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich,
    R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa,
    S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A.
    W., Tramèr, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M., Yasunaga,
    M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L.,
    Zhou, K., and Liang, P. 基础模型的机遇与风险, 2022年7月。
- en: (23) Quantum Computing Service - Amazon Braket - AWS, 2023.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) 量子计算服务 - 亚马逊 Braket - AWS, 2023。
- en: '(24) Broughton, M., Verdon, G., McCourt, T., Martinez, A. J., Yoo, J. H., Isakov,
    S. V., Massey, P., Halavati, R., Niu, M. Y., Zlokapa, A., Peters, E., Lockwood,
    O., Skolik, A., Jerbi, S., Dunjko, V., Leib, M., Streif, M., Dollen, D. V., Chen,
    H., Cao, S., Wiersema, R., Huang, H.-Y., McClean, J. R., Babbush, R., Boixo, S.,
    Bacon, D., Ho, A. K., Neven, H., and Mohseni, M. TensorFlow Quantum: A Software
    Framework for Quantum Machine Learning, 2021.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(24) Broughton, M., Verdon, G., McCourt, T., Martinez, A. J., Yoo, J. H., Isakov,
    S. V., Massey, P., Halavati, R., Niu, M. Y., Zlokapa, A., Peters, E., Lockwood,
    O., Skolik, A., Jerbi, S., Dunjko, V., Leib, M., Streif, M., Dollen, D. V., Chen,
    H., Cao, S., Wiersema, R., Huang, H.-Y., McClean, J. R., Babbush, R., Boixo, S.,
    Bacon, D., Ho, A. K., Neven, H., and Mohseni, M. TensorFlow Quantum: 一种用于量子机器学习的软件框架，2021。'
- en: (25) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J.,
    Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B.,
    Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei,
    D. Language Models are Few-Shot Learners. CoRR abs/2005.14165 (2020).
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (25) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J.,
    Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B.,
    Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei,
    D. 语言模型是少样本学习者。CoRR abs/2005.14165（2020）。
- en: (26) Bruschi, N., Tagliavini, G., Garofalo, A., Conti, F., Boybat, I., Benini,
    L., and Rossi, D. End-to-end DNN inference on a massively parallel analog in memory
    computing architecture. In Design, Automation & Test in Europe Conference & Exhibition,
    DATE (2023), IEEE, pp. 1–6.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (26) Bruschi, N., Tagliavini, G., Garofalo, A., Conti, F., Boybat, I., Benini,
    L., and Rossi, D. 在大规模并行模拟内存计算架构上的端到端DNN推理。在2023年欧洲设计、自动化与测试会议（DATE），IEEE，第1–6页。
- en: (27) Cadence. Stratus High-Level Synthesis, 2022.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (27) Cadence. Stratus 高级综合，2022。
- en: '(28) Canis, A., Choi, J., Aldham, M., Zhang, V., Kammoona, A., Czajkowski,
    T. S., Brown, S. D., and Anderson, J. H. LegUp: An open-source high-level synthesis
    tool for FPGA-based processor/accelerator systems. ACM Trans. Embed. Comput. Syst.
    13, 2 (2013), 24:1–24:27.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(28) Canis, A., Choi, J., Aldham, M., Zhang, V., Kammoona, A., Czajkowski,
    T. S., Brown, S. D., and Anderson, J. H. LegUp: 一款用于FPGA处理器/加速器系统的开源高级综合工具。ACM
    嵌入式计算系统学报 13, 2（2013），24:1–24:27。'
- en: '(29) Cao, W., Zhao, Y., Boloor, A., Han, Y., Zhang, X., and Jiang, L. Neural-PIM:
    Efficient Processing-In-Memory With Neural Approximation of Peripherals. IEEE
    Transactions on Computers 71, 9 (2022), 2142–2155.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(29) Cao, W., Zhao, Y., Boloor, A., Han, Y., Zhang, X., and Jiang, L. Neural-PIM:
    高效的内存处理，通过神经网络近似外围设备。IEEE 计算机学报 71, 9（2022），2142–2155。'
- en: '(30) Carsello, A., Feng, K., Kong, T., Koul, K., Liu, Q., Melchert, J., Nyengele,
    G., Strange, M., Zhang, K., Nayak, A., Setter, J., Thomas, J., Sreedhar, K., Chen,
    P.-H., Bhagdikar, N., Myers, Z., D’Agostino, B., Joshi, P., Richardson, S., Bahr,
    R., Torng, C., Horowitz, M., and Raina, P. Amber: A 367 GOPS, 538 GOPS/W 16nm
    SoC with a Coarse-Grained Reconfigurable Array for Flexible Acceleration of Dense
    Linear Algebra. In 2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology
    and Circuits) (2022), pp. 70–71.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(30) Carsello, A., Feng, K., Kong, T., Koul, K., Liu, Q., Melchert, J., Nyengele,
    G., Strange, M., Zhang, K., Nayak, A., Setter, J., Thomas, J., Sreedhar, K., Chen,
    P.-H., Bhagdikar, N., Myers, Z., D’Agostino, B., Joshi, P., Richardson, S., Bahr,
    R., Torng, C., Horowitz, M., and Raina, P. Amber: 一款367 GOPS, 538 GOPS/W 16nm
    SoC，具有用于灵活加速稠密线性代数的粗粒度可重构阵列。在2022年IEEE VLSI技术与电路研讨会（VLSI Technology and Circuits）上（2022），第70–71页。'
- en: '(31) Cassidy, A. S., Merolla, P., Arthur, J. V., Esser, S. K., Jackson, B.,
    Alvarez-Icaza, R., Datta, P., Sawada, J., Wong, T. M., Feldman, V., Amir, A.,
    Rubin, D. B.-D., Akopyan, F., McQuinn, E., Risk, W. P., and Modha, D. S. Cognitive
    computing building block: A versatile and efficient digital neuron model for neurosynaptic
    cores. In The 2013 International Joint Conference on Neural Networks (IJCNN) (2013),
    pp. 1–10.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (31) Cassidy, A. S., Merolla, P., Arthur, J. V., Esser, S. K., Jackson, B.,
    Alvarez-Icaza, R., Datta, P., Sawada, J., Wong, T. M., Feldman, V., Amir, A.,
    Rubin, D. B.-D., Akopyan, F., McQuinn, E., Risk, W. P., and Modha, D. S. 认知计算构建模块：用于神经突触核心的多功能高效数字神经元模型。在2013年国际神经网络联合会议（IJCNN）（2013），第1–10页。
- en: '(32) Cavalcante, M., Schuiki, F., Zaruba, F., Schaffner, M., and Benini, L.
    Ara: A 1-GHz+ Scalable and Energy-Efficient RISC-V Vector Processor With Multiprecision
    Floating-Point Support in 22-Nm FD-SOI. IEEE Transactions on Very Large Scale
    Integration (VLSI) Systems 28, 2 (Feb. 2020), 530–543.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(32) Cavalcante, M., Schuiki, F., Zaruba, F., Schaffner, M., and Benini, L.
    Ara: 一款1-GHz+ 可扩展且节能的RISC-V向量处理器，支持22-Nm FD-SOI中的多精度浮点运算。IEEE 超大规模集成（VLSI）系统学报
    28, 2（2020年2月），530–543。'
- en: '(33) Cavalcante, M., Wüthrich, D., Perotti, M., Riedel, S., and Benini, L.
    Spatz: A Compact Vector Processing Unit for High-Performance and Energy-Efficient
    Shared-L1 Clusters. In 41st IEEE/ACM International Conference on Computer-Aided
    Design (San Diego California, Oct. 2022), ACM, pp. 1–9.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(33) Cavalcante, M., Wüthrich, D., Perotti, M., Riedel, S., 和 Benini, L. Spatz:
    一种紧凑型向量处理单元，适用于高性能和节能共享 L1 集群。在第41届 IEEE/ACM 计算机辅助设计国际会议（加利福尼亚州圣地亚哥，2022年10月），ACM，第
    1–9 页。'
- en: '(34) Cavigelli, L., and Benini, L. Origami: A 803-GOp/s/W Convolutional Network
    Accelerator. IEEE Transactions on Circuits and Systems for Video Technology 27,
    11 (2017), 2461–2475.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(34) Cavigelli, L., 和 Benini, L. Origami: 一种 803-GOp/s/W 的卷积网络加速器。IEEE Transactions
    on Circuits and Systems for Video Technology 27, 11 (2017), 2461–2475。'
- en: (35) Chang, J.-W., Kang, K.-W., and Kang, S.-J. An Energy-Efficient FPGA-Based
    Deconvolutional Neural Networks Accelerator for Single Image Super-Resolution.
    IEEE Transactions on Circuits and Systems for Video Technology 30, 1 (2020), 281–295.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (35) Chang, J.-W., Kang, K.-W., 和 Kang, S.-J. 一种节能的基于 FPGA 的去卷积神经网络加速器，用于单幅图像超分辨率。IEEE
    Transactions on Circuits and Systems for Video Technology 30, 1 (2020), 281–295。
- en: '(36) Chatha, K. Qualcomm® Cloud Al 100 : 12TOPS/W Scalable, High Performance
    and Low Latency Deep Learning Inference Accelerator. In 2021 IEEE Hot Chips 33
    Symposium (HCS) (2021), pp. 1–19.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(36) Chatha, K. Qualcomm® Cloud AI 100: 12TOPS/W 可扩展的高性能低延迟深度学习推理加速器。在 2021
    IEEE Hot Chips 33 研讨会（HCS）（2021年），第 1–19 页。'
- en: '(37) Chen, C., Xiang, X., Liu, C., Shang, Y., Guo, R., Liu, D., Lu, Y., Hao,
    Z., Luo, J., Chen, Z., Li, C., Pu, Y., Meng, J., Yan, X., Xie, Y., and Qi, X.
    Xuantie-910: A Commercial Multi-Core 12-Stage Pipeline Out-of-Order 64-Bit High
    Performance RISC-V Processor with Vector Extension : Industrial Product. In 2020
    ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA) (May
    2020), pp. 52–64.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(37) Chen, C., Xiang, X., Liu, C., Shang, Y., Guo, R., Liu, D., Lu, Y., Hao,
    Z., Luo, J., Chen, Z., Li, C., Pu, Y., Meng, J., Yan, X., Xie, Y., 和 Qi, X. Xuantie-910:
    一款商业化的多核 12 阶段流水线乱序 64 位高性能 RISC-V 处理器，具有向量扩展：工业产品。在 2020 ACM/IEEE 第47届年度计算机架构国际研讨会（ISCA）（2020年5月），第
    52–64 页。'
- en: (38) Chen, G. K., Knag, P. C., Tokunaga, C., and Krishnamurthy, R. K. An Eight-Core
    RISC-V Processor With Compute Near Last Level Cache in Intel 4 CMOS. IEEE Journal
    of Solid-State Circuits (2022), 1–12.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (38) Chen, G. K., Knag, P. C., Tokunaga, C., 和 Krishnamurthy, R. K. 一种八核 RISC-V
    处理器，计算接近最后一级缓存，采用 Intel 4 CMOS。IEEE Journal of Solid-State Circuits (2022), 1–12。
- en: '(39) Chen, Y., Chen, T., Xu, Z., Sun, N., and Temam, O. DianNao Family: Energy-Efficient
    Hardware Accelerators for Machine Learning. Commun. ACM 59, 11 (Oct 2016), 105–112.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (39) Chen, Y., Chen, T., Xu, Z., Sun, N., 和 Temam, O. DianNao 家族：用于机器学习的节能硬件加速器。Commun.
    ACM 59, 11 (2016年10月), 105–112。
- en: (40) Chen, Y., Xie, Y., Song, L., Chen, F., and Tang, T. A Survey of Accelerator
    Architectures for Deep Neural Networks. Engineering 6, 3 (2020), 264–274.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (40) Chen, Y., Xie, Y., Song, L., Chen, F., 和 Tang, T. 深度神经网络加速器架构综述。Engineering
    6, 3 (2020), 264–274。
- en: '(41) Chen, Y.-H., Krishna, T., Emer, J. S., and Sze, V. Eyeriss: An Energy-Efficient
    Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE Journal
    of Solid-State Circuits 52, 1 (Jan. 2017), 127–138.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(41) Chen, Y.-H., Krishna, T., Emer, J. S., 和 Sze, V. Eyeriss: 一种节能的可重构加速器用于深度卷积神经网络。IEEE
    Journal of Solid-State Circuits 52, 1 (2017年1月), 127–138。'
- en: '(42) Chen, Y.-H., Yang, T.-J., Emer, J., and Sze, V. Eyeriss v2: A Flexible
    Accelerator for Emerging Deep Neural Networks on Mobile Devices. IEEE Journal
    on Emerging and Selected Topics in Circuits and Systems 9, 2 (2019), 292–308.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(42) Chen, Y.-H., Yang, T.-J., Emer, J., 和 Sze, V. Eyeriss v2: 一种灵活的加速器，适用于移动设备上的新兴深度神经网络。IEEE
    Journal on Emerging and Selected Topics in Circuits and Systems 9, 2 (2019), 292–308。'
- en: '(43) Cheng, C., and Parhi, K. Hardware efficient fast parallel FIR filter structures
    based on iterated short convolution. IEEE Transactions on Circuits and Systems
    I: Regular Papers 51, 8 (2004), 1492–1500.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(43) Cheng, C., 和 Parhi, K. 基于迭代短卷积的硬件高效快速并行 FIR 滤波器结构。IEEE Transactions on
    Circuits and Systems I: Regular Papers 51, 8 (2004), 1492–1500。'
- en: '(44) Cheng, C., and Parhi, K. K. Fast 2D Convolution Algorithms for Convolutional
    Neural Networks. IEEE Transactions on Circuits and Systems I: Regular Papers 67,
    5 (2020), 1678–1691.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(44) Cheng, C., 和 Parhi, K. K. 快速 2D 卷积算法用于卷积神经网络。IEEE Circuits and Systems
    I: Regular Papers 67, 5 (2020), 1678–1691。'
- en: '(45) Chi, P., Li, S., Xu, C., Zhang, T., Zhao, J., Liu, Y., Wang, Y., and Xie,
    Y. PRIME: A Novel Processing-in-Memory Architecture for Neural Network Computation
    in ReRAM-Based Main Memory. In 2016 ACM/IEEE 43rd Annual International Symposium
    on Computer Architecture (ISCA) (2016), pp. 27–39.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(45) Chi, P., Li, S., Xu, C., Zhang, T., Zhao, J., Liu, Y., Wang, Y., 和 Xie,
    Y. PRIME: 一种新型处理内存架构，用于 ReRAM 基主存中的神经网络计算。在 2016 ACM/IEEE 第43届年度计算机架构国际研讨会（ISCA）（2016年），第
    27–39 页。'
- en: '(46) Chitty-Venkata, K. T., and Somani, A. K. Neural Architecture Search Survey:
    A Hardware Perspective. ACM Comput. Surv. 55, 4 (nov 2022).'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (46) Chitty-Venkata, K. T., 和 Somani, A. K. 神经架构搜索调查：硬件视角。ACM Comput. Surv.
    55, 4 (2022年11月)。
- en: '(47) Choquette, J. NVIDIA Hopper H100 GPU: Scaling Performance. IEEE Micro
    (2023), 1–13.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (47) Choquette, J. NVIDIA Hopper H100 GPU：性能扩展。IEEE Micro (2023), 1–13。
- en: '(48) Choquette, J., Giroux, O., and Foley, D. Volta: Performance and Programmability.
    IEEE Micro 38, 2 (Mar. 2018), 42–52.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (48) Choquette, J., Giroux, O., 和 Foley, D. Volta：性能与可编程性。IEEE Micro 38, 2 (2018年3月),
    42–52。
- en: (49) Choquette, J., Lee, E., Krashinsky, R., Balan, V., and Khailany, B. 3.2
    The A100 Datacenter GPU and Ampere Architecture. In 2021 IEEE International Solid-
    State Circuits Conference (ISSCC) (Feb. 2021), vol. 64, pp. 48–50.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (49) Choquette, J., Lee, E., Krashinsky, R., Balan, V., 和 Khailany, B. 3.2 A100
    数据中心 GPU 和 Ampere 架构。在 2021 IEEE 国际固态电路会议（ISSCC） (2021年2月), 第64卷，第48–50页。
- en: '(50) Clarke, P. Intel, Micron Launch “Bulk-Switching” ReRAM. [https://www.eetimes.com/intel-micron-launch-bulk-switching-reram/](https://www.eetimes.com/intel-micron-launch-bulk-switching-reram/),
    Jul. 2015. Accessed: 18/04/2023.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (50) Clarke, P. 英特尔、美光推出“批量切换”ReRAM。 [https://www.eetimes.com/intel-micron-launch-bulk-switching-reram/](https://www.eetimes.com/intel-micron-launch-bulk-switching-reram/),
    2015年7月。访问日期：2023年4月18日。
- en: (51) Cococcioni, M., Rossi, F., Ruffaldi, E., and Saponara, S. A Lightweight
    Posit Processing Unit for RISC-V Processors in Deep Neural Network Applications.
    IEEE Transactions on Emerging Topics in Computing 10, 4 (2022), 1898–1908.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (51) Cococcioni, M., Rossi, F., Ruffaldi, E., 和 Saponara, S. 用于深度神经网络应用的 RISC-V
    处理器轻量级 Posit 处理单元。IEEE Transactions on Emerging Topics in Computing 10, 4 (2022),
    1898–1908。
- en: (52) Conference, A. . P. R. T., on Packaging, E., of Electronic, I., Photonic Systems,
    M., and 1, N. V., Eds. The Most Cost-Effective Integrator (TSV Interposer) for
    3D IC Integration System-in-Package (SiP) (07 2011), International Electronic
    Packaging Technical Conference and Exhibition.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (52) Conference, A. . P. R. T., 关于电子、光子系统的封装（E.）、集成电路（I.）、和 1, N. V., 主编。3D
    IC 集成系统封装内（SiP）的最具成本效益的集成器（TSV Interposer） (07 2011), 国际电子封装技术会议暨展览。
- en: (53) Conti, F., Rossi, D., Paulin, G., Garofalo, A., Di Mauro, A., Rutishauer,
    G., marco Ottavi, G., Eggimann, M., Okuhara, H., Huard, V., Montfort, O., Jure,
    L., Exibard, N., Gouedo, P., Louvat, M., Botte, E., and Benini, L. 22.1 A 12.4TOPS/W
    @ 136GOPS AI-IoT System-on-Chip with 16 RISC-V, 2-to-8b Precision-Scalable DNN
    Acceleration and 30%-Boost Adaptive Body Biasing. In 2023 IEEE International Solid-
    State Circuits Conference (ISSCC) (Feb. 2023), pp. 21–23.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (53) Conti, F., Rossi, D., Paulin, G., Garofalo, A., Di Mauro, A., Rutishauer,
    G., marco Ottavi, G., Eggimann, M., Okuhara, H., Huard, V., Montfort, O., Jure,
    L., Exibard, N., Gouedo, P., Louvat, M., Botte, E., 和 Benini, L. 22.1 一种 12.4TOPS/W
    @ 136GOPS AI-IoT 系统芯片，具有 16 个 RISC-V、2 到 8 位精度可扩展 DNN 加速和 30% 提升的自适应体偏置。在 2023
    IEEE 国际固态电路会议（ISSCC） (2023年2月), 第21–23页。
- en: (54) Cordeiro, A. S., Santos, S. R. d., Moreira, F. B., Santos, P. C., Carro,
    L., and Alves, M. A. Z. Machine Learning Migration for Efficient Near-Data Processing.
    In 2021 29th Euromicro International Conference on Parallel, Distributed and Network-Based
    Processing (PDP) (2021), pp. 212–219.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (54) Cordeiro, A. S., Santos, S. R. d., Moreira, F. B., Santos, P. C., Carro,
    L., 和 Alves, M. A. Z. 高效近数据处理的机器学习迁移。在 2021 第29届欧盟微处理器国际会议（PDP） (2021年), 第212–219页。
- en: (55) D-Wave Systems - The Practical Quantum Computing Company, 2023.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (55) D-Wave Systems - 实用量子计算公司，2023年。
- en: '(56) Davidson, S., Xie, S., Torng, C., Al-Hawai, K., Rovinski, A., Ajayi, T.,
    Vega, L., Zhao, C., Zhao, R., Dai, S., Amarnath, A., Veluri, B., Gao, P., Rao,
    A., Liu, G., Gupta, R. K., Zhang, Z., Dreslinski, R., Batten, C., and Taylor,
    M. B. The Celerity Open-Source 511-Core RISC-V Tiered Accelerator Fabric: Fast
    Architectures and Design Methodologies for Fast Chips. IEEE Micro 38, 2 (Mar.
    2018), 30–41.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (56) Davidson, S., Xie, S., Torng, C., Al-Hawai, K., Rovinski, A., Ajayi, T.,
    Vega, L., Zhao, C., Zhao, R., Dai, S., Amarnath, A., Veluri, B., Gao, P., Rao,
    A., Liu, G., Gupta, R. K., Zhang, Z., Dreslinski, R., Batten, C., 和 Taylor, M.
    B. Celerity 开源 511 核心 RISC-V 分级加速器结构：用于快速芯片的快速架构和设计方法。IEEE Micro 38, 2 (2018年3月),
    30–41。
- en: '(57) Davies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y., Choday, S. H.,
    Dimou, G., Joshi, P., Imam, N., Jain, S., Liao, Y., Lin, C.-K., Lines, A., Liu,
    R., Mathaikutty, D., McCoy, S., Paul, A., Tse, J., Venkataramanan, G., Weng, Y.-H.,
    Wild, A., Yang, Y., and Wang, H. Loihi: A Neuromorphic Manycore Processor with
    On-Chip Learning. IEEE Micro 38, 1 (2018), 82–99.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (57) Davies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y., Choday, S.
    H., Dimou, G., Joshi, P., Imam, N., Jain, S., Liao, Y., Lin, C.-K., Lines, A.,
    Liu, R., Mathaikutty, D., McCoy, S., Paul, A., Tse, J., Venkataramanan, G., Weng,
    Y.-H., Wild, A., Yang, Y., 和 Wang, H. Loihi：一种带有片上学习的神经形态多核处理器。IEEE Micro 38,
    1 (2018), 82–99。
- en: (58) Davis, T. Wilkinson’s Sparse Matrix Definition. NA Digest 7, 12 (2007),
    379–401.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (58) Davis, T. Wilkinson的稀疏矩阵定义。NA Digest 7, 12 (2007), 379–401。
- en: '(59) Delmas Lascorz, A., Judd, P., Stuart, D. M., Poulos, Z., Mahmoud, M.,
    Sharify, S., Nikolic, M., Siu, K., and Moshovos, A. Bit-Tactical: A Software/Hardware
    Approach to Exploiting Value and Bit Sparsity in Neural Networks. In Twenty-Fourth
    International Conference on Architectural Support for Programming Languages and
    Operating Systems (2019), ASPLOS ’19, pp. 749–763.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(59) Delmas Lascorz, A., Judd, P., Stuart, D. M., Poulos, Z., Mahmoud, M.,
    Sharify, S., Nikolic, M., Siu, K., 和 Moshovos, A. Bit-Tactical: 一种利用神经网络中的值和位稀疏性的软硬件方法。载于第二十四届编程语言与操作系统架构支持国际会议（2019），ASPLos
    ’19，第749–763页。'
- en: '(60) Deng, C., Liao, S., Xie, Y., Parhi, K. K., Qian, X., and Yuan, B. PermDNN:
    Efficient Compressed DNN Architecture with Permuted Diagonal Matrices. In 51st
    Annual IEEE/ACM International Symposium on Microarchitecture (2018), MICRO-51,
    pp. 189–202.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(60) Deng, C., Liao, S., Xie, Y., Parhi, K. K., Qian, X., 和 Yuan, B. PermDNN:
    一种具有置换对角矩阵的高效压缩DNN架构。载于第51届IEEE/ACM国际微架构研讨会（2018），MICRO-51，第189–202页。'
- en: (61) Desoli, G., Chawla, N., Boesch, T., Avodhyawasi, M., Rawat, H., Chawla,
    H., Abhijith, V., Zambotti, P., Sharma, A., Cappetta, C., Rossi, M., De Vita,
    A., and Girardi, F. A 40-310TOPS/W SRAM-Based All-Digital Up to 4b In-Memory Computing
    Multi-Tiled NN Accelerator in FD-SOI 18nm for Deep-Learning Edge Applications.
    In 2023 IEEE International Solid- State Circuits Conference (ISSCC) (2023), pp. 260–262.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (61) Desoli, G., Chawla, N., Boesch, T., Avodhyawasi, M., Rawat, H., Chawla,
    H., Abhijith, V., Zambotti, P., Sharma, A., Cappetta, C., Rossi, M., De Vita,
    A., 和 Girardi, F. 一款在FD-SOI 18nm工艺下用于深度学习边缘应用的40-310TOPS/W基于SRAM的全数字多核NN加速器。载于2023年IEEE国际固态电路会议（ISSCC）（2023），第260–262页。
- en: (62) Desoli, G., Chawla, N., Boesch, T., Singh, S.-p., Guidetti, E., De Ambroggi,
    F., Majo, T., Zambotti, P., Ayodhyawasi, M., Singh, H., and Aggarwal, N. A 2.9TOPS/W
    deep convolutional neural network SoC in FD-SOI 28nm for intelligent embedded
    systems. In 2017 IEEE International Solid-State Circuits Conference (ISSCC) (2017),
    pp. 238–239.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (62) Desoli, G., Chawla, N., Boesch, T., Singh, S.-p., Guidetti, E., De Ambroggi,
    F., Majo, T., Zambotti, P., Ayodhyawasi, M., Singh, H., 和 Aggarwal, N. 一款在FD-SOI
    28nm工艺下用于智能嵌入式系统的2.9TOPS/W深度卷积神经网络SoC。载于2017年IEEE国际固态电路会议（ISSCC）（2017），第238–239页。
- en: '(63) Dhilleswararao, P., Boppu, S., Manikandan, M. S., and Cenkeramaddi, L. R.
    Efficient Hardware Architectures for Accelerating Deep Neural Networks: Survey.
    IEEE Access 10 (2022), 131788–131828.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (63) Dhilleswararao, P., Boppu, S., Manikandan, M. S., 和 Cenkeramaddi, L. R.
    加速深度神经网络的高效硬件架构：综述。IEEE Access 10 (2022), 131788–131828。
- en: '(64) Di Mauro, A., Scherer, M., Rossi, D., and Benini, L. Kraken: A Direct
    Event/Frame-Based Multi-sensor Fusion SoC for Ultra-Efficient Visual Processing
    in Nano-UAVs, Aug. 2022.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(64) Di Mauro, A., Scherer, M., Rossi, D., 和 Benini, L. Kraken: 一款用于超高效视觉处理的直接事件/帧多传感器融合SoC，用于Nano-UAVs，2022年8月。'
- en: (65) Ditzel, D. R., and team, t. E. Accelerating ML Recommendation With Over
    1,000 RISC-V/Tensor Processors on Esperanto’s ET-SoC-1 Chip. IEEE Micro 42, 3
    (May 2022), 31–38.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (65) Ditzel, D. R., 和团队 t. E. 通过Esperanto的ET-SoC-1芯片加速超过1000个RISC-V/Tensor处理器的机器学习推荐。IEEE
    Micro 42, 3 (2022年5月), 31–38。
- en: '(66) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
    Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit,
    J., and Houlsby, N. An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale, June 2021.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (66) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
    Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit,
    J., 和 Houlsby, N. 一张图片胜过16x16个单词：大规模图像识别的变换器，2021年6月。
- en: '(67) Du, L., Du, Y., Li, Y., and Chang, M.-C. F. A Reconfigurable Streaming
    Deep Convolutional Neural Network Accelerator for Internet of Things. IEEE Transactions
    on Circuits and Systems I: Regular Papers PP (07 2017).'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(67) Du, L., Du, Y., Li, Y., 和 Chang, M.-C. F. 一款面向物联网的可重构流式深度卷积神经网络加速器。IEEE电路与系统学报
    I: 常规论文 PP (2017年7月)。'
- en: (68) Duarte, J., et al. Fast inference of deep neural networks in FPGAs for
    particle physics. JINST 13, 07 (2018), P07027.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (68) Duarte, J., 等. 在FPGA上快速推理深度神经网络用于粒子物理学。JINST 13, 07 (2018), P07027。
- en: '(69) Durant, L., Giroux, O., Harris, M., and Stam, N. Inside Volta: The World’s
    Most Advanced Data Center GPU, May 2017.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (69) Durant, L., Giroux, O., Harris, M., 和 Stam, N. 探索Volta：全球最先进的数据中心GPU，2017年5月。
- en: (70) Elster, A. C., and Haugdahl, T. A. Nvidia Hopper GPU and Grace CPU Highlights.
    Computing in Science & Engineering 24, 2 (Mar. 2022), 95–100.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (70) Elster, A. C., 和 Haugdahl, T. A. Nvidia Hopper GPU 和 Grace CPU 亮点。计算机科学与工程
    24, 2 (2022年3月), 95–100。
- en: (71) Esmaeilzadeh, H., Blem, E., Amant, R. S., Sankaralingam, K., and Burger,
    D. Dark silicon and the end of multicore scaling. In 2011 38th Annual International
    Symposium on Computer Architecture (ISCA) (2011), pp. 365–376.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (71) Esmaeilzadeh, H., Blem, E., Amant, R. S., Sankaralingam, K., 和 Burger,
    D. 黑暗硅和多核扩展的终结。在2011年第38届国际计算机体系结构年会（ISCA）（2011年），第365–376页。
- en: (72) Esposito, D., Strollo, A. G. M., and Alioto, M. Low-power approximate MAC
    unit. In 2017 13th Conference on Ph.D. Research in Microelectronics and Electronics
    (PRIME) (2017), pp. 81–84.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (72) Esposito, D., Strollo, A. G. M., 和 Alioto, M. 低功耗近似MAC单元。在2017年第13届博士研究微电子与电子会议（PRIME）（2017年），第81–84页。
- en: '(73) Fedus, W., Zoph, B., and Shazeer, N. Switch Transformers: Scaling to Trillion
    Parameter Models with Simple and Efficient Sparsity. Journal of Machine Learning
    Research 23, 120 (2022), 1–39.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (73) Fedus, W., Zoph, B., 和 Shazeer, N. Switch Transformers：通过简单高效的稀疏性扩展至万亿参数模型。机器学习研究期刊
    23，第120期（2022年），第1–39页。
- en: '(74) Ferrandi, F., Castellana, V. G., Curzel, S., Fezzardi, P., Fiorito, M.,
    Lattuada, M., Minutoli, M., Pilato, C., and Tumeo, A. Invited: Bambu: an Open-Source
    Research Framework for the High-Level Synthesis of Complex Applications. In 2021
    58th ACM/IEEE Design Automation Conference (DAC) (2021), pp. 1327–1330.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (74) Ferrandi, F., Castellana, V. G., Curzel, S., Fezzardi, P., Fiorito, M.,
    Lattuada, M., Minutoli, M., Pilato, C., 和 Tumeo, A. 邀请：Bambu：一个开源研究框架，用于复杂应用的高级综合。在2021年第58届ACM/IEEE设计自动化会议（DAC）（2021年），第1327–1330页。
- en: (75) Filippone, S., Cardellini, V., Barbieri, D., and Fanfarillo, A. Sparse
    Matrix-Vector Multiplication on GPGPUs. ACM Trans. Math. Softw. 43, 4 (2017).
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (75) Filippone, S., Cardellini, V., Barbieri, D., 和 Fanfarillo, A. 在GPGPUs上的稀疏矩阵-向量乘法。ACM数学软件汇刊
    43，第4期（2017年）。
- en: (76) Fowers, J., Ovtcharov, K., Papamichael, M., Massengill, T., Liu, M., Lo,
    D., Alkalay, S., Haselman, M., Adams, L., Ghandi, M., Heil, S., Patel, P., Sapek,
    A., Weisz, G., Woods, L., Lanka, S., Reinhardt, S. K., Caulfield, A. M., Chung,
    E. S., and Burger, D. A Configurable Cloud-Scale DNN Processor for Real-Time AI.
    In ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)
    (2018), pp. 1–14.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (76) Fowers, J., Ovtcharov, K., Papamichael, M., Massengill, T., Liu, M., Lo,
    D., Alkalay, S., Haselman, M., Adams, L., Ghandi, M., Heil, S., Patel, P., Sapek,
    A., Weisz, G., Woods, L., Lanka, S., Reinhardt, S. K., Caulfield, A. M., Chung,
    E. S., 和 Burger, D. 一个可配置的云规模DNN处理器，用于实时AI。在ACM/IEEE第45届国际计算机体系结构年会（ISCA）（2018年），第1–14页。
- en: '(77) Frenkel, C., Bol, D., and Indiveri, G. Bottom-Up and Top-Down Neural Processing
    Systems Design: Neuromorphic Intelligence as the Convergence of Natural and Artificial
    Intelligence. CoRR abs/2106.01288 (2021).'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (77) Frenkel, C., Bol, D., 和 Indiveri, G. 自下而上与自上而下的神经处理系统设计：神经形态智能作为自然与人工智能的融合。CoRR
    abs/2106.01288（2021年）。
- en: (78) Frenkel, C., Lefebvre, M., Legat, J.-D., and Bol, D. A 0.086-mm² 12.7-pJ/SOP
    64k-Synapse 256-Neuron Online-Learning Digital Spiking Neuromorphic Processor
    in 28-nm CMOS. IEEE Transactions on Biomedical Circuits and Systems 13, 1 (2019),
    145–158.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (78) Frenkel, C., Lefebvre, M., Legat, J.-D., 和 Bol, D. 一个0.086-mm² 12.7-pJ/SOP
    64k-突触 256-神经元 在线学习数字脉冲神经形态处理器，采用28-nm CMOS技术。IEEE生物医学电路与系统汇刊13，第1期（2019年），第145–158页。
- en: '(79) Frenkel, C., Legat, J.-D., and Bol, D. MorphIC: A 65-nm 738k-Synapse/mm²
    Quad-Core Binary-Weight Digital Neuromorphic Processor With Stochastic Spike-Driven
    Online Learning. IEEE Transactions on Biomedical Circuits and Systems 13, 5 (2019),
    999–1010.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (79) Frenkel, C., Legat, J.-D., 和 Bol, D. MorphIC：一个65-nm 738k-突触/mm² 四核二进制权重数字神经形态处理器，具有随机脉冲驱动的在线学习。IEEE生物医学电路与系统汇刊
    13，第5期（2019年），第999–1010页。
- en: '(80) Frustaci, F., Perri, S., Corsonello, P., and Alioto, M. Approximate multipliers
    with dynamic truncation for energy reduction via graceful quality degradation.
    IEEE Transactions on Circuits and Systems II: Express Briefs 67, 12 (2020), 3427–3431.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (80) Frustaci, F., Perri, S., Corsonello, P., 和 Alioto, M. 通过优雅的质量退化来减少能量的近似乘法器和动态截断。IEEE电路与系统II：简报
    67，第12期（2020年），第3427–3431页。
- en: (81) Gallo, M. L., Khaddam-Aljameh, R., Stanisavljevic, M., Vasilopoulos, A.,
    Kersting, B., Dazzi, M., Karunaratne, G., Braendli, M., Singh, A., Mueller, S. M.,
    et al. A 64-core mixed-signal in-memory compute chip based on phase-change memory
    for deep neural network inference. arXiv preprint arXiv:2212.02872 (2022).
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (81) Gallo, M. L., Khaddam-Aljameh, R., Stanisavljevic, M., Vasilopoulos, A.,
    Kersting, B., Dazzi, M., Karunaratne, G., Braendli, M., Singh, A., Mueller, S.
    M., 等。基于相变存储器的64核混合信号内存计算芯片，用于深度神经网络推理。arXiv预印本 arXiv:2212.02872（2022年）。
- en: (82) Gao, J., Ji, W., Chang, F., Han, S., Wei, B., Liu, Z., and Wang, Y. A Systematic
    Survey of General Sparse Matrix-Matrix Multiplication. ACM Comput. Surv. 55, 12
    (2023).
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (82) Gao, J., Ji, W., Chang, F., Han, S., Wei, B., Liu, Z., 和 Wang, Y. 关于一般稀疏矩阵-矩阵乘法的系统性调查。ACM计算机调查
    55，第12期（2023年）。
- en: '(83) Gao, M., Pu, J., Yang, X., Horowitz, M., and Kozyrakis, C. TETRIS: Scalable
    and Efficient Neural Network Acceleration with 3D Memory. SIGARCH Comput. Archit.
    News 45, 1 (2017), 751–764.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(83) Gao, M., Pu, J., Yang, X., Horowitz, M., 和 Kozyrakis, C. TETRIS: 具有3D内存的可扩展和高效的神经网络加速。《SIGARCH计算机架构新闻》45卷，第1期（2017年），第751–764页。'
- en: '(84) Gao, M., Yang, X., Pu, J., Horowitz, M., and Kozyrakis, C. TANGRAM: Optimized
    Coarse-Grained Dataflow for Scalable NN Accelerators. In Twenty-Fourth International
    Conference on Architectural Support for Programming Languages and Operating Systems
    (New York, NY, USA, 2019), ASPLOS ’19, Association for Computing Machinery, p. 807–820.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(84) Gao, M., Yang, X., Pu, J., Horowitz, M., 和 Kozyrakis, C. TANGRAM: 为可扩展的神经网络加速器优化的粗粒度数据流。发表于第二十四届程序语言与操作系统架构支持国际会议（纽约，纽约，美国，2019年），ASPLOS
    ’19, 计算机协会，第807–820页。'
- en: (85) Garofalo, A., Ottavi, G., Conti, F., Karunaratne, G., Boybat, I., Benini,
    L., and Rossi, D. A Heterogeneous In-Memory Computing Cluster for Flexible End-to-End
    Inference of Real-World Deep Neural Networks. IEEE Journal on Emerging and Selected
    Topics in Circuits and Systems 12, 2 (June 2022), 422–435.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (85) Garofalo, A., Ottavi, G., Conti, F., Karunaratne, G., Boybat, I., Benini,
    L., 和 Rossi, D. 一个用于灵活端到端推理的异构内存计算集群。 《IEEE Journal on Emerging and Selected Topics
    in Circuits and Systems》12卷，第2期（2022年6月），第422–435页。
- en: '(86) Garofalo, A., Tortorella, Y., Perotti, M., Valente, L., Nadalini, A.,
    Benini, L., Rossi, D., and Conti, F. DARKSIDE: A Heterogeneous RISC-V Compute
    Cluster for Extreme-Edge On-Chip DNN Inference and Training. IEEE Open Journal
    of the Solid-State Circuits Society 2 (2022), 231–243.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(86) Garofalo, A., Tortorella, Y., Perotti, M., Valente, L., Nadalini, A.,
    Benini, L., Rossi, D., 和 Conti, F. DARKSIDE: 一个用于极端边缘芯片 DNN 推理和训练的异构 RISC-V 计算集群。《IEEE
    Open Journal of the Solid-State Circuits Society》2卷（2022年），第231–243页。'
- en: '(87) Genc, H., Kim, S., Amid, A., Haj-Ali, A., Iyer, V., Prakash, P., Zhao,
    J., Grubb, D., Liew, H., Mao, H., Ou, A., Schmidt, C., Steffl, S., Wright, J.,
    Stoica, I., Ragan-Kelley, J., Asanovic, K., Nikolic, B., and Shao, Y. S. Gemmini:
    Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration.
    In 2021 58th ACM/IEEE Design Automation Conference (DAC) (Dec. 2021), pp. 769–774.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(87) Genc, H., Kim, S., Amid, A., Haj-Ali, A., Iyer, V., Prakash, P., Zhao,
    J., Grubb, D., Liew, H., Mao, H., Ou, A., Schmidt, C., Steffl, S., Wright, J.,
    Stoica, I., Ragan-Kelley, J., Asanovic, K., Nikolic, B., 和 Shao, Y. S. Gemmini:
    通过全栈集成实现系统化的深度学习架构评估。发表于2021年第58届ACM/IEEE设计自动化会议（DAC）（2021年12月），第769–774页。'
- en: (88) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., and Keutzer, K.
    A Survey of Quantization Methods for Efficient Neural Network Inference, 2021.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (88) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., 和 Keutzer, K.
    关于高效神经网络推理的量化方法调查，2021年。
- en: '(89) Giri, D., Chiu, K.-L., Di Guglielmo, G., Mantovani, P., and Carloni, L. P.
    ESP4ML: Platform-Based Design of Systems-on-Chip for Embedded Machine Learning.
    In 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE) (Mar.
    2020), pp. 1049–1054.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(89) Giri, D., Chiu, K.-L., Di Guglielmo, G., Mantovani, P., 和 Carloni, L.
    P. ESP4ML: 基于平台的系统级芯片设计用于嵌入式机器学习。发表于2020年设计、自动化与测试欧洲会议暨展览（DATE）（2020年3月），第1049–1054页。'
- en: (90) Giri, D., Chiu, K.-L., Eichler, G., Mantovani, P., and Carloni, L. P. Accelerator
    Integration for Open-Source SoC Design. IEEE Micro 41, 4 (July 2021), 8–14.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (90) Giri, D., Chiu, K.-L., Eichler, G., Mantovani, P., 和 Carloni, L. P. 加速器集成用于开源
    SoC 设计。《IEEE Micro》41卷，第4期（2021年7月），第8–14页。
- en: '(91) Gondimalla, A., Chesnut, N., Thottethodi, M., and Vijaykumar, T. N. SparTen:
    A Sparse Tensor Accelerator for Convolutional Neural Networks. In 52nd Annual
    IEEE/ACM International Symposium on Microarchitecture (2019), MICRO’52, pp. 151–165.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(91) Gondimalla, A., Chesnut, N., Thottethodi, M., 和 Vijaykumar, T. N. SparTen:
    一个用于卷积神经网络的稀疏张量加速器。发表于第52届年度IEEE/ACM国际微架构研讨会（2019年），MICRO’52，第151–165页。'
- en: (92) Gonzalez, A., Zhao, J., Korpan, B., Genc, H., Schmidt, C., Wright, J.,
    Biswas, A., Amid, A., Sheikh, F., Sorokin, A., Kale, S., Yalamanchi, M., Yarlagadda,
    R., Flannigan, M., Abramowitz, L., Alon, E., Shao, Y. S., Asanovic, K., and Nikolic,
    B. A 16mm ² 106.1 GOPS/W Heterogeneous RISC-V Multi-Core Multi-Accelerator SoC
    in Low-Power 22nm FinFET. In ESSCIRC 2021 - IEEE 47th European Solid State Circuits
    Conference (ESSCIRC) (Grenoble, France, Sept. 2021), IEEE, pp. 259–262.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (92) Gonzalez, A., Zhao, J., Korpan, B., Genc, H., Schmidt, C., Wright, J.,
    Biswas, A., Amid, A., Sheikh, F., Sorokin, A., Kale, S., Yalamanchi, M., Yarlagadda,
    R., Flannigan, M., Abramowitz, L., Alon, E., Shao, Y. S., Asanovic, K., 和 Nikolic,
    B. 一个16mm² 106.1 GOPS/W的异构RISC-V多核多加速器 SoC，采用低功耗22nm FinFET技术。发表于2021年ESSCIRC
    - IEEE第47届欧洲固态电路会议（ESSCIRC）（法国格勒诺布尔，2021年9月），IEEE，第259–262页。
- en: (93) Goodfellow, I., Bengio, Y., and Courville, A. Deep Learning. MIT Press,
    2016. [http://www.deeplearningbook.org](http://www.deeplearningbook.org).
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (93) Goodfellow, I., Bengio, Y., 和 Courville, A. 《深度学习》。麻省理工学院出版社，2016年。 [http://www.deeplearningbook.org](http://www.deeplearningbook.org)。
- en: '(94) GreenWaves Technologies GAP9 Processor. [https://greenwaves-technologies.com/gap9_processor/](https://greenwaves-technologies.com/gap9_processor/),
    2023. Accessed: 2023-04-18.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (94) GreenWaves Technologies GAP9处理器。 [https://greenwaves-technologies.com/gap9_processor/](https://greenwaves-technologies.com/gap9_processor/)，2023。访问日期：2023-04-18。
- en: '(95) Gu, P., Xie, X., Ding, Y., Chen, G., Zhang, W., Niu, D., and Xie, Y. iPIM:
    Programmable In-Memory Image Processing Accelerator Using Near-Bank Architecture.
    In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture
    (ISCA) (2020), pp. 804–817.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (95) Gu, P., Xie, X., Ding, Y., Chen, G., Zhang, W., Niu, D., 和 Xie, Y. iPIM：使用近银行架构的可编程内存图像处理加速器。在2020
    ACM/IEEE第47届年度国际计算机架构研讨会（ISCA）（2020），第804–817页。
- en: '(96) Guan, Y., Liang, H., Xu, N., Wang, W., Shi, S., Chen, X., Sun, G., Zhang,
    W., and Cong, J. FP-DNN: An Automated Framework for Mapping Deep Neural Networks
    onto FPGAs with RTL-HLS Hybrid Templates. In 2017 IEEE 25th Annual International
    Symposium on Field-Programmable Custom Computing Machines (FCCM) (2017), pp. 152–159.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (96) Guan, Y., Liang, H., Xu, N., Wang, W., Shi, S., Chen, X., Sun, G., Zhang,
    W., 和 Cong, J. FP-DNN：一种将深度神经网络映射到FPGA的自动化框架，使用RTL-HLS混合模板。2017 IEEE第25届年度国际现场可编程定制计算机会议（FCCM）（2017），第152–159页。
- en: (97) Guo, C., Hsueh, B. Y., Leng, J., Qiu, Y., Guan, Y., Wang, Z., Jia, X.,
    Li, X., Guo, M., and Zhu, Y. Accelerating Sparse DNN Models without Hardware-Support
    via Tile-Wise Sparsity. In International Conference for High Performance Computing,
    Networking, Storage and Analysis (2020), SC’20.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (97) Guo, C., Hsueh, B. Y., Leng, J., Qiu, Y., Guan, Y., Wang, Z., Jia, X.,
    Li, X., Guo, M., 和 Zhu, Y. 通过平铺稀疏性加速无硬件支持的稀疏DNN模型。国际高性能计算、网络、存储与分析会议（2020），SC’20。
- en: (98) Guo, K., Zeng, S., Yu, J., Wang, Y., and Yang, H. [DL] A survey of FPGA-based
    neural network inference accelerators. ACM Transactions on Reconfigurable Technology
    and Systems (TRETS) 12, 1 (2019), 1–26.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (98) Guo, K., Zeng, S., Yu, J., Wang, Y., 和 Yang, H. [DL] 基于FPGA的神经网络推理加速器综述。ACM可重构技术与系统（TRETS）12,
    1 (2019), 1–26。
- en: (99) Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. Deep Learning
    with Limited Numerical Precision. In 32nd International Conference on Machine
    Learning (07-09 Jul 2015), vol. 37 of Proceedings of Machine Learning Research,
    pp. 1737–1746.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (99) Gupta, S., Agrawal, A., Gopalakrishnan, K., 和 Narayanan, P. 限定数值精度的深度学习。第32届国际机器学习会议（07-09
    Jul 2015），第37卷，机器学习研究会议记录，第1737–1746页。
- en: (100) Gyongyosi, L., and Imre, S. A Survey on quantum computing technology.
    Computer Science Review 31 (2019), 51–71.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (100) Gyongyosi, L., 和 Imre, S. 量子计算技术综述。计算机科学评论 31 (2019), 51–71。
- en: (101) Ha, M., and Lee, S. Multipliers With Approximate 4–2 Compressors and Error
    Recovery Modules. IEEE Embedded Systems Letters 10, 1 (2018), 6–9.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (101) Ha, M. 和 Lee, S. 使用近似4–2压缩器和误差恢复模块的乘法器。IEEE嵌入式系统快报 10, 1 (2018), 6–9。
- en: (102) Hamanaka, F., Odan, T., Kise, K., and Chu, T. V. An Exploration of State-of-the-Art
    Automation Frameworks for FPGA-Based DNN Acceleration. IEEE Access 11 (2023),
    5701–5713.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (102) Hamanaka, F., Odan, T., Kise, K., 和 Chu, T. V. 对基于FPGA的DNN加速的最先进自动化框架的探索。IEEE
    Access 11 (2023), 5701–5713。
- en: '(103) Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., and Dally,
    W. J. EIE: Efficient Inference Engine on Compressed Deep Neural Network. In 43rd
    International Symposium on Computer Architecture (2016), ISCA’16, p. 243–254.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (103) Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., 和 Dally,
    W. J. EIE：压缩深度神经网络上的高效推理引擎。第43届计算机架构国际研讨会（2016），ISCA’16，第243–254页。
- en: '(104) Han, S., Mao, H., and Dally, W. J. Deep Compression: Compressing Deep
    Neural Networks with Pruning, Trained Quantization and Huffman Coding. CoRR abs/1510.00149
    (2015).'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (104) Han, S., Mao, H., 和 Dally, W. J. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。CoRR abs/1510.00149
    (2015)。
- en: (105) Han, S., Pool, J., Tran, J., and Dally, W. Learning both Weights and Connections
    for Efficient Neural Network. In Advances in Neural Information Processing Systems
    (2015), vol. 28, Curran Associates, Inc.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (105) Han, S., Pool, J., Tran, J., 和 Dally, W. 学习权重和连接以提高神经网络的效率。神经信息处理系统进展（2015），第28卷，Curran
    Associates, Inc.
- en: '(106) Harris, M. Inside Pascal: NVIDIA’s Newest Computing Platform, Apr. 2016.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (106) Harris, M. Pascal内部：NVIDIA最新的计算平台，2016年4月。
- en: '(107) Hashemi, S., Bahar, R. I., and Reda, S. DRUM: A Dynamic Range Unbiased
    Multiplier for approximate applications. In 2015 IEEE/ACM International Conference
    on Computer-Aided Design (ICCAD) (2015), pp. 418–425.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (107) Hashemi, S., Bahar, R. I., 和 Reda, S. DRUM：一种用于近似应用的动态范围无偏乘法器。2015 IEEE/ACM国际计算机辅助设计会议（ICCAD）（2015），第418–425页。
- en: (108) Hassanpour, M., Riera, M., and González, A. A Survey of Near-Data Processing
    Architectures for Neural Networks. Machine Learning and Knowledge Extraction 4,
    1 (2022), 66–102.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (108) Hassanpour, M., Riera, M., 和 González, A. 神经网络近数据处理架构综述。机器学习与知识提取 4, 1（2022），第66–102页。
- en: '(109) He, M., Song, C., Kim, I., Jeong, C., Kim, S., Park, I., Thottethodi,
    M., and Vijaykumar, T. N. Newton: A DRAM-maker’s Accelerator-in-Memory (AiM) Architecture
    for Machine Learning. In 2020 53rd Annual IEEE/ACM International Symposium on
    Microarchitecture (MICRO) (2020), pp. 372–385.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(109) He, M., Song, C., Kim, I., Jeong, C., Kim, S., Park, I., Thottethodi,
    M., 和 Vijaykumar, T. N. Newton: 一种 DRAM 制造商的内存加速器（AiM）架构，用于机器学习。在 2020 年第 53 届
    IEEE/ACM 国际微架构研讨会（MICRO）（2020），第372–385页。'
- en: '(110) Hegde, K., Yu, J., Agrawal, R., Yan, M., Pellauer, M., and Fletcher,
    C. W. UCNN: Exploiting Computational Reuse in Deep Neural Networks via Weight
    Repetition. In 45th Annual International Symposium on Computer Architecture (2018),
    ISCA’18, p. 674–687.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(110) Hegde, K., Yu, J., Agrawal, R., Yan, M., Pellauer, M., 和 Fletcher, C.
    W. UCNN: 通过权重重复在深度神经网络中利用计算重用。在第 45 届国际计算机架构年会（2018年），ISCA’18，第674–687页。'
- en: '(111) Heyman, K. DRAM Thermal Issues Reach Crisis Point. [https://semiengineering.com/dram-thermal-issues-reach-crisis-point/](https://semiengineering.com/dram-thermal-issues-reach-crisis-point/),
    Jun. 2022. Accessed: 18/04/2023.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (111) Heyman, K. DRAM 热问题达到危机点。 [https://semiengineering.com/dram-thermal-issues-reach-crisis-point/](https://semiengineering.com/dram-thermal-issues-reach-crisis-point/)，2022年6月。访问日期：2023年4月18日。
- en: '(112) Houshmand, P., Sarda, G. M., Jain, V., Ueyoshi, K., Papistas, I. A.,
    Shi, M., Zheng, Q., Bhattacharjee, D., Mallik, A., Debacker, P., Verkest, D.,
    and Verhelst, M. DIANA: An End-to-End Hybrid DIgital and ANAlog Neural Network
    SoC for the Edge. IEEE Journal of Solid-State Circuits 58, 1 (Jan. 2023), 203–215.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(112) Houshmand, P., Sarda, G. M., Jain, V., Ueyoshi, K., Papistas, I. A.,
    Shi, M., Zheng, Q., Bhattacharjee, D., Mallik, A., Debacker, P., Verkest, D.,
    和 Verhelst, M. DIANA: 一种端到端混合数字与模拟神经网络 SoC，用于边缘计算。IEEE 固态电路期刊 58, 1（2023年1月），第203–215页。'
- en: (113) Quantum computers emerging as accelerators in HPC, 2022.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (113) 量子计算机成为高性能计算的加速器，2022年。
- en: (114) Hu, X., Stow, D., and Xie, Y. Die Stacking Is Happening. IEEE Micro 38,
    1 (2018), 22–28.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (114) Hu, X., Stow, D., 和 Xie, Y. 芯片堆叠正在发生。IEEE Micro 38, 1（2018），第22–28页。
- en: (115) Hung, J.-M., Jhang, C.-J., Wu, P.-C., Chiu, Y.-C., and Chang, M.-F. Challenges
    and Trends of Nonvolatile In-Memory-Computation Circuits for AI Edge Devices.
    IEEE Open Journal of the Solid-State Circuits Society 1 (2021), 171–183.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (115) Hung, J.-M., Jhang, C.-J., Wu, P.-C., Chiu, Y.-C., 和 Chang, M.-F. AI 边缘设备非易失性内存计算电路的挑战与趋势。IEEE
    固态电路学会开放期刊 1（2021），第171–183页。
- en: '(116) Hwang, R., Kim, T., Kwon, Y., and Rhu, M. Centaur: A Chiplet-based, Hybrid
    Sparse-Dense Accelerator for Personalized Recommendations. In 2020 ACM/IEEE 47th
    Annual International Symposium on Computer Architecture (ISCA) (Los Alamitos,
    CA, USA, jun 2020), IEEE Computer Society, pp. 968–981.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(116) Hwang, R., Kim, T., Kwon, Y., 和 Rhu, M. Centaur: 一种基于芯片的小型混合稀疏-密集加速器，用于个性化推荐。在
    2020 年 ACM/IEEE 第 47 届国际计算机架构年会（ISCA）（洛杉矶，CA，美国，2020年6月），IEEE 计算机学会，第968–981页。'
- en: (117) Hybrid Memory Cube Consortium. Hybrid Memory Cube specification 2.1, Nov.
    2014.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (117) 混合内存立方体联盟。混合内存立方体规范 2.1，2014年11月。
- en: (118) Ielmini, D., and Wong, H.-S. P. In-memory computing with resistive switching
    devices. Nature Electronics 1, 6 (Jun 2018), 333–343.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (118) Ielmini, D., 和 Wong, H.-S. P. 采用电阻开关设备的内存计算。自然电子学 1, 6（2018年6月），第333–343页。
- en: (119) Intel. Intel Arc A770 Graphics 16GB, Jul 2022.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (119) Intel. Intel Arc A770 图形卡 16GB，2022年7月。
- en: (120) Intel. Intel High Level Synthesis Compiler Reference Manual, 2022.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (120) Intel. Intel 高级综合编译器参考手册，2022年。
- en: '(121) Jain, V., Giraldo, S., Roose, J. D., Mei, L., Boons, B., and Verhelst,
    M. TinyVers: A Tiny Versatile System-on-Chip With State-Retentive eMRAM for ML
    Inference at the Extreme Edge. IEEE Journal of Solid-State Circuits (2023), 1–12.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(121) Jain, V., Giraldo, S., Roose, J. D., Mei, L., Boons, B., 和 Verhelst,
    M. TinyVers: 一种小型多功能芯片系统，具有状态保持的 eMRAM，用于极端边缘的机器学习推断。IEEE 固态电路期刊（2023），第1–12页。'
- en: (122) Jeddeloh, J., and Keeth, B. Hybrid memory cube new DRAM architecture increases
    density and performance. In 2012 Symposium on VLSI Technology (VLSIT) (2012),
    pp. 87–88.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (122) Jeddeloh, J., 和 Keeth, B. 混合内存立方体新的 DRAM 架构提高了密度和性能。在 2012 年 VLSI 技术研讨会（VLSIT）（2012），第87–88页。
- en: (123) JEDEC Solid State Technology Association. High Bandwidth Memory DRAM (HBM3),
    JESD238A, Jan. 2023.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (123) JEDEC 固态技术协会。高带宽内存 DRAM (HBM3)，JESD238A，2023年1月。
- en: (124) Jia, T., Mantovani, P., Dos Santos, M. C., Giri, D., Zuckerman, J., Loscalzo,
    E. J., Cochet, M., Swaminathan, K., Tombesi, G., Zhang, J. J., Chandramoorthy,
    N., Wellman, J.-D., Tien, K., Carloni, L., Shepard, K., Brooks, D., Wei, G.-Y.,
    and Bose, P. A 12nm Agile-Designed SoC for Swarm-Based Perception with Heterogeneous
    IP Blocks, a Reconfigurable Memory Hierarchy, and an 800MHz Multi-Plane NoC. In
    ESSCIRC 2022- IEEE 48th European Solid State Circuits Conference (ESSCIRC) (Sept.
    2022), pp. 269–272.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (124) Jia, T., Mantovani, P., Dos Santos, M. C., Giri, D., Zuckerman, J., Loscalzo,
    E. J., Cochet, M., Swaminathan, K., Tombesi, G., Zhang, J. J., Chandramoorthy,
    N., Wellman, J.-D., Tien, K., Carloni, L., Shepard, K., Brooks, D., Wei, G.-Y.,
    和 Bose, P. 一款12纳米敏捷设计的SoC，适用于基于蜂群的感知，具有异构IP块、可重构内存层次结构和800MHz多平面NoC。发表于ESSCIRC
    2022- IEEE第48届欧洲固态电路会议 (ESSCIRC) (2022年9月)，第269–272页。
- en: '(125) Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick,
    R., Guadarrama, S., and Darrell, T. Caffe: Convolutional Architecture for Fast
    Feature Embedding. arXiv preprint arXiv:1408.5093 (2014).'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (125) Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick,
    R., Guadarrama, S., 和 Darrell, T. Caffe：快速特征嵌入的卷积架构。arXiv预印本 arXiv:1408.5093 (2014)。
- en: (126) Jia, Z., Tillman, B., Maggioni, M., and Scarpazza, D. P. Dissecting the
    Graphcore IPU Architecture via Microbenchmarking, Dec. 2019.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (126) Jia, Z., Tillman, B., Maggioni, M., 和 Scarpazza, D. P. 通过微基准测试解剖Graphcore
    IPU架构，2019年12月。
- en: '(127) Jiao, Q., Hu, W., Liu, F., and Dong, Y. RISC-VTF: RISC-V Based Extended
    Instruction Set for Transformer. In 2021 IEEE International Conference on Systems,
    Man, and Cybernetics (SMC) (2021), pp. 1565–1570.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (127) Jiao, Q., Hu, W., Liu, F., 和 Dong, Y. RISC-VTF：基于RISC-V的扩展指令集，用于变压器。发表于2021年IEEE系统、人与控制论国际会议
    (SMC) (2021)，第1565–1570页。
- en: (128) Jiao, Y., Han, L., Jin, R., Su, Y.-J., Ho, C., Yin, L., Li, Y., Chen,
    L., Chen, Z., Liu, L., He, Z., Yan, Y., He, J., Mao, J., Zai, X., Wu, X., Zhou,
    Y., Gu, M., Zhu, G., Zhong, R., Lee, W., Chen, P., Chen, Y., Li, W., Xiao, D.,
    Yan, Q., Zhuang, M., Chen, J., Tian, Y., Lin, Y., Wu, W., Li, H., and Dou, Z.
    A 12nm Programmable Convolution-Efficient Neural-Processing-Unit Chip Achieving
    825TOPS. In 2020 IEEE International Solid- State Circuits Conference - (ISSCC)
    (2020), pp. 136–140.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (128) Jiao, Y., Han, L., Jin, R., Su, Y.-J., Ho, C., Yin, L., Li, Y., Chen,
    L., Chen, Z., Liu, L., He, Z., Yan, Y., He, J., Mao, J., Zai, X., Wu, X., Zhou,
    Y., Gu, M., Zhu, G., Zhong, R., Lee, W., Chen, P., Chen, Y., Li, W., Xiao, D.,
    Yan, Q., Zhuang, M., Chen, J., Tian, Y., Lin, Y., Wu, W., Li, H., and Dou, Z.
    一款12纳米可编程卷积高效神经处理单元芯片，实现了825TOPS。发表于2020年IEEE国际固态电路会议 - (ISSCC) (2020)，第136–140页。
- en: '(129) Jin, Q., Ren, J., Zhuang, R., Hanumante, S., Li, Z., Chen, Z., Wang,
    Y., Yang, K., and Tulyakov, S. F8Net: Fixed-Point 8-bit Only Multiplication for
    Network Quantization. In International Conference on Learning Representations
    (2022).'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (129) Jin, Q., Ren, J., Zhuang, R., Hanumante, S., Li, Z., Chen, Z., Wang, Y.,
    Yang, K., 和 Tulyakov, S. F8Net：仅8位固定点乘法用于网络量化。发表于国际学习表征会议 (2022)。
- en: (130) John, S. The future of computing beyond Moore’s Law. Phil. Trans. R. Soc.
    (2020).
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (130) John, S. 超越摩尔定律的计算未来。哲学会报 R. Soc. (2020)。
- en: '(131) Jou, J. M., Kuang, S. R., and Chen, R. D. Design of low-error fixed-width
    multipliers for DSP applications. IEEE Transactions on Circuits and Systems II:
    Analog and Digital Signal Processing 46, 6 (1999), 836–842.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (131) Jou, J. M., Kuang, S. R., 和 Chen, R. D. 低误差固定宽度乘法器的设计，用于DSP应用。IEEE电路与系统二类：模拟和数字信号处理
    46, 6 (1999)，第836–842页。
- en: (132) Jouppi, N., Borchers, A., Boyle, R., Cantin, P.-l., Chao, C., Clark, C.,
    Coriell, J., Daley, M., Dau, M., Dean, J., Gelb, B., Young, C., Ghaemmaghami,
    T., Gottipati, R., Gulland, W., Hagmann, R., Ho, C., Hogberg, D., Hu, J., and
    Boden, N. In-Datacenter Performance Analysis of a Tensor Processing Unit. In 44th
    Annual International Symposium on Computer Architecture (06 2017), Association
    for Computing Machinery, pp. 1–12.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (132) Jouppi, N., Borchers, A., Boyle, R., Cantin, P.-l., Chao, C., Clark, C.,
    Coriell, J., Daley, M., Dau, M., Dean, J., Gelb, B., Young, C., Ghaemmaghami,
    T., Gottipati, R., Gulland, W., Hagmann, R., Ho, C., Hogberg, D., Hu, J., 和 Boden,
    N. 数据中心内的张量处理单元性能分析。发表于第44届国际计算机架构年会 (06 2017)，计算机协会，第1–12页。
- en: '(133) Jouppi, N., Kurian, G., Li, S., Ma, P., Nagarajan, R., Nai, L., Patil,
    N., Subramanian, S., Swing, A., Towles, B., Young, C., Zhou, X., Zhou, Z., and
    Patterson, D. A. TPU v4: An Optically Reconfigurable Supercomputer for Machine
    Learning with Hardware Support for Embeddings. In 50th Annual International Symposium
    on Computer Architecture (2023).'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (133) Jouppi, N., Kurian, G., Li, S., Ma, P., Nagarajan, R., Nai, L., Patil,
    N., Subramanian, S., Swing, A., Towles, B., Young, C., Zhou, X., Zhou, Z., 和 Patterson,
    D. A. TPU v4：一种光学可重配置超级计算机，支持机器学习的硬件嵌入。发表于第50届国际计算机架构年会 (2023)。
- en: (134) Jouppi, N., Young, C., Patil, N., and Patterson, D. Motivation for and
    Evaluation of the First Tensor Processing Unit. IEEE Micro 38, 3 (May 2018), 10–19.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (134) Jouppi, N., Young, C., Patil, N., 和 Patterson, D. 首个张量处理单元的动机与评估。IEEE
    微处理器 38, 3 (2018年5月), 10–19。
- en: (135) Jouppi, N. P., Yoon, D. H., Kurian, G., Li, S., Patil, N., Laudon, J.,
    Young, C., and Patterson, D. A Domain-Specific Supercomputer for Training Deep
    Neural Networks. Communications of the ACM 63, 7 (June 2020), 67–78.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (135) Jouppi, N. P., Yoon, D. H., Kurian, G., Li, S., Patil, N., Laudon, J.,
    Young, C., 和 Patterson, D. 专门用于训练深度神经网络的领域特定超级计算机。ACM 通讯 63, 7 (2020年6月), 67–78。
- en: (136) Ju, Y., and Gu, J. A Systolic Neural CPU Processor Combining Deep Learning
    and General-Purpose Computing With Enhanced Data Locality and End-to-End Performance.
    IEEE Journal of Solid-State Circuits 58, 1 (Jan. 2023), 216–226.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (136) Ju, Y., 和 Gu, J. 一种结合深度学习和通用计算的脉动神经 CPU 处理器，具有增强的数据局部性和端到端性能。IEEE 固态电路期刊
    58, 1 (2023年1月), 216–226。
- en: (137) Kada, M. Research and Development History of Three-Dimensional Integration
    Technology. Springer International Publishing, Cham, 2015, pp. 1–23.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (137) Kada, M. 三维集成技术的研究与开发历史。Springer International Publishing, Cham, 2015,
    pp. 1–23。
- en: (138) Kannan, A., Jerger, N. E., and Loh, G. H. Enabling interposer-based disintegration
    of multi-core processors. In 2015 48th Annual IEEE/ACM International Symposium
    on Microarchitecture (MICRO) (2015), pp. 546–558.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (138) Kannan, A., Jerger, N. E., 和 Loh, G. H. 实现基于介质的多核处理器解耦。在 2015 年 IEEE/ACM
    第 48 届国际微架构研讨会 (MICRO) (2015), pp. 546–558。
- en: (139) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child,
    R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling Laws for Neural Language
    Models, Jan. 2020.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (139) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child,
    R., Gray, S., Radford, A., Wu, J., 和 Amodei, D. 神经语言模型的尺度规律，2020年1月。
- en: (140) Khaddam-Aljameh, R., Stanisavljevic, M., Mas, J. F., Karunaratne, G.,
    Brändli, M., Liu, F., Singh, A., Müller, S. M., Egger, U., Petropoulos, A., et al.
    HERMES-core—A 1.59-TOPS/mm 2 PCM on 14-nm CMOS in-memory compute core using 300-ps/LSB
    linearized CCO-based ADCs. IEEE Journal of Solid-State Circuits 57, 4 (2022),
    1027–1038.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (140) Khaddam-Aljameh, R., Stanisavljevic, M., Mas, J. F., Karunaratne, G.,
    Brändli, M., Liu, F., Singh, A., Müller, S. M., Egger, U., Petropoulos, A., 等.
    HERMES-core—一种 1.59-TOPS/mm² PCM 的 14 纳米 CMOS 内存计算核心，使用 300-ps/LSB 线性化 CCO 基 ADC。IEEE
    固态电路期刊 57, 4 (2022), 1027–1038。
- en: (141) Khairy, M., Wassal, A. G., and Zahran, M. A survey of architectural approaches
    for improving GPGPU performance, programmability and heterogeneity. Journal of
    Parallel and Distributed Computing 127 (2019), 65–88.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (141) Khairy, M., Wassal, A. G., 和 Zahran, M. 改进 GPGPU 性能、可编程性和异质性的架构方法综述。并行与分布式计算期刊
    127 (2019), 65–88。
- en: '(142) Kim, D., Kung, J., Chai, S., Yalamanchili, S., and Mukhopadhyay, S. Neurocube:
    A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory.
    In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture
    (ISCA) (2016), pp. 380–392.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (142) Kim, D., Kung, J., Chai, S., Yalamanchili, S., 和 Mukhopadhyay, S. Neurocube：一种具有高密度
    3D 内存的可编程数字神经形态架构。在 2016 年 ACM/IEEE 第 43 届国际计算机架构研讨会 (ISCA) (2016), pp. 380–392。
- en: (143) Kim, D., Yu, C., Xie, S., Chen, Y., Kim, J.-Y., Kim, B., Kulkarni, J. P.,
    and Kim, T. T.-H. An Overview of Processing-in-Memory Circuits for Artificial
    Intelligence and Machine Learning. IEEE Journal on Emerging and Selected Topics
    in Circuits and Systems 12, 2 (2022), 338–353.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (143) Kim, D., Yu, C., Xie, S., Chen, Y., Kim, J.-Y., Kim, B., Kulkarni, J.
    P., 和 Kim, T. T.-H. 人工智能和机器学习中的内存处理电路概述。IEEE 电子与选择主题电路与系统期刊 12, 2 (2022), 338–353。
- en: '(144) Kim, J., and Kim, Y. HBM: Memory solution for bandwidth-hungry processors.
    In 2014 IEEE Hot Chips 26 Symposium (HCS) (2014), pp. 1–24.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (144) Kim, J., 和 Kim, Y. HBM：带宽需求高处理器的内存解决方案。在 2014 年 IEEE Hot Chips 26 研讨会
    (HCS) (2014), pp. 1–24。
- en: '(145) Kim, S., Kim, S., Um, S., Kim, S., Kim, K., and Yoo, H.-J. Neuro-CIM:
    A 310.4 TOPS/W Neuromorphic Computing-in-Memory Processor with Low WL/BL activity
    and Digital-Analog Mixed-mode Neuron Firing. In 2022 IEEE Symposium on VLSI Technology
    and Circuits (VLSI Technology and Circuits) (2022), pp. 38–39.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (145) Kim, S., Kim, S., Um, S., Kim, S., Kim, K., 和 Yoo, H.-J. Neuro-CIM：一种
    310.4 TOPS/W 的神经形态内存计算处理器，具有低 WL/BL 活动和数字-模拟混合模式神经元放电。在 2022 年 IEEE VLSI 技术与电路研讨会
    (VLSI 技术与电路) (2022), pp. 38–39。
- en: '(146) Knechtel, J., Sinanoglu, O., Elfadel, I. A. M., Lienig, J., and Sze,
    C. C. N. Large-Scale 3D Chips: Challenges and Solutions for Design Automation,
    Testing, and Trustworthy Integration. IPSJ Transactions on System and LSI Design
    Methodology 10 (2017), 45–62.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (146) Knechtel, J., Sinanoglu, O., Elfadel, I. A. M., Lienig, J., 和 Sze, C.
    C. N. 大规模 3D 芯片：设计自动化、测试和可信集成的挑战与解决方案。IPSJ 系统与 LSI 设计方法期刊 10 (2017), 45–62。
- en: (147) Knight, J. C., and Nowotny, T. GPUs Outperform Current HPC and Neuromorphic
    Solutions in Terms of Speed and Energy When Simulating a Highly-Connected Cortical
    Model. Frontiers in Neuroscience 12 (2018).
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (147) Knight, J. C., 和 Nowotny, T. GPU 在模拟高度连接的皮层模型时，在速度和能效方面优于当前的 HPC 和神经形态解决方案。发表于《前沿神经科学》12（2018年）。
- en: (148) Knowles, S. Graphcore. In 2021 IEEE Hot Chips 33 Symposium (HCS) (Aug.
    2021), pp. 1–25.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (148) Knowles, S. Graphcore。发表于 2021 IEEE 热芯片 33 研讨会（HCS）（2021年8月），第1–25页。
- en: (149) Kong, T., and Li, S. Design and Analysis of Approximate 4–2 Compressors
    for High-Accuracy Multipliers. IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems 29, 10 (2021), 1771–1781.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (149) Kong, T., 和 Li, S. 高精度乘法器的近似 4–2 压缩器设计与分析。发表于 IEEE 超大规模集成（VLSI）系统汇刊 29,
    10（2021年），第1771–1781页。
- en: (150) Krashinsky, R., Giroux, O., Jones, S., Stam, N., and Ramaswamy, S. NVIDIA
    Ampere Architecture In-Depth, May 2020.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (150) Krashinsky, R., Giroux, O., Jones, S., Stam, N., 和 Ramaswamy, S. NVIDIA
    Ampere 架构深入分析，2020年5月。
- en: (151) Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet Classification
    with Deep Convolutional Neural Networks. In Advances in Neural Information Processing
    Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds.
    Curran Associates, Inc., 2012, pp. 1097–1105.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (151) Krizhevsky, A., Sutskever, I., 和 Hinton, G. E. 使用深度卷积神经网络进行 ImageNet 分类。发表于《神经信息处理系统进展》25卷，F.
    Pereira, C. J. C. Burges, L. Bottou, 和 K. Q. Weinberger 编，Curran Associates, Inc.,
    2012年，第1097–1105页。
- en: (152) Kulkarni, P., Gupta, P., and Ercegovac, M. Trading Accuracy for Power
    with an Underdesigned Multiplier Architecture. In 2011 24th Internatioal Conference
    on VLSI Design (2011), pp. 346–351.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (152) Kulkarni, P., Gupta, P., 和 Ercegovac, M. 用欠设计的乘法器架构在精度和功耗之间进行权衡。发表于 2011年第24届国际
    VLSI 设计会议（2011年），第346–351页。
- en: (153) Kwon, Y., Han, J., Cho, Y. P., Kim, J., Chung, J., Choi, J., Park, S.,
    Kim, I., Kwon, H., Kim, J., Kim, H., Jeon, W., Jeon, Y., Cho, M., and Choi, M.
    Chiplet Heterogeneous-Integration AI Processor. In 2023 International Conference
    on Electronics, Information, and Communication (ICEIC) (2023).
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (153) Kwon, Y., Han, J., Cho, Y. P., Kim, J., Chung, J., Choi, J., Park, S.,
    Kim, I., Kwon, H., Kim, J., Kim, H., Jeon, W., Jeon, Y., Cho, M., 和 Choi, M. Chiplet
    异质集成 AI 处理器。发表于 2023 国际电子、信息与通信会议（ICEIC）（2023年）。
- en: (154) Kwon, Y.-C., Lee, S. H., Lee, J., Kwon, S.-H., Ryu, J. M., Son, J.-P.,
    Seongil, O., Yu, H.-S., Lee, H., Kim, S. Y., Cho, Y., Kim, J. G., Choi, J., Shin,
    H.-S., Kim, J., Phuah, B., Kim, H., Song, M. J., Choi, A., Kim, D., Kim, S., Kim,
    E.-B., Wang, D., Kang, S., Ro, Y., Seo, S., Song, J., Youn, J., Sohn, K., and
    Kim, N. S. 25.4 A 20nm 6GB Function-In-Memory DRAM, Based on HBM2 with a 1.2TFLOPS
    Programmable Computing Unit Using Bank-Level Parallelism, for Machine Learning
    Applications. In 2021 IEEE International Solid- State Circuits Conference (ISSCC)
    (2021), vol. 64, pp. 350–352.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (154) Kwon, Y.-C., Lee, S. H., Lee, J., Kwon, S.-H., Ryu, J. M., Son, J.-P.,
    Seongil, O., Yu, H.-S., Lee, H., Kim, S. Y., Cho, Y., Kim, J. G., Choi, J., Shin,
    H.-S., Kim, J., Phuah, B., Kim, H., Song, M. J., Choi, A., Kim, D., Kim, S., Kim,
    E.-B., Wang, D., Kang, S., Ro, Y., Seo, S., Song, J., Youn, J., Sohn, K., 和 Kim,
    N. S. 25.4 一款基于 HBM2 的 20nm 6GB 内存计算 DRAM，配备 1.2TFLOPS 可编程计算单元，利用银行级并行处理，适用于机器学习应用。发表于
    2021 IEEE 国际固态电路会议（ISSCC）（2021年），第64卷，第350–352页。
- en: (155) Lan, J., Nambiar, V. P., Sabapathy, R., Rotaru, M. D., and Do, A. T. Chiplet-based
    Architecture Design for Multi-Core Neuromorphic Processor. In 2021 IEEE 23rd Electronics
    Packaging Technology Conference (EPTC) (2021), pp. 410–412.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (155) Lan, J., Nambiar, V. P., Sabapathy, R., Rotaru, M. D., 和 Do, A. T. 基于
    Chiplet 的多核神经形态处理器架构设计。发表于 2021 IEEE 第23届电子封装技术会议（EPTC）（2021年），第410–412页。
- en: '(156) Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar,
    J., Riddle, R., Shpeisman, T., Vasilache, N., and Zinenko, O. MLIR: Scaling Compiler
    Infrastructure for Domain Specific Computation. In IEEE/ACM International Symposium
    on Code Generation and Optimization (CGO) (2021), pp. 2–14.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (156) Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar,
    J., Riddle, R., Shpeisman, T., Vasilache, N., 和 Zinenko, O. MLIR：为特定领域计算扩展编译器基础设施。发表于
    IEEE/ACM 国际代码生成与优化研讨会（CGO）（2021年），第2–14页。
- en: (157) Lazo, C. R., Reggiani, E., Morales, C. R., Figueras Bagué, R., Villa Vargas,
    L. A., Ramírez Salinas, M. A., Cortés, M. V., Sabri Ünsal, O., and Cristal, A.
    Adaptable Register File Organization for Vector Processors. In 2022 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA) (Apr. 2022), pp. 786–799.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (157) Lazo, C. R., Reggiani, E., Morales, C. R., Figueras Bagué, R., Villa Vargas,
    L. A., Ramírez Salinas, M. A., Cortés, M. V., Sabri Ünsal, O., 和 Cristal, A. 向量处理器的可适应寄存器文件组织。发表于
    2022 IEEE 国际高性能计算机架构研讨会（HPCA）（2022年4月），第786–799页。
- en: (158) LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Nature 521, 7553
    (2015), 436.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (158) LeCun, Y., Bengio, Y., 和 Hinton, G. 深度学习。《自然》521, 7553（2015年），第436页。
- en: (159) Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning
    applied to document recognition. Proceedings of the IEEE 86, 11 (1998), 2278–2324.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (159) Lecun，Y.，Bottou，L.，Bengio，Y.，和 Haffner，P. 基于梯度的学习应用于文档识别。IEEE汇刊 86，11（1998），2278–2324。
- en: (160) Lee, S. K., Agrawal, A., Silberman, J., Ziegler, M., Kang, M., Venkataramani,
    S., Cao, N., Fleischer, B., Guillorn, M., Cohen, M., Mueller, S. M., Oh, J., Lutz,
    M., Jung, J., Koswatta, S., Zhou, C., Zalani, V., Kar, M., Bonanno, J., Casatuta,
    R., Chen, C.-Y., Choi, J., Haynie, H., Herbert, A., Jain, R., Kim, K.-H., Li,
    Y., Ren, Z., Rider, S., Schaal, M., Schelm, K., Scheuermann, M. R., Sun, X., Tran,
    H., Wang, N., Wang, W., Zhang, X., Shah, V., Curran, B., Srinivasan, V., Lu, P.-F.,
    Shukla, S., Gopalakrishnan, K., and Chang, L. A 7-nm Four-Core Mixed-Precision
    AI Chip With 26.2-TFLOPS Hybrid-FP8 Training, 104.9-TOPS INT4 Inference, and Workload-Aware
    Throttling. IEEE Journal of Solid-State Circuits 57, 1 (2022), 182–197.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (160) 李，S. K.，Agrawal，A.，Silberman，J.，Ziegler，M.，Kang，M.，Venkataramani，S.，曹，N.，Fleischer，B.，Guillorn，M.，Cohen，M.，Mueller，S.
    M.，Oh，J.，Lutz，M.，Jung，J.，Koswatta，S.，周，C.，Zalani，V.，Kar，M.，Bonanno，J.，Casatuta，R.，Chen，C.-Y.，Choi，J.，Haynie，H.，Herbert，A.，Jain，R.，Kim，K.-H.，李，Y.，Ren，Z.，Rider，S.，Schaal，M.，Schelm，K.，Scheuermann，M.
    R.，Sun，X.，Tran，H.，Wang，N.，Wang，W.，Zhang，X.，Shah，V.，Curran，B.，Srinivasan，V.，Lu，P.-F.，Shukla，S.，Gopalakrishnan，K.，和
    Chang，L. 一款7nm四核混合精度AI芯片，具备26.2-TFLOPS混合FP8训练、104.9-TOPS INT4推理以及工作负载感知节流。IEEE固态电路学会汇刊
    57，1（2022），182–197。
- en: (161) Lee, S. M., Kim, H., Yeon, J., Lee, J., Choi, Y., Kim, M., Park, C., Jang,
    K., Kim, Y., Kim, Y., Lee, C., Han, H., Kim, W. E., Tang, R., and Baek, J. H.
    A 64-TOPS Energy-Efficient Tensor Accelerator in 14nm With Reconfigurable Fetch
    Network and Processing Fusion for Maximal Data Reuse. IEEE Open Journal of the
    Solid-State Circuits Society 2 (2022), 219–230.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (161) 李，S. M.，金，H.，Yeon，J.，李，J.，Choi，Y.，Kim，M.，Park，C.，张，K.，金，Y.，金，Y.，李，C.，Han，H.，金，W.
    E.，Tang，R.，和 Baek，J. H. 一款14nm 64-TOPS高效能张量加速器，具有可重构取数网络和处理融合，以实现最大数据重用。IEEE开放固态电路学会汇刊
    2（2022），219–230。
- en: (162) Lee, Y., Waterman, A., Avizienis, R., Cook, H., Sun, C., Stojanović, V.,
    and Asanović, K. A 45nm 1.3 GHz 16.7 double-precision GFLOPS/W RISC-V processor
    with vector accelerators. In ESSCIRC 2014-40th European Solid State Circuits Conference
    (ESSCIRC) (2014), IEEE, pp. 199–202.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (162) 李，Y.，Waterman，A.，Avizienis，R.，Cook，H.，Sun，C.，Stojanović，V.，和 Asanović，K.
    一款45nm 1.3 GHz 16.7双精度GFLOPS/W RISC-V处理器，配有向量加速器。见于ESSCIRC 2014-第40届欧洲固态电路会议（ESSCIRC）（2014），IEEE，第199–202页。
- en: (163) Lepri, N., Glukhov, A., Cattaneo, L., Farronato, M., Mannocci, P., and
    Ielmini, D. In-memory computing for machine learning and deep learning. IEEE Journal
    of the Electron Devices Society (2023), 1–1.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (163) Lepri，N.，Glukhov，A.，Cattaneo，L.，Farronato，M.，Mannocci，P.，和 Ielmini，D.
    面向机器学习和深度学习的内存计算。IEEE电子器件学会汇刊（2023），1–1。
- en: '(164) Li, G., Liu, Z., Li, F., and Cheng, J. Block Convolution: Towards Memory-Efficient
    Inference of Large-Scale CNNs on FPGA. CoRR abs/2105.08937 (2021).'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (164) 李，G.，刘，Z.，李，F.，和 郑，J. 块卷积：实现大规模CNN在FPGA上内存高效推理。CoRR abs/2105.08937（2021）。
- en: '(165) Li, J., Jiang, S., Gong, S., Wu, J., Yan, J., Yan, G., and Li, X. SqueezeFlow:
    A Sparse CNN Accelerator Exploiting Concise Convolution Rules. IEEE Transactions
    on Computers 68, 11 (2019), 1663–1677.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (165) 李，J.，姜，S.，龚，S.，吴，J.，闫，J.，闫，G.，和 李，X. SqueezeFlow：利用简洁卷积规则的稀疏CNN加速器。IEEE计算机汇刊
    68，11（2019），1663–1677。
- en: (166) Li, L., Hammad, I., and El-Sankary, K. Dual segmentation approximate multiplier.
    Electronics Letters 57, 19 (2021), 718–720.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (166) 李，L.，Hammad，I.，和 El-Sankary，K. 双分段近似乘法器。电子通讯信函 57，19（2021），718–720。
- en: '(167) Li, Y., Louri, A., and Karanth, A. SPRINT: A high-performance, energy-efficient,
    and scalable chiplet-based accelerator with photonic interconnects for CNN inference.
    IEEE Transactions on Parallel and Distributed Systems 33, 10 (2021), 2332–2345.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (167) 李，Y.，Louri，A.，和 Karanth，A. SPRINT：一种高性能、节能且可扩展的基于芯片的加速器，具有用于CNN推理的光子互连。IEEE并行与分布式系统汇刊
    33，10（2021），2332–2345。
- en: (168) Lin, C.-H., Cheng, C.-C., Tsai, Y.-M., Hung, S.-J., Kuo, Y.-T., Wang,
    P. H., Tsung, P.-K., Hsu, J.-Y., Lai, W.-C., Liu, C.-H., Wang, S.-Y., Kuo, C.-H.,
    Chang, C.-Y., Lee, M.-H., Lin, T.-Y., and Chen, C.-C. A 3.4-to-13.3TOPS/W 3.6TOPS
    Dual-Core Deep-Learning Accelerator for Versatile AI Applications in 7nm 5G Smartphone
    SoC. In 2020 IEEE International Solid- State Circuits Conference - (ISSCC) (2020),
    pp. 134–136.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (168) 林，C.-H.，郑，C.-C.，蔡，Y.-M.，洪，S.-J.，郭，Y.-T.，王，P. H.，Tsung，P.-K.，许，J.-Y.，赖，W.-C.，刘，C.-H.，王，S.-Y.，郭，C.-H.，张，C.-Y.，李，M.-H.，林，T.-Y.，和
    陈，C.-C. 一款3.4至13.3TOPS/W 3.6TOPS双核深度学习加速器，用于7nm 5G智能手机SoC中的多功能AI应用。见于2020年IEEE国际固态电路会议（ISSCC）（2020），第134–136页。
- en: (169) Lin, M.-S., Huang, T.-C., Tsai, C.-C., Tam, K.-H., Hsieh, C.-H., Chen,
    T., Huang, W.-H., Hu, J., Chen, Y.-C., Goel, S. K., Fu, C.-M., Rusu, S., Li, C.-C.,
    Yang, S.-Y., Wong, M., Yang, S.-C., and Lee, F. A 7nm 4GHz Arm-core-based CoWoS
    Chiplet Design for High Performance Computing. In 2019 Symposium on VLSI Circuits
    (2019).
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (169) Lin, M.-S., Huang, T.-C., Tsai, C.-C., Tam, K.-H., Hsieh, C.-H., Chen,
    T., Huang, W.-H., Hu, J., Chen, Y.-C., Goel, S. K., Fu, C.-M., Rusu, S., Li, C.-C.,
    Yang, S.-Y., Wong, M., Yang, S.-C., 和 Lee, F. 基于 7nm 4GHz Arm 核心的 CoWoS 芯片设计用于高性能计算。在
    2019 年 VLSI 电路研讨会（2019年）。
- en: (170) Liu, C., Bellec, G., Vogginger, B., Kappel, D., Partzsch, J., Neumärker,
    F., Höppner, S., Maass, W., Furber, S. B., Legenstein, R., and Mayr, C. G. Memory-Efficient
    Deep Learning on a SpiNNaker 2 Prototype. Frontiers in Neuroscience 12 (2018).
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (170) Liu, C., Bellec, G., Vogginger, B., Kappel, D., Partzsch, J., Neumärker,
    F., Höppner, S., Maass, W., Furber, S. B., Legenstein, R., 和 Mayr, C. G. 在 SpiNNaker
    2 原型上进行内存高效的深度学习。前沿神经科学 12（2018年）。
- en: '(171) Liu, L., Zhu, J., Li, Z., Lu, Y., Deng, Y., Han, J., Yin, S., and Wei,
    S. A Survey of Coarse-Grained Reconfigurable Architecture and Design: Taxonomy,
    Challenges, and Applications. ACM Comput. Surv. 52, 6 (2019).'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (171) Liu, L., Zhu, J., Li, Z., Lu, Y., Deng, Y., Han, J., Yin, S., 和 Wei, S.
    粗粒度可重构架构与设计的调查：分类、挑战与应用。ACM计算机调查 52，6（2019年）。
- en: '(172) Liu, X., Mao, M., Liu, B., Li, H., Chen, Y., Li, B., Wang, Y., Jiang,
    H., Barnell, M., Wu, Q., and Yang, J. RENO: A high-efficient reconfigurable neuromorphic
    computing accelerator design. In 2015 52nd ACM/EDAC/IEEE Design Automation Conference
    (DAC) (2015), pp. 1–6.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (172) Liu, X., Mao, M., Liu, B., Li, H., Chen, Y., Li, B., Wang, Y., Jiang,
    H., Barnell, M., Wu, Q., 和 Yang, J. RENO：一种高效的可重构神经形态计算加速器设计。在 2015 年第52届 ACM/EDAC/IEEE
    设计自动化会议（DAC）（2015年），第1–6页。
- en: '(173) Liu, Z., Cheng, K.-T., Huang, D., Xing, E., and Shen, Z. Nonuniform-to-Uniform
    Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation.
    In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
    (2022), pp. 4932–4942.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (173) Liu, Z., Cheng, K.-T., Huang, D., Xing, E., 和 Shen, Z. 非均匀到均匀量化：通过广义直通估计实现精确量化。在
    2022 年 IEEE/CVF 计算机视觉与模式识别会议（CVPR）（2022年），第4932–4942页。
- en: (174) Ma, Y., Cao, Y., Vrudhula, S., and Seo, J.-s. Optimizing the Convolution
    Operation to Accelerate Deep Neural Networks on FPGA. IEEE Transactions on Very
    Large Scale Integration (VLSI) Systems 26, 7 (2018), 1354–1367.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (174) Ma, Y., Cao, Y., Vrudhula, S., 和 Seo, J.-s. 优化卷积操作以加速 FPGA 上的深度神经网络。IEEE大规模集成（VLSI）系统交易
    26，7（2018年），1354–1367。
- en: (175) Ma, Y., Suda, N., Cao, Y., Seo, J.-s., and Vrudhula, S. Scalable and modularized
    RTL compilation of Convolutional Neural Networks onto FPGA. In 2016 26th International
    Conference on Field Programmable Logic and Applications (FPL) (2016), pp. 1–8.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (175) Ma, Y., Suda, N., Cao, Y., Seo, J.-s., 和 Vrudhula, S. 可扩展和模块化的 RTL 编译将卷积神经网络映射到
    FPGA。在 2016年第26届国际现场可编程逻辑与应用大会（FPL）（2016年），第1–8页。
- en: (176) Machupalli, R., Hossain, M., and Mandal, M. Review of ASIC accelerators
    for deep neural network. Microprocessors and Microsystems 89 (2022), 104441.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (176) Machupalli, R., Hossain, M., 和 Mandal, M. 对深度神经网络 ASIC 加速器的回顾。微处理器与微系统
    89（2022年），104441。
- en: (177) Machura, M., Danilowicz, M., and Kryjak, T. Embedded Object Detection
    with Custom LittleNet, FINN and Vitis AI DCNN Accelerators. Journal of Low Power
    Electronics and Applications 12, 2 (2022).
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (177) Machura, M., Danilowicz, M., 和 Kryjak, T. 使用自定义 LittleNet、FINN 和 Vitis
    AI DCNN 加速器进行嵌入式对象检测。低功耗电子与应用期刊 12，2（2022年）。
- en: '(178) Mallasén, D., Murillo, R., Barrio, A. A. D., Botella, G., Piñuel, L.,
    and Prieto-Matias, M. PERCIVAL: Open-Source Posit RISC-V Core With Quire Capability.
    IEEE Transactions on Emerging Topics in Computing 10, 3 (2022), 1241–1252.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (178) Mallasén, D., Murillo, R., Barrio, A. A. D., Botella, G., Piñuel, L.,
    和 Prieto-Matias, M. PERCIVAL：具有 Quire 功能的开源 Posit RISC-V 核心。IEEE新兴计算主题交易 10，3（2022年），1241–1252。
- en: '(179) Martinez, P. Y., Beilliard, Y., Godard, M., Danovitch, D., Drouin, D.,
    Charbonnier, J., Coudrain, P., Garnier, A., Lattard, D., Vivet, P., Cheramy, S.,
    Guthmuller, E., Tortolero, C. F., Mengue, V., Durupt, J., Philippe, A., and Dutoit,
    D. ExaNoDe: Combined Integration of Chiplets on Active Interposer with Bare Dice
    in a Multi-Chip-Module for Heterogeneous and Scalable High Performance Compute
    Nodes. In 2020 IEEE Symposium on VLSI Technology (2020).'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (179) Martinez, P. Y., Beilliard, Y., Godard, M., Danovitch, D., Drouin, D.,
    Charbonnier, J., Coudrain, P., Garnier, A., Lattard, D., Vivet, P., Cheramy, S.,
    Guthmuller, E., Tortolero, C. F., Mengue, V., Durupt, J., Philippe, A., 和 Dutoit,
    D. ExaNoDe：在多芯片模块中将芯片片段与裸片结合集成以实现异构和可扩展的高性能计算节点。在 2020 年 IEEE VLSI 技术研讨会（2020年）。
- en: (180) Mathur, R., Kumar, A. K. A., John, L., and Kulkarni, J. P. Thermal-Aware
    Design Space Exploration of 3-D Systolic ML Accelerators. IEEE Journal on Exploratory
    Solid-State Computational Devices and Circuits 7, 1 (2021), 70–78.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (180) Mathur, R., Kumar, A. K. A., John, L., 和 Kulkarni, J. P. 热感知设计空间探索的3-D流线型机器学习加速器。IEEE探索性固态计算器与电路杂志
    7, 1（2021），第70–78页。
- en: (181) Mattson, P., Cheng, C., Diamos, G., Coleman, C., Micikevicius, P., Patterson,
    D., Tang, H., Wei, G.-Y., Bailis, P., Bittorf, V., et al. Mlperf training benchmark.
    Proceedings of Machine Learning and Systems 2 (2020), 336–349.
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (181) Mattson, P., Cheng, C., Diamos, G., Coleman, C., Micikevicius, P., Patterson,
    D., Tang, H., Wei, G.-Y., Bailis, P., Bittorf, V., 等。Mlperf训练基准测试。机器学习与系统会议论文集
    2（2020），第336–349页。
- en: '(182) Mattson, P., Reddi, V. J., Cheng, C., Coleman, C., Diamos, G., Kanter,
    D., Micikevicius, P., Patterson, D., Schmuelling, G., Tang, H., Wei, G.-Y., and
    Wu, C.-J. MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance.
    IEEE Micro 40, 2 (2020), 8–16.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(182) Mattson, P., Reddi, V. J., Cheng, C., Coleman, C., Diamos, G., Kanter,
    D., Micikevicius, P., Patterson, D., Schmuelling, G., Tang, H., Wei, G.-Y., 和
    Wu, C.-J. MLPerf: 一种机器学习性能的行业标准基准套件。IEEE Micro 40, 2（2020），第8–16页。'
- en: (183) Medina, E. [Habana Labs presentation]. In 2019 IEEE Hot Chips 31 Symposium
    (HCS) (2019), pp. 1–29.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (183) Medina, E. [Habana Labs演讲]。发表于2019年IEEE Hot Chips 31研讨会（HCS）（2019），第1–29页。
- en: (184) Micron. Hybrid Memory Cube – HMC Gen2 HMC Memory Features, 2018.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (184) Micron. 混合内存立方体 – HMC Gen2 HMC 内存特性，2018年。
- en: '(185) Min, C., Mao, J., Li, H., and Chen, Y. NeuralHMC: An Efficient HMC-Based
    Accelerator for Deep Neural Networks. In 24th Asia and South Pacific Design Automation
    Conference (2019), ACM, p. 394–399.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(185) Min, C., Mao, J., Li, H., 和 Chen, Y. NeuralHMC: 一种高效的基于HMC的深度神经网络加速器。发表于第24届亚太设计自动化会议（2019），ACM，第394–399页。'
- en: '(186) Minervini, F., Palomar, O., Unsal, O., Reggiani, E., Quiroga, J., Marimon,
    J., Rojas, C., Figueras, R., Ruiz, A., Gonzalez, A., Mendoza, J., Vargas, I.,
    Hernandez, C., Cabre, J., Khoirunisya, L., Bouhali, M., Pavon, J., Moll, F., Olivieri,
    M., Kovac, M., Kovac, M., Dragic, L., Valero, M., and Cristal, A. Vitruvius+:
    An Area-Efficient RISC-V Decoupled Vector Coprocessor for High Performance Computing
    Applications. ACM Transactions on Architecture and Code Optimization 20, 2 (Mar.
    2023), 28:1–28:25.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(186) Minervini, F., Palomar, O., Unsal, O., Reggiani, E., Quiroga, J., Marimon,
    J., Rojas, C., Figueras, R., Ruiz, A., Gonzalez, A., Mendoza, J., Vargas, I.,
    Hernandez, C., Cabre, J., Khoirunisya, L., Bouhali, M., Pavon, J., Moll, F., Olivieri,
    M., Kovac, M., Kovac, M., Dragic, L., Valero, M., 和 Cristal, A. Vitruvius+: 一种高性能计算应用的面积高效RISC-V解耦向量协处理器。ACM建筑与代码优化杂志
    20, 2（2023年3月），第28:1–28:25页。'
- en: '(187) Miro-Panades, I., Tain, B., Christmann, J.-F., Coriat, D., Lemaire, R.,
    Jany, C., Martineau, B., Chaix, F., Waltener, G., Pluchart, E., Noel, J.-P., Makosiej,
    A., Montoya, M., Bacles-Min, S., Briand, D., Philippe, J.-M., Thonnart, Y., Valentian,
    A., Heitzmann, F., and Clermidy, F. SamurAI: A Versatile IoT Node With Event-Driven
    Wake-Up and Embedded ML Acceleration. IEEE Journal of Solid-State Circuits (2022),
    1–0.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(187) Miro-Panades, I., Tain, B., Christmann, J.-F., Coriat, D., Lemaire, R.,
    Jany, C., Martineau, B., Chaix, F., Waltener, G., Pluchart, E., Noel, J.-P., Makosiej,
    A., Montoya, M., Bacles-Min, S., Briand, D., Philippe, J.-M., Thonnart, Y., Valentian,
    A., Heitzmann, F., 和 Clermidy, F. SamurAI: 一种具有事件驱动唤醒和嵌入式机器学习加速的多功能物联网节点。IEEE固态电路杂志（2022），1–0。'
- en: (188) Mishra, A. K., Latorre, J. A., Pool, J., Stosic, D., Stosic, D., Venkatesh,
    G., Yu, C., and Micikevicius, P. Accelerating Sparse Deep Neural Networks. CoRR
    abs/2104.08378 (2021).
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (188) Mishra, A. K., Latorre, J. A., Pool, J., Stosic, D., Stosic, D., Venkatesh,
    G., Yu, C., 和 Micikevicius, P. 加速稀疏深度神经网络。CoRR abs/2104.08378 (2021)。
- en: '(189) MLPerf Inference Tiny v1.0 Results. [https://mlcommons.org/en/inference-tiny-10/](https://mlcommons.org/en/inference-tiny-10/),
    2023. Accessed: 2023-04-18.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (189) MLPerf Inference Tiny v1.0 结果。 [https://mlcommons.org/en/inference-tiny-10/](https://mlcommons.org/en/inference-tiny-10/)，2023年。访问时间：2023-04-18。
- en: (190) Mounce, G., Lyke, J., Horan, S., Powell, W., Doyle, R., and Some, R. Chiplet
    based approach for heterogeneous processing and packaging architectures. In 2016
    IEEE Aerospace Conference (2016), pp. 1–12.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (190) Mounce, G., Lyke, J., Horan, S., Powell, W., Doyle, R., 和 Some, R. 基于芯片单元的方法用于异构处理和封装架构。发表于2016年IEEE航空航天会议（2016），第1–12页。
- en: '(191) Muñoz Martínez, F., Garg, R., Pellauer, M., Abellán, J. L., Acacio, M. E.,
    and Krishna, T. Flexagon: A Multi-Dataflow Sparse-Sparse Matrix Multiplication
    Accelerator for Efficient DNN Processing. In 28th ACM International Conference
    on Architectural Support for Programming Languages and Operating Systems, Volume
    3 (2023), ASPLOS 2023, pp. 252–265.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(191) Muñoz Martínez, F., Garg, R., Pellauer, M., Abellán, J. L., Acacio, M.
    E., 和 Krishna, T. Flexagon: 一种多数据流稀疏-稀疏矩阵乘法加速器，用于高效的深度神经网络处理。发表于第28届ACM国际程序语言与操作系统架构支持会议，第3卷（2023），ASPLos
    2023，第252–265页。'
- en: (192) Nahmias, M. A., de Lima, T. F., Tait, A. N., Peng, H.-T., Shastri, B. J.,
    and Prucnal, P. R. Photonic Multiply-Accumulate Operations for Neural Networks.
    IEEE Journal of Selected Topics in Quantum Electronics 26, 1 (2020), 1–18.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (192) Nahmias, M. A., de Lima, T. F., Tait, A. N., Peng, H.-T., Shastri, B.
    J., 和 Prucnal, P. R. 光子乘加操作用于神经网络。IEEE 量子电子学选定主题学报 26, 1 (2020)，第 1–18 页。
- en: (193) Narayanamoorthy, S., Moghaddam, H. A., Liu, Z., Park, T., and Kim, N. S.
    Energy-Efficient Approximate Multiplication for Digital Signal Processing and
    Classification Applications. IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems 23, 6 (2015), 1180–1184.
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (193) Narayanamoorthy, S., Moghaddam, H. A., Liu, Z., Park, T., 和 Kim, N. S.
    节能的近似乘法，用于数字信号处理和分类应用。IEEE 超大规模集成（VLSI）系统学报 23, 6 (2015)，第 1180–1184 页。
- en: (194) Narayanan, P., Ambrogio, S., Okazaki, A., Hosokawa, K., Tsai, H., Nomura,
    A., Yasuda, T., Mackin, C., Lewis, S. C., Friz, A., Ishii, M., Kohda, Y., Mori,
    H., Spoon, K., Khaddam-Aljameh, R., Saulnier, N., Bergendahl, M., Demarest, J.,
    Brew, K. W., Chan, V., Choi, S., Ok, I., Ahsan, I., Lie, F. L., Haensch, W., Narayanan,
    V., and Burr, G. W. Fully On-Chip MAC at 14 nm Enabled by Accurate Row-Wise Programming
    of PCM-Based Weights and Parallel Vector-Transport in Duration-Format. IEEE Transactions
    on Electron Devices 68, 12 (2021), 6629–6636.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (194) Narayanan, P., Ambrogio, S., Okazaki, A., Hosokawa, K., Tsai, H., Nomura,
    A., Yasuda, T., Mackin, C., Lewis, S. C., Friz, A., Ishii, M., Kohda, Y., Mori,
    H., Spoon, K., Khaddam-Aljameh, R., Saulnier, N., Bergendahl, M., Demarest, J.,
    Brew, K. W., Chan, V., Choi, S., Ok, I., Ahsan, I., Lie, F. L., Haensch, W., Narayanan,
    V., 和 Burr, G. W. 14 nm 完全片上 MAC 由 PCM 基础权重的准确行级编程和持续格式的并行矢量传输实现。IEEE 电子器件学报 68,
    12 (2021)，第 6629–6636 页。
- en: '(195) Nitin, ., Thottethodi, M., and Vijaykumar, T. N. Millipede: Die-Stacked
    Memory Optimizations for Big Data Machine Learning Analytics. In 2018 IEEE International
    Parallel and Distributed Processing Symposium (IPDPS) (2018), pp. 160–171.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (195) Nitin, ., Thottethodi, M., 和 Vijaykumar, T. N. Millipede：用于大数据机器学习分析的堆叠式内存优化。发表于
    2018 IEEE 国际并行和分布处理研讨会（IPDPS）（2018），第 160–171 页。
- en: '(196) Nurvitadhi, E., Kwon, D., Jafari, A., Boutros, A., Sim, J., Tomson, P.,
    Sumbul, H., Chen, G., Knag, P., Kumar, R., Krishnamurthy, R., Gribok, S., Pasca,
    B., Langhammer, M., Marr, D., and Dasu, A. Why Compete When You Can Work Together:
    FPGA-ASIC Integration for Persistent RNNs. In 2019 IEEE 27th Annual International
    Symposium on Field-Programmable Custom Computing Machines (FCCM) (2019), pp. 199–207.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (196) Nurvitadhi, E., Kwon, D., Jafari, A., Boutros, A., Sim, J., Tomson, P.,
    Sumbul, H., Chen, G., Knag, P., Kumar, R., Krishnamurthy, R., Gribok, S., Pasca,
    B., Langhammer, M., Marr, D., 和 Dasu, A. 为什么要竞争，而不是合作：FPGA-ASIC 集成用于持久性 RNN。发表于
    2019 IEEE 第 27 届年度国际现场可编程定制计算机器研讨会（FCCM）（2019），第 199–207 页。
- en: (197) NVIDIA. Fermi, 2009.
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (197) NVIDIA. Fermi，2009 年。
- en: (198) NVIDIA. Kepler GK110, 2012.
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (198) NVIDIA. Kepler GK110，2012 年。
- en: (199) NVidia. NVIDIA Announces DGX H100 Systems – World’s Most Advanced Enterprise
    AI Infrastructure, Mar. 2022.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (199) NVidia. NVIDIA 宣布 DGX H100 系统——全球最先进的企业 AI 基础设施，2022 年 3 月。
- en: (200) NVidia. NVIDIA DGX Platform The best of NVIDIA AI—all in one place, May
    2023.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (200) NVidia. NVIDIA DGX 平台 NVIDIA AI 的最佳之选——一站式解决方案，2023 年 5 月。
- en: (201) NVidia. NVLink and NVSwitch, May 2023.
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (201) NVidia. NVLink 和 NVSwitch，2023 年 5 月。
- en: (202) Oh, J., Lee, S. K., Kang, M., Ziegler, M., Silberman, J., Agrawal, A.,
    Venkataramani, S., Fleischer, B., Guillorn, M., Choi, J., Wang, W., Mueller, S.,
    Ben-Yehuda, S., Bonanno, J., Cao, N., Casatuta, R., Chen, C.-Y., Cohen, M., Erez,
    O., Fox, T., Gristede, G., Haynie, H., Ivanov, V., Koswatta, S., Lo, S.-H., Lutz,
    M., Maier, G., Mesh, A., Nustov, Y., Rider, S., Schaal, M., Scheuermann, M., Sun,
    X., Wang, N., Yee, F., Zhou, C., Shah, V., Curran, B., Srinivasan, V., Lu, P.-F.,
    Shukla, S., Gopalakrishnan, K., and Chang, L. A 3.0 TFLOPS 0.62V Scalable Processor
    Core for High Compute Utilization AI Training and Inference. In 2020 IEEE Symposium
    on VLSI Circuits (2020), pp. 1–2.
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (202) Oh, J., Lee, S. K., Kang, M., Ziegler, M., Silberman, J., Agrawal, A.,
    Venkataramani, S., Fleischer, B., Guillorn, M., Choi, J., Wang, W., Mueller, S.,
    Ben-Yehuda, S., Bonanno, J., Cao, N., Casatuta, R., Chen, C.-Y., Cohen, M., Erez,
    O., Fox, T., Gristede, G., Haynie, H., Ivanov, V., Koswatta, S., Lo, S.-H., Lutz,
    M., Maier, G., Mesh, A., Nustov, Y., Rider, S., Schaal, M., Scheuermann, M., Sun,
    X., Wang, N., Yee, F., Zhou, C., Shah, V., Curran, B., Srinivasan, V., Lu, P.-F.,
    Shukla, S., Gopalakrishnan, K., 和 Chang, L. 一个 3.0 TFLOPS 0.62V 可扩展处理器核心，用于高计算利用率的
    AI 训练和推理。发表于 2020 IEEE VLSI Circuits 研讨会（2020），第 1–2 页。
- en: '(203) Oliveira, G. F., Santos, P. C., Alves, M. A. Z., and Carro, L. NIM: An
    HMC-Based Machine for Neuron Computation. In Applied Reconfigurable Computing
    (Cham, 2017), S. Wong, A. C. Beck, K. Bertels, and L. Carro, Eds., Springer International
    Publishing, pp. 28–35.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (203) Oliveira, G. F., Santos, P. C., Alves, M. A. Z., 和 Carro, L. NIM：一种基于
    HMC 的神经元计算机。发表于 可重构计算应用（Cham, 2017），S. Wong, A. C. Beck, K. Bertels 和 L. Carro
    编辑，Springer 国际出版公司，第 28–35 页。
- en: (204) OpenAI. GPT-4 Technical Report, Mar. 2023.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (204) OpenAI. GPT-4 技术报告，2023年3月。
- en: '(205) Ottavi, G., Garofalo, A., Tagliavini, G., Conti, F., Mauro, A. D., Benini,
    L., and Rossi, D. Dustin: A 16-Cores Parallel Ultra-Low-Power Cluster With 2b-to-32b
    Fully Flexible Bit-Precision and Vector Lockstep Execution Mode. IEEE Transactions
    on Circuits and Systems I: Regular Papers (2023), 1–14.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(205) Ottavi, G., Garofalo, A., Tagliavini, G., Conti, F., Mauro, A. D., Benini,
    L., 和 Rossi, D. Dustin: 一种 16 核并行超低功耗集群，具有 2b 至 32b 完全灵活的位精度和矢量锁步执行模式。IEEE 电路与系统
    I：常规论文 (2023), 1–14。'
- en: '(206) Painkras, E., Plana, L. A., Garside, J., Temple, S., Galluppi, F., Patterson,
    C., Lester, D. R., Brown, A. D., and Furber, S. B. SpiNNaker: A 1-W 18-Core System-on-Chip
    for Massively-Parallel Neural Network Simulation. IEEE Journal of Solid-State
    Circuits 48, 8 (2013), 1943–1953.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(206) Painkras, E., Plana, L. A., Garside, J., Temple, S., Galluppi, F., Patterson,
    C., Lester, D. R., Brown, A. D., 和 Furber, S. B. SpiNNaker: 一种 1 瓦 18 核系统芯片，用于大规模并行神经网络仿真。IEEE
    固态电路学报 48, 8 (2013), 1943–1953。'
- en: '(207) Parashar, A., Rhu, M., Mukkara, A., Puglielli, A., Venkatesan, R., Khailany,
    B., Emer, J., Keckler, S. W., and Dally, W. J. SCNN: An Accelerator for Compressed-Sparse
    Convolutional Neural Networks. In Proceedings of the 2017 ACM/IEEE 44th Annual
    International Symposium on Computer Architecture (2017), ISCA’17, pp. 27–40.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(207) Parashar, A., Rhu, M., Mukkara, A., Puglielli, A., Venkatesan, R., Khailany,
    B., Emer, J., Keckler, S. W., 和 Dally, W. J. SCNN: 一种用于压缩稀疏卷积神经网络的加速器。发表于 2017
    年 ACM/IEEE 第 44 届国际计算机架构研讨会 (ISCA’17) (2017), 第 27–40 页。'
- en: '(208) Park, G., Kung, J., and Lee, Y. Design and Analysis of Approximate Compressors
    for Balanced Error Accumulation in MAC Operator. IEEE Transactions on Circuits
    and Systems I: Regular Papers 68, 7 (2021), 2950–2961.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (208) Park, G., Kung, J., 和 Lee, Y. 近似压缩器的设计与分析，用于 MAC 运算符中的平衡误差累积。IEEE 电路与系统
    I：常规论文 68, 7 (2021), 2950–2961。
- en: '(209) Park, J., Li, S. R., Wen, W., Li, H., Chen, Y., and Dubey, P. Holistic
    SparseCNN: Forging the Trident of Accuracy, Speed, and Size. CoRR abs/1608.01409
    (2016).'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(209) Park, J., Li, S. R., Wen, W., Li, H., Chen, Y., 和 Dubey, P. Holistic
    SparseCNN: 锻造准确性、速度和规模的三叉戟。CoRR abs/1608.01409 (2016)。'
- en: '(210) Park, J., Naumov, M., Basu, P., Deng, S., Kalaiah, A., Khudia, D. S.,
    Law, J., Malani, P., Malevich, A., Satish, N., Pino, J. M., Schatz, M., Sidorov,
    A., Sivakumar, V., Tulloch, A., Wang, X., Wu, Y., Yuen, H., Diril, U., Dzhulgakov,
    D., Hazelwood, K. M., Jia, B., Jia, Y., Qiao, L., Rao, V., Rotem, N., Yoo, S.,
    and Smelyanskiy, M. Deep Learning Inference in Facebook Data Centers: Characterization,
    Performance Optimizations and Hardware Implications. CoRR abs/1811.09886 (2018).'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (210) Park, J., Naumov, M., Basu, P., Deng, S., Kalaiah, A., Khudia, D. S.,
    Law, J., Malani, P., Malevich, A., Satish, N., Pino, J. M., Schatz, M., Sidorov,
    A., Sivakumar, V., Tulloch, A., Wang, X., Wu, Y., Yuen, H., Diril, U., Dzhulgakov,
    D., Hazelwood, K. M., Jia, B., Jia, Y., Qiao, L., Rao, V., Rotem, N., Yoo, S.,
    和 Smelyanskiy, M. Facebook 数据中心中的深度学习推理：特征、性能优化和硬件影响。CoRR abs/1811.09886 (2018)。
- en: (211) Park, J.-S., Jang, J.-W., Lee, H., Lee, D., Lee, S., Jung, H., Lee, S.,
    Kwon, S., Jeong, K., Song, J.-H., Lim, S., and Kang, I. A 6K-MAC Feature-Map-Sparsity-Aware
    Neural Processing Unit in 5nm Flagship Mobile SoC. In 2021 IEEE International
    Solid- State Circuits Conference (ISSCC) (2021), vol. 64, pp. 152–154.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (211) Park, J.-S., Jang, J.-W., Lee, H., Lee, D., Lee, S., Jung, H., Lee, S.,
    Kwon, S., Jeong, K., Song, J.-H., Lim, S., 和 Kang, I. 一种 6K-MAC 特征图稀疏感知神经处理单元，采用
    5nm 主力移动 SoC。发表于 2021 IEEE 国际固态电路会议 (ISSCC) (2021), 第 64 卷，第 152–154 页。
- en: (212) Park, J.-S., Park, C., Kwon, S., Kim, H.-S., Jeon, T., Kang, Y., Lee,
    H., Lee, D., Kim, J., Lee, Y., Park, S., Jang, J.-W., Ha, S., Kim, M., Bang, J.,
    Lim, S. H., and Kang, I. A Multi-Mode 8K-MAC HW-Utilization-Aware Neural Processing
    Unit with a Unified Multi-Precision Datapath in 4nm Flagship Mobile SoC. In 2022
    IEEE International Solid- State Circuits Conference (ISSCC) (2022), vol. 65, pp. 246–248.
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (212) Park, J.-S., Park, C., Kwon, S., Kim, H.-S., Jeon, T., Kang, Y., Lee,
    H., Lee, D., Kim, J., Lee, Y., Park, S., Jang, J.-W., Ha, S., Kim, M., Bang, J.,
    Lim, S. H., 和 Kang, I. 一种多模式 8K-MAC 硬件利用感知神经处理单元，具有 4nm 主力移动 SoC 中统一的多精度数据通路。发表于
    2022 IEEE 国际固态电路会议 (ISSCC) (2022), 第 65 卷，第 246–248 页。
- en: (213) Park, S.-W., Park, J., Bong, K., Shin, D., Lee, J., Choi, S., and Yoo,
    H.-J. An Energy-Efficient and Scalable Deep Learning/Inference Processor With
    Tetra-Parallel MIMD Architecture for Big Data Applications. IEEE Transactions
    on Biomedical Circuits and Systems 9, 6 (2015), 838–848.
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (213) Park, S.-W., Park, J., Bong, K., Shin, D., Lee, J., Choi, S., 和 Yoo, H.-J.
    一种节能且可扩展的深度学习/推理处理器，具有四倍并行 MIMD 架构，适用于大数据应用。IEEE 生物医学电路与系统学报 9, 6 (2015), 838–848。
- en: '(214) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G.,
    Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang,
    E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
    Bai, J., and Chintala, S. PyTorch: An Imperative Style, High-Performance Deep
    Learning Library. In Advances in Neural Information Processing Systems 32. Curran
    Associates, Inc., 2019, pp. 8024–8035.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (214) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G.,
    Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang,
    E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
    Bai, J., 和 Chintala, S. PyTorch：一种命令式风格的高性能深度学习库。载于《神经信息处理系统进展》第 32 卷。Curran Associates,
    Inc., 2019，第 8024–8035 页。
- en: (215) Paulin, G., Andri, R., Conti, F., and Benini, L. RNN-Based Radio Resource
    Management on Multicore RISC-V Accelerator Architectures. IEEE Transactions on
    Very Large Scale Integration (VLSI) Systems 29, 9 (Sept. 2021), 1624–1637.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (215) Paulin, G., Andri, R., Conti, F., 和 Benini, L. 基于 RNN 的多核 RISC-V 加速器架构上的无线电资源管理。IEEE
    大规模集成 (VLSI) 系统期刊 29, 9 (2021年9月)，第 1624–1637 页。
- en: (216) Pawlowski, J. T. Hybrid memory cube (HMC). In 2011 IEEE Hot Chips 23 Symposium
    (HCS) (2011), pp. 1–24.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (216) Pawlowski, J. T. 混合内存立方体 (HMC)。载于 2011 IEEE 热芯片 23 号研讨会 (HCS)（2011年），第
    1–24 页。
- en: (217) Peemen, M., Setio, A. A. A., Mesman, B., and Corporaal, H. Memory-centric
    accelerator design for Convolutional Neural Networks. In 2013 IEEE 31st International
    Conference on Computer Design (ICCD) (2013), pp. 13–19.
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (217) Peemen, M., Setio, A. A. A., Mesman, B., 和 Corporaal, H. 面向卷积神经网络的内存中心加速器设计。载于
    2013 IEEE 第 31 届计算机设计国际会议 (ICCD)（2013年），第 13–19 页。
- en: '(218) Perotti, M., Cavalcante, M., Wistoff, N., Andri, R., Cavigelli, L., and
    Benini, L. A “New Ara” for Vector Computing: An Open Source Highly Efficient RISC-V
    V 1.0 Vector Processor Design. In 2022 IEEE 33rd International Conference on Application-specific
    Systems, Architectures and Processors (ASAP) (July 2022), pp. 43–51.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (218) Perotti, M., Cavalcante, M., Wistoff, N., Andri, R., Cavigelli, L., 和
    Benini, L. “新纪元”的向量计算：一个开源的高效 RISC-V V 1.0 向量处理器设计。载于 2022 IEEE 第 33 届应用系统、架构和处理器国际会议
    (ASAP)（2022年7月），第 43–51 页。
- en: (219) Perri, S., Sestito, C., Spagnolo, F., and Corsonello, P. Efficient Deconvolution
    Architecture for Heterogeneous Systems-on-Chip. Journal of Imaging 6, 9 (2020).
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (219) Perri, S., Sestito, C., Spagnolo, F., 和 Corsonello, P. 面向异构系统芯片的高效去卷积架构。成像期刊
    6, 9 (2020)。
- en: '(220) Petra, N., De Caro, D., Garofalo, V., Napoli, E., and Strollo, A. G. M.
    Design of Fixed-Width Multipliers With Linear Compensation Function. IEEE Transactions
    on Circuits and Systems I: Regular Papers 58, 5 (2011), 947–960.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (220) Petra, N., De Caro, D., Garofalo, V., Napoli, E., 和 Strollo, A. G. M.
    具有线性补偿功能的固定宽度乘法器设计。IEEE 电路与系统 I：常规论文 58, 5 (2011)，第 947–960 页。
- en: '(221) Prabhu, K., Gural, A., Khan, Z. F., Radway, R. M., Giordano, M., Koul,
    K., Doshi, R., Kustin, J. W., Liu, T., Lopes, G. B., et al. CHIMERA: A 0.92-TOPS,
    2.2-TOPS/W edge AI accelerator with 2-MByte on-chip foundry resistive RAM for
    efficient training and inference. IEEE Journal of Solid-State Circuits 57, 4 (2022),
    1013–1026.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (221) Prabhu, K., Gural, A., Khan, Z. F., Radway, R. M., Giordano, M., Koul,
    K., Doshi, R., Kustin, J. W., Liu, T., Lopes, G. B., 等。CHIMERA：一款 0.92-TOPS，2.2-TOPS/W
    的边缘 AI 加速器，配备 2-MByte 的片上工艺电阻 RAM，支持高效训练和推理。IEEE 固态电路期刊 57, 4 (2022)，第 1013–1026
    页。
- en: '(222) Prasad, A., Benini, L., and Conti, F. Specialization Meets Flexibility:
    A Heterogeneous Architecture for High-Efficiency, High-flexibility AR/VR Processing.
    In Proceedings of the 2023 Design Automation Conference (DAC 2023), to Appear
    (2023).'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (222) Prasad, A., Benini, L., 和 Conti, F. 专门化与灵活性的结合：一种用于高效、高灵活性 AR/VR 处理的异构架构。载于
    2023 年设计自动化会议 (DAC 2023) 论文集，待出现（2023年）。
- en: '(223) Prickett, M. The HBM3 roadmap is just getting started. [https://www.nextplatform.com/2022/04/06/the-hbm3-roadmap-is-just-getting-started/](https://www.nextplatform.com/2022/04/06/the-hbm3-roadmap-is-just-getting-started/),
    Apr. 2022. Accessed: 18/04/2023.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (223) Prickett, M. HBM3 路线图刚刚起步。[https://www.nextplatform.com/2022/04/06/the-hbm3-roadmap-is-just-getting-started/](https://www.nextplatform.com/2022/04/06/the-hbm3-roadmap-is-just-getting-started/)，2022年4月。访问日期：2023年4月18日。
- en: '(224) Proud, M. ACHIEVING MAXIMUM COMPUTE THROUGHPUT: PCIE VS. SXM2, May 2018.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (224) Proud, M. 实现最大计算吞吐量：PCIE 与 SXM2，2018年5月。
- en: '(225) Qadeer, W., Hameed, R., Shacham, O., Venkatesan, P., Kozyrakis, C., and
    Horowitz, M. Convolution engine: balancing efficiency and flexibility in specialized
    computing. Communications of the ACM 58, 4 (2015), 85–93.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (225) Qadeer, W., Hameed, R., Shacham, O., Venkatesan, P., Kozyrakis, C., 和
    Horowitz, M. 卷积引擎：在专用计算中平衡效率和灵活性。ACM 通讯 58, 4 (2015)，第 85–93 页。
- en: '(226) Qin, E., Samajdar, A., Kwon, H., Nadella, V., Srinivasan, S., Das, D.,
    Kaul, B., and Krishna, T. SIGMA: A Sparse and Irregular GEMM Accelerator with
    Flexible Interconnects for DNN Training. In 2020 IEEE International Symposium
    on High Performance Computer Architecture (2020), HPCA’20, pp. 58–70.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (226) Qin, E., Samajdar, A., Kwon, H., Nadella, V., Srinivasan, S., Das, D.,
    Kaul, B., 和 Krishna, T. SIGMA：一种具有灵活互连的稀疏和不规则 GEMM 加速器，用于 DNN 训练。见于 2020 IEEE
    高性能计算架构国际研讨会（2020），HPCA’20，第 58–70 页。
- en: '(227) Qin, H., Gong, R., Liu, X., Bai, X., Song, J., and Sebe, N. Binary neural
    networks: A survey. Pattern Recognition 105 (2020), 107281.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (227) Qin, H., Gong, R., Liu, X., Bai, X., Song, J., 和 Sebe, N. 二值神经网络：综述。模式识别
    105 (2020)，107281。
- en: (228) IBM Qiskit Simulator, 2023.
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (228) IBM Qiskit 模拟器，2023。
- en: (229) Qiu, J., Wang, J., Yao, S., Guo, K., Li, B., Zhou, E., Yu, J., Tang, T.,
    Xu, N., Song, S., Wang, Y., and Yang, H. Going Deeper with Embedded FPGA Platform
    for Convolutional Neural Network. 2016 ACM/SIGDA International Symposium on Field-Programmable
    Gate Arrays (2016).
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (229) Qiu, J., Wang, J., Yao, S., Guo, K., Li, B., Zhou, E., Yu, J., Tang, T.,
    Xu, N., Song, S., Wang, Y., 和 Yang, H. 使用嵌入式 FPGA 平台深入研究卷积神经网络。2016 ACM/SIGDA
    国际现场可编程门阵列研讨会（2016）。
- en: (230) Rahman, A., Oh, S., Lee, J., and Choi, K. Design space exploration of
    FPGA accelerators for convolutional neural networks. In Design, Automation & Test
    in Europe Conference & Exhibition (DATE) (2017), pp. 1147–1152.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (230) Rahman, A., Oh, S., Lee, J., 和 Choi, K. 卷积神经网络的 FPGA 加速器设计空间探索。见于设计、自动化与欧洲会议与展览（DATE）（2017），第
    1147–1152 页。
- en: '(231) Rathi, N., Chakraborty, I., Kosta, A., Sengupta, A., Ankit, A., Panda,
    P., and Roy, K. Exploring Neuromorphic Computing Based on Spiking Neural Networks:
    Algorithms to Hardware. ACM Comput. Surv. 55, 12 (2023).'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (231) Rathi, N., Chakraborty, I., Kosta, A., Sengupta, A., Ankit, A., Panda,
    P., 和 Roy, K. 探索基于尖峰神经网络的神经形态计算：从算法到硬件。ACM 计算机调查 55, 12 (2023)。
- en: (232) Reuther, A., Michaleas, P., Jones, M., Gadepally, V., Samsi, S., and Kepner,
    J. AI and ML Accelerator Survey and Trends. In 2022 IEEE High Performance Extreme
    Computing Conference (HPEC) (2022), pp. 1–10.
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (232) Reuther, A., Michaleas, P., Jones, M., Gadepally, V., Samsi, S., 和 Kepner,
    J. AI 和 ML 加速器调查与趋势。见于 2022 IEEE 高性能极限计算会议（HPEC）（2022），第 1–10 页。
- en: '(233) Robinson, C. NVIDIA H100 Hopper Details at HC34 as it Waits for Next-Gen
    CPUs. [https://www.servethehome.com/nvidia-h100-hopper-details-at-hc34-as-it-waits-for-next-gen-cpus/](https://www.servethehome.com/nvidia-h100-hopper-details-at-hc34-as-it-waits-for-next-gen-cpus/),
    Aug. 2022. Accessed: 18/04/2023.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (233) Robinson, C. NVIDIA H100 Hopper 在 HC34 的详细信息，等待下一代 CPU。[https://www.servethehome.com/nvidia-h100-hopper-details-at-hc34-as-it-waits-for-next-gen-cpus/](https://www.servethehome.com/nvidia-h100-hopper-details-at-hc34-as-it-waits-for-next-gen-cpus/)，2022
    年 8 月。访问日期：2023年4月18日。
- en: (234) Rosenblatt, F. The perceptron - A perceiving and recognizing automaton.
    Tech. Rep. 85-460-1, Cornell Aeronautical Laboratory, Ithaca, New York, January
    1957.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (234) Rosenblatt, F. 感知机 - 一种感知和识别自动机。技术报告 85-460-1，康奈尔航空实验室，伊萨卡，纽约，1957 年 1
    月。
- en: '(235) Rossi, D., Conti, F., Eggiman, M., Mauro, A. D., Tagliavini, G., Mach,
    S., Guermandi, M., Pullini, A., Loi, I., Chen, J., Flamand, E., and Benini, L.
    Vega: A Ten-Core SoC for IoT Endnodes With DNN Acceleration and Cognitive Wake-Up
    From MRAM-Based State-Retentive Sleep Mode. IEEE Journal of Solid-State Circuits
    57, 1 (Jan. 2022), 127–139.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (235) Rossi, D., Conti, F., Eggiman, M., Mauro, A. D., Tagliavini, G., Mach,
    S., Guermandi, M., Pullini, A., Loi, I., Chen, J., Flamand, E., 和 Benini, L. Vega：一款用于物联网终端节点的十核
    SoC，具备 DNN 加速和 MRAM 基础的状态保持睡眠模式的认知唤醒功能。IEEE 固态电路学报 57, 1 (2022年1月)，第 127–139 页。
- en: (236) Sankaradas, M., Jakkula, V., Cadambi, S., Chakradhar, S., Durdanovic,
    I., Cosatto, E., and Graf, H. P. A Massively Parallel Coprocessor for Convolutional
    Neural Networks. In 20th IEEE International Conference on Application-Specific
    Systems, Architectures and Processors (USA, 2009), IEEE Computer Society, p. 53–60.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (236) Sankaradas, M., Jakkula, V., Cadambi, S., Chakradhar, S., Durdanovic,
    I., Cosatto, E., 和 Graf, H. P. 一种大规模并行的卷积神经网络协处理器。见于第 20 届 IEEE 应用特定系统、架构和处理器国际会议（USA,
    2009），IEEE 计算机学会，第 53–60 页。
- en: '(237) Schmidhuber, J. Deep Learning in Neural Networks: An Overview. CoRR abs/1404.7828
    (2014).'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (237) Schmidhuber, J. 神经网络中的深度学习：概述。CoRR abs/1404.7828 (2014)。
- en: '(238) Schmidhuber, J. Deep learning in neural networks: An overview. Neural
    Networks 61 (2015), 85–117.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (238) Schmidhuber, J. 神经网络中的深度学习：概述。神经网络 61 (2015)，85–117。
- en: (239) Seo, J.-s., Brezzo, B., Liu, Y., Parker, B. D., Esser, S. K., Montoye,
    R. K., Rajendran, B., Tierno, J. A., Chang, L., Modha, D. S., and Friedman, D. J.
    A 45nm CMOS neuromorphic chip with a scalable architecture for learning in networks
    of spiking neurons. In 2011 IEEE Custom Integrated Circuits Conference (CICC)
    (2011), pp. 1–4.
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (239) Seo, J.-s., Brezzo, B., Liu, Y., Parker, B. D., Esser, S. K., Montoye,
    R. K., Rajendran, B., Tierno, J. A., Chang, L., Modha, D. S. 和 Friedman, D. J.
    一款45nm CMOS神经形态芯片，具有可扩展的架构，用于脉冲神经网络的学习。在2011年IEEE定制集成电路会议（CICC）（2011），第1–4页。
- en: (240) Sestito, C., Spagnolo, F., and Perri, S. Design of Flexible Hardware Accelerators
    for Image Convolutions and Transposed Convolutions. Journal of Imaging 7, 10 (2021).
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (240) Sestito, C.、Spagnolo, F. 和 Perri, S. 设计用于图像卷积和转置卷积的灵活硬件加速器。《成像期刊》7, 10
    (2021)。
- en: '(241) Shafiee, A., Nag, A., Muralimanohar, N., Balasubramonian, R., Strachan,
    J. P., Hu, M., Williams, R. S., and Srikumar, V. ISAAC: A Convolutional Neural
    Network Accelerator with in-Situ Analog Arithmetic in Crossbars. In 43rd International
    Symposium on Computer Architecture (2016), p. 14–26.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (241) Shafiee, A., Nag, A., Muralimanohar, N., Balasubramonian, R., Strachan,
    J. P., Hu, M., Williams, R. S. 和 Srikumar, V. ISAAC：一种具有现场模拟算术功能的卷积神经网络加速器。在第43届国际计算机架构研讨会（2016），第14–26页。
- en: (242) Shan, J., Casu, M. R., Cortadella, J., Lavagno, L., and Lazarescu, M. T.
    Exact and Heuristic Allocation of MuIti-kernel Applications to Multi-FPGA Platforms.
    In 2019 56th ACM/IEEE Design Automation Conference (DAC) (2019), pp. 1–6.
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (242) Shan, J., Casu, M. R., Cortadella, J., Lavagno, L. 和 Lazarescu, M. T.
    精确和启发式分配多核应用于多FPGA平台。在2019年第56届ACM/IEEE设计自动化会议（DAC）（2019），第1–6页。
- en: '(243) Shao, Y. S., Clemons, J., Venkatesan, R., Zimmer, B., Fojtik, M., Jiang,
    N., Keller, B., Klinefelter, A., Pinckney, N., Raina, P., Tell, S. G., Zhang,
    Y., Dally, W. J., Emer, J., Gray, C. T., Khailany, B., and Keckler, S. W. Simba:
    Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture. In
    52nd Annual IEEE/ACM International Symposium on Microarchitecture (Oct. 2019),
    Association for Computing Machinery, pp. 14–27.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (243) Shao, Y. S., Clemons, J., Venkatesan, R., Zimmer, B., Fojtik, M., Jiang,
    N., Keller, B., Klinefelter, A., Pinckney, N., Raina, P., Tell, S. G., Zhang,
    Y., Dally, W. J., Emer, J., Gray, C. T., Khailany, B. 和 Keckler, S. W. Simba：通过多芯片模块架构扩展深度学习推断。在第52届年IEEE/ACM国际微体系结构研讨会（2019年10月），计算机协会，第14–27页。
- en: '(244) Sharma, H., Mandal, S. K., Doppa, J. R., Ogras, U. Y., and Pande, P. P.
    SWAP: A Server-Scale Communication-Aware Chiplet-Based Manycore PIM Accelerator.
    IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
    41, 11 (2022), 4145–4156.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (244) Sharma, H., Mandal, S. K., Doppa, J. R., Ogras, U. Y. 和 Pande, P. P. SWAP：一种服务器级通信感知芯片化多核PIM加速器。《IEEE计算机辅助设计集成电路与系统学报》41,
    11 (2022)，第4145–4156页。
- en: (245) Siemens. Catapult C++/Systemc Synthesis, 2022.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (245) Siemens. Catapult C++/Systemc综合，2022。
- en: (246) Smagulova, K., Fouda, M. E., Kurdahi, F., Salama, K. N., and Eltawil,
    A. Resistive Neural Hardware Accelerators. Proceedings of the IEEE 111, 5 (2023),
    500–527.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (246) Smagulova, K., Fouda, M. E., Kurdahi, F., Salama, K. N. 和 Eltawil, A.
    电阻性神经硬件加速器。《IEEE学报》111, 5 (2023)，第500–527页。
- en: (247) Sohn, K., Yun, W.-J., Oh, R., Oh, C.-S., Seo, S.-Y., Park, M.-S., Shin,
    D.-H., Jung, W.-C., Shin, S.-H., Ryu, J.-M., Yu, H.-S., Jung, J.-H., Lee, H.,
    Kang, S.-Y., Sohn, Y.-S., Choi, J.-H., Bae, Y.-C., Jang, S.-J., and Jin, G. A
    1.2 V 20 nm 307 GB/s HBM DRAM With At-Speed Wafer-Level IO Test Scheme and Adaptive
    Refresh Considering Temperature Distribution. IEEE Journal of Solid-State Circuits
    52, 1 (2017), 250–260.
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (247) Sohn, K., Yun, W.-J., Oh, R., Oh, C.-S., Seo, S.-Y., Park, M.-S., Shin,
    D.-H., Jung, W.-C., Shin, S.-H., Ryu, J.-M., Yu, H.-S., Jung, J.-H., Lee, H.,
    Kang, S.-Y., Sohn, Y.-S., Choi, J.-H., Bae, Y.-C., Jang, S.-J. 和 Jin, G. 一款1.2
    V 20 nm 307 GB/s HBM DRAM，具有高速晶圆级IO测试方案和考虑温度分布的自适应刷新功能。《IEEE固态电路期刊》52, 1 (2017)，第250–260页。
- en: (248) Song, J., Cho, Y., Park, J.-S., Jang, J.-W., Lee, S., Song, J.-H., Lee,
    J.-G., and Kang, I. An 11.5TOPS/W 1024-MAC Butterfly Structure Dual-Core Sparsity-Aware
    Neural Processing Unit in 8nm Flagship Mobile SoC. In 2019 IEEE International
    Solid- State Circuits Conference - (ISSCC) (2019), pp. 130–132.
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (248) Song, J., Cho, Y., Park, J.-S., Jang, J.-W., Lee, S., Song, J.-H., Lee,
    J.-G. 和 Kang, I. 一款11.5TOPS/W 1024-MAC蝶形结构双核稀疏感知神经处理单元，采用8nm旗舰移动SoC。在2019年IEEE国际固态电路会议（ISSCC）（2019），第130–132页。
- en: '(249) Song, L., Qian, X., Li, H., and Chen, Y. PipeLayer: A Pipelined ReRAM-Based
    Accelerator for Deep Learning. In 2017 IEEE International Symposium on High Performance
    Computer Architecture (HPCA) (2017), pp. 541–552.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (249) Song, L., Qian, X., Li, H. 和 Chen, Y. PipeLayer：一种基于ReRAM的深度学习流水线加速器。在2017年IEEE国际高性能计算架构研讨会（HPCA）（2017），第541–552页。
- en: '(250) Soussan, P., Sabuncuoglu Tezcan, D., Iker, F., Ruythooren, W., Swinnen,
    B., Majeed, B., and Beyne, E. 3D Wafer Level Packaging: Processes and Materials
    for Trough Silicon Vias & Thin Die Embedding. In MRS Online Proceedings Library
    (01 2008), vol. 1112.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (250) Soussan, P., Sabuncuoglu Tezcan, D., Iker, F., Ruythooren, W., Swinnen,
    B., Majeed, B., 和 Beyne, E. 3D晶圆级封装：硅通孔和薄芯片嵌入的工艺与材料。在MRS在线会议文献库 (01 2008)，第1112卷。
- en: (251) Spagnolo, F., Corsonello, P., Frustaci, F., and Perri, S. Design of a
    Low-Power Super-Resolution Architecture for Virtual Reality Wearable Devices.
    IEEE Sensors Journal 23, 8 (2023), 9009–9016.
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (251) Spagnolo, F., Corsonello, P., Frustaci, F., 和 Perri, S. 低功耗超分辨率架构设计用于虚拟现实可穿戴设备。《IEEE
    Sensors Journal》23，第8期 (2023)，9009–9016。
- en: (252) Spagnolo, F., Perri, S., and Corsonello, P. Design of a real-time face
    detection architecture for heterogeneous systems-on-chips. Integration 74 (2020),
    1–10.
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (252) Spagnolo, F., Perri, S., 和 Corsonello, P. 针对异构系统芯片的实时人脸检测架构设计。《Integration》74
    (2020)，1–10。
- en: '(253) Spagnolo, F., Perri, S., and Corsonello, P. Aggressive Approximation
    of the SoftMax Function for Power-Efficient Hardware Implementations. IEEE Transactions
    on Circuits and Systems II: Express Briefs 69, 3 (2022), 1652–1656.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(253) Spagnolo, F., Perri, S., 和 Corsonello, P. 针对功效高效硬件实现的SoftMax函数的激进近似。《IEEE
    Transactions on Circuits and Systems II: Express Briefs》69，第3期 (2022)，1652–1656。'
- en: (254) Spagnolo, F., Perri, S., and Corsonello, P. Approximate Down-Sampling
    Strategy for Power-Constrained Intelligent Systems. IEEE Access 10 (2022), 7073–7081.
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (254) Spagnolo, F., Perri, S., 和 Corsonello, P. 针对功耗受限智能系统的近似降采样策略。《IEEE Access》10
    (2022)，7073–7081。
- en: (255) Srinivas, S., and Babu, R. V. Data-free Parameter Pruning for Deep Neural
    Networks. CoRR abs/1507.06149 (2015).
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (255) Srinivas, S., 和 Babu, R. V. 深度神经网络的数据无关参数剪枝。CoRR abs/1507.06149 (2015)。
- en: (256) Sriram, V., Cox, D., Tsoi, K., and Luk, W. Towards an embedded biologically-inspired
    machine vision processor. In 2010 International Conference on Field-Programmable
    Technology (01 2011), pp. 273–278.
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (256) Sriram, V., Cox, D., Tsoi, K., 和 Luk, W. 朝着嵌入式生物启发式机器视觉处理器的方向发展。在2010年国际现场可编程技术会议
    (01 2011)，第273–278页。
- en: (257) Stow, D., Akgun, I., Barnes, R., Gu, P., and Xie, Y. Cost analysis and
    cost-driven IP reuse methodology for SoC design based on 2.5D/3D integration.
    In 2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD) (2016).
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (257) Stow, D., Akgun, I., Barnes, R., Gu, P., 和 Xie, Y. 基于2.5D/3D集成的SoC设计的成本分析与成本驱动IP重用方法。在2016
    IEEE/ACM国际计算机辅助设计会议 (ICCAD) (2016)。
- en: (258) Stow, D., Xie, Y., Siddiqua, T., and Loh, G. H. Cost-effective design
    of scalable high-performance systems using active and passive interposers. In
    2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD) (2017),
    pp. 728–735.
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (258) Stow, D., Xie, Y., Siddiqua, T., 和 Loh, G. H. 使用主动和被动插层的可扩展高性能系统的成本效益设计。在2017
    IEEE/ACM国际计算机辅助设计会议 (ICCAD) (2017)，第728–735页。
- en: '(259) Strollo, A. G. M., Napoli, E., De Caro, D., Petra, N., and Di Meo, G.
    Comparison and Extension of Approximate 4-2 Compressors for Low-Power Approximate
    Multipliers. IEEE Transactions on Circuits and Systems I: Regular Papers 67, 9
    (2020), 3021–3034.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(259) Strollo, A. G. M., Napoli, E., De Caro, D., Petra, N., 和 Di Meo, G. 低功耗近似乘法器的近似4-2压缩器的比较与扩展。《IEEE
    Transactions on Circuits and Systems I: Regular Papers》67，第9期 (2020)，3021–3034。'
- en: '(260) Strollo, A. G. M., Napoli, E., De Caro, D., Petra, N., Saggese, G., and
    Di Meo, G. Approximate Multipliers Using Static Segmentation: Error Analysis and
    Improvements. IEEE Transactions on Circuits and Systems I: Regular Papers 69,
    6 (2022), 2449–2462.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(260) Strollo, A. G. M., Napoli, E., De Caro, D., Petra, N., Saggese, G., 和
    Di Meo, G. 使用静态分段的近似乘法器：误差分析与改进。《IEEE Transactions on Circuits and Systems I:
    Regular Papers》69，第6期 (2022)，2449–2462。'
- en: '(261) Sze, V., Chen, Y. H., Yang, T. J., and Emer, J. S. Efficient Processing
    of Deep Neural Networks: A Tutorial and Survey. Proceedings of the IEEE 105, 12
    (2017), 2295–2329.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (261) Sze, V., Chen, Y. H., Yang, T. J., 和 Emer, J. S. 深度神经网络的高效处理：教程与调查。《IEEE》105，第12期
    (2017)，2295–2329。
- en: (262) Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan,
    D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In 2015
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015), pp. 1–9.
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (262) Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan,
    D., Vanhoucke, V., 和 Rabinovich, A. 深入卷积研究。在2015 IEEE计算机视觉与模式识别会议 (CVPR) (2015)，第1–9页。
- en: (263) Tambe, T., Zhang, J., Hooper, C., Jia, T., Whatmough, P. N., Zuckerman,
    J., Santos, M. C. D., Loscalzo, E. J., Giri, D., Shepard, K., Carloni, L., Rush,
    A., Brooks, D., and Wei, G.-Y. 22.9 A 12nm 18.1TFLOPs/W Sparse Transformer Processor
    with Entropy-Based Early Exit, Mixed-Precision Predication and Fine-Grained Power
    Management. In 2023 IEEE International Solid- State Circuits Conference (ISSCC)
    (San Francisco, CA, USA, Feb. 2023), IEEE, pp. 342–344.
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (263) Tambe, T., Zhang, J., Hooper, C., Jia, T., Whatmough, P. N., Zuckerman,
    J., Santos, M. C. D., Loscalzo, E. J., Giri, D., Shepard, K., Carloni, L., Rush,
    A., Brooks, D., 和 Wei, G.-Y. 22.9 一款12nm 18.1TFLOPs/W 稀疏 Transformer 处理器，具有基于熵的早期退出、混合精度预测和细粒度功率管理。在2023年IEEE国际固态电路会议（ISSCC）（美国旧金山，2023年2月），IEEE，页342–344。
- en: '(264) Tang, W., and Zhang, P. GPGCN: A General-Purpose Graph Convolution Neural
    Network Accelerator Based on RISC-V ISA Extension. Electronics 11, 22 (2022).'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (264) Tang, W., 和 Zhang, P. GPGCN：一种基于 RISC-V ISA 扩展的通用图卷积神经网络加速器。Electronics
    11, 22 (2022)。
- en: '(265) Tortorella, Y., Bertaccini, L., Benini, L., Rossi, D., and Conti, F.
    RedMule: A Mixed-Precision Matrix-Matrix Operation Engine for Flexible and Energy-Efficient
    On-Chip Linear Algebra and TinyML Training Acceleration. CoRR abs/2301.03904,
    arXiv:2301.03904 (Jan. 2023).'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (265) Tortorella, Y., Bertaccini, L., Benini, L., Rossi, D., 和 Conti, F. RedMule：一种混合精度的矩阵-矩阵运算引擎，用于灵活且节能的片上线性代数和
    TinyML 训练加速。CoRR abs/2301.03904, arXiv:2301.03904（2023年1月）。
- en: '(266) Tortorella, Y., Bertaccini, L., Rossi, D., Benini, L., and Conti, F.
    RedMulE: A Compact FP16 Matrix-Multiplication Accelerator for Adaptive Deep Learning
    on RISC-V-based Ultra-Low-Power SoCs. In 2022 Conference & Exhibition on Design,
    Automation & Test in Europe (May 2022), European Design and Automation Association,
    pp. 1099–1102.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (266) Tortorella, Y., Bertaccini, L., Rossi, D., Benini, L., 和 Conti, F. RedMulE：一种紧凑的
    FP16 矩阵乘法加速器，用于基于 RISC-V 的超低功耗 SoC 上的自适应深度学习。2022年欧洲设计、自动化与测试会议暨展览（2022年5月），欧洲设计与自动化协会，页1099–1102。
- en: '(267) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., and Lample, G. LLaMA: Open and Efficient Foundation Language Models,
    Feb. 2023.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (267) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., 和 Lample, G. LLaMA：开源高效的基础语言模型，2023年2月。
- en: '(268) Trabelsi Ajili, M., and Hara-Azumi, Y. Multimodal Neural Network Acceleration
    on a Hybrid CPU-FPGA Architecture: A Case Study. IEEE Access 10 (2022), 9603–9617.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (268) Trabelsi Ajili, M., 和 Hara-Azumi, Y. 基于混合 CPU-FPGA 架构的多模态神经网络加速：一个案例研究。IEEE
    Access 10 (2022), 9603–9617。
- en: '(269) Tsao, Y.-C., and Choi, K. Area-Efficient VLSI Implementation for Parallel
    Linear-Phase FIR Digital Filters of Odd Length Based on Fast FIR Algorithm. IEEE
    Transactions on Circuits and Systems II: Express Briefs 59, 6 (2012), 371–375.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (269) Tsao, Y.-C., 和 Choi, K. 面积高效的基于快速 FIR 算法的奇数长度并行线性相位 FIR 数字滤波器的 VLSI 实现。IEEE《电路与系统
    II：快速通讯》59, 6 (2012), 371–375。
- en: '(270) Umuroglu, Y., Fraser, N. J., Gambardella, G., Blott, M., Leong, P., Jahre,
    M., and Vissers, K. FINN: A Framework for Fast, Scalable Binarized Neural Network
    Inference. In 2017 ACM/SIGDA International Symposium on Field-Programmable Gate
    Arrays (2017), ACM, p. 65–74.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (270) Umuroglu, Y., Fraser, N. J., Gambardella, G., Blott, M., Leong, P., Jahre,
    M., 和 Vissers, K. FINN：一个快速、可扩展的二值神经网络推理框架。在2017年ACM/SIGDA现场可编程门阵列国际研讨会（2017年），ACM，页65–74。
- en: '(271) Upama, P. B., Faruk, M. J. H., Nazim, M., Masum, M., Shahriar, H., Uddin,
    G., Barzanjeh, S., Ahamed, S. I., and Rahman, A. Evolution of Quantum Computing:
    A Systematic Survey on the Use of Quantum Computing Tools, 2022.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (271) Upama, P. B., Faruk, M. J. H., Nazim, M., Masum, M., Shahriar, H., Uddin,
    G., Barzanjeh, S., Ahamed, S. I., 和 Rahman, A. 量子计算的发展：对量子计算工具使用的系统调查，2022年。
- en: (272) Ushiroyama, A., Watanabe, M., Watanabe, N., and Nagoya, A. Convolutional
    neural network implementations using Vitis AI. In 2022 IEEE 12th Annual Computing
    and Communication Workshop and Conference (CCWC) (2022), pp. 0365–0371.
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (272) Ushiroyama, A., Watanabe, M., Watanabe, N., 和 Nagoya, A. 使用 Vitis AI 的卷积神经网络实现。在2022年IEEE第12届年度计算与通信研讨会暨会议（CCWC）（2022年），页0365–0371。
- en: '(273) Vahdat, S., Kamal, M., Afzali-Kusha, A., and Pedram, M. TOSAM: An Energy-Efficient
    Truncation- and Rounding-Based Scalable Approximate Multiplier. IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems 27, 5 (2019), 1161–1173.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (273) Vahdat, S., Kamal, M., Afzali-Kusha, A., 和 Pedram, M. TOSAM：一种节能的基于截断和四舍五入的可扩展近似乘法器。IEEE《大规模集成（VLSI）系统学报》27,
    5 (2019), 1161–1173。
- en: (274) Vandendriessche, J., Da Silva, B., and Touhafi, A. Frequency Evaluation
    of the Xilinx DPU Towards Energy Efficiency. In IECON 2022 – 48th Annual Conference
    of the IEEE Industrial Electronics Society (2022), pp. 1–6.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (274) Vandendriessche, J., Da Silva, B., 和 Touhafi, A. Xilinx DPU 频率评估以实现能效。在
    IECON 2022 – IEEE 工业电子学会第48届年会（2022年），第1–6页。
- en: (275) Vasiljevic, J., Bajic, L., Capalija, D., Sokorac, S., Ignjatovic, D.,
    Bajic, L., Trajkovic, M., Hamer, I., Matosevic, I., Cejkov, A., Aydonat, U., Zhou,
    T., Gilani, S. Z., Paiva, A., Chu, J., Maksimovic, D., Chin, S. A., Moudallal,
    Z., Rakhmati, A., Nijjar, S., Bhullar, A., Drazic, B., Lee, C., Sun, J., Kwong,
    K.-M., Connolly, J., Dooley, M., Farooq, H., Chen, J. Y. T., Walker, M., Dabiri,
    K., Mabee, K., Lal, R. S., Rajatheva, N., Retnamma, R., Karodi, S., Rosen, D.,
    Munoz, E., Lewycky, A., Knezevic, A., Kim, R., Rui, A., Drouillard, A., and Thompson,
    D. Compute Substrate for Software 2.0. IEEE Micro 41, 2 (Mar. 2021), 50–55.
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (275) Vasiljevic, J., Bajic, L., Capalija, D., Sokorac, S., Ignjatovic, D.,
    Bajic, L., Trajkovic, M., Hamer, I., Matosevic, I., Cejkov, A., Aydonat, U., Zhou,
    T., Gilani, S. Z., Paiva, A., Chu, J., Maksimovic, D., Chin, S. A., Moudallal,
    Z., Rakhmati, A., Nijjar, S., Bhullar, A., Drazic, B., Lee, C., Sun, J., Kwong,
    K.-M., Connolly, J., Dooley, M., Farooq, H., Chen, J. Y. T., Walker, M., Dabiri,
    K., Mabee, K., Lal, R. S., Rajatheva, N., Retnamma, R., Karodi, S., Rosen, D.,
    Munoz, E., Lewycky, A., Knezevic, A., Kim, R., Rui, A., Drouillard, A., 和 Thompson,
    D. 软件2.0的计算基板。《IEEE Micro》41卷，2期（2021年3月），50–55页。
- en: (276) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., Kaiser, Ł., and Polosukhin, I. Attention is All you Need. In Advances in
    Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio,
    H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. Curran Associates,
    Inc., 2017, pp. 5998–6008.
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (276) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., Kaiser, Ł., 和 Polosukhin, I. 注意力机制是你所需要的一切。在《神经信息处理系统进展 30》，I. Guyon, U.
    V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan 和 R. Garnett 编。Curran
    Associates, Inc., 2017年，第5998–6008页。
- en: '(277) Venieris, S. I., and Bouganis, C.-S. fpgaConvNet: Mapping Regular and
    Irregular Convolutional Neural Networks on FPGAs. IEEE Transactions on Neural
    Networks and Learning Systems 30, 2 (2019), 326–342.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (277) Venieris, S. I., 和 Bouganis, C.-S. fpgaConvNet：在FPGAs上映射规则和不规则卷积神经网络。《IEEE
    神经网络与学习系统汇刊》30卷，2期（2019年），326–342页。
- en: '(278) Venieris, S. I., Kouris, A., and Bouganis, C.-S. Toolflows for mapping
    convolutional neural networks on FPGAs: A survey and future directions. ACM Computing
    Surveys (CSUR) 51, 3 (2018), 1–39.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (278) Venieris, S. I., Kouris, A., 和 Bouganis, C.-S. 在FPGAs上映射卷积神经网络的工具流：综述与未来方向。《ACM计算调查（CSUR）》51卷，3期（2018年），1–39页。
- en: '(279) Venkataramani, S., Srinivasan, V., Wang, W., Sen, S., Zhang, J., Agrawal,
    A., Kar, M., Jain, S., Mannari, A., Tran, H., Li, Y., Ogawa, E., Ishizaki, K.,
    Inoue, H., Schaal, M., Serrano, M., Choi, J., Sun, X., Wang, N., Chen, C.-Y.,
    Allain, A., Bonano, J., Cao, N., Casatuta, R., Cohen, M., Fleischer, B., Guillorn,
    M., Haynie, H., Jung, J., Kang, M., Kim, K.-h., Koswatta, S., Lee, S., Lutz, M.,
    Mueller, S., Oh, J., Ranjan, A., Ren, Z., Rider, S., Schelm, K., Scheuermann,
    M., Silberman, J., Yang, J., Zalani, V., Zhang, X., Zhou, C., Ziegler, M., Shah,
    V., Ohara, M., Lu, P.-F., Curran, B., Shukla, S., Chang, L., and Gopalakrishnan,
    K. RaPiD: AI Accelerator for Ultra-low Precision Training and Inference. In 2021
    ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) (June
    2021), pp. 153–166.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (279) Venkataramani, S., Srinivasan, V., Wang, W., Sen, S., Zhang, J., Agrawal,
    A., Kar, M., Jain, S., Mannari, A., Tran, H., Li, Y., Ogawa, E., Ishizaki, K.,
    Inoue, H., Schaal, M., Serrano, M., Choi, J., Sun, X., Wang, N., Chen, C.-Y.,
    Allain, A., Bonano, J., Cao, N., Casatuta, R., Cohen, M., Fleischer, B., Guillorn,
    M., Haynie, H., Jung, J., Kang, M., Kim, K.-h., Koswatta, S., Lee, S., Lutz, M.,
    Mueller, S., Oh, J., Ranjan, A., Ren, Z., Rider, S., Schelm, K., Scheuermann,
    M., Silberman, J., Yang, J., Zalani, V., Zhang, X., Zhou, C., Ziegler, M., Shah,
    V., Ohara, M., Lu, P.-F., Curran, B., Shukla, S., Chang, L., 和 Gopalakrishnan,
    K. RaPiD：超低精度训练和推理的AI加速器。在2021年 ACM/IEEE 第48届国际计算机体系结构年会（ISCA）（2021年6月），第153–166页。
- en: '(280) Ventana Micro. [https://www.ventanamicro.com/](https://www.ventanamicro.com/),
    2023. Accessed: 2023-04-18.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (280) Ventana Micro. [https://www.ventanamicro.com/](https://www.ventanamicro.com/)，2023年。访问时间：2023年4月18日。
- en: '(281) Verhelst, M., Shi, M., and Mei, L. ML Processors Are Going Multi-Core:
    A performance dream or a scheduling nightmare? IEEE Solid-State Circuits Magazine
    14, 4 (2022), 18–27.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (281) Verhelst, M., Shi, M., 和 Mei, L. ML 处理器正在走向多核：性能梦还是调度噩梦？《IEEE 固态电路杂志》14卷，4期（2022年），18–27页。
- en: (282) Vijayaraghavan, T., Eckert, Y., Loh, G. H., Schulte, M. J., Ignatowski,
    M., Beckmann, B. M., Brantley, W. C., Greathouse, J. L., Huang, W., Karunanithi,
    A., Kayiran, O., Meswani, M., Paul, I., Poremba, M., Raasch, S., Reinhardt, S. K.,
    Sadowski, G., and Sridharan, V. Design and Analysis of an APU for Exascale Computing.
    In 2017 IEEE International Symposium on High Performance Computer Architecture
    (HPCA) (2017), pp. 85–96.
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (282) Vijayaraghavan, T., Eckert, Y., Loh, G. H., Schulte, M. J., Ignatowski,
    M., Beckmann, B. M., Brantley, W. C., Greathouse, J. L., Huang, W., Karunanithi,
    A., Kayiran, O., Meswani, M., Paul, I., Poremba, M., Raasch, S., Reinhardt, S.
    K., Sadowski, G., and Sridharan, V. 设计与分析用于极大规模计算的APU。载于2017 IEEE国际高性能计算机架构研讨会（HPCA）（2017年），第85–96页。
- en: '(283) Vivet, P., Guthmuller, E., Thonnart, Y., Pillonnet, G., Fuguet, C., Miro-Panades,
    I., Moritz, G., Durupt, J., Bernard, C., Varreau, D., Pontes, J., Thuries, S.,
    Coriat, D., Harrand, M., Dutoit, D., Lattard, D., Arnaud, L., Charbonnier, J.,
    Coudrain, P., Garnier, A., Berger, F., Gueugnot, A., Greiner, A., Meunier, Q. L.,
    Farcy, A., Arriordaz, A., Chéramy, S., and Clermidy, F. IntAct: A 96-Core Processor
    With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects
    and Integrated Power Management. IEEE Journal of Solid-State Circuits 56, 1 (2021),
    79–97.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(283) Vivet, P., Guthmuller, E., Thonnart, Y., Pillonnet, G., Fuguet, C., Miro-Panades,
    I., Moritz, G., Durupt, J., Bernard, C., Varreau, D., Pontes, J., Thuries, S.,
    Coriat, D., Harrand, M., Dutoit, D., Lattard, D., Arnaud, L., Charbonnier, J.,
    Coudrain, P., Garnier, A., Berger, F., Gueugnot, A., Greiner, A., Meunier, Q.
    L., Farcy, A., Arriordaz, A., Chéramy, S., and Clermidy, F. IntAct: 一种带有六个芯片集成在有源中介板上的96核处理器，具有分布式互连和集成电源管理功能。IEEE固态电路期刊56,
    1（2021年），79–97页。'
- en: (284) Wan, W., Kubendran, R., Schaefer, C., Eryilmaz, S. B., Zhang, W., Wu,
    D., Deiss, S., Raina, P., Qian, H., Gao, B., Joshi, S., Wu, H., Wong, H.-S. P.,
    and Cauwenberghs, G. A compute-in-memory chip based on resistive random-access
    memory. Nature 608, 7923 (Aug 2022), 504–512.
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (284) Wan, W., Kubendran, R., Schaefer, C., Eryilmaz, S. B., Zhang, W., Wu,
    D., Deiss, S., Raina, P., Qian, H., Gao, B., Joshi, S., Wu, H., Wong, H.-S. P.,
    and Cauwenberghs, G. 一种基于电阻随机存取存储器的计算内存芯片。自然 608, 7923（2022年8月），504–512页。
- en: '(285) Wang, H., Xu, W., Zhang, Z., You, X., and Zhang, C. An Efficient Stochastic
    Convolution Architecture Based on Fast FIR Algorithm. IEEE Transactions on Circuits
    and Systems II: Express Briefs 69, 3 (2022), 984–988.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (285) Wang, H., Xu, W., Zhang, Z., You, X., and Zhang, C. 基于快速FIR算法的高效随机卷积架构。IEEE电路与系统II期刊：快速简报69,
    3（2022年），984–988页。
- en: (286) Wang, J., and Gu, S. FPGA Implementation of Object Detection Accelerator
    Based on Vitis-AI. In 2021 11th International Conference on Information Science
    and Technology (ICIST) (2021), pp. 571–577.
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (286) Wang, J., and Gu, S. 基于Vitis-AI的对象检测加速器的FPGA实现。载于2021年第11届信息科学与技术国际会议（ICIST）（2021年），第571–577页。
- en: '(287) Wang, J., Lin, J., and Wang, Z. Efficient Hardware Architectures for
    Deep Convolutional Neural Network. IEEE Transactions on Circuits and Systems I:
    Regular Papers 65, 6 (2018), 1941–1953.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (287) Wang, J., Lin, J., and Wang, Z. 深度卷积神经网络的高效硬件架构。IEEE电路与系统I期刊：常规论文65, 6（2018年），1941–1953页。
- en: (288) Wang, S., Zhu, J., Wang, Q., He, C., and Ye, T. T. Customized Instruction
    on RISC-V for Winograd-Based Convolution Acceleration. In 2021 IEEE 32nd International
    Conference on Application-specific Systems, Architectures and Processors (ASAP)
    (2021), pp. 65–68.
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (288) Wang, S., Zhu, J., Wang, Q., He, C., and Ye, T. T. 针对Winograd卷积加速的RISC-V定制指令。载于2021年IEEE第32届应用特定系统、架构和处理器国际会议（ASAP）（2021年），第65–68页。
- en: '(289) Wang, Y., Lin, J., and Wang, Z. FPAP: A Folded Architecture for Energy-Quality
    Scalable Convolutional Neural Networks. IEEE Transactions on Circuits and Systems
    I: Regular Papers 66 (2019), 288–301.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(289) Wang, Y., Lin, J., and Wang, Z. FPAP: 一种用于能源-质量可扩展卷积神经网络的折叠架构。IEEE电路与系统I期刊：常规论文66（2019年），288–301页。'
- en: (290) Ward-Foxton, S. Axelera Demos AI Test Chip After Taping Out in Four Months,
    2022.
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (290) Ward-Foxton, S. Axelera在四个月内完成AI测试芯片的演示，2022年。
- en: (291) Warden, P., and Situnayake, D. TinyML. O’Reilly Media, Inc., 2019.
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (291) Warden, P., and Situnayake, D. TinyML。O'Reilly Media, Inc., 2019年。
- en: (292) Wei, X., Yu, C. H., Zhang, P., Chen, Y., Wang, Y., Hu, H., Liang, Y.,
    and Cong, J. Automated systolic array architecture synthesis for high throughput
    CNN inference on FPGAs. In 2017 54th ACM/EDAC/IEEE Design Automation Conference
    (DAC) (2017), pp. 1–6.
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (292) Wei, X., Yu, C. H., Zhang, P., Chen, Y., Wang, Y., Hu, H., Liang, Y.,
    and Cong, J. 针对FPGA的高吞吐量CNN推理的自动化系统阵列架构综合。载于2017年第54届ACM/EDAC/IEEE设计自动化会议（DAC）（2017年），第1–6页。
- en: (293) Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H. Learning Structured Sparsity
    in Deep Neural Networks. In Proceedings of the 30th International Conference on
    Neural Information Processing Systems (Red Hook, NY, USA, 2016), NIPS’16, Curran
    Associates Inc., pp. 2082–2090.
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (293) Wen, W., Wu, C., Wang, Y., Chen, Y., 和 Li, H. 深度神经网络中的结构稀疏学习。发表于第30届国际神经信息处理系统大会（Red
    Hook, NY, USA, 2016），NIPS’16，Curran Associates Inc.，第2082–2090页。
- en: (294) Xilinx Inc. Vitis High-Level Synthesis User Guide, 2022.
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (294) Xilinx Inc. Vitis 高级合成用户指南，2022年。
- en: '(295) Xuan, Z. Y., Lee, C.-J., and Yeh, T. T. Lego: Dynamic Tensor-Splitting
    Multi-Tenant DNN Models on Multi-Chip-Module Architecture. In 2022 19th International
    SoC Design Conference (ISOCC) (2022), pp. 173–174.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(295) Xuan, Z. Y., Lee, C.-J., 和 Yeh, T. T. Lego: 动态张量拆分多租户 DNN 模型在多芯片模块架构上。发表于
    2022 第19届国际 SoC 设计会议（ISOCC）（2022），第173–174页。'
- en: (296) Xue, C.-X., Chen, W.-H., Liu, J.-S., Li, J.-F., Lin, W.-Y., Lin, W.-E.,
    Wang, J.-H., Wei, W.-C., Chang, T.-W., Chang, T.-C., Huang, T.-Y., Kao, H.-Y.,
    Wei, S.-Y., Chiu, Y.-C., Lee, C.-Y., Lo, C.-C., King, Y.-C., Lin, C.-J., Liu,
    R.-S., Hsieh, C.-C., Tang, K.-T., and Chang, M.-F. A 1Mb Multibit ReRAM Computing-In-Memory
    Macro with 14.6ns Parallel MAC Computing Time for CNN Based AI Edge Processors.
    In 2019 IEEE International Solid- State Circuits Conference - (ISSCC) (2019),
    pp. 388–390.
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (296) Xue, C.-X., Chen, W.-H., Liu, J.-S., Li, J.-F., Lin, W.-Y., Lin, W.-E.,
    Wang, J.-H., Wei, W.-C., Chang, T.-W., Chang, T.-C., Huang, T.-Y., Kao, H.-Y.,
    Wei, S.-Y., Chiu, Y.-C., Lee, C.-Y., Lo, C.-C., King, Y.-C., Lin, C.-J., Liu,
    R.-S., Hsieh, C.-C., Tang, K.-T., 和 Chang, M.-F. 一个 1Mb 多位 ReRAM 内存计算宏，具有 14.6ns
    的 CNN 基于 AI 边缘处理器的并行 MAC 计算时间。发表于 2019 IEEE 国际固态电路会议 - (ISSCC)（2019），第388–390页。
- en: (297) Yang, Z., Han, J., and Lombardi, F. Approximate compressors for error-resilient
    multiplier design. In 2015 IEEE International Symposium on Defect and Fault Tolerance
    in VLSI and Nanotechnology Systems (DFTS) (2015), pp. 183–186.
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (297) Yang, Z., Han, J., 和 Lombardi, F. 面向错误恢复的乘法器设计的近似压缩器。发表于2015年IEEE国际VLSI与纳米技术系统缺陷与故障容忍研讨会（DFTS）（2015），第183–186页。
- en: '(298) Yazdanbakhsh, A., Samadi, K., Kim, N. S., Esmaeilzadeh, H., Falahati,
    H., and Wolfe, P. J. GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial
    Networks. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture
    (ISCA) (2018), pp. 650–661.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(298) Yazdanbakhsh, A., Samadi, K., Kim, N. S., Esmaeilzadeh, H., Falahati,
    H., 和 Wolfe, P. J. GANAX: 生成对抗网络的统一 MIMD-SIMD 加速。发表于 2018 ACM/IEEE 第45届国际计算机架构研讨会（ISCA）（2018），第650–661页。'
- en: '(299) Ye, H., Jun, H., Jeong, H., Neuendorffer, S., and Chen, D. ScaleHLS:
    A Scalable High-Level Synthesis Framework with Multi-Level Transformations and
    Optimizations. In Proceedings of the 59th ACM/IEEE Design Automation Conference
    (DAC) (2022), pp. 1355–1358.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(299) Ye, H., Jun, H., Jeong, H., Neuendorffer, S., 和 Chen, D. ScaleHLS: 一个可扩展的高级合成框架，具有多级变换和优化。发表于第59届
    ACM/IEEE 设计自动化会议（DAC）（2022），第1355–1358页。'
- en: (300) Zacharelos, E., Nunziata, I., Saggese, G., Strollo, A. G., and Napoli,
    E. Approximate Recursive Multipliers Using Low Power Building Blocks. IEEE Transactions
    on Emerging Topics in Computing 10, 3 (2022), 1315–1330.
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (300) Zacharelos, E., Nunziata, I., Saggese, G., Strollo, A. G., 和 Napoli, E.
    使用低功耗构建块的近似递归乘法器。IEEE 计算新兴主题交易 10, 3 (2022), 第1315–1330页。
- en: '(301) Zaruba, F., Schuiki, F., and Benini, L. Manticore: A 4096-Core RISC-V
    Chiplet Architecture for Ultraefficient Floating-Point Computing. IEEE Micro 41,
    2 (Mar. 2021), 36–42.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(301) Zaruba, F., Schuiki, F., 和 Benini, L. Manticore: 一个 4096 核 RISC-V 芯片架构，用于超高效浮点计算。IEEE
    Micro 41, 2 (2021年3月), 第36–42页。'
- en: (302) Zervakis, G., Tsoumanis, K., Xydis, S., Soudris, D., and Pekmestzi, K.
    Design-Efficient Approximate Multiplication Circuits Through Partial Product Perforation.
    IEEE Transactions on Very Large Scale Integration (VLSI) Systems 24, 10 (2016),
    3105–3117.
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (302) Zervakis, G., Tsoumanis, K., Xydis, S., Soudris, D., 和 Pekmestzi, K. 通过部分乘积穿孔设计高效近似乘法电路。IEEE
    大规模集成 (VLSI) 系统交易 24, 10 (2016), 第3105–3117页。
- en: (303) Zhang, C., Wu, D., Sun, J., Sun, G., Luo, G., and Cong, J. Energy-Efficient
    CNN Implementation on a Deeply Pipelined FPGA Cluster. In 2016 International Symposium
    on Low Power Electronics and Design (2016), ACM, p. 326–331.
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (303) Zhang, C., Wu, D., Sun, J., Sun, G., Luo, G., 和 Cong, J. 在深度流水线 FPGA 集群上的节能
    CNN 实现。发表于 2016 国际低功耗电子与设计研讨会（2016），ACM，第326–331页。
- en: '(304) Zhang, J.-F., Lee, C.-E., Liu, C., Shao, Y. S., Keckler, S. W., and Zhang,
    Z. SNAP: A 1.67 — 21.55TOPS/W Sparse Neural Acceleration Processor for Unstructured
    Sparse Deep Neural Network Inference in 16nm CMOS. In 2019 Symposium on VLSI Circuits
    (2019), pp. C306–C307.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(304) Zhang, J.-F., Lee, C.-E., Liu, C., Shao, Y. S., Keckler, S. W., 和 Zhang,
    Z. SNAP: 一个 1.67 — 21.55TOPS/W 稀疏神经加速处理器，用于 16nm CMOS 的非结构化稀疏深度神经网络推理。发表于 2019
    VLSI 电路研讨会（2019），第C306–C307页。'
- en: '(305) Zhang, S., Du, Z., Zhang, L., Lan, H., Liu, S., Li, L., Guo, Q., Chen,
    T., and Chen, Y. Cambricon-X: An accelerator for sparse neural networks. In 2016
    49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO) (2016),
    MICRO-49, pp. 1–12.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(305) Zhang, S., Du, Z., Zhang, L., Lan, H., Liu, S., Li, L., Guo, Q., Chen,
    T., 和 Chen, Y. Cambricon-X: 一种用于稀疏神经网络的加速器。发表于2016年第49届IEEE/ACM国际微架构研讨会（MICRO）（2016），MICRO-49，页码1–12。'
- en: (306) Zhang, X., Lin, J. K., Wickramanayaka, S., Zhang, S., Weerasekera, R.,
    Dutta, R., Chang, K. F., Chui, K.-J., Li, H. Y., Wee Ho, D. S., Ding, L., Katti,
    G., Bhattacharya, S., and Kwong, D.-L. Heterogeneous 2.5D integration on through
    silicon interposer. Applied Physics Reviews 2, 2 (2015).
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (306) Zhang, X., Lin, J. K., Wickramanayaka, S., Zhang, S., Weerasekera, R.,
    Dutta, R., Chang, K. F., Chui, K.-J., Li, H. Y., Wee Ho, D. S., Ding, L., Katti,
    G., Bhattacharya, S., 和 Kwong, D.-L. 通过硅插层的异构2.5D集成。应用物理评论 2, 2 (2015)。
- en: '(307) Zhao, Y., Liu, C., Du, Z., Guo, Q., Hu, X., Zhuang, Y., Zhang, Z., Song,
    X., Li, W., Zhang, X., Li, L., Xu, Z., and Chen, T. Cambricon-Q: A Hybrid Architecture
    for Efficient Training. In 2021 ACM/IEEE 48th Annual International Symposium on
    Computer Architecture (ISCA) (June 2021), pp. 706–719.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(307) Zhao, Y., Liu, C., Du, Z., Guo, Q., Hu, X., Zhuang, Y., Zhang, Z., Song,
    X., Li, W., Zhang, X., Li, L., Xu, Z., 和 Chen, T. Cambricon-Q: 一种高效训练的混合架构。发表于2021年ACM/IEEE第48届国际计算机架构研讨会（ISCA）（2021年6月），页码706–719。'
- en: (308) Zimmer, B., Venkatesan, R., Shao, Y. S., Clemons, J., Fojtik, M., Jiang,
    N., Keller, B., Klinefelter, A., Pinckney, N., Raina, P., Tell, S. G., Zhang,
    Y., Dally, W. J., Emer, J. S., Gray, C. T., Keckler, S. W., and Khailany, B. A
    0.32–128 TOPS, Scalable Multi-Chip-Module-Based Deep Neural Network Inference
    Accelerator With Ground-Referenced Signaling in 16 nm. IEEE Journal of Solid-State
    Circuits 55, 4 (2020), 920–932.
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (308) Zimmer, B., Venkatesan, R., Shao, Y. S., Clemons, J., Fojtik, M., Jiang,
    N., Keller, B., Klinefelter, A., Pinckney, N., Raina, P., Tell, S. G., Zhang,
    Y., Dally, W. J., Emer, J. S., Gray, C. T., Keckler, S. W., 和 Khailany, B. 一个0.32–128
    TOPS的可扩展多芯片模块深度神经网络推理加速器，采用16 nm地面参考信号。IEEE固态电路期刊 55, 4 (2020)，920–932。
