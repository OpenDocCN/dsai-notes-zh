- en: 'Machine Learning 1: Lesson 11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习1：第11课
- en: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-11-7564c3c18bbb](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-11-7564c3c18bbb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-11-7564c3c18bbb](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-11-7564c3c18bbb)
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我从*[*机器学习课程*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*中的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*[*Jeremy*](https://twitter.com/jeremyphoward)*和*[*Rachel*](https://twitter.com/math_rachel)*给了我这个学习的机会。*'
- en: Review of optimizing multi-layer functions with SGD [[0:00](https://youtu.be/XJ_waZlJU8g)]
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SGD优化多层函数的回顾[[0:00](https://youtu.be/XJ_waZlJU8g)]
- en: The idea is that we’ve got some data (*x*) and then we do something to that
    data, for example, we multiply it by a weight matrix (*f(x)*). Then we do something
    to that, for example, we put it through a softmax or a sigmoid(*g(f(x))*). Then
    we do something to that, such as do a cross entropy loss or a root mean squared
    error loss (*h(g(f(x)))*). That’s going to give us some scaler. This is going
    to have no hidden layers. This has got a linear layer, a non-linear activation
    being a softmax, and a loss function being a root mean squared error or a cross
    entropy. Then we’ve got our input data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是我们有一些数据（*x*），然后我们对这些数据做一些操作，例如，我们用一个权重矩阵乘以它（*f(x)*）。然后我们对这个结果做一些操作，例如，我们通过softmax或sigmoid函数处理它（*g(f(x))*）。然后我们对这个结果做一些操作，比如计算交叉熵损失或均方根误差损失（*h(g(f(x)))*）。这将给我们一些标量。这里没有隐藏层。这有一个线性层，一个非线性激活函数是softmax，一个损失函数是均方根误差或交叉熵。然后我们有我们的输入数据。
- en: For example [[1:16](https://youtu.be/XJ_waZlJU8g?t=76)], if the non-linear activation
    was sigmoid or softmax, and the loss function was cross entropy, then that would
    be logistic regression. So how do we calculate the derivative of that with respect
    to our weights?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 例如[[1:16](https://youtu.be/XJ_waZlJU8g?t=76)]，如果非线性激活函数是sigmoid或softmax，损失函数是交叉熵，那就是逻辑回归。那么我们如何计算对权重的导数？
- en: '![](../Images/891063f1f0aa064cca1b5a1a7efe7660.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/891063f1f0aa064cca1b5a1a7efe7660.png)'
- en: 'To do that, basically we do the chain rule:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，基本上我们使用链式法则：
- en: '![](../Images/a22745642c5caa1c4556801a99b2d66d.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a22745642c5caa1c4556801a99b2d66d.png)'
- en: In order to take the derivative with resect to the weights, therefore, we just
    have to calculate the derivative with respect to w using that exact formula [[3:29](https://youtu.be/XJ_waZlJU8g?t=209)].
    Then if we went further here and had another linear layer with weight w2, there
    is no difference now to calculate the derivative with respect to all of the parameters.
    We can still use the exact same chain rule.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了对权重求导，我们只需使用那个确切的公式计算对w的导数[[3:29](https://youtu.be/XJ_waZlJU8g?t=209)]。然后如果我们在这里进一步，有另一个带有权重w2的线性层，现在计算对所有参数的导数没有区别。我们仍然可以使用完全相同的链式法则。
- en: '![](../Images/3c9961197333efdbb023e968791bb3e9.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c9961197333efdbb023e968791bb3e9.png)'
- en: So don’t think of the multi-layer network as being like things that occur at
    different times. It’s just a composition of functions. So we just use the chain
    rule to calculate all the derivatives at once. They are just a set of parameters
    that happen to appear in different parts of the function, but the calculus is
    no different. So calculate this with with respect to w1 and w2, you can just now
    call it w and say w1 is all of those weights.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以不要把多层网络想象成在不同时间发生的事情。它只是函数的组合。所以我们只需使用链式法则一次计算所有导数。它们只是在函数的不同部分出现的一组参数，但微积分并没有不同。所以计算对w1和w2的导数，你现在可以称之为w，说w1就是所有这些权重。
- en: 'So what you are going to have then is a list of parameters [[5:26](https://youtu.be/XJ_waZlJU8g?t=326)].
    Here is w1, it’s probably some kind of higher rank tensor. If it’s a convolutional
    layer, it will be rank 3 tensor, but we can flatten it out. We’ll just make it
    a list of parameters. Here is w2\. It’s just another list of parameters. Here
    is our loss which is a single number. Therefore, our derivative is just a vector
    of that same length. How much does changing that value of w affect the loss? So
    you can think of it as a function like *y = ax1 + bx2 + c* and say oh what’s the
    derivative of that with respect to a, b and c? And you would have three numbers:
    the derivative with respect to a, b, and c. That’s all this is. If the derivative
    with respect to that weight, that weight, …'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你将会得到一个参数列表[[5:26](https://youtu.be/XJ_waZlJU8g?t=326)]。这里是w1，可能是某种高阶张量。如果是卷积层，它将是一个三阶张量，但我们可以展开它。我们将把它变成一个参数列表。这里是w2。这只是另一个参数列表。这是我们的损失，是一个单一的数字。因此，我们的导数就是同样长度的向量。改变w的值会对损失产生多大影响？你可以把它想象成一个函数，比如*y
    = ax1 + bx2 + c*，然后问对a、b和c的导数是多少？你会得到三个数字：对a、b和c的导数。就是这样。如果对那个权重求导，那个权重，...
- en: '![](../Images/eca8b24abb256ea35f89f475a9a31c44.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eca8b24abb256ea35f89f475a9a31c44.png)'
- en: To get there, inside the chain rule, we had to calculate like Jacobian so the
    derivative, when you take a matrix product, is you’ve now got something where
    you’ve got a weight matrix and input vector which are the activations from the
    previous layer, and you’ve got some new output activations. So now you have to
    say for this particular weight, how does changing this particular weight change
    this particular output? How does changing this particular weight change this particular
    output? And so forth. So you end up with these higher dimensional tensors showing
    for every weight, how it affects every output. Then by the time you get to the
    loss function, the loss function is going to have a mean or sum, so they are going
    to get added up in the end.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了到达这一点，在链式法则中，我们必须计算像雅可比这样的导数，当你进行矩阵乘法时，你现在得到的是一个权重矩阵和输入向量，这些是来自前一层的激活，还有一些新的输出激活。所以现在你必须说对于这个特定的权重，改变这个特定的权重如何改变这个特定的输出？改变这个特定的权重如何改变这个特定的输出？等等。所以你最终会得到这些更高维度的张量，显示每个权重如何影响每个输出。然后当你到达损失函数时，损失函数将有一个均值或和，所以它们最终会被加起来。
- en: '![](../Images/df25d73101b184a1ee815a30472a5c7c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df25d73101b184a1ee815a30472a5c7c.png)'
- en: It drives me a bit crazy to try and calculate it out by hand or even think of
    it step by step, because you tend to have like … you just have to remember, for
    every weight for every output, you’re going to have to have a separate gradient.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试手动计算或逐步考虑这一点让我有点疯狂，因为你倾向于像…你只需要记住，对于每个权重对于每个输出，你都必须有一个单独的梯度。
- en: One good way to look at this is to learn to use PyTorch’s `.grad` attribute
    and `.backward` method manually and look up PyTorch tutorials. So you can actually
    start setting up some calculations with a vector input and vector output, and
    then type `.backward` and then type `grad` and look at it. Then to some really
    small ones with just 2 or 3 items in the input and output vectors and make the
    operation like plus 2 or something and see what the shapes are and make sure it
    makes sense. Because vector matrix calculus introduces zero new concepts to anything
    you learned in high school, strictly speaking. But getting a feel for how these
    shapes move around took a lot of practice. The good news is, you almost never
    have to worry about it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的方法是学习使用PyTorch的`.grad`属性和`.backward`方法，并手动查阅PyTorch教程。这样你就可以开始设置一些具有向量输入和向量输出的计算，然后输入`.backward`，然后输入`grad`并查看它。然后对只有2或3个项目的输入和输出向量进行一些非常小的操作，比如加2或其他操作，看看形状是什么，确保它是有意义的。因为向量矩阵微积分在严格意义上并没有为你在高中学到的任何概念引入新的概念。但是对这些形状如何移动有了一定的感觉需要大量的练习。好消息是，你几乎永远不必担心这个。
- en: Review of Naive Bayes & Logistic Regression for NLP [[9:53](https://youtu.be/XJ_waZlJU8g?t=593)]
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP的朴素贝叶斯和逻辑回归回顾[[9:53](https://youtu.be/XJ_waZlJU8g?t=593)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson5-nlp.ipynb)
    / [Excel](https://github.com/fastai/fastai/blob/master/courses/ml1/excel/naivebayes.xlsx)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson5-nlp.ipynb)
    / [Excel](https://github.com/fastai/fastai/blob/master/courses/ml1/excel/naivebayes.xlsx)'
- en: We were talking about using this kind of logistic regression for NLP. And before
    we got to that point, we were talking about using Naive Bayes for NLP. And the
    basic idea was that we could take a document (e.g. a movie review), and turn it
    into a bag of words representation consisting of the number of times each word
    appears. We call the unique list of words vocabulary. And we used the sklearn
    CountVectorizer to automatically generate both the vocabulary which in sklearn
    they call the “features” and create the bag of words representations and the whole
    group of them is called a term document matrix.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在讨论使用这种逻辑回归进行NLP。在达到这一点之前，我们正在讨论使用朴素贝叶斯进行NLP。基本思想是我们可以取一个文档（例如电影评论），并将其转换为一个词袋表示，其中包含每个单词出现的次数。我们称单词的唯一列表为词汇表。我们使用sklearn的CountVectorizer自动生成词汇表，他们称之为“特征”，并创建词袋表示，所有这些袋表示的整体称为术语文档矩阵。
- en: We kind of realized that we could calculate the probability that a positive
    review contains the word “this” by just averaging the number of time this appears
    in the positive reviews, we could do the same for the negatives, then we could
    take the ratio of them to get something which if it’s greater than one was a word
    appeared more often in the positive review, or less than one was a word that appeared
    more often in the negative reviews.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有点意识到，我们可以通过简单地平均积极评论中单词“this”出现的次数来计算积极评论包含单词“this”的概率，我们可以对消极评论做同样的事情，然后我们可以取它们的比率，得到一个结果，如果大于一，则表示该单词在积极评论中出现得更频繁，如果小于一，则表示该单词在消极评论中出现得更频繁。
- en: Then we realized using Bayes rules and taking the logs, that we could basically
    end up with something where we could add up the logs of these (highlighted below)
    plus the log of the ratio of the probabilities that things are in class 1 versus
    class 0, and end up with something we can compare to zero [[11:32](https://youtu.be/XJ_waZlJU8g?t=692)].
    If it’s greater than zero then we can predict a document is positive or if it’s
    less than zero, we can predict the document is negative. And that was our Bayes
    rule.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们意识到，使用贝叶斯规则并取对数，我们基本上可以得到一个结果，我们可以将这些（下面突出显示的）的对数加起来，再加上类别1与类别0的概率比的对数，最终得到一个可以与零进行比较的结果[[11:32](https://youtu.be/XJ_waZlJU8g?t=692)]。如果结果大于零，我们可以预测文档是积极的，如果结果小于零，我们可以预测文档是消极的。这就是我们的贝叶斯规则。
- en: '![](../Images/c5237d967f984d229fe7920e695d2c60.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5237d967f984d229fe7920e695d2c60.png)'
- en: 'We kind of did that from math first principles and I think we agreed that the
    “naive” in Naive Bayes was a good description because it assumes independence
    when it’s definitely not true. But it’s an interesting starting point and I think
    it was interesting to observe when we actually got to the point where like okay,
    now we’ve calculated the ratio of the probabilities and took the log, and now
    rather than multiply them together, of course, we have to add them up. And when
    we actually wrote that down, we realized oh that is just a standard weight matrix
    product plus a bias:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从数学的第一原理开始做了这个，我认为我们都同意“朴素”在朴素贝叶斯中是一个很好的描述，因为它假设了独立性，而这显然是不正确的。但这是一个有趣的起点，当我们实际上到达这一点时，我们计算了概率的比率并取了对数，现在不是将它们相乘，当然，我们必须将它们相加。当我们实际写下这个时，我们意识到哦，这只是一个标准的权重矩阵乘积加上一个偏差：
- en: '![](../Images/5024bd0133208af90a67af82c1b81e1b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5024bd0133208af90a67af82c1b81e1b.png)'
- en: Then we realized okay, so if this is not very good accuracy (80%), why not improve
    it by saying hey, we know other ways to calculate a bunch of coefficients and
    a bunch of biases which is to learn them in a logistic regression. In other words,
    this is the formula we use for a logistic regression and so why don’t we just
    create a logistic regression and fit it? It’s going to give us the same thing,
    but rather than coefficients and biases which are theoretically correct based
    on this assumption of independence and based on Bayes rule, they’ll be the coefficients
    and biases that are actually the best in this data. So that was where we got to.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们意识到，如果这不是很好的准确率（80%），为什么不通过说，嘿，我们知道其他计算一堆系数和一堆偏差的方法，即在逻辑回归中学习它们来改进呢？换句话说，这是我们用于逻辑回归的公式，那么为什么我们不只是创建一个逻辑回归并拟合它呢？它会给我们同样的结果，但与基于独立性假设和贝叶斯规则的理论上正确的系数和偏差不同，它们将是实际上在这些数据中最好的系数和偏差。这就是我们的结论。
- en: The key insight here is just about everything, a machine learning ends up being
    either a tree or a bunch of matrix products and nonlinearities [[13:54](https://youtu.be/XJ_waZlJU8g?t=834)].
    Everything seems to end up coming down to the same thing including, as it turns
    out, Bayes rule. Then it turns out whatever the parameters are in that function
    turns out that they are better learnt than calculated based on the theory. And
    indeed that’s what happened when we actually tried learning those coefficients,
    we got 85%.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键见解是，几乎所有的机器学习最终都变成了一棵树或一堆矩阵乘积和非线性[[13:54](https://youtu.be/XJ_waZlJU8g?t=834)]。一切似乎最终都归结为相同的事情，包括贝叶斯规则。然后事实证明，无论该函数中的参数是什么，它们都比基于理论计算更好地学习。事实上，当我们实际尝试学习这些系数时，我们得到了85%的准确率。
- en: Then we noticed that we could also, rather than take the whole term document
    matrix, we could instead just take the ones and zeros for presence or absence
    of a word. And sometimes it was equally as good but then we actually tried something
    else which is we tried adding regularization. With regularization, the binarized
    approach turned out to be a little better.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们注意到，与其采用整个术语文档矩阵，我们可以只取单词存在或不存在的一和零。有时这样做同样有效，但后来我们实际尝试了另一种方法，即添加正则化。通过正则化，二值化方法结果略好一些。
- en: So then regularization was where we took the loss function, and again, let’s
    start with RMSE and then we’ll talk about cross entropy. Loss function was our
    predictions minus our actuals, sum that up, take the average plus a penalty.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后正则化是我们取损失函数，再次，让我们从RMSE开始，然后我们将讨论交叉熵。损失函数是我们的预测减去我们的实际值，将其相加，取平均再加上一个惩罚。
- en: '![](../Images/c157f2b2453c525b3242f0dea7c43196.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c157f2b2453c525b3242f0dea7c43196.png)'
- en: This specifically is the L2 penalty. If this, instead, was the absolute value
    of w, then that would be the L1 penalty. We also noted that we don’t really care
    about the loss function per se, we only case about its derivatives that’s actually
    the thing that updates the weights, so because this is a sum, we can take the
    derivative of each part separately and so the derivative of the penalty was just
    2*aw*. So we learnt that even though these are mathematically equivalent, they
    have different names. This version (2*aw*) is called weight decay and that term
    is used in the neural net literature.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，这是L2惩罚。如果这是w的绝对值，那就是L1惩罚。我们还注意到，我们实际上并不关心损失函数本身，我们只关心它的导数，这实际上是更新权重的东西，因此因为这是一个总和，我们可以分别对每个部分求导，所以惩罚的导数只是2*aw*。因此，我们了解到，尽管这些在数学上是等价的，但它们有不同的名称。这个版本（2*aw*）被称为权重衰减，这个术语在神经网络文献中使用。
- en: Cross entropy [[16:34](https://youtu.be/XJ_waZlJU8g?t=994)]
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉熵[[16:34](https://youtu.be/XJ_waZlJU8g?t=994)]
- en: '[Excel](https://github.com/fastai/fastai/blob/master/courses/dl1/excel/entropy_example.xlsx)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[Excel](https://github.com/fastai/fastai/blob/master/courses/dl1/excel/entropy_example.xlsx)'
- en: Cross entropy on the other hand, it’s just another loss function like root mean
    squared error, but it’s specifically designed for classification. Here is an example
    of a binary cross entropy. Let’s say this is our “is it a cat or a dog?” So is
    to say `isCat` 1 or 0\. And `Preds` are our predictions so this is the output
    of our final layer of our neural net, a logistic regression, etc.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，交叉熵只是另一个损失函数，就像均方根误差一样，但它专门设计用于分类。这是一个二元交叉熵的例子。假设这是我们的“是猫还是狗？”所以说`isCat`是1还是0。而`Preds`是我们的预测，这是我们神经网络的最终层的输出，一个逻辑回归等等。
- en: '![](../Images/4958d27199a2d4226999cf54c034365c.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4958d27199a2d4226999cf54c034365c.png)'
- en: Then all we do is to say okay let’s take the actual times the log of the prediction,
    then we add to that 1 minus actual times the log of 1 minus the prediction, then
    take the negative of that whole thing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们所做的就是说好吧，让我们取实际值乘以预测的对数，然后加上1减去实际值乘以1减去预测的对数，然后取整个东西的负值。
- en: '![](../Images/66852b6b4c7f7662eb7261c0e07f79db.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66852b6b4c7f7662eb7261c0e07f79db.png)'
- en: 'I suggested to you all that you try to write the if statement version of this,
    so hopefully you’ve done that by now, otherwise I’m about to spoil it for you.
    So this was:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你们尝试写出这个if语句版本，希望你们现在已经做到了，否则我将为你们揭示。所以这是：
- en: '![](../Images/bfc766e4fea8471b28084e16cefcc998.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfc766e4fea8471b28084e16cefcc998.png)'
- en: How do we write this as an if statement?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将这写成一个if语句？
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'So the key insight is that y has two possibilities: 1 or 0\. So very often
    the math can hide the key insight which I think happens here until you actually
    think about what the values it can take. So that is all it’s saying. Either give
    me: `-log(ŷ)` or `-log(1-ŷ)`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以关键的洞察是 y 有两种可能性：1 或 0。所以很多时候数学会隐藏关键的洞察，我认为这里发生了，直到你真正思考它可以取什么值。所以这就是它所说的。要么给我：`-log(ŷ)`，要么给我：`-log(1-ŷ)`
- en: Okay, so then the multi category version is just the same thing but you’re saying
    if for more than just `y == 1` but `y == 0, 1, 2, 3, 4, 5 . . .` , for instance
    [[19:26](https://youtu.be/XJ_waZlJU8g?t=1166)]. So that loss function has a particularly
    simple derivative and also another thing you could play with at home if you’d
    like is thinking about how the derivative looks when you add a sigmoid or softmax
    before it. It turns out you’ll end up with very well behaved derivatives.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么多类别版本就是同样的事情，但你说的不仅仅是 `y == 1`，而是 `y == 0, 1, 2, 3, 4, 5 . . .` ，例如 [[19:26](https://youtu.be/XJ_waZlJU8g?t=1166)]。所以这个损失函数有一个特别简单的导数，另外一个你可以在家里尝试的东西是想一想在它之前加上一个
    sigmoid 或 softmax 之后导数是什么样子。结果会是非常好的导数。
- en: There’s lots of reasons that people use RMSE for regression and cross entropy
    for classification, but most of it comes back to the statistical idea of a best
    linear unbiased estimator and based on the likelihood function that turns out
    that these have some nice statistical properties. It turns out, however, in practice
    root mean square error in particular the properties are perhaps more theoretical
    than actual and actually nowadays using the absolute deviation rather than sum
    of the squared deviation can often work better. So in practice, everything in
    machine learning, I normally try both. For particular dataset, I’ll try both loss
    functions and see which one works better. And of course if it’s a Kaggle competition
    in which case you’re told how Kaggle is going to judge it and you should use the
    same loss function as Kaggle’s evaluation metric.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 人们使用均方根误差用于回归和交叉熵用于分类的原因有很多，但大部分都可以追溯到最佳线性无偏估计的统计概念，基于可能性函数的结果表明这些函数具有一些良好的统计性质。然而，实际上，尤其是均方根误差的性质可能更多是理论上的而不是实际的，实际上，现在使用绝对偏差而不是平方偏差的和通常效果更好。所以在实践中，机器学习中的一切，我通常都会尝试两种。对于特定的数据集，我会尝试两种损失函数，看哪一个效果更好。当然，如果是
    Kaggle 竞赛的话，那么你会被告知 Kaggle 将如何评判，你应该使用与 Kaggle 评估指标相同的损失函数。
- en: So this is really the key insight [[21:16](https://youtu.be/XJ_waZlJU8g?t=1276)].
    Let’s not use theory but instead learn things from the data. And we hope that
    we’re going to get better results. Particularly with regularization, we do. Then
    I think the key regularization insight here is let’s not try to reduce the number
    of parameters in our model, but instead use lots of parameters and then use regularization
    to figure out which ones are actually useful.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这真的是关键的洞察 [[21:16](https://youtu.be/XJ_waZlJU8g?t=1276)]。让我们不要使用理论，而是从数据中学习。我们希望我们会得到更好的结果。特别是在正则化方面，我们确实得到了。然后我认为这里的关键正则化洞察是让我们不要试图减少模型中的参数数量，而是使用大量的参数，然后使用正则化来找出哪些实际上是有用的。
- en: More features with n-grams [[21:41](https://youtu.be/XJ_waZlJU8g?t=1301)]
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多的 n-grams 特征 [[21:41](https://youtu.be/XJ_waZlJU8g?t=1301)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson5-nlp.ipynb)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson5-nlp.ipynb)'
- en: So then we took that step further by saying given we can do that with regularization,
    let’s create lots more by adding bigrams and trigrams. Bigrams such as `by vast`,
    `by vengeance` and trigrams such as `by vengeance .` and `by vera miles`. To keep
    things run a little bit faster, we limited it to 800,000 features but even with
    the full 70 million features, it works just as well and it’s not a heck of a lot
    slower.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们进一步说，鉴于我们可以通过正则化做到这一点，让我们通过添加二元组和三元组来创建更多。例如 `by vast`、`by vengeance` 这样的二元组，以及
    `by vengeance .`、`by vera miles` 这样的三元组。为了让事情运行得更快一些，我们将其限制为 800,000 个特征，但即使使用完整的
    70 百万个特征，它的效果也一样好，而且速度并没有慢多少。
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'So we created a term document matrix using the full set of n-grams for the
    training set and the validation set. So now we can go ahead and say our labels
    are as the training set labels as before, our independent variables is binarized
    term document matrix as before:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们使用了完整的 n-grams 集合为训练集和验证集创建了一个术语文档矩阵。现在我们可以继续说我们的标签是训练集标签如前所述，我们的自变量是二值化的术语文档矩阵如前所述：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And Then let’s fit a logistic regression to that, and do some predictions,
    and we get 90% accuracy:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后让我们对其进行逻辑回归拟合，并进行一些预测，我们得到了 90% 的准确率：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So this is looking pretty good.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以看起来很不错。
- en: Back to Naive Bayes [[22:54](https://youtu.be/XJ_waZlJU8g?t=1374)]
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回到朴素贝叶斯 [[22:54](https://youtu.be/XJ_waZlJU8g?t=1374)]
- en: Let’s go back to our Naive Bayes. In our Naive Bayes, we have this term document
    matrix and then for every feature, we are calculating the probability of that
    feature occurring if it’s class 1, that probability of that feature occurring
    if it’s class 0, and the ratio of those two.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的朴素贝叶斯。在我们的朴素贝叶斯中，我们有这个术语文档矩阵，然后对于每个特征，我们正在计算如果它是类别 1 出现的概率，如果它是类别 0
    出现的概率，以及这两者的比率。
- en: '![](../Images/4ab3b2c459d45fb31034348d1c7c350f.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ab3b2c459d45fb31034348d1c7c350f.png)'
- en: And in the paper that we are actually basing this off, they call `p(f|1)` *p*,
    and they call `p(f|0)` *q* and the ratio *r*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实际基于的论文中，他们将 `p(f|1)` 称为 *p*，将 `p(f|0)` 称为 *q*，将比率称为 *r*。
- en: '![](../Images/bb1e80547989656bc1b62058b71fc982.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb1e80547989656bc1b62058b71fc982.png)'
- en: So then we say let’s not use these ratios as the coefficients in that matrix
    multiply. But let’s instead, try and learn some coefficients. So maybe start out
    with some random numbers, and then try and use stochastic gradient descent to
    find slightly better ones.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们说让我们不要把这些比率作为矩阵乘法中的系数。而是，尝试学习一些系数。也许开始用一些随机数，然后尝试使用随机梯度下降找到稍微更好的系数。
- en: '![](../Images/e856e9173b076568fc89ab08e7f735c6.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e856e9173b076568fc89ab08e7f735c6.png)'
- en: 'So you’ll notice some important features here. The *r* vector is a vector of
    rank 1 and its length is equal to the number of features. And of course, our logistic
    regression coefficient matrix is also rank 1 and length equal to the number of
    features. And we are saying they are two ways of calculating the same kind of
    thing: one based on theory, one based on data. So here is some of the numbers
    in *r*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你会注意到这里一些重要的特征。*r*向量是一个秩为1的向量，其长度等于特征的数量。当然，我们的逻辑回归系数矩阵也是秩为1且长度等于特征数量的。我们说它们是计算相同类型的东西的两种方式：一种基于理论，一种基于数据。所以这里是*r*中的一些数字：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Remember it’s using the log so these number which are less than zero represent
    things which are more likely to be negative and the one greater than zero is likely
    to be positive. So here is e to the power of that (e^). So there are the ones
    we can compare to one rather than zero:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 记住它使用对数，所以这些小于零的数字代表更有可能是负数的东西，而大于零的数字可能是正数。所以这里是e的幂次方。所以这些是我们可以与1而不是0进行比较的数字：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: I’m going to do something that hopefully is going to seem weird [[25:13](https://youtu.be/XJ_waZlJU8g?t=1513)].
    First of all, I’m going to say what we are going to do and then I’m going to try
    and describe why it’s weird, and then we’ll talk about why it may not be as weird
    as we first thought. So here is what we are going to do. We are going to take
    our term document matrix and we’re going to multiply it by *r*. So what that means
    is, I can do it here in Excel, we are going to say let’s grab everything in our
    term document matrix and multiply it by the equivalent value in the vector of
    *r*. So this is like a broadcasted element-wise multiplication, not a matrix multiplication.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我要做一些希望看起来很奇怪的事情。首先，我会说我们要做什么，然后我会尝试描述为什么这很奇怪，然后我们会讨论为什么它可能并不像我们最初想的那么奇怪。所以这就是我们要做的事情。我们将取我们的术语文档矩阵，然后将其乘以*r*。这意味着，我可以在Excel中做到这一点，我们将说让我们抓取我们的术语文档矩阵中的所有内容，并将其乘以向量*r*中的等值。所以这就像是一个广播的逐元素乘法，而不是矩阵乘法。
- en: '![](../Images/0f551cdb8b7269f3363b3dc0055e8580.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f551cdb8b7269f3363b3dc0055e8580.png)'
- en: So here is the value of the term document matrix times *r*, in other words,
    everywhere a zero appears in the term document matrix, a zero appears in the multiplied
    version. And every time a one appears in the term document matrix, the equivalent
    value of *r* appears on the bottom. So we haven’t really changed much. We’ve just
    kind of changed the ones into something else i.e. *r*’s from that feature. So
    what we are now going to do is we’re going to use this our independent variables,
    instead, in our logistic regression.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是术语文档矩阵乘以*r*的值，换句话说，在术语文档矩阵中出现零的地方，在乘以版本中也出现零。而在术语文档矩阵中每次出现一个的地方，等效的*r*值出现在底部。所以我们并没有真正改变太多。我们只是将一个变成了其他东西，即来自该特征的*r*。所以我们现在要做的是，我们将使用这些独立变量，而不是在我们的逻辑回归中。
- en: 'So here we are [[26:56](https://youtu.be/XJ_waZlJU8g?t=1616)]. `x_nb` (x Naive
    Bayes version) is `x` times `r`. And now let’s do a logistic regression, fitting
    using those independent variables. Let’s then do that for the validation set,
    get the predictions, and lo and behold, we have a better number:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里。`x_nb`（x朴素贝叶斯版本）是`x`乘以`r`。现在让我们使用这些独立变量进行逻辑回归拟合。然后对验证集进行预测，结果我们得到了一个更好的数字：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let me explain why this hopefully seem surprising. So that’s our independent
    variable (highlighted below) and then the logistic regression has come up with
    some set of coefficients (let’s pretend for a moment that these are the coefficients
    that it happened to come up with).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我解释为什么这可能会令人惊讶。这是我们的独立变量（下面突出显示），然后逻辑回归得出了一些系数集（假设这些是它恰好得出的系数）。
- en: '![](../Images/36591fbab544d793b398b718d4d2f116.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36591fbab544d793b398b718d4d2f116.png)'
- en: We could now say, well, let’s not use this set (`x_nb`) of independent variables
    but let’s use the original binarized feature matrix. And then divide all of our
    coefficients by the values in *r* and we’re going to get exactly the same result
    mathematically.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以说，好吧，让我们不使用这组独立变量（`x_nb`），而是使用原始的二值化特征矩阵。然后将所有系数除以*r*中的值，数学上我们将得到完全相同的结果。
- en: So we’ve got our x Naive Bayes version (`x_nb`) of the independent variables
    and we’ve got some set of weights/coefficients (`w1`) where it’s found this is
    a good set of coefficients for making our predictions from. But x_nb is simply
    equal to `x` times (element-wise) `r`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有了我们的独立变量的x朴素贝叶斯版本（`x_nb`），以及一组权重/系数（`w1`），它发现这是一个用于进行预测的好系数集。但是x_nb简单地等于`x`乘以（逐元素）`r`。
- en: '![](../Images/ed4aa7b961e91f3e172d3300e5caadc1.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed4aa7b961e91f3e172d3300e5caadc1.png)'
- en: So in other words, this (`xnb·w1`) is equal to `x*r·w1` . So we could just change
    the weights to be `r·w1` and get the same number. So this ought to mean that the
    change that we made to the independent variable should not have made any difference
    because we can calculate exactly the same thing without making that change. So
    there’s the question. Why did it make a difference? So in order to answer this
    question, you need to think about what are the thing that aren’t mathematically
    the same. Why is it not identical? Come up with some hypotheses what are some
    reasons that maybe we’ve actually ended up with a better answer. And to figure
    that out, we need to first of all start with why is it even a different answer?
    This is subtle.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这（`xnb·w1`）等于`x*r·w1`。所以我们可以只改变权重为`r·w1`，得到相同的数字。这应该意味着我们对独立变量所做的更改不应该有任何影响，因为我们可以在不进行这种改变的情况下计算出完全相同的结果。所以问题就在这里。为什么会有所不同呢？为了回答这个问题，你需要考虑哪些数学上不同的事情。为什么它们不完全相同？提出一些假设，也许我们实际上得到了更好的答案的一些原因。要弄清楚这一点，首先我们需要弄清楚为什么会有不同的答案？这是微妙的。
- en: Discussions [[30:33](https://youtu.be/XJ_waZlJU8g?t=1833)~[32:46](https://youtu.be/XJ_waZlJU8g?t=1966)]
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论
- en: 'They are getting impacted differently by regularization. Our loss was equal
    to our cross entropy loss based on the predictions and actuals plus our penalty:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 它们受到正则化的影响是不同的。我们的损失等于基于预测和实际值的交叉熵损失加上我们的惩罚：
- en: '![](../Images/ac0d3c4446e454e4b325f989e314a253.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac0d3c4446e454e4b325f989e314a253.png)'
- en: So if your weights are large, then the penalty(`aw²`) gets bigger, and it drowns
    out the cross entropy piece (`x.e.(xw, y)`). But that’s actually the piece we
    care about. We actually want it to be a good fit. So we want to have as little
    regularization going on as we can get away with. So we want lesser weights (I
    kind of use the two words, “less” and “lesser”, a little equivalently which is
    not quite fair, I agree, but the idea is that weights that are pretty close to
    zero are kind of not there).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你的权重很大，那么惩罚（`aw²`）就会变大，而且会淹没掉交叉熵部分（`x.e.(xw, y)`）。但那实际上是我们关心的部分。我们实际上希望它是一个很好的拟合。所以我们希望尽可能少地进行正则化。所以我们希望权重更小（我有点把“更少”和“更小”这两个词用得有点等同，这不太公平，我同意，但想法是接近零的权重实际上是不存在的）。
- en: Here is the thing [[34:38](https://youtu.be/XJ_waZlJU8g?t=2078)]. Our values
    of *r*, and I’m not a Bayesian weenie but I’m still going to use the word “prior”.
    They are kind of like a prior — we think that the different levels of importance
    and positive or negative of these different features might be something like that.
    We think that “bad” might be more correlated with negative than “good”. So our
    kind of implicit assumption before was that we have no priors, so in other words,
    when we said squared weights (w²), we are saying a non-zero weight is something
    we don’t want to have. But actually what I really want to say is that differing
    from the Naive Bayes expectation is something I don’t want to do. Only vary from
    the Naive Bayes prior unless you have good reason to believe otherwise.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是问题所在。我们的*r*值，我不是一个贝叶斯怪胎，但我仍然要使用“先验”这个词。它们有点像一个先验 - 我们认为不同级别的重要性和这些不同特征的积极或消极可能是这样的。我们认为“坏”可能与负面更相关，而不是“好”。所以我们之前的隐含假设是我们没有先验，换句话说，当我们说平方权重（w²）时，我们是在说非零权重是我们不想要的。但实际上我想说的是，与朴素贝叶斯的期望不同是我不想做的事情。除非你有充分的理由相信其他情况，否则只有与朴素贝叶斯先验有所不同。
- en: So that’s actually what this ends up doing. We end up saying we think this value
    is probably 3\. So if you’re going to make it a lot bigger or a lot smaller, that’s
    going to create the kind of variation in weights that’s going to cause that squared
    term to go up . So if you can, just leave all these values about similar to where
    they are now. So that’s what the penalty term is now doing. The penalty term when
    our input is already multiplied by *r*, it’s saying penalize things where we’re
    varying from our Naive Bayes prior.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上就是这样做的。我们最终说我们认为这个值可能是3。所以如果你要把它变得更大或更小，那将会导致权重的变化，从而使平方项增加。所以如果可以的话，就让所有这些值保持与现在大致相似。这就是现在惩罚项正在做的事情。当我们的输入已经乘以*r*时，它在说惩罚那些与我们的朴素贝叶斯先验有所不同的事物。
- en: '![](../Images/9d868bc0fb1e98a09a988969a6dc22f7.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d868bc0fb1e98a09a988969a6dc22f7.png)'
- en: '**Question**: Why multiply only with r and not like r² or something like that
    when the variance would be much higher this time [[36:40](https://youtu.be/XJ_waZlJU8g?t=2200)]?
    Because our prior comes from an actual theoretical model. So I said I don’t like
    to rely oh theory but if I have some theory, then maybe we should use that as
    our starting point rather than starting off by assuming everything is equal. So
    our prior said hey, we’ve got this model called Naive Bayes and the Naive Bayes
    model said if the Naive Bayes’ assumptions were correct, then *r* is the correct
    coefficient in this specific formulation. That’s why we picked that because our
    prior is based on that theory.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：为什么只与r相乘，而不是像r²这样，这次方差会高得多呢？因为我们的先验来自一个实际的理论模型。所以我说我不喜欢依赖理论，但如果我有一些理论，那么也许我们应该将其作为我们的起点，而不是假设一切都是相等的。所以我们的先验说，嘿，我们有这个叫做朴素贝叶斯的模型，朴素贝叶斯模型说，如果朴素贝叶斯的假设是正确的，那么*r*就是这个特定公式中的正确系数。这就是我们选择它的原因，因为我们的先验是基于那个理论的。
- en: So this is a really interesting insight which I never really see covered [[37:34](https://youtu.be/XJ_waZlJU8g?t=2254)].
    The idea that we can use these traditional machine learning techniques, we can
    imbue them with this kind of Bayesian sense by starting out incorporating our
    theoretical expectations into the data that we give our model. And when we do
    so, that then means we don’t have to regularize as much. And that’s good because
    we regularized a lot… let’s just try it!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常有趣的见解，我从未真正看到过。这个想法是我们可以使用这些传统的机器学习技术，通过将我们的理论期望纳入我们给模型的数据中，赋予它们这种贝叶斯感。当我们这样做时，这意味着我们就不必那么经常进行正则化了。这很好，因为我们经常进行正则化...让我们试试吧！
- en: Remember, the way they do it in the sklearn logistic regression is `C` is the
    reciprocal of the amount of regularization penalty. So we will add lots of regularization
    by making it small (`1e-5`).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在sklearn逻辑回归中，`C`是正则化惩罚的倒数。所以我们通过使其变小（`1e-5`）来增加大量的正则化。
- en: '![](../Images/9dc7d77967c5998fdfcbd649bd4c8b97.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9dc7d77967c5998fdfcbd649bd4c8b97.png)'
- en: So that really hurts our accuracy because now it’s trying really hard to get
    those weights down, the loss function is overwhelmed by the need to reduce the
    weights. And the need to make it predictive now seems totally unimportant. So
    by starting out and saying don’t push the weights down so that you end up ignoring
    the terms, but instead push them down so that you try to get rid of ones that
    ignore differences from our expectation based on the Naive Bayes formulation.
    So that ends up giving us a very nice result
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的会影响我们的准确性，因为现在它非常努力地降低这些权重，损失函数被需要减少权重的需求所压倒。而现在使其具有预测性似乎完全不重要。所以通过开始并说不要推动权重降低，以至于最终忽略这些项，而是推动它们降低，以便尝试消除那些忽略了我们基于朴素贝叶斯公式期望的差异的权重。这最终给我们带来了一个非常好的结果
- en: 'Baselines and Bigrams: Simple, Good Sentiment and Topic Classification [[39:44](https://youtu.be/XJ_waZlJU8g?t=2384)]'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基线和二元组：简单、良好的情感和主题分类
- en: '[Paper](https://www.aclweb.org/anthology/P12-2018)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://www.aclweb.org/anthology/P12-2018)'
- en: 'This technique was originally presented in 2012\. Chris Manning is a terrific
    NLP researcher at Stanford and Sida Wang who I don’t know but I assume is awesome
    because his paper is awesome. They basically came up with this idea. What they
    did was they compared it to a number of other approaches on a number of other
    datasets. So one of the things they tried is the IMDB dataset. So here is Naive
    Bayes SVM on bigrams:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术最初是在2012年提出的。Chris Manning是斯坦福大学出色的自然语言处理研究人员，而Sida Wang我不认识，但我认为他很棒，因为他的论文很棒。他们基本上提出了这个想法。他们所做的是将其与其他方法在其他数据集上进行比较。其中一件事是他们尝试了IMDB数据集。这里是大二元组上的朴素贝叶斯SVM：
- en: '![](../Images/c8fc681713456fe836fc67707e195106.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8fc681713456fe836fc67707e195106.png)'
- en: As you can see, this approach outperformed the other linear based approaches
    that they looked at and also some restricted Boltzmann machine kind of neural
    net based approaches they looked at. Nowadays, there are better ways to do this
    and in fact in the deep learning course, we showed new state-of-the-art result
    we just developed at Fast AI that gets well over 94%. But still particularly for
    a linear technique that’s easy, fast, and intuitive, this is pretty good. And
    you’ll notice, when they did this, they only used bigrams. And I assume that’s
    because I looked at their code and it was kind of pretty slow and ugly. I figured
    out a way to optimize it a lot more as you saw and so we were able to use trigrams
    so we get quite a lot better and we’ve got 91.8% versus 91.2% but other than that,
    it’s identical. Oh, also they used support vector machine which is almost identical
    to a logistic regression in this case, so there’re some minor differences. So
    I think that’s a pretty cool result and
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这种方法胜过了他们研究的其他基于线性的方法，以及他们研究的一些受限玻尔兹曼机的神经网络方法。如今，有更好的方法来做这个，事实上在深度学习课程中，我们展示了我们在Fast
    AI刚刚开发的最新成果，可以达到94%以上的准确率。但是特别是对于一种简单、快速、直观的线性技术来说，这还是相当不错的。你会注意到，当他们这样做时，他们只使用了二元组。我猜这是因为我看了他们的代码，发现它相当慢且难看。我找到了一种更优化的方法，正如你所看到的，所以我们能够使用三元组，因此我们得到了更好的结果，我们的准确率是91.8%，而不是91.2%，但除此之外，它是相同的。哦，他们还使用了支持向量机，在这种情况下几乎与逻辑回归相同，所以有一些细微的差异。所以我认为这是一个相当酷的结果。
- en: I will mention, what you get to see here in class is the result of many weeks
    and often many months of research that I do [[41:32](https://youtu.be/XJ_waZlJU8g?t=2492)].
    So I don’t want you to think this stuff is obvious. It’s not at all. Like reading
    this paper, there’s no description in the paper of why they use this model, how
    it’s different, why they thought it works. It took me a week or two to even realize
    that it’s mathematically equivalent to a normal logistic regressions and then
    a few more weeks to realize that the difference is actually in the regularization.
    This is kind of like machine learning as I’m sure you’ve noticed from the Kaggle
    competition you enter. Like you come up with a thousand good ideas, 999 of them
    no matter how confident you are they are going to be great, they always turn out
    to be crap. Then finally after four weeks, one of them finally works and kind
    of gives you the enthusiasm to spend another four weeks of misery and frustration.
    This is the norm. And for sure that the best practitioners I know in machine learning
    all share one particular trait in common which is that they are very very tenacious
    — also known as stubborn and bloody-minded which is definitely a reputation I
    seem to have, probably fair, along with another thing which is that they are all
    very good coders. They are very good at turning their ideas into new code. So
    this was a really interesting experience for me working through this a few months
    ago to try and figure out at least how to explain why this, at the time, state-of-the-art-result
    exists.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我要提一下，在课堂上你看到的是我经过多周甚至多个月的研究得出的结果。所以我不希望你认为这些东西是显而易见的。完全不是。就像阅读这篇论文，论文中没有描述为什么他们使用这个模型，它与其他模型有何不同，为什么他们认为它有效。我花了一两周的时间才意识到它在数学上等同于普通的逻辑回归，然后又花了几周的时间才意识到区别实际上在于正则化。这有点像机器学习，我相信你从你参加的Kaggle竞赛中已经注意到了。就像你提出了一千个好主意，其中999个无论你有多么自信它们会很棒，最终都会变成垃圾。然后最终在四周后，其中一个终于奏效，给了你继续度过另外四周的痛苦和挫折的热情。这是正常的。而且我可以确定，我所认识的机器学习领域最优秀的从业者都有一个共同的特点，那就是他们非常顽强，也被称为固执和执着，这绝对是我似乎拥有的声誉，可能是公平的，还有另一点，他们都是非常擅长编码的。他们非常擅长将他们的想法转化为新的代码。对我来说，几个月前通过这个工作是一个非常有趣的经历，试图至少弄清楚为什么这个当时的最新成果存在。
- en: 'Even better version: NBSVM++ [[43:31](https://youtu.be/XJ_waZlJU8g?t=2611)]'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更好的版本：NBSVM++ [43:31]
- en: So once I figured that out, I was actually able to build on top of it and make
    it quite a bit better, and I’ll show you what I did. And this is where it was
    very very handy to have PyTorch at my disposal because I was able to create something
    that was customized just the way I wanted it to be and also very fast by using
    the GPU. So here is the kind of Fast AI version of NBSVM. Actually my friend Stephen
    Merity who is a terrific researcher in NLP has christened this the NBSVM++ which
    I thought was lovely, so here’s that, even though there is no SVM, it’s a logistic
    regression but as I said, nearly exactly the same thing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所以一旦我弄清楚了，我实际上能够在此基础上进行改进，并且我会向你展示我做了什么。这就是我非常幸运能够使用PyTorch的原因，因为我能够创建出我想要的定制化内容，并且通过使用GPU也非常快速。这就是Fast
    AI版本的NBSVM。实际上，我的朋友Stephen Merity是一位在自然语言处理领域出色的研究人员，他将其命名为NBSVM++，我觉得这很可爱，所以这就是，尽管没有SVM，但是它是一个逻辑回归，但正如我所说，几乎完全相同。
- en: So let me first of all show you the code. Once I figured out this is the best
    way I can come up with to do a linear bag of words model, I embedded it into Fast
    AI so you can just write a couple lines of code.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 所以首先让我向你展示代码。一旦我弄清楚这是我能想到的最好的线性词袋模型的方法，我将其嵌入到Fast AI中，这样你只需写几行代码就可以了。
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So the code is basically, hey, I want to create a data class for text classification,
    I want to create it form a bag of words(`from_bow`). Here is my bag of words (`trn_term_doc`)
    and here are my labels (`trn_y`), here are the same thing for the validation set
    and use up to 2000 unique words per review, which is plenty.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以代码基本上是，嘿，我想为文本分类创建一个数据类，我想从词袋（`from_bow`）中创建它。这是我的词袋（`trn_term_doc`），这是我们的标签（`trn_y`），这是验证集的相同内容，并且每个评论最多使用2000个独特的单词，这已经足够了。
- en: So then from that model data, construct a learner which is kind of the Fast
    AI generalization of a model which is based on a dot product of Naive Bayes and
    then fit that model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后从那个模型数据中，构建一个学习器，这是Fast AI对基于朴素贝叶斯点积的模型的泛化，然后拟合该模型。
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: And after 5 epochs, I was already up to 92.2\. So this is now getting quite
    well above the linear baseline (in the original paper). So let me show you the
    code for that.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 经过5个时代，我的准确率已经达到了92.2。所以现在已经远远超过了线性基准（在原始论文中）。所以让我给你展示一下那段代码。
- en: '![](../Images/41d46ea0fbd3a243fb835e05cc6d0f9d.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41d46ea0fbd3a243fb835e05cc6d0f9d.png)'
- en: So the code is horrifying short. This is it. And this will also look, on the
    whole, extremely familiar. There are a few tweaks here, pretend this thing that
    says `Embedding` pretend that it actually says `Linear`. I’m going to show you
    embedding in a moment. So we’ve got basically a linear layer where the number
    of features as the rows and remember, sklearn features means number of words basically.
    Then for each word, we’re going to create one weight which makes sense — a logistic
    regression, each word has one weight. And then we are going to be multiplying
    it by the *r* value, so each word, we have one *r* value per class. So I actually
    made this so this can handle not just positive versus negative but maybe figuring
    out which author created this work — there could be five or six authors, for example.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所以代码非常简短。就是这样。这看起来也非常熟悉。这里有一些小调整，假装这个写着`Embedding`的东西实际上写着`Linear`。我马上会展示给你看embedding。所以我们基本上有一个线性层，特征的数量作为行，记住，sklearn特征意味着基本上是单词的数量。然后对于每个单词，我们将创建一个权重，这是有道理的——逻辑回归，每个单词有一个权重。然后我们将它乘以*r*值，所以每个单词，我们有一个*r*值每个类。所以我实际上做了这个，这样可以处理不仅仅是正面和负面，还可以找出是哪个作者创作了这个作品——例如可能有五六个作者。
- en: 'And basically we use those linear layers to get the value of the weight and
    the value of the *r,* then we take the weight times the *r* and then sum it up.
    So that’s just a simple dot product just as we would do for any logistic regression
    and then do the softmax. The very minor tweak we added to get the better result
    is this `+self.w_adj`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上我们使用这些线性层来得到权重和*r*的值，然后我们取权重乘以*r*，然后相加。所以这只是一个简单的点积，就像我们为任何逻辑回归所做的那样，然后进行softmax。我们为了获得更好的结果所做的非常小的调整是这个`+self.w_adj`：
- en: '![](../Images/589553ad33a72181572851200fb4c0c2.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/589553ad33a72181572851200fb4c0c2.png)'
- en: The thing I’m adding is, it’s a parameter, but I pretty much always use this
    default value 0.4\. So what does this do? What this is doing is it’s again changing
    the prior. If you think about it, even once we used this *r* times the term document
    matrix as their independent variables, you really want to start with a question,
    okay, the penalty terms are still pushing `w` down to zero.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我添加的东西是，这是一个参数，但我几乎总是使用这个默认值0.4。那么这是做什么的呢？这再次改变了先验。如果你考虑一下，即使我们将这个*r*乘以文档矩阵作为它们的自变量，你真的想从一个问题开始，好的，惩罚项仍然在将`w`推向零。
- en: '![](../Images/63d75d3e8a695af9aaa6190bb53be376.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63d75d3e8a695af9aaa6190bb53be376.png)'
- en: So what does it mean for `w` to be zero? What would it mean if we had coefficient
    all 0's?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 那么`w`为零意味着什么？如果我们的系数都是0会怎么样？
- en: '![](../Images/874c67e85e8c3a548fb3cf47b95c14e3.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/874c67e85e8c3a548fb3cf47b95c14e3.png)'
- en: When we multiply this matrix with these coefficients, we still get zero. So
    a weight of zero still ends up saying “I have no opinion on whether this thing
    is positive or negative.” On the other hand, if they were all 1's, then it basically
    says my opinion is that the Naive Bayes coefficients are exactly right. So the
    idea is that I said zero is almost certainly not the right prior. We shouldn’t
    really be saying if there’s no coefficient, it means ignore the Naive Bayes coefficient.
    1 is probably too high because we actually think that Naive Bayes is only part
    of the answer. So I played around with a few different datasets where I basically
    said take the weights and add to them some constant. So zero would become, in
    this case, 0.4\. In other words, the regularization penalty is pushing the weights
    not towards zero but towards this value. And I found that across a number of datasets,
    0.4 works pretty well and pretty resilient. Again, the basic idea is to get the
    best of both worlds where we are learning from the data using a simple model,
    but we are incorporating our prior knowledge as best as we can. So it turns out
    when you say let’s tell it that weight matrix of zeros actually means you should
    use about a half of the *r* values, that ends up working better than prior that
    the weights should all be zero.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这个矩阵与这些系数相乘时，我们仍然得到零。所以权重为零最终会说“我对这个事情是正面还是负面没有意见。”另一方面，如果它们都是1，那么基本上就是说我的意见是朴素贝叶斯系数是完全正确的。所以我说零几乎肯定不是正确的先验。我们不应该真的说如果没有系数，那就意味着忽略朴素贝叶斯系数。1可能太高了，因为我们实际上认为朴素贝叶斯只是答案的一部分。所以我尝试了几个不同的数据集，基本上是说取这些权重并加上一些常数。所以零在这种情况下会变成0.4。换句话说，正则化惩罚将权重推向这个值而不是零。我发现在许多数据集中，0.4效果非常好且非常稳健。再次，基本思想是在使用简单模型从数据中学习的同时，尽可能地融入我们的先验知识。所以结果是，当你说让权重矩阵的零实际上意味着你应该使用大约一半的*r*值时，这比权重应该全部为零的先验效果更好。
- en: '**Question**: Is `w` the point for denoting the amount of regularization required
    [[50:31](https://youtu.be/XJ_waZlJU8g?t=3031)]? `w` are the weights. So `x = ((w+self.w_adj)*r/self.r_adj).sum(1)`
    is calculating our activations. We calculate our activations as being equal to
    the weights times the *r*, them sum. So that’s just our normal linear function.
    The thing which is being penalized is my weight matrix. That’s what gets penalized.
    So by saying, hey you know what, don’t just use `w` — use `w+0.4`. 0.4 (i.e. `self.w_adj`)
    is not being penalized. It’s not part of the weight matrix. So effectively, the
    weight matrix gets 0.4 for free.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：`w`是表示所需正则化量的点吗？`w`是权重。所以`x = ((w+self.w_adj)*r/self.r_adj).sum(1)`正在计算我们的激活。我们计算我们的激活等于权重乘以*r*，然后求和。所以这只是我们的正常线性函数。被惩罚的是我的权重矩阵。这就是受到惩罚的地方。所以通过说，嘿，你知道，不要只使用`w`
    —— 使用`w+0.4`。0.4（即`self.w_adj`）不受惩罚。它不是权重矩阵的一部分。因此，权重矩阵实际上免费获得了0.4。
- en: '**Question**: By doing this, even after regularization, every feature is getting
    some form of minimum weight [[51:50](https://youtu.be/XJ_waZlJU8g?t=3110)]? Not
    necessarily because it could end up choosing a coefficient of `-0.4` for a feature
    and that would say “you know what, even though Naive Bayes says it’s the *r* should
    be whatever for this feature. I think you should totally ignore it”.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：通过这样做，即使经过正则化，每个特征都会获得一些形式的最小权重吗？不一定，因为它最终可能会为一个特征选择一个系数为`-0.4`，这将表示“你知道，即使朴素贝叶斯说对于这个特征*r*应该是什么，我认为你应该完全忽略它”。
- en: 'A couple of questions during the break [[52:46](https://youtu.be/XJ_waZlJU8g?t=3166)].
    The first was a bit of a summary as to what’s going on here:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 休息期间有几个问题。第一个是关于这里正在发生的事情的总结：
- en: '![](../Images/b523bbad3f5f47180315a917221222c7.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b523bbad3f5f47180315a917221222c7.png)'
- en: 'Here we have `w` plus weight adjustment times *r*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有`w`加上权重调整乘以*r*：
- en: '![](../Images/0572467976b682232467c2a7fe543621.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0572467976b682232467c2a7fe543621.png)'
- en: 'So normally, what we are doing is saying logistic regression is basically `wx`
    (I’m going to ignore the bias). Then we are changing it to be `rx·w`. Then we
    were saying that let’s do `x·w` bit first. This thing here, I actually call w
    which is probably pretty bad, it’s actually `w` times `x` :'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所以通常，我们所做的是说逻辑回归基本上是`wx`（我将忽略偏差）。然后我们将其更改为`rx·w`。然后我们说让我们先做`x·w`这部分。这里的这个东西，我实际上称之为w，这可能很糟糕，实际上是`w`乘以`x`：
- en: '![](../Images/06303dc4e6c5658a85e6f29a3151a651.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06303dc4e6c5658a85e6f29a3151a651.png)'
- en: So instead of `r(x·w)`, I’ve got `w·x` plus a constant times *r*. So the key
    idea here is that regularization wants the weights to be zero, because it’s trying
    to reduce Σ*w*². So what we are saying is, ok we want to push the weights towards
    zero because that’s our default starting point expectation. So we want to be in
    a situation where if the weights are zero then we have a model that makes theoretical
    or intuitive sense to us. This model (`r(x·w)`), if the weights are zero, doesn’t
    make intuitive sense to us. Because it’s saying hey, multiply everything by zero
    gets rid of everything. We are actually saying “no, we actually think our *r*
    is useful and we actually want to keep that.” So instead, let’s take `(x·w)` and
    add 0.4 to it. So now, if the regularizer is pushing the weights towards zero,
    then it’s pushing the value of the sum to 0.4.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我没有`r(x·w)`，我有`w·x`加上一个常数乘以*r*。所以这里的关键思想是正则化希望权重为零，因为它试图减少Σ*w*²。所以我们所说的是，好吧，我们希望将权重推向零，因为这是我们的默认起点期望。所以我们希望处于这样一种情况，即如果权重为零，那么我们有一个对我们来说在理论上或直观上有意义的模型。这个模型（`r(x·w)`），如果权重为零，对我们来说没有直观意义。因为它在说，嘿，将所有东西乘以零会消除一切。我们实际上在说“不，我们实际上认为我们的*r*是有用的，我们实际上想保留它。”所以，让我们取`(x·w)`并加上0.4。所以现在，如果正则化器将权重推向零，那么它将将总和的值推向0.4。
- en: '![](../Images/d2f6701443bf2e2976243686ceb0152b.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2f6701443bf2e2976243686ceb0152b.png)'
- en: Therefore, it’s pushing a whole model to 0.4 times *r*. So in other words, our
    kind of default starting point if you’ve regularize to all the weights out all
    together is to say “yeah, you know, let’s use a bit of *r.* That’s probably a
    good idea.” So that’s the idea. The idea is basically what happens when that weight
    is zero. And you want that to be something sensible because otherwise regularizing
    the weights to move in that direction wouldn’t be such a good idea.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它将整个模型推向0.4倍*r*。换句话说，如果您将所有权重一起正则化到0.4倍*r*，那么我们的默认起点是说“是的，您知道，让我们使用一点*r*。这可能是一个好主意。”这就是这个想法。这个想法基本上是当权重为零时会发生什么。您希望那是有意义的，否则正则化权重朝着那个方向移动就不是一个好主意。
- en: 'The second question was about n-grams [[56:55](https://youtu.be/XJ_waZlJU8g?t=3415)].
    So the N in n-gram can be uni, bi, tri, whatever. 1, 2, 3, whatever grams. So
    “This movie is good” has four unigrams: `This`, `movie`, `is`, `good`. It has
    three bigrams: `This movie`, `movie is`, `is good`. It has two trigrams: `This
    movie is`, `movie is good`.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是关于n-grams。所以n-gram中的N可以是uni，bi，tri，等等。1，2，3，等等个grams。所以“This movie is
    good”有四个unigrams：`This`，`movie`，`is`，`good`。它有三个bigrams：`This movie`，`movie is`，`is
    good`。它有两个trigrams：`This movie is`，`movie is good`。
- en: 'Question: Do you mind going back to `w_adj` or `0.4` stuff? I was wondering
    if this adjustment will harm the predictability of the model because think of
    extreme case if it’s not 0.4, if it’s 4,000 and all coefficients will be essentially…
    [[57:45](https://youtu.be/XJ_waZlJU8g?t=3465)]? Exactly. So our prior needs to
    make sense. This is why it’s called DotProdNB, so the prior is that this is something
    where we think Naive Bayes is a good prior. So Naive Bayes says that *r = p/q*
    is a good prior and not only do we think it’s a good prior but we think *rx+b*
    is a good model. That’s the Naive Bayes model. So in other words, we expect that
    a coefficient of 1 is a good coefficient, not 4,000\. Specifically, we think zero
    is probably not a good coefficient. But we also think that maybe the Naive Bayes
    version is a little over confident. So maybe 1 is a little high. So we are pretty
    sure that the right number, assuming that Naive Bayes model is appropriate, is
    between 0 and 1.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：您介意回到`w_adj`或`0.4`的内容吗？我在想这种调整会不会损害模型的可预测性，因为想象一下极端情况，如果不是0.4，如果是4,000，那么所有系数基本上会是...？确切地说。因此，我们的先验需要有意义。这就是为什么它被称为DotProdNB，因此先验是我们认为朴素贝叶斯是一个好的先验的地方。因此，朴素贝叶斯认为*r
    = p/q*是一个好的先验，我们不仅认为这是一个好的先验，而且我们认为*rx+b*是一个好的模型。这就是朴素贝叶斯模型。换句话说，我们期望系数为1是一个好的系数，而不是4,000。具体来说，我们认为零可能不是一个好的系数。但我们也认为也许朴素贝叶斯版本有点过于自信。所以也许1有点高。因此，我们相当确定，假设朴素贝叶斯模型是适当的，正确的数字在0和1之间。
- en: '![](../Images/18e3268b8c4cef95bc4a3348a5aed1af.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18e3268b8c4cef95bc4a3348a5aed1af.png)'
- en: '**Question continued**: But what I was thinking is as long as it’s not zero,
    you are pushing those coefficients that are supposed to be zero to something not
    zero and make the high coefficients less distinctive from zero coefficients [[59:24](https://youtu.be/XJ_waZlJU8g?t=3564)]?
    Well, but you see, they are not supposed to be zero. They are supposed to be *r*.
    And remember, this is inside our forward function, so this is part of what we
    are taking the gradient of. So it’s basically saying, okay, you can still set
    self.w to anything you like. But just the regularizer wants it to be zero. So
    all we are saying is okay if you want it to be zero, then I’ll try to make zero
    give a sensible answer.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题继续**：但我在想的是只要不是零，您就会将那些应该为零的系数推到非零的地方，并使高系数与零系数之间的差异变小。嗯，但是您看，它们本来就不应该是零。它们应该是*r*。请记住，这是在我们的前向函数中，所以这是我们正在计算梯度的一部分。所以基本上是说，好吧，您仍然可以将self.w设置为您喜欢的任何值。但是正则化器希望它为零。所以我们所说的是，好吧，如果您希望它为零，那么我将尝试使零给出一个合理的答案。'
- en: Nothing says 0.4 is perfect for every dataset. I’ve tried a few different datasets
    and found various numbers between 0.3 and 0.6 that are optimal. But I’ve never
    found one where 0.4 is less good than zero which is not surprising. And I’ve also
    never found where one is better. So the idea is this is a reasonable default but
    it’s another parameter you can play with which I kind of like. It’s another thing
    you could use a grid search or whatever to figure out for your dataset what’s
    best. Really, the key here being every model before this one, as far as I know,
    has implicitly assumed it should be zero because they don’t have this parameter.
    And by the way, I’ve actually got a second parameter here (`r_adj=10`) as well
    which is the same thing I do to r is actually divide by a parameter which I’m
    not going to worry too much about it now but it’s another parameter you can use
    to adjust what the nature of the regularization is. In the end, I’m a empiricist,
    not a theoretician. I thought this seemed like a good idea. Nearly all of my things
    that seem like a good idea turn out to be stupid. This particular one gave good
    result on this dataset and a few other ones as well.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人说0.4对于每个数据集都是完美的。我尝试了一些不同的数据集，并发现在0.3和0.6之间有一些最佳值。但我从未发现一个比零更好的数据集，这并不奇怪。我也从未发现一个更好的数据集。因此，这个想法是一个合理的默认值，但这是另一个您可以玩耍的参数，我有点喜欢。这是另一件您可以使用网格搜索或其他方法来找出对您的数据集最佳的东西。实际上，关键在于在这个模型之前的每个模型，据我所知，都隐含地假设它应该为零，因为它们没有这个参数。顺便说一句，我实际上还有第二个参数（`r_adj=10`），它是我对r做的相同的事情，实际上是通过一个参数除以r，我现在不会太担心，但这是另一个您可以用来调整正则化性质的参数。最终，我是一个实证主义者，而不是一个理论家。我认为这似乎是一个好主意。几乎所有我认为是一个好主意的事情最终都被证明是愚蠢的。这个特定的想法在这个数据集上给出了良好的结果，也在其他一些数据集上给出了良好的结果。
- en: '**Question:** I am still confused about `w + w_adj`. You mentioned we do `w
    + w_adj` so that the coefficients don’t get set to zero that we place some importance
    on the priors. But you also said that the effect of learning can be that `w` get
    set to a negative value which could make `w + w_adj` to be zero. So if we are
    allowing the learning process to indeed set the coefficients to zero, why is that
    different from just having `w` [[1:01:47](https://youtu.be/XJ_waZlJU8g?t=3707)]?
    Because of regularization. Because we are penalizing it by Σ*w*². So in other
    words, we are saying, you know what, if the best thing to do is to ignore the
    value of *r*, that’ll cost you (Σ*w*²). You are going to have to set `w` to a
    negative number. So only do that if that’s clearly a good idea. Unless it’s clearly
    a good idea, then you should leave it where it is. That’s the only reason. Like
    all of this stuff we’ve done today is basically entirely about maximizing the
    advantage we get from regularization and saying regularization pushes us towards
    some default assumption, and nearly all of the machine learning literature assumes
    that default assumption is everything is zero. And I am saying it turns out, it
    makes sense theoretically and turns out empirically that actually you should decide
    what your default assumption is and that’ll give you better results.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：**我仍然对`w + w_adj`感到困惑。你提到我们执行`w + w_adj`是为了不让系数设为零，我们对先验赋予了一些重要性。但你也说过学习的效果可能是`w`被设为负值，这可能会使`w
    + w_adj`为零。所以如果我们允许学习过程确实将系数设为零，那为什么这与只有`w`不同呢？因为正则化。因为我们通过Σ*w*²对其进行惩罚。换句话说，我们在说，你知道，如果忽略*r*是最好的选择，那将会花费你（Σ*w*²）。你将不得不将`w`设为负数。所以只有在这显然是一个好主意的情况下才这样做。除非这显然是一个好主意，否则你应该将其保留在原处。这是唯一的原因。今天我们所做的所有事情基本上都是为了最大化我们从正则化中获得的优势，并且说正则化将我们推向某种默认假设，几乎所有的机器学习文献都假设默认假设是所有事物都是零。我在说的是，从理论上讲这是有道理的，而从经验上讲，事实证明你应该决定你的默认假设是什么，这将给你带来更好的结果。'
- en: '**Question continued**: So would it be right to say that in a way you are putting
    an additional hurdle along the way towards getting all coefficients to zero, and
    it will be able to do that if it is really worth it [[1:03:30](https://youtu.be/XJ_waZlJU8g?t=3810)]?
    Yes, exactly. So I’d say the default hurdle without this is making a coefficient
    non-zero is the hurdle. And now I’m saying, no, the hurdle is making the coefficient
    not be equal to 0.4*r*.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题继续：**那么可以这样说，在某种程度上你是在前往将所有系数设为零的过程中设置了一个额外的障碍，如果确实值得的话，它将能够做到这一点吗？是的，确实如此。所以我会说，没有这个默认障碍，使系数非零是障碍。现在我要说的是，不，障碍是使系数不等于0.4*r*。'
- en: '**Question:** So this is sum of w² times some constant. If the constant was,
    say 0.1, then the weight might not go towards zero. Then we might not need weight
    decay [[1:04:03](https://youtu.be/XJ_waZlJU8g?t=3843)]? If the value of the constant
    is zero, then there is no regularization. But if this value is higher than zero,
    then there is some penalty. And presumably, we’ve set it to nonzero because we
    were overfitting. So we want some penalty. So if there is some penalty, then my
    assertion is that we should penalize things that are different to our prior, not
    that we should penalize things that are different to zero. And our prior is that
    things should be around about equal to *r.*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：**所以这是w²乘以某个常数的总和。如果常数是，比如说0.1，那么权重可能不会趋向于零。那么我们可能就不需要权重衰减了？如果常数的值为零，那么就没有正则化。但如果这个值大于零，那么就会有一些惩罚。而且可以推测，我们将其设置为非零是因为我们过拟合了。所以我们想要一些惩罚。所以如果有一些惩罚，那么我的观点是我们应该惩罚那些与我们的先验不同的事物，而不是惩罚那些与零不同的事物。我们的先验是事物应该大致等于*r*。'
- en: Embedding [[1:05:17](https://youtu.be/XJ_waZlJU8g?t=3917)]
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入[[1:05:17](https://youtu.be/XJ_waZlJU8g?t=3917)]
- en: I want to talk about Embedding. I said pretend it’s linear and indeed we can
    pretend it’s linear. Let me show you how much we can pretend it’s linear as in
    `nn.Linear`, create a linear layer.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我想谈谈嵌入。我说假装它是线性的，实际上我们可以假装它是线性的。让我向你展示我们可以多么地假装它是线性的，就像`nn.Linear`，创建一个线性层。
- en: Here is our data matrix, here are our coefficients *r* if we are doing the *r*
    version. So if we were to put *r* into a column vector, then we could do a matrix
    multiply of the data matrix by the coefficients.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的数据矩阵，这是我们的系数*r*如果我们正在进行*r*版本。所以如果我们将*r*放入列向量中，那么我们可以通过系数对数据矩阵进行矩阵乘法。
- en: '![](../Images/b14bff4d37b2331d47519325143246b4.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b14bff4d37b2331d47519325143246b4.png)'
- en: 'So the matrix multiply of this independent variable matrix by this coefficient
    matrix is going to give us an answer. So the question is, okay, why didn’t Jeremy
    write `nn.Linear`? Why did Jeremy write `nn.Embedding`? The reason is, if you
    recall, we don’t actually store it like this. Because this actually of width 800,000
    and of height 25,000\. So rather than storing it like this, we actually store
    it as this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个自变量矩阵乘以这个系数矩阵的矩阵乘法将给我们一个答案。所以问题是，好吧，为什么Jeremy没有写`nn.Linear`？为什么Jeremy写了`nn.Embedding`？原因是，如果你回忆一下，我们实际上并不是这样存储的。因为这实际上是宽度为800,000，高度为25,000。所以我们实际上是这样存储的：
- en: '![](../Images/ffec34e3bccbefc168e3f50310577e32.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffec34e3bccbefc168e3f50310577e32.png)'
- en: 'The way we store is, this bag of words contain which word indexes. This is
    a sparse way of storing it. It just lists out the indexes in each sentence. So
    given that, I want to now do that matrix multiply that I just showed you to create
    that same outcome. But I want to do it from the sparse representation. This is
    basically one hot encoded:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的存储方式是，这个词袋包含哪些单词索引。这是一种稀疏的存储方式。它只列出每个句子中的索引。鉴于此，我现在想要执行我刚刚向你展示的那种矩阵乘法，以创建相同的结果。但我想要从稀疏表示中执行。这基本上是一种独热编码：
- en: '![](../Images/8a1933af48ed522c36bf35bca13760d5.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a1933af48ed522c36bf35bca13760d5.png)'
- en: 'It’s kind of like a dummy matrix version. Does it have a word “this”? Does
    it have a word “movie”? And so forth. So if we took the simple version of does
    it have a word “this” (i.e. 1, 0, 0, 0, 0, 0) and we multiplied that by *r*, then
    it’s just going to return the first item:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点像一个虚拟矩阵版本。它有一个单词“this”吗？它有一个单词“movie”吗？等等。所以如果我们采用简单版本的有没有单词“this”（即1, 0,
    0, 0, 0, 0），然后我们将其乘以*r*，那么它只会返回第一个项目：
- en: '![](../Images/0ec14f58529397f72aec5d677c0ae198.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ec14f58529397f72aec5d677c0ae198.png)'
- en: 'So in general, a one hot encoded vector times a matrix is identical to looking
    up that matrix to find the *n-*th row in it. So this is just saying find the 0th,
    first, second, and fifth coefficients:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，一个独热编码向量乘以一个矩阵等同于查找该矩阵中的第 n 行。 所以这只是说找到第 0、第一个、第二个和第五个系数：
- en: '![](../Images/78ee2e21890ede77335d59533eb88b3f.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78ee2e21890ede77335d59533eb88b3f.png)'
- en: 'They are exactly the same thing. In this case, I only have one coefficient
    per feature, but actually the way I did this was to have one coefficient per feature
    for each class. So in this case, classes are positive and negative. So I actually
    had *r* positive (*p/q*), *r* negative (*q/r*):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 它们完全相同。 在这种情况下，每个特征只有一个系数，但实际上我这样做的方式是为每个类别的每个特征都有一个系数。 所以在这种情况下，类别是正面和负面。 所以我实际上有
    *r* 正面 (*p/q*)，*r* 负面 (*q/r*)：
- en: '![](../Images/66f4f8e4cba3f1f72a2d2a6f3b290aae.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66f4f8e4cba3f1f72a2d2a6f3b290aae.png)'
- en: In the binary case, obviously it’s redundant to have both. But what if it was
    like what’s the author of this text? Is it Jeremy, Savannah, or Terrence? Now
    we’ve got three categories, we want three values of `r`. So the nice thing is
    doing this sparse version, you can just look up the 0th, first, second and fifth.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在二进制情况下，显然同时拥有两者是多余的。 但是如果是像这个文本的作者是谁？ 是 Jeremy，Savannah 还是 Terrence？ 现在我们有三个类别，我们想要三个
    `r` 的值。 所以做这个稀疏版本的好处是，你可以查找第 0、第一个、第二个和第五个。
- en: '![](../Images/99b1f323d6a9dc931cb38cdd938f4650.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99b1f323d6a9dc931cb38cdd938f4650.png)'
- en: 'Again, it’s mathematically identical to multiplying by a one hot encoded matrix.
    But when you have sparse inputs, it’s obviously much much more efficient. So this
    computational trick which is mathematically identical to, not conceptually analogous
    to, multiplying by one hot encoded matrix is called an embedding. I’m sure most
    of you probably heard about embeddings, like word embeddings: Word2Vec, GloVe,
    etc. And people love to make them sound like this amazing new complex neural net
    thing. They are not. Embedding means make a multiplication by a one hot encoded
    matrix faster by replacing it with a simple array lookup. That’s why I said you
    can think of this as if it said `self.w = nn.Linear(nf+1, 1)` :'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，从数学上讲，这与乘以一个独热编码矩阵是相同的。 但是，当输入稀疏时，效率显然要高得多。 因此，这个计算技巧在数学上与乘以一个独热编码矩阵是相同的，而不是概念上类似于。
    这被称为嵌入。 我相信大多数人可能已经听说过嵌入，比如词嵌入：Word2Vec，GloVe等。 人们喜欢把它们说成是这种令人惊叹的新复杂神经网络东西。 但事实并非如此。
    嵌入意味着通过简单的数组查找来加快乘以一个独热编码矩阵的过程。 这就是为什么我说你可以把这个想象成说 `self.w = nn.Linear(nf+1, 1)`：
- en: '![](../Images/9570eeb2e7bdab3a61aa5258be418878.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9570eeb2e7bdab3a61aa5258be418878.png)'
- en: 'Because it actually does the same thing. It actually is a matrix with these
    dimensions. It’s a linear layer, but it’s expecting that the input we are going
    to give it is not actually one hot encoded matrix but is actually a list of integers
    — the indexes for each word of each item. So you can see that the `forward` function
    in Fast AI automatically gets (for DotProdNB leaner) the feature indexes (`feature_idx`):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它实际上做的是相同的事情。 它实际上是一个具有这些维度的矩阵。 这是一个线性层，但它期望我们要给它的输入实际上不是一个独热编码矩阵，而是一个整数列表
    —— 每个项目的每个单词的索引。 所以你可以看到 Fast AI 中的 `forward` 函数自动获取（对于 DotProdNB leaner）特征索引（`feature_idx`）：
- en: '![](../Images/f151c4ed098357257fccf5b69e3998a8.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f151c4ed098357257fccf5b69e3998a8.png)'
- en: So they come from the sparse matrix automatically. Numpy makes it very easy
    to just grab those indexes. So in other words, we’ve got here (`feat_idx`)a list
    of each word index of the 800,000 that are in this document. So then this here
    (`self.w(feat_idx)`) says look up each of those in our embedding matrix which
    got 800,000 rows and return each thing that you find. So mathematically identical
    to multiplying by the one hot encoded matrix. That’s all embedding is. And what
    that means is we can now handle building any kind of model like whatever kind
    of neural network where we have potentially very high cardinality categorical
    variables as our inputs. We can then just turn them into a numeric code between
    zero and the number of levels, and then we can learn a linear layer from that
    as if we had one hot encoded it without ever actually constructing the one hot
    encoded version and without ever actually doing that matrix multiply. Instead,
    we will just store the index version and simply do the array lookup. So the gradients
    that are flowing back, basically in the one hot encoded version, everything that
    was zero has no gradient so the gradients flowing back is just going to update
    the particular row of the embedding matrix that we used. So that’s fundamentally
    important for NLP just like here, I wanted to create a PyTorch model that would
    implement this ridiculously simple little equation.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它们自动来自稀疏矩阵。 Numpy使得很容易只需抓取这些索引。 换句话说，我们在这里（`feat_idx`）有一个包含这个文档中的 800,000
    个单词索引的列表。 所以这里（`self.w(feat_idx)`）说的是查找我们的嵌入矩阵中的每一个，该矩阵有 800,000 行，并返回你找到的每一个东西。
    从数学上讲，这与乘以一个独热编码矩阵是相同的。 这就是所有嵌入的含义。 这意味着我们现在可以处理构建任何类型的模型，比如任何类型的神经网络，其中我们的输入可能是非常高基数的分类变量。
    然后我们只需将它们转换为介于零和级别数之间的数字代码，然后我们可以学习一个线性层，就好像我们已经对其进行了独热编码，而实际上并没有构建独热编码版本，也没有进行矩阵乘法。
    相反，我们将只存储索引版本并简单地进行数组查找。 因此，回流的梯度基本上是在独热编码版本中，所有为零的东西都没有梯度，因此回流的梯度只会更新我们使用的嵌入矩阵的特定行。
    这对于自然语言处理非常重要，就像在这里一样，我想创建一个 PyTorch 模型，该模型将实现这个非常简单的方程。
- en: '![](../Images/9859769f29c61b80d1fed1938979a5c3.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9859769f29c61b80d1fed1938979a5c3.png)'
- en: To do it without this trick would have meant I was feeding in a 25,000 by 80,000
    element array which would have been kind of crazy. So this trick allowed me to
    write this. I just replaced the word Linear with Embedding, replaced the thing
    that feeds the one hot encodings in with something that just feeds the indexes
    in. And that was it. Then it kept working and so this now trains in about a minute
    per epoch.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有这个技巧，那意味着我要输入一个 25,000 x 80,000 元素的数组，这将有点疯狂。 所以这个技巧让我写下了这个。 我只是用 Embedding
    替换了 Linear，用一些只输入索引而不是输入独热编码的东西来替换那个，就这样。 然后它继续工作，所以现在每个时代的训练时间大约是一分钟。
- en: What we can now do is we can now take this idea and apply it not just to language
    but to anything [[1:15:30](https://youtu.be/XJ_waZlJU8g?t=4530)]. For example
    predicting the sales of items at a grocery store.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以把这个想法应用到不仅仅是语言，而是任何东西上。 例如，预测杂货店商品的销售情况。
- en: '**Question**: We are not actually looking up anything, right? We are just seeing
    that array with the indices that is the representation [[1:15:52](https://youtu.be/XJ_waZlJU8g?t=4552)]?
    So we are doing a lookup. The representation that’s being stored for the bag of
    words is now not 1 1 1 0 0 1 but 0 1 2 5\. So then we actually have to do our
    matrix product. But rather than doing the matrix product, we look up the zero-th
    thing and the first thing, the second thing, and the fifth thing.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：我们实际上并没有查找任何东西，对吧？我们只是看到了那个带有索引的数组表示？所以我们正在查找。现在存储的词袋的表示不再是1 1 1 0 0 1，而是0
    1 2 5。因此，我们实际上必须进行矩阵乘法。但是我们不是进行矩阵乘法，而是查找第零个东西，第一个东西，第二个东西和第五个东西。
- en: 'Question continued: So that means we are still retaining the one hot encoded
    matrix [[1:16:31](https://youtu.be/XJ_waZlJU8g?t=4591)]? No, we didn’t. There
    is no one hot encoded matrix used here. The one hot encoded matrix is not currently
    highlighted. We’ve currently highlighted the list of indexes and the list of coefficients
    from the weight matrix:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 问题继续：这意味着我们仍然保留了独热编码矩阵吗？不，我们没有。这里没有使用独热编码矩阵。目前没有突出显示独热编码矩阵。我们目前突出显示的是索引列表和权重矩阵中的系数列表：
- en: '![](../Images/c53eea454293ac4bd3291fb95c51ba6c.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c53eea454293ac4bd3291fb95c51ba6c.png)'
- en: So what we are going to do now is we are going to go a step further and saying
    let’s not use a linear model at all, let’s use a multi layer neural network [[1:16:58](https://youtu.be/XJ_waZlJU8g?t=4618)].
    And let’s have the input to that potentially be include some categorical variables.
    And those categorical variables, we will just have as numeric indexes. So the
    first layer for those won’t be a normal linear layer, they will be an embedding
    layer which we know behaves exactly like a linear layer mathematically. So then
    our hope will be that we can now use this to create a neural network for any kind
    of data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们要做的是更进一步，我们要说根本不使用线性模型，让我们使用一个多层神经网络。让我们的输入可能包括一些分类变量。这些分类变量，我们将只将其作为数值索引。因此，这些的第一层不会是一个普通的线性层，它们将是一个嵌入层，我们知道在数学上它的行为与线性层完全相同。因此，我们的希望是现在我们可以使用这个来为任何类型的数据创建一个神经网络。
- en: Rossmann competition [[1:17:40](https://youtu.be/XJ_waZlJU8g?t=4660)]
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 罗斯曼竞赛
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb)'
- en: There was a competition on Kaggle a few years ago called Rossmann which is a
    German grocery chain where they asked to predict the sales of items in their stores.
    And that included the mixture of categorical and continuous variables. In [this
    paper](https://arxiv.org/abs/1604.06737) by Guo/Berkhahn, they described their
    third-place winning entry which was much simpler than the first place winning
    entry but nearly as good but much much simpler because they took advantage of
    this idea of what they call entity embeddings. In the paper, they thought they
    had invented this, actually it had been written before earlier by Yoshua Bengio
    and his co-authors in another Kaggle competition which was predicting taxi destinations.
    Although, I will say I feel like Guo went a lot further in describing how this
    can be used in many other ways, so we’ll talk about that as well.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前在Kaggle上有一个名为Rossmann的竞赛，这是一个德国的杂货连锁店，他们要求预测他们商店中商品的销售情况。这包括分类和连续变量的混合。在Guo/Berkhahn的[这篇论文](https://arxiv.org/abs/1604.06737)中，他们描述了他们的第三名作品，这比第一名作品简单得多，但几乎一样好，但简单得多，因为他们利用了这个所谓的实体嵌入的想法。在论文中，他们认为他们发明了这个，实际上早些时候由Yoshua
    Bengio和他的合著者在另一个Kaggle竞赛中写过。尽管如此，我觉得Guo在描述这个如何在许多其他方面使用上走得更远，所以我们也会谈论这个。
- en: The notebook is in deep learning repo because we talked about some of the deep
    learning specific aspects in the deep learning course, where else in this course,
    we are going to be talking mainly about the feature engineering and we are also
    going to be talking about this embedding idea.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本在深度学习存储库中，因为我们在深度学习课程中讨论了一些深度学习特定方面，在这门课程中，我们主要将讨论特征工程，我们还将讨论这个嵌入的想法。
- en: '![](../Images/8139761316639a5d57d4c92b9543aeee.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8139761316639a5d57d4c92b9543aeee.png)'
- en: Let’s start with the data. So the data was, store number 1 on the 31st of July
    2015 was open. They had a promotion going on. There was a school holiday. It was
    not a state holiday, and they sold 5,263 items. So that’s the key data they provided.
    So the goal is obviously to predict sales in a test set that has the information
    without sales. They also tell you that for each store, it’s of some particular
    type, it sells some particular assortment of goods, its nearest competitor is
    some distance away, the competitor opened in September 2008, and there’s some
    more information about promos I don’t know the details of what that means. Like
    in many Kaggle competitions, they let you download external datasets if you wish
    as long as you share them with other competitors. They also told you what state
    each store is in, so people downloaded the name of the different states of Germany,
    they downloaded a file for each state in Germany for each week some kind of Google
    trend data. I don’t know what specific Google trend they got but there was that.
    For each date they downloaded a bunch of temperature information. And that’s it.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据开始。所以数据是，2015年7月31日，第1号店开业。他们正在进行促销活动。有学校假期。不是国家假期，他们卖出了5263件商品。这是他们提供的关键数据。所以目标显然是在没有销售信息的测试集中预测销售额。他们还告诉你，对于每家商店，它是某种特定类型的，销售某种特定种类的商品，其最近的竞争对手距离一定距离，竞争对手在2008年9月开业，还有一些关于促销的更多信息，我不知道这意味着什么。就像许多Kaggle竞赛一样，他们允许您下载外部数据集，只要您与其他竞争者分享。他们还告诉您每家商店所在的州，因此人们下载了德国不同州的名称，他们为德国每周下载了一些谷歌趋势数据。我不知道他们得到了什么具体的谷歌趋势，但是有的。对于每个日期，他们下载了一堆温度信息。就是这样。
- en: One interesting insight here is that there was probably a mistake in some ways
    for Rossmann to design this competition as being one where you could use external
    data [[1:21:05](https://youtu.be/XJ_waZlJU8g?t=4865)]. Because in reality, you
    don’t actually get to find out next week’s weather or next week’s Google trends.
    But when you are competing in Kaggle, you don’t care about that. You just want
    to win so you use whatever you can get.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这里一个有趣的见解是，Rossmann可能在某种程度上犯了一个错误，设计这个比赛是一个可以使用外部数据的比赛。因为实际上，你并不能知道下周的天气或下周的谷歌趋势。但当你参加Kaggle比赛时，你并不在乎这些。你只是想赢，所以你会利用一切可以得到的。
- en: Data cleaning [[1:21:35](https://youtu.be/XJ_waZlJU8g?t=4895)]
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清理
- en: Let’s talk, first of all, about data cleaning. There wasn’t really much feature
    engineering done in this third place winning entry, particularly by Kaggle standards
    where normally every last thing counts. This is a great example of how far you
    can get with a neural net and it certainly reminds me of claims prediction competition
    we talked about yesterday where the winner did no feature engineering and entirely
    relied on deep learning. The laughter in the room, I guess, is from people who
    did a little bit more than no feature engineering in that competition 😄
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们谈谈数据清理。在这个获得第三名的参赛作品中，实际上并没有进行太多的特征工程，特别是按照Kaggle的标准，通常每一个细节都很重要。这是一个很好的例子，展示了使用神经网络可以取得多大的成就，这让我想起了昨天我们谈到的索赔预测比赛，获胜者没有进行任何特征工程，完全依赖深度学习。房间里的笑声，我猜，是来自那些在比赛中进行了一点点特征工程的人们😄
- en: I should mention, by the way, I find that bit where you work hard at a competition
    and then it closes and you didn’t win. And the winner comes out and says this
    is how I won. That’s the bit where you learn the most. Sometimes that’s happened
    to me and it’s been like oh, I thought of that, I thought I tried that, and then
    I go back and I realize I had a bug there, I didn’t test properly, and I learn
    oh okay, I really need to learn to test this thing in this different way. Sometimes
    it’s like, oh I thought of that but I assumed it wouldn’t work, I’ve really got
    to remember to check everything before I make any assumptions. And you know, sometimes
    it’s just like oh, I did not think of that technique, wow, now I know it’s better
    than everything I just tried. Because otherwise if somebody says, hey you know
    here is a really good technique, you’re like okay great. But when you spent months
    trying to do something and somebody else did it better by using that technique,
    that’s pretty convincing. So it’s kind of hard I’m standing up in front of you
    saying here is a bunch of techniques I’ve used and I’ve won some Kaggle competitions
    and I’ve got some state of the art results. But that’s kind of second-hand information
    by the time it hits you. So it’s really great to try things out. And also it’s
    been nice to see particularly I’ve noticed in the deep learning course, quite
    a few of my students have, I’ve said this technique works really well and they’ve
    tried it and they’ve got into the top ten of a Kaggle competition the next day
    and they’re like okay, that counts as working really well. So Kaggle competitions
    are helpful for lots and lots of reasons. But one of the best ways is what happens
    after it finishes and so definitely for the ones that are now finishing up, make
    sure you watch the forums, see what people are sharing in terms of their solutions,
    and if you want to learn more about them, feel free to ask the winners, hey, would
    you tell me more about this or that. People are normally good about explaining.
    Then ideally, try and replicate it yourself. That can turn into a great blog post
    or great kernel to be able to say okay, such-and-such said that they used this
    technique, here is a really short explanation of what that technique is, and here
    is a little bit of code showing how it’s implemented, and here is the results
    showing you can get the same result. That can be a really interesting write-up
    as well.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，我发现在比赛中努力工作，然后比赛结束了你没有赢得比赛。然后获胜者出来说这就是我赢得比赛的方法。这是你学到最多的时候。有时候这种情况发生在我身上，我会想，哦，我想到了那个，我试过了，然后我回去发现我那里有个bug，我没有正确测试，然后我意识到，哦，好吧，我真的需要学会以不同的方式测试这个东西。有时候就像，哦，我想到了那个，但我假设它不会起作用，我真的要记住在做任何假设之前检查一切。你知道，有时候就像，哦，我没有想到那个技术，哇，现在我知道它比我刚刚尝试的一切都要好。否则，如果有人说，嘿，你知道这是一个非常好的技术，你会说好的。但是当你花了几个月的时间尝试做某事，然后别人用那个技术做得更好时，那就相当有说服力了。所以这有点困难，我站在你面前说这里有一堆我用过的技术，我赢得了一些Kaggle比赛，我得到了一些最先进的结果。但是当这些信息传达给你时，已经是二手信息了。所以尝试一些东西真的很棒。而且尤其是在深度学习课程中，我注意到，我的一些学生尝试了我说的这个技术，他们第二天就进入了Kaggle比赛的前十名，他们说，好的，这算是非常有效。Kaggle比赛有很多原因是有帮助的。但其中一个最好的方式是比赛结束后发生的事情，所以对于现在即将结束的比赛，确保你观看论坛，看看人们在分享解决方案方面分享了什么，如果你想了解更多，可以自由地问问获胜者，嘿，你能告诉我更多关于这个或那个吗。人们通常很乐意解释。然后最好是尝试自己复制一下。这可以变成一个很棒的博客文章或很棒的内核，可以说，某某说他们使用了这个技术，这里是这个技术的一个非常简短的解释，这里是一点代码展示它是如何实现的，这里是结果展示你可以得到相同的结果。这也可以是一个非常有趣的写作。
- en: 'It’s always nice to have your data be as easy to understand as possible [[1:24:58](https://youtu.be/XJ_waZlJU8g?t=5098)].
    So in this case the data that came from Kaggle used various integers for the holidays.
    We can just use a boolean of was it a holiday or not. So just clean that up:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 数据尽可能易于理解总是很好的。因此，在这种情况下，来自Kaggle的数据使用各种整数表示假期。我们可以只使用一个布尔值来表示是否是假期。所以只需清理一下：
- en: '[PRE9]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We got quite a few different tables we need to join them all together. I have
    a standard way of joining things together with pandas. I just used the pandas
    merge function and specifically I always do a left join. Left join is where you
    retain all the rows in the left table, and you have a key column and you match
    that with a key column in the right side table and you just merge those that are
    also present in the right table.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有很多不同的表需要将它们全部合并在一起。我有一种用Pandas合并事物的标准方法。我只是使用了Pandas的合并函数，具体来说我总是进行左连接。左连接是保留左表中的所有行，你有一个关键列，将其与右侧表中的关键列匹配，然后合并那些也存在于右表中的行。
- en: '[PRE10]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The key reason that I always do a left join is that after I do the join, I
    always then check if there were things in the right-hand side that are now null:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我总是进行左连接的关键原因是，在进行连接之后，我总是检查右侧是否有现在为空的内容：
- en: '[PRE11]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Because if so, it means that I missed some things. I haven’t shown it here,
    but I also check the number of rows hasn’t varied before and after. If it has,
    that means that the right hand side table wasn’t unique. So even when I’m sure
    something is true, I always also assume that I’ve screwed it up. So I always check.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 因为如果是这样，那就意味着我漏掉了一些东西。我没有在这里展示，但我也检查了行数在之前和之后是否有变化。如果有变化，那就意味着右侧表不是唯一的。所以即使我确定某件事是真的，我也总是假设我搞砸了。所以我总是检查。
- en: 'I could go ahead and merge the state names into the weather:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以继续将州名合并到天气中：
- en: '[PRE12]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you look at the Google trends table, it’s got this week range which I need
    to turn into a date in order to join it [[1:26:45](https://youtu.be/XJ_waZlJU8g?t=5205)]:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看一下谷歌趋势表，它有这个周范围，我需要将其转换为日期以便加入它：
- en: '![](../Images/90e2bb0ba15e474bbd4f66ad5834f12f.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90e2bb0ba15e474bbd4f66ad5834f12f.png)'
- en: The nice thing about doing this in Pandas is that Pandas gives us access to
    all of Python. So for example, inside the series object, there is a `.str` attribute
    that gives you access to all the string processing functions. Just like `.cat`
    gives you access to the categorical functions, `.dt` gives you access to the date
    time functions. So I can now split everything in that column.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pandas中这样做的好处是，Pandas让我们可以访问所有的Python。例如，在系列对象内部，有一个`.str`属性，可以让你访问所有的字符串处理函数。就像`.cat`让你访问分类函数一样，`.dt`让你访问日期时间函数。所以现在我可以拆分该列中的所有内容。
- en: '[PRE13]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And it’s really important to use these Pandas functions because they are going
    to be vectorized, accelerated, often through SIMD at least through C code so that
    runs nice and quickly.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些Pandas函数非常重要，因为它们将被向量化，加速，通常通过SIMD至少通过C代码，以便运行得又快又顺利。
- en: 'And as per usual, let’s add date metadata to our dates [[1:27:41](https://youtu.be/XJ_waZlJU8g?t=5261)]:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，让我们为我们的日期添加日期元数据：
- en: '[PRE14]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the end, we are basically denormalizing all these tables. We are going to
    put them all into one table. So in the Google trend table, they were mainly trends
    by state but there was also trends for the whole of Germany, so we put the whole
    of Germany ones into a separate data frame so that we can join that:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们基本上是在对所有这些表进行去规范化。我们将把它们全部放入一个表中。因此，在谷歌趋势表中，它们主要是按州划分的趋势，但也有整个德国的趋势，所以我们将整个德国的趋势放入一个单独的数据框中，以便我们可以加入它：
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So we are going to have Google trend for this state and Google trend for the
    whole of Germany.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将有这个州的谷歌趋势和整个德国的谷歌趋势。
- en: Now we can go ahead and start joining both for the training set and for the
    test set [[1:28:19](https://youtu.be/XJ_waZlJU8g?t=5299)]. Then both check that
    we don’t have null’s.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续为训练集和测试集同时加入。然后检查两者都没有空值。
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: My merge function, if there are two columns that are the same, I set their suffix
    on the left to be nothing at all, so it doesn’t screw around with the name, and
    the right hand side to be `_y` .
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我的合并函数，如果有两列是相同的，我将左侧的后缀设置为空，这样它就不会影响名称，右侧设置为`_y`。
- en: '![](../Images/55f61038b07c9b9d548572a98aaffb2f.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55f61038b07c9b9d548572a98aaffb2f.png)'
- en: 'In this case, I didn’t want any of the duplicate ones, so I just went through
    and deleted them:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我不想要任何重复的内容，所以我只是浏览并删除了它们：
- en: '[PRE17]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The main competitor for this store has been open since some date [[1:28:54](https://youtu.be/XJ_waZlJU8g?t=5334)].
    So we can just use Pandas `to_datetime`, I’m passing in the year, the month, and
    the day. So that’s going to give us an error unless they all have years and months,
    so we are going to fill in the missing ones with the 1900 and 1 (see above). And
    what we really want to know is how long this store has been open for at the time
    fo this particular record, so we can just do a date subtract:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这家商店的主要竞争对手自某个日期以来一直开业。因此，我们可以使用Pandas的`to_datetime`，我传入年、月和日。所以这将给我们一个错误，除非它们都有年和月，所以我们将缺失的部分填充为1900年和1月（见上文）。而我们真正想知道的是这家商店在这个特定记录时已经开业多久了，所以我们可以进行日期相减：
- en: '[PRE18]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now if you think about it, sometimes the competition opened later than this
    particular row, so sometimes it’s going to be negative. And it doesn’t probably
    make sense to have negatives (i.e. it’s going to open in x days time). Now having
    said that, I would never put in something like this without first of all running
    a model with it in and without it in. Because our assumption about the data very
    often turned out not to be true. In this case, I didn’t invent any of these pre-processing
    steps. I wrote all the code but it’s all based on the third place winner’s GitHub
    repo. So knowing what it takes to get third place in the Kaggle competition, I’m
    pretty sure they would have checked every one of these pre-processing steps and
    made sure it actually improved their validation set score.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果你考虑一下，有时竞争对手的开业时间晚于这一行，所以有时会是负数。而且可能没有意义有负数（即将在x天后开业）。现在话虽如此，我绝不会在没有先运行包含它和不包含它的模型的情况下放入这样的东西。因为我们对数据的假设往往是不正确的。在这种情况下，我没有发明任何这些预处理步骤。我写了所有的代码，但它都是基于第三名获奖者的GitHub存储库。因此，知道在Kaggle竞赛中获得第三名需要做什么，我相当肯定他们会检查每一个这些预处理步骤，并确保它实际上提高了他们的验证集分数。
- en: '[PRE19]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[[1:30:44](https://youtu.be/XJ_waZlJU8g?t=5444)]'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1:30:44](https://youtu.be/XJ_waZlJU8g?t=5444)]'
- en: So what we are going to be doing is creating a neural network where some of
    the inputs to it are continuous and some of them are categorical. So what that
    means in the neural net that we have, we are basically going to have this kind
    of initial weight matrix. And we are going to have this input feature vector.
    Some of the inputs are just going to be plain continuous numbers (e.g. the maximum
    temperature, the number of kilometer to the nearest store) and some of them are
    going to be one hot encoded, effectively. But we are not actually going to store
    it as one hot encoded. We are actually going to store it as the index.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将创建一个神经网络，其中一些输入是连续的，而另一些是分类的。这意味着在我们的神经网络中，我们基本上会有这种初始权重矩阵。我们将有这个输入特征向量。一些输入将只是普通的连续数字（例如最高温度，到最近商店的公里数），而另一些将被有效地独热编码。但我们实际上不会将其存储为独热编码。我们实际上会将其存储为索引。
- en: '![](../Images/2cf0cf6d50c4c5d0dc3a0ff175ad4a11.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cf0cf6d50c4c5d0dc3a0ff175ad4a11.png)'
- en: So the neural net model is going to need to know which of these columns should
    you basically create an embedding for (i.e. which ones should you treat as if
    they were one hot encoded) and which ones should you just feed directly into the
    linear layer. We are going to tell the model when we get there which is which,
    but we actually need to think ahead of time about which ones do we want to treat
    as categorical and which ones are continuous. In particular, things that we are
    going to treat it as categorical, we don’t want to create more categories than
    we need. So let me show you what I mean.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络模型需要知道这些列中的哪些应该基本上创建一个嵌入（即哪些应该被视为独热编码），哪些应该直接输入到线性层中。当我们到达那里时，我们将告诉模型哪个是哪个，但实际上我们需要提前考虑哪些我们想要视为分类变量，哪些是连续变量。特别是，我们要将其视为分类的东西，我们不希望创建比我们需要的更多的类别。让我告诉你我的意思。
- en: The third place getter in this competition decided that the number of months
    that the competition was open was something they were going to use as a categorical
    variable. So in order to avoid having more categories than they need, they truncated
    it at 24 months. They said that anything more than 24 months old, truncate to
    24\. So here are the unique values of competition months open and it’s all the
    numbers from naught to 24\. So what that means is that there’s going to be an
    embedding matrix that’s going to have basically an embedding vector for things
    that aren’t open yet (0), for things that are open for a month (1), and so forth.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这次比赛的第三名决定将比赛开放的月数作为一个他们要用作分类变量的东西。为了避免创建比需要的更多的类别，他们将其截断到24个月。他们说，超过24个月的任何东西，截断到24个。因此，这里是比赛开放的唯一值，从零到24。这意味着将会有一个嵌入矩阵，基本上会有一个嵌入向量，用于尚未开放的事物（0），用于一个月开放的事物（1），依此类推。
- en: '[PRE20]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, they absolutely could have done that as a continuous variable [[1:33:14](https://youtu.be/XJ_waZlJU8g?t=5594)].
    They could have just had a number here which is just a single number of how many
    months has it been open and they could have treated it as continuous and fed it
    straight into the initial weight matrix. What I found though, and obviously what
    these competitors found is where possible, it’s best to treat things as categorical
    variables. The reason for that is that when you feed some things through an embedding
    matrix, it means every level can be treated like totally differently. So for example,
    in this case, whether something has been open for zero months or one months is
    really different. So if you fed that in as a continuous variable, it would be
    difficult for the neural net to try and find a functional form that has that big
    difference. It’s possible because neural net can do anything. But if you are not
    making it easy for it. Where else, if you used an embedding, treated it as categorical,
    then it will have a totally different vector for zero versus one. So it seems
    like, particularly as long as you’ve got enough data, treating columns as categorical
    variable where possible is a better idea. When I say where possible, that basically
    means where the cardinality is not too high. So if this was like the sales ID
    number that was uniquely different on every row, you can’t treat that as a categorical
    variable. Because it would be a huge embedding matrix and everything only appears
    once, or ditto for kilometers away from the nearest store to two decimal places,
    you wouldn’t make that a categorical variable.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，他们绝对可以将其作为一个连续变量来处理[[1:33:14](https://youtu.be/XJ_waZlJU8g?t=5594)]。他们本可以只是在这里放一个数字，表示开放了多少个月，然后将其视为连续变量，直接输入到初始权重矩阵中。但我发现，显然这些竞争对手也发现了，尽可能地将事物视为分类变量是最好的。这样做的原因是，当你通过一个嵌入矩阵传递一些内容时，意味着每个级别可以被完全不同地处理。例如，在这种情况下，某物是否开放了零个月或一个月是非常不同的。因此，如果你将其作为连续变量输入，神经网络将很难找到具有这种巨大差异的功能形式。这是可能的，因为神经网络可以做任何事情。但如果你不让它变得容易。另一方面，如果你使用嵌入，将其视为分类变量，那么零和一将有完全不同的向量。因此，尤其是在你有足够的数据时，尽可能地将列视为分类变量是一个更好的主意。当我说尽可能时，基本上意味着基数不要太高。因此，如果这是每一行上唯一不同的销售ID号码，你不能将其视为分类变量。因为那将是一个巨大的嵌入矩阵，而且每样东西只出现一次，或者是距离最近商店的公里数到小数点后两位，你也不会将其作为分类变量。
- en: 'So that’s kind of the rule of thumb that they both used in this competition.
    In fact, if we scroll down to their choices, here is how they did it:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这是他们在这次比赛中都使用的经验法则。事实上，如果我们滚动到他们的选择，这是他们的做法：
- en: '![](../Images/289e43357831f212a9ca60ea99103803.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/289e43357831f212a9ca60ea99103803.png)'
- en: Their continuous variable were things that were genuinely continuous, like number
    of kilometers away to the competitor, the temperature stuff, specific number in
    Google trend, etc. Where else, everything else, basically, they treated as categorical.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的连续变量是真正连续的东西，比如到竞争对手的公里数，温度等。而其他一切，基本上，他们都视为分类变量。
- en: That’s it for today. So next time, we’ll finish this off. We’ll see how to turn
    this into a neural network and kind of wrap things up. See you then!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 今天就到这里。下次，我们将结束这个话题。我们将看看如何将这个转化为神经网络，并总结一下。到时见！
