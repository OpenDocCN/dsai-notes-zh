- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: æœªåˆ†ç±»'
- en: 'date: 2024-09-06 19:44:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:44:58ï¼ˆæ—¥æœŸï¼š2024å¹´09æœˆ06æ—¥ 19:44:58ï¼‰'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2207.14452] Deep Learning-based Occluded Person Re-identification: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2207.14452] Deep Learning-based Occluded Person Re-identification: A Surveyï¼ˆæ˜“æ›¼ï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡äººç‰©å†è¯†åˆ«ï¼šä¸€é¡¹è°ƒæŸ¥ç ”ç©¶ï¼‰'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2207.14452](https://ar5iv.labs.arxiv.org/html/2207.14452)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2207.14452](https://ar5iv.labs.arxiv.org/html/2207.14452)
- en: 'Deep Learning-based Occluded Person Re-identification: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Deep Learning-based Occluded Person Re-identification: A Surveyï¼ˆåŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡äººç‰©å†è¯†åˆ«ï¼šä¸€é¡¹è°ƒæŸ¥ç ”ç©¶ï¼‰'
- en: YunjieÂ Peng,Â SaihuiÂ Hou,Â ChunshuiÂ Cao,Â XuÂ Liu,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: äº‘æ´å½­ï¼Œèµ«æŸäººï¼Œæ›¹æ˜¥æ°´ï¼Œåˆ˜æ—­ï¼Œ
- en: 'YongzhenÂ Huang,Â ZhiqiangÂ He1 1Corresponding author.Yunjie Peng is with the
    School of Computer Science and Technology, Beihang University, Beijing 100191,
    China (email: yunjiepeng@buaa.edu.cn).Saihui Hou and Yongzhen Huang are with the
    School of Artificial Intelligence, Beijing Normal University, Beijing 100875,
    China and also with Watrix Technology Limited Co. Ltd, Beijing, China (email:
    housaihui@bnu.edu.cn; huangyongzhen@bnu.edu.cn).Chunshui Cao and Xu Liu are with
    the Watrix Technology Limited Co. Ltd, Beijing, China (email: chunshui.cao@watrix.ai;
    xu.liu@watrix.ai).Zhiqiang He is with the School of Computer Science and Technology,
    Beihang University, Beijing 100191, China and the Lenovo Corporation, Beijing,
    China (email: zqhe1963@gmail.com).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: äº‘æ´å½­ï¼Œå¼ å¼ºä½•1 1é€šè®¯ä½œè€…ã€‚äº‘æ´å½­å°±èŒäºåŒ—èˆªè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢ï¼Œä¸­å›½åŒ—äº¬å¸‚ï¼Œé‚®ç¼–100191ï¼ˆç”µå­é‚®ä»¶ï¼šyunjiepeng@buaa.edu.cnï¼‰ã€‚èµ«æŸäººå’Œé»„å‹‡è‡»å°±èŒäºåŒ—äº¬å¸ˆèŒƒå¤§å­¦äººå·¥æ™ºèƒ½å­¦é™¢ï¼Œä¸­å›½åŒ—äº¬å¸‚ï¼Œå¹¶ä¸”è¿˜åœ¨åŒ—äº¬æ™ºæ…§ç§‘æŠ€æœ‰é™å…¬å¸ä»»èŒï¼ˆç”µå­é‚®ä»¶ï¼šhousaihui@bnu.edu.cnï¼›huangyongzhen@bnu.edu.cnï¼‰ã€‚æ›¹æ˜¥æ°´å’Œåˆ˜æ—­å°±èŒäºåŒ—äº¬æ™ºæ…§ç§‘æŠ€æœ‰é™å…¬å¸ï¼Œä¸­å›½åŒ—äº¬å¸‚ï¼ˆç”µå­é‚®ä»¶ï¼šchunshui.cao@watrix.aiï¼›xu.liu@watrix.aiï¼‰ã€‚å¼ å¼ºä½•å°±èŒäºåŒ—èˆªè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢ï¼Œä¸­å›½åŒ—äº¬å¸‚ï¼Œä»¥åŠè”æƒ³å…¬å¸ï¼Œä¸­å›½åŒ—äº¬å¸‚ï¼ˆç”µå­é‚®ä»¶ï¼šzqhe1963@gmail.comï¼‰ã€‚
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Abstractï¼ˆæ‘˜è¦ï¼‰
- en: 'Occluded person re-identification (Re-ID) aims at addressing the occlusion
    problem when retrieving the person of interest across multiple cameras. With the
    promotion of deep learning technology and the increasing demand for intelligent
    video surveillance, the frequent occlusion in real-world applications has made
    occluded person Re-ID draw considerable interest from researchers. A large number
    of occluded person Re-ID methods have been proposed while there are few surveys
    that focus on occlusion. To fill this gap and help boost future research, this
    paper provides a systematic survey of occluded person Re-ID. Through an in-depth
    analysis of the occlusion in person Re-ID, most existing methods are found to
    only consider part of the problems brought by occlusion. Therefore, we review
    occlusion-related person Re-ID methods from the perspective of issues and solutions.
    We summarize four issues caused by occlusion in person Re-ID, i.e., position misalignment,
    scale misalignment, noisy information, and missing information. The occlusion-related
    methods addressing different issues are then categorized and introduced accordingly.
    After that, we summarize and compare the performance of recent occluded person
    Re-ID methods on four popular datasets: Partial-ReID, Partial-iLIDS, Occluded-ReID,
    and Occluded-DukeMTMC. Finally, we provide insights on promising future research
    directions.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é®æŒ¡äººç‰©å†è¯†åˆ«ï¼ˆRe-IDï¼‰æ—¨åœ¨è§£å†³åœ¨å¤šä¸ªæ‘„åƒå¤´ä¸­æ£€ç´¢æ„Ÿå…´è¶£äººç‰©æ—¶çš„é®æŒ¡é—®é¢˜ã€‚éšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„æ¨åŠ¨å’Œæ™ºèƒ½è§†é¢‘ç›‘æ§éœ€æ±‚çš„å¢åŠ ï¼Œç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„é¢‘ç¹é®æŒ¡ä½¿å¾—é®æŒ¡äººç‰©Re-IDå¼•èµ·äº†ç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ã€‚å·²ç»æå‡ºäº†å¤§é‡çš„é®æŒ¡äººç‰©Re-IDæ–¹æ³•ï¼Œä½†å…³äºé®æŒ¡çš„è°ƒæŸ¥æŠ¥å‘Šå¾ˆå°‘ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½å¹¶å¸®åŠ©æ¨åŠ¨æœªæ¥çš„ç ”ç©¶ï¼Œæœ¬æ–‡å¯¹é®æŒ¡äººç‰©Re-IDè¿›è¡Œäº†ç³»ç»Ÿçš„è°ƒæŸ¥ã€‚é€šè¿‡æ·±å…¥åˆ†æäººç‰©Re-IDä¸­çš„é®æŒ¡ï¼Œå‘ç°ç°æœ‰æ–¹æ³•åªè€ƒè™‘éƒ¨åˆ†ç”±é®æŒ¡å¸¦æ¥çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»é—®é¢˜å’Œè§£å†³æ–¹æ³•çš„è§’åº¦å¯¹ç›¸å…³çš„é®æŒ¡äººç‰©Re-IDæ–¹æ³•è¿›è¡Œäº†å›é¡¾ã€‚æˆ‘ä»¬æ€»ç»“äº†äººç‰©Re-IDä¸­ç”±é®æŒ¡å¼•èµ·çš„å››ä¸ªé—®é¢˜ï¼Œå³ä½ç½®ä¸å¯¹é½ã€å°ºåº¦ä¸å¯¹é½ã€å™ªå£°ä¿¡æ¯å’Œä¸¢å¤±ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹è§£å†³ä¸åŒé—®é¢˜çš„é®æŒ¡ç›¸å…³æ–¹æ³•è¿›è¡Œäº†åˆ†ç±»å’Œä»‹ç»ã€‚ä¹‹åï¼Œæˆ‘ä»¬æ€»ç»“å¹¶æ¯”è¾ƒäº†è¿‘æœŸé®æŒ¡äººç‰©Re-IDæ–¹æ³•åœ¨å››ä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼šPartial-ReIDï¼ŒPartial-iLIDSï¼ŒOccluded-ReIDå’ŒOccluded-DukeMTMCã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æœªæ¥æœ‰å¸Œæœ›çš„ç ”ç©¶æ–¹å‘æä¾›äº†ä¸€äº›è§è§£ã€‚
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Index Terms:ï¼ˆç´¢å¼•æœ¯è¯­ï¼‰
- en: Occluded Person Re-identification, Partial Person Re-identification, Literature
    Survey, Deep Learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Occluded Person Re-identification, Partial Person Re-identification, Literature
    Survey, Deep Learning.
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I Introductionï¼ˆå¯¼è¨€ï¼‰
- en: Person re-identification (Re-ID) retrieves persons of the same identity across
    different camerasÂ [[1](#bib.bib1)]. With the expanding deployment of surveillance
    cameras and the increasing demand for public safety, person Re-ID which plays
    the fundamental role in intelligent surveillance has become a research hotspot
    in the computer vision community. In practice, the internal variations of a person
    (e.g., pose variationsÂ [[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)] and clothes
    changesÂ [[5](#bib.bib5), [6](#bib.bib6)]), as well as the complex environments
    (e.g., illumination changesÂ [[7](#bib.bib7), [8](#bib.bib8)], viewpoint variationsÂ [[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)], and occlusionÂ [[12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14)]), bring significant challenges to person Re-ID. Among them,
    the occlusion which occurs frequently in real-world applications and affects the
    accuracy greatly has received considerable interest from researchers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: äººåƒé‡è¯†åˆ«ï¼ˆRe-IDï¼‰åœ¨ä¸åŒæ‘„åƒå¤´ä¸­æ£€ç´¢ç›¸åŒèº«ä»½çš„äººå‘˜Â [[1](#bib.bib1)]ã€‚éšç€ç›‘æ§æ‘„åƒå¤´çš„ä¸æ–­å¢åŠ å’Œå¯¹å…¬å…±å®‰å…¨éœ€æ±‚çš„å¢åŠ ï¼Œäººåƒé‡è¯†åˆ«ä½œä¸ºæ™ºèƒ½ç›‘æ§ä¸­çš„åŸºç¡€è§’è‰²ï¼Œå·²æˆä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç ”ç©¶çƒ­ç‚¹ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¸ªäººçš„å†…éƒ¨å˜åŒ–ï¼ˆä¾‹å¦‚ï¼Œå§¿æ€å˜åŒ–Â [[2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)] å’Œæœè£…å˜åŒ–Â [[5](#bib.bib5), [6](#bib.bib6)]ï¼‰ï¼Œä»¥åŠå¤æ‚çš„ç¯å¢ƒï¼ˆä¾‹å¦‚ï¼Œå…‰ç…§å˜åŒ–Â [[7](#bib.bib7),
    [8](#bib.bib8)]ï¼Œè§†è§’å˜åŒ–Â [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)] å’Œé®æŒ¡Â [[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)]ï¼‰ï¼Œç»™äººåƒé‡è¯†åˆ«å¸¦æ¥äº†æ˜¾è‘—æŒ‘æˆ˜ã€‚å…¶ä¸­ï¼Œé®æŒ¡åœ¨å®é™…åº”ç”¨ä¸­é¢‘ç¹å‡ºç°ï¼Œå¹¶ä¸”å¯¹å‡†ç¡®æ€§å½±å“å¾ˆå¤§ï¼Œå·²ç»å¼•èµ·äº†ç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ã€‚
- en: Occluded person re-identificationÂ [[15](#bib.bib15), [16](#bib.bib16), [14](#bib.bib14)]
    is proposed to address the occlusion problem for real-world person re-identification.
    Different from general person Re-ID approaches which assume the retrieval process
    with whole human body available, occluded person Re-ID highlights the scenario
    of pedestrian images occluded by various obstacles (e.g., cars, trees, and crowds)
    and focuses on retrieving persons of the same identity when given an occluded
    query.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¢«é®æŒ¡äººåƒé‡è¯†åˆ«Â [[15](#bib.bib15), [16](#bib.bib16), [14](#bib.bib14)]æ˜¯ä¸ºè§£å†³å®é™…äººåƒé‡è¯†åˆ«ä¸­çš„é®æŒ¡é—®é¢˜è€Œæå‡ºçš„ã€‚ä¸å‡è®¾æ£€ç´¢è¿‡ç¨‹ä¸­æœ‰å®Œæ•´äººä½“å­˜åœ¨çš„ä¸€èˆ¬äººåƒé‡è¯†åˆ«æ–¹æ³•ä¸åŒï¼Œè¢«é®æŒ¡äººåƒé‡è¯†åˆ«çªå‡ºäº†è¢«å„ç§éšœç¢ç‰©ï¼ˆä¾‹å¦‚æ±½è½¦ã€æ ‘æœ¨å’Œäººç¾¤ï¼‰é®æŒ¡çš„è¡Œäººå›¾åƒçš„åœºæ™¯ï¼Œå¹¶ä¸“æ³¨äºåœ¨ç»™å®šè¢«é®æŒ¡æŸ¥è¯¢æ—¶æ£€ç´¢ç›¸åŒèº«ä»½çš„äººå‘˜ã€‚
- en: '![Refer to caption](img/d251e866dce637738c1c7a400478e7cc.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/d251e866dce637738c1c7a400478e7cc.png)'
- en: 'Figure 1: The taxonomy of occluded person Re-ID methods from the perspective
    of issues and solutions. Using the above taxonomy, it is easy to know the inherent
    challenges for occluded person Re-ID and have a general understanding of overall
    technical routes.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šä»é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆçš„è§’åº¦æ¥çœ‹ï¼Œè¢«é®æŒ¡äººåƒé‡è¯†åˆ«æ–¹æ³•çš„åˆ†ç±»ã€‚ä½¿ç”¨ä¸Šè¿°åˆ†ç±»ï¼Œå¯ä»¥è½»æ¾äº†è§£è¢«é®æŒ¡äººåƒé‡è¯†åˆ«çš„å›ºæœ‰æŒ‘æˆ˜ï¼Œå¹¶å¯¹æ•´ä½“æŠ€æœ¯è·¯çº¿æœ‰ä¸€ä¸ªå¤§è‡´äº†è§£ã€‚
- en: With the advancement of deep learning, a large number of occluded person Re-ID
    methods have been proposed while there are few surveys that focus on occlusion.
    To fill this gap, this paper summarizes occlusion-related person Re-ID works and
    provides a systematic survey of occluded person Re-ID. Through an in-depth analysis
    of the occlusion in person re-identification, most existing methods are found
    to only consider part of the problems caused by occlusion. Therefore, we review
    occluded person Re-ID from the perspective of issues and solutions to facilitate
    the understanding of the latest approaches and inspire new ideas in the field.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ·±åº¦å­¦ä¹ çš„è¿›æ­¥ï¼Œå·²ç»æå‡ºäº†å¤§é‡çš„è¢«é®æŒ¡äººåƒé‡è¯†åˆ«æ–¹æ³•ï¼Œä½†é’ˆå¯¹é®æŒ¡é—®é¢˜çš„ç»¼è¿°å´å¾ˆå°‘ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æ€»ç»“äº†ä¸é®æŒ¡ç›¸å…³çš„äººåƒé‡è¯†åˆ«å·¥ä½œï¼Œå¹¶æä¾›äº†å¯¹è¢«é®æŒ¡äººåƒé‡è¯†åˆ«çš„ç³»ç»Ÿæ€§ç»¼è¿°ã€‚é€šè¿‡å¯¹äººåƒé‡è¯†åˆ«ä¸­çš„é®æŒ¡è¿›è¡Œæ·±å…¥åˆ†æï¼Œå‘ç°å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»…è€ƒè™‘äº†é®æŒ¡å¼•å‘çš„ä¸€éƒ¨åˆ†é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆçš„è§’åº¦å›é¡¾è¢«é®æŒ¡äººåƒé‡è¯†åˆ«ï¼Œä»¥ä¿ƒè¿›å¯¹æœ€æ–°æ–¹æ³•çš„ç†è§£ï¼Œå¹¶æ¿€å‘è¯¥é¢†åŸŸçš„æ–°æ€è·¯ã€‚
- en: 'The issues caused by occlusion for person Re-ID are carefully summarized from
    the whole process of person re-identification. Technically speaking, a practical
    person Re-ID system in video surveillance mainly consists of three stagesÂ [[17](#bib.bib17)]:
    pedestrian detection, trajectory tracking, and person retrieval. Although it is
    generally believed that the first two stages are independent computer vision tasks
    and most person Re-ID works focus on person retrieval, the occlusion will affect
    the whole process and bring great challenges to the final re-identification. In
    summary, four significant issues are considered for occluded person Re-ID in this
    paper: the position misalignment, the scale misalignment, the noisy information,
    and the missing information. Each issue is illustrated in Fig.Â [2](#S1.F2 "Figure
    2 â€£ I Introduction â€£ Deep Learning-based Occluded Person Re-identification: A
    Survey") and we briefly introduce each issue as follows.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç”±é®æŒ¡å¼•èµ·çš„è¡Œäººé‡è¯†åˆ«é—®é¢˜åœ¨æ•´ä¸ªè¡Œäººé‡è¯†åˆ«è¿‡ç¨‹ä¸­è¿›è¡Œäº†è¯¦ç»†æ€»ç»“ã€‚ä»æŠ€æœ¯è§’åº¦æ¥çœ‹ï¼Œè§†é¢‘ç›‘æ§ä¸­çš„å®é™…è¡Œäººé‡è¯†åˆ«ç³»ç»Ÿä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µ[[17](#bib.bib17)]ï¼šè¡Œäººæ£€æµ‹ã€è½¨è¿¹è·Ÿè¸ªå’Œè¡Œäººæ£€ç´¢ã€‚è™½ç„¶é€šå¸¸è®¤ä¸ºå‰ä¸¤ä¸ªé˜¶æ®µæ˜¯ç‹¬ç«‹çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå¤§å¤šæ•°è¡Œäººé‡è¯†åˆ«å·¥ä½œé›†ä¸­äºè¡Œäººæ£€ç´¢ï¼Œä½†é®æŒ¡ä¼šå½±å“æ•´ä¸ªè¿‡ç¨‹ï¼Œå¹¶ç»™æœ€ç»ˆé‡è¯†åˆ«å¸¦æ¥å·¨å¤§æŒ‘æˆ˜ã€‚æ€»ä¹‹ï¼Œæœ¬æ–‡è€ƒè™‘äº†å››ä¸ªé‡è¦çš„é®æŒ¡è¡Œäººé‡è¯†åˆ«é—®é¢˜ï¼šä½ç½®é”™ä½ã€å°ºåº¦é”™ä½ã€å™ªå£°ä¿¡æ¯å’Œç¼ºå¤±ä¿¡æ¯ã€‚æ¯ä¸ªé—®é¢˜åœ¨å›¾[2](#S1.F2
    "Figure 2 â€£ I Introduction â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey")ä¸­è¿›è¡Œäº†è¯´æ˜ï¼Œæˆ‘ä»¬ç®€è¦ä»‹ç»æ¯ä¸ªé—®é¢˜å¦‚ä¸‹ã€‚'
- en: 1). *The position misalignment.* Generally, the detected human boxes are resized
    by height to obtain the same size of input data for person retrieval. In the case
    of occlusion, the detected box of the person contains only part of the human body
    while it undergoes the same alignment processing as that of a non-occluded person.
    The contents at the same position of the processed partial image and the processed
    holistic image are likely to be mismatched, resulting in the position misalignment
    issue. 2). *The scale misalignment.* Similar to the position misalignment, the
    scale misalignment also arises from the upstream data processing procedure. The
    occlusion may affect the height of the detected box and thus influence the resizing
    ratio in the data processing, resulting in the scale misalignment between a partial
    and a holistic image. 3). *The noisy information.* In the detected boxes of occluded
    pedestrians, the occlusion is inevitably included in whole or in part and brings
    the noisy information for person Re-ID. 4). *The missing information.* In the
    detected boxes of occluded pedestrians, the identity information of occluded regions
    is missing, resulting in the missing information issue.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 1). *ä½ç½®é”™ä½ã€‚* é€šå¸¸ï¼Œæ£€æµ‹åˆ°çš„äººä½“æ¡†ä¼šæ ¹æ®é«˜åº¦è¿›è¡Œè°ƒæ•´ï¼Œä»¥è·å¾—ç›¸åŒå¤§å°çš„è¾“å…¥æ•°æ®è¿›è¡Œè¡Œäººæ£€ç´¢ã€‚åœ¨é®æŒ¡æƒ…å†µä¸‹ï¼Œæ£€æµ‹åˆ°çš„äººçš„æ¡†ä»…åŒ…å«äººä½“çš„ä¸€éƒ¨åˆ†ï¼Œè€Œå®ƒç»è¿‡çš„å¯¹é½å¤„ç†ä¸éé®æŒ¡è¡Œäººç›¸åŒã€‚å¤„ç†çš„éƒ¨åˆ†å›¾åƒå’Œå®Œæ•´å›¾åƒåœ¨ç›¸åŒä½ç½®çš„å†…å®¹å¯èƒ½ä¼šä¸åŒ¹é…ï¼Œå¯¼è‡´ä½ç½®é”™ä½é—®é¢˜ã€‚2).
    *å°ºåº¦é”™ä½ã€‚* ç±»ä¼¼äºä½ç½®é”™ä½ï¼Œå°ºåº¦é”™ä½ä¹Ÿæ¥æºäºä¸Šæ¸¸æ•°æ®å¤„ç†è¿‡ç¨‹ã€‚é®æŒ¡å¯èƒ½å½±å“æ£€æµ‹æ¡†çš„é«˜åº¦ï¼Œä»è€Œå½±å“æ•°æ®å¤„ç†ä¸­çš„è°ƒæ•´æ¯”ä¾‹ï¼Œå¯¼è‡´éƒ¨åˆ†å›¾åƒå’Œæ•´ä½“å›¾åƒä¹‹é—´çš„å°ºåº¦é”™ä½ã€‚3).
    *å™ªå£°ä¿¡æ¯ã€‚* åœ¨é®æŒ¡è¡Œäººçš„æ£€æµ‹æ¡†ä¸­ï¼Œé®æŒ¡ä¸å¯é¿å…åœ°å…¨éƒ¨æˆ–éƒ¨åˆ†åŒ…å«åœ¨å†…ï¼Œå¸¦æ¥å™ªå£°ä¿¡æ¯ï¼Œå½±å“è¡Œäººé‡è¯†åˆ«ã€‚4). *ç¼ºå¤±ä¿¡æ¯ã€‚* åœ¨é®æŒ¡è¡Œäººçš„æ£€æµ‹æ¡†ä¸­ï¼Œé®æŒ¡åŒºåŸŸçš„èº«ä»½ä¿¡æ¯ä¸¢å¤±ï¼Œå¯¼è‡´ç¼ºå¤±ä¿¡æ¯é—®é¢˜ã€‚
- en: 'This paper analyzes occlusion-related person Re-ID methods regarding the above-mentioned
    four issues and provides a multi-dimensional taxonomy to categorize solutions
    for each issue (see Fig.Â [1](#S1.F1 "Figure 1 â€£ I Introduction â€£ Deep Learning-based
    Occluded Person Re-identification: A Survey")). Specifically, we mainly review
    published publications of deep learning-based occluded person Re-ID from top conferences
    and journals before June, 2022, and meanwhile we also introduce some methods from
    other conferences and journals as supplements. We discuss the issues brought by
    occlusion for person Re-ID and provide an in-depth analysis of how the issues
    are addressed in recent works with evaluation results summarized accordingly.
    Particularly, some person Re-ID methods are closely related to occlusion and we
    also summarize these methods to obtain a more comprehensive survey for occluded
    person Re-ID. The main contributions of this survey lie in three aspects:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 'æœ¬æ–‡åˆ†æäº†ä¸é®æŒ¡ç›¸å…³çš„äººè„¸ Re-ID æ–¹æ³•ï¼Œé’ˆå¯¹ä¸Šè¿°å››ä¸ªé—®é¢˜æä¾›äº†å¤šç»´åº¦çš„åˆ†ç±»æ–¹æ³•ï¼Œå¯¹æ¯ä¸ªé—®é¢˜çš„è§£å†³æ–¹æ¡ˆè¿›è¡Œåˆ†ç±»ï¼ˆå‚è§å›¾ [1](#S1.F1 "Figure
    1 â€£ I Introduction â€£ Deep Learning-based Occluded Person Re-identification: A
    Survey")ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸»è¦å›é¡¾äº†åœ¨ 2022 å¹´ 6 æœˆä¹‹å‰çš„é¡¶çº§ä¼šè®®å’ŒæœŸåˆŠä¸Šå‘è¡¨çš„åŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡äººè„¸ Re-ID ç›¸å…³çš„å‡ºç‰ˆç‰©ï¼ŒåŒæ—¶ä¹Ÿä»‹ç»äº†ä¸€äº›æ¥è‡ªå…¶ä»–ä¼šè®®å’ŒæœŸåˆŠçš„æ–¹æ³•ä½œä¸ºè¡¥å……ã€‚æˆ‘ä»¬è®¨è®ºäº†é®æŒ¡å¯¹äººè„¸
    Re-ID å¸¦æ¥çš„é—®é¢˜ï¼Œå¹¶å¯¹å¦‚ä½•åœ¨è¿‘æœŸå·¥ä½œä¸­è§£å†³è¿™äº›é—®é¢˜è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶æ€»ç»“äº†è¯„ä¼°ç»“æœã€‚ç‰¹åˆ«åœ°ï¼Œä¸€äº›äººè„¸ Re-ID æ–¹æ³•ä¸é®æŒ¡å¯†åˆ‡ç›¸å…³ï¼Œæˆ‘ä»¬ä¹Ÿæ€»ç»“äº†è¿™äº›æ–¹æ³•ï¼Œä»¥è·å¾—å¯¹é®æŒ¡äººè„¸
    Re-ID æ›´å…¨é¢çš„è°ƒæŸ¥ã€‚æ­¤è°ƒæŸ¥çš„ä¸»è¦è´¡çŒ®æœ‰ä¸‰æ–¹é¢ï¼š'
- en: â€¢
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: To fill the gap of the occluded person Re-ID survey, we review recent person
    re-identification methods for occlusion and provide a systematic survey from the
    perspective of issues and solutions.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¡«è¡¥é®æŒ¡äººè„¸ Re-ID è°ƒæŸ¥çš„ç©ºç™½ï¼Œæˆ‘ä»¬å›é¡¾äº†è¿‘æœŸçš„é®æŒ¡äººè„¸é‡æ–°è¯†åˆ«æ–¹æ³•ï¼Œå¹¶ä»é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆçš„è§’åº¦æä¾›äº†ç³»ç»Ÿçš„è°ƒæŸ¥ã€‚
- en: â€¢
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We summarize and compare the performance of mainstream occluded person Re-ID
    approaches for researchers and industries to use based on their practical needs.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ€»ç»“å¹¶æ¯”è¾ƒäº†ä¸»æµé®æŒ¡äººè„¸ Re-ID æ–¹æ³•çš„æ€§èƒ½ï¼Œä»¥ä¾›ç ”ç©¶äººå‘˜å’Œå·¥ä¸šç•Œæ ¹æ®å®é™…éœ€æ±‚ä½¿ç”¨ã€‚
- en: â€¢
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: We summarize and analyze the advantages and disadvantages of different types
    of solutions for occluded person Re-ID and provide insights on promising research
    directions in this field.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ€»ç»“å’Œåˆ†æäº†ä¸åŒç±»å‹çš„é®æŒ¡äººè„¸é‡æ–°è¯†åˆ«ï¼ˆRe-IDï¼‰è§£å†³æ–¹æ¡ˆçš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶æä¾›äº†è¯¥é¢†åŸŸæœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘çš„è§è§£ã€‚
- en: The rest of this paper is organized as follows. Section 2 presents a summary
    of previous surveys and elaborates on efforts made by this survey compared with
    others. Section 3 summarizes the common datasets and evaluation metrics of occluded
    person Re-ID. Section 4 provides an in-depth analysis of deep learning-based occluded
    person Re-ID methods from the perspective of issues and solutions. Section 5 compares
    the performance of various solutions and provides insights on promising research
    directions. Section 6 gives our conclusions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ï¼šç¬¬ 2 èŠ‚æ€»ç»“äº†å…ˆå‰çš„è°ƒæŸ¥ï¼Œå¹¶è¯¦ç»†é˜è¿°äº†æœ¬è°ƒæŸ¥ä¸å…¶ä»–è°ƒæŸ¥çš„æ¯”è¾ƒã€‚ç¬¬ 3 èŠ‚æ€»ç»“äº†é®æŒ¡äººè„¸ Re-ID çš„å¸¸ç”¨æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ã€‚ç¬¬
    4 èŠ‚ä»é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆçš„è§’åº¦å¯¹åŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡äººè„¸ Re-ID æ–¹æ³•è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ç¬¬ 5 èŠ‚æ¯”è¾ƒäº†å„ç§è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ï¼Œå¹¶æä¾›äº†å¯¹æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘çš„è§è§£ã€‚ç¬¬
    6 èŠ‚ç»™å‡ºäº†æˆ‘ä»¬çš„ç»“è®ºã€‚
- en: '![Refer to caption](img/469d50fa3a831e3e17d421a9740a0318.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/469d50fa3a831e3e17d421a9740a0318.png)'
- en: 'Figure 2: The position misalignment, scale misalignment, noisy information,
    and missing information issues caused by occlusion for person Re-ID.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šé®æŒ¡å¯¹äººè„¸ Re-ID é€ æˆçš„ä½ç½®é”™ä½ã€å°ºåº¦é”™ä½ã€å™ªå£°ä¿¡æ¯å’Œç¼ºå¤±ä¿¡æ¯é—®é¢˜ã€‚
- en: II Previous Surveys
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II å…ˆå‰çš„è°ƒæŸ¥
- en: 'In the previous literature, there are some surveys that have also reviewed
    the field of person Re-ID. To obtain a more comprehensive comparison, we summarize
    the surveys of person Re-ID since 2012. The taxonomies of these surveys are listed
    in TableÂ [I](#S2.T1 "TABLE I â€£ II Previous Surveys â€£ Deep Learning-based Occluded
    Person Re-identification: A Survey"). On the whole, previous surveys of person
    Re-ID can be roughly divided into traditional surveysÂ [[18](#bib.bib18), [19](#bib.bib19)]
    and deep learning-based surveysÂ [[17](#bib.bib17), [20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26),
    [27](#bib.bib27), [1](#bib.bib1), [28](#bib.bib28)].'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨ä»¥å¾€çš„æ–‡çŒ®ä¸­ï¼Œä¹Ÿæœ‰ä¸€äº›è°ƒæŸ¥å›é¡¾äº†äººå‘˜é‡è¯†åˆ«ï¼ˆRe-IDï¼‰é¢†åŸŸã€‚ä¸ºäº†è·å¾—æ›´å…¨é¢çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬æ€»ç»“äº†è‡ª2012å¹´ä»¥æ¥çš„äººå‘˜é‡è¯†åˆ«è°ƒæŸ¥ã€‚è¿™äº›è°ƒæŸ¥çš„åˆ†ç±»åˆ—åœ¨è¡¨æ ¼Â [I](#S2.T1
    "TABLE I â€£ II Previous Surveys â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey")ä¸­ã€‚æ€»ä½“æ¥è¯´ï¼Œä»¥å¾€çš„äººå‘˜é‡è¯†åˆ«è°ƒæŸ¥å¯ä»¥å¤§è‡´åˆ†ä¸ºä¼ ç»Ÿè°ƒæŸ¥[[18](#bib.bib18), [19](#bib.bib19)]å’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„è°ƒæŸ¥[[17](#bib.bib17),
    [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27), [1](#bib.bib1), [28](#bib.bib28)]ã€‚'
- en: 'Traditional surveys mainly review person Re-ID methods that manually design
    the feature extraction procedure and learn a better similarity measurement. Mazzon
    et al., [[18](#bib.bib18)] summarize four main phases for person Re-ID: multi-person
    detection, feature extraction, cross-camera calibration, and person association.
    Assuming that the first phase (i.e., multi-person detection) has been solved,
    methods of extracting color/texture/shape appearance features, grouping temporal
    information, conducting color/spatio-temporal cross-camera calibration, and distance/learning/optimization-based
    person association are reviewed accordingly. Apurva et al., [[19](#bib.bib19)]
    regard the tracking across multiple cameras as the open set matching problem and
    the identity retrieval as the close set matching problem. According to whether
    additional camera geometry/calibration information is available, methods are divided
    into contextual Re-ID and non-contextual Re-ID for open-set and close-set matching
    respectively.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿè°ƒæŸ¥ä¸»è¦å›é¡¾äº†æ‰‹åŠ¨è®¾è®¡ç‰¹å¾æå–è¿‡ç¨‹å¹¶å­¦ä¹ æ›´å¥½çš„ç›¸ä¼¼åº¦æµ‹é‡çš„äººå‘˜é‡è¯†åˆ«æ–¹æ³•ã€‚Mazzon ç­‰äºº[[18](#bib.bib18)]æ€»ç»“äº†äººå‘˜é‡è¯†åˆ«çš„å››ä¸ªä¸»è¦é˜¶æ®µï¼šå¤šç›®æ ‡æ£€æµ‹ã€ç‰¹å¾æå–ã€è·¨æ‘„åƒå¤´æ ‡å®šå’Œäººå‘˜å…³è”ã€‚å‡è®¾ç¬¬ä¸€ä¸ªé˜¶æ®µï¼ˆå³å¤šç›®æ ‡æ£€æµ‹ï¼‰å·²è¢«è§£å†³ï¼Œé‚£ä¹ˆé’ˆå¯¹æå–é¢œè‰²/çº¹ç†/å½¢çŠ¶å¤–è§‚ç‰¹å¾ã€åˆ†ç»„æ—¶é—´ä¿¡æ¯ã€è¿›è¡Œé¢œè‰²/æ—¶ç©ºè·¨æ‘„åƒå¤´æ ‡å®šä»¥åŠåŸºäºè·ç¦»/å­¦ä¹ /ä¼˜åŒ–çš„äººå‘˜å…³è”çš„æ–¹æ³•è¿›è¡Œäº†ç›¸åº”çš„å›é¡¾ã€‚Apurva
    ç­‰äºº[[19](#bib.bib19)]å°†è·¨æ‘„åƒå¤´è·Ÿè¸ªè§†ä¸ºå¼€æ”¾é›†åŒ¹é…é—®é¢˜ï¼Œå°†èº«ä»½æ£€ç´¢è§†ä¸ºé—­åˆé›†åŒ¹é…é—®é¢˜ã€‚æ ¹æ®æ˜¯å¦æœ‰é¢å¤–çš„æ‘„åƒå¤´å‡ ä½•/æ ‡å®šä¿¡æ¯ï¼Œæ–¹æ³•è¢«åˆ†ä¸ºä¸Šä¸‹æ–‡ç›¸å…³çš„é‡è¯†åˆ«å’Œéä¸Šä¸‹æ–‡ç›¸å…³çš„é‡è¯†åˆ«ï¼Œåˆ†åˆ«ç”¨äºå¼€æ”¾é›†å’Œé—­åˆé›†åŒ¹é…ã€‚
- en: Deep learning-based surveys mainly summarize the person Re-ID methods using
    deep learning techniques from different perspectives. A few of these surveysÂ [[17](#bib.bib17),
    [24](#bib.bib24)] also review traditional methods for the sake of completeness.
    In general, previous deep learning-based surveys have involved loss designÂ [[20](#bib.bib20),
    [25](#bib.bib25)], technical meansÂ [[25](#bib.bib25), [28](#bib.bib28)], data
    augmentationÂ [[20](#bib.bib20), [21](#bib.bib21)], image and videoÂ [[17](#bib.bib17),
    [1](#bib.bib1)], classification and verificationÂ [[20](#bib.bib20), [21](#bib.bib21),
    [23](#bib.bib23)], open-world and close-worldÂ [[22](#bib.bib22), [1](#bib.bib1)],
    multi-modalityÂ [[22](#bib.bib22), [27](#bib.bib27)], ranking optimizationÂ [[24](#bib.bib24),
    [1](#bib.bib1)], noisy annotationÂ [[1](#bib.bib1)], unsupervised learningÂ [[26](#bib.bib26)],
    and metric learningÂ [[21](#bib.bib21), [24](#bib.bib24), [25](#bib.bib25), [28](#bib.bib28),
    [1](#bib.bib1)]. Despite such a number of surveys on person Re-ID, the occlusion
    problem has not drawn enough attention and only a few surveys pay attention to
    occluded person Re-ID. As far as we know, Ye et al., Â [[1](#bib.bib1)] have made
    a rough summary of occluded person Re-ID as a part of Noise-Robust Re-ID. Ming
    et al., Â [[28](#bib.bib28)] have included several occluded person Re-ID methods
    in local feature learning and sequence feature learning. Considering the practical
    importance of occlusion for person Re-ID, a systematic investigation for occluded
    person Re-ID is essential. Therefore, we provide an in-depth survey of issues
    and solutions involved in occlusion-related person Re-ID works to help boost future
    research.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ·±åº¦å­¦ä¹ çš„è°ƒæŸ¥ä¸»è¦æ€»ç»“äº†ä»ä¸åŒè§’åº¦ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„è¡Œäººå†è¯†åˆ«æ–¹æ³•ã€‚å…¶ä¸­ä¸€äº›è°ƒæŸ¥Â [[17](#bib.bib17), [24](#bib.bib24)]Â ä¹Ÿä¸ºäº†å®Œæ•´æ€§å›é¡¾äº†ä¼ ç»Ÿæ–¹æ³•ã€‚æ€»ä½“è€Œè¨€ï¼Œä»¥å‰çš„æ·±åº¦å­¦ä¹ è°ƒæŸ¥æ¶‰åŠäº†æŸå¤±è®¾è®¡Â [[20](#bib.bib20),
    [25](#bib.bib25)], æŠ€æœ¯æ‰‹æ®µÂ [[25](#bib.bib25), [28](#bib.bib28)], æ•°æ®å¢å¼ºÂ [[20](#bib.bib20),
    [21](#bib.bib21)], å›¾åƒå’Œè§†é¢‘Â [[17](#bib.bib17), [1](#bib.bib1)], åˆ†ç±»å’ŒéªŒè¯Â [[20](#bib.bib20),
    [21](#bib.bib21), [23](#bib.bib23)], å¼€æ”¾ä¸–ç•Œå’Œå°é—­ä¸–ç•ŒÂ [[22](#bib.bib22), [1](#bib.bib1)],
    å¤šæ¨¡æ€Â [[22](#bib.bib22), [27](#bib.bib27)], æ’åä¼˜åŒ–Â [[24](#bib.bib24), [1](#bib.bib1)],
    å™ªå£°æ³¨é‡ŠÂ [[1](#bib.bib1)], æ— ç›‘ç£å­¦ä¹ Â [[26](#bib.bib26)], å’Œåº¦é‡å­¦ä¹ Â [[21](#bib.bib21), [24](#bib.bib24),
    [25](#bib.bib25), [28](#bib.bib28), [1](#bib.bib1)]ã€‚å°½ç®¡æœ‰è¿™ä¹ˆå¤šå…³äºè¡Œäººå†è¯†åˆ«çš„è°ƒæŸ¥ï¼Œä½†é®æŒ¡é—®é¢˜å°šæœªå¼•èµ·è¶³å¤Ÿçš„å…³æ³¨ï¼Œåªæœ‰å°‘æ•°è°ƒæŸ¥å…³æ³¨åˆ°é®æŒ¡è¡Œäººå†è¯†åˆ«ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒYe
    ç­‰äººÂ [[1](#bib.bib1)] å¯¹é®æŒ¡è¡Œäººå†è¯†åˆ«åšäº†ç²—ç•¥çš„æ€»ç»“ï¼Œä½œä¸ºå™ªå£°é²æ£’å†è¯†åˆ«çš„ä¸€éƒ¨åˆ†ã€‚Ming ç­‰äººÂ [[28](#bib.bib28)] å°†å‡ ä¸ªé®æŒ¡è¡Œäººå†è¯†åˆ«æ–¹æ³•çº³å…¥äº†å±€éƒ¨ç‰¹å¾å­¦ä¹ å’Œåºåˆ—ç‰¹å¾å­¦ä¹ ã€‚è€ƒè™‘åˆ°é®æŒ¡å¯¹è¡Œäººå†è¯†åˆ«çš„å®é™…é‡è¦æ€§ï¼Œå¯¹é®æŒ¡è¡Œäººå†è¯†åˆ«çš„ç³»ç»Ÿæ€§ç ”ç©¶è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹é®æŒ¡ç›¸å…³çš„è¡Œäººå†è¯†åˆ«å·¥ä½œçš„æ·±å…¥è°ƒæŸ¥ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚
- en: 'TABLE I: The Overview of Person Re-ID Surveys in Recent Years.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ I: è¿‘å¹´æ¥è¡Œäººå†è¯†åˆ«è°ƒæŸ¥æ¦‚è¿°ã€‚'
- en: '| Surveys | Reference | Taxonomy |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| è°ƒæŸ¥ | å‚è€ƒæ–‡çŒ® | åˆ†ç±» |'
- en: '| --- | --- | --- |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Traditional | 2012 PRLÂ [[18](#bib.bib18)] |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| ä¼ ç»Ÿ | 2012 PRLÂ [[18](#bib.bib18)] |'
- en: '&#124; Feature Extraction / Cross-camera Calibration / &#124;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ç‰¹å¾æå– / è·¨ç›¸æœºæ ¡å‡† / &#124;'
- en: '&#124; Person Association &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; è¡Œäººå…³è” &#124;'
- en: '|'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2014 IVCÂ [[19](#bib.bib19)] |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 2014 IVCÂ [[19](#bib.bib19)] |'
- en: '&#124; Contextual Methods (camera geometry info; &#124;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ä¸Šä¸‹æ–‡æ–¹æ³•ï¼ˆç›¸æœºå‡ ä½•ä¿¡æ¯ï¼› &#124;'
- en: '&#124; camera calibration.) / Non-contextual Methods &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ç›¸æœºæ ¡å‡†ã€‚ï¼‰ / éä¸Šä¸‹æ–‡æ–¹æ³• &#124;'
- en: '&#124; (passive methods; active methods) &#124;'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (è¢«åŠ¨æ–¹æ³•ï¼›ä¸»åŠ¨æ–¹æ³•) &#124;'
- en: '|'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Deep learning-based | 2016 arXivÂ [[17](#bib.bib17)] |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| åŸºäºæ·±åº¦å­¦ä¹ çš„ | 2016 arXivÂ [[17](#bib.bib17)] |'
- en: '&#124; Image-based Methods / Video-based Methods &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; åŸºäºå›¾åƒçš„æ–¹æ³• / åŸºäºè§†é¢‘çš„æ–¹æ³• &#124;'
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 TCSVTÂ [[22](#bib.bib22)] |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 2019 TCSVTÂ [[22](#bib.bib22)] |'
- en: '&#124; Person Verification / Application-driven Methods &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; è¡ŒäººéªŒè¯ / åº”ç”¨é©±åŠ¨æ–¹æ³• &#124;'
- en: '&#124; (raw data; practial procedure; efficiency) &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (åŸå§‹æ•°æ®ï¼›å®é™…ç¨‹åºï¼›æ•ˆç‡) &#124;'
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 TPAMIÂ [[24](#bib.bib24)] |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 2019 TPAMIÂ [[24](#bib.bib24)] |'
- en: '&#124; Feature Extraction / Metric Learning / Multi-shot &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ç‰¹å¾æå– / åº¦é‡å­¦ä¹  / å¤šé•œå¤´ &#124;'
- en: '&#124; Ranking &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; æ’å &#124;'
- en: '|'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2020 arXivÂ [[23](#bib.bib23)] |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 2020 arXivÂ [[23](#bib.bib23)] |'
- en: '&#124; Identification Task / Verification Task &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; è¯†åˆ«ä»»åŠ¡ / éªŒè¯ä»»åŠ¡ &#124;'
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2020 IVCÂ [[25](#bib.bib25)] |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 2020 IVCÂ [[25](#bib.bib25)] |'
- en: '&#124; Feature Learning / Model Architecture Design / &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ç‰¹å¾å­¦ä¹  / æ¨¡å‹æ¶æ„è®¾è®¡ / &#124;'
- en: '&#124; Metric Learning / Loss Function &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; åº¦é‡å­¦ä¹  / æŸå¤±å‡½æ•° &#124;'
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2021 arXivÂ [[26](#bib.bib26)] |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 2021 arXivÂ [[26](#bib.bib26)] |'
- en: '&#124; Pseudo-label Estimation / Deep Feature &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ä¼ªæ ‡ç­¾ä¼°è®¡ / æ·±åº¦ç‰¹å¾ &#124;'
- en: '&#124; Representation Learning / Camera-aware &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; è¡¨ç¤ºå­¦ä¹  / ç›¸æœºæ„ŸçŸ¥ &#124;'
- en: '&#124; Invariance Learning / Unsupervised Domain &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ä¸å˜æ€§å­¦ä¹  / æ— ç›‘ç£é¢†åŸŸ &#124;'
- en: '&#124; Adaptation &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; é€‚åº” &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2021 IJCAIÂ [[29](#bib.bib29)] |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 2021 IJCAI [[29](#bib.bib29)] |'
- en: '&#124; Low Resolution / Infrared / Sketch / Text &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ä½åˆ†è¾¨ç‡ / çº¢å¤– / è‰å›¾ / æ–‡æœ¬ &#124;'
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2021 IJCAIÂ [[27](#bib.bib27)] |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 2021 IJCAI [[27](#bib.bib27)] |'
- en: '&#124; Deep Feature Representation Learning / Deep &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; æ·±åº¦ç‰¹å¾è¡¨ç¤ºå­¦ä¹  / æ·±åº¦ &#124;'
- en: '&#124; Metric Learning / Identity-driven detection &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; åº¦é‡å­¦ä¹  / åŸºäºèº«ä»½çš„æ£€æµ‹ &#124;'
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2021 TPAMIÂ [[1](#bib.bib1)] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 2021 TPAMI [[1](#bib.bib1)] |'
- en: '&#124; Closed-world Setting (deep feature representation &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; å°é—­ä¸–ç•Œè®¾ç½®ï¼ˆæ·±åº¦ç‰¹å¾è¡¨ç¤º &#124;'
- en: '&#124; learning; deep metric learning; ranking optimization) &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; å­¦ä¹ ï¼›æ·±åº¦åº¦é‡å­¦ä¹ ï¼›æ’åºä¼˜åŒ–) &#124;'
- en: '&#124; / Open-world Setting (heterogeneous data; raw &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; / å¼€æ”¾ä¸–ç•Œè®¾ç½®ï¼ˆå¼‚è´¨æ•°æ®ï¼›åŸå§‹ &#124;'
- en: '&#124; images or videos; unavailable or limited labels; &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; å›¾åƒæˆ–è§†é¢‘ï¼›ä¸å¯ç”¨æˆ–æœ‰é™æ ‡ç­¾ï¼› &#124;'
- en: '&#124; open-set; noisy annotation) &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; å¼€é›†ï¼›å™ªå£°æ ‡æ³¨) &#124;'
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2022 IVCÂ [[28](#bib.bib28)] |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 2022 IVC [[28](#bib.bib28)] |'
- en: '&#124; Deep Metric Learning / Local Feature Learning &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; æ·±åº¦åº¦é‡å­¦ä¹  / å±€éƒ¨ç‰¹å¾å­¦ä¹  &#124;'
- en: '&#124; / Generative Adversarial Networks / Sequence &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; / ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ / åºåˆ— &#124;'
- en: '&#124; Feature Learning / Graph Convolutional Networks &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ç‰¹å¾å­¦ä¹  / å›¾å·ç§¯ç½‘ç»œ &#124;'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: III Datasets and Evaluations
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III æ•°æ®é›†ä¸è¯„ä¼°
- en: III-A Datasets
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A æ•°æ®é›†
- en: We review eight widely-used datasets for occluded person Re-ID, including 3
    image-based partialÂ¹Â¹1The partial person Re-ID assumes that the visible region
    of occluded person image is manually cropped for identification. Re-ID datasets,
    4 image-based occludedÂ²Â²2The occluded person Re-ID does not require the manually
    cropping process of occluded images.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å›é¡¾äº†å…«ä¸ªå¹¿æ³›ä½¿ç”¨çš„é®æŒ¡äººé‡è¯†åˆ«æ•°æ®é›†ï¼ŒåŒ…æ‹¬3ä¸ªåŸºäºå›¾åƒçš„éƒ¨åˆ†Â¹Â¹1 éƒ¨åˆ†äººé‡è¯†åˆ«å‡è®¾é®æŒ¡äººå›¾åƒçš„å¯è§åŒºåŸŸç»è¿‡æ‰‹åŠ¨è£å‰ªä»¥è¿›è¡Œè¯†åˆ«ã€‚é‡è¯†åˆ«æ•°æ®é›†ï¼Œ4ä¸ªåŸºäºå›¾åƒçš„é®æŒ¡Â²Â²2
    é®æŒ¡äººé‡è¯†åˆ«ä¸éœ€è¦æ‰‹åŠ¨è£å‰ªé®æŒ¡å›¾åƒçš„è¿‡ç¨‹ã€‚
- en: 'Unless otherwise specified, occluded person Re-ID in this survey includes both
    partial person Re-ID and occluded person Re-ID. Re-ID datasets, and 1 video-based
    occluded person Re-ID dataset. The statistics of these datasets are summarized
    in TableÂ [II](#S3.T2 "TABLE II â€£ III-A Datasets â€£ III Datasets and Evaluations
    â€£ Deep Learning-based Occluded Person Re-identification: A Survey") and the details
    of each dataset are reviewed as follows. Examples of partial/occluded person Re-ID
    datasets are shown in Fig.Â [3](#S3.F3 "Figure 3 â€£ III-A Datasets â€£ III Datasets
    and Evaluations â€£ Deep Learning-based Occluded Person Re-identification: A Survey").'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éå¦æœ‰è¯´æ˜ï¼Œæœ¬è°ƒæŸ¥ä¸­çš„é®æŒ¡äººé‡è¯†åˆ«åŒ…æ‹¬éƒ¨åˆ†äººé‡è¯†åˆ«å’Œé®æŒ¡äººé‡è¯†åˆ«ã€‚é‡è¯†åˆ«æ•°æ®é›†åŒ…æ‹¬1ä¸ªåŸºäºè§†é¢‘çš„é®æŒ¡äººé‡è¯†åˆ«æ•°æ®é›†ã€‚è¿™äº›æ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯æ€»ç»“åœ¨è¡¨[II](#S3.T2
    "TABLE II â€£ III-A æ•°æ®é›† â€£ III æ•°æ®é›†ä¸è¯„ä¼° â€£ åŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡äººé‡è¯†åˆ«ï¼šç»¼è¿°")ä¸­ï¼Œæ¯ä¸ªæ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯å¦‚ä¸‹å®¡æŸ¥ã€‚éƒ¨åˆ†/é®æŒ¡äººé‡è¯†åˆ«æ•°æ®é›†çš„ç¤ºä¾‹å¦‚å›¾[3](#S3.F3
    "Figure 3 â€£ III-A æ•°æ®é›† â€£ III æ•°æ®é›†ä¸è¯„ä¼° â€£ åŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡äººé‡è¯†åˆ«ï¼šç»¼è¿°")æ‰€ç¤ºã€‚
- en: Partial-REIDÂ [[15](#bib.bib15)] is an image-based partial Re-ID dataset with
    a variety of viewpoints, backgrounds, and occlusion types. It contains 600 images
    of 60 people, with 5 full-body images and 5 partial images per person. The partial
    observation is generated by manually cropping the occluded region in occluded
    images.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Partial-REID [[15](#bib.bib15)] æ˜¯ä¸€ä¸ªåŸºäºå›¾åƒçš„éƒ¨åˆ†é‡è¯†åˆ«æ•°æ®é›†ï¼Œå…·æœ‰å¤šç§è§†è§’ã€èƒŒæ™¯å’Œé®æŒ¡ç±»å‹ã€‚å®ƒåŒ…å«äº†60ä¸ªäººçš„600å¼ å›¾åƒï¼Œæ¯äººæœ‰5å¼ å…¨èº«å›¾åƒå’Œ5å¼ éƒ¨åˆ†å›¾åƒã€‚éƒ¨åˆ†è§‚æµ‹æ˜¯é€šè¿‡æ‰‹åŠ¨è£å‰ªé®æŒ¡å›¾åƒä¸­çš„é®æŒ¡åŒºåŸŸç”Ÿæˆçš„ã€‚
- en: Partial-iLIDSÂ [[30](#bib.bib30)] is an image-based simulated partial Re-ID dataset
    derived from iLIDSÂ [[31](#bib.bib31)]. It is captured by multiple non-overlapping
    cameras in the airport and contains 238 images from 119 people, with 1 full-body
    image and 1 manually cropped non-occluded partial image per person.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Partial-iLIDS [[30](#bib.bib30)] æ˜¯ä¸€ä¸ªåŸºäºå›¾åƒçš„æ¨¡æ‹Ÿéƒ¨åˆ†é‡è¯†åˆ«æ•°æ®é›†ï¼Œæºè‡ª iLIDS [[31](#bib.bib31)]ã€‚å®ƒç”±æœºåœºä¸­çš„å¤šä¸ªéé‡å æ‘„åƒå¤´æ•è·ï¼ŒåŒ…å«119äººçš„238å¼ å›¾åƒï¼Œæ¯äººæœ‰1å¼ å…¨èº«å›¾åƒå’Œ1å¼ æ‰‹åŠ¨è£å‰ªçš„éé®æŒ¡éƒ¨åˆ†å›¾åƒã€‚
- en: p-CUHK03Â [[32](#bib.bib32)] is an image-based partial Re-ID dataset constructed
    from CUHK03Â [[33](#bib.bib33)]. It contains 1360 person identities captured in
    campus environment. In general, 1160 person identities are used as training set,
    100 person identities are used as validation set, and 100 person identities are
    used as test set. It selects 5 images with same view point from the raw dataset
    for each identity and generates 10 partial body probe images out of selected two
    images. The rest 3 images of each identity are used as full body gallery image.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: p-CUHK03Â [[32](#bib.bib32)] æ˜¯ä¸€ä¸ªåŸºäºå›¾åƒçš„éƒ¨åˆ† Re-ID æ•°æ®é›†ï¼Œæ„å»ºè‡ª CUHK03Â [[33](#bib.bib33)]ã€‚å®ƒåŒ…å«1360ä¸ªåœ¨æ ¡å›­ç¯å¢ƒä¸­æ•è·çš„äººç‰©èº«ä»½ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œ1160ä¸ªäººç‰©èº«ä»½ç”¨ä½œè®­ç»ƒé›†ï¼Œ100ä¸ªäººç‰©èº«ä»½ç”¨ä½œéªŒè¯é›†ï¼Œ100ä¸ªäººç‰©èº«ä»½ç”¨ä½œæµ‹è¯•é›†ã€‚å®ƒä»åŸå§‹æ•°æ®é›†ä¸­ä¸ºæ¯ä¸ªèº«ä»½é€‰æ‹©5å¼ ç›¸åŒè§†è§’çš„å›¾åƒï¼Œå¹¶ä»ä¸­ç”Ÿæˆ10å¼ éƒ¨åˆ†èº«ä½“æ¢æµ‹å›¾åƒã€‚å…¶ä½™3å¼ å›¾åƒç”¨äºå…¨èº«å›¾åº“å›¾åƒã€‚
- en: P-ETHZÂ [[16](#bib.bib16)] is an image-based occluded person Re-ID dataset modified
    from ETHZÂ [[34](#bib.bib34)]. It has 3897 images of 85 person identities. Each
    identity has 1 to 30 full-body person images and occluded person images respectively.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: P-ETHZÂ [[16](#bib.bib16)] æ˜¯ä¸€ä¸ªåŸºäºå›¾åƒçš„é®æŒ¡è¡Œäºº Re-ID æ•°æ®é›†ï¼Œä¿®æ”¹è‡ª ETHZÂ [[34](#bib.bib34)]ã€‚å®ƒåŒ…å«3897å¼ 85ä¸ªèº«ä»½çš„å›¾åƒã€‚æ¯ä¸ªèº«ä»½åˆ†åˆ«æ‹¥æœ‰1åˆ°30å¼ å…¨èº«å›¾åƒå’Œé®æŒ¡å›¾åƒã€‚
- en: Occluded-REIDÂ [[16](#bib.bib16)] is an image-based occluded person Re-ID dataset
    captured by mobile cameras with different viewpoints and different types of severe
    occlusion. It consists of 2000 images of 200 people, with 5 full-body images and
    5 occluded images per person.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Occluded-REIDÂ [[16](#bib.bib16)] æ˜¯ä¸€ä¸ªåŸºäºå›¾åƒçš„é®æŒ¡è¡Œäºº Re-ID æ•°æ®é›†ï¼Œç”±ç§»åŠ¨æ‘„åƒæœºæ•è·ï¼Œå…·æœ‰ä¸åŒçš„è§†è§’å’Œä¸¥é‡çš„é®æŒ¡ç±»å‹ã€‚å®ƒåŒ…å«2000å¼ 200äººçš„å›¾åƒï¼Œæ¯äººæœ‰5å¼ å…¨èº«å›¾åƒå’Œ5å¼ é®æŒ¡å›¾åƒã€‚
- en: '![Refer to caption](img/046ca4ecfd337a309bd5427adffdc953.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/046ca4ecfd337a309bd5427adffdc953.png)'
- en: 'Figure 3: Examples of partial/occluded person Re-ID datasets. Partial/occluded
    person images (the first row) and full-body person images (the second row).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šéƒ¨åˆ†/é®æŒ¡è¡Œäºº Re-ID æ•°æ®é›†çš„ç¤ºä¾‹ã€‚éƒ¨åˆ†/é®æŒ¡è¡Œäººå›¾åƒï¼ˆç¬¬ä¸€è¡Œï¼‰å’Œå…¨èº«è¡Œäººå›¾åƒï¼ˆç¬¬äºŒè¡Œï¼‰ã€‚
- en: 'TABLE II: Occluded/Partial Person Re-id Datasets. Occluded-dukemtmc and Occluded-dukemtmc-videoreid
    are Abbreviated as Occ-dukemtmc and Occ-dukemtmc-video Respectively.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ IIï¼šé®æŒ¡/éƒ¨åˆ†è¡Œäºº Re-ID æ•°æ®é›†ã€‚Occluded-dukemtmc å’Œ Occluded-dukemtmc-videoreid åˆ†åˆ«ç¼©å†™ä¸º
    Occ-dukemtmc å’Œ Occ-dukemtmc-videoã€‚
- en: '| Dataset | Training Set (ID/Image) | Test Set (ID/Image) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| æ•°æ®é›† | è®­ç»ƒé›†ï¼ˆID/å›¾åƒï¼‰ | æµ‹è¯•é›†ï¼ˆID/å›¾åƒï¼‰ |'
- en: '| Gallery | Query |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| å›¾åº“ | æŸ¥è¯¢ |'
- en: '| Partial-REID | - | 60/300 | 60/300 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Partial-REID | - | 60/300 | 60/300 |'
- en: '| Partial-iLIDS | - | 119/119 | 119/119 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Partial-iLIDS | - | 119/119 | 119/119 |'
- en: '| p-CUHK03 | 1160/15080 | 100/300 | 100/1000 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| p-CUHK03 | 1160/15080 | 100/300 | 100/1000 |'
- en: '| P-ETHZ | 43/ - | 42/ - | 42/ - |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| P-ETHZ | 43/ - | 42/ - | 42/ - |'
- en: '| Occluded-REID | - | 200/1000 | 200/1000 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Occluded-REID | - | 200/1000 | 200/1000 |'
- en: '| P-DukeMTMC-reID | 650/ - | 649/ - | 649/ - |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| P-DukeMTMC-reID | 650/ - | 649/ - | 649/ - |'
- en: '| Occ-DukeMTMC | 702/15,618 | 1,110/17,661 | 519/2,210 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Occ-DukeMTMC | 702/15,618 | 1,110/17,661 | 519/2,210 |'
- en: '| Occ-DukeMTMC-Video | 702/292,343 | 1,110/281,114 | 661/39,526 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Occ-DukeMTMC-Video | 702/292,343 | 1,110/281,114 | 661/39,526 |'
- en: P-DukeMTMC-reIDÂ [[16](#bib.bib16)] is an image-based occluded person Re-ID dataset
    modified from DukeMTMC-reIDÂ [[35](#bib.bib35)]. It has 24143 images of 1299 person
    identities and contains images with target persons occluded by different types
    of occlusion in public, e.g., people, cars, and guideboards. Each identity has
    both full-body images and occluded images.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: P-DukeMTMC-reIDÂ [[16](#bib.bib16)] æ˜¯ä¸€ä¸ªåŸºäºå›¾åƒçš„é®æŒ¡è¡ŒäººRe-IDæ•°æ®é›†ï¼Œä¿®æ”¹è‡ªDukeMTMC-reIDÂ [[35](#bib.bib35)]ã€‚å®ƒåŒ…å«24143å¼ 1299ä¸ªèº«ä»½çš„å›¾åƒï¼Œå¹¶ä¸”åŒ…å«ç”±å…¬å…±åœºæ‰€ä¸åŒç±»å‹çš„é®æŒ¡ç‰©ï¼ˆå¦‚äººã€è½¦å’ŒæŒ‡ç¤ºç‰Œï¼‰é®æŒ¡çš„ç›®æ ‡äººç‰©å›¾åƒã€‚æ¯ä¸ªèº«ä»½éƒ½æœ‰å…¨èº«å›¾åƒå’Œé®æŒ¡å›¾åƒã€‚
- en: Occluded-DukeMTMCÂ [[36](#bib.bib36)] is an image-based occluded person Re-ID
    dataset built from DukeMTMC-reIDÂ [[35](#bib.bib35)]. It contains 15,618 images
    of 708 people for training while including 2,210 query images of 519 persons and
    17,661 gallery images of 1110 persons for testing. The 9% of the training set,
    the 100% of the query set, and the 10% of the gallery set are occluded images.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Occluded-DukeMTMCÂ [[36](#bib.bib36)] æ˜¯ä¸€ä¸ªåŸºäºå›¾åƒçš„é®æŒ¡è¡ŒäººRe-IDæ•°æ®é›†ï¼Œæ¥æºäºDukeMTMC-reIDÂ [[35](#bib.bib35)]ã€‚å®ƒåŒ…å«15,618å¼ 708äººçš„å›¾åƒç”¨äºè®­ç»ƒï¼ŒåŒæ—¶åŒ…æ‹¬2,210å¼ 519äººçš„æŸ¥è¯¢å›¾åƒå’Œ17,661å¼ 1110äººçš„å›¾åº“å›¾åƒç”¨äºæµ‹è¯•ã€‚è®­ç»ƒé›†ä¸­9%çš„å›¾åƒã€æŸ¥è¯¢é›†ä¸­100%çš„å›¾åƒä»¥åŠå›¾åº“é›†ä¸­10%çš„å›¾åƒæ˜¯é®æŒ¡å›¾åƒã€‚
- en: Occluded-DukeMTMC-VideoReIDÂ [[14](#bib.bib14)] is a video-based occluded Re-ID
    dataset reorganized from the DukeMTMC-VideoReIDÂ [[37](#bib.bib37)]. It includes
    large variety of obstacles, e.g., cars, trees, bicycles, and other persons. It
    contains 1,702 image sequences of 702 identities for training, 661 query image
    sequences of 661 identities and 2,636 gallery image sequences of 1110 identities
    for testing. More than 1/3 frames of each query sequence in the testing set contain
    occlusion.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Occluded-DukeMTMC-VideoReIDÂ [[14](#bib.bib14)] æ˜¯ä¸€ä¸ªåŸºäºè§†é¢‘çš„é®æŒ¡ Re-ID æ•°æ®é›†ï¼Œé‡ç»„è‡ª DukeMTMC-VideoReIDÂ [[37](#bib.bib37)]ã€‚å®ƒåŒ…æ‹¬å„ç§éšœç¢ç‰©ï¼Œä¾‹å¦‚æ±½è½¦ã€æ ‘æœ¨ã€è‡ªè¡Œè½¦å’Œå…¶ä»–äººã€‚æ•°æ®é›†åŒ…å«
    1,702 ä¸ªå›¾åƒåºåˆ—ç”¨äºè®­ç»ƒï¼Œ661 ä¸ªæŸ¥è¯¢å›¾åƒåºåˆ—ç”¨äº 661 ä¸ªèº«ä»½çš„æŸ¥è¯¢ï¼Œä»¥åŠ 2,636 ä¸ªå›¾åƒåºåˆ—ç”¨äº 1110 ä¸ªèº«ä»½çš„æµ‹è¯•ã€‚æµ‹è¯•é›†ä¸­çš„æ¯ä¸ªæŸ¥è¯¢åºåˆ—çš„è¶…è¿‡
    1/3 å¸§åŒ…å«é®æŒ¡ã€‚
- en: III-B Evaluation Metrics
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B è¯„ä¼°æŒ‡æ ‡
- en: The occluded person Re-ID evaluates the performance of a Re-ID system under
    the scenario of occlusion. Therefore, the settings of partial/occluded person
    Re-ID datasets are usually specially designed. In principle, the query images/videos
    for testing are all occluded samples. The evaluation focuses on whether the correct
    identities can be retrieved when only occluded queries are provided. To evaluate
    a Re-ID system, the Cumulative Matching Characteristics (CMC) curves and the mean
    Average Precision (mAP) are two widely used metrics.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: é®æŒ¡äºº Re-ID è¯„ä¼° Re-ID ç³»ç»Ÿåœ¨é®æŒ¡åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œéƒ¨åˆ†/é®æŒ¡äºº Re-ID æ•°æ®é›†çš„è®¾ç½®é€šå¸¸æ˜¯ç‰¹åˆ«è®¾è®¡çš„ã€‚åŸåˆ™ä¸Šï¼Œæµ‹è¯•ç”¨çš„æŸ¥è¯¢å›¾åƒ/è§†é¢‘éƒ½æ˜¯é®æŒ¡æ ·æœ¬ã€‚è¯„ä¼°ä¾§é‡äºå½“ä»…æä¾›é®æŒ¡æŸ¥è¯¢æ—¶æ˜¯å¦èƒ½æ£€ç´¢åˆ°æ­£ç¡®çš„èº«ä»½ã€‚ä¸ºäº†è¯„ä¼°
    Re-ID ç³»ç»Ÿï¼Œç´¯è®¡åŒ¹é…ç‰¹æ€§ï¼ˆCMCï¼‰æ›²çº¿å’Œå‡å€¼å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰æ˜¯ä¸¤ç§å¹¿æ³›ä½¿ç”¨çš„æŒ‡æ ‡ã€‚
- en: 'The CMC curves calculate the probability that a correct match appears in the
    top-$k$ ranked retrieval results, $k\in\left\{1,2,3,...\right\}$. Specifically,
    the top-$k$ accuracy of the query $i$ is calculated as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: CMC æ›²çº¿è®¡ç®—æ­£ç¡®åŒ¹é…å‡ºç°åœ¨å‰-$k$ æ’åæ£€ç´¢ç»“æœä¸­çš„æ¦‚ç‡ï¼Œå…¶ä¸­ $k\in\left\{1,2,3,...\right\}$ã€‚å…·ä½“æ¥è¯´ï¼ŒæŸ¥è¯¢ $i$
    çš„å‰-$k$ å‡†ç¡®ç‡è®¡ç®—ä¸ºï¼š
- en: '|  | <math   alttext="{Acc_{k}^{i}}=\begin{cases}1,&amp;{\text{if the top-$k$
    ranked gallery samples}}\\ &amp;{\text{contain the sample(s) of query $i$}};\\'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="{Acc_{k}^{i}}=\begin{cases}1,&amp;{\text{å¦‚æœå‰-$k$ æ’åçš„å›¾åº“æ ·æœ¬}}\\
    &amp;{\text{åŒ…å«æŸ¥è¯¢ $i$ çš„æ ·æœ¬}};\\'
- en: 0,&amp;{\text{otherwise}}.\end{cases}" display="block"><semantics ><mrow ><mrow  ><mi
    mathsize="90%"  >A</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi mathsize="90%"
    >c</mi><mo lspace="0em" rspace="0em" >â€‹</mo><msubsup ><mi mathsize="90%"  >c</mi><mi
    mathsize="90%"  >k</mi><mi mathsize="90%"  >i</mi></msubsup></mrow><mo mathsize="90%"  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="left"  ><mrow ><mn mathsize="90%"  >1</mn><mo mathsize="90%"  >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mtext mathsize="90%" >if the top-</mtext><mi mathsize="90%"  >k</mi><mtext
    mathsize="90%"  >Â ranked gallery samples</mtext></mrow></mtd></mtr><mtr  ><mtd
    columnalign="left"  ><mrow ><mrow ><mtext mathsize="90%" >contain the sample(s)
    of queryÂ </mtext><mi mathsize="90%"  >i</mi></mrow><mo mathsize="90%"  >;</mo></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mn mathsize="90%"  >0</mn><mo mathsize="90%"  >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mtext mathsize="90%" >otherwise</mtext><mo lspace="0em"
    mathsize="90%"  >.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci >ğ´</ci><ci  >ğ‘</ci><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><ci  >ğ‘˜</ci></apply><ci >ğ‘–</ci></apply></apply><apply ><csymbol cd="latexml"
    >cases</csymbol><cn type="integer" >1</cn><ci  ><mrow ><mtext mathsize="90%" >if
    the top-</mtext><mi mathsize="90%"  >k</mi><mtext mathsize="90%"  >Â ranked gallery
    samples</mtext></mrow></ci><ci  ><mtext mathsize="90%"  >otherwise</mtext></ci><ci
    ><mrow ><mtext mathsize="90%" >contain the sample(s) of queryÂ </mtext><mi mathsize="90%"
    >i</mi></mrow></ci><cn type="integer"  >0</cn><ci ><mtext mathsize="90%" >otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >{Acc_{k}^{i}}=\begin{cases}1,&{\text{if the top-$k$
    ranked gallery samples}}\\ &{\text{contain the sample(s) of query $i$}};\\ 0,&{\text{otherwise}}.\end{cases}</annotation></semantics></math>
    |  | (1) |
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 0,&amp;{\text{å¦åˆ™}}.\end{cases}" display="block"><semantics ><mrow ><mrow  ><mi
    mathsize="90%"  >A</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi mathsize="90%"
    >c</mi><mo lspace="0em" rspace="0em" >â€‹</mo><msubsup ><mi mathsize="90%"  >c</mi><mi
    mathsize="90%"  >k</mi><mi mathsize="90%"  >i</mi></msubsup></mrow><mo mathsize="90%"  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="left"  ><mrow ><mn mathsize="90%"  >1</mn><mo mathsize="90%"  >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mtext mathsize="90%" >å¦‚æœæ’åå‰-</mtext><mi mathsize="90%"  >k</mi><mtext
    mathsize="90%" >çš„ç”»å»Šæ ·æœ¬</mtext></mrow></mtd></mtr><mtr  ><mtd columnalign="left"  ><mrow
    ><mrow ><mtext mathsize="90%" >åŒ…å«æŸ¥è¯¢æ ·æœ¬Â </mtext><mi mathsize="90%"  >i</mi></mrow><mo
    mathsize="90%"  >;</mo></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><mn mathsize="90%"  >0</mn><mo mathsize="90%"  >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mtext mathsize="90%" >å¦åˆ™</mtext><mo lspace="0em" mathsize="90%"  >.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci >ğ´</ci><ci  >ğ‘</ci><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><ci  >ğ‘˜</ci></apply><ci >ğ‘–</ci></apply></apply><apply ><csymbol cd="latexml"
    >cases</csymbol><cn type="integer" >1</cn><ci  ><mrow ><mtext mathsize="90%" >å¦‚æœæ’åå‰-</mtext><mi
    mathsize="90%"  >k</mi><mtext mathsize="90%"  >çš„ç”»å»Šæ ·æœ¬</mtext></mrow></ci><ci  ><mtext
    mathsize="90%"  >å¦åˆ™</mtext></ci><ci ><mrow ><mtext mathsize="90%" >åŒ…å«æŸ¥è¯¢æ ·æœ¬Â </mtext><mi
    mathsize="90%" >i</mi></mrow></ci><cn type="integer"  >0</cn><ci ><mtext mathsize="90%"
    >å¦åˆ™</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >{Acc_{k}^{i}}=\begin{cases}1,&{\text{å¦‚æœæ’åå‰-$k$ çš„ç”»å»Šæ ·æœ¬}}\\ &{\text{åŒ…å«æŸ¥è¯¢æ ·æœ¬ $i$}};\\
    0,&{\text{å¦åˆ™}}.\end{cases}</annotation></semantics></math> |  | (1) |
- en: 'Supposing there are $N$ queries in the test set, the CMC-$k$ (a.k.a., the rank-$k$
    accuracy) that calculates the probability of the top-$k$ accuracy for all queries
    is computed as:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æµ‹è¯•é›†ä¸­æœ‰ $N$ ä¸ªæŸ¥è¯¢ï¼ŒCMC-$k$ï¼ˆå³ rank-$k$ ç²¾åº¦ï¼‰è®¡ç®—æ‰€æœ‰æŸ¥è¯¢çš„å‰-$k$ ç²¾åº¦çš„æ¦‚ç‡ã€‚
- en: '|  | $\text{CMC-}k=\frac{1}{N}\sum_{i=1}^{N}Acc_{k}^{i}$ |  | (2) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{CMC-}k=\frac{1}{N}\sum_{i=1}^{N}Acc_{k}^{i}$ |  | (2) |'
- en: Since only the first match is concerned in the calculation, the CMC curves are
    acceptable when there are only one ground truth for each query or when we care
    more about the ground truth match in the top positions of the rank list.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè®¡ç®—ä¸­ä»…è€ƒè™‘ç¬¬ä¸€ä¸ªåŒ¹é…ï¼Œå½“æ¯ä¸ªæŸ¥è¯¢åªæœ‰ä¸€ä¸ªçœŸå®å€¼æˆ–æˆ‘ä»¬æ›´å…³æ³¨æ’ååˆ—è¡¨é¡¶éƒ¨ä½ç½®çš„çœŸå®åŒ¹é…æ—¶ï¼ŒCMC æ›²çº¿æ˜¯å¯ä»¥æ¥å—çš„ã€‚
- en: 'The mAP measures the average retrieval performance that takes the order of
    all true matches in the ranked retrieval results into consideration. Specifically,
    the average precision (AP) of the query $i$ is calculated as:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: mAP è¡¡é‡äº†è€ƒè™‘æ‰€æœ‰çœŸå®åŒ¹é…åœ¨æ’åæ£€ç´¢ç»“æœä¸­é¡ºåºçš„å¹³å‡æ£€ç´¢æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒæŸ¥è¯¢ $i$ çš„å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰è®¡ç®—å¦‚ä¸‹ï¼š
- en: '|  | $\text{AP}_{i}=\frac{1}{M_{i}}\sum_{j=1}^{M_{i}}\frac{j}{Rank_{j}}$ |  |
    (3) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{AP}_{i}=\frac{1}{M_{i}}\sum_{j=1}^{M_{i}}\frac{j}{Rank_{j}}$ |  |
    (3) |'
- en: 'where $M_{i}$ denotes there are $M_{i}$ samples with identity $i$ in the gallery
    set and $Rank_{j}$ denotes the rank of the $j$-th ground truth in the retrieval
    gallery list for query $i$. Supposing there are $N$ queries in the test set, the
    mAP is computed as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $M_{i}$ è¡¨ç¤ºåœ¨å›¾åº“é›†ä¸­èº«ä»½ä¸º $i$ çš„æ ·æœ¬æ•°ï¼Œ$Rank_{j}$ è¡¨ç¤ºæŸ¥è¯¢ $i$ åœ¨æ£€ç´¢å›¾åº“åˆ—è¡¨ä¸­çš„ç¬¬ $j$ ä¸ªçœŸå®åŒ¹é…çš„æ’åã€‚å‡è®¾æµ‹è¯•é›†ä¸­æœ‰
    $N$ ä¸ªæŸ¥è¯¢ï¼Œåˆ™ mAP çš„è®¡ç®—å…¬å¼ä¸ºï¼š
- en: '|  | $\text{mAP}=\frac{1}{N}\sum_{i=1}^{N}\text{AP}_{i}$ |  | (4) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{mAP}=\frac{1}{N}\sum_{i=1}^{N}\text{AP}_{i}$ |  | (4) |'
- en: Since the order of all true matches in the ranked retrieval results participates
    in the calculation of mAP, the mAP measures the average retrieval performance
    and is suitable for the gallery with multiple true matches.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ’åæ£€ç´¢ç»“æœä¸­çš„æ‰€æœ‰çœŸå®åŒ¹é…çš„é¡ºåºå‚ä¸äº† mAP çš„è®¡ç®—ï¼ŒmAP æµ‹é‡äº†å¹³å‡æ£€ç´¢æ€§èƒ½ï¼Œå¹¶é€‚ç”¨äºå…·æœ‰å¤šä¸ªçœŸå®åŒ¹é…çš„å›¾åº“ã€‚
- en: On the whole, the mAP pays more attention to the ability of retrieval recall
    while the CMC curves focus on the ability of retrieving a true match in candidate
    lists with different sizes. Consequently, the CMC curves and the mAP always work
    together for the evaluation of a Re-ID system.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼ŒmAP æ›´å…³æ³¨äºæ£€ç´¢å¬å›èƒ½åŠ›ï¼Œè€Œ CMC æ›²çº¿åˆ™ä¾§é‡äºåœ¨ä¸åŒå¤§å°çš„å€™é€‰åˆ—è¡¨ä¸­æ£€ç´¢çœŸå®åŒ¹é…çš„èƒ½åŠ›ã€‚å› æ­¤ï¼ŒCMC æ›²çº¿å’Œ mAP é€šå¸¸ä¼šä¸€èµ·ç”¨äº
    Re-ID ç³»ç»Ÿçš„è¯„ä¼°ã€‚
- en: '![Refer to caption](img/acd9cf0d13fa63a3884f4d4344c8ee04.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/acd9cf0d13fa63a3884f4d4344c8ee04.png)'
- en: 'Figure 4: Examples of the four issues caused by occlusion in real-world applications:
    (*a*) position misalignment, (*b*) scale misalignment, (*c*) noisy information,
    and (*d*) missing information.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šå®é™…åº”ç”¨ä¸­ç”±äºé®æŒ¡å¼•èµ·çš„å››ä¸ªé—®é¢˜çš„ç¤ºä¾‹ï¼š(*a*) ä½ç½®é”™ä½ï¼Œ(*b*) å°ºå¯¸é”™ä½ï¼Œ(*c*) å™ªå£°ä¿¡æ¯ï¼Œä»¥åŠ (*d*) ä¿¡æ¯ç¼ºå¤±ã€‚
- en: IV Occluded Person Re-ID
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV é®æŒ¡äººå‘˜ Re-ID
- en: 'Occluded person re-identification (Re-ID) aims at addressing the occlusion
    problem when retrieving the person of interest across multiple cameras. In real-world
    applications of person Re-ID, a person may be occluded by a variety of obstacles
    (e.g., cars, trees, streetlights, and other persons) and the surveillance system
    often fails to capture the holistic person. A practical person Re-ID system in
    video surveillance generally includes three stages: pedestrian detection, trajectory
    tracking, and person retrieval. The occlusion will affect the whole process and
    bring great challenges to the final Re-ID.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: é®æŒ¡äººå‘˜å†è¯†åˆ«ï¼ˆRe-IDï¼‰æ—¨åœ¨è§£å†³åœ¨å¤šä¸ªæ‘„åƒå¤´ä¸­æ£€ç´¢ç›®æ ‡äººç‰©æ—¶çš„é®æŒ¡é—®é¢˜ã€‚åœ¨äººå‘˜ Re-ID çš„å®é™…åº”ç”¨ä¸­ï¼Œäººç‰©å¯èƒ½ä¼šè¢«å„ç§éšœç¢ç‰©ï¼ˆå¦‚æ±½è½¦ã€æ ‘æœ¨ã€è·¯ç¯å’Œå…¶ä»–äººï¼‰é®æŒ¡ï¼Œç›‘æ§ç³»ç»Ÿå¾€å¾€æ— æ³•æ•æ‰åˆ°å®Œæ•´çš„äººç‰©ã€‚ä¸€ä¸ªå®é™…çš„è§†é¢‘ç›‘æ§äººå‘˜
    Re-ID ç³»ç»Ÿé€šå¸¸åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šè¡Œäººæ£€æµ‹ã€è½¨è¿¹è·Ÿè¸ªå’Œäººç‰©æ£€ç´¢ã€‚é®æŒ¡ä¼šå½±å“æ•´ä¸ªè¿‡ç¨‹ï¼Œå¹¶ç»™æœ€ç»ˆçš„ Re-ID å¸¦æ¥å·¨å¤§çš„æŒ‘æˆ˜ã€‚
- en: 'In general, there are four issues to be considered when developing a solution
    for occluded person Re-ID. The first two issues are the position and the scale
    misalignments between partial and holistic images. This is mainly caused by the
    upstream data processing: the detected box of a partial person undergoes the same
    alignment processing as that of a holistic person to obtain a consistent input
    size, resulting in the position misalignment issue and the scale misalignment
    issue. The last two issues are the noisy information and the missing information
    caused by occlusion. In the detected boxes of occluded pedestrians, occlusion
    is inevitably included in whole or in part and the identity information of occluded
    regions is missing, resulting in the noisy information issue and the missing information
    issue. Each issue is shown in Fig.Â [2](#S1.F2 "Figure 2 â€£ I Introduction â€£ Deep
    Learning-based Occluded Person Re-identification: A Survey") and real-world examples
    of the four issues are presented in Fig.Â [4](#S3.F4 "Figure 4 â€£ III-B Evaluation
    Metrics â€£ III Datasets and Evaluations â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey").'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 'é€šå¸¸ï¼Œåœ¨å¼€å‘é®æŒ¡äººå‘˜ Re-ID è§£å†³æ–¹æ¡ˆæ—¶ï¼Œéœ€è¦è€ƒè™‘å››ä¸ªé—®é¢˜ã€‚å‰ä¸¤ä¸ªé—®é¢˜æ˜¯éƒ¨åˆ†å›¾åƒå’Œæ•´ä½“å›¾åƒä¹‹é—´çš„ä½ç½®å’Œå°ºå¯¸é”™ä½ã€‚è¿™ä¸»è¦æ˜¯ç”±äºä¸Šæ¸¸æ•°æ®å¤„ç†é€ æˆçš„ï¼šéƒ¨åˆ†äººçš„æ£€æµ‹æ¡†ç»è¿‡ä¸æ•´ä½“äººç›¸åŒçš„å¯¹é½å¤„ç†ä»¥è·å¾—ä¸€è‡´çš„è¾“å…¥å°ºå¯¸ï¼Œå¯¼è‡´äº†ä½ç½®é”™ä½é—®é¢˜å’Œå°ºå¯¸é”™ä½é—®é¢˜ã€‚æœ€åä¸¤ä¸ªé—®é¢˜æ˜¯ç”±é®æŒ¡å¼•èµ·çš„å™ªå£°ä¿¡æ¯å’Œä¿¡æ¯ç¼ºå¤±ã€‚åœ¨é®æŒ¡è¡Œäººçš„æ£€æµ‹æ¡†ä¸­ï¼Œé®æŒ¡ä¸å¯é¿å…åœ°éƒ¨åˆ†æˆ–å…¨éƒ¨åŒ…å«åœ¨å†…ï¼Œé®æŒ¡åŒºåŸŸçš„èº«ä»½ä¿¡æ¯ç¼ºå¤±ï¼Œä»è€Œå¯¼è‡´å™ªå£°ä¿¡æ¯é—®é¢˜å’Œä¿¡æ¯ç¼ºå¤±é—®é¢˜ã€‚æ¯ä¸ªé—®é¢˜åœ¨å›¾Â [2](#S1.F2
    "Figure 2 â€£ I Introduction â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey") ä¸­æ˜¾ç¤ºï¼Œå››ä¸ªé—®é¢˜çš„å®é™…ç¤ºä¾‹å‘ˆç°åœ¨å›¾Â [4](#S3.F4 "Figure 4 â€£ III-B Evaluation Metrics â€£
    III Datasets and Evaluations â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey") ä¸­ã€‚'
- en: 'In this section, we analyze occlusion-related person Re-ID methods from the
    perspective of above-mentioned four issues. Corresponding solutions for each issue
    are summarized following the taxonomy illustrated in Fig.Â [1](#S1.F1 "Figure 1
    â€£ I Introduction â€£ Deep Learning-based Occluded Person Re-identification: A Survey").
    It should be noticed that some methods simultaneously address more than one issue
    and these methods will be introduced multiple times from different perspectives
    accordingly.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»ä¸Šè¿°å››ä¸ªé—®é¢˜çš„è§’åº¦åˆ†æäº†ä¸é®æŒ¡ç›¸å…³çš„è¡Œäººé‡è¯†åˆ«æ–¹æ³•ã€‚é’ˆå¯¹æ¯ä¸ªé—®é¢˜çš„ç›¸åº”è§£å†³æ–¹æ¡ˆæŒ‰ç…§å›¾Â [1](#S1.F1 "å›¾ 1 â€£ I å¼•è¨€ â€£
    åŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡è¡Œäººé‡è¯†åˆ«ï¼šç»¼è¿°")ä¸­æ‰€ç¤ºçš„åˆ†ç±»æ³•è¿›è¡Œäº†æ€»ç»“ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæœ‰äº›æ–¹æ³•åŒæ—¶è§£å†³å¤šä¸ªé—®é¢˜ï¼Œè¿™äº›æ–¹æ³•å°†æ ¹æ®ä¸åŒçš„è§’åº¦å¤šæ¬¡ä»‹ç»ã€‚
- en: IV-A Position Misalignment
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A ä½ç½®é”™ä½
- en: 'Deep learning-based solutions to address the position misalignment issue caused
    by occlusion in person Re-ID can be roughly summarized into four categories: matchingÂ [[15](#bib.bib15),
    [38](#bib.bib38), [30](#bib.bib30), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)], auxiliary model for positionÂ [[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49),
    [42](#bib.bib42), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55)], additional supervision for positionÂ [[56](#bib.bib56),
    [57](#bib.bib57), [38](#bib.bib38), [58](#bib.bib58), [14](#bib.bib14), [59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61)], and attention mechanism for positionÂ [[32](#bib.bib32),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68)]. Firstly, the matching-based solutions take
    person Re-ID as a matching task and propose a variety of matching components,
    as well as the matching strategies, to address the position misalignment issue.
    Secondly, the auxiliary model-based solutions for person Re-ID rely on the position
    information provided by external models to boost performance. Thirdly, the additional
    supervision-based solutions for person Re-ID utilize extra information to guide
    the position-related learning process while being independent during the inference
    stage. Fourthly, the attention mechanism-based solutions for person Re-ID learn
    attention to address the position misalignment issue without any additional information.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆç”¨äºè§£å†³é®æŒ¡å¼•èµ·çš„è¡Œäººé‡è¯†åˆ«ä¸­çš„ä½ç½®é”™ä½é—®é¢˜ï¼Œå¤§è‡´å¯ä»¥æ€»ç»“ä¸ºå››ç±»ï¼šåŒ¹é…Â [[15](#bib.bib15), [38](#bib.bib38),
    [30](#bib.bib30), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)], ä½ç½®è¾…åŠ©æ¨¡å‹Â [[36](#bib.bib36), [45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [42](#bib.bib42),
    [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54),
    [55](#bib.bib55)], ä½ç½®é™„åŠ ç›‘ç£Â [[56](#bib.bib56), [57](#bib.bib57), [38](#bib.bib38),
    [58](#bib.bib58), [14](#bib.bib14), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61)],
    å’Œä½ç½®æ³¨æ„æœºåˆ¶Â [[32](#bib.bib32), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68)]ã€‚é¦–å…ˆï¼ŒåŸºäºåŒ¹é…çš„è§£å†³æ–¹æ¡ˆå°†è¡Œäººé‡è¯†åˆ«è§†ä¸ºåŒ¹é…ä»»åŠ¡ï¼Œå¹¶æå‡ºäº†å¤šç§åŒ¹é…ç»„ä»¶ä»¥åŠåŒ¹é…ç­–ç•¥ï¼Œä»¥è§£å†³ä½ç½®é”™ä½é—®é¢˜ã€‚å…¶æ¬¡ï¼ŒåŸºäºè¾…åŠ©æ¨¡å‹çš„è¡Œäººé‡è¯†åˆ«è§£å†³æ–¹æ¡ˆä¾èµ–å¤–éƒ¨æ¨¡å‹æä¾›çš„ä½ç½®ä¿¡æ¯æ¥æå‡æ€§èƒ½ã€‚ç¬¬ä¸‰ï¼ŒåŸºäºé™„åŠ ç›‘ç£çš„è¡Œäººé‡è¯†åˆ«è§£å†³æ–¹æ¡ˆåˆ©ç”¨é¢å¤–çš„ä¿¡æ¯æ¥å¼•å¯¼ä½ç½®ç›¸å…³çš„å­¦ä¹ è¿‡ç¨‹ï¼ŒåŒæ—¶åœ¨æ¨ç†é˜¶æ®µä¿æŒç‹¬ç«‹ã€‚ç¬¬å››ï¼ŒåŸºäºæ³¨æ„æœºåˆ¶çš„è¡Œäººé‡è¯†åˆ«è§£å†³æ–¹æ¡ˆé€šè¿‡å­¦ä¹ æ³¨æ„æœºåˆ¶è§£å†³ä½ç½®é”™ä½é—®é¢˜ï¼Œæ— éœ€ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚
- en: IV-A1 Matching
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 åŒ¹é…
- en: 'The main points of a matching-based method can probably be summarized as the
    matching component and the matching strategy (see Fig.Â [5](#S4.F5 "Figure 5 â€£
    IV-A1 Matching â€£ IV-A Position Misalignment â€£ IV Occluded Person Re-ID â€£ Deep
    Learning-based Occluded Person Re-identification: A Survey")). Diverse definitions
    of the matching component, as well as the corresponding matching strategies, have
    been proposed for addressing the position misalignment issue. On the whole, matching-based
    methods can be further grouped into sliding window matchingÂ [[15](#bib.bib15)],
    shortest path matchingÂ [[38](#bib.bib38)], reconstruction-based matchingÂ [[15](#bib.bib15),
    [30](#bib.bib30)], denoising matchingÂ [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)],
    graph-based matchingÂ [[42](#bib.bib42), [43](#bib.bib43)], and set-based matchingÂ [[44](#bib.bib44)].'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºåŒ¹é…çš„æ–¹æ³•çš„ä¸»è¦è¦ç‚¹å¯ä»¥æ€»ç»“ä¸ºåŒ¹é…ç»„ä»¶å’ŒåŒ¹é…ç­–ç•¥ï¼ˆè§å›¾[5](#S4.F5 "å›¾ 5 â€£ IV-A1 åŒ¹é… â€£ IV-A ä½ç½®å¯¹é½é—®é¢˜ â€£ IV é®æŒ¡äººç‰©é‡æ–°è¯†åˆ«
    â€£ åŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡äººç‰©é‡æ–°è¯†åˆ«ç»¼è¿°")ï¼‰ã€‚ä¸ºè§£å†³ä½ç½®å¯¹é½é—®é¢˜ï¼Œå·²ç»æå‡ºäº†å¤šç§åŒ¹é…ç»„ä»¶çš„å®šä¹‰ä»¥åŠç›¸åº”çš„åŒ¹é…ç­–ç•¥ã€‚æ€»ä½“è€Œè¨€ï¼ŒåŸºäºåŒ¹é…çš„æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥åˆ†ä¸ºæ»‘åŠ¨çª—å£åŒ¹é…[[15](#bib.bib15)]ã€æœ€çŸ­è·¯å¾„åŒ¹é…[[38](#bib.bib38)]ã€åŸºäºé‡å»ºçš„åŒ¹é…[[15](#bib.bib15),
    [30](#bib.bib30)]ã€å»å™ªåŒ¹é…[[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)]ã€åŸºäºå›¾çš„åŒ¹é…[[42](#bib.bib42),
    [43](#bib.bib43)]å’ŒåŸºäºé›†åˆçš„åŒ¹é…[[44](#bib.bib44)]ã€‚
- en: The sliding window matchingÂ [[15](#bib.bib15)] treats the partial probe image
    as a whole and slides it exhaustively over a full-body gallery image to match
    the most similar local region. The L1-norm distance between the partial image
    and its most similar match on a full-body image is employed for the measurement.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æ»‘åŠ¨çª—å£åŒ¹é…[[15](#bib.bib15)]å°†éƒ¨åˆ†æ¢æµ‹å›¾åƒè§†ä¸ºä¸€ä¸ªæ•´ä½“ï¼Œå¹¶åœ¨å…¨èº«ç”»å»Šå›¾åƒä¸Šè¿›è¡Œç©·ä¸¾æ»‘åŠ¨ï¼Œä»¥åŒ¹é…æœ€ç›¸ä¼¼çš„å±€éƒ¨åŒºåŸŸã€‚éƒ¨åˆ†å›¾åƒä¸å…¨èº«å›¾åƒä¸Šæœ€ç›¸ä¼¼åŒ¹é…çš„L1èŒƒæ•°è·ç¦»ç”¨äºæµ‹é‡ã€‚
- en: The shortest path matchingÂ [[38](#bib.bib38)] performs the matching by calculating
    the shortest path between two sets of local features and uses the matched local
    features to compute the similarity, explicitly accomplishing the position alignment.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€çŸ­è·¯å¾„åŒ¹é…[[38](#bib.bib38)]é€šè¿‡è®¡ç®—ä¸¤ç»„å±€éƒ¨ç‰¹å¾ä¹‹é—´çš„æœ€çŸ­è·¯å¾„æ¥è¿›è¡ŒåŒ¹é…ï¼Œå¹¶ä½¿ç”¨åŒ¹é…çš„å±€éƒ¨ç‰¹å¾æ¥è®¡ç®—ç›¸ä¼¼åº¦ï¼Œæ˜ç¡®å®ç°ä½ç½®å¯¹é½ã€‚
- en: The reconstruction-based matchingÂ [[15](#bib.bib15), [30](#bib.bib30)] assumes
    the identity information in an occluded image is a subset of that in a non-occluded
    image and thus the partial image can be reconstructed in whole or in part from
    a holistic image of the same identity. The patch-level reconstruction-based matchingÂ [[15](#bib.bib15)]
    decomposes the partial and the full-body images into regular grid patches as matching
    components. Each probe patch is matched to a set of gallery patches by optimizing
    the gallery patch selection for self reconstruction. The block-level reconstruction-based
    matchingÂ [[30](#bib.bib30)] defines $c\times c$ pixels on a feature map as an
    independent block for matching. It is assumed that each block of a partial probe
    image can be reconstructed from the sparse linear combination of blocks of a full-body
    gallery image with the same identity.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºé‡å»ºçš„åŒ¹é…[[15](#bib.bib15), [30](#bib.bib30)]å‡è®¾é®æŒ¡å›¾åƒä¸­çš„èº«ä»½ä¿¡æ¯æ˜¯éé®æŒ¡å›¾åƒä¸­èº«ä»½ä¿¡æ¯çš„ä¸€ä¸ªå­é›†ï¼Œå› æ­¤å¯ä»¥ä»ç›¸åŒèº«ä»½çš„æ•´ä½“å›¾åƒä¸­å®Œå…¨æˆ–éƒ¨åˆ†é‡å»ºéƒ¨åˆ†å›¾åƒã€‚åŸºäºè¡¥ä¸çš„é‡å»ºåŒ¹é…[[15](#bib.bib15)]å°†éƒ¨åˆ†å’Œå…¨èº«å›¾åƒåˆ†è§£ä¸ºè§„åˆ™çš„ç½‘æ ¼è¡¥ä¸ä½œä¸ºåŒ¹é…ç»„ä»¶ã€‚é€šè¿‡ä¼˜åŒ–ç”»å»Šè¡¥ä¸é€‰æ‹©ä»¥è¿›è¡Œè‡ªæˆ‘é‡å»ºï¼Œå°†æ¯ä¸ªæ¢æµ‹è¡¥ä¸ä¸ä¸€ç»„ç”»å»Šè¡¥ä¸è¿›è¡ŒåŒ¹é…ã€‚åŸºäºå—çš„é‡å»ºåŒ¹é…[[30](#bib.bib30)]å°†ç‰¹å¾å›¾ä¸Šçš„$c\times
    c$åƒç´ å®šä¹‰ä¸ºä¸€ä¸ªç‹¬ç«‹çš„å—è¿›è¡ŒåŒ¹é…ã€‚å‡è®¾éƒ¨åˆ†æ¢æµ‹å›¾åƒçš„æ¯ä¸ªå—å¯ä»¥ä»ç›¸åŒèº«ä»½çš„å…¨èº«ç”»å»Šå›¾åƒçš„å—çš„ç¨€ç–çº¿æ€§ç»„åˆä¸­é‡å»ºã€‚
- en: '![Refer to caption](img/f0b555976102256e79671db004ef1f94.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/f0b555976102256e79671db004ef1f94.png)'
- en: 'Figure 5: The diagram of matching-based methods: constructing local matching
    elements and designing matching strategies to address the position misalignment,
    scale misalignment, and noise information issues.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šåŸºäºåŒ¹é…çš„æ–¹æ³•çš„ç¤ºæ„å›¾ï¼šæ„å»ºå±€éƒ¨åŒ¹é…å…ƒç´ å¹¶è®¾è®¡åŒ¹é…ç­–ç•¥ï¼Œä»¥è§£å†³ä½ç½®å¯¹é½ã€å°ºåº¦å¯¹é½å’Œå™ªå£°ä¿¡æ¯é—®é¢˜ã€‚
- en: The denoising matchingÂ [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)]
    proposes to focus on foreground visible human parts while discarding the noisy
    information during matching. Specifically, GASMÂ [[39](#bib.bib39)] learns a saliency
    heatmap with the supervision of pose estimation and human segmentation to highlight
    foreground visible human parts. Guided by the saliency heatmap, the matching scores
    of spatial element-wise features in foreground visible human parts are assigned
    with large weights while that in background or occlusion regions are assigned
    with small weights, adaptively. Co-AttentionÂ [[40](#bib.bib40)] performs the matching
    between a partial and a holistic image under the guidance of body parsing masks.
    Particularly, the self-attention mechanismÂ [[69](#bib.bib69)] is adopted in Co-Attention
    matching where the parsing mask of the partial image is viewed as the query, parsing
    masks and CNN features of the partial and the holistic images serve as the key
    and the value respectively. ASANÂ [[41](#bib.bib41)] proposes to replace the segmentation
    mask of holistic gallery images with the mask of the current target person image
    in each retrieval matching process. In this way, the feature extraction guided
    by ASAN is able to suppress the interference from useless parts.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: å»å™ªåŒ¹é…[[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)]æå‡ºåœ¨åŒ¹é…è¿‡ç¨‹ä¸­å…³æ³¨å‰æ™¯å¯è§çš„äººä½“éƒ¨ä½ï¼ŒåŒæ—¶å¿½ç•¥å™ªå£°ä¿¡æ¯ã€‚å…·ä½“è€Œè¨€ï¼ŒGASM[[39](#bib.bib39)]é€šè¿‡å§¿æ€ä¼°è®¡å’Œäººä½“åˆ†å‰²çš„ç›‘ç£æ¥å­¦ä¹ æ˜¾è‘—æ€§çƒ­å›¾ï¼Œä»¥çªå‡ºå‰æ™¯å¯è§çš„äººä½“éƒ¨ä½ã€‚åœ¨æ˜¾è‘—æ€§çƒ­å›¾çš„æŒ‡å¯¼ä¸‹ï¼Œå‰æ™¯å¯è§äººä½“éƒ¨ä½çš„ç©ºé—´å…ƒç´ ç‰¹å¾çš„åŒ¹é…åˆ†æ•°è¢«èµ‹äºˆè¾ƒå¤§æƒé‡ï¼Œè€ŒèƒŒæ™¯æˆ–é®æŒ¡åŒºåŸŸçš„åŒ¹é…åˆ†æ•°åˆ™è¢«èµ‹äºˆè¾ƒå°æƒé‡ï¼Œè¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚Co-Attention[[40](#bib.bib40)]åœ¨èº«ä½“è§£ææ©ç çš„æŒ‡å¯¼ä¸‹æ‰§è¡Œéƒ¨åˆ†å›¾åƒå’Œæ•´ä½“å›¾åƒä¹‹é—´çš„åŒ¹é…ã€‚ç‰¹åˆ«åœ°ï¼ŒCo-AttentionåŒ¹é…ä¸­é‡‡ç”¨äº†è‡ªæ³¨æ„æœºåˆ¶[[69](#bib.bib69)]ï¼Œå…¶ä¸­éƒ¨åˆ†å›¾åƒçš„è§£ææ©ç è¢«è§†ä¸ºæŸ¥è¯¢ï¼Œéƒ¨åˆ†å’Œæ•´ä½“å›¾åƒçš„è§£ææ©ç ä»¥åŠCNNç‰¹å¾åˆ†åˆ«ä½œä¸ºé”®å’Œå€¼ã€‚ASAN[[41](#bib.bib41)]å»ºè®®åœ¨æ¯æ¬¡æ£€ç´¢åŒ¹é…è¿‡ç¨‹ä¸­ï¼Œå°†æ•´ä½“åº“å›¾åƒçš„åˆ†å‰²æ©ç æ›¿æ¢ä¸ºå½“å‰ç›®æ ‡äººç‰©å›¾åƒçš„æ©ç ã€‚è¿™æ ·ï¼ŒASANå¼•å¯¼çš„ç‰¹å¾æå–èƒ½å¤ŸæŠ‘åˆ¶æ¥è‡ªæ— ç”¨éƒ¨åˆ†çš„å¹²æ‰°ã€‚
- en: The graph-based matchingÂ [[42](#bib.bib42), [43](#bib.bib43)] formulates the
    occluded person re-identification as the graph matching problem. HOReIDÂ [[42](#bib.bib42)]
    employs the key-point heatmaps to extract the semantic local features of an image
    as nodes of a graph. The graph convolutional network (GCN) with learnable adjacent
    matrices is designed to pass messages between nodes for capturing the high-order
    relation information. To measure the similarity between two graphs, node features
    of the two graphs are further processed with relevant information extracted from
    each other for learning topology information. Both the high-order relation information
    extracted by GCN and the topology information learned from each other are employed
    to compute the final similarity for the two graphs. The multi-granular hypergraph
    matchingÂ [[43](#bib.bib43)] designs multiple hypergraphs with different spatial
    and temporal granularities to address the misalignment and occlusion issues for
    video-based person re-identification. Different from the conventional graphs,
    the hypergraphsÂ [[70](#bib.bib70)] are able to model the high-order dependency
    involving multiple nodes and are more suitable for modeling the multi-granular
    correlations in a sequence.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå›¾çš„åŒ¹é…[[42](#bib.bib42), [43](#bib.bib43)]å°†é®æŒ¡äººç‰©çš„å†è¯†åˆ«é—®é¢˜è¡¨è¿°ä¸ºå›¾åŒ¹é…é—®é¢˜ã€‚HOReID[[42](#bib.bib42)]ä½¿ç”¨å…³é”®ç‚¹çƒ­å›¾å°†å›¾åƒçš„è¯­ä¹‰å±€éƒ¨ç‰¹å¾æå–ä¸ºå›¾çš„èŠ‚ç‚¹ã€‚å›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰é€šè¿‡å¯å­¦ä¹ çš„é‚»æ¥çŸ©é˜µè®¾è®¡ï¼Œç”¨äºåœ¨èŠ‚ç‚¹ä¹‹é—´ä¼ é€’ä¿¡æ¯ï¼Œä»¥æ•æ‰é«˜é˜¶å…³ç³»ä¿¡æ¯ã€‚ä¸ºäº†æµ‹é‡ä¸¤ä¸ªå›¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä¸¤ä¸ªå›¾çš„èŠ‚ç‚¹ç‰¹å¾è¿›ä¸€æ­¥å¤„ç†ï¼Œé€šè¿‡æå–çš„ç›¸å…³ä¿¡æ¯å­¦ä¹ æ‹“æ‰‘ä¿¡æ¯ã€‚GCNæå–çš„é«˜é˜¶å…³ç³»ä¿¡æ¯å’Œç›¸äº’å­¦ä¹ çš„æ‹“æ‰‘ä¿¡æ¯è¢«ç”¨äºè®¡ç®—ä¸¤ä¸ªå›¾çš„æœ€ç»ˆç›¸ä¼¼æ€§ã€‚å¤šç²’åº¦è¶…å›¾åŒ¹é…[[43](#bib.bib43)]è®¾è®¡äº†å¤šä¸ªå…·æœ‰ä¸åŒç©ºé—´å’Œæ—¶é—´ç²’åº¦çš„è¶…å›¾ï¼Œä»¥è§£å†³è§†é¢‘åŸºç¡€äººç‰©å†è¯†åˆ«ä¸­çš„å¯¹é½å’Œé®æŒ¡é—®é¢˜ã€‚ä¸ä¼ ç»Ÿå›¾ä¸åŒï¼Œè¶…å›¾[[70](#bib.bib70)]èƒ½å¤Ÿå»ºæ¨¡æ¶‰åŠå¤šä¸ªèŠ‚ç‚¹çš„é«˜é˜¶ä¾èµ–å…³ç³»ï¼Œæ›´é€‚åˆäºå»ºæ¨¡åºåˆ—ä¸­çš„å¤šç²’åº¦ç›¸å…³æ€§ã€‚
- en: The set-based matching takes occluded person Re-ID as a set matching task without
    requiring explicit spatial alignment. Specifically, MoSÂ [[44](#bib.bib44)] employs
    a CNN backbone to capture diverse visual patterns along the channel dimension
    as matching elements. And the Jaccard similarity coefficient is introduced as
    the metric to compute the similarity between pattern sets of person images. The
    minimization and maximization are used to approximate the operations of intersection
    and union of sets, and the ratio of intersection over union is calculated to measure
    the similarity between two sets.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºé›†åˆçš„åŒ¹é…å°†é®æŒ¡äººç‰©é‡è¯†åˆ«è§†ä¸ºé›†åˆåŒ¹é…ä»»åŠ¡ï¼Œæ— éœ€æ˜ç¡®çš„ç©ºé—´å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼ŒMoSÂ [[44](#bib.bib44)] ä½¿ç”¨CNNéª¨å¹²ç½‘æ•æ‰æ²¿é€šé“ç»´åº¦çš„å¤šæ ·è§†è§‰æ¨¡å¼ä½œä¸ºåŒ¹é…å…ƒç´ ã€‚å¼•å…¥Jaccardç›¸ä¼¼ç³»æ•°ä½œä¸ºè®¡ç®—äººç‰©å›¾åƒæ¨¡å¼é›†åˆä¹‹é—´ç›¸ä¼¼åº¦çš„åº¦é‡ã€‚é€šè¿‡æœ€å°åŒ–å’Œæœ€å¤§åŒ–æ¥è¿‘ä¼¼é›†åˆçš„äº¤é›†å’Œå¹¶é›†æ“ä½œï¼Œå¹¶è®¡ç®—äº¤é›†ä¸å¹¶é›†çš„æ¯”ç‡æ¥æµ‹é‡ä¸¤ä¸ªé›†åˆä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚
- en: IV-A2 Auxiliary Model for Position
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 ä½ç½®çš„è¾…åŠ©æ¨¡å‹
- en: To address the position misalignment issue caused by occlusion, some methods
    directly use the auxiliary information provided by external models for position
    alignment. According to the type of employed auxiliary models, these methods can
    be roughly summarized into pose-basedÂ [[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [42](#bib.bib42), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52)], parsing-basedÂ [[53](#bib.bib53)], and hybrid-basedÂ [[54](#bib.bib54),
    [55](#bib.bib55)] solutions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³å› é®æŒ¡å¼•èµ·çš„ä½ç½®ä¿¡å·é”™ä½é—®é¢˜ï¼Œä¸€äº›æ–¹æ³•ç›´æ¥åˆ©ç”¨å¤–éƒ¨æ¨¡å‹æä¾›çš„è¾…åŠ©ä¿¡æ¯è¿›è¡Œä½ç½®å¯¹é½ã€‚æ ¹æ®æ‰€ä½¿ç”¨çš„è¾…åŠ©æ¨¡å‹ç±»å‹ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥å¤§è‡´æ€»ç»“ä¸ºåŸºäºå§¿æ€çš„Â [[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49),
    [42](#bib.bib42), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52)], åŸºäºè§£æçš„Â [[53](#bib.bib53)]ï¼Œä»¥åŠåŸºäºæ··åˆçš„Â [[54](#bib.bib54),
    [55](#bib.bib55)] è§£å†³æ–¹æ¡ˆã€‚
- en: The pose-based methodsÂ [[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [42](#bib.bib42), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52)] utilize the position information predicted
    by an external pose estimation model to address the position misalignment issue.
    PGFAÂ [[36](#bib.bib36)], PDVMÂ [[45](#bib.bib45)], and PMFBÂ [[46](#bib.bib46)]
    generate heatmaps consisting of a 2D Gaussian centered on key-point locations
    to extract aligned pose features through a dot product with the CNN feature map.
    KBFMÂ [[47](#bib.bib47)] utilizes shared visible keypoints between images to locate
    aligned rectangular regions for calculating the similarity. PVPMÂ [[48](#bib.bib48)]
    uses the key-point heatmaps and the part affinity fields estimated by OpenPoseÂ [[71](#bib.bib71)]
    to generate part attention maps for extracting aligned local features. Similarly,
    PGMANetÂ [[49](#bib.bib49)] generates heatmaps of key-points locations and employs
    them to calculate part attention masks on the CNN feature map. Based on the part
    features aggregated by part attention masks, PGMANet further computes the correlation
    among different part features to exploit the second-order information to enrich
    the feature extraction. HOReIDÂ [[42](#bib.bib42)] employs the key-point heatmaps
    to extract the semantic local features on a person image. The local features of
    an image are taken as nodes of a graph and HOReID designs a graph convolutional
    network with learnable adjacent matrices to exploit the more discriminative relation
    information among nodes for re-identification. Further, CTLÂ [[50](#bib.bib50)]
    proposes to divide the human body into three granularities and uses key-point
    heatmaps to extract multi-scale part features as graph nodes. The cross-scale
    graph convolution and the 3D graph convolution are designed to capture the structural
    information and the hierarchical spatial-temporal dependencies for addressing
    the spatial misalignment issues in video person Re-ID. ACSAPÂ [[51](#bib.bib51)]
    proposes to use the external pose information to guide the adversarial generation
    of aligned feature maps. PFDÂ [[52](#bib.bib52)] generates local patch features
    and multiplies them with the processed keypoint heatmaps element-wisely to obtain
    the pose-guided features. Throught the measurement of set similarity, PFD performs
    the matching between local features and pose-guided features to disentangle the
    pose information from patch features for position alignment.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå§¿æ€çš„æ–¹æ³•[[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [42](#bib.bib42), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52)]åˆ©ç”¨å¤–éƒ¨å§¿æ€ä¼°è®¡æ¨¡å‹é¢„æµ‹çš„ä½ç½®æ¥è§£å†³ä½ç½®é”™ä½é—®é¢˜ã€‚PGFA [[36](#bib.bib36)], PDVM [[45](#bib.bib45)],
    å’Œ PMFB [[46](#bib.bib46)] ç”ŸæˆåŒ…å«ä»¥å…³é”®ç‚¹ä½ç½®ä¸ºä¸­å¿ƒçš„ 2D é«˜æ–¯çš„çƒ­å›¾ï¼Œé€šè¿‡ä¸ CNN ç‰¹å¾å›¾çš„ç‚¹ç§¯æ¥æå–å¯¹é½çš„å§¿æ€ç‰¹å¾ã€‚KBFM
    [[47](#bib.bib47)] åˆ©ç”¨å›¾åƒé—´å…±äº«çš„å¯è§å…³é”®ç‚¹æ¥å®šä½å¯¹é½çš„çŸ©å½¢åŒºåŸŸä»¥è®¡ç®—ç›¸ä¼¼æ€§ã€‚PVPM [[48](#bib.bib48)] ä½¿ç”¨å…³é”®ç‚¹çƒ­å›¾å’Œç”±
    OpenPose [[71](#bib.bib71)] ä¼°è®¡çš„éƒ¨ä»¶äº²å’Œåœºæ¥ç”Ÿæˆéƒ¨ä»¶æ³¨æ„åŠ›å›¾ï¼Œä»¥æå–å¯¹é½çš„å±€éƒ¨ç‰¹å¾ã€‚ç±»ä¼¼åœ°ï¼ŒPGMANet [[49](#bib.bib49)]
    ç”Ÿæˆå…³é”®ç‚¹ä½ç½®çš„çƒ­å›¾ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬åœ¨ CNN ç‰¹å¾å›¾ä¸Šè®¡ç®—éƒ¨ä»¶æ³¨æ„åŠ›æ©æ¨¡ã€‚åŸºäºéƒ¨ä»¶æ³¨æ„åŠ›æ©æ¨¡èšåˆçš„éƒ¨ä»¶ç‰¹å¾ï¼ŒPGMANet è¿›ä¸€æ­¥è®¡ç®—ä¸åŒéƒ¨ä»¶ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»¥åˆ©ç”¨äºŒé˜¶ä¿¡æ¯æ¥ä¸°å¯Œç‰¹å¾æå–ã€‚HOReID
    [[42](#bib.bib42)] ä½¿ç”¨å…³é”®ç‚¹çƒ­å›¾æå–ä¸ªäººå›¾åƒä¸Šçš„è¯­ä¹‰å±€éƒ¨ç‰¹å¾ã€‚å›¾åƒçš„å±€éƒ¨ç‰¹å¾è¢«è§†ä¸ºå›¾çš„èŠ‚ç‚¹ï¼ŒHOReID è®¾è®¡äº†ä¸€ä¸ªå…·æœ‰å¯å­¦ä¹ é‚»æ¥çŸ©é˜µçš„å›¾å·ç§¯ç½‘ç»œï¼Œä»¥åˆ©ç”¨èŠ‚ç‚¹ä¹‹é—´çš„æ›´å¤šåŒºåˆ†ä¿¡æ¯è¿›è¡Œå†è¯†åˆ«ã€‚æ­¤å¤–ï¼ŒCTL
    [[50](#bib.bib50)] æè®®å°†äººä½“åˆ†ä¸ºä¸‰ç§ç²’åº¦ï¼Œå¹¶ä½¿ç”¨å…³é”®ç‚¹çƒ­å›¾æå–å¤šå°ºåº¦éƒ¨ä»¶ç‰¹å¾ä½œä¸ºå›¾èŠ‚ç‚¹ã€‚äº¤å‰å°ºåº¦å›¾å·ç§¯å’Œ 3D å›¾å·ç§¯æ—¨åœ¨æ•æ‰ç»“æ„ä¿¡æ¯å’Œå±‚æ¬¡ç©ºé—´-æ—¶é—´ä¾èµ–å…³ç³»ï¼Œä»¥è§£å†³è§†é¢‘äººç‰©å†è¯†åˆ«ä¸­çš„ç©ºé—´é”™ä½é—®é¢˜ã€‚ACSAP
    [[51](#bib.bib51)] æè®®ä½¿ç”¨å¤–éƒ¨å§¿æ€ä¿¡æ¯æ¥æŒ‡å¯¼å¯¹é½ç‰¹å¾å›¾çš„å¯¹æŠ—ç”Ÿæˆã€‚PFD [[52](#bib.bib52)] ç”Ÿæˆå±€éƒ¨è¡¥ä¸ç‰¹å¾ï¼Œå¹¶å°†å…¶ä¸å¤„ç†åçš„å…³é”®ç‚¹çƒ­å›¾é€å…ƒç´ ç›¸ä¹˜ï¼Œä»¥è·å¾—å§¿æ€å¼•å¯¼çš„ç‰¹å¾ã€‚é€šè¿‡é›†ç›¸ä¼¼æ€§çš„åº¦é‡ï¼ŒPFD
    åœ¨å±€éƒ¨ç‰¹å¾å’Œå§¿æ€å¼•å¯¼ç‰¹å¾ä¹‹é—´æ‰§è¡ŒåŒ¹é…ï¼Œä»¥ä»è¡¥ä¸ç‰¹å¾ä¸­è§£å¼€å§¿æ€ä¿¡æ¯ä»¥å®ç°ä½ç½®å¯¹é½ã€‚
- en: '![Refer to caption](img/02cfafecf5140d83d7e7a66d47032acd.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/02cfafecf5140d83d7e7a66d47032acd.png)'
- en: 'Figure 6: (*a*) The diagram of pose-based methods: keypoint coordinates for
    addressing the position misalignment and confidence scores for excluding the noisy
    information. (*b*) The diagram of segmentation-based methods: part masks for position
    locating and noisy information excluding.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 6: (*a*) åŸºäºå§¿æ€çš„æ–¹æ³•ç¤ºæ„å›¾ï¼šè§£å†³ä½ç½®é”™ä½çš„å…³é”®ç‚¹åæ ‡å’Œç”¨äºæ’é™¤å™ªå£°ä¿¡æ¯çš„ç½®ä¿¡åº¦åˆ†æ•°ã€‚(*b*) åŸºäºåˆ†å‰²çš„æ–¹æ³•ç¤ºæ„å›¾ï¼šç”¨äºå®šä½ä½ç½®çš„éƒ¨ä»¶æ©æ¨¡å’Œæ’é™¤å™ªå£°ä¿¡æ¯ã€‚'
- en: The parsing-based methods directly take advantage of the parsing information
    generated from an external human parsing model to address the position misalignment
    issue. SPReIDÂ [[53](#bib.bib53)] trains an extra semantic parsing model on the
    human parsing dataset LIPÂ [[72](#bib.bib72)] to predict probability maps associated
    to 5 pre-defined semantic regions of human body. These probability maps are then
    used to extract different semantic features through the weighted sum operation
    on the feature map generated by a modified Inception-V3Â [[73](#bib.bib73)].
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè§£æçš„æ–¹æ³•ç›´æ¥åˆ©ç”¨ä»å¤–éƒ¨äººå·¥è§£ææ¨¡å‹ç”Ÿæˆçš„è§£æä¿¡æ¯æ¥è§£å†³ä½ç½®å¯¹é½é—®é¢˜ã€‚SPReID [[53](#bib.bib53)] åœ¨äººå·¥è§£ææ•°æ®é›† LIP
    [[72](#bib.bib72)] ä¸Šè®­ç»ƒäº†ä¸€ä¸ªé¢å¤–çš„è¯­ä¹‰è§£ææ¨¡å‹ï¼Œä»¥é¢„æµ‹ä¸äººä½“çš„5ä¸ªé¢„å®šä¹‰è¯­ä¹‰åŒºåŸŸç›¸å…³çš„æ¦‚ç‡å›¾ã€‚è¿™äº›æ¦‚ç‡å›¾éšåç”¨äºé€šè¿‡å¯¹ç”±ä¿®æ”¹åçš„ Inception-V3
    [[73](#bib.bib73)] ç”Ÿæˆçš„ç‰¹å¾å›¾è¿›è¡ŒåŠ æƒæ±‚å’Œæ“ä½œæ¥æå–ä¸åŒçš„è¯­ä¹‰ç‰¹å¾ã€‚
- en: The hybrid-based methodsÂ [[54](#bib.bib54), [55](#bib.bib55)] employ two or
    more external models to provide auxiliary information for addressing the position
    misalignment issue. Specifically, SSP-ReIDÂ [[54](#bib.bib54)] exploits the capabilities
    of both clues, i.e., the saliency and the semantic parsing, to guide the CNN backbone
    to learn complementary representations for re-identification. The external off-the-shelf
    deep modelsÂ [[72](#bib.bib72), [74](#bib.bib74)] are employed to generate the
    semantic parsing masks and the saliency masks. The element-wise product is applied
    between masks and a CNN feature map to obtain parsing features and saliency features
    for fusion. TSAÂ [[55](#bib.bib55)] employs off-the-shelf HRNetÂ [[75](#bib.bib75)]
    and DensePoseÂ [[76](#bib.bib76)] to provide extra key-points information and body
    parts information. Based on the key-points locations, the TSA divides the whole
    person into 5 regions and obtains region features on the CNN feature map through
    the soft region pooling. Based on the body part masks, the TSA extracts corresponding
    region features on the texture image produced by a texture generator. The region
    features guided by the key-points are then concatenated with the region features
    guided by the part masks accordingly to learn robust representations for re-identification.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ··åˆçš„æ–¹æ³• [[54](#bib.bib54), [55](#bib.bib55)] ä½¿ç”¨ä¸¤ä¸ªæˆ–æ›´å¤šå¤–éƒ¨æ¨¡å‹æä¾›è¾…åŠ©ä¿¡æ¯ä»¥è§£å†³ä½ç½®å¯¹é½é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼ŒSSP-ReID
    [[54](#bib.bib54)] åˆ©ç”¨ä¸¤ç§çº¿ç´¢ï¼Œå³æ˜¾è‘—æ€§å’Œè¯­ä¹‰è§£æï¼Œæ¥å¼•å¯¼ CNN ä¸»å¹²å­¦ä¹ ç”¨äºé‡æ–°è¯†åˆ«çš„äº’è¡¥è¡¨ç¤ºã€‚å¤–éƒ¨ç°æˆçš„æ·±åº¦æ¨¡å‹ [[72](#bib.bib72),
    [74](#bib.bib74)] ç”¨äºç”Ÿæˆè¯­ä¹‰è§£ææ©æ¨¡å’Œæ˜¾è‘—æ€§æ©æ¨¡ã€‚åœ¨æ©æ¨¡ä¸ CNN ç‰¹å¾å›¾ä¹‹é—´åº”ç”¨é€å…ƒç´ ä¹˜ç§¯ï¼Œä»¥è·å–è§£æç‰¹å¾å’Œæ˜¾è‘—æ€§ç‰¹å¾è¿›è¡Œèåˆã€‚TSA
    [[55](#bib.bib55)] ä½¿ç”¨ç°æˆçš„ HRNet [[75](#bib.bib75)] å’Œ DensePose [[76](#bib.bib76)]
    æä¾›é¢å¤–çš„å…³é”®ç‚¹ä¿¡æ¯å’Œèº«ä½“éƒ¨ä½ä¿¡æ¯ã€‚åŸºäºå…³é”®ç‚¹ä½ç½®ï¼ŒTSA å°†æ•´ä¸ªäººä½“åˆ’åˆ†ä¸º5ä¸ªåŒºåŸŸï¼Œå¹¶é€šè¿‡è½¯åŒºåŸŸæ± åŒ–åœ¨ CNN ç‰¹å¾å›¾ä¸Šè·å¾—åŒºåŸŸç‰¹å¾ã€‚åŸºäºèº«ä½“éƒ¨ä½æ©æ¨¡ï¼ŒTSA
    ä»çº¹ç†ç”Ÿæˆå™¨ç”Ÿæˆçš„çº¹ç†å›¾åƒä¸­æå–ç›¸åº”çš„åŒºåŸŸç‰¹å¾ã€‚ç”±å…³é”®ç‚¹å¼•å¯¼çš„åŒºåŸŸç‰¹å¾ä¸ç”±éƒ¨ä½æ©æ¨¡å¼•å¯¼çš„åŒºåŸŸç‰¹å¾è¿›è¡Œè¿æ¥ï¼Œä»¥å­¦ä¹ ç”¨äºé‡æ–°è¯†åˆ«çš„é²æ£’è¡¨ç¤ºã€‚
- en: IV-A3 Additional Supervision for Position
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 ä½ç½®çš„é¢å¤–ç›‘ç£
- en: Different from the auxiliary model-based solutions that rely on extra information
    in both training and test phases, the additional supervision-based solutions for
    position misalignment only use the extra information to guide the learning process
    and are independent during inference. According to the type of external information
    used, the additional supervision-based solutions for position misalignment can
    be coarsely summarized into pose-basedÂ [[56](#bib.bib56), [57](#bib.bib57), [38](#bib.bib38),
    [58](#bib.bib58), [14](#bib.bib14)], segmentation-basedÂ [[59](#bib.bib59), [60](#bib.bib60)],
    and hybrid-basedÂ [[61](#bib.bib61)] methods.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µéƒ½ä¾èµ–é¢å¤–ä¿¡æ¯çš„è¾…åŠ©æ¨¡å‹è§£å†³æ–¹æ¡ˆä¸åŒï¼Œä½ç½®å¯¹é½çš„é¢å¤–ç›‘ç£åŸºäºè§£å†³æ–¹æ¡ˆä»…åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä½¿ç”¨é¢å¤–ä¿¡æ¯è¿›è¡ŒæŒ‡å¯¼ï¼Œå¹¶ä¸”åœ¨æ¨ç†é˜¶æ®µæ˜¯ç‹¬ç«‹çš„ã€‚æ ¹æ®ä½¿ç”¨çš„å¤–éƒ¨ä¿¡æ¯ç±»å‹ï¼Œä½ç½®å¯¹é½çš„é¢å¤–ç›‘ç£åŸºäºè§£å†³æ–¹æ¡ˆå¯ä»¥ç²—ç•¥åœ°æ€»ç»“ä¸ºåŸºäºå§¿æ€
    [[56](#bib.bib56), [57](#bib.bib57), [38](#bib.bib38), [58](#bib.bib58), [14](#bib.bib14)]ã€åŸºäºåˆ†å‰²
    [[59](#bib.bib59), [60](#bib.bib60)] å’ŒåŸºäºæ··åˆ [[61](#bib.bib61)] çš„æ–¹æ³•ã€‚
- en: The pose-based methodsÂ [[56](#bib.bib56), [57](#bib.bib57), [38](#bib.bib38),
    [58](#bib.bib58), [14](#bib.bib14)] employ external pose information to guide
    the learning process for alleviating the position misalignment problem. Specifically,
    AACNÂ [[56](#bib.bib56)] and DAReIDÂ [[38](#bib.bib38)] learn part attention maps
    to locate and extract part features with the ground truth built from external
    pose information. DSAGÂ [[57](#bib.bib57)] and PGFL-KDÂ [[58](#bib.bib58)] use features
    located by pose information to guide the feature learning process. DSAG constructs
    a set of densely semantically aligned part images with the external pose information
    provided by DensePoseÂ [[76](#bib.bib76)]. Taking the semantically aligned part
    images as the input, DSAG serves as a regulator to guide the feature learning
    on original images through the carefully designed fusion and loss. Similarly,
    PGFL-KD uses external keypoint heatmaps to extract semantically aligned features.
    The aligned features are then employed to regularize the global feature learning
    through knowledge distillation and interaction-based training. Differently, the
    RFCNetÂ [[14](#bib.bib14)] trains an adaptive partition unit supervised by external
    pose information to split the CNN feature map into different regions and extract
    region features for further processing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå§¿æ€çš„æ–¹æ³•Â [[56](#bib.bib56), [57](#bib.bib57), [38](#bib.bib38), [58](#bib.bib58),
    [14](#bib.bib14)] åˆ©ç”¨å¤–éƒ¨å§¿æ€ä¿¡æ¯æ¥å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ï¼Œä»¥ç¼“è§£ä½ç½®å¯¹é½é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒAACNÂ [[56](#bib.bib56)] å’Œ DAReIDÂ [[38](#bib.bib38)]
    é€šè¿‡å¤–éƒ¨å§¿æ€ä¿¡æ¯æ„å»ºçš„åœ°é¢çœŸå®æ•°æ®æ¥å­¦ä¹ éƒ¨ä»¶æ³¨æ„åŠ›å›¾ï¼Œä»è€Œå®šä½å’Œæå–éƒ¨ä»¶ç‰¹å¾ã€‚DSAGÂ [[57](#bib.bib57)] å’Œ PGFL-KDÂ [[58](#bib.bib58)]
    ä½¿ç”¨ç”±å§¿æ€ä¿¡æ¯å®šä½çš„ç‰¹å¾æ¥å¼•å¯¼ç‰¹å¾å­¦ä¹ è¿‡ç¨‹ã€‚DSAG ä½¿ç”¨ DensePoseÂ [[76](#bib.bib76)] æä¾›çš„å¤–éƒ¨å§¿æ€ä¿¡æ¯æ„å»ºä¸€ç»„å¯†é›†è¯­ä¹‰å¯¹é½çš„éƒ¨ä»¶å›¾åƒã€‚å°†è¯­ä¹‰å¯¹é½çš„éƒ¨ä»¶å›¾åƒä½œä¸ºè¾“å…¥ï¼ŒDSAG
    ä½œä¸ºè°ƒèŠ‚å™¨ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„èåˆå’ŒæŸå¤±æ¥å¼•å¯¼åŸå§‹å›¾åƒä¸Šçš„ç‰¹å¾å­¦ä¹ ã€‚åŒæ ·ï¼ŒPGFL-KD ä½¿ç”¨å¤–éƒ¨å…³é”®ç‚¹çƒ­å›¾æ¥æå–è¯­ä¹‰å¯¹é½çš„ç‰¹å¾ã€‚ç„¶åï¼Œè¿™äº›å¯¹é½çš„ç‰¹å¾ç”¨äºé€šè¿‡çŸ¥è¯†è’¸é¦å’ŒåŸºäºäº’åŠ¨çš„è®­ç»ƒæ¥è§„èŒƒå…¨å±€ç‰¹å¾å­¦ä¹ ã€‚ä¸åŒçš„æ˜¯ï¼ŒRFCNetÂ [[14](#bib.bib14)]
    è®­ç»ƒä¸€ä¸ªç”±å¤–éƒ¨å§¿æ€ä¿¡æ¯ç›‘ç£çš„è‡ªé€‚åº”åˆ†åŒºå•å…ƒï¼Œå°† CNN ç‰¹å¾å›¾åˆ†å‰²æˆä¸åŒçš„åŒºåŸŸï¼Œå¹¶æå–åŒºåŸŸç‰¹å¾ä»¥è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†ã€‚
- en: The segmentation-based methodsÂ [[59](#bib.bib59), [60](#bib.bib60)] utilize
    extra segmentation masks provided by the segmentation model, e.g., human parsing
    model or scene segmentation model, to guide the learning process for addressing
    the position misalignment problem. Specifically, MMGAÂ [[59](#bib.bib59)] learns
    to generate the whole-body, upper-body, and lower-body attention maps with the
    parsing labels estimated by JPPNetÂ [[77](#bib.bib77)]. The learned attention maps
    are then employed to extract global and local features for re-identification.
    HPNetÂ [[60](#bib.bib60)] introduces human parsing as an auxiliary task and employs
    the parsing masks to extract part-level features for addressing the position misalignment
    issue. The person Re-ID and the human parsing are learned in a multi-task manner
    where the pseudo parsing label are predicted by the scene segmentation modelÂ [[78](#bib.bib78)]
    trained on the COCO DensePoseÂ [[76](#bib.bib76)] dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºåˆ†å‰²çš„æ–¹æ³•Â [[59](#bib.bib59), [60](#bib.bib60)] åˆ©ç”¨ç”±åˆ†å‰²æ¨¡å‹æä¾›çš„é¢å¤–åˆ†å‰²æ©ç ï¼Œä¾‹å¦‚ï¼Œäººç±»è§£ææ¨¡å‹æˆ–åœºæ™¯åˆ†å‰²æ¨¡å‹ï¼Œæ¥å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ï¼Œä»¥è§£å†³ä½ç½®å¯¹é½é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒMMGAÂ [[59](#bib.bib59)]
    å­¦ä¹ ç”Ÿæˆå…¨èº«ã€ä¸ŠåŠèº«å’Œä¸‹åŠèº«çš„æ³¨æ„åŠ›å›¾ï¼Œè¿™äº›æ³¨æ„åŠ›å›¾ä½¿ç”¨ JPPNetÂ [[77](#bib.bib77)] ä¼°è®¡çš„è§£ææ ‡ç­¾ã€‚ç„¶åï¼Œå­¦ä¹ åˆ°çš„æ³¨æ„åŠ›å›¾ç”¨äºæå–å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ä»¥è¿›è¡Œå†è¯†åˆ«ã€‚HPNetÂ [[60](#bib.bib60)]
    å¼•å…¥äººç±»è§£æä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨è§£ææ©ç æå–éƒ¨ä»¶çº§ç‰¹å¾ï¼Œä»¥è§£å†³ä½ç½®å¯¹é½é—®é¢˜ã€‚ä¸ªäºº Re-ID å’Œäººç±»è§£æä»¥å¤šä»»åŠ¡æ–¹å¼è¿›è¡Œå­¦ä¹ ï¼Œå…¶ä¸­ä¼ªè§£ææ ‡ç­¾ç”±åœ¨ COCO
    DensePoseÂ [[76](#bib.bib76)] æ•°æ®é›†ä¸Šè®­ç»ƒçš„åœºæ™¯åˆ†å‰²æ¨¡å‹Â [[78](#bib.bib78)] é¢„æµ‹ã€‚
- en: The hybrid-based methods utilize more than one type of external information
    to guide the learning process. Specifically, FGSAÂ [[61](#bib.bib61)] mines fine-grained
    local features with the supervision of both the pose information and the attribute
    information to address the position misalignment issue. FGSA designs a pose resolve
    net (pre-trained on MSCOCOÂ [[79](#bib.bib79)]) to provide part confidence maps
    and part affinity fields of the key parts. These part maps are then used to extract
    part features on a CNN feature map through compact bilinear pooling. Given the
    extracted part features, FGSA treats the attribute recognition as multiple classification
    tasks and trains an intermediate model for attribute classification along with
    the person Re-ID. In this way, the attribute classification tasks guide the pose
    resolve net and the CNN backbone to learn more discriminated local information
    for re-identification.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ··åˆçš„æ–¹æ³•åˆ©ç”¨å¤šç§ç±»å‹çš„å¤–éƒ¨ä¿¡æ¯æ¥æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼ŒFGSAÂ [[61](#bib.bib61)] åœ¨å§¿æ€ä¿¡æ¯å’Œå±æ€§ä¿¡æ¯çš„ç›‘ç£ä¸‹æŒ–æ˜ç»†ç²’åº¦çš„å±€éƒ¨ç‰¹å¾ï¼Œä»¥è§£å†³ä½ç½®é”™ä½é—®é¢˜ã€‚FGSA
    è®¾è®¡äº†ä¸€ä¸ªå§¿æ€è§£æç½‘ç»œï¼ˆåœ¨ MSCOCOÂ [[79](#bib.bib79)] ä¸Šé¢„è®­ç»ƒï¼‰ï¼Œæä¾›å…³é”®éƒ¨ä½çš„éƒ¨ä»¶ç½®ä¿¡åº¦å›¾å’Œéƒ¨ä»¶äº²å’Œåœºã€‚è¿™äº›éƒ¨ä»¶å›¾ç”¨äºé€šè¿‡ç´§å‡‘åŒçº¿æ€§æ± åŒ–åœ¨
    CNN ç‰¹å¾å›¾ä¸Šæå–éƒ¨ä»¶ç‰¹å¾ã€‚ç»™å®šæå–çš„éƒ¨ä»¶ç‰¹å¾ï¼ŒFGSA å°†å±æ€§è¯†åˆ«è§†ä¸ºå¤šåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªä¸­é—´æ¨¡å‹è¿›è¡Œå±æ€§åˆ†ç±»ä»¥åŠäººç‰©é‡æ–°è¯†åˆ«ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå±æ€§åˆ†ç±»ä»»åŠ¡æŒ‡å¯¼å§¿æ€è§£æç½‘ç»œå’Œ
    CNN ä¸»å¹²å­¦ä¹ æ›´å…·åŒºåˆ†æ€§çš„å±€éƒ¨ä¿¡æ¯ä»¥ä¾¿é‡æ–°è¯†åˆ«ã€‚
- en: IV-A4 Attention Mechanism for Position
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A4 ç”¨äºä½ç½®çš„æ³¨æ„æœºåˆ¶
- en: 'The attention mechanism-based methods learn attention to address the position
    misalignment issue without any additional information (see Fig.Â [7](#S4.F7 "Figure
    7 â€£ IV-A4 Attention Mechanism for Position â€£ IV-A Position Misalignment â€£ IV Occluded
    Person Re-ID â€£ Deep Learning-based Occluded Person Re-identification: A Survey")).
    According to the main point of the attention learning process, the attention mechanism-based
    solutions for position misalignment can be further grouped into cropping-basedÂ [[32](#bib.bib32)],
    clustering-basedÂ [[62](#bib.bib62)], self-supervisedÂ [[63](#bib.bib63), [64](#bib.bib64)],
    and constraint-basedÂ [[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68)]
    methods. It should be noticed that some methods have also involved the attention
    mechanism but rely on external information provided by auxiliary models or use
    additional information for supervision. These methods are summarized in Auxiliary
    Model for position or Additional Supervision for Position accordingly.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ³¨æ„æœºåˆ¶çš„æ–¹æ³•é€šè¿‡å­¦ä¹ æ³¨æ„åŠ›æ¥è§£å†³ä½ç½®é”™ä½é—®é¢˜ï¼Œè€Œä¸éœ€è¦ä»»ä½•é¢å¤–ä¿¡æ¯ï¼ˆè§å›¾Â [7](#S4.F7 "å›¾ 7 â€£ IV-A4 ç”¨äºä½ç½®çš„æ³¨æ„æœºåˆ¶ â€£
    IV-A ä½ç½®é”™ä½ â€£ IV é®æŒ¡äººç‰©é‡æ–°è¯†åˆ« â€£ åŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡äººç‰©é‡æ–°è¯†åˆ«ï¼šç»¼è¿°")ï¼‰ã€‚æ ¹æ®æ³¨æ„å­¦ä¹ è¿‡ç¨‹çš„ä¸»è¦è§‚ç‚¹ï¼ŒåŸºäºæ³¨æ„æœºåˆ¶çš„ä½ç½®é”™ä½è§£å†³æ–¹æ¡ˆå¯ä»¥è¿›ä¸€æ­¥åˆ†ä¸ºåŸºäºè£å‰ªçš„Â [[32](#bib.bib32)]ã€åŸºäºèšç±»çš„Â [[62](#bib.bib62)]ã€è‡ªç›‘ç£çš„Â [[63](#bib.bib63),
    [64](#bib.bib64)] å’ŒåŸºäºçº¦æŸçš„Â [[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68)] æ–¹æ³•ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸€äº›æ–¹æ³•è™½ç„¶ä¹Ÿæ¶‰åŠåˆ°æ³¨æ„æœºåˆ¶ï¼Œä½†ä¾èµ–äºè¾…åŠ©æ¨¡å‹æä¾›çš„å¤–éƒ¨ä¿¡æ¯æˆ–ä½¿ç”¨é¢å¤–çš„ä¿¡æ¯è¿›è¡Œç›‘ç£ã€‚è¿™äº›æ–¹æ³•åˆ†åˆ«æ€»ç»“åœ¨ä½ç½®çš„è¾…åŠ©æ¨¡å‹æˆ–ä½ç½®çš„é¢å¤–ç›‘ç£ä¸­ã€‚
- en: The cropping-based methods crop images into different local regions and learn
    attention to find the same local regions across different images for similarity
    calculation. Specifically, DPPRÂ [[32](#bib.bib32)] crops 13 predefined partial
    regions on holistic images and designs an attention module conditioned on the
    partial probe image to assign the partial regions with larger attention weights
    if the same body parts are included.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè£å‰ªçš„æ–¹æ³•å°†å›¾åƒè£å‰ªä¸ºä¸åŒçš„å±€éƒ¨åŒºåŸŸï¼Œå¹¶å­¦ä¹ æ³¨æ„åŠ›ä»¥åœ¨ä¸åŒå›¾åƒä¸­æ‰¾åˆ°ç›¸åŒçš„å±€éƒ¨åŒºåŸŸè¿›è¡Œç›¸ä¼¼æ€§è®¡ç®—ã€‚å…·ä½“è€Œè¨€ï¼ŒDPPRÂ [[32](#bib.bib32)]
    åœ¨æ•´ä½“å›¾åƒä¸Šè£å‰ª 13 ä¸ªé¢„å®šä¹‰çš„éƒ¨åˆ†åŒºåŸŸï¼Œå¹¶è®¾è®¡ä¸€ä¸ªæ¡ä»¶äºéƒ¨åˆ†æ¢æµ‹å›¾åƒçš„æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥åœ¨åŒ…å«ç›¸åŒèº«ä½“éƒ¨ä½çš„æƒ…å†µä¸‹ä¸ºè¿™äº›éƒ¨åˆ†åŒºåŸŸåˆ†é…æ›´å¤§çš„æ³¨æ„åŠ›æƒé‡ã€‚
- en: '![Refer to caption](img/ad9b14b2ae74aeb4dbd62a98bd6626a9.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/ad9b14b2ae74aeb4dbd62a98bd6626a9.png)'
- en: 'Figure 7: Examples of technical routes in attention mechanism-based methods.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 7ï¼šåŸºäºæ³¨æ„æœºåˆ¶æ–¹æ³•çš„æŠ€æœ¯è·¯çº¿ç¤ºä¾‹ã€‚
- en: The clustering-based methods generate pseudo-labels from clustering to supervise
    the attention learning. Specifically, ISPÂ [[62](#bib.bib62)] designs the cascaded
    clustering on CNN feature maps to gradually generate pixel-level pseudo-labels
    of human parts for part attention learning. Based on the assumption that the foreground
    pixels have higher responses than the background ones, all pixels of a feature
    map are first clustered into foreground or background according to the activation.
    Secondly, the foreground pixels are further clustered into different human parts
    according to the similarities between pixels. The clustered pixel-level pseudo-labels
    of human parts are then employed to guide the part attention learning for extracting
    local part features to address the position misalignment issue.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºèšç±»çš„æ–¹æ³•é€šè¿‡èšç±»ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œä»¥ç›‘ç£æ³¨æ„åŠ›å­¦ä¹ ã€‚å…·ä½“è€Œè¨€ï¼ŒISP [[62](#bib.bib62)] åœ¨ CNN ç‰¹å¾å›¾ä¸Šè®¾è®¡çº§è”èšç±»ï¼Œé€æ­¥ç”Ÿæˆç”¨äºéƒ¨åˆ†æ³¨æ„åŠ›å­¦ä¹ çš„åƒç´ çº§ä¼ªæ ‡ç­¾ã€‚åŸºäºå‰æ™¯åƒç´ å“åº”é«˜äºèƒŒæ™¯åƒç´ çš„å‡è®¾ï¼Œé¦–å…ˆå°†ç‰¹å¾å›¾çš„æ‰€æœ‰åƒç´ æ ¹æ®æ¿€æ´»å€¼èšç±»ä¸ºå‰æ™¯æˆ–èƒŒæ™¯ã€‚å…¶æ¬¡ï¼Œæ ¹æ®åƒç´ ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå°†å‰æ™¯åƒç´ è¿›ä¸€æ­¥èšç±»ä¸ºä¸åŒçš„äººä½“éƒ¨åˆ†ã€‚ç„¶åï¼Œåˆ©ç”¨èšç±»è€Œå¾—åˆ°çš„åƒç´ çº§äººä½“éƒ¨åˆ†ä¼ªæ ‡ç­¾æ¥æŒ‡å¯¼éƒ¨åˆ†æ³¨æ„åŠ›å­¦ä¹ ï¼Œæå–å±€éƒ¨éƒ¨åˆ†ç‰¹å¾ä»¥è§£å†³ä½ç½®ä¸åŒ¹é…é—®é¢˜ã€‚
- en: The self-supervised methodsÂ [[63](#bib.bib63), [64](#bib.bib64)] construct self-supervision
    to guide the attention learning. Given a holistic image, VPMÂ [[63](#bib.bib63)]
    defines $m\times n$ rectangle regions and randomly crops a patch in which every
    pixel is assigned with the region label accordingly for self-supervision. VPM
    appends a region locator upon the extracted CNN feature map to discover different
    regions through the pixel-wise classification (attention) with the self-supervision
    constructed above. APNÂ [[64](#bib.bib64)] randomly crops the holistic image into
    different partial types as the self-supervision for attention-based cropping type
    classification of a partial image. The holistic gallery images are cropped for
    person retrieval according to the predicted cropping type of the partial probe
    image to accomplish the position alignment.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç›‘ç£æ–¹æ³• [[63](#bib.bib63), [64](#bib.bib64)] æ„å»ºè‡ªæˆ‘ç›‘ç£ä»¥æŒ‡å¯¼æ³¨æ„åŠ›å­¦ä¹ ã€‚ç»™å®šä¸€ä¸ªæ•´ä½“å›¾åƒï¼ŒVPM [[63](#bib.bib63)]
    å®šä¹‰ $m\times n$ çš„çŸ©å½¢åŒºåŸŸï¼Œå¹¶éšæœºè£å‰ªä¸€ä¸ª patchï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ æ ¹æ®ç›¸åº”çš„åŒºåŸŸæ ‡ç­¾è¿›è¡Œè‡ªæˆ‘ç›‘ç£ã€‚VPM åœ¨æå–çš„ CNN ç‰¹å¾å›¾ä¸Šæ·»åŠ äº†åŒºåŸŸå®šä½å™¨ï¼Œé€šè¿‡ä¸Šè¿°æ„å»ºçš„è‡ªæˆ‘ç›‘ç£è¿›è¡Œåƒç´ çº§åˆ†ç±»ï¼ˆæ³¨æ„åŠ›ï¼‰ï¼Œä»¥å‘ç°ä¸åŒçš„åŒºåŸŸã€‚APN
    [[64](#bib.bib64)] å°†æ•´ä½“å›¾åƒéšæœºè£å‰ªä¸ºä¸åŒçš„éƒ¨åˆ†ç±»å‹ï¼Œä½œä¸ºéƒ¨åˆ†å›¾åƒçš„è‡ªç›‘ç£ä»»åŠ¡ï¼Œç”¨äºåŸºäºæ³¨æ„åŠ›çš„éƒ¨åˆ†è£å‰ªç±»å‹åˆ†ç±»ã€‚æ ¹æ®éƒ¨åˆ†æ¢æµ‹å›¾åƒçš„é¢„æµ‹è£å‰ªç±»å‹ï¼Œå¯¹æ•´ä½“æ ·æœ¬å›¾åƒè¿›è¡Œè£å‰ªä»¥å®ç°ä½ç½®å¯¹é½ã€‚
- en: The constraint-based methodsÂ [[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68)] build constraints among different attention maps to help locate
    diverse body parts for addressing the position misalignment issue. The multiple
    spatial attentions inÂ [[65](#bib.bib65)] employ a diversity regularization term
    on attention maps to ensure each attention focuses on different regions of the
    given image. SBPAÂ [[66](#bib.bib66)] separates local attention maps through minimizing
    the L1-norm distance between the local attention and the masked global attention.
    PATÂ [[67](#bib.bib67)] maintains vectors of part prototypes to generate part-aware
    attention masks on contextual CNN features and designs the part diversity mechanism
    to help achieve diverse part discovery. Specifically, the part diversity mechanism
    minimizes the cosine similarity between every two part features to expand the
    discrepancy among different part features. MHSA-NetÂ [[68](#bib.bib68)] proposes
    the feature diversity regularization term to encourage the diversity of local
    features captured by a multi-head self-attention mechanism. Specifically, in order
    to obtain diverse local features, the regularization term restricts the Gram matrix
    of local features to be close to an identity matrix.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºçº¦æŸçš„æ–¹æ³• [[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68)]
    åœ¨ä¸åŒçš„æ³¨æ„åŠ›å›¾ä¹‹é—´å»ºç«‹çº¦æŸï¼Œä»¥å¸®åŠ©å®šä½ä¸åŒçš„èº«ä½“éƒ¨ä½ï¼Œä»¥è§£å†³ä½ç½®ä¸åŒ¹é…é—®é¢˜ã€‚[[65](#bib.bib65)] ä¸­çš„å¤šä¸ªç©ºé—´æ³¨æ„åŠ›å¯¹æ³¨æ„åŠ›å›¾åº”ç”¨å¤šæ ·æ€§æ­£åˆ™åŒ–é¡¹ï¼Œç¡®ä¿æ¯ä¸ªæ³¨æ„åŠ›éƒ½ä¸“æ³¨äºç»™å®šå›¾åƒçš„ä¸åŒåŒºåŸŸã€‚SBPA
    [[66](#bib.bib66)] é€šè¿‡æœ€å°åŒ–å±€éƒ¨æ³¨æ„åŠ›ä¸è¢«å±è”½çš„å…¨å±€æ³¨æ„åŠ›ä¹‹é—´çš„ L1-èŒƒæ•°è·ç¦»æ¥åˆ†ç¦»å±€éƒ¨æ³¨æ„åŠ›å›¾ã€‚PAT [[67](#bib.bib67)]
    ç»´æŠ¤éƒ¨åˆ†åŸå‹å‘é‡ï¼Œä»¥åœ¨ä¸Šä¸‹æ–‡ CNN ç‰¹å¾ä¸Šç”Ÿæˆéƒ¨åˆ†æ„ŸçŸ¥æ³¨æ„åŠ›æ©ç ï¼Œå¹¶è®¾è®¡éƒ¨åˆ†å¤šæ ·æ€§æœºåˆ¶æ¥å¸®åŠ©å®ç°å¤šæ ·çš„éƒ¨åˆ†å‘ç°ã€‚å…·ä½“è€Œè¨€ï¼Œéƒ¨åˆ†å¤šæ ·æ€§æœºåˆ¶å°†æ‰€æœ‰éƒ¨åˆ†ç‰¹å¾ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æœ€å°åŒ–ï¼Œæ‰©å¤§ä¸åŒéƒ¨åˆ†ç‰¹å¾ä¹‹é—´çš„å·®å¼‚ã€‚MHSA-Net
    [[68](#bib.bib68)] æå‡ºç‰¹å¾å¤šæ ·æ€§æ­£åˆ™åŒ–é¡¹ï¼Œé¼“åŠ±å¤šå¤´è‡ªæ³¨æ„æœºåˆ¶æ•æ‰å¤šæ ·çš„å±€éƒ¨ç‰¹å¾ã€‚å…·ä½“è€Œè¨€ï¼Œä¸ºäº†è·å¾—å¤šæ ·çš„å±€éƒ¨ç‰¹å¾ï¼Œæ­£åˆ™åŒ–é¡¹é™åˆ¶å±€éƒ¨ç‰¹å¾çš„
    Gram çŸ©é˜µæ¥è¿‘æ’ç­‰çŸ©é˜µã€‚
- en: IV-B Scale Misalignment
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B å°ºåº¦é”™ä½
- en: Deep learning-based methodsÂ [[80](#bib.bib80), [43](#bib.bib43), [30](#bib.bib30),
    [81](#bib.bib81), [66](#bib.bib66)] propose to construct pyramid features or multi-scale
    features for alleviating the scale misalignment issue in person Re-ID. Given a
    CNN feature map, the pyramid features are extracted from global to local while
    the multi-scale features maintain features of different receptive fields at the
    same position. On the whole, the core of both pyramid features and multi-scale
    features is to extract features of different scales to construct robust representations
    to scale variation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•Â [[80](#bib.bib80), [43](#bib.bib43), [30](#bib.bib30), [81](#bib.bib81),
    [66](#bib.bib66)] æå‡ºæ„å»ºé‡‘å­—å¡”ç‰¹å¾æˆ–å¤šå°ºåº¦ç‰¹å¾ä»¥ç¼“è§£äººè„¸é‡è¯†åˆ«ä¸­çš„å°ºåº¦é”™ä½é—®é¢˜ã€‚ç»™å®šä¸€ä¸ª CNN ç‰¹å¾å›¾ï¼Œé‡‘å­—å¡”ç‰¹å¾ä»å…¨å±€åˆ°å±€éƒ¨æå–ï¼Œè€Œå¤šå°ºåº¦ç‰¹å¾åœ¨åŒä¸€ä½ç½®ä¿æŒä¸åŒæ„Ÿå—é‡çš„ç‰¹å¾ã€‚æ€»ä½“è€Œè¨€ï¼Œé‡‘å­—å¡”ç‰¹å¾å’Œå¤šå°ºåº¦ç‰¹å¾çš„æ ¸å¿ƒéƒ½æ˜¯æå–ä¸åŒå°ºåº¦çš„ç‰¹å¾ï¼Œä»¥æ„å»ºå¯¹å°ºåº¦å˜åŒ–å…·æœ‰é²æ£’æ€§çš„è¡¨ç¤ºã€‚
- en: IV-B1 Pyramid Features
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 é‡‘å­—å¡”ç‰¹å¾
- en: The pyramid featuresÂ [[80](#bib.bib80), [43](#bib.bib43)] are hierarchical and
    are extracted from global to local on the feature map. The pyramidal model inÂ [[80](#bib.bib80)]
    horizontally slices the feature map into $n$ basic parts and builds corresponding
    branches for every $l\in\{1,2,...,n\}$ adjacent parts to obtain the pyramid features.
    MGHÂ [[43](#bib.bib43)] hierarchically divides the feature map into $p\in\{1,2,4,8\}$
    horizontal strips and average pools each strip to obtain multi-granular spatial
    features.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: é‡‘å­—å¡”ç‰¹å¾Â [[80](#bib.bib80), [43](#bib.bib43)] æ˜¯å±‚æ¬¡åŒ–çš„ï¼Œå¹¶ä¸”ä»ç‰¹å¾å›¾çš„å…¨å±€åˆ°å±€éƒ¨æå–ã€‚[[80](#bib.bib80)]
    ä¸­çš„é‡‘å­—å¡”æ¨¡å‹æ°´å¹³åˆ‡åˆ†ç‰¹å¾å›¾ä¸º $n$ ä¸ªåŸºæœ¬éƒ¨åˆ†ï¼Œå¹¶ä¸ºæ¯ä¸ª $l\in\{1,2,...,n\}$ ç›¸é‚»éƒ¨åˆ†å»ºç«‹ç›¸åº”çš„åˆ†æ”¯ä»¥è·å–é‡‘å­—å¡”ç‰¹å¾ã€‚MGHÂ [[43](#bib.bib43)]
    å°†ç‰¹å¾å›¾å±‚æ¬¡åŒ–åœ°åˆ’åˆ†ä¸º $p\in\{1,2,4,8\}$ ä¸ªæ°´å¹³æ¡å¸¦ï¼Œå¹¶å¯¹æ¯ä¸ªæ¡å¸¦è¿›è¡Œå¹³å‡æ± åŒ–ä»¥è·å¾—å¤šç²’åº¦ç©ºé—´ç‰¹å¾ã€‚
- en: IV-B2 Multi-scale Features
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 å¤šå°ºåº¦ç‰¹å¾
- en: The multi-scale featuresÂ [[30](#bib.bib30), [81](#bib.bib81), [66](#bib.bib66)]
    focus on restricted local regions and maintain features of different receptive
    fields at the same position. Specifically, DSRÂ [[30](#bib.bib30)] average pools
    the square area of $s\times s$ pixels on a feature map to obtain the multi-scale
    block representations for alleviating the influence of scale mismatching, $s=\{1,2,3\}$.
    FPRÂ [[81](#bib.bib81)] performs multiple max-pooling layers of different kernel
    sizes upon the feature map to capture diverse spatial features from small local
    regions to relatively large regions. SBPAÂ [[66](#bib.bib66)] maintains features
    in different scales at each pixel through the scale-wise residual connection for
    addressing the scale misalignment issue.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå°ºåº¦ç‰¹å¾Â [[30](#bib.bib30), [81](#bib.bib81), [66](#bib.bib66)] èšç„¦äºé™åˆ¶çš„å±€éƒ¨åŒºåŸŸï¼Œå¹¶åœ¨åŒä¸€ä½ç½®ä¿æŒä¸åŒæ„Ÿå—é‡çš„ç‰¹å¾ã€‚å…·ä½“è€Œè¨€ï¼ŒDSRÂ [[30](#bib.bib30)]
    å¯¹ç‰¹å¾å›¾ä¸Šçš„ $s\times s$ åƒç´ çš„æ­£æ–¹å½¢åŒºåŸŸè¿›è¡Œå¹³å‡æ± åŒ–ï¼Œä»¥è·å–å¤šå°ºåº¦å—è¡¨ç¤ºï¼Œä»è€Œç¼“è§£å°ºåº¦é”™ä½çš„å½±å“ï¼Œ$s=\{1,2,3\}$ã€‚FPRÂ [[81](#bib.bib81)]
    å¯¹ç‰¹å¾å›¾æ‰§è¡Œä¸åŒæ ¸å¤§å°çš„å¤šæ¬¡æœ€å¤§æ± åŒ–å±‚ï¼Œä»¥æ•æ‰ä»å°çš„å±€éƒ¨åŒºåŸŸåˆ°ç›¸å¯¹è¾ƒå¤§åŒºåŸŸçš„å¤šæ ·ç©ºé—´ç‰¹å¾ã€‚SBPAÂ [[66](#bib.bib66)] é€šè¿‡å°ºåº¦æ–¹å‘çš„æ®‹å·®è¿æ¥åœ¨æ¯ä¸ªåƒç´ å¤„ç»´æŒä¸åŒå°ºåº¦çš„ç‰¹å¾ï¼Œä»¥è§£å†³å°ºåº¦é”™ä½é—®é¢˜ã€‚
- en: IV-C Position and Scale Misalignment
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C ä½ç½®ä¸å°ºåº¦é”™ä½
- en: 'In the literature, there are some methodsÂ [[82](#bib.bib82), [83](#bib.bib83),
    [40](#bib.bib40), [51](#bib.bib51), [84](#bib.bib84), [64](#bib.bib64)] that simultaneously
    address the position and scale misalignment issues through the transformation
    of partial or holistic images (see Fig.Â [8](#S4.F8 "Figure 8 â€£ IV-C Position and
    Scale Misalignment â€£ IV Occluded Person Re-ID â€£ Deep Learning-based Occluded Person
    Re-identification: A Survey") (*a*)). These methods are specifically summarized
    in this subsection for clarity.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ–‡çŒ®ä¸­ï¼Œæœ‰ä¸€äº›æ–¹æ³•Â [[82](#bib.bib82), [83](#bib.bib83), [40](#bib.bib40), [51](#bib.bib51),
    [84](#bib.bib84), [64](#bib.bib64)] é€šè¿‡éƒ¨åˆ†æˆ–æ•´ä½“å›¾åƒçš„å˜æ¢åŒæ—¶è§£å†³ä½ç½®å’Œå°ºåº¦é”™ä½é—®é¢˜ï¼ˆè§å›¾Â [8](#S4.F8 "Figure
    8 â€£ IV-C Position and Scale Misalignment â€£ IV Occluded Person Re-ID â€£ Deep Learning-based
    Occluded Person Re-identification: A Survey") (*a*)ï¼‰ã€‚è¿™äº›æ–¹æ³•åœ¨æœ¬å°èŠ‚ä¸­è¿›è¡Œäº†ä¸“é—¨æ€»ç»“ï¼Œä»¥ä¾¿äºç†è§£ã€‚'
- en: The partial image transformationÂ [[82](#bib.bib82), [83](#bib.bib83), [40](#bib.bib40),
    [51](#bib.bib51)] aims to transform partial images to obtain the rectified results
    that are spatially aligned with holistic ones for re-identification. Specifically,
    APNetÂ [[82](#bib.bib82)] designs a bounding box aligner (BBA) which predicts 4
    offset values (top, bottom, left, and right) to shift the detected bounding boxes
    to cover the estimated holistic body region. Without manual annotations, APNet
    is trained by constructing automatic data augmentation. PPCLÂ [[83](#bib.bib83)]
    employs a gated transformation regression CNN module to predict the affine transformation
    coefficients between the partial and the holistic images for generating rectified
    partial images with proper scale and layout. The prediction of transformation
    coefficients is self-supervised by randomly cropping holistic images to simulate
    partial images with known transformation coefficients. The Image Rescaler (IR)
    inÂ [[40](#bib.bib40)] predicts the 2D affine transformation parameters to transform
    a partial image into a desirable distortion-free image for addressing the spatial
    misalignment issue. Specifically, the partial image and the desirable distortion-free
    image are obtained by randomly cropping the holistic image and masking the uncropped
    regions, for self-supervision. Differently, ACSAPÂ [[51](#bib.bib51)] designs a
    pose-guided generator that utilizes extra pose information of both partial and
    holistic images to guide the generation of aligned features for partial images.
    The pose-guided generator is adversarially learned by training a pose-guided discriminator,
    which aims to distinguish the authenticity of image features.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: éƒ¨åˆ†å›¾åƒå˜æ¢Â [[82](#bib.bib82), [83](#bib.bib83), [40](#bib.bib40), [51](#bib.bib51)]
    æ—¨åœ¨å¯¹éƒ¨åˆ†å›¾åƒè¿›è¡Œå˜æ¢ï¼Œä»¥è·å¾—ä¸æ•´ä½“å›¾åƒç©ºé—´å¯¹é½çš„æ ¡æ­£ç»“æœä»¥ç”¨äºé‡æ–°è¯†åˆ«ã€‚å…·ä½“è€Œè¨€ï¼ŒAPNetÂ [[82](#bib.bib82)] è®¾è®¡äº†ä¸€ç§è¾¹ç•Œæ¡†å¯¹é½å™¨ï¼ˆBBAï¼‰ï¼Œé¢„æµ‹4ä¸ªåç§»å€¼ï¼ˆä¸Šã€ä¸‹ã€å·¦ã€å³ï¼‰ä»¥å°†æ£€æµ‹åˆ°çš„è¾¹ç•Œæ¡†ç§»åŠ¨åˆ°è¦†ç›–ä¼°è®¡çš„æ•´ä½“èº«ä½“åŒºåŸŸã€‚APNet
    é€šè¿‡æ„å»ºè‡ªåŠ¨æ•°æ®å¢å¼ºè¿›è¡Œè®­ç»ƒï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šã€‚PPCLÂ [[83](#bib.bib83)] é‡‡ç”¨äº†ä¸€ä¸ªé—¨æ§å˜æ¢å›å½’CNNæ¨¡å—æ¥é¢„æµ‹éƒ¨åˆ†å›¾åƒä¸æ•´ä½“å›¾åƒä¹‹é—´çš„ä»¿å°„å˜æ¢ç³»æ•°ï¼Œä»è€Œç”Ÿæˆå…·æœ‰é€‚å½“å°ºåº¦å’Œå¸ƒå±€çš„æ ¡æ­£éƒ¨åˆ†å›¾åƒã€‚å˜æ¢ç³»æ•°çš„é¢„æµ‹é€šè¿‡éšæœºè£å‰ªæ•´ä½“å›¾åƒæ¥æ¨¡æ‹Ÿå…·æœ‰å·²çŸ¥å˜æ¢ç³»æ•°çš„éƒ¨åˆ†å›¾åƒè¿›è¡Œè‡ªç›‘ç£ã€‚[[40](#bib.bib40)]
    ä¸­çš„å›¾åƒé‡æ ‡å®šå™¨ï¼ˆIRï¼‰é¢„æµ‹2Dä»¿å°„å˜æ¢å‚æ•°ï¼Œä»¥å°†éƒ¨åˆ†å›¾åƒå˜æ¢ä¸ºç†æƒ³çš„æ— ç•¸å˜å›¾åƒï¼Œä»¥è§£å†³ç©ºé—´ä¸å¯¹é½é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œéƒ¨åˆ†å›¾åƒå’Œç†æƒ³çš„æ— ç•¸å˜å›¾åƒé€šè¿‡éšæœºè£å‰ªæ•´ä½“å›¾åƒå¹¶é®è”½æœªè£å‰ªåŒºåŸŸæ¥è·å¾—ï¼Œç”¨äºè‡ªç›‘ç£ã€‚ä¸åŒçš„æ˜¯ï¼ŒACSAPÂ [[51](#bib.bib51)]
    è®¾è®¡äº†ä¸€ä¸ªå§¿æ€å¼•å¯¼ç”Ÿæˆå™¨ï¼Œåˆ©ç”¨éƒ¨åˆ†å›¾åƒå’Œæ•´ä½“å›¾åƒçš„é¢å¤–å§¿æ€ä¿¡æ¯æ¥å¼•å¯¼å¯¹é½ç‰¹å¾çš„ç”Ÿæˆã€‚å§¿æ€å¼•å¯¼ç”Ÿæˆå™¨é€šè¿‡è®­ç»ƒä¸€ä¸ªå§¿æ€å¼•å¯¼åˆ¤åˆ«å™¨è¿›è¡Œå¯¹æŠ—å­¦ä¹ ï¼Œæ—¨åœ¨åŒºåˆ†å›¾åƒç‰¹å¾çš„çœŸå®æ€§ã€‚
- en: '![Refer to caption](img/cad2655e9ac03bb02c21c0b60eb5c3a8.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/cad2655e9ac03bb02c21c0b60eb5c3a8.png)'
- en: 'Figure 8: (*a*) The diagram of image transformation methods: addressing the
    position and scale misalignments simultaneously. (*b*) The diagram of attribute-based
    methods: associating the attribute annotations with person Re-ID.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8ï¼š(*a*) å›¾åƒå˜æ¢æ–¹æ³•ç¤ºæ„å›¾ï¼šåŒæ—¶å¤„ç†ä½ç½®å’Œå°ºåº¦çš„ä¸åŒ¹é…ã€‚ (*b*) åŸºäºå±æ€§çš„æ–¹æ³•ç¤ºæ„å›¾ï¼šå°†å±æ€§æ³¨é‡Šä¸äººå‘˜é‡æ–°è¯†åˆ«ï¼ˆRe-IDï¼‰å…³è”èµ·æ¥ã€‚
- en: The holistic image transformationÂ [[84](#bib.bib84), [64](#bib.bib64)] aims
    to find and transform the corresponding regions of holistic images to obtain the
    images that are spatially aligned with the query partial images for re-identification.
    Specifically, STNReIDÂ [[84](#bib.bib84)] utilizes the high-level CNN features
    of the partial and the holistic images to predict the 2D affine transformation
    parameters between them to transform the holistic image for matching with the
    partial image. In STNReID, the holistic images are randomly cropped to partial
    images for building the self-supervised training of 2D affine transformation parameters
    prediction. APNÂ [[64](#bib.bib64)] predicts the cropping type of a partial probe
    image and crops the corresponding regions of holistic gallery images for person
    retrieval. In APN, the prediction of the cropping type is trained with the self-supervision
    constructed by randomly cropping the holistic image into different partial types.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: æ•´ä½“å›¾åƒå˜æ¢Â [[84](#bib.bib84), [64](#bib.bib64)] æ—¨åœ¨æŸ¥æ‰¾å’Œå˜æ¢æ•´ä½“å›¾åƒçš„å¯¹åº”åŒºåŸŸï¼Œä»¥è·å¾—ä¸æŸ¥è¯¢éƒ¨åˆ†å›¾åƒç©ºé—´å¯¹é½çš„å›¾åƒä»¥è¿›è¡Œé‡æ–°è¯†åˆ«ã€‚å…·ä½“è€Œè¨€ï¼ŒSTNReIDÂ [[84](#bib.bib84)]
    åˆ©ç”¨éƒ¨åˆ†å›¾åƒå’Œæ•´ä½“å›¾åƒçš„é«˜çº§CNNç‰¹å¾æ¥é¢„æµ‹å®ƒä»¬ä¹‹é—´çš„2Dä»¿å°„å˜æ¢å‚æ•°ï¼Œä»¥å˜æ¢æ•´ä½“å›¾åƒä»¥åŒ¹é…éƒ¨åˆ†å›¾åƒã€‚åœ¨STNReIDä¸­ï¼Œæ•´ä½“å›¾åƒè¢«éšæœºè£å‰ªä¸ºéƒ¨åˆ†å›¾åƒï¼Œä»¥å»ºç«‹2Dä»¿å°„å˜æ¢å‚æ•°é¢„æµ‹çš„è‡ªç›‘ç£è®­ç»ƒã€‚APNÂ [[64](#bib.bib64)]
    é¢„æµ‹éƒ¨åˆ†æ¢æµ‹å›¾åƒçš„è£å‰ªç±»å‹ï¼Œå¹¶è£å‰ªæ•´ä½“å›¾åº“å›¾åƒçš„å¯¹åº”åŒºåŸŸä»¥è¿›è¡Œäººå‘˜æ£€ç´¢ã€‚åœ¨APNä¸­ï¼Œè£å‰ªç±»å‹çš„é¢„æµ‹é€šè¿‡éšæœºè£å‰ªæ•´ä½“å›¾åƒä¸ºä¸åŒéƒ¨åˆ†ç±»å‹çš„è‡ªç›‘ç£è¿›è¡Œè®­ç»ƒã€‚
- en: IV-D Noisy Information
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D å™ªå£°ä¿¡æ¯
- en: 'The occlusion is inevitably included in the detected boxes of occluded pedestrians
    and therefore brings noisy information to person Re-ID (see Fig.Â [4](#S3.F4 "Figure
    4 â€£ III-B Evaluation Metrics â€£ III Datasets and Evaluations â€£ Deep Learning-based
    Occluded Person Re-identification: A Survey") (*c*)). This is especially problematic
    since the similar obstacles across different identities disturb the appearance-based
    similarity calculation, i.e., retrieval results easily contain negative images
    with similar obstaclesÂ [[85](#bib.bib85)]. Deep learning-based solutions for addressing
    the noisy information issue can be divided into auxiliary model for noiseÂ [[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48),
    [86](#bib.bib86), [52](#bib.bib52), [87](#bib.bib87), [55](#bib.bib55), [40](#bib.bib40)],
    additional supervision for noiseÂ [[56](#bib.bib56), [38](#bib.bib38), [81](#bib.bib81),
    [60](#bib.bib60), [88](#bib.bib88), [39](#bib.bib39)], and attention mechanism
    for noiseÂ [[16](#bib.bib16), [63](#bib.bib63), [89](#bib.bib89), [82](#bib.bib82),
    [90](#bib.bib90), [91](#bib.bib91), [13](#bib.bib13), [92](#bib.bib92), [93](#bib.bib93),
    [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97)].'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: é®æŒ¡ä¸å¯é¿å…åœ°è¢«åŒ…å«åœ¨é®æŒ¡è¡Œäººçš„æ£€æµ‹æ¡†ä¸­ï¼Œå› æ­¤ä¸ºè¡Œäººé‡è¯†åˆ«ï¼ˆè§å›¾[4](#S3.F4 "å›¾ 4 â€£ III-B è¯„ä¼°æŒ‡æ ‡ â€£ III æ•°æ®é›†ä¸è¯„ä¼° â€£
    åŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡è¡Œäººé‡è¯†åˆ«ï¼šç»¼è¿°") (*c*)ï¼‰å¸¦æ¥äº†å™ªå£°ä¿¡æ¯ã€‚è¿™å°¤å…¶æˆé—®é¢˜ï¼Œå› ä¸ºä¸åŒèº«ä»½çš„ç›¸ä¼¼éšœç¢ç‰©æ‰°ä¹±äº†åŸºäºå¤–è§‚çš„ç›¸ä¼¼æ€§è®¡ç®—ï¼Œå³æ£€ç´¢ç»“æœå®¹æ˜“åŒ…å«å…·æœ‰ç›¸ä¼¼éšœç¢ç‰©çš„è´Ÿé¢å›¾åƒ[[85](#bib.bib85)]ã€‚è§£å†³å™ªå£°ä¿¡æ¯é—®é¢˜çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å¯ä»¥åˆ†ä¸ºå™ªå£°è¾…åŠ©æ¨¡å‹[[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48),
    [86](#bib.bib86), [52](#bib.bib52), [87](#bib.bib87), [55](#bib.bib55), [40](#bib.bib40)]ã€é¢å¤–ç›‘ç£[[56](#bib.bib56),
    [38](#bib.bib38), [81](#bib.bib81), [60](#bib.bib60), [88](#bib.bib88), [39](#bib.bib39)]ï¼Œä»¥åŠå™ªå£°æ³¨æ„æœºåˆ¶[[16](#bib.bib16),
    [63](#bib.bib63), [89](#bib.bib89), [82](#bib.bib82), [90](#bib.bib90), [91](#bib.bib91),
    [13](#bib.bib13), [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95),
    [96](#bib.bib96), [97](#bib.bib97)]ã€‚
- en: IV-D1 Auxiliary Model for Noise
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D1 å™ªå£°è¾…åŠ©æ¨¡å‹
- en: The auxiliary model-based methods rely on external information provided by auxiliary
    models to help identify and suppress the noisy occlusion. According to the type
    of employed auxiliary models, these methods can be further divided into pose-basedÂ [[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48),
    [86](#bib.bib86), [52](#bib.bib52), [87](#bib.bib87)] and parsing-basedÂ [[55](#bib.bib55),
    [40](#bib.bib40)].
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè¾…åŠ©æ¨¡å‹çš„æ–¹æ³•ä¾èµ–äºè¾…åŠ©æ¨¡å‹æä¾›çš„å¤–éƒ¨ä¿¡æ¯ï¼Œä»¥å¸®åŠ©è¯†åˆ«å’ŒæŠ‘åˆ¶å™ªå£°é®æŒ¡ã€‚æ ¹æ®æ‰€ä½¿ç”¨çš„è¾…åŠ©æ¨¡å‹ç±»å‹ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥åˆ†ä¸ºåŸºäºå§¿æ€çš„[[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48),
    [86](#bib.bib86), [52](#bib.bib52), [87](#bib.bib87)]å’ŒåŸºäºè§£æçš„[[55](#bib.bib55),
    [40](#bib.bib40)]ã€‚
- en: The pose-based methodsÂ [[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46),
    [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48), [86](#bib.bib86), [52](#bib.bib52),
    [87](#bib.bib87)] exploits pose landmarks predicted by external pose estimation
    models to disentangle the useful information from the occlusion noise. Specifically,
    PGFAÂ [[36](#bib.bib36)], PDVMÂ [[45](#bib.bib45)], and PMFBÂ [[46](#bib.bib46)]
    preset a threshold to filter out occluded invisible pose landmarks based on their
    prediction confidence. The visible pose landmarks are used to generate heatmaps
    for extracting non-occluded pose features. These methods also construct different
    local part features on the CNN feature map and employ the confidence of pose estimation
    to determine whether the corresponding parts are occluded or not. Only the shared
    visible parts between probe and gallery images are used in these methods to compute
    the similarity, explicitly avoiding the disturbance from occlusion. Similarly,
    ACSAPÂ [[51](#bib.bib51)] utilizes the confidence of pose estimation to decide
    the visibility of horizontally partitioned parts and assigns the shared visible
    parts with larger weights in similarity metrics. KBFMÂ [[47](#bib.bib47)] builds
    rectangular regions based on the shared visible keypoints between probe and gallery
    images to extract features for measuring the similarity. Moreover, PMFB employs
    the pose embeddings generated from visible landmarks as gates to adaptively recalibrate
    channel-wise feature responses based on the visible body parts. Given part features
    extracted with external pose information, PVPMÂ [[48](#bib.bib48)] utilizes the
    characteristic of part correspondence between images of the same identity to mine
    correspondence scores as pseudo-labels for training a visibility predictor that
    estimates whether a part suffers from the occlusion. Considering that the provided
    external pose information may be sparse or noisy, LKWSÂ [[86](#bib.bib86)] proposes
    to discretize the pose information to obtain robust visibility label of horizontally
    divided body parts. Utilizing the label of visibility, LKWS trains a visibility
    discriminator to help suppress the influence of invisible horizontal parts. Specifically,
    the visibility of each horizontal part is voted by all keypoints within the part
    region based on their prediction confidence scores and the preset threshold. PFDÂ [[52](#bib.bib52)]
    assigns the keypoint heatmap of higher confidence score than the preset threshold
    with $label=1$ and the keypoint heatmap of lower confidence score with $label=0$.
    With the assigned labels, PFD divides the view feature set into high-confidence
    and low-confidence keypoint view feature sets. The high-confidence keypoint view
    features are employed in the inference stage to explicitly match visible body
    parts and automatically separate occlusion features. Moreover, PFD designs a Pose-guided
    push loss that encourages the difference between human body parts (i.e., the high-confidence
    features) and non-human body parts (i.e., the low-confidence features) to help
    focus on human body parts and alleviate the interference of occlusion. Differently,
    STALÂ [[87](#bib.bib87)] uses external pose landmarks to slice the video into multiple
    spatial-temporal units. To down-weight the possible occluded units, STAL designs
    a joint spatial-temporal attention module to evaluate the quality scores of each
    unit with carefully designed loss functions.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå§¿æ€çš„æ–¹æ³•[[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46), [51](#bib.bib51),
    [47](#bib.bib47), [48](#bib.bib48), [86](#bib.bib86), [52](#bib.bib52), [87](#bib.bib87)]
    åˆ©ç”¨å¤–éƒ¨å§¿æ€ä¼°è®¡æ¨¡å‹é¢„æµ‹çš„å§¿æ€æ ‡å¿—æ¥ä»é®æŒ¡å™ªå£°ä¸­åˆ†ç¦»å‡ºæœ‰ç”¨çš„ä¿¡æ¯ã€‚å…·ä½“è€Œè¨€ï¼ŒPGFAÂ [[36](#bib.bib36)]ã€PDVMÂ [[45](#bib.bib45)]
    å’Œ PMFBÂ [[46](#bib.bib46)] é¢„è®¾é˜ˆå€¼æ¥åŸºäºé¢„æµ‹ç½®ä¿¡åº¦è¿‡æ»¤æ‰è¢«é®æŒ¡çš„ä¸å¯è§å§¿æ€æ ‡å¿—ã€‚å¯è§çš„å§¿æ€æ ‡å¿—ç”¨äºç”Ÿæˆçƒ­å›¾ï¼Œä»¥æå–éé®æŒ¡çš„å§¿æ€ç‰¹å¾ã€‚è¿™äº›æ–¹æ³•è¿˜åœ¨
    CNN ç‰¹å¾å›¾ä¸Šæ„å»ºä¸åŒçš„å±€éƒ¨éƒ¨åˆ†ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å§¿æ€ä¼°è®¡çš„ç½®ä¿¡åº¦æ¥ç¡®å®šç›¸åº”éƒ¨åˆ†æ˜¯å¦è¢«é®æŒ¡ã€‚è¿™äº›æ–¹æ³•ä»…ä½¿ç”¨æ¢æµ‹å›¾åƒå’Œåº“å›¾åƒä¹‹é—´å…±äº«çš„å¯è§éƒ¨åˆ†æ¥è®¡ç®—ç›¸ä¼¼æ€§ï¼Œæ˜ç¡®é¿å…äº†é®æŒ¡å¸¦æ¥çš„å¹²æ‰°ã€‚ç±»ä¼¼åœ°ï¼ŒACSAPÂ [[51](#bib.bib51)]
    åˆ©ç”¨å§¿æ€ä¼°è®¡çš„ç½®ä¿¡åº¦æ¥å†³å®šæ°´å¹³åˆ’åˆ†éƒ¨åˆ†çš„å¯è§æ€§ï¼Œå¹¶åœ¨ç›¸ä¼¼æ€§åº¦é‡ä¸­ä¸ºå…±äº«çš„å¯è§éƒ¨åˆ†åˆ†é…æ›´å¤§çš„æƒé‡ã€‚KBFMÂ [[47](#bib.bib47)] åŸºäºæ¢æµ‹å›¾åƒå’Œåº“å›¾åƒä¹‹é—´å…±äº«çš„å¯è§å…³é”®ç‚¹æ„å»ºçŸ©å½¢åŒºåŸŸï¼Œä»¥æå–ç‰¹å¾æ¥æµ‹é‡ç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼ŒPMFB
    åˆ©ç”¨ä»å¯è§æ ‡å¿—ç”Ÿæˆçš„å§¿æ€åµŒå…¥ä½œä¸ºé—¨æ§ï¼Œè‡ªé€‚åº”åœ°é‡æ–°æ ¡å‡†åŸºäºå¯è§èº«ä½“éƒ¨ä½çš„é€šé“çº§ç‰¹å¾å“åº”ã€‚ç»™å®šé€šè¿‡å¤–éƒ¨å§¿æ€ä¿¡æ¯æå–çš„éƒ¨ä»¶ç‰¹å¾ï¼ŒPVPMÂ [[48](#bib.bib48)]
    åˆ©ç”¨ç›¸åŒèº«ä»½å›¾åƒä¹‹é—´éƒ¨ä»¶å¯¹åº”çš„ç‰¹æ€§ï¼ŒæŒ–æ˜å¯¹åº”åˆ†æ•°ä½œä¸ºä¼ªæ ‡ç­¾ï¼Œç”¨äºè®­ç»ƒä¸€ä¸ªå¯è§æ€§é¢„æµ‹å™¨ï¼Œä»¥ä¼°è®¡éƒ¨ä»¶æ˜¯å¦å—åˆ°é®æŒ¡ã€‚è€ƒè™‘åˆ°æä¾›çš„å¤–éƒ¨å§¿æ€ä¿¡æ¯å¯èƒ½æ˜¯ç¨€ç–æˆ–å˜ˆæ‚çš„ï¼ŒLKWSÂ [[86](#bib.bib86)]
    æå‡ºå°†å§¿æ€ä¿¡æ¯ç¦»æ•£åŒ–ï¼Œä»¥è·å¾—æ°´å¹³åˆ†å‰²èº«ä½“éƒ¨ä½çš„é²æ£’å¯è§æ€§æ ‡ç­¾ã€‚åˆ©ç”¨å¯è§æ€§æ ‡ç­¾ï¼ŒLKWS è®­ç»ƒä¸€ä¸ªå¯è§æ€§é‰´åˆ«å™¨ï¼Œä»¥å¸®åŠ©æŠ‘åˆ¶ä¸å¯è§æ°´å¹³éƒ¨åˆ†çš„å½±å“ã€‚å…·ä½“è€Œè¨€ï¼ŒåŸºäºé¢„æµ‹ç½®ä¿¡åº¦åˆ†æ•°å’Œé¢„è®¾é˜ˆå€¼ï¼Œæ‰€æœ‰å…³é”®ç‚¹å¯¹æ¯ä¸ªæ°´å¹³éƒ¨åˆ†çš„å¯è§æ€§è¿›è¡ŒæŠ•ç¥¨ã€‚PFDÂ [[52](#bib.bib52)]
    å°†ç½®ä¿¡åº¦åˆ†æ•°é«˜äºé¢„è®¾é˜ˆå€¼çš„å…³é”®ç‚¹çƒ­å›¾åˆ†é… $label=1$ï¼Œå°†ç½®ä¿¡åº¦åˆ†æ•°ä½äºé¢„è®¾é˜ˆå€¼çš„å…³é”®ç‚¹çƒ­å›¾åˆ†é… $label=0$ã€‚æ ¹æ®åˆ†é…çš„æ ‡ç­¾ï¼ŒPFD å°†è§†å›¾ç‰¹å¾é›†åˆ†ä¸ºé«˜ç½®ä¿¡åº¦å’Œä½ç½®ä¿¡åº¦çš„å…³é”®ç‚¹è§†å›¾ç‰¹å¾é›†ã€‚é«˜ç½®ä¿¡åº¦çš„å…³é”®ç‚¹è§†å›¾ç‰¹å¾åœ¨æ¨æ–­é˜¶æ®µç”¨äºæ˜¾å¼åŒ¹é…å¯è§èº«ä½“éƒ¨ä½ï¼Œå¹¶è‡ªåŠ¨åˆ†ç¦»é®æŒ¡ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒPFD
    è®¾è®¡äº†ä¸€ä¸ªå§¿æ€å¼•å¯¼çš„æ¨é€æŸå¤±ï¼Œé¼“åŠ±äººä½“éƒ¨ä½ï¼ˆå³é«˜ç½®ä¿¡åº¦ç‰¹å¾ï¼‰å’Œéäººä½“éƒ¨ä½ï¼ˆå³ä½ç½®ä¿¡åº¦ç‰¹å¾ï¼‰ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥å¸®åŠ©ä¸“æ³¨äºäººä½“éƒ¨ä½å¹¶ç¼“è§£é®æŒ¡çš„å¹²æ‰°ã€‚ä¸åŒçš„æ˜¯ï¼ŒSTALÂ [[87](#bib.bib87)]
    ä½¿ç”¨å¤–éƒ¨å§¿æ€æ ‡å¿—å°†è§†é¢‘åˆ‡å‰²æˆå¤šä¸ªæ—¶ç©ºå•å…ƒã€‚ä¸ºäº†é™ä½å¯èƒ½çš„é®æŒ¡å•å…ƒçš„æƒé‡ï¼ŒSTAL è®¾è®¡äº†ä¸€ä¸ªè”åˆæ—¶ç©ºæ³¨æ„åŠ›æ¨¡å—ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æŸå¤±å‡½æ•°æ¥è¯„ä¼°æ¯ä¸ªå•å…ƒçš„è´¨é‡åˆ†æ•°ã€‚
- en: The parsing-based methodsÂ [[55](#bib.bib55), [40](#bib.bib40)] employ parsing
    masks estimated by human parsing models to help suppress the noisy occlusion.
    Specifically, TSAÂ [[55](#bib.bib55)] utilizes external parsing results estimated
    by DensePoseÂ [[76](#bib.bib76)] to provide the visible signal to guide the learning
    of visible regions, suppressing the invisible occluded regions. Co-AttentionÂ [[40](#bib.bib40)]
    employs parsing masks as the query in the self-attention mechanism to perform
    image matching on associated regions for alleviating the noisy information brought
    by occlusion.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè§£æçš„æ–¹æ³•[[55](#bib.bib55), [40](#bib.bib40)]é‡‡ç”¨ç”±äººå·¥è§£ææ¨¡å‹ä¼°è®¡çš„è§£ææ©æ¨¡æ¥å¸®åŠ©æŠ‘åˆ¶å™ªå£°é®æŒ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒTSA[[55](#bib.bib55)]åˆ©ç”¨DensePose[[76](#bib.bib76)]ä¼°è®¡çš„å¤–éƒ¨è§£æç»“æœæä¾›å¯è§ä¿¡å·ï¼Œä»¥æŒ‡å¯¼å¯¹å¯è§åŒºåŸŸçš„å­¦ä¹ ï¼ŒæŠ‘åˆ¶ä¸å¯è§çš„é®æŒ¡åŒºåŸŸã€‚Co-Attention[[40](#bib.bib40)]å°†è§£ææ©æ¨¡ä½œä¸ºè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŸ¥è¯¢ï¼Œä»¥åœ¨ç›¸å…³åŒºåŸŸè¿›è¡Œå›¾åƒåŒ¹é…ï¼Œä»è€Œå‡è½»é®æŒ¡å¸¦æ¥çš„å™ªå£°ä¿¡æ¯ã€‚
- en: IV-D2 Additional Supervision for Noise
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D2 å™ªå£°çš„é¢å¤–ç›‘ç£
- en: The additional supervision-based methods employ extra information to guide the
    learning of suppressing the occlusion noise while being independent during inference.
    According to the type of the extra information employed, the additional supervision-based
    solutions can be further summarized into pose-basedÂ [[56](#bib.bib56), [38](#bib.bib38)],
    segmentation-basedÂ [[81](#bib.bib81), [60](#bib.bib60), [88](#bib.bib88)], attribute-basedÂ [[41](#bib.bib41)],
    and hybrid-basedÂ [[39](#bib.bib39)] methods.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: é¢å¤–ç›‘ç£åŸºæ–¹æ³•é€šè¿‡ä½¿ç”¨é¢å¤–ä¿¡æ¯æ¥æŒ‡å¯¼æŠ‘åˆ¶é®æŒ¡å™ªå£°çš„å­¦ä¹ ï¼ŒåŒæ—¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¿æŒç‹¬ç«‹ã€‚æ ¹æ®æ‰€ä½¿ç”¨çš„é¢å¤–ä¿¡æ¯çš„ç±»å‹ï¼Œé¢å¤–ç›‘ç£åŸºæ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æ€»ç»“ä¸ºåŸºäºå§¿æ€çš„[[56](#bib.bib56),
    [38](#bib.bib38)], åŸºäºåˆ†å‰²çš„[[81](#bib.bib81), [60](#bib.bib60), [88](#bib.bib88)],
    åŸºäºå±æ€§çš„[[41](#bib.bib41)]ï¼Œå’Œæ··åˆåŸºçš„[[39](#bib.bib39)]æ–¹æ³•ã€‚
- en: The pose-based methodsÂ [[56](#bib.bib56), [38](#bib.bib38)] utilize additional
    pose information to supervise the learning of excluding noisy occlusion. Specifically,
    AACNÂ [[56](#bib.bib56)] uses external pose information to supervise the part attention
    learning. Based on the intensities of each part attention map, AACN computes part
    visibility scores to measure the occlusion extent of each body part. Similarly,
    with the ground truth built from external pose information, DAReIDÂ [[38](#bib.bib38)]
    learns to predict upper and lower body masks to extract non-occluded part features.
    Moreover, DAReID regards the heatmap of upper or lower features with large high-activation
    areas as reliable. The significance of the reliable regions is enhanced to suppress
    the occlusion noise.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå§¿æ€çš„æ–¹æ³•[[56](#bib.bib56), [38](#bib.bib38)]åˆ©ç”¨é¢å¤–çš„å§¿æ€ä¿¡æ¯æ¥ç›‘ç£æ’é™¤å™ªå£°é®æŒ¡çš„å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼ŒAACN[[56](#bib.bib56)]ä½¿ç”¨å¤–éƒ¨å§¿æ€ä¿¡æ¯æ¥ç›‘ç£éƒ¨åˆ†æ³¨æ„åŠ›çš„å­¦ä¹ ã€‚æ ¹æ®æ¯ä¸ªéƒ¨åˆ†æ³¨æ„åŠ›å›¾çš„å¼ºåº¦ï¼ŒAACNè®¡ç®—éƒ¨åˆ†å¯è§æ€§åˆ†æ•°ä»¥è¡¡é‡æ¯ä¸ªèº«ä½“éƒ¨ä½çš„é®æŒ¡ç¨‹åº¦ã€‚ç±»ä¼¼åœ°ï¼Œåˆ©ç”¨ä»å¤–éƒ¨å§¿æ€ä¿¡æ¯æ„å»ºçš„çœŸå®æƒ…å†µï¼ŒDAReID[[38](#bib.bib38)]å­¦ä¹ é¢„æµ‹ä¸ŠåŠèº«å’Œä¸‹åŠèº«æ©æ¨¡ï¼Œä»¥æå–æœªè¢«é®æŒ¡çš„éƒ¨ä»¶ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒDAReIDå°†å…·æœ‰å¤§é¢ç§¯é«˜æ¿€æ´»åŒºåŸŸçš„ä¸ŠåŠèº«æˆ–ä¸‹åŠèº«ç‰¹å¾çƒ­å›¾è§†ä¸ºå¯é ã€‚é€šè¿‡å¢å¼ºå¯é åŒºåŸŸçš„æ˜¾è‘—æ€§æ¥æŠ‘åˆ¶é®æŒ¡å™ªå£°ã€‚
- en: The segmentation-based methodsÂ [[81](#bib.bib81), [60](#bib.bib60), [88](#bib.bib88)]
    utilize extra segmentation masks provided by a segmentation model, e.g., the human
    parsing model or the scene segmentation model, to help address the noisy information
    issue. Specifically, FPRÂ [[81](#bib.bib81)] employs the person mask obtained by
    CE2PÂ [[98](#bib.bib98)] to supervise the generation of foreground probability
    maps, encouraging the feature extraction to concentrate more on clean human body
    parts to refine the similarity computation with less contamination from occlusion.
    HPNetÂ [[60](#bib.bib60)] uses part labels provided by a COCO-trained human parsing
    model to learn human parsing and person re-identification in a multi-task manner.
    In HPNet, the predicted part probability maps are binarized with a threshold of
    0.5 to extract part-level features and determine the visibility of each part for
    alleviating the occlusion noise. SORNÂ [[88](#bib.bib88)] trains a semantic branch
    with pseudo-labels predicted by external semantic segmentation model to generate
    foreground-background masks for extracting features from non-occluded areas.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºåˆ†å‰²çš„æ–¹æ³•Â [[81](#bib.bib81), [60](#bib.bib60), [88](#bib.bib88)] åˆ©ç”¨ç”±åˆ†å‰²æ¨¡å‹ï¼ˆå¦‚äººç±»è§£ææ¨¡å‹æˆ–åœºæ™¯åˆ†å‰²æ¨¡å‹ï¼‰æä¾›çš„é¢å¤–åˆ†å‰²æ©ç ï¼Œä»¥å¸®åŠ©è§£å†³å™ªå£°ä¿¡æ¯é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒFPRÂ [[81](#bib.bib81)]
    ä½¿ç”¨ç”±CE2PÂ [[98](#bib.bib98)] è·å¾—çš„äººç‰©æ©ç æ¥ç›‘ç£å‰æ™¯æ¦‚ç‡å›¾çš„ç”Ÿæˆï¼Œé¼“åŠ±ç‰¹å¾æå–æ›´å¤šé›†ä¸­åœ¨å¹²å‡€çš„äººä½“éƒ¨ä½ï¼Œä»è€Œå‡å°‘é®æŒ¡å¸¦æ¥çš„æ±¡æŸ“ï¼Œç²¾ç‚¼ç›¸ä¼¼åº¦è®¡ç®—ã€‚HPNetÂ [[60](#bib.bib60)]
    ä½¿ç”¨ç”±COCOè®­ç»ƒçš„äººä½“è§£ææ¨¡å‹æä¾›çš„éƒ¨ä»¶æ ‡ç­¾ï¼Œä»¥å¤šä»»åŠ¡çš„æ–¹å¼å­¦ä¹ äººä½“è§£æå’Œäººç‰©å†è¯†åˆ«ã€‚åœ¨HPNetä¸­ï¼Œé¢„æµ‹çš„éƒ¨ä»¶æ¦‚ç‡å›¾é€šè¿‡0.5çš„é˜ˆå€¼äºŒå€¼åŒ–ï¼Œä»¥æå–éƒ¨ä»¶çº§ç‰¹å¾å¹¶ç¡®å®šæ¯ä¸ªéƒ¨ä»¶çš„å¯è§æ€§ï¼Œä»è€Œå‡è½»é®æŒ¡å™ªå£°ã€‚SORNÂ [[88](#bib.bib88)]
    è®­ç»ƒä¸€ä¸ªè¯­ä¹‰åˆ†æ”¯ï¼Œä½¿ç”¨å¤–éƒ¨è¯­ä¹‰åˆ†å‰²æ¨¡å‹é¢„æµ‹çš„ä¼ªæ ‡ç­¾ç”Ÿæˆå‰æ™¯-èƒŒæ™¯æ©ç ï¼Œä»éé®æŒ¡åŒºåŸŸæå–ç‰¹å¾ã€‚
- en: 'The attribute-based methods leverage semantic-level attribute annotations of
    person Re-ID datasets to help suppress the noisy information brought by occlusion
    (see Fig.Â [8](#S4.F8 "Figure 8 â€£ IV-C Position and Scale Misalignment â€£ IV Occluded
    Person Re-ID â€£ Deep Learning-based Occluded Person Re-identification: A Survey")
    (*b*)). ASANÂ [[41](#bib.bib41)] employs the attribute information to guide the
    learning of occlusion-sensitive segmentation in a weakly supervised manner to
    extract non-occluded human body features.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 'åŸºäºå±æ€§çš„æ–¹æ³•åˆ©ç”¨äººç‰©Re-IDæ•°æ®é›†çš„è¯­ä¹‰çº§å±æ€§æ³¨é‡Šï¼Œå¸®åŠ©æŠ‘åˆ¶ç”±é®æŒ¡å¸¦æ¥çš„å™ªå£°ä¿¡æ¯ï¼ˆè§å›¾Â [8](#S4.F8 "Figure 8 â€£ IV-C Position
    and Scale Misalignment â€£ IV Occluded Person Re-ID â€£ Deep Learning-based Occluded
    Person Re-identification: A Survey") (*b*)ï¼‰ã€‚ASANÂ [[41](#bib.bib41)] ä½¿ç”¨å±æ€§ä¿¡æ¯æ¥æŒ‡å¯¼å¼±ç›‘ç£ä¸‹çš„é®æŒ¡æ•æ„Ÿåˆ†å‰²å­¦ä¹ ï¼Œä»¥æå–æœªé®æŒ¡çš„äººä½“ç‰¹å¾ã€‚'
- en: The hybrid-based methods employ more than one type of external information to
    guide the learning process for alleviating the noisy information issue. Specifically,
    GASMÂ [[39](#bib.bib39)] trains a mask layer and a pose layer with the ground truth
    predicted by the semantic segmentation model PSPNetÂ [[99](#bib.bib99)] and the
    pose estimation model CenterNetÂ [[100](#bib.bib100)]. GASM then combines the mask
    heatmap predicted by the mask layer and the keypoint heatmaps estimated by the
    pose layer into a saliency map to extract salient features, explicitly excluding
    the noisy information brought by occlusion.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ··åˆçš„æ–¹æ³•é‡‡ç”¨å¤šç§å¤–éƒ¨ä¿¡æ¯æ¥æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹ï¼Œä»¥å‡è½»å™ªå£°ä¿¡æ¯é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒGASMÂ [[39](#bib.bib39)] è®­ç»ƒä¸€ä¸ªæ©ç å±‚å’Œä¸€ä¸ªå§¿æ€å±‚ï¼Œåˆ©ç”¨è¯­ä¹‰åˆ†å‰²æ¨¡å‹PSPNetÂ [[99](#bib.bib99)]
    å’Œå§¿æ€ä¼°è®¡æ¨¡å‹CenterNetÂ [[100](#bib.bib100)] é¢„æµ‹çš„çœŸå®æ ‡ç­¾ã€‚GASM ç„¶åå°†æ©ç å±‚é¢„æµ‹çš„æ©ç çƒ­å›¾å’Œå§¿æ€å±‚ä¼°è®¡çš„å…³é”®ç‚¹çƒ­å›¾ç»„åˆæˆä¸€ä¸ªæ˜¾è‘—æ€§å›¾ï¼Œä»¥æå–æ˜¾è‘—ç‰¹å¾ï¼Œæ˜ç¡®æ’é™¤ç”±é®æŒ¡å¸¦æ¥çš„å™ªå£°ä¿¡æ¯ã€‚
- en: IV-D3 Attention Mechanism for Noise
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D3 å™ªå£°çš„æ³¨æ„åŠ›æœºåˆ¶
- en: The attention mechanism-based methods learn to generate the attention maps that
    assign smaller weights to occluded regions to address the noisy information issue,
    without the requirement of any extra information. According to the main idea of
    the attention learning process, the attention mechanism-based solutions for noise
    can be further grouped into data augmentationÂ [[16](#bib.bib16), [63](#bib.bib63),
    [89](#bib.bib89), [82](#bib.bib82), [90](#bib.bib90), [91](#bib.bib91), [13](#bib.bib13),
    [92](#bib.bib92), [93](#bib.bib93)], query-guidedÂ [[94](#bib.bib94), [95](#bib.bib95)],
    drop-basedÂ [[96](#bib.bib96)], and relation-basedÂ [[97](#bib.bib97)] methods.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ–¹æ³•å­¦ä¹ ç”Ÿæˆæ³¨æ„åŠ›å›¾ï¼Œå°†è¾ƒå°çš„æƒé‡åˆ†é…ç»™é®æŒ¡åŒºåŸŸï¼Œä»¥è§£å†³å™ªå£°ä¿¡æ¯é—®é¢˜ï¼Œæ— éœ€ä»»ä½•é¢å¤–ä¿¡æ¯ã€‚æ ¹æ®æ³¨æ„åŠ›å­¦ä¹ è¿‡ç¨‹çš„ä¸»è¦æ€æƒ³ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å™ªå£°è§£å†³æ–¹æ¡ˆå¯ä»¥è¿›ä¸€æ­¥åˆ†ä¸ºæ•°æ®å¢å¼ºÂ [[16](#bib.bib16),
    [63](#bib.bib63), [89](#bib.bib89), [82](#bib.bib82), [90](#bib.bib90), [91](#bib.bib91),
    [13](#bib.bib13), [92](#bib.bib92), [93](#bib.bib93)], æŸ¥è¯¢å¼•å¯¼Â [[94](#bib.bib94),
    [95](#bib.bib95)], ä¸¢å¼ƒåŸºÂ [[96](#bib.bib96)] å’Œå…³ç³»åŸºÂ [[97](#bib.bib97)] æ–¹æ³•ã€‚
- en: The data augmentation methodsÂ [[16](#bib.bib16), [63](#bib.bib63), [89](#bib.bib89),
    [82](#bib.bib82), [90](#bib.bib90), [91](#bib.bib91), [13](#bib.bib13), [92](#bib.bib92),
    [93](#bib.bib93)] generate artificial occlusion to train the network to focus
    on clean body parts and exclude the noisy occlusion. Specifically, AFPBÂ [[16](#bib.bib16)]
    constructs an occlusion simulator where a random patch from the background of
    source images is used as an occlusion to cover a part of holistic persons. AFPB
    designs the multi-task losses that force the network to simultaneously identify
    the person and classify whether the sample is from the occluded data distribution.
    VPMÂ [[63](#bib.bib63)] pre-defines $m\times n$ rectangle regions on an image and
    randomly crops partial pedestrian images from the holistic ones where every pixel
    in partial images is assigned with the region label accordingly for self-supervision.
    In VPM, a region locator is designed to generate probability maps that infer the
    location of each region. Through the sum operation over each probability map,
    VPM obtains the region visibility scores to suppress the occluded noisy regions.
    REÂ [[89](#bib.bib89)] proposes the operation of random erasing, which randomly
    selects a rectangle region in an image and erases its pixels with random values,
    to generate images with various levels of occlusion for training the model to
    extract non-occluded discriminative identity information. APNetÂ [[82](#bib.bib82)]
    trains a part identifier with the self-supervision built from data augmentation
    to identify visible strip parts. The visible part features are then selected for
    similarity computation while the invisible part features on occluded or noisy
    regions are discarded. IGOASÂ [[90](#bib.bib90)] designs an incremental generative
    occlusion block that randomly generates a uniform occlusion mask from small to
    large on images in a batch, training the model more robust to occlusion through
    gradually learning harder occlusion. With the synthesized occlusion data and their
    corresponding occlusion masks, IGOAS focuses more on foreground information by
    suppressing the response of generated occlusion regions to zero. SSGRÂ [[91](#bib.bib91)]
    employs the random erasing and the batch-constant erasing, which equally divides
    images into horizontal strips and randomly erases the same strip in a sub-batch,
    to simulate occlusion for training the disentangled non-local (DNLÂ [[101](#bib.bib101)])
    attention network. OAMNÂ [[13](#bib.bib13)] designs a novel occlusion augmentation
    scheme that crops a rectangular patch of a randomly chosen training image and
    scales the patch onto four pre-defined locations of the target image, producing
    diverse and precisely labeled occlusion. With the supervision of labeled occlusion
    data, the OAMN learns to generate spatial attention maps which precisely capture
    body parts regardless of the occlusion. DRL-NetÂ [[92](#bib.bib92)] utilizes the
    obstacles appearing in the train set to synthesize more diverse and realistic
    occluded samples to guide the contrast feature learning for mitigating the interference
    of occlusion noises. Apart from this, DRL-Net designs a transformer that simultaneously
    maintains the ID-relevant and ID-irrelevant queries for disentangled representation
    learning. Differently, FEDÂ [[93](#bib.bib93)] considers the occlusion from not
    only the non-pedestrian obstacles but also the non-target pedestrians for augmentation.
    To simulate reasonable non-pedestrian occlusions, FED manually crops the patches
    of backgrounds and occlusion objects from training images, and pastes them on
    pedestrian images with carefully designed augmentation process. To synthesize
    non-target pedestrian occlusions, FED maintains a memory bank of the feature centers
    of different identities. The memory bank is then employed to search $K$-nearest
    features of different identities for the current pedestrian. Finally, FED diffuses
    the pedestrian representations with these memorized features and generates non-target
    pedestrian occlusions at the feature level for training.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¢å¼ºæ–¹æ³•[[16](#bib.bib16), [63](#bib.bib63), [89](#bib.bib89), [82](#bib.bib82),
    [90](#bib.bib90), [91](#bib.bib91), [13](#bib.bib13), [92](#bib.bib92), [93](#bib.bib93)]
    ç”Ÿæˆäººå·¥é®æŒ¡ï¼Œä»¥è®­ç»ƒç½‘ç»œå…³æ³¨å¹²å‡€çš„èº«ä½“éƒ¨ä½å¹¶æ’é™¤å™ªå£°é®æŒ¡ã€‚å…·ä½“è€Œè¨€ï¼ŒAFPB [[16](#bib.bib16)] æ„å»ºäº†ä¸€ä¸ªé®æŒ¡æ¨¡æ‹Ÿå™¨ï¼Œå…¶ä¸­æºå›¾åƒçš„èƒŒæ™¯éšæœºåŒºåŸŸä½œä¸ºé®æŒ¡ï¼Œè¦†ç›–æ•´ä½“äººç‰©çš„ä¸€éƒ¨åˆ†ã€‚AFPB
    è®¾è®¡äº†å¤šä»»åŠ¡æŸå¤±ï¼Œè¿«ä½¿ç½‘ç»œåŒæ—¶è¯†åˆ«äººç‰©å¹¶åˆ†ç±»æ ·æœ¬æ˜¯å¦æ¥è‡ªé®æŒ¡æ•°æ®åˆ†å¸ƒã€‚VPM [[63](#bib.bib63)] åœ¨å›¾åƒä¸Šé¢„å®šä¹‰ $m\times n$
    çŸ©å½¢åŒºåŸŸï¼Œå¹¶éšæœºä»æ•´ä½“å›¾åƒä¸­è£å‰ªéƒ¨åˆ†è¡Œäººå›¾åƒï¼Œæ¯ä¸ªéƒ¨åˆ†å›¾åƒçš„æ¯ä¸ªåƒç´ éƒ½ç›¸åº”åˆ†é…äº†åŒºåŸŸæ ‡ç­¾ä»¥è¿›è¡Œè‡ªæˆ‘ç›‘ç£ã€‚åœ¨ VPM ä¸­ï¼Œè®¾è®¡äº†ä¸€ä¸ªåŒºåŸŸå®šä½å™¨æ¥ç”Ÿæˆæ¦‚ç‡å›¾ï¼Œæ¨æ–­æ¯ä¸ªåŒºåŸŸçš„ä½ç½®ã€‚é€šè¿‡å¯¹æ¯ä¸ªæ¦‚ç‡å›¾è¿›è¡Œæ±‚å’Œæ“ä½œï¼ŒVPM
    è·å¾—äº†åŒºåŸŸå¯è§æ€§è¯„åˆ†ï¼Œä»¥æŠ‘åˆ¶é®æŒ¡çš„å™ªå£°åŒºåŸŸã€‚RE [[89](#bib.bib89)] æå‡ºäº†éšæœºæ“¦é™¤æ“ä½œï¼Œå³éšæœºé€‰æ‹©å›¾åƒä¸­çš„ä¸€ä¸ªçŸ©å½¢åŒºåŸŸï¼Œå¹¶ç”¨éšæœºå€¼æ“¦é™¤å…¶åƒç´ ï¼Œä»¥ç”Ÿæˆå…·æœ‰ä¸åŒé®æŒ¡çº§åˆ«çš„å›¾åƒï¼Œä»¥è®­ç»ƒæ¨¡å‹æå–æœªé®æŒ¡çš„åˆ¤åˆ«æ€§èº«ä»½ä¿¡æ¯ã€‚APNet
    [[82](#bib.bib82)] è®­ç»ƒä¸€ä¸ªéƒ¨åˆ†è¯†åˆ«å™¨ï¼Œé€šè¿‡æ•°æ®å¢å¼ºæ„å»ºçš„è‡ªæˆ‘ç›‘ç£æ¥è¯†åˆ«å¯è§çš„æ¡å½¢éƒ¨ä»¶ã€‚ç„¶åé€‰æ‹©å¯è§éƒ¨ä»¶ç‰¹å¾è¿›è¡Œç›¸ä¼¼æ€§è®¡ç®—ï¼Œè€Œé®æŒ¡æˆ–å™ªå£°åŒºåŸŸä¸Šçš„ä¸å¯è§éƒ¨ä»¶ç‰¹å¾åˆ™è¢«ä¸¢å¼ƒã€‚IGOAS
    [[90](#bib.bib90)] è®¾è®¡äº†ä¸€ä¸ªå¢é‡ç”Ÿæˆé®æŒ¡å—ï¼Œä»å°åˆ°å¤§éšæœºç”Ÿæˆå‡åŒ€çš„é®æŒ¡æ©ç ï¼Œå¹¶åœ¨ä¸€æ‰¹å›¾åƒä¸Šè®­ç»ƒæ¨¡å‹æ›´å¼ºå¥åœ°åº”å¯¹é®æŒ¡ï¼Œé€šè¿‡é€æ­¥å­¦ä¹ æ›´éš¾çš„é®æŒ¡ã€‚é€šè¿‡åˆæˆçš„é®æŒ¡æ•°æ®åŠå…¶ç›¸åº”çš„é®æŒ¡æ©ç ï¼ŒIGOAS
    é€šè¿‡å°†ç”Ÿæˆçš„é®æŒ¡åŒºåŸŸçš„å“åº”æŠ‘åˆ¶ä¸ºé›¶ï¼Œæ›´åŠ å…³æ³¨å‰æ™¯ä¿¡æ¯ã€‚SSGR [[91](#bib.bib91)] é‡‡ç”¨éšæœºæ“¦é™¤å’Œæ‰¹æ¬¡æ’å®šæ“¦é™¤ï¼Œå°†å›¾åƒå‡åˆ†ä¸ºæ°´å¹³æ¡å¸¦ï¼Œå¹¶éšæœºæ“¦é™¤å­æ‰¹æ¬¡ä¸­çš„ç›¸åŒæ¡å¸¦ï¼Œä»¥æ¨¡æ‹Ÿé®æŒ¡ï¼Œè®­ç»ƒè§£è€¦çš„éæœ¬åœ°ï¼ˆDNL
    [[101](#bib.bib101)]) æ³¨æ„åŠ›ç½‘ç»œã€‚OAMN [[13](#bib.bib13)] è®¾è®¡äº†ä¸€ç§æ–°å‹çš„é®æŒ¡å¢å¼ºæ–¹æ¡ˆï¼Œè£å‰ªéšæœºé€‰æ‹©çš„è®­ç»ƒå›¾åƒçš„çŸ©å½¢è¡¥ä¸ï¼Œå¹¶å°†è¡¥ä¸ç¼©æ”¾åˆ°ç›®æ ‡å›¾åƒçš„å››ä¸ªé¢„å®šä¹‰ä½ç½®ï¼Œäº§ç”Ÿå¤šæ ·ä¸”ç²¾ç¡®æ ‡è®°çš„é®æŒ¡ã€‚åœ¨æ ‡è®°çš„é®æŒ¡æ•°æ®ç›‘ç£ä¸‹ï¼ŒOAMN
    å­¦ä¹ ç”Ÿæˆç©ºé—´æ³¨æ„å›¾ï¼Œè¿™äº›å›¾ç²¾ç¡®æ•æ‰èº«ä½“éƒ¨ä½ï¼Œæ— è®ºé®æŒ¡æƒ…å†µå¦‚ä½•ã€‚DRL-Net [[92](#bib.bib92)] åˆ©ç”¨è®­ç»ƒé›†ä¸­å‡ºç°çš„éšœç¢ç‰©åˆæˆæ›´å¤šæ ·åŒ–å’ŒçœŸå®çš„é®æŒ¡æ ·æœ¬ï¼Œä»¥å¼•å¯¼å¯¹æ¯”ç‰¹å¾å­¦ä¹ ï¼Œä»¥å‡è½»é®æŒ¡å™ªå£°çš„å¹²æ‰°ã€‚æ­¤å¤–ï¼ŒDRL-Net
    è®¾è®¡äº†ä¸€ä¸ªå˜å‹å™¨ï¼ŒåŒæ—¶ç»´æŠ¤ä¸ ID ç›¸å…³å’Œä¸ ID æ— å…³çš„æŸ¥è¯¢ï¼Œä»¥è¿›è¡Œè§£è€¦è¡¨ç¤ºå­¦ä¹ ã€‚ä¸åŒçš„æ˜¯ï¼ŒFED [[93](#bib.bib93)] åœ¨å¢å¼ºæ—¶ä¸ä»…è€ƒè™‘éè¡Œäººéšœç¢ç‰©ï¼Œè¿˜è€ƒè™‘éç›®æ ‡è¡Œäººã€‚ä¸ºäº†æ¨¡æ‹Ÿåˆç†çš„éè¡Œäººé®æŒ¡ï¼ŒFED
    æ‰‹åŠ¨è£å‰ªè®­ç»ƒå›¾åƒä¸­çš„èƒŒæ™¯å’Œé®æŒ¡ç‰©ä½“è¡¥ä¸ï¼Œå¹¶å°†å…¶ç²˜è´´åˆ°è¡Œäººå›¾åƒä¸Šï¼Œç»è¿‡ç²¾å¿ƒè®¾è®¡çš„å¢å¼ºè¿‡ç¨‹ã€‚ä¸ºäº†åˆæˆéç›®æ ‡è¡Œäººé®æŒ¡ï¼ŒFED ç»´æŠ¤ä¸åŒèº«ä»½ç‰¹å¾ä¸­å¿ƒçš„è®°å¿†åº“ã€‚ç„¶ååˆ©ç”¨è®°å¿†åº“æœç´¢å½“å‰è¡Œäººçš„
    $K$ ä¸ªæœ€è¿‘ç‰¹å¾ã€‚æœ€åï¼ŒFED ä½¿ç”¨è¿™äº›è®°å¿†ç‰¹å¾æ‰©æ•£è¡Œäººè¡¨ç¤ºï¼Œå¹¶åœ¨ç‰¹å¾çº§åˆ«ç”Ÿæˆéç›®æ ‡è¡Œäººé®æŒ¡è¿›è¡Œè®­ç»ƒã€‚
- en: The query-guided methodsÂ [[94](#bib.bib94), [95](#bib.bib95)] aims to generate
    consistent attentions between query and gallery images for addressing the noisy
    information issue. Specifically, CASNÂ [[94](#bib.bib94)] designs an attention-driven
    siamese learning architecture which enforces the attention consistency among images
    of the same identity to deal with viewpoint variations, occlusion, and background
    clutter. PISNetÂ [[95](#bib.bib95)] focuses on the crowded occlusion in which the
    detected bounding boxes may involve multiple people and include distractive information.
    Under the guidance of the query image (single-person), PISNet calculates the inner
    product of the query and gallery features to formulate the pixel-wise query-guided
    attention to enhance the feature of the target in the gallery image (multi-person).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥è¯¢å¼•å¯¼çš„æ–¹æ³•Â [[94](#bib.bib94), [95](#bib.bib95)] æ—¨åœ¨ç”ŸæˆæŸ¥è¯¢å›¾åƒä¸å›¾åº“å›¾åƒä¹‹é—´çš„ä¸€è‡´æ³¨æ„åŠ›ï¼Œä»¥è§£å†³å™ªå£°ä¿¡æ¯é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒCASNÂ [[94](#bib.bib94)]
    è®¾è®¡äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„å­ªç”Ÿå­¦ä¹ æ¶æ„ï¼Œè¯¥æ¶æ„å¼ºåˆ¶åŒä¸€èº«ä»½çš„å›¾åƒä¹‹é—´ä¿æŒæ³¨æ„åŠ›ä¸€è‡´ï¼Œä»¥å¤„ç†è§†è§’å˜åŒ–ã€é®æŒ¡å’ŒèƒŒæ™¯æ‚ä¹±çš„é—®é¢˜ã€‚PISNetÂ [[95](#bib.bib95)]
    ä¸“æ³¨äºæ‹¥æŒ¤é®æŒ¡çš„æƒ…å†µï¼Œå…¶ä¸­æ£€æµ‹åˆ°çš„è¾¹ç•Œæ¡†å¯èƒ½æ¶‰åŠå¤šä¸ªäººå¹¶åŒ…å«å¹²æ‰°ä¿¡æ¯ã€‚åœ¨æŸ¥è¯¢å›¾åƒï¼ˆå•äººï¼‰çš„æŒ‡å¯¼ä¸‹ï¼ŒPISNet è®¡ç®—æŸ¥è¯¢å’Œå›¾åº“ç‰¹å¾çš„å†…ç§¯ï¼Œä»¥åˆ¶å®šåƒç´ çº§æŸ¥è¯¢å¼•å¯¼çš„æ³¨æ„åŠ›ï¼Œä»è€Œå¢å¼ºå›¾åº“å›¾åƒï¼ˆå¤šäººï¼‰ä¸­ç›®æ ‡çš„ç‰¹å¾ã€‚
- en: The drop-based methods develop the training strategy based on dropping to guide
    the network to learn a more robust representation, alleviating the noisy information
    brought by occlusion. Specifically, CBDB-NetÂ [[96](#bib.bib96)] uniformly partitions
    the feature map into strips and continuously drop each strip from top to bottom.
    Trained with drop-based incomplete features, the model is forced to learn a more
    robust person descriptor for re-identification.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºä¸¢å¼ƒçš„æ–¹æ³•å¼€å‘äº†åŸºäºä¸¢å¼ƒçš„è®­ç»ƒç­–ç•¥ï¼Œä»¥å¼•å¯¼ç½‘ç»œå­¦ä¹ æ›´é²æ£’çš„è¡¨ç¤ºï¼Œä»è€Œå‡è½»é®æŒ¡å¸¦æ¥çš„å™ªå£°ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼ŒCBDB-NetÂ [[96](#bib.bib96)]
    å°†ç‰¹å¾å›¾å‡åŒ€åˆ’åˆ†ä¸ºæ¡å¸¦ï¼Œå¹¶ä»ä¸Šåˆ°ä¸‹è¿ç»­ä¸¢å¼ƒæ¯ä¸ªæ¡å¸¦ã€‚é€šè¿‡åŸºäºä¸¢å¼ƒçš„ä¸å®Œæ•´ç‰¹å¾è¿›è¡Œè®­ç»ƒï¼Œæ¨¡å‹è¢«è¿«å­¦ä¹ æ›´é²æ£’çš„äººç‰©æè¿°ç¬¦ä»¥ç”¨äºé‡æ–°è¯†åˆ«ã€‚
- en: The relation-based methods mine the relation among different regions to refine
    features, alleviating the interference of occlusion. Specifically, OCNetÂ [[97](#bib.bib97)]
    predefines the global region, top region (i.e., $1/2$ top horizontal strip), bottom
    region (i.e., $1/2$ bottom horizontal strip), and center region (i.e., $1/3$ center
    vertical strip) on an image, and extracts four region features through the group
    convolution and the carefully designed attention mechanism. In OCNet, the relational
    adaptive module consisting of two fully connected shared layers is proposed to
    capture the relation between different region features. The relational weights
    are then used to refine region features to suppress the occluded or insignificant
    information.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºå…³ç³»çš„æ–¹æ³•æŒ–æ˜ä¸åŒåŒºåŸŸä¹‹é—´çš„å…³ç³»ä»¥ç»†åŒ–ç‰¹å¾ï¼Œä»è€Œå‡è½»é®æŒ¡çš„å¹²æ‰°ã€‚å…·ä½“æ¥è¯´ï¼ŒOCNetÂ [[97](#bib.bib97)] åœ¨å›¾åƒä¸Šé¢„å®šä¹‰äº†å…¨å±€åŒºåŸŸã€ä¸Šéƒ¨åŒºåŸŸï¼ˆå³
    $1/2$ ä¸Šéƒ¨æ°´å¹³æ¡å¸¦ï¼‰ã€ä¸‹éƒ¨åŒºåŸŸï¼ˆå³ $1/2$ ä¸‹éƒ¨æ°´å¹³æ¡å¸¦ï¼‰å’Œä¸­å¿ƒåŒºåŸŸï¼ˆå³ $1/3$ ä¸­éƒ¨å‚ç›´æ¡å¸¦ï¼‰ï¼Œå¹¶é€šè¿‡åˆ†ç»„å·ç§¯å’Œç²¾å¿ƒè®¾è®¡çš„æ³¨æ„åŠ›æœºåˆ¶æå–å››ä¸ªåŒºåŸŸç‰¹å¾ã€‚åœ¨
    OCNet ä¸­ï¼Œæå‡ºäº†ç”±ä¸¤ä¸ªå…¨è¿æ¥å…±äº«å±‚ç»„æˆçš„å…³ç³»è‡ªé€‚åº”æ¨¡å—ï¼Œä»¥æ•æ‰ä¸åŒåŒºåŸŸç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚ç„¶åä½¿ç”¨å…³ç³»æƒé‡ç»†åŒ–åŒºåŸŸç‰¹å¾ï¼Œä»¥æŠ‘åˆ¶é®æŒ¡æˆ–æ— å…³çš„ä¿¡æ¯ã€‚
- en: IV-E Missing Information
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E ç¼ºå¤±ä¿¡æ¯
- en: 'The lack of identity information in occluded regions results in the missing
    information issue for person Re-ID (see Fig.Â [2](#S1.F2 "Figure 2 â€£ I Introduction
    â€£ Deep Learning-based Occluded Person Re-identification: A Survey") (*d*)). There
    are mainly two ways to recover the missing information in occluded regions: spatial
    recoveryÂ [[102](#bib.bib102), [14](#bib.bib14)] and temporal recoveryÂ [[103](#bib.bib103),
    [102](#bib.bib102), [14](#bib.bib14)].'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 'é®æŒ¡åŒºåŸŸä¸­ç¼ºä¹èº«ä»½ä¿¡æ¯ä¼šå¯¼è‡´ç¼ºå¤±ä¿¡æ¯çš„é—®é¢˜ï¼ˆè§å›¾Â [2](#S1.F2 "Figure 2 â€£ I Introduction â€£ Deep Learning-based
    Occluded Person Re-identification: A Survey") (*d*)ï¼‰ã€‚åœ¨é®æŒ¡åŒºåŸŸä¸­æ¢å¤ç¼ºå¤±ä¿¡æ¯ä¸»è¦æœ‰ä¸¤ç§æ–¹å¼ï¼šç©ºé—´æ¢å¤Â [[102](#bib.bib102),
    [14](#bib.bib14)] å’Œæ—¶é—´æ¢å¤Â [[103](#bib.bib103), [102](#bib.bib102), [14](#bib.bib14)]ã€‚'
- en: IV-E1 Spatial Recovery
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-E1 ç©ºé—´æ¢å¤
- en: The spatial recoveryÂ [[102](#bib.bib102), [14](#bib.bib14)] utilizes the spatial
    structure of a pedestrian image to infer the missing information. Specifically,
    VRSTCÂ [[102](#bib.bib102)] designs an auto-encoder which takes a image masked
    with white pixels as input and generates the contents for the occluded white region.
    To improve the quality of generated contents of the occluded parts, VRSTC adopts
    a local and a global discriminator to adversarially judge the reality and the
    contextual consistency of the synthesized contents. RFCNetÂ [[14](#bib.bib14)]
    exploits the long-range spatial contexts from non-occluded regions to predict
    the features of occluded regions, recovering the missing information at the feature
    level. Specifically, RFCNet estimates four keypoints to divide the feature map
    into 6 regions. In RFCNet, the encoder-decoder architecture is adopted, in which
    the encoder models the correlation between regions through clustering and the
    decoder utilizes the spatial correlation to recover occluded region features.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ç©ºé—´æ¢å¤ [[102](#bib.bib102), [14](#bib.bib14)] åˆ©ç”¨è¡Œäººå›¾åƒçš„ç©ºé—´ç»“æ„æ¥æ¨æ–­ç¼ºå¤±çš„ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼ŒVRSTC [[102](#bib.bib102)]
    è®¾è®¡äº†ä¸€ä¸ªè‡ªç¼–ç å™¨ï¼Œè¯¥è‡ªç¼–ç å™¨ä»¥è¢«ç™½è‰²åƒç´ é®æŒ¡çš„å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆé®æŒ¡ç™½è‰²åŒºåŸŸçš„å†…å®¹ã€‚ä¸ºäº†æé«˜é®æŒ¡éƒ¨åˆ†ç”Ÿæˆå†…å®¹çš„è´¨é‡ï¼ŒVRSTC é‡‡ç”¨äº†å±€éƒ¨å’Œå…¨å±€é‰´åˆ«å™¨æ¥å¯¹åˆæˆå†…å®¹çš„çœŸå®æ€§å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§è¿›è¡Œå¯¹æŠ—æ€§åˆ¤æ–­ã€‚RFCNet
    [[14](#bib.bib14)] åˆ©ç”¨éé®æŒ¡åŒºåŸŸçš„é•¿èŒƒå›´ç©ºé—´ä¸Šä¸‹æ–‡æ¥é¢„æµ‹é®æŒ¡åŒºåŸŸçš„ç‰¹å¾ï¼Œåœ¨ç‰¹å¾çº§åˆ«æ¢å¤ç¼ºå¤±çš„ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼ŒRFCNet ä¼°è®¡å››ä¸ªå…³é”®ç‚¹ï¼Œå°†ç‰¹å¾å›¾åˆ’åˆ†ä¸º
    6 ä¸ªåŒºåŸŸã€‚åœ¨ RFCNet ä¸­ï¼Œé‡‡ç”¨äº†ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå…¶ä¸­ç¼–ç å™¨é€šè¿‡èšç±»å»ºæ¨¡åŒºåŸŸé—´çš„ç›¸å…³æ€§ï¼Œè€Œè§£ç å™¨åˆ©ç”¨ç©ºé—´ç›¸å…³æ€§æ¥æ¢å¤é®æŒ¡åŒºåŸŸç‰¹å¾ã€‚
- en: IV-E2 Temporal Recovery
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-E2 æ—¶é—´æ¢å¤
- en: The temporal recoveryÂ [[103](#bib.bib103), [102](#bib.bib102), [14](#bib.bib14)]
    requires the continuous sequence of images (i.e., video-based person Re-ID) and
    utilizes the temporal information to recover the missing part in occluded regions.
    Assuming that the information at the same position in other frames can help recover
    the lost information in the current frame, the Refining Recurrent Unit (RRU) inÂ [[103](#bib.bib103)]
    is designed to remove noise and recover missing activation regions by implicitly
    referring the appearance and motion information extracted from historical frames.
    VRSTCÂ [[102](#bib.bib102)] proposes a differentiable temporal attention layer
    which employs the cosine similarity to determine where to attend from adjacent
    frames for recovering the contents of the occluded parts. RFCNetÂ [[14](#bib.bib14)]
    employs a query-memory attention mechanism in which the current region is regarded
    as the query and the corresponding regions of remaining frames serve as the memory.
    The dot-product similarityÂ [[104](#bib.bib104)] between the query and each item
    of the memory is employed for re-weighting all items in the memory to obtain the
    long-term temporal contexts to refine the query.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´æ¢å¤ [[103](#bib.bib103), [102](#bib.bib102), [14](#bib.bib14)] éœ€è¦è¿ç»­çš„å›¾åƒåºåˆ—ï¼ˆå³åŸºäºè§†é¢‘çš„äººç‰©é‡è¯†åˆ«ï¼‰å¹¶åˆ©ç”¨æ—¶é—´ä¿¡æ¯æ¢å¤é®æŒ¡åŒºåŸŸçš„ç¼ºå¤±éƒ¨åˆ†ã€‚å‡è®¾å…¶ä»–å¸§ç›¸åŒä½ç½®çš„ä¿¡æ¯å¯ä»¥å¸®åŠ©æ¢å¤å½“å‰å¸§ä¸­çš„ä¸¢å¤±ä¿¡æ¯ï¼Œ[[103](#bib.bib103)]
    ä¸­çš„ Refining Recurrent Unit (RRU) è¢«è®¾è®¡ç”¨æ¥é€šè¿‡éšå¼å‚è€ƒä»å†å²å¸§æå–çš„å¤–è§‚å’Œè¿åŠ¨ä¿¡æ¯æ¥å»é™¤å™ªå£°å¹¶æ¢å¤ç¼ºå¤±çš„æ¿€æ´»åŒºåŸŸã€‚VRSTC
    [[102](#bib.bib102)] æå‡ºäº†ä¸€ä¸ªå¯å¾®åˆ†çš„æ—¶é—´æ³¨æ„åŠ›å±‚ï¼Œè¯¥å±‚åˆ©ç”¨ä½™å¼¦ç›¸ä¼¼æ€§æ¥ç¡®å®šä»ç›¸é‚»å¸§ä¸­å…³æ³¨çš„ä½ç½®ï¼Œä»¥æ¢å¤é®æŒ¡éƒ¨åˆ†çš„å†…å®¹ã€‚RFCNet
    [[14](#bib.bib14)] é‡‡ç”¨äº†æŸ¥è¯¢-è®°å¿†æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­å½“å‰åŒºåŸŸè¢«è§†ä¸ºæŸ¥è¯¢ï¼Œå‰©ä½™å¸§çš„ç›¸åº”åŒºåŸŸä½œä¸ºè®°å¿†ã€‚é€šè¿‡æŸ¥è¯¢ä¸è®°å¿†ä¸­æ¯é¡¹çš„ç‚¹ç§¯ç›¸ä¼¼æ€§ [[104](#bib.bib104)]
    å¯¹æ‰€æœ‰è®°å¿†é¡¹è¿›è¡Œé‡æ–°åŠ æƒï¼Œä»¥è·å¾—é•¿æœŸæ—¶é—´ä¸Šä¸‹æ–‡ï¼Œä»è€Œä¼˜åŒ–æŸ¥è¯¢ã€‚
- en: 'TABLE III: Performance Comparison on Partial-reid and Partial-ilids. S-ST is
    Short for Sing-ShoT, which Denotes that Each Identity Only Contains One Gallery
    Image in the Inference Stage.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ III: éƒ¨åˆ†é‡è¯†åˆ«å’Œéƒ¨åˆ†iLIDSçš„æ€§èƒ½æ¯”è¾ƒã€‚S-ST æ˜¯ Single-ShoT çš„ç¼©å†™ï¼Œè¡¨ç¤ºæ¯ä¸ªèº«ä»½åœ¨æ¨ç†é˜¶æ®µä»…åŒ…å«ä¸€ä¸ªå›¾åº“å›¾åƒã€‚'
- en: '| Issues | Technical Routes | Methods | Publications | Partial-REID | Partial-iLIDS
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| é—®é¢˜ | æŠ€æœ¯è·¯çº¿ | æ–¹æ³• | å‘è¡¨è®ºæ–‡ | éƒ¨åˆ†-REID | éƒ¨åˆ†-iLIDS |'
- en: '| Rank-1 | Rank-3 | S-ST | Rank-1 | Rank-3 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Rank-1 | Rank-3 | S-ST | Rank-1 | Rank-3 |'
- en: '| Position Misalignment | Matching | AMC+SWMÂ [[15](#bib.bib15)] | ICCV2015
    | 37.3 | 46.0 | âœ“ | 21.0 | 32.8 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Position Misalignment | Matching | AMC+SWM [[15](#bib.bib15)] | ICCV2015
    | 37.3 | 46.0 | âœ“ | 21.0 | 32.8 |'
- en: '| DSRÂ [[30](#bib.bib30)] | CVPR2018 | 50.7 | 70.0 | âœ“ | 58.8 | 67.2 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| DSR [[30](#bib.bib30)] | CVPR2018 | 50.7 | 70.0 | âœ“ | 58.8 | 67.2 |'
- en: '| DAReIDÂ [[38](#bib.bib38)] | KBS2021 | 68.1 | 79.5 | Ã— | 76.7 | 85.3 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| DAReID [[38](#bib.bib38)] | KBS2021 | 68.1 | 79.5 | Ã— | 76.7 | 85.3 |'
- en: '| APNÂ [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | Ã— | 66.4 | 76.5 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | Ã— | 66.4 | 76.5 |'
- en: '| Co-AttentionÂ [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | Ã— | 73.1 | 83.2
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Co-Attention [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | Ã— | 73.1 | 83.2
    |'
- en: '| ASANÂ [[41](#bib.bib41)] | TCSVT2021 | 86.8 | 93.5 | Ã— | 81.7 | 88.3 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| ASANÂ [[41](#bib.bib41)] | TCSVT2021 | 86.8 | 93.5 | Ã— | 81.7 | 88.3 |'
- en: '| Auxiliary Model for Position | PDVMÂ [[45](#bib.bib45)] | PRL2020 | 43.3 |
    - | Ã— | - | - |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| ä½ç½®çš„è¾…åŠ©æ¨¡å‹ | PDVMÂ [[45](#bib.bib45)] | PRL2020 | 43.3 | - | Ã— | - | - |'
- en: '| PGFAÂ [[36](#bib.bib36)] | ICCV2019 | 68.0 | 80.0 | âœ“ | 69.1 | 80.9 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| PGFAÂ [[36](#bib.bib36)] | ICCV2019 | 68.0 | 80.0 | âœ“ | 69.1 | 80.9 |'
- en: '| KBFMÂ [[47](#bib.bib47)] | ICIP2020 | 69.7 | 82.2 | Ã— | 64.1 | 73.9 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| KBFMÂ [[47](#bib.bib47)] | ICIP2020 | 69.7 | 82.2 | Ã— | 64.1 | 73.9 |'
- en: '| PMFBÂ [[46](#bib.bib46)] | TNNLS2021 | 72.5 | 83.0 | Ã— | 70.6 | 81.3 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| PMFBÂ [[46](#bib.bib46)] | TNNLS2021 | 72.5 | 83.0 | Ã— | 70.6 | 81.3 |'
- en: '| TSAÂ [[55](#bib.bib55)] | ACM MM2020 | 72.7 | 85.2 | âœ“ | 73.9 | 84.7 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| TSAÂ [[55](#bib.bib55)] | ACM MM2020 | 72.7 | 85.2 | âœ“ | 73.9 | 84.7 |'
- en: '| ACSAPÂ [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | Ã— | 76.5 | 87.4 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| ACSAPÂ [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | Ã— | 76.5 | 87.4 |'
- en: '| PVPMÂ [[48](#bib.bib48)] | CVPR2020 | 78.3 | - | âœ“ | - | - |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| PVPMÂ [[48](#bib.bib48)] | CVPR2020 | 78.3 | - | âœ“ | - | - |'
- en: '| HOReIDÂ [[42](#bib.bib42)] | CVPR2020 | 85.3 | 91.0 | Ã— | 72.6 | 86.4 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| HOReIDÂ [[42](#bib.bib42)] | CVPR2020 | 85.3 | 91.0 | Ã— | 72.6 | 86.4 |'
- en: '| Additional Supervision for Position | DAReIDÂ [[38](#bib.bib38)] | KBS2021
    | 68.1 | 79.5 | Ã— | 76.7 | 85.3 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| ä½ç½®çš„é¢å¤–ç›‘ç£ | DAReIDÂ [[38](#bib.bib38)] | KBS2021 | 68.1 | 79.5 | Ã— | 76.7 |
    85.3 |'
- en: '| PGFL-KDÂ [[58](#bib.bib58)] | MM2021 | 85.1 | 90.8 | Ã— | 74.0 | 86.7 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| PGFL-KDÂ [[58](#bib.bib58)] | MM2021 | 85.1 | 90.8 | Ã— | 74.0 | 86.7 |'
- en: '| HPNetÂ [[60](#bib.bib60)] | ICME2020 | 85.7 | - | âœ“ | 68.9 | 80.7 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| HPNetÂ [[60](#bib.bib60)] | ICME2020 | 85.7 | - | âœ“ | 68.9 | 80.7 |'
- en: '| Attention Mechanism for Position | VPMÂ [[63](#bib.bib63)] | CVPR2019 | 67.7
    | 81.9 | âœ“ | 65.5 | 74.8 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| ä½ç½®çš„æ³¨æ„åŠ›æœºåˆ¶ | VPMÂ [[63](#bib.bib63)] | CVPR2019 | 67.7 | 81.9 | âœ“ | 65.5 | 74.8
    |'
- en: '| APNÂ [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | Ã— | 66.4 | 76.5 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| APNÂ [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | Ã— | 66.4 | 76.5 |'
- en: '| MHSA-NetÂ [[68](#bib.bib68)] | TNNLS2022 | 85.7 | 91.3 | Ã— | 74.9 | 87.2 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| MHSA-NetÂ [[68](#bib.bib68)] | TNNLS2022 | 85.7 | 91.3 | Ã— | 74.9 | 87.2 |'
- en: '| PATÂ [[67](#bib.bib67)] | CVPR2021 | 88.0 | 92.3 | Ã— | 76.5 | 88.2 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| PATÂ [[67](#bib.bib67)] | CVPR2021 | 88.0 | 92.3 | Ã— | 76.5 | 88.2 |'
- en: '| Image Transformation | STNReIDÂ [[84](#bib.bib84)] | TMM2020 | 66.7 | 80.3
    | Ã— | 54.6 | 71.3 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| å›¾åƒå˜æ¢ | STNReIDÂ [[84](#bib.bib84)] | TMM2020 | 66.7 | 80.3 | Ã— | 54.6 | 71.3
    |'
- en: '| APNÂ [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | Ã— | 66.4 | 76.5 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| APNÂ [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | Ã— | 66.4 | 76.5 |'
- en: '| ACSAPÂ [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | Ã— | 76.5 | 87.4 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| ACSAPÂ [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | Ã— | 76.5 | 87.4 |'
- en: '| Co-AttentionÂ [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | Ã— | 73.1 | 83.2
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| ååŒæ³¨æ„åŠ›æœºåˆ¶Â [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | Ã— | 73.1 | 83.2 |'
- en: '| PPCLÂ [[83](#bib.bib83)] | CVPR2021 | 83.7 | 88.7 | Ã— | 71.4 | 85.7 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| PPCLÂ [[83](#bib.bib83)] | CVPR2021 | 83.7 | 88.7 | Ã— | 71.4 | 85.7 |'
- en: '| Scale Misalignment | Multi-scale Features | DSRÂ [[30](#bib.bib30)] | CVPR2018
    | 50.7 | 70.0 | âœ“ | 58.8 | 67.2 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| å°ºåº¦ä¸å¯¹é½ | å¤šå°ºåº¦ç‰¹å¾ | DSRÂ [[30](#bib.bib30)] | CVPR2018 | 50.7 | 70.0 | âœ“ | 58.8
    | 67.2 |'
- en: '| FPRÂ [[81](#bib.bib81)] | ICCV2019 | 81.0 | - | Ã— | 68.1 | - |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| FPRÂ [[81](#bib.bib81)] | ICCV2019 | 81.0 | - | Ã— | 68.1 | - |'
- en: '| Image Transformation | STNReIDÂ [[84](#bib.bib84)] | TMM2020 | 66.7 | 80.3
    | Ã— | 54.6 | 71.3 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| å›¾åƒå˜æ¢ | STNReIDÂ [[84](#bib.bib84)] | TMM2020 | 66.7 | 80.3 | Ã— | 54.6 | 71.3
    |'
- en: '| APNÂ [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | Ã— | 66.4 | 76.5 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| APNÂ [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | Ã— | 66.4 | 76.5 |'
- en: '| ACSAPÂ [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | Ã— | 76.5 | 87.4 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| ACSAPÂ [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | Ã— | 76.5 | 87.4 |'
- en: '| Co-AttentionÂ [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | Ã— | 73.1 | 83.2
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| ååŒæ³¨æ„åŠ›æœºåˆ¶Â [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | Ã— | 73.1 | 83.2 |'
- en: '| PPCLÂ [[83](#bib.bib83)] | CVPR2021 | 83.7 | 88.7 | Ã— | 71.4 | 85.7 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| PPCLÂ [[83](#bib.bib83)] | CVPR2021 | 83.7 | 88.7 | Ã— | 71.4 | 85.7 |'
- en: '| Noisy Information | Auxiliary Model for Noise | PDVMÂ [[45](#bib.bib45)] |
    PRL2020 | 43.3 | - | Ã— | - | - |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| å™ªå£°ä¿¡æ¯ | å™ªå£°çš„è¾…åŠ©æ¨¡å‹ | PDVMÂ [[45](#bib.bib45)] | PRL2020 | 43.3 | - | Ã— | - | -
    |'
- en: '| PGFAÂ [[36](#bib.bib36)] | ICCV2019 | 68.0 | 80.0 | âœ“ | 69.1 | 80.9 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| PGFAÂ [[36](#bib.bib36)] | ICCV2019 | 68.0 | 80.0 | âœ“ | 69.1 | 80.9 |'
- en: '| KBFMÂ [[47](#bib.bib47)] | ICIP2020 | 69.7 | 82.2 | Ã— | 64.1 | 73.9 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| KBFMÂ [[47](#bib.bib47)] | ICIP2020 | 69.7 | 82.2 | Ã— | 64.1 | 73.9 |'
- en: '| PMFBÂ [[46](#bib.bib46)] | TNNLS2021 | 72.5 | 83.0 | Ã— | 70.6 | 81.3 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| PMFBÂ [[46](#bib.bib46)] | TNNLS2021 | 72.5 | 83.0 | Ã— | 70.6 | 81.3 |'
- en: '| TSAÂ [[55](#bib.bib55)] | ACM MM2020 | 72.7 | 85.2 | âœ“ | 73.9 | 84.7 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| TSAÂ [[55](#bib.bib55)] | ACM MM2020 | 72.7 | 85.2 | âœ“ | 73.9 | 84.7 |'
- en: '| ACSAPÂ [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | Ã— | 76.5 | 87.4 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| ACSAPÂ [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | Ã— | 76.5 | 87.4 |'
- en: '| PVPMÂ [[48](#bib.bib48)] | CVPR2020 | 78.3 | - | âœ“ | - | - |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| PVPMÂ [[48](#bib.bib48)] | CVPR2020 | 78.3 | - | âœ“ | - | - |'
- en: '| Co-AttentionÂ [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | Ã— | 73.1 | 83.2
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| ååŒæ³¨æ„åŠ›æœºåˆ¶Â [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | Ã— | 73.1 | 83.2 |'
- en: '| LKWSÂ [[86](#bib.bib86)] | ICCV2021 | 85.7 | 93.7 | Ã— | 80.7 | 88.2 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| LKWSÂ [[86](#bib.bib86)] | ICCV2021 | 85.7 | 93.7 | Ã— | 80.7 | 88.2 |'
- en: '| Additional Supervision for Noise | DAReIDÂ [[38](#bib.bib38)] | KBS2021 |
    68.1 | 79.5 | Ã— | 76.7 | 85.3 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| å™ªå£°çš„é¢å¤–ç›‘ç£ | DAReIDÂ [[38](#bib.bib38)] | KBS2021 | 68.1 | 79.5 | Ã— | 76.7 |
    85.3 |'
- en: '| FPRÂ [[81](#bib.bib81)] | ICCV2019 | 81.0 | - | Ã— | 68.1 | - |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| FPRÂ [[81](#bib.bib81)] | ICCV2019 | 81.0 | - | Ã— | 68.1 | - |'
- en: '| HPNetÂ [[60](#bib.bib60)] | ICME2020 | 85.7 | - | âœ“ | 68.9 | 80.7 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| HPNetÂ [[60](#bib.bib60)] | ICME2020 | 85.7 | - | âœ“ | 68.9 | 80.7 |'
- en: '| Attention Mechanism for Noise | CBDB-NetÂ [[96](#bib.bib96)] | TCSVT2021 |
    66.7 | 78.3 | Ã— | 68.4 | 81.5 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| å™ªå£°çš„æ³¨æ„åŠ›æœºåˆ¶ | CBDB-NetÂ [[96](#bib.bib96)] | TCSVT2021 | 66.7 | 78.3 | Ã— | 68.4
    | 81.5 |'
- en: '| VPMÂ [[63](#bib.bib63)] | CVPR2019 | 67.7 | 81.9 | âœ“ | 65.5 | 74.8 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| VPMÂ [[63](#bib.bib63)] | CVPR2019 | 67.7 | 81.9 | âœ“ | 65.5 | 74.8 |'
- en: '| FEDÂ [[93](#bib.bib93)] | CVPR2022 | 83.1 | - | Ã— | - | - |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| FEDÂ [[93](#bib.bib93)] | CVPR2022 | 83.1 | - | Ã— | - | - |'
- en: '| OAMNÂ [[13](#bib.bib13)] | ICCV2021 | 86.0 | - | Ã— | 77.3 | - |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| OAMNÂ [[13](#bib.bib13)] | ICCV2021 | 86.0 | - | Ã— | 77.3 | - |'
- en: V Discussion
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V è®¨è®º
- en: In this section, we summarize and analyze the evaluation results of occluded
    person Re-ID methods based on the four issues discussed earlier. Following the
    proposed taxonomy, we aim to present potential factors that boost the performance
    of occluded person Re-ID to help facilitate future research.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ€»ç»“å’Œåˆ†æäº†åŸºäºå‰è¿°å››ä¸ªé—®é¢˜çš„é®æŒ¡è¡Œäººé‡è¯†åˆ«æ–¹æ³•çš„è¯„ä¼°ç»“æœã€‚æ ¹æ®æå‡ºçš„åˆ†ç±»æ³•ï¼Œæˆ‘ä»¬æ—¨åœ¨æå‡ºå¯èƒ½æå‡é®æŒ¡è¡Œäººé‡è¯†åˆ«æ€§èƒ½çš„å› ç´ ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚
- en: V-A Performance Comparison
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A æ€§èƒ½æ¯”è¾ƒ
- en: 'We evaluate the occluded person re-identification methods on four widely-used
    image-based datasets, i.e., Partial-REIDÂ [[15](#bib.bib15)], Partial-iLIDSÂ [[30](#bib.bib30)],
    Occluded-DukeMTMC (Occ-DukeMTMC)Â [[36](#bib.bib36)], and Occluded-REID (Occ-ReID)Â [[16](#bib.bib16)].
    Details about the four datasets are illustrated in Sec.Â [III-A](#S3.SS1 "III-A
    Datasets â€£ III Datasets and Evaluations â€£ Deep Learning-based Occluded Person
    Re-identification: A Survey"). Since the Partial-REID, Partial-iLIDS, and Occluded-REID
    are small, methods are generally trained on the training set of Market-1501Â [[105](#bib.bib105)]
    and tested on these three datasets for evaluation. The performance comparisons
    on two partial person Re-ID datasets and two occluded person Re-ID datasets are
    summarized in TableÂ [III](#S4.T3 "TABLE III â€£ IV-E2 Temporal Recovery â€£ IV-E Missing
    Information â€£ IV Occluded Person Re-ID â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey") and TableÂ [IV](#S5.T4 "TABLE IV â€£ V-B Future Directions â€£ V Discussion
    â€£ Deep Learning-based Occluded Person Re-identification: A Survey") respectively.
    From the two tables we obtain the following three observations:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºäºå›¾åƒçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†é®æŒ¡è¡Œäººé‡è¯†åˆ«æ–¹æ³•ï¼Œå³ Partial-REIDÂ [[15](#bib.bib15)]ã€Partial-iLIDSÂ [[30](#bib.bib30)]ã€Occluded-DukeMTMC
    (Occ-DukeMTMC)Â [[36](#bib.bib36)] å’Œ Occluded-REID (Occ-ReID)Â [[16](#bib.bib16)]ã€‚å…³äºè¿™å››ä¸ªæ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯åœ¨ç¬¬[III-A](#S3.SS1
    "III-A æ•°æ®é›† â€£ III æ•°æ®é›†å’Œè¯„ä¼° â€£ åŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡è¡Œäººé‡è¯†åˆ«ï¼šç»¼è¿°")èŠ‚ä¸­è¿›è¡Œäº†è¯´æ˜ã€‚ç”±äº Partial-REIDã€Partial-iLIDS
    å’Œ Occluded-REID è¾ƒå°ï¼Œæ–¹æ³•é€šå¸¸åœ¨ Market-1501Â [[105](#bib.bib105)] çš„è®­ç»ƒé›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨è¿™ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•å’Œè¯„ä¼°ã€‚ä¸¤ä¸ªéƒ¨åˆ†è¡Œäººé‡è¯†åˆ«æ•°æ®é›†å’Œä¸¤ä¸ªé®æŒ¡è¡Œäººé‡è¯†åˆ«æ•°æ®é›†çš„æ€§èƒ½æ¯”è¾ƒåˆ†åˆ«æ€»ç»“åœ¨è¡¨[III](#S4.T3
    "TABLE III â€£ IV-E2 æ—¶é—´æ¢å¤ â€£ IV-E ç¼ºå¤±ä¿¡æ¯ â€£ IV é®æŒ¡è¡Œäººé‡è¯†åˆ« â€£ åŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡è¡Œäººé‡è¯†åˆ«ï¼šç»¼è¿°")å’Œè¡¨[IV](#S5.T4
    "TABLE IV â€£ V-B æœªæ¥æ–¹å‘ â€£ V è®¨è®º â€£ åŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡è¡Œäººé‡è¯†åˆ«ï¼šç»¼è¿°")ä¸­ã€‚ä»è¿™ä¸¤ä¸ªè¡¨ä¸­æˆ‘ä»¬å¾—å‡ºä»¥ä¸‹ä¸‰ä¸ªè§‚å¯Ÿç»“æœï¼š
- en: '1). For addressing the position misalignment issue and the noisy information
    issue, effective solutions are rich and diverse. To be specific, firstly, there
    is not a dominant technical route to accomplish the position alignment: On Partial-ReID,
    attention mechanism-based method PATÂ [[67](#bib.bib67)] has achieved the top rank-1
    accuracy; On Partial-iLIDS, matching-based method ASANÂ [[41](#bib.bib41)] has
    reached the best rank-1 and rank-3 accuracy; On Occluded-DukeMTMC, auxiliary model-based
    method PFDÂ [[52](#bib.bib52)] has achieved the top rank-1 accuracy and the best
    mAP; On Occluded-ReID, additional supervision-based method HPNetÂ [[60](#bib.bib60)]
    has obtained the best rank-1 accuracy. Secondly, the most promising technical
    route to suppress the noisy occlusion is difficult to judge: On Partial-ReID,
    the rank-1 accuracies of three technical routes, i.e., auxiliary model, additional
    mechanism, and attention mechanism for noise, are comparable; On Partial-iLIDS
    and Occluded-DukeMTMC, auxiliary model-based methods LKWSÂ [[86](#bib.bib86)] and
    PFDÂ [[52](#bib.bib52)] have reached the best rank-1 accuracy accordingly; On Occluded-ReID,
    additional supervision-based method HPNetÂ [[60](#bib.bib60)] has achieved the
    top rank-1 accuracy. Although neither the position misalignment issue nor the
    noisy information issue has a dominant technical route, the advantages and disadvantages
    of different technical routes can be summarized (see Sec.[V-B](#S5.SS2 "V-B Future
    Directions â€£ V Discussion â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey")).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '1). é’ˆå¯¹ä½ç½®ä¸å¯¹é½é—®é¢˜å’Œå™ªå£°ä¿¡æ¯é—®é¢˜ï¼Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆä¸°å¯Œå¤šæ ·ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆï¼Œæ²¡æœ‰ä¸»å¯¼çš„æŠ€æœ¯è·¯çº¿æ¥å®ç°ä½ç½®å¯¹é½ï¼šåœ¨Partial-ReIDä¸­ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ–¹æ³•PAT
    [[67](#bib.bib67)] è¾¾åˆ°äº†æœ€ä½³çš„rank-1å‡†ç¡®ç‡ï¼›åœ¨Partial-iLIDSä¸­ï¼ŒåŸºäºåŒ¹é…çš„æ–¹æ³•ASAN [[41](#bib.bib41)]
    è¾¾åˆ°äº†æœ€ä½³çš„rank-1å’Œrank-3å‡†ç¡®ç‡ï¼›åœ¨Occluded-DukeMTMCä¸­ï¼ŒåŸºäºè¾…åŠ©æ¨¡å‹çš„æ–¹æ³•PFD [[52](#bib.bib52)] å®ç°äº†æœ€ä½³çš„rank-1å‡†ç¡®ç‡å’Œæœ€ä½³çš„mAPï¼›åœ¨Occluded-ReIDä¸­ï¼ŒåŸºäºé¢å¤–ç›‘ç£çš„æ–¹æ³•HPNet
    [[60](#bib.bib60)] è·å¾—äº†æœ€ä½³çš„rank-1å‡†ç¡®ç‡ã€‚å…¶æ¬¡ï¼Œå‹åˆ¶å™ªå£°é®æŒ¡çš„æœ€æœ‰å‰æ™¯çš„æŠ€æœ¯è·¯çº¿éš¾ä»¥åˆ¤æ–­ï¼šåœ¨Partial-ReIDä¸­ï¼Œä¸‰ç§æŠ€æœ¯è·¯çº¿ï¼Œå³è¾…åŠ©æ¨¡å‹ã€é¢å¤–æœºåˆ¶å’Œå™ªå£°æ³¨æ„åŠ›æœºåˆ¶çš„rank-1å‡†ç¡®ç‡ç›¸å½“ï¼›åœ¨Partial-iLIDSå’ŒOccluded-DukeMTMCä¸­ï¼ŒåŸºäºè¾…åŠ©æ¨¡å‹çš„æ–¹æ³•LKWS
    [[86](#bib.bib86)] å’ŒPFD [[52](#bib.bib52)] åˆ†åˆ«è¾¾åˆ°äº†æœ€ä½³çš„rank-1å‡†ç¡®ç‡ï¼›åœ¨Occluded-ReIDä¸­ï¼ŒåŸºäºé¢å¤–ç›‘ç£çš„æ–¹æ³•HPNet
    [[60](#bib.bib60)] è¾¾åˆ°äº†æœ€ä½³çš„rank-1å‡†ç¡®ç‡ã€‚è™½ç„¶ä½ç½®ä¸å¯¹é½é—®é¢˜å’Œå™ªå£°ä¿¡æ¯é—®é¢˜éƒ½æ²¡æœ‰ä¸»å¯¼çš„æŠ€æœ¯è·¯çº¿ï¼Œä½†ä¸åŒæŠ€æœ¯è·¯çº¿çš„ä¼˜ç¼ºç‚¹å¯ä»¥æ€»ç»“ï¼ˆè§Sec.[V-B](#S5.SS2
    "V-B Future Directions â€£ V Discussion â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey")ï¼‰ã€‚'
- en: 2). The scale misalignment issue and the missing information issue have drawn
    less attention in existing methods. Compared with the methods for addressing the
    position misalignment issue or the noisy information issue, the number of methods
    intended for the scale misalignment issue or the missing information issue is
    significantly smaller.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 2). åœ¨ç°æœ‰æ–¹æ³•ä¸­ï¼Œå°ºåº¦ä¸å¯¹é½é—®é¢˜å’Œä¿¡æ¯ç¼ºå¤±é—®é¢˜å¼•èµ·çš„å…³æ³¨è¾ƒå°‘ã€‚ä¸è§£å†³ä½ç½®ä¸å¯¹é½é—®é¢˜æˆ–å™ªå£°ä¿¡æ¯é—®é¢˜çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ—¨åœ¨è§£å†³å°ºåº¦ä¸å¯¹é½é—®é¢˜æˆ–ä¿¡æ¯ç¼ºå¤±é—®é¢˜çš„æ–¹æ³•æ•°é‡æ˜æ˜¾è¾ƒå°‘ã€‚
- en: 3). There are a large number of methods that have considered more than one issue
    at the same time. For instance, PPCLÂ [[83](#bib.bib83)] addresses the position
    and scale misalignment issues; PFDÂ [[52](#bib.bib52)] focuses on the position
    misalignment and the noisy information issues; Co-AttentionÂ [[40](#bib.bib40)]
    takes the position misalignment, the scale misalignment, and the noisy information
    issues into consideration. However, there are none of the methods that have considered
    all the four issues caused by occlusion. The in-depth analysis of issues and solutions
    in this survey aims to fill this gap and help develop a more comprehensive solution
    for occluded person Re-ID.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 3). æœ‰å¤§é‡æ–¹æ³•åŒæ—¶è€ƒè™‘äº†å¤šä¸ªé—®é¢˜ã€‚ä¾‹å¦‚ï¼ŒPPCL [[83](#bib.bib83)] è§£å†³äº†ä½ç½®å’Œå°ºåº¦ä¸å¯¹é½é—®é¢˜ï¼›PFD [[52](#bib.bib52)]
    ä¸“æ³¨äºä½ç½®ä¸å¯¹é½å’Œå™ªå£°ä¿¡æ¯é—®é¢˜ï¼›Co-Attention [[40](#bib.bib40)] è€ƒè™‘äº†ä½ç½®ä¸å¯¹é½ã€å°ºåº¦ä¸å¯¹é½å’Œå™ªå£°ä¿¡æ¯é—®é¢˜ã€‚ç„¶è€Œï¼Œå°šæ— æ–¹æ³•åŒæ—¶è€ƒè™‘ç”±é®æŒ¡å¼•èµ·çš„æ‰€æœ‰å››ä¸ªé—®é¢˜ã€‚æœ¬è°ƒæŸ¥å¯¹è¿™äº›é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆçš„æ·±å…¥åˆ†ææ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œå¹¶å¸®åŠ©å¼€å‘æ›´å…¨é¢çš„é®æŒ¡è¡Œäººå†è¯†åˆ«è§£å†³æ–¹æ¡ˆã€‚
- en: V-B Future Directions
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B æœªæ¥æ–¹å‘
- en: 'As shown in TableÂ [III](#S4.T3 "TABLE III â€£ IV-E2 Temporal Recovery â€£ IV-E
    Missing Information â€£ IV Occluded Person Re-ID â€£ Deep Learning-based Occluded
    Person Re-identification: A Survey") and TableÂ [IV](#S5.T4 "TABLE IV â€£ V-B Future
    Directions â€£ V Discussion â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey"), there have been consistent improvements in different technical routes
    for addressing different issues over the past few years. Based on the analysis
    of issues and solutions, the following insights can be drawn for the future research
    of occluded person Re-ID.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚è¡¨æ ¼[III](#S4.T3 "TABLE III â€£ IV-E2 Temporal Recovery â€£ IV-E Missing Information
    â€£ IV Occluded Person Re-ID â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey")å’Œè¡¨æ ¼[IV](#S5.T4 "TABLE IV â€£ V-B Future Directions â€£ V Discussion â€£ Deep
    Learning-based Occluded Person Re-identification: A Survey")æ‰€ç¤ºï¼Œè¿‡å»å‡ å¹´ä¸­åœ¨è§£å†³ä¸åŒé—®é¢˜çš„ä¸åŒæŠ€æœ¯è·¯çº¿æ–¹é¢å–å¾—äº†ä¸€è‡´çš„è¿›å±•ã€‚æ ¹æ®å¯¹é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆçš„åˆ†æï¼Œå¯ä»¥å¾—å‡ºä»¥ä¸‹å…³äºé®æŒ¡äºº
    Re-ID æœªæ¥ç ”ç©¶çš„è§è§£ã€‚'
- en: From the perspective of four significant issues caused by occlusion, the position
    misalignment and the noisy information issues have been widely studied while the
    scale misalignment and the missing information issues are rarely considered in
    existing methods. With the four issues summarized and analyzed in this survey,
    the deeper understanding of occluded person Re-ID can be obtained to contribute
    a more comprehensive solution and help inspire new ideas in the filed.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é®æŒ¡é€ æˆçš„å››ä¸ªä¸»è¦é—®é¢˜çš„è§’åº¦æ¥çœ‹ï¼Œä½ç½®é”™ä½å’Œå™ªå£°ä¿¡æ¯é—®é¢˜å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œè€Œå°ºåº¦é”™ä½å’Œç¼ºå¤±ä¿¡æ¯é—®é¢˜åœ¨ç°æœ‰æ–¹æ³•ä¸­è¾ƒå°‘è€ƒè™‘ã€‚é€šè¿‡å¯¹è¿™å››ä¸ªé—®é¢˜è¿›è¡Œæ€»ç»“å’Œåˆ†æï¼Œå¯ä»¥æ›´æ·±å…¥åœ°ç†è§£é®æŒ¡äºº
    Re-IDï¼Œè¿›è€Œè´¡çŒ®æ›´å…¨é¢çš„è§£å†³æ–¹æ¡ˆå¹¶æ¿€å‘è¯¥é¢†åŸŸçš„æ–°æ€è·¯ã€‚
- en: From the perspective of promising technical routes, it remains an open question
    since the evaluation results of state-of-the-art methods in different technical
    routes are comparable. In spite of this, we analyze and summarize the advantages
    and disadcantages of different technical routes as follows to help boost future
    research.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æœ‰å‰æ™¯çš„æŠ€æœ¯è·¯çº¿è§’åº¦æ¥çœ‹ï¼Œç”±äºä¸åŒæŠ€æœ¯è·¯çº¿çš„æœ€å…ˆè¿›æ–¹æ³•çš„è¯„ä¼°ç»“æœå…·æœ‰å¯æ¯”æ€§ï¼Œå°šæ— æ˜ç¡®ç­”æ¡ˆã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬è¿˜æ˜¯åˆ†æå¹¶æ€»ç»“äº†ä¸åŒæŠ€æœ¯è·¯çº¿çš„ä¼˜ç¼ºç‚¹ï¼Œä»¥å¸®åŠ©æ¨åŠ¨æœªæ¥çš„ç ”ç©¶ã€‚
- en: '1). *Matching.* The well-designed matching components and matching strategies
    greatly improves the performance of occluded person Re-ID. Local and scalable
    matching components with the corresponding matching strategy can help address
    the position misalignment, scale misalignment, and noisy information issues. Moreover,
    it can be easily integrated with other technical routes, e.g., the repeatedly
    reported methods Co-AttentionÂ [[40](#bib.bib40)] and ASANÂ [[41](#bib.bib41)] in
    TableÂ [III](#S4.T3 "TABLE III â€£ IV-E2 Temporal Recovery â€£ IV-E Missing Information
    â€£ IV Occluded Person Re-ID â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey").'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '1). *åŒ¹é….* è®¾è®¡è‰¯å¥½çš„åŒ¹é…ç»„ä»¶å’ŒåŒ¹é…ç­–ç•¥æå¤§åœ°æé«˜äº†é®æŒ¡äºº Re-ID çš„æ€§èƒ½ã€‚å…·æœ‰ç›¸åº”åŒ¹é…ç­–ç•¥çš„å±€éƒ¨å’Œå¯æ‰©å±•åŒ¹é…ç»„ä»¶å¯ä»¥å¸®åŠ©è§£å†³ä½ç½®é”™ä½ã€å°ºåº¦é”™ä½å’Œå™ªå£°ä¿¡æ¯é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥ä¸å…¶ä»–æŠ€æœ¯è·¯çº¿è½»æ¾é›†æˆï¼Œä¾‹å¦‚è¡¨æ ¼[III](#S4.T3
    "TABLE III â€£ IV-E2 Temporal Recovery â€£ IV-E Missing Information â€£ IV Occluded
    Person Re-ID â€£ Deep Learning-based Occluded Person Re-identification: A Survey")ä¸­åå¤æŠ¥å‘Šçš„æ–¹æ³•
    Co-AttentionÂ [[40](#bib.bib40)] å’Œ ASANÂ [[41](#bib.bib41)]ã€‚'
- en: '2). *Auxiliary Model and Additional Supervision.* In general, there are mainly
    three types of extra information employed for occluded person Re-ID: poses, segments,
    and attributes. The position information, as well as their estimation confidence,
    provided by pose estimation or segmentation are used to help address the position
    misalignment and the noisy information issues respectively (see Fig.Â [6](#S4.F6
    "Figure 6 â€£ IV-A2 Auxiliary Model for Position â€£ IV-A Position Misalignment â€£
    IV Occluded Person Re-ID â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey")). And the extra attribute information provided by person Re-ID datasets
    is generally utilized to formulate an extra task to help alleviate the issues
    brought by occlusion. The two technical routes bring a lot of benefits and convenience
    while they are dependent on the extra labels or external models.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '2). *è¾…åŠ©æ¨¡å‹å’Œé¢å¤–ç›‘ç£.* é€šå¸¸ï¼Œç”¨äºé®æŒ¡äºº Re-ID çš„é¢å¤–ä¿¡æ¯ä¸»è¦æœ‰ä¸‰ç§ç±»å‹ï¼šå§¿æ€ã€åˆ†å‰²å’Œå±æ€§ã€‚å§¿æ€ä¼°è®¡æˆ–åˆ†å‰²æä¾›çš„ä½ç½®ä¿¡æ¯åŠå…¶ä¼°è®¡ç½®ä¿¡åº¦åˆ†åˆ«ç”¨äºå¸®åŠ©è§£å†³ä½ç½®é”™ä½å’Œå™ªå£°ä¿¡æ¯é—®é¢˜ï¼ˆè§å›¾[6](#S4.F6
    "Figure 6 â€£ IV-A2 Auxiliary Model for Position â€£ IV-A Position Misalignment â€£
    IV Occluded Person Re-ID â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey")ï¼‰ã€‚æ­¤å¤–ï¼Œé®æŒ¡äºº Re-ID æ•°æ®é›†æä¾›çš„é¢å¤–å±æ€§ä¿¡æ¯é€šå¸¸ç”¨äºåˆ¶å®šé¢å¤–ä»»åŠ¡ï¼Œä»¥å¸®åŠ©ç¼“è§£é®æŒ¡å¸¦æ¥çš„é—®é¢˜ã€‚è¿™ä¸¤ç§æŠ€æœ¯è·¯çº¿å¸¦æ¥äº†è®¸å¤šå¥½å¤„å’Œä¾¿åˆ©ï¼Œä½†å®ƒä»¬ä¾èµ–äºé¢å¤–çš„æ ‡ç­¾æˆ–å¤–éƒ¨æ¨¡å‹ã€‚'
- en: '3). *Attention Mechanism.* The attention mechanism has been widely studied
    in existing methods for its huge potential and flexibility. In recent three years,
    the methodsÂ [[106](#bib.bib106), [40](#bib.bib40), [92](#bib.bib92), [68](#bib.bib68),
    [93](#bib.bib93)] introduce the self-attention (Transformer) to occluded person
    Re-ID have made remarkable improvements on public datasets. Similar to matching-based
    technical route, the attention mechanism can also be integrated with other technical
    routes, e.g., APNÂ [[64](#bib.bib64)] in TableÂ [IV](#S5.T4 "TABLE IV â€£ V-B Future
    Directions â€£ V Discussion â€£ Deep Learning-based Occluded Person Re-identification:
    A Survey").'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '3). *æ³¨æ„åŠ›æœºåˆ¶ã€‚* æ³¨æ„åŠ›æœºåˆ¶åœ¨ç°æœ‰æ–¹æ³•ä¸­å› å…¶å·¨å¤§æ½œåŠ›å’Œçµæ´»æ€§è€Œè¢«å¹¿æ³›ç ”ç©¶ã€‚åœ¨æœ€è¿‘ä¸‰å¹´ä¸­ï¼Œæ–¹æ³•Â [[106](#bib.bib106), [40](#bib.bib40),
    [92](#bib.bib92), [68](#bib.bib68), [93](#bib.bib93)] å¼•å…¥äº†è‡ªæ³¨æ„åŠ›ï¼ˆTransformerï¼‰ï¼Œåœ¨å…¬å…±æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç±»ä¼¼äºåŸºäºåŒ¹é…çš„æŠ€æœ¯è·¯çº¿ï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¹Ÿå¯ä»¥ä¸å…¶ä»–æŠ€æœ¯è·¯çº¿é›†æˆï¼Œä¾‹å¦‚è¡¨Â [IV](#S5.T4
    "TABLE IV â€£ V-B Future Directions â€£ V Discussion â€£ Deep Learning-based Occluded
    Person Re-identification: A Survey")ä¸­çš„ APNÂ [[64](#bib.bib64)]ã€‚'
- en: '4). *Image Transformation.* The partial (holistic) image is transformed to
    obtain the image of consistent contents with the holistic (partial) image, addressing
    the position and the scale misalignment issues simultaneously (see Fig.Â [8](#S4.F8
    "Figure 8 â€£ IV-C Position and Scale Misalignment â€£ IV Occluded Person Re-ID â€£
    Deep Learning-based Occluded Person Re-identification: A Survey") (*a*)). This
    technical route does make sense and is close to the ideal process while it requires
    more computation costs for the conditional image transformation in the inference
    stage. Furthermore, the image transformation has not achieved a satisfying result
    in the current stage and the performance of this technical route is a little bit
    lower than others.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '4). *å›¾åƒå˜æ¢ã€‚* éƒ¨åˆ†ï¼ˆæ•´ä½“ï¼‰å›¾åƒè¢«è½¬æ¢ï¼Œä»¥è·å¾—ä¸æ•´ä½“ï¼ˆéƒ¨åˆ†ï¼‰å›¾åƒå†…å®¹ä¸€è‡´çš„å›¾åƒï¼ŒåŒæ—¶è§£å†³ä½ç½®å’Œå°ºåº¦ä¸å¯¹é½é—®é¢˜ï¼ˆè§å›¾Â [8](#S4.F8 "Figure
    8 â€£ IV-C Position and Scale Misalignment â€£ IV Occluded Person Re-ID â€£ Deep Learning-based
    Occluded Person Re-identification: A Survey") (*a*)ï¼‰ã€‚è¿™ç§æŠ€æœ¯è·¯çº¿ç¡®å®æœ‰æ„ä¹‰ä¸”æ¥è¿‘ç†æƒ³è¿‡ç¨‹ï¼Œä½†å®ƒåœ¨æ¨ç†é˜¶æ®µéœ€è¦æ›´å¤šçš„è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œå½“å‰é˜¶æ®µå›¾åƒå˜æ¢å°šæœªå–å¾—ä»¤äººæ»¡æ„çš„ç»“æœï¼Œè¯¥æŠ€æœ¯è·¯çº¿çš„æ€§èƒ½ç•¥ä½äºå…¶ä»–æ–¹æ³•ã€‚'
- en: 'TABLE IV: Performance Comparison on Occ-dukemtmc and Occ-reid.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ IVï¼šOcc-dukemtmc å’Œ Occ-reid çš„æ€§èƒ½æ¯”è¾ƒã€‚
- en: '| Issues | Technical Routes | Methods | Publications | Occ-DukeMTMC | Occ-ReID
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| é—®é¢˜ | æŠ€æœ¯è·¯çº¿ | æ–¹æ³• | å‡ºç‰ˆç‰© | Occ-DukeMTMC | Occ-ReID |'
- en: '| Rank-1 | mAP | Rank-1 | mAP |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| Rank-1 | mAP | Rank-1 | mAP |'
- en: '| Position Misalignment | Matching | AMC+SWMÂ [[15](#bib.bib15)] | ICCV2015
    | - | - | 31.1 | 27.3 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| ä½ç½®ä¸å¯¹é½ | åŒ¹é… | AMC+SWMÂ [[15](#bib.bib15)] | ICCV2015 | - | - | 31.1 | 27.3
    |'
- en: '| GASMÂ [[39](#bib.bib39)] | ECCV2020 | - | - | 74.5 | 65.6 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| GASMÂ [[39](#bib.bib39)] | ECCV2020 | - | - | 74.5 | 65.6 |'
- en: '| DSRÂ [[30](#bib.bib30)] | CVPR2018 | 40.8 | 30.4 | 72.8 | 62.8 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| DSRÂ [[30](#bib.bib30)] | CVPR2018 | 40.8 | 30.4 | 72.8 | 62.8 |'
- en: '| HOReIDÂ [[42](#bib.bib42)] | CVPR2020 | 55.1 | 43.8 | 80.3 | 70.2 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| HOReIDÂ [[42](#bib.bib42)] | CVPR2020 | 55.1 | 43.8 | 80.3 | 70.2 |'
- en: '| DAReIDÂ [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| DAReIDÂ [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
- en: '| MoSÂ [[44](#bib.bib44)] | AAAI2021 | 66.6 | 55.1 | - | - |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| MoSÂ [[44](#bib.bib44)] | AAAI2021 | 66.6 | 55.1 | - | - |'
- en: '| Auxiliary Model for Position | PVPMÂ [[48](#bib.bib48)] | CVPR2020 | - | -
    | 70.4 | 61.2 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| ç”¨äºä½ç½®çš„è¾…åŠ©æ¨¡å‹ | PVPMÂ [[48](#bib.bib48)] | CVPR2020 | - | - | 70.4 | 61.2 |'
- en: '| PGFAÂ [[36](#bib.bib36)] | ICCV2019 | 51.4 | 37.3 | - | - |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| PGFAÂ [[36](#bib.bib36)] | ICCV2019 | 51.4 | 37.3 | - | - |'
- en: '| PDVMÂ [[45](#bib.bib45)] | PRL2020 | 53.0 | 38.1 | - | - |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| PDVMÂ [[45](#bib.bib45)] | PRL2020 | 53.0 | 38.1 | - | - |'
- en: '| HOReIDÂ [[42](#bib.bib42)] | CVPR2020 | 55.1 | 43.8 | 80.3 | 70.2 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| HOReIDÂ [[42](#bib.bib42)] | CVPR2020 | 55.1 | 43.8 | 80.3 | 70.2 |'
- en: '| PMFBÂ [[46](#bib.bib46)] | TNNLS2021 | 56.3 | 43.5 | - | - |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| PMFBÂ [[46](#bib.bib46)] | TNNLS2021 | 56.3 | 43.5 | - | - |'
- en: '| PFDÂ [[52](#bib.bib52)] | AAAI2022 | 69.5 | 61.8 | 81.5 | 83.0 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| PFDÂ [[52](#bib.bib52)] | AAAI2022 | 69.5 | 61.8 | 81.5 | 83.0 |'
- en: '| Additional Supervision for Position | HPNetÂ [[60](#bib.bib60)] | ICME2020
    | - | - | 87.3 | 77.4 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| ç”¨äºä½ç½®çš„é¢å¤–ç›‘ç£ | HPNetÂ [[60](#bib.bib60)] | ICME2020 | - | - | 87.3 | 77.4 |'
- en: '| PGFL-KDÂ [[58](#bib.bib58)] | MM2021 | 63.0 | 54.1 | 80.7 | 70.3 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| PGFL-KDÂ [[58](#bib.bib58)] | MM2021 | 63.0 | 54.1 | 80.7 | 70.3 |'
- en: '| DAReIDÂ [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| DAReIDÂ [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
- en: '| RFCNetÂ [[14](#bib.bib14)] | TPAMI2021 | 63.9 | 54.5 | - | - |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| RFCNetÂ [[14](#bib.bib14)] | TPAMI2021 | 63.9 | 54.5 | - | - |'
- en: '| Attention Mechanism for Position | MHSA-NetÂ [[68](#bib.bib68)] | TNNLS2022
    | 59.7 | 44.8 | - | - |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| ç”¨äºä½ç½®çš„æ³¨æ„åŠ›æœºåˆ¶ | MHSA-NetÂ [[68](#bib.bib68)] | TNNLS2022 | 59.7 | 44.8 | - |
    - |'
- en: '| ISPÂ [[62](#bib.bib62)] | ECCV2020 | 62.8 | 52.3 | - | - |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| ISPÂ [[62](#bib.bib62)] | ECCV2020 | 62.8 | 52.3 | - | - |'
- en: '| PATÂ [[67](#bib.bib67)] | CVPR2021 | 64.5 | 53.6 | 81.6 | 72.1 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| PATÂ [[67](#bib.bib67)] | CVPR2021 | 64.5 | 53.6 | 81.6 | 72.1 |'
- en: '| SBPAÂ [[66](#bib.bib66)] | SPL2021 | 64.5 | 54.0 | - | - |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| SBPAÂ [[66](#bib.bib66)] | SPL2021 | 64.5 | 54.0 | - | - |'
- en: '| Scale Misalignment | Multi-scale Features | FPRÂ [[81](#bib.bib81)] | ICCV2019
    | - | - | 78.3 | 68.0 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| å°ºåº¦ä¸å¯¹é½ | å¤šå°ºåº¦ç‰¹å¾ | FPRÂ [[81](#bib.bib81)] | ICCV2019 | - | - | 78.3 | 68.0 |'
- en: '| DSRÂ [[30](#bib.bib30)] | CVPR2018 | 40.8 | 30.4 | 72.8 | 62.8 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| DSRÂ [[30](#bib.bib30)] | CVPR2018 | 40.8 | 30.4 | 72.8 | 62.8 |'
- en: '| Noisy Information | Auxiliary Model for Noise | PVPMÂ [[48](#bib.bib48)] |
    CVPR2020 | - | - | 70.4 | 61.2 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| å™ªå£°ä¿¡æ¯ | è¾…åŠ©å™ªå£°æ¨¡å‹ | PVPMÂ [[48](#bib.bib48)] | CVPR2020 | - | - | 70.4 | 61.2
    |'
- en: '| PGFAÂ [[36](#bib.bib36)] | ICCV2019 | 51.4 | 37.3 | - | - |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| PGFAÂ [[36](#bib.bib36)] | ICCV2019 | 51.4 | 37.3 | - | - |'
- en: '| PDVMÂ [[45](#bib.bib45)] | PRL2020 | 53.0 | 38.1 | - | - |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| PDVMÂ [[45](#bib.bib45)] | PRL2020 | 53.0 | 38.1 | - | - |'
- en: '| PMFBÂ [[46](#bib.bib46)] | TNNLS2021 | 56.3 | 43.5 | - | - |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| PMFBÂ [[46](#bib.bib46)] | TNNLS2021 | 56.3 | 43.5 | - | - |'
- en: '| LKWSÂ [[86](#bib.bib86)] | ICCV2021 | 62.2 | 46.3 | 81.0 | 71.0 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| LKWSÂ [[86](#bib.bib86)] | ICCV2021 | 62.2 | 46.3 | 81.0 | 71.0 |'
- en: '| PFDÂ [[52](#bib.bib52)] | AAAI2022 | 69.5 | 61.8 | 81.5 | 83.0 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| PFDÂ [[52](#bib.bib52)] | AAAI2022 | 69.5 | 61.8 | 81.5 | 83.0 |'
- en: '| Additional Supervision for Noise | GASMÂ [[39](#bib.bib39)] | ECCV2020 | -
    | - | 74.5 | 65.6 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| å™ªå£°çš„é™„åŠ ç›‘ç£ | GASMÂ [[39](#bib.bib39)] | ECCV2020 | - | - | 74.5 | 65.6 |'
- en: '| FPRÂ [[81](#bib.bib81)] | ICCV2019 | - | - | 78.3 | 68.0 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| FPRÂ [[81](#bib.bib81)] | ICCV2019 | - | - | 78.3 | 68.0 |'
- en: '| HPNetÂ [[60](#bib.bib60)] | ICME2020 | - | - | 87.3 | 77.4 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| HPNetÂ [[60](#bib.bib60)] | ICME2020 | - | - | 87.3 | 77.4 |'
- en: '| DAReIDÂ [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| DAReIDÂ [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
- en: '| Attention Mechanism for Noise | IGOASÂ [[66](#bib.bib66)] | TIP2021 | 60.1
    | 49.4 | 81.1 | - |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| å™ªå£°çš„æ³¨æ„æœºåˆ¶ | IGOASÂ [[66](#bib.bib66)] | TIP2021 | 60.1 | 49.4 | 81.1 | - |'
- en: '| OAMNÂ [[13](#bib.bib13)] | ICCV2021 | 62.6 | 46.1 | - | - |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| OAMNÂ [[13](#bib.bib13)] | ICCV2021 | 62.6 | 46.1 | - | - |'
- en: '| OCNetÂ [[97](#bib.bib97)] | ICASSP2022 | 64.3 | 54.4 | - | - |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| OCNetÂ [[97](#bib.bib97)] | ICASSP2022 | 64.3 | 54.4 | - | - |'
- en: '| DRL-NetÂ [[92](#bib.bib92)] | TMM2021 | 65.8 | 53.9 | - | - |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| DRL-NetÂ [[92](#bib.bib92)] | TMM2021 | 65.8 | 53.9 | - | - |'
- en: '| SSGRÂ [[91](#bib.bib91)] | ICCV2021 | 65.8 | 57.2 | 78.5 | 72.9 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| SSGRÂ [[91](#bib.bib91)] | ICCV2021 | 65.8 | 57.2 | 78.5 | 72.9 |'
- en: '|  | FEDÂ [[93](#bib.bib93)] | CVPR2022 | 68.1 | 56.4 | 86.3 | 79.3 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  | FEDÂ [[93](#bib.bib93)] | CVPR2022 | 68.1 | 56.4 | 86.3 | 79.3 |'
- en: '| Missing Information | Spatial Recovery | RFCNetÂ [[14](#bib.bib14)] | TPAMI2021
    | 63.9 | 54.5 | - | - |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| ç¼ºå¤±ä¿¡æ¯ | ç©ºé—´æ¢å¤ | RFCNetÂ [[14](#bib.bib14)] | TPAMI2021 | 63.9 | 54.5 | - | -
    |'
- en: '| Temporal Recovery | RFCNetÂ [[14](#bib.bib14)] | TPAMI2021 | 63.9 | 54.5 |
    - | - |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| æ—¶é—´æ¢å¤ | RFCNetÂ [[14](#bib.bib14)] | TPAMI2021 | 63.9 | 54.5 | - | - |'
- en: VI Conclusion
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI ç»“è®º
- en: 'This paper aims at providing a systematic survey of occluded person Re-ID to
    help promote future research. We first analyze and summarize four issues brought
    by occlusion in person Re-ID: the position misalignment, the scale misalignment,
    the noisy information, and the missing information. The published publications
    of deep learning-based occluded person Re-ID from top conferences and journals
    before June, 2022 are categorized and introduced accordingly. We provide the performance
    comparison of recent occluded person Re-ID methods on four popular datasets: Partial-ReID,
    Partial-iLIDS, Occluded-ReID, and Occluded-DukeMTMC. Based on the analysis of
    evaluation results, we finally discuss the promising future research directions.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ—¨åœ¨æä¾›ä¸€ä¸ªå…³äºé®æŒ¡äººç‰©é‡è¯†åˆ«çš„ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚æˆ‘ä»¬é¦–å…ˆåˆ†æå’Œæ€»ç»“äº†é®æŒ¡åœ¨äººç‰©é‡è¯†åˆ«ä¸­å¸¦æ¥çš„å››ä¸ªé—®é¢˜ï¼šä½ç½®ä¸å¯¹é½ã€å°ºåº¦ä¸å¯¹é½ã€å™ªå£°ä¿¡æ¯å’Œç¼ºå¤±ä¿¡æ¯ã€‚å¯¹2022å¹´6æœˆä¹‹å‰åœ¨é¡¶çº§ä¼šè®®å’ŒæœŸåˆŠä¸Šå‘è¡¨çš„åŸºäºæ·±åº¦å­¦ä¹ çš„é®æŒ¡äººç‰©é‡è¯†åˆ«æ–‡çŒ®è¿›è¡Œäº†åˆ†ç±»å’Œä»‹ç»ã€‚æˆ‘ä»¬æä¾›äº†æœ€è¿‘é®æŒ¡äººç‰©é‡è¯†åˆ«æ–¹æ³•åœ¨å››ä¸ªçƒ­é—¨æ•°æ®é›†ä¸Šçš„æ€§èƒ½æ¯”è¾ƒï¼šPartial-ReIDã€Partial-iLIDSã€Occluded-ReID
    å’Œ Occluded-DukeMTMCã€‚åŸºäºè¯„ä¼°ç»“æœçš„åˆ†æï¼Œæˆ‘ä»¬æœ€åè®¨è®ºäº†æœ‰å‰æ™¯çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚
- en: References
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] M.Â Ye etÂ al. Deep learning for person re-identification: A survey and outlook.
    IEEE Trans. Pattern Anal. Mach. Intell., 2021.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M.Â Ye ç­‰. åŸºäºæ·±åº¦å­¦ä¹ çš„äººç‰©é‡è¯†åˆ«ï¼šç»¼è¿°ä¸å±•æœ›ã€‚IEEE è®¡ç®—æœºå­¦ä¼šæ¨¡å¼åˆ†æä¸æœºå™¨æ™ºèƒ½æœŸåˆŠï¼Œ2021å¹´ã€‚'
- en: '[2] H.Â Zhao etÂ al. Spindle net: Person re-identification with human body region
    guided feature decomposition and fusion. In CVPR, pp. 1077â€“1085, 2017.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] H.Â Zhao ç­‰. Spindle net: é€šè¿‡äººä½“åŒºåŸŸå¼•å¯¼çš„ç‰¹å¾åˆ†è§£ä¸èåˆè¿›è¡Œäººç‰©é‡è¯†åˆ«ã€‚å‘è¡¨äº CVPRï¼Œç¬¬1077â€“1085é¡µï¼Œ2017å¹´ã€‚'
- en: '[3] M.Â S. Sarfraz etÂ al. A pose-sensitive embedding for person re-identification
    with expanded cross neighborhood re-ranking. In CVPR, pp. 420â€“429, 2018.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M.Â S. Sarfraz ç­‰. å…·æœ‰æ‰©å±•äº¤å‰é‚»åŸŸé‡æ’åºçš„å§¿æ€æ•æ„ŸåµŒå…¥ç”¨äºäººç‰©é‡è¯†åˆ«ã€‚å‘è¡¨äº CVPRï¼Œç¬¬420â€“429é¡µï¼Œ2018å¹´ã€‚'
- en: '[4] J.Â Liu etÂ al. Pose transferrable person re-identification. In CVPR, pp.
    4099â€“4108, 2018.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J.Â Liu ç­‰. å¯è½¬ç§»å§¿æ€çš„äººç‰©é‡è¯†åˆ«ã€‚å‘è¡¨äº CVPRï¼Œç¬¬4099â€“4108é¡µï¼Œ2018å¹´ã€‚'
- en: '[5] X.Â Qian etÂ al. Long-term cloth-changing person re-identification. In ACCV,
    2020.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] X.Â Qian ç­‰. é•¿æœŸæ¢è¡£äººç‰©é‡è¯†åˆ«ã€‚å‘è¡¨äº ACCVï¼Œ2020å¹´ã€‚'
- en: '[6] P.Â Hong etÂ al. Fine-grained shape-appearance mutual learning for cloth-changing
    person re-identification. In CVPR, pp. 10513â€“10522, 2021.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] P. Hongç­‰. ç”¨äºæ¢è¡£è¡Œäººé‡è¯†åˆ«çš„ç»†ç²’åº¦å½¢çŠ¶-å¤–è§‚äº’å­¦ã€‚ In CVPR, pp. 10513â€“10522, 2021ã€‚'
- en: '[7] Y.Â Huang etÂ al. Illumination-invariant person re-identification. In MM
    - Proc. ACM Int. Conf. Multimed., pp. 365â€“373, 2019.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Huangç­‰. ä¸å˜ç…§æ˜çš„è¡Œäººé‡è¯†åˆ«ã€‚ In MM - Proc. ACM Int. Conf. Multimed., pp. 365â€“373,
    2019ã€‚'
- en: '[8] G.Â Zhang etÂ al. Illumination unification for person re-identification.
    IEEE Trans. Circuits Syst. Video Technol., pp. 1â€“1, 2022.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] G. Zhangç­‰. è¡Œäººé‡è¯†åˆ«çš„ç…§æ˜ç»Ÿä¸€ã€‚IEEE Trans. Circuits Syst. Video Technol., pp. 1â€“1,
    2022ã€‚'
- en: '[9] S.Â Karanam etÂ al. Person re-identification with discriminatively trained
    viewpoint invariant dictionaries. In ICCV, pp. 4516â€“4524, 2015.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Karanamç­‰. ä½¿ç”¨åˆ¤åˆ«è®­ç»ƒçš„è§†è§’ä¸å˜å­—å…¸è¿›è¡Œè¡Œäººé‡è¯†åˆ«ã€‚ In ICCV, pp. 4516â€“4524, 2015ã€‚'
- en: '[10] X.Â Sun and L.Â Zheng. Dissecting person re-identification from the viewpoint
    of viewpoint. In CVPR, pp. 608â€“617, 2019.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] X. Sunå’ŒL. Zheng. ä»è§†è§’çš„è§’åº¦è§£å‰–è¡Œäººé‡è¯†åˆ«ã€‚ In CVPR, pp. 608â€“617, 2019ã€‚'
- en: '[11] L.Â Wu etÂ al. Cross-entropy adversarial view adaptation for person re-identification.
    IEEE Trans. Circuits Syst. Video Technol., 30(7):2081â€“2092, 2020.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] L. Wuç­‰. ç”¨äºè¡Œäººé‡è¯†åˆ«çš„äº¤å‰ç†µå¯¹æŠ—è§†è§’é€‚åº”ã€‚IEEE Trans. Circuits Syst. Video Technol., 30(7):2081â€“2092,
    2020ã€‚'
- en: '[12] J.Â Miao etÂ al. Identifying visible parts via pose estimation for occluded
    person re-identification. IEEE Trans. Neural Netw. Learn. Syst., 2021.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] J. Miaoç­‰. é€šè¿‡å§¿æ€ä¼°è®¡è¯†åˆ«å¯è§éƒ¨åˆ†ä»¥å®ç°é®æŒ¡è¡Œäººé‡è¯†åˆ«ã€‚IEEE Trans. Neural Netw. Learn. Syst.,
    2021ã€‚'
- en: '[13] P.Â Chen etÂ al. Occlude them all: Occlusion-aware attention network for
    occluded person re-id. In ICCV, pp. 11833â€“11842, 2021.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] P. Chenç­‰. é®æŒ¡ä»–ä»¬å…¨éƒ¨ï¼šç”¨äºé®æŒ¡è¡Œäººé‡è¯†åˆ«çš„é®æŒ¡æ„ŸçŸ¥æ³¨æ„ç½‘ç»œã€‚ In ICCV, pp. 11833â€“11842, 2021ã€‚'
- en: '[14] R.Â Hou etÂ al. Feature completion for occluded person re-identification.
    IEEE Trans. Pattern Anal. Mach. Intell., 2021.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] R. Houç­‰. ç”¨äºé®æŒ¡è¡Œäººé‡è¯†åˆ«çš„ç‰¹å¾å®Œæˆã€‚IEEE Trans. Pattern Anal. Mach. Intell., 2021ã€‚'
- en: '[15] W.-S. Zheng etÂ al. Partial person re-identification. In ICCV, pp. 4678â€“4686,
    2015.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] W.-S. Zhengç­‰. éƒ¨åˆ†è¡Œäººé‡è¯†åˆ«ã€‚ In ICCV, pp. 4678â€“4686, 2015ã€‚'
- en: '[16] J.Â Zhuo etÂ al. Occluded person re-identification. In ICME, pp. 1â€“6\. IEEE,
    2018.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Zhuoç­‰. é®æŒ¡è¡Œäººé‡è¯†åˆ«ã€‚ In ICME, pp. 1â€“6. IEEE, 2018ã€‚'
- en: '[17] L.Â Zheng etÂ al. Person re-identification: Past, present and future. arXiv
    preprint arXiv:1610.02984, 2016.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] L. Zhengç­‰. è¡Œäººé‡è¯†åˆ«ï¼šè¿‡å»ã€ç°åœ¨ä¸æœªæ¥ã€‚arXiv preprint arXiv:1610.02984, 2016ã€‚'
- en: '[18] R.Â Mazzon etÂ al. Person re-identification in crowd. Pattern Recognit.
    Lett., 33(14):1828â€“1837, 2012.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] R. Mazzonç­‰. äººç¾¤ä¸­çš„è¡Œäººé‡è¯†åˆ«ã€‚Pattern Recognit. Lett., 33(14):1828â€“1837, 2012ã€‚'
- en: '[19] A.Â Bedagkar-Gala and S.Â K. Shah. A survey of approaches and trends in
    person re-identification. Image Vision Comput., 32(4):270â€“286, 2014.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. Bedagkar-Galaå’ŒS. K. Shah. è¡Œäººé‡è¯†åˆ«æ–¹æ³•å’Œè¶‹åŠ¿è°ƒæŸ¥ã€‚Image Vision Comput., 32(4):270â€“286,
    2014ã€‚'
- en: '[20] B.Â Lavi etÂ al. Survey on deep learning techniques for person re-identification
    task. arXiv preprint arXiv:1807.05284, 2018.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] B. Laviç­‰. è¡Œäººé‡è¯†åˆ«ä»»åŠ¡çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯è°ƒæŸ¥ã€‚arXiv preprint arXiv:1807.05284, 2018ã€‚'
- en: '[21] D.Â Wu etÂ al. Deep learning-based methods for person re-identification:
    A comprehensive review. Neurocomputing, 337:354â€“371, 2019.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] D. Wuç­‰. åŸºäºæ·±åº¦å­¦ä¹ çš„è¡Œäººé‡è¯†åˆ«æ–¹æ³•ï¼šå…¨é¢ç»¼è¿°ã€‚Neurocomputing, 337:354â€“371, 2019ã€‚'
- en: '[22] Q.Â Leng etÂ al. A survey of open-world person re-identification. IEEE Trans.
    Circuits Syst. Video Technol., 30(4):1092â€“1108, 2019.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Q. Lengç­‰. å¼€æ”¾ä¸–ç•Œè¡Œäººé‡è¯†åˆ«çš„è°ƒæŸ¥ã€‚IEEE Trans. Circuits Syst. Video Technol., 30(4):1092â€“1108,
    2019ã€‚'
- en: '[23] B.Â Lavi etÂ al. Survey on reliable deep learning-based person re-identification
    models: Are we there yet? arXiv preprint arXiv:2005.00355, 2020.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] B. Laviç­‰. åŸºäºæ·±åº¦å­¦ä¹ çš„è¡Œäººé‡è¯†åˆ«æ¨¡å‹è°ƒæŸ¥ï¼šæˆ‘ä»¬å·²ç»åˆ°è¾¾äº†å—ï¼ŸarXiv preprint arXiv:2005.00355, 2020ã€‚'
- en: '[24] S.Â karanam etÂ al. A systematic evaluation and benchmark for person re-identification:
    Features, metrics, and datasets. IEEE Trans. Pattern Anal. Mach. Intell., 41(3):523â€“536,
    2019.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Karanamç­‰. è¡Œäººé‡è¯†åˆ«çš„ç³»ç»Ÿè¯„ä¼°å’ŒåŸºå‡†ï¼šç‰¹å¾ã€åº¦é‡å’Œæ•°æ®é›†ã€‚IEEE Trans. Pattern Anal. Mach. Intell.,
    41(3):523â€“536, 2019ã€‚'
- en: '[25] K.Â Islam. Person search: New paradigm of person re-identification: A survey
    and outlook of recent works. Image Vision Comput., 101:103970, 2020.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] K. Islam. è¡Œäººæœç´¢ï¼šè¡Œäººé‡è¯†åˆ«çš„æ–°èŒƒå¼ï¼šå¯¹è¿‘æœŸå·¥ä½œçš„è°ƒæŸ¥å’Œå±•æœ›ã€‚Image Vision Comput., 101:103970,
    2020ã€‚'
- en: '[26] X.Â Lin etÂ al. Unsupervised person re-identification: A systematic survey
    of challenges and solutions. arXiv preprint arXiv:2109.06057, 2021.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] X. Linç­‰. æ— ç›‘ç£è¡Œäººé‡è¯†åˆ«ï¼šæŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆçš„ç³»ç»Ÿè°ƒæŸ¥ã€‚arXiv preprint arXiv:2109.06057, 2021ã€‚'
- en: '[27] X.Â Lin etÂ al. Person search challenges and solutions: A survey. In IJCAI
    Int. Joint Conf. Artif. Intell., pp. 1â€“10\. RMIT University, 2021.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] X. Linç­‰. è¡Œäººæœç´¢æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆï¼šè°ƒæŸ¥ã€‚ In IJCAI Int. Joint Conf. Artif. Intell., pp.
    1â€“10. RMIT University, 2021ã€‚'
- en: '[28] Z.Â Ming etÂ al. Deep learning-based person re-identification methods: A
    survey and outlook of recent works. Image Vision Comput., pp. 104394, 2022.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Z. Ming ç­‰. åŸºäºæ·±åº¦å­¦ä¹ çš„äººç‰©å†è¯†åˆ«æ–¹æ³•ï¼šè¿‘æœŸå·¥ä½œçš„ç»¼è¿°ä¸å±•æœ›ã€‚å›¾åƒè§†è§‰è®¡ç®—, ç¬¬ 104394 é¡µ, 2022ã€‚'
- en: '[29] Z.Â Wang etÂ al. Beyond intra-modality: a survey of heterogeneous person
    re-identification. In IJCAI Int. Joint Conf. Artif. Intell., pp. 4973â€“4980, 2021.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Z. Wang ç­‰. è¶…è¶Šå†…éƒ¨æ¨¡æ€ï¼šå¼‚è´¨äººç‰©å†è¯†åˆ«çš„ç»¼è¿°ã€‚å‘è¡¨äº IJCAI å›½é™…è”åˆäººå·¥æ™ºèƒ½ä¼šè®®, ç¬¬ 4973â€“4980 é¡µ, 2021ã€‚'
- en: '[30] L.Â He etÂ al. Deep spatial feature reconstruction for partial person re-identification:
    Alignment-free approach. In CVPR, pp. 7073â€“7082, 2018.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] L. He ç­‰. éƒ¨åˆ†äººç‰©å†è¯†åˆ«çš„æ·±åº¦ç©ºé—´ç‰¹å¾é‡å»ºï¼šæ— å¯¹é½æ–¹æ³•ã€‚å‘è¡¨äº CVPR, ç¬¬ 7073â€“7082 é¡µ, 2018ã€‚'
- en: '[31] W.-S. Zheng etÂ al. Person re-identification by probabilistic relative
    distance comparison. In CVPR 2011, pp. 649â€“656\. IEEE, 2011.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] W.-S. Zheng ç­‰. åŸºäºæ¦‚ç‡ç›¸å¯¹è·ç¦»æ¯”è¾ƒçš„äººç‰©å†è¯†åˆ«ã€‚å‘è¡¨äº CVPR 2011, ç¬¬ 649â€“656 é¡µã€‚IEEE, 2011ã€‚'
- en: '[32] J.Â Kim and C.Â D. Yoo. Deep partial person re-identification via attention
    model. In ICIP, pp. 3425â€“3429\. IEEE, 2017.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Kim å’Œ C. D. Yoo. é€šè¿‡æ³¨æ„åŠ›æ¨¡å‹è¿›è¡Œæ·±åº¦éƒ¨åˆ†äººç‰©å†è¯†åˆ«ã€‚å‘è¡¨äº ICIP, ç¬¬ 3425â€“3429 é¡µã€‚IEEE, 2017ã€‚'
- en: '[33] W.Â Li etÂ al. Deepreid: Deep filter pairing neural network for person re-identification.
    In CVPR, pp. 152â€“159, 2014.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] W. Li ç­‰. Deepreid: ç”¨äºäººç‰©å†è¯†åˆ«çš„æ·±åº¦è¿‡æ»¤é…å¯¹ç¥ç»ç½‘ç»œã€‚å‘è¡¨äº CVPR, ç¬¬ 152â€“159 é¡µ, 2014ã€‚'
- en: '[34] A.Â Ess etÂ al. A mobile vision system for robust multi-person tracking.
    In CVPR, pp. 1â€“8\. IEEE, 2008.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. Ess ç­‰. ä¸€ç§ç”¨äºç¨³å¥å¤šäººç‰©è¿½è¸ªçš„ç§»åŠ¨è§†è§‰ç³»ç»Ÿã€‚å‘è¡¨äº CVPR, ç¬¬ 1â€“8 é¡µã€‚IEEE, 2008ã€‚'
- en: '[35] Z.Â Zheng etÂ al. Unlabeled samples generated by gan improve the person
    re-identification baseline in vitro. In ICCV, pp. 3754â€“3762, 2017.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Z. Zheng ç­‰. ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç”Ÿæˆçš„æ— æ ‡ç­¾æ ·æœ¬æé«˜äº†ä½“å¤–çš„äººç‰©å†è¯†åˆ«åŸºçº¿ã€‚å‘è¡¨äº ICCV, ç¬¬ 3754â€“3762 é¡µ, 2017ã€‚'
- en: '[36] J.Â Miao etÂ al. Pose-guided feature alignment for occluded person re-identification.
    In ICCV, pp. 542â€“551, 2019.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Miao ç­‰. åŸºäºå§¿æ€å¼•å¯¼çš„ç‰¹å¾å¯¹é½ç”¨äºé®æŒ¡äººç‰©å†è¯†åˆ«ã€‚å‘è¡¨äº ICCV, ç¬¬ 542â€“551 é¡µ, 2019ã€‚'
- en: '[37] Y.Â Wu etÂ al. Exploit the unknown gradually: One-shot video-based person
    re-identification by stepwise learning. In CVPR, pp. 5177â€“5186, 2018.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Y. Wu ç­‰. é€æ­¥å¼€å‘æœªçŸ¥ï¼šé€šè¿‡é€æ­¥å­¦ä¹ çš„ä¸€æ¬¡æ€§è§†é¢‘åŸºäººç‰©å†è¯†åˆ«ã€‚å‘è¡¨äº CVPR, ç¬¬ 5177â€“5186 é¡µ, 2018ã€‚'
- en: '[38] Y.Â Xu etÂ al. Dual attention-based method for occluded person re-identification.
    Knowledge-Based Systems, 212:106554, 2021.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Xu ç­‰. åŸºäºåŒé‡æ³¨æ„åŠ›çš„æ–¹æ³•ç”¨äºé®æŒ¡äººç‰©å†è¯†åˆ«ã€‚çŸ¥è¯†åŸºç¡€ç³»ç»Ÿ, 212:106554, 2021ã€‚'
- en: '[39] L.Â He and W.Â Liu. Guided saliency feature learning for person re-identification
    in crowded scenes. In ECCV, pp. 357â€“373\. Springer, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] L. He å’Œ W. Liu. é’ˆå¯¹æ‹¥æŒ¤åœºæ™¯ä¸­äººç‰©å†è¯†åˆ«çš„å¼•å¯¼æ˜¾è‘—æ€§ç‰¹å¾å­¦ä¹ ã€‚å‘è¡¨äº ECCV, ç¬¬ 357â€“373 é¡µã€‚Springer,
    2020ã€‚'
- en: '[40] C.-S. Lin and Y.-C.Â F. Wang. Self-supervised bodymap-to-appearance co-attention
    for partial person re-identification. In ICIP, pp. 2299â€“2303\. IEEE, 2021.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] C.-S. Lin å’Œ Y.-C. F. Wang. è‡ªç›‘ç£ä½“å›¾åˆ°å¤–è§‚çš„å…±åŒæ³¨æ„åŠ›ç”¨äºéƒ¨åˆ†äººç‰©å†è¯†åˆ«ã€‚å‘è¡¨äº ICIP, ç¬¬ 2299â€“2303
    é¡µã€‚IEEE, 2021ã€‚'
- en: '[41] H.Â Jin etÂ al. Occlusion-sensitive person re-identification via attribute-based
    shift attention. IEEE Trans. Circuits Syst. Video Technol., 32(4):2170â€“2185, 2022.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H. Jin ç­‰. é€šè¿‡åŸºäºå±æ€§çš„åç§»æ³¨æ„åŠ›è¿›è¡Œé®æŒ¡æ•æ„Ÿçš„äººç‰©å†è¯†åˆ«ã€‚IEEE ç”µè·¯ç³»ç»Ÿä¸è§†é¢‘æŠ€æœ¯å­¦æŠ¥, 32(4):2170â€“2185,
    2022ã€‚'
- en: '[42] G.Â Wang etÂ al. High-order information matters: Learning relation and topology
    for occluded person re-identification. In CVPR, pp. 6449â€“6458, 2020.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] G. Wang ç­‰. é«˜é˜¶ä¿¡æ¯è‡³å…³é‡è¦ï¼šå­¦ä¹ å…³ç³»å’Œæ‹“æ‰‘ç”¨äºé®æŒ¡äººç‰©å†è¯†åˆ«ã€‚å‘è¡¨äº CVPR, ç¬¬ 6449â€“6458 é¡µ, 2020ã€‚'
- en: '[43] Y.Â Yan etÂ al. Learning multi-granular hypergraphs for video-based person
    re-identification. In CVPR, pp. 2899â€“2908, 2020.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Y. Yan ç­‰. å­¦ä¹ å¤šç²’åº¦è¶…å›¾ç”¨äºåŸºäºè§†é¢‘çš„äººç‰©å†è¯†åˆ«ã€‚å‘è¡¨äº CVPR, ç¬¬ 2899â€“2908 é¡µ, 2020ã€‚'
- en: '[44] M.Â Jia etÂ al. Matching on sets: Conquer occluded person re-identification
    without alignment. In AAAI Conf. Artif. Intell., pp. 1673â€“1681, 2021.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] M. Jia ç­‰. åœ¨é›†åˆä¸ŠåŒ¹é…ï¼šæ— å¯¹é½å¾æœé®æŒ¡äººç‰©å†è¯†åˆ«ã€‚å‘è¡¨äº AAAI äººå·¥æ™ºèƒ½ä¼šè®®, ç¬¬ 1673â€“1681 é¡µ, 2021ã€‚'
- en: '[45] S.Â Zhou etÂ al. Depth occlusion perception feature analysis for person
    re-identification. Pattern Recognit. Lett., 138:617â€“623, 2020.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] S. Zhou ç­‰. é’ˆå¯¹äººç‰©å†è¯†åˆ«çš„æ·±åº¦é®æŒ¡æ„ŸçŸ¥ç‰¹å¾åˆ†æã€‚æ¨¡å¼è¯†åˆ«é€šè®¯, 138:617â€“623, 2020ã€‚'
- en: '[46] J.Â Miao etÂ al. Identifying visible parts via pose estimation for occluded
    person re-identification. IEEE Trans. Neural Netw. Learn. Syst., 2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Miao ç­‰. é€šè¿‡å§¿æ€ä¼°è®¡è¯†åˆ«å¯è§éƒ¨åˆ†ç”¨äºé®æŒ¡äººç‰©å†è¯†åˆ«ã€‚IEEE ç¥ç»ç½‘ç»œä¸å­¦ä¹ ç³»ç»Ÿå­¦æŠ¥, 2021ã€‚'
- en: '[47] C.Â Han etÂ al. Keypoint-based feature matching for partial person re-identification.
    In ICIP, pp. 226â€“230\. IEEE, 2020.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] C. Han ç­‰. åŸºäºå…³é”®ç‚¹çš„ç‰¹å¾åŒ¹é…ç”¨äºéƒ¨åˆ†äººç‰©å†è¯†åˆ«ã€‚å‘è¡¨äº ICIP, ç¬¬ 226â€“230 é¡µã€‚IEEE, 2020ã€‚'
- en: '[48] S.Â Gao etÂ al. Pose-guided visible part matching for occluded person reid.
    In CVPR, pp. 11744â€“11752, 2020.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] S. Gao ç­‰. åŸºäºå§¿æ€å¼•å¯¼çš„å¯è§éƒ¨åˆ†åŒ¹é…ç”¨äºé®æŒ¡äººç‰©å†è¯†åˆ«ã€‚å‘è¡¨äº CVPR, ç¬¬ 11744â€“11752 é¡µ, 2020ã€‚'
- en: '[49] Y.Â Zhai etÂ al. Pgmanet: Pose-guided mixed attention network for occluded
    person re-identification. In IJCNN, pp. 1â€“8\. IEEE, 2021.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Y. Zhai ç­‰äºº. Pgmanet: å¸¦æœ‰å§¿æ€å¼•å¯¼æ··åˆæ³¨æ„åŠ›ç½‘ç»œçš„é®æŒ¡äººç‰©é‡è¯†åˆ«. åœ¨ IJCNN, ç¬¬ 1â€“8 é¡µ. IEEE, 2021
    å¹´ã€‚'
- en: '[50] J.Â Liu etÂ al. Spatial-temporal correlation and topology learning for person
    re-identification in videos. In CVPR, pp. 4370â€“4379, 2021.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. Liu ç­‰äºº. è§†é¢‘ä¸­äººç‰©é‡è¯†åˆ«çš„æ—¶ç©ºç›¸å…³æ€§å’Œæ‹“æ‰‘å­¦ä¹ . åœ¨ CVPR, ç¬¬ 4370â€“4379 é¡µ, 2021 å¹´ã€‚'
- en: '[51] Y.Â He etÂ al. Adversarial cross-scale alignment pursuit for seriously misaligned
    person re-identification. In ICIP, pp. 2373â€“2377\. IEEE, 2021.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. He ç­‰äºº. ä¸ºä¸¥é‡å¤±è°ƒçš„äººç‰©é‡è¯†åˆ«è¿›è¡Œå¯¹æŠ—æ€§è·¨å°ºåº¦å¯¹é½è¿½æ±‚. åœ¨ ICIP, ç¬¬ 2373â€“2377 é¡µ. IEEE, 2021 å¹´ã€‚'
- en: '[52] W.Â Tao etÂ al. Pose-guided feature disentangling for occluded person re-identification
    based on transformer. AAAI Conf. Artif. Intell., 2022.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] W. Tao ç­‰äºº. åŸºäºå˜æ¢å™¨çš„å§¿æ€å¼•å¯¼ç‰¹å¾è§£ç¼ ç»“ç”¨äºé®æŒ¡äººç‰©é‡è¯†åˆ«. AAAI äººå·¥æ™ºèƒ½ä¼šè®®, 2022 å¹´ã€‚'
- en: '[53] M.Â M. Kalayeh etÂ al. Human semantic parsing for person re-identification.
    In CVPR, pp. 1062â€“1071, 2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. M. Kalayeh ç­‰äºº. äººç±»è¯­ä¹‰è§£æç”¨äºäººç‰©é‡è¯†åˆ«. åœ¨ CVPR, ç¬¬ 1062â€“1071 é¡µ, 2018 å¹´ã€‚'
- en: '[54] R.Â Quispe and H.Â Pedrini. Improved person re-identification based on saliency
    and semantic parsing with deep neural network models. Image Vision Comput., 92:103809,
    2019.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] R. Quispe å’Œ H. Pedrini. åŸºäºæ˜¾è‘—æ€§å’Œæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„è¯­ä¹‰è§£ææ”¹è¿›äººç‰©é‡è¯†åˆ«. å›¾åƒè§†è§‰è®¡ç®—, 92:103809,
    2019 å¹´ã€‚'
- en: '[55] L.Â Gao etÂ al. Texture semantically aligned with visibility-aware for partial
    person re-identification. In MM - Proc. ACM Int. Conf. Multimed., pp. 3771â€“3779,
    2020.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] L. Gao ç­‰äºº. ä¸å¯è§æ€§æ„ŸçŸ¥å¯¹é½çš„çº¹ç†ç”¨äºéƒ¨åˆ†äººç‰©é‡è¯†åˆ«. åœ¨ MM - ACM å›½é™…å¤šåª’ä½“ä¼šè®®, ç¬¬ 3771â€“3779 é¡µ, 2020
    å¹´ã€‚'
- en: '[56] J.Â Xu etÂ al. Attention-aware compositional network for person re-identification.
    In CVPR, pp. 2119â€“2128, 2018.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] J. Xu ç­‰äºº. å…³æ³¨æ„è¯†çš„ç»„åˆç½‘ç»œç”¨äºäººç‰©é‡è¯†åˆ«. åœ¨ CVPR, ç¬¬ 2119â€“2128 é¡µ, 2018 å¹´ã€‚'
- en: '[57] Z.Â Zhang etÂ al. Densely semantically aligned person re-identification.
    In CVPR, pp. 667â€“676, 2019.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Z. Zhang ç­‰äºº. å¯†é›†è¯­ä¹‰å¯¹é½çš„äººç‰©é‡è¯†åˆ«. åœ¨ CVPR, ç¬¬ 667â€“676 é¡µ, 2019 å¹´ã€‚'
- en: '[58] K.Â Zheng etÂ al. Pose-guided feature learning with knowledge distillation
    for occluded person re-identification. In MM - Proc. ACM Int. Conf. Multimed.,
    pp. 4537â€“4545, 2021.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] K. Zheng ç­‰äºº. å¸¦æœ‰çŸ¥è¯†è’¸é¦çš„å§¿æ€å¼•å¯¼ç‰¹å¾å­¦ä¹ ç”¨äºé®æŒ¡äººç‰©é‡è¯†åˆ«. åœ¨ MM - ACM å›½é™…å¤šåª’ä½“ä¼šè®®, ç¬¬ 4537â€“4545
    é¡µ, 2021 å¹´ã€‚'
- en: '[59] H.Â Cai etÂ al. Multi-scale body-part mask guided attention for person re-identification.
    In CVPR Workshops, pp. 0â€“0, 2019.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] H. Cai ç­‰äºº. å¤šå°ºåº¦èº«ä½“éƒ¨ä»¶æ©ç å¼•å¯¼æ³¨æ„åŠ›ç”¨äºäººç‰©é‡è¯†åˆ«. åœ¨ CVPR ç ”è®¨ä¼š, ç¬¬ 0â€“0 é¡µ, 2019 å¹´ã€‚'
- en: '[60] H.Â Huang etÂ al. Human parsing based alignment with multi-task learning
    for occluded person re-identification. In ICME, pp. 1â€“6\. IEEE, 2020.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] H. Huang ç­‰äºº. åŸºäºäººç±»è§£æå¯¹é½çš„å¤šä»»åŠ¡å­¦ä¹ ç”¨äºé®æŒ¡äººç‰©é‡è¯†åˆ«. åœ¨ ICME, ç¬¬ 1â€“6 é¡µ. IEEE, 2020 å¹´ã€‚'
- en: '[61] Q.Â Zhou etÂ al. Fine-grained spatial alignment model for person re-identification
    with focal triplet loss. IEEE Trans. Image Process., 29:7578â€“7589, 2020.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Q. Zhou ç­‰äºº. å…·æœ‰ç„¦ç‚¹ä¸‰å…ƒç»„æŸå¤±çš„äººç‰©é‡è¯†åˆ«ç»†ç²’åº¦ç©ºé—´å¯¹é½æ¨¡å‹. IEEE å›¾åƒå¤„ç†æ±‡åˆŠ, 29:7578â€“7589, 2020
    å¹´ã€‚'
- en: '[62] K.Â Zhu etÂ al. Identity-guided human semantic parsing for person re-identification.
    In ECCV, pp. 346â€“363\. Springer, 2020.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] K. Zhu ç­‰äºº. èº«ä»½å¼•å¯¼çš„äººç±»è¯­ä¹‰è§£æç”¨äºäººç‰©é‡è¯†åˆ«. åœ¨ ECCV, ç¬¬ 346â€“363 é¡µ. Springer, 2020 å¹´ã€‚'
- en: '[63] Y.Â Sun etÂ al. Perceive where to focus: Learning visibility-aware part-level
    features for partial person re-identification. In CVPR, pp. 393â€“402, 2019.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Y. Sun ç­‰äºº. è¯†åˆ«èšç„¦åŒºåŸŸ: å­¦ä¹ å¯è§æ€§æ„ŸçŸ¥çš„éƒ¨ä»¶çº§ç‰¹å¾ç”¨äºéƒ¨åˆ†äººç‰©é‡è¯†åˆ«. åœ¨ CVPR, ç¬¬ 393â€“402 é¡µ, 2019 å¹´ã€‚'
- en: '[64] L.Â Huo etÂ al. Attentive part-aware networks for partial person re-identification.
    In ICPR, pp. 3652â€“3659\. IEEE, 2021.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] L. Huo ç­‰äºº. å…³æ³¨éƒ¨ä»¶æ„ŸçŸ¥ç½‘ç»œç”¨äºéƒ¨åˆ†äººç‰©é‡è¯†åˆ«. åœ¨ ICPR, ç¬¬ 3652â€“3659 é¡µ. IEEE, 2021 å¹´ã€‚'
- en: '[65] S.Â Li etÂ al. Diversity regularized spatiotemporal attention for video-based
    person re-identification. In CVPR, pp. 369â€“378, 2018.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] S. Li ç­‰äºº. å¤šæ ·æ€§æ­£åˆ™åŒ–çš„æ—¶ç©ºæ³¨æ„åŠ›ç”¨äºåŸºäºè§†é¢‘çš„äººç‰©é‡è¯†åˆ«. åœ¨ CVPR, ç¬¬ 369â€“378 é¡µ, 2018 å¹´ã€‚'
- en: '[66] G.Â Wang etÂ al. Self-guided body part alignment with relation transformers
    for occluded person re-identification. IEEE Signal Processing Letters, 28:1155â€“1159,
    2021.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] G. Wang ç­‰äºº. å¸¦æœ‰å…³ç³»å˜æ¢å™¨çš„è‡ªå¼•å¯¼èº«ä½“éƒ¨ä½å¯¹é½ç”¨äºé®æŒ¡äººç‰©é‡è¯†åˆ«. IEEE ä¿¡å·å¤„ç†é€šè®¯, 28:1155â€“1159, 2021
    å¹´ã€‚'
- en: '[67] Y.Â Li etÂ al. Diverse part discovery: Occluded person re-identification
    with part-aware transformer. In CVPR, pp. 2898â€“2907, 2021.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Y. Li ç­‰äºº. å¤šæ ·åŒ–éƒ¨ä»¶å‘ç°: å¸¦æœ‰éƒ¨ä»¶æ„ŸçŸ¥å˜æ¢å™¨çš„é®æŒ¡äººç‰©é‡è¯†åˆ«. åœ¨ CVPR, ç¬¬ 2898â€“2907 é¡µ, 2021 å¹´ã€‚'
- en: '[68] H.Â Tan etÂ al. Mhsa-net: Multihead self-attention network for occluded
    person re-identification. IEEE Trans. Neural Netw. Learn. Syst., 2022.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] H. Tan ç­‰äºº. Mhsa-net: å¤šå¤´è‡ªæ³¨æ„åŠ›ç½‘ç»œç”¨äºé®æŒ¡äººç‰©é‡è¯†åˆ«. IEEE ç¥ç»ç½‘ç»œä¸å­¦ä¹ ç³»ç»Ÿæ±‡åˆŠ, 2022 å¹´ã€‚'
- en: '[69] A.Â Vaswani etÂ al. Attention is all you need. Advances in neural information
    processing systems, 30, 2017.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. Vaswani ç­‰äºº. å…³æ³¨å³ä½ æ‰€éœ€. ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•, 30, 2017 å¹´ã€‚'
- en: '[70] J.Â Jiang etÂ al. Dynamic hypergraph neural networks. In IJCAI Int. Joint
    Conf. Artif. Intell., pp. 2635â€“2641, 2019.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. Jiang ç­‰. åŠ¨æ€è¶…å›¾ç¥ç»ç½‘ç»œ. In IJCAI Int. Joint Conf. Artif. Intell., pp. 2635â€“2641,
    2019.'
- en: '[71] Z.Â Cao etÂ al. Realtime multi-person 2d pose estimation using part affinity
    fields. In CVPR, pp. 7291â€“7299, 2017.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Z. Cao ç­‰. å®æ—¶å¤šäººäººä½“2Då§¿æ€ä¼°è®¡ä½¿ç”¨éƒ¨ä»¶äº²å’Œåœº. In CVPR, pp. 7291â€“7299, 2017.'
- en: '[72] K.Â Gong etÂ al. Look into person: Self-supervised structure-sensitive learning
    and a new benchmark for human parsing. In CVPR, pp. 932â€“940, 2017.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] K. Gong ç­‰. è§‚å¯Ÿè¡Œäºº: è‡ªç›‘ç£ç»“æ„æ•æ„Ÿå­¦ä¹ å’Œä¸€ä¸ªæ–°åŸºå‡†ç”¨äºäººä½“è§£æ. In CVPR, pp. 932â€“940, 2017.'
- en: '[73] C.Â Szegedy etÂ al. Rethinking the inception architecture for computer vision.
    In CVPR, pp. 2818â€“2826, 2016.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] C. Szegedy ç­‰. é‡æ–°æ€è€ƒè®¡ç®—æœºè§†è§‰ä¸­çš„Inceptionæ¶æ„. In CVPR, pp. 2818â€“2826, 2016.'
- en: '[74] X.Â Li etÂ al. Deepsaliency: Multi-task deep neural network model for salient
    object detection. IEEE Trans. Image Process., 25(8):3919â€“3930, 2016.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] X. Li ç­‰. Deepsaliency: å¤šä»»åŠ¡æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ç”¨äºæ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹. IEEE Trans. Image Process.,
    25(8):3919â€“3930, 2016.'
- en: '[75] K.Â Sun etÂ al. Deep high-resolution representation learning for human pose
    estimation. In CVPR, pp. 5693â€“5703, 2019.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] K. Sun ç­‰. æ·±åº¦é«˜åˆ†è¾¨ç‡è¡¨ç¤ºå­¦ä¹ ç”¨äºäººä½“å§¿æ€ä¼°è®¡. In CVPR, pp. 5693â€“5703, 2019.'
- en: '[76] R.Â A. GÃ¼ler etÂ al. Densepose: Dense human pose estimation in the wild.
    In CVPR, pp. 7297â€“7306, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] R. A. GÃ¼ler ç­‰. Densepose: é‡å¤–å¯†é›†äººä½“å§¿æ€ä¼°è®¡. In CVPR, pp. 7297â€“7306, 2018.'
- en: '[77] X.Â Liang etÂ al. Look into person: Joint body parsing & pose estimation
    network and a new benchmark. IEEE Trans. Pattern Anal. Mach. Intell., 41(4):871â€“885,
    2018.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] X. Liang ç­‰. è§‚å¯Ÿè¡Œäºº: è”åˆäººä½“è§£æä¸å§¿æ€ä¼°è®¡ç½‘ç»œå’Œä¸€ä¸ªæ–°åŸºå‡†. IEEE Trans. Pattern Anal. Mach.
    Intell., 41(4):871â€“885, 2018.'
- en: '[78] J.Â Fu etÂ al. Dual attention network for scene segmentation. In CVPR, pp.
    3146â€“3154, 2019.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] J. Fu ç­‰. åŒé‡æ³¨æ„åŠ›ç½‘ç»œç”¨äºåœºæ™¯åˆ†å‰². In CVPR, pp. 3146â€“3154, 2019.'
- en: '[79] T.-Y. Lin etÂ al. Microsoft coco: Common objects in context. In ECCV, pp.
    740â€“755\. Springer, 2014.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] T.-Y. Lin ç­‰. Microsoft coco: ä¸Šä¸‹æ–‡ä¸­çš„å¸¸è§ç‰©ä½“. In ECCV, pp. 740â€“755. Springer,
    2014.'
- en: '[80] F.Â Zheng etÂ al. Pyramidal person re-identification via multi-loss dynamic
    training. In CVPR, pp. 8514â€“8522, 2019.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] F. Zheng ç­‰. é€šè¿‡å¤šæŸå¤±åŠ¨æ€è®­ç»ƒçš„é‡‘å­—å¡”è¡Œäººé‡è¯†åˆ«. In CVPR, pp. 8514â€“8522, 2019.'
- en: '[81] L.Â He etÂ al. Foreground-aware pyramid reconstruction for alignment-free
    occluded person re-identification. In ICCV, pp. 8450â€“8459, 2019.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] L. He ç­‰. å‰æ™¯æ„ŸçŸ¥é‡‘å­—å¡”é‡å»ºç”¨äºå¯¹é½æ— å…³çš„é®æŒ¡è¡Œäººé‡è¯†åˆ«. In ICCV, pp. 8450â€“8459, 2019.'
- en: '[82] Y.Â Zhong etÂ al. Robust partial matching for person search in the wild.
    In CVPR, pp. 6827â€“6835, 2020.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Zhong ç­‰. é’ˆå¯¹é‡å¤–åœºæ™¯çš„ç¨³å¥éƒ¨åˆ†åŒ¹é…. In CVPR, pp. 6827â€“6835, 2020.'
- en: '[83] T.Â He etÂ al. Partial person re-identification with part-part correspondence
    learning. In CVPR, pp. 9105â€“9115, 2021.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] T. He ç­‰. åŸºäºéƒ¨åˆ†-éƒ¨åˆ†å¯¹åº”å­¦ä¹ çš„éƒ¨åˆ†è¡Œäººé‡è¯†åˆ«. In CVPR, pp. 9105â€“9115, 2021.'
- en: '[84] H.Â Luo etÂ al. Stnreid: Deep convolutional networks with pairwise spatial
    transformer networks for partial person re-identification. IEEE Trans. Multimedia,
    22(11):2905â€“2913, 2020.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] H. Luo ç­‰. Stnreid: ä½¿ç”¨é…å¯¹ç©ºé—´å˜æ¢ç½‘ç»œçš„æ·±åº¦å·ç§¯ç½‘ç»œè¿›è¡Œéƒ¨åˆ†è¡Œäººé‡è¯†åˆ«. IEEE Trans. Multimedia,
    22(11):2905â€“2913, 2020.'
- en: '[85] S.Â Yu etÂ al. Neighbourhood-guided feature reconstruction for occluded
    person re-identification. arXiv preprint arXiv:2105.07345, 2021.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. Yu ç­‰. é‚»åŸŸå¼•å¯¼ç‰¹å¾é‡å»ºç”¨äºé®æŒ¡è¡Œäººé‡è¯†åˆ«. arXiv preprint arXiv:2105.07345, 2021.'
- en: '[86] J.Â Yang etÂ al. Learning to know where to see: A visibility-aware approach
    for occluded person re-identification. In ICCV, pp. 11885â€“11894, 2021.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. Yang ç­‰. å­¦ä¹ çŸ¥é“åœ¨å“ªé‡ŒæŸ¥çœ‹: ä¸€ç§é’ˆå¯¹é®æŒ¡è¡Œäººé‡è¯†åˆ«çš„å¯è§æ€§æ„ŸçŸ¥æ–¹æ³•. In ICCV, pp. 11885â€“11894, 2021.'
- en: '[87] G.Â Chen etÂ al. Spatial-temporal attention-aware learning for video-based
    person re-identification. IEEE Trans. Image Process., 28(9):4192â€“4205, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] G. Chen ç­‰. åŸºäºè§†é¢‘çš„äººä½“é‡è¯†åˆ«çš„æ—¶ç©ºæ³¨æ„åŠ›æ„ŸçŸ¥å­¦ä¹ . IEEE Trans. Image Process., 28(9):4192â€“4205,
    2019.'
- en: '[88] X.Â Zhang etÂ al. Semantic-aware occlusion-robust network for occluded person
    re-identification. IEEE Trans. Circuits Syst. Video Technol., 31(7):2764â€“2778,
    2021.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] X. Zhang ç­‰. è¯­ä¹‰æ„ŸçŸ¥çš„é®æŒ¡é²æ£’ç½‘ç»œç”¨äºé®æŒ¡è¡Œäººé‡è¯†åˆ«. IEEE Trans. Circuits Syst. Video Technol.,
    31(7):2764â€“2778, 2021.'
- en: '[89] Z.Â Zhong etÂ al. Random erasing data augmentation. In AAAI Conf. Artif.
    Intell., volumeÂ 34, pp. 13001â€“13008, 2020.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Z. Zhong ç­‰. éšæœºæ“¦é™¤æ•°æ®å¢å¼º. In AAAI Conf. Artif. Intell., volume 34, pp. 13001â€“13008,
    2020.'
- en: '[90] C.Â Zhao etÂ al. Incremental generative occlusion adversarial suppression
    network for person reid. IEEE Trans. Image Process., 30:4212â€“4224, 2021.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] C. Zhao ç­‰. å¢é‡ç”Ÿæˆé®æŒ¡å¯¹æŠ—æŠ‘åˆ¶ç½‘ç»œç”¨äºè¡Œäººé‡è¯†åˆ«. IEEE Trans. Image Process., 30:4212â€“4224,
    2021.'
- en: '[91] C.Â Yan etÂ al. Occluded person re-identification with single-scale global
    representations. In ICCV, pp. 11875â€“11884, 2021.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] C. Yan ç­‰. å•å°ºåº¦å…¨å±€è¡¨ç¤ºçš„é®æŒ¡è¡Œäººé‡è¯†åˆ«. In ICCV, pp. 11875â€“11884, 2021.'
- en: '[92] M.Â Jia etÂ al. Learning disentangled representation implicitly via transformer
    for occluded person re-identification. IEEE Trans. Multimedia, 2022.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] M. Jia ç­‰. é€šè¿‡å˜æ¢å™¨éšå¼å­¦ä¹ è§£è€¦è¡¨ç¤ºç”¨äºé®æŒ¡äººç‰©å†è¯†åˆ«. IEEE å¤šåª’ä½“æœŸåˆŠ, 2022å¹´ã€‚'
- en: '[93] Z.Â Wang etÂ al. Feature erasing and diffusion network for occluded person
    re-identification. In CVPR, pp. 4754â€“4763, 2022.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Z. Wang ç­‰. ç”¨äºé®æŒ¡äººç‰©å†è¯†åˆ«çš„ç‰¹å¾æ“¦é™¤ä¸æ‰©æ•£ç½‘ç»œ. åœ¨ CVPR ä¼šè®®ä¸Šï¼Œé¡µç  4754â€“4763, 2022å¹´ã€‚'
- en: '[94] M.Â Zheng etÂ al. Re-identification with consistent attentive siamese networks.
    In CVPR, pp. 5735â€“5744, 2019.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] M. Zheng ç­‰. ä½¿ç”¨ä¸€è‡´çš„æ³¨æ„åŠ›å­ªç”Ÿç½‘ç»œè¿›è¡Œå†è¯†åˆ«. åœ¨ CVPR ä¼šè®®ä¸Šï¼Œé¡µç  5735â€“5744, 2019å¹´ã€‚'
- en: '[95] S.Â Zhao etÂ al. Do not disturb me: Person re-identification under the interference
    of other pedestrians. In ECCV, pp. 647â€“663\. Springer, 2020.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] S. Zhao ç­‰. ä¸è¦æ‰“æ‰°æˆ‘ï¼šåœ¨å…¶ä»–è¡Œäººå¹²æ‰°ä¸‹çš„äººç‰©å†è¯†åˆ«. åœ¨ ECCV ä¼šè®®ä¸Šï¼Œé¡µç  647â€“663ã€‚Springer, 2020å¹´ã€‚'
- en: '[96] H.Â Tan etÂ al. Incomplete descriptor mining with elastic loss for person
    re-identification. IEEE Trans. Circuits Syst. Video Technol., 2021.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] H. Tan ç­‰. å¸¦æœ‰å¼¹æ€§æŸå¤±çš„ä¸å®Œæ•´æè¿°ç¬¦æŒ–æ˜ç”¨äºäººç‰©å†è¯†åˆ«. IEEE æœŸåˆŠ. ç”µè·¯ç³»ç»Ÿä¸è§†é¢‘æŠ€æœ¯, 2021å¹´ã€‚'
- en: '[97] M.Â Kim etÂ al. Occluded person re-identification via relational adaptive
    feature correction learning. In ICASSP IEEE Int Conf Acoust Speech Signal Process
    Proc, pp. 2719â€“2723\. IEEE, 2022.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] M. Kim ç­‰. é€šè¿‡å…³ç³»è‡ªé€‚åº”ç‰¹å¾ä¿®æ­£å­¦ä¹ è¿›è¡Œé®æŒ¡äººç‰©å†è¯†åˆ«. åœ¨ ICASSP IEEE å›½é™…å£°å­¦ã€è¯­éŸ³å’Œä¿¡å·å¤„ç†ä¼šè®®è®ºæ–‡é›†ä¸­ï¼Œé¡µç 
    2719â€“2723ã€‚IEEE, 2022å¹´ã€‚'
- en: '[98] T.Â Ruan etÂ al. Devil in the details: Towards accurate single and multiple
    human parsing. In AAAI Conf. Artif. Intell., volumeÂ 33, pp. 4814â€“4821, 2019.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] T. Ruan ç­‰. ç»†èŠ‚ä¸­çš„æ¶é­”ï¼šæœç€å‡†ç¡®çš„å•äººå’Œå¤šäººè§£æè¿ˆè¿›. åœ¨ AAAI äººå·¥æ™ºèƒ½ä¼šè®®ä¸Šï¼Œç¬¬33å·ï¼Œé¡µç  4814â€“4821, 2019å¹´ã€‚'
- en: '[99] H.Â Zhao etÂ al. Pyramid scene parsing network. In CVPR, pp. 2881â€“2890,
    2017.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] H. Zhao ç­‰. é‡‘å­—å¡”åœºæ™¯è§£æç½‘ç»œ. åœ¨ CVPR ä¼šè®®ä¸Šï¼Œé¡µç  2881â€“2890, 2017å¹´ã€‚'
- en: '[100] X.Â Zhou etÂ al. Objects as points. arXiv preprint arXiv:1904.07850, 2019.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] X. Zhou ç­‰. å¯¹è±¡ä½œä¸ºç‚¹. arXiv é¢„å°æœ¬ arXiv:1904.07850, 2019å¹´ã€‚'
- en: '[101] M.Â Yin etÂ al. Disentangled non-local neural networks. In ECCV, pp. 191â€“207\.
    Springer, 2020.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] M. Yin ç­‰. è§£è€¦éå±€éƒ¨ç¥ç»ç½‘ç»œ. åœ¨ ECCV ä¼šè®®ä¸Šï¼Œé¡µç  191â€“207ã€‚Springer, 2020å¹´ã€‚'
- en: '[102] R.Â Hou etÂ al. Vrstc: Occlusion-free video person re-identification. In
    CVPR, pp. 7183â€“7192, 2019.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] R. Hou ç­‰. Vrstcï¼šæ— é®æŒ¡çš„è§†é¢‘äººç‰©å†è¯†åˆ«. åœ¨ CVPR ä¼šè®®ä¸Šï¼Œé¡µç  7183â€“7192, 2019å¹´ã€‚'
- en: '[103] Y.Â Liu etÂ al. Spatial and temporal mutual promotion for video-based person
    re-identification. In AAAI Conf. Artif. Intell., volumeÂ 33, pp. 8786â€“8793, 2019.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Y. Liu ç­‰. åŸºäºè§†é¢‘çš„äººç‰©å†è¯†åˆ«çš„ç©ºé—´å’Œæ—¶é—´äº’ä¿ƒè¿›. åœ¨ AAAI äººå·¥æ™ºèƒ½ä¼šè®®ä¸Šï¼Œç¬¬33å·ï¼Œé¡µç  8786â€“8793, 2019å¹´ã€‚'
- en: '[104] H.Â Zhang etÂ al. Self-attention generative adversarial networks. In ICML,
    pp. 7354â€“7363\. PMLR, 2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] H. Zhang ç­‰. è‡ªæ³¨æ„åŠ›ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ. åœ¨ ICML ä¼šè®®ä¸Šï¼Œé¡µç  7354â€“7363ã€‚PMLR, 2019å¹´ã€‚'
- en: '[105] L.Â Zheng etÂ al. Scalable person re-identification: A benchmark. In ICCV,
    pp. 1116â€“1124, 2015.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] L. Zheng ç­‰. å¯æ‰©å±•çš„äººç‰©å†è¯†åˆ«ï¼šä¸€ä¸ªåŸºå‡†æµ‹è¯•. åœ¨ ICCV ä¼šè®®ä¸Šï¼Œé¡µç  1116â€“1124, 2015å¹´ã€‚'
- en: '[106] A.Â Dosovitskiy etÂ al. An image is worth 16x16 words: Transformers for
    image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] A. Dosovitskiy ç­‰. ä¸€å¼ å›¾ç‰‡å€¼ 16x16 ä¸ªè¯ï¼šç”¨äºå¤§è§„æ¨¡å›¾åƒè¯†åˆ«çš„å˜æ¢å™¨. arXiv é¢„å°æœ¬ arXiv:2010.11929,
    2020å¹´ã€‚'
- en: '| Yunjie Peng received her B.S. degree in the College of Computer and Information
    Science & College of Software from Southwest University, China, in 2018. She is
    currently a Ph.D. student in the School of Computer Science and Technology, Beihang
    University, China. Her research interests include gait recognition, perosn re-identification,
    computer vision, and machine learning. |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| å½­äº‘æ°äº2018å¹´åœ¨ä¸­å›½è¥¿å—å¤§å­¦è®¡ç®—æœºä¸ä¿¡æ¯ç§‘å­¦å­¦é™¢åŠè½¯ä»¶å­¦é™¢è·å¾—å­¦å£«å­¦ä½ã€‚ç›®å‰å¥¹æ˜¯ä¸­å›½åŒ—èˆªå¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢çš„åšå£«ç”Ÿã€‚å¥¹çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬æ­¥æ€è¯†åˆ«ã€äººç‰©å†è¯†åˆ«ã€è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ ã€‚
    |'
- en: '| Saihui Hou received the B.E. and Ph.D. degrees from University of Science
    and Technology of China in 2014 and 2019, respectively. He is currently an Assistant
    Professor with School of Artificial Intelligence, Beijing Normal University. His
    research interests include computer vision and machine learning. He recently focuses
    on gait recognition which aims to identify different people according to the walking
    patterns. |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| ä¾¯èµ›è¾‰äº2014å¹´å’Œ2019å¹´åˆ†åˆ«è·å¾—ä¸­å›½ç§‘æŠ€å¤§å­¦çš„å·¥å­¦å­¦å£«å’Œåšå£«å­¦ä½ã€‚ä»–ç›®å‰æ˜¯åŒ—äº¬å¸ˆèŒƒå¤§å­¦äººå·¥æ™ºèƒ½å­¦é™¢çš„åŠ©ç†æ•™æˆã€‚ä»–çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ ã€‚ä»–æœ€è¿‘å…³æ³¨äºæ­¥æ€è¯†åˆ«ï¼Œæ—¨åœ¨æ ¹æ®æ­¥æ€æ¨¡å¼è¯†åˆ«ä¸åŒçš„äººã€‚
    |'
- en: '| Chunshui Cao received the B.E. and Ph.D. degrees from University of Science
    and Technology of China in 2013 and 2018, respectively. During his Ph.D. study,
    he joined Center for Research on Intelligent Perception and Computing, National
    Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of
    Sciences. From 2018 to 2020, he worked as a Postdoctoral Fellow with PBC School
    of Finance, Tsinghua University. He is currently a Research Scientist with Watrix
    Technology Limited Co. Ltd. His research interests include pattern recognition,
    computer vision and machine learning. |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| æ›¹æ˜¥æ°´äº2013å¹´å’Œ2018å¹´åˆ†åˆ«è·å¾—ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦çš„å·¥å­¦å­¦å£«å’Œåšå£«å­¦ä½ã€‚åœ¨æ”»è¯»åšå£«å­¦ä½æœŸé—´ï¼Œä»–åŠ å…¥äº†ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€æ¨¡å¼è¯†åˆ«å›½å®¶å®éªŒå®¤æ™ºèƒ½æ„ŸçŸ¥ä¸è®¡ç®—ç ”ç©¶ä¸­å¿ƒã€‚2018å¹´è‡³2020å¹´ï¼Œä»–åœ¨æ¸…åå¤§å­¦äº”é“å£é‡‘èå­¦é™¢æ‹…ä»»åšå£«åç ”ç©¶å‘˜ã€‚ä»–ç›®å‰æ˜¯Watrix
    Technology Limited Co. Ltd.çš„ç ”ç©¶ç§‘å­¦å®¶ã€‚ä»–çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬æ¨¡å¼è¯†åˆ«ã€è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ ã€‚ |'
- en: '| Xu Liu received the B.S. and Ph.D. degrees in control science and engineering
    from the University of Science and Technology of China (USTC), Hefei, China, in
    2013 and 2018, respectively. He is currently an algorithm researcher at Watrix
    Technology Limited Co. Ltd. His current research interests include gait recognition,
    object detection, segmentation and deep learning. |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| åˆ˜æ—­äº2013å¹´å’Œ2018å¹´åˆ†åˆ«è·å¾—ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ï¼ˆUSTCï¼‰çš„æ§åˆ¶ç§‘å­¦ä¸å·¥ç¨‹ä¸“ä¸šçš„å­¦å£«å’Œåšå£«å­¦ä½ã€‚ä»–ç›®å‰æ˜¯Watrix Technology
    Limited Co. Ltd.çš„ç®—æ³•ç ”ç©¶å‘˜ã€‚ä»–çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬æ­¥æ€è¯†åˆ«ã€ç‰©ä½“æ£€æµ‹ã€åˆ†å‰²å’Œæ·±åº¦å­¦ä¹ ã€‚ |'
- en: '| Yongzhen Huang received the B.E. degree from Huazhong University of Science
    and Technology in 2006, and the Ph.D. degree from Institute of Automation, Chinese
    Academy of Sciences in 2011. He is currently an Associate Professor with School
    of Artificial Intelligence, Beijing Normal University. He has published one book
    and more than 80 papers at international journals and conferences such as TPAMI,
    IJCV, TIP, TSMCB, TMM, TCSVT, CVPR, ICCV, ECCV, NIPS, AAAI. His research interests
    include pattern recognition, computer vision and machine learning. |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| é»„æ°¸çäº2006å¹´è·å¾—åä¸­ç§‘æŠ€å¤§å­¦å·¥å­¦å­¦å£«å­¦ä½ï¼Œ2011å¹´è·å¾—ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€åšå£«å­¦ä½ã€‚ä»–ç›®å‰æ˜¯åŒ—äº¬å¸ˆèŒƒå¤§å­¦äººå·¥æ™ºèƒ½å­¦é™¢çš„å‰¯æ•™æˆã€‚ä»–å·²ç»å‡ºç‰ˆäº†ä¸€æœ¬ä¹¦ï¼Œå¹¶åœ¨å›½é™…æœŸåˆŠå’Œä¼šè®®ä¸Šå‘è¡¨äº†80å¤šç¯‡è®ºæ–‡ï¼Œå¦‚TPAMIã€IJCVã€TIPã€TSMCBã€TMMã€TCSVTã€CVPRã€ICCVã€ECCVã€NIPSã€AAAIã€‚ä»–çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬æ¨¡å¼è¯†åˆ«ã€è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ ã€‚
    |'
- en: '| Zhiqiang He is currently the Senior Vice President of Lenovo Company and
    President of Lenovo Capital and Incubator Group. This group is responsible for
    exploring external innovation as well as accelerating internal innovation for
    Lenovo Group, leveraging Lenovo global resources, power of capital, and entrepreneurship.
    Previously, he was the Chief Technology Officer and held various leadership positions
    in Lenovo, particularly in overseeing LenovoÃ¢s Research & Technology initiatives
    and systems. He is a doctoral supervisor at the Institute of Computing Technology,
    Chinese Academy of Sciences and Beihang University. |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| ä½•å¿—å¼ºç›®å‰æ˜¯è”æƒ³å…¬å¸é«˜çº§å‰¯æ€»è£å’Œè”æƒ³èµ„æœ¬ä¸å­µåŒ–å™¨é›†å›¢ä¸»å¸­ã€‚è¯¥é›†å›¢è´Ÿè´£æ¢ç´¢å¤–éƒ¨åˆ›æ–°ä»¥åŠåŠ é€Ÿè”æƒ³é›†å›¢å†…éƒ¨åˆ›æ–°ï¼Œåˆ©ç”¨è”æƒ³çš„å…¨çƒèµ„æºã€èµ„æœ¬åŠ›é‡å’Œåˆ›ä¸šç²¾ç¥ã€‚æ­¤å‰ï¼Œä»–æ›¾æ‹…ä»»è”æƒ³çš„é¦–å¸­æŠ€æœ¯å®˜ï¼Œå¹¶åœ¨è”æƒ³çš„ç ”ç©¶ä¸æŠ€æœ¯é¢†åŸŸä»¥åŠç³»ç»Ÿæ–¹é¢æ‹…ä»»è¿‡å¤šä¸ªé¢†å¯¼èŒä½ã€‚ä»–æ˜¯ä¸­å›½ç§‘å­¦é™¢è®¡ç®—æŠ€æœ¯ç ”ç©¶æ‰€å’ŒåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦çš„åšå£«ç”Ÿå¯¼å¸ˆã€‚
    |'
