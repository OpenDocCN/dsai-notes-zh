- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:56:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:56:06
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2104.01789] Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks
    and Defenses'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2104.01789] 基于深度学习的自动驾驶系统：攻击与防御的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2104.01789](https://ar5iv.labs.arxiv.org/html/2104.01789)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2104.01789](https://ar5iv.labs.arxiv.org/html/2104.01789)
- en: 'Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的自动驾驶系统：攻击与防御的调查
- en: 'Yao Deng,  Tiehua Zhang,  Guannan Lou,  Xi Zheng,  Jiong Jin,  and Qing-Long Han
    This work was supported in part by the Australian Research Council Linkage Project
    under Grant LP190100676.Yao Deng, Guannan Lou and Xi Zheng are with the Department
    of Computing, Macquarie University, Sydney, NSW, 2109 Australia (e-mail: yao.deng@hdr.mq.edu.au;
    lougnroy@gmail.com; james.zheng@mq.edu.au).Tiehua Zhang, Jiong Jin and Qing-Long Han
    are with the School of Software and Electrical Engineering, Swinburne University
    of Technology, Melbourne, VIC, 3122 Australia (e-mail: tiehuazhang@swin.edu.au;
    jiongjin@swin.edu.au; qhan@swin.edu.au).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yao Deng、Tiehua Zhang、Guannan Lou、Xi Zheng、Jiong Jin 和 Qing-Long Han 这项工作部分由澳大利亚研究委员会联接项目资助，资助编号
    LP190100676。Yao Deng、Guannan Lou 和 Xi Zheng 现为澳大利亚悉尼麦考瑞大学计算系成员（电子邮件：yao.deng@hdr.mq.edu.au;
    lougnroy@gmail.com; james.zheng@mq.edu.au）。Tiehua Zhang、Jiong Jin 和 Qing-Long
    Han 现为澳大利亚墨尔本斯温伯恩科技大学软件与电气工程学院成员（电子邮件：tiehuazhang@swin.edu.au; jiongjin@swin.edu.au;
    qhan@swin.edu.au）。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The rapid development of artificial intelligence, especially deep learning technology,
    has advanced autonomous driving systems (ADSs) by providing precise control decisions
    to counterpart almost any driving event, spanning from anti-fatigue safe driving
    to intelligent route planning. However, ADSs are still plagued by increasing threats
    from different attacks, which could be categorized into physical attacks, cyberattacks
    and learning-based adversarial attacks. Inevitably, the safety and security of
    deep learning-based autonomous driving are severely challenged by these attacks,
    from which the countermeasures should be analyzed and studied comprehensively
    to mitigate all potential risks. This survey provides a thorough analysis of different
    attacks that may jeopardize ADSs, as well as the corresponding state-of-the-art
    defense mechanisms. The analysis is unrolled by taking an in-depth overview of
    each step in the ADS workflow, covering adversarial attacks for various deep learning
    models and attacks in both physical and cyber context. Furthermore, some promising
    research directions are suggested in order to improve deep learning-based autonomous
    driving safety, including model robustness training, model testing and verification,
    and anomaly detection based on cloud/edge servers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的快速发展，特别是深度学习技术，推动了自动驾驶系统（ADSs）的进步，提供了几乎可以应对任何驾驶事件的精确控制决策，从抗疲劳安全驾驶到智能路线规划。然而，ADSs
    仍然面临来自不同攻击的日益增加的威胁，这些威胁可以分为物理攻击、网络攻击和基于学习的对抗攻击。不可避免地，基于深度学习的自动驾驶的安全性和保障受到这些攻击的严重挑战，必须全面分析和研究应对措施，以减少所有潜在风险。该调查提供了对可能危害ADSs的不同攻击的深入分析，以及相应的最先进防御机制。分析通过深入概述ADS工作流程中的每一步展开，涵盖了各种深度学习模型的对抗攻击以及物理和网络环境中的攻击。此外，建议了一些有前景的研究方向，以提高基于深度学习的自动驾驶安全性，包括模型鲁棒性训练、模型测试和验证，以及基于云/边缘服务器的异常检测。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Autonomous driving, deep learning, cyberattacks, adversarial attacks, defenses
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶，深度学习，网络攻击，对抗攻击，防御
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: With the development of artificial intelligence technologies, autonomous driving
    has been receiving considerable attention in both academia and industry. From
    1987 to 1995, the Eureka PROMETHEUS Project (PROgraMme for a European Traffic
    of Highest Efficiency and Unprecedented Safety) [[1](#bib.bib1)], one of the earliest
    autonomous driving projects, was carried out by Daimler-Benz. In 2005, a famous
    autonomous driving competition called DARPA [[2](#bib.bib2)] Grand Challenge was
    organized. Since then, numerous development/refinement on advanced autonomous
    driving systems (ADSs) have been proposed. For now, autonomous vehicles are still
    going through the transformation through five levels, from level 0 (no automation)
    to level 4 (high self-driving automation). Most of companies like Tesla [[3](#bib.bib3)]
    focus on the development of level 3 ADSs that could achieve limited self-driving
    in some conditions (e.g., on highway). The top runner Google Waymo [[4](#bib.bib4)]
    is currently committed to research and industrializing on Level 4 ADSs that do
    not require human interaction in most circumstances. More importantly, a consensus
    has been reached that the advent of autonomous vehicles will improve people’s
    driving experience significantly. However, research on self-driving vehicles is
    still in its infancy stage. Some critical issues, especially for issues related
    to safety, need to be well tackled before proceeding to the full-scale of industrialization.
    For instance, the recent Uber’s vehicle’s fatal accident [[5](#bib.bib5)] reveals
    the importance of prioritizing the research on the safety of autonomous driving.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能技术的发展，自主驾驶在学术界和工业界都受到相当大的关注。从1987年到1995年，最早的自主驾驶项目之一——Eureka PROMETHEUS
    项目（PROgraMme for a European Traffic of Highest Efficiency and Unprecedented Safety）[[1](#bib.bib1)]由戴姆勒-奔驰公司实施。2005年，举办了一场著名的自主驾驶竞赛——DARPA
    [[2](#bib.bib2)] Grand Challenge。从那时起，许多先进自主驾驶系统（ADSs）的开发和改进方案陆续提出。目前，自主驾驶汽车仍在经历从第0级（无自动化）到第4级（高度自动驾驶）五个级别的转型。像特斯拉
    [[3](#bib.bib3)] 这样的公司大多专注于开发第3级ADS，这些系统能够在某些条件下（如高速公路上）实现有限的自动驾驶。领先者谷歌Waymo [[4](#bib.bib4)]
    目前致力于研究和工业化第4级ADS，这些系统在大多数情况下不需要人类干预。更重要的是，人们已经达成共识，认为自主驾驶汽车的出现将显著改善人们的驾驶体验。然而，自主驾驶汽车的研究仍处于初级阶段。一些关键问题，尤其是与安全相关的问题，需要在全面工业化之前得到妥善解决。例如，最近Uber车辆的致命事故
    [[5](#bib.bib5)] 显示了优先研究自主驾驶安全性的重要性。
- en: 'Deep learning, the most popular technique of artificial intelligence, is widely
    applied in autonomous vehicles to fulfill different perception tasks as well as
    making real-time decisions. Figure [1](#S2.F1 "Figure 1 ‣ II Workflow of deep
    learning-based ADSs ‣ Deep Learning-Based Autonomous Driving Systems: A Survey
    of Attacks and Defenses") demonstrates the workflow and architecture of a deep
    learning-based ADS. In a nutshell, raw data collected by diverse sensors and high-definition
    (HD) map information from the cloud are first fed into deep learning models in
    the perception layer to extract the ambient information of the environment, after
    which different designated deep/reinforcement learning models in the decision
    layer kicks off the real-time decisions making process. For example, in Baidu
    Apollo [[6](#bib.bib6)], which is the ADS applied in Baidu Go Robotaxi service [[7](#bib.bib7)],
    several deep learning models are used in perception and decision modules. Tesla
    also deploys advanced AI models for object detection to implement Autopilot [[8](#bib.bib8)].
    However, there exist a number of issues against the further development of deep
    learning-based ADSs adopting this pipeline structure. First of all, sensors are
    vulnerable to numerous physical attacks, under which most of the sensors are no
    longer able to function as normal to collect data in good quality, or they may
    be adversely instructed to collect fake data, leading to a severe degradation
    of performance of all learning-based models in the following layers. Furthermore,
    recent research shows that deep neural networks are vulnerable to adversarial
    attacks [[9](#bib.bib9)] that are designed specifically to induce learning-based
    models to wrong predictions. The most common adversarial attack is by constructing
    the so-called adversarial examples that only have slight difference from the original
    inputs to baffle the neurons in the model. There are some results available from
    prior research literature that focus on investigating such adversarial attacks
    [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)], exhibiting the level of
    significance of these threats to the safety of deep learning-based ADSs.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习，作为人工智能中最流行的技术，广泛应用于自动驾驶车辆以完成不同的感知任务以及进行实时决策。图 [1](#S2.F1 "Figure 1 ‣ II
    Workflow of deep learning-based ADSs ‣ Deep Learning-Based Autonomous Driving
    Systems: A Survey of Attacks and Defenses") 展示了基于深度学习的自动驾驶系统（ADS）的工作流程和架构。简而言之，由各种传感器收集的原始数据和来自云端的高清（HD）地图信息首先被输入到感知层的深度学习模型中，以提取环境的周围信息，然后，决策层中的不同指定的深度/强化学习模型启动实时决策过程。例如，在百度
    Apollo [[6](#bib.bib6)] 中，该系统应用于百度 Go Robotaxi 服务 [[7](#bib.bib7)]，在感知和决策模块中使用了多种深度学习模型。特斯拉也部署了先进的
    AI 模型进行物体检测，以实现 Autopilot [[8](#bib.bib8)]。然而，采用这种管道结构的基于深度学习的 ADSs 进一步发展的过程中存在许多问题。首先，传感器容易受到多种物理攻击，在这些攻击下，大多数传感器无法正常工作，无法以良好的质量收集数据，或者可能被恶意指令收集虚假数据，导致所有后续层的学习模型性能严重下降。此外，最近的研究表明，深度神经网络容易受到对抗性攻击
    [[9](#bib.bib9)]，这些攻击专门设计用来引导学习模型产生错误的预测。最常见的对抗性攻击是构造所谓的对抗样本，这些样本与原始输入仅有微小差异，以迷惑模型中的神经元。先前的研究文献中有一些结果集中在研究这些对抗性攻击
    [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)]，显示了这些威胁对基于深度学习的 ADSs 安全性的重大影响。'
- en: The potential risks of ADSs have the effects on the development and deployment
    of autonomous vehicles in industry. If autonomous vehicles cannot ensure safety
    when they are running, they will not be accepted by the public. Therefore, it
    is essential to figure out whether deep learning-based ADSs are vulnerable, how
    they could be attacked, how much damage can be caused by attacks, and what measurements
    have been proposed to defend these attacks. The industry needs this information
    and further insights to improve their development of safety and robustness of
    ADSs. Though safety threats and defenses of autonomous vehicles and autonomous
    vehicular networks have been studied before References [[22](#bib.bib22), [23](#bib.bib23)],
    none of these investigated on security problems in deep learning-based ADSs. On
    the other hand, most of researchers on safety deep learning focus on adversarial
    attacks on the image classification task. For example, in [[24](#bib.bib24)] and [[25](#bib.bib25)],
    adversarial attacks and defenses for computer vision tasks were thoroughly introduced.
    However, related analysis on attacks and defenses on deep learning systems for
    more complicated autonomous driving tasks were not covered in these works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ADS的潜在风险对自主车辆的开发和部署具有影响。如果自主车辆在运行时无法确保安全，它们将不会被公众接受。因此，弄清楚深度学习基础的ADS是否存在漏洞、如何被攻击、攻击可能造成的损害程度以及提出了哪些防御措施是至关重要的。行业需要这些信息和进一步的见解，以提高ADS的安全性和鲁棒性。尽管之前对自主车辆和自主车辆网络的安全威胁和防御进行了研究 参考文献
    [[22](#bib.bib22), [23](#bib.bib23)]，但没有研究深度学习基础的ADS中的安全问题。另一方面，大多数安全深度学习研究集中在图像分类任务上的对抗攻击。例如，在[[24](#bib.bib24)]和[[25](#bib.bib25)]中，对计算机视觉任务的对抗攻击和防御进行了详细介绍。然而，这些工作没有涉及深度学习系统在更复杂的自主驾驶任务中的攻击和防御分析。
- en: 'Therefore, in this paper, we conduct a comprehensive survey that pulls together
    the recent research efforts on the workflow of deep learning-based ADSs, the state-of-the-art
    attacks and the corresponding defending strategies. The contributions of this
    paper are listed as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本文进行了一项全面的调查，汇集了最近对深度学习基础的ADS工作流程、最先进攻击及相应防御策略的研究努力。本文的贡献如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A variety of attacks towards the pipeline of deep learning-based ADSs are reviewed
    and analyzed in detail.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对深度学习基础的ADS管道中各种攻击进行了详细的回顾和分析。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The state-of-the-art attacks and the defending methods in deep learning-based
    ADSs are comprehensively elucidated.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度学习基础的自动驾驶系统（ADS）的最先进攻击方法及防御方法被全面阐述。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future research directions of applying new attacks as well as securing and improving
    the robustness of deep learning-based ADSs are proposed.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出了应用新攻击以及保护和提高深度学习基础的ADS鲁棒性的未来研究方向。
- en: 'The paper is organized as follows: Section [II](#S2 "II Workflow of deep learning-based
    ADSs ‣ Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and
    Defenses") introduces the detail of pipeline in deep learning-based ADSs and possible
    threat models adopted by adversaries against the systems. Section [III](#S3 "III
    Attacks in ADSs ‣ Deep Learning-Based Autonomous Driving Systems: A Survey of
    Attacks and Defenses") walks through different attacks that could occur in the
    pipeline as well as their threat models. Section [IV](#S4 "IV Defense methods
    ‣ Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses")
    summarizes defenses corresponding to the mentioned attacks and discusses their
    effectiveness in protecting ADSs. Section [V](#S5 "V Future Directions ‣ Deep
    Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses")
    reveals future research directions for securing ADSs. Section [VI](#S6 "VI Conclusion
    ‣ Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses")
    draws the conclusion.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '论文组织如下：第[II](#S2 "II Workflow of deep learning-based ADSs ‣ Deep Learning-Based
    Autonomous Driving Systems: A Survey of Attacks and Defenses")节介绍了深度学习基础的ADS的管道细节以及对系统采取的潜在威胁模型。第[III](#S3
    "III Attacks in ADSs ‣ Deep Learning-Based Autonomous Driving Systems: A Survey
    of Attacks and Defenses")节介绍了管道中可能发生的不同攻击及其威胁模型。第[IV](#S4 "IV Defense methods
    ‣ Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses")节总结了与上述攻击相关的防御方法，并讨论了这些方法在保护ADS方面的有效性。第[V](#S5
    "V Future Directions ‣ Deep Learning-Based Autonomous Driving Systems: A Survey
    of Attacks and Defenses")节揭示了未来保护ADS的研究方向。第[VI](#S6 "VI Conclusion ‣ Deep Learning-Based
    Autonomous Driving Systems: A Survey of Attacks and Defenses")节给出了结论。'
- en: II Workflow of deep learning-based ADSs
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 基于深度学习的自动驾驶系统工作流程
- en: 'A deep learning-based ADS is normally composed of three functional layers,
    including a sensing layer, a perception layer and a decision layer, as well as
    an additional cloud service layer as shown in Figure [1](#S2.F1 "Figure 1 ‣ II
    Workflow of deep learning-based ADSs ‣ Deep Learning-Based Autonomous Driving
    Systems: A Survey of Attacks and Defenses"). In the sensing layer, heterogeneous
    sensors such as GPS, camera, LiDAR, radar, and ultrasonic sensors are used to
    collect real-time ambient information including the current position and spatial-temporal
    data (e.g. time series image frames). The perception layer, on the other hand,
    contains deep learning models to analyze the data collected by the sensing layer
    and then extract useful environmental information from the raw data for further
    process. The decision layer would act as a decision-making unit to output instructions
    concerning the change of speed and steering angle based on the extracted information
    from the perception layer. The following part of this section will unveil the
    workflow of a deep learning-based ADS.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '基于深度学习的自动驾驶系统通常由三个功能层组成，包括感知层、感知层和决策层，以及一个附加的云服务层，如图[1](#S2.F1 "Figure 1 ‣
    II Workflow of deep learning-based ADSs ‣ Deep Learning-Based Autonomous Driving
    Systems: A Survey of Attacks and Defenses")所示。在感知层中，使用异构传感器如GPS、相机、激光雷达、雷达和超声波传感器来收集实时环境信息，包括当前位置和时空数据（例如时间序列图像帧）。另一方面，感知层包含深度学习模型，以分析感知层收集的数据，然后从原始数据中提取有用的环境信息以供进一步处理。决策层则作为决策单元，根据从感知层提取的信息输出有关速度和转向角度的指令。接下来的部分将揭示基于深度学习的自动驾驶系统的工作流程。'
- en: '![Refer to caption](img/1391aa0724e9c53b16e4a4821013478b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1391aa0724e9c53b16e4a4821013478b.png)'
- en: 'Figure 1: ADS architecture'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '图1: 自动驾驶系统架构'
- en: II-A The sensing layer
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 感知层
- en: The sensing layer encompasses heterogeneous sensors to collect surrounding information
    around an autonomous vehicle. The most preferred sensors adopted and deployed
    by leading autonomous driving vehicle companies like Baidu are GPS/Inertial Measurement
    Units (IMU), cameras, Light Detection and Ranging (LiDAR), Radio Detection and
    Ranging (Radar), and ultrasonic sensors. More specifically, GPS could provide
    absolute position data through the help of geostationary satellites, while IMU
    provides orientation, velocity and acceleration data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 感知层包括异构传感器，用于收集自动驾驶车辆周围的环境信息。领先的自动驾驶车辆公司，如百度，最常使用和部署的传感器包括GPS/惯性测量单元（IMU）、相机、激光雷达（LiDAR）、雷达和超声波传感器。具体来说，GPS通过地球静止卫星提供绝对位置数据，而IMU则提供方向、速度和加速度数据。
- en: Cameras are also used to capture visual information around an autonomous vehicle,
    providing abundant information to the perception layer to analyze so that the
    vehicle could recognize traffic signs and obstacles. Furthermore, LiDAR is also
    applied to help detect objects by measuring distances between objects and the
    vehicle based on the reflection of light. It is also helpful for more accurate
    real-time localization. Additionally, radar and ultrasonic sensors are also used
    to detect objects by electromagnetic pulses and ultrasonic pulse waves.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 相机也用于捕捉自动驾驶车辆周围的视觉信息，为感知层提供丰富的信息以进行分析，从而使车辆能够识别交通标志和障碍物。此外，激光雷达也用于通过测量物体与车辆之间的距离来帮助检测物体，该距离基于光的反射。这对于更精确的实时定位也很有帮助。此外，雷达和超声波传感器也通过电磁脉冲和超声波脉冲波来检测物体。
- en: II-B The perception layer
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 感知层
- en: In the perception layer, semantic information is extracted from raw data by
    algorithms such as optical flow [[26](#bib.bib26)] and deep learning models. Currently,
    image data from cameras and cloud point data from LiDAR are widely used by deep
    learning models in the perception layer for various tasks such as localization,
    object detection and semantic segmentation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在感知层中，通过光流[[26](#bib.bib26)]和深度学习模型等算法从原始数据中提取语义信息。目前，摄像头拍摄的图像数据和激光雷达获得的点云数据被广泛应用于感知层的深度学习模型中，用于各种任务，如定位、物体检测和语义分割。
- en: II-B1 Localization
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 定位
- en: Localization plays a critical role in the route planning task in an ADS. By
    leveraging localization technologies, the autonomous vehicle is capable of obtaining
    its accurate location on the map and understand the real-time ambient environment.
    Currently, localization is mostly implemented by the fused data from GPS, IMU,
    LiDAR point clouds, and HD map. Specifically, the fused data is used for odometry
    estimation and map reconstruction tasks. These tasks aim to estimate the movement
    of an autonomous vehicle, reconstruct the map of the vehicle’s surroundings, and
    finally determine the current location of the vehicle. In [[27](#bib.bib27)],
    CNN and RNN were used to estimate the movement and poses of a vehicle, through
    continuous images taken by a camera. In [[28](#bib.bib28)], a deep autoencoder
    was applied to encode observed images into a compact format for map reconstruction
    and localization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 定位在自动驾驶系统中的路线规划任务中扮演着至关重要的角色。通过利用定位技术，自动驾驶车辆能够获得其在地图上的准确位置，并了解实时的环境信息。目前，定位主要是通过GPS、IMU、LiDAR点云和高清地图的融合数据来实现的。具体来说，这些融合数据用于里程估计和地图重建任务。这些任务旨在估计自动驾驶车辆的运动，重建车辆周围的地图，并最终确定车辆的当前位置。在[[27](#bib.bib27)]中，CNN和RNN被用来通过相机拍摄的连续图像来估计车辆的运动和姿态。在[[28](#bib.bib28)]中，应用了深度自编码器将观察到的图像编码成紧凑的格式，用于地图重建和定位。
- en: II-B2 Road object detection and recognition
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 道路物体检测与识别
- en: Road object detection is a key issue for autonomous vehicles owing to the complexity
    of detecting large amounts of objects with different shape such as lanes, traffic
    signs, other vehicles, and pedestrians correctly in real time and ever-changing
    surrounding environments. In the object detection field, Faster RCNN [[31](#bib.bib31)]
    is considered effective to detect objects in images. You Only Look Once (YOLO)
    [[32](#bib.bib32)] is another famous object detection algorithm that converts
    the detection task to a regression issue. Currently, LiDAR-based object detection
    deep learning models are studied extensively by both researchers and industry
    practitioners. VoxelNet [[33](#bib.bib33)] is the first end-to-end model that
    directly predicts objects based on LiDAR point cloud. PointRCNN [[34](#bib.bib34)]
    adapts the architecture of RCNN to take 3D point cloud as input for object detection
    and achieves a superior performance.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 道路物体检测是自动驾驶车辆面临的一个关键问题，因为在实时和不断变化的环境中正确检测大量不同形状的物体，如车道、交通标志、其他车辆和行人，具有很大的复杂性。在物体检测领域，Faster
    RCNN [[31](#bib.bib31)] 被认为是一种有效的图像物体检测方法。You Only Look Once (YOLO) [[32](#bib.bib32)]
    是另一种著名的物体检测算法，它将检测任务转换为回归问题。目前，基于LiDAR的物体检测深度学习模型受到研究人员和行业从业者的广泛研究。VoxelNet [[33](#bib.bib33)]
    是第一个端到端的模型，直接基于LiDAR点云预测物体。PointRCNN [[34](#bib.bib34)] 将RCNN的架构适应为以3D点云作为输入进行物体检测，并取得了优异的性能。
- en: II-B3 Semantic segmentation
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B3 语义分割
- en: Semantic segmentation in autonomous driving semantically segments different
    parts of an image into specific classes such as vehicles, pedestrians and ground.
    It is helpful for localizing the vehicle, detecting objects, marking lanes and
    reconstructing the map. In the semantic segmentation field, Fully Convolutional
    Network (FCN) [[35](#bib.bib35)] is a basic deep learning model able to achieve
    good performance, which essentially modifies the fully connected layer in a normal
    CNN to convolutional layer. Also, PSPNet [[36](#bib.bib36)] is a famous semantic
    segmentation network that applies a Pyramid pooling architecture to better extract
    information from images.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割在自动驾驶中将图像的不同部分语义地分割成特定类别，如车辆、行人和地面。这有助于定位车辆、检测物体、标记车道和重建地图。在语义分割领域，Fully
    Convolutional Network (FCN) [[35](#bib.bib35)] 是一个基本的深度学习模型，能够实现良好的性能，其本质上将普通CNN中的全连接层修改为卷积层。此外，PSPNet
    [[36](#bib.bib36)] 是一个著名的语义分割网络，应用了金字塔池化架构来更好地从图像中提取信息。
- en: II-C The cloud service
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 云服务
- en: The cloud server is commonly used as a service provider for many resource-reliant
    services in the autonomous driving field. First, a prior HD Map, which could be
    deployed at the cloud, is constructed by autonomous driving companies using LiDAR
    as well as other sensors. The HD Map contains much valuable information like road
    lanes, signs and obstacles. Therefore, the vehicle could use such data to initiate
    the pre-route planning and enhance the perception of the surrounding environment.
    Meanwhile, real-time raw data and perception data of other autonomous vehicles
    could be uploaded to the cloud by Vehicle to Everything (V2X) service to help
    keep HD Maps up-to-date, enabling HD Maps to provide more relevant real-time information
    such as surrounding vehicles on the same road. On the other hand, all deep learning
    models applied in an autonomous vehicle are first trained on the cloud in a simulation
    environment. When these models are verified, the cloud provides Over-the-Air (OTA)
    update to upgrade their software and deep learning models in autonomous vehicles
    remotely.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务器通常被用作自动驾驶领域许多依赖资源的服务的服务提供者。首先，自动驾驶公司利用LiDAR以及其他传感器在云端构建一个先验的高清地图（HD Map）。高清地图包含大量有价值的信息，如道路车道、标志和障碍物。因此，车辆可以利用这些数据启动预路规划并增强对周围环境的感知。同时，其他自动驾驶车辆的实时原始数据和感知数据可以通过车联网（V2X）服务上传至云端，以帮助保持高清地图的最新，使高清地图能够提供更多相关的实时信息，如同一路上的周围车辆。另一方面，所有应用于自动驾驶车辆的深度学习模型首先在云端的模拟环境中进行训练。当这些模型经过验证后，云端提供空中升级（OTA）以远程升级自动驾驶车辆的软件和深度学习模型。
- en: II-D The decision layer
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 决策层
- en: II-D1 Path planning and object trajectory prediction
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D1 路径规划与物体轨迹预测
- en: Path planning is considered as a basic task for autonomous vehicles with respect
    to deciding a route between a start location and the desired destination and the
    object trajectory prediction task requires autonomous vehicles to predict trajectories
    of perceived obstacles with the help of sensors and perception layer. Recently,
    some researchers have tried to use Inverse Reinforcement Learning in order to
    achieve a superior results in path planning. By learning reward functions from
    human drivers, the vehicle is trained to be capable of generating a route more
    like a human being [[37](#bib.bib37)]. For trajectory prediction, some variations
    of RNN and LSTM [[38](#bib.bib38)] are proposed to achieve high prediction accuracy
    and efficiency. In addition, 3D spatial-temporal data and single CNN are tried
    by Luo at al. to forecast car trajectories [[39](#bib.bib39)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 路径规划被视为自动驾驶车辆的基本任务，它涉及在起始位置和期望目的地之间决定路线，而物体轨迹预测任务要求自动驾驶车辆利用传感器和感知层预测感知障碍物的轨迹。最近，一些研究人员尝试使用逆向强化学习以在路径规划中取得更优的结果。通过学习来自人类驾驶员的奖励函数，车辆被训练成能够生成更像人类的路线[[37](#bib.bib37)]。对于轨迹预测，提出了一些RNN和LSTM的变体[[38](#bib.bib38)]以实现高预测精度和效率。此外，Luo等人尝试了3D时空数据和单一CNN来预测汽车轨迹[[39](#bib.bib39)]。
- en: II-D2 Vehicle control via deep reinforcement learning
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D2 通过深度强化学习进行车辆控制
- en: Traditional rule-based algorithms cannot simply cover all complex driving scenarios.
    Deep reinforcement learning that trains an agent to learn how to act under different
    scenarios is thus more promising in autonomous driving scenarios. In [[40](#bib.bib40)],
    a CNN-based Inverse Reinforcement Learning model to plan a driving path using
    2D and 3D data collected in many normal driving scenarios was proposed. In [[41](#bib.bib41)],
    a DQN based RL model was proposed for autonomous driving steering control.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的基于规则的算法无法简单地覆盖所有复杂的驾驶场景。因此，训练智能体在不同场景下学习如何行动的深度强化学习在自动驾驶场景中更具前景。在[[40](#bib.bib40)]中，提出了一种基于CNN的逆向强化学习模型，用于利用在许多普通驾驶场景中收集的2D和3D数据来规划驾驶路径。在[[41](#bib.bib41)]中，提出了一种基于DQN的强化学习模型用于自动驾驶的转向控制。
- en: II-D3 End-to-End driving
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D3 端到端驾驶
- en: An E2E driving model is a special deep learning model that combines the perception
    and decision processes. In this scenario, the model predicts the current steering
    angle and driving speed based on the ambient sensing information. In [[42](#bib.bib42)],
    a CNN architecture E2E driving model called DAVE-2 system takes front-face camera
    images as the input and predicts the current steering angle.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: E2E 驾驶模型是一种特殊的深度学习模型，它结合了感知和决策过程。在这种情况下，模型基于环境传感信息预测当前的转向角度和行驶速度。在 [[42](#bib.bib42)]
    中，一种名为 DAVE-2 系统的 CNN 架构 E2E 驾驶模型以前置摄像头图像作为输入，并预测当前的转向角度。
- en: III Attacks in ADSs
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 自动驾驶系统中的攻击
- en: 'In this section, we introduce various attacks towards ADSs in detail. Figure [2](#S3.F2
    "Figure 2 ‣ III Attacks in ADSs ‣ Deep Learning-Based Autonomous Driving Systems:
    A Survey of Attacks and Defenses") demonstrates the overview of attacks on each
    part in an ADS, which will be introduced in detail in this section. Table [I](#S3.T1
    "TABLE I ‣ III-A2 Spoofing attack ‣ III-A Physical attacks on sensors ‣ III Attacks
    in ADSs ‣ Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks
    and Defenses") and [II](#S3.T2 "TABLE II ‣ III-D Analysis of attacks ‣ III Attacks
    in ADSs ‣ Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks
    and Defenses") summarize both physical and adversarial attacks on ADSs.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们详细介绍了针对自动驾驶系统的各种攻击。图 [2](#S3.F2 "Figure 2 ‣ III Attacks in ADSs ‣ Deep
    Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses")
    展示了自动驾驶系统各部分攻击的概述，本节将详细介绍这些攻击。表 [I](#S3.T1 "TABLE I ‣ III-A2 Spoofing attack ‣
    III-A Physical attacks on sensors ‣ III Attacks in ADSs ‣ Deep Learning-Based
    Autonomous Driving Systems: A Survey of Attacks and Defenses") 和 [II](#S3.T2 "TABLE
    II ‣ III-D Analysis of attacks ‣ III Attacks in ADSs ‣ Deep Learning-Based Autonomous
    Driving Systems: A Survey of Attacks and Defenses") 总结了对自动驾驶系统的物理攻击和对抗攻击。'
- en: '![Refer to caption](img/a3d66d02fa77a23ccef4fc0a2dd05376.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a3d66d02fa77a23ccef4fc0a2dd05376.png)'
- en: 'Figure 2: An overview of attacks on each part in an ADS'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：自动驾驶系统各部分攻击的概述
- en: III-A Physical attacks on sensors
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 传感器的物理攻击
- en: The sensing layer, commonly considered as the frontier layer of an ADS, is naturally
    seen as an attack target by adversaries. Attackers intend to degrade the quality
    of sensor data by adding noise signals or making sensors collect fake data by
    counterfeiting data signals. The low-quality or even fake data would affect the
    performance of deep learning models in the perception layer and the decision layer
    and further influence the behaviors of an autonomous vehicle. In this threat model,
    adversaries are assumed with a certain knowledge of hardware and specification
    of sensors applied on an autonomous vehicle, but they do not need to know details
    of deep learning models in other layers. Therefore, physical attacks on the sensing
    layer could be seen as black-box attacks on the deep learning-based ADS.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 传感层通常被认为是自动驾驶系统（ADS）的前沿层，自然成为对手攻击的目标。攻击者意图通过添加噪声信号或伪造数据信号来降低传感器数据的质量，或者使传感器收集虚假的数据。低质量或甚至虚假的数据会影响感知层和决策层中深度学习模型的性能，进一步影响自动驾驶车辆的行为。在这一威胁模型中，假设对手对自动驾驶车辆上应用的传感器的硬件和规格有一定了解，但不需要了解其他层中深度学习模型的详细信息。因此，对传感层的物理攻击可以被视为对基于深度学习的自动驾驶系统的黑箱攻击。
- en: For physical attacks on sensors, attackers could disturb the data collected
    from sensors or fabricate signals to fool sensors using some external hardware.
    There are two most common physical attacks in this context, namely jamming attacks
    and spoofing attacks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对传感器的物理攻击中，攻击者可以通过一些外部硬件扰乱从传感器收集的数据或伪造信号以欺骗传感器。在这种情况下，最常见的两种物理攻击是干扰攻击和伪造攻击。
- en: III-A1 Jamming attack
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 干扰攻击
- en: The jamming attack is deemed as the most basic physical attack that uses specific
    hardware to add noises into the environment to degrade sensors’ data quality so
    as to make objects in the environment undetectable.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 干扰攻击被视为最基本的物理攻击，使用特定的硬件向环境中添加噪声，以降低传感器数据的质量，使环境中的物体无法检测到。
- en: Jamming attacks on a camera was experimented in [[47](#bib.bib47)] and the camera
    is blinded by emitting the intense light into it. When the camera receives a much
    stronger incoming light than the normal environment, the auto-exposure function
    of the camera will not work normally, and the captured images would then be overexposed
    and not recognizable by deep learning models in the perception layer. In the experiment,
    front/side attacks with different distances and different light intensities were
    set. The results show that blinding attacks at a short distance in the dark environment
    setting could severely damage the quality of the captured images, which means
    that the perception system is not capable of recognizing objects effectively when
    such attack occurs. Another blinding attack was experimented in [[49](#bib.bib49)],
    where attackers used a laser to cause temperature damages on cameras. A blinding
    attack for LiDAR was proposed in [[50](#bib.bib50)], in which the LiDAR is exposed
    under a strong light source that has the same wavelength as the LiDAR. Then, the
    LiDAR failed on perceiving objects from the direction of the light source. Jamming
    attacks on ultrasonic sensors and radars were experimented in [[49](#bib.bib49)],
    where a roadside attack is launched through an ultrasound jammer to attack the
    parking assistance system of four vehicles. The results showed that under jamming
    attacks, the vehicles were incapable of detecting the surrounding obstacles. To
    attack the radar, a signal generator and frequency multiplier were used to generate
    electromagnetic waves against the Tesla Autopilot system in which the Autopilot
    system was also compromised. A jamming attack on ultrasonic sensors was simulated
    in [[51](#bib.bib51)]. It was shown that other opposite placed ultrasonic sensors
    could substantially interfere the readings of the target ultrasonic sensor. In [[52](#bib.bib52)],
    the sound noise attacks on Gyroscopic sensors were launched, which were heavily
    used in the Unmanned Aerial Vehicles (UAV), leading to the fall of one UAV. In [[53](#bib.bib53)],
    GPS signals were found that they were vulnerable towards attacks from GPS jamming
    devices capable of producing large radio noises, which could adversely affect
    the navigation system.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[47](#bib.bib47)]中实验了对摄像头的干扰攻击，攻击者通过向摄像头发射强光来使其失明。当摄像头接收到的光线强度远超正常环境时，摄像头的自动曝光功能无法正常工作，从而导致捕获的图像过度曝光，深度学习模型在感知层无法识别。在实验中设置了不同距离和光强的前/侧面攻击。结果显示，在黑暗环境下的短距离失明攻击会严重损害捕获图像的质量，这意味着在发生这种攻击时，感知系统无法有效识别物体。另一个失明攻击在[[49](#bib.bib49)]中进行了实验，攻击者使用激光对摄像头造成温度损害。在[[50](#bib.bib50)]中提出了一种针对LiDAR的失明攻击，其中LiDAR暴露在与其相同波长的强光源下。结果是LiDAR未能从光源方向感知物体。在[[49](#bib.bib49)]中实验了对超声波传感器和雷达的干扰攻击，其中通过超声波干扰器对四辆车的停车辅助系统发起路边攻击。结果表明，在干扰攻击下，车辆无法检测周围的障碍物。为了攻击雷达，使用了信号发生器和频率倍增器生成电磁波，对特斯拉自动驾驶系统进行攻击，导致自动驾驶系统也受到影响。在[[51](#bib.bib51)]中模拟了对超声波传感器的干扰攻击。结果显示，其他方向的超声波传感器可能会显著干扰目标超声波传感器的读数。在[[52](#bib.bib52)]中，针对陀螺传感器的声音噪声攻击被发起，这些传感器在无人机（UAV）中使用广泛，导致了一架无人机的坠毁。在[[53](#bib.bib53)]中发现，GPS信号容易受到能够产生大量射频噪声的GPS干扰设备的攻击，这可能对导航系统产生不利影响。
- en: III-A2 Spoofing attack
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 伪装攻击
- en: Spoofing is a type of attack where adversaries use hardware to fabricate or
    inject signals during sensors data collection phase. The forged signal data could
    affect the perception of the environment and further cause abnormal behaviours
    of autonomous vehicles. In [[47](#bib.bib47)], a spoofing attack on LiDAR was
    tested. Specifically, as LiDAR could distinguish different objects at different
    positions by listening reflections of light achieving objects and echoing back,
    the counterfeit signal could return back ahead of the real signal. Consequently,
    LiDAR received with the counterfeit signal would lead to the wrong distance calculation
    between the vehicle and the object. Based on this idea, in the experiment, the
    real output signal of a wall was delayed and a counterfeited signal of the wall
    was created to produce the wrong distance information and successfully made LiDAR
    detect objects at the wrong distance. In [[48](#bib.bib48)], a spoofing attack
    against LiDAR was implemented by injecting deceiving physical signals into the
    victim sensor, which makes the LiDAR ignore legitimate inputs. Similarly, ultrasound
    pulses and radar signals fabricated [[49](#bib.bib49)] to attack ultrasound sensors
    and a radar. GPS is another victim under the threat of spoofing attacks. In 2013,
    a yacht encountered a GPS spoofing attack, causing it to deviate away from the
    pre-set route [[54](#bib.bib54)]. In [[55](#bib.bib55)], an open-source GPS spoofing
    generator was proposed, which can block all the legitimate signals. In [[56](#bib.bib56)],
    a similar GPS spoofing device was implemented to successfully attack commercial
    civilian GPS receivers. In [[57](#bib.bib57)], a GPS spoofing attack designed
    specifically for manipulating the navigation system was proposed. A GPS spoofing
    device, which could slightly shift the GPS location and then further manipulate
    the routing algorithm of the navigation system, was implemented. Subsequently,
    the autonomous vehicle would deviate from the original route. In addition to the
    attacks towards sensors, there are also spoofing attacks on cameras. In [[58](#bib.bib58)],
    a spoofing attack, aiming at the optical-flow sensing of UAV, was proposed. Attackers
    could alter the appearance of the ground plane, which would be captured by optical-flow
    cameras. Then altered images could adversely influence how the algorithms process
    the optical-flow information, and attackers could take over the control of the
    UAV by adopting this simple approach. There is another type of spoofing attack
    called relaying attack that usually occurs on LiDAR, aiming to deceive the target
    sensors by re-sending the original signal again from another position. The experiment
    in [[47](#bib.bib47)] showed that two ghost walls in different locations were
    detected by LiDAR because of relaying attacks. In [[59](#bib.bib59)], a projector
    was used to project spoofed traffic signs on cameras of a vehicle to make the
    vehicle interpret spoofed traffic signs as real signs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 欺骗是一种攻击方式，攻击者使用硬件在传感器数据收集阶段伪造或注入信号。伪造的信号数据可能会影响对环境的感知，并进一步导致自主车辆的异常行为。在 [[47](#bib.bib47)]中，对LiDAR进行了伪造攻击。具体而言，由于LiDAR可以通过监听光的反射来区分不同位置的物体，伪造信号可能会在真实信号之前返回。因此，LiDAR接收到伪造信号会导致车辆与物体之间距离的错误计算。基于这个想法，在实验中，将墙壁的真实输出信号延迟，并创建了墙壁的伪造信号，以产生错误的距离信息，并成功使LiDAR检测到错误距离的物体。在 [[48](#bib.bib48)]中，通过将欺骗性的物理信号注入受害传感器来实现了对LiDAR的伪造攻击，这使得LiDAR忽略了合法输入。类似地，伪造的超声波脉冲和雷达信号 [[49](#bib.bib49)]被用来攻击超声波传感器和雷达。GPS也是伪造攻击的另一个受害者。2013年，一艘游艇遇到了GPS伪造攻击，导致它偏离了预设的航线 [[54](#bib.bib54)]。在 [[55](#bib.bib55)]中，提出了一种开源GPS伪造生成器，可以阻塞所有合法信号。在 [[56](#bib.bib56)]中，实现了一种类似的GPS伪造设备，成功攻击了商业民用GPS接收器。在 [[57](#bib.bib57)]中，提出了一种专门设计用于操控导航系统的GPS伪造攻击。一种GPS伪造设备可以稍微移动GPS位置，然后进一步操控导航系统的路由算法，从而使自主车辆偏离原始路线。除了对传感器的攻击，还有对摄像头的伪造攻击。在 [[58](#bib.bib58)]中，提出了一种针对无人机光流传感的伪造攻击。攻击者可以改变地面平面的外观，这将被光流摄像头捕捉到。然后，改变后的图像可能会不利地影响算法处理光流信息的方式，攻击者可以通过这种简单的方法控制无人机。另一种伪造攻击称为中继攻击，通常发生在LiDAR上，旨在通过从另一个位置重新发送原始信号来欺骗目标传感器。实验在 [[47](#bib.bib47)]中显示，由于中继攻击，LiDAR检测到了两个不同位置的虚假墙壁。在 [[59](#bib.bib59)]中，使用投影仪在车辆的摄像头上投射伪造的交通标志，使车辆将伪造的交通标志解读为真实标志。
- en: 'TABLE I: Physical attacks on ADSs'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '表I: 对ADS的物理攻击'
- en: '| Attack | Target sensor | Action | Implication | Examples |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | 目标传感器 | 行动 | 影响 | 示例 |'
- en: '| Jamming attack | Camera | Extensive light blinding attack |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 干扰攻击 | 相机 | 大范围光线致盲攻击 |'
- en: '&#124; Make images overexposed and not recognizable; &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使图像过曝且无法识别; &#124;'
- en: '&#124; Cause temperature damage on cameras &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 造成相机温度损坏 &#124;'
- en: '| [[47](#bib.bib47)] |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| [[47](#bib.bib47)] |'
- en: '| LiDAR |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| LiDAR |'
- en: '&#124; Blinding attacks by strong light &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 强光致盲攻击 &#124;'
- en: '&#124; with same wavelength as LiDAR &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与LiDAR波长相同 &#124;'
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LiDAR cannot perceive objects from &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LiDAR无法感知物体 &#124;'
- en: '&#124; the direction of light source &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 光源方向 &#124;'
- en: '| [[50](#bib.bib50)] |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| [[50](#bib.bib50)] |'
- en: '| Ultrasonic sensor | Ultrasonic jamming device | Obstacles cannot be detected
    | [[49](#bib.bib49)] |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 超声波传感器 | 超声波干扰设备 | 障碍物无法检测 | [[49](#bib.bib49)] |'
- en: '| Ultrasonic sensor |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 超声波传感器 |'
- en: '&#124; Putting another ultrasound sensor &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 放置另一个超声波传感器 &#124;'
- en: '&#124; opposite to the target one &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与目标物体相反 &#124;'
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Both ultrasonic sensors cannot collect &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两个超声波传感器都无法收集 &#124;'
- en: '&#124; accurate data &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精确数据 &#124;'
- en: '| [[51](#bib.bib51)] |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| [[51](#bib.bib51)] |'
- en: '| Radar | Generating electromagnetic waves | Detected obstacles are disappeared
    | [[49](#bib.bib49)] |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 雷达 | 生成电磁波 | 被检测到的障碍物消失 | [[49](#bib.bib49)] |'
- en: '| Gyroscopic sensor | Sound noise | An UAV fall down | [[52](#bib.bib52)] |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 陀螺仪传感器 | 声音噪声 | 无人机坠落 | [[52](#bib.bib52)] |'
- en: '| GPS | GPS jamming device | Navigation system cannot work normally | [[53](#bib.bib53)]
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| GPS | GPS干扰设备 | 导航系统无法正常工作 | [[53](#bib.bib53)] |'
- en: '| Spoofing attack | LiDAR |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 欺骗攻击 | LiDAR |'
- en: '&#124; Relaying signals of objects from &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 中继物体的信号 &#124;'
- en: '&#124; another position &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 另一个位置 &#124;'
- en: '| LiDAR detects ’ghost’ objects | [[47](#bib.bib47)] |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| LiDAR检测“幽灵”物体 | [[47](#bib.bib47)] |'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LiDAR, Radar; &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LiDAR, 雷达; &#124;'
- en: '&#124; Ultrasonic sensor &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 超声波传感器 &#124;'
- en: '| Fabricating fake signals |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 伪造虚假信号 |'
- en: '&#124; Sensors detect objects at wrong distance; &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 传感器检测到错误的距离; &#124;'
- en: '&#124; LiDAR ignores legitimate objects &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LiDAR忽略合法物体 &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[47](#bib.bib47)], [[49](#bib.bib49)] &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[47](#bib.bib47)], [[49](#bib.bib49)] &#124;'
- en: '&#124; [[48](#bib.bib48)] &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[48](#bib.bib48)] &#124;'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| GPS |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| GPS |'
- en: '&#124; Using GPS-spoofing device &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用GPS欺骗设备 &#124;'
- en: '&#124; to inject fake signals &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注入虚假信号 &#124;'
- en: '| Navigation system is manipulated |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 导航系统被操控 |'
- en: '&#124; [[54](#bib.bib54)], [[57](#bib.bib57)], &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[54](#bib.bib54)], [[57](#bib.bib57)], &#124;'
- en: '&#124; [[55](#bib.bib55), [56](#bib.bib56)] &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[55](#bib.bib55), [56](#bib.bib56)] &#124;'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Optical-flow &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 光流 &#124;'
- en: '&#124; camera &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相机 &#124;'
- en: '|'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Altering the appearance of &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 改变 &#124;'
- en: '&#124; ground plane &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 地面平面 &#124;'
- en: '| UAV is taken over | [[58](#bib.bib58)] |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 无人机被接管 | [[58](#bib.bib58)] |'
- en: '|'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Camera &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相机 &#124;'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Using a projector to project &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用投影仪进行投射 &#124;'
- en: '&#124; deceptive traffic signs onto ADAS &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 欺骗性的交通标志对ADAS的影响 &#124;'
- en: '|'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The vehicle recognized the deceptive &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 车辆将欺骗性 &#124;'
- en: '&#124; traffic signs as real signs &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交通标志识别为真实标志 &#124;'
- en: '| [[59](#bib.bib59)] |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| [[59](#bib.bib59)] |'
- en: III-B Cyberattacks on cloud services
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 云服务上的网络攻击
- en: The cloud could be the target for many attacks from adversaries’ perspective
    because of continuous communications between the cloud and autonomous vehicles,
    consequently resulting in instability of autonomous vehicles.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从对手的角度来看，云可能成为许多攻击的目标，因为云与自动驾驶车辆之间的持续通信，这最终可能导致自动驾驶车辆的不稳定。
- en: Note that an HD Map could be updated in real time by information from other
    vehicles via V2X. This process is possibly controlled by attackers. For instance,
    Sybil attacks [[60](#bib.bib60)] and message falsification attacks [[60](#bib.bib60)]
    are designed to interfere the efficiency of the automatic navigation. Precisely,
    Sybil attacks focus on the real-time HD map updating in V2X, creating a large
    number of “fake drivers” in the target location system with fake GPS information.
    These attacks are designed to delude the system through the traffic jam and further
    interferes localization and navigation tasks in the vehicle. For the message falsification
    attacks, they intercept and tamper the traffic information updated from vehicle
    to the HD map server and spoofs other vehicles when updating the HD map information
    through this server.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，高清地图可以通过V2X实时更新来自其他车辆的信息。这个过程可能会被攻击者控制。例如，Sybil攻击[[60](#bib.bib60)]和消息伪造攻击[[60](#bib.bib60)]旨在干扰自动导航的效率。具体而言，Sybil攻击集中于V2X中的实时高清地图更新，通过伪造GPS信息在目标位置系统中创建大量“假司机”。这些攻击旨在通过交通拥堵来欺骗系统，并进一步干扰车辆中的定位和导航任务。对于消息伪造攻击，它们拦截和篡改从车辆到高清地图服务器更新的交通信息，并在通过该服务器更新高清地图信息时伪装其他车辆。
- en: Traditional cloud attacks are threatening the V2X network in which autonomous
    vehicles are connected to exchange information. Both Denial of Service (DoS) and
    Distributed DoS (DDoS) [[61](#bib.bib61), [62](#bib.bib62)] could cause the exhaustion
    of service resources, resulting in high latency or even the network unavailability
    of the V2X network. In this situation, autonomous vehicles may not be able to
    connect to the HD map for accurate navigation and perception service, which substantially
    endangers the safety of the autonomous vehicles.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的云攻击威胁到V2X网络，其中自主车辆连接以交换信息。无论是服务拒绝攻击（DoS）还是分布式拒绝服务攻击（DDoS）[[61](#bib.bib61),
    [62](#bib.bib62)]，都可能导致服务资源的耗尽，从而导致高延迟甚至V2X网络的不可用。在这种情况下，自主车辆可能无法连接到高清地图以获得准确的导航和感知服务，这严重危及自主车辆的安全。
- en: One variation of attack aims at the over-the-air (OTA) channel in the cloud,
    where attackers could hijack the data transfer channel between the cloud and an
    autonomous vehicle and inject the malware into the vehicle [[63](#bib.bib63)].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一种攻击变体旨在云中的空中传输（OTA）通道，攻击者可能会劫持云与自主车辆之间的数据传输通道，并将恶意软件注入到车辆中[[63](#bib.bib63)]。
- en: However, as attacks for cloud services are more relative to cyberattacks, we
    will not detail on such attacks and corresponding defending methods in this survey.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于针对云服务的攻击更多地涉及网络攻击，我们不会在本次调查中详细讨论这些攻击及相应的防御方法。
- en: III-C Adversarial attacks on deep learning models in perception and decision
    layers
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 对抗攻击在感知层和决策层的深度学习模型中的应用
- en: Recent research shows that deep learning models are particularly vulnerable
    to adversarial examples that add imperceptible noises on original input images.
    Even though adversarial examples look similar to normal images from human’s view,
    they could mislead deep learning models to produce wrong predictions. By definition,
    an adversarial attack is a type of attack to construct such adversarial examples.
    Therefore, adversarial attacks pose considerable threats to ADSs due to the widespread
    usage of deep learning models in both the perception layer and the decision layer.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 近期研究表明，深度学习模型特别容易受到对抗样本的攻击，这些样本在原始输入图像上添加了难以察觉的噪声。尽管对抗样本在人类视角下看起来与正常图像相似，但它们可能会误导深度学习模型产生错误的预测。根据定义，对抗攻击是一种构造此类对抗样本的攻击。因此，由于深度学习模型在感知层和决策层的广泛使用，对抗攻击对自动驾驶系统（ADSs）构成了相当大的威胁。
- en: In this section, we first introduce the definition of adversarial attacks along
    with some relevant concepts. Then we summarize the literature review the progress
    of adversarial attacks on different deep learning models in ADSs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍了对抗攻击的定义以及一些相关概念。然后我们总结了文献综述，回顾了对抗攻击在不同深度学习模型在自动驾驶系统（ADSs）中的进展。
- en: III-C1 Introduction to adversarial attacks
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 对抗攻击简介
- en: Depending on attackers’ ability, adversarial attacks could be categorized as
    either white-box or black-box attacks. In white-box attacks, attackers are assumed
    to know all the details of the target deep learning model including training data,
    neural network architecture, parameters, and hyper-parameters, while having the
    privilege to visit the gradients and results of the model at run time.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 根据攻击者的能力，对抗攻击可以分为白盒攻击和黑盒攻击。在白盒攻击中，假设攻击者知道目标深度学习模型的所有细节，包括训练数据、神经网络架构、参数和超参数，并且可以访问模型在运行时的梯度和结果。
- en: There are two types of adversarial attacks, i.e., adversarial evasion attacks
    occurring at the model inference time and poisoning attacks that happen in the
    model training period. Adversarial evasion attacks to deep learning models are
    first investigated for image classification tasks. Given a target deep learning
    model $f$ and an original image $x\in\mathcal{X}$ with its class $c$, an adversarial
    attack could construct a human imperceptible perturbation $\delta$ to form an
    adversarial example $x^{\prime}=x+\delta$, which could delude the model to make
    a wrong prediction $c^{\prime}\neq c$.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击有两种类型，即发生在模型推断时的对抗规避攻击和发生在模型训练期间的投毒攻击。对抗规避攻击首次在图像分类任务中研究。给定目标深度学习模型$f$和原始图像$x\in\mathcal{X}$及其类别$c$，对抗攻击可以构造一个人眼不可察觉的扰动$\delta$，形成对抗样本$x^{\prime}=x+\delta$，这可以*误导*模型做出错误的预测$c^{\prime}\neq
    c$。
- en: Commonly, there are three different kinds of white-box methods to generate adversarial
    examples, namely, gradient-based methods, optimization-based methods and generative
    model-based methods.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有三种不同的白盒方法生成对抗样本，即基于梯度的方法、基于优化的方法和基于生成模型的方法。
- en: •
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Gradient-based methods: These attack methods [[10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)] are based on the Fast Gradient Sign Method
    (FGSM), as shown in Equation ([1](#S3.E1 "In 1st item ‣ III-C1 Introduction to
    adversarial attacks ‣ III-C Adversarial attacks on deep learning models in perception
    and decision layers ‣ III Attacks in ADSs ‣ Deep Learning-Based Autonomous Driving
    Systems: A Survey of Attacks and Defenses")), to directly generate adversarial
    examples by adding the gradients of loss with respect to each pixel on original
    images [[10](#bib.bib10)].'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '基于梯度的方法：这些攻击方法[[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]基于快速梯度符号法（FGSM），如公式([1](#S3.E1
    "In 1st item ‣ III-C1 Introduction to adversarial attacks ‣ III-C Adversarial
    attacks on deep learning models in perception and decision layers ‣ III Attacks
    in ADSs ‣ Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks
    and Defenses")）所示，通过在原始图像的每个像素上添加损失的梯度来直接生成对抗样本[[10](#bib.bib10)]。'
- en: '|  | $x^{\prime}=x+\epsilon\,sign(\nabla J_{\theta}(x,c))$ |  | (1) |'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $x^{\prime}=x+\epsilon\,sign(\nabla J_{\theta}(x,c))$ |  | (1) |'
- en: •
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Optimization-based methods: These attack methods [[9](#bib.bib9), [14](#bib.bib14),
    [16](#bib.bib16)] solve an optimization problem as'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于优化的方法：这些攻击方法[[9](#bib.bib9), [14](#bib.bib14), [16](#bib.bib16)]将问题转化为如下优化问题
- en: '|  | $\operatorname*{argmin}_{x^{\prime}}\alpha&#124;&#124;x-x^{\prime}&#124;&#124;_{p}+\ell(J_{\theta,\textbf{c}^{\prime}}(x^{\prime}))$
    |  | (2) |'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\operatorname*{argmin}_{x^{\prime}}\alpha&#124;&#124;x-x^{\prime}&#124;&#124;_{p}+\ell(J_{\theta,\textbf{c}^{\prime}}(x^{\prime}))$
    |  | (2) |'
- en: where the first part is the $L_{p}$ distance between an original image and an
    adversarial image, and the second part is the constraint on the loss of the adversarial
    image [[64](#bib.bib64)]. By solving this optimization problem, one could generate
    an adversarial image $x^{\prime}$ that is close to $x$ in $L_{p}$ distance but
    be classified as $c^{\prime}$.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中第一部分是原始图像和对抗图像之间的$L_{p}$距离，第二部分是对抗图像的损失约束[[64](#bib.bib64)]。通过解决这个优化问题，可以生成一个对抗图像$x^{\prime}$，该图像在$L_{p}$距离上接近$x$，但被分类为$c^{\prime}$。
- en: •
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Generative model-based methods: This type of attack [[19](#bib.bib19), [20](#bib.bib20)]
    leverages generative models to generate targeted adversarial examples from original
    images. These methods normally learn a generative model $\mathcal{G}$ by optimizing
    an objective function as'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于生成模型的方法：这类攻击[[19](#bib.bib19), [20](#bib.bib20)]利用生成模型从原始图像生成针对性的对抗样本。这些方法通常通过优化目标函数来学习生成模型$\mathcal{G}$。
- en: '|  | $\mathcal{L}=\mathcal{L}_{\mathcal{Y}}+\alpha\mathcal{L}_{\mathcal{G}}$
    |  | (3) |'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\mathcal{L}_{\mathcal{Y}}+\alpha\mathcal{L}_{\mathcal{G}}$
    |  | (3) |'
- en: where $\mathcal{L}_{\mathcal{Y}}$ denotes the cross-entropy loss between classification
    of adversarial examples and targeted class, and $\mathcal{L}_{\mathcal{G}}$ measures
    the similarity between adversarial examples and original images.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{L}_{\mathcal{Y}}$ 表示对抗样本分类与目标类别之间的交叉熵损失，$\mathcal{L}_{\mathcal{G}}$
    衡量对抗样本与原始图像之间的相似性。
- en: 'When it comes to the black-box attacks, attackers are assumed not having prior
    knowledge of the target model, but they can query the model and obtain the output
    of the model unlimitedly. There are also three different approaches to generate
    black-box adversarial examples:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于黑箱攻击，假设攻击者没有目标模型的先验知识，但他们可以无限制地查询模型并获取模型的输出。生成黑箱对抗样本的三种不同方法如下：
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transfer-based methods: It was discovered that adversarial images targeted
    on a specific model were also found effective when dealing with other deep learning
    models, and this attribute is called the transferability of adversarial examples [[64](#bib.bib64)].
    Therefore, attackers could implement a similar model based on the input and output
    derived from the target model, and then initiate white-box attacks on their own
    model instead. The adversarial examples constructed based on their own model could
    be utilized to attack the target black-box model.'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于迁移的方法：发现针对特定模型的对抗图像在处理其他深度学习模型时也有效，这种属性称为对抗样本的迁移性 [[64](#bib.bib64)]。因此，攻击者可以基于目标模型的输入和输出实现一个类似的模型，然后对自己的模型进行白箱攻击。基于自己模型构建的对抗样本可以用于攻击目标黑箱模型。
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Score-based methods: Although gradients information in a black-box model cannot
    be directly retrieved, the value of gradients could be estimated based on the
    probability score output of the target model and then used to craft adversarial
    examples [[65](#bib.bib65)].'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于评分的方法：尽管在黑箱模型中无法直接获取梯度信息，但可以根据目标模型的概率得分输出来估计梯度值，然后用来生成对抗样本 [[65](#bib.bib65)]。
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Decision-based methods: These methods only rely on the final decision (e.g.,
    top-1 classification result) of the target model to craft adversarial examples
    based on a randomly generated large perturbation and then iteratively reduce the
    perturbation while keeping adversarial features [[66](#bib.bib66)].'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于决策的方法：这些方法仅依赖于目标模型的最终决策（例如，top-1 分类结果）来生成对抗样本，基于随机生成的大扰动，然后迭代地减少扰动，同时保持对抗特征 [[66](#bib.bib66)]。
- en: For attacks on ADSs, black-box attacks are more realistic. In addition, attacks
    on ADSs should occur in the physical world where sensors collect environmental
    information (e.g. images and point clouds) from different angles, light conditions,
    and distances. Therefore, this paper intends to cover physical black-box evasion
    attacks experimented in both simulation environment and the real world.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自动驾驶系统的攻击，黑箱攻击更具现实性。此外，针对自动驾驶系统的攻击应发生在物理世界中，其中传感器从不同的角度、光照条件和距离收集环境信息（例如图像和点云）。因此，本文旨在涵盖在模拟环境和现实世界中实验的物理黑箱规避攻击。
- en: III-C2 Adversarial evasion attacks on ADSs
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 对自动驾驶系统的对抗性规避攻击
- en: This section first reviewed related attacks that were experimented in simulation
    environments, either real-world recording data or in simulated real-world scenarios.
    In addition, research experimented in the real world was also reviewed, which
    showed the harm of adversarial evasion attacks on ADSs in real life.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先回顾了在模拟环境中实验的相关攻击，无论是实际世界录制的数据还是模拟的实际世界场景。此外，还回顾了在现实世界中进行的研究，展示了对抗性规避攻击对自动驾驶系统在现实生活中的危害。
- en: 'In [[67](#bib.bib67)], an approach called DeepBillboard to attack end-to-end
    driving models was proposed by replacing the original billboards on the roadside
    with adversarial perturbations. Specifically, the adversarial billboards were
    generated by the aforementioned optimization-based method. The method was tested
    on two end-to-end driving models on three datasets, along with different scenarios
    where billboards are positioned at different positions and angles. The result
    showed that their attack could make steering angle predictions deviate at most
    23 degrees. In [[68](#bib.bib68)], a Bayes Optimization-based approach was proposed
    to generate the painting of black lines on the road to counterfeit lane lines
    and make the vehicle deviate from the original orientation. Experiments were conducted
    in CARLA simulator [[69](#bib.bib69)], and results showed that E2E driving models
    were attacked and deviated to the orientation chosen by attackers. An updated
    approach proposed in [[70](#bib.bib70)] applied gradient-based optimization algorithm
    again to achieve quicker generation of black lines with higher deviation. In [[71](#bib.bib71)],
    a decision-based approach was proposed to search and craft adversarial texture
    of vehicles. The average prediction score and the precision of object detectors
    in ADSs decreased sharply when presenting vehicles with adversarial texture (shown
    as Figure [3](#S3.F3 "Figure 3 ‣ III-C2 Adversarial evasion attacks on ADSs ‣
    III-C Adversarial attacks on deep learning models in perception and decision layers
    ‣ III Attacks in ADSs ‣ Deep Learning-Based Autonomous Driving Systems: A Survey
    of Attacks and Defenses")). Apart from that, some works also investigate attacks
    on LiDAR-based object detection in the simulation environment. In [[72](#bib.bib72)],
    a white-box optimization-based method was proposed to generate adversarial points
    and demonstrated how to inject these points into the original point cloud of an
    obstacle through laser. Experiments were conducted using LiDAR sensor data through
    a simulator released by Baidu Apollo. Experiment results showed that the average
    success rates of the attack were up to 90%, while the number of injected adversarial
    points was larger than 60\. The first black-box attack on LiDAR was proposed in [[73](#bib.bib73)],
    aiming to insert attack traces into point clouds to baffle LiDAR-based object
    detector. The experiment result on KITTI dataset achieved mean success rate at
    around 80%.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[67](#bib.bib67)]中，提出了一种名为 DeepBillboard 的方法，通过用对抗扰动替换路边的原始广告牌来攻击端到端驾驶模型。具体来说，对抗广告牌是通过前述的基于优化的方法生成的。该方法在三个数据集上的两个端到端驾驶模型上进行了测试，并测试了广告牌在不同位置和角度的不同场景。结果显示，他们的攻击可以使转向角度预测最多偏差
    23 度。在[[68](#bib.bib68)]中，提出了一种基于贝叶斯优化的方法，生成路面上的黑线画作以伪造车道线，使车辆偏离原始方向。实验在 CARLA
    模拟器[[69](#bib.bib69)]中进行，结果显示端到端驾驶模型被攻击并偏离了攻击者选择的方向。在[[70](#bib.bib70)]中，提出了一种更新的方法，再次应用了基于梯度的优化算法，实现了更快生成黑线并产生更大的偏差。在[[71](#bib.bib71)]中，提出了一种基于决策的方法，搜索并制作车辆的对抗纹理。当呈现带有对抗纹理的车辆时，ADS
    中物体检测器的平均预测分数和精度急剧下降（如图 [3](#S3.F3 "Figure 3 ‣ III-C2 Adversarial evasion attacks
    on ADSs ‣ III-C Adversarial attacks on deep learning models in perception and
    decision layers ‣ III Attacks in ADSs ‣ Deep Learning-Based Autonomous Driving
    Systems: A Survey of Attacks and Defenses") 所示）。此外，一些研究还探讨了在模拟环境中对基于 LiDAR 的物体检测的攻击。在[[72](#bib.bib72)]中，提出了一种基于白盒优化的方法来生成对抗点，并展示了如何通过激光将这些点注入原始的障碍物点云中。实验使用了由百度
    Apollo 发布的模拟器中的 LiDAR 传感器数据。实验结果表明，攻击的平均成功率高达 90%，而注入的对抗点数量超过 60。第一种针对 LiDAR 的黑盒攻击在[[73](#bib.bib73)]中提出，旨在将攻击痕迹插入点云中，以迷惑基于
    LiDAR 的物体检测器。KITTI 数据集上的实验结果达到了约 80% 的平均成功率。'
- en: '![Refer to caption](img/7ba614ae12b7059b9826e06ce221454e.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7ba614ae12b7059b9826e06ce221454e.png)'
- en: 'Figure 3: Left: The vehicle can be detected normally; Right: The vehicle with
    adversarial texture cannot be recognized (image credit: [[71](#bib.bib71)])'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：左侧：车辆可以正常检测；右侧：带有对抗纹理的车辆无法识别（图片来源：[[71](#bib.bib71)]）
- en: In addition to research conducted under simulation environments, others study
    adversarial evasion attacks in the real world. For instance, an approach called
    ShapeShifter was proposed in [[74](#bib.bib74)] to attack object detection model
    Faster R-CNN. The adversarial perturbation was generated by solve an optimization
    problem named Expectation over Transformation that aims to create a robust perturbation
    when it is captured from different angles with different lighting conditions.
    In the experiment, traffic signs with adversarial perturbations were printed in
    real world. The targeted attack success rate and the non-targeted one were reported
    to be $87\%$ and $93\%$, respectively. In [[75](#bib.bib75)], a method to generate
    robust physical perturbations was proposed. In the experiments, attackers could
    print the perturbed road signs and replace the true road signs with the perturbed
    ones (subtle poster attacks), or only print the physical perturbations as stickers
    with different colors and attach them on road signs (camouflage abstract art attacks).
    In the road test, success rates for the subtle poster attacks and camouflage abstract
    art attacks reached $100\%$ and $84.8\%$, respectively, both of which used a CNN
    model called LISA-CNN [[76](#bib.bib76)].
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在模拟环境下进行的研究外，还有其他研究探讨了现实世界中的对抗性规避攻击。例如，在[[74](#bib.bib74)]中提出了一种名为 ShapeShifter
    的方法，用于攻击目标检测模型 Faster R-CNN。对抗性扰动是通过解决一个名为“期望变换”的优化问题生成的，该问题旨在创建一个在不同角度和不同光照条件下都能保持鲁棒的扰动。在实验中，对抗性扰动的交通标志被打印到现实世界中。目标攻击成功率和非目标攻击成功率分别报告为
    $87\%$ 和 $93\%$。在[[75](#bib.bib75)]中，提出了一种生成鲁棒物理扰动的方法。在实验中，攻击者可以打印带有扰动的道路标志，并用这些扰动标志替换真实的道路标志（微妙海报攻击），或仅打印不同颜色的物理扰动作为贴纸并贴在道路标志上（伪装抽象艺术攻击）。在道路测试中，微妙海报攻击和伪装抽象艺术攻击的成功率分别达到了
    $100\%$ 和 $84.8\%$，两者都使用了名为 LISA-CNN 的 CNN 模型[[76](#bib.bib76)]。
- en: '![Refer to caption](img/4ef827c98dbbf59b443868bf7ff6da06.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4ef827c98dbbf59b443868bf7ff6da06.png)'
- en: 'Figure 4: The stop sign with an adversarial sticker cannot be recognized from
    different distance and angles (image credit: [[77](#bib.bib77)])'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：带有对抗性贴纸的停车标志在不同距离和角度下无法识别（图片来源：[77](#bib.bib77)）
- en: 'A generative model-based approach called Perceptual-Sensitive GAN was also
    proposed in [[21](#bib.bib21)] in which an attention model was incorporated into
    the GAN to generate adversarial patches. The experiments conducted based on the
    physical world in a black-box setting showed that attacks could reduce classification
    accuracy from 86.7% to 17.2% on average. Similarly, methods proposed in [[77](#bib.bib77)]
    can generate robust adversarial stickers to attack object detectors in two modes:
    Hiding attack that makes detectors fail to detect objects, and Appearing attack
    that makes detectors recognize wrong objects. Besides object detectors, E2E driving
    models were attacked in the physical world settings as revealed in [[79](#bib.bib79)].
    A method called PhysGAN was proposed to generate realistic billboard similar to
    the original one, but it could make autonomous vehicles deviating from their original
    route. The experiment results showed that billboards generated by PhysGAN could
    deviate steering angle predictions of E2E driving models up to 19.17 degrees.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[21](#bib.bib21)]中，提出了一种基于生成模型的方法，称为感知敏感 GAN，其中将注意力模型整合到 GAN 中，以生成对抗性补丁。基于黑箱设置下的物理世界进行的实验表明，这些攻击可以将分类准确率从
    86.7% 降低到平均 17.2%。类似地，在[[77](#bib.bib77)]中提出的方法可以生成鲁棒的对抗性贴纸，用于攻击目标检测器，有两种模式：隐藏攻击使检测器无法检测到物体，出现攻击使检测器识别错误的物体。除了目标检测器，[[79](#bib.bib79)]揭示了在物理世界设置中对
    E2E 驾驶模型的攻击。提出了一种名为 PhysGAN 的方法，用于生成与原始广告牌相似的真实广告牌，但它可能使自动驾驶车辆偏离原始路线。实验结果表明，PhysGAN
    生成的广告牌可以使 E2E 驾驶模型的转向角度预测偏差达到 19.17 度。
- en: '![Refer to caption](img/63534008ecfef062a6bee8fdf11b1a40.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/63534008ecfef062a6bee8fdf11b1a40.png)'
- en: 'Figure 5: Top Left: original billboard; Top Right: adversarial billboard generated
    by PhysGAN; Bottom: placing adversarial billboard in real world (image credit: [[79](#bib.bib79)])'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：左上角：原始广告牌；右上角：由 PhysGAN 生成的对抗性广告牌；下方：在现实世界中放置对抗性广告牌（图片来源：[79](#bib.bib79)）
- en: III-C3 Adversarial poisoning attacks on ADSs
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C3 对自适应系统的对抗性中毒攻击
- en: Poisoning attacks also fall into the types of adversarial attacks. More specifically,
    a poisoning attack works by injecting malicious data with triggers and misleading
    labels into original training data to make models learn specific patterns of triggers.
    During inference time, models are induced to produce wrong predictions when inputs
    contain malicious triggers. The poisoning attack is also considered to resemble
    the Trojan attack or backdoor attack. In [[88](#bib.bib88)], the Trojan attack
    for E2E driving models was simulated. Adversarial triggers such as a square or
    an Apple logo were constructed and put on the corner of original input images.
    Experiment results showed that if the road images contained these malicious triggers,
    the vehicle could easily deviate from the pre-planned track. In [[89](#bib.bib89)],
    poisoning attacks were conducted with four different triggers on four traffic
    sign recognition datasets, where one specific class of traffic signs was targeted.
    The experiment results showed that the CNN model could learn patterns of triggers
    and achieve more than 95% accuracy on those poisoned images when the ratio of
    poisoned images was more than 5%. Meanwhile, the overall accuracy of the total
    test dataset was more than 99%, suggesting that it was difficult to determine
    if a model encounters poisoning attacks by only observing the results of test
    accuracy. In [[90](#bib.bib90)], a poisoning attack on deep generative models
    like GAN for raindrop removing was proposed. Malicious data pairs were injected
    into original training data to make GAN learn the wrong map from the input domain
    to the output domain. The experiment result showed that when GAN removed raindrops,
    it simultaneously transformed red traffic light to green, or altered the number
    on speed limit sign.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 投毒攻击也属于对抗性攻击类型。更具体地说，投毒攻击通过向原始训练数据中注入带有触发器和误导标签的恶意数据，迫使模型学习触发器的特定模式。在推理阶段，当输入包含恶意触发器时，模型会被诱导产生错误的预测。投毒攻击也被认为类似于木马攻击或后门攻击。在
    [[88](#bib.bib88)] 中，对E2E驾驶模型进行了木马攻击模拟。构造了如方块或苹果标志等对抗性触发器，并将其放置在原始输入图像的角落。实验结果表明，如果道路图像中包含这些恶意触发器，车辆可能会轻易偏离预定轨道。在
    [[89](#bib.bib89)] 中，使用四种不同触发器对四个交通标志识别数据集进行了投毒攻击，其中一个特定类别的交通标志成为目标。实验结果表明，当投毒图像的比例超过5%时，CNN模型能够学习触发器的模式，并在这些投毒图像上达到95%以上的准确率。同时，总测试数据集的整体准确率超过99%，这表明仅通过观察测试准确率的结果，很难判断模型是否遇到投毒攻击。在
    [[90](#bib.bib90)] 中，提出了对深度生成模型如GAN进行的投毒攻击，用于去除雨滴。恶意数据对被注入到原始训练数据中，迫使GAN学习从输入域到输出域的错误映射。实验结果表明，当GAN去除雨滴时，它同时将红灯变为绿灯，或改变限速标志上的数字。
- en: III-D Analysis of attacks
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 攻击分析
- en: 1\. Physical attacks are straightforward but limited in a certain range. Physical
    attacks on sensors could disrupt deep learning models by interfering the data
    collecting process. However, this type of attack requires the target in the proximity
    of the adversaries. For example, the camera blinding attack only occurs if the
    laser light is placed in front of the target vehicles, which makes such attacks
    difficult to implement.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 物理攻击直接但在一定范围内有限。对传感器的物理攻击可能通过干扰数据收集过程来破坏深度学习模型。然而，这种攻击类型要求目标在对手的接近范围内。例如，摄像头致盲攻击只有在激光光束放置在目标车辆前面时才会发生，这使得此类攻击难以实施。
- en: 2\. Cyberattacks are harmful while challenging. Cyberattacks on the cloud could
    affect numerous autonomous vehicles connected in V2X network and thus result in
    severe consequences. However, for cyberattacks on the cloud, adversaries need
    to fabricate data transferred between the cloud and the vehicle or implement DDoS
    attacks by large Botnet. However, the encryption of data transmission process
    could hinder both attacks, and the cloud could deploy detection systems like [[91](#bib.bib91),
    [92](#bib.bib92)] to defend DDoS attacks to some extent.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 网络攻击有害且具有挑战性。对云的网络攻击可能会影响连接在V2X网络中的众多自动驾驶车辆，从而导致严重后果。然而，对于云的网络攻击，对手需要伪造云与车辆之间传输的数据或通过大型僵尸网络实施DDoS攻击。然而，数据传输过程的加密可能会阻碍这两种攻击，云还可以部署如
    [[91](#bib.bib91), [92](#bib.bib92)] 所示的检测系统，在一定程度上防御DDoS攻击。
- en: '3\. Adversarial attacks are effective and pose threats in real world. Adversarial
    attacks, especially evasion attacks, would pose considerable risks to deep learning
    models in ADSs due to the existence of adversarial perturbations in the black-box
    setting. Table [II](#S3.T2 "TABLE II ‣ III-D Analysis of attacks ‣ III Attacks
    in ADSs ‣ Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks
    and Defenses") shows some research works implemented black-box evasion attacks
    and experimented the effectiveness of their methods for attacking E2E driving
    models or object detectors in the perception layer of ADSs from different angles,
    distances, and light conditions in simulation environment or in the real world.
    For this line of attacks, adversaries could arbitrarily make malicious stickers
    and stealthily paste them everywhere. Adversarial poisoning attacks could occur
    in a scenario where corporate espionage has the chance to pollute training data,
    in which the attack could also be stealthy and hazardous. Therefore, it is essential
    to put a summary of current research on defenses against adversarial attacks.
    From the perspective of attacks, there may exist more powerful attacks to destroy
    autonomous vehicles, from which further research can be drawn.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '3\. 对抗性攻击是有效的，并且在现实世界中构成威胁。对抗性攻击，特别是躲避攻击，由于黑盒设置中的对抗性扰动的存在，会对深度学习模型构成相当大的风险。表[II](#S3.T2
    "TABLE II ‣ III-D Analysis of attacks ‣ III Attacks in ADSs ‣ Deep Learning-Based
    Autonomous Driving Systems: A Survey of Attacks and Defenses")展示了一些研究工作，这些工作实现了黑盒躲避攻击，并从不同角度、距离和光照条件下在模拟环境或真实世界中实验了其方法对端到端驾驶模型或自动驾驶系统感知层中的目标检测器的攻击效果。对于这一类攻击，攻击者可以随意制作恶意贴纸并偷偷粘贴在各处。对抗性污染攻击可能发生在公司间谍有机会污染训练数据的情况下，这种攻击也可能是隐蔽且危险的。因此，总结当前对抗攻击防御的研究是至关重要的。从攻击的角度来看，可能存在更强大的攻击来破坏自动驾驶车辆，从中可以进行进一步的研究。'
- en: 'TABLE II: Adversarial attacks on autonomous driving'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 对抗性攻击在自动驾驶中的应用'
- en: '| Attack type | Attack objective | Literature | Method | Attack setting | Experiment
    setting |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 攻击类型 | 攻击目标 | 文献 | 方法 | 攻击设置 | 实验设置 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Envasion attacks | E2E driving model | [[67](#bib.bib67)] |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 躲避攻击 | 端到端驾驶模型 | [[67](#bib.bib67)] |'
- en: '&#124; Replacing original billboard with adversarial billboard by &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过 &#124; 用对抗性广告牌替换原始广告牌'
- en: '&#124; solving an optimization problem &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 解决优化问题 &#124;'
- en: '| White-box | Digital dataset |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 白盒 | 数字数据集 |'
- en: '| [[68](#bib.bib68)] |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| [[68](#bib.bib68)] |'
- en: '&#124; Drawing black strips on the road by Bayesian &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过贝叶斯方法在路面上绘制黑条 &#124;'
- en: '&#124; Optimization method &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 优化方法 &#124;'
- en: '| Black-box | Simulation environment |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 黑盒 | 模拟环境 |'
- en: '| [[70](#bib.bib70)] |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| [[70](#bib.bib70)] |'
- en: '&#124; Drawing black strips on the road by Gradient-based &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过基于梯度的方法在路面上绘制黑条 &#124;'
- en: '&#124; Optimization method &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 优化方法 &#124;'
- en: '| Black-box |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 黑盒 |'
- en: '| Object detection | [[71](#bib.bib71)] |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 目标检测 | [[71](#bib.bib71)] |'
- en: '&#124; Drawing adversarial texture on other vehicles by &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过 &#124; 在其他车辆上绘制对抗性纹理'
- en: '&#124; a discrete search method &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 离散搜索方法 &#124;'
- en: '| Black-box |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 黑盒 |'
- en: '| 3D Object detection | [[72](#bib.bib72)] |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 3D对象检测 | [[72](#bib.bib72)] |'
- en: '&#124; Generating adversarial points by optimization- &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过优化生成对抗点 &#124;'
- en: '&#124; based method &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于方法 &#124;'
- en: '| White-box |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 白盒 |'
- en: '| [[73](#bib.bib73)] | Inserting attack trace into original point clouds |
    Black-box | Digital dataset |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| [[73](#bib.bib73)] | 将攻击痕迹插入原始点云 | 黑盒 | 数字数据集 |'
- en: '| Traffic sign recognition | [[74](#bib.bib74)] |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 交通标志识别 | [[74](#bib.bib74)] |'
- en: '&#124; Replacing true traffic signs with adversarial traffic signs &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用对抗性交通标志替换真实交通标志 &#124;'
- en: '&#124; generated by solving an optimization problem &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过解决优化问题生成的 &#124;'
- en: '| White-box | Real world |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 白盒 | 真实世界 |'
- en: '| [[75](#bib.bib75)] |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| [[75](#bib.bib75)] |'
- en: '&#124; Pasting adversarial stickers that generated by optimization-based &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 粘贴通过优化生成的对抗性贴纸 &#124;'
- en: '&#124; approach on traffic signs &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交通标志的攻击方法 &#124;'
- en: '| White-box | Real world |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 白盒 | 真实世界 |'
- en: '| [[21](#bib.bib21)] | Generating transferable adversarial patches by GAN |
    Black-box | Real world |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| [[21](#bib.bib21)] | 通过GAN生成可转移的对抗性补丁 | 黑盒 | 真实世界 |'
- en: '| [[77](#bib.bib77)] |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| [[77](#bib.bib77)] |'
- en: '&#124; Generate transferable adversarial traffic signs and stickers by &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过 &#124; 生成可转移的对抗性交通标志和贴纸'
- en: '&#124; Feature-interference reinforcement &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征干扰增强 &#124;'
- en: '| Black-box | Real world |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 黑盒 | 现实世界 |'
- en: '| E2E driving model | [[79](#bib.bib79)] | Generate adversarial billboard by
    GAN | White-box | Real-world |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| E2E 驾驶模型 | [[79](#bib.bib79)] | 通过 GAN 生成对抗性广告牌 | 白盒 | 现实世界 |'
- en: '| Poisoning attacks | E2E driving model | [64] | Adding poisoning images with
    triggers into training data | White-box |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 中毒攻击 | E2E 驾驶模型 | [64] | 将带触发器的中毒图像添加到训练数据中 | 白盒 |'
- en: '&#124; Simulation &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟 &#124;'
- en: '&#124; environment &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 环境 &#124;'
- en: '|'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Traffic sign recognition | [[89](#bib.bib89)] | White-box | Digital dataset
    |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 交通标志识别 | [[89](#bib.bib89)] | 白盒 | 数字数据集 |'
- en: '| Rain drop removing | [[90](#bib.bib90)] | Adding poisoning image pairs with
    triggers into training data | White-box | Digital dataset |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 雨滴去除 | [[90](#bib.bib90)] | 将带触发器的中毒图像对添加到训练数据中 | 白盒 | 数字数据集 |'
- en: IV Defense methods
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 防御方法
- en: 'In this section, we take a close look at some existing defense methods against
    both physical attacks and adversarial attacks. We also briefly discuss about defenses
    for cloud services. The limitations of current defenses against adversarial evasion
    and poisoning attacks are further analyzed and summarized in Table [III](#S4.T3
    "TABLE III ‣ IV-C Defense against adversarial evasion attacks ‣ IV Defense methods
    ‣ Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses").'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将详细查看一些针对物理攻击和对抗攻击的现有防御方法。我们还将简要讨论云服务的防御。当前对抗规避和中毒攻击的防御局限性在表[III](#S4.T3
    "TABLE III ‣ IV-C Defense against adversarial evasion attacks ‣ IV Defense methods
    ‣ Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses")中进一步分析和总结。'
- en: IV-A Defense against physical sensor attacks
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 针对物理传感器攻击的防御
- en: Among all the countermeasures for physical sensor attacks, redundancy [[47](#bib.bib47),
    [49](#bib.bib49), [51](#bib.bib51)] is the most promising strategy to defend jamming
    attacks. Redundancy means that a number of the same sensors are deployed to collect
    a designated type of data and fuse them as the final input for the perception
    layer. For example, when attackers commit the blinding attack on one camera, others
    could still collect normal images for the environment perception. Undoubtedly,
    this method leads to more financial costs. Also, sensor data fusion is generally
    considered as an intractable research issue. To improve the robustness of cameras,
    another approach is that a near-infrared-cut filter is applied to filter the near-infrared
    light in the daytime to improve the quality of collected images [[47](#bib.bib47)],
    which is however unable to work effectively at night time. Alternatively, using
    photochromic lenses to filter a specific type of light is also an option to upgrade
    cameras. Subsequently, jamming attacks on these cameras could be mitigated. For
    ultrasonic sensors and radars, as noises hardly occur in a normal working environment,
    it is not difficult to build a detection system to detect the incoming jamming
    attacks [[49](#bib.bib49)]. Moreover, a jamming detection system for GPS was proposed
    in [[53](#bib.bib53)], which expedites GPS information from multiple sources such
    as roadside monitoring stations and mobile phones to improve the accuracy of GPS
    information.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有物理传感器攻击的对策中，冗余[[47](#bib.bib47), [49](#bib.bib49), [51](#bib.bib51)] 是防御干扰攻击最有前途的策略。冗余意味着部署多个相同的传感器来收集指定类型的数据，并将它们融合为感知层的最终输入。例如，当攻击者对一个摄像头实施盲目攻击时，其他摄像头仍能收集环境感知所需的正常图像。毫无疑问，这种方法会导致更多的财务成本。此外，传感器数据融合通常被认为是一个难以处理的研究问题。为了提高摄像头的鲁棒性，另一种方法是使用近红外切割滤镜来过滤白天的近红外光，以提高图像质量[[47](#bib.bib47)]，但该方法在夜间效果有限。另一种选择是使用光致变色镜片来过滤特定类型的光，以提升摄像头性能。随后，可以减轻对这些摄像头的干扰攻击。对于超声波传感器和雷达，由于在正常工作环境中噪声很少出现，构建一个检测系统来检测即将到来的干扰攻击并不困难[[49](#bib.bib49)]。此外，[[53](#bib.bib53)]提出了一种用于
    GPS 的干扰检测系统，通过来自路边监测站和手机的多个来源加速 GPS 信息，以提高 GPS 信息的准确性。
- en: One effective way to defend spoofing attacks is to introduce randomness into
    data collection [[47](#bib.bib47), [49](#bib.bib49)]. For example, attackers could
    commit accurate attacks on LiDAR because there is a fixed probe window for LiDAR
    to receive signals. If the probe time is set to be random, it then becomes harder
    for adversaries to send fake signals. PyCRA is a spoofing detection system based
    on this idea [[93](#bib.bib93)]. Furthermore, data fusion mechanism is considered
    effective to defend spoofing attacks. Therefore, fusing data from cameras, LiDAR,
    radars and ultrasonic sensors could help stabilize the performance of the perception
    layer.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 防御欺骗攻击的一种有效方法是引入数据收集的随机性 [[47](#bib.bib47), [49](#bib.bib49)]。例如，攻击者可能会对LiDAR进行准确攻击，因为LiDAR有一个固定的探测窗口来接收信号。如果探测时间设置为随机的，攻击者发送伪造信号将变得更加困难。PyCRA是基于这一理念的欺骗检测系统 [[93](#bib.bib93)]。此外，数据融合机制被认为对防御欺骗攻击有效。因此，融合来自摄像头、LiDAR、雷达和超声波传感器的数据可以帮助稳定感知层的性能。
- en: There are some obvious limitations to the existing sensor attacks. For instance,
    many attacks require external hardware to generate noises and fake signals within
    a short distance near the target vehicle. A human may recognize the occurrence
    of attacks such as the camera blinding attack from the front of the vehicle and
    take over the vehicle to avoid accidents. Therefore, even if the development of
    autonomous vehicles achieves a highly automated level, it is still necessary to
    set a security guard in the vehicle as a guarantee.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现有传感器攻击存在一些明显的限制。例如，许多攻击需要外部硬件在目标车辆附近短距离内生成噪音和伪造信号。人类可能会识别如车辆前方的摄像头盲区攻击，并接管车辆以避免事故。因此，即使自动驾驶车辆的发展达到了高度自动化水平，车辆中仍然需要设置安全防护作为保障。
- en: IV-B Defense for cloud services
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 云服务防御
- en: In the V2X map updating process, the HD map needs to be secured for authenticity
    and integrity. Each map package should contain the unique identity of the service
    provider. The integrity and confidentiality should also be ensured during the
    updating to prevent stealing or changing of data. In [[57](#bib.bib57)], encryption
    and authentication are applied for GPS data during transmission to defend message
    falsification attacks. In [[94](#bib.bib94)], a symmetric key encryption-based
    update technique was proposed to apply a link key between the service supplier
    and vehicles to form a secure package updating connection. In [[95](#bib.bib95)],
    a hash function-based update technique was proposed. This technique first divided
    the package into several data fragments and then created a hash chain of these
    data fragments in the decreasing order. Before the package being collected by
    the vehicle, elements in the hash chain were encoded using the pre-shared encryption
    key.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在V2X地图更新过程中，HD地图需要确保其真实性和完整性。每个地图包应包含服务提供者的唯一身份。更新过程中也应确保完整性和机密性，以防数据被窃取或篡改。在 [[57](#bib.bib57)]中，应用了加密和认证来防御GPS数据传输中的消息伪造攻击。在
    [[94](#bib.bib94)]中，提出了一种基于对称密钥加密的更新技术，用于在服务供应商和车辆之间应用链路密钥，以形成安全的包更新连接。在 [[95](#bib.bib95)]中，提出了一种基于哈希函数的更新技术。该技术首先将包分成若干数据片段，然后按递减顺序创建这些数据片段的哈希链。在车辆收集包之前，哈希链中的元素使用预共享的加密密钥进行了编码。
- en: IV-C Defense against adversarial evasion attacks
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 对抗对抗性规避攻击
- en: 'Currently, many defenses against adversarial evasion attacks are proposed.
    In this survey, we reviewed existing defenses and divided them into different
    categories. Adversarial defenses can be categorized into proactive and reactive
    methods. The former focuses on improving the robustness of the targeted deep learning
    models, while the latter aims to detect and counter adversarial examples before
    they are fed into models. There are five main types of proactive defense methods,
    namely, adversarial training, network distillation, network regularization, model
    ensemble, and certified defense. The primary reactive defenses are called adversarial
    detection and adversarial transformation. Though most of defenses have only experimented
    on image classification tasks, ideas of these defenses is a good generalization
    to other tasks in autonomous driving, considering the similar approaches in improving
    the robustness of models or pre-processing model inputs that are not limited on
    image classification. To validate whether these defenses can be applied in ADSs,
    we analyzed and compared them in Section [IV-E](#S4.SS5 "IV-E Analysis of defenses
    ‣ IV Defense methods ‣ Deep Learning-Based Autonomous Driving Systems: A Survey
    of Attacks and Defenses").'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，许多对抗逃避攻击的防御方法已被提出。在本次调查中，我们回顾了现有的防御方法，并将其分为不同的类别。对抗性防御可以分为主动防御和被动防御方法。前者关注于提高目标深度学习模型的鲁棒性，而后者则旨在检测并反制在模型输入前的对抗样本。主要的主动防御方法包括对抗性训练、网络蒸馏、网络正则化、模型集成和认证防御。主要的被动防御方法称为对抗性检测和对抗性变换。虽然大多数防御方法仅在图像分类任务中进行了实验，但这些防御方法的思想可以很好地推广到自主驾驶的其他任务中，考虑到提升模型鲁棒性或对模型输入进行预处理的类似方法不仅限于图像分类。为了验证这些防御是否可以应用于自动驾驶系统，我们在第[IV-E](#S4.SS5
    "IV-E 防御分析 ‣ IV 防御方法 ‣ 基于深度学习的自主驾驶系统：攻击与防御调查")节中分析和比较了它们。
- en: 'TABLE III: Summary of adversarial defenses'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 对抗性防御的总结'
- en: '|  | Name | Function | Example | Analysis |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | 名称 | 功能 | 示例 | 分析 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Proactive defenses | Adversarial training |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 主动防御 | 对抗性训练 |'
- en: '&#124; Train a new robust model based on new &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于新的预测训练新的鲁棒模型 &#124;'
- en: '&#124; dataset that involves adversarial examples. &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 包含对抗样本的数据集。 &#124;'
- en: '| [[10](#bib.bib10)] [[11](#bib.bib11)] [[13](#bib.bib13)] | Increasing time
    and resource consumption for autonomous driving model training; only effective
    for simple attacks |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| [[10](#bib.bib10)] [[11](#bib.bib11)] [[13](#bib.bib13)] | 增加时间和资源消耗以进行自主驾驶模型训练；仅对简单攻击有效
    |'
- en: '| Defensive distillation |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 防御性蒸馏 |'
- en: '&#124; Train a new robust model by distilling hidden &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过蒸馏原始模型的隐藏 &#124;'
- en: '&#124; layer information from the original model &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 层信息训练新的鲁棒模型 &#124;'
- en: '| [[96](#bib.bib96)] |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] |'
- en: '| Model ensemble |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 模型集成 |'
- en: '&#124; Ensemble multiple models for making the final &#124;'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 集成多个模型以进行最终决策 &#124;'
- en: '&#124; prediction to improve the robustness &#124;'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以提高鲁棒性 &#124;'
- en: '| [[97](#bib.bib97)] [[98](#bib.bib98)] [[99](#bib.bib99)] | Increasing resource
    consumption |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| [[97](#bib.bib97)] [[98](#bib.bib98)] [[99](#bib.bib99)] | 增加资源消耗 |'
- en: '| Network regularization |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 网络正则化 |'
- en: '&#124; Train a robust model based on a new objective &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于新的目标训练鲁棒模型 &#124;'
- en: '&#124; function containing perturbation-based regularizer &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 包含基于扰动的正则化函数 &#124;'
- en: '| [[100](#bib.bib100)] [[101](#bib.bib101)] [[102](#bib.bib102)] | Increasing
    time and resource consumption for autonomous driving model training; only effective
    for simple attacks |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| [[100](#bib.bib100)] [[101](#bib.bib101)] [[102](#bib.bib102)] | 增加时间和资源消耗以进行自主驾驶模型训练；仅对简单攻击有效
    |'
- en: '| Certified robustness |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 认证鲁棒性 |'
- en: '&#124; Change the architecture of the model to make &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 更改模型的架构以使 &#124;'
- en: '&#124; it provably robustness against certain adversarial examples &#124;'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对某些对抗样本具有可证明的鲁棒性 &#124;'
- en: '| [[103](#bib.bib103)] [[104](#bib.bib104)] [[105](#bib.bib105)] |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| [[103](#bib.bib103)] [[104](#bib.bib104)] [[105](#bib.bib105)] |'
- en: '| Reactive defenses | Adversarial detection |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 被动防御 | 对抗性检测 |'
- en: '&#124; Detect adversarial examples by a detector or &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过检测器检测对抗样本或 &#124;'
- en: '&#124; verifying the feature representation of inputs; &#124;'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 验证输入的特征表示； &#124;'
- en: '&#124; Detect hijacked image with triggers or identify &#124;'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测带有触发器的劫持图像或识别 &#124;'
- en: '&#124; poisoning attack in the model &#124;'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型中的中毒攻击 &#124;'
- en: '|'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[106](#bib.bib106)] [[107](#bib.bib107)] [[108](#bib.bib108)] &#124;'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[106](#bib.bib106)] [[107](#bib.bib107)] [[108](#bib.bib108)] &#124;'
- en: '&#124; [[115](#bib.bib115)] [[116](#bib.bib116)] [[117](#bib.bib117)] &#124;'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[115](#bib.bib115)] [[116](#bib.bib116)] [[117](#bib.bib117)] &#124;'
- en: '|'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Detector is not available if it requires much &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果探测器需要大量资源，它将不可用 &#124;'
- en: '&#124; resource &#124;'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 资源 &#124;'
- en: '|'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Adversarial transformation |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 对抗变换 |'
- en: '&#124; Apply transformation to convert adversarial examples &#124;'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用变换以转换对抗样本 &#124;'
- en: '&#124; back to clean images &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 返回以清理图像 &#124;'
- en: '| [[109](#bib.bib109)] [[110](#bib.bib110)] [[111](#bib.bib111)] [[112](#bib.bib112)]
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| [[109](#bib.bib109)] [[110](#bib.bib110)] [[111](#bib.bib111)] [[112](#bib.bib112)]
    |'
- en: '&#124; They may reduce the performance of &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 它们可能会降低 &#124;'
- en: '&#124; autonomous driving models under normal conditions &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正常条件下的自动驾驶模型 &#124;'
- en: '|'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: IV-C1 Proactive defenses
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 主动防御
- en: Adversarial training was initially proposed in [[10](#bib.bib10)]. This defense
    method targeted to re-train a more robust model on the dataset that combines original
    data and adversarial examples. In [[11](#bib.bib11)], the experiment result showed
    that adversarial training was just useful to defend against one-step attacks that
    generate adversarial examples by only one-time operation. In [[13](#bib.bib13)],
    a method to combine multiple attacks together was proposed to generate adversarial
    examples for adversarial training. However, it failed to improve the robustness
    of models against unseen attacks.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练最初在 [[10](#bib.bib10)] 中提出。这种防御方法旨在在结合原始数据和对抗样本的数据集上重新训练一个更鲁棒的模型。在 [[11](#bib.bib11)]
    中，实验结果显示对抗训练仅对通过一次操作生成对抗样本的一步攻击有效。在 [[13](#bib.bib13)] 中，提出了一种将多个攻击组合在一起的方法，以生成用于对抗训练的对抗样本。然而，它未能提高模型对未见攻击的鲁棒性。
- en: Defensive distillation was proposed in [[96](#bib.bib96)]. This defense method
    trained a new model by using probability logits information of the original model
    as the soft labels. The new model trained in this way is less sensitive to the
    change of gradients, so it is more robust against adversarial examples. However,
    a new optimization-based attacks was proposed in [[14](#bib.bib14)] to bypass
    the defense.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 防御性蒸馏在 [[96](#bib.bib96)]中被提出。这种防御方法通过使用原始模型的概率对数信息作为软标签来训练新模型。这样训练的新模型对梯度的变化不那么敏感，因此对抗样本的鲁棒性更强。然而，在 [[14](#bib.bib14)]中提出了一种新的基于优化的攻击来绕过该防御。
- en: Network regularization methods train models against adversarial examples by
    adding another adversarial-perturbation based regularizer into the original objective
    function [[100](#bib.bib100)]. In [[101](#bib.bib101)], contractive autoencoders
    were proposed and generalized into neural networks by using $L_{2}$ norm of the
    layer-wise Jacobian matrices as the regularizer. In [[102](#bib.bib102)], a parameter
    $\alpha$ was introduced to control the overall Lipschitz constant of the whole
    model. Experiments on CIFAR-10/CIFAR-100 [[113](#bib.bib113)] showed that such
    network regularized models have higher robustness against FGSM attack than the
    original models.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 网络正则化方法通过将另一种基于对抗扰动的正则项添加到原始目标函数中来训练模型以应对对抗样本 [[100](#bib.bib100)]。在 [[101](#bib.bib101)]
    中，提出了收缩自编码器，并通过使用层次雅可比矩阵的 $L_{2}$ 范数作为正则项将其推广到神经网络中。在 [[102](#bib.bib102)] 中，引入了一个参数
    $\alpha$ 来控制整个模型的 Lipschitz 常数。对 CIFAR-10/CIFAR-100 [[113](#bib.bib113)] 的实验表明，这种网络正则化模型对
    FGSM 攻击的鲁棒性优于原始模型。
- en: Model ensemble methods were designed to improve the robustness by constructing
    an ensemble model that aggregates several individual models [[97](#bib.bib97)].
    In [[98](#bib.bib98)], a random self-ensemble approach was proposed to derive
    the final prediction results by averaging predictions over random noises that
    are injected into the model. This approach is equivalent to ensembling infinite
    number of noisy models. In [[99](#bib.bib99)], an adaptive approach was proposed
    to train individual models with larger diversity. Then the ensemble of individual
    models could achieve better robustness because the attack is more difficult to
    transfer among individual models.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 模型集成方法旨在通过构建一个集成模型，该模型聚合了多个单独的模型来提高鲁棒性 [[97](#bib.bib97)]。在 [[98](#bib.bib98)]
    中，提出了一种随机自集成的方法，通过对注入模型的随机噪声进行预测平均来得出最终的预测结果。这种方法相当于集成无限数量的噪声模型。在 [[99](#bib.bib99)]
    中，提出了一种自适应方法来训练具有更大多样性的单个模型。然后，单个模型的集成可以实现更好的鲁棒性，因为攻击在单个模型之间转移更困难。
- en: Certified robustness methods aim to provide provable defense against adversarial
    attacks with adversarial examples generated by several threat models [[103](#bib.bib103),
    [104](#bib.bib104), [105](#bib.bib105)]. In [[103](#bib.bib103)], a method called
    PixelDP was proposed as certified defense. This method adds an additional noise
    layer into the original model to serve the purpose of random perturbation whose
    size is smaller than a threshold on original inputs or features representations.
    The new model is then more robust against adversarial examples if the injected
    perturbations smaller than the pre-defined threshold.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 认证鲁棒性方法旨在提供对抗攻击的可证明防御，使用由多种威胁模型生成的对抗样本[[103](#bib.bib103), [104](#bib.bib104),
    [105](#bib.bib105)]。在[[103](#bib.bib103)]中，提出了一种称为PixelDP的认证防御方法。该方法在原始模型中增加了一层额外的噪声，以实现随机扰动，其大小小于原始输入或特征表示上的阈值。如果注入的扰动小于预定义的阈值，则新模型对对抗样本具有更强的鲁棒性。
- en: IV-C2 Reactive defense
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 反应性防御
- en: Adversarial detection could detect adversarial examples by introducing another
    classifier that could differentiate the feature representation of adversarial
    examples from natural images. In [[106](#bib.bib106)], an intrinsic-defender (I-defender)
    was proposed to identify adversarial examples from original images under unknown
    attack methods. I-defender explores an intrinsic property of the target model,
    e.g., the distribution of hidden states of normal training sets, and then uses
    the intrinsic property to detect adversarial examples. Similarly, an effective
    method for DNNs with the softmax layer was proposed in [[107](#bib.bib107)] to
    detect abnormal samples including out-of-distribution (OOD) and adversarial examples.
    The idea was to use Gaussian discriminant analysis [[114](#bib.bib114)] to measure
    the probability density of test samples on feature spaces of DNNs. In [[108](#bib.bib108)],
    an approach called Feature Squeezing was proposed to detect adversarial examples
    by squeezing the color bit depth of each pixel. If the difference between the
    predictions on the original input and the squeezed input is over a threshold,
    the original input is more likely an adversarial example.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗检测可以通过引入另一个分类器来检测对抗样本，该分类器能够区分对抗样本的特征表示与自然图像。在[[106](#bib.bib106)]中，提出了一种内在防御器（I-defender）来识别来自原始图像的对抗样本，适用于未知攻击方法。I-defender探索目标模型的内在特性，例如正常训练集的隐藏状态分布，然后利用这些内在特性来检测对抗样本。类似地，在[[107](#bib.bib107)]中，提出了一种有效的方法来检测包括分布外（OOD）和对抗样本在内的异常样本，适用于具有softmax层的DNN。该方法的思路是使用高斯判别分析[[114](#bib.bib114)]来测量测试样本在DNN特征空间上的概率密度。在[[108](#bib.bib108)]中，提出了一种称为特征压缩的方法，通过压缩每个像素的颜色位深来检测对抗样本。如果原始输入和压缩输入上的预测差异超过阈值，则原始输入更可能是对抗样本。
- en: Adversarial transformation is a set of approaches that could apply transformations
    on adversarial examples to reconstruct them back to clean images. In [[109](#bib.bib109)],
    the effects of five image transformations for defending against FGSM, I-FGSM,
    DeepFool and C&W attacks were investigated. The results showed that transformations
    were partially effective to defend against adversarial perturbations, while randomized
    (e.g., image cropping) and non-differentiable (e.g., total variation minimization)
    transformations were stronger defenses. In [[110](#bib.bib110)], a framework called
    defense-GAN was proposed, which learns the underlying distribution of the image
    dataset and can generate images falling in this distribution. When an adversarial
    example was fed into the target model, the defense-GAN generated many images that
    are similar to the adversarial example in $L_{2}$ distance and then search the
    optimal one as the input of the target model. In [[111](#bib.bib111)], another
    GAN model called Adversarial Perturbation Elimination GAN (APE-GAN) was proposed
    to denoise adversarial examples, which uses adversarial examples $X^{\prime}$
    as the input to directly output their corresponding denoised images $\mathbf{G}(X^{\prime})$.
    The experiments confirmed that APE-GAN is able to defend against common attacks.
    In [[112](#bib.bib112)], the High-Level Representation Guided Denoiser (HGD) was
    proposed to transform adversarial examples through an auto-encoder network. The
    key idea of HGD is that it does not minimize the $L_{2}$ distance between the
    generated image and original image but shortens the distance of feature representations
    at $l$-th layer of the target model $f$. The experiment result shows that HGD
    ranks first in NIPS adversarial defenses competition [[97](#bib.bib97)].
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性变换是一组可以对对抗样本应用变换以将其重新构建为干净图像的方法。在 [[109](#bib.bib109)] 中，研究了五种图像变换在防御 FGSM、I-FGSM、DeepFool
    和 C&W 攻击中的效果。结果表明，变换对防御对抗扰动有一定效果，而随机变换（如图像裁剪）和不可微分变换（如总变差最小化）则表现出更强的防御能力。在 [[110](#bib.bib110)]
    中，提出了一个名为 defense-GAN 的框架，它学习图像数据集的底层分布，并能够生成符合该分布的图像。当对抗样本输入到目标模型中时，defense-GAN
    生成许多在 $L_{2}$ 距离上与对抗样本相似的图像，然后搜索最佳图像作为目标模型的输入。在 [[111](#bib.bib111)] 中，提出了另一种
    GAN 模型，称为对抗扰动消除 GAN（APE-GAN），用于去噪对抗样本，它使用对抗样本 $X^{\prime}$ 作为输入，直接输出其对应的去噪图像 $\mathbf{G}(X^{\prime})$。实验确认
    APE-GAN 能够防御常见的攻击。在 [[112](#bib.bib112)] 中，提出了高层次表示引导去噪器（HGD），通过自编码器网络对对抗样本进行变换。HGD
    的关键思想是，它不是最小化生成图像和原始图像之间的 $L_{2}$ 距离，而是缩短目标模型 $f$ 的第 $l$ 层特征表示的距离。实验结果显示，HGD 在
    NIPS 对抗性防御竞赛中排名第一 [[97](#bib.bib97)]。
- en: IV-D Defense against adversarial poisoning attacks
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 对抗性投毒攻击的防御
- en: A number of defense methods for poisoning attacks have been proposed in some
    recent research works. The general philosophy is to only detect whether the current
    input image is a hijacked image with triggers. Another high level thought is to
    identify the poisoning attack in the model and then remove the backdoor or Trojan.
    Both of the ideas belong to reactive adversarial detection defenses. In [[115](#bib.bib115)],
    a detection method called STRIP was proposed, which compared the predictions of
    the original input image and a perturbed input image that is generated by superimposing
    another clean image from training data. If the input image did not contain a trigger,
    the predictions of input image and perturbed image should be different. However,
    if the input image was deemed to contain a trigger, the predictions should be
    same because the perturbed image also contains the trigger that dominates the
    prediction of the model. In this manner, the hijacked image with a trigger could
    thus be detected.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 一些近期研究工作中提出了针对投毒攻击的防御方法。一般的思路是仅检测当前输入图像是否为带有触发器的劫持图像。另一种高层次的想法是识别模型中的投毒攻击，然后移除后门或特洛伊木马。这两种思路都属于反应性对抗检测防御。在
    [[115](#bib.bib115)] 中，提出了一种名为 STRIP 的检测方法，它比较了原始输入图像和通过叠加来自训练数据的另一张干净图像生成的扰动输入图像的预测。如果输入图像不包含触发器，则输入图像和扰动图像的预测应该不同。然而，如果输入图像被认为包含触发器，则预测应该相同，因为扰动图像也包含触发器，这主导了模型的预测。通过这种方式，可以检测到带有触发器的劫持图像。
- en: In [[116](#bib.bib116)], a detection method was proposed to distinguish the
    clean input image from the malicious ones with a trigger. The method was based
    on an observation that even though clean images and hijacked images were classified
    to have the same label, the final output of the last activation layer is drastically
    different. Due to the observation, the method adopted a clustering algorithm to
    group the poisonous data owing to this difference. In [[117](#bib.bib117)], a
    comprehensive method was proposed to identify and mitigate poisoning attacks at
    the model level. Firstly, different triggers were created to attack each label,
    and the weights of neurons activated by the detected trigger were then removed
    to makes the trigger ineffective. The experiment results illustrated that this
    approach could significantly reduce attack success rates, even going down from
    over 90% to 0% for some poisoning attacks.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[116](#bib.bib116)]中，提出了一种检测方法，用于通过触发器区分干净的输入图像和恶意图像。该方法基于一个观察结果，即使干净的图像和被劫持的图像被分类为相同标签，最后激活层的输出也会大相径庭。基于这一观察，方法采用了聚类算法来将有毒数据分组。在[[117](#bib.bib117)]中，提出了一种全面的方法来识别和缓解模型层面的中毒攻击。首先，为每个标签创建了不同的触发器，然后移除了由检测到的触发器激活的神经元的权重，使触发器失效。实验结果表明，这种方法可以显著降低攻击成功率，有些中毒攻击的成功率甚至从超过90%降到了0%。
- en: IV-E Analysis of defenses
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 防御分析
- en: 1). Defense against physical sensor attacks is costly but effective. Redundancy
    defence requires to use numerous sensors in the same type to collect the target
    data and combines the data together before sending it to the perception layer.
    Even though it leads to a significant expenditure on the sensors, redundancy is
    considered as a simple yet effective way to defence jamming attack. Apart from
    the cost, the technical issue of data fusion also needs to be taken into account.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 防御物理传感器攻击虽然成本高，但效果显著。冗余防御要求使用多种相同类型的传感器来收集目标数据，并在将数据发送到感知层之前将其合并。尽管这会导致传感器的开支显著增加，但冗余被认为是一种简单而有效的防御干扰攻击的方式。除了成本之外，数据融合的技术问题也需要考虑。
- en: '2). Current adversarial defense methods are not suitable in autonomous vehicles.
    Table [III](#S4.T3 "TABLE III ‣ IV-C Defense against adversarial evasion attacks
    ‣ IV Defense methods ‣ Deep Learning-Based Autonomous Driving Systems: A Survey
    of Attacks and Defenses") summarizes the reviewed defense techniques. For proactive
    methods, adversarial training and defensive distillation need to train a new robust
    model following the original model training. However, the training of autonomous
    driving models generally requires large datasets and incurs a significant training
    time. Importing these techniques will undoubtedly result in the resource overhead.
    Moreover, adversarial training and defensive distillation are only effective when
    dealing with simple adversarial attacks like FGSM. As stated in the preceding
    section, model ensemble methods take advantage of results from multiple models
    to improve the robustness, which also cause large extra resource overhead. On
    the other hand, network regularization and robustness methods can be integrated
    into the training process of autonomous driving models without incurring large
    extra resource overhead. Yet, it is worth mentioning that such methods mostly
    experimented on DL models with simple network architecture, and its effectiveness
    needs to be further verified in ADS settings. For reactive methods, adversarial
    transformation process could achieve a satisfactory result when applying on adversarial
    examples. Still, the performance may degrade on normal inputs, which is unacceptable
    for safety-critical autonomous vehicles. When it comes to the adversarial detection,
    some techniques suggest to take advantage of other classifiers to detect adversarial
    examples, which is also infeasible as the classifier requires additional computation
    resources and might violate stringent timing constraints in ADSs. Therefore, other
    adversarial detection methods that do not cause considerable resource overhead
    could be incorporated in autonomous driving models. Also, other techniques that
    are helpful for improving the robustness of autonomous driving models should be
    explored in the future. In addition, as autonomous driving is a real-time interactive
    process, thus real-time monitoring and defense are of great importance in order
    to keep the safety of autonomous vehicles.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '2). 当前的对抗防御方法不适用于自动驾驶车辆。表格[III](#S4.T3 "TABLE III ‣ IV-C Defense against adversarial
    evasion attacks ‣ IV Defense methods ‣ Deep Learning-Based Autonomous Driving
    Systems: A Survey of Attacks and Defenses")总结了评审的防御技术。对于主动防御方法，对抗训练和防御蒸馏需要在原始模型训练后训练一个新的鲁棒模型。然而，自动驾驶模型的训练通常需要大规模的数据集，并且耗时较长。引入这些技术无疑会导致资源开销。此外，对抗训练和防御蒸馏只对处理像FGSM这样的简单对抗攻击有效。如前文所述，模型集成方法利用多个模型的结果来提高鲁棒性，但也会导致大量额外的资源开销。另一方面，网络正则化和鲁棒性方法可以集成到自动驾驶模型的训练过程中，而不会产生大量额外的资源开销。然而，值得一提的是，这些方法大多是在具有简单网络架构的深度学习模型上进行实验的，其在自动驾驶系统中的有效性需要进一步验证。对于反应性方法，对抗变换过程在应用于对抗样本时可能会取得令人满意的结果。然而，它在正常输入上的表现可能会下降，这对安全至关重要的自动驾驶车辆来说是不可接受的。至于对抗检测，一些技术建议利用其他分类器来检测对抗样本，但这也是不可行的，因为分类器需要额外的计算资源，并可能违反自动驾驶系统中的严格时限要求。因此，可以在自动驾驶模型中结合其他不会造成较大资源开销的对抗检测方法。此外，还应在未来探索其他有助于提高自动驾驶模型鲁棒性的技术。另外，由于自动驾驶是一个实时交互过程，因此实时监控和防御对于保持自动驾驶车辆的安全至关重要。'
- en: '![Refer to caption](img/d07fb7a9a726220cbf587b58459d5130.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d07fb7a9a726220cbf587b58459d5130.png)'
- en: 'Figure 6: Overview of defense framework on ADS'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：自动驾驶系统防御框架概览
- en: V Future Directions
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 未来方向
- en: In this survey, we conduct a comprehensive review on some existing attacks,
    including both physical and adversarial ones, as well as corresponding defense
    methods along with the detailed analysis of their availability and limitation
    in the deep learning-based ADSs. This survey discusses various adversarial attacks
    that could be detrimental to deep learning autonomous driving models and identify
    relevant safety threats. In this section, we uncover further research directions
    for possible attacks on ADSs and strategies for improving the robustness of ADSs
    against adversarial attacks. In particular, we propose the potential detection
    mechanisms explicitly applicable for current autonomous vehicles to defend against
    adversarial attacks as the majority of existing adversarial defense methods are
    not designated for deep learning-based ADSs in the first place.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在本调查中，我们对一些现有的攻击进行了全面回顾，包括物理攻击和对抗攻击，以及相应的防御方法，并详细分析了它们在基于深度学习的自动驾驶系统中的可用性和局限性。本调查讨论了可能对深度学习自动驾驶模型造成危害的各种对抗攻击，并识别相关的安全威胁。在这一部分，我们揭示了对自动驾驶系统可能的攻击的进一步研究方向以及提高自动驾驶系统对抗对抗攻击鲁棒性的策略。特别是，我们提出了明确适用于当前自主驾驶车辆的潜在检测机制，以防御对抗攻击，因为大多数现有的对抗防御方法最初并未针对基于深度学习的自动驾驶系统设计。
- en: V-A Potential attacks in future research
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 未来研究中的潜在攻击
- en: V-A1 Adversarial attacks on the whole ADS
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 对整个自动驾驶系统的对抗攻击
- en: Most of existing attack-related research normally focus on single target (e.g.,
    physical attacks on cameras or GPS) or a sub-task in an ADS (e.g., adversarial
    attacks on object detectors). Some research simplify an ADS as an E2E driving
    model for attacking. However, as an ADS is composed of several layers, and inputs
    from different sensors are tend to be fused at first to provide environment information.
    The success launch of attaching on one sensor or one deep learning model does
    not necessarily mean that it would effectively make the ADS produce wrong control
    decisions. For example, object detection in autonomous vehicle could be realized
    through the fusion of camera-based and LiDAR-based deep learning models, and only
    attacking either one of them may not affect the final recognition results. Therefore,
    it is essential to investigate attacks against models based on multi-modal inputs
    and attacks against full-stack ADS like Apollo and Autoware.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现有的攻击相关研究通常关注单一目标（例如，物理攻击摄像头或GPS）或自动驾驶系统中的子任务（例如，对目标检测器的对抗攻击）。一些研究将自动驾驶系统简化为一个端到端的驾驶模型进行攻击。然而，由于自动驾驶系统由多个层组成，并且来自不同传感器的输入通常会首先融合以提供环境信息，对一个传感器或一个深度学习模型的攻击成功并不一定意味着它会有效地使自动驾驶系统产生错误的控制决策。例如，自动驾驶车辆中的目标检测可能通过基于摄像头和激光雷达的深度学习模型的融合实现，仅攻击其中一个可能不会影响最终的识别结果。因此，研究针对多模态输入模型的攻击和针对完整堆栈自动驾驶系统（如Apollo和Autoware）的攻击至关重要。
- en: V-A2 Semantic adversarial attacks
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 语义对抗攻击
- en: Currently, some research starts to investigate semantic adversarial attacks
    that focus on changing specific attributes such as light conditions and clarity
    of the input to generate natural adversarial examples. The existence of semantic
    adversarial attacks demonstrates that deep learning models tend to make mistakes
    in real-world even without adversary, meaning that weather, light, or other conditions
    could easily be turned into semantic adversarial attributes in coincidence. Such
    uncertainty would pose unexpected threats to autonomous vehicles. Therefore, the
    research of semantic adversarial attacks is necessary in terms of achieving better
    performance and robustness of deep learning models applied in ADSs.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，一些研究开始探讨专注于改变特定属性（如光照条件和输入清晰度）以生成自然对抗样本的语义对抗攻击。语义对抗攻击的存在表明，即使没有对抗者，深度学习模型在现实世界中也容易出错，这意味着天气、光线或其他条件很容易巧合地变成语义对抗属性。这种不确定性将对自主驾驶车辆构成意外威胁。因此，在实现深度学习模型在自动驾驶系统中更好性能和鲁棒性方面，研究语义对抗攻击是必要的。
- en: V-A3 Reverse-engineering attacks
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A3 逆向工程攻击
- en: Other than adversarial attacks, reverse-engineering attacks on ADSs are another
    possible research direction. For instance, an approach to construct a metamodel
    was proposed in [[118](#bib.bib118)] for predicting attributes of black-box classifiers.
    Based on extracted attributes, adversarial examples could be created to attack
    black-box classifiers. Also, the parameters of a neural network could be recovered
    by using the side-channel analysis technique [[119](#bib.bib119)]. Since deep
    learning models are now widely adopted in the industry, the valuable information
    contained in the structure and parameters should be protected securely. Simply
    put, the model should be robust enough against various reverse-engineering attacks
    to preserve both integrity and stability of the model.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对抗性攻击，针对自动驾驶系统（ADS）的逆向工程攻击也是一个可能的研究方向。例如，在[[118](#bib.bib118)]中提出了一种构建元模型的方法，用于预测黑箱分类器的属性。基于提取的属性，可以创建对抗样本来攻击黑箱分类器。此外，还可以使用侧信道分析技术[[119](#bib.bib119)]来恢复神经网络的参数。由于深度学习模型现在在行业中被广泛应用，因此应该安全地保护模型结构和参数中包含的有价值信息。简单来说，模型应该足够稳健，以抵御各种逆向工程攻击，从而保持模型的完整性和稳定性。
- en: V-B Strategies for robustness improvement
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 增强鲁棒性的策略
- en: 'Based on reviewed attacks and defenses, we propose a defense framework to improve
    the robustness of ADSs as shown in Figure [6](#S4.F6 "Figure 6 ‣ IV-E Analysis
    of defenses ‣ IV Defense methods ‣ Deep Learning-Based Autonomous Driving Systems:
    A Survey of Attacks and Defenses"). The framework could be applied as practice
    in industry. Specifically, we propose four strategies hardware redundancy, robust
    model training, model testing and verification, and anomaly detection that could
    be investigated in the future.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '基于已审查的攻击和防御，我们提出了一个防御框架，以提高自动驾驶系统的鲁棒性，如图[6](#S4.F6 "Figure 6 ‣ IV-E Analysis
    of defenses ‣ IV Defense methods ‣ Deep Learning-Based Autonomous Driving Systems:
    A Survey of Attacks and Defenses")所示。该框架可以在行业中应用于实际操作。具体来说，我们提出了四种策略：硬件冗余、鲁棒模型训练、模型测试与验证以及异常检测，这些策略可以在未来进行研究。'
- en: V-B1 Hardware redundancy
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B1 硬件冗余
- en: 'As discussed in Section [V-A1](#S5.SS1.SSS1 "V-A1 Adversarial attacks on the
    whole ADS ‣ V-A Potential attacks in future research ‣ V Future Directions ‣ Deep
    Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses"),
    current attacks only focus on one specific target in ADSs, applying multiple sensors
    to perceive the environment hence is a good way to improve the robustness. In
    addition, with the development of V2X, an autonomous vehicle can receive information
    from roadside units like surveillance cameras or from other nearby vehicles. By
    fusing sensor data from V2X clients and data collected by sensors on the vehicle,
    the perceived environment information would be more robust against being turned
    into adversarial input.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '正如在章节[V-A1](#S5.SS1.SSS1 "V-A1 Adversarial attacks on the whole ADS ‣ V-A Potential
    attacks in future research ‣ V Future Directions ‣ Deep Learning-Based Autonomous
    Driving Systems: A Survey of Attacks and Defenses")中讨论的，目前的攻击仅关注自动驾驶系统中的一个特定目标，因此应用多个传感器感知环境是一种提高鲁棒性的良好方式。此外，随着V2X的发展，自动驾驶车辆可以从路边单元（如监控摄像头）或其他附近车辆接收信息。通过融合来自V2X客户端的传感器数据和车辆上传感器收集的数据，感知环境信息会更能抵御被转化为对抗性输入的风险。'
- en: V-B2 Model robustness training
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B2 模型鲁棒性训练
- en: From the perspective of adversarial defense, training autonomous driving models
    that are naturally robust against adversarial examples is a promising research
    direction. For instance, network regularization follows this line of thought.
    However, many network regularization methods merely focus on specific adversarial
    examples. Recently, in [[120](#bib.bib120)], a new regularization method was proposed
    by introducing surrogate loss to improve the robustness of models. This method
    won the first place in the NeurIPS 2018 Adversarial Vision Challenge to defend
    adversarial examples. Another assuring approach to improve the robustness is to
    modify the network architecture of models.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 从对抗防御的角度来看，训练对抗样本自然鲁棒的自动驾驶模型是一个有前景的研究方向。例如，网络正则化遵循了这种思路。然而，许多网络正则化方法仅关注特定的对抗样本。最近，在[[120](#bib.bib120)]中，提出了一种通过引入代理损失来提高模型鲁棒性的新的正则化方法。这种方法在2018年NeurIPS对抗视觉挑战赛中获得了第一名，成功防御了对抗样本。另一种提升鲁棒性的可靠方法是修改模型的网络结构。
- en: V-B3 Model testing and verification
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B3 模型测试与验证
- en: After the model training stage, it is also essential to apply viable testing
    and verification techniques on the trained models to measure their performance
    against adversarial examples. Data-driven deep learning models are vastly different
    from traditional software and thus difficult to benefiting from the existing software
    engineering test methods [[122](#bib.bib122)]. Currently, some testing and verification
    tools have been developed to cope with this issue. For example, in [[123](#bib.bib123)],
    a white-box framework was proposed to exhaustively search adversarial examples.
    Therefore, applying testing and verification techniques to prevent the adversarial
    examples is another promising research direction.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练阶段之后，同样需要对训练好的模型应用可行的测试和验证技术，以测量其对对抗样本的性能。数据驱动的深度学习模型与传统软件有很大不同，因此难以从现有的软件工程测试方法中受益[[122](#bib.bib122)]。目前，一些测试和验证工具已被开发出来以应对这个问题。例如，在[[123](#bib.bib123)]中，提出了一个白盒框架，用于全面搜索对抗样本。因此，应用测试和验证技术来预防对抗样本是另一个有前景的研究方向。
- en: V-B4 Adversarial attacks detection in real time
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B4 实时对抗攻击检测
- en: 'Lastly, before deploying a robust ADS, sound adversarial attack detection and
    monitoring system are urgently needed as the last-line defense against a variety
    of attacks for autonomous vehicles in real time. Current adversarial attack detection
    methods usually rely on an auxiliary model to detect adversarial examples, which
    may not be feasible for the resource-constrained autonomous vehicles. Therefore,
    detecting abnormal behavior caused by adversarial examples without incurring the
    resource overhead is an important research direction. Adversarial detection techniques
    such as the one in  [[107](#bib.bib107)] explored in Section [IV](#S4 "IV Defense
    methods ‣ Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks
    and Defenses") do not introduce new models or layers into original autonomous
    driving models, they hence do not cause large overhead. However, these works were
    only experimented on the public datasets like MNIST and CIFAR-10\. It is essential
    to conduct comprehensive experiments on datasets of real-world autonomous driving
    tasks. Another possible research direction is to deploy an anomaly detection system
    on the Cloud/Edge server to monitor and analyze the data uploaded by autonomous
    vehicles. The Cloud/Edge server has powerful computation so we could implement
    more accurate detection methods to detect adversarial examples. However, how to
    ensure the timely response, handle time synchronization and deal with a large
    amount of sensor data in an anomaly detection system at the running time remain
    unsolved. In [[124](#bib.bib124)], a decentralized swift vigilance framework was
    proposed to recognize abnormal inputs with ultra-low latency. In [[128](#bib.bib128)],
    a highly scalable anomaly detection mechanism was created to enable the gathering
    and compression of event data in a highly distributed environment, in which a
    desired balance between response time and accuracy is well achieved.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，在部署一个稳健的自动驾驶系统（ADS）之前，急需一个健全的对抗攻击检测和监控系统，作为对各种实时攻击的最后防线。目前的对抗攻击检测方法通常依赖于辅助模型来检测对抗样本，这在资源受限的自动驾驶车辆上可能不可行。因此，如何在不增加资源开销的情况下检测由对抗样本引起的异常行为是一个重要的研究方向。像[[107](#bib.bib107)]中在第[IV](#S4
    "IV Defense methods ‣ Deep Learning-Based Autonomous Driving Systems: A Survey
    of Attacks and Defenses")节探讨的对抗检测技术，不会在原始自动驾驶模型中引入新的模型或层，因此不会造成大量开销。然而，这些研究仅在公开数据集如MNIST和CIFAR-10上进行过实验。对真实世界自动驾驶任务的数据集进行全面实验是至关重要的。另一个可能的研究方向是在云/边缘服务器上部署异常检测系统，以监控和分析自动驾驶车辆上传的数据。云/边缘服务器具有强大的计算能力，因此我们可以实施更准确的检测方法来检测对抗样本。然而，如何确保及时响应、处理时间同步以及在运行时处理大量传感器数据仍然是未解决的问题。在[[124](#bib.bib124)]中，提出了一种去中心化的快速警觉框架，用于以超低延迟识别异常输入。在[[128](#bib.bib128)]中，创建了一种高度可扩展的异常检测机制，实现了在高度分布的环境中事件数据的收集和压缩，在响应时间和准确性之间达到了良好的平衡。'
- en: VI Conclusion
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: The deep learning-based ADS is the key to realize a more intelligent self-driving
    system. However, the system is vulnerable to diverse attacks. In this survey,
    potential safe-threatening attacks are analyzed in the workflow of the deep learning-based
    ADS, including physical attacks, cyberattacks and adversarial attacks. The physical
    attack is straightforward but shows certain limits that could be dealt with defence
    methods effectively. The cyberattack is considered difficult to launch in large
    scale, while system defence methods are easy to implement. The adversarial attack
    is effective, and more defence methods against it are needed, as traditional defence
    methods are not suitable in the self-driving context. In future research, adversarial
    attacks on LiDAR and deep reinforcement models and reverse-engineering attacks
    are potential attacks must be researched. To improve the robustness of the ADS,
    model robustness training, model testing and verification and adversarial attacks
    detection in real-time should also be studied thoroughly.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的自动驾驶系统（ADS）是实现更智能自驾系统的关键。然而，该系统对各种攻击非常脆弱。在本次调查中，分析了深度学习基于ADS工作流程中的潜在安全威胁攻击，包括物理攻击、网络攻击和对抗攻击。物理攻击直接了当，但展示了某些局限性，这些局限性可以通过防御方法有效应对。网络攻击被认为难以大规模发起，而系统防御方法实施起来较为容易。对抗攻击有效，需要更多的防御方法，因为传统的防御方法在自驾环境中并不适用。在未来的研究中，LiDAR和深度强化模型上的对抗攻击及逆向工程攻击是需要研究的潜在攻击。为了提高ADS的鲁棒性，模型鲁棒性训练、模型测试与验证以及实时对抗攻击检测也应彻底研究。
- en: References
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Eureka, Programme for a European traffic system with highest efficiency
    and unprecedented safety, https://www.eurekanetwork.org/, (Accessed: 1 Dec. 220).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Eureka，欧洲交通系统最高效率和前所未有安全性的计划，https://www.eurekanetwork.org/，（访问时间：220年12月1日）。'
- en: '[2] M. Buehler, K. Iagnemma, and S. Singh, The 2005 DARPA grand challenge:
    the great robot race, Springer, 2007.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] M. Buehler, K. Iagnemma 和 S. Singh，《2005年DARPA大挑战：伟大的机器人竞赛》，Springer，2007年。'
- en: '[3] Tesla, Telsa autopilot, https://www.tesla.com/autopilot, (Accessed: 30
    Sept. 2019).'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Tesla，特斯拉自动驾驶，https://www.tesla.com/autopilot，（访问时间：2019年9月30日）。'
- en: '[4] Waymo, Waymo llc, https://waymo.com/, (Accessed: 30 Sep. 2019).'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Waymo，Waymo llc，https://waymo.com/，（访问时间：2019年9月30日）。'
- en: '[5] M. Berboucha, Uber self-driving car crash: What really happened, https://bit.ly/2YKu9WN,
    (Accessed: 30 Sep. 2019).'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. Berboucha，Uber自动驾驶车事故：究竟发生了什么，https://bit.ly/2YKu9WN，（访问时间：2019年9月30日）。'
- en: '[6] Baidu, ApolloAuto, https://github.com/ApolloAuto/apollo, 2020.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 百度，ApolloAuto，https://github.com/ApolloAuto/apollo，2020年。'
- en: '[7] Global Times, Baidu fully opens Apollo Go Robotaxi services in Beijing,
    https://www.globaltimes.cn/content/1203174.shtml, (Accessed: 1 Mar. 2021).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 环球时报，百度在北京全面开放Apollo Go Robotaxi服务，https://www.globaltimes.cn/content/1203174.shtml，（访问时间：2021年3月1日）。'
- en: '[8] Tesla, Autopilot, https://www.tesla.com/en_AU/autopilotAI, (Accessed: 1
    Mar. 2021).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Tesla，自动驾驶，https://www.tesla.com/en_AU/autopilotAI，（访问时间：2021年3月1日）。'
- en: '[9] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow,
    and R. Fergus, “Intriguing properties of neural networks,” in Proc. ICLR, Banff,
    AB, Canada, Apr. 2014.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow
    和 R. Fergus，“神经网络的迷人特性”，发表于 Proc. ICLR, Banff, AB, Canada, 2014年4月。'
- en: '[10] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
    adversarial examples,” in Proc. ICLR, San Diego, CA, USA, May. 2015.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] I. J. Goodfellow, J. Shlens 和 C. Szegedy，“解释和利用对抗样本”，发表于 Proc. ICLR, San
    Diego, CA, USA, 2015年5月。'
- en: '[11] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial machine learning
    at scale,” in Proc. ICLR, Toulon, France, Apr. 2017.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Kurakin, I. Goodfellow 和 S. Bengio，“大规模对抗性机器学习”，发表于 Proc. ICLR, Toulon,
    France, 2017年4月。'
- en: '[12] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial examples in
    the physical world,” in Proc. ICLR, Toulon, France, Apr. 2017.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Kurakin, I. J. Goodfellow 和 S. Bengio，“物理世界中的对抗样本”，发表于 Proc. ICLR,
    Toulon, France, 2017年4月。'
- en: '[13] F. Tramèr, A. Kurakin, N. Papernot, I. J. Goodfellow, D. Boneh, and P. D.
    McDaniel, “Ensemble adversarial training: Attacks and defenses,” in Proc. ICLR,
    Vancouver, BC, Canada, Apr. 2018.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] F. Tramèr, A. Kurakin, N. Papernot, I. J. Goodfellow, D. Boneh 和 P. D.
    McDaniel，“集成对抗性训练：攻击与防御”，发表于 Proc. ICLR, Vancouver, BC, Canada, 2018年4月。'
- en: '[14] N. Carlini and D. A. Wagner, “Towards evaluating the robustness of neural
    networks,” in Proc. SP, San Jose, CA, USA, May 2017, pp. 39–57.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] N. Carlini 和 D. A. Wagner，“评估神经网络鲁棒性”，发表于 Proc. SP, San Jose, CA, USA,
    2017年5月，第39–57页。'
- en: '[15] P. Y. Chen, Y. Sharma, H. Zhang, J. F. Yi, and C. Hsieh, “EAD: Elastic-net
    attacks to deep neural networks via adversarial examples,” in Proc. AAAI, New
    Orleans, Louisiana, USA, Feb. 2018, pp. 10–17.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] P. Y. Chen, Y. Sharma, H. Zhang, J. F. Yi, 和 C. Hsieh，“EAD：通过对抗样本对深度神经网络进行弹性网络攻击，”在Proc.
    AAAI, 新奥尔良, 路易斯安那州, USA, 2018年2月，pp. 10–17。'
- en: '[16] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “DeepFool: A simple and
    accurate method to fool deep neural networks,” in Proc. CVPR, Las Vegas, NV, USA,
    Jun. 2016, pp. 2574–2582.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. Moosavi-Dezfooli, A. Fawzi, 和 P. Frossard，“DeepFool：一种简单且准确的欺骗深度神经网络的方法，”在Proc.
    CVPR, 拉斯维加斯, NV, USA, 2016年6月，pp. 2574–2582。'
- en: '[17] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep
    neural networks,” IEEE Trans. Evolutionary Computation, vol. 23, no. 5, pp. 828–841,
    Oct. 2019.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J. Su, D. V. Vargas, 和 K. Sakurai，“针对深度神经网络的单像素攻击，”IEEE Trans. Evolutionary
    Computation, vol. 23, no. 5, pp. 828–841, 2019年10月。'
- en: '[18] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Universal adversarial
    perturbations,” in Proc. CVPR, Honolulu, HI, USA, Jul. 2017, pp. 86–94.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, 和 P. Frossard，“通用对抗扰动，”在Proc.
    CVPR, 檀香山, HI, USA, 2017年7月，pp. 86–94。'
- en: '[19] O. Poursaeed, I. Katsman, B. Gao, and S. J. Belongie, “Generative adversarial
    perturbations,” in Proc. CVPR, Salt Lake City, UT, USA, Jun. 2018, pp. 4422–4431.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] O. Poursaeed, I. Katsman, B. Gao, 和 S. J. Belongie，“生成对抗扰动，”在Proc. CVPR,
    盐湖城, UT, USA, 2018年6月，pp. 4422–4431。'
- en: '[20] C. Xiao, B. Li, J. Zhu, W. He, M. Liu, and D. Song, “Generating adversarial
    examples with adversarial networks,” in Proc. IJCAI, Stockholm, Sweden, Jul. 2018,
    pp. 3905–3911 .'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] C. Xiao, B. Li, J. Zhu, W. He, M. Liu, 和 D. Song，“利用对抗网络生成对抗样本，”在Proc.
    IJCAI, 斯德哥尔摩, 瑞典, 2018年7月，pp. 3905–3911。'
- en: '[21] A. Liu, X. Liu, J. Fan, Y. Ma, A. Zhang, H. Xie, and D. Tao, “Perceptual-sensitive
    GAN for generating adversarial patches,” in Proc. AAAI, Honolulu, Hawaii, USA,
    Feb. 2019, vol. 33, pp. 1028–1035.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] A. Liu, X. Liu, J. Fan, Y. Ma, A. Zhang, H. Xie, 和 D. Tao，“用于生成对抗补丁的感知敏感GAN，”在Proc.
    AAAI, 檀香山, 夏威夷, USA, 2019年2月, vol. 33, pp. 1028–1035。'
- en: '[22] K. Ren, Q. Wang, C. Wang, Z. Qin, and X. Lin, “The security of autonomous
    driving: Threats, defenses, and future directions,” Proceeding of the IEEE, vol. 108,
    no. 2, pp. 357–372, 2019.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] K. Ren, Q. Wang, C. Wang, Z. Qin, 和 X. Lin，“自动驾驶的安全性：威胁、防御与未来方向，”Proceeding
    of the IEEE, vol. 108, no. 2, pp. 357–372, 2019年。'
- en: '[23] M. Pham and K. Xiong, “A Survey on security attacks and defense techniques
    for connected and autonomous vehicles,” CoRR, vol. abs/2007.08041, 2020.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] M. Pham 和 K. Xiong，“关于连接和自动驾驶车辆的安全攻击和防御技术的调查，”CoRR, vol. abs/2007.08041,
    2020年。'
- en: '[24] N. Akhtar and A. Mian, “Threat of adversarial attacks on deep learning
    in computer vision: A survey,” IEEE Access, vol. 6, pp. 14410–14430, 2018.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] N. Akhtar 和 A. Mian，“计算机视觉中深度学习的对抗攻击威胁：一项调查，”IEEE Access, vol. 6, pp.
    14410–14430, 2018年。'
- en: '[25] X. Yuan, P. He, Q. Zhu and X. Li, “Adversarial examples: Attacks and defenses
    for deep learning,” IEEE Trans. Neural Networks Learn. Syst., vol. 30, no. 9,
    pp. 2805–2824, 2019.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] X. Yuan, P. He, Q. Zhu 和 X. Li，“对抗样本：深度学习的攻击与防御，”IEEE Trans. Neural Networks
    Learn. Syst., vol. 30, no. 9, pp. 2805–2824, 2019年。'
- en: '[26] A. Agarwal, S. Gupta, and D. K. Singh, “Review of optical flow technique
    for moving object detection,” in Proc. IC3I, Noida, India, Dec. 2016, pp. 409–413.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Agarwal, S. Gupta, 和 D. K. Singh，“移动目标检测的光流技术综述，”在Proc. IC3I, 诺伊达,
    印度, 2016年12月，pp. 409–413。'
- en: '[27] S. Wang, R. Clark, H. Wen, and N. Trigoni, “DeepVO : Towards end-to-end
    visual odometry with deep recurrent convolutional neural networks,” CoRR, vol. abs/1709.08429,
    2017.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Wang, R. Clark, H. Wen, 和 N. Trigoni，“DeepVO：利用深度递归卷积神经网络实现端到端视觉里程计，”CoRR,
    vol. abs/1709.08429, 2017年。'
- en: '[28] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and A. J. Davison,
    “CodeSLAM-Learning a compact, optimisable representation for dense visual SLAM,”
    in Proc. CVPR, Salt Lake City, UT, USA, Jun. 2018, pp. 2560–2568.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, 和 A. J. Davison，“CodeSLAM—学习一种紧凑且可优化的密集视觉SLAM表示，”在Proc.
    CVPR, 盐湖城, UT, USA, 2018年6月，pp. 2560–2568。'
- en: '[29] M. Lu, W. Chen, X. Shen, H.-C. Lam, and J. Liu, “Positioning and tracking
    construction vehicles in highly dense urban areas and building construction sites,”
    Automat. Constr., vol. 16, no. 5, pp. 647–656, Aug. 2007.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] M. Lu, W. Chen, X. Shen, H.-C. Lam, 和 J. Liu，“在高度密集的城市区域和建筑工地中定位和跟踪建筑车辆，”Automat.
    Constr., vol. 16, no. 5, pp. 647–656, 2007年8月。'
- en: '[30] F. Ghallabi, F. Nashashibi, G. El-Haj-Shhade, and M. Mittet, “Lidar-based
    lane marking detection for vehicle positioning in an HD map,” in Proc. ITSC. Maui,
    HI, USA, Nov. 2018, pp. 2209–2214.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] F. Ghallabi, F. Nashashibi, G. El-Haj-Shhade, 和 M. Mittet，“基于激光雷达的车道标记检测用于HD地图中的车辆定位，”在Proc.
    ITSC, 茂宜岛, HI, USA, 2018年11月，pp. 2209–2214。'
- en: '[31] R. B. Girshick, “Fast R-CNN,” in Proc. ICCV, Santiago, Chile, Dec. 2015,
    pp. 1440–1448.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] R. B. Girshick, “快速 R-CNN，”发表于Proc. ICCV, 圣地亚哥, 智利, 2015年12月, 页码1440–1448。'
- en: '[32] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi, “You only look
    once: Unified, real-time object detection,” in Proc. CVPR, Las Vegas, NV, USA,
    Jun. 2016, pp. 779–788.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Redmon, S. K. Divvala, R. B. Girshick, 和 A. Farhadi, “你只需看一次：统一的实时目标检测，”发表于Proc.
    CVPR, 拉斯维加斯, NV, USA, 2016年6月, 页码779–788。'
- en: '[33] Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud based
    3D object detection,” in Proc. CVPR, Salt Lake City, UT, USA, Jun. 2018, pp. 4490–4499.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Y. Zhou 和 O. Tuzel, “Voxelnet：用于点云基础的3D目标检测的端到端学习，”发表于Proc. CVPR, 盐湖城,
    UT, USA, 2018年6月, 页码4490–4499。'
- en: '[34] S. Shi, X. Wang, and H. Li, “PointRCNN: 3D object proposal generation
    and detection from point cloud,” in Proc. CVPR, Long Beach, CA, USA, Jun. 2019,
    pp. 770–779.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Shi, X. Wang, 和 H. Li, “PointRCNN：点云中的3D目标提议生成与检测，”发表于Proc. CVPR, 长滩,
    CA, USA, 2019年6月, 页码770–779。'
- en: '[35] J.Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in Proc. CVPR, Boston, MA, USA, Jun. 2015, pp. 3431–3440.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J.Long, E. Shelhamer, 和 T. Darrell, “全卷积网络用于语义分割，”发表于Proc. CVPR, 波士顿,
    MA, USA, 2015年6月, 页码3431–3440。'
- en: '[36] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
    in Proc. CVPR, Honolulu, HI, USA, Jul. 2017, pp. 6230–6239.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] H. Zhao, J. Shi, X. Qi, X. Wang, 和 J. Jia, “金字塔场景解析网络，”发表于Proc. CVPR,
    檀香山, HI, USA, 2017年7月, 页码6230–6239。'
- en: '[37] T. Y. Gu, J. M. Dolan, and J. Lee, “Human-like planning of swerve maneuvers
    for autonomous vehicles,” in Proc. IV, Gotenburg, Sweden, Jun. 2016, pp. 716–721.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] T. Y. Gu, J. M. Dolan, 和 J. Lee, “类人计划的自动驾驶车辆转向操作，”发表于Proc. IV, 哥德堡, 瑞典,
    2016年6月, 页码716–721。'
- en: '[38] A. Gupta, J. Johnson, F. F. Li, S. Savarese, and A. Alahi, “Social GAN:
    Socially acceptable trajectories with generative adversarial networks,” in Proc.
    CVPR, Salt Lake City, UT, USA, Jun. 2018, pp. 2255–2264.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. Gupta, J. Johnson, F. F. Li, S. Savarese, 和 A. Alahi, “社会化 GAN：生成对抗网络中的社会可接受轨迹，”发表于Proc.
    CVPR, 盐湖城, UT, USA, 2018年6月, 页码2255–2264。'
- en: '[39] W. Luo, B. Yang, and R. Urtasun, “Fast and furious: Real time end-to-end
    3D detection, tracking and motion forecasting with a single convolutional net,”
    in Proc. CVPR, Salt Lake City, UT, USA, , Jun. 2018, pp. 3569–3577.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] W. Luo, B. Yang, 和 R. Urtasun, “快速与激烈：实时端到端3D检测、跟踪和运动预测通过单一卷积网络，”发表于Proc.
    CVPR, 盐湖城, UT, USA, 2018年6月, 页码3569–3577。'
- en: '[40] M. Wulfmeier, D. Z. Wang, and I. Posner, “Watch this: Scalable cost-function
    learning for path planning in urban environments,” in Proc. IROS, Daejeon, South
    Korea, Oct. 2016, pp. 2089–2095.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] M. Wulfmeier, D. Z. Wang, 和 I. Posner, “观看这个：城市环境中路径规划的可扩展成本函数学习，”发表于Proc.
    IROS, 大田, 韩国, 2016年10月, 页码2089–2095。'
- en: '[41] P. Wolf, C. Hubschneider, M. Weber, A. Bauer, J. Härtl, F. Durr, and J. M.
    Zöllner, “Learning how to drive in a real world simulation with deep Q-Networks,”
    in Proc. IV, Los Angeles, CA, USA, Jun. 2017, pp. 244–250.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] P. Wolf, C. Hubschneider, M. Weber, A. Bauer, J. Härtl, F. Durr, 和 J.
    M. Zöllner, “在真实世界仿真中利用深度 Q 网络学习驾驶，”发表于Proc. IV, 洛杉矶, CA, USA, 2017年6月, 页码244–250。'
- en: '[42] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. K. Zhang, X. Zhang, J. Zhang, and K. Zieba,
    “End to end learning for self-driving cars,” CoRR, vol. abs/1604.07316, Apr. 2016.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. K. Zhang, X. Zhang, J. Zhang, 和 K. Zieba,
    “端到端自驾车学习，”CoRR, vol. abs/1604.07316, 2016年4月。'
- en: '[43] A. Hussein, M.M. Gaber, E. Elyan, and C. Jayne, “Imitation learning: A
    survey of learning methods,” ACM Computing Surveys, vol. 50, no. 2, pp. 1–35,
    Jun. 2017.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] A. Hussein, M.M. Gaber, E. Elyan, 和 C. Jayne, “模仿学习：学习方法的综述，”ACM Computing
    Surveys, vol. 50, no. 2, 页码1–35, 2017年6月。'
- en: '[44] F. Codevilla, M. Miiller, A. López, V. Koltun, and A. Dosovitskiy, “End-to-end
    driving via conditional imitation learning,” in Proc. ICRA, Brisbane, Australia,
    May. 2018, pp. 1–9.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] F. Codevilla, M. Miuller, A. López, V. Koltun, 和 A. Dosovitskiy, “通过条件模仿学习的端到端驾驶，”发表于Proc.
    ICRA, 布里斯班, 澳大利亚, 2018年5月, 页码1–9。'
- en: '[45] H. Xu, Y. Gao, F. Yu, and T. Darrell, “End-to-end learning of driving
    models from large-scale video datasets,” in Proc. CVPR, Honolulu, HI, USA, Jul.
    2017, pp. 2174–2182.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] H. Xu, Y. Gao, F. Yu, 和 T. Darrell, “从大规模视频数据集中学习驾驶模型的端到端方法，”发表于Proc.
    CVPR, 檀香山, HI, USA, 2017年7月, 页码2174–2182。'
- en: '[46] M. Sundermeyer, R. Schlüter, and H. Ney, “LSTM neural networks for language
    modeling,” in Proc. ISCA, Portland, OR, USA, Sept. 2012, pp. 194–197.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] M. Sundermeyer, R. Schlüter, 和 H. Ney, “LSTM神经网络用于语言建模，”发表于Proc. ISCA,
    波特兰, OR, USA, 2012年9月, 页码194–197。'
- en: '[47] J. Petit, B. Stottelaar, M. Feiri, and F. Kargl, “Remote attacks on automated
    vehicles sensors: Experiments on camera and lidar,” Black Hat Europe, Amsterdam,
    Netherlands, Nov. 2015.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. Petit, B. Stottelaar, M. Feiri 和 F. Kargl，"对自动化车辆传感器的远程攻击：相机和激光雷达实验"，Black
    Hat Europe，荷兰阿姆斯特丹，2015 年 11 月。'
- en: '[48] Y. Park, S. Yunmok, S. Hocheol, and D. Kim and Y. Kim, “This ain’t your
    dose: Sensor spoofing attack on medical infusion pump,” in Proc. WOOT, Austin,
    TX, USA, Aug. 2016.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. Park, S. Yunmok, S. Hocheol, D. Kim 和 Y. Kim，"这不是你的剂量：对医疗注射泵的传感器欺骗攻击"，在
    WOOT 会议中，奥斯汀，美国，2016 年 8 月。'
- en: '[49] C. Yan, W. Xu, and J. Liu, “Can you trust autonomous vehicles: Contactless
    attacks against sensors of self-driving vehicle,” DEF CON, Paris, France, Aug.
    2016.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] C. Yan, W. Xu 和 J. Liu，"你能信任自动驾驶车辆吗：针对自驾车传感器的无接触攻击"，DEF CON，法国巴黎，2016
    年 8 月。'
- en: '[50] H. Shin, D. Kim, Y. Kwon, and Y. Kim, “Illusion and dazzle: Adversarial
    optical channel exploits against lidars for automotive applications,” in Proc.
    CHES, Taipei, Taiwan, Sept. 2017, pp. 445–467.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] H. Shin, D. Kim, Y. Kwon 和 Y. Kim，"幻觉与炫目：针对汽车应用激光雷达的对抗光学通道利用"，在 CHES 会议中，台北，台湾，2017
    年 9 月，页码 445–467。'
- en: '[51] B. S. Lim, S. L. Keoh, and V. L. L. Thing, “Autonomous vehicle ultrasonic
    sensor vulnerability and impact assessment,” in Proc. IoTWF, Singapore, Feb. 2018,
    pp. 231–236.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] B. S. Lim, S. L. Keoh 和 V. L. L. Thing，"自主车辆超声波传感器的脆弱性及影响评估"，在 IoTWF 会议中，新加坡，2018
    年 2 月，页码 231–236。'
- en: '[52] Y. Son, H. Shin, D. Kim, Y. Park, J. Noh, K. Choi, J. Choi, and Y. Kim,
    “Rocking drones with intentional sound noise on gyroscopic sensors,” in Proc.
    USENIX, Washington, D.C., USA, Aug. 2015, pp. 881–896.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Son, H. Shin, D. Kim, Y. Park, J. Noh, K. Choi, J. Choi 和 Y. Kim，"通过意图性声音噪声摇晃无人机的陀螺仪传感器"，在
    USENIX 会议中，华盛顿特区，美国，2015 年 8 月，页码 881–896。'
- en: '[53] G. Kar, H. A. Mustafa, Y. Wang, Y. Chen, W. Xu, M. Gruteser, and T. Vu,
    “Detection of on-road vehicles emanating GPS interference,” in Proc. SIGSAC, Scottsdale,
    AZ, USA, Nov. 2014, pp. 621–632.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] G. Kar, H. A. Mustafa, Y. Wang, Y. Chen, W. Xu, M. Gruteser 和 T. Vu，"检测发出
    GPS 干扰的道路车辆"，在 SIGSAC 会议中，斯科茨代尔，美国，2014 年 11 月，页码 621–632。'
- en: '[54] M. Psiaki and T. Humphreys, “Protecting GPS from spoofers is critical
    to the future of navigation,” IEEE Spectrum, vol. 10, Jul. 2016.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] M. Psiaki 和 T. Humphreys，"保护 GPS 免受欺骗对未来导航至关重要"，IEEE 频谱，第 10 卷，2016 年
    7 月。'
- en: '[55] Q. Meng, L. T. Hsu, B. Xu, X. Luo, and A. El-Mowafy, “A GPS Spoofing generator
    using an open sourced vector tracking-based receiver,” Sensors, vol. 19, no. 18,
    p. 3993, May. 2019.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Q. Meng, L. T. Hsu, B. Xu, X. Luo 和 A. El-Mowafy，"一种基于开源矢量追踪接收器的 GPS 欺骗生成器"，传感器，第
    19 卷，第 18 期，页码 3993，2019 年 5 月。'
- en: '[56] J. S. Warner and G. Roger, “A Simple Demonstration that the Global Positioning
    System ( GPS ) is Vulnerable to Spoofing,” Journal of security administration,
    vol. 25, no. 22, pp. 19–27, 2002.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] J. S. Warner 和 G. Roger，"简单演示全球定位系统 (GPS) 对欺骗攻击的脆弱性"，安全管理期刊，第 25 卷，第 22
    期，页码 19–27，2002 年。'
- en: '[57] K. Zeng, S. Liu, Y. Shu, D. Wang, H. Li, Y. Dou, G. Wang, and Y. Yang,
    “All your GPS are belong to us: Towards stealthy manipulation of road navigation
    systems,” in Proc. USENIX, Baltimore, MD, USA, Aug. 2018, pp. 1527–1544.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] K. Zeng, S. Liu, Y. Shu, D. Wang, H. Li, Y. Dou, G. Wang 和 Y. Yang，"你所有的
    GPS 都归我们：朝向隐蔽操控道路导航系统"，在 USENIX 会议中，巴尔的摩，美国，2018 年 8 月，页码 1527–1544。'
- en: '[58] D. Davidson, H. Wu, R. Jellinek, V. Singh, and T. Ristenpart, “Controlling
    UAVs with sensor input spoofing attacks,” in Proc. UNISEX Workshop, Austin, TX,
    USA, Aug. 2016.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] D. Davidson, H. Wu, R. Jellinek, V. Singh 和 T. Ristenpart，"通过传感器输入欺骗攻击控制无人机"，在
    UNISEX 研讨会中，奥斯汀，美国，2016 年 8 月。'
- en: '[59] D. Nassi, R. B. Netanel, Y .Elovici, B. Nassi, “MobilBye: Attacking ADAS
    with camera spoofing,” CoRR, vol. abs/1906.09765, 2019.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] D. Nassi, R. B. Netanel, Y. Elovici 和 B. Nassi，"MobilBye：通过相机欺骗攻击ADAS"，CoRR，卷
    abs/1906.09765，2019 年。'
- en: '[60] M. B. Sinai, N. Partush, S. Yadid, and E. Yahav, “Exploiting social navigation,”
    CoRR, vol. abs/1410.0151, Oct. 2014.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] M. B. Sinai, N. Partush, S. Yadid 和 E. Yahav，"利用社交导航"，CoRR，卷 abs/1410.0151，2014
    年 10 月。'
- en: '[61] M. Long, C. Wu, and J. Y. Hung, “Denial of service attacks on network-based
    control systems: impact and mitigation,” IEEE Trans. Industrial Informatics, vol. 1,
    no. 2, pp. 85–96, May 2005.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] M. Long, C. Wu 和 J. Y. Hung，"对网络控制系统的拒绝服务攻击：影响与缓解"，IEEE 工业信息学汇刊，第 1 卷，第
    2 期，页码 85–96，2005 年 5 月。'
- en: '[62] M. Du and K. Wang, “An sdn-enabled pseudo-honeypot strategy for distributed
    denial of service attacks in industrial internet of things,” IEEE Trans. Industrial
    Informatics, vol. 16, no. 1, pp. 648–657, Jan. 2020.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] M. Du 和 K. Wang，"一种用于工业物联网中分布式拒绝服务攻击的 SDN 启用伪蜜罐策略"，IEEE 工业信息学汇刊，第 16 卷，第
    1 期，页码 648–657，2020 年 1 月。'
- en: '[63] L. B. Othmane, H. Weffers, M. Mohamad, and M. Wolf, “A survey of security
    and privacy in connected vehicles,” in Wireless Sensor and Mobile Ad-hoc Networks,
    pp. 217–247, 2015.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] L. B. Othmane, H. Weffers, M. Mohamad, 和 M. Wolf, “连接车辆的安全性与隐私调查，” 在无线传感器和移动自组网，
    第217–247页，2015年。'
- en: '[64] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transferable adversarial
    examples and black-box attacks,” in Proc. ICLR, Toulon, France, Apr. 2017'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Y. Liu, X. Chen, C. Liu, 和 D. Song, “深入研究可转移的对抗样本和黑盒攻击，” 在ICLR会议论文集，法国土伦，2017年4月。'
- en: '[65] P. Chen, H. Zhang, Y. Sharma, J. Yi, and C. Hsieh, “Zoo: Zeroth order
    optimization based black-box attacks to deep neural networks without training
    substitute models,” in Proc. AISec, New York, NY, USA, Aug. 2017, pp. 15–26.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] P. Chen, H. Zhang, Y. Sharma, J. Yi, 和 C. Hsieh, “Zoo：基于零阶优化的黑盒攻击，针对无需训练替代模型的深度神经网络，”
    在AISec会议论文集，美国纽约，2017年8月，第15–26页。'
- en: '[66] W. Brendel, J. Rauber, and M. Bethge, “Decision-based adversarial attacks:
    Reliable attacks against black-box machine learning models,” in Proc. ICLR, Vancouver,
    BC, Canada, Feb. 2018.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] W. Brendel, J. Rauber, 和 M. Bethge, “基于决策的对抗攻击：针对黑盒机器学习模型的可靠攻击，” 在ICLR会议论文集，加拿大温哥华，2018年2月。'
- en: '[67] H. Zhou, W. Li, Y. Zhu, Y. Zhang, B. Yu, L. Zhang, and C. Liu, “Deepbillboard:
    Systematic physical-world testing of autonomous driving systems,” in Proc. ICSE,
    Seoul, South Korea, Jun. 2020.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] H. Zhou, W. Li, Y. Zhu, Y. Zhang, B. Yu, L. Zhang, 和 C. Liu, “Deepbillboard：系统化的自动驾驶系统物理世界测试，”
    在ICSE会议论文集，韩国首尔，2020年6月。'
- en: '[68] A. Boloor, K. Garimella, X. He, C. Gill, Y. Vorobeychik, and X. Zhang,
    “Attacking vision-based perception in end-to-end autonomous driving models,” Journal
    of Systems Architecture, 101766.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] A. Boloor, K. Garimella, X. He, C. Gill, Y. Vorobeychik, 和 X. Zhang, “攻击端到端自动驾驶模型中的基于视觉的感知，”
    系统架构杂志，101766。'
- en: '[69] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA:
    An open urban driving simulator,” CoRR, vol. abs/1711.03938, 2017.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, 和 V. Koltun, “CARLA：一个开放的城市驾驶模拟器，”
    CoRR，第abs/1711.03938卷，2017年。'
- en: '[70] J. Yang, A. Boloor, A. Chakrabarti, X. Zhang, and Y. Vorobeychik, “Finding
    physical adversarial examples for autonomous driving with fast and differentiable
    image compositing,” CoRR, vol. abs/2010.08844, 2020.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. Yang, A. Boloor, A. Chakrabarti, X. Zhang, 和 Y. Vorobeychik, “通过快速且可微分的图像合成寻找自动驾驶的物理对抗样本，”
    CoRR，第abs/2010.08844卷，2020年。'
- en: '[71] T. Wu, X. Ning, W. Li, R. Huang, H. Yang, and Y. Wang, “Physical adversarial
    attack on vehicle detector in the carla simulator,” CoRR, vol. abs/2007.16118,
    2020.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] T. Wu, X. Ning, W. Li, R. Huang, H. Yang, 和 Y. Wang, “在CARLA模拟器中对车辆检测器进行物理对抗攻击，”
    CoRR，第abs/2007.16118卷，2020年。'
- en: '[72] Y. Cao, C. Xiao, B. Cyr, Y. M. Zhou, W. Park, S. Rampazzi, Q. A. Chen,
    K. Fu, and Z. M. Mao, “Adversarial sensor attack on lidar-based perception in
    autonomous driving,” in Proc. CCS, London, UK, Nov. 2019, pp. 2267–2281.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Y. Cao, C. Xiao, B. Cyr, Y. M. Zhou, W. Park, S. Rampazzi, Q. A. Chen,
    K. Fu, 和 Z. M. Mao, “激光雷达感知中的对抗传感器攻击，” 在CCS会议论文集，英国伦敦，2019年11月，第2267–2281页。'
- en: '[73] J. Sun, Y. Cao, Q.A. Chen, and Z.M. Mao, “Towards robust lidar-based perception
    in autonomous driving: General black-box adversarial sensor attack and countermeasures,”
    in Proc. USENIX Security Symposium, Aug. 2018, pp. 877–894.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] J. Sun, Y. Cao, Q.A. Chen, 和 Z.M. Mao, “迈向自动驾驶中的鲁棒激光雷达感知：通用黑盒对抗传感器攻击及其对策，”
    在USENIX安全研讨会论文集，2018年8月，第877–894页。'
- en: '[74] S. T. Chen, C. Cornelius, J. Martin, and D. H. Chau, “Shapeshifter: Robust
    physical adversarial attack on faster R-CNN object detector,” in Proc. ECML PKDD,
    Dublin, Ireland, Sept. 2018, pp. 3354–3361.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] S. T. Chen, C. Cornelius, J. Martin, 和 D. H. Chau, “Shapeshifter：针对Faster
    R-CNN目标检测器的鲁棒物理对抗攻击，” 在ECML PKDD会议论文集，爱尔兰都柏林，2018年9月，第3354–3361页。'
- en: '[75] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. W. Xiao, A. Prakash,
    T. Kohno, and D. Song, “Robust physical-world attacks on deep learning visual
    classification,” in Proc. CVPR, Salt Lake City, UT, USA, Jun. 2018, pp. 1625–1634.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. W. Xiao, A.
    Prakash, T. Kohno, 和 D. Song, “对深度学习视觉分类的鲁棒物理世界攻击，” 在CVPR会议论文集，美国盐湖城，2018年6月，第1625–1634页。'
- en: '[76] A. Møgelmose, M. M. Trivedi, and T. B. Moeslund, “Vision-based traffic
    sign detection and analysis for intelligent driver assistance systems: Perspectives
    and survey,” IEEE Trans. Intelligent Transportation Systems, vol. 13, no. 4, pp. 1484–1497,
    Dec. 2012.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] A. Møgelmose, M. M. Trivedi, 和 T. B. Moeslund, “基于视觉的交通标志检测与分析用于智能驾驶辅助系统：展望与调查，”
    IEEE 智能交通系统汇刊，第13卷，第4期，第1484–1497页，2012年12月。'
- en: '[77] Y. Zhao, H. Zhu, R. Liang, Q. Shen, S. Zhang, and K. Chen, “Seeing isn’t
    Believing: Towards More Robust Adversarial Attack Against Real World Object Detectors,”
    in Proc. SIGSAC, London, UK, Nov. 2019, pp. 1989-2004.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Y. Zhao, H. Zhu, R. Liang, Q. Shen, S. Zhang, 和 K. Chen, “眼见未必为实: 针对现实世界物体检测器的更鲁棒对抗攻击,”
    见于 Proc. SIGSAC, 英国伦敦, 2019年11月, 第1989-2004页。'
- en: '[78] N. Papernot, P. D. McDaniel, I. J. Goodfellow, S. Jha, Z. B. Celik, and
    A. Swami, “Practical black-box attacks against machine learning,” in Proc. AsiaCCS,
    Abu Dhabi, United Arab Emirates, Apr. 2017, pp. 506–519.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] N. Papernot, P. D. McDaniel, I. J. Goodfellow, S. Jha, Z. B. Celik, 和
    A. Swami, “针对机器学习的实际黑箱攻击,” 见于 Proc. AsiaCCS, 阿联酋阿布扎比, 2017年4月, 第506–519页。'
- en: '[79] Z. Kong, J. Guo, A. Li, and C. Liu, “PhysGAN: Generating Physical-World-Resilient
    Adversarial Examples for Autonomous Driving,” in Proc. CVPR, Seattle, WA, USA,
    Jun. 2020, pp. 14254–14263.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Z. Kong, J. Guo, A. Li, 和 C. Liu, “PhysGAN: 为自动驾驶生成物理世界抗干扰的对抗样本,” 见于 Proc.
    CVPR, 美国华盛顿州西雅图, 2020年6月, 第14254–14263页。'
- en: '[80] M. Wicker and M. Kwiatkowska, “Robustness of 3D deep learning in an adversarial
    setting,” in Proc. CVPR, Long Beach, CA, USA, Jun. 2019, pp. 11767–11775.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] M. Wicker 和 M. Kwiatkowska, “在对抗设置下3D深度学习的鲁棒性,” 见于 Proc. CVPR, 美国加州长滩,
    2019年6月, 第11767–11775页。'
- en: '[81] C. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep learning on point
    sets for 3D classification and segmentation,” in Proc. CVPR, Honolulu, HI, USA,
    Jul. 2017, pp. 77–85.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] C. Qi, H. Su, K. Mo, 和 L. J. Guibas, “PointNet: 用于3D分类和分割的点集深度学习,” 见于
    Proc. CVPR, 美国夏威夷檀香山, 2017年7月, 第77–85页。'
- en: '[82] D. Maturana and S. Scherer, “Voxnet: A 3D convolutional neural network
    for real-time object recognition,” in Proc. IROS, Hamburg, Germany, Sept. 2015,
    pp. 922–928.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] D. Maturana 和 S. Scherer, “Voxnet: 一种用于实时物体识别的3D卷积神经网络,” 见于 Proc. IROS,
    德国汉堡, 2015年9月, 第922–928页。'
- en: '[83] C. Xiang, C. R. Qi, and B. Li, “Generating 3D adversarial point clouds,”
    in Proc. CVPR, Long Beach, CA, USA, Jun. 2019, pp. 9136–9144.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] C. Xiang, C. R. Qi, 和 B. Li, “生成3D对抗点云,” 见于 Proc. CVPR, 美国加州长滩, 2019年6月,
    第9136–9144页。'
- en: '[84] S. H. Huang, N. Papernot, I. J. Goodfellow, Y. Duan, and P. Abbeel, “Adversarial
    attacks on neural network policies,” in Proc. ICLR, Toulon, France, Apr. 2017'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] S. H. Huang, N. Papernot, I. J. Goodfellow, Y. Duan, 和 P. Abbeel, “对神经网络策略的对抗攻击,”
    见于 Proc. ICLR, 法国图盎, 2017年4月。'
- en: '[85] V. Mnih, A. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    Proc. ICML, New York City, NY, USA, Jun. 2016, pp. 1928–1937.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] V. Mnih, A. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D.
    Silver, 和 K. Kavukcuoglu, “深度强化学习的异步方法,” 见于 Proc. ICML, 美国纽约市, 2016年6月, 第1928–1937页。'
- en: '[86] J. Kos and D. Song, “Delving into adversarial attacks on deep policies,”
    in Proc. ICLR, Toulon, France, Nov. 2017.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. Kos 和 D. Song, “深入探讨对深度策略的对抗攻击,” 见于 Proc. ICLR, 法国图盎, 2017年11月。'
- en: '[87] Y. Lin, Z. Hong, Y. Liao, M. Shih, M. Liu, and M. Sun, “Tactics of adversarial
    attack on deep reinforcement learning agents,” in Proc. ICLR, Toulon, France,
    Nov. 2017.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Y. Lin, Z. Hong, Y. Liao, M. Shih, M. Liu, 和 M. Sun, “对深度强化学习代理的对抗攻击策略,”
    见于 Proc. ICLR, 法国图盎, 2017年11月。'
- en: '[88] Y. Liu, S. Ma, Y. Aafer, W. Lee, J.Zhai, W. Wang, and X. Zhang, “Trojaning
    attack on neural networks,” in Proc. NDSS, San Diego, California, USA, Feb. 2018.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Y. Liu, S. Ma, Y. Aafer, W. Lee, J.Zhai, W. Wang, 和 X. Zhang, “对神经网络的特洛伊木马攻击,”
    见于 Proc. NDSS, 美国加利福尼亚州圣地亚哥, 2018年2月。'
- en: '[89] H. Rehman, A. Ekelhart, and R. Mayer, “Backdoor attacks in neural networks
    - A systematic evaluation on multiple traffic sign datasets,” in Proc. CD-MAKE,
    Canterbury, UK, Aug. 2019, pp. 285–300.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] H. Rehman, A. Ekelhart, 和 R. Mayer, “神经网络中的后门攻击 - 对多个交通标志数据集的系统评估,” 见于
    Proc. CD-MAKE, 英国坎特伯雷, 2019年8月, 第285–300页。'
- en: '[90] S. Ding, Y. Tian, F Xu, Q Li, and S. Zhong, “Poisoning Attack on Deep
    Generative Models in Autonomous Driving“ in Proc. of EAI SecureComm, Oct. 2019.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] S. Ding, Y. Tian, F Xu, Q Li, 和 S. Zhong, “对深度生成模型的投毒攻击,” 见于 EAI SecureComm,
    2019年10月。'
- en: '[91] F. Zhang, H. Kodituwakku, J. W. Hines, and J. Coble, “Multilayer data-driven
    cyber-attack detection system for industrial control systems based on network,
    system, and process data,” IEEE Trans. Industrial Informatics, vol. 15, no. 7,
    pp. 4362–4369, Jul. 2019.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] F. Zhang, H. Kodituwakku, J. W. Hines, 和 J. Coble, “基于网络、系统和过程数据的工业控制系统多层数据驱动网络攻击检测系统,”
    IEEE Trans. Industrial Informatics, 第15卷，第7期，第4362–4369页, 2019年7月。'
- en: '[92] Q. Sun, K. Zhang, and Y. Shi, “Resilient model predictive control of cyber-physical
    systems under dos attacks,” IEEE Trans. Industrial Informatics, vol. 16, no. 7,
    pp. 4920–4927, Jul. 2020.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Q. Sun, K. Zhang, 和 Y. Shi, “在DOS攻击下的网络物理系统弹性模型预测控制,” IEEE Trans. Industrial
    Informatics, 第16卷，第7期，第4920–4927页, 2020年7月。'
- en: '[93] Y. Shoukry, P. Martin, Y. Yona, S. N. Diggavi, and M. B. Srivastava, “Pycra:
    Physical challenge-response authentication for active sensors under spoofing attacks,”
    in Proc. SIGSAC, Denver, CO, USA, Oct. 2015, pp. 1004–1015.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Y. Shoukry, P. Martin, Y. Yona, S. N. Diggavi, 和 M. B. Srivastava，“Pycra：针对主动传感器的物理挑战响应认证，防御欺骗攻击”，发表于
    SIGSAC 会议，科罗拉多州丹佛，美国，2015年10月，页码1004–1015。'
- en: '[94] S. Mahmud, S. Shanker, and I. Hossain, “Secure software upload in an intelligent
    vehicle via wireless communication links,” in Proc. IV, Las Vegas, NV, USA, 2005,
    pp. 588–593.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] S. Mahmud, S. Shanker, 和 I. Hossain，“通过无线通信链路在智能车辆中安全地上传软件”，发表于 IV 会议，拉斯维加斯，内华达，美国，2005年，页码588–593。'
- en: '[95] D. Nilsson and U. E. Larson, “Secure firmware updates over the air in
    intelligent vehicles,” in Proc. ICC Workshops, Beijing, China, 2008, pp. 380–384.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] D. Nilsson 和 U. E. Larson， “智能车辆的安全固件更新”，发表于 ICC Workshops 会议，北京，中国，2008年，页码380–384。'
- en: '[96] N. Papernot, P. D. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
    as a defense to adversarial perturbations against deep neural networks,” in Proc.
    SP, San Jose, CA, USA, May 2016, pp. 582–597.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] N. Papernot, P. D. McDaniel, X. Wu, S. Jha, 和 A. Swami，“通过蒸馏对抗深度神经网络的对抗性扰动”，发表于
    SP 会议，加州圣荷西，美国，2016年5月，页码582–597。'
- en: '[97] A. Kurakin, I. Goodfellow, S. Bengio, Y. Dong, F. Liao, M. Liang, and
    J. Wang, “Adversarial attacks and defences competition,” CoRR, vol. abs/1804.00097,
    2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] A. Kurakin, I. Goodfellow, S. Bengio, Y. Dong, F. Liao, M. Liang, 和 J.
    Wang，“对抗攻击和防御竞赛”，CoRR，第abs/1804.00097卷，2018年。'
- en: '[98] X. Liu, M. Cheng, H. Zhang, and C. J. Hsieh, “Towards robust neural networks
    via random self-ensemble,” in Proc. ECCV, Munich, Germany, Nov. 2018, pp. 369–385.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] X. Liu, M. Cheng, H. Zhang, 和 C. J. Hsieh，“通过随机自集成实现鲁棒神经网络”，发表于 ECCV 会议，德国慕尼黑，2018年11月，页码369–385。'
- en: '[99] T. Pang, K. Xu, C. Du, N. Chen, and J.. Zhu, “Improving adversarial robustness
    via promoting ensemble diversity,” in Proc. ICML, Long Beach, CA, USA, May. 2019,
    pp. 4970–4979.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] T. Pang, K. Xu, C. Du, N. Chen, 和 J. Zhu，“通过促进集成多样性来提高对抗鲁棒性”，发表于 ICML
    会议，加州长滩，美国，2019年5月，页码4970–4979。'
- en: '[100] Z. Yan, Y. Guo, and C. Zhang, “Deep Defense: Training DNNs with improved
    adversarial robustness,” in Proc. NeurIPS, Montréal, Canada, Dec. 2018, pp. 417–426.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Z. Yan, Y. Guo, 和 C. Zhang，“深度防御：训练具有改进对抗鲁棒性的 DNN”，发表于 NeurIPS 会议，蒙特利尔，加拿大，2018年12月，页码417–426。'
- en: '[101] S. X. Gu and L. Rigazio, “Towards deep neural network architectures robust
    to adversarial examples,” in Proc. ICLR, San Diego, CA, USA, May 2015,'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S. X. Gu 和 L. Rigazio，“朝着对抗样本鲁棒的深度神经网络架构前进”，发表于 ICLR 会议，加州圣地亚哥，美国，2015年5月。'
- en: '[102] M. Cissé, P. Bojanowski, E. Grave, Y. N. Dauphin, and N. Usunier, “Parseval
    networks: Improving robustness to adversarial examples,” in Proc. ICML, Sydney,
    NSW, Australia, Aug. 2017, vol. 70, pp. 854–863.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] M. Cissé, P. Bojanowski, E. Grave, Y. N. Dauphin, 和 N. Usunier，“Parseval
    网络：提高对抗性样本的鲁棒性”，发表于 ICML 会议，悉尼，新南威尔士，澳大利亚，2017年8月，第70卷，页码854–863。'
- en: '[103] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana, “Certified
    robustness to adversarial examples with differential privacy,” in Proc. SP, San
    Francisco, CA, USA, May 2019, pp. 656–672.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, 和 S. Jana，“具有差分隐私的对抗样本的认证鲁棒性”，发表于
    SP 会议，加州旧金山，美国，2019年5月，页码656–672。'
- en: '[104] A. Raghunathan, J. Steinhardt, and P. Liang, “Certified defenses against
    adversarial examples,” CoRR, vol. abs/1801.09344, 2018.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] A. Raghunathan, J. Steinhardt, 和 P. Liang，“针对对抗样本的认证防御”，CoRR，第abs/1801.09344卷，2018年。'
- en: '[105] E. Wong, and Z. Kolter, “Provable defenses against adversarial examples
    via the convex outer adversarial polytope,” in Proc. ICML, Stockholm, Sweden,
    Jul. 2018, pp. 5286–5295.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] E. Wong 和 Z. Kolter，“通过凸外对抗多面体的可证明防御”，发表于 ICML 会议，斯德哥尔摩，瑞典，2018年7月，页码5286–5295。'
- en: '[106] Z. Zheng and P. Hong, “Robust detection of adversarial attacks by modeling
    the intrinsic properties of deep neural networks,” in Proc. NeurIPS, Montréal,
    Canada, Dec. 2018, pp. 7924–7933.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Z. Zheng 和 P. Hong，“通过建模深度神经网络的内在特性来强健地检测对抗性攻击”，发表于 NeurIPS 会议，蒙特利尔，加拿大，2018年12月，页码7924–7933。'
- en: '[107] K. Lee, K. Lee, H. Lee, and J. Shin, “A simple unified framework for
    detecting out-of-distribution samples and adversarial attacks,” in Proc. NeurIPS,
    Montréal, Canada, Dec. 2018, pp. 7167–7177.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] K. Lee, K. Lee, H. Lee, 和 J. Shin，“用于检测分布外样本和对抗攻击的简单统一框架”，发表于 NeurIPS
    会议，蒙特利尔，加拿大，2018年12月，页码7167–7177。'
- en: '[108] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adversarial
    examples in deep neural networks,” CoRR, vol. abs/1704.01155, 2017.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] W. Xu, D. Evans, 和 Y. Qi，“特征压缩：检测深度神经网络中的对抗性样本”，CoRR，第abs/1704.01155卷，2017年。'
- en: '[109] C. Guo, M. Rana, M. Cissé, and L. V. D. Maaten, “Countering adversarial
    images using input transformations,” in Proc. ICLR, Vancouver, BC, Canada, Apr.
    2018.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C. Guo, M. Rana, M. Cissé, 和 L. V. D. Maaten，“通过输入变换对抗对抗性图像”，发表于 ICLR
    会议，加拿大不列颠哥伦比亚省温哥华，2018年4月。'
- en: '[110] P. Samangouei, M. Kabkab, and R. Chellappa, “Defense-gan: Protecting
    classifiers against adversarial attacks using generative models,” in Proc. ICLR,
    Vancouver, BC, Canada, Apr. 2018.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] P. Samangouei, M. Kabkab, 和 R. Chellappa，“Defense-gan：利用生成模型保护分类器免受对抗攻击”，发表于
    ICLR 会议，加拿大不列颠哥伦比亚省温哥华，2018年4月。'
- en: '[111] G. Jin, S. Shen, D. Zhang, F.Dai, and Y. Zhang, “APE-GAN: Adversarial
    perturbation elimination with GAN,” in Proc. ICASSP, Brighton, United Kingdom,
    May. 2019, pp. 3842–3846.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] G. Jin, S. Shen, D. Zhang, F. Dai, 和 Y. Zhang，“APE-GAN：利用GAN消除对抗扰动”，发表于
    ICASSP 会议，英国布莱顿，2019年5月，第3842–3846页。'
- en: '[112] F. Liao, M. Liang, Y. Dong, T. Pang, X. Hu, and J. Zhu, “Defense against
    adversarial attacks using high-level representation guided denoiser,” in Proc.
    CVPR, Salt Lake City, UT, USA, Jun. 2018, pp. 1778–1787.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] F. Liao, M. Liang, Y. Dong, T. Pang, X. Hu, 和 J. Zhu，“利用高层表示引导去噪器防御对抗攻击”，发表于
    CVPR 会议，美国犹他州盐湖城，2018年6月，第1778–1787页。'
- en: '[113] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from
    tiny images,” Tech. Rep., Apr. 2009.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Krizhevsky 和 G. Hinton，“从微小图像中学习多层特征”，技术报告，2009年4月。'
- en: '[114] T. Hastie and R. Tibshirani, “Discriminant analysis by gaussian mixtures,”
    J R Stat Soc Series B Stat Methodol, vol. 58, no. 1, pp. 155–176, 1996.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] T. Hastie 和 R. Tibshirani，“通过高斯混合进行判别分析”，《皇家统计学会B系列统计方法论杂志》，第58卷，第1期，第155–176页，1996年。'
- en: '[115] Y. S. Gao, C. G. Xu, D. R. Wang, S. P. Chen, D. C. Ranasinghe, and S. Nepal,
    “STRIP: A defence against trojan attacks on deep neural networks,” in Proc. ACSAC,
    San Juan, PR, USA, Dec. 2019, pp. 113–125.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Y. S. Gao, C. G. Xu, D. R. Wang, S. P. Chen, D. C. Ranasinghe, 和 S. Nepal，“STRIP：对抗深度神经网络中的特洛伊攻击的防御”，发表于
    ACSAC 会议，美国波多黎各圣胡安，2019年12月，第113–125页。'
- en: '[116] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I. Molloy,
    and B. Srivastava, “Detecting backdoor attacks on deep neural networks by activation
    clustering,” in Proc. AAAI Workshop, Honolulu, HI, USA, Jan. 2019,'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I.
    Molloy, 和 B. Srivastava，“通过激活聚类检测深度神经网络中的后门攻击”，发表于 AAAI 研讨会，美国夏威夷州檀香山，2019年1月，'
- en: '[117] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Zhao,
    “Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,”
    in Proc. SP, San Francisco, CA, USA, May 2019, pp. 707–723.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, 和 B. Zhao，“Neural
    cleanse：识别和缓解神经网络中的后门攻击”，发表于 SP 会议，美国加利福尼亚州旧金山，2019年5月，第707–723页。'
- en: '[118] S. J. Oh, B. Schiele, and M. Fritz, “Towards reverse-engineering black-box
    neural networks,” in Explainable AI: Interpreting, Explaining and Visualizing
    Deep Learning, Sept. 2019, pp. 121–144.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] S. J. Oh, B. Schiele, 和 M. Fritz，“反向工程黑箱神经网络”，发表于《可解释的AI：解释、说明和可视化深度学习》，2019年9月，第121–144页。'
- en: '[119] L. Batina, S. Bhasin, D. Jap, and S. Picek, “CSI NN: Reverse engineering
    of neural network architectures through electromagnetic side channel,” in Proc.
    UNISEX, Santa Clara, CA, USA, Aug. 2019, pp. 515–532.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] L. Batina, S. Bhasin, D. Jap, 和 S. Picek，“CSI NN：通过电磁侧信道逆向工程神经网络架构”，发表于
    UNISEX 会议，美国加利福尼亚州圣克拉拉，2019年8月，第515–532页。'
- en: '[120] H. Y. Zhang, Y. D. Yu, J. T. Jiao, E. P. Xing, L. E. Ghaoui, and M. I.
    Jordan, “Theoretically principled trade-off between robustness and accuracy,”
    in Proc. ICML, Long Beach, California, USA, vol. 97, Jun. 2019, pp. 7472–7482.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] H. Y. Zhang, Y. D. Yu, J. T. Jiao, E. P. Xing, L. E. Ghaoui, 和 M. I.
    Jordan，“理论上原则性的鲁棒性和准确性之间的权衡”，发表于 ICML 会议，美国加利福尼亚州长滩，第97卷，2019年6月，第7472–7482页。'
- en: '[121] M. Lécuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana, “Certified
    robustness to adversarial examples with differential privacy,” in Proc. SP, San
    Francisco, CA, USA, May 2019, pp. 656–672.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] M. Lécuyer, V. Atlidakis, R. Geambasu, D. Hsu, 和 S. Jana，“具有差分隐私的对抗样本的认证鲁棒性”，发表于
    SP 会议，美国加利福尼亚州旧金山，2019年5月，第656–672页。'
- en: '[122] C. Murphy, G. E. Kaiser, and M. Arias, “An approach to software testing
    of machine learning applications,” in Proc. SEKE, Boston, Massachusetts, USA,
    Jul. 2007.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] C. Murphy, G. E. Kaiser, 和 M. Arias，“机器学习应用的软件测试方法”，发表于 SEKE 会议，美国马萨诸塞州波士顿，2007年7月。'
- en: '[123] X. W. Huang, M. Kwiatkowska, S. Wang, and M. Wu, “Safety verification
    of deep neural networks,” in Proc. CAV, Heidelberg, Germany, Jul. 2017, pp. 3–29.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] X. W. Huang, M. Kwiatkowska, S. Wang, 和 M. Wu，“深度神经网络的安全性验证”，发表于 CAV
    会议，德国海德堡，2017年7月，第3–29页。'
- en: '[124] G. Li, K. Ota, M. Dong, J. Wu, and J. Li, “Desvig: Decentralized swift
    vigilance against adversarial attacks in industrial artificial intelligence systems,”
    IEEE Trans. Industrial Informatics, vol. 16, no. 5, pp. 3267–3277, May 2020.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] G. Li, K. Ota, M. Dong, J. Wu, 和 J. Li, “Desvig: 去中心化的快速警觉系统对抗工业人工智能系统中的对抗攻击,”
    IEEE Trans. Industrial Informatics, 卷16, 期5, 页3267–3277, 2020年5月。'
- en: '[125] X. Zheng, C. Julien, R. M. Podorozhny, F. Cassez, and T. Rakotoarivelo,
    “Efficient and scalable runtime monitoring for cyber-physical system,” IEEE Systems
    Journal, vol. 12, no. 2, pp. 1667–1678, Jun. 2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] X. Zheng, C. Julien, R. M. Podorozhny, F. Cassez, 和 T. Rakotoarivelo,
    “高效且可扩展的网络物理系统运行时监控，” IEEE Systems Journal, 卷12, 期2, 页1667–1678, 2018年6月。'
- en: '[126] W. Lu, Y. Zhou, G. Wan, S. Hou, and S. Song, “L3-net: Towards learning
    based lidar localization for autonomous driving,” in Proc. CVPR, Long Beach, CA,
    USA, Jun. 2019, pp. 6389–-6398.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] W. Lu, Y. Zhou, G. Wan, S. Hou, 和 S. Song, “L3-net: 基于学习的激光雷达定位用于自动驾驶,”
    在 Proc. CVPR, 洛杉矶, CA, USA, 2019年6月, 页6389–6398。'
- en: '[127] W. Zhang and C. Xiao, “Pcan: 3d attention map learning using contextual
    information for point cloud based retrieval,” in Proc. CVPR, Long Beach, CA, USA,
    Jun. 2019, pp. 12436–-12445.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] W. Zhang 和 C. Xiao, “Pcan: 利用上下文信息进行点云检索的3D注意力图学习,” 在 Proc. CVPR, 洛杉矶,
    CA, USA, 2019年6月, 页12436–12445。'
- en: '[128] X. Zheng, C. Julien, R. M. Podorozhny, F. Cassez, and T. Rakotoarivelo,
    “Efficient and scalable runtime monitoring for cyber-physical system,” IEEE Systems
    Journal, vol. 12, no. 2, pp. 1667–1678, Jun. 2018.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] X. Zheng, C. Julien, R. M. Podorozhny, F. Cassez, 和 T. Rakotoarivelo,
    “高效且可扩展的网络物理系统运行时监控，” IEEE Systems Journal, 卷12, 期2, 页1667–1678, 2018年6月。'
- en: '| ![[Uncaptioned image]](img/7b3f2e0e90fc4f4db415ea15ab10c076.png) | Yao Deng
    (S’21) received the Bachelor degree of Information Technology from Deakin University,
    Australia in March 2018, the Bachelor degree of Software Engineering from Southwest
    University, China in July 2018, and the Master of Research degree in Computing
    from Macquarie University, Australia in 2020\. He is currently pursuing his PhD
    degree at Macquarie University. His current research interests include adversarial
    attacks and defenses, metamorphic testing, simulation testing, and anomaly detection
    on autonomous driving systems. |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/7b3f2e0e90fc4f4db415ea15ab10c076.png) | 姚邓 (S’21) 于2018年3月获得澳大利亚迪肯大学信息技术学士学位，2018年7月获得中国西南大学软件工程学士学位，并于2020年获得澳大利亚麦考瑞大学计算学研究硕士学位。他目前在麦考瑞大学攻读博士学位。他的研究兴趣包括对抗攻击与防御、变形测试、模拟测试和自动驾驶系统中的异常检测。
    |'
- en: '| ![[Uncaptioned image]](img/fa51cf29bed2d20dd8469e7d525e44c8.png) | Tiehua
    Zhang (S’21) received the B.S. degree from the School of Computer Science and
    Technology at Jilin University, China in 2013, the M.E. from the School of Computing
    and Information Systems at the University of Melbourne, Australia in 2015, and
    the Ph.D. degree from the School of Software and Electrical Engineering at Swinburne
    University of Technology, Australia in 2020, respectively. He then worked as a
    Postdoctoral Researcher in the Department of Computing at Macquarie University.
    From 2015 to 2017, he was a Software Engineer in Australia, focusing on industrial
    projects and solutions. He is currently an AI Specialist at Ant Group, China.
    His research interests include collaborative learning/optimization, the Internet
    of Things, fog computing, and edge intelligence. |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/fa51cf29bed2d20dd8469e7d525e44c8.png) | 张铁华 (S’21) 于2013年获得中国吉林大学计算机科学与技术学院的学士学位，2015年获得澳大利亚墨尔本大学计算与信息系统学院的硕士学位，并于2020年获得澳大利亚斯威本科技大学软件与电气工程学院的博士学位。随后，他在麦考瑞大学计算系担任博士后研究员。2015年至2017年，他在澳大利亚担任软件工程师，专注于工业项目和解决方案。他目前在中国蚂蚁集团担任AI专家。他的研究兴趣包括协作学习/优化、物联网、雾计算和边缘智能。
    |'
- en: '| ![[Uncaptioned image]](img/a1b25dbf25fa3abc32b8c2f894b13452.png) | Guannan
    Lou (S’21) received the Bachelor degree of Information Technology from Deakin
    University, Australia in March 2018, the Bachelor degree of Software Engineering
    from Southwest University, China in July 2018, and the Master of Data Science
    from Sydney University, Australia in 2020\. Currently he is pursuing the PhD degree
    at Macquaire University. His research interests include machine learning, machine
    learning security, metamorphic testing and natural language processing. |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/a1b25dbf25fa3abc32b8c2f894b13452.png) | Guannan Lou (S’21)
    于2018年3月获得澳大利亚迪肯大学的信息技术学士学位，于2018年7月获得中国西南大学的软件工程学士学位，并于2020年获得澳大利亚悉尼大学的数据科学硕士学位。目前，他正在麦考瑞大学攻读博士学位。他的研究兴趣包括机器学习、机器学习安全、变形测试和自然语言处理。
    |'
- en: '| ![[Uncaptioned image]](img/47ad9f60a4094a83896e91e4518cde87.png) | Xi Zheng
    (M’12) received the Ph.D. in Software Engineering from UT Austin in 2015\. From
    2005 to 2012, he was the Chief Solution Architect for Menulog Australia. He is
    currently the Director of Intelligent Systems Research Group (ITSEG.ORG), Senior
    Lecturer (aka Associate Professor US) and Deputy Program Leader in Software Engineering,
    Macquarie University, Sydney, Australia. His research interests include CPS verification,
    machine learning security, human vehicle interaction, edge intelligence and intelligent
    software engineering. He has a number of highly cited papers and served as the
    PC member for IEEE International Conference on Pervasive Computing and Communications
    (PerCom) (CORE A*) and International Conference on Trust, Security and Privacy
    in Computing and Communications (TrustCom) (CORE A). |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/47ad9f60a4094a83896e91e4518cde87.png) | Xi Zheng (M’12) 于2015年在德克萨斯大学奥斯汀分校获得软件工程博士学位。从2005年到2012年，他担任Menulog澳大利亚的首席解决方案架构师。他目前是智能系统研究组（ITSEG.ORG）的主任、讲师（即美国副教授）以及麦考瑞大学软件工程副项目负责人，所在地点为澳大利亚悉尼。他的研究兴趣包括CPS验证、机器学习安全、人车交互、边缘智能和智能软件工程。他有多篇被引用频次很高的论文，并曾担任IEEE国际普适计算与通信会议（PerCom）（CORE
    A*）和国际计算与通信中的信任、安全和隐私会议（TrustCom）（CORE A）的程序委员会成员。 |'
- en: '| ![[Uncaptioned image]](img/b13158f724bc96dc7d6df903a801434b.png) | Jiong
    Jin (M’11) received the B.E. degree with First Class Honours in Computer Engineering
    from Nanyang Technological University, Singapore, in 2006, and the Ph.D. degree
    in Electrical and Electronic Engineering from the University of Melbourne, Australia,
    in 2011\. From 2011 to 2013, he was a Research Fellow in the Department of Electrical
    and Electronic Engineering at the University of Melbourne. He is currently an
    Associate Professor in the School of Software and Electrical Engineering, Swinburne
    University of Technology, Melbourne, Australia. His research interests include
    network design and optimization, edge computing and distributed systems, robotics
    and automation, and cyber-physical systems and Internet of Things as well as their
    applications in smart manufacturing, smart transportation and smart cities. |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/b13158f724bc96dc7d6df903a801434b.png) | Jiong Jin (M’11)
    于2006年在新加坡南洋理工大学获得计算机工程一等荣誉学士学位，并于2011年在澳大利亚墨尔本大学获得电气与电子工程博士学位。从2011年到2013年，他在墨尔本大学电气与电子工程系担任研究员。他目前是澳大利亚斯威本科技大学软件与电气工程学院的副教授。他的研究兴趣包括网络设计与优化、边缘计算与分布式系统、机器人技术与自动化，以及网络物理系统和物联网及其在智能制造、智能交通和智能城市中的应用。'
- en: '| ![[Uncaptioned image]](img/74b7e089252e14713bb354efbaafea38.png) | Qing-Long Han
    (M’09-SM’13-F’19) received the B.Sc. degree in Mathematics from Shandong Normal
    University, Jinan, China, in 1983, and the M.Sc. and Ph.D. degrees in Control
    Engineering from East China University of Science and Technology, Shanghai, China,
    in 1992 and 1997, respectively. Professor Han is Pro Vice-Chancellor (Research
    Quality) and a Distinguished Professor at Swinburne University of Technology,
    Melbourne, Australia. He held various academic and management positions at Griffith
    University and Central Queensland University, Australia. His research interests
    include networked control systems, multi-agent systems, time-delay systems, smart
    grids, unmanned surface vehicles, and neural networks. Professor Han is a Highly
    Cited Researcher according to Clarivate Analytics. He is a Fellow of The Institution
    of Engineers Australia. He was one of Australia’s Top 5 Lifetime Achievers (Research
    Superstars) in Engineering and Computer Science (The Australian’s 2020 Research
    Magazine). He was the recipient of The 2021 M. A. Sargent Medal, which is awarded
    by Engineers Australia for longstanding eminence in science or the practice of
    electrical engineering. The prestigious M. A. Sargent Medal is the Highest Award
    of the Electrical College Board of Engineers Australia and consists of a Bronze
    Medal and a Certificate. He was the recipient of The 2020 IEEE Systems, Man, and
    Cybernetics (SMC) Society Andrew P. Sage Best Transactions Paper Award, The 2020
    IEEE Transactions on Industrial Informatics Outstanding Paper Award, and The 2019
    IEEE SMC Society Andrew P. Sage Best Transactions Paper Award. Professor Han is
    Co-Editor of Australian Journal of Electrical and Electronic Engineering, an Associate
    Editor for 12 international journals, including the IEEE TRANSACTIONS ON CYBERNETICS,
    the IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, IEEE INDUSTRIAL ELECTRONICS MAGAZINE,
    the IEEE/CAA JOURNAL OF AUTOMATICA SINICA, Control Engineering Practice, and Information
    Sciences, and a Guest Editor for 13 Special Issues. |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/74b7e089252e14713bb354efbaafea38.png) | **韩青龙**（M’09-SM’13-F’19）于1983年获得中国济南山东师范大学的数学学士学位，1992年和1997年分别获得中国上海东华大学控制工程硕士和博士学位。韩教授是澳大利亚墨尔本斯威本科技大学的研究质量副校长和杰出教授。他曾在澳大利亚格里菲斯大学和中央昆士兰大学担任过多个学术和管理职位。他的研究兴趣包括网络控制系统、多代理系统、时延系统、智能电网、无人水面车辆和神经网络。根据Clarivate
    Analytics的数据，韩教授是一位高被引研究员。他是澳大利亚工程师学会的会士。他曾被评为澳大利亚工程与计算机科学领域的前五名终身成就者（《澳大利亚人》2020年研究杂志）。他获得了2021年M.
    A. Sargent奖，该奖项由澳大利亚工程师学会颁发，以表彰在科学或电气工程实践中的长期杰出贡献。荣誉M. A. Sargent奖是澳大利亚工程师学会电气学院的最高奖项，包括一枚铜奖牌和一张证书。他还获得了2020年IEEE系统、人类和控制论（SMC）学会Andrew
    P. Sage最佳论文奖、2020年IEEE工业信息学交易杰出论文奖，以及2019年IEEE SMC学会Andrew P. Sage最佳论文奖。韩教授是《澳大利亚电气与电子工程杂志》的副编辑，还担任12个国际期刊的副编辑，包括IEEE
    TRANSACTIONS ON CYBERNETICS、IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS、IEEE INDUSTRIAL
    ELECTRONICS MAGAZINE、IEEE/CAA JOURNAL OF AUTOMATICA SINICA、控制工程实践和信息科学，并且是13个特刊的客座编辑。'
