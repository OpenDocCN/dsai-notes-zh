- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:51:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:51:58
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2108.09091] DL-Traff: Survey and Benchmark of Deep Learning Models for Urban
    Traffic Prediction'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2108.09091] DL-Traff: 城市交通预测的深度学习模型调查与基准'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2108.09091](https://ar5iv.labs.arxiv.org/html/2108.09091)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2108.09091](https://ar5iv.labs.arxiv.org/html/2108.09091)
- en: 'DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'DL-Traff: 城市交通预测的深度学习模型调查与基准'
- en: Renhe Jiang^(1,2)*, Du Yin²*, Zhaonan Wang¹, Yizhuo Wang², Jiewen Deng², Hangchen
    Liu², Zekun Cai¹, Jinliang Deng^(2,3), Xuan Song${}^{2,1}\dagger$, Ryosuke Shibasaki¹
    ¹The University of Tokyo Japan ²Southern University of Science and Technology
    China ³University of Technology Sydney Australia [jiangrh@csis.u-tokyo.ac.jp;
    yind7@outlook.com; songxuan@csis.u-tokyo.ac.jp](mailto:jiangrh@csis.u-tokyo.ac.jp;%20yind7@outlook.com;%20songxuan@csis.u-tokyo.ac.jp)(2021)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Renhe Jiang^(1,2)*, Du Yin²*, Zhaonan Wang¹, Yizhuo Wang², Jiewen Deng², Hangchen
    Liu², Zekun Cai¹, Jinliang Deng^(2,3), Xuan Song${}^{2,1}\dagger$, Ryosuke Shibasaki¹
    ¹东京大学 日本 ²南方科技大学 中国 ³悉尼科技大学 澳大利亚 [jiangrh@csis.u-tokyo.ac.jp; yind7@outlook.com;
    songxuan@csis.u-tokyo.ac.jp](mailto:jiangrh@csis.u-tokyo.ac.jp;%20yind7@outlook.com;%20songxuan@csis.u-tokyo.ac.jp)(2021)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Nowadays, with the rapid development of IoT (Internet of Things) and CPS (Cyber-Physical
    Systems) technologies, big spatiotemporal data are being generated from mobile
    phones, car navigation systems, and traffic sensors. By leveraging state-of-the-art
    deep learning technologies on such data, urban traffic prediction has drawn a
    lot of attention in AI and Intelligent Transportation System community. The problem
    can be uniformly modeled with a 3D tensor (T, N, C), where T denotes the total
    time steps, N denotes the size of the spatial domain (i.e., mesh-grids or graph-nodes),
    and C denotes the channels of information. According to the specific modeling
    strategy, the state-of-the-art deep learning models can be divided into three
    categories: grid-based, graph-based, and multivariate time-series models. In this
    study, we first synthetically review the deep traffic models as well as the widely
    used datasets, then build a standard benchmark to comprehensively evaluate their
    performances with the same settings and metrics. Our study named DL-Traff is implemented
    with two most popular deep learning frameworks, i.e., TensorFlow and PyTorch,
    which is already publicly available as two GitHub repositories [https://github.com/deepkashiwa20/DL-Traff-Grid](https://github.com/deepkashiwa20/DL-Traff-Grid)
    and [https://github.com/deepkashiwa20/DL-Traff-Graph](https://github.com/deepkashiwa20/DL-Traff-Graph).
    With DL-Traff, we hope to deliver a useful resource to researchers who are interested
    in spatiotemporal data analysis.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，随着物联网（IoT）和网络物理系统（CPS）技术的快速发展，大量的时空数据正在通过手机、车载导航系统和交通传感器生成。通过利用这些数据上的最先进深度学习技术，城市交通预测在人工智能和智能交通系统社区中引起了广泛关注。这个问题可以用一个3D张量（T,
    N, C）来统一建模，其中T表示总时间步，N表示空间域的大小（即网格或图节点），C表示信息通道。根据具体的建模策略，最先进的深度学习模型可以分为三类：基于网格的、基于图的和多变量时间序列模型。在这项研究中，我们首先综合回顾了深度交通模型及其广泛使用的数据集，然后构建了一个标准基准，以相同的设置和指标全面评估它们的性能。我们的研究DL-Traff采用了两个最流行的深度学习框架，即TensorFlow和PyTorch，已经公开为两个GitHub库：[https://github.com/deepkashiwa20/DL-Traff-Grid](https://github.com/deepkashiwa20/DL-Traff-Grid)
    和 [https://github.com/deepkashiwa20/DL-Traff-Graph](https://github.com/deepkashiwa20/DL-Traff-Graph)。通过DL-Traff，我们希望为那些对时空数据分析感兴趣的研究人员提供一个有用的资源。
- en: 'traffic prediction, multivariate time-series, deep learning, ubiquitous and
    mobile computing, survey and benchmark* Equal contribution; $\dagger$ Corresponding
    author.This work was supported by Grant-in-Aid for Early-Career Scientists (20K19859)
    of Japan Society for the Promotion of Science (JSPS).^†^†journalyear: 2021^†^†copyright:
    acmcopyright^†^†conference: Proceedings of the 30th ACM International Conference
    on Information and Knowledge Management; November 1–5, 2021; Virtual Event, QLD,
    Australia^†^†booktitle: Proceedings of the 30th ACM International Conference on
    Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event,
    QLD, Australia^†^†price: 15.00^†^†doi: 10.1145/3459637.3482000^†^†isbn: 978-1-4503-8446-9/21/11^†^†ccs:
    Information systems Information systems applications^†^†ccs: Information systems Geographic
    information systems^†^†ccs: Computing methodologies Artificial intelligence'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '交通预测，多变量时间序列，深度学习，普适计算和移动计算，调查和基准* 平等贡献；$\dagger$ 通讯作者。此项工作得到了日本学术振兴会（JSPS）早期职业科学家资助（20K19859）的支持。^†^†期刊年份：2021^†^†版权：acmcopyright^†^†会议：第30届ACM国际信息与知识管理会议论文集；2021年11月1日至5日；虚拟会议，澳大利亚昆士兰州^†^†书名：第30届ACM国际信息与知识管理会议论文集（CIKM
    ’21），2021年11月1日至5日，虚拟会议，澳大利亚昆士兰州^†^†价格：15.00^†^†doi: 10.1145/3459637.3482000^†^†isbn:
    978-1-4503-8446-9/21/11^†^†ccs: 信息系统 信息系统应用^†^†ccs: 信息系统 地理信息系统^†^†ccs: 计算方法 人工智能'
- en: '| Grid-Based | Venue | Cite | Dataset (* means Open) | Prediction Task | Metric
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 基于网格 | 会议 | 引用 | 数据集（*表示开放） | 预测任务 | 指标 |'
- en: '| ST-ResNet(Zhang et al., [2017](#bib.bib48)) | AAAI17 | 867 | TaxiBJ*, BikeNYC*
    | Taxi In-Out Flow | RMSE |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| ST-ResNet(Zhang et al., [2017](#bib.bib48)) | AAAI17 | 867 | TaxiBJ*, BikeNYC*
    | 出租车进出流 | RMSE |'
- en: '| DeepSD(Wang et al., [2017](#bib.bib36)) | ICDE17 | 125 | Didi Taxi (HangZhou)
    | Taxi Demand | MAE, RMSE |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| DeepSD(Wang et al., [2017](#bib.bib36)) | ICDE17 | 125 | 滴滴出租车（杭州） | 出租车需求
    | MAE, RMSE |'
- en: '| DMVST-Net(Yao et al., [2018](#bib.bib42)) | AAAI18 | 456 | Didi Taxi (GuangZhou)
    | Taxi Demand | RMSE, MAPE |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| DMVST-Net(Yao et al., [2018](#bib.bib42)) | AAAI18 | 456 | 滴滴出租车（广州） | 出租车需求
    | RMSE, MAPE |'
- en: '| Periodic-CRN(Zonoozi et al., [2018](#bib.bib55)) | IJCAI18 | 57 | TaxiBJ*,
    TaxiSG | Taxi Density/In-Out Flow | RMSE |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| Periodic-CRN(Zonoozi et al., [2018](#bib.bib55)) | IJCAI18 | 57 | TaxiBJ*,
    TaxiSG | 出租车密度/进出流 | RMSE |'
- en: '| Hetero-ConvLSTM(Yuan et al., [2018](#bib.bib46)) | KDD18 | 121 | Vehicle
    Crash Data* | Traffic Accident | MSE, RMSE, CE |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| Hetero-ConvLSTM(Yuan et al., [2018](#bib.bib46)) | KDD18 | 121 | 车辆事故数据*
    | 交通事故 | MSE, RMSE, CE |'
- en: '| DeepSTN+(Lin et al., [2019](#bib.bib26)) | AAAI19 | 53 | MobileBJ, BikeNYC-I*
    | Crowd/Taxi In-Out Flow | MAE, RMSE |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| DeepSTN+(Lin et al., [2019](#bib.bib26)) | AAAI19 | 53 | MobileBJ, BikeNYC-I*
    | 人群/出租车进出流 | MAE, RMSE |'
- en: '| STDN(Yao et al., [2019](#bib.bib41)) | AAAI19 | 204 | TaxiNYC*, BikeNYC-II*
    | Taxi/Bike O-D Number | RMSE, MAPE |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| STDN(Yao et al., [2019](#bib.bib41)) | AAAI19 | 204 | TaxiNYC*, BikeNYC-II*
    | 出租车/自行车O-D数量 | RMSE, MAPE |'
- en: '| MDL(Zhang et al., [2019](#bib.bib50)) | TKDE19 | 92 | TaxiBJ, BikeNYC | Taxi
    Transition/In-Out Flow | MAE, RMSE |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| MDL(Zhang et al., [2019](#bib.bib50)) | TKDE19 | 92 | TaxiBJ, BikeNYC | 出租车过渡/进出流
    | MAE, RMSE |'
- en: '| DeepUrbanEvent(Jiang et al., [2019](#bib.bib19)) | KDD19 | 32 | Konzatsu
    Toukei | Crowd Density/Flow | MSE |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| DeepUrbanEvent(Jiang et al., [2019](#bib.bib19)) | KDD19 | 32 | Konzatsu
    Toukei | 人群密度/流量 | MSE |'
- en: '| Curb-GAN (Zhang et al., [2020b](#bib.bib52)) | KDD20 | 1 | Taxi Speed/Inflow
    (Shenzhen)* | Taxi Speed/Inflow | RMSE, MAPE |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Curb-GAN (Zhang et al., [2020b](#bib.bib52)) | KDD20 | 1 | 出租车速度/流入（深圳）*
    | 出租车速度/流入 | RMSE, MAPE |'
- en: '| Graph-Based | Venue | Cite | Dataset (* means Open) | Prediction Task | Metric
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 基于图 | 会议 | 引用 | 数据集（*表示开放） | 预测任务 | 指标 |'
- en: '| STGCN(Yu et al., [2018](#bib.bib43)) | IJCAI18 | 660 | BJER4, PeMSD7(M)*,
    PeMSD7(L) | Traffic Volume/Speed | MAE, RMSE, MAPE |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| STGCN(Yu et al., [2018](#bib.bib43)) | IJCAI18 | 660 | BJER4, PeMSD7(M)*,
    PeMSD7(L) | 交通流量/速度 | MAE, RMSE, MAPE |'
- en: '| DCRNN(GCGRU)(Li et al., [2018](#bib.bib24)) | ICLR18 | 691 | METR-LA*, PeMS-BAY*
    | Traffic Volume/Speed | MAE, RMSE, MAPE |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| DCRNN(GCGRU)(Li et al., [2018](#bib.bib24)) | ICLR18 | 691 | METR-LA*, PeMS-BAY*
    | 交通流量/速度 | MAE, RMSE, MAPE |'
- en: '| Multi-graph(Chai et al., [2018](#bib.bib5)) | SIGSPATIAL18 | 89 | Bike Flow
    (New York and Chicago) | Bike In-Out Flow | RMSE |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Multi-graph(Chai et al., [2018](#bib.bib5)) | SIGSPATIAL18 | 89 | 自行车流量（纽约和芝加哥）
    | 自行车进出流 | RMSE |'
- en: '| ASTGCN(Guo et al., [2019](#bib.bib15)) | AAAI19 | 239 | PeMSD4-I*, PeMSD8-I*
    | Traffic Volume/Speed | MAE, RMSE |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| ASTGCN(Guo et al., [2019](#bib.bib15)) | AAAI19 | 239 | PeMSD4-I*, PeMSD8-I*
    | 交通流量/速度 | MAE, RMSE |'
- en: '| DGCNN(Diao et al., [2019](#bib.bib12)) | AAAI19 | 56 | TrafficNYC, PeMS |
    Traffic Volume/Speed | MAE, RMSE |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| DGCNN(Diao et al., [2019](#bib.bib12)) | AAAI19 | 56 | TrafficNYC, PeMS |
    交通流量/速度 | MAE, RMSE |'
- en: '| ST-MGCN(Geng et al., [2019](#bib.bib13)) | AAAI19 | 182 | Bike Demand (Beijing
    and Shanghai) | Bike Demand | MAE, RMSE, MAPE |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| ST-MGCN(Geng et al., [2019](#bib.bib13)) | AAAI19 | 182 | 自行车需求（北京和上海） |
    自行车需求 | MAE, RMSE, MAPE |'
- en: '| Graph WaveNet(Wu et al., [2019](#bib.bib39)) | IJCAI19 | 144 | METR-LA*,
    PeMS-BAY* | Traffic Volume/Speed | MAE, RMSE, MAPE |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| Graph WaveNet（Wu 等，[2019](#bib.bib39)） | IJCAI19 | 144 | METR-LA*，PeMS-BAY*
    | 交通量/速度 | MAE，RMSE，MAPE |'
- en: '| STG2Seq(Bai et al., [2019](#bib.bib3)) | IJCAI19 | 33 | DidiSY, BikeNYC*,
    TaxiBJ* | Taxi/Bike Demand | MAE, RMSE, MAPE |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| STG2Seq（Bai 等，[2019](#bib.bib3)） | IJCAI19 | 33 | DidiSY，BikeNYC*，TaxiBJ*
    | 出租车/自行车需求 | MAE，RMSE，MAPE |'
- en: '| T-GCN(Zhao et al., [2019](#bib.bib53)) | TITS19 | 195 | TaxiSZ*, METR-LA*
    | Traffic Volume/Speed | MAE, RMSE, Acc, $R^{2}$, var |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| T-GCN（Zhao 等，[2019](#bib.bib53)） | TITS19 | 195 | TaxiSZ*，METR-LA* | 交通量/速度
    | MAE，RMSE，Acc，$R^{2}$，var |'
- en: '| TGC-LSTM(Cui et al., [2019](#bib.bib9)) | TITS19 | 166 | Seattle-Loop*, INRIX
    Traffic | Traffic Volume/Speed | MAE, RMSE, MAPE |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| TGC-LSTM（Cui 等，[2019](#bib.bib9)） | TITS19 | 166 | Seattle-Loop*，INRIX Traffic
    | 交通量/速度 | MAE，RMSE，MAPE |'
- en: '| GCGA(Yu and Gu, [2019](#bib.bib45)) | TITS19 | 27 | Cologne Traffic | Traffic
    Volume/Speed | MAPE |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| GCGA（Yu 和 Gu，[2019](#bib.bib45)） | TITS19 | 27 | 科隆交通 | 交通量/速度 | MAPE |'
- en: '| GMAN(Zheng et al., [2020](#bib.bib54)) | AAAI20 | 73 | Taxi Xiamen, PeMS-BAY*
    | Traffic Volume/Speed | MAE, RMSE, MAPE |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| GMAN（Zheng 等，[2020](#bib.bib54)） | AAAI20 | 73 | Taxi Xiamen，PeMS-BAY* |
    交通量/速度 | MAE，RMSE，MAPE |'
- en: '| MRA-BGCN(Chen et al., [2020](#bib.bib6)) | AAAI20 | 28 | METR-LA*, PeMS-BAY*
    | Traffic Volume/Speed | MAE, RMSE, MAPE |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| MRA-BGCN（Chen 等，[2020](#bib.bib6)） | AAAI20 | 28 | METR-LA*，PeMS-BAY* | 交通量/速度
    | MAE，RMSE，MAPE |'
- en: '| STSGCN(Song et al., [2020](#bib.bib34)) | AAAI20 | 39 | PeMS03*, PeMS04*,
    PeMS07*, PeMS08* | Traffic Volume/Speed | MAE, RMSE, MAPE |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| STSGCN（Song 等，[2020](#bib.bib34)） | AAAI20 | 39 | PeMS03*，PeMS04*，PeMS07*，PeMS08*
    | 交通量/速度 | MAE，RMSE，MAPE |'
- en: '| SLCNN(Zhang et al., [2020a](#bib.bib51)) | AAAI20 | 13 | METR-LA*, PeMS-BAY*,
    | Traffic Volume/Speed | MAE, RMSE, MAPE |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| SLCNN（Zhang 等，[2020a](#bib.bib51)） | AAAI20 | 13 | METR-LA*，PeMS-BAY* | 交通量/速度
    | MAE，RMSE，MAPE |'
- en: '| PeMSD7(M)*, BJF, BRF, BRF-L |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| PeMSD7（M）*，BJF，BRF，BRF-L |'
- en: '| STGNN(Wang et al., [2020](#bib.bib37)) | WWW20 | 24 | METR-LA*, PeMS-BAY*
    | Traffic Volume/Speed | MAE, RMSE, MAPE |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| STGNN（Wang 等，[2020](#bib.bib37)） | WWW20 | 24 | METR-LA*，PeMS-BAY* | 交通量/速度
    | MAE，RMSE，MAPE |'
- en: '| H-STGCN(Dai et al., [2020](#bib.bib10)) | KDD20 | 9 | W3-715, E5-2907 (Beijing)
    | Traffic Volume/Speed | MAE, MAPE, RMSE |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| H-STGCN（Dai 等，[2020](#bib.bib10)） | KDD20 | 9 | W3-715，E5-2907（北京） | 交通量/速度
    | MAE，MAPE，RMSE |'
- en: '| AGCRN(Bai et al., [2020](#bib.bib4)) | NeurIPS20 | 9 | PeMSD4*, PeMSD8* |
    Traffic Volume/Speed | MAE, RMSE, MAPE |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| AGCRN（Bai 等，[2020](#bib.bib4)） | NeurIPS20 | 9 | PeMSD4*，PeMSD8* | 交通量/速度
    | MAE，RMSE，MAPE |'
- en: '| T-MGCN(Lv et al., [2020](#bib.bib27)) | TITS20 | 7 | HZJTD*, PeMSD10* | Traffic
    Volume/Speed | MAE, RMSE, MAPE |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| T-MGCN（Lv 等，[2020](#bib.bib27)） | TITS20 | 7 | HZJTD*，PeMSD10* | 交通量/速度 |
    MAE，RMSE，MAPE |'
- en: '| DGCN(Guo et al., [2020](#bib.bib14)) | TITS20 | 1 | PeMSD4*, PeMSD8*, PHILADELPHIA
    | Traffic Volume/Speed | MAE, RMSE |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| DGCN（Guo 等，[2020](#bib.bib14)） | TITS20 | 1 | PeMSD4*，PeMSD8*，PHILADELPHIA
    | 交通量/速度 | MAE，RMSE |'
- en: '| Multivariate Time-Series | Venue | Cite | Dataset (* means Open) | Prediction
    Task | Metric |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 多变量时间序列 | 场所 | 引用 | 数据集（* 表示公开） | 预测任务 | 评价指标 |'
- en: '| LSTNet(Lai et al., [2018](#bib.bib21)) | SIGIR18 | 318 | PeMS-BAY*, Solar-Energy*
    | Multivariate Time-Series | RSE, CORR |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| LSTNet（Lai 等，[2018](#bib.bib21)） | SIGIR18 | 318 | PeMS-BAY*，太阳能* | 多变量时间序列
    | RSE，CORR |'
- en: '| Electricity*, Exchange Rate* | Traffic Volume/Speed |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 电力*，汇率* | 交通量/速度 |'
- en: '| GaAN(GGRU)(Zhang et al., [2018](#bib.bib47)) | UAI18 | 214 | PPI, Reddit,
    METR-LA* | Node Classification | MAE, RMSE |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| GaAN（GGRU）（Zhang 等，[2018](#bib.bib47)） | UAI18 | 214 | PPI，Reddit，METR-LA*
    | 节点分类 | MAE，RMSE |'
- en: '| Traffic Volume/Speed |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 交通量/速度 |'
- en: '| GeoMAN(Liang et al., [2018](#bib.bib25)) | IJCAI18 | 182 | Water Quality,
    Air Quality | Multivariate Time-Series | MAE, RMSE |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| GeoMAN（Liang 等，[2018](#bib.bib25)） | IJCAI18 | 182 | 水质，空气质量 | 多变量时间序列 |
    MAE，RMSE |'
- en: '| ST-MetaNet(Pan et al., [2019](#bib.bib31)) | KDD19 | 91 | TaxiBJ-I*, METR-LA*
    | Taxi In-Out Flow | MAE, RMSE |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| ST-MetaNet（Pan 等，[2019](#bib.bib31)） | KDD19 | 91 | TaxiBJ-I*，METR-LA* |
    出租车进出流量 | MAE，RMSE |'
- en: '| Traffic Volume/Speed |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 交通量/速度 |'
- en: '| TPA-LSTM (Shih et al., [2019](#bib.bib33)) | ECMLPKDD19 | 88 | Solar Energy*,
    Traffic(PeMS)*, , | Multivariate Time-Series | RAE, RSE, CORR |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| TPA-LSTM（Shih 等，[2019](#bib.bib33)） | ECMLPKDD19 | 88 | 太阳能*，交通（PeMS）* |
    多变量时间序列 | RAE，RSE，CORR |'
- en: '| Electricity*, Music*, Exchange Rate* | Traffic Volume/Speed |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 电力*，音乐*，汇率* | 交通量/速度 |'
- en: '| Transformer(Li et al., [2019](#bib.bib23)) | NeurIPS19 | 84 | Electricity*,
    Traffic*, Solar*, Wind* | Multivariate Time-Series | $\rho$-quantile |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Transformer（Li 等，[2019](#bib.bib23)） | NeurIPS19 | 84 | 电力*，交通*，太阳能*，风能*
    | 多变量时间序列 | $\rho$-分位数 |'
- en: '| MTGNN(Wu et al., [2020](#bib.bib38)) | KDD20 | 32 | Solar-Energy*, Taffic(PeMS)*
    | Multivariate Time-Series | MAE, RMSE, MAPE |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| MTGNN（Wu 等，[2020](#bib.bib38)） | KDD20 | 32 | 太阳能*，交通（PeMS）* | 多变量时间序列 |
    MAE，RMSE，MAPE |'
- en: '| Electricity*, Exchange-Rate* | Traffic Volume/Speed | RSE, CORR |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 电力*，汇率* | 交通量/速度 | RSE，CORR |'
- en: Table 1\. Summary of The State-Of-The-Art Models *Citation number was referred
    from Google Scholar by 2021/6/13*
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 先进模型的总结 *引用编号来源于Google Scholar，截止至2021/6/13*
- en: 1\. Introduction
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: 'Nowadays, with the rapid development of IoT (Internet of Things) and CPS (Cyber-Physical
    Systems) technologies, big spatiotemporal data are being generated from mobile
    phones, car navigation systems, and traffic sensors. Based on such data, urban
    traffic prediction has been taken as a significant research problem and a key
    technique for building smart city, especially intelligent transportation system.
    From 2014 to 2017, encouraged by the huge success of deep learning technologies
    in the Computer Vision and Natural Language Processing field, researchers in the
    Intelligent Transportation System community, started to apply Long-Term Short
    Memory (LSTM) and Convolution Neural Network (CNN) to the well-established traffic
    prediction task(Huang et al., [2014](#bib.bib18); Lv et al., [2014](#bib.bib28);
    Ma et al., [2015](#bib.bib30), [2017](#bib.bib29)), and also achieved an unprecedented
    success. Following these pioneers, researchers have leveraged the state-of-the-art
    deep learning technologies to develop various prediction models and publish a
    big amount of studies on the major AI and transportation venues as listed in Table
    [1](#S0.T1 "Table 1 ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for
    Urban Traffic Prediction"). Although the prediction tasks may slightly differ
    from each other, they can all be categorized as deep traffic models.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '如今，随着物联网（IoT）和网络物理系统（CPS）技术的快速发展，大量时空数据正从手机、车载导航系统和交通传感器中产生。基于这些数据，城市交通预测被认为是一个重要的研究问题，也是构建智能城市特别是智能交通系统的关键技术。从2014年到2017年，在深度学习技术在计算机视觉和自然语言处理领域取得巨大成功的鼓舞下，智能交通系统领域的研究人员开始将长短期记忆（LSTM）和卷积神经网络（CNN）应用于成熟的交通预测任务（Huang等，[2014](#bib.bib18)；Lv等，[2014](#bib.bib28)；Ma等，[2015](#bib.bib30)，[2017](#bib.bib29)），并取得了前所未有的成功。继这些先驱者之后，研究人员利用先进的深度学习技术开发了各种预测模型，并在主要的人工智能和交通领域发表了大量研究，如表[1](#S0.T1
    "Table 1 ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic
    Prediction")所示。尽管预测任务可能有所不同，但它们都可以归类为深度交通模型。'
- en: '![Refer to caption](img/16e0bbcd8a9e450891d5fd29623e0d34.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/16e0bbcd8a9e450891d5fd29623e0d34.png)'
- en: Figure 1\. Grid-Based Traffic and Graph-Based Traffic.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 基于网格的交通与基于图的交通。
- en: 'No matter based on grid or graph, the traffic data illustrated in Fig.[1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ DL-Traff: Survey and Benchmark of Deep Learning
    Models for Urban Traffic Prediction") can be uniformly represented with a 3D tensor
    $\mathbb{R}^{T\times N\times C}$, where T denotes the size of the temporal domain
    (i.e., timeslots with constant sampling rate), N denotes the size of the spatial
    domain (i.e., mesh-grids or graph-nodes), and C denotes the number of information
    channels. For instance, assuming 300 traffic sensors are deployed to record traffic
    speed (channel1) and volume (channel2) every 30 minutes for 100 consecutive days,
    then the total data can be represented by tensor $\mathbb{R}^{4800\times 300\times
    2}$. Besides traffic volume and speed, channels can also be used to store crowd
    density, taxi demand, traffic accident, car/ride-hailing order, and crowd/taxi/bike
    inflow and outflow. More specifically, grid-based model meshes the entire spatial
    domain into $H\times W$ fine-grained mesh-grids and converts the 3D representation
    into 4D tensor $\mathbb{R}^{T\times H\times W\times C}$ format. Graph-based model
    introduces directed or undirected graph $G$ = $(V,E)$ to utilize the topological
    structure of the urban road network for modeling, where $v\in V$ is a node, $|V|$
    = $N$, and $e\in E$ is an edge. Multivariate time-series model naturally takes
    the N spatial units as N time-series variates and shares the same representation,
    i.e., $\mathbb{R}^{T\times N\times C}$ with graph-based model. Thus, the deep
    learning models listed in Table [1](#S0.T1 "Table 1 ‣ DL-Traff: Survey and Benchmark
    of Deep Learning Models for Urban Traffic Prediction") can be divided into three
    groups according to the specific modeling strategy along the spatial axis.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是基于网格还是图形，图中的交通数据[1](#S1.F1 "图 1 ‣ 1. 介绍 ‣ DL-Traff：城市交通预测深度学习模型的调查与基准")都可以用一个3D张量$\mathbb{R}^{T\times
    N\times C}$来统一表示，其中T表示时间域的大小（即具有恒定采样率的时间段），N表示空间域的大小（即网格或图节点），C表示信息通道的数量。例如，假设部署了300个交通传感器，每30分钟记录一次交通速度（channel1）和流量（channel2），持续100天，则总数据可以由张量$\mathbb{R}^{4800\times
    300\times 2}$表示。除了交通流量和速度，通道还可以用于存储人群密度、出租车需求、交通事故、车/打车订单以及人群/出租车/自行车的流入和流出。更具体地说，基于网格的模型将整个空间域划分为$H\times
    W$细粒度的网格，并将3D表示转换为4D张量$\mathbb{R}^{T\times H\times W\times C}$格式。基于图形的模型引入了有向或无向图$G$
    = $(V,E)$，利用城市道路网络的拓扑结构进行建模，其中$v\in V$是一个节点，$|V|$ = $N$，$e\in E$是一个边。多变量时间序列模型自然将N个空间单元视为N个时间序列变量，并与基于图形的模型共享相同的表示，即$\mathbb{R}^{T\times
    N\times C}$。因此，表[1](#S0.T1 "表 1 ‣ DL-Traff：城市交通预测深度学习模型的调查与基准")中列出的深度学习模型可以根据空间轴上的特定建模策略分为三类。
- en: 'Through the citation number in Table [1](#S0.T1 "Table 1 ‣ DL-Traff: Survey
    and Benchmark of Deep Learning Models for Urban Traffic Prediction"), we can know
    how much attention these studies have drawn in our AI and data science community.
    But due to the huge amount of the related works, researchers are often too exhausted
    to follow up with the specific details of each model. More importantly, the evaluations
    on this family of models are still confusing and not well organized. For instance,
    some models demonstrated superior performances to the existing ones by using different
    datasets or metrics as shown in Table [1](#S0.T1 "Table 1 ‣ DL-Traff: Survey and
    Benchmark of Deep Learning Models for Urban Traffic Prediction"), while some models
    utilized a self-designed objective function or employed extra data sources such
    as Point-of-Interest (POI) data (Lin et al., [2019](#bib.bib26)) or navigation
    app data (Dai et al., [2020](#bib.bib10)) to achieve better prediction accuracy.
    To address the problems above, a concise but precise survey will be a great help
    for researchers involved in this emerging topic. But only a survey is not enough.
    It is also significant to conduct standard performance evaluations to examine
    the true function of each spatial and temporal component by using the same datasets,
    metrics, and other experimental settings. This paper fills these needs by providing
    a concise survey followed by a comprehensive benchmark evaluation on the recent
    deep traffic models.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '通过表格[1](#S0.T1 "Table 1 ‣ DL-Traff: Survey and Benchmark of Deep Learning Models
    for Urban Traffic Prediction")中的引用编号，我们可以了解这些研究在我们的AI和数据科学社区中引起了多少关注。但由于相关工作的数量庞大，研究人员往往难以跟进每个模型的具体细节。更重要的是，这类模型的评估仍然令人困惑且不够有组织。例如，某些模型通过使用不同的数据集或指标（如表格[1](#S0.T1
    "Table 1 ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic
    Prediction")所示）显示出比现有模型更优的性能，而一些模型利用了自设计的目标函数或额外的数据来源，如兴趣点（POI）数据（Lin et al.,
    [2019](#bib.bib26)）或导航应用数据（Dai et al., [2020](#bib.bib10)）来提高预测准确性。为了解决这些问题，简明而精确的综述将对参与这一新兴主题的研究人员大有帮助。然而，仅有综述还不够。进行标准化的性能评估同样重要，通过使用相同的数据集、指标和其他实验设置来检验每个空间和时间组件的实际功能。本论文通过提供简明的综述并跟随最近深度交通模型的全面基准评估来满足这些需求。'
- en: 'We first define two benchmark tasks in Section 2, one is single-step prediction
    for inflow and outflow based on grid-based traffic data, another is multi-step
    prediction for traffic speed based on graph-based data. Second, in Section 3,
    we investigate both of the grid-based and graph-based datasets and pick up some
    open and widely used ones as our benchmark data including TaxiBJ, BikeNYC, TaxiNYC,
    METR-LA, PeMS-BAY, and PeMSD7M. Next, in Section 4, we decompose the models into
    spatial and temporal units and give the roadmap that how the models evolve along
    the spatial and temporal axis. Further, we draw the architectures for a bunch
    of representative models (e.g., ST-ResNet(Zhang et al., [2017](#bib.bib48)), DMVST-Net(Yao
    et al., [2018](#bib.bib42)), STDN(Yao et al., [2019](#bib.bib41)), DeepSTN+(Lin
    et al., [2019](#bib.bib26)), STGCN(Yu et al., [2018](#bib.bib43)), DCRNN(Li et al.,
    [2018](#bib.bib24)), Graph WaveNet(Wu et al., [2019](#bib.bib39))) in an intuitive
    and comparative manner. Then, in Section 5, we do a comprehensive evaluation on
    both the grid-based and graph-based models by using the benchmark tasks and datasets
    under the same settings and metrics (RMSE, MAE, MAPE). In Section 6, we briefly
    introduce the implementation details, the availability, and the usability of our
    benchmark. Finally, we give our conclusion in Section 7. The contributions of
    our work are summarized as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在第2节定义了两个基准任务，一个是基于网格数据的单步流入和流出预测，另一个是基于图数据的多步交通速度预测。其次，在第3节中，我们调查了网格数据集和图数据集，并选择了一些公开的、广泛使用的基准数据，包括TaxiBJ、BikeNYC、TaxiNYC、METR-LA、PeMS-BAY和PeMSD7M。接下来，在第4节中，我们将模型分解为空间和时间单元，并给出模型在空间和时间轴上演变的路线图。此外，我们以直观且对比的方式绘制了一些代表性模型的架构（例如ST-ResNet(Zhang
    et al., [2017](#bib.bib48))，DMVST-Net(Yao et al., [2018](#bib.bib42))，STDN(Yao
    et al., [2019](#bib.bib41))，DeepSTN+(Lin et al., [2019](#bib.bib26))，STGCN(Yu
    et al., [2018](#bib.bib43))，DCRNN(Li et al., [2018](#bib.bib24))，Graph WaveNet(Wu
    et al., [2019](#bib.bib39)))。然后，在第5节中，我们使用基准任务和数据集在相同的设置和指标（RMSE，MAE，MAPE）下对网格数据模型和图数据模型进行全面评估。在第6节中，我们简要介绍了我们的基准测试的实现细节、可用性和实用性。最后，我们在第7节给出了结论。我们的工作贡献总结如下：
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We give a concise but concrete survey on the recent deep traffic models. The
    technique detail and the evolution are clearly summarized along spatial and temporal
    axes.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对近期的深度交通模型进行了简明但具体的综述。技术细节和演变在空间和时间轴上得到了清晰的总结。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We carefully select two traffic flow prediction tasks, four grid-based traffic
    datasets, and three graph-based traffic datasets, and implement plenty of grid/graph-based
    state-of-the-arts to form a complete benchmark called DL-Traff.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们精心选择了两个交通流量预测任务、四个基于网格的交通数据集和三个基于图的交通数据集，并实现了大量的网格/图形状态的最新技术，形成了一个完整的基准测试集，称为DL-Traff。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: On this benchmark, we conduct a comprehensive evaluation of the effectiveness
    and efficiency performances of the-state-of-the-arts.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个基准测试集上，我们对最新技术的有效性和效率进行了全面评估。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our benchmark is implemented with the two most popular deep learning frameworks,
    i.e., TensorFlow and PyTorch. DL-Traff is already publicly available as two GitHub
    repositories [https://github.com/deepkashiwa20/DL-Traff-Grid](https://github.com/deepkashiwa20/DL-Traff-Grid)
    and [https://github.com/deepkashiwa20/DL-Traff-Graph](https://github.com/deepkashiwa20/DL-Traff-Graph).
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的基准测试集是用两个最受欢迎的深度学习框架实现的，即TensorFlow和PyTorch。DL-Traff已经作为两个GitHub仓库公开：[https://github.com/deepkashiwa20/DL-Traff-Grid](https://github.com/deepkashiwa20/DL-Traff-Grid)
    和 [https://github.com/deepkashiwa20/DL-Traff-Graph](https://github.com/deepkashiwa20/DL-Traff-Graph)。
- en: With DL-Traff, (1) users can quickly grasp the technical details about the state-of-the-art
    deep spatiotemporal models; (2) users can smoothly reproduce the prediction results
    reported in this paper and use them as the baselines; (3) users can easily launch
    a new deep solution with either TensorFlow or PyTorch for not only traffic flow
    prediction tasks, but also for other spatiotemporal problems such as anomaly/accident,
    electricity consumption, air quality, etc.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DL-Traff，（1）用户可以快速掌握有关最新深度时空模型的技术细节；（2）用户可以顺利复现本文报告的预测结果并将其作为基准；（3）用户可以轻松启动新的深度解决方案，使用TensorFlow或PyTorch，不仅适用于交通流量预测任务，还可以用于其他时空问题，如异常/事故、电力消耗、空气质量等。
- en: 2\. Problem
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 问题
- en: In this paper, we employ the following two prediction tasks into our benchmark.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将以下两个预测任务纳入我们的基准测试集。
- en: (1)
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Grid-based inflow and outflow prediction proposed by(Hoang et al., [2016](#bib.bib16);
    Zhang et al., [2016](#bib.bib49)). The problem is to predict how many taxis/bikes
    will flow into or out from each mesh-grid in the next time interval. It takes
    $\alpha$ steps of historical observations as input and gives the next step prediction
    as follows: [$X_{t-(\alpha-1)}$,…,$X_{t-1}$,$X_{t}$] $\rightarrow$ $X_{t+1}$,
    where $X_{i}$ $\in$ $\mathbb{R}^{H\times W\times C}$, $H,W$ are the indexes for
    the mesh, and C is equal to 2, respectively used for inflow and outflow.'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于网格的流入和流出预测由(Hoang et al., [2016](#bib.bib16); Zhang et al., [2016](#bib.bib49))提出。问题是预测在下一个时间间隔内每个网格中将有多少出租车/自行车流入或流出。它以$\alpha$步的历史观测作为输入，给出下一个步骤的预测如下：[$X_{t-(\alpha-1)}$,…,$X_{t-1}$,$X_{t}$]
    $\rightarrow$ $X_{t+1}$，其中$X_{i}$ $\in$ $\mathbb{R}^{H\times W\times C}$，$H,W$是网格的索引，C分别等于2，用于流入和流出。
- en: (2)
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Graph-based traffic speed prediction as defined in (Yu et al., [2018](#bib.bib43);
    Li et al., [2018](#bib.bib24)). In order to make a variation to the first task,
    we define this task as multi-step-to-multi-step one as follows: [$X_{t-(\alpha-1)}$,…,$X_{t-1}$,$X_{t}$]
    $\rightarrow$ [$X_{t+1}$,$X_{t+2}$,…,$X_{t+\beta}$], where $X_{i}$ $\in$ $\mathbb{R}^{N\times
    C}$, $\alpha$/$\beta$ is the number of steps of observations/predictions, $N$
    is the number of traffic sensors (i.e., nodes), and $C$ is equal to 1 that only
    stores the traffic speed.'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图形基础的交通速度预测如(Yu et al., [2018](#bib.bib43); Li et al., [2018](#bib.bib24))中所定义。为了对第一个任务进行变化，我们将此任务定义为多步到多步的任务，如下所示：[$X_{t-(\alpha-1)}$,…,$X_{t-1}$,$X_{t}$]
    $\rightarrow$ [$X_{t+1}$,$X_{t+2}$,…,$X_{t+\beta}$]，其中$X_{i}$ $\in$ $\mathbb{R}^{N\times
    C}$，$\alpha$/$\beta$是观测/预测的步骤数，$N$是交通传感器的数量（即节点），$C$等于1，仅存储交通速度。
- en: Table 2\. Summary of The Public Traffic Datasets
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 公共交通数据集的总结
- en: '| Grid-Based | Reference | Data Description / Data Source | Spatial Domain
    | Time Period | Time Interval |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 基于网格的 | 参考文献 | 数据描述 / 数据来源 | 空间范围 | 时间段 | 时间间隔 |'
- en: '| TaxiBJ* | (Zhang et al., [2017](#bib.bib48); Zonoozi et al., [2018](#bib.bib55))
    | Taxi In-Out Flow / Taxi GPS Data of Beijing | 32$\times$32 grids | 2013/7/1$\sim$2016/4/10
    | 30 minutes |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| TaxiBJ* | (Zhang et al., [2017](#bib.bib48); Zonoozi et al., [2018](#bib.bib55))
    | 北京的出租车进出流量 / 出租车GPS数据 | 32$\times$32网格 | 2013/7/1$\sim$2016/4/10 | 30分钟 |'
- en: '| (Bai et al., [2019](#bib.bib3)) | *Four Time Periods |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| (Bai et al., [2019](#bib.bib3)) | *四个时间段* |'
- en: '| TaxiBJ-I* | (Pan et al., [2019](#bib.bib31)) | Taxi In-Out Flow / Taxi GPS
    Data of Beijing (TDrive) | 32$\times$32 grids | 2015/2/1$\sim$2015/6/2 | 60 minutes
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| TaxiBJ-I* | (Pan 等，[2019](#bib.bib31)) | 北京出租车进出流量 / 出租车 GPS 数据（TDrive） |
    32$\times$32 网格 | 2015/2/1$\sim$2015/6/2 | 60 分钟 |'
- en: '| BikeNYC* | (Zhang et al., [2017](#bib.bib48); Bai et al., [2019](#bib.bib3))
    | Bike In-Out Flow / Bike Trip Data of New York City | 16$\times$8 grids | 2014/4/1$\sim$2014/9/30
    | 60 minutes |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| BikeNYC* | (Zhang 等，[2017](#bib.bib48)； Bai 等，[2019](#bib.bib3)) | 纽约市自行车进出流量
    / 自行车出行数据 | 16$\times$8 网格 | 2014/4/1$\sim$2014/9/30 | 60 分钟 |'
- en: '| Citi Bike: [https://www.citibikenyc.com/system-data](https://www.citibikenyc.com/system-data)
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Citi Bike: [https://www.citibikenyc.com/system-data](https://www.citibikenyc.com/system-data)
    |'
- en: '| BikeNYC-I* | (Lin et al., [2019](#bib.bib26)) | Bike In-Out Flow / Bike Trip
    Data of New York City | 21$\times$12 grids | 2014/4/1$\sim$2014/9/30 | 60 minutes
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| BikeNYC-I* | (Lin 等，[2019](#bib.bib26)) | 纽约市自行车进出流量 / 自行车出行数据 | 21$\times$12
    网格 | 2014/4/1$\sim$2014/9/30 | 60 分钟 |'
- en: '| BikeNYC-II* | (Yao et al., [2019](#bib.bib41)) | Bike In-Out Flow / Bike
    Trip Data of New York City | 10$\times$20 grids | 2016/7/1$\sim$2016/8/29 | 30
    minutes |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| BikeNYC-II* | (Yao 等，[2019](#bib.bib41)) | 纽约市自行车进出流量 / 自行车出行数据 | 10$\times$20
    网格 | 2016/7/1$\sim$2016/8/29 | 30 分钟 |'
- en: '| TaxiNYC* | (Yao et al., [2019](#bib.bib41)) | Taxi In-Out Flow / Taxi Trip
    Data of New York City | 10$\times$20 grids | 2015/1/1$\sim$2015/3/1 | 30 minutes
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| TaxiNYC* | (Yao 等，[2019](#bib.bib41)) | 纽约市出租车进出流量 / 出租车出行数据 | 10$\times$20
    网格 | 2015/1/1$\sim$2015/3/1 | 30 分钟 |'
- en: '| The New York City Taxi&Limousine Commission |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 纽约市出租车及豪华轿车委员会 |'
- en: '| (TLC) [https://www1.nyc.gov/site/tlc/about/data.page](https://www1.nyc.gov/site/tlc/about/data.page)
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| (TLC) [https://www1.nyc.gov/site/tlc/about/data.page](https://www1.nyc.gov/site/tlc/about/data.page)
    |'
- en: '| Graph-Based | Reference | Data Description / Data Source | Spatial Domain
    | Time Period | Time Interval |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 基于图的 | 参考 | 数据描述 / 数据来源 | 空间领域 | 时间周期 | 时间间隔 |'
- en: '| METR-LA* | (Wang et al., [2020](#bib.bib37)) | Traffic Speed Sensors in Los
    Angeles County | 207 sensors | 2012/3/1$\sim$2012/6/30 | 5 minutes |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| METR-LA* | (Wang 等，[2020](#bib.bib37)) | 洛杉矶县交通速度传感器 | 207 个传感器 | 2012/3/1$\sim$2012/6/30
    | 5 分钟 |'
- en: '| (Zhao et al., [2019](#bib.bib53); Li et al., [2018](#bib.bib24)) | Los Angeles
    Metropolitan Transportation Authority |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| (Zhao 等，[2019](#bib.bib53)； Li 等，[2018](#bib.bib24)) | 洛杉矶大都市运输管理局 |'
- en: '| (Wu et al., [2019](#bib.bib39); Zhang et al., [2018](#bib.bib47)) | *Collaborated
    with University of Southern California |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| (Wu 等，[2019](#bib.bib39)； Zhang 等，[2018](#bib.bib47)) | *与南加州大学合作* |'
- en: '| (Chen et al., [2020](#bib.bib6); Pan et al., [2019](#bib.bib31)) | [https://imsc.usc.edu/platforms/transdec/](https://imsc.usc.edu/platforms/transdec/)
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| (Chen 等，[2020](#bib.bib6)； Pan 等，[2019](#bib.bib31)) | [https://imsc.usc.edu/platforms/transdec/](https://imsc.usc.edu/platforms/transdec/)
    |'
- en: '| PeMS-BAY* | (Li et al., [2018](#bib.bib24); Lai et al., [2018](#bib.bib21))
    | Traffic Speed Sensors in California | 325 sensors | 2017/1/1$\sim$2017/5/31
    | 5 minutes |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| PeMS-BAY* | (Li 等，[2018](#bib.bib24)； Lai 等，[2018](#bib.bib21)) | 加州交通速度传感器
    | 325 个传感器 | 2017/1/1$\sim$2017/5/31 | 5 分钟 |'
- en: '| (Wu et al., [2019](#bib.bib39); Chen et al., [2020](#bib.bib6)) | Caltrans
    Performance Measurement System (PeMS) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| (Wu 等，[2019](#bib.bib39)； Chen 等，[2020](#bib.bib6)) | Caltrans 性能测量系统（PeMS）
    |'
- en: '| (Wang et al., [2020](#bib.bib37); Zheng et al., [2020](#bib.bib54)) | PeMS:
    [http://pems.dot.ca.gov/](http://pems.dot.ca.gov/) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| (Wang 等，[2020](#bib.bib37)； Zheng 等，[2020](#bib.bib54)) | PeMS: [http://pems.dot.ca.gov/](http://pems.dot.ca.gov/)
    |'
- en: '| PeMSD7(M)* | (Yu et al., [2018](#bib.bib43)) | Traffic Speed Sensors in California
    (PeMS) | 228 sensors | 2012/5/1$\sim$2012/6/30 | 5 minutes |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| PeMSD7(M)* | (Yu 等，[2018](#bib.bib43)) | 加州交通速度传感器（PeMS） | 228 个传感器 | 2012/5/1$\sim$2012/6/30
    | 5 分钟 |'
- en: '| PeMS03* | (Song et al., [2020](#bib.bib34)) | Traffic Speed Sensors in California
    (PeMS) | 358 sensors | 2018/9/1$\sim$2018/11/30 | 5 minutes |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| PeMS03* | (Song 等，[2020](#bib.bib34)) | 加州交通速度传感器（PeMS） | 358 个传感器 | 2018/9/1$\sim$2018/11/30
    | 5 分钟 |'
- en: '| PeMSD4(PeMS04)* | (Song et al., [2020](#bib.bib34); Bai et al., [2020](#bib.bib4))
    | Traffic Speed Sensors in California (PeMS) | 307 sensors | 2018/1/1$\sim$2018/2/28
    | 5 minutes |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| PeMSD4(PeMS04)* | (Song 等，[2020](#bib.bib34)； Bai 等，[2020](#bib.bib4)) |
    加州交通速度传感器（PeMS） | 307 个传感器 | 2018/1/1$\sim$2018/2/28 | 5 分钟 |'
- en: '| PeMS07* | (Song et al., [2020](#bib.bib34)) | Traffic Speed Sensors in California
    (PeMS) | 883 sensors | 2017/5/1$\sim$2017/8/31 | 5 minutes |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| PeMS07* | (Song 等，[2020](#bib.bib34)) | 加州交通速度传感器（PeMS） | 883 个传感器 | 2017/5/1$\sim$2017/8/31
    | 5 分钟 |'
- en: '| PeMSD8(PeMS08)* | (Song et al., [2020](#bib.bib34); Bai et al., [2020](#bib.bib4))
    | Traffic Speed Sensors in California (PeMS) | 170 sensors | 2016/7/1$\sim$2016/8/31
    | 5 minutes |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| PeMSD8(PeMS08)* | (Song 等，[2020](#bib.bib34)； Bai 等，[2020](#bib.bib4)) |
    加州交通速度传感器（PeMS） | 170 个传感器 | 2016/7/1$\sim$2016/8/31 | 5 分钟 |'
- en: '| PeMSD4-I* | (Guo et al., [2019](#bib.bib15)) | Traffic Speed Sensors in California
    (PeMS) | 3848 sensors | 2018/1/1$\sim$2018/2/28 | 5 minutes |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| PeMSD4-I* | (郭等，[2019](#bib.bib15)) | 加利福尼亚的交通速度传感器（PeMS） | 3848个传感器 | 2018/1/1$\sim$2018/2/28
    | 5分钟 |'
- en: '| PeMSD8-I* | (Guo et al., [2019](#bib.bib15)) | Traffic Speed Sensors in California
    (PeMS) | 1979 sensors | 2016/7/1$\sim$2016/8/31 | 5 minutes |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| PeMSD8-I* | (郭等，[2019](#bib.bib15)) | 加利福尼亚的交通速度传感器（PeMS） | 1979个传感器 | 2016/7/1$\sim$2016/8/31
    | 5分钟 |'
- en: '| PeMSD10* | (Lv et al., [2020](#bib.bib27)) | Traffic Speed Sensors in California
    (PeMS) | 608 sensors | 2018/1/1$\sim$2018/3/31 | 15 minutes |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| PeMSD10* | (吕等，[2020](#bib.bib27)) | 加利福尼亚的交通速度传感器（PeMS） | 608个传感器 | 2018/1/1$\sim$2018/3/31
    | 15分钟 |'
- en: '| Traffic(PeMS)* | (Shih et al., [2019](#bib.bib33); Wu et al., [2020](#bib.bib38))
    | Traffic Speed Sensors in California (PeMS) | 862 sensors | 2015/1/1$\sim$2016/12/31
    | 60 minutes |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Traffic(PeMS)* | (史等，[2019](#bib.bib33)；吴等，[2020](#bib.bib38)) | 加利福尼亚的交通速度传感器（PeMS）
    | 862个传感器 | 2015/1/1$\sim$2016/12/31 | 60分钟 |'
- en: '| LOOP-SEATTLE* | (Cui et al., [2019](#bib.bib9)) | Traffic Speed Sensors in
    Greater Seattle Area | 323 sensors | 2015/1/1$\sim$2015/12/31 | 5 minutes |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| LOOP-SEATTLE* | (崔等，[2019](#bib.bib9)) | 大西雅图地区的交通速度传感器 | 323个传感器 | 2015/1/1$\sim$2015/12/31
    | 5分钟 |'
- en: '| TaxiSZ* | (Zhao et al., [2019](#bib.bib53)) | Taxi Speed on Roads / Taxi
    GPS Data of Shenzhen | 156 roads | 2015/1/1$\sim$2015/1/31 | 15 minutes |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| TaxiSZ* | (赵等，[2019](#bib.bib53)) | 深圳的出租车道路速度/出租车GPS数据 | 156条道路 | 2015/1/1$\sim$2015/1/31
    | 15分钟 |'
- en: '| HZJTD* | (Lv et al., [2020](#bib.bib27)) | Traffic Speed Sensors in Hangzhou
    | 202 sensors | 2013/10/16$\sim$2014/10/3 | 15 minutes |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| HZJTD* | (吕等，[2020](#bib.bib27)) | 杭州的交通速度传感器 | 202个传感器 | 2013/10/16$\sim$2014/10/3
    | 15分钟 |'
- en: '| Hangzhou Integrated Transportation Research Center |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 杭州综合交通研究中心 |'
- en: 3\. Dataset
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 数据集
- en: 'The public datasets for urban traffic prediction are summarized in Table [2](#S2.T2
    "Table 2 ‣ 2\. Problem ‣ DL-Traff: Survey and Benchmark of Deep Learning Models
    for Urban Traffic Prediction"), where the reference, source, and spatial and temporal
    spec are enumerated. We pick up some widely used ones as our benchmark datasets.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '用于城市交通预测的公开数据集汇总在表格[2](#S2.T2 "表2 ‣ 2\. 问题 ‣ DL-Traff: 深度学习模型在城市交通预测中的调查和基准")中，其中列出了参考文献、来源、空间和时间规格。我们选择了一些广泛使用的数据集作为我们的基准数据集。'
- en: '![Refer to caption](img/9e5865da592096f7eb110083d9ab4149.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9e5865da592096f7eb110083d9ab4149.png)'
- en: Figure 2\. Visualization of METR-LA.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. METR-LA的可视化。
- en: 3.1\. Grid-Based Traffic Dataset
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 基于网格的交通数据集
- en: 'TaxiBJ. This is taxi in-out flow data published by (Zhang et al., [2017](#bib.bib48)),
    created from the taxicab GPS data in Beijing from four separate time periods:
    2013/7/1-2013/10/30, 2014/3/1-2014/6/30, 2015/3/1-2015/6/30, and 2015/11/1-2016/4/10\.
    Based on the same underlying taxi GPS data (T-Drive), a similar dataset denoted
    as TaxiBJ-I is created by (Pan et al., [2019](#bib.bib31)).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: TaxiBJ。这是由（张等，[2017](#bib.bib48)）发布的出租车进出流量数据，来源于北京市四个不同时间段的出租车GPS数据：2013/7/1-2013/10/30、2014/3/1-2014/6/30、2015/3/1-2015/6/30，以及2015/11/1-2016/4/10。基于相同的出租车GPS数据（T-Drive），（潘等，[2019](#bib.bib31)）创建了一个类似的数据集，标记为TaxiBJ-I。
- en: 'BikeNYC. This is bike in-out flow data of New York City from 2014/4/1 to 2014/9/30
    used by (Zhang et al., [2017](#bib.bib48)). The original bike trip data is published
    by Citi Bike, NYC’s official bike-sharing system, which includes: trip duration,
    starting and ending station IDs, and start and end times. Similar datasets BikeNYC-I,
    BikeNYC-II were used by (Lin et al., [2019](#bib.bib26)) and (Yao et al., [2019](#bib.bib41))
    respectively. These two will be used in our experiment due to the larger spatial
    domain.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: BikeNYC。这是来自2014/4/1到2014/9/30的纽约市自行车进出流量数据，使用了（张等，[2017](#bib.bib48)）。原始自行车行程数据由纽约市官方自行车共享系统Citi
    Bike发布，包含：行程时长、起始和结束车站ID、开始和结束时间。类似的数据集BikeNYC-I和BikeNYC-II分别被（林等，[2019](#bib.bib26)）和（姚等，[2019](#bib.bib41)）使用。由于空间范围更大，我们将在实验中使用这两个数据集。
- en: TaxiNYC. This is taxi in-out flow data of New York City from 2015/1/1 2015/3/1
    used by (Yao et al., [2019](#bib.bib41)). The original taxi trip data is published
    by the New York City Taxi and Limousine Commission (TLC), that includes pick-up
    and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized
    fares, driver-reported passenger counts, etc.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: TaxiNYC。这是2015/1/1到2015/3/1的纽约市出租车进出流量数据，由（姚等，[2019](#bib.bib41)）使用。原始出租车行程数据由纽约市出租车和豪华车委员会（TLC）发布，包括接送时间/日期、接送地点、行程距离、详细费用、司机报告的乘客数量等。
- en: 3.2\. Graph-Based Traffic Dataset
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 基于图的交通数据集
- en: 'METR-LA. This is a Los Angeles traffic data published by (Li et al., [2018](#bib.bib24)).
    The data are collected from 207 highway sensors within 4 months from 2012/3/1
    to 2012/6/30\. A quite number of studies used this dataset as shown in Table [2](#S2.T2
    "Table 2 ‣ 2\. Problem ‣ DL-Traff: Survey and Benchmark of Deep Learning Models
    for Urban Traffic Prediction"). To be intuitive, a data visualization has been
    made as Fig.[2](#S3.F2 "Figure 2 ‣ 3\. Dataset ‣ DL-Traff: Survey and Benchmark
    of Deep Learning Models for Urban Traffic Prediction").'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'METR-LA。这是由(Li et al., [2018](#bib.bib24))发布的洛杉矶交通数据。数据收集自2012年3月1日至2012年6月30日的207个高速公路传感器。许多研究使用了这个数据集，如表[2](#S2.T2
    "Table 2 ‣ 2\. Problem ‣ DL-Traff: Survey and Benchmark of Deep Learning Models
    for Urban Traffic Prediction")所示。为了直观展示，制作了数据可视化图，如图[2](#S3.F2 "Figure 2 ‣ 3\.
    Dataset ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic
    Prediction")所示。'
- en: PeMS-BAY. This is a traffic flow dataset collected from California Transportation
    Agencies Performance Measurement System (PeMS). It contains 325 traffic sensors
    in the Bay Area from 2017/1/1 to 2017/5/31\. Massive studies also generate a variety
    of PeMS datasets by using the same source.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: PeMS-BAY。这是从加州交通管理局绩效测量系统（PeMS）收集的交通流量数据集。它包含2017年1月1日至2017年5月31日的325个交通传感器。大量研究还利用相同的来源生成了各种PeMS数据集。
- en: PeMSD7M. This traffic dataset is created and published by (Yu et al., [2018](#bib.bib43)),
    also collected from PeMS. It covers 228 traffic sensors lasting from 2012/5/1
    to 2012/6/30 with a 5-minute sampling rate on weekdays.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: PeMSD7M。这一交通数据集由(Yu et al., [2018](#bib.bib43))创建和发布，也收集自PeMS。它涵盖了2012年5月1日至2012年6月30日的228个交通传感器，工作日采样率为5分钟。
- en: 'Summary. The taxi and bike trip data published by Citi Bike and TLC of New
    York City and the traffic sensor data from PeMS of California are taken as three
    trustworthy and wildly-used data sources for traffic prediction. Researchers can
    easily access the data through the URLs listed in Table [2](#S2.T2 "Table 2 ‣
    2\. Problem ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for Urban
    Traffic Prediction").'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '总结。由Citi Bike和纽约市TLC发布的出租车和自行车出行数据，以及来自加州PeMS的交通传感器数据，被认为是三种可靠且广泛使用的交通预测数据来源。研究人员可以通过表[2](#S2.T2
    "Table 2 ‣ 2\. Problem ‣ DL-Traff: Survey and Benchmark of Deep Learning Models
    for Urban Traffic Prediction")中列出的URL轻松访问这些数据。'
- en: Table 3\. Base Technologies Employed for Spatial and Temporal Modeling
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 空间和时间建模所采用的基础技术
- en: '|  | Spatial Axis | Temporal Axis |  | Spatial Axis | Temporal Axis |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | 空间轴 | 时间轴 |  | 空间轴 | 时间轴 |'
- en: '| models | CNN | GCN | Attn. | LSTM | GRU | TCN | Attn. | models | CNN | GCN
    | Attn. | LSTM | GRU | TCN | Attn. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | CNN | GCN | Attn. | LSTM | GRU | TCN | Attn. | 模型 | CNN | GCN | Attn.
    | LSTM | GRU | TCN | Attn. |'
- en: '| ST-ResNet(Zhang et al., [2017](#bib.bib48)) | ✓ |  |  |  |  |  |  | STGCN(Yu
    et al., [2018](#bib.bib43)) |  | ✓ |  |  |  | ✓ |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| ST-ResNet(Zhang et al., [2017](#bib.bib48)) | ✓ |  |  |  |  |  |  | STGCN(Yu
    et al., [2018](#bib.bib43)) |  | ✓ |  |  |  | ✓ |  |'
- en: '| DMVST-Net(Yao et al., [2018](#bib.bib42)) | ✓ |  |  | ✓ |  |  |  | GaAN(GGRU)(Zhang
    et al., [2018](#bib.bib47)) |  | ✓ | ✓ |  | ✓ |  |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| DMVST-Net(Yao et al., [2018](#bib.bib42)) | ✓ |  |  | ✓ |  |  |  | GaAN(GGRU)(Zhang
    et al., [2018](#bib.bib47)) |  | ✓ | ✓ |  | ✓ |  |  |'
- en: '| STDN(Yao et al., [2019](#bib.bib41)) | ✓ |  |  | ✓ |  |  | ✓ | DCRNN(GCGRU)(Li
    et al., [2018](#bib.bib24)) |  | ✓ |  |  | ✓ |  |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| STDN(Yao et al., [2019](#bib.bib41)) | ✓ |  |  | ✓ |  |  | ✓ | DCRNN(GCGRU)(Li
    et al., [2018](#bib.bib24)) |  | ✓ |  |  | ✓ |  |  |'
- en: '| DeepSTN+(Lin et al., [2019](#bib.bib26)) | ✓ |  |  |  |  |  |  | Multi-graph(Chai
    et al., [2018](#bib.bib5)) |  | ✓ |  | ✓ |  |  |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| DeepSTN+(Lin et al., [2019](#bib.bib26)) | ✓ |  |  |  |  |  |  | 多图（Chai
    et al., [2018](#bib.bib5)) |  | ✓ |  | ✓ |  |  |  |'
- en: '| LSTNet(Lai et al., [2018](#bib.bib21)) | ✓ |  |  |  | ✓ | ✓ | ✓ | ASTGCN(Guo
    et al., [2019](#bib.bib15)) |  | ✓ | ✓ |  |  |  | ✓ |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| LSTNet(Lai et al., [2018](#bib.bib21)) | ✓ |  |  |  | ✓ | ✓ | ✓ | ASTGCN(Guo
    et al., [2019](#bib.bib15)) |  | ✓ | ✓ |  |  |  | ✓ |'
- en: '| GeoMAN(Liang et al., [2018](#bib.bib25)) |  |  | ✓ | ✓ |  |  | ✓ | TGCN(Zhao
    et al., [2019](#bib.bib53)) |  | ✓ |  |  | ✓ |  |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| GeoMAN(Liang et al., [2018](#bib.bib25)) |  |  | ✓ | ✓ |  |  | ✓ | TGCN(Zhao
    et al., [2019](#bib.bib53)) |  | ✓ |  |  | ✓ |  |  |'
- en: '| TPA-LSTM(Shih et al., [2019](#bib.bib33)) |  |  |  | ✓ |  | ✓ | ✓ | Graph
    WaveNet(Wu et al., [2019](#bib.bib39)) |  | ✓ |  |  |  | ✓ |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| TPA-LSTM(Shih et al., [2019](#bib.bib33)) |  |  |  | ✓ |  | ✓ | ✓ | Graph
    WaveNet(Wu et al., [2019](#bib.bib39)) |  | ✓ |  |  |  | ✓ |  |'
- en: '| Transformer(Li et al., [2019](#bib.bib23)) |  |  |  |  |  |  | ✓ | MTGNN(Wu
    et al., [2020](#bib.bib38)) |  | ✓ |  |  |  | ✓ |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Transformer(Li et al., [2019](#bib.bib23)) |  |  |  |  |  |  | ✓ | MTGNN(Wu
    et al., [2020](#bib.bib38)) |  | ✓ |  |  |  | ✓ |  |'
- en: '| ST-MetaNet(Pan et al., [2019](#bib.bib31)) |  |  | ✓ |  | ✓ |  |  | STGNN(Wang
    et al., [2020](#bib.bib37)) |  | ✓ | ✓ |  | ✓ |  | ✓ |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| ST-MetaNet(Pan et al., [2019](#bib.bib31)) |  |  | ✓ |  | ✓ |  |  | STGNN(Wang
    et al., [2020](#bib.bib37)) |  | ✓ | ✓ |  | ✓ |  | ✓ |'
- en: '| GMAN(Zheng et al., [2020](#bib.bib54)) |  |  | ✓ |  |  |  | ✓ | AGCRN(Bai
    et al., [2020](#bib.bib4)) |  | ✓ |  |  | ✓ |  |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| GMAN（郑等，[2020](#bib.bib54)） |  |  | ✓ |  |  |  | ✓ | AGCRN（白等，[2020](#bib.bib4)）
    |  | ✓ |  |  | ✓ |  |  |'
- en: 4\. Model
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 模型
- en: 'Complex spatial and temporal dependencies are the key challenges in urban traffic
    prediction tasks. Temporally, future prediction depends on the recent observations
    as well as the past periodical patterns; Spatially, the traffic states in certain
    mesh-grid or graph-node are affected by the nearby ones as well as distant ones.
    To capture the temporal dependency, LSTM(Hochreiter and Schmidhuber, [1997](#bib.bib17))
    and its simplified variant GRU(Chung et al., [2014](#bib.bib8)) are respectively
    utilized by the models as shown in Table [3](#S3.T3 "Table 3 ‣ 3.2\. Graph-Based
    Traffic Dataset ‣ 3\. Dataset ‣ DL-Traff: Survey and Benchmark of Deep Learning
    Models for Urban Traffic Prediction"). In parallel with the RNNs, 1D CNN and its
    enhanced version TCN (Yu and Koltun, [2016](#bib.bib44)) are also employed as
    the core technology for temporal modeling, and demonstrate the superior time efficiency
    and matchable effectiveness to LSTM and GRU.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '复杂的空间和时间依赖性是城市交通预测任务中的关键挑战。从时间上看，未来预测依赖于近期观察结果以及过去的周期性模式；从空间上看，某个网格或图节点的交通状态受到附近及远处节点的影响。为了捕捉时间依赖性，LSTM（Hochreiter
    和 Schmidhuber，[1997](#bib.bib17)）及其简化变体 GRU（Chung 等，[2014](#bib.bib8)）被模型分别利用，如表[3](#S3.T3
    "Table 3 ‣ 3.2\. Graph-Based Traffic Dataset ‣ 3\. Dataset ‣ DL-Traff: Survey
    and Benchmark of Deep Learning Models for Urban Traffic Prediction")所示。与 RNNs
    并行，1D CNN 及其增强版 TCN（Yu 和 Koltun，[2016](#bib.bib44)）也被用作时间建模的核心技术，并展现了优越的时间效率和与
    LSTM 及 GRU 相匹配的效果。'
- en: On the other hand, to capture the spatial dependency, grid-based models simply
    use the normal convolution operation(LeCun et al., [1998](#bib.bib22)) thanks
    to the natural euclidean property of grid spacing; graph-based models leverage
    the graph convolution in non-euclidean space (Defferrard et al., [2016](#bib.bib11);
    Kipf and Welling, [2017](#bib.bib20)) by involving the adjacency relation $A\in$
    $\mathbb{R}^{N*N}$ between each pair of spatial units. Meanwhile, attention mechanism(Vaswani
    et al., [2017](#bib.bib35)) also known as Transformer has rapidly taken over the
    AI community from natural language (GPT-3) to vision since 2020\. Thus, attention
    is also introduced as base technology for modeling both spatial and temporal dependencies.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，为了捕捉空间依赖性，基于网格的模型由于网格间距的自然欧几里得属性，简单地使用常规卷积操作（LeCun 等，[1998](#bib.bib22)）；基于图的模型则利用非欧几里得空间中的图卷积（Defferrard
    等，[2016](#bib.bib11)；Kipf 和 Welling，[2017](#bib.bib20)），通过涉及每对空间单元之间的邻接关系 $A\in$
    $\mathbb{R}^{N*N}$。与此同时，自注意力机制（Vaswani 等，[2017](#bib.bib35)），也称为 Transformer，自
    2020 年以来已迅速在 AI 社区从自然语言（GPT-3）到视觉领域取得了突破。因此，自注意力机制也被引入作为建模空间和时间依赖性的基础技术。
- en: 'We select the most representative models in Table [1](#S0.T1 "Table 1 ‣ DL-Traff:
    Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction") and
    summarize the base technologies employed by each model for spatial and temporal
    modeling as Table [3](#S3.T3 "Table 3 ‣ 3.2\. Graph-Based Traffic Dataset ‣ 3\.
    Dataset ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic
    Prediction"). On the other hand, for better understanding, we simplify and plot
    the network architectures in a unified manner for five grid-based models including
    ST-ResNet(Zhang et al., [2017](#bib.bib48)), DMVST-Net(Yao et al., [2018](#bib.bib42)),
    Periodic-CRN(PCRN)(Zonoozi et al., [2018](#bib.bib55)), STDN(Yao et al., [2019](#bib.bib41)),
    and DeepSTN+(Lin et al., [2019](#bib.bib26)) as Fig.[3](#S4.F3 "Figure 3 ‣ 4.1\.
    Roadmap for Grid-Based Model ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark of Deep
    Learning Models for Urban Traffic Prediction"), and five graph-based models, namely
    STGCN(Yu et al., [2018](#bib.bib43)), DCRNN(Li et al., [2018](#bib.bib24)), Graph
    WaveNet(Wu et al., [2019](#bib.bib39)), ASTGCN(Guo et al., [2019](#bib.bib15)),
    and GMAN(Zheng et al., [2020](#bib.bib54)) as Fig.[4](#S4.F4 "Figure 4 ‣ 4.1\.
    Roadmap for Grid-Based Model ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark of Deep
    Learning Models for Urban Traffic Prediction"). Through Fig.[3](#S4.F3 "Figure
    3 ‣ 4.1\. Roadmap for Grid-Based Model ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark
    of Deep Learning Models for Urban Traffic Prediction")$\sim$[4](#S4.F4 "Figure
    4 ‣ 4.1\. Roadmap for Grid-Based Model ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark
    of Deep Learning Models for Urban Traffic Prediction"), we can easily understand
    how the spatial and temporal modules listed in Table [3](#S3.T3 "Table 3 ‣ 3.2\.
    Graph-Based Traffic Dataset ‣ 3\. Dataset ‣ DL-Traff: Survey and Benchmark of
    Deep Learning Models for Urban Traffic Prediction") are assembled to form an integrated
    model.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '我们选择了表格[1](#S0.T1 "Table 1 ‣ DL-Traff: Survey and Benchmark of Deep Learning
    Models for Urban Traffic Prediction")中的最具代表性的模型，并总结了每个模型在空间和时间建模中使用的基础技术，如表格[3](#S3.T3
    "Table 3 ‣ 3.2\. Graph-Based Traffic Dataset ‣ 3\. Dataset ‣ DL-Traff: Survey
    and Benchmark of Deep Learning Models for Urban Traffic Prediction")所示。另一方面，为了更好地理解，我们将包括ST-ResNet(Zhang
    et al., [2017](#bib.bib48))、DMVST-Net(Yao et al., [2018](#bib.bib42))、Periodic-CRN(PCRN)(Zonoozi
    et al., [2018](#bib.bib55))、STDN(Yao et al., [2019](#bib.bib41))和DeepSTN+(Lin
    et al., [2019](#bib.bib26))在内的五种基于网格的模型的网络架构统一简化并绘制，如图[3](#S4.F3 "Figure 3 ‣ 4.1\.
    Roadmap for Grid-Based Model ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark of Deep
    Learning Models for Urban Traffic Prediction")所示，以及五种基于图的模型，即STGCN(Yu et al.,
    [2018](#bib.bib43))、DCRNN(Li et al., [2018](#bib.bib24))、Graph WaveNet(Wu et al.,
    [2019](#bib.bib39))、ASTGCN(Guo et al., [2019](#bib.bib15))和GMAN(Zheng et al.,
    [2020](#bib.bib54))，如图[4](#S4.F4 "Figure 4 ‣ 4.1\. Roadmap for Grid-Based Model
    ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for Urban
    Traffic Prediction")所示。通过图[3](#S4.F3 "Figure 3 ‣ 4.1\. Roadmap for Grid-Based
    Model ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for
    Urban Traffic Prediction")$\sim$[4](#S4.F4 "Figure 4 ‣ 4.1\. Roadmap for Grid-Based
    Model ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for
    Urban Traffic Prediction")，我们可以轻松理解表格[3](#S3.T3 "Table 3 ‣ 3.2\. Graph-Based Traffic
    Dataset ‣ 3\. Dataset ‣ DL-Traff: Survey and Benchmark of Deep Learning Models
    for Urban Traffic Prediction")中列出的空间和时间模块如何组装成一个集成模型。'
- en: Moreover, we describe how the employed technologies are evolving along the spatial
    and temporal axis for both grid-based and graph-based models in the next two subsections.
    Note that the multivariate time-series (MTS) models such as LSTNet(Lai et al.,
    [2018](#bib.bib21)), TPA-LSTM(Shih et al., [2019](#bib.bib33)), GeoMAN(Liang et al.,
    [2018](#bib.bib25)), and Transformer(Li et al., [2019](#bib.bib23)) are also gradually
    evolving along the spatial and temporal axis. From the spatial perspective, they
    focus on correlation/dependence between variates; from the temporal perspective,
    they aim to utilize the periodic patterns occurred in time series. But due to
    space limitations, we don’t expand the details of those MTS models in this paper.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在接下来的两个小节中描述了所采用技术在空间和时间轴上的演变情况，包括网格模型和图模型。需要注意的是，多变量时间序列（MTS）模型，如LSTNet(Lai
    et al., [2018](#bib.bib21))、TPA-LSTM(Shih et al., [2019](#bib.bib33))、GeoMAN(Liang
    et al., [2018](#bib.bib25))和Transformer(Li et al., [2019](#bib.bib23))也在逐渐沿着空间和时间轴演变。从空间角度来看，它们关注于变数之间的相关性/依赖性；从时间角度来看，它们旨在利用时间序列中的周期性模式。但由于篇幅限制，本文不详细扩展这些MTS模型的细节。
- en: 4.1\. Roadmap for Grid-Based Model
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 基于网格模型的路线图
- en: 'ST-ResNet(Zhang et al., [2017](#bib.bib48)) is the earliest and the most representative
    grid-based deep learning method for traffic in-out flow prediction. It converts
    4D tensor ($T$,$H$,$W$,$C$) into 3D tensor ($H$,$W$,$T$*$C$) by concatenating
    the channels at each time step so that CNN can be used to capture spatial dependency
    similarly to an image. Then, it creatively proposes a set of temporal features
    called $Closeness$, $Period$, and $Trend$, which correspond to *the most recent
    observations*, *daily periodicity*, and *weekly trend* respectively. Intuitively,
    the three parts of the features can be represented by:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ST-ResNet（Zhang等，[2017](#bib.bib48)）是最早且最具代表性的网格基深度学习方法，用于交通流量预测。它通过在每个时间步连接通道，将4D张量（$T$,$H$,$W$,$C$）转换为3D张量（$H$,$W$,$T$*$C$），以便CNN可以像处理图像一样捕捉空间依赖性。然后，它创造性地提出了一组称为$Closeness$、$Period$和$Trend$的时间特征，分别对应于*最新的观察数据*、*每日周期性*和*每周趋势*。直观地，这三部分特征可以表示为：
- en: $X^{Closeness}$ = [$X_{t-l_{c}}$, $X_{t-(l_{c}-1)}$, …, $X_{t-1}$]
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: $X^{Closeness}$ = [$X_{t-l_{c}}$, $X_{t-(l_{c}-1)}$, …, $X_{t-1}$]
- en: $X^{Period}$ = [$X_{t-l_{p}\times s_{p}}$, $X_{t-(l_{p}-1)\times s_{p}}$, …,
    $X_{t-s_{p}}$]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: $X^{Period}$ = [$X_{t-l_{p}\times s_{p}}$, $X_{t-(l_{p}-1)\times s_{p}}$, …,
    $X_{t-s_{p}}$]
- en: $X^{Trend}$ = [$X_{t-l_{q}\times s_{q}}$, $X_{t-(l_{q}-1)\times s_{q}}$, …,
    $X_{t-s_{q}}$]
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: $X^{Trend}$ = [$X_{t-l_{q}\times s_{q}}$, $X_{t-(l_{q}-1)\times s_{q}}$, …,
    $X_{t-s_{q}}$]
- en: where $l_{c}$, $l_{p}$, $l_{q}$ are the sequence length of {$Closeness$, $Period$,
    $Trend$}, $s_{p}$ and $s_{q}$ are the time span of $Period$ and $Trend$, the $Closeness$
    span $s_{c}$ is equal to 1 by default. This feature is not only inherited by the
    later grid-based models including STDN(Yao et al., [2019](#bib.bib41)) and DeepSTN+(Lin
    et al., [2019](#bib.bib26)), but also some graph-based models like ASTGCN(Guo
    et al., [2019](#bib.bib15)), which is still regarded as the state-of-the-art temporal
    feature by now. To capture the long-range spatial dependency between mesh-grids,
    it employs Residual Learning to construct deep enough CNN networks. Additionally,
    it further utilizes external information including weather, event, and metadata(i.e.
    DayOfWeek, WeekdayOrWeekend) to auxiliarily enhance spatiotemporal modeling.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$l_{c}$、$l_{p}$、$l_{q}$分别是{$Closeness$、$Period$、$Trend$}的序列长度，$s_{p}$和$s_{q}$是$Period$和$Trend$的时间跨度，$Closeness$的跨度$s_{c}$默认为1。这个特征不仅被后来的网格基模型如STDN（Yao等，[2019](#bib.bib41)）和DeepSTN+（Lin等，[2019](#bib.bib26)）继承，还被一些图基模型如ASTGCN（Guo等，[2019](#bib.bib15)）所采用，目前仍被视为最先进的时间特征。为了捕捉网格间的长距离空间依赖性，它采用了Residual
    Learning来构建足够深的CNN网络。此外，它还进一步利用外部信息，包括天气、事件和元数据（即DayOfWeek、WeekdayOrWeekend），来辅助增强时空建模。
- en: '![Refer to caption](img/6e6ce6d92e30d7357ffc6d4b17ad02be.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/6e6ce6d92e30d7357ffc6d4b17ad02be.png)'
- en: Figure 3\. Architectures of Representative Grid-Based Models.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 代表性网格基模型的架构。
- en: '![Refer to caption](img/b39c0c8877a196a867d41ee3252ebb62.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/b39c0c8877a196a867d41ee3252ebb62.png)'
- en: Figure 4\. Architectures of Representative Graph-Based Models.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 代表性图基模型的架构。
- en: Improvement along Spatial Axis. Different from ST-ResNet that takes the entire
    mesh-grids as input, DMVST-Net(Yao et al., [2018](#bib.bib42)) and STDN(Yao et al.,
    [2019](#bib.bib41)) take one grid and its surrounding grids (i.e. $S$$\times$$S$
    region) as input, thus a local CNN is enough to capture spatial dependency only
    among nearby grids. For the global spatial dependency, DMVST-Net introduces a
    weighted graph as an extra input, where nodes are the grids, and each edge represents
    the similarity of two time-series values (i.e. historical taxi demand) between
    any two grids. The graph will be manually embedded into a feature vector so that
    it can be concatenated with the other part. Through this, DMVST-Net gains the
    ability to capture long-range spatial dependency. Furthermore, STDN and (Zhang
    et al., [2019](#bib.bib50); Jiang et al., [2019](#bib.bib19)) consider the local
    flow information (i.e. flow from one central grid to its surrounding $S$$\times$$S$
    grids) to facilitate predicting the traffic volume in the central grid, which
    is implemented with a flow gating mechanism in STDN and multitask learning in
    (Zhang et al., [2019](#bib.bib50); Jiang et al., [2019](#bib.bib19)). DeepSTN+
    (Lin et al., [2019](#bib.bib26)) uses Point-Of-Interest (POI) data as external
    information (e.g., office/residential/shopping area) to take the influence of
    location function on the crowd/traffic flow into consideration.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 空间轴的改进。与 ST-ResNet 使用整个网格作为输入不同，DMVST-Net（Yao 等，[2018](#bib.bib42)）和 STDN（Yao
    等，[2019](#bib.bib41)）仅使用一个网格及其周围网格（即 $S$$\times$$S$ 区域）作为输入，因此一个局部 CNN 足以捕捉仅在附近网格之间的空间依赖性。对于全局空间依赖性，DMVST-Net
    引入了一个加权图作为额外输入，其中节点为网格，每个边表示任意两个网格之间两个时间序列值（即历史出租车需求）的相似性。该图将被手动嵌入到特征向量中，以便与其他部分进行连接。通过这种方式，DMVST-Net
    获得了捕捉长距离空间依赖性的能力。此外，STDN 和（Zhang 等，[2019](#bib.bib50)；Jiang 等，[2019](#bib.bib19)）考虑了局部流信息（即从一个中心网格到其周围
    $S$$\times$$S$ 网格的流量），以促进对中心网格交通量的预测，这在 STDN 中通过流量门控机制实现，在（Zhang 等，[2019](#bib.bib50)；Jiang
    等，[2019](#bib.bib19)）中通过多任务学习实现。DeepSTN+（Lin 等，[2019](#bib.bib26)）使用兴趣点（POI）数据作为外部信息（例如办公室/住宅/购物区），以考虑位置功能对人群/交通流的影响。
- en: Improvement along Temporal Axis. One major drawback of ST-ResNet is it does
    not explicitly handle the temporal axis, because it forces the video-like tensor
    ($T$,$H$,$W$,$C$) to be converted into an image-like tensor ($H$,$W$,$T$*$C$).
    To address this, DMVST-Net and STDN employ LSTM to connect with a separate and
    unshared CNN for each timestamp. STDN further considers the temporal shifting
    problem about periodicity (i.e. traffic data is not strictly periodic) and designs
    a *Periodically Shifted Attention Mechanism* to solve the issue. Specifically,
    it sets a small time window to collect $Q$ time intervals right before and after
    the currently-predicting one. And the attention is used to obtain a weighted average
    representation $h$ from the $Q$ representations {$h_{1}$, $h_{2}$, $...$, $h_{Q}$}
    generated by LSTM. To this end, LSTM, and CNN work together to separately and
    sequentially model the spatial and temporal dependency. Convolutional LSTM (Xingjian
    et al., [2015](#bib.bib40)) extends the fully connected LSTM (FC-LSTM) to have
    convolutional structures in both the input-to-state and state-to-state transitions
    and achieves a lot of successes on video modeling tasks. Motivated by this, ConvLSTM
    and its variant ConvGRU are utilized by (Zonoozi et al., [2018](#bib.bib55); Yuan
    et al., [2018](#bib.bib46); Jiang et al., [2019](#bib.bib19)) to simultaneously
    capture the spatial and temporal dependency.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 时间轴的改进。ST-ResNet 的一个主要缺点是它没有明确处理时间轴，因为它强制将类似视频的张量（$T$，$H$，$W$，$C$）转换为类似图像的张量（$H$，$W$，$T$*$C$）。为了解决这个问题，DMVST-Net
    和 STDN 使用 LSTM 与每个时间戳的独立且不共享的 CNN 连接。STDN 进一步考虑了周期性（即交通数据并不严格周期性）的时间偏移问题，并设计了
    *周期性偏移注意机制* 来解决该问题。具体而言，它设置了一个小时间窗口，以收集当前预测时间前后 $Q$ 个时间间隔的数据。然后，注意力机制用于从 LSTM
    生成的 $Q$ 个表示 {$h_{1}$，$h_{2}$，$...$，$h_{Q}$} 中获得加权平均表示 $h$。为此，LSTM 和 CNN 共同工作，分别且顺序地建模空间和时间依赖性。卷积
    LSTM（Xingjian 等，[2015](#bib.bib40)）将全连接 LSTM（FC-LSTM）扩展为在输入到状态和状态到状态转换中都具有卷积结构，并在视频建模任务上取得了许多成功。受到此启发，（Zonoozi
    等，[2018](#bib.bib55)；Yuan 等，[2018](#bib.bib46)；Jiang 等，[2019](#bib.bib19)）利用 ConvLSTM
    及其变体 ConvGRU 来同时捕捉空间和时间依赖性。
- en: Table 4\. Performance Evaluation for Single-Step Prediction on Grid-Based Traffic
    Datasets
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 基于网格的交通数据集单步预测性能评估
- en: '|  | TaxiBJ | BikeNYC-I | BikeNYC-II | TaxiNYC |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | TaxiBJ | BikeNYC-I | BikeNYC-II | TaxiNYC |'
- en: '| Model | RMSE | MAE | MAPE | RMSE | MAE | MAPE | RMSE | MAE | MAPE | RMSE
    | MAE | MAPE |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | RMSE | MAE | MAPE | RMSE | MAE | MAPE | RMSE | MAE | MAPE | RMSE | MAE
    | MAPE |'
- en: '| HistoricalAverage | 45.004 | 24.475 | 8.04% | 15.676 | 4.882 | 5.45% | 4.874
    | 1.500 | 3.30% | 21.535 | 7.121 | 4.56% |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 历史平均 | 45.004 | 24.475 | 8.04% | 15.676 | 4.882 | 5.45% | 4.874 | 1.500 |
    3.30% | 21.535 | 7.121 | 4.56% |'
- en: '| CopyLastStep | 23.609 | 13.372 | 6.20% | 14.152 | 4.344 | 5.01% | 4.999 |
    1.606 | 3.50% | 18.660 | 6.497 | 4.91% |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| CopyLastStep | 23.609 | 13.372 | 6.20% | 14.152 | 4.344 | 5.01% | 4.999 |
    1.606 | 3.50% | 18.660 | 6.497 | 4.91% |'
- en: '| CNN(LeCun et al., [1998](#bib.bib22)) | 23.550 | 13.797 | 8.46% | 12.064
    | 4.088 | 5.82% | 4.511 | 1.574 | 3.98% | 16.741 | 6.884 | 8.08% |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| CNN(LeCun et al., [1998](#bib.bib22)) | 23.550 | 13.797 | 8.46% | 12.064
    | 4.088 | 5.82% | 4.511 | 1.574 | 3.98% | 16.741 | 6.884 | 8.08% |'
- en: '| ConvLSTM(Xingjian et al., [2015](#bib.bib40)) | 19.247 | 10.816 | 5.61% |
    6.616 | 2.412 | 3.90% | 3.174 | 1.133 | 2.90% | 12.143 | 4.811 | 5.16% |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| ConvLSTM(Xingjian et al., [2015](#bib.bib40)) | 19.247 | 10.816 | 5.61% |
    6.616 | 2.412 | 3.90% | 3.174 | 1.133 | 2.90% | 12.143 | 4.811 | 5.16% |'
- en: '| ST-ResNet(Zhang et al., [2017](#bib.bib48)) | 18.702 | 10.493 | 5.19% | 6.106
    | 2.360 | 3.72% | 3.191 | 1.169 | 2.86% | 11.553 | 4.535 | 4.32% |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| ST-ResNet(Zhang et al., [2017](#bib.bib48)) | 18.702 | 10.493 | 5.19% | 6.106
    | 2.360 | 3.72% | 3.191 | 1.169 | 2.86% | 11.553 | 4.535 | 4.32% |'
- en: '| DMVST-Net(Yao et al., [2018](#bib.bib42)) | 20.389 | 11.832 | 5.99% | 7.990
    | 2.833 | 3.93% | 3.521 | 1.287 | 2.97% | 13.605 | 4.928 | 4.49% |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| DMVST-Net(Yao et al., [2018](#bib.bib42)) | 20.389 | 11.832 | 5.99% | 7.990
    | 2.833 | 3.93% | 3.521 | 1.287 | 2.97% | 13.605 | 4.928 | 4.49% |'
- en: '| PCRN(Zonoozi et al., [2018](#bib.bib55)) | 18.629 | 10.432 | 5.45% | 6.680
    | 2.351 | 3.63% | 3.149 | 1.107 | 2.78% | 12.027 | 4.606 | 4.62% |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| PCRN(Zonoozi et al., [2018](#bib.bib55)) | 18.629 | 10.432 | 5.45% | 6.680
    | 2.351 | 3.63% | 3.149 | 1.107 | 2.78% | 12.027 | 4.606 | 4.62% |'
- en: '| DeepSTN+(Lin et al., [2019](#bib.bib26)) | 18.141 | 10.126 | 5.14% | 6.205
    | 2.489 | 3.48% | 3.205 | 1.245 | 2.80% | 11.420 | 4.441 | 4.45% |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| DeepSTN+(Lin et al., [2019](#bib.bib26)) | 18.141 | 10.126 | 5.14% | 6.205
    | 2.489 | 3.48% | 3.205 | 1.245 | 2.80% | 11.420 | 4.441 | 4.45% |'
- en: '| STDN(Yao et al., [2019](#bib.bib41)) | 17.826 | 9.901 | 4.81% | 5.783 | 2.410
    | 3.35% | 3.004 | 1.167 | 2.67% | 11.252 | 4.474 | 4.09% |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| STDN(Yao et al., [2019](#bib.bib41)) | 17.826 | 9.901 | 4.81% | 5.783 | 2.410
    | 3.35% | 3.004 | 1.167 | 2.67% | 11.252 | 4.474 | 4.09% |'
- en: Table 5\. Performance Evaluation for Multi-Step Prediction on Graph-Based Traffic
    Datasets
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 图基交通数据集多步预测的性能评估
- en: '|  |  | 3 Steps / 15 Minutes Ahead | 6 Steps / 30 Minutes Ahead | 12 Steps
    / 60 Minutes Ahead |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 3 Steps / 15 Minutes Ahead | 6 Steps / 30 Minutes Ahead | 12 Steps
    / 60 Minutes Ahead |'
- en: '| Dataset | Model | RMSE | MAE | MAPE | RMSE | MAE | MAPE | RMSE | MAE | MAPE
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 模型 | RMSE | MAE | MAPE | RMSE | MAE | MAPE | RMSE | MAE | MAPE |'
- en: '| METR-LA | HistoricalAverage | 14.737 | 11.013 | 23.34% | 14.737 | 11.010
    | 23.34% | 14.736 | 11.005 | 23.33% |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| METR-LA | 历史平均 | 14.737 | 11.013 | 23.34% | 14.737 | 11.010 | 23.34% | 14.736
    | 11.005 | 23.33% |'
- en: '| CopyLastSteps | 14.215 | 6.799 | 16.73% | 14.214 | 6.799 | 16.73% | 14.214
    | 6.798 | 16.72% |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| CopyLastSteps | 14.215 | 6.799 | 16.73% | 14.214 | 6.799 | 16.73% | 14.214
    | 6.798 | 16.72% |'
- en: '| LSTNet(Lai et al., [2018](#bib.bib21)) | 8.067 | 3.914 | 9.27% | 10.181 |
    5.219 | 12.22% | 11.890 | 6.335 | 15.38% |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| LSTNet(Lai et al., [2018](#bib.bib21)) | 8.067 | 3.914 | 9.27% | 10.181 |
    5.219 | 12.22% | 11.890 | 6.335 | 15.38% |'
- en: '| STGCN(Yu et al., [2018](#bib.bib43)) | 7.918 | 3.469 | 8.57% | 9.948 | 4.263
    | 10.70% | 11.813 | 5.079 | 13.09% |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| STGCN(Yu et al., [2018](#bib.bib43)) | 7.918 | 3.469 | 8.57% | 9.948 | 4.263
    | 10.70% | 11.813 | 5.079 | 13.09% |'
- en: '| DCRNN(Li et al., [2018](#bib.bib24)) | 7.509 | 3.261 | 8.00% | 9.543 | 4.021
    | 10.12% | 11.854 | 5.080 | 13.08% |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| DCRNN(Li et al., [2018](#bib.bib24)) | 7.509 | 3.261 | 8.00% | 9.543 | 4.021
    | 10.12% | 11.854 | 5.080 | 13.08% |'
- en: '| Graph WaveNet(Wu et al., [2019](#bib.bib39)) | 7.512 | 3.204 | 7.62% | 9.445
    | 3.922 | 9.52% | 11.485 | 4.848 | 11.93% |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Graph WaveNet(Wu et al., [2019](#bib.bib39)) | 7.512 | 3.204 | 7.62% | 9.445
    | 3.922 | 9.52% | 11.485 | 4.848 | 11.93% |'
- en: '| ASTGCN(Guo et al., [2019](#bib.bib15)) | 7.977 | 3.624 | 9.13% | 10.042 |
    4.514 | 11.57% | 12.092 | 5.776 | 14.85% |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| ASTGCN(Guo et al., [2019](#bib.bib15)) | 7.977 | 3.624 | 9.13% | 10.042 |
    4.514 | 11.57% | 12.092 | 5.776 | 14.85% |'
- en: '| GMAN(Zheng et al., [2020](#bib.bib54)) | 8.869 | 4.139 | 10.88% | 9.917 |
    4.517 | 11.77% | 11.910 | 5.475 | 14.10% |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| GMAN(Zheng et al., [2020](#bib.bib54)) | 8.869 | 4.139 | 10.88% | 9.917 |
    4.517 | 11.77% | 11.910 | 5.475 | 14.10% |'
- en: '|  | MTGNN(Wu et al., [2020](#bib.bib38)) | 7.707 | 3.277 | 8.02% | 9.625 |
    3.999 | 10.00% | 11.624 | 4.867 | 12.17% |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | MTGNN(Wu et al., [2020](#bib.bib38)) | 7.707 | 3.277 | 8.02% | 9.625 |
    3.999 | 10.00% | 11.624 | 4.867 | 12.17% |'
- en: '|  | AGCRN(Bai et al., [2020](#bib.bib4)) | 7.558 | 3.292 | 8.17% | 9.499 |
    4.016 | 10.16% | 11.502 | 4.901 | 12.43% |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | AGCRN(Bai et al., [2020](#bib.bib4)) | 7.558 | 3.292 | 8.17% | 9.499 |
    4.016 | 10.16% | 11.502 | 4.901 | 12.43% |'
- en: '| PeMS-BAY | HistoricalAverage | 6.687 | 3.333 | 8.10% | 6.686 | 3.333 | 8.10%
    | 6.685 | 3.332 | 8.10% |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| PeMS-BAY | 历史平均 | 6.687 | 3.333 | 8.10% | 6.686 | 3.333 | 8.10% | 6.685 |
    3.332 | 8.10% |'
- en: '| CopyLastSteps | 7.022 | 3.052 | 6.84% | 7.016 | 3.049 | 6.84% | 7.05 | 3.044
    | 6.83% |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| CopyLastSteps | 7.022 | 3.052 | 6.84% | 7.016 | 3.049 | 6.84% | 7.05 | 3.044
    | 6.83% |'
- en: '| LSTNet(Lai et al., [2018](#bib.bib21)) | 3.224 | 1.643 | 3.47% | 4.375 |
    2.383 | 5.04% | 5.515 | 2.974 | 6.86% |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| LSTNet（Lai 等，[2018](#bib.bib21)） | 3.224 | 1.643 | 3.47% | 4.375 | 2.383
    | 5.04% | 5.515 | 2.974 | 6.86% |'
- en: '| STGCN(Yu et al., [2018](#bib.bib43)) | 2.827 | 1.327 | 2.79% | 3.887 | 1.698
    | 3.81% | 4.748 | 2.055 | 5.02% |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| STGCN（Yu 等，[2018](#bib.bib43)） | 2.827 | 1.327 | 2.79% | 3.887 | 1.698 |
    3.81% | 4.748 | 2.055 | 5.02% |'
- en: '| DCRNN(Li et al., [2018](#bib.bib24)) | 2.867 | 1.377 | 2.96% | 3.905 | 1.726
    | 3.97% | 4.798 | 2.091 | 4.99% |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| DCRNN（Li 等，[2018](#bib.bib24)） | 2.867 | 1.377 | 2.96% | 3.905 | 1.726 |
    3.97% | 4.798 | 2.091 | 4.99% |'
- en: '| Graph WaveNet(Wu et al., [2019](#bib.bib39)) | 2.759 | 1.322 | 2.78% | 3.737
    | 1.660 | 3.75% | 4.562 | 1.991 | 4.75% |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Graph WaveNet（Wu 等，[2019](#bib.bib39)） | 2.759 | 1.322 | 2.78% | 3.737 |
    1.660 | 3.75% | 4.562 | 1.991 | 4.75% |'
- en: '| ASTGCN(Guo et al., [2019](#bib.bib15)) | 3.057 | 1.435 | 3.25% | 4.066 |
    1.795 | 4.40% | 4.770 | 2.103 | 5.30% |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| ASTGCN（Guo 等，[2019](#bib.bib15)） | 3.057 | 1.435 | 3.25% | 4.066 | 1.795
    | 4.40% | 4.770 | 2.103 | 5.30% |'
- en: '| GMAN(Zheng et al., [2020](#bib.bib54)) | 4.219 | 1.802 | 4.47% | 4.143 |
    1.794 | 4.40% | 5.034 | 2.186 | 5.29% |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| GMAN（Zheng 等，[2020](#bib.bib54)） | 4.219 | 1.802 | 4.47% | 4.143 | 1.794
    | 4.40% | 5.034 | 2.186 | 5.29% |'
- en: '|  | MTGNN(Wu et al., [2020](#bib.bib38)) | 2.849 | 1.334 | 2.84% | 3.800 |
    1.658 | 3.77% | 4.491 | 1.950 | 4.59% |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | MTGNN（Wu 等，[2020](#bib.bib38)） | 2.849 | 1.334 | 2.84% | 3.800 | 1.658
    | 3.77% | 4.491 | 1.950 | 4.59% |'
- en: '|  | AGCRN(Bai et al., [2020](#bib.bib4)) | 2.856 | 1.354 | 2.94% | 3.818 |
    1.670 | 3.84% | 4.570 | 1.964 | 4.69% |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | AGCRN（Bai 等，[2020](#bib.bib4)） | 2.856 | 1.354 | 2.94% | 3.818 | 1.670
    | 3.84% | 4.570 | 1.964 | 4.69% |'
- en: '| PEMSD7M | HistoricalAverage | 7.077 | 3.917 | 9.90% | 7.083 | 3.920 | 9.92%
    | 7.095 | 3.925 | 9.95% |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| PEMSD7M | HistoricalAverage | 7.077 | 3.917 | 9.90% | 7.083 | 3.920 | 9.92%
    | 7.095 | 3.925 | 9.95% |'
- en: '| CopyLastSteps | 9.591 | 5.021 | 12.33% | 9.594 | 5.022 | 12.33% | 9.597 |
    5.024 | 12.34% |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| CopyLastSteps | 9.591 | 5.021 | 12.33% | 9.594 | 5.022 | 12.33% | 9.597 |
    5.024 | 12.34% |'
- en: '| LSTNet(Lai et al., [2018](#bib.bib21)) | 4.308 | 2.423 | 5.73% | 8.951 |
    5.132 | 12.22% | 10.881 | 6.624 | 16.72% |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| LSTNet（Lai 等，[2018](#bib.bib21)） | 4.308 | 2.423 | 5.73% | 8.951 | 5.132
    | 12.22% | 10.881 | 6.624 | 16.72% |'
- en: '| STGCN(Yu et al., [2018](#bib.bib43)) | 4.051 | 2.124 | 5.02% | 5.532 | 2.783
    | 6.96% | 6.695 | 3.374 | 8.74% |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| STGCN（Yu 等，[2018](#bib.bib43)） | 4.051 | 2.124 | 5.02% | 5.532 | 2.783 |
    6.96% | 6.695 | 3.374 | 8.74% |'
- en: '| DCRNN(Li et al., [2018](#bib.bib24)) | 4.143 | 2.213 | 5.33% | 5.679 | 2.907
    | 7.41% | 7.138 | 3.670 | 9.81% |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| DCRNN（Li 等，[2018](#bib.bib24)） | 4.143 | 2.213 | 5.33% | 5.679 | 2.907 |
    7.41% | 7.138 | 3.670 | 9.81% |'
- en: '| Graph WaveNet(Wu et al., [2019](#bib.bib39)) | 3.992 | 2.130 | 5.00% | 5.332
    | 2.715 | 6.75% | 6.431 | 3.266 | 8.47% |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| Graph WaveNet（Wu 等，[2019](#bib.bib39)） | 3.992 | 2.130 | 5.00% | 5.332 |
    2.715 | 6.75% | 6.431 | 3.266 | 8.47% |'
- en: '| ASTGCN(Guo et al., [2019](#bib.bib15)) | 4.257 | 2.340 | 5.83% | 5.506 |
    2.992 | 7.69% | 6.587 | 3.572 | 9.48% |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| ASTGCN（Guo 等，[2019](#bib.bib15)） | 4.257 | 2.340 | 5.83% | 5.506 | 2.992
    | 7.69% | 6.587 | 3.572 | 9.48% |'
- en: '| GMAN(Zheng et al., [2020](#bib.bib54)) | 5.711 | 2.877 | 7.25% | 6.171 |
    3.084 | 7.77% | 7.897 | 3.988 | 10.02% |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| GMAN（Zheng 等，[2020](#bib.bib54)） | 5.711 | 2.877 | 7.25% | 6.171 | 3.084
    | 7.77% | 7.897 | 3.988 | 10.02% |'
- en: '|  | MTGNN(Wu et al., [2020](#bib.bib38)) | 4.032 | 2.120 | 5.02% | 5.373 |
    2.687 | 6.70% | 6.496 | 3.204 | 8.24% |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | MTGNN（Wu 等，[2020](#bib.bib38)） | 4.032 | 2.120 | 5.02% | 5.373 | 2.687
    | 6.70% | 6.496 | 3.204 | 8.24% |'
- en: '|  | AGCRN(Bai et al., [2020](#bib.bib4)) | 4.073 | 2.167 | 5.19% | 5.479 |
    2.769 | 6.89% | 6.733 | 3.358 | 8.55% |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | AGCRN（Bai 等，[2020](#bib.bib4)） | 4.073 | 2.167 | 5.19% | 5.479 | 2.769
    | 6.89% | 6.733 | 3.358 | 8.55% |'
- en: 4.2\. Roadmap for Graph-Based Model
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 基于图的模型路线图
- en: 'STGCN(Yu et al., [2018](#bib.bib43)) is one of the earliest models that use
    graph neural networks to predict traffic flow. Temporally, instead of RNN, it
    uses TCN (Yu and Koltun, [2016](#bib.bib44)) with a gated mechanism as shown in
    Fig.[4](#S4.F4 "Figure 4 ‣ 4.1\. Roadmap for Grid-Based Model ‣ 4\. Model ‣ DL-Traff:
    Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction") to
    capture the dependency only from $Closeness$ features. Spatially, it applies two
    spectral graph convolution, ChebyNet(Defferrard et al., [2016](#bib.bib11)) and
    1st-order approximation of ChebyNet (Kipf and Welling, [2017](#bib.bib20)). TCN
    and GCN are stacked together as an ST-Conv block to sequentially do the spatial
    and temporal modeling. One major limitation of STGCN is that it uses a symmetrical
    adjacency matrix (i.e., undirected graph) that considers the euclidean distance
    between two road sensors. Thus it is difficult to model the difference of the
    two-way traffic flow in one road. DCRNN (Li et al., [2018](#bib.bib24)) is another
    pioneer to utilize graph convolution for traffic flow prediction. In contrast
    to the spectral convolution in STGCN, DCRNN applies a spatial graph convolution
    called Diffusion Convolution implemented with bidirectional random walks on a
    directed graph (i.e., non-symmetric adjacent matrix), so that it can capture the
    spatial influence from both the upstream and the downstream traffic flows. For
    the temporal axis, similar to ConvLSTM, it replaces the normal matrix multiplication
    in GRU with the proposed diffusion convolution, then a Diffusion Convolution Gated
    Recurrent Unit (DCGRU) is assembled that can simultaneously do the spatial and
    temporal modeling. With this DCGRU, it further implements an encoder-decoder structure
    to enable the multi-step-to-multi-step prediction. Inspired by STGCN and DCRNN,
    massive graph-based traffic models have been proposed as summarized in Table [1](#S0.T1
    "Table 1 ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic
    Prediction").'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 'STGCN（Yu et al., [2018](#bib.bib43)）是最早利用图神经网络进行交通流预测的模型之一。从时间上看，它使用了TCN（Yu
    and Koltun, [2016](#bib.bib44)）和一个门控机制，如图[4](#S4.F4 "Figure 4 ‣ 4.1\. Roadmap
    for Grid-Based Model ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark of Deep Learning
    Models for Urban Traffic Prediction")所示，而不是RNN，以捕捉仅来自$Closeness$特征的依赖关系。从空间上看，它应用了两个谱图卷积，ChebyNet（Defferrard
    et al., [2016](#bib.bib11)）和ChebyNet的1阶近似（Kipf and Welling, [2017](#bib.bib20)）。TCN和GCN被堆叠在一起，形成一个ST-Conv块，用于顺序地进行空间和时间建模。STGCN的一个主要限制是，它使用对称的邻接矩阵（即，无向图），只考虑两个路传感器之间的欧几里得距离。因此，很难建模一条道路上双向交通流的差异。DCRNN（Li
    et al., [2018](#bib.bib24)）是另一个开创性地利用图卷积进行交通流预测的模型。与STGCN中的谱卷积相比，DCRNN应用了一种称为扩散卷积的空间图卷积，该卷积通过在有向图（即，非对称邻接矩阵）上进行双向随机游走来实现，从而可以捕捉来自上下游交通流的空间影响。对于时间轴，类似于ConvLSTM，它用提议的扩散卷积替代了GRU中的普通矩阵乘法，然后组装了一个扩散卷积门控递归单元（DCGRU），能够同时进行空间和时间建模。利用这个DCGRU，它进一步实现了一个编码器-解码器结构，以支持多步到多步的预测。受到STGCN和DCRNN的启发，已经提出了大量基于图的交通模型，如表[1](#S0.T1
    "Table 1 ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic
    Prediction")所总结的。'
- en: Improvement along Temporal Axis. For the temporal feature, ASTGCN(Guo et al.,
    [2019](#bib.bib15)) inherits $Closeness$, $Period$, and $Trend$ from ST-ResNet,
    and improves STGCN that only takes $Closeness$. Besides, STSGCN(Song et al., [2020](#bib.bib34))
    constructs a localized temporal graph by connecting all nodes with themselves
    at the previous and the next steps, updating the adjacency matrix from $A$$\in$$\mathbb{R}^{N*N}$
    to $A^{\prime}$$\in$$\mathbb{R}^{3N*3N}$, then only uses GCN to simultaneously
    do the spatial and temporal modeling. On the other hand, to get better ability
    of temporal modeling, T-GCN(Zhao et al., [2019](#bib.bib53)) and TGC-LSTM(Cui
    et al., [2019](#bib.bib9)) respectively use GRU and LSTM instead of TCN to improve
    STGCN; GCGA(Yu and Gu, [2019](#bib.bib45)) combines Generative Adversarial Network(GAN)
    and Autoencoder with GCN; STGNN(Wang et al., [2020](#bib.bib37)) adopts transformer
    (attention) for better global/long-term temporal modeling; STG2Seq(Bai et al.,
    [2019](#bib.bib3)) utilized GCN for temporal modeling, which is an interesting
    attempt.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 沿时间轴的改进。对于时间特征，ASTGCN(Guo et al., [2019](#bib.bib15)) 继承了 ST-ResNet 中的 $Closeness$、$Period$
    和 $Trend$，并改进了仅采用 $Closeness$ 的 STGCN。此外，STSGCN(Song et al., [2020](#bib.bib34))
    通过将所有节点与前后步骤的节点连接起来来构建局部时间图，将邻接矩阵从 $A$$\in$$\mathbb{R}^{N*N}$ 更新为 $A^{\prime}$$\in$$\mathbb{R}^{3N*3N}$，然后仅使用
    GCN 同时进行空间和时间建模。另一方面，为了获得更好的时间建模能力，T-GCN(Zhao et al., [2019](#bib.bib53)) 和 TGC-LSTM(Cui
    et al., [2019](#bib.bib9)) 分别使用 GRU 和 LSTM 替代 TCN 来改进 STGCN；GCGA(Yu and Gu, [2019](#bib.bib45))
    将生成对抗网络(GAN)和自编码器与 GCN 结合；STGNN(Wang et al., [2020](#bib.bib37)) 采用 transformer（注意力机制）以实现更好的全球/长期时间建模；STG2Seq(Bai
    et al., [2019](#bib.bib3)) 使用 GCN 进行时间建模，这是一个有趣的尝试。
- en: Improvement along Spatial Axis. A lot of effort has been put on the spatial
    axis, that is the graph. (1) From single-graph to multi-graph. STGCN and DCRNN
    only use a single graph, directed or non-directed, to describe the spatial relationship.
    However, multimodal correlations and compound spatial dependencies exist among
    regions. Therefore, a series of researches elevate single-graph to multi-graph.
    For instance, (Chai et al., [2018](#bib.bib5)) and ST-MGCN(Geng et al., [2019](#bib.bib13))
    consider spatial proximity, functional similarity, and road connectivity as mutli-graph,
    and so as T-MGCN(Lv et al., [2020](#bib.bib27)); H-STGCN(Dai et al., [2020](#bib.bib10))
    takes travel time correlation matrix and shortest-path distance matrix as compound
    matrix; MRA-BGCN(Chen et al., [2020](#bib.bib6)) builds the node-wise graph according
    to the road network distance, and the edge-wise graph according to the connectivity
    and competition. (2) From static graph to adaptive graph. Graph WaveNet(Wu et al.,
    [2019](#bib.bib39)), TGC-LSTM(Cui et al., [2019](#bib.bib9)), and AGCRN(Bai et al.,
    [2020](#bib.bib4)) adopt adaptive/learnable graph rather than a static one used
    in STGCN and DCRNN; DGCNN(Diao et al., [2019](#bib.bib12)) proposes dynamic Laplacian
    matrix learning through tensor decomposition; SLCNN(Zhang et al., [2020a](#bib.bib51))
    designs Structure Learning Convolution (SLC) to dynamically learn the global/local
    graph structure. In addition to the above, attention-augmented GCN also demonstrated
    better performance in terms of spatial modeling in GaAN(Zhang et al., [2018](#bib.bib47)),
    ASTGCN, and GMAN(Zheng et al., [2020](#bib.bib54)).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 沿空间轴的改进。大量工作集中在空间轴，即图上。(1) 从单图到多图。STGCN 和 DCRNN 仅使用单个图（有向或无向）来描述空间关系。然而，区域之间存在多模态相关性和复合空间依赖。因此，一系列研究将单图提升为多图。例如，(Chai
    et al., [2018](#bib.bib5)) 和 ST-MGCN(Geng et al., [2019](#bib.bib13)) 将空间邻近、功能相似性和道路连通性视为多图，T-MGCN(Lv
    et al., [2020](#bib.bib27)) 也是如此；H-STGCN(Dai et al., [2020](#bib.bib10)) 将旅行时间相关矩阵和最短路径距离矩阵视为复合矩阵；MRA-BGCN(Chen
    et al., [2020](#bib.bib6)) 根据道路网络距离构建节点图，根据连通性和竞争构建边缘图。(2) 从静态图到自适应图。Graph WaveNet(Wu
    et al., [2019](#bib.bib39))、TGC-LSTM(Cui et al., [2019](#bib.bib9)) 和 AGCRN(Bai
    et al., [2020](#bib.bib4)) 采用自适应/可学习图，而不是 STGCN 和 DCRNN 中使用的静态图；DGCNN(Diao et
    al., [2019](#bib.bib12)) 提出通过张量分解学习动态拉普拉斯矩阵；SLCNN(Zhang et al., [2020a](#bib.bib51))
    设计了结构学习卷积（SLC）以动态学习全局/局部图结构。除此之外，增强注意力的 GCN 在 GaAN(Zhang et al., [2018](#bib.bib47))、ASTGCN
    和 GMAN(Zheng et al., [2020](#bib.bib54)) 中也表现出了更好的空间建模性能。
- en: 5\. Evaluation
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. 评估
- en: 5.1\. Setting
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1. 设置
- en: 'Towards the benchmark tasks listed in Section 2, we pick up some representative
    models and conduct comprehensive evaluations about their actual performances.
    Besides the deep models, we also implement two naive baselines as follows: (1)
    HistoricalAverage(HA). We average the corresponding values from historical days
    as the prediction result; (2) CopyLastStep(s). We directly copy the last one or
    multiple steps as the prediction result. Our experiments were performed on a GPU
    server with four GeForce GTX 2080Ti graphics cards. As a benchmark evaluation,
    the following settings are kept the same for each model. The observation step
    is set to 6 for grid-based models, while the observation and prediction step are
    both set to 12 for graph-based models. The data ratio for training, validation,
    and testing is set as 7:1:2\. Adam was set as the default optimizer, where the
    learning rate was set to 0.001 and the batch size was set to 64 by default. Mean
    Absolute Error is uniformly used as the loss function. The training algorithm
    would either be early-stopped if the validation error was converged within 10
    epochs or be stopped after 200 epochs, and the best model on validation data would
    be saved. Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute
    Percentage Error (MAPE) are used as metrics, where zero values will be ignored.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 针对第2节列出的基准任务，我们挑选了一些代表性模型并进行了全面的性能评估。除了深度模型外，我们还实现了两个简单的基线模型，如下所示：（1）HistoricalAverage(HA)。我们将历史天的数据值取平均作为预测结果；（2）CopyLastStep(s)。我们直接将最后一步或多步的结果作为预测结果。我们的实验在配备四块GeForce
    GTX 2080Ti显卡的GPU服务器上进行。作为基准评估，每个模型的设置保持一致。网格模型的观察步长设置为6，而图形模型的观察步长和预测步长都设置为12。训练、验证和测试的数据比例设置为7:1:2。Adam被设置为默认优化器，学习率设为0.001，批量大小默认设为64。均方误差（Mean
    Absolute Error）被统一用作损失函数。如果验证误差在10个周期内收敛，则训练算法将提前停止，或者在200个周期后停止，最佳模型将保存于验证数据上。根均方误差（RMSE）、均方误差（MAE）和平均绝对百分比误差（MAPE）被用作评估指标，其中零值将被忽略。
- en: 5.2\. Effectiveness Evaluation
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 效果评估
- en: 'The evaluation of grid-based models for single-step prediction is shown in
    Table [4](#S4.T4 "Table 4 ‣ 4.1\. Roadmap for Grid-Based Model ‣ 4\. Model ‣ DL-Traff:
    Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction"); the
    evaluation of graph-based models for multi-step prediction is shown in Table [5](#S4.T5
    "Table 5 ‣ 4.1\. Roadmap for Grid-Based Model ‣ 4\. Model ‣ DL-Traff: Survey and
    Benchmark of Deep Learning Models for Urban Traffic Prediction").'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '单步预测的网格模型评估见表 [4](#S4.T4 "Table 4 ‣ 4.1\. Roadmap for Grid-Based Model ‣ 4\.
    Model ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic
    Prediction")；多步预测的图形模型评估见表 [5](#S4.T5 "Table 5 ‣ 4.1\. Roadmap for Grid-Based
    Model ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for
    Urban Traffic Prediction")。'
- en: 'Evaluation for Grid-Based Model: Table [4](#S4.T4 "Table 4 ‣ 4.1\. Roadmap
    for Grid-Based Model ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark of Deep Learning
    Models for Urban Traffic Prediction") shows that the state-of-the-art models did
    have advantages over the baselines (HA $\sim$ ConvLSTM). In particular, STDN showed
    better performances in general, PCRN and DeepSTN+ achieved the lowest MAE on BikeNYC-I
    and TaxiNYC respectively. None of these grid-based models could be acknowledged
    as a dominant one at the current stage. Through the experiment, we find that their
    main limitations are as follows: (1) ST-ResNet converts the video-like data to
    high-dimensional image-like data and uses a simple fusion-mechanism to handle
    different types of temporal dependency; (2) through the experiment, it was found
    PCRN took more epochs to converge and tended to cause overfitting; (3) DMVST-Net
    and STDN use local CNN to take grid (pixel) as computation unit, resulting in
    long training time; (4) DeepSTN+ utilized a fully-connected layer in $ConvPlus$
    block, which would result in a big number of parameters on TaxiBJ; (4) Multitask
    Learning model(Zhang et al., [2019](#bib.bib50); Jiang et al., [2019](#bib.bib19))
    needs multiple data sources as the inputs, which hinders the applicability.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '对于网格模型的评估：表格 [4](#S4.T4 "表 4 ‣ 4.1\. 网格模型的路线图 ‣ 4\. 模型 ‣ DL-Traff: 城市交通预测深度学习模型的调查和基准")
    显示，最先进的模型确实在基准（HA $\sim$ ConvLSTM）上具有优势。特别是，STDN 表现出更好的总体性能，PCRN 和 DeepSTN+ 分别在
    BikeNYC-I 和 TaxiNYC 上达到了最低 MAE。目前，没有这些基于网格的模型能够被认为是主导的。通过实验，我们发现它们的主要局限性如下：（1）ST-ResNet
    将视频样的数据转换为高维图像样的数据，并使用简单的融合机制来处理不同类型的时间依赖关系；（2）通过实验发现，PCRN 需要更多的训练轮次才能收敛，并且容易导致过拟合；（3）DMVST-Net
    和 STDN 使用局部 CNN 将网格（像素）作为计算单元，导致训练时间较长；（4）DeepSTN+ 在 $ConvPlus$ 模块中使用了全连接层，这会导致
    TaxiBJ 上参数数量较大；（5）多任务学习模型（Zhang et al., [2019](#bib.bib50); Jiang et al., [2019](#bib.bib19)）需要多个数据源作为输入，这限制了其适用性。'
- en: 'Evaluation for Graph-Based Model: Table [5](#S4.T5 "Table 5 ‣ 4.1\. Roadmap
    for Grid-Based Model ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark of Deep Learning
    Models for Urban Traffic Prediction") compares the prediction performances of
    our selected models at 15 minutes, 30 minutes, 60 minutes ahead on METR-LA, PeMS-BAY,
    PEMSD7M datasets. Through Table [5](#S4.T5 "Table 5 ‣ 4.1\. Roadmap for Grid-Based
    Model ‣ 4\. Model ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for
    Urban Traffic Prediction"), we can find that: (1) Despite the effect of time-series
    model LSTNet in short-term prediction, its performance would deteriorate as the
    horizon gets longer; (2) Almost all of the graph-based models achieved better
    performance than traditional methods and time-series models on all metrics, which
    proved that the addition of spatial information would bring substantial performance
    improvements; (3) Although the models’ performances depended more or less on the
    dataset, the scores of DCRNN, Graph WaveNet, and MTGNN on all datasets ranked
    in the top 3, which also proved their robustness and versatility in traffic prediction
    tasks; (4) GMAN was found more prone to overfitting, due to which its performances
    on all three datasets were not as good as LSTNet; This is probably because GMAN
    adopted a global attention mechanism to capture the spatial dependency between
    each pair of nodes; (5) MTGNN and GraphWaveNet got most of the highest scores
    on different datasets and metrics. The self-adaptive/learnable graph demonstrated
    its great effectiveness for traffic prediction.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '对于基于图的模型的评估：表格 [5](#S4.T5 "表 5 ‣ 4.1\. 网格模型的路线图 ‣ 4\. 模型 ‣ DL-Traff: 城市交通预测深度学习模型的调查和基准")
    比较了我们选择的模型在 METR-LA、PeMS-BAY、PEMSD7M 数据集上 15 分钟、30 分钟、60 分钟的预测性能。通过表格 [5](#S4.T5
    "表 5 ‣ 4.1\. 网格模型的路线图 ‣ 4\. 模型 ‣ DL-Traff: 城市交通预测深度学习模型的调查和基准")，我们可以发现：（1）尽管时间序列模型
    LSTNet 在短期预测中的效果良好，但随着预测时间的延长，其性能会恶化；（2）几乎所有基于图的模型在所有指标上均优于传统方法和时间序列模型，这证明了空间信息的增加会带来显著的性能提升；（3）尽管模型的性能或多或少取决于数据集，但
    DCRNN、Graph WaveNet 和 MTGNN 在所有数据集上的评分均排在前 3 名，这也证明了它们在交通预测任务中的鲁棒性和多样性；（4）GMAN
    被发现更容易过拟合，因此在所有三个数据集上的表现不如 LSTNet；这可能是因为 GMAN 采用了全局注意力机制来捕捉节点对之间的空间依赖；（5）MTGNN
    和 GraphWaveNet 在不同数据集和指标上获得了大多数最高分。自适应/可学习的图展示了其在交通预测中的巨大有效性。'
- en: '![Refer to caption](img/3849d482f1c60c619dd5aded991688cb.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3849d482f1c60c619dd5aded991688cb.png)'
- en: Figure 5\. Case Study on Graph-Based Datasets.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 基于图的数据显示的案例研究。
- en: 'Case Study: We randomly selected one day (24 hours) and one sensor (node) from
    the three datasets (i.e., METR-LA, PEMS-BAY, and PEMSD7M) and plotted the time
    series of the ground truth and the predictions as Fig.[5](#S5.F5 "Figure 5 ‣ 5.2\.
    Effectiveness Evaluation ‣ 5\. Evaluation ‣ DL-Traff: Survey and Benchmark of
    Deep Learning Models for Urban Traffic Prediction"). To make the time-series chart
    clear, in addition to the ground truth, we only plot the prediction results of
    LSTNet, DCRNN, and Graph WaveNet instead of all of the models listed in Table
    [5](#S4.T5 "Table 5 ‣ 4.1\. Roadmap for Grid-Based Model ‣ 4\. Model ‣ DL-Traff:
    Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction"). Through
    Fig.[5](#S5.F5 "Figure 5 ‣ 5.2\. Effectiveness Evaluation ‣ 5\. Evaluation ‣ DL-Traff:
    Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction"), we
    can observe that: (1) All of the three models could learn the peak and valley
    trend on all three datasets; (2) The graph-based models DCRNN and Graph WaveNet
    always outperformed the time-series model LSTNet, which proved the excellent performance
    of GCN in capturing spatial correlation and dependency; (3) Time lag could be
    observed on all of the three prediction results, especially when violent fluctuations
    occur in the original time series such as 2012/6/20 21:00 in PEMSD7M. This problem
    will be magnified in the longer-term forecast like 60 minutes lead time. Despite
    this, the graph-based models still show better performance in terms of volatility
    prediction errors, which further confirmed the effectiveness and robustness of
    GCN in traffic prediction.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '案例研究：我们随机选择了一天（24 小时）和一个传感器（节点）从三个数据集（即 METR-LA、PEMS-BAY 和 PEMSD7M），并绘制了真实值和预测值的时间序列，如图
    [5](#S5.F5 "图 5 ‣ 5.2\. 效果评估 ‣ 5\. 评估 ‣ DL-Traff: 城市交通预测深度学习模型的调查与基准测试")。为了使时间序列图清晰，除了真实值外，我们仅绘制了
    LSTNet、DCRNN 和 Graph WaveNet 的预测结果，而不是表 [5](#S4.T5 "表 5 ‣ 4.1\. 基于网格的模型路线图 ‣ 4\.
    模型 ‣ DL-Traff: 城市交通预测深度学习模型的调查与基准测试") 中列出的所有模型。通过图 [5](#S5.F5 "图 5 ‣ 5.2\. 效果评估
    ‣ 5\. 评估 ‣ DL-Traff: 城市交通预测深度学习模型的调查与基准测试")，我们可以观察到：(1) 三个模型都能学习所有三个数据集中的峰值和谷值趋势；(2)
    基于图的模型 DCRNN 和 Graph WaveNet 始终优于时间序列模型 LSTNet，这证明了 GCN 在捕捉空间相关性和依赖性方面的优异性能；(3)
    在所有三个预测结果中都能观察到时间滞后，特别是当原始时间序列中发生剧烈波动时，如 PEMSD7M 中的 2012/6/20 21:00。这一问题在长期预测如
    60 分钟预测时会被放大。尽管如此，基于图的模型在波动预测误差方面仍表现出更好的性能，这进一步验证了 GCN 在交通预测中的有效性和鲁棒性。'
- en: '![Refer to caption](img/b6ed8b2a53aadefe506a0194a6b34732.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b6ed8b2a53aadefe506a0194a6b34732.png)'
- en: Figure 6\. Efficiency Summary.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 效率总结。
- en: 5.3\. Efficiency Evaluation
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 效率评估
- en: 'Besides comparing the effectiveness of these deep models, we also provide an
    analysis of their efficiency. The space complexity and time complexity of these
    approaches are significant for their future application, so we exhibit the number
    of the trainable parameters and training time of each model through bar charts
    Fig.[6](#S5.F6 "Figure 6 ‣ 5.2\. Effectiveness Evaluation ‣ 5\. Evaluation ‣ DL-Traff:
    Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction").'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '除了比较这些深度模型的效果外，我们还提供了它们效率的分析。这些方法的空间复杂度和时间复杂度对其未来应用至关重要，因此我们通过柱状图展示了每个模型的可训练参数数量和训练时间，如图
    [6](#S5.F6 "图 6 ‣ 5.2\. 效果评估 ‣ 5\. 评估 ‣ DL-Traff: 城市交通预测深度学习模型的调查与基准测试")。'
- en: 'Evaluation for Grid-Based Model: From Fig.[6](#S5.F6 "Figure 6 ‣ 5.2\. Effectiveness
    Evaluation ‣ 5\. Evaluation ‣ DL-Traff: Survey and Benchmark of Deep Learning
    Models for Urban Traffic Prediction")-(a) and Fig.[6](#S5.F6 "Figure 6 ‣ 5.2\.
    Effectiveness Evaluation ‣ 5\. Evaluation ‣ DL-Traff: Survey and Benchmark of
    Deep Learning Models for Urban Traffic Prediction")-(b), we can observe that:
    (1) the parameter numbers of DeepSTN+ and STDN are more than other models, especially
    DeepSTN+, which captures the citywide spatial correlation by utilizing fully connected
    layer; (2) ST-ResNet has the fewest trainable parameters, and demonstrates its
    superiority to other models in terms of space complexity; (3) The training time
    of STDN and DMVST is longer than other models because they utilize the LSTM to
    capture the temporal dependency and take each mesh-grid as the computation unit
    rather than the entire mesh.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '对于基于网格的模型：从图[6](#S5.F6 "Figure 6 ‣ 5.2\. Effectiveness Evaluation ‣ 5\. Evaluation
    ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction")-(a)
    和图[6](#S5.F6 "Figure 6 ‣ 5.2\. Effectiveness Evaluation ‣ 5\. Evaluation ‣ DL-Traff:
    Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction")-(b)
    可以观察到：(1) DeepSTN+ 和 STDN 的参数数量多于其他模型，尤其是 DeepSTN+，它通过利用全连接层捕获了城市范围的空间关联；(2) ST-ResNet
    拥有最少的可训练参数，在空间复杂度方面优于其他模型；(3) STDN 和 DMVST 的训练时间比其他模型长，因为它们利用 LSTM 捕捉时间依赖性，并将每个网格作为计算单位，而不是整个网格。'
- en: 'Evaluation for Graph-Based Model: From Fig.[6](#S5.F6 "Figure 6 ‣ 5.2\. Effectiveness
    Evaluation ‣ 5\. Evaluation ‣ DL-Traff: Survey and Benchmark of Deep Learning
    Models for Urban Traffic Prediction")-(c) and Fig.[6](#S5.F6 "Figure 6 ‣ 5.2\.
    Effectiveness Evaluation ‣ 5\. Evaluation ‣ DL-Traff: Survey and Benchmark of
    Deep Learning Models for Urban Traffic Prediction")-(d), we can conclude that:
    (1) STGCN and GMAN have relatively lower space complexity than others; (2) AGCRN
    and DCRNN need more parameters than other models because they are based on RNNs;
    (3) On PEMSBAY, the parameter numbers of ASTGCN and MTGNN dramatically increase.
    The reason for this is those two models have more GNN layers and they are more
    sensitive to the node number; (4) The training time of GMAN on PEMSBAY outdistances
    other models because it applies a global attention mechanism to the entire graph
    (nodes). In summary, TCN-based models like STGCN and Graph WaveNet have higher
    computation efficiency.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '图[6](#S5.F6 "Figure 6 ‣ 5.2\. Effectiveness Evaluation ‣ 5\. Evaluation ‣ DL-Traff:
    Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction")-(c)
    和图[6](#S5.F6 "Figure 6 ‣ 5.2\. Effectiveness Evaluation ‣ 5\. Evaluation ‣ DL-Traff:
    Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction")-(d)
    显示了以下结论：(1) STGCN 和 GMAN 的空间复杂度相对较低；(2) AGCRN 和 DCRNN 需要比其他模型更多的参数，因为它们基于 RNN；(3)
    在 PEMSBAY 上，ASTGCN 和 MTGNN 的参数数量显著增加。这是因为这两个模型有更多的 GNN 层，并且对节点数量更为敏感；(4) GMAN
    在 PEMSBAY 上的训练时间超过其他模型，因为它对整个图（节点）应用了全局注意力机制。总之，像 STGCN 和 Graph WaveNet 这样的基于
    TCN 的模型具有更高的计算效率。'
- en: 6\. Availability and Usability
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 可用性和实用性
- en: 'DL-Traff is already available at GitHub as the following two repositories under
    the MIT License: one is for grid-based datasets/models [https://github.com/deepkashiwa20/DL-Traff-Grid](https://github.com/deepkashiwa20/DL-Traff-Grid),
    and another is for graph-based datasets/models [https://github.com/deepkashiwa20/DL-Traff-Graph](https://github.com/deepkashiwa20/DL-Traff-Graph).
    It is implemented with Python and the most popular deep learning frameworks: Keras(Chollet,
    [2015](#bib.bib7)) on TensorFlow(Abadi et al., [2015](#bib.bib2)) and PyTorch(Paszke
    et al., [2019](#bib.bib32)). Fig.[7](#S6.F7 "Figure 7 ‣ 6\. Availability and Usability
    ‣ DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction")
    shows a use case by taking DCRNN model on METR-LA dataset as an example. To run
    the benchmark, the repository should be cloned locally and a conda environment
    with the necessary dependencies should be created. The directory is structured
    in a flat style and only with two levels. The traffic datasets are stored in DATA
    directories (e.g., METRLA), and the python files are put in workDATA directories
    (e.g., workMETRLA). Entering the work directory for a certain dataset, we can
    find MODEL class file (e.g., DCRNN.py) and its corresponding running program named
    pred_MODEL.py (e.g., pred_DCRNN.py). We can run “python MODEL.py” to simply check
    the model architecture without feeding the training data and run “python pred_MODEL.py”
    to train and test the model. Additionally, Param.py file contains a variety of
    hyper-parameters as described in Section 5.1 that allow the experiment to be customized
    in a unified way. Metrics.py file contains the metric functions listed in Section
    5.1\. Utils.py file integrates a set of supporting functions such as pickle file
    reader and self-defined loss function. More details about the usability and implementation
    can be found at GitHub.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 'DL-Traff 已在 GitHub 上以 MIT 许可证发布，包含以下两个库：一个用于基于网格的数据集/模型 [https://github.com/deepkashiwa20/DL-Traff-Grid](https://github.com/deepkashiwa20/DL-Traff-Grid)，另一个用于基于图的的数据集/模型
    [https://github.com/deepkashiwa20/DL-Traff-Graph](https://github.com/deepkashiwa20/DL-Traff-Graph)。它使用
    Python 和最流行的深度学习框架实现：Keras(Chollet, [2015](#bib.bib7)) 在 TensorFlow(Abadi et al.,
    [2015](#bib.bib2)) 和 PyTorch(Paszke et al., [2019](#bib.bib32)) 上。图[7](#S6.F7
    "Figure 7 ‣ 6\. Availability and Usability ‣ DL-Traff: Survey and Benchmark of
    Deep Learning Models for Urban Traffic Prediction")展示了以 DCRNN 模型在 METR-LA 数据集上的用例。要运行基准测试，应将仓库克隆到本地并创建一个包含必要依赖项的
    conda 环境。目录结构为扁平样式，只有两个层级。交通数据集存储在 DATA 目录（例如，METRLA）中，Python 文件放在 workDATA 目录（例如，workMETRLA）中。进入某个数据集的工作目录，我们可以找到
    MODEL 类文件（例如，DCRNN.py）及其相应的运行程序名为 pred_MODEL.py（例如，pred_DCRNN.py）。我们可以运行“python
    MODEL.py”来简单检查模型架构而无需输入训练数据，并运行“python pred_MODEL.py”来训练和测试模型。此外，Param.py 文件包含第
    5.1 节中描述的各种超参数，以统一方式定制实验。Metrics.py 文件包含第 5.1 节中列出的度量函数。Utils.py 文件集成了一组支持函数，如
    pickle 文件读取器和自定义损失函数。有关可用性和实现的更多细节，请参见 GitHub。'
- en: '![Refer to caption](img/edd3457256f122ed1f484cad5dfaa526.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/edd3457256f122ed1f484cad5dfaa526.png)'
- en: Figure 7\. Illustration of The Use Case for DL-Traff.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. DL-Traff 使用案例示意图。
- en: 7\. Conclusion
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 结论
- en: In this paper, we first survey the deep learning models as well as the widely
    used datasets for urban traffic prediction. Then we build a standard benchmark
    to comprehensively evaluate the deep traffic models on the selected open datasets.
    The survey and the benchmark combine together to form our study called DL-Traff,
    which is already available at [https://github.com/deepkashiwa20/DL-Traff-Grid](https://github.com/deepkashiwa20/DL-Traff-Grid)
    and [https://github.com/deepkashiwa20/DL-Traff-Graph](https://github.com/deepkashiwa20/DL-Traff-Graph).
    With DL-Traff, we hope to deliver a useful and timely resource to researchers
    in AI and data science community.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们首先调查了用于城市交通预测的深度学习模型及广泛使用的数据集。然后，我们建立了一个标准基准，全面评估选定开放数据集上的深度交通模型。调查和基准结合起来形成了我们的研究，称为
    DL-Traff，该研究已经在[https://github.com/deepkashiwa20/DL-Traff-Grid](https://github.com/deepkashiwa20/DL-Traff-Grid)和[https://github.com/deepkashiwa20/DL-Traff-Graph](https://github.com/deepkashiwa20/DL-Traff-Graph)上提供。通过
    DL-Traff，我们希望向人工智能和数据科学社区的研究人员提供一个有用且及时的资源。
- en: References
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Abadi et al. (2015) Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,
    Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu
    Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael
    Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
    Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
    Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
    Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin
    Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale
    Machine Learning on Heterogeneous Systems. [http://tensorflow.org/](http://tensorflow.org/)
    Software available from tensorflow.org.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abadi 等人（2015）Martín Abadi、Ashish Agarwal、Paul Barham、Eugene Brevdo、Zhifeng
    Chen、Craig Citro、Greg S. Corrado、Andy Davis、Jeffrey Dean、Matthieu Devin、Sanjay
    Ghemawat、Ian Goodfellow、Andrew Harp、Geoffrey Irving、Michael Isard、Yangqing Jia、Rafal
    Jozefowicz、Lukasz Kaiser、Manjunath Kudlur、Josh Levenberg、Dan Mané、Rajat Monga、Sherry
    Moore、Derek Murray、Chris Olah、Mike Schuster、Jonathon Shlens、Benoit Steiner、Ilya
    Sutskever、Kunal Talwar、Paul Tucker、Vincent Vanhoucke、Vijay Vasudevan、Fernanda
    Viégas、Oriol Vinyals、Pete Warden、Martin Wattenberg、Martin Wicke、Yuan Yu 和 Xiaoqiang
    Zheng. 2015. TensorFlow: 大规模异构系统上的机器学习。 [http://tensorflow.org/](http://tensorflow.org/)
    软件可从 tensorflow.org 获取。'
- en: 'Bai et al. (2019) Lei Bai, Lina Yao, Salil Kanhere, Xianzhi Wang, Quan Sheng,
    et al. 2019. Stg2seq: Spatial-temporal graph to sequence model for multi-step
    passenger demand forecasting. In *IJCAI*. 1981–1987.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai 等人（2019）Lei Bai、Lina Yao、Salil Kanhere、Xianzhi Wang、Quan Sheng 等. 2019.
    Stg2seq: 空间-时间图到序列模型用于多步乘客需求预测。见于 *IJCAI*。1981–1987。'
- en: Bai et al. (2020) Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020.
    Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting. In *34th
    Conference on Neural Information Processing Systems*.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人（2020）Lei Bai、Lina Yao、Can Li、Xianzhi Wang 和 Can Wang. 2020. 用于交通预测的自适应图卷积递归网络。见于
    *第34届神经信息处理系统会议*。
- en: Chai et al. (2018) Di Chai, Leye Wang, and Qiang Yang. 2018. Bike flow prediction
    with multi-graph convolutional networks. In *Proceedings of the 26th ACM SIGSPATIAL
    International Conference on Advances in Geographic Information Systems*. 397–400.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chai 等人（2018）Di Chai、Leye Wang 和 Qiang Yang. 2018. 使用多图卷积网络进行自行车流量预测。见于 *第26届
    ACM SIGSPATIAL 国际地理信息系统进展会议论文集*。397–400。
- en: Chen et al. (2020) Weiqi Chen, Ling Chen, Yu Xie, Wei Cao, Yusong Gao, and Xiaojie
    Feng. 2020. Multi-range attentive bicomponent graph convolutional network for
    traffic forecasting. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 34\. 3529–3536.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2020）Weiqi Chen、Ling Chen、Yu Xie、Wei Cao、Yusong Gao 和 Xiaojie Feng.
    2020. 用于交通预测的多范围注意力双组件图卷积网络。见于 *AAAI 人工智能会议论文集*，第34卷。3529–3536。
- en: Chollet (2015) François Chollet. 2015. keras. [https://github.com/fchollet/keras](https://github.com/fchollet/keras).
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet（2015）François Chollet. 2015. keras. [https://github.com/fchollet/keras](https://github.com/fchollet/keras)。
- en: Chung et al. (2014) Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua
    Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence
    modeling. In *NIPS 2014 Workshop on Deep Learning, December 2014*.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等人（2014）Junyoung Chung、Caglar Gulcehre、Kyunghyun Cho 和 Yoshua Bengio.
    2014. 对序列建模中的门控递归神经网络的实证评估。见于 *NIPS 2014 深度学习研讨会，2014年12月*。
- en: 'Cui et al. (2019) Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang.
    2019. Traffic graph convolutional recurrent neural network: A deep learning framework
    for network-scale traffic learning and forecasting. *IEEE Transactions on Intelligent
    Transportation Systems* (2019).'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等人（2019）Zhiyong Cui、Kristian Henrickson、Ruimin Ke 和 Yinhai Wang. 2019. 交通图卷积递归神经网络：网络规模交通学习与预测的深度学习框架。
    *IEEE 智能交通系统汇刊*（2019）。
- en: 'Dai et al. (2020) Rui Dai, Shenkun Xu, Qian Gu, Chenguang Ji, and Kaikui Liu.
    2020. Hybrid Spatio-Temporal Graph Convolutional Network: Improving Traffic Prediction
    with Navigation Data. In *Proceedings of the 26th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*. 3074–3082.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人（2020）Rui Dai、Shenkun Xu、Qian Gu、Chenguang Ji 和 Kaikui Liu. 2020. 混合时空图卷积网络：通过导航数据改善交通预测。见于
    *第26届 ACM SIGKDD 知识发现与数据挖掘国际会议论文集*。3074–3082。
- en: Defferrard et al. (2016) Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst.
    2016. Convolutional neural networks on graphs with fast localized spectral filtering.
    In *Advances in neural information processing systems*. 3844–3852.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Defferrard 等人（2016）Michaël Defferrard、Xavier Bresson 和 Pierre Vandergheynst.
    2016. 图上的卷积神经网络与快速局部光谱过滤。见于 *神经信息处理系统进展*。3844–3852。
- en: Diao et al. (2019) Zulong Diao, Xin Wang, Dafang Zhang, Yingru Liu, Kun Xie,
    and Shaoyao He. 2019. Dynamic spatial-temporal graph convolutional neural networks
    for traffic forecasting. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 33\. 890–897.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diao 等人（2019）Zulong Diao、Xin Wang、Dafang Zhang、Yingru Liu、Kun Xie 和 Shaoyao
    He. 2019. 用于交通预测的动态时空图卷积神经网络。载于 *AAAI 人工智能会议论文集*，第 33 卷，890–897 页。
- en: Geng et al. (2019) Xu Geng, Yaguang Li, Leye Wang, Lingyu Zhang, Qiang Yang,
    Jieping Ye, and Yan Liu. 2019. Spatiotemporal multi-graph convolution network
    for ride-hailing demand forecasting. In *2019 AAAI Conference on Artificial Intelligence
    (AAAI’19)*.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geng 等人（2019）Xu Geng、Yaguang Li、Leye Wang、Lingyu Zhang、Qiang Yang、Jieping Ye
    和 Yan Liu. 2019. 用于打车需求预测的时空多图卷积网络。载于 *2019 AAAI 人工智能会议（AAAI’19）*。
- en: Guo et al. (2020) Kan Guo, Yongli Hu, Zhen Qian, Yanfeng Sun, Junbin Gao, and
    Baocai Yin. 2020. Dynamic Graph Convolution Network for Traffic Forecasting Based
    on Latent Network of Laplace Matrix Estimation. *IEEE Transactions on Intelligent
    Transportation Systems* (2020).
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人（2020）Kan Guo、Yongli Hu、Zhen Qian、Yanfeng Sun、Junbin Gao 和 Baocai Yin.
    2020. 基于拉普拉斯矩阵估计的潜在网络的动态图卷积网络用于交通预测。*IEEE 智能交通系统汇刊*（2020）。
- en: Guo et al. (2019) Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu
    Wan. 2019. Attention based spatial-temporal graph convolutional networks for traffic
    flow forecasting. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 33\. 922–929.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人（2019）Shengnan Guo、Youfang Lin、Ning Feng、Chao Song 和 Huaiyu Wan. 2019.
    基于注意力的时空图卷积网络用于交通流量预测。载于 *AAAI 人工智能会议论文集*，第 33 卷，922–929 页。
- en: Hoang et al. (2016) Minh X Hoang, Yu Zheng, and Ambuj K Singh. 2016. Forecasting
    citywide crowd flows based on big data. *ACM SIGSPATIAL* (2016).
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoang 等人（2016）Minh X Hoang、Yu Zheng 和 Ambuj K Singh. 2016. 基于大数据的城市范围人群流量预测。*ACM
    SIGSPATIAL*（2016）。
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long short-term memory. *Neural computation* 9, 8 (1997), 1735–1780.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber（1997）Sepp Hochreiter 和 Jürgen Schmidhuber. 1997. 长短期记忆。*神经计算*
    9, 8（1997），1735–1780 页。
- en: 'Huang et al. (2014) Wenhao Huang, Guojie Song, Haikun Hong, and Kunqing Xie.
    2014. Deep architecture for traffic flow prediction: Deep belief networks with
    multitask learning. *IEEE Transactions on Intelligent Transportation Systems*
    15, 5 (2014), 2191–2201.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2014）Wenhao Huang、Guojie Song、Haikun Hong 和 Kunqing Xie. 2014. 用于交通流预测的深度架构：带有多任务学习的深度信念网络。*IEEE
    智能交通系统汇刊* 15, 5（2014），2191–2201 页。
- en: 'Jiang et al. (2019) Renhe Jiang, Xuan Song, Dou Huang, Xiaoya Song, Tianqi
    Xia, Zekun Cai, Zhaonan Wang, Kyoung-Sook Kim, and Ryosuke Shibasaki. 2019. DeepUrbanEvent:
    A System for Predicting Citywide Crowd Dynamics at Big Events. In *Proceedings
    of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining*. ACM, 2114–2122.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人（2019）Renhe Jiang、Xuan Song、Dou Huang、Xiaoya Song、Tianqi Xia、Zekun Cai、Zhaonan
    Wang、Kyoung-Sook Kim 和 Ryosuke Shibasaki. 2019. DeepUrbanEvent：一个预测城市重大事件人群动态的系统。载于
    *第 25 届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*。ACM，2114–2122 页。
- en: Kipf and Welling (2017) Thomas N Kipf and Max Welling. 2017. Semi-Supervised
    Classification with Graph Convolutional Networks. (2017).
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf 和 Welling（2017）Thomas N Kipf 和 Max Welling. 2017. 基于图卷积网络的半监督分类。（2017）。
- en: Lai et al. (2018) Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu.
    2018. Modeling long-and short-term temporal patterns with deep neural networks.
    In *The 41st International ACM SIGIR Conference on Research & Development in Information
    Retrieval*. 95–104.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai 等人（2018）Guokun Lai、Wei-Cheng Chang、Yiming Yang 和 Hanxiao Liu. 2018. 使用深度神经网络建模长期和短期时间模式。载于
    *第 41 届国际 ACM SIGIR 信息检索研究与开发会议*。95–104 页。
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. Gradient-based learning applied to document recognition. *Proc. IEEE* 86,
    11 (1998), 2278–2324.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等人（1998）Yann LeCun、Léon Bottou、Yoshua Bengio 和 Patrick Haffner. 1998.
    应用于文档识别的基于梯度的学习。*IEEE 计算机学报* 86, 11（1998），2278–2324 页。
- en: Li et al. (2019) Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen,
    Yu-Xiang Wang, and Xifeng Yan. 2019. Enhancing the locality and breaking the memory
    bottleneck of transformer on time series forecasting. In *Advances in Neural Information
    Processing Systems*. 5243–5253.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2019）Shiyang Li、Xiaoyong Jin、Yao Xuan、Xiyou Zhou、Wenhu Chen、Yu-Xiang Wang
    和 Xifeng Yan. 2019. 增强变换器在时间序列预测中的局部性并突破记忆瓶颈。载于 *神经信息处理系统进展*。5243–5253 页。
- en: 'Li et al. (2018) Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2018. Diffusion
    Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting. In *International
    Conference on Learning Representations*.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2018）**雅光·李**、**罗斯·余**、**赛勒斯·沙赫比** 和 **燕·刘**。2018。扩散卷积递归神经网络：数据驱动的交通预测。在*国际学习表征会议*。
- en: 'Liang et al. (2018) Yuxuan Liang, Songyu Ke, Junbo Zhang, Xiuwen Yi, and Yu
    Zheng. 2018. Geoman: Multi-level attention networks for geo-sensory time series
    prediction. In *IJCAI*. 3428–3434.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等（2018）**玉轩·梁**、**松宇·柯**、**俊博·张**、**修文·易** 和 **宇·郑**。2018。Geoman：用于地理传感时间序列预测的多层注意力网络。在*国际人工智能会议（IJCAI）*。3428–3434。
- en: 'Lin et al. (2019) Ziqian Lin, Jie Feng, Ziyang Lu, Yong Li, and Depeng Jin.
    2019. DeepSTN+: Context-aware Spatial-Temporal Neural Network for Crowd Flow Prediction
    in Metropolis. AAAI.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2019）**子倩·林**、**杰·冯**、**紫阳·卢**、**勇·李** 和 **德朋·金**。2019。DeepSTN+：用于大都市人群流动预测的上下文感知空间-时间神经网络。AAAI。
- en: Lv et al. (2020) Mingqi Lv, Zhaoxiong Hong, Ling Chen, Tieming Chen, Tiantian
    Zhu, and Shouling Ji. 2020. Temporal multi-graph convolutional network for traffic
    flow prediction. *IEEE Transactions on Intelligent Transportation Systems* (2020).
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv 等（2020）**明奇·吕**、**赵雄·洪**、**凌·陈**、**铁名·陈**、**天甜·朱** 和 **寿龄·季**。2020。用于交通流预测的时间多图卷积网络。*IEEE
    智能交通系统汇刊*（2020）。
- en: 'Lv et al. (2014) Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue
    Wang. 2014. Traffic flow prediction with big data: a deep learning approach. *IEEE
    Transactions on Intelligent Transportation Systems* 16, 2 (2014), 865–873.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv 等（2014）**义生·吕**、**艳杰·段**、**文文·康**、**正熙·李** 和 **飞跃·王**。2014。利用大数据进行交通流预测：一种深度学习方法。*IEEE
    智能交通系统汇刊* 16, 2（2014），865–873。
- en: 'Ma et al. (2017) Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Ma, Yong Wang,
    and Yunpeng Wang. 2017. Learning traffic as images: a deep convolutional neural
    network for large-scale transportation network speed prediction. *Sensors* 17,
    4 (2017), 818.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2017）**晓磊·马**、**庄·戴**、**正兵·何**、**季辉·马**、**勇·王** 和 **云鹏·王**。2017。将交通学习视为图像：用于大规模交通网络速度预测的深度卷积神经网络。*传感器*
    17, 4（2017），818。
- en: 'Ma et al. (2015) Xiaolei Ma, Zhimin Tao, Yinhai Wang, Haiyang Yu, and Yunpeng
    Wang. 2015. Long short-term memory neural network for traffic speed prediction
    using remote microwave sensor data. *Transportation Research Part C: Emerging
    Technologies* 54 (2015), 187–197.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2015）**晓磊·马**、**志敏·陶**、**银海·王**、**海洋·余** 和 **云鹏·王**。2015。利用遥感微波传感器数据进行交通速度预测的长短期记忆神经网络。*交通研究C部分：新兴技术*
    54（2015），187–197。
- en: Pan et al. (2019) Zheyi Pan, Yuxuan Liang, Weifeng Wang, Yong Yu, Yu Zheng,
    and Junbo Zhang. 2019. Urban Traffic Prediction from Spatio-Temporal Data Using
    Deep Meta Learning. In *Proceedings of the 25th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*. ACM, 1720–1730.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等（2019）**哲毅·潘**、**玉轩·梁**、**伟峰·王**、**勇·余**、**宇·郑** 和 **俊博·张**。2019。利用深度元学习进行城市交通预测。在*第25届ACM
    SIGKDD国际知识发现与数据挖掘大会论文集*。ACM，1720–1730。
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,
    Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
    Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning
    Library. In *Advances in Neural Information Processing Systems 32*, H. Wallach,
    H. Larochelle, A. Beygelzimer, F. d''Alché-Buc, E. Fox, and R. Garnett (Eds.).
    Curran Associates, Inc., 8024–8035. [http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf](http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke 等（2019）**亚当·帕兹克**、**萨姆·格罗斯**、**弗朗西斯科·马萨**、**亚当·勒雷**、**詹姆斯·布拉德伯里**、**格雷戈里·查南**、**特雷弗·基林**、**曾铭·林**、**娜塔利亚·吉梅尔谢因**、**卢卡·安蒂加**、**阿尔班·德斯梅松**、**安德烈亚斯·科普夫**、**爱德华·杨**、**扎克里·德维托**、**马丁·赖森**、**阿利汗·特贾尼**、**萨桑克·基拉姆库尔西**、**布诺瓦·斯坦**、**卢·方**、**俊杰·白**
    和 **苏米特·钦塔拉**。2019。PyTorch：一种命令式风格的高性能深度学习库。在*神经信息处理系统进展 32*，H. Wallach、H. Larochelle、A.
    Beygelzimer、F. d'Alché-Buc、E. Fox 和 R. Garnett（编）。Curran Associates, Inc., 8024–8035。
    [http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf](http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf)
- en: Shih et al. (2019) Shun-Yao Shih, Fan-Keng Sun, and Hung-yi Lee. 2019. Temporal
    pattern attention for multivariate time series forecasting. *Machine Learning*
    108, 8-9 (2019), 1421–1441.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shih 等（2019）**舜耀·石**、**凡铿·孙** 和 **洪一·李**。2019。用于多变量时间序列预测的时间模式注意力。*机器学习* 108,
    8-9（2019），1421–1441。
- en: 'Song et al. (2020) Chao Song, Youfang Lin, Shengnan Guo, and Huaiyu Wan. 2020.
    Spatial-Temporal Synchronous Graph Convolutional Networks: A New Framework for
    Spatial-Temporal Network Data Forecasting. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 34\. 914–921.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2020) Chao Song, Youfang Lin, Shengnan Guo, 和 Huaiyu Wan. 2020.
    空间-时间同步图卷积网络：一种用于空间-时间网络数据预测的新框架。发表于 *AAAI人工智能会议*，第34卷。914–921。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is All you Need. In *NIPS*.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Lukasz Kaiser, 和 Illia Polosukhin. 2017. 注意力机制即为一切。发表于
    *NIPS*。
- en: 'Wang et al. (2017) Dong Wang, Wei Cao, Jian Li, and Jieping Ye. 2017. DeepSD:
    supply-demand prediction for online car-hailing services using deep neural networks.
    In *2017 IEEE 33rd International Conference on Data Engineering (ICDE)*. IEEE,
    243–254.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2017) Dong Wang, Wei Cao, Jian Li, 和 Jieping Ye. 2017. DeepSD：使用深度神经网络进行在线打车服务的供需预测。发表于
    *2017 IEEE第33届数据工程国际会议 (ICDE)*。IEEE, 243–254。
- en: Wang et al. (2020) Xiaoyang Wang, Yao Ma, Yiqi Wang, Wei Jin, Xin Wang, Jiliang
    Tang, Caiyan Jia, and Jian Yu. 2020. Traffic Flow Prediction via Spatial Temporal
    Graph Neural Network. In *Proceedings of The Web Conference 2020*. 1082–1092.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020) Xiaoyang Wang, Yao Ma, Yiqi Wang, Wei Jin, Xin Wang, Jiliang
    Tang, Caiyan Jia, 和 Jian Yu. 2020. 通过空间-时间图神经网络进行交通流量预测。发表于 *The Web Conference
    2020*。1082–1092。
- en: 'Wu et al. (2020) Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun
    Chang, and Chengqi Zhang. 2020. Connecting the dots: Multivariate time series
    forecasting with graph neural networks. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 753–763.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2020) Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang,
    和 Chengqi Zhang. 2020. 连接点：使用图神经网络进行多变量时间序列预测。发表于 *第26届ACM SIGKDD国际知识发现与数据挖掘会议*。753–763。
- en: Wu et al. (2019) Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi
    Zhang. 2019. Graph wavenet for deep spatial-temporal graph modeling. In *IJCAI*.
    1907–1913.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2019) Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, 和 Chengqi
    Zhang. 2019. 图WaveNet用于深度空间-时间图建模。发表于 *IJCAI*。1907–1913。
- en: 'Xingjian et al. (2015) SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung,
    Wai-Kin Wong, and Wang-chun Woo. 2015. Convolutional LSTM network: A machine learning
    approach for precipitation nowcasting. In *Advances in neural information processing
    systems*. 802–810.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xingjian et al. (2015) SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung,
    Wai-Kin Wong, 和 Wang-chun Woo. 2015. 卷积LSTM网络：一种用于降水预报的机器学习方法。发表于 *Advances in
    neural information processing systems*。802–810。
- en: 'Yao et al. (2019) Huaxiu Yao, Xianfeng Tang, Hua Wei, Guanjie Zheng, and Zhenhui
    Li. 2019. Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for
    Traffic Prediction. In *2019 AAAI Conference on Artificial Intelligence (AAAI’19)*.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2019) Huaxiu Yao, Xianfeng Tang, Hua Wei, Guanjie Zheng, 和 Zhenhui
    Li. 2019. 重新审视空间-时间相似性：一种用于交通预测的深度学习框架。发表于 *2019 AAAI人工智能会议 (AAAI’19)*。
- en: Yao et al. (2018) Huaxiu Yao, Fei Wu, Jintao Ke, Xianfeng Tang, Yitian Jia,
    Siyu Lu, Pinghua Gong, Jieping Ye, and Zhenhui Li. 2018. Deep multi-view spatial-temporal
    network for taxi demand prediction. In *Thirty-Second AAAI Conference on Artificial
    Intelligence*.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2018) Huaxiu Yao, Fei Wu, Jintao Ke, Xianfeng Tang, Yitian Jia,
    Siyu Lu, Pinghua Gong, Jieping Ye, 和 Zhenhui Li. 2018. 深度多视图时空网络用于出租车需求预测。发表于
    *第三十二届AAAI人工智能会议*。
- en: 'Yu et al. (2018) Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018. Spatio-temporal
    graph convolutional networks: a deep learning framework for traffic forecasting.
    In *Proceedings of the 27th International Joint Conference on Artificial Intelligence*.
    AAAI Press, 3634–3640.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2018) Bing Yu, Haoteng Yin, 和 Zhanxing Zhu. 2018. 时空图卷积网络：一种用于交通预测的深度学习框架。发表于
    *第27届国际人工智能联合会议*。AAAI Press, 3634–3640。
- en: Yu and Koltun (2016) Fisher Yu and Vladlen Koltun. 2016. Multi-Scale Context
    Aggregation by Dilated Convolutions. In *ICLR*.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu and Koltun (2016) Fisher Yu 和 Vladlen Koltun. 2016. 通过扩张卷积进行多尺度上下文聚合。发表于
    *ICLR*。
- en: Yu and Gu (2019) James Jian Qiao Yu and Jiatao Gu. 2019. Real-time traffic speed
    estimation with graph convolutional generative autoencoder. *IEEE Transactions
    on Intelligent Transportation Systems* 20, 10 (2019), 3940–3951.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu and Gu (2019) James Jian Qiao Yu 和 Jiatao Gu. 2019. 基于图卷积生成自编码器的实时交通速度估计。*IEEE
    Transactions on Intelligent Transportation Systems* 20, 10 (2019), 3940–3951。
- en: 'Yuan et al. (2018) Zhuoning Yuan, Xun Zhou, and Tianbao Yang. 2018. Hetero-ConvLSTM:
    A Deep Learning Approach to Traffic Accident Prediction on Heterogeneous Spatio-Temporal
    Data. In *Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
    Discovery & Data Mining*. ACM, 984–992.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan et al. (2018) Zhuoning Yuan, Xun Zhou, 和 Tianbao Yang. 2018. Hetero-ConvLSTM:
    一种用于异质时空数据交通事故预测的深度学习方法。见于*第24届 ACM SIGKDD 国际知识发现与数据挖掘会议*。ACM, 984–992。'
- en: 'Zhang et al. (2018) Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King,
    and Dit Yan Yeung. 2018. GaAN: Gated Attention Networks for Learning on Large
    and Spatiotemporal Graphs. In *34th Conference on Uncertainty in Artificial Intelligence
    2018, UAI 2018*.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2018) Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King,
    和 Dit Yan Yeung. 2018. GaAN: 用于大规模时空图学习的门控注意力网络。见于*第34届人工智能不确定性会议 2018，UAI 2018*。'
- en: Zhang et al. (2017) Junbo Zhang, Yu Zheng, and Dekang Qi. 2017. Deep Spatio-Temporal
    Residual Networks for Citywide Crowd Flows Prediction.. In *AAAI*. 1655–1661.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2017) Junbo Zhang, Yu Zheng, 和 Dekang Qi. 2017. 城市范围人群流动预测的深度时空残差网络。见于*AAAI*。1655–1661。
- en: Zhang et al. (2016) Junbo Zhang, Yu Zheng, Dekang Qi, Ruiyuan Li, and Xiuwen
    Yi. 2016. DNN-based prediction model for spatio-temporal data. In *Proceedings
    of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic
    Information Systems*. ACM, 92.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2016) Junbo Zhang, Yu Zheng, Dekang Qi, Ruiyuan Li, 和 Xiuwen Yi.
    2016. 基于 DNN 的时空数据预测模型。见于*第24届 ACM SIGSPATIAL 国际地理信息系统进展会议*。ACM, 92。
- en: Zhang et al. (2019) Junbo Zhang, Yu Zheng, Junkai Sun, and Dekang Qi. 2019.
    Flow Prediction in Spatio-Temporal Networks Based on Multitask Deep Learning.
    *IEEE Transactions on Knowledge and Data Engineering* (2019).
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2019) Junbo Zhang, Yu Zheng, Junkai Sun, 和 Dekang Qi. 2019. 基于多任务深度学习的时空网络流量预测。*IEEE
    知识与数据工程学报* (2019)。
- en: Zhang et al. (2020a) Qi Zhang, Jianlong Chang, Gaofeng Meng, Shiming Xiang,
    and Chunhong Pan. 2020a. Spatio-temporal graph structure learning for traffic
    forecasting. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 34. 1177–1185.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020a) Qi Zhang, Jianlong Chang, Gaofeng Meng, Shiming Xiang,
    和 Chunhong Pan. 2020a. 用于交通预测的时空图结构学习。见于*AAAI 人工智能会议论文集*，第34卷。1177–1185。
- en: 'Zhang et al. (2020b) Yingxue Zhang, Yanhua Li, Xun Zhou, Xiangnan Kong, and
    Jun Luo. 2020b. Curb-GAN: Conditional Urban Traffic Estimation through Spatio-Temporal
    Generative Adversarial Networks. In *Proceedings of the 26th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*. 842–852.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2020b) Yingxue Zhang, Yanhua Li, Xun Zhou, Xiangnan Kong, 和 Jun
    Luo. 2020b. Curb-GAN: 通过时空生成对抗网络的城市交通条件估计。见于*第26届 ACM SIGKDD 国际知识发现与数据挖掘会议*。842–852。'
- en: 'Zhao et al. (2019) Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao
    Lin, Min Deng, and Haifeng Li. 2019. T-gcn: A temporal graph convolutional network
    for traffic prediction. *IEEE Transactions on Intelligent Transportation Systems*
    (2019).'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. (2019) Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao
    Lin, Min Deng, 和 Haifeng Li. 2019. T-gcn: 一种用于交通预测的时间图卷积网络。*IEEE 智能交通系统学报* (2019)。'
- en: 'Zheng et al. (2020) Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong
    Qi. 2020. Gman: A graph multi-attention network for traffic prediction. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 34. 1234–1241.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2020) Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, 和 Jianzhong
    Qi. 2020. Gman: 用于交通预测的图形多重注意力网络。见于*AAAI 人工智能会议论文集*，第34卷。1234–1241。'
- en: 'Zonoozi et al. (2018) Ali Zonoozi, Jung-jae Kim, Xiao-Li Li, and Gao Cong.
    2018. Periodic-CRN: A Convolutional Recurrent Model for Crowd Density Prediction
    with Recurring Periodic Patterns.. In *IJCAI*. 3732–3738.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zonoozi et al. (2018) Ali Zonoozi, Jung-jae Kim, Xiao-Li Li, 和 Gao Cong. 2018.
    Periodic-CRN: 用于人群密度预测的卷积递归模型与周期性模式。见于*IJCAI*。3732–3738。'
