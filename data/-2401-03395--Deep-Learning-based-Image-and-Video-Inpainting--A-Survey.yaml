- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:35:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:35:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2401.03395] Deep Learning-based Image and Video Inpainting: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2401.03395] 基于深度学习的图像和视频修复：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.03395](https://ar5iv.labs.arxiv.org/html/2401.03395)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.03395](https://ar5iv.labs.arxiv.org/html/2401.03395)
- en: '¹¹institutetext: D.-M. Yan'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构文本：D.-M. Yan
- en: 'yandongming@gmail.com ²²institutetext: MAIS & NLPR, Institute of Automation,
    Chinese Academy of Sciences, Beijing, China School of Artificial Intelligence,
    University of Chinese Academy of Sciences, Beijing, China College of Computer
    Science, Sichuan University, Chengdu, China Computer, Electrical and Mathematical
    Science and Engineering Division, King Abdullah University of Science and Technology,
    Thuwal, Saudi Arabia'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: yandongming@gmail.com ²²机构文本：自动化研究所，中国科学院，北京，中国 中国科学院大学人工智能学院，北京，中国 四川大学计算机学院，成都，中国
    国王阿卜杜拉科技大学计算机、电气和数学科学与工程部门，图瓦尔，沙特阿拉伯
- en: 'Deep Learning-based Image and Video Inpainting: A Survey'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的图像和视频修复：综述
- en: 'Weize Quan ^(1,2)    Jiaxi Chen ^(1,2)    Yanli Liu ³    Dong-Ming Yan ^(1,2, ✉)
       Peter Wonka ⁴(Received: date / Accepted: date)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Weize Quan ^(1,2)    Jiaxi Chen ^(1,2)    Yanli Liu ³    Dong-Ming Yan ^(1,2, ✉)
       Peter Wonka ⁴(收到日期：日期 / 接受日期：日期)
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Image and video inpainting is a classic problem in computer vision and computer
    graphics, aiming to fill in the plausible and realistic content in the missing
    areas of images and videos. With the advance of deep learning, this problem has
    achieved significant progress recently. The goal of this paper is to comprehensively
    review the deep learning-based methods for image and video inpainting. Specifically,
    we sort existing methods into different categories from the perspective of their
    high-level inpainting pipeline, present different deep learning architectures,
    including CNN, VAE, GAN, diffusion models, etc., and summarize techniques for
    module design. We review the training objectives and the common benchmark datasets.
    We present evaluation metrics for low-level pixel and high-level perceptional
    similarity, conduct a performance evaluation, and discuss the strengths and weaknesses
    of representative inpainting methods. We also discuss related real-world applications.
    Finally, we discuss open challenges and suggest potential future research directions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和视频修复是计算机视觉和计算机图形学中的经典问题，旨在填补图像和视频中缺失区域的合理和真实内容。随着深度学习的进步，这一问题最近取得了显著的进展。本文的目标是全面回顾基于深度学习的图像和视频修复方法。具体来说，我们从高层修复流程的角度对现有方法进行分类，介绍不同的深度学习架构，包括CNN、VAE、GAN、扩散模型等，并总结模块设计的技术。我们回顾了训练目标和常见的基准数据集。我们提出了低级像素和高级感知相似性的评估指标，进行性能评估，并讨论了代表性修复方法的优缺点。我们还讨论了相关的实际应用。最后，我们讨论了未解决的挑战，并建议了潜在的未来研究方向。
- en: 'Keywords:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'Image inpainting Video inpainting Deep learning Generation^†^†journal: International
    Journal of Computer Vision![Refer to caption](img/d3888a0030f516be40054b8cd73ceed8.png)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图像修复 视频修复 深度学习 生成^†^†期刊：国际计算机视觉期刊![参见说明](img/d3888a0030f516be40054b8cd73ceed8.png)
- en: 'Figure 1: Application examples of inpainting techniques: photo restoration
    (top left: image from (Bertalmio et al., [2000](#bib.bib7))), text removal (top
    right: image from (Bertalmio et al., [2000](#bib.bib7))), undesired target removal
    (bottom left: image from  (Chen, [2018](#bib.bib20))), and face verification (bottom
    right: image from  (Zhang et al., [2018c](#bib.bib259))).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：修复技术的应用示例：照片修复（左上：图像来自(Bertalmio et al., [2000](#bib.bib7)))，文本去除（右上：图像来自(Bertalmio
    et al., [2000](#bib.bib7)))，不需要的目标去除（左下：图像来自(Chen, [2018](#bib.bib20)))，以及面部验证（右下：图像来自(Zhang
    et al., [2018c](#bib.bib259))）。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Image and video inpainting (Masnou and Morel, [1998](#bib.bib137); Bertalmio
    et al., [2000](#bib.bib7)) refers to the task of restoring missing/occluded regions
    of a digital image or video with plausible and natural content. Inpainting is
    an underconstrained problem with multiple plausible solutions, especially if there
    are large missing regions. Inpainting has many important applications in multiple
    fields, such as cultural relic restoration, virtual scene editing, digital forensics,
    and film and television production, etc. Fig. [1](#S0.F1 "Figure 1 ‣ Deep Learning-based
    Image and Video Inpainting: A Survey") shows some important applications of inpainting
    techniques. Video is composed of multiple images exhibiting temporal coherence,
    therefore, video inpainting is closely related to image inpainting, where the
    former often learns from or extends the latter. For this reason, we simultaneously
    review image and video inpainting in this survey, and the number of papers is
    shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning-based Image
    and Video Inpainting: A Survey").'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和视频修复（Masnou 和 Morel, [1998](#bib.bib137); Bertalmio et al., [2000](#bib.bib7)）指的是用合理和自然的内容恢复数字图像或视频中缺失/遮挡区域的任务。修复是一个欠约束的问题，尤其是当存在大面积缺失区域时，可能有多种合理的解决方案。修复在多个领域有许多重要应用，如文化遗产修复、虚拟场景编辑、数字取证以及影视制作等。图
    [1](#S0.F1 "图 1 ‣ 基于深度学习的图像和视频修复：综述") 显示了修复技术的一些重要应用。视频由多个展示时间一致性的图像组成，因此，视频修复与图像修复密切相关，其中前者通常从后者学习或扩展。因此，我们在本综述中同时回顾了图像和视频修复，论文数量如图
    [2](#S1.F2 "图 2 ‣ 1 引言 ‣ 基于深度学习的图像和视频修复：综述") 所示。
- en: '![Refer to caption](img/58fa22ca36b7434d4d4b20a68422fcaf.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/58fa22ca36b7434d4d4b20a68422fcaf.png)'
- en: 'Figure 2: The rough number of papers on image and video inpainting per year.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：每年关于图像和视频修复的论文大致数量。
- en: Early image inpainting methods mainly depend on low-level features of corrupted
    images, including PDE-based methods (Bertalmio et al., [2000](#bib.bib7); Ballester
    et al., [2001](#bib.bib4); Tschumperlé and Deriche, [2005](#bib.bib185)) and patch-based
    methods (Efros and Leung, [1999](#bib.bib41); Barnes et al., [2009](#bib.bib6);
    Darabi et al., [2012](#bib.bib29); Huang et al., [2014](#bib.bib74); Herling and
    Broll, [2014](#bib.bib63); Guo et al., [2018](#bib.bib55)). PDE-based approaches
    usually propagate the information from the boundary to create a smooth inpainting.
    It is possible to propagate edge information, but it is hard to inpaint textures.
    Instead of only considering the boundary information, patch-based approaches recover
    the unknown regions by matching and duplicating similar patches of known regions.
    For smaller areas, patch-based methods can inpaint textures and also inpaint complete
    objects if similar objects are available in other image regions. However, these
    traditional methods have limited ability to generate new semantically plausible
    content, especially for large missing regions and missing regions that are not
    similar to other image regions. A comprehensive review on classical image inpainting
    methods is beyond our scope, and we refer readers to the surveys (Guillemot and
    Meur, [2014](#bib.bib54); Jam et al., [2021](#bib.bib82)) for more details.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的图像修复方法主要依赖于损坏图像的低级特征，包括基于 PDE 的方法（Bertalmio et al., [2000](#bib.bib7); Ballester
    et al., [2001](#bib.bib4); Tschumperlé 和 Deriche, [2005](#bib.bib185)）和基于补丁的方法（Efros
    和 Leung, [1999](#bib.bib41); Barnes et al., [2009](#bib.bib6); Darabi et al.,
    [2012](#bib.bib29); Huang et al., [2014](#bib.bib74); Herling 和 Broll, [2014](#bib.bib63);
    Guo et al., [2018](#bib.bib55)）。基于 PDE 的方法通常从边界传播信息以创建平滑的修复。虽然可以传播边缘信息，但修复纹理却较为困难。相比仅考虑边界信息，基于补丁的方法通过匹配和复制已知区域的相似补丁来恢复未知区域。对于较小的区域，基于补丁的方法可以修复纹理，如果其他图像区域有相似的对象，还能修复完整的对象。然而，这些传统方法在生成新的语义上合理内容方面能力有限，尤其是对于较大的缺失区域以及与其他图像区域不相似的缺失区域。对经典图像修复方法的全面回顾超出了我们的范围，我们建议读者参考综述（Guillemot
    和 Meur, [2014](#bib.bib54); Jam et al., [2021](#bib.bib82)）以获取更多细节。
- en: 'By contrast, deep learning holds the promise to inpaint large regions and also
    inpaint new plausible content that was learned from a larger set of images. In
    the beginning convolutional neural networks (CNNs) and generative adversarial
    networks (GANs) were the most popular choices in the inpainting literature. CNNs
    are a class of feed-forward neural networks that consist of convolutional, activation,
    and down-/up-sampling layers. They learn a highly non-linear mapping from the
    input image to the output image. GANs are a type of generative model consisting
    of a generator and a discriminator that estimates the data distribution through
    an adversarial process. Recently, more attention has been paid to the transformer
    architecture and generative diffusion models (Sohl-Dickstein et al., [2015](#bib.bib176);
    Ho et al., [2020](#bib.bib66)). Transformers are a prevalent network architecture
    based on parallel multi-head attention modules. Compared to the locality of CNNs,
    transformers have a better ability for contextual understanding. Diffusion probabilistic
    models are a type of latent variable model, which mainly contain the forward process,
    the reverse process, and the sampling procedure. Diffusion models learn to reverse
    a stochastic process (i.e., diffusion process) that progressively destroys data
    via adding noise. These deep learning-based image inpainting methods can achieve
    attractive results that surpass traditional methods in many aspects. From the
    perspective of the high-level inpainting pipeline, existing inpainting methods
    can be classified into three categories: a single-shot framework, a two-stage
    framework, and a progressive framework. Orthogonal to these main approaches, different
    technical methods can be observed in their realization, including mask-aware design,
    attention mechanisms, multi-scale aggregation, transform domain, deep prior guidance,
    multi-task learning, structure representations, loss functions, etc.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，深度学习有望修复大区域图像，并且能够通过学习更大的图像集合来修复新的合理内容。最初，在修复方面，卷积神经网络（CNNs）和生成对抗网络（GANs）是最受欢迎的选择。CNNs是一类由卷积层、激活层和下/上采样层组成的前馈神经网络。它们学习从输入图像到输出图像的高度非线性映射。GANs是一类生成模型，包括一个生成器和一个鉴别器，通过对抗过程估计数据分布。最近，更多的关注被放在了变换器架构和生成扩散模型上（Sohl-Dickstein
    et al.，[2015](#bib.bib176); Ho et al.，[2020](#bib.bib66)）。变换器是一种基于并行多头注意力模块的流行网络架构。与CNNs的局部性相比，变换器有更好的上下文理解能力。扩散概率模型是一种潜变量模型，主要包含正向过程、反向过程和采样过程。扩散模型学习逆转一个随机过程（即扩散过程），通过添加噪声逐渐破坏数据。这些基于深度学习的图像修复方法在许多方面都能取得比传统方法更有吸引力的结果。从高级修复流程的角度看，现有的修复方法可以分为三类：一次性框架、两阶段框架和渐进式框架。与这些主要方法正交的是，它们的实现中可以观察到不同的技术方法，包括面具感知设计、注意机制、多尺度聚合、变换域、深先验指导、多任务学习、结构表示、损失函数等。
- en: 'Compared with images, video data has an additional time dimension. Therefore,
    video inpainting not only fills in reasonable content in the missing regions for
    each frame but also aims to recover a temporally consistent solution. Because
    of this close relationship between image inpainting and video inpainting, many
    technical ideas used in image inpainting are often applied and extended to solve
    video inpainting tasks. Traditional video inpainting methods are usually based
    on patch sampling and synthesis (Wexler et al., [2007](#bib.bib209); Granados
    et al., [2012](#bib.bib52); Newson et al., [2014](#bib.bib140); Huang et al.,
    [2016](#bib.bib75)). These methods have limited ability to synthesize consistent
    content and capture complex motion and are often computationally expensive. To
    address these shortcomings, many deep learning-based methods have been proposed
    and achieved significant progress. There mainly exist four research directions:
    3D CNN-based methods, shift-based methods, flow-guided methods, and attention-based
    methods. The core idea of these methods is to transfer information from neighboring
    frames to the target frame. 3D CNNs are the direct extension of 2D CNNs and work
    in an end-to-end manner. However, they often suffer from spatial misalignment
    and high computational cost. Shift-based methods can address these limitations
    to some extent, but within a limited temporal window only. Flow-guided approaches
    can produce higher resolution and temporally consistent results but are vulnerable
    to imperfect optical flow completion due to occlusion and complex motion. Attention-based
    methods fuse known information from short and long distances. Unfortunately, inaccurate
    attention score estimation often leads to blurry results.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于图像，视频数据具有额外的时间维度。因此，视频修复不仅在每一帧的缺失区域填充合理的内容，还旨在恢复时间上连贯的解决方案。由于图像修复和视频修复之间的紧密关系，许多用于图像修复的技术思想通常被应用和扩展到视频修复任务中。传统的视频修复方法通常基于补丁采样和合成
    (Wexler 等，[2007](#bib.bib209); Granados 等，[2012](#bib.bib52); Newson 等，[2014](#bib.bib140);
    Huang 等，[2016](#bib.bib75))。这些方法在合成一致内容和捕捉复杂运动方面能力有限，并且通常计算开销较大。为了解决这些不足，提出了许多基于深度学习的方法，并取得了显著进展。主要存在四个研究方向：基于
    3D CNN 的方法、基于位移的方法、基于光流引导的方法以及基于注意力的方法。这些方法的核心思想是将来自邻近帧的信息转移到目标帧。3D CNN 是 2D CNN
    的直接扩展，工作方式为端到端。然而，它们通常会遭遇空间错位和高计算成本的问题。基于位移的方法在一定程度上可以解决这些限制，但仅限于有限的时间窗口。基于光流引导的方法可以产生更高分辨率和时间上连贯的结果，但由于遮挡和复杂运动，易受到光流不完善的影响。基于注意力的方法融合了来自短距离和长距离的已知信息。不幸的是，不准确的注意力得分估计往往导致模糊的结果。
- en: To our knowledge, there are several papers that review the deep learning-based
    inpainting works in the literature. Elharrouss et al. ([2020](#bib.bib42)) categorizes
    image inpainting methods into sequential-based, CNN-based, and GAN-based methods,
    and reviews related papers. To improve on their work, we also discuss common methodological
    approaches, loss functions, and evaluation metrics. We also add more discussion
    about further research directions and include newer work. Jam et al. ([2021](#bib.bib82))
    reviews the traditional and deep learning-based image inpainting methods. However,
    they paid much attention to the traditional methods but have significantly fewer
    deep learning-based works compared to our survey. Weng et al. ([2022](#bib.bib208))
    reviews some GAN-based image inpainting methods, but is generally shorter. Moreover,
    these existing surveys do not review the image and video inpainting simultaneously.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，有几篇论文回顾了文献中的深度学习修复工作。Elharrouss 等 ([2020](#bib.bib42)) 将图像修复方法分类为基于序列、基于
    CNN 和基于 GAN 的方法，并回顾了相关论文。为了改进他们的工作，我们还讨论了常见的方法论、损失函数和评估指标。我们还增加了对进一步研究方向的讨论，并包括了更新的工作。Jam
    等 ([2021](#bib.bib82)) 回顾了传统和基于深度学习的图像修复方法。然而，他们更关注传统方法，但相比我们的调查，深度学习方法显著较少。Weng
    等 ([2022](#bib.bib208)) 回顾了一些基于 GAN 的图像修复方法，但总体较短。此外，这些现有的调查没有同时回顾图像和视频修复。
- en: 2 Image Inpainting
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 图像修复
- en: 'For the restoration of missing regions in an image, the results sometimes are
    not unique, especially for large missing areas. Consequently, there mainly exists
    two lines of research in the literature: (1) deterministic image inpainting and
    (2) stochastic image inpainting. Given a corrupted image, deterministic image
    inpainting methods only output an inpainted result while stochastic image inpainting
    methods can output multiple plausible results with a random sampling process.
    Inspired by multi-modal learning, some researchers have recently focused on text-guided
    image inpainting by providing additional information with text prompts.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像中缺失区域的修复，结果有时不是唯一的，特别是对于大面积缺失区域。因此，文献中主要存在两个研究方向：（1）确定性图像修复和（2）随机图像修复。给定一个受损图像，确定性图像修复方法仅输出修复结果，而随机图像修复方法可以通过随机采样过程输出多个可能的结果。受多模态学习的启发，一些研究人员最近专注于通过文本提示提供额外信息的文本引导图像修复。
- en: 2.1 Deterministic Image Inpainting
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 确定性图像修复
- en: 'From the perspective of a high-level inpainting pipeline, existing works for
    deterministic image inpainting usually adopt three types of frameworks: single-shot,
    two-stage, and progressive. The single-shot framework usually adopts a generator
    network with a corrupted image as input and an inpainted image as output; The
    two-stage framework mainly consists of two generators, where the first generator
    achieves a rough result and then the second generator improves upon it; The progressive
    framework applies one or more generators to iteratively recover missing regions
    along the boundary.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从高级修复流程的角度来看，现有的确定性图像修复工作通常采用三种类型的框架：单次修复、两阶段和渐进式。单次修复框架通常采用一个生成器网络，以受损图像作为输入，以修复图像作为输出；两阶段框架主要由两个生成器组成，第一个生成器得到一个粗略结果，然后第二个生成器对此进行改进；渐进式框架应用一个或多个生成器迭代恢复沿边界的缺失区域。
- en: '![Refer to caption](img/12e66b143e909bd2f784204e149d64e1.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/12e66b143e909bd2f784204e149d64e1.png)'
- en: 'Figure 3: Representative pipeline of the single-shot inpainting framework.
    The generator takes as input the concatenation of a binary mask and a corrupted
    image and outputs the completed image. Training objectives are used for training
    the generator.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：单次修复框架的代表性流程。生成器将二进制掩模和受损图像的拼接作为输入，输出完成的图像。训练目标用于训练生成器。
- en: 2.1.1 Single-shot framework
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 单次修复框架
- en: 'Many existing inpainting methods adopt a single-shot framework, as shown in
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Deterministic Image Inpainting ‣ 2 Image Inpainting
    ‣ Deep Learning-based Image and Video Inpainting: A Survey"). It essentially learns
    a mapping from a corrupted image to a complete image. The framework usually consists
    of generators and corresponding training objectives.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现有的修复方法采用单次修复框架，如图[3](#S2.F3 "图3 ‣ 2.1 确定性图像修复 ‣ 2 图像修复 ‣ 基于深度学习的图像和视频修复：综述")所示。它基本上学习了从受损图像到完整图像的映射。该框架通常由生成器和相应的训练目标组成。
- en: 'Generators. To improve the inpainting ability of the generator, there exist
    several lines of research: mask-aware design, attention mechanism, multi-scale
    aggregation, transform domain, encoder-decoder connection, and deep prior guidance.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器。为了提高生成器的修复能力，存在几种研究方向：掩模感知设计、注意力机制、多尺度聚合、变换域、编码器-解码器连接和深度先验指导。
- en: (1) Mask-aware design.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 掩模感知设计。
- en: The missing regions (indicated with a binary mask) have different shapes and
    convolutional operations overlapping with these missing regions may be the source
    of visual artifacts. Therefore, some researchers proposed mask-aware solutions
    for classical convolutional operation and normalization. Inspired by the inherent
    spatially varying property of image inpainting, Ren et al. ([2015](#bib.bib157))
    designed a Shepard interpolation layer where the feature map and mask both conduct
    the same convolution operation. Its output is the fraction of feature convolution
    and mask convolution results. Mask convolution can simultaneously update the mask.
    To better handle various irregular holes and evolve the hole during mask updating,
    Liu et al. ([2018](#bib.bib122)) proposed a mask-guided convolution operation,
    i.e., partial convolution, which distinguishes between the valid region and hole
    in a convolutional window. Xie et al. ([2019](#bib.bib215)) proposed trainable
    bidirectional attention maps to extend the partial convolution (Liu et al., [2018](#bib.bib122)),
    which can adaptively learn the feature re-normalization and mask-updating.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失的区域（用二进制掩码表示）具有不同的形状，与这些缺失区域重叠的卷积操作可能是视觉伪影的来源。因此，一些研究人员提出了针对经典卷积操作和归一化的掩码感知解决方案。受到图像修复中固有空间变化特性的启发，Ren
    等人 ([2015](#bib.bib157)) 设计了一个 Shepard 插值层，其中特征图和掩码都进行相同的卷积操作。它的输出是特征卷积和掩码卷积结果的比例。掩码卷积可以同时更新掩码。为了更好地处理各种不规则孔洞并在掩码更新过程中演变孔洞，Liu
    等人 ([2018](#bib.bib122)) 提出了掩码引导卷积操作，即部分卷积，它区分了卷积窗口中的有效区域和孔洞。Xie 等人 ([2019](#bib.bib215))
    提出了可训练的双向注意力图，以扩展部分卷积 (Liu 等人，[2018](#bib.bib122))，这种方法可以自适应地学习特征的重新归一化和掩码更新。
- en: Different from the feature normalization considered by previous methods, Yu
    et al. ([2020](#bib.bib235)) focused on the mean and variance shift-related normalization
    and introduced a spatial region-wise normalization into the inpainting network.
    Wang et al. ([2020c](#bib.bib205)) designed a visual consistency network for blind
    image inpainting. They first predicted the damaged regions yielding a mask, and
    then applied an inpainting network with the proposed probabilistic context normalization,
    which transfers the mean and variance from known features to unknown parts building
    on different layers. Inspired by filling holes with pixel priorities (Criminisi
    et al., [2004](#bib.bib26); Zhang et al., [2019b](#bib.bib249)), Wang et al. ([2021c](#bib.bib201))
    used a structure priority (in low-resolution features) and a texture priority
    (in high-resolution features) in partial convolution (Liu et al., [2018](#bib.bib122)).
    Wang et al. ([2021a](#bib.bib197)) proposed a dynamic selection network to utilize
    the valid pixels better. Specifically, they designed a validness migratable convolution
    to dynamically sample the convolutional locations, and a regional composite normalization
    module to adaptively composite batch, instance, and layer normalization on mask-based
    selective feature maps. Zhu et al. ([2021](#bib.bib273)) learned to derive the
    convolutional kernel from the mask for each convolutional window and proposed
    a point-wise normalization that produces the mask-aware scale and bias for batch
    normalization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前方法考虑的特征归一化不同，Yu 等人 ([2020](#bib.bib235)) 关注于均值和方差偏移相关的归一化，并将空间区域级归一化引入到修复网络中。Wang
    等人 ([2020c](#bib.bib205)) 设计了一个视觉一致性网络用于盲图像修复。他们首先预测了损坏区域生成掩码，然后应用了带有提议的概率上下文归一化的修复网络，该网络基于不同层将均值和方差从已知特征转移到未知部分。受到用像素优先级填补孔洞的启发
    (Criminisi 等人，[2004](#bib.bib26); Zhang 等人，[2019b](#bib.bib249))，Wang 等人 ([2021c](#bib.bib201))
    在部分卷积 (Liu 等人，[2018](#bib.bib122)) 中使用了结构优先级（在低分辨率特征中）和纹理优先级（在高分辨率特征中）。Wang 等人
    ([2021a](#bib.bib197)) 提出了一个动态选择网络，以更好地利用有效像素。具体而言，他们设计了一种有效性可迁移卷积，以动态地采样卷积位置，以及一个区域复合归一化模块，以自适应地在基于掩码的选择性特征图上复合批次、实例和层归一化。Zhu
    等人 ([2021](#bib.bib273)) 学习了从掩码中推导每个卷积窗口的卷积核，并提出了一种点-wise 归一化，生成掩码感知的批量归一化的尺度和偏置。
- en: (2) Attention mechanism.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 注意力机制。
- en: Attention is a prevalent tool to model correlation in the field of natural language
    processing Vaswani et al. ([2017](#bib.bib187)) and computer vision (Wang et al.,
    [2018b](#bib.bib203); Fu et al., [2019](#bib.bib47)). Attention is better at accessing
    features of distant spatial locations than convolution. In the literature, Yu
    et al. ([2018](#bib.bib233)) was the first to introduce a contextual attention
    mechanism to image inpainting. This pioneering work inspired many following works.
    To enhance both visual and semantic coherence, Zeng et al. ([2019](#bib.bib240))
    proposed a pyramid-context encoder network with an attention transfer method,
    where the attention score computed in a high-level feature is used for low-level
    feature updating. Instead of using one fixed patch size for attention computation,
    Wang et al. ([2019b](#bib.bib195)) proposed a multi-scale contextual attention
    model with two different patch sizes followed by a channel attention block (Hu
    et al., [2018](#bib.bib72)). Wang et al. ([2020b](#bib.bib196)) introduced a multistage
    attention module that performs large patch swapping in the first stage and small
    patch swapping in the next stage. Qin et al. ([2021](#bib.bib152)) combined spatial-channel
    attention (Chen et al., [2017](#bib.bib19)) and a spatial pyramid structure to
    construct a multi-scale attention unit (MSAU). This unit separately conducts spatial
    attention on four feature maps obtained by different dilation convolutions and
    then applies augmented channel attention on concatenated attentive features. Zhang
    et al. ([2022e](#bib.bib258)) proposed a structure and texture interaction network
    for image inpainting. They designed a texture spatial attention module to recover
    texture details with robust attention scores guided by coarse structures and introduced
    a structure channel excitation module to recalibrate structures according to the
    difference between coarse and refined structures.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制是自然语言处理领域（Vaswani et al. ([2017](#bib.bib187))）和计算机视觉领域（Wang et al., [2018b](#bib.bib203);
    Fu et al., [2019](#bib.bib47)）中一种广泛使用的工具来建模相关性。注意力机制比卷积更擅长访问远距离的空间位置特征。在文献中，Yu
    et al. ([2018](#bib.bib233)) 首次将上下文注意力机制引入图像修复。这项开创性工作激发了许多后续研究。为了增强视觉和语义的一致性，Zeng
    et al. ([2019](#bib.bib240)) 提出了一个金字塔上下文编码网络与注意力转移方法，其中在高层特征中计算的注意力分数用于低层特征的更新。Wang
    et al. ([2019b](#bib.bib195)) 提出了一个多尺度上下文注意力模型，采用两种不同的补丁大小，并随后使用了通道注意力块（Hu et
    al., [2018](#bib.bib72)）。Wang et al. ([2020b](#bib.bib196)) 引入了一个多阶段注意力模块，该模块在第一阶段执行大补丁交换，在下一阶段执行小补丁交换。Qin
    et al. ([2021](#bib.bib152)) 结合了空间-通道注意力（Chen et al., [2017](#bib.bib19)）和空间金字塔结构来构建一个多尺度注意力单元（MSAU）。该单元分别对由不同膨胀卷积获得的四个特征图进行空间注意力，然后对拼接的注意力特征应用增强的通道注意力。Zhang
    et al. ([2022e](#bib.bib258)) 提出了一个结构与纹理交互网络用于图像修复。他们设计了一个纹理空间注意力模块，通过粗略结构引导的强健注意力分数来恢复纹理细节，并引入了一个结构通道激发模块，根据粗糙结构和精细结构之间的差异来重新校准结构。
- en: In addition, some recent works proposed image inpainting networks based on vision
    transformers (Dosovitskiy et al., [2021](#bib.bib39)). Deng et al. ([2021](#bib.bib33))
    proposed a contextual transformer network to complete the corrupted images. Their
    network mainly depends on the multi-scale multi-sub-head attention, which is extended
    from the original multi-head attention proposed by (Vaswani et al., [2017](#bib.bib187)).
    Cao et al. ([2022](#bib.bib12)) incorporated rich prior information from the ViT-based
    masked autoencoder (MAE) (He et al., [2022](#bib.bib62)) into image inpainting.
    Specifically, the pre-trained MAE model provides the features prior to the encoder
    of the inpainting network and the attention prior to making the long-distance
    relationship modeling easier. Instead of using shallow projections or large receptive
    field convolutions to sequence the incomplete image, Zheng et al. ([2022a](#bib.bib267))
    designed a restrictive CNN head with a small and non-overlapping receptive field
    as token representation. Deng et al. ([2022](#bib.bib34)) modified multi-head
    self-attention by inserting a Laplace distance prior, which computes the similarity
    considering the features and their locations simultaneously.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些最近的研究提出了基于视觉变换器的图像修复网络（Dosovitskiy 等，[2021](#bib.bib39)）。Deng 等人（[2021](#bib.bib33)）提出了一种上下文变换器网络来完成损坏的图像。他们的网络主要依赖于多尺度多子头注意力，这种注意力是从（Vaswani
    等人，[2017](#bib.bib187)）提出的原始多头注意力扩展而来的。Cao 等人（[2022](#bib.bib12)）将来自基于 ViT 的掩码自编码器（MAE）（He
    等人，[2022](#bib.bib62)）的丰富先验信息融入图像修复中。具体来说，预训练的 MAE 模型提供了修复网络编码器的特征先验，并提供了注意力先验，以简化长距离关系建模。Zheng
    等人（[2022a](#bib.bib267)）设计了一种具有小且不重叠感受野的限制性 CNN 头作为标记表示，而不是使用浅层投影或大接收域卷积来对不完整图像进行排序。Deng
    等人（[2022](#bib.bib34)）通过插入拉普拉斯距离先验修改了多头自注意力，这种先验同时考虑了特征及其位置来计算相似性。
- en: (3) Multi-scale aggregation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 多尺度聚合。
- en: In the literature on image processing, multi-scale aggregation is a common method
    to fuse information from different resolutions. Wang et al. ([2018c](#bib.bib204))
    designed a generative multi-column inpainting network, consisting of three convolution
    branches with different filter kernel sizes, to fuse multi-scale feature representations.
    To create a smooth transition between the inpainted regions with existing content,
    Hong et al. ([2019](#bib.bib68)) proposed a deep fusion network with multiple
    fusion modules and reconstruction loss applied on multi-scale layers. The fusion
    module merged predicted content with the input image via a learnable alpha composition.
    Hui et al. ([2020](#bib.bib77)) proposed a dense multi-scale fusion module, which
    fuses hierarchical features obtained by multiple convolutional branches with different
    dilation rates. Zheng et al. ([2021b](#bib.bib268)) designed a progressive multi-scale
    fusion module to extract multi-scale features in parallel and progressively fuse
    these features, yielding more representative local features. Inspired by the high-resolution
    network (HRNet) for visual recognition (Sun et al., [2019](#bib.bib181); Wan et al.,
    [2021](#bib.bib191)), Wang et al. ([2021c](#bib.bib201)) introduced a parallel
    multi-resolution fusion network for image inpainting. This network can simultaneously
    conduct inpainting in multiple resolutions with mask-aware and attention-guided
    representation fusion methods. Phutke and Murala ([2021](#bib.bib151)) also followed
    a multi-path design, where they introduce four concurrent branches with different
    resolutions in the encoder. A residual module with diverse receptive fields is
    designed as the building block of the encoder. Cao and Fu ([2021](#bib.bib11))
    proposed a multi-scale sketch tensor network for man-made scene inpainting. This
    network reconstructs different types of structures by adding constraints on predicted
    lines, edges, and coarse images with different scales. Different from the mask-blind
    processing (Li et al., [2020b](#bib.bib104); Qin et al., [2021](#bib.bib152))
    of multi-scale features produced by convolution with different dilation rates,
    Zeng et al. ([2022](#bib.bib245)) carefully designed a gated residual connection,
    which considers the difference between holes and valid regions. They also proposed
    a soft mask-guided PatchGAN, where the discriminator is trained to predict the
    soft mask obtained by Gaussian filtering.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像处理文献中，多尺度聚合是一种常见的方法，用于融合来自不同分辨率的信息。王等人（[2018c](#bib.bib204)）设计了一个生成式多列修补网络，由三个具有不同滤波器核大小的卷积分支组成，以融合多尺度特征表示。为了在修补区域与现有内容之间创建平滑过渡，洪等人（[2019](#bib.bib68)）提出了一种深度融合网络，该网络具有多个融合模块，并在多尺度层上应用重建损失。融合模块通过可学习的
    alpha 组合将预测内容与输入图像合并。汇等人（[2020](#bib.bib77)）提出了一种密集多尺度融合模块，该模块通过多个具有不同膨胀率的卷积分支融合分层特征。郑等人（[2021b](#bib.bib268)）设计了一种渐进式多尺度融合模块，以并行方式提取多尺度特征，并逐步融合这些特征，从而产生更具代表性的局部特征。受高分辨率网络（HRNet）用于视觉识别的启发（孙等人，[2019](#bib.bib181)；万等人，[2021](#bib.bib191)），王等人（[2021c](#bib.bib201)）引入了一种并行多分辨率融合网络用于图像修补。该网络可以同时在多个分辨率下进行修补，采用基于掩模感知和注意力引导的表示融合方法。Phutke
    和 Murala（[2021](#bib.bib151)）也采用了多路径设计，其中在编码器中引入了四个具有不同分辨率的并发分支。一个具有多样接收场的残差模块被设计为编码器的构建块。曹和傅（[2021](#bib.bib11)）提出了一种用于人工场景修补的多尺度草图张量网络。该网络通过对预测的线条、边缘和不同尺度的粗略图像添加约束，重建不同类型的结构。不同于通过具有不同膨胀率的卷积处理的掩模盲处理（李等人，[2020b](#bib.bib104)；秦等人，[2021](#bib.bib152)），曾等人（[2022](#bib.bib245)）精心设计了一种门控残差连接，考虑了孔洞和有效区域之间的差异。他们还提出了一种软掩模引导的
    PatchGAN，其中判别器被训练来预测通过高斯滤波获得的软掩模。
- en: (4) Transform domain.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 变换域。
- en: Instead of conducting image inpainting in the spatial domain, some existing
    works designed inpainting frameworks in a transformed domain via the DWT (discrete
    wavelet transform) (Daubechies, [1990](#bib.bib30)) and the FFT (fast Fourier
    transform). Wang et al. ([2020a](#bib.bib194)) recast the image inpainting problem
    as predicting low-frequency semantic structures and high-frequency texture details.
    Specifically, they decomposed the corrupted image into different frequency components
    via the Haar wavelet transform (Mallat, [1989](#bib.bib135)), designed a multi-frequency
    probabilistic inference model to predict the frequency content in missing regions,
    and inversely transformed back to image space. Yu et al. ([2021a](#bib.bib236))
    adopted a similar inpainting pipeline. For the multi-frequency completion, they
    proposed a frequency region attentive normalization module to align and fuse the
    features with different frequencies and applied two discriminators to two high-frequency
    streams. Li et al. ([2021](#bib.bib103)) extracted high-frequency subbands as
    the texture and introduced a DWT loss to constrain the fidelity of low- and high-frequency
    subbands. LaMa (Suvorov et al., [2022](#bib.bib183)) combined the residual design (He
    et al., [2016](#bib.bib61)) and fast Fourier convolution (Chi et al., [2020](#bib.bib22))
    to construct a fast Fourier convolution residual block, which is integrated into
    the encoder-decoder network to handle large mask inpainting. Lu et al. ([2022](#bib.bib131))
    further improved LaMa by introducing various types of masks and adding the focal
    frequency loss (Jiang et al., [2021](#bib.bib83)) to constrain the spectrum of
    the images.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与其在空间域中进行图像修补，一些现有的工作设计了通过离散小波变换（DWT）（Daubechies, [1990](#bib.bib30)）和快速傅里叶变换（FFT）的变换域中的修补框架。Wang
    等（[2020a](#bib.bib194)）将图像修补问题重新定义为预测低频语义结构和高频纹理细节。具体来说，他们通过Haar小波变换（Mallat, [1989](#bib.bib135)）将受损图像分解为不同的频率成分，设计了一个多频率概率推断模型以预测缺失区域中的频率内容，并反向变换回图像空间。Yu
    等（[2021a](#bib.bib236)）采用了类似的修补流程。对于多频率完成，他们提出了一种频率区域注意归一化模块，用于对齐和融合不同频率的特征，并对两个高频流应用了两个鉴别器。Li
    等（[2021](#bib.bib103)）提取了高频子带作为纹理，并引入了DWT损失以约束低频和高频子带的保真度。LaMa（Suvorov 等，[2022](#bib.bib183)）结合了残差设计（He
    等，[2016](#bib.bib61)）和快速傅里叶卷积（Chi 等，[2020](#bib.bib22)）来构建一个快速傅里叶卷积残差块，并将其集成到编码器-解码器网络中，以处理大面积的掩模修补。Lu
    等（[2022](#bib.bib131)）通过引入各种类型的掩模并添加焦点频率损失（Jiang 等，[2021](#bib.bib83)）来进一步改进LaMa，以约束图像的频谱。
- en: (5) Encoder-decoder connection.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 编码器-解码器连接。
- en: Some works modify the basic encoder-decoder architecture by introducing carefully
    designed feature connections. Shift-Net (Yan et al., [2018](#bib.bib226)) modified
    the U-Net architecture by introducing a specific shift-connection layer, which
    shifts the encoder features of the valid region to the missing regions with a
    guidance loss. Dolhansky and Ferrer ([2018](#bib.bib37)) introduced an eye inpainting
    network that merges the identifying information of the reference image encoding
    as a code. Shen et al. ([2019](#bib.bib172)) designed a densely connected generative
    network for semantic image inpainting. They combined four symmetric U-Nets with
    dense skip connections. Liu et al. ([2020](#bib.bib124)) introduced a mutual encoder-decoder
    CNN, fusing the texture and structure features (from the shallow and deep layers
    of the encoder), to jointly restore the structure and texture with feature equalization.
    Similarly, Guo et al. ([2021](#bib.bib56)) designed a two-stream image inpainting
    network, which combines a structure-constrained texture synthesis submodel and
    a texture-guided structure reconstruction submodel. In addition, they introduced
    a bi-directional gated feature fusion module and a contextual feature aggregation
    module to fuse and refine the resulting images. Feng et al. ([2022](#bib.bib46))
    inserted generative memory into the classical encoder-decoder network to jointly
    exploit the high-level semantic reasoning and the pixel-level content reasoning.
    Based on (Liu et al., [2020](#bib.bib124)), Liu et al. ([2022](#bib.bib127)) inferred
    the texture and structure with a content-consistent reference image through a
    feature alignment module.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作通过引入精心设计的特征连接来修改基本的编码器-解码器架构。Shift-Net (Yan et al., [2018](#bib.bib226))
    通过引入特定的移位连接层来修改 U-Net 架构，该层将有效区域的编码器特征移位到缺失区域，并使用引导损失。Dolhansky 和 Ferrer ([2018](#bib.bib37))
    引入了一种眼睛修复网络，该网络将参考图像编码的识别信息合并为代码。Shen et al. ([2019](#bib.bib172)) 设计了一种密集连接的生成网络用于语义图像修复。他们结合了四个对称的
    U-Nets 和密集跳跃连接。Liu et al. ([2020](#bib.bib124)) 引入了一种相互编码器-解码器 CNN，融合了纹理和结构特征（来自编码器的浅层和深层），以特征均衡来联合恢复结构和纹理。类似地，Guo
    et al. ([2021](#bib.bib56)) 设计了一种双流图像修复网络，结合了结构约束的纹理合成子模型和纹理引导的结构重建子模型。此外，他们引入了双向门控特征融合模块和上下文特征聚合模块，以融合和优化生成的图像。Feng
    et al. ([2022](#bib.bib46)) 在经典的编码器-解码器网络中插入了生成记忆，以共同利用高级语义推理和像素级内容推理。基于 (Liu
    et al., [2020](#bib.bib124))，Liu et al. ([2022](#bib.bib127)) 通过特征对齐模块从内容一致的参考图像中推断纹理和结构。
- en: (6) Deep prior guidance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 深度先验指导。
- en: To enhance the performance of the inpainting generator, some works have explored
    the deep prior from a single image or a large image database. Lempitsky et al.
    ([2018](#bib.bib99)) utilized a randomly-initialized generator network as the
    prior to completing the corrupted image by only reconstructing the known regions.
    Gu et al. ([2020](#bib.bib53)) proposed mGANprior by incorporating a pre-trained
    GAN as prior for image inpainting. Specifically, this method reconstructs the
    incorrupt regions while filling in the missing areas by adaptively merging multiple
    generative feature maps from different latent codes. Richardson et al. ([2021](#bib.bib160))
    developed a pixel2style2pixel (pSp) framework for image inpainting. They introduced
    an encoder consisting of a feature pyramid and multiple mapping networks to encode
    the damaged image into extended latent space $\mathcal{W}+$ (18 512-dimensional
    style vectors), which is the extension of latent space $\mathcal{W}$ (Karras et al.,
    [2019](#bib.bib87)), and reused a pre-trained StyleGAN generator as priors to
    achieve the complete image. To handle the large missing regions and complex semantics,
    Wang et al. ([2022b](#bib.bib202)) designed a dual-path image inpainting framework
    with GAN inversion (Xia et al., [2022](#bib.bib214)). Given a corrupted image,
    the inversion path infers the close latent code and extracts the corresponding
    multi-layer features from the trained GAN model, and the feed-forward path fills
    the missing regions by merging the above semantic priors with a deformable fusion
    module. To guarantee the invariance of the valid area in the corrupted and completed
    images, Yu et al. ([2022b](#bib.bib239)) modified the GAN inversion pipeline (Richardson
    et al., [2021](#bib.bib160)) by designing the mapping network with a pre-modulation
    module and introducing $\mathcal{F}\&amp;\mathcal{W}+$ latent space, where $\mathcal{F}$
    are the feature maps of the corrupted image.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高修复生成器的性能，一些研究探索了来自单张图像或大型图像数据库的深度先验。Lempitsky 等人 ([2018](#bib.bib99)) 利用随机初始化的生成网络作为先验，通过仅重建已知区域来完成受损图像。Gu
    等人 ([2020](#bib.bib53)) 提出了 mGANprior，通过将预训练的 GAN 作为图像修复的先验。具体而言，该方法在填补缺失区域时，通过自适应地融合来自不同潜在代码的多个生成特征图来重建未受损区域。Richardson
    等人 ([2021](#bib.bib160)) 开发了一个用于图像修复的 pixel2style2pixel (pSp) 框架。他们引入了一个包含特征金字塔和多个映射网络的编码器，将受损图像编码为扩展的潜在空间
    $\mathcal{W}+$（18 个 512 维的风格向量），这是潜在空间 $\mathcal{W}$ (Karras 等人，[2019](#bib.bib87))
    的扩展，并重复使用预训练的 StyleGAN 生成器作为先验以实现完整图像。为了处理大面积缺失区域和复杂的语义，Wang 等人 ([2022b](#bib.bib202))
    设计了一个带有 GAN 反演的双路径图像修复框架 (Xia 等人，[2022](#bib.bib214))。给定一张受损图像，反演路径推断出接近的潜在代码并从训练好的
    GAN 模型中提取相应的多层特征，前馈路径通过将上述语义先验与变形融合模块合并来填补缺失区域。为了保证受损图像和完成图像中有效区域的一致性，Yu 等人 ([2022b](#bib.bib239))
    通过设计具有预调制模块的映射网络并引入 $\mathcal{F}\&\mathcal{W}+$ 潜在空间来修改 GAN 反演管道 (Richardson 等人，[2021](#bib.bib160))，其中
    $\mathcal{F}$ 是受损图像的特征图。
- en: Training objectives. The training objective is a very important component of
    deep learning-based image inpainting methods. Pixel-wise reconstruction loss,
    perceptual loss (Johnson et al., [2016](#bib.bib84)), style loss (Gatys et al.,
    [2016](#bib.bib50)), and adversarial loss (Goodfellow et al., [2014](#bib.bib51))
    are the prevalent training objectives. The adversarial loss is obtained by a discriminator
    network. Pathak et al. ([2016](#bib.bib148)) and Li et al. ([2019b](#bib.bib106))
    adopted the discriminator (stacked convolution and down-sampling) from DCGAN (Radford
    et al., [2016](#bib.bib155)). Considering Pathak et al. ([2016](#bib.bib148))’s
    method struggles to maintain local consistency with the surrounding regions, Iizuka
    et al. ([2017](#bib.bib78)) proposed local and global discriminators, which generate
    more realistic contents. Yu et al. ([2018](#bib.bib233)) proposed a patch-based
    discriminator, which can be regarded as the generalized version of local and global
    discriminators (Iizuka et al., [2017](#bib.bib78)). This patch-based discriminator
    is subsequently used in many following works. Liu et al. ([2021c](#bib.bib128))
    designed two discriminators with small- and large-scale receptive fields to guide
    the inpainting network for fine-grained image detail generation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练目标。训练目标是基于深度学习的图像修复方法中一个非常重要的组成部分。逐像素重建损失、感知损失（Johnson et al., [2016](#bib.bib84)）、风格损失（Gatys
    et al., [2016](#bib.bib50)）和对抗损失（Goodfellow et al., [2014](#bib.bib51)）是普遍使用的训练目标。对抗损失是通过判别网络获得的。Pathak
    et al. ([2016](#bib.bib148)) 和 Li et al. ([2019b](#bib.bib106)) 采用了来自 DCGAN（Radford
    et al., [2016](#bib.bib155)）的判别器（堆叠卷积和下采样）。考虑到 Pathak et al. ([2016](#bib.bib148))
    方法在与周围区域保持局部一致性方面的困难，Iizuka et al. ([2017](#bib.bib78)) 提出了局部和全局判别器，生成了更为真实的内容。Yu
    et al. ([2018](#bib.bib233)) 提出了基于补丁的判别器，这可以视为局部和全局判别器的广义版本（Iizuka et al., [2017](#bib.bib78)）。这种基于补丁的判别器随后在许多后续工作中被使用。Liu
    et al. ([2021c](#bib.bib128)) 设计了两个具有小尺度和大尺度感受野的判别器，以引导修复网络生成细粒度的图像细节。
- en: Besides, researchers have also introduced some carefully designed losses. Li
    et al. ([2017](#bib.bib111)) introduced a semantic parsing loss for face completion.
    Yeh et al. ([2017](#bib.bib230)) proposed context and prior losses to search the
    closest encoding in the latent image manifold for inferring the missing content.
    Vo et al. ([2018](#bib.bib188)) proposed a structural reconstruction loss, which
    is the combination of reconstruction errors in pixel and feature space. For explicitly
    exploring the structural and textural coherence between filled contents and their
    surrounding contexts, Li et al. ([2019a](#bib.bib100)) utilized the local intrinsic
    dimensionality (Houle, [2017a](#bib.bib70), [b](#bib.bib71)) in the image- and
    patch-level to measure and constrain the alignment between data submanifolds of
    inpainted contents and those of the valid pixels. To stabilize the training process
    of face inpainting, i.e., weakening gradient vanishing and model collapse, Han
    and Wang ([2021](#bib.bib58)) trained the generator via neuro-evolution and optimized
    the generator’s parameters by mutation and crossover.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究人员还引入了一些精心设计的损失函数。Li et al. ([2017](#bib.bib111)) 为面部补全引入了语义解析损失。Yeh et
    al. ([2017](#bib.bib230)) 提出了上下文和先验损失，用于在潜在图像流形中搜索最接近的编码，以推断缺失的内容。Vo et al. ([2018](#bib.bib188))
    提出了结构重建损失，这是一种像素和特征空间重建误差的组合。为了明确探索填充内容与其周围上下文之间的结构和纹理一致性，Li et al. ([2019a](#bib.bib100))
    利用图像和补丁级别的局部内在维度（Houle, [2017a](#bib.bib70), [b](#bib.bib71)）来测量和约束修复内容的数据子流形与有效像素的对齐。为了稳定面部修复的训练过程，即减弱梯度消失和模型崩溃，Han
    和 Wang ([2021](#bib.bib58)) 通过神经进化训练生成器，并通过突变和交叉优化生成器的参数。
- en: Some researchers introduced additional training objectives via multi-task learning.
    Liao et al. ([2018a](#bib.bib114)) presented a novel collaborative framework by
    training a generator simultaneously on multiple tasks, i.e., face completion,
    landmark detection, and semantic parsing. To enhance the inpainting capability
    of the network for image structure, Yang et al. ([2020](#bib.bib228)) designed
    a structure restoration branch in the decoder and explicitly inserted the structure
    features into the primary inpainting process. Appropriate semantic guidance is
    a suitable tool for image inpainting (Song et al., [2018b](#bib.bib179)), inspired
    by this, Liao et al. ([2020](#bib.bib116), [2021b](#bib.bib118), [2021a](#bib.bib117))
    proposed a unified framework to jointly predict the segmentation maps and recover
    the corrupted images. Specifically, Liao et al. ([2020](#bib.bib116), [2021b](#bib.bib118))
    designed a semantic guidance and evaluation network that iteratively updates and
    evaluates a semantic map and infers the missing contents in multiple scales. However,
    this method may create implausible textures and blurry boundaries, especially
    on mixed semantic regions. To solve this problem, Liao et al. ([2021a](#bib.bib117))
    devised a semantic-wise attention propagation module to apply the attention operation
    on the same semantic regions. They also introduced two coherence losses to constrain
    the consistency between the semantic map and the structure and texture of the
    inpainted image. Zhang et al. ([2020b](#bib.bib257)) studied how to improve the
    visual quality of inpainted images and proposed a pixel-wise dense detector for
    image inpainting. This detection-based framework can localize the artifacts of
    completed images, and the corresponding position information is combined with
    the reconstruction loss to better guide the training of the inpainting network.
    Zhang et al. ([2021](#bib.bib260)) introduced the semantic prior estimation as
    a pretext task with a pre-trained multi-label classification model, and then utilized
    the learned semantic priors to guide the inpainting process through a spatially-adaptive
    normalization module (Park et al., [2019](#bib.bib146)). Yu et al. ([2022a](#bib.bib238))
    jointly solved image reconstruction, semantic segmentation, and edge texture generation.
    Each branch is implemented with a transformer network, and a multi-scale spatial-aware
    attention block is developed to guide the main image inpainting branch from the
    other two branches. Similar to (Zhang et al., [2020b](#bib.bib257)), Zhang et al.
    ([2022d](#bib.bib255)) first localized the perceptual artifacts from the completed
    image, and then used this information to guide the iterative refinement process.
    They also manually annotated an inpainting artifact dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员通过多任务学习引入了额外的训练目标。Liao 等人 ([2018a](#bib.bib114)) 提出了一个新颖的协作框架，通过在多个任务上同时训练生成器，即面部补全、地标检测和语义解析。为了增强网络在图像结构方面的修复能力，Yang
    等人 ([2020](#bib.bib228)) 设计了一个在解码器中的结构恢复分支，并将结构特征明确插入到主要的修复过程中。适当的语义指导是图像修复的一个合适工具（Song
    等人，[2018b](#bib.bib179)），受此启发，Liao 等人 ([2020](#bib.bib116), [2021b](#bib.bib118),
    [2021a](#bib.bib117)) 提出了一个统一框架来共同预测分割图和恢复损坏的图像。具体而言，Liao 等人 ([2020](#bib.bib116),
    [2021b](#bib.bib118)) 设计了一个语义指导和评估网络，迭代更新和评估语义图，并在多个尺度上推断缺失内容。然而，这种方法可能会在混合语义区域产生不切实际的纹理和模糊的边界。为了解决这个问题，Liao
    等人 ([2021a](#bib.bib117)) 设计了一个语义级注意力传播模块，以在相同的语义区域应用注意力操作。他们还引入了两个一致性损失，以约束语义图与修复图像的结构和纹理之间的一致性。Zhang
    等人 ([2020b](#bib.bib257)) 研究了如何提高修复图像的视觉质量，并提出了一个像素级密集检测器用于图像修复。该基于检测的框架可以定位完成图像的伪影，相关的位置信息与重建损失结合，以更好地指导修复网络的训练。Zhang
    等人 ([2021](#bib.bib260)) 将语义先验估计引入作为预训练的多标签分类模型的前提任务，然后利用学习到的语义先验通过空间自适应归一化模块指导修复过程（Park
    等人，[2019](#bib.bib146)）。Yu 等人 ([2022a](#bib.bib238)) 联合解决了图像重建、语义分割和边缘纹理生成。每个分支都通过一个变换器网络实现，并开发了一个多尺度空间感知注意力块，从其他两个分支指导主要的图像修复分支。类似于
    (Zhang 等人，[2020b](#bib.bib257))，Zhang 等人 ([2022d](#bib.bib255)) 首先定位了完成图像中的感知伪影，然后利用这些信息指导迭代优化过程。他们还手动注释了一个修复伪影数据集。
- en: '![Refer to caption](img/1aba9ba08b6141b44f83a676a481cd1a.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1aba9ba08b6141b44f83a676a481cd1a.png)'
- en: (a) Coarse-to-fine
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 粗到细
- en: '![Refer to caption](img/cde0aa2ec01e8337079958da1df642fc.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cde0aa2ec01e8337079958da1df642fc.png)'
- en: (b) Structure-then-texture
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 结构先于纹理
- en: 'Figure 4: Two types of the two-stage inpainting framework: (a) coarse-to-fine (Yu
    et al., [2018](#bib.bib233)) where the first network predicts an initial coarse
    result and the second network predicts a refined result; (b) structure-then-texture (Nazeri
    et al., [2019](#bib.bib139)) where the first network predicts a structure map
    and the second network predicts a complete image. An apparent difference between
    these two types is that the structure-then-texture methods explicitly predict
    the structure map in the first stage.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：两种类型的两阶段修复框架：(a) 粗到细（Yu et al., [2018](#bib.bib233)），其中第一个网络预测初步的粗略结果，第二个网络预测精细结果；(b)
    结构再到纹理（Nazeri et al., [2019](#bib.bib139)），其中第一个网络预测结构图，第二个网络预测完整图像。这两种方法的明显区别在于，结构再到纹理方法在第一阶段明确地预测结构图。
- en: 2.1.2 Two-stage framework
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 两阶段框架
- en: 'Coarse-to-fine methods. This kind of method first applies a generator to fill
    the holes with coarse contents, and then refine them via the second generator,
    as shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.1 Single-shot framework ‣ 2.1 Deterministic
    Image Inpainting ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting:
    A Survey")(a). Yu et al. ([2018](#bib.bib233)) modified the generative inpainting
    framework with cascaded coarse and refinement networks. In the refinement stage,
    they designed a contextual attention module modeling the long-term correlation
    to facilitate the inpainting process.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '粗到细方法。这种方法首先使用生成器填充粗略内容的空洞，然后通过第二个生成器进行细化，如图[4](#S2.F4 "Figure 4 ‣ 2.1.1 Single-shot
    framework ‣ 2.1 Deterministic Image Inpainting ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey")(a)所示。Yu et al. ([2018](#bib.bib233)) 修改了生成修复框架，采用了级联的粗略和细化网络。在细化阶段，他们设计了一个上下文注意模块，建模长期相关性，以促进修复过程。'
- en: Many later works refined different aspects of this classical coarse-to-fine
    framework. Inspired by mask-aware convolution (Liu et al., [2018](#bib.bib122))
    for irregular holes, Yu et al. ([2019](#bib.bib234)) improved the previous network (Yu
    et al., [2018](#bib.bib233)) by introducing gated convolution that adaptively
    perceives the mask location. In the coarse stage, Ma et al. ([2019](#bib.bib134))
    proposed region-wise convolutions and a non-local operation to process the discrepancy
    and correlation between intact and damaged areas. PEPSI Sagong et al. ([2019](#bib.bib166))
    modified the two-stage feature encoding processes in (Yu et al., [2018](#bib.bib233))
    by sharing the encoding network and organizing the coarse and fine inpainting
    network in a parallel manner. PEPSI can enhance the inpainting capability while
    reducing the number of convolution operations and computational resources. To
    further reduce the network parameters, Shin et al. ([2021](#bib.bib173)) extended
    PEPSI by replacing the original dilated convolutional layers (Yu and Koltun, [2016](#bib.bib232))
    with a so-called rate-adaptive version, which shares the weights for each layer
    but produces dynamic features via dilation rates-related scaling and shifting
    operations. The contextual attention proposed by (Yu et al., [2018](#bib.bib233))
    has a limited ability to model the relationships between patches inside the holes,
    therefore, Liu et al. ([2019](#bib.bib123)) introduced a coherent semantic attention
    layer, which can enhance the semantic relevance and feature continuity in the
    attention computation of hole regions. In (Yu et al., [2018](#bib.bib233)), several
    dilated convolutions are applied to enlarge the receptive field. Li et al. ([2020b](#bib.bib104))
    replaced the dilated convolution with a spatial pyramid dilation ResNet block
    with eight different dilation rates to extract multi-scale features. Navasardyan
    and Ohanyan ([2020](#bib.bib138)) designed a patch-based onion convolution mechanism
    to continuously propagate information from known regions to the missing ones.
    This convolution mechanism can capture long-range pixel dependencies and achieve
    high efficiency and low latency.  Wadhwa et al. ([2021](#bib.bib189)) proposed
    a hypergraph convolution with a trainable incidence matrix to generate globally
    semantic completed images and replaced the regular convolutions with gated convolution
    in the discriminator to enhance the local consistency of inpainted images.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 后来的许多研究在这个经典的粗到细框架的不同方面进行了改进。受到用于不规则孔洞的**掩膜感知卷积**（Liu et al., [2018](#bib.bib122)）的启发，Yu
    et al. ([2019](#bib.bib234)) 通过引入**门控卷积**来改进之前的网络（Yu et al., [2018](#bib.bib233)），使其能够自适应地感知掩膜的位置。在粗略阶段，Ma
    et al. ([2019](#bib.bib134)) 提出了区域卷积和非局部操作，以处理完整区域与损坏区域之间的差异和相关性。PEPSI Sagong
    et al. ([2019](#bib.bib166)) 通过共享编码网络，并将粗略和精细修复网络以并行方式组织，修改了 (Yu et al., [2018](#bib.bib233))
    中的两阶段特征编码过程。PEPSI 可以提高修复能力，同时减少卷积操作的数量和计算资源。为了进一步减少网络参数，Shin et al. ([2021](#bib.bib173))
    通过将原始的**膨胀卷积层**（Yu and Koltun, [2016](#bib.bib232)）替换为一种所谓的**自适应率版本**，扩展了 PEPSI，该版本共享每层的权重，但通过膨胀率相关的缩放和位移操作生成动态特征。由
    (Yu et al., [2018](#bib.bib233)) 提出的**上下文注意力**在建模孔洞内部补丁之间的关系方面能力有限，因此 Liu et al.
    ([2019](#bib.bib123)) 引入了一个**连贯的语义注意力层**，以增强孔洞区域注意力计算中的语义相关性和特征连续性。在 (Yu et al.,
    [2018](#bib.bib233)) 中，应用了几种膨胀卷积以扩大感受野。Li et al. ([2020b](#bib.bib104)) 用一个具有八种不同膨胀率的**空间金字塔膨胀
    ResNet 模块**替代了膨胀卷积，以提取多尺度特征。Navasardyan 和 Ohanyan ([2020](#bib.bib138)) 设计了一种基于补丁的**洋葱卷积机制**，以持续传播已知区域的信息到缺失区域。这种卷积机制可以捕捉长距离的像素依赖性，并实现高效率和低延迟。Wadhwa
    et al. ([2021](#bib.bib189)) 提出了一个带有可训练的**发生矩阵**的**超图卷积**，以生成全球语义完成的图像，并用**门控卷积**替代了判别器中的常规卷积，以增强修复图像的局部一致性。
- en: Due to the computational overhead and the lack of supervision for the contextual
    attention in (Yu et al., [2018](#bib.bib233)), Zeng et al. ([2021b](#bib.bib244))
    removed this attention block and learned its patch-borrowing behavior with a so-called
    contextual reconstruction loss. Based on the insight that recovering different
    types of missing areas need a different scope of neighboring areas, Quan et al.
    ([2022](#bib.bib154)) designed a local and global refinement network with small
    and large receptive fields, which can be directly applied to the end of existing
    networks to further enhance their inpainting capability. Kim et al. ([2022](#bib.bib92))
    developed a coarse-super-resolution-refine pipeline, where they add a super-resolution
    network to reconstruct finer details after the coarse network and introduce a
    progressive learning mechanism to repair larger holes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算开销以及对上下文注意力的缺乏监督（Yu等人，[2018](#bib.bib233)），曾等人（[2021b](#bib.bib244)）移除了该注意力模块，并通过所谓的上下文重建损失来学习其补丁借用行为。基于恢复不同类型缺失区域需要不同邻域范围的洞察，全等人（[2022](#bib.bib154)）设计了一个具有小和大感受野的局部和全局精修网络，可以直接应用于现有网络的末端，以进一步增强其修复能力。金等人（[2022](#bib.bib92)）开发了一个粗略-超分辨率-精修管道，其中他们在粗略网络之后添加了一个超分辨率网络以重建更精细的细节，并引入了渐进学习机制以修复更大的孔洞。
- en: Some works adopt a coarse-to-fine framework to obtain high-resolution inpainting.
    Yang et al. ([2017](#bib.bib227)) designed a two-stage inpainting framework consisting
    of a content network and a texture network. The former predicts the holistic content
    in the low resolution ($128\times 128$) and the latter iteratively optimizes the
    texture details of missing regions from low to high resolution ($512\times 512$).
    Song et al. ([2018a](#bib.bib178)) developed an image-to-feature network to infer
    coarse results, and then designed a patch-swap method to refine the coarse features.
    The swapped feature map is translated to a complete image via a Feature2Image
    network. In addition, this framework can be directly used for high-resolution
    inpainting by upsampling the complete image as the input of refine stage with
    a multi-scale inference. Yi et al. ([2020](#bib.bib231)) proposed a contextual
    residual aggregation mechanism for ultra high-resolution image inpainting (up
    to 8K). Specifically, a low-resolution inpainting result was first predicted via
    a two-stage coarse-to-fine network and then the high-resolution result was generated
    by adding the large blurry image with the aggregated residuals, which are obtained
    by aggregating weighted high-frequency residuals from contextual patches. Zhang
    et al. ([2022c](#bib.bib254)) focused on image inpainting for 4K or more resolution.
    They first fill the hole via LaMa (Suvorov et al., [2022](#bib.bib183)), predict
    depth, structure, and segmentation map from the initially completed image, then
    generate multiple candidates with a multiply-guided PatchMatch (Barnes et al.,
    [2009](#bib.bib6)), and finally choose a good output using the proposed auto-curation
    network. To complete the high-resolution image with limited resources, these methods
    first predicted the coarse content at the low-resolution level and then refine
    the texture details at the high-resolution level (sometimes with multi-scale inferences).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作采用粗到细的框架来实现高分辨率的图像修复。杨等人（[2017](#bib.bib227)）设计了一个由内容网络和纹理网络组成的两阶段修复框架。前者预测低分辨率（$128\times
    128$）下的整体内容，后者则从低分辨率到高分辨率（$512\times 512$）迭代优化缺失区域的纹理细节。宋等人（[2018a](#bib.bib178)）开发了一个图像到特征网络来推断粗略结果，然后设计了一种补丁交换方法来细化粗略特征。交换后的特征图通过Feature2Image网络转化为完整图像。此外，该框架可以通过多尺度推理直接用于高分辨率修复，将完整图像上采样作为精修阶段的输入。易等人（[2020](#bib.bib231)）提出了一种上下文残差聚合机制用于超高分辨率图像修复（最高可达8K）。具体而言，首先通过两阶段的粗到细网络预测低分辨率修复结果，然后通过将大模糊图像与聚合的残差相加生成高分辨率结果，这些残差是通过从上下文补丁中聚合加权高频残差获得的。张等人（[2022c](#bib.bib254)）专注于4K或更高分辨率的图像修复。他们首先通过LaMa（Suvorov等人，[2022](#bib.bib183)）填补空洞，从最初完成的图像中预测深度、结构和分割图，然后使用多指导的PatchMatch（Barnes等人，[2009](#bib.bib6)）生成多个候选结果，并最终使用所提出的自动策划网络选择良好的输出。为了在资源有限的情况下完成高分辨率图像，这些方法首先在低分辨率级别预测粗略内容，然后在高分辨率级别细化纹理细节（有时使用多尺度推理）。
- en: Other works also follow the basic coarse-to-fine strategy, but they are clearly
    different from the framework proposed by (Yu et al., [2018](#bib.bib233)). After
    obtaining the coarse result with an initial prediction network, Li et al. ([2019d](#bib.bib112))
    applied a super-resolution network as the refinement stage to produce high-frequency
    details. Roy et al. ([2021](#bib.bib163)) predicted the coarse results in the
    frequency domain by learning the mapping of the DFT of the corrupted image and
    its ground truth. Based on the insight that patch-based methods (Barnes et al.,
    [2009](#bib.bib6); He and Sun, [2012](#bib.bib60)) fill the missing regions with
    high-quality texture details, Xu et al. ([2021](#bib.bib223)) proposed a texture
    memory-augmented patch synthesis network with a patch distribution loss after
    the coarse inpainting network.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其他作品也遵循基本的粗到细策略，但它们显然不同于(Yu et al., [2018](#bib.bib233))提出的框架。在通过初始预测网络获得粗略结果后，Li
    et al. ([2019d](#bib.bib112)) 采用了超分辨率网络作为细化阶段，以产生高频细节。Roy et al. ([2021](#bib.bib163))
    通过学习腐蚀图像及其真实值的DFT映射，在频域中预测粗略结果。基于补丁方法(Barnes et al., [2009](#bib.bib6); He and
    Sun, [2012](#bib.bib60)) 用高质量纹理细节填补缺失区域的见解，Xu et al. ([2021](#bib.bib223)) 提出了一个纹理记忆增强的补丁合成网络，在粗略修补网络之后加入了补丁分布损失。
- en: 'Structure-then-texture methods. Structure and texture are two important components
    of the image, therefore, some works decompose the image inpainting as the structure
    inference and the texture restoration, as shown in Fig. [4](#S2.F4 "Figure 4 ‣
    2.1.1 Single-shot framework ‣ 2.1 Deterministic Image Inpainting ‣ 2 Image Inpainting
    ‣ Deep Learning-based Image and Video Inpainting: A Survey")(b). Sun et al. ([2018b](#bib.bib182))
    designed a two-stage head inpainting obfuscation network. The first stage generates
    facial landmarks and the second stage recovers the head image guided by the landmarks.
    Song et al. ([2019](#bib.bib177)) first estimated the facial geometry including
    landmark heatmaps and parsing maps, and then concatenated these results with a
    corrupted face image as the input of the complete network to recover face images
    and disentangle masks. Liao et al. ([2018b](#bib.bib115)) and Nazeri et al. ([2019](#bib.bib139))
    both proposed an edge-guided image inpainting method, which first estimates the
    edge map for the missing regions, and then utilizes this edge map prior to predicting
    the texture details. Similarly, Xiong et al. ([2019](#bib.bib220)) explicitly
    disentangled the image inpainting problem into two sub-tasks of foreground contour
    prediction and content completion. To improve the structural guidance of coarse
    edge maps, Ren et al. ([2019](#bib.bib158)) introduced another representation
    of the structure, i.e., the edge-preserving smoothing via filtering operation.
    Based on the structure reconstruction of the first network, they inpainted missing
    regions using appearance flow. Shao et al. ([2020](#bib.bib171)) combined the
    edge map and color aware map as the representation of the structure, where the
    former is captured via the Canny operator (Canny, [1986](#bib.bib10)) and the
    latter is obtained through Gaussian blur with a large kernel. For the specific
    Manga inpainting, Xie et al. ([2021](#bib.bib217)) first completed a semantic
    structure map, including the structural lines and the ScreenVAE map (a point-wise
    representation of screentones) (Xie et al., [2020](#bib.bib216)), using a semantic
    inpainting network. Then, the completed semantic map is used for guiding the appearance
    synthesis. Wang et al. ([2021b](#bib.bib199)) designed an external-internal learning
    inpainting framework. It first reconstructs the structures in the monochromatic
    space using the knowledge externally learned from large datasets. Based on internal
    learning, then, it applies a multi-stage network to recover the color information
    via iterative optimization. Besides the edge map used in (Nazeri et al., [2019](#bib.bib139)),
    Yamashita et al. ([2022](#bib.bib225)) incorporated the depth image to provide
    the boundaries between different objects. Their method first completed the masked
    edge and depth images separately and then recovered the missing regions via an
    RGB image inpainting network taking as input the concatenation of masked images,
    inpainted edges, and depth images. To contain richer structural information, Wu
    et al. ([2022](#bib.bib211)) choose the local binary pattern (LBP) (Ojala et al.,
    [1996](#bib.bib143), [2002](#bib.bib144)), which describes the distribution information
    of edges, speckles, and other local features (Zhang et al., [2010](#bib.bib246)).
    In (Wu et al., [2022](#bib.bib211)), the first network infers the LBP information
    of the holes, and the second network with spatial attention conducts the actual
    image inpainting. Dong et al. ([2022](#bib.bib38)) utilized a transformer to complete
    the holistic structure in a grayscale space and proposed a masking positional
    encoding for large irregular masks.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '结构-纹理方法。结构和纹理是图像的两个重要组成部分，因此，一些工作将图像修复分解为结构推断和纹理恢复，如图[4](#S2.F4 "Figure 4 ‣
    2.1.1 Single-shot framework ‣ 2.1 Deterministic Image Inpainting ‣ 2 Image Inpainting
    ‣ Deep Learning-based Image and Video Inpainting: A Survey")(b)所示。Sun 等人 ([2018b](#bib.bib182))
    设计了一个两阶段的头部修复遮蔽网络。第一阶段生成面部特征点，第二阶段则根据这些特征点恢复头部图像。Song 等人 ([2019](#bib.bib177))
    首先估计了面部几何信息，包括特征点热图和解析图，然后将这些结果与受损的面部图像连接，作为完整网络的输入，以恢复面部图像并解开遮蔽。Liao 等人 ([2018b](#bib.bib115))
    和 Nazeri 等人 ([2019](#bib.bib139)) 都提出了一种边缘引导的图像修复方法，该方法首先估计缺失区域的边缘图，然后利用这个边缘图来预测纹理细节。同样，Xiong
    等人 ([2019](#bib.bib220)) 明确将图像修复问题分解为前景轮廓预测和内容补全两个子任务。为了改善粗糙边缘图的结构引导，Ren 等人 ([2019](#bib.bib158))
    引入了另一种结构表示，即通过滤波操作进行的边缘保持平滑。基于第一网络的结构重建，他们使用外观流修复缺失区域。Shao 等人 ([2020](#bib.bib171))
    将边缘图和颜色感知图结合作为结构表示，其中前者通过 Canny 算子 (Canny, [1986](#bib.bib10)) 捕获，后者通过大内核的高斯模糊获得。针对特定的漫画修复，Xie
    等人 ([2021](#bib.bib217)) 首先完成了一个语义结构图，包括结构线和 ScreenVAE 图（屏幕纹理的逐点表示）(Xie 等人, [2020](#bib.bib216))，使用语义修复网络完成。然后，完成的语义图用于指导外观合成。Wang
    等人 ([2021b](#bib.bib199)) 设计了一个外部-内部学习修复框架。它首先使用从大数据集中外部学习的知识在单色空间中重建结构。基于内部学习，它应用了一个多阶段网络，通过迭代优化恢复颜色信息。除了
    (Nazeri 等人, [2019](#bib.bib139)) 中使用的边缘图外，Yamashita 等人 ([2022](#bib.bib225)) 还结合了深度图像以提供不同对象之间的边界。他们的方法首先分别完成了遮蔽的边缘和深度图像，然后通过
    RGB 图像修复网络恢复缺失区域，输入为遮蔽图像、修复边缘和深度图像的连接。为了包含更丰富的结构信息，Wu 等人 ([2022](#bib.bib211))
    选择了局部二值模式 (LBP) (Ojala 等人, [1996](#bib.bib143), [2002](#bib.bib144))，它描述了边缘、斑点和其他局部特征的分布信息
    (Zhang 等人, [2010](#bib.bib246))。在 (Wu 等人, [2022](#bib.bib211)) 中，第一个网络推断了孔的 LBP
    信息，第二个具有空间注意力的网络进行了实际的图像修复。Dong 等人 ([2022](#bib.bib38)) 使用了变压器在灰度空间中完成整体结构，并提出了用于大不规则遮蔽的掩模位置编码。'
- en: In addition, semantic segmentation maps are also used as the proxy of structure (Song
    et al., [2018b](#bib.bib179); Qiu et al., [2021](#bib.bib153); Zhou et al., [2021](#bib.bib272)).
    Song et al. ([2018b](#bib.bib179)) introduced the semantic segmentation information
    into the image inpainting process to improve the recovered boundary between different
    class regions. They first predict the segmentation map of missing regions via
    a U-Net and then recover the missing contents with the guidance of the above inpainted
    semantic map using the second generator network. Song et al. ([2018b](#bib.bib179))
    utilized the pre-classification algorithm (Felzenszwalb and Huttenlocher, [2004](#bib.bib45))
    to extract a semantic structure map. After the completion of the semantic map,
    they employed a spatial-channel attention module to generate the texture information.
    Zhou et al. ([2021](#bib.bib272)) first predicted the complete segmentation map
    via a segmentation reconstructor, and then recovered fine-grained texture details
    with an image generator based on a relation network. The relation network is an
    extension of SPADE (Park et al., [2019](#bib.bib146)) to better modulate features
    via spatially-adaptive normalization with the relation graph.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，语义分割图也被用作结构的代理（宋等， [2018b](#bib.bib179)；邱等， [2021](#bib.bib153)；周等， [2021](#bib.bib272)）。宋等（[2018b](#bib.bib179)）将语义分割信息引入图像修复过程，以改善不同类别区域之间恢复的边界。他们首先通过
    U-Net 预测缺失区域的分割图，然后在第二个生成网络的指导下，利用上述修复的语义图恢复缺失的内容。宋等（[2018b](#bib.bib179)）利用预分类算法（Felzenszwalb
    和 Huttenlocher， [2004](#bib.bib45)）提取语义结构图。完成语义图后，他们采用空间通道注意模块生成纹理信息。周等（[2021](#bib.bib272)）首先通过分割重构器预测完整的分割图，然后利用基于关系网络的图像生成器恢复细粒度纹理细节。关系网络是
    SPADE（Park 等， [2019](#bib.bib146)）的扩展，能够通过空间自适应归一化和关系图更好地调节特征。
- en: '![Refer to caption](img/5ad316c6fca9d9ac64eaf5941ae59b44.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5ad316c6fca9d9ac64eaf5941ae59b44.png)'
- en: 'Figure 5: Progressive image inpainting. The image comes from (Zhang et al.,
    [2018a](#bib.bib247)).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：渐进式图像修复。该图像来源于（张等， [2018a](#bib.bib247)）。
- en: 2.1.3 Progressive frameworks
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 渐进框架
- en: 'Following the basic idea of traditional inpainting methods, some works have
    been proposed to exploit progressive inpainting with deep models. As shown in
    Fig. [5](#S2.F5 "Figure 5 ‣ 2.1.2 Two-stage framework ‣ 2.1 Deterministic Image
    Inpainting ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting:
    A Survey"), the progressive methods iteratively fill in the holes from the boundary
    to the center of the holes, and the missing area gradually becomes smaller until
    it disappears. Zhang et al. ([2018a](#bib.bib247)) formulated image inpainting
    as a sequential problem, where the missing regions are filled in four inpainting
    phases. They designed an LSTM (long short-term memory) (Hochreiter and Schmidhuber,
    [1997](#bib.bib67))-based framework to string these four inpainting phases together.
    However, this method cannot handle irregular holes common in real-world applications.
    Guo et al. ([2019](#bib.bib57)) devised a residual architecture to progressively
    update irregular masks and introduced a full-resolution network to facilitate
    feature integration and texture reconstruction. Inspired by structure-guided inpainting
    methods (Nazeri et al., [2019](#bib.bib139); Xiong et al., [2019](#bib.bib220)),
    Li et al. ([2019c](#bib.bib107)) proposed a progressive reconstruction with a
    visual structure network to incorporate structure information into the visual
    features step by step, which can generate a more structured image. Progressive
    inpainting methods have the potential to fill in large holes, however, it is still
    difficult due to the lack of constraints on the hole center. To handle this drawback,
    Li et al. ([2020c](#bib.bib108)) designed a recurrence feature reasoning network
    with consistent attention and weighted feature fusion. This network recurrently
    infers and gathers the hole boundaries of the feature map so as to progressively
    strengthen the constraints for estimating internal contents. Zeng et al. ([2020b](#bib.bib242))
    proposed an iterative inpainting method with confidence feedback for high-resolution
    images. SRInpaintor (Li et al., [2022a](#bib.bib105)) combined super-resolution
    and the transformer in a progressive pipeline. It reasons about the global structure
    in low resolution, and progressively refines the texture details in high resolution.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '继承传统修补方法的基本思想，一些研究提出了利用深度模型进行渐进修补的方法。如图 [5](#S2.F5 "Figure 5 ‣ 2.1.2 Two-stage
    framework ‣ 2.1 Deterministic Image Inpainting ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey")所示，渐进方法迭代地从孔的边界到中心填补孔洞，缺失区域逐渐变小直到消失。Zhang
    等（[2018a](#bib.bib247)）将图像修补表述为一个序列问题，其中缺失区域在四个修补阶段中被填补。他们设计了一个基于 LSTM（长短期记忆）（Hochreiter
    和 Schmidhuber，[1997](#bib.bib67)）的框架，将这四个修补阶段串联起来。然而，这种方法无法处理实际应用中常见的不规则孔洞。Guo
    等（[2019](#bib.bib57)）设计了一种残差架构，以渐进方式更新不规则掩码，并引入了全分辨率网络以促进特征融合和纹理重建。受结构引导修补方法（Nazeri
    等，[2019](#bib.bib139)；Xiong 等，[2019](#bib.bib220)）的启发，Li 等（[2019c](#bib.bib107)）提出了一种渐进重建方法，采用视觉结构网络将结构信息逐步融入视觉特征中，从而生成更结构化的图像。渐进修补方法有潜力填补大孔，但由于缺乏对孔中心的约束，仍然很困难。为了解决这一缺陷，Li
    等（[2020c](#bib.bib108)）设计了一种具有一致注意力和加权特征融合的递归特征推理网络。该网络递归地推断并收集特征图的孔边界，以逐步加强估计内部内容的约束。Zeng
    等（[2020b](#bib.bib242)）提出了一种具有信心反馈的迭代修补方法，适用于高分辨率图像。SRInpaintor（Li 等，[2022a](#bib.bib105)）在渐进管道中结合了超分辨率和变换器。它在低分辨率下推断全球结构，并在高分辨率下逐步细化纹理细节。'
- en: 'To this end, we organize the important and prevalent technical aspects for
    the network design, as shown in Table [1](#S2.T1 "Table 1 ‣ 2.1.3 Progressive
    frameworks ‣ 2.1 Deterministic Image Inpainting ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '为此，我们组织了网络设计的重要和普遍的技术方面，如表 [1](#S2.T1 "Table 1 ‣ 2.1.3 Progressive frameworks
    ‣ 2.1 Deterministic Image Inpainting ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey")所示。'
- en: 'Table 1: The summary of important techniques for deep learning-based image
    inpainting.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：深度学习图像修补的关键技术总结。
- en: '| Aspects | Blocks | Core idea |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 模块 | 核心思想 |'
- en: '| mask-aware convolution | Shepard interpolation (Ren et al., [2015](#bib.bib157))
    | translation variant interpolation |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 掩码感知卷积 | Shepard 插值 （Ren 等，[2015](#bib.bib157)） | 平移变体插值 |'
- en: '| partial convolution (Liu et al., [2018](#bib.bib122)) | convolution on valid
    regions |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 部分卷积 （Liu 等，[2018](#bib.bib122)） | 有效区域的卷积 |'
- en: '| gated convolution (Yu et al., [2019](#bib.bib234)) | adaptive gating |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 门控卷积 （Yu 等，[2019](#bib.bib234)） | 自适应门控 |'
- en: '| priority-guided partial convolution (Wang et al., [2021c](#bib.bib201)) |
    structure and texture priority |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 优先引导的部分卷积 (Wang et al., [2021c](#bib.bib201)) | 结构和纹理优先级 |'
- en: '| Attention | contextual attention (Yu et al., [2018](#bib.bib233)) | background
    patches with high |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 注意力 | 上下文注意力 (Yu et al., [2018](#bib.bib233)) | 高背景补丁 |'
- en: '|  | similarity to the coarse prediction |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | 与粗略预测的相似性 |'
- en: '| coherent semantic attention (Liu et al., [2019](#bib.bib123)) | correlation
    between patches within the hole |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 连贯语义注意力 (Liu et al., [2019](#bib.bib123)) | 孔内补丁之间的相关性 |'
- en: '| multi-scale attention module (Wang et al., [2019b](#bib.bib195)) | attention
    with two patch sizes |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 多尺度注意力模块 (Wang et al., [2019b](#bib.bib195)) | 带有两种补丁尺寸的注意力 |'
- en: '| multi-scale attention uint (Qin et al., [2021](#bib.bib152)) | attention
    with four different dilation rates |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 多尺度注意力单元 (Qin et al., [2021](#bib.bib152)) | 带有四种不同扩张率的注意力 |'
- en: '| Normalization | region normalization (Yu et al., [2020](#bib.bib235)) | spatial
    and region-wise |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 归一化 | 区域归一化 (Yu et al., [2020](#bib.bib235)) | 空间和区域级别 |'
- en: '| probabilistic context normalization (Wang et al., [2020c](#bib.bib205)) |
    transfer mean and variance |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 概率上下文归一化 (Wang et al., [2020c](#bib.bib205)) | 转移均值和方差 |'
- en: '| regional composite normalization (Wang et al., [2021a](#bib.bib197)) | batch,
    instance, and layer normalization |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 区域复合归一化 (Wang et al., [2021a](#bib.bib197)) | 批量、实例和层归一化 |'
- en: '| point-wise normalization (Zhu et al., [2021](#bib.bib273)) | mask-ware batch
    normalization |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 点级归一化 (Zhu et al., [2021](#bib.bib273)) | 面具感知批量归一化 |'
- en: '| frequency region attentive normalization (Zhu et al., [2021](#bib.bib273))
    | align low- and high-frequency features |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 频率区域注意归一化 (Zhu et al., [2021](#bib.bib273)) | 对齐低频和高频特征 |'
- en: '| Discriminator | global discriminator (Pathak et al., [2016](#bib.bib148))
    | entire image |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 判别器 | 全局判别器 (Pathak et al., [2016](#bib.bib148)) | 整个图像 |'
- en: '| local discriminator (Iizuka et al., [2017](#bib.bib78)) | corrupted region
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 局部判别器 (Iizuka et al., [2017](#bib.bib78)) | 损坏区域 |'
- en: '| patch-based discriminator (PatchDis) (Yu et al., [2019](#bib.bib234)) | eense
    local patches |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 基于补丁的判别器 (PatchDis) (Yu et al., [2019](#bib.bib234)) | 密集局部补丁 |'
- en: '| conditional multi-scale discriminator (Li et al., [2020b](#bib.bib104)) |
    PatchDis with two different scales |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 条件多尺度判别器 (Li et al., [2020b](#bib.bib104)) | 具有两种不同尺度的 PatchDis |'
- en: '| soft mask-guided PatchDis (Zeng et al., [2022](#bib.bib245)) | central parts
    of the missing regions |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 软面具引导的 PatchDis (Zeng et al., [2022](#bib.bib245)) | 丢失区域的中心部分 |'
- en: 2.2 Stochastic Image Inpainting
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 随机图像修复
- en: Image inpainting is an underdetermined inverse problem. Therefore, multiple
    plausible solutions exist. We use the term stochastic image inpainting to refer
    to methods capable of producing multiple solutions with a random sampling process.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图像修复是一个欠定的逆问题。因此，存在多个可能的解决方案。我们使用“随机图像修复”一词来指代能够通过随机采样过程产生多种解决方案的方法。
- en: VAE-based methods. A variational autoencoder (VAE) (Kingma and Welling, [2014](#bib.bib94))
    is a generative model that combines an encoder and a decoder. The encoder learns
    an appropriated latent space and the decoder transforms sampled latent representations
    back into new data. Zheng et al. ([2019](#bib.bib265)) proposed a two-branch completion
    network, where the reconstructive branch models the prior distribution of missing
    parts and reconstructs the original complete image from this distribution. The
    generative branch infers the latent conditional prior distribution for the missing
    areas. This framework is optimized by balancing the variance of the conditional
    distribution and the reconstruction of the original training data. Zheng et al.
    ([2021a](#bib.bib266)) extended this work by estimating the distributions in a
    separate training stage and introducing the patch-level short-long term attention
    module. For stochastic fashion image inpainting, Han et al. ([2019](#bib.bib59))
    decomposed the inpainting process as the shape and appearance generation. The
    network design for these two generation tasks mainly adopts the VAE architecture.
    Based on a pre-trained VAE on facial images, Tu and Chen ([2019](#bib.bib186))
    first searched for the possible set of solutions in the coding vector space for
    the corrupted image, and then recovers possible face images with the decoder of
    the VAE. Zhao et al. ([2020](#bib.bib262)) proposed an instance-guided conditional
    image-to-image translation network to learn conditional completion distribution.
    Specifically, they first encode the instance and masked images into two probability
    feature spaces, and then design a cross-semantic attention layer to fuse two feature
    maps. A decoder is finally used to generate the inpainted image. However, Han
    et al. ([2019](#bib.bib59)) and Zhao et al. ([2020](#bib.bib262)) often suffer
    from distorted structures and blurry textures due to the joint optimization of
    structure and appearance. Peng et al. ([2021](#bib.bib149)) designed a two-stage
    pipeline, where the first stage produces multiple coarse results with different
    structures based on a hierarchical vector quantized variational auto-encoder,
    and the second stage synthesizes the texture under the guidance of the discrete
    structural features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基于VAE的方法。变分自动编码器（VAE）（Kingma and Welling，[2014](#bib.bib94)）是一种生成模型，它结合了编码器和解码器。编码器学习一个适当的潜在空间，解码器将抽样的潜在表示转换回新数据。Zheng等人([2019](#bib.bib265))提出了一个两分支补全网络，其中重构分支模拟缺失部分的先验分布，并从这个分布重构原始完整图像。生成分支推断出缺失区域的潜在条件先验分布。通过平衡条件分布的方差和原始训练数据的重构来优化这一框架。Zheng等人([2021a](#bib.bib266))通过在单独的训练阶段估计分布并引入补丁级短长期注意模块来扩展了这项工作。对于随机时尚图像修补，Han等人([2019](#bib.bib59))将修补过程分解为形状和外观生成。这两个生成任务的网络设计主要采用了VAE架构。基于面部图像上的预训练VAE，Tu和Chen([2019](#bib.bib186))首先在编码向量空间中搜索损坏图像的可能解集，然后用VAE的解码器恢复可能的面部图像。赵等人([2020](#bib.bib262))提出了一个实例引导的条件图像到图像转换网络来学习条件完成分布。具体而言，他们首先将实例和掩蔽图像编码成两个概率特征空间，然后设计一个跨语义注意层来融合两个特征图。最后使用解码器生成修补图像。然而，Han等人([2019](#bib.bib59))和赵等人([2020](#bib.bib262))常常因为结构和外观的联合优化而遭受扭曲的结构和模糊的纹理。Peng等人([2021](#bib.bib149))设计了一个两阶段流水线，第一阶段根据分层矢量量化变分自动编码器产生多个不同结构的粗略结果，第二阶段在离散结构特征的引导下合成纹理。
- en: GAN-based methods. GAN (Goodfellow et al., [2014](#bib.bib51)) learns the data
    distribution via an adversarial process. A generator is applied to transform sampled
    Gaussian random noise into image space and a discriminator is used to differentiate
    the real sample and fake sample. Based on the premise that the degree of freedom
    increases from the hole boundary to the hole center, Liu et al. ([2021a](#bib.bib125))
    introduced a spatially probabilistic diversity normalization to modulate the pixel
    generation with diversity maps. Considering that minimizing the classical reconstruction
    loss hampers the diversity of results, they also proposed a perceptual diversity
    loss that maximizes the distance of two generated images in the feature space.
    By combining the image-conditional and unconditional generative architectures,
    Zhao et al. ([2021](#bib.bib263)) proposed a co-modulated GAN for large-scale
    image inpainting. Technically, they encode the incomplete input image into a conditional
    latent vector, which is then concatenated with the original style vector of StyleGAN2 (Karras
    et al., [2020](#bib.bib88)). To enhance the diversity and control of image inpainting,
    Zeng et al. ([2021a](#bib.bib243)) applied the patch matching from the training
    samples on the basis of coarse inpainted results. In particular, they designed
    the nearest neighbor-based pixel-wise global matching (from a single image) and
    compositional matching (from multiple images). Inspired by CoModGAN (Zhao et al.,
    [2021](#bib.bib263)), Zheng et al. ([2022b](#bib.bib269)) proposed a cascaded
    modulation GAN, which combines the global modulation and the spatially-adaptive
    modulation in each scale of the decoder, and replaces the common convolution with
    fast Fourier convolution (Chi et al., [2020](#bib.bib22)) in the encoder. To directly
    complete the high-resolution image, Li et al. ([2022b](#bib.bib109)) proposed
    a mask-aware transformer module with a dynamic mask updating as (Liu et al., [2018](#bib.bib122)).
    This module conducts non-local interactions only using partially valid tokens
    in a shifted-window manner Liu et al. ([2021d](#bib.bib130)). Following (Chen
    et al., [2019](#bib.bib21); Karras et al., [2019](#bib.bib87)), they developed
    a style manipulation module for stochastic generations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GAN 的方法。GAN（Goodfellow 等, [2014](#bib.bib51)）通过对抗过程学习数据分布。一个生成器将采样的高斯随机噪声转换为图像空间，而一个鉴别器用于区分真实样本和假样本。基于从孔边界到孔中心自由度增加的前提，Liu
    等人 ([2021a](#bib.bib125)) 引入了一种空间概率多样性标准化来通过多样性图调整像素生成。考虑到最小化经典重建损失会妨碍结果的多样性，他们还提出了一种感知多样性损失，最大化两个生成图像在特征空间中的距离。通过结合图像条件和无条件生成架构，Zhao
    等人 ([2021](#bib.bib263)) 提出了用于大规模图像修复的共调制 GAN。从技术上讲，他们将不完整的输入图像编码为条件潜在向量，然后与 StyleGAN2（Karras
    等, [2020](#bib.bib88)）的原始风格向量连接。为了增强图像修复的多样性和控制，Zeng 等人 ([2021a](#bib.bib243))
    基于粗略修复结果应用了训练样本中的块匹配。特别地，他们设计了基于最近邻的像素级全局匹配（来自单个图像）和组合匹配（来自多个图像）。受到 CoModGAN（Zhao
    等, [2021](#bib.bib263)）的启发，Zheng 等人 ([2022b](#bib.bib269)) 提出了一个级联调制 GAN，该 GAN
    结合了每个解码器尺度中的全局调制和空间自适应调制，并用快速傅里叶卷积（Chi 等, [2020](#bib.bib22)）替代了编码器中的常规卷积。为了直接完成高分辨率图像，Li
    等人 ([2022b](#bib.bib109)) 提出了一个具有动态掩码更新的掩码感知 Transformer 模块（Liu 等, [2018](#bib.bib122)）。该模块仅使用部分有效标记以移位窗口的方式进行非局部交互（Liu
    等, [2021d](#bib.bib130)）。按照（Chen 等, [2019](#bib.bib21); Karras 等, [2019](#bib.bib87)），他们开发了一种用于随机生成的风格操控模块。
- en: Flow-based methods. Normalizing Flows (Tabak and Vanden-Eijnden, [2010](#bib.bib184);
    Dinh et al., [2014](#bib.bib35); Rezende and Mohamed, [2015](#bib.bib159)) are
    a generative method that constructs a complex probability distribution by assembling
    a sequence of invertible mappings. Inspired by Glow (Kingma and Dhariwal, [2018](#bib.bib93))
    and its conditional extension (Lugmayr et al., [2020](#bib.bib132)), Wang et al.
    ([2022a](#bib.bib193)) proposed a conditional normalizing flow network to learn
    the probability distribution of structure priors. Then, another generator is applied
    to produce the final complete image with rich texture.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基于流的方法。标准化流（Tabak 和 Vanden-Eijnden, [2010](#bib.bib184); Dinh 等, [2014](#bib.bib35);
    Rezende 和 Mohamed, [2015](#bib.bib159)）是一种生成方法，通过组合一系列可逆映射来构建复杂的概率分布。受到 Glow（Kingma
    和 Dhariwal, [2018](#bib.bib93)）及其条件扩展（Lugmayr 等, [2020](#bib.bib132)）的启发，Wang
    等人 ([2022a](#bib.bib193)) 提出了一个条件标准化流网络，以学习结构先验的概率分布。然后，应用另一个生成器生成具有丰富纹理的最终完整图像。
- en: MLM-based methods. To produce a stochastic structure in the missing region,
    Yu et al. ([2021b](#bib.bib237)) and Wan et al. ([2021](#bib.bib191)) adopted
    a sequence prediction pipeline based on a masked language model (MLM). Yu et al.
    ([2021b](#bib.bib237)) proposed a bidirectional and auto-regressive transformer
    as the low-resolution stochastic-structure generator, which predicts masked token
    (missing regions) via a top-$\mathcal{K}$ sampling strategy during inference.
    Then, a texture generator was applied to generate multiple inpainted results.
    Similarly, Wan et al. ([2021](#bib.bib191)) proposed a Transformer-CNN framework.
    They first apply a transformer training with MLM objective to produce a low-resolution
    image with pluralistic structures and some coarse textures, and then utilize an
    encoder-decoder network to enhance the local texture details of the high-resolution
    complete image.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 MLM 的方法。为了在缺失区域生成随机结构，Yu 等（[2021b](#bib.bib237)）和 Wan 等（[2021](#bib.bib191)）采用了基于掩码语言模型（MLM）的序列预测流程。Yu
    等（[2021b](#bib.bib237)）提出了一种双向和自回归的变换器作为低分辨率随机结构生成器，通过 top-$\mathcal{K}$ 采样策略在推断过程中预测掩码标记（缺失区域）。然后，应用纹理生成器生成多个修复结果。类似地，Wan
    等（[2021](#bib.bib191)）提出了 Transformer-CNN 框架。他们首先应用具有 MLM 目标的变换器训练生成具有多元结构和一些粗糙纹理的低分辨率图像，然后利用编码器-解码器网络增强高分辨率完整图像的局部纹理细节。
- en: '![Refer to caption](img/93235889d84940d3fdf42ec45566c88f.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/93235889d84940d3fdf42ec45566c88f.png)'
- en: 'Figure 6: Representative examples of masks.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：遮罩的代表性示例。
- en: Diffusion model-based methods. Diffusion models (DM) are emerging generative
    models for image synthesis. Here, we only review diffusion model-based inpainting
    methods, and we refer readers to the surveys (Yang et al., [2023](#bib.bib229);
    Croitoru et al., [2023](#bib.bib27)) about a comprehensive introduction to diffusion
    models. Generally, diffusion-based inpainting models employ a U-Net architecture.
    The training objectives are usually based on $\mathcal{L}_{DM}=\mathbb{E}_{x,\epsilon\in\mathcal{N}(0,1),t}[\|\epsilon-\epsilon_{\theta}(x_{t},t)\|_{2}^{2}]$,
    where $t=1\dots T$, $x_{t}$ is a noised version of $x$, and $\epsilon_{\theta}(\cdot,t)$
    is a neural network. In the literature, existing works mainly focused on the sampling
    strategy design and the computational cost reduction.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 基于扩散模型的方法。扩散模型（DM）是用于图像合成的新兴生成模型。在这里，我们只回顾基于扩散模型的修复方法，并且我们建议读者参考关于扩散模型的全面介绍的调查文章（Yang
    等，[2023](#bib.bib229)；Croitoru 等，[2023](#bib.bib27)）。通常，基于扩散的修复模型采用 U-Net 架构。训练目标通常基于
    $\mathcal{L}_{DM}=\mathbb{E}_{x,\epsilon\in\mathcal{N}(0,1),t}[\|\epsilon-\epsilon_{\theta}(x_{t},t)\|_{2}^{2}]$，其中
    $t=1\dots T$，$x_{t}$ 是 $x$ 的噪声版本，$\epsilon_{\theta}(\cdot,t)$ 是一个神经网络。在文献中，现有的工作主要集中在采样策略设计和计算成本减少上。
- en: (1) Sampling strategy design.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 采样策略设计。
- en: Based on an unconditionally pre-trained denoising diffusion probabilistic model
    (DDPM) (Ho et al., [2020](#bib.bib66)), Lugmayr et al. ([2022](#bib.bib133)) modified
    the standard denoising strategy by sampling the masked regions from the diffusion
    model and sampling the unmasked areas from the given image. To preserve the background
    and improve the consistency, Xie et al. ([2023](#bib.bib218)) added an extra mask
    prediction to the diffusion model. In the inference stage, the predicted mask
    is used to guide the sampling process.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 基于无条件预训练的去噪扩散概率模型（DDPM）（Ho 等，[2020](#bib.bib66)），Lugmayr 等（[2022](#bib.bib133)）通过从扩散模型中采样遮罩区域，从给定图像中采样未遮罩区域来修改了标准去噪策略。为了保持背景并改善一致性，Xie
    等（[2023](#bib.bib218)）向扩散模型中添加了额外的遮罩预测。在推断阶段，预测的遮罩用于指导采样过程。
- en: (2) Computational cost reduction.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 计算成本减少。
- en: Instead of applying the diffusion process in pixel space, Esser et al. ([2021](#bib.bib43))
    utilized a multinomial diffusion process (Hoogeboom et al., [2021](#bib.bib69);
    Austin et al., [2021](#bib.bib2)) on a discrete latent space and autoregressively
    factorized models for the reverse process. These designs enable ImageBART to generate
    high-resolution images, e.g., $300\times 1800$. Similarly, Rombach et al. ([2022](#bib.bib161))
    proposed a latent diffusion model (LDM) to reduce the training cost of DMs while
    boosting visual quality, which can be applied to the image inpainting task at
    a high resolution of $1024^{2}$ pixels. To overcome the limitation of massive
    iterations in the diffusion model, Li et al. ([2022c](#bib.bib110)) proposed a
    spatial diffusion model (SDM) with decoupled probabilistic modeling, where the
    mean term refers to the inpainted result and the variance term measures the uncertainty.
    Instead of starting with random Gaussian noise in the reverse conditional diffusion,
    Chung et al. ([2022](#bib.bib24)) remarkably reduced the number of sampling steps
    with a better initialization by starting from forward-diffused data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与在像素空间应用扩散过程不同，Esser 等人（[2021](#bib.bib43)）在离散潜在空间上使用了多项式扩散过程（Hoogeboom 等人，[2021](#bib.bib69)；Austin
    等人，[2021](#bib.bib2)），并对逆过程进行了自回归因式分解。这些设计使得 ImageBART 能够生成高分辨率的图像，例如 $300\times
    1800$。类似地，Rombach 等人（[2022](#bib.bib161)）提出了一种潜在扩散模型（LDM），以降低 DMs 的训练成本，同时提高视觉质量，该模型可以应用于高分辨率
    $1024^{2}$ 像素的图像修复任务。为了克服扩散模型中的大量迭代限制，Li 等人（[2022c](#bib.bib110)）提出了一种具有解耦概率建模的空间扩散模型（SDM），其中均值项指的是修复结果，方差项则测量不确定性。与在逆条件扩散中从随机高斯噪声开始不同，Chung
    等人（[2022](#bib.bib24)）通过从前向扩散数据开始，以更好的初始化显著减少了采样步骤的数量。
- en: 2.3 Text-guided Image Inpainting
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 文本引导的图像修复
- en: Text-guided image inpainting takes an incomplete image and text description
    as input and generates text-aligned inpainting results. The main challenge lies
    in how to fuse the text and image semantic features, and how to focus on effective
    information in the text. Zhang et al. ([2020a](#bib.bib253)) proposed a dual attention
    mechanism to obtain the semantic feature of the masked region by finding unmatched
    words compared to the image and applied DAMSM loss (Xu et al., [2018b](#bib.bib224))
    to measure the similarity of text and image. Lin et al. ([2020](#bib.bib121))
    introduced an image-adaptive word demand module that removes redundant information
    and aggregates text features in the coarse stage. They also proposed a text-guided
    attention loss that pays more attention to the reconstruction of the region affected
    by the text. Zhang et al. ([2020c](#bib.bib261)) encoded text and image to sequential
    data and exploited the transformer architecture to let cross-modal features interact.
    To ensure that the inpainted image matches the text, they took the masked text
    and inpainted image as input to restore the text prompt. Wu et al. ([2021](#bib.bib213))
    incorporated word-level and sentence-level textual features into a two-stage generator
    by introducing a dual-attention module. To eliminate the effection of the background,
    the mask reconstruction module was devised to recover the corrupted object mask.
    Xie et al. ([2022](#bib.bib219)) applied multi-head self-attention as text-image
    interactive encoder. They created a semantic relation graph to compute non-Euclidean
    semantic relations between text and image, and used graph convolution to aggregate
    node features. Li et al. ([2023](#bib.bib102)) followed a coarse-to-fine image
    inpainting framework. They first employed a visual-aware textual filtering mechanism
    to adaptively concentrate on required words and then inserted filtered text features
    into the coarse network. Unlike (Zhang et al., [2020c](#bib.bib261)), they directly
    reconstructed text descriptions from inpainted images to guarantee multi-modal
    semantic alignment. To better preserve the non-defective regions during the text
    guidance, Ni et al. ([2023](#bib.bib141)) proposed a defect-free VQGAN to control
    receptive spreading and a sequence-to-sequence module to enable visual-language
    learning from multiple different perspectives, including text descriptions, low-level
    pixels, and high-level tokens. Recent methods are based on diffusion models.Shukla
    et al. ([2023](#bib.bib174)) focused on how to generate a high-quality text prompt
    to guide a text-to-image model-based inpainting network by analyzing inter-object
    relationships. They first constructed a scene graph based on object detector outputs
    and expanded it via a graph convolution network to obtain the features of the
    corrupted node. Finally, the generated text prompt and masked image were fed to
    the diffusion model to obtain the inpainted result. Wang et al. ([2023](#bib.bib198))
    found that object masks would force the inpainted images to rely more on text
    descriptions instead of the random mask. Then, they proposed Imagen Editor fine-tuned
    from Imagen (Saharia et al., [2022b](#bib.bib168)) with a new convolutional layer
    and designed an object masking strategy for better training. To facilitate the
    systematic evaluation of text-guided image inpainting, they established a benchmark
    called EditBench.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 文本引导的图像修复技术以不完整的图像和文本描述为输入，生成与文本对齐的修复结果。主要的挑战在于如何融合文本和图像的语义特征，以及如何关注文本中的有效信息。张等人（[2020a](#bib.bib253)）提出了一种双重注意力机制，通过寻找与图像不匹配的词汇来获得掩蔽区域的语义特征，并应用DAMSM损失（徐等人，[2018b](#bib.bib224)）来衡量文本和图像的相似性。林等人（[2020](#bib.bib121)）引入了一种图像自适应词需求模块，该模块在粗略阶段移除冗余信息并聚合文本特征。他们还提出了一种文本引导注意力损失，更加关注文本影响区域的重建。张等人（[2020c](#bib.bib261)）将文本和图像编码为序列数据，并利用变换器架构使跨模态特征进行交互。为了确保修复的图像与文本匹配，他们将掩蔽文本和修复图像作为输入，以恢复文本提示。吴等人（[2021](#bib.bib213)）通过引入双重注意力模块，将词级和句子级文本特征融合到一个两阶段生成器中。为了消除背景的影响，设计了掩蔽重建模块来恢复损坏的物体掩蔽。谢等人（[2022](#bib.bib219)）将多头自注意力应用于文本-图像交互编码器。他们创建了一个语义关系图来计算文本和图像之间的非欧几里得语义关系，并使用图卷积来聚合节点特征。李等人（[2023](#bib.bib102)）遵循了一种粗到细的图像修复框架。他们首先使用视觉感知的文本过滤机制自适应地集中于所需的词汇，然后将过滤后的文本特征插入到粗略网络中。与（张等人，[2020c](#bib.bib261)）不同，他们直接从修复后的图像中重建文本描述，以确保多模态语义对齐。为了更好地保留文本引导过程中非缺陷区域，倪等人（[2023](#bib.bib141)）提出了一种无缺陷的VQGAN来控制感受野扩展，以及一个序列到序列模块，以从文本描述、低级像素和高级标记等多个不同角度进行视觉语言学习。近期的方法基于扩散模型。Shukla等人（[2023](#bib.bib174)）关注于如何生成高质量的文本提示，以指导基于文本到图像模型的修复网络，通过分析物体之间的关系。他们首先基于物体检测器的输出构建场景图，并通过图卷积网络扩展它，以获取损坏节点的特征。最后，将生成的文本提示和掩蔽图像输入扩散模型，以获得修复结果。王等人（[2023](#bib.bib198)）发现物体掩蔽会迫使修复图像更多地依赖于文本描述，而不是随机掩蔽。然后，他们提出了从Imagen（Saharia等人，[2022b](#bib.bib168)）微调的Imagen
    Editor，增加了新的卷积层，并设计了一种物体掩蔽策略以便更好地进行训练。为了促进对文本引导图像修复的系统评估，他们建立了一个名为EditBench的基准。
- en: 2.4 Inpainting Mask
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 修补掩码
- en: 'In the development of image inpainting techniques, various artificial masks
    have been introduced. These masks can be roughly divided into two categories:
    regular masks and irregular masks. Fig. [6](#S2.F6 "Figure 6 ‣ 2.2 Stochastic
    Image Inpainting ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting:
    A Survey") summarizes these masks, where white pixels indicate missing regions.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '在图像修补技术的发展中，引入了各种人工掩码。这些掩码大致可分为两类：规则掩码和不规则掩码。图 [6](#S2.F6 "Figure 6 ‣ 2.2 Stochastic
    Image Inpainting ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting:
    A Survey") 总结了这些掩码，其中白色像素表示缺失区域。'
- en: Regular masks. A *square hole* that blocks the center area or random location
    are generally easier to construct. Lugmayr et al. ([2022](#bib.bib133)) introduced
    more regular masks, including *Super-Resolution $2\times$* (reserving pixels with
    a stride of 2), *Alternating lines* (removing every second row), *Expand* (leaving
    a small center crop of the input image), and *Half* (masking the half of the input
    image).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 规则掩码。一个*方形孔*，它阻挡中心区域或随机位置，通常更容易构造。Lugmayr 等人 ([2022](#bib.bib133)) 介绍了更多的规则掩码，包括*超分辨率
    $2\times$*（以2的步幅保留像素）、*交替线*（去除每第二行）、*扩展*（保留输入图像的小中心裁剪）和*一半*（遮罩输入图像的一半）。
- en: Irregular masks. *Letter masks* ((Bertalmio et al., [2000](#bib.bib7); Bian
    et al., [2022](#bib.bib8))) and *object-shaped masks* ((Criminisi et al., [2004](#bib.bib26);
    Yi et al., [2020](#bib.bib231))) are particularly designed for specific tasks,
    for example, caption removal and object removal. Liu et al. ([2018](#bib.bib122))
    introduced free-form masks, where the former collected random streaks and arbitrary
    holes from the results of the occlusion/dis-occlusion mask estimation method.
    The irregular masks shared by (Liu et al., [2018](#bib.bib122)) are very common
    in the existing inpainting methods. Suvorov et al. ([2022](#bib.bib183)) further
    split free-form masks into *narrow masks*, *large wide masks*, and *large box
    masks*, where two types of large masks are generated via an aggressive mask method
    sampling polygonal chains with a high random width and rectangles of random aspect
    ratios, respectively.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 不规则掩码。*字母掩码*（Bertalmio 等人，[2000](#bib.bib7); Bian 等人，[2022](#bib.bib8)）和*物体形状掩码*（Criminisi
    等人，[2004](#bib.bib26); Yi 等人，[2020](#bib.bib231)）特别设计用于特定任务，例如，去除标题和物体移除。Liu 等人
    ([2018](#bib.bib122)) 介绍了自由形式掩码，其中前者从遮挡/去遮挡掩码估计方法的结果中收集随机条纹和任意孔。不规则掩码由 (Liu 等人，[2018](#bib.bib122))
    分享，在现有的修补方法中非常常见。Suvorov 等人 ([2022](#bib.bib183)) 进一步将自由形式掩码分为*窄掩码*、*大宽掩码* 和*大框掩码*，其中两种大掩码通过激进的掩码方法生成，分别采用高随机宽度的多边形链和随机长宽比的矩形。
- en: 2.5 Loss Functions
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 损失函数
- en: For image inpainting, the loss functions affect features of different sizes.
    At the lowest level, a pixel reconstruction loss aims to recover the exact pixel
    values. We further discuss the total-variational (TV) loss (Rudin et al., [1992](#bib.bib165)),
    feature consistency loss, the perceptual loss (Johnson et al., [2016](#bib.bib84)),
    style loss (Gatys et al., [2016](#bib.bib50)), and adversarial loss (Goodfellow
    et al., [2014](#bib.bib51)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像修补，损失函数影响不同大小的特征。在最低层次，像素重建损失旨在恢复精确的像素值。我们进一步讨论总变差（TV）损失 (Rudin 等人，[1992](#bib.bib165))、特征一致性损失、感知损失
    (Johnson 等人，[2016](#bib.bib84))、风格损失 (Gatys 等人，[2016](#bib.bib50)) 和对抗损失 (Goodfellow
    等人，[2014](#bib.bib51))。
- en: As input, an inpainting network accepts an input image $\mathbf{I}_{in}$ and
    a binary mask $\mathbf{M}$ describing the missing regions (where 0 means the valid
    pixel and 1 means the missing pixel). The output of the network is a complete
    image $\mathbf{I}_{out}$. The loss functions are formulated as follows.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输入，修补网络接受输入图像 $\mathbf{I}_{in}$ 和描述缺失区域的二进制掩码 $\mathbf{M}$（其中0表示有效像素，1表示缺失像素）。网络的输出是完整的图像
    $\mathbf{I}_{out}$。损失函数的公式如下。
- en: 'Pixel-wise reconstruction loss. In the literature, the pixel-wise reconstruction
    loss often has two types: $\ell_{1}$ loss (Eq. ([1](#S2.E1 "In 2.5 Loss Functions
    ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey")))
    and weighted $\ell_{1}$ loss (Eq. ([3](#S2.E3 "In 2.5 Loss Functions ‣ 2 Image
    Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey"))). The
    key point is how the valid and unknown regions differ in the loss function. The
    detailed formulations are as follows,'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '像素级重建损失。在文献中，像素级重建损失通常有两种类型：$\ell_{1}$ 损失（见 Eq. ([1](#S2.E1 "In 2.5 Loss Functions
    ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey")))
    和加权 $\ell_{1}$ 损失（见 Eq. ([3](#S2.E3 "In 2.5 Loss Functions ‣ 2 Image Inpainting
    ‣ Deep Learning-based Image and Video Inpainting: A Survey"))）。关键在于有效区域和未知区域在损失函数中的区别。详细公式如下，'
- en: '|  | $\mathcal{L}_{wpr}=&#124;&#124;(\mathbf{I}_{gt}-\mathbf{I}_{out})&#124;&#124;_{1}.$
    |  | (1) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{wpr}=&#124;&#124;(\mathbf{I}_{gt}-\mathbf{I}_{out})&#124;&#124;_{1}.$
    |  | (1) |'
- en: where $\mathbf{I}_{gt}$ is the ground-truth complete image.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{I}_{gt}$ 是真实完整图像。
- en: '|  | $\begin{split}\mathcal{L}_{valid}&amp;=\frac{1}{sum(\mathbbm{1}-\mathbf{M})}&#124;&#124;(\mathbf{I}_{gt}-\mathbf{I}_{out})\odot(\mathbbm{1}-\mathbf{M})&#124;&#124;_{1},\\
    \mathcal{L}_{hole}&amp;=\frac{1}{sum(\mathbf{M})}&#124;&#124;(\mathbf{I}_{gt}-\mathbf{I}_{out})\odot\mathbf{M}&#124;&#124;_{1},\end{split}$
    |  | (2) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}_{valid}&amp;=\frac{1}{sum(\mathbbm{1}-\mathbf{M})}&#124;&#124;(\mathbf{I}_{gt}-\mathbf{I}_{out})\odot(\mathbbm{1}-\mathbf{M})&#124;&#124;_{1},\\
    \mathcal{L}_{hole}&amp;=\frac{1}{sum(\mathbf{M})}&#124;&#124;(\mathbf{I}_{gt}-\mathbf{I}_{out})\odot\mathbf{M}&#124;&#124;_{1},\end{split}$
    |  | (2) |'
- en: where $\odot$ is the element-wise product operation, and $sum(\mathbf{M})$ is
    the number of non-zero elements in $\mathbf{M}$. Then the weighted $\ell_{1}$
    loss is formulated as
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\odot$ 是逐元素乘积操作，而 $sum(\mathbf{M})$ 是 $\mathbf{M}$ 中非零元素的数量。然后加权 $\ell_{1}$
    损失公式为
- en: '|  | $\mathcal{L}_{pr}=\mathcal{L}_{valid}+\alpha\cdot\mathcal{L}_{hole},$
    |  | (3) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{pr}=\mathcal{L}_{valid}+\alpha\cdot\mathcal{L}_{hole},$
    |  | (3) |'
- en: where $\alpha$ is the balancing factor. It is well known that the $\ell_{1}$
    loss can capture the low-frequency components, whereas it struggles to restore
    the high-frequency components (Isola et al., [2017](#bib.bib81); Ledig et al.,
    [2017](#bib.bib97)).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 是平衡因子。众所周知，$\ell_{1}$ 损失可以捕捉低频分量，而在恢复高频分量方面则较为困难（Isola 等， [2017](#bib.bib81);
    Ledig 等， [2017](#bib.bib97)）。
- en: 'Total-variation loss. Total-variation loss can be applied to ameliorate the
    potential checkerboard artifacts introduced by the perceptual loss. The formulation
    is:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 总变差损失。总变差损失可以用于改善感知损失引入的潜在棋盘效应。其公式为：
- en: '|  | $\begin{split}\mathcal{L}_{tv}=&#124;&#124;\mathbf{I}_{mer}(i,j+1)-\mathbf{I}_{mer}(i,j)&#124;&#124;_{1}\\
    +&#124;&#124;\mathbf{I}_{mer}(i+1,j)-\mathbf{I}_{mer}(i,j)&#124;&#124;_{1}.\end{split}$
    |  | (4) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}_{tv}=&#124;&#124;\mathbf{I}_{mer}(i,j+1)-\mathbf{I}_{mer}(i,j)&#124;&#124;_{1}\\
    +&#124;&#124;\mathbf{I}_{mer}(i+1,j)-\mathbf{I}_{mer}(i,j)&#124;&#124;_{1}.\end{split}$
    |  | (4) |'
- en: where $\mathbf{I}_{mer}=\mathbf{I}_{out}\odot\mathbf{M}+\mathbf{I}_{gt}\odot(\mathbbm{1}-\mathbf{M})$
    is the merged (completed) image.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{I}_{mer}=\mathbf{I}_{out}\odot\mathbf{M}+\mathbf{I}_{gt}\odot(\mathbbm{1}-\mathbf{M})$
    是合并（完成）图像。
- en: 'Feature consistency loss. This loss constrains extracted feature maps of the
    prediction with guidance from ground truth images:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 特征一致性损失。该损失约束了预测中提取的特征图，以从真实图像中获取指导：
- en: '|  | $\begin{split}\mathcal{L}_{fc}=\sum_{y\in\Omega}&#124;&#124;\Phi_{m}(\mathbf{I}_{in})_{y}-\Phi_{n}(\mathbf{I}_{gt})_{y}&#124;&#124;_{2}^{2}.\end{split}$
    |  | (5) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}_{fc}=\sum_{y\in\Omega}&#124;&#124;\Phi_{m}(\mathbf{I}_{in})_{y}-\Phi_{n}(\mathbf{I}_{gt})_{y}&#124;&#124;_{2}^{2}.\end{split}$
    |  | (5) |'
- en: where $\Omega$ is the missing regions, $\Phi_{m}(\cdot)$ is the feature map
    of the selected layer in the inpainting network, and $\Phi_{n}(\cdot)$ is the
    feature map of the corresponding layer in the inpainting network or pre-trained
    VGG models. $\Phi_{m}(\cdot)$ and $\Phi_{n}(\cdot)$ must have the same shape.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Omega$ 是缺失区域，$\Phi_{m}(\cdot)$ 是修补网络中选定层的特征图，$\Phi_{n}(\cdot)$ 是修补网络或预训练
    VGG 模型中相应层的特征图。$\Phi_{m}(\cdot)$ 和 $\Phi_{n}(\cdot)$ 必须具有相同的形状。
- en: '![Refer to caption](img/b594ccf04e1e047806485c2834d3b020.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b594ccf04e1e047806485c2834d3b020.png)'
- en: 'Figure 7: Some examples of image inpainting datasets.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：一些图像修补数据集的示例。
- en: 'Perceptual loss. The perceptual loss is first proposed in style transfer and
    super-resolution tasks. This loss measures the semantic/content difference between
    inpainted and ground-truth images, and thus encourages the inpainting generator
    to restore the semantics of missing regions. The perceptual loss is computed in
    high-level feature representations and is formulated as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 感知损失。感知损失首次在风格迁移和超分辨率任务中提出。此损失衡量修补图像与真实图像之间的语义/内容差异，从而鼓励修补生成器恢复缺失区域的语义。感知损失在高层特征表示中计算，公式为：
- en: '|  | $\mathcal{L}_{per}=\sum_{i}&#124;&#124;\Psi_{i}(\mathbf{I}_{out})-\Psi_{i}(\mathbf{I}_{gt})&#124;&#124;_{1}+&#124;&#124;\Psi_{i}(\mathbf{I}_{mer})-\Psi_{i}(\mathbf{I}_{gt})&#124;&#124;_{1},$
    |  | (6) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{per}=\sum_{i}&#124;&#124;\Psi_{i}(\mathbf{I}_{out})-\Psi_{i}(\mathbf{I}_{gt})&#124;&#124;_{1}+&#124;&#124;\Psi_{i}(\mathbf{I}_{mer})-\Psi_{i}(\mathbf{I}_{gt})&#124;&#124;_{1},$
    |  | (6) |'
- en: 'where $\Psi_{i}(*)$ is the feature map of $i$-th layer in the VGG-16/19 network (Simonyan
    and Zisserman, [2014](#bib.bib175)) pre-trained on ImageNet (Deng et al., [2009](#bib.bib31)).
    Instead of using the common VGG network, Suvorov et al. ([2022](#bib.bib183))
    suggested using a base network with a fast-growing receptive field for large-mask
    inpainting and utilized the pre-trained segmentation network (ResNet50 with dilated
    convolutions (Zhou et al., [2018](#bib.bib271))) to compute the so-called high
    receptive field perceptual loss. Note that, some works used 2-norm in Eq. ([6](#S2.E6
    "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video
    Inpainting: A Survey")) to compute perceptual loss.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$\Psi_{i}(*)$是VGG-16/19网络（Simonyan和Zisserman，[2014](#bib.bib175)）在ImageNet（Deng等，[2009](#bib.bib31)）上预训练的第$i$层特征图。Suvorov等人（[2022](#bib.bib183)）建议使用具有快速增长感受野的基础网络进行大遮罩修补，并利用预训练的分割网络（带有膨胀卷积的ResNet50（Zhou等，[2018](#bib.bib271)））计算所谓的高感受野感知损失。需要注意的是，一些工作在公式（[6](#S2.E6
    "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video
    Inpainting: A Survey")）中使用了2范数来计算感知损失。'
- en: Style loss. Similar to the perceptual loss, the style loss also depends on higher-level
    features extracted from a pre-trained network. This loss is applied to penalize
    the style difference between inpainted and ground-truth images, e.g., texture
    details and common patterns. Mathematically, the style loss measures the similarities
    of Gram matrices of image features, instead of the feature reconstruction in the
    perceptual loss. The detailed formulation can be written,
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 风格损失。与感知损失类似，风格损失也依赖于从预训练网络提取的高层特征。此损失用于惩罚修补图像与真实图像之间的风格差异，例如纹理细节和常见模式。从数学上讲，风格损失衡量的是图像特征的Gram矩阵的相似性，而不是感知损失中的特征重建。其详细公式可以写作：
- en: '|  | $\mathcal{L}_{sty}=\sum_{i}&#124;&#124;\Phi_{i}(\mathbf{I}_{out})-\Phi_{i}(\mathbf{I}_{gt})&#124;&#124;_{1}+&#124;&#124;\Phi_{i}(\mathbf{I}_{mer})-\Phi_{i}(\mathbf{I}_{gt})&#124;&#124;_{1},$
    |  | (7) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{sty}=\sum_{i}&#124;&#124;\Phi_{i}(\mathbf{I}_{out})-\Phi_{i}(\mathbf{I}_{gt})&#124;&#124;_{1}+&#124;&#124;\Phi_{i}(\mathbf{I}_{mer})-\Phi_{i}(\mathbf{I}_{gt})&#124;&#124;_{1},$
    |  | (7) |'
- en: where $\Phi_{i}(\cdot)=\Psi_{i}(\cdot)\Psi_{i}(\cdot)^{T}$ is the Gram matrix (Gatys
    et al., [2016](#bib.bib50)).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\Phi_{i}(\cdot)=\Psi_{i}(\cdot)\Psi_{i}(\cdot)^{T}$是Gram矩阵（Gatys等，[2016](#bib.bib50)）。
- en: Besides using Gram matrices to model the style information, the mean and standard
    deviation of image features are commonly used in style transfer (Huang and Belongie,
    [2017](#bib.bib76); Deng et al., [2020](#bib.bib32)). The formulation is written
    as,
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用Gram矩阵来建模风格信息，图像特征的均值和标准差也常用于风格迁移（Huang和Belongie，[2017](#bib.bib76)；Deng等，[2020](#bib.bib32)）。其公式写作：
- en: '|  | <math   alttext="\begin{split}\mathcal{L}_{sty\_mean}=\sum_{i}&#124;&#124;\mu(\Psi_{i}(\mathbf{I}_{out}))-\mu(\Psi_{i}(\mathbf{I}_{gt}))&#124;&#124;_{2}\\
    +&#124;&#124;\mu(\Psi_{i}(\mathbf{I}_{mer}))-\mu(\Psi_{i}(\mathbf{I}_{gt}))&#124;&#124;_{2},\\'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}\mathcal{L}_{sty\_mean}=\sum_{i}&#124;&#124;\mu(\Psi_{i}(\mathbf{I}_{out}))-\mu(\Psi_{i}(\mathbf{I}_{gt}))&#124;&#124;_{2}\\
    +&#124;&#124;\mu(\Psi_{i}(\mathbf{I}_{mer}))-\mu(\Psi_{i}(\mathbf{I}_{gt}))&#124;&#124;_{2},\\'
- en: \mathcal{L}_{sty\_std}=\sum_{i}&#124;&#124;\sigma(\Psi_{i}(\mathbf{I}_{out}))-\sigma(\Psi_{i}(\mathbf{I}_{gt}))&#124;&#124;_{2}\\
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: \mathcal{L}_{sty\_std}=\sum_{i}&#124;&#124;\sigma(\Psi_{i}(\mathbf{I}_{out}))-\sigma(\Psi_{i}(\mathbf{I}_{gt}))&#124;&#124;_{2}\\
- en: +&#124;&#124;\sigma(\Psi_{i}(\mathbf{I}_{mer}))-\sigma(\Psi_{i}(\mathbf{I}_{gt}))&#124;&#124;_{2},\\
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: +&#124;&#124;\sigma(\Psi_{i}(\mathbf{I}_{mer}))-\sigma(\Psi_{i}(\mathbf{I}_{gt}))&#124;&#124;_{2},\\
- en: \mathcal{L}_{sty\_meanstd}=\mathcal{L}_{sty\_mean}+\mathcal{L}_{sty\_std},\end{split}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd columnalign="right" ><mrow ><msub ><mi >ℒ</mi><mrow  ><mi >s</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >t</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >y</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >_</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >a</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >n</mi></mrow></msub><mo rspace="0.111em"  >=</mo><mrow ><munder ><mo movablelimits="false"
    rspace="0em"  >∑</mo><mi >i</mi></munder><msub ><mrow ><mo stretchy="false" >‖</mo><mrow
    ><mrow ><mi  >μ</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><msub ><mi mathvariant="normal"  >Ψ</mi><mi >i</mi></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi >𝐈</mi><mrow
    ><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >u</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >t</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >μ</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi mathvariant="normal"  >Ψ</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐈</mi><mrow ><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >t</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo stretchy="false" >‖</mo></mrow><mn >2</mn></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><mo >+</mo><msub ><mrow ><mo stretchy="false"
    >‖</mo><mrow ><mrow ><mi >μ</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><msub ><mi mathvariant="normal" >Ψ</mi><mi >i</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi
    >𝐈</mi><mrow ><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >r</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >μ</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi mathvariant="normal"  >Ψ</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐈</mi><mrow ><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >t</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo stretchy="false" >‖</mo></mrow><mn >2</mn></msub></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="right" ><mrow ><msub ><mi >ℒ</mi><mrow
    ><mi >s</mi><mo lspace="0em" rspace="0em" >​</mo><mi >t</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >y</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathvariant="normal"
    >_</mi><mo lspace="0em" rspace="0em" >​</mo><mi >s</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >t</mi><mo lspace="0em" rspace="0em" >​</mo><mi >d</mi></mrow></msub><mo
    rspace="0.111em" >=</mo><mrow ><munder ><mo movablelimits="false" rspace="0em"
    >∑</mo><mi >i</mi></munder><msub ><mrow ><mo stretchy="false" >‖</mo><mrow ><mrow
    ><mi >σ</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow
    ><msub ><mi mathvariant="normal" >Ψ</mi><mi >i</mi></msub><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi >𝐈</mi><mrow ><mi >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >u</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >t</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo stretchy="false"
    >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >σ</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi mathvariant="normal"  >Ψ</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐈</mi><mrow ><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >t</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo stretchy="false" >‖</mo></mrow><mn >2</mn></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><mo >+</mo><msub ><mrow ><mo stretchy="false"
    >‖</mo><mrow ><mrow ><mi >σ</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><msub ><mi mathvariant="normal" >Ψ</mi><mi >i</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi
    >𝐈</mi><mrow ><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >r</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >σ</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi mathvariant="normal"  >Ψ</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐈</mi><mrow ><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >t</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo stretchy="false" >‖</mo></mrow><mn >2</mn></msub></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="right" ><mrow ><mrow ><msub
    ><mi >ℒ</mi><mrow ><mi >s</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >t</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >y</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathvariant="normal"  >_</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >m</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >a</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >s</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >t</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >d</mi></mrow></msub><mo >=</mo><mrow ><msub ><mi >ℒ</mi><mrow ><mi >s</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >t</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >y</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >_</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >e</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >a</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >n</mi></mrow></msub><mo >+</mo><msub ><mi >ℒ</mi><mrow ><mi >s</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >t</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >y</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >_</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >s</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >t</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >d</mi></mrow></msub></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℒ</ci><apply ><ci  >𝑠</ci><ci
    >𝑡</ci><ci >𝑦</ci><ci  >_</ci><ci >𝑚</ci><ci >𝑒</ci><ci  >𝑎</ci><ci >𝑛</ci></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="latexml" >norm</csymbol><apply
    ><apply ><ci >𝜇</ci><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >Ψ</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐈</ci><apply ><ci >𝑜</ci><ci >𝑢</ci><ci >𝑡</ci></apply></apply></apply></apply><apply
    ><ci >𝜇</ci><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >Ψ</ci><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐈</ci><apply
    ><ci >𝑔</ci><ci >𝑡</ci></apply></apply></apply></apply></apply></apply><cn type="integer"  >2</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="latexml" >norm</csymbol><apply
    ><apply ><ci >𝜇</ci><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >Ψ</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐈</ci><apply ><ci >𝑚</ci><ci >𝑒</ci><ci >𝑟</ci></apply></apply></apply></apply><apply
    ><ci >𝜇</ci><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >Ψ</ci><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐈</ci><apply
    ><ci >𝑔</ci><ci >𝑡</ci></apply></apply></apply></apply></apply></apply><cn type="integer"  >2</cn></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ℒ</ci><apply ><ci >𝑠</ci><ci >𝑡</ci><ci
    >𝑦</ci><ci >_</ci><ci >𝑠</ci><ci >𝑡</ci><ci >𝑑</ci></apply></apply><apply ><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="latexml" >norm</csymbol><apply
    ><apply ><ci >𝜎</ci><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >Ψ</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐈</ci><apply ><ci >𝑜</ci><ci >𝑢</ci><ci >𝑡</ci></apply></apply></apply></apply><apply
    ><ci >𝜎</ci><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >Ψ</ci><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐈</ci><apply
    ><ci >𝑔</ci><ci >𝑡</ci></apply></apply></apply></apply></apply></apply><cn type="integer"  >2</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="latexml" >norm</csymbol><apply
    ><apply ><ci >𝜎</ci><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >Ψ</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐈</ci><apply ><ci >𝑚</ci><ci >𝑒</ci><ci >𝑟</ci></apply></apply></apply></apply><apply
    ><ci >𝜎</ci><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >Ψ</ci><ci
    >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐈</ci><apply
    ><ci >𝑔</ci><ci >𝑡</ci></apply></apply></apply></apply></apply></apply><cn type="integer"  >2</cn></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℒ</ci><apply ><ci >𝑠</ci><ci
    >𝑡</ci><ci >𝑦</ci><ci >_</ci><ci >𝑚</ci><ci >𝑒</ci><ci >𝑎</ci><ci >𝑛</ci><ci >𝑠</ci><ci
    >𝑡</ci><ci >𝑑</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℒ</ci><apply ><ci >𝑠</ci><ci >𝑡</ci><ci >𝑦</ci><ci >_</ci><ci >𝑚</ci><ci >𝑒</ci><ci
    >𝑎</ci><ci >𝑛</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℒ</ci><apply ><ci >𝑠</ci><ci >𝑡</ci><ci >𝑦</ci><ci >_</ci><ci >𝑠</ci><ci >𝑡</ci><ci
    >𝑑</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\mathcal{L}_{sty\_mean}=\sum_{i}&#124;&#124;\mu(\Psi_{i}(\mathbf{I}_{out}))-\mu(\Psi_{i}(\mathbf{I}_{gt}))&#124;&#124;_{2}\\
    +&#124;&#124;\mu(\Psi_{i}(\mathbf{I}_{mer}))-\mu(\Psi_{i}(\mathbf{I}_{gt}))&#124;&#124;_{2},\\
    \mathcal{L}_{sty\_std}=\sum_{i}&#124;&#124;\sigma(\Psi_{i}(\mathbf{I}_{out}))-\sigma(\Psi_{i}(\mathbf{I}_{gt}))&#124;&#124;_{2}\\
    +&#124;&#124;\sigma(\Psi_{i}(\mathbf{I}_{mer}))-\sigma(\Psi_{i}(\mathbf{I}_{gt}))&#124;&#124;_{2},\\
    \mathcal{L}_{sty\_meanstd}=\mathcal{L}_{sty\_mean}+\mathcal{L}_{sty\_std},\end{split}</annotation></semantics></math>
    |  | (8) |
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: \mathcal{L}_{sty\_meanstd}=\mathcal{L}_{sty\_mean}+\mathcal{L}_{sty\_std},\end{split}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd columnalign="right" ><mrow ><msub ><mi >ℒ</mi><mrow  ><mi >s</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >t</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >y</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >_</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >a</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >n</mi></mrow></msub><mo rspace="0.111em"  >=</mo><mrow ><munder ><mo movablelimits="false"
    rspace="0em"  >∑</mo><mi >i</mi></munder><msub ><mrow ><mo stretchy="false" >‖</mo><mrow
    ><mrow ><mi  >μ</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><msub ><mi mathvariant="normal"  >Ψ</mi><mi >i</mi></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi >𝐈</mi><mrow
    ><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >u</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >t</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >μ</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi mathvariant="normal"  >Ψ</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐈</mi><mrow ><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >t</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo stretchy="false" >‖</mo></mrow><mn >2</mn></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><mo >+</mo><msub ><mrow ><mo stretchy="false"
    >‖</mo><mrow ><mrow ><mi >μ</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><msub ><mi mathvariant="normal" >Ψ</mi><mi >i</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi
    >𝐈</mi><mrow ><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >r</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >μ</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi mathvariant="normal"  >Ψ</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐈</mi><mrow ><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >t</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo stretchy="false" >‖</mo></mrow><mn >2</mn></msub></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="right" ><mrow ><mrow ><msub
    ><mi >ℒ</mi><mrow ><mi >s</mi><mo lspace="0em" rspace="0em" >​</mo><mi >t</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >y</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    mathvariant="normal" >_</mi><mo lspace="0em" rspace="0em" >​</mo><mi >s</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >t</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >d</mi></mrow></msub><mo rspace="0.111em"  >=</mo><mrow ><munder ><mo movablelimits="false"
    rspace="0em"  >∑</mo><mi >i</mi></munder><msub ><mrow ><mo stretchy="false" >‖</mo><mrow
    ><mrow ><mi >σ</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><msub ><mi mathvariant="normal" >Ψ</mi><mi >i</mi></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi >𝐈</mi><mrow
    ><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >u</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >t</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo >−</mo><mrow ><mi >σ</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi mathvariant="normal"  >Ψ</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐈</mi><mrow ><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >t</mi></mrow></msub><mo stretchy="false" >)</mo></mrow></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo stretchy="false" >‖</mo></mrow><mn >2</mn></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><msub ><mi >ℒ</mi><mrow ><mi >s</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >t</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >y</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathvariant="normal"  >_</mi><mo
    lspace="0em" r
- en: where $\mu(*)$, $\sigma(*)$ are the mean and standard deviation, computed over
    spatial dimensions independently for each sample and each channel.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu(*)$ 和 $\sigma(*)$ 是均值和标准差，分别在空间维度上独立计算，每个样本和每个通道。
- en: 'Adversarial loss. GANs (Goodfellow et al., [2014](#bib.bib51)) are widely used
    in many image generation tasks. They employ an adversarial loss to force the output
    distribution to be close to the “real” distribution. The adversarial loss can
    counteract blurry results and enhance the visual realism of the output image.
    Therefore, it is often applied in GAN-based inpainting networks. To compute the
    adversarial loss, a discriminator network ($D$) is necessary, which interacts
    with the generator network ($G$). The hinge version (Lim and Ye, [2017](#bib.bib119))
    of the adversarial loss can be formulated as:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗损失。GANs（Goodfellow 等，[2014](#bib.bib51)）在许多图像生成任务中被广泛使用。它们使用对抗损失来强制输出分布接近“真实”分布。对抗损失可以抵消模糊结果，增强输出图像的视觉真实感。因此，它通常应用于基于
    GAN 的图像修复网络。为了计算对抗损失，需要一个鉴别网络（$D$），它与生成网络（$G$）进行交互。对抗损失的 hinge 版本（Lim 和 Ye，[2017](#bib.bib119)）可以表示为：
- en: '|  | $\begin{split}\mathcal{L}_{D}=\mathbb{E}_{\mathbf{\mathbf{I}}\sim p_{data}(\mathbf{\mathbf{I}})}\Bigl{[}max(0,1-D(\mathbf{I}_{gt}))\Bigr{]}\\
    +\mathbb{E}_{\mathbf{\mathbf{I}_{mer}}\sim p_{\mathbf{I}_{mer}}(\mathbf{\mathbf{I}_{mer}})}\Bigl{[}max(0,1+D(\mathbf{I}_{mer}))\Bigr{]},\end{split}$
    |  | (9) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}_{D}=\mathbb{E}_{\mathbf{\mathbf{I}}\sim p_{data}(\mathbf{\mathbf{I}})}\Bigl{[}max(0,1-D(\mathbf{I}_{gt}))\Bigr{]}\\
    +\mathbb{E}_{\mathbf{\mathbf{I}_{mer}}\sim p_{\mathbf{I}_{mer}}(\mathbf{\mathbf{I}_{mer}})}\Bigl{[}max(0,1+D(\mathbf{I}_{mer}))\Bigr{]},\end{split}$
    |  | (9) |'
- en: 'where $D(\mathbf{I}_{gt})$ and $D(\mathbf{I}_{mer})$ are the logits output
    from discriminator $D$. The objective function for generator $G$ can be denoted
    as:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D(\mathbf{I}_{gt})$ 和 $D(\mathbf{I}_{mer})$ 是来自鉴别器 $D$ 的 logits 输出。生成器 $G$
    的目标函数可以表示为：
- en: '|  | $\mathcal{L}_{G}=-\mathbb{E}_{\mathbf{\mathbf{I}_{mer}}\sim p_{\mathbf{I}_{mer}}(\mathbf{\mathbf{I}_{mer}})}\Bigl{[}D(\mathbf{I}_{mer})\Bigr{]}.$
    |  | (10) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{G}=-\mathbb{E}_{\mathbf{\mathbf{I}_{mer}}\sim p_{\mathbf{I}_{mer}}(\mathbf{\mathbf{I}_{mer}})}\Bigl{[}D(\mathbf{I}_{mer})\Bigr{]}.$
    |  | (10) |'
- en: 'Except for the above hinge version, other types of adversarial losses are also
    adopted: GAN (Goodfellow et al., [2014](#bib.bib51)), WGAN (Arjovsky et al., [2017](#bib.bib1)),
    LSGAN (Mao et al., [2017](#bib.bib136)), etc.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述的 hinge 版本，还有其他类型的对抗损失被采用：GAN（Goodfellow 等，[2014](#bib.bib51)）、WGAN（Arjovsky
    等，[2017](#bib.bib1)）、LSGAN（Mao 等，[2017](#bib.bib136)）等。
- en: 2.6 Datasets
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 数据集
- en: 'In the literature, there are six prevalent and public datasets for evaluating
    image inpainting. These datasets cover various types of images, including faces
    (CelebA and CelebA-HQ), real-world encountered scenes (Places2), street scenes
    (Paris), texture (DTD), and objects (ImageNet). Several examples are shown in
    Fig. [7](#S2.F7 "Figure 7 ‣ 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey"). The details of the datasets are described
    below.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '文献中有六个流行的公开数据集用于评估图像修复。这些数据集涵盖了各种类型的图像，包括人脸（CelebA 和 CelebA-HQ）、真实世界遇到的场景（Places2）、街景（Paris）、纹理（DTD）和物体（ImageNet）。图示见图
    [7](#S2.F7 "Figure 7 ‣ 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey")。数据集的详细信息如下所述。'
- en: •
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CelebA dataset (Liu et al., [2015](#bib.bib129)): A large-scale face attribute
    dataset that contains 10,177 identities, each of which has about 20 images. In
    total, CelebA has 202,599 face images, each with 40 attribute annotations.'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CelebA 数据集（Liu 等，[2015](#bib.bib129)）：一个大规模的人脸属性数据集，包含 10,177 个身份，每个身份大约有 20
    张图像。总共有 202,599 张人脸图像，每张图像有 40 个属性注释。
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CelebA-HQ dataset (Karras et al., [2018](#bib.bib86)): The high-quality version
    of CelebA (Liu et al., [2015](#bib.bib129)) with JPEG artifacts removal, super-resolution
    operation, and cropping, etc. This dataset consists of 30,000 face images.'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CelebA-HQ 数据集（Karras 等，[2018](#bib.bib86)）：CelebA（Liu 等，[2015](#bib.bib129)）的高质量版本，包含
    JPEG 压缩伪影去除、超分辨率操作和裁剪等。该数据集包含 30,000 张人脸图像。
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Places2 dataset (Zhou et al., [2017](#bib.bib270)): A large-scale scene recognition
    dataset. Places365-Standard has 365 scene categories. The training set has 1,803,460
    images and the validation set contains 18,250 images.'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Places2 数据集（Zhou 等，[2017](#bib.bib270)）：一个大规模的场景识别数据集。Places365-Standard 包含
    365 个场景类别。训练集有 1,803,460 张图像，验证集包含 18,250 张图像。
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Paris StreetView dataset (Doersch et al., [2012](#bib.bib36)): This dataset
    consists of street-level imagery. It contains 14,900 images for training and 100
    images for testing.'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 巴黎街景数据集（Doersch 等，[2012](#bib.bib36)）：该数据集包含街道级别的图像。它包括 14,900 张用于训练的图像和 100
    张用于测试的图像。
- en: •
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DTD dataset (Cimpoi et al., [2014](#bib.bib25)): A describable texture dataset
    consisting of 5,640 images. According to human perception, these images are divided
    into 47 categories with 120 images per category.'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DTD 数据集（Cimpoi et al., [2014](#bib.bib25)）：一个可描述的纹理数据集，包含 5,640 张图像。根据人类感知，这些图像被分为
    47 类，每类 120 张图像。
- en: •
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ImageNet dataset (Deng et al., [2009](#bib.bib31)): A large-scale benchmark
    for object category classification. There are about 1.2 million training images
    and 50 thousand validation images.'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ImageNet 数据集（Deng et al., [2009](#bib.bib31)）：一个大规模的对象类别分类基准数据集。包含约 120 万张训练图像和
    5 万张验证图像。
- en: 'Table 2: Quantitative comparison of several representative image inpainting
    methods on CelebA-HQ and Places2\. $\ddagger$ Higher is better. $\dagger$ Lower
    is better. From M1 to M6, the mask ratios are 1%-10%, 10%-20%, 20%-30%, 30%-40%,
    40%-50%, and 50%-60%, respectively. Because of the heavy inference time, we do
    not show the results of RePaint for M1, M2, M4, and M6.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 CelebA-HQ 和 Places2 数据集上几种代表性图像修复方法的定量比较。 $\ddagger$ 数值越高越好。 $\dagger$
    数值越低越好。从 M1 到 M6，掩码比例分别为 1%-10%、10%-20%、20%-30%、30%-40%、40%-50% 和 50%-60%。由于推理时间较长，我们未显示
    RePaint 在 M1、M2、M4 和 M6 的结果。
- en: '|  | Dataset | CelebA-HQ | Places2 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据集 | CelebA-HQ | Places2 |'
- en: '|  | Mask | M1 | M2 | M3 | M4 | M5 | M6 | M1 | M2 | M3 | M4 | M5 | M6 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | Mask | M1 | M2 | M3 | M4 | M5 | M6 | M1 | M2 | M3 | M4 | M5 | M6 |'
- en: '| $\ell_{1}(\%)$ $\dagger$ | RFR | 1.59 | 2.47 | 3.58 | 4.90 | 6.44 | 9.47
    | 0.83 | 2.20 | 3.93 | 5.83 | 7.96 | 11.37 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| $\ell_{1}(\%)$ $\dagger$ | RFR | 1.59 | 2.47 | 3.58 | 4.90 | 6.44 | 9.47
    | 0.83 | 2.20 | 3.93 | 5.83 | 7.96 | 11.37 |'
- en: '| MADF | 0.47 | 1.30 | 2.40 | 3.72 | 5.26 | 8.43 | 0.80 | 2.18 | 3.96 | 5.91
    | 8.10 | 11.68 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| MADF | 0.47 | 1.30 | 2.40 | 3.72 | 5.26 | 8.43 | 0.80 | 2.18 | 3.96 | 5.91
    | 8.10 | 11.68 |'
- en: '| DSI | 0.60 | 1.65 | 3.08 | 4.80 | 6.83 | 11.11 | 0.88 | 2.42 | 4.48 | 6.75
    | 9.32 | 13.82 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| DSI | 0.60 | 1.65 | 3.08 | 4.80 | 6.83 | 11.11 | 0.88 | 2.42 | 4.48 | 6.75
    | 9.32 | 13.82 |'
- en: '| CR-Fill | 0.79 | 2.15 | 3.95 | 6.01 | 8.33 | 13.18 | 0.78 | 2.17 | 4.02 |
    6.11 | 8.46 | 12.43 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| CR-Fill | 0.79 | 2.15 | 3.95 | 6.01 | 8.33 | 13.18 | 0.78 | 2.17 | 4.02 |
    6.11 | 8.46 | 12.43 |'
- en: '| CoModGAN | 0.48 | 1.38 | 2.66 | 4.28 | 6.20 | 10.53 | 0.72 | 2.05 | 3.83
    | 5.89 | 8.27 | 12.58 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| CoModGAN | 0.48 | 1.38 | 2.66 | 4.28 | 6.20 | 10.53 | 0.72 | 2.05 | 3.83
    | 5.89 | 8.27 | 12.58 |'
- en: '| LGNet | 0.46 | 1.28 | 2.38 | 3.72 | 5.27 | 8.38 | 0.68 | 1.89 | 3.51 | 5.33
    | 7.41 | 10.86 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| LGNet | 0.46 | 1.28 | 2.38 | 3.72 | 5.27 | 8.38 | 0.68 | 1.89 | 3.51 | 5.33
    | 7.41 | 10.86 |'
- en: '| MAT | 0.83 | 1.74 | 3.00 | 4.52 | 6.30 | 9.98 | 1.07 | 2.53 | 4.48 | 6.69
    | 9.20 | 13.70 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| MAT | 0.83 | 1.74 | 3.00 | 4.52 | 6.30 | 9.98 | 1.07 | 2.53 | 4.48 | 6.69
    | 9.20 | 13.70 |'
- en: '| RePaint | - | - | 3.37 | - | 7.47 | - | - | - | 4.96 | - | 10.01 | 15.27
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| RePaint | - | - | 3.37 | - | 7.47 | - | - | - | 4.96 | - | 10.01 | 15.27
    |'
- en: '| PSNR $\ddagger$ | RFR | 36.39 | 31.87 | 29.07 | 26.87 | 25.09 | 22.51 | 35.74
    | 30.24 | 27.24 | 25.13 | 23.48 | 21.33 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| PSNR $\ddagger$ | RFR | 36.39 | 31.87 | 29.07 | 26.87 | 25.09 | 22.51 | 35.74
    | 30.24 | 27.24 | 25.13 | 23.48 | 21.33 |'
- en: '| MADF | 39.68 | 33.77 | 30.42 | 27.95 | 25.99 | 23.07 | 36.17 | 30.37 | 27.17
    | 25.00 | 23.31 | 21.10 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| MADF | 39.68 | 33.77 | 30.42 | 27.95 | 25.99 | 23.07 | 36.17 | 30.37 | 27.17
    | 25.00 | 23.31 | 21.10 |'
- en: '| DSI | 37.68 | 31.74 | 28.39 | 25.88 | 23.91 | 20.87 | 35.40 | 29.47 | 26.15
    | 23.91 | 22.19 | 19.75 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| DSI | 37.68 | 31.74 | 28.39 | 25.88 | 23.91 | 20.87 | 35.40 | 29.47 | 26.15
    | 23.91 | 22.19 | 19.75 |'
- en: '| CR-Fill | 35.67 | 29.87 | 26.60 | 24.29 | 22.53 | 19.70 | 36.35 | 30.32 |
    26.96 | 24.63 | 22.85 | 20.50 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| CR-Fill | 35.67 | 29.87 | 26.60 | 24.29 | 22.53 | 19.70 | 36.35 | 30.32 |
    26.96 | 24.63 | 22.85 | 20.50 |'
- en: '| CoModGAN | 39.56 | 33.15 | 29.41 | 26.62 | 24.49 | 21.16 | 37.00 | 30.82
    | 27.35 | 24.92 | 23.05 | 20.43 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| CoModGAN | 39.56 | 33.15 | 29.41 | 26.62 | 24.49 | 21.16 | 37.00 | 30.82
    | 27.35 | 24.92 | 23.05 | 20.43 |'
- en: '| LGNet | 40.04 | 33.99 | 30.54 | 27.99 | 26.01 | 23.12 | 37.62 | 31.61 | 28.18
    | 25.84 | 24.05 | 21.69 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| LGNet | 40.04 | 33.99 | 30.54 | 27.99 | 26.01 | 23.12 | 37.62 | 31.61 | 28.18
    | 25.84 | 24.05 | 21.69 |'
- en: '| MAT | 38.44 | 32.62 | 29.21 | 26.70 | 24.72 | 21.78 | 35.66 | 29.76 | 26.41
    | 24.09 | 22.30 | 19.81 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| MAT | 38.44 | 32.62 | 29.21 | 26.70 | 24.72 | 21.78 | 35.66 | 29.76 | 26.41
    | 24.09 | 22.30 | 19.81 |'
- en: '| RePaint | - | - | 28.38 | - | 23.16 | - | - | - | 26.04 | - | 21.72 | 18.99
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| RePaint | - | - | 28.38 | - | 23.16 | - | - | - | 26.04 | - | 21.72 | 18.99
    |'
- en: '| SSIM $\ddagger$ | RFR | 0.991 | 0.976 | 0.957 | 0.932 | 0.902 | 0.834 | 0.983
    | 0.952 | 0.911 | 0.862 | 0.805 | 0.699 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| SSIM $\ddagger$ | RFR | 0.991 | 0.976 | 0.957 | 0.932 | 0.902 | 0.834 | 0.983
    | 0.952 | 0.911 | 0.862 | 0.805 | 0.699 |'
- en: '| MADF | 0.995 | 0.984 | 0.967 | 0.945 | 0.917 | 0.848 | 0.984 | 0.953 | 0.910
    | 0.859 | 0.800 | 0.690 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| MADF | 0.995 | 0.984 | 0.967 | 0.945 | 0.917 | 0.848 | 0.984 | 0.953 | 0.910
    | 0.859 | 0.800 | 0.690 |'
- en: '| DSI | 0.992 | 0.976 | 0.951 | 0.918 | 0.877 | 0.778 | 0.982 | 0.945 | 0.892
    | 0.832 | 0.763 | 0.636 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| DSI | 0.992 | 0.976 | 0.951 | 0.918 | 0.877 | 0.778 | 0.982 | 0.945 | 0.892
    | 0.832 | 0.763 | 0.636 |'
- en: '| CR-Fill | 0.988 | 0.965 | 0.931 | 0.890 | 0.842 | 0.729 | 0.985 | 0.954 |
    0.909 | 0.855 | 0.794 | 0.675 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| CR-Fill | 0.988 | 0.965 | 0.931 | 0.890 | 0.842 | 0.729 | 0.985 | 0.954 |
    0.909 | 0.855 | 0.794 | 0.675 |'
- en: '| CoModGAN | 0.994 | 0.981 | 0.960 | 0.929 | 0.891 | 0.792 | 0.987 | 0.957
    | 0.914 | 0.860 | 0.796 | 0.671 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| CoModGAN | 0.994 | 0.981 | 0.960 | 0.929 | 0.891 | 0.792 | 0.987 | 0.957
    | 0.914 | 0.860 | 0.796 | 0.671 |'
- en: '| LGNet | 0.995 | 0.985 | 0.968 | 0.945 | 0.917 | 0.849 | 0.988 | 0.963 | 0.925
    | 0.878 | 0.823 | 0.714 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| LGNet | 0.995 | 0.985 | 0.968 | 0.945 | 0.917 | 0.849 | 0.988 | 0.963 | 0.925
    | 0.878 | 0.823 | 0.714 |'
- en: '| MAT | 0.993 | 0.980 | 0.959 | 0.931 | 0.897 | 0.814 | 0.983 | 0.948 | 0.898
    | 0.839 | 0.772 | 0.645 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| MAT | 0.993 | 0.980 | 0.959 | 0.931 | 0.897 | 0.814 | 0.983 | 0.948 | 0.898
    | 0.839 | 0.772 | 0.645 |'
- en: '| RePaint | - | - | 0.952 | - | 0.867 | - | - | - | 0.892 | - | 0.750 | 0.606
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| RePaint | - | - | 0.952 | - | 0.867 | - | - | - | 0.892 | - | 0.750 | 0.606
    |'
- en: '| MS-SSIM $\ddagger$ | RFR | 0.992 | 0.976 | 0.956 | 0.933 | 0.900 | 0.830
    | 0.986 | 0.960 | 0.924 | 0.880 | 0.828 | 0.731 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| MS-SSIM $\ddagger$ | RFR | 0.992 | 0.976 | 0.956 | 0.933 | 0.900 | 0.830
    | 0.986 | 0.960 | 0.924 | 0.880 | 0.828 | 0.731 |'
- en: '| MADF | 0.994 | 0.983 | 0.966 | 0.942 | 0.913 | 0.846 | 0.987 | 0.961 | 0.923
    | 0.877 | 0.824 | 0.722 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| MADF | 0.994 | 0.983 | 0.966 | 0.942 | 0.913 | 0.846 | 0.987 | 0.961 | 0.923
    | 0.877 | 0.824 | 0.722 |'
- en: '| DSI | 0.992 | 0.976 | 0.952 | 0.919 | 0.878 | 0.784 | 0.984 | 0.952 | 0.905
    | 0.850 | 0.785 | 0.664 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| DSI | 0.992 | 0.976 | 0.952 | 0.919 | 0.878 | 0.784 | 0.984 | 0.952 | 0.905
    | 0.850 | 0.785 | 0.664 |'
- en: '| CR-Fill | 0.987 | 0.963 | 0.928 | 0.887 | 0.839 | 0.732 | 0.987 | 0.960 |
    0.920 | 0.872 | 0.814 | 0.704 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| CR-Fill | 0.987 | 0.963 | 0.928 | 0.887 | 0.839 | 0.732 | 0.987 | 0.960 |
    0.920 | 0.872 | 0.814 | 0.704 |'
- en: '| CoModGAN | 0.994 | 0.980 | 0.958 | 0.926 | 0.888 | 0.793 | 0.988 | 0.961
    | 0.921 | 0.870 | 0.810 | 0.692 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| CoModGAN | 0.994 | 0.980 | 0.958 | 0.926 | 0.888 | 0.793 | 0.988 | 0.961
    | 0.921 | 0.870 | 0.810 | 0.692 |'
- en: '| LGNet | 0.995 | 0.984 | 0.968 | 0.945 | 0.917 | 0.851 | 0.990 | 0.968 | 0.935
    | 0.894 | 0.844 | 0.744 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| LGNet | 0.995 | 0.984 | 0.968 | 0.945 | 0.917 | 0.851 | 0.990 | 0.968 | 0.935
    | 0.894 | 0.844 | 0.744 |'
- en: '| MAT | 0.994 | 0.980 | 0.960 | 0.932 | 0.898 | 0.818 | 0.986 | 0.957 | 0.913
    | 0.859 | 0.796 | 0.676 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| MAT | 0.994 | 0.980 | 0.960 | 0.932 | 0.898 | 0.818 | 0.986 | 0.957 | 0.913
    | 0.859 | 0.796 | 0.676 |'
- en: '| RePaint | - | - | 0.953 | - | 0.870 | - | - | - | 0.903 | - | 0.771 | 0.633
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| RePaint | - | - | 0.953 | - | 0.870 | - | - | - | 0.903 | - | 0.771 | 0.633
    |'
- en: '| FID $\dagger$ | RFR | 0.86 | 1.68 | 2.67 | 3.77 | 5.21 | 7.60 | 2.62 | 5.99
    | 9.47 | 12.90 | 16.62 | 22.13 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| FID $\dagger$ | RFR | 0.86 | 1.68 | 2.67 | 3.77 | 5.21 | 7.60 | 2.62 | 5.99
    | 9.47 | 12.90 | 16.62 | 22.13 |'
- en: '| MADF | 0.52 | 1.55 | 3.28 | 5.43 | 8.35 | 13.54 | 2.15 | 5.58 | 9.20 | 13.08
    | 17.36 | 24.42 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| MADF | 0.52 | 1.55 | 3.28 | 5.43 | 8.35 | 13.54 | 2.15 | 5.58 | 9.20 | 13.08
    | 17.36 | 24.42 |'
- en: '| DSI | 0.59 | 1.58 | 3.01 | 4.50 | 6.51 | 9.76 | 2.51 | 6.52 | 11.35 | 15.99
    | 21.75 | 29.38 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| DSI | 0.59 | 1.58 | 3.01 | 4.50 | 6.51 | 9.76 | 2.51 | 6.52 | 11.35 | 15.99
    | 21.75 | 29.38 |'
- en: '| CR-Fill | 1.06 | 2.86 | 5.26 | 7.79 | 11.23 | 19.52 | 2.37 | 6.24 | 10.54
    | 15.17 | 20.36 | 26.43 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| CR-Fill | 1.06 | 2.86 | 5.26 | 7.79 | 11.23 | 19.52 | 2.37 | 6.24 | 10.54
    | 15.17 | 20.36 | 26.43 |'
- en: '| CoModGAN | 0.44 | 1.25 | 2.45 | 3.65 | 5.03 | 6.89 | 2.11 | 5.63 | 9.58 |
    13.65 | 17.68 | 22.58 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| CoModGAN | 0.44 | 1.25 | 2.45 | 3.65 | 5.03 | 6.89 | 2.11 | 5.63 | 9.58 |
    13.65 | 17.68 | 22.58 |'
- en: '| LGNet | 0.39 | 1.06 | 2.08 | 3.16 | 4.61 | 7.07 | 1.97 | 5.25 | 8.90 | 13.02
    | 17.60 | 25.99 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| LGNet | 0.39 | 1.06 | 2.08 | 3.16 | 4.61 | 7.07 | 1.97 | 5.25 | 8.90 | 13.02
    | 17.60 | 25.99 |'
- en: '| MAT | 0.41 | 1.13 | 2.05 | 2.96 | 4.05 | 5.43 | 2.13 | 5.47 | 9.26 | 13.00
    | 16.62 | 21.88 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| MAT | 0.41 | 1.13 | 2.05 | 2.96 | 4.05 | 5.43 | 2.13 | 5.47 | 9.26 | 13.00
    | 16.62 | 21.88 |'
- en: '| RePaint | - | - | 2.14 | - | 4.24 | - | - | - | 8.85 | - | 15.90 | 21.58
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| RePaint | - | - | 2.14 | - | 4.24 | - | - | - | 8.85 | - | 15.90 | 21.58
    |'
- en: '| LPIPS $\dagger$ | RFR | 0.015 | 0.028 | 0.042 | 0.060 | 0.081 | 0.118 | 0.021
    | 0.047 | 0.074 | 0.106 | 0.142 | 0.201 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| LPIPS $\dagger$ | RFR | 0.015 | 0.028 | 0.042 | 0.060 | 0.081 | 0.118 | 0.021
    | 0.047 | 0.074 | 0.106 | 0.142 | 0.201 |'
- en: '| MADF | 0.009 | 0.025 | 0.048 | 0.077 | 0.109 | 0.168 | 0.014 | 0.038 | 0.068
    | 0.102 | 0.141 | 0.209 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| MADF | 0.009 | 0.025 | 0.048 | 0.077 | 0.109 | 0.168 | 0.014 | 0.038 | 0.068
    | 0.102 | 0.141 | 0.209 |'
- en: '| DSI | 0.010 | 0.026 | 0.048 | 0.074 | 0.104 | 0.160 | 0.018 | 0.047 | 0.085
    | 0.125 | 0.169 | 0.242 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| DSI | 0.010 | 0.026 | 0.048 | 0.074 | 0.104 | 0.160 | 0.018 | 0.047 | 0.085
    | 0.125 | 0.169 | 0.242 |'
- en: '| CR-Fill | 0.017 | 0.043 | 0.074 | 0.107 | 0.143 | 0.212 | 0.016 | 0.042 |
    0.076 | 0.114 | 0.156 | 0.226 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| CR-Fill | 0.017 | 0.043 | 0.074 | 0.107 | 0.143 | 0.212 | 0.016 | 0.042 |
    0.076 | 0.114 | 0.156 | 0.226 |'
- en: '| CoModGAN | 0.008 | 0.022 | 0.041 | 0.065 | 0.092 | 0.143 | 0.016 | 0.044
    | 0.080 | 0.121 | 0.164 | 0.236 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| CoModGAN | 0.008 | 0.022 | 0.041 | 0.065 | 0.092 | 0.143 | 0.016 | 0.044
    | 0.080 | 0.121 | 0.164 | 0.236 |'
- en: '| LGNet | 0.006 | 0.017 | 0.031 | 0.048 | 0.069 | 0.108 | 0.014 | 0.035 | 0.064
    | 0.096 | 0.132 | 0.198 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| LGNet | 0.006 | 0.017 | 0.031 | 0.048 | 0.069 | 0.108 | 0.014 | 0.035 | 0.064
    | 0.096 | 0.132 | 0.198 |'
- en: '| MAT | 0.007 | 0.019 | 0.035 | 0.054 | 0.077 | 0.120 | 0.014 | 0.040 | 0.073
    | 0.111 | 0.152 | 0.224 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| MAT | 0.007 | 0.019 | 0.035 | 0.054 | 0.077 | 0.120 | 0.014 | 0.040 | 0.073
    | 0.111 | 0.152 | 0.224 |'
- en: '| RePaint | - | - | 0.038 | - | 0.093 | - | - | - | 0.077 | - | 0.167 | 0.259
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| RePaint | - | - | 0.038 | - | 0.093 | - | - | - | 0.077 | - | 0.167 | 0.259
    |'
- en: 2.7 Evaluation Protocol
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7 评估协议
- en: 'The evaluation metrics can be classified into two categories: pixel-aware metrics
    and (human) perception-aware metrics. The former focus on the precision of reconstructed
    pixels, including $\ell_{1}$ error, $\ell_{2}$ error, and PSNR (peak signal-to-noise
    ratio), SSIM (the structural similarity index) (Wang et al., [2004](#bib.bib207)),
    and MS-SSIM (multi-scale SSIM) (Wang et al., [2003](#bib.bib206)). The latter
    pay more attention to the visual perception quality, including FID (Fréchet inception
    distance) (Heusel et al., [2017](#bib.bib65)), LPIPS (learned perceptual image
    patch similarity) (Zhang et al., [2018b](#bib.bib256)), P/U-IDS (paired/unpaired
    inception discriminative score) (Zhao et al., [2021](#bib.bib263)), and user study
    results. The detailed descriptions are given in the following.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标可以分为两类：像素感知指标和（人类）感知指标。前者关注于重建像素的精度，包括 $\ell_{1}$ 误差、$\ell_{2}$ 误差、PSNR（峰值信噪比）、SSIM（结构相似性指数）（Wang
    等人，[2004](#bib.bib207)），以及 MS-SSIM（多尺度 SSIM）（Wang 等人，[2003](#bib.bib206)）。后者则更加关注视觉感知质量，包括
    FID（Fréchet 开始距离）（Heusel 等人，[2017](#bib.bib65)），LPIPS（学习感知图像块相似性）（Zhang 等人，[2018b](#bib.bib256)），P/U-IDS（配对/非配对开始判别得分）（Zhao
    等人，[2021](#bib.bib263)），以及用户研究结果。详细描述见下文。
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\ell_{1}$ error: The mean absolute differences between the complete image
    ($\mathbf{I}_{c}$) and the ground-truth image ($\mathbf{I}_{g}$).'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\ell_{1}$ 误差：完整图像 ($\mathbf{I}_{c}$) 和真实图像 ($\mathbf{I}_{g}$) 之间的平均绝对差异。
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\ell_{2}$ error: The mean squared differences between the complete image and
    the ground-truth image.'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\ell_{2}$ 误差：完整图像与真实图像之间的均方差异。
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'PSNR: It is mainly used to measure the quality of reconstruction of the complete
    image. Its formulation is $\mathrm{PSNR}=20\cdot log_{10}(255)-10\cdot log_{10}(\mathrm{MSE})$,
    where $\mathrm{MSE}$ is the mean squared error between the complete image and
    the ground-truth image.'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PSNR：它主要用于衡量完整图像的重建质量。其公式为 $\mathrm{PSNR}=20\cdot log_{10}(255)-10\cdot log_{10}(\mathrm{MSE})$，其中
    $\mathrm{MSE}$ 是完整图像与真实图像之间的均方误差。
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SSIM: Instead of estimating absolute errors, SSIM measures the similarity in
    structural information by incorporating luminance masking and contrast masking.
    It is written as $\mathrm{SSIM}=\frac{(2\mu_{\mathbf{I}_{c}}\mu_{I_{g}}+c_{1})(2\sigma_{\mathbf{I}_{c}\mathbf{I}_{g}}+c_{2})}{(\mu_{\mathbf{I}_{c}}^{2}+\mu_{\mathbf{I}_{g}}^{2}+c_{1})(\sigma_{\mathbf{I}_{c}}^{2}+\sigma_{\mathbf{I}_{g}}^{2}+c_{2})},$
    where $\mu$ and $\sigma$ refer to the average and the variance, respectively;
    and $c_{1}=0.01^{2}$ and $c_{2}=0.03^{2}$ are two variables to stabilize the division.'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SSIM：SSIM 不估计绝对误差，而是通过结合亮度掩蔽和对比度掩蔽来测量结构信息的相似性。它表示为 $\mathrm{SSIM}=\frac{(2\mu_{\mathbf{I}_{c}}\mu_{I_{g}}+c_{1})(2\sigma_{\mathbf{I}_{c}\mathbf{I}_{g}}+c_{2})}{(\mu_{\mathbf{I}_{c}}^{2}+\mu_{\mathbf{I}_{g}}^{2}+c_{1})(\sigma_{\mathbf{I}_{c}}^{2}+\sigma_{\mathbf{I}_{g}}^{2}+c_{2})}$，其中
    $\mu$ 和 $\sigma$ 分别表示平均值和方差；$c_{1}=0.01^{2}$ 和 $c_{2}=0.03^{2}$ 是两个用于稳定分母的变量。
- en: •
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MS-SSIM: Dosselmann and Yang ([2011](#bib.bib40)) illustrated that SSIM is
    very close to the windowed mean squared error and Wang et al. ([2003](#bib.bib206))
    highlighted the single-scale nature of SSIM as a drawback. As an alternative,
    MS-SSIM embraces more flexibility for image quality assessment. To compute the
    MS-SSIM, two input images are iteratively processed with low-pass filters and
    downsampled with a stride of 2 (in total, five scales). Then, the contrast comparison
    and structure comparison are computed at each scale and the luminance comparison
    is calculated at the last scale. These measurements are combined with appropriate
    weights (Wang et al., [2003](#bib.bib206)).'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MS-SSIM：Dosselmann 和 Yang ([2011](#bib.bib40)) 说明了 SSIM 非常接近窗口化的均方误差，而 Wang
    等人 ([2003](#bib.bib206)) 强调了 SSIM 单尺度特性的缺陷。作为替代方案，MS-SSIM 在图像质量评估中具有更大的灵活性。为了计算
    MS-SSIM，两幅输入图像通过低通滤波器迭代处理，并以步幅为 2 的方式下采样（总共五个尺度）。然后，在每个尺度下计算对比度比较和结构比较，最后一个尺度下计算亮度比较。这些测量结果与适当的权重结合（Wang
    等人，[2003](#bib.bib206)）。
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FID: The Fréchet inception distance compares two sets of images. It computes
    a Gaussian with mean and covariance $(\mathbf{m},\mathbf{C})$ and a Gaussian $(\mathbf{m}_{g},\mathbf{C}_{g})$
    from deep features of the set of completed images and the set of ground-truth
    images. Specifically, FID is defined as $\mathrm{FID}=\lVert\mathbf{m}-\mathbf{m}_{g}\rVert_{2}^{2}+\mathrm{Tr}(\mathbf{C}+\mathbf{C}_{g}-2(\mathbf{C}\mathbf{C}_{g})^{\frac{1}{2}})$.'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'FID: Fréchet inception distance 比较两组图像。它计算一个均值和协方差为 $(\mathbf{m},\mathbf{C})$
    的高斯分布以及一个均值为 $(\mathbf{m}_{g},\mathbf{C}_{g})$ 的高斯分布，分别来自完成图像集和真实图像集。具体而言，FID
    定义为 $\mathrm{FID}=\lVert\mathbf{m}-\mathbf{m}_{g}\rVert_{2}^{2}+\mathrm{Tr}(\mathbf{C}+\mathbf{C}_{g}-2(\mathbf{C}\mathbf{C}_{g})^{\frac{1}{2}})$。'
- en: •
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LPIPS: The distance of multi-layer deep features of complete and ground-truth
    images. Let $\mathbf{F}_{c},\mathbf{F}_{g}\in\mathbb{R}^{H_{l}\times W_{l}\times
    C_{l}}$ denote the channel-wise normalized features in the $l$-th layer, the LPIPS
    is given by $\mathrm{LPIPS}=\sum_{l}\frac{1}{H_{l}W_{l}}\sum_{h,w}\lVert\mathbf{W}_{l}\odot({\mathbf{F}_{c}^{l}}_{hw}-{\mathbf{F}_{g}^{l}}_{hw})\rVert_{2}^{2}$,
    where $\mathbf{W}_{l}\in\mathbb{R}^{C_{l}}$ is the channel weight vector.'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'LPIPS: 完整图像和真实图像的多层深度特征的距离。设 $\mathbf{F}_{c},\mathbf{F}_{g}\in\mathbb{R}^{H_{l}\times
    W_{l}\times C_{l}}$ 为第 $l$ 层的通道归一化特征，LPIPS 由 $\mathrm{LPIPS}=\sum_{l}\frac{1}{H_{l}W_{l}}\sum_{h,w}\lVert\mathbf{W}_{l}\odot({\mathbf{F}_{c}^{l}}_{hw}-{\mathbf{F}_{g}^{l}}_{hw})\rVert_{2}^{2}$
    给出，其中 $\mathbf{W}_{l}\in\mathbb{R}^{C_{l}}$ 为通道权重向量。'
- en: •
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'P/U-IDS: The linear separability of complete and ground-truth images in a pre-trained
    feature space. Let $\phi(\cdot)$ denote the Inception v3 model mapping the image
    to the 2048D feature space, $f(\cdot)$ be the decision function of the SVM, the
    P-IDS is formulated as $\mathrm{P\text{-}IDS}=\mathrm{Pr}\{f(\phi(\mathbf{I}_{c}))>f(\phi(\mathbf{I}_{g})\}$.
    Due to the unpaired nature, U-IDS is obtained by directly calculating the misclassification
    rate.'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'P/U-IDS: 预训练特征空间中完整图像和真实图像的线性可分性。设 $\phi(\cdot)$ 为将图像映射到 2048D 特征空间的 Inception
    v3 模型，$f(\cdot)$ 为 SVM 的决策函数，P-IDS 表述为 $\mathrm{P\text{-}IDS}=\mathrm{Pr}\{f(\phi(\mathbf{I}_{c}))>f(\phi(\mathbf{I}_{g})\}$.
    由于未配对的性质，U-IDS 通过直接计算误分类率获得。'
- en: •
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'User Study: FID, LPIPS, and P/U-IDS cannot be able to comprehensively evaluate
    the visual quality of complete images, therefore, a user study is often conducted
    to complement the above metrics. User studies typically let a human chooses a
    preferred image among two (or multiple) images generated from two (or multiple)
    competitors. Based on the collected votes, the preference ratio is calculated
    for comparison.'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '用户研究: FID、LPIPS 和 P/U-IDS 无法全面评估完整图像的视觉质量，因此通常会进行用户研究以补充上述指标。用户研究通常让人类在由两个（或多个）竞争者生成的两个（或多个）图像中选择一个偏好的图像。基于收集的投票，计算偏好比例进行比较。'
- en: '![Refer to caption](img/a3576d8dbaf2459d82f30c5461141f2d.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a3576d8dbaf2459d82f30c5461141f2d.png)'
- en: 'Figure 8: Qualitative comparison of representative image inpainting methods
    on CelebA-HQ (the first three rows) and Places2 (the last four rows).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 在 CelebA-HQ（前三行）和 Places2（最后四行）上的代表性图像修复方法的定性比较。'
- en: 'Table 3: Model computational complexity statistics.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 模型计算复杂度统计。'
- en: '| Model | #Parameter | GPU Memory | Infer. time |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Model | #Parameter | GPU Memory | Infer. time |'
- en: '| RFR | 30.59 M | 1.23 G | 28.95 ms |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| RFR | 30.59 M | 1.23 G | 28.95 ms |'
- en: '| MADF | 85.14 M | 2.42 G | 15.59 ms |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| MADF | 85.14 M | 2.42 G | 15.59 ms |'
- en: '| DSI | 70.32 M | 6.54 G | 40.20 s |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| DSI | 70.32 M | 6.54 G | 40.20 s |'
- en: '| CR-Fill | 4.10 M | 0.96 G | 9.18 ms |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| CR-Fill | 4.10 M | 0.96 G | 9.18 ms |'
- en: '| CoModGAN | 79.80 M | 1.71 G | 42.24 ms |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| CoModGAN | 79.80 M | 1.71 G | 42.24 ms |'
- en: '| LGNet | 115.00 M | 1.52 G | 13.59 ms |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| LGNet | 115.00 M | 1.52 G | 13.59 ms |'
- en: '| MAT | 59.78 M | 1.69 G | 78.35 ms |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| MAT | 59.78 M | 1.69 G | 78.35 ms |'
- en: '| RePaint | 552.81 M | 4.14 G | 6 min 30 s |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| RePaint | 552.81 M | 4.14 G | 6 min 30 s |'
- en: 'Table 4: Quantitative comparison of different loss functions on CelebA-HQ (“C”)
    and Paris StreetView (“P”). $\ddagger$ Higher is better. $\dagger$ Lower is better.
    “16” refers to Eq. ([3](#S2.E3 "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep
    Learning-based Image and Video Inpainting: A Survey")) with $\alpha=6$, and the
    remaining loss settings both include “16” (We omit it for simplicity). “percept”
    refers to Eq. ([6](#S2.E6 "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey")) based on pretrained VGG16; “resnetpl”
    refers to Eq. ([6](#S2.E6 "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey")) based on the pre-trained segmentation
    network ResNet50, which is proposed by (Suvorov et al., [2022](#bib.bib183)).;
    “style” refers to Eq. ([7](#S2.E7 "In 2.5 Loss Functions ‣ 2 Image Inpainting
    ‣ Deep Learning-based Image and Video Inpainting: A Survey")); “stylemeanstd”
    refers to Eq. ([8](#S2.E8 "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey")); “percept_style” refers to Eq. ([6](#S2.E6
    "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video
    Inpainting: A Survey")) plus Eq. ([7](#S2.E7 "In 2.5 Loss Functions ‣ 2 Image
    Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey")); “lsgan”
    refers to Eq. ([9](#S2.E9 "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey")) and ([10](#S2.E10 "In 2.5 Loss Functions
    ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey")).
    Different percentage numbers in the first row refer to the hole ratios, where
    a large number implies large missing regions. Following the common setting, the
    test mask is from (Liu et al., [2018](#bib.bib122)).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4：不同损失函数在 CelebA-HQ (“C”) 和 Paris StreetView (“P”) 上的定量比较。$\ddagger$ 数值越高越好。$\dagger$
    数值越低越好。“16”指的是公式 ([3](#S2.E3 "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep
    Learning-based Image and Video Inpainting: A Survey"))，其中 $\alpha=6$，其余损失设置都包含“16”（为了简洁我们省略了它）。“percept”指的是基于预训练的
    VGG16 的公式 ([6](#S2.E6 "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey"))；“resnetpl”指的是基于预训练的分割网络 ResNet50 的公式 ([6](#S2.E6
    "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video
    Inpainting: A Survey"))，这是由 (Suvorov et al., [2022](#bib.bib183)) 提出的；“style”指的是公式 ([7](#S2.E7
    "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video
    Inpainting: A Survey"))；“stylemeanstd”指的是公式 ([8](#S2.E8 "In 2.5 Loss Functions
    ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey"))；“percept_style”指的是公式 ([6](#S2.E6
    "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video
    Inpainting: A Survey")) 加上公式 ([7](#S2.E7 "In 2.5 Loss Functions ‣ 2 Image Inpainting
    ‣ Deep Learning-based Image and Video Inpainting: A Survey"))；“lsgan”指的是公式 ([9](#S2.E9
    "In 2.5 Loss Functions ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video
    Inpainting: A Survey")) 和 ([10](#S2.E10 "In 2.5 Loss Functions ‣ 2 Image Inpainting
    ‣ Deep Learning-based Image and Video Inpainting: A Survey"))。第一行中的不同百分比数字表示孔洞比例，大数值意味着较大的缺失区域。根据常见设置，测试掩码来自
    (Liu et al., [2018](#bib.bib122))。'
- en: '|  | Mask | 1%-10% | 10%-20% | 20%-30% | 30%-40% | 40%-50% | 50%-60% |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | Mask | 1%-10% | 10%-20% | 20%-30% | 30%-40% | 40%-50% | 50%-60% |'
- en: '|  | Dataset | C | P | C | P | C | P | C | P | C | P | C | P |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | Dataset | C | P | C | P | C | P | C | P | C | P |'
- en: '| $\ell_{1}(\%)$ $\dagger$ | 16 | 0.46 | 0.57 | 1.26 | 1.53 | 2.34 | 2.81 |
    3.63 | 4.25 | 5.14 | 5.93 | 8.37 | 9.07 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| $\ell_{1}(\%)$ $\dagger$ | 16 | 0.46 | 0.57 | 1.26 | 1.53 | 2.34 | 2.81 |
    3.63 | 4.25 | 5.14 | 5.93 | 8.37 | 9.07 |'
- en: '| percept | 0.45 | 0.57 | 1.24 | 1.53 | 2.30 | 2.81 | 3.58 | 4.26 | 5.08 |
    5.95 | 8.33 | 9.11 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| percept | 0.45 | 0.57 | 1.24 | 1.53 | 2.30 | 2.81 | 3.58 | 4.26 | 5.08 |
    5.95 | 8.33 | 9.11 |'
- en: '| resnetpl | 0.45 | 0.59 | 1.25 | 1.58 | 2.32 | 2.88 | 3.60 | 4.35 | 5.10 |
    6.06 | 8.35 | 9.21 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| resnetpl | 0.45 | 0.59 | 1.25 | 1.58 | 2.32 | 2.88 | 3.60 | 4.35 | 5.10 |
    6.06 | 8.35 | 9.21 |'
- en: '| style | 0.48 | 0.59 | 1.33 | 1.60 | 2.47 | 2.97 | 3.85 | 4.53 | 5.45 | 6.37
    | 8.86 | 9.78 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| style | 0.48 | 0.59 | 1.33 | 1.60 | 2.47 | 2.97 | 3.85 | 4.53 | 5.45 | 6.37
    | 8.86 | 9.78 |'
- en: '| stylemeanstd | 0.46 | 0.59 | 1.27 | 1.60 | 2.39 | 2.96 | 3.75 | 4.51 | 5.35
    | 6.34 | 8.80 | 9.75 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| stylemeanstd | 0.46 | 0.59 | 1.27 | 1.60 | 2.39 | 2.96 | 3.75 | 4.51 | 5.35
    | 6.34 | 8.80 | 9.75 |'
- en: '| percept_style | 0.47 | 0.60 | 1.30 | 1.63 | 2.43 | 3.00 | 3.79 | 4.58 | 5.40
    | 6.42 | 8.83 | 9.84 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| percept_style | 0.47 | 0.60 | 1.30 | 1.63 | 2.43 | 3.00 | 3.79 | 4.58 | 5.40
    | 6.42 | 8.83 | 9.84 |'
- en: '| lsgan | 0.47 | 0.59 | 1.30 | 1.61 | 2.42 | 2.97 | 3.76 | 4.52 | 5.33 | 6.34
    | 8.67 | 9.72 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| lsgan | 0.47 | 0.59 | 1.30 | 1.61 | 2.42 | 2.97 | 3.76 | 4.52 | 5.33 | 6.34
    | 8.67 | 9.72 |'
- en: '| PSNR $\ddagger$ | 16 | 40.03 | 38.74 | 34.13 | 33.17 | 30.76 | 29.92 | 28.27
    | 27.67 | 26.29 | 25.88 | 23.23 | 23.28 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| PSNR $\ddagger$ | 16 | 40.03 | 38.74 | 34.13 | 33.17 | 30.76 | 29.92 | 28.27
    | 27.67 | 26.29 | 25.88 | 23.23 | 23.28 |'
- en: '| percept | 40.14 | 38.77 | 34.19 | 33.17 | 30.81 | 29.91 | 28.31 | 27.65 |
    26.33 | 25.85 | 23.23 | 23.25 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| percept | 40.14 | 38.77 | 34.19 | 33.17 | 30.81 | 29.91 | 28.31 | 27.65 |
    26.33 | 25.85 | 23.23 | 23.25 |'
- en: '| resnetpl | 40.12 | 38.56 | 34.18 | 33.03 | 30.80 | 29.83 | 28.29 | 27.61
    | 26.31 | 25.83 | 23.23 | 23.26 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| resnetpl | 40.12 | 38.56 | 34.18 | 33.03 | 30.80 | 29.83 | 28.29 | 27.61
    | 26.31 | 25.83 | 23.23 | 23.26 |'
- en: '| style | 39.63 | 38.36 | 33.65 | 32.71 | 30.24 | 29.37 | 27.72 | 27.07 | 25.74
    | 25.22 | 22.71 | 22.62 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| style | 39.63 | 38.36 | 33.65 | 32.71 | 30.24 | 29.37 | 27.72 | 27.07 | 25.74
    | 25.22 | 22.71 | 22.62 |'
- en: '| stylemeanstd | 39.91 | 38.38 | 33.89 | 32.81 | 30.42 | 29.49 | 27.85 | 27.20
    | 25.83 | 25.35 | 22.72 | 22.70 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| stylemeanstd | 39.91 | 38.38 | 33.89 | 32.81 | 30.42 | 29.49 | 27.85 | 27.20
    | 25.83 | 25.35 | 22.72 | 22.70 |'
- en: '| percept_style | 39.78 | 38.20 | 33.74 | 32.60 | 30.31 | 29.30 | 27.76 | 27.01
    | 25.76 | 25.20 | 22.68 | 22.58 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| percept_style | 39.78 | 38.20 | 33.74 | 32.60 | 30.31 | 29.30 | 27.76 | 27.01
    | 25.76 | 25.20 | 22.68 | 22.58 |'
- en: '| lsgan | 39.71 | 38.40 | 33.73 | 32.78 | 30.39 | 29.50 | 27.91 | 27.18 | 25.96
    | 25.35 | 22.95 | 22.72 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| lsgan | 39.71 | 38.40 | 33.73 | 32.78 | 30.39 | 29.50 | 27.91 | 27.18 | 25.96
    | 25.35 | 22.95 | 22.72 |'
- en: '| SSIM $\ddagger$ | 16 | 0.995 | 0.991 | 0.985 | 0.973 | 0.969 | 0.946 | 0.948
    | 0.911 | 0.921 | 0.867 | 0.847 | 0.767 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| SSIM $\ddagger$ | 16 | 0.995 | 0.991 | 0.985 | 0.973 | 0.969 | 0.946 | 0.948
    | 0.911 | 0.921 | 0.867 | 0.847 | 0.767 |'
- en: '| percept | 0.995 | 0.991 | 0.985 | 0.973 | 0.970 | 0.946 | 0.949 | 0.911 |
    0.921 | 0.866 | 0.847 | 0.765 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| percept | 0.995 | 0.991 | 0.985 | 0.973 | 0.970 | 0.946 | 0.949 | 0.911 |
    0.921 | 0.866 | 0.847 | 0.765 |'
- en: '| resnetpl | 0.995 | 0.991 | 0.985 | 0.972 | 0.970 | 0.945 | 0.948 | 0.909
    | 0.921 | 0.865 | 0.848 | 0.764 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| resnetpl | 0.995 | 0.991 | 0.985 | 0.972 | 0.970 | 0.945 | 0.948 | 0.909
    | 0.921 | 0.865 | 0.848 | 0.764 |'
- en: '| style | 0.995 | 0.991 | 0.983 | 0.971 | 0.966 | 0.940 | 0.943 | 0.902 | 0.913
    | 0.853 | 0.834 | 0.746 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| style | 0.995 | 0.991 | 0.983 | 0.971 | 0.966 | 0.940 | 0.943 | 0.902 | 0.913
    | 0.853 | 0.834 | 0.746 |'
- en: '| stylemeanstd | 0.995 | 0.991 | 0.984 | 0.971 | 0.968 | 0.941 | 0.944 | 0.903
    | 0.914 | 0.854 | 0.835 | 0.747 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| stylemeanstd | 0.995 | 0.991 | 0.984 | 0.971 | 0.968 | 0.941 | 0.944 | 0.903
    | 0.914 | 0.854 | 0.835 | 0.747 |'
- en: '| percept_style | 0.995 | 0.990 | 0.984 | 0.970 | 0.967 | 0.940 | 0.943 | 0.901
    | 0.913 | 0.852 | 0.834 | 0.745 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| percept_style | 0.995 | 0.990 | 0.984 | 0.970 | 0.967 | 0.940 | 0.943 | 0.901
    | 0.913 | 0.852 | 0.834 | 0.745 |'
- en: '| lsgan | 0.995 | 0.991 | 0.984 | 0.971 | 0.967 | 0.941 | 0.944 | 0.903 | 0.915
    | 0.854 | 0.839 | 0.746 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| lsgan | 0.995 | 0.991 | 0.984 | 0.971 | 0.967 | 0.941 | 0.944 | 0.903 | 0.915
    | 0.854 | 0.839 | 0.746 |'
- en: '| FID $\dagger$ | 16 | 0.56 | 4.74 | 1.57 | 13.74 | 3.31 | 26.55 | 5.38 | 40.79
    | 8.37 | 57.49 | 15.18 | 86.51 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| FID $\dagger$ | 16 | 0.56 | 4.74 | 1.57 | 13.74 | 3.31 | 26.55 | 5.38 | 40.79
    | 8.37 | 57.49 | 15.18 | 86.51 |'
- en: '| percept | 0.53 | 4.64 | 1.51 | 13.41 | 3.20 | 26.13 | 5.22 | 40.35 | 8.18
    | 57.22 | 14.63 | 88.10 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| percept | 0.53 | 4.64 | 1.51 | 13.41 | 3.20 | 26.13 | 5.22 | 40.35 | 8.18
    | 57.22 | 14.63 | 88.10 |'
- en: '| resnetpl | 0.52 | 4.62 | 1.47 | 13.23 | 3.11 | 25.60 | 5.13 | 39.08 | 7.99
    | 54.75 | 13.81 | 83.93 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| resnetpl | 0.52 | 4.62 | 1.47 | 13.23 | 3.11 | 25.60 | 5.13 | 39.08 | 7.99
    | 54.75 | 13.81 | 83.93 |'
- en: '| style | 0.42 | 3.91 | 1.13 | 10.67 | 2.25 | 19.65 | 3.38 | 28.87 | 5.00 |
    39.09 | 7.90 | 57.00 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| style | 0.42 | 3.91 | 1.13 | 10.67 | 2.25 | 19.65 | 3.38 | 28.87 | 5.00 |
    39.09 | 7.90 | 57.00 |'
- en: '| stylemeanstd | 0.44 | 4.21 | 1.21 | 11.42 | 2.38 | 20.54 | 3.65 | 29.68 |
    5.36 | 39.59 | 8.55 | 56.38 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| stylemeanstd | 0.44 | 4.21 | 1.21 | 11.42 | 2.38 | 20.54 | 3.65 | 29.68 |
    5.36 | 39.59 | 8.55 | 56.38 |'
- en: '| percept_style | 0.40 | 3.98 | 1.13 | 10.91 | 2.26 | 19.76 | 3.42 | 29.12
    | 5.07 | 39.28 | 7.87 | 57.07 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| percept_style | 0.40 | 3.98 | 1.13 | 10.91 | 2.26 | 19.76 | 3.42 | 29.12
    | 5.07 | 39.28 | 7.87 | 57.07 |'
- en: '| lsgan | 0.54 | 4.26 | 1.57 | 11.72 | 3.34 | 21.54 | 5.57 | 31.21 | 8.85 |
    41.80 | 16.03 | 60.01 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| lsgan | 0.54 | 4.26 | 1.57 | 11.72 | 3.34 | 21.54 | 5.57 | 31.21 | 8.85 |
    41.80 | 16.03 | 60.01 |'
- en: '| LPIPS $\dagger$ | 16 | 0.011 | 0.016 | 0.032 | 0.048 | 0.063 | 0.091 | 0.102
    | 0.142 | 0.144 | 0.197 | 0.222 | 0.298 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| LPIPS $\dagger$ | 16 | 0.011 | 0.016 | 0.032 | 0.048 | 0.063 | 0.091 | 0.102
    | 0.142 | 0.144 | 0.197 | 0.222 | 0.298 |'
- en: '| percept | 0.010 | 0.015 | 0.029 | 0.045 | 0.057 | 0.086 | 0.092 | 0.134 |
    0.129 | 0.185 | 0.200 | 0.279 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| percept | 0.010 | 0.015 | 0.029 | 0.045 | 0.057 | 0.086 | 0.092 | 0.134 |
    0.129 | 0.185 | 0.200 | 0.279 |'
- en: '| resnetpl | 0.009 | 0.015 | 0.027 | 0.044 | 0.052 | 0.083 | 0.083 | 0.126
    | 0.116 | 0.174 | 0.174 | 0.259 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| resnetpl | 0.009 | 0.015 | 0.027 | 0.044 | 0.052 | 0.083 | 0.083 | 0.126
    | 0.116 | 0.174 | 0.174 | 0.259 |'
- en: '| style | 0.007 | 0.011 | 0.018 | 0.031 | 0.033 | 0.056 | 0.052 | 0.086 | 0.073
    | 0.120 | 0.117 | 0.186 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| style | 0.007 | 0.011 | 0.018 | 0.031 | 0.033 | 0.056 | 0.052 | 0.086 | 0.073
    | 0.120 | 0.117 | 0.186 |'
- en: '| stylemeanstd | 0.008 | 0.013 | 0.021 | 0.035 | 0.037 | 0.062 | 0.055 | 0.092
    | 0.076 | 0.125 | 0.119 | 0.189 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| stylemeanstd | 0.008 | 0.013 | 0.021 | 0.035 | 0.037 | 0.062 | 0.055 | 0.092
    | 0.076 | 0.125 | 0.119 | 0.189 |'
- en: '| percept_style | 0.006 | 0.011 | 0.018 | 0.032 | 0.033 | 0.057 | 0.052 | 0.087
    | 0.074 | 0.122 | 0.118 | 0.188 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| percept_style | 0.006 | 0.011 | 0.018 | 0.032 | 0.033 | 0.057 | 0.052 | 0.087
    | 0.074 | 0.122 | 0.118 | 0.188 |'
- en: '| lsgan | 0.009 | 0.013 | 0.028 | 0.036 | 0.054 | 0.066 | 0.086 | 0.098 | 0.123
    | 0.135 | 0.196 | 0.208 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| lsgan | 0.009 | 0.013 | 0.028 | 0.036 | 0.054 | 0.066 | 0.086 | 0.098 | 0.123
    | 0.135 | 0.196 | 0.208 |'
- en: '![Refer to caption](img/ee515c99bc31bb635f26040ea054f64b.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ee515c99bc31bb635f26040ea054f64b.png)'
- en: 'Figure 9: Qualitative comparison of different loss functions on CelebA-HQ (the
    first two rows) and Paris StreetView (the last two rows). “StyleMS” refers to
    “stylemeanstd”; “Per_Style” refers to “Percept_style”.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：不同损失函数在CelebA-HQ（前两行）和巴黎街景（最后两行）的定性比较。“StyleMS”指的是“stylemeanstd”；“Per_Style”指的是“Percept_style”。
- en: 2.8 Performance Evaluation
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8 性能评估
- en: 2.8.1 Representative Image Inpainting Methods
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.8.1 代表性图像修复方法
- en: 'We qualitatively and quantitatively compare some representative image inpainting
    methods: RFR (Li et al., [2020c](#bib.bib108)), MADF (Zhu et al., [2021](#bib.bib273)),
    DSI (Peng et al., [2021](#bib.bib149)), CR-Fill (Zeng et al., [2021b](#bib.bib244)),
    CoModGAN (Zhao et al., [2021](#bib.bib263)), LGNet (Quan et al., [2022](#bib.bib154)),
    MAT (Li et al., [2022b](#bib.bib109)), RePaint (Lugmayr et al., [2022](#bib.bib133)).
    The test mask is from (Liu et al., [2018](#bib.bib122)). Specifically, RFR follows
    a progressive inpainting strategy, MADF adopts a mask-aware design, DSI generates
    stochastic structures with hierarchical vq-vae, CR-Fill designs an attention-free
    generator, CoModGAN embeds the known content of corrupted images into style vectors
    of styleGAN2, LGNet introduces local and global refinement networks with different
    receptive fields, MAT designs a mask-aware transformer architecture, and RePaint
    utilizes a pre-trained unconditional diffusion model.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对一些代表性的图像修复方法进行了定性和定量比较：RFR（Li et al., [2020c](#bib.bib108)）、MADF（Zhu et al.,
    [2021](#bib.bib273)）、DSI（Peng et al., [2021](#bib.bib149)）、CR-Fill（Zeng et al.,
    [2021b](#bib.bib244)）、CoModGAN（Zhao et al., [2021](#bib.bib263)）、LGNet（Quan et
    al., [2022](#bib.bib154)）、MAT（Li et al., [2022b](#bib.bib109)）、RePaint（Lugmayr
    et al., [2022](#bib.bib133)）。测试掩膜来源于（Liu et al., [2018](#bib.bib122)）。具体来说，RFR采用渐进式修复策略，MADF采用了掩膜感知设计，DSI通过层次化的vq-vae生成随机结构，CR-Fill设计了无注意力生成器，CoModGAN将损坏图像的已知内容嵌入到styleGAN2的风格向量中，LGNet引入了具有不同感受野的局部和全局细化网络，MAT设计了掩膜感知的变换器架构，RePaint利用了预训练的无条件扩散模型。
- en: 'Table [2](#S2.T2 "Table 2 ‣ 2.6 Datasets ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey") reports the quantitative results of these
    advanced image inpainting methods on CelebA-HQ and Places2 datasets. In this experiment,
    we use the irregular masks shared by (Liu et al., [2018](#bib.bib122)) for the
    evaluation. From this table, we can find that MS-SSIM is very close to SSIM in
    the CelebA-HQ dataset; MS-SSIM is consistently higher than SSIM in the Places2
    dataset and this phenomenon is more apparent for large masks. The reason may be
    that face images are relatively regular and uniform compared to the natural scene
    images in Places2, and thus the latter is more sensitive to structural similarity
    with different scales. Among these methods, MAT and RePaint have relatively superior
    FID, especially for large masks ($>30\%$), while CoModGAN and LGNet perform better
    in PSNR. For DSI, the inpainting performance on CelebA-HQ is slightly better than
    that on Places2, and the possible reason is that the structure of face images
    is easier to model than diverse natural scene images. CR-Fill has limited inpainting
    performance.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '表[2](#S2.T2 "Table 2 ‣ 2.6 Datasets ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey")报告了这些先进图像修复方法在CelebA-HQ和Places2数据集上的定量结果。在这个实验中，我们使用了由（Liu
    et al., [2018](#bib.bib122)）分享的不规则掩膜进行评估。从表中可以发现，在CelebA-HQ数据集中，MS-SSIM与SSIM非常接近；而在Places2数据集中，MS-SSIM始终高于SSIM，这一现象在大掩膜下更为明显。原因可能是面部图像相较于Places2中的自然场景图像更为规则和均匀，因此后者对不同尺度的结构相似性更为敏感。在这些方法中，MAT和RePaint的FID表现相对较好，特别是对于大掩膜（$>30\%$），而CoModGAN和LGNet在PSNR上表现更佳。对于DSI，CelebA-HQ上的修复性能略好于Places2，可能的原因是面部图像的结构比多样的自然场景图像更容易建模。CR-Fill的修复性能有限。'
- en: 'Fig. [8](#S2.F8 "Figure 8 ‣ 2.7 Evaluation Protocol ‣ 2 Image Inpainting ‣
    Deep Learning-based Image and Video Inpainting: A Survey") shows the visual results
    of some representative image inpainting methods on CelebA-HQ and Places2 datasets.
    MADF adopts a mask-aware design, which can predict reasonable structures (the
    second and third rows), but has limited ability for detail restoration. By contrast,
    MAT has better inpainting performance with mask-aware transformer blocks. Through
    introducing local and global refinement with different receptive fields, LGNet
    can perceive local details (the black stroke in the first row) and global structure
    (the second row). For large missing regions, RFR can recover the helicopter rotor
    blade (the fourth row) and waterfall (the fifth row) with progressive inpainting.
    With the help of the generative capability of the unconditional modulated model
    (StyleGAN2), CoModGAN demonstrates relatively good inpainting performance (the
    fourth and sixth rows). DSI can perceive the structure with hierarchical VQ-VAE
    (the third and fourth rows). Based on the powerful generation ability of the diffusion
    model, RePaint can correctly infer the missing background (the sixth row) and
    the human body (the seventh row). Interestingly, it may have an incorrect semantic
    prediction (a woman’s head in the waterfall of the fifth row). Due to the implicit
    attention mechanism and simple network, CR-Fill achieves comparatively inferior
    inpainted results, which is also consistent with the quantitative comparisons
    as shown in Table [2](#S2.T2 "Table 2 ‣ 2.6 Datasets ‣ 2 Image Inpainting ‣ Deep
    Learning-based Image and Video Inpainting: A Survey").'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8](#S2.F8 "图 8 ‣ 2.7 评估协议 ‣ 2 图像修复 ‣ 基于深度学习的图像和视频修复：综述") 展示了在 CelebA-HQ 和
    Places2 数据集上，一些代表性的图像修复方法的视觉效果。MADF 采用了一个面向掩膜的设计，它可以预测合理的结构（第二行和第三行），但在细节恢复方面能力有限。相比之下，MAT
    使用了面向掩膜的 transformer 块，表现出更好的修复效果。通过引入具有不同感受野的局部和全局细化，LGNet 能够感知局部细节（第一行的黑色笔划）和全局结构（第二行）。对于大面积缺失区域，RFR
    能够通过渐进修复恢复直升机旋翼（第四行）和瀑布（第五行）。借助无条件调制模型（StyleGAN2）的生成能力，CoModGAN 展现了相对较好的修复性能（第四行和第六行）。DSI
    可以通过层级 VQ-VAE 感知结构（第三行和第四行）。基于扩散模型的强大生成能力，RePaint 能够正确推断缺失的背景（第六行）和人体（第七行）。有趣的是，它可能会出现不正确的语义预测（第五行瀑布中的女人头部）。由于隐式注意机制和简单的网络结构，CR-Fill
    的修复结果相对较差，这也与表 [2](#S2.T2 "表 2 ‣ 2.6 数据集 ‣ 2 图像修复 ‣ 基于深度学习的图像和视频修复：综述") 中的定量比较一致。
- en: 'In addition, we evaluate the computational complexity of the representative
    inpainting methods in terms of the number of parameters, GPU memory of single
    image inference, and inference time on a GPU (the time of a forward pass through
    the networks. The statistical results are shown in Table [3](#S2.T3 "Table 3 ‣
    2.7 Evaluation Protocol ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video
    Inpainting: A Survey"). CR-Fill implicitly learns the patch-borrowing behavior
    without an attention layer, its model is the smallest and thus needs less GPU
    memory and running time. Because RePaint is based on a diffusion model, it has
    the largest number of parameters and a very long inference time. The GPU memory
    and inference time of DSI are also very high. LGNet follows a coarse-to-fine framework
    with local and global refinement, therefore, the number of parameters is high.
    The running time of MAT and CoModGAN is relatively high because the former conducts
    many attention computations and the latter has multiple style modulations with
    progressive growing. RFR and MADF are in the middle.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们评估了代表性修复方法的计算复杂性，包括参数数量、单张图像推理的 GPU 内存和 GPU 上的推理时间（网络的前向传递时间）。统计结果见表 [3](#S2.T3
    "表 3 ‣ 2.7 评估协议 ‣ 2 图像修复 ‣ 基于深度学习的图像和视频修复：综述")。CR-Fill 在没有注意层的情况下隐式学习了补丁借用行为，其模型最小，因此需要较少的
    GPU 内存和运行时间。由于 RePaint 基于扩散模型，它具有最多的参数和非常长的推理时间。DSI 的 GPU 内存和推理时间也很高。LGNet 采用了粗到细的框架进行局部和全局细化，因此参数数量较多。MAT
    和 CoModGAN 的运行时间相对较高，因为前者进行大量的注意力计算，后者则具有多个风格调制和渐进增长。RFR 和 MADF 的复杂性居中。
- en: 2.8.2 Loss Functions
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.8.2 损失函数
- en: 'As summarized in Sec. [2.5](#S2.SS5 "2.5 Loss Functions ‣ 2 Image Inpainting
    ‣ Deep Learning-based Image and Video Inpainting: A Survey"), many loss functions
    have been proposed for image inpainting. In this part, we evaluate the effect
    of each loss term. We train an inpainting network with different loss settings
    on the CelebA-HQ and Paris StreetView datasets. This network consists of two downsampling
    layers, 11 ResNet residual blocks with dilation, and two upsampling layers. The
    corresponding numerical results are reported in Table [4](#S2.T4 "Table 4 ‣ 2.7
    Evaluation Protocol ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video
    Inpainting: A Survey"). In the case of masks at 1%-10%, the SSIM values of different
    loss settings are (almost) the same for CelebA-HQ and Paris StreetView datasets.
    The reason is that different loss settings only have a slight impact on the inpainting
    of very small missing regions. We can see that pixel-wise reconstruction loss
    (“16”) provides the baseline performance. After adding the perceptual loss (“percept”),
    FID and LPIPS are improved. Compared with “percept”, “resnetpl” achieves slightly
    better results, especially for the large mask. The style loss can remarkably decrease
    the FID and LPIPS at the expense of PSNR and SSIM. In other words, there exists
    a trade-off between pixel-wise reconstruction loss and style loss, where the former
    focuses on low-level pixel recovery, and the latter emphasizes visual quality.
    A similar finding is reported and studied in (Blau and Michaeli, [2018](#bib.bib9)).
    In addition, combining the perceptual loss with style loss (“percept_style”) has
    a very slight effect on the results compared to only style loss (“style”). The
    style loss based on Gram matrix (“style”) and style loss based on mean and standard
    deviation (“stylemeanstd”) have comparable results. Comparing with adversarial
    loss (“lsgan”), style loss (“style”) obtain significantly lower FID and LPIPS.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[2.5](#S2.SS5 "2.5 损失函数 ‣ 2 图像修复 ‣ 基于深度学习的图像和视频修复：综述")节总结的那样，已经提出了许多用于图像修复的损失函数。在这一部分，我们评估了每个损失项的效果。我们在CelebA-HQ和Paris
    StreetView数据集上，使用不同的损失设置训练了一个修复网络。该网络由两个下采样层、11个带有扩张的ResNet残差块和两个上采样层组成。相应的数值结果报告在表[4](#S2.T4
    "表4 ‣ 2.7 评估协议 ‣ 2 图像修复 ‣ 基于深度学习的图像和视频修复：综述")中。对于1%-10%的掩码情况，不同损失设置的SSIM值在CelebA-HQ和Paris
    StreetView数据集上（几乎）相同。原因是不同损失设置对非常小的缺失区域的修复影响很小。我们可以看到，像素级重建损失（“16”）提供了基线性能。在加入感知损失（“percept”）后，FID和LPIPS得到了改善。与“percept”相比，“resnetpl”取得了稍好的结果，特别是在大掩码的情况下。风格损失可以显著降低FID和LPIPS，但会以PSNR和SSIM为代价。换句话说，像素级重建损失和风格损失之间存在权衡，前者关注低级像素恢复，后者则强调视觉质量。类似的发现已在(Blau和Michaeli,
    [2018](#bib.bib9))中报告和研究。此外，将感知损失与风格损失结合（“percept_style”）相比于仅使用风格损失（“style”）对结果的影响非常小。基于Gram矩阵的风格损失（“style”）和基于均值和标准差的风格损失（“stylemeanstd”）结果相当。与对抗性损失（“lsgan”）相比，风格损失（“style”）获得了显著更低的FID和LPIPS。
- en: 'Fig. [9](#S2.F9 "Figure 9 ‣ 2.7 Evaluation Protocol ‣ 2 Image Inpainting ‣
    Deep Learning-based Image and Video Inpainting: A Survey") illustrates the corresponding
    qualitative comparison. “16” fills the missing regions with smooth structures
    and textures. After introducing the content loss, this phenomenon is slightly
    improved, for example, the nose and mouth of the first row are better recovered
    in column “Percept”. Compared with “Percept”, the inpainted results of “Resnetpl”
    have slightly improved visual quality, which is attributed to the perceptual loss
    computation with higher receptive field (Suvorov et al., [2022](#bib.bib183)).
    We can find that the results of “Style” are significantly superior to the previous
    three columns, especially for the restoration of texture details. This is consistent
    with the numerical results in Table [4](#S2.T4 "Table 4 ‣ 2.7 Evaluation Protocol
    ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey").
    For three settings with style loss, i.e., “Style”, “StyleMS”, and “Per_Style”,
    “Style” and “Per_Style” are on par, “StyleMS” is slightly worse. The performance
    of “LSGan” is in between “Percept” and “Style”.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [9](#S2.F9 "Figure 9 ‣ 2.7 Evaluation Protocol ‣ 2 Image Inpainting ‣ Deep
    Learning-based Image and Video Inpainting: A Survey") 说明了相应的定性比较。“16”用平滑的结构和纹理填补了缺失的区域。引入内容损失后，这一现象略有改善，例如，第一排中的鼻子和嘴巴在“Percept”列中恢复得更好。与“Percept”相比，“Resnetpl”的修复结果视觉质量略有提升，这归因于感知损失计算具有更高的感受野（Suvorov
    et al., [2022](#bib.bib183)）。我们发现“Style”的结果显著优于前面三列，特别是在纹理细节的恢复方面。这与表 [4](#S2.T4
    "Table 4 ‣ 2.7 Evaluation Protocol ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey") 中的数值结果一致。在三种带有风格损失的设置中，即“Style”、“StyleMS”和“Per_Style”，“Style”和“Per_Style”表现相当，“StyleMS”稍逊。“LSGan”的性能介于“Percept”和“Style”之间。'
- en: 2.9 Inpainting-based Applications
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.9 基于修复的应用
- en: Image inpainting can be used in many real-world applications, such as object
    removal, text editing, old photo restoration, image compression, text-guided image
    editing, etc.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图像修复可以用于许多现实世界的应用，如物体去除、文本编辑、旧照片修复、图像压缩、文本引导的图像编辑等。
- en: '![Refer to caption](img/e0f389dbac22ced5a22b16d882165f00.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e0f389dbac22ced5a22b16d882165f00.png)'
- en: 'Figure 10: Two representative examples of object removal.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 两个物体去除的代表性示例。'
- en: 2.9.1 Object Removal
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.9.1 物体去除
- en: 'Almost all image editing tools include the function of object removal, which
    is directly accomplished with image inpainting. To illustrate the capability of
    several current inpainting methods on the object removal application, we apply
    the respective trained models to remove objects from selected real-world images
    with different scenes, and the corresponding results are shown in Fig. [10](#S2.F10
    "Figure 10 ‣ 2.9 Inpainting-based Applications ‣ 2 Image Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey"). The first row is generated by CNN-based
    method (Suvorov et al., [2022](#bib.bib183)) and the second one is inpainted by
    a transformer-based method (Zheng et al., [2022a](#bib.bib267)). These two methods
    can achieve visually realistic results, successfully removing the objects highlighted
    with shadow markers.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '几乎所有的图像编辑工具都包括物体去除的功能，这直接通过图像修复实现。为了说明当前几种图像修复方法在物体去除应用中的能力，我们将各自训练的模型应用于从选定的现实世界图像中去除物体，这些图像包含不同的场景，相关结果如图
    [10](#S2.F10 "Figure 10 ‣ 2.9 Inpainting-based Applications ‣ 2 Image Inpainting
    ‣ Deep Learning-based Image and Video Inpainting: A Survey") 所示。第一排由基于 CNN 的方法（Suvorov
    et al., [2022](#bib.bib183)）生成，第二排由基于 Transformer 的方法（Zheng et al., [2022a](#bib.bib267)）修复。这两种方法都能实现视觉上真实的结果，成功去除了用阴影标记突出显示的物体。'
- en: '![Refer to caption](img/1a3117dad26d14978142fd843544d21b.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1a3117dad26d14978142fd843544d21b.png)'
- en: 'Figure 11: Representative samples of text editing.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 文本编辑的代表性样本。'
- en: 2.9.2 Text Editing
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.9.2 文本编辑
- en: 'On social media sites, users often share their pictures and also want to hide
    their personal information for privacy. For real-time text translation applications
    in smartphones, the original content needs to be replaced with the translated
    version. These text editing-related tasks can be solved via inpainting techniques.
    Fig. [11](#S2.F11 "Figure 11 ‣ 2.9.1 Object Removal ‣ 2.9 Inpainting-based Applications
    ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey")(a)
    shows the results of text removal with the method proposed by (Quan et al., [2022](#bib.bib154)),
    and Fig. [11](#S2.F11 "Figure 11 ‣ 2.9.1 Object Removal ‣ 2.9 Inpainting-based
    Applications ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting:
    A Survey")(b) illustrates two samples of text replacement from  (Wu et al., [2019](#bib.bib212)).
    These results have a pleasing visual quality.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在社交媒体网站上，用户经常分享他们的照片，并且希望隐藏个人信息以保护隐私。对于智能手机上的实时文本翻译应用，原始内容需要被翻译版本替换。这些与文本编辑相关的任务可以通过修补技术来解决。图
    [11](#S2.F11 "图 11 ‣ 2.9.1 对象去除 ‣ 2.9 基于修补的应用 ‣ 2 图像修补 ‣ 基于深度学习的图像和视频修补：综述")(a)
    显示了使用 (Quan 等人，[2022](#bib.bib154)) 提出的文本去除方法的结果，图 [11](#S2.F11 "图 11 ‣ 2.9.1
    对象去除 ‣ 2.9 基于修补的应用 ‣ 2 图像修补 ‣ 基于深度学习的图像和视频修补：综述")(b) 展示了 (Wu 等人，[2019](#bib.bib212))
    提出的文本替换的两个样本。这些结果具有令人满意的视觉质量。
- en: 2.9.3 Old Photo Restoration
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.9.3 旧照片修复
- en: 'Photos are helpful to record important moments. Unfortunately, some photos
    are damaged over time, resulting in various missing regions. Image inpainting
    can be used to recover these incomplete photos automatically. It is difficult
    to collect paired training data for this task, therefore, we synthesize old photos
    using the Pascal VOC dataset Everingham et al. ([2015](#bib.bib44)) inspired by Wan
    et al. ([2020](#bib.bib190)). Specifically, we collect some paper and scratch
    texture images to simulate the realistic defects in the old photos. To blend the
    above texture images with the VOC images, we randomly choose a mode from three
    candidates (screen, lighten-only, and layer addition) with a random opacity. In
    addition, some operations, e.g., random flipping, random position, rescaling,
    cropping, etc, are also used for augmenting the diversity of texture images. To
    this end, the paired samples of the original VOC images and the corresponding
    blended results are used for training the inpainting network (Quan et al., [2022](#bib.bib154)).
    Fig. [12](#S2.F12 "Figure 12 ‣ 2.9.3 Old Photo Restoration ‣ 2.9 Inpainting-based
    Applications ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting:
    A Survey") shows several examples of old photo restoration, where the inpainting
    method restores the original appearance of old photos.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 照片有助于记录重要时刻。不幸的是，随着时间的推移，一些照片会受到损坏，导致各种缺失区域。图像修补技术可以用来自动恢复这些不完整的照片。由于难以收集配对的训练数据，我们使用
    Pascal VOC 数据集 Everingham 等人 ([2015](#bib.bib44)) 生成旧照片，灵感来自 Wan 等人 ([2020](#bib.bib190))。具体而言，我们收集了一些纸张和划痕纹理图像，以模拟旧照片中的真实缺陷。为了将这些纹理图像与
    VOC 图像混合，我们从三种候选模式（屏幕、仅混合和图层叠加）中随机选择一种，并应用随机不透明度。此外，还使用了随机翻转、随机位置、缩放、裁剪等操作来增加纹理图像的多样性。为此，原始
    VOC 图像及其对应的混合结果配对样本用于训练修补网络（Quan 等人，[2022](#bib.bib154)）。图 [12](#S2.F12 "图 12
    ‣ 2.9.3 旧照片修复 ‣ 2.9 基于修补的应用 ‣ 2 图像修补 ‣ 基于深度学习的图像和视频修补：综述") 显示了几个旧照片修复的例子，其中修补方法恢复了旧照片的原始外观。
- en: '![Refer to caption](img/062e90c187c042d0addfe235483e42aa.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/062e90c187c042d0addfe235483e42aa.png)'
- en: 'Figure 12: Several representative examples of old photo restoration.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：旧照片修复的几个代表性例子。
- en: 2.9.4 Image Compression
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.9.4 图像压缩
- en: 'Image compression is a fundamental image processing technique to reduce the
    cost of storage or transmission of digital images. This technique mainly consists
    of two stages: compression and reconstruction. The former reduces the data size
    to obtain a sparse image representation and the latter reconstructs the original
    image. Different from waveform-based methods, Carlsson ([1988](#bib.bib13)) proposed
    a sketch-based method to obtain a sparse representation and reconstructed images
    via an interpolation process. Galić et al. ([2008](#bib.bib48)) introduced partial
    differential equation (PDE)-based inpainting to image compression, where image
    coding and decoding both are based on edge-enhancing anisotropic diffusion. Recently,
    some researchers (Baluja et al., [2019](#bib.bib5); Dai et al., [2020](#bib.bib28);
    Schrader et al., [2023](#bib.bib169)) applied deep learning methods to generate
    the sampling mask and reconstruct the image with an inpainting network. Fig. [13](#S2.F13
    "Figure 13 ‣ 2.9.4 Image Compression ‣ 2.9 Inpainting-based Applications ‣ 2 Image
    Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey") shows
    several examples of image compression with inpainting, where the reconstructed
    images have good quality based on adaptive sparse sampling with inpainting.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '图像压缩是减少数字图像存储或传输成本的基本图像处理技术。该技术主要包括两个阶段：压缩和重建。前者通过减少数据大小来获得稀疏的图像表示，后者则重建原始图像。与基于波形的方法不同，Carlsson（[1988](#bib.bib13)）提出了一种基于草图的方法，通过插值过程获得稀疏表示和重建图像。Galić
    et al.（[2008](#bib.bib48)）将基于偏微分方程（PDE）的修复引入图像压缩，其中图像编码和解码都基于边缘增强各向异性扩散。最近，一些研究人员（Baluja
    et al., [2019](#bib.bib5); Dai et al., [2020](#bib.bib28); Schrader et al., [2023](#bib.bib169)）应用深度学习方法生成采样掩模并通过修复网络重建图像。图[13](#S2.F13
    "Figure 13 ‣ 2.9.4 Image Compression ‣ 2.9 Inpainting-based Applications ‣ 2 Image
    Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey")展示了几个图像压缩和修复的示例，其中重建图像的质量基于自适应稀疏采样和修复。'
- en: '![Refer to caption](img/01f1fbbdce00c6e210989951547d37ee.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/01f1fbbdce00c6e210989951547d37ee.png)'
- en: 'Figure 13: Two representative examples of image compression with inpainting.
    From left to right: input image, sampling mask, sampled image, and reconstructed
    image. Images come from (Dai et al., [2020](#bib.bib28)).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：图像修复的两个代表性图像压缩示例。从左到右：输入图像、采样掩模、采样图像和重建图像。图像来自（Dai et al., [2020](#bib.bib28)）。
- en: 2.9.5 Text-guided image editing
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.9.5 基于文本的图像编辑
- en: 'Image inpainting is a basic processing tool for image editing. Recent generative
    models based on probabilistic diffusion have the powerful capability of text-to-image
    generation, which provides the potential for text-guided image editing with diffusion
    model-based image inpainting approaches. For example, diffusion-based SmartBrush (Xie
    et al., [2023](#bib.bib218)) edited images with the guidance of text and shape.
    Fig. [14](#S2.F14 "Figure 14 ‣ 2.9.5 Text-guided image editing ‣ 2.9 Inpainting-based
    Applications ‣ 2 Image Inpainting ‣ Deep Learning-based Image and Video Inpainting:
    A Survey") illustrates several samples generated by SmartBrush. The first row
    adds new objects and the second row replaces original objects with new contents.
    We can see that the edited results have high visual realism and are consistent
    with text prompts.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '图像修复是图像编辑的基本处理工具。基于概率扩散的最新生成模型具有强大的文本到图像生成能力，这为基于扩散模型的图像修复方法提供了文本引导的图像编辑潜力。例如，基于扩散的SmartBrush（Xie
    et al., [2023](#bib.bib218)）在文本和形状的指导下编辑图像。图[14](#S2.F14 "Figure 14 ‣ 2.9.5 Text-guided
    image editing ‣ 2.9 Inpainting-based Applications ‣ 2 Image Inpainting ‣ Deep
    Learning-based Image and Video Inpainting: A Survey")展示了SmartBrush生成的几个样本。第一排添加了新对象，第二排用新内容替换了原始对象。我们可以看到，编辑结果具有很高的视觉真实感，并且与文本提示一致。'
- en: '![Refer to caption](img/67c482953730339b8da73137e27474a2.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/67c482953730339b8da73137e27474a2.png)'
- en: 'Figure 14: Selected examples of text-based image editing. Each group includes
    the input image with mask (red transparent) and text prompts and edited results.
    Images come from (Xie et al., [2023](#bib.bib218)).'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：基于文本的图像编辑的选定示例。每组包括带掩模（红色透明）和文本提示的输入图像及编辑结果。图像来自（Xie et al., [2023](#bib.bib218)）。
- en: 3 Video Inpainting
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 视频修复
- en: 3.1 Method
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 方法
- en: 'Unlike images, videos have an additional temporal dimension which provides
    extra information about objects or camera movement. This information helps networks
    to obtain a better understanding of the context of the video. Therefore, the video
    inpainting task aims to ensure both spatial consistency and temporal coherence.
    Existing deep learning-based video inpainting methods can be roughly divided into
    four categories: 3D CNN-based approaches, shift-based approaches, flow-guided
    approaches, and attention-based approaches. We refer the readers to more conventional
    methods in (Ilan and Shamir, [2015](#bib.bib79)).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像不同，视频具有额外的时间维度，这提供了有关物体或相机运动的额外信息。这些信息帮助网络更好地理解视频的上下文。因此，视频修补任务旨在确保空间一致性和时间连贯性。现有的深度学习视频修补方法大致可分为四类：3D
    CNN方法、基于位移的方法、流引导的方法和基于注意力的方法。我们建议读者参考更传统的方法（**Ilan and Shamir**, [2015](#bib.bib79)）。
- en: 3.1.1 3D CNN-based Approaches
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 基于3D CNN的方法
- en: To deal with the temporal dimension, researchers proposed 3D CNN-based approaches,
    which often combine temporal restoration and image inpainting. Wang et al. ([2019a](#bib.bib192))
    proposed a two-stage pipeline to jointly infer temporal structure and spatial
    texture details. The first sub-network processes the low-resolution videos with
    a 3D CNN, and the second sub-network completes the original-resolution video frames
    with an extended 2D inpainting network (Iizuka et al., [2017](#bib.bib78)). Inspired
    by the gated convolution in image inpainting (Yu et al., [2019](#bib.bib234)),
    Chang et al. ([2019a](#bib.bib15)) proposed a 3D gated convolution and a temporal
    SN-PatchGAN for free-form video inpainting. They also integrated the perceptual
    loss (Johnson et al., [2016](#bib.bib84)) and style loss (Gatys et al., [2016](#bib.bib50))
    into the training objective. Hu et al. ([2020](#bib.bib73)) proposed a two-stage
    video inpainting network, where they obtain a coarse inpainting result with a
    3D CNN and then fuse inpainting proposals generated by matching valid pixels and
    pixels in coarse inpainting results.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理时间维度，研究人员提出了基于**3D CNN**的方法，这些方法通常结合了时间恢复和图像修补。**Wang et al.** ([2019a](#bib.bib192))
    提出了一个两阶段管道，联合推断时间结构和空间纹理细节。第一个子网络使用3D CNN处理低分辨率视频，第二个子网络则利用扩展的2D修补网络（**Iizuka
    et al.**, [2017](#bib.bib78)）完成原始分辨率的视频帧。受图像修补中**gated convolution**的启发（**Yu et
    al.**, [2019](#bib.bib234)），**Chang et al.** ([2019a](#bib.bib15)) 提出了3D gated
    convolution和一个用于自由形状视频修补的时间SN-PatchGAN。他们还将**感知损失**（**Johnson et al.**, [2016](#bib.bib84)）和**风格损失**（**Gatys
    et al.**, [2016](#bib.bib50)）整合到训练目标中。**Hu et al.** ([2020](#bib.bib73)) 提出了一个两阶段的视频修补网络，其中他们通过3D
    CNN获得粗略的修补结果，然后融合通过匹配有效像素和粗略修补结果中的像素生成的修补提案。
- en: 3.1.2 Shift-based Approaches
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 基于位移的方法
- en: Considering the high computational cost of 3D convolution, Lin et al. ([2019](#bib.bib120))
    proposed a generic temporal shift module (TSM) to capture temporal relationships
    with high efficiency. This technique is extended for video inpainting. Chang et al.
    ([2019b](#bib.bib16)) developed a learnable gated TSM, which combines a TSM with
    learnable shifting kernels and gated convolution (Yu et al., [2019](#bib.bib234)).
    They also equipped the 2D convolution layers in SN-PatchGAN (Yu et al., [2019](#bib.bib234))
    with gated TSM. However, TSM often leads to blurry content due to misaligned features.
    To solve this, Zou et al. ([2021](#bib.bib274)) proposed a spatially-aligned TSM
    (TSAM), aligning features to the current frame after shifting features. The alignment
    process is based on estimated flow with a validity mask. Ouyang et al. ([2021](#bib.bib145))
    applied an internal learning strategy for video inpainting, which implicitly learns
    the information shift from valid regions to unknown parts in a single video sample.
    They also designed the gradient regularization term and the anti-ambiguity loss
    term for temporal consistency reconstruction and realistic detail generation.
    Ke et al. ([2021](#bib.bib89)) presented an occlusion-aware video object inpainting
    method. Specifically, they completed the object shape with a transformer-based
    network, recovered the flow within the completed object region under the guidance
    of the object contour, and filled missing content with an occlusion-aware TSM
    after the flow-guided pixel propagation.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 3D 卷积的高计算成本，Lin 等人 ([2019](#bib.bib120)) 提出了一个通用的时间偏移模块（TSM），以高效地捕捉时间关系。该技术已扩展应用于视频修复。Chang
    等人 ([2019b](#bib.bib16)) 开发了一种可学习的门控 TSM，它将 TSM 与可学习的偏移核和门控卷积（Yu 等人，[2019](#bib.bib234)）结合起来。他们还在
    SN-PatchGAN（Yu 等人，[2019](#bib.bib234)）中为 2D 卷积层配备了门控 TSM。然而，由于特征对齐不准确，TSM 常常导致模糊内容。为了解决这个问题，Zou
    等人 ([2021](#bib.bib274)) 提出了一个空间对齐 TSM（TSAM），在特征偏移后将特征对齐到当前帧。对齐过程基于带有有效性掩码的估计流。Ouyang
    等人 ([2021](#bib.bib145)) 应用了一种内部学习策略用于视频修复，该策略隐式地学习了从有效区域到单个视频样本中未知部分的信息偏移。他们还为时间一致性重建和逼真细节生成设计了梯度正则化项和反歧义损失项。Ke
    等人 ([2021](#bib.bib89)) 提出了一种考虑遮挡的视频对象修复方法。具体而言，他们使用基于变换器的网络完成了对象形状，在对象轮廓的指导下恢复了完成对象区域内的流，并在流引导的像素传播后使用遮挡感知
    TSM 填充缺失内容。
- en: 3.1.3 Flow-guided Approaches
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 流引导方法
- en: Optical flow is a common tool to model the temporal information in videos, which
    is also applied to solve video inpainting. Based on the completed flow, the missing
    pixels in the current frame can be filled by propagating pixels from neighboring
    frames. Kim et al. ([2019b](#bib.bib91)) modeled the video inpainting task as
    a multi-to-single frame inpainting problem and proposed a 3D-2D encoder-decoder
    network VINet. This network includes several flow and mask sub-networks in a progressive
    manner. They also introduced the flow and warp loss to further enforce temporal
    consistency. Chang et al. ([2019c](#bib.bib17)) proposed a three-stage video inpainting
    framework consisting of a warping network, an inpainting network, and a refinement
    network. In the warping network, bilinear interpolation is used to recover background
    flow without learning. Then the refinement network selected the best candidate
    from two frames completed by warping and inpainting network to generate the final
    output. Zhang et al. ([2019a](#bib.bib248)) applied internal learning to infer
    both frames and flow from input random noise and used flow generation loss to
    enhance temporal coherence. Xu et al. ([2019](#bib.bib222)) proposed a flow-guided
    completion framework consisting of three steps. It first fills the incomplete
    optical flow with stacked CNN networks, then propagates pixels from known regions
    to holes with inpainted flow guidance, and finally completes unseen regions with
    an image inpainting network (Yu et al., [2019](#bib.bib234)). To reduce the over-smoothing
    in the boundary regions during flow completion, they leveraged hard flow example
    mining to encourage the network to produce sharp edges. To solve the same problem,
    Gao et al. ([2020](#bib.bib49)) explicitly completed motion edges and used them
    to guide flow completion. In addition, they introduced a non-local flow connection
    to enable content propagation from distant frames.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 光流是一种常用的工具来建模视频中的时间信息，也用于解决视频修复问题。基于完成的流，当前帧中的缺失像素可以通过从邻近帧传播像素来填充。Kim等人（[2019b](#bib.bib91)）将视频修复任务建模为多帧到单帧的修复问题，并提出了一个3D-2D编码解码网络VINet。该网络包括几个逐步进行的流和掩模子网络。他们还引入了流和变形损失以进一步增强时间一致性。Chang等人（[2019c](#bib.bib17)）提出了一个三阶段的视频修复框架，包括变形网络、修复网络和精炼网络。在变形网络中，使用双线性插值来恢复背景流而不进行学习。然后，精炼网络从由变形和修复网络完成的两帧中选择最佳候选帧以生成最终输出。Zhang等人（[2019a](#bib.bib248)）应用内部学习从输入随机噪声中推断帧和流，并使用流生成损失来增强时间一致性。Xu等人（[2019](#bib.bib222)）提出了一个流引导的完成框架，包括三个步骤。首先用堆叠CNN网络填充不完整的光流，然后用修复的流指导将像素从已知区域传播到孔洞中，最后用图像修复网络（Yu等人，[2019](#bib.bib234)）完成未见区域。为了减少流完成过程中边界区域的过度平滑，他们利用了硬流示例挖掘以鼓励网络产生清晰的边缘。为了解决同样的问题，Gao等人（[2020](#bib.bib49)）显式地完成了运动边缘，并用它们来指导流完成。此外，他们引入了非局部流连接，以实现远程帧的内容传播。
- en: These previous methods cannot guarantee the consistency of flow, and even small
    errors in the flow may lead to geometric distortion in the video. Inspired by
    this, Lao et al. ([2021](#bib.bib96)) transformed the background of a 3D scene
    to a 2D scene template and learned the mapping of the template to the mask in
    the image. Given that the complex motion of objects between consecutive frames
    will increase the difficulty to recover flow, Zhang et al. ([2022b](#bib.bib251))
    introduced an inertia prior in flow completion to align and aggregate flow features.
    To alleviate the spatial incoherence problem, they proposed an adaptive style
    fusion network to correct the distribution in the warped regions with the guidance
    of feature distribution in valid regions. Kang et al. ([2022](#bib.bib85)) offset
    the weaknesses of the error accumulation of a multi-stage pipeline in flow-based
    methods by introducing an error compensation strategy, which iteratively detects
    and corrects the inconsistency errors during the flow-guided pixel propagation.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这些之前的方法无法保证流的连贯性，甚至流中的小错误可能导致视频中的几何扭曲。受此启发，Lao等人（[2021](#bib.bib96)）将3D场景的背景转化为2D场景模板，并学习了模板到图像中的掩模的映射。鉴于物体在连续帧之间的复杂运动会增加恢复流的难度，Zhang等人（[2022b](#bib.bib251)）在流完成中引入了惯性先验，以对齐和聚合流特征。为了解决空间不连贯问题，他们提出了一种自适应样式融合网络，以有效区域的特征分布为指导，纠正变形区域的分布。Kang等人（[2022](#bib.bib85)）通过引入误差补偿策略来弥补基于流的方法中的多阶段管道的误差累积弱点，该策略迭代地检测和纠正流引导的像素传播中的不一致性错误。
- en: The above hand-crafted flow-based methods restored videos with high computation
    and memory consumption because these processes cannot be accelerated by GPU. To
    speed up training and inference, Li et al. ([2022d](#bib.bib113)) proposed an
    end-to-end framework. They propagated features based on completed flow in low
    resolution and used deformable convolution to decrease the distortion caused by
    errors in flow. The temporal focal transformer blocks were stacked to aggregate
    local and non-local features.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 上述手工制作的基于流的方法在恢复视频时计算和内存消耗较高，因为这些过程不能通过GPU加速。为了加速训练和推理，Li et al. ([2022d](#bib.bib113))
    提出了一个端到端的框架。他们在低分辨率下基于完成的流传播特征，并使用可变形卷积来减少流错误造成的失真。时间焦点变换器块被堆叠以聚合局部和非局部特征。
- en: 3.1.4 Attention-based Approaches
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 基于注意力的方法
- en: The attention mechanism is often applied to model the contextual information
    and enlarge the spatial-temporal window. Oh et al. ([2019](#bib.bib142)) recurrently
    calculated the attention scores between the target and reference frames, and progressively
    filled holes of the target frame from the boundary. Lee et al. ([2019](#bib.bib98))
    firstly aligned frames by an affine transformation, and then copied pixels based
    on the similarity between the target frame and aligned reference frames. Woo et al.
    ([2020](#bib.bib210)) proposed a coarse-to-fine framework for video inpainting.
    The first stage roughly recovers the target holes based on the computed homography
    between the target and reference frames, and the second stage refines the filled
    contents with non-local attention. They also introduced an optical flow estimator
    to enhance temporal consistency. Considering the motion of the foreground objects
    is diverse, the choice of reference frames becomes more important. While other
    methods take neighboring frames or frames in a specific distance as reference
    frames, Li et al. ([2020a](#bib.bib101)) dynamically updated long-term reference
    frames after aggregating short-term aligned features.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制通常用于建模上下文信息并扩大时空窗口。Oh et al. ([2019](#bib.bib142)) 递归地计算了目标帧和参考帧之间的注意力分数，并逐步填补目标帧的边界孔洞。Lee
    et al. ([2019](#bib.bib98)) 首先通过仿射变换对齐帧，然后基于目标帧与对齐参考帧之间的相似性复制像素。Woo et al. ([2020](#bib.bib210))
    提出了一个粗到细的视频修复框架。第一阶段基于目标帧和参考帧之间的计算单应性粗略恢复目标孔洞，第二阶段通过非局部注意力细化填补的内容。他们还引入了光流估计器以增强时间一致性。考虑到前景对象的运动多样性，参考帧的选择变得更加重要。虽然其他方法以邻近帧或特定距离的帧作为参考帧，但Li
    et al. ([2020a](#bib.bib101)) 在聚合短期对齐特征后动态更新长期参考帧。
- en: 'Table 5: Summary of video inpainting methods. Like image inpainting, we also
    split existing video inpainting approaches into three types according to the number
    of stages: 1) one-stage framework (\Circled[inner color=blue]1) usually designs
    a generator to recover the missing contents for each frame; 2) two-stage framework
    (\Circled[inner color=green]2) often consists of two networks for different purposes;
    and 3) multi-stage framework (\Circled[inner color=red]m) splits video inpainting
    into multiple steps.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：视频修复方法总结。与图像修复类似，我们也根据阶段数量将现有的视频修复方法分为三类：1) 一阶段框架（\Circled[inner color=blue]1）通常设计一个生成器来恢复每帧的缺失内容；2)
    二阶段框架（\Circled[inner color=green]2）通常由两个网络组成，用于不同的目的；3) 多阶段框架（\Circled[inner color=red]m）将视频修复拆分为多个步骤。
- en: '| Category | Method | Stage | Loss details |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 阶段 | 损失详细信息 |'
- en: '| L1 loss | GAN loss | Perceptual loss | Style loss | TV loss | Flow loss |
    Warp loss |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| L1 损失 | GAN 损失 | 感知损失 | 风格损失 | TV 损失 | 流损失 | 变形损失 |'
- en: '| 3D CNN | Wang et al. ([2019a](#bib.bib192)) | \Circled[inner color=green]2
    | ✓ |  |  |  |  |  |  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 3D CNN | Wang et al. ([2019a](#bib.bib192)) | \Circled[inner color=green]2
    | ✓ |  |  |  |  |  |  |'
- en: '| Chang et al. ([2019a](#bib.bib15)) | \Circled[inner color=blue]1 | ✓ | ✓
    | ✓ | ✓ |  |  |  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| Chang et al. ([2019a](#bib.bib15)) | \Circled[inner color=blue]1 | ✓ | ✓
    | ✓ | ✓ |  |  |  |'
- en: '| Hu et al. ([2020](#bib.bib73)) | \Circled[inner color=green]2 | ✓ | ✓ |  |  |  |  |  |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| Hu et al. ([2020](#bib.bib73)) | \Circled[inner color=green]2 | ✓ | ✓ |  |  |  |  |  |'
- en: '| Shift | Chang et al. ([2019b](#bib.bib16)) | \Circled[inner color=blue]1
    | ✓ | ✓ | ✓ | ✓ |  |  |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Shift | Chang et al. ([2019b](#bib.bib16)) | \Circled[inner color=blue]1
    | ✓ | ✓ | ✓ | ✓ |  |  |  |'
- en: '| Zou et al. ([2021](#bib.bib274)) | \Circled[inner color=blue]1 | ✓ | ✓ |
    ✓ | ✓ |  |  |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Zou et al. ([2021](#bib.bib274)) | \Circled[inner color=blue]1 | ✓ | ✓ |
    ✓ | ✓ |  |  |  |'
- en: '| Ouyang et al. ([2021](#bib.bib145)) | \Circled[inner color=blue]1 | ✓ |  |  |  |  |  |  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Ouyang et al. ([2021](#bib.bib145)) | \Circled[inner color=blue]1 | ✓ |  |  |  |  |  |  |'
- en: '| Ke et al. ([2021](#bib.bib89)) | \Circled[inner color=red]m | ✓ | ✓ |  |  |  |
    ✓ | ✓ |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 柯等人 ([2021](#bib.bib89)) | \Circled[inner color=red]m | ✓ | ✓ |  |  |  |
    ✓ | ✓ |'
- en: '| Flow | Kim et al. ([2019b](#bib.bib91)) | \Circled[inner color=blue]1 | ✓
    |  |  |  |  | ✓ | ✓ |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Flow | 金等人 ([2019b](#bib.bib91)) | \Circled[inner color=blue]1 | ✓ |  |  |  |  |
    ✓ | ✓ |'
- en: '| Chang et al. ([2019c](#bib.bib17)) | \Circled[inner color=red]m | ✓ | ✓ |
    ✓ |  |  |  |  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 常等人 ([2019c](#bib.bib17)) | \Circled[inner color=red]m | ✓ | ✓ | ✓ |  |  |  |  |'
- en: '| Zhang et al. ([2019a](#bib.bib248)) | \Circled[inner color=blue]1 | ✓ |  |
    ✓ |  |  | ✓ | ✓ |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 张等人 ([2019a](#bib.bib248)) | \Circled[inner color=blue]1 | ✓ |  | ✓ |  |  |
    ✓ | ✓ |'
- en: '| Xu et al. ([2019](#bib.bib222)) | \Circled[inner color=red]m |  |  |  |  |  |
    ✓ |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 许等人 ([2019](#bib.bib222)) | \Circled[inner color=red]m |  |  |  |  |  | ✓
    |  |'
- en: '| Gao et al. ([2020](#bib.bib49)) | \Circled[inner color=red]m | ✓ |  |  |  |  |
    ✓ |  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 高等人 ([2020](#bib.bib49)) | \Circled[inner color=red]m | ✓ |  |  |  |  | ✓
    |  |'
- en: '| Lao et al. ([2021](#bib.bib96)) | \Circled[inner color=green]2 | ✓ |  |  |  |  |  |
    ✓ |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 老等人 ([2021](#bib.bib96)) | \Circled[inner color=green]2 | ✓ |  |  |  |  |  |
    ✓ |'
- en: '| Zhang et al. ([2022b](#bib.bib251)) | \Circled[inner color=red]m | ✓ | ✓
    |  |  |  | ✓ | ✓ |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 张等人 ([2022b](#bib.bib251)) | \Circled[inner color=red]m | ✓ | ✓ |  |  |  |
    ✓ | ✓ |'
- en: '| Li et al. ([2022d](#bib.bib113)) | \Circled[inner color=blue]1 | ✓ | ✓ |  |  |  |
    ✓ |  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 李等人 ([2022d](#bib.bib113)) | \Circled[inner color=blue]1 | ✓ | ✓ |  |  |  |
    ✓ |  |'
- en: '| Kang et al. ([2022](#bib.bib85)) | \Circled[inner color=red]m | ✓ | ✓ |  |  |  |
    ✓ |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 康等人 ([2022](#bib.bib85)) | \Circled[inner color=red]m | ✓ | ✓ |  |  |  |
    ✓ |  |'
- en: '| Attention | Oh et al. ([2019](#bib.bib142)) | \Circled[inner color=red]m
    | ✓ |  | ✓ | ✓ | ✓ |  |  |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| Attention | 欧等人 ([2019](#bib.bib142)) | \Circled[inner color=red]m | ✓ |  |
    ✓ | ✓ | ✓ |  |  |'
- en: '| Lee et al. ([2019](#bib.bib98)) | \Circled[inner color=green]2 | ✓ |  | ✓
    | ✓ | ✓ |  |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 李等人 ([2019](#bib.bib98)) | \Circled[inner color=green]2 | ✓ |  | ✓ | ✓ |
    ✓ |  |  |'
- en: '| Woo et al. ([2020](#bib.bib210)) | \Circled[inner color=green]2 | ✓ | ✓ |  |  |  |
    ✓ | ✓ |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| Woo等人 ([2020](#bib.bib210)) | \Circled[inner color=green]2 | ✓ | ✓ |  |  |  |
    ✓ | ✓ |'
- en: '| Li et al. ([2020a](#bib.bib101)) | \Circled[inner color=blue]1 | ✓ |  | ✓
    | ✓ |  |  |  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 李等人 ([2020a](#bib.bib101)) | \Circled[inner color=blue]1 | ✓ |  | ✓ | ✓ |  |  |  |'
- en: '| Zeng et al. ([2020a](#bib.bib241)) | \Circled[inner color=blue]1 | ✓ | ✓
    |  |  |  |  |  |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 曾等人 ([2020a](#bib.bib241)) | \Circled[inner color=blue]1 | ✓ | ✓ |  |  |  |  |  |'
- en: '| Liu et al. ([2021b](#bib.bib126)) | \Circled[inner color=blue]1 | ✓ | ✓ |  |  |  |  |  |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 刘等人 ([2021b](#bib.bib126)) | \Circled[inner color=blue]1 | ✓ | ✓ |  |  |  |  |  |'
- en: '| Chen et al. ([2021](#bib.bib18)) | \Circled[inner color=green]2 | ✓ |  |
    ✓ | ✓ |  |  |  |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 陈等人 ([2021](#bib.bib18)) | \Circled[inner color=green]2 | ✓ |  | ✓ | ✓ |  |  |  |'
- en: '| Zhang et al. ([2022a](#bib.bib250)) | \Circled[inner color=green]2 | ✓ |
    ✓ |  |  |  | ✓ | ✓ |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 张等人 ([2022a](#bib.bib250)) | \Circled[inner color=green]2 | ✓ | ✓ |  |  |  |
    ✓ | ✓ |'
- en: '| Ren et al. ([2022](#bib.bib156)) | \Circled[inner color=green]2 | ✓ |  |  |  |
    ✓ |  |  |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 任等人 ([2022](#bib.bib156)) | \Circled[inner color=green]2 | ✓ |  |  |  | ✓
    |  |  |'
- en: Instead of a frame-by-frame inpainting strategy, Zeng et al. ([2020a](#bib.bib241))
    adopted a “multi-to-multi” mechanism to fill in the holes in all input frames.
    Specifically, they proposed a spatial-temporal transformer network (STTN) to compute
    attention in both spatial and temporal dimensions. Based on STTN (Zeng et al.,
    [2020a](#bib.bib241)), Liu et al. ([2021b](#bib.bib126)) separated feature maps
    into overlapping patches, enabling more interactions between neighboring patches.
    In addition, they modified the common transformer block by inserting soft split
    and soft composition modules into the feed-forward network. Chen et al. ([2021](#bib.bib18))
    proposed an interactive video inpainting method to jointly perform object segmentation
    and video inpainting with user guidance. For network design, they introduce a
    spatial time attention block to update the target frames’ features with the reference
    frames’ features. Zhang et al. ([2022a](#bib.bib250)) designed a flow-guided transformer
    to combine the flow and the attention. They first utilized the completed flow
    to propagate pixels from neighboring frames, and then synthesized the remaining
    missing regions with a flow-guided spatial transformer and a temporal transformer.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 代替逐帧修补策略，曾等人 ([2020a](#bib.bib241)) 采用了“多对多”机制来填补所有输入帧中的空洞。具体而言，他们提出了一种空间-时间变换网络（STTN）来计算空间和时间维度的注意力。基于
    STTN （曾等人，[2020a](#bib.bib241)），刘等人 ([2021b](#bib.bib126)) 将特征图分离成重叠的块，从而使相邻块之间有更多的交互。此外，他们通过在前馈网络中插入软分割和软合成模块来修改常见的变换器块。陈等人
    ([2021](#bib.bib18)) 提出了一个互动视频修补方法，通过用户指导共同进行目标分割和视频修补。在网络设计方面，他们引入了一个空间时间注意力块，用参考帧的特征来更新目标帧的特征。张等人
    ([2022a](#bib.bib250)) 设计了一个流引导变换器，将流和注意力结合在一起。他们首先利用完成的流来传播来自邻近帧的像素，然后使用流引导空间变换器和时间变换器合成剩余的缺失区域。
- en: These attention-based methods still suffer from blurry content in high frequency
    due to mapping videos into a continuous feature space. By learning a specific
    codebook for each video and using subscripts of code to represent images, Ren
    et al. ([2022](#bib.bib156)) transformed videos to a discrete latent space. Then
    a discrete latent transformer was applied to infer content in masked regions.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基于注意力的方法仍然因为将视频映射到连续特征空间而导致高频内容模糊。通过为每个视频学习一个特定的代码本并使用代码的下标来表示图像，Ren et al.
    ([2022](#bib.bib156)) 将视频转换为离散潜在空间。然后，应用离散潜在变换器来推断掩蔽区域的内容。
- en: 'Table [5](#S3.T5 "Table 5 ‣ 3.1.4 Attention-based Approaches ‣ 3.1 Method ‣
    3 Video Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey")
    summarizes the technical details of existing video inpainting methods.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [5](#S3.T5 "Table 5 ‣ 3.1.4 Attention-based Approaches ‣ 3.1 Method ‣ 3 Video
    Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey") 总结了现有视频修复方法的技术细节。'
- en: 'Table 6: Quantitative comparisons of representative video inpainting methods
    on YouTube-VOS and DAVIS dataset. $\ddagger$ Higher is better. $\dagger$ Lower
    is better. *: our results using the method described in STTN (Zeng et al., [2020a](#bib.bib241)),
    and numerical differences may be due to different optical flow models during evaluation.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6：YouTube-VOS 和 DAVIS 数据集上具有代表性的视频修复方法的定量比较。$\ddagger$ 数值越高越好。$\dagger$ 数值越低越好。*:
    我们使用 STTN (Zeng et al., [2020a](#bib.bib241)) 中描述的方法的结果，数值差异可能由于评估期间使用了不同的光流模型。'
- en: '| Methods | YouTube-VOS | DAVIS |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | YouTube-VOS | DAVIS |'
- en: '| PSNR$\ddagger$ | SSIM$\ddagger$ | VFID$\dagger$ | FWE($\times 10^{-2}$)$\dagger$
    | PSNR$\ddagger$ | SSIM$\ddagger$ | VFID$\dagger$ | FWE($\times 10^{-2}$)$\dagger$
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| PSNR$\ddagger$ | SSIM$\ddagger$ | VFID$\dagger$ | FWE($\times 10^{-2}$)$\dagger$
    | PSNR$\ddagger$ | SSIM$\ddagger$ | VFID$\dagger$ | FWE($\times 10^{-2}$)$\dagger$
    |'
- en: '| VINet (Kim et al., [2019b](#bib.bib91)) | 29.20 | 0.9434 | 0.072 | 0.1490
    / - | 28.96 | 0.9411 | 0.199 | 0.1785 / - |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| VINet (Kim et al., [2019b](#bib.bib91)) | 29.20 | 0.9434 | 0.072 | 0.1490
    / - | 28.96 | 0.9411 | 0.199 | 0.1785 / - |'
- en: '| DFVI (Xu et al., [2019](#bib.bib222)) | 29.16 | 0.9429 | 0.066 | 0.1509 /
    - | 28.81 | 0.9404 | 0.187 | 0.1880 / 0.1608* |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| DFVI (Xu et al., [2019](#bib.bib222)) | 29.16 | 0.9429 | 0.066 | 0.1509 /
    - | 28.81 | 0.9404 | 0.187 | 0.1880 / 0.1608* |'
- en: '| LGTSM (Chang et al., [2019b](#bib.bib16)) | 29.74 | 0.9504 | 0.070 | 0.1859
    / - | 28.57 | 0.9409 | 0.170 | 0.2566 / 0.1640* |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| LGTSM (Chang et al., [2019b](#bib.bib16)) | 29.74 | 0.9504 | 0.070 | 0.1859
    / - | 28.57 | 0.9409 | 0.170 | 0.2566 / 0.1640* |'
- en: '| CAP (Lee et al., [2019](#bib.bib98)) | 31.58 | 0.9607 | 0.071 | 0.1470 /
    - | 30.28 | 0.9521 | 0.182 | 0.1824 / 0.1533* |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| CAP (Lee et al., [2019](#bib.bib98)) | 31.58 | 0.9607 | 0.071 | 0.1470 /
    - | 30.28 | 0.9521 | 0.182 | 0.1824 / 0.1533* |'
- en: '| FGVC (Gao et al., [2020](#bib.bib49)) | 29.68 | 0.9396 | 0.064 | - / 0.0858*
    | 30.24 | 0.9444 | 0.143 | - / 0.1530* |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| FGVC (Gao et al., [2020](#bib.bib49)) | 29.68 | 0.9396 | 0.064 | - / 0.0858*
    | 30.24 | 0.9444 | 0.143 | - / 0.1530* |'
- en: '| STTN (Zeng et al., [2020a](#bib.bib241)) | 32.34 | 0.9655 | 0.053 | 0.1451
    / 0.0884* | 30.67 | 0.9560 | 0.149 | 0.1779 / 0.1449* |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| STTN (Zeng et al., [2020a](#bib.bib241)) | 32.34 | 0.9655 | 0.053 | 0.1451
    / 0.0884* | 30.67 | 0.9560 | 0.149 | 0.1779 / 0.1449* |'
- en: '| FuseFormer (Liu et al., [2021b](#bib.bib126)) | 33.16 | 0.9673 | 0.051 |
    - / 0.0875* | 32.54 | 0.9700 | 0.138 | - / 0.1336* |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| FuseFormer (Liu et al., [2021b](#bib.bib126)) | 33.16 | 0.9673 | 0.051 |
    - / 0.0875* | 32.54 | 0.9700 | 0.138 | - / 0.1336* |'
- en: '| FGT (Zhang et al., [2022a](#bib.bib250)) | 32.11 | 0.9598 | 0.054 | - / 0.0860*
    | 32.39 | 0.9633 | 0.1095 | - / 0.1517* |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| FGT (Zhang et al., [2022a](#bib.bib250)) | 32.11 | 0.9598 | 0.054 | - / 0.0860*
    | 32.39 | 0.9633 | 0.1095 | - / 0.1517* |'
- en: '| ISVI (Zhang et al., [2022b](#bib.bib251)) | 32.80 | 0.9611 | 0.048 | - /
    0.0856* | 33.70 | 0.967 | 0.1028 | - / 0.1509* |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| ISVI (Zhang et al., [2022b](#bib.bib251)) | 32.80 | 0.9611 | 0.048 | - /
    0.0856* | 33.70 | 0.967 | 0.1028 | - / 0.1509* |'
- en: '| E2FGVI (Li et al., [2022d](#bib.bib113)) | 33.50 | 0.9692 | 0.046 | - / 0.0864*
    | 32.71 | 0.9700 | 0.096 | - / 0.1383* |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| E2FGVI (Li et al., [2022d](#bib.bib113)) | 33.50 | 0.9692 | 0.046 | - / 0.0864*
    | 32.71 | 0.9700 | 0.096 | - / 0.1383* |'
- en: 3.2 Loss Functions
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 损失函数
- en: 'Video inpainting is very close to image inpainting. Therefore, many loss functions
    for training image inpainting networks are also applied to train video inpainting
    models, including reconstruction loss, GAN loss, perceptual loss, and style loss.
    To complete the corrupted flow, two losses are often used:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 视频修复与图像修复非常相似。因此，许多用于训练图像修复网络的损失函数也被应用于训练视频修复模型，包括重建损失、GAN损失、感知损失和风格损失。为了完成损坏的流，通常使用两种损失：
- en: 'Flow loss. Similar to the image reconstruction loss, the flow loss measures
    the difference between inpainted flow and its ground-truth version, which is defined
    as:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 流失。类似于图像重建损失，流失测量修复流与其真实版本之间的差异，定义如下：
- en: '|  | $\mathcal{L}_{flow}=&#124;&#124;O_{i,j}\odot(F_{i,j}-\hat{F}_{i,j})&#124;&#124;_{1},$
    |  | (11) |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{flow}=&#124;&#124;O_{i,j}\odot(F_{i,j}-\hat{F}_{i,j})&#124;&#124;_{1},$
    |  | (11) |'
- en: where $\hat{F}_{i,j}$ is the inpainted optical flow from frame $i$ to frame
    $j$, $F_{i,j}$ is the ground-truth flow estimated by pre-trained flow estimation
    networks, e.g., FlowNet2 (Ilg et al., [2017](#bib.bib80)) and PWC-Net (Sun et al.,
    [2018a](#bib.bib180)), and $O_{i,j}$ denotes the occlusion map obtained by the
    forward-backward consistency check.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\hat{F}_{i,j}$是从帧$i$到帧$j$的修复光流，$F_{i,j}$是由预训练的光流估计网络（例如FlowNet2 (Ilg et al.,
    [2017](#bib.bib80))和PWC-Net (Sun et al., [2018a](#bib.bib180))）估计的真实光流，$O_{i,j}$表示通过前向-反向一致性检查获得的遮挡图。
- en: 'Warp loss. This loss encourages image-flow consistency:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: Warp loss。这种损失函数鼓励图像流的一致性：
- en: '|  | $\mathcal{L}_{warp}=&#124;&#124;I_{i}-I_{j}(\hat{F}_{i,j})&#124;&#124;_{1},$
    |  | (12) |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{warp}=&#124;&#124;I_{i}-I_{j}(\hat{F}_{i,j})&#124;&#124;_{1},$
    |  | (12) |'
- en: where $I_{j}(\hat{F}_{i,j})$ refers to the warped result of the frame $I_{j}$
    using the generated flow $\hat{F}_{i,j}$ through backward warping.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$I_{j}(\hat{F}_{i,j})$指的是使用生成的光流$\hat{F}_{i,j}$通过反向变形得到的帧$I_{j}$的变形结果。
- en: '![Refer to caption](img/23ee36e8e19cdbd9e1d2b51ef20fb03a.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/23ee36e8e19cdbd9e1d2b51ef20fb03a.png)'
- en: 'Figure 15: Qualitative comparisons of representative video inpainting methods
    on YouTube-VOS and DAVIS dataset. The light blue mask highlights the corrupted
    regions. The first three columns are random masks and the remaining two columns
    are object masks.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：YouTube-VOS和DAVIS数据集中具有代表性的视频修复方法的定性比较。浅蓝色掩码突出显示了受损区域。前三列为随机掩码，后两列为对象掩码。
- en: 3.3 Datasets
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据集
- en: For video inpainting, three common video datasets, i.e., FaceForensics (Rössler
    et al., [2018](#bib.bib162)), DAVIS (Perazzi et al., [2016](#bib.bib150)) and
    YouTube-VOS (Xu et al., [2018a](#bib.bib221)), are used for training and evaluation.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 对于视频修复，使用了三个常见的视频数据集，即FaceForensics (Rössler et al., [2018](#bib.bib162))、DAVIS
    (Perazzi et al., [2016](#bib.bib150)) 和 YouTube-VOS (Xu et al., [2018a](#bib.bib221))，用于训练和评估。
- en: •
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FaceForensics: A face forgery detection video dataset consisting of 1,004 videos.
    Among them, 854 videos are used for training and the rest are used for evaluation.'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FaceForensics：一个包含1,004个视频的面部伪造检测视频数据集。其中854个视频用于训练，其余用于评估。
- en: •
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DAVIS dataset: A densely annotated video segmentation dataset contains 150
    videos with challenging motion-blur and appearance motions. For the data split,
    60 videos are used for training and 90 videos for testing.'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DAVIS数据集：一个密集注释的视频分割数据集，包含150个具有挑战性的运动模糊和外观运动的视频。数据划分为60个视频用于训练，90个视频用于测试。
- en: •
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'YouTube-VOS dataset: A large-scale video object segmentation dataset containing
    4,453 video clips and 94 object categories. The video clips have on average 150
    frames and show various scenes. The original data split, i.e., 3,471/474/508,
    is adopted for experimental comparisons.'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: YouTube-VOS数据集：一个大规模的视频对象分割数据集，包含4,453个视频剪辑和94个对象类别。视频剪辑平均有150帧，展示了各种场景。采用原始数据划分，即3,471/474/508，用于实验比较。
- en: 3.4 Evaluation Protocol
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 评估协议
- en: Video contains many image frames, therefore, the two most widely-used metrics
    in image inpainting (i.e., PSNR and SSIM) are also used for video quality assessment.
    In addition, there are two other video-specific metrics (considering the temporal
    aspect), i.e., flow warping error (FWE) Lai et al. ([2018](#bib.bib95)) and video-based
    Fréchet inception distance (VFID) Wang et al. ([2018a](#bib.bib200)). The former
    evaluates the temporal stability of inpainted videos and the latter measures the
    perceptual realism in the video setting.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 视频包含许多图像帧，因此，图像修复中最常用的两个指标（即PSNR和SSIM）也用于视频质量评估。此外，还有两个其他视频特定的指标（考虑时间因素），即流动变形误差（FWE）
    Lai et al. ([2018](#bib.bib95)) 和基于视频的Fréchet生成距离（VFID） Wang et al. ([2018a](#bib.bib200))。前者评估修复视频的时间稳定性，后者衡量视频环境中的感知真实性。
- en: •
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FWE: The flow warping error between two consecutive video frames is calculated
    as $\mathcal{E}(\mathbf{I}_{t},\mathbf{I}_{t+1})=\frac{1}{\sum_{n=1}^{N}\mathbf{M}_{t}^{n}}\sum_{n=1}^{N}\mathbf{M}_{t}^{n}||\mathbf{I}_{t}^{n}-\hat{\mathbf{I}}_{t+1}^{n}||^{2}_{2}$,
    where $\mathbf{M}_{t}$ is a binary mask indicating non-occluded areas and $\hat{\mathbf{I}}_{t+1}$
    is the warped frame of $\mathbf{I}_{t+1}$. The non-occlusion mask can be estimated
    by using the method Ruder et al. ([2016](#bib.bib164)). Then, the warping error
    of a video is defined as the average error over the entire frames, and the formulation
    is $\mathcal{E}=\frac{1}{T-1}\sum_{t=1}^{T-1}\mathcal{E}(\mathbf{I}_{t},\mathbf{I}_{t+1})$.'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FWE：两个连续视频帧之间的光流变形误差计算为 $\mathcal{E}(\mathbf{I}_{t},\mathbf{I}_{t+1})=\frac{1}{\sum_{n=1}^{N}\mathbf{M}_{t}^{n}}\sum_{n=1}^{N}\mathbf{M}_{t}^{n}||\mathbf{I}_{t}^{n}-\hat{\mathbf{I}}_{t+1}^{n}||^{2}_{2}$，其中
    $\mathbf{M}_{t}$ 是一个二值掩膜，指示未遮挡区域，$\hat{\mathbf{I}}_{t+1}$ 是 $\mathbf{I}_{t+1}$
    的变形帧。未遮挡掩膜可以通过使用 Ruder 等人（[2016](#bib.bib164)）的方法来估计。然后，视频的变形误差定义为所有帧的平均误差，其公式为
    $\mathcal{E}=\frac{1}{T-1}\sum_{t=1}^{T-1}\mathcal{E}(\mathbf{I}_{t},\mathbf{I}_{t+1})$。
- en: •
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VFID: A variant of FID for video evaluation. Instead of using a pre-trained
    image recognition network, the spatiotemporal feature map of each video is extracted
    via a pre-trained video recognition network, e.g., I3D Carreira and Zisserman
    ([2017](#bib.bib14)). Then, the VFID is calculated following the same procedure
    as the FID.'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VFID：一种用于视频评估的 FID 变体。与使用预训练图像识别网络不同，VFID 通过预训练的视频识别网络（例如 I3D Carreira 和 Zisserman（[2017](#bib.bib14)））提取每个视频的时空特征图。然后，VFID
    按照与 FID 相同的程序计算。
- en: 3.5 Performance Evaluation
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 性能评估
- en: In this section, we report the performance evaluation of representative video
    inpainting methods.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们报告了代表性视频修复方法的性能评估。
- en: 'Table [6](#S3.T6 "Table 6 ‣ 3.1.4 Attention-based Approaches ‣ 3.1 Method ‣
    3 Video Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey")
    shows the numerical results on YouTube-VOS and DAVIS datasets. We use the evaluated
    masks shared by (Liu et al., [2021b](#bib.bib126)). Early video inpainting methods
    based on 3D convolution (e.g., VINet (Kim et al., [2019b](#bib.bib91))) and shift
    (e.g., LGTSM (Chang et al., [2019b](#bib.bib16))) have relatively limited inpainting
    performance. After introducing optical flow and attention mechanisms, the quality
    of video inpainting is remarkably improved. DFVI (Xu et al., [2019](#bib.bib222))
    generates the baseline result with flow guidance, and FGVC (Gao et al., [2020](#bib.bib49))
    achieves better performance by completing flow with sharp edges and propagating
    information from distant frames. ISVI (Zhang et al., [2022b](#bib.bib251)) obtains
    more exact flow completion under the inertia prior, and thus enhances the inpainting
    quality. STTN (Zeng et al., [2020a](#bib.bib241)) and FuseFormer (Liu et al.,
    [2021b](#bib.bib126)) both design video inpainting frameworks through stacking
    multiple transformer blocks with multi-scale attention and dense patch-wise attention,
    respectively. FGT (Zhang et al., [2022a](#bib.bib250)) and E2FGVI (Li et al.,
    [2022d](#bib.bib113)) combine the flow completion and transformer as a whole,
    and the end-to-end pipeline as adopted by E2FGVI is slightly better.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [6](#S3.T6 "Table 6 ‣ 3.1.4 Attention-based Approaches ‣ 3.1 Method ‣ 3 Video
    Inpainting ‣ Deep Learning-based Image and Video Inpainting: A Survey") 显示了 YouTube-VOS
    和 DAVIS 数据集上的数值结果。我们使用了由 (Liu 等人，[2021b](#bib.bib126)) 共享的评估掩膜。早期基于 3D 卷积（例如 VINet
    (Kim 等人，[2019b](#bib.bib91))）和移位（例如 LGTSM (Chang 等人，[2019b](#bib.bib16))）的视频修复方法性能相对有限。引入光流和注意机制后，视频修复的质量显著提高。DFVI
    (Xu 等人，[2019](#bib.bib222)) 在光流指导下生成基线结果，FGVC (Gao 等人，[2020](#bib.bib49)) 通过完成光流的锐利边缘和传播远距离帧的信息获得更好的性能。ISVI
    (Zhang 等人，[2022b](#bib.bib251)) 在惯性先验下获得了更准确的光流完成，从而提升了修复质量。STTN (Zeng 等人，[2020a](#bib.bib241))
    和 FuseFormer (Liu 等人，[2021b](#bib.bib126)) 通过堆叠多个变换器块设计了视频修复框架，分别采用了多尺度注意力和密集的块级注意力。FGT
    (Zhang 等人，[2022a](#bib.bib250)) 和 E2FGVI (Li 等人，[2022d](#bib.bib113)) 将光流完成和变换器结合为一个整体，其中
    E2FGVI 采用的端到端管道略好。'
- en: 'Fig. [15](#S3.F15 "Figure 15 ‣ 3.2 Loss Functions ‣ 3 Video Inpainting ‣ Deep
    Learning-based Image and Video Inpainting: A Survey") illustrates some inpainted
    results with different types of scenes and masks. From the first and second columns,
    we find that the flow-based pixel propagation methods, including FGVC, FGT, and
    ISVI, have a good ability to recover the texture details and objects with the
    guidance of neighboring frames. Through contextual correlation modeling, transformer-based
    video inpainting methods, such as STTN, FuseFormer, and E2FGVI, can complete the
    structure of objects, e.g., the window of a bus in the third column. Compared
    to STTN, FuseFormer introduces more dense attention computation (with overlapping),
    which can help the global structure recovery, e.g., the trunk in the fourth column
    and the post in the last column. In the fourth column, the coverage area is better
    filled with the realistic grass texture by the ISVI method, which is attributed
    to the more accurate flow completion compared to FGVC and FGT.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 图[15](#S3.F15 "图15 ‣ 3.2 损失函数 ‣ 3 视频修补 ‣ 基于深度学习的图像和视频修补：综述")展示了具有不同类型场景和掩模的一些修补结果。从第一列和第二列可以看出，包括FGVC、FGT和ISVI在内的基于流的像素传播方法具有良好的恢复纹理细节和物体的能力，通过相邻帧的指导。通过上下文相关建模，基于变换器的视频修补方法，如STTN、FuseFormer和E2FGVI，可以完成物体的结构，例如第三列中的公交车窗。与STTN相比，FuseFormer引入了更多密集的注意力计算（带有重叠），这有助于全局结构恢复，例如第四列中的树干和最后一列中的柱子。在第四列中，ISVI方法通过更准确的流动完成，相比于FGVC和FGT，更好地填补了现实的草地纹理。
- en: '![Refer to caption](img/98f3ab9bb70f5d62a2ba0ebcf4da6888.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/98f3ab9bb70f5d62a2ba0ebcf4da6888.png)'
- en: 'Figure 16: Several representative examples of blind video decaptioning produced
    by (Chu et al., [2021](#bib.bib23)).'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：由(Chu等，[2021](#bib.bib23))制作的几个盲视频去字幕的代表性例子。
- en: 3.6 Applications
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 应用
- en: 3.6.1 Blind Video Decaptioning
  id: totrans-398
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.1 盲视频去字幕
- en: 'Blind video decaptioning aims to automatically remove subscripts and recover
    the occluded regions in videos without mask information. Kim et al. ([2019a](#bib.bib90))
    designed an encoder-decoder framework based on 3D convolution. They applied residual
    learning to directly touch the corrupted regions and leveraged feedback connections
    to enforce temporal coherence with the warping loss. However, this method often
    suffers from the problem of incomplete subtitle removal. Chu et al. ([2021](#bib.bib23))
    proposed a two-stage video decaptioning network including a mask extraction module
    and a frame attention-based decaptioning module. Several examples produced by (Chu
    et al., [2021](#bib.bib23)) are shown in Fig. [16](#S3.F16 "Figure 16 ‣ 3.5 Performance
    Evaluation ‣ 3 Video Inpainting ‣ Deep Learning-based Image and Video Inpainting:
    A Survey"). The regions originally covered by subtitles are filled with plausible
    content.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 盲视频去字幕的目标是自动移除字幕并恢复视频中被遮挡的区域，而无需掩模信息。Kim等([2019a](#bib.bib90))设计了一个基于3D卷积的编码器-解码器框架。他们应用残差学习直接触及损坏区域，并利用反馈连接通过扭曲损失来强制时间一致性。然而，该方法经常面临字幕移除不完全的问题。Chu等([2021](#bib.bib23))提出了一个两阶段的视频去字幕网络，包括一个掩模提取模块和一个基于帧注意力的去字幕模块。由(Chu等，[2021](#bib.bib23))制作的几个例子如图[16](#S3.F16
    "图16 ‣ 3.5 性能评估 ‣ 3 视频修补 ‣ 基于深度学习的图像和视频修补：综述")所示，原本被字幕覆盖的区域被填充了合理的内容。
- en: 3.6.2 Dynamic Object Removal
  id: totrans-400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.2 动态物体移除
- en: 'A common practical application of video inpainting technology is to automatically
    remove undesired objects, which are static or dynamic at the time of recording.
    In this part, we show two examples of dynamic object removal with the recent video
    inpainting methods (Liu et al., [2021b](#bib.bib126); Ren et al., [2022](#bib.bib156);
    Kang et al., [2022](#bib.bib85)). As shown in Fig. [17](#S3.F17 "Figure 17 ‣ 3.6.2
    Dynamic Object Removal ‣ 3.6 Applications ‣ 3 Video Inpainting ‣ Deep Learning-based
    Image and Video Inpainting: A Survey"), the regions covered by dynamic objects
    can be filled with plausible content.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 视频修补技术的一个常见实际应用是自动移除录制时的静态或动态物体。在这一部分，我们展示了使用最近的视频修补方法进行动态物体移除的两个例子(Liu等，[2021b](#bib.bib126);
    Ren等，[2022](#bib.bib156); Kang等，[2022](#bib.bib85))。如图[17](#S3.F17 "图17 ‣ 3.6.2
    动态物体移除 ‣ 3.6 应用 ‣ 3 视频修补 ‣ 基于深度学习的图像和视频修补：综述")所示，动态物体覆盖的区域可以填充合理的内容。
- en: '![Refer to caption](img/3bb6105145e47ce54fc3d93ef860c259.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3bb6105145e47ce54fc3d93ef860c259.png)'
- en: 'Figure 17: Three examples of dynamic object removal produced by FuseFormer (Liu
    et al., [2021b](#bib.bib126)), DlFormer (Ren et al., [2022](#bib.bib156)), and
    ECFVI (Kang et al., [2022](#bib.bib85)).'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：由FuseFormer（Liu等，[2021b](#bib.bib126)），DlFormer（Ren等，[2022](#bib.bib156)），和ECFVI（Kang等，[2022](#bib.bib85)）生成的三种动态对象移除示例。
- en: 4 Future Work
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 未来工作
- en: Image and video inpainting essentially is a conditional generative task, therefore,
    the common generative models, such as VAE and GAN, are often adopted by the existing
    inpainting methods. Currently, diffusion models have become the most popular generative
    models with powerful capability of content synthesis. DMs would have the potential
    to improve the performance of image and video inpainting and may attract a lot
    of research effort in the future. For this promising direction, several challenging
    problems need to be solved.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和视频修复本质上是一种条件生成任务，因此，现有修复方法通常采用通用生成模型，如VAE和GAN。目前，扩散模型已成为最受欢迎的生成模型，具有强大的内容合成能力。扩散模型有潜力提升图像和视频修复的性能，并可能在未来吸引大量研究工作。对于这一有前景的方向，还需要解决几个挑战性问题。
- en: How to use large pre-trained diffusion models (e.g., denoising diffusion) for
    image inpainting?
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用大型预训练的扩散模型（例如，去噪扩散模型）进行图像修复？
- en: DMs synthesize an image by a sequential application of denoising steps, which
    are conducted in pixel or latent space. For the inpainting task, the core idea
    is to fill in the missing regions while preserving the originally valid content.
    Some researchers have made preliminary attempts, such as Palette (Saharia et al.,
    [2022a](#bib.bib167)), Blended Diffusion (Avrahami et al., [2022](#bib.bib3)),
    and ControlNet (Zhang and Agrawala, [2023](#bib.bib252)), etc. One research challenge
    is how to inject conditioning information into the denoising processes of large
    pre-trained diffusion models. Following the pipeline of diffusion models, they
    need many iterations to generate the final image and thus require a longer inference
    time compared to existing VAE- and GAN-based approaches. Another research challenge
    is to implement fast inpainting methods based on diffusion. Also, while video-based
    generative diffusion models are still in their infancy, it is expected that large
    pre-trained video generation models will become available in the near future.
    Leveraging these models for video inpainting will be an interesting task once
    these models become available.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型通过逐步应用去噪步骤来合成图像，这些步骤在像素或潜在空间中进行。对于修复任务，核心思想是填补缺失区域，同时保留原本有效的内容。一些研究人员已进行了初步尝试，如Palette（Saharia等，[2022a](#bib.bib167)），Blended
    Diffusion（Avrahami等，[2022](#bib.bib3)），和ControlNet（Zhang和Agrawala，[2023](#bib.bib252)）等。一个研究挑战是如何将条件信息注入到大型预训练扩散模型的去噪过程中。按照扩散模型的流程，它们需要多次迭代才能生成最终图像，因此相比于现有的VAE和GAN方法，它们需要更长的推理时间。另一个研究挑战是实现基于扩散的快速修复方法。此外，虽然基于视频的生成扩散模型仍处于起步阶段，但预计不久后大型预训练的视频生成模型将会问世。一旦这些模型可用，利用它们进行视频修复将是一项有趣的任务。
- en: How to use large pre-trained models for joint text and image embedding (e.g.,
    the latest CLIP style architecture) for image inpainting?
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用大型预训练模型进行联合文本和图像嵌入（例如，最新的CLIP风格架构）来进行图像修复？
- en: Mainstream inpainting methods are uncontrollable, where the inpainted content
    is unknown in advance and sometimes this is undesired for users. Reference-based
    inpainting cannot fully satisfy this requirement. On the other hand, recent studies (Rombach
    et al., [2022](#bib.bib161); Hertz et al., [2022](#bib.bib64); Parmar et al.,
    [2023](#bib.bib147)) have shown that large pre-trained diffusion models with massive
    text-image pairs can synthesize high-quality images with rich low-level attributes
    and details. In addition,  Zhao et al. ([2023](#bib.bib264)) implied that such
    pre-trained DMs also contain high-level visual concepts. As a result, text-guided
    inpainting based on the large pre-trained text-to-image diffusion models would
    be able to fill the content under the control of users. The first problem is to
    design the appropriate prompt exactly indicating the user’s intention. It is also
    challenging to merge the image embedding from the user prompt with the corresponding
    embedding of the input corrupted image. In addition, text-based video inpainting
    will be a great avenue for future work.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 主流的修复方法是不可控的，其中修复的内容事先未知，有时这对用户来说是不期望的。基于参考的修复方法不能完全满足这一要求。另一方面，最近的研究（Rombach
    等，[2022](#bib.bib161)；Hertz 等，[2022](#bib.bib64)；Parmar 等，[2023](#bib.bib147)）表明，大型预训练的扩散模型与大量的文本-图像对结合可以合成具有丰富低级属性和细节的高质量图像。此外，赵等（[2023](#bib.bib264)）暗示这种预训练的扩散模型也包含高级视觉概念。因此，基于大型预训练的文本到图像扩散模型的文本引导修复将能够在用户控制下填充内容。第一个问题是设计合适的提示，准确指示用户的意图。同时，将用户提示中的图像嵌入与输入受损图像的相应嵌入合并也是一个挑战。此外，基于文本的视频修复将是未来研究的一个重要方向。
- en: How to scale up training to datasets of 5B images (e.g. LAION)? Deep learning
    models are hungry for training datasets. Currently, advanced diffusion models
    are pre-trained on large-scale datasets containing millions or even billions of
    text-image data pairs. However, these models are mainly dominated by several industrial
    research labs, where the datasets and training processes are not transparent to
    the research community. Very recently, the largest text-image dataset LAION-5B (Schuhmann
    et al., [2022](#bib.bib170)) containing  5.8 billion samples is publicly available.
    In future work, it is worth designing efficient methods for image and video inpainting
    that are trained on such very large datasets directly.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将训练规模扩展到50亿张图像的数据集（例如 LAION）？深度学习模型对训练数据集有很高的需求。目前，先进的扩散模型是在包含数百万甚至数十亿个文本-图像数据对的大规模数据集上预训练的。然而，这些模型主要由几个工业研究实验室主导，这些实验室的数据集和训练过程对研究界并不透明。最近，最大的文本-图像数据集
    LAION-5B（Schuhmann 等，[2022](#bib.bib170)）包含58亿个样本，已公开可用。在未来的工作中，值得设计高效的方法，直接在如此大规模的数据集上进行图像和视频修复训练。
- en: How to utilize image data and pre-trained image inpainting models to improve
    the models of video inpainting? In addition to considering the spatial aspect
    as in image inpainting, video inpainting also needs to consider the temporal aspect.
    Therefore, it is important and beneficial to transfer the inpainting ability from
    image to video. A simple and direct solution is to take the result of image inpainting
    on each frame as the initialization and then revise the spatial and temporal consistency
    via carefully designed deep models. Another possible research line is to take
    the well-trained image inpainting models as the backbone and aggregate the multiple
    frames in the feature space with appropriate modules, such as deformable convolution
    or attention. It’s still worth exploring combining the pre-trained image inpainting
    model with deep video prior.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 如何利用图像数据和预训练的图像修复模型来改进视频修复模型？除了考虑图像修复中的空间方面，视频修复还需要考虑时间方面。因此，将修复能力从图像迁移到视频是重要且有益的。一个简单而直接的解决方案是将每帧图像修复的结果作为初始化，然后通过精心设计的深度模型修正空间和时间一致性。另一个可能的研究方向是以训练良好的图像修复模型为基础，在特征空间中聚合多个帧，并使用适当的模块，如可变形卷积或注意力机制。将预训练的图像修复模型与深度视频先验结合仍然值得探索。
- en: How to create a large video dataset of 5B videos and leverage it for video inpainting?
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如何创建一个包含50亿视频的大型视频数据集，并利用它进行视频修复？
- en: 'Like image inpainting, taking advantage of large pre-trained text-video diffusion
    models may be a new research direction for video inpainting. However, current
    text-video DMs are trained on datasets with  10 million captioned videos, which
    inevitably limits the generation and generalization ability of DMs. One potential
    direction of future research is to collect large-scale text-video datasets (e.g.,
    5B pairs) and design the pre-training methods scaling up to this amount. As we
    all know, video inpainting is more difficult compared to its image counterpart.
    Therefore, it is valuable to spend time on all aspects of large video datasets:
    building large publicly available video datasets, generating large diffusion methods
    for video synthesis and using these pre-trained methods for video inpainting,
    and separately designing and training large-scale video architectures directly.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像修复类似，利用大型预训练文本视频扩散模型可能是视频修复的一个新的研究方向。然而，当前的文本视频扩散模型是在拥有1000万字幕视频的数据集上进行训练的，这不可避免地限制了扩散模型的生成和泛化能力。未来研究的一个潜在方向是收集大规模的文本视频数据集（例如50亿对），并设计能够扩展到这个数量级别的预训练方法。众所周知，视频修复相对于图像修复更加困难。因此，值得花时间在大型视频数据集的各个方面上：建立大规模可公开获取的视频数据集，生成用于视频合成的大型扩散方法，并使用这些预训练方法进行视频修复，并单独地设计和训练直接面向大规模视频的架构。
- en: 5 Concluding Remarks
  id: totrans-414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 总结
- en: The prevalence of visual data, including images and video, promotes the development
    of related processing technologies, e.g., image and video inpainting. Due to their
    practical applications in many fields, these techniques have attracted great attention
    from both the industrial and research communities over the past decade. We presented
    a review of deep learning-based methods for image and video inpainting. Specifically,
    we outline different aspects of the research, including a taxonomy of existing
    methods, training objectives, benchmark datasets, evaluation protocols, performance
    evaluation, and real-world applications. Future research directions are also discussed.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉数据的普及，包括图像和视频，推动了相关处理技术的发展，例如图像和视频修复。由于这些技术在许多领域的实际应用中，过去十年来受到了工业和研究界的广泛关注。我们介绍了基于深度学习的图像和视频修复方法的综述。具体来说，我们概述了研究的不同方面，包括现有方法的分类、训练目标、基准数据集、评估协议、性能评估和实际应用。未来的研究方向也在讨论中。
- en: 'Although current deep learning-based inpainting approaches have achieved remarkable
    performance improvement, there are still several limitations: (1) Uncertainty
    of artifacts. The results generated by inpainting methods often exhibit visual
    artifacts, which are difficult to predict and prevent. There is almost no research
    work to systematically and comprehensively study these artifacts. (2) Specificity.
    Current inpainting models are usually trained on specific datasets, for example,
    face images or natural scene images. In other words, models trained on face images
    have bad predictions on natural scene images, and vice versa. Not enough models
    are trained on large scale datasets such as LIAON. (3) Large-scale inpainting.
    Current advanced inpainting methods still have limited performance on large-scale
    missing regions. Many methods are based on attention mechanisms, which are more
    fragile in large-scale scenarios. (4) High training costs. Current deep learning-based
    inpainting methods often need one or more weeks on multiple GPUs, which places
    very high demands on resource consumption. (5) Long inference time. Diffusion
    model-based methods can achieve better inpainting performance, however, they need
    a very long running time, which limits the application scope of inpainting techniques.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管当前基于深度学习的修复方法已经取得了显著的性能改进，但仍然存在几个限制：（1）伪影的不确定性。修复方法生成的结果经常出现视觉伪影，这些伪影难以预测和防止。几乎没有研究系统和全面地研究这些伪影。
    （2）特异性。目前的修复模型通常是在特定数据集上训练的，例如脸部图像或自然场景图像。换句话说，在脸部图像上训练的模型在自然场景图像上有糟糕的预测表现，反之亦然。目前还没有足够多的模型在类似LIAON这样的大规模数据集上进行训练。
    （3）大规模修复。目前的先进修复方法在大规模缺失区域上仍然性能有限。许多方法基于注意力机制，在大规模场景下更容易出现问题。 （4）高昂的训练成本。目前基于深度学习的修复方法通常需要在多块GPU上训练一周以上，这对资源消耗提出了非常高的要求。
    （5）长的推理时间。扩散模型的方法可以实现更好的修复性能，但需要非常长的运行时间，这限制了修复技术的应用范围。
- en: 'Deep image/video inpainting techniques have a wide range of real-world applications,
    however, they also raise potential ethical issues that need to be carefully considered
    and addressed: (1) Security risks. Inpainting-based visual data editing, e.g.,
    object removal, may maliciously be exploited, such as tampering with visual data
    or altering evidence. (2) Ownership and copyright. When there is no appropriate
    authorization, deep inpainting techniques used to manipulate and enhance images/videos
    could raise questions about ownership and copyright. The inpainting result may
    strongly resemble or be strongly inspired by copyrighted material. (3) Historical
    accuracy. Inpainting methods can be used for the restoration of old photos/films
    or artworks. This process could raise risks of inadvertently changing the initial
    creative intention or historical accuracy of the content, which requires careful
    verification by domain experts. (4) Bias. If not properly trained, an inpainting
    model may introduce bias or unfairness, especially when the training data is biased
    or unrepresentative. This has the potential to perpetuate social prejudices or
    inaccurately portray certain groups.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图像/视频修复技术具有广泛的现实应用，但它们也引发了需要仔细考虑和解决的潜在伦理问题：（1）安全风险。基于修复的视觉数据编辑，例如，物体移除，可能被恶意利用，如篡改视觉数据或更改证据。（2）所有权和版权。当没有适当授权时，深度修复技术用于操控和增强图像/视频可能会引发关于所有权和版权的问题。修复结果可能与受版权保护的材料非常相似或受到其强烈启发。（3）历史准确性。修复方法可用于修复旧照片/影片或艺术作品。此过程可能会无意中改变初始创作意图或历史准确性，这需要领域专家的仔细验证。（4）偏见。如果训练不充分，修复模型可能会引入偏见或不公平，尤其是当训练数据有偏或不具代表性时。这可能会延续社会偏见或不准确地描绘某些群体。
- en: References
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Arjovsky et al. (2017) Arjovsky M, Chintala S, Bottou L (2017) Wasserstein
    Generative Adversarial Networks. *In: Int. Conf. Mach. Learn.*, vol 70, pp 214–223'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky等人（2017）Arjovsky M, Chintala S, Bottou L（2017）Wasserstein生成对抗网络。*在：国际机器学习会议*，第70卷，第214–223页
- en: 'Austin et al. (2021) Austin J, Johnson DD, Ho J, Tarlow D, van den Berg R (2021)
    Structured Denoising Diffusion Models in Discrete State-Spaces. *In: Adv. Neural
    Inform. Process. Syst.*, vol 34, pp 17981–17993'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin等人（2021）Austin J, Johnson DD, Ho J, Tarlow D, van den Berg R（2021）离散状态空间中的结构化去噪扩散模型。*在：先进神经信息处理系统会议*，第34卷，第17981–17993页
- en: 'Avrahami et al. (2022) Avrahami O, Lischinski D, Fried O (2022) Blended Diffusion
    for Text-Driven Editing of Natural Images. *In: IEEE Conf. Comput. Vis. Pattern
    Recog.*, pp 18208–18218'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Avrahami等人（2022）Avrahami O, Lischinski D, Fried O（2022）用于文本驱动的自然图像编辑的混合扩散。*在：IEEE计算机视觉与模式识别会议*，第18208–18218页
- en: Ballester et al. (2001) Ballester C, Bertalmio M, Caselles V, Sapiro G, Verdera
    J (2001) Filling-in by joint interpolation of vector fields and gray levels. *IEEE
    Trans Image Process* 10(8):1200–1211
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ballester等人（2001）Ballester C, Bertalmio M, Caselles V, Sapiro G, Verdera J（2001）通过向量场和灰度级的联合插值进行填充。*IEEE图像处理汇刊*
    10(8)：1200–1211
- en: 'Baluja et al. (2019) Baluja S, Marwood D, Johnston N, Covell M (2019) Learning
    to Render Better Image Previews. *In: IEEE Int. Conf. Image Process.*, pp 1700–1704'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baluja等人（2019）Baluja S, Marwood D, Johnston N, Covell M（2019）学习渲染更好图像预览。*在：IEEE
    国际图像处理会议*，第1700–1704页
- en: 'Barnes et al. (2009) Barnes C, Shechtman E, Finkelstein A, Goldman DB (2009)
    PatchMatch: A randomized correspondence algorithm for structural image editing.
    *ACM Trans Graph* 28(3):24'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barnes等人（2009）Barnes C, Shechtman E, Finkelstein A, Goldman DB（2009）PatchMatch：一种用于结构图像编辑的随机对应算法。*ACM图形学汇刊*
    28(3)：24
- en: 'Bertalmio et al. (2000) Bertalmio M, Sapiro G, Caselles V, Ballester C (2000)
    Image inpainting. *In: Proc. ACM SIGGRAPH*, pp 417–424'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertalmio等人（2000）Bertalmio M, Sapiro G, Caselles V, Ballester C（2000）图像修复。*在：ACM
    SIGGRAPH会议论文集*，第417–424页
- en: Bian et al. (2022) Bian X, Wang C, Quan W, Ye J, Zhang X, Yan DM (2022) Scene
    text removal via cascaded text stroke detection and erasing. *Computational Visual
    Media* 8:273–287
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bian等人（2022）Bian X, Wang C, Quan W, Ye J, Zhang X, Yan DM（2022）通过级联文本笔画检测和擦除进行场景文本去除。*计算视觉媒体*
    8：273–287
- en: 'Blau and Michaeli (2018) Blau Y, Michaeli T (2018) The Perception-Distortion
    Tradeoff. *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 6228–6237'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blau和Michaeli（2018）Blau Y, Michaeli T（2018）感知-失真权衡。*在：IEEE计算机视觉与模式识别会议*，第6228–6237页
- en: Canny (1986) Canny J (1986) A computational approach to edge detection. *IEEE
    Trans Pattern Anal Mach Intell* (6):679–698
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Canny（1986）Canny J（1986）边缘检测的计算方法。*IEEE模式分析与机器智能汇刊*（6）：679–698
- en: 'Cao and Fu (2021) Cao C, Fu Y (2021) Learning a Sketch Tensor Space for Image
    Inpainting of Man-Made Scenes. *In: Int. Conf. Comput. Vis.*, pp 14509–14518'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao和Fu（2021）Cao C，Fu Y（2021）《学习用于人造场景图像修复的草图张量空间》。*见：国际计算机视觉会议*，第14509–14518页
- en: 'Cao et al. (2022) Cao C, Dong Q, Fu Y (2022) Learning Prior Feature and Attention
    Enhanced Image Inpainting. *In: Eur. Conf. Comput. Vis.*'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等（2022）Cao C，Dong Q，Fu Y（2022）《学习先验特征和注意力增强图像修复》。*见：欧洲计算机视觉会议*。
- en: Carlsson (1988) Carlsson S (1988) Sketch based coding of grey level images.
    *Sign Process* 15(1):57–83
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlsson（1988）Carlsson S（1988）《基于草图的灰度图像编码》。*信号处理* 15(1):57–83
- en: 'Carreira and Zisserman (2017) Carreira J, Zisserman A (2017) Quo Vadis, Action
    Recognition? A New Model and the Kinetics Dataset. *In: IEEE Conf. Comput. Vis.
    Pattern Recog.*, pp 4724–4733'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carreira和Zisserman（2017）Carreira J，Zisserman A（2017）《行动识别的未来？一种新模型和Kinetics数据集》。*见：IEEE计算机视觉与模式识别会议*，第4724–4733页
- en: 'Chang et al. (2019a) Chang YL, Liu ZY, Lee KY, Hsu W (2019a) Free-form video
    inpainting with 3d gated convolution and temporal patchgan. *In: Int. Conf. Comput.
    Vis.*, pp 9066–9075'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang等（2019a）Chang YL，Liu ZY，Lee KY，Hsu W（2019a）《利用3D门控卷积和时间PatchGAN的自由形式视频修复》。*见：国际计算机视觉会议*，第9066–9075页
- en: 'Chang et al. (2019b) Chang YL, Liu ZY, Lee KY, Hsu W (2019b) Learnable gated
    temporal shift module for deep video inpainting. *In: Brit. Mach. Vis. Conf.*'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang等（2019b）Chang YL，Liu ZY，Lee KY，Hsu W（2019b）《用于深度视频修复的可学习门控时间移位模块》。*见：英国机器视觉会议*。
- en: 'Chang et al. (2019c) Chang YL, Yu Liu Z, Hsu W (2019c) Vornet: Spatio-temporally
    consistent video inpainting for object removal. *In: IEEE Conf. Comput. Vis. Pattern
    Recog. Worksh.*, pp 1785–1794'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang等（2019c）Chang YL，Yu Liu Z，Hsu W（2019c）《Vornet：用于物体移除的时空一致视频修复》。*见：IEEE计算机视觉与模式识别会议研讨会*，第1785–1794页
- en: 'Chen et al. (2021) Chen C, Cai J, Hu Y, Tang X, Wang X, Yuan C, Bai X, Bai
    S (2021) Deep Interactive Video Inpainting: An Invisibility Cloak for Harry Potter.
    *In: ACM Int. Conf. Multimedia*, p 862–870'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021）陈C，蔡J，胡Y，唐X，王X，袁C，白X，白S（2021）《深度互动视频修复：哈利·波特的隐形斗篷》。*见：ACM国际多媒体会议*，第862–870页
- en: 'Chen et al. (2017) Chen L, Zhang H, Xiao J, Nie L, Shao J, Liu W, Chua TS (2017)
    SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image
    Captioning. *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 6298–6306'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2017）陈L，张H，肖J，聂L，邵J，刘W，蔡TS（2017）《SCA-CNN：卷积网络中的空间和通道注意力用于图像描述》。*见：IEEE计算机视觉与模式识别会议*，第6298–6306页
- en: 'Chen (2018) Chen P (2018) Video retouch: Object removal. [http://www.12371.cn/2021/02/08/ARTI1612745858192472.shtml](http://www.12371.cn/2021/02/08/ARTI1612745858192472.shtml)'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈（2018）陈P（2018）《视频修饰：物体移除》。 [http://www.12371.cn/2021/02/08/ARTI1612745858192472.shtml](http://www.12371.cn/2021/02/08/ARTI1612745858192472.shtml)
- en: 'Chen et al. (2019) Chen T, Lucic M, Houlsby N, Gelly S (2019) On Self Modulation
    for Generative Adversarial Networks. *In: Int. Conf. Learn. Represent.*'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019）陈T，Lucic M，Houlsby N，Gelly S（2019）《关于生成对抗网络的自我调节》。*见：国际学习表征会议*。
- en: 'Chi et al. (2020) Chi L, Jiang B, Mu Y (2020) Fast Fourier Convolution. *In:
    Adv. Neural Inform. Process. Syst.*, vol 33, pp 4479–4488'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chi等（2020）Chi L，Jiang B，Mu Y（2020）《快速傅里叶卷积》。*见：先进神经信息处理系统*，第33卷，第4479–4488页
- en: 'Chu et al. (2021) Chu P, Quan W, Wang T, Wang P, Ren P, Yan DM (2021) Deep
    video decaptioning. *In: Brit. Mach. Vis. Conf.*'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu等（2021）Chu P，Quan W，Wang T，Wang P，Ren P，Yan DM（2021）《深度视频解说》。*见：英国机器视觉会议*。
- en: 'Chung et al. (2022) Chung H, Sim B, Ye JC (2022) Come-Closer-Diffuse-Faster:
    Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic
    Contraction. *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 12403–12412'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung等（2022）Chung H，Sim B，Ye JC（2022）《更近-扩散-更快：通过随机收缩加速条件扩散模型以解决反问题》。*见：IEEE计算机视觉与模式识别会议*，第12403–12412页
- en: 'Cimpoi et al. (2014) Cimpoi M, Maji S, Kokkinos I, Mohamed S, Vedaldi A (2014)
    Describing Textures in the Wild. *In: IEEE Conf. Comput. Vis. Pattern Recog.*,
    pp 3606–3613'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cimpoi等（2014）Cimpoi M，Maji S，Kokkinos I，Mohamed S，Vedaldi A（2014）《描述野外纹理》。*见：IEEE计算机视觉与模式识别会议*，第3606–3613页
- en: Criminisi et al. (2004) Criminisi A, Perez P, Toyama K (2004) Region filling
    and object removal by exemplar-based image inpainting. *IEEE Trans Image Process*
    13(9):1200–1212
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Criminisi等（2004）Criminisi A，Perez P，Toyama K（2004）《基于样本的图像修复中的区域填充和物体移除》。*IEEE图像处理学报*
    13(9):1200–1212
- en: 'Croitoru et al. (2023) Croitoru FA, Hondru V, Ionescu RT, Shah M (2023) Diffusion
    models in vision: A survey. *IEEE Trans Pattern Anal Mach Intell* 45(9):10850–10869'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croitoru等（2023）Croitoru FA，Hondru V，Ionescu RT，Shah M（2023）《视觉中的扩散模型：综述》。*IEEE模式分析与机器智能学报*
    45(9):10850–10869
- en: Dai et al. (2020) Dai Q, Chopp H, Pouyet E, Cossairt O, Walton M, Katsaggelos
    AK (2020) Adaptive image sampling using deep learning and its application on x-ray
    fluorescence image reconstruction. *IEEE Trans Multimedia* 22(10):2564–2578
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai等人（2020）Dai Q, Chopp H, Pouyet E, Cossairt O, Walton M, Katsaggelos AK（2020）使用深度学习的自适应图像采样及其在X射线荧光图像重建中的应用。*IEEE
    Trans Multimedia* 22(10):2564–2578
- en: 'Darabi et al. (2012) Darabi S, Shechtman E, Barnes C, Goldman DB, Sen P (2012)
    Image Melding: combining inconsistent images using patch-based synthesis. *ACM
    Trans Graph (Proc SIGGRAPH)* 31(4):1–10'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Darabi等人（2012）Darabi S, Shechtman E, Barnes C, Goldman DB, Sen P（2012）图像融合：使用基于补丁的合成组合不一致的图像。*ACM
    Trans Graph (Proc SIGGRAPH)* 31(4):1–10
- en: Daubechies (1990) Daubechies I (1990) The wavelet transform, time-frequency
    localization and signal analysis. *IEEE Trans Inf Theory* 36(5):961–1005
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daubechies（1990）Daubechies I（1990）小波变换、时频定位和信号分析。*IEEE Trans Inf Theory* 36(5):961–1005
- en: 'Deng et al. (2009) Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009)
    Imagenet: A large-scale hierarchical image database. *In: IEEE Conf. Comput. Vis.
    Pattern Recog.*, pp 248–255'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng等人（2009）Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L（2009）Imagenet：一个大规模层次化图像数据库。*In:
    IEEE Conf. Comput. Vis. Pattern Recog.*, pp 248–255'
- en: 'Deng et al. (2020) Deng Y, Tang F, Dong W, Sun W, Huang F, Xu C (2020) Arbitrary
    Style Transfer via Multi-Adaptation Network. *In: ACM Int. Conf. Multimedia*,
    p 2719–2727'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng等人（2020）Deng Y, Tang F, Dong W, Sun W, Huang F, Xu C（2020）通过多适应网络进行任意风格迁移。*In:
    ACM Int. Conf. Multimedia*, p 2719–2727'
- en: 'Deng et al. (2021) Deng Y, Hui S, Zhou S, Meng D, Wang J (2021) Learning Contextual
    Transformer Network for Image Inpainting. *In: ACM Int. Conf. Multimedia*, p 2529–2538'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng等人（2021）Deng Y, Hui S, Zhou S, Meng D, Wang J（2021）学习上下文变换网络进行图像修复。*In:
    ACM Int. Conf. Multimedia*, p 2529–2538'
- en: 'Deng et al. (2022) Deng Y, Hui S, Meng R, Zhou S, Wang J (2022) Hourglass Attention
    Network for Image Inpainting. *In: Eur. Conf. Comput. Vis.*, pp 483–501'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng等人（2022）Deng Y, Hui S, Meng R, Zhou S, Wang J（2022）沙漏注意力网络用于图像修复。*In: Eur.
    Conf. Comput. Vis.*, pp 483–501'
- en: 'Dinh et al. (2014) Dinh L, Krueger D, Bengio Y (2014) Nice: Non-linear independent
    components estimation. *Int Conf Learn Represent Worksh*'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dinh等人（2014）Dinh L, Krueger D, Bengio Y（2014）Nice: 非线性独立成分估计。*Int Conf Learn
    Represent Worksh*'
- en: Doersch et al. (2012) Doersch C, Singh S, Gupta A, Sivic J, Efros AA (2012)
    What makes paris look like paris? *ACM Trans Graph* 31(4):101:1–101:9
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doersch等人（2012）Doersch C, Singh S, Gupta A, Sivic J, Efros AA（2012）是什么使巴黎看起来像巴黎？*ACM
    Trans Graph* 31(4):101:1–101:9
- en: 'Dolhansky and Ferrer (2018) Dolhansky B, Ferrer CC (2018) Eye In-painting with
    Exemplar Generative Adversarial Networks. *In: IEEE Conf. Comput. Vis. Pattern
    Recog.*, pp 7902–7911'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dolhansky和Ferrer（2018）Dolhansky B, Ferrer CC（2018）用示例生成对抗网络进行眼部修复。*In: IEEE
    Conf. Comput. Vis. Pattern Recog.*, pp 7902–7911'
- en: 'Dong et al. (2022) Dong Q, Cao C, Fu Y (2022) Incremental Transformer Structure
    Enhanced Image Inpainting With Masking Positional Encoding. *In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 11358–11368'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong等人（2022）Dong Q, Cao C, Fu Y（2022）增量式变换器结构增强图像修复与掩码位置编码。*In: IEEE Conf.
    Comput. Vis. Pattern Recog.*, pp 11358–11368'
- en: 'Dosovitskiy et al. (2021) Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn
    D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit
    J, Houlsby N (2021) An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale. *In: Int. Conf. Learn. Represent.*'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dosovitskiy等人（2021）Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai
    X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit J, Houlsby
    N（2021）图像价值16x16个单词：大规模图像识别的变换器。*In: Int. Conf. Learn. Represent.*'
- en: Dosselmann and Yang (2011) Dosselmann R, Yang XD (2011) A comprehensive assessment
    of the structural similarity index. *Sign Image and Video Process* 5:81–91
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosselmann和Yang（2011）Dosselmann R, Yang XD（2011）结构相似性指数的全面评估。*Sign Image and
    Video Process* 5:81–91
- en: 'Efros and Leung (1999) Efros A, Leung T (1999) Texture synthesis by non-parametric
    sampling. *In: Int. Conf. Comput. Vis.*, vol 2, pp 1033–1038'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Efros和Leung（1999）Efros A, Leung T（1999）通过非参数采样进行纹理合成。*In: Int. Conf. Comput.
    Vis.*, vol 2, pp 1033–1038'
- en: 'Elharrouss et al. (2020) Elharrouss O, Almaadeed N, Al-Maadeed S, Akbari Y
    (2020) Image Inpainting: A Review. *Neural Process Letters* 51(2):2007–2028'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elharrouss等人（2020）Elharrouss O, Almaadeed N, Al-Maadeed S, Akbari Y（2020）图像修复：综述。*Neural
    Process Letters* 51(2):2007–2028
- en: 'Esser et al. (2021) Esser P, Rombach R, Blattmann A, Ommer B (2021) ImageBART:
    Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis.
    *In: Adv. Neural Inform. Process. Syst.*, vol 34, pp 3518–3532'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Esser等人（2021）Esser P, Rombach R, Blattmann A, Ommer B（2021）ImageBART：用于自回归图像合成的双向上下文与多项式扩散。*In:
    Adv. Neural Inform. Process. Syst.*, vol 34, pp 3518–3532'
- en: 'Everingham et al. (2015) Everingham M, Eslami SMA, Gool LV, Williams CKI, Winn
    J, Zisserman A (2015) The pascal visual object classes challenge: A retrospective.
    *Int J Comput Vis* 111:98–136'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Everingham 等 (2015) Everingham M, Eslami SMA, Gool LV, Williams CKI, Winn J,
    Zisserman A (2015) Pascal 视觉目标类别挑战：回顾。 *Int J Comput Vis* 111:98–136
- en: Felzenszwalb and Huttenlocher (2004) Felzenszwalb PF, Huttenlocher DP (2004)
    Efficient graph-based image segmentation. *Int J Comput Vis* (59):167–181
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Felzenszwalb 和 Huttenlocher (2004) Felzenszwalb PF, Huttenlocher DP (2004) 高效的基于图的图像分割。
    *Int J Comput Vis* (59):167–181
- en: Feng et al. (2022) Feng X, Pei W, Li F, Chen F, Zhang D, Lu G (2022) Generative
    memory-guided semantic reasoning model for image inpainting. *IEEE Trans Circuit
    Syst Video Technol* 32(11):7432–7447
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等 (2022) Feng X, Pei W, Li F, Chen F, Zhang D, Lu G (2022) 基于生成记忆引导的语义推理模型用于图像修复。
    *IEEE Trans Circuit Syst Video Technol* 32(11):7432–7447
- en: 'Fu et al. (2019) Fu J, Liu J, Tian H, Li Y, Bao Y, Fang Z, Lu H (2019) Dual
    Attention Network for Scene Segmentation. *In: IEEE Conf. Comput. Vis. Pattern
    Recog.*, pp 3141–3149'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu 等 (2019) Fu J, Liu J, Tian H, Li Y, Bao Y, Fang Z, Lu H (2019) 场景分割的双重注意力网络。
    *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 3141–3149'
- en: Galić et al. (2008) Galić I, Weickert J, Welk M, Bruhn A, Belyaev A, Seidel
    HP (2008) Image compression with anisotropic diffusion. *J Math Imaging Vis* 31:255–269
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galić 等 (2008) Galić I, Weickert J, Welk M, Bruhn A, Belyaev A, Seidel HP (2008)
    基于各向异性扩散的图像压缩。 *J Math Imaging Vis* 31:255–269
- en: 'Gao et al. (2020) Gao C, Saraf A, Huang JB, Kopf J (2020) Flow-edge guided
    video completion. *In: Eur. Conf. Comput. Vis.*, pp 713–729'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等 (2020) Gao C, Saraf A, Huang JB, Kopf J (2020) 流边引导的视频完成。 *In: Eur. Conf.
    Comput. Vis.*, pp 713–729'
- en: 'Gatys et al. (2016) Gatys LA, Ecker AS, Bethge M (2016) Image Style Transfer
    Using Convolutional Neural Networks. *In: IEEE Conf. Comput. Vis. Pattern Recog.*,
    pp 2414–2423'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gatys 等 (2016) Gatys LA, Ecker AS, Bethge M (2016) 使用卷积神经网络的图像风格迁移。 *In: IEEE
    Conf. Comput. Vis. Pattern Recog.*, pp 2414–2423'
- en: 'Goodfellow et al. (2014) Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley
    D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial nets. *In: Adv.
    Neural Inform. Process. Syst.*, pp 2672–2680'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goodfellow 等 (2014) Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley
    D, Ozair S, Courville A, Bengio Y (2014) 生成对抗网络。 *In: Adv. Neural Inform. Process.
    Syst.*, pp 2672–2680'
- en: 'Granados et al. (2012) Granados M, Kim KI, Tompkin J, Kautz J, Theobalt C (2012)
    Background Inpainting for Videos with Dynamic Objects and a Free-Moving Camera.
    *In: Eur. Conf. Comput. Vis.*, pp 682–695'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Granados 等 (2012) Granados M, Kim KI, Tompkin J, Kautz J, Theobalt C (2012)
    动态物体和自由移动相机的视频背景修复。 *In: Eur. Conf. Comput. Vis.*, pp 682–695'
- en: 'Gu et al. (2020) Gu J, Shen Y, Zhou B (2020) Image Processing Using Multi-Code
    GAN Prior. *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 3009–3018'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu 等 (2020) Gu J, Shen Y, Zhou B (2020) 使用多码 GAN 先验的图像处理。 *In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 3009–3018'
- en: 'Guillemot and Meur (2014) Guillemot C, Meur OL (2014) Image inpainting: Overview
    and recent advances. *IEEE Sign Process Magazine* 31(1):127–144'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guillemot 和 Meur (2014) Guillemot C, Meur OL (2014) 图像修复：概述与最新进展。 *IEEE Sign
    Process Magazine* 31(1):127–144
- en: Guo et al. (2018) Guo Q, Gao S, Zhang X, Yin Y, Zhang C (2018) Patch-based image
    inpainting via two-stage low rank approximation. *IEEE Trans Vis Comput Graph*
    24(6):2023–2036
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2018) Guo Q, Gao S, Zhang X, Yin Y, Zhang C (2018) 基于块的图像修复通过两阶段低秩近似。
    *IEEE Trans Vis Comput Graph* 24(6):2023–2036
- en: 'Guo et al. (2021) Guo X, Yang H, Huang D (2021) Image Inpainting via Conditional
    Texture and Structure Dual Generation. *In: Int. Conf. Comput. Vis.*, pp 14134–14143'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等 (2021) Guo X, Yang H, Huang D (2021) 通过条件纹理和结构双重生成进行图像修复。 *In: Int. Conf.
    Comput. Vis.*, pp 14134–14143'
- en: 'Guo et al. (2019) Guo Z, Chen Z, Yu T, Chen J, Liu S (2019) Progressive Image
    Inpainting with Full-Resolution Residual Network. *In: ACM Int. Conf. Multimedia*,
    p 2496–2504'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等 (2019) Guo Z, Chen Z, Yu T, Chen J, Liu S (2019) 使用全分辨率残差网络的渐进图像修复。 *In:
    ACM Int. Conf. Multimedia*, p 2496–2504'
- en: Han and Wang (2021) Han C, Wang J (2021) Face image inpainting with evolutionary
    generators. *IEEE Sign Process Letters* 28:190–193
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 和 Wang (2021) Han C, Wang J (2021) 使用进化生成器的面部图像修复。 *IEEE Sign Process Letters*
    28:190–193
- en: 'Han et al. (2019) Han X, Wu Z, Huang W, Scott MR, Davis L (2019) FiNet: Compatible
    and Diverse Fashion Image Inpainting. *In: Int. Conf. Comput. Vis.*, pp 4480–4490'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han 等 (2019) Han X, Wu Z, Huang W, Scott MR, Davis L (2019) FiNet: 兼容且多样的时尚图像修复。
    *In: Int. Conf. Comput. Vis.*, pp 4480–4490'
- en: 'He and Sun (2012) He K, Sun J (2012) Statistics of Patch Offsets for Image
    Completion. *In: Eur. Conf. Comput. Vis.*, pp 16–29'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 和 Sun (2012) He K, Sun J (2012) 用于图像完成的块偏移统计。 *In: Eur. Conf. Comput. Vis.*,
    pp 16–29'
- en: 'He et al. (2016) He K, Zhang X, Ren S, Sun J (2016) Deep residual learning
    for image recognition. *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 770–778'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等 (2016) He K, Zhang X, Ren S, Sun J (2016) 用于图像识别的深度残差学习。 *In: IEEE Conf.
    Comput. Vis. Pattern Recog.*, pp 770–778'
- en: 'He et al. (2022) He K, Chen X, Xie S, Li Y, Dollár P, Girshick R (2022) Masked
    Autoencoders Are Scalable Vision Learners. *In: IEEE Conf. Comput. Vis. Pattern
    Recog.*, pp 16000–16009'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 (2022) He K, Chen X, Xie S, Li Y, Dollár P, Girshick R (2022) Masked Autoencoders
    Are Scalable Vision Learners. *在：IEEE 计算机视觉与模式识别会议*，第16000–16009页
- en: Herling and Broll (2014) Herling J, Broll W (2014) High-quality real-time video
    inpainting with pixmix. *IEEE Trans Vis Comput Graph* 20(6):866–879
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Herling 和 Broll (2014) Herling J, Broll W (2014) High-quality real-time video
    inpainting with pixmix. *IEEE 视觉计算图形学期刊* 20(6):866–879
- en: Hertz et al. (2022) Hertz A, Mokady R, Tenenbaum J, Aberman K, Pritch Y, Cohen-Or
    D (2022) Prompt-to-prompt image editing with cross attention control. *arXiv preprint
    arXiv:220801626*
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hertz 等人 (2022) Hertz A, Mokady R, Tenenbaum J, Aberman K, Pritch Y, Cohen-Or
    D (2022) Prompt-to-prompt image editing with cross attention control. *arXiv 预印本
    arXiv:220801626*
- en: 'Heusel et al. (2017) Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter
    S (2017) Gans trained by a two time-scale update rule converge to a local nash
    equilibrium. *In: Adv. Neural Inform. Process. Syst.*, pp 6626–6637'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heusel 等人 (2017) Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter
    S (2017) Gans trained by a two time-scale update rule converge to a local nash
    equilibrium. *在：先进的神经信息处理系统*，第6626–6637页
- en: 'Ho et al. (2020) Ho J, Jain A, Abbeel P (2020) Denoising Diffusion Probabilistic
    Models. *In: Adv. Neural Inform. Process. Syst.*, vol 33, pp 6840–6851'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等人 (2020) Ho J, Jain A, Abbeel P (2020) Denoising Diffusion Probabilistic
    Models. *在：先进的神经信息处理系统*，第33卷，第6840–6851页
- en: Hochreiter and Schmidhuber (1997) Hochreiter S, Schmidhuber J (1997) Long short-term
    memory. *Neural Comput* 9(8):1735–1780
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber (1997) Hochreiter S, Schmidhuber J (1997) Long short-term
    memory. *神经计算* 9(8):1735–1780
- en: 'Hong et al. (2019) Hong X, Xiong P, Ji R, Fan H (2019) Deep Fusion Network
    for Image Completion. *In: ACM Int. Conf. Multimedia*, pp 2033–2042'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等人 (2019) Hong X, Xiong P, Ji R, Fan H (2019) Deep Fusion Network for Image
    Completion. *在：ACM 国际多媒体会议*，第2033–2042页
- en: 'Hoogeboom et al. (2021) Hoogeboom E, Nielsen D, Jaini P, Forré P, Welling M
    (2021) Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions.
    *In: Adv. Neural Inform. Process. Syst.*, vol 34, pp 12454–12465'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hoogeboom 等人 (2021) Hoogeboom E, Nielsen D, Jaini P, Forré P, Welling M (2021)
    Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions. *在：先进的神经信息处理系统*，第34卷，第12454–12465页'
- en: 'Houle (2017a) Houle ME (2017a) Local Intrinsic Dimensionality I: An Extreme-Value-Theoretic
    Foundation for Similarity Applications. *In: Int. Conf. Similarity Search App.*,
    pp 64–79'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Houle (2017a) Houle ME (2017a) Local Intrinsic Dimensionality I: An Extreme-Value-Theoretic
    Foundation for Similarity Applications. *在：国际相似性搜索应用会议*，第64–79页'
- en: 'Houle (2017b) Houle ME (2017b) Local Intrinsic Dimensionality II: Multivariate
    Analysis and Distributional Support. *In: Int. Conf. Similarity Search App.*'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Houle (2017b) Houle ME (2017b) Local Intrinsic Dimensionality II: Multivariate
    Analysis and Distributional Support. *在：国际相似性搜索应用会议*'
- en: 'Hu et al. (2018) Hu J, Shen L, Sun G (2018) Squeeze-and-excitation networks.
    *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 7132–7141'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 (2018) Hu J, Shen L, Sun G (2018) Squeeze-and-excitation networks. *在：IEEE
    计算机视觉与模式识别会议*，第7132–7141页
- en: 'Hu et al. (2020) Hu YT, Wang H, Ballas N, Grauman K, Schwing AG (2020) Proposal-based
    video completion. *In: Eur. Conf. Comput. Vis.*, pp 38–54'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 (2020) Hu YT, Wang H, Ballas N, Grauman K, Schwing AG (2020) Proposal-based
    video completion. *在：欧洲计算机视觉会议*，第38–54页
- en: Huang et al. (2014) Huang JB, Kang SB, Ahuja N, Kopf J (2014) Image completion
    using planar structure guidance. *ACM Trans Graph (Proc SIGGRAPH)* 33(4):1–10
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 (2014) Huang JB, Kang SB, Ahuja N, Kopf J (2014) Image completion using
    planar structure guidance. *ACM 图形学期刊（SIGGRAPH 会议论文集）* 33(4):1–10
- en: Huang et al. (2016) Huang JB, Kang SB, Ahuja N, Kopf J (2016) Temporally coherent
    completion of dynamic video. *ACM Trans Graph* 35(6):1–11
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 (2016) Huang JB, Kang SB, Ahuja N, Kopf J (2016) Temporally coherent
    completion of dynamic video. *ACM 图形学期刊* 35(6):1–11
- en: 'Huang and Belongie (2017) Huang X, Belongie S (2017) Arbitrary Style Transfer
    in Real-Time with Adaptive Instance Normalization. *In: Int. Conf. Comput. Vis.*,
    pp 1510–1519'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 和 Belongie (2017) Huang X, Belongie S (2017) Arbitrary Style Transfer
    in Real-Time with Adaptive Instance Normalization. *在：国际计算机视觉会议*，第1510–1519页
- en: Hui et al. (2020) Hui Z, Li J, Wang X, Gao X (2020) Image fine-grained inpainting.
    *arXiv preprint arXiv:200202609*
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hui 等人 (2020) Hui Z, Li J, Wang X, Gao X (2020) Image fine-grained inpainting.
    *arXiv 预印本 arXiv:200202609*
- en: Iizuka et al. (2017) Iizuka S, Simo-Serra E, Ishikawa H (2017) Globally and
    locally consistent image completion. *ACM Trans Graph (Proc SIGGRAPH)* 36(4):1–14
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iizuka 等人 (2017) Iizuka S, Simo-Serra E, Ishikawa H (2017) Globally and locally
    consistent image completion. *ACM 图形学期刊（SIGGRAPH 会议论文集）* 36(4):1–14
- en: Ilan and Shamir (2015) Ilan S, Shamir A (2015) A survey on data-driven video
    completion. *Comput Graph Forum* 34(6):60–85
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilan 和 Shamir (2015) Ilan S, Shamir A (2015) A survey on data-driven video completion.
    *计算机图形学论坛* 34(6):60–85
- en: 'Ilg et al. (2017) Ilg E, Mayer N, Saikia T, Keuper M, Dosovitskiy A, Brox T
    (2017) FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks. *In:
    IEEE Conf. Comput. Vis. Pattern Recog.*, pp 1647–1655'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilg 等（2017）Ilg E, Mayer N, Saikia T, Keuper M, Dosovitskiy A, Brox T (2017)
    《FlowNet 2.0：深度网络的光流估计演变》。*见：IEEE 计算机视觉与模式识别会议*，第 1647–1655 页
- en: 'Isola et al. (2017) Isola P, Zhu JY, Zhou T, Efros AA (2017) Image-to-image
    translation with conditional adversarial networks. *In: IEEE Conf. Comput. Vis.
    Pattern Recog.*, pp 1125–1134'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isola 等（2017）Isola P, Zhu JY, Zhou T, Efros AA (2017) 《基于条件对抗网络的图像到图像翻译》。*见：IEEE
    计算机视觉与模式识别会议*，第 1125–1134 页
- en: Jam et al. (2021) Jam J, Kendrick C, Walker K, Drouard V, Hsu JGS, Yap MH (2021)
    A comprehensive review of past and present image inpainting methods. *Comput Vis
    Image Understand* 203:103147
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jam 等（2021）Jam J, Kendrick C, Walker K, Drouard V, Hsu JGS, Yap MH (2021) 《对过去和现在图像修复方法的全面回顾》。*计算机视觉与图像理解*
    203:103147
- en: 'Jiang et al. (2021) Jiang L, Dai B, Wu W, Loy CC (2021) Focal Frequency Loss
    for Image Reconstruction and Synthesis. *In: Int. Conf. Comput. Vis.*, pp 13899–13909'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2021）Jiang L, Dai B, Wu W, Loy CC (2021) 《图像重建与合成的焦点频率损失》。*见：国际计算机视觉会议*，第
    13899–13909 页
- en: 'Johnson et al. (2016) Johnson J, Alahi A, Fei-Fei L (2016) Perceptual losses
    for real-time style transfer and super-resolution. *In: Eur. Conf. Comput. Vis.*,
    pp 694–711'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等（2016）Johnson J, Alahi A, Fei-Fei L (2016) 《实时风格转移和超分辨率的感知损失》。*见：欧洲计算机视觉会议*，第
    694–711 页
- en: 'Kang et al. (2022) Kang J, Oh SW, Kim SJ (2022) Error Compensation Framework
    for Flow-Guided Video Inpainting. *In: Eur. Conf. Comput. Vis.*, pp 375–390'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 等（2022）Kang J, Oh SW, Kim SJ (2022) 《基于流的视频修复的误差补偿框架》。*见：欧洲计算机视觉会议*，第 375–390
    页
- en: 'Karras et al. (2018) Karras T, Aila T, Laine S, Lehtinen J (2018) Progressive
    Growing of GANs for Improved Quality, Stability, and Variation. *In: Int. Conf.
    Learn. Represent.*'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras 等（2018）Karras T, Aila T, Laine S, Lehtinen J (2018) 《逐步增长的生成对抗网络：提高质量、稳定性和变异性》。*见：国际学习表征会议*
- en: 'Karras et al. (2019) Karras T, Laine S, Aila T (2019) A Style-Based Generator
    Architecture for Generative Adversarial Networks. *In: IEEE Conf. Comput. Vis.
    Pattern Recog.*, pp 4396–4405'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras 等（2019）Karras T, Laine S, Aila T (2019) 《基于风格的生成对抗网络生成器架构》。*见：IEEE 计算机视觉与模式识别会议*，第
    4396–4405 页
- en: 'Karras et al. (2020) Karras T, Laine S, Aittala M, Hellsten J, Lehtinen J,
    Aila T (2020) Analyzing and Improving the Image Quality of StyleGAN. *In: IEEE
    Conf. Comput. Vis. Pattern Recog.*'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras 等（2020）Karras T, Laine S, Aittala M, Hellsten J, Lehtinen J, Aila T (2020)
    《分析和提高 StyleGAN 图像质量》。*见：IEEE 计算机视觉与模式识别会议*
- en: 'Ke et al. (2021) Ke L, Tai YW, Tang CK (2021) Occlusion-aware video object
    inpainting. *In: Int. Conf. Comput. Vis.*, pp 14468–14478'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke 等（2021）Ke L, Tai YW, Tang CK (2021) 《考虑遮挡的视频对象修复》。*见：国际计算机视觉会议*，第 14468–14478
    页
- en: 'Kim et al. (2019a) Kim D, Woo S, Lee JY, Kweon IS (2019a) Deep blind video
    decaptioning by temporal aggregation and recurrence. *In: IEEE Conf. Comput. Vis.
    Pattern Recog.*, pp 4263–4272'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2019a）Kim D, Woo S, Lee JY, Kweon IS (2019a) 《通过时间聚合和递归进行深度盲视频解说》。*见：IEEE
    计算机视觉与模式识别会议*，第 4263–4272 页
- en: 'Kim et al. (2019b) Kim D, Woo S, Lee JY, Kweon IS (2019b) Deep video inpainting.
    *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 5792–5801'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2019b）Kim D, Woo S, Lee JY, Kweon IS (2019b) 《深度视频修复》。*见：IEEE 计算机视觉与模式识别会议*，第
    5792–5801 页
- en: 'Kim et al. (2022) Kim SY, Aberman K, Kanazawa N, Garg R, Wadhwa N, Chang H,
    Karnad N, Kim M, Liba O (2022) Zoom-to-Inpaint: Image Inpainting With High-Frequency
    Details. *In: IEEE Conf. Comput. Vis. Pattern Recog. Worksh.*, pp 477–487'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2022）Kim SY, Aberman K, Kanazawa N, Garg R, Wadhwa N, Chang H, Karnad
    N, Kim M, Liba O (2022) 《缩放修复：具有高频细节的图像修复》。*见：IEEE 计算机视觉与模式识别会议研讨会*，第 477–487
    页
- en: 'Kingma and Dhariwal (2018) Kingma DP, Dhariwal P (2018) Glow: Generative Flow
    with Invertible 1x1 Convolutions. *In: Adv. Neural Inform. Process. Syst.*, vol 31'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Dhariwal（2018）Kingma DP, Dhariwal P (2018) 《Glow：具有可逆 1x1 卷积的生成流》。*见：先进神经信息处理系统会议*，第
    31 卷
- en: 'Kingma and Welling (2014) Kingma DP, Welling M (2014) Auto-Encoding Variational
    Bayes. *In: Int. Conf. Learn. Represent.*'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Welling（2014）Kingma DP, Welling M (2014) 《自编码变分贝叶斯》。*见：国际学习表征会议*
- en: 'Lai et al. (2018) Lai WS, Huang JB, Wang O, Shechtman E, Yumer E, Yang MH (2018)
    Learning Blind Video Temporal Consistency. *In: Eur. Conf. Comput. Vis.*, pp 179–195'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai 等（2018）Lai WS, Huang JB, Wang O, Shechtman E, Yumer E, Yang MH (2018) 《学习盲视频时间一致性》。*见：欧洲计算机视觉会议*，第
    179–195 页
- en: 'Lao et al. (2021) Lao D, Zhu P, Wonka P, Sundaramoorthi G (2021) Flow-Guided
    Video Inpainting with Scene Templates. *In: Int. Conf. Comput. Vis.*, pp 14599–14608'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lao 等（2021）Lao D, Zhu P, Wonka P, Sundaramoorthi G (2021) 《基于流的场景模板的视频修复》。*见：国际计算机视觉会议*，第
    14599–14608 页
- en: 'Ledig et al. (2017) Ledig C, Theis L, Huszár F, Caballero J, Cunningham A,
    Acosta A, Aitken A, Tejani A, Totz J, Wang Z, Shi W (2017) Photo-Realistic Single
    Image Super-Resolution Using a Generative Adversarial Network. *In: IEEE Conf.
    Comput. Vis. Pattern Recog.*, pp 105–114'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ledig et al. (2017) Ledig C, Theis L, Huszár F, Caballero J, Cunningham A,
    Acosta A, Aitken A, Tejani A, Totz J, Wang Z, Shi W (2017) 使用生成对抗网络的照片级单图像超分辨率。*见:
    IEEE Conf. Comput. Vis. Pattern Recog.*, 页 105–114'
- en: 'Lee et al. (2019) Lee S, Oh SW, Won D, Kim SJ (2019) Copy-and-paste networks
    for deep video inpainting. *In: Int. Conf. Comput. Vis.*, pp 4413–4421'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee et al. (2019) Lee S, Oh SW, Won D, Kim SJ (2019) 用于深度视频修复的复制粘贴网络。*见: Int.
    Conf. Comput. Vis.*, 页 4413–4421'
- en: 'Lempitsky et al. (2018) Lempitsky V, Vedaldi A, Ulyanov D (2018) Deep Image
    Prior. *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 9446–9454'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lempitsky et al. (2018) Lempitsky V, Vedaldi A, Ulyanov D (2018) 深度图像先验。*见:
    IEEE Conf. Comput. Vis. Pattern Recog.*, 页 9446–9454'
- en: 'Li et al. (2019a) Li A, Qi J, Zhang R, Ma X, Ramamohanarao K (2019a) Generative
    Image Inpainting with Submanifold Alignment. *In: Int. Joint Conf. Artificial
    Intell.*, pp 811–817'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2019a) Li A, Qi J, Zhang R, Ma X, Ramamohanarao K (2019a) 具有子流形对齐的生成图像修复。*见:
    Int. Joint Conf. Artificial Intell.*, 页 811–817'
- en: 'Li et al. (2020a) Li A, Zhao S, Ma X, Gong M, Qi J, Zhang R, Tao D, Kotagiri
    R (2020a) Short-term and long-term context aggregation network for video inpainting.
    *In: Eur. Conf. Comput. Vis.*, pp 728–743'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2020a) Li A, Zhao S, Ma X, Gong M, Qi J, Zhang R, Tao D, Kotagiri
    R (2020a) 视频修复的短期和长期上下文聚合网络。*见: Eur. Conf. Comput. Vis.*, 页 728–743'
- en: 'Li et al. (2023) Li A, Zhao L, Zuo Z, Wang Z, Xing W, Lu D (2023) Migt: Multi-modal
    image inpainting guided with text. *Neurocomputing* 520:376–385'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) Li A, Zhao L, Zuo Z, Wang Z, Xing W, Lu D (2023) Migt: 多模态图像修复与文本引导。*Neurocomputing*
    520:376–385'
- en: Li et al. (2021) Li B, Zheng B, Li H, Li Y (2021) Detail-enhanced image inpainting
    based on discrete wavelet transforms. *Sign Process* 189:108278
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021) Li B, Zheng B, Li H, Li Y (2021) 基于离散小波变换的细节增强图像修复。*Sign Process*
    189:108278
- en: 'Li et al. (2020b) Li CT, Siu WC, Liu ZS, Wang LW, Lun DPK (2020b) DeepGIN:
    Deep Generative Inpainting Network for Extreme Image Inpainting. *In: Eur. Conf.
    Comput. Vis. Worksh.*, pp 5–22'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2020b) Li CT, Siu WC, Liu ZS, Wang LW, Lun DPK (2020b) DeepGIN:
    极端图像修复的深度生成修复网络。*见: Eur. Conf. Comput. Vis. Worksh.*, 页 5–22'
- en: 'Li et al. (2022a) Li F, Li A, Qin J, Bai H, Lin W, Cong R, Zhao Y (2022a) Srinpaintor:
    When super-resolution meets transformer for image inpainting. *IEEE Trans Computational
    Imaging* 8:743–758'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2022a) Li F, Li A, Qin J, Bai H, Lin W, Cong R, Zhao Y (2022a) Srinpaintor:
    当超分辨率遇上变换器进行图像修复。*IEEE Trans Computational Imaging* 8:743–758'
- en: Li et al. (2019b) Li H, Li G, Lin L, Yu H, Yu Y (2019b) Context-aware semantic
    inpainting. *IEEE Trans Cybern* 49(12):4398–4411
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2019b) Li H, Li G, Lin L, Yu H, Yu Y (2019b) 上下文感知语义修复。*IEEE Trans
    Cybern* 49(12):4398–4411
- en: 'Li et al. (2019c) Li J, He F, Zhang L, Du B, Tao D (2019c) Progressive Reconstruction
    of Visual Structure for Image Inpainting. *In: Int. Conf. Comput. Vis.*, pp 5961–5970'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2019c) Li J, He F, Zhang L, Du B, Tao D (2019c) 图像修复的视觉结构渐进重建。*见:
    Int. Conf. Comput. Vis.*, 页 5961–5970'
- en: 'Li et al. (2020c) Li J, Wang N, Zhang L, Du B, Tao D (2020c) Recurrent Feature
    Reasoning for Image Inpainting. *In: IEEE Conf. Comput. Vis. Pattern Recog.*,
    pp 7757–7765'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2020c) Li J, Wang N, Zhang L, Du B, Tao D (2020c) 图像修复的递归特征推理。*见:
    IEEE Conf. Comput. Vis. Pattern Recog.*, 页 7757–7765'
- en: 'Li et al. (2022b) Li W, Lin Z, Zhou K, Qi L, Wang Y, Jia J (2022b) MAT: Mask-Aware
    Transformer for Large Hole Image Inpainting. *In: IEEE Conf. Comput. Vis. Pattern
    Recog.*'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2022b) Li W, Lin Z, Zhou K, Qi L, Wang Y, Jia J (2022b) MAT: 面具感知变换器用于大孔图像修复。*见:
    IEEE Conf. Comput. Vis. Pattern Recog.*'
- en: 'Li et al. (2022c) Li W, Yu X, Zhou K, Song Y, Lin Z, Jia J (2022c) Sdm: Spatial
    diffusion model for large hole image inpainting. *arXiv preprint arXiv:221202963*'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2022c) Li W, Yu X, Zhou K, Song Y, Lin Z, Jia J (2022c) Sdm: 用于大孔图像修复的空间扩散模型。*arXiv
    预印本 arXiv:221202963*'
- en: 'Li et al. (2017) Li Y, Liu S, Yang J, Yang MH (2017) Generative Face Completion.
    *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 5892–5900'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2017) Li Y, Liu S, Yang J, Yang MH (2017) 生成面部修复。*见: IEEE Conf.
    Comput. Vis. Pattern Recog.*, 页 5892–5900'
- en: 'Li et al. (2019d) Li Y, Jiang B, Lu Y, Shen L (2019d) Fine-grained Adversarial
    Image Inpainting with Super Resolution. *In: Int. Joint Conf. Neural Networks*,
    pp 1–8'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2019d) Li Y, Jiang B, Lu Y, Shen L (2019d) 细粒度对抗图像修复与超分辨率。*见: Int.
    Joint Conf. Neural Networks*, 页 1–8'
- en: 'Li et al. (2022d) Li Z, Lu CZ, Qin J, Guo CL, Cheng MM (2022d) Towards an end-to-end
    framework for flow-guided video inpainting. *In: IEEE Conf. Comput. Vis. Pattern
    Recog.*, pp 17562–17571'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2022d) Li Z, Lu CZ, Qin J, Guo CL, Cheng MM (2022d) 面向流引导视频修复的端到端框架。*见:
    IEEE Conf. Comput. Vis. Pattern Recog.*, 页 17562–17571'
- en: 'Liao et al. (2018a) Liao H, Funka-Lea G, Zheng Y, Luo J, Zhou SK (2018a) Face
    Completion with Semantic Knowledge and Collaborative Adversarial Learning. *In:
    Asian Conf. Comput. Vis.*, vol 11361, pp 382–397'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 廖等（2018a）廖H、冯卡-利亚G、郑Y、罗J、周SK（2018a）《带有语义知识和协作对抗学习的面部修复》。*见：亚洲计算机视觉会议*，卷11361，第382–397页
- en: 'Liao et al. (2018b) Liao L, Hu R, Xiao J, Wang Z (2018b) Edge-Aware Context
    Encoder for Image Inpainting. *In: Int. Conf. Acou. Speech Sign. Process.*, pp
    3156–3160'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 廖等（2018b）廖L、胡R、肖J、王Z（2018b）《边缘感知上下文编码器用于图像修复》。*见：国际声学、语音与信号处理会议*，第3156–3160页
- en: 'Liao et al. (2020) Liao L, Xiao J, Wang Z, Lin CW, Satoh S (2020) Guidance
    and Evaluation: Semantic-Aware Image Inpainting for Mixed Scenes. *In: Eur. Conf.
    Comput. Vis.*'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 廖等（2020）廖L、肖J、王Z、林CW、佐藤S（2020）《引导与评估：用于混合场景的语义感知图像修复》。*见：欧洲计算机视觉会议*
- en: 'Liao et al. (2021a) Liao L, Xiao J, Wang Z, Lin CW, Satoh S (2021a) Image Inpainting
    Guided by Coherence Priors of Semantics and Textures. *In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 6539–6548'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 廖等（2021a）廖L、肖J、王Z、林CW、佐藤S（2021a）《由语义和纹理一致性先验引导的图像修复》。*见：IEEE计算机视觉与模式识别会议*，第6539–6548页
- en: Liao et al. (2021b) Liao L, Xiao J, Wang Z, Lin CW, Satoh S (2021b) Uncertainty-aware
    semantic guidance and estimation for image inpainting. *IEEE J Selected Topics
    Sign Process* 15(2):310–323
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 廖等（2021b）廖L、肖J、王Z、林CW、佐藤S（2021b）《用于图像修复的误差感知语义引导和估计》。*IEEE选定主题信号处理杂志* 15(2)：310–323
- en: Lim and Ye (2017) Lim JH, Ye JC (2017) Geometric gan. *arXiv preprint arXiv:170502894*
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林和叶（2017）林JH、叶JC（2017）《几何GAN》。*arXiv预印本 arXiv:170502894*
- en: 'Lin et al. (2019) Lin J, Gan C, Han S (2019) Tsm: Temporal shift module for
    efficient video understanding. *In: Int. Conf. Comput. Vis.*, pp 7083–7093'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2019）林J、甘C、韩S（2019）《TSM：用于高效视频理解的时间移位模块》。*见：国际计算机视觉会议*，第7083–7093页
- en: 'Lin et al. (2020) Lin Q, Yan B, Li J, Tan W (2020) Mmfl: Multimodal fusion
    learning for text-guided image inpainting. *In: ACM Int. Conf. Multimedia*, pp
    1094–1102'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2020）林Q、颜B、李J、谭W（2020）《Mmfl：用于文本引导的图像修复的多模态融合学习》。*见：ACM国际多媒体会议*，第1094–1102页
- en: 'Liu et al. (2018) Liu G, Reda FA, Shih KJ, Wang TC, Tao A, Catanzaro B (2018)
    Image inpainting for irregular holes using partial convolutions. *In: Eur. Conf.
    Comput. Vis.*, pp 85–100'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2018）刘G、雷达FA、施KJ、王TC、陶A、卡坦扎罗B（2018）《使用部分卷积的非规则孔图像修复》。*见：欧洲计算机视觉会议*，第85–100页
- en: 'Liu et al. (2019) Liu H, Jiang B, Xiao Y, Yang C (2019) Coherent semantic attention
    for image inpainting. *In: Int. Conf. Comput. Vis.*, pp 4170–4179'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2019）刘H、江B、肖Y、杨C（2019）《用于图像修复的连贯语义注意力》。*见：国际计算机视觉会议*，第4170–4179页
- en: 'Liu et al. (2020) Liu H, Jiang B, Song Y, Huang W, Yang C (2020) Rethinking
    Image Inpainting via a Mutual Encoder-Decoder with Feature Equalizations. *In:
    Eur. Conf. Comput. Vis.*'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2020）刘H、江B、宋Y、黄W、杨C（2020）《通过互编码器-解码器与特征均衡重新思考图像修复》。*见：欧洲计算机视觉会议*
- en: 'Liu et al. (2021a) Liu H, Wan Z, Huang W, Song Y, Han X, Liao J (2021a) PD-GAN:
    Probabilistic Diverse GAN for Image Inpainting. *In: IEEE Conf. Comput. Vis. Pattern
    Recog.*, pp 9367–9376'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2021a）刘H、万Z、黄W、宋Y、韩X、廖J（2021a）《PD-GAN：用于图像修复的概率性多样化GAN》。*见：IEEE计算机视觉与模式识别会议*，第9367–9376页
- en: 'Liu et al. (2021b) Liu R, Deng H, Huang Y, Shi X, Lu L, Sun W, Wang X, Dai
    J, Li H (2021b) FuseFormer: Fusing Fine-Grained Information in Transformers for
    Video Inpainting. *In: Int. Conf. Comput. Vis.*, pp 14040–14049'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2021b）刘R、邓H、黄Y、石X、卢L、孙W、王X、戴J、李H（2021b）《FuseFormer：在Transformers中融合细粒度信息以进行视频修复》。*见：国际计算机视觉会议*，第14040–14049页
- en: 'Liu et al. (2022) Liu T, Liao L, Wang Z, Satoh S (2022) Reference-Guided Texture
    and Structure Inference for Image Inpainting. *In: IEEE Int. Conf. Image Process.*,
    pp 1996–2000'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2022）刘T、廖L、王Z、佐藤S（2022）《基于参考的图像修复纹理和结构推断》。*见：IEEE国际图像处理会议*，第1996–2000页
- en: Liu et al. (2021c) Liu W, Cao C, Liu J, Ren C, Wei Y, Guo H (2021c) Fine-grained
    image inpainting with scale-enhanced generative adversarial network. *Pattern
    Recog Letters* 143:81–87
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2021c）刘W、曹C、刘J、任C、魏Y、郭H（2021c）《具有尺度增强生成对抗网络的细粒度图像修复》。*模式识别通讯* 143：81–87
- en: 'Liu et al. (2015) Liu Z, Luo P, Wang X, Tang X (2015) Deep learning face attributes
    in the wild. *In: Int. Conf. Comput. Vis.*, pp 3730–3738'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2015）刘Z、罗P、王X、唐X（2015）《在野外深度学习面部属性》。*见：国际计算机视觉会议*，第3730–3738页
- en: 'Liu et al. (2021d) Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B
    (2021d) Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.
    *In: Int. Conf. Comput. Vis.*, pp 9992–10002'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2021d）刘Z、林Y、曹Y、胡H、魏Y、张Z、林S、郭B（2021d）《Swin Transformer：使用位移窗口的分层视觉Transformer》。*见：国际计算机视觉会议*，第9992–10002页
- en: 'Lu et al. (2022) Lu Z, Jiang J, Huang J, Wu G, Liu X (2022) GLaMa: Joint Spatial
    and Frequency Loss for General Image Inpainting. *In: IEEE Conf. Comput. Vis.
    Pattern Recog. Worksh.*, pp 1301–1310'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等 (2022) Lu Z, Jiang J, Huang J, Wu G, Liu X (2022) GLaMa: 用于一般图像修复的联合空间和频率损失。*见：IEEE
    计算机视觉与模式识别会议工作坊*，第 1301–1310 页'
- en: 'Lugmayr et al. (2020) Lugmayr A, Danelljan M, Van Gool L, Timofte R (2020)
    SRFlow: Learning the Super-Resolution Space with Normalizing Flow. *In: Eur. Conf.
    Comput. Vis.*, p 715–732'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lugmayr 等 (2020) Lugmayr A, Danelljan M, Van Gool L, Timofte R (2020) SRFlow:
    使用归一化流学习超分辨率空间。*见：欧洲计算机视觉会议*，第 715–732 页'
- en: 'Lugmayr et al. (2022) Lugmayr A, Danelljan M, Romero A, Yu F, Timofte R, Van Gool
    L (2022) RePaint: Inpainting using Denoising Diffusion Probabilistic Models. *In:
    IEEE Conf. Comput. Vis. Pattern Recog.*, pp 11451–11461'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lugmayr 等 (2022) Lugmayr A, Danelljan M, Romero A, Yu F, Timofte R, Van Gool
    L (2022) RePaint: 使用去噪扩散概率模型的修复。*见：IEEE 计算机视觉与模式识别会议*，第 11451–11461 页'
- en: 'Ma et al. (2019) Ma Y, Liu X, Bai S, Wang L, He D, Liu A (2019) Coarse-to-Fine
    Image Inpainting via Region-wise Convolutions and Non-Local Correlation. *In:
    Int. Joint Conf. Artificial Intell.*, pp 3123–3129'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等 (2019) Ma Y, Liu X, Bai S, Wang L, He D, Liu A (2019) 通过区域卷积和非局部相关的粗到细图像修复。*见：国际人工智能联合会议*，第
    3123–3129 页
- en: 'Mallat (1989) Mallat SG (1989) A theory for multiresolution signal decomposition:
    the wavelet representation. *IEEE Trans Pattern Anal Mach Intell* 11(7):674–693'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mallat (1989) Mallat SG (1989) 多分辨率信号分解理论：小波表示。*IEEE 模式分析与机器智能汇刊* 11(7):674–693
- en: 'Mao et al. (2017) Mao X, Li Q, Xie H, Lau RY, Wang Z, Paul Smolley S (2017)
    Least Squares Generative Adversarial Networks. *In: Int. Conf. Comput. Vis.*'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等 (2017) Mao X, Li Q, Xie H, Lau RY, Wang Z, Paul Smolley S (2017) 最小二乘生成对抗网络。*见：国际计算机视觉会议*
- en: 'Masnou and Morel (1998) Masnou S, Morel JM (1998) Level lines based disocclusion.
    *In: IEEE Int. Conf. Image Process.*, vol 3, pp 259–263'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Masnou 和 Morel (1998) Masnou S, Morel JM (1998) 基于水平线的遮挡去除。*见：IEEE 国际图像处理会议*，卷
    3，第 259–263 页
- en: 'Navasardyan and Ohanyan (2020) Navasardyan S, Ohanyan M (2020) Image Inpainting
    with Onion Convolutions. *In: Asian Conf. Comput. Vis.*'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Navasardyan 和 Ohanyan (2020) Navasardyan S, Ohanyan M (2020) 使用洋葱卷积的图像修复。*见：亚洲计算机视觉会议*
- en: 'Nazeri et al. (2019) Nazeri K, Ng E, Joseph T, Qureshi F, Ebrahimi M (2019)
    EdgeConnect: Structure Guided Image Inpainting using Edge Prediction. *In: Int.
    Conf. Comput. Vis. Worksh.*'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nazeri 等 (2019) Nazeri K, Ng E, Joseph T, Qureshi F, Ebrahimi M (2019) EdgeConnect:
    使用边缘预测的结构引导图像修复。*见：国际计算机视觉会议工作坊*'
- en: Newson et al. (2014) Newson A, Almansa A, Fradet M, Gousseau Y, Pérez P (2014)
    Video inpainting of complex scenes. *SIAM J Imaging Sciences* 7(4):1993–2019
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newson 等 (2014) Newson A, Almansa A, Fradet M, Gousseau Y, Pérez P (2014) 复杂场景的视频修复。*SIAM
    成像科学期刊* 7(4):1993–2019
- en: 'Ni et al. (2023) Ni M, Li X, Zuo W (2023) NÜWA-LIP: Language-guided Image Inpainting
    with Defect-free VQGAN. *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 14183–14192'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ni 等 (2023) Ni M, Li X, Zuo W (2023) NÜWA-LIP: 使用无缺陷 VQGAN 的语言引导图像修复。*见：IEEE
    计算机视觉与模式识别会议*，第 14183–14192 页'
- en: 'Oh et al. (2019) Oh SW, Lee S, Lee JY, Kim SJ (2019) Onion-peel networks for
    deep video completion. *In: Int. Conf. Comput. Vis.*, pp 4403–4412'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh 等 (2019) Oh SW, Lee S, Lee JY, Kim SJ (2019) 用于深度视频补全的洋葱剥离网络。*见：国际计算机视觉会议*，第
    4403–4412 页
- en: Ojala et al. (1996) Ojala T, Pietikäinen M, Harwood D (1996) A comparative study
    of texture measures with classification based on featured distributions. *Pattern
    Recog* 29(1):51–59
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ojala 等 (1996) Ojala T, Pietikäinen M, Harwood D (1996) 基于特征分布的纹理度量比较研究。*模式识别*
    29(1):51–59
- en: Ojala et al. (2002) Ojala T, Pietikainen M, Maenpaa T (2002) Multiresolution
    gray-scale and rotation invariant texture classification with local binary patterns.
    *IEEE Trans Pattern Anal Mach Intell* 24(7):971–987
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ojala 等 (2002) Ojala T, Pietikainen M, Maenpaa T (2002) 使用局部二进制模式的多分辨率灰度和旋转不变纹理分类。*IEEE
    模式分析与机器智能汇刊* 24(7):971–987
- en: 'Ouyang et al. (2021) Ouyang H, Wang T, Chen Q (2021) Internal video inpainting
    by implicit long-range propagation. *In: Int. Conf. Comput. Vis.*, pp 14579–14588'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等 (2021) Ouyang H, Wang T, Chen Q (2021) 通过隐式长程传播进行内部视频修复。*见：国际计算机视觉会议*，第
    14579–14588 页
- en: 'Park et al. (2019) Park T, Liu MY, Wang TC, Zhu JY (2019) Semantic Image Synthesis
    With Spatially-Adaptive Normalization. *In: IEEE Conf. Comput. Vis. Pattern Recog.*,
    pp 2332–2341'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 (2019) Park T, Liu MY, Wang TC, Zhu JY (2019) 具有空间自适应归一化的语义图像合成。*见：IEEE
    计算机视觉与模式识别会议*，第 2332–2341 页
- en: Parmar et al. (2023) Parmar G, Singh KK, Zhang R, Li Y, Lu J, Zhu JY (2023)
    Zero-shot image-to-image translation. *arXiv preprint arxiv230203027*
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parmar 等 (2023) Parmar G, Singh KK, Zhang R, Li Y, Lu J, Zhu JY (2023) Zero-shot
    图像到图像转换。*arXiv 预印本 arxiv230203027*
- en: 'Pathak et al. (2016) Pathak D, Krahenbuhl P, Donahue J, Darrell T, Efros AA
    (2016) Context encoders: Feature learning by inpainting. *In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 2536–2544'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pathak 等人 (2016) Pathak D, Krahenbuhl P, Donahue J, Darrell T, Efros AA (2016)
    上下文编码器：通过修复进行特征学习。*In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 2536–2544'
- en: 'Peng et al. (2021) Peng J, Liu D, Xu S, Li H (2021) Generating Diverse Structure
    for Image Inpainting With Hierarchical VQ-VAE. *In: IEEE Conf. Comput. Vis. Pattern
    Recog.*, pp 10770–10779'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等人 (2021) Peng J, Liu D, Xu S, Li H (2021) 使用层次化 VQ-VAE 为图像修复生成多样化结构。*In:
    IEEE Conf. Comput. Vis. Pattern Recog.*, pp 10770–10779'
- en: 'Perazzi et al. (2016) Perazzi F, Pont-Tuset J, McWilliams B, Van Gool L, Gross
    M, Sorkine-Hornung A (2016) A Benchmark Dataset and Evaluation Methodology for
    Video Object Segmentation. *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 724–732'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Perazzi 等人 (2016) Perazzi F, Pont-Tuset J, McWilliams B, Van Gool L, Gross
    M, Sorkine-Hornung A (2016) 视频目标分割的基准数据集和评估方法。*In: IEEE Conf. Comput. Vis. Pattern
    Recog.*, pp 724–732'
- en: Phutke and Murala (2021) Phutke SS, Murala S (2021) Diverse receptive field
    based adversarial concurrent encoder network for image inpainting. *IEEE Sign
    Process Letters* 28:1873–1877
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phutke 和 Murala (2021) Phutke SS, Murala S (2021) 基于多样化感受野的对抗并发编码网络用于图像修复。*IEEE
    Sign Process Letters* 28:1873–1877
- en: Qin et al. (2021) Qin J, Bai H, Zhao Y (2021) Multi-scale attention network
    for image inpainting. *Comput Vis Image Understand* 204:103155
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人 (2021) Qin J, Bai H, Zhao Y (2021) 多尺度注意力网络用于图像修复。*Comput Vis Image Understand*
    204:103155
- en: 'Qiu et al. (2021) Qiu J, Gao Y, Shen M (2021) Semantic-sca: Semantic structure
    image inpainting with the spatial-channel attention. *IEEE Access* 9:12997–13008'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiu 等人 (2021) Qiu J, Gao Y, Shen M (2021) Semantic-sca: 基于空间-通道注意力的语义结构图像修复。*IEEE
    Access* 9:12997–13008'
- en: Quan et al. (2022) Quan W, Zhang R, Zhang Y, Li Z, Wang J, Yan DM (2022) Image
    inpainting with local and global refinement. *IEEE Trans Image Process* 31:2405–2420
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quan 等人 (2022) Quan W, Zhang R, Zhang Y, Li Z, Wang J, Yan DM (2022) 具有局部和全局精细化的图像修复。*IEEE
    Trans Image Process* 31:2405–2420
- en: 'Radford et al. (2016) Radford A, Metz L, Chintala S (2016) Unsupervised Representation
    Learning with Deep Convolutional Generative Adversarial Networks. *In: Int. Conf.
    Learn. Represent.*'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Radford 等人 (2016) Radford A, Metz L, Chintala S (2016) 使用深度卷积生成对抗网络进行无监督表示学习。*In:
    Int. Conf. Learn. Represent.*'
- en: 'Ren et al. (2022) Ren J, Zheng Q, Zhao Y, Xu X, Li C (2022) DLFormer: Discrete
    Latent Transformer for Video Inpainting. *In: IEEE Conf. Comput. Vis. Pattern
    Recog.*, pp 3511–3520'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等人 (2022) Ren J, Zheng Q, Zhao Y, Xu X, Li C (2022) DLFormer: 离散潜在变换器用于视频修复。*In:
    IEEE Conf. Comput. Vis. Pattern Recog.*, pp 3511–3520'
- en: 'Ren et al. (2015) Ren JS, Xu L, Yan Q, Sun W (2015) Shepard Convolutional Neural
    Networks. *In: Adv. Neural Inform. Process. Syst.*, vol 28, p 901–909'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等人 (2015) Ren JS, Xu L, Yan Q, Sun W (2015) Shepard 卷积神经网络。*In: Adv. Neural
    Inform. Process. Syst.*, vol 28, p 901–909'
- en: 'Ren et al. (2019) Ren Y, Yu X, Zhang R, Li TH, Liu S, Li G (2019) StructureFlow:
    Image Inpainting via Structure-aware Appearance Flow. *In: Int. Conf. Comput.
    Vis.*, pp 181–190'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等人 (2019) Ren Y, Yu X, Zhang R, Li TH, Liu S, Li G (2019) StructureFlow：通过结构感知外观流进行图像修复。*In:
    Int. Conf. Comput. Vis.*, pp 181–190'
- en: 'Rezende and Mohamed (2015) Rezende DJ, Mohamed S (2015) Variational Inference
    with Normalizing Flows. *In: Int. Conf. Mach. Learn.*, p 1530–1538'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rezende 和 Mohamed (2015) Rezende DJ, Mohamed S (2015) 使用归一化流的变分推断。*In: Int.
    Conf. Mach. Learn.*, p 1530–1538'
- en: 'Richardson et al. (2021) Richardson E, Alaluf Y, Patashnik O, Nitzan Y, Azar
    Y, Shapiro S, Cohen-Or D (2021) Encoding in Style: a StyleGAN Encoder for Image-to-Image
    Translation. *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 2287–2296'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Richardson 等人 (2021) Richardson E, Alaluf Y, Patashnik O, Nitzan Y, Azar Y,
    Shapiro S, Cohen-Or D (2021) 风格编码：用于图像到图像翻译的 StyleGAN 编码器。*In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 2287–2296'
- en: 'Rombach et al. (2022) Rombach R, Blattmann A, Lorenz D, Esser P, Ommer B (2022)
    High-Resolution Image Synthesis with Latent Diffusion Models. *In: IEEE Conf.
    Comput. Vis. Pattern Recog.*, pp 10674–10685'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rombach 等人 (2022) Rombach R, Blattmann A, Lorenz D, Esser P, Ommer B (2022)
    使用潜在扩散模型进行高分辨率图像合成。*In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 10674–10685'
- en: 'Rössler et al. (2018) Rössler A, Cozzolino D, Verdoliva L, Riess C, Thies J,
    Nießner M (2018) Faceforensics: A large-scale video dataset for forgery detection
    in human faces. *arXiv preprint arXiv:180309179*'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rössler 等人 (2018) Rössler A, Cozzolino D, Verdoliva L, Riess C, Thies J, Nießner
    M (2018) Faceforensics：用于人脸伪造检测的大规模视频数据集。*arXiv preprint arXiv:180309179*
- en: Roy et al. (2021) Roy H, Chaudhury S, Yamasaki T, Hashimoto T (2021) Image inpainting
    using frequency-domain priors. *J Electronic Imaging* 30(2):023016
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy 等人 (2021) Roy H, Chaudhury S, Yamasaki T, Hashimoto T (2021) 使用频域先验进行图像修复。*J
    Electronic Imaging* 30(2):023016
- en: 'Ruder et al. (2016) Ruder M, Dosovitskiy A, Brox T (2016) Artistic Style Transfer
    for Videos. *In: German Conf. Pattern Recog.*, pp 26–36'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruder 等人 (2016) Ruder M, Dosovitskiy A, Brox T (2016) 视频艺术风格转移。*见：德国模式识别会议*，第
    26–36 页
- en: 'Rudin et al. (1992) Rudin LI, Osher S, Fatemi E (1992) Nonlinear total variation
    based noise removal algorithms. *Physica D: Nonlinear Phenomena* 60(1):259–268'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rudin 等人 (1992) Rudin LI, Osher S, Fatemi E (1992) 基于非线性总变差的噪声去除算法。*物理学 D：非线性现象*
    60(1):259–268
- en: 'Sagong et al. (2019) Sagong Mc, Shin Yg, Kim Sw, Park S, Ko Sj (2019) PEPSI
    : Fast Image Inpainting With Parallel Decoding Network. *In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 11352–11360'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sagong 等人 (2019) Sagong Mc, Shin Yg, Kim Sw, Park S, Ko Sj (2019) PEPSI : 快速图像修复与并行解码网络。*见：IEEE
    计算机视觉与模式识别会议*，第 11352–11360 页'
- en: 'Saharia et al. (2022a) Saharia C, Chan W, Chang H, Lee C, Ho J, Salimans T,
    Fleet D, Norouzi M (2022a) Palette: Image-to-Image Diffusion Models. *In: ACM
    SIGGRAPH Conf.*'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saharia 等人 (2022a) Saharia C, Chan W, Chang H, Lee C, Ho J, Salimans T, Fleet
    D, Norouzi M (2022a) Palette：图像到图像的扩散模型。*见：ACM SIGGRAPH 会议*
- en: 'Saharia et al. (2022b) Saharia C, Chan W, Saxena S, Li L, Whang J, Denton E,
    Ghasemipour SKS, Gontijo-Lopes R, Ayan BK, Salimans T, Ho J, Fleet DJ, Norouzi
    M (2022b) Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.
    *In: Adv. Neural Inform. Process. Syst.*'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saharia 等人 (2022b) Saharia C, Chan W, Saxena S, Li L, Whang J, Denton E, Ghasemipour
    SKS, Gontijo-Lopes R, Ayan BK, Salimans T, Ho J, Fleet DJ, Norouzi M (2022b) 具有深度语言理解的逼真文本到图像扩散模型。*见：先进的神经信息处理系统*
- en: 'Schrader et al. (2023) Schrader K, Peter P, Kämper N, Weickert J (2023) Efficient
    Neural Generation of 4K Masks for Homogeneous Diffusion Inpainting. *In: Int.
    Conf. Scale Space Variational Methods Comput. Vis.*, pp 16–28'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schrader 等人 (2023) Schrader K, Peter P, Kämper N, Weickert J (2023) 高效的 4K 掩模神经生成用于均匀扩散修复。*见：国际尺度空间变分方法计算机视觉会议*，第
    16–28 页
- en: 'Schuhmann et al. (2022) Schuhmann C, Beaumont R, Vencu R, Gordon CW, Wightman
    R, Cherti M, Coombes T, Katta A, Mullis C, Wortsman M, Schramowski P, Kundurthy
    SR, Crowson K, Schmidt L, Kaczmarczyk R, Jitsev J (2022) LAION-5B: An open large-scale
    dataset for training next generation image-text models. *In: Adv. Neural Inform.
    Process. Syst.*'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schuhmann 等人 (2022) Schuhmann C, Beaumont R, Vencu R, Gordon CW, Wightman R,
    Cherti M, Coombes T, Katta A, Mullis C, Wortsman M, Schramowski P, Kundurthy SR,
    Crowson K, Schmidt L, Kaczmarczyk R, Jitsev J (2022) LAION-5B：用于训练下一代图像文本模型的开放大规模数据集。*见：先进的神经信息处理系统*
- en: 'Shao et al. (2020) Shao H, Wang Y, Fu Y, Yin Z (2020) Generative image inpainting
    via edge structure and color aware fusion. *Sign Process: Image Communication*
    87:115929'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等人 (2020) Shao H, Wang Y, Fu Y, Yin Z (2020) 通过边缘结构和颜色感知融合的生成图像修复。*信号处理：图像通信*
    87:115929
- en: 'Shen et al. (2019) Shen L, Hong R, Zhang H, Zhang H, Wang M (2019) Single-Shot
    Semantic Image Inpainting with Densely Connected Generative Networks. *In: ACM
    Int. Conf. Multimedia*, p 1861–1869'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人 (2019) Shen L, Hong R, Zhang H, Zhang H, Wang M (2019) 单次语义图像修复与密集连接生成网络。*见：ACM
    国际多媒体会议*，第 1861–1869 页
- en: 'Shin et al. (2021) Shin YG, Sagong MC, Yeo YJ, Kim SW, Ko SJ (2021) Pepsi++:
    Fast and lightweight network for image inpainting. *IEEE Trans Neural Networks
    Learn Syst* 32(1):252–265'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin 等人 (2021) Shin YG, Sagong MC, Yeo YJ, Kim SW, Ko SJ (2021) Pepsi++：用于图像修复的快速轻量级网络。*IEEE
    神经网络与学习系统汇刊* 32(1):252–265
- en: 'Shukla et al. (2023) Shukla T, Maheshwari P, Singh R, Shukla A, Kulkarni K,
    Turaga P (2023) Scene Graph Driven Text-Prompt Generation for Image Inpainting.
    *In: IEEE Conf. Comput. Vis. Pattern Recog. Worksh.*, pp 759–768'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shukla 等人 (2023) Shukla T, Maheshwari P, Singh R, Shukla A, Kulkarni K, Turaga
    P (2023) 场景图驱动的文本提示生成用于图像修复。*见：IEEE 计算机视觉与模式识别研讨会*，第 759–768 页
- en: Simonyan and Zisserman (2014) Simonyan K, Zisserman A (2014) Very deep convolutional
    networks for large-scale image recognition. *arXiv preprint arXiv:14091556*
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman (2014) Simonyan K, Zisserman A (2014) 用于大规模图像识别的非常深的卷积网络。*arXiv
    预印本 arXiv:14091556*
- en: 'Sohl-Dickstein et al. (2015) Sohl-Dickstein J, Weiss E, Maheswaranathan N,
    Ganguli S (2015) Deep Unsupervised Learning using Nonequilibrium Thermodynamics.
    *In: Int. Conf. Mach. Learn.*, vol 37, pp 2256–2265'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohl-Dickstein 等人 (2015) Sohl-Dickstein J, Weiss E, Maheswaranathan N, Ganguli
    S (2015) 使用非平衡热力学的深度无监督学习。*见：国际机器学习会议*，第 37 卷，第 2256–2265 页
- en: 'Song et al. (2019) Song L, Cao J, Song L, Hu Y, He R (2019) Geometry-Aware
    Face Completion and Editing. *In: AAAI Conf. Artificial Intell.*, pp 2506–2513'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 (2019) Song L, Cao J, Song L, Hu Y, He R (2019) 几何感知的面部补全和编辑。*见：AAAI
    人工智能会议*，第 2506–2513 页
- en: 'Song et al. (2018a) Song Y, Yang C, Lin Z, Liu X, Huang Q, Li H, Kuo CCJ (2018a)
    Contextual-Based Image Inpainting: Infer, Match, and Translate. *In: Eur. Conf.
    Comput. Vis.*, pp 3–18'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 (2018a) Song Y, Yang C, Lin Z, Liu X, Huang Q, Li H, Kuo CCJ (2018a)
    基于上下文的图像修复：推断、匹配和翻译。*见：欧洲计算机视觉会议*，第 3–18 页
- en: 'Song et al. (2018b) Song Y, Yang C, Shen Y, Wang P, Huang Q, Kuo CCJ (2018b)
    SPG-Net: Segmentation Prediction and Guidance Network for Image Inpainting. *In:
    Brit. Mach. Vis. Conf.*'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2018b) Song Y, Yang C, Shen Y, Wang P, Huang Q, Kuo CCJ (2018b)
    SPG-Net：用于图像修复的分割预测与引导网络。*在：英国机器视觉会议*
- en: 'Sun et al. (2018a) Sun D, Yang X, Liu MY, Kautz J (2018a) PWC-Net: CNNs for
    Optical Flow Using Pyramid, Warping, and Cost Volume. *In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 8934–8943'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2018a) Sun D, Yang X, Liu MY, Kautz J (2018a) PWC-Net：用于光流的CNN，包括金字塔、变形和代价体积。*在：IEEE
    计算机视觉与模式识别会议*，第8934–8943页
- en: 'Sun et al. (2019) Sun K, Xiao B, Liu D, Wang J (2019) Deep High-Resolution
    Representation Learning for Human Pose Estimation. *In: IEEE Conf. Comput. Vis.
    Pattern Recog.*, pp 5686–5696'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2019) Sun K, Xiao B, Liu D, Wang J (2019) 用于人体姿态估计的深度高分辨率表示学习。*在：IEEE
    计算机视觉与模式识别会议*，第5686–5696页
- en: 'Sun et al. (2018b) Sun Q, Ma L, Joon Oh S, Gool LV, Schiele B, Fritz M (2018b)
    Natural and Effective Obfuscation by Head Inpainting. *In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 5050–5059'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2018b) Sun Q, Ma L, Joon Oh S, Gool LV, Schiele B, Fritz M (2018b)
    通过头部修复实现自然且有效的遮蔽。*在：IEEE 计算机视觉与模式识别会议*，第5050–5059页
- en: 'Suvorov et al. (2022) Suvorov R, Logacheva E, Mashikhin A, Remizova A, Ashukha
    A, Silvestrov A, Kong N, Goka H, Park K, Lempitsky V (2022) Resolution-robust
    Large Mask Inpainting with Fourier Convolutions. *In: Winter Conf. App. Comput.
    Vis.*, pp 3172–3182'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suvorov et al. (2022) Suvorov R, Logacheva E, Mashikhin A, Remizova A, Ashukha
    A, Silvestrov A, Kong N, Goka H, Park K, Lempitsky V (2022) 分辨率稳健的大型遮罩修复与傅里叶卷积。*在：冬季应用计算机视觉会议*，第3172–3182页
- en: Tabak and Vanden-Eijnden (2010) Tabak EG, Vanden-Eijnden E (2010) Density estimation
    by dual ascent of the log-likelihood. *Commun Math Sci* 8(1):217 – 233
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tabak and Vanden-Eijnden (2010) Tabak EG, Vanden-Eijnden E (2010) 通过对数似然的双重上升进行密度估计。*通讯数学科学*
    8(1)：217 – 233
- en: 'Tschumperlé and Deriche (2005) Tschumperlé D, Deriche R (2005) Vector-valued
    image regularization with pdes: a common framework for different applications.
    *IEEE Trans Pattern Anal Mach Intell* 27(4):506–517'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tschumperlé and Deriche (2005) Tschumperlé D, Deriche R (2005) 使用偏微分方程的向量值图像正则化：不同应用的共同框架。*IEEE
    模式分析与机器智能学报* 27(4)：506–517
- en: 'Tu and Chen (2019) Tu CT, Chen YF (2019) Facial Image Inpainting with Variational
    Autoencoder. *In: Inter. Conf. Intell. Robot. Control Engin.*, pp 119–122'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu and Chen (2019) Tu CT, Chen YF (2019) 使用变分自编码器的面部图像修复。*在：国际智能机器人控制工程会议*，第119–122页
- en: 'Vaswani et al. (2017) Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L,
    Gomez AN, Kaiser u, Polosukhin I (2017) Attention is All You Need. *In: Adv. Neural
    Inform. Process. Syst.*, p 6000–6010'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L,
    Gomez AN, Kaiser u, Polosukhin I (2017) 注意力即你所需。*在：神经信息处理系统进展*，第6000–6010页
- en: 'Vo et al. (2018) Vo HV, Duong NQK, Pérez P (2018) Structural Inpainting. *In:
    ACM Int. Conf. Multimedia*, p 1948–1956'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vo et al. (2018) Vo HV, Duong NQK, Pérez P (2018) 结构修复。*在：ACM 国际多媒体会议*，第1948–1956页
- en: 'Wadhwa et al. (2021) Wadhwa G, Dhall A, Murala S, Tariq U (2021) Hyperrealistic
    Image Inpainting with Hypergraphs. *In: Winter Conf. App. Comput. Vis.*, pp 3911–3920'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wadhwa et al. (2021) Wadhwa G, Dhall A, Murala S, Tariq U (2021) 用超图进行超现实图像修复。*在：冬季应用计算机视觉会议*，第3911–3920页
- en: 'Wan et al. (2020) Wan Z, Zhang B, Chen D, Zhang P, Chen D, Liao J, Wen F (2020)
    Bringing Old Photos Back to Life. *In: IEEE Conf. Comput. Vis. Pattern Recog.*,
    pp 2747–2757'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan et al. (2020) Wan Z, Zhang B, Chen D, Zhang P, Chen D, Liao J, Wen F (2020)
    让旧照片重焕生机。*在：IEEE 计算机视觉与模式识别会议*，第2747–2757页
- en: 'Wan et al. (2021) Wan Z, Zhang J, Chen D, Liao J (2021) High-Fidelity Pluralistic
    Image Completion With Transformers. *In: Int. Conf. Comput. Vis.*, pp 4692–4701'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan et al. (2021) Wan Z, Zhang J, Chen D, Liao J (2021) 高保真多元图像补全与变压器。*在：国际计算机视觉会议*，第4692–4701页
- en: 'Wang et al. (2019a) Wang C, Huang H, Han X, Wang J (2019a) Video Inpainting
    by Jointly Learning Temporal Structure and Spatial Details. *In: AAAI Conf. Artificial
    Intell.*, pp 5232–5239'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019a) Wang C, Huang H, Han X, Wang J (2019a) 通过联合学习时间结构和空间细节的视频修复。*在：AAAI
    人工智能会议*，第5232–5239页
- en: 'Wang et al. (2022a) Wang C, Zhu Y, Yuan C (2022a) Diverse Image Inpainting
    with Normalizing Flow. *In: Eur. Conf. Comput. Vis.*, pp 53–69'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022a) Wang C, Zhu Y, Yuan C (2022a) 使用归一化流的多样化图像修复。*在：欧洲计算机视觉会议*，第53–69页
- en: 'Wang et al. (2020a) Wang J, Wang C, Huang Q, Shi Y, Cai JF, Zhu Q, Yin B (2020a)
    Image Inpainting Based on Multi-Frequency Probabilistic Inference Model. *In:
    ACM Int. Conf. Multimedia*, p 1–9'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020a) Wang J, Wang C, Huang Q, Shi Y, Cai JF, Zhu Q, Yin B (2020a)
    基于多频率概率推断模型的图像修复。*在：ACM 国际多媒体会议*，第1–9页
- en: 'Wang et al. (2019b) Wang N, Li J, Zhang L, Du B (2019b) MUSICAL: Multi-Scale
    Image Contextual Attention Learning for Inpainting. *In: Int. Joint Conf. Artificial
    Intell.*, pp 3748–3754'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2019b）Wang N, Li J, Zhang L, Du B（2019b）MUSICAL：用于修复的多尺度图像上下文注意力学习。*见：国际联合人工智能会议*，第3748–3754页
- en: Wang et al. (2020b) Wang N, Ma S, Li J, Zhang Y, Zhang L (2020b) Multistage
    attention network for image inpainting. *Pattern Recog* 106:107448
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020b）Wang N, Ma S, Li J, Zhang Y, Zhang L（2020b）用于图像修复的多阶段注意力网络。*模式识别*
    106:107448
- en: Wang et al. (2021a) Wang N, Zhang Y, Zhang L (2021a) Dynamic selection network
    for image inpainting. *IEEE Trans Image Process* 30:1784–1798
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2021a）Wang N, Zhang Y, Zhang L（2021a）用于图像修复的动态选择网络。*IEEE图像处理学报* 30:1784–1798
- en: 'Wang et al. (2023) Wang S, Saharia C, Montgomery C, Pont-Tuset J, Noy S, Pellegrini
    S, Onoe Y, Laszlo S, Fleet DJ, Soricut R, et al. (2023) Imagen editor and editbench:
    Advancing and evaluating text-guided image inpainting. *In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 18359–18369'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023）Wang S, Saharia C, Montgomery C, Pont-Tuset J, Noy S, Pellegrini
    S, Onoe Y, Laszlo S, Fleet DJ, Soricut R 等（2023）Imagen 编辑器和 Editbench：推进和评估基于文本的图像修复。*见：IEEE计算机视觉与模式识别会议*，第18359–18369页
- en: 'Wang et al. (2021b) Wang T, Ouyang H, Chen Q (2021b) Image Inpainting with
    External-internal Learning and Monochromic Bottleneck. *In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 5116–5125'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2021b）Wang T, Ouyang H, Chen Q（2021b）具有外部-内部学习和单色瓶颈的图像修复。*见：IEEE计算机视觉与模式识别会议*，第5116–5125页
- en: 'Wang et al. (2018a) Wang TC, Liu MY, Zhu JY, Liu G, Tao A, Kautz J, Catanzaro
    B (2018a) Video-to-Video Synthesis. *In: Adv. Neural Inform. Process. Syst.*,
    vol 31'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2018a）Wang TC, Liu MY, Zhu JY, Liu G, Tao A, Kautz J, Catanzaro B（2018a）视频到视频合成。*见：神经信息处理系统进展*，第31卷
- en: 'Wang et al. (2021c) Wang W, Zhang J, Niu L, Ling H, Yang X, Zhang L (2021c)
    Parallel Multi-Resolution Fusion Network for Image Inpainting. *In: Int. Conf.
    Comput. Vis.*, pp 14559–14568'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2021c）Wang W, Zhang J, Niu L, Ling H, Yang X, Zhang L（2021c）用于图像修复的并行多分辨率融合网络。*见：国际计算机视觉会议*，第14559–14568页
- en: 'Wang et al. (2022b) Wang W, Niu L, Zhang J, Yang X, Zhang L (2022b) Dual-path
    Image Inpainting with Auxiliary GAN Inversion. *In: IEEE Conf. Comput. Vis. Pattern
    Recog.*, pp 11411–11420'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2022b）Wang W, Niu L, Zhang J, Yang X, Zhang L（2022b）具有辅助GAN反演的双路径图像修复。*见：IEEE计算机视觉与模式识别会议*，第11411–11420页
- en: 'Wang et al. (2018b) Wang X, Girshick RB, Gupta A, He K (2018b) Non-local Neural
    Networks. *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 7794–7803'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2018b）Wang X, Girshick RB, Gupta A, He K（2018b）非局部神经网络。*见：IEEE计算机视觉与模式识别会议*，第7794–7803页
- en: 'Wang et al. (2018c) Wang Y, Tao X, Qi X, Shen X, Jia J (2018c) Image Inpainting
    via Generative Multi-column Convolutional Neural Networks. *In: Adv. Neural Inform.
    Process. Syst.*, pp 331–340'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2018c）Wang Y, Tao X, Qi X, Shen X, Jia J（2018c）通过生成多列卷积神经网络进行图像修复。*见：神经信息处理系统进展*，第331–340页
- en: 'Wang et al. (2020c) Wang Y, Chen YC, Tao X, Jia J (2020c) VCNet: A Robust Approach
    to Blind Image Inpainting. *In: Eur. Conf. Comput. Vis.*'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020c）Wang Y, Chen YC, Tao X, Jia J（2020c）VCNet：一种鲁棒的盲图像修复方法。*见：欧洲计算机视觉会议*
- en: 'Wang et al. (2003) Wang Z, Simoncelli E, Bovik A (2003) Multiscale structural
    similarity for image quality assessment. *In: Asilomar Conf. Sign. Syst. Comput.*,
    vol 2, pp 1398–1402'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2003）Wang Z, Simoncelli E, Bovik A（2003）用于图像质量评估的多尺度结构相似性。*见：Asilomar信号系统计算会议*，第2卷，第1398–1402页
- en: 'Wang et al. (2004) Wang Z, Bovik AC, Sheikh HR, Simoncelli EP (2004) Image
    quality assessment: from error visibility to structural similarity. *IEEE Trans
    Image Process* 13(4):600–612'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2004）Wang Z, Bovik AC, Sheikh HR, Simoncelli EP（2004）图像质量评估：从误差可见性到结构相似性。*IEEE图像处理学报*
    13(4):600–612
- en: 'Weng et al. (2022) Weng Y, Ding S, Zhou T (2022) A Survey on Improved GAN based
    Image Inpainting. *In: Inter. Conf. Consumer Electronics and Comput. Engin.*,
    pp 319–322'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng 等（2022）Weng Y, Ding S, Zhou T（2022）关于改进的基于GAN的图像修复的综述。*见：国际消费电子与计算工程会议*，第319–322页
- en: Wexler et al. (2007) Wexler Y, Shechtman E, Irani M (2007) Space-time completion
    of video. *IEEE Trans Pattern Anal Mach Intell* 29(3):463–476
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wexler 等（2007）Wexler Y, Shechtman E, Irani M（2007）视频的时空完成。*IEEE模式分析与机器智能学报*
    29(3):463–476
- en: 'Woo et al. (2020) Woo S, Kim D, Park K, Lee JY, Kweon IS (2020) Align-and-Attend
    Network for Globally and Locally Coherent Video Inpainting. *In: Brit. Mach. Vis.
    Conf.*, pp 1–13'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Woo 等（2020）Woo S, Kim D, Park K, Lee JY, Kweon IS（2020）用于全局和局部一致视频修复的对齐与关注网络。*见：英国机器视觉会议*，第1–13页
- en: Wu et al. (2022) Wu H, Zhou J, Li Y (2022) Deep generative model for image inpainting
    with local binary pattern learning and spatial attention. *IEEE Trans Multimedia*
    24:4016–4027
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2022）Wu H, Zhou J, Li Y（2022）用于图像修复的深度生成模型，具有局部二进制模式学习和空间注意力。*IEEE多媒体学报*
    24:4016–4027
- en: 'Wu et al. (2019) Wu L, Zhang C, Liu J, Han J, Liu J, Ding E, Bai X (2019) Editing
    Text in the Wild. *In: ACM Int. Conf. Multimedia*, p 1500–1508'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人 (2019) Wu L, Zhang C, Liu J, Han J, Liu J, Ding E, Bai X (2019) 在自然环境中编辑文本。*In:
    ACM Int. Conf. Multimedia*, p 1500–1508'
- en: 'Wu et al. (2021) Wu X, Xie Y, Zeng J, Yang Z, Yu Y, Li Q, Liu W (2021) Adversarial
    learning with mask reconstruction for text-guided image inpainting. *In: ACM Int.
    Conf. Multimedia*, pp 3464–3472'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人 (2021) Wu X, Xie Y, Zeng J, Yang Z, Yu Y, Li Q, Liu W (2021) 具有掩膜重建的对抗学习用于文本引导的图像修复。*In:
    ACM Int. Conf. Multimedia*, pp 3464–3472'
- en: 'Xia et al. (2022) Xia W, Zhang Y, Yang Y, Xue JH, Zhou B, Yang MH (2022) Gan
    inversion: A survey. *IEEE Trans Pattern Anal Mach Intell* pp 1–17'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia 等人 (2022) Xia W, Zhang Y, Yang Y, Xue JH, Zhou B, Yang MH (2022) GAN 逆变：综述。*IEEE
    Trans Pattern Anal Mach Intell* pp 1–17
- en: 'Xie et al. (2019) Xie C, Liu S, Li C, Cheng MM, Zuo W, Liu X, Wen S, Ding E
    (2019) Image Inpainting with Learnable Bidirectional Attention Maps. *In: Int.
    Conf. Comput. Vis.*, pp 8858–8867'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie 等人 (2019) Xie C, Liu S, Li C, Cheng MM, Zuo W, Liu X, Wen S, Ding E (2019)
    具有可学习的双向注意力图像修复。*In: Int. Conf. Comput. Vis.*, pp 8858–8867'
- en: Xie et al. (2020) Xie M, Li C, Liu X, Wong TT (2020) Manga filling style conversion
    with screentone variational autoencoder. *ACM Trans Graph* 39(6)
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人 (2020) Xie M, Li C, Liu X, Wong TT (2020) 基于筛选纹理变分自编码器的漫画风格转换。*ACM Trans
    Graph* 39(6)
- en: Xie et al. (2021) Xie M, Xia M, Liu X, Li C, Wong TT (2021) Seamless manga inpainting
    with semantics awareness. *ACM Trans Graph* 40(4)
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人 (2021) Xie M, Xia M, Liu X, Li C, Wong TT (2021) 无缝漫画修复与语义感知。*ACM Trans
    Graph* 40(4)
- en: 'Xie et al. (2023) Xie S, Zhang Z, Lin Z, Hinz T, Zhang K (2023) SmartBrush:
    Text and Shape Guided Object Inpainting With Diffusion Model. *In: IEEE Conf.
    Comput. Vis. Pattern Recog.*, pp 22428–22437'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie 等人 (2023) Xie S, Zhang Z, Lin Z, Hinz T, Zhang K (2023) SmartBrush：基于扩散模型的文本和形状引导对象修复。*In:
    IEEE Conf. Comput. Vis. Pattern Recog.*, pp 22428–22437'
- en: Xie et al. (2022) Xie Y, Lin Z, Yang Z, Deng H, Wu X, Mao X, Li Q, Liu W (2022)
    Learning semantic alignment from image for text-guided image inpainting. *The
    Visual Computer* 38(9-10):3149–3161
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人 (2022) Xie Y, Lin Z, Yang Z, Deng H, Wu X, Mao X, Li Q, Liu W (2022)
    从图像中学习语义对齐以进行文本引导的图像修复。*The Visual Computer* 38(9-10):3149–3161
- en: 'Xiong et al. (2019) Xiong W, Yu J, Lin Z, Yang J, Lu X, Barnes C, Luo J (2019)
    Foreground-Aware Image Inpainting. *In: IEEE Conf. Comput. Vis. Pattern Recog.*,
    pp 5833–5841'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiong 等人 (2019) Xiong W, Yu J, Lin Z, Yang J, Lu X, Barnes C, Luo J (2019)
    前景感知图像修复。*In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 5833–5841'
- en: 'Xu et al. (2018a) Xu N, Yang L, Fan Y, Yang J, Yue D, Liang Y, Price B, Cohen
    S, Huang T (2018a) YouTube-VOS: Sequence-to-Sequence Video Object Segmentation.
    *In: Eur. Conf. Comput. Vis.*, pp 585–601'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人 (2018a) Xu N, Yang L, Fan Y, Yang J, Yue D, Liang Y, Price B, Cohen S,
    Huang T (2018a) YouTube-VOS：序列到序列的视频对象分割。*In: Eur. Conf. Comput. Vis.*, pp 585–601'
- en: 'Xu et al. (2019) Xu R, Li X, Zhou B, Loy CC (2019) Deep flow-guided video inpainting.
    *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 3723–3732'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人 (2019) Xu R, Li X, Zhou B, Loy CC (2019) 深度流引导的视频修复。*In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 3723–3732'
- en: Xu et al. (2021) Xu R, Guo M, Wang J, Li X, Zhou B, Loy CC (2021) Texture memory-augmented
    deep patch-based image inpainting. *IEEE Trans Image Process* 30:9112–9124
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2021) Xu R, Guo M, Wang J, Li X, Zhou B, Loy CC (2021) 纹理记忆增强的深度补丁图像修复。*IEEE
    Trans Image Process* 30:9112–9124
- en: 'Xu et al. (2018b) Xu T, Zhang P, Huang Q, Zhang H, Gan Z, Huang X, He X (2018b)
    Attngan: Fine-grained text to image generation with attentional generative adversarial
    networks. *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 1316–1324'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人 (2018b) Xu T, Zhang P, Huang Q, Zhang H, Gan Z, Huang X, He X (2018b)
    Attngan：基于注意力生成对抗网络的细粒度文本到图像生成。*In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp
    1316–1324'
- en: 'Yamashita et al. (2022) Yamashita Y, Shimosato K, Ukita N (2022) Boundary-Aware
    Image Inpainting With Multiple Auxiliary Cues. *In: IEEE Conf. Comput. Vis. Pattern
    Recog. Worksh.*, pp 619–629'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yamashita 等人 (2022) Yamashita Y, Shimosato K, Ukita N (2022) 边界感知图像修复与多个辅助线索。*In:
    IEEE Conf. Comput. Vis. Pattern Recog. Worksh.*, pp 619–629'
- en: 'Yan et al. (2018) Yan Z, Li X, Li M, Zuo W, Shan S (2018) Shift-Net: Image
    Inpainting via Deep Feature Rearrangement. *In: Eur. Conf. Comput. Vis.*, pp 3–19'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yan 等人 (2018) Yan Z, Li X, Li M, Zuo W, Shan S (2018) Shift-Net：通过深度特征重排进行图像修复。*In:
    Eur. Conf. Comput. Vis.*, pp 3–19'
- en: 'Yang et al. (2017) Yang C, Lu X, Lin Z, Shechtman E, Wang O, Li H (2017) High-resolution
    image inpainting using multi-scale neural patch synthesis. *In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 6721–6729'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 (2017) Yang C, Lu X, Lin Z, Shechtman E, Wang O, Li H (2017) 使用多尺度神经补丁合成的高分辨率图像修复。*In:
    IEEE Conf. Comput. Vis. Pattern Recog.*, pp 6721–6729'
- en: 'Yang et al. (2020) Yang J, Qi Z, Shi Y (2020) Learning to Incorporate Structure
    Knowledge for Image Inpainting. *In: AAAI Conf. Artificial Intell.*, vol 34, pp
    12605–12612'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 (2020) Yang J, Qi Z, Shi Y (2020) 学习融合结构知识进行图像修复。*In: AAAI Conf. Artificial
    Intell.*, vol 34, pp 12605–12612'
- en: 'Yang et al. (2023) Yang L, Zhang Z, Song Y, Hong S, Xu R, Zhao Y, Zhang W,
    Cui B, Yang MH (2023) Diffusion models: A comprehensive survey of methods and
    applications. [2209.00796](2209.00796)'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等 (2023) Yang L, Zhang Z, Song Y, Hong S, Xu R, Zhao Y, Zhang W, Cui B,
    Yang MH (2023) 扩散模型: 方法和应用的全面调查。 [2209.00796](2209.00796)'
- en: 'Yeh et al. (2017) Yeh RA, Chen C, Lim TY, Schwing AG, Hasegawa-Johnson M, Do
    MN (2017) Semantic Image Inpainting with Deep Generative Models. *In: IEEE Conf.
    Comput. Vis. Pattern Recog.*, pp 6882–6890'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yeh 等 (2017) Yeh RA, Chen C, Lim TY, Schwing AG, Hasegawa-Johnson M, Do MN
    (2017) 基于深度生成模型的语义图像修复。*在: IEEE 计算机视觉与模式识别会议*，第 6882–6890 页。'
- en: 'Yi et al. (2020) Yi Z, Tang Q, Azizi S, Jang D, Xu Z (2020) Contextual Residual
    Aggregation for Ultra High-Resolution Image Inpainting. *In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 7508–7517'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yi 等 (2020) Yi Z, Tang Q, Azizi S, Jang D, Xu Z (2020) 超高分辨率图像修复的上下文残差聚合。*在:
    IEEE 计算机视觉与模式识别会议*，第 7508–7517 页。'
- en: 'Yu and Koltun (2016) Yu F, Koltun V (2016) Multi-Scale Context Aggregation
    by Dilated Convolutions. *In: Int. Conf. Learn. Represent.*'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 和 Koltun (2016) Yu F, Koltun V (2016) 多尺度上下文聚合通过膨胀卷积。*在: 国际会议学习表示*。'
- en: 'Yu et al. (2018) Yu J, Lin Z, Yang J, Shen X, Lu X, Huang TS (2018) Generative
    image inpainting with contextual attention. *In: IEEE Conf. Comput. Vis. Pattern
    Recog.*, pp 5505–5514'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 (2018) Yu J, Lin Z, Yang J, Shen X, Lu X, Huang TS (2018) 具有上下文注意的生成图像修复。*在:
    IEEE 计算机视觉与模式识别会议*，第 5505–5514 页。'
- en: 'Yu et al. (2019) Yu J, Lin Z, Yang J, Shen X, Lu X, Huang TS (2019) Free-form
    image inpainting with gated convolution. *In: Int. Conf. Comput. Vis.*, pp 4471–4480'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 (2019) Yu J, Lin Z, Yang J, Shen X, Lu X, Huang TS (2019) 具有门控卷积的自由形状图像修复。*在:
    国际会议计算机视觉*，第 4471–4480 页。'
- en: 'Yu et al. (2020) Yu T, Guo Z, Jin X, Wu S, Chen Z, Li W, Zhang Z, Liu S (2020)
    Region Normalization for Image Inpainting. *In: AAAI Conf. Artificial Intell.*,
    pp 12733–12740'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 (2020) Yu T, Guo Z, Jin X, Wu S, Chen Z, Li W, Zhang Z, Liu S (2020) 图像修复的区域归一化。*在:
    AAAI 人工智能会议*，第 12733–12740 页。'
- en: 'Yu et al. (2021a) Yu Y, Zhan F, Lu S, Pan J, Ma F, Xie X, Miao C (2021a) WaveFill:
    A Wavelet-Based Generation Network for Image Inpainting. *In: Int. Conf. Comput.
    Vis.*, pp 14114–14123'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 (2021a) Yu Y, Zhan F, Lu S, Pan J, Ma F, Xie X, Miao C (2021a) WaveFill:
    基于小波的图像修复生成网络。*在: 国际会议计算机视觉*，第 14114–14123 页。'
- en: 'Yu et al. (2021b) Yu Y, Zhan F, WU R, Pan J, Cui K, Lu S, Ma F, Xie X, Miao
    C (2021b) Diverse Image Inpainting with Bidirectional and Autoregressive Transformers.
    *In: ACM Int. Conf. Multimedia*, p 69–78'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 (2021b) Yu Y, Zhan F, WU R, Pan J, Cui K, Lu S, Ma F, Xie X, Miao C (2021b)
    具有双向和自回归变换器的多样化图像修复。*在: ACM 国际会议多媒体*，第 69–78 页。'
- en: 'Yu et al. (2022a) Yu Y, Du D, Zhang L, Luo T (2022a) Unbiased Multi-modality
    Guidance for Image Inpainting. *In: Eur. Conf. Comput. Vis.*, pp 668–684'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 (2022a) Yu Y, Du D, Zhang L, Luo T (2022a) 无偏多模态指导的图像修复。*在: 欧洲会议计算机视觉*，第
    668–684 页。'
- en: 'Yu et al. (2022b) Yu Y, Zhang L, Fan H, Luo T (2022b) High-Fidelity Image Inpainting
    with GAN Inversion. *In: Eur. Conf. Comput. Vis.*, pp 242–258'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 (2022b) Yu Y, Zhang L, Fan H, Luo T (2022b) 高保真图像修复通过 GAN 反演。*在: 欧洲会议计算机视觉*，第
    242–258 页。'
- en: 'Zeng et al. (2019) Zeng Y, Fu J, Chao H, Guo B (2019) Learning pyramid-context
    encoder network for high-quality image inpainting. *In: IEEE Conf. Comput. Vis.
    Pattern Recog.*, pp 1486–1494'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 等 (2019) Zeng Y, Fu J, Chao H, Guo B (2019) 学习金字塔上下文编码网络用于高质量图像修复。*在:
    IEEE 计算机视觉与模式识别会议*，第 1486–1494 页。'
- en: 'Zeng et al. (2020a) Zeng Y, Fu J, Chao H (2020a) Learning Joint Spatial-Temporal
    Transformations for Video Inpainting. *In: Eur. Conf. Comput. Vis.*, Springer,
    pp 528–543'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 等 (2020a) Zeng Y, Fu J, Chao H (2020a) 学习联合空间-时间变换用于视频修复。*在: 欧洲会议计算机视觉*，Springer，第
    528–543 页。'
- en: 'Zeng et al. (2020b) Zeng Y, Lin Z, Yang J, Zhang J, Shechtman E, Lu H (2020b)
    High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided
    Upsampling. *In: Eur. Conf. Comput. Vis.*'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 等 (2020b) Zeng Y, Lin Z, Yang J, Zhang J, Shechtman E, Lu H (2020b) 通过迭代置信反馈和引导上采样进行高分辨率图像修复。*在:
    欧洲会议计算机视觉*。'
- en: Zeng et al. (2021a) Zeng Y, Gong Y, Zhang J (2021a) Feature learning and patch
    matching for diverse image inpainting. *Pattern Recog* 119:108036
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等 (2021a) Zeng Y, Gong Y, Zhang J (2021a) 多样化图像修复的特征学习和补丁匹配。*模式识别* 119:108036
- en: 'Zeng et al. (2021b) Zeng Y, Lin Z, Lu H, Patel VM (2021b) CR-Fill: Generative
    Image Inpainting With Auxiliary Contextual Reconstruction. *In: Int. Conf. Comput.
    Vis.*, pp 14164–14173'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 等 (2021b) Zeng Y, Lin Z, Lu H, Patel VM (2021b) CR-Fill: 带有辅助上下文重建的生成图像修复。*在:
    国际会议计算机视觉*，第 14164–14173 页。'
- en: Zeng et al. (2022) Zeng Y, Fu J, Chao H, Guo B (2022) Aggregated contextual
    transformations for high-resolution image inpainting. *IEEE Trans Vis Comput Graph*
    pp 1–1
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等 (2022) Zeng Y, Fu J, Chao H, Guo B (2022) 高分辨率图像修复的聚合上下文变换。*IEEE 视觉计算图形学会杂志*
    第 1–1 页。
- en: 'Zhang et al. (2010) Zhang B, Gao Y, Zhao S, Liu J (2010) Local derivative pattern
    versus local binary pattern: Face recognition with high-order local pattern descriptor.
    *IEEE Trans Image Process* 19(2):533–544'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2010) Zhang B, Gao Y, Zhao S, Liu J (2010) 局部导数模式与局部二值模式：使用高阶局部模式描述符进行人脸识别。*IEEE
    图像处理期刊* 19(2):533–544
- en: 'Zhang et al. (2018a) Zhang H, Hu Z, Luo C, Zuo W, Wang M (2018a) Semantic Image
    Inpainting with Progressive Generative Networks. *In: ACM Int. Conf. Multimedia*,
    p 1939–1947'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2018a) Zhang H, Hu Z, Luo C, Zuo W, Wang M (2018a) 使用渐进生成网络的语义图像修复。*收录于：ACM
    国际多媒体会议*，第 1939–1947 页
- en: 'Zhang et al. (2019a) Zhang H, Mai L, Xu N, Wang Z, Collomosse J, Jin H (2019a)
    An internal learning approach to video inpainting. *In: Int. Conf. Comput. Vis.*,
    pp 2720–2729'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019a) Zhang H, Mai L, Xu N, Wang Z, Collomosse J, Jin H (2019a) 内部学习方法用于视频修复。*收录于：国际计算机视觉会议*，第
    2720–2729 页
- en: 'Zhang et al. (2019b) Zhang J, Niu L, Yang D, Kang L, Li Y, Zhao W, Zhang L
    (2019b) GAIN: Gradient Augmented Inpainting Network for Irregular Holes. *In:
    ACM Int. Conf. Multimedia*, p 1870–1878'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019b) Zhang J, Niu L, Yang D, Kang L, Li Y, Zhao W, Zhang L (2019b)
    GAIN：用于不规则孔洞的梯度增强修复网络。*收录于：ACM 国际多媒体会议*，第 1870–1878 页
- en: 'Zhang et al. (2022a) Zhang K, Fu J, Liu D (2022a) Flow-Guided Transformer for Video
    Inpainting. *In: Eur. Conf. Comput. Vis.*, pp 74–90'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2022a) Zhang K, Fu J, Liu D (2022a) 流引导的变压器用于视频修复。*收录于：欧洲计算机视觉会议*，第
    74–90 页
- en: 'Zhang et al. (2022b) Zhang K, Fu J, Liu D (2022b) Inertia-Guided Flow Completion
    and Style Fusion for Video Inpainting. *In: IEEE Conf. Comput. Vis. Pattern Recog.*,
    pp 5982–5991'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2022b) Zhang K, Fu J, Liu D (2022b) 惯性引导的流完成与风格融合用于视频修复。*收录于：IEEE 计算机视觉与模式识别会议*，第
    5982–5991 页
- en: Zhang and Agrawala (2023) Zhang L, Agrawala M (2023) Adding conditional control
    to text-to-image diffusion models. [2302.05543](2302.05543)
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Agrawala (2023) Zhang L, Agrawala M (2023) 向文本到图像扩散模型添加条件控制。 [2302.05543](2302.05543)
- en: 'Zhang et al. (2020a) Zhang L, Chen Q, Hu B, Jiang S (2020a) Text-Guided Neural
    Image Inpainting. *In: ACM Int. Conf. Multimedia*, p 1302–1310'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020a) Zhang L, Chen Q, Hu B, Jiang S (2020a) 文本引导的神经图像修复。*收录于：ACM
    国际多媒体会议*，第 1302–1310 页
- en: 'Zhang et al. (2022c) Zhang L, Barnes C, Wampler K, Amirghodsi S, Shechtman
    E, Lin Z, Shi J (2022c) Inpainting at Modern Camera Resolution by Guided PatchMatch
    with Auto-curation. *In: Eur. Conf. Comput. Vis.*, pp 51–67'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2022c) Zhang L, Barnes C, Wampler K, Amirghodsi S, Shechtman E, Lin
    Z, Shi J (2022c) 通过引导 PatchMatch 实现现代相机分辨率的修复。*收录于：欧洲计算机视觉会议*，第 51–67 页
- en: 'Zhang et al. (2022d) Zhang L, Zhou Y, Barnes C, Amirghodsi S, Lin Z, Shechtman
    E, Shi J (2022d) Perceptual Artifacts Localization for Inpainting. *In: Computer
    Vision – ECCV 2022*, pp 146–164'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2022d) Zhang L, Zhou Y, Barnes C, Amirghodsi S, Lin Z, Shechtman E,
    Shi J (2022d) 修复的感知伪影定位。*收录于：计算机视觉 – ECCV 2022*，第 146–164 页
- en: 'Zhang et al. (2018b) Zhang R, Isola P, Efros AA, Shechtman E, Wang O (2018b)
    The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. *In: IEEE
    Conf. Comput. Vis. Pattern Recog.*, pp 586–595'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2018b) Zhang R, Isola P, Efros AA, Shechtman E, Wang O (2018b) 深度特征作为感知度量的非凡有效性。*收录于：IEEE
    计算机视觉与模式识别会议*，第 586–595 页
- en: Zhang et al. (2020b) Zhang R, Quan W, Wu B, Li Z, Yan DM (2020b) Pixel-wise
    dense detector for image inpainting. *Comput Graph Forum* 39(7)
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020b) Zhang R, Quan W, Wu B, Li Z, Yan DM (2020b) 用于图像修复的像素级密集检测器。*计算机图形论坛*
    39(7)
- en: 'Zhang et al. (2022e) Zhang R, Quan W, Zhang Y, Wang J, Yan DM (2022e) W-net:
    Structure and texture interaction for image inpainting. *IEEE Trans Multimedia*
    pp 1–12'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2022e) Zhang R, Quan W, Zhang Y, Wang J, Yan DM (2022e) W-net：图像修复的结构与纹理交互。*IEEE
    多媒体期刊* 第 1–12 页
- en: 'Zhang et al. (2018c) Zhang S, He R, Sun Z, Tan T (2018c) Demeshnet: Blind face
    inpainting for deep meshface verification. *IEEE Trans Inf Forensics Secur* 13(3):637–647'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2018c) Zhang S, He R, Sun Z, Tan T (2018c) Demeshnet：盲人面部修复用于深度网格面部验证。*IEEE
    信息取证与安全期刊* 13(3):637–647
- en: 'Zhang et al. (2021) Zhang W, Zhu J, Tai Y, Wang Y, Chu W, Ni B, Wang C, Yang
    X (2021) Context-Aware Image Inpainting with Learned Semantic Priors. *In: Int.
    Joint Conf. Artificial Intell.*, pp 1323–1329'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2021) Zhang W, Zhu J, Tai Y, Wang Y, Chu W, Ni B, Wang C, Yang X (2021)
    基于学习语义先验的上下文感知图像修复。*收录于：国际人工智能联合会议*，第 1323–1329 页
- en: 'Zhang et al. (2020c) Zhang Z, Zhao Z, Zhang Z, Huai B, Yuan J (2020c) Text-guided
    image inpainting. *In: ACM Int. Conf. Multimedia*, pp 4079–4087'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020c) Zhang Z, Zhao Z, Zhang Z, Huai B, Yuan J (2020c) 文本引导的图像修复。*收录于：ACM
    国际多媒体会议*，第 4079–4087 页
- en: 'Zhao et al. (2020) Zhao L, Mo Q, Lin S, Wang Z, Zuo Z, Chen H, Xing W, Lu D
    (2020) UCTGAN: Diverse Image Inpainting Based on Unsupervised Cross-Space Translation.
    *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 5740–5749'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. (2020) Zhao L, Mo Q, Lin S, Wang Z, Zuo Z, Chen H, Xing W, Lu D
    (2020) UCTGAN: 基于无监督跨空间翻译的多样图像修复。*In: IEEE Conf. Comput. Vis. Pattern Recog.*,
    pp 5740–5749'
- en: 'Zhao et al. (2021) Zhao S, Cui J, Sheng Y, Dong Y, Liang X, Chang EI, Xu Y
    (2021) Large Scale Image Completion via Co-Modulated Generative Adversarial Networks.
    *In: Int. Conf. Learn. Represent.*'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. (2021) Zhao S, Cui J, Sheng Y, Dong Y, Liang X, Chang EI, Xu Y
    (2021) 通过共同调制生成对抗网络的大规模图像补全。*In: Int. Conf. Learn. Represent.*'
- en: Zhao et al. (2023) Zhao W, Rao Y, Liu Z, Liu B, Zhou J, Lu J (2023) Unleashing
    text-to-image diffusion models for visual perception. *arXiv preprint arXiv:230302153*
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2023) Zhao W, Rao Y, Liu Z, Liu B, Zhou J, Lu J (2023) 发掘文本到图像扩散模型的视觉感知能力。*arXiv
    preprint arXiv:230302153*
- en: 'Zheng et al. (2019) Zheng C, Cham TJ, Cai J (2019) Pluralistic Image Completion.
    *In: IEEE Conf. Comput. Vis. Pattern Recog.*, pp 1438–1447'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2019) Zheng C, Cham TJ, Cai J (2019) 多元图像补全。*In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 1438–1447'
- en: Zheng et al. (2021a) Zheng C, Cham TJ, Cai J (2021a) Pluralistic free-form image
    completion. *Int J Comput Vis* 129:2786–2805
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2021a) Zheng C, Cham TJ, Cai J (2021a) 多元自由形式图像补全。*Int J Comput
    Vis* 129:2786–2805
- en: 'Zheng et al. (2022a) Zheng C, Cham TJ, Cai J, Phung D (2022a) Bridging Global
    Context Interactions for High-Fidelity Image Completion. *In: IEEE Conf. Comput.
    Vis. Pattern Recog.*, pp 11512–11522'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2022a) Zheng C, Cham TJ, Cai J, Phung D (2022a) 为高保真图像补全架起全球上下文交互的桥梁。*In:
    IEEE Conf. Comput. Vis. Pattern Recog.*, pp 11512–11522'
- en: 'Zheng et al. (2021b) Zheng H, Zhang Z, Wang Y, Zhang Z, Xu M, Yang Y, Wang
    M (2021b) GCM-Net: Towards Effective Global Context Modeling for Image Inpainting.
    *In: ACM Int. Conf. Multimedia*, p 2586–2594'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2021b) Zheng H, Zhang Z, Wang Y, Zhang Z, Xu M, Yang Y, Wang
    M (2021b) GCM-Net: 朝着有效的全球上下文建模用于图像修复。*In: ACM Int. Conf. Multimedia*, p 2586–2594'
- en: 'Zheng et al. (2022b) Zheng H, Lin Z, Lu J, Cohen S, Shechtman E, Barnes C,
    Zhang J, Xu N, Amirghodsi S, Luo J (2022b) Image Inpainting with Cascaded Modulation
    GAN and Object-Aware Training. *In: Eur. Conf. Comput. Vis.*, pp 277–296'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2022b) Zheng H, Lin Z, Lu J, Cohen S, Shechtman E, Barnes C,
    Zhang J, Xu N, Amirghodsi S, Luo J (2022b) 使用级联调制 GAN 和物体感知训练的图像修复。*In: Eur. Conf.
    Comput. Vis.*, pp 277–296'
- en: 'Zhou et al. (2017) Zhou B, Lapedriza A, Khosla A, Oliva A, Torralba A (2017)
    Places: A 10 million image database for scene recognition. *IEEE Trans Pattern
    Anal Mach Intell* 40(6):1452–1464'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2017) Zhou B, Lapedriza A, Khosla A, Oliva A, Torralba A (2017)
    Places: 一个用于场景识别的 1000 万图像数据库。*IEEE Trans Pattern Anal Mach Intell* 40(6):1452–1464'
- en: Zhou et al. (2018) Zhou B, Zhao H, Puig X, Xiao T, Fidler S, Barriuso A, Torralba
    A (2018) Semantic understanding of scenes through the ade20k dataset. *Int J Comput
    Vis* 127:302–321
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2018) Zhou B, Zhao H, Puig X, Xiao T, Fidler S, Barriuso A, Torralba
    A (2018) 通过 ade20k 数据集的语义理解场景。*Int J Comput Vis* 127:302–321
- en: 'Zhou et al. (2021) Zhou X, Li J, Wang Z, He R, Tan T (2021) Image Inpainting
    with Contrastive Relation Network. *In: Int. Conf. Pattern Recog.*, pp 4420–4427'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2021) Zhou X, Li J, Wang Z, He R, Tan T (2021) 使用对比关系网络的图像修复。*In:
    Int. Conf. Pattern Recog.*, pp 4420–4427'
- en: Zhu et al. (2021) Zhu M, He D, Li X, Li C, Li F, Liu X, Ding E, Zhang Z (2021)
    Image inpainting by end-to-end cascaded refinement with mask awareness. *IEEE
    Trans Image Process* 30:4855–4866
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2021) Zhu M, He D, Li X, Li C, Li F, Liu X, Ding E, Zhang Z (2021)
    通过端到端级联优化与掩码感知的图像修复。*IEEE Trans Image Process* 30:4855–4866
- en: 'Zou et al. (2021) Zou X, Yang L, Liu D, Lee YJ (2021) Progressive temporal
    feature alignment network for video inpainting. *In: IEEE Conf. Comput. Vis. Pattern
    Recog.*, pp 16448–16457'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zou et al. (2021) Zou X, Yang L, Liu D, Lee YJ (2021) 用于视频修复的渐进时间特征对齐网络。*In:
    IEEE Conf. Comput. Vis. Pattern Recog.*, pp 16448–16457'
