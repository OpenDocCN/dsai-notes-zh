- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:01:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:01:05
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2005.13247] A Quantitative Survey of Communication Optimizations in Distributed
    Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2005.13247] 分布式深度学习中的通信优化定量调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2005.13247](https://ar5iv.labs.arxiv.org/html/2005.13247)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2005.13247](https://ar5iv.labs.arxiv.org/html/2005.13247)
- en: A Quantitative Survey of Communication Optimizations in Distributed Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式深度学习中的通信优化定量调查
- en: Shaohuai Shi, Zhenheng Tang, Xiaowen Chu1, Chengjian Liu, Wei Wang, and Bo Li
    *Corresponding author. Shaohuai Shi, Wei Wang and Bo Li are with the Department
    of Computer Science and Engineering, The Hong Kong University of Science and Technology.
    Zhenheng Tang and Xiaowen Chu are with the Department of Computer Science, Hong
    Kong Baptist University. Chengjian Liu is with the College of Big Data and Internet,
    Shenzhen Technology University.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Shaohuai Shi、Zhenheng Tang、Xiaowen Chu1、Chengjian Liu、Wei Wang 和 Bo Li *通讯作者。Shaohuai
    Shi、Wei Wang 和 Bo Li 来自香港科技大学计算机科学与工程系。Zhenheng Tang 和 Xiaowen Chu 来自香港浸会大学计算机科学系。Chengjian
    Liu 来自深圳技术大学大数据与互联网学院。*
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Nowadays, large and complex deep learning (DL) models are increasingly trained
    in a distributed manner across multiple worker machines, in which extensive communications
    between workers pose serious scaling problems. In this article, we present a quantitative
    survey of communication optimization techniques for data parallel distributed
    DL. We first identify the major communication challenges and classify the existing
    solutions into three levels, namely the learning algorithm, the system architecture,
    and the network infrastructure. We present the state-of-the-art communication
    optimization techniques and conduct a comparative study of seven common lossless
    distributed DL methods on a 32-GPU cluster with 100Gbps InfiniBand (IB). We show
    that (1) the DL models with low model intensity (such as BERT and BERT-Large)
    are difficult to scale out even with the best available lossless algorithm over
    100Gbps IB; (2) the system architecture and scheduling algorithms have a critical
    impact on the scaling property. We conclude the article with discussions on the
    open issues for further investigations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大型和复杂的深度学习（DL）模型越来越多地在多个工作机器上以分布式方式进行训练，其中工人之间的广泛通信带来了严重的扩展问题。在这篇文章中，我们提供了对数据并行分布式深度学习通信优化技术的定量调查。我们首先识别主要的通信挑战，并将现有解决方案分为三个层次，即学习算法、系统架构和网络基础设施。我们介绍了最先进的通信优化技术，并在一个32-GPU集群上进行了一项关于七种常见无损分布式深度学习方法的比较研究，该集群配备了100Gbps
    InfiniBand (IB)。我们展示了（1）即使在100Gbps IB上使用最佳可用无损算法，模型强度较低的DL模型（例如BERT和BERT-Large）仍然难以扩展；（2）系统架构和调度算法对扩展特性有重要影响。我们以对进一步研究的开放问题进行讨论作为结尾。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The remarkable technological advances of deep learning (DL) have enabled a multitude
    of practical AI applications, ranging from computer vision to natural language
    processing and to robotics. In a typical DL workflow, deep neural network models
    are trained to solve a learning problem (e.g., image classification) on a labeled
    dataset; the trained models can then be used to make an inference given a new
    input (e.g., predicting the image label). Popular DL training algorithms include
    the standard mini-batch stochastic gradient descent (SGD) and its variants. These
    algorithms minimize a pre-defined loss function by iteratively updating the model
    parameters with stochastic gradients, calculated by sampling a mini-batch of data
    from the training set.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）的显著技术进步使得众多实际的人工智能应用成为可能，涵盖了从计算机视觉到自然语言处理再到机器人技术的各个领域。在一个典型的深度学习工作流程中，深度神经网络模型被训练以解决一个学习问题（例如，图像分类）在标记数据集上；训练后的模型可以用来对新的输入进行推断（例如，预测图像标签）。流行的深度学习训练算法包括标准的小批量随机梯度下降（SGD）及其变种。这些算法通过不断更新模型参数来最小化预定义的损失函数，更新过程是通过从训练集中采样一个小批量数据来计算随机梯度完成的。
- en: According to a recent study from OpenAI, the computational complexity required
    in DL training has doubled every 3.4 months since 2012, outpacing the Moore’s
    Law. As the training data and the DL models grow exponentially larger (e.g., the
    BDD100K auto-driving dataset has 120 million images, and the BERT-xlarge language
    model has over 1 billion parameters), training deep models on a single GPU or
    TPU device results in an exceedingly long time. A common practice is to parallelize
    DL training across multiple processors¹¹1Throughout this article, worker and processor
    are used interchangeably. that collaboratively update the model parameters. However,
    such distributed training requires iterative communications between processors,
    creating a severe performance bottleneck as the improvement of device interconnections
    lags far behind the rapidly increased computing power of AI processors. The result
    is the limited system scalability, as suggested by the Amdahl’s law. Therefore,
    how to address the communication bottlenecks in distributed DL has attracted great
    attention from both academia and industry in recent years.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 根据OpenAI的一项最新研究，自2012年以来，深度学习训练所需的计算复杂度每3.4个月翻一番，超出了摩尔定律。随着训练数据和深度学习模型的指数级增长（例如，BDD100K自动驾驶数据集有1.2亿张图像，而BERT-xlarge语言模型拥有超过10亿个参数），在单个GPU或TPU设备上训练深度模型会导致极其漫长的时间。一个常见的做法是将深度学习训练并行化到多个处理器¹¹1在本文中，工人和处理器可以互换使用。上，以协同更新模型参数。然而，这种分布式训练需要处理器之间的迭代通信，造成了严重的性能瓶颈，因为设备互连的改进远远落后于AI处理器计算能力的迅猛增长。结果是系统可扩展性受限，正如Amdahl定律所建议的。因此，如何解决分布式深度学习中的通信瓶颈近年来引起了学术界和工业界的广泛关注。
- en: Model parallelism and data parallelism are the two major parallelization schemes [[1](#bib.bib1)]
    that enable multiple processors to collaboratively train a single model. Model
    parallelism splits the set of model parameters and distributes them to all processors,
    but the high dependency between different neurons and the unbalanced parameter
    sizes in deep models make model parallelism difficult to scale out. Data parallelism,
    on the other hand, distributes the computational workload of different data samples
    to different processors that share the same set of model parameters. Compared
    with model parallelism, data parallelism is more appealing due to its improved
    scalability and simpler implementation. In this article, we mainly focus on data
    parallelism.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行和数据并行是使多个处理器协同训练单个模型的两种主要并行化方案[[1](#bib.bib1)]。模型并行将模型参数集合拆分并分配到所有处理器，但由于不同神经元之间的高度依赖性以及深度模型中不平衡的参数大小，使得模型并行难以扩展。另一方面，数据并行则将不同数据样本的计算负载分配给共享相同模型参数的不同处理器。与模型并行相比，由于其改进的可扩展性和更简单的实现，数据并行更具吸引力。在本文中，我们主要关注数据并行。
- en: Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Quantitative Survey of Communication
    Optimizations in Distributed Deep Learning")(a) illustrates the popular synchronized
    SGD algorithm for distributed DL with data parallelism, which has the same convergence
    performance (in terms of the number of iterations) as SGD on a single worker.
    In this method, workers load different data samples to calculate the gradients
    independently; all gradients are aggregated to update the model parameters. Data
    parallel synchronous SGD can be modeled by a directed acyclic graph (DAG), as
    shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Quantitative Survey of
    Communication Optimizations in Distributed Deep Learning")(b). The backpropagation
    computations of gradients are from the last layer to the first (denoted by $b_{P-1},...,b_{1},b_{0}$),
    and the distributed gradients should be aggregated (denoted by $c_{P-1},...,c_{1},c_{0}$)
    before going into the feed-forward computations (denoted by $f_{0},f_{1},...,f_{P-1}$)
    of the next iteration. The distributed synchronized SGD is also known as bulk
    synchronous parallel (BSP) SGD as it requires communication and synchronization
    in every iteration. The gradients can be aggregated through one or more dedicated
    parameter servers (PS) [[2](#bib.bib2)] or by all-to-all (A2A) communications [[3](#bib.bib3)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 分布式深度学习中的通信优化定量调查")(a) 展示了分布式DL中数据并行的流行同步SGD算法，其收敛性能（以迭代次数为标准）与单个工作节点上的SGD相同。在这种方法中，工作节点加载不同的数据样本来独立计算梯度；所有梯度被聚合以更新模型参数。数据并行同步SGD可以用有向无环图（DAG）来建模，如图 [1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 分布式深度学习中的通信优化定量调查")(b) 所示。梯度的反向传播计算从最后一层到第一层（用 $b_{P-1},...,b_{1},b_{0}$
    表示），分布式梯度在进入下一次迭代的前向计算（用 $f_{0},f_{1},...,f_{P-1}$ 表示）之前应该被聚合（用 $c_{P-1},...,c_{1},c_{0}$
    表示）。分布式同步SGD 也被称为大批量同步并行（BSP）SGD，因为它要求在每次迭代中进行通信和同步。梯度可以通过一个或多个专用参数服务器（PS）[[2](#bib.bib2)]
    或通过全对全（A2A）通信 [[3](#bib.bib3)] 进行聚合。
- en: '![Refer to caption](img/b86b866fa3ec25eb3931ef24d29be65b.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b86b866fa3ec25eb3931ef24d29be65b.png)'
- en: (a) Data parallelism
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 数据并行
- en: '![Refer to caption](img/709f501adfa20c848c7c9a03867062e1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/709f501adfa20c848c7c9a03867062e1.png)'
- en: (b) A DAG example
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (b) DAG示例
- en: 'Figure 1: Data parallelism of distributed DL.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：分布式DL的数据并行。
- en: 'Much work has been proposed recently to improve the scalability of distributed
    DL. In this article, we develop a taxonomy for describing communication-efficient
    techniques in distributed DL, and present a quantitative survey of communication
    optimization techniques for the BSP-style training algorithms. We identify the
    model intensity and batch size as two key factors that affect the system scalability,
    and conduct a quantitative study to compare seven state-of-the-art distributed
    training methods on a 32-GPU cluster with 100Gbps IB. Our evaluation method and
    results can serve as a reference for the practitioners to design their distributed
    DL platforms²²2Our source code is publicly available at https://github.com/HKBU-HPML/ddl-benchmarks..
    Our main observations through this study are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近提出了许多工作来改进分布式深度学习（DL）的可扩展性。在本文中，我们制定了一个分类法来描述分布式DL中的通信高效技术，并对BSP风格训练算法的通信优化技术进行了定量调查。我们确定了模型强度和批量大小是影响系统可扩展性的两个关键因素，并进行了定量研究，以比较在32-GPU集群和100Gbps
    IB下的七种最先进的分布式训练方法。我们的评估方法和结果可以为从业者设计他们的分布式DL平台提供参考²²2我们的源代码可以在 https://github.com/HKBU-HPML/ddl-benchmarks.
    上公开获取。通过这项研究，我们的主要观察结果如下：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A model with low model intensity and small batch size (thus a high communication-to-computation
    ratio) is difficult to scale out.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型强度低且批量大小小（因此通信与计算比率高）的模型难以扩展。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The decentralized A2A architecture is more latency-sensitive than the centralized
    PS architecture, but the latter requires extra servers and network ports to achieve
    good performance.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 去中心化的A2A架构对延迟更敏感，而中心化的PS架构需要额外的服务器和网络端口才能实现良好的性能。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Scheduling algorithms can be useful to hide the communication costs in both
    PS and A2A architectures. In particular, tensor fusion is suitable for A2A, while
    tensor partition is more suitable for PS.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调度算法可以用于隐藏PS和A2A架构中的通信成本。特别地，张量融合适用于A2A，而张量分区更适用于PS。
- en: The remainder of this article is organized as follows. We first identify the
    communication issues and existing solutions in distributed DL. Then we elaborate
    commonly used communication optimization techniques, followed by our experimental
    study. Finally, we discuss the challenges and possible future research directions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。我们首先识别分布式深度学习中的通信问题和现有解决方案。然后详细说明常用的通信优化技术，接着是我们的实验研究。最后，我们讨论挑战和可能的未来研究方向。
- en: 2 Communication Issues and Solutions
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 通信问题与解决方案
- en: 2.1 Scope, Assumptions, and Terminologies
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 范围、假设和术语
- en: In this article, we mainly discuss the communication issues in data parallel
    distributed DL, and focus on the data center or HPC environments where network
    speed is high and stable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们主要讨论数据并行分布式深度学习中的通信问题，重点关注网络速度高且稳定的数据中心或高性能计算环境。
- en: In a typical data parallel distributed DL (e.g., BSP-SGD), each training iteration
    consists of several steps. First, each worker loads a mini-batch of data as the
    input and performs feed-forward calculations to calculate the loss value against
    the corresponding labels. Next, each worker backpropagates the loss and calculates
    the first-order gradients of model parameters. The local gradients are aggregated
    among all workers, and the averaged gradients are finally used to update the model
    parameters. The algorithm proceeds to the next iteration, until a certain convergence
    condition is met. In this article, we assume data I/O can be overlapped with the
    computations, and hence will not consider the data I/O time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的数据并行分布式深度学习（例如 BSP-SGD）中，每次训练迭代包括几个步骤。首先，每个工作节点加载一个小批量的数据作为输入，并进行前向计算以计算与相应标签的损失值。接下来，每个工作节点进行反向传播以计算模型参数的一阶梯度。所有工作节点的本地梯度被聚合，最终使用平均梯度来更新模型参数。算法进入下一次迭代，直到满足某个收敛条件。在本文中，我们假设数据
    I/O 可以与计算重叠，因此不考虑数据 I/O 时间。
- en: Consider a training job of a deep model with $D$ parameters that uses SGD with
    a mini-batch size of $M$. Assume the number of arithmetic operations required
    for a single data sample in each training iteration is $C$. A data parallelism
    solution with $N$ workers will distribute the $MC$ arithmetic operations to the
    $N$ workers (e.g., each worker has a local mini-batch size of $M/N$). In the simplest
    case where communication tasks do not overlap with computing tasks, the speedup
    achieved by $N$ workers is $\frac{t_{s}}{t_{s}/N+t_{m}}$, where $t_{s}$ is the
    computing time with a single worker, and $t_{m}$ is the communication time of
    distributed training with $N$ workers. As $N$ becomes larger, the speedup approaches
    $t_{s}/t_{m}$, which explains the significance of communication optimization in
    distributed DL. To eliminate the impact of computing speed and communication speed
    on the analysis of speedup, we define communication-to-computation (C2C) ratio
    of a distributed training job as the total amount of communication traffic divided
    by the total amount of computations. Due to the dependency between communication
    tasks and computation tasks (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Quantitative
    Survey of Communication Optimizations in Distributed Deep Learning")(b)), C2C
    ratio is the key factor that affects the system scalability.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有 $D$ 个参数的深度模型训练任务，使用小批量大小为 $M$ 的 SGD。假设每次训练迭代中每个数据样本所需的算术操作数量为 $C$。一个拥有
    $N$ 个工作节点的数据并行解决方案将把 $MC$ 个算术操作分配到 $N$ 个工作节点（例如，每个工作节点的本地小批量大小为 $M/N$）。在通信任务与计算任务不重叠的最简单情况下，$N$
    个工作节点实现的加速比为 $\frac{t_{s}}{t_{s}/N+t_{m}}$，其中 $t_{s}$ 是单个工作节点的计算时间，$t_{m}$ 是 $N$
    个工作节点分布式训练的通信时间。随着 $N$ 的增大，加速比接近 $t_{s}/t_{m}$，这解释了通信优化在分布式深度学习中的重要性。为了消除计算速度和通信速度对加速比分析的影响，我们定义了分布式训练任务的通信到计算（C2C）比率，即通信流量总量除以计算总量。由于通信任务和计算任务之间的依赖关系（见图
    [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 分布式深度学习中通信优化的定量调研")(b)），C2C 比率是影响系统可扩展性的关键因素。
- en: In practice, the total amount of communication traffic is linearly proportional
    to the model size $D$ and also depends on the number of workers $N$. So we can
    use $D\cdot f(N)$ to model the amount of communication³³3For simplicity, the unit
    of communication is the size of one model parameter or gradient. But in practice,
    the size of a model parameter could be different from the size of a gradient.
    where $f(N)$ depends on the communication scheme. The C2C ratio can then be calculated
    by $\frac{D\cdot f(N)}{M\cdot C}$. We define model intensity $I=\frac{C}{D}$,
    which is the average number of arithmetic operations in an iteration per data
    sample per model parameter. Here, $I$ is an intrinsic feature of the model that
    captures the difficulty of parallelism. The C2C ratio can then be simplified as
    $\frac{f(N)}{M\cdot I}$. Our experimental results in Section [4](#S4 "4 Comparative
    Study ‣ A Quantitative Survey of Communication Optimizations in Distributed Deep
    Learning") verify that a model with low intensity $I$ and/or small batch size
    $M$ is difficult to scale. To reduce the C2C ratio of a given DL model, we need
    to design good communication schemes with small $f(N)$ and choose a large batch
    size $M$.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，总通信流量与模型大小$D$成线性比例，还依赖于工作节点的数量$N$。因此，我们可以使用$D\cdot f(N)$来建模通信量³³3为了简单起见，通信的单位是一个模型参数或梯度的大小。但实际上，模型参数的大小可能与梯度的大小不同。这里的$f(N)$取决于通信方案。然后，C2C比率可以通过$\frac{D\cdot
    f(N)}{M\cdot C}$计算。我们定义模型强度为$I=\frac{C}{D}$，它是每次迭代中每个数据样本每个模型参数的平均算术运算次数。这里的$I$是模型的内在特征，捕捉了并行性的难度。然后，C2C比率可以简化为$\frac{f(N)}{M\cdot
    I}$。我们在第[4](#S4 "4 Comparative Study ‣ A Quantitative Survey of Communication
    Optimizations in Distributed Deep Learning")节中的实验结果验证了低强度$I$和/或小批量大小$M$的模型很难扩展。为了降低给定深度学习模型的C2C比率，我们需要设计具有小$f(N)$的良好通信方案，并选择较大的批量大小$M$。
- en: 2.2 Communication Issues
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 通信问题
- en: We use the BERT-Large language model (with 336 million parameters) as an example
    to illustrate the communication challenges in distributed training. Given a local
    batch size of 8 (which is limited by the available GPU memory size), each iteration
    requires $597\times 10^{9}$ floating point operations (FLOPs) which take 163ms
    on an Nvidia RTX2080Ti. There are several communication challenges that limit
    the system scalability of distributed training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以BERT-Large语言模型（具有3.36亿个参数）为例来说明分布式训练中的通信挑战。给定一个本地批量大小为8（受限于可用GPU内存大小），每次迭代需要$597\times
    10^{9}$次浮点运算（FLOPs），在Nvidia RTX2080Ti上需要163ms。存在一些通信挑战限制了分布式训练系统的可扩展性。
- en: 'Communication Size: In each training iteration, the whole set of model parameters
    or their gradients should be exchanged across all workers. The BERT-Large model
    has a size of 1.34GB if the parameters are stored in a 32-bit format. Given $N$
    workers, finding the average of $N$ sets of data and synchronizing the updated
    model within a short time period can be very challenging. For instance, when training
    BERT-Large on a server with 4 RTX2080Ti connected through PCIe 3.0, each iteration
    requires 441ms of communication time for the all-reduce operations, resulting
    in a poor speedup of $1.08\times$.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通信大小：在每次训练迭代中，所有的模型参数或其梯度需要在所有工作节点之间交换。如果参数以32位格式存储，则BERT-Large模型的大小为1.34GB。考虑到$N$个工作节点，在短时间内找到$N$组数据的平均值并同步更新后的模型是非常具有挑战性的。例如，当在连接通过PCIe
    3.0的4个RTX2080Ti的服务器上训练BERT-Large时，每次迭代需要441ms的通信时间进行全归约操作，导致了$1.08\times$的较差加速。
- en: 'Communication Performance: Deep models have a layered structure, and the parameters
    and their corresponding gradients are typically stored as tens to hundreds of
    tensors. First of all, these tensors are calculated layer by layer on the fly,
    creating intrinsic time dependency that limits the design space of scheduling
    computing and communication tasks. Second, the tensor size ranges from kilo-bytes
    to mega-bytes. It is difficult to fully utilize the high network bandwidth when
    exchanging small messages [[3](#bib.bib3)]. For example, in our testbed, transmitting
    1MB of message across the 10GbE (TCP/IP), 100GbE (TCP/IP), and 100GbIB (RDMA)
    achieves an effective throughput of 8.2Gbps, 16.5Gbps, and 83.2Gbps, respectively;
    while transmitting a smaller message of 16KB across the 10GbE, 100GbE, and 100GbIB
    can only achieve much lower throughput of 1.2Gbps, 4.6Gbps, and 16.7Gbps, respectively.
    Optimally exchanging various tensors among a set of workers requires a co-design
    of message exchange algorithm and network system architecture that considers both
    bandwidth and communication latency.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通信性能：深度模型具有分层结构，参数及其对应的梯度通常存储为数十到数百个张量。首先，这些张量在运算中逐层计算，从而产生内在的时间依赖性，限制了调度计算和通信任务设计空间。其次，张量的大小范围从千字节到兆字节不等。在交换小消息时，很难充分利用高网络带宽[[3](#bib.bib3)]。例如，在我们的测试环境中，通过10GbE（TCP/IP）、100GbE（TCP/IP）和100GbIB（RDMA）传输1MB的消息，分别实现了有效吞吐量为8.2Gbps、16.5Gbps和83.2Gbps；而在10GbE、100GbE和100GbIB之间传输较小的16KB消息时，只能实现较低的吞吐量，分别为1.2Gbps、4.6Gbps和16.7Gbps。在一组工作者之间优化地交换各种张量，需要同时考虑带宽和通信延迟的消息交换算法和网络系统架构的联合设计。
- en: '![Refer to caption](img/9fbc35aa7d6682760dd8c7b93e7ba735.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参阅说明](img/9fbc35aa7d6682760dd8c7b93e7ba735.png)'
- en: 'Figure 2: A three-level taxonomy of communication-efficient distributed DL.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：通信高效的分布式深度学习的三级分类法。
- en: 2.3 Solutions
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 解决方案
- en: 'There have been three different directions taken to address the above challenges:
    1) reducing the C2C ratio, 2) overlapping the communication tasks with the computation
    tasks, and 3) improving the communication performance by the advanced design of
    system architectures and communication primitives. In Fig. [2](#S2.F2 "Figure
    2 ‣ 2.2 Communication Issues ‣ 2 Communication Issues and Solutions ‣ A Quantitative
    Survey of Communication Optimizations in Distributed Deep Learning"), we develop
    a three-level taxonomy to describe communication-efficient distributed DL.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述挑战，已经采取了三个不同的方向：1）减小C2C比率，2）将通信任务与计算任务重叠，3）通过先进的系统架构和通信原语的设计来改进通信性能。在图[2](#S2.F2
    "图2 ‣ 2.2 通信问题 ‣ 2 通信问题和解决方案 ‣ 关于分布式深度学习通信优化的定量调查") 中，我们制定了一个三级分类法来描述通信高效的分布式深度学习。
- en: 2.3.1 Learning Algorithms
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 学习算法
- en: 'At the top, there are high-level learning algorithms with different communication
    complexity (aiming to reduce the C2C ratio), which can be classified into two
    types: 1) increasing the workload of computation (e.g., large-batch training [[4](#bib.bib4)])
    and 2) reducing the communication complexity by quantization and/or sparsification.
    These algorithms are usually lossy in the sense that they generate inconsistent
    results with the single-worker SGD. Lossy algorithms may need more iterations
    to achieve the same level of convergence, though each iteration completes faster.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶层是具有不同通信复杂度的高级学习算法（旨在减小C2C比率），可分为两种类型：1）增加计算的工作量（例如，大批量训练[[4](#bib.bib4)））和2）通过量化和/或稀疏化减少通信复杂度。这些算法通常是有损的，因为它们产生与单工作者随机梯度下降不一致的结果。有损算法可能需要更多的迭代才能达到相同的收敛水平，尽管每次迭代完成得更快。
- en: Large-batch training is an immediate way to reduce the C2C ratio by enlarging
    the batch size. With proper optimization tricks (e.g., layer-wise adaptive rate
    scaling), large-batch training can maintain the same generalization ability as
    single-worker SGD. However, the local batch size is limited by the memory size
    of the AI processor.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 大批量训练是通过增大批量大小来减小C2C比率的一种直接方法。通过适当的优化技巧（例如，分层自适应率缩放），大批量训练可以保持与单工作者随机梯度下降相同的泛化能力。然而，本地批量大小受AI处理器内存大小的限制。
- en: We can also relax the synchronization or reduce the communication frequency
    among workers (e.g., staled synchronized parallel (SSP) [[5](#bib.bib5)], local
    SGD [[4](#bib.bib4)], and asynchronous parallel (ASP) [[6](#bib.bib6)] SGD). SSP
    SGD allows some workers to run more iterations before synchronization, which is
    efficient in a heterogeneous environment where different workers have different
    computing horsepower. Local SGD allows all workers to run a specific number of
    local updates independently before synchronization. ASP SGD enables all workers
    to train the model without waiting for any other workers to update the model parameters.
    Compression techniques such as gradient quantization [[7](#bib.bib7)] and sparsification
    [[8](#bib.bib8)] are another thread of lossy algorithms. Gradient quantization
    quantifies each gradient into a few bits with little impact on the convergence,
    while gradient sparsification selects a small portion of the gradients for model
    updates.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以放宽同步要求或减少工作节点之间的通信频率（例如，过时的同步并行（SSP）[[5](#bib.bib5)]、局部SGD [[4](#bib.bib4)]
    和异步并行（ASP）[[6](#bib.bib6)] SGD）。SSP SGD 允许某些工作节点在同步之前运行更多的迭代，这在不同计算能力的工作节点的异质环境中是高效的。局部SGD
    允许所有工作节点在同步之前独立运行特定数量的本地更新。ASP SGD 使所有工作节点可以在不等待其他工作节点更新模型参数的情况下训练模型。压缩技术如梯度量化
    [[7](#bib.bib7)] 和稀疏化 [[8](#bib.bib8)] 也是一种有损算法的线程。梯度量化将每个梯度量化为几个比特，对收敛影响很小，而梯度稀疏化则选择一小部分梯度用于模型更新。
- en: 2.3.2 System Architectures
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 系统架构
- en: The middle level is the system architectures that define how the workers exchange
    the information. Parameter server (PS) (e.g., [[2](#bib.bib2)]) and all-to-all
    (A2A) (e.g., [[3](#bib.bib3)]) are the two most popular system architectures,
    and they can be equipped with different communication scheduling algorithms that
    can either overlap communication with computation or improve the communication
    performance by tensor fusion/partition. PS is a centralized architecture that
    requires one or more central servers to manage the model parameters, while A2A
    is a decentralized architecture that exploits message passing interface (MPI)
    or alike to perform data communication tasks. The optimization techniques in this
    level are usually lossless as they don’t change the training results of the learning
    algorithms.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层是定义工作节点如何交换信息的系统架构。参数服务器（PS）（例如，[[2](#bib.bib2)]）和全对全（A2A）（例如，[[3](#bib.bib3)]）是两种最流行的系统架构，它们可以配备不同的通信调度算法，这些算法可以重叠通信与计算，或通过张量融合/分区来提高通信性能。PS
    是一种集中式架构，需要一个或多个中央服务器来管理模型参数，而 A2A 是一种分散式架构，利用消息传递接口（MPI）或类似技术来执行数据通信任务。这一层的优化技术通常是无损的，因为它们不会改变学习算法的训练结果。
- en: 2.3.3 Communication Infrastructure
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 通信基础设施
- en: At the bottom level, there are diverse communication infrastructures offering
    the fundamental data communication services, which include communication protocols
    and network topologies. The optimization techniques in this level are also lossless.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在最底层，有多种通信基础设施提供基本的数据通信服务，包括通信协议和网络拓扑。在这一层的优化技术通常是无损的。
- en: Popular communication protocols are TCP/IP, RDMA on InfiniBand, and RoCE. TCP/IP
    is widely supported by commodity Ethernet switches. However, it is inefficient
    for high-speed data communications due to the cost of data copy between the kernel
    buffer and the application buffer. RDMA can deliver lower latency and higher throughput
    than TCP/IP [[9](#bib.bib9)]. RDMA was originally run on InfiniBand, while RoCE
    (RDMA over converged Ethernet) enables the cheaper Ethernet to support RDMA. Network
    topology design is also important to improve the performance of distributed DL.
    E.g., Wang et al. [[10](#bib.bib10)] showed that BCube is more suitable than Fat-tree
    for distributed DL.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的通信协议有 TCP/IP、InfiniBand 上的 RDMA 和 RoCE。TCP/IP 被商品以太网交换机广泛支持。然而，由于内核缓冲区和应用程序缓冲区之间的数据拷贝成本，它在高速数据通信中效率较低。RDMA
    能提供比 TCP/IP 更低的延迟和更高的吞吐量 [[9](#bib.bib9)]。RDMA 最初在 InfiniBand 上运行，而 RoCE（通过以太网的
    RDMA）使得更便宜的以太网能够支持 RDMA。网络拓扑设计对于提高分布式深度学习的性能也很重要。例如，王等人 [[10](#bib.bib10)] 显示
    BCube 比 Fat-tree 更适合用于分布式深度学习。
- en: 'In summary, a distributed training method may involve six different aspects:
    <svg   height="13.74" overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg> Communication
    Synchronization, <svg height="13.74" overflow="visible" version="1.1" width="13.74"><g
    transform="translate(0,13.74) matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>
    Communication Compression, <svg   height="13.74" overflow="visible" version="1.1"
    width="13.74"><g transform="translate(0,13.74) matrix(1 0 0 -1 0 0) translate(6.87,0)
    translate(0,6.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    System Architecture, <svg   height="13.74" overflow="visible" version="1.1" width="13.74"><g
    transform="translate(0,13.74) matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>
    Scheduling, <svg   height="13.74" overflow="visible" version="1.1" width="13.74"><g
    transform="translate(0,13.74) matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>
    Communication Protocol, and <svg   height="13.74" overflow="visible" version="1.1"
    width="13.74"><g transform="translate(0,13.74) matrix(1 0 0 -1 0 0) translate(6.87,0)
    translate(0,6.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>
    Network Topology. This can be described as “it exploits <svg height="13.74" overflow="visible"
    version="1.1" width="13.74"><g transform="translate(0,13.74) matrix(1 0 0 -1 0
    0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    with/without <svg   height="13.74" overflow="visible" version="1.1" width="13.74"><g
    transform="translate(0,13.74) matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>,
    running on <svg   height="13.74" overflow="visible" version="1.1" width="13.74"><g
    transform="translate(0,13.74) matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
    with/without <svg   height="13.74" overflow="visible" version="1.1" width="13.74"><g
    transform="translate(0,13.74) matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>
    building on <svg   height="13.74" overflow="visible" version="1.1" width="13.74"><g
    transform="translate(0,13.74) matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg>
    and <svg height="13.74" overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg>.” In practice,
    BSP SGD with large-batch training is more popular than the other learning algorithms
    due to its good convergence property. Therefore, given a GPU cluster with a fixed
    communication infrastructure, the system architecture and scheduling algorithms
    become the key communication optimization techniques to improve the system scalability.
    In the next section, we continue to discuss the impact of system architectures
    and scheduling algorithms on the performance of distributed DL.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，分布式训练方法可能涉及六个不同方面：<svg height="13.74" overflow="visible" version="1.1"
    width="13.74"><g transform="translate(0,13.74) matrix(1 0 0 -1 0 0) translate(6.87,0)
    translate(0,6.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg>
    通信同步，<svg height="13.74" overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg> 通信压缩，<svg height="13.74"
    overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg> 系统架构，<svg height="13.74"
    overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg> 调度，<svg height="13.74"
    overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg> 通信协议，以及 <svg
    height="13.74" overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">6</foreignobject></g></g></svg> 网络拓扑。这可以描述为“它利用
    <svg height="13.74" overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></svg> 有/没有 <svg height="13.74"
    overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g></svg>，在 <svg height="13.74"
    overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg> 上运行，或有/没有 <svg
    height="13.74" overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>，在 <svg height="13.74"
    overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">5</foreignobject></g></g></svg> 和 <svg height="13.74"
    overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject
- en: 3 A Popular Communication Optimization Portfolio
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 一个流行的通信优化组合
- en: '![Refer to caption](img/9c186ea2a1567cb585d865b5afe0b424.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9c186ea2a1567cb585d865b5afe0b424.png)'
- en: 'Figure 3: A communication optimization portfolio in distributed DL.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：分布式深度学习中的通信优化组合。
- en: In this section, we focus on the communication optimization techniques in system
    architecture design and scheduling algorithms. These techniques are lossless,
    making them particularly appealing to industry practitioners because model accuracy
    is the most important for many AI applications. Fig. [3](#S3.F3 "Figure 3 ‣ 3
    A Popular Communication Optimization Portfolio ‣ A Quantitative Survey of Communication
    Optimizations in Distributed Deep Learning") gives an illustration of the communication
    optimization techniques.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们重点讨论系统架构设计和调度算法中的通信优化技术。这些技术是无损的，使它们对于行业从业者特别具有吸引力，因为模型准确性对许多AI应用来说是最重要的。图[3](#S3.F3
    "Figure 3 ‣ 3 A Popular Communication Optimization Portfolio ‣ A Quantitative
    Survey of Communication Optimizations in Distributed Deep Learning")展示了通信优化技术的示意图。
- en: 3.1 System Architectures
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 系统架构
- en: PS and A2A represent two different design philosophies, with different communication
    properties.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PS和A2A代表了两种不同的设计理念，它们具有不同的通信特性。
- en: 3.1.1 Parameter Server (PS)
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 参数服务器（PS）
- en: In the PS architecture, a PS is logically a central server that aggregates the
    gradients from the workers, updates the model parameters, and sends back the latest
    model to the workers. It provides a simple and flexible framework for the system
    implementation. However, since PS needs to receive gradients from and send parameters
    (or averaged gradients) to all workers, it could easily become the system bottleneck
    in the BSP algorithm where all workers communicate with the PS almost simultaneously.
    With a single PS, the communication traffic is $2D$ for each worker and $2ND$
    for the PS. To alleviate the communication pressure on a single PS, one can deploy
    multiple PSes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在PS架构中，PS在逻辑上是一个中央服务器，聚合来自工作节点的梯度，更新模型参数，并将最新模型发送回工作节点。它为系统实现提供了一个简单而灵活的框架。然而，由于PS需要接收来自所有工作节点的梯度，并将参数（或平均梯度）发送到所有工作节点，它可能很容易成为BSP算法中的系统瓶颈，其中所有工作节点几乎同时与PS进行通信。使用单个PS时，每个工作节点的通信流量为$2D$，PS的通信流量为$2ND$。为了缓解单个PS的通信压力，可以部署多个PS。
- en: Here we introduce a representative PS implementation called BytePS⁴⁴4https://github.com/bytedance/byteps,
    a highly optimized framework that supports multiple PSes by partitioning the gradient
    tensors in a load-balanced manner. Given $S$ PSes, the $D$-dimensional gradient
    is partitioned into $D/S$ parts so that each PS receives $ND/S$ gradients from
    $N$ workers. The received $N$ gradient tensors are averaged on the server side
    and sent back to all $N$ workers. Therefore, the communication traffic of each
    PS is reduced to $2ND/S$.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们介绍一个代表性的PS实现，称为BytePS⁴⁴4https://github.com/bytedance/byteps，这是一个高度优化的框架，通过均衡负载的方式对梯度张量进行分区，支持多个PS。给定$S$个PS，$D$维梯度被划分为$D/S$部分，以便每个PS从$N$个工作节点接收$ND/S$个梯度。接收到的$N$个梯度张量在服务器端进行平均，并返回给所有$N$个工作节点。因此，每个PS的通信流量减少到$2ND/S$。
- en: 3.1.2 All-to-all (A2A)
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 全到全（A2A）
- en: The average of the distributed gradient or parameter tensors can be calculated
    by an A2A operation, e.g., the all-reduce primitive in MPI. The ring-based all-reduce
    collective is commonly used in distributed DL, which is bandwidth optimal by dividing
    the tensors into small messages and exchanging those messages simultaneously in
    a pipelined manner. However, ring-based all-reduce has a latency term that is
    linear to the number of workers, which becomes inefficient for large clusters.
    In the high-performance communication library (NCCL⁵⁵5https://developer.nvidia.com/nccl),
    the double binary trees algorithm [[11](#bib.bib11)] is integrated for dense-GPU
    clusters, which delivers a logarithmic latency while preserving the bandwidth
    optimality. For some imbalance network bandwidth systems, using the hierarchy
    of communication bandwidths could further improve the communication efficiency
    [[12](#bib.bib12)].
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过A2A操作计算分布式梯度或参数张量的平均值，例如MPI中的all-reduce原语。在分布式深度学习中，环形all-reduce集合是常用的，它通过将张量划分为小消息并以流水线方式同时交换这些消息来达到带宽最优。然而，基于环的all-reduce有一个与工作节点数量成线性关系的延迟项，这在大型集群中会变得低效。在高性能通信库（NCCL⁵⁵5https://developer.nvidia.com/nccl）中，双二叉树算法[[11](#bib.bib11)]被集成用于密集型GPU集群，这在保持带宽最优性的同时提供对数级延迟。对于一些带宽不平衡的网络系统，使用通信带宽的层次结构可以进一步提高通信效率[[12](#bib.bib12)]。
- en: Horovod⁶⁶6https://github.com/horovod/horovod is a popular distributed DL framework
    built for the A2A architecture and supports many state-of-the-art distributed
    communication libraries (e.g., MPI, NCCL, and Gloo⁷⁷7https://github.com/facebookincubator/gloo).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Horovod⁶⁶6https://github.com/horovod/horovod 是一个流行的分布式深度学习框架，构建于A2A架构，并支持许多先进的分布式通信库（例如，MPI、NCCL
    和 Gloo⁷⁷7https://github.com/facebookincubator/gloo）。
- en: 3.2 Scheduling
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 调度
- en: During the training process of distributed DL, the computing and communication
    tasks can be described by a DAG. The layer-wise (or tensor-wise) structure of
    deep models makes it possible to schedule different tasks intelligently so that
    part of the communication cost can be hidden, as shown in Fig. [3](#S3.F3 "Figure
    3 ‣ 3 A Popular Communication Optimization Portfolio ‣ A Quantitative Survey of
    Communication Optimizations in Distributed Deep Learning") <svg height="13.74"
    overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg>.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式深度学习的训练过程中，计算和通信任务可以用DAG来描述。深度模型的层级（或张量级）结构使得可以智能地调度不同的任务，从而隐藏部分通信开销，如图
    [3](#S3.F3 "Figure 3 ‣ 3 A Popular Communication Optimization Portfolio ‣ A Quantitative
    Survey of Communication Optimizations in Distributed Deep Learning") <svg height="13.74"
    overflow="visible" version="1.1" width="13.74"><g transform="translate(0,13.74)
    matrix(1 0 0 -1 0 0) translate(6.87,0) translate(0,6.87)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000"
    stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g></svg> 所示。
- en: 3.2.1 Layer-wise Pipelining and Tensor Fusion
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 层级流水线和张量融合
- en: A deep model consists of a stack of layers, and the learnable parameters of
    each layer are generally represented by one or two tensors. During the backpropagation,
    if the gradients of layer $P$ have been computed, then they can be immediately
    communicated so that the communication task can be pipelined with the computing
    task of layer $P-1$. The naive pipelining between communications and computations
    during backpropagation is also called wait-free backpropagation (WFBP) [[13](#bib.bib13)],
    which can be applied to both PS and A2A architectures.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 深度模型由一系列层组成，每层的可学习参数通常由一个或两个张量表示。在反向传播过程中，如果层 $P$ 的梯度已经计算完成，则可以立即进行通信，从而使通信任务与层
    $P-1$ 的计算任务进行流水线处理。反向传播过程中通信和计算之间的简单流水线也称为无等待反向传播（WFBP） [[13](#bib.bib13)]，这种方法可以应用于PS和A2A架构。
- en: In A2A with pipelining, an all-reduce operation is required for each tensor,
    which usually divides the tensor into multiple small messages. Considering that
    transmitting two small messages together is generally faster than transmitting
    the two messages separately (e.g., in our 100Gbps InfiniBand cluster, transmitting
    a 16KiB message takes 7.85us, while transmitting a 32KiB message takes 10.1us),
    the MG-WFBP algorithm adopts the idea of tensor fusion by optimally merging the
    gradients of several consecutive layers to minimize the iteration time [[3](#bib.bib3)].
    Tensor fusion can effectively alleviate the negative impact of transmitting small
    messages.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在带有流水线的A2A中，每个张量需要进行全归约操作，这通常将张量划分为多个小消息。考虑到同时传输两个小消息通常比分别传输两个消息更快（例如，在我们的100Gbps
    InfiniBand集群中，传输一个16KiB的消息需要7.85微秒，而传输一个32KiB的消息需要10.1微秒），MG-WFBP算法通过最优地合并几个连续层的梯度来实现张量融合，以最小化迭代时间
    [[3](#bib.bib3)]。张量融合可以有效缓解传输小消息带来的负面影响。
- en: 3.2.2 Tensor Partitioning and Priority Scheduling
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 张量划分和优先级调度
- en: 'In the PS architecture, the communication happens between a worker and a PS
    and a tensor can be transmitted as a single message, making tensor fusion less
    beneficial than in A2A. Other than pipelining, there is another opportunity for
    performance improvement by priority scheduling. In PS, there are two directions
    of communications: push of gradients and pull of parameters. For each layer, the
    pull of parameters is commonly followed by the push of gradients. If the current
    layer has a large tensor, it would block other layers with small tensors. ByteScheduler [[14](#bib.bib14)]
    is the efficient scheduling strategy that partitions a large tensor into multiple
    smaller ones and allows the lower layers to be scheduled ahead of the higher layers.
    By using the priority scheduling, it is possible to overlap the communication
    tasks with both feed-forward and backpropagation computing tasks [[15](#bib.bib15),
    [14](#bib.bib14)].'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在PS架构中，通信发生在工作节点和PS之间，张量可以作为单一消息传输，使得张量融合的效果不如A2A。除了流水线处理外，优先级调度提供了另一种性能提升的机会。在PS中，有两个通信方向：梯度推送和参数拉取。对于每一层，参数拉取通常紧随梯度推送。如果当前层的张量很大，它会阻塞其他小张量的层。ByteScheduler [[14](#bib.bib14)]是一种高效的调度策略，它将大张量划分为多个较小的张量，并允许较低层先于较高层进行调度。通过优先级调度，可以将通信任务与前向传播和反向传播计算任务重叠 [[15](#bib.bib15),
    [14](#bib.bib14)]。
- en: 4 Comparative Study
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 比较研究
- en: To demonstrate the key factors that affect the scalability of the optimization
    portfolio presented in Section [3](#S3 "3 A Popular Communication Optimization
    Portfolio ‣ A Quantitative Survey of Communication Optimizations in Distributed
    Deep Learning"), we evaluate and compare the system performance of seven representative
    distributed training methods listed in Table [I](#S4.T1 "TABLE I ‣ 4 Comparative
    Study ‣ A Quantitative Survey of Communication Optimizations in Distributed Deep
    Learning"), which are widely used in practice and serve as good examples to quantitatively
    study different optimization techniques. BSP-PS and BSP-A2A are the baseline cases
    without special optimization, which are used to compare the efficiency of PS and
    A2A. WFBP-PS and WFBP-A2A are with WFBP scheduling, which can evaluate the effectiveness
    of WFBP on different architectures. MG-WFBP uses tensor fusion to address the
    latency problem of WFBP-A2A. ByteScheduler-PS and ByteScheduler-A2A are with both
    pipelining and tensor partition under PS and A2A architectures respectively, which
    can show the performance of tensor partition.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示第[3](#S3 "3 A Popular Communication Optimization Portfolio ‣ A Quantitative
    Survey of Communication Optimizations in Distributed Deep Learning")节中提出的优化组合的关键因素，我们评估并比较了表[I](#S4.T1
    "TABLE I ‣ 4 Comparative Study ‣ A Quantitative Survey of Communication Optimizations
    in Distributed Deep Learning")中列出的七种具有代表性的分布式训练方法，这些方法在实践中广泛使用，是定量研究不同优化技术的良好示例。BSP-PS和BSP-A2A是没有特殊优化的基线情况，用于比较PS和A2A的效率。WFBP-PS和WFBP-A2A采用WFBP调度，评估WFBP在不同架构上的有效性。MG-WFBP使用张量融合解决WFBP-A2A的延迟问题。ByteScheduler-PS和ByteScheduler-A2A在PS和A2A架构下分别采用流水线和张量划分，可以展示张量划分的性能。
- en: We choose three representative deep models for evaluation, namely ResNet-50,
    BERT-Base, and BERT-Large, which are commonly used in image classification and
    natural language processing. Their model intensities are 470, 249, and 248, respectively.
    On RTX2080Ti, ResNet-50 and BERT-Base can support a local batch size of 64, while
    BERT-Large can only support 8\. These three models can well illustrate the impact
    of model intensity and batch size on the system scalability.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了三个具有代表性的深度模型进行评估，即ResNet-50、BERT-Base和BERT-Large，这些模型通常用于图像分类和自然语言处理。它们的模型强度分别为470、249和248。在RTX2080Ti上，ResNet-50和BERT-Base支持本地批量大小为64，而BERT-Large只能支持8。
    这三个模型可以很好地说明模型强度和批量大小对系统可扩展性的影响。
- en: 'TABLE I: Experimental Settings for Evaluation. For BytePS, as suggested by
    the official release, we use multiple PSes whose amount is the same as the number
    of worker servers. Each worker server has multiple workers (i.e., GPUs).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：评估的实验设置。对于BytePS，按照官方发布的建议，我们使用的PS数量与工作服务器的数量相同。每个工作服务器有多个工作节点（即GPU）。
- en: '| Method | System Architecture | Scheduling | Distributed Software | Common
    Libraries |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 系统架构 | 调度 | 分布式软件 | 常用库 |'
- en: '| PS/All-to-all | Pipelining | Tensor Fusion | Tensor Partition |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| PS/全到全 | 流水线 | 张量融合 | 张量划分 |'
- en: '| BSP-PS [[13](#bib.bib13)] | PS | ✘ | ✘ | ✘ | BytePS | PyTorch-1.4 CUDA-10.1
    NCCL-2.4.8 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| BSP-PS [[13](#bib.bib13)] | PS | ✘ | ✘ | ✘ | BytePS | PyTorch-1.4 CUDA-10.1
    NCCL-2.4.8 |'
- en: '| BSP-A2A [[3](#bib.bib3), [11](#bib.bib11)] | All-to-all | ✘ | ✘ | ✘ | Horovod
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| BSP-A2A [[3](#bib.bib3), [11](#bib.bib11)] | 全到全 | ✘ | ✘ | ✘ | Horovod |'
- en: '| WFBP-PS [[13](#bib.bib13)] | PS | ✔ | ✘ | ✘ | BytePS |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| WFBP-PS [[13](#bib.bib13)] | PS | ✔ | ✘ | ✘ | BytePS |'
- en: '| WFBP-A2A [[3](#bib.bib3), [11](#bib.bib11)] | All-to-all | ✔ | ✘ | ✘ | Horovod
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| WFBP-A2A [[3](#bib.bib3), [11](#bib.bib11)] | 全到全 | ✔ | ✘ | ✘ | Horovod |'
- en: '| MG-WFBP [[3](#bib.bib3)] | All-to-all | ✔ | ✔ | ✘ | Horovod |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| MG-WFBP [[3](#bib.bib3)] | 全到全 | ✔ | ✔ | ✘ | Horovod |'
- en: '| ByteScheduler-PS [[14](#bib.bib14)] | PS | ✔ | ✘ | ✔ | BytePS |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| ByteScheduler-PS [[14](#bib.bib14)] | PS | ✔ | ✘ | ✔ | BytePS |'
- en: '| ByteScheduler-A2A [[14](#bib.bib14)] | All-to-all | ✔ | ✘ | ✔ | Horovod |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ByteScheduler-A2A [[14](#bib.bib14)] | 全到全 | ✔ | ✘ | ✔ | Horovod |'
- en: 4.1 Experimental Settings
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'Hardware: We conduct experiments on a GPU cluster with RDMA over 100Gbps IB.
    The cluster consists of 8 nodes (or worker servers). Each node has four Nvidia
    RTX2080Ti GPUs (11GB RAM) interconnected by PCIe3.0 x16, two Intel(R) Xeon(R)
    Gold 6230 CPUs, and 512GB memory.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '硬件: 我们在一个 GPU 集群上进行实验，该集群通过 100Gbps IB 的 RDMA 连接。该集群由 8 个节点（或工作服务器）组成。每个节点配有四个
    Nvidia RTX2080Ti GPU（11GB RAM），通过 PCIe3.0 x16 互联，两个 Intel(R) Xeon(R) Gold 6230
    CPU 和 512GB 内存。'
- en: 'Software: We exploit PyTorch-1.4⁸⁸8https://pytorch.org/ as the backbone framework
    with GPU libraries of CUDA-10.1, cuDNN-7.6 and NCCL-2.4.8\. We use the highly
    optimized libraries of BytePS and Horovod for PS and A2A architectures, respectively.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '软件: 我们使用 PyTorch-1.4⁸⁸8https://pytorch.org/ 作为核心框架，配备 CUDA-10.1、cuDNN-7.6 和
    NCCL-2.4.8 的 GPU 库。我们分别使用高度优化的 BytePS 和 Horovod 库来支持 PS 和 A2A 架构。'
- en: 'Measurements: We use the metric of system throughput (i.e., samples per second)
    in processing the data samples to evaluate the performance. For ResNet-50, a sample
    is an image with a resolution of $224\times 224\times 3$; for BERT-Base and BERT-Large,
    a sample is a sentence with a length of $64$ words. We use the SGD training with
    a single RTX2080Ti as the baseline to calculate the speedup. Note that when comparing
    the results between different number of workers, they have different effective
    batch sizes and their convergence might be different.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '测量: 我们使用系统吞吐量（即每秒样本数）作为处理数据样本的性能评估指标。对于 ResNet-50，一个样本是分辨率为 $224\times 224\times
    3$ 的图像；对于 BERT-Base 和 BERT-Large，一个样本是长度为 $64$ 个单词的句子。我们使用单 RTX2080Ti 的 SGD 训练作为基准来计算加速比。注意，在比较不同工作节点的结果时，它们具有不同的有效批次大小，其收敛性可能不同。'
- en: 4.2 Experimental Results
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验结果
- en: '![Refer to caption](img/caca3583160c0cc85f9c082b565872ce.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/caca3583160c0cc85f9c082b565872ce.png)'
- en: (a) ResNet-50 ($I$ = 470, $LBS$ = 64)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ResNet-50 ($I$ = 470, $LBS$ = 64)
- en: '![Refer to caption](img/08b8f76fb368f402a11060540cb91f94.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/08b8f76fb368f402a11060540cb91f94.png)'
- en: (b) BERT-Base ($I$ = 249, $LBS$ = 64)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (b) BERT-Base ($I$ = 249, $LBS$ = 64)
- en: '![Refer to caption](img/b5c506e0684e1af1da88291e17ca6da0.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b5c506e0684e1af1da88291e17ca6da0.png)'
- en: (c) BERT-Large ($I$ = 248, $LBS$ = 8)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: (c) BERT-Large ($I$ = 248, $LBS$ = 8)
- en: 'Figure 4: System throughput comparison. $I$: model intensity. $LBS$: local
    batch size. The numbers on the top of the bars are the best speedups among the
    seven evaluated methods over the single-GPU SGD algorithm.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 系统吞吐量比较。$I$: 模型强度。$LBS$: 本地批次大小。条形图顶部的数字是七种评估方法中相对于单GPU SGD算法的最佳加速比。'
- en: Fig. [4](#S4.F4 "Figure 4 ‣ 4.2 Experimental Results ‣ 4 Comparative Study ‣
    A Quantitative Survey of Communication Optimizations in Distributed Deep Learning")
    depicts the experimental results, averaged over five independent experiments.
    For each run, we conduct 10 training iterations for warm-up, and another 100 iterations
    for measuring the average throughput. We summarize our major findings in Table [II](#S4.T2
    "TABLE II ‣ 4.2 Experimental Results ‣ 4 Comparative Study ‣ A Quantitative Survey
    of Communication Optimizations in Distributed Deep Learning").
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S4.F4 "图 4 ‣ 4.2 实验结果 ‣ 4 比较研究 ‣ 分布式深度学习中的通信优化定量调查") 展示了实验结果，这些结果是五次独立实验的平均值。对于每次实验，我们进行10次训练迭代以进行热身，再进行100次迭代以测量平均吞吐量。我们在表 [II](#S4.T2
    "表 II ‣ 4.2 实验结果 ‣ 4 比较研究 ‣ 分布式深度学习中的通信优化定量调查") 中总结了主要发现。
- en: 'TABLE II: Major Findings of Experimental Results.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 实验结果的主要发现。'
- en: '| Section | Related Factors | Major Findings |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | 相关因素 | 主要发现 |'
- en: '| --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [4.2.1](#S4.SS2.SSS1 "4.2.1 Impact of Model Intensity and Batch Size ‣ 4.2
    Experimental Results ‣ 4 Comparative Study ‣ A Quantitative Survey of Communication
    Optimizations in Distributed Deep Learning") | Model intensity and | 1) The model
    with higher model intensity is easier to be parallelized. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| [4.2.1](#S4.SS2.SSS1 "4.2.1 模型强度和批量大小的影响 ‣ 4.2 实验结果 ‣ 4 比较研究 ‣ 分布式深度学习中的通信优化定量调查")
    | 模型强度和 | 1) 模型强度较高的模型更容易进行并行化。 |'
- en: '|  | batch size | 2) Increasing the batch size to reduce the C2C ratio makes
    the parallelism easier, but the maximum local |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | 批量大小 | 2) 增加批量大小以减少 C2C 比率使得并行化更容易，但最大本地批量大小 |'
- en: '|  |  | batch size is limited by GPU memory. |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 批量大小受限于 GPU 内存。 |'
- en: '| [4.2.2](#S4.SS2.SSS2 "4.2.2 System Architecture: PS vs. A2A ‣ 4.2 Experimental
    Results ‣ 4 Comparative Study ‣ A Quantitative Survey of Communication Optimizations
    in Distributed Deep Learning") | PS vs. A2A | 3) There is no single winner in
    PS and A2A. Both can achieve comparable performance when enhanced |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| [4.2.2](#S4.SS2.SSS2 "4.2.2 系统架构：PS 与 A2A ‣ 4.2 实验结果 ‣ 4 比较研究 ‣ 分布式深度学习中的通信优化定量调查")
    | PS 与 A2A | 3) 在 PS 和 A2A 中没有单一的赢家。两者在增强时都能实现可比的性能。 |'
- en: '|  |  | with different optimization algorithms. But PS needs extra servers
    and network switch ports to be |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 使用不同的优化算法。但 PS 需要额外的服务器和网络交换端口。 |'
- en: '|  |  | competitive with A2A. |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 与 A2A 竞争。 |'
- en: '| [4.2.3](#S4.SS2.SSS3 "4.2.3 Scheduling ‣ 4.2 Experimental Results ‣ 4 Comparative
    Study ‣ A Quantitative Survey of Communication Optimizations in Distributed Deep
    Learning") | Scheduling | 4) Wait-free backpropagation (WFBP) can generally hide
    some communication costs. Scheduling is |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| [4.2.3](#S4.SS2.SSS3 "4.2.3 调度 ‣ 4.2 实验结果 ‣ 4 比较研究 ‣ 分布式深度学习中的通信优化定量调查")
    | 调度 | 4) 等待无关反向传播（WFBP）通常可以隐藏一些通信成本。调度是 |'
- en: '|  |  | helpful when the communication time is comparable to the computing
    time per worker. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 当通信时间与每个工作节点的计算时间相当时，效果显著。 |'
- en: '|  |  | 5) Tensor fusion (e.g., MG-WFBP) is suitable for A2A because it addresses
    the inefficiency of transmitting |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 5) 张量融合（例如，MG-WFBP）适用于 A2A，因为它解决了传输小消息的低效问题。 |'
- en: '|  |  | small messages in A2A. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  |  | A2A 中的小消息。 |'
- en: '|  |  | 6) Tensor partition (e.g., ByteScheduler) is suitable for PS, which
    makes communications better |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 6) 张量分割（例如，ByteScheduler）适用于 PS，这使得通信效果更佳。 |'
- en: '|  |  | overlapped with computations. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 与计算重叠。 |'
- en: 4.2.1 Impact of Model Intensity and Batch Size
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 模型强度和批量大小的影响
- en: 'ResNet-50 vs. BERT-Base: As the model intensity of ResNet-50 is about twice
    as large as BERT-Base, and their local batch sizes are both 64, the C2C ratio
    of ResNet-50 is around half of BERT-Base. Comparing Fig. [4](#S4.F4 "Figure 4
    ‣ 4.2 Experimental Results ‣ 4 Comparative Study ‣ A Quantitative Survey of Communication
    Optimizations in Distributed Deep Learning")(a) with Fig. [4](#S4.F4 "Figure 4
    ‣ 4.2 Experimental Results ‣ 4 Comparative Study ‣ A Quantitative Survey of Communication
    Optimizations in Distributed Deep Learning")(b), we see that ResNet-50 has much
    better scalability than BERT-Base. For example, on the intra-node training with
    4 GPUs, we can achieve an optimal speedup of $4\times$ on ResNet-50, but only
    $3.1\times$ on BERT-Base; with 32 GPUs, ResNet-50 has a speedup of $31.6\times$,
    while BERT-Base has only $23.2\times$. The results confirm that a model with higher
    intensity is easier to be parallelized.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet-50 与 BERT-Base：由于 ResNet-50 的模型强度约为 BERT-Base 的两倍，并且它们的本地批量大小都是 64，ResNet-50
    的 C2C 比率约为 BERT-Base 的一半。比较图 [4](#S4.F4 "图 4 ‣ 4.2 实验结果 ‣ 4 比较研究 ‣ 分布式深度学习中的通信优化定量调查")(a)
    与图 [4](#S4.F4 "图 4 ‣ 4.2 实验结果 ‣ 4 比较研究 ‣ 分布式深度学习中的通信优化定量调查")(b)，我们可以看到 ResNet-50
    的扩展性远优于 BERT-Base。例如，在使用 4 个 GPU 的节点内训练中，ResNet-50 实现了 $4\times$ 的最佳加速，而 BERT-Base
    只有 $3.1\times$；使用 32 个 GPU 时，ResNet-50 的加速比为 $31.6\times$，而 BERT-Base 只有 $23.2\times$。结果确认了强度更高的模型更容易并行化。
    |
- en: 'BERT-Base vs. BERT-Large: The model intensities of BERT-Base and BERT-Large
    are very close, but the local batch size for BERT-Base is $8\times$ larger than
    BERT-Large due to the smaller GPU memory footprint. Therefore, the C2C ratio of
    BERT-Large is about $8\times$ higher than BERT-Base, which makes BERT-Large much
    more difficult to be parallelized, as confirmed by comparing Fig. [4](#S4.F4 "Figure
    4 ‣ 4.2 Experimental Results ‣ 4 Comparative Study ‣ A Quantitative Survey of
    Communication Optimizations in Distributed Deep Learning")(c) with Fig. [4](#S4.F4
    "Figure 4 ‣ 4.2 Experimental Results ‣ 4 Comparative Study ‣ A Quantitative Survey
    of Communication Optimizations in Distributed Deep Learning")(b). The smaller
    speedups of BERT-Large are mainly due to the small batch size and limited bandwidth
    of PCIe3.0\. For example, 4-GPU training on BERT-Large has a maximum of $1.2\times$
    speedup, while it is $3.1\times$ for BERT-Base. The small GPU memory size of RTX2080Ti
    and the limited bandwidth of PCIe3.0 are not suitable for distributed training
    of BERT-Large. For comparison, when training BERT-Large on a much more expensive
    server with four Nvidia V100 GPUs (with 32GB memory) interconnected by NVLink
    (with more than $10\times$ higher bandwidth than PCIe3.0), the local batch size
    can be as large as 128, and we achieved a speedup of $3.82\times$.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: BERT-Base 与 BERT-Large：BERT-Base 和 BERT-Large 的模型强度非常接近，但由于BERT-Base的GPU内存占用较小，其本地批量大小比BERT-Large大$8\times$。因此，BERT-Large
    的C2C比率约为BERT-Base的$8\times$，这使得BERT-Large的并行化难度大大增加，这一点从图[4](#S4.F4 "Figure 4
    ‣ 4.2 Experimental Results ‣ 4 Comparative Study ‣ A Quantitative Survey of Communication
    Optimizations in Distributed Deep Learning")(c)与图[4](#S4.F4 "Figure 4 ‣ 4.2 Experimental
    Results ‣ 4 Comparative Study ‣ A Quantitative Survey of Communication Optimizations
    in Distributed Deep Learning")(b)的比较中可以确认。BERT-Large的较小加速主要由于小批量大小和PCIe3.0的带宽限制。例如，BERT-Large的4-GPU训练最大加速为$1.2\times$，而BERT-Base为$3.1\times$。RTX2080Ti的较小GPU内存和PCIe3.0的带宽限制不适合BERT-Large的分布式训练。作为对比，当在一个更昂贵的服务器上训练BERT-Large，该服务器配有四个Nvidia
    V100 GPU（32GB内存），并通过NVLink（带宽比PCIe3.0高出$10\times$）互连，本地批量大小可以达到128，我们获得了$3.82\times$的加速。
- en: '4.2.2 System Architecture: PS vs. A2A'
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 系统架构：PS 与 A2A
- en: It is well known that the PS architecture with a single PS does not scale well.
    In our evaluation on the PS architecture, we use the same number of PSes and worker
    servers [[14](#bib.bib14)]. Notice that, in this setting, the PS architecture
    consumes more network switch ports and more total network bandwidth than A2A.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，单个PS的PS架构扩展性较差。在我们的PS架构评估中，我们使用相同数量的PS和工作服务器 [[14](#bib.bib14)]。注意，在这种设置下，PS架构消耗的网络交换机端口和总网络带宽都比A2A要多。
- en: Regarding BSP-PS and BSP-A2A without pipelining, BSP-A2A outperforms BSP-PS
    in all cases. However, when exploiting WFBP [[13](#bib.bib13)] to pipeline communications
    with computations, WFBP-PS outperforms WFBP-A2A, especially on 32 workers. This
    is because the A2A architecture has a non-negligible latency term that is logarithmic/linear
    to the number of workers with tree/ring-based algorithms, and WFBP requires the
    gradients aggregated tensor-wisely, resulting in noticeable startup overheads [[3](#bib.bib3)].
    The tensor fusion technique [[3](#bib.bib3)] can well address this startup problem.
    As we observe from Fig. [4](#S4.F4 "Figure 4 ‣ 4.2 Experimental Results ‣ 4 Comparative
    Study ‣ A Quantitative Survey of Communication Optimizations in Distributed Deep
    Learning"), MG-WFBP achieves the best speedup on BERT-Base (except the case of
    8 workers) and BERT-Large. But for ResNet-50 with higher model intensity, ByteScheduler-PS
    performs slightly better than MG-WFBP. In summary, there is no clear winner between
    PS and A2A. Both architectures can achieve comparable performance when equipped
    with suitable optimization techniques. However, PS needs extra servers and switch
    ports to keep the competitive edge with A2A.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 关于没有管道化的BSP-PS和BSP-A2A，BSP-A2A在所有情况下都优于BSP-PS。然而，当利用WFBP [[13](#bib.bib13)]将通信与计算进行管道化时，WFBP-PS优于WFBP-A2A，特别是在32个工作节点上。这是因为A2A架构有一个不可忽略的延迟项，该延迟与使用树形/环形算法的工作节点数量呈对数/线性关系，而WFBP需要梯度按张量聚合，这导致了明显的启动开销
    [[3](#bib.bib3)]。张量融合技术 [[3](#bib.bib3)] 可以很好地解决这个启动问题。正如我们从图[4](#S4.F4 "Figure
    4 ‣ 4.2 Experimental Results ‣ 4 Comparative Study ‣ A Quantitative Survey of
    Communication Optimizations in Distributed Deep Learning")中观察到的，MG-WFBP在BERT-Base（除了8个工作节点的情况）和BERT-Large上实现了最佳加速。但对于模型强度较高的ResNet-50，ByteScheduler-PS的性能略优于MG-WFBP。总之，PS和A2A之间没有明确的胜者。当配备适当的优化技术时，两种架构都能实现相当的性能。然而，PS需要额外的服务器和交换机端口才能在竞争中保持领先。
- en: 4.2.3 Scheduling
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 调度
- en: The idea of scheduling is to overlap communication tasks with computing tasks.
    Regarding the WFBP algorithm, in most cases WFBP-PS and WFBP-A2A both run faster
    than BSP-PS and BSP-A2A, respectively. But WFBP-A2A sometimes suffers from the
    startup latency problem as many small messages need to be transferred, e.g., under
    the case of BERT-Base and BERT-Large with 32 workers. MG-WFBP significantly improves
    the scalability of WFBP-A2A, especially with a large number of workers. ByteScheduler-A2A
    schedules the communications in the opposite direction with MG-WFBP by partitioning
    tensors instead of merging tensors, and its performance is not very promising.
    However, with the PS architecture, ByteScheduler-PS slightly outperforms WFBP-PS
    in ResNet-50. This indicates that without bringing extra heavy latency by partitioning
    tensors, communications of partitioned tensors can be better scheduled to overlap
    with backpropagation and feed-forward computations [[14](#bib.bib14)]. In summary,
    scheduling algorithms can improve the system scalability by hiding the communication
    overhead. However, when the communication time dominates the training time (e.g.,
    BERT-Large), the overall speedup becomes rather limited and we need to either
    improve the network speed or consider lossy algorithms.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 调度的思想是将通信任务与计算任务重叠。关于WFBP算法，在大多数情况下，WFBP-PS和WFBP-A2A的运行速度均快于BSP-PS和BSP-A2A。但WFBP-A2A有时会受到启动延迟问题的影响，因为需要传输许多小消息，例如在32个工作节点下的BERT-Base和BERT-Large。MG-WFBP显著提高了WFBP-A2A的可扩展性，特别是在大量工作节点的情况下。ByteScheduler-A2A通过分割张量而不是合并张量来将通信调度在MG-WFBP的相反方向，其性能并不非常理想。然而，使用PS架构时，ByteScheduler-PS在ResNet-50中略微优于WFBP-PS。这表明，在不增加额外的延迟的情况下，通过分割张量的通信可以更好地调度与反向传播和前向计算重叠[[14](#bib.bib14)]。总之，调度算法可以通过隐藏通信开销来提高系统的可扩展性。然而，当通信时间主导训练时间时（例如BERT-Large），总体加速效果会受到限制，我们需要要么提高网络速度，要么考虑有损算法。
- en: 5 Challenges and Future Directions
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 挑战与未来方向
- en: Despite many techniques are proposed to address the communication problem in
    distributed DL, some technical challenges remain open to answer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经提出了许多技术来解决分布式深度学习中的通信问题，但仍然存在一些技术挑战待解答。
- en: 5.1 Communication Compression
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 通信压缩
- en: As the model size increases, the communication cost grows, which could result
    in a very high C2C ratio. Lossless optimization algorithms in system architecture
    design and scheduling can only achieve marginal improvement since the communication
    cost dominates the training time. The communication compression techniques would
    be useful to significantly reduce the communication traffic in such cases. The
    primary challenge is how to maintain the model accuracy while keeping the convergence
    performance. Existing methods have proven that communication compression can achieve
    the same asymptotic convergence speed as vanilla SGD. Yet in practice, with a
    very high compression ratio, it generally requires more iterations to achieve
    the target loss value. One possible direction is to set different compression
    ratios for different layers to maximize the exchanged information. Another possibility
    is to dynamically set appropriate compression ratios at different training iterations.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型规模的增加，通信成本也会增长，这可能导致非常高的C2C比率。在系统架构设计和调度中的无损优化算法只能实现边际改进，因为通信成本主导了训练时间。通信压缩技术在这种情况下将非常有用，可以显著减少通信流量。主要挑战是如何在保持收敛性能的同时维护模型准确性。现有方法已经证明，通信压缩可以实现与普通SGD相同的渐进收敛速度。然而在实践中，具有非常高压缩比的情况通常需要更多的迭代才能达到目标损失值。一种可能的方向是为不同层设置不同的压缩比，以最大化交换信息。另一种可能性是在不同训练迭代中动态设置适当的压缩比。
- en: 5.2 Automatically Selected System Architecture
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 自动选择系统架构
- en: The PS and A2A architectures are widely deployed for the BSP algorithm in both
    industry and academia. Intuitively, the A2A architecture is more efficient than
    PS as it requires no central servers; but A2A is more latency-sensitive than PS.
    Furthermore, one can use multiple PSes to reduce the central server’s network
    footprint. More uncertainly, with different hardware configurations, model properties,
    and scheduling algorithms, no solution is always better in all cases. An interesting
    yet challenging problem is to build mathematical performance models for both PS
    and A2A according to the training environments (e.g., the number of GPUs, network
    topology, link bandwidth and latency, model properties, etc.), so that a better
    architecture can be chosen for training the target model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: PS和A2A架构在工业界和学术界广泛应用于BSP算法。直观上，A2A架构比PS更高效，因为它不需要中央服务器；但A2A对延迟的敏感性比PS更高。此外，可以使用多个PS来减少中央服务器的网络负担。更不确定的是，在不同的硬件配置、模型属性和调度算法下，没有一种解决方案在所有情况下始终最佳。一个有趣但具有挑战性的问题是根据训练环境（例如，GPU数量、网络拓扑、链路带宽和延迟、模型属性等）为PS和A2A构建数学性能模型，以便为训练目标模型选择更好的架构。
- en: 5.3 Generic Scheduling
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 通用调度
- en: According to the characteristics of distributed DL, various scheduling algorithms
    try to maximize the parallelism of computing tasks and communication tasks. However,
    these algorithms were built upon the DAG of BSP with three types of tasks (i.e.,
    feed-forward, backpropagation, and gradient communication). The scheduling algorithm
    only brings marginal improvement if the communication time is much longer than
    the computing time. Although communication compression can reduce the communication
    cost, current scheduling methods are not directly applicable to the BSP with gradient
    compression because compression introduces extra non-negligible computational
    costs and smaller communication traffic, which makes the scheduling more difficult.
    One possible solution is to design a generic scheduler for configured DAGs. The
    DAG would be changed due to tensor partition or fusion. For the configured DAG,
    the scheduler can use some heuristic algorithms to search for the configuration
    with better performance. Furthermore, current scheduling techniques such as MG-WFBP [[3](#bib.bib3)]
    and ByteScheduler [[14](#bib.bib14)] take two opposite directions (i.e., tensor
    fusion and tensor partition) for scheduling. In practice, no one is always better.
    An intelligent scheduler should be adaptive to the training environment and dynamically
    determine whether the tensors should be merged or partitioned to achieve higher
    performance.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 根据分布式深度学习的特点，各种调度算法尝试最大化计算任务和通信任务的并行性。然而，这些算法是基于具有三种任务类型（即前向传播、反向传播和梯度通信）的BSP有向无环图（DAG）。如果通信时间远长于计算时间，调度算法仅能带来微小的改进。尽管通信压缩可以减少通信成本，但当前的调度方法不适用于具有梯度压缩的BSP，因为压缩引入了额外的不可忽视的计算成本和较小的通信流量，这使得调度更加困难。一个可能的解决方案是为配置的DAG设计一个通用调度器。由于张量分割或融合，DAG会发生变化。对于配置的DAG，调度器可以使用一些启发式算法来搜索性能更好的配置。此外，目前的调度技术如MG-WFBP [[3](#bib.bib3)]和ByteScheduler [[14](#bib.bib14)]在调度上采取了两种相反的方向（即张量融合和张量分割）。在实际应用中，没有哪一种始终优越。一个智能调度器应当能够适应训练环境，并动态确定是否应该合并或分割张量，以实现更高的性能。
- en: 6 Conclusion
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this article, we gave an overview of the techniques to address the communication
    challenges in distributed deep learning. We first analyzed the communication problems
    in distributed training of deep learning models, and then presented a taxonomy
    and survey of the existing state-of-the-art technologies. We particularly focused
    on the commonly used lossless methods and provided a quantitative analysis to
    these methods based on real-world experiments. Finally, we discussed the challenges
    and possible future research directions in this area.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们概述了应对分布式深度学习通信挑战的技术。我们首先分析了深度学习模型分布式训练中的通信问题，然后展示了现有前沿技术的分类法和调研。我们特别关注了常用的无损方法，并根据实际实验对这些方法进行了定量分析。最后，我们讨论了该领域的挑战和未来可能的研究方向。
- en: Acknowledgments
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The research was supported in part by Hong Kong RGC GRF grants under the contracts
    HKBU 12200418, HKUST 16206417 and 16207818, and in part by National Natural Science
    Foundation of China under Grant 62002240.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究部分得到了香港RGC GRF资助（合同号HKBU 12200418，HKUST 16206417和16207818），以及中国国家自然科学基金资助（资助号62002240）。
- en: References
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior,
    P. Tucker, K. Yang *et al.*, “Large scale distributed deep networks,” in *Advances
    in neural information processing systems*, 2012, pp. 1223–1231.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A.
    Senior, P. Tucker, K. Yang *等人*，“大规模分布式深度网络”，收录于 *《神经信息处理系统进展》*，2012年，第1223–1231页。'
- en: '[2] C. Chen, W. Wang, and B. Li, “Round-robin synchronization: Mitigating communication
    bottlenecks in parameter servers,” in *IEEE INFOCOM 2019-IEEE Conference on Computer
    Communications*.   IEEE, 2019, pp. 532–540.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] C. Chen, W. Wang, 和 B. Li，“轮询同步：缓解参数服务器中的通信瓶颈”，收录于 *IEEE INFOCOM 2019-IEEE计算机通信会议*。IEEE，2019年，第532–540页。'
- en: '[3] S. Shi, X. Chu, and B. Li, “MG-WFBP: Efficient data communication for distributed
    synchronous SGD algorithms,” in *IEEE INFOCOM 2019-IEEE Conference on Computer
    Communications*.   IEEE, 2019, pp. 172–180.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] S. Shi, X. Chu, 和 B. Li，“MG-WFBP：分布式同步SGD算法的高效数据通信”，收录于 *IEEE INFOCOM 2019-IEEE计算机通信会议*。IEEE，2019年，第172–180页。'
- en: '[4] T. Lin, S. U. Stich, K. K. Patel, and M. Jaggi, “Don’t use large mini-batches,
    use local SGD,” in *International Conference on Learning Representations*, 2020.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] T. Lin, S. U. Stich, K. K. Patel, 和 M. Jaggi，“不要使用大批量，使用本地SGD”，收录于 *国际学习表征会议*，2020年。'
- en: '[5] X. Zhao, A. An, J. Liu, and B. X. Chen, “Dynamic stale synchronous parallel
    distributed training for deep learning,” in *2019 IEEE 39th International Conference
    on Distributed Computing Systems (ICDCS)*.   IEEE, 2019, pp. 1507–1517.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] X. Zhao, A. An, J. Liu, 和 B. X. Chen，“深度学习的动态过时同步并行分布式训练”，收录于 *2019 IEEE第39届国际分布式计算系统会议（ICDCS）*。IEEE，2019年，第1507–1517页。'
- en: '[6] B. Recht, C. Re, S. Wright, and F. Niu, “Hogwild: A lock-free approach
    to parallelizing stochastic gradient descent,” in *Advances in neural information
    processing systems*, 2011, pp. 693–701.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] B. Recht, C. Re, S. Wright, 和 F. Niu，“Hogwild：一种无锁的并行随机梯度下降方法”，收录于 *《神经信息处理系统进展》*，2011年，第693–701页。'
- en: '[7] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar, “signSGD:
    Compressed optimisation for non-convex problems,” in *International Conference
    on Machine Learning*, 2018, pp. 560–569.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, 和 A. Anandkumar，“signSGD：非凸问题的压缩优化”，收录于
    *国际机器学习大会*，2018年，第560–569页。'
- en: '[8] S. Shi, Q. Wang, K. Zhao, Z. Tang, Y. Wang, X. Huang, and X. Chu, “A distributed
    synchronous SGD algorithm with global Top-k sparsification for low bandwidth networks,”
    in *2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)*.   IEEE,
    2019, pp. 2238–2247.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] S. Shi, Q. Wang, K. Zhao, Z. Tang, Y. Wang, X. Huang, 和 X. Chu，“一种具有全局Top-k稀疏化的分布式同步SGD算法，用于低带宽网络”，收录于
    *2019 IEEE第39届国际分布式计算系统会议（ICDCS）*。IEEE，2019年，第2238–2247页。'
- en: '[9] J. Xue, Y. Miao, C. Chen, M. Wu, L. Zhang, and L. Zhou, “Fast distributed
    deep learning over RDMA,” in *Proceedings of the Fourteenth EuroSys Conference
    2019*, 2019, pp. 1–14.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] J. Xue, Y. Miao, C. Chen, M. Wu, L. Zhang, 和 L. Zhou，“通过RDMA进行快速分布式深度学习”，收录于
    *第十四届EuroSys会议论文集2019*，2019年，第1–14页。'
- en: '[10] S. Wang, D. Li, J. Geng, Y. Gu, and Y. Cheng, “Impact of network topology
    on the performance of DML: Theoretical analysis and practical factors,” in *IEEE
    INFOCOM 2019-IEEE Conference on Computer Communications*.   IEEE, 2019, pp. 1729–1737.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S. Wang, D. Li, J. Geng, Y. Gu, 和 Y. Cheng，“网络拓扑对DML性能的影响：理论分析和实际因素”，收录于
    *IEEE INFOCOM 2019-IEEE计算机通信会议*。IEEE，2019年，第1729–1737页。'
- en: '[11] P. Sanders, J. Speck, and J. L. Träff, “Two-tree algorithms for full bandwidth
    broadcast, reduction and scan,” *Parallel Computing*, vol. 35, no. 12, pp. 581–594,
    2009.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] P. Sanders, J. Speck, 和 J. L. Träff，“用于全带宽广播、归约和扫描的双树算法”，*《并行计算》*，第35卷，第12期，第581–594页，2009年。'
- en: '[12] M. Cho, U. Finkler, D. S. Kung, and H. C. Hunter, “BlueConnect: Decomposing
    all-reduce for deep learning on heterogeneous network hierarchy,” in *Proceedings
    of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31
    - April 2, 2019*, 2019.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. Cho, U. Finkler, D. S. Kung, 和 H. C. Hunter，“BlueConnect：在异构网络层级上分解全归约以进行深度学习”，收录于
    *《2019年机器学习与系统会议论文集，MLSys 2019，斯坦福，加州，美国，2019年3月31日 - 4月2日》*，2019年。'
- en: '[13] H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P. Xie,
    and E. P. Xing, “Poseidon: An efficient communication architecture for distributed
    deep learning on GPU clusters,” in *2017 USENIX Annual Technical Conference (USENIX
    ATC 17)*, 2017, pp. 181–193.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P.
    Xie, 和 E. P. Xing，“Poseidon：一种高效的 GPU 集群分布式深度学习通信架构”，发表于 *2017年 USENIX 年度技术会议（USENIX
    ATC 17）*，2017，页码 181–193。'
- en: '[14] Y. Peng, Y. Zhu, Y. Chen, Y. Bao, B. Yi, C. Lan, C. Wu, and C. Guo, “A
    generic communication scheduler for distributed DNN training acceleration,” in
    *Proceedings of the 27th ACM Symposium on Operating Systems Principles*.   ACM,
    2019, pp. 16–29.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Peng, Y. Zhu, Y. Chen, Y. Bao, B. Yi, C. Lan, C. Wu, 和 C. Guo，"用于分布式
    DNN 训练加速的通用通信调度器"，发表于 *第27届 ACM 操作系统原理研讨会论文集*。ACM，2019，页码 16–29。'
- en: '[15] A. Jayarajan, J. Wei, G. Gibson, A. Fedorova, and G. Pekhimenko, “Priority-based
    parameter propagation for distributed DNN training,” in *Proceedings of Machine
    Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2,
    2019*, 2019.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] A. Jayarajan, J. Wei, G. Gibson, A. Fedorova, 和 G. Pekhimenko，“基于优先级的分布式
    DNN 训练参数传播”，发表于 *2019年机器学习与系统会议论文集，MLSys 2019，斯坦福，加州，美国，2019年3月31日至4月2日*，2019。'
- en: Biographies
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传记
- en: '|  | Shaohuai Shi (shaohuais@cse.ust.hk) received a B.E. degree in software
    engineering from South China University of Technology, P.R. China, in 2010, an
    MS degree in computer science from Harbin Institute of Technology, P.R. China
    in 2013, and a Ph.D. degree in computer science from Hong Kong Baptist University
    in 2020\. He is currently a research assistant professor in the Department of
    Computer Science and Engineering at the Hong Kong University of Science and Technology.
    His research interests include GPU computing and machine learning systems. He
    is a member of the IEEE. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | Shaohuai Shi (shaohuais@cse.ust.hk) 2010年获得华南理工大学软件工程学士学位，2013年获得哈尔滨工业大学计算机科学硕士学位，2020年获得香港浸会大学计算机科学博士学位。他目前是香港科技大学计算机科学与工程系的研究助理教授。他的研究兴趣包括
    GPU 计算和机器学习系统。他是 IEEE 的会员。 |'
- en: '|  | Zhenheng Tang (zhtang@comp.hkbu.edu.hk) received a B.E. degree in communication
    engineering from Huazhong University Of Science and Technology, P.R. China, in
    2018\. He is a Ph.D. student at Hong Kong Baptist University. His research interests
    include GPU computing and distributed deep learning. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | Zhenheng Tang (zhtang@comp.hkbu.edu.hk) 2018年获得华中科技大学通信工程学士学位。他是香港浸会大学的博士生。他的研究兴趣包括
    GPU 计算和分布式深度学习。 |'
- en: '|  | Xiaowen Chu (chxw@comp.hkbu.edu.hk) received a B.E. degree in computer
    science from Tsinghua University, P.R. China, in 1999, and a Ph.D. degree in computer
    science from The Hong Kong University of Science and Technology in 2003\. Currently,
    he is a full professor in the Department of Computer Science, Hong Kong Baptist
    University. His research interests include parallel and distributed computing,
    cloud computing and wireless networks. He is serving as an Associate Editor of
    IEEE Access and IEEE Internet of Things Journal. He is a senior member of the
    IEEE. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | Xiaowen Chu (chxw@comp.hkbu.edu.hk) 1999年获得清华大学计算机科学学士学位，2003年获得香港科技大学计算机科学博士学位。目前，他是香港浸会大学计算机科学系的全职教授。他的研究兴趣包括并行和分布式计算、云计算和无线网络。他担任
    IEEE Access 和 IEEE Internet of Things Journal 的副编辑，是 IEEE 的高级会员。 |'
- en: '|  | Chengjian Liu (liuchengjian@sztu.edu.cn) received his MS degree in College
    of Computer Science and Software Engineering, Shenzhen University, P.R. China,
    in 2013, and his Ph.D. degree in computer science from the Hong Kong Baptist University
    in 2018\. Currently, he is an assistant professor in the College of Big Data and
    Internet, Shenzhen Technology University. His research interests include Distributed
    Storage, Blockchain, General-Purpose GPU Computing. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | Chengjian Liu (liuchengjian@sztu.edu.cn) 2013年在深圳大学计算机科学与软件工程学院获得硕士学位，2018年在香港浸会大学获得计算机科学博士学位。目前，他是深圳技术大学大数据与互联网学院的助理教授。他的研究兴趣包括分布式存储、区块链、通用
    GPU 计算。 |'
- en: '|  | Wei Wang (weiwa@cse.ust.hk) received his B.Eng. (Hons.) and M.Eng. degrees
    from Shanghai Jiao Tong University, and a Ph.D. degree from the University of
    Toronto in 2015, all in the Department of Electrical and Computer Engineering.
    He is an Assistant Professor in the Department of Computer Science and Engineering
    at the Hong Kong University of Science and Technology (HKUST). He is also affiliated
    with HKUST Big Data Institute. His research interests cover the broad area of
    distributed systems, with special emphasis on big data and machine learning systems,
    cloud computing, and computer networks in general. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | Wei Wang (weiwa@cse.ust.hk) 获得了上海交通大学电气与计算机工程学士（荣誉）学位和硕士学位，并于2015年获得了多伦多大学电气与计算机工程博士学位。他是香港科技大学计算机科学与工程系的助理教授。他还与香港科技大学大数据研究所相关联。他的研究兴趣涵盖了分布式系统的广泛领域，特别关注大数据和机器学习系统、云计算和计算机网络等方面。
    |'
- en: '|  | Bo Li (bli@cse.ust.hk) received a BEng in computer science from Tsinghua
    University, Beijing and a Ph.D. degree in electrical and computer engineering
    from the University of Massachusetts at Amherst. He is a professor in the Department
    of Computer Science and Engineering, Hong Kong University of Science and Technology.
    He was the chief technical advisor of ChinaCache Corp. (NASDAQ CCIH), the largest
    CDN operator in China. He was a Cheung Kong visiting chair professor with Shanghai
    Jiao Tong University (2010-2013) and an adjunct researcher with Microsoft Research
    Asia (1999-2007) and with Microsoft Advance Technology Center (2007-2009). His
    current research interests include: multimedia communications, the Internet content
    distribution, datacenter networking, cloud computing, and wireless sensor networks.
    He made pioneering contributions in the Internet video broadcast with the system,
    Coolstreaming, which was credited as the world first large-scale Peer-to-Peer
    live video streaming system. The work appeared in IEEE INFOCOM (2005) received
    the IEEE INFOCOM 2015 Test-of-Time Award. He has been an Editor or a Guest Editor
    of more than a dozen of the IEEE journals and magazines. He was the co-TPC chair
    of the IEEE INFOCOM 2004\. He received five Best Paper Awards from the IEEE. He
    received the Young Investigator Award from Natural Science Foundation of China
    (NFSC) in 2005, the State Natural Science Award (2nd Class) from China in 2011\.
    He is a fellow of the IEEE. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | Bo Li (bli@cse.ust.hk) 获得了清华大学计算机科学学士学位和马萨诸塞大学安姆斯特分校电气与计算机工程博士学位。他是香港科技大学计算机科学与工程系的教授。他曾担任中国缓存公司（NASDAQ
    CCIH）的首席技术顾问，该公司是中国最大的CDN运营商。他曾担任上海交通大学的长江学者访问讲座教授（2010-2013），以及微软亚洲研究院（1999-2007）和微软高级技术中心（2007-2009）的兼职研究员。他目前的研究兴趣包括：多媒体通信、互联网内容分发、数据中心网络、云计算和无线传感器网络。他在互联网视频广播领域做出了开创性贡献，其系统Coolstreaming被认为是世界上第一个大规模点对点实时视频流系统。这项工作发表在IEEE
    INFOCOM（2005），并获得了IEEE INFOCOM 2015测试时间奖。他曾担任十多个IEEE期刊和杂志的编辑或特邀编辑。他是IEEE INFOCOM
    2004的共同程序委员会主席。他获得了五次IEEE最佳论文奖。他于2005年获得了中国自然科学基金青年科学家奖，并于2011年获得了中国国家自然科学奖（二等奖）。他是IEEE的会士。
    |'
