- en: 'Deep Learning 2: Part 2 Lesson 11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习2：第2部分第11课
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34)
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我从* [*fast.ai 课程*](http://www.fast.ai/)*中得到的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*
    [*Jeremy*](https://twitter.com/jeremyphoward) *和* [*Rachel*](https://twitter.com/math_rachel)
    *给了我这个学习的机会。*'
- en: Links
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接
- en: '[**Forum**](http://forums.fast.ai/t/part-2-lesson-11-in-class/14699/1) **/**
    [**Video**](https://youtu.be/tY0n9OT5_nA)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[**论坛**](http://forums.fast.ai/t/part-2-lesson-11-in-class/14699/1) **/** [**视频**](https://youtu.be/tY0n9OT5_nA)'
- en: 'Before getting started:'
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始之前：
- en: '[The 1cycle policy](https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy)
    by Sylvain Gugger. Based on Leslie Smith’s new paper which takes the previous
    two key papers (cyclical learning rate and super convergence) and built on them
    with a number of experiments to show how you can achieve super convergence. Super
    convergence lets you train models five times faster than the previous stepwise
    approach (and faster than CLR, although it is less than five times). Super convergence
    lets you get up to massively high learning rates by somewhere between 1 and 3\.
    The interesting thing about super convergence is that you train at those very
    high learning rates for quite a large percentage of your epochs and during that
    time, the loss doesn’t really improve very much. But the trick is it’s doing a
    lot of searching through the space to find really generalizable areas it seems.
    Sylvain implemented it in fastai by flushing out the pieces that were missing
    then confirmed that he actually achieved super convergence on training on CIFAR10\.
    It is currently called `use_clr_beta` but will be renamed in future. He also added
    cyclical momentum to fastai library.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1cycle策略](https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy)
    由Sylvain Gugger提出。基于Leslie Smith的新论文，该论文结合了之前的两篇关键论文（循环学习率和超级收敛），并通过一系列实验来展示如何实现超级收敛。超级收敛让您训练模型比之前的分阶段方法快五倍（比CLR更快，尽管不到五倍）。超级收敛让您可以通过1到3之间的极高学习率进行训练。超级收敛的有趣之处在于，您在相当大比例的epochs中以非常高的学习率进行训练，而在此期间，损失并没有真正得到很大的改善。但诀窍在于它在空间中进行了大量搜索，以找到真正通用的区域。Sylvain在fastai中实现了这一点，通过补充缺失的部分，然后确认他确实在CIFAR10上实现了超级收敛。目前称为`use_clr_beta`，但将来会更名。他还在fastai库中添加了循环动量。'
- en: '[How To Create Data Products That Are Magical Using Sequence-to-Sequence Model](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)s
    by Hamel Husain. He blogged about training a model to summarize GitHub issues.
    Here is the [demo](http://gh-demo.kubeflow.org/) Kubeflow team created based on
    his blog.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用序列到序列模型创建神奇的数据产品](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)
    由Hamel Husain撰写。他在博客中介绍了训练一个模型来总结GitHub问题。这是基于他的博客创建的Kubeflow团队的[演示](http://gh-demo.kubeflow.org/)。'
- en: Neural Machine Translation [[5:36](https://youtu.be/tY0n9OT5_nA?t=5m36s)]
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经机器翻译 [[5:36](https://youtu.be/tY0n9OT5_nA?t=5m36s)]
- en: Let’s build a sequence-to-sequence model! We are going to be working on machine
    translation. Machine translation is something that’s been around for a long time,
    but we are going to look at an approach called neural translation which uses neural
    networks for translation. Neural machine translation appeared a couple years ago
    and it was not as good as the statistical machine translation approaches that
    use classic feature engineering and standard NLP approaches like stemming, fiddling
    around with word frequencies, n-grams, etc. By a year later, it was better than
    everything else. It is based on a metric called BLEU — we are not going to discuss
    the metric because it is not a very good metric and it is not interesting, but
    everybody uses it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个序列到序列模型！我们将致力于机器翻译。机器翻译已经存在很长时间了，但我们将看一种称为神经翻译的方法，它使用神经网络进行翻译。神经机器翻译几年前出现，当时并不像使用经典特征工程和标准NLP方法（如词干处理、调整词频、n-gram等）的统计机器翻译方法那么好。一年后，它比其他所有方法都要好。它基于一个叫做BLEU的指标——我们不会讨论这个指标，因为它不是一个很好的指标，也不是很有趣，但每个人都在使用它。
- en: We are seeing machine translation starting down the path that we saw starting
    computer vision object classification in 2012 which just surpassed the state-of-the-art
    and now zipping past it at great rate. It is unlikely that anybody watching this
    is actually going to build a machine translation model because [https://translate.google.com/](https://translate.google.com/)
    works quite well. So why are we learning about machine translation? The reason
    we are learning about machine translation is that the general idea of taking some
    kind of input like a sentence in French and transforming it into some other kind
    of output with arbitrary length such as a sentence in English is a really useful
    thing to do. For example, as we just saw, Hamel took GitHub issues and turn them
    into summaries. Another example is taking videos and turning them into descriptions,
    or basically anything where you are spitting out an arbitrary sized output which
    is very often a sentence. Maybe taking a CT scan and spitting out a radiology
    report — this is where you can use sequence to sequence learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到机器翻译开始沿着我们在2012年看到的计算机视觉对象分类的道路前进，后者刚刚超越了最先进技术，现在正在以很快的速度超越它。看这个视频的人不太可能会构建一个机器翻译模型，因为[https://translate.google.com/](https://translate.google.com/)效果相当不错。那么我们为什么要学习机器翻译呢？我们学习机器翻译的原因是，将一些输入（比如法语句子）转换为任意长度的其他输出（比如英语句子）的一般想法是一件非常有用的事情。例如，正如我们刚才看到的，Hamel将GitHub问题转换为摘要。另一个例子是将视频转换为描述，或者基本上任何你需要输出任意长度输出的地方，通常是一个句子。也许是将CT扫描转换为放射学报告——这就是你可以使用序列到序列学习的地方。
- en: Four big wins of Neural Machine Translation [[8:36](https://youtu.be/tY0n9OT5_nA?t=8m36s)]
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经机器翻译的四个重大优势[[8:36](https://youtu.be/tY0n9OT5_nA?t=8m36s)]
- en: 'End-to-end training: No fussing around with heuristics and hacky feature engineering.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端到端训练：不需要围绕启发式和繁琐的特征工程纠缠。
- en: We are able to build these distributed representations which are shared by lots
    of concepts within a single network.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能够构建这些分布式表示，这些表示被单个网络中的许多概念共享。
- en: We are able to use long term state in the RNN so it uses a lot more context
    than n-gram type approaches.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能够在RNN中使用长期状态，因此它比n-gram类型的方法使用了更多的上下文。
- en: In the end, text we are generating uses RNN as well so we can build something
    that is more fluid.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终，我们生成的文本也使用了RNN，因此我们可以构建更加流畅的东西。
- en: BiLSTMs(+Attn) not just for neural MT [[9:20](https://youtu.be/tY0n9OT5_nA?t=9m20s)]
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BiLSTMs（+Attn）不仅适用于神经机器翻译
- en: We are going to use bi-directional GRU (basically the same as LSTM) with attention
    — these general ideas can also be used for lots of other things as you see above.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用带有注意力的双向GRU（基本上与LSTM相同）-正如您在上面看到的，这些一般想法也可以用于许多其他事情。
- en: Let’s jump into the code [[9:47](https://youtu.be/tY0n9OT5_nA?t=9m47s)]
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们来看代码[[9:47](https://youtu.be/tY0n9OT5_nA?t=9m47s)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb)'
- en: 'We are going to try to translate French into English by following the standard
    neural network approach:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试按照标准的神经网络方法将法语翻译成英语：
- en: Data
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据
- en: Architecture
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 架构
- en: Loss Function
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失函数
- en: 1\. Data
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 数据
- en: 'As usual, we need `(x, y)` pair. In this case, x: French sentence, y: English
    sentence which you will compare your prediction against. We need lots of these
    tuples of French sentences with their equivalent English sentence — that is called
    “parallel corpus” and harder to find than a corpus for a language model. For a
    language model, we just need text in some language. For any living language, there
    will be a few gigabytes at least of text floating around the internet for you
    to grab. For translation, there are some pretty good parallel corpus available
    for European languages. The European Parliament has every sentence in every European
    language. Anything that goes to the UN is translated to lots of languages. For
    French to English, we have particularly nice thing which is pretty much any semi
    official Canadian website will have a French version and an English version[[12:13](https://youtu.be/tY0n9OT5_nA?t=12m13s)].'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们需要`(x, y)`对。在这种情况下，x：法语句子，y：英语句子，您将与之比较您的预测。我们需要许多这些法语句子及其相应的英语句子的元组-这被称为“平行语料库”，比语言模型的语料库更难找到。对于语言模型，我们只需要某种语言的文本。对于任何生活语言，互联网上至少会有几千兆字节的文本供您获取。对于翻译，有一些非常好的欧洲语言的平行语料库可用。欧洲议会有每种欧洲语言的每个句子。任何提交给联合国的东西都会被翻译成许多语言。对于法语到英语，我们有一个特别好的东西，那就是几乎任何半官方的加拿大网站都会有法语版本和英语版本[[12:13](https://youtu.be/tY0n9OT5_nA?t=12m13s)]。
- en: Translation files
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 翻译文件
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: French/English parallel texts from [http://www.statmt.org/wmt15/translation-task.html](http://www.statmt.org/wmt15/translation-task.html)
    . It was created by Chris Callison-Burch, who crawled millions of web pages and
    then used *a set of simple heuristics to transform French URLs onto English URLs
    (i.e. replacing “fr” with “en” and about 40 other hand-written rules), and assume
    that these documents are translations of each other*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从[http://www.statmt.org/wmt15/translation-task.html](http://www.statmt.org/wmt15/translation-task.html)获取的法语/英语平行文本。这是由克里斯·卡利森-伯奇（Chris
    Callison-Burch）创建的，他爬取了数百万个网页，然后使用*一组简单的启发式规则将法语URL转换为英语URL（即用“fr”替换为“en”和其他大约40个手写规则），并假设这些文档是彼此的翻译*。
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For bounding boxes, all of the interesting stuff was in the loss function, but
    for neural translation, all of the interesting stuff is going to be int he architecture
    [[13:01](https://youtu.be/tY0n9OT5_nA?t=13m1s)]. Let’s zip through this pretty
    quickly and one of the things Jeremy wants you to think about particularly is
    what are the relationships or the similarities in terms of the tasks we are doing
    and how we do it between language modeling vs. neural translation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于边界框，所有有趣的东西都在损失函数中，但对于神经翻译，所有有趣的东西都将在架构中[[13:01](https://youtu.be/tY0n9OT5_nA?t=13m1s)]。让我们快速浏览一下，杰里米希望你特别考虑的一件事是我们在语言建模与神经翻译之间所做任务及如何做任务的关系或相似之处。
- en: The first step is to do the exact same thing we do in a language model which
    is to take a sentence and chuck it through an RNN[[13:35](https://youtu.be/tY0n9OT5_nA?t=13m35s)].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是做与语言模型中相同的事情，即通过RNN传递一个句子[[13:35](https://youtu.be/tY0n9OT5_nA?t=13m35s)]。
- en: 'Now with the classification model, we had a decoder which took the RNN output
    and grabbed three things: `maxpool` and `meanpool` over all of the time steps,
    and the value of the RNN at the last time step, stack all those together and put
    it through a linear layer [[14:24](https://youtu.be/tY0n9OT5_nA?t=14m24s)]. Most
    people do not do that and just use the last time step, so all the things we will
    be talking about today uses the last time step.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在分类模型中，我们有一个解码器，它接收RNN输出并提取三个内容：在所有时间步上的`maxpool`和`meanpool`，以及在最后一个时间步上的RNN的值，将所有这些堆叠在一起并通过一个线性层[[14:24](https://youtu.be/tY0n9OT5_nA?t=14m24s)]。大多数人不这样做，只使用最后一个时间步，所以我们今天将要讨论的所有内容都使用最后一个时间步。
- en: We start out by chucking the input sentence through an RNN and out of it comes
    some “hidden state” (i.e. some vector that represents the output of an RNN that
    has encoded the sentence).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过RNN将输入句子传递，然后得到一些“隐藏状态”（即代表编码了句子的RNN的输出的向量）。
- en: Encoder ≈ Backbone [[15:18](https://youtu.be/tY0n9OT5_nA?t=15m18s)]
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器≈骨干[[15:18](https://youtu.be/tY0n9OT5_nA?t=15m18s)]
- en: Stephen used the word “encoder”, but we tend to use the word “backbone”. Like
    when we talked about adding a custom head to an existing model, the existing pre-trained
    ImageNet model, for example, we say that is our backbone and then we stick on
    top of it some head that does the task we want. In sequence to sequence learning,
    they use the word encoder, but it basically is the same thing — it is some piece
    of a neural network architecture that takes the input and turns it into some representation
    which we can then stick a few more layers on top to grab something out of it such
    as we did for the classifier where we stack a linear layer on top of it to turn
    int into a sentiment. This time though, we have something that’s a little bit
    harder than just creating sentiment [[16:12](https://youtu.be/tY0n9OT5_nA?t=16m12s)].
    Instead of turning the hidden state into a positive or negative sentiment, we
    want to turn it into a sequence of tokens where that sequence of token is the
    German sentence in Stephen’s example.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Stephen使用了“编码器”这个词，但我们倾向于使用“骨干”。就像当我们谈论向现有模型添加自定义头部时，例如现有的预训练ImageNet模型，我们说这是我们的骨干，然后我们在其上添加一些执行我们想要的任务的头部。在序列到序列学习中，他们使用“编码器”这个词，但基本上是一样的——它是神经网络架构的一部分，它接受输入并将其转换为我们可以在其上添加几层以获取某些内容的表示，就像我们为分类器所做的那样，我们在其上堆叠一个线性层以将其转换为情感。不过，这次我们要做的事情比创建情感要困难一点。我们不是将隐藏状态转换为积极或消极情感，而是要将其转换为一系列标记，这些标记是Stephen示例中的德语句子。
- en: This is sounding more like the language model than the classifier because the
    language had multiple tokens (for every input word, there was an output word).
    But the language model was also much easier because the number of tokens in the
    language model output was the same length as the number of tokens in the language
    model input. Not only they were the same length, but they exactly matched up (e.g.
    after word one comes word two, after word two comes word three, and so forth).
    For translating language, you don’t necessarily know that the word “he” will be
    translated as the first word in the output (unfortunately, it is in this particular
    case). Very often, the subject object order will be different or there will be
    some extra words inserted, or some pronouns we will need to add some gendered
    article, etc. This is the key issue we are going to have to deal with is the fact
    that we have an arbitrary length output where the tokens in the output do not
    correspond to the same order or the specific tokens in the input [[17:31](https://youtu.be/tY0n9OT5_nA?t=17m31s)].
    But the general idea is the same. Here is an RNN to encode the input, turns it
    into some hidden state, then this is the new thing we are going to learn is generating
    a sequence output.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来更像是语言模型而不是分类器，因为语言有多个标记（对于每个输入单词，都有一个输出单词）。但语言模型也更容易，因为语言模型输出中的标记数量与语言模型输入中的标记数量相同。不仅它们的长度相同，而且它们完全匹配（例如，单词一后面是单词二，单词二后面是单词三，依此类推）。对于翻译语言，你不一定知道单词“he”会被翻译为输出的第一个单词（不幸的是，在这种特殊情况下是这样的）。很多时候，主语宾语顺序会有所不同，或者会插入一些额外的单词，或者我们需要添加一些代词，性别化的文章等。我们将要处理的关键问题是，我们有一个任意长度的输出，其中输出中的标记与输入中的特定标记的顺序不对应。但总体思路是一样的。这里有一个RNN来编码输入，将其转换为一些隐藏状态，然后我们要学习的新内容是生成一个序列输出。
- en: Sequence output [[17:47](https://youtu.be/tY0n9OT5_nA?t=17m47s)]
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列输出[[17:47](https://youtu.be/tY0n9OT5_nA?t=17m47s)]
- en: 'We already know:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道：
- en: Sequence to class (IMDB classifier)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从序列到类别（IMDB分类器）
- en: Sequence to equal length sequence (Language model)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从序列到等长序列（语言模型）
- en: But we do not know yet how to do a general purpose sequence to sequence, so
    that’s the new thing today. Very little of this will make sense unless you really
    understand lesson 6 how an RNN works.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们还不知道如何做一个通用的序列到序列，所以这是今天的新内容。除非你真正理解第6课中RNN的工作原理，否则很少有人能理解这一点。
- en: Quick review of [Lesson 6](/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c)
    [[18:20](https://youtu.be/tY0n9OT5_nA?t=18m20s)]
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速回顾[第6课](/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c) [[18:20](https://youtu.be/tY0n9OT5_nA?t=18m20s)]
- en: We learnt that an RNN at its heart is a standard fully connected network. Below
    is one with 4 layers — takes an input and puts it through four layers, but at
    the second layer, it concatenates in the second input, third layer concatenated
    in the third input, but we actually wrote this in Python as just a four layer
    neural network. There was nothing else we used other than linear layers and ReLUs.
    We used the same weight matrix every time when an input came in, we used the same
    matrix every time when we went from one of the hidden states to the next — that
    is why these arrows are the same color.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到，RNN在其核心是一个标准的全连接网络。下面是一个有4层的网络——接受一个输入并通过四层，但在第二层，它将第二个输入连接起来，第三层将第三个输入连接起来，但实际上我们在Python中只写了一个四层的神经网络。除了线性层和ReLU之外，我们没有使用其他东西。每次输入时我们使用相同的权重矩阵，每次从一个隐藏状态到下一个时我们也使用相同的矩阵——这就是为什么这些箭头是相同颜色的原因。
- en: We can redraw the above diagram like the below [[19:29](https://youtu.be/tY0n9OT5_nA?t=19m29s)].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将上面的图重新绘制成下面的样子[[19:29](https://youtu.be/tY0n9OT5_nA?t=19m29s)]。
- en: Not only did we redraw it but we took the four lines of linear linear linear
    linear code in PyTorch and we replaced it with a for loop. Remember, we had something
    that did exactly the same thing as below, but it just had four lines of code saying
    `self.l_in(input)` and we replaced it with a for loop because that’s nice to refactor.
    The refactoring which does not change any of the math, any of the ideas, or any
    of the outputs is an RNN. It’s turning a bunch of separate lines in the code into
    a Python for loop.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅重新绘制了它，还将PyTorch中的四行线性代码替换为一个for循环。记住，我们有一个与下面完全相同的东西，但只有四行代码说`self.l_in(input)`，我们用一个for循环替换了它，因为这样重构很好。不改变任何数学、任何想法或任何输出的重构是一个RNN。它将代码中的一堆单独的行转换为Python的for循环。
- en: We could take the output so that it is not outside the loop and put it inside
    the loop [[20:25](https://youtu.be/tY0n9OT5_nA?t=20m25s)]. If we do that, we are
    now going to generate a separate output for every input. The code above, the hidden
    state gets replaced each time and we end up just spitting out the final hidden
    state. But if instead, we had something that said `hs.append(h)` and returned
    `hs` at the end, that would be the picture below.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将输出放在循环内部而不是在循环外部[[20:25](https://youtu.be/tY0n9OT5_nA?t=20m25s)]。如果这样做，我们现在将为每个输入生成一个单独的输出。上面的代码，隐藏状态每次都被替换，最终我们只输出最终的隐藏状态。但是如果我们有一个说`hs.append(h)`并在最后返回`hs`的东西，那就是下面的图片。
- en: The main thing to remember is when we say hidden state, we are referring to
    a vector — technically a vector for each thing in the mini-batch so it’s a matrix,
    but generally when Jeremy speaks about these things, he ignores the mini-batch
    piece and treat it for just a single item.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要记住的主要事情是当我们说隐藏状态时，我们指的是一个向量——技术上是每个小批量中的每个东西的向量，所以它是一个矩阵，但通常当Jeremy谈到这些事情时，他忽略了小批量部分，将其视为单个项目。
- en: We also learned that you can stack these layers on top of each other [[21:41](https://youtu.be/tY0n9OT5_nA?t=21m41s)].
    So rather than the left RNN (in the diagram above) spitting out output, they could
    just spit out inputs into a second RNN. If you are thinking at this point “I think
    I understand this but I am not quite sure” that means you don’t understand this.
    The only way you know that you actually understand it is to go and write this
    from scratch in PyTorch or Numpy. If you can’t do that, then you know you don’t
    understand it and you can go back and re-watch lesson 6 and check out the notebook
    and copy some of the ideas until you can. It is really important that you can
    write that from scratch — it’s less than a screen of code. So you want to make
    sure you can create a 2 layer RNN. Below is what it looks like if you unroll it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学到可以将这些层堆叠在一起[[21:41](https://youtu.be/tY0n9OT5_nA?t=21m41s)]。所以与其上面图中的左侧RNN输出，它们可以将输入传递到第二个RNN中。如果你此时在想“我想我理解了，但我不太确定”，那意味着你并没有理解。你真正理解的唯一方法是从头开始用PyTorch或Numpy编写这个。如果你做不到，那么你知道你并没有理解，你可以回去重新观看第6课，并查看笔记本，复制一些想法，直到你能够。重要的是你能够从头开始编写它——不到一屏的代码。所以你要确保你可以创建一个2层的RNN。下面是展开它的样子。
- en: To get to a point that we have (x, y) pairs of sentences, we will start by downloading
    the dataset [[22:39](https://youtu.be/tY0n9OT5_nA?t=22m39s)]. Training a translation
    model takes a long time. Google’s translation model has eight layers of RNN stacked
    on top of each other. There is no conceptual difference between eight layers and
    two layers. If you are Google and you have more GPUs or TPUs than you know what
    to do with, then you are fine doing that. Where else, in our case, it’s pretty
    likely that the kind of sequence to sequence models we are building are not going
    to require that level of computation. So to keep things simple [[23:22](https://youtu.be/tY0n9OT5_nA?t=23m22s)],
    let’s do a cut-down thing where rather than learning how to translate French into
    English for any sentence, let’s learn to translate French questions into English
    questions — specifically questions that start with what/where/which/when. So here
    is a regex which looks for things that start with “wh” and end with a question
    mark.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到（x，y）句子对，我们将从下载数据集开始[[22:39](https://youtu.be/tY0n9OT5_nA?t=22m39s)]。训练一个翻译模型需要很长时间。谷歌的翻译模型有八层RNN堆叠在一起。八层和两层之间没有概念上的区别。如果你是谷歌，有更多的GPU或TPU，那么你可以这样做。否则，在我们的情况下，我们构建的序列到序列模型很可能不需要那种计算水平。所以为了保持简单[[23:22](https://youtu.be/tY0n9OT5_nA?t=23m22s)]，让我们做一个简化的事情，而不是学习如何翻译法语到英语的任何句子，让我们学习如何将法语问题翻译成英语问题——具体是以what/where/which/when开头的问题。这里有一个正则表达式，寻找以“wh”开头并以问号结尾的内容。
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We go through the corpus [[23:43](https://youtu.be/tY0n9OT5_nA?t=23m43s)], open
    up each of the two files, each line is one parallel text, zip them together, grab
    the English question and the French question, and check whether they match the
    regular expressions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历语料库，打开两个文件中的每一个，每一行是一个平行文本，将它们压缩在一起，获取英语问题和法语问题，并检查它们是否匹配正则表达式。
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Dump that out as a pickle so we don’t have to do it again and so now we have
    52,000 sentence pairs and here are some examples:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 将其转储为一个pickle，这样我们就不必再次执行它，现在我们有52,000个句子对，这里有一些示例：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: One nice thing about this is that what/who/where type questions tend to be fairly
    short [[24:08](https://youtu.be/tY0n9OT5_nA?t=24m8s)]. But the idea that we could
    learn from scratch with no previous understanding of the idea of language let
    alone of English or French that we could create something that can translate one
    to the other for any arbitrary question with only 50k sentences sounds like a
    ludicrously difficult thing to ask this to do. So it would be impressive if we
    can make any progress what so ever. This is very little data to do a very complex
    exercise.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的一个好处是，关于什么/谁/在哪里类型的问题往往相当简短。但是，我们可以从零开始学习，没有对语言概念的先前理解，更不用说英语或法语，我们可以创建一个可以将一个语言翻译成另一种语言的东西，对于任何任意问题，只需要50k个句子，听起来像是一个难以置信的困难任务。因此，如果我们能取得任何进展，那将是令人印象深刻的。这是一个非常少的数据来进行一个非常复杂的练习。
- en: '`qs` contains the tuples of French and English [[24:48](https://youtu.be/tY0n9OT5_nA?t=24m48s)].
    You can use this handy idiom to split them apart into a list of English questions
    and a list of French questions.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`qs`包含法语和英语的元组。你可以使用这个方便的习语将它们分开成一个英语问题列表和一个法语问题列表。'
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then we tokenize the English questions and tokenize the French questions. So
    remember that just means splitting them up into separate words or word-like things.
    By default [[25:11](https://youtu.be/tY0n9OT5_nA?t=25m11s)], the tokenizer that
    we have here (remember this is a wrapper around the spaCy tokenizer which is a
    fantastic tokenizer) assumes English. So to ask for French, you just add an extra
    parameter `'fr'`. The first time you do this, you will get an error saying you
    don’t have the spaCy French model installed so you can run `python -m spacy download
    fr` to grab the French model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对英语问题进行标记化，对法语问题进行标记化。所以记住，这只是将它们分剒成单独的单词或类似单词的东西。默认情况下，我们这里有的标记器（记住这是一个包装在spaCy标记器周围的标记器，它是一个很棒的标记器）假设是英语。所以要求法语，你只需添加一个额外的参数'fr'。第一次这样做时，你会收到一个错误，说你没有安装spaCy法语模型，所以你可以运行`python
    -m spacy download fr`来获取法语模型。
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It is unlikely that any of you are going to have RAM problems here because
    this is not particularly big corpus but some of the students were trying to train
    a new language models during the week and were having RAM problems. If you do,
    it’s worth knowing what these functions (`proc_all_mp`) are actually doing. `proc_all_mp`
    is processing every sentence across multiple processes [[25:59](https://youtu.be/tY0n9OT5_nA?t=25m59s)]:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你们中没有人会遇到RAM问题，因为这不是特别大的语料库，但是有些学生在这一周尝试训练新的语言模型时遇到了RAM问题。如果你遇到了，了解这些函数（`proc_all_mp`）实际在做什么是值得的。`proc_all_mp`正在跨多个进程处理每个句子：
- en: The function above finds out how many CPUs you have, divide it by two (because
    normally with hyper-threading they don’t actually all work in parallel), then
    in parallel run this `proc_all` function. So that is going to spit out a whole
    separate Python processes for every CPU you have. If you have a lot of cores,
    that is a lot of Python processes — everyone is going to load all this data in
    and that can potentially use up all your RAM. So you could replace that with just
    `proc_all` rather than `proc_all_mp` to use less RAM. Or you could just use less
    cores. At the moment, we are calling `partition_by_cores` which calls `partition`
    on a list and asks to split it into a number of equal length things according
    to how many CPUs you have. So you could replace that to split into a smaller list
    and run it on less things.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的函数找出你有多少个CPU，将其除以二（因为通常情况下，由于超线程，它们实际上并不都是并行工作的），然后并行运行这个`proc_all`函数。这将为你的每个CPU生成一个完全独立的Python进程。如果你有很多核心，那就是很多Python进程——每个人都将加载所有这些数据，这可能会使用完所有你的RAM。所以你可以用`proc_all`替换它，而不是用`proc_all_mp`来使用更少的RAM。或者你可以只使用更少的核心。目前，我们正在调用`partition_by_cores`，它在列表上调用`partition`，并要求根据你有多少个CPU将其分剒成一些等长的部分。所以你可以将其替换为将列表分割成更小的部分，并在更少的部分上运行它。
- en: 'Having tokenized the English and French, you can see how it gets split up [[28:04](https://youtu.be/tY0n9OT5_nA?t=28m4s)]:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在对英语和法语进行标记化后，你可以看到它是如何分割的。
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can see the tokenization for French is quite different looking because French
    loves their apostrophes and their hyphens. So if you try to use an English tokenizer
    for a French sentence, you’re going to get a pretty crappy outcome. You don’t
    need to know heaps of NLP ideas to use deep learning for NLP, but just some basic
    stuff like use the right tokenizer for your language is important [[28:23](https://youtu.be/tY0n9OT5_nA?t=28m23s)].
    Some of the students this week in our study group have been trying to build language
    models for Chinese instance which of course doesn’t really have the concept of
    a tokenizer in the same way, so we’ve been starting to look at [sentence piece](https://github.com/google/sentencepiece)
    which splits things into arbitrary sub-word units and so when Jeremy says tokenize,
    if you are using a language that doesn’t have spaces in, you should probably be
    checking out sentence piece or some other similar sub-word unit thing instead.
    Hopefully in the next week or two, we will be able to report back with some early
    results of these experiments with Chinese.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到法语的标记化看起来非常不同，因为法语喜欢他们的撇号和连字符。因此，如果您尝试为法语句子使用英语标记器，您将得到一个相当糟糕的结果。您不需要了解大量的自然语言处理（NLP）理念来使用深度学习进行自然语言处理，但只需要一些基本的东西，比如使用正确的标记器对于您的语言是重要的。本周我们研究小组中的一些学生一直在尝试为中文实例构建语言模型，当然中文并没有真正的标记化概念，所以我们开始研究[sentence
    piece](https://github.com/google/sentencepiece)，它将事物分割成任意的子词单元，所以当Jeremy说标记化时，如果您使用的是没有空格的语言，您应该考虑使用sentence
    piece或其他类似的子词单元。希望在接下来的一两周内，我们将能够报告这些中文实验的一些早期结果。
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So having tokenized it [[29:25](https://youtu.be/tY0n9OT5_nA?t=29m25s)], we
    will save that to disk. Then remember, the next step after we create tokens is
    to turn them into numbers . To do that, we have two steps — the first is to get
    a list of all of the words that appear and then we turn every word into the index.
    If there are more than 40,000 words that appear, then let’s cut it off there so
    it doesn’t go too crazy. We insert a few extra tokens for beginning of stream
    (`_bos_`), padding (`_pad_`), end of stream (`_eos_`), and unknown (`_unk`). So
    if we try to look up something that wasn’t in the 40,000 most common, then we
    use a `deraultdict` to return 3 which is unknown.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在标记化之后，我们将其保存到磁盘。然后记住，在我们创建标记之后的下一步是将它们转换为数字。为此，我们有两个步骤——第一步是获取所有出现的单词的列表，然后我们将每个单词转换为索引。如果出现的单词超过40,000个，那么让我们在那里截断，以免变得太疯狂。我们插入一些额外的标记，用于流的开始（`_bos_`）、填充（`_pad_`）、流的结束（`_eos_`）和未知（`_unk`）。因此，如果我们尝试查找不在最常见的40,000个单词中的东西，那么我们使用`deraultdict`返回3，即未知。
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we can go ahead and turn every token into an ID by putting it through the
    string to integer dictionary (`stoi`) we just created and then at the end of that
    let’s add the number 2 which is the end of stream. The code you see here is the
    code Jeremy writes when he is iterating and experimenting [[30:25](https://youtu.be/tY0n9OT5_nA?t=30m25s)].
    Because 99% of the code he writes while iterating and experimenting turns out
    to be totally wrong or stupid or embarrassing and you don’t get to see it. But
    there is not point refactoring that and making it beautiful when he’s writing
    it so he wanted you to see all the little shortcuts he has. Rather than having
    some constant for `_eos_` marker and using that, when he is prototyping he just
    does the easy stuff. Not so much that he ends up with broken code but he tries
    to find some middle ground between beautiful code and code that works.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续，通过将每个标记放入我们刚刚创建的字符串到整数字典（`stoi`）中，将每个标记转换为ID，然后在最后添加数字2，即流的结束。你在这里看到的代码是Jeremy在迭代和实验时编写的代码。因为他在迭代和实验时编写的代码中，99%都是完全错误的、愚蠢的或令人尴尬的，你看不到。但是在他编写代码时，没有必要重构它并使其变得美观，所以他希望你看到他所有的小技巧。与其为`_eos_`标记使用某个常量并使用它，当他在原型设计时，他只做简单的事情。并不是说他最终会得到错误的代码，但他试图在美丽的代码和可行的代码之间找到一些折中。
- en: '**Question**: Just heard him mention that we divide the number of CPUs by 2
    because with hyper-threading, we don’t get a speed-up using all the hyper threaded
    cores. Is this based on practical experience or is there some underlying reason
    why we wouldn’t get additional speedup [[31:18](https://youtu.be/tY0n9OT5_nA?t=31m18s)]?
    Yes, it’s just practical experience and it’s not all things seemed like this,
    but I definitely noticed with tokenization — hyper-threading seemed to slow things
    down a little bit. Also if I use all the cores, often I want to do something else
    at the same time (like running some interactive notebook) and I don’t have any
    spare room to do that.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：刚听他提到我们将CPU数量除以2，因为使用超线程时，我们不会通过使用所有超线程核心来加速。这是基于实际经验还是有一些潜在原因导致我们无法获得额外的加速？是的，这只是实际经验，并不是所有事情都像这样，但我确实注意到在标记化时，超线程似乎会使事情变慢一点。此外，如果我使用所有核心，通常我想同时做一些其他事情（比如运行一些交互式笔记本），我没有多余的空间来做那些事情。'
- en: Now for our English and French, we can grab a list of IDs `en_ids` [[32:01](https://youtu.be/tY0n9OT5_nA?t=32m1s)].
    When we do that, of course, we need to make sure that we also store the vocabulary.
    There is no point having IDs if we don’t know what a number 5 represents, there
    is no point having a number 5\. So that’s our vocabulary `en_itos` and reverse
    mapping `en_stoi` that we can use to convert more corpuses in the future.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对于我们的英语和法语，我们可以获取一个ID列表`en_ids`。当我们这样做时，当然，我们需要确保我们也存储了词汇。如果我们不知道数字5代表什么，那么拥有ID就没有意义，拥有数字5也没有意义。所以这就是我们的词汇`en_itos`和反向映射`en_stoi`，我们可以用它们来在将来转换更多的语料库。
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Just to confirm it’s working, we can go through each ID, convert the int to
    a string, and spit that out — there we have our sentence back now with an end
    of stream marker at the end. Our English vocab is 17,000 and our French vocab
    is 25,000, so that’s not too big and not too complex vocab that we are dealing
    with.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认它是否有效，我们可以通过每个ID，将int转换为字符串，并将其输出 - 现在我们的句子已经回来了，末尾有一个流标记。我们的英语词汇量为17,000，法语词汇量为25,000，所以我们处理的词汇量既不太大也不太复杂。
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Word vectors [[32:53](https://youtu.be/tY0n9OT5_nA?t=32m53s)]
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词向量[[32:53](https://youtu.be/tY0n9OT5_nA?t=32m53s)]
- en: We spent a lot of time on the forum during the week discussing how pointless
    word vectors are and how you should stop getting so excited about them — and now
    we are going to use them. Why? All the stuff we’ve been learning about using language
    models and pre-trained proper models rather than pre-trained linear single layers
    which is what word vectors are, applies equally well to sequence to sequence.
    But Jeremy and Sebastian are starting to look at that. There is a whole thing
    for anybody interested in creating some genuinely new highly publishable results,
    the entire area of sequence to sequence with pre-trained language models has not
    been touched yet. Jeremy believes it is going to be just as good as classifications.
    If you work on this and you get to the point where you have something that is
    looking exciting and you want help publishing it, Jeremy is very happy to help
    co-author papers. So feel free to reach out when you have some interesting results.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一周的论坛上，我们花了很多时间讨论词向量是多么无聊，以及你应该停止对它们感到兴奋 - 现在我们要使用它们。为什么？我们一直在学习如何使用语言模型和预训练的正确模型，而不是预训练的线性单层，这就是词向量的内容，同样适用于序列到序列。但Jeremy和Sebastian正在开始研究这个问题。对于任何有兴趣创造一些真正新颖且高度可发表的结果的人来说，序列到序列与预训练语言模型的整个领域尚未被触及。Jeremy相信这将和分类一样好。如果您在这方面有所作为，并且您已经有了一些看起来令人兴奋的东西，并且您希望得到帮助发表它，Jeremy非常乐意帮助共同撰写论文。因此，当您有一些有趣的结果时，请随时联系。
- en: At this stage, we do not have any of that, so we are going to use very little
    fastai [[34:14](https://youtu.be/tY0n9OT5_nA?t=34m14s)]. All we have is word vectors
    — so let’s at least use decent word vectors. Word2vec is very old word word vectors.
    There are better word vectors now and fast.text is a pretty good source of word
    vectors. There is hundreds of languages available for them, and your language
    is likely to be represented.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们没有任何东西，所以我们将使用非常少的fastai[[34:14](https://youtu.be/tY0n9OT5_nA?t=34m14s)]。我们只有词向量
    - 所以让我们至少使用体面的词向量。Word2vec是非常古老的词向量。现在有更好的词向量，而fast.text是一个相当不错的词向量来源。有数百种语言可用，您的语言可能会被代表。
- en: fasttext word vectors available from [https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: fasttext词向量可从[https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html)获取
- en: 'fasttext Python library is not available in PyPI but here is a handy trick
    [[35:03](https://youtu.be/tY0n9OT5_nA?t=35m3s)]. If there is a GitHub repo that
    has a setup.py and reqirements.txt in it, you can just chuck `git+` at the start
    then stick that in your `pip install` and it works. Hardly anybody seems to know
    this and if you go to the fasttext repo, they won’t tell you this — they’ll say
    you have to download it and `cd` into it and blah but you don’t. You can just
    run this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: fasttext Python库在PyPI中不可用，但这里有一个方便的技巧[[35:03](https://youtu.be/tY0n9OT5_nA?t=35m3s)]。如果有一个GitHub存储库，其中包含setup.py和reqirements.txt，您只需在开头加上`git+`，然后将其放入`pip
    install`中，它就会起作用。几乎没有人似乎知道这一点，如果您去fasttext存储库，他们不会告诉您这一点 - 他们会告诉您必须下载它并`cd`进入它，等等，但您不必这样做。您只需运行以下命令：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To use the fastText library, you’ll need to download [fasttext word vectors](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)
    for your language (download the ‘bin plus text’ ones).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用fastText库，您需要下载[fasttext词向量](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)（下载“bin
    plus text”）。
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Above are our English and French models. There are a text version and a binary
    version. The binary version is faster, so we will use that. The text version is
    also a bit buggy. We are going to convert it into a standard Python dictionary
    to make it a bit easier to work with [[35:55](https://youtu.be/tY0n9OT5_nA?t=35m55s)].
    This is just going through each word with a dictionary comprehension and save
    it as a pickle dictionary:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以上是我们的英语和法语模型。有文本版本和二进制版本。二进制版本更快，所以我们将使用它。文本版本也有点buggy。我们将把它转换为标准的Python字典，以使其更容易使用[[35:55](https://youtu.be/tY0n9OT5_nA?t=35m55s)]。这只是通过字典理解遍历每个单词，并将其保存为pickle字典：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now we have our pickle dictionary, we can go ahead and look up a word, for example,
    a comma [[36:07](https://youtu.be/tY0n9OT5_nA?t=36m7s)]. That will return a vector.
    The length of the vector is the dimensionality of this set of word vectors. In
    this case, we have 300 dimensional English and French word vectors.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的pickle字典，我们可以继续查找一个单词，例如逗号[[36:07](https://youtu.be/tY0n9OT5_nA?t=36m7s)]。这将返回一个向量。向量的长度是这组词向量的维度。在这种情况下，我们有300维的英语和法语词向量。
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For reasons you will see in a moment, we also want to find out what the mean
    and standard deviation of our vectors are. So the mean is about zero and standard
    deviation is about 0.3.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 出于即将看到的原因，我们还想找出我们的向量的平均值和标准差。所以平均值约为零，标准差约为0.3。
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Model data [[36:48](https://youtu.be/tY0n9OT5_nA?t=36m48s)]
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型数据[[36:48](https://youtu.be/tY0n9OT5_nA?t=36m48s)]
- en: 'Often corpuses have a pretty long tailed distribution of sequence length and
    it’s the longest sequences that tend to overwhelm how long things take, how much
    memory is used, etc. So in this case, we are going to grab 99th to 97th percentile
    of the English and French and truncate them to that amount. Originally Jeremy
    was using 90 percentiles (hence the variable name):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，语料库的序列长度具有相当长尾的分布，而最长的序列往往会压倒性地影响时间、内存使用等。因此，在这种情况下，我们将获取英语和法语的第99到97百分位数，并将它们截断到该数量。最初Jeremy使用的是90百分位数（因此变量名）：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We are nearly there [[37:24](https://youtu.be/tY0n9OT5_nA?t=37m24s)]. We’ve
    got our tokenized, numerixalized English and French dataset. We’ve got some word
    vectors. So now we need to get it ready for PyTorch. PyTorch expects a `Dataset`
    object and hopefully by now you can say that a Dataset object requires two things
    — a length (`__len__`)and an indexer (`__getitem__`). Jeremy started out writing
    `Seq2SeqDataset` which turned out to be just a generic `Dataset` [[37:52](https://youtu.be/tY0n9OT5_nA?t=37m52s)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快要完成了[[37:24](https://youtu.be/tY0n9OT5_nA?t=37m24s)]。我们已经有了我们的标记化、数字化的英语和法语数据集。我们有一些词向量。现在我们需要为PyTorch准备好它。PyTorch需要一个`Dataset`对象，希望到现在为止你可以说一个Dataset对象需要两个东西——一个长度(`__len__`)和一个索引器(`__getitem__`)。Jeremy开始编写`Seq2SeqDataset`，结果只是一个通用的`Dataset`[[37:52](https://youtu.be/tY0n9OT5_nA?t=37m52s)]。
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`A` : Arrays. It will go through each of the thing you pass it, if it is not
    already a numpy array, it converts into a numpy array and returns back a tuple
    of all of the things you passed it which are now guaranteed to be numpy arrays
    [[38:32](https://youtu.be/tY0n9OT5_nA?t=38m32s)].'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`A`：数组。它将遍历您传递的每个对象，如果它还不是一个numpy数组，它会将其转换为一个numpy数组，并返回一个元组，其中包含您传递的所有现在保证为numpy数组的对象[[38:32](https://youtu.be/tY0n9OT5_nA?t=38m32s)]。'
- en: '`V` : Variables'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V`：变量'
- en: '`T` : Tensors'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`T`：张量'
- en: Training set and validation set [[39:03](https://youtu.be/tY0n9OT5_nA?t=39m3s)]
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练集和验证集[[39:03](https://youtu.be/tY0n9OT5_nA?t=39m3s)]
- en: Now we need to grab our English and French IDs and get a training set and a
    validation set. One of the things which is pretty disappointing about a lot of
    code out there on the internet is that they don’t follow some simple best practices.
    For example, if you go to PyTorch website, they have an example section for sequence
    to sequence translation. Their example does not have a separate validation set.
    Jeremy tried training according to their settings and tested it with a validation
    set and it turned out that it overfit massively. So this is not just a theoretical
    problem — the actual PyTorch repo has the actual official sequence to sequence
    translation example which does not check for overfitting and overfits horribly
    [[39:41](https://youtu.be/tY0n9OT5_nA?t=39m41s)]. Also it fails to use mini-batches
    so it actually fails to utilize any of the efficiency of PyTorch whatsoever. Even
    if you find code in the official PyTorch repo, don’t assume it’s any good at all.
    The other thing you’ll notice is that pretty much every other sequence to sequence
    model Jeremy found in PyTorch anywhere on the internet has clearly copied from
    that crappy PyTorch repo because all has the same variable names, it has the same
    problems, it has the same mistakes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要获取我们的英语和法语ID，并获得一个训练集和一个验证集。互联网上许多代码令人失望的一点是它们没有遵循一些简单的最佳实践。例如，如果你去PyTorch网站，他们有一个关于序列到序列翻译的示例部分。他们的示例没有单独的验证集。Jeremy尝试根据他们的设置进行训练，并使用验证集进行测试，结果发现它严重过拟合。因此，这不仅仅是一个理论问题——实际的PyTorch存储库有实际的官方序列到序列翻译示例，它没有检查过拟合，严重过拟合[[39:41](https://youtu.be/tY0n9OT5_nA?t=39m41s)]。此外，它没有使用小批量，因此实际上没有充分利用PyTorch的任何效率。即使你在官方PyTorch存储库中找到代码，也不要认为它是好的。你会注意到的另一件事是，Jeremy在互联网上找到的几乎每个PyTorch序列到序列模型都明显是从那个糟糕的PyTorch存储库中复制的，因为它们都有相同的变量名，有相同的问题，有相同的错误。
- en: Another example is that nearly every PyTorch convolutional neural network Jeremy
    found does not use an adaptive pooling layer [[40:27](https://youtu.be/tY0n9OT5_nA?t=40m27s)].
    So in other words, the final layer is always average pool (7,7). They assume that
    the previous layer is 7 by 7 and if you use any other size input, you get an exception,
    and therefore nearly everybody Jeremy has spoken who uses PyTorch thinks that
    there is a fundamental limitation of CNNs that they are tied to the input size
    and that has not been true since VGG. So every time Jeremy grabs a new model and
    stick it in the fastai repo, he has to go and search for “pool” and add “adaptive”
    to the start and replace the 7 with a 1and now it works on any sized object. So
    just be careful. It’s still early days and believe it or not, even though most
    of you have only started in the last year your deep learning journey, you know
    quite a lot more about a lot of the more important practical aspects than the
    vast majority of people that have publishing and writing stuff in official repos.
    So you need to have a little more self-confidence than you might expect when it
    comes to reading other people’s code. If you find yourself thinking “that looks
    odd”, it’s not necessarily you.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，Jeremy找到的几乎每个PyTorch卷积神经网络都没有使用自适应池化层[[40:27](https://youtu.be/tY0n9OT5_nA?t=40m27s)]。换句话说，最终层总是平均池化(7,7)。他们假设前一层是7乘7，如果你使用任何其他大小的输入，你会得到一个异常，因此几乎每个使用PyTorch的人都认为CNNs有一个基本限制，即它们与输入大小相关联，这自从VGG以来就不再成立。因此，每当Jeremy拿到一个新模型并将其放入fastai存储库时，他都必须搜索“pool”并在开头添加“adaptive”，将7替换为1，现在它适用于任何大小的对象。所以要小心。现在仍然是早期阶段，信不信由你，即使你们大多数人只在过去一年开始了深度学习之旅，你们对许多更重要的实际方面了解的要比大多数在官方存储库中发布和编写东西的人多得多。因此，当阅读其他人的代码时，你需要比你期望的更有一些自信。如果你发现自己在想“那看起来很奇怪”，那不一定是你。
- en: If the repo you are looking at doesn’t have a section on it saying here is the
    test we did where we got the same results as the paper that’s supposed to be implementing,
    that almost certainly means they haven’t got the same results of the paper they’re
    implementing, but probably haven’t even checked [[42:13](https://youtu.be/tY0n9OT5_nA?t=42m13s)].
    If you run it, definitely won’t get those results because it’s hard to get things
    right the first time — it takes Jeremy 12 goes. If they haven’t tested it once,
    it’s almost certainly won’t work.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在查看的存储库没有一个部分说这里是我们做的测试，我们得到了与应该实现的论文相同的结果，那几乎肯定意味着他们没有得到他们正在实现的论文相同的结果，甚至可能根本没有检查[[42:13](https://youtu.be/tY0n9OT5_nA?t=42m13s)]。如果你运行它，肯定不会得到那些结果，因为第一次做对事情很难——Jeremy需要尝试12次。如果他们没有测试过一次，几乎肯定不会起作用。
- en: Here is an easy way to get training and validation sets [[42:45](https://youtu.be/tY0n9OT5_nA?t=42m45s)].
    Grab a bunch of random numbers — one for each row of your data, and see if they
    are bigger than 0.1 or not. That gets you a list of booleans. Index into your
    array with that list of booleans to grab a training set, index into that array
    with the opposite of that list of booleans to get your validation set.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个获取训练和验证集的简单方法[42:45]。获取一堆随机数 - 每行数据一个，然后看它们是否大于0.1。这会给你一个布尔值列表。使用该布尔值列表索引到你的数组中以获取一个训练集，使用该布尔值列表的相反值索引到该数组中以获取你的验证集。
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now we can create our dataset with our X’s and Y’s (i.e. French and English)[[43:12](https://youtu.be/tY0n9OT5_nA?t=43m12s)].
    If you want to translate instead English to French, switch these two around and
    you’re done.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以用我们的X和Y（即法语和英语）创建我们的数据集[43:12]。如果你想将英语翻译成法语，只需交换这两个，就完成了。
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now we need to create DataLoaders [[43:22](https://youtu.be/tY0n9OT5_nA?t=43m22s)].
    We can just grab our data loader and pass in our dataset and batch size. We actually
    have to transpose the arrays — we won’t go into the details about why, but we
    can talk about it during the week if you’re interested but have a think about
    why we might need to transpose their orientation. Since we’ve already done all
    the pre-processing, there is no point spawning off multiple workers to do augmentation,
    etc because there is no work to do. So `making num_workers=1` will save you some
    time. We have to tell it what our padding index is — that is pretty important
    because what’s going to happen is that we’ve got different length sentences and
    fastai will automatically stick them together and pad the shorter ones so that
    they are all equal length. Remember a tensor has to be rectangular.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要创建DataLoaders[43:22]。我们只需获取我们的数据加载器并传入我们的数据集和批量大小。我们实际上必须转置数组 - 我们不会详细讨论为什么，但如果你感兴趣，我们可以在这一周讨论，但想一想为什么我们可能需要转置它们的方向。由于我们已经完成了所有的预处理，没有必要启动多个工作人员来进行增强等工作，因为没有工作要做。因此，`使num_workers=1`会节省一些时间。我们必须告诉它我们的填充索引是什么
    - 这非常重要，因为将会发生的是，我们有不同长度的句子，fastai将自动将它们粘在一起并填充较短的句子，使它们长度相等。记住张量必须是矩形的。
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the decoder in particular, we want our padding to be at the end, not at
    the start [[44:29](https://youtu.be/tY0n9OT5_nA?t=44m29s)]:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在解码器中，我们希望我们的填充在末尾，而不是在开头[44:29]：
- en: Classifier → padding in the beginning. Because we want that final token to represent
    the last word of the movie review.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器 → 在开头填充。因为我们希望最终的标记代表电影评论的最后一个单词。
- en: Decoder → padding at the end. As you will see, it actually is going to work
    out a bit better to have the padding at the end.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器 → 在末尾填充。正如你将看到的，将填充放在末尾实际上会更好一些。
- en: '**Sampler [**[**44:54**](https://youtu.be/tY0n9OT5_nA?t=44m54s)**]** Finally,
    since we’ve got sentences of different lengths coming in and they all have to
    be put together in a mini-batch to be the same size by padding, we would much
    prefer that the sentences in a mini-batch are of similar sizes already. Otherwise
    it is going to be as long as the longest sentence and that is going to end up
    wasting time and memory. Therefore, we are going to use the sampler tricks that
    we learnt last time which is the validation set, we are going to ask it to sort
    everything by length first. Then for the training set, we are going to randomize
    the order of things but to roughly make it so that things of similar length are
    about in the same spot.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**采样器 [44:54]** 最后，由于我们输入的句子长度不同，它们都必须通过填充放在一个小批次中以使它们具有相同的大小，我们更希望小批次中的句子已经具有相似的大小。否则，它将与最长的句子一样长，这将浪费时间和内存。因此，我们将使用上次学到的采样器技巧，即对验证集，我们将要求它首先按长度排序。然后对于训练集，我们将随机排列事物的顺序，但大致使得长度相似的事物大致在同一位置。'
- en: '**Model Data [**[**45:40**](https://youtu.be/tY0n9OT5_nA?t=45m40s)**]** At
    this point, we can create a model data object — remember a model data object really
    does one thing which is it says “I have a training set and a validation set, and
    an optional test set” and sticks them into a single object. We also has a path
    so that it has somewhere to store temporary files, models, stuff like that.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型数据 [45:40]** 在这一点上，我们可以创建一个模型数据对象 - 记住，模型数据对象实际上只做一件事，那就是它说“我有一个训练集和一个验证集，还有一个可选的测试集”，然后把它们放入一个单一对象中。我们还有一个路径，这样它就有地方存储临时文件、模型等等。'
- en: We are not using fastai for very much at all in this example. We used PyTorch
    compatible Dataset and and DataLoader — behind the scene it is actually using
    the fastai version because we need it to do the automatic padding for convenience,
    so there is a few tweaks in fastai version that are a bit faster and a bit more
    convenient. We are also using fastai’s Samplers, but there is not too much going
    on here.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们几乎没有使用fastai。我们使用了与PyTorch兼容的数据集和数据加载器 - 在幕后实际上使用的是fastai版本，因为我们需要它来方便地进行自动填充，因此在fastai版本中有一些稍微快速和更方便的调整。我们还使用了fastai的采样器，但这里没有太多的事情发生。
- en: Architecture [[46:59](https://youtu.be/tY0n9OT5_nA?t=46m59s)]
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构[46:59]
- en: The architecture is going to take our sequence of tokens.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个架构将接受我们的标记序列。
- en: It is going to spit them into an encoder (a.k.a. backbone).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将把它们传递给一个编码器（又名骨干）。
- en: That is going to spit out the final hidden state which for each sentence, it’s
    just a single vector.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将输出最终的隐藏状态，对于每个句子，它只是一个单一的向量。
- en: None of this is going to be new [[47:41](https://youtu.be/tY0n9OT5_nA?t=47m41s)].
    That is all going to be using very direct simple techniques that we’ve already
    learned.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都不会是新的[47:41]。这一切都将使用我们已经学过的非常直接简单的技术。
- en: Then we are going to take that, and we will spit it into a different RNN which
    is a decoder. That’s going to have some new stuff because we need something that
    can go through one word at a time. And it keeps going until it thinks it’s finished
    the sentence. It doesn’t know how long the sentence is going to be ahead of time.
    It keeps going until it thinks it’s finished the sentence and then it stops and
    returns a sentence.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将把它传递到另一个RNN中，这是一个解码器。这将有一些新的东西，因为我们需要一个可以逐个单词地进行处理的东西。它会一直进行下去，直到它认为已经完成了句子。它不知道句子将有多长。它会一直进行下去，直到它认为已经完成了句子，然后停止并返回一个句子。
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Let’s start with the encoder [[48:15](https://youtu.be/tY0n9OT5_nA?t=48m15s)].
    In terms of the variable naming here, there is identical attributes for encoder
    and decoder. The encoder version has `enc` the decoder version has `dec`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从编码器开始[[48:15](https://youtu.be/tY0n9OT5_nA?t=48m15s)]。在这里的变量命名方面，编码器和解码器具有相同的属性。编码器版本有`enc`，解码器版本有`dec`。
- en: '`emb_enc`: Embeddings for the encoder'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emb_enc`：编码器的嵌入'
- en: '`gru` : RNN. GRU and LSTM are nearly the same thing.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gru`：RNN。GRU和LSTM几乎是相同的东西。'
- en: We need to create an embedding layer because remember — what we are being passed
    is the index of the words into a vocabulary. And we want to grab their fast.text
    embedding. Then over time, we might want to also fine tune to train that embedding
    end-to-end.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个嵌入层，因为要记住 - 我们传递的是单词在词汇表中的索引。我们想要获取它们的fast.text嵌入。随着时间的推移，我们可能还想微调以端到端地训练该嵌入。
- en: '`create_emb` [[49:37](https://youtu.be/tY0n9OT5_nA?t=49m37s)]: It is important
    that you know now how to set the rows and columns for your embedding so the number
    of rows has to be equal to your vocabulary size — so each vocabulary has a word
    vector. The size of the embedding is determined by fast.text and fast.text embeddings
    are size 300\. So we have to use size 300 as well otherwise we can’t start out
    by using their embeddings.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_emb`[[49:37](https://youtu.be/tY0n9OT5_nA?t=49m37s)]：现在重要的是您知道如何设置嵌入的行和列，因此行数必须等于您的词汇量大小
    - 因此每个词汇都有一个词向量。嵌入的大小由fast.text确定，fast.text嵌入的大小为300。因此我们也必须使用大小300，否则我们无法使用它们的嵌入开始。'
- en: '`nn.Embedding` will initially going to give us a random set of embeddings [[50:12](https://youtu.be/tY0n9OT5_nA?t=50m12s)].
    So we will go through each one of these and if we find it in fast.text, we will
    replace it with the fast.text embedding. Again, something you should already know
    is that (`emb.weight.data`):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.Embedding`最初会给我们一组随机的嵌入[[50:12](https://youtu.be/tY0n9OT5_nA?t=50m12s)]。所以我们将遍历每一个，如果在fast.text中找到它，我们将用fast.text嵌入替换它。再次提醒您应该已经知道的是（`emb.weight.data`）：'
- en: A PyTorch module that is learnable has `weight` attribute
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可学习的PyTorch模块具有`weight`属性。
- en: '`weight` attribute is a `Variable` that has `data` attribute'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight`属性是一个具有`data`属性的`Variable`。'
- en: The `data` attribute is a tensor
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data`属性是一个张量'
- en: Now that we’ve got our weight tensor, we can just go through our vocabulary
    and we can look up the word in our pre-trained vectors and if we find it, we will
    replace the random weights with that pre-trained vector [[52:35](https://youtu.be/tY0n9OT5_nA?t=52m35s)].
    The random weights have a standard deviation of 1\. Our pre-trained vectors has
    a standard deviation of about 0.3\. So again, this is the kind of hacky thing
    Jeremy does when he is prototyping stuff, he just multiplied it by 3\. By the
    time you see the video of this, we may able to put all this sequence to sequence
    stuff into the fastai library, you won’t find horrible hacks like that in there
    (sure hope). But hack away when you are prototyping. Some things won’t be in fast.text
    in which case, we’ll just keep track of it [[53:22](https://youtu.be/tY0n9OT5_nA?t=53m22s)].
    The print statement is there so that we can see what’s going on (i.e. why are
    we missing stuff?). Remember we had about 30,000 so we are not missing too many.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了权重张量，我们可以遍历我们的词汇表，查找我们预训练向量中的单词，如果找到，我们将用该预训练向量替换随机权重[[52:35](https://youtu.be/tY0n9OT5_nA?t=52m35s)]。随机权重的标准差为1。我们的预训练向量的标准差约为0.3。所以，这是Jeremy在原型设计时做的一种巧妙的事情，他只是将其乘以3。当您看到这个视频时，我们可能已经能够将所有这些序列到序列的内容放入fastai库中，您在那里不会找到这样的可怕的黑客行为（希望如此）。但在原型设计时可以尝试各种方法。有些东西可能不在fast.text中，这种情况下，我们将继续跟踪[[53:22](https://youtu.be/tY0n9OT5_nA?t=53m22s)]。打印语句是为了让我们看到发生了什么（即为什么我们会丢失东西？）。记住我们大约有30,000个，所以我们不会丢失太多。
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Jeremy has started doing some stuff around incorporating large vocabulary handling
    into fastai — it’s not finished yet but hopefully by the time we get here, this
    kind of stuff will be possible [[56:50](https://youtu.be/tY0n9OT5_nA?t=56m50s)].
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy已经开始做一些关于将大词汇量处理整合到fastai中的工作 - 还没有完成，但希望到达这里时，这种工作将是可能的[[56:50](https://youtu.be/tY0n9OT5_nA?t=56m50s)]。
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The key thing to know is that encoder takes our inputs and spits out a hidden
    vector that hopefully will learn to contain all of the information about what
    that sentence says and how it sets it [[58:49](https://youtu.be/tY0n9OT5_nA?t=58m49s)].
    If it can’t do that, we can’t feed it into a decoder and hope it to spit our our
    sentence in a different language. So that’s what we want it to learn to do. We
    are not going to do anything special to make it learn to do that — we are just
    going to do the three things (data, architecture, loss function) and cross our
    fingers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要知道的关键是编码器接收我们的输入并输出一个隐藏向量，希望它能学会包含关于句子内容以及如何设置的所有信息[[58:49](https://youtu.be/tY0n9OT5_nA?t=58m49s)]。如果它做不到，我们就不能将其输入解码器，并希望它将句子翻译成另一种语言。这就是我们希望它学会的。我们不会采取任何特殊措施来让它学会这样做
    - 我们只会做三件事（数据、架构、损失函数），然后抱着幸运的心态。
- en: '**Decoder [**[**59:58**](https://youtu.be/tY0n9OT5_nA?t=59m58s)**]**: How do
    we now do the new bit? The basic idea of the new bit is the same. We are going
    to do exactly the same thing, but we are going to write our own for loop. The
    for loop is going to do exactly what the for loop inside PyTorch does for encoder,
    but we are going to do it manually. How big is the for loop? It’s an output sequence
    length (`out_sl`) which was something passed to the constructor which is equal
    to the length of the largest English sentence. Since we are translating into English,
    so it can’t possibly be longer than that at least in this corpus. If we then used
    it on some different corpus that was longer, this is going to fail — you could
    always pass in a different parameter, of course. So the basic idea is the same
    [[1:01:06](https://youtu.be/tY0n9OT5_nA?t=1h1m6s)].'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**解码器[**[**59:58**](https://youtu.be/tY0n9OT5_nA?t=59m58s)**]**：我们现在如何处理新的部分？新部分的基本思想是相同的。我们将做完全相同的事情，但我们将编写自己的for循环。这个for循环将完全执行PyTorch中编码器内部的for循环，但我们将手动执行。for循环有多大？它是一个输出序列长度（`out_sl`），这是传递给构造函数的一个参数，它等于最长英语句子的长度。因为我们正在翻译成英语，所以在这个语料库中至少不可能比这更长。如果我们将其用于某个更长的不同语料库，这将失败
    —— 当然你可以传入不同的参数。因此，基本思想是相同的[[1:01:06](https://youtu.be/tY0n9OT5_nA?t=1h1m6s)]。'
- en: We are going to go through and put it through the embedding.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将通过嵌入层。
- en: We are going to stick it through the RNN, dropout, and a linear layer.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将通过RNN、dropout和线性层。
- en: We will then append the output to a list which will be stacked into a single
    tensor and get returned.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将输出附加到一个列表中，该列表将堆叠成一个单个张量并返回。
- en: Normally, a recurrent neural network works on a whole sequence at a time, but
    we have a for loop to go through each part of the sequence separately [[1:01:37](https://youtu.be/tY0n9OT5_nA?t=1h1m37s)].
    Wo we have to add a leading unit axis to the start (`.unsqueeze(0)`) to basicaly
    say this is a sequence of length one. We are not really taking advantage of the
    recurrent net much at all — we could easily re-write this with a linear layer.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，递归神经网络一次处理整个序列，但我们有一个for循环来分别处理序列的每个部分[[1:01:37](https://youtu.be/tY0n9OT5_nA?t=1h1m37s)]。因此，我们必须在开头添加一个主要单位轴（`.unsqueeze(0)`）来表示这是一个长度为一的序列。我们实际上并没有充分利用递归网络
    —— 我们可以很容易地用线性层重写这个。
- en: 'One thing to be aware of is `dec_inp`[[1:02:34](https://youtu.be/tY0n9OT5_nA?t=1h2m34s)]:
    What is the input to the embedding? The answer is it is the previous word that
    we translated. The basic idea is if you are trying to translate the 4th word of
    the new sentence but you don’t know what the third word you just said was, that
    is going to be really hard. So we are going to feed that in at each time step.
    What was the previous word at the start? There was none. Specifically, we are
    going to start out with a beginning of stream token (`_bos_`) which is zero.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要注意的一件事是`dec_inp`[[1:02:34](https://youtu.be/tY0n9OT5_nA?t=1h2m34s)]：嵌入的输入是什么？答案是前一个我们翻译的单词。基本思想是，如果你试图翻译新句子的第四个单词，但你不知道刚说的第三个单词是什么，那将非常困难。因此，我们将在每个时间步骤中提供这个信息。在开始时，前一个单词是什么？没有。具体来说，我们将从一个流的开始标记（`_bos_`）开始，该标记为零。
- en: '`outp` [[1:05:24](https://youtu.be/tY0n9OT5_nA?t=1h5m24s)]: it is a tensor
    whose length is equal to the number of words in our English vocabulary and it
    contains the probability for every one of those words that it is that word.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`outp`[[1:05:24](https://youtu.be/tY0n9OT5_nA?t=1h5m24s)]：它是一个张量，其长度等于我们英语词汇中的单词数，其中包含每个单词是该单词的概率。'
- en: '`outp.data.max` : It looks in its tensor to find out which word has the highest
    probability. `max` in PyTorch returns two things: the first thing is what is that
    max probability and the second is what is the index into the array of that max
    probability. So we want that second item which is the word index with the largest
    thing.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`outp.data.max`：它在其张量中查找具有最高概率的单词。PyTorch中的`max`返回两个值：第一个是最大概率，第二个是该最大概率在数组中的索引。因此，我们想要第二个项目，即具有最大值的单词索引。'
- en: '`dec_inp` : It contains the word index into the vocabulary of the word. If
    it’s one (i.e. padding), that means we are done — we reached the end because we
    finished with a bunch of padding. If it’s not one, let’s go back and continue.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`dec_inp`：它包含单词在词汇表中的索引。如果是1（即填充），那么表示我们已经完成了 —— 我们已经以一堆填充结束了。如果不是1，让我们回去继续。'
- en: Each time, we appended our outputs (not the word but the probabilities) to the
    list [[1:06:48](https://youtu.be/tY0n9OT5_nA?t=1h6m48s)] which we stack up into
    a tensor and we can now go ahead and feed that to a loss function.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 每次，我们将输出（不是单词，而是概率）附加到列表[[1:06:48](https://youtu.be/tY0n9OT5_nA?t=1h6m48s)]中，然后将其堆叠成一个张量，然后我们可以继续将其馈送到损失函数中。
- en: Loss function [[1:07:13](https://youtu.be/tY0n9OT5_nA?t=1h7m13s)]
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数[[1:07:13](https://youtu.be/tY0n9OT5_nA?t=1h7m13s)]
- en: The loss function is categorical cross entropy loss. We have a list of probabilities
    for each of our classes where the classes are all the words in our English vocab
    and we have a target which is the correct class (i.e. which is the correct word
    at this location). There are two tweaks which is why we need to write our own
    loss function but you can see basically it is going to be cross entropy loss.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是分类交叉熵损失。我们有一个概率列表，对应每个类别，其中类别是我们英语词汇中的所有单词，我们有一个目标，即正确的类别（即在此位置的正确单词）。有两个调整，这就是为什么我们需要编写自己的损失函数，但基本上可以看到它将是交叉熵损失。
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Tweaks [[1:07:40](https://youtu.be/tY0n9OT5_nA?t=1h7m40s)]:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 调整[[1:07:40](https://youtu.be/tY0n9OT5_nA?t=1h7m40s)]：
- en: If the generated sequence length is shorter than the sequence length of the
    target, we need to add some padding. PyTorch padding function requires a tuple
    of 6 to pad a rank 3 tensor (sequence length, batch size, by number of words in
    the vocab). Each pair represents padding before and after that dimension.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果生成的序列长度短于目标序列长度，我们需要添加一些填充。PyTorch填充函数需要一个6元组来填充一个秩为3的张量（序列长度、批量大小、词汇表中的单词数）。每对表示在该维度之前和之后的填充。
- en: 2\. `F.cross_entropy` expects a rank 2 tensor, but we have sequence length by
    batch size, so let’s just flatten out. That is what `view(-1, ...)` does.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`F.cross_entropy` 期望一个秩为2的张量，但我们有序列长度乘以批量大小，所以让我们展平它。这就是 `view(-1, ...)` 做的事情。'
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The difference between `.cuda()` and `to_gpu()` : `to_gpu` will not put to
    in the GPU if you do not have one. You can also set `fastai.core.USE_GPU` to `false`
    to force it to not use GPU that can be handy for debugging.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`.cuda()` 和 `to_gpu()` 之间的区别：如果没有 GPU，`to_gpu` 不会将其放入 GPU。您还可以将 `fastai.core.USE_GPU`
    设置为 `false`，以强制它不使用 GPU，这对调试很方便。'
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We then need something that tells it how to handle learning rate groups so there
    is a thing called `SingleModel` that you can pass it to which treats the whole
    thing as a single learning rate group [[1:09:40](https://youtu.be/tY0n9OT5_nA?t=1h9m40s)].
    So this is the easiest way to turn a PyTorch module into a fastai model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要一些东西告诉它如何处理学习率组，所以有一个叫做 `SingleModel` 的东西，你可以传递给它，它将整个东西视为一个单一的学习率组。这是将
    PyTorch 模块转换为 fastai 模型的最简单方法。
- en: We could just call Learner to turn that into a learner, but if we call RNN_Learner,
    it does add in `save_encoder` and `load_encoder` that can be handy sometimes.
    In this case, we really could have said `Leaner` but `RNN_Learner` also works.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接调用 Learner 将其转换为一个学习器，但如果我们调用 RNN_Learner，它会添加 `save_encoder` 和 `load_encoder`，有时会很方便。在这种情况下，我们确实可以说
    `Leaner`，但 `RNN_Learner` 也可以。
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Test [[1:11:01](https://youtu.be/tY0n9OT5_nA?t=1h11m1s)]
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试
- en: Remember the model attribute of a learner is a standard PyTorch model so we
    can pass some `x` which we can grab out of our validation set or you could `learn.predict_array`
    or whatever you like to get some predictions. Then we convert those predictions
    into words by going `.max()[1]` to grab the index of the highest probability words
    to get some predictions. Then we can go through a few examples and print out the
    French, the correct English, and the predicted English for things that are not
    padding.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，学习器的模型属性是一个标准的 PyTorch 模型，所以我们可以传递一些 `x`，我们可以从验证集中获取，或者您可以使用 `learn.predict_array`
    或其他方法来获取一些预测。然后我们通过 `.max()[1]` 将这些预测转换为单词，以获取概率最高的单词的索引。然后我们可以通过一些示例，打印出法语、正确的英语和预测的英语，对于那些不是填充的内容。
- en: '[PRE30]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Amazingly enough, this kind of simplest possible written largely from scratch
    PyTorch module on only fifty thousand sentences is sometimes capable, on validation
    set, of giving you exactly the right answer. Sometimes the right answer is in
    slightly different wording, and sometimes sentences that really aren’t grammatically
    sensible or even have too many question marks. So we are well on the right track.
    We think you would agree even the simplest possible seq-to-seq trained for a very
    small number of epochs without any pre-training other than the use of word embeddings
    is surprisingly good. We are going to improve this later but the message here
    is even sequence to sequence models you think is simpler than they could possibly
    work even with less data than you think you could learn from can be surprisingly
    effective and in certain situations this may be enough for your needs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，这种可能是最简单的从头开始编写的 PyTorch 模块，仅有五万个句子，有时在验证集上能够给出完全正确的答案。有时正确答案略有不同措辞，有时句子真的不通顺，甚至有太多的问号。所以我们在正确的轨道上。我们认为您会同意，即使是可能是最简单的
    seq-to-seq 模型，经过很少的迭代训练，除了使用词嵌入之外没有任何预训练，效果也出奇的好。我们以后会改进这一点，但这里的信息是，即使您认为序列到序列模型比您认为的更简单，即使使用比您认为的更少的数据进行学习，也可能会出奇地有效，在某些情况下，这可能已经足够满足您的需求。
- en: '**Question**: Would it help to normalize punctuation (e.g. `’` vs. `''`)? [[1:13:10](https://youtu.be/tY0n9OT5_nA?t=1h13m10s)]
    The answer to this particular case is probably yes — the difference between curly
    quotes and straight quotes is really semantic. You do have to be very careful
    though because it may turn out that people using beautiful curly quotes like using
    more formal language and they are writing in a different way. So if you are going
    to do some kind of pre-processing like punctuation normalization, you should definitely
    check your results with and without because nearly always that kind of pre-processing
    make things worse even when you’re sure it won’t.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：规范标点符号（例如 `’` vs. `'`）会有帮助吗？这种特定情况的答案可能是肯定的——弯引号和直引号之间的区别实际上是语义上的。但是你必须非常小心，因为可能会发现使用漂亮的弯引号的人更喜欢使用更正式的语言，他们的写作方式也不同。因此，如果你要进行某种类似标点符号规范化的预处理，你应该绝对检查带有和不带有这种预处理的结果，因为几乎总是这种预处理会使事情变得更糟，即使你确信它不会。
- en: '**Question**: What might be some ways of regularizing these seq2seq models
    besides dropout and weight decay? [[1:14:17](https://youtu.be/tY0n9OT5_nA?t=1h14m17s)]
    Let me think about that during the week. AWD-LSTM which we have been relying a
    lot has dropouts of many different kinds and there is also a kind of a regularization
    based on activations and on changes. Jeremy has not seen anybody put anything
    like that amount of work into regularizing sequence to sequence model and there
    is a huge opportunity for somebody to do like the AWD-LSTM of seq-to-seq which
    might be as simple as stealing all the ideas from AWD-LSTM and using them directly
    in seq-to-seq that would be pretty easy to try. There’s been an interesting paper
    that Stephen Merity added in the last couple weeks where he used an idea which
    take all of these different AWD-LSTM hyper parameters and train a bunch of different
    models and then use a random forest to find out the feature importance — which
    ones actually matter the most and then figure out how to set them. You could totally
    use this approach to figure out for sequence to sequence regularization approaches
    which one is the best and optimize them and that would be amazing. But at the
    moment, we don’t know if there are additional ideas to sequence to sequence regularization
    beyond what is in that paper for regular language model.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：除了dropout和权重衰减，有哪些正则化这些seq2seq模型的方法？让我在这一周内考虑一下。我们一直依赖的AWD-LSTM有许多不同种类的dropout，还有一种基于激活和变化的正则化。Jeremy还没有看到有人将这么多工作投入到正则化序列到序列模型中，有一个巨大的机会让某人像AWD-LSTM一样对seq-to-seq进行正则化，这可能就像从AWD-LSTM中窃取所有想法并直接在seq-to-seq中使用它们一样容易尝试。最近几周Stephen
    Merity添加了一篇有趣的论文，他使用了一个想法，即获取所有这些不同的AWD-LSTM超参数并训练一堆不同的模型，然后使用随机森林找出最重要的特征，然后找出如何设置它们。你完全可以使用这种方法来找出对序列到序列正则化方法哪种是最好的，并优化它们，这将是令人惊讶的。但目前，我们不知道除了那篇关于常规语言模型的论文之外，是否还有其他关于序列到序列正则化的想法。'
- en: Tricks [[1:16:28](https://youtu.be/tY0n9OT5_nA?t=1h16m28s)]
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技巧
- en: '**Trick #1 : Go bi-directional**'
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**技巧#1：使用双向**'
- en: For classification, the approach to bi-directional Jeremy suggested to use is
    take all of your token sequences, spin them around, train a new language model,
    and train a new classifier. He also mentioned that wikitext pre-trained model
    if you replace `fwd` with `bwd` in the name, you will get the pre-trained backward
    model he created for you. Get a set of predictions and then average the predictions
    just like a normal ensemble. That is how we do bi-dir for that kind of classification.
    There may be ways to do it end-to-end, but Jeremy hasn’t quite figured them out
    yet and they are not in fastai yet. So if you figure it out, that’s an interesting
    line of research. But because we are not doing massive documents where we have
    to chunk it into separate bits and then pool over them, we can do bi-dir very
    easily in this case. It is literally as simple as adding `bidirectional=True`
    to our encoder. People tend not to do bi-directional for the decoder partly because
    it is kind of considered cheating but maybe it can work in some situations although
    it might need to be more of an ensemble approach in the decoder because it’s a
    bit less obvious. But encoder it’s very simple — `bidirectional=True` and we now
    have a second RNN that is going the opposite direction. The second RNN is visiting
    each token in the opposing order so when we get to the final hidden state, it
    is the first (i.e. left most) token . But the hidden state is the same size, so
    the final result is that we end up with a tensor with an extra axis of length
    2\. Depending on what library you use, often that will be then combined with the
    number of layers, so if you have 2 layers and bi-directional — that tensor dimension
    is now length 4\. With PyTorch it depends which bit of the process you are looking
    at as to whether you get a separate result for each layer and/or for each bidirectional
    bit. You have to look up the documentation and it will tell you input’s output’s
    tensor sizes appropriate for the number of layers and whether you have `bidirectional=True`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类，Jeremy建议使用的双向方法是获取所有的标记序列，旋转它们，训练一个新的语言模型，然后训练一个新的分类器。他还提到，如果你在名称中用`bwd`替换`fwd`，你将得到他为你创建的预训练的后向模型。获取一组预测，然后像普通集成一样对预测进行平均。这就是我们在这种分类中进行双向的方式。可能有一些方法可以端到端地完成，但Jeremy还没有完全弄清楚，而且它们还没有在fastai中。所以如果你弄清楚了，那是一个有趣的研究方向。但因为我们不是在处理大量文档，需要将其分成单独的部分，然后对它们进行汇总，所以在这种情况下我们可以很容易地进行双向。只需将`bidirectional=True`添加到我们的编码器中就可以了。人们倾向于不对解码器进行双向处理，部分原因是因为这被认为是作弊，但也许在某些情况下它可能有效，尽管在解码器中可能需要更多的集成方法，因为这不太明显。但对于编码器来说很简单——`bidirectional=True`，现在我们有了一个沿着相反方向的第二个RNN。第二个RNN按相反顺序访问每个标记，因此当我们到达最终隐藏状态时，它是第一个（即最左边）标记。但隐藏状态的大小是相同的，因此最终的结果是我们得到了一个长度为2的额外轴的张量。根据你使用的库，通常这将与层数相结合，所以如果你有2层和双向——那个张量维度现在是长度4。对于PyTorch，取决于你查看的过程的哪一部分，你是否会得到每一层和/或每个双向位的单独结果。你必须查阅文档，它会告诉你适用于层数的输入输出张量大小以及是否有`bidirectional=True`。
- en: In this particular case, you will see all the changes that had to be made [[1:19:38](https://youtu.be/tY0n9OT5_nA?t=1h19m38s)].
    For example ,when we added `bidirectional=True`, the `Linear` layer now needs
    number of hidden times 2 (i.e. `nh*2`) to reflect the fact that we have that second
    direction in our hidden state. Also in `initHidden` it’s now `self.nl*2`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特殊情况下，你将看到必须进行的所有更改。例如，当我们添加了`bidirectional=True`时，`Linear`层现在需要隐藏数量乘以2（即`nh*2`）来反映我们隐藏状态中有第二个方向的事实。在`initHidden`中现在是`self.nl*2`。
- en: '[PRE31]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Question**: Why is making the decoder bi-directional considered cheating?
    [[1:20:13](https://youtu.be/tY0n9OT5_nA?t=1h20m13s)] It’s not just cheating but
    we have this loop going on so it is not as simple as having two tensors. Then
    how do you turn those two separate loops into a final result? After talking about
    it during the break, Jeremy has gone from “everybody knows it doesn’t work” to
    “maybe it could work”, but it requires more thought. It is quite possible during
    the week, he’ll realize it’s a dumb idea, but we’ll think about it.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么将解码器设置为双向被认为是作弊？这不仅仅是作弊，而且我们有这种循环进行，所以不仅仅是有两个张量那么简单。那么如何将这两个单独的循环转换为最终结果呢？在休息期间讨论过后，Jeremy已经从“每个人都知道这不起作用”变成“也许它可能起作用”，但需要更多的思考。在这一周期间，他可能会意识到这是一个愚蠢的想法，但我们会考虑一下。
- en: '**Question**: Why do you need to set a range to the loop? [[1:20:58](https://youtu.be/tY0n9OT5_nA?t=1h20m58s)]
    Because when we start training, everything is random so `if (dec_inp==1).all():
    break` will probably never be true. Later on, it will pretty much always break
    out eventually but basically we are going to go forever. It’s really important
    to remember when you are designing an architecture that when you start, the model
    knows nothing about anything. So you want to make sure if it’s going to do something
    at least it’s vaguely sensible.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '为什么需要为循环设置一个范围？因为当我们开始训练时，一切都是随机的，所以`if (dec_inp==1).all(): break`可能永远不会成立。后来，它最终几乎总是会中断，但基本上我们会永远进行下去。在设计架构时，非常重要的一点是要记住，当你开始时，模型对任何事情一无所知。所以你要确保如果它要做一些事情，至少它是模糊合理的。'
- en: We got 3.58 cross entropy loss with single direction [[1:21:46](https://youtu.be/tY0n9OT5_nA?t=1h21m46s)].
    With bi-direction, we got down to 3.51, so that improved a little. It shouldn’t
    really slow things down too much. Bi-directional does mean there is a little bit
    more sequential processing have to happen, but it is generally a good win. In
    the Google translation model, of the 8 layers, only the first layer is bi-directional
    because it allows it to do more in parallel so if you create really deep models
    you may need to think about which ones are bi-directional otherwise we have performance
    issues.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用单向获得了3.58的交叉熵损失。使用双向后，我们降到了3.51，所以稍微有所改善。这不会真正减慢速度太多。双向意味着需要进行更多的顺序处理，但通常是一个很好的胜利。在Google翻译模型中，8层中只有第一层是双向的，因为它允许它更多地并行进行，所以如果你创建了非常深的模型，你可能需要考虑哪些是双向的，否则我们会有性能问题。
- en: '[PRE32]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Trick #2 Teacher Forcing [[1:22:39](https://youtu.be/tY0n9OT5_nA?t=1h22m39s)]'
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技巧#2 教师强制
- en: Now let’s talk about teacher forcing. When a model starts learning, it knows
    nothing about nothing. So when the model starts learning, it is not going to spit
    out “Er” at the first step, it is going to spit out some random meaningless word
    because it doesn’t know anything about German or about English or about the idea
    of language. And it is going to feed it to the next process as an input and be
    totally unhelpful. That means, early learning is going to be very difficult because
    it is feeding in an input that is stupid into a model that knows nothing and somehow
    it’s going to get better. So it is not asking too much eventually it gets there,
    but it’s definitely not as helpful as we can be. So what if instead of feeing
    in the thing I predicted just now, what if we instead we feed in the actual correct
    word was meant to be. We can’t do that at inference time because by definition
    we don’t know the correct word - it has to translate it. We can’t require the
    correct translation in order to do translation.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们谈谈教师强制。当模型开始学习时，它对一无所知。所以当模型开始学习时，它不会在第一步就吐出“Er”，它会吐出一些随机无意义的单词，因为它对德语、英语或语言的概念一无所知。它会将其作为输入馈送到下一个过程中，并且完全没有帮助。这意味着早期学习会非常困难，因为它将一个愚蠢的输入馈送到一个一无所知的模型中，但不知何故它会变得更好。所以最终它会到达那里，但肯定不像我们可以做的那样有帮助。那么，如果我们不是输入我刚才预测的东西，而是输入实际正确的单词，会怎样呢？我们在推理时无法这样做，因为根据定义，我们不知道正确的单词
    - 必须将其翻译。我们不能要求正确的翻译来进行翻译。
- en: So the way it’s set up is we have this thing called `pr_force` which is probability
    of forcing [[1:24:01](https://youtu.be/tY0n9OT5_nA?t=1h24m1s)]. If some random
    number is less than that probability then we are going to replace our decoder
    input with the actual correct thing. If we have already gone too far and if it
    is already longer than the target sequence, we are just going to stop because
    obviously we can’t give it the correct thing. So you can see how beautiful PyTorch
    is for this. The key reasons that we switched to PyTorch at this exact point in
    last year’s class was because Jeremy tried to implement teacher forcing in Keras
    and TensorFlow and went even more insane than he started. It was weeks of getting
    nowhere then he saw on Twitter Andrej Karpathy said something about this thing
    called PyTorch that just came out and it’s really cool. He tried it that day,
    by the next day, he had teacher forcing. All this stuff of trying to debug things
    was suddenly so much easier and and this kind of dynamic thing is so much easier.
    So this is a great example of “hey, I get to use random numbers and if statements”.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 设置的方式是我们有一个叫做`pr_force`的东西，它是强制的概率。如果某个随机数小于该概率，那么我们将用实际正确的东西替换我们的解码器输入。如果我们已经走得太远，如果它已经比目标序列长，我们就会停止，因为显然我们无法给出正确的东西。你可以看到PyTorch在这方面是多么美妙。去年课程的这个确切时刻，我们切换到PyTorch的关键原因是因为Jeremy尝试在Keras和TensorFlow中实现教师强制，结果比之前更疯狂。几周都没有进展，然后他在Twitter上看到Andrej
    Karpathy提到了一个叫做PyTorch的东西，很酷。他当天尝试了一下，第二天就有了教师强制。所有这些尝试调试的事情突然变得容易得多，这种动态的东西也变得容易得多。所以这是一个很好的例子，“嘿，我可以使用随机数和if语句”。
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here is the basic idea [[1:25:29](https://youtu.be/tY0n9OT5_nA?t=1h25m29s)].
    At the start of training, let’s set `pr_force` really high so that nearly always
    it gets the actual correct previous word and so it has a useful input. Then as
    we trained a bit more, let’s decrease `pr_force` so that by the end `pr_force`
    is zero and it has to learn properly which is fine because it is now actually
    feeding in sensible inputs most of the time anyway.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是基本思想[[1:25:29](https://youtu.be/tY0n9OT5_nA?t=1h25m29s)]。在训练开始时，让`pr_force`非常高，以便几乎总是得到实际正确的前一个单词，因此它有一个有用的输入。然后随着我们训练的进一步，让`pr_force`逐渐减少，直到最后`pr_force`为零，它必须正确学习，这是可以的，因为现在它几乎总是输入合理的输入。
- en: '[PRE34]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`pr_force`: “probability of forcing”. High in the beginning zero by the end.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`pr_force`: “probability of forcing”. High in the beginning zero by the end.'
- en: Let’s now write something such that in the training loop, it gradually decreases
    `pr_force` [[1:26:01](https://youtu.be/tY0n9OT5_nA?t=1h26m1s)]. How do we do that?
    One approach would be to write our own training loop but let’s not do that because
    we already have a training loop that has progress bars, uses exponential weighted
    averages to smooth out the losses, keeps track of metrics, and does bunch of things.
    They also keep track of calling the reset for RNN at the start of the epoch to
    make sure the hidden state is set to zeros. What we’ve tended to find is that
    as we start to write some new thing and we need to replace some part of the code,
    we then add some little hook so that we can all use that hook to make things easier.
    In this particular case, there is a hook that Jeremy has ended up using all the
    time which is the hook called the stepper. If you look at the source code, model.py
    is where our fit function lives which is the lowest level thing that does not
    require learner or anything much at all — just requires a standard PyTorch model
    and a model data object. You just need to know how many epochs, a standard PyTorch
    optimizer, and a standard PyTorch loss function. We hardly ever used in the class,
    we normally call `learn.fit`, but `learn.fit` calls this.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们写一些东西，使得在训练循环中逐渐减少`pr_force`[[1:26:01](https://youtu.be/tY0n9OT5_nA?t=1h26m1s)]。我们如何做到这一点？一种方法是编写我们自己的训练循环，但我们不要这样做，因为我们已经有一个训练循环，它有进度条，使用指数加权平均值来平滑损失，跟踪指标，并做了一堆事情。它们还跟踪在epoch开始时调用RNN的重置，以确保隐藏状态设置为零。我们发现的趋势是，当我们开始编写一些新东西并需要替换代码的某些部分时，我们会添加一些小钩子，以便我们可以使用该钩子使事情变得更容易。在这种特殊情况下，Jeremy一直在使用的一个钩子是称为stepper的钩子。如果你查看源代码，model.py是我们的fit函数所在的地方，这是最低级的东西，不需要学习者或任何其他东西，只需要一个标准的PyTorch模型和一个模型数据对象。你只需要知道多少个epochs，一个标准的PyTorch优化器和一个标准的PyTorch损失函数。我们在课堂上几乎从未使用过，我们通常调用`learn.fit`，但`learn.fit`调用这个。
- en: 'We have looked at the source code sometime [[1:27:49](https://youtu.be/tY0n9OT5_nA?t=1h27m49s)].
    We’ve seen how it loos through each epoch and that loops through each thing in
    our batch and calls `stepper.step`. `stepper.step` is the thing that is responsible
    for:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有时查看源代码[[1:27:49](https://youtu.be/tY0n9OT5_nA?t=1h27m49s)]。我们看到它如何通过每个epoch循环，然后循环遍历批处理中的每个内容并调用`stepper.step`。`stepper.step`是负责的事情：
- en: calling the model
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用模型
- en: getting the loss
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取损失
- en: finding the loss function
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到损失函数
- en: calling the optimizer
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用优化器
- en: 'So by default, `stepper.step` uses a particular class called `Stepper` which
    basically calls the model, zeros the gradient, calls the loss function, calls
    `backward`, does gradient clipping if necessary, then calls the optimizer. They
    are basic steps that back when we looked at “PyTorch from scratch” we had to do.
    The nice thing is, we can replace that with something else rather than replacing
    the training loop. If you inherit from `Stepper`, then write your own version
    of `step` , you can just copy and paste the contents of step and add whatever
    you like. Or if it’s something that you’re going to do before or afterwards, you
    could even call `super.step`. In this case, Jeremy rather suspects he has been
    unnecessarily complicated [[1:29:12](https://youtu.be/tY0n9OT5_nA?t=1h29m12s)]
    — he probably could have done something like:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 所以默认情况下，`stepper.step`使用一个称为`Stepper`的特定类，基本上调用模型，将梯度置零，调用损失函数，调用`backward`，如果需要进行梯度裁剪，然后调用优化器。这些是我们在“从头开始的PyTorch”中看到的基本步骤。好处是，我们可以用其他东西替换它，而不是替换训练循环。如果你继承自`Stepper`，然后编写你自己版本的`step`，你可以只需复制并粘贴step的内容并添加任何你喜欢的内容。或者如果这是你要在之前或之后做的事情，你甚至可以调用`super.step`。在这种情况下，Jeremy相当怀疑他不必要地复杂[[1:29:12](https://youtu.be/tY0n9OT5_nA?t=1h29m12s)]
    - 他可能本来可以做一些像这样的事情：
- en: '[PRE35]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'But as he said, when he is prototyping, he doesn’t think carefully about how
    to minimize his code — he copied and pasted the contents of the `step` and he
    added a single line to the top which was to replace `pr_force` in the module with
    something that gradually decreased linearly for the first 10 epochs, and after
    10 epochs, it is zero. So total hack but good enough to try it out. The nice thing
    is that everything else is the same except for the addition of these three lines:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 但正如他所说的，当他在原型设计时，他并没有仔细考虑如何最小化他的代码 - 他复制并粘贴了`step`的内容，并在顶部添加了一行，用于将模块中的`pr_force`逐渐线性减少前10个epochs，10个epochs后，它为零。所以总体上是一个hack，但足够好用来尝试一下。好处是除了添加这三行之外，其他一切都是一样的：
- en: '[PRE36]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: And the only thing we need to do differently is when we call `fit` , we pass
    in our customized stepper class.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一需要做的不同之处是当我们调用`fit`时，我们传入我们定制的stepper类。
- en: '[PRE37]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: And now our loss is down to 3.49\. We needed to make sure at least do 10 epochs
    because before that, it was cheating by using the teacher forcing.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的损失降至3.49。我们需要确保至少进行10个epochs，因为在那之前，通过使用强制教师，这是作弊的。
- en: 'Trick #3 Attentional model [[1:31:00](https://youtu.be/tY0n9OT5_nA?t=1h31m)]'
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Trick #3 注意力模型[[1:31:00](https://youtu.be/tY0n9OT5_nA?t=1h31m)]'
- en: This next trick is a bigger and pretty cool trick. It’s called “attention.”
    The basic idea of attention is this — expecting the entirety of the sentence to
    be summarized into this single hidden vector is asking a lot. It has to know what
    was said, how it was said, and everything necessary to create the sentence in
    German. The idea of attention is basically maybe we are asking too much. Particularly
    because we could use this form of model (below) where we output every step of
    the loop to not just have a hidden state at the end but to have a hidden state
    after every single word. Why not try and use that information? It’s already there
    but so far we’ve just been throwing it away. Not only that but bi-directional,
    we got two vectors of state every step that we can use. How can we do this?
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个技巧是一个更大、更酷的技巧。它被称为“注意力”。注意力的基本思想是这样的——期望将整个句子总结为这个单一的隐藏向量是要求太多了。它必须知道说了什么，怎么说的，以及创建德语句子所需的一切。注意力的想法基本上是我们可能要求太多了。特别是因为我们可以使用这种形式的模型（下面），在这种模型中我们不仅输出循环的每一步，而不仅仅是在最后有一个隐藏状态，而是在每个单词之后都有一个隐藏状态。为什么不尝试利用这些信息呢？它已经存在，但到目前为止我们只是把它丢掉了。不仅如此，而且双向的，我们在每一步都有两个状态向量，我们可以利用。我们怎么做呢？
- en: Let’s say we are translating a word “liebte” right now [[1:32:34](https://youtu.be/tY0n9OT5_nA?t=1h32m34s)].
    Which of previous 5 pieces of state do we want? We clearly want “love” because
    it is the word. How about “zu”? We probably need “eat” and “to” and loved” to
    make sure we have gotten the tense right and know that I actually need this part
    of the verb and so forth. So depending on which bit we are translating, we would
    need one or more bits of these various hidden states. In fact, we probably want
    some weighting of them. In other words, for these five pieces of hidden state,
    we want a weighted average [[1:33:47](https://youtu.be/tY0n9OT5_nA?t=1h33m47s)].
    We want it weighted by something that can figure out which bits of the sentence
    is the most important right now. How do we figure out something like which bits
    of the sentence are important right now? We create a neural net and we train the
    neural net to figure it out. When do we train that neural net? End to end. So
    let’s now train two neural nets [[1:34:18](https://youtu.be/tY0n9OT5_nA?t=1h34m18s)].
    Well, we’ve already got a bunch — RNN encoder, RNN decoder, a couple of linear
    layers, what the heck, let’s add another neural net into the mix. This neural
    net is going to spit out a weight for every one of these states and we will take
    the weighted average at every step, and it’s just another set of parameters that
    we learn all at the same time. So that is called “attention”.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们现在正在翻译一个词“liebte”[[1:32:34](https://youtu.be/tY0n9OT5_nA?t=1h32m34s)]。我们想要之前的5个隐藏状态中的哪一个？显然我们想要“love”，因为这是这个词。那么“zu”呢？我们可能需要“eat”、“to”和“loved”来确保我们已经得到了正确的时态，并知道我实际上需要动词的这部分等等。因此，根据我们正在翻译的部分，我们可能需要这些不同隐藏状态的一个或多个部分。实际上，我们可能需要对它们进行加权。换句话说，对于这五个隐藏状态，我们想要一个加权平均值[[1:33:47](https://youtu.be/tY0n9OT5_nA?t=1h33m47s)]。我们希望它根据某种可以确定哪些句子部分现在最重要的东西进行加权。我们如何找出哪些句子部分现在很重要？我们创建一个神经网络，训练神经网络来找出。我们什么时候训练这个神经网络？端到端。所以现在让我们训练两个神经网络[[1:34:18](https://youtu.be/tY0n9OT5_nA?t=1h34m18s)]。嗯，我们已经有了一堆——RNN编码器，RNN解码器，几个线性层，那就再加一个神经网络吧。这个神经网络将为每一个这些状态输出一个权重，我们将在每一步进行加权平均，这只是我们同时学习的另一组参数。这就是所谓的“注意力”。
- en: The idea is that once that attention has been learned, each word is going to
    take a weighted average as you can see in this terrific demo from Chris Olah and
    Shan Carter [[1:34:50](https://youtu.be/tY0n9OT5_nA?t=1h34m50s)]. Check out this
    [distill.pub article](https://distill.pub/2016/augmented-rnns/) — these things
    are interactive diagrams that shows you how the attention works and what the actual
    attention looks like in a trained translation model.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是一旦学会了注意力，每个单词都会进行加权平均，你可以在Chris Olah和Shan Carter的这个精彩演示中看到[[1:34:50](https://youtu.be/tY0n9OT5_nA?t=1h34m50s)]。查看这篇[distill.pub文章](https://distill.pub/2016/augmented-rnns/)——这些都是交互式图表，向你展示了注意力是如何工作的，以及在训练翻译模型中实际的注意力是什么样子。
- en: '[](https://distill.pub/2016/augmented-rnns/)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://distill.pub/2016/augmented-rnns/)'
- en: 'Let’s try and implement attention [[1:35:47](https://youtu.be/tY0n9OT5_nA?t=1h35m47s)]:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们尝试实现注意力[[1:35:47](https://youtu.be/tY0n9OT5_nA?t=1h35m47s)]:'
- en: '[PRE38]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'With attention, most of the code is identical. The one major difference is
    this line: `Xa = (a.unsqueeze(2) * enc_out).sum(0)` . We are going to take a weighted
    average and the way we are going to do the weighted average is we create a little
    neural net which we are going to see here:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 有了注意力，大部分代码都是相同的。唯一的主要区别是这一行：`Xa = (a.unsqueeze(2) * enc_out).sum(0)`。我们将进行加权平均，我们将如何进行加权平均是我们创建一个小型神经网络，我们将在这里看到：
- en: '[PRE39]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We use softmax because the nice thing about softmax is that we want to ensure
    all of the weights that we are using add up to 1 and we also expect that one of
    those weights should probably be higher than the other ones [[1:36:38](https://youtu.be/tY0n9OT5_nA?t=1h36m38s)].
    Softmax gives us the guarantee that they add up to 1 and because it has `e^` in
    it, it tends to encourage one of the weights to be higher than the other ones.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用softmax，因为softmax的好处是我们希望确保我们使用的所有权重加起来等于1，而且我们也希望其中一个权重可能比其他权重更高[[1:36:38](https://youtu.be/tY0n9OT5_nA?t=1h36m38s)]。Softmax给了我们这样的保证，它们加起来等于1，因为它里面有`e^`，它倾向于鼓励其中一个权重比其他权重更高。
- en: Let’s see how this works [[1:37:09](https://youtu.be/tY0n9OT5_nA?t=1h37m9s)].
    We are going to take the last layer’s hidden state and we are going to stick it
    into a linear layer. Then we are going to stick it into a nonlinear activation,
    then we are going to do a matrix multiply. So if you think about it — a linear
    layer, nonlinear activation, matrix multiple — it’s a neural net. It is a neural
    net with one hidden layer. Stick it into a softmax and then we can use that to
    weight our encoder outputs. Now rather than just taking the last encoder output,
    we have the whole tensor of all of the encoder outputs which we just weight by
    this neural net we created.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是如何工作的[[1:37:09](https://youtu.be/tY0n9OT5_nA?t=1h37m9s)]。我们将取最后一层的隐藏状态，然后将其放入一个线性层中。然后我们将其放入一个非线性激活函数，然后进行矩阵相乘。所以如果你考虑一下——一个线性层，非线性激活函数，矩阵相乘——这就是一个神经网络。这是一个具有一个隐藏层的神经网络。将其放入softmax，然后我们可以使用它来加权我们的编码器输出。现在，我们不再只是取最后一个编码器输出，而是有了所有编码器输出的张量，我们只需用我们创建的这个神经网络来加权。
- en: In Python, `A @ B` is the matrix product, `A * B` the element-wise product
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，`A @ B`是矩阵乘积，`A * B`是逐元素乘积
- en: Papers [[1:38:18](https://youtu.be/tY0n9OT5_nA?t=1h38m18s)]
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 论文[[1:38:18](https://youtu.be/tY0n9OT5_nA?t=1h38m18s)]
- en: '[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
    — One amazing paper that originally introduced this idea of attention as well
    as a couple of key things which have really changed how people work in this field.
    They say area of attention has been used not just for text but for things like
    reading text out of pictures or doing various things with computer vi sion.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过联合学习对齐和翻译进行神经机器翻译](https://arxiv.org/abs/1409.0473)——这是一篇令人惊叹的论文，最初介绍了注意力的概念，以及一些真正改变了人们在这一领域工作方式的关键事项。他们说，注意力领域不仅用于文本，还用于从图片中读取文本或在计算机视觉中执行各种任务。'
- en: '[Grammar as a Foreign Language](https://arxiv.org/abs/1412.7449) — The second
    paper which Geoffrey Hinton was involved in that used this idea of RNN with attention
    to try to replace rules based grammar with an RNN which automatically tagged each
    word based on the grammar. It turned out to do it better than any rules based
    system which today seems obvious but at that time it was considered really surprising.
    They are summary of how attention works which is really nice and concise.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[作为外语的语法](https://arxiv.org/abs/1412.7449)——Geoffrey Hinton参与的第二篇论文，使用了RNN与注意力的想法，试图用自动标记每个单词的RNN替换基于规则的语法。结果表明，它比任何基于规则的系统做得更好，这在今天看来是显而易见的，但在当时被认为是非常令人惊讶的。它们是关于注意力如何工作的摘要，非常清晰简洁。'
- en: '**Question**: Could you please explain attention again? [[1:39:46](https://youtu.be/tY0n9OT5_nA?t=1h39m46s)]
    Sure! Let’s go back and look at our original encoder.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：你能再解释一下注意力吗？[[1:39:46](https://youtu.be/tY0n9OT5_nA?t=1h39m46s)] 当然！让我们回头看看我们最初的编码器。'
- en: 'The RNN spits out two things: it spits out a list of the state after every
    time step (`enc_out`), and it also tells you the state at the last time step (`h`)and
    we used the state at the last time step to create the input state for our decoder
    which is one vector `s` below:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: RNN输出两个东西：它在每个时间步骤之后输出一个状态列表（`enc_out`），并且还告诉您在最后一个时间步骤的状态（`h`），我们使用最后一个时间步骤的状态来创建我们的解码器的输入状态，这是下面的一个向量`s`：
- en: 'But we know that it’s creating a vector at every time steps (orange arrows),
    so wouldn’t it be nice to use them all? But wouldn’t it be nice to use the one
    or ones that’s most relevant to translating the word we are translating now? So
    wouldn’t it be nice to be able to take a weighted average of the hidden state
    at each time step weighted by whatever is the appropriate weight right now. For
    example, “liebte” would definitely be time step #2 is what it’s all about because
    that is the word I’m translating. So how do we get a list of weights that is suitable
    fore the word we are training right now? The answer is by training a neural net
    to figure out the list of weights. So anytime we want to figure out how to train
    a little neural net that does any task, the easiest way, normally always to do
    that is to include it in your module and train it in line with everything else.
    The minimal possible neural net is something that contains two layers and one
    nonlinear activation function, so `self.l2` is one linear layer.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们知道它在每个时间步骤都创建一个向量（橙色箭头），那么使用它们所有不是更好吗？但使用哪一个或哪些对于翻译我们正在翻译的单词最相关呢？所以能否按照当前适当的权重取每个时间步骤的隐藏状态的加权平均值会更好。例如，“liebte”肯定是时间步骤＃2，因为那是我正在翻译的单词。那么我们如何得到一个适合当前训练的单词的权重列表呢？答案是通过训练一个神经网络来找出权重列表。所以每当我们想要弄清楚如何训练一个小型神经网络来执行任何任务时，通常最简单的方法就是将其包含在你的模块中，并与其他所有内容一起训练。最简单的神经网络是包含两层和一个非线性激活函数的东西，所以`self.l2`是一个线性层。
- en: In fact, instead of a linear layer, we can even just grab a random matrix if
    we do not care about bias [[1:42:18](https://youtu.be/tY0n9OT5_nA?t=1h42m18s)].
    `self.W1` is a random tensor wrapped up in a `Parameter`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们甚至可以只是随机选择一个矩阵，如果我们不关心偏差[[1:42:18](https://youtu.be/tY0n9OT5_nA?t=1h42m18s)]。`self.W1`是一个随机张量，包装在一个`Parameter`中。
- en: '`Parameter` : Remember, a `Parameter` is identical to PyTorch `Variable` but
    it just tells PyTorch “I want you to learn the weights for this please.” [[1:42:35](https://youtu.be/tY0n9OT5_nA?t=1h42m35s)]'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`Parameter`：记住，`Parameter`与PyTorch的`Variable`是相同的，但它只是告诉PyTorch“请学习这些权重”。[[1:42:35](https://youtu.be/tY0n9OT5_nA?t=1h42m35s)]'
- en: 'So when we start out our decoder, let’s take the current hidden state of the
    decoder, put that into a linear layer (`self.l2`) because what is the information
    we use to decide what words we should focus on next — the only information we
    have to go on is what the decoder’s hidden state is now. So let’s grab that:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们开始我们的解码器时，让我们取解码器当前的隐藏状态，将其放入一个线性层（`self.l2`），因为我们用来决定接下来应该关注哪些单词的信息——我们唯一可以依赖的信息就是解码器当前的隐藏状态。所以让我们抓住它：
- en: put it into the linear layer (`self.l2`)
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其放入线性层（`self.l2`）
- en: put it through a non-linearity (`F.tanh`)
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其通过非线性激活函数（`F.tanh`）处理
- en: put it through one more nonlinear layer (`u @ self.V` doesn’t have a bias in
    it so it’s just matrix multiply)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一个非线性层（`u @ self.V`中没有偏差，所以只是矩阵相乘）
- en: put that through softmax
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过softmax
- en: That’s it — a little neural net. It doesn’t do anything. It’s just a neural
    net and no neural nets do anything they are just linear layers with nonlinear
    activations with random weights. But it starts to do something if we give it a
    job to do. In this case, the job we give it to do is to say don’t just take the
    final state but now let’s use all of the encoder states and let’s take all of
    them and multiply them by the output of that little neural net. So given that
    the things in this little neural net are learnable weights, hopefully it’s going
    to learn to weight those encoder hidden states by something useful. That is all
    neural net ever does is we give it some random weights to start with and a job
    to do, and hope that it learns to do the job. It turns out, it does.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样——一个小型神经网络。它什么也不做。它只是一个神经网络，没有神经网络做任何事情，它们只是具有随机权重的线性层和非线性激活。但是如果我们给它一个任务，它就开始做一些事情。在这种情况下，我们给它的任务是不要只取最终状态，而是现在让我们使用所有编码器状态，并且让我们取出所有这些状态，并将它们乘以那个小型神经网络的输出。因此，考虑到这个小型神经网络中的东西是可学习的权重，希望它学会对这些编码器隐藏状态进行有用的加权。神经网络所做的一切就是我们给它一些随机权重作为起点和一个任务，并希望它学会完成这个任务。结果表明，它确实做到了。
- en: Everything else in here is identical to what it was before. We have teacher
    forcing, it’s not bi-directional, so we can see how this goes.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的其他一切与以前完全相同。我们有教师强制，它不是双向的，所以我们可以看看情况如何。
- en: '[PRE40]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Teacher forcing had 3.49 and now with nearly exactly the same thing but we’ve
    got this little minimal neural net figuring out what weightings to give our inputs
    and we are down to 3.37\. Remember, these loss are logs, so `e^3.37` is quite
    a significant change.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 教师强制为3.49，现在几乎完全相同的东西，但我们有这个小型神经网络来找出给我们输入的权重，我们降到了3.37。记住，这些损失是对数，所以`e^3.37`是一个相当显著的变化。
- en: '[PRE41]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Test [[1:45:37](https://youtu.be/tY0n9OT5_nA?t=1h45m37s)]
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试[[1:45:37](https://youtu.be/tY0n9OT5_nA?t=1h45m37s)]
- en: '[PRE42]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Not bad. It’s still not perfect but quite a few of them are correct and again
    considering that we are asking it to learn about the very idea of language for
    two different languages and how to translate them between the two, and grammar,
    and vocabulary, and we only have 50,000 sentences and a lot of the words only
    appear once, I would say this is actually pretty amazing.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 还不错。仍然不完美，但相当多的结果是正确的，考虑到我们要求它学习两种不同语言之间的语言概念，以及如何在两种语言之间进行翻译，以及语法和词汇，我们只有50,000个句子，很多词只出现一次，我会说这实际上是非常惊人的。
- en: '**Question:** Why do we use tanh instead of ReLU for the attention mini net?
    [[1:46:23](https://youtu.be/tY0n9OT5_nA?t=1h46m23s)] I don’t quite remember —
    it’s been a while since I looked at it. You should totally try using value and
    see how it goes. Obviously tanh the key difference is that it can go in each direction
    and it’s limited both at the top and the bottom. I know very often for the gates
    inside RNNs, LSTMs, and GRUs, tanh often works out better but it’s been about
    a year since I actually looked at that specific question so I’ll look at it during
    the week. The short answer is you should try a different activation function and
    see if you can get a better result.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：**为什么我们在注意力小网络中使用tanh而不是ReLU？[[1:46:23](https://youtu.be/tY0n9OT5_nA?t=1h46m23s)]我不太记得——我很久没有看过了。你完全可以尝试使用值并看看效果如何。显然，tanh的关键区别在于它可以在每个方向上移动，并且在顶部和底部都受限。我知道在RNNs、LSTMs和GRUs内部的门中，tanh通常效果更好，但是我已经大约一年没有看过这个具体问题了，所以我会在这周看一下。简短的答案是你应该尝试不同的激活函数，看看是否可以得到更好的结果。'
- en: 'From Lesson 7 [[44:06](https://youtu.be/H3g26EVADgY?t=44m6s)]: As we have seen
    last week, tanh is forcing the value to be between -1 and 1\. Since we are multiplying
    by this weight matrix again and again, we would worry that relu (since it is unbounded)
    might have more gradient explosion problem. Having said that, you can specify
    RNNCell to use different nonlineality whose default is tanh and ask it to use
    relu if you wanted to.'
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来自第7课[[44:06](https://youtu.be/H3g26EVADgY?t=44m6s)]：正如我们上周所看到的，tanh强制值在-1和1之间。由于我们一遍又一遍地乘以这个权重矩阵，我们担心relu（因为它是无界的）可能会有更多的梯度爆炸问题。话虽如此，你可以指定RNNCell使用不同的非线性函数，其默认值为tanh，并要求它使用relu。
- en: Visualization [[1:47:12](https://youtu.be/tY0n9OT5_nA?t=1h47m12s)]
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化[[1:47:12](https://youtu.be/tY0n9OT5_nA?t=1h47m12s)]
- en: 'What we can do also is we can grab the attentions out of the model by adding
    return attention parameter to `forward` function. You can put anything you’d like
    in `forward` function argument. So we added a return attention parameter, false
    by default because obviously the training loop it doesn’t know anything about
    it but then we just had something here says if return attention, then stick the
    attentions on as well (`if ret_attn: res = res,torch.stack(attns)`). The attentions
    is simply the value `a` just chuck it on a list (`attns.append(a)`). We can now
    call the model with return attention equals true and get back the probabilities
    and the attentions [[1:47:53](https://youtu.be/tY0n9OT5_nA?t=1h47m53s)]:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还可以通过将`forward`函数添加返回注意力参数来从模型中提取注意力。你可以在`forward`函数参数中放任何你想要的东西。所以我们添加了一个返回注意力参数，默认为false，因为显然训练循环不知道这一点，但然后我们在这里添加了一些东西，如果返回注意力，那么也将注意力添加进去（`if
    ret_attn: res = res,torch.stack(attns)`）。注意力就是值`a`，只需将其放入列表中（`attns.append(a)`）。现在我们可以调用带有返回注意力等于true的模型，并获得概率和注意力[[1:47:53](https://youtu.be/tY0n9OT5_nA?t=1h47m53s)]：'
- en: '[PRE43]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We can now draw pictures, at each time step, of the attention.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在每个时间步绘制注意力的图片。
- en: '[PRE44]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: When you are Chris Olah and Shan Carter, you make things that looks like ☟when
    you are Jeremy Howard, the exact same information looks like ☝︎[[1:48:24](https://youtu.be/tY0n9OT5_nA?t=1h48m24s)].
    You can see at each different time step, we have a different attention.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当你是Chris Olah和Shan Carter时，你做出的东西看起来像☟，当你是Jeremy Howard时，完全相同的信息看起来像☝︎。你可以看到在每个不同的时间步，我们有不同的注意力。
- en: It’s very important when you try to build something like this, you don’t really
    know if it’s not working right because if it’s not working (as per usual Jeremy’s
    first 12 attempts of this were broken) and they were broken in a sense that it
    wasn’t really learning anything useful. Therefore, it was giving equal attention
    to everything and it wasn’t worse — it just wasn’t much better. Until you actually
    find ways to visualize the thing in a way that you know what it ought to look
    like ahead of time, you don’t really know if it’s working [[1:49:16](https://youtu.be/tY0n9OT5_nA?t=1h49m16s)].
    So it’s really important that you try to find ways to check your intermediate
    steps in your outputs.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试构建这样的东西时，非常重要的一点是，你不知道它是否工作正常，因为如果它不工作（通常情况下，Jeremy的前12次尝试都失败了），它们失败的意义在于它并没有真正学到任何有用的东西。因此，它对每件事都给予了同等的关注，它并没有变得更糟——只是并没有变得更好。直到你真正找到一种方法来以一种你事先知道它应该是什么样子的方式来可视化这个东西，你才真正知道它是否有效。因此，非常重要的一点是，你要尝试找到一种方法来检查你的中间步骤和输出。
- en: '**Question**: What is the loss function of the attentional neural network?
    [[1:49:31](https://youtu.be/tY0n9OT5_nA?t=1h49m31s)] No, there is no loss function
    for the attentional neural network. It is trained end-to-end. It is just sitting
    inside our decoder loop. The loss function for the decoder loop is the same loss
    function because the result contains exactly same thing as before — the probabilities
    of the words. How come the mini neural net learning something? Because in order
    to make the outputs better and better, it would be great if it made the weights
    of weighted-average better and better. So part of creating our output is to please
    do a good job of finding a good set of weights and if it doesn’t do a good job
    of finding good set of weights, then the loss function won’t improve from that
    bit. So end-to-end learning means you throw in everything you can into one loss
    function and the gradients of all the different parameters point in a direction
    that says “hey, you know if you had put more weight over there, it would have
    been better.” And thanks to the magic of the chain rule, it knows to put more
    weight over there, change the parameter in the matrix multiply a little, etc.
    That is the magic of end-to-end learning. It is a very understandable question
    but you have to realize there is nothing particular about this code that says
    this particular bits are separate mini neural network anymore than the GRU is
    a separate little neural network, or a linear layer is a separate little function.
    It’s all ends up pushed into one output which is a bunch of probabilities which
    ends up in one loss function that returns a single number that says this either
    was or wasn’t a good translation. So thanks to the magic of the chain rule, we
    then back propagate little updates to all the parameters to make them a little
    bit better. This is a big, weird, counterintuitive idea and it’s totally okay
    if it’s a bit mind-bending. It is the bit where even back to lesson 1 “how did
    we make it find dogs vs. cats?” — we didn’t. All we did was we said “this is our
    data, this is our architecture, this is our loss function. Please back propagate
    into the weights to make them better and after you’ve made them better a while,
    it will start finding cats from dogs.” In this case (i.e. translation), we haven’t
    used somebody else’s convolutional network architecture. We said “here is a custom
    architecture which we hope is going to be particularly good at this problem.”
    Even without this custom architecture, it was still okay. But we made it in a
    way that made more sense or we think it ought to do worked even better. But at
    no point, did we do anything different other than say “here is a data, here is
    an architecture, here is a loss function — go and find the parameters please”
    And it did it because that’s what neural nets do.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：注意力神经网络的损失函数是什么？没有，注意力神经网络没有损失函数。它是端到端训练的。它只是坐在我们的解码器循环中。解码器循环的损失函数是相同的损失函数，因为结果包含的东西完全相同——单词的概率。为什么这个小型神经网络在学习？因为为了使输出变得更好，如果它使加权平均的权重变得更好，那将是很好的。因此，创建我们的输出的一部分是请尽量找到一组好的权重，如果它不能找到一组好的权重，那么损失函数就不会从那一部分改善。因此，端到端学习意味着你将一切都放入一个损失函数中，所有不同参数的梯度都指向一个方向，即“嘿，你知道如果你在那里放更多的权重，那会更好。”多亏了链式法则的魔力，它知道要在那里放更多的权重，稍微改变矩阵乘法中的参数等。这就是端到端学习的魔力。这是一个非常容易理解的问题，但你必须意识到这段代码中没有任何特定的东西表明这些特定的部分是单独的小型神经网络，就像GRU不是一个单独的小型神经网络，或者线性层不是一个单独的小型函数一样。所有这些最终都被推送到一个输出中，这个输出是一堆概率，最终进入一个返回单个数字的损失函数，这个数字表示这是一个好的翻译还是不是一个好的翻译。多亏了链式法则的魔力，我们然后向所有参数反向传播一点更新，使它们变得更好一点。这是一个很大、很奇怪、很反直觉的想法，如果它有点令人费解，那完全没关系。这是一个让我们回到第一课“我们是如何让它找到狗和猫的？”的地方——我们没有。我们所做的只是说“这是我们的数据，这是我们的架构，这是我们的损失函数。请反向传播到权重，使它们变得更好，当你让它们变得更好一段时间后，它将开始从狗中找到猫。”在这种情况下（即翻译），我们没有使用别人的卷积网络架构。我们说“这是一个我们希望在这个问题上特别擅长的自定义架构。”即使没有这个自定义架构，也还可以。但我们制作的方式更有意义，或者我们认为它应该做得更好。但在任何时候，我们都没有做任何不同的事情，只是说“这是数据，这是架构，这是损失函数——请找到参数”它做到了，因为这就是神经网络所做的事情。'
- en: So that is sequence-to-sequence learning [[1:53:19](https://youtu.be/tY0n9OT5_nA?t=1h53m19s)].
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是序列到序列学习。
- en: If you want to encode an image into a CNN backbone of some kind, and then pass
    that into a decoder which is like RNN with attention, and you make your y-values
    the actual correct caption of each of those image, you will end up with an image
    caption generator.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想将图像编码到某种CNN骨干中，然后将其传递到一个类似带有注意力的RNN的解码器中，然后将你的y值设为每个图像的实际正确字幕，你最终会得到一个图像字幕生成器。
- en: If you do the same thing with videos and captions, you will end up with a video
    caption generator.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你用视频和字幕做同样的事情，你最终会得到一个视频字幕生成器。
- en: If you do the same thing with 3D CT scan and radiology reports, you will end
    up with a radiology report generator.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你用 3D CT 扫描和放射学报告做同样的事情，你最终会得到一个放射学报告生成器。
- en: If you do the same thing with Github issues and people’s chosen summaries of
    them, you’ll get a Github issue summary generator.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你用 Github 问题和人们选择的摘要做同样的事情，你会得到一个 Github 问题摘要生成器。
- en: Seq-to-seq is magical but they work [[1:54:07](https://youtu.be/tY0n9OT5_nA?t=1h54m7s)].
    And I don’t feel like people have begun to scratch the surface of how to use seq-to-seq
    models in their own domains. Not being a Github person, it would never have occurred
    to me that “it would be kind of cool to start with some issue and automatically
    create a summary”. But now, of course, next time I go into Github, I want to see
    a summary written there for me. I don’t want to write my own commit message. Why
    should I write my own summary of the code review when I finished adding comments
    to lots of lines — it should do that for me as well. Now I’m thinking Github so
    behind, it could be doing this stuff. So what are the thing in your industry?
    You could start with a sequence and generate something from it. I can’t begin
    to imagine. Again, it is a fairly new area and the tools for it are not easy to
    use — they are not even built into fastai yet. Hopefully there will be soon. I
    don’t think anybody knows what the opportunities are.
  id: totrans-239
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Seq-to-seq 是神奇的，但它们起作用。我觉得人们还没有开始深入研究如何在自己的领域中使用 seq-to-seq 模型。作为一个不太用 Github
    的人，我从来没有想到“从某个问题开始并自动生成摘要会很酷”。但现在，当然，下次我进入 Github 时，我想看到一个为我写的摘要。我不想写自己的提交消息。当我完成对很多行添加注释后，为什么我要自己写代码审查的摘要呢
    — 它也应该为我做这件事。现在我在想 Github 太落后了，它本来可以做这些事情。那么在你的行业中有什么事情呢？你可以从一个序列开始并生成一些东西。我无法想象。再次强调，这是一个相当新的领域，用于它的工具并不容易使用
    — 它们甚至还没有内置到 fastai 中。希望很快会有。我认为没有人知道机会在哪里。
- en: Devise [[1:55:23](https://youtu.be/tY0n9OT5_nA?t=1h55m23s)]
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Devise [[1:55:23](https://youtu.be/tY0n9OT5_nA?t=1h55m23s)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/devise.ipynb)
    / [Paper](http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model.pdf)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/devise.ipynb)
    / [论文](http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model.pdf)'
- en: We are going to do something bringing together for the first time our two little
    worlds we focused on — text and images [[1:55:49](https://youtu.be/tY0n9OT5_nA?t=1h55m49s)].
    This idea came up in a paper by an extraordinary deep learning practitioner and
    researcher named Andrea Frome. Andrea was at Google at the time and her crazy
    idea was words can have a distributed representation, a space, which particularly
    at that time was just word vectors. And images can be represented in a space.
    In the end, if we have a fully connected layer, they ended up as a vector representation.
    Could we merge the two? Could we somehow encourage the vector space that the images
    end up with be the same vector space that the words are in? And if we could do
    that, what would that mean? What could we do with that? So what could we do with
    that covers things like well, what if I’m wrong what if I’m predicting that this
    image is a beagle and I predict jumbo jet and Yannet’s model predicts corgi. The
    normal loss function says that Yannet’s and Jeremy’s models are equally good (i.e.
    they are both wrong). But what if we could somehow say though you know what corgi
    is closer to beagle than it is to jumbo jets. So Yannet’s model is better than
    Jeremy’s. We should be able to do that because in word vector space, beagle and
    corgi are pretty close together but jumbo jet not so much. So it would give us
    a nice situation where hopefully our inferences would be wrong in saner ways if
    they are wrong. It would also allow us to search for things that are not in ImageNet
    Synset ID (i.e. a category in ImageNet). Why did we have to train a whole new
    model to find dog vs. cats when we already have something that found corgis and
    tabbies. Why can’t we just say find me dogs? If we had trained it in word vector
    space, we totally could because they are word vector, we can find things with
    the right image vector and so forth. We will look at some cool things we can do
    with it in a moment but first of all let’s train a model where this model is not
    learning a category (one hot encoded ID) where every category is equally far from
    every other category, let’s instead train a model where we’re finding a dependent
    variable which is a word vector. so What word vector? Obviously the word vector
    for the word you want. So if it’s corgi, let’s train it to create a word vector
    that’s the corgi word vector, and if it’s a jumbo jet, let’s train it with a dependent
    variable that says this is the word vector for a jumbo jet.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要做一些事情，第一次将我们专注的两个小世界——文本和图像[[1:55:49](https://youtu.be/tY0n9OT5_nA?t=1h55m49s)]结合起来。这个想法是由一位名叫Andrea
    Frome的杰出深度学习从业者和研究人员提出的。当时Andrea在谷歌工作，她疯狂的想法是单词可以有一个分布式表示，一个空间，特别是在那个时候只是单词向量。图像也可以在一个空间中表示。最后，如果我们有一个全连接层，它们最终会成为一个向量表示。我们能够合并这两者吗？我们能否以某种方式鼓励图像最终得到的向量空间与单词所在的向量空间相同？如果我们能做到这一点，那意味着什么？我们可以用它做什么？那么我们可以用它做什么，涵盖了诸如“如果我错了怎么办，如果我预测这张图片是一只猎犬，而我预测是大型喷气机，Yannet的模型预测是柯基。正常的损失函数表示Yannet和Jeremy的模型一样好（即它们都是错误的）。但如果我们能以某种方式说，你知道柯基更接近猎犬而不是大型喷气机。所以Yannet的模型比Jeremy的更好。我们应该能够做到这一点，因为在单词向量空间中，猎犬和柯基是非常接近的，但大型喷气机不是那么接近。所以这将给我们一个很好的情况，希望我们的推理如果错误的话会以更合理的方式出错。这也将使我们能够搜索不在ImageNet
    Synset ID（即ImageNet中的一个类别）中的事物。为什么我们必须训练一个全新的模型来找到狗和猫，当我们已经有找到柯基和虎斑猫的东西。为什么我们不能只是说找到狗？如果我们在单词向量空间中训练过它，我们完全可以，因为它们是单词向量，我们可以找到具有正确图像向量的东西等等。我们将在一会儿看一些我们可以用它做的很酷的事情，但首先让我们训练一个模型，这个模型不是学习一个类别（独热编码ID），其中每个类别与其他每个类别的距离都是相等的，而是训练一个模型，我们正在寻找一个依赖变量，这是一个单词向量。那么什么单词向量？显然是你想要的单词的单词向量。所以如果是柯基，让我们训练它创建一个柯基单词向量，如果是大型喷气机，让我们训练它与一个依赖变量说这是大型喷气机的单词向量。
- en: '[PRE45]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: It is shockingly easy [[1:59:17](https://youtu.be/tY0n9OT5_nA?t=1h59m17s)].
    Let’s grab the fast text word vectors again, load them in (we only need English
    this time).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这实在太容易了。让我们再次获取fast text单词向量，加载它们进来（这次我们只需要英语）。
- en: '[PRE46]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: So for example, “jeremy” and “Jeremy” have a correlation of .6.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，“jeremy”和“Jeremy”的相关系数为0.6。
- en: '[PRE47]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Jeremy doesn’t like bananas at all, and “banana” and “Jeremy” .14\. So words
    that you would expect to be correlated are correlated and words that should be
    as far away from each other as possible, unfortunately, they are still slightly
    correlated but not so much [[1:59:41](https://youtu.be/tY0n9OT5_nA?t=1h59m41s)].
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy一点也不喜欢香蕉，“香蕉”和“Jeremy”相关系数为0.14。所以你期望相关的词是相关的，而应该尽可能远离彼此的词，不幸的是，它们仍然略微相关，但不那么明显。
- en: Map ImageNet classes to word vectors
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将ImageNet类别映射到单词向量
- en: Let’s now grab all of the ImageNet classes because we actually want to know
    which one is corgi and which one is jumbo jet.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们获取所有ImageNet类别，因为我们实际上想知道哪一个是柯基，哪一个是大型喷气机。
- en: '[PRE48]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We have a list of all of those up on files.fast.ai that we can grab them.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在files.fast.ai上有一个所有这些的列表，我们可以获取它们。
- en: '[PRE49]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let’s also grab a list of all of the nouns in English which Jeremy made available
    here:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还获取Jeremy提供的所有英语名词的列表：
- en: '[PRE50]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: So we have the names of each of the thousand ImageNet classes and all of the
    nouns in English according to WordNet which is a popular thing for representing
    what words are and are not. We can now load that list of ImageNet classes, turn
    that into a dictionary, so `classids_1k` contains the class IDs for the 1000 images
    that are in the competition dataset.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有每个千个ImageNet类别的名称，以及根据WordNet列出的所有英语名词，这是一个用于表示哪些词是什么的流行工具。我们现在可以加载ImageNet类别列表，将其转换为字典，因此`classids_1k`包含了比赛数据集中的1000个图像的类别ID。
- en: '[PRE51]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Here is an example. A “tench” apparently is a kind of fish.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子。一个“tench”显然是一种鱼。
- en: '[PRE52]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Let’s do the same thing for all those WordNet nouns [[2:01:11](https://youtu.be/tY0n9OT5_nA?t=2h1m11s)].
    It turns out that ImageNet is using WordNet class names so that makes it nice
    and easy to map between the two.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为所有这些WordNet名词做同样的事情。结果发现ImageNet正在使用WordNet类名，这样在两者之间进行映射就变得简单了。
- en: '[PRE53]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: So these are our two worlds — we have the ImageNet thousand and we have the
    82,000 which are in WordNet.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的两个世界 — 我们有ImageNet的一千个和WordNet中的82,000个。
- en: '[PRE54]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: So we want to map the two together which is as simple as creating a couple of
    dictionaries to map them based on the Synset ID or the WordNet ID.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要将这两者联系起来，这只是简单地创建一些字典来基于Synset ID或WordNet ID进行映射。
- en: '[PRE55]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: What we need to do now is grab the 82,000 nouns in WordNet and try and look
    them up in fast text. We’ve managed to look up 49,469 of them in fast text. We
    now have a dictionary that goes from synset ID which is what WordNet calls them
    to word vectors. We also have the same thing specifically for the 1k ImageNet
    classes.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要做的是获取WordNet中的82,000个名词，并尝试在快速文本中查找它们。我们已经在快速文本中查找到了49,469个名词。我们现在有一个字典，从synset
    ID（即WordNet称之为的ID）到单词向量。我们还为1k个ImageNet类别做了同样的事情。
- en: '[PRE56]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Now we grab all of the ImageNet which you can download from Kaggle now [[2:02:54](https://youtu.be/tY0n9OT5_nA?t=2h2m54s)].
    If you look at the Kaggle ImageNet localization competition, that contains the
    entirety of the ImageNet classifications as well.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们获取了所有的ImageNet，你现在可以从Kaggle下载。如果你看一下Kaggle的ImageNet本地化比赛，其中包含了所有的ImageNet分类。
- en: '[PRE57]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: It has a validation set of 28,650 items in it. For every image in ImageNet,
    we can grab its fast text word vector using the synset to word vector (`syn2wv`)
    and we can stick that into the image vectors array (`img_vecs`), stack that all
    up into a single matrix and save that away.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 其中有28,650个项目的验证集。对于ImageNet中的每个图像，我们可以使用synset到单词向量（`syn2wv`）获取其快速文本词向量，并将其放入图像向量数组（`img_vecs`），将所有这些堆叠到一个矩阵中并保存下来。
- en: '[PRE58]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Now what we have is something for every ImageNet image, we also have the fast
    text word vector that it is associated with [[2:03:43](https://youtu.be/tY0n9OT5_nA?t=2h3m43s)]
    by looking up the synset ID → WordNet → Fast text → word vector.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为每个ImageNet图像都有一个与之相关联的快速文本词向量。通过查找synset ID → WordNet → 快速文本 → 单词向量。
- en: '[PRE59]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Here is a cool trick [[2:04:06](https://youtu.be/tY0n9OT5_nA?t=2h4m6s)]. We
    can now create a model data object which specifically is an image classifier data
    object and we have this thing called `from_names_and_array` I’m not sure if we’ve
    used it before but we can pass it a list of file names (all of the file names
    in ImageNet) and an array of our dependent variables (all of the fast text word
    vectors). We can then pass in the validation indexes which in this case is just
    all of the last IDs — we need to make sure that they are the same as ImageNet
    uses otherwise we will be cheating. Then we pass in `continuous=True` which means
    this puts a lie again to this image classifier data is now an image regressive
    data so continuous equals True means don’t one hot encode my outputs but treat
    them just as continuous values. So now we have a model data object that contains
    all of our file names and for every file name a continuous array representing
    the word vector for that. So we have data, now we need an architecture and the
    loss function.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个很酷的技巧。我们现在可以创建一个模型数据对象，它专门是一个图像分类器数据对象，我们有一个叫做`from_names_and_array`的东西，我不确定我们以前是否使用过，但我们可以传递一个文件名列表（ImageNet中的所有文件名）和一个我们的因变量数组（所有快速文本词向量）。然后我们传入验证索引，这种情况下只是所有最后的ID
    — 我们需要确保它们与ImageNet使用的相同，否则我们会作弊。然后我们传入`continuous=True`，这意味着这个图像分类器数据现在是一个图像回归数据，连续等于True意味着不要对我的输出进行独热编码，而是将它们视为连续值。现在我们有一个模型数据对象，其中包含所有文件名，对于每个文件名，都有一个表示该单词向量的连续数组。所以我们有了数据，现在我们需要一个架构和损失函数。
- en: '[PRE60]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let’s create an architecture [[2:05:26](https://youtu.be/tY0n9OT5_nA?t=2h5m26s)].
    We’ll revise this next week, but we can use the tricks we’ve learnt so far and
    it’s actually incredibly simple. Fastai has a `ConvnetBuilder` which is what gets
    called when you say `ConvLerner.pretrained` and you specify:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个架构。我们下周会对此进行修订，但我们可以使用到目前为止学到的技巧，实际上非常简单。Fastai有一个`ConvnetBuilder`，当你说`ConvLerner.pretrained`时就会调用它，并指定：
- en: '`f`: the architecture (we are going to use ResNet50)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f`: 架构（我们将使用ResNet50）'
- en: '`c`: how many classes you want (in this case, it’s not really classes — it’s
    how many outputs you want which is the length of the fast text word vector i.e.
    300).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`c`: 你想要多少类（在这种情况下，它实际上不是类别，而是你想要的输出数量，即快速文本词向量的长度，即300）。'
- en: '`is_multi`: It is not a multi classification as it is not classification at
    all.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_multi`: 这不是多分类，因为根本不是分类。'
- en: '`is_reg`: Yes, it is a regression.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_reg`: 是的，这是一个回归。'
- en: '`xtra_fc` : What fully connected layers you want. We are just going to add
    one fully connected hidden layer of a length of 1024\. Why 1024? The last layer
    of ResNet50 I think is 1024 long, the final output we need is 300 long. We obviously
    need our penultimate (second to the last) layer to be longer than 300\. Otherwise
    it’s not enough information, so we just picked something a bit bigger. Maybe different
    numbers would be better but this worked for Jeremy.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xtra_fc`: 你想要什么全连接层。我们将添加一个长度为1024的全连接隐藏层。为什么是1024？我认为ResNet50的最后一层是1024，我们需要的最终输出是300。显然，我们需要倒数第二层比300长。否则信息不足，所以我们选择了稍大一点的值。也许不同的数字会更好，但这对Jeremy有效。'
- en: '`ps` : how much dropout you want. Jeremy found that the default dropout, he
    was consistently under fitting so he just decreased the dropout from 0.5 to 0.2.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ps`: 你想要多少dropout。Jeremy发现默认的dropout，他一直欠拟合，所以他将dropout从0.5降低到0.2。'
- en: So this is now a convolutional neural network that does not have any softmax
    or anything like that because it’s regression it’s just a linear layer at the
    end and that’s our model [[2:06:55](https://youtu.be/tY0n9OT5_nA?t=2h6m55s)].
    We can create a ConvLearner from that model and give it an optimization function.
    So now all we need is a loss function.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是一个卷积神经网络，没有任何softmax之类的东西，因为它是回归，最后只是一个线性层，这就是我们的模型。我们可以从该模型创建一个ConvLearner，并为其提供一个优化函数。现在我们只需要一个损失函数。
- en: '[PRE61]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '**Loss Function** [[2:07:38](https://youtu.be/tY0n9OT5_nA?t=2h7m38s)]: Default
    loss function for regression is L1 loss (the absolute differences) — that is not
    bad. But unfortunately in really high dimensional spaces (anybody who has studied
    a bit of machine learning probably knows this) everything is on the outside (in
    this case, it’s 300 dimensional). When everything is on the outside, distance
    is not meaningless but a little bit awkward. Things tend to be close together
    or far away, it doesn’t really mean much in these really high dimensional spaces
    where everything is on the edge. What does mean something, though, is that if
    one thing is on the edge over here, and one thing is on the edge over there, we
    can form an angle between those vectors and the angle is meaningful. That is why
    we use cosine similarity when we are looking for how close or far apart things
    are in high dimensional spaces. If you haven’t seen cosine similarity before,
    it is basically the same as Euclidean distance but it’s normalized to be a unit
    norm (i.e. divided by the length). So we don’t care about the length of the vector,
    we only care about its angle. There is a bunch of stuff that you could easily
    learn in a couple of hours but if you haven’t seen it before, it’s a bit mysterious.
    For now, just know that loss functions and high dimensional spaces where you are
    trying to find similarity, you care about angle and you don’t care about distance
    [[2:09:13](https://youtu.be/tY0n9OT5_nA?t=2h9m13s)]. If you didn’t use the following
    custom loss function, it would still work but it’s a little bit less good. Now
    we have data, architecture, and loss function, therefore, we are done. We can
    go ahead and fit.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数**：回归的默认损失函数是L1损失（绝对差异）- 这并不坏。但不幸的是，在真正高维空间中（任何稍微了解一点机器学习的人可能都知道），一切都在外面（在这种情况下，是300维）。当一切都在外面时，距离并不毫无意义，但有点尴尬。事物往往要么靠在一起，要么远离，在这些真正高维空间中，一切都在边缘，这并不意味着太多。然而，有意义的是，如果一件事在这边的边缘，另一件事在那边的边缘，我们可以形成这些向量之间的角度，这个角度是有意义的。这就是为什么在寻找高维空间中事物之间的接近或远离时，我们使用余弦相似度。如果你以前没有见过余弦相似度，它基本上与欧几里德距离相同，但被归一化为单位范数（即除以长度）。因此，我们不关心向量的长度，我们只关心它的角度。有很多东西你可以在几个小时内轻松学会，但如果你以前没有见过，它可能有点神秘。现在，只需知道损失函数和高维空间中，你在尝试找到相似性时，你关心角度，而不关心距离。如果你没有使用以下自定义损失函数，它仍然可以工作，但效果会差一点。现在我们有了数据、架构和损失函数，因此，我们完成了。我们可以继续拟合。'
- en: '[PRE62]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: We are training on all of ImageNet that is going to take a long time. So `precompute=True`
    is your friend. Remember `precompute=True`? That is the thing we’ve learnt ages
    ago that caches the output of the final convolutional layer and just trains the
    fully connected bit. Even with `precompute=True`, it takes about 3 minutes to
    train an epoch on all of ImageNet. So this is about an hour worth of training,
    but it’s pretty cool that with fastai, we can train a new custom head on all of
    ImageNet for 40 epochs in an hour or so.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在训练所有的ImageNet，这将需要很长时间。所以`precompute=True`是你的朋友。还记得`precompute=True`吗？那是我们很久以前学到的东西，它会缓存最终卷积层的输出，然后只训练完全连接的部分。即使使用`precompute=True`，在所有的ImageNet上训练一个时代大约需要3分钟。所以这大约是一个小时的训练时间，但很酷的是，使用fastai，我们可以在一个小时左右的时间内在所有的ImageNet上训练一个新的自定义头部40个时代。
- en: '[PRE63]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Image search
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像搜索
- en: Search imagenet classes
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 搜索imagenet类
- en: At the end of all that, we can now say let’s grab the 1000 ImageNet classes,
    let’s predict on our whole validation set, and take a look at a few pictures [[2:10:26](https://youtu.be/tY0n9OT5_nA?t=2h10m26s)].
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些之后，我们现在可以说让我们获取1000个ImageNet类，让我们在整个验证集上进行预测，并查看一些图片。
- en: '[PRE64]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Because validation set is ordered, tall the stuff of the same type are in the
    same place.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 因为验证集是有序的，所有相同类型的东西都在同一个地方。
- en: '[PRE65]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '**Nearest neighbor search** [[2:10:56](https://youtu.be/tY0n9OT5_nA?t=2h10m56s)]:
    What we can now do is we can now use nearest neighbors search. So nearest neighbors
    search means here is one300 dimensional vector and here is a whole a lot of other
    300 dimensional vectors, which things is it closest to? Normally that takes a
    very long time because you have to look through every 300 dimensional vector,
    calculate its distance, and find out how far away it is. But there is an amazing
    almost unknown library called **NMSLib** that does that incredibly fast. Some
    of you may have tried other nearest neighbor’s libraries, I guarantee this is
    faster than what you are using — I can tell you that because it’s been bench marked
    by people who do this stuff for a living. This is by far the fastest on every
    possible dimension. We want to create an index on angular distance, and we need
    to do it on all of our ImageNet word vectors. Adding a whole batch, create the
    index, and now we can query a bunch of vectors all at once, get the 10 nearest
    neighbors. The library uses multi-threading and is absolutely fantastic. You can
    install from pip (`pip install nmslib`) and it just works.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**最近邻搜索**[[2:10:56](https://youtu.be/tY0n9OT5_nA?t=2h10m56s)]：现在我们可以使用最近邻搜索。所谓最近邻搜索意味着这里有一个300维向量，这里有很多其他300维向量，它最接近哪个？通常这需要很长时间，因为你必须查看每个300维向量，计算其距离，找出它有多远。但是有一个几乎不为人知的神奇库叫做**NMSLib**，它可以做得非常快。你们中有些人可能尝试过其他最近邻库，我保证这比你们使用的更快
    —— 我可以告诉你这一点，因为这是由专业人士进行基准测试的。在每个可能的维度上，这是迄今为止最快的。我们想要在角距离上创建一个索引，并且需要在我们所有的ImageNet词向量上执行。添加一个完整的批次，创建索引，现在我们可以一次查询一堆向量，获取最近的10个邻居。该库使用多线程，绝对棒。你可以从pip安装（`pip
    install nmslib`），它就能用了。'
- en: '[PRE66]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: It tells you how far away they are and their indexes [[2:12:13](https://youtu.be/tY0n9OT5_nA?t=2h12m13s)].
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 它告诉你它们有多远以及它们的索引[[2:12:13](https://youtu.be/tY0n9OT5_nA?t=2h12m13s)].
- en: '[PRE67]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: So now we can go through and print out the top 3 so it turns out that bird actually
    is a limpkin. Interestingly the fourth one does not say it’s a limpkin and Jeremy
    looked it up. He doesn’t know much about birds but everything else is brown with
    white spots, but the 4th one isn’t. So we don’t know if that is actually a limpkin
    or if it is mislabeled but sure as heck it doesn’t look like the other birds.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们可以浏览并打印出前3个，结果是鸟实际上是一只鹭鸟。有趣的是第四个并没有说它是一只鹭鸟，Jeremy查了一下。他对鸟类了解不多，但其他一切都是棕色带白色斑点，但第四个不是。所以我们不知道那是否真的是一只鹭鸟，或者是否被错误标记，但它看起来绝对不像其他鸟类。
- en: '[PRE68]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This is not a particularly hard thing to do because there is only a thousand
    ImageNet classes and it is not doing anything new. But what if we now bring in
    the entirety of WordNet and we now say which of those 45 thousand things is it
    closest to?
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是一件特别困难的事情，因为ImageNet只有一千个类别，而且并没有做任何新的事情。但是如果我们现在引入整个WordNet，然后说它最接近那45,000个东西中的哪一个呢？
- en: Search all WordMet noun classes
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 搜索所有WordMet名词类别
- en: '[PRE69]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Exactly the same result. It is now searching all of the WordNet.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 结果完全相同。现在正在搜索所有的WordNet。
- en: Text -> image search [[2:13:16](https://youtu.be/tY0n9OT5_nA?t=2h13m16s)]
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本->图像搜索[[2:13:16](https://youtu.be/tY0n9OT5_nA?t=2h13m16s)]
- en: Now let’s do something a bit different — which is to take all of our predictions
    (`pred_wv`) so basically take our whole validation set of images and create a
    KNN index of the image representations because remember, it is predicting things
    that are meant to be word vectors. Now let’s grab the fast text vector for “boat”
    and boat is not an ImageNet concept — yet we can now find all of the images in
    our predicted word vectors (i.e. our validation set) that are closest to the word
    boat and it works even though it is not something that was ever trained on.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们做一些有点不同的事情 —— 就是取出我们所有的预测（`pred_wv`），基本上取出我们整个验证图像集的预测，并创建一个图像表示的KNN索引，因为记住，它正在预测那些应该是词向量的东西。现在让我们获取“船”的快速文本向量，船不是ImageNet的概念
    —— 然而我们现在可以找到所有在我们预测的词向量（即我们的验证集）中最接近“船”这个词的图像，即使它并没有被训练过。
- en: '[PRE70]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: What if we now take engine’s vector and boat’s vector and take their average
    and what if we now look in our nearest neighbors for that [[2:14:04](https://youtu.be/tY0n9OT5_nA?t=2h14m4s)]?
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在取引擎的向量和船的向量并取它们的平均值，如果我们现在在最近的邻居中寻找那个[[2:14:04](https://youtu.be/tY0n9OT5_nA?t=2h14m4s)]呢？
- en: '[PRE71]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: These are boats with engines. I mean, yes, the middle one is actually a boat
    with an engine — it just happens to have wings on as well. By the way, sail is
    not an ImageNet thing , neither is boat. Here is the average of two things that
    are not ImageNet things and yet with one exception, it’s found us two sailboats.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是带引擎的船。我的意思是，是的，中间那个实际上是一艘带引擎的船 —— 它碰巧也有翅膀。顺便说一句，帆不是ImageNet的东西，船也不是。这是两个不是ImageNet的东西的平均值，然而除了一个例外，它给我们找到了两艘帆船。
- en: '[PRE72]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Image->image [[2:14:35](https://youtu.be/tY0n9OT5_nA?t=2h14m35s)]
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像->图像[[2:14:35](https://youtu.be/tY0n9OT5_nA?t=2h14m35s)]
- en: Okay, let’s do something else crazy. Let’s open up an image in the validation
    set. Let’s call `predict_array` on that image to get its word vector like thing,
    and let’s do a nearest neighbor search on all the other images.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们做一些疯狂的事情。让我们在验证集中打开一张图像。让我们对该图像调用`predict_array`以获取其类似词向量的东西，并让我们在所有其他图像上进行最近邻搜索。
- en: '[PRE73]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: And here are all the other images of whatever that is. So you can see, this
    is crazy — we’ve trained a thing on all of ImageNet in an hour, using a custom
    head that required basically like two lines fo code, and these things run in 300
    milliseconds to do these searches.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是所有其他任何东西的图像。所以你可以看到，这很疯狂 —— 我们在一个小时内对所有ImageNet进行了训练，使用了一个基本上只需要两行代码的自定义头部，这些搜索运行在300毫秒内。
- en: Jeremy taught this basic idea last year as well, but it was in Keras, and it
    was pages and pages of code, and everything took a long time and complicated.
    And back then, Jeremy said he can’t begin to think all of the stuff you could
    do with this. He doesn’t think anybody has really thought deeply about this yet,
    but he thinks it’s fascinating. So go back and read the DeVICE paper because Andrea
    had a whole bunch of other thoughts and now that it is so easy to do, hopefully
    people will dig into this now. Jeremy thinks it’s crazy and amazing.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy去年也教过这个基本概念，但是当时是在Keras中，代码很长，一切都很复杂。当时Jeremy说他无法想象你可以用这个做什么。他认为没有人真正深入思考过这个问题，但他觉得这很迷人。所以回去读DeVICE论文，因为Andrea有很多其他想法，现在做起来很容易，希望人们现在会深入研究这个问题。Jeremy觉得这太疯狂和令人惊讶了。
- en: Alright, see you next week!
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，下周见！
