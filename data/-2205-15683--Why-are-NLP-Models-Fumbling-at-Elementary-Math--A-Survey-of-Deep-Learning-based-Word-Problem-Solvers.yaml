- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:46:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:46:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2205.15683] Why are NLP Models Fumbling at Elementary Math? A Survey of Deep
    Learning based Word Problem Solvers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2205.15683] 为什么NLP模型在基础数学问题上表现不佳？基于深度学习的文字问题求解器综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2205.15683](https://ar5iv.labs.arxiv.org/html/2205.15683)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2205.15683](https://ar5iv.labs.arxiv.org/html/2205.15683)
- en: Why are NLP Models Fumbling at Elementary Math?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么NLP模型在基础数学问题上表现不佳？
- en: A Survey of Deep Learning based Word Problem Solvers
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的文字问题求解器综述
- en: Sowmya S Sundaram L3S Research Center, Hannover, Germany Sairam Gurajada IBM
    Research, Almaden, USA¹¹1Work done while author was here Marco Fisichella L3S
    Research Center, Hannover, Germany Deepak P Queen’s University, Belfast, UK Savitha
    Sam Abraham University of Örebro, Sweden
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Sowmya S Sundaram L3S研究中心，德国汉诺威 Sairam Gurajada IBM研究，USA¹¹1作者在此处进行的工作 Marco
    Fisichella L3S研究中心，德国汉诺威 Deepak P 女王大学，英国贝尔法斯特 Savitha Sam Abraham Örebro大学，瑞典
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: From the latter half of the last decade, there has been a growing interest in
    developing algorithms for automatically solving mathematical word problems (MWP).
    It is a challenging and unique task that demands blending surface level text pattern
    recognition with mathematical reasoning. In spite of extensive research, we are
    still miles away from building robust representations of elementary math word
    problems and effective solutions for the general task. In this paper, we critically
    examine the various models that have been developed for solving word problems,
    their pros and cons and the challenges ahead. In the last two years, a lot of
    deep learning models have recorded competing results on benchmark datasets, making
    a critical and conceptual analysis of literature highly useful at this juncture.
    We take a step back and analyse why, in spite of this abundance in scholarly interest,
    the predominantly used experiment and dataset designs continue to be a stumbling
    block. From the vantage point of having analyzed the literature closely, we also
    endeavour to provide a road-map for future math word problem research.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从上世纪最后十年下半期开始，对自动解决数学文字问题（MWP）算法的兴趣逐渐增加。这是一项具有挑战性且独特的任务，需要将表层文本模式识别与数学推理相结合。尽管进行了广泛的研究，我们仍然离建立稳健的基础数学文字问题表示和有效的通用解决方案还有很远的距离。在本文中，我们对各种已开发的解决问题模型进行批判性检查，分析它们的优缺点及未来面临的挑战。在过去两年中，许多深度学习模型在基准数据集上取得了具有竞争力的结果，使得在这一时刻对文献进行批判性和概念性的分析非常有用。我们退一步分析，尽管学术兴趣如此丰富，主要使用的实验和数据集设计仍然是一个障碍。从密切分析文献的角度出发，我们还努力为未来的数学文字问题研究提供一个路线图。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Natural language processing has been one of the most popular and intriguing
    AI-complete sub-fields of artificial intelligence. One of the earliest systems
    arguably was the PhD Thesis on automatically solving arithmetic word problems
    Bobrow ([1964](#bib.bib2)). The challenge lay on two fronts (a) analysing unconstrained
    natural language, and (b) mapping intricate text patterns onto a small mathematical
    vocabulary, for usage within its reasoning framework.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理一直是人工智能中最受欢迎且最引人入胜的AI-完全子领域之一。最早的系统之一可以说是Bobrow的博士论文，题为自动解决算术文字问题（[1964](#bib.bib2)）。挑战主要有两个方面：（a）分析无约束的自然语言，以及（b）将复杂的文本模式映射到一个小的数学词汇中，以便在其推理框架内使用。
- en: Right up until 2010, there has been prolific exploration of MWP solvers, for
    various domains (such as algebra, percentages, ratio etc). These solvers relied
    heavily on hand-crafted rules for bridging the gap between language and the corresponding
    mathematical notation. As can be surmised, these approaches, while being effective
    within their niches, did not generalise well to address the broader problem of
    solving MWPs. Moreover, due to the lack of well accepted datasets, it is hard
    to measure the relative performance across proposed systems Mukherjee and Garain
    ([2008](#bib.bib45)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2010年，已经在各种领域（如代数、百分比、比例等）进行了大量的MWP求解器探索。这些求解器在很大程度上依赖于手工制作的规则，以弥合语言和相应数学符号之间的差距。可以推测，虽然这些方法在其特定领域中有效，但未能很好地推广到解决MWP的更广泛问题。此外，由于缺乏被广泛接受的数据集，很难衡量所提出系统的相对性能（Mukherjee和Garain
    [2008](#bib.bib45)）。
- en: '| Input | Kevin has 3 books. Kylie has 7 books. How many books do they have
    together? |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | Kevin有3本书。Kylie有7本书。他们一共有多少本书？ |'
- en: '| Answer | 10 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 答案 | 10 |'
- en: 'Table 1: Typical Example'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：典型示例
- en: 'The pioneering work by Kushman et al. ([2014](#bib.bib27)) employed statistical
    methods to solve word problems, which set the stage for the development of automatic
    MWP solvers using traditional machine learning methods. The work also introduced
    the first dataset, popularly referred to as Alg514, that had multiple linear equations
    associated with a problem. The machine learning task was to map the coefficients
    in the equation to the numbers in the problem. The dataset comprises data units
    with a triplet structure: natural language question, equation set, and the final
    answer.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Kushman 等人 ([2014](#bib.bib27)) 的开创性工作使用统计方法解决文字问题，为利用传统机器学习方法开发自动 MWP 解算器奠定了基础。该工作还引入了第一个数据集，通常称为
    Alg514，包含与问题相关的多个线性方程。机器学习任务是将方程中的系数映射到问题中的数字。该数据集由三元组结构的数据单元组成：自然语言问题、方程集和最终答案。
- en: Mirroring recent trends in NLP, there has been an explosion of deep learning
    models for MWP. Some of the early ones Wang et al. ([2017](#bib.bib68)); Ling
    et al. ([2017](#bib.bib38)) modeled the task of converting the text to equation
    as a sequence-to-sequence (seq2seq, for short) problem. In this context, increasingly
    complex models have been proposed to capture semantics beyond the surface text.
    Some have captured structural information (pertaining to input text, domain knowledge,
    output equation structure) in the form of graphs and used advances in graph neural
    networks (Li et al. ([2020](#bib.bib33)), Zhang et al. ([2020c](#bib.bib80)),
    etc.). Others have utilised the benefits of transformers in their modelling (Liang
    et al. ([2021](#bib.bib35)), Piękos et al. ([2021](#bib.bib49)), etc.). We will
    explore these models in detail.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 镜鉴自然语言处理（NLP）的最新趋势，针对数学文字问题（MWP）的深度学习模型迅猛增长。一些早期的模型，如 Wang 等人 ([2017](#bib.bib68))
    和 Ling 等人 ([2017](#bib.bib38))，将文本转换为方程的任务建模为序列到序列（seq2seq，简称）问题。在这种背景下，越来越复杂的模型被提出以捕捉超越表面文本的语义。一些模型以图的形式捕捉结构信息（涉及输入文本、领域知识、输出方程结构），并利用图神经网络的进展（Li
    等人 ([2020](#bib.bib33))，Zhang 等人 ([2020c](#bib.bib80)) 等）。另一些模型则利用了变换器在建模中的优势（Liang
    等人 ([2021](#bib.bib35))，Piękos 等人 ([2021](#bib.bib49)) 等）。我们将详细探讨这些模型。
- en: Since this is a problem that has consistently attracted steady (arguably, slow
    and steady) attention, ostensibly right from the birth of the field of NLP, a
    survey of the problem solving techniques offers a good horizon for researchers.
    The authors collected 30+ papers on deep learning for word problem solving, published
    over the last three years across premier NLP avenues. Each paper has its own unique
    intuitive basis, but most achieve comparable empirical performance. The profusion
    of methods has made it hard to crisply point out the state-of-the-art, even for
    fairly general word problem solving settings. Hence, a broad overview of the techniques
    employed gives a good grounding for further research. Similarly, understanding
    the source, settings and relevance of datasets is often important. For example,
    there are many datasets that are often referred to by multiple names at different
    points in time. Also, the finer aspects of problem scenario varies across systems
    (whether multiple equations can be solved, whether it is restricted to algebra
    or more domains etc.). In this survey, we systematically analyse the models, list
    the benchmark datasets and examine word problem solving literature using a critical
    analysis perspective.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个从自然语言处理领域诞生之初就一直受到稳定（可以说是缓慢而稳定）关注的问题，对解决该问题技术的综述为研究人员提供了良好的视野。作者收集了过去三年在主要
    NLP 领域发表的30+篇关于文字问题解决的深度学习论文。每篇论文都有其独特的直观基础，但大多数实现了相当的实证表现。方法的丰富使得即便是对于比较一般的文字问题解决设置，也难以明确指出最新技术水平。因此，对采用的技术进行广泛概述为进一步研究提供了良好的基础。同样，理解数据集的来源、设置和相关性也很重要。例如，许多数据集在不同时间点通常被不同名称引用。此外，问题场景的细微方面在系统之间有所不同（例如是否可以解决多个方程，是否仅限于代数或更多领域等）。在本综述中，我们系统分析了模型，列出了基准数据集，并从批判性分析的角度审查了文字问题解决文献。
- en: Related Surveys
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关综述
- en: 'There are two seminal surveys that cover word problem solving research. One, Mukherjee
    and Garain ([2008](#bib.bib45)), has a detailed overview of the symbolic solvers
    for this problem. The second, more recent one Zhang et al. ([2020a](#bib.bib78)),
    covers models proposed up until 2020\. In the last two years, there has been a
    sharp spike in algorithms developed, that focus on various aspects of deep learning,
    to model this problem. Our survey is predominantly based on these deep learning
    models. The differentiating aspects of our survey from another related one, Faldu
    et al. ([2021](#bib.bib11)) are: the usage of a critical perspective to analyze
    deep learning models, which enables us to identify robustness deficiencies in
    the methods analytically, and also to trace them back to model design and dataset
    choice issues. We will also include empirical performance values of various methods
    on popular datasets, and deliberate on future directions.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有两项开创性的调查涵盖了文字问题求解研究。一项是Mukherjee和Garain（[2008](#bib.bib45)），详细概述了这一问题的符号求解器。第二项较新的调查是Zhang等人（[2020a](#bib.bib78)），涵盖了截至2020年提出的模型。在过去两年里，专注于深度学习的各种算法开发出现了急剧上升，以建模这一问题。我们的调查主要基于这些深度学习模型。我们调查与另一相关调查，Faldu等人（[2021](#bib.bib11)）的区别在于：使用批判性的视角分析深度学习模型，使我们能够在分析中识别方法的稳健性缺陷，并追溯到模型设计和数据集选择问题。我们还将包括各种方法在热门数据集上的经验性能值，并讨论未来的方向。
- en: Symbolic Solvers
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 符号求解器
- en: We begin our discussion with traditional solvers that employ a rule-based method
    to convert text input to a set of symbols. Early solvers within this family such
    as STUDENT Bobrow ([1964](#bib.bib2)) and other subsequent ones (Fletcher ([1985](#bib.bib13)),
    Dellarosa ([1986](#bib.bib8))), the dominant methodology was to map natural language
    input to an underlying pre-defined schema. This calls for a mechanism to distil
    common expectations of language, word problems and the corresponding mathematical
    notation, to form bespoke rulesets that will power the conversion. This may be
    seen as setting up a slot-filling mechanism that map the main entities of the
    word problem to a slots within a set of equation templates.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的讨论从使用基于规则的方法将文本输入转换为一组符号的传统求解器开始。在这一领域的早期求解器，如STUDENT Bobrow（[1964](#bib.bib2)）及其他后续的求解器（Fletcher（[1985](#bib.bib13)），Dellarosa（[1986](#bib.bib8)）），主要的方法是将自然语言输入映射到一个预定义的基础模式。这需要一种机制来提炼语言、文字问题及相应数学符号的共同预期，以形成定制的规则集来驱动转换。这可以被视为建立一个槽填充机制，将文字问题的主要实体映射到一组方程模板中的槽位。
- en: An example of a schema for algebraic MWP is shown in Table [2](#Sx2.T2 "Table
    2 ‣ Symbolic Solvers ‣ Why are NLP Models Fumbling at Elementary Math? A Survey
    of Deep Learning based Word Problem Solvers").
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表示代数MWP（数学文字问题）的模式的示例见表[2](#Sx2.T2 "表 2 ‣ 符号求解器 ‣ 为什么NLP模型在基础数学问题上踉踉跄跄？深度学习基础的文字问题求解器调查")。
- en: '| Problem | John has 5 apples. He gave 2 to Mary. How many does he have now?
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 约翰有5个苹果。他给了玛丽2个，他现在还剩多少个？ |'
- en: '| --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Template | [Owner[1]] has [X] [obj]. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | [所有者[1]] 拥有 [X] [对象]。 |'
- en: '|  | [Owner[1]] [transfer] [Y] [obj] to [Owner[2]]. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | [所有者[1]] [转移] [Y] [对象] 给 [所有者[2]]。 |'
- en: '|  | [Owner[1]] has [Z] [obj]. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | [所有者[1]] 拥有 [Z] [对象]。 |'
- en: '|  | Z = X - Y |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | Z = X - Y |'
- en: '| Slot-Filling | [John] has [5] [apple]. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 槽填充 | [约翰] 拥有 [5] [苹果]。 |'
- en: '|  | [John] [give] [2] [apple] to [Mary]. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | [约翰] [给] [2] [苹果] 给 [玛丽]。 |'
- en: '|  | [Mary] has [Z] [apple]. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | [玛丽] 拥有 [Z] [苹果]。 |'
- en: '|  | Z = 5 - 2 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | Z = 5 - 2 |'
- en: '| Answer | Z = 3 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 答案 | Z = 3 |'
- en: 'Table 2: Workflow of Symbolic Solvers'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：符号求解器的工作流程
- en: The advantage is that these systems are robust in handling irrelevant information,
    with expert-authored rulesets enabling focus towards pertinent parts of the problem.
    To further enhance the practical effectiveness within applications focusing niche
    domains, research focused on tailoring these symbolic systems for target domains Mukherjee
    and Garain ([2008](#bib.bib45)). As one can observe, the rules would need to be
    exhaustive to capture the myriad nuances of language. Thus, they did not generalise
    well across varying language styles. Since each system was designed for a particular
    domain, comparative performance evaluation was hindered by the unavailability
    of cross-domain datasets.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 优势在于这些系统在处理无关信息方面具有鲁棒性，专家编写的规则集使其能够专注于问题的相关部分。为了进一步提高在专注于特定领域的应用中的实际效果，研究致力于为目标领域量身定制这些符号系统 Mukherjee
    和 Garain ([2008](#bib.bib45))。如观察所示，规则需要全面，以捕捉语言的各种细微差别。因此，它们在不同语言风格之间的泛化效果不佳。由于每个系统都是为特定领域设计的，因此由于缺乏跨领域数据集，比较性能评估受到了阻碍。
- en: Statistical Solvers
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计求解器
- en: As with many tasks in natural language processing, statistical machine learning
    techniques to solve word problems started dominating the field from 2014\. The
    central theme of these techniques has been to score a number of potential solutions
    (may be equations or expression trees as we will see shortly) within an optimization
    based scoring framework, and subsequently arrive at the correct mathematical model
    for the given text. This may be thought of as viewing the task as a structure
    prediction challenge Zhang et al. ([2020a](#bib.bib78)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与自然语言处理中的许多任务一样，统计机器学习技术在解决词汇问题上自2014年起开始主导该领域。这些技术的核心主题是，在基于优化的评分框架中评分多个潜在解决方案（可能是方程或表达树，如我们将很快看到的），然后得出给定文本的正确数学模型。这可以视为将任务视为结构预测挑战
    Zhang 等 ([2020a](#bib.bib78))。
- en: '|  | $P(y&#124;x;\theta)=\frac{e^{\theta.\phi(x,y)}}{\sum_{y^{\prime}\in Y}e^{\theta.\phi(x,y^{\prime})}}$
    |  | (1) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(y\|x;\theta)=\frac{e^{\theta.\phi(x,y)}}{\sum_{y^{\prime}\in Y}e^{\theta.\phi(x,y^{\prime})}}$
    |  | (1) |'
- en: As with optimization problems, Equation 1 refers to the problem of learning
    parameters $\theta$, which relate to the feature function $\phi$. Consider labeled
    dataset $D$ consisting of $n$ pairs $(x,y,a)$ where $x$ is the natural language
    question, $y$ is the mathematical expression and $a$ is the numerical answer.
    The task is to score all possible expressions $Y$, and maximise the choice of
    the labelled $y$ through an optimisation setting. This is done by modifying the
    parameters $\theta$ of the feature function $\phi(x,y)$. Different models propose
    different formulations of $\phi$. In practise, beam search is used as a control
    mechanism. We grouped the prolific algorithms that were developed, based on the
    type of mathematical structure $y$ - either as equation templates or expression
    trees. Equation templates were mined from training data, much like the slot filling
    idea of symbolic systems. However, they became a bottleneck to generalizability,
    if the word problem at inference time, was from an unseen equation template. To
    address this issue, expression trees, with unambiguous post-fix traversals, were
    used to model equations. Though they restricted the complexity of the systems
    to single equation models, they offered wider scope for generalizability.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与优化问题类似，方程1涉及学习参数 $\theta$ 的问题，这些参数与特征函数 $\phi$ 相关。考虑标记数据集 $D$，它由 $n$ 对 $(x,y,a)$
    组成，其中 $x$ 是自然语言问题，$y$ 是数学表达式，$a$ 是数值答案。任务是对所有可能的表达式 $Y$ 进行评分，并通过优化设置最大化标记的 $y$
    选择。这是通过修改特征函数 $\phi(x,y)$ 的参数 $\theta$ 完成的。不同的模型提出了不同的 $\phi$ 公式。在实践中，束搜索被用作控制机制。我们根据数学结构
    $y$ 的类型，将开发的丰富算法分为两类——方程模板或表达树。方程模板是从训练数据中挖掘的，有点类似于符号系统的槽填充概念。然而，如果推理时的词汇问题来自未见过的方程模板，它们成为泛化的瓶颈。为了解决这个问题，使用具有明确后缀遍历的表达树来建模方程。尽管它们将系统的复杂性限制在单一方程模型中，但它们提供了更广泛的泛化范围。
- en: Equation Templates
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方程模板
- en: Equation templates extract out the numeric coefficients and maintain the variable
    and operator structure. This was used as a popular representation of mathematical
    modelling. To begin with, Kushman et al. ([2014](#bib.bib27)), used structure
    prediction to score both equation templates and alignment of the numerals in the
    input text to coefficients in the template. Using a state based representation,
    Hosseini et al. ([2014](#bib.bib18)) modelled simple elementary level word problems
    with emphasis on verb categorisation. Zhou et al. ([2015](#bib.bib81)) enhanced
    the work done by Kushman et al. ([2014](#bib.bib27)) by using quadratic programming
    to increase efficiency. Upadhyay and Chang ([2017](#bib.bib65)) introduced a sophisticated
    method of representing derivations in this space.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 方程模板提取出数值系数，并保持变量和运算符结构。这被用作数学建模的流行表示。首先，Kushman 等 ([2014](#bib.bib27)) 使用结构预测来对方程模板和输入文本中数字与模板系数的对齐进行评分。使用基于状态的表示，Hosseini
    等 ([2014](#bib.bib18)) 模型化了简单的基础级别单词问题，重点关注动词分类。Zhou 等 ([2015](#bib.bib81)) 通过使用二次规划来提高效率，从而增强了
    Kushman 等 ([2014](#bib.bib27)) 的工作。Upadhyay 和 Chang ([2017](#bib.bib65)) 介绍了一种在此空间中表示推导的复杂方法。
- en: Expression Trees
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表达式树
- en: Expression trees are applicable only to single equation systems. The single
    equation is represented as a tree, with leaves of the tree being numbers and the
    internal nodes being operators as illustrated in Koncel-Kedziorski et al. ([2015](#bib.bib24)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式树仅适用于单方程系统。单个方程被表示为一棵树，树的叶子是数字，内部节点是运算符，如 Koncel-Kedziorski 等 ([2015](#bib.bib24))
    所示。
- en: Expression tree based methods converge faster, understandably due to the diminished
    complexity of the model. Some solvers (such as Roy and Roth ([2015](#bib.bib52)))
    had a joint optimisation objective to identify relevant numbers and populating
    the expression tree. On the other hand, Koncel-Kedziorski et al. ([2015](#bib.bib24));
    Mitra and Baral ([2016](#bib.bib44)) used domain knowledge to constrain the search
    space.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式树基于的方法收敛速度更快，这显然是因为模型复杂度降低。一些解算器（如 Roy 和 Roth ([2015](#bib.bib52))) 设有联合优化目标，以识别相关数字并填充表达式树。另一方面，Koncel-Kedziorski
    等 ([2015](#bib.bib24))；Mitra 和 Baral ([2016](#bib.bib44)) 使用领域知识来限制搜索空间。
- en: Neural Solvers
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经解算器
- en: Among the major challenges for the solvers we have seen so far was that of converting
    the input text into a meaningful feature space to enable downstream solving; the
    main divergences across papers seen across the previous sections has been based
    on the technological flavour and methodology employed for such text-to-representation
    conversion.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止我们看到的解算器的主要挑战之一是将输入文本转换为有意义的特征空间，以便进行后续求解；前几节中看到的论文之间的主要分歧基于技术风格和用于文本到表示转换的方法论。
- en: '{forest}'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '{森林}'
- en: for tree= draw, text width=1.4cm, align=center , forked edges, [Automatic
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 用于树= 绘制，文本宽度=1.4cm，居中对齐，分叉边缘，[自动
- en: Word
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 单词
- en: Problem
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 问题
- en: Solvers [Symbolic
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 解算器 [符号
- en: Solvers] [Statistical
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 解算器] [统计
- en: Solvers [Expression
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 解算器 [表达式
- en: Trees] [Equation
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 树] [方程
- en: Templates] ] [Neural
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 模板] ] [神经
- en: Solvers [Seq2Seq] [Graph-Based] [Transformers] [Contrastive] [Knowledge
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 解算器 [Seq2Seq] [基于图的] [变换器] [对比] [知识
- en: Distillation] ] ] \node[draw, fit=(current bounding box.south east) (current
    bounding box.north west)] ;
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏] ] ] \node[draw, fit=(current bounding box.south east) (current bounding
    box.north west)] ;
- en: 'Figure 1: Types of Word Problem Solvers'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：单词问题解算器类型
- en: The advent of distributed representations for text Le and Mikolov ([2014](#bib.bib29));
    Peters et al. ([2018](#bib.bib48)); Pennington et al. ([2014](#bib.bib47)); Devlin
    et al. ([2018](#bib.bib9)), marked a sharp departure in the line of inquiry towards
    solving math word problems, focusing on the details of the learning architecture
    rather than feature-space modelling. There have even been domain specific distributed
    representation learners for word problems Sundaram et al. ([2020](#bib.bib61)).
    As an example of solvers, Ling et al. ([2017](#bib.bib38)) designed a seq2seq
    model that incorporated learning a program as an intermediate step. This and other
    early works made it fashionable to treat the word problem solving task as a language
    translation task, i.e., translating from the input natural language text to a
    sequence of characters representing either the equation or a sequence of predicates.
    This design choice, however, has its limitations, which are sometimes severe in
    terms of the restrictions they place on math problems that can be admitted within
    such architectures Patel et al. ([2021](#bib.bib46)). A few of these linguistic
    vs. math structure understanding challenges, especially for neural solvers, are
    illustrated in Table  [3](#Sx4.T3 "Table 3 ‣ Neural Solvers ‣ Why are NLP Models
    Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers").
    As an important example, equation systems that involve solving multiple equations
    are not straightforward to address within such a framework. A notable exception
    to this is the popular baseline MathDQN Wang et al. ([2018](#bib.bib67)), which
    employs deep reinforcement learning. We consider different families of deep learning
    solvers within separate sub-sections herein.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式表示的出现 Le 和 Mikolov ([2014](#bib.bib29)); Peters 等 ([2018](#bib.bib48)); Pennington
    等 ([2014](#bib.bib47)); Devlin 等 ([2018](#bib.bib9))，标志着在解决数学文字问题的研究方向上的重大转变，重点关注学习架构的细节，而不是特征空间建模。甚至出现了针对文字问题的领域特定分布式表示学习者
    Sundaram 等 ([2020](#bib.bib61))。例如，Ling 等 ([2017](#bib.bib38)) 设计了一种 seq2seq 模型，将学习程序作为中间步骤。这些早期工作使得将文字问题求解任务视为语言翻译任务，即从输入自然语言文本翻译为表示方程或谓词序列的字符序列，成为一种流行做法。然而，这种设计选择有其局限性，有时对可以被纳入这种架构的数学问题有严重的限制
    Patel 等 ([2021](#bib.bib46))。表 [3](#Sx4.T3 "表 3 ‣ 神经求解器 ‣ 为什么 NLP 模型在基础数学上失误？深度学习基础的文字问题求解器的调查")
    中展示了这些语言结构与数学结构理解挑战的一些例子，特别是对于神经求解器而言。一个重要的例子是，涉及解决多个方程的方程组在这种框架下并不容易处理。一个显著的例外是流行的基准
    MathDQN Wang 等 ([2018](#bib.bib67))，该方法采用了深度强化学习。我们在下文的不同子节中考虑了不同类别的深度学习求解器。
- en: '| Input | Kevin has 3 books. Kylie has 7 books and 3 pencils. How many books
    do they have together? |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | Kevin 有 3 本书。Kylie 有 7 本书和 3 支铅笔。他们一共拥有多少本书？ |'
- en: '| Mathematical Structure | 3 + 7 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 数学结构 | 3 + 7 |'
- en: '| Linguistic Structure | (Person1) has (X) (object1). (Person2) has (Y) (object1)
    and (Z) (object2). |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 语言结构 | (Person1) 有 (X) (object1)。 (Person2) 有 (Y) (object1) 和 (Z) (object2)。
    |'
- en: '| Challenges | (1) Order of X and Y does not matter in addition (2) multiple
    equations do not make a sequence (3) Similar objects need to be grouped together
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 挑战 | (1) X 和 Y 的顺序在加法中不重要 (2) 多个方程不形成序列 (3) 相似对象需要归为一组 |'
- en: 'Table 3: Typical Challenges'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 典型挑战'
- en: Seq2Seq Solvers
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Seq2Seq 求解器
- en: The ubiquitous Seq2Seq Sutskever et al. ([2014](#bib.bib63)) architecture is
    widely popular for automatic word problem solving. From early direct use of LSTMs
    Hochreiter and Schmidhuber ([1997](#bib.bib16)) / GRUs Cho et al. ([2014](#bib.bib6))
    in Seq2Seq models (Huang et al. ([2017](#bib.bib19)), Wang et al. ([2017](#bib.bib68)))
    to complex models that include domain knowledge Ling et al. ([2017](#bib.bib38));
    Qin et al. ([2020](#bib.bib51)); Chiang and Chen ([2019](#bib.bib5)); Qin et al.
    ([2021](#bib.bib50))), diverse formulations of this basic architecture have been
    employed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛使用的 Seq2Seq Sutskever 等 ([2014](#bib.bib63)) 架构在自动文字问题求解中非常流行。从早期直接使用 LSTMs
    Hochreiter 和 Schmidhuber ([1997](#bib.bib16)) / GRUs Cho 等 ([2014](#bib.bib6))
    的 Seq2Seq 模型 (Huang 等 ([2017](#bib.bib19)), Wang 等 ([2017](#bib.bib68))) 到包括领域知识的复杂模型
    Ling 等 ([2017](#bib.bib38)); Qin 等 ([2020](#bib.bib51)); Chiang 和 Chen ([2019](#bib.bib5));
    Qin 等 ([2021](#bib.bib50)))，对这一基本架构的多种形式已被采用。
- en: <svg   height="198.51" overflow="visible" version="1.1" width="353.91"><g transform="translate(0,198.51)
    matrix(1 0 0 -1 0 0) translate(61.76,0) translate(0,178.55)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -45.39 -3.38)" fill="#000000"
    stroke="#000000"><foreignobject width="90.79" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Input sequence</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 97.84 -3.38)" fill="#000000" stroke="#000000"><foreignobject width="101.93"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Output sequence</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -44.12 -84.1)" fill="#000000" stroke="#000000"><foreignobject
    width="88.25" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Math
    Problem</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 121.23 -82.68)"
    fill="#000000" stroke="#000000"><foreignobject width="55.16" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Equation</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -55.89 -162.05)" fill="#000000" stroke="#000000"><foreignobject width="112.16"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word Embeddings</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 107.27 -162.05)" fill="#000000" stroke="#000000"><foreignobject
    width="180.27" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Character/Word
    Embeddings</foreignobject></g></g></svg>
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="198.51" overflow="visible" version="1.1" width="353.91"><g transform="translate(0,198.51)
    matrix(1 0 0 -1 0 0) translate(61.76,0) translate(0,178.55)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -45.39 -3.38)" fill="#000000"
    stroke="#000000"><foreignobject width="90.79" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">输入序列</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 97.84 -3.38)" fill="#000000" stroke="#000000"><foreignobject width="101.93"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">输出序列</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -44.12 -84.1)" fill="#000000" stroke="#000000"><foreignobject
    width="88.25" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">数学问题</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 121.23 -82.68)" fill="#000000" stroke="#000000"><foreignobject
    width="55.16" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">方程</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -55.89 -162.05)" fill="#000000" stroke="#000000"><foreignobject
    width="112.16" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">词嵌入</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 107.27 -162.05)" fill="#000000" stroke="#000000"><foreignobject
    width="180.27" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">字符/词嵌入</foreignobject></g></g></svg>
- en: 'Figure 2: General Seq2Seq Formulations'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：通用 Seq2Seq 表达式
- en: The initial set of models used Seq2Seq as is, with small variations in the usage
    of LSTM or GRUs or with simple heuristics (for example, Huang et al. ([2016](#bib.bib20))
    used retrieval to enhance the results). Significant improvements were made by
    including some mathematical aspects. This, once again, demonstrates that the task
    is not merely that of language translation. Ling et al. ([2017](#bib.bib38)) converted
    the word problem to a text containing the explanation or rationale. This was done
    through an intermediate step of generating a step-by-step program on a large dataset.
    Though the accuracy values reported were low, the domains spanned anywhere between
    probability to relative velocity, and the unified framework demonstrated performing
    meaningful analysis through qualitative illustrations. This was improved upon
    by Amini et al. ([2019](#bib.bib1)), which enhanced the dataset and added domain
    information through a label on the category. The SAU-Solver Qin et al. ([2020](#bib.bib51))
    introduced a tree like representation with semantic elements that align to the
    word problem. As seen in Table  [6](#Sx8.T6 "Table 6 ‣ Performance of Deep Models
    ‣ Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based
    Word Problem Solvers"), this is a formidable contender. In Chiang and Chen ([2019](#bib.bib5)),
    a novel way of decomposing the equation construction into a set of stack operations
    - such that more nuanced mapping between language and operators can be learned
    - was designed. There is a burgeoning section of the literature that is invested
    in using neuro-symbolic reasoning to bridge this gap between perception level
    tasks (language understanding) and cognitive level tasks (mathematical reasoning).
    An example of this is Qin et al. ([2021](#bib.bib50)). With this discussion, it
    is clear that adding some form of domain knowledge benefits an automatic solver.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 初始的一组模型直接使用了Seq2Seq，使用LSTM或GRUs的变体或简单启发式方法（例如，黄等人（[2016](#bib.bib20)）通过检索来提升结果）。通过引入一些数学方面的内容，取得了显著的改进。这再次证明了这个任务不仅仅是语言翻译的问题。凌等人（[2017](#bib.bib38)）将文字问题转换为包含解释或理由的文本。这是通过在大数据集上生成逐步程序的中间步骤完成的。尽管报告的准确率较低，但领域范围从概率到相对速度不等，统一框架通过定性示例展示了有意义的分析。这一点在阿米尼等人（[2019](#bib.bib1)）的研究中得到了改进，该研究通过对类别的标签增强了数据集并添加了领域信息。SAU-Solver秦等人（[2020](#bib.bib51)）引入了一种树状表示法，其中的语义元素与文字问题对齐。如表[6](#Sx8.T6
    "Table 6 ‣ Performance of Deep Models ‣ Why are NLP Models Fumbling at Elementary
    Math? A Survey of Deep Learning based Word Problem Solvers")所示，这是一个强有力的竞争者。在江和陈（[2019](#bib.bib5)）的研究中，设计了一种将方程构建分解为一组栈操作的新方法——以便可以学习语言与运算符之间更细致的映射。文献中有一个新兴领域致力于使用神经符号推理来弥合感知层面任务（语言理解）和认知层面任务（数学推理）之间的差距。秦等人（[2021](#bib.bib50)）的研究就是一个例子。通过这些讨论，显而易见，添加某种形式的领域知识对自动求解器有益。
- en: Graph-based Solvers
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于图的求解器
- en: With the advent of graph modeling Xia et al. ([2019](#bib.bib74)) and enhanced
    interest in multi-modal processing, the graph data structure became a vehicle
    for adding knowledge to solvers. One way of enabling this has been to simply model
    the input problem as a graph Feng et al. ([2021](#bib.bib12)); Li et al. ([2020](#bib.bib33));
    Yu et al. ([2021](#bib.bib76)); Hong et al. ([2021](#bib.bib17)). This incorporates
    domain knowledge of (a) language interactions pertinent to mathematical reasoning,
    or (b) quantity graphs stating how various numerals in the text are connected.
    Another way is to model the decoder side to accept graphical input of equations
    Xie and Sun ([2019](#bib.bib75)); Lin et al. ([2021](#bib.bib37)); Zaporojets
    et al. ([2021](#bib.bib77)); Cao et al. ([2021](#bib.bib3)); Liu et al. ([2019](#bib.bib40));
    Wu et al. ([2021b](#bib.bib72)). Another natural pathway that has been employed
    towards leveraging graphs is to use graph neural networks for both encoder and
    decoder Zhang et al. ([2020c](#bib.bib80)); Wu et al. ([2020](#bib.bib70), [2021a](#bib.bib71));
    Shen and Jin ([2020](#bib.bib58)).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 随着图模型的出现 Xia 等人 ([2019](#bib.bib74)) 和对多模态处理的兴趣增加，图数据结构成为了向求解器添加知识的一种手段。实现这一点的一种方式是简单地将输入问题建模为图
    Feng 等人 ([2021](#bib.bib12)); Li 等人 ([2020](#bib.bib33)); Yu 等人 ([2021](#bib.bib76));
    Hong 等人 ([2021](#bib.bib17))。这包括领域知识 (a) 与数学推理相关的语言交互，或 (b) 量图表明文本中各种数字如何连接。另一种方式是建模解码器以接受方程的图形输入
    Xie 和 Sun ([2019](#bib.bib75)); Lin 等人 ([2021](#bib.bib37)); Zaporojets 等人 ([2021](#bib.bib77));
    Cao 等人 ([2021](#bib.bib3)); Liu 等人 ([2019](#bib.bib40)); Wu 等人 ([2021b](#bib.bib72))。另一种自然的途径是利用图形神经网络用于编码器和解码器
    Zhang 等人 ([2020c](#bib.bib80)); Wu 等人 ([2020](#bib.bib70), [2021a](#bib.bib71));
    Shen 和 Jin ([2020](#bib.bib58))。
- en: <svg   height="198.51" overflow="visible" version="1.1" width="258.25"><g transform="translate(0,198.51)
    matrix(1 0 0 -1 0 0) translate(54.72,0) translate(0,178.55)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -38.31 -3.46)" fill="#000000"
    stroke="#000000"><foreignobject width="76.62" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Graph Input</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 96.73 -3.38)" fill="#000000" stroke="#000000"><foreignobject width="104.16"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Output Sequence</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -46.51 -82.68)" fill="#000000" stroke="#000000"><foreignobject
    width="93.02" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Input
    Sequence</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 104.93 -82.75)"
    fill="#000000" stroke="#000000"><foreignobject width="87.77" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Graph Output</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -38.31 -162.05)" fill="#000000" stroke="#000000"><foreignobject width="76.62"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Graph Input</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 104.93 -162.05)" fill="#000000" stroke="#000000"><foreignobject
    width="87.77" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Graph
    Output</foreignobject></g></g></svg>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="198.51" overflow="visible" version="1.1" width="258.25"><g transform="translate(0,198.51)
    matrix(1 0 0 -1 0 0) translate(54.72,0) translate(0,178.55)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -38.31 -3.46)" fill="#000000"
    stroke="#000000"><foreignobject width="76.62" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">图形输入</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 96.73 -3.38)" fill="#000000" stroke="#000000"><foreignobject width="104.16"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">输出序列</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -46.51 -82.68)" fill="#000000" stroke="#000000"><foreignobject
    width="93.02" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">输入序列</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 104.93 -82.75)" fill="#000000" stroke="#000000"><foreignobject
    width="87.77" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">图形输出</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -38.31 -162.05)" fill="#000000" stroke="#000000"><foreignobject
    width="76.62" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">图形输入</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 104.93 -162.05)" fill="#000000" stroke="#000000"><foreignobject
    width="87.77" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">图形输出</foreignobject></g></g></svg>
- en: 'Figure 3: General Graph based Formulations'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：基于图的通用公式
- en: Graphs are capable of representing complex relationships. With the time-tested
    success of graph neural networks (GNNs) Wu et al. ([2021c](#bib.bib73)), they
    fit easily into the encoder-decoder architecture. Intuitively, when graphs are
    used on the input side, we can model complex semantic relationships in the linguistic
    side of the task. When graphs are used on the decoder side, relationships between
    the numerical entities or an intermediate representation of the problem can be
    captured. Analogously, graph-to-graph modelling enables matching the semantics
    of both language and math. This does not necessarily imply graph-to-graph outperforms
    all the other formulations. There are unique pros and cons of each of the graph-based
    papers, as both language and mathematical models are hard to (a) model separately
    and (b) model the interactions. The interesting observation as seen in Table  [6](#Sx8.T6
    "Table 6 ‣ Performance of Deep Models ‣ Why are NLP Models Fumbling at Elementary
    Math? A Survey of Deep Learning based Word Problem Solvers"), graph based models
    are both popular and powerful. Unlike sequences, when the input text is represented
    as a graph, the focus is more on relevant entities rather than a stream of text.
    Similarly quantity graphs or semantics informed graphs, eliminate ordering ambiguities
    in equations. This formulation, however, still does not address the multiple equation
    problem.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图形能够表示复杂的关系。凭借图神经网络（GNNs） Wu 等人 ([2021c](#bib.bib73)) 的成功，它们很容易融入编码器-解码器架构。从直观上讲，当在输入侧使用图形时，我们可以建模任务的语言侧的复杂语义关系。当在解码器侧使用图形时，可以捕捉到数值实体之间的关系或问题的中间表示。类似地，图到图建模使语言和数学的语义匹配成为可能。这并不一定意味着图到图超越了所有其他形式化。每种基于图的论文都有独特的优缺点，因为语言和数学模型都很难
    (a) 单独建模和 (b) 建模交互。正如在表 [6](#Sx8.T6 "表 6 ‣ 深度模型的性能 ‣ 为什么 NLP 模型在基础数学上表现不佳？深度学习基础词汇问题解算器的调查")
    中看到的那样，基于图的模型既流行又强大。与序列不同，当输入文本表示为图形时，重点更多地放在相关实体上，而不是文本流。同样，数量图或语义信息图消除了方程中的排序歧义。然而，这种形式化仍未解决多个方程问题。
- en: Transformers
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformer
- en: Transformers Vaswani et al. ([2017](#bib.bib66)) have lately revolutionised
    the field of NLP. Word problem solving has been no exception. Through the use
    of BERT Devlin et al. ([2018](#bib.bib9)) embeddings or through transformer based
    encoder-decoder models, some recent research has leveraged concepts from transformer
    models Liu et al. ([2019](#bib.bib40)); Kim et al. ([2020](#bib.bib22)). The translation
    has been modeled variously, such as from text to explanation Piękos et al. ([2021](#bib.bib49));
    Griffith and Kalita ([2020](#bib.bib15)), or from text to equation Shen et al.
    ([2021](#bib.bib57)); Liang et al. ([2021](#bib.bib35)).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer Vaswani 等人 ([2017](#bib.bib66)) 最近彻底改变了自然语言处理（NLP）领域。词汇问题解决也不例外。通过使用
    BERT Devlin 等人 ([2018](#bib.bib9)) 嵌入或通过基于 Transformer 的编码器-解码器模型，一些近期的研究利用了 Transformer
    模型的概念 Liu 等人 ([2019](#bib.bib40)); Kim 等人 ([2020](#bib.bib22))。翻译被多种方式建模，例如从文本到解释
    Piękos 等人 ([2021](#bib.bib49)); Griffith 和 Kalita ([2020](#bib.bib15))，或者从文本到方程
    Shen 等人 ([2021](#bib.bib57)); Liang 等人 ([2021](#bib.bib35))。
- en: When moving from Word2Vec Mikolov et al. ([2013](#bib.bib43)) vectors to BERT
    embeddings Devlin et al. ([2018](#bib.bib9)), massive gains were expected due
    to (a) greater incorporation of context level information and (b) automatic capturing
    of relevant information as BERT is essentially a Masked Language Model. Interestingly,
    the gains do not have as large a margin as seen in other language tasks such as
    question answering or machine translation Devlin et al. ([2018](#bib.bib9)). BERT
    is a large model that needs to be fine tuned with domain specific information.
    The small gains point towards low quality of word problem datasets, which is in
    line with the fact that the datasets are either quite small by deep learning standards
    or that they have high lexical overlap, effectively suggesting that the set of
    characteristic word problems are small.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当从 Word2Vec Mikolov 等人 ([2013](#bib.bib43)) 向 BERT 嵌入 Devlin 等人 ([2018](#bib.bib9))
    迁移时，预计会有巨大的收益，因为 (a) 更大程度地融入了上下文信息和 (b) 自动捕获了相关信息，因为 BERT 本质上是一个掩蔽语言模型。有趣的是，这些收益并不像在其他语言任务如问答或机器翻译
    Devlin 等人 ([2018](#bib.bib9)) 中那样大。BERT 是一个大型模型，需要用领域特定的信息进行微调。这些小的收益表明词汇问题数据集的质量较低，这与数据集要么在深度学习标准下相当小，要么具有高词汇重叠的事实相符，实际上暗示了特征词汇问题的集合很小。
- en: Contrastive Solvers
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对比解算器
- en: With the widespread usage of Siamese networks Koch et al. ([2015](#bib.bib23)),
    the idea of building representations that contrast between vectorial representations
    across classes in data has seen some interest. In the context of word problem
    solving, a few bespoke transformer based encoder-decoder models Li et al. ([2021b](#bib.bib34));
    Hong et al. ([2021](#bib.bib17)) have been proposed; these seek to effectively
    leverage contrastive learning Le-Khac et al. ([2020](#bib.bib30)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Siamese网络的广泛使用（Koch et al. ([2015](#bib.bib23)）），构建在数据类间矢量表示对比的表示的想法引起了一些关注。在解决文字问题的背景下，一些定制的基于变换器的编码器-解码器模型（Li
    et al. ([2021b](#bib.bib34))；Hong et al. ([2021](#bib.bib17))）已经被提出；这些模型旨在有效利用对比学习（Le-Khac
    et al. ([2020](#bib.bib30)））。
- en: This is a relatively new paradigm and more research needs to emerge to ascertain
    definite trends. One of the main stumbling blocks of word problem solving is that
    two highly linguistically similar looking word problems may have entirely different
    mathematical structure. Since contrastive learning is built on the principle that
    similar input examples lead to closer representations, it allows one to use the
    notion of similarity and dissimilarity to overcome this bottleneck and consciously
    design semantically informed intermediate representations, such that the similarity
    is built not only from the language vocabulary, but also from the mathematical
    concepts.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相对较新的范式，仍需要更多研究来确定明确的趋势。解决文字问题的主要障碍之一是两个在语言上高度相似的文字问题可能具有完全不同的数学结构。由于对比学习基于相似输入示例会导致更接近的表示的原则，它使得我们可以利用相似性和差异性来克服这个瓶颈，并有意识地设计语义信息丰富的中间表示，从而使相似性不仅来自语言词汇，还来自数学概念。
- en: Teacher-Student Solvers
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 教师-学生解算器
- en: The paradigm of knowledge distillation, in the wake of large, generic end-to-end
    models, has become popular in NLP Li et al. ([2021a](#bib.bib32)). The underlying
    idea behind this is to distill smaller task-specific models from a generic large
    pre-trained or generic model. Since word problem datasets are of comparatively
    smaller size, it is but logical that large generic networks can be fine-tuned
    for downstream processing of word problem solving, as favourably demonstrated
    by Zhang et al. ([2020b](#bib.bib79)) and Hong et al. ([2021](#bib.bib17)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型通用端到端模型的背景下，知识蒸馏的范式在自然语言处理领域变得流行（Li et al. ([2021a](#bib.bib32)））。其基本思想是从通用的大型预训练模型中提炼出较小的任务特定模型。由于文字问题数据集的规模相对较小，因此大型通用网络可以针对文字问题解决进行微调，这一点在Zhang
    et al. ([2020b](#bib.bib79)) 和 Hong et al. ([2021](#bib.bib17)) 的研究中得到了有利的证明。
- en: Once again, this is an emerging paradigm. Similar to the discussion we presented
    with transformer based models, the fact that the presence of pre-trained language
    models alone is not sufficient for this task has bolstered initial efforts in
    this direction. Knowledge distillation enables a model to focus the learnings
    of one generic model on to a smaller, more focussed one, especially with less
    datapoints. Hence, the method of adding semantic information through the usage
    of knowledge distillation algorithms is promising and one to look out for.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这也是一个新兴的范式。与我们之前讨论的基于变换器的模型类似，预训练语言模型单独存在的事实不足以完成这一任务，这增强了这一方向的初步努力。知识蒸馏使得一个模型能够将一个通用模型的学习集中到一个更小、更专注的模型上，尤其是在数据点较少的情况下。因此，通过使用知识蒸馏算法添加语义信息的方法是有前景的，值得关注。
- en: Domain-Niche Solvers
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领域-细分解算器
- en: Some research, encompassing families of statistical solvers and deep models,
    focus on the pertinent characteristics of a particular domain in mathematics,
    such as probability word problems Dries et al. ([2017](#bib.bib10)); Suster et al.
    ([2021](#bib.bib62)); Tsai et al. ([2021](#bib.bib64)), number theory word problems
    Shi et al. ([2015](#bib.bib59)), geometry word problems Seo et al. ([2015](#bib.bib56));
    Chen et al. ([2021](#bib.bib4)) and age word problems Sundaram and Abraham ([2019](#bib.bib60)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究，涵盖了统计解算器和深度模型的家族，关注数学中特定领域的相关特征，例如概率文字问题（Dries et al. ([2017](#bib.bib10))；Suster
    et al. ([2021](#bib.bib62))；Tsai et al. ([2021](#bib.bib64)）），数论文字问题（Shi et al.
    ([2015](#bib.bib59)）），几何文字问题（Seo et al. ([2015](#bib.bib56))；Chen et al. ([2021](#bib.bib4)））和年龄文字问题（Sundaram
    and Abraham ([2019](#bib.bib60)））。
- en: Datasets
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: Datasets used for math word problem solving are listed in Table [4](#Sx6.T4
    "Table 4 ‣ Datasets ‣ Why are NLP Models Fumbling at Elementary Math? A Survey
    of Deep Learning based Word Problem Solvers") with their characteristics. The
    top section of the table describes datasets with relatively fewer data objects
    ($\leq 1k$, to be specific). The bottom half consists of more recent datasets
    that are larger and more popularly used within deep learning methodologies.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 用于数学文字问题求解的数据集列在表[4](#Sx6.T4 "Table 4 ‣ Datasets ‣ Why are NLP Models Fumbling
    at Elementary Math? A Survey of Deep Learning based Word Problem Solvers")中，并附有其特点。表的上半部分描述了数据对象相对较少的数据集（具体为$\leq
    1k$）。下半部分包含了更新的数据集，这些数据集更大且在深度学习方法中更为流行。
- en: '| Dataset | Type | Domain | Size | Source |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 | 领域 | 大小 | 来源 |'
- en: '| Alg514 | Multi-equation | (+,-,*,/) | 514 | Kushman et al. ([2014](#bib.bib27))
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Alg514 | 多方程 | (+,-,*,/) | 514 | Kushman 等 ([2014](#bib.bib27)) |'
- en: '| (SimulEq-S) |  |  |  |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| (SimulEq-S) |  |  |  |  |'
- en: '| AddSub | Single-equation | (+,-) | 340 | Hosseini et al. ([2014](#bib.bib18))
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| AddSub | 单方程 | (+,-) | 340 | Hosseini 等 ([2014](#bib.bib18)) |'
- en: '| (AI2) |  |  |  |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| (AI2) |  |  |  |  |'
- en: '| SingleOp | Single-equation | (+,-,*,/) | 562 | Roy et al. ([2015](#bib.bib55))
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| SingleOp | 单方程 | (+,-,*,/) | 562 | Roy 等 ([2015](#bib.bib55)) |'
- en: '| (Illinois, IL) |  |  |  |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| (Illinois, IL) |  |  |  |  |'
- en: '| SingleEq | Single-equation | (+,-,*,/) | 508 | Koncel-Kedziorski et al. ([2015](#bib.bib24))
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| SingleEq | 单方程 | (+,-,*,/) | 508 | Koncel-Kedziorski 等 ([2015](#bib.bib24))
    |'
- en: '| MAWPS | Multi-equation | (+,-,*,/) | 3320 | Koncel-Kedziorski et al. ([2016](#bib.bib25))
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| MAWPS | 多方程 | (+,-,*,/) | 3320 | Koncel-Kedziorski 等 ([2016](#bib.bib25))
    |'
- en: '| MultiArith | Single-equation | (+,-,*,/) | 600 | Roy and Roth ([2015](#bib.bib52))
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| MultiArith | 单方程 | (+,-,*,/) | 600 | Roy 和 Roth ([2015](#bib.bib52)) |'
- en: '| (Common Core, CC) |  |  |  |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| (Common Core, CC) |  |  |  |  |'
- en: '| AllArith | Single-equation | (+,-,*,/) | 831 | Roy and Roth ([2017](#bib.bib53))
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| AllArith | 单方程 | (+,-,*,/) | 831 | Roy 和 Roth ([2017](#bib.bib53)) |'
- en: '| Perturb | Single-equation | (+,-,*,/) | 661 | Roy and Roth ([2017](#bib.bib53))
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Perturb | 单方程 | (+,-,*,/) | 661 | Roy 和 Roth ([2017](#bib.bib53)) |'
- en: '| Aggregate | Single-equation | (+,-,*,/) | 1492 | Roy and Roth ([2017](#bib.bib53))
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Aggregate | 单方程 | (+,-,*,/) | 1492 | Roy 和 Roth ([2017](#bib.bib53)) |'
- en: '| DRAW-1k | Multi-equation | (+,-,*,/) | 1k | Upadhyay and Chang ([2017](#bib.bib65))
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| DRAW-1k | 多方程 | (+,-,*,/) | 1k | Upadhyay 和 Chang ([2017](#bib.bib65)) |'
- en: '| AsDIV-A | Single-equation | (+,-,*,/) | 2373 | Miao et al. ([2020](#bib.bib42))
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| AsDIV-A | 单方程 | (+,-,*,/) | 2373 | Miao 等 ([2020](#bib.bib42)) |'
- en: '| SVAMP | Single-equation | (+,-,*,/) | 1000 | Patel et al. ([2021](#bib.bib46))
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| SVAMP | 单方程 | (+,-,*,/) | 1000 | Patel 等 ([2021](#bib.bib46)) |'
- en: '| Dolphin18k | Multi-equation | (+,-,*,/) | 18k | Huang et al. ([2016](#bib.bib20))
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Dolphin18k | 多方程 | (+,-,*,/) | 18k | Huang 等 ([2016](#bib.bib20)) |'
- en: '| AQuA-RAT | Multiple-choice | - | 100k | Ling et al. ([2017](#bib.bib38))
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| AQuA-RAT | 多项选择 | - | 100k | Ling 等 ([2017](#bib.bib38)) |'
- en: '| Math23k* | Single-equation | (+,-,*,/) | 23k | Huang et al. ([2017](#bib.bib19))
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Math23k* | 单方程 | (+,-,*,/) | 23k | Huang 等 ([2017](#bib.bib19)) |'
- en: '| MathQA | Single-equation | (+,-,*,/) | 35k | Amini et al. ([2019](#bib.bib1))
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| MathQA | 单方程 | (+,-,*,/) | 35k | Amini 等 ([2019](#bib.bib1)) |'
- en: '| HMWP* | Multi-equation | (+,-,*,/) | 5k | Qin et al. ([2020](#bib.bib51))
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| HMWP* | 多方程 | (+,-,*,/) | 5k | Qin 等 ([2020](#bib.bib51)) |'
- en: '| Ape210k* | Single-equation | (+,-,*,/) | 210k | Liang et al. ([2021](#bib.bib35))
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Ape210k* | 单方程 | (+,-,*,/) | 210k | Liang 等 ([2021](#bib.bib35)) |'
- en: '| GSM8k | Single-equation | (+,-,*,/) | 8.5k | Cobbe et al. ([2021](#bib.bib7))
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| GSM8k | 单方程 | (+,-,*,/) | 8.5k | Cobbe 等 ([2021](#bib.bib7)) |'
- en: '| CM17k* | Multi-equation | (+,-,*,/) | 17k | Qin et al. ([2021](#bib.bib50))
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| CM17k* | 多方程 | (+,-,*,/) | 17k | Qin 等 ([2021](#bib.bib50)) |'
- en: 'Table 4: Datasets'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：数据集
- en: (*Chinese Datasets)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: (*中文数据集)
- en: Small Datasets
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小型数据集
- en: The pioneering work in solving word problems Kushman et al. ([2014](#bib.bib27)),
    introduced a classical dataset (Alg514) of 514 word problems, across various domains
    in algebra (such as percentages, mixtures, speeds etc). This dataset was annotated
    with multiple equations per problem. AddSub was introduced in Hosseini et al.
    ([2014](#bib.bib18)), with simple addition/subtraction problems, exhibiting limited
    language complexity. SingleOp Roy et al. ([2015](#bib.bib55)) and MultiArith Roy
    and Roth ([2015](#bib.bib52)) were proposed such that there is a control over
    the operators (single operator in the former and two operators in the latter).
    SingleEq Koncel-Kedziorski et al. ([2015](#bib.bib24)) is unique in incorporating
    long sentence structures for elementary level school problems. AllArith Roy and
    Roth ([2017](#bib.bib53)) is a subset of the union of AddSub, SingleEq and SingleOp.
    "Perturb" is a set of slightly perturbed word problems of AllArith, whereas Aggregate
    is the union of AllArith and Perturb. MAWPS (A Math Word Problem Solving Repository)
    Koncel-Kedziorski et al. ([2016](#bib.bib25)) is a curated dataset (with deliberate
    template overlap control) that comprises all proposed datasets till that date.
    A single equation subset of MAWPS (AsDIV-A) Miao et al. ([2020](#bib.bib42)) has
    been studied , for diagnostic analysis of solvers. Similarly, the critique offered
    by Patel et al. ([2021](#bib.bib46)) was demonstrated using their newly proposed
    dataset SVAMP. In SVAMP, minutely perturbed word problems from the popular dataset
    AsDIV-A. This particular subset is used to demonstrate that, while high values
    of accuracy can be obtained on AsDIV-A easily, SVAMP poses a formidable challenge
    to most solvers, as it captures nuances in the relationship between similar language
    formation and dissimilar equations. All aforementioned datasets incorporate an
    annotation of both the equation and the answer. Given the subset-superset relationships
    between some of these datasets, empirical usage of these datasets would need to
    ensure careful sampling to creating subsets for training, testing and cross-validation.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Kushman 等人（[2014](#bib.bib27)）在解决文字题问题方面的开创性工作，介绍了一个经典数据集（Alg514），包含514个跨各种代数领域（如百分比、混合、速度等）的文字题。这个数据集为每个问题标注了多个方程。AddSub
    由 Hosseini 等人（[2014](#bib.bib18)）提出，包含简单的加法/减法问题，展示了有限的语言复杂性。SingleOp 由 Roy 等人（[2015](#bib.bib55)）和
    MultiArith 由 Roy 和 Roth（[2015](#bib.bib52)）提出，允许对操作符进行控制（前者是单一操作符，后者是两个操作符）。SingleEq
    由 Koncel-Kedziorski 等人（[2015](#bib.bib24)）提出，其独特之处在于为小学水平的问题融入了长句结构。AllArith 由
    Roy 和 Roth（[2017](#bib.bib53)）提出，是 AddSub、SingleEq 和 SingleOp 的并集子集。 "Perturb"
    是 AllArith 的轻微扰动的文字题集合，而 Aggregate 是 AllArith 和 Perturb 的并集。MAWPS（数学文字题解决库）由 Koncel-Kedziorski
    等人（[2016](#bib.bib25)）创建，是一个经过精心策划的数据集（具有故意的模板重叠控制），包括了截至那时所有提出的数据集。MAWPS 的单一方程子集（AsDIV-A）由
    Miao 等人（[2020](#bib.bib42)）研究，用于解算器的诊断分析。类似地，Patel 等人（[2021](#bib.bib46)）提供的批评使用了他们新提出的数据集
    SVAMP。在 SVAMP 中，来自流行数据集 AsDIV-A 的细微扰动的文字题。这个特定子集用于展示，尽管在 AsDIV-A 上可以轻松获得高准确值，但
    SVAMP 对大多数解算器构成了严峻挑战，因为它捕捉了相似语言形式与不相似方程之间的细微差别。所有上述数据集都包括方程和答案的注释。鉴于这些数据集之间的子集-超集关系，实际使用这些数据集时需要确保谨慎采样，以创建用于训练、测试和交叉验证的子集。
- en: Large Datasets
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型数据集
- en: Dolphin18k Huang et al. ([2016](#bib.bib20)) is an early proprietary dataset
    that was evaluated primarily with the statistical solvers. AQuA-RAT Ling et al.
    ([2017](#bib.bib38)) introduced the first large crowd-sourced dataset for word
    problems with rationales or explanations. This makes the setting quite different
    from the aforementioned datasets, not only with respect to size, but also in the
    wide variety of domain areas (spanning physics, algebra, geometry, probability
    etc). Another point of difference is that the annotation involves the entire textual
    explanation, rather than just the equations. MathQA Amini et al. ([2019](#bib.bib1))
    critically analysed AQuA-RAT and selected the core subset and annotated it with
    a predicate list, to widen the remit of its usage. Once again, researchers must
    be mindful of the fact that MathQA is a subset of AQuA-RAT. GSM8k Cobbe et al.
    ([2021](#bib.bib7)) is a recent single-equation dataset, that is the large scale
    version of AsDIV-A Miao et al. ([2020](#bib.bib42)). Math23K is a popular Chinese
    dataset for single equation math word problem solving. A recent successor is Ape210k
    Liang et al. ([2021](#bib.bib35)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Dolphin18k Huang 等人 ([2016](#bib.bib20)) 是一个早期的专有数据集，主要使用统计解算器进行评估。AQuA-RAT
    Ling 等人 ([2017](#bib.bib38)) 引入了第一个大型众包数据集，包含具有理由或解释的词题。这使得设置与上述数据集大相径庭，不仅在于规模，还涵盖了多种领域（如物理学、代数、几何、概率等）。另一个不同之处在于注释包括了整个文本解释，而不仅仅是方程式。MathQA
    Amini 等人 ([2019](#bib.bib1)) 对 AQuA-RAT 进行了关键分析，选择了核心子集并用谓词列表进行了注释，以扩大其使用范围。研究人员必须注意，MathQA
    是 AQuA-RAT 的一个子集。GSM8k Cobbe 等人 ([2021](#bib.bib7)) 是一个最近的单方程数据集，是 AsDIV-A Miao
    等人 ([2020](#bib.bib42)) 的大规模版本。Math23K 是一个流行的中文单方程数学词题解决数据集。最近的继任者是 Ape210k Liang
    等人 ([2021](#bib.bib35))。
- en: Evaluation Measures
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估指标
- en: The most popular metric is answer accuracy, which evaluates the predicted equation
    and checks whether it is the same as the labelled one. The other metric is equation
    accuracy, which predominantly does string matching and assesses the match between
    the produced equation and the equation from the annotation label.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的指标是答案准确度，它评估预测的方程式并检查是否与标记的方程式相同。另一个指标是方程式准确度，它主要进行字符串匹配，并评估生成的方程式与注释标签中的方程式之间的匹配度。
- en: Performance of Deep Models
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度模型的性能
- en: In this section, we describe the performance of neural solvers towards providing
    the reader with a high-level view of the comparative performance across the several
    proposed models.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们描述了神经解算器的表现，以向读者提供一个高层次的视角，比较各个提出的模型的性能。
- en: We have listed the performance of the deep models in Table  [6](#Sx8.T6 "Table
    6 ‣ Performance of Deep Models ‣ Why are NLP Models Fumbling at Elementary Math?
    A Survey of Deep Learning based Word Problem Solvers"), on two major datasets
    - Math23K and MAWPS.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格 [6](#Sx8.T6 "Table 6 ‣ Performance of Deep Models ‣ Why are NLP Models
    Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers")
    中列出了深度模型的性能，涉及两个主要数据集——Math23K 和 MAWPS。
- en: Some of these deep models report scores on other datasets as well. For conciseness,
    we have chosen the most popular datasets for deep models. We see that, in general,
    the models achieve around 70-80 percentage points on answer accuracy. Shen et al.
    ([2021](#bib.bib57)) outperforms all other models on Math23k whereas RPKHS Yu
    et al. ([2021](#bib.bib76)) is the best model for MAWPS till date. As mentioned
    before, graph based models are both popular and effective. A note of caution is
    that, as inferred from the discussion on datasets, (a) both Math23k and MAWPS
    are single equation datasets and (b) though some lexical overlap has been performed
    in the design of these two datasets, the semantic quality of these datasets are
    quite similar. This aspect has also been experimented and explored in Patel et al.
    ([2021](#bib.bib46)). Hence, though we present the best performing algorithms
    in this table, more research is required to design a suitable metric or a suitable
    dataset, such that one can conclusively compare these various algorithms.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些深度模型中有一些也在其他数据集上报告了分数。为了简洁起见，我们选择了最受欢迎的深度模型数据集。总体来看，模型在答案准确性上通常达到 70-80 个百分点。Shen
    等 ([2021](#bib.bib57)) 在 Math23k 上超越了所有其他模型，而 RPKHS Yu 等 ([2021](#bib.bib76))
    仍是 MAWPS 最佳模型。如前所述，基于图的模型既受欢迎又有效。需要注意的是，正如数据集讨论中所推断的，(a) Math23k 和 MAWPS 都是单方程数据集，并且
    (b) 尽管这两个数据集在设计时进行了某些词汇重叠，但它们的语义质量非常相似。Patel 等 ([2021](#bib.bib46)) 也对这一方面进行了实验和探索。因此，尽管我们在表中展示了最佳表现的算法，但仍需进一步研究以设计合适的度量标准或数据集，以便可以最终比较这些不同的算法。
- en: '| Model | Type | AQuA-RAT | MathQA | Source |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 类型 | AQuA-RAT | MathQA | 来源 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| AQuA | Seq2Seq | 36.4 | - | Ling et al. ([2017](#bib.bib38)) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| AQuA | Seq2Seq | 36.4 | - | Ling 等 ([2017](#bib.bib38)) |'
- en: '| Seq2Prog | Seq2Seq | 37.9 | 57.2 | Amini et al. ([2019](#bib.bib1)) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Seq2Prog | Seq2Seq | 37.9 | 57.2 | Amini 等 ([2019](#bib.bib1)) |'
- en: '| BERT-NPROP | Transformer | 37.0 | - | Piękos et al. ([2021](#bib.bib49))
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| BERT-NPROP | Transformer | 37.0 | - | Piękos 等 ([2021](#bib.bib49)) |'
- en: '| Graph-To-Tree | Graph-based | - | 69.65 | Li et al. ([2020](#bib.bib33))
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Graph-To-Tree | 基于图的 | - | 69.65 | Li 等 ([2020](#bib.bib33)) |'
- en: 'Table 5: Performance on Large Multi-Domain Datasets'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：大型多领域数据集上的表现
- en: Apart from these algebraic datasets, multi-domain datasets MathQA and AquA are
    also of special interest. This is described in Table  [5](#Sx8.T5 "Table 5 ‣ Performance
    of Deep Models ‣ Why are NLP Models Fumbling at Elementary Math? A Survey of Deep
    Learning based Word Problem Solvers"). The interesting takeaway is that, the addition
    of BERT modelling to AQuA Piękos et al. ([2021](#bib.bib49)), still performed
    slightly worse than the Seq2Prog Amini et al. ([2019](#bib.bib1)) model, which
    is a derivative of the Seq2Seq paradigm.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些代数数据集，多个领域的数据集 MathQA 和 AQuA 也备受关注。这在表格 [5](#Sx8.T5 "Table 5 ‣ Performance
    of Deep Models ‣ Why are NLP Models Fumbling at Elementary Math? A Survey of Deep
    Learning based Word Problem Solvers") 中有所描述。值得注意的是，将 BERT 模型添加到 AQuA Piękos 等
    ([2021](#bib.bib49)) 中，表现仍略逊于 Seq2Prog Amini 等 ([2019](#bib.bib1)) 模型，后者是 Seq2Seq
    范式的衍生模型。
- en: '| Model Name | Type | Math23k | MAWPS | Source |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 类型 | Math23k | MAWPS | 来源 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GTS | Graph-based | 74.3 | - | Xie and Sun ([2019](#bib.bib75)) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| GTS | 基于图的 | 74.3 | - | Xie 和 Sun ([2019](#bib.bib75)) |'
- en: '| SAU-SOLVER | Graph-based | 74.8 | - | Chiang and Chen ([2019](#bib.bib5))
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| SAU-SOLVER | 基于图的 | 74.8 | - | Chiang 和 Chen ([2019](#bib.bib5)) |'
- en: '| Group-att | Transformer | 69.5 | 76.1 | Li et al. ([2019](#bib.bib31)) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Group-att | Transformer | 69.5 | 76.1 | Li 等 ([2019](#bib.bib31)) |'
- en: '| Graph2Tree | Graph-based | 77.4 | - | Li et al. ([2020](#bib.bib33)) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Graph2Tree | 基于图的 | 77.4 | - | Li 等 ([2020](#bib.bib33)) |'
- en: '| KA-S2T | Graph-based | 76.3 | - | Wu et al. ([2020](#bib.bib70)) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| KA-S2T | 基于图的 | 76.3 | - | Wu 等 ([2020](#bib.bib70)) |'
- en: '| NS-Solver | Seq2Seq | 75.67 | - | Qin et al. ([2020](#bib.bib51)) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| NS-Solver | Seq2Seq | 75.67 | - | Qin 等 ([2020](#bib.bib51)) |'
- en: '| Graph-To-Tree | Graph-based | 78.8 | - | Li et al. ([2020](#bib.bib33)) |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Graph-To-Tree | 基于图的 | 78.8 | - | Li 等 ([2020](#bib.bib33)) |'
- en: '| TSN-MD | Teacher Student | 77.4 | 84.4 | Zhang et al. ([2020b](#bib.bib79))
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| TSN-MD | 教师-学生 | 77.4 | 84.4 | Zhang 等 ([2020b](#bib.bib79)) |'
- en: '| Graph-Teacher | Graph & Teacher | 79.1 | 84.2 | Liang and Zhang ([2021](#bib.bib36))
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Graph-Teacher | 图与教师 | 79.1 | 84.2 | Liang 和 Zhang ([2021](#bib.bib36)) |'
- en: '| NumS2T | Graph-based | 78.1 | - | Wu et al. ([2020](#bib.bib70)) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| NumS2T | 基于图的 | 78.1 | - | Wu 等 ([2020](#bib.bib70)) |'
- en: '| Multi-E/D | Graph-based | 78.4 | - | Shen and Jin ([2020](#bib.bib58)) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Multi-E/D | 基于图的 | 78.4 | - | Shen 和 Jin ([2020](#bib.bib58)) |'
- en: '| EPT | Transformer | - | 84.5 | Kim et al. ([2020](#bib.bib22)) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| EPT | Transformer | - | 84.5 | Kim 等 ([2020](#bib.bib22)) |'
- en: '| Seq2DAG | Graph-based | 77.1 | - | Cao et al. ([2021](#bib.bib3)) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Seq2DAG | 基于图的 | 77.1 | - | Cao et al. ([2021](#bib.bib3)) |'
- en: '| EEH-D2T | Graph-based | 78.5 | 84.8 | Wu et al. ([2021a](#bib.bib71)) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| EEH-D2T | 基于图的 | 78.5 | 84.8 | Wu et al. ([2021a](#bib.bib71)) |'
- en: '| Generate and Rank | Graph-based | 85.4 | 84.0 | Shen et al. ([2021](#bib.bib57))
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 生成与排序 | 基于图的 | 85.4 | 84.0 | Shen et al. ([2021](#bib.bib57)) |'
- en: '| HMS | Graph-based | 76.1 | 80.3 | Lin et al. ([2021](#bib.bib37)) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| HMS | 基于图的 | 76.1 | 80.3 | Lin et al. ([2021](#bib.bib37)) |'
- en: '| RPKHS | Graph-based | 83.9 | 89.8 | Yu et al. ([2021](#bib.bib76)) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| RPKHS | 基于图的 | 83.9 | 89.8 | Yu et al. ([2021](#bib.bib76)) |'
- en: '| CL | Contrastive Learning | 83.2 | - | Li et al. ([2021b](#bib.bib34)) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| CL | 对比学习 | 83.2 | - | Li et al. ([2021b](#bib.bib34)) |'
- en: '| GTS+RODA | Graph-based | 77.9 | - | Liu et al. ([2022](#bib.bib39)) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| GTS+RODA | 基于图的 | 77.9 | - | Liu et al. ([2022](#bib.bib39)) |'
- en: 'Table 6: Answer Accuracy of Deep Models'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：深度模型的回答准确率
- en: Analysis of Deep Models
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度模型分析
- en: 'In this section of the paper, we analyze the pros and cons of applying deep
    learning techniques to solve word problems automatically. At the outset, two layers
    of understanding are imperative: (i) linguistic structures that describe a situation
    or a sequence of events and (ii) mathematical structures that govern these language
    descriptions. Though deep learning models have rapidly scaled and demonstrated
    commendable results for capturing these two characteristics, a closer look reveals
    much potential for further exploration. The predominant modus-operandus is to
    create a deep model that converts the input natural language to the underlying
    equation. In some cases, the input is converted into a set of predicates Amini
    et al. ([2019](#bib.bib1)) or explanations Ling et al. ([2017](#bib.bib38)).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的这一部分，我们深入分析了将深度学习技术应用于自动解决文字题的优缺点。首先，理解需要两个层次：（i）描述情况或事件序列的语言结构，以及（ii）支配这些语言描述的数学结构。尽管深度学习模型迅速扩展并展示了捕捉这两个特征的良好结果，但仔细观察仍发现了进一步探索的巨大潜力。主要的操作模式是创建一个深度模型，将输入自然语言转换为基础方程。在某些情况下，输入会转换为一组谓词
    Amini et al. ([2019](#bib.bib1)) 或解释 Ling et al. ([2017](#bib.bib38))。
- en: What Shortcuts are being Learned?
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学到了什么捷径？
- en: Shortcut Learning Geirhos et al. ([2020](#bib.bib14)) is a recently well-studied
    phenomenon of deep neural networks. It describes how deep learning models learn
    patterns in a shallow way and fall prey to questionable generalizations across
    datasets (an example is an image being classified as sheep if there was grass
    alone; due to peculiarities in the dataset).This is a function of the low-level
    input we provide to such models (pixels, word embeddings etc.). In the context
    of word problems, Patel et al. ([2021](#bib.bib46)) exposed how removing the question
    and simply passing the situational context, leads to the correct equation being
    predicted. This suggests two things, issues with model design as well as issues
    with dataset design. The datasets have high equation template overlap, as well
    as text overlap. Word problem solving is a hard because two otherwise identical
    word problems, with a small word change (say changing the word give to take),
    would completely change the equation. Hence high lexical similarity does not translate
    to corresponding similarity in the mathematical realm Patel et al. ([2021](#bib.bib46));
    Sundaram et al. ([2020](#bib.bib61)), and attention to key aspects within the
    text is critical.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*Shortcut Learning* Geirhos et al. ([2020](#bib.bib14)) 是最近对深度神经网络进行广泛研究的现象。它描述了深度学习模型如何以浅层的方式学习模式，并因数据集中的可疑泛化而受害（例如，如果图像中只有草，就被分类为羊；由于数据集中的特殊性）。这与我们提供给模型的低级输入（如像素、词嵌入等）有关。在文字问题的背景下，Patel
    et al. ([2021](#bib.bib46)) 揭示了如何通过去除问题并仅传递情境背景，来预测正确的方程。这表明了模型设计以及数据集设计的问题。数据集具有高方程模板重叠和文本重叠。文字问题的解决很困难，因为两个完全相同的文字问题，仅有一个小的词汇变化（例如将“give”改为“take”），就会完全改变方程。因此，高词汇相似性并不意味着在数学领域的对应相似性
    Patel et al. ([2021](#bib.bib46)); Sundaram et al. ([2020](#bib.bib61))，对文本中关键方面的关注至关重要。'
- en: Is Language or Math being Learned?
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 是语言还是数学在被学习？
- en: '| Problem | Solved? |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 解决？ |'
- en: '| --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| John has 5 apples. Mary has 2 apples more than John. How many apples does
    Mary have? | Yes |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| John 有 5 个苹果。Mary 比 John 多 2 个苹果。Mary 有多少个苹果？ | 是 |'
- en: '| John has 5 apples. Mary has 2 apples more than John. Who has less apples?
    | No |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| John 有 5 个苹果。Mary 比 John 多 2 个苹果。谁有的苹果更少？ | 否 |'
- en: '| What should be added to two to make it five? | No |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 要使两变成五，应添加什么？ | 否 |'
- en: 'Table 7: Behaviour of Baseline BERT Model'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：基线 BERT 模型的行为
- en: The question that looms large is whether adequate mapping of language to math
    has been modelled, whether linguistic modelling has been unfavourably highlighted
    or that the mathematical aspects have been captured succinctly. We observe that
    there are opportunities to refine the modelling of both language and math aspects
    of word problems. Apart from the perturbations experiment done by SVAMP Patel
    et al. ([2021](#bib.bib46)), which exposes that the mapping between linguistic
    and mathematical structures is not captured, we suggest two more experimental
    analysis frameworks that illustrate deficiencies in linguistic and mathematical
    modelling. The first one involves imposing a question answering task on top of
    the word problem as a probing test. For example, a baseline BERT model that converts
    from input language to equation (Table  [7](#Sx9.T7 "Table 7 ‣ Is Language or
    Math being Learned? ‣ Analysis of Deep Models ‣ Why are NLP Models Fumbling at
    Elementary Math? A Survey of Deep Learning based Word Problem Solvers")), trained
    on MAWPS, can solve a simple word problem such as "John has 5 apples. Mary has
    2 apples more than John. How many apples does Mary have?", but cannot answer the
    following allied question "John has 5 apples. Mary has 2 apples more than John.
    Who has less apples?". One reason is of course, dataset design. The governing
    equation for this problem is "X = 5-2". However, the text version of this, "What
    should be added to two to make it five?", cannot be solved by the baseline model.
    Similarly, many solvers wrongly output equations such as "X = 2 - 5" Patel et al.
    ([2021](#bib.bib46)), which suggests mathematical modelling of subtraction of
    whole numbers could potentially be improved by simply embedding more basic mathematical
    aspects. Hence, we observe, that deep translation models neither model language,
    nor the math sufficiently.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要的问题是是否已经对语言与数学的映射进行了充分建模，语言建模是否被不利地强调，还是数学方面已被简洁地捕捉。我们观察到在解决文字问题的语言和数学建模方面有改进的机会。除了SVAMP
    Patel等人（[2021](#bib.bib46)）进行的扰动实验，揭示了语言结构与数学结构之间的映射未被捕捉之外，我们建议两个额外的实验分析框架，以展示语言和数学建模的不足。第一个框架涉及在文字问题上添加问答任务作为探测测试。例如，基于MAWPS训练的基准BERT模型可以解决类似“约翰有5个苹果。玛丽比约翰多2个苹果。玛丽有多少个苹果？”的简单文字问题，但无法回答以下相关问题“约翰有5个苹果。玛丽比约翰多2个苹果。谁的苹果更少？”当然，一个原因是数据集设计。该问题的主方程是“X
    = 5-2”。然而，文本版本的“要使其成为5，应该加到2上多少？”无法被基准模型解决。同样，许多解算器错误地输出了“X = 2 - 5”等方程，Patel等人（[2021](#bib.bib46)）表明，基本数学方面的嵌入可能会改进整数减法的数学建模。因此，我们观察到，深度翻译模型既没有充分建模语言，也没有充分建模数学。
- en: Is Accuracy Enough?
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确性是否足够？
- en: As suggested by the discussion above, a natural line of investigation is to
    examine the evaluation measures, and perhaps the error measures for the deep models,
    in order to bring about a closer coupling between syntax and semantics. High accuracy
    of the models to predicting the answer or the equation suggests a shallow mapping
    between the text and the mathematical symbols. This is analogous to the famously
    observed McNamara fallacy²²2[https://en.wikipedia.org/wiki/McNamara_fallacy](https://en.wikipedia.org/wiki/McNamara_fallacy),
    which cautions against the overuse of a single metric to evaluate a complex problem.
    One direction of exploration is data augmentation with a single word problem annotated
    with multiple equivalent equations. Metrics that measure the soundness of the
    equations generated, the robustness of the model to simple perturbations (perhaps
    achieved using a denoising autoencoder) and the ability of the model to discern
    important entities in a word problem (perhaps using an attention analysis based
    metric), are the need of the future. An endeavour has been done by Kumar et al.
    ([2021](#bib.bib26)), where adversarial examples have been generated and utilised
    to evaluate SOTA models.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，自然的研究方向是检查评估措施，也许还包括深度模型的误差度量，以便在语法和语义之间建立更紧密的联系。模型在预测答案或方程的准确性高，表明文本与数学符号之间存在浅层映射。这类似于著名的
    McNamara 谬误²²2 [https://en.wikipedia.org/wiki/McNamara_fallacy](https://en.wikipedia.org/wiki/McNamara_fallacy)，该谬误警告不要过度使用单一指标来评估复杂问题。一个探索方向是数据增强，即用多个等效方程标注单个词汇问题。未来需要衡量生成方程的合理性、模型对简单扰动的鲁棒性（也许可以通过去噪自编码器实现）以及模型识别词汇问题中重要实体的能力（也许使用基于注意力分析的指标）。Kumar
    et al. ([2021](#bib.bib26)) 已经进行了一项努力，生成了对抗样本并利用其评估 SOTA 模型。
- en: Are the Trained Models Accessible?
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型是否可访问？
- en: Most of the SOTA systems come with their own, well-documented repositories.
    Though an aggregated toolkit Lan et al. ([2021](#bib.bib28)) (open-source MIT
    License) is available, running saved models in inference mode, to probe the quality
    of the datasets, proved to be a hard task, with varying missing hyper-parameters
    or missing saved models. This, however, interestingly suggests that API’s that
    can take a single word problem as input and computes the output, would be highly
    useful for application designers. This has been done in the earlier systems such
    as Roy and Roth ([2018](#bib.bib54)) and Wolfram ([2015](#bib.bib69)).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数SOTA系统都有自己详细记录的代码库。尽管有一个聚合工具包 Lan et al. ([2021](#bib.bib28))（开源MIT许可证）可用，但运行保存的模型进行推理模式，以探测数据集的质量，证明是一项艰巨的任务，存在各种缺失的超参数或缺少的保存模型。然而，这有趣地表明，能够接受单个词汇问题作为输入并计算输出的API，对于应用设计师将非常有用。这在早期的系统中已经实现，例如
    Roy and Roth ([2018](#bib.bib54)) 和 Wolfram ([2015](#bib.bib69))。
- en: Analysis of Benchmark Datasets
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准数据集分析
- en: In this section of the paper, we explore the various dimensions of the popular
    datasets (Table  [4](#Sx6.T4 "Table 4 ‣ Datasets ‣ Why are NLP Models Fumbling
    at Elementary Math? A Survey of Deep Learning based Word Problem Solvers")) with
    a critical and constructive perspective.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文的这一部分，我们以批判性和建设性的视角探讨了流行数据集的各种维度（表 [4](#Sx6.T4 "Table 4 ‣ Datasets ‣ Why
    are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word
    Problem Solvers")）。
- en: Low Resource Setting
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 低资源环境
- en: Compared to usual text related tasks, the available datasets are quite small
    in size. They also suffer from a large lexical overlap Amini et al. ([2019](#bib.bib1)).
    This taxes algorithms, that now have to generalise from an effectively small dataset.
    The fact that the field of word problem solving is niche, where we cannot simply
    lift text from generic sources like Wikipedia, is one of the primary reasons why
    these datasets are small. Language precision is required, while maintaining mathematical
    sense. Hence, language generation is also a hard task.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与通常的文本相关任务相比，可用的数据集规模相当小。它们还存在较大的词汇重叠问题 Amini et al. ([2019](#bib.bib1))。这对算法提出了挑战，因为现在它们必须从一个实际很小的数据集中进行泛化。词汇问题解决领域的冷门性，无法简单地从像维基百科这样的通用来源中提取文本，是这些数据集规模小的主要原因之一。语言精确性是必需的，同时保持数学意义。因此，语言生成也是一个困难的任务。
- en: Annotation Cost
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注释成本
- en: The datasets currently have little to no annotation costs involved as they are
    usually scraped from homework websites. There are some exceptions that involve
    crowd-sourcing Ling et al. ([2017](#bib.bib38)) or intermediate representations
    apart from equations Amini et al. ([2019](#bib.bib1)).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的数据集几乎没有注释成本，因为它们通常是从作业网站抓取的。也有一些例外，涉及到众包Ling等（[2017](#bib.bib38)）或除了方程之外的中间表示Amini等（[2019](#bib.bib1)）。
- en: Template Overlap
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模板重叠
- en: Many studies Zhang et al. ([2020a](#bib.bib78)) have demonstrated that there
    is a high lexical and mathematical overlap between the word problems in popular
    datasets. While lexical overlap is desirable in a principled fashion, as demonstrated
    by Patel et al. ([2021](#bib.bib46)), it often limits the diversity and thus utility
    of the datasets. Consequently, many strategies have been adopted to mitigate such
    issues. Early attempts include controlling linguistic and equation template overlap
    (Koncel-Kedziorski et al. ([2016](#bib.bib25)), Miao et al. ([2020](#bib.bib42))).
    Later ideas revolve around controlled design and quality control of crowd-sourcing
    Amini et al. ([2019](#bib.bib1)).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究Zhang等（[2020a](#bib.bib78)）已经证明，流行数据集中的词题存在较高的词汇和数学重叠。尽管词汇重叠在原则上是可取的，如Patel等（[2021](#bib.bib46)）所示，但它往往限制了数据集的多样性，从而影响其实用性。因此，许多策略已被采纳以缓解这些问题。早期的尝试包括控制语言学和方程模板重叠（Koncel-Kedziorski等（[2016](#bib.bib25)），Miao等（[2020](#bib.bib42)））。后来的想法围绕着众包的控制设计和质量控制（Amini等（[2019](#bib.bib1)））。
- en: Road Ahead
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前路展望
- en: In this section, we describe exciting frontiers of research for word problem
    solving algorithms.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了解决词题算法的令人兴奋的研究前沿。
- en: Semantic Parsing
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义解析
- en: As rightly suggested by Zhang et al. ([2020a](#bib.bib78)), the closest natural
    language task for word problem solving is that of semantic parsing, and not translation
    as most of the deep learning models have modelled. The mapping between extremely
    long chunks of text to short equation sentences has the advantage of generalising
    on the decoder side, but equally has the danger of overloading many involved semantics
    into a simplistic equation model. To illustrate, an equation may be derived after
    applying a sequence of steps that is lost in a simple translation process. A lot
    of efforts have already been employed in adding such nuances in the modelling.
    One way is to model the input intelligently (for e.g., Liang et al. ([2021](#bib.bib35)))
    Here, sophisticated embeddings are learned from BERT based models, using the word
    problem text as a training bed. The intermediate representations include simple
    predicates Roy and Roth ([2018](#bib.bib54)), while others involve a programmatic
    description (Ling et al. ([2017](#bib.bib38)), Amini et al. ([2019](#bib.bib1))).
    Yet another way is to include semantic information in the form of graphs as shown
    in (Huang et al. ([2018](#bib.bib21)), Chiang and Chen ([2019](#bib.bib5)), Qin
    et al. ([2020](#bib.bib51)), Li et al. ([2020](#bib.bib33)), etc.)).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Zhang等（[2020a](#bib.bib78)）正确指出的那样，解决词题的最接近自然语言任务是语义解析，而不是大多数深度学习模型所建模的翻译。将极长的文本片段映射到简短的方程句子的优点是可以在解码器端进行泛化，但同样也有将许多相关语义简化为单一方程模型的危险。例如，方程可能是通过应用一系列步骤推导出来的，而这些步骤在简单的翻译过程中可能会丢失。已经投入了大量的努力来添加这种细微差别。一种方法是智能地建模输入（例如，Liang等（[2021](#bib.bib35)））。在这里，从基于BERT的模型中学习到复杂的嵌入，将词题文本作为训练数据。中间表示包括简单的谓词Roy和Roth（[2018](#bib.bib54)），而其他则涉及程序化描述（Ling等（[2017](#bib.bib38)），Amini等（[2019](#bib.bib1)））。另一种方法是以图的形式包含语义信息，如（Huang等（[2018](#bib.bib21)），Chiang和Chen（[2019](#bib.bib5)），Qin等（[2020](#bib.bib51)），Li等（[2020](#bib.bib33)）等）所示。
- en: Informed Dataset Design
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 知识数据集设计
- en: 'As most datasets are sourced from websites, there is bound to be repetition.
    Efforts invested in modelling things such as the following could help aiding word
    problem research: (a) different versions of the same problem, (b) different equivalent
    equation types, (c) semantics of the language and the math. A step in this direction
    has been explored by Patel et al. ([2021](#bib.bib46)), which provides a challenge
    dataset for evaluating word problems, and Kumar et al. ([2021](#bib.bib26)) where
    adversarial examples are automatically generated.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数数据集来源于网站，不可避免地会出现重复。投入精力建模以下内容可能有助于推进文字问题研究：（a）同一问题的不同版本，（b）不同的等效方程类型，（c）语言和数学的语义。Patel等人（[2021](#bib.bib46)）探讨了朝这个方向迈出的第一步，提供了一个用于评估文字问题的挑战数据集，Kumar等人（[2021](#bib.bib26)）则自动生成了对抗性示例。
- en: Dataset Augmentation
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集增强
- en: A natural extension of dataset design, is dataset augmentation. Augmentation
    is a natural choice when we have datasets that are small and focused on a single
    domain. Then, linguistic and mathematical augmentation can be automated by domain
    experts. While template overlap is a concern in dataset design, it can be leveraged
    in contrastive designs as in Sundaram et al. ([2020](#bib.bib61)); Li et al. ([2021b](#bib.bib34)).
    A principled approach of reversing operators and building equivalent expression
    trees for augmentation has been explored here Liu et al. ([2022](#bib.bib39)).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集设计的自然延伸是数据集增强。增强是当我们有小型且专注于单一领域的数据集时的自然选择。然后，领域专家可以自动化语言和数学增强。虽然数据集设计中的模板重叠是一个问题，但在对比设计中可以利用这种重叠，如Sundaram等人（[2020](#bib.bib61)）；Li等人（[2021b](#bib.bib34)）。刘等人（[2022](#bib.bib39)）在此探索了逆操作和构建等效表达树用于增强的原则方法。
- en: Few Shot Learning
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 少量学习
- en: This is useful if we have a large number of non-annotated word problems or if
    we can come up with complex annotations (that capture semantics) for a small set
    of word problems. In this way few shot learning can generalise from few annotated
    examples.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有大量未标注的文字问题，或者可以为少量文字问题提供复杂的标注（捕捉语义），那么少量学习是有用的。这样，少量学习可以从少量标注示例中泛化。
- en: Knowledge Aware Models
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 知识感知模型
- en: We propose that word problem solving is more involved than even semantic parsing.
    From an intuitive space, we learn language from examples and interactions but
    we need to be explicitly trained in math to solve word problems Marshall ([1996](#bib.bib41)).
    This suggests we need to include mathematical models into our deep learning models
    to build generalisability and robustness. As mentioned before, a common approach
    is to include domain knowledge as a graph Chiang and Chen ([2019](#bib.bib5));
    Wu et al. ([2020](#bib.bib70)); Qin et al. ([2020](#bib.bib51), [2021](#bib.bib50)).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议，解答文字问题比语义解析更复杂。从直观的角度来看，我们通过示例和互动学习语言，但我们需要在数学方面进行明确的训练才能解决文字问题Marshall（[1996](#bib.bib41)）。这表明我们需要将数学模型纳入我们的深度学习模型，以建立泛化能力和鲁棒性。如前所述，一种常见的方法是将领域知识作为图表纳入Chiang和Chen（[2019](#bib.bib5)）；Wu等人（[2020](#bib.bib70)）；Qin等人（[2020](#bib.bib51)，[2021](#bib.bib50)）。
- en: Conclusion
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this paper, we surveyed the existing math word problem solvers, with a focus
    on deep learning models. Deep models are predominantly modeled as encoder-decoder
    models, with input as text and decoder output as equations. We listed several
    interesting formulations of this paradigm - namely Seq2Seq models, graph-based
    models, transformer-based models, contrastive models and teacher-student models.
    In general, graph based models tend to capture complex structural elements that
    can benefit both linguistic and mathematical aspects. We then explored in detail
    the various datasets in use. Subsequently, we analysed the various approaches
    of modelling word problem solving, followed by the characteristics of the popular
    datasets. We saw an overwhelming trend that paying heed to the mathematical modelling
    and tying to the linguistic aspects reaped rich dividends. We concluded that the
    brittleness of the SOTA models was due to: (a) modelling decisions, and (b) dataset
    design. This is intended as a comprehensive survey, but the authors acknowledge
    that there may be methods that have escaped their attention. We also caution that
    the analysis provided could be subjective and opinionated, and there could be
    legitimate disagreements with the perspectives put forward. Finally, we mentioned
    few avenues of further exploration such as the use of semantically rich models,
    informed dataset design and incorporation of domain knowledge.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们对现有的数学问题求解器进行了调查，重点关注深度学习模型。深度模型主要被建模为编码器-解码器模型，其中输入为文本，解码器输出为方程式。我们列举了这种范式的几种有趣的表现形式
    - 即Seq2Seq模型，基于图的模型，基于transformer的模型，对比模型和师生模型。一般来说，基于图的模型往往能够捕捉到有益于语言和数学方面的复杂结构元素。然后，我们详细探讨了各种正在使用的数据集。随后，我们分析了建模单词问题解决的各种方法，以及流行数据集的特点。我们看到，非常重视数学建模并与语言方面联系在一起，会带来丰厚的回报。我们得出结论，SOTA模型的脆弱性是由于：（a）建模决策，以及（b）数据集设计。这是一项综合调查，但作者承认可能有一些方法逃脱了他们的注意。我们还警告说，所提供的分析可能是主观和有意见的，并且可能与提出的观点存在合理的分歧。最后，我们提到了几个进一步探索的途径，比如使用语义丰富的模型，知情数据集设计和融入领域知识。
- en: References
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Amini et al. (2019) Amini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R.,
    Choi, Y., and Hajishirzi, H. (2019). MathQA: Towards interpretable math word problem
    solving with operation-based formalisms. In Proceedings of the 2019 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357–2367,
    Minneapolis, Minnesota. Association for Computational Linguistics.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Amini et al. (2019) Amini, A., Gabriel, S., Lin, S., Koncel-Kedziorski, R.,
    Choi, Y., and Hajishirzi, H. (2019). MathQA: Towards interpretable math word problem
    solving with operation-based formalisms. In Proceedings of the 2019 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357–2367,
    Minneapolis, Minnesota. Association for Computational Linguistics.'
- en: Bobrow (1964) Bobrow, D. G. (1964). A question-answering system for high school
    algebra word problems. In Proceedings of the October 27-29, 1964, fall joint computer
    conference, part I, pages 591–614\. ACM.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bobrow (1964) Bobrow, D. G. (1964). A question-answering system for high school
    algebra word problems. In Proceedings of the October 27-29, 1964, fall joint computer
    conference, part I, pages 591–614\. ACM.
- en: Cao et al. (2021) Cao, Y., Hong, F., Li, H., and Luo, P. (2021). A bottom-up
    dag structure extraction model for math word problems. Proceedings of the AAAI
    Conference on Artificial Intelligence, 35(1):39–46.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao et al. (2021) Cao, Y., Hong, F., Li, H., and Luo, P. (2021). A bottom-up
    dag structure extraction model for math word problems. Proceedings of the AAAI
    Conference on Artificial Intelligence, 35(1):39–46.
- en: 'Chen et al. (2021) Chen, J., Tang, J., Qin, J., Liang, X., Liu, L., Xing, E.,
    and Lin, L. (2021). GeoQA: A geometric question answering benchmark towards multimodal
    numerical reasoning. In Findings of the Association for Computational Linguistics:
    ACL-IJCNLP 2021, pages 513–523, Online. Association for Computational Linguistics.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2021) Chen, J., Tang, J., Qin, J., Liang, X., Liu, L., Xing, E.,
    and Lin, L. (2021). GeoQA: A geometric question answering benchmark towards multimodal
    numerical reasoning. In Findings of the Association for Computational Linguistics:
    ACL-IJCNLP 2021, pages 513–523, Online. Association for Computational Linguistics.'
- en: 'Chiang and Chen (2019) Chiang, T.-R. and Chen, Y.-N. (2019). Semantically-aligned
    equation generation for solving and reasoning math word problems. In Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages
    2656–2668, Minneapolis, Minnesota. Association for Computational Linguistics.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang and Chen (2019) Chiang, T.-R. 和 Chen, Y.-N. (2019). 语义对齐方程生成用于解决和推理数学文字问题。发表于2019年北美计算语言学协会人类语言技术会议：长篇和短篇论文集，第1卷，页码2656–2668，美国明尼苏达州明尼阿波利斯。计算语言学协会。
- en: Cho et al. (2014) Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D.,
    Bougares, F., Schwenk, H., and Bengio, Y. (2014). Learning phrase representations
    using RNN encoder–decoder for statistical machine translation. In Proceedings
    of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    pages 1724–1734, Doha, Qatar. Association for Computational Linguistics.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho et al. (2014) Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D.,
    Bougares, F., Schwenk, H., 和 Bengio, Y. (2014). 使用RNN编码器–解码器学习短语表示用于统计机器翻译。发表于2014年自然语言处理经验方法会议论文集（EMNLP），页码1724–1734，卡塔尔多哈。计算语言学协会。
- en: Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano,
    R., Hesse, C., and Schulman, J. (2021). Training verifiers to solve math word
    problems. CoRR, abs/2110.14168.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano,
    R., Hesse, C., 和 Schulman, J. (2021). 培训验证器以解决数学文字问题。CoRR, abs/2110.14168。
- en: Dellarosa (1986) Dellarosa, D. (1986). A computer simulation of children’s arithmetic
    word-problem solving. Behavior Research Methods, Instruments, & Computers, 18(2):147–154.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dellarosa (1986) Dellarosa, D. (1986). 儿童算术文字问题解决的计算机模拟。行为研究方法、工具和计算机, 18(2):147–154。
- en: 'Devlin et al. (2018) Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2018).
    BERT: pre-training of deep bidirectional transformers for language understanding.
    CoRR, abs/1810.04805.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2018) Devlin, J., Chang, M., Lee, K., 和 Toutanova, K. (2018).
    BERT: 深度双向变换器的预训练用于语言理解。CoRR, abs/1810.04805。'
- en: Dries et al. (2017) Dries, A., Kimmig, A., Davis, J., Belle, V., and De Raedt,
    L. (2017). Solving probability problems in natural language. Proceedings Twenty-Sixth
    International Joint Conference on Artificial Intelligence, pages 3981–3987.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dries et al. (2017) Dries, A., Kimmig, A., Davis, J., Belle, V., 和 De Raedt,
    L. (2017). 解决自然语言中的概率问题。第二十六届国际人工智能联合会议论文集，页码3981–3987。
- en: 'Faldu et al. (2021) Faldu, K., Sheth, A. P., Kikani, P., Gaur, M., and Avasthi,
    A. (2021). Towards tractable mathematical reasoning: Challenges, strategies, and
    opportunities for solving math word problems. CoRR, abs/2111.05364.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faldu et al. (2021) Faldu, K., Sheth, A. P., Kikani, P., Gaur, M., 和 Avasthi,
    A. (2021). 朝向可处理的数学推理：解决数学文字问题的挑战、策略和机遇。CoRR, abs/2111.05364。
- en: 'Feng et al. (2021) Feng, W., Liu, B., Xu, D., Zheng, Q., and Xu, Y. (2021).
    GraphMR: Graph neural network for mathematical reasoning. In Proceedings of the
    2021 Conference on Empirical Methods in Natural Language Processing, pages 3395–3404,
    Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng et al. (2021) Feng, W., Liu, B., Xu, D., Zheng, Q., 和 Xu, Y. (2021). GraphMR:
    用于数学推理的图神经网络。发表于2021年自然语言处理经验方法会议论文集，页码3395–3404，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。'
- en: 'Fletcher (1985) Fletcher, C. R. (1985). Understanding and solving arithmetic
    word problems: A computer simulation. Behavior Research Methods, Instruments,
    & Computers, 17(5):565–571.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fletcher (1985) Fletcher, C. R. (1985). 理解和解决算术文字问题：计算机模拟。行为研究方法、工具和计算机, 17(5):565–571。
- en: Geirhos et al. (2020) Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R.,
    Brendel, W., Bethge, M., and Wichmann, F. A. (2020). Shortcut learning in deep
    neural networks. Nature Machine Intelligence, 2(11):665–673.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geirhos et al. (2020) Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R.,
    Brendel, W., Bethge, M., 和 Wichmann, F. A. (2020). 深度神经网络中的捷径学习。自然机器智能, 2(11):665–673。
- en: Griffith and Kalita (2020) Griffith, K. and Kalita, J. (2020). Solving arithmetic
    word problems using transformer and pre-processing of problem texts. In Proceedings
    of the 17th International Conference on Natural Language Processing (ICON), pages
    76–84, Indian Institute of Technology Patna, Patna, India. NLP Association of
    India (NLPAI).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Griffith and Kalita (2020) Griffith, K. 和 Kalita, J. (2020). 使用变换器和问题文本预处理解决算术文字问题。发表于第17届国际自然语言处理会议（ICON），页码76–84，印度理工学院帕特纳，印度。印度自然语言处理协会（NLPAI）。
- en: Hochreiter and Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. (1997).
    Long short-term memory. Neural Comput., 9(8):1735–1780.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber (1997) Hochreiter, S. 和 Schmidhuber, J. (1997). 长短期记忆。神经计算,
    9(8):1735–1780。
- en: 'Hong et al. (2021) Hong, Y., Li, Q., Ciao, D., Huang, S., and Zhu, S.-C. (2021).
    Learning by fixing: Solving math word problems with weak supervision. Proceedings
    of the AAAI Conference on Artificial Intelligence, 35(6):4959–4967.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等 (2021) Hong, Y., Li, Q., Ciao, D., Huang, S., 和 Zhu, S.-C. (2021). 通过修正进行学习：利用弱监督解决数学文字问题。发表于AAAI人工智能会议论文集，35(6):4959–4967。
- en: Hosseini et al. (2014) Hosseini, M. J., Hajishirzi, H., Etzioni, O., and Kushman,
    N. (2014). Learning to solve arithmetic word problems with verb categorization.
    In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), pages 523–533.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosseini 等 (2014) Hosseini, M. J., Hajishirzi, H., Etzioni, O., 和 Kushman, N.
    (2014). 通过动词分类学习解决算术文字问题。发表于2014年自然语言处理经验方法会议论文集 (EMNLP)，页码 523–533。
- en: Huang et al. (2017) Huang, D., Shi, S., Lin, C.-Y., and Yin, J. (2017). Learning
    fine-grained expressions to solve math word problems. Proceedings of the 2017
    Conference on Empirical Methods in Natural Language Processing, pages 805–814.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2017) Huang, D., Shi, S., Lin, C.-Y., 和 Yin, J. (2017). 学习细粒度表达以解决数学文字问题。发表于2017年自然语言处理经验方法会议论文集，页码
    805–814。
- en: 'Huang et al. (2016) Huang, D., Shi, S., Lin, C.-Y., Yin, J., and Ma, W.-Y.
    (2016). How well do computers solve math word problems? large-scale dataset construction
    and evaluation. In Proceedings of the 54th Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers), pages 887–896, Berlin, Germany.
    Association for Computational Linguistics.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2016) Huang, D., Shi, S., Lin, C.-Y., Yin, J., 和 Ma, W.-Y. (2016).
    计算机解决数学文字问题的效果如何？大规模数据集构建与评估。发表于第54届计算语言学协会年会 (卷1：长篇论文)，页码 887–896，德国柏林。计算语言学协会。
- en: 'Huang et al. (2018) Huang, D., Yao, J.-G., Lin, C.-Y., Zhou, Q., and Yin, J.
    (2018). Using intermediate representations to solve math word problems. In Proceedings
    of the 56th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 419–428, Melbourne, Australia. Association for Computational
    Linguistics.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2018) Huang, D., Yao, J.-G., Lin, C.-Y., Zhou, Q., 和 Yin, J. (2018).
    使用中间表示解决数学文字问题。发表于第56届计算语言学协会年会 (卷1：长篇论文)，页码 419–428，澳大利亚墨尔本。计算语言学协会。
- en: 'Kim et al. (2020) Kim, B., Ki, K. S., Lee, D., and Gweon, G. (2020). Point
    to the Expression: Solving Algebraic Word Problems using the Expression-Pointer
    Transformer Model. In Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing (EMNLP), pages 3768–3779, Online. Association for
    Computational Linguistics.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 (2020) Kim, B., Ki, K. S., Lee, D., 和 Gweon, G. (2020). 指向表达：使用表达指针转换模型解决代数文字问题。发表于2020年自然语言处理经验方法会议论文集
    (EMNLP)，页码 3768–3779，在线。计算语言学协会。
- en: Koch et al. (2015) Koch, G., Zemel, R., and Salakhutdinov, R. (2015). Siamese
    neural networks for one-shot image recognition. In ICML Deep Learning Workshop,
    volume 2.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koch 等 (2015) Koch, G., Zemel, R., 和 Salakhutdinov, R. (2015). 用于一次性图像识别的孪生神经网络。发表于ICML深度学习研讨会，第2卷。
- en: Koncel-Kedziorski et al. (2015) Koncel-Kedziorski, R., Hajishirzi, H., Sabharwal,
    A., Etzioni, O., and Ang, S. D. (2015). Parsing algebraic word problems into equations.
    Transactions of the Association for Computational Linguistics, 3:585–597.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koncel-Kedziorski 等 (2015) Koncel-Kedziorski, R., Hajishirzi, H., Sabharwal,
    A., Etzioni, O., 和 Ang, S. D. (2015). 将代数文字问题解析为方程。计算语言学协会会刊, 3:585–597。
- en: 'Koncel-Kedziorski et al. (2016) Koncel-Kedziorski, R., Roy, S., Amini, A.,
    Kushman, N., and Hajishirzi, H. (2016). Mawps: A math word problem repository.
    In Proceedings of the 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, pages 1152–1157.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koncel-Kedziorski 等 (2016) Koncel-Kedziorski, R., Roy, S., Amini, A., Kushman,
    N., 和 Hajishirzi, H. (2016). Mawps：一个数学文字问题库。发表于2016年北美计算语言学协会会议：人类语言技术论文集，页码
    1152–1157。
- en: 'Kumar et al. (2021) Kumar, V., Maheshwary, R., and Pudi, V. (2021). Adversarial
    examples for evaluating math word problem solvers. In Findings of the Association
    for Computational Linguistics: EMNLP 2021, pages 2705–2712, Punta Cana, Dominican
    Republic. Association for Computational Linguistics.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等 (2021) Kumar, V., Maheshwary, R., 和 Pudi, V. (2021). 用于评估数学文字问题求解器的对抗样本。发表于计算语言学协会：EMNLP
    2021会议论文集，页码 2705–2712，多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Kushman et al. (2014) Kushman, N., Artzi, Y., Zettlemoyer, L., and Barzilay,
    R. (2014). Learning to automatically solve algebra word problems. ACL (1), pages
    271–281.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kushman 等（2014）Kushman, N., Artzi, Y., Zettlemoyer, L., 和 Barzilay, R.（2014）。学习自动解决代数文字问题。ACL
    (1), 页码 271–281。
- en: 'Lan et al. (2021) Lan, Y., Wang, L., Zhang, Q., Lan, Y., Dai, B. T., Wang,
    Y., Zhang, D., and Lim, E.-P. (2021). Mwptoolkit: An open-source framework for
    deep learning-based math word problem solvers. arXiv preprint arXiv:2109.00799.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan 等（2021）Lan, Y., Wang, L., Zhang, Q., Lan, Y., Dai, B. T., Wang, Y., Zhang,
    D., 和 Lim, E.-P.（2021）。Mwptoolkit：一种开源框架，用于基于深度学习的数学文字问题求解器。arXiv 预印本 arXiv:2109.00799。
- en: Le and Mikolov (2014) Le, Q. and Mikolov, T. (2014). Distributed representations
    of sentences and documents. In International conference on machine learning, pages
    1188–1196.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 和 Mikolov（2014）Le, Q. 和 Mikolov, T.（2014）。句子和文档的分布式表示。在国际机器学习会议上，页码 1188–1196。
- en: 'Le-Khac et al. (2020) Le-Khac, P. H., Healy, G., and Smeaton, A. F. (2020).
    Contrastive representation learning: A framework and review. IEEE Access, 8:193907–193934.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le-Khac 等（2020）Le-Khac, P. H., Healy, G., 和 Smeaton, A. F.（2020）。对比表示学习：框架与综述。《IEEE
    Access》，8:193907–193934。
- en: Li et al. (2019) Li, J., Wang, L., Zhang, J., Wang, Y., Dai, B. T., and Zhang,
    D. (2019). Modeling intra-relation in math word problems with different functional
    multi-head attentions. In Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics, pages 6162–6167, Florence, Italy. Association for
    Computational Linguistics.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2019）Li, J., Wang, L., Zhang, J., Wang, Y., Dai, B. T., 和 Zhang, D.（2019）。使用不同功能的多头注意力建模数学文字问题中的内部关系。在《第57届计算语言学协会年会论文集》，页码
    6162–6167，意大利佛罗伦萨。计算语言学协会。
- en: Li et al. (2021a) Li, L., Lin, Y., Ren, S., Li, P., Zhou, J., and Sun, X. (2021a).
    Dynamic knowledge distillation for pre-trained language models. In Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
    379–389, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021a）Li, L., Lin, Y., Ren, S., Li, P., Zhou, J., 和 Sun, X.（2021a）。针对预训练语言模型的动态知识蒸馏。在《2021年自然语言处理实证方法会议论文集》，页码
    379–389，在线及多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Li et al. (2020) Li, S., Wu, L., Feng, S., Xu, F., Xu, F., and Zhong, S. (2020).
    Graph-to-tree neural networks for learning structured input-output translation
    with applications to semantic parsing and math word problem. In Findings of the
    Association for Computational Linguistics: EMNLP 2020, pages 2841–2852, Online.
    Association for Computational Linguistics.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2020）Li, S., Wu, L., Feng, S., Xu, F., Xu, F., 和 Zhong, S.（2020）。图到树神经网络，用于学习结构化的输入输出翻译及其在语义解析和数学文字问题中的应用。在《计算语言学协会发现：EMNLP
    2020》，页码 2841–2852，在线。计算语言学协会。
- en: 'Li et al. (2021b) Li, Z., Zhang, W., Yan, C., Zhou, Q., Li, C., Liu, H., and
    Cao, Y. (2021b). Seeking patterns, not just memorizing procedures: Contrastive
    learning for solving math word problems. CoRR, abs/2110.08464.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021b）Li, Z., Zhang, W., Yan, C., Zhou, Q., Li, C., Liu, H., 和 Cao, Y.（2021b）。寻找模式，而不仅仅是记忆程序：对比学习用于解决数学文字问题。CoRR,
    abs/2110.08464。
- en: 'Liang et al. (2021) Liang, Z., Zhang, J., Shao, J., and Zhang, X. (2021). MWP-BERT:
    A strong baseline for math word problems. CoRR, abs/2107.13435.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等（2021）Liang, Z., Zhang, J., Shao, J., 和 Zhang, X.（2021）。MWP-BERT：数学文字问题的强基准。CoRR,
    abs/2107.13435。
- en: Liang and Zhang (2021) Liang, Z. and Zhang, X. (2021). Solving math word problems
    with teacher supervision. In Zhou, Z.-H., editor, Proceedings of the Thirtieth
    International Joint Conference on Artificial Intelligence, IJCAI-21, pages 3522–3528\.
    International Joint Conferences on Artificial Intelligence Organization. Main
    Track.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 和 Zhang（2021）Liang, Z. 和 Zhang, X.（2021）。在教师监督下解决数学文字问题。在 Zhou, Z.-H.（主编），《第三十届国际联合人工智能会议论文集》，IJCAI-21，页码
    3522–3528。国际联合人工智能会议组织。主会议轨道。
- en: 'Lin et al. (2021) Lin, X., Huang, Z., Zhao, H., Chen, E., Liu, Q., Wang, H.,
    and Wang, S. (2021). Hms: A hierarchical solver with dependency-enhanced understanding
    for math word problem. Proceedings of the AAAI Conference on Artificial Intelligence,
    35(5):4232–4240.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2021）Lin, X., Huang, Z., Zhao, H., Chen, E., Liu, Q., Wang, H., 和 Wang,
    S.（2021）。Hms：一种具有依赖关系增强理解的分层求解器，用于数学文字问题。《AAAI 人工智能会议论文集》，35(5)：4232–4240。
- en: 'Ling et al. (2017) Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. (2017).
    Program induction by rationale generation: Learning to solve and explain algebraic
    word problems. arXiv preprint arXiv:1705.04146.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling 等（2017）Ling, W., Yogatama, D., Dyer, C., 和 Blunsom, P.（2017）。通过理由生成进行程序归纳：学习解决和解释代数文字问题。arXiv
    预印本 arXiv:1705.04146。
- en: 'Liu et al. (2022) Liu, Q., Guan, W., Li, S., Cheng, F., Kawahara, D., and Kurohashi,
    S. (2022). Roda: Reverse operation based data augmentation for solving math word
    problems. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:1–11.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022) Liu, Q., Guan, W., Li, S., Cheng, F., Kawahara, D., and Kurohashi,
    S. (2022). Roda：基于反向操作的数据增强用于解决数学题。IEEE/ACM语音、音频与语言处理交易，30:1–11。
- en: Liu et al. (2019) Liu, Q., Guan, W., Li, S., and Kawahara, D. (2019). Tree-structured
    decoding for solving math word problems. In Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2370–2379,
    Hong Kong, China. Association for Computational Linguistics.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019) Liu, Q., Guan, W., Li, S., 和 Kawahara, D. (2019). 树结构解码用于解决数学题。在2019年自然语言处理实证方法会议和第9届国际自然语言处理联合会议（EMNLP-IJCNLP）上，页码2370–2379，中国香港。计算语言学协会。
- en: Marshall (1996) Marshall, S. P. (1996). Schemas in Problem Solving. Cambridge
    University Press.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marshall (1996) Marshall, S. P. (1996). 问题解决中的图式。剑桥大学出版社。
- en: Miao et al. (2020) Miao, S.-y., Liang, C.-C., and Su, K.-Y. (2020). A diverse
    corpus for evaluating and developing English math word problem solvers. In Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pages
    975–984, Online. Association for Computational Linguistics.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao et al. (2020) Miao, S.-y., Liang, C.-C., and Su, K.-Y. (2020). 用于评估和开发英语数学题解答器的多样化语料库。在第58届计算语言学协会年会上，页码975–984，在线。计算语言学协会。
- en: Mikolov et al. (2013) Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S.,
    and Dean, J. (2013). Distributed representations of words and phrases and their
    compositionality. In Advances in neural information processing systems, pages
    3111–3119.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov et al. (2013) Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S.,
    和 Dean, J. (2013). 词语和短语的分布式表示及其组合性。在神经信息处理系统进展中，页码3111–3119。
- en: 'Mitra and Baral (2016) Mitra, A. and Baral, C. (2016). Learning to use formulas
    to solve simple arithmetic problems. In Proceedings of the 54th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), pages
    2144–2153, Berlin, Germany. Association for Computational Linguistics.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitra and Baral (2016) Mitra, A. 和 Baral, C. (2016). 学习使用公式解决简单算术问题。在第54届计算语言学协会年会（第1卷：长篇论文）上，页码2144–2153，德国柏林。计算语言学协会。
- en: Mukherjee and Garain (2008) Mukherjee, A. and Garain, U. (2008). A review of
    methods for automatic understanding of natural language mathematical problems.
    Artificial Intelligence Review, 29(2):93–122.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukherjee and Garain (2008) Mukherjee, A. 和 Garain, U. (2008). 自动理解自然语言数学问题的方法综述。人工智能评论，29(2):93–122。
- en: 'Patel et al. (2021) Patel, A., Bhattamishra, S., and Goyal, N. (2021). Are
    NLP models really able to solve simple math word problems? In Proceedings of the
    2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, pages 2080–2094, Online. Association
    for Computational Linguistics.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patel et al. (2021) Patel, A., Bhattamishra, S., and Goyal, N. (2021). NLP模型真的能解决简单的数学题吗？在2021年北美计算语言学协会会议：人类语言技术会议上，页码2080–2094，在线。计算语言学协会。
- en: 'Pennington et al. (2014) Pennington, J., Socher, R., and Manning, C. (2014).
    Glove: Global vectors for word representation. In Proceedings of the 2014 conference
    on empirical methods in natural language processing (EMNLP), pages 1532–1543.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pennington et al. (2014) Pennington, J., Socher, R., and Manning, C. (2014).
    Glove：全球词向量表示。第2014年自然语言处理实证方法会议（EMNLP）论文集，页码1532–1543。
- en: Peters et al. (2018) Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,
    C., Lee, K., and Zettlemoyer, L. (2018). Deep contextualized word representations.
    In Proc. of NAACL.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters et al. (2018) Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,
    C., Lee, K., and Zettlemoyer, L. (2018). 深层上下文化词表示。在NAACL会议论文集中。
- en: 'Piękos et al. (2021) Piękos, P., Malinowski, M., and Michalewski, H. (2021).
    Measuring and improving BERT’s mathematical abilities by predicting the order
    of reasoning. In Proceedings of the 59th Annual Meeting of the Association for
    Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 2: Short Papers), pages 383–394, Online. Association
    for Computational Linguistics.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piękos et al. (2021) Piękos, P., Malinowski, M., and Michalewski, H. (2021).
    通过预测推理顺序来测量和提高BERT的数学能力。在第59届计算语言学协会年会和第11届国际自然语言处理联合会议（第2卷：短篇论文）上，页码383–394，在线。计算语言学协会。
- en: 'Qin et al. (2021) Qin, J., Liang, X., Hong, Y., Tang, J., and Lin, L. (2021).
    Neural-symbolic solver for math word problems with auxiliary tasks. In Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers), pages 5870–5881, Online. Association for Computational Linguistics.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin等（2021）Qin, J., Liang, X., Hong, Y., Tang, J., 和 Lin, L. (2021). 具有辅助任务的神经符号求解器用于数学词问题。在第59届计算语言学协会年会和第11届国际自然语言处理联合会议（第1卷：长篇论文）论文集中，第5870–5881页，在线。计算语言学协会。
- en: Qin et al. (2020) Qin, J., Lin, L., Liang, X., Zhang, R., and Lin, L. (2020).
    Semantically-aligned universal tree-structured solver for math word problems.
    In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), pages 3780–3789, Online. Association for Computational Linguistics.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin等（2020）Qin, J., Lin, L., Liang, X., Zhang, R., and Lin, L. (2020). 语义对齐的通用树结构求解器用于数学词问题。在2020年自然语言处理实证方法会议论文集中，第3780–3789页，在线。计算语言学协会。
- en: Roy and Roth (2015) Roy, S. and Roth, D. (2015). Solving general arithmetic
    word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural
    Language Processing, pages 1743–1752, Lisbon, Portugal. Association for Computational
    Linguistics.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy和Roth（2015）Roy, S. 和 Roth, D. (2015). 解决一般算术词问题。在2015年自然语言处理实证方法会议论文集中，第1743–1752页，葡萄牙里斯本。计算语言学协会。
- en: Roy and Roth (2017) Roy, S. and Roth, D. (2017). Unit dependency graph and its
    application to arithmetic word problem solving. In Proceedings of the Thirty-First
    AAAI Conference on Artificial Intelligence, AAAI’17, page 3082–3088\. AAAI Press.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy和Roth（2017）Roy, S. 和 Roth, D. (2017). 单位依赖图及其在算术词问题解决中的应用。在第31届AAAI人工智能会议论文集中，AAAI’17，第3082–3088页。AAAI出版社。
- en: Roy and Roth (2018) Roy, S. and Roth, D. (2018). Mapping to declarative knowledge
    for word problem solving. Transactions of the Association of Computational Linguistics,
    6:159–172.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy和Roth（2018）Roy, S. 和 Roth, D. (2018). 映射到声明性知识以解决词问题。《计算语言学协会会刊》，6:159–172。
- en: Roy et al. (2015) Roy, S., Vieira, T., and Roth, D. (2015). Reasoning about
    quantities in natural language. Transactions of the Association for Computational
    Linguistics, 3:1–13.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy等（2015）Roy, S., Vieira, T., 和 Roth, D. (2015). 处理自然语言中的数量推理。《计算语言学协会会刊》，3:1–13。
- en: 'Seo et al. (2015) Seo, M., Hajishirzi, H., Farhadi, A., Etzioni, O., and Malcolm,
    C. (2015). Solving geometry problems: Combining text and diagram interpretation.
    Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,
    pages 1466–1476.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seo等（2015）Seo, M., Hajishirzi, H., Farhadi, A., Etzioni, O., 和 Malcolm, C. (2015).
    解决几何问题：结合文本和图示解释。2015年自然语言处理实证方法会议论文集，第1466–1476页。
- en: 'Shen et al. (2021) Shen, J., Yin, Y., Li, L., Shang, L., Jiang, X., Zhang,
    M., and Liu, Q. (2021). Generate & rank: A multi-task framework for math word
    problems. In Findings of the Association for Computational Linguistics: EMNLP
    2021, pages 2269–2279, Punta Cana, Dominican Republic. Association for Computational
    Linguistics.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen等（2021）Shen, J., Yin, Y., Li, L., Shang, L., Jiang, X., Zhang, M., 和 Liu,
    Q. (2021). 生成与排序：一个用于数学词问题的多任务框架。在计算语言学协会：EMNLP 2021论文集，第2269–2279页，多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Shen and Jin (2020) Shen, Y. and Jin, C. (2020). Solving math word problems
    with multi-encoders and multi-decoders. In Proceedings of the 28th International
    Conference on Computational Linguistics, pages 2924–2934, Barcelona, Spain (Online).
    International Committee on Computational Linguistics.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen和Jin（2020）Shen, Y. 和 Jin, C. (2020). 使用多编码器和多解码器解决数学词问题。在第28届国际计算语言学会议论文集中，第2924–2934页，西班牙巴塞罗那（在线）。国际计算语言学委员会。
- en: Shi et al. (2015) Shi, S., Wang, Y., Lin, C., Liu, X., and Rui, Y. (2015). Automatically
    solving number word problems by semantic parsing and reasoning. Proceedings of
    the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2015, Lisbon, Portugal, September 17-21, 2015, pages 1132–1142.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi等（2015）Shi, S., Wang, Y., Lin, C., Liu, X., and Rui, Y. (2015). 通过语义解析和推理自动解决数字词问题。2015年自然语言处理实证方法会议论文集，EMNLP
    2015，葡萄牙里斯本，2015年9月17-21日，第1132–1142页。
- en: Sundaram and Abraham (2019) Sundaram, S. S. and Abraham, S. S. (2019). Semantic
    representation for age word problems with schemas. New Generation Computing, 37(4):429–452.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sundaram和Abraham（2019）Sundaram, S. S. 和 Abraham, S. S. (2019). 带有模式的年龄词问题的语义表示。《新一代计算》，37(4):429–452。
- en: Sundaram et al. (2020) Sundaram, S. S., P, D., and Abraham, S. S. (2020). Distributed
    representations for arithmetic word problems. Thirty-Fourth AAAI Conference on
    Artificial Intelligence.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sundaram等（2020）Sundaram, S. S., P, D., 和 Abraham, S. S.（2020）。用于算术文字问题的分布式表示。第34届AAAI人工智能会议。
- en: Suster et al. (2021) Suster, S., Fivez, P., Totis, P., Kimmig, A., Davis, J.,
    de Raedt, L., and Daelemans, W. (2021). Mapping probability word problems to executable
    representations. In Proceedings of the 2021 Conference on Empirical Methods in
    Natural Language Processing, pages 3627–3640, Online and Punta Cana, Dominican
    Republic. Association for Computational Linguistics.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suster等（2021）Suster, S., Fivez, P., Totis, P., Kimmig, A., Davis, J., de Raedt,
    L., 和 Daelemans, W.（2021）。将概率文字问题映射到可执行表示。在2021年自然语言处理实证方法会议论文集，第3627–3640页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Sutskever et al. (2014) Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence
    to sequence learning with neural networks. In Proceedings of the 27th International
    Conference on Neural Information Processing Systems - Volume 2, NIPS’14, page
    3104–3112, Cambridge, MA, USA. MIT Press.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever等（2014）Sutskever, I., Vinyals, O., 和 Le, Q. V.（2014）。使用神经网络进行序列到序列的学习。在第27届国际神经信息处理系统会议论文集
    - 第2卷，NIPS’14，第3104–3112页，剑桥，马萨诸塞州，美国。麻省理工学院出版社。
- en: 'Tsai et al. (2021) Tsai, S.-h., Liang, C.-C., Wang, H.-M., and Su, K.-Y. (2021).
    Sequence to general tree: Knowledge-guided geometry word problem solving. In Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    2: Short Papers), pages 964–972, Online. Association for Computational Linguistics.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai等（2021）Tsai, S.-h., Liang, C.-C., Wang, H.-M., 和 Su, K.-Y.（2021）。序列到通用树：知识指导的几何文字问题解决。在第59届计算语言学协会年会和第11届国际自然语言处理联合会议（第2卷：短篇论文）论文集，第964–972页，在线。计算语言学协会。
- en: 'Upadhyay and Chang (2017) Upadhyay, S. and Chang, M.-W. (2017). Annotating
    derivations: A new evaluation strategy and dataset for algebra word problems.
    In Proceedings of the 15th Conference of the European Chapter of the Association
    for Computational Linguistics: Volume 1, Long Papers, pages 494–504, Valencia,
    Spain. Association for Computational Linguistics.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Upadhyay和Chang（2017）Upadhyay, S. 和 Chang, M.-W.（2017）。注释推导：一种用于代数文字问题的新评价策略和数据集。在第15届计算语言学协会欧洲分会会议论文集：第1卷，长篇论文，第494–504页，西班牙瓦伦西亚。计算语言学协会。
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017). Attention is all you
    need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan,
    S., and Garnett, R., editors, Advances in Neural Information Processing Systems,
    volume 30\. Curran Associates, Inc.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等（2017）Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
    Gomez, A. N., Kaiser, L. u., 和 Polosukhin, I.（2017）。注意力机制就是你所需的一切。在Guyon, I.,
    Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., 和 Garnett,
    R.（编辑），《神经信息处理系统进展》，第30卷。Curran Associates, Inc.
- en: 'Wang et al. (2018) Wang, L., Zhang, D., Gao, L., Song, J., Guo, L., and Shen,
    H. T. (2018). Mathdqn: Solving arithmetic word problems via deep reinforcement
    learning. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1).'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2018）Wang, L., Zhang, D., Gao, L., Song, J., Guo, L., 和 Shen, H. T.（2018）。Mathdqn：通过深度强化学习解决算术文字问题。AAAI人工智能会议论文集，第32卷第1期。
- en: Wang et al. (2017) Wang, Y., Liu, X., and Shi, S. (2017). Deep neural solver
    for math word problems. In Proceedings of the 2017 Conference on Empirical Methods
    in Natural Language Processing, pages 845–854.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2017）Wang, Y., Liu, X., 和 Shi, S.（2017）。用于数学文字问题的深度神经解算器。在2017年自然语言处理实证方法会议论文集，第845–854页。
- en: Wolfram (2015) Wolfram, S. (2015). Wolfram|alpha. On the WWW. URL http://www.
    wolframalpha. com.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolfram（2015）Wolfram, S.（2015）。Wolfram|alpha。在WWW上。网址 http://www.wolframalpha.com。
- en: Wu et al. (2020) Wu, Q., Zhang, Q., Fu, J., and Huang, X. (2020). A knowledge-aware
    sequence-to-tree network for math word problem solving. In Proceedings of the
    2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
    7137–7146, Online. Association for Computational Linguistics.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2020）Wu, Q., Zhang, Q., Fu, J., 和 Huang, X.（2020）。用于数学文字问题解决的知识感知序列到树网络。在2020年自然语言处理实证方法会议（EMNLP）论文集，第7137–7146页，在线。计算语言学协会。
- en: 'Wu et al. (2021a) Wu, Q., Zhang, Q., and Wei, Z. (2021a). An edge-enhanced
    hierarchical graph-to-tree network for math word problem solving. In Findings
    of the Association for Computational Linguistics: EMNLP 2021, pages 1473–1482,
    Punta Cana, Dominican Republic. Association for Computational Linguistics.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2021a）Wu, Q., Zhang, Q., 和 Wei, Z. (2021a). 一种边增强的层次图到树网络用于数学应用题求解。在计算语言学协会的发现：EMNLP
    2021会议论文集中，第1473–1482页，多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Wu et al. (2021b) Wu, Q., Zhang, Q., Wei, Z., and Huang, X. (2021b). Math word
    problem solving with explicit numerical values. In Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages
    5859–5869, Online. Association for Computational Linguistics.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2021b）Wu, Q., Zhang, Q., Wei, Z., 和 Huang, X. (2021b). 具有显式数值的数学应用题求解。在第59届计算语言学协会年会上及第11届国际自然语言处理联合会议（第1卷：长篇论文）论文集中，第5859–5869页，在线。计算语言学协会。
- en: Wu et al. (2021c) Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Yu, P. S.
    (2021c). A comprehensive survey on graph neural networks. IEEE Transactions on
    Neural Networks and Learning Systems, 32(1):4–24.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2021c）Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., 和 Yu, P. S. (2021c).
    图神经网络的综合调查。IEEE神经网络与学习系统汇刊，32(1)：4–24。
- en: Xia et al. (2019) Xia, M., Huang, G., Liu, L., and Shi, S. (2019). Graph based
    translation memory for neural machine translation. In Proceedings of the AAAI
    Conference on Artificial Intelligence, volume 33, pages 7297–7304.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia等（2019）Xia, M., Huang, G., Liu, L., 和 Shi, S. (2019). 基于图的翻译记忆用于神经机器翻译。在AAAI人工智能会议论文集中，第33卷，第7297–7304页。
- en: Xie and Sun (2019) Xie, Z. and Sun, S. (2019). A goal-driven tree-structured
    neural model for math word problems. In Proceedings of the Twenty-Eighth International
    Joint Conference on Artificial Intelligence, IJCAI-19, pages 5299–5305. International
    Joint Conferences on Artificial Intelligence Organization.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie和Sun（2019）Xie, Z. 和 Sun, S. (2019). 一种目标驱动的树结构神经模型用于数学应用题。在第二十八届国际人工智能联合会议论文集中，IJCAI-19，第5299–5305页。国际人工智能联合会议组织。
- en: Yu et al. (2021) Yu, W., Wen, Y., Zheng, F., and Xiao, N. (2021). Improving
    math word problems with pre-trained knowledge and hierarchical reasoning. In Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing, pages
    3384–3394, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等（2021）Yu, W., Wen, Y., Zheng, F., 和 Xiao, N. (2021). 通过预训练知识和层次推理改进数学应用题。在2021年自然语言处理经验方法会议论文集中，第3384–3394页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Zaporojets et al. (2021) Zaporojets, K., Bekoulis, G., Deleu, J., Demeester,
    T., and Develder, C. (2021). Solving arithmetic word problems by scoring equations
    with recursive neural networks. Expert Systems with Applications, 174:114704.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaporojets等（2021）Zaporojets, K., Bekoulis, G., Deleu, J., Demeester, T., 和 Develder,
    C. (2021). 通过递归神经网络评分方程来解决算术应用题。专家系统与应用，174：114704。
- en: 'Zhang et al. (2020a) Zhang, D., Wang, L., Zhang, L., Dai, B. T., and Shen,
    H. T. (2020a). The gap of semantic parsing: A survey on automatic math word problem
    solvers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(9):2287–2305.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2020a）Zhang, D., Wang, L., Zhang, L., Dai, B. T., 和 Shen, H. T. (2020a).
    语义解析的差距：关于自动数学应用题求解器的调查。IEEE模式分析与机器智能汇刊，42(9)：2287–2305。
- en: Zhang et al. (2020b) Zhang, J., Lee, R. K.-W., Lim, E.-P., Qin, W., Wang, L.,
    Shao, J., and Sun, Q. (2020b). Teacher-student networks with multiple decoders
    for solving math word problem. In Bessiere, C., editor, Proceedings of the Twenty-Ninth
    International Joint Conference on Artificial Intelligence, IJCAI-20, pages 4011–4017\.
    International Joint Conferences on Artificial Intelligence Organization. Main
    track.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2020b）Zhang, J., Lee, R. K.-W., Lim, E.-P., Qin, W., Wang, L., Shao,
    J., 和 Sun, Q. (2020b). 带有多个解码器的师生网络用于解决数学应用题。在Bessiere, C.（编辑），第二十九届国际人工智能联合会议论文集中，IJCAI-20，第4011–4017页。国际人工智能联合会议组织。主要分会。
- en: Zhang et al. (2020c) Zhang, J., Wang, L., Lee, R. K.-W., Bin, Y., Wang, Y.,
    Shao, J., and Lim, E.-P. (2020c). Graph-to-tree learning for solving math word
    problems. In Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, pages 3928–3937, Online. Association for Computational Linguistics.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2020c）Zhang, J., Wang, L., Lee, R. K.-W., Bin, Y., Wang, Y., Shao, J.,
    和 Lim, E.-P.（2020c）。图到树学习用于解决数学文字问题。在《第58届计算语言学协会年会论文集》中，页码3928–3937，在线。计算语言学协会。
- en: Zhou et al. (2015) Zhou, L., Dai, S., and Chen, L. (2015). Learn to solve algebra
    word problems using quadratic programming. In Proceedings of the 2015 Conference
    on Empirical Methods in Natural Language Processing, pages 817–822, Lisbon, Portugal.
    Association for Computational Linguistics.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等（2015）Zhou, L., Dai, S., 和 Chen, L.（2015）。利用二次规划解决代数文字问题。在《2015年自然语言处理经验方法会议论文集》中，页码817–822，葡萄牙里斯本。计算语言学协会。
