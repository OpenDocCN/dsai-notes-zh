- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:36:04'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:36:04'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2311.02608] Deep Learning-based 3D Point Cloud Classification: A Systematic
    Survey and Outlook'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2311.02608] 基于深度学习的 3D 点云分类：系统综述与展望'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.02608](https://ar5iv.labs.arxiv.org/html/2311.02608)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.02608](https://ar5iv.labs.arxiv.org/html/2311.02608)
- en: \WarningFilter
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \WarningFilter
- en: latexText page
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: latexText page
- en: 'Deep Learning-based 3D Point Cloud Classification: A Systematic Survey and
    Outlook'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的 3D 点云分类：系统综述与展望
- en: 'Huang Zhang†, Changshuo Wang†, Shengwei Tian*, Baoli Lu, Liping Zhang, Xin
    Ning, Xiao Bai Huang Zhang and Shengwei Tian are with the School of Software,
    Xinjiang University, Xinjiang 830000, China (E-mail: zhhh1998@outlook.com; 357348035@qq.com).Changshuo
    Wang is with the Institute of Semiconductors, Chinese Academy of Sciences, Beijing,
    100083, China, Center of Materials Science and Optoelectronics Engineering $\&amp;$
    School of Microelectronics, Beijing Key Laboratory of Semiconductor Neural Network
    Intelligent Sensing and Computing Technology, Beijing 100083, China, University
    of Chinese Academy of Sciences, Beijing, 100049, China, and Cognitive Computing
    Technology Joint Laboratory, Wave Group, Beijing, 102208, China (E-mail: wangchangshuo@semi.ac.cn).Baoli
    Lu and Liping Zhang are with the Institute of Semiconductors, Chinese Academy
    of Sciences, Beijing, 100083, China, and Beijing Key Laboratory of Semiconductor
    Neural Network Intelligent Sensing and Computing Technology, Beijing, 100083,
    China (E-mail: lubaoli@semi.ac.cn; zliping@semi.ac.cn).Xin Ning is with the Institute
    of Semiconductors, Chinese Academy of Sciences, Beijing, 100083, China, and Cognitive
    Computing Technology Joint Laboratory, Wave Group, Beijing, 102208, China (E-mail:
    ningxin@semi.ac.cn).Xiao Bai is with the School of Computer Science and Engineering,
    Beihang University, Beijing, 100083, China (E-mail: baixiao@buaa.edu.cn).†The
    author contributed equally to this work and should be considered co-first author.*Shengwei
    Tian is the corresponding authors.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 黄章†、常硕王†、盛伟田*、宝力·卢、李平·张、新宁、肖白 黄章和盛伟田均来自中国新疆大学软件学院，新疆 830000，中国（电子邮件：zhhh1998@outlook.com；357348035@qq.com）。常硕王就职于中国科学院半导体研究所，北京
    100083，中国，材料科学与光电子工程中心 $\&amp;$ 微电子学院，北京半导体神经网络智能感知与计算技术重点实验室，北京 100083，中国，中国科学院大学，北京
    100049，中国，以及认知计算技术联合实验室，Wave Group，北京 102208，中国（电子邮件：wangchangshuo@semi.ac.cn）。宝力·卢和李平·张均来自中国科学院半导体研究所，北京
    100083，中国，以及北京半导体神经网络智能感知与计算技术重点实验室，北京 100083，中国（电子邮件：lubaoli@semi.ac.cn；zliping@semi.ac.cn）。新宁来自中国科学院半导体研究所，北京
    100083，中国，以及认知计算技术联合实验室，Wave Group，北京 102208，中国（电子邮件：ningxin@semi.ac.cn）。肖白就职于北京航空航天大学计算机科学与工程学院，北京
    100083，中国（电子邮件：baixiao@buaa.edu.cn）。†这些作者对本工作做出了同等贡献，应视为共同第一作者。*盛伟田为通讯作者。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent years, point cloud representation has become one of the research hotspots
    in the field of computer vision, and has been widely used in many fields, such
    as autonomous driving, virtual reality, robotics, etc. Although deep learning
    techniques have achieved great success in processing regular structured 2D grid
    image data, there are still great challenges in processing irregular, unstructured
    point cloud data. Point cloud classification is the basis of point cloud analysis,
    and many deep learning-based methods have been widely used in this task. Therefore,
    the purpose of this paper is to provide researchers in this field with the latest
    research progress and future trends. First, we introduce point cloud acquisition,
    characteristics, and challenges. Second, we review 3D data representations, storage
    formats, and commonly used datasets for point cloud classification. We then summarize
    deep learning-based methods for point cloud classification and complement recent
    research work. Next, we compare and analyze the performance of the main methods.
    Finally, we discuss some challenges and future directions for point cloud classification.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，点云表示已成为计算机视觉领域的研究热点之一，并在许多领域得到广泛应用，如自动驾驶、虚拟现实、机器人等。虽然深度学习技术在处理规则结构的二维网格图像数据方面取得了巨大成功，但在处理不规则、无结构的点云数据时仍面临巨大挑战。点云分类是点云分析的基础，许多基于深度学习的方法已广泛应用于这一任务。因此，本文旨在为该领域的研究人员提供最新的研究进展和未来趋势。首先，我们介绍点云获取、特征和挑战。其次，我们回顾三维数据表示、存储格式以及点云分类的常用数据集。然后，我们总结基于深度学习的点云分类方法，并补充最近的研究工作。接着，我们对主要方法的性能进行比较和分析。最后，我们讨论一些点云分类的挑战和未来方向。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: In recent computer vision field, the processing technology of two-dimensional
    images is close to maturity [[1](#bib.bib1)] [[2](#bib.bib2)] [[3](#bib.bib3)]
    [[4](#bib.bib4)], and many researchers have shifted their research focus to three-dimensional
    scenes that are more in line with the real world. In a 3D scene, point cloud [[5](#bib.bib5)]
    plays an important role in representing the 3D scene because of its rich expression
    information. Therefore, point cloud has become a common form of data expression
    in the study of 3D vision. As technology advances, the acquisition of point cloud
    data is becoming increasingly
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的计算机视觉领域，二维图像的处理技术接近成熟 [[1](#bib.bib1)] [[2](#bib.bib2)] [[3](#bib.bib3)]
    [[4](#bib.bib4)]，许多研究人员已经将研究重点转移到更贴近现实世界的三维场景上。在三维场景中，点云 [[5](#bib.bib5)] 在表示三维场景方面扮演了重要角色，因为它具有丰富的表达信息。因此，点云已经成为研究三维视觉中的一种常见数据表达形式。随着技术的发展，点云数据的获取变得越来越
- en: '![Refer to caption](img/5df7790fa9a982c96945ff58567ea0a8.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5df7790fa9a982c96945ff58567ea0a8.png)'
- en: 'Figure 1: 3D data representation'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：3D 数据表示
- en: 'intelligent and convenient, and there are many acquisition methods, such as:
    LIDAR laser detection, point cloud acquisition through 3D model calculation, point
    cloud acquisition through 3D reconstruction through 2D images [[6](#bib.bib6)],
    etc.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 智能且方便，并且有多种获取方法，如：LIDAR 激光探测，通过 3D 模型计算获取点云，通过 2D 图像 3D 重建获取点云 [[6](#bib.bib6)]
    等。
- en: As the most basic point cloud analysis task, point cloud classification has
    been widely used in many fields such as security detection [[7](#bib.bib7)] [[8](#bib.bib8)],
    target object detection [[9](#bib.bib9)] [[10](#bib.bib10)], medicine [[11](#bib.bib11)]
    [[12](#bib.bib12)], and three-dimensional reconstruction [[13](#bib.bib13)] [[14](#bib.bib14)].
    The purpose of point cloud classification is to equip each point in the point
    cloud with a marker to identify the overall or part properties of the point cloud.
    Since the component attributes of point clouds belong to the category of point
    cloud segmentation, in this paper, we mainly focus on the overall attributes of
    point clouds, namely point cloud classification.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最基础的点云分析任务，点云分类已广泛应用于许多领域，如安全检测 [[7](#bib.bib7)] [[8](#bib.bib8)]、目标物体检测 [[9](#bib.bib9)]
    [[10](#bib.bib10)]、医学 [[11](#bib.bib11)] [[12](#bib.bib12)] 和三维重建 [[13](#bib.bib13)]
    [[14](#bib.bib14)]。点云分类的目的是为点云中的每个点配备标记，以识别点云的整体或部分属性。由于点云的组件属性属于点云分割的范畴，本文主要关注点云的整体属性，即点云分类。
- en: 'As shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning-based
    3D Point Cloud Classification: A Systematic Survey and Outlook"), 3D data comes
    in a variety of representations. Currently, it is possible to convert point clouds
    into mesh, voxel, or multi-view data to learn 3D object representation through
    indirect methods, but these methods are prone to problems such as loss of 3D geometric
    information of objects or excessive memory consumption. Before PointNet, due to
    the disorder and irregularity of point cloud, deep learning technology cannot
    directly process point cloud. Early point cloud processing used hand-designed
    rules for feature extraction, and then used machine learning-based classifiers
    (such as Support Vector Machines (SVM) [[15](#bib.bib15)], AdaBoost [[16](#bib.bib16)],
    Random Forest (RF) [[17](#bib.bib17)], etc.) to predict the class label of the
    point cloud, but these Class methods have poor adaptive ability and are prone
    to noise [[18](#bib.bib18)]. Some researches solved the noise problem by synthesizing
    context information, such as Conditional Random Field (CRF) [[19](#bib.bib19)],
    Markov Random Field (MRF) [[20](#bib.bib20)], etc., which improved the classification
    performance to a certain extent. However, the feature expression ability of hand-designed
    rule extraction is limited, especially in complex scenes, the accuracy and generalization
    ability of the model cannot meet the requirements of human beings, and this method
    relies heavily on researchers with professional knowledge and experience.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning-based 3D Point Cloud
    Classification: A Systematic Survey and Outlook")所示，3D 数据有多种表示方式。目前，可以将点云转换为网格、体素或多视角数据，通过间接方法学习
    3D 对象表示，但这些方法容易出现如对象的 3D 几何信息丢失或过度消耗内存等问题。在 PointNet 之前，由于点云的无序和不规则性，深度学习技术无法直接处理点云。早期点云处理使用手工设计的规则进行特征提取，然后使用基于机器学习的分类器（如支持向量机（SVM）[[15](#bib.bib15)]，AdaBoost
    [[16](#bib.bib16)]，随机森林（RF）[[17](#bib.bib17)] 等）来预测点云的类别标签，但这些分类方法适应性差，容易受到噪声[[18](#bib.bib18)]的影响。一些研究通过合成上下文信息来解决噪声问题，如条件随机场（CRF）[[19](#bib.bib19)]，马尔可夫随机场（MRF）[[20](#bib.bib20)]等，这在一定程度上提高了分类性能。然而，手工设计规则提取的特征表达能力有限，尤其是在复杂场景中，模型的准确性和泛化能力无法满足人类的要求，并且该方法对具有专业知识和经验的研究人员依赖较大。'
- en: With the rapid development of computer computing and data processing capabilities,
    the application of deep learning technology in point cloud analysis has also been
    promoted. The paper published by Charles et al. [[21](#bib.bib21)] of Stanford
    University in 2017 proposes a deep learning network, PointNet, that directly processes
    point clouds. This paper is a landmark, and methods for directly processing point
    clouds gradually dominate.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算机计算和数据处理能力的快速发展，深度学习技术在点云分析中的应用也得到了促进。斯坦福大学的 Charles 等人 [[21](#bib.bib21)]
    于 2017 年发表的论文提出了一个直接处理点云的深度学习网络 PointNet。这篇论文具有里程碑式的意义，直接处理点云的方法逐渐占据主导地位。
- en: 'Faced with the irregularity, disorder, and sparsity of 3D point clouds, point
    cloud classification is still a challenging problem. There are currently some
    reviews that analyze and summarize deep learning-based 3D point cloud classification
    methods. This paper improves on the previous work and adds new deep learning-based
    3D point cloud classification methods, such as the recently popular transformer-based
    method. Finally, the future research direction of 3D point cloud classification
    technology is prospected. The overall structure of the article is shown in Fig. [2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ Deep Learning-based 3D Point Cloud Classification:
    A Systematic Survey and Outlook").'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '面对 3D 点云的无规则性、无序性和稀疏性，点云分类仍然是一个具有挑战性的问题。目前已有一些综述分析和总结了基于深度学习的 3D 点云分类方法。本文在前人工作的基础上进行了改进，并加入了新的基于深度学习的
    3D 点云分类方法，如最近流行的基于变换器的方法。最后，对 3D 点云分类技术的未来研究方向进行了展望。文章的整体结构如图[2](#S1.F2 "Figure
    2 ‣ I Introduction ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic
    Survey and Outlook")所示。'
- en: '![Refer to caption](img/2a29297e5096f542dc14c046bd75ee3a.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/2a29297e5096f542dc14c046bd75ee3a.png)'
- en: 'Figure 2: Overall structure of the article. First, the point cloud is used
    as input, and the point cloud can be transformed by voxel or multi-view. Secondly,
    features are extracted from the original point cloud, the transformed voxels or
    multi-views. The final output is the probability value of each class.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：文章的总体结构。首先，点云作为输入，点云可以通过体素或多视角进行转换。其次，从原始点云、转换后的体素或多视角中提取特征。最终输出是每个类别的概率值。
- en: 'Specifically, the main contributions of our work are as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们工作的主要贡献如下：
- en: $\bullet$ We first give a detailed introduction to the 3D data and make a deeper
    interpretation of the point cloud for the reader’s understanding, and then give
    the datasets used for point cloud classification and their acquisition methods.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 我们首先详细介绍了3D数据，并对点云进行了更深入的解释，以帮助读者理解，然后提供了用于点云分类的数据集及其获取方法。
- en: $\bullet$ We summarize recently published research on point cloud classification
    reviews, building on which to complement state-of-the-art research methods. These
    methods are classified into four categories according to their characteristics,
    including multi-view-based, voxel-based, point-cloud-based methods, and polymorphic
    fusion-based methods. And then the point-cloud-based methods are subdivided.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 我们总结了最近发表的点云分类评论研究，基于这些研究来补充最先进的研究方法。这些方法根据其特征被分类为四类，包括多视角基方法、体素基方法、点云基方法和多态融合基方法。然后对点云基方法进行细分。
- en: $\bullet$ We discuss the advantages and limitations of subcategories of methods
    based on their classification. This classification is more suitable for researchers
    to explore these methods on actual needs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 我们讨论了基于方法分类的子类别的优点和局限性。这种分类更适合研究人员根据实际需求探索这些方法。
- en: $\bullet$ We give the evaluation metrics and performance comparisons of the
    methods to better demonstrate the performance of various methods on the dataset,
    and then analyze some current challenges and future trends in this field.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 我们提供了评估指标和方法的性能比较，以更好地展示各种方法在数据集上的表现，然后分析该领域当前的一些挑战和未来的发展趋势。
- en: II 3D data
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 3D数据
- en: II-A 3D data representation
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 3D数据表示
- en: There are various representations of 3D data [[22](#bib.bib22)], such as point
    cloud, mesh, and voxel. Here we introduce them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 3D数据的表示形式有多种[[22](#bib.bib22)]，如点云、网格和体素。这里我们介绍它们。
- en: 'Point cloud: A point cloud is essentially a large collection of tiny points
    drawn in 3D space, as shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep
    Learning-based 3D Point Cloud Classification: A Systematic Survey and Outlook")(a),
    which consists of a large collection of points captured using a 3D laser scanner.
    These points can express the spatial distribution and surface characteristics
    of the target. Each point in the point cloud contains rich information, such as:
    three-dimensional coordinates (x, y, z), color information (r, g, b) and surface
    normal vector, etc.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '点云：点云本质上是一个在3D空间中绘制的大量微小点的集合，如图 [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep
    Learning-based 3D Point Cloud Classification: A Systematic Survey and Outlook")(a)所示，它由使用3D激光扫描仪捕获的大量点组成。这些点可以表达目标的空间分布和表面特征。点云中的每个点都包含丰富的信息，如：三维坐标（x,
    y, z）、颜色信息（r, g, b）和表面法线向量等。'
- en: 'Mesh: 3D data can also be represented by a mesh grid, which can be viewed as
    a collection of points that build local relationships between points. Triangular
    mesh, also known as triangular patch (as shown in Fig. [1](#S1.F1 "Figure 1 ‣
    I Introduction ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic
    Survey and Outlook")(b)), is one of the commonly used mesh grids to describe 3D
    objects. A collection of points and edges of a slice is called a mesh.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '网格：3D数据也可以通过网格表示，网格可以看作是一个点的集合，这些点在局部之间建立了关系。三角网格，也称为三角形贴图（如图 [1](#S1.F1 "Figure
    1 ‣ I Introduction ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic
    Survey and Outlook")(b)所示），是描述3D物体的常用网格之一。一片点和边的集合被称为网格。'
- en: 'Voxel: In 3D object representation, voxels are also an important form of 3D
    data representation, as shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣
    Deep Learning-based 3D Point Cloud Classification: A Systematic Survey and Outlook")(c),
    voxels are good at representing non-uniformly filled regularly sampled spaces,
    therefore, voxels can effectively represent point cloud data with a lot of empty
    or evenly filled spaces. By voxelizing the point cloud data, it is beneficial
    to increase the data computing efficiency and reduce the access to random memory,
    but the voxelization of the point cloud data will inevitably bring a certain degree
    of information loss.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'Voxel: 在3D对象表示中，体素也是一种重要的3D数据表示形式，如图 [1](#S1.F1 "图 1 ‣ I 引言 ‣ 基于深度学习的3D点云分类：系统评估与展望")(c)所示，体素擅长表示非均匀填充的规则采样空间，因此，体素可以有效地表示具有大量空白或均匀填充空间的点云数据。通过对点云数据进行体素化，可以提高数据计算效率并减少对随机内存的访问，但点云数据的体素化不可避免地会带来一定程度的信息丢失。'
- en: 'Multi-view: A multi-view image (as shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic Survey and Outlook")(d))
    is also a representation of point cloud data, which is derived from a single-view
    image, and is an image that renders a 3D object into multiple viewpoints at a
    specific angle. The challenges are mainly the choice of perspective and perspective
    fusion.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'Multi-view: 多视角图像（如图 [1](#S1.F1 "图 1 ‣ I 引言 ‣ 基于深度学习的3D点云分类：系统评估与展望")(d)所示）也是点云数据的一种表示方式，它源自单视角图像，是将3D对象在特定角度渲染成多个视点的图像。主要挑战是透视选择和透视融合。'
- en: II-B Point cloud data storage format
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 点云数据存储格式
- en: 'There are hundreds of 3D file formats available for point clouds, and different
    scanners produce raw data in many formats. The biggest difference between point
    cloud data files is the use of ASCII and binary. Binary systems store data directly
    in binary code. Common point cloud binary formats include FLS, PCD, LAS, etc.
    Several other common file types can support both ASCII and binary formats. These
    include PLY, FBX. The E57 stores data in both binary and ASCII formats, combining
    many of the advantages of both in one file type. Below we introduce some commonly
    used point cloud data storage formats:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有数百种3D文件格式可用于点云，不同的扫描仪产生多种格式的原始数据。点云数据文件的最大区别在于使用ASCII和二进制。二进制系统直接以二进制代码存储数据。常见的点云二进制格式包括FLS、PCD、LAS等。其他一些常见文件类型可以支持ASCII和二进制格式。这些包括PLY、FBX。E57以二进制和ASCII格式存储数据，将两者的许多优点结合在一个文件类型中。下面我们介绍一些常用的点云数据存储格式：
- en: 'Obj: The point cloud file in obj format is developed by Wavefront Technologies.
    It is a text file. It is a simple data format that only represents the geometry,
    normal, color, and texture information of the 3D data. This format is usually
    represented in ASCII, but there are also proprietary obj binary versions.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 'Obj: obj格式的点云文件由Wavefront Technologies开发。这是一种文本文件。这是一种简单的数据格式，仅表示3D数据的几何形状、法线、颜色和纹理信息。该格式通常以ASCII表示，但也有专有的obj二进制版本。'
- en: 'Las: The las format is mainly used to store LIDAR point cloud data, which is
    essentially a binary format file. A LAS file consists of three parts: the header
    file area (including total number of points, data range, dimension information
    of each point), variable-length record area (including coordinate system, extra
    dimensions, etc.), and point set record area(including point coordinate information,
    R, G, B information, classification information, intensity information, etc.),
    the las format takes into account the characteristics of LIDAR data, the structure
    is reasonable, and it is easy to expand.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 'Las: las格式主要用于存储LIDAR点云数据，本质上是一种二进制格式文件。LAS文件由三部分组成：头文件区域（包括点的总数、数据范围、每个点的维度信息）、可变长度记录区域（包括坐标系统、额外维度等）和点集记录区域（包括点坐标信息、R、G、B信息、分类信息、强度信息等），las格式考虑了LIDAR数据的特点，结构合理，易于扩展。'
- en: 'Ply: The full name of ply is Polygon File Format, which is inspired by obj
    and is specially used to store 3D data. Ply uses a nominally flat list of polygons
    to represent objects. It can store information including color, transparency,
    surface normal vector, texture coordinates and data confidence, and can set different
    properties for the front and back sides of the polygon. There are two versions
    of this file, an ASCII version, and a binary version.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Ply：Ply 的全名是多边形文件格式（Polygon File Format），其灵感来源于 obj，专门用于存储 3D 数据。Ply 使用一个名义上平坦的多边形列表来表示对象。它可以存储包括颜色、透明度、表面法向量、纹理坐标和数据置信度在内的信息，并可以为多边形的正反两面设置不同的属性。该文件有两个版本：ASCII
    版本和二进制版本。
- en: 'E57: E57 is a vendor neutral file format for point cloud storage. It can also
    be used to store image and metadata information generated by laser scanners and
    other 3D imaging systems and is a strict format using fixed-size fields and records.
    It saves data using ASCII and binary codes and provides most of the accessibility
    of ASCII and the speed of binary, and it can store 3D point cloud data, attributes,
    images.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: E57：E57 是一种供应商中立的点云存储文件格式。它也可以用于存储由激光扫描仪和其他 3D 成像系统生成的图像和元数据，并且是一个严格使用固定大小字段和记录的格式。它使用
    ASCII 和二进制代码保存数据，并提供了 ASCII 的大部分可访问性和二进制的速度，它可以存储 3D 点云数据、属性和图像。
- en: 'PCD: PCD is the official designated format of Point Cloud Library. It consists
    of two parts: header file and point cloud data. It is used to describe the overall
    information of point cloud. It has two data storage types, ASCII, and binary,
    but the header file of PCD file must use ASCII. Encoding, a nice benefit of PCD
    is that it adapts well to PCL, resulting in the highest performance compared to
    PCL applications.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: PCD：PCD 是点云库（Point Cloud Library）的官方指定格式。它由两个部分组成：头文件和点云数据。它用于描述点云的整体信息。它有两种数据存储类型：ASCII
    和二进制，但 PCD 文件的头文件必须使用 ASCII 编码。PCD 的一个好处是它能够很好地适应 PCL，相比 PCL 应用程序，性能最高。
- en: II-C 3D point cloud public datasets
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 3D 点云公共数据集
- en: 'Today, there are many point cloud datasets provided by industries and universities.
    The performance of different methods on these datasets reflects the reliability
    and accuracy of the methods. These datasets consist of virtual or real scenes,
    which can provide ground-truth labels for training the network. In this section,
    we will introduce some commonly used point cloud classification datasets, and
    the division of each dataset is shown in Table. [I](#S2.T1 "TABLE I ‣ II-C 3D
    point cloud public datasets ‣ II 3D data ‣ Deep Learning-based 3D Point Cloud
    Classification: A Systematic Survey and Outlook").'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '目前，许多工业界和学术界提供了点云数据集。这些数据集上的不同方法的性能反映了这些方法的可靠性和准确性。这些数据集包含虚拟或真实场景，可以为网络训练提供真实标签。在本节中，我们将介绍一些常用的点云分类数据集，每个数据集的划分见表。[I](#S2.T1
    "TABLE I ‣ II-C 3D point cloud public datasets ‣ II 3D data ‣ Deep Learning-based
    3D Point Cloud Classification: A Systematic Survey and Outlook")。'
- en: 'ModelNet40 [[23](#bib.bib23)]: The dataset was developed by the Vision and
    Robotics Laboratory at Princeton University. The ModelNet40 dataset consists of
    synthetic CAD objects. As the most widely used benchmark for point cloud analysis,
    ModelNet40 is popular for its diverse categories, clear shapes, and well-structured
    datasets. The dataset consists of objects of 40 categories (e.g. airplane, car,
    plant, lamp), of which 9843 are used for training and 2468 are used for testing.
    The corresponding points are uniformly sampled from the mesh surface and then
    further preprocessed by moving to the origin and scaling to a unit sphere. Download
    link: [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ModelNet40 [[23](#bib.bib23)]：该数据集由普林斯顿大学的视觉与机器人实验室开发。ModelNet40 数据集由合成的 CAD
    对象组成。作为点云分析中最广泛使用的基准，ModelNet40 因其多样的类别、清晰的形状和结构良好的数据集而受到欢迎。数据集包含 40 个类别的对象（例如飞机、汽车、植物、灯具），其中
    9843 个用于训练，2468 个用于测试。对应的点从网格表面均匀采样，然后进一步预处理，包括移动到原点和缩放到单位球体。下载链接：[https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/)
- en: 'ModelNet-C [[24](#bib.bib24)]: The ModelNet-C set contains 185,000 different
    point clouds and was created based on the ModelNet40 validation set. This dataset
    is mainly used to benchmark damage robustness for 3D point cloud recognition,
    with 15 damage types and 5 severity levels for each damage type, such as noise,
    density, etc. Helps to understand the robustness of the model. Download link:
    [https://sites.google.com/umich.edu/modelnet40c](https://sites.google.com/umich.edu/modelnet40c)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 'ModelNet-C [[24](#bib.bib24)]: ModelNet-C 集合包含 185,000 个不同的点云，基于 ModelNet40
    验证集创建。该数据集主要用于基准测试 3D 点云识别的损坏鲁棒性，每种损坏类型有 15 种损坏类型和 5 个严重程度等级，如噪声、密度等。帮助理解模型的鲁棒性。下载链接:
    [https://sites.google.com/umich.edu/modelnet40c](https://sites.google.com/umich.edu/modelnet40c)'
- en: 'ModelNet10 [[23](#bib.bib23)]: ModelNet10: ModelNet10 is a subset of ModelNet40,
    the dataset contains only 10 classes and it is divided into 3991 training and
    908 testing shapes. Download link: [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'ModelNet10 [[23](#bib.bib23)]: ModelNet10 是 ModelNet40 的一个子集，该数据集仅包含 10 个类别，并分为
    3991 个训练样本和 908 个测试样本。下载链接: [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/)'
- en: 'Sydney Urban Objects [[25](#bib.bib25)]: The dataset, collected in Sydney CBD,
    contains a variety of common urban road objects, including 631 scanned objects
    in the categories of vehicles, pedestrians, signs, and trees. Download link: [https://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml](https://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 'Sydney Urban Objects [[25](#bib.bib25)]: 该数据集收集自悉尼中央商务区，包含各种常见的城市道路物体，包括 631
    个扫描物体，类别涵盖车辆、行人、标志和树木。下载链接: [https://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml](https://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml)'
- en: 'ShapeNet [[26](#bib.bib26)]: ShapeNet is a large repository of 3D CAD models
    developed by researchers at Stanford University, Princeton University, and the
    Toyota Institute of Technology in Chicago, USA. The repository contains over 300
    million models, of which 220,000 models are classified into 3,135 classes arranged
    using WordNet hypernym-hyponymy relations. ShapeNetCore is a subset of ShapeNet
    that includes nearly 51,300 unique 3D models. It provides 55 common object categories
    and annotations. ShapeNetSem is also a subset of ShapeNet, which contains 12,000
    models. It is smaller in scale but has a wider coverage, including 270 categories.
    Download link: [https://shapenet.org/](https://shapenet.org/)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 'ShapeNet [[26](#bib.bib26)]: ShapeNet 是由斯坦福大学、普林斯顿大学和美国芝加哥丰田技术研究所的研究人员开发的大型
    3D CAD 模型库。该库包含超过 3 亿个模型，其中 220,000 个模型被分类为 3,135 个类别，按照 WordNet 超类-子类关系排列。ShapeNetCore
    是 ShapeNet 的一个子集，包含近 51,300 个独特的 3D 模型。它提供了 55 个常见物体类别和注释。ShapeNetSem 也是 ShapeNet
    的一个子集，包含 12,000 个模型。它规模较小但覆盖面更广，包括 270 个类别。下载链接: [https://shapenet.org/](https://shapenet.org/)'
- en: 'ScanNet [[27](#bib.bib27)]: ScanNet is an instance-level indoor RGB-D dataset
    containing 2D and 3D data. It is a collection of labeled voxels, not points or
    objects. As of now, ScanNet v2, the latest version of ScanNet, has collected 1513
    annotated scans with about 90$\%$ surface coverage. In the semantic segmentation
    task, this dataset is labeled with 20 classes of annotated 3D voxelized objects.
    Download link: [http://www.scan-net.org/](http://www.scan-net.org/)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 'ScanNet [[27](#bib.bib27)]: ScanNet 是一个实例级别的室内 RGB-D 数据集，包含 2D 和 3D 数据。它是标注体素的集合，而非点或物体。目前最新版本的
    ScanNet v2 已收集了 1513 个带注释的扫描图，表面覆盖率约为 90$\%$。在语义分割任务中，该数据集标注了 20 个类别的 3D 体素对象。下载链接:
    [http://www.scan-net.org/](http://www.scan-net.org/)'
- en: 'ScanObjectNN [[28](#bib.bib28)]: ScanObjectNN is a real-world dataset consisting
    of 2902 3D objects divided into 15 categories, which is a challenging point cloud
    classification dataset due to background, missing parts, and deformations in the
    dataset. Download link: [https://hkust-vgd.github.io/scanobjectnn/](https://hkust-vgd.github.io/scanobjectnn/).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'ScanObjectNN [[28](#bib.bib28)]: ScanObjectNN 是一个由 2902 个 3D 对象组成的现实世界数据集，分为
    15 个类别。由于数据集中的背景、缺失部分和变形，它是一个具有挑战性的点云分类数据集。下载链接: [https://hkust-vgd.github.io/scanobjectnn/](https://hkust-vgd.github.io/scanobjectnn/)'
- en: 'TABLE I: Point cloud classification datasets'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 点云分类数据集'
- en: '| Dataset | Year | Samples | Classes | Training | Test | Type | Form |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | 样本数量 | 类别 | 训练 | 测试 | 类型 | 形式 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| ModelNet40[[23](#bib.bib23)] | 2015 | 12311 | 40 | 9843 | 2468 | Synthetic
    | Mesh |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| ModelNet40[[23](#bib.bib23)] | 2015 | 12311 | 40 | 9843 | 2468 | 合成 | 网格
    |'
- en: '| ModelNet40-C[[24](#bib.bib24)] | 2015 | 185000 | 15 | - | - | Synthetic |
    Point Cloud |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ModelNet40-C[[24](#bib.bib24)] | 2015 | 185000 | 15 | - | - | 合成 | 点云 |'
- en: '| ModelNet10[[23](#bib.bib23)] | 2015 | 4899 | 10 | 3991 | 605 | Synthetic
    | Mesh |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| ModelNet10[[23](#bib.bib23)] | 2015 | 4899 | 10 | 3991 | 605 | 合成 | 网格 |'
- en: '| SyDney Urban Objects[[25](#bib.bib25)] | 2013 | 588 | 14 | - | - | Real Word
    | Point Cloud |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| SyDney Urban Objects[[25](#bib.bib25)] | 2013 | 588 | 14 | - | - | 真实世界 |
    点云 |'
- en: '| ShapeNet[[26](#bib.bib26)] | 2015 | 51190 | 55 | - | - | Synthetic | Mesh
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ShapeNet[[26](#bib.bib26)] | 2015 | 51190 | 55 | - | - | 合成 | 网格 |'
- en: '| ScanNet[[27](#bib.bib27)] | 2017 | 12283 | 17 | 9677 | 2606 | Real Word |
    RGB-D |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| ScanNet[[27](#bib.bib27)] | 2017 | 12283 | 17 | 9677 | 2606 | 真实世界 | RGB-D
    |'
- en: '| ScanObjectNN[[28](#bib.bib28)] | 2019 | 2902 | 15 | 2321 | 581 | Real Word
    | Point Cloud |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ScanObjectNN[[28](#bib.bib28)] | 2019 | 2902 | 15 | 2321 | 581 | 真实世界 | 点云
    |'
- en: III Point cloud classification method based on deep learning
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 基于深度学习的点云分类方法
- en: '![Refer to caption](img/c8ce289c8692fdad4609220e38b3b1f7.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c8ce289c8692fdad4609220e38b3b1f7.png)'
- en: 'Figure 3: Timeline of the development of classification methods'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：分类方法的发展时间线
- en: 'The point cloud classification model based on deep learning [[29](#bib.bib29)]
    has been widely used in point cloud analysis due to its advantages of strong generalization
    ability and high classification accuracy. This section provides a detailed division
    of deep learning-based point cloud classification methods and supplements some
    recent research works. Fig. [3](#S3.F3 "Figure 3 ‣ III Point cloud classification
    method based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification:
    A Systematic Survey and Outlook") shows the publication timeline of each classification
    method.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的点云分类模型[[29](#bib.bib29)]由于其强大的泛化能力和高分类准确率，已广泛应用于点云分析。本节详细划分了基于深度学习的点云分类方法，并补充了一些近期的研究工作。图[3](#S3.F3
    "图 3 ‣ III 基于深度学习的点云分类方法 ‣ 基于深度学习的3D点云分类：系统性调查与展望")显示了每种分类方法的出版时间线。
- en: III-A Multi-view-based methods
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 基于多视角的方法
- en: 'Multi-view learning is a machine learning framework in which data is represented
    by multiple distinct feature groups, each of which is called a specific view.
    The multi-view-based method is a deep learning based on 2D images. This method
    is divided into three steps: First, project the 3D image into multiple views.
    Second, extract the view function. Third, fuse these functions to accurately classify
    3D shapes. In 2015, Su et al. [[30](#bib.bib30)] first proposed the multi-view
    convolutional neural network MVCNN method. Since the collection of 2D views can
    provide a lot of information for 3D shape recognition, this method recognizes
    3D shapes from the collection of rendering views on 2D images. Representing 3D
    Shapes” is a longstanding problem. This method first demonstrates a standard CNN
    architecture trained to recognize shapes independently of rendered views and shows
    that 3D shapes can be recognized even from a single view. When multiple views
    of an object are provided, the recognition rate is further improved by using the
    CNN architecture to combine the information of multiple views into a compact shape
    descriptor. The network architecture is shown in Fig. [4](#S3.F4 "Figure 4 ‣ III-A
    Multi-view-based methods ‣ III Point cloud classification method based on deep
    learning ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic Survey
    and Outlook"). This method requires a large amount of computation in the projection
    retrieval process, and when converting multiple view features into global features
    through max-pooling, other non-maximum element information is ignored, so it will
    inevitably cause information missing.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角学习是一种机器学习框架，其中数据通过多个不同的特征组来表示，每个特征组被称为一个特定的视角。基于多视角的方法是基于2D图像的深度学习方法。该方法分为三个步骤：首先，将3D图像投影到多个视角中。其次，提取视角函数。最后，融合这些函数以准确分类3D形状。2015年，Su等人[[30](#bib.bib30)]首次提出了基于多视角的卷积神经网络MVCNN方法。由于2D视角的集合可以为3D形状识别提供大量信息，该方法通过2D图像上的渲染视角集合来识别3D形状。表现3D形状一直是一个长期存在的问题。该方法首先展示了一种标准CNN架构，训练以独立于渲染视角来识别形状，并表明即使从单一视角也可以识别3D形状。当提供多个视角时，识别率通过使用CNN架构将多个视角的信息组合成一个紧凑的形状描述符而进一步提高。网络架构如图[4](#S3.F4
    "图 4 ‣ III-A 基于多视角的方法 ‣ III 基于深度学习的点云分类方法 ‣ 基于深度学习的3D点云分类：系统性调查与展望")所示。该方法在投影检索过程中需要大量计算，并且在通过最大池化将多个视角特征转换为全局特征时，其他非最大元素的信息被忽略，因此不可避免地会导致信息丢失。
- en: '![Refer to caption](img/d1307fc58883875de044416d77d177ad.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d1307fc58883875de044416d77d177ad.png)'
- en: 'Figure 4: Schematic diagram of MVCNN[[30](#bib.bib30)] structure'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：MVCNN[[30](#bib.bib30)]结构示意图
- en: Therefore, in view of the large amount of computation and efficiency of MVCNN,
    Bai et al. [[31](#bib.bib31)] proposed a real-time 3D shape search engine, namely
    GIFT, this method uses GPU acceleration in the stages of projection and view feature
    extraction, which greatly shortens the time spent on retrieval tasks, and has
    high efficiency and ability to handle large-scale data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到MVCNN的大量计算和效率，Bai等人[[31](#bib.bib31)]提出了一个实时3D形状搜索引擎，即GIFT，该方法在投影和视图特征提取阶段使用GPU加速，这大大缩短了检索任务所花费的时间，并且具有高效率和处理大规模数据的能力。
- en: Based on MVCNN, the GVCNN architecture proposed by Feng et al. [[32](#bib.bib32)]
    introduces the ”view-group-shape” architecture to extract descriptors, which can
    effectively utilize the feature relationship between views. In order to overcome
    the problem of information loss due to max pooling, Wang et al. [[33](#bib.bib33)]
    introduced the view clustering and pooling layer based on the dominant set to
    improve the MVCNN method and proposed RCPCNN. It entails building a view similarity
    graph, clustering nodes (views) in this graph based on dominance sets, and pooling
    information from within each cluster. The recurrent clustering and pooling layer
    are to aggregate multi-view features in a way that provides more discriminative
    power for 3D object recognition. This recurrent clustering and pooling convolutional
    neural network (RCPCNN) module is plugged into ready-made pre-trained CNN which
    improves the performance of multi-view 3D object recognition.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基于MVCNN，Feng等人提出的GVCNN架构[[32](#bib.bib32)]引入了“视图-组-形状”架构来提取描述符，这可以有效地利用视图之间的特征关系。为了克服由于最大池化导致的信息丢失问题，Wang等人[[33](#bib.bib33)]引入了基于主导集的视图聚类和池化层，以改进MVCNN方法，并提出了RCPCNN。这涉及到构建一个视图相似性图，基于主导集对图中的节点（视图）进行聚类，并从每个集群中池化信息。递归聚类和池化层用于以提供更多判别能力的方式聚合多视图特征，用于3D对象识别。这一递归聚类和池化卷积神经网络（RCPCNN）模块被插入到现成的预训练CNN中，从而提高了多视图3D对象识别的性能。
- en: 'The MHBN method proposed by Yu et al. [[34](#bib.bib34)] aggregates local convolutional
    features by bilinear pooling to obtain efficient 3D object representations. Fig. [5](#S3.F5
    "Figure 5 ‣ III-A Multi-view-based methods ‣ III Point cloud classification method
    based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification: A
    Systematic Survey and Outlook") shows the network architecture of this method.
    To achieve an end-to-end trainable framework, this method coordinates the merging
    of bilinear pooling into one layer in the network, from the perspective of patch-to-patch
    similarity measurement, to address the problem of view-based methods pooling view
    features that differ.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 'Yu等人提出的MHBN方法[[34](#bib.bib34)]通过双线性池化聚合局部卷积特征，以获得高效的3D对象表示。图[5](#S3.F5 "Figure
    5 ‣ III-A Multi-view-based methods ‣ III Point cloud classification method based
    on deep learning ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic
    Survey and Outlook")显示了该方法的网络架构。为了实现端到端可训练的框架，该方法从补丁到补丁相似度测量的角度协调双线性池化的合并到网络中的一层，以解决视图方法池化视图特征差异的问题。'
- en: '![Refer to caption](img/d3583d25173a15d2690a3c0a35592978.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d3583d25173a15d2690a3c0a35592978.png)'
- en: 'Figure 5: Architecture of MHBN[[34](#bib.bib34)]'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：MHBN的架构[[34](#bib.bib34)]
- en: To avoid the lack of dynamism of current multi-view methods, Hamdi et al. [[35](#bib.bib35)]
    propose the Multi-View Transition Network (MVTN), which proposes a differentiable
    module that predicts the best viewpoint for a task-specific multi-view network,
    regressing the best for 3D shape recognition Viewpoint. MVTN can be trained end-to-end
    with any multi-view network, showing significant performance gains in 3D shape
    classification and retrieval tasks without additional training supervision.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免当前多视图方法的缺乏动态性，Hamdi等人[[35](#bib.bib35)]提出了多视图过渡网络（MVTN），该方法提出了一个可微分模块，用于预测任务特定多视图网络的最佳视点，回归3D形状识别的最佳视点。MVTN可以与任何多视图网络进行端到端训练，在3D形状分类和检索任务中显示出显著的性能提升，无需额外的训练监督。
- en: 'Summary: Compared with the traditional manual extraction feature classification,
    the multi-view-based method has a better effect in point cloud classification,
    but it is still difficult to make full use of information. The application of
    large-scale scenes, and the inherent geometric relationship of 3D data is the
    challenge we need to face.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：与传统的手工特征提取分类相比，多视角方法在点云分类中效果更好，但仍难以充分利用信息。大规模场景的应用以及 3D 数据的固有几何关系是我们需要面对的挑战。
- en: III-B Voxel-based methods
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 基于体素的方法
- en: This type of approach converts a 3D point cloud model into voxel form that approximates
    the shape of an object, each voxel block contains a set of associated points,
    and then uses 3D CNNs to classify the voxels.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法将 3D 点云模型转换为近似物体形状的体素形式，每个体素块包含一组相关的点，然后使用 3D CNN 来对体素进行分类。
- en: 'Maturana et al. [[36](#bib.bib36)] proposed a convolutional neural network
    architecture called VoxNet to represent 3D information with a volumetric occupancy
    grid. VoxNet is the earliest voxel-based 3DCNN model. As shown in Fig. [6](#S3.F6
    "Figure 6 ‣ III-B Voxel-based methods ‣ III Point cloud classification method
    based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification: A
    Systematic Survey and Outlook"), this method normalizes each grid, then constructs
    a feature map through convolution and max pooling a single voxel block. This architecture
    uses 2.5D to represent the features of the local description scan, and adopts
    a full volume representation, which improves the ability to express environmental
    information and enables powerful 3D object recognition.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Maturana 等人 [[36](#bib.bib36)] 提出了一个称为 VoxNet 的卷积神经网络架构，用于用体积占据网格表示 3D 信息。VoxNet
    是最早的基于体素的 3DCNN 模型。如图 [6](#S3.F6 "图 6 ‣ III-B 基于体素的方法 ‣ III 基于深度学习的点云分类方法 ‣ 基于深度学习的
    3D 点云分类：系统评估与展望") 所示，该方法对每个网格进行归一化，然后通过卷积和最大池化单个体素块来构建特征图。该架构使用 2.5D 来表示局部描述扫描的特征，并采用全体积表示，提升了表达环境信息的能力，并实现了强大的
    3D 物体识别。
- en: '![Refer to caption](img/a5cf1ed3d9a8a66cc9ee597538067adf.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a5cf1ed3d9a8a66cc9ee597538067adf.png)'
- en: 'Figure 6: Architecture of VoxNet[[36](#bib.bib36)]'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：VoxNet 的架构[[36](#bib.bib36)]
- en: Wu et al. [[23](#bib.bib23)] proposed 3D ShapeNets to recognize 3D objects.
    The model represents a 3D shape as a probability distribution of binary variables
    on a grid of 3D voxels, each of which can be represented by a binary tensor, and
    predicts the next best view in the presence of uncertain initial recognition.
    Finally, 3D ShapeNets can incorporate new views to identify objects in common
    with all views.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Wu 等人 [[23](#bib.bib23)] 提出了 3D ShapeNets 来识别 3D 物体。该模型将 3D 形状表示为 3D 体素网格上二进制变量的概率分布，每个体素可以用二进制张量表示，并在初步识别不确定的情况下预测最佳视角。最后，3D
    ShapeNets 能够结合新的视角来识别所有视角中共有的物体。
- en: Since 3D convolutions are computationally expensive, the spatial resolution
    increases model complexity when convolving 3D voxels. In response to this phenomenon,
    Li et al. [[37](#bib.bib37)] proposed a field detection neural network (FPNN),
    which represented 3D data as a 3D field, then sampled the input field through
    a set of detection filters, and finally used the field detection filter to characterize
    extract. The field detection layer can be used with other inference layers, but
    this approach makes the semantic segmentation results lower resolution.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 3D 卷积计算开销较大，空间分辨率的增加会使模型复杂度在卷积 3D 体素时增加。针对这一现象，Li 等人 [[37](#bib.bib37)] 提出了一个场检测神经网络（FPNN），它将
    3D 数据表示为 3D 场，然后通过一组检测滤波器对输入场进行采样，最后使用场检测滤波器进行特征提取。场检测层可以与其他推理层配合使用，但这种方法使得语义分割结果的分辨率较低。
- en: To reduce memory consumption and improve computational efficiency, some scholars
    use octree structure instead of fixed-resolution voxel structure. Riegler et al.
    [[38](#bib.bib38)] proposed OctNet, which exploits the sparsity of 3D data to
    hierarchically partition the space with a set of unbalanced octrees, where each
    leaf node stores a pooled feature representation. This method enables deeper networks
    without affecting their resolution. Wang et al. [[39](#bib.bib39)] proposed an
    octree-based convolutional neural network O-CNN, which uses octrees to represent
    3D data information and discretize its surface. The 3D CNN operation is only performed
    on the octant occupied by the 3D shape surface, which improves the computational
    efficiency and power.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少内存消耗并提高计算效率，一些学者使用八叉树结构来替代固定分辨率体素结构。Riegler等人 [[38](#bib.bib38)] 提出了OctNet，该方法利用3D数据的稀疏性通过一组不平衡的八叉树对空间进行分层划分，每个叶节点存储一个汇聚特征表示。该方法使得更深的网络得以实现而不影响其分辨率。Wang等人
    [[39](#bib.bib39)] 提出了基于八叉树的卷积神经网络O-CNN，该网络使用八叉树来表示3D数据的信息并离散化其表面。3D CNN操作仅在被3D形状表面占据的八分体上进行，从而提高了计算效率和性能。
- en: Following the use of the octree structure in the 3D data representation, the
    Kd-tree structure is also used in the point cloud classification model. The Kd-network
    proposed by Klokov et al. [[40](#bib.bib40)] adopts, compared with voxel and mesh,
    the ability of Kd-tree to index and structure 3D data has been improved, so Kd-network
    has a smaller memory footprint and more efficient computation during training
    and testing. The 3D Context Net proposed by Zeng et al. [[41](#bib.bib41)] utilizes
    the method of local and global context cues imposed by Kd-tree for semantic segmentation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在3D数据表示中使用八叉树结构后，Kd-tree结构也被应用于点云分类模型中。Klokov等人提出的Kd-network [[40](#bib.bib40)]
    相较于体素和网格，改进了Kd-tree在索引和结构化3D数据方面的能力，因此Kd-network具有更小的内存占用和更高效的训练和测试计算。Zeng等人提出的3D
    Context Net [[41](#bib.bib41)] 利用Kd-tree施加的局部和全局上下文提示方法进行语义分割。
- en: Octree structure and Kd-tree structure reduce memory consumption to a certain
    extent and improve computational efficiency, but due to the influence of voxel
    boundary value, these two structures cannot make full use of local data features
    and the accuracy is affected . Wang et al. [[42](#bib.bib42)] proposed a multi-scale
    convolutional network, MSNet. This method first divides the space into voxels
    of different scales, then applies MSNet on multiple scales simultaneously to learn
    local features, and finally uses a conditional random field (CRF) The prediction
    results of MSNet are globally optimized to achieve a more accurate point cloud
    classification task.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 八叉树结构和Kd-tree结构在一定程度上减少了内存消耗并提高了计算效率，但由于体素边界值的影响，这两种结构无法充分利用局部数据特征，从而影响了准确性。Wang等人
    [[42](#bib.bib42)] 提出了一个多尺度卷积网络MSNet。该方法首先将空间划分为不同尺度的体素，然后在多个尺度上同时应用MSNet来学习局部特征，最后使用条件随机场（CRF）对MSNet的预测结果进行全局优化，以实现更准确的点云分类任务。
- en: The VV-Net proposed by Meng et al. [[43](#bib.bib43)] uses a kernel-based interpolating
    variational autoencoder (VAE) to encode localities in voxels, each voxel is further
    subdivided into sub-voxels, which are within voxels interpolate sparse point samples
    to efficiently handle noisy point cloud datasets.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Meng等人提出的VV-Net [[43](#bib.bib43)] 使用基于核的插值变分自编码器（VAE）对体素中的局部区域进行编码，每个体素进一步细分为子体素，这些子体素在体素内插值稀疏点样本，以高效处理噪声点云数据集。
- en: 'Summary: Compared with the multi-view method, the voxel-based method pays attention
    to the relationship between 3D data, and groups the point clouds with internal
    connections into a set of points, thereby establishing voxels. Although the voxel-based
    model solves the point cloud disorder and unstructured problem, the sparseness
    and incomplete information of point cloud data lead to the low efficiency of the
    classification task, so the information in the point cloud cannot be fully utilized.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：与多视角方法相比，基于体素的方法关注3D数据之间的关系，将具有内部连接的点云分组为一组点，从而建立体素。尽管基于体素的模型解决了点云无序和非结构化的问题，但点云数据的稀疏性和信息不完整性导致分类任务的效率较低，因此无法充分利用点云中的信息。
- en: III-C Point cloud-based method
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 基于点云的方法
- en: 'Many current research methods focus more on directly processing point clouds
    using deep learning techniques. Feature aggregation operator is the core of processing
    point cloud, which realizes the information transfer of discrete points. Feature
    aggregation operators are mainly divided into two categories: local feature aggregation
    and global feature aggregation. In this section, from the perspective of feature
    aggregation, the two categories of methods are divided and introduced in more
    detail.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 许多当前的研究方法更侧重于使用深度学习技术直接处理点云。特征聚合算子是处理点云的核心，它实现了离散点的信息传递。特征聚合算子主要分为两类：局部特征聚合和全局特征聚合。在本节中，从特征聚合的角度，对这两类方法进行更详细的划分和介绍。
- en: 'In 2017, the PointNet proposed by Qi et al. [[21](#bib.bib21)] (shown in Fig. [7](#S3.F7
    "Figure 7 ‣ III-C Point cloud-based method ‣ III Point cloud classification method
    based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification: A
    Systematic Survey and Outlook")) is a pioneering study of point cloud-based methods,
    which is a method of global feature aggregation. This method directly takes the
    point cloud as input, transforms it through the T-Net module, then learns each
    point by sharing the full connection, and finally aggregates the features of the
    point into global features through the maximum pooling function. Although PointNet
    is a pioneer based on deep learning, it still has defects. For example, PointNet
    only captures the feature information of a single point and global points but
    does not consider the relational representation of adjacent points, which makes
    PointNet unable to effectively perform fine-grained classification.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，Qi 等人提出的 PointNet [[21](#bib.bib21)]（如图[7](#S3.F7 "图 7 ‣ III-C 基于点云的方法
    ‣ III 基于深度学习的点云分类方法 ‣ 基于深度学习的 3D 点云分类：系统性调查与展望")所示）是基于点云方法的开创性研究，这是一种全局特征聚合的方法。该方法直接以点云作为输入，通过
    T-Net 模块进行变换，然后通过全连接进行每个点的学习，最后通过最大池化函数将点的特征聚合为全局特征。尽管 PointNet 是基于深度学习的先锋，但仍存在缺陷。例如，PointNet
    仅捕获单个点和全局点的特征信息，但没有考虑相邻点的关系表示，这使得 PointNet 无法有效进行细粒度分类。
- en: '![Refer to caption](img/25c416b5884c5c1eaf4742c7ad7a0a31.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/25c416b5884c5c1eaf4742c7ad7a0a31.png)'
- en: 'Figure 7: PointNet[[21](#bib.bib21)] network architecture'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：PointNet[[21](#bib.bib21)] 网络架构
- en: III-C1 Local feature aggregation
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 局部特征聚合
- en: '1'
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '1'
- en: Point-by-point method
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 点对点方法
- en: 'Qi et al.[[44](#bib.bib44)] successively proposed PointNet++ based on PointNet.
    This method processes point clouds in a hierarchical manner, and each layer consists
    of a sampling layer, a grouping layer, and a PointNet layer. Among them, the sampling
    layer obtains the centroid of the local neighborhood, the grouping layer constructs
    a subset of the local neighborhood, and the PointNet layer obtains the relationship
    between the points in the local area, as shown in Fig. [8](#S3.F8 "Figure 8 ‣
    1 ‣ III-C1 Local feature aggregation ‣ III-C Point cloud-based method ‣ III Point
    cloud classification method based on deep learning ‣ Deep Learning-based 3D Point
    Cloud Classification: A Systematic Survey and Outlook"). PointNet++ needs to solve
    two problems: dividing the generated point set and aggregating local features
    through a local feature learner. But PointNet++ still does not ignore the prior
    relationship between points.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Qi 等人[[44](#bib.bib44)] 继承了基于 PointNet 的 PointNet++。该方法以分层方式处理点云，每一层包括一个采样层、一个分组层和一个
    PointNet 层。其中，采样层获取局部邻域的质心，分组层构建局部邻域的子集，PointNet 层获取局部区域内点之间的关系，如图[8](#S3.F8 "图
    8 ‣ 1 ‣ III-C1 局部特征聚合 ‣ III-C 基于点云的方法 ‣ III 基于深度学习的点云分类方法 ‣ 基于深度学习的 3D 点云分类：系统性调查与展望")所示。PointNet++
    需要解决两个问题：划分生成的点集和通过局部特征学习器聚合局部特征。但 PointNet++ 仍然没有忽视点之间的先验关系。
- en: '![Refer to caption](img/193e3315911eef939f544b39bff19dae.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/193e3315911eef939f544b39bff19dae.png)'
- en: 'Figure 8: PointNet++[[44](#bib.bib44)] point cloud segmentation and classification
    architecture diagram'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：PointNet++[[44](#bib.bib44)] 点云分割与分类架构图
- en: 'Based on PointNet++, Qian et al.[[45](#bib.bib45)] improved the training and
    training strategy to improve the performance of PointNet++ and introduced a separable
    MLP and an inverted residual bottleneck design in the PointNet++ framework, and
    named its framework PointNeXt (as shown in Fig. [9](#S3.F9 "Figure 9 ‣ 1 ‣ III-C1
    Local feature aggregation ‣ III-C Point cloud-based method ‣ III Point cloud classification
    method based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification:
    A Systematic Survey and Outlook")).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 PointNet++，Qian 等人[[45](#bib.bib45)] 改进了训练策略以提升 PointNet++ 的性能，并在 PointNet++
    框架中引入了可分离的 MLP 和倒置残差瓶颈设计，将其框架命名为 PointNeXt（如图 [9](#S3.F9 "图 9 ‣ 1 ‣ III-C1 局部特征聚合
    ‣ III-C 点云方法 ‣ III 基于深度学习的点云分类方法 ‣ 基于深度学习的 3D 点云分类：系统综述与展望") 所示）。
- en: '![Refer to caption](img/0dec9d202da08e03c845fd5fc8400925.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0dec9d202da08e03c845fd5fc8400925.png)'
- en: 'Figure 9: PointNeXt[[45](#bib.bib45)] architecture'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: PointNeXt[[45](#bib.bib45)] 架构'
- en: Zhao et al.[[46](#bib.bib46)] proposed a method of extracting features from
    the local neighborhood of point clouds - PointWeb, which connects the points in
    the local neighborhood with each other, and proposes a new module, Adaptive Feature
    Adjustment (AFA), which is used to find information transfer between points. This
    method makes full use of the local features of points and realizes adaptive adjustment.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao 等人[[46](#bib.bib46)] 提出了从点云局部邻域提取特征的方法 - PointWeb，它将局部邻域中的点相互连接，并提出了一个新模块，适应性特征调整（AFA），用于查找点之间的信息传递。该方法充分利用点的局部特征，实现了自适应调整。
- en: Hu et al. [[47](#bib.bib47)] proposed an efficient and lightweight neural architecture
    - RandLA-Net, which uses random point sampling, and increases the receptive field
    of each point through an efficient local feature aggregation module, so as to
    better capturing complex local structures reduces memory footprint and computational
    cost, but this approach may discard some key features of sparse points.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Hu 等人[[47](#bib.bib47)] 提出了一个高效且轻量级的神经网络架构 - RandLA-Net，它使用随机点采样，通过高效的局部特征聚合模块增加每个点的感受野，从而更好地捕捉复杂的局部结构，减少内存占用和计算成本，但这种方法可能会丢失一些稀疏点的关键特征。
- en: The SO-Net proposed by Li et al. [[48](#bib.bib48)] constructs a self-organizing
    map (SOM), extracts the features of each point and SOM node hierarchically, and
    uses a single feature vector to represent the point cloud, by appending a 3-layer
    perception to the encoded global feature vector Multi-Layer Perceptron(MLP) is
    used to classify point clouds. SO-Net has good parallelism and simple structure,
    but it has limitations in processing large-scale point cloud data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人提出的 SO-Net [[48](#bib.bib48)] 构建了自组织映射（SOM），分层提取每个点和 SOM 节点的特征，并使用单一特征向量表示点云，通过在编码的全局特征向量上附加
    3 层感知器，利用多层感知器（MLP）对点云进行分类。SO-Net 具有良好的并行性和简单结构，但在处理大规模点云数据时存在局限性。
- en: 'In order to fully capture the most critical geometric information, Xu et al.
    [[49](#bib.bib49)] proposed the geometric disentanglement attention network GDANet,
    and introduced the Geometry-Disentangle module to decompose the original point
    cloud into two parts: contour and plane, so as to capture and refine 3D semantics
    to supplementing local information. The method has good robustness.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面捕捉最关键的几何信息，Xu 等人[[49](#bib.bib49)] 提出了几何解缠注意网络 GDANet，并引入了 Geometry-Disentangle
    模块，将原始点云分解为轮廓和平面两个部分，从而捕捉和优化 3D 语义，以补充局部信息。该方法具有良好的鲁棒性。
- en: 'Goyal et al. [[50](#bib.bib50)] proposed a projection-based method, SimpleView,
    which showed that training and evaluation factors independent of the network architecture
    have a large impact on point cloud classification performance. The PointSCNet
    proposed by Chen et al. [[51](#bib.bib51)] is used to capture the geometric information
    and local area information of the point cloud. It consists of three modules: a
    space-filling curve-guided sampling module, an information fusion module, and
    a channel spatial attention module. In PointSCNet the raw point cloud is fed to
    the sampling and grouping block, which is sampled using a Z-order curve to obtain
    the high correlation between points and local regions. After extracting the sampled
    point cloud features, a feature fusion module is designed to learn the structure
    and related information. Finally, the keypoint features are enhanced by the channel
    space module.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Goyal等人[[50](#bib.bib50)]提出了一种基于投影的方法SimpleView，该方法显示了与网络架构无关的训练和评估因素对点云分类性能的巨大影响。Chen等人[[51](#bib.bib51)]提出的PointSCNet用于捕捉点云的几何信息和局部区域信息。它由三个模块组成：一个空间填充曲线引导的采样模块，一个信息融合模块，以及一个通道空间注意力模块。在PointSCNet中，原始点云被输入到采样和分组块，通过Z-order曲线进行采样，以获得点与局部区域之间的高相关性。在提取了采样点云特征后，设计了一个特征融合模块来学习结构和相关信息。最后，关键点特征通过通道空间模块得到增强。
- en: Ma et al. [[52](#bib.bib52)] noticed that detailed local geometric information
    may not be the key to analyzing point clouds, so they introduced a pure residual
    network called PointMLP, which does not have a complex local geometry extractor,
    but is equipped with a lightweight geometric affine module, there is a significant
    improvement in inference speed. Huang et al. [[53](#bib.bib53)] calculated the
    rate of change of recognition confidence when a shape-invariant perturbation with
    explicit constraints was added to each point in the point cloud, and according
    to this method, they proposed a point cloud sensitivity map, which was then used
    to propose a shape-invariant adversarial point cloud.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Ma等人[[52](#bib.bib52)]注意到详细的局部几何信息可能不是分析点云的关键，因此他们引入了一种称为PointMLP的纯残差网络，该网络没有复杂的局部几何提取器，但配备了一个轻量级几何仿射模块，推理速度显著提升。Huang等人[[53](#bib.bib53)]计算了在点云中添加具有显式约束的形状不变扰动时识别置信度的变化率，并根据这种方法提出了点云敏感度图，然后用该图提出了形状不变的对抗点云。
- en: 'Ran et al. [[54](#bib.bib54)] proposed using RepSurf (representative surfaces)
    to represent point clouds, which has two variants: Triangular RepSurf and Umbrella
    RepSurf. This representation can be used as a module in the point cloud classification
    and segmentation framework to improve the performance of the point cloud framework.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Ran等人[[54](#bib.bib54)]提出使用RepSurf（代表性表面）来表示点云，RepSurf有两个变体：三角形RepSurf和伞形RepSurf。这种表示方法可以作为点云分类和分割框架中的一个模块，以提高点云框架的性能。
- en: '2'
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '2'
- en: Convolution-based methods
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 基于卷积的方法
- en: Convolutional Neural Network (CNN) plays an important role in deep learning
    and is the most basic deep learning model. Its excellent performance in the field
    of 2D image processing has led researchers to apply it to 3D point clouds and
    design point convolution for point cloud classification.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）在深度学习中扮演着重要角色，是最基本的深度学习模型。它在2D图像处理领域的优异表现促使研究人员将其应用于3D点云，并为点云分类设计了点卷积。
- en: Atzmon et al. [[55](#bib.bib55)] proposed the point convolutional neural network
    (PCNN), which applied convolutional Neural Network (CNN) to point cloud. First,
    the function on the point cloud is extended to a continuous volume function in
    space; then a continuous volume convolution is applied to the function; the final
    result is a constrained point cloud, as shown in Figure 13.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Atzmon等人[[55](#bib.bib55)]提出了点卷积神经网络（PCNN），将卷积神经网络（CNN）应用于点云。首先，将点云上的函数扩展为空间中的连续体积函数；然后对该函数应用连续体积卷积；最终结果是一个约束的点云，如图13所示。
- en: Liu et al. [[56](#bib.bib56)] proposed Relational Shape Convolutional Networks
    (RS-CNN) to extend regular CNNs to irregular point clouds for analysis of point
    clouds. Yousefhussien et al. [[57](#bib.bib57)] proposed an one-dimensional fully
    convolutional network. Wang et al. [[58](#bib.bib58)] proposed a deep neural network
    DNNSP with spatial pooling to classify large-scale point clouds. This method can
    learn the features from the entire region to the center point of the point cluster,
    to achieve a robust representation of point features. Komarichev et al. [[59](#bib.bib59)]
    propose a point cloud-based annular convolutional neural network (A-CNN) model.
    Ran et al. [[60](#bib.bib60)] proposed RPNet based on a group relation aggregation
    module, which is robust to rigid transformations and noise. The SCN (ShapeContextNet)
    proposed by Xie et al. [[61](#bib.bib61)] is represented by using the shape context
    as a building block, so that it can capture and propagate object part information.
    SCN is an end-to-end model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人 [[56](#bib.bib56)] 提出了关系形状卷积网络（RS-CNN），以扩展常规 CNN 到不规则点云以进行点云分析。Yousefhussien
    等人 [[57](#bib.bib57)] 提出了一个一维全卷积网络。王等人 [[58](#bib.bib58)] 提出了一个具有空间池化的深度神经网络 DNNSP，用于分类大规模点云。该方法可以从整个区域到点簇的中心点学习特征，以实现对点特征的鲁棒表示。Komarichev
    等人 [[59](#bib.bib59)] 提出了一个基于点云的环形卷积神经网络（A-CNN）模型。冉等人 [[60](#bib.bib60)] 提出了基于组关系聚合模块的
    RPNet，该模块对刚性变换和噪声具有鲁棒性。谢等人 [[61](#bib.bib61)] 提出的 SCN（ShapeContextNet）使用形状上下文作为构建块，从而能够捕获和传播对象部件信息。SCN
    是一个端到端模型。
- en: Since directly convolving kernels with point-related features would result in
    the discarding of shape information and variance in point ordering, Li et al.
    [[62](#bib.bib62)] proposed PointCNN to address this problem, which confirmed
    the development of local structures for point cloud classification networks importance.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于直接对点相关特征进行卷积会导致形状信息和点排序的变异被丢弃，李等人 [[62](#bib.bib62)] 提出了 PointCNN 以解决这一问题，确认了局部结构对点云分类网络发展的重要性。
- en: Due to the sparsity, irregularity, and disorder of the point cloud, it is difficult
    to directly perform the convolution operation on it. Wu et al. [[63](#bib.bib63)]
    proposed to apply the dynamic filter to the convolution operation called PointConv,
    which is simple and reduces the computer storage pressure. MCCNN [[64](#bib.bib64)]
    represents the convolution kernel itself as a multilayer perceptron and describes
    the convolution as a Monte Carlo integration. SpiderCNN [[65](#bib.bib65)] inherits
    the multi-scale hierarchical structure of CNN and consists of SpiderConv units,
    which extend the convolution operation from regular grids to irregular point sets
    that can be embedded in n-dimensional space by parameterizing a series of convolution
    filters to effectively extract geometric features from point clouds. Mao et al.
    [[66](#bib.bib66)] designed interpolating convolutional neural networks (InterpCNNs)
    based on combining InterConv(interpolating convolution operation).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于点云的稀疏性、不规则性和混乱性，直接对其进行卷积操作非常困难。吴等人[[63](#bib.bib63)] 提出了应用动态滤波器进行卷积操作的方法，称为
    PointConv，这种方法简单且减少了计算机存储压力。MCCNN [[64](#bib.bib64)] 将卷积核本身表示为多层感知机，并将卷积描述为蒙特卡罗积分。SpiderCNN
    [[65](#bib.bib65)] 继承了 CNN 的多尺度层次结构，由 SpiderConv 单元组成，这些单元将卷积操作从规则网格扩展到不规则点集，这些点集可以通过参数化一系列卷积滤波器嵌入到
    n 维空间中，以有效地从点云中提取几何特征。毛等人 [[66](#bib.bib66)] 设计了基于 InterConv（插值卷积操作）结合的插值卷积神经网络（InterpCNNs）。
- en: Esteves et al. [[67](#bib.bib67)] use multi-valued spherical functions to model
    3D data, and propose a spherical convolutional network that implements them by
    implementing precise convolutions on spheres in the spherical harmonic domain,
    thus solving the 3D rotation variance problem in convolutional neural networks.
    SPHNet [[68](#bib.bib68)] is based on PCNN to achieve rotation invariance by using
    spherical harmonics in different layers of the network.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Esteves 等人 [[67](#bib.bib67)] 使用多值球面函数来建模 3D 数据，并提出了一个球面卷积网络，通过在球面谐波域中实现精确的卷积，从而解决了卷积神经网络中的
    3D 旋转变异性问题。SPHNet [[68](#bib.bib68)] 基于 PCNN 通过在网络的不同层中使用球面谐波来实现旋转不变性。
- en: Since the local features of point clouds are difficult to aggregate and transfer
    effectively, Wang et al.[[69](#bib.bib69)] proposed a spatial coverage convolutional
    neural network (SC-CNN), the core of which is the spatial coverage convolution
    (SC-Conv). Anisotropic spatial geometry is constructed in the point cloud to implement
    depthwise separable convolution, replacing depthwise convolution with the spatial
    coverage operator (SCOP).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于点云的局部特征难以有效聚合和转移，Wang 等人 [[69](#bib.bib69)] 提出了一个空间覆盖卷积神经网络（SC-CNN），其核心是空间覆盖卷积（SC-Conv）。在点云中构建各向异性空间几何体以实现深度可分卷积，将深度卷积替换为空间覆盖操作符（SCOP）。
- en: '3'
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '3'
- en: Graph-based methods
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的方法
- en: The graph neural network (GNN) was first proposed by Scarselli et al. [[70](#bib.bib70)].
    Bruna et al. [[71](#bib.bib71)] are the first to apply convolution to low-dimensional
    graph structures to effectively represent depth. Kipf et al. [[72](#bib.bib72)]
    further proposed that a graph convolutional network (GCN) works well in semi-supervised
    classification tasks, and in fact, GCN is an optimization of CNN.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNN）最早由 Scarselli 等人 [[70](#bib.bib70)] 提出。Bruna 等人 [[71](#bib.bib71)]
    是首个将卷积应用于低维图结构以有效表示深度的研究者。Kipf 等人 [[72](#bib.bib72)] 进一步提出图卷积网络（GCN）在半监督分类任务中表现良好，实际上，GCN
    是 CNN 的一种优化。
- en: Simonovsky [[73](#bib.bib73)] proposed an ECC(edge conditional convolutional)
    network that can be applied to any graph structure in combination with the application
    of edge labels. This method uses the points as the vertices of the graph and the
    distance between the points as the weight, and performs a weighted average convolution
    operation, using the maximum sampling the information of aggregated vertices.
    It can be used for large-scale point cloud segmentation, but the amount of computation
    is large.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Simonovsky [[73](#bib.bib73)] 提出了 ECC（边条件卷积）网络，该网络可以与边缘标签的应用相结合，应用于任何图结构。这种方法使用点作为图的顶点，点之间的距离作为权重，执行加权平均卷积操作，利用最大采样聚合顶点的信息。它可以用于大规模点云分割，但计算量较大。
- en: SpecGCN [[74](#bib.bib74)] replaces the standard max-pooling step with a recursive
    clustering and pooling strategy. Grid-GCN [[75](#bib.bib75)] uses a coverage-aware
    network query (CAGQ), which improves spatial coverage and reduces theoretical
    time complexity by exploiting the efficiency of grid space.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: SpecGCN [[74](#bib.bib74)] 用递归聚类和池化策略替代了标准的最大池化步骤。Grid-GCN [[75](#bib.bib75)]
    使用了一种关注覆盖的网络查询（CAGQ），通过利用网格空间的效率来提高空间覆盖率并减少理论时间复杂度。
- en: Mohammadi et al. [[76](#bib.bib76)] proposed PointView-GCN with a multi-level
    graph convolutional network (GCN) to hierarchically aggregate shape features of
    single-view point clouds, which facilitates encoding of object geometric cues
    and multi-view relationships, resulting in more specific global features.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Mohammadi 等人 [[76](#bib.bib76)] 提出了 PointView-GCN，该方法具有多层次图卷积网络（GCN），用于分层聚合单视图点云的形状特征，这有助于编码对象几何线索和多视图关系，从而得到更具体的全局特征。
- en: Wang et al. [[77](#bib.bib77)] proposed a dynamic graph CNN (DGCNN) for point
    cloud learning and proposed an edge convolutional (EdgeConv) network module, which
    can better capture the local geometric features of point clouds and maintain arrangement
    invariance, which demonstrates the importance of local geometric features for
    3D recognition tasks. Zhang et al. [[78](#bib.bib78)] further optimized DGCNN
    and proposed a Linked Dynamic Graph Convolutional Neural Network (LDGCNN), which
    removed the transformation network in DGCNN to simplify the network model and
    optimized the network by connecting hierarchical features of different dynamic
    graphs, which can better solve the gradient vanishing problem.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[77](#bib.bib77)] 提出了一个用于点云学习的动态图 CNN（DGCNN），并提出了一种边缘卷积（EdgeConv）网络模块，这可以更好地捕捉点云的局部几何特征并保持排列不变，这展示了局部几何特征对
    3D 识别任务的重要性。Zhang 等人 [[78](#bib.bib78)] 进一步优化了 DGCNN，并提出了一个链接动态图卷积神经网络（LDGCNN），该网络去除了
    DGCNN 中的变换网络以简化网络模型，并通过连接不同动态图的层次特征来优化网络，这样可以更好地解决梯度消失问题。
- en: The PointNGCNN proposed by Lu et al. [[79](#bib.bib79)] describes the relationship
    between points in the neighborhood in a neighborhood graph and uses neighborhood
    graph filters to extract neighborhood feature information and spatial distribution
    information in feature space and Cartesian space.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Lu 等人 [[79](#bib.bib79)] 提出的 PointNGCNN 描述了邻域图中点之间的关系，并使用邻域图滤波器来提取特征空间和笛卡尔空间中的邻域特征信息和空间分布信息。
- en: '4'
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '4'
- en: Attention-based methods
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的方法
- en: The basic idea of the attention mechanism is to apply human perception to the
    machine, but human perception selectively focuses on part of the scene instead
    of processing the entire scene at a time, so researchers focus on the attention
    mechanism to carry out research and apply to the field of point cloud classification.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的基本思想是将人类的感知应用于机器，但人类的感知会选择性地关注场景的一部分，而不是一次性处理整个场景，因此研究人员集中在注意力机制上进行研究，并将其应用于点云分类领域。
- en: Yang et al. [[80](#bib.bib80)] developed a point attention transformer (PAT)
    based on point cloud reasoning. It is proposed to use efficient GSA (Group-Shuffle
    Attention) instead of expensive MHA (Multi-Head Attention) for modeling the relationship
    between points, and propose a method called Gumbel Subset Sampling (GSS) to select
    a subset of representative points, which reduces the computational cost.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 杨等人[[80](#bib.bib80)]开发了一种基于点云推理的点注意力变换器（PAT）。他们提出使用高效的GSA（组洗牌注意力）代替昂贵的MHA（多头注意力）来建模点之间的关系，并提出一种称为Gumbel子集采样（GSS）的方法来选择具有代表性的点子集，从而降低计算成本。
- en: Li et al. [[81](#bib.bib81)] proposed Feature Pyramid Attention Module (FPA)
    and Global Attention Upsampling Module (GAU) by combining attention mechanism
    and spatial pyramid. Chen et al. [[82](#bib.bib82)] designed a Local Spatial Awareness
    (LSA) layer and proposed an LSANet network architecture based on the LSA layer.
    LSA can learn the spatial relationship layer in the local area to generate spatially
    distributed weights, so that spatially independent operations can be performed.
    The spatial information extraction function of this method is powerful.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 李等人[[81](#bib.bib81)]通过将注意力机制和空间金字塔相结合，提出了特征金字塔注意力模块（FPA）和全局注意力上采样模块（GAU）。陈等人[[82](#bib.bib82)]设计了一个局部空间意识（LSA）层，并提出了一种基于LSA层的LSANet网络架构。LSA可以学习局部区域的空间关系层，生成空间分布的权重，从而进行空间独立的操作。这种方法的空间信息提取功能非常强大。
- en: Wang et al. [[83](#bib.bib83)] proposed GACNet based on graph attention convolution
    (GAC). The GAPointNet proposed by Chen et al. [[84](#bib.bib84)] combines the
    self-attention mechanism with graph convolution, learns local information representation
    by embedding a graph attention mechanism in stacked MLP layers, and uses a parallel
    mechanism to aggregate the attention features of different GAPLayer layers, where
    GAPLayer layers and attention layers can be embedded into existing trained models
    to better extract local contextual features from unordered point clouds.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 王等人[[83](#bib.bib83)]提出了基于图注意力卷积（GAC）的GACNet。陈等人[[84](#bib.bib84)]提出的GAPointNet将自注意力机制与图卷积相结合，通过在堆叠的MLP层中嵌入图注意力机制来学习局部信息表示，并使用并行机制聚合不同GAPLayer层的注意力特征，其中GAPLayer层和注意力层可以嵌入到现有的训练模型中，以更好地从无序点云中提取局部上下文特征。
- en: III-C2 Global feature aggregation
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 全局特征聚合
- en: '1'
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '1'
- en: Transformer-based methods
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器的方法
- en: Since the transformer was first proposed in 2017 [[85](#bib.bib85)], it has
    achieved world-renowned results in the field of computer vision. Many researchers
    also use this structure in point cloud processing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 自从2017年首次提出变换器[[85](#bib.bib85)]以来，它在计算机视觉领域取得了世界著名的成果。许多研究人员也在点云处理领域中使用这一结构。
- en: 'Engel et al. [[86](#bib.bib86)] propose a deep neural network Point Transformer
    that operates directly on unordered and unstructured point sets, and propose a
    learning score-based focus module, ScorNet, as part of the Point Transformer.
    The point cloud is used as the input of the Point Transformer, and local and global
    features are extracted from it, and then SortNet is used to rank the local features,
    and finally the local global attention is used to associate the local global features,
    as shown in Fig. [10](#S3.F10 "Figure 10 ‣ 1 ‣ III-C2 Global feature aggregation
    ‣ III-C Point cloud-based method ‣ III Point cloud classification method based
    on deep learning ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic
    Survey and Outlook").'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 'Engel等人[[86](#bib.bib86)]提出了一种深度神经网络Point Transformer，该网络直接作用于无序和无结构的点集，并提出了作为Point
    Transformer一部分的基于学习评分的聚焦模块ScorNet。点云作为Point Transformer的输入，从中提取局部和全局特征，然后使用SortNet对局部特征进行排序，最后使用局部全局注意力关联局部和全局特征，如图[10](#S3.F10
    "Figure 10 ‣ 1 ‣ III-C2 Global feature aggregation ‣ III-C Point cloud-based method
    ‣ III Point cloud classification method based on deep learning ‣ Deep Learning-based
    3D Point Cloud Classification: A Systematic Survey and Outlook")所示。'
- en: '![Refer to caption](img/51e36a9434fbcbf90a5e5ccfaee1bb87.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/51e36a9434fbcbf90a5e5ccfaee1bb87.png)'
- en: 'Figure 10: Architecture of Point Transformer[[86](#bib.bib86)]'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 点 Transformer 的架构 [[86](#bib.bib86)]'
- en: Berg et al. [[87](#bib.bib87)] found that the self-attention operator grows
    rapidly and inefficiently with the growth of the input point set, and the attention
    mechanism is difficult to find the relationship between each point in the global,
    so they propose a two-stage method - Point TnT, this method enables a single point
    and a point set to pay attention to each other effectively.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Berg 等人 [[87](#bib.bib87)] 发现自注意力操作随着输入点集的增长而迅速且低效地增长，且注意力机制难以在全局中找到每个点之间的关系，因此他们提出了一种两阶段的方法
    - Point TnT，该方法使得单个点和点集能够有效地互相关注。
- en: The Visual Transformer (VT) proposed by Wu et al. [[88](#bib.bib88)], which
    applies Transformer to label-based images from feature maps, can learn and associate
    high-level concepts of sparse distributions more efficiently. Carion et al. [[89](#bib.bib89)]
    propose a method to treat object detection as a direct ensemble prediction problem
    called Detection Transformer (DETR), which is an end-to-end detection transformer
    that takes CNN features as input and uses a Transformer encoder-decoder to generate
    boundaries.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Wu 等人 [[88](#bib.bib88)] 提出的视觉 Transformer (VT) 将 Transformer 应用于从特征图生成的标签图像，可以更有效地学习和关联稀疏分布的高层次概念。Carion
    等人 [[89](#bib.bib89)] 提出了将目标检测视为直接集成预测问题的方法，称为检测 Transformer (DETR)，这是一种端到端检测
    Transformer，它以 CNN 特征作为输入，并使用 Transformer 编码器-解码器来生成边界。
- en: Guo et al. [[90](#bib.bib90)] proposed a Transformer-based point cloud learning
    framework - Point Cloud Transformer (PCT), and proposed offset attention with
    implicit Laplacian operator and normalization refinement, the framework has permutation
    invariance and is more suitable for point cloud learning
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Guo 等人 [[90](#bib.bib90)] 提出了一个基于 Transformer 的点云学习框架 - 点云 Transformer (PCT)，并提出了具有隐式拉普拉斯算子和归一化精炼的偏移注意力，该框架具有排列不变性，更适合点云学习。
- en: 3DMedPT is a Transformer network proposed by Yu et al. [[12](#bib.bib12)] for
    3D medical point cloud analysis.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 3DMedPT 是 Yu 等人 [[12](#bib.bib12)] 提出的用于 3D 医学点云分析的 Transformer 网络。
- en: Inspired by BERT, Yu et al. [[91](#bib.bib91)] proposed a new method for learning
    Transformer called Point-BERT. This method first divides the point cloud into
    several local blocks and generates discrete point labels of local information
    through a point cloud marker, and then by randomly masking some input point clouds
    and feeding them into the backbone Transformer, this method can generalize the
    concept of BERT to point clouds.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 BERT 的启发，Yu 等人 [[91](#bib.bib91)] 提出了一个新的 Transformer 学习方法，称为 Point-BERT。该方法首先将点云划分为几个局部块，并通过点云标记器生成局部信息的离散点标签，然后通过随机遮蔽一些输入点云并将其输入到主干
    Transformer 中，该方法能够将 BERT 的概念推广到点云。
- en: Pang et al. [[92](#bib.bib92)] proposed Point-MAE, which is a masked autoencoder
    method for point cloud self-supervised learning to solve the problems caused by
    point cloud location information leakage and uneven information density. question.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Pang 等人 [[92](#bib.bib92)] 提出了 Point-MAE，这是一种用于点云自监督学习的掩蔽自编码器方法，用于解决点云位置数据泄漏和信息密度不均等问题。
- en: He et al. [[93](#bib.bib93)] introduced a voxel-based set attention module (VSA)
    to establish the Voxel Set Transformer (VoxSeT) architecture. VoxSeT can manage
    point clusters through the VSA module and process them in parallel with linear
    complexity. This method combines the high performance of Transformer with the
    high efficiency of voxel-based model, it has good performance in point cloud modeling.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: He 等人 [[93](#bib.bib93)] 引入了一种基于体素的集合注意力模块 (VSA)，建立了体素集合 Transformer (VoxSeT)
    架构。VoxSeT 可以通过 VSA 模块管理点簇，并以线性复杂度并行处理它们。该方法结合了 Transformer 的高性能和基于体素模型的高效率，在点云建模中表现良好。
- en: '2'
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '2'
- en: Global module-based methods
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 全局模块化方法
- en: Wang et al. [[94](#bib.bib94)] propose a global module that computes the response
    at a location as a weighted sum of all location features, providing a solution
    to aggregating global features, but the global point-to-point mapping may still
    be insufficient to extract the underlying patterns implied by the point cloud
    shape.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[94](#bib.bib94)] 提出了一个全局模块，该模块计算某个位置的响应作为所有位置特征的加权和，提供了聚合全局特征的解决方案，但全局点对点映射仍可能不足以提取点云形状所隐含的基本模式。
- en: 'Yan et al. [[95](#bib.bib95)] propose an end-to-end network PointASNL that
    combines an adaptive sampling module (AS) and a local non-local module (L-NL),
    which can effectively deal with noisy point clouds. The AS module updates the
    features of the points by reasoning, then normalizes the weight parameters and
    re-weights the initial sampling points, which can effectively alleviate the bias
    effect. The L-NL module consists of local and non-local units of points, reducing
    the sensitivity of the learning process to noise. The PointASNL architecture is
    shown in Fig. [11](#S3.F11 "Figure 11 ‣ 2 ‣ III-C2 Global feature aggregation
    ‣ III-C Point cloud-based method ‣ III Point cloud classification method based
    on deep learning ‣ Deep Learning-based 3D Point Cloud Classification: A Systematic
    Survey and Outlook").'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Yan等人[[95](#bib.bib95)]提出了一种端到端网络PointASNL，该网络结合了自适应采样模块（AS）和局部非局部模块（L-NL），能够有效处理噪声点云。AS模块通过推理更新点的特征，然后对权重参数进行归一化，并重新加权初始采样点，从而有效缓解偏差效应。L-NL模块由局部和非局部点单元组成，减少了学习过程对噪声的敏感性。PointASNL架构如图[11](#S3.F11
    "图11 ‣ 2 ‣ III-C2 全局特征聚合 ‣ III-C 基于点云的方法 ‣ III 基于深度学习的点云分类方法 ‣ 基于深度学习的3D点云分类：系统综述与展望")所示。
- en: '![Refer to caption](img/c5771018cc87042b3fc8298277adc886.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c5771018cc87042b3fc8298277adc886.png)'
- en: 'Figure 11: Architecture of PointASNL[[95](#bib.bib95)]'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：PointASNL的架构[[95](#bib.bib95)]
- en: 'Li et al. [[96](#bib.bib96)] adopted some CNN methods to support a deep GCN
    architecture, called DeepGCN, which consists of three blocks: GCN Backbone block
    for input point cloud feature transformation, fusion block for generating and
    fusing global features, MLP block prediction block used to predict labels. To
    solve the problem of gradient disappearance during GCN training, it is possible
    to train deeper GCN networks.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Li等人[[96](#bib.bib96)]采用了一些CNN方法来支持深度GCN架构，称为DeepGCN，该架构由三个块组成：用于输入点云特征转换的GCN
    Backbone块、用于生成和融合全局特征的融合块、用于预测标签的MLP预测块。为了在GCN训练过程中解决梯度消失的问题，有可能训练更深的GCN网络。
- en: Xiang et al. [[97](#bib.bib97)] proposed a method based on aggregating hypothetical
    curves in point clouds, CurveNet, and effectively implemented the aggregation
    strategy, including a curve grouping operator and a curve aggregation operator.
    The network consists of a bunch of building blocks, FPS represents the farthest
    point sampling method.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Xiang等人[[97](#bib.bib97)]提出了一种基于点云中聚合假设曲线的方法——CurveNet，并有效实现了聚合策略，包括曲线分组操作符和曲线聚合操作符。该网络由一系列构建块组成，FPS表示最远点采样方法。
- en: '3'
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '3'
- en: RNN or LSTM-based methods
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 基于RNN或LSTM的方法
- en: RNN (Recurrent Neural Network) can usually effectively utilize context information
    to process sequence data. LSTM (Long Short-term Memory) is a special RNN, which
    can effectively solve the problems of gradient disappearance and gradient explosion
    in the training process of longer sequence data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: RNN（递归神经网络）通常能够有效利用上下文信息来处理序列数据。LSTM（长短期记忆网络）是一个特殊的RNN，它能够有效解决长序列数据训练过程中的梯度消失和梯度爆炸问题。
- en: Engelmann et al. [[98](#bib.bib98)] extended the input-level context information
    and output-level context information based on PointNet, which enables PointNet
    to be applied in large-scale scenarios. It can make PointNet applied in large-scale
    scenarios.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Engelmann等人[[98](#bib.bib98)]扩展了基于PointNet的输入级上下文信息和输出级上下文信息，使PointNet能够应用于大规模场景。这样可以使PointNet在大规模场景中得到应用。
- en: Liu et al. [[99](#bib.bib99)] proposed a 3DCNN-DQN-RNN method, which fuses 3D
    Convolutional Neural Network (CNN), Deep Q Network (DQN) and residual Recurrent
    Neural Network (RNN). First, the feature representation of points is obtained
    by 3DCNN. Second, DQN can detect and localize objects, and can automatically perceive
    the scene and adjust the feedback of 3DCNN. Finally, the RNN is used to identify
    the connections and differences of multi-scale features. Where LSTM (Long Short-term
    Memory) units are used to prevent vanishing gradients and make the network have
    long-term memory, the method improves the accuracy of processing large-scale point
    clouds.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人[[99](#bib.bib99)]提出了一种3DCNN-DQN-RNN方法，该方法融合了3D卷积神经网络（CNN）、深度Q网络（DQN）和残差递归神经网络（RNN）。首先，通过3DCNN获得点的特征表示。其次，DQN可以检测和定位对象，并能够自动感知场景并调整3DCNN的反馈。最后，RNN用于识别多尺度特征的连接和差异。LSTM（长短期记忆网络）单元用于防止梯度消失，使网络具有长期记忆，这种方法提高了处理大规模点云的准确性。
- en: The RSNet network proposed by Huang et al. [[100](#bib.bib100)] takes the original
    point cloud as input, and then performs feature extraction, and then passes through
    the slice pooling layers in the three directions of x, y, and z. Each layer uses
    a bidirectional RNN to extract local features, and then uses slice parsing. The
    layer assigns the features of the point cloud sequence to each point, and finally
    outputs the predicted semantic label of each point.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 黄等人提出的RSNet网络[[100](#bib.bib100)]以原始点云作为输入，然后进行特征提取，再通过x、y和z三个方向的切片池化层。每一层使用双向RNN提取局部特征，然后通过切片解析层将点云序列的特征分配给每个点，最后输出每个点的预测语义标签。
- en: Ye et al. [[101](#bib.bib101)] proposed an end-to-end semantic segmentation
    method, 3P-RNN, by combining CNN and RNN, which consists of a point pyramid module
    and a bidirectional hierarchical RNN module. In the work of distinguishing the
    same semantics, this method has certain limitations.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 叶等人[[101](#bib.bib101)]提出了一种端到端的语义分割方法3P-RNN，通过结合CNN和RNN，包括一个点金字塔模块和一个双向层次RNN模块。在区分相同语义的工作中，该方法具有一定的局限性。
- en: Point2Sequence proposed by Liu et al. [[102](#bib.bib102)] uses RNN to capture
    fine-grained contextual information to learn 3D shape features, it introduces
    an attention mechanism to enhance feature extraction.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人提出的Point2Sequence [[102](#bib.bib102)] 使用RNN捕捉细粒度的上下文信息以学习三维形状特征，并引入了注意机制以增强特征提取。
- en: III-D Polymorphic Fusion-based methods
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 多形态融合方法
- en: The strategy of combining PointGrid with grids proposed by Le et al. [[103](#bib.bib103)]
    is to mix points and grids for representation. PointGrid is composed of several
    convolution blocks, which represent the features of different layers through maximum
    pooling. Each convolution layer includes a convolution kernel, and the over-fitting
    phenomenon is controlled by the pooling layer, and then completed by full connection.
    For inference, PointGrid has two fully connected layers, and finally performs
    regression with a softmax classifier, which can better identify fine-grained models
    and represent local shapes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Le等人提出的将PointGrid与网格结合的策略[[103](#bib.bib103)]，是将点和网格混合进行表示。PointGrid由几个卷积块组成，通过最大池化表示不同层的特征。每个卷积层包含一个卷积核，过拟合现象通过池化层控制，然后通过全连接层完成。对于推理，PointGrid具有两个全连接层，最后通过softmax分类器进行回归，可以更好地识别细粒度模型并表示局部形状。
- en: Zhang et al. [[104](#bib.bib104)] proposed a novel point cloud learning method,
    PVT (Point-Voxel Transformer), combining sparse window attention module (SWA)
    and relative attention module (RA), which combines voxel-based and point-based
    model idea, this method excels in the accuracy of point cloud classification.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 张等人[[104](#bib.bib104)]提出了一种新颖的点云学习方法PVT（点-体素变换器），结合了稀疏窗口注意模块（SWA）和相对注意模块（RA），结合了基于体素和基于点的模型思想，该方法在点云分类的准确性方面表现出色。
- en: 'PointCLIP proposed by Zhang et al. [[105](#bib.bib105)] learns point clouds
    based on pre-trained CLIP. Encoding the point cloud by projecting it into a multi-view
    depth map without rendering, enables zero-shot recognition by transferring the
    2D pretrained knowledge to the 3D domain. And an inter-view adapter is designed
    to better extract global features. The network architecture is shown in Fig. [12](#S3.F12
    "Figure 12 ‣ III-D Polymorphic Fusion-based methods ‣ III Point cloud classification
    method based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification:
    A Systematic Survey and Outlook").'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '张等人提出的PointCLIP[[105](#bib.bib105)]基于预训练的CLIP学习点云。通过将点云投影到多视角深度图中进行编码，无需渲染，使得通过将二维预训练知识转移到三维领域，实现零样本识别。并且设计了一个视图间适配器，以更好地提取全局特征。网络架构如图[12](#S3.F12
    "Figure 12 ‣ III-D Polymorphic Fusion-based methods ‣ III Point cloud classification
    method based on deep learning ‣ Deep Learning-based 3D Point Cloud Classification:
    A Systematic Survey and Outlook")所示。'
- en: '![Refer to caption](img/ad1ff5843a6af6faf59cc24a3934f7a7.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ad1ff5843a6af6faf59cc24a3934f7a7.png)'
- en: 'Figure 12: PointCLIP[[105](#bib.bib105)] architecture'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：PointCLIP[[105](#bib.bib105)]架构
- en: CrossPoint [[106](#bib.bib106)] achieves 2D-to-3D correspondence by maximizing
    the point cloud and the corresponding rendered 2D image in invariant space and
    keeping the point cloud unchanged across transformations.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: CrossPoint [[106](#bib.bib106)] 通过最大化点云与对应渲染的二维图像在不变空间中的对应关系，并保持点云在变换中的不变，实现了二维到三维的对应。
- en: 'Summary: Compared with the multi-view-based method and the voxel-based method,
    the point cloud-based method directly processes the original points and can make
    full use of the point cloud information. Therefore, the point cloud-based method
    is also a future research direction, and the transformer-based method will be
    more widely used in the future.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：与基于多视角的方法和基于体素的方法相比，点云方法直接处理原始点，并能充分利用点云信息。因此，点云方法也是未来的研究方向，基于变换器的方法将在未来得到更广泛的应用。
- en: IV Evaluation
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 评估
- en: The evaluation index is used to evaluate the performance of the point cloud
    classification method. The accuracy, space complexity, execution time, etc. are
    the evaluation indexes of the method, and the accuracy is the key index for evaluating
    various methods. Generally, accuracy (Acc), precision (Pre), recall (Rec), and
    intersection-over-union (IoU) are used to evaluate the accuracy of the method.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标用于评估点云分类方法的性能。准确率、空间复杂度、执行时间等都是方法的评估指标，而准确率是评估各种方法的关键指标。一般来说，准确率（Acc）、精确率（Pre）、召回率（Rec）和交集-并集比（IoU）用于评估方法的准确性。
- en: $\bullet$ Accuracy refers to the ratio of the number of correctly predicted
    samples to the total number of predicted samples.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 准确率指的是正确预测样本的数量与总预测样本数量的比率。
- en: $\bullet$ The accuracy rate refers to the proportion of the true positive class
    that is predicted as a positive class.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 精确率指的是实际正类中被预测为正类的比例。
- en: $\bullet$ Recall refers to the ratio of samples predicted to be positive classes
    to the total number of true positive classes.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 召回率指的是预测为正类的样本与实际正类样本总数的比率。
- en: $\bullet$ The intersection ratio refers to the ratio of the intersection and
    union of the predicted value and the true value.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 交集比指的是预测值与真实值的交集和并集的比率。
- en: 'The accuracy, precision, recall, and intersection ratio can be calculated by
    the following formulas: TP is the sample that is predicted to be a positive class
    and is actually a positive class, TN is a sample that is predicted to be a positive
    class and is actually a negative class, and FP is the sample. The samples that
    are predicted to be negative classes are actually negative classes, and FN is
    the samples that are predicted to be negative classes and are actually positive
    classes. Suppose there are N classes:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率、精确率、召回率和交集比可以通过以下公式计算：TP是预测为正类且实际为正类的样本，TN是预测为正类但实际为负类的样本，FP是预测为负类但实际为负类的样本，FN是预测为负类但实际为正类的样本。假设有N个类别：
- en: 'The accuracy of the i-th class:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 第i类的准确率：
- en: '|  | $Acc_{i}=\frac{TP_{i}+TN_{i}}{TP_{i}+TN_{i}+FP_{i}+FN_{i}}$ |  | (1) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $Acc_{i}=\frac{TP_{i}+TN_{i}}{TP_{i}+TN_{i}+FP_{i}+FN_{i}}$ |  | (1) |'
- en: 'The precision of the i-th class:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 第i类的精确率：
- en: '|  | $\operatorname{Pre}_{i}=\frac{TP_{i}}{TP_{i}+FP_{i}}$ |  | (2) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{Pre}_{i}=\frac{TP_{i}}{TP_{i}+FP_{i}}$ |  | (2) |'
- en: 'Recall:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率：
- en: '|  | $Rec=\frac{TP}{TP+FN}$ |  | (3) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $Rec=\frac{TP}{TP+FN}$ |  | (3) |'
- en: 'The intersection ratio of the i-th class:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 第i类的交集比：
- en: '|  | $IoU_{i}=\frac{TP_{i}}{TP_{i}+FP_{i}+FN_{i}}$ |  | (4) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $IoU_{i}=\frac{TP_{i}}{TP_{i}+FP_{i}+FN_{i}}$ |  | (4) |'
- en: 'The current accuracy of point cloud classification is measured by indicators:
    overall accuracy (OA), average accuracy (MA), and average intersection-over-union
    ratio (mIoU), which are calculated as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当前点云分类的准确率通过指标来衡量：总体准确率（OA）、平均准确率（MA）和平均交集-并集比（mIoU），其计算方式如下：
- en: '|  | $OA=\frac{\sum_{i=1}^{N}TP_{i}}{\sum_{i=1}^{N}\left(TP_{i}+FP_{i}\right)}$
    |  | (5) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $OA=\frac{\sum_{i=1}^{N}TP_{i}}{\sum_{i=1}^{N}\left(TP_{i}+FP_{i}\right)}$
    |  | (5) |'
- en: '|  | $MA=\frac{1}{N}\sum_{i=1}^{N}Acc_{i}$ |  | (6) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | $MA=\frac{1}{N}\sum_{i=1}^{N}Acc_{i}$ |  | (6) |'
- en: '|  | $mIoU=\frac{1}{N}\sum_{i=1}^{N}IoU$ |  | (7) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | $mIoU=\frac{1}{N}\sum_{i=1}^{N}IoU$ |  | (7) |'
- en: 'This section summarizes the performance of the mentioned methods on different
    datasets in Table. [II](#S4.T2 "TABLE II ‣ IV Evaluation ‣ Deep Learning-based
    3D Point Cloud Classification: A Systematic Survey and Outlook").'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '本节总结了不同数据集上提到的方法的性能，详见表格[II](#S4.T2 "TABLE II ‣ IV Evaluation ‣ Deep Learning-based
    3D Point Cloud Classification: A Systematic Survey and Outlook")。'
- en: 'TABLE II: Accuracy comparison of point cloud classification methods on different
    datasets'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：不同数据集上点云分类方法的准确率比较
- en: '| Methods | Year | Accuracy/% on different datasets |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 在不同数据集上的准确率/% |'
- en: '| ModelNet 40 | ModelNet 10 | ScanObjectNN | ShapeNet |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| ModelNet 40 | ModelNet 10 | ScanObjectNN | ShapeNet |'
- en: '| OA | MA | mIoU | OA | MA | mIoU | OA | MA | mIoU | OA | MA | mIoU |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| OA | MA | mIoU | OA | MA | mIoU | OA | MA | mIoU | OA | MA | mIoU |'
- en: '| Multi-view based methods | MVCNN[[30](#bib.bib30)] | 2016 | 90.10 | 78.90
    | - | - | - | - | - | - | - | - | - | - |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 基于多视角的方法 | MVCNN[[30](#bib.bib30)] | 2016 | 90.10 | 78.90 | - | - | - | -
    | - | - | - | - | - | - |'
- en: '| GIFT[[31](#bib.bib31)] | 2016 | 83.10 | 81.94 | - | 92.35 | 91.12 | - | -
    | - | - | - | - | - |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| GIFT[[31](#bib.bib31)] | 2016 | 83.10 | 81.94 | - | 92.35 | 91.12 | - | -
    | - | - | - | - | - |'
- en: '| GCVNN[[32](#bib.bib32)] | 2018 | 93.10 | 79.70 | - | - | - | - | - | - |
    - | - | - | - |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| GCVNN[[32](#bib.bib32)] | 2018 | 93.10 | 79.70 | - | - | - | - | - | - |
    - | - | - | - |'
- en: '| MHBN[[34](#bib.bib34)] | 2018 | 94.91 | - | - | 92.93 | - | - | - | - | -
    | - | - | - |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| MHBN[[34](#bib.bib34)] | 2018 | 94.91 | - | - | 92.93 | - | - | - | - | -
    | - | - | - |'
- en: '| RCPCNN[[33](#bib.bib33)] | 2019 | 93.80 | - | - | - | - | - | - | - | - |
    - | - | - |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| RCPCNN[[33](#bib.bib33)] | 2019 | 93.80 | - | - | - | - | - | - | - | - |
    - | - | - |'
- en: '| MVTN[[35](#bib.bib35)] | 2021 | 92.00 | 93.80 | - | - | - | - | 82.80 | -
    | - | - | - | - |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| MVTN[[35](#bib.bib35)] | 2021 | 92.00 | 93.80 | - | - | - | - | 82.80 | -
    | - | - | - | - |'
- en: '| Voxel-based methods | VoxNet[[36](#bib.bib36)] | 2015 | 85.90 | 83.00 | -
    | - | 92.00 | - | - | - | - | - | - | - |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 基于体素的方法 | VoxNet[[36](#bib.bib36)] | 2015 | 85.90 | 83.00 | - | - | 92.00
    | - | - | - | - | - | - | - |'
- en: '| 3D shapeNet[[23](#bib.bib23)] | 2015 | 84.70 | 77.30 | - | - | 83.50 | -
    | - | - | - | - | - | - |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 3D shapeNet[[23](#bib.bib23)] | 2015 | 84.70 | 77.30 | - | - | 83.50 | -
    | - | - | - | - | - | - |'
- en: '| FPNN[[37](#bib.bib37)] | 2016 | 87.50 | - | - | - | - | - | - | - | - | -
    | - | - |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| FPNN[[37](#bib.bib37)] | 2016 | 87.50 | - | - | - | - | - | - | - | - | -
    | - | - |'
- en: '| OctNet[[38](#bib.bib38)] | 2017 | 86.50 | - | - | 90.90 | - | - | - | - |
    - | - | - | - |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| OctNet[[38](#bib.bib38)] | 2017 | 86.50 | - | - | 90.90 | - | - | - | - |
    - | - | - | - |'
- en: '| O-CNN[[39](#bib.bib39)] | 2017 | 90.60 | - | 85.90 | - | - | - | - | - |
    - | - | - | - |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| O-CNN[[39](#bib.bib39)] | 2017 | 90.60 | - | 85.90 | - | - | - | - | - |
    - | - | - | - |'
- en: '| Kd-Net[[40](#bib.bib40)] | 2017 | 91.80 | 88.50 | - | 94.00 | 93.50 | 77.20
    | - | - | - | - | - | 82.30 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Kd-Net[[40](#bib.bib40)] | 2017 | 91.80 | 88.50 | - | 94.00 | 93.50 | 77.20
    | - | - | - | - | - | 82.30 |'
- en: '| 3D Context Net[[41](#bib.bib41)] | 2018 | - | - | - | - | - | - | - | - |
    - | - | - | - |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 3D Context Net[[41](#bib.bib41)] | 2018 | - | - | - | - | - | - | - | - |
    - | - | - | - |'
- en: '| MSNet[[42](#bib.bib42)] | 2018 | - | - | - | - | - | - | - | - | - | - |
    - | - |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| MSNet[[42](#bib.bib42)] | 2018 | - | - | - | - | - | - | - | - | - | - |
    - | - |'
- en: '| VV-Net[[43](#bib.bib43)] | 2019 | - | - | - | - | - | - | - | - | - | - |
    - | 87.40 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| VV-Net[[43](#bib.bib43)] | 2019 | - | - | - | - | - | - | - | - | - | - |
    - | 87.40 |'
- en: '| Point cloud-based method | Local feature aggregation | Point-by-point methods
    | PointNet++[[44](#bib.bib44)] | 2017 | 91.90 | - | - | - | - | - | 77.90 | 75.40
    | - | - | - | 85.10 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 基于点云的方法 | 局部特征聚合 | 点对点方法 | PointNet++[[44](#bib.bib44)] | 2017 | 91.90 |
    - | - | - | - | - | 77.90 | 75.40 | - | - | - | 85.10 |'
- en: '| PointNeXt[[45](#bib.bib45)] | 2022 | - | - | - | - | - | - | 87.70 | 85.80
    | - | - | - | 87.20 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| PointNeXt[[45](#bib.bib45)] | 2022 | - | - | - | - | - | - | 87.70 | 85.80
    | - | - | - | 87.20 |'
- en: '| PointWeb[[46](#bib.bib46)] | 2019 | 92.30 | 89.40 | - | - | - | - | - | -
    | - | - | - | - |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| PointWeb[[46](#bib.bib46)] | 2019 | 92.30 | 89.40 | - | - | - | - | - | -
    | - | - | - | - |'
- en: '| RandLA-Net[[47](#bib.bib47)] | 2020 | - | - | - | - | - | - | - | - | - |
    - | - | - |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| RandLA-Net[[47](#bib.bib47)] | 2020 | - | - | - | - | - | - | - | - | - |
    - | - | - |'
- en: '| SO-Net[[48](#bib.bib48)] | 2018 | 90.80 | 87.30 | - | 94.10 | 93.90 | - |
    - | - | - | - | - | 84.60 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| SO-Net[[48](#bib.bib48)] | 2018 | 90.80 | 87.30 | - | 94.10 | 93.90 | - |
    - | - | - | - | - | 84.60 |'
- en: '| GDANet[[49](#bib.bib49)] | 2021 | 93.80 | - | - | - | - | - | 88.50 | - |
    - | - | - | 86.50 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| GDANet[[49](#bib.bib49)] | 2021 | 93.80 | - | - | - | - | - | 88.50 | - |
    - | - | - | 86.50 |'
- en: '| SimpleView[[50](#bib.bib50)] | 2020 | 93.00 | - | - | - | - | - | 80.50 |
    - | - | - | - | - |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| SimpleView[[50](#bib.bib50)] | 2020 | 93.00 | - | - | - | - | - | 80.50 |
    - | - | - | - | - |'
- en: '| PintSCNet[[51](#bib.bib51)] | 2021 | 93.70 | - | - | - | - | - | - | - |
    - | - | - | - |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| PintSCNet[[51](#bib.bib51)] | 2021 | 93.70 | - | - | - | - | - | - | - |
    - | - | - | - |'
- en: '|  | PointMLP[[52](#bib.bib52)] | 2022 | 94.50 | 91.40 | - | - | - | - | 85.40
    | 83.90 | - | - | - | - |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | PointMLP[[52](#bib.bib52)] | 2022 | 94.50 | 91.40 | - | - | - | - | 85.40
    | 83.90 | - | - | - | - |'
- en: '|  | RepSurf[[54](#bib.bib54)] | 2022 | 94.70 | 91.70 | - | - | - | - | 84.60
    | 81.90 | - | - | - | - |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | RepSurf[[54](#bib.bib54)] | 2022 | 94.70 | 91.70 | - | - | - | - | 84.60
    | 81.90 | - | - | - | - |'
- en: '| Convolution based methods | PCNN[[55](#bib.bib55)] | 2018 | 92.30 | - | -
    | 94.90 | - | - | - | - | - | - | - | - |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 基于卷积的方法 | PCNN[[55](#bib.bib55)] | 2018 | 92.30 | - | - | 94.90 | - | - |
    - | - | - | - | - | - |'
- en: '| RS-CNN[[56](#bib.bib56)] | 2019 | 93.60 | - | - | - | - | - | - | - | - |
    - | - | 86.20 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| RS-CNN[[56](#bib.bib56)] | 2019 | 93.60 | - | - | - | - | - | - | - | - |
    - | - | 86.20 |'
- en: '| DNNSP[[58](#bib.bib58)] | 2018 | - | - | - | - | - | - | - | - | - | - |
    - | - |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| DNNSP[[58](#bib.bib58)] | 2018 | - | - | - | - | - | - | - | - | - | - |
    - | - |'
- en: '| A-CNN[[59](#bib.bib59)] | 2019 | 92.60 | 90.30 | - | 95.50 | 95.30 | - |
    - | - | - | - | - | 86.10 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| A-CNN[[59](#bib.bib59)] | 2019 | 92.60 | 90.30 | - | 95.50 | 95.30 | - |
    - | - | - | - | - | 86.10 |'
- en: '| RP-Net[[60](#bib.bib60)] | 2021 | 94.10 | - | - | - | - | - | - | - | - |
    - | - | - |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| RP-Net[[60](#bib.bib60)] | 2021 | 94.10 | - | - | - | - | - | - | - | - |
    - | - | - |'
- en: '| SCN[[61](#bib.bib61)] | 2018 | 90.00 | - | - | - | - | - | - | - | - | -
    | - | 84.60 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| SCN[[61](#bib.bib61)] | 2018 | 90.00 | - | - | - | - | - | - | - | - | -
    | - | 84.60 |'
- en: '| PointCNN[[62](#bib.bib62)] | 2018 | 92.20 | 88.10 | - | - | - | - | 87.90
    | - | - | - | - | - |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| PointCNN[[62](#bib.bib62)] | 2018 | 92.20 | 88.10 | - | - | - | - | 87.90
    | - | - | - | - | - |'
- en: '| PointCove[[63](#bib.bib63)] | 2019 | 92.50 | - | - | - | - | - | - | - |
    - | - | - | - |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| PointCove[[63](#bib.bib63)] | 2019 | 92.50 | - | - | - | - | - | - | - |
    - | - | - | - |'
- en: '| MCCNN[[64](#bib.bib64)] | 2018 | 90.90 | - | - | - | - | - | - | - | - |
    - | - | - |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| MCCNN[[64](#bib.bib64)] | 2018 | 90.90 | - | - | - | - | - | - | - | - |
    - | - | - |'
- en: '| SpiderCNN[[65](#bib.bib65)] | 2018 | 92.40 | - | - | - | - | - | 73.70 |
    69.80 | - | - | - | 85.30 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| SpiderCNN[[65](#bib.bib65)] | 2018 | 92.40 | - | - | - | - | - | 73.70 |
    69.80 | - | - | - | 85.30 |'
- en: '| InterpCNN[[66](#bib.bib66)] | 2019 | 93.00 | - | - | - | - | - | - | - |
    - | - | - | 86.30 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| InterpCNN[[66](#bib.bib66)] | 2019 | 93.00 | - | - | - | - | - | - | - |
    - | - | - | 86.30 |'
- en: '| SPHNet[[68](#bib.bib68)] | 2019 | 87.70 | - | - | - | - | - | - | - | - |
    - | - | - |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| SPHNet[[68](#bib.bib68)] | 2019 | 87.70 | - | - | - | - | - | - | - | - |
    - | - | - |'
- en: '|  | SC-CNN[[69](#bib.bib69)] | 2022 | 93.8 | - | - | 96.4 | - | - | - | -
    | - | - | - | 86.40 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | SC-CNN[[69](#bib.bib69)] | 2022 | 93.8 | - | - | 96.4 | - | - | - | -
    | - | - | - | 86.40 |'
- en: '| Graph-based methods | ECC[[73](#bib.bib73)] | 2017 | - | 83.20 | - | - |
    90.00 | - | - | - | - | - | - | - |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 基于图的方法 | ECC[[73](#bib.bib73)] | 2017 | - | 83.20 | - | - | 90.00 | - | -
    | - | - | - | - | - |'
- en: '| SpecGCN[[74](#bib.bib74)] | 2018 | 91.50 | - | - | - | - | - | - | - | -
    | - | - | 84.60 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| SpecGCN[[74](#bib.bib74)] | 2018 | 91.50 | - | - | - | - | - | - | - | -
    | - | - | 84.60 |'
- en: '| Grid-GCN[[75](#bib.bib75)] | 2020 | 93.10 | 91.30 | - | 97.50 | 97.40 | -
    | - | - | - | - | - | - |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Grid-GCN[[75](#bib.bib75)] | 2020 | 93.10 | 91.30 | - | 97.50 | 97.40 | -
    | - | - | - | - | - | - |'
- en: '| PointView-GCN[[76](#bib.bib76)] | 2021 | 95.40 | - | - | - | - | - | 85.50
    | - | - | - | - | - |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| PointView-GCN[[76](#bib.bib76)] | 2021 | 95.40 | - | - | - | - | - | 85.50
    | - | - | - | - | - |'
- en: '| DGCNN[[77](#bib.bib77)] | 2019 | 92.20 | 90.20 | - | - | - | - | 86.20 |
    - | - | - | - | - |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| DGCNN[[77](#bib.bib77)] | 2019 | 92.20 | 90.20 | - | - | - | - | 86.20 |
    - | - | - | - | - |'
- en: '| LDGCNN[[78](#bib.bib78)] | 2019 | 92.90 | 90.30 | - | - | - | - | - | - |
    - | - | - | - |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| LDGCNN[[78](#bib.bib78)] | 2019 | 92.90 | 90.30 | - | - | - | - | - | - |
    - | - | - | - |'
- en: '| PointNGCNN[[79](#bib.bib79)] | 2020 | 92.80 | - | - | - | - | - | - | - |
    - | - | - | 85.60 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| PointNGCNN[[79](#bib.bib79)] | 2020 | 92.80 | - | - | - | - | - | - | - |
    - | - | - | 85.60 |'
- en: '| attention-based methods | PAT[[80](#bib.bib80)] | 2019 | 91.70 | - | - |
    - | - | - | - | - | - | - | - | - |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 基于注意力的方法 | PAT[[80](#bib.bib80)] | 2019 | 91.70 | - | - | - | - | - | - |
    - | - | - | - | - |'
- en: '| FPA/GAU[[81](#bib.bib81)] | 2018 | - | - | - | - | - | - | - | - | - | -
    | - | - |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| FPA/GAU[[81](#bib.bib81)] | 2018 | - | - | - | - | - | - | - | - | - | -
    | - | - |'
- en: '| LSANet[[82](#bib.bib82)] | 2019 | 92.30 | 89.20 | - | - | - | - | - | - |
    - | - | - | 83.20 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| LSANet[[82](#bib.bib82)] | 2019 | 92.30 | 89.20 | - | - | - | - | - | - |
    - | - | - | 83.20 |'
- en: '|  | GACNet[[83](#bib.bib83)] | 2019 | - | - | - | - | - | - | - | - | - |
    - | - | - |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | GACNet[[83](#bib.bib83)] | 2019 | - | - | - | - | - | - | - | - | - |
    - | - | - |'
- en: '|  | GAPointNet[[84](#bib.bib84)] | 2021 | 92.40 | 89.70 | - | - | - | - |
    - | - | - | - | - | - |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | GAPointNet[[84](#bib.bib84)] | 2021 | 92.40 | 89.70 | - | - | - | - |
    - | - | - | - | - | - |'
- en: '| Global feature aggregation | Transformer-based methods | Point Transformer[[86](#bib.bib86)]
    | 2021 | 92.80 | - | - | - | - | - | - | - | - | 85.90 | - | - |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 全局特征聚合 | 基于Transformer的方法 | Point Transformer[[86](#bib.bib86)] | 2021 |
    92.80 | - | - | - | - | - | - | - | - | 85.90 | - | - |'
- en: '| Visual Transformer[[88](#bib.bib88)] | 2020 | - | - | - | - | - | - | - |
    - | - | - | - | - |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 视觉Transformer[[88](#bib.bib88)] | 2020 | - | - | - | - | - | - | - | - |
    - | - | - | - |'
- en: '| PCT[[90](#bib.bib90)] | 2021 | 93.20 | - | - | - | - | - | - | - | - | -
    | - | - |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| PCT[[90](#bib.bib90)] | 2021 | 93.20 | - | - | - | - | - | - | - | - | -
    | - | - |'
- en: '| 3DMedPT[[12](#bib.bib12)] | 2021 | 93.40 | - | - | - | - | - | - | - | -
    | - | - | - |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 3DMedPT[[12](#bib.bib12)] | 2021 | 93.40 | - | - | - | - | - | - | - | -
    | - | - | - |'
- en: '| Point-BERT[[91](#bib.bib91)] | 2021 | 93.80 | - | - | - | - | - | 83.07 |
    - | - | - | - | - |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Point-BERT[[91](#bib.bib91)] | 2021 | 93.80 | - | - | - | - | - | 83.07 |
    - | - | - | - | - |'
- en: '| Point-TnT[[87](#bib.bib87)] | 2022 | 92.60 | - | - | - | - | - | 84.60 |
    82.60 | - | - | - | - |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Point-TnT[[87](#bib.bib87)] | 2022 | 92.60 | - | - | - | - | - | 84.60 |
    82.60 | - | - | - | - |'
- en: '|  | Point-MAE[[92](#bib.bib92)] | 2022 | 93.80 | - | - | - | - | - | 85.18
    | - | - | - | - | 86.10 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | Point-MAE[[92](#bib.bib92)] | 2022 | 93.80 | - | - | - | - | - | 85.18
    | - | - | - | - | 86.10 |'
- en: '| Global module-based methods | PointNet[[21](#bib.bib21)] | 2017 | 89.20 |
    86.20 | - | - | - | - | 68.20 | 63.40 | - | - | - | 83.70 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 基于全局模块的方法 | PointNet[[21](#bib.bib21)] | 2017 | 89.20 | 86.20 | - | - | -
    | - | 68.20 | 63.40 | - | - | - | 83.70 |'
- en: '| PointASNL[[95](#bib.bib95)] | 2020 | 93.20 | - | - | 95.90 | - | - | - |
    - | - | - | - | - |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| PointASNL[[95](#bib.bib95)] | 2020 | 93.20 | - | - | 95.90 | - | - | - |
    - | - | - | - | - |'
- en: '| CurveNet[[97](#bib.bib97)] | 2021 | 94.20 | - | - | 96.30 | - | - | - | -
    | - | - | - | 86.80 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| CurveNet[[97](#bib.bib97)] | 2021 | 94.20 | - | - | 96.30 | - | - | - | -
    | - | - | - | 86.80 |'
- en: '|  | DeepGCNs[[96](#bib.bib96)] | 2021 | 93.20 | 90.30 | - | - | - | - | -
    | - | - | - | - | - |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | DeepGCNs[[96](#bib.bib96)] | 2021 | 93.20 | 90.30 | - | - | - | - | -
    | - | - | - | - | - |'
- en: '|  | RNN or LSTM based methods | 3DCNN-DQN-RNN[[99](#bib.bib99)] | 2017 | -
    | - | - | - | - | - | - | - | - | - | - | - |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于 RNN 或 LSTM 的方法 | 3DCNN-DQN-RNN[[99](#bib.bib99)] | 2017 | - | - | -
    | - | - | - | - | - | - | - | - | - |'
- en: '|  |  | RSNet[[100](#bib.bib100)] | 2018 | - | - | - | - | - | - | - | - |
    - | - | - | - |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  |  | RSNet[[100](#bib.bib100)] | 2018 | - | - | - | - | - | - | - | - |
    - | - | - | - |'
- en: '|  |  | 3P-RNN[[101](#bib.bib101)] | 2018 | - | - | - | - | - | - | - | - |
    - | - | - | - |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 3P-RNN[[101](#bib.bib101)] | 2018 | - | - | - | - | - | - | - | - |
    - | - | - | - |'
- en: '|  |  | Point2Sequence[[102](#bib.bib102)] | 2019 | 92.60 | 90.40 | - | 95.30
    | 95.10 | - | - | - | - | - | - | - |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Point2Sequence[[102](#bib.bib102)] | 2019 | 92.60 | 90.40 | - | 95.30
    | 95.10 | - | - | - | - | - | - | - |'
- en: '| Polymorphic Fusion based methods | PointGrid[[103](#bib.bib103)] | 2018 |
    92.00 | 88.90 | - | - | - | - | - | - | - | 86.10 | 80.50 | - |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Polymorphic Fusion 基于的方法 | PointGrid[[103](#bib.bib103)] | 2018 | 92.00 |
    88.90 | - | - | - | - | - | - | - | 86.10 | 80.50 | - |'
- en: '| PVT[[104](#bib.bib104)] | 2021 | 94.00 | - | - | - | - | - | - | - | - |
    - | - | 86.60 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| PVT[[104](#bib.bib104)] | 2021 | 94.00 | - | - | - | - | - | - | - | - |
    - | - | 86.60 |'
- en: '| PointCLIP[[105](#bib.bib105)] | 2022 | 94.05 | - | - | - | - | - | - | -
    | - | - | - | - |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| PointCLIP[[105](#bib.bib105)] | 2022 | 94.05 | - | - | - | - | - | - | -
    | - | - | - | - |'
- en: '| CrossPoint[[106](#bib.bib106)] | 2022 | 94.90 | - | - | - | - | - | 79.00
    | - | - | - | - | 85.50 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| CrossPoint[[106](#bib.bib106)] | 2022 | 94.90 | - | - | - | - | - | 79.00
    | - | - | - | - | 85.50 |'
- en: V Future trends
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 未来趋势
- en: 'With the development of today’s technology, the demand for point cloud classification
    methods in various fields is getting higher and higher. There are many methods
    of point cloud classification, researchers are constantly proposing new methods
    to improve accuracy and efficiency. This continues to drive the development of
    3D applications. However, the existing methods still have different limitations.
    This section will summarize the problems in the current deep learning-based point
    cloud classification methods and prospect the future research directions, details
    as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 随着今天技术的发展，各个领域对点云分类方法的需求越来越高。点云分类方法有很多，研究人员不断提出新方法以提高准确性和效率。这推动了3D应用的发展。然而，现有的方法仍然存在不同的局限性。本节将总结当前基于深度学习的点云分类方法中的问题，并展望未来的研究方向，具体如下：
- en: $\bullet$ Some of the current point cloud classification methods focus on improving
    accuracy, while others focus on improving efficiency. We need to solve the problem
    of ”how to achieve high accuracy while being efficient?”
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 当前的一些点云分类方法侧重于提高准确性，而另一些则侧重于提高效率。我们需要解决“如何在保持效率的同时实现高准确性？”的问题。
- en: $\bullet$ In the practical application of 3D, the information structure of outdoor
    scenes is complex and changeable. Although some methods have been applied in outdoor
    scenes, the efficiency and accuracy need to be further improved. Therefore, future
    research direction should focus more on the network for outdoor scenes.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 在3D的实际应用中，户外场景的信息结构复杂且多变。虽然一些方法已经应用于户外场景，但效率和准确性仍需进一步提高。因此，未来的研究方向应更多地关注户外场景的网络。
- en: $\bullet$ From the above data, the method based on the original point cloud
    has certain advantages in algorithm performance. But the network model is more
    complicated, because the input original point cloud data has information integrity,
    therefore simple point cloud-based methods are a future research trend.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 从上述数据可以看出，基于原始点云的方法在算法性能上具有一定优势。但由于输入的原始点云数据具有信息完整性，网络模型变得更加复杂，因此简单的点云方法是未来的研究趋势。
- en: $\bullet$ Most of the current methods are aimed at improving rather than changing,
    so the innovative type of methods needs our continued research and discussion.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 目前大多数方法旨在改进而非改变，因此需要我们持续研究和讨论创新型方法。
- en: $\bullet$ In the network model we will propose in the future, we should pay
    attention to the optimization of the network architecture, which can reduce the
    computational complexity and memory usage while facing the complex and irregular
    point cloud.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 在我们未来提出的网络模型中，我们应关注网络架构的优化，这可以在面对复杂和不规则的点云时减少计算复杂度和内存使用。
- en: VI Conclusion
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: This paper provides a comprehensive survey and discussion of deep learning-based
    point cloud classification methods in recent years.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了近年来基于深度学习的点云分类方法的全面调查和讨论。
- en: 'First, we introduced the point cloud and its application in the introduction
    and discussed the characteristics and processing difficulties of the point cloud.
    In the second section, the 3D data were introduced, and the commonly used 3D data
    representations, point cloud data storage formats and point cloud classification
    datasets are summarized. Building on the previous sections, we comprehensively
    review deep learning-based point cloud classification methods, classifying these
    methods into four broad categories: multi-view-based methods, voxel-based methods,
    point cloud-based methods, and polymorphic fusion-based methods. And then we compare
    the performance of existing methods. Finally, this paper points out the problems
    of the current method and prospects the future research direction.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在介绍部分介绍了点云及其应用，并讨论了点云的特性和处理难点。在第二部分中，介绍了 3D 数据，并总结了常用的 3D 数据表示、点云数据存储格式和点云分类数据集。在前述部分的基础上，我们全面回顾了基于深度学习的点云分类方法，将这些方法分为四大类：基于多视角的方法、基于体素的方法、基于点云的方法和多态融合方法。接着，我们比较了现有方法的性能。最后，本文指出了当前方法的问题，并展望了未来的研究方向。
- en: Acknowledgment
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported by Xinjiang University School-Enterprise Joint Project.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了新疆大学校企联合项目的支持。
- en: References
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] L. Zhang, L. Sun, W. Li, J. Zhang, W. Cai, C. Cheng, and X. Ning, “A joint
    bayesian framework based on partial least squares discriminant analysis for finger
    vein recognition,” *IEEE Sensors Journal*, vol. 22, no. 1, pp. 785–794, 2021.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] L. Zhang, L. Sun, W. Li, J. Zhang, W. Cai, C. Cheng, 和 X. Ning, “基于偏最小二乘判别分析的联合贝叶斯框架用于指静脉识别，”
    *IEEE 传感器期刊*，第 22 卷，第 1 期，第 785–794 页，2021 年。'
- en: '[2] L. Zhou, X. Bai, X. Liu, J. Zhou, and E. R. Hancock, “Learning binary code
    for fast nearest subspace search,” *Pattern Recognition*, vol. 98, p. 107040,
    2020.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] L. Zhou, X. Bai, X. Liu, J. Zhou, 和 E. R. Hancock, “用于快速最近子空间搜索的二进制码学习，”
    *模式识别*，第 98 卷，第 107040 页，2020 年。'
- en: '[3] X. Ning, W. Tian, Z. Yu, W. Li, and X. Bai, “Hcfnn: High-order coverage
    function neural network for image classification,” *Pattern Recognition*, p. 108873,
    2022.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] X. Ning, W. Tian, Z. Yu, W. Li, 和 X. Bai, “HCFNN：用于图像分类的高阶覆盖函数神经网络，” *模式识别*，第
    108873 页，2022 年。'
- en: '[4] C. Wang, C. Wang, W. Li, and H. Wang, “A brief survey on rgb-d semantic
    segmentation using deep learning,” *Displays*, vol. 70, p. 102080, 2021.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] C. Wang, C. Wang, W. Li, 和 H. Wang, “基于深度学习的 RGB-D 语义分割简要调查，” *Displays*，第
    70 卷，第 102080 页，2021 年。'
- en: '[5] C. Wang, H. Wang, X. Ning, S. Tian, and W. Li, “3d point cloud classification
    method based on dynamic coverage of local area,” *Journal of Software*, vol. 34,
    no. 4, pp. 1962–1976, 2022.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] C. Wang, H. Wang, X. Ning, S. Tian, 和 W. Li, “基于局部区域动态覆盖的 3D 点云分类方法，” *软件期刊*，第
    34 卷，第 4 期，第 1962–1976 页，2022 年。'
- en: '[6] W. Cai, D. Liu, X. Ning, C. Wang, and G. Xie, “Voxel-based three-view hybrid
    parallel network for 3d object classification,” *Displays*, vol. 69, p. 102076,
    2021.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] W. Cai, D. Liu, X. Ning, C. Wang, 和 G. Xie, “基于体素的三视角混合并行网络用于 3D 目标分类，”
    *Displays*，第 69 卷，第 102076 页，2021 年。'
- en: '[7] C. Yan, G. Pang, X. Bai, C. Liu, X. Ning, L. Gu, and J. Zhou, “Beyond triplet
    loss: person re-identification with fine-grained difference-aware pairwise loss,”
    *IEEE Transactions on Multimedia*, vol. 24, pp. 1665–1677, 2021.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. Yan, G. Pang, X. Bai, C. Liu, X. Ning, L. Gu, 和 J. Zhou, “超越三元组损失：具有细粒度差异感知对损失的人物重识别，”
    *IEEE 多媒体学报*，第 24 卷，第 1665–1677 页，2021 年。'
- en: '[8] X. Ning, K. Gong, W. Li, and L. Zhang, “Jwsaa: joint weak saliency and
    attention aware for person re-identification,” *Neurocomputing*, vol. 453, pp.
    801–811, 2021.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] X. Ning, K. Gong, W. Li, 和 L. Zhang, “JWSAA：联合弱显著性和注意力感知的人物重识别，” *神经计算*，第
    453 卷，第 801–811 页，2021 年。'
- en: '[9] W. Zhangyu, Y. Guizhen, W. Xinkai, L. Haoran, and L. Da, “A camera and
    lidar data fusion method for railway object detection,” *IEEE Sensors Journal*,
    vol. 21, no. 12, pp. 13 442–13 454, 2021.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] W. Zhangyu, Y. Guizhen, W. Xinkai, L. Haoran, 和 L. Da, “用于铁路物体检测的相机和激光雷达数据融合方法，”
    *IEEE 传感器期刊*，第 21 卷，第 12 期，第 13 442–13 454 页，2021 年。'
- en: '[10] Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud based
    3d object detection,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 4490–4499.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Zhou 和 O. Tuzel, “VoxelNet：用于点云的端到端学习的 3D 目标检测，” 在 *IEEE 计算机视觉与模式识别会议论文集*，2018
    年，第 4490–4499 页。'
- en: '[11] J. Guo, X. Yao, M. Shen, J. Wang, and W. Liao, “A deep learning network
    for point cloud of medicine structure,” in *2018 9th International Conference
    on Information Technology in Medicine and Education (ITME)*.   IEEE, 2018, pp.
    683–687.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Guo, X. Yao, M. Shen, J. Wang 和 W. Liao, “一种用于医学结构点云的深度学习网络”，见于 *2018年第9届信息技术医学与教育国际会议（ITME）*。IEEE，2018年，第683–687页。'
- en: '[12] J. Yu, C. Zhang, H. Wang, D. Zhang, Y. Song, T. Xiang, D. Liu, and W. Cai,
    “3d medical point transformer: Introducing convolution to attention networks for
    medical point cloud analysis,” *arXiv preprint arXiv:2112.04863*, 2021.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] J. Yu, C. Zhang, H. Wang, D. Zhang, Y. Song, T. Xiang, D. Liu 和 W. Cai,
    “三维医学点变换器：将卷积引入注意力网络用于医学点云分析”，*arXiv预印本arXiv:2112.04863*，2021年。'
- en: '[13] T. Xu, D. An, Y. Jia, and Y. Yue, “A review: Point cloud-based 3d human
    joints estimation,” *Sensors*, vol. 21, no. 5, p. 1684, 2021.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] T. Xu, D. An, Y. Jia 和 Y. Yue, “综述：基于点云的三维人体关节估计”，*传感器*，第21卷，第5期，第1684页，2021年。'
- en: '[14] B. Yang, R. Huang, J. Li, M. Tian, W. Dai, and R. Zhong, “Automated reconstruction
    of building lods from airborne lidar point clouds using an improved morphological
    scale space,” *Remote Sensing*, vol. 9, no. 1, p. 14, 2016.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] B. Yang, R. Huang, J. Li, M. Tian, W. Dai 和 R. Zhong, “基于改进的形态学尺度空间从航空激光雷达点云自动重建建筑LOD”，*遥感*，第9卷，第1期，第14页，2016年。'
- en: '[15] M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt, and B. Scholkopf, “Support
    vector machines,” *IEEE Intelligent Systems and their applications*, vol. 13,
    no. 4, pp. 18–28, 1998.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt 和 B. Scholkopf, “支持向量机”，*IEEE智能系统及其应用*，第13卷，第4期，第18–28页，1998年。'
- en: '[16] R. Kumar and R. Verma, “Classification algorithms for data mining: A survey,”
    *International Journal of Innovations in Engineering and Technology (IJIET)*,
    vol. 1, no. 2, pp. 7–14, 2012.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] R. Kumar 和 R. Verma, “数据挖掘分类算法：综述”，*工程与技术创新国际杂志（IJIET）*，第1卷，第2期，第7–14页，2012年。'
- en: '[17] Z. Gan, L. Zhong, Y. Li, and H. Guan, “A random forest based method for
    urban object classification using lidar data and aerial imagery,” in *2015 23rd
    International Conference on Geoinformatics*.   IEEE, 2015, pp. 1–4.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Z. Gan, L. Zhong, Y. Li 和 H. Guan, “一种基于随机森林的城市物体分类方法，使用激光雷达数据和航空影像”，见于
    *2015年第23届国际地理信息学会议*。IEEE，2015年，第1–4页。'
- en: '[18] L. Zhou, Y. Liu, Z. Pencheng, B. Xiao, Y. Yazhou, J. Zhou, G. Lin, T. Harada,
    and E. R. Hancock, “Information bottleneck and selective noise supervision for
    zero-shot learning,” *Machine Learning*, 2022.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] L. Zhou, Y. Liu, Z. Pencheng, B. Xiao, Y. Yazhou, J. Zhou, G. Lin, T.
    Harada 和 E. R. Hancock, “信息瓶颈和选择性噪声监督用于零样本学习”，*机器学习*，2022年。'
- en: '[19] N. Plath, M. Toussaint, and S. Nakajima, “Multi-class image segmentation
    using conditional random fields and global classification,” in *Proceedings of
    the 26th annual international conference on machine learning*, 2009, pp. 817–824.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] N. Plath, M. Toussaint 和 S. Nakajima, “使用条件随机场和全局分类的多类图像分割”，见于 *第26届年度国际机器学习会议论文集*，2009年，第817–824页。'
- en: '[20] D. Munoz, J. A. Bagnell, N. Vandapel, and M. Hebert, “Contextual classification
    with functional max-margin markov networks,” in *2009 IEEE Conference on Computer
    Vision and Pattern Recognition*.   IEEE, 2009, pp. 975–982.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] D. Munoz, J. A. Bagnell, N. Vandapel 和 M. Hebert, “基于功能最大边际马尔可夫网络的上下文分类”，见于
    *2009 IEEE计算机视觉与模式识别会议*。IEEE，2009年，第975–982页。'
- en: '[21] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 652–660.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] C. R. Qi, H. Su, K. Mo 和 L. J. Guibas, “Pointnet：用于三维分类和分割的点集深度学习”，见于
    *IEEE计算机视觉与模式识别会议论文集*，2017年，第652–660页。'
- en: '[22] X. Bai, J. Zhou, X. Ning, and C. Wang, “3d data computation and visualization,”
    p. 102169, 2022.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] X. Bai, J. Zhou, X. Ning 和 C. Wang, “三维数据计算与可视化”，第102169页，2022年。'
- en: '[23] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3d
    shapenets: A deep representation for volumetric shapes,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2015, pp. 1912–1920.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang 和 J. Xiao, “三维ShapeNet：用于体积形状的深度表示”，见于
    *IEEE计算机视觉与模式识别会议论文集*，2015年，第1912–1920页。'
- en: '[24] J. Sun, Q. Zhang, B. Kailkhura, Z. Yu, C. Xiao, and Z. M. Mao, “Benchmarking
    robustness of 3d point cloud recognition against common corruptions,” *arXiv preprint
    arXiv:2201.12296*, 2022.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Sun, Q. Zhang, B. Kailkhura, Z. Yu, C. Xiao 和 Z. M. Mao, “基准测试三维点云识别对常见损坏的鲁棒性”，*arXiv预印本arXiv:2201.12296*，2022年。'
- en: '[25] M. De Deuge, A. Quadros, C. Hung, and B. Douillard, “Unsupervised feature
    learning for classification of outdoor 3d scans,” in *Australasian conference
    on robitics and automation*, vol. 2.   University of New South Wales Kensington,
    Australia, 2013, p. 1.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] M. De Deuge, A. Quadros, C. Hung, 和 B. Douillard，“用于户外3D扫描分类的无监督特征学习，”见于*澳大利亚机器人与自动化会议*，第2卷。新南威尔士大学肯辛顿，澳大利亚，2013年，第1页。'
- en: '[26] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, H. Su *et al.*, “Shapenet: An information-rich 3d model repository,”
    *arXiv preprint arXiv:1512.03012*, 2015.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S.
    Savarese, M. Savva, S. Song, H. Su *等*，“ShapeNet：一个信息丰富的3D模型库，”*arXiv预印本 arXiv:1512.03012*，2015年。'
- en: '[27] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in *Proc. Computer
    Vision and Pattern Recognition (CVPR), IEEE*, 2017.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, 和 M. Nießner，“ScanNet：丰富标注的室内场景3D重建，”见于*计算机视觉与模式识别（CVPR）会议论文集，IEEE*，2017年。'
- en: '[28] M. A. Uy, Q.-H. Pham, B.-S. Hua, D. T. Nguyen, and S.-K. Yeung, “Revisiting
    point cloud classification: A new benchmark dataset and classification model on
    real-world data,” in *International Conference on Computer Vision (ICCV)*, 2019.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] M. A. Uy, Q.-H. Pham, B.-S. Hua, D. T. Nguyen, 和 S.-K. Yeung，“重访点云分类：基于真实数据的新基准数据集和分类模型，”见于*国际计算机视觉大会（ICCV）*，2019年。'
- en: '[29] C. Wang, X. Wang, J. Zhang, L. Zhang, X. Bai, X. Ning, J. Zhou, and E. Hancock,
    “Uncertainty estimation for stereo matching based on evidential deep learning,”
    *Pattern Recognition*, vol. 124, p. 108498, 2022.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] C. Wang, X. Wang, J. Zhang, L. Zhang, X. Bai, X. Ning, J. Zhou, 和 E. Hancock，“基于证据深度学习的立体匹配不确定性估计，”*模式识别*，第124卷，第108498页，2022年。'
- en: '[30] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-view convolutional
    neural networks for 3d shape recognition,” in *IEEE International Conference on
    Computer Vision*, 2015.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] H. Su, S. Maji, E. Kalogerakis, 和 E. Learned-Miller，“用于3D形状识别的多视角卷积神经网络，”见于*IEEE国际计算机视觉会议*，2015年。'
- en: '[31] B. Song, B. Xiang, Z. Zhou, Z. Zhang, and L. J. Latecki, “Gift: A real-time
    and scalable 3d shape search engine,” in *2016 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2016.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] B. Song, B. Xiang, Z. Zhou, Z. Zhang, 和 L. J. Latecki，“GIFT：一个实时且可扩展的3D形状搜索引擎，”见于*2016年IEEE计算机视觉与模式识别会议（CVPR）*，2016年。'
- en: '[32] Y. Feng, Z. Zhang, X. Zhao, R. Ji, and G. Yue, “Gvcnn: Group-view convolutional
    neural networks for 3d shape recognition,” in *2018 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2018.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Y. Feng, Z. Zhang, X. Zhao, R. Ji, 和 G. Yue，“GVCNN：用于3D形状识别的组视角卷积神经网络，”见于*2018年IEEE/CVF计算机视觉与模式识别会议*，2018年。'
- en: '[33] C. Wang, M. Pelillo, and K. Siddiqi, “Dominant set clustering and pooling
    for multi-view 3d object recognition,” *arXiv preprint arXiv:1906.01592*, 2019.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] C. Wang, M. Pelillo, 和 K. Siddiqi，“用于多视角3D物体识别的主导集聚类与池化，”*arXiv预印本 arXiv:1906.01592*，2019年。'
- en: '[34] T. Yu, J. Meng, and J. Yuan, “Multi-view harmonized bilinear network for
    3d object recognition,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 186–194.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] T. Yu, J. Meng, 和 J. Yuan，“用于3D物体识别的多视角和谐双线性网络，”见于*IEEE计算机视觉与模式识别会议论文集*，2018年，第186–194页。'
- en: '[35] A. Hamdi, S. Giancola, and B. Ghanem, “Mvtn: Multi-view transformation
    network for 3d shape recognition,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 1–11.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] A. Hamdi, S. Giancola, 和 B. Ghanem，“MVTN：用于3D形状识别的多视角变换网络，”见于*IEEE/CVF国际计算机视觉会议论文集*，2021年，第1–11页。'
- en: '[36] D. Maturana and S. Scherer, “Voxnet: A 3d convolutional neural network
    for real-time object recognition,” in *2015 IEEE/RSJ international conference
    on intelligent robots and systems (IROS)*.   IEEE, 2015, pp. 922–928.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] D. Maturana 和 S. Scherer，“VoxNet：用于实时物体识别的3D卷积神经网络，”见于*2015年IEEE/RSJ国际智能机器人与系统会议（IROS）*。IEEE，2015年，第922–928页。'
- en: '[37] Y. Li, S. Pirk, H. Su, C. R. Qi, and L. J. Guibas, “Fpnn: Field probing
    neural networks for 3d data,” *Advances in neural information processing systems*,
    vol. 29, 2016.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Y. Li, S. Pirk, H. Su, C. R. Qi, 和 L. J. Guibas，“FPNN：用于3D数据的场探测神经网络，”*神经信息处理系统进展*，第29卷，2016年。'
- en: '[38] G. Riegler, A. Osman Ulusoy, and A. Geiger, “Octnet: Learning deep 3d
    representations at high resolutions,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2017, pp. 3577–3586.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] G. Riegler, A. Osman Ulusoy, 和 A. Geiger，“OctNet：在高分辨率下学习深度3D表示，”见于*IEEE计算机视觉与模式识别会议论文集*，2017年，第3577–3586页。'
- en: '[39] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong, “O-cnn: Octree-based
    convolutional neural networks for 3d shape analysis,” *ACM Transactions On Graphics
    (TOG)*, vol. 36, no. 4, pp. 1–11, 2017.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, 和 X. Tong, “O-cnn: 基于八叉树的卷积神经网络用于3D形状分析，”
    *ACM图形学会会刊（TOG）*，第36卷，第4期，第1–11页，2017年。'
- en: '[40] R. Klokov and V. Lempitsky, “Escape from cells: Deep kd-networks for the
    recognition of 3d point cloud models,” in *Proceedings of the IEEE international
    conference on computer vision*, 2017, pp. 863–872.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] R. Klokov 和 V. Lempitsky, “逃离细胞：用于3D点云模型识别的深度kd网络，” 在 *IEEE国际计算机视觉大会论文集*
    中，2017年，第863–872页。'
- en: '[41] W. Zeng and T. Gevers, “3dcontextnet: Kd tree guided hierarchical learning
    of point clouds using local and global contextual cues,” in *Proceedings of the
    European Conference on Computer Vision (ECCV) Workshops*, 2018, pp. 0–0.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] W. Zeng 和 T. Gevers, “3dcontextnet: 使用局部和全局上下文线索的kd树引导的点云层次学习，” 在 *欧洲计算机视觉会议（ECCV）研讨会*
    中，2018年，第0–0页。'
- en: '[42] L. Wang, Y. Huang, J. Shan, and L. He, “Msnet: Multi-scale convolutional
    network for point cloud classification,” *Remote Sensing*, vol. 10, no. 4, p.
    612, 2018.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] L. Wang, Y. Huang, J. Shan, 和 L. He, “Msnet: 用于点云分类的多尺度卷积网络，” *遥感*，第10卷，第4期，第612页，2018年。'
- en: '[43] H.-Y. Meng, L. Gao, Y.-K. Lai, and D. Manocha, “Vv-net: Voxel vae net
    with group convolutions for point cloud segmentation,” in *Proceedings of the
    IEEE/CVF international conference on computer vision*, 2019, pp. 8500–8508.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] H.-Y. Meng, L. Gao, Y.-K. Lai, 和 D. Manocha, “Vv-net: 带有组卷积的体素vae网络用于点云分割，”
    在 *IEEE/CVF国际计算机视觉大会论文集* 中，2019年，第8500–8508页。'
- en: '[44] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
    feature learning on point sets in a metric space,” *Advances in neural information
    processing systems*, vol. 30, 2017.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] C. R. Qi, L. Yi, H. Su, 和 L. J. Guibas, “Pointnet++: 在度量空间中的点集上的深层次层次特征学习，”
    *神经信息处理系统进展*，第30卷，2017年。'
- en: '[45] G. Qian, Y. Li, H. Peng, J. Mai, H. A. A. K. Hammoud, M. Elhoseiny, and
    B. Ghanem, “Pointnext: Revisiting pointnet++ with improved training and scaling
    strategies,” *arXiv preprint arXiv:2206.04670*, 2022.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] G. Qian, Y. Li, H. Peng, J. Mai, H. A. A. K. Hammoud, M. Elhoseiny, 和
    B. Ghanem, “Pointnext: 通过改进的训练和扩展策略重新审视Pointnet++，” *arXiv预印本 arXiv:2206.04670*，2022年。'
- en: '[46] H. Zhao, L. Jiang, C.-W. Fu, and J. Jia, “Pointweb: Enhancing local neighborhood
    features for point cloud processing,” in *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*, 2019, pp. 5565–5573.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] H. Zhao, L. Jiang, C.-W. Fu, 和 J. Jia, “Pointweb: 增强点云处理的局部邻域特征，” 在 *IEEE/CVF计算机视觉与模式识别大会论文集*
    中，2019年，第5565–5573页。'
- en: '[47] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, and A. Markham,
    “Randla-net: Efficient semantic segmentation of large-scale point clouds,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 11 108–11 117.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, 和 A. Markham,
    “Randla-net: 大规模点云的高效语义分割，” 在 *IEEE/CVF计算机视觉与模式识别大会论文集* 中，2020年，第11 108–11 117页。'
- en: '[48] J. Li, B. M. Chen, and G. H. Lee, “So-net: Self-organizing network for
    point cloud analysis,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 9397–9406.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] J. Li, B. M. Chen, 和 G. H. Lee, “So-net: 自组织网络用于点云分析，” 在 *IEEE计算机视觉与模式识别大会论文集*
    中，2018年，第9397–9406页。'
- en: '[49] M. Xu, J. Zhang, Z. Zhou, M. Xu, X. Qi, and Y. Qiao, “Learning geometry-disentangled
    representation for complementary understanding of 3d object point cloud,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 35, no. 4, 2021, pp.
    3056–3064.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] M. Xu, J. Zhang, Z. Zhou, M. Xu, X. Qi, 和 Y. Qiao, “学习几何解耦表示以补充理解3D对象点云，”
    在 *AAAI人工智能大会论文集* 中，第35卷，第4期，2021年，第3056–3064页。'
- en: '[50] A. Goyal, H. Law, B. Liu, A. Newell, and J. Deng, “Revisiting point cloud
    classification with a simple and effective baseline,” 2020.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] A. Goyal, H. Law, B. Liu, A. Newell, 和 J. Deng, “重新审视点云分类的简单有效基线，” 2020年。'
- en: '[51] X. Chen, Y. Wu, W. Xu, J. Li, H. Dong, and Y. Chen, “Pointscnet: Point
    cloud structure and correlation learning based on space-filling curve-guided sampling,”
    *Symmetry*, vol. 14, no. 1, p. 8, 2021.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] X. Chen, Y. Wu, W. Xu, J. Li, H. Dong, 和 Y. Chen, “Pointscnet: 基于空间填充曲线引导采样的点云结构和相关性学习，”
    *对称性*，第14卷，第1期，第8页，2021年。'
- en: '[52] X. Ma, C. Qin, H. You, H. Ran, and Y. Fu, “Rethinking network design and
    local geometry in point cloud: A simple residual mlp framework,” *arXiv preprint
    arXiv:2202.07123*, 2022.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] X. Ma, C. Qin, H. You, H. Ran, 和 Y. Fu, “重新思考点云中的网络设计和局部几何：一个简单的残差mlp框架，”
    *arXiv预印本 arXiv:2202.07123*，2022年。'
- en: '[53] Q. Huang, X. Dong, D. Chen, H. Zhou, W. Zhang, and N. Yu, “Shape-invariant
    3d adversarial point clouds,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022, pp. 15 335–15 344.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Q. Huang, X. Dong, D. Chen, H. Zhou, W. Zhang, 和 N. Yu，"形状不变的3D对抗点云"，在*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，第15 335–15 344页。'
- en: '[54] H. Ran, J. Liu, and C. Wang, “Surface representation for point clouds,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 18 942–18 952.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] H. Ran, J. Liu, 和 C. Wang，"点云的表面表示"，在*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，第18 942–18 952页。'
- en: '[55] M. Atzmon, H. Maron, and Y. Lipman, “Point convolutional neural networks
    by extension operators,” *arXiv preprint arXiv:1803.10091*, 2018.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] M. Atzmon, H. Maron, 和 Y. Lipman，"通过扩展算子进行点卷积神经网络"，*arXiv预印本 arXiv:1803.10091*，2018年。'
- en: '[56] Y. Liu, B. Fan, S. Xiang, and C. Pan, “Relation-shape convolutional neural
    network for point cloud analysis,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 8895–8904.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Liu, B. Fan, S. Xiang, 和 C. Pan，"用于点云分析的关系形状卷积神经网络"，在*IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，第8895–8904页。'
- en: '[57] M. Yousefhussien, D. J. Kelbe, E. J. Ientilucci, and C. Salvaggio, “A
    multi-scale fully convolutional network for semantic labeling of 3d point clouds,”
    *ISPRS journal of photogrammetry and remote sensing*, vol. 143, pp. 191–204, 2018.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] M. Yousefhussien, D. J. Kelbe, E. J. Ientilucci, 和 C. Salvaggio，"用于3D点云语义标记的多尺度全卷积网络"，*ISPRS摄影测量与遥感杂志*，第143卷，第191–204页，2018年。'
- en: '[58] Z. Wang, L. Zhang, L. Zhang, R. Li, Y. Zheng, and Z. Zhu, “A deep neural
    network with spatial pooling (dnnsp) for 3-d point cloud classification,” *IEEE
    Transactions on Geoscience and Remote Sensing*, vol. 56, no. 8, pp. 4594–4604,
    2018.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Z. Wang, L. Zhang, L. Zhang, R. Li, Y. Zheng, 和 Z. Zhu，"具有空间池化（dnnsp）的深度神经网络用于3D点云分类"，*IEEE地球科学与遥感杂志*，第56卷，第8期，第4594–4604页，2018年。'
- en: '[59] A. Komarichev, Z. Zhong, and J. Hua, “A-cnn: Annularly convolutional neural
    networks on point clouds,” in *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*, 2019, pp. 7421–7430.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] A. Komarichev, Z. Zhong, 和 J. Hua，"A-cnn: 点云上的环形卷积神经网络"，在*IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，第7421–7430页。'
- en: '[60] H. Ran, W. Zhuo, J. Liu, and L. Lu, “Learning inner-group relations on
    point clouds,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2021, pp. 15 477–15 487.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] H. Ran, W. Zhuo, J. Liu, 和 L. Lu，"点云上的内组关系学习"，在*IEEE/CVF国际计算机视觉会议论文集*，2021年，第15 477–15 487页。'
- en: '[61] S. Xie, S. Liu, Z. Chen, and Z. Tu, “Attentional shapecontextnet for point
    cloud recognition,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 4606–4615.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] S. Xie, S. Liu, Z. Chen, 和 Z. Tu，"用于点云识别的注意力形状上下文网络"，在*IEEE计算机视觉与模式识别会议论文集*，2018年，第4606–4615页。'
- en: '[62] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, “Pointcnn: Convolution
    on x-transformed points,” *Advances in neural information processing systems*,
    vol. 31, 2018.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, 和 B. Chen，"Pointcnn: X-变换点上的卷积"，*神经信息处理系统进展*，第31卷，2018年。'
- en: '[63] W. Wu, Z. Qi, and L. Fuxin, “Pointconv: Deep convolutional networks on
    3d point clouds,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2019, pp. 9621–9630.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] W. Wu, Z. Qi, 和 L. Fuxin，"Pointconv: 深度卷积网络在3D点云上的应用"，在*IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，第9621–9630页。'
- en: '[64] P. Hermosilla, T. Ritschel, P.-P. Vázquez, À. Vinacua, and T. Ropinski,
    “Monte carlo convolution for learning on non-uniformly sampled point clouds,”
    *ACM Transactions on Graphics (TOG)*, vol. 37, no. 6, pp. 1–12, 2018.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] P. Hermosilla, T. Ritschel, P.-P. Vázquez, À. Vinacua, 和 T. Ropinski，"蒙特卡洛卷积用于非均匀采样点云的学习"，*ACM
    Transactions on Graphics (TOG)*，第37卷，第6期，第1–12页，2018年。'
- en: '[65] Y. Xu, T. Fan, M. Xu, L. Zeng, and Y. Qiao, “Spidercnn: Deep learning
    on point sets with parameterized convolutional filters,” in *Proceedings of the
    European Conference on Computer Vision (ECCV)*, 2018, pp. 87–102.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Y. Xu, T. Fan, M. Xu, L. Zeng, 和 Y. Qiao，"Spidercnn: 使用参数化卷积滤波器的点集深度学习"，在*欧洲计算机视觉会议（ECCV）论文集*，2018年，第87–102页。'
- en: '[66] J. Mao, X. Wang, and H. Li, “Interpolated convolutional networks for 3d
    point cloud understanding,” in *Proceedings of the IEEE/CVF international conference
    on computer vision*, 2019, pp. 1578–1587.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. Mao, X. Wang, 和 H. Li，"用于3D点云理解的插值卷积网络"，在*IEEE/CVF国际计算机视觉会议论文集*，2019年，第1578–1587页。'
- en: '[67] C. Esteves, C. Allen-Blanchette, A. Makadia, and K. Daniilidis, “Learning
    so (3) equivariant representations with spherical cnns,” in *Proceedings of the
    European Conference on Computer Vision (ECCV)*, 2018, pp. 52–68.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] C. Esteves, C. Allen-Blanchette, A. Makadia, 和 K. Daniilidis，“通过球面CNN学习SO(3)等变表示”，见于*欧洲计算机视觉会议
    (ECCV)*，2018，第52–68页。'
- en: '[68] A. Poulenard, M.-J. Rakotosaona, Y. Ponty, and M. Ovsjanikov, “Effective
    rotation-invariant point cnn with spherical harmonics kernels,” in *2019 International
    Conference on 3D Vision (3DV)*.   IEEE, 2019, pp. 47–56.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] A. Poulenard, M.-J. Rakotosaona, Y. Ponty, 和 M. Ovsjanikov，“具有球面调和核的有效旋转不变点CNN”，见于*2019年国际三维视觉会议
    (3DV)*。IEEE，2019，第47–56页。'
- en: '[69] C. Wang, X. Ning, L. Sun, L. Zhang, W. Li, and X. Bai, “Learning discriminative
    features by covering local geometric space for point cloud analysis,” *IEEE Transactions
    on Geoscience and Remote Sensing*, vol. 60, pp. 1–15, 2022.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] C. Wang, X. Ning, L. Sun, L. Zhang, W. Li, 和 X. Bai，“通过覆盖局部几何空间学习判别特征用于点云分析”，*IEEE地球科学与遥感学报*，第60卷，第1–15页，2022。'
- en: '[70] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE transactions on neural networks*, vol. 20,
    no. 1, pp. 61–80, 2008.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, 和 G. Monfardini，“图神经网络模型”，*IEEE神经网络学报*，第20卷，第1期，第61–80页，2008。'
- en: '[71] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and locally
    connected networks on graphs,” *arXiv preprint arXiv:1312.6203*, 2013.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] J. Bruna, W. Zaremba, A. Szlam, 和 Y. LeCun，“图上的谱网络和局部连接网络”，*arXiv预印本 arXiv:1312.6203*，2013。'
- en: '[72] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” *arXiv preprint arXiv:1609.02907*, 2016.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] T. N. Kipf 和 M. Welling，“图卷积网络的半监督分类”，*arXiv预印本 arXiv:1609.02907*，2016。'
- en: '[73] M. Simonovsky and N. Komodakis, “Dynamic edge-conditioned filters in convolutional
    neural networks on graphs,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2017, pp. 3693–3702.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. Simonovsky 和 N. Komodakis，“图上的卷积神经网络中的动态边条件滤波器”，见于*IEEE计算机视觉与模式识别会议论文集*，2017，第3693–3702页。'
- en: '[74] C. Wang, B. Samari, and K. Siddiqi, “Local spectral graph convolution
    for point set feature learning,” in *Proceedings of the European conference on
    computer vision (ECCV)*, 2018, pp. 52–66.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] C. Wang, B. Samari, 和 K. Siddiqi，“用于点集特征学习的局部谱图卷积”，见于*欧洲计算机视觉会议 (ECCV)*，2018，第52–66页。'
- en: '[75] Q. Xu, X. Sun, C.-Y. Wu, P. Wang, and U. Neumann, “Grid-gcn for fast and
    scalable point cloud learning,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2020, pp. 5661–5670.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Q. Xu, X. Sun, C.-Y. Wu, P. Wang, 和 U. Neumann，“用于快速和可扩展点云学习的Grid-GCN”，见于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020，第5661–5670页。'
- en: '[76] S. S. Mohammadi, Y. Wang, and A. Del Bue, “Pointview-gcn: 3d shape classification
    with multi-view point clouds,” in *2021 IEEE International Conference on Image
    Processing (ICIP)*.   IEEE, 2021, pp. 3103–3107.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] S. S. Mohammadi, Y. Wang, 和 A. Del Bue，“PointView-GCN：使用多视角点云进行3D形状分类”，见于*2021年IEEE国际图像处理会议
    (ICIP)*。IEEE，2021，第3103–3107页。'
- en: '[77] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon,
    “Dynamic graph cnn for learning on point clouds,” *Acm Transactions On Graphics
    (tog)*, vol. 38, no. 5, pp. 1–12, 2019.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, 和 J. M. Solomon，“用于点云学习的动态图CNN”，*ACM图形学会会刊
    (TOG)*，第38卷，第5期，第1–12页，2019。'
- en: '[78] K. Zhang, M. Hao, J. Wang, C. W. de Silva, and C. Fu, “Linked dynamic
    graph cnn: Learning on point cloud via linking hierarchical features,” *arXiv
    preprint arXiv:1904.10014*, 2019.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] K. Zhang, M. Hao, J. Wang, C. W. de Silva, 和 C. Fu，“链接动态图CNN：通过链接层次特征在点云上学习”，*arXiv预印本
    arXiv:1904.10014*，2019。'
- en: '[79] Q. Lu, C. Chen, W. Xie, and Y. Luo, “Pointngcnn: Deep convolutional networks
    on 3d point clouds with neighborhood graph filters,” *Computers & Graphics*, vol. 86,
    pp. 42–51, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Q. Lu, C. Chen, W. Xie, 和 Y. Luo，“PointNGCNN：具有邻域图滤波器的3D点云深度卷积网络”，*计算机与图形学*，第86卷，第42–51页，2020。'
- en: '[80] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian, “Modeling
    point clouds with self-attention and gumbel subset sampling,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2019,
    pp. 3323–3332.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, 和 Q. Tian，“通过自注意力和Gumbel子集抽样对点云建模”，见于*IEEE/CVF计算机视觉与模式识别会议论文集*，2019，第3323–3332页。'
- en: '[81] H. Li, P. Xiong, J. An, and L. Wang, “Pyramid attention network for semantic
    segmentation,” *arXiv preprint arXiv:1805.10180*, 2018.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] H. Li, P. Xiong, J. An, 和 L. Wang，“用于语义分割的金字塔注意力网络”，*arXiv预印本 arXiv:1805.10180*，2018。'
- en: '[82] L.-Z. Chen, X.-Y. Li, D.-P. Fan, K. Wang, S.-P. Lu, and M.-M. Cheng, “Lsanet:
    Feature learning on point sets by local spatial aware layer,” *arXiv preprint
    arXiv:1905.05442*, 2019.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] L.-Z. Chen, X.-Y. Li, D.-P. Fan, K. Wang, S.-P. Lu, 和 M.-M. Cheng，“Lsanet:
    通过局部空间感知层进行点集特征学习，” *arXiv 预印本 arXiv:1905.05442*，2019。'
- en: '[83] L. Wang, Y. Huang, Y. Hou, S. Zhang, and J. Shan, “Graph attention convolution
    for point cloud semantic segmentation,” in *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*, 2019, pp. 10 296–10 305.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] L. Wang, Y. Huang, Y. Hou, S. Zhang, 和 J. Shan，“点云语义分割的图注意力卷积，” 在 *IEEE/CVF计算机视觉与模式识别会议论文集*，2019，第
    10 296–10 305 页。'
- en: '[84] C. Chen, L. Z. Fragonara, and A. Tsourdos, “Gapointnet: Graph attention
    based point neural network for exploiting local feature of point cloud,” *Neurocomputing*,
    vol. 438, pp. 122–132, 2021.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] C. Chen, L. Z. Fragonara, 和 A. Tsourdos，“Gapointnet: 基于图注意力的点神经网络，用于挖掘点云的局部特征，”
    *神经计算*，第 438 卷，第 122–132 页，2021。'
- en: '[85] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“注意力即全部，” *神经信息处理系统进展*，第 30 卷，2017。'
- en: '[86] N. Engel, V. Belagiannis, and K. Dietmayer, “Point transformer,” *IEEE
    Access*, vol. 9, pp. 134 826–134 840, 2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] N. Engel, V. Belagiannis, 和 K. Dietmayer，“点变换器，” *IEEE Access*，第 9 卷，第
    134 826–134 840 页，2021。'
- en: '[87] A. Berg, M. Oskarsson, and M. O’Connor, “Points to patches: Enabling the
    use of self-attention for 3d shape recognition,” *arXiv preprint arXiv:2204.03957*,
    2022.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] A. Berg, M. Oskarsson, 和 M. O’Connor，“点到补丁：使自注意力用于 3D 形状识别成为可能，” *arXiv
    预印本 arXiv:2204.03957*，2022。'
- en: '[88] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, Z. Yan, M. Tomizuka, J. Gonzalez,
    K. Keutzer, and P. Vajda, “Visual transformers: Token-based image representation
    and processing for computer vision,” *arXiv preprint arXiv:2006.03677*, 2020.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, Z. Yan, M. Tomizuka, J. Gonzalez,
    K. Keutzer, 和 P. Vajda，“视觉变换器：基于标记的图像表示与处理用于计算机视觉，” *arXiv 预印本 arXiv:2006.03677*，2020。'
- en: '[89] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko,
    “End-to-end object detection with transformers,” in *European conference on computer
    vision*.   Springer, 2020, pp. 213–229.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, 和 S. Zagoruyko，“端到端的物体检测与变换器，”
    在 *欧洲计算机视觉会议*。 Springer，2020，第 213–229 页。'
- en: '[90] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu,
    “Pct: Point cloud transformer,” *Computational Visual Media*, vol. 7, no. 2, pp.
    187–199, 2021.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, 和 S.-M. Hu，“Pct:
    点云变换器，” *计算视觉媒体*，第 7 卷，第 2 期，第 187–199 页，2021。'
- en: '[91] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, “Point-bert: Pre-training
    3d point cloud transformers with masked point modeling,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp. 19 313–19 322.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, 和 J. Lu，“Point-bert: 使用掩码点建模预训练
    3D 点云变换器，” 在 *IEEE/CVF计算机视觉与模式识别会议论文集*，2022，第 19 313–19 322 页。'
- en: '[92] Y. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, and L. Yuan, “Masked autoencoders
    for point cloud self-supervised learning,” *arXiv preprint arXiv:2203.06604*,
    2022.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Y. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, 和 L. Yuan，“用于点云自监督学习的掩码自编码器，”
    *arXiv 预印本 arXiv:2203.06604*，2022。'
- en: '[93] C. He, R. Li, S. Li, and L. Zhang, “Voxel set transformer: A set-to-set
    approach to 3d object detection from point clouds,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2022, pp. 8417–8427.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] C. He, R. Li, S. Li, 和 L. Zhang，“体素集变换器：一种从点云中进行 3D 物体检测的集合到集合的方法，” 在
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2022，第 8417–8427 页。'
- en: '[94] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,”
    in *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2018, pp. 7794–7803.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] X. Wang, R. Girshick, A. Gupta, 和 K. He，“非局部神经网络，” 在 *IEEE计算机视觉与模式识别大会论文集*，2018，第
    7794–7803 页。'
- en: '[95] X. Yan, C. Zheng, Z. Li, S. Wang, and S. Cui, “Pointasnl: Robust point
    clouds processing using nonlocal neural networks with adaptive sampling,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020,
    pp. 5589–5598.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] X. Yan, C. Zheng, Z. Li, S. Wang, 和 S. Cui，“Pointasnl: 使用自适应采样的非局部神经网络进行鲁棒的点云处理，”
    在 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020，第 5589–5598 页。'
- en: '[96] G. Li, M. Müller, G. Qian, I. C. D. Perez, A. Abualshour, A. K. Thabet,
    and B. Ghanem, “Deepgcns: Making gcns go as deep as cnns,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2021.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] G. Li, M. Müller, G. Qian, I. C. D. Perez, A. Abualshour, A. K. Thabet,
    和 B. Ghanem，"Deepgcns：让GCNs像CNNs一样深"，*IEEE模式分析与机器智能汇刊*，2021年。'
- en: '[97] T. Xiang, C. Zhang, Y. Song, J. Yu, and W. Cai, “Walk in the cloud: Learning
    curves for point clouds shape analysis,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 915–924.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] T. Xiang, C. Zhang, Y. Song, J. Yu, 和 W. Cai，"云中的行走：点云形状分析的学习曲线"，见于*IEEE/CVF国际计算机视觉会议论文集*，2021年，页915–924。'
- en: '[98] F. Engelmann, T. Kontogianni, A. Hermans, and B. Leibe, “Exploring spatial
    context for 3d semantic segmentation of point clouds,” in *Proceedings of the
    IEEE international conference on computer vision workshops*, 2017, pp. 716–724.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] F. Engelmann, T. Kontogianni, A. Hermans, 和 B. Leibe，"探索点云三维语义分割的空间上下文"，见于*IEEE国际计算机视觉研讨会论文集*，2017年，页716–724。'
- en: '[99] F. Liu, S. Li, L. Zhang, C. Zhou, R. Ye, Y. Wang, and J. Lu, “3dcnn-dqn-rnn:
    A deep reinforcement learning framework for semantic parsing of large-scale 3d
    point clouds,” in *Proceedings of the IEEE international conference on computer
    vision*, 2017, pp. 5678–5687.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] F. Liu, S. Li, L. Zhang, C. Zhou, R. Ye, Y. Wang, 和 J. Lu，"3dcnn-dqn-rnn：一个用于大规模三维点云语义解析的深度强化学习框架"，见于*IEEE国际计算机视觉会议论文集*，2017年，页5678–5687。'
- en: '[100] Q. Huang, W. Wang, and U. Neumann, “Recurrent slice networks for 3d segmentation
    of point clouds,” in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, 2018, pp. 2626–2635.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Q. Huang, W. Wang, 和 U. Neumann，"用于点云三维分割的递归切片网络"，见于*IEEE计算机视觉与模式识别会议论文集*，2018年，页2626–2635。'
- en: '[101] X. Ye, J. Li, H. Huang, L. Du, and X. Zhang, “3d recurrent neural networks
    with context fusion for point cloud semantic segmentation,” in *Proceedings of
    the European conference on computer vision (ECCV)*, 2018, pp. 403–417.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] X. Ye, J. Li, H. Huang, L. Du, 和 X. Zhang，"具有上下文融合的三维递归神经网络用于点云语义分割"，见于*欧洲计算机视觉会议（ECCV）论文集*，2018年，页403–417。'
- en: '[102] X. Liu, Z. Han, Y.-S. Liu, and M. Zwicker, “Point2sequence: Learning
    the shape representation of 3d point clouds with an attention-based sequence to
    sequence network,” in *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 33, no. 01, 2019, pp. 8778–8785.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] X. Liu, Z. Han, Y.-S. Liu, 和 M. Zwicker，"Point2sequence：使用基于注意力的序列到序列网络学习三维点云的形状表示"，见于*AAAI人工智能会议论文集*，第33卷，第01期，2019年，页8778–8785。'
- en: '[103] T. Le and Y. Duan, “Pointgrid: A deep network for 3d shape understanding,”
    in *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2018, pp. 9204–9214.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] T. Le 和 Y. Duan，"Pointgrid：一种用于三维形状理解的深度网络"，见于*IEEE计算机视觉与模式识别会议论文集*，2018年，页9204–9214。'
- en: '[104] C. Zhang, H. Wan, X. Shen, and Z. Wu, “Pvt: Point-voxel transformer for
    point cloud learning,” *arXiv preprint arXiv:2108.06076*, 2021.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] C. Zhang, H. Wan, X. Shen, 和 Z. Wu，"Pvt：用于点云学习的点-体素变换器"，*arXiv预印本 arXiv:2108.06076*，2021年。'
- en: '[105] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao,
    and H. Li, “Pointclip: Point cloud understanding by clip,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp.
    8552–8562.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao,
    和 H. Li，"Pointclip：通过clip进行点云理解"，见于*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，页8552–8562。'
- en: '[106] M. Afham, I. Dissanayake, D. Dissanayake, A. Dharmasiri, K. Thilakarathna,
    and R. Rodrigo, “Crosspoint: Self-supervised cross-modal contrastive learning
    for 3d point cloud understanding,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 9902–9912.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] M. Afham, I. Dissanayake, D. Dissanayake, A. Dharmasiri, K. Thilakarathna,
    和 R. Rodrigo，"Crosspoint：用于三维点云理解的自监督跨模态对比学习"，见于*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，页9902–9912。'
