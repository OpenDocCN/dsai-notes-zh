- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç±»åˆ«: æœªåˆ†ç±»'
- en: 'date: 2024-09-06 19:31:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ—¥æœŸ: 2024-09-06 19:31:30'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2407.08137] Abstract'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2407.08137] æ‘˜è¦'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2407.08137](https://ar5iv.labs.arxiv.org/html/2407.08137)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2407.08137](https://ar5iv.labs.arxiv.org/html/2407.08137)
- en: Survey on Fundamental Deep Learning 3D Reconstruction Techniques Yonge BaiÂ¹,
    LikHang WongÂ², TszYin TwanÂ²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºåŸºç¡€æ·±åº¦å­¦ä¹  3D é‡å»ºæŠ€æœ¯çš„è°ƒç ” ç”± Yonge BaiÂ¹ã€LikHang WongÂ²ã€TszYin TwanÂ²
- en: Â¹McMaster University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Â¹éº¦å…‹é©¬æ–¯ç‰¹å¤§å­¦
- en: Â²City University of Hong Kong
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Â²é¦™æ¸¯åŸå¸‚å¤§å­¦
- en: baiy58@mcmaster.ca, klhwong3-c@my.cityu.edu.hk, tytwan2-c@my.cityu.edu.hk July
    10, 2024
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: baiy58@mcmaster.ca, klhwong3-c@my.cityu.edu.hk, tytwan2-c@my.cityu.edu.hk 2024å¹´7æœˆ10æ—¥
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: This survey aims to investigate fundamental deep learning (DL) based 3D reconstruction
    techniques that produce photo-realistic 3D models and scenes, highlighting Neural
    Radiance Fields (NeRFs), Latent Diffusion Models (LDM), and 3D Gaussian Splatting.
    We dissect the underlying algorithms, evaluate their strengths and tradeoffs,
    and project future research trajectories in this rapidly evolving field. We provide
    a comprehensive overview of the fundamental in DL-driven 3D scene reconstruction,
    offering insights into their potential applications and limitations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è°ƒç ”æ—¨åœ¨æ¢è®¨ç”Ÿæˆç…§ç‰‡çº§çœŸå®æ„Ÿ 3D æ¨¡å‹å’Œåœºæ™¯çš„åŸºç¡€æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰3D é‡å»ºæŠ€æœ¯ï¼Œé‡ç‚¹ä»‹ç»ç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰ã€æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å’Œ 3D
    é«˜æ–¯æº…å°„ã€‚æˆ‘ä»¬å‰–æäº†å…¶åŸºç¡€ç®—æ³•ï¼Œè¯„ä¼°å…¶ä¼˜ç¼ºç‚¹ï¼Œå¹¶é¢„æµ‹è¿™ä¸€å¿«é€Ÿå‘å±•çš„é¢†åŸŸçš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬æä¾›äº†å…³äº DL é©±åŠ¨çš„ 3D åœºæ™¯é‡å»ºçš„å…¨é¢æ¦‚è¿°ï¼Œæ·±å…¥äº†è§£å…¶æ½œåœ¨åº”ç”¨å’Œå±€é™æ€§ã€‚
- en: Background
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èƒŒæ™¯
- en: 3D reconstruction is a process aimed at creating volumetric surfaces from image
    and/or video data. This area of research has gained immense traction in recent
    months and finds applications in numerous domains, including virtual reality,
    augmented reality, autonomous driving, and robotics. Deep learning has emerged
    to the forefront of 3D reconstruction techniques and has demonstrated impressive
    results enhancing realism and accuracy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 3D é‡å»ºæ˜¯ä¸€ä¸ªæ—¨åœ¨ä»å›¾åƒå’Œ/æˆ–è§†é¢‘æ•°æ®ä¸­åˆ›å»ºä½“ç§¯è¡¨é¢çš„è¿‡ç¨‹ã€‚è¿™ä¸€ç ”ç©¶é¢†åŸŸåœ¨æœ€è¿‘å‡ ä¸ªæœˆè·å¾—äº†æå¤§çš„å…³æ³¨ï¼Œå¹¶åœ¨å¤šä¸ªé¢†åŸŸä¸­æ‰¾åˆ°äº†åº”ç”¨ï¼ŒåŒ…æ‹¬è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ã€è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯ã€‚æ·±åº¦å­¦ä¹ å·²æˆä¸º
    3D é‡å»ºæŠ€æœ¯çš„å‰æ²¿ï¼Œå¹¶å±•ç¤ºäº†å¢å¼ºç°å®æ„Ÿå’Œå‡†ç¡®æ€§çš„ä»¤äººå°è±¡æ·±åˆ»çš„æˆæœã€‚
- en: Neural Radiance Fields
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ç»è¾å°„åœº
- en: Neural Radiance Field (NeRF) is a method for novel view synthesis of complex
    scenes using a set of input perspectives and optimizes a model to approximate
    a continuous volumetric scene or surface[[9](#bib.bib9)]. The method represents
    the volume using a multilayer preceptron (MLP) whose input is a 5D vector $(x,y,z,\theta,\phi)$.
    $(x,y,z)$ representing the spatial location and $(\theta,\phi)$ representing the
    viewing direction, with an output of a 4D vector ($R,G,B,\sigma)$ representing
    the RGB color and a volume density. NeRFs achieved SOTA results on quantitative
    benchmarks as well as qualitative tests on neural rendering and view synthesis.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ˜¯ä¸€ç§ç”¨äºå¤æ‚åœºæ™¯æ–°è§†è§’åˆæˆçš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ä¸€ç»„è¾“å…¥è§†è§’å¹¶ä¼˜åŒ–æ¨¡å‹ä»¥è¿‘ä¼¼ä¸€ä¸ªè¿ç»­çš„ä½“ç§¯åœºæ™¯æˆ–è¡¨é¢[[9](#bib.bib9)]ã€‚è¯¥æ–¹æ³•ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰è¡¨ç¤ºä½“ç§¯ï¼Œå…¶è¾“å…¥æ˜¯ä¸€ä¸ª
    5D å‘é‡ $(x,y,z,\theta,\phi)$ã€‚å…¶ä¸­ $(x,y,z)$ è¡¨ç¤ºç©ºé—´ä½ç½®ï¼Œ$(\theta,\phi)$ è¡¨ç¤ºè§‚å¯Ÿæ–¹å‘ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ª 4D
    å‘é‡ ($R,G,B,\sigma$)ï¼Œè¡¨ç¤º RGB é¢œè‰²å’Œä½“ç§¯å¯†åº¦ã€‚NeRFs åœ¨å®šé‡åŸºå‡†æµ‹è¯•ä»¥åŠç¥ç»æ¸²æŸ“å’Œè§†å›¾åˆæˆçš„å®šæ€§æµ‹è¯•ä¸­è¾¾åˆ°äº† SOTA ç»“æœã€‚
- en: Prior Work
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¹‹å‰çš„å·¥ä½œ
- en: NeRFs build upon prior work in RGB-alpha volume rendering for view-synthesis
    and the use of neural networks (NN) as implicit continuous shape representations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: NeRFs åŸºäº RGB-alpha ä½“ç§¯æ¸²æŸ“å’Œç¥ç»ç½‘ç»œï¼ˆNNï¼‰ä½œä¸ºéšå¼è¿ç»­å½¢çŠ¶è¡¨ç¤ºçš„å…ˆå‰å·¥ä½œã€‚
- en: Volume Rendering for View-Synthesis
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç”¨äºè§†å›¾åˆæˆçš„ä½“ç§¯æ¸²æŸ“
- en: This process involves, using a set of images to learn a 3D discrete volume representation,
    the model estimate the volume density and emitted color at each point in the 3D
    space, which is then used to synthesize images from various viewpoints. Prior
    methods include Soft 3D, which implements a soft 3D representation of the scene
    by using traditional stereo methods, this representation is used directly to model
    ray visibility and occlusion during view-synthesis [[12](#bib.bib12)]. Along with
    deep learning methods such as Neural Volumes which uses a an encoder-decoder network
    that transforms the input images into a 3D voxel grid, used to generate new views
    [[6](#bib.bib6)]. While these volumetric representation are easy to optimize by
    being trained on how well they render the ground truth views, but as the resolution
    or complexity of the scene increases the compute and memory needed to store these
    discretized representations become unpractical.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹æ¶‰åŠä½¿ç”¨ä¸€ç»„å›¾åƒæ¥å­¦ä¹  3D ç¦»æ•£ä½“ç§¯è¡¨ç¤ºï¼Œæ¨¡å‹ä¼°è®¡ 3D ç©ºé—´ä¸­æ¯ä¸ªç‚¹çš„ä½“ç§¯å¯†åº¦å’Œå‘å°„çš„é¢œè‰²ï¼Œç„¶åç”¨è¿™äº›ä¿¡æ¯ä»ä¸åŒçš„è§†è§’åˆæˆå›¾åƒã€‚ä¹‹å‰çš„æ–¹æ³•åŒ…æ‹¬
    Soft 3Dï¼Œå®ƒé€šè¿‡ä½¿ç”¨ä¼ ç»Ÿçš„ç«‹ä½“è§†è§‰æ–¹æ³•å®ç°äº†åœºæ™¯çš„è½¯ 3D è¡¨ç¤ºï¼Œè¿™ç§è¡¨ç¤ºç›´æ¥ç”¨äºå»ºæ¨¡å…‰çº¿å¯è§æ€§å’Œé®æŒ¡ [[12](#bib.bib12)]ã€‚è¿˜æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå¦‚
    Neural Volumesï¼Œå®ƒä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œå°†è¾“å…¥å›¾åƒè½¬æ¢ä¸º 3D ä½“ç´ ç½‘æ ¼ï¼Œç”¨äºç”Ÿæˆæ–°è§†è§’ [[6](#bib.bib6)]ã€‚å°½ç®¡è¿™äº›ä½“ç§¯è¡¨ç¤ºé€šè¿‡è®­ç»ƒæ¥ä¼˜åŒ–å…¶å¯¹åœ°é¢çœŸå®è§†å›¾çš„æ¸²æŸ“æ•ˆæœè¾ƒä¸ºå®¹æ˜“ï¼Œä½†éšç€åœºæ™¯åˆ†è¾¨ç‡æˆ–å¤æ‚åº¦çš„å¢åŠ ï¼Œå­˜å‚¨è¿™äº›ç¦»æ•£åŒ–è¡¨ç¤ºæ‰€éœ€çš„è®¡ç®—å’Œå†…å­˜å˜å¾—ä¸åˆ‡å®é™…ã€‚
- en: Neural Networks as Shape Representations
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œä½œä¸ºå½¢çŠ¶è¡¨ç¤º
- en: This field of study aims to implicitly represent the 3D surface with a NNâ€™s
    weights. In contrast to the volumetric approach this representation encodes a
    description of a 3D surface at infinite resolution without excessive memory footprint
    as described here[[8](#bib.bib8)]. The NN encodes the 3D surface by learning to
    map a point in space to a property of that point in the 3D space, for example
    occupancy [[8](#bib.bib8)] or signed distance fields [[11](#bib.bib11)]. While
    this approach saves significant memory it is harder to optimize, leading to poor
    synthetic views compared to the discrete representations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç ”ç©¶é¢†åŸŸæ—¨åœ¨ç”¨ç¥ç»ç½‘ç»œçš„æƒé‡éšå¼è¡¨ç¤º 3D è¡¨é¢ã€‚ä¸ä½“ç§¯æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§è¡¨ç¤ºä»¥æ— é™åˆ†è¾¨ç‡ç¼–ç  3D è¡¨é¢çš„æè¿°ï¼Œè€Œä¸ä¼šäº§ç”Ÿè¿‡å¤šçš„å†…å­˜å ç”¨ï¼Œå¦‚æ­¤å¤„æ‰€è¿°
    [[8](#bib.bib8)]ã€‚ç¥ç»ç½‘ç»œé€šè¿‡å­¦ä¹ å°†ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹æ˜ å°„åˆ°è¯¥ç‚¹åœ¨ 3D ç©ºé—´ä¸­çš„ä¸€ä¸ªå±æ€§ï¼ˆä¾‹å¦‚å ç”¨ [[8](#bib.bib8)] æˆ–å¸¦ç¬¦å·è·ç¦»åœº
    [[11](#bib.bib11)]ï¼‰æ¥ç¼–ç  3D è¡¨é¢ã€‚å°½ç®¡è¿™ç§æ–¹æ³•èŠ‚çœäº†å¤§é‡å†…å­˜ï¼Œä½†å…¶ä¼˜åŒ–éš¾åº¦è¾ƒå¤§ï¼Œå¯¼è‡´ä¸ç¦»æ•£è¡¨ç¤ºç›¸æ¯”ç”Ÿæˆçš„è§†å›¾æ•ˆæœè¾ƒå·®ã€‚
- en: 'Approach: NeRF'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–¹æ³•ï¼šNeRF
- en: NeRFs combine these two approaches by representing the scene in the weights
    of an MLP but view synthesis is trained using the techniques in traditional volume
    rendering.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: NeRFs é€šè¿‡åœ¨ MLP çš„æƒé‡ä¸­è¡¨ç¤ºåœºæ™¯æ¥ç»“åˆè¿™ä¸¤ç§æ–¹æ³•ï¼Œä½†è§†å›¾åˆæˆä½¿ç”¨ä¼ ç»Ÿä½“ç§¯æ¸²æŸ“ä¸­çš„æŠ€æœ¯è¿›è¡Œè®­ç»ƒã€‚
- en: '![Refer to caption](img/0b471327a5afe49d6e657cab24050730.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/0b471327a5afe49d6e657cab24050730.png)'
- en: 'Figure 1: An overview of the neural radiance field scene representation and
    differentiable rendering procedure. Synthesize images by sampling 5D coordinates
    (location and viewing direction) along camera rays (a), feeding those locations
    into an MLP to predict a color and volume density (b), and using volume rendering
    techniques to composite these values into an image (c). This rendering function
    is differentiable, so we can optimize our scene representation by minimizing the
    residual between synthesized color and ground truth of the actual color(d).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šç¥ç»è¾å°„åœºåœºæ™¯è¡¨ç¤ºå’Œå¯å¾®åˆ†æ¸²æŸ“è¿‡ç¨‹çš„æ¦‚è¿°ã€‚é€šè¿‡åœ¨ç›¸æœºå…‰çº¿ï¼ˆaï¼‰ä¸Šé‡‡æ · 5D åæ ‡ï¼ˆä½ç½®å’Œè§†è§’ï¼‰ï¼Œå°†è¿™äº›ä½ç½®è¾“å…¥åˆ° MLP ä¸­ä»¥é¢„æµ‹é¢œè‰²å’Œä½“ç§¯å¯†åº¦ï¼ˆbï¼‰ï¼Œå¹¶ä½¿ç”¨ä½“ç§¯æ¸²æŸ“æŠ€æœ¯å°†è¿™äº›å€¼åˆæˆåˆ°å›¾åƒä¸­ï¼ˆcï¼‰ã€‚è¿™ä¸ªæ¸²æŸ“å‡½æ•°æ˜¯å¯å¾®åˆ†çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡æœ€å°åŒ–åˆæˆé¢œè‰²ä¸å®é™…é¢œè‰²ä¹‹é—´çš„æ®‹å·®æ¥ä¼˜åŒ–æˆ‘ä»¬çš„åœºæ™¯è¡¨ç¤ºï¼ˆdï¼‰ã€‚
- en: Neural Radiance Field Scene Representation
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¥ç»è¾å°„åœºåœºæ™¯è¡¨ç¤º
- en: The scene is represented by 5D vector comprised of $\mathbf{x}=(x,y,z)$ and
    $\mathbf{d}=(\theta,\phi)$. This continuous 5D scene representation is approximated
    by a MLP network $F_{\Theta}:(\mathbf{x},\mathbf{d})\to(\mathbf{c},\sigma)$, whose
    weights $\Theta$ are optimized to predict each 5D inputâ€™s $\mathbf{c}=(R,G,B)$
    representing RGB color and $\sigma$ representing density. Density can be thought
    of as occlusion, points with a high occlusion having a higher $\sigma$ value than
    points with lower occlusion.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœºæ™¯ç”± 5D å‘é‡è¡¨ç¤ºï¼ŒåŒ…æ‹¬ $\mathbf{x}=(x,y,z)$ å’Œ $\mathbf{d}=(\theta,\phi)$ã€‚è¿™ä¸ªè¿ç»­çš„ 5D åœºæ™¯è¡¨ç¤ºç”±ä¸€ä¸ª
    MLP ç½‘ç»œ $F_{\Theta}:(\mathbf{x},\mathbf{d})\to(\mathbf{c},\sigma)$ è¿‘ä¼¼ï¼Œå…¶æƒé‡ $\Theta$
    è¢«ä¼˜åŒ–ä»¥é¢„æµ‹æ¯ä¸ª 5D è¾“å…¥çš„ $\mathbf{c}=(R,G,B)$ ä»£è¡¨ RGB é¢œè‰²ï¼Œå’Œ $\sigma$ ä»£è¡¨å¯†åº¦ã€‚å¯†åº¦å¯ä»¥è¢«è§†ä¸ºé®æŒ¡ï¼Œé«˜é®æŒ¡çš„ç‚¹å…·æœ‰æ¯”ä½é®æŒ¡ç‚¹æ›´é«˜çš„
    $\sigma$ å€¼ã€‚
- en: 'The implicit representation is held consist by forcing the network to predict
    $\sigma$ only as a function of $\mathbf{x}$, as density should not change as a
    result of viewing angle. While $\mathbf{c}$ is trained as a function of both $\mathbf{x}$
    and $\mathbf{d}$. The MLP $F_{\Theta}$ has 9 fully-connected layers using ReLU
    activation functions and 256 channels per layer for the first 8 layers and 128
    channels for the last layer. $F_{\Theta}$ first processes $\mathbf{x}$ with the
    first 8 layers outputting $\sigma$ and a 256-dimensional feature vector $\mathbf{v}$.
    $\mathbf{v}$ is then concatenated with $\mathbf{d}$ and passed into the final
    layer that outputs $\mathbf{c}$. This process is shown in Figure [2](#Sx4.F2 "Figure
    2 â€£ Neural Radiance Field Scene Representation â€£ Approach: NeRF").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 'éšå¼è¡¨ç¤ºé€šè¿‡å¼ºåˆ¶ç½‘ç»œä»…å°†$\sigma$ä½œä¸º$\mathbf{x}$çš„å‡½æ•°æ¥ä¿æŒä¸€è‡´ï¼Œå› ä¸ºå¯†åº¦ä¸åº”å› è§†è§’å˜åŒ–è€Œæ”¹å˜ã€‚è€Œ$\mathbf{c}$åˆ™ä½œä¸º$\mathbf{x}$å’Œ$\mathbf{d}$çš„å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚MLP
    $F_{\Theta}$å…·æœ‰9ä¸ªå…¨è¿æ¥å±‚ï¼Œå‰8å±‚æ¯å±‚ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°å’Œ256ä¸ªé€šé“ï¼Œæœ€åä¸€å±‚ä½¿ç”¨128ä¸ªé€šé“ã€‚$F_{\Theta}$é¦–å…ˆç”¨å‰8å±‚å¤„ç†$\mathbf{x}$ï¼Œè¾“å‡º$\sigma$å’Œä¸€ä¸ª256ç»´çš„ç‰¹å¾å‘é‡$\mathbf{v}$ã€‚ç„¶åå°†$\mathbf{v}$ä¸$\mathbf{d}$è¿æ¥ï¼Œå¹¶ä¼ é€’åˆ°æœ€ç»ˆå±‚ï¼Œè¾“å‡º$\mathbf{c}$ã€‚è¿™ä¸€è¿‡ç¨‹å¦‚å›¾[2](#Sx4.F2
    "Figure 2 â€£ Neural Radiance Field Scene Representation â€£ Approach: NeRF")æ‰€ç¤ºã€‚'
- en: '![Refer to caption](img/53ff97fc2463572e853aa4342a577918.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/53ff97fc2463572e853aa4342a577918.png)'
- en: 'Figure 2: An overview of the NeRF model. $\mathbf{x}$ is passed into the first
    8 layers, which output $\mathbf{v}$ and $\sigma$ a). $\mathbf{v}$ is concatenated
    with $\mathbf{d}$ and passed into the last layer, which outputs $\mathbf{c}$ b).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šNeRFæ¨¡å‹æ¦‚è¿°ã€‚$\mathbf{x}$è¾“å…¥å‰8å±‚ï¼Œè¾“å‡º$\mathbf{v}$å’Œ$\sigma$ a)ã€‚$\mathbf{v}$ä¸$\mathbf{d}$è¿æ¥ï¼Œä¼ é€’åˆ°æœ€åä¸€å±‚ï¼Œè¾“å‡º$\mathbf{c}$
    b)ã€‚
- en: Volume Rendering with Radiance Fields
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŸºäºè¾å°„åœºçš„ä½“ç§¯æ¸²æŸ“
- en: The color of any ray passing through the scene is rendered using principles
    from classical volume rendering.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä»»ä½•ç©¿è¿‡åœºæ™¯çš„å…‰çº¿é¢œè‰²éƒ½ä½¿ç”¨ç»å…¸ä½“ç§¯æ¸²æŸ“çš„åŸç†è¿›è¡Œæ¸²æŸ“ã€‚
- en: '|  | $\hat{C}(\mathbf{r})=\sum_{i=1}^{N}w_{i}c_{i},\text{ where }w_{i}=T_{i}\alpha_{i}$
    |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{C}(\mathbf{r})=\sum_{i=1}^{N}w_{i}c_{i},\text{ where }w_{i}=T_{i}\alpha_{i}$
    |  | (1) |'
- en: 'Equation ([1](#Sx4.E1 "In Volume Rendering with Radiance Fields â€£ Approach:
    NeRF")) can be explained as the color $c_{i}$ of each point being weighted by
    $w_{i}$. $w_{i}$ is made up of $T_{i}=\exp(-\sum_{j=1}^{i-1}\sigma_{i}\delta_{i})$
    where $\sigma_{i}$ is the density and $\delta_{i}$ is the distance between adjacently
    sampled points. $T_{i}$ denotes the accumulated transmittance until point $i$
    which can be thought of as the amount of light blocked earlier along the ray,
    and $\alpha_{i}=1-\exp(-\sigma_{i}\delta_{i})$ denoting the opacity at point $i$.
    Thus the color predicted at point with higher transmittance and opacity (the beginning
    of surfaces) contribute more to final predicted color of ray $\mathbf{r}$.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ–¹ç¨‹ ([1](#Sx4.E1 "In Volume Rendering with Radiance Fields â€£ Approach: NeRF"))
    å¯ä»¥è§£é‡Šä¸ºæ¯ä¸ªç‚¹çš„é¢œè‰²$c_{i}$æŒ‰$w_{i}$åŠ æƒã€‚$w_{i}$ç”±$T_{i}=\exp(-\sum_{j=1}^{i-1}\sigma_{i}\delta_{i})$ç»„æˆï¼Œå…¶ä¸­$\sigma_{i}$æ˜¯å¯†åº¦ï¼Œ$\delta_{i}$æ˜¯ç›¸é‚»é‡‡æ ·ç‚¹ä¹‹é—´çš„è·ç¦»ã€‚$T_{i}$è¡¨ç¤ºç‚¹$i$çš„ç´¯è®¡é€å°„ç‡ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯å…‰åœ¨å…‰çº¿ä¸­æ—©æœŸè¢«é˜»æŒ¡çš„é‡ï¼Œè€Œ$\alpha_{i}=1-\exp(-\sigma_{i}\delta_{i})$è¡¨ç¤ºç‚¹$i$çš„é€æ˜åº¦ã€‚å› æ­¤ï¼Œå…·æœ‰æ›´é«˜é€å°„ç‡å’Œé€æ˜åº¦ï¼ˆå³è¡¨é¢èµ·å§‹ç‚¹ï¼‰çš„ç‚¹å¯¹å…‰çº¿$\mathbf{r}$çš„æœ€ç»ˆé¢„æµ‹é¢œè‰²è´¡çŒ®æ›´å¤šã€‚'
- en: Optimizing a NeRF
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–NeRF
- en: The previous sections covered the core components to NeRFs but the original
    paper had two more techniques to achieve SOTA qualityâ€”positional encoding and
    hierarchical volume sampling.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢çš„éƒ¨åˆ†æ¶µç›–äº†NeRFçš„æ ¸å¿ƒç»„ä»¶ï¼Œä½†åŸå§‹è®ºæ–‡è¿˜æœ‰ä¸¤ç§æŠ€æœ¯å®ç°SOTAè´¨é‡â€”â€”ä½ç½®ç¼–ç å’Œåˆ†å±‚ä½“ç§¯é‡‡æ ·ã€‚
- en: Positional Encoding
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä½ç½®ç¼–ç 
- en: 'The authors found that directly feeding in $(x,y,z,\theta,\phi)$ to $F_{\Theta}$
    resulted in poor performance. As a result, they chose to map the inputs to a higher
    dimensional space using high frequency functions, this enabled the model to better
    fit data with high variations. Thus $F_{\Theta}$ is reformulated as a composition
    of two functions $F_{\Theta}=F_{\Theta}^{\prime}\circ\gamma$. $F_{\Theta}^{\prime}$
    being the original MLP and $\gamma$ defined as:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…å‘ç°ç›´æ¥å°†$(x,y,z,\theta,\phi)$è¾“å…¥$F_{\Theta}$ä¼šå¯¼è‡´æ€§èƒ½è¾ƒå·®ã€‚å› æ­¤ï¼Œä»–ä»¬é€‰æ‹©å°†è¾“å…¥æ˜ å°„åˆ°æ›´é«˜ç»´ç©ºé—´ï¼Œä½¿ç”¨é«˜é¢‘å‡½æ•°ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ‹Ÿåˆå…·æœ‰é«˜å˜åŒ–çš„æ•°æ®ã€‚å› æ­¤ï¼Œ$F_{\Theta}$è¢«é‡æ–°è¡¨è¿°ä¸ºä¸¤ä¸ªå‡½æ•°çš„ç»„åˆ$F_{\Theta}=F_{\Theta}^{\prime}\circ\gamma$ã€‚$F_{\Theta}^{\prime}$æ˜¯åŸå§‹MLPï¼Œ$\gamma$å®šä¹‰ä¸ºï¼š
- en: '|  | <math   alttext="\gamma(x)=\left(\begin{array}[]{cc}\sin(2^{0}\pi x),&amp;\cos(2^{0}\pi
    x)\\ \vdots&amp;\vdots\\'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\gamma(x)=\left(\begin{array}[]{cc}\sin(2^{0}\pi x),&amp;\cos(2^{0}\pi
    x)\\ \vdots&amp;\vdots\\'
- en: \sin(2^{L-1}\pi x),&amp;\cos(2^{L-1}\pi x)\end{array}\right)" display="block"><semantics
    ><mrow  ><mrow ><mi  >Î³</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"
    >(</mo><mi  >x</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo >=</mo><mrow
    ><mo  >(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  ><mrow ><mrow ><mi  >sin</mi><mo >â¡</mo><mrow ><mo stretchy="false" >(</mo><mrow
    ><msup ><mn >2</mn><mn >0</mn></msup><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >Ï€</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >x</mi></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd ><mrow ><mi  >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false"
    >(</mo><mrow  ><msup ><mn >2</mn><mn >0</mn></msup><mo lspace="0em" rspace="0em"
    >â€‹</mo><mi >Ï€</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >x</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd  ><mi mathvariant="normal"  >â‹®</mi></mtd><mtd
    ><mi mathvariant="normal" >â‹®</mi></mtd></mtr><mtr  ><mtd ><mrow ><mrow  ><mi >sin</mi><mo
    >â¡</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msup ><mn >2</mn><mrow ><mi
    >L</mi><mo >âˆ’</mo><mn >1</mn></mrow></msup><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >Ï€</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >x</mi></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd ><mrow ><mi  >cos</mi><mo >â¡</mo><mrow ><mo stretchy="false"
    >(</mo><mrow  ><msup ><mn >2</mn><mrow ><mi >L</mi><mo >âˆ’</mo><mn >1</mn></mrow></msup><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >Ï€</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >x</mi></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr></mtable><mo
    >)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><apply
    ><ci >ğ›¾</ci><ci  >ğ‘¥</ci></apply><matrix ><matrixrow ><apply  ><apply ><apply ><csymbol
    cd="ambiguous"  >superscript</csymbol><cn type="integer"  >2</cn><cn type="integer"  >0</cn></apply><ci
    >ğœ‹</ci><ci >ğ‘¥</ci></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><cn type="integer" >2</cn><cn type="integer" >0</cn></apply><ci
    >ğœ‹</ci><ci >ğ‘¥</ci></apply></apply></matrixrow><matrixrow ><ci >â‹®</ci><ci  >â‹®</ci></matrixrow><matrixrow
    ><apply ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer"
    >2</cn><apply ><ci >ğ¿</ci><cn type="integer"  >1</cn></apply></apply><ci >ğœ‹</ci><ci
    >ğ‘¥</ci></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn
    type="integer" >2</cn><apply ><ci >ğ¿</ci><cn type="integer" >1</cn></apply></apply><ci
    >ğœ‹</ci><ci >ğ‘¥</ci></apply></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\gamma(x)=\left(\begin{array}[]{cc}\sin(2^{0}\pi
    x),&\cos(2^{0}\pi x)\\ \vdots&\vdots\\ \sin(2^{L-1}\pi x),&\cos(2^{L-1}\pi x)\end{array}\right)</annotation></semantics></math>
    |  | (2) |
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: $\gamma(x)=\left(\begin{array}{cc}\sin(2^{0}\pi x),&\cos(2^{0}\pi x)\\ \vdots&\vdots\\
    \sin(2^{L-1}\pi x),&\cos(2^{L-1}\pi x)\end{array}\right)$
- en: $\gamma(\cdot)$ is applied to $(x,y,z)$ in $\mathbf{x}$ with $L=10$ and $(\theta,\phi)$
    with $L=4$.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: $\gamma(\cdot)$ è¢«åº”ç”¨äº $\mathbf{x}$ ä¸­çš„ $(x,y,z)$ï¼Œå…¶ä¸­ $L=10$ï¼Œä»¥åŠ $(\theta,\phi)$
    ä¸­çš„ $L=4$ã€‚
- en: '$\gamma$ is a mapping from $\mathbb{R}$ to $\mathbb{R}^{2L}$ that significantly
    improves performance (Figure [3](#Sx4.F3 "Figure 3 â€£ Positional Encoding â€£ Optimizing
    a NeRF â€£ Approach: NeRF")).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: $\gamma$ æ˜¯ä» $\mathbb{R}$ åˆ° $\mathbb{R}^{2L}$ çš„æ˜ å°„ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ˆå›¾ [3](#Sx4.F3 "å›¾3 â€£
    ä½ç½®ç¼–ç  â€£ ä¼˜åŒ–NeRF â€£ æ–¹æ³•ï¼šNeRF")ï¼‰ã€‚
- en: '![Refer to caption](img/86cbd6695e1b64342591a54b9e1106a9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/86cbd6695e1b64342591a54b9e1106a9.png)'
- en: 'Figure 3: Visualizing how the model improves the positional encoding. Without
    it the model is unable to represent high variation geometries and textures resulting
    in an over smoothed, blurred appearance. Also how removing view dependency affect
    the models ability to render lighting and reflections.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3ï¼šå¯è§†åŒ–æ¨¡å‹å¦‚ä½•æ”¹å–„ä½ç½®ç¼–ç ã€‚å¦‚æœæ²¡æœ‰å®ƒï¼Œæ¨¡å‹å°†æ— æ³•è¡¨ç¤ºé«˜å˜åŒ–çš„å‡ ä½•å½¢çŠ¶å’Œçº¹ç†ï¼Œå¯¼è‡´è¿‡åº¦å¹³æ»‘ã€æ¨¡ç³Šçš„å¤–è§‚ã€‚åŒæ—¶ï¼Œå»é™¤è§†å›¾ä¾èµ–æ€§å¦‚ä½•å½±å“æ¨¡å‹æ¸²æŸ“å…‰ç…§å’Œåå°„çš„èƒ½åŠ›ã€‚
- en: Hierarchical Volume Sampling
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åˆ†å±‚ä½“ç§¯é‡‡æ ·
- en: 'Free space and occluded region contribute much less to the quality of the NeRF
    compared to areas at the beginning of a surface, but with uniform sampling, are
    sampled at the same rate. So the authors proposed a hierarchical representation
    that increases rendering efficiency and quality by allocating samples proportional
    to their expected effect shown in [4](#Sx4.F4 "Figure 4 â€£ Hierarchical Volume
    Sampling â€£ Optimizing a NeRF â€£ Approach: NeRF"). For example, if the object in
    question was a ball, there would be less samples taken in the open space in front
    of the ball and inside of the ball verses samples directly on the ballâ€™s surface.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç”±ç©ºé—´å’Œé®æŒ¡åŒºåŸŸå¯¹NeRFçš„è´¨é‡è´¡çŒ®è¿œå°äºè¡¨é¢èµ·å§‹åŒºåŸŸï¼Œä½†åœ¨å‡åŒ€é‡‡æ ·ä¸‹ï¼Œå®ƒä»¬çš„é‡‡æ ·ç‡æ˜¯ç›¸åŒçš„ã€‚å› æ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åˆ†å±‚è¡¨ç¤ºï¼Œé€šè¿‡å°†æ ·æœ¬åˆ†é…ä¸å…¶é¢„æœŸæ•ˆæœæˆæ¯”ä¾‹ï¼Œä»è€Œæé«˜æ¸²æŸ“æ•ˆç‡å’Œè´¨é‡ï¼Œå¦‚[4](#Sx4.F4
    "å›¾4 â€£ åˆ†å±‚ä½“ç§¯é‡‡æ · â€£ ä¼˜åŒ–NeRF â€£ æ–¹æ³•ï¼šNeRF")æ‰€ç¤ºã€‚ä¾‹å¦‚ï¼Œå¦‚æœå¯¹è±¡æ˜¯ä¸€ä¸ªçƒï¼Œåˆ™åœ¨çƒå‰çš„å¼€æ”¾ç©ºé—´å’Œçƒå†…éƒ¨çš„æ ·æœ¬ä¼šæ¯”ç›´æ¥åœ¨çƒè¡¨é¢ä¸Šçš„æ ·æœ¬å°‘ã€‚
- en: '![Refer to caption](img/7f621d39468278022b3ae5b5656b63a9.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/7f621d39468278022b3ae5b5656b63a9.png)'
- en: 'Figure 4: Illustrating hierarchical sampling, where samples are proportional
    to their contribution to the final volume render.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šè¯´æ˜åˆ†å±‚é‡‡æ ·ï¼Œå…¶ä¸­æ ·æœ¬ä¸å…¶å¯¹æœ€ç»ˆä½“ç§¯æ¸²æŸ“çš„è´¡çŒ®æˆæ­£æ¯”ã€‚
- en: 'This is done by optimizing two networks. One â€coarseâ€ and one â€fineâ€. The course
    network samples points uniformly along the ray, while the fine network is biases
    toward the relevant part of the volume by normalizing the per sample weights $w_{i}$
    described in equation ([1](#Sx4.E1 "In Volume Rendering with Radiance Fields â€£
    Approach: NeRF")), this allows one to treat the weight of each point as a probability
    distribution which is sampled to train the fine network. This procedure allocates
    more samples to regions expected to contain visible content.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯é€šè¿‡ä¼˜åŒ–ä¸¤ä¸ªç½‘ç»œæ¥å®ç°çš„ã€‚ä¸€ä¸ªæ˜¯â€œç²—ç•¥â€ç½‘ç»œï¼Œä¸€ä¸ªæ˜¯â€œç²¾ç»†â€ç½‘ç»œã€‚ç²—ç•¥ç½‘ç»œæ²¿å…‰çº¿å‡åŒ€é‡‡æ ·ç‚¹ï¼Œè€Œç²¾ç»†ç½‘ç»œé€šè¿‡æ ‡å‡†åŒ–æ¯ä¸ªæ ·æœ¬çš„æƒé‡$w_{i}$ï¼ˆå¦‚æ–¹ç¨‹([1](#Sx4.E1
    "åœ¨ä½“ç§¯æ¸²æŸ“ä¸è¾å°„åœºä¸­ â€£ æ–¹æ³•ï¼šNeRF")ï¼‰æ‰€è¿°ï¼‰å€¾å‘äºä½“ç§¯çš„ç›¸å…³éƒ¨åˆ†ï¼Œè¿™å…è®¸å°†æ¯ä¸ªç‚¹çš„æƒé‡è§†ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶è¿›è¡Œé‡‡æ ·ä»¥è®­ç»ƒç²¾ç»†ç½‘ç»œã€‚è¯¥è¿‡ç¨‹å°†æ›´å¤šæ ·æœ¬åˆ†é…åˆ°é¢„è®¡åŒ…å«å¯è§å†…å®¹çš„åŒºåŸŸã€‚
- en: Limitations
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å±€é™æ€§
- en: 'While having groundbreaking abilities to render photorealistic 3D volumes from
    2D images, the original NeRF methodology suffered from several limitations. These
    limitations include:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å…·æœ‰ä»2Då›¾åƒæ¸²æŸ“ç…§ç‰‡çº§çœŸå®3Dä½“ç§¯çš„çªç ´æ€§èƒ½åŠ›ï¼ŒåŸå§‹çš„NeRFæ–¹æ³•ä»å­˜åœ¨è‹¥å¹²å±€é™æ€§ã€‚è¿™äº›å±€é™æ€§åŒ…æ‹¬ï¼š
- en: Computational Efficiency
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®¡ç®—æ•ˆç‡
- en: The optimization of a single scene took 100-300k iterations to converge of a
    single NVIDIA V100 GPU which corresponded to 1-2 days [[9](#bib.bib9)]. This poor
    computational efficiency is a product of dense sampling of rays for rendering.
    This dense sampling approach helped in capturing fine details and accurately representing
    complex scenes, but it significantly increases the computational load.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å•ä¸ªåœºæ™¯çš„ä¼˜åŒ–éœ€è¦100-300kæ¬¡è¿­ä»£æ‰èƒ½åœ¨å•ä¸ªNVIDIA V100 GPUä¸Šæ”¶æ•›ï¼Œè¿™å¤§çº¦å¯¹åº”1-2å¤©[[9](#bib.bib9)]ã€‚è¿™ç§ä½æ•ˆçš„è®¡ç®—æ€§èƒ½æ˜¯ç”±äºæ¸²æŸ“æ—¶å¯¹å…‰çº¿çš„å¯†é›†é‡‡æ ·é€ æˆçš„ã€‚è¿™ç§å¯†é›†é‡‡æ ·æ–¹æ³•æœ‰åŠ©äºæ•æ‰ç»†èŠ‚å¹¶å‡†ç¡®è¡¨ç¤ºå¤æ‚åœºæ™¯ï¼Œä½†æ˜¾è‘—å¢åŠ äº†è®¡ç®—è´Ÿæ‹…ã€‚
- en: Lack of Generalizability
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ³›åŒ–èƒ½åŠ›ä¸è¶³
- en: NeRFs are inheritably inflexible due to the as models overfit to one scene.
    A NeRF cannot be adapted for novel scenes without complete retraining.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ¨¡å‹å¯¹å•ä¸€åœºæ™¯è¿‡æ‹Ÿåˆï¼ŒNeRFså¤©ç”Ÿç¼ºä¹çµæ´»æ€§ã€‚NeRFæ— æ³•åœ¨æ²¡æœ‰å®Œå…¨é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹é€‚åº”æ–°åœºæ™¯ã€‚
- en: Difficulty of Editing
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¼–è¾‘éš¾åº¦
- en: Modifying content in NeRFs such as moving or removing object is very difficult.
    Since the model represents the scene as a continuous function and does not store
    geometric information.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨NeRFsä¸­ä¿®æ”¹å†…å®¹ï¼Œå¦‚ç§»åŠ¨æˆ–åˆ é™¤å¯¹è±¡ï¼Œéå¸¸å›°éš¾ã€‚å› ä¸ºæ¨¡å‹å°†åœºæ™¯è¡¨ç¤ºä¸ºè¿ç»­å‡½æ•°ï¼Œè€Œä¸å­˜å‚¨å‡ ä½•ä¿¡æ¯ã€‚
- en: Data requirements
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ•°æ®è¦æ±‚
- en: NeRFs require a lot of data to produce high quality results show in the original
    paper. The synthetic 3D models as lego bulldozer and pirate ship took about 100
    image and the real life scenes such as the flower and conference room each requiring
    around 60 [[9](#bib.bib9)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: NeRFséœ€è¦å¤§é‡æ•°æ®æ¥äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœï¼Œå¦‚åŸå§‹è®ºæ–‡æ‰€ç¤ºã€‚åˆæˆ3Dæ¨¡å‹å¦‚ä¹é«˜æ¨åœŸæœºå’Œæµ·ç›—èˆ¹å¤§çº¦éœ€è¦100å¼ å›¾åƒï¼Œè€Œç°å®ç”Ÿæ´»åœºæ™¯å¦‚èŠ±æœµå’Œä¼šè®®å®¤æ¯ä¸ªå¤§çº¦éœ€è¦60å¼ [[9](#bib.bib9)]ã€‚
- en: Transient Artifacts
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¬æ€ä¼ªå½±
- en: 'The original NeRFs assume that the world is geometrically, materially, and
    photometrically static. Therefore requiring that any two photographs taken at
    the same position and orientation must be identical [[7](#bib.bib7)] they do not
    have a way to adjust for transient occlusions or variable appearance which result
    in artifacts and noise when this assumption fails such as with real world images.
    This is clearly show in [5](#Sx4.F5 "Figure 5 â€£ Transient Artifacts â€£ Approach:
    NeRF").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹NeRFså‡è®¾ä¸–ç•Œåœ¨å‡ ä½•ã€ææ–™å’Œå…‰åº¦ä¸Šæ˜¯é™æ€çš„ã€‚å› æ­¤ï¼Œè¦æ±‚åœ¨ç›¸åŒä½ç½®å’Œæ–¹å‘æ‹æ‘„çš„ä»»ä½•ä¸¤å¼ ç…§ç‰‡å¿…é¡»å®Œå…¨ä¸€è‡´[[7](#bib.bib7)]ï¼Œå®ƒä»¬æ²¡æœ‰åŠæ³•è°ƒæ•´ç¬æ€é®æŒ¡æˆ–å˜åŒ–çš„å¤–è§‚ï¼Œè¿™å¯¼è‡´äº†å½“è¿™ä¸€å‡è®¾å¤±è´¥æ—¶ï¼Œä¾‹å¦‚åœ¨ç°å®ä¸–ç•Œå›¾åƒä¸­å‡ºç°ä¼ªå½±å’Œå™ªå£°ã€‚è¿™åœ¨[5](#Sx4.F5
    "å›¾5 â€£ ç¬æ€ä¼ªå½± â€£ æ–¹æ³•ï¼šNeRF")ä¸­æœ‰æ¸…æ™°çš„å±•ç¤ºã€‚
- en: '![Refer to caption](img/20240280a95fa7d3cff863d3a09daa40.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/20240280a95fa7d3cff863d3a09daa40.png)'
- en: 'Figure 5: Comparison made in the paper NeRF in the Wild [[7](#bib.bib7)], where
    the original NeRF (left) noisy artifacts compared to NeRF-W (right).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5ï¼šè®ºæ–‡ã€ŠNeRF in the Wildã€‹[[7](#bib.bib7)]ä¸­çš„æ¯”è¾ƒï¼Œå…¶ä¸­åŸå§‹NeRFï¼ˆå·¦ï¼‰ä¸NeRF-Wï¼ˆå³ï¼‰ç›¸æ¯”ï¼Œå‡ºç°äº†å™ªå£°ä¼ªå½±ã€‚
- en: Instant-NGP
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å³æ—¶ç¥ç»å›¾å½¢åŸè¯­
- en: Overview
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: Instant-NGP[[10](#bib.bib10)], proposed by Nvlabs, is a method that significantly
    reduce the computation demand of original NeRFs. It leverages multi-resolution
    hash grids to improve memory usage and optimizes 3D reconstruction performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Instant-NGP[[10](#bib.bib10)]ï¼Œç”±Nvlabsæå‡ºï¼Œæ˜¯ä¸€ç§æ˜¾è‘—é™ä½åŸå§‹NeRFsè®¡ç®—éœ€æ±‚çš„æ–¹æ³•ã€‚å®ƒåˆ©ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼æ¥æé«˜å†…å­˜ä½¿ç”¨æ•ˆç‡ï¼Œå¹¶ä¼˜åŒ–3Dé‡å»ºæ€§èƒ½ã€‚
- en: Prior work
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…ˆå‰å·¥ä½œ
- en: Learnable positional encoding
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¯å­¦ä¹ çš„ä½ç½®ä¿¡æ¯ç¼–ç 
- en: 'Learnable positional encoding refers to positional encodings that are parameterized
    for specific positions in a continuous 3D space. The positional encoding for a
    point $p$ in 3D space can be represented as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¯å­¦ä¹ çš„ä½ç½®ä¿¡æ¯ç¼–ç æ˜¯æŒ‡é’ˆå¯¹è¿ç»­3Dç©ºé—´ä¸­ç‰¹å®šä½ç½®å‚æ•°åŒ–çš„ä½ç½®ä¿¡æ¯ç¼–ç ã€‚3Dç©ºé—´ä¸­ä¸€ä¸ªç‚¹$p$çš„ä½ç½®ä¿¡æ¯ç¼–ç å¯ä»¥è¡¨ç¤ºä¸ºï¼š
- en: '|  | $\mathbf{pe}(p)=\sigma(\mathbf{W}\mathbf{p}+\mathbf{b})$ |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{pe}(p)=\sigma(\mathbf{W}\mathbf{p}+\mathbf{b})$ |  |'
- en: where $\mathbf{p}=(x,y,z)^{T}$ represents the coordinates of the position in
    3D space, $\mathbf{W}$ is a learnable weight matrix, $\mathbf{b}$ is a bias vector,
    and $\sigma$ denotes a non-linear activation function.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ$\mathbf{p}=(x,y,z)^{T}$è¡¨ç¤º3Dç©ºé—´ä¸­ä½ç½®çš„åæ ‡ï¼Œ$\mathbf{W}$æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„æƒé‡çŸ©é˜µï¼Œ$\mathbf{b}$æ˜¯ä¸€ä¸ªåç½®å‘é‡ï¼Œ$\sigma$è¡¨ç¤ºéçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚
- en: These positional encodings can then be integrated into a neural network model
    to facilitate the learning of spatial relationships.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä½ç½®ä¿¡æ¯ç¼–ç å¯ä»¥é›†æˆåˆ°ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œä»¥ä¾¿å­¦ä¹ ç©ºé—´å…³ç³»ã€‚
- en: '![Refer to caption](img/e51d7086583d44a5682ce23d98b831d1.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/e51d7086583d44a5682ce23d98b831d1.png)'
- en: 'Figure 6: Experiment performed in the original paper. as the number of parameters
    used for learning the positional encoding increases, the image becomes clearer
    and sharper.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6ï¼šåŸå§‹è®ºæ–‡ä¸­è¿›è¡Œçš„å®éªŒã€‚éšç€ç”¨äºå­¦ä¹ ä½ç½®ä¿¡æ¯ç¼–ç çš„å‚æ•°æ•°é‡å¢åŠ ï¼Œå›¾åƒå˜å¾—æ›´æ¸…æ™°ã€æ›´é”åˆ©ã€‚
- en: Algorithm
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç®—æ³•
- en: Multi-Resolution Hash Encoding
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¤šåˆ†è¾¨ç‡å“ˆå¸Œç¼–ç 
- en: '![Refer to caption](img/3e7c325913acb7910757dacfbc443679.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/3e7c325913acb7910757dacfbc443679.png)'
- en: 'Figure 7: Illustration of the multiresolution hash encoding represented in
    2D in the original paper.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šåŸå§‹è®ºæ–‡ä¸­ä»¥2Då½¢å¼è¡¨ç¤ºçš„å¤šåˆ†è¾¨ç‡å“ˆå¸Œç¼–ç ç¤ºæ„å›¾ã€‚
- en: 'One of the key components of Instant-NGP (Instant Neural Graphics Primitives)
    is the Multi-Resolution Hash Encoding. Instead of learning the positional encoding
    for the entire 3D space, the 3D space is first scaled to fit within a normalized
    range of 0 to 1\. This normalized space is then replicated across multiple resolutions
    and each subdivided into grids of varying densities. This captures both coarse
    and fine details in the scene. Each level focuses on learning the positional encodings
    at the vertices of the grids. Mathematically, this can be expressed as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Instant-NGPï¼ˆå³æ—¶ç¥ç»å›¾å½¢åŸè¯­ï¼‰çš„å…³é”®ç»„ä»¶ä¹‹ä¸€æ˜¯å¤šåˆ†è¾¨ç‡å“ˆå¸Œç¼–ç ã€‚ä¸å…¶å­¦ä¹ æ•´ä¸ª3Dç©ºé—´çš„ä½ç½®ä¿¡æ¯ç¼–ç ï¼Œä¸å¦‚é¦–å…ˆå°†3Dç©ºé—´ç¼©æ”¾åˆ°0åˆ°1çš„æ ‡å‡†åŒ–èŒƒå›´å†…ã€‚ç„¶åï¼Œå°†è¿™ä¸ªæ ‡å‡†åŒ–ç©ºé—´å¤åˆ¶åˆ°å¤šä¸ªåˆ†è¾¨ç‡ä¸Šï¼Œæ¯ä¸ªåˆ†è¾¨ç‡å†ç»†åˆ†æˆä¸åŒå¯†åº¦çš„ç½‘æ ¼ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ•æ‰åœºæ™¯ä¸­çš„ç²—ç•¥å’Œç»†è‡´ç»†èŠ‚ã€‚æ¯ä¸ªçº§åˆ«ä¸“æ³¨äºåœ¨ç½‘æ ¼é¡¶ç‚¹å¤„å­¦ä¹ ä½ç½®ä¿¡æ¯ç¼–ç ã€‚ä»æ•°å­¦ä¸Šæ¥çœ‹ï¼Œè¿™å¯ä»¥è¡¨è¾¾ä¸ºä»¥ä¸‹å½¢å¼ï¼š
- en: '|  | $\mathbf{p}_{scaled}=\mathbf{p}\cdot\mathbf{s}$ |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{p}_{scaled}=\mathbf{p}\cdot\mathbf{s}$ |  |'
- en: 'where $\mathbf{p}$ represents the original coordinates in 3D space, and $\mathbf{s}$
    is a scaling factor that normalizes the space to the [0, 1] range. Following the
    scaling, the coordinates are hashed into a multi-resolution structure using a
    spatial hash function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '-   å…¶ä¸­$\mathbf{p}$è¡¨ç¤º3Dç©ºé—´ä¸­çš„åŸå§‹åæ ‡ï¼Œ$\mathbf{s}$æ˜¯å°†ç©ºé—´æ ‡å‡†åŒ–åˆ°[0, 1]èŒƒå›´çš„ç¼©æ”¾å› å­ã€‚ç¼©æ”¾åï¼Œåæ ‡é€šè¿‡ç©ºé—´å“ˆå¸Œå‡½æ•°è¢«å“ˆå¸Œåˆ°å¤šåˆ†è¾¨ç‡ç»“æ„ä¸­ï¼š'
- en: '|  | $h(\mathbf{x})=\left(\bigoplus_{i=1}^{d}(x_{i}\cdot\pi_{i})\right)\mod
    T$ |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $h(\mathbf{x})=\left(\bigoplus_{i=1}^{d}(x_{i}\cdot\pi_{i})\right)\mod
    T$ |  |'
- en: Here, $d$ is the dimensionality of the space (e.g., 3 for 3D coordinates), $\mathbf{x}=(x_{1},x_{2},\dots,x_{d})$
    represents the scaled coordinates, $\bigoplus$ denotes the bit-wise XOR operation,
    $\pi_{i}$ are large prime numbers unique to each dimension, and $T$ is the size
    of the hash table. This function maps spatial coordinates to indices in the hash
    table, where the neural networkâ€™s parameters are stored or retrieved, linking
    specific spatial locations to neural network parameters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '-   è¿™é‡Œï¼Œ$d$æ˜¯ç©ºé—´çš„ç»´åº¦ï¼ˆä¾‹å¦‚ï¼Œå¯¹äº3Dåæ ‡ä¸º3ï¼‰ï¼Œ$\mathbf{x}=(x_{1},x_{2},\dots,x_{d})$è¡¨ç¤ºç¼©æ”¾åæ ‡ï¼Œ$\bigoplus$è¡¨ç¤ºæŒ‰ä½å¼‚æˆ–æ“ä½œï¼Œ$\pi_{i}$æ˜¯æ¯ä¸ªç»´åº¦ç‰¹æœ‰çš„å¤§è´¨æ•°ï¼Œ$T$æ˜¯å“ˆå¸Œè¡¨çš„å¤§å°ã€‚è¯¥å‡½æ•°å°†ç©ºé—´åæ ‡æ˜ å°„åˆ°å“ˆå¸Œè¡¨ä¸­çš„ç´¢å¼•ï¼Œå…¶ä¸­å­˜å‚¨æˆ–æ£€ç´¢ç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œå°†ç‰¹å®šç©ºé—´ä½ç½®ä¸ç¥ç»ç½‘ç»œå‚æ•°å…³è”ã€‚'
- en: In a nutshell, a hash table is assigned to each level of resolution. For each
    resolution, each vertex is mapped to a entry in the resolutionâ€™s hash table. Higher-resolution
    have larger hash tables compared to lower-resolutions. Every resolutionâ€™s hash
    table map each of itâ€™s vertices to an individual set of parameters that learn
    their positional encodings.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '-   ç®€è€Œè¨€ä¹‹ï¼Œæ¯ä¸ªåˆ†è¾¨ç‡çº§åˆ«éƒ½æœ‰ä¸€ä¸ªå“ˆå¸Œè¡¨ã€‚å¯¹äºæ¯ä¸ªåˆ†è¾¨ç‡ï¼Œæ¯ä¸ªé¡¶ç‚¹éƒ½è¢«æ˜ å°„åˆ°è¯¥åˆ†è¾¨ç‡å“ˆå¸Œè¡¨ä¸­çš„ä¸€ä¸ªæ¡ç›®ã€‚ç›¸æ¯”äºä½åˆ†è¾¨ç‡ï¼Œé«˜åˆ†è¾¨ç‡å…·æœ‰æ›´å¤§çš„å“ˆå¸Œè¡¨ã€‚æ¯ä¸ªåˆ†è¾¨ç‡çš„å“ˆå¸Œè¡¨å°†å…¶æ¯ä¸ªé¡¶ç‚¹æ˜ å°„åˆ°ä¸€ç»„å•ç‹¬çš„å‚æ•°ï¼Œè¿™äº›å‚æ•°å­¦ä¹ å®ƒä»¬çš„ä½ç½®ç¼–ç ã€‚'
- en: Learning positional encoding
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '-   å­¦ä¹ ä½ç½®ç¼–ç '
- en: 'During training, when the model is exposed to images from different viewpoints,
    the NN adjusts the parameters stored in the hash table to minimize the difference
    between the rendered images and the actual training images. The loss $L$ can be
    expressed as:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '-   åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå½“æ¨¡å‹æš´éœ²äºä¸åŒè§†è§’çš„å›¾åƒæ—¶ï¼Œç¥ç»ç½‘ç»œä¼šè°ƒæ•´å­˜å‚¨åœ¨å“ˆå¸Œè¡¨ä¸­çš„å‚æ•°ï¼Œä»¥æœ€å°åŒ–æ¸²æŸ“å›¾åƒä¸å®é™…è®­ç»ƒå›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚æŸå¤±$L$å¯ä»¥è¡¨ç¤ºä¸ºï¼š'
- en: '|  | $L(\theta)=\frac{1}{2}\sum_{i=1}^{m}(R(x_{i},\theta)-y_{i})^{2}$ |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $L(\theta)=\frac{1}{2}\sum_{i=1}^{m}(R(x_{i},\theta)-y_{i})^{2}$ |  |'
- en: where $R(x_{i},\theta)$ is the rendered image based on parameters $\theta$ and
    view point $x_{i}$, $y_{i}$ is the corresponding actual image, and $m$ is the
    number of pixels or data points considered. We can then learn these parameters
    using different optimization techniques like gradient descent.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '-   å…¶ä¸­$R(x_{i},\theta)$æ˜¯åŸºäºå‚æ•°$\theta$å’Œè§†è§’$x_{i}$æ¸²æŸ“çš„å›¾åƒï¼Œ$y_{i}$æ˜¯ç›¸åº”çš„å®é™…å›¾åƒï¼Œ$m$æ˜¯è€ƒè™‘çš„åƒç´ æˆ–æ•°æ®ç‚¹æ•°é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚æ¢¯åº¦ä¸‹é™ï¼Œæ¥å­¦ä¹ è¿™äº›å‚æ•°ã€‚'
- en: Hash Collisions
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '-   å“ˆå¸Œå†²çª'
- en: Hash collisions are avoided by assigning a hash table at each resolution that
    long enough to ensure one-to-one mapping from entries to positional encodings.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '-   é€šè¿‡ä¸ºæ¯ä¸ªåˆ†è¾¨ç‡åˆ†é…ä¸€ä¸ªè¶³å¤Ÿé•¿çš„å“ˆå¸Œè¡¨æ¥é¿å…å“ˆå¸Œå†²çªï¼Œä»è€Œç¡®ä¿æ¡ç›®åˆ°ä½ç½®ç¼–ç çš„ä¸€å¯¹ä¸€æ˜ å°„ã€‚'
- en: Performance
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   æ€§èƒ½'
- en: As shown in figure [8](#Sx5.F8 "Figure 8 â€£ Performance â€£ Instant-NGP"), Instant-NGP
    achieved a notable 20-60Ã— speed improvement compared to compared to the original
    NeRFs while maintaining itâ€™s quality.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '-   å¦‚å›¾[8](#Sx5.F8 "Figure 8 â€£ Performance â€£ Instant-NGP")æ‰€ç¤ºï¼ŒInstant-NGPå®ç°äº†ç›¸è¾ƒäºåŸå§‹NeRFsæ˜¾è‘—çš„20-60å€é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†å…¶è´¨é‡ã€‚'
- en: '![Refer to caption](img/416d3486eaa17a75360a764b027deedf.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/416d3486eaa17a75360a764b027deedf.png)'
- en: 'Figure 8: The figure adapted from the original paper compares the Peak Signal
    to Noise Ratio (PSNR) performance of various NeRF implementations, including the
    authorâ€™s multi-resolution hash encoding method, against other models that require
    hours of training. First row is the name of the object constructed.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '-   å›¾8ï¼šä»åŸå§‹è®ºæ–‡æ”¹ç¼–çš„å›¾è¡¨æ¯”è¾ƒäº†å„ç§NeRFå®ç°çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ€§èƒ½ï¼ŒåŒ…æ‹¬ä½œè€…çš„å¤šåˆ†è¾¨ç‡å“ˆå¸Œç¼–ç æ–¹æ³•ï¼Œä¸å…¶ä»–éœ€è¦æ•°å°æ—¶è®­ç»ƒçš„æ¨¡å‹ã€‚ç¬¬ä¸€è¡Œæ˜¯æ„å»ºå¯¹è±¡çš„åç§°ã€‚'
- en: Limitations
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   å±€é™æ€§'
- en: Instant-NGP focuses on speeding up the computation and training processes of
    NeRFs. However, it still suffers from many of the same issues such as generalability
    different datasets or unseen scenarios. In the next section, we introduce LDM
    based techniques for 3D reconstruction to address the issue of generalizable.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '-   Instant-NGPä¸“æ³¨äºåŠ å¿«NeRFsçš„è®¡ç®—å’Œè®­ç»ƒè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶å­˜åœ¨è®¸å¤šç›¸åŒçš„é—®é¢˜ï¼Œå¦‚å¯¹ä¸åŒæ•°æ®é›†æˆ–æœªè§åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»åŸºäºLDMçš„3Dé‡å»ºæŠ€æœ¯ï¼Œä»¥è§£å†³æ³›åŒ–é—®é¢˜ã€‚'
- en: Latent-Diffusion-Model based 3D reconstruction
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '-   åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„3Dé‡å»º'
- en: Background
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: èƒŒæ™¯
- en: Traditional 3D-Reconstruction algorithms rely heavily on the training data to
    capture all aspects of the volume. Humans, however, are able to estimate a 3D
    surface from a single image. This concept is the foundation of the Zero-1-to-3[[5](#bib.bib5)]
    framework developed out of Columbia University which introduces a diffusion-based
    3D reconstruction method. Zero-1-to-3 utilizes a LDM, originally designed for
    text-conditioned image generation, to generate new perspectives of an image based
    on a cameraâ€™s extrinsic parameters like rotation and translation. Zero-1-to-3
    leverages the geometric priors learned by large-scale LDMs, allowing the generation
    of novel views from a single image. Zero-1-to-3 demonstrates strong zero-shot
    generalization capabilities, outperforming prior models in both single-view 3D
    reconstruction and novel view synthesis tasks. See Figure [9](#Sx6.F9 "Figure
    9 â€£ Background â€£ Latent-Diffusion-Model based 3D reconstruction").
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿçš„ 3D é‡å»ºç®—æ³•ä¸¥é‡ä¾èµ–è®­ç»ƒæ•°æ®æ¥æ•æ‰ä½“ç§¯çš„æ‰€æœ‰æ–¹é¢ã€‚ç„¶è€Œï¼Œäººç±»èƒ½å¤Ÿä»å•å¼ å›¾åƒä¸­ä¼°è®¡å‡º 3D è¡¨é¢ã€‚è¿™ä¸€æ¦‚å¿µæ˜¯å“¥ä¼¦æ¯”äºšå¤§å­¦å¼€å‘çš„ Zero-1-to-3[[5](#bib.bib5)]
    æ¡†æ¶çš„åŸºç¡€ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åŸºäºæ‰©æ•£çš„ 3D é‡å»ºæ–¹æ³•ã€‚Zero-1-to-3 åˆ©ç”¨ LDMï¼Œæœ€åˆè®¾è®¡ç”¨äºæ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆï¼Œæ ¹æ®ç›¸æœºçš„å¤–éƒ¨å‚æ•°ï¼ˆå¦‚æ—‹è½¬å’Œä½ç§»ï¼‰ç”Ÿæˆå›¾åƒçš„æ–°è§†è§’ã€‚Zero-1-to-3
    åˆ©ç”¨å¤§è§„æ¨¡ LDM è®­ç»ƒå¾—åˆ°çš„å‡ ä½•å…ˆéªŒï¼Œå…è®¸ä»å•å¼ å›¾åƒç”Ÿæˆæ–°è§†è§’ã€‚Zero-1-to-3 å±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å•è§†è§’ 3D é‡å»ºå’Œæ–°è§†è§’åˆæˆä»»åŠ¡ä¸­è¶…è¶Šäº†ä¹‹å‰çš„æ¨¡å‹ã€‚è¯·å‚è§å›¾
    [9](#Sx6.F9 "å›¾ 9 â€£ èƒŒæ™¯ â€£ åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ 3D é‡å»º")ã€‚
- en: '![Refer to caption](img/a29a1d797ac0d6fbaaea703e935eeb8a.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/a29a1d797ac0d6fbaaea703e935eeb8a.png)'
- en: 'Figure 9: High Level Picture of Zero-1-to-3 from the original paper'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9ï¼šåŸå§‹è®ºæ–‡ä¸­çš„ Zero-1-to-3 é«˜çº§ç¤ºæ„å›¾
- en: 'Prior Work: Denoising Diffusion Probabilistic Models'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…ˆå‰çš„å·¥ä½œï¼šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹
- en: Denoising Diffusion Probabilistic Models (DDPMs)[[1](#bib.bib1)] are a class
    of generative models that transform data by gradually adding noise over a sequence
    of steps, then learning to reverse this process to generate new samples from noise.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰[[1](#bib.bib1)] æ˜¯ä¸€ç±»ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡é€æ­¥æ·»åŠ å™ªå£°æ¥è½¬åŒ–æ•°æ®ï¼Œç„¶åå­¦ä¹ é€†å‘è¿‡ç¨‹ä»å™ªå£°ä¸­ç”Ÿæˆæ–°çš„æ ·æœ¬ã€‚
- en: Forward Process
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å‰å‘è¿‡ç¨‹
- en: 'The forward process in DDPM is a Markov chain that gradually adds Gaussian
    noise to the data over $T$ timesteps. The process can be mathematically described
    as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: DDPMä¸­çš„å‰å‘è¿‡ç¨‹æ˜¯ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œå®ƒåœ¨ $T$ ä¸ªæ—¶é—´æ­¥ä¸­é€æ¸å‘æ•°æ®ä¸­æ·»åŠ é«˜æ–¯å™ªå£°ã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥æ•°å­¦ä¸Šæè¿°ä¸ºï¼š
- en: '|  | $x_{t}=\sqrt{\alpha_{t}}x_{t-1}+\sqrt{1-\alpha_{t}}\epsilon,\quad\epsilon\sim\mathcal{N}(0,I)$
    |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t}=\sqrt{\alpha_{t}}x_{t-1}+\sqrt{1-\alpha_{t}}\epsilon,\quad\epsilon\sim\mathcal{N}(0,I)$
    |  |'
- en: where $x_{0}$ is the original data, $x_{t}$ is the data at timestep $t$, $\epsilon$
    is the noise, and $\alpha_{t}$ is the variance schedule parameters that determine
    how much noise is added at each step.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ$x_{0}$ æ˜¯åŸå§‹æ•°æ®ï¼Œ$x_{t}$ æ˜¯æ—¶é—´æ­¥ $t$ çš„æ•°æ®ï¼Œ$\epsilon$ æ˜¯å™ªå£°ï¼Œ$\alpha_{t}$ æ˜¯æ–¹å·®è°ƒåº¦å‚æ•°ï¼Œç”¨äºç¡®å®šæ¯ä¸€æ­¥æ·»åŠ å¤šå°‘å™ªå£°ã€‚
- en: '![Refer to caption](img/fec1d74bc4a2d2fcbdbb096e79d0b5a6.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/fec1d74bc4a2d2fcbdbb096e79d0b5a6.png)'
- en: 'Figure 10: Forward Process of DDPM. Adapted from [[4](#bib.bib4)].'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10ï¼šDDPM çš„å‰å‘è¿‡ç¨‹ã€‚æ”¹ç¼–è‡ª [[4](#bib.bib4)]ã€‚
- en: Reverse Process
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åå‘è¿‡ç¨‹
- en: 'The reverse process aims to reconstruct the original data from the noise by
    learning a parameterized model $p_{\theta}$. The reverse process is also a Markov
    chain, described as:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘è¿‡ç¨‹æ—¨åœ¨é€šè¿‡å­¦ä¹ ä¸€ä¸ªå‚æ•°åŒ–æ¨¡å‹ $p_{\theta}$ ä»å™ªå£°ä¸­é‡å»ºåŸå§‹æ•°æ®ã€‚åå‘è¿‡ç¨‹ä¹Ÿæ˜¯ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œæè¿°ä¸ºï¼š
- en: '|  | $x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{\theta}(x_{t},t)\right)+\sigma_{t}z,\quad
    z\sim\mathcal{N}(0,I)$ |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{\theta}(x_{t},t)\right)+\sigma_{t}z,\quad
    z\sim\mathcal{N}(0,I)$ |  |'
- en: where $\epsilon_{\theta}(x_{t},t)$ is a neural network predicting the noise,
    $\sigma_{t}$ is the standard deviation of the reverse process noise, and $\bar{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{s}$
    is the cumulative product of the $\alpha_{t}$ values.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ$\epsilon_{\theta}(x_{t},t)$ æ˜¯ä¸€ä¸ªé¢„æµ‹å™ªå£°çš„ç¥ç»ç½‘ç»œï¼Œ$\sigma_{t}$ æ˜¯åå‘è¿‡ç¨‹å™ªå£°çš„æ ‡å‡†å·®ï¼Œ$\bar{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{s}$
    æ˜¯ $\alpha_{t}$ å€¼çš„ç´¯ç§¯ä¹˜ç§¯ã€‚
- en: Training
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: 'The training of DDPMs involves optimizing the parameters $\theta$ of the neural
    network to minimize the difference between the noise predicted by the model and
    the actual noise added during the forward process. The loss function is typically
    the mean squared error between these two noise terms:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM çš„è®­ç»ƒåŒ…æ‹¬ä¼˜åŒ–ç¥ç»ç½‘ç»œçš„å‚æ•° $\theta$ï¼Œä»¥æœ€å°åŒ–æ¨¡å‹é¢„æµ‹çš„å™ªå£°ä¸å‰å‘è¿‡ç¨‹å®é™…æ·»åŠ çš„å™ªå£°ä¹‹é—´çš„å·®å¼‚ã€‚æŸå¤±å‡½æ•°é€šå¸¸æ˜¯è¿™ä¸¤ä¸ªå™ªå£°é¡¹ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼š
- en: '|  | $L(\theta)=\mathbb{E}_{t,x_{0},\epsilon}\left[\&#124;\epsilon-\epsilon_{\theta}(x_{t},t)\&#124;_{2}^{2}\right]$
    |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | $L(\theta)=\mathbb{E}_{t,x_{0},\epsilon}\left[\| \epsilon - \epsilon_{\theta}(x_{t},t)
    \|_{2}^{2}\right]$ |  |'
- en: where $x_{t}$ is computed during the forward process and $\epsilon$ is the Gaussian
    noise added at each step.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $x_{t}$ åœ¨å‰å‘è¿‡ç¨‹ä¸­è®¡ç®—ï¼Œ$\epsilon$ æ˜¯æ¯æ­¥æ·»åŠ çš„é«˜æ–¯å™ªå£°ã€‚
- en: Sampling
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é‡‡æ ·
- en: To generate new samples, the reverse process is initialized with pure noise
    $x_{T}\sim\mathcal{N}(0,I)$ and iteratively applies the reverse steps to produce
    samples approximating the distribution of the original data $x_{0}$.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºç”Ÿæˆæ–°æ ·æœ¬ï¼Œåå‘è¿‡ç¨‹ä»¥çº¯å™ªå£° $x_{T}\sim\mathcal{N}(0,I)$ åˆå§‹åŒ–ï¼Œå¹¶é€šè¿‡è¿­ä»£åº”ç”¨åå‘æ­¥éª¤æ¥äº§ç”Ÿè¿‘ä¼¼åŸå§‹æ•°æ®åˆ†å¸ƒ $x_{0}$
    çš„æ ·æœ¬ã€‚
- en: Algorithm 1 DDPM Sampling Algorithm
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³• 1 DDPM é‡‡æ ·ç®—æ³•
- en: '1:procedureÂ DdpmSampling($\theta,T,\{\alpha_{t}\}$)2:Â Â Â Â Â Input: Trained model
    parameters $\theta$, total timesteps $T$, noise schedule $\{\alpha_{t}\}$3:Â Â Â Â Â Output:
    A sample approximating the data distribution4:Â Â Â Â Â Initialize: Draw $x_{T}\sim\mathcal{N}(0,I)$
    Start with pure noise5:Â Â Â Â Â forÂ $t=T$ down to 1Â do6:Â Â Â Â Â Â Â Â Â Calculate $\bar{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{s}$7:Â Â Â Â Â Â Â Â Â Calculate
    $\sigma_{t}^{2}=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\cdot(1-\alpha_{t})$8:Â Â Â Â Â Â Â Â Â Predict
    noise $\epsilon_{t}=\epsilon_{\theta}(x_{t},t)$9:Â Â Â Â Â Â Â Â Â ifÂ $t>1$Â then10:Â Â Â Â Â Â Â Â Â Â Â Â Â Â $x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{t}\right)+\sigma_{t}z,\quad
    z\sim\mathcal{N}(0,I)$11:Â Â Â Â Â Â Â Â Â else12:Â Â Â Â Â Â Â Â Â Â Â Â Â Â $x_{0}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{t}\right)$
    Final denoising step13:Â Â Â Â Â Â Â Â Â endÂ if14:Â Â Â Â Â endÂ for15:Â Â Â Â Â Return $x_{0}$16:endÂ procedure![Refer
    to caption](img/fde45972e6e7c59a5b529e8472b4cadb.png)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '1:è¿‡ç¨‹Â DdpmSampling($\theta,T,\{\alpha_{t}\}$)2:Â Â Â Â Â è¾“å…¥: è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•° $\theta$ï¼Œæ€»æ—¶é—´æ­¥
    $T$ï¼Œå™ªå£°è°ƒåº¦ $\{\alpha_{t}\}$3:Â Â Â Â Â è¾“å‡º: ä¸€ä¸ªè¿‘ä¼¼æ•°æ®åˆ†å¸ƒçš„æ ·æœ¬4:Â Â Â Â Â åˆå§‹åŒ–: ç”Ÿæˆ $x_{T}\sim\mathcal{N}(0,I)$
    ä»çº¯å™ªå£°å¼€å§‹5:Â Â Â Â Â å¯¹Â $t=T$ ç›´åˆ° 1Â è¿›è¡Œ6:Â Â Â Â Â Â Â Â Â è®¡ç®— $\bar{\alpha}_{t}=\prod_{s=1}^{t}\alpha_{s}$7:Â Â Â Â Â Â Â Â Â è®¡ç®—
    $\sigma_{t}^{2}=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\cdot(1-\alpha_{t})$8:Â Â Â Â Â Â Â Â Â é¢„æµ‹å™ªå£°
    $\epsilon_{t}=\epsilon_{\theta}(x_{t},t)$9:Â Â Â Â Â Â Â Â Â å¦‚æœÂ $t>1$Â åˆ™10:Â Â Â Â Â Â Â Â Â Â Â Â Â Â $x_{t-1}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{t}\right)+\sigma_{t}z,\quad
    z\sim\mathcal{N}(0,I)$11:Â Â Â Â Â Â Â Â Â å¦åˆ™12:Â Â Â Â Â Â Â Â Â Â Â Â Â Â $x_{0}=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{t}\right)$
    æœ€ç»ˆå»å™ªæ­¥éª¤13:Â Â Â Â Â Â Â Â Â ç»“æŸÂ if14:Â Â Â Â Â ç»“æŸÂ for15:Â Â Â Â Â è¿”å› $x_{0}$16:ç»“æŸÂ è¿‡ç¨‹![å‚è§è¯´æ˜](img/fde45972e6e7c59a5b529e8472b4cadb.png)'
- en: 'Figure 11: Sampling Process. Adapted from [[4](#bib.bib4)].'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 11: é‡‡æ ·è¿‡ç¨‹ã€‚æ”¹ç¼–è‡ª [[4](#bib.bib4)]ã€‚'
- en: Latent Diffusion Model in Zero-1-to-3
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é›¶åˆ°ä¸‰çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹
- en: Latent Diffusion Models[[13](#bib.bib13)] proposed in 2021 are a type of generative
    model that combines the strengths of diffusion models and Variationsal Autoencoders(VAEs).
    Traditional DDPMs operates in the image pixel space, which requires more computation.
    LDMs compress the full image data space into a latent space before the diffusion
    and denosing process, improving efficiency and scalability in generating high-quality
    images.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æ½œåœ¨æ‰©æ•£æ¨¡å‹[[13](#bib.bib13)]äº 2021 å¹´æå‡ºï¼Œæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰çš„ä¼˜ç‚¹ã€‚ä¼ ç»Ÿçš„ DDPM
    åœ¨å›¾åƒåƒç´ ç©ºé—´ä¸­æ“ä½œï¼Œè¿™éœ€è¦æ›´å¤šçš„è®¡ç®—ã€‚LDM åœ¨æ‰©æ•£å’Œå»å™ªè¿‡ç¨‹ä¹‹å‰å°†å®Œæ•´çš„å›¾åƒæ•°æ®ç©ºé—´å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼Œä»è€Œæé«˜ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚
- en: '![Refer to caption](img/fd190672eaf100bdf9ac10b48d174cdf.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/fd190672eaf100bdf9ac10b48d174cdf.png)'
- en: 'Figure 12: The architecture of Latent Diffusion Model in the original paper.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 12: åŸå§‹è®ºæ–‡ä¸­æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ¶æ„ã€‚'
- en: Training Latent Diffusion Models $\epsilon_{\theta}$
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ $\epsilon_{\theta}$
- en: LDM is trained in two main stages. First, a VAE is used to learn an encoding
    function $E(x)$ and a decoding function $D(z)$, where $x$ represents the high-resolution
    image and $z$ is itâ€™s latent representation. The encoder compresses $x$ to $z$,
    and the decoder attempts to reconstruct $x$ from $z$.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: LDM çš„è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚é¦–å…ˆï¼Œä½¿ç”¨ VAE å­¦ä¹ ç¼–ç å‡½æ•° $E(x)$ å’Œè§£ç å‡½æ•° $D(z)$ï¼Œå…¶ä¸­ $x$ è¡¨ç¤ºé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œ$z$ æ˜¯å®ƒçš„æ½œåœ¨è¡¨ç¤ºã€‚ç¼–ç å™¨å°†
    $x$ å‹ç¼©åˆ° $z$ï¼Œè§£ç å™¨å°è¯•ä» $z$ é‡å»º $x$ã€‚
- en: Training VAE
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®­ç»ƒ VAE
- en: 'The VAE optimizes the parameters $\phi$ (encoder) and $\psi$ (decoder) by minimizing
    the reconstruction loss combined with the KL-divergence loss:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: VAE é€šè¿‡æœ€å°åŒ–é‡å»ºæŸå¤±ä¸ KL æ•£åº¦æŸå¤±çš„ç»“åˆæ¥ä¼˜åŒ–å‚æ•° $\phi$ï¼ˆç¼–ç å™¨ï¼‰å’Œ $\psi$ï¼ˆè§£ç å™¨ï¼‰ï¼š
- en: '![Refer to caption](img/65b3721fa5e277712537d9c30756131b.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/65b3721fa5e277712537d9c30756131b.png)'
- en: 'Figure 13: Training VAE. Figure from Lightning AI'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 13: VAE è®­ç»ƒã€‚å›¾æ¥è‡ª Lightning AI'
- en: '|  | $\mathcal{L}_{VAE}(\phi,\psi)=\mathbb{E}_{q_{\phi}(z&#124;x)}[\log p_{\psi}(x&#124;z)]-D_{KL}(q_{\phi}(z&#124;x)\&#124;p(z))$
    |  | (3) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{VAE}(\phi,\psi)=\mathbb{E}_{q_{\phi}(z|x)}[\log p_{\psi}(x|z)]-D_{KL}(q_{\phi}(z|x)\|p(z))$
    |  | (3) |'
- en: Training Attention-U-Net Denoiser
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ³¨æ„åŠ› U-Net å»å™ªå™¨
- en: 'In the second stage, an Attention-U-Net is trained as the denoising model in
    the latent space. This model learns a sequence of denoising steps that transform
    a sample from a noise distribution $p(z_{T})$ to the data distribution $p(z_{0})$
    over T timesteps. The U-Net model parameter $\theta$ are optimized by minimizing
    the expected reverse KL-divergence between the true data distribution and the
    model distribution as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬äºŒé˜¶æ®µï¼Œè®­ç»ƒä¸€ä¸ªæ³¨æ„åŠ› U-Net ä½œä¸ºæ½œåœ¨ç©ºé—´ä¸­çš„å»å™ªæ¨¡å‹ã€‚è¯¥æ¨¡å‹å­¦ä¹ ä¸€ç³»åˆ—å»å™ªæ­¥éª¤ï¼Œå°†æ¥è‡ªå™ªå£°åˆ†å¸ƒ $p(z_{T})$ çš„æ ·æœ¬è½¬å˜ä¸ºæ•°æ®åˆ†å¸ƒ
    $p(z_{0})$ï¼Œå¹¶ç»è¿‡ T ä¸ªæ—¶é—´æ­¥ã€‚é€šè¿‡æœ€å°åŒ–çœŸå®æ•°æ®åˆ†å¸ƒä¸æ¨¡å‹åˆ†å¸ƒä¹‹é—´çš„é¢„æœŸåå‘ KL æ•£åº¦æ¥ä¼˜åŒ– U-Net æ¨¡å‹å‚æ•° $\theta$ï¼Œå…¬å¼å¦‚ä¸‹ï¼š
- en: '|  | $\mathcal{L}(\theta)=\mathbb{E}_{z_{0},\epsilon\sim\mathcal{N}(0,I),t}\left[\&#124;\epsilon-\epsilon_{\theta}(z_{t},t)\&#124;^{2}\right]$
    |  | (4) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\theta)=\mathbb{E}_{z_{0},\epsilon\sim\mathcal{N}(0,I),t}\left[\&#124;\epsilon-\epsilon_{\theta}(z_{t},t)\&#124;^{2}\right]$
    |  | (4) |'
- en: where $z_{t}=\sqrt{\bar{\alpha}_{t}}z_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon$,
    and $\bar{\alpha}_{t}$ is the variance schedule. We use KL-divergence to calculate
    â€how differentâ€ the true data distribution and the model distribution are. We
    aim to make the latent space distribution of the model similar to that of the
    real world. This is the key to generating realistic images. Figure [14](#Sx6.F14
    "Figure 14 â€£ Training Attention-U-Net Denoiser â€£ Latent Diffusion Model in Zero-1-to-3
    â€£ Latent-Diffusion-Model based 3D reconstruction") shows example of minimizing
    two distributions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $z_{t}=\sqrt{\bar{\alpha}_{t}}z_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon$ï¼Œè€Œ
    $\bar{\alpha}_{t}$ æ˜¯æ–¹å·®è°ƒåº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ KL æ•£åº¦æ¥è®¡ç®—çœŸå®æ•°æ®åˆ†å¸ƒä¸æ¨¡å‹åˆ†å¸ƒä¹‹é—´çš„â€œå·®å¼‚â€ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿æ¨¡å‹çš„æ½œåœ¨ç©ºé—´åˆ†å¸ƒç±»ä¼¼äºç°å®ä¸–ç•Œã€‚è¿™æ˜¯ç”ŸæˆçœŸå®å›¾åƒçš„å…³é”®ã€‚å›¾
    [14](#Sx6.F14 "å›¾ 14 â€£ è®­ç»ƒæ³¨æ„åŠ› U-Net å»å™ªå™¨ â€£ Zero-1-to-3 ä¸­çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ â€£ åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ 3D é‡å»º")
    æ˜¾ç¤ºäº†æœ€å°åŒ–ä¸¤ä¸ªåˆ†å¸ƒçš„ç¤ºä¾‹ã€‚
- en: '![Refer to caption](img/460481baf7eb06b300b1e91a577e3c98.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/460481baf7eb06b300b1e91a577e3c98.png)'
- en: 'Figure 14: On the left (bad) example, the difference between $Q(x)$ distribution
    and $P(x)$ is not minimized. On the right (good) example, the difference between
    $Q(x)$ distribution and $P(x)$ is minimized. (from [[3](#bib.bib3)])'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 14ï¼šå·¦ä¾§ï¼ˆä¸ä½³ï¼‰ç¤ºä¾‹ä¸­ï¼Œ$Q(x)$ åˆ†å¸ƒä¸ $P(x)$ ä¹‹é—´çš„å·®å¼‚æœªè¢«æœ€å°åŒ–ã€‚å³ä¾§ï¼ˆè‰¯å¥½ï¼‰ç¤ºä¾‹ä¸­ï¼Œ$Q(x)$ åˆ†å¸ƒä¸ $P(x)$ ä¹‹é—´çš„å·®å¼‚å·²è¢«æœ€å°åŒ–ã€‚ï¼ˆæ¥è‡ª
    [[3](#bib.bib3)]ï¼‰
- en: '![Refer to caption](img/7359350a51c2ef90b1274d4b52bdb839.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/7359350a51c2ef90b1274d4b52bdb839.png)'
- en: 'Figure 15: Attention-U-Net Architecture in the original paper'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 15ï¼šåŸå§‹è®ºæ–‡ä¸­çš„æ³¨æ„åŠ› U-Net æ¶æ„
- en: '![Refer to caption](img/5dfa0e92ea394081b28abc82f375d70d.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/5dfa0e92ea394081b28abc82f375d70d.png)'
- en: 'Figure 16: Training Attention-U-Net Denoiser. Figure from Lightning AI'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 16ï¼šè®­ç»ƒæ³¨æ„åŠ› U-Net å»å™ªå™¨ã€‚å›¾æºè‡ª Lightning AI
- en: Conditioning on Camera Parameters
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åŸºäºç›¸æœºå‚æ•°çš„æ¡ä»¶è®¾å®š
- en: The third stage in the Zero-1-to-3 framework focuses on the conditioning of
    the LDM based on camera extrinsic parameters such as rotation ($R$) and translation
    ($t$). This conditioning is critical for generating novel views of the object,
    which are essential for effective 3D reconstruction from a single image.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Zero-1-to-3 æ¡†æ¶çš„ç¬¬ä¸‰é˜¶æ®µä¸“æ³¨äºåŸºäºç›¸æœºå¤–éƒ¨å‚æ•°ï¼ˆå¦‚æ—‹è½¬ ($R$) å’Œå¹³ç§» ($t$)ï¼‰å¯¹ LDM çš„æ¡ä»¶è®¾å®šã€‚è¿™ç§æ¡ä»¶è®¾å®šå¯¹äºç”Ÿæˆç‰©ä½“çš„æ–°è§†è§’è‡³å…³é‡è¦ï¼Œè¿™å¯¹äºä»å•å¼ å›¾åƒä¸­æœ‰æ•ˆåœ°è¿›è¡Œ
    3D é‡å»ºè‡³å…³é‡è¦ã€‚
- en: Mechanism of Conditioning
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ¡ä»¶è®¾å®šæœºåˆ¶
- en: 'In this stage, the previously trained latent representations are manipulated
    according to the desired camera transformations to simulate new perspectives.
    This process involves:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤é˜¶æ®µï¼Œä¹‹å‰è®­ç»ƒçš„æ½œåœ¨è¡¨ç¤ºæ ¹æ®æ‰€éœ€çš„ç›¸æœºå˜æ¢è¿›è¡Œæ“ä½œï¼Œä»¥æ¨¡æ‹Ÿæ–°çš„è§†è§’ã€‚è¯¥è¿‡ç¨‹åŒ…æ‹¬ï¼š
- en: â€¢
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Camera Transformations: Adjusting the latent variables $z$ to reflect changes
    in viewpoint due to different rotations $R$ and translations $t$.'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç›¸æœºå˜æ¢ï¼šè°ƒæ•´æ½œåœ¨å˜é‡ $z$ ä»¥åæ˜ ç”±äºä¸åŒæ—‹è½¬ $R$ å’Œå¹³ç§» $t$ å¼•èµ·çš„è§†ç‚¹å˜åŒ–ã€‚
- en: â€¢
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Transformation Implementation: This could be achieved either through a learned
    transformation model within the LDM framework or by applying predefined transformation
    matrices directly to the latent vectors.'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å˜æ¢å®ç°ï¼šå¯ä»¥é€šè¿‡åœ¨ LDM æ¡†æ¶å†…å­¦ä¹ çš„å˜æ¢æ¨¡å‹å®ç°ï¼Œä¹Ÿå¯ä»¥ç›´æ¥å°†é¢„å®šä¹‰çš„å˜æ¢çŸ©é˜µåº”ç”¨äºæ½œåœ¨å‘é‡ã€‚
- en: Training the Model for Conditional Output
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ä¸ºæ¡ä»¶è¾“å‡ºè®­ç»ƒæ¨¡å‹
- en: 'The model is further trained to handle conditional outputs effectively, which
    involves:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¿›ä¸€æ­¥è®­ç»ƒä»¥æœ‰æ•ˆå¤„ç†æ¡ä»¶è¾“å‡ºï¼Œè¿™åŒ…æ‹¬ï¼š
- en: â€¢
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Data Preparation: The official code used the RTMV dataset[[15](#bib.bib15)]
    where objects are captured from multiple viewpoints to pair latent representations
    with corresponding camera parameters.'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ•°æ®å‡†å¤‡ï¼šå®˜æ–¹ä»£ç ä½¿ç”¨äº† RTMV æ•°æ®é›†[[15](#bib.bib15)]ï¼Œè¯¥æ•°æ®é›†ä»å¤šä¸ªè§†è§’æ•è·ç‰©ä½“ï¼Œä»¥å°†æ½œåœ¨è¡¨ç¤ºä¸ç›¸åº”çš„ç›¸æœºå‚æ•°é…å¯¹ã€‚
- en: â€¢
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Model Adaptation: Extending the latent diffusion model training to not only
    generate images from the latent representation $z$ but also new perspectives of
    the images from itâ€™s transformed latent representation $z^{\prime}$.'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹é€‚åº”ï¼šæ‰©å±•æ½œåœ¨æ‰©æ•£æ¨¡å‹è®­ç»ƒï¼Œä¸ä»…ç”Ÿæˆæ¥è‡ªæ½œåœ¨è¡¨ç¤º $z$ çš„å›¾åƒï¼Œè¿˜ç”Ÿæˆå…¶å˜æ¢åçš„æ½œåœ¨è¡¨ç¤º $z^{\prime}$ çš„æ–°è§†è§’å›¾åƒã€‚
- en: â€¢
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'MSE Loss: We compute the MSE between the output image and real image with respect
    to $R$ and $t$. A â€near-view consistency lossâ€ that calculate the MSE between
    the image rendered from a view and the image rendered from a nearby view is also
    used to maintain the consistency in 3D reconstruction.'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MSE æŸå¤±ï¼šæˆ‘ä»¬è®¡ç®—è¾“å‡ºå›¾åƒä¸çœŸå®å›¾åƒä¹‹é—´çš„ MSEï¼Œé’ˆå¯¹ $R$ å’Œ $t$ã€‚è¿˜ä½¿ç”¨äº†â€œè¿‘è§†ä¸€è‡´æ€§æŸå¤±â€ï¼Œè¯¥æŸå¤±è®¡ç®—ä»ä¸€ä¸ªè§†è§’æ¸²æŸ“çš„å›¾åƒä¸ä»é™„è¿‘è§†è§’æ¸²æŸ“çš„å›¾åƒä¹‹é—´çš„
    MSEï¼Œä»¥ä¿æŒ 3D é‡å»ºä¸­çš„ä¸€è‡´æ€§ã€‚
- en: Novel View generation
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ–°è§†è§’ç”Ÿæˆ
- en: 'To generate a novel view, the following transformation is applied to the latent
    space:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç”Ÿæˆæ–°è§†è§’ï¼Œå¯¹æ½œåœ¨ç©ºé—´åº”ç”¨ä»¥ä¸‹å˜æ¢ï¼š
- en: '|  | $z^{\prime}=f(z,R,t)$ |  | (5) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $z^{\prime}=f(z,R,t)$ |  | (5) |'
- en: 'Where $f$ is the transformation function that modifies $z$ based on $R$ and
    $t$. The Zero-1-to-3 model generates the novel view as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $f$ æ˜¯åŸºäº $R$ å’Œ $t$ ä¿®æ”¹ $z$ çš„å˜æ¢å‡½æ•°ã€‚Zero-1-to-3 æ¨¡å‹ç”Ÿæˆæ–°è§†è§’å¦‚ä¸‹ï¼š
- en: '|  | $x^{\prime}=D(\epsilon_{\theta}(z^{\prime}))$ |  | (6) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | $x^{\prime}=D(\epsilon_{\theta}(z^{\prime}))$ |  | (6) |'
- en: Where $x^{\prime}$ represents the image generated from the new perspective,
    and $D$ is the decoder part of the LDM that synthesizes the final image output
    from the transformed latent representation $z^{\prime}$.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $x^{\prime}$ è¡¨ç¤ºä»æ–°è§†è§’ç”Ÿæˆçš„å›¾åƒï¼Œ$D$ æ˜¯ LDM çš„è§£ç å™¨éƒ¨åˆ†ï¼Œå®ƒä»å˜æ¢åçš„æ½œåœ¨è¡¨ç¤º $z^{\prime}$ åˆæˆæœ€ç»ˆçš„å›¾åƒè¾“å‡ºã€‚
- en: 3D reconstruction $\epsilon_{\theta}$
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3D é‡å»º $\epsilon_{\theta}$
- en: 'The 3D reconstruction is performed by first generating multiple views of the
    object using the above method for various $R$ and $t$ matrices. Each generated
    image $x^{\prime}$ provides a different perspective of the object. These images
    are then used to reconstruct the 3D model:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 3D é‡å»ºæ˜¯é€šè¿‡é¦–å…ˆä½¿ç”¨ä¸Šè¿°æ–¹æ³•ç”Ÿæˆç‰©ä½“çš„å¤šä¸ªè§†å›¾æ¥æ‰§è¡Œçš„ï¼Œè¿™äº›è§†å›¾æ˜¯é€šè¿‡ä¸åŒçš„ $R$ å’Œ $t$ çŸ©é˜µç”Ÿæˆçš„ã€‚æ¯ä¸ªç”Ÿæˆçš„å›¾åƒ $x^{\prime}$
    æä¾›äº†ç‰©ä½“çš„ä¸åŒè§†è§’ã€‚è¿™äº›å›¾åƒéšåè¢«ç”¨æ¥é‡å»º 3D æ¨¡å‹ï¼š
- en: '|  | $\text{3D Model}=\text{Integrate}(\{x^{\prime}\})$ |  | (7) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{3D Model}=\text{Integrate}(\{x^{\prime}\})$ |  | (7) |'
- en: The integration process typically involves techniques like volumetric fusion
    or multi-view stereo algorithms, which consolidate the information from different
    images to create a detailed 3D representation of the object, as shown in figure
    [17](#Sx6.F17 "Figure 17 â€£ 3D reconstruction Ïµ_ğœƒ â€£ Latent Diffusion Model in Zero-1-to-3
    â€£ Latent-Diffusion-Model based 3D reconstruction").
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: é›†æˆè¿‡ç¨‹é€šå¸¸æ¶‰åŠä½“ç§¯èåˆæˆ–å¤šè§†å›¾ç«‹ä½“ç®—æ³•ç­‰æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯æ•´åˆæ¥è‡ªä¸åŒå›¾åƒçš„ä¿¡æ¯ä»¥åˆ›å»ºç‰©ä½“çš„è¯¦ç»† 3D è¡¨ç¤ºï¼Œå¦‚å›¾ [17](#Sx6.F17 "Figure
    17 â€£ 3D reconstruction Ïµ_ğœƒ â€£ Latent Diffusion Model in Zero-1-to-3 â€£ Latent-Diffusion-Model
    based 3D reconstruction") æ‰€ç¤ºã€‚
- en: '![Refer to caption](img/1a7baef9ae0cc4e9550967d80125c530.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/1a7baef9ae0cc4e9550967d80125c530.png)'
- en: 'Figure 17: Demo of 3D reconstruction from Zero-1-to-3'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 17ï¼šZero-1-to-3 çš„ 3D é‡å»ºæ¼”ç¤º
- en: Limitations for diffusion-based and NeRF-based 3D reconstruction
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŸºäºæ‰©æ•£å’Œ NeRF çš„ 3D é‡å»ºçš„å±€é™æ€§
- en: â€¢
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Flexibility and Real-Time 3D scene Rendering: Training the Zero-1-to-3 model
    for 3D scenes reconstruction typically require iterative denoising processes during
    sampling, which can be computationally intensive and slow. This makes them less
    suitable for applications requiring real-time rendering.'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: çµæ´»æ€§å’Œå®æ—¶ 3D åœºæ™¯æ¸²æŸ“ï¼šè®­ç»ƒ Zero-1-to-3 æ¨¡å‹è¿›è¡Œ 3D åœºæ™¯é‡å»ºé€šå¸¸éœ€è¦åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­è¿›è¡Œè¿­ä»£å»å™ªè¿‡ç¨‹ï¼Œè¿™å¯èƒ½è®¡ç®—å¯†é›†ä¸”é€Ÿåº¦è¾ƒæ…¢ã€‚è¿™ä½¿å¾—å®ƒä»¬ä¸å¤ªé€‚åˆéœ€è¦å®æ—¶æ¸²æŸ“çš„åº”ç”¨ã€‚
- en: â€¢
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Implicit Representation Ambiguity: Both NeRF and Diffusion models represent
    the 3D object implicitly; they do not explicitly construct the 3D space. NeRFs
    utilize the weights of an MLP to represent a 3D scene and LDMs use the latent
    space for new perspectives generation for 3D reconstruction. While implicit representation
    saves significant space it may lead to ambiguities in interpreting the model.'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: éšå¼è¡¨ç¤ºæ¨¡ç³Šæ€§ï¼šNeRF å’Œæ‰©æ•£æ¨¡å‹éƒ½éšå¼åœ°è¡¨ç¤º 3D ç‰©ä½“ï¼›å®ƒä»¬ä¸æ˜¾å¼æ„å»º 3D ç©ºé—´ã€‚NeRF åˆ©ç”¨ MLP çš„æƒé‡æ¥è¡¨ç¤º 3D åœºæ™¯ï¼Œè€Œ LDM
    åˆ™ä½¿ç”¨æ½œåœ¨ç©ºé—´ç”Ÿæˆ 3D é‡å»ºçš„æ–°è§†è§’ã€‚è™½ç„¶éšå¼è¡¨ç¤ºèŠ‚çœäº†å¤§é‡ç©ºé—´ï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´å¯¹æ¨¡å‹è§£é‡Šçš„æ¨¡ç³Šæ€§ã€‚
- en: 'Approach: 3D Gaussian Splatting'
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–¹æ³•ï¼š3D é«˜æ–¯æ•£ç‚¹æ³•
- en: Background
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: èƒŒæ™¯
- en: Throughout the evolution of 3D scene reconstruction, explicit representations
    such as meshes and point clouds have always been preferred by developers and researchers
    due to their clearly defined structure, fast rendering, and ease of editing. NeRF-based
    methods have shifted towards continuous representation of 3D scenes. While the
    continuous nature of these methods helps optimization, the stochastic sampling
    required for rendering is costly and can result in noise [[2](#bib.bib2)]. On
    top of that the implicit representationâ€™s lack of geometric information does not
    lend itself well to editing.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨3Dåœºæ™¯é‡å»ºçš„å‘å±•è¿‡ç¨‹ä¸­ï¼Œç”±äºç½‘æ ¼å’Œç‚¹äº‘ç­‰æ˜¾å¼è¡¨ç¤ºå…·æœ‰æ˜ç¡®çš„ç»“æ„ã€å¿«é€Ÿæ¸²æŸ“å’Œæ˜“äºç¼–è¾‘çš„ç‰¹ç‚¹ï¼Œä¸€ç›´å—åˆ°å¼€å‘è€…å’Œç ”ç©¶äººå‘˜çš„é’çã€‚åŸºäºNeRFçš„æ–¹æ³•åˆ™è½¬å‘3Dåœºæ™¯çš„è¿ç»­è¡¨ç¤ºã€‚è™½ç„¶è¿™äº›æ–¹æ³•çš„è¿ç»­æ€§æœ‰åŠ©äºä¼˜åŒ–ï¼Œä½†æ¸²æŸ“æ‰€éœ€çš„éšæœºé‡‡æ ·ä»£ä»·é«˜ä¸”å¯èƒ½å¯¼è‡´å™ªå£°[[2](#bib.bib2)]ã€‚æ­¤å¤–ï¼Œéšå¼è¡¨ç¤ºç¼ºä¹å‡ ä½•ä¿¡æ¯ï¼Œä¸åˆ©äºç¼–è¾‘ã€‚
- en: Overview
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: 3D Gaussian Splatting provides a high-quality novel view with real-time rendering
    for 3D scenes, these are achieved with the utilization of the Gaussian function
    to present a smooth and accurate texture using captured photos of a scene.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 3D é«˜æ–¯ç‚¹äº‘æ¸²æŸ“æä¾›äº†é«˜è´¨é‡çš„æ–°è§†è§’å’Œå®æ—¶æ¸²æŸ“çš„3Dåœºæ™¯ï¼Œè¿™äº›éƒ½æ˜¯é€šè¿‡åˆ©ç”¨é«˜æ–¯å‡½æ•°æ¥å‘ˆç°å¹³æ»‘ä¸”å‡†ç¡®çš„çº¹ç†ï¼Œåˆ©ç”¨æ‹æ‘„åˆ°çš„åœºæ™¯ç…§ç‰‡å®ç°çš„ã€‚
- en: '![Refer to caption](img/4233b37fbc835e72cbe140d357cd30d7.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/4233b37fbc835e72cbe140d357cd30d7.png)'
- en: 'Figure 18: Overview of 3D Gaussian Splatting process.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 18: 3D é«˜æ–¯ç‚¹äº‘æ¸²æŸ“è¿‡ç¨‹æ¦‚è¿°ã€‚'
- en: To reconstruct a 3D model using 3D Gaussian splatting, first, a video of the
    object is taken from different angles, then converted into frames representing
    static scenes at different camera angles. Structure from Motion (SfM) with feature
    detection and matching techniques such as SIFT is then applied to these images
    to produce a sparse point cloud. The 3D data points of the point cloud are then
    represented by 3D Gaussians. With the optimization process, adaptive density control
    of Gaussians, and high-efficiency algorithm design, realistic views of the 3D
    model can be reconstructed with a high frame rate.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨3Dé«˜æ–¯ç‚¹äº‘æ¸²æŸ“é‡å»º3Dæ¨¡å‹ï¼Œé¦–å…ˆéœ€è¦ä»ä¸åŒè§’åº¦æ‹æ‘„ç‰©ä½“çš„è§†é¢‘ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºè¡¨ç¤ºä¸åŒç›¸æœºè§’åº¦çš„é™æ€åœºæ™¯çš„å¸§ã€‚ç„¶åå¯¹è¿™äº›å›¾åƒåº”ç”¨ç»“æ„å…‰æŸï¼ˆSfMï¼‰åŠç‰¹å¾æ£€æµ‹å’ŒåŒ¹é…æŠ€æœ¯ï¼Œå¦‚SIFTï¼Œä»¥ç”Ÿæˆç¨€ç–ç‚¹äº‘ã€‚ç‚¹äº‘çš„3Dæ•°æ®ç‚¹éšåç”±3Dé«˜æ–¯å‡½æ•°è¡¨ç¤ºã€‚é€šè¿‡ä¼˜åŒ–è¿‡ç¨‹ã€é«˜æ–¯çš„è‡ªé€‚åº”å¯†åº¦æ§åˆ¶å’Œé«˜æ•ˆç®—æ³•è®¾è®¡ï¼Œå¯ä»¥ä»¥é«˜å¸§ç‡é‡å»º3Dæ¨¡å‹çš„é€¼çœŸè§†å›¾ã€‚
- en: 'The algorithm of 3D Gaussian Splatting is demonstrated below, where it can
    be slit into 3 parts: initialization, optimization, and density control of Gaussians.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 3D é«˜æ–¯ç‚¹äº‘æ¸²æŸ“ç®—æ³•å¦‚ä¸‹é¢æ‰€ç¤ºï¼Œå®ƒå¯ä»¥åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼šåˆå§‹åŒ–ã€ä¼˜åŒ–å’Œé«˜æ–¯çš„å¯†åº¦æ§åˆ¶ã€‚
- en: Algorithm
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç®—æ³•
- en: Algorithm 2 Optimization and Densification
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³• 2 ä¼˜åŒ–å’Œå¯†åº¦è°ƒæ•´
- en: '$w$, $h$: width and height of the training images'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '$w$, $h$: è®­ç»ƒå›¾åƒçš„å®½åº¦å’Œé«˜åº¦'
- en: '$M\leftarrow$ SfM Points $\triangleright$ Positions$i\leftarrow 0$ $\triangleright$
    Iteration CountwhileÂ not convergedÂ doÂ Â Â Â Â $V,\hat{I}\leftarrow$ SampleTrainingView()
    $\triangleright$ Camera $V$ and ImageÂ Â Â Â Â $I\leftarrow$ Rasterize($M$, $S$, $C$,
    $A$, $V$) $\triangleright$ Alg.Â [3](#alg3 "Algorithm 3 â€£ Tile-based rasterizer
    for real-time rendering â€£ Approach: 3D Gaussian Splatting")Â Â Â Â Â $L\leftarrow Loss(I,\hat{I})$
    $\triangleright$ LossÂ Â Â Â Â $M$, $S$, $C$, $A$ $\leftarrow$ Adam($\nabla L$) $\triangleright$
    Backprop & StepÂ Â Â Â Â ifÂ IsRefinementIteration($i$)Â thenÂ Â Â Â Â Â Â Â Â for allÂ Gaussians
    $(\mu,\Sigma,c,\alpha)$ in $(M,S,C,A)$Â doÂ Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ $\alpha<\epsilon$ or
    IsTooLarge($\mu,\Sigma)$Â then $\triangleright$ PruningÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â RemoveGaussian()Â Â Â Â Â Â Â Â Â Â Â Â Â Â endÂ ifÂ Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ $\nabla_{p}L>\tau_{p}$Â then
    $\triangleright$ DensificationÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ $\|S\|>\tau_{S}$Â then $\triangleright$
    Over-reconstructionÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â SplitGaussian($\mu,\Sigma,c,\alpha$)Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â else$\triangleright$
    Under-reconstructionÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â CloneGaussian($\mu,\Sigma,c,\alpha$)Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â endÂ ifÂ Â Â Â Â Â Â Â Â Â Â Â Â Â endÂ ifÂ Â Â Â Â Â Â Â Â endÂ forÂ Â Â Â Â endÂ ifÂ Â Â Â Â $i\leftarrow
    i+1$endÂ while'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: $M\leftarrow$ SfM ç‚¹ $\triangleright$ ä½ç½® $i\leftarrow 0$ $\triangleright$ è¿­ä»£è®¡æ•°
    whileÂ not convergedÂ doÂ Â Â Â Â $V,\hat{I}\leftarrow$ SampleTrainingView() $\triangleright$
    ç›¸æœº $V$ å’Œå›¾åƒÂ Â Â Â Â $I\leftarrow$ Rasterize($M$, $S$, $C$, $A$, $V$) $\triangleright$
    ç®—æ³• [3](#alg3 "ç®—æ³• 3 â€£ åŸºäºç“¦ç‰‡çš„å…‰æ …åŒ–å™¨ç”¨äºå®æ—¶æ¸²æŸ“ â€£ æ–¹æ³•ï¼š3D é«˜æ–¯æ•£å°„")Â Â Â Â Â $L\leftarrow Loss(I,\hat{I})$
    $\triangleright$ æŸå¤±Â Â Â Â Â $M$, $S$, $C$, $A$ $\leftarrow$ Adam($\nabla L$) $\triangleright$
    åå‘ä¼ æ’­ä¸æ­¥éª¤Â Â Â Â Â ifÂ IsRefinementIteration($i$)Â thenÂ Â Â Â Â Â Â Â Â for allÂ é«˜æ–¯åˆ†å¸ƒ $(\mu,\Sigma,c,\alpha)$
    åœ¨ $(M,S,C,A)$Â doÂ Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ $\alpha<\epsilon$ æˆ– IsTooLarge($\mu,\Sigma)$Â then
    $\triangleright$ å‰ªæÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â RemoveGaussian()Â Â Â Â Â Â Â Â Â Â Â Â Â Â endÂ ifÂ Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ $\nabla_{p}L>\tau_{p}$Â then
    $\triangleright$ å¯†åº¦åŒ–Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ifÂ $\|S\|>\tau_{S}$Â then $\triangleright$
    è¿‡åº¦é‡å»ºÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â SplitGaussian($\mu,\Sigma,c,\alpha$)Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â else$\triangleright$
    æ¬ é‡å»ºÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â CloneGaussian($\mu,\Sigma,c,\alpha$)Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â endÂ ifÂ Â Â Â Â Â Â Â Â Â Â Â Â Â endÂ ifÂ Â Â Â Â Â Â Â Â endÂ forÂ Â Â Â Â endÂ ifÂ Â Â Â Â $i\leftarrow
    i+1$ endÂ while
- en: Initialization
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–
- en: 'Points in the sparse 3D data point cloud generated by SfM, are initialized
    to 3D Gaussians. The Gaussians are defined by the following variables: position
    $p$, world space 3D covariance matrix $\Sigma$, opacity $\alpha$, and spherical
    harmonics coefficient c (representation of RBG color), given formula:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±SfMç”Ÿæˆçš„ç¨€ç–3Dæ•°æ®ç‚¹äº‘ä¸­çš„ç‚¹è¢«åˆå§‹åŒ–ä¸º3Dé«˜æ–¯åˆ†å¸ƒã€‚é«˜æ–¯åˆ†å¸ƒç”±ä»¥ä¸‹å˜é‡å®šä¹‰ï¼šä½ç½®$p$ã€ä¸–ç•Œç©ºé—´3Dåæ–¹å·®çŸ©é˜µ$\Sigma$ã€é€æ˜åº¦$\alpha$å’Œçƒé¢è°æ³¢ç³»æ•°$c$ï¼ˆRGBé¢œè‰²çš„è¡¨ç¤ºï¼‰ï¼Œç»™å®šå…¬å¼ï¼š
- en: '|  | $G(x)~{}=e^{-\frac{1}{2}(x)^{T}\Sigma^{-1}(x)}$ |  | (8) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | $G(x)~{}=e^{-\frac{1}{2}(x)^{T}\Sigma^{-1}(x)}$ |  | (8) |'
- en: Optimization
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–
- en: Initially, the Gaussians are sparse and not representative, but they are gradually
    optimized to better represent the scene. To do this, a random camera view $V$
    with itâ€™s corresponding image $\hat{I}$ is chosen from the training set. A rasterized
    Gaussian image $I$ is generated by passing the Gaussian means, $\Sigma$, $c$,
    $\alpha$, and $V$ to a differentiable rasterizer function $Rasterize()$.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åˆï¼Œé«˜æ–¯åˆ†å¸ƒæ˜¯ç¨€ç–çš„ä¸”ä¸å…·æœ‰ä»£è¡¨æ€§ï¼Œä½†å®ƒä»¬ä¼šé€æ¸ä¼˜åŒ–ï¼Œä»¥æ›´å¥½åœ°è¡¨ç¤ºåœºæ™¯ã€‚ä¸ºæ­¤ï¼Œä»è®­ç»ƒé›†ä¸­é€‰æ‹©ä¸€ä¸ªéšæœºç›¸æœºè§†è§’$V$åŠå…¶å¯¹åº”çš„å›¾åƒ$\hat{I}$ã€‚é€šè¿‡å°†é«˜æ–¯å‡å€¼ã€$\Sigma$ã€$c$ã€$\alpha$å’Œ$V$ä¼ é€’ç»™å¯å¾®åˆ†çš„å…‰æ …åŒ–å‡½æ•°$Rasterize()$ï¼Œç”Ÿæˆå…‰æ …åŒ–çš„é«˜æ–¯å›¾åƒ$I$ã€‚
- en: 'A loss function, shown in equation ([9](#Sx7.E9 "In Optimization â€£ Algorithm
    â€£ Approach: 3D Gaussian Splatting")), is used to compute the gradients of the
    two images $\hat{I}$ and $I$.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æŸå¤±å‡½æ•°ï¼ˆè§æ–¹ç¨‹ï¼ˆ[9](#Sx7.E9 "åœ¨ä¼˜åŒ– â€£ ç®—æ³• â€£ æ–¹æ³•ï¼š3Dé«˜æ–¯æ•£å°„")ï¼‰ï¼‰ï¼Œè®¡ç®—ä¸¤ä¸ªå›¾åƒ$\hat{I}$å’Œ$I$çš„æ¢¯åº¦ã€‚
- en: '|  | $\mathcal{L}=(1-\lambda)\mathcal{L}_{1}+\lambda\mathcal{L_{\textrm{D-SSIM}}}$
    |  | (9) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=(1-\lambda)\mathcal{L}_{1}+\lambda\mathcal{L_{\textrm{D-SSIM}}}$
    |  | (9) |'
- en: Here, $\mathcal{L}_{1}$ is the Mean Absolute Error of $\hat{I}$ and $I$, $\mathcal{L}_{D-SSMI}$
    is the Difference Structural Similarity Index based loss, and $\lambda$ is a pre-defined
    weight that adjusts the contribution of $\mathcal{L}_{1}$ and $\mathcal{L}_{D-SSMI}$
    to the final loss $\mathcal{L}$. The parameters of the Gaussians are adjusted
    accordingly with the Adam optimizer [[2](#bib.bib2)].
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ$\mathcal{L}_{1}$æ˜¯$\hat{I}$å’Œ$I$çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼Œ$\mathcal{L}_{D-SSMI}$æ˜¯åŸºäºå·®å¼‚ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°çš„æŸå¤±ï¼Œ$\lambda$æ˜¯ä¸€ä¸ªé¢„å®šä¹‰çš„æƒé‡ï¼Œç”¨äºè°ƒæ•´$\mathcal{L}_{1}$å’Œ$\mathcal{L}_{D-SSMI}$å¯¹æœ€ç»ˆæŸå¤±$\mathcal{L}$çš„è´¡çŒ®ã€‚é«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ä¼šé€šè¿‡Adamä¼˜åŒ–å™¨[[2](#bib.bib2)]è¿›è¡Œç›¸åº”è°ƒæ•´ã€‚
- en: Adaptive control of Gaussians
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é«˜æ–¯åˆ†å¸ƒçš„è‡ªé€‚åº”æ§åˆ¶
- en: 'After initialization, an adaptive approach is used to control the number and
    density of Guassians. Adaptive control refers to automatically adjusting the size
    and number of Gaussians to optimize the representation of the static 3D scene.
    The adaptive density control follows the following behaviors (see Fig. 12):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–åï¼Œä½¿ç”¨è‡ªé€‚åº”æ–¹æ³•æ¥æ§åˆ¶é«˜æ–¯çš„æ•°é‡å’Œå¯†åº¦ã€‚è‡ªé€‚åº”æ§åˆ¶æŒ‡çš„æ˜¯è‡ªåŠ¨è°ƒæ•´é«˜æ–¯çš„å¤§å°å’Œæ•°é‡ï¼Œä»¥ä¼˜åŒ–é™æ€3Dåœºæ™¯çš„è¡¨ç¤ºã€‚è‡ªé€‚åº”å¯†åº¦æ§åˆ¶éµå¾ªä»¥ä¸‹è¡Œä¸ºï¼ˆè§å›¾12ï¼‰ï¼š
- en: â€¢
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Gaussian Removal: For every 100 iterations, if the Gaussians are too large
    in the 3D space or have opacity under a defined threshold of opacity $\epsilon_{\alpha}$(essentially
    transparent), they are removed.'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é«˜æ–¯ç§»é™¤ï¼šæ¯100æ¬¡è¿­ä»£ï¼Œå¦‚æœé«˜æ–¯åœ¨3Dç©ºé—´ä¸­è¿‡å¤§æˆ–å…¶ä¸é€æ˜åº¦ä½äºå®šä¹‰çš„ä¸é€æ˜åº¦é˜ˆå€¼$\epsilon_{\alpha}$ï¼ˆåŸºæœ¬ä¸Šæ˜¯é€æ˜çš„ï¼‰ï¼Œåˆ™å°†å…¶ç§»é™¤ã€‚
- en: â€¢
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Gaussian Duplication: For regions filled by gaussians that are greater than
    the defined threshold but is too small, the Gaussians are cloned and moved along
    their direction to cover the empty space. This behavior adaptively and gradually
    increases the number and volume of the Gaussians until the area is well-fitted.'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é«˜æ–¯å…‹éš†ï¼šå¯¹äºé‚£äº›å¤§äºå®šä¹‰é˜ˆå€¼ä½†åˆè¿‡å°çš„åŒºåŸŸä¸­çš„é«˜æ–¯ï¼Œé«˜æ–¯ä¼šè¢«å…‹éš†å¹¶æ²¿å…¶æ–¹å‘ç§»åŠ¨ä»¥è¦†ç›–ç©ºç™½åŒºåŸŸã€‚è¿™ç§è¡Œä¸ºè‡ªé€‚åº”åœ°é€æ¸å¢åŠ é«˜æ–¯çš„æ•°é‡å’Œä½“ç§¯ï¼Œç›´åˆ°åŒºåŸŸå¾—åˆ°è‰¯å¥½æ‹Ÿåˆã€‚
- en: â€¢
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'Gaussian Split: For regions that are over-reconstructed by large Gaussians
    (variance is too high), they are split into smaller Gaussians by a factor $\phi$,
    the original paper used $\phi=1.6$.'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é«˜æ–¯åˆ†è£‚ï¼šå¯¹äºé‚£äº›è¢«å¤§é«˜æ–¯ï¼ˆæ–¹å·®è¿‡é«˜ï¼‰é‡å»ºè¿‡åº¦çš„åŒºåŸŸï¼Œå®ƒä»¬ä¼šè¢«åˆ†è£‚æˆè¾ƒå°çš„é«˜æ–¯ï¼Œåˆ†è£‚å› å­ä¸º$\phi$ï¼ŒåŸå§‹è®ºæ–‡ä¸­ä½¿ç”¨äº†$\phi=1.6$ã€‚
- en: '![Refer to caption](img/4239a49b90764d069b404d1913e97e72.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒæ ‡é¢˜](img/4239a49b90764d069b404d1913e97e72.png)'
- en: 'Figure 19: Gaussians split in over-reconstructed areas while clone in under-reconstructed
    areas.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾19ï¼šé«˜æ–¯åœ¨é‡å»ºè¿‡åº¦çš„åŒºåŸŸè¢«åˆ†è£‚ï¼Œè€Œåœ¨é‡å»ºä¸è¶³çš„åŒºåŸŸè¢«å…‹éš†ã€‚
- en: Due to the splitting and duplicating, the number of Gaussians increases. To
    address this, every Gaussiansâ€™ opacity $\alpha$ close to zero every $N=3000$ iteration,
    after the optimization step then increases the $\alpha$ values for the Guassian
    where this is needed while allowing the unused ones to be removed.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåˆ†è£‚å’Œå…‹éš†ï¼Œé«˜æ–¯çš„æ•°é‡å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ¯$N=3000$æ¬¡è¿­ä»£æ—¶å°†é«˜æ–¯çš„ä¸é€æ˜åº¦$\alpha$æ¥è¿‘äºé›¶ï¼Œä¼˜åŒ–æ­¥éª¤åå†å¢åŠ éœ€è¦çš„é«˜æ–¯çš„$\alpha$å€¼ï¼ŒåŒæ—¶å…è®¸ç§»é™¤æœªä½¿ç”¨çš„é«˜æ–¯ã€‚
- en: Gradient computation of $\Sigma$ and $\Sigma^{\prime}$
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: $\Sigma$å’Œ$\Sigma^{\prime}$çš„æ¢¯åº¦è®¡ç®—
- en: Each Gaussian is represented as an ellipsoid in the 3D space, modelled by a
    covariance matrices $\Sigma$.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªé«˜æ–¯åœ¨3Dç©ºé—´ä¸­è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªæ¤­çƒä½“ï¼Œç”±åæ–¹å·®çŸ©é˜µ$\Sigma$å»ºæ¨¡ã€‚
- en: '|  | $\Sigma=RSS^{T}R^{T}$ |  | (10) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Sigma=RSS^{T}R^{T}$ |  | (10) |'
- en: Where S is the scaling matrix and R is the rotation matrix.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­Sæ˜¯ç¼©æ”¾çŸ©é˜µï¼ŒRæ˜¯æ—‹è½¬çŸ©é˜µã€‚
- en: For each camera angle, 3D Gaussians are rasterized to the 2D screen. The 3D
    covariance matrix $\Sigma^{\prime}$ is derived using the viewing transformation
    matrix $W$ and Jacobian $J$, which approximates the projective transformation.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªç›¸æœºè§’åº¦ï¼Œ3Dé«˜æ–¯è¢«å…‰æ …åŒ–åˆ°2Då±å¹•ä¸Šã€‚3Dåæ–¹å·®çŸ©é˜µ$\Sigma^{\prime}$é€šè¿‡è§†å›¾å˜æ¢çŸ©é˜µ$W$å’Œé›…å¯æ¯”çŸ©é˜µ$J$æ¨å¯¼å‡ºæ¥ï¼Œè¿™è¿‘ä¼¼äºæŠ•å½±å˜æ¢ã€‚
- en: '|  | $\Sigma^{\prime}=JW{\Sigma}W^{T}J^{T}$ |  | (11) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Sigma^{\prime}=JW{\Sigma}W^{T}J^{T}$ |  | (11) |'
- en: $\Sigma^{\prime}$, a 3x3 matrix, can be simplified by ignoring the elements
    in the third row and column while retaining the properties of their corresponding
    planar points representation [[19](#bib.bib19)].
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: $\Sigma^{\prime}$ï¼Œä¸€ä¸ª3x3çŸ©é˜µï¼Œå¯ä»¥é€šè¿‡å¿½ç•¥ç¬¬ä¸‰è¡Œå’Œç¬¬ä¸‰åˆ—ä¸­çš„å…ƒç´ æ¥ç®€åŒ–ï¼ŒåŒæ—¶ä¿ç•™å…¶å¯¹åº”å¹³é¢ç‚¹è¡¨ç¤ºçš„å±æ€§[[19](#bib.bib19)]ã€‚
- en: 'To compute the gradient of the 3D space covariance, the chain rule is applied
    to $\Sigma$ and $\Sigma^{\prime}$ with reference to their rotation $q$ and scaling
    $s$. This results in the expressions $\frac{d\Sigma^{\prime}}{ds}=\frac{d\Sigma^{\prime}}{d\Sigma}\frac{d\Sigma}{ds}$
    and $\frac{d\Sigma^{\prime}}{dq}=\frac{d\Sigma^{\prime}}{d\Sigma}\frac{d\Sigma}{dq}$.
    By substituting $U=JW$ into the equation for $\Sigma^{\prime}$, we get ${\Sigma^{\prime}}=U\Sigma
    U^{T}$. This equation allows the partial derivative of each element in $\frac{d\Sigma^{\prime}}{d\Sigma}$
    to be represented in terms of $U$:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®¡ç®—3Dç©ºé—´åæ–¹å·®çš„æ¢¯åº¦ï¼Œé“¾å¼æ³•åˆ™è¢«åº”ç”¨äº$\Sigma$å’Œ$\Sigma^{\prime}$ï¼Œå‚è€ƒå®ƒä»¬çš„æ—‹è½¬$q$å’Œç¼©æ”¾$s$ã€‚è¿™å¯¼è‡´äº†è¡¨è¾¾å¼$\frac{d\Sigma^{\prime}}{ds}=\frac{d\Sigma^{\prime}}{d\Sigma}\frac{d\Sigma}{ds}$å’Œ$\frac{d\Sigma^{\prime}}{dq}=\frac{d\Sigma^{\prime}}{d\Sigma}\frac{d\Sigma}{dq}$ã€‚é€šè¿‡å°†$U=JW$ä»£å…¥$\Sigma^{\prime}$çš„æ–¹ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°${\Sigma^{\prime}}=U\Sigma
    U^{T}$ã€‚è¿™ä¸ªæ–¹ç¨‹å…è®¸å°†$\frac{d\Sigma^{\prime}}{d\Sigma}$ä¸­æ¯ä¸ªå…ƒç´ çš„åå¯¼æ•°ç”¨$U$è¡¨ç¤ºï¼š
- en: <math   alttext="\frac{\partial\Sigma^{\prime}}{\partial\Sigma_{ij}}=\left(\begin{smallmatrix}U_{1,i}U_{1,j}&amp;U_{1,i}U_{2,j}\\
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="\frac{\partial\Sigma^{\prime}}{\partial\Sigma_{ij}}=\left(\begin{smallmatrix}U_{1,i}U_{1,j}&amp;U_{1,i}U_{2,j}\\
- en: U_{1,j}U_{2,i}&amp;U_{2,i}U_{2,j}\end{smallmatrix}\right)" display="inline"><semantics
    ><mrow  ><mfrac ><mrow ><mo rspace="0em" >âˆ‚</mo><msup ><mi mathvariant="normal"
    >Î£</mi><mo >â€²</mo></msup></mrow><mrow ><mo rspace="0em" >âˆ‚</mo><msub ><mi mathvariant="normal"
    >Î£</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi >j</mi></mrow></msub></mrow></mfrac><mo
    >=</mo><mrow ><mo >(</mo><mtable columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mrow
    ><msub ><mi >U</mi><mrow ><mn >1</mn><mo >,</mo><mi >i</mi></mrow></msub><mo lspace="0em"
    rspace="0em"  >â€‹</mo><msub ><mi >U</mi><mrow ><mn >1</mn><mo >,</mo><mi >j</mi></mrow></msub></mrow></mtd><mtd
    ><mrow ><msub ><mi >U</mi><mrow ><mn >1</mn><mo >,</mo><mi >i</mi></mrow></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >U</mi><mrow ><mn >2</mn><mo >,</mo><mi
    >j</mi></mrow></msub></mrow></mtd></mtr><mtr ><mtd ><mrow  ><msub ><mi >U</mi><mrow
    ><mn >1</mn><mo >,</mo><mi >j</mi></mrow></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >U</mi><mrow ><mn >2</mn><mo >,</mo><mi >i</mi></mrow></msub></mrow></mtd><mtd
    ><mrow ><msub ><mi >U</mi><mrow ><mn >2</mn><mo >,</mo><mi >i</mi></mrow></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >U</mi><mrow ><mn >2</mn><mo >,</mo><mi
    >j</mi></mrow></msub></mrow></mtd></mtr></mtable><mo >)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >Î£</ci><ci >â€²</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >Î£</ci><apply ><ci >ğ‘–</ci><ci >ğ‘—</ci></apply></apply></apply></apply><matrix ><matrixrow
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ˆ</ci><list
    ><cn type="integer" >1</cn><ci >ğ‘–</ci></list></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘ˆ</ci><list ><cn type="integer" >1</cn><ci >ğ‘—</ci></list></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ˆ</ci><list ><cn type="integer"
    >1</cn><ci >ğ‘–</ci></list></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘ˆ</ci><list ><cn type="integer" >2</cn><ci >ğ‘—</ci></list></apply></apply></matrixrow><matrixrow
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ˆ</ci><list
    ><cn type="integer" >1</cn><ci >ğ‘—</ci></list></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘ˆ</ci><list ><cn type="integer" >2</cn><ci >ğ‘–</ci></list></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘ˆ</ci><list ><cn type="integer"  >2</cn><ci
    >ğ‘–</ci></list></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ˆ</ci><list ><cn type="integer"  >2</cn><ci >ğ‘—</ci></list></apply></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\frac{\partial\Sigma^{\prime}}{\partial\Sigma_{ij}}=\left(\begin{smallmatrix}U_{1,i}U_{1,j}&U_{1,i}U_{2,j}\\
    U_{1,j}U_{2,i}&U_{2,i}U_{2,j}\end{smallmatrix}\right)</annotation></semantics></math>
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: \(\frac{\partial\Sigma^{\prime}}{\partial\Sigma_{ij}}=\left(\begin{smallmatrix}U_{1,i}U_{1,j}&U_{1,i}U_{2,j}\\
    U_{1,j}U_{2,i}&U_{2,i}U_{2,j}\end{smallmatrix}\right)\)
- en: 'By substituting $M=RS$ into equation ([10](#Sx7.E10 "In Gradient computation
    of Î£ and Î£'' â€£ Approach: 3D Gaussian Splatting")), it can then be rewritten as
    $\Sigma=MM^{T}$. Using the chain rule, we can derive $\frac{d\Sigma}{ds}=\frac{d\Sigma}{dM}\frac{dM}{ds}$
    and $\frac{d\Sigma}{dq}=\frac{d\Sigma}{dM}\frac{dM}{dq}$. This allows us to calculate
    the scaling gradient at position $(i,j)$ of the covariance matrix with:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: å°†$M=RS$ä»£å…¥æ–¹ç¨‹ï¼ˆ[10](#Sx7.E10 "åœ¨Î£å’ŒÎ£'çš„æ¢¯åº¦è®¡ç®—ä¸­ â€£ æ–¹æ³•ï¼š3Dé«˜æ–¯ç‚¹ç§¯"ï¼‰ï¼‰ï¼Œå¯ä»¥å°†å…¶é‡å†™ä¸º$\Sigma=MM^{T}$ã€‚ä½¿ç”¨é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥æ¨å¯¼å‡º$\frac{d\Sigma}{ds}=\frac{d\Sigma}{dM}\frac{dM}{ds}$å’Œ$\frac{d\Sigma}{dq}=\frac{d\Sigma}{dM}\frac{dM}{dq}$ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®¡ç®—åæ–¹å·®çŸ©é˜µåœ¨ä½ç½®$(i,j)$çš„ç¼©æ”¾æ¢¯åº¦ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼š
- en: '|  | $\frac{\partial M_{i,j}}{\partial s_{k}}=\left\{\begin{array}[]{lr}R_{i,k}&amp;\text{if
    }j=k\\ 0&amp;\text{otherwise}\end{array}\right.$ |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial M_{i,j}}{\partial s_{k}}=\left\{\begin{array}[]{lr}R_{i,k}&amp;\text{å¦‚æœ
    }j=k\\ 0&amp;\text{å¦åˆ™}\end{array}\right.$ |  |'
- en: 'For defining derivatives of $M$ with respect to rotation matrix $R$ in terms
    of quaternion $q$ components, the following formula demonstrating how quaternion
    components affect $R$ is involved:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºå¦‚ä½•ç”¨å››å…ƒæ•°$q$çš„åˆ†é‡æ¥å®šä¹‰æ—‹è½¬çŸ©é˜µ$R$çš„$M$çš„å¯¼æ•°ï¼Œæ¶‰åŠåˆ°ä»¥ä¸‹å…¬å¼ï¼Œè¿™ä¸ªå…¬å¼å±•ç¤ºäº†å››å…ƒæ•°åˆ†é‡å¦‚ä½•å½±å“$R$ï¼š
- en: '|  | <math   alttext="R(q)=2\begin{pmatrix}\frac{1}{2}-(q_{j}^{2}+q_{k}^{2})&amp;(q_{i}q_{j}-q_{r}q_{k})&amp;(q_{i}q_{k}+q_{r}q_{j})\\
    (q_{i}q_{j}+q_{r}q_{k})&amp;\frac{1}{2}-(q_{i}^{2}+q_{k}^{2})&amp;(q_{j}q_{k}-q_{r}q_{i})\\'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="R(q)=2\begin{pmatrix}\frac{1}{2}-(q_{j}^{2}+q_{k}^{2})&amp;(q_{i}q_{j}-q_{r}q_{k})&amp;(q_{i}q_{k}+q_{r}q_{j})\\
    (q_{i}q_{j}+q_{r}q_{k})&amp;\frac{1}{2}-(q_{i}^{2}+q_{k}^{2})&amp;(q_{j}q_{k}-q_{r}q_{i})\\'
- en: (q_{i}q_{k}-q_{r}q_{j})&amp;(q_{j}q_{k}+q_{r}q_{i})&amp;\frac{1}{2}-(q_{i}^{2}+q_{j}^{2})\end{pmatrix}"
    display="block"><semantics ><mrow  ><mrow ><mi >R</mi><mo lspace="0em" rspace="0em"
    >â€‹</mo><mrow  ><mo stretchy="false"  >(</mo><mi >q</mi><mo stretchy="false" >)</mo></mrow></mrow><mo  >=</mo><mrow
    ><mn >2</mn><mo lspace="0em" rspace="0em" >â€‹</mo><mrow  ><mo >(</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd ><mrow  ><mstyle displaystyle="false"  ><mfrac
    ><mn >1</mn><mn >2</mn></mfrac></mstyle><mo >âˆ’</mo><mrow ><mo stretchy="false"  >(</mo><mrow
    ><msubsup ><mi >q</mi><mi >j</mi><mn >2</mn></msubsup><mo >+</mo><msubsup ><mi
    >q</mi><mi >k</mi><mn >2</mn></msubsup></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mtd><mtd
    ><mrow ><mo stretchy="false" >(</mo><mrow ><mrow ><msub ><mi >q</mi><mi >i</mi></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow><mo
    >âˆ’</mo><mrow ><msub ><mi >q</mi><mi >r</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mtd><mtd
    ><mrow ><mo stretchy="false" >(</mo><mrow ><mrow ><msub ><mi >q</mi><mi >i</mi></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow><mo
    >+</mo><mrow ><msub ><mi >q</mi><mi >r</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >q</mi><mi >j</mi></msub></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><mo stretchy="false"  >(</mo><mrow ><mrow ><msub ><mi >q</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow><mo
    >+</mo><mrow ><msub ><mi >q</mi><mi >r</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mtd><mtd
    ><mrow ><mstyle displaystyle="false" ><mfrac ><mn >1</mn><mn >2</mn></mfrac></mstyle><mo
    >âˆ’</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msubsup ><mi >q</mi><mi >i</mi><mn
    >2</mn></msubsup><mo >+</mo><msubsup ><mi >q</mi><mi >k</mi><mn >2</mn></msubsup></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mtd><mtd ><mrow ><mo stretchy="false"
    >(</mo><mrow ><mrow ><msub ><mi >q</mi><mi >j</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow><mo >âˆ’</mo><mrow ><msub ><mi >q</mi><mi >r</mi></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi >i</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mtd></mtr><mtr ><mtd ><mrow  ><mo stretchy="false"  >(</mo><mrow
    ><mrow ><msub ><mi >q</mi><mi >i</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow><mo >âˆ’</mo><mrow ><msub ><mi >q</mi><mi >r</mi></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mtd><mtd ><mrow ><mo stretchy="false" >(</mo><mrow
    ><mrow ><msub ><mi >q</mi><mi >j</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow><mo >+</mo><mrow ><msub ><mi >q</mi><mi >r</mi></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi >i</mi></msub></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mtd><mtd ><mrow ><mstyle displaystyle="false"
    ><mfrac ><mn >1</mn><mn >2</mn></mfrac></mstyle><mo >âˆ’</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><msubsup ><mi >q</mi><mi >i</mi><mn >2</mn></msubsup><mo >+</mo><msubsup
    ><mi >q</mi><mi >j</mi><mn >2</mn></msubsup></mrow><mo stretchy="false" >)</mo></mrow></mrow></mtd></mtr></mtable><mo
    >)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><apply
    ><ci >ğ‘…</ci><ci  >ğ‘</ci></apply><apply ><cn type="integer" >2</cn><apply  ><csymbol
    cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><apply  ><apply ><cn type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘—</ci></apply><cn
    type="integer"  >2</cn></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘˜</ci></apply><cn
    type="integer"  >2</cn></apply></apply></apply><apply ><apply ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘–</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘—</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘Ÿ</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘˜</ci></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘–</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘˜</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘Ÿ</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘—</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci
    >ğ‘–</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci
    >ğ‘—</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘Ÿ</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘˜</ci></apply></apply></apply><apply ><apply ><cn type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘–</ci></apply><cn
    type="integer"  >2</cn></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘˜</ci></apply><cn
    type="integer"  >2</cn></apply></apply></apply><apply ><apply ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘—</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘˜</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘Ÿ</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘–</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci
    >ğ‘–</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci
    >ğ‘˜</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘Ÿ</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘—</ci></apply></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘—</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘˜</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘Ÿ</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘–</ci></apply></apply></apply><apply ><apply ><cn type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘–</ci></apply><cn
    type="integer"  >2</cn></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘—</ci></apply><cn
    type="integer"  >2</cn></apply></apply></apply></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >R(q)=2\begin{pmatrix}\frac{1}{2}-(q_{j}^{2}+q_{k}^{2})&(q_{i}q_{j}-q_{r}q_{k})&(q_{i}q_{k}+q_{r}q_{j})\\
    (q_{i}q_{j}+q_{r}q_{k})&\frac{1}{2}-(q_{i}^{2}+q_{k}^{2})&(q_{j}q_{k}-q_{r}q_{i})\\
    (q_{i}q_{k}-q_{r}q_{j})&(q_{j}q_{k}+q_{r}q_{i})&\frac{1}{2}-(q_{i}^{2}+q_{j}^{2})\end{pmatrix}</annotation></semantics></math>
    |  | (12) |
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: \( R(q) = 2 \begin{pmatrix} \frac{1}{2} - (q_{j}^{2} + q_{k}^{2}) & (q_{i}q_{j}
    - q_{r}q_{k}) & (q_{i}q_{k} + q_{r}q_{j}) \\ (q_{i}q_{j} + q_{r}q_{k}) & \frac{1}{2}
    - (q_{i}^{2} + q_{k}^{2}) & (q_{j}q_{k} - q_{r}q_{i}) \\ (q_{i}q_{k} - q_{r}q_{j})
    & (q_{j}q_{k} + q_{r}q_{i}) & \frac{1}{2} - (q_{i}^{2} + q_{j}^{2}) \end{pmatrix}
    \)
- en: 'And therefore the gradient $\frac{\partial M}{\partial q_{x}}$ for 4 components
    of quaternion $r,i,j,k$ can be calculated as follow:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå››å…ƒæ•°$r,i,j,k$çš„æ¢¯åº¦$\frac{\partial M}{\partial q_{x}}$å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼è®¡ç®—ï¼š
- en: '|  |  | <math   alttext="\displaystyle\frac{\partial M}{\partial q_{r}}=2\left(\begin{smallmatrix}0&amp;-s_{y}q_{k}&amp;s_{z}q_{j}\\
    s_{x}q_{k}&amp;0&amp;-s_{z}q_{i}\\'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '|  |  | <math   alttext="\displaystyle\frac{\partial M}{\partial q_{r}}=2\left(\begin{smallmatrix}0&amp;-s_{y}q_{k}&amp;s_{z}q_{j}\\
    s_{x}q_{k}&amp;0&amp;-s_{z}q_{i}\\'
- en: -s_{x}q_{j}&amp;s_{y}q_{i}&amp;0\end{smallmatrix}\right)," display="inline"><semantics
    ><mrow  ><mrow ><mstyle displaystyle="true" ><mfrac ><mrow ><mo rspace="0em"  >âˆ‚</mo><mi
    >M</mi></mrow><mrow ><mo rspace="0em"  >âˆ‚</mo><msub ><mi >q</mi><mi >r</mi></msub></mrow></mfrac></mstyle><mo
    >=</mo><mrow ><mn >2</mn><mo lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo >(</mo><mtable
    columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mn >0</mn></mtd><mtd ><mrow
    ><mo >âˆ’</mo><mrow ><msub ><mi >s</mi><mi >y</mi></msub><mo lspace="0em" rspace="0em"
    >â€‹</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow></mrow></mtd><mtd ><mrow ><msub
    ><mi >s</mi><mi >z</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><msub ><mi
    >q</mi><mi >j</mi></msub></mrow></mtd></mtr><mtr ><mtd ><mrow  ><msub ><mi >s</mi><mi
    >x</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow></mtd><mtd
    ><mn >0</mn></mtd><mtd ><mrow  ><mo >âˆ’</mo><mrow ><msub ><mi >s</mi><mi >z</mi></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi >i</mi></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><mo >âˆ’</mo><mrow ><msub ><mi >s</mi><mi >x</mi></msub><mo lspace="0em"
    rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow></mrow></mtd><mtd
    ><mrow ><msub ><mi >s</mi><mi >y</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><msub
    ><mi >q</mi><mi >i</mi></msub></mrow></mtd><mtd ><mn >0</mn></mtd></mtr></mtable><mo
    >)</mo></mrow></mrow></mrow><mo >,</mo></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><apply ><ci >ğ‘€</ci></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘Ÿ</ci></apply></apply></apply><apply ><cn type="integer" >2</cn><matrix
    ><matrixrow ><cn type="integer" >0</cn><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘¦</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘˜</ci></apply></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘§</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘—</ci></apply></apply></matrixrow><matrixrow ><apply ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¥</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘˜</ci></apply></apply><cn
    type="integer"  >0</cn><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘§</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘–</ci></apply></apply></apply></matrixrow><matrixrow ><apply ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¥</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘—</ci></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¦</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘–</ci></apply></apply><cn
    type="integer"  >0</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\frac{\partial M}{\partial q_{r}}=2\left(\begin{smallmatrix}0&-s_{y}q_{k}&s_{z}q_{j}\\
    s_{x}q_{k}&0&-s_{z}q_{i}\\ -s_{x}q_{j}&s_{y}q_{i}&0\end{smallmatrix}\right),</annotation></semantics></math>
    | <math   alttext="\displaystyle\frac{\partial M}{\partial q_{i}}=2\left(\begin{smallmatrix}0&amp;s_{y}q_{j}&amp;s_{z}q_{k}\\
    s_{x}q_{j}&amp;-2s_{y}q_{i}&amp;-s_{z}q_{r}\\
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: \(\frac{\partial M}{\partial q_{r}}=2\left(\begin{smallmatrix}0&-s_{y}q_{k}&s_{z}q_{j}\\
    s_{x}q_{k}&0&-s_{z}q_{i}\\ -s_{x}q_{j}&s_{y}q_{i}&0\end{smallmatrix}\right),\)
- en: s_{x}q_{k}&amp;s_{y}q_{r}&amp;-2s_{z}q_{i}\end{smallmatrix}\right)" display="inline"><semantics
    ><mrow  ><mstyle displaystyle="true"  ><mfrac ><mrow ><mo rspace="0em" >âˆ‚</mo><mi
    >M</mi></mrow><mrow ><mo rspace="0em" >âˆ‚</mo><msub ><mi >q</mi><mi >i</mi></msub></mrow></mfrac></mstyle><mo
    >=</mo><mrow ><mn  >2</mn><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo >(</mo><mtable
    columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mn >0</mn></mtd><mtd ><mrow
    ><msub ><mi >s</mi><mi >y</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><msub
    ><mi >q</mi><mi >j</mi></msub></mrow></mtd><mtd ><mrow ><msub ><mi >s</mi><mi
    >z</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><msub ><mi >s</mi><mi >x</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >q</mi><mi >j</mi></msub></mrow></mtd><mtd ><mrow ><mo >âˆ’</mo><mrow ><mn
    >2</mn><mo lspace="0em" rspace="0em" >â€‹</mo><msub ><mi >s</mi><mi >y</mi></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi >i</mi></msub></mrow></mrow></mtd><mtd
    ><mrow ><mo >âˆ’</mo><mrow ><msub ><mi >s</mi><mi >z</mi></msub><mo lspace="0em"
    rspace="0em" >â€‹</mo><msub ><mi >q</mi><mi >r</mi></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><msub ><mi >s</mi><mi >x</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow></mtd><mtd ><mrow ><msub ><mi >s</mi><mi
    >y</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><msub ><mi >q</mi><mi >r</mi></msub></mrow></mtd><mtd
    ><mrow ><mo >âˆ’</mo><mrow ><mn >2</mn><mo lspace="0em" rspace="0em" >â€‹</mo><msub
    ><mi >s</mi><mi >z</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi
    >q</mi><mi >i</mi></msub></mrow></mrow></mtd></mtr></mtable><mo >)</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >ğ‘€</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘–</ci></apply></apply></apply><apply
    ><cn type="integer" >2</cn><matrix ><matrixrow ><cn type="integer" >0</cn><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¦</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘—</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘§</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘˜</ci></apply></apply></matrixrow><matrixrow
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¥</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘—</ci></apply></apply><apply
    ><apply ><cn type="integer" >2</cn><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘¦</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘–</ci></apply></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘§</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘Ÿ</ci></apply></apply></apply></matrixrow><matrixrow ><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¥</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘˜</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¦</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘Ÿ</ci></apply></apply><apply
    ><apply ><cn type="integer" >2</cn><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘§</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘–</ci></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\frac{\partial M}{\partial q_{i}}=2\left(\begin{smallmatrix}0&s_{y}q_{j}&s_{z}q_{k}\\
    s_{x}q_{j}&-2s_{y}q_{i}&-s_{z}q_{r}\\ s_{x}q_{k}&s_{y}q_{r}&-2s_{z}q_{i}\end{smallmatrix}\right)</annotation></semantics></math>
    |  | (13) |
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: \(\frac{\partial M}{\partial q_{i}} = 2\left(\begin{smallmatrix}0 & s_{y}q_{j}
    & s_{z}q_{k}\\ s_{x}q_{j} & -2s_{y}q_{i} & -s_{z}q_{r}\\ s_{x}q_{k} & s_{y}q_{r}
    & -2s_{z}q_{i}\end{smallmatrix}\right)\)
- en: '|  |  | <math   alttext="\displaystyle\frac{\partial M}{\partial q_{j}}=2\left(\begin{smallmatrix}-2s_{x}q_{j}&amp;s_{y}q_{i}&amp;s_{z}q_{r}\\
    s_{x}q_{i}&amp;0&amp;s_{z}q_{k}\\'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '|  |  | <math alttext="\displaystyle\frac{\partial M}{\partial q_{j}}=2\left(\begin{smallmatrix}-2s_{x}q_{j}&amp;s_{y}q_{i}&amp;s_{z}q_{r}\\
    s_{x}q_{i}&amp;0&amp;s_{z}q_{k}\\'
- en: -s_{x}q_{r}&amp;s_{y}q_{k}&amp;-2s_{z}q_{j}\end{smallmatrix}\right)," display="inline"><semantics
    ><mrow  ><mrow ><mstyle displaystyle="true"  ><mfrac ><mrow ><mo rspace="0em"  >âˆ‚</mo><mi
    >M</mi></mrow><mrow ><mo rspace="0em"  >âˆ‚</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow></mfrac></mstyle><mo
    >=</mo><mrow ><mn >2</mn><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo >(</mo><mtable
    columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mrow ><mo >âˆ’</mo><mrow ><mn
    >2</mn><mo lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >s</mi><mi >x</mi></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow></mrow></mtd><mtd
    ><mrow ><msub ><mi >s</mi><mi >y</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><msub
    ><mi >q</mi><mi >i</mi></msub></mrow></mtd><mtd ><mrow ><msub ><mi >s</mi><mi
    >z</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><msub ><mi >q</mi><mi >r</mi></msub></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><msub ><mi >s</mi><mi >x</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >q</mi><mi >i</mi></msub></mrow></mtd><mtd ><mn >0</mn></mtd><mtd ><mrow  ><msub
    ><mi >s</mi><mi >z</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi
    >q</mi><mi >k</mi></msub></mrow></mtd></mtr><mtr ><mtd ><mrow  ><mo >âˆ’</mo><mrow
    ><msub ><mi >s</mi><mi >x</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >q</mi><mi >r</mi></msub></mrow></mrow></mtd><mtd ><mrow ><msub ><mi >s</mi><mi
    >y</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow></mtd><mtd
    ><mrow ><mo >âˆ’</mo><mrow ><mn >2</mn><mo lspace="0em" rspace="0em" >â€‹</mo><msub
    ><mi >s</mi><mi >z</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi
    >q</mi><mi >j</mi></msub></mrow></mrow></mtd></mtr></mtable><mo >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow><annotation-xml encoding="MathML-Content" ><apply ><apply ><apply
    ><ci >ğ‘€</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘—</ci></apply></apply></apply><apply ><cn type="integer"  >2</cn><matrix
    ><matrixrow ><apply ><apply ><cn type="integer"  >2</cn><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘¥</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘—</ci></apply></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘¦</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘–</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘§</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘Ÿ</ci></apply></apply></matrixrow><matrixrow ><apply ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¥</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘–</ci></apply></apply><cn
    type="integer"  >0</cn><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘§</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘˜</ci></apply></apply></matrixrow><matrixrow ><apply ><apply ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¥</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘Ÿ</ci></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¦</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘˜</ci></apply></apply><apply
    ><apply ><cn type="integer"  >2</cn><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘§</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘—</ci></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\frac{\partial M}{\partial q_{j}}=2\left(\begin{smallmatrix}-2s_{x}q_{j}&s_{y}q_{i}&s_{z}q_{r}\\
    s_{x}q_{i}&0&s_{z}q_{k}\\ -s_{x}q_{r}&s_{y}q_{k}&-2s_{z}q_{j}\end{smallmatrix}\right),</annotation></semantics></math>
    | <math   alttext="\displaystyle\frac{\partial M}{\partial q_{k}}=2\left(\begin{smallmatrix}-2s_{x}q_{k}&amp;-s_{y}q_{r}&amp;s_{z}q_{i}\\
    s_{x}q_{r}&amp;-2s_{y}q_{k}&amp;s_{z}q_{j}\\
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: \(\displaystyle\frac{\partial M}{\partial q_{j}}=2\left(\begin{smallmatrix}-2s_{x}q_{j}&s_{y}q_{i}&s_{z}q_{r}\\
    s_{x}q_{i}&0&s_{z}q_{k}\\ -s_{x}q_{r}&s_{y}q_{k}&-2s_{z}q_{j}\end{smallmatrix}\right),\)
- en: s_{x}q_{i}&amp;s_{y}q_{j}&amp;0\end{smallmatrix}\right)" display="inline"><semantics
    ><mrow  ><mstyle displaystyle="true"  ><mfrac ><mrow ><mo rspace="0em" >âˆ‚</mo><mi
    >M</mi></mrow><mrow ><mo rspace="0em" >âˆ‚</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow></mfrac></mstyle><mo
    >=</mo><mrow ><mn >2</mn><mo lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo  >(</mo><mtable
    columnspacing="5pt" rowspacing="0pt"  ><mtr ><mtd ><mrow  ><mo >âˆ’</mo><mrow ><mn
    >2</mn><mo lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >s</mi><mi >x</mi></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi >k</mi></msub></mrow></mrow></mtd><mtd
    ><mrow ><mo >âˆ’</mo><mrow ><msub ><mi >s</mi><mi >y</mi></msub><mo lspace="0em"
    rspace="0em" >â€‹</mo><msub ><mi >q</mi><mi >r</mi></msub></mrow></mrow></mtd><mtd
    ><mrow ><msub ><mi >s</mi><mi >z</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><msub
    ><mi >q</mi><mi >i</mi></msub></mrow></mtd></mtr><mtr ><mtd ><mrow  ><msub ><mi
    >s</mi><mi >x</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >q</mi><mi
    >r</mi></msub></mrow></mtd><mtd ><mrow ><mo >âˆ’</mo><mrow ><mn >2</mn><mo lspace="0em"
    rspace="0em" >â€‹</mo><msub ><mi >s</mi><mi >y</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >q</mi><mi >k</mi></msub></mrow></mrow></mtd><mtd ><mrow ><msub ><mi >s</mi><mi
    >z</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><msub ><mi >s</mi><mi >x</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >q</mi><mi >i</mi></msub></mrow></mtd><mtd ><mrow ><msub ><mi >s</mi><mi
    >y</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><msub ><mi >q</mi><mi >j</mi></msub></mrow></mtd><mtd
    ><mn >0</mn></mtd></mtr></mtable><mo >)</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >ğ‘€</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘˜</ci></apply></apply></apply><apply
    ><cn type="integer" >2</cn><matrix ><matrixrow ><apply ><apply ><cn type="integer"
    >2</cn><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¥</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘˜</ci></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¦</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘Ÿ</ci></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘§</ci></apply><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘–</ci></apply></apply></matrixrow><matrixrow
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¥</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘Ÿ</ci></apply></apply><apply
    ><apply ><cn type="integer" >2</cn><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘¦</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘˜</ci></apply></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ </ci><ci >ğ‘§</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><ci >ğ‘—</ci></apply></apply></matrixrow><matrixrow ><apply ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¥</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘–</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><ci >ğ‘¦</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><ci >ğ‘—</ci></apply></apply><cn
    type="integer" >0</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle\frac{\partial M}{\partial q_{k}}=2\left(\begin{smallmatrix}-2s_{x}q_{k}&-s_{y}q_{r}&s_{z}q_{i}\\
    s_{x}q_{r}&-2s_{y}q_{k}&s_{z}q_{j}\\ s_{x}q_{i}&s_{y}q_{j}&0\end{smallmatrix}\right)</annotation></semantics></math>
    |  |
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: \(\frac{\partial M}{\partial q_{k}}=2\left(\begin{smallmatrix}-2s_{x}q_{k}&-s_{y}q_{r}&s_{z}q_{i}\\
    s_{x}q_{r}&-2s_{y}q_{k}&s_{z}q_{j}\\ s_{x}q_{i}&s_{y}q_{j}&0\end{smallmatrix}\right)\)
- en: Tile-based rasterizer for real-time rendering
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç”¨äºå®æ—¶æ¸²æŸ“çš„åŸºäºç“¦ç‰‡çš„å…‰æ …åŒ–å™¨
- en: A technique called Tile-based Rasterizer is used to quickly render the 3D model
    constructed by the Gaussians (see Fig 13). This approach first uses a given view
    angle $V$ of the camera and its position $p$ to filter out the Gaussians that
    are not contributing to the view frustum. In this way only the useful Gaussians
    are involved in the $\alpha$-blending, improving the rendering efficiency.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§ç§°ä¸ºåŸºäºç“¦ç‰‡çš„å…‰æ …åŒ–å™¨çš„æŠ€æœ¯è¢«ç”¨æ¥å¿«é€Ÿæ¸²æŸ“ç”±é«˜æ–¯æ„å»ºçš„3Dæ¨¡å‹ï¼ˆè§å›¾13ï¼‰ã€‚è¿™ç§æ–¹æ³•é¦–å…ˆä½¿ç”¨ç»™å®šçš„æ‘„åƒæœºè§†è§’$V$å’Œä½ç½®$p$æ¥è¿‡æ»¤æ‰ä¸è´¡çŒ®äºè§†é”¥ä½“çš„é«˜æ–¯ã€‚è¿™æ ·ï¼Œåªæœ‰æœ‰ç”¨çš„é«˜æ–¯å‚ä¸$\alpha$-æ··åˆï¼Œæé«˜äº†æ¸²æŸ“æ•ˆç‡ã€‚
- en: '![Refer to caption](img/5ef26be5b9c73def8f5da0b6a46b7199.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/5ef26be5b9c73def8f5da0b6a46b7199.png)'
- en: 'Figure 20: Overview of Tile-based rasterize areas, where green Gaussians are
    contributing to the view frustum, and red Gaussians are not, given camera angle
    $V$.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 20: åŸºäºç“¦ç‰‡çš„å…‰æ …åŒ–åŒºåŸŸæ¦‚è¿°ï¼Œå…¶ä¸­ç»¿è‰²é«˜æ–¯å¯¹è§†é”¥ä½“æœ‰è´¡çŒ®ï¼Œçº¢è‰²é«˜æ–¯æ²¡æœ‰ï¼Œç»™å®šæ‘„åƒæœºè§’åº¦$V$ã€‚'
- en: Instead of sorting all the Gaussian individually for per-pixel $\alpha$-blending,
    tiles are created to divide the 2D screen into smaller 16x16 sections. An instance
    key is assigned to each of the Gaussians using their corresponding view-space
    depth and the tiles they reside with a tile ID. The Gaussians are then sorted
    according to their instance key using a GPU Radix sort. The Radix sort is capable
    of handling large sets of data in parallel with GPU threads. The result of the
    Gaussian sorting can also demonstrate the depth level of tiles.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶å¯¹æ¯ä¸ªåƒç´ è¿›è¡Œå•ç‹¬æ’åºä»¥å®ç°$\alpha$-æ··åˆï¼Œä¸å¦‚åˆ›å»ºç“¦ç‰‡å°†2Då±å¹•åˆ’åˆ†ä¸ºæ›´å°çš„16x16åŒºåŸŸã€‚æ¯ä¸ªé«˜æ–¯ä½¿ç”¨å…¶ç›¸åº”çš„è§†ç©ºé—´æ·±åº¦å’Œå®ƒä»¬æ‰€åœ¨çš„ç“¦ç‰‡
    ID åˆ†é…ä¸€ä¸ªå®ä¾‹é”®ã€‚ç„¶åï¼Œé«˜æ–¯æ ¹æ®å…¶å®ä¾‹é”®ä½¿ç”¨GPUåŸºæ•°æ’åºè¿›è¡Œæ’åºã€‚åŸºæ•°æ’åºèƒ½å¤Ÿåœ¨GPUçº¿ç¨‹ä¸­å¹¶è¡Œå¤„ç†å¤§è§„æ¨¡æ•°æ®ã€‚é«˜æ–¯æ’åºçš„ç»“æœè¿˜å¯ä»¥å±•ç¤ºç“¦ç‰‡çš„æ·±åº¦çº§åˆ«ã€‚
- en: After sorting, a thread block is assigned to each tile, and the Gaussians are
    loaded into the corresponding memory. $\alpha$-blending is then performed from
    front to back on the sorted list of Gaussians onto the 2D scene, using the cumulative
    color and opacity $\alpha$ of the Gaussians until each pixel reaches a target
    alpha saturation. This design maximizes computation efficiency by enabling parallelism
    of both tile rendering and Gaussian loading in the shared memory space.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: æ’åºåï¼Œå°†æ¯ä¸ªçº¿ç¨‹å—åˆ†é…ç»™æ¯ä¸ªç“¦ç‰‡ï¼Œé«˜æ–¯åˆ†å¸ƒè¢«åŠ è½½åˆ°ç›¸åº”çš„å†…å­˜ä¸­ã€‚ç„¶åï¼Œåœ¨å·²æ’åºçš„é«˜æ–¯åˆ—è¡¨ä¸Šè¿›è¡Œä»å‰åˆ°åçš„$\alpha$-æ··åˆï¼Œä½¿ç”¨é«˜æ–¯çš„ç´¯è®¡é¢œè‰²å’Œä¸é€æ˜åº¦$\alpha$ï¼Œç›´åˆ°æ¯ä¸ªåƒç´ è¾¾åˆ°ç›®æ ‡
    alpha é¥±å’Œåº¦ã€‚è¿™ç§è®¾è®¡é€šè¿‡åœ¨å…±äº«å†…å­˜ç©ºé—´ä¸­å®ç°ç“¦ç‰‡æ¸²æŸ“å’Œé«˜æ–¯åŠ è½½çš„å¹¶è¡Œæ€§ï¼Œä»è€Œæœ€å¤§åŒ–è®¡ç®—æ•ˆç‡ã€‚
- en: Algorithm 3 GPU software rasterization of 3D Gaussians
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³• 3 GPU è½¯ä»¶å…‰æ …åŒ– 3D é«˜æ–¯åˆ†å¸ƒ
- en: '$w$, $h$: width and height of the image to rasterize'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '$w$, $h$: éœ€è¦å…‰æ …åŒ–çš„å›¾åƒçš„å®½åº¦å’Œé«˜åº¦'
- en: '$M$, $S$: Gaussian means and covariances in world space'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '$M$, $S$: ä¸–ç•Œç©ºé—´ä¸­çš„é«˜æ–¯å‡å€¼å’Œåæ–¹å·®'
- en: '$C$, $A$: Gaussian colors and opacities'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '$C$, $A$: é«˜æ–¯é¢œè‰²å’Œä¸é€æ˜åº¦'
- en: '$V$: view configuration of current camera'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '$V$: å½“å‰æ‘„åƒæœºçš„è§†å›¾é…ç½®'
- en: functionÂ Rasterize($w$, $h$, $M$, $S$, $C$, $A$, $V$)Â Â Â Â Â CullGaussian($p$,
    $V$) $\triangleright$ Frustum CullingÂ Â Â Â Â $M^{\prime},S^{\prime}$ $\leftarrow$
    ScreenspaceGaussians($M$, $S$, $V$) $\triangleright$ TransformÂ Â Â Â Â $T$ $\leftarrow$
    CreateTiles($w$, $h$)Â Â Â Â Â $L$, $K$ $\leftarrow$ DuplicateWithKeys($M^{\prime}$,
    $T$) $\triangleright$ Indices and KeysÂ Â Â Â Â SortByKeys($K$, $L$) $\triangleright$
    Globally SortÂ Â Â Â Â $R$ $\leftarrow$ IdentifyTileRanges($T$, $K$)Â Â Â Â Â $I\leftarrow\mathbf{0}$
    $\triangleright$ Init CanvasÂ Â Â Â Â for allÂ Tiles $t$ in $I$Â doÂ Â Â Â Â Â Â Â Â for allÂ Pixels
    $i$ in $t$Â doÂ Â Â Â Â Â Â Â Â Â Â Â Â Â $r\leftarrow$ GetTileRange($R$, $t$)Â Â Â Â Â Â Â Â Â Â Â Â Â Â $I[i]\leftarrow$
    BlendInOrder($i$, $L$, $r$, $K$, $M^{\prime}$, $S^{\prime}$, $C$, $A$)Â Â Â Â Â Â Â Â Â endÂ forÂ Â Â Â Â endÂ forreturn
    $I$endÂ function
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: function Rasterize($w$, $h$, $M$, $S$, $C$, $A$, $V$)Â Â Â Â Â CullGaussian($p$,
    $V$) $\triangleright$ è£å‰ªÂ Â Â Â Â $M^{\prime},S^{\prime}$ $\leftarrow$ ScreenspaceGaussians($M$,
    $S$, $V$) $\triangleright$ è½¬æ¢Â Â Â Â Â $T$ $\leftarrow$ CreateTiles($w$, $h$)Â Â Â Â Â $L$,
    $K$ $\leftarrow$ DuplicateWithKeys($M^{\prime}$, $T$) $\triangleright$ ç´¢å¼•å’Œé”®Â Â Â Â Â SortByKeys($K$,
    $L$) $\triangleright$ å…¨å±€æ’åºÂ Â Â Â Â $R$ $\leftarrow$ IdentifyTileRanges($T$, $K$)Â Â Â Â Â $I\leftarrow\mathbf{0}$
    $\triangleright$ åˆå§‹åŒ–ç”»å¸ƒÂ Â Â Â Â for all Tiles $t$ in $I$ doÂ Â Â Â Â Â Â Â Â for all Pixels
    $i$ in $t$ doÂ Â Â Â Â Â Â Â Â Â Â Â Â Â $r\leftarrow$ GetTileRange($R$, $t$)Â Â Â Â Â Â Â Â Â Â Â Â Â Â $I[i]\leftarrow$
    BlendInOrder($i$, $L$, $r$, $K$, $M^{\prime}$, $S^{\prime}$, $C$, $A$)Â Â Â Â Â Â Â Â Â end
    forÂ Â Â Â Â end forreturn $I$end function
- en: Limitations
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é™åˆ¶
- en: Large memory bandwidth
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¤§å†…å­˜å¸¦å®½
- en: To achieve real-time rendering with high frames per second, a parallel computing
    approach is used in the $Rasterize()$ function. This involves a large amount of
    dynamic data loading occurring in the shared memory of each tile during the $\alpha$-blending
    process. Therefore, a large GPU memory bandwidth is required to support the data
    traffic.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°é«˜å¸§ç‡çš„å®æ—¶æ¸²æŸ“ï¼Œ$Rasterize()$ å‡½æ•°ä¸­ä½¿ç”¨äº†å¹¶è¡Œè®¡ç®—æ–¹æ³•ã€‚è¿™æ¶‰åŠåˆ°åœ¨æ¯ä¸ªç“¦ç‰‡çš„å…±äº«å†…å­˜ä¸­å‘ç”Ÿå¤§é‡çš„åŠ¨æ€æ•°æ®åŠ è½½ï¼Œåœ¨ $\alpha$-æ··åˆè¿‡ç¨‹ä¸­ã€‚å› æ­¤ï¼Œéœ€è¦è¾ƒå¤§çš„
    GPU å†…å­˜å¸¦å®½æ¥æ”¯æŒæ•°æ®æµé‡ã€‚
- en: Robustness in Sparse Viewpoints
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¨€ç–è§†è§’ä¸‹çš„é²æ£’æ€§
- en: Gaussians are optimized by taking the gradient compared with the true camera
    view. However, viewpoints with few or no data points have less data to optimize
    the Guassians in their region resulting in artifacts and distortions.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯é€šè¿‡ä¸çœŸå®ç›¸æœºè§†å›¾æ¯”è¾ƒæ¢¯åº¦è¿›è¡Œä¼˜åŒ–ã€‚ç„¶è€Œï¼Œæ•°æ®ç‚¹è¾ƒå°‘æˆ–æ²¡æœ‰æ•°æ®ç‚¹çš„è§†è§’åœ¨å…¶åŒºåŸŸä¼˜åŒ–é«˜æ–¯æ—¶çš„æ•°æ®è¾ƒå°‘ï¼Œä»è€Œå¯¼è‡´ä¼ªå½±å’Œå¤±çœŸã€‚
- en: Future Trends
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœªæ¥è¶‹åŠ¿
- en: Semantic-Driven 3D Reconstruction
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è¯­ä¹‰é©±åŠ¨çš„ 3D é‡å»º
- en: 'Many 3D reconstruction techniques focus on generating 3D models from images.
    Yet, the integration of text prompts as a guiding factor presents an exciting
    avenue for future research. For example, the method outlined in the paper â€Semantic-Driven
    3D Reconstruction from Single Imagesâ€[[18](#bib.bib18)] demonstrates how textual
    cues can significantly enhance both the precision and contextual relevance of
    reconstructed models. While, â€LGM: Large Multi-View Gaussian Model for High-Resolution
    3D Content Creationâ€ [[14](#bib.bib14)] demonstrate impressive zero-shot 3D generations
    from only a text prompt.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤š 3D é‡å»ºæŠ€æœ¯ä¸“æ³¨äºä»å›¾åƒä¸­ç”Ÿæˆ 3D æ¨¡å‹ã€‚ç„¶è€Œï¼Œæ–‡æœ¬æç¤ºä½œä¸ºæŒ‡å¯¼å› ç´ çš„æ•´åˆå‘ˆç°å‡ºæœªæ¥ç ”ç©¶çš„æ¿€åŠ¨äººå¿ƒçš„æ–¹å‘ã€‚ä¾‹å¦‚ï¼Œè®ºæ–‡ã€Šä»å•å¼ å›¾åƒè¿›è¡Œè¯­ä¹‰é©±åŠ¨çš„
    3D é‡å»ºã€‹ä¸­[[18](#bib.bib18)]å±•ç¤ºäº†æ–‡æœ¬æç¤ºå¦‚ä½•æ˜¾è‘—æå‡é‡å»ºæ¨¡å‹çš„ç²¾åº¦å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚è€Œã€ŠLGMï¼šé«˜åˆ†è¾¨ç‡ 3D å†…å®¹åˆ›å»ºçš„å¤§å‹å¤šè§†è§’é«˜æ–¯æ¨¡å‹ã€‹[[14](#bib.bib14)]å±•ç¤ºäº†ä»…é€šè¿‡æ–‡æœ¬æç¤ºå°±èƒ½è¿›è¡Œä»¤äººå°è±¡æ·±åˆ»çš„é›¶-shot
    3D ç”Ÿæˆã€‚
- en: Dynamic 3D scene reconstruction
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åŠ¨æ€ 3D åœºæ™¯é‡å»º
- en: The previously mentioned approaches can only use captured information in static
    scenes to reproduce static 3D models, where any structure change of scene during
    information capturing will result in misinformation that leads to under-reconstruction
    in specific areas. To achieve dynamic 3D scene reconstruction, 4D Gaussian Splatting
    utilizes a set of canonical 3D Gaussians and transforming them through a deformation
    field at different times, resulting in producing dynamically changing 3D models
    that can represent the motion of objects over time [[16](#bib.bib16)].
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: å‰è¿°æ–¹æ³•ä»…èƒ½åˆ©ç”¨é™æ€åœºæ™¯ä¸­çš„æ•è·ä¿¡æ¯é‡å»ºé™æ€ 3D æ¨¡å‹ï¼Œå…¶ä¸­ä¿¡æ¯æ•è·è¿‡ç¨‹ä¸­åœºæ™¯çš„ä»»ä½•ç»“æ„å˜åŒ–éƒ½ä¼šå¯¼è‡´è¯¯ä¿¡æ¯ï¼Œä»è€Œåœ¨ç‰¹å®šåŒºåŸŸä¸‹é‡å»ºä¸è¶³ã€‚ä¸ºäº†å®ç°åŠ¨æ€ 3D
    åœºæ™¯é‡å»ºï¼Œ4D é«˜æ–¯å–·æº…åˆ©ç”¨ä¸€ç»„å…¸å‹çš„ 3D é«˜æ–¯ï¼Œå¹¶é€šè¿‡åœ¨ä¸åŒæ—¶é—´çš„å˜å½¢åœºè¿›è¡Œè½¬æ¢ï¼Œä»è€Œç”Ÿæˆèƒ½å¤Ÿè¡¨ç¤ºç‰©ä½“éšæ—¶é—´å˜åŒ–çš„åŠ¨æ€ 3D æ¨¡å‹[[16](#bib.bib16)]ã€‚
- en: Single View 3D reconstruction
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å•è§†è§’ 3D é‡å»º
- en: Building on the methodology introduced in Zero 1-to-3, an area that has gained
    significant traction is Single View 3D reconstruction. Leveraging diffusion models
    to generate 3D objects from a single image, [[14](#bib.bib14)] and [[17](#bib.bib17)]
    have demonstrated promising work in this domain.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºã€ŠZero 1-to-3ã€‹ä¸­ä»‹ç»çš„æ–¹æ³•ï¼Œä¸€ä¸ªè·å¾—æ˜¾è‘—å…³æ³¨çš„é¢†åŸŸæ˜¯å•è§†è§’ 3D é‡å»ºã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä»å•å¼ å›¾åƒç”Ÿæˆ 3D å¯¹è±¡ï¼Œ[[14](#bib.bib14)]
    å’Œ [[17](#bib.bib17)] åœ¨è¿™ä¸€é¢†åŸŸå±•ç¤ºäº†æœ‰å‰æ™¯çš„å·¥ä½œã€‚
- en: References
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic
    models, 2020.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Jonathan Ho, Ajay Jain, å’Œ Pieter Abbeel. å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼Œ2020å¹´ã€‚'
- en: '[2] Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis.
    3d gaussian splatting for real-time radiance field rendering, 2023.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, å’Œ George Drettakis.
    å®æ—¶è¾å°„åœºæ¸²æŸ“çš„ 3D é«˜æ–¯å–·æº…ï¼Œ2023å¹´ã€‚'
- en: '[3] Agustinus Kristiadi. Kl divergence: Forward vs reverse? https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/,
    2016. Accessed: 2024-04-22.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Agustinus Kristiadi. KL æ•£åº¦ï¼šå‰å‘ä¸åå‘ï¼Ÿ https://agustinus.kristia.de/techblog/2016/12/21/forward-reverse-kl/ï¼Œ2016å¹´ã€‚è®¿é—®æ—¶é—´ï¼š2024å¹´4æœˆ22æ—¥ã€‚'
- en: '[4] Hung-yi Lee. Forward process of ddpm, April 2023. https://www.youtube.com/watch?v=ifCDXFdeaaM&t=608.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Hung-yi Lee. ddpm çš„å‰å‘è¿‡ç¨‹ï¼Œ2023å¹´4æœˆã€‚ https://www.youtube.com/watch?v=ifCDXFdeaaM&t=608.'
- en: '[5] Ruoshi Liu, Rundi Wu, BasileÂ Van Hoorick, Pavel Tokmakov, Sergey Zakharov,
    and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov,
    å’Œ Carl Vondrick. Zero-1-to-3ï¼šé›¶-shot ä¸€å¼ å›¾åƒåˆ° 3D å¯¹è±¡ï¼Œ2023å¹´ã€‚'
- en: '[6] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas
    Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes
    from images. ACM Trans. Graph., 38(4):65:1â€“65:14, July 2019.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas
    Lehrmann, å’Œ Yaser Sheikhã€‚ç¥ç»ä½“ç§¯ï¼šä»å›¾åƒä¸­å­¦ä¹ åŠ¨æ€å¯æ¸²æŸ“ä½“ç§¯ã€‚ã€ŠACM Trans. Graph.ã€‹ï¼Œ38(4):65:1â€“65:14ï¼Œ2019å¹´7æœˆã€‚'
- en: '[7] Ricardo Martin-Brualla, Noha Radwan, Mehdi S.Â M. Sajjadi, JonathanÂ T. Barron,
    Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields
    for unconstrained photo collections, 2021.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron,
    Alexey Dosovitskiy, å’Œ Daniel Duckworthã€‚Nerf in the wild: ç”¨äºæ— çº¦æŸç…§ç‰‡é›†çš„ç¥ç»è¾å°„åœºï¼Œ2021å¹´ã€‚'
- en: '[8] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and
    Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space,
    2019.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, å’Œ
    Andreas Geigerã€‚å æ®ç½‘ç»œï¼šåœ¨å‡½æ•°ç©ºé—´ä¸­å­¦ä¹ 3Dé‡å»ºï¼Œ2019å¹´ã€‚'
- en: '[9] Ben Mildenhall, PratulÂ P. Srinivasan, Matthew Tancik, JonathanÂ T. Barron,
    Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields
    for view synthesis, 2020.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron,
    Ravi Ramamoorthi, å’Œ Ren Ngã€‚Nerf: é€šè¿‡ç¥ç»è¾å°„åœºè¡¨ç¤ºåœºæ™¯ç”¨äºè§†å›¾åˆæˆï¼Œ2020å¹´ã€‚'
- en: '[10] Thomas MÃ¼ller, Alex Evans, Christoph Schied, and Alexander Keller. Instant
    neural graphics primitives with a multiresolution hash encoding. ACM Transactions
    on Graphics, 41(4):1â€“15, July 2022.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Thomas MÃ¼ller, Alex Evans, Christoph Schied, å’Œ Alexander Kellerã€‚ä½¿ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œç¼–ç çš„å³æ—¶ç¥ç»å›¾å½¢åŸè¯­ã€‚ã€ŠACM
    Transactions on Graphicsã€‹ï¼Œ41(4):1â€“15ï¼Œ2022å¹´7æœˆã€‚'
- en: '[11] JeongÂ Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and
    Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape
    representation, 2019.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, å’Œ Steven
    Lovegroveã€‚Deepsdf: ç”¨äºå½¢çŠ¶è¡¨ç¤ºçš„è¿ç»­ç­¾åè·ç¦»å‡½æ•°å­¦ä¹ ï¼Œ2019å¹´ã€‚'
- en: '[12] Eric Penner and LiÂ Zhang. Soft 3d reconstruction for view synthesis. 36(6),
    2017.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Eric Penner å’Œ Li Zhangã€‚ç”¨äºè§†å›¾åˆæˆçš„è½¯3Dé‡å»ºã€‚36(6)ï¼Œ2017å¹´ã€‚'
- en: '[13] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn
    Ommer. High-resolution image synthesis with latent diffusion models, 2022.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, å’Œ BjÃ¶rn
    Ommerã€‚åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆï¼Œ2022å¹´ã€‚'
- en: '[14] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and
    Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content
    creation. arXiv preprint arXiv:2402.05054, 2024.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, å’Œ
    Ziwei Liuã€‚Lgm: ç”¨äºé«˜åˆ†è¾¨ç‡3Då†…å®¹åˆ›å»ºçš„å¤§å‹å¤šè§†è§’é«˜æ–¯æ¨¡å‹ã€‚arXivé¢„å°æœ¬ arXiv:2402.05054ï¼Œ2024å¹´ã€‚'
- en: '[15] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller,
    Sameh Khamis, Thomas MÃ¼ller, Charles Loop, Nathan Morrical, Koki Nagano, Towaki
    Takikawa, and Stan Birchfield. Rtmv: A ray-traced multi-view synthetic dataset
    for novel view synthesis, 2022.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller,
    Sameh Khamis, Thomas MÃ¼ller, Charles Loop, Nathan Morrical, Koki Nagano, Towaki
    Takikawa, å’Œ Stan Birchfieldã€‚Rtmv: ç”¨äºæ–°è§†å›¾åˆæˆçš„å…‰çº¿è¿½è¸ªå¤šè§†è§’åˆæˆæ•°æ®é›†ï¼Œ2022å¹´ã€‚'
- en: '[16] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei,
    Wenyu Liu, QiÂ Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic
    scene rendering, 2023.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei,
    Wenyu Liu, Qi Tian, å’Œ Xinggang Wangã€‚4Dé«˜æ–¯ç‚¹äº‘ç”¨äºå®æ—¶åŠ¨æ€åœºæ™¯æ¸²æŸ“ï¼Œ2023å¹´ã€‚'
- en: '[17] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying
    Shan. Instantmesh: Efficient 3d mesh generation from a single image with sparse-view
    large reconstruction models, 2024.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, å’Œ Ying
    Shanã€‚Instantmesh: ä»å•å¼ å›¾åƒä¸­é«˜æ•ˆç”Ÿæˆ3Dç½‘æ ¼ï¼Œé…åˆç¨€ç–è§†å›¾çš„å¤§è§„æ¨¡é‡å»ºæ¨¡å‹ï¼Œ2024å¹´ã€‚'
- en: '[18] Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing Tai, and Chi-Keung Tang. Facednerf:
    Semantics-driven face reconstruction, prompt editing and relighting with diffusion
    models, 2023.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing Tai, å’Œ Chi-Keung Tangã€‚Facednerf:
    åŸºäºè¯­ä¹‰çš„é¢éƒ¨é‡å»ºã€æç¤ºç¼–è¾‘å’Œæ‰©æ•£æ¨¡å‹ä¸‹çš„é‡å…‰ç…§ï¼Œ2023å¹´ã€‚'
- en: '[19] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. Differentiable
    point-based radiance fields for efficient view synthesis, 2023.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, å’Œ Felix Heideã€‚ç”¨äºé«˜æ•ˆè§†å›¾åˆæˆçš„å¯å¾®åˆ†ç‚¹äº‘è¾å°„åœºï¼Œ2023å¹´ã€‚'
