- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:31:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:31:06
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2407.17877] Advancing 3D Point Cloud Understanding through Deep Transfer Learning:
    A Comprehensive Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2407.17877] 通过深度迁移学习推动3D点云理解：综合调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.17877](https://ar5iv.labs.arxiv.org/html/2407.17877)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.17877](https://ar5iv.labs.arxiv.org/html/2407.17877)
- en: \useunder
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \useunder
- en: \ul
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \ul
- en: \cormark
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \cormark
- en: '[1]'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]'
- en: '[] []'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[] []'
- en: 'Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过深度迁移学习推动3D点云理解：综合调查
- en: Shahab Saquib Sohail    Yassine Himeur yhimeur@ud.ac.ae    Hamza Kheddar   
    Abbes Amira    Fodil Fadli    Shadi Atalla    Abigail Copiaco    Wathiq Mansoor
    School of Computing Science and Engineering, VIT Bhopal University, Sehore, MP
    466114, India College of Engineering and Information Technology, University of
    Dubai, Dubai, UAE LSEA Laboratory, Department of Electrical Engineering, University
    of Medea, 26000, Algeria Department of Computer Science, University of Sharjah,
    UAE Institute of Artificial Intelligence, De Montfort University, Leicester, United
    Kingdom Department of Architecture & Urban Planning, Qatar University, Doha, 2713,
    Qatar
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Shahab Saquib Sohail    Yassine Himeur yhimeur@ud.ac.ae    Hamza Kheddar   
    Abbes Amira    Fodil Fadli    Shadi Atalla    Abigail Copiaco    Wathiq Mansoor
    计算机科学与工程学院，VIT 博帕尔大学，印度塞霍尔，MP 466114 迪拜大学工程与信息技术学院，阿联酋迪拜 Medea大学电气工程系LSEA实验室，阿尔及利亚26000
    沙迦大学计算机科学系，阿联酋 迪蒙福特大学人工智能研究所，英国莱斯特 卡塔尔大学建筑与城市规划系，卡塔尔多哈，2713
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The 3D point cloud (3DPC) has significantly evolved and benefited from the advance
    of deep learning (DL). However, the latter faces various issues, including the
    lack of data or annotated data, the existence of a significant gap between training
    data and test data, and the requirement for high computational resources. To that
    end, deep transfer learning (DTL), which decreases dependency and costs by utilizing
    knowledge gained from a source data/task in training a target data/task, has been
    widely investigated. Numerous DTL frameworks have been suggested for aligning
    point clouds obtained from several scans of the same scene. Additionally, DA,
    which is a subset of DTL, has been modified to enhance the point cloud data’s
    quality by dealing with noise and missing points. Ultimately, fine-tuning and
    DA approaches have demonstrated their effectiveness in addressing the distinct
    difficulties inherent in point cloud data. This paper presents the first review
    shedding light on this aspect. it provides a comprehensive overview of the latest
    techniques for understanding 3DPC using DTL and domain adaptation (DA). Accordingly,
    DTL’s background is first presented along with the datasets and evaluation metrics.
    A well-defined taxonomy is introduced, and detailed comparisons are presented,
    considering different aspects such as different knowledge transfer strategies,
    and performance. The paper covers various applications, such as 3DPC object detection,
    semantic labeling, segmentation, classification, registration, downsampling/upsampling,
    and denoising. Furthermore, the article discusses the advantages and limitations
    of the presented frameworks, identifies open challenges, and suggests potential
    research directions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 3D点云（3DPC）随着深度学习（DL）的进步显著发展并受益。然而，后者面临各种问题，包括数据或标注数据的缺乏、训练数据与测试数据之间存在显著差距以及对高计算资源的需求。因此，深度迁移学习（DTL）通过利用从源数据/任务中获得的知识来减少对目标数据/任务的依赖和成本，已被广泛研究。许多DTL框架已被提出用于对齐来自同一场景的多个扫描点云。此外，作为DTL的一个子集，领域适应（DA）已被修改，以通过处理噪声和缺失点来提高点云数据的质量。最终，微调和DA方法在解决点云数据固有的不同难题方面表现出了有效性。本文首次回顾了这一方面，提供了使用DTL和领域适应（DA）理解3DPC的最新技术的全面概述。相应地，首先介绍DTL的背景以及数据集和评估指标。引入了明确的分类法，并考虑不同方面如知识转移策略和性能，提供了详细的比较。本文涵盖了各种应用，如3DPC目标检测、语义标注、分割、分类、配准、下采样/上采样和去噪。此外，文章讨论了所提出框架的优缺点，识别了开放挑战，并提出了潜在的研究方向。
- en: 'keywords:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 3D point cloud \sepDeep transfer learning \sepDomain adaption \sepFine-tuning
    \sepClassification \sepSegmentation and registration
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 3D点云 \sep 深度迁移学习 \sep 领域适应 \sep 微调 \sep 分类 \sep 分割与配准
- en: 3DPC
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 3DPC
- en: 3D point cloud
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 点云
- en: GAN
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: GAN
- en: generative adversarial network
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: CV
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: CV
- en: computer vision
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: DL
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: DL
- en: deep learning
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习
- en: ML
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ML
- en: machine learning
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习
- en: DTL
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DTL
- en: deep transfer learning
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 深度迁移学习
- en: DA
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: DA
- en: domain adaptation
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应
- en: UDA
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: UDA
- en: unsupervised domain adaptation
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督领域适应
- en: TD
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: TD
- en: target domain
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 目标领域
- en: SD
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: SD
- en: source omain
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 源领域
- en: CNN
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: CNN
- en: convolutional neural network
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: MAE
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: MAE
- en: multi-scale masked autoencoder
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度掩码自编码器
- en: IoU
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: IoU
- en: intersection over union
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 交并比
- en: SSL
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: SSL
- en: self-supervised learning
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习
- en: SVCN
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SVCN
- en: sparse voxel completion network
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏体素补全网络
- en: OA
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: OA
- en: overall accuracy
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 总体准确性
- en: CD
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: CD
- en: Chamfer distance
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Chamfer距离
- en: LiDAR
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: LiDAR
- en: light detection and ranging
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 光探测与测距
- en: BiLSTM
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: BiLSTM
- en: bidirectional LSTM
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 双向LSTM
- en: ITL
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ITL
- en: inductive transfer learning
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳迁移学习
- en: SFUDA
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SFUDA
- en: source-free unsupervised DA
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 无源无监督DA
- en: PCM
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: PCM
- en: point cloud mixup
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 点云混合
- en: MLP
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: MLP
- en: multi-layer perceptron
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器
- en: CLIP
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP
- en: contrastive vision-language pretraining
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对比视觉语言预训练
- en: TTL
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: TTL
- en: transductive transfer learning
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 传导迁移学习
- en: CLDA
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: CLDA
- en: cross-modal learning DA
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 跨模态学习DA
- en: AE
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: AE
- en: auto-encoder
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器
- en: RNN
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: RNN
- en: recurrent neural network
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: CRF
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: CRF
- en: conditional random field
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 条件随机场
- en: 1 Introduction
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 1.1 Preliminary
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 初步
- en: \Ac
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: CV continues to attract significant interest as a growing branch of [machine
    learning](#id5.5.id5) ([ML](#id5.5.id5)), which targets different problems in
    smart cities, medicine, autonomous driving, medicine, video surveillance, scene
    understanding, safety, and security [[1](#bib.bib1), [2](#bib.bib2)]. With the
    rapid development of sensor technology, 3D sensors have recently been widely adopted,
    which increased the interest of the [computer vision](#id3.3.id3) ([CV](#id3.3.id3))
    research community in developing 3D sensor data processing methodologies [[3](#bib.bib3),
    [4](#bib.bib4)]. Additionally, with the use of augmented reality and virtual reality
    (AR/VR), 3D vision problems become more important since they provide much richer
    information than 2D [[5](#bib.bib5), [6](#bib.bib6)]. Typically, numerous 3D sensors
    for acquiring 3D data have been used, including depth-sensing cameras (such as
    Apple Depth, RealSense, and Kinect cameras), [light detection and ranging](#id18.18.id18)
    ([LiDAR](#id18.18.id18)), which is used for mobile mapping terrestrial laser scanning,
    aerial [LiDAR](#id18.18.id18) [[7](#bib.bib7)]. Moreover, [generative adversarial
    networks](#id2.2.id2) can be used to augment data when data scarcity problem occurs.
    In this context, 3D data can provide rich scale, shape, and geometric information
    to be complemented with 2D images for better representing the surrounding environment
    [[8](#bib.bib8), [9](#bib.bib9)].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉（CV）作为[机器学习](#id5.5.id5)（[ML](#id5.5.id5)）的一个不断增长的分支，继续吸引大量关注，涉及智能城市、医学、自动驾驶、视频监控、场景理解、安全和保障等不同问题[[1](#bib.bib1)、[2](#bib.bib2)]。随着传感器技术的迅速发展，3D传感器最近被广泛采用，这增加了[计算机视觉](#id3.3.id3)（[CV](#id3.3.id3)）研究社区对开发3D传感器数据处理方法的兴趣[[3](#bib.bib3)、[4](#bib.bib4)]。此外，随着增强现实和虚拟现实（AR/VR）的使用，3D视觉问题变得更加重要，因为它们提供的信息比2D更加丰富[[5](#bib.bib5)、[6](#bib.bib6)]。通常，已经使用了许多获取3D数据的3D传感器，包括深度感应相机（如Apple
    Depth、RealSense和Kinect相机）、[光探测与测距](#id18.18.id18)（[LiDAR](#id18.18.id18)），用于移动测绘、地面激光扫描、航空[LiDAR](#id18.18.id18)
    [[7](#bib.bib7)]。此外，[生成对抗网络](#id2.2.id2)可以在数据稀缺问题发生时用于增强数据。在这种情况下，3D数据可以提供丰富的尺度、形状和几何信息，以与2D图像互补，从而更好地表示周围环境[[8](#bib.bib8)、[9](#bib.bib9)]。
- en: While there are different approaches to representing 3D data, such as volumetric
    grids, meshes, and depth images, the [3D point clouds](#id1.1.id1) are the most
    used. Typically, a [3DPC](#id1.1.id1) representation conserves the original geometric
    information in 3D space [[10](#bib.bib10)]. Besides, a [3DPC](#id1.1.id1) is an
    ensemble of data points representing an object’s surfaces in 3D coordinates. In
    this regard, spatial coordinates are used to represent data points, surface normals,
    and color information and format ( e.g., RGB, HSV, and others) [[11](#bib.bib11)].
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有多种方法来表示3D数据，例如体积网格、网格和深度图像，但[3D点云](#id1.1.id1)是最常用的。通常，[3DPC](#id1.1.id1)表示法保留了3D空间中的原始几何信息[[10](#bib.bib10)]。此外，[3DPC](#id1.1.id1)是一个数据点集合，表示物体在3D坐标中的表面。在这方面，空间坐标用于表示数据点、表面法线和颜色信息及格式（例如RGB、HSV等）[[11](#bib.bib11)]。
- en: Additionally, although [3DPC](#id1.1.id1) can be considered as non-Euclidean
    geometric data, in practice, delineated as small Euclidean subgroups with a standard
    coordinates system and global parametrization [[12](#bib.bib12)]. The success
    of this representation is due to its invariability to transformations and attacks,
    including rotation, scaling, translation, etc., which makes it robust to extracting
    objects’ features. Moreover, using [deep learnings](#id4.4.id4) techniques to
    extract [3DPC](#id1.1.id1) data and perform complex tasks such as detection, classification,
    recognition, and retrieval has expanded their adoption in many research and development
    areas [[13](#bib.bib13)]. Typically, [3DPCs](#id1.1.id1) are widely adopted to
    [CV](#id3.3.id3) tasks, such as object recognition, semantic segmentation, and
    scene understanding [[14](#bib.bib14), [15](#bib.bib15)]. Additionally, they are
    used to create detailed models of environments for navigation and localization,
    detailed maps and models of buildings, landscapes, and other structures, as well
    as detailed models of buildings and other structures for design and planning [[16](#bib.bib16)].
    Moreover, they can be utilized to inspect and maintain industrial equipment, infrastructure,
    and other assets [[17](#bib.bib17), [18](#bib.bib18)]. Besides, the [3DPC](#id1.1.id1)
    technology helps implement realistic and immersive experiences in AR and VR applications.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管[3DPC](#id1.1.id1)可以被视为非欧几里得几何数据，但在实践中，它被划分为小的欧几里得子群，并采用标准坐标系统和全局参数化[[12](#bib.bib12)]。这种表示法成功的原因在于其对变换和攻击的不可变性，包括旋转、缩放、平移等，这使得它在提取对象特征时具有鲁棒性。此外，利用[深度学习](#id4.4.id4)技术提取[3DPC](#id1.1.id1)数据，并执行检测、分类、识别和检索等复杂任务，已经扩大了它们在许多研究和开发领域的应用[[13](#bib.bib13)]。通常，[3DPCs](#id1.1.id1)被广泛用于[计算机视觉](#id3.3.id3)任务，如物体识别、语义分割和场景理解[[14](#bib.bib14),
    [15](#bib.bib15)]。此外，它们还被用于创建环境的详细模型，用于导航和定位，详细的建筑物、景观和其他结构的地图和模型，以及用于设计和规划的建筑物和其他结构的详细模型[[16](#bib.bib16)]。此外，它们还可用于检查和维护工业设备、基础设施和其他资产[[17](#bib.bib17),
    [18](#bib.bib18)]。此外，[3DPC](#id1.1.id1)技术有助于在增强现实和虚拟现实应用中实现逼真和沉浸式体验。
- en: The traditional [ML](#id5.5.id5) approaches and, recently, [DL](#id4.4.id4)
    methods witnessed rapid growth and have attracted researchers because of their
    applicability in many real-life applications, including smart healthcare [[19](#bib.bib19)],
    disease diagnosis and medical image classifications [[4](#bib.bib4)], business
    and marketing [[20](#bib.bib20)], recommender systems [[21](#bib.bib21)], energy
    [[22](#bib.bib22)], agriculture [[23](#bib.bib23)], robotics [[24](#bib.bib24)],
    and more. The primary advantage of these learning algorithms is that they train
    a model that learns the hidden pattern and can be exploited for any specific purpose
    with high accuracy. However, with time, some issues have been raised by the research
    communities. First, these learning algorithms require extensive training datasets,
    especially [DL](#id4.4.id4) algorithms [[1](#bib.bib1)]. Second, most [DL](#id4.4.id4)
    models are based on supervised learning and then require huge amounts of ground-truth
    data. Third, it is a common assumption that the data that is trained and the future
    data to be processed must have the same distribution and be in the same feature
    space [[25](#bib.bib25)]. It becomes difficult and sometimes impossible to maintain
    the aforementioned assumption in several real-life scenarios. For example, when
    performing specific tasks, such as classification in a particular domain; however,
    we may have the requisite trained data in another domain, where both datasets
    may not have the same distribution or same feature space.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的[机器学习](#id5.5.id5)方法以及最近的[深度学习](#id4.4.id4)方法都经历了快速增长，并吸引了研究人员，因为它们在许多实际应用中的适用性，包括智能医疗[[19](#bib.bib19)]、疾病诊断和医学图像分类[[4](#bib.bib4)]、商业和市场营销[[20](#bib.bib20)]、推荐系统[[21](#bib.bib21)]、能源[[22](#bib.bib22)]、农业[[23](#bib.bib23)]、机器人技术[[24](#bib.bib24)]等。这些学习算法的主要优势在于它们能够训练出一个学习隐藏模式的模型，并且可以以高精度用于任何特定目的。然而，随着时间的推移，研究社区提出了一些问题。首先，这些学习算法需要大量的训练数据集，尤其是[深度学习](#id4.4.id4)算法[[1](#bib.bib1)]。其次，大多数[深度学习](#id4.4.id4)模型基于监督学习，因此需要大量的真实数据。第三，普遍的假设是训练数据和未来要处理的数据必须具有相同的分布，并且在相同的特征空间[[25](#bib.bib25)]。在许多现实场景中，保持上述假设变得困难，有时甚至是不可能的。例如，在执行特定任务时，如在特定领域进行分类；然而，我们可能在另一个领域拥有所需的训练数据，其中两个数据集可能没有相同的分布或特征空间。
- en: '![Refer to caption](img/cdc48809bf2a69e00c992328d4267ece.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cdc48809bf2a69e00c992328d4267ece.png)'
- en: 'Figure 1: Difference between conventional ML and DTL techniques for multiple
    tasks: (a) conventional ML and (b) DTL.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：传统机器学习与深度迁移学习技术在多个任务上的区别：（a）传统机器学习和（b）深度迁移学习。
- en: 'Furthermore, an essential factor for ensuring accurate performance of [ML](#id5.5.id5)
    algorithms is consistency in the distribution and feature space of the datasets
    used for training and testing. If the distribution of data changes, it becomes
    necessary to rebuild the model from scratch by collecting new training data [[26](#bib.bib26)].
    However, this process is not only costly but also often impractical due to the
    challenges associated with re-collecting training data. Therefore, there is a
    need for a mechanism that can reduce the expenses associated with re-collecting
    training data, minimize the cost of data labeling, and still achieve high performance
    without requiring extensive amounts of training data. To that end, knowledge transfer
    has been suggested, which would fit the above constraints and can significantly
    improve performance. This knowledge transfer mechanism is termed [deep transfer
    learning](#id6.6.id6) ([DTL](#id6.6.id6)) [[1](#bib.bib1), [26](#bib.bib26), [27](#bib.bib27)].
    Fig. [1](#S1.F1 "Figure 1 ‣ 1.1 Preliminary ‣ 1 Introduction ‣ Advancing 3D Point
    Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey") explains
    briefly the difference between conventional [ML](#id5.5.id5) and [DTL](#id6.6.id6)
    techniques.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，确保[机器学习](#id5.5.id5)算法准确性能的一个重要因素是训练和测试数据集的分布和特征空间的一致性。如果数据的分布发生变化，则需要通过收集新的训练数据从头开始重建模型[[26](#bib.bib26)]。然而，这一过程不仅成本高昂，而且由于重新收集训练数据的挑战，通常不可行。因此，需要一种机制来减少重新收集训练数据的费用，降低数据标注成本，同时在不需要大量训练数据的情况下实现高性能。为此，建议使用知识迁移，这种机制可以满足上述限制，并显著提高性能。该知识迁移机制称为[深度迁移学习](#id6.6.id6)（[DTL](#id6.6.id6)）[[1](#bib.bib1),
    [26](#bib.bib26), [27](#bib.bib27)]。图[1](#S1.F1 "Figure 1 ‣ 1.1 Preliminary ‣
    1 Introduction ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey")简要解释了传统[机器学习](#id5.5.id5)和[深度迁移学习](#id6.6.id6)技术之间的区别。'
- en: 'On the one hand, employing [DL](#id4.4.id4) for [3DPC](#id1.1.id1) poses a
    rather complex challenges because of: (i) the variability in point density and
    reflective intensity, influenced by the distance between objects and [LiDAR](#id18.18.id18)
    sensors, (ii) existence of noise originated from sensors ( e.g., perturbations
    and outliers), (iii) data incompleteness caused by occlusion between objects and
    cluttered background, and (iv) confusion categories caused by shape-similar or
    reflectance-similar objects. On the other hand, challenges of [DL](#id4.4.id4)/[DTL](#id6.6.id6)
    models include (i) permutation and orientation invariance, (ii) 3D translation
    and rotation challenges, (iii) difficulty of handling large-scale datasets, (iv)
    securing computational resources, and (v) low performance. Fig. [2](#S1.F2 "Figure
    2 ‣ 1.1 Preliminary ‣ 1 Introduction ‣ Advancing 3D Point Cloud Understanding
    through Deep Transfer Learning: A Comprehensive Survey") summarizes the tasks
    and challenges related to data and [DL](#id4.4.id4)/[DTL](#id6.6.id6)-based applications
    on [3DPCs](#id1.1.id1).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '一方面，使用[深度学习](#id4.4.id4)处理[3D点云](#id1.1.id1)面临相当复杂的挑战，这些挑战包括：（i）点密度和反射强度的变化，受对象与[激光雷达](#id18.18.id18)传感器之间的距离影响，（ii）来自传感器的噪声（例如，扰动和异常值），（iii）由于对象之间的遮挡和背景杂乱导致的数据不完整，以及（iv）形状相似或反射相似对象引起的混淆类别。另一方面，[深度学习](#id4.4.id4)/[深度迁移学习](#id6.6.id6)模型面临的挑战包括：（i）排列和方向不变性，（ii）3D平移和旋转挑战，（iii）处理大规模数据集的困难，（iv）确保计算资源，以及（v）低性能。图[2](#S1.F2
    "Figure 2 ‣ 1.1 Preliminary ‣ 1 Introduction ‣ Advancing 3D Point Cloud Understanding
    through Deep Transfer Learning: A Comprehensive Survey")总结了与数据和[深度学习](#id4.4.id4)/[深度迁移学习](#id6.6.id6)相关的任务和挑战。'
- en: '![Refer to caption](img/216d9873281d2b6633f8bd0c799ba2dd.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/216d9873281d2b6633f8bd0c799ba2dd.png)'
- en: 'Figure 2: Tasks and challenges related to data and [DL](#id4.4.id4)/[DTL](#id6.6.id6)-based
    applications on [3DPCs](#id1.1.id1).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：与数据和[深度学习](#id4.4.id4)/[深度迁移学习](#id6.6.id6)相关的任务和挑战在[3D点云](#id1.1.id1)上的应用。
- en: 1.2 Our contributions
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 我们的贡献
- en: 'Arguably, one of the top success stories of [DL](#id4.4.id4) is [DTL](#id6.6.id6).
    Accordingly, the finding that pretraining a [DL](#id4.4.id4) network on a rich
    source set (e.g., ImageNet) can help boost performance once fine-tuned on a usually
    much smaller target set has been instrumental to many applications in language
    and [CV](#id3.3.id3). Likewise, several studies have explored the potential of
    [DTL](#id6.6.id6) in [3DPC](#id1.1.id1) applications to address the aforementioned
    challenges. This offers an opportunity to present the first review article that
    comprehensively examines the contributions of [DTL](#id6.6.id6)-based approaches
    in advancing our understanding of 3D scenes, such as [3DPC](#id1.1.id1) segmentation,
    3D object detection, 3D object classification, [3DPC](#id1.1.id1) registration,
    and more. For instance, [DTL](#id6.6.id6) has been shown to be effective in various
    ways, including (i) leveraging knowledge learned from synthetic data to improve
    semantic segmentation in real [LiDAR](#id18.18.id18) [3DPC](#id1.1.id1), (ii)
    enabling accurate classification of [3DPCs](#id1.1.id1) even with limited training
    data, (iii) mitigating the issue of overfitting in [3DPC](#id1.1.id1) classification,
    and (iv) reducing the labor-intensive process of annotating [3DPC](#id1.1.id1)
    datasets, among other benefits. In this respect, this paper first presents a background
    of [DTL](#id6.6.id6), where a well-defined taxonomy is conducted. Moving on, datasets
    and evaluation metrics used to evaluate existing [DTL](#id6.6.id6)-based [3DPCs](#id1.1.id1)
    techniques are discussed. Next, existing studies are overviewed based on different
    aspects, and their pros and cons are identified. After that, the challenges encountered
    when using [DTL](#id6.6.id6) for [3DPCs](#id1.1.id1) are identified before deriving
    future research directions. To summarize, the main contributions of this review
    can be stated as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，[DL](#id4.4.id4)的顶级成功故事之一是[DTL](#id6.6.id6)。因此，发现对[DL](#id4.4.id4)网络进行丰富源集（例如，ImageNet）的预训练，一旦在通常更小的目标集上微调，就能提升性能，这对语言和[CV](#id3.3.id3)的许多应用至关重要。同样，一些研究已经探索了[DTL](#id6.6.id6)在[3DPC](#id1.1.id1)应用中的潜力，以应对上述挑战。这提供了一个机会，呈现第一篇全面审视[DTL](#id6.6.id6)方法在推进我们对3D场景的理解中的贡献的综述文章，例如[3DPC](#id1.1.id1)分割、3D物体检测、3D物体分类、[3DPC](#id1.1.id1)配准等。例如，[DTL](#id6.6.id6)已被证明在多种方式上有效，包括（i）利用从合成数据中学到的知识来改善真实[LiDAR](#id18.18.id18)[3DPC](#id1.1.id1)的语义分割，（ii）即使在训练数据有限的情况下也能实现准确的[3DPCs](#id1.1.id1)分类，（iii）缓解[3DPC](#id1.1.id1)分类中的过拟合问题，（iv）减少标注[3DPC](#id1.1.id1)数据集的劳动密集型过程，以及其他好处。在这方面，本文首先介绍[DTL](#id6.6.id6)的背景，并进行详细的分类。接下来，讨论用于评估现有[DTL](#id6.6.id6)-基于[3DPCs](#id1.1.id1)技术的数据集和评估指标。接着，基于不同方面概述现有研究，并识别其优缺点。之后，识别在使用[DTL](#id6.6.id6)进行[3DPCs](#id1.1.id1)时遇到的挑战，然后推导未来的研究方向。总之，这篇综述的主要贡献可以概括为以下几点：
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Presenting, to the best of the authors’ knowledge, the first review article
    on using [DTL](#id6.6.id6) and [domain adaptation](#id7.7.id7) ([DA](#id7.7.id7))
    for [3DPC](#id1.1.id1) applications;
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 介绍，依据作者的知识，首次综述文章关于使用[DTL](#id6.6.id6)和[领域适配](#id7.7.id7)（[DA](#id7.7.id7)）用于[3DPC](#id1.1.id1)应用的情况；
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Discussing existing datasets and evaluation metrics used for assessing the performance
    of [3DPC](#id1.1.id1) frameworks based in [DTL](#id6.6.id6);
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 讨论现有数据集和用于评估[3DPC](#id1.1.id1)框架性能的评估指标，这些框架基于[DTL](#id6.6.id6)；
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Introducing a well-defined taxonomy to overview existing [DTL](#id6.6.id6)-based
    [3DPC](#id1.1.id1) studies;
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 介绍一个详细的分类法，以概述现有的[DTL](#id6.6.id6)-基于[3DPC](#id1.1.id1)的研究；
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Identifying the current challenges encountered when using [DTL](#id6.6.id6)
    for [3DPC](#id1.1.id1) understanding tasks; and
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 识别在使用[DTL](#id6.6.id6)进行[3DPC](#id1.1.id1)理解任务时遇到的当前挑战；
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Deriving future research directions that can attract significant research interest
    in the near future.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推导出未来研究方向，以吸引近期显著的研究兴趣。
- en: 1.3 Methodology of the survey
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 调查方法
- en: 'The present review study is based on the protocols and procedure suggested
    in [[28](#bib.bib28)], which has recently been adopted widely in many studies
    such as [[29](#bib.bib29)]. This study is motivated by the recent developments
    in [3DPC](#id1.1.id1) technology. There are some reviews available on the theme
    which has exploited [ML](#id5.5.id5) [[30](#bib.bib30)], [DL](#id4.4.id4) [[31](#bib.bib31)],
    and reinforcement learning [[32](#bib.bib32)]; however, to the best of our search
    and efforts, we could not find a review exclusively exploring [DTL](#id6.6.id6)
    for different [3DPC](#id1.1.id1) tasks. The proliferation of [DTL](#id6.6.id6)
    techniques has influenced researchers by virtue of which the span of related techniques
    has been expanding and has covered much recent AI-driven research. Our study gives
    the readers an insight into how [DTL](#id6.6.id6) is being implemented for performing
    many [3DPC](#id1.1.id1) tasks. This review aims at finding the answer to research
    questions presented in Table [1](#S1.T1 "Table 1 ‣ 1.3 Methodology of the survey
    ‣ 1 Introduction ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey").'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '本综述研究基于[[28](#bib.bib28)]中建议的协议和程序，该协议和程序最近在许多研究中得到广泛采用，如[[29](#bib.bib29)]。本研究受到[3DPC](#id1.1.id1)技术近期发展的激励。虽然有一些关于该主题的综述研究探讨了[ML](#id5.5.id5)
    [[30](#bib.bib30)]、[DL](#id4.4.id4) [[31](#bib.bib31)]和强化学习[[32](#bib.bib32)]，但据我们所知，我们没有找到专门探讨[DTL](#id6.6.id6)在不同[3DPC](#id1.1.id1)任务中的综述。由于[DTL](#id6.6.id6)技术的普及，研究人员受到了影响，因此相关技术的范围不断扩展，涵盖了许多最新的人工智能驱动的研究。我们的研究为读者提供了对[DTL](#id6.6.id6)在执行多个[3DPC](#id1.1.id1)任务中应用的见解。本综述旨在回答表[1](#S1.T1
    "Table 1 ‣ 1.3 Methodology of the survey ‣ 1 Introduction ‣ Advancing 3D Point
    Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")中提出的研究问题。
    |'
- en: 'Table 1: Research questions covered in this review.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：本综述中涉及的研究问题。
- en: '| No. | Question | Objective |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| No. | Question | Objective |'
- en: '| RQ1 | What are the key principles and theoretical foundations of [DTL](#id6.6.id6)
    and [3DPC](#id1.1.id1)? | Provide a foundational understanding of the key principles
    and theories behind [DTL](#id6.6.id6) and [3DPC](#id1.1.id1). This will aid readers
    in grasifying the basic tenets upon which the field of study is built and support
    comprehension of more complex concepts discussed later in the review. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| RQ1 | [DTL](#id6.6.id6)和[3DPC](#id1.1.id1)的关键原理和理论基础是什么？ | 提供对[DTL](#id6.6.id6)和[3DPC](#id1.1.id1)背后关键原理和理论的基础理解。这将帮助读者掌握构建该研究领域的基本原则，并支持对后续讨论的更复杂概念的理解。
    |'
- en: '| RQ2 | Why the application of [DTL](#id6.6.id6) in [3DPC](#id1.1.id1) is receiving
    increasing attention? | Examine and explain why [DTL](#id6.6.id6) is increasingly
    being used for [3DPC](#id1.1.id1). This will help highlight the unique benefits
    and opportunities offered by [DTL](#id6.6.id6) in handling 3D data, indicating
    its growing importance in the field. |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| RQ2 | 为什么[DTL](#id6.6.id6)在[3DPC](#id1.1.id1)中的应用受到越来越多的关注？ | 研究并解释为什么[DTL](#id6.6.id6)在[3DPC](#id1.1.id1)中的使用越来越多。这将有助于突出[DTL](#id6.6.id6)在处理3D数据时提供的独特优势和机会，表明其在该领域的日益重要性。
    |'
- en: '| RQ3 | How the different [3DPC](#id1.1.id1) tasks can be implemented using
    [DTL](#id6.6.id6)? | Present a comprehensive overview of how [DTL](#id6.6.id6)
    can be utilized in various [3DPC](#id1.1.id1) tasks. This objective aims to showcase
    the versatility and wide-ranging applicability of [DTL](#id6.6.id6) in [3DPC](#id1.1.id1)
    tasks, providing practical insights for researchers and practitioners. |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| RQ3 | 如何利用[DTL](#id6.6.id6)实现不同的[3DPC](#id1.1.id1)任务？ | 提供关于如何在各种[3DPC](#id1.1.id1)任务中使用[DTL](#id6.6.id6)的全面概述。这个目标旨在展示[DTL](#id6.6.id6)在[3DPC](#id1.1.id1)任务中的多功能性和广泛适用性，为研究人员和实践者提供实际见解。
    |'
- en: '| RQ4 | What are the [DTL](#id6.6.id6) mechanisms and settings for performing
    the [3DPC](#id1.1.id1) tasks and optimizing their performance? | Delve into the
    specific mechanisms and settings of [DTL](#id6.6.id6) that optimize performance
    in [3DPC](#id1.1.id1) tasks. The objective here is to provide a clear understanding
    of the operational details of [DTL](#id6.6.id6), which can serve as a guide for
    those intending to implement [DTL](#id6.6.id6) in their own [3DPC](#id1.1.id1)
    tasks. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| RQ4 | [DTL](#id6.6.id6)在执行[3DPC](#id1.1.id1)任务和优化其性能时的机制和设置是什么？ | 深入探讨[DTL](#id6.6.id6)在优化[3DPC](#id1.1.id1)任务性能时的具体机制和设置。目标是提供对[DTL](#id6.6.id6)操作细节的清晰理解，这可以作为那些打算在自己的[3DPC](#id1.1.id1)任务中实施[DTL](#id6.6.id6)的指南。
    |'
- en: '| RQ5 | Which [DTL](#id6.6.id6) models are better appropriate for [3DPC](#id1.1.id1)
    tasks? | Evaluate and suggest [DTL](#id6.6.id6) models that are most suitable
    for [3DPC](#id1.1.id1) tasks. By doing this, the review will provide practical
    advice for readers looking to apply [DTL](#id6.6.id6) in [3DPC](#id1.1.id1), assisting
    them in model selection. |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| RQ5 | 哪些[DTL](#id6.6.id6)模型更适合[3DPC](#id1.1.id1)任务？ | 评估并建议最适合[3DPC](#id1.1.id1)任务的[DTL](#id6.6.id6)模型。通过这样做，评审将为希望在[3DPC](#id1.1.id1)中应用[DTL](#id6.6.id6)的读者提供实际建议，帮助他们进行模型选择。
    |'
- en: '| RQ6 | What are the issues in implementing [DTL](#id6.6.id6) for performing
    the above tasks, and how to tackle these challenges? | Identify the key challenges
    associated with implementing [DTL](#id6.6.id6) for [3DPC](#id1.1.id1) tasks and
    to propose potential solutions to these challenges. This will help improve the
    application of [DTL](#id6.6.id6) in [3DPC](#id1.1.id1) by addressing problems
    head-on, promoting a robust and reliable usage of these techniques. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| RQ6 | 实施[DTL](#id6.6.id6)以执行上述任务时存在哪些问题，如何应对这些挑战？ | 识别实施[DTL](#id6.6.id6)在[3DPC](#id1.1.id1)任务中的关键挑战，并提出应对这些挑战的潜在解决方案。这将通过直面问题来改善[DTL](#id6.6.id6)在[3DPC](#id1.1.id1)中的应用，促进这些技术的稳健和可靠使用。
    |'
- en: '| RQ7 | What are the future research directions for improving [DTL](#id6.6.id6)
    in [3DPC](#id1.1.id1)? | Forecast future research directions for [DTL](#id6.6.id6)
    in [3DPC](#id1.1.id1). This objective aims to stimulate new research initiatives
    by identifying promising avenues for further exploration, driving innovation and
    development in the field. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| RQ7 | 改进[DTL](#id6.6.id6)在[3DPC](#id1.1.id1)中的未来研究方向是什么？ | 预测[DTL](#id6.6.id6)在[3DPC](#id1.1.id1)中的未来研究方向。这个目标旨在通过识别有前途的进一步探索方向，激发新的研究举措，推动该领域的创新和发展。
    |'
- en: itemize
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 项目列表
- en: Why the application of [DTL](#id6.6.id6) in [3DPC](#id1.1.id1) is receiving
    increasing attention?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么[DTL](#id6.6.id6)在[3DPC](#id1.1.id1)中的应用受到越来越多的关注？
- en: How the different [3DPC](#id1.1.id1) tasks can be implemented using [DTL](#id6.6.id6)?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用[DTL](#id6.6.id6)实现不同的[3DPC](#id1.1.id1)任务？
- en: What are the [DTL](#id6.6.id6) mechanisms and settings for performing the [3DPC](#id1.1.id1)
    tasks and optimizing their performance?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 执行[3DPC](#id1.1.id1)任务和优化其性能的[DTL](#id6.6.id6)机制和设置是什么？
- en: Which [DTL](#id6.6.id6) models are better appropriate for [3DPC](#id1.1.id1)
    tasks?
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 哪些[DTL](#id6.6.id6)模型更适合[3DPC](#id1.1.id1)任务？
- en: What are the issues in implementing [DTL](#id6.6.id6) for performing the above
    tasks, and how to tackle these challenges?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 实施[DTL](#id6.6.id6)以执行上述任务时存在哪些问题，如何应对这些挑战？
- en: What are the future research directions for improving [DTL](#id6.6.id6) in [3DPC](#id1.1.id1)?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 改进[DTL](#id6.6.id6)在[3DPC](#id1.1.id1)中的未来研究方向是什么？
- en: '![Refer to caption](img/ef9092bb3d06ecc9812424405788867f.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ef9092bb3d06ecc9812424405788867f.png)'
- en: (a)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/fb83c035bd36fd36e665ced37564e6a6.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fb83c035bd36fd36e665ced37564e6a6.png)'
- en: (b)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 3: Summary of the approach used to search and select articles included
    in the review: (a) The adopted search procedure and (b) The selection criteria.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：总结用于搜索和选择纳入评审的文章的方法：（a）采用的搜索程序和（b）选择标准。
- en: 'The bibliometric research is performed in the context of a narrative review.
    The recent works related to the [3DPC](#id1.1.id1) involving [DTL](#id6.6.id6)
    techniques for performing their respective tasks have been searched. We searched
    the relevant keywords, like, "3D point cloud", "3D point cloud segmentation",
    "3D point cloud classification", "3D object detection", "deep transfer learning",
    and "domain adaptation", with different combinations on Scopus database in titles,
    abstracts, and keywords. The adopted search procedure is explained in Figs. [3](#S1.F3
    "Figure 3 ‣ 1.3 Methodology of the survey ‣ 1 Introduction ‣ Advancing 3D Point
    Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")(a)
    and [3](#S1.F3 "Figure 3 ‣ 1.3 Methodology of the survey ‣ 1 Introduction ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")(b).
    Typically, in response to the relevant queries, 176 articles were retrieved, with
    149 of them being non-duplicates. These articles underwent further scrutiny to
    determine their relevance to the theme of the present study. Ultimately, 108 papers
    were selected for inclusion in this study.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '文献计量研究是在叙述性综述的背景下进行的。我们检索了涉及[3DPC](#id1.1.id1)的近期工作，这些工作使用了[DTL](#id6.6.id6)技术来执行各自的任务。我们在Scopus数据库中搜索了相关的关键词，如“3D
    点云”、“3D 点云分割”、“3D 点云分类”、“3D 物体检测”、“深度迁移学习”和“领域适应”，并对标题、摘要和关键词进行了不同组合的检索。采用的搜索程序在图
    [3](#S1.F3 "Figure 3 ‣ 1.3 Methodology of the survey ‣ 1 Introduction ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")(a)
    和 [3](#S1.F3 "Figure 3 ‣ 1.3 Methodology of the survey ‣ 1 Introduction ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")(b)
    中进行了说明。通常，针对相关查询检索到176篇文章，其中149篇为非重复文章。这些文章经过进一步审查以确定它们与本研究主题的相关性。最终，选择了108篇论文纳入本研究。'
- en: 'The paper is organized as follows: Section [2](#S2 "2 Background ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")
    introduces background related to [DTL](#id6.6.id6) definitions and terms, pre-trained
    models, useful datasets, and metrics used in [3DPC](#id1.1.id1). Section [3](#S3
    "3 Overview of DTL ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey") reviews relevant literature and work related
    to [DTL](#id6.6.id6) and [DA](#id7.7.id7). Section [4](#S4 "4 Applications of
    TL-based 3DPCs ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey") outlines state-of-the-art proposed applications
    based on [DTL](#id6.6.id6). Section [5](#S5 "5 Open Challenges ‣ Advancing 3D
    Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")
    discusses open challenges. Section [6](#S6 "6 Future Research directions ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")
    suggests recent future research directions. Finally, Section [7](#S7 "7 Conclusion
    ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey") concludes the survey. Fig. [4](#S1.F4 "Figure 4 ‣ 1.3 Methodology of
    the survey ‣ 1 Introduction ‣ Advancing 3D Point Cloud Understanding through Deep
    Transfer Learning: A Comprehensive Survey") illustrates the survey’s structure,
    enhancing readability, and offering guidance to readers.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '论文的组织结构如下：第[2](#S2 "2 Background ‣ Advancing 3D Point Cloud Understanding through
    Deep Transfer Learning: A Comprehensive Survey")节介绍了与[DTL](#id6.6.id6)定义和术语、预训练模型、有用数据集和[3DPC](#id1.1.id1)中使用的指标相关的背景。第[3](#S3
    "3 Overview of DTL ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey")节回顾了与[DTL](#id6.6.id6)和[DA](#id7.7.id7)相关的文献和工作。第[4](#S4
    "4 Applications of TL-based 3DPCs ‣ Advancing 3D Point Cloud Understanding through
    Deep Transfer Learning: A Comprehensive Survey")节概述了基于[DTL](#id6.6.id6)的最新应用。第[5](#S5
    "5 Open Challenges ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey")节讨论了开放挑战。第[6](#S6 "6 Future Research directions
    ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey")节提出了近期未来研究方向。最后，第[7](#S7 "7 Conclusion ‣ Advancing 3D Point Cloud Understanding
    through Deep Transfer Learning: A Comprehensive Survey")节总结了调查内容。图 [4](#S1.F4
    "Figure 4 ‣ 1.3 Methodology of the survey ‣ 1 Introduction ‣ Advancing 3D Point
    Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey") 说明了调查的结构，增强了可读性，并为读者提供了指导。'
- en: '![Refer to caption](img/92dc1d2795d117db3802eed71b5e31bd.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/92dc1d2795d117db3802eed71b5e31bd.png)'
- en: 'Figure 4: Survey structure with sections and sub-sections disctribution.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：带有部分和子部分分布的调查结构。
- en: 2 Background
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 3D Point Cloud
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 3D 点云
- en: 'A 3DPC is a collection of data points in a three-dimensional coordinate system.
    Each point in the point cloud is represented as:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 3DPC 是三维坐标系统中数据点的集合。点云中的每个点表示为：
- en: '|  | $P=\{p_{1},p_{2},...,p_{n}\}$ |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $P=\{p_{1},p_{2},...,p_{n}\}$ |  |'
- en: 'where each point $p_{i}$ is a vector in $\mathbb{R}^{3}$:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 其中每个点 $p_{i}$ 是一个 $\mathbb{R}^{3}$ 中的向量：
- en: '|  | $p_{i}=(x_{i},y_{i},z_{i})$ |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{i}=(x_{i},y_{i},z_{i})$ |  |'
- en: 2.1.1 Operations and Transformations
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 操作和变换
- en: 'Common operations and transformations applied to 3DPCs include:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对 3DPC 应用的常见操作和变换包括：
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Translation: Moving the point cloud by adding a constant vector $\mathbf{t}=(t_{x},t_{y},t_{z})$
    to each point:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平移：通过将常量向量 $\mathbf{t}=(t_{x},t_{y},t_{z})$ 加到每个点来移动点云：
- en: '|  | $p_{i}^{\prime}=p_{i}+\mathbf{t}$ |  |'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $p_{i}^{\prime}=p_{i}+\mathbf{t}$ |  |'
- en: •
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Rotation: Rotating the point cloud using a rotation matrix $\mathbf{R}$. The
    rotation matrix is a 3x3 matrix:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 旋转：使用旋转矩阵 $\mathbf{R}$ 来旋转点云。旋转矩阵是一个 3x3 矩阵：
- en: '|  | $p_{i}^{\prime}=\mathbf{R}p_{i}$ |  |'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $p_{i}^{\prime}=\mathbf{R}p_{i}$ |  |'
- en: Rotation matrices can be derived from Euler angles, axis-angle, or quaternion
    representations.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 旋转矩阵可以从欧拉角、轴-角或四元数表示中推导出来。
- en: •
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scaling: Changing the size by scaling each point by a scalar $s$ or different
    scalars for each axis:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缩放：通过用标量 $s$ 或不同轴的标量来缩放每个点，从而改变大小：
- en: '|  | $p_{i}^{\prime}=s\cdot p_{i}\text{ or }p_{i}^{\prime}=(s_{x}x_{i},s_{y}y_{i},s_{z}z_{i})$
    |  |'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $p_{i}^{\prime}=s\cdot p_{i}\text{ 或 }p_{i}^{\prime}=(s_{x}x_{i},s_{y}y_{i},s_{z}z_{i})$
    |  |'
- en: •
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transformation: A combination of translation, rotation, and scaling, represented
    as matrix multiplication in homogeneous coordinates:'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变换：平移、旋转和缩放的组合，用齐次坐标的矩阵乘法表示：
- en: '|  | <math   alttext="p_{i}^{\prime}=\mathbf{T}\begin{bmatrix}x_{i}\\ y_{i}\\'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="p_{i}^{\prime}=\mathbf{T}\begin{bmatrix}x_{i}\\ y_{i}\\'
- en: z_{i}\\
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: z_{i}\\
- en: 1\end{bmatrix}" display="block"><semantics ><mrow ><msubsup  ><mi >p</mi><mi
    >i</mi><mo  >′</mo></msubsup><mo >=</mo><mrow ><mi  >𝐓</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo  >[</mo><mtable displaystyle="true" rowspacing="0pt"  ><mtr ><mtd ><msub  ><mi
    >x</mi><mi >i</mi></msub></mtd></mtr><mtr ><mtd ><msub  ><mi >y</mi><mi >i</mi></msub></mtd></mtr><mtr
    ><mtd ><msub  ><mi >z</mi><mi >i</mi></msub></mtd></mtr><mtr ><mtd ><mn  >1</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑝</ci><ci >𝑖</ci></apply><ci  >′</ci></apply><apply ><ci >𝐓</ci><apply  ><csymbol
    cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><ci >𝑖</ci></apply></matrixrow><matrixrow ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑦</ci><ci >𝑖</ci></apply></matrixrow><matrixrow ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑧</ci><ci >𝑖</ci></apply></matrixrow><matrixrow
    ><cn type="integer" >1</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >p_{i}^{\prime}=\mathbf{T}\begin{bmatrix}x_{i}\\
    y_{i}\\ z_{i}\\ 1\end{bmatrix}</annotation></semantics></math> |  |
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1\end{bmatrix}" display="block"><semantics ><mrow ><msubsup  ><mi >p</mi><mi
    >i</mi><mo  >′</mo></msubsup><mo >=</mo><mrow ><mi  >𝐓</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo  >[</mo><mtable displaystyle="true" rowspacing="0pt"  ><mtr ><mtd ><msub  ><mi
    >x</mi><mi >i</mi></msub></mtd></mtr><mtr ><mtd ><msub  ><mi >y</mi><mi >i</mi></msub></mtd></mtr><mtr
    ><mtd ><msub  ><mi >z</mi><mi >i</mi></msub></mtd></mtr><mtr ><mtd ><mn  >1</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑝</ci><ci >𝑖</ci></apply><ci  >′</ci></apply><apply ><ci >𝐓</ci><apply  ><csymbol
    cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><ci >𝑖</ci></apply></matrixrow><matrixrow ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑦</ci><ci >𝑖</ci></apply></matrixrow><matrixrow ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑧</ci><ci >𝑖</ci></apply></matrixrow><matrixrow
    ><cn type="integer" >1</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >p_{i}^{\prime}=\mathbf{T}\begin{bmatrix}x_{i}\\
    y_{i}\\ z_{i}\\ 1\end{bmatrix}</annotation></semantics></math> |  |
- en: 'where $\mathbf{T}$ is a 4x4 transformation matrix:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{T}$ 是一个 4x4 的变换矩阵：
- en: '|  | $\mathbf{T}=\begin{bmatrix}\mathbf{R}&amp;\mathbf{t}\\ 0&amp;1\end{bmatrix}$
    |  |'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathbf{T}=\begin{bmatrix}\mathbf{R}&amp;\mathbf{t}\\ 0&amp;1\end{bmatrix}$
    |  |'
- en: 2.2 Definitions pertaining to DTL
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 DTL 相关定义
- en: This section presents the main definitions used in [DTL](#id6.6.id6)-based [3DPC](#id1.1.id1)
    applications.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了在 [DTL](#id6.6.id6)-基础的 [3DPC](#id1.1.id1) 应用中使用的主要定义。
- en: 'Def. 1 - Domain: Consider a dataset $X$ consisting of $n$ observations, $x_{1},\cdots,x_{n}$,
    in a feature space $\chi$. The marginal probability distribution of $X$ is represented
    by $P(X)$. A domain, denoted as $\mathbb{D}$, is defined as the set containing
    $X$ and $P(X)$. In the field of [DTL](#id6.6.id6), the domain containing the initial
    knowledge is referred to as the [source omain](#id10.10.id10) ([SD](#id10.10.id10)),
    represented by $\mathbb{D}_{S}$, while the domain containing the unknown knowledge
    to be learned is called the [target domain](#id9.9.id9) ([TD](#id9.9.id9)) and
    represented by $\mathbb{D}_{T}$ .'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 1 - 领域：考虑一个包含$ n $个观察值$x_{1},\cdots,x_{n}$的特征空间$\chi$中的数据集$X$。$X$的边际概率分布表示为$P(X)$。一个领域，表示为$\mathbb{D}$，被定义为包含$X$和$P(X)$的集合。在[DTL](#id6.6.id6)领域中，包含初始知识的领域称为[源领域](#id10.10.id10)
    ([SD](#id10.10.id10))，表示为$\mathbb{D}_{S}$，而包含待学习的未知知识的领域称为[目标领域](#id9.9.id9) ([TD](#id9.9.id9))，表示为$\mathbb{D}_{T}$。
- en: 'Def. 2 - Task: The dataset $X$ contains $n$ observations, $x_{1},\cdots,x_{n}$,
    in a feature space $\chi$, and it is associated with a set of labels $Y$, $y_{1},\cdots,y_{n}$,
    in a label space $\gamma$. A task can be defined as a set containing the labels
    $Y$ and a learning objective predictive function $\mathbb{F}(X)$, represented
    by $\mathbb{T}=\{Y,\mathbb{F}(X)\}$. This function is also denoted as the conditional
    distribution $P(Y|X)$. In accordance with this definition of task, the label spaces
    of the [SD](#id10.10.id10) and [TD](#id9.9.id9) are represented as $\gamma_{S}$
    and $\gamma_{T}$ respectively [[33](#bib.bib33)].'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 2 - 任务：数据集$X$包含$ n $个观察值$x_{1},\cdots,x_{n}$，位于特征空间$\chi$中，并且与标签集$Y$、$y_{1},\cdots,y_{n}$在标签空间$\gamma$中相关联。任务可以定义为包含标签$Y$和学习目标预测函数$\mathbb{F}(X)$的集合，表示为$\mathbb{T}=\{Y,\mathbb{F}(X)\}$。这个函数也被表示为条件分布$P(Y|X)$。根据这个任务的定义，[SD](#id10.10.id10)和[TD](#id9.9.id9)的标签空间分别表示为$\gamma_{S}$和$\gamma_{T}$
    [[33](#bib.bib33)]。
- en: One way to classify [DTL](#id6.6.id6) methods is based on their approach to
    transferring knowledge, which can be broken down into what, when, and how knowledge
    is transferred.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对[DTL](#id6.6.id6)方法的一个分类方式是基于它们转移知识的方法，这可以细分为知识的转移内容、时间和方式。
- en: '(a) What knowledge is transferred: The classification of [DTL](#id6.6.id6)
    methods based on "what knowledge is transferred" examines the specific characteristics
    of knowledge that can be transferred across domains or tasks. Some information
    is unique to a particular domain or task, while other knowledge is general and
    can improve the performance of the [TD](#id9.9.id9) or task. Based on this criterion,
    [DTL](#id6.6.id6) methods can be categorized as model-based, relation-based, instance-based,
    and feature-based [[34](#bib.bib34)].'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 转移什么知识：基于“转移什么知识”的[DTL](#id6.6.id6)方法分类检查可以在不同领域或任务之间转移的知识的具体特征。一些信息是特定于某个领域或任务的，而其他知识是通用的，可以提高[TD](#id9.9.id9)或任务的表现。根据这一标准，[DTL](#id6.6.id6)方法可以被分类为基于模型的、基于关系的、基于实例的和基于特征的
    [[34](#bib.bib34)]。
- en: '(b) How knowledge is transferred: The classification of [DTL](#id6.6.id6) methods
    based on this question focuses on the specific algorithms or techniques used to
    transfer knowledge across domains or tasks.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 知识如何转移：基于这个问题对[DTL](#id6.6.id6)方法的分类专注于用于在不同领域或任务之间转移知识的具体算法或技术。
- en: '(c) When knowledge is transferred: inquires as to when and under what circumstances
    knowledge should or should not be transferred.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 知识何时转移：探讨知识在何时以及在何种情况下应该或不应该转移。
- en: 2.3 Theoretical taxonomy of DTL
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 DTL的理论分类
- en: 'In this section, a clear categorization of [DTL](#id6.6.id6) methodologies
    used for [3DPCs](#id1.1.id1) is presented. The proposed classification, shown
    in Fig. [5](#S2.F5 "Figure 5 ‣ 2.3 Theoretical taxonomy of DTL ‣ 2 Background
    ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey"), is arranged based on the following criteria: (i) learning style, (ii)
    methodology, (iii) DTL type, (iv) data annotation, and (v) widely used [DTL](#id6.6.id6)
    models. [DTL](#id6.6.id6) techniques can generally be divided into different groups
    depending on whether the source and [TDs](#id9.9.id9) and tasks are alike or not.
    The mathematical descriptions of the different [DTL](#id6.6.id6) groups and their
    distinctions are presented in Fig. [6](#S2.F6 "Figure 6 ‣ 2.3 Theoretical taxonomy
    of DTL ‣ 2 Background ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey"). While this information is discussed in the
    literature review, aiming to simplify it and present it in a more reader-friendly
    manner in this study.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '本节中，呈现了用于[3DPCs](#id1.1.id1)的[DTL](#id6.6.id6)方法的清晰分类。所提出的分类，如图 [5](#S2.F5
    "Figure 5 ‣ 2.3 Theoretical taxonomy of DTL ‣ 2 Background ‣ Advancing 3D Point
    Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")所示，是基于以下标准进行排列的：(i)
    学习风格，(ii) 方法论，(iii) DTL 类型，(iv) 数据标注，以及(v) 广泛使用的[DTL](#id6.6.id6)模型。[DTL](#id6.6.id6)技术通常可以根据源任务和[TDs](#id9.9.id9)的相似程度分为不同的组。不同[DTL](#id6.6.id6)组的数学描述及其区别在图
    [6](#S2.F6 "Figure 6 ‣ 2.3 Theoretical taxonomy of DTL ‣ 2 Background ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")中呈现。虽然这些信息在文献综述中有所讨论，本研究旨在简化这些内容，并以更易于读者理解的方式呈现。'
- en: '![Refer to caption](img/14eba21422506cf68fecefef19db8fea.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/14eba21422506cf68fecefef19db8fea.png)'
- en: 'Figure 5: Proposed taxonomy of existing [DTL](#id6.6.id6) algorithms for [3DPCs](#id1.1.id1).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 现有[DTL](#id6.6.id6)算法在[3DPCs](#id1.1.id1)中的分类方案。'
- en: '![Refer to caption](img/6465ca58ee5f36fe55a94f3f1405d72b.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6465ca58ee5f36fe55a94f3f1405d72b.png)'
- en: 'Figure 6: The [DTL](#id6.6.id6) classification is determined by how similar
    the source and [TDs](#id9.9.id9) and tasks are. The symbol ($\varsubsetneq$) is
    used to indicate that the domains/tasks are distinct yet related. The symbol ($\exists!$)
    indicates the presence of a single unique domain/task. While ($\cong$) denotes
    that the domains, tasks, or spaces do not match.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: [DTL](#id6.6.id6)的分类取决于源任务和[TDs](#id9.9.id9)的相似程度。符号 ($\varsubsetneq$)
    用于表示领域/任务是不同但相关的。符号 ($\exists!$) 表示存在一个唯一的领域/任务。符号 ($\cong$) 表示领域、任务或空间不匹配。'
- en: 2.3.1 Inductive DTL
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 归纳式 DTL
- en: 'The goal of inductive [DTL](#id6.6.id6) is to improve the target prediction
    function $\mathbb{F}T$ in the [TDs](#id9.9.id9) when compared to classical [ML](#id5.5.id5).
    This is achieved even when the target tasks $\mathbb{T}{T}$ are different from
    the source tasks $\mathbb{T}{S}$. However, the [SD](#id10.10.id10) $\mathbb{D}{S}$
    and [TD](#id9.9.id9) $\mathbb{D}_{T}$ may not always be identical (as shown in
    Fig. [6](#S2.F6 "Figure 6 ‣ 2.3 Theoretical taxonomy of DTL ‣ 2 Background ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")).
    Inductive [DTL](#id6.6.id6) can take two forms, depending on the availability
    of labeled or unlabeled data:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '归纳式[DTL](#id6.6.id6)的目标是改善[TDs](#id9.9.id9)中目标预测函数 $\mathbb{F}T$，与经典[ML](#id5.5.id5)相比。这是即使目标任务
    $\mathbb{T}{T}$ 与源任务 $\mathbb{T}{S}$ 不同也能实现的。然而，[SD](#id10.10.id10) $\mathbb{D}{S}$
    和 [TD](#id9.9.id9) $\mathbb{D}_{T}$ 可能并不总是相同的（如图 [6](#S2.F6 "Figure 6 ‣ 2.3 Theoretical
    taxonomy of DTL ‣ 2 Background ‣ Advancing 3D Point Cloud Understanding through
    Deep Transfer Learning: A Comprehensive Survey")所示）。归纳式[DTL](#id6.6.id6)可以有两种形式，具体取决于标注或未标注数据的可用性：'
- en: '(a) Multi-task DTL: This approach is used when the [SDs](#id10.10.id10) has
    a large labeled dataset ($X_{S}$ labeled with $Y_{S}$). This is a specific form
    of multi-task learning where multiple tasks $(T_{1},T_{2},\dots,T_{n})$ are learned
    simultaneously (in parallel), including both the source and target tasks[[35](#bib.bib35)].'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '(a) 多任务 DTL: 当[SDs](#id10.10.id10)具有大量标注数据集（$X_{S}$与$Y_{S}$标注）时使用此方法。这是一种多任务学习的特定形式，其中多个任务
    $(T_{1},T_{2},\dots,T_{n})$ 同时（并行）进行学习，包括源任务和目标任务[[35](#bib.bib35)]。'
- en: '(b) Sequential learning: also known as self-taught learning, is a method used
    when the dataset in the [SD](#id10.10.id10) is unlabeled. It relies on (i) transferring
    the feature representation learned from a large collection of unlabeled datasets,
    and (ii) applying the learned representation to labeled data for classification
    tasks. This [DTL](#id6.6.id6) method involves sequentially learning multiple tasks
    where the gaps between the [SDs](#id10.10.id10) and [TDs](#id9.9.id9) may differ.
    For instance, assume we have a pre-trained model (PTM) $M$ and we apply [DTL](#id6.6.id6)
    to several tasks $(T_{1},T_{2},\dots,T_{n})$. In this approach, a specific task
    $\mathbb{T}_{T}$ is learned at each time step $t$ and it is slower than multi-task
    learning, however, when not all the tasks are present at the time of training,
    it may be advantageous. Sequential learning can be further classified into different
    types [[36](#bib.bib36)].'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 序列学习：也称为自学，是在 [SD](#id10.10.id10) 数据集未标记时使用的方法。它依赖于 (i) 从大量未标记数据集中转移学到的特征表示，以及
    (ii) 将学到的表示应用于标记数据进行分类任务。这种 [DTL](#id6.6.id6) 方法涉及依次学习多个任务，其中 [SDs](#id10.10.id10)
    和 [TDs](#id9.9.id9) 之间的差距可能不同。例如，假设我们有一个预训练模型 (PTM) $M$，并将 [DTL](#id6.6.id6) 应用于几个任务
    $(T_{1},T_{2},\dots,T_{n})$。在这种方法中，每次时间步 $t$ 学习一个特定任务 $\mathbb{T}_{T}$，它比多任务学习慢，但当训练时不是所有任务都存在时，它可能具有优势。序列学习还可以进一步分类为不同类型
    [[36](#bib.bib36)]。
- en: 1-
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1-
- en: 'Fine-tuning: involves training a new function, $\mathbb{F}_{T}$, that adapts
    the parameters of a PTM $M$ from a source task, $\mathbb{T}_{S}$, to a target
    task, $\mathbb{T}_{T}$, by translating the weights of the source task, $W_{S}$,
    to the weights of the target task, $W_{T}$. This can be done across all layers
    or just a subset of them, and the learning rate for each layer can be adjusted
    independently (known as discriminative fine-tuning). Additionally, new parameters,
    $K$, can be added to the model to improve its performance on the target task [[37](#bib.bib37)].'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调：涉及训练一个新的函数 $\mathbb{F}_{T}$，将 PTM $M$ 的参数从源任务 $\mathbb{T}_{S}$ 适配到目标任务 $\mathbb{T}_{T}$，通过将源任务的权重
    $W_{S}$ 转换为目标任务的权重 $W_{T}$。这可以在所有层中完成，也可以只在其中的一个子集上进行，每层的学习率可以独立调整（称为判别微调）。此外，可以向模型中添加新的参数
    $K$ 以提高其在目标任务上的性能 [[37](#bib.bib37)]。
- en: '|  | $\mathbb{F}_{T}(W_{T},K)=W_{S}\times K$ |  | (1) |'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathbb{F}_{T}(W_{T},K)=W_{S}\times K$ |  | (1) |'
- en: 2-
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2-
- en: 'Adapter modules: they are designed to take a PTM, $M_{S}$, and adapt its weights,
    $W_{S}$, to a target task, $\mathbb{T}_{T}$. The adapter module achieves this
    by introducing a new set of parameters, $K$, that are smaller in size compared
    to $W_{S}$, i.e. $K\ll W_{S}$. Both $K$ and $W_{S}$ are decomposed into smaller,
    more compact modules, such that $W_{S}={w}_{n}$ and $K={k}_{n}$. This allows the
    adapter module to learn a new function, $\mathbb{F}_{T}$, which adapts the model
    to the target task.'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 适配器模块：它们旨在将 PTM（预训练模型）$M_{S}$ 的权重 $W_{S}$ 适配到目标任务 $\mathbb{T}_{T}$。适配器模块通过引入一组新的参数
    $K$ 来实现这一点，这些参数的大小比 $W_{S}$ 小，即 $K\ll W_{S}$。$K$ 和 $W_{S}$ 都被分解成更小、更紧凑的模块，使得 $W_{S}={w}_{n}$
    和 $K={k}_{n}$。这使得适配器模块可以学习一个新的函数 $\mathbb{F}_{T}$，该函数将模型适配到目标任务。
- en: '|  | $\mathbb{F}_{T}(K,W_{S})=k_{1}^{\prime}\times w_{1}\times\cdots k_{n}^{\prime}\times
    w_{n}$ |  | (2) |'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathbb{F}_{T}(K,W_{S})=k_{1}^{\prime}\times w_{1}\times\cdots k_{n}^{\prime}\times
    w_{n}$ |  | (2) |'
- en: 'The equation ([2](#S2.E2 "In item 2- ‣ 2.3.1 Inductive DTL ‣ 2.3 Theoretical
    taxonomy of DTL ‣ 2 Background ‣ Advancing 3D Point Cloud Understanding through
    Deep Transfer Learning: A Comprehensive Survey")) illustrates the procedure of
    adapting a model to a new task by dynamic weight adjustment, original weights
    $W_{S}={w}_{n}$ remain unchanged, whereas the set of weights $K$ are updated to
    $K^{\prime}={k^{\prime}}_{n}$. This principle of Dynamic DA is illustrated in
    Fig. [7](#S2.F7 "Figure 7 ‣ 2.3.1 Inductive DTL ‣ 2.3 Theoretical taxonomy of
    DTL ‣ 2 Background ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey").'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方程 ([2](#S2.E2 "在第 2- ‣ 2.3.1 诱导 DTL ‣ 2.3 DTL 的理论分类 ‣ 2 背景 ‣ 通过深度迁移学习推进 3D
    点云理解：全面调查")) 说明了通过动态权重调整将模型适配到新任务的过程，原始权重 $W_{S}={w}_{n}$ 保持不变，而权重集 $K$ 被更新为 $K^{\prime}={k^{\prime}}_{n}$。动态
    DA 原则在图 [7](#S2.F7 "图 7 ‣ 2.3.1 诱导 DTL ‣ 2.3 DTL 的理论分类 ‣ 2 背景 ‣ 通过深度迁移学习推进 3D
    点云理解：全面调查") 中有所示。
- en: 3-
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3-
- en: 'Feature based: it focuses on learning concepts and representations at various
    levels of an image, such as corners or interest points, blobs or regions of interest
    points, ridges, or edges $E$. In this approach, the collection of $E$ obtained
    from a PTM $M$ is kept unchanged and only $W^{\prime}$ is fine-tuned, such that
    the function $\mathbb{F}_{T}$ can be represented as $E\times W^{\prime}$. The
    idea is that $E$ are the feature learned from PTM and fine-tuning only the last
    layer $W^{\prime}$ to adapt to the new task.'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于特征：它侧重于在图像的不同层次上学习概念和表示，如角点或兴趣点、斑点或感兴趣区域、脊线或边缘$E$。在这种方法中，从PTM $M$获得的$E$集合保持不变，只微调$W^{\prime}$，使得函数$\mathbb{F}_{T}$可以表示为$E\times
    W^{\prime}$。其理念是$E$是从PTM学习到的特征，仅微调最后一层$W^{\prime}$以适应新任务。
- en: 4-
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4-
- en: 'Zero-shot: is the simplest approach among all the others. It does not involve
    modifying or adding new parameters to a PTM by assuming that the existing parameters,
    denoted as $W_{S}$, cannot be changed. Essentially, zero-shot does not require
    any training to optimize or learn new parameters."'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零样本：是在所有其他方法中最简单的方法。它不涉及修改或添加新参数到PTM中，而是假设现有参数（记作$W_{S}$）不能更改。本质上，零样本不需要任何训练来优化或学习新参数。
- en: '![Refer to caption](img/839f97b25fb82bfd171671e144377ca8.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/839f97b25fb82bfd171671e144377ca8.png)'
- en: 'Figure 7: Example of DTL models used in [3DPCs](#id1.1.id1): (a) fine-tuning,
    and (b) deep DA.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：在[3D点云](#id1.1.id1)中使用的DTL模型示例：（a）微调，以及（b）深度领域自适应。
- en: 2.3.2 Transductive DTL
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 传导性深度迁移学习
- en: 'In comparison to traditional [ML](#id5.5.id5), which can be used as a benchmark
    for DA and [DTL](#id6.6.id6), [DTL](#id6.6.id6) addresses the scenario where the
    [TD](#id9.9.id9) data, denoted as $\mathbb{D}{T}$, differs from the [SDs](#id10.10.id10)
    data, denoted as $\mathbb{D}{S}$. While the [SDs](#id10.10.id10) has annotated
    data ($X_{S}$ paired with $Y_{S}$), the [TD](#id9.9.id9) has no labeled data.
    The source and target tasks are similar as outlined in Fig. [6](#S2.F6 "Figure
    6 ‣ 2.3 Theoretical taxonomy of DTL ‣ 2 Background ‣ Advancing 3D Point Cloud
    Understanding through Deep Transfer Learning: A Comprehensive Survey"). Transductive
    [DTL](#id6.6.id6) aims at constructing a target prediction function $\mathbb{F}_{T}$
    using the knowledge from both the [SD](#id10.10.id10) and [TD](#id9.9.id9). Additionally,
    transductive [DTL](#id6.6.id6) can be further categorized into two groups based
    on the relationship between the [SD](#id10.10.id10) and [TD](#id9.9.id9), as described
    in [[38](#bib.bib38)].'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的[机器学习](#id5.5.id5)相比，后者可以作为DA和[深度迁移学习](#id6.6.id6)的基准，[深度迁移学习](#id6.6.id6)解决了[目标数据](#id9.9.id9)数据（记作$\mathbb{D}{T}$）与[源数据](#id10.10.id10)数据（记作$\mathbb{D}{S}$）不同的情况。虽然[源数据](#id10.10.id10)有标注数据（$X_{S}$与$Y_{S}$配对），但[目标数据](#id9.9.id9)没有标注数据。源任务和目标任务相似，如图[6](#S2.F6
    "图 6 ‣ 2.3 DTL的理论分类 ‣ 2 背景 ‣ 通过深度迁移学习推进3D点云理解：一项全面调查")所示。传导性[深度迁移学习](#id6.6.id6)旨在利用[源数据](#id10.10.id10)和[目标数据](#id9.9.id9)的知识构建目标预测函数$\mathbb{F}_{T}$。此外，传导性[深度迁移学习](#id6.6.id6)还可以根据[源数据](#id10.10.id10)和[目标数据](#id9.9.id9)之间的关系进一步分为两类，如[[38](#bib.bib38)]中所述。
- en: '(a) Deep domain adaptation (DDA): refers to the situation where the feature
    spaces of the [SD](#id10.10.id10), denoted as $\chi_{S}$, and the [TDs](#id9.9.id9),
    denoted as $\chi_{T}$, are the same. However, the probability distributions of
    the input data are different, with $P(Y_{S}/X_{S})\neq P(Y_{T}/X_{T})$ as described
    in [[39](#bib.bib39)]. DDA is particularly useful when the target task has a unique
    distribution or limited labeled data, as highlighted in [[40](#bib.bib40)].'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 深度领域自适应（DDA）：指的是[源数据](#id10.10.id10)的特征空间（记作$\chi_{S}$）和[目标数据](#id9.9.id9)的特征空间（记作$\chi_{T}$）相同的情况。然而，输入数据的概率分布是不同的，$P(Y_{S}/X_{S})\neq
    P(Y_{T}/X_{T})$，如[[39](#bib.bib39)]所述。当目标任务具有独特的分布或标注数据有限时，DDA特别有用，如[[40](#bib.bib40)]中所强调的。
- en: '(b) Cross-modality DTL: most [DTL](#id6.6.id6) techniques require some form
    of relationship between feature spaces (or label spaces) of the source and [TDs](#id9.9.id9),
    i.e., $\mathbb{D}{S}$ and $\mathbb{D}{T}$. This means that [DTL](#id6.6.id6) can
    only be applied when the source and target have the same modality, such as text,
    speech, or video. In contrast, cross-modality [DTL](#id6.6.id6) is one of the
    most challenging areas of [DTL](#id6.6.id6), as it assumes that the feature spaces
    of the source and [TDs](#id9.9.id9) are completely different ($\chi_{S}\neq\chi_{T}$),
    such as speech-to-image, image-to-text, and text-to-speech. Additionally, the
    label spaces of the source $Y_{S}$ and target $Y_{S}$ domains may also differ
    ($Y_{S}\neq Y_{T}$) as described in [[41](#bib.bib41)].'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 跨模态 DTL：大多数 [DTL](#id6.6.id6) 技术要求源和 [TDs](#id9.9.id9) 的特征空间（或标签空间）之间存在某种关系，即
    $\mathbb{D}{S}$ 和 $\mathbb{D}{T}$。这意味着 [DTL](#id6.6.id6) 只能在源和目标具有相同模态时应用，如文本、语音或视频。相比之下，跨模态
    [DTL](#id6.6.id6) 是 [DTL](#id6.6.id6) 中最具挑战性的领域之一，因为它假设源和 [TDs](#id9.9.id9) 的特征空间完全不同（$\chi_{S}\neq\chi_{T}$），例如语音到图像、图像到文本和文本到语音。此外，源的标签空间
    $Y_{S}$ 和目标的标签空间 $Y_{S}$ 也可能不同（$Y_{S}\neq Y_{T}$），如 [[41](#bib.bib41)] 中所述。
- en: '(c) Unsupervised DTL: is a method of improving the learning of the target prediction
    function $\mathbb{F}T$ in the [TD](#id9.9.id9) $\mathbb{D}{T}$ by using knowledge
    from the [SDs](#id10.10.id10) $\mathbb{D}{S}$ and the source task $\mathbb{T}{S}$,
    even when the labels $Y_{S}$ and $Y_{T}$ are not present. It is important to note
    that the source and target tasks, $\mathbb{T}{S}$ and $\mathbb{T}{T}$, are related
    but distinct. Such kind of approach is useful when labels are not available for
    [TD](#id9.9.id9) data, as stated in [[42](#bib.bib42)].'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 无监督 DTL：是一种通过使用来自 [SDs](#id10.10.id10) $\mathbb{D}{S}$ 和源任务 $\mathbb{T}{S}$
    的知识来提高 [TD](#id9.9.id9) $\mathbb{D}{T}$ 中目标预测函数 $\mathbb{F}T$ 的学习的方法，即使在没有标签 $Y_{S}$
    和 $Y_{T}$ 的情况下。值得注意的是，源任务和目标任务 $\mathbb{T}{S}$ 和 $\mathbb{T}{T}$ 相关但不同。当 [TD](#id9.9.id9)
    数据没有标签时，这种方法是有用的，如 [[42](#bib.bib42)] 中所述。
- en: 2.3.3 Adversarial DTL
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 对抗性 DTL
- en: Adversarial learning, as introduced in [[43](#bib.bib43)], is a method that
    helps to learn more transferable and discriminative representations. The first
    method using this approach, the domain-adversarial neural network (DANN) was presented
    in [[44](#bib.bib44)]. Unlike traditional methods that use predefined distance
    functions, DANN utilizes a domain-adversarial loss within the network. This approach
    has shown to improve the network’s ability to learn discriminative data and has
    been used in many visual surveillance studies [[45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48)]. However, prior works in DANN have not considered
    the different effects of marginal and conditional distributions. An alternative
    approach, dynamic distribution alignment, was proposed in [[49](#bib.bib49)] that
    dynamically evaluates the importance of each distribution, thus providing a more
    nuanced approach." Another approach called the adversarial scoring network (ASNet)
    was introduced in [[50](#bib.bib50)] to bridge the gap between domains at different
    levels of granularity. This approach uses adversarial learning to align the [SDs](#id10.10.id10)
    with the [TD](#id9.9.id9) in the global and local feature space during the coarse-grained
    stage. The transferability of source attributes is then evaluated in fine-grained
    stage by comparing the similarity between the source and target samples at multiple
    levels, utilizing the generative probability obtained in the coarse stage. The
    transferable elements are then selectively used to assist the [DTL](#id6.6.id6)
    adaptation process. This coarse-to-fine architecture effectively reduces the problem
    of domain disparity. Specifically, photographs are encoded into density maps by
    a generator, and then classified as [SDs](#id10.10.id10) or [TD](#id9.9.id9) using
    a dual-discriminator. Adversarial training is employed between the dual-discriminator
    and generator to bring the domains’ distributions closer together. The dual-discriminator
    also generates four different scores which are used as a signal to optimize the
    density of the [SDs](#id10.10.id10) during adaptation for fine-grained transfer
    as stated in [[50](#bib.bib50)].
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗学习，如在[[43](#bib.bib43)]中介绍，是一种帮助学习更具可迁移性和区分性表示的方法。首个使用这种方法的模型，领域对抗神经网络（DANN），在[[44](#bib.bib44)]中提出。与使用预定义距离函数的传统方法不同，DANN在网络内部利用领域对抗损失。这种方法已显示出提高网络学习区分数据的能力，并已被用于许多视觉监控研究[[45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48)]。然而，之前的DANN工作没有考虑边际分布和条件分布的不同影响。另一种方法，动态分布对齐，提出于[[49](#bib.bib49)]，动态评估每个分布的重要性，从而提供更细致的方法。另一种方法，称为对抗评分网络（ASNet），在[[50](#bib.bib50)]中引入，以弥合不同层次的领域之间的差距。这种方法使用对抗学习在粗粒度阶段对齐[SDs](#id10.10.id10)与[TD](#id9.9.id9)在全局和局部特征空间中的位置。然后在细粒度阶段通过比较源样本和目标样本在多个层次上的相似性来评估源属性的可迁移性，利用在粗粒度阶段获得的生成概率。然后选择性地使用可迁移元素来协助[DTL](#id6.6.id6)适应过程。这种从粗到细的架构有效地减少了领域差异问题。具体而言，照片通过生成器编码为密度图，然后使用双判别器分类为[SDs](#id10.10.id10)或[TD](#id9.9.id9)。在双判别器和生成器之间进行对抗训练，以使领域分布更接近。双判别器还生成四种不同的分数，用作优化[SDs](#id10.10.id10)密度的信号，以便在细粒度传输期间适应，如[[50](#bib.bib50)]中所述。
- en: 'In [[51](#bib.bib51)], the authors studied the adversarial robustness in [3DPC](#id1.1.id1)
    recognition using three different architectures: [multi-layer perceptron](#id23.23.id23)
    ([MLP](#id23.23.id23)) network (PointNet), convolutional network (DGCNN), and
    transformer-based network. They employed two methods: adversarial pretraining
    for fine-tuning, where [self-supervised learning](#id14.14.id14) ([SSL](#id14.14.id14))
    tasks are utilized for pretraining, and adversarial joint training, where the
    self-supervised task is trained alongside the recognition task, as illustrated
    in Fig. [8](#S2.F8 "Figure 8 ‣ 2.3.3 Adversarial DTL ‣ 2.3 Theoretical taxonomy
    of DTL ‣ 2 Background ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey").'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[51](#bib.bib51)]中，作者研究了在[3DPC](#id1.1.id1)识别中的对抗鲁棒性，使用了三种不同的架构：[多层感知机](#id23.23.id23)
    ([MLP](#id23.23.id23)) 网络（PointNet）、卷积网络（DGCNN）和基于变换器的网络。他们采用了两种方法：对抗预训练用于微调，其中利用[自监督学习](#id14.14.id14)
    ([SSL](#id14.14.id14)) 任务进行预训练；对抗联合训练，其中自监督任务与识别任务一起训练，如图[8](#S2.F8 "Figure 8
    ‣ 2.3.3 Adversarial DTL ‣ 2.3 Theoretical taxonomy of DTL ‣ 2 Background ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")所示。'
- en: '![Refer to caption](img/a7ca1fd95225088bb486d91859547ee3.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a7ca1fd95225088bb486d91859547ee3.png)'
- en: 'Figure 8: Flowchart of the adversarial TL-based [3DPC](#id1.1.id1) classification
    approach proposed in [[51](#bib.bib51)].'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：在[[51](#bib.bib51)]中提出的对抗性迁移学习（TL）基于[3DPC](#id1.1.id1)分类方法的流程图。
- en: 2.4 Popular DL-based PC models
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 流行的基于深度学习的点云模型
- en: The development of DL-based 3DPC approaches is characterized by a rich diversity
    of deep learning models each aimed at overcoming specific challenges related to
    point cloud data processing. While these models offer significant improvements
    in processing speed, accuracy, and applicability, they also underscore the ongoing
    need for models that can generalize across different environments and handle the
    inherent complexities of 3D point data more effectively.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的3DPC方法的发展特点是深度学习模型的丰富多样，每种模型旨在克服与点云数据处理相关的特定挑战。尽管这些模型在处理速度、准确性和适用性方面提供了显著改进，但它们也突显了需要能够跨不同环境进行泛化并更有效处理3D点数据固有复杂性的模型。
- en: 2.4.1 Feature Extraction and Geometric Detail Enhancement Models
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 特征提取与几何细节增强模型
- en: Models such as PointNet [[52](#bib.bib52)] and PointNet++ [[53](#bib.bib53)]
    spearheaded the direct processing of point clouds by respecting permutation invariance
    and enhancing the capture of hierarchical structures, respectively. They are pivotal
    in tasks like object classification and segmentation, though they struggle with
    local detail due to uniform point processing. PointVGG [[54](#bib.bib54)] and
    PointPAVGG [[55](#bib.bib55)] extend these capabilities by adapting image-based
    convolutional and attention mechanisms to point clouds, striving to bridge gaps
    in capturing intricate geometric details but facing challenges in computational
    efficiency. SpiderCNN [[56](#bib.bib56)] and PointCNN [[57](#bib.bib57)] further
    innovate by adapting traditional CNN transformations to unordered point data,
    enhancing the models’ ability to learn from complex datasets.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 模型如 PointNet [[52](#bib.bib52)] 和 PointNet++ [[53](#bib.bib53)] 引领了直接处理点云的方法，分别通过尊重置换不变性和增强对层次结构的捕捉。它们在目标分类和分割任务中发挥了关键作用，但由于统一点处理而在局部细节方面存在困难。PointVGG
    [[54](#bib.bib54)] 和 PointPAVGG [[55](#bib.bib55)] 通过将基于图像的卷积和注意力机制适配到点云中，扩展了这些能力，力图弥补捕捉复杂几何细节的差距，但在计算效率方面面临挑战。SpiderCNN
    [[56](#bib.bib56)] 和 PointCNN [[57](#bib.bib57)] 通过将传统的卷积神经网络（CNN）变换适配到无序点数据中，进一步创新，提高了模型从复杂数据集中学习的能力。
- en: 2.4.2 Segmentation and Object Detection Models
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2 分割与目标检测模型
- en: Models like SplatNet [[58](#bib.bib58)], SGPN [[59](#bib.bib59)], and FoldingNet
    [[60](#bib.bib60)] represent advances in segmentation and unsupervised learning.
    SplatNet leverages sparse bilateral convolutional layers for processing high-dimensional
    lattices in point clouds, suitable for segmentation but hampered by scaling issues.
    SGPN focuses on instance segmentation by predicting semantic classes and point
    groupings, which becomes complex in cluttered environments. FoldingNet explores
    unsupervised learning with a folding-based decoder that may not generalize across
    all point cloud types.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 模型如 SplatNet [[58](#bib.bib58)]、SGPN [[59](#bib.bib59)] 和 FoldingNet [[60](#bib.bib60)]
    代表了在分割和无监督学习方面的进展。SplatNet 利用稀疏双边卷积层处理点云中的高维格点，适合于分割，但受限于缩放问题。SGPN 通过预测语义类别和点分组来专注于实例分割，这在混乱环境中变得复杂。FoldingNet
    探索了使用基于折叠的解码器的无监督学习，这可能无法跨所有点云类型进行泛化。
- en: 2.4.3 Data Integration and Upsampling Models
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3 数据集成与上采样模型
- en: PU-Net [[61](#bib.bib61)] and PointGrid [[62](#bib.bib62)] focus on upsampling
    and integrating grid-based approaches with point processing. PU-Net enhances point
    cloud density using multi-level feature integration, while PointGrid blends point
    and grid-based methods for recognizing 3D models, though it struggles with sparse
    point clouds.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: PU-Net [[61](#bib.bib61)] 和 PointGrid [[62](#bib.bib62)] 侧重于上采样以及将基于网格的方法与点处理相结合。PU-Net
    通过多层次特征融合来增强点云密度，而 PointGrid 则将点云和网格方法相结合以识别3D模型，但在处理稀疏点云时存在困难。
- en: 2.4.4 Specialized Application Models
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.4 专用应用模型
- en: Models such as Hand PointNet [[63](#bib.bib63)] and PointNetVLAD [[64](#bib.bib64)]
    are tailored for specific applications like 3D hand pose estimation and global
    descriptor extraction, critical for place recognition. However, these models often
    face limitations when applied outside their intended scope, such as dynamic environments
    or varying object types.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Hand PointNet [[63](#bib.bib63)] 和 PointNetVLAD [[64](#bib.bib64)] 这样的模型专为特定应用如
    3D 手部姿态估计和全局描述符提取而设计，这些在场景识别中至关重要。然而，这些模型在应用于其预定范围之外时常常面临限制，例如动态环境或不同类型的对象。
- en: 2.4.5 Innovative Approaches in Object Detection and Registration
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.5 对象检测与配准中的创新方法
- en: Emerging models like PREDATOR [[65](#bib.bib65)] and 3DIoUMatch [[66](#bib.bib66)]
    showcase the potential of deep learning in object detection and registration.
    PREDATOR addresses low-overlap point cloud registration with a deep attention
    mechanism, suited for benchmark scenarios but limited in broader applications.
    3DIoUMatch leverages a semi-supervised learning approach to enhance 3D object
    detection, showing dependency on initial label quality for performance efficacy.
    Table LABEL:tab2 summarizes the most popular 3DPC models proposed in the literature.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 新兴模型如 PREDATOR [[65](#bib.bib65)] 和 3DIoUMatch [[66](#bib.bib66)] 展示了深度学习在对象检测和配准中的潜力。PREDATOR
    通过深度注意机制解决了低重叠点云配准的问题，适用于基准场景，但在更广泛的应用中有限。3DIoUMatch 利用半监督学习方法增强了 3D 对象检测，显示出性能效果依赖于初始标签质量。表
    LABEL:tab2 总结了文献中提出的最受欢迎的 3DPC 模型。
- en: 'Table 2: Summary of popular DL-based [3DPC](#id1.1.id1) understanding models.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基于深度学习的 [3DPC](#id1.1.id1) 理解模型总结。
- en: '| Ref. | Model Name | Contribution Description | Dataset | Application | Limitation
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 模型名称 | 贡献描述 | 数据集 | 应用 | 限制 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| [[52](#bib.bib52)] | PointNet | Introduced a neural network that processes
    point clouds directly, respecting permutation invariance. | Various 3D benchmarks
    | Object classification, part segmentation, semantic parsing | Does not capture
    local structural details. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| [[52](#bib.bib52)] | PointNet | 引入了一种直接处理点云的神经网络，尊重排列不变性。 | 各种 3D 基准测试 |
    对象分类、部件分割、语义解析 | 未能捕捉局部结构细节。 |'
- en: '| [[67](#bib.bib67)] | ScanNet | Developed an RGB-D video dataset with extensive
    annotations for deep learning applications. | ScanNet dataset | 3D object classification,
    semantic voxel labeling, CAD model retrieval | Limited diversity in scene views
    and semantic annotations. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| [[67](#bib.bib67)] | ScanNet | 开发了一个 RGB-D 视频数据集，并提供了广泛的注释，用于深度学习应用。 | ScanNet
    数据集 | 3D 对象分类、语义体素标注、CAD 模型检索 | 场景视图和语义注释的多样性有限。 |'
- en: '| [[68](#bib.bib68)] | OctNet | Proposed a sparse 3D data representation that
    allows for deep, high-resolution 3D convolutional networks. | Not specified |
    3D object classification, orientation estimation, point cloud labeling | Focus
    on sparse data might not generalize to denser datasets. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| [[68](#bib.bib68)] | OctNet | 提出了一个稀疏 3D 数据表示，允许深度、高分辨率的 3D 卷积网络。 | 未指定 |
    3D 对象分类、方向估计、点云标注 | 对稀疏数据的关注可能不适用于更密集的数据集。 |'
- en: '| [[53](#bib.bib53)] | PointNet++ | Extended PointNet to capture local structures
    using a hierarchical neural network. | Challenging benchmarks of 3DPCs | Enhanced
    3D recognition tasks | Performance drops with non-uniform point densities. |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| [[53](#bib.bib53)] | PointNet++ | 扩展了 PointNet，以分层神经网络捕捉局部结构。 | 挑战性的 3DPC
    基准 | 增强的 3D 识别任务 | 对非均匀点密度的性能下降。 |'
- en: '| [[54](#bib.bib54)] | PointVGG | Introduced point convolution and pooling
    methods to adapt image-based techniques for point clouds. | Challenging benchmarks
    of 3DPCs | Object classification, part segmentation | May not fully capture intricate
    geometric details. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| [[54](#bib.bib54)] | PointVGG | 引入了点卷积和池化方法，将基于图像的技术适配到点云中。 | 挑战性的 3DPC 基准
    | 对象分类、部件分割 | 可能无法完全捕捉复杂的几何细节。 |'
- en: '| [[55](#bib.bib55)] | PointPAVGG | A VGG-based network that incorporates a
    point attention mechanism for feature extraction from point clouds. | ShapeNet,
    ModelNet | Point cloud classification, segmentation | Increased computational
    demand due to complex feature integration. |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| [[55](#bib.bib55)] | PointPAVGG | 一种基于 VGG 的网络，结合点注意机制从点云中提取特征。 | ShapeNet,
    ModelNet | 点云分类、分割 | 由于复杂特征集成而增加的计算需求。 |'
- en: '| [[58](#bib.bib58)] | SplatNet | Utilized sparse bilateral convolutional layers
    for processing point clouds in a high-dimensional lattice. | Not specified | 3D
    segmentation | Scaling issues with memory and computational cost in larger lattices.
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| [[58](#bib.bib58)] | SplatNet | 利用稀疏双边卷积层在高维格子中处理点云。 | 未指定 | 3D分割 | 在更大的格子中存在内存和计算成本的扩展问题。
    |'
- en: '| [[60](#bib.bib60)] | FoldingNet | Developed a deep auto-encoder with a folding-based
    decoder for unsupervised learning on point clouds. | Not specified | Unsupervised
    learning, 3D object reconstruction | Generic decoder structure may not work for
    all point cloud types. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| [[60](#bib.bib60)] | FoldingNet | 开发了一个深度自编码器，采用基于折叠的解码器，用于点云的无监督学习。 | 未指定
    | 无监督学习、3D物体重建 | 通用解码器结构可能不适用于所有点云类型。 |'
- en: '| [[61](#bib.bib61)] | PU-Net | Presented a method for upsampling 3DPCs using
    multi-level features. | Synthesis and scan data | Point cloud upsampling | Focus
    on upsampling may not improve other manipulations. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| [[61](#bib.bib61)] | PU-Net | 提出了一个利用多层次特征对3D点云进行上采样的方法。 | 合成和扫描数据 | 点云上采样
    | 关注于上采样可能无法改善其他操作。 |'
- en: '| [[69](#bib.bib69)] | SO-Net | Built a permutation invariant architecture
    using Self-Organizing Maps for point cloud processing. | Not specified | Point
    cloud reconstruction, classification, segmentation, shape retrieval | Requires
    tuning of the network’s receptive field. |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| [[69](#bib.bib69)] | SO-Net | 使用自组织映射构建了一个具有置换不变性的架构，用于点云处理。 | 未指定 | 点云重建、分类、分割、形状检索
    | 需要调整网络的感受野。 |'
- en: '| [[70](#bib.bib70)] | PIXOR | Developed a real-time 3D object detection system
    from point clouds using BEV, optimized for autonomous driving. | KITTI, large-scale
    3D vehicle detection benchmark | Real-time 3D object detection in autonomous driving
    | Primarily optimized for vehicle detection, may not generalize to other object
    types. |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| [[70](#bib.bib70)] | PIXOR | 开发了一个基于BEV的实时3D物体检测系统，用于点云数据，优化了自动驾驶应用。 | KITTI，大规模3D车辆检测基准
    | 自动驾驶中的实时3D物体检测 | 主要针对车辆检测进行了优化，可能不适用于其他物体类型。 |'
- en: '| [[59](#bib.bib59)] | SGPN | Introduced a network for 3D instance segmentation
    by predicting point groupings and semantic classes. | Various 3D scenes | 3D instance
    segmentation, object detection, semantic segmentation | May struggle with highly
    cluttered or complex environments. |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| [[59](#bib.bib59)] | SGPN | 引入了一个通过预测点分组和语义类别进行3D实例分割的网络。 | 各种3D场景 | 3D实例分割、物体检测、语义分割
    | 可能在高度杂乱或复杂的环境中表现不佳。 |'
- en: '| [[71](#bib.bib71)] | VoxelNet | Unified feature extraction and bounding box
    prediction for 3DPCs into a single deep network. | KITTI | 3D detection of cars,
    pedestrians, and cyclists | High computational cost due to dense voxelization
    of point clouds. |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| [[71](#bib.bib71)] | VoxelNet | 将3D点云的统一特征提取和边界框预测整合到一个深度网络中。 | KITTI | 车辆、行人和自行车的3D检测
    | 由于点云的密集体素化导致计算成本高。 |'
- en: '| [[63](#bib.bib63)] | Hand PointNet | Developed a hand pose regression network
    using 3DPCs to capture complex hand structures. | Three challenging hand pose
    datasets | 3D hand pose estimation | Focuses on hand pose, limiting its applicability
    to other forms of point cloud processing. |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| [[63](#bib.bib63)] | Hand PointNet | 开发了一个手部姿势回归网络，利用3D点云捕捉复杂的手部结构。 | 三个具有挑战性的手部姿势数据集
    | 3D手部姿势估计 | 专注于手部姿势，限制了其在其他点云处理形式中的应用。 |'
- en: '| [[64](#bib.bib64)] | PointNetVLAD | Proposed a network for global descriptor
    extraction from point clouds for place recognition. | Created benchmark datasets
    for point cloud based retrieval | Place recognition from point clouds | May not
    be as effective in highly dynamic environments. |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| [[64](#bib.bib64)] | PointNetVLAD | 提出了一个用于从点云中提取全局描述符的网络，用于地点识别。 | 创建了基于点云检索的基准数据集
    | 从点云中进行地点识别 | 在高度动态的环境中可能效果不佳。 |'
- en: '| [[72](#bib.bib72)] | PPFNet | Introduced a network for learning globally
    informed 3D local feature descriptors from point clouds. | Not specified | Finding
    correspondences in unorganized point clouds | Dependency on the quality of local
    features and global context understanding. |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| [[72](#bib.bib72)] | PPFNet | 引入了一个网络，用于从点云中学习全局信息的3D局部特征描述符。 | 未指定 | 在无组织的点云中寻找对应关系
    | 依赖于局部特征和全局上下文理解的质量。 |'
- en: '| [[62](#bib.bib62)] | PointGrid | Combined point and grid-based approaches
    to recognize 3D models from point clouds. | Popular shape recognition benchmarks
    | 3D model recognition, classification, and segmentation | Might not handle extremely
    sparse or irregular point clouds effectively. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| [[62](#bib.bib62)] | PointGrid | 结合点云和网格方法以识别3D模型。 | 流行的形状识别基准 | 3D模型识别、分类和分割
    | 可能无法有效处理极度稀疏或不规则的点云。 |'
- en: '| [[73](#bib.bib73)] | PointFusion | Leverages image and point cloud data for
    3D object detection without dataset-specific tuning. | KITTI, SUN-RGBD | Generic
    3D object detection across diverse environments | The fusion process can be complex
    and computationally intensive. |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| [[73](#bib.bib73)] | PointFusion | 利用图像和点云数据进行3D目标检测，无需数据集特定调整 | KITTI, SUN-RGBD
    | 在多样环境中的通用3D目标检测 | 融合过程可能复杂且计算密集 |'
- en: '| [[74](#bib.bib74)] | Frustum PointNets | Operates on raw point clouds for
    3D object detection, combining 2D and 3D detection methods. | KITTI, SUN RGB-D
    | 3D object detection in both indoor and outdoor scenes | Might encounter difficulties
    with very sparse point clouds or heavy occlusions. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| [[74](#bib.bib74)] | Frustum PointNets | 对原始点云进行3D目标检测，结合2D和3D检测方法 | KITTI,
    SUN RGB-D | 室内和室外场景中的3D目标检测 | 可能遇到非常稀疏的点云或严重遮挡的问题 |'
- en: '| [[75](#bib.bib75)] | 3DFeat-Net | Learns 3D feature detectors and descriptors
    for point cloud matching using weak supervision. | Outdoor Lidar datasets | Point
    cloud matching for localization and mapping | Performance highly dependent on
    the effectiveness of weak supervision learning mechanisms. |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| [[75](#bib.bib75)] | 3DFeat-Net | 学习3D特征检测器和描述符用于点云匹配，使用弱监督 | 户外Lidar数据集
    | 点云匹配用于定位和映射 | 性能高度依赖于弱监督学习机制的有效性 |'
- en: '| [[56](#bib.bib56)] | SpiderCNN | Developed SpiderConv units to handle 3DPCs
    | ModelNet40 | 3DPC classification and segmentation | May struggle with very large
    or noisy datasets |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| [[56](#bib.bib56)] | SpiderCNN | 开发了SpiderConv单元以处理3D点云 | ModelNet40 | 3D点云分类和分割
    | 可能对非常大或噪声数据集表现不佳 |'
- en: '| [[57](#bib.bib57)] | PointCNN | Introduced a X-transformation to handle unordered
    point clouds for CNNs | Multiple challenging benchmark datasets | Feature learning
    from point clouds | Dependent on the effectiveness of the X-transformation |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| [[57](#bib.bib57)] | PointCNN | 引入了X变换来处理无序点云以适应CNN | 多个具有挑战性的基准数据集 | 从点云中学习特征
    | 依赖于X变换的有效性 |'
- en: '| [[76](#bib.bib76)] | SqueezeSeg | Created a CNN pipeline for semantic segmentation
    of LiDAR data | KITTI, GTA-V (simulated) | Semantic segmentation in autonomous
    driving | Reliance on synthetic data for improved accuracy |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| [[76](#bib.bib76)] | SqueezeSeg | 创建了一个用于LiDAR数据语义分割的CNN管道 | KITTI, GTA-V（模拟）
    | 自动驾驶中的语义分割 | 依赖于合成数据以提高准确性 |'
- en: '| [[77](#bib.bib77)] | 2DPASS | Boosted point cloud learning via 2D image fusion
    during training | SemanticKITTI, NuScenes | Semantic segmentation of point clouds
    | Requires multi-modal data during training, complex implementation |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| [[77](#bib.bib77)] | 2DPASS | 通过训练期间的2D图像融合增强点云学习 | SemanticKITTI, NuScenes
    | 点云的语义分割 | 训练期间需要多模态数据，实施复杂 |'
- en: '| [[65](#bib.bib65)] | PREDATOR | Focused on low-overlap point cloud registration
    with deep attention | 3DMatch benchmark | Point cloud registration | Limited to
    pairwise registration, may not generalize beyond benchmark scenarios |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| [[65](#bib.bib65)] | PREDATOR | 专注于低重叠点云配准的深度注意力方法 | 3DMatch基准 | 点云配准 | 限于成对配准，可能无法推广到基准场景之外
    |'
- en: '| [[66](#bib.bib66)] | 3DIoUMatch | Implemented a semi-supervised 3D object
    detection with teacher-student learning | ScanNet, SUN-RGBD, KITTI | 3D object
    detection | High task complexity and dependency on initial label quality |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| [[66](#bib.bib66)] | 3DIoUMatch | 实现了基于教师-学生学习的半监督3D目标检测 | ScanNet, SUN-RGBD,
    KITTI | 3D目标检测 | 任务复杂度高且依赖于初始标签质量 |'
- en: '| [[78](#bib.bib78)] | PointPoseNet | Developed a pipeline for 6D object pose
    estimation from point clouds | LINEMOD, Occlusion LINEMOD | 6D pose estimation
    | Limited to known objects in complex scenes, sensitivity to occlusions |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| [[78](#bib.bib78)] | PointPoseNet | 开发了一个用于从点云中进行6D目标姿态估计的管道 | LINEMOD, Occlusion
    LINEMOD | 6D姿态估计 | 限于复杂场景中的已知物体，对遮挡敏感 |'
- en: '| [[79](#bib.bib79)] | ASAP-Net | Enhanced spatio-temporal modeling of point
    clouds | Synthia, SemanticKITTI | Point cloud sequence segmentation | Requires
    specific attention and structure-aware algorithms for optimal performance |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| [[79](#bib.bib79)] | ASAP-Net | 增强了点云的时空建模 | Synthia, SemanticKITTI | 点云序列分割
    | 需要特定的注意和结构感知算法以实现最佳性能 |'
- en: '| [[80](#bib.bib80)] | MUSCLE | Proposed a compression algorithm for LiDAR
    data using spatio-temporal relationships | UrbanCity, SemanticKITTI | LiDAR data
    compression | Efficiency depends on the variability of the input data |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| [[80](#bib.bib80)] | MUSCLE | 提出了一个基于时空关系的LiDAR数据压缩算法 | UrbanCity, SemanticKITTI
    | LiDAR数据压缩 | 效率依赖于输入数据的变异性 |'
- en: '| [[81](#bib.bib81)] | PC-RGNN | Addressed sparse and partial point clouds
    for 3D detection using GNNs | KITTI | 3D object detection | Highly dependent on
    the quality of initial point cloud data |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| [[81](#bib.bib81)] | PC-RGNN | 使用GNN解决稀疏和部分点云的3D检测问题 | KITTI | 3D目标检测 | 高度依赖初始点云数据的质量
    |'
- en: 2.5 Pre-trained 2D models
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 预训练2D模型
- en: Xu et al. [[82](#bib.bib82)] demonstrate how pretrained 2D image models can
    be adapted for 3DPC understanding with minimal modification. By extending 2D ConvNets
    and vision transformers to handle 3D data, the method involves inflating 2D filters
    to 3D and only finetuning specific layers like the input, output, and normalization
    layers. This approach leverages the deep feature representations learned from
    large-scale 2D image datasets, enabling the models to perform competitively on
    3DPC tasks such as classification and segmentation, while significantly reducing
    the training time and data requirements compared to training from scratch. The
    transferability is facilitated by the similarity in feature representation between
    the 2D and 3D tasks, despite their differences in data modality.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 徐等人[[82](#bib.bib82)]展示了如何将预训练的2D图像模型通过最小修改适配到3D点云理解。该方法通过扩展2D ConvNets和视觉变换器来处理3D数据，涉及将2D滤波器膨胀为3D，并仅微调特定层，如输入层、输出层和归一化层。这种方法利用了从大规模2D图像数据集中学到的深层特征表示，使得模型在3D点云任务（如分类和分割）中表现竞争力，同时显著减少了训练时间和数据需求。尽管2D和3D任务的数据模态不同，但由于特征表示的相似性，转移性得以实现。
- en: Besides, shape classification and part segmentation pose significant challenges
    in [CV](#id3.3.id3). While trained [convolutional neural networks](#id11.11.id11)
    have shown impressive performance on regular grid data like images, accurately
    capturing shape information and geometric representation from irregular and disordered
    point clouds is problematic. To address this, [[54](#bib.bib54)] proposes point
    convolution (Pconv) and point pooling (Ppool) techniques inspired by convolution
    and pooling in image processing, specifically designed for point clouds to learn
    high-level features. Pconv gradually magnifies receptive fields to capture local
    geometric information, while Ppool tackles the disorder of point clouds using
    a symmetric function that aggregates points progressively for a more detailed
    local geometric representation. Our novel network, named PointVGG, incorporates
    Pconv, Ppool, and a graph structure for feature learning in point clouds, and
    is applied to object classification and part segmentation tasks. Experimental
    results demonstrate that PointVGG achieves state-of-the-art performance on challenging
    benchmarks of [3DPC](#id1.1.id1). Moreover, extracting high-level features from
    disordered point cloud data using pre-trained 2D [CNN](#id11.11.id11) remains
    challenging. To address this, [[55](#bib.bib55)] proposes a VGG-based network
    called point positional attention VGG (PointPAVGG), inspired by the classical
    VGG network. Our approach combines global and local features by extracting local
    geometric information from every sphere domain and analyzing the global position
    score using our point attention (PA) module. PointPAVGG, with its graph structure
    point cloud feature extraction and PA, is applied to point cloud classification
    and segmentation tasks. Through comprehensive experiments on ShapeNet and ModelNet,
    our method demonstrates superior performance, achieving state-of-the-art results
    in classification and segmentation tasks.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，形状分类和部件分割在[计算机视觉](#id3.3.id3)领域面临着重大挑战。尽管经过训练的[卷积神经网络](#id11.11.id11)在图像等规则网格数据上表现出色，但从不规则和无序的点云中准确捕捉形状信息和几何表示仍然存在问题。为了解决这个问题，[[54](#bib.bib54)]
    提出了点卷积（Pconv）和点池化（Ppool）技术，这些技术受到图像处理中的卷积和池化的启发，专门设计用于点云以学习高级特征。Pconv 逐渐放大感受野以捕捉局部几何信息，而
    Ppool 则使用对称函数来处理点云的无序性，通过逐步聚合点来获得更详细的局部几何表示。我们的新型网络，名为 PointVGG，结合了 Pconv、Ppool
    和图结构，用于点云的特征学习，并应用于物体分类和部件分割任务。实验结果表明，PointVGG 在[3DPC](#id1.1.id1)的挑战性基准测试中实现了最先进的性能。此外，从无序点云数据中提取高级特征仍然具有挑战性，即使使用预训练的
    2D [CNN](#id11.11.id11) 网络。为了解决这个问题，[[55](#bib.bib55)] 提出了一个基于 VGG 的网络，称为点位置注意力
    VGG（PointPAVGG），它受到经典 VGG 网络的启发。我们的方法通过从每个球体域中提取局部几何信息，并使用我们的点注意力（PA）模块分析全局位置分数，从而结合全局和局部特征。PointPAVGG
    通过其图结构点云特征提取和 PA 模块，应用于点云分类和分割任务。通过在 ShapeNet 和 ModelNet 上的全面实验，我们的方法表现出色，在分类和分割任务中达到了最先进的结果。
- en: The work in [[83](#bib.bib83)] presents a method for automatic detection of
    manhole covers from mobile mapping point cloud data, which are large-scale spatial
    databases used for various purposes. The method uses a fully [CNN](#id11.11.id11)
    with [DTL](#id6.6.id6) and a simplified class activation mapping (CAM) location
    algorithm to accurately determine the position of manhole covers. Different source
    model architectures, such as AlexNet, VGG-16, Inception-v3, and ResNet-101, are
    assessed. Results showed that VGG-16 achieved the best detection performance among
    others, with recall, precision, and F2-score of 0.973 each. The approach also
    achieves a horizontal 95% confidence interval of 16.5 cm for location performance
    using VGG-16 architecture. The study highlights the importance of incorporating
    geometric information channels in the ground image for improved detection and
    location accuracy. Aerial imaging using drones, an efficient timely data collection
    after natural hazards for post-event management, provides detailed site characterization
    with minimal ground support, but results in large amounts of 2D orthomosaic images
    and [3DPC](#id1.1.id1). Effective data processing workflows are needed to identify
    structural damage states. Liao et al. [[84](#bib.bib84)] introduce two [DL](#id4.4.id4)
    models, based on 2D and 3D [CNNs](#id11.11.id11), for post-windstorm classification.
    [DTL](#id6.6.id6) from AlexNet and VGGNet is used for the 2D [CNNs](#id11.11.id11),
    while a 3D fully convolutional network (3DFCN) with skip connections is developed
    and trained for point cloud data. The models are compared using quantitative performance
    measures, and the 3DFCN shows greater robustness in detecting different damage
    classes. This highlights the importance of 3D datasets, particularly depth information,
    in distinguishing between different damage states in structures.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 文献[[83](#bib.bib83)]提出了一种从移动测绘点云数据中自动检测检查井盖的方法，这些数据是用于各种目的的大规模空间数据库。该方法使用了一个完整的[卷积神经网络（CNN）](#id11.11.id11)，结合[深度学习（DTL）](#id6.6.id6)和简化的类别激活映射（CAM）位置算法，以准确确定检查井盖的位置。评估了不同的源模型架构，如AlexNet、VGG-16、Inception-v3和ResNet-101。结果显示，VGG-16在检测性能方面优于其他模型，召回率、精确度和F2分数均为0.973。该方法还使用VGG-16架构实现了位置性能的横向95%置信区间为16.5厘米。研究突出了在地面图像中纳入几何信息通道以提高检测和定位准确性的必要性。使用无人机进行空中成像，在自然灾害后的高效及时数据收集中提供了详细的现场特征描述，尽管这会产生大量的2D正射影像和[3D点云（3DPC）](#id1.1.id1)。有效的数据处理工作流程对于识别结构损坏状态至关重要。Liao等人[[84](#bib.bib84)]介绍了基于2D和3D[卷积神经网络（CNNs）](#id11.11.id11)的两种[深度学习（DL）](#id4.4.id4)模型，用于风暴后分类。对于2D
    [CNNs](#id11.11.id11)，使用了AlexNet和VGGNet的[DTL](#id6.6.id6)，而开发并训练了一个带有跳跃连接的3D全卷积网络（3DFCN）用于点云数据。使用定量性能指标比较了这些模型，3DFCN在检测不同损坏类别方面显示出更大的鲁棒性。这突出了3D数据集，特别是深度信息，在区分结构中不同损坏状态方面的重要性。
- en: In [[85](#bib.bib85)], a [DL](#id4.4.id4)-based approach is proposed, which
    involves projecting [3DPC](#id1.1.id1) into 2D rendering views and then feeding
    them into a [CNN](#id11.11.id11) for quality score prediction. [DTL](#id6.6.id6)
    is employed to leverage the capabilities of VGG-16 trained on the ImageNet database.
    The performance of the proposed model is evaluated on two benchmark databases,
    ICIP2020 and SJTU, and the results show a strong correlation between the predicted
    and subjective quality scores, outperforming state-of-the-art point cloud quality
    assessment models. [[86](#bib.bib86)] addresses the problem of learning outdoor
    [3DPC](#id1.1.id1) from monocular data using a sparse ground-truth dataset. A
    [DL](#id4.4.id4)-based approach called Pix2Point is proposed, which uses a 2D-3D
    hybrid neural network architecture and a supervised end-to-end minimization of
    an optimal transport divergence between point clouds. The proposed approach outperformed
    efficient monocular depth methods when trained on sparse point clouds. The paper
    highlights the potential of [DL](#id4.4.id4) for monocular [3DPC](#id1.1.id1)
    prediction and its ability to handle complete and challenging outdoor scenes.
    The encoding block, in the target model, consists of convolution, pooling, and
    normalization layers to extract feature descriptions from the RGB image. These
    features are then processed by a fully connected layer to obtain a preliminary
    set of 3D point coordinates. Several source models, based on VGG, DenseNet, and
    ResNet architectures, are explored and compared, which are referred to as backbones.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[85](#bib.bib85)]中，提出了一种基于[DL](#id4.4.id4)的方法，该方法涉及将[3DPC](#id1.1.id1)投影到2D渲染视图中，然后将其输入到[CNN](#id11.11.id11)中进行质量评分预测。[DTL](#id6.6.id6)被用于利用在ImageNet数据库上训练的VGG-16的能力。所提模型的性能在两个基准数据库ICIP2020和SJTU上进行了评估，结果显示预测的质量评分与主观质量评分之间存在较强的相关性，超越了当前最先进的点云质量评估模型。[[86](#bib.bib86)]解决了从单目数据中学习户外[3DPC](#id1.1.id1)的问题，该方法使用稀疏真实数据集。提出了一种名为Pix2Point的基于[DL](#id4.4.id4)的方法，该方法使用2D-3D混合神经网络架构和端到端的监督最小化点云之间的最优传输散度。所提方法在稀疏点云上训练时优于高效的单目深度方法。论文强调了[DL](#id4.4.id4)在单目[3DPC](#id1.1.id1)预测中的潜力及其处理完整且具有挑战性的户外场景的能力。目标模型中的编码块由卷积、池化和归一化层组成，用于从RGB图像中提取特征描述。这些特征随后通过全连接层处理，以获得初步的3D点坐标。探讨和比较了基于VGG、DenseNet和ResNet架构的多个源模型，这些模型被称为骨干网络。
- en: Balado et al. [[87](#bib.bib87)] propose in 2020, a method that minimizes the
    use of point cloud samples for training [CNNs](#id11.11.id11) by converting point
    clouds to images (pc-images). This enables the generation of multiple samples
    per object through multi-view, and the combination of pc-images with images from
    online datasets such as ImageNet and Google Images. Results suggest keeping some
    point cloud images in training; even 10% can lead to high classification accuracy.
    The work presented in [[88](#bib.bib88)] showcases a prototypical implementation
    of a service-oriented architecture for classifying indoor point cloud scenes in
    office environments. This approach utilizes multi-view techniques for semantic
    enrichment of captured scans and subsequent classification. The approach is tested
    using a pretrained [CNN](#id11.11.id11) model, Inception V3, to classify common
    office furniture objects such as chairs, sofas, and desks in [3DPC](#id1.1.id1)
    scans. The results show that the approach can achieve acceptable accuracy in classifying
    common office furniture, based on RGB cubemap images of the octree partitioned
    areas of the [3DPC](#id1.1.id1) scan. Additional methods for web-based 3D visualization,
    editing, and annotation of point clouds are also discussed.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Balado等人[[87](#bib.bib87)]在2020年提出了一种方法，该方法通过将点云转换为图像（pc-images）来最小化训练[CNNs](#id11.11.id11)时对点云样本的使用。这使得通过多视角生成每个对象的多个样本成为可能，并将pc-images与来自在线数据集（如ImageNet和Google
    Images）的图像结合。结果表明，保留一些点云图像用于训练，即使是10%的点云图像也可以获得较高的分类准确性。在[[88](#bib.bib88)]中展示的工作展示了一种面向服务的架构原型，用于在办公环境中分类室内点云场景。该方法利用多视角技术对捕获的扫描进行语义丰富和后续分类。该方法使用预训练的[CNN](#id11.11.id11)模型Inception
    V3对[3DPC](#id1.1.id1)扫描中的常见办公家具对象（如椅子、沙发和桌子）进行分类。结果显示，该方法能够在基于RGB立方体图像的[3DPC](#id1.1.id1)扫描的八叉树分区区域中实现对常见办公家具的可接受分类准确性。还讨论了基于Web的3D可视化、编辑和注释点云的额外方法。
- en: The authors in [[89](#bib.bib89)] propose a visual recognition and location
    method for object detection in soft robotic manipulation using RGB-D information
    fusion. The method involves scanning and reconstructing the environment using
    ORB-SLAM2, constructing an object feature database, matching point clouds using
    the iterative closest point (ICP) algorithm, identifying regions of interest,
    and using the inception-v3 model and [DTL](#id6.6.id6) for object recognition.
    The position of the object relative to the camera is obtained through correspondence
    between color information and point cloud data. The method showed that objects
    belonging to the same object have much lower matching error compared to those
    not belonging to the same object, and successful object identification and location
    were achieved through color recognition. Moving on, the authors in [[90](#bib.bib90)]
    adopt the ResNet50 [[91](#bib.bib91)] pretrained on the ImageNet data set to extract
    deep features from each feature image and obtain five multi-scale and multi-view
    (MSMV) deep features per point.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在 [[89](#bib.bib89)] 中提出了一种基于 RGB-D 信息融合的软体机器人操作中的对象检测的视觉识别和定位方法。该方法涉及使用 ORB-SLAM2
    扫描和重建环境，构建对象特征数据库，使用迭代最近点 (ICP) 算法匹配点云，识别感兴趣区域，并使用 inception-v3 模型和 [DTL](#id6.6.id6)
    进行对象识别。通过颜色信息与点云数据的对应关系获得对象相对于相机的位置。该方法显示，相同对象的匹配误差远低于不同对象，且通过颜色识别成功实现了对象识别和定位。接下来，作者在
    [[90](#bib.bib90)] 中采用了在 ImageNet 数据集上预训练的 ResNet50 [[91](#bib.bib91)] 从每个特征图像中提取深层特征，并获得每个点的五个多尺度多视角
    (MSMV) 深层特征。
- en: Table LABEL:tab3 presents a summary of DTL-based [3DPC](#id1.1.id1) models.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 表 LABEL:tab3 展示了基于 DTL 的 [3DPC](#id1.1.id1) 模型的总结。
- en: 'Table 3: Summary of DTL-based [3DPC](#id1.1.id1) models.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：基于 DTL 的 [3DPC](#id1.1.id1) 模型总结。
- en: '| Category | Model | Highlights | Limitations |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 模型 | 亮点 | 局限性 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Point-based methods | AltasNet [[12](#bib.bib12)] | In AtlasNet, a surface
    representation is inferred by regarding a 3D shape as a collection of parametric
    surface elements. | The reiterated many times largely determines the reconstruction.
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 基于点的方法 | AltasNet [[12](#bib.bib12)] | 在 AtlasNet 中，通过将 3D 形状视为一组参数化表面元素来推断表面表示。
    | 反复的过程在很大程度上决定了重建结果。 |'
- en: '| MSN [[92](#bib.bib92)] | A sampling algorithm combines a set of parametric
    surface elements, which MSN predicts, with the partial input. | Fine-grained details
    of object shape aren’t generated successfully. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| MSN [[92](#bib.bib92)] | 一个采样算法将 MSN 预测的参数化表面元素集合与部分输入结合。 | 对象形状的细粒度细节未能成功生成。
    |'
- en: '| ASHF-Net [[93](#bib.bib93)] | A hierarchical folding decoder with the gated
    skip-attention and multi-resolution completion target to exploit the local structure
    details of the incomplete inputs is proposed by ASHF-Net. | The decoder [113]
    obtains unstructured predictions, and the surface of the results doesn’t remain
    smooth. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| ASHF-Net [[93](#bib.bib93)] | ASHF-Net 提出了一个分层折叠解码器，结合了门控跳跃注意力和多分辨率完成目标，以利用不完整输入的局部结构细节。
    | 解码器 [113] 得到的预测不够结构化，结果的表面不保持平滑。 |'
- en: '| Point-based methods | PCN [[94](#bib.bib94)] | The coarse-to-fine completion
    is performed by PCN, which combines the fully connected network and FoldingNet.
    | The synthesis of shape details is not achievable. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 基于点的方法 | PCN [[94](#bib.bib94)] | PCN 通过结合全连接网络和 FoldingNet 执行粗到细的完成。 | 形状细节的合成不可实现。
    |'
- en: '| SA-Net [[95](#bib.bib95)] | Hierarchical folding in the multi-stage points
    generation decoder is proposed by SA-Net. | The implicit representation of the
    target shape from the intermediate layer, which helps refine the shape in the
    local region, is difficult to interpret and constrain. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| SA-Net [[95](#bib.bib95)] | SA-Net 提出了一个多阶段点生成解码器中的分层折叠方法。 | 从中间层获取目标形状的隐式表示，帮助在局部区域细化形状，难以解释和约束。
    |'
- en: '| FoldingNet [[60](#bib.bib60)] | It is commonly assumed in a two-stage generation
    process that a 2D-manifold can recover 3D objects. | Explicitly constraining the
    implicit intermediate is challenging. |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| FoldingNet [[60](#bib.bib60)] | 在两阶段生成过程中，通常假设 2D 流形可以恢复 3D 对象。 | 明确约束隐式中间表示是具有挑战性的。
    |'
- en: '| SK-PCN [[96](#bib.bib96)] | The global structure is acquired by predicting
    the 3D skeleton with SK-PCN, and the surface completion is achieved by learning
    the displacements of skeletal points. | The overall shapes are the sole focus
    of the meso-skeleton. |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| SK-PCN [[96](#bib.bib96)] | 通过预测3D骨架来获得全球结构，并通过学习骨架点的位移来实现表面补全。 | 整体形状是中间骨架的唯一关注点。
    |'
- en: '| Point-based methods | GRNet [[97](#bib.bib97)] | Unordered point clouds are
    regularized by GRNet, which introduces 3D grids as intermediate representations.
    | The resolution still governs it. A significant computational cost is incurred
    when a higher resolution is used. |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 基于点的方法 | GRNet [[97](#bib.bib97)] | GRNet通过引入3D网格作为中间表示来对无序点云进行规则化。 | 分辨率仍然决定了结果。使用更高分辨率时会产生显著的计算成本。
    |'
- en: '| VE-PCN [[98](#bib.bib98)] | The structure information is incorporated into
    the shape completion by VE-PCN through the utilization of edge generation. | High
    frequency components are the sole focus of the edges. |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| VE-PCN [[98](#bib.bib98)] | VE-PCN通过边缘生成将结构信息纳入形状补全中。 | 边缘的唯一关注点是高频成分。 |'
- en: '| Point-PEFT [[99](#bib.bib99)] | Introduces a Parameter-Efficient Fine-Tuning
    method for 3D models, minimizing adaptation costs for downstream tasks with minimal
    trainable parameters. | While effective, the specialized method’s broader applicability
    and long-term adaptability across diverse domains remain unproven. |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Point-PEFT [[99](#bib.bib99)] | 引入了一种用于3D模型的参数高效微调方法，以最小化下游任务的适应成本，且可训练参数最少。
    | 尽管有效，但这种专门化方法在多样化领域的广泛适用性和长期适应性仍未得到证明。 |'
- en: '| DAPT [[100](#bib.bib100)] | Implement a Dynamic Adapter for point cloud analysis,
    offering efficient parameter use and reducing training resources significantly.
    | Although reducing trainable parameters, the adaptability and effectiveness in
    extremely diverse environments is yet to be fully assessed. |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| DAPT [[100](#bib.bib100)] | 为点云分析实现动态适配器，提供高效的参数使用，并显著减少训练资源。 | 尽管减少了可训练参数，但在极为多样化的环境中的适应性和有效性仍未完全评估。
    |'
- en: '| AgileGAN3D [[101](#bib.bib101)] | A novel framework for 3D artistic portrait
    stylization using unpaired 2D exemplars and advanced 3D GAN models. | The dependency
    on the quality and diversity of the 2D exemplars might limit the model’s versatility
    in less controlled scenarios. |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| AgileGAN3D [[101](#bib.bib101)] | 使用无配对的2D样本和先进的3D GAN模型进行3D艺术肖像风格化的新框架。
    | 对2D样本的质量和多样性的依赖可能限制了模型在较少控制的场景中的通用性。 |'
- en: '|  | 3D-TRAM [[102](#bib.bib102)] | Combines transfer learning with a memory
    component to enhance 3D reconstruction from 2D images, leveraging CAD models for
    better accuracy. | The method’s performance can vary significantly with the complexity
    of the scene and the quality of the available CAD models. |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | 3D-TRAM [[102](#bib.bib102)] | 将迁移学习与记忆组件相结合，以提高从2D图像中进行3D重建的效果，利用CAD模型提高准确性。
    | 方法的性能可能会因场景的复杂性和可用CAD模型的质量而显著变化。 |'
- en: 2.6 Pre-trained 3D models
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 预训练的3D模型
- en: Implementing a standardized approach to [3DPC](#id1.1.id1) neural network design
    has the potential to yield comparable advancements seen in the extensive research
    conducted on pre-training visual models, especially in the image domain. However,
    compared to the 2D domain, the design of neural networks for point cloud data
    is less mature, as evidenced by the numerous new architectures proposed recently.
    This is due to several factors, including the challenge of processing unordered
    sets [[103](#bib.bib103)], the choice of neighborhood aggregation mechanism, which
    could be hierarchical [[53](#bib.bib53), [104](#bib.bib104), [105](#bib.bib105)],
    spatial CNN-like [[106](#bib.bib106), [56](#bib.bib56), [57](#bib.bib57), [107](#bib.bib107)],
    spectral [[108](#bib.bib108), [109](#bib.bib109)], or graph-based [[110](#bib.bib110),
    [111](#bib.bib111)], and the fact that points are discrete samples of an underlying
    surface, which has led to the consideration of continuous convolutions [[112](#bib.bib112),
    [113](#bib.bib113)].
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 实施标准化的[3DPC](#id1.1.id1)神经网络设计有可能带来与在图像领域进行的广泛预训练研究中看到的类似进展。然而，与2D领域相比，点云数据的神经网络设计尚不成熟，近期提出了许多新架构。这是由于几个因素，包括处理无序集合的挑战
    [[103](#bib.bib103)]、邻域聚合机制的选择，这可能是层次化的 [[53](#bib.bib53), [104](#bib.bib104),
    [105](#bib.bib105)]、空间CNN-like [[106](#bib.bib106), [56](#bib.bib56), [57](#bib.bib57),
    [107](#bib.bib107)]、谱的 [[108](#bib.bib108), [109](#bib.bib109)] 或基于图的 [[110](#bib.bib110),
    [111](#bib.bib111)]，以及点是底层表面的离散样本，这导致了对连续卷积的考虑 [[112](#bib.bib112), [113](#bib.bib113)]。
- en: While pre-training image models have been successful in achieving high levels
    of prosperity, pre-training 3D models is still in the developmental stage. To
    address this issue, numerous researchers have explored various [SSL](#id14.14.id14)
    mechanisms that utilize different pretext tasks, such as solving jigsaw puzzles
    [[114](#bib.bib114)], estimating orientation [[115](#bib.bib115)], and reconstructing
    deformations [[116](#bib.bib116)]. Drawing inspiration from pre-training strategies
    in the image domain, several approaches have been proposed in the 3D domain, including
    point contrast [[117](#bib.bib117)], which uses a contrastive learning principle,
    and OcCo [[118](#bib.bib118)], Point-BERT [[119](#bib.bib119)], and Point-M2AE
    [[120](#bib.bib120)], which introduce reconstruction pretext tasks to facilitate
    better representation learning. However, the lack of available data in the 3D
    domain remains a significant obstacle in developing more effective pre-training
    strategies.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图像模型的预训练已成功实现高水平的繁荣，3D模型的预训练仍处于开发阶段。为了解决这个问题，众多研究人员探索了各种[SSL](#id14.14.id14)机制，这些机制利用了不同的预任务，例如解谜
    [[114](#bib.bib114)]、估计方向 [[115](#bib.bib115)] 和重建变形 [[116](#bib.bib116)]。受到图像领域预训练策略的启发，3D领域也提出了几种方法，包括采用对比学习原则的点对比
    [[117](#bib.bib117)]，以及引入重建预任务以促进更好表示学习的OcCo [[118](#bib.bib118)]、Point-BERT [[119](#bib.bib119)]
    和 Point-M2AE [[120](#bib.bib120)]。然而，3D领域缺乏可用数据仍然是开发更有效预训练策略的一大障碍。
- en: black For instance, [[117](#bib.bib117)] focuses on developing effective representation
    learning techniques for point cloud data. The authors draw inspiration from the
    success of standardized neural architectures in the 2D domain, such as VGGNet
    [simonyan2014very] and ResNet/ResNeXt [[91](#bib.bib91)], and conjecture that
    a similar standardization of point cloud neural network design could enable similar
    progress. However, compared to the 2D domain, the design of neural networks for
    point cloud data is less mature, as evidenced by the numerous new architectures
    proposed recently. This is due to several factors, including the challenge of
    processing unordered sets [ravanbakhsh2016deep, [52](#bib.bib52), [103](#bib.bib103)],
    the choice of neighborhood aggregation mechanism (e.g., hierarchical [[53](#bib.bib53),
    [104](#bib.bib104), [105](#bib.bib105)], spatial [CNN](#id11.11.id11)-like [[106](#bib.bib106),
    [56](#bib.bib56), [57](#bib.bib57), [107](#bib.bib107)], spectral [[108](#bib.bib108),
    [109](#bib.bib109)], or graph-based [[110](#bib.bib110), [111](#bib.bib111)]),
    and the fact that points are discrete samples of an underlying surface, which
    has led to the consideration of continuous convolutions [[112](#bib.bib112), [113](#bib.bib113)].
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[[117](#bib.bib117)] 专注于为点云数据开发有效的表征学习技术。作者从 2D 领域中标准化神经网络架构的成功中汲取灵感，如 VGGNet
    [simonyan2014very] 和 ResNet/ResNeXt [[91](#bib.bib91)]，并推测类似的点云神经网络设计标准化可能带来类似的进展。然而，与
    2D 领域相比，点云数据的神经网络设计尚不成熟，最近提出了许多新的架构，这表明这一点。这是由于几个因素，包括处理无序集合的挑战 [ravanbakhsh2016deep,
    [52](#bib.bib52), [103](#bib.bib103)]，邻域聚合机制的选择（例如，分层 [[53](#bib.bib53), [104](#bib.bib104),
    [105](#bib.bib105)]、空间 [CNN](#id11.11.id11)-类似 [[106](#bib.bib106), [56](#bib.bib56),
    [57](#bib.bib57), [107](#bib.bib107)]、谱 [[108](#bib.bib108), [109](#bib.bib109)]
    或基于图的 [[110](#bib.bib110), [111](#bib.bib111)]），以及点是潜在表面的离散样本，这导致了对连续卷积 [[112](#bib.bib112),
    [113](#bib.bib113)] 的考虑。
- en: In this respect, Choy et al. proposed the Minkowski Engine [[121](#bib.bib121)],
    which is an extension of sub-manifold sparse convolutional networks [[122](#bib.bib122)]
    to higher dimensions. By facilitating the adoption of common deep architectures
    from 2D vision, sparse convolutional networks could help standardize [DL](#id4.4.id4)
    for point cloud. In [[117](#bib.bib117)], the authors use a unified UNet [[123](#bib.bib123)]
    architecture built with Minkowski Engine as the backbone network in all our experiments
    and show that it can seamlessly transfer between tasks and datasets. [[120](#bib.bib120)]
    The paper proposes Point-M2AE scheme, a [multi-scale masked autoencoder](#id12.12.id12)
    ([MAE](#id12.12.id12)) pre-training framework for [SSL](#id14.14.id14) of [3DPC](#id1.1.id1).
    Unlike standard [auto-encoder](#id27.27.id27) ([AE](#id27.27.id27))-based transformers,
    Point-M2AE modifies the encoder and decoder into pyramid architectures to model
    spatial geometries and capture fine-grained and high-level semantics of 3D shapes.
    The encoder uses a multi-scale masking strategy for consistent visible regions
    across scales and a local spatial self-attention [[124](#bib.bib124)] mechanism
    during fine-tuning. The lightweight decoder gradually upsamples point tokens with
    skip connections from the encoder, promoting reconstruction from a global-to-local
    perspective. Point-M2AE achieves 92.9% accuracy on ModelNet40 using a linear SVM.
    Fine-tuning enhances performance to 86.43% on ScanObjectNN and provides benefits
    in various tasks, such as few-shot classification, part segmentation, and 3D object
    detection.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，Choy 等人提出了 Minkowski Engine [[121](#bib.bib121)]，这是对子流形稀疏卷积网络 [[122](#bib.bib122)]
    在更高维度上的扩展。通过促进从 2D 视觉中常见深度架构的采用，稀疏卷积网络可以帮助标准化[深度学习](#id4.4.id4)在点云中的应用。在 [[117](#bib.bib117)]
    中，作者使用了一个统一的 UNet [[123](#bib.bib123)] 架构，Minkowski Engine 作为所有实验中的骨干网络，并展示了它可以在任务和数据集之间无缝转移。[[120](#bib.bib120)]
    论文提出了 Point-M2AE 方案，一个[多尺度掩码自编码器](#id12.12.id12)（[MAE](#id12.12.id12)）预训练框架，用于[自监督学习](#id14.14.id14)的[3D
    点云](#id1.1.id1)。与基于标准[自编码器](#id27.27.id27)（[AE](#id27.27.id27)）的变换器不同，Point-M2AE
    将编码器和解码器修改为金字塔架构，以建模空间几何并捕捉 3D 形状的细粒度和高级语义。编码器使用多尺度掩码策略以确保跨尺度的一致可见区域，并在微调期间采用局部空间自注意力
    [[124](#bib.bib124)] 机制。轻量级解码器通过从编码器中跳跃连接逐渐上采样点令牌，从全局到局部的视角促进重建。Point-M2AE 在 ModelNet40
    上使用线性 SVM 达到了 92.9% 的准确率。微调将性能提升到 86.43% 在 ScanObjectNN 上，并在各种任务中提供了好处，如少样本分类、部件分割和
    3D 目标检测。
- en: The reference [[125](#bib.bib125)] introduces the PointCLIP method, which encodes
    point clouds by projecting them into multi-view depth maps and aggregates view-wise
    zero-shot predictions for knowledge transfer from 2D to 3D. An inter-view adapter
    is designed to extract global features and adaptively fuse few-shot knowledge
    from 3D into CLIP pre-trained in 2D. Fine-tuning the lightweight adapter in few-shot
    settings significantly improves PointCLIP’s performance. PointCLIP also exhibits
    complementary properties with classical 3D-supervised networks, and ensembling
    with baseline models further boosts performance, surpassing state-of-the-art models.
    PointCLIP is a promising alternative for effective [3DPC](#id1.1.id1) understanding
    with low resource cost and data regime, as demonstrated through experiments on
    ModelNet10, ModelNet40, and ScanObjectNN datasets. Huang et al. [[126](#bib.bib126)]
    proposes a method called MF-PointNN for surrogate modeling of melt pool in metallic
    additive manufacturing. Melt pool modeling is important for uncertainty quantification
    and quality control in metal additive manufacturing, but finite element simulation
    for thermal modeling can be time-consuming. MF-PointNN is a multi-fidelity approach
    that combines low-fidelity analytical models and high-fidelity finite element
    simulation data using [DTL](#id6.6.id6). A basic PointNN is first trained with
    low-fidelity data to establish correlation between inputs and thermal field of
    analytical models. Then, the basic PointNN is updated and fine-tuned using a small
    amount of high-fidelity data to build the MF-PointNN. This latter efficiently
    maps input variables and spatial positions to thermal histories, allowing for
    efficient prediction of the three-dimensional melt pool. Results of melt pool
    modeling for Ti-6Al-4V in electron beam additive manufacturing under uncertainty
    show the effectiveness of the proposed approach.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献[[125](#bib.bib125)]介绍了PointCLIP方法，该方法通过将点云投影到多视角深度图中进行编码，并聚合视角级的零样本预测，以实现从2D到3D的知识转移。设计了一个视角间适配器，用于提取全局特征并自适应地将来自3D的少量样本知识融合到2D预训练的CLIP中。在少样本设置下对轻量级适配器进行微调显著提高了PointCLIP的性能。PointCLIP还表现出与经典3D监督网络的互补特性，与基准模型的集成进一步提升了性能，超过了最先进的模型。实验表明，PointCLIP是一个有前景的替代方案，用于低资源成本和数据制度下有效的[3DPC](#id1.1.id1)理解，实验数据来自ModelNet10、ModelNet40和ScanObjectNN数据集。黄等人[[126](#bib.bib126)]提出了一种名为MF-PointNN的方法，用于金属增材制造中熔池的代理建模。熔池建模对于金属增材制造中的不确定性量化和质量控制非常重要，但热模型的有限元仿真可能非常耗时。MF-PointNN是一种多保真度方法，它结合了低保真度的分析模型和高保真度的有限元仿真数据，使用[DTL](#id6.6.id6)。首先使用低保真度数据训练一个基本的PointNN，以建立输入与分析模型热场之间的关联。然后，使用少量高保真度数据更新和微调基本PointNN，以构建MF-PointNN。后者有效地将输入变量和空间位置映射到热历史，从而实现三维熔池的高效预测。针对Ti-6Al-4V在电子束增材制造中的熔池建模结果显示了该方法的有效性。
- en: 2.7 Evaluation metrics
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7 评估指标
- en: Several metrics have been reported to evaluate the performance of different
    [DTL](#id6.6.id6) approaches for various [3DPC](#id1.1.id1) tasks. F1 measures
    and [overall accuracy](#id16.16.id16) ([OA](#id16.16.id16)), are the most frequently
    used measures to assess the performance of algorithms over benchmark datasets.
    These metrics are used for many purposes, including segmentation, classification,
    and registration. F1 measures give an idea about the behavior of the precision
    and recall curve, whereas [OA](#id16.16.id16) conveys the mean accuracy for instances
    of the test. Besides, [intersection over union](#id13.13.id13) ([IoU](#id13.13.id13))
    and mean [IoU](#id13.13.id13) have been extensively used, especially for object
    detection and segmentation purpose. For instance, in [[127](#bib.bib127)], authors
    have used mean [IoU](#id13.13.id13) between point-wise ground truth and prediction.
    In [[128](#bib.bib128)], authors have computed [IoU](#id13.13.id13) for different
    shapes. It is calculated as the average of [IoUs](#id13.13.id13) of all parts
    in a shape. Furthermore, for some particular categories, mean IoUs (mIoUs) are
    obtained by finding out the average of all [IoUs](#id13.13.id13) shapes. Other
    metrics, instance mIoU (Ins. mIoU) and category mIoU (Cat. mIoU) are also introduced
    and computed by calculating the average of all shapes and the average of mIoUs
    for all categories, respectively.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 已有多个指标被报告用于评估不同[DTL](#id6.6.id6)方法在各种[3DPC](#id1.1.id1)任务中的表现。F1度量和[总体准确率](#id16.16.id16)（[OA](#id16.16.id16)）是最常用的衡量算法在基准数据集上表现的指标。这些指标用于许多目的，包括分割、分类和配准。F1度量提供了精度和召回率曲线的行为信息，而[OA](#id16.16.id16)则传达测试实例的平均准确性。此外，[交并比](#id13.13.id13)（[IoU](#id13.13.id13)）和平均[IoU](#id13.13.id13)被广泛使用，特别是在目标检测和分割任务中。例如，在[[127](#bib.bib127)]中，作者使用了点对点的平均[IoU](#id13.13.id13)进行预测。在[[128](#bib.bib128)]中，作者计算了不同形状的[IoU](#id13.13.id13)。它是形状中所有部分[IoUs](#id13.13.id13)的平均值。此外，对于一些特定类别，平均[IoUs](#id13.13.id13)（mIoUs）是通过计算所有形状的[IoUs](#id13.13.id13)的平均值获得的。其他指标，如实例mIoU（Ins.
    mIoU）和类别mIoU（Cat. mIoU），也是通过计算所有形状的平均值以及所有类别的mIoUs的平均值来引入和计算的。
- en: Metrics such as OA and mIoU provide a holistic view of the model’s performance
    across different categories. Additionally, the frame per second (FPS) metric evaluates
    the efficiency and speed of real-time applications, while root mean squared error
    (RMSE) and mean absolute percentage error (MAPE) are used for precision in measurement
    tasks. Moving on, consistency rate (CR), consistency proportion (CP), and weight
    coverage (WCov) are advanced evaluation metrics used to assess the quality of
    3D point cloud reconstructions. CR measures the rate at which a reconstructed
    scene maintains geometric consistency across different views or instances. CP
    evaluates the proportion of consistent data points within the entire dataset,
    providing a sense of how uniformly the model performs. WCov assesses the coverage
    of the weighted areas in the point cloud, indicating how well the model captures
    the essential features and structures of the scene.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 像OA和mIoU这样的指标提供了模型在不同类别中的表现的整体视图。此外，帧每秒（FPS）指标评估实时应用的效率和速度，而均方根误差（RMSE）和平均绝对百分比误差（MAPE）用于测量任务的精度。接下来，一致性率（CR）、一致性比例（CP）和加权覆盖（WCov）是用于评估3D点云重建质量的高级评估指标。CR衡量重建场景在不同视图或实例中保持几何一致的比例。CP评估数据集中一致数据点的比例，提供模型表现均匀性的感知。WCov评估点云中加权区域的覆盖情况，指示模型捕捉场景关键特征和结构的能力。
- en: To evaluate the performance or semantic segmentation, authors in [[129](#bib.bib129)]
    have used errors of commission and errors of omission metrics over the CMP façade
    dataset. Frame per second (FPS) metric is used by [[130](#bib.bib130)] on the
    ouster LiDAR-64 dataset. In addition to this, Zong et al. [[131](#bib.bib131)]
    have used average precision (mPrec) and average recall (mRec) to test their segmentation
    method. Moreover, the best performance for [OA](#id16.16.id16) on the Shapenet
    dataset is 94.5% [[132](#bib.bib132)]. However, the highest precision (85.99%)
    is achieved on CMP facade dataset [[129](#bib.bib129)]. The F1 score has been
    calculated on different datasets, viz. CMP facade, ISPRS, Semantic 3D dataset,
    ModelNet4, and other. The best performance has been reported by [[133](#bib.bib133)]
    on ModelNet40\. The aforementioned metrics are not only applicable to [3DPC](#id1.1.id1)
    tasks but also to other artificial intelligence-related tasks such as classification,
    segmentation, and other. However, subsequent subsections thoroughly elaborate
    on specific metrics relevant to [3DPC](#id1.1.id1) tasks.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估性能或语义分割，作者在[[129](#bib.bib129)]中使用了CMP立面数据集上的委托错误和遗漏错误度量。[[130](#bib.bib130)]使用帧率（FPS）度量于ouster
    LiDAR-64数据集。此外，Zong等人[[131](#bib.bib131)]使用了平均精度（mPrec）和平均召回率（mRec）来测试他们的分割方法。此外，在Shapenet数据集上[OA](#id16.16.id16)的最佳性能为94.5%[[132](#bib.bib132)]。然而，最高精度（85.99%）是在CMP立面数据集[[129](#bib.bib129)]上获得的。F1分数已在不同数据集上计算，即CMP立面、ISPRS、Semantic
    3D数据集、ModelNet4等。最佳性能由[[133](#bib.bib133)]在ModelNet40上报告。上述度量不仅适用于[3DPC](#id1.1.id1)任务，也适用于其他人工智能相关任务，如分类、分割等。然而，后续小节将详细阐述与[3DPC](#id1.1.id1)任务相关的特定度量。
- en: 2.7.1 Distance metrics
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.1 距离度量
- en: Distance metrics play a pivotal role in the processing of point clouds for tasks
    such as nearest-neighbor search, clustering, and segmentation. They provide a
    measure of similarity or dissimilarity between points in space, influencing the
    outcomes of many algorithms.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 距离度量在点云处理中的作用至关重要，涉及最近邻搜索、聚类和分割等任务。它们提供了空间中点之间的相似性或差异性的度量，从而影响许多算法的结果。
- en: 2.7.2 Euclidean Distance
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.2 欧几里得距离
- en: 'Defined as the square root of the sum of the squared differences between the
    coordinates of two points:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 定义为两个点坐标之间平方差的平方根：
- en: '|  | $d(\mathbf{p},\mathbf{q})=\sqrt{(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\cdots+(p_{n}-q_{n})^{2}}$
    |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $d(\mathbf{p},\mathbf{q})=\sqrt{(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\cdots+(p_{n}-q_{n})^{2}}$
    |  |'
- en: where $\mathbf{p}=(p_{1},p_{2},\ldots,p_{n})$ and $\mathbf{q}=(q_{1},q_{2},\ldots,q_{n})$
    represent the coordinates of two points in an $n$-dimensional space. The Euclidean
    distance $d(\mathbf{p},\mathbf{q})$ calculates the "as-the-crow-flies" distance
    between the two points, effectively measuring the length of the straight line
    segment that connects them.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{p}=(p_{1},p_{2},\ldots,p_{n})$ 和 $\mathbf{q}=(q_{1},q_{2},\ldots,q_{n})$
    表示 $n$ 维空间中两个点的坐标。欧几里得距离 $d(\mathbf{p},\mathbf{q})$ 计算两个点之间的“直线距离”，有效地测量连接它们的直线段的长度。
- en: 2.7.3 Manhattan Distance
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.3 曼哈顿距离
- en: 'Also known as taxicab or city block distance, it is the sum of the absolute
    differences of their coordinates:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为出租车或城市街区距离，是其坐标绝对差值的总和：
- en: '|  | $d(\mathbf{p},\mathbf{q})=&#124;p_{1}-q_{1}&#124;+&#124;p_{2}-q_{2}&#124;+\cdots+&#124;p_{n}-q_{n}&#124;$
    |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $d(\mathbf{p},\mathbf{q})=&#124;p_{1}-q_{1}&#124;+&#124;p_{2}-q_{2}&#124;+\cdots+&#124;p_{n}-q_{n}&#124;$
    |  |'
- en: This metric is ideal for grid-based and urban environments where travel paths
    are constrained to grid layouts.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 该度量适用于网格基础和城市环境，其中旅行路径被限制在网格布局中。
- en: 2.7.4 Mahalanobis Distance
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.4 马哈拉诺比斯距离
- en: 'Takes into account the correlations of the dataset and is scale-invariant,
    defined as:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑了数据集的相关性，并且对尺度不变，定义为：
- en: '|  | $d(\mathbf{x},\mathbf{\mu})=\sqrt{(\mathbf{x}-\mathbf{\mu})^{T}\mathbf{S}^{-1}(\mathbf{x}-\mathbf{\mu})}$
    |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | $d(\mathbf{x},\mathbf{\mu})=\sqrt{(\mathbf{x}-\mathbf{\mu})^{T}\mathbf{S}^{-1}(\mathbf{x}-\mathbf{\mu})}$
    |  |'
- en: 'where:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathbf{x}$ is the vector representing the point whose distance from the distribution
    is being measured.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathbf{x}$ 是表示点的向量，该点的距离正被测量。
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathbf{\mu}$ is the mean vector of the distribution, representing the central
    tendency.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathbf{\mu}$ 是分布的均值向量，代表中心趋势。
- en: •
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathbf{S}^{-1}$ is the inverse of the covariance matrix $\mathbf{S}$ of the
    distribution, which adjusts the distance measure to account for the spread and
    orientation of the data points.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathbf{S}^{-1}$ 是分布的协方差矩阵 $\mathbf{S}$ 的逆矩阵，用于调整距离度量，以考虑数据点的分布和方向。
- en: This formulation accounts for the shape of the data distribution, correcting
    distances based on how data spreads and correlates across dimensions, thus providing
    a more nuanced measure of distance.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这种公式考虑了数据分布的形状，基于数据在各维度的扩展和相关性来修正距离，从而提供了更细致的距离度量。
- en: 2.7.5 Hausdorff distance
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.5 Hausdorff距离
- en: 'Let us assume that there are two point clouds $S_{1}$ and $S_{2}$ in $R^{3}$
    with $N_{1}$ and $N_{2}$ points in each cloud, respectively. In order to compare
    these point sets, one of the earliest methods that was proposed is the Hausdorff
    distance ($\mathcal{D}_{H}$), which is based on finding the minimum distance from
    a point to a set [[134](#bib.bib134)]:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在$R^{3}$中有两个点云$S_{1}$和$S_{2}$，每个点云中分别有$N_{1}$和$N_{2}$个点。为了比较这些点集，早期提出的一种方法是Hausdorff距离（$\mathcal{D}_{H}$），其基于从点到集合的最小距离[[134](#bib.bib134)]：
- en: '|  | $d(x,y)=\left\&#124;x-y\right\&#124;_{2}$ |  | (3) |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | $d(x,y)=\left\|x-y\right\|_{2}$ |  | (3) |'
- en: '|  | $D(x,S)=\underset{y\in S}{\text{min }}d(x,y)$ |  | (4) |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | $D(x,S)=\underset{y\in S}{\text{min }}d(x,y)$ |  | (4) |'
- en: 'and calculates a symmetric max min distance [[135](#bib.bib135)]:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 并计算一个对称的最大最小距离[[135](#bib.bib135)]：
- en: '|  | $\mathcal{D}_{H}(S_{1},S_{2})=\max\left\{\underset{a\in S_{1}}{\max}D(a,S_{2}),\underset{b\in
    S_{2}}{\max}D(b,S_{1})\right\}$ |  | (5) |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D}_{H}(S_{1},S_{2})=\max\left\{\underset{a\in S_{1}}{\max}D(a,S_{2}),\underset{b\in
    S_{2}}{\max}D(b,S_{1})\right\}$ |  | (5) |'
- en: 2.7.6 Chamfer distance
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.6 Chamfer距离
- en: 'The [Chamfer distance](#id17.17.id17) ([CD](#id17.17.id17)) is a metric used
    to evaluate the similarity between two sets of points in space. It considers the
    distance between each point in both sets and finds the nearest point in the other
    set, then sums the square of these distances.[CD](#id17.17.id17) is commonly used
    in the ShapeNet’s shape reconstruction challenge. The [CD](#id17.17.id17) between
    two point clouds, S1 and S2, is defined as [[136](#bib.bib136)]:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[Chamfer距离](#id17.17.id17) ([CD](#id17.17.id17))是一种用于评估空间中两个点集之间相似性的度量。它考虑了两个集合中每个点之间的距离，找到在另一个集合中最接近的点，然后将这些距离的平方求和。[CD](#id17.17.id17)通常用于ShapeNet的形状重建挑战。两个点云S1和S2之间的[CD](#id17.17.id17)定义为[[136](#bib.bib136)]：'
- en: '|  | $\mathcal{D}_{CD}\left(S_{1},S_{2}\right)=\frac{1}{\left&#124;S_{1}\right&#124;}\sum_{x\in
    S_{1}}\underset{y\in S_{2}}{\min}\left\&#124;x-y\right\&#124;_{2}^{2}+\frac{1}{\left&#124;S_{2}\right&#124;}\sum_{y\in
    S_{2}}\underset{x\in S_{1}}{\min}\left\&#124;x-y\right\&#124;_{2}^{2}$ |  | (6)
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D}_{CD}\left(S_{1},S_{2}\right)=\frac{1}{\left|S_{1}\right|}\sum_{x\in
    S_{1}}\underset{y\in S_{2}}{\min}\left\|x-y\right\|_{2}^{2}+\frac{1}{\left|S_{2}\right|}\sum_{y\in
    S_{2}}\underset{x\in S_{1}}{\min}\left\|x-y\right\|_{2}^{2}$ |  | (6) |'
- en: 2.7.7 Earth mover’s distance
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.7 地球搬运工距离
- en: In contrast to the Hausdorff distance and its derivative methods that involve
    identifying the closest neighboring point for each point, the Earth mover’s distance
    (EMD) or the Wasserstein distance operates by establishing a one-to-one correspondence
    (i.e., bijection represented by $\zeta$) between the two sets of points. The objective
    is to minimize the total distance between the corresponding points in the two
    sets [[134](#bib.bib134)].
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 与Hausdorff距离及其派生方法涉及为每个点识别最近邻点不同，地球搬运工距离（EMD）或Wasserstein距离通过在两点集之间建立一对一的对应关系（即，由$\zeta$表示的双射）来操作。其目标是最小化两个集合中对应点之间的总距离[[134](#bib.bib134)]。
- en: '|  | $\mathcal{D}_{EMD}(S_{1},S_{2})=\underset{\zeta:S_{1}\longrightarrow S_{2}}{\min}\sum_{a\in
    S_{1}}\left\&#124;a-\zeta(a)\right\&#124;_{2}$ |  | (7) |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D}_{EMD}(S_{1},S_{2})=\underset{\zeta:S_{1}\longrightarrow S_{2}}{\min}\sum_{a\in
    S_{1}}\left\|a-\zeta(a)\right\|_{2}$ |  | (7) |'
- en: 2.8 Datasets
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8 数据集
- en: There is a considerable number of datasets available for performing various
    [3DPC](#id1.1.id1) tasks using state-of-the-art techniques. A detail about datasets
    on which [DL](#id4.4.id4) has been implemented is reported in [[31](#bib.bib31)].
    However, in our study, we have considered the datasets for which [DTL](#id6.6.id6)
    has been incorporated to carry out [3DPC](#id1.1.id1) tasks. These include widely
    used benchmarks like, PointNet [[137](#bib.bib137)], ShapeNet [[117](#bib.bib117)],
    ModelNet [[128](#bib.bib128)], ScanNet [[117](#bib.bib117)], ISPRS [[138](#bib.bib138)],
    KITTI 3D object detection [[130](#bib.bib130)], Campus3D [[139](#bib.bib139)]
    and semantic 3D dataset [[140](#bib.bib140)]. Apart from these well-known benchmarks,
    in this study, newly introduced datasets are also included. These comprises of
    Scaffolds dataset [[140](#bib.bib140)], CMP façade dataset [[129](#bib.bib129)],
    DFC [[138](#bib.bib138)], Ouster LiDAR-64 [[130](#bib.bib130)], Santa Monica point
    cloud data [[141](#bib.bib141)], UTD-MHAD [[142](#bib.bib142)], PointDA-10 [[143](#bib.bib143)],
    NYU [[144](#bib.bib144)], SHREC2018 [[145](#bib.bib145)], and EndoSLAM dataset
    [[146](#bib.bib146)].
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 有大量的数据集可用于使用最先进技术执行各种[3DPC](#id1.1.id1)任务。关于已实施[DL](#id4.4.id4)的数据集的详细信息在[[31](#bib.bib31)]中报告。然而，在我们的研究中，我们考虑了已结合[DTL](#id6.6.id6)以执行[3DPC](#id1.1.id1)任务的数据集。这些数据集包括广泛使用的基准，如PointNet
    [[137](#bib.bib137)]、ShapeNet [[117](#bib.bib117)]、ModelNet [[128](#bib.bib128)]、ScanNet
    [[117](#bib.bib117)]、ISPRS [[138](#bib.bib138)]、KITTI 3D物体检测 [[130](#bib.bib130)]、Campus3D
    [[139](#bib.bib139)]和语义3D数据集 [[140](#bib.bib140)]。除了这些知名的基准数据集，本研究还包括了新引入的数据集。这些数据集包括Scaffolds数据集
    [[140](#bib.bib140)]、CMP立面数据集 [[129](#bib.bib129)]、DFC [[138](#bib.bib138)]、Ouster
    LiDAR-64 [[130](#bib.bib130)]、Santa Monica点云数据 [[141](#bib.bib141)]、UTD-MHAD [[142](#bib.bib142)]、PointDA-10
    [[143](#bib.bib143)]、NYU [[144](#bib.bib144)]、SHREC2018 [[145](#bib.bib145)] 和
    EndoSLAM数据集 [[146](#bib.bib146)]。
- en: 'These datasets are used for [3DPC](#id1.1.id1) segmentation (semantic, panoptic,
    and instance), classification, and object detection and tracking and contain features
    beneficial for carrying out related tasks. For instance, datasets like ModelNet
    40, ShapeNet and ScanNet, among others, are used for shape classification and
    contain columns like sample size, classes, training, testing data percentage,
    and other useful features. In addition to this, SHREC2018 and EndoSLAM are medical-related
    datasets. The latter is created through the recording of multiple endoscope cameras
    for six porcine organs, as well as synthetically generated records. However, the
    former, the SHREC2018 protein dataset, contains 2267 protein details. To record
    the details, the protein data bank (PDB) format has been used. The datasets, references,
    and available links are given in Table [4](#S2.T4 "Table 4 ‣ 2.8 Datasets ‣ 2
    Background ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning:
    A Comprehensive Survey").'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '这些数据集用于[3DPC](#id1.1.id1)分割（语义、全景和实例）、分类、以及物体检测和跟踪，包含对执行相关任务有益的特征。例如，像ModelNet
    40、ShapeNet和ScanNet等数据集用于形状分类，并包含样本大小、类别、训练、测试数据百分比及其他有用特征。除此之外，SHREC2018和EndoSLAM是医学相关的数据集。后者通过录制多个内窥镜相机对六个猪器官进行记录，并生成合成记录。然而，前者SHREC2018蛋白质数据集包含2267个蛋白质细节。为记录这些细节，使用了蛋白质数据银行（PDB）格式。数据集、参考文献和可用链接见表[4](#S2.T4
    "Table 4 ‣ 2.8 Datasets ‣ 2 Background ‣ Advancing 3D Point Cloud Understanding
    through Deep Transfer Learning: A Comprehensive Survey")。'
- en: 'Table 4: Dataset and evaluation metrics used by [DTL](#id6.6.id6) methods for
    various [3DPC](#id1.1.id1) tasks.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: [DTL](#id6.6.id6)方法用于各种[3DPC](#id1.1.id1)任务的数据集和评估指标。'
- en: '| Ref. | Dataset | Dataset availability | Evaluation metric | Best performance
    (%) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Ref. | 数据集 | 数据集可用性 | 评估指标 | 最佳性能 (%) |'
- en: '| [[117](#bib.bib117)] | ShapeNet, ScanNet | [http://www.scan-net.org/#code-and-data](http://www.scan-net.org/#code-and-data)
    | - | - |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| [[117](#bib.bib117)] | ShapeNet、ScanNet | [http://www.scan-net.org/#code-and-data](http://www.scan-net.org/#code-and-data)
    | - | - |'
- en: '| [[137](#bib.bib137)] | PointNet [[52](#bib.bib52)], ShapeNet [[147](#bib.bib147)],
    and ModelNet-40 | [http://stanford.edu/rqi/pointnet/](http://stanford.edu/rqi/pointnet/)
    [https://shapenet.org/](https://shapenet.org/)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '| [[137](#bib.bib137)] | PointNet [[52](#bib.bib52)]、ShapeNet [[147](#bib.bib147)]
    和 ModelNet-40 | [http://stanford.edu/rqi/pointnet/](http://stanford.edu/rqi/pointnet/)
    [https://shapenet.org/](https://shapenet.org/)'
- en: '[https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/) |
    - | - |'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/) |
    - | - |'
- en: '| [[148](#bib.bib148)] | UTD-MHAD | [http://www.utdallas.edu/kehtar/Kinect2Dataset.zip](http://www.utdallas.edu/kehtar/Kinect2Dataset.zip)
    | Recognition rate | 92.50 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| [[148](#bib.bib148)] | UTD-MHAD | [http://www.utdallas.edu/kehtar/Kinect2Dataset.zip](http://www.utdallas.edu/kehtar/Kinect2Dataset.zip)
    | 识别率 | 92.50 |'
- en: '| [[133](#bib.bib133)] | ModelNet40 | [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/)
    | F1, precision, recall | F1 (91) |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| [[133](#bib.bib133)] | ModelNet40 | [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/)
    | F1, 精确度, 召回率 | F1 (91) |'
- en: '| [[149](#bib.bib149)] | ModelNet | [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/)
    | MAE-T, MAE-F | CA (97.6) |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| [[149](#bib.bib149)] | ModelNet | [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/)
    | MAE-T, MAE-F | CA (97.6) |'
- en: '| [[128](#bib.bib128)] | ShapeNet, ModelNet | [https://shapenet.org/](https://shapenet.org/)
    [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/) | OA
    | 90.40 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| [[128](#bib.bib128)] | ShapeNet, ModelNet | [https://shapenet.org/](https://shapenet.org/)
    [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/) | OA
    | 90.40 |'
- en: '| [[138](#bib.bib138)] | ISPRS, DFC | [https://www.isprs.org/data/](https://www.isprs.org/data/)
    | F1, [OA](#id16.16.id16) | F1 (83.62), OA (89.84) |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bib138)] | ISPRS, DFC | [https://www.isprs.org/data/](https://www.isprs.org/data/)
    | F1, [OA](#id16.16.id16) | F1 (83.62), OA (89.84) |'
- en: '| [[130](#bib.bib130)] | KITTI 3D object detection, Ouster LiDAR-64 | [https://ouster.com/resources/lidar-sample-data/](https://ouster.com/resources/lidar-sample-data/)
    | FPS (frame per second) | FPS (30.6) |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| [[130](#bib.bib130)] | KITTI 3D 目标检测, Ouster LiDAR-64 | [https://ouster.com/resources/lidar-sample-data/](https://ouster.com/resources/lidar-sample-data/)
    | 每秒帧数 (FPS) | FPS (30.6) |'
- en: '| [[150](#bib.bib150)] | ImageNet, KITTI 2D, VLP-16 | [http://www.image-net.org/about-stats](http://www.image-net.org/about-stats)
    | IoU, F1, mAP | mAP(81.27), IoU(65.11), F1 (0.82) |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| [[150](#bib.bib150)] | ImageNet, KITTI 2D, VLP-16 | [http://www.image-net.org/about-stats](http://www.image-net.org/about-stats)
    | IoU, F1, 平均精度 (mAP) | mAP (81.27), IoU (65.11), F1 (0.82) |'
- en: '| [[140](#bib.bib140)] | Semantic3D dataset [22], Scaffolds dataset | [http://www.semantic3d.net/view_dbase.php?chl=1](http://www.semantic3d.net/view_dbase.php?chl=1)
    | F1, precision, recall | F1 (90.84) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| [[140](#bib.bib140)] | Semantic3D 数据集 [22], Scaffolds 数据集 | [http://www.semantic3d.net/view_dbase.php?chl=1](http://www.semantic3d.net/view_dbase.php?chl=1)
    | F1, 精确度, 召回率 | F1 (90.84) |'
- en: '| [[139](#bib.bib139)] | Campus3D | [https://3d.nus.app/](https://3d.nus.app/)
    | OA, IoU, mIoU, CR, CP and WCov | OA(90.9), mIoU(61.5) |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| [[139](#bib.bib139)] | Campus3D | [https://3d.nus.app/](https://3d.nus.app/)
    | OA, IoU, mIoU, CR, CP 和 WCov | OA (90.9), mIoU (61.5) |'
- en: '| [[129](#bib.bib129)] | CMP façade dataset | [https://cmp.felk.cvut.cz/tylecr1/facade/](https://cmp.felk.cvut.cz/tylecr1/facade/)
    | Precision, recall, F1, Errors of commission, Errors of omission | Precision
    (85.97), recall (89.80), F1 (87.85) |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| [[129](#bib.bib129)] | CMP 外立面数据集 | [https://cmp.felk.cvut.cz/tylecr1/facade/](https://cmp.felk.cvut.cz/tylecr1/facade/)
    | 精确度, 召回率, F1, 误报, 漏报 | 精确度 (85.97), 召回率 (89.80), F1 (87.85) |'
- en: '| [[141](#bib.bib141)] | Santa Monica point cloud data, KITTI 2D, VLP-16 |
    [http://www.cvlibs.net/datasets/kitti/](http://www.cvlibs.net/datasets/kitti/)
    | F1, precision, recall | - |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| [[141](#bib.bib141)] | Santa Monica 点云数据, KITTI 2D, VLP-16 | [http://www.cvlibs.net/datasets/kitti/](http://www.cvlibs.net/datasets/kitti/)
    | F1, 精确度, 召回率 | - |'
- en: '| [[143](#bib.bib143)] | PointDA-10 | - | Accuracy | 49.70 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| [[143](#bib.bib143)] | PointDA-10 | - | 准确率 | 49.70 |'
- en: '| [[145](#bib.bib145)] | SHREC2018 | - | precision, recall, NN, T1, T2, EM,
    DCG | - |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| [[145](#bib.bib145)] | SHREC2018 | - | 精确度, 召回率, 最近邻 (NN), T1, T2, EM, DCG
    | - |'
- en: '| [[146](#bib.bib146)] | EndoSLAM dataset | [https://github.com/CapsuleEndoscope/EndoSLAM](https://github.com/CapsuleEndoscope/EndoSLAM)
    | RMSE | - |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| [[146](#bib.bib146)] | EndoSLAM 数据集 | [https://github.com/CapsuleEndoscope/EndoSLAM](https://github.com/CapsuleEndoscope/EndoSLAM)
    | 均方根误差 (RMSE) | - |'
- en: '| [[131](#bib.bib131)] | [3DPC](#id1.1.id1) tunnel dataset | - | Average precision
    (mPrec), average recall (mRec) | mPrec (71.2) |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| [[131](#bib.bib131)] | [3DPC](#id1.1.id1) 隧道数据集 | - | 平均精度 (mPrec), 平均召回率
    (mRec) | mPrec (71.2) |'
- en: '| [[151](#bib.bib151)] | 2014 scans | - | MAPE, RMSE | MAPE (0.416), RMSE (0.112)
    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| [[151](#bib.bib151)] | 2014 扫描数据 | - | 平均绝对百分比误差 (MAPE), 均方根误差 (RMSE) | MAPE
    (0.416), RMSE (0.112) |'
- en: '| [[152](#bib.bib152)] | NYU | - | RMSE, Accuracy | (81.8), |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| [[152](#bib.bib152)] | NYU | - | 均方根误差 (RMSE), 准确率 | (81.8) |'
- en: '| [[132](#bib.bib132)] | ShapeNetPart | [https://shapenet.org/](https://shapenet.org/)
    | OA, mIoU, Cat. mIoU, Ins. mIoU | OA (94.5) |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bib132)] | ShapeNetPart | [https://shapenet.org/](https://shapenet.org/)
    | OA, mIoU, 类别 mIoU, 实例 mIoU | OA (94.5) |'
- en: 3 Overview of DTL
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 DTL 概述
- en: Arguably one of the top success stories of [DL](#id4.4.id4) is [DTL](#id6.6.id6).
    Many applications in language and vision have benefited from the discovery that
    pretraining a network on a rich source set (e.g., ImageNet) can help boost performance
    once fine-tuned on a typically much smaller target set. Yet, very little is known
    about its usefulness in [3DPC](#id1.1.id1) understanding. We see this as an opportunity
    considering the effort required for annotating data in 3D.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，[DL](#id4.4.id4)的顶尖成功案例之一是[DTL](#id6.6.id6)。语言和视觉领域的许多应用受益于这样的发现：在丰富的源数据集（例如ImageNet）上对网络进行预训练，可以在微调到通常要小得多的目标数据集后提升性能。然而，对于其在[3DPC](#id1.1.id1)理解中的有用性仍知之甚少。考虑到在3D中注释数据所需的努力，我们视此为一个机会。
- en: 3.1 Domain adaptation
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 领域适应
- en: \Acf
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: \Acf
- en: DA has shown significant improvements in various [ML](#id5.5.id5) and [CV](#id3.3.id3)
    tasks, such as classification, detection, and segmentation. However, there are
    limited methods that have achieved [DA](#id7.7.id7) directly on [3DPC](#id1.1.id1)
    data, to the best of our knowledge. The challenge of point cloud data lies in
    its rich spatial geometric information, where the semantics of the entire object
    are contributed by regional geometric structures. Most general-purpose DA methods
    that focus on global feature alignment and disregard local geometric information
    may not be suitable for 3D domain alignment. Wu and their colleagues [[153](#bib.bib153)]
    discusse the challenges of generating and annotating large amounts of real-world
    data for [DL](#id4.4.id4)-based approaches in robotics [CV](#id3.3.id3) tasks.
    To overcome this, the authors propose using simulation-to-reality (sim2real) [DTL](#id6.6.id6)
    for point cloud data in an industrial application case. They provide insights
    on generating and processing synthetic point cloud data to improve model performance
    when transferred to real-world data. The issue of imbalanced learning is also
    investigated, and the authors propose a novel patch-based attention network as
    a strategy to address this problem. Another work in [[154](#bib.bib154)], presents
    a new method called [DTL](#id6.6.id6)-based sampling-attention network (TSANet)
    for semantic segmentation of 3D urban point clouds to facilitate the development
    of smart cities. The method includes a segmentation model with point downsampling–upsampling
    structure, embedding method, attention mechanism, and focal loss for feature processing
    and learning. The [DTL](#id6.6.id6) technique aims to reduce data requirements
    and labeling efforts by leveraging prior knowledge. The method is evaluated on
    a realistic point cloud dataset of Cambridge and Birmingham cities, demonstrating
    promising performance, surpassing other state-of-the-art models in terms of accuracy
    and mean [IoU](#id13.13.id13). [[116](#bib.bib116)] introduces [SSL](#id14.14.id14)
    for [DA](#id7.7.id7) in 3D perception problems, specifically on point clouds.
    It describes a new family of pretext tasks called deformation reconstruction,
    inspired by sim-to-real transformations, and proposes a novel training procedure
    called point cloud mixup (PCM) motivated by the MixUp method for labeled point
    cloud data. Evaluations on [DA](#id7.7.id7) datasets for classification and segmentation
    show significant improvement over existing and baseline methods, demonstrating
    the effectiveness of SSL-PCM-DA on point clouds. Similarly, [[155](#bib.bib155)]
    introduces a new model called SqueezeSegV2 for point cloud segmentation that is
    more robust to dropout noise in [LiDAR](#id18.18.id18) point clouds. The improved
    model structure, training loss, batch normalization, and additional input channel
    result in significant accuracy improvement when trained on real data. To overcome
    the challenge of limited labeled point-cloud data, the proposed scheme employs
    [DA](#id7.7.id7) training pipeline, consisting of learned intensity rendering,
    geodesic correlation alignment, and progressive domain calibration. When trained
    on real data, the new model exhibits significant segmentation accuracy improvements
    over the original SqueezeSeg. Moreover, when trained on synthetic data using the
    proposed [DA](#id7.7.id7) pipeline, the test accuracy on real-world data nearly
    doubles. A similar scheme for segmentation is proposed in [[156](#bib.bib156)],
    introducing LiDARNet, a boundary-aware [DA](#id7.7.id7) model tailored for semantic
    segmentation of [LiDAR](#id18.18.id18) point cloud data. The model uses a two-branch
    structure to extract domain private and shared features, and incorporates Gated-SCNN
    to learn boundary information during segmentation. The domain gap is further reduced
    by learning a mapping between domains using shared and private features. They
    also introduce a new dataset, SemanticUSL, for [DA](#id7.7.id7) in [LiDAR](#id18.18.id18)
    semantic segmentation, which has the same format and ontology as Semantic KITTI.
    Experiments on real-world datasets show that LiDARNet achieves comparable performance
    on the [SD](#id10.10.id10) and significant performance improvement (8-22% mIoU)
    on the [TDs](#id9.9.id9) after adaptation.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: DA在各种[机器学习](#id5.5.id5)和[计算机视觉](#id3.3.id3)任务中表现出显著的改进，例如分类、检测和分割。然而，据我们所知，直接在[3D点云](#id1.1.id1)数据上实现[DA](#id7.7.id7)的方法仍然有限。点云数据的挑战在于其丰富的空间几何信息，其中整个对象的语义由区域几何结构贡献。大多数关注全局特征对齐而忽略局部几何信息的通用DA方法可能不适用于3D领域对齐。Wu及其同事[[153](#bib.bib153)]讨论了在机器人[计算机视觉](#id3.3.id3)任务中生成和注释大量真实世界数据的挑战。为此，作者建议在工业应用案例中使用仿真到现实（sim2real）[深度迁移学习](#id6.6.id6)来处理点云数据。他们提供了生成和处理合成点云数据的见解，以提高模型在转移到真实世界数据时的性能。同时，作者还研究了不平衡学习问题，并提出了一种新颖的基于补丁的注意力网络作为解决该问题的策略。另一项工作[[154](#bib.bib154)]提出了一种新的方法——基于[深度迁移学习](#id6.6.id6)的采样-注意力网络（TSANet），用于3D城市点云的语义分割，以促进智能城市的发展。该方法包括具有点下采样–上采样结构的分割模型、嵌入方法、注意力机制和用于特征处理和学习的焦点损失。[深度迁移学习](#id6.6.id6)技术旨在通过利用先验知识来减少数据需求和标注工作。该方法在剑桥和伯明翰城市的现实点云数据集上进行评估，表现出良好的性能，在准确性和平均[IoU](#id13.13.id13)方面超越了其他最先进的模型。[[116](#bib.bib116)]引入了针对3D感知问题，特别是点云的[自监督学习](#id14.14.id14)用于[DA](#id7.7.id7)。它描述了一种新的前置任务家族，称为变形重建，灵感来自仿真到现实的转换，并提出了一种新的训练过程，称为点云混合（PCM），受MixUp方法的启发，用于标注点云数据。对分类和分割的[DA](#id7.7.id7)数据集的评估显示，相较于现有和基线方法有显著改善，展示了SSL-PCM-DA在点云上的有效性。同样，[[155](#bib.bib155)]引入了一种新的模型，称为SqueezeSegV2，用于点云分割，对[激光雷达](#id18.18.id18)点云中的丢失噪声更具鲁棒性。改进的模型结构、训练损失、批归一化和额外的输入通道在真实数据上训练时显著提高了准确性。为克服标注点云数据有限的问题，提出的方案采用了[DA](#id7.7.id7)训练流程，包括学习强度渲染、测地相关对齐和渐进域校准。在真实数据上训练时，新模型在分割准确性上显著超过了原始SqueezeSeg。此外，在使用所提出的[DA](#id7.7.id7)流程训练合成数据时，真实世界数据的测试准确性几乎翻倍。[[156](#bib.bib156)]提出了一种用于[激光雷达](#id18.18.id18)点云数据的边界感知[DA](#id7.7.id7)模型，称为LiDARNet，专为语义分割而设计。该模型使用双分支结构提取领域私有和共享特征，并结合Gated-SCNN在分割过程中学习边界信息。通过使用共享和私有特征学习领域之间的映射，进一步缩小了领域差距。他们还引入了一个新的数据集，SemanticUSL，用于[激光雷达](#id18.18.id18)语义分割的[DA](#id7.7.id7)，其格式和本体与Semantic
    KITTI相同。在真实世界数据集上的实验表明，LiDARNet在[SD](#id10.10.id10)上表现相当，在[TDs](#id9.9.id9)上经过适应后有显著的性能提升（8-22%
    mIoU）。
- en: 'Other researchers have advanced unsupervised [DA](#id7.7.id7) approaches, demonstrated
    by the method outlined in [[157](#bib.bib157)], aimed at enhancing semantic labeling
    accuracy for [3DPC](#id1.1.id1) in autonomous driving scenarios. The proposed
    approach uses a complete and label approach, leveraging a [sparse voxel completion
    network](#id15.15.id15) ([SVCN](#id15.15.id15)) to recover underlying surfaces
    of sparse point clouds and transfer semantic labels across different [LiDAR](#id18.18.id18)
    sensors. The approach does not require manual labeling for training pairs and
    introduces local adversarial learning to the model surface prior. Experimental
    results on a new benchmark dataset show significant performance improvements ranging
    from 8.2% to 36.6% compared to previous [DA](#id7.7.id7) methods. Likewise, [[158](#bib.bib158)]
    introduces PointDAN, a 3D [DA](#id7.7.id7) network tailored for point cloud data,
    aligning global and local features at multiple levels to achieve domain alignment.
    It introduces a Self-adaptive node module for local alignment, which models discriminative
    local structures, and a node-attention module for hierarchical feature representation.
    For global alignment, an adversarial-training strategy is employed. PointDAN outperforms
    state-of-the-art [DA](#id7.7.id7) methods in terms of adapting [3DPC](#id1.1.id1)
    data across domains, as demonstrated on the benchmark dataset PointDA-10, created
    by the authors. Zhao et al. [[159](#bib.bib159)] proposes an end-to-end framework
    called ePointDA for simulation-to-real [DA](#id7.7.id7) (SRDA) in [LiDAR](#id18.18.id18)
    point cloud segmentation. ePointDA consists of three modules: self-supervised
    dropout noise rendering, statistics-invariant and spatially-adaptive feature alignment,
    and transferable segmentation learning. The framework bridges the domain shift
    at pixel-level and feature-level, without requiring real-world statistics. Experimental
    results on adapting from synthetic to real datasets demonstrate the superiority
    of ePointDA in [LiDAR](#id18.18.id18) point cloud segmentation. Xu and their colleagues
    [[160](#bib.bib160)] propose semantic point generation to enhance the reliability
    of LiDAR-based object detectors against domain shifts in autonomous driving. The
    scheme generates semantic points to recover missing parts of foreground objects
    caused by occlusions, low reflectance, or weather interference. By merging the
    semantic points with the original points, an augmented point cloud is obtained,
    which significantly improves the performance of modern LiDAR-based detectors in
    unsupervised [DA](#id7.7.id7) tasks. The method also benefits object detection
    in the original domain, surpassing KITTI when combined with PV-RCNN.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究人员提出了无监督的[DA](#id7.7.id7)方法，如[[157](#bib.bib157)]中概述的方法，旨在提高自动驾驶场景中[3DPC](#id1.1.id1)的语义标注准确性。该方法使用了完整和标注的方法，通过[稀疏体素补全网络](#id15.15.id15)（[SVCN](#id15.15.id15)）来恢复稀疏点云的底层表面，并将语义标签转移到不同的[LiDAR](#id18.18.id18)传感器上。该方法不需要人工标注训练对，并引入了局部对抗学习来优化模型表面先验。实验结果表明，在新的基准数据集上，该方法相比于之前的[DA](#id7.7.id7)方法，性能显著提高，提升幅度在8.2%到36.6%之间。同样，[[158](#bib.bib158)]提出了PointDAN，一个针对点云数据的3D[DA](#id7.7.id7)网络，通过在多个层次上对齐全局和局部特征来实现领域对齐。它引入了一个自适应节点模块用于局部对齐，建模出具有区分性的局部结构，以及一个节点注意模块用于分层特征表示。在全局对齐方面，采用了对抗训练策略。PointDAN在将[3DPC](#id1.1.id1)数据适配到不同领域方面超越了最先进的[DA](#id7.7.id7)方法，这在作者创建的基准数据集PointDA-10上得到了证明。赵等人[[159](#bib.bib159)]提出了一个名为ePointDA的端到端框架，用于[LiDAR](#id18.18.id18)点云分割中的仿真到现实[DA](#id7.7.id7)（SRDA）。ePointDA包含三个模块：自监督丢弃噪声渲染、统计不变和空间自适应特征对齐，以及可迁移的分割学习。该框架在像素级和特征级桥接领域迁移，无需真实世界统计。实验结果表明，在从合成数据集到真实数据集的适配中，ePointDA在[LiDAR](#id18.18.id18)点云分割中具有明显优势。徐及其同事[[160](#bib.bib160)]提出了语义点生成方案，以增强基于LiDAR的物体检测器在自动驾驶中的域迁移可靠性。该方案生成语义点以恢复因遮挡、低反射或天气干扰导致的前景物体的缺失部分。通过将语义点与原始点融合，获得了增强的点云，从而显著提升了现代基于LiDAR的检测器在无监督[DA](#id7.7.id7)任务中的表现。该方法在原始领域的物体检测中也有所裨益，当与PV-RCNN结合使用时，超越了KITTI。
- en: Lang et al. [[161](#bib.bib161)] utilize PointNets to represent point clouds
    organized in vertical columns. The scheme, called PointPillars, achieves superior
    speed and accuracy, surpassing KITTI benchmarks while running at a significantly
    higher frame rate of 62 Hz. A faster version achieves state-of-the-art performance
    at 105 Hz, making it a suitable encoding approach for object detection in point
    clouds in robotics applications like autonomous driving. For object detection
    from [3DPC](#id1.1.id1), the authors [[162](#bib.bib162)] propose a Semi-Supervised
    [DA](#id7.7.id7) method for 3D object detection that leverages a small amount
    of labeled target data to improve adaptation performance. The method consists
    of an inter-[DA](#id7.7.id7) stage, which uses a Point-CutMix module to align
    point cloud distribution across domains, and an intra-domain generalization stage,
    which employs intra-domain Point-MixUp in semi-supervised learning to enhance
    model generalization on the unlabeled target set. Experimental results show that
    the proposed scheme, with only 10% labeled target data, outperforms a fully-supervised
    oracle model with 100% target labels on the Waymo to nuScenes domain shift. However,
    Du et al. [[163](#bib.bib163)] tackle the challenge of enhancing feature representation
    robustness in [3DPC](#id1.1.id1) by employing the [DA](#id7.7.id7) approach. The
    severe spatial occlusion and point density variance in point cloud data make designing
    robust features crucial. The proposed approach bridges the gap between the perceptual
    domain (real scene) and the conceptual domain (augmented scene with non-occluded
    point clouds), mimicking the functionality of human perception. Experimental results
    show that this simple yet effective approach significantly improves the performance
    of [3DPC](#id1.1.id1) object detection, achieving state-of-the-art results.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Lang等人[[161](#bib.bib161)] 利用PointNets来表示以垂直列组织的点云。这种方案被称为PointPillars，具有优越的速度和准确性，超越了KITTI基准，同时以显著更高的帧率62
    Hz运行。一个更快的版本在105 Hz下实现了最先进的性能，使其成为机器人应用中如自动驾驶的点云物体检测的合适编码方法。对于[3DPC](#id1.1.id1)的物体检测，作者[[162](#bib.bib162)]
    提出了一种半监督[DA](#id7.7.id7)方法，用于3D物体检测，该方法利用少量标注的目标数据来提高适应性能。该方法包括一个跨[DA](#id7.7.id7)阶段，使用Point-CutMix模块对齐跨领域的点云分布，以及一个领域内泛化阶段，在半监督学习中采用领域内的Point-MixUp以增强模型在未标记目标集上的泛化能力。实验结果表明，该方案仅使用10%的标注目标数据，在Waymo到nuScenes领域转移上超越了具有100%目标标签的全监督oracle模型。然而，Du等人[[163](#bib.bib163)]
    通过采用[DA](#id7.7.id7)方法来解决提升[3DPC](#id1.1.id1)中特征表示鲁棒性的挑战。点云数据中的严重空间遮挡和点密度变化使得设计鲁棒特征至关重要。所提出的方法弥合了感知领域（真实场景）和概念领域（带有非遮挡点云的增强场景）之间的差距，模拟了人类感知的功能。实验结果表明，这种简单而有效的方法显著提高了[3DPC](#id1.1.id1)物体检测的性能，达到了最先进的结果。
- en: 3.2 Fine-tuning
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 微调
- en: Fine-tuning stands as a ubiquitous [DTL](#id6.6.id6) approach, aiding pretrained
    models to adapt to new tasks through iterative training, often supplemented with
    additional layers known as the target model. [[164](#bib.bib164)]. The recent
    research studies, such as [[165](#bib.bib165), [166](#bib.bib166), [167](#bib.bib167)],
    have argued that even if the target task and source task are different, transferring
    features via [DTL](#id6.6.id6) approaches outperforms random features selections.
    This characteristic has led to the great success of [DTL](#id6.6.id6) in different
    spheres of real-life applications such as medical imaging [[168](#bib.bib168),
    [4](#bib.bib4)], recognition tasks like speech recognition [[26](#bib.bib26)],
    hand-gesture [[169](#bib.bib169)], face recognition [[170](#bib.bib170)], and
    rcently, in the processing of [3DPC](#id1.1.id1) data [[130](#bib.bib130), [171](#bib.bib171)].
    One of the major concerns raised in fine-tuning a pretrained model is to identify
    which layer to fine-tune. The authors in [[172](#bib.bib172)] have explicitly
    added regularization terms to the loss function for obtaining the parameters of
    the fine-tuned model close to the original pretrained model. AdaFilter [[164](#bib.bib164)],
    is an adaptive fine-tuning approach, which considers criterion for optimization
    by selecting only a part of the convolutional filters in the pretrained model.
    Furthermore, they have exploited a recurrent gated network and considered activation
    of the previous layer for carefully fine-tuning the desired convolutional filters
    only. The adaptive fine-tuning scheme considers the similarity between the source
    and target tasks and datasets to reuse more pretrained filters.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 微调作为一种普遍存在的[DTL](#id6.6.id6)方法，帮助预训练模型通过迭代训练适应新任务，通常还会附加一些额外的层，称为目标模型。[[164](#bib.bib164)]。近期的研究，如[[165](#bib.bib165)、[166](#bib.bib166)、[167](#bib.bib167)]，已经提出，即使目标任务和源任务不同，通过[DTL](#id6.6.id6)方法转移特征也优于随机特征选择。这一特性使得[DTL](#id6.6.id6)在医学影像[[168](#bib.bib168)、[4](#bib.bib4)]、语音识别[[26](#bib.bib26)]、手势识别[[169](#bib.bib169)]、人脸识别[[170](#bib.bib170)]等实际应用领域取得了巨大成功，最近还在[3DPC](#id1.1.id1)数据处理[[130](#bib.bib130)、[171](#bib.bib171)]中取得了良好的效果。微调预训练模型时一个主要的关注点是识别需要微调的层。[[172](#bib.bib172)]中的作者明确地在损失函数中添加了正则化项，以使得微调模型的参数接近于原始的预训练模型。AdaFilter[[164](#bib.bib164)]是一种自适应微调方法，它通过仅选择预训练模型中的部分卷积滤波器来考虑优化标准。此外，他们还利用了递归门控网络，并考虑了前一层的激活状态，以仅对所需的卷积滤波器进行细致的微调。自适应微调方案考虑了源任务和目标任务及数据集之间的相似性，以便重用更多的预训练滤波器。
- en: 'Many works that involve processing [3DPC](#id1.1.id1) data have exploited [DTL](#id6.6.id6)
    for many tasks, include 3D reconstructions of scaffolds [[140](#bib.bib140)],
    [3DPC](#id1.1.id1) instance segmentation [[131](#bib.bib131)], classification
    of soft-story buildings using [3DPC](#id1.1.id1) data [[141](#bib.bib141)], [3DPC](#id1.1.id1)
    understanding [[117](#bib.bib117)], 3D orientation recognition [[149](#bib.bib149)],
    [SSL](#id14.14.id14) on [3DPCs](#id1.1.id1)  [[148](#bib.bib148)], and remaining
    useful life prediction [[151](#bib.bib151)], among others. Furthermore, some fine-tuning
    strategies involve all the pretrained parameters, whereas others fine-tuning the
    last few layers [[164](#bib.bib164)], as illusted in Fig. [7](#S2.F7 "Figure 7
    ‣ 2.3.1 Inductive DTL ‣ 2.3 Theoretical taxonomy of DTL ‣ 2 Background ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")
    (a). G Diraco et al. [[151](#bib.bib151)] have performed an extensive experiment
    for a particular scenario in which the amount of data in the [TD](#id9.9.id9)
    is small and explored how it behaves if only part of the network is fine-tuned
    on the given target dataset. They have updated the decoder of their model, which
    has been trained on [SDs](#id10.10.id10) and evaluated on the target dataset.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '许多涉及处理[3DPC](#id1.1.id1)数据的工作已经利用[DTL](#id6.6.id6)完成了多项任务，包括支架的3D重建[[140](#bib.bib140)]、[3DPC](#id1.1.id1)实例分割[[131](#bib.bib131)]、使用[3DPC](#id1.1.id1)数据对软层建筑进行分类[[141](#bib.bib141)]、[3DPC](#id1.1.id1)理解[[117](#bib.bib117)]、3D方向识别[[149](#bib.bib149)]、对[3DPCs](#id1.1.id1)进行[SSL](#id14.14.id14)
    [[148](#bib.bib148)]以及剩余使用寿命预测[[151](#bib.bib151)]等。此外，一些微调策略涉及所有预训练参数，而其他策略则只微调最后几层[[164](#bib.bib164)]，如图[7](#S2.F7
    "Figure 7 ‣ 2.3.1 Inductive DTL ‣ 2.3 Theoretical taxonomy of DTL ‣ 2 Background
    ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey") (a)所示。G Diraco等人[[151](#bib.bib151)]针对[TD](#id9.9.id9)数据量较小的特定场景进行了广泛实验，并探索了仅微调网络部分对给定目标数据集的影响。他们更新了在[SDs](#id10.10.id10)上训练并在目标数据集上评估的模型的解码器。'
- en: One factor contributing to the success of [DTL](#id6.6.id6) is its ability to
    perform well with smaller datasets. In support of this, the authors [[141](#bib.bib141)]
    have employed [DTL](#id6.6.id6) to address the challenge of training large parameters
    in deep convolutional networks. Their experimental results demonstrate the transferability
    of [DTL](#id6.6.id6) from [SDs](#id10.10.id10) to [TD](#id9.9.id9), as the dataset
    under consideration lacked sufficient data to train large parameters. Additionally,
    compared to traditional training methods, [DTL](#id6.6.id6) consumes less time,
    making it a more efficient approach. To support this claim, the authors noted
    that models such as VGGNet, Inception and ResNet, which were trained using fine-tuning
    techniques, exhibited shorter training times by triggering the early stopping
    mechanism. This improvement in training time has enabled the workflow to quickly
    identify soft-story buildings on a city scale. Furthermore, the authors have shown
    that [DTL](#id6.6.id6) can effectively reduce the risk of overfitting that can
    occur with [DL](#id4.4.id4) techniques. By leveraging a pretrained network, [DTL](#id6.6.id6)
    can transfer knowledge and prevent the model from memorizing the training data,
    leading to more robust and generalizable performance. Moving on, Xiu et al. [[173](#bib.bib173)]
    propose using airborne [LiDAR](#id18.18.id18) to detect collapsed buildings during
    earthquake emergency response, as [DTL](#id6.6.id6)-based damage detection with
    aerial images has limitations in detecting collapsed buildings with undamaged
    roofs. The authors develop a [3DPC](#id1.1.id1)-based dataset for building damage
    detection and propose a general extension framework and a visual explanation method
    to validate model decisions. The results conducted using PointNet [[52](#bib.bib52)],
    PointNet++ [[53](#bib.bib53)] and DGCNN [[174](#bib.bib174)] show that [3DPC](#id1.1.id1)-based
    methods can achieve high accuracy and are robust even with reduced training data.
    The model also achieves moderate accuracy on another dataset with different architectural
    styles without additional training.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 影响[DTL](#id6.6.id6)成功的一个因素是它在处理较小数据集时的出色表现。为此，作者[[141](#bib.bib141)]使用了[DTL](#id6.6.id6)来应对在深度卷积网络中训练大参数的挑战。他们的实验结果展示了[DTL](#id6.6.id6)从[SDs](#id10.10.id10)到[TD](#id9.9.id9)的迁移能力，因为所考虑的数据集缺乏足够的数据来训练大参数。此外，与传统训练方法相比，[DTL](#id6.6.id6)消耗的时间更少，使其成为一种更高效的方法。为了支持这一主张，作者指出，使用微调技术训练的模型，如VGGNet、Inception和ResNet，通过触发早期停止机制，表现出了更短的训练时间。这一训练时间的改进使得工作流程能够在城市范围内快速识别软层建筑。此外，作者还表明，[DTL](#id6.6.id6)可以有效减少[DL](#id4.4.id4)技术可能出现的过拟合风险。通过利用预训练网络，[DTL](#id6.6.id6)能够转移知识并防止模型记忆训练数据，从而实现更稳健和更具泛化能力的性能。接下来，Xiu等人[[173](#bib.bib173)]建议使用空中[LiDAR](#id18.18.id18)来检测地震紧急响应期间的倒塌建筑，因为基于[DTL](#id6.6.id6)的损坏检测在使用航拍图像检测没有损坏屋顶的倒塌建筑方面存在局限性。作者开发了一个基于[3DPC](#id1.1.id1)的数据集用于建筑损坏检测，并提出了一个通用的扩展框架和可视化解释方法来验证模型决策。通过使用PointNet[[52](#bib.bib52)]、PointNet++[[53](#bib.bib53)]和DGCNN[[174](#bib.bib174)]进行的实验结果表明，基于[3DPC](#id1.1.id1)的方法可以实现高精度，即使在减少训练数据的情况下也具有鲁棒性。该模型在另一个具有不同建筑风格的数据集上也取得了适度的准确性，无需额外训练。
- en: 3.3 Unsupervised DTL
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 无监督的DTL
- en: 'Unsupervised [DTL](#id6.6.id6) is mainly based on [unsupervised domain adaptation](#id8.8.id8)
    ([UDA](#id8.8.id8)) to remove the need or addressing the issue of lack for labeled
    data and allows any image to be used as a datapoint for any [CV](#id3.3.id3) tasks
    [[175](#bib.bib175), [176](#bib.bib176)]. Numerous methods have been proposed
    for performing [UDA](#id8.8.id8) on 2D images, which can be divided into two main
    categories: methods based on domain-invariant feature learning and methods for
    learning domain mapping. The former [[177](#bib.bib177), [178](#bib.bib178), [179](#bib.bib179),
    [180](#bib.bib180), [181](#bib.bib181)] aim to minimize the discrepancy between
    two distributions in the feature space, while the latter [[182](#bib.bib182),
    [183](#bib.bib183), [184](#bib.bib184)] use neural networks, such as CycleGAN
    [[53](#bib.bib53)], to directly learn the translation from the [SD](#id10.10.id10)
    to the [TDs](#id9.9.id9). In [[185](#bib.bib185)], 2D translation is extended
    to depth images using a differential contrastive learning strategy for preserving
    underlying geometries. Despite their differences, these methods widely exploit
    domain adversarial training. Additionally, several useful techniques, such as
    pseudo-labeling [[186](#bib.bib186)] and batch normalization tailored for [DA](#id7.7.id7)  [[187](#bib.bib187)],
    have also been proposed.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督[DTL](#id6.6.id6)主要基于[无监督领域适应](#id8.8.id8)（[UDA](#id8.8.id8)），以消除对标记数据的需求或解决标记数据缺乏的问题，并允许任何图像作为任何[CV](#id3.3.id3)任务的数据点[[175](#bib.bib175),
    [176](#bib.bib176)]。已经提出了多种方法来执行2D图像上的[UDA](#id8.8.id8)，这些方法可以分为两大类：基于领域不变特征学习的方法和学习领域映射的方法。前者[[177](#bib.bib177),
    [178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180), [181](#bib.bib181)]旨在最小化特征空间中两个分布之间的差异，而后者[[182](#bib.bib182),
    [183](#bib.bib183), [184](#bib.bib184)]使用神经网络，如CycleGAN [[53](#bib.bib53)]，直接学习从[SD](#id10.10.id10)到[TDs](#id9.9.id9)的转换。在[[185](#bib.bib185)]中，2D翻译扩展到深度图像，使用差分对比学习策略来保持基础几何。尽管这些方法有所不同，但它们广泛利用了领域对抗训练。此外，还提出了几种有用的技术，如伪标签
    [[186](#bib.bib186)] 和针对[DA](#id7.7.id7)的批量归一化[[187](#bib.bib187)]。
- en: Although there have been significant efforts made in [UDA](#id8.8.id8) for 2D
    images and depth, [UDA](#id8.8.id8) on [3DPC](#id1.1.id1) is still in its early
    stages. [UDA](#id8.8.id8) on point clouds involves extending domain adversarial
    training from 2D images to [3DPC](#id1.1.id1) to align features on both local
    and global levels [[188](#bib.bib188)]. Nevertheless, adversarial methods on [3DPC](#id1.1.id1)
    struggle to balance local geometry alignment and global semantic alignment. CycleGAN
    [[189](#bib.bib189)] is utilized by both [[159](#bib.bib159)] and [[190](#bib.bib190)]
    to generate more realistic [LiDAR](#id18.18.id18) point clouds from synthetic
    data. This Sim2Real approach is used to minimize feature distances between the
    [SD](#id10.10.id10) and [TD](#id9.9.id9). The work in [[157](#bib.bib157)], on
    the other hand, leverages segmentation on completed surface reconstructed from
    sparse point cloud for better adaptation.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在[UDA](#id8.8.id8)中已经对2D图像和深度进行了重大努力，但对[3DPC](#id1.1.id1)的[UDA](#id8.8.id8)仍处于初期阶段。[UDA](#id8.8.id8)对点云的研究涉及将领域对抗训练从2D图像扩展到[3DPC](#id1.1.id1)，以在局部和全局层面上对齐特征[[188](#bib.bib188)]。然而，针对[3DPC](#id1.1.id1)的对抗方法在平衡局部几何对齐和全局语义对齐方面仍面临挑战。CycleGAN
    [[189](#bib.bib189)]被[[159](#bib.bib159)]和[[190](#bib.bib190)]使用，以从合成数据生成更逼真的[LiDAR](#id18.18.id18)点云。这种Sim2Real方法用于最小化[SD](#id10.10.id10)和[TD](#id9.9.id9)之间的特征距离。另一方面，[[157](#bib.bib157)]的工作利用从稀疏点云重建的完成表面上的分割，以实现更好的适应。
- en: For object-level tasks, [[191](#bib.bib191), [158](#bib.bib158)] align global
    and local features, while [[155](#bib.bib155)] and [[190](#bib.bib190)] project
    point clouds to 2D and birds-eye view, respectively, to reduce sparsity. [[163](#bib.bib163)]
    creates a car model set and adapts their features for detection object features,
    but only targets general car 3D detection on a single point cloud domain. Recently,
    [[192](#bib.bib192)] published the first study targeting [UDA](#id8.8.id8) for
    3D [LiDAR](#id18.18.id18) detection. They identify the vehicle size as the domain
    gap between KITTI [[193](#bib.bib193)] and other datasets and resize the vehicles
    in the data. In contrast, [[160](#bib.bib160)] identifies point cloud quality
    as the major domain gap between Waymo’s two datasets [[194](#bib.bib194)] and
    proposes a learning-based approach to close the gap.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对象级任务，[[191](#bib.bib191), [158](#bib.bib158)] 对齐全局和局部特征，而[[155](#bib.bib155)]和[[190](#bib.bib190)]分别将点云投影到2D和鸟瞰图中，以减少稀疏性。[[163](#bib.bib163)]
    创建了一个汽车模型集，并调整其特征以检测对象特征，但仅针对单一点云领域的通用汽车3D检测。最近，[[192](#bib.bib192)] 发表了首个针对3D
    [LiDAR](#id18.18.id18)检测的[UDA](#id8.8.id8)研究。他们将车辆尺寸识别为KITTI [[193](#bib.bib193)]
    和其他数据集之间的领域差距，并在数据中调整车辆大小。相比之下，[[160](#bib.bib160)] 识别出点云质量是Waymo两个数据集[[194](#bib.bib194)]之间的主要领域差距，并提出了一种基于学习的方法来弥合这一差距。
- en: Recent works on [UDA](#id8.8.id8) on point clouds mainly focus on designing
    suitable self-supervised tasks on point clouds to facilitate learning domain invariant
    features, which are discussed in detail in the following subsection. In addition
    to [UDA](#id8.8.id8) on object point clouds, several methods have been proposed
    to address specific domain gaps on [LiDAR](#id18.18.id18) point clouds. These
    methods commonly address depth missing and sampling difference between sensors.
    ST3D [[195](#bib.bib195)] presents a task-specific self-training pipeline with
    curriculum data augmentation to further improve the adaptation process
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在[UDA](#id8.8.id8)点云上的研究主要集中在设计合适的自监督任务，以促进学习领域不变特征，这在以下小节中将详细讨论。除了[UDA](#id8.8.id8)在对象点云上的应用外，还提出了几种方法来解决[LiDAR](#id18.18.id18)点云上的特定领域差距。这些方法通常解决深度缺失和传感器之间的采样差异。ST3D
    [[195](#bib.bib195)] 提出了一个任务特定的自训练流程，通过课程数据增强进一步改进适应过程。
- en: Alternatively, most existing [UDA](#id8.8.id8) approaches focus on uni-modal
    data, despite the availability of multi-modal datasets. In [[196](#bib.bib196)],
    the authors propose a cross-modal [UDA](#id8.8.id8) (xMUDA) approach for 3D semantic
    segmentation, where both 2D images and [3DPC](#id1.1.id1) are utilized. This is
    challenging as the two input modalities are heterogeneous and can be affected
    differently by domain shift. In xMUDA, the modalities learn from each other through
    mutual mimicking, separate from the segmentation objective, to prevent the stronger
    modality from adopting false predictions from the weaker one. The proposed xMUDA
    approach is evaluated on various [UDA](#id8.8.id8) scenarios, such as day-to-night,
    country-to-country, and dataset-to-dataset shifts, using recent autonomous driving
    datasets. Results show that xMUDA significantly improves over uni-modal [UDA](#id8.8.id8)
    in all tested scenarios and complements state-of-the-art [UDA](#id8.8.id8) techniques.
    Saltori et al. [[197](#bib.bib197)] introduces a novel source-free [UDA](#id8.8.id8)
    (SF-UDA) framework for 3D object detection using [LiDAR](#id18.18.id18) point
    clouds, which does not require any annotations from the [SD](#id10.10.id10) or
    images/annotations from the [TD](#id9.9.id9). It addresses domain shift in [LiDAR](#id18.18.id18)
    data, which is not only due to changes in environment and object appearances,
    but also to geometry (e.g., point density variations). SF-UDA^(3D) utilizes pseudo-annotations,
    reversible scale-transformations, and motion coherency for [DA](#id7.7.id7). Experimental
    results on large-scale datasets, KITTI and nuScenes, show that SF-UDA^(3D) outperforms
    previous methods based on feature alignment and state-of-the-art 3D object detection
    methods that use few-shot target annotations or target annotation statistics.
    Also, [[198](#bib.bib198)] introduces RefRec, an approach that investigate pseudo-labels
    and self-training for [UDA](#id8.8.id8) in point cloud classification. Instead
    of relying on multi-task learning, RefRec proposes two innovations for effective
    self-training on 3D data. First, it refines noisy pseudo-labels by matching shape
    descriptors learned from the unsupervised task of shape reconstruction on both
    domains. Second, it proposes a novel self-training protocol that learns domain-specific
    decision boundaries and mitigates the negative impact of mislabelled target samples
    and in-domain intra-class variability. RefRec achieves state-of-the-art performance
    on standard benchmarks for [UDA](#id8.8.id8) in point cloud classification, demonstrating
    the effectiveness of self-training for this emerging research problem. Additionally,
    [[199](#bib.bib199)] focuses on [UDA](#id8.8.id8) in 3D [CV](#id3.3.id3) tasks,
    specifically point cloud visual tasks. The proposed approach introduces a dual-branch
    feature alignment network (DFAN) architecture that leverages the characteristics
    of local and global features in point clouds. The approach utilizes different
    strategies for feature extraction and alignment in each branch, complementing
    each other. Hierarchical alignment for local features and distribution alignment
    for global features are also introduced. Experimental results on benchmark datasets
    demonstrate that the proposed approach achieves state-of-the-art performance in
    point cloud classification and segmentation tasks.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，大多数现有的[UDA](#id8.8.id8)方法专注于单模态数据，尽管多模态数据集是可用的。在[[196](#bib.bib196)]中，作者提出了一种用于3D语义分割的跨模态[UDA](#id8.8.id8)（xMUDA）方法，其中同时利用了2D图像和[3DPC](#id1.1.id1)。这很具有挑战性，因为两个输入模态是异质的，可能会受到不同的领域偏移影响。在xMUDA中，这些模态通过相互模仿进行学习，独立于分割目标，以防止更强的模态从较弱的模态中采纳错误的预测。提出的xMUDA方法在各种[UDA](#id8.8.id8)场景下进行了评估，如昼夜、国家间以及数据集间的偏移，使用了最新的自动驾驶数据集。结果表明，xMUDA在所有测试场景中显著优于单模态[UDA](#id8.8.id8)，并且补充了最先进的[UDA](#id8.8.id8)技术。Saltori等人[[197](#bib.bib197)]介绍了一种新颖的无源[UDA](#id8.8.id8)（SF-UDA）框架，用于利用[LiDAR](#id18.18.id18)点云进行3D目标检测，该框架不需要[SD](#id10.10.id10)的任何标注或[TD](#id9.9.id9)的图像/标注。它解决了[LiDAR](#id18.18.id18)数据中的领域偏移，这不仅是由于环境和物体外观的变化，还包括几何变化（例如，点密度的变化）。SF-UDA^(3D)利用伪标注、可逆尺度变换和运动一致性进行[DA](#id7.7.id7)。在大规模数据集KITTI和nuScenes上的实验结果表明，SF-UDA^(3D)优于基于特征对齐的先前方法和使用少量目标标注或目标标注统计的最先进的3D目标检测方法。此外，[[198](#bib.bib198)]介绍了RefRec，一种研究伪标签和自我训练在点云分类中用于[UDA](#id8.8.id8)的方法。RefRec提出了两种创新来有效自我训练3D数据，而不是依赖多任务学习。首先，通过匹配从形状重建的无监督任务中学习的形状描述符来细化噪声伪标签。其次，提出了一种新颖的自我训练协议，该协议学习领域特定的决策边界，并减轻了错误标注目标样本和领域内同类变异的负面影响。RefRec在点云分类的标准基准测试中达到了最先进的性能，展示了自我训练在这一新兴研究问题上的有效性。此外，[[199](#bib.bib199)]专注于3D[CV](#id3.3.id3)任务中的[UDA](#id8.8.id8)，特别是点云视觉任务。所提出的方法引入了一个双分支特征对齐网络（DFAN）架构，利用了点云中局部和全局特征的特性。该方法在每个分支中采用不同的特征提取和对齐策略，相互补充。还引入了局部特征的分层对齐和全局特征的分布对齐。在基准数据集上的实验结果表明，所提出的方法在点云分类和分割任务中达到了最先进的性能。
- en: Previous studies on unsupervised 3D learning have principally concentrated on
    ShapeNet [[147](#bib.bib147)], which is a repository of single-object computer
    aided design (CAD) models. Typically, the idea is to use ShapeNet as the ImageNet
    counterpart in 3D so that the characteristics learned on single synthetic objects
    can be transferred to other real-world applications. In this regard, numerous
    works have then been proposed. For instance, [[200](#bib.bib200)] introduces a
    point-level DA by searched transformations. Typically, transformations of [3DPCs](#id1.1.id1)
    are learned via the search of the best combination of operations on [3DPCs](#id1.1.id1),
    which transfers data from the [SD](#id10.10.id10) to the [TD](#id9.9.id9) while
    keeping the [TD](#id9.9.id9) unlabeled. Besides, most [3DPC](#id1.1.id1)-based
    [UDA](#id8.8.id8) techniques focus on extracting domain-invariant features in
    different domains for feature alignment. In this regard, [UDA](#id8.8.id8) is
    advocated for the detection of 3D objects in the context of semantic point generation,
    as proposed in [[160](#bib.bib160)].
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 以往对无监督3D学习的研究主要集中在ShapeNet[[147](#bib.bib147)]上，这是一个单对象计算机辅助设计（CAD）模型的存储库。通常，目的是将ShapeNet用作3D领域中的ImageNet，以便将单一合成对象上学到的特征转移到其他现实世界的应用中。在这方面，提出了许多工作。例如，[[200](#bib.bib200)]通过搜索变换介绍了点级DA。通常，通过搜索对[3DPCs](#id1.1.id1)的最佳操作组合来学习[3DPCs](#id1.1.id1)的变换，从而在保持[TD](#id9.9.id9)未标记的情况下将数据从[SD](#id10.10.id10)转移到[TD](#id9.9.id9)。此外，大多数基于[3DPC](#id1.1.id1)的[UDA](#id8.8.id8)技术集中在提取不同领域中的领域不变特征以进行特征对齐。在这方面，[UDA](#id8.8.id8)被提倡用于语义点生成中的3D对象检测，如[[160](#bib.bib160)]中提出的。
- en: 'Moving on, in [[157](#bib.bib157)], a [UDA](#id8.8.id8) problem approach for
    the semantic labeling of [3DPCs](#id1.1.id1) is proposed, focusing on domain gap
    reduction of [LiDAR](#id18.18.id18) sensors. Based on the observation that sparse
    [3DPCs](#id1.1.id1) are sampled from 3D surfaces, a [SVCN](#id15.15.id15) was
    designed to complete the 3D surfaces of a sparse [3DPC](#id1.1.id1). Unlike semantic
    labels, obtaining training pairs for [SVCN](#id15.15.id15) requires no manual
    labeling. A local adversarial learning to model the surface prior was also introduced.
    The recovered 3D surfaces serve as a canonical domain from which semantic labels
    can transfer across different [LiDAR](#id18.18.id18) sensors. Experiments and
    ablation studies with benchmark for cross-domain semantic labeling of [LiDAR](#id18.18.id18)
    data show that this approach provides 6.3-37.6% better performance than previous
    DA methods. Fig. [9](#S3.F9 "Figure 9 ‣ 3.3 Unsupervised DTL ‣ 3 Overview of DTL
    ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey") portrays the block diagram of the the [UDA](#id8.8.id8) scheme introduced
    in [[157](#bib.bib157)] for semantic segmentation of [LiDAR](#id18.18.id18)  [3DPCs](#id1.1.id1).'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，在[[157](#bib.bib157)]中，提出了一种用于[3DPCs](#id1.1.id1)语义标注的[UDA](#id8.8.id8)问题方法，重点关注[LiDAR](#id18.18.id18)传感器的领域间差距减少。基于观察到稀疏[3DPCs](#id1.1.id1)是从3D表面采样的，设计了一种[SVCN](#id15.15.id15)来完成稀疏[3DPC](#id1.1.id1)的3D表面。与语义标签不同，获得用于[SVCN](#id15.15.id15)的训练对无需人工标注。还引入了局部对抗学习以建模表面先验。恢复的3D表面作为一个标准域，从中可以将语义标签转移到不同的[LiDAR](#id18.18.id18)传感器。通过基准实验和消融研究，针对[LiDAR](#id18.18.id18)数据的跨域语义标注表明，该方法比以前的DA方法提供了6.3-37.6%的更好性能。图[9](#S3.F9
    "Figure 9 ‣ 3.3 Unsupervised DTL ‣ 3 Overview of DTL ‣ Advancing 3D Point Cloud
    Understanding through Deep Transfer Learning: A Comprehensive Survey")展示了[[157](#bib.bib157)]中提出的用于[LiDAR](#id18.18.id18)
    [3DPCs](#id1.1.id1)的[UDA](#id8.8.id8)方案的框图。'
- en: '![Refer to caption](img/79770984fc4459add9e9b5abff6f519b.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/79770984fc4459add9e9b5abff6f519b.png)'
- en: 'Figure 9: Flowchart of the [UDA](#id8.8.id8) approach proposed in [[157](#bib.bib157)]
    for semantic segmentation of [LiDAR](#id18.18.id18) [3DPCs](#id1.1.id1)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: [UDA](#id8.8.id8) 方法的流程图，该方法在[[157](#bib.bib157)]中提出，用于[LiDAR](#id18.18.id18)
    [3DPCs](#id1.1.id1)的语义分割。'
- en: 3.4 Semi-supervised DTL
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 半监督 DTL
- en: 'The integration of semi-supervised DTL into 3DPC understanding for tasks like
    object detection and segmentation is rapidly transforming the field, particularly
    in contexts where labeled data are scarce. This approach leverages both labeled
    and unlabeled data, enhancing learning efficiency and model performance across
    various applications, from autonomous driving to tunnel monitoring. Typically,
    Tang et al. [[201](#bib.bib201)] develope a semi-supervised 3D object detection
    model that utilizes a novel network to transfer knowledge from well-labeled object
    classes to weakly labeled classes, improving detection in classes with only 2D
    labels. Moving on, Ji et al. [[5](#bib.bib5)] propose the Semi-supervised Learning-based
    Point Cloud Network (SPCNet) which integrates various learning modules to enhance
    tunnel scene segmentation from 3DPCs, significantly reducing the reliance on extensive
    labeled datasets. Imad et al. [[130](#bib.bib130)]: Focused on utilizing 3D LiDAR
    data for autonomous driving perception, enhancing object detection through a semi-supervised
    learning framework that minimizes the need for large-scale annotated datasets.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '将半监督DTL集成到3DPC理解中，用于目标检测和分割等任务，正在迅速改变该领域，特别是在标记数据稀缺的情况下。这种方法利用标记和未标记数据，提高了学习效率和模型性能，适用于从自动驾驶到隧道监测的各种应用。通常，Tang
    等人 [[201](#bib.bib201)] 开发了一种半监督3D目标检测模型，该模型利用一种新颖的网络将知识从标记良好的目标类别转移到标记弱的类别，改善了仅有2D标签的类别检测。接着，Ji
    等人 [[5](#bib.bib5)] 提出了基于半监督学习的点云网络 (SPCNet)，该网络整合了多种学习模块，以增强3DPC的隧道场景分割，显著减少了对大规模标记数据集的依赖。Imad
    等人 [[130](#bib.bib130)]: 关注于利用3D LiDAR数据进行自动驾驶感知，通过半监督学习框架提升目标检测，减少了对大规模标注数据集的需求。'
- en: In this same direction, Huang et al. [[202](#bib.bib202)] present a point cloud
    registration framework that minimizes a feature-metric projection error without
    needing correspondences, using a semi-supervised approach. This method is particularly
    robust to noise and density differences in point clouds. Horache et al. [[203](#bib.bib203)]
    propose a method for generalizing deep learning for 3DPC registration on entirely
    different datasets using a combination of Multi-Scale Sparse Voxel Convolution
    and an unsupervised transfer algorithm, UDGE, enhancing adaptability across varied
    real-world datasets. Li et al. [[204](#bib.bib204)] introduce a semi-supervised
    point cloud segmentation method that utilizes both labeled and unlabeled data.
    By employing adversarial architecture for confidence discrimination of label predictions
    on unlabeled point clouds, their approach enhances segmentation performance.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同方向上，黄等人 [[202](#bib.bib202)] 提出了一个点云配准框架，该框架通过半监督方法最小化特征度量投影误差，而无需对应关系。该方法特别对点云中的噪声和密度差异具有很强的鲁棒性。Horache
    等人 [[203](#bib.bib203)] 提出了一种方法，用于将深度学习泛化到完全不同的数据集上的3DPC配准，结合了多尺度稀疏体素卷积和无监督迁移算法UDG，增强了对不同真实世界数据集的适应性。Li
    等人 [[204](#bib.bib204)] 引入了一种半监督点云分割方法，该方法利用标记和未标记的数据。通过采用对抗架构对未标记点云的标签预测进行置信度判别，他们的方法提高了分割性能。
- en: Besides, Chen et al. [[205](#bib.bib205)] propose a multimodal semi-supervised
    learning framework that utilizes instance-level consistency and a novel multimodal
    contrastive prototype loss to enforce consistent representations across different
    3D data modalities of the same object. Similarly, Xiao et al. [[206](#bib.bib206)]
    tackle the synthetic-to-real data gap in 3DPCs segmentation by developing a large-scale
    synthetic dataset and a novel translation method that decomposes and addresses
    the differences in appearance and sparsity between synthetic and real datasets.
    Moving on, Mei et al. [[207](#bib.bib207)] develop a system for the semantic segmentation
    of 3D LiDAR data in dynamic scenes, using a semi-supervised learning strategy
    that combines limited manual annotations with large amounts of constraint data
    to enhance scene adaptability. Additionally, Huang et al. [[208](#bib.bib208)]
    introduce a spatio-temporal representation learning framework for learning from
    unlabeled 3DPCs in a self-supervised manner, facilitating the generalization of
    pre-trained models to a variety of downstream tasks. Moreover, Qin et al. [[209](#bib.bib209)]
    propose a weakly supervised framework for 3D object detection that leverages unsupervised
    3D proposal generation and cross-modal knowledge distillation, reducing the dependency
    on annotated 3D bounding boxes.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，陈等人[[205](#bib.bib205)]提出了一种多模态半监督学习框架，该框架利用实例级一致性和新颖的多模态对比原型损失，以强制在同一物体的不同3D数据模态之间保持一致的表示。类似地，肖等人[[206](#bib.bib206)]通过开发大规模合成数据集和一种新颖的翻译方法来解决3DPCs分割中的合成到真实数据差距，该方法分解并解决了合成数据集和真实数据集之间的外观和稀疏性差异。接下来，梅等人[[207](#bib.bib207)]开发了一种用于动态场景中3D
    LiDAR数据语义分割的系统，使用结合有限手动注释和大量约束数据的半监督学习策略，以增强场景适应性。此外，黄等人[[208](#bib.bib208)]引入了一种空间-时间表示学习框架，用于以自监督的方式从未标记的3DPCs中学习，促进预训练模型对各种下游任务的泛化。此外，秦等人[[209](#bib.bib209)]提出了一种弱监督框架用于3D物体检测，该框架利用无监督3D提议生成和跨模态知识蒸馏，减少对标注3D边界框的依赖。
- en: 'On the other hand, Yu et al. [[210](#bib.bib210)] tackle the data scarcity
    challenge in 3D tasks by transferring knowledge from robust 2D models to augment
    RGB-D images with pseudo-labels, significantly improving the pre-training of 3D
    models with limited labeled data. Xu et al. [[211](#bib.bib211)] develop a hierarchical
    point-based active learning strategy for 3DPC segmentation, which measures uncertainty
    at multiple levels and selects important points for manual labeling, effectively
    utilizing limited annotations. Zhang et al. [[212](#bib.bib212)] study weakly
    semi-supervised 3D object detection with point annotations to generate high-quality
    pseudo-bounding boxes, enabling 3D detectors to perform comparably to fully-supervised
    models with substantially fewer labeled data. Lastly, Wang et al. [[213](#bib.bib213)]
    introduce a Semi-Supervised Domain Adaptation method for 3D object detection (SSDA3D)
    that employs an Inter-domain Point-CutMix module to align point cloud distributions
    across domains and an Intra-domain Point-MixUp for enhancing model generalization
    on unlabeled target data. Table [5](#S3.T5 "Table 5 ‣ 3.4 Semi-supervised DTL
    ‣ 3 Overview of DTL ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey") summarizes and compares the above-discussed
    studies based on several aspects. This comparison underscores the dynamic evolution
    of semi-supervised DTL techniques in handling 3DPCs, which is pivotal for applications
    ranging from autonomous vehicles to environmental scanning and medical imaging.
    These advancements promise to drive significant improvements in how 3D data is
    processed and utilized across various fields.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，Yu等人[[210](#bib.bib210)]通过将知识从强大的2D模型转移到RGB-D图像，以伪标签增强3D任务中的数据稀缺挑战，显著改善了在有限标注数据下的3D模型预训练。Xu等人[[211](#bib.bib211)]开发了一种基于层次的点云主动学习策略，用于3D点云分割，该策略在多个层次上测量不确定性并选择重要点进行人工标注，有效利用有限的标注。Zhang等人[[212](#bib.bib212)]研究了带有点注释的弱半监督3D目标检测，生成高质量的伪边界框，使得3D检测器在标注数据显著减少的情况下，与完全监督模型表现相当。最后，Wang等人[[213](#bib.bib213)]介绍了一种用于3D目标检测的半监督领域自适应方法（SSDA3D），该方法采用跨域点云分布对齐的Inter-domain
    Point-CutMix模块和用于增强模型在未标注目标数据上泛化能力的Intra-domain Point-MixUp。表[5](#S3.T5 "Table
    5 ‣ 3.4 Semi-supervised DTL ‣ 3 Overview of DTL ‣ Advancing 3D Point Cloud Understanding
    through Deep Transfer Learning: A Comprehensive Survey")总结并比较了上述研究，基于多个方面的比较突显了半监督DTL技术在处理3D点云中的动态演变，这对于从自动驾驶到环境扫描及医学成像等应用至关重要。这些进展有望推动3D数据处理和利用方式的显著改善。'
- en: 'Table 5: Comparison of Studies on Semi-Supervised DTL in 3DPCs'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：3D点云半监督DTL研究比较
- en: '| Ref. | ML Model | Dataset | Application / Task | Advantage | Limitation |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | ML模型 | 数据集 | 应用/任务 | 优势 | 局限性 |'
- en: '| [[201](#bib.bib201)] | Semi-supervised 3D Object Detection | SUN-RGBD, KITTI
    | 3D object detection from 2D and 3D labels | Efficiently transfers 3D info from
    strong to weak classes | Requires sufficient data in strong classes for effective
    transfer |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| [[201](#bib.bib201)] | 半监督3D目标检测 | SUN-RGBD, KITTI | 从2D和3D标签中进行3D目标检测 |
    高效地将3D信息从强类转移到弱类 | 需要在强类中有足够的数据以实现有效转移 |'
- en: '| [[205](#bib.bib205)] | Multimodal Semi-supervised Learning | ModelNet10,
    ModelNet40 | 3D classification and retrieval | Improves data efficiency using
    multimodal data consistency | Performance depends on the quality of multimodal
    data integration |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| [[205](#bib.bib205)] | 多模态半监督学习 | ModelNet10, ModelNet40 | 3D分类和检索 | 通过多模态数据一致性提高数据效率
    | 性能依赖于多模态数据集成的质量 |'
- en: '| [[5](#bib.bib5)] | SPCNet | Real tunnel point clouds | Multi-class object
    segmentation in tunnel scenes | Reduces reliance on labeled data, enhances segmentation
    performance | Specific to tunnel environments, may not generalize |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| [[5](#bib.bib5)] | SPCNet | 实际隧道点云 | 隧道场景中的多类物体分割 | 降低对标注数据的依赖，增强分割性能 | 专门针对隧道环境，可能不具备普遍性
    |'
- en: '| [[130](#bib.bib130)] | DTL based Semantic Segmentation | KITTI, Ouster LiDAR-64
    | 3D object detection in autonomous driving | Reduces need for large-scale datasets,
    fast processing | Limited by the initial data quality and transfer efficiency
    |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| [[130](#bib.bib130)] | 基于DTL的语义分割 | KITTI, Ouster LiDAR-64 | 自动驾驶中的3D目标检测
    | 减少对大规模数据集的需求，处理速度快 | 受初始数据质量和转移效率的限制 |'
- en: '| [[213](#bib.bib213)] | Semi-Supervised Domain Adaptation (SSDA3D) | Waymo,
    nuScenes | 3D object detection in diverse conditions | Adapts to new domains with
    minimal labeled data | Performance may degrade with severe domain shifts |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| [[213](#bib.bib213)] | 半监督领域适应（SSDA3D） | Waymo, nuScenes | 多样条件下的 3D 物体检测
    | 在最少标注数据下适应新领域 | 在严重领域偏移的情况下性能可能会下降 |'
- en: '| [[206](#bib.bib206)] | DTL with PCT | SynLiDAR | 3DPC segmentation | Bridges
    the gap between synthetic and real data, extensive dataset | Focuses on segmentation;
    may not directly apply to other 3D tasks |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| [[206](#bib.bib206)] | 使用 PCT 的 DTL | SynLiDAR | 3D 点云分割 | 弥合合成和真实数据之间的差距，数据集广泛
    | 专注于分割；可能不直接适用于其他 3D 任务 |'
- en: '| [[202](#bib.bib202)] | Semi-supervised or unsupervised feature-metric registration
    | N/A | Point cloud registration | Robust to noise and does not require correspondences;
    fast optimization | Limited details on specific dataset adaptability and real-world
    implementation |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| [[202](#bib.bib202)] | 半监督或无监督特征度量配准 | 无 | 点云配准 | 对噪声具有鲁棒性且不需要对应关系；快速优化 |
    对特定数据集的适应性和实际应用的细节有限 |'
- en: '| [[203](#bib.bib203)] | MS-SVConv and UDGE | 3DMatch, ETH, TUM | 3DPC registration
    | Generalizes across different datasets using unsupervised DTL | May require substantial
    computational resources; specific adaptation challenges not addressed |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| [[203](#bib.bib203)] | MS-SVConv 和 UDGE | 3DMatch, ETH, TUM | 3D 点云配准 | 使用无监督
    DTL 在不同数据集之间进行泛化 | 可能需要大量计算资源；特定适应挑战未解决 |'
- en: '| [[207](#bib.bib207)] | CNN-based classifier | Custom LiDAR dataset | Semantic
    segmentation of dynamic scenes | Combines few annotations with large constraint
    data for improved adaptability | Primarily tailored to dynamic scenes, may not
    generalize to static or varied environments |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| [[207](#bib.bib207)] | 基于 CNN 的分类器 | 自定义 LiDAR 数据集 | 动态场景的语义分割 | 结合少量注释和大量约束数据以改善适应性
    | 主要针对动态场景，可能不适用于静态或多样化环境 |'
- en: '| [[208](#bib.bib208)] | Spatio-Temporal Representation Learning (STRL) | Synthetic,
    indoor, outdoor datasets | 3D scene understanding | Learns from unlabeled data;
    generalizes to multiple 3D tasks | Dependence on temporal correlation which may
    not be present in all datasets |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| [[208](#bib.bib208)] | 时空表示学习（STRL） | 合成、室内、室外数据集 | 3D 场景理解 | 从未标注数据中学习；泛化到多个
    3D 任务 | 依赖于时间相关性，而这可能在所有数据集中不存在 |'
- en: '| [[210](#bib.bib210)] | DTL from 2D to 3D models | ScanNet | Semantic segmentation
    | Uses pseudo-labels for pre-training 3D models; enhances data efficiency | Relies
    on the quality and relevance of 2D model training to 3D tasks |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| [[210](#bib.bib210)] | 从 2D 到 3D 模型的 DTL | ScanNet | 语义分割 | 使用伪标签进行 3D 模型的预训练；提高数据效率
    | 依赖于 2D 模型训练对 3D 任务的质量和相关性 |'
- en: '| [[209](#bib.bib209)] | Weakly supervised 3D object detection | KITTI | 3D
    object detection | Reduces need for detailed annotations; uses unsupervised proposal
    generation | Performance may lag behind fully-supervised methods; adaptation to
    other datasets not detailed |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| [[209](#bib.bib209)] | 弱监督 3D 物体检测 | KITTI | 3D 物体检测 | 减少对详细注释的需求；使用无监督提议生成
    | 性能可能落后于完全监督的方法；对其他数据集的适应性未详细说明 |'
- en: '| [[204](#bib.bib204)] | Semi-supervised point cloud segmentation | N/A | Point
    cloud segmentation | Utilizes both labeled and unlabeled data; improves with self-training
    | Specifics on dataset and environmental adaptability are not detailed |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| [[204](#bib.bib204)] | 半监督点云分割 | 无 | 点云分割 | 利用标注和未标注数据；通过自我训练提高 | 数据集和环境适应性的具体细节未详细说明
    |'
- en: '| [[211](#bib.bib211)] | Hierarchical point-based active learning | S3DIS,
    ScanNetV2 | Point cloud semantic segmentation | Efficient use of very few labeled
    data; incorporates active learning | May require intricate setup for uncertainty
    measurement and point selection |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| [[211](#bib.bib211)] | 基于层次点的主动学习 | S3DIS, ScanNetV2 | 点云语义分割 | 高效利用极少量的标注数据；结合主动学习
    | 可能需要复杂的设置来测量不确定性和选择点 |'
- en: '| [[212](#bib.bib212)] | Vision Transformer-based WSS3D | SUN RGBD, KITTI |
    3D object detection | Low reliance on fully labeled data; uses point annotations
    effectively | Challenges with varying detector compatibility and scene diversity
    |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| [[212](#bib.bib212)] | 基于视觉 Transformer 的 WSS3D | SUN RGBD, KITTI | 3D 物体检测
    | 对完全标注数据的依赖低；有效使用点注释 | 面临探测器兼容性和场景多样性的挑战 |'
- en: 3.5 Inductive transfer learning
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 归纳迁移学习
- en: In [inductive transfer learning](#id20.20.id20) ([ITL](#id20.20.id20)), usually
    [SD](#id10.10.id10) and [TD](#id9.9.id9) remain the same, however target task
    differs from the source task. In [ITL](#id20.20.id20) setting, knowledge is transferred
    from source task to attain high performance in the target task. As suggested in
    [[214](#bib.bib214), [215](#bib.bib215)], the target is labeled, whereas source
    may be labeled, unlabeled, or both. However, in the field of [3DPC](#id1.1.id1),
    it is observed that only labeled source has been explored.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在[归纳迁移学习](#id20.20.id20)（[ITL](#id20.20.id20)）中，通常[SD](#id10.10.id10)和[TD](#id9.9.id9)保持不变，但目标任务与源任务不同。在[ITL](#id20.20.id20)设置中，知识从源任务转移以在目标任务中取得高性能。如[[214](#bib.bib214),
    [215](#bib.bib215)]所示，目标是有标签的，而源任务可以是有标签的、无标签的，或两者兼有。然而，在[3DPC](#id1.1.id1)领域中，仅探索了有标签的源任务。
- en: For instance, Chen et al. [[141](#bib.bib141)] focus on classifying soft-story
    buildings using [CNNs](#id11.11.id11) and density features extracted from [3DPCs](#id1.1.id1).
    More specifically, Once the [3DPC](#id1.1.id1) data of Santa Monica city is collected,
    density features are extracted and converted into 2D imagery data. The task of
    identifying a soft-story building is then approached as a binary classification
    problem, which has effectively been addressed using [CNN](#id11.11.id11) models,
    including VGG, Inception, ResNet and Naive [CNN](#id11.11.id11). They have trained
    the [SD](#id10.10.id10) with labeled settings exploiting more than 1.2 million
    images and 1,000 labeled categories, and surprisingly VGGNet has proved to be
    the best in 2014, with its different versions namely, VGG19 and VGG16\. In addition
    to this, 138 million parameters are trained in the network. VGGNet uses 3 x 3
    filters, contrary to this, Inception uses 1 × 1 filters, limiting the number of
    input channels. Hence, the trainable parameters for Inception is low, approximately
    6.4 million trainable parameters. In an another work, Murtiyoso et al. [[129](#bib.bib129)],
    have taken advantage of DeepLabv3+ network which they trained on labeled dataset
    of building façade images. The trained network is deployed on [TD](#id9.9.id9),
    2D orthoimages received from photogrammetry. Similarly, a [DL](#id4.4.id4)-based
    encoder-decoder lightweight neural architecture, RandLA-Net, for the semantic
    segmentation of [3DPC](#id1.1.id1) data, has been utilized in [[140](#bib.bib140)]
    for performing per-point segmentation of large-scale [3DPCs](#id1.1.id1), as detailed
    in [[216](#bib.bib216)], and demonstrated a good semantic segmentation performance
    in the Semantic 3D benchmark [[140](#bib.bib140), [217](#bib.bib217)].
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Chen等人[[141](#bib.bib141)]专注于使用[CNNs](#id11.11.id11)和从[3DPCs](#id1.1.id1)中提取的密度特征对软故事建筑进行分类。更具体地说，一旦收集了Santa
    Monica市的[3DPC](#id1.1.id1)数据，密度特征会被提取并转换为2D图像数据。识别软故事建筑的任务被视为二分类问题，这一问题有效地通过包括VGG、Inception、ResNet和Naive[CNN](#id11.11.id11)模型解决。他们利用超过120万张图像和1000个标签类别对[SD](#id10.10.id10)进行了有标签设置的训练，令人惊讶的是，VGGNet在2014年被证明是最佳的，其不同版本为VGG19和VGG16。此外，网络中训练了1.38亿个参数。VGGNet使用3
    x 3滤波器，而Inception使用1 × 1滤波器，从而限制了输入通道的数量。因此，Inception的可训练参数较少，大约为640万。在另一项工作中，Murtiyoso等人[[129](#bib.bib129)]利用了他们在建筑立面图像有标签数据集上训练的DeepLabv3+网络。训练后的网络被部署在[TD](#id9.9.id9)上，即从摄影测量中获得的2D正射影像。类似地，基于[DL](#id4.4.id4)的编码器-解码器轻量级神经网络架构RandLA-Net被用于[[140](#bib.bib140)]中对大规模[3DPCs](#id1.1.id1)进行逐点分割，如[[216](#bib.bib216)]所述，并在Semantic
    3D基准测试中表现出良好的语义分割性能[[140](#bib.bib140), [217](#bib.bib217)]。
- en: In [[218](#bib.bib218)], authors have exploited [ITL](#id20.20.id20) for [3DPC](#id1.1.id1)
    classification using an airborne laser scanning method. They have extracted deep
    features using deep residual network, and [CNN](#id11.11.id11) was dedicated to
    classification task. To exploit [CNN](#id11.11.id11) further, the unevenly distributed
    [3DPC](#id1.1.id1) is transformed into voxels [[219](#bib.bib219)]. However, this
    transformation can lead to information redundancy as well as some feature loss.PointNet
    and later PointNet++ were proposed in [[52](#bib.bib52)] and [[53](#bib.bib53)],
    respectively. They successfully directly classify the original [3DPC](#id1.1.id1),
    leading to the proliferation of PointNet-like approaches for [3DPC](#id1.1.id1)
    classification. However, classification accuracy is affected when small range
    of multi-view projection influence the detailed description on each 3D point.
    In addition, there is a need of huge computing time for generating feature maps.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[218](#bib.bib218)]中，作者利用[ITL](#id20.20.id20)进行[3DPC](#id1.1.id1)分类，采用了空中激光扫描方法。他们使用深度残差网络提取深层特征，并将[CNN](#id11.11.id11)专门用于分类任务。为了进一步利用[CNN](#id11.11.id11)，不均匀分布的[3DPC](#id1.1.id1)被转换为体素[[219](#bib.bib219)]。然而，这种转换可能导致信息冗余以及某些特征丢失。PointNet
    和后来提出的 PointNet++ 在[[52](#bib.bib52)]和[[53](#bib.bib53)]中分别提出。它们成功地直接对原始[3DPC](#id1.1.id1)进行分类，从而推动了类似
    PointNet 的方法在[3DPC](#id1.1.id1)分类中的普及。然而，当多视角投影的范围较小时，分类准确性会受到影响，从而影响对每个 3D 点的详细描述。此外，生成特征图需要大量的计算时间。
- en: Lie and his team [[138](#bib.bib138)] have used DensNet201 for obtaining deep
    features, and make use of an improved fully [CNN](#id11.11.id11) by incorporating
    it into the [ITL](#id20.20.id20). The result was very promising and superior to
    state-of-the-art on ISPRS dataset for [OA](#id16.16.id16). In many cases weight
    transfer has found to be instrumental in classification performance. For instance,
    Kamil and Marian [[142](#bib.bib142)] has suggested weights transfer while they
    use [ITL](#id20.20.id20) to improve the classification accuracy of [bidirectional
    LSTM](#id19.19.id19) ([BiLSTM](#id19.19.id19))-based approach for identifying
    human activities. Similarly, Sun et al. [[220](#bib.bib220)] has incorporated
    deep [ITL](#id20.20.id20) to explore the options for solving the issues that are
    encountered with SAE networks in the prediction of remaining useful life by utilizing
    weight and feature transfer.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: Lie 和他的团队[[138](#bib.bib138)]使用 DensNet201 来获取深层特征，并通过将其集成到[ITL](#id20.20.id20)中来利用改进的完全[CNN](#id11.11.id11)。结果非常有希望，并在
    ISPRS 数据集上的[OA](#id16.16.id16)方面优于当前的最先进技术。在许多情况下，权重转移被发现对分类性能具有重要作用。例如，Kamil
    和 Marian[[142](#bib.bib142)]建议在使用[ITL](#id20.20.id20)时进行权重转移，以提高基于[双向 LSTM](#id19.19.id19)（[BiLSTM](#id19.19.id19)）的人体活动识别分类准确性。类似地，Sun
    等人[[220](#bib.bib220)]结合了深度[ITL](#id20.20.id20)，以探索解决 SAE 网络在预测剩余使用寿命时遇到的问题的选项，利用权重和特征转移。
- en: For semantic labeling of heritage, Arnold et al. [[133](#bib.bib133)] has utilized
    [ITL](#id20.20.id20) by considering geometric shape characteristic for the purpose
    of segmentation of memorial objects from the scene. Further, they pretrained [CNN](#id11.11.id11)
    on a model from ModelNet’s labeled dataset. Similarly, for body measurement of
    cattle, Huang et al. [[221](#bib.bib221)] have pretrained the Kd-network by obtaining
    3D deep model’s initial parameters from ShapeNet. The authors extracted the [3DPC](#id1.1.id1)
    spatial feature information which are transferred for identifying cattle body
    silhouette. In addition, their [ITL](#id20.20.id20) method is applicable even
    if the data distribution is different for source and target data.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在遗产的语义标注中，Arnold 等人[[133](#bib.bib133)]利用了[ITL](#id20.20.id20)，通过考虑几何形状特征来分割场景中的纪念物体。此外，他们在
    ModelNet 的标注数据集上预训练了[CNN](#id11.11.id11)模型。类似地，在对牛体测量方面，Huang 等人[[221](#bib.bib221)]通过从
    ShapeNet 获取 3D 深度模型的初始参数来预训练 Kd-network。作者提取了[3DPC](#id1.1.id1)空间特征信息，这些信息被转移用于识别牛体轮廓。此外，他们的[ITL](#id20.20.id20)方法即使在源数据和目标数据的数据分布不同的情况下也适用。
- en: 3.6 Transductive transfer learning
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 诱导性迁移学习
- en: The [transductive transfer learning](#id25.25.id25) ([TTL](#id25.25.id25)) was
    introduced by Arnold et al. [[222](#bib.bib222)] to solve a task in which domains
    are different however the source and target tasks remain same . The authors advocate
    that at the training time, all unlabeled data must be available in the [TD](#id9.9.id9),
    contrary to this, Pan and Yang [[215](#bib.bib215)] urges that for finding out
    marginal probability, it would be enough to seeing part of the unlabeled target
    data at the training time.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '[迁移学习中的传递性转移学习](#id25.25.id25) ([TTL](#id25.25.id25)) 是由 Arnold 等人提出的 [[222](#bib.bib222)]，旨在解决领域不同但源任务和目标任务相同的任务。作者主张，在训练期间，所有未标记的数据必须在
    [TD](#id9.9.id9) 中可用，而 Pan 和 Yang [[215](#bib.bib215)] 则认为，为了找出边际概率，在训练期间看到部分未标记的目标数据就足够了。'
- en: Applying [DTL](#id6.6.id6) for different domain encounters many issues. One
    of these issues for [SDs](#id10.10.id10) and [TDs](#id9.9.id9) is distribution
    mismatching. There are few works based on [UDA](#id8.8.id8) methods that have
    been reported in the literature for the adaptation of a model to an unlabeled
    [TD](#id9.9.id9) from a labeled [SD](#id10.10.id10) for various different applications
    [[223](#bib.bib223), [224](#bib.bib224), [177](#bib.bib177), [225](#bib.bib225),
    [226](#bib.bib226)]. In addition to [UDA](#id8.8.id8), authors in [[143](#bib.bib143)]
    has suggested [source-free unsupervised DA](#id21.21.id21) ([SFUDA](#id21.21.id21)).
    They have used virtual domain modeling to address of one the most talked issue
    in [SFUDA](#id21.21.id21) without requiring original source, reducing source and
    task data mismatching. To fill the gap between [SD](#id10.10.id10) and [TD](#id9.9.id9)
    distributions, they have introduced an intermediate virtual domain, reducing the
    distribution mismatch into two steps, minimizing the domain gap between [SD](#id10.10.id10)
    and virtual domains and then between the virtual and [TDs](#id9.9.id9). To achieve
    this goal, the authors have generated the virtual domain samples in the feature
    space using an approximated Gaussian mixture model (GMM) and the pretrained source
    model, so that the virtual domain maintains a similar distribution to the [SD](#id10.10.id10)
    without access to the original source data. On the other hand, they have proposed
    an effective distribution alignment method that gradually improves the compactness
    of the [TD](#id9.9.id9) distribution through model learning to reduce the aforementioned
    distribution gap. However, the first [SFUDA](#id21.21.id21) framework was proposed
    in [[197](#bib.bib197)], where the authors exploited the source model to fine-tune
    the [TD](#id9.9.id9) data without access to the source data for 3D object detection.
    The paper [[116](#bib.bib116)] discussed the use of [SSL](#id14.14.id14) to learn
    useful representations from unlabeled data in [DA](#id7.7.id7) for 3D perception
    problems. The authors proposed a new family of pretext tasks employing [SSL](#id14.14.id14)
    for [DA](#id7.7.id7) on point clouds, called deformation reconstruction, along
    with a novel training procedure called [point cloud mixup](#id22.22.id22) ([PCM](#id22.22.id22))
    for labeled point cloud data. The results demonstrated that employing [SSL](#id14.14.id14)
    with this technique significantly improves classification and segmentation in
    [DA](#id7.7.id7) datasets.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 应用[领域自适应（DTL）](#id6.6.id6)在不同领域时会遇到许多问题。其中，[SDs](#id10.10.id10)和[TDs](#id9.9.id9)的一个问题是分布不匹配。关于从标记的[SD](#id10.10.id10)适应到未标记的[TD](#id9.9.id9)的模型，文献中报道的基于[UDA](#id8.8.id8)方法的研究很少[[223](#bib.bib223)，[224](#bib.bib224)，[177](#bib.bib177)，[225](#bib.bib225)，[226](#bib.bib226)]。除了[UDA](#id8.8.id8)之外，[[143](#bib.bib143)]的作者建议了[无源无监督领域自适应（SFUDA）](#id21.21.id21)。他们使用虚拟领域建模来解决[SFUDA](#id21.21.id21)中最受关注的问题之一，无需原始源数据，从而减少源数据和任务数据的不匹配。为了填补[SD](#id10.10.id10)和[TD](#id9.9.id9)分布之间的差距，他们引入了一个中间虚拟领域，将分布不匹配分为两个步骤：首先减少[SD](#id10.10.id10)与虚拟领域之间的域差距，然后减少虚拟领域与[TDs](#id9.9.id9)之间的差距。为实现这一目标，作者使用了近似高斯混合模型（GMM）和预训练的源模型，在特征空间中生成虚拟领域样本，使虚拟领域的分布与[SD](#id10.10.id10)保持相似，而无需访问原始源数据。另一方面，他们提出了一种有效的分布对齐方法，通过模型学习逐步改善[TD](#id9.9.id9)分布的紧凑性，以减少上述的分布差距。然而，第一个[SFUDA](#id21.21.id21)框架在[[197](#bib.bib197)]中提出，其中作者利用源模型对[TD](#id9.9.id9)数据进行微调，而无需访问源数据，应用于3D目标检测。论文[[116](#bib.bib116)]讨论了使用[自监督学习（SSL）](#id14.14.id14)从未标记数据中学习有用表示的方法，用于3D感知问题的[领域适应（DA）](#id7.7.id7)。作者提出了一系列新的预训练任务，采用[SSL](#id14.14.id14)进行点云[DA](#id7.7.id7)，称为变形重建，并提出了一种新的训练程序，称为[点云混合（PCM）](#id22.22.id22)，用于标记的点云数据。结果表明，使用这种技术的[SSL](#id14.14.id14)显著改善了[DA](#id7.7.id7)数据集中的分类和分割。
- en: As the research on DA for [3DPC](#id1.1.id1) is evolving, researchers are working
    to improve the method to reduce the [SD](#id10.10.id10)’ differences from [TD](#id9.9.id9),
    and to maximize the domain adaptability so that it can be generalized. However,
    most of the research addresses [DA](#id7.7.id7) through experimentation over different
    domain meant for different but related task. Table LABEL:UDA summarizes some of
    the relevant studies, compares their characteristics and identifies their pros
    and cons.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对[3DPC](#id1.1.id1)的DA研究不断发展，研究人员正在努力改进该方法，以减少[SD](#id10.10.id10)与[TD](#id9.9.id9)之间的差异，并最大化领域适应性，以便实现广泛应用。然而，大多数研究通过在不同领域进行实验来解决[DA](#id7.7.id7)问题，这些领域适用于不同但相关的任务。表LABEL:UDA总结了一些相关研究，比较了它们的特点，并识别了它们的优缺点。
- en: 'The works [[227](#bib.bib227), [228](#bib.bib228)] require handling preprocessing
    tasks and additional data, which in turn increases complexity. Rist et al. [[227](#bib.bib227)]
    exploit cross-sensor [DA](#id7.7.id7) via 3D voxels, while Wang et al. [[228](#bib.bib228)]
    utilize adversarial networks for global adaptation by exploiting cross-range adaptation.
    In this context, the authors in [[229](#bib.bib229)] present a potentially significant
    contribution. Their approach circumvents complex procedures and the need for additional
    data. Instead, they leverage the KITTI benchmark dataset to align strong-weak
    features and enhance feature representation, supplementing it with available data
    when necessary. Specifically, they focus on the ’car’ example from the benchmark
    dataset, considering ’near-range’ and ’far-range’ objects as the [SD](#id10.10.id10)
    and [TD](#id9.9.id9), respectively. In addition to the underlying issues in handling
    different domain for DA problems, distribution mismatch within a domain has also
    been raised by researchers. In this regard, to adapt the different scenario within
    a single domain, Zhang et al. [[230](#bib.bib230)] have taken the advantage of
    cross-dataset and considered several adaptation scenario like day-night adaptation
    and adaptation of different scenes (say, from different places) as in the case
    with nuScenes dataset consisting of driving scenes from different geographical
    locations, Boston and Singapore. Their extensive experiments enable understanding
    how researchers can leverage cross-datasets for [UDA](#id8.8.id8). Moreover, they
    have introduced range-aware and scale-aware detection mechanism for [3DPC](#id1.1.id1)
    tasks. In the similar direction, Jaritz et al. [[196](#bib.bib196)] has considered
    the same scenario, i.e., day-night adaptation and Boston-Singapore scenario, however
    hey have primarily focused on cross-modality adaptation using their xMUDA model.
    By the virtue of their proposed method, they have shown a better performance can
    be achieved for cross-modality. Moreover, A2D2 and Semantic KITTI datasets were
    used as source and target respectively. Moving forward, Nunes et al. [[231](#bib.bib231)]
    present a new contrastive learning approach for representation learning of 3DPs
    point cloud data in the context of autonomous driving. The approach extracts class-agnostic
    segments and applies contrastive loss to discriminate between similar and dissimilar
    structures. The method is applied on data recorded with a 3D [LiDAR](#id18.18.id18)
    and achieves competitive performance in comparison to other self-supervised contrastive
    point cloud methods. Fig. [10](#S3.F10 "Figure 10 ‣ 3.6 Transductive transfer
    learning ‣ 3 Overview of DTL ‣ Advancing 3D Point Cloud Understanding through
    Deep Transfer Learning: A Comprehensive Survey") presents an example of fine-tuning
    in [3DPC](#id1.1.id1) segmentation [[196](#bib.bib196)].'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工作[[227](#bib.bib227), [228](#bib.bib228)]需要处理预处理任务和额外的数据，这反过来增加了复杂性。Rist等人[[227](#bib.bib227)]利用3D体素通过传感器间的[DA](#id7.7.id7)，而Wang等人[[228](#bib.bib228)]利用对抗网络通过跨范围适应进行全局适应。在这种背景下，[[229](#bib.bib229)]的作者提出了一个可能具有重要意义的贡献。他们的方法绕过了复杂的程序和额外数据的需求。相反，他们利用KITTI基准数据集来对齐强弱特征并增强特征表示，必要时补充可用数据。具体来说，他们关注基准数据集中的“汽车”示例，将“近距离”和“远距离”物体视为[SD](#id10.10.id10)和[TD](#id9.9.id9)。除了处理不同领域的DA问题中的潜在问题外，研究人员还提出了在一个领域内的分布不匹配问题。在这方面，为了适应单一领域内的不同场景，Zhang等人[[230](#bib.bib230)]利用了跨数据集，并考虑了如日夜适应和不同场景的适应（例如，来自不同地点的情况），如nuScenes数据集包含来自不同地理位置（波士顿和新加坡）的驾驶场景。他们的广泛实验帮助理解了研究人员如何利用跨数据集进行[UDA](#id8.8.id8)。此外，他们为[3DPC](#id1.1.id1)任务引入了范围感知和尺度感知的检测机制。在类似的方向上，Jaritz等人[[196](#bib.bib196)]考虑了相同的场景，即日夜适应和波士顿-新加坡场景，但他们主要关注使用他们的xMUDA模型进行跨模态适应。凭借其提出的方法，他们展示了跨模态可以实现更好的性能。此外，A2D2和Semantic
    KITTI数据集分别被用作源数据和目标数据。展望未来，Nunes等人[[231](#bib.bib231)]提出了一种新的对比学习方法，用于在自主驾驶背景下对3D点云数据进行表征学习。该方法提取类无关的片段，并应用对比损失来区分相似和不同的结构。该方法应用于使用3D
    [LiDAR](#id18.18.id18)记录的数据，并与其他自监督对比点云方法相比，表现出竞争力。图[10](#S3.F10 "图 10 ‣ 3.6 转导性迁移学习
    ‣ 3 DTL概述 ‣ 通过深度迁移学习推进3D点云理解：全面综述")展示了在[3DPC](#id1.1.id1)分割中的微调示例[[196](#bib.bib196)]。
- en: '![Refer to caption](img/0059369f88435ceb1aba7014d30b8d27.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0059369f88435ceb1aba7014d30b8d27.png)'
- en: 'Figure 10: Example of using fine-tuning in [3DPC](#id1.1.id1) segmentation:
    (A) Data augmentation is used to generate the augmented views $\mathcal{P}^{q}$
    and $\mathcal{P}^{k}$ from a point cloud $\mathcal{P}$, and class-agnostic segments
    $S$ are extracted from $\mathcal{P}$. The augmented segments $\mathcal{S}^{q}$
    and $\mathcal{S}^{k}$ are determined with their point-wise features using the
    point indexes of $\mathcal{S}$ extracted from $\mathcal{P}$. Point-wise features
    $\mathcal{F}^{q}$ and $\mathcal{F}^{k}$ are computed, followed by dropout and
    global max pooling over each segment. The segment feature vectors are projected
    using the projection head to obtain the final features $\textbf{s}^{q}_{m}$ and
    $\textbf{s}^{k}_{m}$ from the $M$ segments, and the contrastive loss is computed,
    and (B) The pre-trained backbone is fine-tuned for the downstream task, i.e.,
    semantic segmentation [[231](#bib.bib231)].'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：在[3DPC](#id1.1.id1)分割中使用微调的示例：(A) 数据增强用于从点云$\mathcal{P}$生成增强视图$\mathcal{P}^{q}$和$\mathcal{P}^{k}$，并从$\mathcal{P}$提取类别无关的分段$S$。通过使用从$\mathcal{P}$中提取的$\mathcal{S}$的点索引，确定增强分段$\mathcal{S}^{q}$和$\mathcal{S}^{k}$的点特征。计算点特征$\mathcal{F}^{q}$和$\mathcal{F}^{k}$，然后对每个分段进行dropout和全局最大池化。使用投影头投影分段特征向量，以从$M$个分段中获得最终特征$\textbf{s}^{q}_{m}$和$\textbf{s}^{k}_{m}$，并计算对比损失，以及(B)
    预训练的骨干网针对下游任务进行微调，即语义分割[[231](#bib.bib231)]。
- en: A UDA-based semantic labeling scheme of [3DPCs](#id1.1.id1) is proposed in [[157](#bib.bib157)],
    where domain discrepancies induced by different [LiDAR](#id18.18.id18) sensors
    have been addressed. Typically, a complete and label technique that recovers 3D
    surfaces is developed before passing them to a segmentation network. More precisely,
    a [SVCN](#id15.15.id15) to complete the 3D surfaces of a sparse [3DPC](#id1.1.id1)
    is designed. By contrast to semantic labeling, obtaining training pairs for [SVCN](#id15.15.id15)
    does not require any manual labeling. Moreover, local adversarial learning has
    been introduced for modeling the surface priors. The recovered 3D surfaces serve
    as a canonical domain, from which semantic labels can transfer across different
    [LiDAR](#id18.18.id18) sensors.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[157](#bib.bib157)]中提出了一种基于UDA的[3DPCs](#id1.1.id1)语义标注方案，解决了由不同[LiDAR](#id18.18.id18)传感器引起的领域差异。通常，开发一个完整的标签技术来恢复3D表面，然后将其传递给分割网络。更精确地说，设计了一个[SVCN](#id15.15.id15)来完成稀疏[3DPC](#id1.1.id1)的3D表面。与语义标注相比，获得[SVCN](#id15.15.id15)的训练对是不需要任何手动标注的。此外，引入了局部对抗学习来建模表面先验。恢复的3D表面作为规范域，从中可以在不同[LiDAR](#id18.18.id18)传感器之间转移语义标签。
- en: 'Table 6: A summary of unsupervised DA frameworks and their characteristics
    used in [3DPC](#id1.1.id1) understanding.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：用于[3DPC](#id1.1.id1)理解的无监督DA框架及其特性的总结。
- en: '| Work | SD | TD | [ML](#id5.5.id5) model | SoDTs | SoDDs | Pros and cons |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | SD | TD | [ML](#id5.5.id5) 模型 | SoDTs | SoDDs | 优缺点 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| [[116](#bib.bib116)] | PointSegDA | PointSegDA | DefRec | S | D | The first
    study of SSL for DA on [3DPCs](#id1.1.id1). |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| [[116](#bib.bib116)] | PointSegDA | PointSegDA | DefRec | 有 | 无 | 第一个针对[3DPCs](#id1.1.id1)的DA的SSL研究。
    |'
- en: '| [[142](#bib.bib142)] | UTD-MHAD | UTD-MHAD | N/A | S | S | Computationally
    efficient and enable human activity detection using depth maps |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| [[142](#bib.bib142)] | UTD-MHAD | UTD-MHAD | 无 | 有 | 有 | 计算上高效，能够使用深度图进行人类活动检测
    |'
- en: '| [[143](#bib.bib143)] | ModelNet40 | ShapeNet | VDM-DA | S | D | Enable virtual
    domain modeling for source data-free DA. |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| [[143](#bib.bib143)] | ModelNet40 | ShapeNet | VDM-DA | 有 | 无 | 实现了虚拟域建模以支持无源数据的DA。'
- en: '| [[155](#bib.bib155)] | GTA-LiDAR | KITTI | SqueezesegV2 | S | D | Enhance
    model structure and UDA for road-object segmentation. |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| [[155](#bib.bib155)] | GTA-LiDAR | KITTI | SqueezesegV2 | 有 | 无 | 提升了模型结构和UDA用于道路对象分割。
    |'
- en: '| [[156](#bib.bib156)] | Semantic (KITTI, POSS, USL) | Semantic (KITTI, POSS,
    USL) | LiDARNet | S | D | Preserve almost the same performance on the SD after
    adaptation and achieve 8%-22% mIoU performance increase in the TD. |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| [[156](#bib.bib156)] | 语义 (KITTI, POSS, USL) | 语义 (KITTI, POSS, USL) | LiDARNet
    | 有 | 无 | 适应后在SD上保持几乎相同的性能，并在TD上实现8%-22%的mIoU性能提升。 |'
- en: '| [[157](#bib.bib157)] | Waymo | KITTI; nuScenes | [SVCN](#id15.15.id15) |
    S | D | Provide 8.2-36.6% better performance than previous DA methods. |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| [[157](#bib.bib157)] | Waymo | KITTI; nuScenes | [SVCN](#id15.15.id15) |
    有 | 无 | 比以前的DA方法提供了8.2-36.6%的性能提升。 |'
- en: '| [[158](#bib.bib158)] | pointDA-10 | pointDA-10 | pointDAN | S | S | 3DPC
    representation using a multi-scale 3D DA network. |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| [[158](#bib.bib158)] | pointDA-10 | pointDA-10 | pointDAN | 有 | 有 | 使用多尺度3D
    DA网络的3DPC表示。 |'
- en: '| [[195](#bib.bib195)] | Waymo | KITTI, nuSenses, Lyft | ST3D | S | D | Exceed
    fully supervised results on KITTI 3D object detection benchmark. |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| [[195](#bib.bib195)] | Waymo | KITTI, nuSenses, Lyft | ST3D | S | D | 超越了KITTI
    3D目标检测基准上的全监督结果。 |'
- en: '| [[197](#bib.bib197)] | nuScenes | KITTI | SF-UDA | S | D | Enable [SFUDA](#id21.21.id21)
    but requires to be tested beyond cars, to detect other types of objects. |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| [[197](#bib.bib197)] | nuScenes | KITTI | SF-UDA | S | D | 实现了[SFUDA](#id21.21.id21)，但需要在汽车以外进行测试，以检测其他类型的对象。
    |'
- en: '| [[198](#bib.bib198)] | PointDA-10; ScanObjectNN | ShapeNet, ModelNet40, ScanNet,
    ScanObjectNN | RefRec | S | D | Refine pseudolabels, offline and online, by leveraging
    shape descriptors learned to solve shape reconstruction on both domains |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| [[198](#bib.bib198)] | PointDA-10; ScanObjectNN | ShapeNet, ModelNet40, ScanNet,
    ScanObjectNN | RefRec | S | D | 通过利用学到的形状描述符在两个领域解决形状重建问题，改进伪标签，离线和在线。 |'
- en: '| [[228](#bib.bib228)] | KITTI | nuScenes | CrAF | S | D | Conduct more challenging
    cross-device adaptation. |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| [[228](#bib.bib228)] | KITTI | nuScenes | CrAF | S | D | 进行更具挑战性的跨设备适配。 |'
- en: '| [[230](#bib.bib230)] | PreSIL; nuScenes | KITTI; nuScenes | SRDAN | S | D
    | Demosntrate the significance of geometric characteristics for cross-dataset
    3D object detection. |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| [[230](#bib.bib230)] | PreSIL; nuScenes | KITTI; nuScenes | SRDAN | S | D
    | 展示几何特征对跨数据集3D目标检测的重要性。 |'
- en: '| [[232](#bib.bib232)] | labeled data, VirtualKITTi | Semantic KITTI | CLDA
    | D | D | Improve segmentation performance for rare classes but still needs more
    enhancement. |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| [[232](#bib.bib232)] | 标记数据，VirtualKITTi | 语义KITTI | CLDA | D | D | 改进了对稀有类别的分割性能，但仍需要更多提升。
    |'
- en: '| [[233](#bib.bib233)] | PointDA-10 | PointDA-10 | BADM; PointDA-10 | S | S
    | Achieve state-of-the-art performance and outperform other 3D DA techniques but
    not appropriate for different tasks or different domains. |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| [[233](#bib.bib233)] | PointDA-10 | PointDA-10 | BADM; PointDA-10 | S | S
    | 实现了最先进的性能，超越了其他3D DA技术，但不适用于不同任务或不同领域。 |'
- en: '| [[234](#bib.bib234)] | bSSFP-MRI; MM-WHS | LGE-MRI; MRI and CT | UDA-GAN
    | D | D | Provide promising performance, compared to the state-of-the-art. |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| [[234](#bib.bib234)] | bSSFP-MRI; MM-WHS | LGE-MRI; MRI和CT | UDA-GAN | D
    | D | 提供了有希望的性能，相比最先进的技术。 |'
- en: '| [[235](#bib.bib235)] | Video data | LiDAR | PALMAR | S | D | 63% improvement
    of multi-person tTracking than state-of-the-art frameworks while maintaining efficient
    computation on the edge devices. |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| [[235](#bib.bib235)] | 视频数据 | LiDAR | PALMAR | S | D | 比最先进的框架在多人物跟踪上提高了63%，同时保持了边缘设备上的高效计算。
    |'
- en: '| [[236](#bib.bib236)] | KITTI object; near-range | KITTI object; far-range
    | SCNET | S | S | Achieve a remarkable improvement in the adaptation capabilities
    but without enable knowledge transfer to different tasks. |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| [[236](#bib.bib236)] | KITTI目标；近距离 | KITTI目标；远距离 | SCNET | S | S | 在适应能力上取得了显著改进，但未能实现知识转移到不同任务。
    |'
- en: '| [[237](#bib.bib237)] | GTA-V | Oxford RobotCar | vLPD-Net R; vLPD-Net V |
    S | D | Achieve state-of-the-art performance on the real-world Oxford RobotCar
    dataset but the investigation of the loop closure and re-localization in real-world
    is needed. |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| [[237](#bib.bib237)] | GTA-V | Oxford RobotCar | vLPD-Net R; vLPD-Net V |
    S | D | 在真实世界的Oxford RobotCar数据集上实现了最先进的性能，但需要对真实世界中的回环闭合和重新定位进行调查。'
- en: '| [[238](#bib.bib238)] | CadData | CamData | DANN; SSLPC;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '| [[238](#bib.bib238)] | CadData | CamData | DANN; SSLPC; |'
- en: PoinDAN | S | D | Less time-consuming for implementation in production. |
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: PoinDAN | S | D | 实现生产中的实施耗时更少。 |
- en: '| [[239](#bib.bib239)] | PointDA-10 | PointDA-10 | DSDAN | S | S | Exceed the
    state-of-the-art performance of cross-dataset 3DPC recognition tasks. |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| [[239](#bib.bib239)] | PointDA-10 | PointDA-10 | DSDAN | S | S | 超越跨数据集3D点云识别任务的最先进性能。
    |'
- en: '| [[240](#bib.bib240)] | High resolution LiDar data | Low resolution LiDar
    data | LAMAR | S | D | 94% Human activity recognition performance in multiple-inhabitant
    scenario. |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| [[240](#bib.bib240)] | 高分辨率LiDar数据 | 低分辨率LiDar数据 | LAMAR | S | D | 在多居住者场景中实现94%的人工活动识别性能。
    |'
- en: '| [[241](#bib.bib241)] | A2D2 | Semantic KITTI | xMUDA | S | D | Enable [cross-modal
    learning DA](#id26.26.id26) ([CLDA](#id26.26.id26)) in 3D semantic segmentation;
    however, knowledge cannot be transferred across different ttasks. |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| [[241](#bib.bib241)] | A2D2 | 语义KITTI | xMUDA | S | D | 在3D语义分割中实现了[跨模态学习DA](#id26.26.id26)（[CLDA](#id26.26.id26)）；然而，知识无法跨不同任务转移。
    |'
- en: '| [[242](#bib.bib242)] | KITTI | MDLS | CycleGAN; YOLOv3 | S | D | Outperformed
    other compared baseline approaches with more than 39% improvement in F 1 -Measure
    score. |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| [[242](#bib.bib242)] | KITTI | MDLS | CycleGAN; YOLOv3 | S | D | 超越其他对比基线方法，F
    1 测量分数提高了39%以上。 |'
- en: 'Abbreviations: Same or different tasks (SoDTs); Same or different domains (SoDDs);
    Different (D); Same(S).'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 缩写：相同或不同任务（SoDTs）；相同或不同领域（SoDDs）；不同（D）；相同（S）。
- en: 4 Applications of TL-based 3DPCs
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基于TL的3DPC应用
- en: TL-based [3DPC](#id1.1.id1) can be applied in various applications, including
    robotics and autonomous systems, augmented reality and virtual reality, medical
    imaging, geo-spatial data analysis, industrial inspection, building information
    modeling, etc. In this section, we focus on some of the main applications that
    attract increasing research effort. Table LABEL:3dpc_tasks summarizes some of
    the pertinent [DTL](#id6.6.id6)-based [3DPC](#id1.1.id1) frameworks proposed to
    perform different tasks.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 基于TL的[3DPC](#id1.1.id1)可以应用于多种领域，包括机器人技术和自主系统、增强现实和虚拟现实、医学成像、地理空间数据分析、工业检查、建筑信息建模等。本节中，我们关注一些吸引越来越多研究关注的主要应用。表LABEL:3dpc_tasks总结了一些相关的[DTL](#id6.6.id6)-based
    [3DPC](#id1.1.id1)框架，这些框架被提出来执行不同的任务。
- en: 4.1 Semantic labeling and segmentation
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 语义标记和分割
- en: Semantic labeling of [3DPC](#id1.1.id1) is crucial for performing segmentation
    tasks to represent scans accurately, using manual as well as automatic labeling.
    For instance, recently, Xie et al. [[117](#bib.bib117)] have used the Stanford
    large-scale 3D indoor spaces (S3DIS) dataset for transferring knowledge to the
    target dataset. However, it contains a much smaller repository as compared to
    source dataset (ScanNet), and manual semantic labeling has been done with 13 categories.
    Arnold et al. [[133](#bib.bib133)] used simple [MLP](#id23.23.id23) to learn the
    diversity of the feature space and acquire knowledge for semantic labeling.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '[3DPC](#id1.1.id1)的语义标记对于执行分割任务以准确表示扫描数据至关重要，包括手动和自动标记。例如，最近，Xie等人[[117](#bib.bib117)]使用了斯坦福大规模3D室内空间（S3DIS）数据集来将知识转移到目标数据集中。然而，与源数据集（ScanNet）相比，它包含的库要小得多，并且进行了13个类别的手动语义标记。Arnold等人[[133](#bib.bib133)]使用了简单的[MLP](#id23.23.id23)来学习特征空间的多样性并获取语义标记的知识。'
- en: One of the prime concerns is to identify similar labels and different data from
    the same labels separately. To address this, authors [[243](#bib.bib243)] have
    suggested a domain-independent approach for semantic labeling. Their proposed
    model considers similarity metrics as features to infer the correct semantic labels
    and learns a similarity function to identify if attributes have the same labels.
    Because the matching function is unrelated to specific labels, their model does
    not depend upon the label and thus are independent of domain ontologies. A similar
    approach for bridge [3DPC](#id1.1.id1) has been suggested in [[244](#bib.bib244)].
    The authors have received an improved IoU with 94.29%.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要的问题是识别相似标签和不同数据中的相同标签。为了解决这个问题，作者[[243](#bib.bib243)]提出了一种领域独立的语义标记方法。他们提出的模型将相似性度量作为特征，以推断正确的语义标签，并学习一个相似性函数来识别属性是否具有相同的标签。由于匹配函数与特定标签无关，因此他们的模型不依赖于标签，因此与领域本体论无关。类似的方法已在[[244](#bib.bib244)]中提出，用于桥接[3DPC](#id1.1.id1)，作者获得了94.29%的改进IoU。
- en: The ability of [DTL](#id6.6.id6) to reduce the need for large-scale dataset
    can be economical and hence can be exploited in [3DPC](#id1.1.id1) tasks for performing
    accurately without requiring more extensive data. For example, with the help of
    [DTL](#id6.6.id6), authors in [[245](#bib.bib245)] have explored the analogy between
    real and synthetic data for semantic segmentation and tried reducing their gap.
    Based on deep [CNN](#id11.11.id11) (DCNN), authors in [[246](#bib.bib246)] have
    suggested a weakly-supervised semantic segmentation technique. Unlike others,
    they have considered auxiliary segmentation annotations for distinct categories
    to guide segmentation on pictures with only image-level class labels. They have
    used a decoupled encoder-decoder architecture with an attention model allowing
    segmentation knowledge to be transferable across categories.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '[DTL](#id6.6.id6)减少对大规模数据集需求的能力可以是经济的，因此可以在[3DPC](#id1.1.id1)任务中加以利用，以便在不需要更多数据的情况下准确执行。例如，在[[245](#bib.bib245)]中，作者利用[DTL](#id6.6.id6)探索了真实数据和合成数据之间的类比，并尝试减少它们之间的差距。基于深度[CN](#id11.11.id11)（DCNN），[[246](#bib.bib246)]中的作者提出了一种弱监督的语义分割技术。与其他方法不同，他们考虑了辅助分割注释用于不同类别，以指导仅具有图像级别类别标签的图像上的分割。他们使用了一个解耦的编码器-解码器架构和一个注意力模型，允许分割知识在类别间可迁移。'
- en: 'Semantic segmentation is conducted in [[206](#bib.bib206)] by transferring
    knowledge from synthetic to real [LiDAR](#id18.18.id18)  [3DPCs](#id1.1.id1).
    Typically, a large-scale synthetic [LiDAR](#id18.18.id18) dataset, is first collected
    from multiple virtual environments with rich layouts and scenes. It includes point-wise
    labeled [3DPCs](#id1.1.id1) with accurate geometric shapes and comprehensive semantic
    classes. Moving on, a [3DPC](#id1.1.id1) translator (PCT) is designed to mitigate
    the discrepancy between real and synthetic [3DPCs](#id1.1.id1). Typically, the
    synthetic-to-real discrepancy is decomposed into a sparsity and an appearance
    component before separately handling them. Fig. [11](#S4.F11 "Figure 11 ‣ 4.1
    Semantic labeling and segmentation ‣ 4 Applications of TL-based 3DPCs ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")
    explains the PCT approach adopted in [[206](#bib.bib206)], disentangling [3DPC](#id1.1.id1)
    translation into appearance and sparsity translation tasks. Accordingly, dense
    [3DPCs](#id1.1.id1) having similar appearances are first learned within the appearance
    translation. The sparsity translation then learns real sparsity distribution in
    2D space and fuses it with the reconstructed [3DPC](#id1.1.id1) in 3D space. Then,
    real sparsity distributions in 2D space are learned within the sparsity translation
    before being fused with the reconstructed [3DPC](#id1.1.id1) in 3D space.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[206](#bib.bib206)]中，通过将知识从合成[LiDAR](#id18.18.id18)[3DPCs](#id1.1.id1)转移到真实[LiDAR](#id18.18.id18)[3DPCs](#id1.1.id1)来进行语义分割。通常，首先从多个虚拟环境中收集大规模的合成[LiDAR](#id18.18.id18)数据集，包含具有准确几何形状和全面语义类别的逐点标记[3DPCs](#id1.1.id1)。接下来，设计一个[3DPC](#id1.1.id1)翻译器（PCT）来减轻真实和合成[3DPCs](#id1.1.id1)之间的差异。通常，合成与真实之间的差异会被分解为稀疏性和外观两个组件，然后分别处理它们。图[11](#S4.F11
    "Figure 11 ‣ 4.1 Semantic labeling and segmentation ‣ 4 Applications of TL-based
    3DPCs ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning:
    A Comprehensive Survey") 解释了在[[206](#bib.bib206)]中采用的PCT方法，将[3DPC](#id1.1.id1)翻译任务解耦为外观和稀疏性翻译任务。因此，首先在外观翻译中学习具有相似外观的密集[3DPCs](#id1.1.id1)。然后，稀疏性翻译学习真实的2D空间稀疏分布，并将其与3D空间中的重建[3DPC](#id1.1.id1)融合。然后，在稀疏性翻译中学习2D空间中的真实稀疏分布，并将其与3D空间中的重建[3DPC](#id1.1.id1)融合。'
- en: '![Refer to caption](img/9301d1642ff250e328177da445ff0369.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9301d1642ff250e328177da445ff0369.png)'
- en: 'Figure 11: The PCT method separates the task of translating [3DPC](#id1.1.id1)
    into two parts: appearance translation and sparsity translation. Using synthetic
    [3DPC](#id1.1.id1) as input, the appearance translation learns to create dense
    [3DPC](#id1.1.id1) that look similar to real ones. The sparsity translation then
    learns the typical sparsity patterns found in real 2D point clouds, and combines
    these patterns with the dense [3DPC](#id1.1.id1) in 3D space. The end result is
    a [3DPC](#id1.1.id1) that looks and is sparse similarly to real [3DPC](#id1.1.id1).'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：PCT方法将[3DPC](#id1.1.id1)翻译任务分为两部分：外观翻译和稀疏性翻译。使用合成[3DPC](#id1.1.id1)作为输入，外观翻译学习创建看起来与真实[3DPC](#id1.1.id1)相似的密集[3DPC](#id1.1.id1)。然后，稀疏性翻译学习真实2D点云中的典型稀疏模式，并将这些模式与3D空间中的密集[3DPC](#id1.1.id1)结合。最终结果是一个在外观和稀疏性上都类似于真实[3DPC](#id1.1.id1)的[3DPC](#id1.1.id1)。
- en: Moving forward, A. Murtiyoso et al. [[129](#bib.bib129)] has used [DTL](#id6.6.id6)
    on a photogrammetric orthoimage by exploiting labeled and rectified images of
    building facades for training neural networks on them. Next, they allow the transition
    from 2D orthoimage to [3DPC](#id1.1.id1) by another program. With their promising
    results for photogrammetric data, their proposed work is a potential option to
    assist in automatic [3DPC](#id1.1.id1) semantic segmentation. Similarly, authors
    in [[171](#bib.bib171)] have claimed that semantic segmentation can perform equally
    competently to the latest [ML](#id5.5.id5) approaches in modeling cultural heritage.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 继续前进，A. Murtiyoso 等人[[129](#bib.bib129)]利用标记和校正后的建筑立面图像对[DTL](#id6.6.id6)进行训练神经网络，并在光测正射影像上使用该方法。接下来，他们通过另一程序实现从2D正射影像到[3DPC](#id1.1.id1)的过渡。由于他们在光测数据上的有希望的结果，他们提出的工作是协助自动化[3DPC](#id1.1.id1)语义分割的一个潜在选择。同样，[[171](#bib.bib171)]的作者声称，语义分割在建模文化遗产方面可以与最新的[ML](#id5.5.id5)方法同样有效。
- en: On the other hand, [3DPC](#id1.1.id1) instance segmentation is equally employed
    for many applications. For example, in [[131](#bib.bib131)], authors have taken
    a problem of height detection of a catenary conductor. For [3DPC](#id1.1.id1)
    instance segmentation, they have incorporated [DTL](#id6.6.id6) and 3D-BoNet model
    with a multiscale grouping (MSG) structure on a smaller dataset, with loss curve
    converging better for their model. In addition to its applications in building,
    tunneling, and construction, [DTL](#id6.6.id6) for [3DPC](#id1.1.id1) segmentation
    has found utility in robotics. Specifically, it is utilized for automatic toolpath
    generation to enable tasks such as masking, deburring, and polishing in various
    industrial applications. Z. Xie and his team [[137](#bib.bib137)] have utilized
    [3DPC](#id1.1.id1) segmentation and classification to extract the object features.
    Furthermore, they have exploited [DTL](#id6.6.id6) to enhance performance and
    avoid investing much time in training data. Their toolpath recommendation automatically
    suggests patterns to choose from and does not require manual efforts. To identify
    human activities, authors [[142](#bib.bib142)] have carried out segmentation to
    separate the human figure from the background so that the descriptor can only
    be determined for a human figure.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，[3DPC](#id1.1.id1) 实例分割在许多应用中同样被广泛使用。例如，在[[131](#bib.bib131)]中，作者讨论了吊索导体的高度检测问题。对于[3DPC](#id1.1.id1)
    实例分割，他们结合了[DTL](#id6.6.id6)和 3D-BoNet 模型，并在较小的数据集上使用了多尺度分组（MSG）结构，使得模型的损失曲线收敛得更好。除了在建筑、隧道和施工中的应用，[DTL](#id6.6.id6)在[3DPC](#id1.1.id1)分割中在机器人领域也找到了用途。具体来说，它被用于自动工具路径生成，以实现各种工业应用中的遮罩、去毛刺和抛光等任务。Z.
    Xie及其团队[[137](#bib.bib137)]利用[3DPC](#id1.1.id1)分割和分类提取物体特征。此外，他们还利用[DTL](#id6.6.id6)提升性能，并避免在训练数据上投入过多时间。他们的工具路径推荐系统会自动建议选择的模式，无需人工操作。为了识别人体活动，作者[[142](#bib.bib142)]进行了分割，将人体从背景中分离，以便描述符仅针对人体进行确定。
- en: 4.2 Classification
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 分类
- en: '[3DPC](#id1.1.id1) classification is the process of classifying the features
    of 3D objects and categorizing the classes to which they belong. The [3DPC](#id1.1.id1)
    segmentation types and classification are illustrated in Fig. [12](#S4.F12 "Figure
    12 ‣ 4.2 Classification ‣ 4 Applications of TL-based 3DPCs ‣ Advancing 3D Point
    Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey").
    In the diagram, it is shown how a building, with the help of [CNN](#id11.11.id11),
    can be classified based on different classifications e.g., concerning time (period),
    region, and structure. Recently, researchers have explored different ways of directly
    classifying [3DPC](#id1.1.id1), mainly to avoid information loss during the conversion
    of [3DPC](#id1.1.id1) into non-[3DPC](#id1.1.id1). Automated classification of
    point cloud data has great potential for various applications, but a limited number
    of labelled points can lead to overfitting and poor generalization in [ML](#id5.5.id5)
    models.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '[3DPC](#id1.1.id1) 分类是对 3D 对象特征进行分类并将其归类到相应类别的过程。[3DPC](#id1.1.id1) 分割类型和分类如图[12](#S4.F12
    "Figure 12 ‣ 4.2 Classification ‣ 4 Applications of TL-based 3DPCs ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")所示。图中展示了如何利用[CNN](#id11.11.id11)对建筑物进行分类，基于不同的分类标准，例如时间（时期）、区域和结构。最近，研究人员探索了直接对[3DPC](#id1.1.id1)进行分类的不同方法，主要是为了避免在将[3DPC](#id1.1.id1)转换为非[3DPC](#id1.1.id1)时信息的丢失。点云数据的自动分类在各种应用中具有巨大的潜力，但有限的标记点可能会导致过拟合和[ML](#id5.5.id5)模型的泛化能力差。'
- en: One of the pioneered works in the field is done by Qi et al. [[52](#bib.bib52)],
    reported being the first [DL](#id4.4.id4) model to classify the original [3DPC](#id1.1.id1)
    directly. Their PointNet model is based on [CNN](#id11.11.id11), and its success
    has attracted many works to follow their proposed architecture. [CNN](#id11.11.id11)
    has been widely used for classification tasks. For example, authors [[129](#bib.bib129)]
    have used [CNN](#id11.11.id11) with [DTL](#id6.6.id6) to classify improved fisher
    vectors used to represent [3DPC](#id1.1.id1). The work [[247](#bib.bib247)] introduces
    a method called Atrous XCRF that induces controlled noise by invoking conditional
    random field similarity penalties using nearby features to improve generalization.
    The method achieves high [OA](#id16.16.id16) and F1 score in a benchmark study
    using the ISPRS 3D labeling dataset and is on par with the current best model.
    Using [DTL](#id6.6.id6) with Bergen 2018 dataset shows improvement in accuracy,
    but more work is needed to address generalization issues in [DA](#id7.7.id7) and
    [DTL](#id6.6.id6). To overcome the requirement of large dataset, Lei et al. [[138](#bib.bib138)]
    have proposed a [3DPC](#id1.1.id1) classification method that integrates an improved
    [CNN](#id11.11.id11) into [DTL](#id6.6.id6). They have used the DensNet201 model
    as a pretrained model for finding deep features able to accurately characterize
    the object. In addition, The authors [[218](#bib.bib218)] have proposed an ALS
    [3DPC](#id1.1.id1) classification method based on [DTL](#id6.6.id6) using [CNN](#id11.11.id11).
    In [[138](#bib.bib138)], an ALS [3DPC](#id1.1.id1) classification approach for
    integrating an enhanced fully-[CNN](#id11.11.id11) into [DTL](#id6.6.id6) with
    multi-view and multiscale and deep features is proposed. Typically, the shallow
    features of the ALS [3DPC](#id1.1.id1), such as height, intensity, and curvature
    change, are extracted to generate feature maps by multiscale voxel and multi-view
    projection. Second, these feature maps are fed into the pretrained DenseNet201
    model to derive deep features. Experimental results show that [OA](#id16.16.id16)
    and the average F1 scores obtained by the proposed method are 89.84% and 83.62%,
    respectively.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的开创性工作之一由 Qi 等人完成 [[52](#bib.bib52)]，报告称这是第一个直接对原始[3DPC](#id1.1.id1)进行分类的[DL](#id4.4.id4)模型。他们的
    PointNet 模型基于[CNN](#id11.11.id11)，其成功吸引了许多后续工作跟随其提出的架构。[CNN](#id11.11.id11)已广泛用于分类任务。例如，作者
    [[129](#bib.bib129)] 使用了带有[DTL](#id6.6.id6)的[CNN](#id11.11.id11)来分类改进的费舍尔向量，这些向量用于表示[3DPC](#id1.1.id1)。工作
    [[247](#bib.bib247)] 介绍了一种名为 Atrous XCRF 的方法，通过调用条件随机场相似性惩罚来引入受控噪声，利用附近的特征来改善泛化。该方法在使用
    ISPRS 3D 标注数据集的基准研究中取得了高[OA](#id16.16.id16)和 F1 分数，与当前最佳模型相当。使用[DTL](#id6.6.id6)和
    Bergen 2018 数据集显示准确性有所提高，但仍需进一步解决[DA](#id7.7.id7)和[DTL](#id6.6.id6)中的泛化问题。为了克服对大型数据集的需求，Lei
    等人 [[138](#bib.bib138)] 提出了一个将改进的[CNN](#id11.11.id11)集成到[DTL](#id6.6.id6)中的[3DPC](#id1.1.id1)分类方法。他们使用了
    DensNet201 模型作为预训练模型，用于寻找能够准确表征对象的深层特征。此外，作者 [[218](#bib.bib218)] 提出了一个基于[DTL](#id6.6.id6)的
    ALS [3DPC](#id1.1.id1)分类方法，该方法使用[CNN](#id11.11.id11)。在 [[138](#bib.bib138)] 中，提出了一种将增强的全[CN](#id11.11.id11)集成到[DTL](#id6.6.id6)中的
    ALS [3DPC](#id1.1.id1)分类方法，该方法结合了多视角、多尺度和深层特征。通常，ALS [3DPC](#id1.1.id1)的浅层特征，如高度、强度和曲率变化，通过多尺度体素和多视角投影生成特征图。其次，这些特征图被输入到预训练的
    DenseNet201 模型中，以提取深层特征。实验结果显示，所提出方法的[OA](#id16.16.id16)和平均 F1 分数分别为 89.84% 和
    83.62%。
- en: '![Refer to caption](img/785d8654f41e3b6e8b0c5bbea9443c82.png)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/785d8654f41e3b6e8b0c5bbea9443c82.png)'
- en: 'Figure 12: The flow chart of the proposed classification method.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：所提分类方法的流程图。
- en: 4.3 Point-set registration
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 点集配准
- en: With the increasing number of 3D sensors and 3D data production, consolidating
    overlapping [3DPCs](#id1.1.id1), which is called registration, has become a significant
    issue in many applications [[248](#bib.bib248)]. Registration can be used online
    (e.g., in a [LiDAR](#id18.18.id18) SLAM pipeline for loop closure detection) or
    offline (e.g., in 3D reconstructions of RGB-D indoor scenes [[249](#bib.bib249)]
    or for outdoor [LiDAR](#id18.18.id18) map building for autonomous vehicles). However,
    [3DPC](#id1.1.id1) registration can be challenging with real-world scans because
    of noise, incomplete 3D scans, outliers, etc. There are numerous existing approaches
    to this issue; however, recently, [DL](#id4.4.id4) approaches have become very
    popular, especially for learning descriptors and computing transformations. These
    data-driven approaches, especially those that involve learning on large datasets
    with the ground truth pose as supervision, have effectively solved registration
    problems [[250](#bib.bib250), [237](#bib.bib237)]. Specifically, in [[237](#bib.bib237)],
    a registration-aided DA model for [3DPC](#id1.1.id1)-based place recognition is
    developed, called vLPD-Net. The method utilizes virtual large-scale point cloud
    descriptors and a structure-aware registration network, along with adversarial
    training, to minimize the gap between synthetic and real-world domains.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 随着3D传感器和3D数据生产数量的增加，合并重叠的[3DPCs](#id1.1.id1)，即所谓的配准，已成为许多应用中的一个重要问题[[248](#bib.bib248)]。配准可以在线使用（例如，在[LiDAR](#id18.18.id18)
    SLAM管道中用于环路闭合检测）或离线使用（例如，在RGB-D室内场景的3D重建[[249](#bib.bib249)]中，或用于自动驾驶车辆的户外[LiDAR](#id18.18.id18)地图构建）。然而，由于噪声、不完整的3D扫描、离群点等原因，[3DPC](#id1.1.id1)配准在现实世界扫描中可能具有挑战性。虽然已经有许多现有的方法来解决这个问题，但最近，[DL](#id4.4.id4)方法变得非常流行，特别是在学习描述符和计算变换方面。这些数据驱动的方法，尤其是那些在大规模数据集上进行学习并以真实姿态作为监督的，已经有效地解决了配准问题[[250](#bib.bib250),
    [237](#bib.bib237)]。具体来说，在[[237](#bib.bib237)]中，开发了一种用于[3DPC](#id1.1.id1)基础的地点识别的配准辅助DA模型，称为vLPD-Net。该方法利用虚拟的大规模点云描述符和结构感知配准网络，以及对抗训练，来缩小合成域和现实世界域之间的差距。
- en: 4.4 Scene understanding
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 场景理解
- en: The method proposed by Xu et al. [[82](#bib.bib82)] uses [DTL](#id6.6.id6) technique
    by inflating weights to transform an image-pretrained model into a [3DPC](#id1.1.id1)
    model. Authors in [[251](#bib.bib251)] have proposed a model ’Shape Self-Correction’
    for [3DPC](#id1.1.id1) analysis where they have explored how unlabeled data can
    be utilized for a better [3DPC](#id1.1.id1) understanding. In addition, their
    approach can identify distorted points and automatically correct them by producing
    state-of-the-art results. Moreover, this model can be used as a pretrained model
    to be fine-tuned for related tasks. Going forward, Zhang et al. [[252](#bib.bib252)]
    also tried to exploit a pretrained large-scale 2D dataset to generalize it to
    [3DPC](#id1.1.id1) understanding using a [contrastive vision-language pretraining](#id24.24.id24)
    ([CLIP](#id24.24.id24)). Their model, PointCLIP, projects [3DPC](#id1.1.id1) into
    multi-view depth maps by encoding it, and successfully, view-wise zeroshot prediction
    is aggregated to transfer knowledge from 2D to 3D. Furthermore, many works exploit
    unsupervised approaches for [3DPC](#id1.1.id1) understanding. For example, Xie
    et al. [[117](#bib.bib117)] have suggested an unsupervised pretraining framework.
    To avoid the domain gap and utilize their approach across the different domains
    in the real world, they have directly made use of a complex network to pretrain
    the model. The major contribution they have is the examination of the transferability
    of learned representations generalization across the domain for [3DPC](#id1.1.id1)
    understanding. In addition, their architecture can also represent point-level
    information by capturing local features. To further exploit the geometric structure
    of the [3DPC](#id1.1.id1) and relationships of local features, Liu et al. [[253](#bib.bib253)]
    have suggested a convolution-based geometric-aware approach. Moreover, they proposed
    a filtering mechanism that identifies noise and outliers for a high-level understanding.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 徐等人提出的方法[[82](#bib.bib82)]利用[DTL](#id6.6.id6)技术，通过膨胀权重将图像预训练模型转换为[3DPC](#id1.1.id1)模型。[[251](#bib.bib251)]中的作者提出了一种“形状自我修正”模型，用于[3DPC](#id1.1.id1)分析，他们探讨了如何利用未标记数据来更好地理解[3DPC](#id1.1.id1)。此外，他们的方法可以识别失真点并通过产生最先进的结果自动纠正这些点。此外，该模型还可以作为预训练模型用于相关任务的微调。未来，张等人[[252](#bib.bib252)]也尝试利用预训练的大规模2D数据集，将其推广到[3DPC](#id1.1.id1)理解中，使用了[对比视觉-语言预训练](#id24.24.id24)（[CLIP](#id24.24.id24)）。他们的模型PointCLIP通过编码将[3DPC](#id1.1.id1)投影到多视图深度图中，并成功地将视图级别的零样本预测聚合，以将知识从2D转移到3D。此外，许多研究利用无监督方法来理解[3DPC](#id1.1.id1)。例如，谢等人[[117](#bib.bib117)]建议了一种无监督预训练框架。为了避免领域差距，并在现实世界中的不同领域中利用他们的方法，他们直接使用复杂网络对模型进行预训练。他们的主要贡献是研究了学习的表示在领域间的迁移性，以便于[3DPC](#id1.1.id1)理解。此外，他们的架构还通过捕捉局部特征来表示点级信息。为了进一步利用[3DPC](#id1.1.id1)的几何结构和局部特征的关系，刘等人[[253](#bib.bib253)]提出了一种基于卷积的几何感知方法。此外，他们还提出了一种过滤机制，用于识别噪声和异常值，以实现更高层次的理解。
- en: 4.5 Object detection and denoising
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 对象检测与去噪
- en: The exploitation of [DTL](#id6.6.id6) for various applications of [3DPC](#id1.1.id1)
    in 3D object detection and recognition has recently attracted researchers, ranging
    from medical science to biological research and robotics. For instance, authors
    in [[145](#bib.bib145)] have used [DTL](#id6.6.id6) for protein structure indexing
    using [3DPC](#id1.1.id1). The suggested approach saves training time by fine-tuning
    the pretrained network, using Euclidean distance to extract shape-matching, with
    generic 3D objects. Furthermore, authors in [[131](#bib.bib131)] have proposed
    a detection method incorporating instance segmentation and [DTL](#id6.6.id6) for
    overhead catenary height detection of a tunnel using a 3D [3DPC](#id1.1.id1) tunnel
    dataset. To recognize human activities, Sidor et al. [[142](#bib.bib142)] have
    suggested an approach by transforming depth maps into [3DPC](#id1.1.id1). The
    authors have used [DTL](#id6.6.id6) combined with multiple networks to improve
    the classification mechanism based on BiLSTM. In [[143](#bib.bib143)], [3DPC](#id1.1.id1)
    semantic segmentation based on [DTL](#id6.6.id6) is used for 3D object detection.
    The advantage of this method is that it reduces large-scale training datasets
    requirement, and consequently, the training duration is reduced as [DTL](#id6.6.id6)
    exploits knowledge from prior classification tasks, aiding object detection after
    preprocessing, thereby promoting cross-task generalization.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 对[DTL](#id6.6.id6)在3D物体检测和识别中各种[3DPC](#id1.1.id1)应用的开发最近吸引了研究人员的关注，从医学科学到生物研究再到机器人技术。例如，[[145](#bib.bib145)]中的作者使用[DTL](#id6.6.id6)进行基于[3DPC](#id1.1.id1)的蛋白质结构索引。建议的方法通过微调预训练网络，使用欧几里得距离提取形状匹配，应用于通用3D物体，从而节省训练时间。此外，[[131](#bib.bib131)]中的作者提出了一种结合实例分割和[DTL](#id6.6.id6)的检测方法，用于通过3D[3DPC](#id1.1.id1)隧道数据集检测隧道的悬挂高度。为了识别人体活动，Sidor等人[[142](#bib.bib142)]建议通过将深度图转换为[3DPC](#id1.1.id1)来实现。作者结合多个网络使用[DTL](#id6.6.id6)以改进基于BiLSTM的分类机制。在[[143](#bib.bib143)]中，基于[DTL](#id6.6.id6)的[3DPC](#id1.1.id1)语义分割用于3D物体检测。该方法的优点是减少了大规模训练数据集的需求，从而缩短了训练时间，因为[DTL](#id6.6.id6)利用了之前分类任务的知识，帮助在预处理后进行物体检测，从而促进了跨任务的泛化。
- en: Denoising is an important [3DPC](#id1.1.id1) task to remove noise usually found
    in acquiring images from scanning devices. These noises can influence many downstream
    tasks such as segmentation, detection, reconstruction and classification, etc.
    To remove these noises at different scales of [3DPC](#id1.1.id1) data, several
    algorithms have been suggested. The first [DTL](#id6.6.id6)-based [3DPC](#id1.1.id1)
    denoising scheme was proposed in [[254](#bib.bib254)]. It involved denoising patches
    of points by projecting them onto a learned local frame and repositioning the
    points onto the surface using a [CNN](#id11.11.id11). Some interesting [3DPC](#id1.1.id1)
    denoising approaches include score-based [3DPC](#id1.1.id1) denoising [[255](#bib.bib255)],
    graph Laplacian regularization [[256](#bib.bib256)], bipartite graph approximation
    and total variation [[257](#bib.bib257)], weighted multi-projection [[258](#bib.bib258)],
    and feature graph learning [[259](#bib.bib259)], etc.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪是一个重要的[3DPC](#id1.1.id1)任务，用于去除通常在扫描设备图像采集中发现的噪声。这些噪声可能会影响许多下游任务，如分割、检测、重建和分类等。为了在不同尺度的[3DPC](#id1.1.id1)数据上去除这些噪声，已经提出了几种算法。第一个基于[DTL](#id6.6.id6)的[3DPC](#id1.1.id1)去噪方案在[[254](#bib.bib254)]中提出。它通过将点补丁投影到学习的局部框架上并使用[CNN](#id11.11.id11)将点重新定位到表面上进行去噪。一些有趣的[3DPC](#id1.1.id1)去噪方法包括基于分数的[3DPC](#id1.1.id1)去噪[[255](#bib.bib255)]、图拉普拉斯正则化[[256](#bib.bib256)]、二分图逼近和全变差[[257](#bib.bib257)]、加权多投影[[258](#bib.bib258)]和特征图学习[[259](#bib.bib259)]等。
- en: 4.6 Upsampling and donwsampling
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 上采样和下采样
- en: Point clouds obtained through 3D scanning are often incomplete, unevenly distributed,
    and corrupted by noise. Upsampling, on the other hand, involves generating a dense
    set of points that not only restores uniformity and proximity to the surface but
    can also address small gaps in the data, all through a single network. For instance,
    authors have proposed Segnet [[260](#bib.bib260)], and they designed decoder to
    upsamples their input feature map(s) having lower resolution enabling non-linear
    upsampling to produce dense feature maps. In a similar way, Eltner et al. [[261](#bib.bib261)]
    have used an encoder that extracts a low-resolution activation map while the decoder
    upsamples it to obtain a pixel-wise classification. More recently, an upsampling
    method for [3DPC](#id1.1.id1) to reduce human annotation cost is presented in
    [[262](#bib.bib262)]. They relied solely on the upsampling operation to perform
    effective feature learning of [3DPCs](#id1.1.id1). Simialrly, Imad et al. [[130](#bib.bib130)]
    have added more convolutional layers to the decoder stage, which has been coupled
    with upsampling layers to increase the size of the spatial tensor and generate
    high-resolution segmentation outputs.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 3D 扫描获得的点云通常是不完整的、分布不均且受到噪声的干扰。而上采样涉及生成一个密集的点集，这不仅恢复了表面的均匀性和接近度，还可以通过单个网络解决数据中的小间隙。例如，作者提出了
    Segnet [[260](#bib.bib260)]，他们设计了一个解码器来上采样其输入特征图（分辨率较低），实现非线性上采样以生成密集的特征图。类似地，Eltner
    等人 [[261](#bib.bib261)] 使用了一个编码器来提取低分辨率的激活图，而解码器则对其进行上采样以获得逐像素分类。最近，[[262](#bib.bib262)]
    提出了用于[3DPC](#id1.1.id1)的上采样方法，以减少人工标注成本。他们完全依赖上采样操作来有效地学习[3DPCs](#id1.1.id1)的特征。类似地，Imad
    等人 [[130](#bib.bib130)] 在解码阶段增加了更多的卷积层，并与上采样层结合，以增大空间张量的尺寸并生成高分辨率的分割输出。
- en: Down-sampling point clouds refers to the process of reducing the number of points
    contained within them. This is commonly done to decrease the processing time required,
    or to select a specific number of points for use in training, among other purposes.Moreover,
    downsampling is also used to transform [3DPC](#id1.1.id1) into 2D representations.
    Typically, in [[263](#bib.bib263)], a 2D scene modelling method is described that
    effectively and meaningfully transforms the 3D data to a 2.5D representation and
    then to a 2D grid map. In addition to this, downsampling has been used in 3D data
    processing to remove the noise. For example, [[264](#bib.bib264)] have used growing
    neural gas (GNG) based downsampling technique for noise removal. However, downsampling
    necessitates careful consideration of whether the points are significant for the
    output, so that all important points are passed to the next layer and not removed.
    To this end, critical points layer (CPL) has been proposed in [[265](#bib.bib265)],
    an adaptive downsampling layer that learns to downsize an unordered [3DPC](#id1.1.id1)
    while keeping the crucial (critical) points. Downsampling has also been used in
    building and automation, for instance, a downsampling technique to optimize Terrestrial
    laser scanning (TLS) datset is suggested in [[266](#bib.bib266)].
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 点云的下采样指的是减少其中点数的过程。这通常是为了减少所需的处理时间，或者为了选择特定数量的点用于训练等其他目的。此外，下采样还用于将[3DPC](#id1.1.id1)转换为
    2D 表示。通常，在 [[263](#bib.bib263)] 中，描述了一种有效且有意义地将 3D 数据转换为 2.5D 表示再到 2D 网格图的方法。此外，下采样还被用于
    3D 数据处理以去除噪声。例如，[[264](#bib.bib264)] 使用了基于生长神经气体 (GNG) 的下采样技术来去除噪声。然而，下采样需要仔细考虑点是否对输出重要，以确保所有重要点都传递到下一层而不会被删除。为此，[[265](#bib.bib265)]
    提出了关键点层 (CPL)，这是一种自适应下采样层，学习在保持关键（重要）点的同时对无序[3DPC](#id1.1.id1)进行缩小。下采样还用于建筑和自动化中，例如，[[266](#bib.bib266)]
    中建议了一种优化地面激光扫描 (TLS) 数据集的下采样技术。
- en: 'Table 7: Summary of [DTL](#id6.6.id6)-based [3DPC](#id1.1.id1) frameworks describing
    different tasks.'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 总结了基于[DTL](#id6.6.id6)的[3DPC](#id1.1.id1)框架，描述了不同任务。'
- en: '| Ref. | MoN | Task/application | DTL | Limitations | Contribution/advantage/key
    point |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | MoN | 任务/应用 | DTL | 局限性 | 贡献/优势/关键点 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| [[117](#bib.bib117)] | ResNet-34, Pointcontrast | [3DPC](#id1.1.id1) understanding
    | FT | Testing for generalizability across multiple datasets and verifying computational
    costs after applying DTL were not conducted. | transferability of learned representation
    in [3DPCs](#id1.1.id1) to high-level scene understanding. |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| [[117](#bib.bib117)] | ResNet-34, Pointcontrast | [3DPC](#id1.1.id1) 理解 |
    FT | 未对多个数据集的普遍性进行测试，也未验证应用 DTL 后的计算成本。 | 将 [3DPCs](#id1.1.id1) 中学到的表示转移到高层次场景理解。
    |'
- en: '| [[128](#bib.bib128)] | UFF | Segmentation and classification | UDTL | they
    optimize the classifiers in a single stage, a multi-stage classifier that can
    reduce the redefined cost function could enhance the result. | Propose a learning
    system with an unsupervised feedforward feature (UFF) by incorporating encoder-decoder
    architecture. |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| [[128](#bib.bib128)] | UFF | 分割与分类 | UDTL | 他们在单一阶段优化分类器，多阶段分类器可以减少重新定义的成本函数，从而提高结果。
    | 提出了一个结合了编码器-解码器架构的无监督前馈特征（UFF）学习系统。 |'
- en: '| [[129](#bib.bib129)] | DeepLabv3+ | Semantic segmentation | ITL | Due to
    the terrestrial nature of the data acquisition, several parts of the orthophoto,
    notably blind spots were distorted by the orthorectification algorithm | An improved
    automated procedure for [3DPC](#id1.1.id1) semantic segmentation, especially for
    photogrammetric data. |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| [[129](#bib.bib129)] | DeepLabv3+ | 语义分割 | ITL | 由于数据采集的地面特性，正射影像的几个部分，特别是盲点，被正射校正算法扭曲
    | 改进的自动化程序用于 [3DPC](#id1.1.id1) 语义分割，特别是用于摄影测量数据。 |'
- en: '| [[130](#bib.bib130)] | AE | [3DPC](#id1.1.id1) semantic segmentation for
    3D object detection | ITL | The method does not work for raw [3DPCs](#id1.1.id1).
    | A [3DPC](#id1.1.id1) is projected into a birds-eye-view by which the amount
    of annotated data and time required for training has been minimized |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| [[130](#bib.bib130)] | AE | [3DPC](#id1.1.id1) 语义分割用于 3D 物体检测 | ITL | 该方法不适用于原始
    [3DPCs](#id1.1.id1)。 | 将 [3DPC](#id1.1.id1) 投影到鸟瞰图中，从而最小化了标注数据的数量和训练所需的时间。 |'
- en: '| [[131](#bib.bib131)] | 3D BoNet + MSG structure | [3DPC](#id1.1.id1) instance
    segmentation | ITL | The proposed RKT method is costly | New tunnel dataset is
    generated for [3DPC](#id1.1.id1) segmentation. |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| [[131](#bib.bib131)] | 3D BoNet + MSG 结构 | [3DPC](#id1.1.id1) 实例分割 | ITL
    | 提出的 RKT 方法成本高 | 为 [3DPC](#id1.1.id1) 分割生成了新的隧道数据集。 |'
- en: '| [[133](#bib.bib133)] | ConvPoint network, MLP | [3DPC](#id1.1.id1) segmentation
    for classification | ITL | Semantic interpretation of objects with fewer visual
    features is missing | automation of extraction and labeling of memorial objects
    from cultural heritage sites using [3DPC](#id1.1.id1) data. |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| [[133](#bib.bib133)] | ConvPoint 网络, MLP | [3DPC](#id1.1.id1) 分类的分割 | ITL
    | 对视觉特征较少的对象的语义解释缺失 | 使用 [3DPC](#id1.1.id1) 数据从文化遗产遗址中自动提取和标记纪念物体。 |'
- en: '| [[138](#bib.bib138)] | DenseNet201 | [3DPC](#id1.1.id1) classification |
    TTL | Algorithms are not integrated efficiently, which makes the feature maps
    generation complex | This algorithm can quickly and accurately extract the features
    and reduce the effect of ground object size on classification results. |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bib138)] | DenseNet201 | [3DPC](#id1.1.id1) 分类 | TTL | 算法集成效率不高，导致特征图生成复杂
    | 该算法可以快速准确地提取特征，并减少地面物体大小对分类结果的影响。 |'
- en: '| [[140](#bib.bib140)] | RandLA-Net | [3DPC](#id1.1.id1) semantic segmentation
    | ITL | The SLAM algorithm is not efficient for indoor spaces, [LiDAR](#id18.18.id18)
    can be located to a limited height only, which provides a minimal view for acquired
    data | Robot dog has been exploited for data acquisition instead of human which
    saves time and minimize the monetary efforts. |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| [[140](#bib.bib140)] | RandLA-Net | [3DPC](#id1.1.id1) 语义分割 | ITL | SLAM
    算法在室内空间的效率不高，[LiDAR](#id18.18.id18) 只能定位到有限的高度，这为获取的数据提供了最小的视图 | 机器人狗被用来代替人类进行数据采集，从而节省时间并减少经济投入。
    |'
- en: '| [[141](#bib.bib141)] | CNN | Classification | ITL | The [3DPC](#id1.1.id1)
    data is used instead of street-view images, which needs open space. Buildings
    with irregularities and reinforcing shapes can misclassify as soft-story buildings.
    | To accurately classify the soft-story building using CNN model pretrained with
    DTL |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| [[141](#bib.bib141)] | CNN | 分类 | ITL | 使用 [3DPC](#id1.1.id1) 数据代替街景图像，这需要开阔空间。具有不规则和加固形状的建筑物可能会被误分类为软层建筑。
    | 使用经过 DTL 预训练的 CNN 模型准确分类软层建筑。 |'
- en: '| [[142](#bib.bib142)] | BiLSTM | Segmentation and classification | TTL | eigenvalue-based
    descriptors for human activity recognition and other state-of-the-art methods
    are not discussed | With the help of [3DPCs](#id1.1.id1) and VFH descriptor, human
    activities are recognized. |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| [[142](#bib.bib142)] | BiLSTM | 分割和分类 | TTL | 没有讨论基于特征值的人类活动识别和其他先进方法 | 借助[3DPCs](#id1.1.id1)和
    VFH 描述符，识别了人类活动。'
- en: '| [[143](#bib.bib143)] | SFUDMA | Object recognition | UDA | The proposed Virtual
    Domain Modeling needs to be deployed for a variety of tasks. | They successfully
    achieve the goal of distribution alignment between the SD and TD by training deep
    networks without accessing the SD data |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| [[143](#bib.bib143)] | SFUDMA | 物体识别 | UDA | 提议的虚拟领域建模需要用于各种任务。 | 他们通过训练深度网络而无需访问
    SD 数据，成功实现了 SD 和 TD 之间的分布对齐目标'
- en: '| [[144](#bib.bib144)] | DenesNet-121 | Depth estimation | ITL | Testing for
    generalizability across multiple datasets was not performed. | Depth and surface
    estimation with a higher resolution |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| [[144](#bib.bib144)] | DenesNet-121 | 深度估计 | ITL | 未对多个数据集的泛化能力进行测试。 | 高分辨率的深度和表面估计'
- en: '| [[145](#bib.bib145)] | GLT4IP, pointNet | Protein structure indexing | FT
    | The validation of real-time performance improvements through transfer learning
    was not conducted. | DTL based indexing of protein structures using [3DPCs](#id1.1.id1)
    |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| [[145](#bib.bib145)] | GLT4IP, pointNet | 蛋白质结构索引 | FT | 没有进行通过迁移学习验证实时性能改进。
    | 使用[3DPCs](#id1.1.id1)的 DTL 基础蛋白质结构索引'
- en: '| [[146](#bib.bib146)] | Endo-SfMLearner | Depth estimation | UTL | Unresolved
    aspects: enhancing data adaptability, resolving issues, integrating with segmentation,
    abnormality detection, and classification tasks for improved performance. | Presented
    a new dataset for endoscopy, “EndoSLAM” |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| [[146](#bib.bib146)] | Endo-SfMLearner | 深度估计 | UTL | 未解决的方面：提高数据适应性、解决问题、与分割、异常检测以及分类任务集成以提高性能。
    | 提出了一个用于内窥镜的新数据集，“EndoSLAM”'
- en: '| [[148](#bib.bib148)] | GMM | Segmentation and classification | ITL | Further
    research into novel pretext tasks tailored specifically to the idiosyncrasies
    of 3D data has not been conducted. | For representation learning, the method does
    not require any procedure like transformation/data augmentation. |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| [[148](#bib.bib148)] | GMM | 分割和分类 | ITL | 尚未对专门针对 3D 数据特征的新预训练任务进行进一步研究。
    | 对于表示学习，该方法不需要任何如变换/数据增强的过程。'
- en: '| [[149](#bib.bib149)] | POCO | 3D orientation, and classification | ITL |
    Orientation accuracy can be improved further | Experimentally shown how DTL can
    be used for a model to serve as a platform for 3D object’s orientation representation.
    |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| [[149](#bib.bib149)] | POCO | 3D 定向和分类 | ITL | 定向精度可以进一步提高 | 实验展示了如何使用 DTL
    作为平台对 3D 物体的定向表示。'
- en: '| [[150](#bib.bib150)] | YOLO | Object detection | ITL | Method heavily relies
    on [LiDAR](#id18.18.id18) data only and may not be that beneficial for another
    domain of image dataset | Suggest a method efficient enough and compatible for
    any [LiDARs](#id18.18.id18), even containing a different number of channels. Moreover,
    they do not require a re-training step. |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| [[150](#bib.bib150)] | YOLO | 物体检测 | ITL | 方法严重依赖于[LiDAR](#id18.18.id18)数据，对于其他图像数据集领域可能效果不佳
    | 建议一种足够高效且兼容任何[LiDARs](#id18.18.id18)的方法，即使包含不同数量的通道。此外，它们不需要重新训练步骤。'
- en: '| [[151](#bib.bib151)] | GoogleNet, AlexNet, VGG | Classification | ITL | verification
    of the degree of dependence of the DNN architecture on the characteristics of
    the mechanical system subject to degradation | representation of punch deformation
    with depth and normal vector maps (DNVMs) obtained from 3D scan [3DPCs](#id1.1.id1)
    |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| [[151](#bib.bib151)] | GoogleNet, AlexNet, VGG | 分类 | ITL | 验证 DNN 架构对机械系统退化特征的依赖程度
    | 通过从 3D 扫描[3DPCs](#id1.1.id1)中获得的深度和法向量图（DNVMs）表示冲击变形'
- en: '| [[221](#bib.bib221)] | Kd-network | Shape classification, feature recognition
    | ITL | This method not applicable for any cattle and may produce heavy mistakes
    and significant errors for adult Qinghai yaks | Livestock can be observed without
    any physical involvement of human |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| [[221](#bib.bib221)] | Kd-network | 形状分类、特征识别 | ITL | 该方法不适用于任何牲畜，并且可能对成年青海牦牛产生重大错误
    | 可以在不涉及人为干预的情况下观察牲畜'
- en: '| [[260](#bib.bib260)] | Segnet | Upsampling | UDTL | This method needs to
    separate the roles between the optimizer and the model to achieve the desired
    outcome. | It is much smaller and can be trained end-to-end using stochastic gradient
    descent. It has been engineered to be effective for memory and processing time
    during inference. |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| [[260](#bib.bib260)] | Segnet | 上采样 | UDTL | 该方法需要将优化器和模型的角色分开，以实现预期结果。 |
    它更小巧，可以通过随机梯度下降进行端到端训练。在推理过程中，它被设计为在内存和处理时间上有效。 |'
- en: '| [[262](#bib.bib262)] | UAE | upsampling | SSTL | The complexity increased
    with the double-head architecture. Additionally, mechanical systems with different
    geometry and material from the punch tool were not investigated. | It allows classification
    and segmentation tasks to benefit from pretrained models. And it reduces the human
    annotation cost. |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| [[262](#bib.bib262)] | UAE | 上采样 | SSTL | 双头架构增加了复杂性。此外，未研究与冲击工具几何形状和材料不同的机械系统。
    | 它允许分类和分割任务从预训练模型中受益，并减少人工标注成本。 |'
- en: '| [[265](#bib.bib265)] | CPL | downsampling | DA | The complexity of CP-Net
    is high, potentially leading to increased computation costs. | an adaptive downsampling
    layer that learns to downsize an unordered [3DPC](#id1.1.id1) while keeping the
    crucial (critical) points |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| [[265](#bib.bib265)] | CPL | 下采样 | DA | CP-Net 的复杂性很高，可能导致计算成本增加。 | 一种自适应下采样层，学习在保持关键点的同时缩小无序的[3DPC](#id1.1.id1)。'
- en: '| [[254](#bib.bib254)] | PointProNets | denoising | S | The [3DPC](#id1.1.id1)
    data utilized in the method was created by adding random noise; hence it is ineffective
    for [3DPCs](#id1.1.id1) that were legitimately created using [LiDAR](#id18.18.id18).
    | The first TL based [3DPC](#id1.1.id1) denoising scheme. |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| [[254](#bib.bib254)] | PointProNets | 去噪 | S | 方法中使用的[3DPC](#id1.1.id1)数据通过添加随机噪声创建，因此对于通过[LiDAR](#id18.18.id18)合法创建的[3DPCs](#id1.1.id1)效果不佳。
    | 首个基于 TL 的 [3DPC](#id1.1.id1) 去噪方案。 |'
- en: 'Abbreviations: Unsupervised TL (UTL); supervised DTL (STL); self- supervised
    DTL (SSTL); supervised (S); upsampling auto-encoder (UAE); model or network (MoN);
    fine-tuning (FT).'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 缩写：无监督 TL (UTL)；监督 DTL (STL)；自监督 DTL (SSTL)；监督 (S)；上采样自编码器 (UAE)；模型或网络 (MoN)；微调
    (FT)。
- en: 4.7 Scene generation
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 场景生成
- en: 3D point cloud scene generation or reconstruction is of paramount importance
    in various fields, including robotics, autonomous driving, virtual reality, and
    architecture. By capturing the precise three-dimensional coordinates of a scene,
    3D point clouds provide a detailed and accurate representation of the environment.
    This capability is crucial for tasks such as object recognition, navigation, and
    interaction within a space, enabling robots and autonomous vehicles to perceive
    and understand their surroundings effectively. In architecture and construction,
    3D reconstruction facilitates accurate modeling and analysis of structures, aiding
    in design, inspection, and renovation processes. Furthermore, in virtual reality
    and gaming, 3D point cloud generation enhances the realism and immersion of digital
    environments, providing users with a more engaging and interactive experience.
    The ability to create detailed and accurate 3D models from point clouds revolutionizes
    various industries by improving precision, efficiency, and safety in numerous
    applications.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 点云场景生成或重建在机器人、自动驾驶、虚拟现实和建筑等多个领域中至关重要。通过捕捉场景的精确三维坐标，3D 点云提供了对环境的详细和准确的表示。这种能力对于物体识别、导航和空间内互动等任务至关重要，使机器人和自动驾驶车辆能够有效地感知和理解其周围环境。在建筑和施工中，3D
    重建有助于结构的准确建模和分析，支持设计、检查和翻新过程。此外，在虚拟现实和游戏中，3D 点云生成增强了数字环境的真实性和沉浸感，为用户提供了更具参与感和互动性的体验。从点云创建详细和准确的
    3D 模型的能力，通过提高各种应用中的精确性、效率和安全性，彻底改变了多个行业。
- en: The SGFormer study [[267](#bib.bib267)] introduces a Semantic Graph Transformer
    for point cloud-based 3D scene graph generation, addressing the limitations of
    graph convolutional networks (GCNs) by using Transformer layers for global information
    passing. The model includes graph embedding and semantic injection layers to enhance
    object visual features with linguistic knowledge from large-scale language models.
    Benchmarked on the 3DSSG dataset, SGFormer demonstrates significant improvements
    in relationship prediction over state-of-the-art methods. The Sat2Scene study
    [[268](#bib.bib268)] proposes a novel architecture for direct 3D scene generation
    from satellite imagery using diffusion models and neural rendering techniques.
    This method generates texture colors at the point level and transforms them into
    a scene representation for rendering arbitrary views, showing proficiency in generating
    photo-realistic street-view image sequences and cross-view urban scenes, validated
    through experiments on city-scale datasets.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: SGFormer 研究[[267](#bib.bib267)]介绍了一个用于点云基础3D场景图生成的语义图转换器，利用Transformer层进行全局信息传递，解决了图卷积网络（GCNs）的局限性。该模型包括图嵌入和语义注入层，以增强物体视觉特征，结合来自大规模语言模型的语言知识。在3DSSG数据集上进行基准测试，SGFormer在关系预测上相较于最先进的方法有了显著提升。Sat2Scene
    研究[[268](#bib.bib268)]提出了一种新的架构，用于从卫星影像直接生成3D场景，使用扩散模型和神经渲染技术。这种方法在点级别生成纹理颜色，并将其转换为场景表示以渲染任意视角，显示出在生成照片级真实街景图像序列和跨视角城市场景方面的熟练度，通过城市规模数据集的实验进行了验证。
- en: The ART3D study [[269](#bib.bib269)] introduces a framework combining diffusion
    models and 3D Gaussian splatting for 3D artistic scene generation, utilizing depth
    information and initial artistic images to generate point cloud maps and enhance
    3D scene consistency. ART3D shows superior performance in content and structural
    consistency metrics compared to existing methods. Liu et al. [[16](#bib.bib16)]
    propose a high-precision 3D building model generation method using multi-source
    3D data fusion, achieving state-of-the-art performance in multi-source 3D data
    quality evaluation and large-scale, high-precision building model generation.
    Chung et al. [[270](#bib.bib270)] present LucidDreamer, a domain-free 3D scene
    generation pipeline leveraging large-scale diffusion-based generative models,
    producing highly detailed Gaussian splats and outperforming previous methods in
    reconstruction quality. The S2HGrasp study [[271](#bib.bib271)] explores generating
    human grasps from single-view scene point clouds, addressing challenges of incomplete
    object point clouds and scene points, showcasing strong generalization capabilities
    and effective grasp generation. Lastly, the SGRec3D study [[272](#bib.bib272)]
    presents a self-supervised pre-training method for 3D scene graph prediction,
    achieving significant improvements in 3D scene graph prediction with reduced labeled
    data requirements during fine-tuning, demonstrating state-of-the-art performance.
    In [[273](#bib.bib273)], Liu et al. introduce the Pyramid Discrete Diffusion model
    (PDD), a framework employing scale-varied diffusion models to generate high-quality
    large-scale 3D scenes. Using a coarse-to-fine paradigm, PDD addresses the complexity
    and size challenges of 3D scenery data, particularly for outdoor scenes. The model
    demonstrates effective generation capabilities and data compatibility, allowing
    easy fine-tuning across different datasets.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: ART3D 研究[[269](#bib.bib269)]介绍了一个结合扩散模型和3D高斯点云的3D艺术场景生成框架，利用深度信息和初始艺术图像生成点云地图，并提升3D场景的一致性。与现有方法相比，ART3D在内容和结构一致性指标上表现出色。刘等人[[16](#bib.bib16)]提出了一种使用多源3D数据融合的高精度3D建筑模型生成方法，在多源3D数据质量评估和大规模高精度建筑模型生成方面取得了领先水平。钟等人[[270](#bib.bib270)]展示了LucidDreamer，一个不依赖领域的3D场景生成管道，利用大规模扩散生成模型，生成高度详细的高斯点云，并在重建质量上优于之前的方法。S2HGrasp
    研究[[271](#bib.bib271)]探讨了从单视角场景点云生成人体抓取，解决了不完整物体点云和场景点的挑战，展示了强大的泛化能力和有效的抓取生成。最后，SGRec3D
    研究[[272](#bib.bib272)]提出了一种自监督预训练方法用于3D场景图预测，通过减少标记数据需求在微调过程中取得了显著改进，展示了领先的性能。在[[273](#bib.bib273)]中，刘等人介绍了Pyramid
    Discrete Diffusion模型（PDD），这是一个采用尺度变化的扩散模型生成高质量大规模3D场景的框架。利用粗到精的范式，PDD应对了3D场景数据的复杂性和规模挑战，特别是户外场景。该模型展示了有效的生成能力和数据兼容性，使得在不同数据集上进行细化调整变得容易。
- en: 'Table [8](#S4.T8 "Table 8 ‣ 4.7 Scene generation ‣ 4 Applications of TL-based
    3DPCs ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning:
    A Comprehensive Survey") compares various studies on 3D scene generation and reconstruction,
    highlighting key aspects such as models used, datasets, contributions, performance
    values, and limitations. The models range from SGFormer, utilizing a Semantic
    Graph Transformer, to MS3DQE-Net for multi-source 3D data fusion, and Pyramid
    Discrete Diffusion (PDD) for large-scale scene generation. Datasets include 3DSSG,
    city-scale datasets from satellite imagery, artistic images with depth information,
    and MLS 3D point clouds.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '表[8](#S4.T8 "Table 8 ‣ 4.7 Scene generation ‣ 4 Applications of TL-based 3DPCs
    ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey")比较了各种3D场景生成和重建研究，突出了如使用的模型、数据集、贡献、性能值和局限性等关键方面。模型包括使用语义图变换器的SGFormer、用于多源3D数据融合的MS3DQE-Net，以及用于大规模场景生成的金字塔离散扩散（PDD）。数据集包括3DSSG、来自卫星影像的城市规模数据集、具有深度信息的艺术图像以及MLS
    3D点云。'
- en: 'Each study offers unique contributions: SGFormer excels in relationship prediction,
    Sat2Scene integrates diffusion models for urban scenes, and ART3D bridges artistic
    and realistic images. MS3DQE-Net focuses on high-precision building models, LucidDreamer
    achieves domain-free scene generation, S2HGrasp generates human grasps from incomplete
    point clouds, and SGRec3D enhances 3D scene graph prediction. Performance metrics
    show significant improvements, such as 40.94% R@50 for SGFormer, FID of 71.98
    for Sat2Scene, and PSNR of 34.24 for LucidDreamer. Limitations include over-smoothing
    in GCNs, handling significant view changes, and managing the complexity of 3D
    data. These challenges highlight areas for future research in 3D scene generation
    and reconstruction.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 每项研究都提供了独特的贡献：SGFormer在关系预测方面表现优异，Sat2Scene整合了用于城市场景的扩散模型，而ART3D在艺术和现实图像之间架起了桥梁。MS3DQE-Net专注于高精度建筑模型，LucidDreamer实现了无领域的场景生成，S2HGrasp从不完整的点云中生成人体抓取，而SGRec3D增强了3D场景图预测。性能指标显示出显著的改进，例如SGFormer的R@50为40.94%，Sat2Scene的FID为71.98，LucidDreamer的PSNR为34.24。局限性包括GCNs中的过度平滑、处理显著视角变化以及管理3D数据的复杂性。这些挑战突显了3D场景生成和重建领域未来研究的方向。
- en: 'Table 8: Comparison of 3D Scene Generation/Reconstruction Studies'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '表8: 3D场景生成/重建研究比较'
- en: '| Ref. | Model(s) Used | Dataset/Data Type | Main Contribution | Best Performance
    Value | Limitation |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: 参考 | 使用的模型 | 数据集/数据类型 | 主要贡献 | 最佳性能值 | 局限性
- en: '| [[267](#bib.bib267)] | SGFormer | 3DSSG | Semantic Graph TransFormer for
    3D scene graph generation | 40.94% R@50, 88.36% boost in complex scenes | Over-smoothing
    in GCNs |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| [[267](#bib.bib267)] | SGFormer | 3DSSG | 用于3D场景图生成的语义图变换器 | 40.94% R@50，复杂场景提升88.36%
    | GCNs中的过度平滑 |'
- en: '| [[268](#bib.bib268)] | Diffusion Models, Neural Rendering | City-scale datasets,
    satellite imagery | Direct 3D scene generation from satellite imagery | FID: 71.98,
    KID: 5.91 | Handling significant view changes and scene scale |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| [[268](#bib.bib268)] | 扩散模型，神经渲染 | 城市规模数据集，卫星影像 | 从卫星影像直接生成3D场景 | FID: 71.98,
    KID: 5.91 | 处理显著的视角变化和场景规模 |'
- en: '| [[269](#bib.bib269)] | Diffusion Models, 3D Gaussian Splatting | Artistic
    images, depth information | 3D artistic scene generation | PSNR: 24.041, SSIM:
    0.863, LPIPS: 0.214 | Bridging artistic and realistic images |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| [[269](#bib.bib269)] | 扩散模型，3D高斯点云渲染 | 艺术图像，深度信息 | 3D艺术场景生成 | PSNR: 24.041,
    SSIM: 0.863, LPIPS: 0.214 | 连接艺术和现实图像 |'
- en: '| [[16](#bib.bib16)] | MS3DQE-Net, Deep Learning | MLS 3D point clouds, 3D
    mesh data | High-precision 3D building model generation | State-of-the-art performance
    in data quality evaluation | Balancing local accuracy and overall integrity |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| [[16](#bib.bib16)] | MS3DQE-Net, 深度学习 | MLS 3D点云，3D网格数据 | 高精度3D建筑模型生成 | 数据质量评估中的最先进性能
    | 平衡局部精度和整体完整性 |'
- en: '| [[270](#bib.bib270)] | Diffusion-based Generative Model | Various datasets,
    VR content | Domain-free 3D scene generation | PSNR: 34.24, SSIM: 0.9781, LPIPS:
    0.0164 | Limited by training on 3D scan datasets |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| [[270](#bib.bib270)] | 基于扩散的生成模型 | 各种数据集，VR内容 | 无领域的3D场景生成 | PSNR: 34.24,
    SSIM: 0.9781, LPIPS: 0.0164 | 受限于3D扫描数据集的训练 |'
- en: '| [[271](#bib.bib271)] | S2HGrasp (Global Perception, DiffuGrasp) | S2HGD dataset,
    single-view scene point clouds | Generating human grasps from single-view point
    clouds | Contact Ratio: 99.41%, Volume: 6.58cm³ | Handling incomplete object point
    clouds |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| [[271](#bib.bib271)] | S2HGrasp（全局感知，DiffuGrasp） | S2HGD数据集，单视角场景点云 | 从单视角点云生成人体抓取
    | 接触比率: 99.41%，体积: 6.58cm³ | 处理不完整的物体点云 |'
- en: '| [[272](#bib.bib272)] | SGRec3D | Various 3D scene understanding datasets
    | Self-supervised pre-training for 3D scene graph prediction | +10% on object
    prediction, +4% on relationship prediction | Requires object-level and relationship
    labels |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| [[272](#bib.bib272)] | SGRec3D | 各种3D场景理解数据集 | 自监督预训练用于3D场景图预测 | 对物体预测提升+10%，对关系预测提升+4%
    | 需要物体级别和关系标签 |'
- en: '| [[273](#bib.bib273)] | Pyramid Discrete Diffusion (PDD) | Large-scale 3D
    scenes, outdoor scenes | Coarse-to-fine 3D scene generation | mIoU: 68.0, MA:
    85.7, F3D: 0.20 | Complexity and size of 3D scenery data |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| [[273](#bib.bib273)] | Pyramid Discrete Diffusion (PDD) | 大规模3D场景，户外场景 |
    从粗到细的3D场景生成 | mIoU: 68.0, MA: 85.7, F3D: 0.20 | 3D场景数据的复杂性和规模 |'
- en: 5 Open Challenges
  id: totrans-485
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个开放挑战
- en: '[DTL](#id6.6.id6) and DA can be effective techniques for [3DPC](#id1.1.id1)
    understanding, but they also come with their own set of challenges. Typically,
    [DL](#id4.4.id4) models require large amounts of labeled data to achieve high
    accuracy. However, in the case of [3DPC](#id1.1.id1) understanding, it can be
    challenging to obtain large amounts of labeled data due to the time-consuming
    and expensive process of manually annotating point clouds [[274](#bib.bib274)].
    Additionally, [3DPC](#id1.1.id1) data can be highly variable in terms of size,
    shape, and orientation. This variability can make it challenging to develop [DTL](#id6.6.id6)
    and DA models that can generalize well to new datasets [[275](#bib.bib275), [276](#bib.bib276)].
    Moreover, DA is needed when the distribution of the source and [TDs](#id9.9.id9)
    is different. In the context of [3DPC](#id1.1.id1) understanding, domain shift
    can occur due to changes in sensor modalities, lighting conditions, and other
    environmental factors. To that end, DA techniques must be used to adapt the model
    to the [TD](#id9.9.id9) and prevent performance degradation. Moving on, [3DPC](#id1.1.id1)
    data can contain complex geometric structures, such as non-uniformly distributed
    points, non-planar surfaces, and varying levels of detail. These complexities
    can make it challenging to develop effective [DL](#id4.4.id4) models that can
    accurately capture the structure and features of the data [[274](#bib.bib274)].
    [DTL](#id6.6.id6) and DA models can be computationally expensive, especially when
    dealing with large [3DPC](#id1.1.id1) datasets. This can limit their practical
    applicability and require specialized hardware to achieve acceptable performance
    [[277](#bib.bib277), [278](#bib.bib278)].'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '[DTL](#id6.6.id6)和DA可以作为[3DPC](#id1.1.id1)理解的有效技术，但它们也带来了自身的一系列挑战。通常，[DL](#id4.4.id4)模型需要大量标注数据才能实现高准确率。然而，在[3DPC](#id1.1.id1)理解的情况下，由于手动标注点云的过程耗时且昂贵，获得大量标注数据可能会非常困难[[274](#bib.bib274)]。此外，[3DPC](#id1.1.id1)数据在大小、形状和方向上可能高度变化。这种变化性可能使得开发能够很好地泛化到新数据集的[DTL](#id6.6.id6)和DA模型变得具有挑战性[[275](#bib.bib275),
    [276](#bib.bib276)]。此外，当源分布和[TDs](#id9.9.id9)不同的时候，需要使用DA。在[3DPC](#id1.1.id1)理解的背景下，由于传感器模态、光照条件和其他环境因素的变化，可能会出现领域偏移。为此，必须使用DA技术来适应[TD](#id9.9.id9)并防止性能下降。继续往下，[3DPC](#id1.1.id1)数据可能包含复杂的几何结构，如非均匀分布的点、非平面表面和不同层次的细节。这些复杂性可能使得开发能够准确捕捉数据结构和特征的有效[DL](#id4.4.id4)模型变得具有挑战性[[274](#bib.bib274)]。此外，[DTL](#id6.6.id6)和DA模型可能计算开销较大，特别是在处理大规模[3DPC](#id1.1.id1)数据集时。这可能限制它们的实际应用性，并需要专门的硬件来达到可接受的性能[[277](#bib.bib277),
    [278](#bib.bib278)]。'
- en: DA methods have shown significant improvements in various [ML](#id5.5.id5) and
    [CV](#id3.3.id3) tasks, such as classification, detection, and segmentation. However,
    to date, there are only a few methods that have been successful in applying DA
    directly to [3DPC](#id1.1.id1) data. The unique challenge in working with [3DPC](#id1.1.id1)
    data is its large amount of spatial geometric information, and the object’s semantics
    which depend on the regional geometric structures. This means that general-purpose
    DA methods, which focus on global feature alignment and neglect local geometric
    information, are not effective for aligning 3D domains, as stated in [[158](#bib.bib158)].
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: DA方法在各种[ML](#id5.5.id5)和[CV](#id3.3.id3)任务中显示了显著的改进，如分类、检测和分割。然而，到目前为止，只有少数方法成功地将DA直接应用于[3DPC](#id1.1.id1)数据。处理[3DPC](#id1.1.id1)数据的独特挑战在于其大量的空间几何信息以及对象的语义，这些都依赖于区域几何结构。这意味着通用的DA方法，侧重于全局特征对齐而忽略局部几何信息，对于对齐3D领域效果不佳，如[[158](#bib.bib158)]所述。
- en: 5.1 The problem of negative transfer
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 负迁移问题
- en: Negative transfer refers to a situation when knowledge transfer can negatively
    influence the model’s accuracy, possibly because the [SD](#id10.10.id10) and [TD](#id9.9.id9)
    are significantly different or they are supposed to perform different tasks [[215](#bib.bib215)].
    Moreover, not taking adequate advantage of the relationship between the [SD](#id10.10.id10)
    and [TD](#id9.9.id9) can also lead to negative transfer [[151](#bib.bib151)].
    In such situations, random and forced knowledge transfer without identifying the
    specific knowledge that can be useful and without considering "what", "when" and
    "how" to transfer can further propel negative transfer [[37](#bib.bib37)]. Hence,
    it can be inferred that high affinity among source and target tasks is of great
    importance for a successful [DTL](#id6.6.id6) and to avoid the negative transfer.
    To this end, researchers have suggested clustering methods for tasks and identifying
    similar tasks by keeping them in the same cluster. In addition to this, an approach
    to group learning tasks together is suggested in [[279](#bib.bib279)]. This approach
    enables what and when to transfer by identifying tasks within a group. Similarly,
    imbalanced training samples can increase negative transfer chances in a [UDA](#id8.8.id8)
    scenario. For Example, for [3DPC](#id1.1.id1) representation, Qin et al. [[158](#bib.bib158)]
    have suggested a 3D DA network and have argued that for domain alignment, unique
    parts can induce negative transfer and hence covering standard features in 3D
    space can help to avoid the negative transfer. Furthermore, to address negative
    transfer while dealing with different domains/modalities for [3DPC](#id1.1.id1),
    Gong et al. [[280](#bib.bib280)] have suggested a multi-[SD](#id10.10.id10) adaptation
    and label unification approach. They have used attention-guided adversarial alignment
    for a strong distribution alignment between the [SD](#id10.10.id10) and [TD](#id9.9.id9).
    Additionally, their proposed uncertainty maximization module limits the self-assured
    predictions for unlabeled samples in the [SDs](#id10.10.id10). To make their approach
    more promising, they have introduced a fusion module based on pseudo-labeling.
    Especially for unlabeled samples, they perform pseudo-labeling for the [SDs](#id10.10.id10)
    and all samples in the [TD](#id9.9.id9).
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 负迁移指的是知识迁移可能对模型的准确性产生负面影响的情况，这可能是因为[SD](#id10.10.id10)和[TD](#id9.9.id9)有显著不同，或者它们需要执行不同的任务[[215](#bib.bib215)]。此外，不充分利用[SD](#id10.10.id10)和[TD](#id9.9.id9)之间的关系也可能导致负迁移[[151](#bib.bib151)]。在这种情况下，如果在没有明确识别出有用知识且没有考虑“什么”，“何时”和“如何”迁移的情况下进行随机和强制的知识迁移，可能会进一步加剧负迁移[[37](#bib.bib37)]。因此，可以推断，源任务和目标任务之间的高度亲和性对于成功的[DTL](#id6.6.id6)和避免负迁移至关重要。为此，研究人员建议采用任务聚类方法，并通过将类似任务保存在同一聚类中来识别类似任务。此外，在[[279](#bib.bib279)]中建议了一种将学习任务分组的方法。这种方法通过识别组内的任务来实现“什么”和“何时”迁移。类似地，不平衡的训练样本在[UDA](#id8.8.id8)场景中可能增加负迁移的机会。例如，对于[3DPC](#id1.1.id1)表示，秦等人[[158](#bib.bib158)]建议了一种3D
    DA网络，并认为对于领域对齐，独特的部分可能会引起负迁移，因此覆盖3D空间中的标准特征有助于避免负迁移。此外，为了解决处理不同领域/模态的[3DPC](#id1.1.id1)时的负迁移，龚等人[[280](#bib.bib280)]建议了一种多[SD](#id10.10.id10)适应和标签统一的方法。他们使用了基于注意力的对抗性对齐，以实现[SD](#id10.10.id10)和[TD](#id9.9.id9)之间的强分布对齐。此外，他们提出的不确定性最大化模块限制了对[SDs](#id10.10.id10)中未标记样本的自信预测。为了使他们的方法更具前景，他们引入了基于伪标签的融合模块。特别是对于未标记样本，他们对[SDs](#id10.10.id10)和所有[TD](#id9.9.id9)样本进行伪标签化。
- en: 5.2 The problem of overfitting
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 过拟合问题
- en: The established successful model, such as PointCNN and other DNN models, despite
    their widespread use and success for [3DPC](#id1.1.id1) tasks, still face some
    potential challenges, especially when the dataset available is relatively small.
    One possible reason for this is significant parameters, even in several million.
    It requires a large amount of data to fit this many parameters adequately. Since
    [3DPC](#id1.1.id1) data suffer from the scarcity of labeled data, this limitation
    makes [3DPC](#id1.1.id1) tasks prone to overfitting, and poor generalization [[247](#bib.bib247)].
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 已建立的成功模型，如PointCNN和其他DNN模型，尽管它们在[3DPC](#id1.1.id1)任务中广泛使用且取得了成功，但在数据集相对较小时仍面临一些潜在挑战。一个可能的原因是参数非常多，甚至有数百万个。需要大量的数据来充分拟合如此多的参数。由于[3DPC](#id1.1.id1)数据在标记数据稀缺的情况下，这种限制使得[3DPC](#id1.1.id1)任务容易出现过拟合，且泛化能力差[[247](#bib.bib247)]。
- en: Several mechanisms help in tackling the overfitting issue. For instance, [[221](#bib.bib221)]
    have used [DTL](#id6.6.id6) on a Kd-network, they have frozen the weights of all
    network layers except the last fully connected layer, and only the fully connected
    layer is modified so that the gradients during backpropagation are not calculated,
    which can avoid the occurrence of overfitting and improve training efficiency
    [[281](#bib.bib281)]. Entropy minimization (EM), a regularization method, has
    also been suggested by [[234](#bib.bib234)] in addressing overfitting. EM helps
    improve generalization and, hence, robustness for preventing overfitting. Arief
    et al. proposed a solution to address the overfitting problem of PointCNN, a DNN-based
    model, by using the Atrous XCRF model, which is a combination of [conditional
    random field](#id29.29.id29) ([CRF](#id29.29.id29)) and [recurrent neural network](#id28.28.id28)
    ([RNN](#id28.28.id28)) [[247](#bib.bib247)]. DNN-based models such as PointCNN
    are highly susceptible to overfitting when the available dataset is relatively
    small, this is because of the large number of parameters in such models which
    requires a large number of training data. To tackle this problem, they introduced
    controlled noise during the training of a DNN-classifier, the method works by
    retraining a validated model using unlabeled test data. The training process is
    guided using the hierarchical structure of the [CRF](#id29.29.id29) penalty procedure.
    With the proposed algorithm XCRF and addition of A-XCRF layer, they were able
    to improve the model’s accuracy by utilizing the unlabelled data.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 解决过拟合问题的机制有多种。例如，[[221](#bib.bib221)] 在 Kd 网络上使用了[DTL](#id6.6.id6)，他们冻结了除最后一层全连接层之外的所有网络层的权重，只有全连接层会被修改，这样在反向传播过程中不会计算梯度，从而避免过拟合的发生并提高训练效率[[281](#bib.bib281)]。[[234](#bib.bib234)]
    还提出了熵最小化（EM）作为一种正则化方法来解决过拟合问题。EM 有助于提高泛化能力，从而增强防止过拟合的鲁棒性。Arief 等人提出了一种解决 PointCNN
    过拟合问题的方法，使用了 Atrous XCRF 模型，该模型是[条件随机场](#id29.29.id29)（[CRF](#id29.29.id29)）和[递归神经网络](#id28.28.id28)（[RNN](#id28.28.id28)）的结合[[247](#bib.bib247)]。如
    PointCNN 这类基于 DNN 的模型在可用数据集相对较小时非常容易过拟合，因为这类模型的参数数量庞大，需要大量的训练数据。为了解决这个问题，他们在 DNN
    分类器的训练过程中引入了控制噪声，该方法通过使用未标记的测试数据重新训练已验证的模型来工作。训练过程使用[CRF](#id29.29.id29)惩罚程序的层次结构进行指导。通过提出的算法
    XCRF 以及添加 A-XCRF 层，他们利用未标记的数据提高了模型的准确性。
- en: 5.3 Domain discrepancies
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 域间差异
- en: Recently, [UDA](#id8.8.id8) has gained a good deal of attraction by the research
    community, especially for various complex [3DPC](#id1.1.id1) tasks like semantic
    segmentation [[140](#bib.bib140)] and object recognition [[143](#bib.bib143)].
    However, domain discrepancy exists due to the domain shift  [[282](#bib.bib282)];
    hence, it becomes difficult for a model to perform across domains accurately.
    Different solutions have been proposed to minimize the domain discrepancy for
    [3DPC](#id1.1.id1) tasks. To this end, [DL](#id4.4.id4) methods based on the adversarial
    network, such as domain adversarial neural networks [18], have improved performance
    for [UDA](#id8.8.id8) tasks. The primary job of bridging the domain gap is performed
    by introducing a generator that keeps fooling the discriminator until it identifies
    the discrepancies [[238](#bib.bib238)]. Moving forward, for classification purposes,
    a model is presented in [[158](#bib.bib158)] that focuses on aligning the local
    and global features. Similarly, for 3D segmentation on [LiDAR](#id18.18.id18)
    sensors, authors in [[155](#bib.bib155)] have considered aligning activation correlation
    [[282](#bib.bib282)]. The out space alignment and entropy minimization has been
    integrated by the authors in [[234](#bib.bib234)] for addressing the above [UDA](#id8.8.id8)
    issues. Domain discrepancies that can arise due to [LiDAR](#id18.18.id18) sensors
    are handled by Yi et al. [[157](#bib.bib157)]. Accordingly, a [SSL](#id14.14.id14)
    framework to improve [UDA](#id8.8.id8) performance has been suggested in [[116](#bib.bib116)].
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，[UDA](#id8.8.id8) 在研究界获得了相当大的关注，特别是在各种复杂的 [3DPC](#id1.1.id1) 任务中，如语义分割 [[140](#bib.bib140)]
    和对象识别 [[143](#bib.bib143)]。然而，由于领域偏移的存在 [[282](#bib.bib282)]，模型在跨领域准确执行变得困难。为
    [3DPC](#id1.1.id1) 任务最小化领域偏差的不同解决方案已被提出。为此，基于对抗网络的 [DL](#id4.4.id4) 方法，如领域对抗神经网络
    [18]，已经提高了 [UDA](#id8.8.id8) 任务的性能。弥合领域差距的主要工作是通过引入一个生成器来完成，生成器不断欺骗判别器，直到它识别出差异
    [[238](#bib.bib238)]。展望未来，为了分类目的，[[158](#bib.bib158)] 提出了一个关注于对齐局部和全局特征的模型。类似地，对于
    [LiDAR](#id18.18.id18) 传感器的 3D 分割，[[155](#bib.bib155)] 的作者考虑了对齐激活相关性 [[282](#bib.bib282)]。[[234](#bib.bib234)]
    的作者通过整合外部空间对齐和熵最小化来解决上述 [UDA](#id8.8.id8) 问题。由于 [LiDAR](#id18.18.id18) 传感器可能引起的领域偏差已由
    Yi 等人 [[157](#bib.bib157)] 处理。因此，[[116](#bib.bib116)] 中建议了一个 [SSL](#id14.14.id14)
    框架，以提高 [UDA](#id8.8.id8) 性能。
- en: 'The proliferation in related research has paved the way for an increased interest
    in domain-invariant representations (DiR). The primary aim of the DiR is to facilitate
    a way that there should be insignificant discrepancies for features coming from
    different domains. To this end, Jaritz et al. [[241](#bib.bib241)] have leveraged
    the cross-modal discrepancies while preserving the best performance of each sensor
    from self-driving data from cameras and [LiDAR](#id18.18.id18)  [3DPC](#id1.1.id1)
    sensors because domain gaps are different for different sensors. For example,
    comparing a [LiDAR](#id18.18.id18) and a camera, the former is more robust to
    lighting changes with respect to the latter. Whereas there are always dense images
    that come as output from a camera while [LiDAR](#id18.18.id18) sensing density
    is proportional to the sensor setup. Notably, ’cross-modality’ differ from multi-modal
    fusion [[241](#bib.bib241)] as multi-modal implies training a single model in
    a supervised way for combining inputs like [LiDAR](#id18.18.id18) and camera [[283](#bib.bib283)],
    RGB-D [[284](#bib.bib284)], etc. This cross-modality DiR can help avoid the limitations
    of [UDA](#id8.8.id8) across modalities in which one modality influences the performance
    of the other modality in a negative way. In Fig. [13](#S5.F13 "Figure 13 ‣ 5.3
    Domain discrepancies ‣ 5 Open Challenges ‣ Advancing 3D Point Cloud Understanding
    through Deep Transfer Learning: A Comprehensive Survey"), an overview of [CLDA](#id26.26.id26)
    is presented. Here, a prediction of 3D segmentation labels is performed through
    2D and a 3D network by providing an image and a [3DPC](#id1.1.id1) as input to
    this network. 3D predictions are converted to 3D while consistency is ensured
    through mutual mimicking. This approach is proved to be advantageous for [UDA](#id8.8.id8).'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 相关研究的繁荣为领域不变表示（DiR）引起了更多关注。DiR的主要目的是确保来自不同领域的特征之间的差异微乎其微。为此，Jaritz等人[[241](#bib.bib241)]利用了跨模态差异，同时保持来自自驾车辆的摄像头和[LiDAR](#id18.18.id18)
    [3DPC](#id1.1.id1)传感器的最佳性能，因为不同传感器之间的领域差异不同。例如，比较[LiDAR](#id18.18.id18)和摄像头时，前者对光照变化的鲁棒性更强。而摄像头的输出总是密集的图像，而[LiDAR](#id18.18.id18)的感应密度与传感器设置成正比。值得注意的是，‘跨模态’与多模态融合[[241](#bib.bib241)]有所不同，因为多模态指的是通过监督方式训练一个单一模型来结合[LiDAR](#id18.18.id18)和摄像头[[283](#bib.bib283)]、RGB-D[[284](#bib.bib284)]等输入。这种跨模态DiR可以帮助避免[UDA](#id8.8.id8)在模态间的限制，其中一个模态会以负面方式影响另一个模态的性能。在图[13](#S5.F13
    "图 13 ‣ 5.3 领域差异 ‣ 5 开放挑战 ‣ 通过深度迁移学习推动3D点云理解：全面调查")中，介绍了[CLDA](#id26.26.id26)的概述。在这里，通过提供图像和[3DPC](#id1.1.id1)作为输入，2D和3D网络对3D分割标签进行预测。3D预测被转换为3D，同时通过相互模仿确保一致性。这种方法被证明对[UDA](#id8.8.id8)有利。
- en: '![Refer to caption](img/d868aca49723304d58e1e2422d310d46.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d868aca49723304d58e1e2422d310d46.png)'
- en: 'Figure 13: The [CLDA](#id26.26.id26) proposed in [[241](#bib.bib241)], where
    consistency has been ensured through mutual mimicking.'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: 提出了在[[241](#bib.bib241)]中提出的[CLDA](#id26.26.id26)，通过相互模仿确保了一致性。'
- en: 5.4 Reproducibility of scientific results
  id: totrans-498
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 科学结果的可重复性
- en: Despite the growing interest in using [DTL](#id6.6.id6) for various energy applications,
    there are still several factors that hinder the widespread adoption of [DTL](#id6.6.id6)-based
    models and impact reproducibility, and the ability to compare [DTL](#id6.6.id6)-based
    solutions. Firstly, it is difficult to evaluate the generality of [DTL](#id6.6.id6)
    models as most of the frameworks are evaluated on datasets collected under similar
    conditions, such as the same climate [[285](#bib.bib285)]. Secondly, there is
    a lack of consistency in using the same datasets and benchmarks to validate new
    [DTL](#id6.6.id6) models, due to the limited availability of open-source, benchmarked
    datasets [[286](#bib.bib286)]. Finally, different metrics and parameter settings
    have been used to measure the distance between the source and [TD](#id9.9.id9)s
    and evaluate the performance of [DTL](#id6.6.id6)-based solutions on different
    datasets. This makes it challenging, and even impossible, to compare [DTL](#id6.6.id6)
    techniques uniformly [[287](#bib.bib287)]. Despite that, there are some studies
    already uploaded on GitHub and other online platforms to facilitate the reproducibility
    and comparison tasks; the effort put in this direction still needs to consider
    the challenges introduced by [DTL](#id6.6.id6) for [3DPC](#id1.1.id1)s.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对[DTL](#id6.6.id6)在各种能源应用中使用的兴趣日益增加，但仍有几个因素阻碍了[DTL](#id6.6.id6)基础模型的广泛采用，并影响了可重复性和对[DTL](#id6.6.id6)解决方案的比较能力。首先，评估[DTL](#id6.6.id6)模型的通用性很困难，因为大多数框架是在相似条件下收集的数据集上进行评估的，例如相同的气候
    [[285](#bib.bib285)]。其次，由于开放源代码的基准数据集有限，缺乏使用相同数据集和基准来验证新[DTL](#id6.6.id6)模型的一致性
    [[286](#bib.bib286)]。最后，已使用不同的度量标准和参数设置来衡量源与[TD](#id9.9.id9)之间的距离，并评估不同数据集上的[DTL](#id6.6.id6)基础解决方案的性能。这使得统一比较[DTL](#id6.6.id6)技术变得具有挑战性，甚至不可能
    [[287](#bib.bib287)]。尽管如此，已有一些研究上传到 GitHub 和其他在线平台以促进可重复性和比较任务，但在这方面付出的努力仍需要考虑[DTL](#id6.6.id6)对[3DPC](#id1.1.id1)所带来的挑战。
- en: 5.5 Measuring knowledge gains
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 知识增益的测量
- en: Assessing the knowledge gained when using [DTL](#id6.6.id6) models for specific
    tasks is crucial, yet this challenge has not been fully addressed in the literature.
    Bengio et al. in [[288](#bib.bib288)] proposed four measures, transfer error,
    transfer loss, transfer ratio, and in-domain ratio, to quantify the gain of knowledge
    in [DTL](#id6.6.id6). However, it is unclear how these measures would perform
    with other [DTL](#id6.6.id6)-based methods, particularly in the energy sector,
    where the class sets are different between problems. Moreover, these measures
    can lead to non-definite performance evaluations if a perfect baseline model is
    achieved. To address these issues, simpler measures such as accuracy, F1 score,
    MSE, RMSE, MAE, performance improvement ratio (PIR) or other statistically-inspired
    coefficients have been widely used to evaluate [DTL](#id6.6.id6)-based solutions
    in the energy sector. These measures provide additional information, such as class
    agreement, and are better suited to the specific requirements of the energy sector.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 评估在使用[DTL](#id6.6.id6)模型处理特定任务时获得的知识至关重要，但这一挑战在文献中尚未得到充分解决。Bengio 等人 [[288](#bib.bib288)]
    提出了四种度量方法：转移误差、转移损失、转移比率和领域内比率，用于量化[DTL](#id6.6.id6)中的知识增益。然而，目前尚不清楚这些度量方法在其他[DTL](#id6.6.id6)基础方法中的表现如何，特别是在能源领域，不同问题的类别集合不同。此外，如果达到了完美的基准模型，这些度量方法可能会导致不确定的性能评估。为解决这些问题，更简单的度量方法如准确率、F1
    分数、MSE、RMSE、MAE、性能提升比率（PIR）或其他统计启发的系数已经广泛用于评估能源领域中的[DTL](#id6.6.id6)解决方案。这些度量方法提供了额外的信息，如类别一致性，更适合能源领域的具体需求。
- en: 5.6 Unification of DTL
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 DTL 的统一
- en: The evolution of [DTL](#id6.6.id6) in energy applications has been rapid and
    innovative but also rather fragmented. This is due to the range of unique mathematical
    formulations utilized to delineate [DTL](#id6.6.id6) algorithms. Numerous researchers
    have developed and proposed various versions of [DTL](#id6.6.id6), each bearing
    its unique label and implementation approach. Examples of these include "Heterogeneous
    [DTL](#id6.6.id6)" as proposed in [[289](#bib.bib289)], "Statistical investigations"
    in [[290](#bib.bib290)], and "DA-DTL" in [[291](#bib.bib291)]. This multiplicity
    of terms and methods can often lead to confusion among scholars in the field.
    It is clear that there is a pressing need to homogenize the terminologies and
    conceptual underpinnings of [DTL](#id6.6.id6) to foster better comprehension and
    collaboration within the research community. A seminal step towards this unification
    was taken in [[292](#bib.bib292)], laying a foundation upon which future studies
    can build. However, the exploration remains incomplete with respect to [3DPC](#id1.1.id1)
    applications in particular.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '[DTL](#id6.6.id6) 在能源应用中的发展迅速且具有创新性，但也相当零散。这是因为用于描述 [DTL](#id6.6.id6) 算法的数学公式各异。许多研究人员开发并提出了不同版本的
    [DTL](#id6.6.id6)，每个版本都有其独特的标签和实施方法。这些例子包括[[289](#bib.bib289)]中提出的“异构 [DTL](#id6.6.id6)”、[[290](#bib.bib290)]中的“统计研究”和[[291](#bib.bib291)]中的“DA-DTL”。这些术语和方法的多样性常常会导致学者之间的困惑。显然，迫切需要统一
    [DTL](#id6.6.id6) 的术语和概念基础，以促进研究社区的更好理解和协作。[[292](#bib.bib292)]在这一统一过程中迈出了开创性的一步，为未来的研究奠定了基础。然而，关于
    [3DPC](#id1.1.id1) 应用的探索仍然不完整。'
- en: The unification of [DTL](#id6.6.id6) concepts entails collating, comparing,
    and reconciling the differing methodologies, with an ultimate aim to develop a
    more cohesive and comprehensive framework. A unified approach would facilitate
    the application of [DTL](#id6.6.id6) in [3DPC](#id1.1.id1) applications by standardizing
    algorithm descriptions, mitigating confusion, and making it easier for researchers
    to implement, adapt, and build upon existing algorithms. This, in turn, would
    expedite the progression of [DTL](#id6.6.id6)-based solutions, thereby accelerating
    advancements in the realm of energy applications. Therefore, it is paramount to
    expand the unification efforts initiated in [[292](#bib.bib292)] to include [3DPC](#id1.1.id1)
    applications. Future research endeavors should prioritize developing a harmonized
    approach that accommodates the diverse range of [DTL](#id6.6.id6) methodologies
    while also meeting the specific needs of [3DPC](#id1.1.id1) applications. This
    would not only streamline the use of [DTL](#id6.6.id6) in the field but also pave
    the way for more innovative developments in energy applications.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 统一 [DTL](#id6.6.id6) 概念涉及整合、比较和调和不同的方法，最终目标是开发出一个更具凝聚力和全面性的框架。统一的方法将通过标准化算法描述，减少混淆，并使研究人员更容易实施、调整和在现有算法基础上进行创新，从而促进
    [DTL](#id6.6.id6) 在 [3DPC](#id1.1.id1) 应用中的应用。这将加速 [DTL](#id6.6.id6) 基于的解决方案的发展，从而推动能源应用领域的进步。因此，有必要扩展[[292](#bib.bib292)]启动的统一工作，以涵盖
    [3DPC](#id1.1.id1) 应用。未来的研究应优先开发一种协调的方法，以适应不同范围的 [DTL](#id6.6.id6) 方法，同时满足 [3DPC](#id1.1.id1)
    应用的具体需求。这不仅将简化 [DTL](#id6.6.id6) 在该领域的使用，还将为能源应用领域的更多创新发展铺平道路。
- en: 6 Future Research directions
  id: totrans-505
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来研究方向
- en: 6.1 3DPC Transformers
  id: totrans-506
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 3DPC Transformers
- en: 'Transformer models have greatly improved NLP [[293](#bib.bib293)] and CV, but
    their application in [3DPC](#id1.1.id1) processing remains uncertain. Questions
    remain about their ability to handle irregular and unordered data in 3D, their
    suitability for different 3D representations and their competence in various 3D
    processing tasks. Fig. [14](#S6.F14 "Figure 14 ‣ 6.1 3DPC Transformers ‣ 6 Future
    Research directions ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey") illustrates an example of a Transformer encoder
    architecture.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型在自然语言处理[[293](#bib.bib293)]和计算机视觉方面有了显著改进，但它们在 [3DPC](#id1.1.id1)
    处理中的应用仍然不确定。关于它们处理 3D 中不规则和无序数据的能力、适用于不同 3D 表示的适用性以及在各种 3D 处理任务中的能力仍然存在疑问。图 [14](#S6.F14
    "图 14 ‣ 6.1 3DPC Transformers ‣ 6 未来研究方向 ‣ 通过深度迁移学习推进 3D 点云理解：综合调查") 说明了一个 Transformer
    编码器架构的示例。
- en: '![Refer to caption](img/ecc17235b883408ab9b39fcb097a2b4d.png)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ecc17235b883408ab9b39fcb097a2b4d.png)'
- en: 'Figure 14: Example of a Transformer’s encoder architecture.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：变换器编码器架构示例。
- en: 'One important challenge when using transformers is that they require large
    amounts of training data. In the field of [3DPC](#id1.1.id1), obtaining large
    datasets is a challenge and this makes training Transformers for 3D tasks difficult.
    The authors in [[294](#bib.bib294)] have investigated the use of knowledge from
    multiple images for [3DPC](#id1.1.id1) understanding. They proposed a pipeline
    called Pix4Point that allows for utilizing pretrained Transformers in the image
    domain to improve downstream [3DPC](#id1.1.id1) tasks. This is achieved by using
    a modality-agnostic pure Transformer backbone with tokenizer and decoder layers
    specialized in the 3D domain. Another model named “Point-BERT” which is based
    on BERT’s architecture [[295](#bib.bib295)] to learn Transformers for generalizing
    the idea of BERT to [3DPC](#id1.1.id1) is proposed in [[119](#bib.bib119)]. Point-BERT
    is trained using a masked point modeling task. The pipeline of Point-BERT is illustrated
    in Fig. [15](#S6.F15 "Figure 15 ‣ 6.1 3DPC Transformers ‣ 6 Future Research directions
    ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey"), where the input [3DPC](#id1.1.id1) is first partitioned into several
    point patches.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 使用变换器时的一个重要挑战是需要大量的训练数据。在[3DPC](#id1.1.id1)领域，获取大规模数据集是一个挑战，这使得为 3D 任务训练变换器变得困难。[[294](#bib.bib294)]的作者研究了从多个图像中获得知识以理解[3DPC](#id1.1.id1)。他们提出了一种名为
    Pix4Point 的管道，允许在图像领域利用预训练的变换器来改进下游[3DPC](#id1.1.id1)任务。这是通过使用与模态无关的纯变换器骨干网络，以及在
    3D 领域专门化的标记器和解码器层来实现的。另一个名为“Point-BERT”的模型基于 BERT 的架构[[295](#bib.bib295)]，旨在将
    BERT 的思想推广到[3DPC](#id1.1.id1)，这一点在[[119](#bib.bib119)]中提出。Point-BERT 使用掩蔽点建模任务进行训练。Point-BERT
    的管道如图 [15](#S6.F15 "图 15 ‣ 6.1 3DPC 变换器 ‣ 6 未来研究方向 ‣ 通过深度迁移学习推进 3D 点云理解：全面调查")所示，其中输入[3DPC](#id1.1.id1)首先被划分为几个点补丁。
- en: '![Refer to caption](img/3b9ea751190868a3d5538ad886343ce6.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3b9ea751190868a3d5538ad886343ce6.png)'
- en: 'Figure 15: An illustration of CNN based DTL for [3DPC](#id1.1.id1) segmentation
    and its types'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：基于 CNN 的 DTL 在 [3DPC](#id1.1.id1) 分割及其类型的示意图
- en: 6.2 Further generalization
  id: totrans-513
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 进一步的泛化
- en: Enhancing the generalization capabilities of [DTL](#id6.6.id6) algorithms for
    [3DPC](#id1.1.id1) understanding involves some unique challenges and strategies
    due to the distinctive nature of 3D data. For instance, data augmentation techniques
    for 3D data can be more complex than for images or text. Techniques could include
    rotations, translations, adding Gaussian noise, or mirroring the point cloud along
    a plane. This would create a more diverse training dataset and can help the model
    generalize better. In order to reduce the domain shift between the source and
    target task, techniques such as feature alignment can be used. This involves minimizing
    the difference between the distributions of the source and target features. Moreover,
    utilizing architectures that maintain geometric invariance can be helpful as [3DPC](#id1.1.id1)
    data can be rotated or scaled. Networks, such as PointNet or PointNet++ which
    are invariant to permutations and transformations of the input points can be utilized.
    Additionally, methods such as PointNet++ use hierarchical neural networks to capture
    local structures induced by the metric space points live in, and also model global
    structures. This can help with generalization as it learns at different scales.
    Moving on, regularization methods such as dropout or weight decay can be implemented
    to prevent overfitting and ensure the model generalizes better. Lastly, designing
    loss functions that better capture the properties of 3D data can also help with
    generalization.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 提高[DTL](#id6.6.id6)算法在[3DPC](#id1.1.id1)理解中的泛化能力涉及一些独特的挑战和策略，因为 3D 数据的特殊性质。例如，3D
    数据的数据增强技术可能比图像或文本的更复杂。这些技术可以包括旋转、平移、添加高斯噪声或沿平面镜像点云。这将创建更为多样化的训练数据集，并有助于模型更好地泛化。为了减少源任务和目标任务之间的领域差异，可以使用特征对齐等技术。这涉及最小化源特征和目标特征之间的分布差异。此外，利用保持几何不变性的架构也是有帮助的，因为[3DPC](#id1.1.id1)数据可以旋转或缩放。可以使用如
    PointNet 或 PointNet++ 这样的网络，它们对输入点的排列和变换具有不变性。此外，像 PointNet++ 这样的算法使用层次神经网络来捕捉由度量空间点引起的局部结构，并建模全局结构。这有助于泛化，因为它在不同的尺度上进行学习。接下来，正则化方法，如
    dropout 或权重衰减，可以用于防止过拟合，并确保模型更好地泛化。最后，设计更好地捕捉 3D 数据特性的损失函数也有助于泛化。
- en: 6.3 Multi-task learning
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 多任务学习
- en: 6.3.1 Segmentation and Detection
  id: totrans-516
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1 分割与检测
- en: This section focuses on frameworks that handle multiple tasks like segmentation,
    detection, and sometimes additional tasks like classification or instance identification
    within point clouds. Typically, Pham et al. 2019 [[296](#bib.bib296)] and Zou
    et al. 2022 [[297](#bib.bib297)] both explore multi-task learning frameworks that
    handle segmentation and classification simultaneously. Pham et al. focus on combining
    semantic and instance segmentation, while Zou et al. integrate classification
    and segmentation tasks to enhance each other, utilizing a Y-shaped graph neural
    network. Moving on, Chen et al. 2022 [[298](#bib.bib298)] propose a method for
    joint semantic and instance segmentation using a shared encoder and dual decoders,
    focusing on enhancing feature representation through cross-task interactions.
    similarly, Ye et al. 2023 [[299](#bib.bib299)] extend the multi-task approach
    to LiDAR-based perception tasks, unifying object detection, semantic segmentation,
    and panoptic segmentation in a single network to optimize performance across tasks.
    These studies collectively push the boundaries in efficient multi-task handling,
    demonstrating that integrating tasks can improve overall performance and reduce
    computational costs by sharing learned features across tasks.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点关注处理多个任务的框架，如分割、检测，有时还有分类或实例识别等附加任务。通常，Pham 等人 2019 [[296](#bib.bib296)]
    和 Zou 等人 2022 [[297](#bib.bib297)] 都探索了处理分割和分类的多任务学习框架。Pham 等人专注于结合语义分割和实例分割，而
    Zou 等人将分类和分割任务整合在一起，利用 Y 形图神经网络来相互增强。接下来，Chen 等人 2022 [[298](#bib.bib298)] 提出了使用共享编码器和双解码器的联合语义和实例分割方法，专注于通过跨任务交互来增强特征表示。同样，Ye
    等人 2023 [[299](#bib.bib299)] 将多任务方法扩展到基于 LiDAR 的感知任务，在一个网络中统一物体检测、语义分割和全景分割，以优化跨任务的性能。这些研究共同推动了高效多任务处理的边界，表明集成任务可以通过在任务之间共享学习到的特征来提高整体性能并降低计算成本。
- en: 6.3.2 Advanced Architectures for Enhanced Feature Extraction
  id: totrans-518
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2 用于增强特征提取的高级架构
- en: MTL can be used for developing sophisticated neural network architectures to
    better handle the complexities of point cloud data. In this regard, Dubey et al.
    2022 [[300](#bib.bib300)] introduce an attention-based architecture for radar
    point cloud data, focusing on enhancing target localization and classification
    through an anchor-free network design. Similarly, Hassani et al. 2019 [[301](#bib.bib301)]
    leverage an unsupervised multi-task model to learn point and shape features effectively,
    employing a graph-based encoder that tackles tasks like clustering and reconstruction
    simultaneously. Moving forward, Lin et al. 2022 [[302](#bib.bib302)] use a multi-task
    learning framework to improve semantic segmentation efficiency in Mobile Laser
    Scanning point clouds by integrating color prediction as an auxiliary task.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: MTL 可以用于开发复杂的神经网络架构，以更好地处理点云数据的复杂性。在这方面，Dubey 等人 2022 [[300](#bib.bib300)] 引入了一种基于注意力的雷达点云数据架构，专注于通过无锚网络设计来增强目标定位和分类。同样，Hassani
    等人 2019 [[301](#bib.bib301)] 利用无监督的多任务模型来有效地学习点和形状特征，采用图形编码器同时处理聚类和重建等任务。接下来，Lin
    等人 2022 [[302](#bib.bib302)] 使用多任务学习框架，通过将颜色预测作为辅助任务，来提高 Mobile Laser Scanning
    点云中的语义分割效率。
- en: 6.3.3 Specific Enhancements
  id: totrans-520
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3 特定的增强
- en: Many studies have focused on applying multi-task learning to achieve specific
    enhancements in processing or understanding point cloud data. In this direction,
    Zhao et al. 2024 [[303](#bib.bib303)] develop a robust network for preprocessing
    LiDAR data, tackling denoising, segmentation, and completion tasks within a shared
    framework to improve data quality. Similarly, Rios et al. 2021 [[304](#bib.bib304)]
    employ a multi-task approach in an evolutionary optimization context, using a
    3DPC autoencoder to unify different design representations in a common latent
    space. Feng et al. 2021 [[305](#bib.bib305)] propose a multi-task network to handle
    various perception tasks for autonomous driving, demonstrating how a unified approach
    can enhance both detection and road understanding tasks.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究集中于应用多任务学习以在处理或理解点云数据方面实现特定的增强。在这方面，Zhao 等人 2024 [[303](#bib.bib303)] 开发了一种用于
    LiDAR 数据预处理的鲁棒网络，处理去噪、分割和补全任务，以共享框架提高数据质量。同样，Rios 等人 2021 [[304](#bib.bib304)]
    在进化优化背景下采用多任务方法，使用 3DPC 自编码器将不同的设计表示统一到一个公共潜在空间中。Feng 等人 2021 [[305](#bib.bib305)]
    提出了一个多任务网络来处理自主驾驶的各种感知任务，展示了统一方法如何增强检测和道路理解任务。
- en: Besides, the study in [[306](#bib.bib306)] introduces FFT-RadNet, a novel HD
    radar sensing model that optimizes the computation of angular positions from radar
    data, facilitating vehicle detection and free driving space segmentation. Moving
    on, Shan et al. 2023 [[307](#bib.bib307)] propose a no-reference point cloud quality
    assessment metric, GPA-Net, which utilizes a multi-task framework to enhance the
    accuracy of quality regression by also predicting distortion type and degree.
    Besides, the authors in [[308](#bib.bib308)] present Point-TTA, a test-time adaptation
    framework for point cloud registration that improves model generalization and
    performance on unknown testing environments through self-supervised auxiliary
    tasks. On the other hand, Zhang et al. 2022 [[309](#bib.bib309)] - The study proposes
    an improved multi-task network for roof plane segmentation from airborne laser
    scanning point clouds, which effectively segments and identifies roof planes.
    Wei et al. 2021 [[310](#bib.bib310)] - This paper proposes a multi-task network
    for 3D keypoint detection, which efficiently captures local and global features
    for accurate keypoint localization and semantic labeling. Lastly, the work in
    [[311](#bib.bib311)] introduces a multi-task network for machining feature recognition
    from point cloud data, aiming to accurately segment and identify complex machining
    features.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究[[306](#bib.bib306)]介绍了FFT-RadNet，这是一种新颖的HD雷达传感模型，优化了从雷达数据中计算角位置的过程，促进了车辆检测和自由驾驶空间的分割。接下来，Shan等人2023年[[307](#bib.bib307)]提出了一种无参考点云质量评估指标GPA-Net，该指标利用多任务框架通过预测失真类型和程度来提高质量回归的准确性。此外，[[308](#bib.bib308)]中的作者展示了Point-TTA，这是一种点云配准的测试时适应框架，通过自监督辅助任务提高模型在未知测试环境中的泛化能力和性能。另一方面，Zhang等人2022年[[309](#bib.bib309)]
    - 该研究提出了一种改进的多任务网络，用于从机载激光扫描点云中分割屋顶平面，有效地分割和识别屋顶平面。Wei等人2021年[[310](#bib.bib310)]
    - 本文提出了一种用于3D关键点检测的多任务网络，该网络高效捕捉局部和全局特征，以实现准确的关键点定位和语义标注。最后，[[311](#bib.bib311)]中的工作介绍了一种用于从点云数据中识别加工特征的多任务网络，旨在准确分割和识别复杂的加工特征。
- en: 6.4 Cross-Modal Transfer Learning
  id: totrans-523
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 跨模态迁移学习
- en: 6.4.1 Enhancing 3DPC Understanding Through 2D-3D Correspondences
  id: totrans-524
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1 通过2D-3D对应关系增强3DPC理解
- en: 'Many studies employ contrastive learning and knowledge distillation strategies
    to align and enhance features between 2D images and 3DPCs. These methods focus
    on leveraging the rich textual and visual information available in 2D data to
    augment the spatial understanding provided by 3DPCs, significantly boosting performance
    in tasks such as classification, segmentation, and dense captioning. For instance,
    Afham et al. [[312](#bib.bib312)] introduce CrossPoint, utilizing self-supervised
    contrastive learning to map 2D image features to 3DPCs for better object classification
    and segmentation. The PointCMT, proposed in Yan et al. [[313](#bib.bib313)], adopts
    a teacher-student framework for cross-modal training, using 2D images to enhance
    3DPC classification through knowledge distillation. Wu et al. [[314](#bib.bib314)]
    propose CrossNet, a method that aligns 3DPC features with both colored and grayscale
    images to enhance classification and segmentation tasks across different domains.
    Group 2: Cross-Modal and Cross-Domain Adaptation for Comprehensive 3D Understanding'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究采用对比学习和知识蒸馏策略来对齐和增强2D图像与3DPC之间的特征。这些方法专注于利用2D数据中丰富的文本和视觉信息来增强3DPC提供的空间理解，显著提升了分类、分割和密集标注等任务的性能。例如，Afham等人[[312](#bib.bib312)]介绍了CrossPoint，利用自监督对比学习将2D图像特征映射到3DPC，以实现更好的物体分类和分割。Yan等人[[313](#bib.bib313)]提出的PointCMT采用了教师-学生框架进行跨模态训练，利用2D图像通过知识蒸馏增强3DPC分类。Wu等人[[314](#bib.bib314)]提出了CrossNet，一种将3DPC特征与彩色和灰度图像对齐的方法，以提高不同领域中的分类和分割任务。组2：跨模态和跨领域适应用于综合3D理解
- en: 6.4.2 Comprehensive 3D Understanding
  id: totrans-526
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2 综合3D理解
- en: CMTL can be employed in various works to tackle the challenge of transferring
    knowledge across not just modalities but also different domains, aiming to improve
    the performance of 3D perception tasks under varying conditions and datasets,
    without relying heavily on labeled data. Typically, Zhang et al. [[315](#bib.bib315)]
    develop a novel adaptation strategy that aligns features between images and point
    clouds to enhance semantic segmentation without requiring 3D labels. The X4D-SceneFormer,
    introduced by Jing et al. [[316](#bib.bib316)], uses a dual-branch Transformer
    architecture to transfer dynamic textual and visual cues from 2D video sequences
    to 4D point cloud sequences, focusing on temporal and semantic coherence. Zhou
    et al. [[317](#bib.bib317)] introduce PointCMC, which utilizes a Local-to-Local
    (L2L) module and a Cross-Modal Local-Global Contrastive (CLGC) loss to enhance
    cross-modal knowledge transfer between image and point cloud data.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: CMTL可以应用于各种工作中，以解决知识迁移跨越不仅仅是模态，还包括不同领域的挑战，旨在提高在不同条件和数据集下3D感知任务的性能，而不依赖大量标注数据。通常，Zhang等人[[315](#bib.bib315)]
    开发了一种新颖的适应策略，该策略在图像和点云之间对齐特征，以增强语义分割，而无需3D标签。Jing等人[[316](#bib.bib316)] 提出的X4D-SceneFormer，采用了双分支变换器架构，将动态文本和视觉线索从2D视频序列迁移到4D点云序列，关注时间和语义一致性。Zhou等人[[317](#bib.bib317)]
    引入了PointCMC，利用本地到本地（L2L）模块和跨模态本地-全局对比（CLGC）损失来增强图像和点云数据之间的跨模态知识迁移。
- en: 6.4.3 Innovative Cross-Modal Applications in Robotics and Dynamic Environments
  id: totrans-528
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.3 机器人及动态环境中的创新跨模态应用
- en: This section discusses the group of studies that applies cross-modal transfer
    learning to specific, challenging scenarios such as robotic tactile recognition
    and dynamic 4D scene understanding, demonstrating the flexibility and potential
    of cross-modal learning in practical and dynamic applications. Specifically, Falco
    et al. [[318](#bib.bib318)] explore visuo-tactile object recognition, where a
    robot uses visual data to enhance its tactile recognition capabilities, effectively
    bridging the gap between seeing and touching. Additionally, the authors in [[316](#bib.bib316)]
    focus on enhancing 4D point cloud understanding by transferring knowledge from
    RGB video sequences, improving the dynamic scene comprehension significantly.
    Murali et al. [[319](#bib.bib319)] propose xAVTNet, a visuo-tactile cross-modal
    framework for object recognition by autonomous robotic systems, utilizing active
    learning and perception strategies.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了将跨模态迁移学习应用于特定且具有挑战性的场景，如机器人触觉识别和动态4D场景理解的一系列研究，展示了跨模态学习在实际和动态应用中的灵活性和潜力。具体而言，Falco等人[[318](#bib.bib318)]
    探讨了视觉-触觉物体识别，其中机器人利用视觉数据来增强其触觉识别能力，有效地弥合了视觉与触觉之间的差距。此外，[[316](#bib.bib316)]中的作者专注于通过将知识从RGB视频序列迁移来增强4D点云理解，显著提高了动态场景的理解。Murali等人[[319](#bib.bib319)]
    提出了xAVTNet，这是一个用于自主机器人系统物体识别的视觉-触觉跨模态框架，利用主动学习和感知策略。
- en: 6.4.4 Enhanced Sensory Perception and Retrieval
  id: totrans-530
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.4 增强的感官感知与检索
- en: CMTL can be used to improve sensory perception and retrieval tasks, enabling
    more effective and efficient recognition and classification across different sensory
    inputs. For example, Shen et al. [[320](#bib.bib320)] implement a simple cross-modal
    transfer from 2D to 3D sensors, using Vision Transformers to handle occlusions
    and improve performance in low-light scenarios. Jing et al. [[321](#bib.bib321)]
    enhances cross-modal retrieval capabilities by training a network to minimize
    feature discrepancies across 2D images, 3DPCs, and mesh data, ensuring robust
    retrieval performance across modalities.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: CMTL可以用于改善感官感知和检索任务，使不同感官输入之间的识别和分类更有效和高效。例如，Shen等人[[320](#bib.bib320)] 实现了从2D到3D传感器的简单跨模态迁移，利用视觉变换器处理遮挡问题，并在低光环境下提高性能。Jing等人[[321](#bib.bib321)]
    通过训练网络来最小化2D图像、3D点云和网格数据之间的特征差异，从而增强了跨模态检索能力，确保了不同模态之间的鲁棒检索性能。
- en: 6.4.5 Point Cloud Completion and Enhancement
  id: totrans-532
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.5 点云完成与增强
- en: Zhu et al. [[322](#bib.bib322)] introduce a cross-modal shape-transfer dual-refinement
    network (CSDN) designed for point cloud completion, utilizing images to guide
    the geometry generation of missing regions and refine the output through dual
    refinement processes. Li et al. [[323](#bib.bib323)] propose a model integrating
    Cross-Domain and Cross-Modal Knowledge Distillation to mitigate domain shifts
    and enhance the interaction between LiDAR and camera data, improving adaptation
    in unsupervised settings. Zhang et al. [[317](#bib.bib317)] explore PointMCD,
    a multi-view cross-modal distillation architecture, which aligns features between
    2D visual and 3D geometric domains to boost the learning capacity of point cloud
    encoders.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 朱等人 [[322](#bib.bib322)] 介绍了一种用于点云补全的跨模态形状转移双重优化网络（CSDN），利用图像指导缺失区域的几何生成，并通过双重优化过程对输出进行精炼。李等人
    [[323](#bib.bib323)] 提出了一个整合跨领域和跨模态知识蒸馏的模型，以减轻领域偏移并增强 LiDAR 和相机数据之间的交互，提高在无监督设置下的适应性。张等人
    [[317](#bib.bib317)] 探索了 PointMCD，这是一种多视图跨模态蒸馏架构，将2D视觉和3D几何域之间的特征对齐，以提升点云编码器的学习能力。
- en: 6.4.6 Domain Adaptation and Generalization in Cross-Modal Settings
  id: totrans-534
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.6 跨模态环境中的领域适应与泛化
- en: Peng et al. [[324](#bib.bib324)] discuss leveraging 2D images for 3D domain
    adaptation through cross-modal learning, addressing the loss of useful 2D features
    and promoting high-level modal complementarity. Nitsch et al. [[325](#bib.bib325)]
    propose a transductive transfer learning approach that uses a multi-modal adversarial
    autoencoder to transfer knowledge from images to point clouds, focusing on object
    detection. Li et al. [[326](#bib.bib326)] introduce a bird’s-eye view approach
    for cross-modal learning under Domain Generalization (DG) for 3D semantic segmentation,
    optimizing domain-irrelevant representation modeling.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 彭等人 [[324](#bib.bib324)] 讨论了通过跨模态学习利用2D图像进行3D领域适应，解决了有用的2D特征丢失问题，并促进了高层次模态的互补性。尼奇茨等人
    [[325](#bib.bib325)] 提出了一个传导性迁移学习方法，该方法使用多模态对抗自编码器将知识从图像转移到点云，重点关注目标检测。李等人 [[326](#bib.bib326)]
    介绍了一种鸟瞰视角方法用于领域泛化（DG）下的跨模态学习，用于3D语义分割，优化领域无关的表示建模。
- en: 6.4.7 Semantic Segmentation
  id: totrans-536
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.7 语义分割
- en: The ProtoTransfer, proposed in [[327](#bib.bib327)], explores knowledge transfer
    from multi-modal sources, such as LiDAR points and images, to enhance point cloud
    semantic segmentation, using a class-wise prototype bank to fully exploit image
    representations and transfer multi-modal knowledge to point cloud features. This
    approach effectively handles the challenge of unmatched point and pixel features
    by employing a pseudo-labeling scheme to integrate these features into the prototype
    bank, demonstrating superior performance on large-scale benchmarks. Similarly,
    Jaritz et al. [[196](#bib.bib196)] explore xMUDA, a cross-modal UDA technique
    for 3D semantic segmentation, leveraging mutual mimicking between 2D images and
    3DPCs. Xing et al. [[328](#bib.bib328)] discuss enhancing domain adaptation for
    3D semantic segmentation using cross-modal contrastive learning to improve interactions
    between 2D-pixel features and 3D point features.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[327](#bib.bib327)] 中提出的ProtoTransfer 探索了从多模态源（如 LiDAR 点云和图像）中进行知识转移，以增强点云的语义分割，利用类别原型库充分利用图像表示，并将多模态知识转移到点云特征中。这种方法通过采用伪标签方案将这些特征整合到原型库中，从而有效解决了点特征和像素特征不匹配的问题，并在大规模基准测试中表现出色。类似地，Jaritz
    等人 [[196](#bib.bib196)] 探索了 xMUDA，一种用于3D语义分割的跨模态UDA技术，利用2D图像和3D点云之间的相互模仿。邢等人 [[328](#bib.bib328)]
    讨论了通过跨模态对比学习增强3D语义分割的领域适应性，以改善2D像素特征和3D点特征之间的交互。
- en: Table LABEL:tab:comparison2 presents a comprehensive comparison of various studies
    focused on cross-modal learning and point cloud analysis, highlighting significant
    contributions and challenges in the field. For example, CrossPoint utilizes a
    self-supervised approach to establish 3D-2D correspondence, improving 3D object
    classification and segmentation, although it faces complexities in cross-modal
    alignment. Similarly, PointCMT enhances the representation of point clouds by
    incorporating view images derived from 2D image quality, thereby boosting 3DPC
    classification capabilities. Other notable methodologies include CrossNet, which
    excels in both intra- and cross-modal learning across multiple benchmark datasets,
    and X-Trans2Cap, which demonstrates effective cross-modal knowledge transfer in
    3D dense captioning, albeit at a high computational cost.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 表 LABEL:tab:comparison2 展示了针对跨模态学习和点云分析的各种研究的全面比较，突出领域中的重要贡献和挑战。例如，CrossPoint
    采用自监督方法建立 3D-2D 对应关系，提高了 3D 对象分类和分割，尽管在跨模态对齐方面存在复杂性。同样，PointCMT 通过整合来自 2D 图像质量的视图图像来增强点云表示，从而提升
    3DPC 分类能力。其他值得注意的方法包括 CrossNet，它在多个基准数据集上表现优异，跨模态学习出色，以及 X-Trans2Cap，它在 3D 密集标注中展示了有效的跨模态知识转移，尽管计算成本较高。
- en: The studies also outline several limitations inherent in current cross-modal
    and point cloud analysis techniques. For instance, many models, like the Cross-modal
    adaptation and simCrossTrans, struggle with domain shifts or performance variations
    across different 3D tasks, respectively. Additionally, while technologies such
    as the Visuotactile object recognition and xAVTNet offer high accuracy in specific
    applications such as object recognition in robotics, they are limited by their
    dependency on particular datasets and high system complexities. Other common challenges
    include the sensitivity to hyperparameters, as seen in Dual-Cross for cross-modal
    UDA, and the difficulty in feature alignment and generalization across different
    domains, which affects models like PointMCD and BEV-DG.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 这些研究还概述了当前跨模态和点云分析技术固有的几个局限性。例如，许多模型，如 Cross-modal adaptation 和 simCrossTrans，分别在不同
    3D 任务中面临领域偏移或性能变化的挑战。此外，尽管如 Visuotactile 对象识别和 xAVTNet 等技术在机器人对象识别等特定应用中提供了高准确度，但它们受到特定数据集和系统复杂度的限制。其他常见的挑战包括对超参数的敏感性，如
    Dual-Cross 在跨模态 UDA 中所示，以及不同领域之间的特征对齐和泛化困难，这影响了像 PointMCD 和 BEV-DG 这样的模型。
- en: 'Table 9: Comparison of Studies on Cross-Modal Learning and Point Cloud Analysis'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 跨模态学习和点云分析研究比较'
- en: '| Ref. | ML Model | Dataset | Application / Task | Advantage | Limitation |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | ML 模型 | 数据集 | 应用 / 任务 | 优势 | 局限性 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| [[312](#bib.bib312)] | CrossPoint | Varied | 3D object classification, segmentation
    | Uses 3D-2D correspondence; self-supervised | May require complex cross-modal
    alignment |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| [[312](#bib.bib312)] | CrossPoint | 多样 | 3D 对象分类、分割 | 使用 3D-2D 对应关系；自监督 |
    可能需要复杂的跨模态对齐 |'
- en: '| [[313](#bib.bib313)] | PointCMT | ModelNet40, ScanObjectNN | 3DPC classification
    | Enhances point-only representation; uses view-images | Depends on 2D image quality
    |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| [[313](#bib.bib313)] | PointCMT | ModelNet40, ScanObjectNN | 3DPC 分类 | 提升仅点表示；使用视图图像
    | 依赖于 2D 图像质量 |'
- en: '| [[314](#bib.bib314)] | CrossNet | Multiple benchmarks | 3DPC classification,
    segmentation | Intra- and cross-modal learning; versatile | Complex model structure
    for fine-tuning |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| [[314](#bib.bib314)] | CrossNet | 多个基准 | 3DPC 分类、分割 | 内部和跨模态学习；通用 | 调优的复杂模型结构'
- en: '| [[329](#bib.bib329)] | X-Trans2Cap | ScanRefer, Nr3D | 3D dense captioning
    | Effective cross-modal knowledge transfer; high accuracy | High computational
    cost in training |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| [[329](#bib.bib329)] | X-Trans2Cap | ScanRefer, Nr3D | 3D 密集标注 | 有效的跨模态知识转移；高准确度
    | 训练中的高计算成本 |'
- en: '| [[315](#bib.bib315)] | Cross-modal adaptation | SemanticKITTI, KITTI360,
    GTA5 | 3DPC semantic segmentation | Leverages 2D datasets; unsupervised | May
    struggle with substantial domain shifts |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| [[315](#bib.bib315)] | 跨模态适应 | SemanticKITTI, KITTI360, GTA5 | 3DPC 语义分割
    | 利用 2D 数据集；无监督 | 可能面临显著的领域偏移 |'
- en: '| [[318](#bib.bib318)] | Visuo-tactile object recognition | 15 objects dataset
    | Object recognition | High accuracy; cross-modal transfer | Limited to specific
    tactile and visual datasets |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| [[318](#bib.bib318)] | 视觉-触觉对象识别 | 15 个对象数据集 | 对象识别 | 高准确度；跨模态转移 | 限于特定的触觉和视觉数据集
    |'
- en: '| [[316](#bib.bib316)] | X4D-SceneFormer | HOI4D | 4D point cloud tasks | Incorporates
    temporal dynamics; high performance | Complex model, heavy on resources |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| [[316](#bib.bib316)] | X4D-SceneFormer | HOI4D | 4D 点云任务 | 融入时间动态；高性能 | 模型复杂，资源消耗大
    |'
- en: '| [[320](#bib.bib320)] | simCrossTrans | SUN RGB-D | 3D sensor performance
    | Utilizes ViTs; robust to occlusions | Performance may vary across different
    3D tasks |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| [[320](#bib.bib320)] | simCrossTrans | SUN RGB-D | 3D 传感器性能 | 利用 ViTs；对遮挡具有鲁棒性
    | 性能可能因不同的 3D 任务而异 |'
- en: '| [[321](#bib.bib321)] | Cross-modal retrieval | ModelNet10, ModelNet40 | Cross-modal
    retrieval | Efficient feature learning; high retrieval accuracy | Depends on network’s
    feature extraction ability |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| [[321](#bib.bib321)] | 跨模态检索 | ModelNet10, ModelNet40 | 跨模态检索 | 高效的特征学习；高检索准确率
    | 依赖网络的特征提取能力 |'
- en: '| [[327](#bib.bib327)] | ProtoTransfer | nuScenes, SemanticKITTI | Point cloud
    semantic segmentation | Exploits multi-modal fusion; high accuracy | May miss
    benefits for unmatched features |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| [[327](#bib.bib327)] | ProtoTransfer | nuScenes, SemanticKITTI | 点云语义分割 |
    利用多模态融合；高准确度 | 可能错过未匹配特征的好处 |'
- en: '| [[322](#bib.bib322)] | CSDN | Varied | Point cloud completion | Coarse-to-fine
    approach with dual-refinement; cross-modal data use | Complex model structure
    |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| [[322](#bib.bib322)] | CSDN | 多样化 | 点云补全 | 从粗到精的双重精炼方法；使用跨模态数据 | 模型结构复杂 |'
- en: '| [[319](#bib.bib319)] | xAVTNet | Robotics system | Visuo-tactile object recognition
    | Integrates visuo-tactile data; uses active learning | Specific to robotics;
    high system complexity |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| [[319](#bib.bib319)] | xAVTNet | 机器人系统 | 视觉-触觉物体识别 | 整合视觉-触觉数据；使用主动学习 | 特定于机器人领域；系统复杂度高
    |'
- en: '| [[323](#bib.bib323)] | Dual-Cross | Multi-modal | Cross-modal UDA | Integrates
    CDKD and CMKD for domain adaptation | Highly sensitive to hyperparameters |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| [[323](#bib.bib323)] | Dual-Cross | 多模态 | 跨模态 UDA | 集成 CDKD 和 CMKD 进行领域适应
    | 对超参数高度敏感 |'
- en: '| [[324](#bib.bib324)] | DsCML, CMAL | Multi-modal | 3D semantic segmentation
    | Enhances cross-modal learning; diverse domain adaptation | Potential feature
    mismatch |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| [[324](#bib.bib324)] | DsCML, CMAL | 多模态 | 3D 语义分割 | 增强跨模态学习；多样的领域适应 | 可能出现特征不匹配
    |'
- en: '| [[325](#bib.bib325)] | Adversarial Auto Encoder | KITTI | Object detection
    | Transductive transfer from images to point clouds | Requires multi-modal data
    |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| [[325](#bib.bib325)] | 对抗自编码器 | KITTI | 物体检测 | 从图像到点云的迁移学习 | 需要多模态数据 |'
- en: '| [[330](#bib.bib330)] | PointCMC | Varied | 3D object classification, segmentation
    | Enhances fine-grained cross-modal knowledge transfer | Alignment challenges
    |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| [[330](#bib.bib330)] | PointCMC | 多样化 | 3D 物体分类，分割 | 增强细粒度跨模态知识转移 | 对齐挑战
    |'
- en: '| [[196](#bib.bib196)] | xMUDA | Autonomous driving datasets | 3D semantic
    segmentation | Utilizes mutual mimicking in multi-modality | Complexity in heterogeneous
    input spaces |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| [[196](#bib.bib196)] | xMUDA | 自动驾驶数据集 | 3D 语义分割 | 在多模态中利用相互模仿 | 异质输入空间的复杂性
    |'
- en: '| [[328](#bib.bib328)] | Cross-modal contrastive learning | Varied | 3D semantic
    segmentation | Improves adaptation effects; uses neighborhood feature aggregation
    | Depends on precise feature correspondence |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| [[328](#bib.bib328)] | 跨模态对比学习 | 多样化 | 3D 语义分割 | 改善适应效果；使用邻域特征聚合 | 依赖精确的特征对应
    |'
- en: '| [[317](#bib.bib317)] | PointMCD | 3D datasets | 3D shape classification,
    segmentation | Uses cross-modal distillation; aligns features across modalities
    | Complicated feature alignment process |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| [[317](#bib.bib317)] | PointMCD | 3D 数据集 | 3D 形状分类，分割 | 使用跨模态蒸馏；对齐跨模态特征 |
    特征对齐过程复杂 |'
- en: '| [[326](#bib.bib326)] | BEV-DG | 3D datasets | 3D semantic segmentation |
    Implements BEV-based cross-modal learning; robust to misalignment | Domain generalization
    complexity |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| [[326](#bib.bib326)] | BEV-DG | 3D 数据集 | 3D 语义分割 | 实现基于 BEV 的跨模态学习；对错位具有鲁棒性
    | 领域泛化复杂性 |'
- en: 6.5 Diffusion models for 3DPC undertanding
  id: totrans-563
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 3D 点云理解的扩散模型
- en: Diffusion models are increasingly used for 3DPC applications due to their ability
    to generate high-quality, detailed structures, which is crucial for tasks such
    as 3D modeling and virtual reality. These models excel in handling complex distributions
    characteristic of 3DPCs, which are typically unordered and involve intricate spatial
    relationships. Their gradual denoising process allows them to implicitly learn
    the nuances of geometric structures, making them suitable for a range of tasks
    including reconstruction, up-sampling, and completion. Additionally, diffusion
    models are robust to noisy data, a common challenge with real-world 3D sensor
    data, and can be integrated with specialized 3D neural network architectures like
    PointNet, enhancing their effectiveness in processing 3DPCs. This flexibility
    and robustness make diffusion models a promising approach for advanced 3D applications
    across various industries.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型由于其能够生成高质量、详细结构的能力，在3D点云（3DPC）应用中被越来越多地使用，这对于3D建模和虚拟现实等任务至关重要。这些模型在处理3DPC特有的复杂分布时表现出色，这些分布通常是无序的，并涉及复杂的空间关系。它们的逐步去噪过程使它们能够隐式学习几何结构的细微差别，使其适用于包括重建、上采样和补全在内的各种任务。此外，扩散模型对噪声数据具有很强的鲁棒性，而噪声数据是实际3D传感器数据中常见的挑战，并且可以与像PointNet这样的专用3D神经网络架构集成，提高其处理3DPC的效果。这种灵活性和鲁棒性使得扩散模型成为各种行业中先进3D应用的有前途的方法。
- en: The studies on diffusion model-based 3DPCs offer innovative solutions across
    various domains. Zheng et al. [[331](#bib.bib331)] introduce PointDif, a pre-training
    method for 3D models that employs a conditional point generator to enhance feature
    aggregation and guide point-to-point recovery, showing notable improvements in
    classification and segmentation tasks. Kasten et al. [[332](#bib.bib332)] describe
    SDS-Complete, a text-to-image diffusion model for completing 3D objects from partial
    point clouds, demonstrating its effectiveness in handling Out-Of-Distribution
    objects. Liu’s study [[333](#bib.bib333)] focuses on a Diffusion Probabilistic
    Network for point cloud segmentation, utilizing a reverse diffusion process to
    refine topological structures. Jiang et al. [[334](#bib.bib334)] develop an SE(3)
    diffusion model-based framework for 3D registration, achieving precise object
    pose alignment by manipulating noise in the SE(3) manifold. Jin introduces [[335](#bib.bib335)]
    a multiway point cloud mosaicking approach, employing a diffusion-based denoising
    process for enhanced registration accuracy. Feng’s [[336](#bib.bib336)] DiffPoint
    combines Vision Transformers with diffusion models for 2D-to-3D reconstruction,
    achieving superior results in both single and multi-view tasks. Sharma [[337](#bib.bib337)]
    proposes a class-conditioned diffusion model to generate synthetic point cloud
    embeddings, significantly enhancing classification performance. Mo’s [[338](#bib.bib338)]
    DiT-3D uses a Diffusion Transformer to generate high-quality 3D shapes from voxelized
    point clouds, integrating 3D window attention for computational efficiency. Yi’s
    [[339](#bib.bib339)] GaussianDreamer leverages both 2D and 3D diffusion models
    for rapid, high-quality 3D generation from text prompts. Lastly, Ho [[340](#bib.bib340)]
    introduces Diffusion-SS3D, integrating diffusion models into a semi-supervised
    learning framework for 3D object detection, improving the quality of pseudo-labels
    and enhancing detection accuracy in diverse 3D spaces. These advancements collectively
    highlight the versatility and effectiveness of diffusion models in enhancing 3DPC
    processing and analysis.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 基于扩散模型的3D点云（3DPC）研究在各个领域提供了创新的解决方案。郑等人[[331](#bib.bib331)]介绍了PointDif，这是一种用于3D模型的预训练方法，采用条件点生成器来增强特征聚合并指导点对点的恢复，在分类和分割任务中显示出显著的改进。卡斯滕等人[[332](#bib.bib332)]描述了SDS-Complete，这是一种将文本转化为图像的扩散模型，用于从部分点云中完成3D对象，展示了其在处理分布外对象方面的有效性。刘的研究[[333](#bib.bib333)]关注于点云分割的扩散概率网络，利用反向扩散过程来优化拓扑结构。姜等人[[334](#bib.bib334)]开发了一个基于SE(3)扩散模型的3D配准框架，通过操控SE(3)流形中的噪声来实现精确的对象姿态对齐。金介绍了[[335](#bib.bib335)]一种多向点云镶嵌方法，采用基于扩散的去噪过程以提高配准精度。冯的[[336](#bib.bib336)]DiffPoint将视觉变换器与扩散模型结合用于2D到3D重建，在单视角和多视角任务中都取得了优异的结果。香玛[[337](#bib.bib337)]提出了一种类别条件的扩散模型，用于生成合成点云嵌入，显著提升了分类性能。莫的[[338](#bib.bib338)]DiT-3D使用扩散变换器从体素化点云生成高质量的3D形状，集成了3D窗口注意力以提高计算效率。易的[[339](#bib.bib339)]GaussianDreamer利用2D和3D扩散模型实现了从文本提示快速生成高质量3D模型。最后，霍[[340](#bib.bib340)]介绍了Diffusion-SS3D，将扩散模型整合到半监督学习框架中用于3D对象检测，改善了伪标签的质量并提升了在多样3D空间中的检测精度。这些进展共同突显了扩散模型在提升3D点云处理和分析中的多样性和有效性。
- en: Ohno et al. (2024) present a privacy-centric pedestrian tracking system using
    3D LiDARs, capturing pedestrians as anonymous point clouds and leveraging a generative
    diffusion model to predict trajectories in unmonitored areas, achieving a high
    F-measure of 0.98 [[341](#bib.bib341)]. In a different approach, Bi et al. (2024)
    introduce DiffusionEMIS, a novel paradigm that models 3-D electromagnetic inverse
    scattering as a denoising diffusion process, significantly improving performance
    and noise resistance over conventional methods [[342](#bib.bib342)]. Dutt (2024)
    develops Diff3F, which distills diffusion features onto untextured shapes, demonstrating
    robustness and reliability across several benchmarks [[343](#bib.bib343)]. Simultaneously,
    Li (2024) proposes a framework for generating stable crystal structures using
    a point cloud-based diffusion model, enhancing material design [[344](#bib.bib344)].
    Expanding the applications, Ze (2024) integrates 3D visual representations into
    diffusion models for robotic imitation learning with the 3D Diffusion Policy (DP3),
    which shows remarkable success rates and generalizability [[345](#bib.bib345)].
    Addressing point cloud registration, She (2024) utilizes graph neural PDEs and
    heat kernel signatures to enhance keypoint matching accuracy and robustness [[346](#bib.bib346)].
    Meanwhile, Li (2024) details a method for reconstructing 3D colored objects from
    images using a conditional diffusion model, achieving high fidelity in shape and
    color reconstruction [[347](#bib.bib347)]. Chen (2024) introduces V3D, adapting
    video diffusion models for 3D generation that produce high-quality outputs rapidly
    with impressive multi-view consistency [[348](#bib.bib348)]. Further, Hu (2024)
    presents a novel generative model combining latent diffusion with topological
    features, enabling diverse and adaptable 3D shape generation [[349](#bib.bib349)].
    Finally, Dong (2024) proposes a novel generative model for man-made shapes, incorporating
    diffusion processes with a quality checker to ensure geometric feasibility and
    physical stability, significantly outperforming traditional methods on ShapeNet-v2
    [[350](#bib.bib350)].
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: Ohno 等人（2024）提出了一种以隐私为中心的行人跟踪系统，使用 3D LiDAR 捕捉行人作为匿名点云，并利用生成扩散模型预测无人监控区域的轨迹，实现了
    0.98 的高 F-值[[341](#bib.bib341)]。另一种方法，Bi 等人（2024）介绍了 DiffusionEMIS，一种新颖的范式，将 3D
    电磁逆散射建模为去噪扩散过程，相比传统方法显著提高了性能和抗噪声能力[[342](#bib.bib342)]。Dutt（2024）开发了 Diff3F，该方法将扩散特征提炼到无纹理形状上，展示了在多个基准测试中的稳健性和可靠性[[343](#bib.bib343)]。与此同时，Li（2024）提出了一个使用基于点云的扩散模型生成稳定晶体结构的框架，提升了材料设计[[344](#bib.bib344)]。扩展应用方面，Ze（2024）将
    3D 视觉表示集成到扩散模型中，用于机器人模仿学习，采用 3D 扩散策略（DP3），显示了显著的成功率和泛化能力[[345](#bib.bib345)]。在点云配准方面，She（2024）利用图神经
    PDE 和热核签名提升了关键点匹配的准确性和鲁棒性[[346](#bib.bib346)]。与此同时，Li（2024）详细描述了一种使用条件扩散模型从图像重建
    3D 彩色物体的方法，实现了高保真度的形状和颜色重建[[347](#bib.bib347)]。Chen（2024）介绍了 V3D，将视频扩散模型用于 3D
    生成，能够快速生成高质量输出，并具有令人印象深刻的多视角一致性[[348](#bib.bib348)]。此外，Hu（2024）提出了一种新颖的生成模型，将潜在扩散与拓扑特征结合，实现了多样化和适应性的
    3D 形状生成[[349](#bib.bib349)]。最后，Dong（2024）提出了一种针对人造形状的新颖生成模型，结合扩散过程和质量检查器，以确保几何可行性和物理稳定性，在
    ShapeNet-v2 上显著超越传统方法[[350](#bib.bib350)]。
- en: Several future directions can be suggested to further improve the use of diffusion
    models in 3DPC. These include enhancing privacy while utilizing detailed data,
    improving computational efficiency in model adaptation with minimal trainable
    parameters, and increasing robustness against data quality variations. Additionally,
    there’s a trend toward cross-domain applications and interdisciplinary approaches,
    such as integrating 2D image processing techniques. The generation of complex
    and detailed 3D structures, especially in dynamic environments, is also a focal
    point, along with the development of semi-supervised and fully automated learning
    systems. Innovations in incorporating topological and geometric features into
    diffusion processes highlight the potential for more precise and versatile models.
    Finally, there’s a growing need for standardization in benchmarking methods to
    evaluate and compare the performance of diffusion models in handling 3DPCs. These
    insights indicate a move towards more efficient, robust, and application-diverse
    uses of diffusion models in the realm of 3D data analysis.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 可以提出几个未来方向，以进一步改善扩散模型在3DPC中的应用。这些包括在利用详细数据时提高隐私保护、改善模型适应的计算效率（尤其是最小可训练参数的情况下）以及增强对数据质量变化的鲁棒性。此外，还有一个趋势是跨领域应用和跨学科方法，如整合2D图像处理技术。复杂和详细3D结构的生成，特别是在动态环境中，也是一个重点，此外，还包括半监督和全自动学习系统的发展。将拓扑和几何特征融入扩散过程的创新突出显示了更精确和多功能模型的潜力。最后，对于在评估和比较扩散模型在处理3DPC时的性能的基准方法，越来越需要标准化。这些见解表明，扩散模型在3D数据分析领域的使用正朝着更高效、更鲁棒以及应用更加多样化的方向发展。
- en: 7 Conclusion
  id: totrans-568
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: With the rapid advancement of 3D scanning technologies, such as [LiDAR](#id18.18.id18)
    and RGB-D cameras, capturing and processing [3DPC](#id1.1.id1) data has become
    increasingly popular in various fields, including robotics, [CV](#id3.3.id3),
    virtual reality, and autonomous vehicles. On the other hand, deep [DTL](#id6.6.id6),
    a cutting-edge technique in [ML](#id5.5.id5) and [DL](#id4.4.id4), has emerged
    as a powerful approach to leverage pre-trained DNNs and transfer their knowledge
    to new [3DPC](#id1.1.id1) tasks overcome several challenges to using [DL](#id4.4.id4).
    To shed light on the latest innovations related to using [DTL](#id6.6.id6) in
    [3DPCs](#id1.1.id1), this article presented a comprehensive review of the state-of-the-art
    techniques for [3DPC](#id1.1.id1) understanding using [DTL](#id6.6.id6) and DA,
    including [3DPC](#id1.1.id1) object detection, [3DPC](#id1.1.id1) semantic labeling,
    segmentation and classification, [3DPC](#id1.1.id1) registration, downsampling/upsampling
    and [3DPC](#id1.1.id1) denoising. In doing so, a well-defined taxonomy has been
    introduced, and detailed comparisons have been conducted, presented with reference
    to different aspects, such as the types of adopted [DL](#id4.4.id4) models, obtained
    performance, and knowledge transfer strategies (fine-tuning, DA, [UDA](#id8.8.id8),
    etc.). Moving forward, the pros and cons of presented frameworks have been covered
    before identifying open challenges. Lastly, potential research directions have
    been listed.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 随着3D扫描技术的快速进步，如[LiDAR](#id18.18.id18)和RGB-D相机，捕捉和处理[3DPC](#id1.1.id1)数据在包括机器人技术、[CV](#id3.3.id3)、虚拟现实和自动驾驶车辆等多个领域变得越来越流行。另一方面，深度[DTL](#id6.6.id6)作为一种前沿技术，在[ML](#id5.5.id5)和[DL](#id4.4.id4)领域中出现，成为利用预训练DNNs并将其知识迁移到新的[3DPC](#id1.1.id1)任务中的一种强大方法，克服了使用[DL](#id4.4.id4)的一些挑战。为了阐明与使用[DTL](#id6.6.id6)在[3DPCs](#id1.1.id1)中的最新创新，本文提供了对使用[DTL](#id6.6.id6)和DA进行[3DPC](#id1.1.id1)理解的前沿技术的全面综述，包括[3DPC](#id1.1.id1)目标检测、[3DPC](#id1.1.id1)语义标注、分割和分类、[3DPC](#id1.1.id1)配准、下采样/上采样以及[3DPC](#id1.1.id1)去噪。在此过程中，介绍了一个明确的分类法，并进行了详细比较，涉及不同方面，如采用的[DL](#id4.4.id4)模型类型、获得的性能以及知识转移策略（微调、DA、[UDA](#id8.8.id8)等）。接下来，讨论了所提出框架的优缺点，并指出了开放性挑战。最后，列出了潜在的研究方向。
- en: Point cloud understanding has made significant progress through years of research,
    but as it becomes more widely used in real-world applications, it faces new challenges.
    One major challenge is partial overlap in sensor-acquired point clouds, which
    makes direct registration difficult. While some solutions have been developed
    for alignment under partial overlap, the rate of overlap is often limited. Finding
    a comprehensive solution to this problem, therefore, is a valuable and promising
    area of research. Moreover, only simple objects can currently be solved for alignment,
    despite the fact that the [3DPC](#id1.1.id1) registration techniques that combine
    [DTL](#id6.6.id6) and conventional methods have made significant advancements.
    For instance, when dealing with complex scenarios and large-scale [3DPCs](#id1.1.id1),
    these approaches fall short of the desired outcomes and continue to use the conventional
    algorithms. However, these algorithms are stochastic, and the number of iterations
    rise exponentially with outliers. By combining [DTL](#id6.6.id6) and conventional
    [ML](#id5.5.id5) methods, better results are obtained. Specifically, traditional
    [ML](#id5.5.id5) approaches are transparent, whereas [DTL](#id6.6.id6) schemes
    excel at fitting data. Hence, one of the trends for the future research is how
    to combine the benefits of both.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 点云理解经过多年的研究取得了显著进展，但随着其在实际应用中的广泛使用，它面临着新的挑战。其中一个主要挑战是传感器获取的点云部分重叠，这使得直接配准变得困难。虽然已经开发出一些用于部分重叠对齐的解决方案，但重叠率通常有限。因此，找到一个全面解决这个问题的方案是一个有价值且充满前景的研究领域。此外，尽管[3DPC](#id1.1.id1)注册技术结合了[DTL](#id6.6.id6)和传统方法，已经取得了显著进展，但目前只能解决简单对象的对齐问题。例如，在处理复杂场景和大规模[3DPCs](#id1.1.id1)时，这些方法无法达到理想的结果，仍然使用传统算法。然而，这些算法是随机的，且异常值的迭代次数呈指数级增长。通过结合[DTL](#id6.6.id6)和传统的[ML](#id5.5.id5)方法，可以获得更好的结果。具体来说，传统的[ML](#id5.5.id5)方法是透明的，而[DTL](#id6.6.id6)方案则在数据拟合方面表现出色。因此，未来研究的一个趋势是如何结合这两者的优点。
- en: Besides, the need for generalizing [3DPC](#id1.1.id1) scene understanding algorithms
    arises from the fact that different application scenarios present the algorithms
    with different challenges. Nevertheless, given the state of the research, it is
    difficult to suggest a general algorithm. For instance, an aircraft’s skin is
    very large, its surface is smooth, and it has few features with a curvature. When
    feature-based methods are used in the registration process, significant misalignment
    will consequently happen. The development of targeted, lightweight, and efficient
    algorithms for particular application scenarios is thus an attractive research
    hotspot in the near future.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，推广[3DPC](#id1.1.id1)场景理解算法的需求源于不同应用场景对算法提出的不同挑战。然而，鉴于目前的研究状态，很难提出一种通用的算法。例如，飞机的外皮非常大，表面光滑，且曲率特征较少。当在配准过程中使用基于特征的方法时，往往会出现显著的错位。因此，开发针对特定应用场景的轻量级高效算法将成为未来研究的一个吸引人的热点。
- en: References
  id: totrans-572
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y. Himeur, S. Al-Maadeed, H. Kheddar, N. Al-Maadeed, K. Abualsaud, A. Mohamed,
    T. Khattab, Video surveillance using deep transfer learning and deep domain adaptation:
    Towards better generalization, Engineering Applications of Artificial Intelligence
    119 (2023) 105698.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. Himeur, S. Al-Maadeed, H. Kheddar, N. Al-Maadeed, K. Abualsaud, A. Mohamed,
    T. Khattab, 使用深度迁移学习和深度领域适应的视频监控：朝着更好的泛化方向，《人工智能工程应用》119（2023）105698。'
- en: '[2] A. Copiaco, Y. Himeur, A. Amira, W. Mansoor, F. Fadli, S. Atalla, S. S.
    Sohail, An innovative deep anomaly detection of building energy consumption using
    energy time-series images, Engineering Applications of Artificial Intelligence
    119 (2023) 105775.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Copiaco, Y. Himeur, A. Amira, W. Mansoor, F. Fadli, S. Atalla, S. S.
    Sohail, 使用能量时间序列图像进行建筑能耗的创新深度异常检测，《人工智能工程应用》119（2023）105775。'
- en: '[3] Y. Zhou, A. Ji, L. Zhang, X. Xue, Attention-enhanced sampling point cloud
    network (aspcnet) for efficient 3d tunnel semantic segmentation, Automation in
    Construction 146 (2023) 104667.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Zhou, A. Ji, L. Zhang, X. Xue, 注意力增强采样点云网络（aspcnet）用于高效的3D隧道语义分割，《建筑自动化》146（2023）104667。'
- en: '[4] Y. Habchi, Y. Himeur, H. Kheddar, A. Boukabou, S. Atalla, A. Chouchane,
    A. Ouamane, W. Mansoor, Ai in thyroid cancer diagnosis: Techniques, trends, and
    future directions, Systems 11 (10) (2023) 519.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Y. Habchi, Y. Himeur, H. Kheddar, A. Boukabou, S. Atalla, A. Chouchane,
    A. Ouamane, W. Mansoor, 甲状腺癌诊断中的人工智能：技术、趋势与未来方向，《系统》11（10）（2023）519。'
- en: '[5] A. Ji, Y. Zhou, L. Zhang, R. L. Tiong, X. Xue, Semi-supervised learning-based
    point cloud network for segmentation of 3d tunnel scenes, Automation in Construction
    146 (2023) 104668.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. Ji, Y. Zhou, L. Zhang, R. L. Tiong, X. Xue, 基于半监督学习的点云网络用于3D隧道场景分割，《建筑自动化》146
    (2023) 104668。'
- en: '[6] O. Kerdjidj, Y. Himeur, S. Atalla, A. Copiaco, A. Amira, F. Fadli, S. S.
    Sohail, W. Mansoor, A. Gawanmeh, S. Miniaoui, Exploiting 2d representations for
    enhanced indoor localization: A transfer learning approach, IEEE Sensors Journal
    (2024).'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] O. Kerdjidj, Y. Himeur, S. Atalla, A. Copiaco, A. Amira, F. Fadli, S. S.
    Sohail, W. Mansoor, A. Gawanmeh, S. Miniaoui, 利用2D表示增强室内定位：一种迁移学习方法，《IEEE传感器学报》（2024）。'
- en: '[7] C. Fotsing, P. Hahn, D. Cunningham, C. Bobda, Volumetric wall detection
    in unorganized indoor point clouds using continuous segments in 2d grids, Automation
    in Construction 141 (2022) 104462.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. Fotsing, P. Hahn, D. Cunningham, C. Bobda, 在无组织的室内点云中使用连续段进行体积墙检测，《建筑自动化》141
    (2022) 104462。'
- en: '[8] X. Lai, J. Liu, L. Jiang, L. Wang, H. Zhao, S. Liu, X. Qi, J. Jia, Stratified
    transformer for 3d point cloud segmentation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, IEEE, New Orleans, LA, USA, 2022,
    pp. 8500–8509.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] X. Lai, J. Liu, L. Jiang, L. Wang, H. Zhao, S. Liu, X. Qi, J. Jia, 用于3D点云分割的分层变换器，见：IEEE/CVF计算机视觉与模式识别会议论文集，IEEE，美国新奥尔良，2022年，第8500–8509页。'
- en: '[9] A. Bechar, Y. Elmir, Y. Himeur, R. Medjoudj, A. Amira, Federated and transfer
    learning for cancer detection based on image analysis, arXiv preprint arXiv:2405.20126
    (2024).'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Bechar, Y. Elmir, Y. Himeur, R. Medjoudj, A. Amira, 基于图像分析的癌症检测的联邦与迁移学习，arXiv预印本
    arXiv:2405.20126 (2024)。'
- en: '[10] R. Abbasi, A. K. Bashir, H. J. Alyamani, F. Amin, J. Doh, J. Chen, Lidar
    point cloud compression, processing and learning for autonomous driving, IEEE
    Transactions on Intelligent Transportation Systems (2022).'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] R. Abbasi, A. K. Bashir, H. J. Alyamani, F. Amin, J. Doh, J. Chen, 激光雷达点云压缩、处理与学习用于自动驾驶，《IEEE智能交通系统学报》（2022）。'
- en: '[11] D. Liu, W. Hu, Imperceptible transfer attack and defense on 3d point cloud
    classification, IEEE Transactions on Pattern Analysis and Machine Intelligence
    (2022).'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] D. Liu, W. Hu, 对3D点云分类的不可感知转移攻击与防御，《IEEE模式分析与机器智能学报》（2022）。'
- en: '[12] Q. Yu, C. Yang, H. Wei, Part-wise atlasnet for 3d point cloud reconstruction
    from a single image, Knowledge-Based Systems 242 (2022) 108395.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Q. Yu, C. Yang, H. Wei, 用于从单张图像进行3D点云重建的部分地图网，《知识基础系统》242 (2022) 108395。'
- en: '[13] Z. Li, Z. Chen, A. Li, L. Fang, Q. Jiang, X. Liu, J. Jiang, B. Zhou, H. Zhao,
    Simipu: Simple 2d image and 3d point cloud unsupervised pre-training for spatial-aware
    visual representations, in: Proceedings of the AAAI Conference on Artificial Intelligence,
    Vol. 36, AAAI, online/virtual, 2022, pp. 1500–1508.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Z. Li, Z. Chen, A. Li, L. Fang, Q. Jiang, X. Liu, J. Jiang, B. Zhou, H.
    Zhao, Simipu：用于空间感知视觉表示的简单2D图像和3D点云无监督预训练，见：AAAI人工智能会议论文集，第36卷，AAAI，在线/虚拟，2022年，第1500–1508页。'
- en: '[14] O. Elharrouss, S. Al-Maadeed, N. Subramanian, N. Ottakath, N. Almaadeed,
    Y. Himeur, Panoptic segmentation: a review, arXiv preprint arXiv:2111.10250 (2021).'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] O. Elharrouss, S. Al-Maadeed, N. Subramanian, N. Ottakath, N. Almaadeed,
    Y. Himeur, 全景分割：综述，arXiv预印本 arXiv:2111.10250 (2021)。'
- en: '[15] S. Cao, H. Zhao, P. Liu, Semantic segmentation for point clouds via semantic-based
    local aggregation and multi-scale global pyramid, Machines 11 (1) (2023) 11.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. Cao, H. Zhao, P. Liu, 通过基于语义的局部聚合和多尺度全局金字塔进行点云语义分割，《Machines》11 (1)
    (2023) 11。'
- en: '[16] W. Liu, Y. Zang, Z. Xiong, X. Bian, C. Wen, X. Lu, C. Wang, J. M. Junior,
    W. N. Gonçalves, J. Li, 3d building model generation from mls point cloud and
    3d mesh using multi-source data fusion, International Journal of Applied Earth
    Observation and Geoinformation 116 (2023) 103171.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] W. Liu, Y. Zang, Z. Xiong, X. Bian, C. Wen, X. Lu, C. Wang, J. M. Junior,
    W. N. Gonçalves, J. Li, 从MLS点云和3D网格生成3D建筑模型，通过多源数据融合，《应用地球观测与地理信息国际期刊》116 (2023)
    103171。'
- en: '[17] X. Yang, E. del Rey Castillo, Y. Zou, L. Wotherspoon, Y. Tan, Automated
    semantic segmentation of bridge components from large-scale point clouds using
    a weighted superpoint graph, Automation in Construction 142 (2022) 104519.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] X. Yang, E. del Rey Castillo, Y. Zou, L. Wotherspoon, Y. Tan, 使用加权超点图从大规模点云中自动化桥梁组件语义分割，《建筑自动化》142
    (2022) 104519。'
- en: '[18] W. Liu, Y. Shao, K. Chen, C. Li, H. Luo, Whale optimization algorithm-based
    point cloud data processing method for sewer pipeline inspection, Automation in
    Construction 141 (2022) 104423.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] W. Liu, Y. Shao, K. Chen, C. Li, H. Luo, 基于鲸鱼优化算法的点云数据处理方法用于下水道管道检测，《建筑自动化》141
    (2022) 104423。'
- en: '[19] I. Ahmed, G. Jeon, F. Piccialli, A deep-learning-based smart healthcare
    system for patient’s discomfort detection at the edge of internet of things, IEEE
    Internet of Things Journal 8 (13) (2021) 10318–10326.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] I. Ahmed, G. Jeon, F. Piccialli, 基于深度学习的智能医疗系统用于物联网边缘患者不适检测，IEEE Internet
    of Things Journal 8 (13) (2021) 10318–10326。'
- en: '[20] N. Mehdiyev, J. Evermann, P. Fettke, A novel business process prediction
    model using a deep learning method, Business & information systems engineering
    62 (2) (2020) 143–157.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] N. Mehdiyev, J. Evermann, P. Fettke, 一种基于深度学习的方法的新型业务过程预测模型，Business &
    information systems engineering 62 (2) (2020) 143–157。'
- en: '[21] R. Kiran, P. Kumar, B. Bhasker, Dnnrec: A novel deep learning based hybrid
    recommender system, Expert Systems with Applications 144 (2020) 113054.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] R. Kiran, P. Kumar, B. Bhasker, Dnnrec：一种新型基于深度学习的混合推荐系统，Expert Systems
    with Applications 144 (2020) 113054。'
- en: '[22] Y. Himeur, K. Ghanem, A. Alsalemi, F. Bensaali, A. Amira, Artificial intelligence
    based anomaly detection of energy consumption in buildings: A review, current
    trends and new perspectives, Applied Energy 287 (2021) 116601.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Y. Himeur, K. Ghanem, A. Alsalemi, F. Bensaali, A. Amira, 基于人工智能的建筑能源消费异常检测：综述、现状和新视角，Applied
    Energy 287 (2021) 116601。'
- en: '[23] P. K. Kashyap, S. Kumar, A. Jaiswal, M. Prasad, A. H. Gandomi, Towards
    precision agriculture: Iot-enabled intelligent irrigation systems using deep learning
    neural network, IEEE Sensors Journal 21 (16) (2021) 17479–17491.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] P. K. Kashyap, S. Kumar, A. Jaiswal, M. Prasad, A. H. Gandomi, 朝着精准农业：基于深度学习神经网络的物联网智能灌溉系统，IEEE
    Sensors Journal 21 (16) (2021) 17479–17491。'
- en: '[24] D. De Gregorio, A. Tonioni, G. Palli, L. Di Stefano, Semiautomatic labeling
    for deep learning in robotics, IEEE Transactions on Automation Science and Engineering
    17 (2) (2019) 611–620.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] D. De Gregorio, A. Tonioni, G. Palli, L. Di Stefano, 机器人深度学习的半自动标注，IEEE
    Transactions on Automation Science and Engineering 17 (2) (2019) 611–620。'
- en: '[25] A. N. Sayed, Y. Himeur, F. Bensaali, Deep and transfer learning for building
    occupancy detection: A review and comparative analysis, Engineering Applications
    of Artificial Intelligence 115 (2022) 105254.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. N. Sayed, Y. Himeur, F. Bensaali, 建筑物占用检测的深度学习与迁移学习：综述与比较分析，Engineering
    Applications of Artificial Intelligence 115 (2022) 105254。'
- en: '[26] H. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, F. Bensaali, Deep transfer
    learning for automatic speech recognition: Towards better generalization, Knowledge-Based
    Systems 277 (2023) 110851. [doi:10.1016/j.knosys.2023.110851](https://doi.org/10.1016/j.knosys.2023.110851).'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] H. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, F. Bensaali, 用于自动语音识别的深度迁移学习：朝着更好的泛化，Knowledge-Based
    Systems 277 (2023) 110851。[doi:10.1016/j.knosys.2023.110851](https://doi.org/10.1016/j.knosys.2023.110851)。'
- en: '[27] H. Kheddar, Y. Himeur, A. I. Awad, Deep transfer learning for intrusion
    detection in industrial control networks: A comprehensive review, J. Netw. Comput.
    Appl. 220 (2023) 103760.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] H. Kheddar, Y. Himeur, A. I. Awad, 工业控制网络入侵检测的深度迁移学习：全面评述，J. Netw. Comput.
    Appl. 220 (2023) 103760。'
- en: '[28] B. Kitchenham, Procedures for performing systematic reviews, Keele, UK,
    Keele University 33 (2004) (2004) 1–26.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] B. Kitchenham, 系统评价执行程序，英国基尔大学 33 (2004) (2004) 1–26。'
- en: '[29] Y. Himeur, A. Alsalemi, A. Al-Kababji, F. Bensaali, A. Amira, C. Sardianos,
    G. Dimitrakopoulos, I. Varlamis, A survey of recommender systems for energy efficiency
    in buildings: Principles, challenges and prospects, Information Fusion 72 (2021)
    1–21.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Y. Himeur, A. Alsalemi, A. Al-Kababji, F. Bensaali, A. Amira, C. Sardianos,
    G. Dimitrakopoulos, I. Varlamis, 关于建筑物能源效率的推荐系统综述：原则、挑战和前景，Information Fusion
    72 (2021) 1–21。'
- en: '[30] S. A. Bello, S. Yu, C. Wang, J. M. Adam, J. Li, Deep learning on 3d point
    clouds, Remote Sensing 12 (11) (2020) 1729.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] S. A. Bello, S. Yu, C. Wang, J. M. Adam, J. Li, 3D点云的深度学习，Remote Sensing
    12 (11) (2020) 1729。'
- en: '[31] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, M. Bennamoun, Deep learning for
    3d point clouds: A survey, IEEE transactions on pattern analysis and machine intelligence
    43 (12) (2020) 4338–4364.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, M. Bennamoun, 3D点云的深度学习：综述，IEEE
    Transactions on Pattern Analysis and Machine Intelligence 43 (12) (2020) 4338–4364。'
- en: '[32] Y. Li, Deep reinforcement learning: An overview, arXiv preprint arXiv:1701.07274
    (2017).'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Y. Li, 深度强化学习：概述，arXiv预印本 arXiv:1701.07274 (2017)。'
- en: '[33] P. Z. Ramirez, A. Tonioni, S. Salti, L. D. Stefano, Learning across tasks
    and domains, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, IEEE, Seoul, Korea (South), 2019, pp. 8110–8119.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] P. Z. Ramirez, A. Tonioni, S. Salti, L. D. Stefano, 跨任务和领域的学习，见：IEEE/CVF国际计算机视觉大会论文集，IEEE，韩国首尔，2019年，pp.
    8110–8119。'
- en: '[34] M. A. Morid, A. Borjali, G. Del Fiol, A scoping review of transfer learning
    research on medical image analysis using imagenet, Computers in biology and medicine
    128 (2021) 104115.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] M. A. Morid, A. Borjali, G. Del Fiol, 关于使用 ImageNet 进行医学图像分析的迁移学习研究的范围综述，《计算机生物医学》128
    (2021) 104115。'
- en: '[35] W. Li, W. Huan, B. Hou, Y. Tian, Z. Zhang, A. Song, Can emotion be transferred?–a
    review on transfer learning for eeg-based emotion recognition, IEEE Transactions
    on Cognitive and Developmental Systems (2021).'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] W. Li, W. Huan, B. Hou, Y. Tian, Z. Zhang, A. Song, 情感能否迁移？——基于 EEG 的情感识别迁移学习综述，《IEEE
    认知与发展系统汇刊》 (2021)。'
- en: '[36] Z. Alyafeai, M. S. AlShaibani, I. Ahmad, A survey on transfer learning
    in natural language processing, arXiv preprint arXiv:2007.04239 (2020).'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Z. Alyafeai, M. S. AlShaibani, I. Ahmad, 关于自然语言处理中的迁移学习的调查，arXiv 预印本 arXiv:2007.04239
    (2020)。'
- en: '[37] R. Ribani, M. Marengoni, A survey of transfer learning for convolutional
    neural networks, in: 2019 32nd SIBGRAPI conference on graphics, patterns and images
    tutorials (SIBGRAPI-T), IEEE, Rio de Janeiro, Brazil, 2019, pp. 47–57.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] R. Ribani, M. Marengoni, 关于卷积神经网络的迁移学习调查，载于：2019年第32届 SIBGRAPI 图形、模式与图像教程会议
    (SIBGRAPI-T)，IEEE，里约热内卢，巴西，2019，页 47–57。'
- en: '[38] Z. Wan, R. Yang, M. Huang, N. Zeng, X. Liu, A review on transfer learning
    in eeg signal analysis, Neurocomputing 421 (2021) 1–14.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Z. Wan, R. Yang, M. Huang, N. Zeng, X. Liu, 关于 EEG 信号分析中的迁移学习的综述，《神经计算》421
    (2021) 1–14。'
- en: '[39] Z.-H. Liu, L.-B. Jiang, H.-L. Wei, L. Chen, X.-H. Li, Optimal transport-based
    deep domain adaptation approach for fault diagnosis of rotating machine, IEEE
    Transactions on Instrumentation and Measurement 70 (2021) 1–12.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Z.-H. Liu, L.-B. Jiang, H.-L. Wei, L. Chen, X.-H. Li, 基于最优传输的深度领域适应方法用于旋转机器的故障诊断，《IEEE
    仪器与测量汇刊》70 (2021) 1–12。'
- en: '[40] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, C. Liu, A survey on deep transfer
    learning, in: International conference on artificial neural networks, Springer,
    Rhodes, Greece, 2018, pp. 270–279.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, C. Liu, 关于深度迁移学习的综述，载于：国际人工神经网络会议，Springer，罗德岛，希腊，2018，页
    270–279。'
- en: '[41] S. Niu, Y. Jiang, B. Chen, J. Wang, Y. Liu, H. Song, Cross-modality transfer
    learning for image-text information management, ACM Transactions on Management
    Information System (TMIS) 13 (1) (2021) 1–14.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Niu, Y. Jiang, B. Chen, J. Wang, Y. Liu, H. Song, 跨模态迁移学习用于图像-文本信息管理，《ACM
    管理信息系统汇刊 (TMIS)》13 (1) (2021) 1–14。'
- en: '[42] J. Si, H. Shi, J. Chen, C. Zheng, Unsupervised deep transfer learning
    with moment matching: A new intelligent fault diagnosis approach for bearings,
    Measurement 172 (2021) 108827.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. Si, H. Shi, J. Chen, C. Zheng, 基于时刻匹配的无监督深度迁移学习：一种新的智能轴承故障诊断方法，《测量》172
    (2021) 108827。'
- en: '[43] Y. Zhou, J. Yang, H. Li, T. Cao, S.-Y. Kung, Adversarial learning for
    multiscale crowd counting under complex scenes, IEEE transactions on cybernetics
    51 (11) (2020) 5423–5432.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Y. Zhou, J. Yang, H. Li, T. Cao, S.-Y. Kung, 在复杂场景下的多尺度人群计数对抗学习，《IEEE
    网络系统汇刊》51 (11) (2020) 5423–5432。'
- en: '[44] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, V. Lempitsky, Domain-adversarial training of neural networks, The
    journal of machine learning research 17 (1) (2016) 2096–2030.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, V. Lempitsky, 神经网络的领域对抗训练，《机器学习研究杂志》17 (1) (2016) 2096–2030。'
- en: '[45] Z. Shen, Y. Xu, B. Ni, M. Wang, J. Hu, X. Yang, Crowd counting via adversarial
    cross-scale consistency pursuit, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 5245–5254.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Z. Shen, Y. Xu, B. Ni, M. Wang, J. Hu, X. Yang, 通过对抗跨尺度一致性追踪进行人群计数，载于：2018
    年 IEEE 计算机视觉与模式识别会议论文集，页 5245–5254。'
- en: '[46] M.-I. Georgescu, R. T. Ionescu, F. S. Khan, M. Popescu, M. Shah, A background-agnostic
    framework with adversarial training for abnormal event detection in video, arXiv
    preprint arXiv:2008.12328 (2020).'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] M.-I. Georgescu, R. T. Ionescu, F. S. Khan, M. Popescu, M. Shah, 一个背景无关的框架，通过对抗训练进行视频中异常事件检测，arXiv
    预印本 arXiv:2008.12328 (2020)。'
- en: '[47] W. Choi, W. Yang, J. Na, J. Park, G. Lee, W. Nam, Unsupervised gait phase
    estimation with domain-adversarial neural network and adaptive window, IEEE Journal
    of Biomedical and Health Informatics (2021).'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] W. Choi, W. Yang, J. Na, J. Park, G. Lee, W. Nam, 基于领域对抗神经网络和自适应窗口的无监督步态相位估计，《IEEE
    生物医学与健康信息学杂志》 (2021)。'
- en: '[48] E. Soleimani, E. Nazerfard, Cross-subject transfer learning in human activity
    recognition systems using generative adversarial networks, Neurocomputing 426
    (2021) 26–34.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] E. Soleimani, E. Nazerfard, 使用生成对抗网络在人类活动识别系统中的跨学科迁移学习，《神经计算》426 (2021)
    26–34。'
- en: '[49] J. Wang, Y. Chen, W. Feng, H. Yu, M. Huang, Q. Yang, Transfer learning
    with dynamic distribution adaptation, ACM Transactions on Intelligent Systems
    and Technology (TIST) 11 (1) (2020) 1–25.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] J. Wang, Y. Chen, W. Feng, H. Yu, M. Huang, Q. Yang, 动态分布适应的迁移学习, ACM智能系统与技术交易
    (TIST) 11 (1) (2020) 1–25。'
- en: '[50] Z. Zou, X. Qu, P. Zhou, S. Xu, X. Ye, W. Wu, J. Ye, Coarse to fine: Domain
    adaptive crowd counting via adversarial scoring network, in: Proceedings of the
    29th ACM International Conference on Multimedia, ACM, online, 2021, pp. 2185–2194.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Z. Zou, X. Qu, P. Zhou, S. Xu, X. Ye, W. Wu, J. Ye, 从粗到细: 通过对抗评分网络进行领域自适应人群计数,
    载于: 第29届ACM国际多媒体会议论文集, ACM, 在线, 2021, 第2185–2194页。'
- en: '[51] J. Sun, Y. Cao, C. B. Choy, Z. Yu, A. Anandkumar, Z. M. Mao, C. Xiao,
    Adversarially robust 3d point cloud recognition using self-supervisions, Advances
    in Neural Information Processing Systems 34 (2021) 15498–15512.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] J. Sun, Y. Cao, C. B. Choy, Z. Yu, A. Anandkumar, Z. M. Mao, C. Xiao,
    使用自我监督的对抗鲁棒3D点云识别, 神经信息处理系统进展 34 (2021) 15498–15512。'
- en: '[52] C. R. Qi, H. Su, K. Mo, L. J. Guibas, Pointnet: Deep learning on point
    sets for 3d classification and segmentation, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, IEEE, Honolulu, HI, USA, 2017, pp.
    652–660.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] C. R. Qi, H. Su, K. Mo, L. J. Guibas, Pointnet: 深度学习在点集上的3D分类和分割, 载于:
    IEEE计算机视觉与模式识别会议论文集, IEEE, 檀香山, HI, USA, 2017, 第652–660页。'
- en: '[53] C. R. Qi, L. Yi, H. Su, L. J. Guibas, Pointnet++: Deep hierarchical feature
    learning on point sets in a metric space, Advances in neural information processing
    systems 30 (2017).'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] C. R. Qi, L. Yi, H. Su, L. J. Guibas, Pointnet++: 在度量空间中对点集进行深度分层特征学习,
    神经信息处理系统进展 30 (2017)。'
- en: '[54] R. Li, Y. Zhang, D. Niu, G. Yang, N. Zafar, C. Zhang, X. Zhao, Pointvgg:
    Graph convolutional network with progressive aggregating features on point clouds,
    Neurocomputing 429 (2021) 187–198.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] R. Li, Y. Zhang, D. Niu, G. Yang, N. Zafar, C. Zhang, X. Zhao, Pointvgg:
    在点云上进行渐进聚合特征的图卷积网络, 神经计算 429 (2021) 187–198。'
- en: '[55] Y. Shi, C. Zhang, X. Zhang, K. Wang, Y. Zhang, X. Zhao, Pointpavgg: An
    incremental algorithm for extraction of points’ positional feature using vgg on
    point clouds, in: Intelligent Computing Theories and Application: 17th International
    Conference, ICIC 2021, Shenzhen, China, August 12–15, 2021, Proceedings, Part
    II, Springer, 2021, pp. 718–731.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Shi, C. Zhang, X. Zhang, K. Wang, Y. Zhang, X. Zhao, Pointpavgg: 使用VGG对点云提取点的位置特征的增量算法,
    载于: 智能计算理论与应用: 第17届国际会议, ICIC 2021, 深圳, 中国, 2021年8月12–15日, 会议论文集, 第II部分, Springer,
    2021, 第718–731页。'
- en: '[56] Y. Xu, T. Fan, M. Xu, L. Zeng, Y. Qiao, Spidercnn: Deep learning on point
    sets with parameterized convolutional filters, in: Proceedings of the European
    conference on computer vision (ECCV), Springer, Munich, Germany, 2018, pp. 87–102.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Xu, T. Fan, M. Xu, L. Zeng, Y. Qiao, Spidercnn: 带有参数化卷积滤波器的点集深度学习,
    载于: 欧洲计算机视觉会议论文集 (ECCV), Springer, 慕尼黑, 德国, 2018, 第87–102页。'
- en: '[57] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, B. Chen, Pointcnn: Convolution on
    x-transformed points, Advances in neural information processing systems 31 (2018).'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, B. Chen, Pointcnn: 在x变换点上的卷积, 神经信息处理系统进展
    31 (2018)。'
- en: '[58] H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis, M.-H. Yang, J. Kautz,
    Splatnet: Sparse lattice networks for point cloud processing, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, 2018, pp. 2530–2539.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis, M.-H. Yang, J. Kautz,
    Splatnet: 用于点云处理的稀疏格网, 载于: IEEE计算机视觉与模式识别会议论文集, 2018, 第2530–2539页。'
- en: '[59] W. Wang, R. Yu, Q. Huang, U. Neumann, Sgpn: Similarity group proposal
    network for 3d point cloud instance segmentation, in: Proceedings of the IEEE
    conference on computer vision and pattern recognition, 2018, pp. 2569–2578.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] W. Wang, R. Yu, Q. Huang, U. Neumann, Sgpn: 用于3D点云实例分割的相似性组提议网络, 载于: IEEE计算机视觉与模式识别会议论文集,
    2018, 第2569–2578页。'
- en: '[60] Y. Yang, C. Feng, Y. Shen, D. Tian, Foldingnet: Point cloud auto-encoder
    via deep grid deformation, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 206–215.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Y. Yang, C. Feng, Y. Shen, D. Tian, Foldingnet: 通过深度网格变形的点云自编码器, 载于: IEEE计算机视觉与模式识别会议论文集,
    2018, 第206–215页。'
- en: '[61] L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, P.-A. Heng, Pu-net: Point cloud upsampling
    network, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, 2018, pp. 2790–2799.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, P.-A. Heng, Pu-net: 点云上采样网络, 载于:
    IEEE计算机视觉与模式识别会议论文集, 2018, 第2790–2799页。'
- en: '[62] T. Le, Y. Duan, Pointgrid: A deep network for 3d shape understanding,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    2018, pp. 9204–9214.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] T. Le, Y. Duan, Pointgrid: 用于三维形状理解的深度网络, 见：IEEE计算机视觉与模式识别会议论文集, 2018,
    页9204–9214。'
- en: '[63] L. Ge, Y. Cai, J. Weng, J. Yuan, Hand pointnet: 3d hand pose estimation
    using point sets, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, 2018, pp. 8417–8426.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] L. Ge, Y. Cai, J. Weng, J. Yuan, Hand pointnet: 使用点集进行三维手部姿态估计, 见：IEEE计算机视觉与模式识别会议论文集,
    2018, 页8417–8426。'
- en: '[64] M. A. Uy, G. H. Lee, Pointnetvlad: Deep point cloud based retrieval for
    large-scale place recognition, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 4470–4479.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] M. A. Uy, G. H. Lee, Pointnetvlad: 基于深度点云的大规模地点识别检索, 见：IEEE计算机视觉与模式识别会议论文集,
    2018, 页4470–4479。'
- en: '[65] S. Huang, Z. Gojcic, M. Usvyatsov, A. Wieser, K. Schindler, Predator:
    Registration of 3d point clouds with low overlap, in: Proceedings of the IEEE/CVF
    Conference on computer vision and pattern recognition, 2021, pp. 4267–4276.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] S. Huang, Z. Gojcic, M. Usvyatsov, A. Wieser, K. Schindler, Predator:
    低重叠三维点云配准, 见：IEEE/CVF计算机视觉与模式识别会议论文集, 2021, 页4267–4276。'
- en: '[66] H. Wang, Y. Cong, O. Litany, Y. Gao, L. J. Guibas, 3dioumatch: Leveraging
    iou prediction for semi-supervised 3d object detection, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 14615–14624.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] H. Wang, Y. Cong, O. Litany, Y. Gao, L. J. Guibas, 3dioumatch: 利用IOU预测进行半监督三维物体检测,
    见：IEEE/CVF计算机视觉与模式识别会议论文集, 2021, 页14615–14624。'
- en: '[67] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, M. Nießner, Scannet:
    Richly-annotated 3d reconstructions of indoor scenes, in: Proceedings of the IEEE
    conference on computer vision and pattern recognition, 2017, pp. 5828–5839.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, M. Nießner, Scannet:
    富注释的室内场景三维重建, 见：IEEE计算机视觉与模式识别会议论文集, 2017, 页5828–5839。'
- en: '[68] G. Riegler, A. Osman Ulusoy, A. Geiger, Octnet: Learning deep 3d representations
    at high resolutions, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2017, pp. 3577–3586.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] G. Riegler, A. Osman Ulusoy, A. Geiger, Octnet: 在高分辨率下学习深度三维表示, 见：IEEE计算机视觉与模式识别会议论文集,
    2017, 页3577–3586。'
- en: '[69] J. Li, B. M. Chen, G. H. Lee, So-net: Self-organizing network for point
    cloud analysis, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, 2018, pp. 9397–9406.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] J. Li, B. M. Chen, G. H. Lee, So-net: 点云分析的自组织网络, 见：IEEE计算机视觉与模式识别会议论文集,
    2018, 页9397–9406。'
- en: '[70] B. Yang, W. Luo, R. Urtasun, Pixor: Real-time 3d object detection from
    point clouds, in: Proceedings of the IEEE conference on Computer Vision and Pattern
    Recognition, 2018, pp. 7652–7660.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] B. Yang, W. Luo, R. Urtasun, Pixor: 实时三维物体检测来自点云, 见：IEEE计算机视觉与模式识别会议论文集,
    2018, 页7652–7660。'
- en: '[71] Y. Zhou, O. Tuzel, Voxelnet: End-to-end learning for point cloud based
    3d object detection, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2018, pp. 4490–4499.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Y. Zhou, O. Tuzel, Voxelnet: 端到端学习用于基于点云的三维物体检测, 见：IEEE计算机视觉与模式识别会议论文集,
    2018, 页4490–4499。'
- en: '[72] H. Deng, T. Birdal, S. Ilic, Ppfnet: Global context aware local features
    for robust 3d point matching, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 195–205.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] H. Deng, T. Birdal, S. Ilic, Ppfnet: 用于稳健三维点匹配的全局上下文感知局部特征, 见：IEEE计算机视觉与模式识别会议论文集,
    2018, 页195–205。'
- en: '[73] D. Xu, D. Anguelov, A. Jain, Pointfusion: Deep sensor fusion for 3d bounding
    box estimation, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, 2018, pp. 244–253.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] D. Xu, D. Anguelov, A. Jain, Pointfusion: 深度传感器融合用于三维边界框估计, 见：IEEE计算机视觉与模式识别会议论文集,
    2018, 页244–253。'
- en: '[74] C. R. Qi, W. Liu, C. Wu, H. Su, L. J. Guibas, Frustum pointnets for 3d
    object detection from rgb-d data, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 918–927.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] C. R. Qi, W. Liu, C. Wu, H. Su, L. J. Guibas, Frustum pointnets用于基于RGB-D数据的三维物体检测,
    见：IEEE计算机视觉与模式识别会议论文集, 2018, 页918–927。'
- en: '[75] Z. J. Yew, G. H. Lee, 3dfeat-net: Weakly supervised local 3d features
    for point cloud registration, in: Proceedings of the European conference on computer
    vision (ECCV), 2018, pp. 607–623.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Z. J. Yew, G. H. Lee, 3dfeat-net: 用于点云配准的弱监督局部三维特征, 见：欧洲计算机视觉会议 (ECCV)
    论文集, 2018, 页607–623。'
- en: '[76] B. Wu, A. Wan, X. Yue, K. Keutzer, Squeezeseg: Convolutional neural nets
    with recurrent crf for real-time road-object segmentation from 3d lidar point
    cloud, in: 2018 IEEE international conference on robotics and automation (ICRA),
    IEEE, 2018, pp. 1887–1893.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] B. 吴, A. 万, X. 岳, K. 科伊策, Squeezeseg: 使用递归条件随机场的卷积神经网络用于实时道路物体分割, 见: 2018
    IEEE 国际机器人与自动化会议 (ICRA), IEEE, 2018, 第 1887–1893 页。'
- en: '[77] X. Yan, J. Gao, C. Zheng, C. Zheng, R. Zhang, S. Cui, Z. Li, 2dpass: 2d
    priors assisted semantic segmentation on lidar point clouds, in: European Conference
    on Computer Vision, Springer, 2022, pp. 677–695.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] X. 闫, J. 高, C. 郑, C. 郑, R. 张, S. 崔, Z. 李, 2dpass: 基于 2D 先验的激光雷达点云语义分割,
    见: 欧洲计算机视觉会议, Springer, 2022, 第 677–695 页。'
- en: '[78] W. Chen, J. Duan, H. Basevi, H. J. Chang, A. Leonardis, Pointposenet:
    Point pose network for robust 6d object pose estimation, in: Proceedings of the
    IEEE/CVF Winter Conference on Applications of Computer Vision, 2020, pp. 2824–2833.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] W. 陈, J. 段, H. 巴斯维, H. J. 张, A. 利奥纳迪斯, Pointposenet: 用于鲁棒 6D 物体姿态估计的点姿态网络,
    见: IEEE/CVF 计算机视觉应用冬季会议论文集, 2020, 第 2824–2833 页。'
- en: '[79] H. Cao, Y. Lu, C. Lu, B. Pang, G. Liu, A. Yuille, Asap-net: Attention
    and structure aware point cloud sequence segmentation, arXiv preprint arXiv:2008.05149
    (2020).'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] H. 曹, Y. 陆, C. 陆, B. 庞, G. 刘, A. 尤伊尔, Asap-net: 注意力和结构感知的点云序列分割, arXiv
    预印本 arXiv:2008.05149 (2020)。'
- en: '[80] S. Biswas, J. Liu, K. Wong, S. Wang, R. Urtasun, Muscle: Multi sweep compression
    of lidar using deep entropy models, Advances in Neural Information Processing
    Systems 33 (2020) 22170–22181.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] S. 比斯瓦斯, J. 刘, K. 王, S. 王, R. 乌尔塔孙, Muscle: 使用深度熵模型的激光雷达多扫压缩, 神经信息处理系统进展
    33 (2020) 22170–22181。'
- en: '[81] Y. Zhang, D. Huang, Y. Wang, Pc-rgnn: Point cloud completion and graph
    neural network for 3d object detection, in: Proceedings of the AAAI conference
    on artificial intelligence, Vol. 35, 2021, pp. 3430–3437.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Y. 张, D. 黄, Y. 王, Pc-rgnn: 点云补全与图神经网络用于 3D 物体检测, 见: AAAI 人工智能会议论文集, 第
    35 卷, 2021, 第 3430–3437 页。'
- en: '[82] C. Xu, S. Yang, B. Zhai, B. Wu, X. Yue, W. Zhan, P. Vajda, K. Keutzer,
    M. Tomizuka, Image2point: 3d point-cloud understanding with 2d image pretrained
    models (2021).'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] C. 许, S. 杨, B. 赵, B. 吴, X. 岳, W. 詹, P. 瓦伊达, K. 科伊策, M. 富美久, Image2point:
    基于 2D 图像预训练模型的 3D 点云理解 (2021)。'
- en: '[83] L. Mattheuwsen, M. Vergauwen, Manhole cover detection on rasterized mobile
    mapping point cloud data using transfer learned fully convolutional neural networks,
    Remote Sensing 12 (22) (2020) 3820.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] L. 马修森, M. 维尔戈恩, 使用迁移学习的全卷积神经网络在栅格化移动测绘点云数据上的检查井盖检测, 遥感 12 (22) (2020)
    3820。'
- en: '[84] Y. Liao, M. E. Mohammadi, R. L. Wood, Deep learning classification of
    2d orthomosaic images and 3d point clouds for post-event structural damage assessment,
    Drones 4 (2) (2020) 24.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Y. 廖, M. E. 莫罕默迪, R. L. 伍德, 深度学习分类的 2D 正射影像和 3D 点云用于事件后结构损伤评估, 无人机 4 (2)
    (2020) 24。'
- en: '[85] S. Bourbia, A. Karine, A. Chetouani, M. El Hassouni, Blind projection-based
    3d point cloud quality assessment method using a convolutional neural network.,
    in: VISIGRAPP (4: VISAPP), SCITEPRES, online, 2022, pp. 518–525.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. 布尔比亚, A. 卡琳, A. 切图阿尼, M. 艾尔·哈苏尼, 基于盲投影的 3D 点云质量评估方法使用卷积神经网络, 见: VISIGRAPP
    (4: VISAPP), SCITEPRES, 在线, 2022, 第 518–525 页。'
- en: '[86] R. Leroy, P. Trouvé-Peloux, F. Champagnat, B. Le Saux, M. Carvalho, Pix2point:
    Learning outdoor 3d using sparse point clouds and optimal transport, in: 2021
    17th International Conference on Machine Vision and Applications (MVA), IEEE,
    online, 2021, pp. 1–5.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] R. 勒罗伊, P. 特鲁维-佩卢, F. 尚帕涅, B. 勒索, M. 卡瓦略, Pix2point: 使用稀疏点云和最优传输学习户外 3D,
    见: 2021 第 17 届国际机器视觉与应用会议 (MVA), IEEE, 在线, 2021, 第 1–5 页。'
- en: '[87] J. Balado, R. Sousa, L. Diaz-Vilarino, P. Arias, Transfer learning in
    urban object classification: Online images to recognize point clouds, Automation
    in Construction 111 (2020) 103058.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] J. 巴拉多, R. 苏萨, L. 迪亚斯-维拉里诺, P. 阿里亚斯, 城市物体分类中的迁移学习: 在线图像识别点云, 自动化建造 111
    (2020) 103058。'
- en: '[88] V. Stojanovic, M. Trapp, R. Richter, J. Döllner, Service-oriented semantic
    enrichment of indoor point clouds using octree-based multiview classification,
    Graphical Models 105 (2019) 101039.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] V. 斯托扬诺维奇, M. 特拉普, R. 里希特, J. 多尔纳, 使用八叉树基于多视图分类的室内点云的服务导向语义增强, Graphical
    Models 105 (2019) 101039。'
- en: '[89] W. Dongyu, H. Fuwen, T. Mikolajczyk, H. Yunhua, Object detection for soft
    robotic manipulation based on rgb-d sensors, in: 2018 WRC Symposium on Advanced
    Robotics and Automation (WRC SARA), IEEE, Beijing, China, 2018, pp. 52–58.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] W. 董宇, H. 富文, T. 米科拉伊奇克, H. 云华, 基于 RGB-D 传感器的软体机器人操作物体检测, 见: 2018 WRC
    高级机器人与自动化研讨会 (WRC SARA), IEEE, 北京, 中国, 2018, 第 52–58 页。'
- en: '[90] C. Zhao, H. Guo, J. Lu, D. Yu, D. Li, X. Chen, Als point cloud classification
    with small training data set based on transfer learning, IEEE Geoscience and Remote
    Sensing Letters 17 (8) (2020) 1406–1410. [doi:10.1109/LGRS.2019.2947608](https://doi.org/10.1109/LGRS.2019.2947608).'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] C. Zhao, H. Guo, J. Lu, D. Yu, D. Li, X. Chen, 基于迁移学习的小训练数据集的ALS点云分类，《IEEE地球科学与遥感快报》17（8）（2020）1406–1410。[doi:10.1109/LGRS.2019.2947608](https://doi.org/10.1109/LGRS.2019.2947608)。'
- en: '[91] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    IEEE, Las Vegas, NV, USA, 2016, pp. 770–778.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] K. He, X. Zhang, S. Ren, J. Sun, 深度残差学习用于图像识别，见：IEEE计算机视觉与模式识别会议论文集，IEEE，拉斯维加斯，NV，美国，2016年，pp.
    770–778。'
- en: '[92] M. Liu, L. Sheng, S. Yang, J. Shao, S.-M. Hu, Morphing and sampling network
    for dense point cloud completion, in: Proceedings of the AAAI conference on artificial
    intelligence, Vol. 34, 2020, pp. 11596–11603.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] M. Liu, L. Sheng, S. Yang, J. Shao, S.-M. Hu, 用于密集点云补全的变形和采样网络，见：AAAI人工智能会议论文集，第34卷，2020年，pp.
    11596–11603。'
- en: '[93] D. Zong, S. Sun, J. Zhao, Ashf-net: Adaptive sampling and hierarchical
    folding network for robust point cloud completion, in: Proceedings of the AAAI
    Conference on Artificial Intelligence, Vol. 35, 2021, pp. 3625–3632.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] D. Zong, S. Sun, J. Zhao, Ashf-net: 自适应采样和分层折叠网络用于鲁棒的点云补全，见：AAAI人工智能会议论文集，第35卷，2021年，pp.
    3625–3632。'
- en: '[94] W. Yuan, T. Khot, D. Held, C. Mertz, M. Hebert, Pcn: Point completion
    network, in: 2018 international conference on 3D vision (3DV), IEEE, 2018, pp.
    728–737.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] W. Yuan, T. Khot, D. Held, C. Mertz, M. Hebert, PCN: 点云补全网络，见：2018年国际3D视觉会议（3DV），IEEE，2018年，pp.
    728–737。'
- en: '[95] X. Wen, T. Li, Z. Han, Y.-S. Liu, Point cloud completion by skip-attention
    network with hierarchical folding, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2020, pp. 1939–1948.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] X. Wen, T. Li, Z. Han, Y.-S. Liu, 通过跳跃注意网络与分层折叠进行点云补全，见：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，pp.
    1939–1948。'
- en: '[96] Y. Nie, Y. Lin, X. Han, S. Guo, J. Chang, S. Cui, J. Zhang, et al., Skeleton-bridged
    point completion: From global inference to local adjustment, Advances in Neural
    Information Processing Systems 33 (2020) 16119–16130.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Y. Nie, Y. Lin, X. Han, S. Guo, J. Chang, S. Cui, J. Zhang, 等，Skeleton-bridged点云补全：从全局推断到局部调整，《神经信息处理系统进展》33（2020）16119–16130。'
- en: '[97] Y. Li, L. Ma, W. Tan, C. Sun, D. Cao, J. Li, Grnet: Geometric relation
    network for 3d object detection from point clouds, ISPRS Journal of Photogrammetry
    and Remote Sensing 165 (2020) 43–53.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Y. Li, L. Ma, W. Tan, C. Sun, D. Cao, J. Li, Grnet: 用于从点云中检测3D物体的几何关系网络，《ISPRS摄影测量与遥感学杂志》165（2020）43–53。'
- en: '[98] X. Wang, M. H. Ang, G. H. Lee, Voxel-based network for shape completion
    by leveraging edge generation, in: Proceedings of the IEEE/CVF international conference
    on computer vision, 2021, pp. 13189–13198.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] X. Wang, M. H. Ang, G. H. Lee, 基于体素的形状补全网络，通过边缘生成，见：IEEE/CVF国际计算机视觉会议论文集，2021年，pp.
    13189–13198。'
- en: '[99] Y. Tang, R. Zhang, Z. Guo, X. Ma, B. Zhao, Z. Wang, D. Wang, X. Li, Point-peft:
    Parameter-efficient fine-tuning for 3d pre-trained models, in: Proceedings of
    the AAAI Conference on Artificial Intelligence, Vol. 38, 2024, pp. 5171–5179.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Y. Tang, R. Zhang, Z. Guo, X. Ma, B. Zhao, Z. Wang, D. Wang, X. Li, Point-peft:
    用于3D预训练模型的参数高效微调，见：AAAI人工智能会议论文集，第38卷，2024年，pp. 5171–5179。'
- en: '[100] X. Zhou, D. Liang, W. Xu, X. Zhu, Y. Xu, Z. Zou, X. Bai, Dynamic adapter
    meets prompt tuning: Parameter-efficient transfer learning for point cloud analysis,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2024, pp. 14707–14717.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] X. Zhou, D. Liang, W. Xu, X. Zhu, Y. Xu, Z. Zou, X. Bai, 动态适配器与提示调优相遇：用于点云分析的参数高效迁移学习，见：IEEE/CVF计算机视觉与模式识别会议论文集，2024年，pp.
    14707–14717。'
- en: '[101] G. Song, H. Xu, J. Liu, T. Zhi, Y. Shi, J. Zhang, Z. Jiang, J. Feng,
    S. Sang, L. Luo, Agilegan3d: Few-shot 3d portrait stylization by augmented transfer
    learning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2024, pp. 765–774.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] G. Song, H. Xu, J. Liu, T. Zhi, Y. Shi, J. Zhang, Z. Jiang, J. Feng,
    S. Sang, L. Luo, Agilegan3d: 通过增强迁移学习进行少样本3D肖像风格化，见：IEEE/CVF计算机视觉与模式识别会议论文集，2024年，pp.
    765–774。'
- en: '[102] M. A. Shoukat, A. B. Sargano, L. You, Z. Habib, 3d estimation of single-view
    2d images using shape priors and transfer learning, Multimedia Tools and Applications
    (2024) 1–13.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] M. A. Shoukat, A. B. Sargano, L. You, Z. Habib, 使用形状先验和迁移学习对单视角2D图像进行3D估计，《多媒体工具与应用》（2024）1–13。'
- en: '[103] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov,
    A. J. Smola, Deep sets, Advances in neural information processing systems 30 (2017).'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov,
    A. J. Smola, 深度集合，神经信息处理系统进展 30 (2017)。'
- en: '[104] R. Klokov, V. Lempitsky, Escape from cells: Deep kd-networks for the
    recognition of 3d point cloud models, in: Proceedings of the IEEE international
    conference on computer vision, Venice, Italy, 2017, pp. 863–872.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] R. Klokov, V. Lempitsky, 摆脱细胞：用于3D点云模型识别的深度kd网络，见：IEEE国际计算机视觉会议论文集，威尼斯，意大利，2017年，页码863–872。'
- en: '[105] W. Zeng, T. Gevers, 3dcontextnet: Kd tree guided hierarchical learning
    of point clouds using local and global contextual cues, in: Proceedings of the
    European Conference on Computer Vision (ECCV) Workshops, Springer, Munich, Germany,,
    2018, pp. 0–0.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] W. Zeng, T. Gevers, 3dcontextnet: Kd树引导的点云的局部和全局上下文提示的层次学习，见：欧洲计算机视觉会议（ECCV）研讨会论文集，Springer，慕尼黑，德国，2018年，页码0–0。'
- en: '[106] B.-S. Hua, M.-K. Tran, S.-K. Yeung, Pointwise convolutional neural networks,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    IEEE, Salt Lake City, UT, USA, 2018, pp. 984–993.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] B.-S. Hua, M.-K. Tran, S.-K. Yeung, 点对点卷积神经网络，见：IEEE计算机视觉与模式识别会议论文集，IEEE，盐湖城，UT，美国，2018年，页码984–993。'
- en: '[107] Z. Zhang, B.-S. Hua, S.-K. Yeung, Shellnet: Efficient point cloud convolutional
    neural networks using concentric shells statistics, in: Proceedings of the IEEE/CVF
    international conference on computer vision, IEEE, Seoul, Korea (South), 2019,
    pp. 1607–1616.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Z. Zhang, B.-S. Hua, S.-K. Yeung, Shellnet: 使用同心壳统计的高效点云卷积神经网络，见：IEEE/CVF国际计算机视觉会议论文集，IEEE，首尔，韩国，2019年，页码1607–1616。'
- en: '[108] G. Te, W. Hu, A. Zheng, Z. Guo, Rgcnn: Regularized graph cnn for point
    cloud segmentation, in: Proceedings of the 26th ACM international conference on
    Multimedia, ACM, Seoul Republic of Korea, 2018, pp. 746–754.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] G. Te, W. Hu, A. Zheng, Z. Guo, rgcnn: 正则化图卷积神经网络用于点云分割，见：第26届ACM国际多媒体会议论文集，ACM，首尔，韩国，2018年，页码746–754。'
- en: '[109] C. Wang, B. Samari, K. Siddiqi, Local spectral graph convolution for
    point set feature learning, in: Proceedings of the European conference on computer
    vision (ECCV), Springer, Munich, Germany, 2018, pp. 52–66.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C. Wang, B. Samari, K. Siddiqi, 点集特征学习的局部谱图卷积，见：欧洲计算机视觉会议（ECCV）论文集，Springer，慕尼黑，德国，2018年，页码52–66。'
- en: '[110] S. Xie, S. Liu, Z. Chen, Z. Tu, Attentional shapecontextnet for point
    cloud recognition, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, IEEE, Salt Lake City, UT, USA, 2018, pp. 4606–4615.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] S. Xie, S. Liu, Z. Chen, Z. Tu, 注意力形状上下文网络用于点云识别，见：IEEE计算机视觉与模式识别会议论文集，IEEE，盐湖城，UT，美国，2018年，页码4606–4615。'
- en: '[111] N. Verma, E. Boyer, J. Verbeek, Feastnet: Feature-steered graph convolutions
    for 3d shape analysis, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, IEEE, Salt Lake City, UT, USA, 2018, pp. 2598–2606.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] N. Verma, E. Boyer, J. Verbeek, Feastnet: 用于3D形状分析的特征引导图卷积，见：IEEE计算机视觉与模式识别会议论文集，IEEE，盐湖城，UT，美国，2018年，页码2598–2606。'
- en: '[112] A. Boulch, Convpoint: Continuous convolutions for point cloud processing,
    Computers & Graphics 88 (2020) 24–34.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] A. Boulch, Convpoint: 连续卷积用于点云处理，Computers & Graphics 88 (2020) 24–34。'
- en: '[113] Z. Yang, O. Litany, T. Birdal, S. Sridhar, L. Guibas, Continuous geodesic
    convolutions for learning on 3d shapes, in: Proceedings of the IEEE/CVF winter
    conference on applications of computer vision, IEEE, online, 2021, pp. 134–144.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Z. Yang, O. Litany, T. Birdal, S. Sridhar, L. Guibas, 连续测地卷积用于3D形状学习，见：IEEE/CVF冬季计算机视觉应用会议论文集，IEEE，线上，2021年，页码134–144。'
- en: '[114] J. Sauder, B. Sievers, Self-supervised deep learning on point clouds
    by reconstructing space, Advances in Neural Information Processing Systems 32
    (2019).'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] J. Sauder, B. Sievers, 通过重建空间的自监督深度学习点云，神经信息处理系统进展 32 (2019)。'
- en: '[115] O. Poursaeed, T. Jiang, H. Qiao, N. Xu, V. G. Kim, Self-supervised learning
    of point clouds via orientation estimation, in: 2020 International Conference
    on 3D Vision (3DV), IEEE, 2020, pp. 1018–1028.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] O. Poursaeed, T. Jiang, H. Qiao, N. Xu, V. G. Kim, 通过方向估计自监督学习点云，见：2020年国际3D视觉会议（3DV），IEEE，2020年，页码1018–1028。'
- en: '[116] I. Achituve, H. Maron, G. Chechik, Self-supervised learning for domain
    adaptation on point clouds, in: Proceedings of the IEEE/CVF winter conference
    on applications of computer vision, IEEE, online, 2021, pp. 123–133.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] I. Achituve, H. Maron, G. Chechik, 点云领域适应的自监督学习，见：IEEE/CVF冬季计算机视觉应用会议论文集，IEEE，线上，2021年，页码123–133。'
- en: '[117] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, O. Litany, Pointcontrast:
    Unsupervised pre-training for 3d point cloud understanding, in: Computer Vision–ECCV
    2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part III 16, Springer, online, 2020, pp. 574–591.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, Pointcontrast: 用于3D点云理解的无监督预训练，见：计算机视觉–ECCV
    2020: 第16届欧洲会议，格拉斯哥，英国，2020年8月23–28日，论文集，第III部分16，Springer，线上，2020年，页码574–591。'
- en: '[118] H. Wang, Q. Liu, X. Yue, J. Lasenby, M. J. Kusner, Unsupervised point
    cloud pre-training via occlusion completion, in: Proceedings of the IEEE/CVF international
    conference on computer vision, Montreal, BC, Canada, 2021, pp. 9782–9792.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] H. Wang, Q. Liu, X. Yue, J. Lasenby, M. J. Kusner, 无监督点云预训练通过遮挡完成，见：IEEE/CVF国际计算机视觉会议论文集，蒙特利尔，加拿大，2021年，页码9782–9792。'
- en: '[119] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, J. Lu, Point-bert: Pre-training
    3d point cloud transformers with masked point modeling, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE, New Orleans,
    LA, USA, 2022, pp. 19313–19322.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, J. Lu, Point-bert: 用掩码点建模进行3D点云变换器的预训练，见：IEEE/CVF计算机视觉与模式识别会议论文集，IEEE，新奥尔良，路易斯安那州，美国，2022年，页码19313–19322。'
- en: '[120] R. Zhang, Z. Guo, P. Gao, R. Fang, B. Zhao, D. Wang, Y. Qiao, H. Li,
    Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training,
    arXiv preprint arXiv:2205.14401 (2022).'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] R. Zhang, Z. Guo, P. Gao, R. Fang, B. Zhao, D. Wang, Y. Qiao, H. Li,
    Point-m2ae: 用于分层点云预训练的多尺度掩码自编码器，arXiv预印本 arXiv:2205.14401 (2022)。'
- en: '[121] C. Choy, J. Gwak, S. Savarese, 4d spatio-temporal convnets: Minkowski
    convolutional neural networks, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, IEEE, Long Beach, CA, USA, 2019, pp. 3075–3084.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] C. Choy, J. Gwak, S. Savarese, 4D时空卷积网络：Minkowski卷积神经网络，见：IEEE/CVF计算机视觉与模式识别会议论文集，IEEE，长滩，加利福尼亚州，美国，2019年，页码3075–3084。'
- en: '[122] B. Graham, M. Engelcke, L. Van Der Maaten, 3d semantic segmentation with
    submanifold sparse convolutional networks, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, IEEE, Salt Lake City, UT, USA, 2018,
    pp. 9224–9232.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] B. Graham, M. Engelcke, L. Van Der Maaten, 使用子流形稀疏卷积网络的3D语义分割，见：IEEE计算机视觉与模式识别会议论文集，IEEE，盐湖城，犹他州，美国，2018年，页码9224–9232。'
- en: '[123] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for
    biomedical image segmentation, in: Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18, Springer, 2015, pp. 234–241.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] O. Ronneberger, P. Fischer, T. Brox, U-net: 用于生物医学图像分割的卷积网络，见：医学图像计算与计算机辅助干预–MICCAI
    2015: 第18届国际会议，慕尼黑，德国，2015年10月5-9日，论文集，第III部分18，Springer，2015年，页码234–241。'
- en: '[124] H. Kheddar, M. Hemis, Y. Himeur, Automatic speech recognition using advanced
    deep learning approaches: A survey, Information Fusion (2024) 102422.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] H. Kheddar, M. Hemis, Y. Himeur, 使用先进深度学习方法的自动语音识别：一项调查，信息融合 (2024) 102422。'
- en: '[125] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao,
    H. Li, Pointclip: Point cloud understanding by clip, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, IEEE, New Orleans, Louisiana,
    USA, 2022, pp. 8552–8562.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao,
    H. Li, Pointclip: 通过clip进行点云理解，见：IEEE/CVF计算机视觉与模式识别会议论文集，IEEE，新奥尔良，路易斯安那州，美国，2022年，页码8552–8562。'
- en: '[126] X. Huang, T. Xie, Z. Wang, L. Chen, Q. Zhou, Z. Hu, A transfer learning-based
    multi-fidelity point-cloud neural network approach for melt pool modeling in additive
    manufacturing, ASCE-ASME J Risk and Uncert in Engrg Sys Part B Mech Engrg 8 (1)
    (2022).'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] X. Huang, T. Xie, Z. Wang, L. Chen, Q. Zhou, Z. Hu, 基于迁移学习的多保真度点云神经网络方法用于增材制造中的熔池建模，ASCE-ASME风险与不确定性工程系统B部分机械工程
    8 (1) (2022)。'
- en: '[127] W. Zhao, H. Zhang, Y. Yan, Y. Fu, H. Wang, A semantic segmentation algorithm
    using fcn with combination of bslic, Applied Sciences 8 (4) (2018) 500.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] W. Zhao, H. Zhang, Y. Yan, Y. Fu, H. Wang, 使用FCN和BSLIC组合的语义分割算法，应用科学
    8 (4) (2018) 500。'
- en: '[128] M. Zhang, P. Kadam, S. Liu, C.-C. J. Kuo, Unsupervised feedforward feature
    (uff) learning for point cloud classification and segmentation, in: 2020 IEEE
    International Conference on Visual Communications and Image Processing (VCIP),
    IEEE, online, 2020, pp. 144–147.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] M. Zhang, P. Kadam, S. Liu, C.-C. J. Kuo, 用于点云分类和分割的无监督前馈特征（uff）学习，见：2020
    IEEE国际视觉通信与图像处理会议（VCIP），IEEE，线上，2020年，页码144–147。'
- en: '[129] A. Murtiyoso, C. Lhenry, T. Landes, P. Grussenmeyer, E. Alby, Semantic
    segmentation for building façade 3d point cloud from 2d orthophoto images using
    transfer learning, ISPRS-International Archives of the Photogrammetry, Remote
    Sensing and Spatial Information Sciences 43 (2021) 201–206.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] A. Murtiyoso, C. Lhenry, T. Landes, P. Grussenmeyer, E. Alby，基于迁移学习的建筑立面
    3D 点云的语义分割，收录于《ISPRS-国际摄影测量、遥感与空间信息科学档案》43 (2021) 201–206。'
- en: '[130] M. Imad, O. Doukhi, D.-J. Lee, Transfer learning based semantic segmentation
    for 3d object detection from point cloud, Sensors 21 (12) (2021) 3964.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] M. Imad, O. Doukhi, D.-J. Lee，基于迁移学习的 3D 对象检测的语义分割，《传感器》21 (12) (2021)
    3964。'
- en: '[131] C. Zong, H. Wang, et al., An improved 3d point cloud instance segmentation
    method for overhead catenary height detection, Computers & Electrical Engineering
    98 (2022) 107685.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] C. Zong, H. Wang 等，改进的 3D 点云实例分割方法用于高架链条高度检测，《计算机与电气工程》98 (2022) 107685。'
- en: '[132] D. Bazazian, D. Nahata, Dcg-net: Dynamic capsule graph convolutional
    network for point clouds, IEEE Access 8 (2020) 188056–188067.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] D. Bazazian, D. Nahata，DCG-Net：用于点云的动态胶囊图卷积网络，《IEEE Access》8 (2020) 188056–188067。'
- en: '[133] N. Arnold, P. Angelov, T. Viney, P. Atkinson, Automatic extraction and
    labelling of memorial objects from 3d point clouds, Journal of Computer Applications
    in Archaeology 4 (1) (2021).'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] N. Arnold, P. Angelov, T. Viney, P. Atkinson，自动从 3D 点云中提取和标记纪念物，《计算机应用于考古学杂志》4
    (1) (2021)。'
- en: '[134] D. Urbach, Y. Ben-Shabat, M. Lindenbaum, Dpdist: Comparing point clouds
    using deep point cloud distance, in: Computer Vision–ECCV 2020: 16th European
    Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16, Springer,
    2020, pp. 545–560.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] D. Urbach, Y. Ben-Shabat, M. Lindenbaum，Dpdist：使用深度点云距离比较点云，收录于《计算机视觉–ECCV
    2020：第十六届欧洲会议，英国格拉斯哥，2020年8月23–28日，会议录，部分 XI 16》，Springer，2020年，页 545–560。'
- en: '[135] D. P. Huttenlocher, G. A. Klanderman, W. J. Rucklidge, Comparing images
    using the hausdorff distance, IEEE Transactions on pattern analysis and machine
    intelligence 15 (9) (1993) 850–863.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] D. P. Huttenlocher, G. A. Klanderman, W. J. Rucklidge，使用 Hausdorff 距离比较图像，《IEEE
    模式分析与机器智能汇刊》15 (9) (1993) 850–863。'
- en: '[136] T. Nguyen, Q.-H. Pham, T. Le, T. Pham, N. Ho, B.-S. Hua, Point-set distances
    for learning representations of 3d point clouds, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, Montreal, BC, Canada, 2021, pp. 10478–10487.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] T. Nguyen, Q.-H. Pham, T. Le, T. Pham, N. Ho, B.-S. Hua，点集距离用于学习 3D 点云的表示，收录于《IEEE/CVF
    国际计算机视觉会议论文集》，加拿大蒙特利尔，2021年，页 10478–10487。'
- en: '[137] Z. Xie, N. Somani, Y. J. S. Tan, J. C. Y. Seng, Automatic toolpath pattern
    recommendation for various industrial applications based on deep learning, in:
    2021 IEEE/SICE International Symposium on System Integration (SII), IEEE, 2021,
    pp. 60–65.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Z. Xie, N. Somani, Y. J. S. Tan, J. C. Y. Seng，基于深度学习的各种工业应用的自动工具路径模式推荐，收录于《2021
    IEEE/SICE 国际系统集成研讨会 (SII)》，IEEE，2021年，页 60–65。'
- en: '[138] X. Lei, H. Wang, C. Wang, Z. Zhao, J. Miao, P. Tian, Als point cloud
    classification by integrating an improved fully convolutional network into transfer
    learning with multi-scale and multi-view deep features, Sensors 20 (23) (2020)
    6969.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] X. Lei, H. Wang, C. Wang, Z. Zhao, J. Miao, P. Tian，通过将改进的全卷积网络与多尺度和多视图深度特征的迁移学习结合进行
    ALS 点云分类，《传感器》20 (23) (2020) 6969。'
- en: '[139] X. Li, C. Li, Z. Tong, A. Lim, J. Yuan, Y. Wu, J. Tang, R. Huang, Campus3d:
    A photogrammetry point cloud benchmark for hierarchical understanding of outdoor
    scene, in: Proceedings of the 28th ACM International Conference on Multimedia,
    2020, pp. 238–246.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] X. Li, C. Li, Z. Tong, A. Lim, J. Yuan, Y. Wu, J. Tang, R. Huang，Campus3D：用于户外场景层次理解的摄影测量点云基准，收录于《第28届
    ACM 国际多媒体会议论文集》，2020年，页 238–246。'
- en: '[140] J. Kim, D. Chung, Y. Kim, H. Kim, Deep learning-based 3d reconstruction
    of scaffolds using a robot dog, Automation in Construction 134 (2022) 104092.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] J. Kim, D. Chung, Y. Kim, H. Kim，基于深度学习的使用机器人狗进行的 3D 脚手架重建，《施工自动化》134
    (2022) 104092。'
- en: '[141] P.-Y. Chen, Z. Y. Wu, E. Taciroglu, Classification of soft-story buildings
    using deep learning with density features extracted from 3d point clouds, Journal
    of Computing in Civil Engineering 35 (3) (2021) 04021005.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] P.-Y. Chen, Z. Y. Wu, E. Taciroglu，利用从 3D 点云提取的密度特征进行深度学习对软层建筑的分类，《土木工程计算杂志》35
    (3) (2021) 04021005。'
- en: '[142] K. Sidor, M. Wysocki, Recognition of human activities using depth maps
    and the viewpoint feature histogram descriptor, Sensors 20 (10) (2020) 2940.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] K. Sidor, M. Wysocki，使用深度图和视点特征直方图描述符进行人体活动识别，《传感器》20 (10) (2020) 2940。'
- en: '[143] J. Tian, J. Zhang, W. Li, D. Xu, Vdm-da: Virtual domain modeling for
    source data-free domain adaptation, IEEE Transactions on Circuits and Systems
    for Video Technology (2021).'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] J. Tian, J. Zhang, W. Li, D. Xu, Vdm-da：用于源数据无关领域适应的虚拟领域建模，《IEEE 视频技术电路与系统汇刊》
    (2021)。'
- en: '[144] K. Huang, X. Qu, S. Chen, Z. Chen, W. Zhang, H. Qi, F. Zhao, Superb monocular
    depth estimation based on transfer learning and surface normal guidance, Sensors
    20 (17) (2020) 4856.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] K. Huang, X. Qu, S. Chen, Z. Chen, W. Zhang, H. Qi, F. Zhao, 基于迁移学习和表面法向引导的卓越单目深度估计，《传感器》20
    (17) (2020) 4856。'
- en: '[145] H. Benhabiles, K. Hammoudi, F. Windal, M. Melkemi, A. Cabani, A transfer
    learning exploited for indexing protein structures from 3d point clouds, in: Sipaim–Miccai
    Biomedical Workshop, Springer, 2018, pp. 82–89.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] H. Benhabiles, K. Hammoudi, F. Windal, M. Melkemi, A. Cabani, 用于从 3D
    点云中索引蛋白质结构的迁移学习，见：Sipaim–Miccai 生物医学研讨会，Springer，2018年，pp. 82–89。'
- en: '[146] K. B. Ozyoruk, G. I. Gokceler, T. L. Bobrow, G. Coskun, K. Incetan, Y. Almalioglu,
    F. Mahmood, E. Curto, L. Perdigoto, M. Oliveira, et al., Endoslam dataset and
    an unsupervised monocular visual odometry and depth estimation approach for endoscopic
    videos, Medical image analysis 71 (2021) 102058.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] K. B. Ozyoruk, G. I. Gokceler, T. L. Bobrow, G. Coskun, K. Incetan, Y.
    Almalioglu, F. Mahmood, E. Curto, L. Perdigoto, M. Oliveira 等，内窥镜视频的 Endoslam
    数据集及无监督单目视觉里程计和深度估计方法，《医学影像分析》71 (2021) 102058。'
- en: '[147] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,
    S. Savarese, M. Savva, S. Song, H. Su, et al., Shapenet: An information-rich 3d
    model repository, arXiv preprint arXiv:1512.03012 (2015).'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,
    S. Savarese, M. Savva, S. Song, H. Su 等，Shapenet：一个信息丰富的 3D 模型库，arXiv 预印本 arXiv:1512.03012
    (2015)。'
- en: '[148] B. Eckart, W. Yuan, C. Liu, J. Kautz, Self-supervised learning on 3d
    point clouds by learning discrete generative models, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, IEEE, Nashville, TN, USA,
    2021, pp. 8248–8257.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] B. Eckart, W. Yuan, C. Liu, J. Kautz, 通过学习离散生成模型进行 3D 点云的自监督学习，见：IEEE/CVF
    计算机视觉与模式识别会议论文集，IEEE，纳什维尔，TN，美国，2021年，pp. 8248–8257。'
- en: '[149] S. Lee, Y. Yang, Progressive deep learning framework for recognizing
    3d orientations and object class based on point cloud representation, Sensors
    21 (18) (2021) 6108.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] S. Lee, Y. Yang, 用于识别基于点云表示的 3D 定向和物体类别的渐进深度学习框架，《传感器》21 (18) (2021)
    6108。'
- en: '[150] B. Dai, C. Le Gentil, T. Vidal-Calleja, Connecting the dots for real-time
    lidar-based object detection with yolo, in: Australasian Conference on Robotics
    and Automation, ACRA, ARAA, Lincoln, Canterbury, New Zealand, 2018.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] B. Dai, C. Le Gentil, T. Vidal-Calleja, 通过 YOLO 连接点以实现实时激光雷达对象检测，见：澳大利亚机器人与自动化会议，ACRA，ARAA，林肯，坎特伯雷，新西兰，2018年。'
- en: '[151] G. Diraco, P. Siciliano, A. Leone, Remaining useful life prediction from
    3d scan data with genetically optimized convolutional neural networks, Sensors
    21 (20) (2021) 6772.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] G. Diraco, P. Siciliano, A. Leone, 基于基因优化卷积神经网络的 3D 扫描数据剩余使用寿命预测，《传感器》21
    (20) (2021) 6772。'
- en: '[152] J. Kang, M. Körner, Y. Wang, H. Taubenböck, X. X. Zhu, Building instance
    classification using street view images, ISPRS journal of photogrammetry and remote
    sensing 145 (2018) 44–59.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] J. Kang, M. Körner, Y. Wang, H. Taubenböck, X. X. Zhu, 利用街景图像进行建筑实例分类，《国际摄影测量与遥感学杂志》145
    (2018) 44–59。'
- en: '[153] C. Wu, X. Bi, J. Pfrommer, A. Cebulla, S. Mangold, J. Beyerer, Sim2real
    transfer learning for point cloud segmentation: An industrial application case
    on autonomous disassembly, in: Proceedings of the IEEE/CVF Winter Conference on
    Applications of Computer Vision, IEEE, Waikoloa, HI, USA, 2023, pp. 4531–4540.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] C. Wu, X. Bi, J. Pfrommer, A. Cebulla, S. Mangold, J. Beyerer, 点云分割的
    Sim2real 转移学习：自主拆解的工业应用案例，见：IEEE/CVF 计算机视觉应用冬季会议论文集，IEEE，怀科洛阿，HI，美国，2023年，pp.
    4531–4540。'
- en: '[154] Y. Zhou, A. Ji, L. Zhang, X. Xue, Sampling-attention deep learning network
    with transfer learning for large-scale urban point cloud semantic segmentation,
    Engineering Applications of Artificial Intelligence 117 (2023) 105554.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Y. Zhou, A. Ji, L. Zhang, X. Xue, 具有迁移学习的大规模城市点云语义分割的采样注意力深度学习网络，《人工智能工程应用》117
    (2023) 105554。'
- en: '[155] B. Wu, X. Zhou, S. Zhao, X. Yue, K. Keutzer, Squeezesegv2: Improved model
    structure and unsupervised domain adaptation for road-object segmentation from
    a lidar point cloud, in: 2019 International Conference on Robotics and Automation
    (ICRA), IEEE, Montreal, Canada, 2019, pp. 4376–4382.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] B. Wu, X. Zhou, S. Zhao, X. Yue, K. Keutzer, Squeezesegv2：改进的模型结构和无监督领域适应，用于从激光雷达点云中进行道路物体分割，见：2019
    国际机器人与自动化会议（ICRA），IEEE，蒙特利尔，加拿大，2019年，pp. 4376–4382。'
- en: '[156] P. Jiang, S. Saripalli, Lidarnet: A boundary-aware domain adaptation
    model for point cloud semantic segmentation, in: 2021 IEEE International Conference
    on Robotics and Automation (ICRA), IEEE, Xi’an, China, 2021, pp. 2457–2464.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] P. Jiang, S. Saripalli, Lidarnet: 一种边界感知的领域适应模型用于点云语义分割, 见于: 2021 IEEE
    国际机器人与自动化大会 (ICRA), IEEE, 西安, 中国, 2021年, 第2457–2464页。'
- en: '[157] L. Yi, B. Gong, T. Funkhouser, Complete & label: A domain adaptation
    approach to semantic segmentation of lidar point clouds, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE, Nashville,
    TN, USA, 2021, pp. 15363–15373.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] L. Yi, B. Gong, T. Funkhouser, Complete & label: 一种用于激光雷达点云语义分割的领域适应方法,
    见于: IEEE/CVF 计算机视觉与模式识别会议论文集, IEEE, 纳什维尔, 田纳西州, 美国, 2021年, 第15363–15373页。'
- en: '[158] C. Qin, H. You, L. Wang, C.-C. J. Kuo, Y. Fu, Pointdan: A multi-scale
    3d domain adaption network for point cloud representation, Advances in Neural
    Information Processing Systems 32 (2019).'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] C. Qin, H. You, L. Wang, C.-C. J. Kuo, Y. Fu, Pointdan: 一种多尺度的3D领域适应网络用于点云表示,
    神经信息处理系统进展 32 (2019)。'
- en: '[159] S. Zhao, Y. Wang, B. Li, B. Wu, Y. Gao, P. Xu, T. Darrell, K. Keutzer,
    epointda: An end-to-end simulation-to-real domain adaptation framework for lidar
    point cloud segmentation, in: Proceedings of the AAAI Conference on Artificial
    Intelligence, Vol. 35, online, 2021, pp. 3500–3509.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] S. Zhao, Y. Wang, B. Li, B. Wu, Y. Gao, P. Xu, T. Darrell, K. Keutzer,
    epointda: 一种端到端的从模拟到真实领域适应框架用于激光雷达点云分割, 见于: AAAI 人工智能会议论文集, 第35卷, 在线, 2021年, 第3500–3509页。'
- en: '[160] Q. Xu, Y. Zhou, W. Wang, C. R. Qi, D. Anguelov, Spg: Unsupervised domain
    adaptation for 3d object detection via semantic point generation, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada,
    2021, pp. 15446–15456.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Q. Xu, Y. Zhou, W. Wang, C. R. Qi, D. Anguelov, Spg: 通过语义点生成进行3D目标检测的无监督领域适应,
    见于: IEEE/CVF 国际计算机视觉大会论文集, 蒙特利尔, 不列颠哥伦比亚省, 加拿大, 2021年, 第15446–15456页。'
- en: '[161] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, O. Beijbom, Pointpillars:
    Fast encoders for object detection from point clouds, in: Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, IEEE, Long Beach, CA, USA,
    2019, pp. 12697–12705.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, O. Beijbom, Pointpillars:
    用于点云目标检测的快速编码器, 见于: IEEE/CVF 计算机视觉与模式识别会议论文集, IEEE, 洛杉矶, 加州, 美国, 2019年, 第12697–12705页。'
- en: '[162] Y. Wang, J. Yin, W. Li, P. Frossard, R. Yang, J. Shen, Ssda3d: Semi-supervised
    domain adaptation for 3d object detection from point cloud, arXiv preprint arXiv:2212.02845
    (2022).'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Y. Wang, J. Yin, W. Li, P. Frossard, R. Yang, J. Shen, Ssda3d: 半监督领域适应用于点云中的3D目标检测,
    arXiv 预印本 arXiv:2212.02845 (2022)。'
- en: '[163] L. Du, X. Ye, X. Tan, J. Feng, Z. Xu, E. Ding, S. Wen, Associate-3ddet:
    Perceptual-to-conceptual association for 3d point cloud object detection, in:
    Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    Seattle, WA, USA, 2020, pp. 13329–13338.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] L. Du, X. Ye, X. Tan, J. Feng, Z. Xu, E. Ding, S. Wen, Associate-3ddet:
    从感知到概念的3D点云目标检测关联, 见于: IEEE/CVF 计算机视觉与模式识别会议论文集, 西雅图, 华盛顿州, 美国, 2020年, 第13329–13338页。'
- en: '[164] Y. Guo, Y. Li, L. Wang, T. Rosing, Adafilter: Adaptive filter fine-tuning
    for deep transfer learning, in: Proceedings of the AAAI Conference on Artificial
    Intelligence, Vol. 34, AAAI, New York Midtown, New York, USA, 2020, pp. 4060–4066.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Y. Guo, Y. Li, L. Wang, T. Rosing, Adafilter: 深度迁移学习的自适应滤波微调, 见于: AAAI
    人工智能会议论文集, 第34卷, AAAI, 纽约中城, 美国, 2020年, 第4060–4066页。'
- en: '[165] A. Kumar, P. Sattigeri, K. Wadhawan, L. Karlinsky, R. Feris, B. Freeman,
    G. Wornell, Co-regularized alignment for unsupervised domain adaptation, Advances
    in Neural Information Processing Systems 31 (2018).'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] A. Kumar, P. Sattigeri, K. Wadhawan, L. Karlinsky, R. Feris, B. Freeman,
    G. Wornell, 共正则化对齐用于无监督领域适应, 神经信息处理系统进展 31 (2018)。'
- en: '[166] Z. Li, X. Li, Y. Wei, L. Bing, Y. Zhang, Q. Yang, Transferable end-to-end
    aspect-based sentiment analysis with selective adversarial learning, arXiv preprint
    arXiv:1910.14192 (2019).'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Z. Li, X. Li, Y. Wei, L. Bing, Y. Zhang, Q. Yang, 可转移的端到端基于方面的情感分析与选择性对抗学习,
    arXiv 预印本 arXiv:1910.14192 (2019)。'
- en: '[167] W. Ge, Y. Yu, Borrowing treasures from the wealthy: Deep transfer learning
    through selective joint fine-tuning, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, IEEE, Honolulu, HI, USA, 2017, pp. 1086–1095.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] W. Ge, Y. Yu, 从富裕者那里借来宝藏: 通过选择性联合微调的深度迁移学习, 见于: IEEE 计算机视觉与模式识别会议论文集,
    IEEE, 檀香山, 夏威夷, 美国, 2017年, 第1086–1095页。'
- en: '[168] M. Maqsood, F. Nazir, U. Khan, F. Aadil, H. Jamal, I. Mehmood, O.-y.
    Song, Transfer learning assisted classification and detection of alzheimer’s disease
    stages using 3d mri scans, Sensors 19 (11) (2019) 2645.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] M. Maqsood, F. Nazir, U. Khan, F. Aadil, H. Jamal, I. Mehmood, O.-y.
    Song, 基于3D MRI扫描的阿尔茨海默病阶段分类和检测的迁移学习辅助，《传感器》19（11）（2019年）2645。'
- en: '[169] U. Côté-Allard, C. L. Fall, A. Drouin, A. Campeau-Lecours, C. Gosselin,
    K. Glette, F. Laviolette, B. Gosselin, Deep learning for electromyographic hand
    gesture signal classification using transfer learning, IEEE transactions on neural
    systems and rehabilitation engineering 27 (4) (2019) 760–771.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] U. Côté-Allard, C. L. Fall, A. Drouin, A. Campeau-Lecours, C. Gosselin,
    K. Glette, F. Laviolette, B. Gosselin, 使用迁移学习的深度学习进行肌电手势信号分类，《IEEE神经系统与康复工程学报》27（4）（2019年）760–771。'
- en: '[170] C.-X. Ren, D.-Q. Dai, K.-K. Huang, Z.-R. Lai, Transfer learning of structured
    representation for face recognition, IEEE Transactions on image processing 23 (12)
    (2014) 5440–5454.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] C.-X. Ren, D.-Q. Dai, K.-K. Huang, Z.-R. Lai, 用于人脸识别的结构化表示迁移学习，《IEEE图像处理学报》23（12）（2014年）5440–5454。'
- en: '[171] F. Matrone, M. Martini, Transfer learning and performance enhancement
    techniques for deep semantic segmentation of built heritage point clouds, Virtual
    Archaeology Review 12 (25) (2021) 73–84.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] F. Matrone, M. Martini, 深度语义分割的迁移学习与性能增强技术用于历史遗产点云，《虚拟考古学评论》12（25）（2021年）73–84。'
- en: '[172] L. Xuhong, Y. Grandvalet, F. Davoine, Explicit inductive bias for transfer
    learning with convolutional networks, in: International Conference on Machine
    Learning, PMLR, PMLR, tockholm, Sweden, 2018, pp. 2825–2834.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] L. Xuhong, Y. Grandvalet, F. Davoine, 卷积网络的显式归纳偏置用于迁移学习，收录于：国际机器学习会议，PMLR，斯德哥尔摩，瑞典，2018年，第2825–2834页。'
- en: '[173] H. Xiu, T. Shinohara, M. Matsuoka, M. Inoguchi, K. Kawabe, K. Horie,
    Collapsed building detection using 3d point clouds and deep learning, Remote Sensing
    12 (24) (2020) 4057.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] H. Xiu, T. Shinohara, M. Matsuoka, M. Inoguchi, K. Kawabe, K. Horie,
    使用3D点云和深度学习检测倒塌建筑，《遥感》12（24）（2020年）4057。'
- en: '[174] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, J. M. Solomon,
    Dynamic graph cnn for learning on point clouds, Acm Transactions On Graphics (tog)
    38 (5) (2019) 1–12.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, J. M. Solomon,
    用于点云学习的动态图卷积神经网络，《ACM计算机图形学与互动技术》38（5）（2019年）1–12。'
- en: '[175] K. Jeon, G. Lee, S. Yang, H. D. Jeong, Named entity recognition of building
    construction defect information from text with linguistic noise, Automation in
    Construction 143 (2022) 104543.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] K. Jeon, G. Lee, S. Yang, H. D. Jeong, 从带有语言噪声的文本中识别建筑施工缺陷信息的命名实体，《建筑自动化》143（2022年）104543。'
- en: '[176] M.-Y. Cheng, R. R. Khasani, K. Setiono, Image quality enhancement using
    hybridgan for automated railway track defect recognition, Automation in Construction
    146 (2023) 104669.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] M.-Y. Cheng, R. R. Khasani, K. Setiono, 使用HybridGAN进行图像质量增强以自动化铁路轨道缺陷识别，《建筑自动化》146（2023年）104669。'
- en: '[177] G. Kang, L. Jiang, Y. Yang, A. G. Hauptmann, Contrastive adaptation network
    for unsupervised domain adaptation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 2019, pp. 4893–4902.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] G. Kang, L. Jiang, Y. Yang, A. G. Hauptmann, 无监督领域适应的对比适应网络，收录于：IEEE/CVF
    计算机视觉与模式识别会议论文集，加州长滩，美国，2019年，第4893–4902页。'
- en: '[178] J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh, A. S. Chung,
    L. Hauswald, V. H. Pham, M. Mühlegg, S. Dorn, T. Fernandez, M. Jänicke, S. Mirashi,
    C. Savani, M. Sturm, O. Vorobiov, M. Oelker, S. Garreis, P. Schuberth, A2d2: Audi
    autonomous driving dataset (2020). [arXiv:2004.06320](http://arxiv.org/abs/2004.06320).'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh, A. S. Chung,
    L. Hauswald, V. H. Pham, M. Mühlegg, S. Dorn, T. Fernandez, M. Jänicke, S. Mirashi,
    C. Savani, M. Sturm, O. Vorobiov, M. Oelker, S. Garreis, A2d2: Audi 自动驾驶数据集（2020）。
    [arXiv:2004.06320](http://arxiv.org/abs/2004.06320)。'
- en: '[179] R. Wang, Z. Wu, Z. Weng, J. Chen, G.-J. Qi, Y.-G. Jiang, Cross-domain
    contrastive learning for unsupervised domain adaptation, IEEE Transactions on
    Multimedia (2022) 1–1[doi:10.1109/TMM.2022.3146744](https://doi.org/10.1109/TMM.2022.3146744).'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] R. Wang, Z. Wu, Z. Weng, J. Chen, G.-J. Qi, Y.-G. Jiang, 跨领域对比学习用于无监督领域适应，《IEEE多媒体学报》（2022年）1–1
    [doi:10.1109/TMM.2022.3146744](https://doi.org/10.1109/TMM.2022.3146744)。'
- en: '[180] B. Xie, S. Li, F. Lv, C. H. Liu, G. Wang, D. Wu, A collaborative alignment
    framework of transferable knowledge extraction for unsupervised domain adaptation,
    IEEE Transactions on Knowledge and Data Engineering (2022). [doi:10.1109/TKDE.2022.3185233](https://doi.org/10.1109/TKDE.2022.3185233).'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] B. Xie, S. Li, F. Lv, C. H. Liu, G. Wang, D. Wu, 用于无监督领域适应的可迁移知识提取的协作对齐框架，《IEEE知识与数据工程学报》（2022年）。
    [doi:10.1109/TKDE.2022.3185233](https://doi.org/10.1109/TKDE.2022.3185233)。'
- en: '[181] C.-X. Ren, Y.-H. Liu, X.-W. Zhang, K.-K. Huang, Multi-source unsupervised
    domain adaptation via pseudo target domain, IEEE Transactions on Image Processing
    31 (2022) 2122–2135. [doi:10.1109/TIP.2022.3152052](https://doi.org/10.1109/TIP.2022.3152052).'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] C.-X. Ren, Y.-H. Liu, X.-W. Zhang, K.-K. Huang, 通过伪目标领域的多源无监督领域适应，IEEE
    图像处理汇刊 31 (2022) 2122–2135。 [doi:10.1109/TIP.2022.3152052](https://doi.org/10.1109/TIP.2022.3152052)。'
- en: '[182] B. Fernando, A. Habrard, M. Sebban, T. Tuytelaars, Unsupervised visual
    domain adaptation using subspace alignment, in: 2013 IEEE International Conference
    on Computer Vision, IEEE, Sydney, Australia, 2013, pp. 2960–2967. [doi:10.1109/ICCV.2013.368](https://doi.org/10.1109/ICCV.2013.368).'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] B. Fernando, A. Habrard, M. Sebban, T. Tuytelaars, 使用子空间对齐的无监督视觉领域适应，载于：2013
    IEEE 国际计算机视觉大会，IEEE，澳大利亚悉尼，2013年，页码2960–2967。 [doi:10.1109/ICCV.2013.368](https://doi.org/10.1109/ICCV.2013.368)。'
- en: '[183] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, P. Vandergheynst, Geometric
    deep learning: Going beyond euclidean data, IEEE Signal Processing Magazine 34 (4)
    (2017) 18–42. [doi:10.1109/MSP.2017.2693418](https://doi.org/10.1109/MSP.2017.2693418).'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, P. Vandergheynst, 几何深度学习：超越欧几里得数据，IEEE
    信号处理杂志 34 (4) (2017) 18–42。 [doi:10.1109/MSP.2017.2693418](https://doi.org/10.1109/MSP.2017.2693418)。'
- en: '[184] C.-Y. Lee, T. Batra, M. H. Baig, D. Ulbricht, Sliced wasserstein discrepancy
    for unsupervised domain adaptation, in: 2019 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), IEEE, Long Beach, CA, USA, 2019, pp. 10277–10287.
    [doi:10.1109/CVPR.2019.01053](https://doi.org/10.1109/CVPR.2019.01053).'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] C.-Y. Lee, T. Batra, M. H. Baig, D. Ulbricht, 用于无监督领域适应的切片 Wasserstein
    差异，载于：2019 IEEE/CVF 计算机视觉与模式识别大会（CVPR），IEEE，美国长滩，2019年，页码10277–10287。 [doi:10.1109/CVPR.2019.01053](https://doi.org/10.1109/CVPR.2019.01053)。'
- en: '[185] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,
    A. Aitken, A. Tejani, J. Totz, Z. Wang, W. Shi, Photo-realistic single image super-resolution
    using a generative adversarial network (2017). [arXiv:1609.04802](http://arxiv.org/abs/1609.04802).'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,
    A. Aitken, A. Tejani, J. Totz, Z. Wang, W. Shi, 使用生成对抗网络的照片级真实单图像超分辨率（2017）。 [arXiv:1609.04802](http://arxiv.org/abs/1609.04802)。'
- en: '[186] J. Huang, H. Zhang, L. Yi, T. Funkhouser, M. NieBner, L. Guibas, Texturenet:
    Consistent local parametrizations for learning from high-resolution signals on
    meshes, in: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), IEEE, Long Beach, CA, USA, 2019, pp. 4435–4444.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] J. Huang, H. Zhang, L. Yi, T. Funkhouser, M. NieBner, L. Guibas, Texturenet:
    用于从高分辨率信号中学习的一致局部参数化，载于：2019 IEEE/CVF 计算机视觉与模式识别大会（CVPR），IEEE，美国长滩，2019年，页码4435–4444。'
- en: '[187] B. Gong, K. Grauman, F. Sha, Reshaping visual datasets for domain adaptation,
    Advances in Neural Information Processing Systems 26 (2013).'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] B. Gong, K. Grauman, F. Sha, 为领域适应重塑视觉数据集，神经信息处理系统进展 26 (2013)。'
- en: '[188] Y. Shen, Y. Yang, M. Yan, H. Wang, Y. Zheng, L. J. Guibas, Domain adaptation
    on point clouds via geometry-aware implicits, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, New Orleans, LA, USA, 2022,
    pp. 7223–7232.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Y. Shen, Y. Yang, M. Yan, H. Wang, Y. Zheng, L. J. Guibas, 通过几何感知隐式进行点云领域适应，载于：IEEE/CVF
    计算机视觉与模式识别大会论文集，新奥尔良，美国路易斯安那州，2022年，页码7223–7232。'
- en: '[189] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image translation
    using cycle-consistent adversarial networks, in: Proceedings of the IEEE international
    conference on computer vision, IEEE, Venice, Italy, 2017, pp. 2223–2232.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, 使用循环一致对抗网络的未配对图像到图像翻译，载于：IEEE
    国际计算机视觉大会论文集，IEEE，意大利威尼斯，2017年，页码2223–2232。'
- en: '[190] K. Saleh, A. Abobakr, M. Attia, J. Iskander, D. Nahavandi, M. Hossny,
    S. Nahvandi, Domain adaptation for vehicle detection from bird’s eye view lidar
    point cloud data, in: Proceedings of the IEEE/CVF International Conference on
    Computer Vision Workshops, IEEE, Seoul, Korea, 2019, pp. 0–0.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] K. Saleh, A. Abobakr, M. Attia, J. Iskander, D. Nahavandi, M. Hossny,
    S. Nahvandi, 从鸟瞰视角的激光雷达点云数据进行车辆检测的领域适应，载于：IEEE/CVF 国际计算机视觉大会研讨会论文集，IEEE，韩国首尔，2019年，页码0–0。'
- en: '[191] X. Zhou, A. Karpur, C. Gan, L. Luo, Q. Huang, Unsupervised domain adaptation
    for 3d keypoint estimation via view consistency, in: Proceedings of the European
    conference on computer vision (ECCV), IEEE, Munich, Germany, 2018, pp. 137–153.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] X. Zhou, A. Karpur, C. Gan, L. Luo, Q. Huang, 通过视图一致性进行的 3D 关键点估计的无监督领域适应，载于：欧洲计算机视觉大会（ECCV）论文集，IEEE，德国慕尼黑，2018年，页码137–153。'
- en: '[192] Y. Wang, X. Chen, Y. You, L. E. Li, B. Hariharan, M. Campbell, K. Q.
    Weinberger, W.-L. Chao, Train in germany, test in the usa: Making 3d object detectors
    generalize, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, IEEE, Seattle, WA, USA, 2020, pp. 11713–11723.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Y. Wang, X. Chen, Y. You, L. E. Li, B. Hariharan, M. Campbell, K. Q.
    Weinberger, W.-L. Chao, 在德国训练，在美国测试：让3d目标检测器具有泛化能力，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，IEEE，西雅图，华盛顿州，美国，2020年，页码
    11713–11723。'
- en: '[193] A. Geiger, P. Lenz, C. Stiller, R. Urtasun, Vision meets robotics: The
    kitti dataset, The International Journal of Robotics Research 32 (11) (2013) 1231–1237.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] A. Geiger, P. Lenz, C. Stiller, R. Urtasun, 视觉遇见机器人：KITTI数据集，《国际机器人研究杂志》32（11）（2013年）1231–1237。'
- en: '[194] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine, et al., Scalability in perception for autonomous
    driving: Waymo open dataset, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, IEEE, Seattle, WA, USA, 2020, pp. 2446–2454.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine 等人，自动驾驶感知的可扩展性：Waymo开放数据集，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，IEEE，西雅图，华盛顿州，美国，2020年，页码
    2446–2454。'
- en: '[195] J. Yang, S. Shi, Z. Wang, H. Li, X. Qi, St3d: Self-training for unsupervised
    domain adaptation on 3d object detection, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, IEEE, Nashville, TN, USA, 2021, pp.
    10368–10378.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] J. Yang, S. Shi, Z. Wang, H. Li, X. Qi, St3d：用于3d目标检测的无监督领域适应的自我训练，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，IEEE，纳什维尔，田纳西州，美国，2021年，页码
    10368–10378。'
- en: '[196] M. Jaritz, T.-H. Vu, R. d. Charette, E. Wirbel, P. Pérez, xmuda: Cross-modal
    unsupervised domain adaptation for 3d semantic segmentation, in: Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition, Seattle, WA,
    USA, 2020, pp. 12605–12614.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] M. Jaritz, T.-H. Vu, R. d. Charette, E. Wirbel, P. Pérez, xmuda：用于3d语义分割的跨模态无监督领域适应，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，西雅图，华盛顿州，美国，2020年，页码
    12605–12614。'
- en: '[197] C. Saltori, S. Lathuiliére, N. Sebe, E. Ricci, F. Galasso, Sf-uda 3d:
    Source-free unsupervised domain adaptation for lidar-based 3d object detection,
    in: 2020 International Conference on 3D Vision (3DV), IEEE, Fukuoka, Japan, 2020,
    pp. 771–780.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] C. Saltori, S. Lathuiliére, N. Sebe, E. Ricci, F. Galasso, Sf-uda 3d：用于激光雷达基础的
    3d 目标检测的无源无监督领域适应，收录于：2020年国际3D视觉会议（3DV），IEEE，福冈，日本，2020年，页码 771–780。'
- en: '[198] A. Cardace, R. Spezialetti, P. Z. Ramirez, S. Salti, L. Di Stefano, Refrec:
    Pseudo-labels refinement via shape reconstruction for unsupervised 3d domain adaptation,
    in: 2021 International Conference on 3D Vision (3DV), IEEE, 2021, pp. 331–341.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] A. Cardace, R. Spezialetti, P. Z. Ramirez, S. Salti, L. Di Stefano, Refrec：通过形状重建进行伪标签精炼的无监督3d领域适应，收录于：2021年国际3D视觉会议（3DV），IEEE，2021年，页码
    331–341。'
- en: '[199] L. Shi, Z. Yuan, M. Cheng, Y. Chen, C. Wang, Dfan: Dual-branch feature
    alignment network for domain adaptation on point clouds, IEEE Transactions on
    Geoscience and Remote Sensing 60 (2022) 1–12.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] L. Shi, Z. Yuan, M. Cheng, Y. Chen, C. Wang, Dfan：用于点云领域适应的双分支特征对齐网络，《IEEE地球科学与遥感学报》60（2022年）1–12。'
- en: '[200] D. Kang, Y. Nam, D. Kyung, J. Choi, Unsupervised domain adaptation for
    3d point clouds by searched transformations, IEEE Access 10 (2022) 56901–56913.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] D. Kang, Y. Nam, D. Kyung, J. Choi, 通过搜索变换实现3d点云的无监督领域适应，《IEEE Access》10（2022年）56901–56913。'
- en: '[201] Y. S. Tang, G. H. Lee, Transferable semi-supervised 3d object detection
    from rgb-d data, in: Proceedings of the IEEE/CVF international conference on computer
    vision, 2019, pp. 1931–1940.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Y. S. Tang, G. H. Lee, 从RGB-D数据中可转移的半监督3d目标检测，收录于：IEEE/CVF国际计算机视觉会议论文集，2019年，页码
    1931–1940。'
- en: '[202] X. Huang, G. Mei, J. Zhang, Feature-metric registration: A fast semi-supervised
    approach for robust point cloud registration without correspondences, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp.
    11366–11374.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] X. Huang, G. Mei, J. Zhang, 特征度量配准：一种快速的半监督方法，用于在没有对应关系的情况下进行稳健的点云配准，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，页码
    11366–11374。'
- en: '[203] S. Horache, J.-E. Deschaud, F. Goulette, 3d point cloud registration
    with multi-scale architecture and unsupervised transfer learning, in: 2021 international
    conference on 3D vision (3DV), IEEE, 2021, pp. 1351–1361.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] S. Horache, J.-E. Deschaud, F. Goulette, 具有多尺度架构和无监督迁移学习的3d点云配准，收录于：2021年国际3D视觉会议（3DV），IEEE，2021年，页码
    1351–1361。'
- en: '[204] H. Li, Z. Sun, Y. Wu, Y. Song, Semi-supervised point cloud segmentation
    using self-training with label confidence prediction, Neurocomputing 437 (2021)
    227–237.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] H. Li, Z. Sun, Y. Wu, Y. Song, 使用带有标签置信度预测的自训练的半监督点云分割，神经计算437（2021）227–237页。'
- en: '[205] Z. Chen, L. Jing, Y. Liang, Y. Tian, B. Li, Multimodal semi-supervised
    learning for 3d objects, arXiv preprint arXiv:2110.11601 (2021).'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Z. Chen, L. Jing, Y. Liang, Y. Tian, B. Li, 多模态半监督学习用于3D物体，arXiv预印本 arXiv:2110.11601（2021）。'
- en: '[206] A. Xiao, J. Huang, D. Guan, F. Zhan, S. Lu, Transfer learning from synthetic
    to real lidar point cloud for semantic segmentation, in: Proceedings of the AAAI
    conference on artificial intelligence, Vol. 36, 2022, pp. 2795–2803.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] A. Xiao, J. Huang, D. Guan, F. Zhan, S. Lu, 从合成到真实激光雷达点云的迁移学习，用于语义分割，见：AAAI人工智能会议论文集，第36卷，2022年，页码2795–2803。'
- en: '[207] J. Mei, B. Gao, D. Xu, W. Yao, X. Zhao, H. Zhao, Semantic segmentation
    of 3d lidar data in dynamic scene using semi-supervised learning, IEEE Transactions
    on Intelligent Transportation Systems 21 (6) (2019) 2496–2509.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] J. Mei, B. Gao, D. Xu, W. Yao, X. Zhao, H. Zhao, 在动态场景中使用半监督学习进行3D激光雷达数据的语义分割，IEEE智能交通系统学报第21卷第6期（2019）2496–2509页。'
- en: '[208] S. Huang, Y. Xie, S.-C. Zhu, Y. Zhu, Spatio-temporal self-supervised
    representation learning for 3d point clouds, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2021, pp. 6535–6545.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] S. Huang, Y. Xie, S.-C. Zhu, Y. Zhu, 空间-时间自监督表示学习用于3D点云，见：IEEE/CVF国际计算机视觉会议论文集，2021年，页码6535–6545。'
- en: '[209] Z. Qin, J. Wang, Y. Lu, Weakly supervised 3d object detection from point
    clouds, in: Proceedings of the 28th ACM International Conference on Multimedia,
    2020, pp. 4144–4152.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Z. Qin, J. Wang, Y. Lu, 从点云中进行弱监督3D物体检测，见：第28届ACM国际多媒体会议论文集，2020年，页码4144–4152。'
- en: '[210] P.-C. Yu, C. Sun, M. Sun, Data efficient 3d learner via knowledge transferred
    from 2d model, in: European Conference on Computer Vision, Springer, 2022, pp.
    182–198.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] P.-C. Yu, C. Sun, M. Sun, 通过从2D模型中转移知识的数据高效3D学习器，见：欧洲计算机视觉会议，Springer，2022年，页码182–198。'
- en: '[211] Z. Xu, B. Yuan, S. Zhao, Q. Zhang, X. Gao, Hierarchical point-based active
    learning for semi-supervised point cloud semantic segmentation, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 18098–18108.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Z. Xu, B. Yuan, S. Zhao, Q. Zhang, X. Gao, 分层点基础的主动学习用于半监督点云语义分割，见：IEEE/CVF国际计算机视觉会议论文集，2023年，页码18098–18108。'
- en: '[212] D. Zhang, D. Liang, Z. Zou, J. Li, X. Ye, Z. Liu, X. Tan, X. Bai, A simple
    vision transformer for weakly semi-supervised 3d object detection, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 8373–8383.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] D. Zhang, D. Liang, Z. Zou, J. Li, X. Ye, Z. Liu, X. Tan, X. Bai, 用于弱半监督3D物体检测的简单视觉变换器，见：IEEE/CVF国际计算机视觉会议论文集，2023年，页码8373–8383。'
- en: '[213] Y. Wang, J. Yin, W. Li, P. Frossard, R. Yang, J. Shen, Ssda3d: Semi-supervised
    domain adaptation for 3d object detection from point cloud, in: Proceedings of
    the AAAI Conference on Artificial Intelligence, Vol. 37, 2023, pp. 2707–2715.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] Y. Wang, J. Yin, W. Li, P. Frossard, R. Yang, J. Shen, Ssda3d: 从点云中进行3D物体检测的半监督领域自适应，见：AAAI人工智能会议论文集，第37卷，2023年，页码2707–2715。'
- en: '[214] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, Q. He, A
    comprehensive survey on transfer learning, Proceedings of the IEEE 109 (1) (2020)
    43–76.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, Q. He, 迁移学习的全面调查，IEEE第109卷第1期（2020）43–76页。'
- en: '[215] S. J. Pan, Q. Yang, A survey on transfer learning, IEEE Transactions
    on knowledge and data engineering 22 (10) (2009) 1345–1359.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] S. J. Pan, Q. Yang, 迁移学习的调查，IEEE知识与数据工程学报第22卷第10期（2009）1345–1359页。'
- en: '[216] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, A. Markham,
    Randla-net: Efficient semantic segmentation of large-scale point clouds, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle,
    WA, USA, 2020, pp. 11108–11117.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, A. Markham,
    Randla-net: 大规模点云的高效语义分割，见：IEEE/CVF计算机视觉与模式识别会议论文集，西雅图，美国，2020年，页码11108–11117。'
- en: '[217] T. Hackel, N. Savinov, L. Ladicky, J. D. Wegner, K. Schindler, M. Pollefeys,
    Semantic3d. net: A new large-scale point cloud classification benchmark, arXiv
    preprint arXiv:1704.03847 (2017).'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] T. Hackel, N. Savinov, L. Ladicky, J. D. Wegner, K. Schindler, M. Pollefeys,
    Semantic3d. net: 一个新的大规模点云分类基准，arXiv预印本 arXiv:1704.03847（2017）。'
- en: '[218] C. Zhao, H. Guo, J. Lu, D. Yu, D. Li, X. Chen, Als point cloud classification
    with small training data set based on transfer learning, IEEE Geoscience and Remote
    Sensing Letters 17 (8) (2019) 1406–1410.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] C. Zhao, H. Guo, J. Lu, D. Yu, D. Li, X. Chen, 基于迁移学习的小型训练数据集的ALS点云分类，《IEEE地球科学与遥感快报》17（8）（2019年）1406–1410。'
- en: '[219] Z. Liu, H. Tang, Y. Lin, S. Han, Point-voxel cnn for efficient 3d deep
    learning, Advances in Neural Information Processing Systems 32 (2019).'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Z. Liu, H. Tang, Y. Lin, S. Han, 高效3D深度学习的点-体素CNN，《神经信息处理系统进展》32（2019年）。'
- en: '[220] C. Sun, M. Ma, Z. Zhao, S. Tian, R. Yan, X. Chen, Deep transfer learning
    based on sparse autoencoder for remaining useful life prediction of tool in manufacturing,
    IEEE Transactions on Industrial Informatics 15 (4) (2018) 2416–2425.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] C. Sun, M. Ma, Z. Zhao, S. Tian, R. Yan, X. Chen, 基于稀疏自编码器的深度迁移学习用于制造中工具剩余使用寿命预测，《IEEE工业信息学学报》15（4）（2018年）2416–2425。'
- en: '[221] L. Huang, H. Guo, Q. Rao, Z. Hou, S. Li, S. Qiu, X. Fan, H. Wang, Body
    dimension measurements of qinchuan cattle with transfer learning from lidar sensing,
    Sensors 19 (22) (2019) 5046.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] L. Huang, H. Guo, Q. Rao, Z. Hou, S. Li, S. Qiu, X. Fan, H. Wang, 基于激光雷达感知的秦川牛体测量的迁移学习，《传感器》19（22）（2019年）5046。'
- en: '[222] A. Arnold, R. Nallapati, W. W. Cohen, A comparative study of methods
    for transductive transfer learning, in: Seventh IEEE international conference
    on data mining workshops (ICDMW 2007), IEEE, IEEE, Omaha, NE, 2007, pp. 77–82.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] A. Arnold, R. Nallapati, W. W. Cohen, 传递性迁移学习方法的比较研究，见：第七届IEEE国际数据挖掘会议工作坊（ICDMW
    2007），IEEE，Omaha, NE，2007年，77–82页。'
- en: '[223] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, D. Krishnan, Unsupervised
    pixel-level domain adaptation with generative adversarial networks, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, IEEE, Honolulu,
    HI, USA, 2017, pp. 3722–3731.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, D. Krishnan, 基于生成对抗网络的无监督像素级领域适应，见：IEEE计算机视觉与模式识别大会论文集，IEEE，檀香山，HI，美国，2017年，3722–3731页。'
- en: '[224] L. Duan, I. W. Tsang, D. Xu, Domain transfer multiple kernel learning,
    IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (3) (2012) 465–479.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] L. Duan, I. W. Tsang, D. Xu, 领域迁移多核学习，《IEEE模式分析与机器智能学报》34（3）（2012年）465–479。'
- en: '[225] M. Long, H. Zhu, J. Wang, M. I. Jordan, Deep transfer learning with joint
    adaptation networks, in: International conference on machine learning, PMLR, PMLR,
    Sydney, Australia, 2017, pp. 2208–2217.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] M. Long, H. Zhu, J. Wang, M. I. Jordan, 基于联合适应网络的深度迁移学习，见：国际机器学习会议，PMLR，悉尼，澳大利亚，2017年，2208–2217页。'
- en: '[226] W. Zhang, W. Ouyang, W. Li, D. Xu, Collaborative and adversarial network
    for unsupervised domain adaptation, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, IEEE, Salt Lake City, UT, USA, 2018,
    pp. 3801–3809.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] W. Zhang, W. Ouyang, W. Li, D. Xu, 协作与对抗网络用于无监督领域适应，见：IEEE计算机视觉与模式识别大会论文集，IEEE，盐湖城，UT，美国，2018年，3801–3809页。'
- en: '[227] C. B. Rist, M. Enzweiler, D. M. Gavrila, Cross-sensor deep domain adaptation
    for lidar detection and segmentation, in: 2019 IEEE Intelligent Vehicles Symposium
    (IV), IEEE, 2019, pp. 1535–1542.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] C. B. Rist, M. Enzweiler, D. M. Gavrila, 跨传感器深度领域适应用于激光雷达检测和分割，见：2019年IEEE智能车辆研讨会（IV），IEEE，2019年，1535–1542页。'
- en: '[228] Z. Wang, S. Ding, Y. Li, M. Zhao, S. Roychowdhury, A. Wallin, G. Sapiro,
    Q. Qiu, Range adaptation for 3d object detection in lidar, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision Workshops, Seoul, Korea,
    2019, pp. 0–0.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] Z. Wang, S. Ding, Y. Li, M. Zhao, S. Roychowdhury, A. Wallin, G. Sapiro,
    Q. Qiu, 激光雷达中的3D物体检测的范围适应，见：IEEE/CVF国际计算机视觉大会研讨会论文集，首尔，韩国，2019年，0–0页。'
- en: '[229] Z. Wang, L. Wang, B. Dai, Strong-weak feature alignment for 3d object
    detection, Electronics 10 (10) (2021) 1205.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] Z. Wang, L. Wang, B. Dai, 强弱特征对齐用于3D物体检测，《电子学》10（10）（2021年）1205。'
- en: '[230] W. Zhang, W. Li, D. Xu, Srdan: Scale-aware and range-aware domain adaptation
    network for cross-dataset 3d object detection, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 2021,
    pp. 6769–6779.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] W. Zhang, W. Li, D. Xu, SRDAN: 适应范围感知和尺度感知的跨数据集3D物体检测网络，见：IEEE/CVF计算机视觉与模式识别大会论文集，纳什维尔，TN，美国，2021年，6769–6779页。'
- en: '[231] L. Nunes, R. Marcuzzi, X. Chen, J. Behley, C. Stachniss, Segcontrast:
    3d point cloud feature representation learning through self-supervised segment
    discrimination, IEEE Robotics and Automation Letters 7 (2) (2022) 2116–2123.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] L. Nunes, R. Marcuzzi, X. Chen, J. Behley, C. Stachniss, Segcontrast:
    通过自监督分段区分进行3D点云特征表示学习，《IEEE机器人与自动化快报》7（2）（2022年）2116–2123。'
- en: '[232] W. Liu, Z. Luo, Y. Cai, Y. Yu, Y. Ke, J. M. Junior, W. N. Gonçalves,
    J. Li, Adversarial unsupervised domain adaptation for 3d semantic segmentation
    with multi-modal learning, ISPRS Journal of Photogrammetry and Remote Sensing
    176 (2021) 211–221.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] W. Liu, Z. Luo, Y. Cai, Y. Yu, Y. Ke, J. M. Junior, W. N. Gonçalves,
    J. Li, 针对3D语义分割的对抗性无监督领域自适应与多模态学习，ISPRS摄影测量与遥感期刊 176 (2021) 211–221。'
- en: '[233] H. Tang, C. Xu, J. Yang, Bi-adversarial discrepancy minimization for
    unsupervised domain adaptation on 3d point cloud, in: 2021 International Joint
    Conference on Neural Networks (IJCNN), IEEE, IEEE, online, 2021, pp. 1–8.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] H. Tang, C. Xu, J. Yang, 3D点云的无监督领域自适应双对抗差异最小化，见：2021年国际神经网络联合会议 (IJCNN)，IEEE，在线，2021年，第1-8页。'
- en: '[234] S. Vesal, M. Gu, R. Kosti, A. Maier, N. Ravikumar, Adapt everywhere:
    unsupervised adaptation of point-clouds and entropy minimization for multi-modal
    cardiac image segmentation, IEEE Transactions on Medical Imaging 40 (7) (2021)
    1838–1851.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] S. Vesal, M. Gu, R. Kosti, A. Maier, N. Ravikumar, 到处适应：点云的无监督适应和多模态心脏图像分割的熵最小化，IEEE医学影像汇刊
    40 (7) (2021) 1838–1851。'
- en: '[235] M. A. U. Alam, M. M. Rahman, J. Q. Widberg, Palmar: Towards adaptive
    multi-inhabitant activity recognition in point-cloud technology, in: IEEE INFOCOM
    2021-IEEE Conference on Computer Communications, IEEE, IEEE, online, 2021, pp.
    1–10.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] M. A. U. Alam, M. M. Rahman, J. Q. Widberg, Palmar: 通过点云技术实现适应性多居住者活动识别，见：IEEE
    INFOCOM 2021-IEEE计算机通信会议，IEEE，在线，2021年，第1-10页。'
- en: '[236] Z. Wang, L. Wang, L. Xiao, B. Dai, Unsupervised subcategory domain adaptive
    network for 3d object detection in lidar, Electronics 10 (8) (2021) 927.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] Z. Wang, L. Wang, L. Xiao, B. Dai, 无监督子类别领域自适应网络用于激光雷达中的3D物体检测，Electronics
    10 (8) (2021) 927。'
- en: '[237] Z. Qiao, H. Hu, W. Shi, S. Chen, Z. Liu, H. Wang, A registration-aided
    domain adaptation network for 3d point cloud based place recognition, in: 2021
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE,
    IEEE, Prague, Czech Republic, 2021, pp. 1317–1322.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] Z. Qiao, H. Hu, W. Shi, S. Chen, Z. Liu, H. Wang, 基于注册辅助领域自适应网络的3D点云场所识别，见：2021
    IEEE/RSJ国际智能机器人与系统会议 (IROS)，IEEE，布拉格，捷克共和国，2021年，第1317-1322页。'
- en: '[238] X. Zhu, H. Manamasa, J. L. J. Sánchez, A. Maki, L. Hanson, Automatic
    assembly quality inspection based on an unsupervised point cloud domain adaptation
    model, Procedia CIRP 104 (2021) 1801–1806.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] X. Zhu, H. Manamasa, J. L. J. Sánchez, A. Maki, L. Hanson, 基于无监督点云领域自适应模型的自动装配质量检测，Procedia
    CIRP 104 (2021) 1801–1806。'
- en: '[239] F. Wang, W. Li, D. Xu, Cross-dataset point cloud recognition using deep-shallow
    domain adaptation network, IEEE Transactions on Image Processing 30 (2021) 7364–7377.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] F. Wang, W. Li, D. Xu, 跨数据集点云识别使用深浅域自适应网络，IEEE图像处理汇刊 30 (2021) 7364–7377。'
- en: '[240] M. A. U. Alam, F. Mazzoni, M. M. Rahman, J. Widberg, Lamar: Lidar based
    multi-inhabitant activity recognition, in: MobiQuitous 2020-17th EAI International
    Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services,
    EAI, online, 2020, pp. 1–9.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] M. A. U. Alam, F. Mazzoni, M. M. Rahman, J. Widberg, Lamar: 基于激光雷达的多居住者活动识别，见：MobiQuitous
    2020-第17届EAI国际移动和普适系统会议：计算、网络和服务，EAI，在线，2020年，第1-9页。'
- en: '[241] M. Jaritz, T.-H. Vu, R. De Charette, É. Wirbel, P. Pérez, Cross-modal
    learning for domain adaptation in 3d semantic segmentation, IEEE Transactions
    on Pattern Analysis and Machine Intelligence 45 (2) (2022) 1533–1544.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] M. Jaritz, T.-H. Vu, R. De Charette, É. Wirbel, P. Pérez, 3D语义分割中的跨模态学习与领域自适应，IEEE模式分析与机器智能汇刊
    45 (2) (2022) 1533–1544。'
- en: '[242] K. Saleh, A. Abobakr, D. Nahavandi, J. Iskander, M. Attia, M. Hossny,
    S. Nahavandi, Cyclist intent prediction using 3d lidar sensors for fully automated
    vehicles, in: 2019 IEEE Intelligent Transportation Systems Conference (ITSC),
    IEEE, IEEE, Auckland, New Zealand, 2019, pp. 2020–2026.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] K. Saleh, A. Abobakr, D. Nahavandi, J. Iskander, M. Attia, M. Hossny,
    S. Nahavandi, 使用3D激光雷达传感器对完全自动化车辆进行骑行者意图预测，见：2019年IEEE智能交通系统会议 (ITSC)，IEEE，奥克兰，新西兰，2019年，第2020-2026页。'
- en: '[243] M. Pham, S. Alse, C. A. Knoblock, P. Szekely, Semantic labeling: a domain-independent
    approach, in: International Semantic Web Conference, Springer, Springer, Kobe,
    Japan, 2016, pp. 446–462.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] M. Pham, S. Alse, C. A. Knoblock, P. Szekely, 语义标注：一种领域独立的方法，见：国际语义网会议，Springer，神户，日本，2016年，第446-462页。'
- en: '[244] T. Xia, J. Yang, L. Chen, Automated semantic segmentation of bridge point
    cloud based on local descriptor and machine learning, Automation in Construction
    133 (2022) 103992.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] T. Xia, J. Yang, L. Chen, 基于局部描述符和机器学习的桥梁点云自动语义分割，自动化施工 133 (2022) 103992。'
- en: '[245] R. Sun, X. Zhu, C. Wu, C. Huang, J. Shi, L. Ma, Not all areas are equal:
    Transfer learning for semantic segmentation via hierarchical region selection,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    Long Beach, CA, USA, 2019, pp. 4360–4369.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] R. Sun, X. Zhu, C. Wu, C. Huang, J. Shi, 并非所有区域都是平等的：通过分层区域选择进行语义分割的迁移学习，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，美国长滩，2019年，第4360–4369页。'
- en: '[246] S. Hong, J. Oh, H. Lee, B. Han, Learning transferrable knowledge for
    semantic segmentation with deep convolutional neural network, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, IEEE, Las Vegas,
    NV, USA, 2016, pp. 3204–3212.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] S. Hong, J. Oh, H. Lee, B. Han, 使用深度卷积神经网络学习可迁移知识进行语义分割，发表于：IEEE计算机视觉与模式识别会议论文集，IEEE，美国拉斯维加斯，2016年，第3204–3212页。'
- en: '[247] H. A. Arief, U. G. Indahl, G.-H. Strand, H. Tveite, Addressing overfitting
    on point cloud classification using atrous xcrf, ISPRS Journal of Photogrammetry
    and Remote Sensing 155 (2019) 90–101.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] H. A. Arief, U. G. Indahl, G.-H. Strand, H. Tveite, 通过使用卷积xcrf解决点云分类中的过拟合问题，ISPRS摄影测量与遥感杂志
    155 (2019) 90–101。'
- en: '[248] H. Yang, J. Shi, L. Carlone, Teaser: Fast and certifiable point cloud
    registration, IEEE Transactions on Robotics 37 (2) (2020) 314–333.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] H. Yang, J. Shi, L. Carlone, Teaser：快速且可认证的点云配准，IEEE机器人学报 37 (2) (2020)
    314–333。'
- en: '[249] S. Choi, Q.-Y. Zhou, V. Koltun, Robust reconstruction of indoor scenes,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    IEEE, Boston, MA, USA, 2015, pp. 5556–5565.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] S. Choi, Q.-Y. Zhou, V. Koltun, 室内场景的稳健重建，发表于：IEEE计算机视觉与模式识别会议论文集，IEEE，美国波士顿，2015年，第5556–5565页。'
- en: '[250] Y. Wang, J. M. Solomon, Deep closest point: Learning representations
    for point cloud registration, in: Proceedings of the IEEE/CVF international conference
    on computer vision, Seoul, Korea (South), 2019, pp. 3523–3532.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] Y. Wang, J. M. Solomon, Deep closest point：用于点云配准的学习表示，发表于：IEEE/CVF国际计算机视觉会议论文集，韩国首尔，2019年，第3523–3532页。'
- en: '[251] Y. Chen, J. Liu, B. Ni, H. Wang, J. Yang, N. Liu, T. Li, Q. Tian, Shape
    self-correction for unsupervised point cloud understanding, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada,
    2021, pp. 8382–8391.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] Y. Chen, J. Liu, B. Ni, H. Wang, J. Yang, N. Liu, T. Li, Q. Tian, 形状自我修正用于无监督点云理解，发表于：IEEE/CVF国际计算机视觉会议论文集，加拿大蒙特利尔，2021年，第8382–8391页。'
- en: '[252] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao,
    H. Li, Pointclip: Point cloud understanding by clip, arXiv preprint arXiv:2112.02413
    (2021).'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao,
    H. Li, Pointclip：通过剪辑进行点云理解，arXiv预印本 arXiv:2112.02413 (2021)。'
- en: '[253] K. Liu, Z. Gao, F. Lin, B. M. Chen, Fg-net: A fast and accurate framework
    for large-scale lidar point cloud understanding, IEEE Transactions on Cybernetics
    (2022).'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] K. Liu, Z. Gao, F. Lin, B. M. Chen, Fg-net：一个快速且精确的大规模激光雷达点云理解框架，IEEE网络学报
    (2022)。'
- en: '[254] R. Roveri, A. C. Öztireli, I. Pandele, M. Gross, Pointpronets: Consolidation
    of point clouds with convolutional neural networks, in: Computer Graphics Forum,
    Vol. 37, Wiley Online Library, 2018, pp. 87–99.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] R. Roveri, A. C. Öztireli, I. Pandele, M. Gross, Pointpronets：使用卷积神经网络整合点云，发表于：计算机图形学论坛，第37卷，Wiley在线图书馆，2018年，第87–99页。'
- en: '[255] S. Luo, W. Hu, Score-based point cloud denoising, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada,
    2021, pp. 4583–4592.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] S. Luo, W. Hu, 基于评分的点云去噪，发表于：IEEE/CVF国际计算机视觉会议论文集，加拿大蒙特利尔，2021年，第4583–4592页。'
- en: '[256] J. Zeng, G. Cheung, M. Ng, J. Pang, C. Yang, 3d point cloud denoising
    using graph laplacian regularization of a low dimensional manifold model, IEEE
    Transactions on Image Processing 29 (2019) 3474–3489.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] J. Zeng, G. Cheung, M. Ng, J. Pang, C. Yang, 使用图拉普拉斯正则化低维流形模型的3D点云去噪，IEEE图像处理学报
    29 (2019) 3474–3489。'
- en: '[257] C. Dinesh, G. Cheung, I. V. Bajic, C. Yang, Fast 3d point cloud denoising
    via bipartite graph approximation & total variation, arXiv preprint arXiv:1804.10831
    (2018).'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] C. Dinesh, G. Cheung, I. V. Bajic, C. Yang, 通过二分图逼近和总变差进行快速3D点云去噪，arXiv预印本
    arXiv:1804.10831 (2018)。'
- en: '[258] C. Duan, S. Chen, J. Kovačević, Weighted multi-projection: 3d point cloud
    denoising with estimated tangent planes, arXiv preprint arXiv:1807.00253 (2018).'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] C. Duan, S. Chen, J. Kovačević, 加权多投影：基于估计切线平面的3D点云去噪，arXiv预印本 arXiv:1807.00253
    (2018)。'
- en: '[259] W. Hu, X. Gao, G. Cheung, Z. Guo, Feature graph learning for 3d point
    cloud denoising, IEEE Transactions on Signal Processing 68 (2020) 2841–2856.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] W. Hu, X. Gao, G. Cheung, Z. Guo，《用于3D点云去噪的特征图学习》，《IEEE信号处理杂志》，68 (2020)
    2841–2856。'
- en: '[260] V. Badrinarayanan, A. Kendall, R. Cipolla, Segnet: A deep convolutional
    encoder-decoder architecture for image segmentation, IEEE transactions on pattern
    analysis and machine intelligence 39 (12) (2017) 2481–2495.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] V. Badrinarayanan, A. Kendall, R. Cipolla，《Segnet：一种用于图像分割的深度卷积编码解码器架构》，《IEEE模式分析与机器智能》期刊，39
    (12) (2017) 2481–2495。'
- en: '[261] A. Eltner, P. O. Bressan, T. Akiyama, W. N. Gonçalves, J. Marcato Junior,
    Using deep learning for automatic water stage measurements, Water Resources Research
    57 (3) (2021) e2020WR027608.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] A. Eltner, P. O. Bressan, T. Akiyama, W. N. Gonçalves, J. Marcato Junior，《使用深度学习进行自动水位测量》，《水资源研究》57
    (3) (2021) e2020WR027608。'
- en: '[262] C. Zhang, J. Shi, X. Deng, Z. Wu, Upsampling autoencoder for self-supervised
    point cloud learning, arXiv preprint arXiv:2203.10768 (2022).'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] C. Zhang, J. Shi, X. Deng, Z. Wu，《用于自监督点云学习的上采样自编码器》，arXiv预印本arXiv:2203.10768
    (2022)。'
- en: '[263] L. Garrote, J. Rosa, J. Paulo, C. Premebida, P. Peixoto, U. J. Nunes,
    3d point cloud downsampling for 2d indoor scene modelling in mobile robotics,
    in: 2017 IEEE international conference on autonomous robot systems and competitions
    (ICARSC), IEEE, IEEE, Coimbra, Portugal, 2017, pp. 228–233.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] L. Garrote, J. Rosa, J. Paulo, C. Premebida, P. Peixoto, U. J. Nunes，《移动机器人中的2D室内场景建模的3D点云降采样》，收录于：2017年IEEE国际自主机器人系统与竞赛会议（ICARSC），IEEE，科英布拉，葡萄牙，2017年，第228–233页。'
- en: '[264] S. Orts-Escolano, V. Morell, J. García-Rodríguez, M. Cazorla, Point cloud
    data filtering and downsampling using growing neural gas, in: The 2013 International
    Joint Conference on Neural Networks (IJCNN), IEEE, IEEE, Dallas, Texas, USA, 2013,
    pp. 1–8.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] S. Orts-Escolano, V. Morell, J. García-Rodríguez, M. Cazorla，《使用增长神经气体的点云数据过滤和降采样》，收录于：2013年国际神经网络联合会议（IJCNN），IEEE，达拉斯，德克萨斯州，美国，2013年，第1–8页。'
- en: '[265] E. Nezhadarya, E. Taghavi, R. Razani, B. Liu, J. Luo, Adaptive hierarchical
    down-sampling for point cloud classification, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 2020,
    pp. 12956–12964.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] E. Nezhadarya, E. Taghavi, R. Razani, B. Liu, J. Luo，《用于点云分类的自适应分层降采样》，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，西雅图，华盛顿，美国，2020年，第12956–12964页。'
- en: '[266] C. Suchocki, W. Błaszczak-Bąk, Down-sampling of point clouds for the
    technical diagnostics of buildings and structures, Geosciences 9 (2) (2019) 70.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] C. Suchocki, W. Błaszczak-Bąk，《用于建筑和结构技术诊断的点云降采样》，《地球科学》9 (2) (2019)
    70。'
- en: '[267] C. Lv, M. Qi, X. Li, Z. Yang, H. Ma, Sgformer: Semantic graph transformer
    for point cloud-based 3d scene graph generation, in: Proceedings of the AAAI Conference
    on Artificial Intelligence, Vol. 38, 2024, pp. 4035–4043.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] C. Lv, M. Qi, X. Li, Z. Yang, H. Ma，《Sgformer：用于基于点云的3D场景图生成的语义图变换器》，收录于：AAAI人工智能会议论文集，第38卷，2024年，第4035–4043页。'
- en: '[268] Z. Li, Z. Li, Z. Cui, M. Pollefeys, M. R. Oswald, Sat2scene: 3d urban
    scene generation from satellite images with diffusion, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 7141–7150.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] Z. Li, Z. Li, Z. Cui, M. Pollefeys, M. R. Oswald，《Sat2scene：通过扩散从卫星图像生成3D城市场景》，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，2024年，第7141–7150页。'
- en: '[269] P. Li, C. Tang, Q. Huang, Z. Li, Art3d: 3d gaussian splatting for text-guided
    artistic scenes generation, arXiv preprint arXiv:2405.10508 (2024).'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] P. Li, C. Tang, Q. Huang, Z. Li，《Art3d：用于文本引导艺术场景生成的3D高斯喷射》，arXiv预印本arXiv:2405.10508
    (2024)。'
- en: '[270] J. Chung, S. Lee, H. Nam, J. Lee, K. M. Lee, Luciddreamer: Domain-free
    generation of 3d gaussian splatting scenes, arXiv preprint arXiv:2311.13384 (2023).'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] J. Chung, S. Lee, H. Nam, J. Lee, K. M. Lee，《Luciddreamer：无领域的3D高斯喷射场景生成》，arXiv预印本arXiv:2311.13384
    (2023)。'
- en: '[271] Y.-K. Wang, C. Xing, Y.-L. Wei, X.-M. Wu, W.-S. Zheng, Single-view scene
    point cloud human grasp generation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2024, pp. 831–841.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] Y.-K. Wang, C. Xing, Y.-L. Wei, X.-M. Wu, W.-S. Zheng，《单视角场景点云人体抓取生成》，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，2024年，第831–841页。'
- en: '[272] S. Koch, P. Hermosilla, N. Vaskevicius, M. Colosi, T. Ropinski, Sgrec3d:
    Self-supervised 3d scene graph learning via object-level scene reconstruction,
    in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision, 2024, pp. 3404–3414.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] S. Koch, P. Hermosilla, N. Vaskevicius, M. Colosi, T. Ropinski，《Sgrec3d：通过对象级场景重建进行自监督3D场景图学习》，收录于：IEEE/CVF计算机视觉应用冬季会议论文集，2024年，第3404–3414页。'
- en: '[273] Y. Liu, X. Li, X. Li, L. Qi, C. Li, M.-H. Yang, Pyramid diffusion for
    fine 3d large scene generation, arXiv preprint arXiv:2311.12085 (2023).'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] Y. Liu, X. Li, X. Li, L. Qi, C. Li, M.-H. Yang, 金字塔扩散用于细化的 3D 大场景生成，arXiv
    预印本 arXiv:2311.12085 (2023)。'
- en: '[274] T. Mathes, D. Seidel, K.-H. Häberle, H. Pretzsch, P. Annighöfer, What
    are we missing? occlusion in laser scanning point clouds and its impact on the
    detection of single-tree morphologies and stand structural variables, Remote Sensing
    15 (2) (2023) 450.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] T. Mathes, D. Seidel, K.-H. Häberle, H. Pretzsch, P. Annighöfer, 我们遗漏了什么？激光扫描点云中的遮挡及其对单棵树木形态和林分结构变量检测的影响，Remote
    Sensing 15 (2) (2023) 450。'
- en: '[275] Y. Zhang, L. Wang, Y. Dai, Plot: a 3d point cloud object detection network
    for autonomous driving, Robotica (2023) 1–17.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] Y. Zhang, L. Wang, Y. Dai, Plot: 一种用于自主驾驶的 3D 点云物体检测网络，Robotica (2023)
    1–17。'
- en: '[276] Y. Himeur, M. Elnour, F. Fadli, N. Meskin, I. Petri, Y. Rezgui, F. Bensaali,
    A. Amira, Next-generation energy systems for sustainable smart cities: Roles of
    transfer learning, Sustainable Cities and Society (2022) 104059.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] Y. Himeur, M. Elnour, F. Fadli, N. Meskin, I. Petri, Y. Rezgui, F. Bensaali,
    A. Amira, 下一代能源系统在可持续智能城市中的角色：迁移学习的作用，Sustainable Cities and Society (2022) 104059。'
- en: '[277] C. Romanengo, A. Raffo, S. Biasotti, B. Falcidieno, Recognising geometric
    primitives in 3d point clouds of mechanical cad objects, Computer-Aided Design
    (2023) 103479.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] C. Romanengo, A. Raffo, S. Biasotti, B. Falcidieno, 在机械 CAD 对象的 3D 点云中识别几何原语，Computer-Aided
    Design (2023) 103479。'
- en: '[278] Y. Himeur, S. Al-Maadeed, I. Varlamis, N. Al-Maadeed, K. Abualsaud, A. Mohamed,
    Face mask detection in smart cities using deep and transfer learning: Lessons
    learned from the covid-19 pandemic, Systems 11 (2) (2023) 107.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] Y. Himeur, S. Al-Maadeed, I. Varlamis, N. Al-Maadeed, K. Abualsaud, A.
    Mohamed, 利用深度学习和迁移学习进行智能城市中的口罩检测：从 COVID-19 大流行中获得的经验教训，Systems 11 (2) (2023)
    107。'
- en: '[279] A. Argyriou, A. Maurer, M. Pontil, An algorithm for transfer learning
    in a heterogeneous environment, in: Joint European Conference on Machine Learning
    and Knowledge Discovery in Databases, Springer, Springer, Antwerp, Belgium, 2008,
    pp. 71–85.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] A. Argyriou, A. Maurer, M. Pontil, 在异构环境中迁移学习的算法，收录于：欧洲联合机器学习与数据库知识发现会议，Springer，Antwerp，Belgium，2008，第
    71–85 页。'
- en: '[280] R. Gong, D. Dai, Y. Chen, W. Li, L. Van Gool, mdalu: Multi-source domain
    adaptation and label unification with partial datasets, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada, 2021,
    pp. 8876–8885.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] R. Gong, D. Dai, Y. Chen, W. Li, L. Van Gool, MDALU：带部分数据集的多源领域适应和标签统一，收录于：IEEE/CVF
    国际计算机视觉会议论文集，Montreal, BC, Canada, 2021，第 8876–8885 页。'
- en: '[281] L. Wang, X. Geng, X. Ma, D. Zhang, Q. Yang, Ridesharing car detection
    by transfer learning, Artificial Intelligence 273 (2019) 1–18.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] L. Wang, X. Geng, X. Ma, D. Zhang, Q. Yang, 通过迁移学习进行拼车汽车检测，Artificial
    Intelligence 273 (2019) 1–18。'
- en: '[282] P. Morerio, J. Cavazza, V. Murino, Minimal-entropy correlation alignment
    for unsupervised deep domain adaptation, arXiv preprint arXiv:1711.10288 (2017).'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] P. Morerio, J. Cavazza, V. Murino, 用于无监督深度领域适应的最小熵相关对齐，arXiv 预印本 arXiv:1711.10288
    (2017)。'
- en: '[283] M. Liang, B. Yang, Y. Chen, R. Hu, R. Urtasun, Multi-task multi-sensor
    fusion for 3d object detection, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, Long Beach, CA, USA, 2019, pp. 7345–7353.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] M. Liang, B. Yang, Y. Chen, R. Hu, R. Urtasun, 用于 3D 物体检测的多任务多传感器融合，收录于：IEEE/CVF
    计算机视觉与模式识别会议论文集，Long Beach, CA, USA, 2019，第 7345–7353 页。'
- en: '[284] A. Valada, R. Mohan, W. Burgard, Self-supervised model adaptation for
    multimodal semantic segmentation, International Journal of Computer Vision 128 (5)
    (2020) 1239–1285.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] A. Valada, R. Mohan, W. Burgard, 用于多模态语义分割的自监督模型适应，《国际计算机视觉杂志》128 (5)
    (2020) 1239–1285。'
- en: '[285] Y. Himeur, S. Al-Maadeed, N. Almadeed, K. Abualsaud, A. Mohamed, T. Khattab,
    O. Elharrouss, Deep visual social distancing monitoring to combat covid-19: A
    comprehensive survey, Sustainable Cities and Society (2022) 104064.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] Y. Himeur, S. Al-Maadeed, N. Almadeed, K. Abualsaud, A. Mohamed, T. Khattab,
    O. Elharrouss, 深度视觉社交距离监测以抗击 COVID-19：全面调查，Sustainable Cities and Society (2022)
    104064。'
- en: '[286] G. Mateo-García, V. Laparra, D. López-Puigdollers, L. Gómez-Chova, Transferring
    deep learning models for cloud detection between landsat-8 and proba-v, ISPRS
    Journal of Photogrammetry and Remote Sensing 160 (2020) 1–17.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] G. Mateo-García, V. Laparra, D. López-Puigdollers, L. Gómez-Chova, 在
    Landsat-8 和 Proba-V 之间转移深度学习模型用于云检测，ISPRS 摄影测量与遥感杂志 160 (2020) 1–17。'
- en: '[287] X. Ling, R. Qin, A graph-matching approach for cross-view registration
    of over-view and street-view based point clouds, ISPRS Journal of Photogrammetry
    and Remote Sensing 185 (2022) 2–15.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] X. Ling, R. Qin, 一种用于从全景视图和街景视图点云的交叉视图配准的图匹配方法, ISPRS 摄影测量与遥感期刊 185 (2022)
    2–15。'
- en: '[288] X. Glorot, A. Bordes, Y. Bengio, Domain adaptation for large-scale sentiment
    classification: A deep learning approach, in: ICML, 2011.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] X. Glorot, A. Bordes, Y. Bengio, 大规模情感分类的领域适应：一种深度学习方法, 载于：ICML, 2011年。'
- en: '[289] W. Hu, Y. Luo, Z. Lu, Y. Wen, Heterogeneous transfer learning for thermal
    comfort modeling, in: Proceedings of the 6th ACM International Conference on Systems
    for Energy-Efficient Buildings, Cities, and Transportation, ACM, New York, NY,
    USA, 2019, pp. 61–70.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] W. Hu, Y. Luo, Z. Lu, Y. Wen, 异质迁移学习在热舒适建模中的应用, 载于：第6届ACM国际能源高效建筑、城市和交通系统会议论文集,
    ACM, 纽约, NY, USA, 2019年，第61–70页。'
- en: '[290] C. Fan, Y. Sun, F. Xiao, J. Ma, D. Lee, J. Wang, Y. C. Tseng, Statistical
    investigations of transfer learning-based methodology for short-term building
    energy predictions, Applied Energy 262 (2020) 114499.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] C. Fan, Y. Sun, F. Xiao, J. Ma, D. Lee, J. Wang, Y. C. Tseng, 基于迁移学习的方法在短期建筑能源预测中的统计研究,
    应用能源 262 (2020) 114499。'
- en: '[291] J. Lin, J. Ma, J. Zhu, H. Liang, Deep domain adaptation for non-intrusive
    load monitoring based on a knowledge transfer learning network, IEEE Transactions
    on Smart Grid (2021).'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] J. Lin, J. Ma, J. Zhu, H. Liang, 基于知识转移学习网络的非侵入式负荷监测的深度领域适应, IEEE 智能电网汇刊
    (2021)。'
- en: '[292] N. Patricia, B. Caputo, Learning to learn, from transfer learning to
    domain adaptation: A unifying perspective, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, IEEE, Columbus, OH, USA, 2014, pp.
    1442–1449.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] N. Patricia, B. Caputo, 学会学习，从迁移学习到领域适应：一个统一的视角, 载于：IEEE计算机视觉与模式识别会议论文集,
    IEEE, 哥伦布, OH, USA, 2014年，第1442–1449页。'
- en: '[293] N. Djeffal, H. Kheddar, D. Addou, A. C. Mazari, Y. Himeur, Automatic
    speech recognition with bert and ctc transformers: A review, in: 2023 2nd International
    Conference on Electronics, Energy and Measurement (IC2EM), Vol. 1, IEEE, 2023,
    pp. 1–8.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] N. Djeffal, H. Kheddar, D. Addou, A. C. Mazari, Y. Himeur, 基于 BERT 和
    CTC 变换器的自动语音识别：综述, 载于：2023年第2届国际电子、能源与测量会议（IC2EM），第1卷，IEEE，2023年，第1–8页。'
- en: '[294] G. Qian, X. Zhang, A. Hamdi, B. Ghanem, Pix4point: Image pretrained transformers
    for 3d point cloud understanding, arXiv preprint arXiv:2208.12259 (2022).'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] G. Qian, X. Zhang, A. Hamdi, B. Ghanem, Pix4Point: 用于 3D 点云理解的图像预训练变换器,
    arXiv 预印本 arXiv:2208.12259 (2022)。'
- en: '[295] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep
    bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805
    (2018).'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: 深度双向变换器预训练用于语言理解,
    arXiv 预印本 arXiv:1810.04805 (2018)。'
- en: '[296] Q.-H. Pham, T. Nguyen, B.-S. Hua, G. Roig, S.-K. Yeung, Jsis3d: Joint
    semantic-instance segmentation of 3d point clouds with multi-task pointwise networks
    and multi-value conditional random fields, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2019, pp. 8827–8836.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] Q.-H. Pham, T. Nguyen, B.-S. Hua, G. Roig, S.-K. Yeung, JSIS3D: 通过多任务点对点网络和多值条件随机场进行
    3D 点云的联合语义-实例分割, 载于：IEEE/CVF 计算机视觉与模式识别会议论文集, 2019年，第8827–8836页。'
- en: '[297] X. Zou, K. Li, Y. Li, W. Wei, C. Chen, Multi-task y-shaped graph neural
    network for point cloud learning in autonomous driving, IEEE Transactions on Intelligent
    Transportation Systems 23 (7) (2022) 9568–9579.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] X. Zou, K. Li, Y. Li, W. Wei, C. Chen, 用于自动驾驶中的点云学习的多任务 Y 形图神经网络, IEEE
    智能运输系统汇刊 23 (7) (2022) 9568–9579。'
- en: '[298] F. Chen, F. Wu, G. Gao, Y. Ji, J. Xu, G.-P. Jiang, X.-Y. Jing, Jspnet:
    Learning joint semantic & instance segmentation of point clouds via feature self-similarity
    and cross-task probability, Pattern Recognition 122 (2022) 108250.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] F. Chen, F. Wu, G. Gao, Y. Ji, J. Xu, G.-P. Jiang, X.-Y. Jing, JSPNet:
    通过特征自相似性和跨任务概率学习点云的联合语义与实例分割, 模式识别 122 (2022) 108250。'
- en: '[299] D. Ye, Z. Zhou, W. Chen, Y. Xie, Y. Wang, P. Wang, H. Foroosh, Lidarmultinet:
    Towards a unified multi-task network for lidar perception, in: Proceedings of
    the AAAI Conference on Artificial Intelligence, Vol. 37, 2023, pp. 3231–3240.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] D. Ye, Z. Zhou, W. Chen, Y. Xie, Y. Wang, P. Wang, H. Foroosh, LiDARMultiNet:
    迈向一个统一的多任务网络用于 LiDAR 感知, 载于：AAAI 人工智能会议论文集，第37卷, 2023年，第3231–3240页。'
- en: '[300] A. Dubey, A. Santra, J. Fuchs, M. Lübke, R. Weigel, F. Lurz, Haradnet:
    Anchor-free target detection for radar point clouds using hierarchical attention
    and multi-task learning, Machine Learning with Applications 8 (2022) 100275.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] A. Dubey, A. Santra, J. Fuchs, M. Lüke, R. Weigel, F. Lurz, Haradnet：使用分层注意力和多任务学习的无锚目标检测，用于雷达点云，机器学习应用
    8 (2022) 100275。'
- en: '[301] K. Hassani, M. Haley, Unsupervised multi-task feature learning on point
    clouds, in: Proceedings of the IEEE/CVF International Conference on Computer Vision,
    2019, pp. 8160–8171.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] K. Hassani, M. Haley, 点云上的无监督多任务特征学习，载于：2019年IEEE/CVF国际计算机视觉大会论文集，第8160–8171页。'
- en: '[302] X. Lin, H. Luo, W. Guo, C. Wang, J. Li, A multi-task learning framework
    for semantic segmentation in mls point clouds, in: International Conference on
    Adaptive and Intelligent Systems, Springer, 2022, pp. 382–392.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] X. Lin, H. Luo, W. Guo, C. Wang, J. Li, 用于MLS点云的语义分割的多任务学习框架，载于：自适应与智能系统国际会议，Springer，2022，第382–392页。'
- en: '[303] L. Zhao, Y. Hu, X. Yang, Z. Dou, L. Kang, Robust multi-task learning
    network for complex lidar point cloud data preprocessing, Expert Systems with
    Applications 237 (2024) 121552.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] L. Zhao, Y. Hu, X. Yang, Z. Dou, L. Kang, 复杂激光雷达点云数据预处理的鲁棒多任务学习网络，专家系统应用
    237 (2024) 121552。'
- en: '[304] T. Rios, B. van Stein, T. Bäck, B. Sendhoff, S. Menzel, Multitask shape
    optimization using a 3-d point cloud autoencoder as unified representation, IEEE
    Transactions on Evolutionary Computation 26 (2) (2021) 206–217.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] T. Rios, B. van Stein, T. Bäck, B. Sendhoff, S. Menzel, 使用3D点云自动编码器作为统一表示的多任务形状优化，IEEE进化计算汇刊
    26 (2) (2021) 206–217。'
- en: '[305] D. Feng, Y. Zhou, C. Xu, M. Tomizuka, W. Zhan, A simple and efficient
    multi-task network for 3d object detection and road understanding, in: 2021 IEEE/RSJ
    International Conference on Intelligent Robots and Systems (IROS), IEEE, 2021,
    pp. 7067–7074.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] D. Feng, Y. Zhou, C. Xu, M. Tomizuka, W. Zhan, 一种简单高效的多任务网络用于3D物体检测和道路理解，载于：2021
    IEEE/RSJ国际智能机器人与系统大会（IROS），IEEE，2021，第7067–7074页。'
- en: '[306] J. Rebut, A. Ouaknine, W. Malik, P. Pérez, Raw high-definition radar
    for multi-task learning, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2022, pp. 17021–17030.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] J. Rebut, A. Ouaknine, W. Malik, P. Pérez, 原始高清雷达用于多任务学习，载于：2022年IEEE/CVF计算机视觉与模式识别大会论文集，第17021–17030页。'
- en: '[307] Z. Shan, Q. Yang, R. Ye, Y. Zhang, Y. Xu, X. Xu, S. Liu, Gpa-net: No-reference
    point cloud quality assessment with multi-task graph convolutional network, IEEE
    Transactions on Visualization and Computer Graphics (2023).'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] Z. Shan, Q. Yang, R. Ye, Y. Zhang, Y. Xu, X. Xu, S. Liu, Gpa-net：无参考点云质量评估的多任务图卷积网络，IEEE可视化与计算机图形学汇刊（2023）。'
- en: '[308] A. Hatem, Y. Qian, Y. Wang, Point-tta: Test-time adaptation for point
    cloud registration using multitask meta-auxiliary learning, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, 2023, pp. 16494–16504.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] A. Hatem, Y. Qian, Y. Wang, Point-tta：用于点云配准的测试时适应的多任务元辅助学习，载于：2023年IEEE/CVF国际计算机视觉大会论文集，第16494–16504页。'
- en: '[309] C. Zhang, H. Fan, An improved multi-task pointwise network for segmentation
    of building roofs in airborne laser scanning point clouds, The Photogrammetric
    Record 37 (179) (2022) 260–284.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] C. Zhang, H. Fan, 改进的多任务点云网络用于空气激光扫描点云中的建筑屋顶分割，摄影测量记录 37 (179) (2022)
    260–284。'
- en: '[310] G. Wei, L. Ma, C. Wang, C. Desrosiers, Y. Zhou, Multi-task joint learning
    of 3d keypoint saliency and correspondence estimation, Computer-Aided Design 141
    (2021) 103105.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] G. Wei, L. Ma, C. Wang, C. Desrosiers, Y. Zhou, 3D关键点显著性与对应估计的多任务联合学习，计算机辅助设计
    141 (2021) 103105。'
- en: '[311] H. Zhang, S. Zhang, Y. Zhang, J. Liang, Z. Wang, Machining feature recognition
    based on a novel multi-task deep learning network, Robotics and Computer-Integrated
    Manufacturing 77 (2022) 102369.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] H. Zhang, S. Zhang, Y. Zhang, J. Liang, Z. Wang, 基于新型多任务深度学习网络的加工特征识别，机器人与计算机集成制造
    77 (2022) 102369。'
- en: '[312] M. Afham, I. Dissanayake, D. Dissanayake, A. Dharmasiri, K. Thilakarathna,
    R. Rodrigo, Crosspoint: Self-supervised cross-modal contrastive learning for 3d
    point cloud understanding, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2022, pp. 9902–9912.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] M. Afham, I. Dissanayake, D. Dissanayake, A. Dharmasiri, K. Thilakarathna,
    R. Rodrigo, Crosspoint：自监督跨模态对比学习用于3D点云理解，载于：2022年IEEE/CVF计算机视觉与模式识别大会论文集，第9902–9912页。'
- en: '[313] X. Yan, H. Zhan, C. Zheng, J. Gao, R. Zhang, S. Cui, Z. Li, Let images
    give you more: Point cloud cross-modal training for shape analysis, Advances in
    Neural Information Processing Systems 35 (2022) 32398–32411.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] X. Yan, H. Zhan, C. Zheng, J. Gao, R. Zhang, S. Cui, Z. Li, 让图像为你提供更多：用于形状分析的点云跨模态训练，《神经信息处理系统进展》35
    (2022) 32398–32411。'
- en: '[314] Y. Wu, J. Liu, M. Gong, P. Gong, X. Fan, A. Qin, Q. Miao, W. Ma, Self-supervised
    intra-modal and cross-modal contrastive learning for point cloud understanding,
    IEEE Transactions on Multimedia (2023).'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] Y. Wu, J. Liu, M. Gong, P. Gong, X. Fan, A. Qin, Q. Miao, W. Ma, 自监督的模态内和跨模态对比学习用于点云理解，《IEEE
    多媒体学报》 (2023)。'
- en: '[315] J. Zhang, H. Yang, D.-J. Wu, J. Keung, X. Li, X. Zhu, Y. Ma, Cross-modal
    and cross-domain knowledge transfer for label-free 3d segmentation, in: Chinese
    Conference on Pattern Recognition and Computer Vision (PRCV), Springer, 2023,
    pp. 465–477.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] J. Zhang, H. Yang, D.-J. Wu, J. Keung, X. Li, X. Zhu, Y. Ma, 无标签的 3D
    分割中的跨模态和跨领域知识迁移，见：中国模式识别与计算机视觉大会 (PRCV)，Springer，2023，页 465–477。'
- en: '[316] L. Jing, Y. Xue, X. Yan, C. Zheng, D. Wang, R. Zhang, Z. Wang, H. Fang,
    B. Zhao, Z. Li, X4d-sceneformer: Enhanced scene understanding on 4d point cloud
    videos through cross-modal knowledge transfer, in: Proceedings of the AAAI Conference
    on Artificial Intelligence, Vol. 38, 2024, pp. 2670–2678.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] L. Jing, Y. Xue, X. Yan, C. Zheng, D. Wang, R. Zhang, Z. Wang, H. Fang,
    B. Zhao, Z. Li, X4D-SceneFormer：通过跨模态知识迁移增强 4D 点云视频的场景理解，见：AAAI 人工智能会议论文集，第 38
    卷，2024，页 2670–2678。'
- en: '[317] Q. Zhang, J. Hou, Y. Qian, Pointmcd: Boosting deep point cloud encoders
    via multi-view cross-modal distillation for 3d shape recognition, IEEE Transactions
    on Multimedia (2023).'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] Q. Zhang, J. Hou, Y. Qian, PointMCD：通过多视角跨模态蒸馏提升深度点云编码器用于 3D 形状识别，《IEEE
    多媒体学报》 (2023)。'
- en: '[318] P. Falco, S. Lu, C. Natale, S. Pirozzi, D. Lee, A transfer learning approach
    to cross-modal object recognition: from visual observation to robotic haptic exploration,
    IEEE Transactions on Robotics 35 (4) (2019) 987–998.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[318] P. Falco, S. Lu, C. Natale, S. Pirozzi, D. Lee, 一种跨模态对象识别的迁移学习方法：从视觉观察到机器人触觉探索，《IEEE
    机器人学报》35 (4) (2019) 987–998。'
- en: '[319] P. K. Murali, C. Wang, D. Lee, R. Dahiya, M. Kaboli, Deep active cross-modal
    visuo-tactile transfer learning for robotic object recognition, IEEE Robotics
    and Automation Letters 7 (4) (2022) 9557–9564.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[319] P. K. Murali, C. Wang, D. Lee, R. Dahiya, M. Kaboli, 深度主动跨模态视觉-触觉迁移学习用于机器人对象识别，《IEEE
    机器人与自动化快报》7 (4) (2022) 9557–9564。'
- en: '[320] X. Shen, I. Stamos, Simcrosstrans: A simple cross-modality transfer learning
    for object detection with convnets or vision transformers, arXiv preprint arXiv:2203.10456
    (2022).'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[320] X. Shen, I. Stamos, SimCrossTrans：一种简单的跨模态迁移学习方法，用于使用卷积网络或视觉变换器进行对象检测，arXiv
    预印本 arXiv:2203.10456 (2022)。'
- en: '[321] L. Jing, E. Vahdani, J. Tan, Y. Tian, Cross-modal center loss for 3d
    cross-modal retrieval, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2021, pp. 3142–3151.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[321] L. Jing, E. Vahdani, J. Tan, Y. Tian, 3D 跨模态检索的跨模态中心损失，见：IEEE/CVF 计算机视觉与模式识别会议论文集，2021，页
    3142–3151。'
- en: '[322] Z. Zhu, L. Nan, H. Xie, H. Chen, J. Wang, M. Wei, J. Qin, Csdn: Cross-modal
    shape-transfer dual-refinement network for point cloud completion, IEEE Transactions
    on Visualization and Computer Graphics (2023).'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[322] Z. Zhu, L. Nan, H. Xie, H. Chen, J. Wang, M. Wei, J. Qin, CSDN：用于点云完成的跨模态形状转换双重细化网络，《IEEE
    可视化与计算机图形学学报》 (2023)。'
- en: '[323] M. Li, Y. Zhang, Y. Xie, Z. Gao, C. Li, Z. Zhang, Y. Qu, Cross-domain
    and cross-modal knowledge distillation in domain adaptation for 3d semantic segmentation,
    in: Proceedings of the 30th ACM International Conference on Multimedia, 2022,
    pp. 3829–3837.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[323] M. Li, Y. Zhang, Y. Xie, Z. Gao, C. Li, Z. Zhang, Y. Qu, 领域适应中的跨领域和跨模态知识蒸馏，用于
    3D 语义分割，见：第 30 届 ACM 国际多媒体会议论文集，2022，页 3829–3837。'
- en: '[324] D. Peng, Y. Lei, W. Li, P. Zhang, Y. Guo, Sparse-to-dense feature matching:
    Intra and inter domain cross-modal learning in domain adaptation for 3d semantic
    segmentation, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, 2021, pp. 7108–7117.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[324] D. Peng, Y. Lei, W. Li, P. Zhang, Y. Guo, 稀疏到密集特征匹配：领域适应中的模态内和跨领域跨模态学习，用于
    3D 语义分割，见：IEEE/CVF 国际计算机视觉会议论文集，2021，页 7108–7117。'
- en: '[325] J. Nitsch, J. Nieto, R. Siegwart, M. Schmidt, C. Cadena, Learning common
    and transferable feature representations for multi-modal data, in: 2020 IEEE Intelligent
    Vehicles Symposium (IV), IEEE, 2020, pp. 1601–1607.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[325] J. Nitsch, J. Nieto, R. Siegwart, M. Schmidt, C. Cadena, 学习多模态数据的共同和可转移特征表示，见：2020
    IEEE 智能车辆研讨会 (IV)，IEEE，2020，页 1601–1607。'
- en: '[326] M. Li, Y. Zhang, X. Ma, Y. Qu, Y. Fu, Bev-dg: Cross-modal learning under
    bird’s-eye view for domain generalization of 3d semantic segmentation, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 11632–11642.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[326] M. Li, Y. Zhang, X. Ma, Y. Qu, Y. Fu, Bev-dg：基于鸟瞰视角的跨模态学习用于3D语义分割的领域泛化，载于：IEEE/CVF国际计算机视觉大会论文集，2023年，第11632–11642页。'
- en: '[327] P. Tang, H.-M. Xu, C. Ma, Prototransfer: Cross-modal prototype transfer
    for point cloud segmentation, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2023, pp. 3337–3347.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[327] P. Tang, H.-M. Xu, C. Ma, Prototransfer：点云分割的跨模态原型转移，载于：IEEE/CVF国际计算机视觉大会论文集，2023年，第3337–3347页。'
- en: '[328] B. Xing, X. Ying, R. Wang, J. Yang, T. Chen, Cross-modal contrastive
    learning for domain adaptation in 3d semantic segmentation, in: Proceedings of
    the AAAI Conference on Artificial Intelligence, Vol. 37, 2023, pp. 2974–2982.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[328] B. Xing, X. Ying, R. Wang, J. Yang, T. Chen, 用于3D语义分割的领域适应的跨模态对比学习，载于：AAAI人工智能会议论文集，第37卷，2023年，第2974–2982页。'
- en: '[329] Z. Yuan, X. Yan, Y. Liao, Y. Guo, G. Li, S. Cui, Z. Li, X-trans2cap:
    Cross-modal knowledge transfer using transformer for 3d dense captioning, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2022, pp. 8563–8573.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[329] Z. Yuan, X. Yan, Y. Liao, Y. Guo, G. Li, S. Cui, Z. Li, X-trans2cap：使用变换器进行3D密集标注的跨模态知识转移，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第8563–8573页。'
- en: '[330] H. Zhou, X. Peng, Y. Luo, Z. Wu, Pointcmc: Cross-modal multi-scale correspondences
    learning for point cloud understanding (2023).'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[330] H. Zhou, X. Peng, Y. Luo, Z. Wu, Pointcmc：用于点云理解的跨模态多尺度对应学习（2023年）。'
- en: '[331] X. Zheng, X. Huang, G. Mei, Y. Hou, Z. Lyu, B. Dai, W. Ouyang, Y. Gong,
    Point cloud pre-training with diffusion models, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2024, pp. 22935–22945.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[331] X. Zheng, X. Huang, G. Mei, Y. Hou, Z. Lyu, B. Dai, W. Ouyang, Y. Gong,
    基于扩散模型的点云预训练，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2024年，第22935–22945页。'
- en: '[332] Y. Kasten, O. Rahamim, G. Chechik, Point cloud completion with pretrained
    text-to-image diffusion models, Advances in Neural Information Processing Systems
    36 (2024).'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[332] Y. Kasten, O. Rahamim, G. Chechik, 使用预训练的文本到图像扩散模型进行点云补全，《神经信息处理系统进展》36（2024年）。'
- en: '[333] C. Liu, A. Jiang, Y. Tang, Y. Zhu, Q. Chen, 3d point cloud semantic segmentation
    based on diffusion model, in: ICASSP 2024-2024 IEEE International Conference on
    Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2024, pp. 4375–4379.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[333] C. Liu, A. Jiang, Y. Tang, Y. Zhu, Q. Chen, 基于扩散模型的3D点云语义分割，载于：ICASSP
    2024-2024 IEEE国际声学、语音与信号处理会议（ICASSP），IEEE，2024年，第4375–4379页。'
- en: '[334] H. Jiang, M. Salzmann, Z. Dang, J. Xie, J. Yang, Se (3) diffusion model-based
    point cloud registration for robust 6d object pose estimation, Advances in Neural
    Information Processing Systems 36 (2024).'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[334] H. Jiang, M. Salzmann, Z. Dang, J. Xie, J. Yang, 基于 Se (3) 扩散模型的点云配准，用于鲁棒的6D物体姿态估计，《神经信息处理系统进展》36（2024年）。'
- en: '[335] S. Jin, I. Armeni, M. Pollefeys, D. Barath, Multiway point cloud mosaicking
    with diffusion and global optimization, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2024, pp. 20838–20849.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[335] S. Jin, I. Armeni, M. Pollefeys, D. Barath, 通过扩散和全局优化进行多途径点云拼接，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2024年，第20838–20849页。'
- en: '[336] Y. Feng, X. Shi, M. Cheng, Y. Xiong, Diffpoint: Single and multi-view
    point cloud reconstruction with vit based diffusion model, arXiv preprint arXiv:2402.11241
    (2024).'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[336] Y. Feng, X. Shi, M. Cheng, Y. Xiong, Diffpoint：基于ViT的扩散模型进行单视图和多视图点云重建，arXiv预印本
    arXiv:2402.11241（2024年）。'
- en: '[337] G. Sharma, C. Gupta, A. Agarwal, L. Sharma, A. Dhall, Generating point
    cloud augmentations via class-conditioned diffusion model, in: Proceedings of
    the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024, pp. 480–488.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[337] G. Sharma, C. Gupta, A. Agarwal, L. Sharma, A. Dhall, 通过类别条件扩散模型生成点云增强，载于：IEEE/CVF冬季计算机视觉应用大会论文集，2024年，第480–488页。'
- en: '[338] S. Mo, E. Xie, R. Chu, L. Hong, M. Niessner, Z. Li, Dit-3d: Exploring
    plain diffusion transformers for 3d shape generation, Advances in Neural Information
    Processing Systems 36 (2024).'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[338] S. Mo, E. Xie, R. Chu, L. Hong, M. Niessner, Z. Li, Dit-3d：探索用于3D形状生成的普通扩散变换器，《神经信息处理系统进展》36（2024年）。'
- en: '[339] T. Yi, J. Fang, J. Wang, G. Wu, L. Xie, X. Zhang, W. Liu, Q. Tian, X. Wang,
    Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and
    3d diffusion models, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2024, pp. 6796–6807.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[339] T. Yi, J. Fang, J. Wang, G. Wu, L. Xie, X. Zhang, W. Liu, Q. Tian, X.
    Wang, Gaussiandreamer: 通过桥接2d和3d扩散模型从文本快速生成3d高斯，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2024，第6796–6807页。'
- en: '[340] C.-J. Ho, C.-H. Tai, Y.-Y. Lin, M.-H. Yang, Y.-H. Tsai, Diffusion-ss3d:
    Diffusion model for semi-supervised 3d object detection, Advances in Neural Information
    Processing Systems 36 (2024).'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[340] C.-J. Ho, C.-H. Tai, Y.-Y. Lin, M.-H. Yang, Y.-H. Tsai, Diffusion-ss3d:
    用于半监督3d物体检测的扩散模型，《神经信息处理系统进展》36 (2024)。'
- en: '[341] M. Ohno, R. Ukyo, T. Amano, H. Rizk, H. Yamaguchi, Privacy-preserving
    pedestrian tracking with path image inpainting and 3d point cloud features, Pervasive
    and Mobile Computing 100 (2024) 101914.'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[341] M. Ohno, R. Ukyo, T. Amano, H. Rizk, H. Yamaguchi, 通过路径图像修补和3d点云特征进行隐私保护的行人追踪，《普适计算与移动计算》100
    (2024) 101914。'
- en: '[342] X. Bi, Y. Chen, L. Li, Diffusionemis: Diffusion model for 3d electromagnetic
    inverse scattering, IEEE Transactions on Geoscience and Remote Sensing (2024).'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[342] X. Bi, Y. Chen, L. Li, Diffusionemis: 用于3d电磁反向散射的扩散模型，《IEEE地球科学与遥感学报》
    (2024)。'
- en: '[343] N. S. Dutt, S. Muralikrishnan, N. J. Mitra, Diffusion 3d features (diff3f):
    Decorating untextured shapes with distilled semantic features, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp.
    4494–4504.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[343] N. S. Dutt, S. Muralikrishnan, N. J. Mitra, 扩散3d特征（diff3f）：用蒸馏语义特征装饰无纹理形状，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2024，第4494–4504页。'
- en: '[344] Z. Li, R. Mrad, R. Jiao, G. Huang, J. Shan, S. Chu, Y. Chen, Generative
    design of crystal structures by point cloud representations and diffusion model,
    arXiv preprint arXiv:2401.13192 (2024).'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[344] Z. Li, R. Mrad, R. Jiao, G. Huang, J. Shan, S. Chu, Y. Chen, 通过点云表示和扩散模型的晶体结构生成设计，arXiv预印本
    arXiv:2401.13192 (2024)。'
- en: '[345] Y. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, H. Xu, 3d diffusion policy,
    arXiv preprint arXiv:2403.03954 (2024).'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[345] Y. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, H. Xu, 3d扩散策略，arXiv预印本 arXiv:2403.03954
    (2024)。'
- en: '[346] R. She, Q. Kang, S. Wang, W. P. Tay, K. Zhao, Y. Song, T. Geng, Y. Xu,
    D. N. Navarro, A. Hartmannsgruber, Pointdifformer: Robust point cloud registration
    with neural diffusion and transformer, IEEE Transactions on Geoscience and Remote
    Sensing (2024).'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[346] R. She, Q. Kang, S. Wang, W. P. Tay, K. Zhao, Y. Song, T. Geng, Y. Xu,
    D. N. Navarro, A. Hartmannsgruber, Pointdifformer: 使用神经扩散和变换器的鲁棒点云配准，《IEEE地球科学与遥感学报》
    (2024)。'
- en: '[347] B. Li, X. Wei, B. Liu, W. Wang, Z.-F. He, Y.-K. Lai, 3d colored object
    reconstruction from a single view image through diffusion, Expert Systems with
    Applications 252 (2024) 124225.'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[347] B. Li, X. Wei, B. Liu, W. Wang, Z.-F. He, Y.-K. Lai, 从单视图图像通过扩散进行3d彩色物体重建，《专家系统与应用》252
    (2024) 124225。'
- en: '[348] Z. Chen, Y. Wang, F. Wang, Z. Wang, H. Liu, V3d: Video diffusion models
    are effective 3d generators, arXiv preprint arXiv:2403.06738 (2024).'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[348] Z. Chen, Y. Wang, F. Wang, Z. Wang, H. Liu, V3d: 视频扩散模型是有效的3d生成器，arXiv预印本
    arXiv:2403.06738 (2024)。'
- en: '[349] J. Hu, B. Fei, B. Xu, F. Hou, W. Yang, S. Wang, N. Lei, C. Qian, Y. He,
    Topology-aware latent diffusion for 3d shape generation, arXiv preprint arXiv:2401.17603
    (2024).'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[349] J. Hu, B. Fei, B. Xu, F. Hou, W. Yang, S. Wang, N. Lei, C. Qian, Y. He,
    拓扑感知的潜在扩散用于3d形状生成，arXiv预印本 arXiv:2401.17603 (2024)。'
- en: '[350] Y. Dong, Q. Zuo, X. Gu, W. Yuan, Z. Zhao, Z. Dong, L. Bo, Q. Huang, Gpld3d:
    Latent diffusion of 3d shape generative models by enforcing geometric and physical
    priors, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2024, pp. 56–66.'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[350] Y. Dong, Q. Zuo, X. Gu, W. Yuan, Z. Zhao, Z. Dong, L. Bo, Q. Huang, Gpld3d:
    通过强制几何和物理先验的3d形状生成模型的潜在扩散，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2024，第56–66页。'
