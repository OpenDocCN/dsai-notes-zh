- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:33:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:33:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2404.17070] Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2404.17070] 双足步态的深度强化学习：简要调研'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17070](https://ar5iv.labs.arxiv.org/html/2404.17070)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17070](https://ar5iv.labs.arxiv.org/html/2404.17070)
- en: 'Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双足步态的深度强化学习：简要调研
- en: Lingfan Bao¹, Joseph Humphreys¹, Tianhu Peng¹ and Chengxu Zhou² This work was
    supported by the Royal Society [grant number RG\R2\232409].¹School of Mechanical
    Engineering, University of Leeds, UK. {mnlb, el20jeh, mntp}@leeds.ac.uk²Department
    of Computer Science, University College London, UK. chengxu.zhou@ucl.ac.uk
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Lingfan Bao¹, Joseph Humphreys¹, Tianhu Peng¹ 和 Chengxu Zhou² 本工作得到了皇家学会[资助编号
    RG\R2\232409]的支持。¹利兹大学机械工程学院，英国。 {mnlb, el20jeh, mntp}@leeds.ac.uk²伦敦大学学院计算机科学系，英国。
    chengxu.zhou@ucl.ac.uk
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Bipedal robots are garnering increasing global attention due to their potential
    applications and advancements in artificial intelligence, particularly in Deep
    Reinforcement Learning (DRL). While DRL has driven significant progress in bipedal
    locomotion, developing a comprehensive and unified framework capable of adeptly
    performing a wide range of tasks remains a challenge. This survey systematically
    categorizes, compares, and summarizes existing DRL frameworks for bipedal locomotion,
    organizing them into end-to-end and hierarchical control schemes. End-to-end frameworks
    are assessed based on their learning approaches, whereas hierarchical frameworks
    are dissected into layers that utilize either learning-based methods or traditional
    model-based approaches. This survey provides a detailed analysis of the composition,
    capabilities, strengths, and limitations of each framework type. Furthermore,
    we identify critical research gaps and propose future directions aimed at achieving
    a more integrated and efficient framework for bipedal locomotion, with potential
    broad applications in everyday life.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 双足机器人由于其潜在应用和人工智能方面的进展，特别是在深度强化学习（DRL）领域，正在引起全球越来越多的关注。虽然DRL在双足步态上推动了显著进展，但开发一个能够熟练执行广泛任务的全面统一框架仍然是一个挑战。本调研系统地分类、比较和总结了现有的DRL框架，将它们组织为端到端和层次控制方案。端到端框架根据其学习方法进行评估，而层次框架则被分解为利用学习方法或传统模型方法的层次。本调研详细分析了每种框架类型的组成、能力、优点和局限性。此外，我们识别了关键的研究空白，并提出了未来的研究方向，旨在实现一个更集成和高效的双足步态框架，具有在日常生活中的广泛应用潜力。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep Reinforcement Learning, Humanoid Robots, Bipedal Locomotion, Legged Robots
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习，类人机器人，双足步态，步行机器人
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: 'Humans navigate complex and varied environments, performing diverse locomotion
    tasks with only two legs. To facilitate dynamic bipedal locomotion, model-based
    methods were introduced in the 1980s and have since evolved significantly [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)]. These methods, characterized by rapid convergence,
    furnish a predictive framework for understanding environmental structures. However,
    they struggle to adapt in dynamically challenging environments that are difficult
    to model precisely. More recently, advancements in machine learning have provided
    novel approaches. Reinforcement learning (RL)-based methods, in particular, are
    adept at navigating the full dynamics of robot-environment interactions [[4](#bib.bib4)].
    Additionally, hybrid approaches that combine model-based and learning-based methods
    have been developed to leverage the advantages of both. Yet, the question remains:
    Is there a unified framework capable of enabling bipedal robots to effectively
    manage a diverse range of locomotion tasks?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在复杂多变的环境中行走，利用仅有的两条腿完成各种步态任务。为了促进动态双足步态的发展，20世纪80年代引入了基于模型的方法，并且这些方法自那时以来有了显著的发展[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)]。这些方法以快速收敛为特征，为理解环境结构提供了预测框架。然而，它们在动态变化且难以准确建模的环境中适应能力不足。最近，机器学习的进展提供了新的方法。特别是基于强化学习（RL）的方法，能够有效处理机器人与环境互动的完整动态[[4](#bib.bib4)]。此外，结合基于模型和基于学习的方法的混合方法也被开发出来，以利用两者的优势。然而，问题依然存在：是否存在一个统一的框架，能够使双足机器人有效管理多样化的步态任务？
- en: '![Refer to caption](img/54a304784fd47ae3543b62b61a8b7515.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/54a304784fd47ae3543b62b61a8b7515.png)'
- en: 'Figure 1: Common bipedal and humanoid robots used as platforms for testing
    DRL frameworks. (a) NAO, a toy-like 3D humanoid robot, actuated by servo motors
    [[5](#bib.bib5)]. (b) Rabbit, a 2D bipedal robot, actuated by torque control [[6](#bib.bib6)].
    (c) Cassie, a 3D bipedal robot, also actuated by torque control [[7](#bib.bib7)].
    (d) ATLAS, a 3D humanoid robot, driven by hydraulics [[8](#bib.bib8)]. (e) Digit,
    a full human-sized 3D humanoid robot, an upgrade based on Cassie and actuated
    by torque control [[9](#bib.bib9)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：用于测试DRL框架的常见双足和类人机器人。(a) NAO，一种玩具般的3D类人机器人，由伺服电机驱动[[5](#bib.bib5)]。(b) Rabbit，一种2D双足机器人，由扭矩控制驱动[[6](#bib.bib6)]。(c)
    Cassie，一种3D双足机器人，也由扭矩控制驱动[[7](#bib.bib7)]。(d) ATLAS，一种3D类人机器人，由液压系统驱动[[8](#bib.bib8)]。(e)
    Digit，一种全人型的3D类人机器人，基于Cassie的升级版，由扭矩控制驱动[[9](#bib.bib9)]。
- en: 'To address this, we explore recent advancements in deep reinforcement learning
    (DRL)-based frameworks, categorizing control schemes into two primary types: (i) end-to-end
    and (ii) hierarchical. End-to-end frameworks map robot states directly to control
    outputs at the joint level, while hierarchical frameworks adopt a structured approach,
    decomposing decision-making into multiple layers. Here, a High-Level (HL) planner
    addresses navigation and path planning, while a Low-Level (LL) controller focuses
    on fundamental locomotion skills. The highest decision-making tier, the task level,
    receives direct input from task or user commands.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一问题，我们探索了基于深度强化学习（DRL）的框架的最新进展，将控制方案分为两种主要类型：（i）端到端和（ii）层次化。端到端框架将机器人状态直接映射到关节级别的控制输出，而层次化框架采用结构化的方法，将决策分解为多个层次。在这里，高层（HL）规划器负责导航和路径规划，而低层（LL）控制器则专注于基本的运动技能。最高决策层级，即任务层，直接接收来自任务或用户命令的输入。
- en: '![Refer to caption](img/d72beb9636fcc209c6563457ad5a915c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d72beb9636fcc209c6563457ad5a915c.png)'
- en: 'Figure 2: Classification of DRL-based control schemes.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于DRL的控制方案分类。
- en: 'The evolution of RL in bipedal robotics has spurred a dynamic growth in innovative
    applications. Although the application of RL to simple 2D bipedal robots began
    in 2004 [[10](#bib.bib10), [11](#bib.bib11)], it took several years before deep
    reinforcement learning (DRL) algorithms emerged. These DRL-based methods have
    since shown promising results in physical simulators [[12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14)]. Agility Robotics introduced the first sim-to-real end-to-end
    learning framework in 2019, which was applied on the 3D torque-controlled bipedal
    robot Cassie, as shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Reinforcement
    Learning for Bipedal Locomotion: A Brief Survey")(c) [[7](#bib.bib7)]. Besides
    model-based reference learning, the policy can also incorporate motion capture
    data [[15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18)],
    or start from scratch [[19](#bib.bib19)] to explore solutions freely. Recent studies
    demonstrate that end-to-end frameworks robustly handle complex and diverse tasks
    [[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)].'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '双足机器人中强化学习（RL）的演变推动了创新应用的动态增长。尽管将RL应用于简单的2D双足机器人始于2004年[[10](#bib.bib10), [11](#bib.bib11)]，但深度强化学习（DRL）算法的出现则需要几年时间。这些基于DRL的方法在物理模拟器中已显示出有前景的结果[[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)]。Agility Robotics于2019年推出了第一个从模拟到现实的端到端学习框架，该框架应用于3D扭矩控制双足机器人Cassie，如图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Deep Reinforcement Learning for Bipedal Locomotion:
    A Brief Survey")(c) [[7](#bib.bib7)]所示。除了基于模型的参考学习外，该策略还可以结合动作捕捉数据[[15](#bib.bib15),
    [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18)]，或者从头开始[[19](#bib.bib19)]自由探索解决方案。最新研究表明，端到端框架可以稳健地处理复杂且多样的任务[[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)]。'
- en: Similarly, hierarchical structures have garnered significant interest. Within
    this subset, the hybrid approach combines RL-based and model-based methods to
    enhance both planning and control strategies. A notable framework employs a learned
    High-Level (HL) planner coupled with a Low-Level (LL) model-based controller,
    often referred to as the Cascade-structure or Deep Planning Hybrid Scheme [[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25)]. Another innovative construction integrates
    a learned feedback controller with an HL planner, categorized under the DRL Feedback
    Control Hybrid Scheme [[26](#bib.bib26), [27](#bib.bib27)]. Additionally, a learned
    hierarchical control scheme [[28](#bib.bib28)] decomposes locomotion into various
    tasks, focusing each layer on specific functions such as navigation and fundamental
    locomotion skills [[12](#bib.bib12), [13](#bib.bib13), [29](#bib.bib29)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，层次结构也引起了显著关注。在这一子集内，混合方法结合了基于RL和模型的方法，以增强规划和控制策略。一个显著的框架采用了一个学习的高层（HL）规划器与一个低层（LL）基于模型的控制器，通常称为级联结构或深度规划混合方案[[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25)]。另一种创新结构将学习的反馈控制器与HL规划器整合，属于DRL反馈控制混合方案[[26](#bib.bib26),
    [27](#bib.bib27)]。此外，一个学习的层次控制方案[[28](#bib.bib28)]将运动分解为各种任务，每一层专注于特定功能，如导航和基本运动技能[[12](#bib.bib12),
    [13](#bib.bib13), [29](#bib.bib29)]。
- en: 'While several review papers discuss RL for general robotics [[4](#bib.bib4)]
    and model-based methods for bipeds [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)],
    none specifically focus on DRL-based frameworks for bipeds. This survey aims to
    address this gap by summarizing current research progress, highlighting the structure
    and capabilities of bipedal locomotion frameworks, and exploring future directions.
    We also catalogue DRL-based frameworks, as depicted in Fig. [2](#S1.F2 "Figure
    2 ‣ I Introduction ‣ Deep Reinforcement Learning for Bipedal Locomotion: A Brief
    Survey"). The primary contributions of this survey are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有几篇综述论文讨论了用于一般机器人[RL](#bib.bib4)和双足机器人[模型基](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)的方法，但没有专门关注基于DRL的双足框架。本综述旨在填补这一空白，通过总结当前研究进展，突出双足运动框架的结构和能力，并探索未来方向。我们还对DRL-based框架进行了编目，如图[2](#S1.F2
    "Figure 2 ‣ I 引言 ‣ 双足运动的深度强化学习：简要综述")所示。本综述的主要贡献包括：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A comprehensive summary and cataloguing of DRL-based frameworks for bipedal
    locomotion.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对基于DRL的双足运动框架进行全面的总结和编目。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A detailed comparison of each control scheme, highlighting their strengths,
    limitations, and characteristics.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对每种控制方案进行详细比较，突出其优点、限制和特点。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The identification of current challenges and the provision of insightful future
    research directions.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前挑战的识别和对未来研究方向的深入洞察。
- en: 'The paper is organized as follows: Section [II](#S2 "II End-to-end framework
    ‣ Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey") focuses
    on end-to-end frameworks, categorized by learning approaches. Section [III](#S3
    "III Hierarchy framework ‣ Deep Reinforcement Learning for Bipedal Locomotion:
    A Brief Survey") details hierarchical frameworks, classified into three main types.
    Section [IV](#S4 "IV Challenges and future research directions ‣ Deep Reinforcement
    Learning for Bipedal Locomotion: A Brief Survey") addresses existing gaps, ongoing
    challenges, and potential future research directions. Finally, Section [V](#S5
    "V Conclusion ‣ Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey")
    concludes the paper.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 论文组织结构如下：第[II](#S2 "II 端到端框架 ‣ 双足运动的深度强化学习：简要综述")节重点讨论端到端框架，按学习方法分类。第[III](#S3
    "III 层次框架 ‣ 双足运动的深度强化学习：简要综述")节详细介绍层次框架，分为三种主要类型。第[IV](#S4 "IV 挑战与未来研究方向 ‣ 双足运动的深度强化学习：简要综述")节探讨现有的不足、持续的挑战和潜在的未来研究方向。最后，第[V](#S5
    "V 结论 ‣ 双足运动的深度强化学习：简要综述")节对论文进行总结。
- en: II End-to-end framework
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 端到端框架
- en: The end-to-end DRL framework represents a holistic approach where a single neural
    network (NN) policy, denoted $\pi(\cdot):\mathcal{X}\rightarrow\mathcal{U}$, directly
    translates sensory inputs—such as images, lidar data, or proprioceptive feedback
    [[30](#bib.bib30)]—along with user commands [[19](#bib.bib19)] or pre-defined
    references [[31](#bib.bib31)], into joint-level control actions. These actions
    encompass motor torques [[32](#bib.bib32)], positions, and velocities [[15](#bib.bib15)].
    This framework obviates the need for manually decomposing the problem into sub-tasks,
    streamlining the control process.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端的DRL框架代表了一种整体方法，其中单一的神经网络（NN）策略，记作$\pi(\cdot):\mathcal{X}\rightarrow\mathcal{U}$，直接将感官输入（如图像、激光雷达数据或本体感觉反馈[[30](#bib.bib30)]）、用户命令[[19](#bib.bib19)]或预定义参考[[31](#bib.bib31)]转换为关节级控制动作。这些动作包括电机扭矩[[32](#bib.bib32)]、位置和速度[[15](#bib.bib15)]。这一框架避免了手动将问题分解为子任务，简化了控制过程。
- en: 'End-to-end strategies primarily simplify the design of low-level tracking to
    basic elements, such as a Proportional-Derivative (PD) controller. These methods
    can be broadly categorized based on their reliance on prior knowledge into two
    types: reference-based and reference-free. The locomotion skills developed through
    these diverse learning approaches exhibit considerable variation in performance
    and adaptability.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端策略主要将低级跟踪设计简化为基本元素，如比例-微分（PD）控制器。这些方法根据其对先验知识的依赖可分为两类：基于参考和无参考。通过这些多样化学习方法开发的运动技能在性能和适应性上表现出显著差异。
- en: 'In the following sections, we will delve into various representation frameworks,
    exploring their characteristics, limitations, and strengths in comprehensive detail.
    To facilitate an understanding of these distinctions, Table [I](#S2.T1 "TABLE
    I ‣ II End-to-end framework ‣ Deep Reinforcement Learning for Bipedal Locomotion:
    A Brief Survey") provides a succinct overview of the frameworks discussed.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '在接下来的部分，我们将深入探讨各种表示框架，全面分析它们的特征、限制和优势。为了方便理解这些区别，表格[I](#S2.T1 "TABLE I ‣ II
    End-to-end framework ‣ Deep Reinforcement Learning for Bipedal Locomotion: A Brief
    Survey")提供了讨论框架的简明概述。'
- en: 'TABLE I: Summary and comparison of reference-based and reference-free Learning
    approaches for end-to-end framework. The dashed line in the implementation flow
    chat refers to optional.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：基于参考和无参考学习方法的总结与比较，用于端到端框架。实现流程图中的虚线表示可选部分。
- en: '| Methods | Works | Capabilities | Characteristic | Advantages and Disadvantages
    | Implementation Flow Chart |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 工作 | 能力 | 特征 | 优缺点 | 实现流程图 |'
- en: '| Residual learning | [[33](#bib.bib33)] [[34](#bib.bib34)] [[35](#bib.bib35)]
    | Forward walk undirectional walk Omni-walk | Add residential term to the known
    motor positions at the current time step. | A: Fast convergence speed D: Require
    high-quality predefined reference, limit to specific motions, and lack robustness
    to complicated terrains. | ![Refer to caption](img/1736ebb4cef6530d9431fc1cb895022c.png)
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 残差学习 | [[33](#bib.bib33)] [[34](#bib.bib34)] [[35](#bib.bib35)] | 前向行走 单向行走
    全向行走 | 在当前时间步将残差项添加到已知的运动位置。 | 优点：收敛速度快。 缺点：需要高质量的预定义参考，限制于特定运动，对复杂地形的鲁棒性差。 |
    ![参见说明](img/1736ebb4cef6530d9431fc1cb895022c.png) |'
- en: '| Guided learning | [[36](#bib.bib36)] [[31](#bib.bib31)] [[37](#bib.bib37)]
    [[22](#bib.bib22)] | Forward walk Versatile walk Versatile jump Versatile motions
    | Mimic the predefined reference and directly specifies joint-level command. |
    A: Accelerate the learning process and robust to the terrains. D: Limited to the
    predefined motions and lack adaptability to unforeseen changes in environment.
    | ![Refer to caption](img/40fad12c8ac5ed87901e4a0f47800f0e.png) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 指导学习 | [[36](#bib.bib36)] [[31](#bib.bib31)] [[37](#bib.bib37)] [[22](#bib.bib22)]
    | 前向行走 多功能行走 多功能跳跃 多功能运动 | 模仿预定义的参考，并直接指定关节级指令。 | 优点：加速学习过程，对地形具有鲁棒性。 缺点：仅限于预定义运动，对环境中的意外变化适应性差。
    | ![参见说明](img/40fad12c8ac5ed87901e4a0f47800f0e.png) |'
- en: '| Reference-free learning | [[19](#bib.bib19)] [[38](#bib.bib38)] [[20](#bib.bib20)]
    | Periodic motions Stepping stones Vision-based | Learn locomotion skills from
    scratch without any prior knowledge. | A: High potential for gait exploration,
    high robust to the complicated terrain. D: Requires intensive reward shaping for
    gait patterns and relatively expensive computational training resources. | ![Refer
    to caption](img/0ea10cb4fc4a8ae7da6fa5e59ae0ff3d.png) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 无参考学习 | [[19](#bib.bib19)] [[38](#bib.bib38)] [[20](#bib.bib20)] | 周期性运动
    垫脚石 基于视觉的 | 从零基础学习运动技能，无需任何先前知识。 | 优点：具有很高的步态探索潜力，对复杂地形具有很强的鲁棒性。缺点：需要对步态模式进行大量奖励调整，并且计算训练资源相对昂贵。
    | ![参见说明](img/0ea10cb4fc4a8ae7da6fa5e59ae0ff3d.png) |'
- en: '⁰⁰footnotetext: Forward Walk involves the bipeds walking straight ahead. Unidirectional
    Walk enables the bipeds to move both forward and backward within a range of desired
    velocities. Omni-Walk grants the bipeds the ability to walk in any direction.
    Versatile Walk allows the bipeds to walk forward, backward, turn, and move sideways,
    providing extensive movement capabilities. Periodic Motions entails the execution
    of various repeated gait patterns, such as walking, hopping, or galloping. Versatile
    Jump refers to jump towards different desired targets. Versatile Motions cover
    performing a broad array of motions, both periodic and aperiodic such as jumping.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ⁰⁰脚注：前向行走涉及双足直线前进。单向行走使双足可以在期望速度范围内前进和后退。全向行走赋予双足任何方向的行走能力。多功能行走允许双足前进、后退、转弯和侧移，提供广泛的运动能力。周期性运动涉及执行各种重复的步态模式，如行走、跳跃或奔跑。多功能跳跃指向不同期望目标跳跃。多功能运动包括执行各种动作，既包括周期性的也包括非周期性的，如跳跃。
- en: II-A Reference-based learning
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 基于参考的学习
- en: 'Reference-based learning utilizes prior knowledge, allowing the policy to develop
    locomotion skills by adhering to predefined references, which may be derived from
    trajectory optimization (TO) techniques or captured through motion capture systems.
    This method facilitates the acquisition of locomotion skills compared to alternative
    approaches, though it typically results in locomotion patterns that closely resemble
    the predefined references or motion clips, thus limiting the variety of gait patterns.
    Generally, this approach can be divided into two primary methods: (i) residual
    learning and (ii) guided learning.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 基于参考的学习利用先前的知识，允许策略通过遵循预定义的参考来发展运动技能，这些参考可能来自轨迹优化（TO）技术或通过运动捕捉系统捕获。这种方法相较于其他方法更容易获得运动技能，但通常会产生与预定义参考或运动剪辑非常相似的步态模式，从而限制了步态模式的多样性。一般来说，这种方法可以分为两种主要方法：（i）残差学习和（ii）指导学习。
- en: II-A1 Residual learning
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 残差学习
- en: This method involves a framework that is aware of the current reference joint
    positions and applies offsets determined by the policy to modify motor commands
    at the current timestep. By utilizing predefined motion trajectories, the residual
    term acts as feedback control, compensating for errors and enabling the biped
    to achieve dynamic locomotion skills.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法涉及一个能够感知当前参考关节位置的框架，并应用由策略确定的偏移量来修改当前时间步的运动指令。通过利用预定义的运动轨迹，残差项作为反馈控制，补偿误差，使双足能够实现动态运动技能。
- en: 'Introduced in 2018, a residual learning framework for the bipedal robot Cassie
    marked a significant advancement [[33](#bib.bib33)]. This framework allowed the
    robot to walk forward by incorporating a policy trained via Proximal Policy Optimization
    (PPO) algorithms, as detailed in Appendix [A](#A1 "Appendix A Deep reinforcement
    learning algorithms ‣ Deep Reinforcement Learning for Bipedal Locomotion: A Brief
    Survey"). The policy receives the robot’s states and reference inputs, outputting
    a residual term that augments the reference at the current timestep. These modified
    references are then processed by a Proportional-Derivative (PD) controller to
    set the desired joint positions. While this framework enhanced the robot’s ability
    to perform tasks beyond standing [[39](#bib.bib39)], its physical deployment on
    a bipedal robot has not yet occurred, potentially rendering it impractical for
    managing walking at varying speeds and limiting movement to a single direction.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年引入的用于双足机器人Cassie的残差学习框架标志着重要的进展 [[33](#bib.bib33)]。该框架通过结合使用Proximal Policy
    Optimization (PPO) 算法训练的策略使机器人能够向前行走，具体细节见附录 [A](#A1 "附录 A 深度强化学习算法 ‣ 双足行走的深度强化学习简要调查")。策略接收机器人的状态和参考输入，输出一个残差项，以在当前时间步增强参考。这些修改后的参考由比例-导数
    (PD) 控制器处理，以设置所需的关节位置。虽然该框架增强了机器人执行超出站立的任务的能力 [[39](#bib.bib39)]，但其在双足机器人上的实际应用尚未发生，可能导致在不同速度下行走的管理不切实际，并且运动仅限于单一方向。
- en: 'To transition this framework to a real robot, a sim-to-real strategy based
    on the previous model was demonstrated, where the policy, trained through a residual
    learning approach, was subsequently applied on a physical bipedal robot [[34](#bib.bib34)].
    This process and its key points are further explored in Appendix [B](#A2 "Appendix
    B Bridging sim-to-real gap ‣ Deep Reinforcement Learning for Bipedal Locomotion:
    A Brief Survey"). Compared to model-based methods, this training policy achieves
    faster running speeds on the same platform, underlining the considerable potential
    of DRL-based frameworks. However, the robot’s movements remain constrained to
    merely walking forward or backward. A novel approach in residual learning was
    introduced to enable unidirectional walking, where the policy outputs a residual
    term added to the current positional states, facilitating gradual omnidirectional
    walking [[35](#bib.bib35)].'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将该框架过渡到实际机器人上，展示了基于之前模型的模拟到实际策略，其中通过残差学习方法训练的策略随后应用于实际双足机器人 [[34](#bib.bib34)]。这一过程及其关键点在附录
    [B](#A2 "附录 B 模拟到实际的桥接 ‣ 双足行走的深度强化学习简要调查")中进一步探讨。与基于模型的方法相比，该训练策略在同一平台上实现了更快的运行速度，突显了DRL框架的巨大潜力。然而，机器人的运动仍然仅限于向前或向后行走。一种新的残差学习方法被引入以实现单向行走，其中策略输出一个附加到当前位置信态的残差项，从而促进渐进的全向行走
    [[35](#bib.bib35)]。
- en: II-A2 Guided learning
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 指导学习
- en: Guided learning trains policies to directly output the desired joint-level commands,
    eschewing the addition of a residual term. The reward structure in this approach
    is focused on closely imitating predefined references.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 指导学习训练策略直接输出所需的关节级命令，避免了附加残差项。这种方法中的奖励结构集中于紧密模仿预定义的参考。
- en: 'A sim-to-real framework that employs periodic references to initiate the training
    phase was proposed in [[36](#bib.bib36)]. In this framework, the action space
    directly maps to the joint angles, and desired joint positions are managed by
    joint PD controllers. The framework also incorporates a Long Short-Term Memory
    (LSTM) network, as detailed in Appendix [A](#A1 "Appendix A Deep reinforcement
    learning algorithms ‣ Deep Reinforcement Learning for Bipedal Locomotion: A Brief
    Survey"), which is synchronised with periodic time inputs. However, this model
    is limited to a single locomotion goal: forward walking. A more diverse and robust
    walking DRL framework that includes a Hybrid Zero Dynamics (HZD) gait library
    was demonstrated [[31](#bib.bib31)], achieving a significant advancement by enabling
    a single end-to-end policy to facilitate walking, turning, and squatting.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '提出了一个使用周期性参考来启动训练阶段的模拟到现实框架[[36](#bib.bib36)]。在该框架中，动作空间直接映射到关节角度，期望的关节位置由关节PD控制器管理。该框架还结合了一个长期短期记忆（LSTM）网络，如附录[A](#A1
    "Appendix A Deep reinforcement learning algorithms ‣ Deep Reinforcement Learning
    for Bipedal Locomotion: A Brief Survey")所述，与周期性时间输入同步。然而，这个模型仅限于一个运动目标：前行步态。一个包括混合零动力学（HZD）步态库的更为多样化和稳健的行走DRL框架被展示[[31](#bib.bib31)]，通过使单一端到端策略实现行走、转向和蹲下，取得了显著进展。'
- en: Despite these advancements, the parameterization of reference motions introduces
    constraints that limit the flexibility of the learning process and the policy’s
    response to disturbances. To broaden the capabilities of guided learning policies,
    a framework capable of handling multiple targets, including jumping, was developed
    [[37](#bib.bib37)]. This approach introduced a novel policy structure that integrates
    long-term input/output (I/O) encoding, complemented by a multi-stage training
    methodology that enables the execution of complex jumping maneuvers. An adversarial
    motion priors approach, employing a style reward mechanism, was also introduced
    to facilitate the acquisition of user-specified gait behaviors [[18](#bib.bib18)].
    This method improves the training of high-dimensional simulated agents by replacing
    complex hand-designed reward functions with more intuitive controls.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些进展，参考动作的参数化引入了限制，这限制了学习过程的灵活性以及策略对干扰的响应。为了拓宽引导学习策略的能力，开发了一个能够处理多个目标（包括跳跃）的框架[[37](#bib.bib37)]。该方法引入了一种新的策略结构，结合了长期输入/输出
    (I/O) 编码，并通过多阶段训练方法实现了复杂跳跃动作的执行。还引入了一种对抗性动作先验方法，采用风格奖励机制，来促进获取用户指定的步态行为[[18](#bib.bib18)]。该方法通过用更直观的控制代替复杂的手工设计奖励函数，从而提高了高维模拟体的训练效果。
- en: While previous works primarily focused on specific locomotion skills, a unified
    framework that accommodates both periodic and non-periodic motions was further
    developed [[22](#bib.bib22)] based on the foundational work in [[37](#bib.bib37)].
    This framework enhances the learning process by incorporating a wide range of
    locomotion skills and introducing a dual I/O history approach, marking a significant
    breakthrough in creating a robust, versatile, and dynamic end-to-end framework.
    However, experimental results indicate that the precision of locomotion features,
    such as velocity tracking, remains suboptimal.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管先前的工作主要集中于特定的运动技能，但基于[[37](#bib.bib37)]的基础工作，进一步开发了一个同时适应周期性和非周期性动作的统一框架[[22](#bib.bib22)]。该框架通过整合广泛的运动技能并引入双I/O历史方法，增强了学习过程，标志着创建一个稳健、多功能且动态的端到端框架的重大突破。然而，实验结果表明，运动特征的精确度（如速度跟踪）仍然不尽如人意。
- en: Guided learning methods expedite the learning process by leveraging expert knowledge
    and demonstrating the capacity to achieve versatile and robust locomotion skills.
    Through the comprehensive evaluation [[22](#bib.bib22)], it is demonstrated that
    guided learning employs references without complete dependence on them. Conversely,
    residual learning exhibits failures or severe deviations when predicated on references
    of inferior quality. This shortfall stems from the framework’s dependency on adhering
    closely to the provided references, which narrows its learning capabilities.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 引导学习方法通过利用专家知识和展示获得多功能且稳健的运动技能，加速了学习过程。通过综合评估[[22](#bib.bib22)]，可以证明引导学习在使用参考资料时并不完全依赖于它们。相反，当依赖于质量较差的参考资料时，残差学习表现出失败或严重偏差。这一不足源于框架对紧密遵循提供的参考资料的依赖，这限制了其学习能力。
- en: Nonetheless, reference-based learning reliance on predefined trajectories confines
    the policy to specific gaits, restricting its capacity to explore a broader range
    of motion possibilities. Additionally, this approach exhibits limited adaptability
    in responding effectively to unforeseen environmental changes or novel challenges.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于参考的学习依赖于预定义的轨迹，将策略限制于特定的步态，限制了其探索更广泛的运动可能性的能力。此外，这种方法在有效应对意外环境变化或新挑战方面表现出有限的适应性。
- en: II-B Reference-free learning
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 无参考学习
- en: In reference-free learning, the policy is trained using a carefully crafted
    reward function rather than relying on predefined trajectories. This approach
    allows the policy to explore a wider range of gait patterns and adapt to unforeseen
    terrains, thereby enhancing innovation and flexibility within the learning process.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在无参考学习中，策略是通过精心设计的奖励函数进行训练，而不是依赖预定义的轨迹。这种方法允许策略探索更广泛的步态模式并适应意外的地形，从而增强了学习过程中的创新和灵活性。
- en: The concept of reference-free learning was initially explored using simulated
    physics engines with somewhat unrealistic bipedal models. A pioneering framework,
    which focused on learning symmetric gaits from scratch without the use of motion
    capture data, was developed and validated within a simulated environment [[14](#bib.bib14)].
    This framework introduced a novel term into the loss function and utilized a curriculum
    learning strategy to effectively shape gait patterns. Another significant advancement
    was made in developing a learning method that enabled a robot to navigate stepping
    stones using curriculum learning, focusing on a physical robot model, Cassie,
    though this has yet to be validated outside of simulation [[40](#bib.bib40)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 无参考学习的概念最初是通过使用有些不现实的双足模型的模拟物理引擎进行探讨的。一个开创性的框架，专注于从零开始学习对称步态而无需使用动作捕捉数据，在模拟环境中进行了开发和验证[[14](#bib.bib14)]。这个框架在损失函数中引入了一个新术语，并采用了课程学习策略来有效塑造步态模式。另一个显著的进展是在开发一种学习方法，使机器人能够使用课程学习导航踏脚石，重点关注物理机器人模型Cassie，尽管这尚未在模拟外进行验证[[40](#bib.bib40)]。
- en: Considering the practical implementation of theoretical models, significant
    efforts have been directed towards developing sim-to-real frameworks in robotics
    studies. A notable example of such a framework accommodates various periodic motions,
    including walking, hopping, and galloping [[19](#bib.bib19)]. This framework employs
    periodic rewards to facilitate initial training within simulations before successfully
    transitioning to a physical robot. It has been further refined to adapt to diverse
    terrains and scenarios. For instance, robust blind walking on stairs was demonstrated
    through terrain randomization techniques in [[38](#bib.bib38)]. Additionally,
    the integration of a vision system has enhanced the framework’s ability to precisely
    determine foot locations [[41](#bib.bib41)], thus enabling the robot to effectively
    navigate stepping stones [[20](#bib.bib20)]. Subsequent developments include the
    incorporation of a vision system equipped with height maps, leading to an end-to-end
    framework that more effectively generalizes terrain information [[42](#bib.bib42)].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到理论模型的实际实施，已经付出了重大努力来开发机器人研究中的模拟到现实框架。一个显著的例子是这样的框架，它适应了各种周期性运动，包括步行、跳跃和疾驰[[19](#bib.bib19)]。该框架使用周期性奖励来促进在模拟中进行初步训练，然后成功过渡到物理机器人。它已经进一步优化以适应不同的地形和场景。例如，通过[[38](#bib.bib38)]中的地形随机化技术展示了在楼梯上的稳健盲行。此外，视觉系统的集成增强了框架精确确定足部位置的能力[[41](#bib.bib41)]，从而使机器人能够有效地导航踏脚石[[20](#bib.bib20)]。后续的发展包括集成带有高度图的视觉系统，形成了一个更有效地推广地形信息的端到端框架[[42](#bib.bib42)]。
- en: This approach to learning enables the exploration of novel solutions and strategies
    that might not be achievable through mere imitation of existing behaviours. However,
    the absence of reference guidance can render the learning process costly, time-consuming,
    and potentially infeasible for certain tasks. Moreover, the success of this method
    hinges critically on the design of the reward function, which presents significant
    challenges in specifying tasks such as jumping.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习方法使得探索可能通过单纯模仿现有行为无法实现的新解决方案和策略成为可能。然而，缺乏参考指导可能使得学习过程变得昂贵、耗时，并且对于某些任务可能不可行。此外，这种方法的成功在很大程度上依赖于奖励函数的设计，这在规定跳跃等任务时面临重大挑战。
- en: III Hierarchy framework
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 层次框架
- en: Unlike end-to-end policies that directly map sensor inputs to motor outputs,
    hierarchical control schemes deconstruct locomotion challenges into discrete,
    manageable layers or stages of decision-making. Each layer within this structure
    is tasked with specific objectives, ranging from high-level navigation to fundamental
    locomotion skills. This division not only enhances the framework’s flexibility
    but also simplifies the problem-solving process for each policy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与将传感器输入直接映射到电机输出的端到端策略不同，层次控制方案将运动挑战分解为离散的、可管理的层次或决策阶段。该结构中的每一层都承担着特定的目标，从高级导航到基本运动技能。这种划分不仅增强了框架的灵活性，还简化了每个策略的问题解决过程。
- en: 'The architecture of a hierarchical framework typically comprises two principal
    modules: an HL planner and an LL controller. This modular approach allows for
    the substitution of each component with either a model-based method or a learning-based
    policy, further enhancing adaptability and customisation to specific needs.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 层次框架的架构通常包括两个主要模块：HL 规划器和 LL 控制器。这种模块化的方法允许用基于模型的方法或基于学习的策略替代每个组件，进一步增强了对特定需求的适应性和定制性。
- en: 'Hierarchical frameworks can be classified into three distinct types based on
    the integration and function of their components:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 层次框架可以根据其组件的集成和功能分为三种不同的类型：
- en: '1.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Deep planning hybrid scheme: This approach combines strategic, high-level planning
    with dynamic low-level execution, leveraging the strengths of both learning-based
    and traditional model-based methods.'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度规划混合方案：这种方法结合了战略性高层规划和动态低层执行，利用了基于学习和传统基于模型的方法的优势。
- en: '2.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Feedback DRL control hybrid scheme: It focuses on integrating direct feedback
    control mechanisms with deep reinforcement learning, allowing for real-time adjustments
    and enhanced responsiveness.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 反馈 DRL 控制混合方案：它侧重于将直接反馈控制机制与深度强化学习结合在一起，实现实时调整和增强响应能力。
- en: '3.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Learned hierarchy scheme: Entirely learning-driven, this scheme develops a
    layered decision-making hierarchy where each level is trained to optimise specific
    aspects of locomotion.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习层次方案：完全以学习为驱动，该方案开发了一个分层决策层次结构，每一层都经过训练以优化运动的特定方面。
- en: 'These frameworks are illustrated in Fig. [3](#S3.F3 "Figure 3 ‣ III-A Deep
    planning hybrid scheme ‣ III Hierarchy framework ‣ Deep Reinforcement Learning
    for Bipedal Locomotion: A Brief Survey"). Each type offers unique capabilities
    and exhibits distinct characteristics, albeit with limitations primarily due to
    the complexities involved in integrating diverse modules and their interactions.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '这些框架如图 [3](#S3.F3 "Figure 3 ‣ III-A Deep planning hybrid scheme ‣ III Hierarchy
    framework ‣ Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey")
    所示。每种类型提供了独特的能力并展示了不同的特征，尽管由于整合不同模块及其相互作用的复杂性，存在一些限制。'
- en: 'For a concise overview, Table [3](#S3.F3 "Figure 3 ‣ III-A Deep planning hybrid
    scheme ‣ III Hierarchy framework ‣ Deep Reinforcement Learning for Bipedal Locomotion:
    A Brief Survey") summarises the various frameworks, detailing their respective
    strengths, limitations, and primary characteristics. The subsequent sections will
    delve deeper into each of these frameworks, providing a thorough analysis of their
    operational mechanics and their application in real-world scenarios.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '为了简洁概述，表格 [3](#S3.F3 "Figure 3 ‣ III-A Deep planning hybrid scheme ‣ III Hierarchy
    framework ‣ Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey")
    总结了各种框架，详细说明了它们各自的优势、限制和主要特征。接下来的部分将深入探讨这些框架，提供对其操作机制及其在实际应用中的分析。'
- en: 'TABLE II: Summary and comparison of Hierarchy framework'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：层次框架的总结和比较
- en: '| Control Scheme | Works | Module | characteristic | Advantages and Disadvantages
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 控制方案 | 工作原理 | 模块 | 特征 | 优势和劣势 |'
- en: '| Deep Planning Hybrid Scheme | [[24](#bib.bib24)] [[43](#bib.bib43)] [[44](#bib.bib44)]
    | Deep planning + ID Deep planning + ID-QP Deep planning + WPG | HL policy is
    learned to guide the LL controller to complete locomotion and navigation tasks.
    | A: Enhanced command tracking capabilities, generalized across different platforms,
    sampling efficiency, and robust. D: Complicate system and communication between
    layers, require precise model, lack generalization regarding different tasks.
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 深度规划混合方案 | [[24](#bib.bib24)] [[43](#bib.bib43)] [[44](#bib.bib44)] | 深度规划
    + ID 深度规划 + ID-QP 深度规划 + WPG | HL策略经过学习以引导LL控制器完成运动和导航任务。 | 优点：增强的指令跟踪能力、跨不同平台的泛化、采样效率高且鲁棒。缺点：系统复杂以及层间通信，需要精确的模型，关于不同任务的泛化能力不足。
    |'
- en: '| Feedback DRL Control Hybrid Scheme | [[45](#bib.bib45)] [[26](#bib.bib26)]
    [[27](#bib.bib27)] | Gait library + feedback policy Footstep planner + feedback
    policy Model-based planner + feedback policy | LL feedback policy receives non-learned
    HL planner as input to achieve locomotion skills. | A: Short inference times,
    robust, navigation locomotion capabilities, interpretability. D: Complicated system
    and communication between layers, reducing sampling efficiency. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 反馈DRL控制混合方案 | [[45](#bib.bib45)] [[26](#bib.bib26)] [[27](#bib.bib27)] |
    步态库 + 反馈策略 步态规划器 + 反馈策略 基于模型的规划器 + 反馈策略 | LL反馈策略接收未学习的HL规划器作为输入，以实现运动技能。 | 优点：推理时间短、鲁棒、导航运动能力强、可解释性。缺点：系统复杂以及层间通信，降低了采样效率。
    |'
- en: '| Learned Hierarchy Framework | [[12](#bib.bib12)] [[13](#bib.bib13)] [[29](#bib.bib29)]
    | HL policy + LL policy HL policy + LL policy HL policy + LL policy | Both HL
    planner and LL feedback controller are learned. LL policy focuses on basic locomotion
    skills; on the other side, HL policy learn navigation skills. | A: provides layer
    flexibility, where each layer can be independently retrained and reused; alleviates
    the challenges associated with training an end-to-end policy. D: inefficienty
    sim-to-real, complicated interface between layers, training expensively. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 学习层次框架 | [[12](#bib.bib12)] [[13](#bib.bib13)] [[29](#bib.bib29)] | HL策略
    + LL策略 HL策略 + LL策略 HL策略 + LL策略 | HL规划器和LL反馈控制器都经过学习。LL策略专注于基本的运动技能，而HL策略则学习导航技能。
    | 优点：提供层次灵活性，每一层都可以独立重新训练和重用；缓解了训练端到端策略相关的挑战。缺点：模拟到现实的效率低、层间接口复杂、训练成本高。 |'
- en: III-A Deep planning hybrid scheme
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 深度规划混合方案
- en: In this scheme, robots are pre-equipped with the ability to execute basic locomotion
    skills such as walking, typically managed through model-based feedback controllers
    or interpretable methods. The addition of an HL learned layer focuses on strategic
    goals or the task space, enhancing locomotion capabilities and equipping the robot
    with advanced navigation abilities to effectively explore its environment.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，机器人预先具备执行基本运动技能（如行走）的能力，这些技能通常通过基于模型的反馈控制器或可解释方法来管理。添加的高层（HL）学习层专注于战略目标或任务空间，增强了运动能力，并使机器人具备先进的导航能力，从而有效地探索环境。
- en: Several studies have demonstrated the integration of an HL planner policy with
    a model-based controller to achieve tasks in world space. A notable framework
    optimises task space level performance, eschewing direct joint level and balancing
    considerations [[24](#bib.bib24)]. This system combines a residual learning planner
    with an inverse dynamics controller, enabling precise control over task-space
    commands to joint-level actions, thereby improving velocity tracking, foot touchdown
    location, and height control. Further advancements include a hybrid framework
    that merges HZD-based residual deep planning with model-based regulators to correct
    errors in learned trajectories, showcasing robustness, training efficiency, and
    effective velocity tracking [[25](#bib.bib25)]. These frameworks have been successfully
    transferred from simulation to reality and validated on robots such as Cassie.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 几项研究已经证明了高层（HL）规划策略与基于模型的控制器的集成，以在世界空间中实现任务。一个显著的框架优化了任务空间级别的性能，避免了直接的关节级别和平衡考虑[[24](#bib.bib24)]。该系统将残差学习规划器与逆向动力学控制器结合起来，实现了对任务空间指令到关节级动作的精确控制，从而改善了速度跟踪、足部着陆位置和高度控制。进一步的进展包括一个混合框架，它将基于HZD的残差深度规划与基于模型的调节器相结合，以修正学习轨迹中的误差，展示了鲁棒性、训练效率和有效的速度跟踪[[25](#bib.bib25)]。这些框架已经成功地从仿真转移到现实，并在诸如Cassie的机器人上进行了验证。
- en: However, the limitations imposed by residual learning constrained the agents’
    capacity to explore a broader array of possibilities. Building on previous work
    [[25](#bib.bib25)], a more efficient hybrid framework was developed, which learns
    from scratch without reliance on prior knowledge [[43](#bib.bib43)]. In this approach,
    a purely learning-based HL planner interacts with an LL controller using an Inverse
    Dynamics with Quadratic Programming formulation (ID-QP). This policy adeptly captures
    dynamic walking gaits through the use of reduced-order states and simplifies the
    learning trajectory. Demonstrating robustness and training efficiency, this framework
    has outperformed other models and was successfully generalized across various
    bipedal platforms, including Digit, Cassie, and RABBIT.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，残余学习所带来的限制约束了智能体探索更广泛可能性的能力。在前期工作的基础上[[25](#bib.bib25)]，开发了一个更高效的混合框架，该框架从头开始学习而不依赖于先前的知识[[43](#bib.bib43)]。在这种方法中，一个纯学习基础的高层（HL）规划器与低层（LL）控制器通过逆动态与二次规划公式（ID-QP）进行互动。这一策略通过使用降阶状态巧妙地捕捉动态步态，并简化学习轨迹。该框架展示了鲁棒性和训练效率，表现优于其他模型，并成功地在多个双足平台上进行了推广，包括Digit、Cassie和RABBIT。
- en: 'In parallel, several research teams have focused on developing navigation planners
    specifically for toy-like humanoid robots, which provide greater physical stability
    compared to torque-driven or hydraulic bipedal robots as shown in Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Deep Reinforcement Learning for Bipedal Locomotion:
    A Brief Survey"). One notable study [[46](#bib.bib46)] implemented a visual navigation
    policy on the NAO robot, depicted in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey")(a), utilizing
    RGB cameras as the primary sensory modality. This system has demonstrated successful
    zero-shot transfer to real-world scenarios, enabling the robot to adeptly navigate
    around obstacles. Further research [[44](#bib.bib44)] has explored complex dynamic
    motion tasks, such as playing soccer, by integrating a learned policy with an
    online footstep planner that utilises weight positioning generation (WPG) to create
    a center of mass (CoM) trajectory. This configuration is coupled with a whole-body
    controller, facilitating dynamic activities like soccer shooting. Despite their
    platform’s stability, provided by large feet and a lightweight structure, these
    robots exhibit limited dynamic movement capabilities compared to full-sized humanoid
    robots. Consequently, this research primarily addresses navigation and task execution.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '与此同时，一些研究团队专注于为类似玩具的类人机器人开发导航规划器，这些机器人相较于扭矩驱动或液压双足机器人提供了更大的物理稳定性，如图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Deep Reinforcement Learning for Bipedal Locomotion:
    A Brief Survey")所示。一项显著的研究[[46](#bib.bib46)]在NAO机器人上实施了一种视觉导航策略，如图[1](#S1.F1 "Figure
    1 ‣ I Introduction ‣ Deep Reinforcement Learning for Bipedal Locomotion: A Brief
    Survey")(a)所示，利用RGB摄像头作为主要的感知方式。该系统成功地实现了对现实场景的零样本迁移，使机器人能够熟练地绕过障碍物。进一步的研究[[44](#bib.bib44)]探索了复杂的动态运动任务，如踢足球，通过将学习的策略与一个在线足步规划器集成，该规划器利用重量定位生成（WPG）创建质心（CoM）轨迹。这一配置与全身控制器配合，使得像踢足球这样的动态活动成为可能。尽管这些机器人由大脚和轻量结构提供了平台的稳定性，但与全尺寸类人机器人相比，这些机器人在动态运动能力上表现有限。因此，这项研究主要关注导航和任务执行。'
- en: Regarding generalization, these frameworks have shown potential for adaptation
    across different types of bipedal and humanoid robots with minimal adjustments,
    demonstrating advanced user command tracking [[43](#bib.bib43)] and sophisticated
    navigation capabilities [[44](#bib.bib44)]. However, limitations are evident,
    notably the absence of capabilities for executing more complex and dynamic motions,
    such as jumping. Furthermore, while these systems adeptly navigate complex terrains
    with obstacles, footstep planning alone is insufficient without concurrent enhancements
    to the robot’s overall locomotion capabilities. Moreover, the requisite communication
    between the two distinct layers of the hierarchical framework may introduce system
    complexities. Enhancing both navigation and dynamic locomotion capabilities within
    the HL planner remains a significant challenge.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 关于泛化，这些框架在不同类型的双足和类人机器人之间展现了适应的潜力，只需进行最小的调整，就能展示出高级的用户命令跟踪[[43](#bib.bib43)]和复杂的导航能力[[44](#bib.bib44)]。然而，明显的局限性在于缺乏执行更复杂和动态动作的能力，例如跳跃。此外，尽管这些系统能够熟练地在有障碍的复杂地形中导航，仅仅依靠足步规划是不够的，需要同步提升机器人的整体运动能力。此外，层级框架中两个不同层次之间的必要通信可能会引入系统复杂性。在HL规划器中提升导航和动态运动能力仍然是一个重大挑战。
- en: '![Refer to caption](img/e667251cd3798ae4315efc8a4ff90248.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e667251cd3798ae4315efc8a4ff90248.png)'
- en: 'Figure 3: Hierarchy Control Scheme Diagram: (a) A basic hierarchical scheme
    with two layers, where each module can be substituted with a learned policy. (b)
    A deep planning hybrid scheme, where the High-Level (HL) planner is learned. (c)
    A learning-based feedback control hybrid scheme, with a learned Low-Level (LL)
    controller. (d) A comprehensive DRL hierarchy control scheme, where both layers
    are learned.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '图3: 层级控制方案图：(a) 一个基本的两层层级方案，其中每个模块可以被学习到的策略替代。(b) 一个深度规划混合方案，其中高层（HL）规划器是学习到的。(c)
    一个基于学习的反馈控制混合方案，具有一个学习到的低层（LL）控制器。(d) 一个全面的DRL层级控制方案，其中两个层级都是学习到的。'
- en: III-B Feedback DRL control hybrid scheme
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 反馈DRL控制混合方案
- en: 'In contrast to the comprehensive approach of end-to-end policies discussed
    in Section [II](#S2 "II End-to-end framework ‣ Deep Reinforcement Learning for
    Bipedal Locomotion: A Brief Survey"), which excels in handling versatile locomotion
    skills and complex terrains with minimal interface times, the Feedback DRL Control
    Hybrid Scheme integrates DRL policies as LL controllers. These LL controllers,
    replacing traditional model-based feedback mechanisms, work in conjunction with
    HL planners that process terrain information, plan future walking paths, and maintain
    robust locomotion stability.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '与[II](#S2 "II End-to-end framework ‣ Deep Reinforcement Learning for Bipedal
    Locomotion: A Brief Survey")节讨论的端到端策略的全面方法相比，该方法在处理多样化运动技能和复杂地形时表现出色，且界面时间最小，而反馈DRL控制混合方案将DRL策略集成作为LL控制器。这些LL控制器替代了传统的基于模型的反馈机制，与处理地形信息、规划未来步态并保持稳健运动稳定性的HL规划器协同工作。'
- en: For instance, gait libraries, which provide predefined movement references based
    on user commands, have been integrated into such frameworks [[45](#bib.bib45)].
    Despite the structured approach of using gait libraries, their static nature offers
    limited adaptability to changing terrains, diminishing their effectiveness. A
    more dynamic approach involves online planning, which has shown greater adaptability
    and efficiency. One notable framework combines a conventional foot planner with
    an LL DRL policy [[26](#bib.bib26)], delivering targeted footsteps and directional
    guidance to the robot, thereby enabling responsive and varied walking commands.
    Moreover, HL controllers can provide additional feedback to LL policies, incorporating
    CoM or end-feet information, either from model-based methods or other conventional
    control strategies. However, this work has not yet been transferred from simulation
    to real-world applications. Later, a similar structure featuring an HL foot planner
    and an LL DRL policy was proposed [[27](#bib.bib27)]. This strategy not only achieved
    a successful sim-to-real transfer but also enabled the robot to navigate omnidirectionally
    and avoid obstacles.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，步态库提供了基于用户命令的预定义运动参考，并已集成到这种框架中 [[45](#bib.bib45)]。尽管使用步态库的方法具有结构化，但其静态特性对变化的地形适应性有限，从而降低了其效果。更具动态性的方法包括在线规划，已显示出更大的适应性和效率。一个显著的框架将传统的脚步规划器与
    LL DRL 策略相结合 [[26](#bib.bib26)]，为机器人提供了有针对性的脚步和方向指导，从而实现了响应性和多样化的行走命令。此外，HL 控制器可以向
    LL 策略提供额外的反馈，结合 CoM 或末端脚的信息，无论是来自基于模型的方法还是其他传统控制策略。然而，这项工作尚未从模拟转移到实际应用中。随后，提出了一个类似的结构，具有
    HL 脚步规划器和 LL DRL 策略 [[27](#bib.bib27)]。这一策略不仅成功实现了从模拟到现实的转移，还使机器人能够全方位导航并避开障碍物。
- en: A recent development has shown that focusing solely on foot placement might
    restrict the stability and adaptability of locomotion, particularly in complex
    maneuvers. A new framework integrates a model-based planner with a DRL feedback
    policy to enhance bipedal locomotion’s agility and versatility, displaying improved
    performance [[47](#bib.bib47)]. This system employs a residual learning architecture,
    where the DRL policy’s outputs are merged with the planner’s directives before
    being relayed to the PD controller. This integrated approach not only concerns
    itself with foot placement but also generates comprehensive trajectories for trunk
    position, orientation, and ankle yaw angle, enabling the robot to perform a wide
    array of locomotion skills including walking, squatting, turning, and stair climbing.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的发展表明，仅仅专注于脚步位置可能会限制运动的稳定性和适应性，尤其是在复杂的动作中。一个新的框架将基于模型的规划器与 DRL 反馈策略集成，以增强双足运动的灵活性和多样性，显示出改善的性能
    [[47](#bib.bib47)]。该系统采用残差学习架构，将 DRL 策略的输出与规划器的指令合并后，再传递给 PD 控制器。这种集成方法不仅关注脚步位置，还生成了全面的躯干位置、方向和踝关节偏航角的轨迹，使机器人能够执行各种运动技能，包括走路、蹲下、转弯和爬楼梯。
- en: Compared to traditional model-based controllers, learned DRL policies provide
    a comprehensive closed-loop control strategy that does not rely on assumptions
    about terrain or robotic capabilities. These policies have demonstrated high efficiency
    in locomotion and accurate reference tracking. Despite their extensive capabilities,
    such policies generally require short inference times, making DRL a preferred
    approach in scenarios where robustness is paramount or computational resources
    on the robot are limited. Nonetheless, these learning algorithms often face challenges
    in environments characterized by sparse rewards, where suitable footholds like
    gaps or stepping stones are infrequent [[48](#bib.bib48)].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的基于模型的控制器相比，学习型 DRL 策略提供了全面的闭环控制策略，不依赖于地形或机器人能力的假设。这些策略在运动中表现出了高效性和准确的参考跟踪。尽管具有广泛的能力，这些策略通常需要较短的推断时间，使
    DRL 成为在鲁棒性至关重要或机器人计算资源有限的情况下的首选方法。然而，这些学习算法在稀疏奖励的环境中通常面临挑战，例如合适的踏脚点（如缝隙或踏脚石）不常出现
    [[48](#bib.bib48)]。
- en: Additionally, an HL planner can process critical data such as terrain variations
    or obstacles and generate precise target locations for feet or desired walking
    paths, instead of detailed terrain data, which can significantly expedite the
    training process [[27](#bib.bib27)]. This capability effectively addresses the
    navigational limitations observed in end-to-end frameworks. Moreover, unlike the
    deep planning hybrid scheme where modifications post-policy establishment can
    be cumbersome, this hybrid scheme offers enhanced flexibility for on-the-fly adjustments.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，HL 规划器可以处理关键数据，如地形变化或障碍物，并生成精确的脚部目标位置或期望的行走路径，而不是详细的地形数据，这可以显著加快训练过程[[27](#bib.bib27)]。这一能力有效地解决了端到端框架中观察到的导航限制。此外，与深度规划混合方案中在策略建立后修改繁琐不同，这种混合方案在即兴调整方面提供了更大的灵活性。
- en: Despite the significant potential demonstrated by previous studies, integrating
    DRL-based controllers with sophisticated and complex HL planners still presents
    limitations compared to more integrated frameworks such as end-to-end and deep
    planning models. Specifically, complex HL model-based planners often require substantial
    computational resources to resolve problems, rely heavily on model assumptions,
    necessitate extensive training periods, demand large datasets for optimization
    and hinder rapid deployment and iterative enhancements [[48](#bib.bib48)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前的研究展示了显著的潜力，但将基于 DRL 的控制器与复杂的 HL 规划器集成相比于更集成的框架（如端到端和深度规划模型）仍然存在局限性。具体来说，复杂的
    HL 模型基于规划器通常需要大量的计算资源来解决问题，严重依赖模型假设，需长时间训练，要求大量数据集进行优化，并阻碍了快速部署和迭代改进[[48](#bib.bib48)]。
- en: III-C Learned hierarchy framework
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 学习层次框架
- en: The Learned Hierarchy Framework merges a learned HL planner with an LL controller,
    focusing initially on refining LL policies to ensure balance and basic locomotion
    capabilities. Subsequently, an HL policy is developed to direct the robot towards
    specific targets, encapsulating a structured approach to robotic autonomy.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 学习层次框架将学习的 HL 规划器与 LL 控制器相结合，初步专注于优化 LL 策略以确保平衡和基本的运动能力。随后，开发了 HL 策略来引导机器人向特定目标前进，体现了对机器人自主性的结构化方法。
- en: The genesis of this framework was within a physics engine, aimed at validating
    its efficiency through simulation [[12](#bib.bib12)]. In this setup, LL policies,
    informed by human motions or trajectories generated via Trajectory Optimization
    (TO), strive to track these trajectories as dictated by the HL planner while maintaining
    balance. An HL policy is then introduced, pre-trained with long-term task goals,
    to navigate the environment and identify optimal paths. This structure enabled
    sophisticated interactions such as guiding a biped to dribble a soccer ball towards
    a goal. The framework was later enhanced to include imitation learning, facilitating
    the replication of dynamic human-like movements within the simulation environment
    [[13](#bib.bib13)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这一框架的起源是在一个物理引擎中，旨在通过模拟验证其效率[[12](#bib.bib12)]。在这个设置中，LL 策略通过人类动作或通过轨迹优化（TO）生成的轨迹来指导这些轨迹，同时保持平衡。然后引入了一个
    HL 策略，该策略通过长期任务目标进行预训练，以导航环境并识别最佳路径。这一结构使得复杂的互动成为可能，例如指导双足机器人将足球带向目标。该框架后来进行了增强，加入了模仿学习，促进了在模拟环境中复制动态类人运动的能力[[13](#bib.bib13)]。
- en: However, despite its structured and layered approach, which allows for the reuse
    of learned behaviors to achieve long-term objectives, these frameworks have predominantly
    been validated only in simulations. The interface designed manually between the
    HL planner and the LL controller sometimes leads to suboptimal behaviors, including
    stability issues like falling.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管其结构化和分层的方法允许重用学习到的行为以实现长期目标，但这些框架主要只在模拟中得到了验证。HL 规划器与 LL 控制器之间手动设计的接口有时会导致次优行为，包括稳定性问题，如摔倒。
- en: Expanding the application of this framework, a sim-to-real strategy for a wheeled
    bipedal robot was proposed, focusing the LL policy on balance and position tracking,
    while the HL policy enhances safety by aiding in collision avoidance and making
    strategic decisions based on the orientation of subgoals [[29](#bib.bib29)].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展该框架的应用时，提出了一种针对轮式双足机器人从模拟到现实的策略，将 LL 策略集中于平衡和位置跟踪，而 HL 策略通过辅助碰撞避免和根据子目标的方向做出战略决策来增强安全性[[29](#bib.bib29)]。
- en: Learning complex locomotion skills, particularly when incorporating navigation
    elements, presents a significant challenge in robotics. Decomposing these tasks
    into distinct locomotion and navigation components allows robots to tackle more
    intricate activities, such as dribbling a soccer ball [[12](#bib.bib12)]. As discussed
    in the previous section, the benefits of integrating RL-based planners with RL-based
    controllers have been effectively demonstrated. This combination enables the framework
    to adeptly manage a diverse array of environments and tasks.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 学习复杂的运动技能，特别是涉及导航元素时，是机器人技术中的一个重大挑战。将这些任务分解为不同的运动和导航组件使机器人能够处理更复杂的活动，例如运球[[12](#bib.bib12)]。正如上一节所讨论的，将基于强化学习的规划者与基于强化学习的控制器相结合的好处已被有效展示。这种组合使得框架能够巧妙地管理各种环境和任务。
- en: Within such a framework, the High-Level (HL) policy is optimized for strategic
    planning and achieving specific goals. This optimization allows for targeted enhancements
    depending on the tasks at hand. Moreover, the potential for continuous improvement
    and adaptation through further training ensures that the system can evolve over
    time, improving its efficiency and effectiveness in response to changing conditions
    or new objectives.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的框架中，高层（HL）策略被优化用于战略规划和实现特定目标。这种优化允许根据任务的需要进行有针对性的增强。此外，通过进一步训练的持续改进和适应的潜力确保了系统能够随时间演变，提高其在应对变化条件或新目标时的效率和效果。
- en: Despite the theoretical advantages, the practical implementation of this type
    of sim-to-real application for bipedal robots remains largely unexplored. The
    transition from simulation to real-world scenarios is fraught with challenges,
    not least because of the complexities involved in training and integrating two
    separate layers within the control hierarchy. Ensuring effective communication
    and cooperation between these layers is critical, requiring a meticulously defined
    communication interface to avoid operational discrepancies.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管理论上有优势，这种用于双足机器人的仿真到现实应用的实际实施仍然基本未被探索。模拟到真实世界场景的过渡充满了挑战，尤其是因为在控制层次结构中训练和整合两个独立层次的复杂性。确保这些层次之间的有效沟通与合作至关重要，需要一个精确定义的沟通接口以避免操作差异。
- en: Additionally, the training process for each policy within the hierarchy demands
    considerable computational resources. The intensive nature of this training can
    lead to a reliance on the simulation environment, potentially causing the system
    to overfit to specific scenarios and thereby fail to generalize to real-world
    conditions. This limitation highlights a significant hurdle that must be addressed
    to enhance the viability of learned hierarchy frameworks in practical applications.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，层次结构中每个策略的训练过程需要大量计算资源。这种高强度的训练可能导致对模拟环境的依赖，从而使系统过度拟合特定场景，无法泛化到现实世界条件。这一局限性突显了一个重要的障碍，必须解决以增强学习层次框架在实际应用中的可行性。
- en: IV Challenges and future research directions
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 挑战与未来研究方向
- en: While learning-based frameworks for bipedal robots have demonstrated considerable
    potential, they have also clearly exposed the limitations inherent to each framework.
    Moreover, several critical areas remain largely unexplored, especially within
    the realm of legged robotics, where the pace of research on bipedal robots lags
    behind that of their quadruped counterparts. This discrepancy in research progress
    can be attributed to several factors, including the higher costs and less mature
    technology associated with bipedal robot hardware, as well as the inherent instability
    issues that bipedal designs face.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于学习的双足机器人框架展示了相当大的潜力，但它们也明显暴露了每个框架固有的局限性。此外，几个关键领域仍然基本未被探索，特别是在腿部机器人领域，在这里，双足机器人的研究进展远远落后于四足机器人。研究进展差异可归因于多个因素，包括双足机器人硬件的高成本和技术不成熟，以及双足设计面临的固有不稳定性问题。
- en: To gain a deeper understanding of these challenges and to outline potential
    future directions, it is instructive to first review existing research on quadruped
    robots. The insights gained from quadrupeds, which benefit from more robust research
    outputs and technological advancements, can provide valuable lessons for overcoming
    similar challenges in bipedal systems.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地了解这些挑战并概述潜在的未来方向，首先回顾现有的四足机器人研究是很有启发性的。从四足机器人中获得的见解，这些机器人受益于更为稳健的研究成果和技术进步，可以为解决双足系统中的类似挑战提供宝贵的经验。
- en: IV-A Recent progress with quadruped robots
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 四足机器人最近的进展
- en: While DRL remains an emerging technology in bipedal robotics, it has firmly
    established its presence in the realm of quadruped robots, another category of
    legged systems. The diversity of frameworks developed for quadrupeds ranges from
    model-based RL designed for training in real-world scenarios, where unpredictable
    dynamics often prevail [[49](#bib.bib49), [50](#bib.bib50)], to systems that include
    the modeling of deformable terrain to enhance locomotion over compliant surfaces
    [[51](#bib.bib51)]. Furthermore, dynamic quadruped models facilitate highly adaptable
    policies [[52](#bib.bib52)], and sophisticated acrobatic motions are achieved
    through imitation learning [[53](#bib.bib53)].
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度强化学习（DRL）仍是双足机器人领域中的一项新兴技术，但它在四足机器人这一另一类步态系统中已经牢固地确立了自己的地位。为四足机器人开发的框架种类繁多，从用于真实场景训练的基于模型的强化学习（RL），在这种场景中不可预测的动态往往占主导地位[[49](#bib.bib49),
    [50](#bib.bib50)]，到包括可变形地形建模的系统，以提高在柔性表面上的运动能力[[51](#bib.bib51)]。此外，动态四足模型促进了高度可适应的策略[[52](#bib.bib52)]，而复杂的杂技动作则通过模仿学习实现[[53](#bib.bib53)]。
- en: 'The domain of quadruped DRL has also seen significant advancements in complex
    hybrid frameworks that integrate vision-based systems. To date, two primary versions
    of such frameworks have been developed: one where a deep planning module is paired
    with model-based control [[54](#bib.bib54)], and another that combines model-based
    planning with low-level DRL control [[48](#bib.bib48), [55](#bib.bib55)]. The
    latter has shown substantial efficacy; it employs a model predictive control (MPC)
    to generate reference motions, which are then followed by a LL feedback DRL policy.
    Additionally, the Terrain-aware Motion Generation for Legged Robots (TAMOLS) module
    [[56](#bib.bib56)] enhances the MPC and DRL policy by providing terrain height
    maps for effective foothold placements across diverse environments, including
    those not encountered during training. However, similar hybrid control schemes
    have not been thoroughly investigated within the field of bipedal locomotion.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 四足深度强化学习（DRL）领域也见证了复杂混合框架的显著进展，这些框架集成了基于视觉的系统。目前，已经开发出两种主要版本的此类框架：一种是将深度规划模块与基于模型的控制相结合[[54](#bib.bib54)]，另一种是将基于模型的规划与低层次DRL控制相结合[[48](#bib.bib48),
    [55](#bib.bib55)]。后一种方案显示出了显著的效果；它采用模型预测控制（MPC）生成参考动作，然后由低层次反馈DRL策略跟随。此外，腿部机器人地形感知运动生成（TAMOLS）模块[[56](#bib.bib56)]通过提供地形高度图，增强了MPC和DRL策略，使其能够在各种环境中（包括训练期间未遇到的环境）实现有效的足部放置。然而，类似的混合控制方案在双足步态领域尚未得到深入研究。
- en: Quadruped DRL frameworks are predominantly designed to navigate complex terrains,
    but efforts to extend their capabilities to other tasks are underway. These include
    mimicking real animals through motion capture data and imitation learning [[57](#bib.bib57),
    [58](#bib.bib58)], as well as augmenting quadrupeds with manipulation abilities.
    This is achieved either by adding a manipulator [[59](#bib.bib59), [60](#bib.bib60)]
    or by using the robots’ legs [[61](#bib.bib61)]. Notably, the research presented
    in [[60](#bib.bib60)] demonstrates that loco-manipulation tasks can be effectively
    managed using a single unified end-to-end framework.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 四足DRL框架主要设计用于导航复杂地形，但扩展其能力到其他任务的努力正在进行中。这些任务包括通过动作捕捉数据和模仿学习来模拟真实动物[[57](#bib.bib57),
    [58](#bib.bib58)]，以及通过增加操纵器[[59](#bib.bib59), [60](#bib.bib60)]或利用机器人的腿部[[61](#bib.bib61)]来增强四足机器人的操控能力。值得注意的是，[[60](#bib.bib60)]中展示的研究表明，单一的端到端框架可以有效地管理运动操控任务。
- en: Despite the progress in quadruped DRL, similar advancements have been limited
    for bipedal robots, particularly in loco-manipulation tasks and vision-based DRL
    frameworks. Establishing a unified framework could bridge this gap, an essential
    step given the integral role of bipedal robots in developing full humanoid systems.
    Moreover, the potential of hybrid frameworks that combine model-based and DRL-based
    methods in bipedal robots remains largely untapped.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管四足 DRL 取得了一定进展，但双足机器人特别是在运动操控任务和基于视觉的 DRL 框架方面的类似进展有限。建立一个统一的框架可以弥合这一差距，这对于开发完整的类人系统至关重要。此外，将基于模型和基于
    DRL 的方法结合起来的混合框架在双足机器人中的潜力仍然大部分未被开发。
- en: IV-B Gaps and challenges
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 差距和挑战
- en: 'Despite numerous promising developments in the field of bipedal and humanoid
    robotics, significant gaps remain between current research outcomes and the ultimate
    goals. This discussion concentrates on the gaps in frameworks and algorithms rather
    than hardware, structured around two pivotal questions: 1) Is it possible to design
    a unified framework that achieves both generalization and precision? 2) Can we
    develop a straightforward end-to-end policy capable of managing all tasks efficiently?'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管双足机器人和类人机器人领域有许多有希望的发展，但当前研究成果与**终极目标**之间仍存在显著差距。本文讨论集中在框架和算法的差距上，而非硬件，围绕两个关键问题展开：1）是否可以设计一个统一的框架，实现通用性和精确性？2）我们能否开发一个简单的端到端政策，能够高效地管理所有任务？
- en: IV-B1 Generalization versus precision
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 通用性与精确性
- en: DRL has demonstrated potential in facilitating versatile locomotion skills [[22](#bib.bib22)];
    however, challenges such as poor velocity tracking and issues with precise control
    often arise. While [[43](#bib.bib43)] shows that deep planning combined with model-based
    control can achieve precise velocity tracking, and [[37](#bib.bib37)] illustrates
    successful end-to-end control for precise jumping, the creation of a policy that
    effectively handles both diverse tasks and precise movements remains elusive.
    Furthermore, [[41](#bib.bib41)] introduces a foot constraint policy framework,
    enabling precise target tracking and accurate touchdown locations. Yet, there
    is still no framework that comprehensively addresses the dual demands of versatility
    and precision in locomotion.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 在促进多功能运动技能方面显示了潜力 [[22](#bib.bib22)]；然而，通常会出现诸如速度跟踪不佳和精确控制问题等挑战。尽管 [[43](#bib.bib43)]
    表明深度规划结合模型控制可以实现精确的速度跟踪，而 [[37](#bib.bib37)] 则展示了成功的端到端控制以实现精确跳跃，但创建一个有效处理多样任务和精确运动的策略仍然难以实现。此外，[[41](#bib.bib41)]
    介绍了一种脚部约束政策框架，使得精确目标跟踪和准确着陆位置成为可能。然而，仍然没有一个框架能全面解决运动中的多样性和精确性的双重需求。
- en: 'The difficulty in simultaneously achieving precise control and a broad range
    of actions in bipedal locomotion using DRL stems from several factors:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在双足运动中，利用 DRL 同时实现精确控制和广泛动作的困难源于几个因素：
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Complex dynamics: Bipedal locomotion involves intricate dynamics, posing a
    significant challenge to maintaining both dynamic motion and precision.'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 复杂动态：双足运动涉及复杂的动态学，这对维持动态运动和精确性提出了重大挑战。
- en: •
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Resource intensity: Executing diverse locomotion tasks requires considerable
    computational power and extensive data, necessitating high-quality hardware and
    efficient DRL algorithms.'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 资源密集型：执行多样化的运动任务需要大量的计算能力和广泛的数据，需依赖高质量硬件和高效的 DRL 算法。
- en: •
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training conflicts: Training DRL systems to achieve both precision and versatility
    often leads to conflicts. Designing reward functions and training policies that
    satisfy both criteria is inherently complex.'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练冲突：训练 DRL 系统以实现精确度和多样性常常会导致冲突。设计既满足这两个标准的奖励函数和训练策略本质上是复杂的。
- en: These challenges underscore the need for innovative solutions that can bridge
    the gap between the capabilities of current frameworks and the ambitious goals
    of advanced bipedal and humanoid robotics.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些挑战突显了需要创新解决方案来弥合当前框架能力与先进双足和类人机器人雄心勃勃目标之间的差距。
- en: IV-B2 Simplifying frameworks to overcome complex tasks
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 简化框架以克服复杂任务
- en: The envisioned ideal in robotic design is an end-to-end framework that enables
    robots to traverse various terrains using versatile locomotion skills. Although
    current research often focuses on enhancing frameworks by adding complex components
    to mitigate inherent limitations, such as the integration of a foot planner for
    omnidirectional locomotion and stair navigation, as demonstrated in [[26](#bib.bib26),
    [27](#bib.bib27)], simpler end-to-end frameworks have also proven effective. These
    frameworks adeptly navigate challenging terrains and perform a diverse range of
    locomotion tasks with fewer components [[20](#bib.bib20), [22](#bib.bib22)].
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人设计中的理想设想是一个端到端的框架，使机器人能够使用多功能的运动技能在各种地形上移动。尽管当前研究往往通过添加复杂组件来增强框架以缓解固有的限制，如集成全向运动和楼梯导航的足部规划器，如[[26](#bib.bib26),
    [27](#bib.bib27)]所示，但简单的端到端框架也已被证明有效。这些框架能够在挑战性地形中自如导航，并执行各种运动任务，组件较少[[20](#bib.bib20),
    [22](#bib.bib22)]。
- en: 'The advantage of maintaining simplicity in the framework lies in its ability
    to streamline decision-making processes, thereby reducing computational overhead
    and potential points of failure. To achieve an optimal end-to-end framework, advancements
    in several key areas are essential:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 维持框架简洁性的优势在于其能够简化决策过程，从而减少计算开销和潜在的故障点。要实现一个优化的端到端框架，几个关键领域的进展是必不可少的：
- en: •
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Robust and efficient DRL algorithms: Development of algorithms that can manage
    high-dimensional and continuous control problems more effectively.'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 强大而高效的深度强化学习（DRL）算法：开发能够更有效地处理高维和连续控制问题的算法。
- en: •
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Specialized neural network architectures: Design of neural architectures tailored
    for specific bipedal tasks, capable of processing extensive sensory data (e.g.,
    visual and tactile inputs), similar to the innovations presented in [[42](#bib.bib42)].'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 专用神经网络架构：设计针对特定双足任务的神经网络架构，能够处理大量感官数据（如视觉和触觉输入），类似于[[42](#bib.bib42)]中提出的创新。
- en: •
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Effective reward functions: Formulation of reward functions that more accurately
    guide the learning process towards achieving desired behaviors and strategic outcomes.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有效的奖励函数：制定能够更准确地指导学习过程以实现期望行为和战略结果的奖励函数。
- en: •
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Advanced computational resources: Enhancement of computational capabilities
    to support more intensive training and faster inference, facilitating real-time
    decision-making in dynamic environments.'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高级计算资源：提升计算能力，以支持更密集的训练和更快速的推理，促进在动态环境中的实时决策。
- en: By focusing on these developmental areas, the potential to create a unified,
    efficient, and less complex framework for handling complex locomotion challenges
    in bipedal robots is significantly increased.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过关注这些发展领域，创建一个统一、高效且较少复杂的框架来处理双足机器人复杂的运动挑战的潜力显著增加。
- en: IV-C Future directions
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 未来方向
- en: The exploration of quadruped robotics has yielded substantial advancements,
    yet the full potential of bipedal robotics remains largely untapped. Building
    on the successes and innovative approaches observed in quadruped robots, several
    key future directions emerge that could significantly enhance bipedal and humanoid
    robotics.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然四足机器人领域取得了显著进展，但双足机器人的潜力仍未被充分挖掘。借鉴四足机器人取得的成功和创新方法，几个关键的未来方向浮现，这些方向有可能显著提升双足及类人机器人技术。
- en: IV-C1 Unified framework
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 统一框架
- en: 'Currently, no single framework exists that enables bipedal or humanoid robots
    to adeptly navigate all types of terrains, including stepping stones, stairs,
    deformable terrain, and slippery surfaces. A promising approach, as evidenced
    by recent work in quadruped robots [[48](#bib.bib48)], utilizes MPC to generate
    reference motions, which a low-level DRL policy then tracks. This method, coupled
    with the Terrain-aware Motion Generation for Legged Robots (TAMOLS) module, simplifies
    the terrain representation into a height map, facilitating more effective navigation.
    This success encourages further exploration into hybrid frameworks that combine
    model-based methods with DRL, inheriting the strengths of both approaches, as
    discussed in Section [III](#S3 "III Hierarchy framework ‣ Deep Reinforcement Learning
    for Bipedal Locomotion: A Brief Survey"). However, hybrid frameworks present challenges
    such as training efficiency and system complexity, which demand considerable computational
    resources and extensive training periods.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '目前尚不存在单一框架，能够使双足或类人机器人熟练地在所有类型的地形上行进，包括踏脚石、楼梯、可变形地形和滑溜表面。近期在四足机器人领域的工作[[48](#bib.bib48)]显示了一种有前景的方法，该方法利用**模型预测控制（MPC）**生成参考运动，低级别的**深度强化学习（DRL）**策略则跟踪这些参考运动。这种方法结合了“地形感知运动生成模块（TAMOLS）”，将地形表示简化为高度图，从而促进了更有效的导航。这一成功鼓励进一步探索结合基于模型的方法和**DRL**的混合框架，继承两种方法的优点，如[III](#S3
    "III Hierarchy framework ‣ Deep Reinforcement Learning for Bipedal Locomotion:
    A Brief Survey")部分所讨论。然而，混合框架面临诸如训练效率和系统复杂性等挑战，这需要大量的计算资源和较长的训练周期。'
- en: Moreover, recent studies [[20](#bib.bib20), [42](#bib.bib42), [22](#bib.bib22)]
    have demonstrated the potential of end-to-end frameworks enhanced with vision-based
    information. These frameworks successfully navigate challenging terrains and execute
    dynamic motions, suggesting the feasibility of a unified framework capable of
    handling diverse environments and tasks. Training strategies such as curriculum
    learning and task randomization could be employed, utilizing visual height maps
    as inputs to the policy, enhancing the robot’s ability to adapt and perform in
    varied scenarios.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，近期研究[[20](#bib.bib20), [42](#bib.bib42), [22](#bib.bib22)]已经展示了增强视觉信息的端到端框架的潜力。这些框架成功地在具有挑战性的地形上导航并执行动态动作，表明了能够处理多样化环境和任务的统一框架的可行性。训练策略如课程学习和任务随机化可以被采用，利用视觉高度图作为策略的输入，提高机器人的适应能力和在各种场景中的表现。
- en: In addition, the introduction of a DRL end-to-end framework incorporating transformer
    models, as in [[62](#bib.bib62)], presents significant possibilities for integrating
    locomotion skills with language and vision capabilities. The use of large-scale
    models capable of processing and condensing extensive data sets into a coherent
    model could expand the robot’s range of capabilities, maintaining versatility
    across a broad spectrum of tasks.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，引入结合**变换器模型**的**DRL**端到端框架，如[[62](#bib.bib62)]所示，为将运动技能与语言和视觉能力整合提供了重要可能性。利用能够处理和浓缩大量数据集的大规模模型可以扩展机器人的能力范围，保持在广泛任务中的多样性。
- en: The exploration of transformers and other large-scale models holds considerable
    promise for enhancing generalizability and adaptability in complex tasks, warranting
    further investigation into their potential applications in bipedal robotics.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对**变换器**和其他大规模模型的探索具有显著潜力，可增强复杂任务中的泛化能力和适应性，这值得进一步研究它们在双足机器人中的潜在应用。
- en: IV-C2 Vision-based learning framework
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 基于视觉的学习框架
- en: Vision plays a critical role in enabling robots to navigate challenging terrains,
    such as blind drops, where tactile and other sensory inputs may not provide sufficient
    information. Despite the importance of vision, many current frameworks, particularly
    in bipedal robotics, do not fully exploit this modality [[38](#bib.bib38), [43](#bib.bib43)].
    Vision-based systems are essential in human locomotion for identifying obstacles
    and assessing terrains, and some studies have begun to show the effectiveness
    of integrating vision into DRL frameworks for bipedal and humanoid robots [[20](#bib.bib20),
    [42](#bib.bib42), [27](#bib.bib27)].
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉在使机器人能够导航挑战性地形（如盲点掉落）中起着至关重要的作用，在这些情况下，触觉和其他感官输入可能无法提供足够的信息。尽管视觉非常重要，但许多现有框架，特别是在双足机器人领域，并没有充分利用这一模态[[38](#bib.bib38),
    [43](#bib.bib43)]。视觉系统在人类运动中对于识别障碍物和评估地形是必不可少的，一些研究已开始展示将视觉集成到**DRL**框架中的效果，对于双足和类人机器人[[20](#bib.bib20),
    [42](#bib.bib42), [27](#bib.bib27)]。
- en: 'Building on the groundwork laid by both bipedal and quadruped robots, two promising
    directions have emerged:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在双足机器人和四足机器人打下的基础上，出现了两个有前景的方向：
- en: •
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Height scanner mapping: This approach, evaluated in works like [[42](#bib.bib42)],
    involves using height maps generated by scanners to inform locomotion strategies.
    These maps provide detailed topographical data, allowing robots to plan steps
    on uneven or obstructed surfaces more effectively.'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高度扫描映射：这种方法在如[[42](#bib.bib42)]的研究中进行了评估，涉及使用扫描仪生成的高度图来指导运动策略。这些图提供详细的地形数据，使机器人能够更有效地在不平坦或被阻碍的表面上规划步伐。
- en: •
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Direct vision inputs: Directly utilizing inputs from cameras, such as depth
    or RGB images, for real-time decision-making in RL policies [[46](#bib.bib46),
    [63](#bib.bib63)]. Although previous studies like [[46](#bib.bib46)] have integrated
    visual navigation by feeding visual information to a High-Level (HL) planner,
    the potential of direct visual inputs to RL policies has not been fully explored.'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接视觉输入：直接利用来自相机的输入，如深度图像或RGB图像，进行实时决策的RL策略[[46](#bib.bib46), [63](#bib.bib63)]。尽管之前的研究如[[46](#bib.bib46)]通过将视觉信息提供给高级（HL）规划器实现了视觉导航，但直接视觉输入在RL策略中的潜力尚未得到充分探索。
- en: Enhancing the capability of bipedal robots to directly interpret and utilize
    visual data without intermediary processing can revolutionize their adaptability
    and efficiency in real-world scenarios. The exploration of direct vision inputs
    to reinforcement learning policies represents a significant opportunity for advancing
    the field, potentially enabling more dynamic and responsive locomotion strategies.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 提升双足机器人直接解读和利用视觉数据的能力，可以在现实世界情境中彻底改变它们的适应性和效率。探索直接视觉输入到强化学习策略中的潜力，为推动该领域的进展提供了重大机会，可能实现更动态和响应迅速的运动策略。
- en: IV-C3 Bridge the gap from simulation to reality
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C3 从模拟到现实的桥梁
- en: 'While simulations offer a safe and cost-effective environment for developing
    robotics policies, the transition from simulation to real-world application often
    encounters significant challenges due to the approximations and simplifications
    made in simulations. Numerous sim-to-real frameworks [[34](#bib.bib34), [64](#bib.bib64),
    [65](#bib.bib65)] have shown high efficiency and performance, as detailed in Appendix
    [B](#A2 "Appendix B Bridging sim-to-real gap ‣ Deep Reinforcement Learning for
    Bipedal Locomotion: A Brief Survey"). Despite these advancements, a significant
    gap persists, exacerbated by the complexity and unpredictability of physical environments.
    Moreover, many studies [[26](#bib.bib26), [66](#bib.bib66), [21](#bib.bib21)]
    remain validated only in simulation settings.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管模拟提供了一个安全且经济的环境来开发机器人政策，但从模拟到实际应用的过渡往往会遇到由于模拟中的近似和简化而带来的重大挑战。许多sim-to-real框架[[34](#bib.bib34),
    [64](#bib.bib64), [65](#bib.bib65)]展示了高效性和性能，如附录[B](#A2 "Appendix B Bridging sim-to-real
    gap ‣ Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey")所述。尽管有这些进展，显著的差距依然存在，这种差距因物理环境的复杂性和不可预测性而加剧。此外，许多研究[[26](#bib.bib26),
    [66](#bib.bib66), [21](#bib.bib21)]仍仅在模拟环境中得到验证。'
- en: IV-C4 Loco-manipulation tasks
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C4 运动操控任务
- en: Loco-manipulation, which combines locomotion and manipulation, presents opportunities
    for humanoid robots to excel beyond purely bipedal capabilities. Few studies have
    addressed this integrated task; one such study [[67](#bib.bib67)] demonstrated
    a ’box transportation’ framework. This framework decomposes the task into five
    distinct policies, each addressing different aspects of the transportation process.
    However, this approach lacks efficiency and does not incorporate vision-based
    information, suggesting substantial room for improvement. Moreover, the challenges
    of managing mobile tools like scooters [[68](#bib.bib68)] or dynamically interacting
    with objects such as balls [[69](#bib.bib69)] introduce further complexities.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 运动操控结合了运动和操控，为类人机器人在纯双足能力之外的表现提供了机会。少数研究探讨了这一综合任务；其中一项研究[[67](#bib.bib67)]展示了一个“箱子运输”框架。该框架将任务分解为五个不同的策略，每个策略处理运输过程中的不同方面。然而，这种方法缺乏效率，也没有结合基于视觉的信息，显示出巨大的改进空间。此外，管理如滑板车[[68](#bib.bib68)]等移动工具或与如球体[[69](#bib.bib69)]等对象动态交互的挑战引入了更多复杂性。
- en: Decomposing loco-manipulation tasks into multiple layers could simplify the
    challenges, allowing for more precise and flexible control by manually tuning
    individual components of the task [[43](#bib.bib43)]. This structured control
    approach provides a more coordinated response to complex interactions within the
    robot’s environment, facilitating the execution of task-specific commands.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 将步态操作任务分解为多个层级可以简化挑战，通过手动调节任务的各个组件实现更精确和灵活的控制[[43](#bib.bib43)]。这种结构化控制方法提供了对机器人环境中复杂交互的更协调响应，促进了任务特定命令的执行。
- en: Alternatively, an end-to-end framework may enable bipeds to perform a variety
    of tasks through task randomization and structured curriculum learning methods,
    progressively teaching the policy [[35](#bib.bib35), [27](#bib.bib27), [22](#bib.bib22)].
    During training, such policies can also learn human-like movements from motion
    capture data [[70](#bib.bib70), [18](#bib.bib18), [16](#bib.bib16)], offering
    promising solutions for future integrated loco-manipulation tasks within a single,
    versatile policy.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，一种端到端框架可能通过任务随机化和结构化课程学习方法，使双足机器人能够执行各种任务，逐步教授策略[[35](#bib.bib35), [27](#bib.bib27),
    [22](#bib.bib22)]。在训练过程中，这些策略还可以从运动捕捉数据[[70](#bib.bib70), [18](#bib.bib18), [16](#bib.bib16)]中学习类似于人类的运动，提供对未来集成的步态操作任务的有希望的解决方案。
- en: IV-C5 Designing reward functions
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C5 设计奖励函数
- en: The development of effective reward functions is a critical challenge in the
    field of deep reinforcement learning (DRL) for bipedal robots. While periodic
    reward functions have been designed to facilitate cyclic movements like walking
    [[19](#bib.bib19)], there remains a significant gap in crafting reward functions
    for non-periodic actions such as jumping. These actions require distinct considerations
    for success and efficiency, yet current research lacks comprehensive methods for
    their reward structure. Furthermore, minimizing the need for extensive manual
    tuning while achieving high performance in DRL systems continues to be a substantial
    challenge, pointing to the need for more adaptive and automatically adjusting
    reward mechanisms.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在双足机器人深度强化学习（DRL）领域，开发有效的奖励函数是一项关键挑战。虽然已经设计了周期性奖励函数以促进诸如行走[[19](#bib.bib19)]等周期性运动，但在为非周期性动作如跳跃设计奖励函数方面仍存在显著差距。这些动作需要针对成功和效率的不同考虑，而现有研究在奖励结构方面缺乏全面的方法。此外，在DRL系统中实现高性能而不需要大量手动调节仍然是一个重大挑战，这突显了需要更具适应性和自动调整的奖励机制。
- en: IV-C6 Integrating large language models
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C6 整合大语言模型
- en: The integration of Large Language Models (LLMs) into bipedal robotics opens
    new avenues for contextual understanding and task execution, significantly enhancing
    the robots’ interaction capabilities. LLMs, when implemented at the highest task
    level, offer substantial promise for improving human-robot interaction, making
    these systems more intuitive and responsive [[71](#bib.bib71)]. The potential
    applications of this technology are broad and impactful, spanning sectors such
    as industrial automation, where robots can perform complex assembly tasks; healthcare,
    offering assistance in patient care and rehabilitation; assistive devices, providing
    support for individuals with disabilities; search and rescue operations, where
    robust and adaptive decision-making is critical; and entertainment and education,
    where interactive and engaging experiences are key [[72](#bib.bib72)]. Each of
    these fields could benefit from the advanced capabilities of LLM-enhanced bipedal
    robots, particularly in environments requiring nuanced understanding and adaptability.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 将大语言模型（LLMs）整合到双足机器人中，开辟了对上下文理解和任务执行的新途径，显著增强了机器人与环境的互动能力。LLMs在最高任务层级实施时，为改善人机互动提供了巨大的潜力，使这些系统更加直观和响应灵敏[[71](#bib.bib71)]。这一技术的潜在应用广泛且具有深远影响，涵盖了工业自动化领域，机器人可以执行复杂的装配任务；医疗保健，提供病人护理和康复帮助；辅助设备，支持残疾人士；搜救行动，要求强大的适应性决策；以及娱乐和教育，提供互动和引人入胜的体验[[72](#bib.bib72)]。这些领域中的每一个都可能受益于LLM增强的双足机器人的先进能力，特别是在需要细致理解和适应的环境中。
- en: IV-D Applications in various fields
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 各领域的应用
- en: 'The advancements in bipedal locomotion technology hold significant promise
    for practical applications beyond the confines of laboratory environments. These
    robots, bolstered by AI, are poised to transform numerous sectors by enhancing
    operational capabilities and interaction with humans. The potential for humanoid
    robots in various fields is detailed in [[72](#bib.bib72)], emphasizing the integration
    of learning-based approaches for more effective implementation. Key areas include:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 双足行走技术的进步为超越实验室环境的实际应用带来了重大前景。这些机器人通过人工智能的支持，准备好通过提升操作能力和与人类的互动来改造多个领域。关于类人机器人在各个领域的潜力，详见[[72](#bib.bib72)]，重点强调了基于学习的方法以实现更有效的实施。关键领域包括：
- en: '1.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Industrial automation and manufacturing: The integration of humanoid robots
    in industrial settings can significantly enhance productivity and efficiency,
    freeing workers from repetitive and labor-intensive tasks. These robots, equipped
    with advanced loco-manipulation capabilities and the ability to cooperate with
    human teams, are particularly effective in assembly line operations, maintenance
    tasks, and the construction of complex machinery [[73](#bib.bib73), [74](#bib.bib74)].
    Their articulated arms and floating bases provide unmatched flexibility, making
    them ideal for human-centric manufacturing environments. The humanoid robot Digit,
    for example, demonstrates remarkable stability and efficiency in industrial tasks
    over extended periods, as seen in video demonstrations [[75](#bib.bib75)]. Moreover,
    these robots are also suited for operation in high-risk environments such as underwater
    or areas with high radiation levels, significantly enhancing safety and operational
    capacity in these contexts.'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工业自动化和制造：类人机器人在工业环境中的集成可以显著提高生产力和效率，解放工人免于重复和劳动密集型的任务。这些机器人配备了先进的运动操作能力，并能与人类团队合作，特别适用于装配线操作、维护任务和复杂机械的制造[[73](#bib.bib73),
    [74](#bib.bib74)]。它们的关节式手臂和浮动底座提供了无与伦比的灵活性，使其非常适合以人为中心的制造环境。例如，类人机器人Digit在工业任务中展现了卓越的稳定性和效率，如视频演示所示[[75](#bib.bib75)]。此外，这些机器人也适合在高风险环境中操作，如水下或高辐射区域，显著提升了这些环境中的安全性和操作能力。
- en: '2.'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Healthcare and assistive devices: In the healthcare sector, bipedal and humanoid
    robots contribute significantly to rehabilitation and assistive technologies.
    Exoskeletons enhanced with DRL methodologies are being used to train individuals
    to achieve more natural gait patterns, improving mobility and rehabilitation outcomes
    [[76](#bib.bib76)]. Beyond mere mobility aids, humanoid robots integrated with
    LLMs show promise in delivering medications, monitoring patient health, and assisting
    in surgeries [[77](#bib.bib77)]. The synergy between LLMs and loco-manipulation
    capabilities paves the way for more interactive, responsive support, aligning
    closely with the needs of personalized care. Additionally, the aging population
    can benefit from humanoid robots performing everyday tasks like house cleaning
    or delivery through simple voice commands, thereby enhancing the quality of life.'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医疗和辅助设备：在医疗领域，双足行走和类人机器人在康复和辅助技术中发挥了重要作用。采用深度强化学习（DRL）方法增强的外骨骼正被用于训练个人以实现更自然的步态模式，从而改善移动能力和康复效果[[76](#bib.bib76)]。超越单纯的移动辅助，集成了大型语言模型（LLMs）的类人机器人在药物递送、监测患者健康和协助手术方面展现了前景[[77](#bib.bib77)]。LLMs与运动操作能力的协同作用为更具互动性和响应性的支持铺平了道路，更好地满足个性化护理的需求。此外，老龄化人口可以受益于类人机器人执行日常任务，如通过简单的语音命令进行家庭清洁或配送，从而提高生活质量。
- en: '3.'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Search and rescue missions: Humanoid robots are exceptionally valuable in search
    and rescue operations, especially in disaster-stricken or hazardous environments
    where human presence is risky or impractical. Unlike traditional wheeled robots,
    humanoid robots can navigate complex terrains filled with debris, gaps, and elevated
    structures, making them indispensable in these scenarios. They also demonstrate
    potential for significant interaction and collaboration with human rescue teams.
    For instance, in environments with high nuclear radiation, humanoid robots can
    perform tasks that would be perilous for humans, handling delicate instruments
    and preventing human exposure to harmful conditions. This capability extends to
    other challenging environments such as underwater [[78](#bib.bib78)], outer space
    [[79](#bib.bib79), [80](#bib.bib80)], and other hazardous areas. However, the
    full realization of these applications remains constrained by the absence of a
    unified framework that can seamlessly navigate all terrains and fully integrate
    loco-manipulation and human interaction functionalities.'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 搜索与救援任务：类人机器人在搜索和救援行动中具有特别重要的价值，尤其是在灾区或危险环境中，人类存在的风险或不切实际。与传统的轮式机器人不同，类人机器人能够在充满碎片、缝隙和高架结构的复杂地形中导航，使它们在这些场景中不可或缺。它们还表现出与人类救援队进行重大互动和协作的潜力。例如，在高核辐射环境中，类人机器人可以执行对人类来说危险的任务，操作精密仪器并防止人类暴露于有害环境中。这种能力还扩展到其他具有挑战性的环境，如水下[[78](#bib.bib78)]、外太空[[79](#bib.bib79),
    [80](#bib.bib80)]和其他危险区域。然而，全面实现这些应用仍受限于缺乏一个可以无缝导航所有地形并完全整合运动操作和人机交互功能的统一框架。
- en: '4.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Entertainment and education: Humanoid robots have the potential to transform
    the realms of entertainment and education by providing highly interactive experiences.
    With their ability to integrate extensive knowledge bases, these robots can significantly
    enhance educational environments. They can assume the roles of butlers, teachers,
    or even babysitters, engaging with users in diverse activities. For example, robots
    can facilitate language learning [[81](#bib.bib81)], participate in storytelling,
    teach various academic subjects, or engage in the performing arts and games. In
    the sphere of entertainment, humanoid robots can act, dance, play ball games [[69](#bib.bib69)],
    and take part in interactive performances, captivating audiences of all ages with
    their versatility and dynamic capabilities.'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 娱乐与教育：类人机器人有潜力通过提供高度互动的体验来改变娱乐和教育领域。凭借其整合广泛知识库的能力，这些机器人可以显著提升教育环境。它们可以担任管家、教师甚至保姆的角色，与用户进行各种活动互动。例如，机器人可以促进语言学习[[81](#bib.bib81)]，参与讲故事，教授各种学科，或参与表演艺术和游戏。在娱乐领域，类人机器人可以表演、跳舞、玩球类游戏[[69](#bib.bib69)]，并参与互动表演，以其多才多艺和动态能力吸引各年龄段的观众。
- en: However, this in turn leads to a variety of ethical issues. First, interacting
    with humans involves collecting human daily behavior data and increases the risk
    of a data breach. Second, another concern is the increasing dependency of humans
    on robots, not just for assistance but also for emotional support. This will result
    in less human-to-human interaction and ultimately affect social constructs and
    emotional development. Third, advancements of humanoid robots will replace humans
    in various jobs, and eventually lead to unemployment issues.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，这也带来了各种伦理问题。首先，与人类互动涉及收集人类日常行为数据，增加了数据泄露的风险。其次，另一个担忧是人类对机器人的依赖日益增加，不仅仅是为了帮助，还有情感支持。这将导致人际互动减少，*最终影响社会结构和情感发展*。第三，类人机器人的进步将取代人类在各种工作中的角色，并最终导致失业问题。
- en: On the positive side, humanoid robots can provide invaluable assistance to people
    with disabilities or the elderly, offering companionship and reducing the care
    burden on families and healthcare systems. Furthermore, their application across
    diverse fields such as education, industry, and healthcare can bring about revolutionary
    changes, improving efficiency and safety while opening up new possibilities for
    technological integration. As we navigate these advancements, it is crucial to
    balance innovation with ethical considerations to ensure that the deployment of
    humanoid robots enhances societal well-being without compromising personal integrity
    or social dynamics.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 积极的一面是，人形机器人可以为残疾人士或老年人提供宝贵的帮助，提供陪伴，减轻家庭和医疗系统的护理负担。此外，它们在教育、工业和医疗等各个领域的应用可以带来革命性的变化，提高效率和安全，同时开辟技术集成的新可能性。在我们推进这些进展时，平衡创新与伦理考虑至关重要，以确保人形机器人的部署能提升社会福祉，而不影响个人诚信或社会动态。
- en: V Conclusion
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: 'Despite significant advances in DRL for robotics, a considerable gap persists
    between current research achievements and the development of a unified framework
    capable of empowering robots to perform a broad spectrum of complex tasks efficiently.
    Presently, DRL research can be categorized into two primary control schemes: end-to-end
    and hierarchical frameworks. End-to-end frameworks have shown promising capabilities
    in executing diverse locomotion skills [[22](#bib.bib22)], climbing stairs [[38](#bib.bib38)],
    and navigating challenging terrains such as stepping stones [[20](#bib.bib20)].
    Conversely, hybrid frameworks, which often integrate an HL planner or an LL model-based
    controller, offer enhanced capabilities, allowing for simultaneous management
    of locomotion and navigation tasks.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器人领域的深度强化学习（DRL）取得了显著进展，但当前研究成果与开发一个能够高效执行广泛复杂任务的统一框架之间仍存在相当大的差距。目前，DRL 研究可以分为两种主要控制方案：端到端和分层框架。端到端框架在执行多样的运动技能[[22](#bib.bib22)]、爬楼梯[[38](#bib.bib38)]以及在如石阶[[20](#bib.bib20)]等挑战性地形中导航方面显示出了有希望的能力。相反，混合框架通常集成了
    HL 规划器或 LL 模型基础的控制器，提供了增强的能力，使得可以同时管理运动和导航任务。
- en: To bridge the existing gaps, further development of hierarchical frameworks,
    particularly those equipped with advanced perception systems and integrated with
    model-based planners, appears promising. Such frameworks could simultaneously
    address issues of precision and generalization. Moreover, the advent of LLMs presents
    a transformative opportunity, potentially enabling the unification of language
    processing and visual functionalities within robotic systems. While numerous challenges
    remain—ranging from the technical intricacies of framework integration to real-world
    application—the steady progression in control framework refinement and DRL development
    provides a hopeful outlook. The vision of achieving an end-to-end unified framework,
    capable of mimicking human-like learning processes and enabling bipedal robots
    to handle a wide range of complex tasks, may soon move within reach.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补现有的差距，进一步开发分层框架，特别是那些配备先进感知系统并与基于模型的规划器集成的框架，似乎颇具前景。这些框架可以同时解决精度和泛化问题。此外，大型语言模型（LLMs）的出现提供了一个变革性的机会，可能使语言处理和视觉功能在机器人系统中得以统一。尽管仍面临许多挑战——从框架集成的技术复杂性到实际应用——但在控制框架优化和
    DRL 发展方面的持续进展提供了一个充满希望的前景。实现一个端到端的统一框架，能够模拟类人学习过程并使双足机器人处理各种复杂任务的愿景，可能很快就会成为现实。
- en: Appendix A Deep reinforcement learning algorithms
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 深度强化学习算法
- en: The advancement and development of RL is crucial for bipedal locomotion. Specifically,
    advancements in deep learning provide deep neural networks serving as function
    approximators to empower RL with the capability to handle tasks characterized
    by high-dimensional and continuous spaces, by efficiently discovering condensed,
    low-dimensional representations of complex data. In comparison to other robots
    of different morphologies, such as wheeled robots, bipedal robots feature much
    higher DoFs and continuously interact with environments, which results in higher
    requirements for the DRL algorithms. Especially in the legged locomotion field,
    policy gradient-based algorithms are prevalent in the field of bipedal locomotion.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: RL 的进步和发展对于双足运动至关重要。具体来说，深度学习的进展提供了作为函数逼近器的深度神经网络，使 RL 具备处理高维和连续空间任务的能力，通过高效地发现复杂数据的浓缩低维表示。与其他不同形态的机器人（如轮式机器人）相比，双足机器人具有更高的自由度，并且持续与环境互动，这导致对
    DRL 算法的要求更高。特别是在腿部运动领域，基于策略梯度的算法在双足运动领域中较为普遍。
- en: Designing an effective neural network architecture is essential for tackling
    complex bipedal locomotion tasks. Multi-layer perceptrons (MLP), a fundamental
    neural network, excel in straightforward regression tasks with lower computational
    resource demands. A comprehensive comparison between MLP and the memory-based
    neural network, Long Short-Term Memory (LSTM) reveals that MLPs have an advantage
    in convergence speed for tasks [[65](#bib.bib65)]. However, LSTM, as a variant
    of Recurrent Neural Networks (RNN), is adept at processing data associated with
    time, effectively relating different states across time, and modeling key physical
    properties vital for periodical gaits [[19](#bib.bib19)] and successful sim-to-real
    transfer in bipedal locomotion. Additionally, Convolutional Neural Networks (CNN)
    specialize in spatial data processing, particularly for image-related tasks, making
    them highly suitable for environments where visual perception is crucial. This
    diverse range of neural network architectures highlights the importance of selecting
    the appropriate model based on the specific requirements of the bipedal locomotion
    tasks.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 设计有效的神经网络架构对于处理复杂的双足运动任务至关重要。多层感知器（MLP），一种基本的神经网络，在简单回归任务中表现优异，并且对计算资源的需求较低。全面比较
    MLP 和基于记忆的神经网络，即长短期记忆网络（LSTM），揭示了 MLP 在任务的收敛速度上具有优势[[65](#bib.bib65)]。然而，LSTM
    作为递归神经网络（RNN）的变种，擅长处理与时间相关的数据，有效地关联不同的时间状态，并建模对周期性步态至关重要的关键物理属性[[19](#bib.bib19)]，以及双足运动中成功的模拟到现实转移。此外，卷积神经网络（CNN）专注于空间数据处理，特别是与图像相关的任务，使其在视觉感知至关重要的环境中非常合适。这些多样的神经网络架构突显了根据双足运动任务的具体要求选择适当模型的重要性。
- en: Considering DRL alogirthms, recent bipedal locomotion studies focus on model-free
    reinforcement algorithms. Unlike model-based RL, which learns a model of the environment
    but may inherit biases from simulations that do not accurately reflect real-world
    conditions, model-free RL directly trains policies through environmental interaction
    without relying on an explicit environmental model. Although model-free RL requires
    more computational samples and resources, it can train a more robust policy allowing
    the robots to travel around challenging environments.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 DRL 算法，最近的双足运动研究集中于无模型强化算法。与基于模型的 RL 不同，基于模型的 RL 学习环境模型，但可能会继承来自与现实世界条件不完全相符的模拟中的偏差，无模型
    RL 通过环境交互直接训练策略，而不依赖于明确的环境模型。虽然无模型 RL 需要更多的计算样本和资源，但它可以训练出更强大的策略，使机器人能够在具有挑战性的环境中自由行走。
- en: 'Many sophisticated model-free RL algorithms exist, which can be broadly classified
    into two categories: policy-based (or policy optimization) and value-based approaches.
    Value-based methods e.g. Q-learning, Deep Q-learning (DQN) [[82](#bib.bib82)]
    only excel in discrete action space and often struggle with high dimensional action
    space. In contrast, policy-based methods, such as policy gradient, can handle
    complex tasks but are generally less sample-efficient compared to value-based
    methods.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 许多复杂的无模型 RL 算法存在，通常可以广泛分为两类：基于策略（或策略优化）和基于价值的方法。基于价值的方法，如 Q 学习、深度 Q 学习（DQN）[[82](#bib.bib82)]，仅在离散动作空间中表现优异，并且通常在高维动作空间中表现欠佳。相比之下，基于策略的方法，如策略梯度，可以处理复杂任务，但通常比基于价值的方法的样本效率低。
- en: '![Refer to caption](img/9b2dbd0e4a7fcc815e321220fa64b137.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9b2dbd0e4a7fcc815e321220fa64b137.png)'
- en: 'Figure 4: Diagram for RL algorithms catalogue'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：RL算法目录图
- en: 'More advanced algorithms combine both policy-based methods and value-based
    methods. Actor-critic (AC) refers to a main idea simultaneously learning both
    a policy (actor) and a value function (critic), where it owns both advantages
    of both algorithms [[83](#bib.bib83), [84](#bib.bib84)]. Popular algorithms e.g.
    Trust region policy optimization (TRPO) [[85](#bib.bib85)] and PPO based on policy-based
    methods, borrow ideas from AC. Moreover, there are other novel algorithms based
    on the AC framework, Deep Deterministic Policy Gradient (DDPG) [[86](#bib.bib86)],
    Twin Delayed Deep Deterministic Policy Gradients (TD3) [[87](#bib.bib87)], A2C
    (Advantage Actor-Critic), and A3C (Asynchronous Advantage Actor-Critic) [[88](#bib.bib88)],
    SAC (Soft Actor-Critic) [[89](#bib.bib89)]. Each algorithm has its strengths considering
    different tasks in the bipedal locomotion scenario. There are several key factors
    to value these algorithms such as: sample efficiency, robustness and generalization,
    and implementation challenges. A comparative analysis work [[90](#bib.bib90)]
    illustrates that SAC-based algorithms excel in stability and achieve the highest
    scores, while their training efficiency significantly trails behind that of PPO
    that obtain relatively high score.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 更高级的算法结合了基于策略的方法和基于价值的方法。演员-评论家（AC）指的是一种主要思想，即同时学习一个策略（演员）和一个价值函数（评论家），它拥有两种算法的优点[[83](#bib.bib83),
    [84](#bib.bib84)]。例如，基于策略的方法中的流行算法，如信任区域策略优化（TRPO）[[85](#bib.bib85)]和PPO，借鉴了AC的思想。此外，还有其他基于AC框架的新颖算法，如深度确定性策略梯度（DDPG）[[86](#bib.bib86)]、双延迟深度确定性策略梯度（TD3）[[87](#bib.bib87)]、A2C（优势演员-评论家）和A3C（异步优势演员-评论家）[[88](#bib.bib88)]、SAC（软演员-评论家）[[89](#bib.bib89)]。每种算法在双足步态场景下考虑不同任务时都有其优势。评价这些算法的几个关键因素包括：样本效率、鲁棒性和泛化能力，以及实施挑战。一项比较分析工作[[90](#bib.bib90)]表明，基于SAC的算法在稳定性方面表现优越，获得了最高分数，但其训练效率显著落后于获得相对较高分数的PPO。
- en: In [[91](#bib.bib91)], PPO demonstrates the robustness and computational economy
    in complex scenarios, such as bipedal locomotion, utilizing fewer resources than
    TRPO. In terms of training time, PPO is much faster than SAC, and DDPG algorithms
    [[90](#bib.bib90)]. Besides, many works [[19](#bib.bib19), [45](#bib.bib45), [36](#bib.bib36)]
    have demonstrated its robustness and ease of implementation and combined with
    the flexibility to integrate with various neural network architectures have made
    PPO the most popular choice in this field. Various work has demonstrated that
    PPO can conduct the exploration of walking [[19](#bib.bib19)], jumping [[37](#bib.bib37)],
    stair climbing [[38](#bib.bib38)], and stepping stones [[20](#bib.bib20)], which
    demonstrates its efficiency, robustness and generalization.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[91](#bib.bib91)]中，PPO展示了在复杂场景下（如双足步态）对资源的经济性和鲁棒性，利用的资源比TRPO少。在训练时间方面，PPO远比SAC和DDPG算法[[90](#bib.bib90)]快。此外，许多研究[[19](#bib.bib19),
    [45](#bib.bib45), [36](#bib.bib36)]已经证明了它的鲁棒性和易于实施，并且与各种神经网络架构的灵活集成使得PPO成为该领域最受欢迎的选择。各种研究表明，PPO可以进行步态[[19](#bib.bib19)]、跳跃[[37](#bib.bib37)]、爬楼梯[[38](#bib.bib38)]和踏石[[20](#bib.bib20)]等探索，展示了它的效率、鲁棒性和泛化能力。
- en: Additionally, the DDPG algorithm integrates the Actor-Critic framework with
    DQN to facilitate off-policy training, further optimizing sampling efficiency.
    In some explicit scenarios such as jumping, DDPG shows higher reward and better
    learning performance than PPO [[21](#bib.bib21), [92](#bib.bib92)]. TD3 is developed
    based on DDPG, and improve over the performance of the DDPG and SAC [[89](#bib.bib89)].
    Soft Actor-Critic (SAC) further the agent’s exploration capabilities and sample
    efficiency [[89](#bib.bib89)]. While A2C offers improved efficiency and stability
    compared to A3C, the asynchronous update mechanism for A3C provides better capabilities
    for exploration and accelerating learning. Although these algorithms show their
    advancements, they are more challenging to apply due to algorithms’ complexity
    compared to PPO.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DDPG算法将演员-评论家框架与DQN集成，以促进离策略训练，进一步优化了采样效率。在一些明确的场景下，如跳跃，DDPG显示出比PPO更高的奖励和更好的学习性能[[21](#bib.bib21),
    [92](#bib.bib92)]。TD3是在DDPG的基础上开发的，改进了DDPG和SAC的性能[[89](#bib.bib89)]。软演员-评论家（SAC）进一步提升了代理的探索能力和样本效率[[89](#bib.bib89)]。虽然A2C相比A3C提供了更好的效率和稳定性，但A3C的异步更新机制提供了更好的探索能力和加速学习的能力。尽管这些算法显示出它们的进步，但由于算法复杂性，相比于PPO，它们的应用更加具有挑战性。
- en: Appendix B Bridging sim-to-real gap
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 弥合模拟到现实的差距
- en: Due to the large number of interactions needed for RL algorithms, training directly
    on robots can lead to costly damage to hardware and the environment. Consequently,
    training a policy in the simulation and then deploying it to the hardware illustrates
    significant potential and efficiency. However, the gap between simulation and
    the real world remains substantial, making sim-to-real challenging. To overcome
    the gap, several sim-to-real approaches are developed, including dynamics randomization
    [[36](#bib.bib36)], system identification [[93](#bib.bib93), [94](#bib.bib94)],
    periodic reward composition [[66](#bib.bib66)], learned actuators dynamics [[93](#bib.bib93),
    [95](#bib.bib95)], regulation feedback controller [[96](#bib.bib96)], adversarial
    motion prior [[18](#bib.bib18), [17](#bib.bib17)].
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强化学习算法需要大量交互，直接在机器人上训练可能会导致硬件和环境的昂贵损坏。因此，在仿真中训练策略然后再部署到硬件上，显示出了显著的潜力和效率。然而，仿真和现实世界之间的差距仍然很大，使得从仿真到现实变得具有挑战性。为克服这一差距，开发了几种从仿真到现实的方法，包括动态随机化
    [[36](#bib.bib36)]、系统识别 [[93](#bib.bib93), [94](#bib.bib94)]、周期性奖励组合 [[66](#bib.bib66)]、学习型执行器动态
    [[93](#bib.bib93), [95](#bib.bib95)]、调节反馈控制器 [[96](#bib.bib96)] 和对抗性运动先验 [[18](#bib.bib18),
    [17](#bib.bib17)]。
- en: There are two primary approaches to training policies under domain randomization.
    One is end-to-end training with a history of robot measurements or I/O [[36](#bib.bib36)]
    and another is policy distillation, an expert policy with environmental insights
    guides a student policy that learns from internal sensory feedback, such as teacher-student
    policy [[66](#bib.bib66)], RMA [[64](#bib.bib64)].
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在领域随机化下训练策略有两种主要方法。一种是端到端训练，使用机器人测量历史或输入/输出 [[36](#bib.bib36)]，另一种是策略蒸馏，具有环境洞察的专家策略指导一个从内部传感反馈中学习的学生策略，例如教师-学生策略
    [[66](#bib.bib66)]，RMA [[64](#bib.bib64)]。
- en: 'Details of these sim-to-real transition approaches are shown below:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这些从仿真到现实过渡方法的详细信息：
- en: 1) The dynamics randomization method involves systematically varying the physical
    parameters of the simulated environment-such as mass, inertia, or stiffness. 2)
    System Identification methods develop mathematical models of dynamics from observed
    data, enhancing the accuracy of robots’ properties within the models, such as
    mass and inertia, to ensure the model faithfully represents the system’s behavior.
    3) The learned actuator dynamics method utilizes experimental data from actuators
    to develop a model of their dynamics, achieving sim-to-real by incorporating realistic
    actuator behavior within the training environment. It is noticeable that the higher-level
    planner can also learn from reference or non-reference. 4) periodic reward composition
    helps capture the essential locomotion information and the periodic gait pattern
    is more general to adapt to uncertainty and variation in the real world. 5) The
    regulation feedback controller manually tunes the setting of the controller to
    mitigate perturbations and gaps between the sim and the real, thereby enhancing
    the robustness and adaptation. Key aspects of sim-to-real, including system identification,
    state estimation with noise measurement, and the selection of state-and-action
    spaces are highlighted in [[34](#bib.bib34)].
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 动态随机化方法涉及系统地改变仿真环境的物理参数，如质量、惯性或刚度。2) 系统识别方法从观察到的数据中开发动力学的数学模型，提高模型中机器人属性（如质量和惯性）的准确性，以确保模型忠实地表示系统的行为。3)
    学习型执行器动态方法利用来自执行器的实验数据开发其动态模型，通过在训练环境中融入现实的执行器行为来实现从仿真到现实。值得注意的是，高级规划器也可以从参考或非参考中学习。4)
    周期性奖励组合帮助捕捉关键的运动信息，周期性步态模式更通用，以适应现实世界中的不确定性和变化。5) 调节反馈控制器手动调整控制器的设置，以减轻仿真与现实之间的扰动和差距，从而提高稳健性和适应性。从仿真到现实的关键方面，包括系统识别、带噪声测量的状态估计以及状态和动作空间的选择，已在
    [[34](#bib.bib34)] 中强调。
- en: References
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. Gupta and A. Kumar, “A brief review of dynamics and control of underactuated
    biped robots,” *Advanced Robotics*, vol. 31, pp. 607–623, 2017.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] S. Gupta 和 A. Kumar, “对欠驱动双足机器人动力学和控制的简要回顾，” *Advanced Robotics*，第31卷，第607–623页，2017年。'
- en: '[2] J. Reher and A. Ames, “Dynamic walking: Toward agile and efficient bipedal
    robots,” *Annual Review of Control, Robotics, and Autonomous Systems*, vol. 4,
    2021.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J. Reher 和 A. Ames, “动态步态：朝着灵活高效的双足机器人迈进，” *Annual Review of Control, Robotics,
    and Autonomous Systems*，第4卷，2021年。'
- en: '[3] J. Carpentier and P.-B. Wieber, “Recent progress in legged robots locomotion
    control,” *Current Robotics Reports*, vol. 2, pp. 231–238, 2021.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] J. Carpentier 和 P.-B. Wieber，“腿部机器人运动控制的最新进展，” *当前机器人报告*，第2卷，第231–238页，2021年。'
- en: '[4] M. A.-M. Khan, M. R. J. Khan, A. Tooshil, N. Sikder, M. A. P. Mahmud, A. Z.
    Kouzani, and A.-A. Nahid, “A systematic review on reinforcement learning-based
    robotics within the last decade,” *IEEE Access*, vol. 8, pp. 176 598–176 623,
    2020.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. A.-M. Khan, M. R. J. Khan, A. Tooshil, N. Sikder, M. A. P. Mahmud, A. Z.
    Kouzani 和 A.-A. Nahid，“过去十年基于强化学习的机器人系统综述，” *IEEE Access*，第8卷，第176 598–176 623页，2020年。'
- en: '[5] J. García and D. Shafie, “Teaching a humanoid robot to walk faster through
    safe reinforcement learning,” *Engineering Applications of Artificial Intelligence*,
    vol. 88, p. 103360, 2020.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. García 和 D. Shafie，“通过安全强化学习教会类人机器人更快行走，” *工程应用人工智能*，第88卷，第103360页，2020年。'
- en: '[6] C. Chevallereau, G. Abba, Y. Aoustin, F. Plestan, E. Westervelt, C. C.
    De Wit, and J. Grizzle, “Rabbit: A testbed for advanced control theory,” *IEEE
    Control Systems Magazine*, vol. 23, pp. 57–79, 2003.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] C. Chevallereau, G. Abba, Y. Aoustin, F. Plestan, E. Westervelt, C. C.
    De Wit 和 J. Grizzle，“Rabbit：一个先进控制理论的测试平台，” *IEEE控制系统杂志*，第23卷，第57–79页，2003年。'
- en: '[7] Y. Gong, R. Hartley, X. Da, A. Hereid, O. Harib, J.-K. Huang, and J. Grizzle,
    “Feedback control of a cassie bipedal robot: Walking, standing, and riding a segway,”
    in *American Control Conference*, 2019, pp. 4559–4566.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Gong, R. Hartley, X. Da, A. Hereid, O. Harib, J.-K. Huang 和 J. Grizzle，“Cassie双足机器人反馈控制：行走、站立和骑行，”
    发表在 *美国控制会议*，2019年，pp. 4559–4566。'
- en: '[8] S. Kuindersma, R. Deits, M. Fallon, A. Valenzuela, H. Dai, F. Permenter,
    T. Koolen, P. Marion, and R. Tedrake, “Optimization-based locomotion planning,
    estimation, and control design for the atlas humanoid robot,” *Autonomous robots*,
    vol. 40, pp. 429–455, 2016.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] S. Kuindersma, R. Deits, M. Fallon, A. Valenzuela, H. Dai, F. Permenter,
    T. Koolen, P. Marion 和 R. Tedrake，“基于优化的运动规划、估计和控制设计，应用于Atlas类人机器人，” *自主机器人*，第40卷，第429–455页，2016年。'
- en: '[9] G. A. Castillo, B. Weng, W. Zhang, and A. Hereid, “Robust feedback motion
    policy design using reinforcement learning on a 3d digit bipedal robot,” in *IEEE/RSJ
    International Conference on Intelligent Robots and Systems*, 2021, pp. 5136–5143.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] G. A. Castillo, B. Weng, W. Zhang 和 A. Hereid，“在3D数字双足机器人上使用强化学习设计鲁棒的反馈运动策略，”
    发表在 *IEEE/RSJ国际智能机器人与系统会议*，2021年，pp. 5136–5143。'
- en: '[10] R. Tedrake, T. Zhang, and H. Seung, “Stochastic policy gradient reinforcement
    learning on a simple 3D biped,” in *IEEE/RSJ International Conference on Intelligent
    Robots and Systems*, 2004, pp. 2849–2854.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] R. Tedrake, T. Zhang 和 H. Seung，“在简单的3D双足机器人上进行随机策略梯度强化学习，” 发表在 *IEEE/RSJ国际智能机器人与系统会议*，2004年，pp.
    2849–2854。'
- en: '[11] J. Morimoto, G. Cheng, C. Atkeson, and G. Zeglin, “A simple reinforcement
    learning algorithm for biped walking,” in *IEEE International Conference on Robotics
    and Automation*, 2004, pp. 3030–3035 Vol.3.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Morimoto, G. Cheng, C. Atkeson 和 G. Zeglin，“一个简单的双足行走强化学习算法，” 发表在 *IEEE国际机器人与自动化会议*，2004年，pp.
    3030–3035 第3卷。'
- en: '[12] X. Peng, G. Berseth, K. Yin, and M. Panne, “DeepLoco: dynamic locomotion
    skills using hierarchical deep reinforcement learning,” *ACM Transactions on Graphics*,
    vol. 36, pp. 1–13, 2017.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] X. Peng, G. Berseth, K. Yin 和 M. Panne，“DeepLoco：使用层级深度强化学习的动态运动技能，” *ACM图形学会期刊*，第36卷，第1–13页，2017年。'
- en: '[13] X. Peng, P. Abbeel, S. Levine, and M. Panne, “DeepMimic: Example-guided
    deep reinforcement learning of physics-based character skills,” *ACM Transactions
    on Graphics*, vol. 37, 2018.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] X. Peng, P. Abbeel, S. Levine 和 M. Panne，“DeepMimic：基于示例的深度强化学习物理角色技能，”
    *ACM图形学会期刊*，第37卷，2018年。'
- en: '[14] W. Yu, G. Turk, and C. K. Liu, “Learning symmetric and low-energy locomotion,”
    *ACM Transactions on Graphics*, vol. 37, pp. 1–12, 2018.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] W. Yu, G. Turk 和 C. K. Liu，“学习对称和低能耗的运动，” *ACM图形学会期刊*，第37卷，第1–12页，2018年。'
- en: '[15] M. Taylor, S. Bashkirov, J. F. Rico, I. Toriyama, N. Miyada, H. Yanagisawa,
    and K. Ishizuka, “Learning bipedal robot locomotion from human movement,” in *IEEE
    International Conference on Robotics and Automation*, 2021, pp. 2797–2803.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. Taylor, S. Bashkirov, J. F. Rico, I. Toriyama, N. Miyada, H. Yanagisawa
    和 K. Ishizuka，“从人类运动中学习双足机器人运动，” 发表在 *IEEE国际机器人与自动化会议*，2021年，pp. 2797–2803。'
- en: '[16] X. Cheng, Y. Ji, J. Chen, R. Yang, G. Yang, and X. Wang, “Expressive whole-body
    control for humanoid robots,” *arXiv preprint arXiv:2402.16796*, 2024.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] X. Cheng, Y. Ji, J. Chen, R. Yang, G. Yang 和 X. Wang，“类人机器人的表达性全身控制，”
    *arXiv预印本 arXiv:2402.16796*，2024年。'
- en: '[17] A. Tang, T. Hiraoka, N. Hiraoka, F. Shi, K. Kawaharazuka, K. Kojima, K. Okada,
    and M. Inaba, “HumanMimic: Learning natural locomotion and transitions for humanoid
    robot via wasserstein adversarial imitation,” *arXiv preprint arXiv:2309.14225*,
    2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Tang, T. Hiraoka, N. Hiraoka, F. Shi, K. Kawaharazuka, K. Kojima, K.
    Okada 和 M. Inaba，“HumanMimic：通过 Wasserstein 对抗模仿学习自然运动和过渡以适应人形机器人，” *arXiv 预印本
    arXiv:2309.14225*，2023 年。'
- en: '[18] Q. Zhang, P. Cui, D. Yan, J. Sun, Y. Duan, A. Zhang, and R. Xu, “Whole-body
    humanoid robot locomotion with human reference,” *arXiv preprint arXiv:2402.18294*,
    2024.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Q. Zhang, P. Cui, D. Yan, J. Sun, Y. Duan, A. Zhang 和 R. Xu，“具有人体参考的全身人形机器人运动，”
    *arXiv 预印本 arXiv:2402.18294*，2024 年。'
- en: '[19] J. Siekmann, Y. Godse, A. Fern, and J. Hurst, “Sim-to-real learning of
    all common bipedal gaits via periodic reward composition,” in *IEEE International
    Conference on Robotics and Automation*, 2021, pp. 7309–7315.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Siekmann, Y. Godse, A. Fern 和 J. Hurst，“通过周期性奖励组合进行的所有常见双足步态的模拟到现实学习，”
    见于 *IEEE 国际机器人与自动化大会*，2021 年，第 7309–7315 页。'
- en: '[20] H. Duan, A. Malik, M. S. Gadde, J. Dao, A. Fern, and J. Hurst, “Learning
    dynamic bipedal walking across stepping stones,” in *IEEE/RSJ International Conference
    on Intelligent Robots and Systems*, 2022, pp. 6746–6752.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] H. Duan, A. Malik, M. S. Gadde, J. Dao, A. Fern 和 J. Hurst，“在跨越石块时学习动态双足行走，”
    见于 *IEEE/RSJ 国际智能机器人与系统大会*，2022 年，第 6746–6752 页。'
- en: '[21] C. Tao, M. Li, F. Cao, Z. Gao, and Z. Zhang, “A multiobjective collaborative
    deep reinforcement learning algorithm for jumping optimization of bipedal robot,”
    *Advanced Intelligent Systems*, vol. 6, p. 2300352, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] C. Tao, M. Li, F. Cao, Z. Gao 和 Z. Zhang，“用于双足机器人跳跃优化的多目标协作深度强化学习算法，”
    *高级智能系统*，第 6 卷，第 2300352 页，2023 年。'
- en: '[22] Z. Li, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and K. Sreenath,
    “Reinforcement learning for versatile, dynamic, and robust bipedal locomotion
    control,” *arXiv e-prints*, pp. arXiv–2401, 2024.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Z. Li, X. B. Peng, P. Abbeel, S. Levine, G. Berseth 和 K. Sreenath，“用于多功能、动态和鲁棒的双足运动控制的强化学习，”
    *arXiv e-prints*，第 arXiv–2401 页，2024 年。'
- en: '[23] T. Li, H. Geyer, C. G. Atkeson, and A. Rai, “Using deep reinforcement
    learning to learn high-level policies on the ATRIAS biped,” in *International
    Conference on Robotics and Automation*, 2019, pp. 263–269.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] T. Li, H. Geyer, C. G. Atkeson 和 A. Rai，“使用深度强化学习在 ATRIAS 双足上学习高级策略，”
    见于 *国际机器人与自动化大会*，2019 年，第 263–269 页。'
- en: '[24] H. Duan, J. Dao, K. Green, T. Apgar, A. Fern, and J. Hurst, “Learning
    task space actions for bipedal locomotion,” in *IEEE International Conference
    on Robotics and Automation*, 2021, pp. 1276–1282.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] H. Duan, J. Dao, K. Green, T. Apgar, A. Fern 和 J. Hurst，“为双足运动学习任务空间动作，”
    见于 *IEEE 国际机器人与自动化大会*，2021 年，第 1276–1282 页。'
- en: '[25] G. A. Castillo, B. Weng, W. Zhang, and A. Hereid, “Reinforcement learning-based
    cascade motion policy design for robust 3d bipedal locomotion,” *IEEE Access*,
    vol. 10, pp. 20 135–20 148, 2022.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] G. A. Castillo, B. Weng, W. Zhang 和 A. Hereid，“基于强化学习的级联运动策略设计用于鲁棒的 3D
    双足运动，” *IEEE Access*，第 10 卷，第 20 135–20 148 页，2022 年。'
- en: '[26] R. P. Singh, M. Benallegue, M. Morisawa, R. Cisneros, and F. Kanehiro,
    “Learning bipedal walking on planned footsteps for humanoid robots,” in *IEEE-RAS
    International Conference on Humanoid Robots*, 2022, pp. 686–693.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] R. P. Singh, M. Benallegue, M. Morisawa, R. Cisneros 和 F. Kanehiro，“在规划步伐上学习双足行走以适应人形机器人，”
    见于 *IEEE-RAS 国际人形机器人大会*，2022 年，第 686–693 页。'
- en: '[27] S. Wang, S. Piao, X. Leng, and Z. He, “Learning 3D bipedal walking with
    planned footsteps and fourier series periodic gait planning,” *Sensors*, vol. 23,
    p. 1873, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Wang, S. Piao, X. Leng 和 Z. He， “利用规划步伐和傅里叶级数周期步态规划学习 3D 双足行走，” *传感器*，第
    23 卷，第 1873 页，2023 年。'
- en: '[28] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep
    reinforcement learning: A brief survey,” *IEEE Signal Processing Magazine*, vol. 34,
    pp. 26–38, 2017.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] K. Arulkumaran, M. P. Deisenroth, M. Brundage 和 A. A. Bharath，“深度强化学习：简要调查，”
    *IEEE 信号处理杂志*，第 34 卷，第 26–38 页，2017 年。'
- en: '[29] W. Zhu and M. Hayashibe, “A hierarchical deep reinforcement learning framework
    with high efficiency and generalization for fast and safe navigation,” *IEEE Transactions
    on Industrial Electronics*, vol. 70, pp. 4962–4971, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] W. Zhu 和 M. Hayashibe，“具有高效性和泛化能力的层次深度强化学习框架用于快速和安全的导航，” *IEEE 工业电子学报*，第
    70 卷，第 4962–4971 页，2023 年。'
- en: '[30] X. B. Peng and M. Van De Panne, “Learning locomotion skills using deeprl:
    Does the choice of action space matter?” in *ACM SIGGRAPH/Eurographics Symposium
    on Computer Animation*, 2017, pp. 1–13.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] X. B. Peng 和 M. Van De Panne，“使用深度强化学习学习运动技能：行动空间的选择是否重要？” 见于 *ACM SIGGRAPH/Eurographics
    计算机动画研讨会*，2017 年，第 1–13 页。'
- en: '[31] Z. Li, X. Cheng, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and K. Sreenath,
    “Reinforcement learning for robust parameterized locomotion control of bipedal
    robots,” in *IEEE International Conference on Robotics and Automation*, 2021,
    pp. 2811–2817.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Z. Li, X. Cheng, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, 和 K. Sreenath，“用于双足机器人的鲁棒参数化运动控制的强化学习，”在
    *IEEE 国际机器人与自动化会议*，2021年，页码 2811–2817。'
- en: '[32] D. Kim, G. Berseth, M. Schwartz, and J. Park, “Torque-based deep reinforcement
    learning for task-and-robot agnostic learning on bipedal robots using sim-to-real
    transfer,” *IEEE Robotics and Automation Letters*, vol. 8, p. 6251–6258, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] D. Kim, G. Berseth, M. Schwartz, 和 J. Park，“基于扭矩的深度强化学习用于双足机器人任务和机器人无关学习，使用仿真到现实转移，”*IEEE
    Robotics and Automation Letters*，第 8 卷，页码 6251–6258，2023年。'
- en: '[33] Z. Xie, G. Berseth, P. Clary, J. Hurst, and M. van de Panne, “Feedback
    control for cassie with deep reinforcement learning,” in *IEEE/RSJ International
    Conference on Intelligent Robots and Systems*, 2018, pp. 1241–1246.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Z. Xie, G. Berseth, P. Clary, J. Hurst, 和 M. van de Panne，“针对 Cassie 的反馈控制与深度强化学习，”在
    *IEEE/RSJ 国际智能机器人与系统会议*，2018年，页码 1241–1246。'
- en: '[34] Z. Xie, P. Clary, J. Dao, P. Morais, J. Hurst, and M. van de Panne, “Learning
    locomotion skills for cassie: Iterative design and sim-to-real,” in *Conference
    on Robot Learning*, 2020, pp. 317–329.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Z. Xie, P. Clary, J. Dao, P. Morais, J. Hurst, 和 M. van de Panne，“为 Cassie
    学习运动技能：迭代设计和仿真到现实，”在 *机器人学习会议*，2020年，页码 317–329。'
- en: '[35] D. Rodriguez and S. Behnke, “Deepwalk: Omnidirectional bipedal gait by
    deep reinforcement learning,” in *IEEE International Conference on Robotics and
    Automation*, 2021, pp. 3033–3039.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] D. Rodriguez 和 S. Behnke，“Deepwalk：通过深度强化学习实现全向双足步态，”在 *IEEE 国际机器人与自动化会议*，2021年，页码
    3033–3039。'
- en: '[36] J. Siekmann, S. Valluri, J. Dao, L. Bermillo, H. Duan, A. Fern, and J. W.
    Hurst, “Learning memory-based control for human-scale bipedal locomotion,” in
    *Robotics science and systems*, 2020.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Siekmann, S. Valluri, J. Dao, L. Bermillo, H. Duan, A. Fern, 和 J. W.
    Hurst，“为人类规模的双足行走学习基于记忆的控制，”在 *机器人科学与系统*，2020年。'
- en: '[37] Z. Li, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and K. Sreenath,
    “Robust and versatile bipedal jumping control through multi-task reinforcement
    learning,” in *Robotics: Science and Systems*, 2023.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Z. Li, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, 和 K. Sreenath，“通过多任务强化学习实现鲁棒且多功能的双足跳跃控制，”在
    *Robotics: Science and Systems*，2023年。'
- en: '[38] J. Siekmann, K. Green, J. Warila, A. Fern, and J. Hurst, “Blind bipedal
    stair traversal via sim-to-real reinforcement learning,” in *Robotics: Science
    and Systems*, 2021.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] J. Siekmann, K. Green, J. Warila, A. Fern, 和 J. Hurst，“通过仿真到现实的强化学习实现盲目的双足楼梯跨越，”在
    *Robotics: Science and Systems*，2021年。'
- en: '[39] C. Yang, K. Yuan, W. Merkt, T. Komura, S. Vijayakumar, and Z. Li, “Learning
    whole-body motor skills for humanoids,” in *IEEE-RAS International Conference
    on Humanoid Robots*, 2019, pp. 270–276.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] C. Yang, K. Yuan, W. Merkt, T. Komura, S. Vijayakumar, 和 Z. Li，“为类人机器人学习全身运动技能，”在
    *IEEE-RAS 国际类人机器人会议*，2019年，页码 270–276。'
- en: '[40] Z. Xie, H. Ling, N. Kim, and M. Panne, “ALLSTEPS: Curriculum‐driven learning
    of stepping stone skills,” *Computer Graphics Forum*, vol. 39, pp. 213–224, 2020.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Z. Xie, H. Ling, N. Kim, 和 M. Panne，“ALLSTEPS：基于课程的踩石技能学习，”*计算机图形论坛*，第
    39 卷，页码 213–224，2020年。'
- en: '[41] H. Duan, A. Malik, J. Dao, A. Saxena, K. Green, J. Siekmann, A. Fern,
    and J. Hurst, “Sim-to-real learning of footstep-constrained bipedal dynamic walking,”
    in *International Conference on Robotics and Automation*, 2022, pp. 10 428–10 434.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H. Duan, A. Malik, J. Dao, A. Saxena, K. Green, J. Siekmann, A. Fern,
    和 J. Hurst，“步态约束的双足动态行走的仿真到现实学习，”在 *国际机器人与自动化会议*，2022年，页码 10 428–10 434。'
- en: '[42] B. Marum, M. Sabatelli, and H. Kasaei, “Learning vision-based bipedal
    locomotion for challenging terrain,” *arXiv preprint arXiv:2309.14594*, 2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] B. Marum, M. Sabatelli, 和 H. Kasaei，“为具有挑战性地形学习基于视觉的双足行走，”*arXiv 预印本 arXiv:2309.14594*，2023年。'
- en: '[43] G. A. Castillo, B. Weng, S. Yang, W. Zhang, and A. Hereid, “Template model
    inspired task space learning for robust bipedal locomotion,” in *IEEE/RSJ International
    Conference on Intelligent Robots and Systems*, 2023, pp. 8582–8589.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] G. A. Castillo, B. Weng, S. Yang, W. Zhang, 和 A. Hereid，“受模板模型启发的任务空间学习用于鲁棒的双足行走，”在
    *IEEE/RSJ 国际智能机器人与系统会议*，2023年，页码 8582–8589。'
- en: '[44] C. Gaspard, G. Passault, M. Daniel, and O. Ly, “FootstepNet: an efficient
    actor-critic method for fast on-line bipedal footstep planning and forecasting,”
    *arXiv preprint arXiv:2403.12589*, 2024.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] C. Gaspard, G. Passault, M. Daniel, 和 O. Ly，“FootstepNet：一种高效的演员-评论家方法用于快速在线双足步态规划和预测，”*arXiv
    预印本 arXiv:2403.12589*，2024年。'
- en: '[45] K. Green, Y. Godse, J. Dao, R. L. Hatton, A. Fern, and J. Hurst, “Learning
    spring mass locomotion: Guiding policies with a reduced-order model,” *IEEE Robotics
    and Automation Letters*, vol. 6, pp. 3926–3932, 2021.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] K. Green, Y. Godse, J. Dao, R. L. Hatton, A. Fern, 和 J. Hurst, “学习弹簧质量运动：使用简化模型引导策略，”
    *IEEE机器人与自动化快报*，第6卷，第3926–3932页，2021年。'
- en: '[46] K. Lobos-Tsunekawa, F. Leiva, and J. Ruiz-del Solar, “Visual navigation
    for biped humanoid robots using deep reinforcement learning,” *IEEE Robotics and
    Automation Letters*, vol. 3, no. 4, pp. 3247–3254, 2018.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] K. Lobos-Tsunekawa, F. Leiva, 和 J. Ruiz-del Solar, “使用深度强化学习进行双足类人机器人视觉导航，”
    *IEEE机器人与自动化快报*，第3卷，第4期，第3247–3254页，2018年。'
- en: '[47] J. Li, L. Ye, Y. Cheng, H. Liu, and B. Liang, “Agile and versatile bipedal
    robot tracking control through reinforcement learning,” *arXiv preprint arXiv:2404.08246*,
    2024.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. Li, L. Ye, Y. Cheng, H. Liu, 和 B. Liang, “通过强化学习实现灵活多变的双足机器人跟踪控制，”
    *arXiv预印本 arXiv:2404.08246*，2024年。'
- en: '[48] F. Jenelten, J. He, F. Farshidian, and M. Hutter, “DTC: Deep tracking
    control,” *Science Robotics*, vol. 9, p. eadh5401, 2024.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] F. Jenelten, J. He, F. Farshidian, 和 M. Hutter, “DTC：深度跟踪控制，” *科学机器人*，第9卷，第eadh5401页，2024年。'
- en: '[49] L. Smith, I. Kostrikov, and S. Levine, “Demonstrating a walk in the park:
    Learning to walk in 20 minutes with model-free reinforcement learning,” *Robotics:
    Science and Systems Demo*, vol. 2, p. 4, 2023.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] L. Smith, I. Kostrikov, 和 S. Levine, “展示公园散步：用无模型强化学习在20分钟内学习行走，” *机器人：科学与系统演示*，第2卷，第4页，2023年。'
- en: '[50] P. Wu, A. Escontrela, D. Hafner, P. Abbeel, and K. Goldberg, “DayDreamer:
    World models for physical robot learning,” in *Conference on Robot Learning*,
    2023, pp. 2226–2240.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] P. Wu, A. Escontrela, D. Hafner, P. Abbeel, 和 K. Goldberg, “DayDreamer：物理机器人学习的世界模型，”
    在 *机器人学习会议*，2023年，第2226–2240页。'
- en: '[51] S. Choi, G. Ji, J. Park, H. Kim, J. Mun, J. H. Lee, and J. Hwangbo, “Learning
    quadrupedal locomotion on deformable terrain,” *Science Robotics*, vol. 8, p.
    eade2256, 2023.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] S. Choi, G. Ji, J. Park, H. Kim, J. Mun, J. H. Lee, 和 J. Hwangbo, “在可变形地形上学习四足运动，”
    *科学机器人*，第8卷，第eade2256页，2023年。'
- en: '[52] G. Feng, H. Zhang, Z. Li, X. B. Peng, B. Basireddy, L. Yue, Z. SONG, L. Yang,
    Y. Liu, K. Sreenath, and S. Levine, “Genloco: Generalized locomotion controllers
    for quadrupedal robots,” in *Conference on Robot Learning*, vol. 205, 2023, pp.
    1893–1903.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] G. Feng, H. Zhang, Z. Li, X. B. Peng, B. Basireddy, L. Yue, Z. SONG, L.
    Yang, Y. Liu, K. Sreenath, 和 S. Levine, “Genloco：四足机器人通用运动控制器，” 在 *机器人学习会议*，第205卷，2023年，第1893–1903页。'
- en: '[53] Y. Fuchioka, Z. Xie, and M. Van de Panne, “OPT-Mimic: Imitation of optimized
    trajectories for dynamic quadruped behaviors,” in *IEEE International Conference
    on Robotics and Automation*, 2023, pp. 5092–5098.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Fuchioka, Z. Xie, 和 M. Van de Panne, “OPT-Mimic：动态四足行为优化轨迹的模仿，” 在 *IEEE国际机器人与自动化会议*，2023年，第5092–5098页。'
- en: '[54] S. Gangapurwala, M. Geisert, R. Orsolino, M. Fallon, and I. Havoutis,
    “RLOC: Terrain-aware legged locomotion using reinforcement learning and optimal
    control,” *IEEE Transactions on Robotics*, vol. 38, pp. 2908–2927, 2022.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] S. Gangapurwala, M. Geisert, R. Orsolino, M. Fallon, 和 I. Havoutis, “RLOC：使用强化学习和最优控制的地形感知腿部运动，”
    *IEEE机器人学报*，第38卷，第2908–2927页，2022年。'
- en: '[55] D. Kang, J. Cheng, M. Zamora, F. Zargarbashi, and S. Coros, “RL + Model-Based
    Control: Using on-demand optimal control to learn versatile legged locomotion,”
    *IEEE Robotics and Automation Letters*, vol. 8, pp. 6619–6626, 2023.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] D. Kang, J. Cheng, M. Zamora, F. Zargarbashi, 和 S. Coros, “RL + 基于模型的控制：使用按需最优控制学习多用途的腿部运动，”
    *IEEE机器人与自动化快报*，第8卷，第6619–6626页，2023年。'
- en: '[56] F. Jenelten, R. Grandia, F. Farshidian, and M. Hutter, “TAMOLS: Terrain-aware
    motion optimization for legged systems,” *IEEE Transactions on Robotics*, vol. 38,
    pp. 3395–3413, 2022.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] F. Jenelten, R. Grandia, F. Farshidian, 和 M. Hutter, “TAMOLS：针对腿部系统的地形感知运动优化，”
    *IEEE机器人学报*，第38卷，第3395–3413页，2022年。'
- en: '[57] X. B. Peng, E. Coumans, T. Zhang, T.-W. E. Lee, J. Tan, and S. Levine,
    “Learning agile robotic locomotion skills by imitating animals,” in *Robotics:
    Science and Systems*, 2020.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] X. B. Peng, E. Coumans, T. Zhang, T.-W. E. Lee, J. Tan, 和 S. Levine, “通过模仿动物学习灵活的机器人运动技能，”
    在 *机器人：科学与系统*，2020年。'
- en: '[58] F. Yin, A. Tang, L. Xu, Y. Cao, Y. Zheng, Z. Zhang, and X. Chen, “Run
    like a dog: Learning based whole-body control framework for quadruped gait style
    transfer,” in *IEEE/RSJ International Conference on Intelligent Robots and Systems*,
    2021, pp. 8508–8514.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] F. Yin, A. Tang, L. Xu, Y. Cao, Y. Zheng, Z. Zhang, 和 X. Chen, “像狗一样跑：基于学习的四足步态风格迁移全身控制框架，”
    在 *IEEE/RSJ国际智能机器人与系统会议*，2021年，第8508–8514页。'
- en: '[59] Y. Ma, F. Farshidian, T. Miki, J. Lee, and M. Hutter, “Combining learning-based
    locomotion policy with model-based manipulation for legged mobile manipulators,”
    *IEEE Robotics and Automation Letters*, vol. 7, pp. 2377–2384, 2022.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Y. Ma, F. Farshidian, T. Miki, J. Lee 和 M. Hutter，“将基于学习的运动政策与基于模型的操控结合用于腿部移动操控器，”
    *IEEE机器人与自动化快报*，第7卷，pp. 2377–2384，2022年。'
- en: '[60] Z. Fu, X. Cheng, and D. Pathak, “Deep whole-body control: Learning a unified
    policy for manipulation and locomotion,” in *Conference on Robot Learning*, 2023,
    pp. 138–149.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Z. Fu, X. Cheng 和 D. Pathak，“深度全身控制：学习操控和运动的统一策略，” 在 *机器人学习大会*，2023年，pp.
    138–149。'
- en: '[61] P. Arm, M. Mittal, H. Kolvenbach, and M. Hutter, “Pedipulate: Enabling
    manipulation skills using a quadruped robot’s leg,” in *IEEE Conference on Robotics
    and Automation*, 2024.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] P. Arm, M. Mittal, H. Kolvenbach 和 M. Hutter，“Pedipulate：利用四足机器人的腿部实现操控技能，”
    在 *IEEE机器人与自动化大会*，2024年。'
- en: '[62] I. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik, and K. Sreenath,
    “Learning humanoid locomotion with transformers,” *arXiv preprint arXiv:2303.03381*,
    2023.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] I. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik 和 K. Sreenath，“使用变压器学习类人机器人运动，”
    *arXiv 预印本 arXiv:2303.03381*，2023年。'
- en: '[63] A. Byravan, J. Humplik, L. Hasenclever, A. Brussee, F. Nori, T. Haarnoja,
    B. Moran, S. Bohez, F. Sadeghi, B. Vujatovic *et al.*, “Nerf2real: Sim2real transfer
    of vision-guided bipedal motion skills using neural radiance fields,” in *IEEE
    International Conference on Robotics and Automation*, 2023, pp. 9362–9369.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] A. Byravan, J. Humplik, L. Hasenclever, A. Brussee, F. Nori, T. Haarnoja,
    B. Moran, S. Bohez, F. Sadeghi, B. Vujatovic *等*，“Nerf2real：使用神经辐射场的视觉引导双足运动技能的模拟到真实迁移，”
    在 *IEEE国际机器人与自动化大会*，2023年，pp. 9362–9369。'
- en: '[64] A. Kumar, Z. Li, J. Zeng, D. Pathak, K. Sreenath, and J. Malik, “Adapting
    rapid motor adaptation for bipedal robots,” in *IEEE/RSJ International Conference
    on Intelligent Robots and Systems*, 2022, pp. 1161–1168.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] A. Kumar, Z. Li, J. Zeng, D. Pathak, K. Sreenath 和 J. Malik，“为双足机器人适应快速运动适应性，”
    在 *IEEE/RSJ国际智能机器人与系统大会*，2022年，pp. 1161–1168。'
- en: '[65] R. P. singh, Z. Xie, P. Gergondet, and F. Kanehiro, “Learning bipedal
    walking for humanoids with current feedback,” *IEEE Access*, vol. 11, p. 82013–82023,
    2023.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] R. P. Singh, Z. Xie, P. Gergondet 和 F. Kanehiro，“通过当前反馈学习双足机器人的步态，” *IEEE
    Access*，第11卷，第82013–82023页，2023年。'
- en: '[66] B. van Marum, M. Sabatelli, and H. Kasaei, “Learning perceptive bipedal
    locomotion over irregular terrain,” *arXiv preprint arXiv:2304.07236*, 2023.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] B. van Marum, M. Sabatelli 和 H. Kasaei，“在不规则地形上学习感知双足运动，” *arXiv 预印本 arXiv:2304.07236*，2023年。'
- en: '[67] J. Dao, H. Duan, and A. Fern, “Sim-to-real learning for humanoid box loco-manipulation,”
    *arXiv preprint arXiv:2310.03191*, 2023.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. Dao, H. Duan 和 A. Fern，“类人机器人箱体机动操控的模拟到真实学习，” *arXiv 预印本 arXiv:2310.03191*，2023年。'
- en: '[68] J. Baltes, G. Christmann, and S. Saeedvand, “A deep reinforcement learning
    algorithm to control a two-wheeled scooter with a humanoid robot,” *Engineering
    Applications of Artificial Intelligence*, vol. 126, p. 106941, 2023.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] J. Baltes, G. Christmann 和 S. Saeedvand，“一种用于控制双轮滑板车的深度强化学习算法与类人机器人，”
    *人工智能工程应用*，第126卷，第106941页，2023年。'
- en: '[69] T. Haarnoja, B. Moran, G. Lever, S. H. Huang, D. Tirumala, J. Humplik,
    M. Wulfmeier, S. Tunyasuvunakool, N. Y. Siegel, R. Hafner *et al.*, “Learning
    agile soccer skills for a bipedal robot with deep reinforcement learning,” *Science
    Robotics*, vol. 9, p. eadi8022, 2024.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] T. Haarnoja, B. Moran, G. Lever, S. H. Huang, D. Tirumala, J. Humplik,
    M. Wulfmeier, S. Tunyasuvunakool, N. Y. Siegel, R. Hafner *等*，“使用深度强化学习为双足机器人学习灵活的足球技能，”
    *科学机器人*，第9卷，第eadi8022页，2024年。'
- en: '[70] M. Seo, S. Han, K. Sim, S. H. Bang, C. Gonzalez, L. Sentis, and Y. Zhu,
    “Deep imitation learning for humanoid loco-manipulation through human teleoperation,”
    in *IEEE-RAS International Conference on Humanoid Robots*, 2023, pp. 1–8.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] M. Seo, S. Han, K. Sim, S. H. Bang, C. Gonzalez, L. Sentis 和 Y. Zhu，“通过人类远程操作进行类人机器人机动操控的深度模仿学习，”
    在 *IEEE-RAS国际类人机器人大会*，2023年，pp. 1–8。'
- en: '[71] K. N. Kumar, I. Essa, and S. Ha, “Words into action: Learning diverse
    humanoid robot behaviors using language guided iterative motion refinement,” in
    *Workshop on Language and Robot Learning: Language as Grounding*, 2023.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] K. N. Kumar, I. Essa 和 S. Ha，“将语言转化为行动：使用语言指导的迭代运动优化学习多样的类人机器人行为，” 在 *语言与机器人学习研讨会：语言作为基础*，2023年。'
- en: '[72] Y. Tong, H. Liu, and Z. Zhang, “Advancements in humanoid robots: A comprehensive
    review and future prospects,” *IEEE/CAA Journal of Automatica Sinica*, vol. 11,
    pp. 301–328, 2024.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Y. Tong, H. Liu 和 Z. Zhang，“类人机器人进展：全面回顾与未来展望，” *IEEE/CAA自动化学报*，第11卷，pp.
    301–328，2024年。'
- en: '[73] A. Dzedzickis, J. Subačiūtė-Žemaitienė, E. Šutinys, U. Samukaitė-Bubnienė,
    and V. Bučinskas, “Advanced applications of industrial robotics: New trends and
    possibilities,” *Applied Sciences*, vol. 12, p. 135, 2021.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] A. Dzedzickis, J. Subačiūtė-Žemaitienė, E. Šutinys, U. Samukaitė-Bubnienė,
    和 V. Bučinskas，“工业机器人先进应用：新趋势与可能性，” *应用科学*，第12卷，第135页，2021年。'
- en: '[74] M. Yang, E. Yang, R. C. Zante, M. Post, and X. Liu, “Collaborative mobile
    industrial manipulator: a review of system architecture and applications,” in
    *International conference on automation and computing*, 2019, pp. 1–6.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] M. Yang, E. Yang, R. C. Zante, M. Post, 和 X. Liu，“协作移动工业操控器：系统架构与应用综述，”
    见 *国际自动化与计算会议*，2019年，第1–6页。'
- en: '[75] “6+ Hours Live Autonomous Robot Demo,” https://www.youtube.com/watch?v=Ke468Mv8ldM,
    Mar. 2024.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] “6+ 小时实时自主机器人演示，” https://www.youtube.com/watch?v=Ke468Mv8ldM，2024年3月。'
- en: '[76] G. Bingjing, H. Jianhai, L. Xiangpan, and Y. Lin, “Human–robot interactive
    control based on reinforcement learning for gait rehabilitation training robot,”
    *International Journal of Advanced Robotic Systems*, vol. 16, p. 1729881419839584,
    2019.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] G. Bingjing, H. Jianhai, L. Xiangpan, 和 Y. Lin，“基于强化学习的人机交互控制用于步态康复训练机器人，”
    *国际先进机器人系统期刊*，第16卷，第1729881419839584页，2019年。'
- en: '[77] A. Diodato, M. Brancadoro, G. De Rossi, H. Abidi, D. Dall’Alba, R. Muradore,
    G. Ciuti, P. Fiorini, A. Menciassi, and M. Cianchetti, “Soft robotic manipulator
    for improving dexterity in minimally invasive surgery,” *Surgical innovation*,
    vol. 25, pp. 69–76, 2018.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] A. Diodato, M. Brancadoro, G. De Rossi, H. Abidi, D. Dall’Alba, R. Muradore,
    G. Ciuti, P. Fiorini, A. Menciassi, 和 M. Cianchetti，“用于提高手术灵活性的软体机器人操控器，” *外科创新*，第25卷，第69–76页，2018年。'
- en: '[78] R. Bogue, “Underwater robots: a review of technologies and applications,”
    *Industrial Robot: An International Journal*, vol. 42, pp. 186–191, 2015.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] R. Bogue，“水下机器人：技术与应用综述，” *工业机器人：国际期刊*，第42卷，第186–191页，2015年。'
- en: '[79] N. Rudin, H. Kolvenbach, V. Tsounis, and M. Hutter, “Cat-like jumping
    and landing of legged robots in low gravity using deep reinforcement learning,”
    *IEEE Transactions on Robotics*, vol. 38, pp. 317–328, 2022.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] N. Rudin, H. Kolvenbach, V. Tsounis, 和 M. Hutter，“基于深度强化学习的类猫跳跃和着陆在低重力环境中的应用，”
    *IEEE机器人学汇刊*，第38卷，第317–328页，2022年。'
- en: '[80] J. Qi, H. Gao, H. Su, L. Han, B. Su, M. Huo, H. Yu, and Z. Deng, “Reinforcement
    learning-based stable jump control method for asteroid-exploration quadruped robots,”
    *Aerospace Science and Technology*, vol. 142, p. 108689, 2023.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] J. Qi, H. Gao, H. Su, L. Han, B. Su, M. Huo, H. Yu, 和 Z. Deng，“基于强化学习的稳定跳跃控制方法用于小行星探索四足机器人，”
    *航空航天科学与技术*，第142卷，第108689页，2023年。'
- en: '[81] O. Mubin, C. Bartneck, L. Feijs, H. Hooft van Huysduynen, J. Hu, and J. Muelver,
    “Improving speech recognition with the robot interaction language,” *Disruptive
    science and Technology*, vol. 1, pp. 79–88, 2012.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] O. Mubin, C. Bartneck, L. Feijs, H. Hooft van Huysduynen, J. Hu, 和 J.
    Muelver，“通过机器人互动语言提高语音识别，” *颠覆性科学与技术*，第1卷，第79–88页，2012年。'
- en: '[82] A. Meduri, M. Khadiv, and L. Righetti, “DeepQ stepper: A framework for
    reactive dynamic walking on uneven terrain,” in *IEEE International Conference
    on Robotics and Automation*, 2021, pp. 2099–2105.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] A. Meduri, M. Khadiv, 和 L. Righetti，“DeepQ步态器：不平坦地形上反应式动态步态的框架，” 见 *IEEE国际机器人与自动化会议*，2021年，第2099–2105页。'
- en: '[83] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    in *International Conference on Learning Representations*, 2016.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, 和 D. Wierstra，“使用深度强化学习的连续控制，” 见 *国际学习表征会议*，2016年。'
- en: '[84] L. Liu, M. V. D. Panne, and K. Yin, “Guided learning of control graphs
    for physics-based characters,” *ACM Transactions on Graphics*, vol. 35, pp. 1–14,
    2016.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] L. Liu, M. V. D. Panne, 和 K. Yin，“物理基础角色的控制图引导学习，” *ACM图形学汇刊*，第35卷，第1–14页，2016年。'
- en: '[85] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region
    policy optimization,” in *International Conference on Machine Learning*, 2015,
    pp. 1889–1897.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] J. Schulman, S. Levine, P. Abbeel, M. Jordan, 和 P. Moritz，“信任区域策略优化，”
    见 *国际机器学习会议*，2015年，第1889–1897页。'
- en: '[86] C. Huang, G. Wang, Z. Zhou, R. Zhang, and L. Lin, “Reward-adaptive reinforcement
    learning: Dynamic policy gradient optimization for bipedal locomotion,” *IEEE
    Transactions on Pattern Analysis and Machine Intelligence*, vol. 45, pp. 7686–7695,
    2023.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] C. Huang, G. Wang, Z. Zhou, R. Zhang, 和 L. Lin，“奖励自适应强化学习：双足行走的动态策略梯度优化，”
    *IEEE模式分析与机器智能汇刊*，第45卷，第7686–7695页，2023年。'
- en: '[87] S. Dankwa and W. Zheng, “Twin-delayed DDPG: A deep reinforcement learning
    technique to model a continuous movement of an intelligent robot agent,” in *International
    conference on vision, image and signal processing*, 2019, pp. 1–5.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] S. Dankwa 和 W. Zheng, “双延迟DDPG：一种深度强化学习技术，用于建模智能机器人代理的连续运动，” 见 *国际视觉、图像与信号处理会议*，2019年，第1–5页。'
- en: '[88] J. Leng, S. Fan, J. Tang, H. Mou, J. Xue, and Q. Li, “M-A3C: A mean-asynchronous
    advantage actor-critic reinforcement learning method for real-time gait planning
    of biped robot,” *IEEE Access*, vol. 10, pp. 76 523–76 536, 2022.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] J. Leng, S. Fan, J. Tang, H. Mou, J. Xue 和 Q. Li, “M-A3C：一种用于双足机器人实时步态规划的均值异步优势演员评论家强化学习方法，”
    *IEEE Access*，第10卷，第76 523–76 536页，2022年。'
- en: '[89] C. Yu and A. Rosendo, “Multi-modal legged locomotion framework with automated
    residual reinforcement learning,” *IEEE Robotics and Automation Letters*, vol. 7,
    pp. 10 312–10 319, 2022.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] C. Yu 和 A. Rosendo, “多模态腿部运动框架与自动化残差强化学习，” *IEEE机器人与自动化快报*，第7卷，第10 312–10 319页，2022年。'
- en: '[90] O. Aydogmus and M. Yilmaz, “Comparative analysis of reinforcement learning
    algorithms for bipedal robot locomotion,” *IEEE Access*, pp. 7490–7499, 2023.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] O. Aydogmus 和 M. Yilmaz, “双足机器人运动的强化学习算法比较分析，” *IEEE Access*，第7490–7499页，2023年。'
- en: '[91] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv e-prints*, pp. arXiv–1707, 2017.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] J. Schulman, F. Wolski, P. Dhariwal, A. Radford 和 O. Klimov, “近端策略优化算法，”
    *arXiv电子印刷本*，第arXiv–1707页，2017年。'
- en: '[92] C. Tao, J. Xue, Z. Zhang, and Z. Gao, “Parallel deep reinforcement learning
    method for gait control of biped robot,” *IEEE Transactions on Circuits and Systems
    II: Express Briefs*, vol. 69, pp. 2802–2806, 2022.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] C. Tao, J. Xue, Z. Zhang 和 Z. Gao, “双足机器人步态控制的并行深度强化学习方法，” *IEEE电路与系统快报II：快速简报*，第69卷，第2802–2806页，2022年。'
- en: '[93] W. Yu, V. C. V. Kumar, G. Turk, and C. K. Liu, “Sim-to-real transfer for
    biped locomotion,” in *IEEE/RSJ International Conference on Intelligent Robots
    and Systems*, 2019, pp. 3503–3510.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] W. Yu, V. C. V. Kumar, G. Turk 和 C. K. Liu, “双足运动的模拟到现实转移，” 见 *IEEE/RSJ国际智能机器人与系统大会*，2019年，第3503–3510页。'
- en: '[94] S. Masuda and K. Takahashi, “Sim-to-real transfer of compliant bipedal
    locomotion on torque sensor-less gear-driven humanoid,” in *IEEE-RAS International
    Conference on Humanoid Robots*, 2023, pp. 1–8.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] S. Masuda 和 K. Takahashi, “无扭矩传感器齿轮驱动人形机器人上合规双足运动的模拟到现实转移，” 见 *IEEE-RAS国际人形机器人大会*，2023年，第1–8页。'
- en: '[95] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun,
    and M. Hutter, “Learning agile and dynamic motor skills for legged robots,” *Science
    Robotics*, vol. 4, p. eaau5872, 2019.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun
    和 M. Hutter, “学习灵活且动态的腿部机器人运动技能，” *科学机器人*，第4卷，第eaau5872页，2019年。'
- en: '[96] G. A. Castillo, B. Weng, W. Zhang, and A. Hereid, “Robust feedback motion
    policy design using reinforcement learning on a 3D digit bipedal robot,” in *IEEE/RSJ
    International Conference on Intelligent Robots and Systems*, 2021, pp. 5136–5143.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] G. A. Castillo, B. Weng, W. Zhang 和 A. Hereid, “基于强化学习的3D数字双足机器人鲁棒反馈运动策略设计，”
    见 *IEEE/RSJ国际智能机器人与系统大会*，2021年，第5136–5143页。'
