- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:40:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:40:47'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2303.08557] Deep Learning for Cross-Domain Few-Shot Visual Recognition: A
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2303.08557] 深度学习在跨领域少样本视觉识别中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2303.08557](https://ar5iv.labs.arxiv.org/html/2303.08557)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '来源: [https://ar5iv.labs.arxiv.org/html/2303.08557](https://ar5iv.labs.arxiv.org/html/2303.08557)'
- en: 'Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在跨领域少样本视觉识别中的应用：综述
- en: Huali Xu [huali.xu@oulu.fi](mailto:huali.xu@oulu.fi) CMVS, University of OuluPentti
    Kaiteran katu 1OuluOuluFinland90570 ,  Shuaifeng Zhi National University of Defense
    TechnologyChangshaHunanChina [zhishuaifeng@outlook.com](mailto:zhishuaifeng@outlook.com)
    ,  Shuzhou Sun [shuzhou.sun@oulu.fi](mailto:shuzhou.sun@oulu.fi) CMVS, University
    of OuluPentti Kaiteran katu 1OuluOuluFinland90570 ,  Vishal M. Patel Johns Hopkins
    University3400 N. Charles StreetBaltimoreMarylandUSA [vpatel36@jhu.edu](mailto:vpatel36@jhu.edu)
     and  Li Liu College of Electronic Science, National University of Defense TechnologyChangshaHunanChina
    CMVS, University of OuluPentti Kaiteran katu 1OuluOuluFinland90570 [li.liu@oulu.fi](mailto:li.liu@oulu.fi)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Huali Xu [huali.xu@oulu.fi](mailto:huali.xu@oulu.fi) CMVS, Oulu大学Pentti Kaiteran
    katu 1OuluOulu芬兰90570 ,  Shuaifeng Zhi 国防科技大学ChangshaHunan中国 [zhishuaifeng@outlook.com](mailto:zhishuaifeng@outlook.com)
    ,  Shuzhou Sun [shuzhou.sun@oulu.fi](mailto:shuzhou.sun@oulu.fi) CMVS, Oulu大学Pentti
    Kaiteran katu 1OuluOulu芬兰90570 ,  Vishal M. Patel 约翰霍普金斯大学3400 N. Charles StreetBaltimoreMaryland美国
    [vpatel36@jhu.edu](mailto:vpatel36@jhu.edu)  和  Li Liu 电子科学学院, 国防科技大学ChangshaHunan中国
    CMVS, Oulu大学Pentti Kaiteran katu 1OuluOulu芬兰90570 [li.liu@oulu.fi](mailto:li.liu@oulu.fi)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Deep learning has been highly successful in computer vision with large amounts
    of labeled data, but struggles with limited labeled training data. To address
    this, Few-shot learning (FSL) is proposed, but it assumes that all samples (including
    source and target task data, where target tasks are performed with prior knowledge
    from source ones) are from the same domain, which is a stringent assumption in
    the real world. To alleviate this limitation, Cross-domain few-shot learning (CDFSL)
    has gained attention as it allows source and target data from different domains
    and label spaces. This paper provides a comprehensive review of CDFSL at the first
    time, which has received far less attention than FSL due to its unique setup and
    difficulties. We expect this paper to serve as both a position paper and a tutorial
    for those doing research in CDFSL. This review first introduces the definition
    of CDFSL and the issues involved, followed by the core scientific question and
    challenge. A comprehensive review of validated CDFSL approaches from the existing
    literature is then presented, along with their detailed descriptions based on
    a rigorous taxonomy. Furthermore, this paper outlines and discusses several promising
    directions of CDFSL that deserve further scientific investigation, covering aspects
    of problem setups, applications and theories.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在计算机视觉领域已取得了显著成功，特别是在拥有大量标注数据的情况下，但在标注数据有限的情况下则表现不佳。为了解决这一问题，提出了少样本学习（FSL），但它假设所有样本（包括源任务和目标任务数据，其中目标任务依赖于源任务的先验知识）都来自同一领域，这在现实世界中是一个严格的假设。为了缓解这一限制，跨领域少样本学习（CDFSL）引起了关注，因为它允许源数据和目标数据来自不同的领域和标签空间。本文首次对CDFSL进行了全面综述，鉴于其独特的设置和困难，CDFSL比FSL受到的关注要少得多。我们期望本文能作为对从事CDFSL研究人员的立场论文和教程。该综述首先介绍了CDFSL的定义及相关问题，然后讨论了核心科学问题和挑战。接着，本文对现有文献中的已验证CDFSL方法进行了全面回顾，并基于严格的分类法对其进行了详细描述。此外，本文还概述并讨论了几个值得进一步科学研究的CDFSL有前景的方向，涵盖了问题设置、应用和理论等方面。
- en: 'Deep Learning, Computer Vision, Cross-Domain Few-Shot Learning, Literature
    Survey^†^†copyright: acmcopyright^†^†journal: CSUR^†^†ccs: Computing methodologies Supervised
    learning by classification^†^†ccs: Computing methodologies Supervised learning
    by classification'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习、计算机视觉、跨领域少样本学习、文献综述^†^†版权：acmcopyright^†^†期刊：CSUR^†^†ccs: 计算方法学 通过分类的监督学习^†^†ccs:
    计算方法学 通过分类的监督学习'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: During the past decade, under the joint driving force of big image data and
    the availability of powerful computing hardware, Machine Learning techniques,
    in particular Deep Learning (LeCun et al., [2015](#bib.bib45)), have brought revolutionary
    progress for various computer vision tasks including fundamental ones like image
    classification (Sharma et al., [2018](#bib.bib87); Wang et al., [2019a](#bib.bib112)),
    segmentation (Fu and Mui, [1981](#bib.bib22); Tavera et al., [2022](#bib.bib96)),
    and synthesis (Magnenat-Thalmann and Thalmann, [2012](#bib.bib64); Wu et al.,
    [2017](#bib.bib117)), and object detection (Liu et al., [2020a](#bib.bib58); Gao
    et al., [2022](#bib.bib27)). For instance, deep learning has achieved (91.10$\%$
    top-1 and 99.02$\%$ top-5) accuracy on the ImageNet image classification challenge,
    exceeding the cognitive abilities of human beings at 95$\%$ top-5\. These capabilities
    are impressive and unprecedented, especially considering the intrinsic advantages
    of automation, such as processing data at a much larger scale and efficiency than
    humans. While it appears that the issue has been resolved, it is important to
    note that this is merely an experimental outcome within a closed dataset. These
    huge achievements have been credited to supervised deep learning demanding adequate
    data and labeling, which, however, remains a substantial disparity from the practical
    implementation. Firstly, data labeling is an expensive and time-consuming process
    in many fields, including industrial inspection, endangered species identification,
    and underwater scene analysis. To address this issue, researchers have explored
    the use of semi-supervised learning algorithms. However, these algorithms often
    require strict assumptions, such as the smoothness assumption, cluster assumption,
    manifold assumption, *etc*., and have high requirements for training data, such
    as the need for unlabeled data to be from the same category as labeled data and
    be evenly distributed. These limitations make them challenging to apply in practice.
    Furthermore, in certain fields, such as medical imaging, military applications,
    and remote sensing, data privacy concerns can make it difficult to collect large
    samples, resulting in only a few available samples.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年里，得益于大规模图像数据和强大计算硬件的共同推动，机器学习技术，尤其是深度学习（LeCun et al., [2015](#bib.bib45)），在各种计算机视觉任务中带来了革命性的进展，包括图像分类（Sharma
    et al., [2018](#bib.bib87); Wang et al., [2019a](#bib.bib112)）、分割（Fu and Mui,
    [1981](#bib.bib22); Tavera et al., [2022](#bib.bib96)）、合成（Magnenat-Thalmann and
    Thalmann, [2012](#bib.bib64); Wu et al., [2017](#bib.bib117)）和物体检测（Liu et al.,
    [2020a](#bib.bib58); Gao et al., [2022](#bib.bib27)）等基础任务。例如，深度学习在ImageNet图像分类挑战中达到了（91.10$\%$
    top-1和99.02$\%$ top-5）的准确率，超越了人类在95$\%$ top-5的认知能力。这些能力令人印象深刻且前所未有，尤其是考虑到自动化的内在优势，如处理数据的规模和效率远超人类。尽管问题似乎已经得到解决，但需要注意的是，这仅仅是在封闭数据集中的实验结果。这些巨大成就归功于监督式深度学习对充足数据和标注的需求，但这与实际应用之间仍存在巨大差距。首先，在许多领域，包括工业检查、濒危物种识别和水下场景分析，数据标注是一个昂贵且耗时的过程。为了解决这个问题，研究人员探讨了半监督学习算法的应用。然而，这些算法通常需要严格的假设，如平滑性假设、聚类假设、流形假设*等*，并且对训练数据有很高的要求，比如需要未标注的数据与标注数据来自相同类别并且均匀分布。这些限制使得它们在实践中难以应用。此外，在某些领域，如医学影像、军事应用和遥感，数据隐私问题使得收集大量样本变得困难，从而导致只有少量可用样本。
- en: Solving problems with limited supervised information using few-shot learning
    (FSL) is feasible based on biological evidence (Carey and Bartlett, [1978](#bib.bib8)).
    Humans have an excellent ability to recognize a new object with only a few samples.
    For instance, children can easily distinguish between a ”cat” and a ”dog” with
    only a few pictures, a capability that machines are yet to attain human-like performance.
    Additionally, in certain scenes such as natural scene images, it is relatively
    easy to acquire large amounts of data. Researchers are inspired by the rapid learning
    ability of humans and transfer learning, and hope that deep learning models can
    quickly learn new categories with only a small number of samples after learning
    a large amount of data of a certain category. Therefore, the goal of FSL is to
    leverage prior knowledge to learn new tasks with only a few labeled samples, which
    has attracted significant attention due to its crucial industrial and academic
    applications. Since the introduction of this problem in 2006 (Fei-Fei et al.,
    [2006](#bib.bib19)), numerous research methods have been proposed (Wang et al.,
    [2020](#bib.bib115); Song et al., [2023](#bib.bib91); Lu et al., [2020](#bib.bib62);
    Shu et al., [2018](#bib.bib88); Parnami and Lee, [2022](#bib.bib76)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 利用有限的监督信息解决问题的少样本学习（FSL）在生物学证据的支持下是可行的（Carey 和 Bartlett，[1978](#bib.bib8)）。人类在仅有少量样本的情况下，具备出色的新物体识别能力。例如，儿童只需看几张图片就能轻松区分“猫”和“狗”，这一能力是机器尚未达到人类水平的。此外，在某些场景下，如自然场景图像，获取大量数据相对容易。研究人员受人类快速学习能力和迁移学习的启发，希望深度学习模型在学习了大量某一类别的数据后，能仅凭少量样本快速学习新类别。因此，FSL
    的目标是利用先验知识以少量标记样本学习新任务，这因其在工业和学术应用中的重要性而受到广泛关注。自2006年该问题被提出以来（Fei-Fei 等，[2006](#bib.bib19)），已经提出了许多研究方法（Wang
    等，[2020](#bib.bib115)；Song 等，[2023](#bib.bib91)；Lu 等，[2020](#bib.bib62)；Shu 等，[2018](#bib.bib88)；Parnami
    和 Lee，[2022](#bib.bib76)）。
- en: 'With the development of FSL, limited training data, domain variations, and
    task modifications make FSL more challenging, leading to the emergence of variants
    such as semi-supervised FSL (Zhmoginov et al., [2022](#bib.bib136)), unsupervised
    FSL (Zhu and Koniusz, [2022](#bib.bib139); Hu et al., [2022](#bib.bib37)), zero-shot
    learning (ZSL) (Pourpanah et al., [2022](#bib.bib81)), cross-domain FSL (CDFSL) (Tseng
    et al., [2020](#bib.bib101); Guo et al., [2020](#bib.bib31)), and more. These
    variants are regarded as distinctive cases of FSL tasks in terms of both samples
    and domain learning. CDFSL addresses the performance degradation in FSL due to
    domain gaps between auxiliary data that provide prior knowledge and the data in
    FSL tasks. Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Deep Learning for
    Cross-Domain Few-Shot Visual Recognition: A Survey") illustrates the difference
    between FSL and CDFSL. It has practical applications in many fields with limited
    supervision information, such as rare cancer detection, video event detection (Yan
    et al., [2015](#bib.bib122)), object tracking (Bertinetto et al., [2016](#bib.bib5)),
    and gesture recognition (Pfister et al., [2014](#bib.bib79)). For instance, in
    the rare cancer detection, obtaining high-quality supervised cancer samples is
    typically a challenging and expensive process, and there are legal concerns related
    to patient privacy. In this case, CDFSL can be used to detect rare cancers by
    utilizing the prior knowledge acquired from a large amount of natural scene images.
    Therefore, CDFSL has significant practical implications for solving real-world
    problems. However, it combines the challenges of both transfer learning and FSL,
    namely the existence of domain gaps and class shift between the auxiliary and
    target data, and the scarcity of sample sizes in the target domain, making it
    a more challenging task. Therefore, after researchers evaluated the cross-domain
    problem in FSL approaches in 2019 (Chen et al., [2019](#bib.bib11); Nakamura and
    Harada, [2019](#bib.bib72)), (Tseng et al., [2020](#bib.bib101)) introduced the
    concept of CDFSL for the first time and proposed corresponding solutions in 2020\.
    Since then, CDFSL has gained widespread attention as a branch of FSL, and numerous
    related works have been published in top publications. Figure [2](#S1.F2 "Figure
    2 ‣ 1\. Introduction ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") presents the milestones of CDFSL technologies from 2020 to the present,
    including representative CDFSL methods and related benchmarks.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着FSL的发展，有限的训练数据、领域变化和任务修改使得FSL变得更加具有挑战性，导致了如半监督FSL（Zhmoginov et al., [2022](#bib.bib136)）、无监督FSL（Zhu
    and Koniusz, [2022](#bib.bib139); Hu et al., [2022](#bib.bib37)）、零样本学习（ZSL）（Pourpanah
    et al., [2022](#bib.bib81)）、跨领域FSL（CDFSL）（Tseng et al., [2020](#bib.bib101); Guo
    et al., [2020](#bib.bib31)）等变种的出现。这些变种被认为是FSL任务在样本和领域学习方面的不同情况。CDFSL解决了由于提供先验知识的辅助数据与FSL任务中的数据之间的领域差距导致FSL性能下降的问题。图[1](#S1.F1
    "图 1 ‣ 1\. 介绍 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述")展示了FSL和CDFSL之间的区别。它在许多具有有限监督信息的领域中具有实际应用，如稀有癌症检测、视频事件检测（Yan
    et al., [2015](#bib.bib122)）、目标跟踪（Bertinetto et al., [2016](#bib.bib5)）和手势识别（Pfister
    et al., [2014](#bib.bib79)）。例如，在稀有癌症检测中，获得高质量的监督癌症样本通常是一个具有挑战性且昂贵的过程，并且涉及患者隐私的法律问题。在这种情况下，可以通过利用从大量自然场景图像中获得的先验知识来使用CDFSL检测稀有癌症。因此，CDFSL在解决实际问题方面具有重要的实际意义。然而，它结合了迁移学习和FSL的挑战，即辅助数据和目标数据之间存在领域差距和类别偏移，以及目标领域中样本数量的稀缺，使得这一任务更加具有挑战性。因此，在2019年，研究人员评估了FSL方法中的跨领域问题（Chen
    et al., [2019](#bib.bib11); Nakamura and Harada, [2019](#bib.bib72)），(Tseng et
    al., [2020](#bib.bib101))首次提出了CDFSL的概念，并在2020年提出了相应的解决方案。从那时起，CDFSL作为FSL的一个分支获得了广泛关注，并在顶级出版物中发表了大量相关工作。图[2](#S1.F2
    "图 2 ‣ 1\. 介绍 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述")展示了从2020年至今CDFSL技术的里程碑，包括代表性的CDFSL方法和相关基准。
- en: '![Refer to caption](img/3687226ba66d1be2219b05077bcef271.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3687226ba66d1be2219b05077bcef271.png)'
- en: Figure 1\. The difference of few-shot learning and cross-domain few-shot learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 少样本学习和跨领域少样本学习的区别。
- en: 'So far, several existing surveys have made detailed summaries and prospects
    for FSL (Shu et al., [2018](#bib.bib88); Wang et al., [2020](#bib.bib115); Parnami
    and Lee, [2022](#bib.bib76); Lu et al., [2020](#bib.bib62); Song et al., [2023](#bib.bib91)). (Shu
    et al., [2018](#bib.bib88)) divides FSL into experience learning and concept learning,
    discussing how to use data from other domains to augment small sample data or
    rectify existing knowledge. More recently, (Wang et al., [2020](#bib.bib115))
    investigates the minimization of empirical risk and define FSL in terms of experience,
    task, and performance, while also introducing CDFSL as one of the branches of
    FSL. Both (Parnami and Lee, [2022](#bib.bib76)) and (Lu et al., [2020](#bib.bib62))
    introduce CDFSL as a variation of FSL. (Parnami and Lee, [2022](#bib.bib76)) discusses
    meta-learning, non-meta-learning, and hybrid meta-learning approaches to FSL,
    and briefly outlines the pioneering work (Tseng et al., [2020](#bib.bib101)) in
    CDFSL, while (Lu et al., [2020](#bib.bib62)) discusses benchmarks and additional
    works in CDFSL. Furthermore, a taxonomy is provided from the perspective of prior
    knowledge in (Song et al., [2023](#bib.bib91)). This paper represents the task
    shift in FSL as a cross near-domain problem and indicates that existing work cannot
    address the cross distance-domain problem. All the aforementioned works envision
    cross-domain issues in FSL as a potential direction. However, there is currently
    a lack of systematic literature that summarizes and discusses the various related
    works for CDFSL. Hence, in this period of rapid development, and to stimulate
    future research and enable newcomers to better understand this challenging problem,
    this paper presents, for the first time, a comprehensive review of the CDFSL problem.
    Firstly, this paper collects and analyzs a large body of literature on the topic.
    The analysis of the reference index reveals that prior to the formal proposal
    of CDFSL, some works had already focused on the cross-domain issues in the field
    of FSL (Chen et al., [2019](#bib.bib11); Nakamura and Harada, [2019](#bib.bib72)).
    Immediately afterward, its introduction as a branch topic of FSL, CDFSL has gained
    significant attention and been widely explored. In addition, we define CDFSL using
    the machine learning definition (Mitchell et al., [1990](#bib.bib67); Mohri et al.,
    [2018a](#bib.bib69)) and transfer learning theory (Tripuraneni et al., [2020](#bib.bib99)).
    Secondly, the analysis of a large number of related papers shows that the unique
    issue of CDFSL is the unreliable two-stage empirical risk minimization problem,
    which stems from the combination of two factors: (1) a significant discrepancy
    between the source and target domains(both in terms of the tasks they perform
    and the domains themselves), (2) the limited amount of supervised information
    available in the target domain. The details are discussed in Section [2](#S2 "2\.
    Background ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey").
    Hence, all the CDFSL work requires organization through a scientific taxonomy
    to address its specific challenges. Next, with regard to the question of how to
    transfer knowledge in CDFSL, this paper provides a comprehensive overview of existing
    approaches and systematically categorizes them into four distinct categories:
    instance-guided, parameter-based, feature post-processing and hybrid approaches.
    To facilitate the understanding of CDFSL and provide a comprehensive evaluation
    of existing methods, the paper also compiles and introduces a comprehensive collection
    of relevant datasets and benchmarks. The information related to these datasets
    and benchmarks is presented in detail, providing valuable insights for researchers
    and practitioners alike. The paper then goes on to analyze and compare the performance
    of the different approaches, providing a comprehensive understanding of the state-of-the-art
    in CDFSL, as discussed in Section [3](#S3 "3\. Approaches ‣ Deep Learning for
    Cross-Domain Few-Shot Visual Recognition: A Survey") and Section [4](#S4 "4\.
    Performance ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey").
    Finally, we explore future research directions for CDFSL by considering three
    perspectives, including problem set-ups, applications, and theories, which provide
    a comprehensive understanding of the field and its potential for future growth.
    Contributions of this survey can be summarized as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '到目前为止，几项现有的调查已对 FSL（Shu 等，[2018](#bib.bib88); Wang 等，[2020](#bib.bib115); Parnami
    和 Lee，[2022](#bib.bib76); Lu 等，[2020](#bib.bib62); Song 等，[2023](#bib.bib91)）进行了详细总结和展望。（Shu
    等，[2018](#bib.bib88)）将 FSL 分为经验学习和概念学习，讨论了如何利用其他领域的数据来增强小样本数据或修正现有知识。最近，（Wang
    等，[2020](#bib.bib115)）探讨了经验风险最小化，并从经验、任务和性能的角度定义 FSL，同时还将 CDFSL 作为 FSL 的一个分支进行介绍。（Parnami
    和 Lee，[2022](#bib.bib76)）和（Lu 等，[2020](#bib.bib62)）将 CDFSL 介绍为 FSL 的一种变体。（Parnami
    和 Lee，[2022](#bib.bib76)）讨论了 FSL 的元学习、非元学习和混合元学习方法，并简要概述了 CDFSL 的开创性工作（Tseng 等，[2020](#bib.bib101)），而（Lu
    等，[2020](#bib.bib62)）讨论了 CDFSL 的基准和其他相关工作。此外，（Song 等，[2023](#bib.bib91)）从先验知识的角度提供了一种分类法。本文将
    FSL 中的任务转移视为跨近域问题，并指出现有工作无法解决跨距离领域的问题。所有上述工作将跨领域问题视为 FSL 的潜在方向。然而，目前缺乏系统的文献来总结和讨论
    CDFSL 的各种相关工作。因此，在这个快速发展的时期，为了激发未来的研究并帮助新人更好地理解这一挑战性问题，本文首次全面回顾了 CDFSL 问题。首先，本文收集和分析了大量关于该主题的文献。对参考文献索引的分析显示，在
    CDFSL 正式提出之前，一些工作已经关注了 FSL 领域的跨领域问题（Chen 等，[2019](#bib.bib11); Nakamura 和 Harada，[2019](#bib.bib72)）。随即，作为
    FSL 的一个分支话题，CDFSL 得到了广泛关注并被深入探讨。此外，我们使用机器学习定义（Mitchell 等，[1990](#bib.bib67); Mohri
    等，[2018a](#bib.bib69)）和迁移学习理论（Tripuraneni 等，[2020](#bib.bib99)）来定义 CDFSL。其次，大量相关论文的分析表明，CDFSL
    的独特问题是不可依赖的两阶段经验风险最小化问题，这源于两个因素的结合：（1）源领域和目标领域之间的显著差异（包括它们执行的任务和领域本身），（2）目标领域中可用的监督信息有限。详细信息在第
    [2](#S2 "2\. Background ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") 节中讨论。因此，所有 CDFSL 工作需要通过科学分类法进行组织，以应对其特定挑战。接下来，关于如何在 CDFSL 中迁移知识的问题，本文提供了现有方法的全面概述，并将其系统地分类为四种不同的类别：实例指导、基于参数、特征后处理和混合方法。为了促进对
    CDFSL 的理解，并提供对现有方法的全面评估，本文还编制和介绍了相关数据集和基准的全面集合。有关这些数据集和基准的信息详细呈现，为研究人员和从业者提供了宝贵的见解。随后，本文分析并比较了不同方法的性能，提供了对
    CDFSL 前沿技术的全面理解，如第 [3](#S3 "3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot
    Visual Recognition: A Survey") 节和第 [4](#S4 "4\. Performance ‣ Deep Learning for
    Cross-Domain Few-Shot Visual Recognition: A Survey") 节所讨论的。最后，我们通过考虑问题设置、应用和理论三个方面探索了
    CDFSL 的未来研究方向，这些方向提供了对该领域及其未来发展潜力的全面理解。本文综述的贡献总结如下：'
- en: '![Refer to caption](img/834efc8241a06bb9c98879f7e17d2891.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/834efc8241a06bb9c98879f7e17d2891.png)'
- en: 'Figure 2\. Chronological milestones on CDFSL from 2019 to the present, including
    representative CDFSL approaches and the related benchmarks. CDFSL was first noticed
    as a topic in 2020 when two related benchmarks, Meta-Dataset (Triantafillou et al.,
    [2019](#bib.bib98)) and BSCD-FSL (Guo et al., [2020](#bib.bib31)), were released
    for CDFSL. The pioneering CDFSL work (Tseng et al., [2020](#bib.bib101)) is proposed
    simultaneously. And (Sun et al., [2021](#bib.bib93); Guan et al., [2020](#bib.bib30))
    are followed proposed, which are the few CDFSL works in 2020\. Subsequently, (Phoo
    and Hariharan, [2020](#bib.bib80); Islam et al., [2021](#bib.bib39); Fu et al.,
    [2021](#bib.bib23); Li et al., [2021a](#bib.bib51); Liu et al., [2021](#bib.bib61))
    explored many new setups for CDFSL like cross multi-domain few-shot learning,
    etc. And (Liang et al., [2021](#bib.bib54); Wang and Deng, [2021](#bib.bib108);
    Hu et al., [2021](#bib.bib38); Xu et al., [2021](#bib.bib119); Du et al., [2021](#bib.bib17);
    Das et al., [2022](#bib.bib15); Chen et al., [2022a](#bib.bib10)) try to improve
    the CDFSL performance by utilizing different manners. Please see Section [3](#S3
    "3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") for details.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 从 2019 年至今 CDFSL 的时间节点，包括代表性的 CDFSL 方法和相关基准。CDFSL 首次作为一个话题受到关注是在 2020
    年，当时发布了两个相关的基准，Meta-Dataset（Triantafillou 等，[2019](#bib.bib98)）和 BSCD-FSL（Guo
    等，[2020](#bib.bib31)）。开创性的 CDFSL 工作（Tseng 等，[2020](#bib.bib101)）同时提出。而（Sun 等，[2021](#bib.bib93)；Guan
    等，[2020](#bib.bib30)）随后提出，它们是 2020 年为数不多的 CDFSL 工作。随后，（Phoo 和 Hariharan，[2020](#bib.bib80)；Islam
    等，[2021](#bib.bib39)；Fu 等，[2021](#bib.bib23)；Li 等，[2021a](#bib.bib51)；Liu 等，[2021](#bib.bib61)）探索了许多新的
    CDFSL 设置，如跨多域小样本学习等。（Liang 等，[2021](#bib.bib54)；Wang 和 Deng，[2021](#bib.bib108)；Hu
    等，[2021](#bib.bib38)；Xu 等，[2021](#bib.bib119)；Du 等，[2021](#bib.bib17)；Das 等，[2022](#bib.bib15)；Chen
    等，[2022a](#bib.bib10)）尝试通过不同的方法提高 CDFSL 的性能。详细信息请参见第 [3](#S3 "3\. 方法 ‣ 深度学习在跨域小样本视觉识别中的应用：一项调查")
    节。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We analyzed existing CDFSL papers and provided a comprehensive survey, a first
    of its kind. We also defined CDFSL formally, connecting it to classic ML (Mitchell
    et al., [1990](#bib.bib67); Mohri et al., [2018a](#bib.bib69)) and transfer learning
    theory (Tripuraneni et al., [2020](#bib.bib99)). This helps guide future research
    in the field.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们分析了现有的 CDFSL 论文，提供了一个全面的调查，这是首次进行的。我们还正式定义了 CDFSL，并将其与经典的机器学习（Mitchell 等，[1990](#bib.bib67)；Mohri
    等，[2018a](#bib.bib69)）和迁移学习理论（Tripuraneni 等，[2020](#bib.bib99)）联系起来。这有助于指导该领域未来的研究。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We listed relevant learning problems for CDFSL with examples, clarifying their
    relation and differences. This helps position CDFSL among various learning problems.
    We also analyzed unique issues and challenges of CDFSL, helping to explore a scientific
    taxonomy for CDFSL work.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们列出了 CDFSL 的相关学习问题及示例，澄清了它们的关系和差异。这有助于将 CDFSL 定位在各种学习问题中。我们还分析了 CDFSL 的独特问题和挑战，帮助探索
    CDFSL 工作的科学分类法。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conducted an extensive literature review, organizing it into a unified taxonomy
    based on instance-guided, parameter-based, feature post-processing, and hybrid
    approaches. We introduced applicable scenarios for each taxonomy, which can help
    to discuss its pros and cons. We also presented datasets and benchmarks for CDFSL,
    summarizing insights from performance results, and discussing each category’s
    pros and cons, improving understanding of CDFSL methods.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进行了广泛的文献综述，将其组织成基于实例引导、基于参数、特征后处理和混合方法的统一分类法。我们介绍了每个分类法的适用场景，这有助于讨论其优缺点。我们还展示了
    CDFSL 的数据集和基准，总结了性能结果的见解，并讨论了每个类别的优缺点，提高了对 CDFSL 方法的理解。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We proposed promising future directions for CDFSL in problem set-ups, applications,
    and theories, based on current weaknesses and potential improvements.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们根据当前的弱点和潜在的改进，提出了 CDFSL 在问题设置、应用和理论方面的有前途的未来方向。
- en: 'The remainder of this survey is organized as follows. Section [2](#S2 "2\.
    Background ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey")
    provides an overview of CDFSL, including its formal definition, relevant learning
    problems, unique issue and challenges, and a taxonomy of existing works in terms
    of instance, parameter, feature, and hybrid. Section [3](#S3 "3\. Approaches ‣
    Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey") deals with
    various approaches to CDFSL problems in detail. Section [4](#S4 "4\. Performance
    ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey") presents
    performance results followed by the pros and cons of approaches from each category.
    And Section [5](#S5 "5\. Future work ‣ Deep Learning for Cross-Domain Few-Shot
    Visual Recognition: A Survey") discusses future directions for CDFSL in terms
    of set-ups, applications, and theories. Finally, the survey provides conclusions
    in Section [6](#S6 "6\. Conclusion ‣ Deep Learning for Cross-Domain Few-Shot Visual
    Recognition: A Survey").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述的其余部分组织如下。第 [2](#S2 "2\. 背景 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述") 节概述了CDFSL，包括其正式定义、相关学习问题、独特问题和挑战，以及现有工作的分类，按实例、参数、特征和混合进行分类。第
    [3](#S3 "3\. 方法 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述") 节详细介绍了针对CDFSL问题的各种方法。第 [4](#S4 "4\.
    性能 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述") 节展示了性能结果，并总结了每个类别方法的优缺点。第 [5](#S5 "5\. 未来工作 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述")
    节讨论了CDFSL的未来方向，包括设置、应用和理论。最后，第 [6](#S6 "6\. 结论 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述") 节提供了综述的结论。
- en: 2\. Background
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景
- en: 'In this section, we first introduce the key concepts related to CDFSL in Section
    [2.1](#S2.SS1 "2.1\. Key Concepts ‣ 2\. Background ‣ Deep Learning for Cross-Domain
    Few-Shot Visual Recognition: A Survey"). And then, we provide formal definitions
    of the vanilla supervised learning, FSL, and CDFSL problems in Section [2.2](#S2.SS2
    "2.2\. Problem Definition ‣ 2\. Background ‣ Deep Learning for Cross-Domain Few-Shot
    Visual Recognition: A Survey") with concrete examples. To differentiate the CDFSL
    problem from relevant problems, we discuss their relatedness and differences in
    Section [2.3](#S2.SS3 "2.3\. Closely Related Problems ‣ 2\. Background ‣ Deep
    Learning for Cross-Domain Few-Shot Visual Recognition: A Survey"). In Section
    [2.4](#S2.SS4 "2.4\. Unique Issue and Challenge ‣ 2\. Background ‣ Deep Learning
    for Cross-Domain Few-Shot Visual Recognition: A Survey"), we discuss the special
    issue and challenges that make CDFSL difficult. Section [2.5](#S2.SS5 "2.5\. Taxonomy
    ‣ 2\. Background ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") presents a unified taxonomy according to how existing works handle
    the unique issue.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先在第 [2.1](#S2.SS1 "2.1\. 关键概念 ‣ 2\. 背景 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述") 节中介绍与CDFSL相关的关键概念。然后，我们在第
    [2.2](#S2.SS2 "2.2\. 问题定义 ‣ 2\. 背景 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述") 节中提供普通监督学习、FSL 和
    CDFSL 问题的正式定义，并附有具体示例。为了区分CDFSL问题与相关问题，我们在第 [2.3](#S2.SS3 "2.3\. 密切相关的问题 ‣ 2\.
    背景 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述") 节中讨论它们的相关性和区别。在第 [2.4](#S2.SS4 "2.4\. 独特问题和挑战 ‣
    2\. 背景 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述") 节中，我们讨论使CDFSL变得困难的特殊问题和挑战。第 [2.5](#S2.SS5 "2.5\.
    分类法 ‣ 2\. 背景 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述") 节根据现有工作如何处理这些独特问题，提出了统一的分类法。
- en: 2.1\. Key Concepts
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 关键概念
- en: Before giving our formal definition of CDFSL, we first define two key basic
    concepts of ‘domain’ and ‘task’  (Pan and Yang, [2009](#bib.bib75); Yang et al.,
    [2020](#bib.bib123)) as their specific contents may differ between the source
    and target problem, inspired by the excellent survey from Pan and Yang (Pan and
    Yang, [2009](#bib.bib75)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在给出CDFSL的正式定义之前，我们首先定义两个关键的基本概念‘领域’和‘任务’（Pan 和 Yang，[2009](#bib.bib75)；Yang
    等，[2020](#bib.bib123)），因为它们的具体内容在源问题和目标问题之间可能有所不同，这受到Pan 和 Yang（Pan 和 Yang，[2009](#bib.bib75)）的优秀综述的启发。
- en: Definition 2.1.1.
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.1.1。
- en: Domain. Given a feature space $\mathcal{X}$ and a marginal probability distribution
    P(X), where $\textit{X}=\{x_{1},x_{2},...,x_{n}\}\subseteq\mathcal{X}$, $n$ is
    the number of instances. A domain $\mathcal{D}=\{\mathcal{X},\textit{P(X)}\}$
    consists of $\mathcal{X}$ and P(X).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 领域。给定一个特征空间 $\mathcal{X}$ 和一个边际概率分布 P(X)，其中 $\textit{X}=\{x_{1},x_{2},...,x_{n}\}\subseteq\mathcal{X}$，$n$
    是实例的数量。一个领域 $\mathcal{D}=\{\mathcal{X},\textit{P(X)}\}$ 由 $\mathcal{X}$ 和 P(X)
    组成。
- en: Specifically, for an image domain $\mathcal{D}$, the original images I is mapped
    to a high-dimensional feature space $\mathcal{X}_{I}$. The features $\textit{X}_{I}$
    in $\mathcal{X}_{I}$ is a higher-dimensional abstraction of I, and the corresponding
    marginal probability distribution is $P(X_{I})$. The image domain $\mathcal{D}$
    can be expressed as $\mathcal{D}=\{\mathcal{X}_{I},P(X_{I})\}$. In general, difference
    in $\mathcal{X}_{I}$ or $P(X_{I})$ can lead to the different domain $\mathcal{D}$.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，对于图像领域$\mathcal{D}$，原始图像I被映射到一个高维特征空间$\mathcal{X}_{I}$。特征$\textit{X}_{I}$在$\mathcal{X}_{I}$中是I的高维抽象，且相应的边际概率分布为$P(X_{I})$。图像领域$\mathcal{D}$可以表示为$\mathcal{D}=\{\mathcal{X}_{I},P(X_{I})\}$。一般来说，$\mathcal{X}_{I}$或$P(X_{I})$的差异可以导致不同的领域$\mathcal{D}$。
- en: Definition 2.1.2.
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.1.2.
- en: Task. Given a domain $\mathcal{D}=\{\mathcal{X},\textit{P(X)}\}$, a task $\mathcal{T}=\{\mathcal{Y},\textit{P(Y|X)}\}$
    consists of the label space $\mathcal{Y}$ and the conditional probability distribution
    P(Y—X), where $\textit{Y}=\{y_{1},y_{2},...,y_{m}\}\in\mathcal{Y}$, $m$ is the
    number of labels.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 任务。给定一个领域$\mathcal{D}=\{\mathcal{X},\textit{P(X)}\}$，一个任务$\mathcal{T}=\{\mathcal{Y},\textit{P(Y|X)}\}$包含标签空间$\mathcal{Y}$和条件概率分布P(Y—X)，其中$\textit{Y}=\{y_{1},y_{2},...,y_{m}\}\in\mathcal{Y}$，$m$是标签的数量。
- en: Specifically, we use $x$ and $y$ to represent the input data and supervision
    terget. For example, for a classification task $\mathcal{T}$, all labels $\textit{Y}^{\mathcal{T}}=\{y^{\mathcal{T}}_{1},y^{\mathcal{T}}_{2},...,y^{\mathcal{T}}_{m}\}\in\mathcal{Y}$
    are in the label space $\mathcal{Y}$, and P(Y—X) can be learned from the training
    data D={$x_{i},y_{i}$}, where $x_{i}\in\textit{X}$ and $y_{i}\in\textit{Y}$. From
    a physical viewpoint, P(Y—X) can be illustrated as a predict function $f(\cdot)$
    that is used to predict the corresponding label y for x.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们用$x$和$y$表示输入数据和监督目标。例如，对于一个分类任务$\mathcal{T}$，所有标签$\textit{Y}^{\mathcal{T}}=\{y^{\mathcal{T}}_{1},y^{\mathcal{T}}_{2},...,y^{\mathcal{T}}_{m}\}\in\mathcal{Y}$都在标签空间$\mathcal{Y}$中，且P(Y—X)可以从训练数据D={$x_{i},y_{i}$}中学习，其中$x_{i}\in\textit{X}$和$y_{i}\in\textit{Y}$。从物理角度来看，P(Y—X)可以表示为一个预测函数$f(\cdot)$，用于预测x的对应标签y。
- en: 2.2\. Problem Definition
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 问题定义
- en: In this subsection, we first define the vanilla supervised learning. The definition
    of FSL is then illustrated before diving into the definition of CDFSL as we consider
    CDFSL a sub-area of FSL.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们首先定义了基础的监督学习。然后展示了FSL的定义，在深入定义CDFSL之前，我们认为CDFSL是FSL的一个子领域。
- en: Definition 2.2.1.
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.2.1.
- en: Vanilla Supervised Learning. Given a domain $\mathcal{D}$, consider a supervised
    learning task $\mathcal{T}$, a training set $\textit{D}^{train}$, and a test set
    $\textit{D}^{test}$ , the goal of vanilla supervised learning is to learn a prediction
    function $f(\cdot)$ for $\mathcal{T}$ on $\textit{D}^{train}$, making $f(\cdot)$
    has a good prediction effect on $\textit{D}^{test}$, where $\{\textit{D}^{train},\textit{D}^{test}\}\subseteq\mathcal{D}$.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基础监督学习。给定一个领域$\mathcal{D}$，考虑一个监督学习任务$\mathcal{T}$、一个训练集$\textit{D}^{train}$和一个测试集$\textit{D}^{test}$，基础监督学习的目标是学习一个预测函数$f(\cdot)$，用于在$\textit{D}^{train}$上对$\mathcal{T}$进行训练，使得$f(\cdot)$在$\textit{D}^{test}$上具有良好的预测效果，其中$\{\textit{D}^{train},\textit{D}^{test}\}\subseteq\mathcal{D}$。
- en: 'For example, an image classification task is categorizing new images into a
    given class using a model learned from training samples. In classic image classification,
    training set $\textit{D}^{train}$ has enough images per class, like ImageNet with
    1000 classes and over 1000 samples per class. Note that the data set D must not
    be confused with the domain $\mathcal{D}$. An illustration of a vanilla supervised
    classification problem is shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.2\. Problem
    Definition ‣ 2\. Background ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") (a).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，图像分类任务是将新图像归类到给定类别中，使用从训练样本中学习到的模型。在经典图像分类中，训练集$\textit{D}^{train}$每个类别有足够的图像，比如ImageNet有1000个类别，每个类别有超过1000个样本。注意，数据集D不能与领域$\mathcal{D}$混淆。基础监督分类问题的示意图见图[3](#S2.F3
    "Figure 3 ‣ 2.2\. Problem Definition ‣ 2\. Background ‣ Deep Learning for Cross-Domain
    Few-Shot Visual Recognition: A Survey") (a)。'
- en: '![Refer to caption](img/df7ee8b54309df532e9f9e49ea3b170b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/df7ee8b54309df532e9f9e49ea3b170b.png)'
- en: Figure 3\. (a) the standard classification, (b) few-shot classification, (c)
    cross-domain few-shot classification. The different shapes mean the different
    categories. $\mathcal{D}$ means domain, $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$
    specifically represent the source and target domains, respectively. ‘?’ indicates
    predict the test data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. (a) 标准分类，(b) 少样本分类，(c) 跨领域少样本分类。不同的形状表示不同的类别。$\mathcal{D}$ 表示领域，$\mathcal{D}^{s}$
    和 $\mathcal{D}^{t}$ 分别代表源领域和目标领域。‘?’ 表示预测测试数据。
- en: Like the goal of vanilla supervised learning, the goal of FSL is also to learn
    a model from the training set $\textit{D}^{train}$ for testing new samples. However,
    the key difference is that $\textit{D}^{train}$ of FSL only includes very little
    supervised information, making it a very challenging task. Due to the few samples
    in $\textit{D}^{train}$, many commonly used supervised algorithms fail to learn
    satisfying classification models, mainly caused by overfitting. Therefore, it
    is necessary and natural to introduce some prior knowledge into the FSL task to
    mitigate the overfitting issue. We call the task of acquiring prior knowledge
    the auxiliary task $\mathcal{T}^{s}$ (or source task). Usually, the categories
    of $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$ have no intersection, *i.e*. $\mathcal{Y}^{s}\cap\mathcal{Y}^{t}=\emptyset$,
    where $\mathcal{Y}^{s}$ and $\mathcal{Y}^{t}$ are the label sets of $\mathcal{T}^{s}$
    and $\mathcal{T}^{t}$, respectively. A formal definition of FSL is given below.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于普通监督学习的目标，FSL 的目标也是从训练集 $\textit{D}^{train}$ 中学习一个模型以测试新的样本。然而，关键的区别在于 FSL
    的 $\textit{D}^{train}$ 仅包含很少的监督信息，使得任务非常具有挑战性。由于 $\textit{D}^{train}$ 中样本数量少，许多常用的监督算法无法学习到令人满意的分类模型，这主要是由于过拟合。因此，引入一些先验知识来缓解过拟合问题是必要且自然的。我们称获取先验知识的任务为辅助任务
    $\mathcal{T}^{s}$（或源任务）。通常，$\mathcal{T}^{s}$ 和 $\mathcal{T}^{t}$ 的类别没有交集，*即* $\mathcal{Y}^{s}\cap\mathcal{Y}^{t}=\emptyset$，其中
    $\mathcal{Y}^{s}$ 和 $\mathcal{Y}^{t}$ 分别是 $\mathcal{T}^{s}$ 和 $\mathcal{T}^{t}$
    的标签集合。FSL 的正式定义如下。
- en: Definition 2.2.2.
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.2.2。
- en: Few Shot Learning (FSL). Given a domain $\mathcal{D}$, a task $\mathcal{T}^{t}$
    described by a T-specific data set $\textit{D}^{t}$ with only a few supervised
    information available, and a task $\mathcal{T}^{s}$ described by T-irrelevant
    auxiliary data set $\textit{D}^{s}$ with sufficient supervised information, FSL
    aims to learn a function $f(\cdot)$ for $\mathcal{T}^{t}$ by utilizing the few
    supervised information in $\textit{D}^{t}$ and the prior knowledge in $(\mathcal{T}^{s},\textit{D}^{s})$,
    where $\{\textit{D}^{t},\textit{D}^{s}\}\subseteq\mathcal{D}$, and $\mathcal{T}^{s}\neq\mathcal{T}^{t}$.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Few Shot Learning (FSL)。给定一个领域 $\mathcal{D}$，一个由 T-specific 数据集 $\textit{D}^{t}$
    描述的任务 $\mathcal{T}^{t}$ 仅有少量监督信息可用，以及一个由 T-irrelevant 辅助数据集 $\textit{D}^{s}$ 描述的任务
    $\mathcal{T}^{s}$ 具有充分的监督信息，FSL 旨在通过利用 $\textit{D}^{t}$ 中的少量监督信息和 $(\mathcal{T}^{s},\textit{D}^{s})$
    中的先验知识来学习一个函数 $f(\cdot)$ 用于 $\mathcal{T}^{t}$，其中 $\{\textit{D}^{t},\textit{D}^{s}\}\subseteq\mathcal{D}$，且
    $\mathcal{T}^{s}\neq\mathcal{T}^{t}$。
- en: 'Specifically, take a few-shot classification task $\mathcal{T}^{t}$ as an example,
    we use the corresponding few-shot data pairs $\{(\textit{x}_{i},\textit{y}_{i})\}^{N^{t}}_{i=1}$
    to represent the input data and supervision target. In addition, $\mathcal{T}^{s}$
    and $\{(\textit{x}_{i},\textit{y}_{i})\}^{N^{s}}_{i=1}$ are utilized to indicate
    the conventional classification task and auxiliary data pairs, where $N^{s}\gg
    N^{t}$. $\mathcal{T}^{t}$ follows a “C-way K-shot” training principle (C indicates
    the number of classes, K represents the sample numbers in each class). We learn
    a function $f$($\cdot$) for $\mathcal{T}^{t}$ from $\textit{D}^{t}$ and $(\mathcal{T}^{s},\textit{D}^{s})$.
    Figure [3](#S2.F3 "Figure 3 ‣ 2.2\. Problem Definition ‣ 2\. Background ‣ Deep
    Learning for Cross-Domain Few-Shot Visual Recognition: A Survey") (b) shows the
    few-shot classification (FSC) problem.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，以一个少样本分类任务 $\mathcal{T}^{t}$ 为例，我们使用对应的少样本数据对 $\{(\textit{x}_{i},\textit{y}_{i})\}^{N^{t}}_{i=1}$
    来表示输入数据和监督目标。此外，$\mathcal{T}^{s}$ 和 $\{(\textit{x}_{i},\textit{y}_{i})\}^{N^{s}}_{i=1}$
    用于表示传统分类任务和辅助数据对，其中 $N^{s}\gg N^{t}$。$\mathcal{T}^{t}$ 遵循 “C-way K-shot” 训练原则（C
    表示类别数，K 代表每个类别中的样本数量）。我们从 $\textit{D}^{t}$ 和 $(\mathcal{T}^{s},\textit{D}^{s})$
    中学习函数 $f$($\cdot$) 以应对 $\mathcal{T}^{t}$。图 [3](#S2.F3 "Figure 3 ‣ 2.2\. Problem
    Definition ‣ 2\. Background ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") (b) 展示了少样本分类（FSC）问题。'
- en: As a branch of FSL, CDFSL also predicts the new samples with the model that
    is learned by $\{(\textit{x}_{i},\textit{y}_{i})\}^{N^{t}}_{i=1}$ and the prior
    knowledge from $\{(\textit{x}_{i},\textit{y}_{i})\}^{N^{s}}_{i=1}$. The difference
    is that $\{(\textit{x}_{i},\textit{y}_{i})\}^{N^{s}}_{i=1}$ and $\{(\textit{x}_{i},\textit{y}_{i})\}^{N^{t}}_{i=1}$
    in CDFSL come from two different domains $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$,
    *i.e*., $\mathcal{D}^{s}\neq\mathcal{D}^{t}$. Compared to the FSL problem that
    the data are independent identically distribution (i.i.d.), CDFSL breaks this
    constraint. Therefore, CDFSL not only inherits the challenges of FSL but also
    contains its unique cross-domain challenges, making it a more challenging problem.
    Consequently, numerous conventional FSL algorithms are no longer applicable to
    CDFSL, which necessitates the development of a viable approach to transfer prior
    knowledge from the source domain $\mathcal{D}^{s}$ to the target domain $\mathcal{D}^{t}$
    without overfitting the model on $\mathcal{D}^{s}$. A definition of CDFSL is formally
    given below.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 FSL 的一个分支，CDFSL 也使用通过 $\{(\textit{x}_{i},\textit{y}_{i})\}^{N^{t}}_{i=1}$
    学习的模型和来自 $\{(\textit{x}_{i},\textit{y}_{i})\}^{N^{s}}_{i=1}$ 的先验知识来预测新样本。不同之处在于，CDFSL
    中的 $\{(\textit{x}_{i},\textit{y}_{i})\}^{N^{s}}_{i=1}$ 和 $\{(\textit{x}_{i},\textit{y}_{i})\}^{N^{t}}_{i=1}$
    来自两个不同的领域 $\mathcal{D}^{s}$ 和 $\mathcal{D}^{t}$，*即*，$\mathcal{D}^{s}\neq\mathcal{D}^{t}$。与数据独立同分布（i.i.d.）的
    FSL 问题相比，CDFSL 打破了这一限制。因此，CDFSL 不仅继承了 FSL 的挑战，还包含其独特的跨域挑战，使其成为一个更具挑战性的问题。因此，许多传统的
    FSL 算法不再适用于 CDFSL，这需要开发一种有效的方法将先验知识从源领域 $\mathcal{D}^{s}$ 转移到目标领域 $\mathcal{D}^{t}$，而不使模型在
    $\mathcal{D}^{s}$ 上过拟合。下面正式给出 CDFSL 的定义。
- en: Definition 2.2.3.
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.2.3。
- en: Cross-Domain Few-Shot learning (CDFSL). Considering a source domain $\mathcal{D}^{s}$
    with sufficient supervised information and learning task $\mathcal{T}^{s}$, a
    target domain $\mathcal{D}^{t}$ with limited supervised information and FSL task
    $\mathcal{T}^{t}$, the goal of CDFSL is to learn a target perdictive function
    $f_{T}(\cdot)$ on $\mathcal{D}^{t}$ with the help of the prior knowledge in $(\mathcal{T}^{s},\mathcal{D}^{s})$,
    where $\mathcal{D}^{s}\neq\mathcal{D}^{t}$, and $\mathcal{T}^{s}\neq\mathcal{T}^{t}$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 跨域少样本学习（CDFSL）。考虑一个具有充分监督信息的源领域 $\mathcal{D}^{s}$ 和学习任务 $\mathcal{T}^{s}$，以及一个具有有限监督信息和
    FSL 任务 $\mathcal{T}^{t}$ 的目标领域 $\mathcal{D}^{t}$，CDFSL 的目标是利用 $(\mathcal{T}^{s},\mathcal{D}^{s})$
    中的先验知识，在 $\mathcal{D}^{t}$ 上学习一个目标预测函数 $f_{T}(\cdot)$，其中 $\mathcal{D}^{s}\neq\mathcal{D}^{t}$，且
    $\mathcal{T}^{s}\neq\mathcal{T}^{t}$。
- en: 'In a cross-domain few-shot classification (CDFSC) problem, as shown in Figure
    [3](#S2.F3 "Figure 3 ‣ 2.2\. Problem Definition ‣ 2\. Background ‣ Deep Learning
    for Cross-Domain Few-Shot Visual Recognition: A Survey") (c), we similarly denote
    a source and a target classification task by $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$,
    respectively. They are described by the data pairs $\{(\bm{x}_{i}^{s},y^{s}_{i})\}_{i=1}^{N^{s}}\subseteq\mathcal{D}^{s}$
    and $\{(\bm{x}_{i}^{t},y^{t}_{i})\}_{i=1}^{N^{t}}\subseteq\mathcal{D}^{t}$, where
    $N^{s}\gg N^{t}$, $y^{s}_{i}\in\mathcal{Y}^{s}$, $y^{t}_{i}\in\mathcal{Y}^{t}$,
    $\mathcal{Y}^{t}\bigcap\mathcal{Y}^{s}=\varnothing$ (*i.e*., the source and target
    domains do not share the label space). Note that $\mathcal{D}^{t}$ and $\mathcal{D}^{s}$
    are sampled from two different probability distributions $p$ and $q$, respectively,
    where $p\neq q$. The objective of the CDFSC is learning a classifier $f_{T}$($\cdot$)
    for $\mathcal{T}^{t}$ using $\mathcal{D}^{t}$ and $(\mathcal{T}^{s},\mathcal{D}^{s})$.
    It addresses the issue that there are no sufficient auxiliary samples in the target
    domain $\mathcal{D}^{t}$ to provide the proper prior knowledge for $\mathcal{T}^{t}$.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个跨域少样本分类（CDFSC）问题中，如图 [3](#S2.F3 "图 3 ‣ 2.2\. 问题定义 ‣ 2\. 背景 ‣ 跨域少样本视觉识别的深度学习：综述")（c）所示，我们类似地用
    $\mathcal{T}^{s}$ 和 $\mathcal{T}^{t}$ 分别表示源和目标分类任务。它们由数据对 $\{(\bm{x}_{i}^{s},y^{s}_{i})\}_{i=1}^{N^{s}}\subseteq\mathcal{D}^{s}$
    和 $\{(\bm{x}_{i}^{t},y^{t}_{i})\}_{i=1}^{N^{t}}\subseteq\mathcal{D}^{t}$ 描述，其中
    $N^{s}\gg N^{t}$，$y^{s}_{i}\in\mathcal{Y}^{s}$，$y^{t}_{i}\in\mathcal{Y}^{t}$，$\mathcal{Y}^{t}\bigcap\mathcal{Y}^{s}=\varnothing$（*即*，源领域和目标领域不共享标签空间）。注意
    $\mathcal{D}^{t}$ 和 $\mathcal{D}^{s}$ 分别从两个不同的概率分布 $p$ 和 $q$ 中采样，其中 $p\neq q$。CDFSC
    的目标是使用 $\mathcal{D}^{t}$ 和 $(\mathcal{T}^{s},\mathcal{D}^{s})$ 为 $\mathcal{T}^{t}$
    学习一个分类器 $f_{T}$($\cdot$)。它解决了目标领域 $\mathcal{D}^{t}$ 中没有足够的辅助样本来为 $\mathcal{T}^{t}$
    提供适当先验知识的问题。
- en: 'Furthermore, CDFSL can be classified into three broad categories based on why
    the image distribution differs: Fine-grain based CDFSL (FG), Art-based CDFSL (Art),
    and Imaging way-based CDFSL (IW). FG-CDFSL pertains to differences in the fine-grained
    categories between $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$. Specifically, the
    categories of $\mathcal{D}^{t}$ are the fine-grained classes of a specific variety
    in $\mathcal{D}^{s}$. A-CDFSL involves differences in artistic expression, such
    as sketches, natural images, stick figures, oil paintings, and watercolors. And
    in IW-CDFSL, dissimilarities in imaging modes between $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$
    arise when the datasets comprise images of distinct modalities, for instance,
    natural images in $\mathcal{D}^{s}$ and medical $X$-ray images in $\mathcal{D}^{t}$.
    IW-CDFSL is generally considered the most challenging of the three categories.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据图像分布差异的原因，CDFSL可以分为三大类：细粒度CDFSL（FG）、艺术基础CDFSL（Art）和成像方式基础CDFSL（IW）。FG-CDFSL涉及$\mathcal{D}^{s}$和$\mathcal{D}^{t}$之间细粒度类别的差异。具体而言，$\mathcal{D}^{t}$的类别是$\mathcal{D}^{s}$中特定种类的细粒度类。A-CDFSL涉及艺术表现的差异，例如素描、自然图像、简笔画、油画和水彩画。在IW-CDFSL中，当数据集包含不同模态的图像时，例如$\mathcal{D}^{s}$中的自然图像和$\mathcal{D}^{t}$中的医疗X射线图像，$\mathcal{D}^{s}$和$\mathcal{D}^{t}$之间会出现成像模式的差异。IW-CDFSL通常被认为是三类中最具挑战性的。
- en: 2.3\. Closely Related Problems
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 密切相关的问题
- en: 'In this section, we discuss the closely relevant problems. The difference and
    relatedness between these problems and CDFSL are illustrated in Figure [4](#S2.F4
    "Figure 4 ‣ 2.3\. Closely Related Problems ‣ 2\. Background ‣ Deep Learning for
    Cross-Domain Few-Shot Visual Recognition: A Survey").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了密切相关的问题。这些问题与CDFSL之间的区别和相关性在图[4](#S2.F4 "图 4 ‣ 2.3. 密切相关的问题 ‣ 2. 背景
    ‣ 跨领域少样本视觉识别：综述")中进行了说明。
- en: '![Refer to caption](img/fa429072bc01f77926a6ef0348b34a2b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/fa429072bc01f77926a6ef0348b34a2b.png)'
- en: Figure 4\. CDFSL related problems. The circles representing target data and
    its size indicating amount.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图4。CDFSL相关问题。圆圈表示目标数据，其大小表示数量。
- en: Semi-supervised Domain Adaptation (Semi-DA). Semi-DA utilizes a large amount
    of supervised data in $\mathcal{D}^{s}$, a few labeled data and a large amount
    of unlabeled data in $\mathcal{D}^{t}$ to improve the performance of $\mathcal{T}$.
    There are the same label space and different but related sample distributions
    between $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$, *i.e*., $\mathcal{D}^{s}\neq\mathcal{D}^{t}$.
    Similar to Semi-DA, the CDFSL problem also uses a large amount of supervised data
    in $\mathcal{D}^{s}$ and limited supervised data in $\mathcal{D}^{t}$ to improve
    the performance of the task $\mathcal{T}$, $\mathcal{D}^{s}\neq\mathcal{D}^{t}$.
    The difference is that CDFSL does not use many unsupervised samples in the target
    domain to help with training. Besides, the label space of $\mathcal{D}^{s}$ and
    $\mathcal{D}^{t}$ are different in the CDFSL problem.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督领域自适应（Semi-DA）。Semi-DA利用$\mathcal{D}^{s}$中的大量监督数据、少量标记数据以及$\mathcal{D}^{t}$中的大量未标记数据来提高$\mathcal{T}$的性能。$\mathcal{D}^{s}$和$\mathcal{D}^{t}$之间存在相同的标签空间和不同但相关的样本分布，即$\mathcal{D}^{s}\neq\mathcal{D}^{t}$。类似于Semi-DA，CDFSL问题也利用$\mathcal{D}^{s}$中的大量监督数据和$\mathcal{D}^{t}$中的有限监督数据来提高任务$\mathcal{T}$的性能，$\mathcal{D}^{s}\neq\mathcal{D}^{t}$。不同之处在于，CDFSL没有使用许多目标领域中的未监督样本来辅助训练。此外，在CDFSL问题中，$\mathcal{D}^{s}$和$\mathcal{D}^{t}$的标签空间是不同的。
- en: Unsupervised Domain Adaptation (UDA). UDA utilizes a large amount of supervised
    data in $\mathcal{D}^{s}$ and a large amount of unlabeled data in $\mathcal{D}^{t}$
    to improve the performance of $\mathcal{T}$. The distributions between $\mathcal{D}^{s}$
    and $\mathcal{D}^{t}$ are different but related, *i.e*., $\mathcal{D}^{s}\neq\mathcal{D}^{t}$.
    And they share the same learning tasks. Similar to UDA, CDFSL also uses a large
    amount of supervised data in $\mathcal{D}^{s}$ to improve the performance of $\mathcal{T}$
    in $\mathcal{D}^{t}$, $\mathcal{D}^{s}\neq\mathcal{D}^{t}$. However, $\mathcal{D}^{t}$
    in CDFSL has only a few amounts of supervised data, and the tasks of $\mathcal{D}^{s}$
    and $\mathcal{D}^{t}$ are different.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督领域自适应（UDA）。UDA利用$\mathcal{D}^{s}$中的大量监督数据和$\mathcal{D}^{t}$中的大量未标记数据来提高$\mathcal{T}$的性能。$\mathcal{D}^{s}$和$\mathcal{D}^{t}$之间的分布不同但相关，即$\mathcal{D}^{s}\neq\mathcal{D}^{t}$。并且它们共享相同的学习任务。类似于UDA，CDFSL也利用$\mathcal{D}^{s}$中的大量监督数据来提高$\mathcal{T}$在$\mathcal{D}^{t}$中的性能，$\mathcal{D}^{s}\neq\mathcal{D}^{t}$。然而，在CDFSL中，$\mathcal{D}^{t}$只有少量监督数据，且$\mathcal{D}^{s}$和$\mathcal{D}^{t}$的任务是不同的。
- en: Domain Generalization (DG). DG uses a large amount of supervised data in M source
    domains $\mathcal{D}^{s}=\{\mathcal{D}^{s}_{i}|i=1,...,M\}$ to improve the performance
    of $\mathcal{T}$ on the unseen $\mathcal{D}^{t}$. The distributions of $\mathcal{D}^{s}$
    and $\mathcal{D}^{t}$ are different but related, *i.e*. $\mathcal{D}^{s}\neq\mathcal{D}^{t}$,
    and the tasks between $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are same. Similar
    to DG, CDFSL also uses a large amount of supervised data in $\mathcal{D}^{s}$
    to improve the performance of $\mathcal{T}$. However, CDFSL is designed to perform
    well on the special $\mathcal{D}^{t}$ but not all unseen $\mathcal{D}^{t}$, and
    the source data usually come from one source domain. Furthermore, the tasks of
    $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are different, i.e., $\mathcal{T}^{s}\neq\mathcal{T}^{t}$.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 领域泛化（DG）。DG 利用 M 个源领域 $\mathcal{D}^{s}=\{\mathcal{D}^{s}_{i}|i=1,...,M\}$ 中的大量监督数据来提高在未见过的
    $\mathcal{D}^{t}$ 上的任务 $\mathcal{T}$ 的表现。$\mathcal{D}^{s}$ 和 $\mathcal{D}^{t}$
    的分布是不同但相关的，即 $\mathcal{D}^{s}\neq\mathcal{D}^{t}$，并且 $\mathcal{D}^{s}$ 和 $\mathcal{D}^{t}$
    之间的任务是相同的。与 DG 类似，CDFSL 也利用 $\mathcal{D}^{s}$ 中的大量监督数据来提升任务 $\mathcal{T}$ 的表现。然而，CDFSL
    被设计为在特定的 $\mathcal{D}^{t}$ 上表现良好，而不是所有未见过的 $\mathcal{D}^{t}$，并且源数据通常来自一个源领域。此外，$\mathcal{D}^{s}$
    和 $\mathcal{D}^{t}$ 的任务是不同的，即 $\mathcal{T}^{s}\neq\mathcal{T}^{t}$。
- en: Domain Adaptation Few-shot Learning (DAFSL). DAFSL leverages a significant amount
    of supervised data in the source domain $\mathcal{D}^{s}$ and a limited number
    of labeled data in the target domain $\mathcal{D}^{t}$ to enhance the performance
    of the task $\mathcal{T}$ on $\mathcal{D}^{t}$. Although the distributions of
    $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ are different, i.e., $\mathcal{D}^{s}\neq\mathcal{D}^{t}$,
    the learning tasks remain the same. Similarly, CDFSL utilizes the same data configurations
    in both domains to train the function for task $\mathcal{T}$. However, in contrast
    to DAFSL, the learning tasks in $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$ differ
    in CDFSL.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应少样本学习（DAFSL）。DAFSL 利用源领域 $\mathcal{D}^{s}$ 中大量的监督数据和目标领域 $\mathcal{D}^{t}$
    中有限的标注数据来提升任务 $\mathcal{T}$ 在 $\mathcal{D}^{t}$ 上的表现。尽管 $\mathcal{D}^{s}$ 和 $\mathcal{D}^{t}$
    的分布不同，即 $\mathcal{D}^{s}\neq\mathcal{D}^{t}$，但学习任务保持不变。同样，CDFSL 利用两个领域中的相同数据配置来训练任务
    $\mathcal{T}$ 的函数。然而，与 DAFSL 不同的是，CDFSL 中的 $\mathcal{D}^{s}$ 和 $\mathcal{D}^{t}$
    的学习任务存在差异。
- en: Multi-task Learning (MTL). MTL utilizes $M$ tasks from $\mathcal{D}$ to improve
    the performance of every $\mathcal{T}_{i}$ (0 ¡ i $\leq$ $M$). All $\{\mathcal{T}_{i}\}^{M}_{i=1}$
    are different but related. Different from MTL, the data of $\mathcal{T}^{s}$ and
    $\mathcal{T}^{t}$ in CDFSL is from different domains $\mathcal{D}^{s}$ and $\mathcal{D}^{t}$,
    *i.e*. $\mathcal{D}^{s}\neq\mathcal{D}^{t}$ and $\mathcal{T}^{s}\neq\mathcal{T}^{t}$,
    and the supervised data in $\mathcal{D}^{t}$ is limited.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习（MTL）。MTL 利用 $\mathcal{D}$ 中的 $M$ 个任务来提高每个 $\mathcal{T}_{i}$ （0 ≤ i ≤ $M$）的表现。所有
    $\{\mathcal{T}_{i}\}^{M}_{i=1}$ 都是不同但相关的。与 MTL 不同的是，CDFSL 中 $\mathcal{T}^{s}$
    和 $\mathcal{T}^{t}$ 的数据来自不同的领域 $\mathcal{D}^{s}$ 和 $\mathcal{D}^{t}$，即 $\mathcal{D}^{s}\neq\mathcal{D}^{t}$
    和 $\mathcal{T}^{s}\neq\mathcal{T}^{t}$，并且 $\mathcal{D}^{t}$ 中的监督数据是有限的。
- en: 2.4\. Unique Issue and Challenge
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 独特的问题和挑战
- en: In machine learning, prediction errors are a common occurrence, making it impossible
    to achieve perfect predictions, *i.e*., the empirical risk minimization (ERM)
    unreliable problem. In this section, we begin by explaining the concept of empirical
    risk minimization (ERM). Next, we delve into the two-stage empirical risk minimization
    (TSERM) problem for CDFSL. Finally, we examine the distinct issues and challenges
    posed by CDFSL.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，预测误差是常见的现象，使得无法实现完美的预测，即实证风险最小化（ERM）不可靠的问题。在本节中，我们首先解释实证风险最小化（ERM）的概念。接着，我们深入探讨
    CDFSL 的两阶段实证风险最小化（TSERM）问题。最后，我们检查 CDFSL 所面临的不同问题和挑战。
- en: 2.4.1\. Empirical Risk Minimization (ERM)
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 实证风险最小化（ERM）
- en: 'Given an input space $\mathcal{X}$ and label space $\mathcal{Y}$, in which
    $X$ and $Y$ satisfy the joint probability distribution $P(X,Y)$, a loss function
    $l(\hat{y},y)$, a hypothesis $h\in\mathcal{H}$ ¹¹1Hypothesis space $\mathcal{H}$
    consists of all functions that can be represented by some choice of values for
    the weights (Mitchell et al., [1990](#bib.bib67)). A hypothesis $h$ is a function
    in Hypothesis space., the risk (expected risk) of hypothesis $h(x)$ is defined
    as the expected value of the loss function:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入空间$\mathcal{X}$和标签空间$\mathcal{Y}$，其中$X$和$Y$满足联合概率分布$P(X,Y)$，损失函数$l(\hat{y},y)$，假设$h\in\mathcal{H}$ ¹¹1假设空间$\mathcal{H}$包括所有可以通过对权重的某些选择来表示的函数（Mitchell等人，[1990](#bib.bib67)）。一个假设$h$是假设空间中的一个函数。则假设$h(x)$的风险（期望风险）定义为损失函数的期望值：
- en: '| (1) |  | $\displaystyle R(h)=\mathbb{E}[l(h(x),y)]=\int l(h(x),y)dP(x,y),$
    |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\displaystyle R(h)=\mathbb{E}[l(h(x),y)]=\int l(h(x),y)dP(x,y),$
    |  |'
- en: 'The ultimate goal of the learning algorithm is to find the hypothesis $h^{\ast}$
    that minimizes the risk $R(h)$ in the hypothesis space $\mathcal{H}$:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 学习算法的最终目标是找到在假设空间$\mathcal{H}$中最小化风险$R(h)$的假设$h^{\ast}$：
- en: '| (2) |  | $\displaystyle h^{\ast}=\text{argmin}_{h\in\mathcal{H}}R(h),$ |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\displaystyle h^{\ast}=\text{argmin}_{h\in\mathcal{H}}R(h),$ |  |'
- en: 'Since $P(x,y)$ is unknown, we compute an approximation called empirical risk
    by averaging the loss function over the training set:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于$P(x,y)$未知，我们通过对训练集上的损失函数进行平均来计算一个叫做经验风险的近似值：
- en: '| (3) |  | $\displaystyle\hat{R}(h)=\frac{1}{n}\sum_{i=1}^{n}l(h(x_{i},y_{i})),$
    |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\displaystyle\hat{R}(h)=\frac{1}{n}\sum_{i=1}^{n}l(h(x_{i},y_{i})),$
    |  |'
- en: 'Therefore, the expected risk is usually infinitely approximated by empirical
    risk minimization  (Mohri et al., [2018b](#bib.bib70); Vapnik, [1991](#bib.bib104)),
    that is, a hypothesis $\hat{h}$ is chosen to minimize the empirical risk:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，期望风险通常通过经验风险最小化进行无限逼近（Mohri等人，[2018b](#bib.bib70)；Vapnik，[1991](#bib.bib104)），即选择一个假设$\hat{h}$来最小化经验风险：
- en: '| (4) |  | $\displaystyle\hat{h}=\text{argmin}_{h\in\mathcal{H}}\hat{R}(h)$
    |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle\hat{h}=\text{argmin}_{h\in\mathcal{H}}\hat{R}(h)$
    |  |'
- en: In FSL, due to limited supervised information, the empirical risk $\hat{R}(h)$
    may be far from an approximation of the expected risk $h^{\ast}$, resulting in
    the overfitting of empirical risk minimization hypothesis $\hat{h}$, *i.e*., the
    core problem of FSL is the unreliable empirical risk caused by insufficient supervised
    data. In current FSL approaches, transfer learning is commonly utilized to address
    overfitting by incorporating additional datasets to aid in task learning. However,
    as the tasks differ between the source and target domains, FSL is confronted with
    knowledge transfer challenges resulting from task shift. This is illustrated in
    the subsequent two-stage empirical risk minimization problem.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在FSL（Few-Shot Learning）中，由于监督信息有限，经验风险$\hat{R}(h)$可能远离期望风险$h^{\ast}$的近似值，这导致了经验风险最小化假设$\hat{h}$的过拟合，即FSL的核心问题是由于监督数据不足而引起的不可靠经验风险。在当前的FSL方法中，通常通过转移学习来解决过拟合问题，方法是引入额外的数据集来辅助任务学习。然而，由于源领域和目标领域的任务不同，FSL面临着由于任务转移而引发的知识转移挑战。这在随后的两阶段经验风险最小化问题中得到了说明。
- en: 2.4.2\. Two-Stage Empirical Risk Minimization (TSERM)
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2\. 两阶段经验风险最小化（TSERM）
- en: We assume that all tasks share a generic nonlinear feature representation. The
    two-stage empirical risk minimization (TSERM) aims to transfer knowledge from
    the source task to the target task by learning this generic feature representation.
    In the first stage, the primary focus is on learning the general feature representation.
    The second stage then utilizes the acquired feature representation to construct
    an optimal hypothesis for the target task.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设所有任务共享一种通用的非线性特征表示。两阶段经验风险最小化（TSERM）旨在通过学习这种通用特征表示，将知识从源任务转移到目标任务。在第一阶段，主要关注学习一般特征表示。第二阶段则利用获取的特征表示来为目标任务构建最优假设。
- en: Specifically, we use $\mathcal{T}^{s}$ and $\mathcal{T}^{t}$ to represent a
    source task and a target task. TSERM learns two hypotheses $f$ and $h$²²2both
    $f$ and $h$ are parametric models due to only limited supervised samples existing
    in a hypothesis space $\mathcal{H}$, where $f$ learns a shared feature representation
    in the first stage, and $h$ utilizes it to learn a recognizer in the second stage.
    For convenience, we use
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们使用 $\mathcal{T}^{s}$ 和 $\mathcal{T}^{t}$ 分别表示源任务和目标任务。TSERM 学习了两个假设 $f$
    和 $h$，由于在假设空间 $\mathcal{H}$ 中仅存在有限的监督样本，$f$ 和 $h$ 都是参数模型，其中 $f$ 在第一阶段学习共享特征表示，$h$
    在第二阶段利用这些特征表示学习识别器。为方便起见，我们使用
- en: (1)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: $(h^{\dagger},f^{\dagger})=\text{argmin}_{(f,h)\in\mathcal{H}}R(h,f)$ indicates
    the function that minimizes the expected risk.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $(h^{\dagger},f^{\dagger})=\text{argmin}_{(f,h)\in\mathcal{H}}R(h,f)$ 表示最小化期望风险的函数。
- en: (2)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2)
- en: $(h^{\ast},f^{\dagger})$³³3we assume that there exist a common nonlinear feature
    representation $f^{\dagger}$ in $\mathcal{H}$ = $\text{argmin}_{(f,h)\in\mathcal{H}}R(h,f)$
    means the function that minimizes the expected risk in $\mathcal{H}$.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $(h^{\ast},f^{\dagger})$ 我们假设在 $\mathcal{H}$ 中存在一个共同的非线性特征表示 $f^{\dagger}$ =
    $\text{argmin}_{(f,h)\in\mathcal{H}}R(h,f)$ 表示在 $\mathcal{H}$ 中最小化期望风险的函数。
- en: (3)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3)
- en: $(\hat{h},\hat{f})=\text{argmin}_{(f,h)\in\mathcal{H}}\hat{R}(h,f)$ represents
    the function that minimizes the empirical risk in $\mathcal{H}$.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $(\hat{h},\hat{f})=\text{argmin}_{(f,h)\in\mathcal{H}}\hat{R}(h,f)$ 代表在 $\mathcal{H}$
    中最小化经验风险的函数。
- en: 'Since $(h^{\dagger},f^{\dagger})$ is unknown, it must be approximated by $(h,f)\in\mathcal{H}$.
    $(h^{\ast},f^{\dagger})$ represents the most optimal approximation in $\mathcal{H}$,
    while $(\hat{h},\hat{f})$ represents the empirical risk minimization optimal hypothesis
    in $\mathcal{H}$. Suppose $(h^{\dagger},f^{\dagger})$, $(h^{\ast},f^{\dagger})$,
    $(\hat{h},\hat{f})$ are all unique. In the first stage, the empirical risk of
    $\mathcal{T}^{s}$ is given by the following formula:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $(h^{\dagger},f^{\dagger})$ 是未知的，因此必须通过 $(h,f)\in\mathcal{H}$ 来近似。$(h^{\ast},f^{\dagger})$
    代表在 $\mathcal{H}$ 中的最优近似，而 $(\hat{h},\hat{f})$ 代表在 $\mathcal{H}$ 中经验风险最小化的最优假设。假设
    $(h^{\dagger},f^{\dagger})$、$(h^{\ast},f^{\dagger})$ 和 $(\hat{h},\hat{f})$ 都是唯一的。在第一阶段，$\mathcal{T}^{s}$
    的经验风险由以下公式给出：
- en: '| (5) |  | $\displaystyle\hat{R}_{s}(h_{s},f)=\frac{1}{N^{s}}\sum^{N^{s}}_{i=1}l(h_{s}\circ
    f(x_{i}^{s}),y_{i}^{s}),$ |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\displaystyle\hat{R}_{s}(h_{s},f)=\frac{1}{N^{s}}\sum^{N^{s}}_{i=1}l(h_{s}\circ
    f(x_{i}^{s}),y_{i}^{s}),$ |  |'
- en: where $l(\cdot,\cdot)$ is the loss function, $N^{s}$ represents the number of
    training samples in $\mathcal{T}^{s}$, and $x_{i}^{s}$ and $y_{i}^{s}$ represent
    the samples and corresponding labels in $\mathcal{T}^{s}$, respectively. $h_{s}$
    is the hypothesis of $\mathcal{T}^{s}$, The optimal shared feature extraction
    function $\hat{f}(\cdot)$ is expressed as $\hat{f}=\text{argmin}_{(f,h_{s})\in\mathcal{H}}\hat{R}_{s}(h_{s},f)$.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l(\cdot,\cdot)$ 是损失函数，$N^{s}$ 表示 $\mathcal{T}^{s}$ 中的训练样本数量，$x_{i}^{s}$
    和 $y_{i}^{s}$ 分别表示 $\mathcal{T}^{s}$ 中的样本及其对应标签。$h_{s}$ 是 $\mathcal{T}^{s}$ 的假设。最优共享特征提取函数
    $\hat{f}(\cdot)$ 表示为 $\hat{f}=\text{argmin}_{(f,h_{s})\in\mathcal{H}}\hat{R}_{s}(h_{s},f)$。
- en: 'In the second stage, the empirical risk of $\mathcal{T}^{t}$ is defined as:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，$\mathcal{T}^{t}$ 的经验风险定义为：
- en: '| (6) |  | $\displaystyle\hat{R}_{t}(h_{t},f)=\frac{1}{N^{t}}\sum^{N^{t}}_{i=1}l(h_{t}\circ\hat{f}(x_{i}^{t}),y_{i}^{t}),$
    |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\displaystyle\hat{R}_{t}(h_{t},f)=\frac{1}{N^{t}}\sum^{N^{t}}_{i=1}l(h_{t}\circ\hat{f}(x_{i}^{t}),y_{i}^{t}),$
    |  |'
- en: 'same as above, $h_{t}$ is the hypothesis of $\mathcal{T}^{t}$, $N^{t}$ denotes
    the number of training samples for $\mathcal{T}^{t}$, and $x_{i}^{t}$ and $y_{i}^{t}$
    represent the samples and corresponding labels in $\mathcal{T}^{t}$, respectively.
    In the second stage, our goal is to estimate a hypothesis $\hat{h_{t}}=\text{argmin}_{(f,h_{t})\in\mathcal{H}}\hat{R}_{t}(h_{t},\hat{f})$
    based on the shared feature representations learned in the first stage. We measure
    the function $(\hat{h_{t}},\hat{f})$ by the excess error on $\mathcal{T}^{t}$,
    namely:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述相同，$h_{t}$ 是 $\mathcal{T}^{t}$ 的假设，$N^{t}$ 表示 $\mathcal{T}^{t}$ 的训练样本数量，而
    $x_{i}^{t}$ 和 $y_{i}^{t}$ 分别表示 $\mathcal{T}^{t}$ 中的样本及其对应标签。在第二阶段，我们的目标是基于第一阶段学习到的共享特征表示来估计一个假设
    $\hat{h_{t}}=\text{argmin}_{(f,h_{t})\in\mathcal{H}}\hat{R}_{t}(h_{t},\hat{f})$。我们通过在
    $\mathcal{T}^{t}$ 上的过量误差来衡量函数 $(\hat{h_{t}},\hat{f})$，即：
- en: '| (7) |  |  | $\displaystyle\mathbb{E}[R_{excess}]=\mathbb{E}[R_{t}(\hat{h_{t}},\hat{f})-R_{t}(h_{t}^{\dagger},f^{\dagger})]$
    |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  |  | $\displaystyle\mathbb{E}[R_{excess}]=\mathbb{E}[R_{t}(\hat{h_{t}},\hat{f})-R_{t}(h_{t}^{\dagger},f^{\dagger})]$
    |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}[R_{t}(h^{\ast}_{t},f^{\dagger})-R_{t}(h^{\dagger}_{t},f^{\dagger})]+\mathbb{E}[R_{t}(\hat{h}_{t},\hat{f})-R_{t}(h^{\ast}_{t},f^{\dagger})],$
    |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}[R_{t}(h^{\ast}_{t},f^{\dagger})-R_{t}(h^{\dagger}_{t},f^{\dagger})]+\mathbb{E}[R_{t}(\hat{h}_{t},\hat{f})-R_{t}(h^{\ast}_{t},f^{\dagger})],$
    |  |'
- en: 'Among them, $R_{t}(\cdot,\cdot)$ represents the expected risk on $\mathcal{T}^{t}$.
    $R_{excess}$ represents the relationship between the expected risk of $(\hat{h_{t}},\hat{f})$
    and the optimal prediction rule $(h_{t}^{\dagger},f^{\dagger})$. Besides, we represents
    the estimation error with $\mathbb{E}[R_{t}(\hat{h}_{t},\hat{f})-R_{t}(h^{\ast}_{t},f^{\dagger})]$,
    *i.e*., minimizing the empirical risk $\hat{R}_{t}(h_{t},f)$ in $\mathcal{H}$
    instead of the expected risk $R_{t}(h_{t},f)$, as shown by the blue dotted line
    in Figure [5](#S2.F5 "Figure 5 ‣ 2.4.2\. Two-Stage Empirical Risk Minimization
    (TSERM) ‣ 2.4\. Unique Issue and Challenge ‣ 2\. Background ‣ Deep Learning for
    Cross-Domain Few-Shot Visual Recognition: A Survey").'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$R_{t}(\cdot,\cdot)$ 代表了 $\mathcal{T}^{t}$ 上的期望风险。$R_{excess}$ 表示 $(\hat{h_{t}},\hat{f})$
    和最佳预测规则 $(h_{t}^{\dagger},f^{\dagger})$ 之间的期望风险关系。此外，我们用 $\mathbb{E}[R_{t}(\hat{h}_{t},\hat{f})-R_{t}(h^{\ast}_{t},f^{\dagger})]$
    表示估计误差，*即*，在 $\mathcal{H}$ 中最小化经验风险 $\hat{R}_{t}(h_{t},f)$ 而不是期望风险 $R_{t}(h_{t},f)$，如图
    [5](#S2.F5 "Figure 5 ‣ 2.4.2\. Two-Stage Empirical Risk Minimization (TSERM) ‣
    2.4\. Unique Issue and Challenge ‣ 2\. Background ‣ Deep Learning for Cross-Domain
    Few-Shot Visual Recognition: A Survey") 中的蓝色虚线所示。'
- en: '![Refer to caption](img/e52b8c96a4fb240bc615316182a7f99f.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e52b8c96a4fb240bc615316182a7f99f.png)'
- en: Figure 5\. Comparison of vanilla supervised learning, FSL, and CDFSL problem.
    Solid circles denote distributions in which the data resides (the size means the
    amount of data), and dotted circles indicate the domain to which the target distribution
    belongs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 普通监督学习、FSL 和 CDFSL 问题的比较。实心圆表示数据所在的分布（大小表示数据量），虚线圆表示目标分布所属的领域。
- en: 2.4.3\. Unique Issue and Challenge
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3\. 独特问题与挑战
- en: 'We cannot optimize the approximation error, *i.e*. $\mathbb{E}[R_{t}(h^{\ast}_{t},f^{\dagger})-R_{t}(h^{\dagger}_{t},f^{\dagger})]$,
    due to the limitation of $\mathcal{H}$. Therefore, our goal is to optimize the
    estimation error, *i.e*. $\mathbb{E}[R_{t}(\hat{h}_{t},\hat{f})-R_{t}(h^{\ast}_{t},f^{\dagger})]$.
    In Figure [5](#S2.F5 "Figure 5 ‣ 2.4.2\. Two-Stage Empirical Risk Minimization
    (TSERM) ‣ 2.4\. Unique Issue and Challenge ‣ 2\. Background ‣ Deep Learning for
    Cross-Domain Few-Shot Visual Recognition: A Survey"), the solid black arrow expresses
    the learning of empirical risk minimization. The solid circles indicate the different
    data distributions (the size of the circle means the amount of supervised information,
    the green and blue circles mean the source domain and target domain, respectively).
    The distribution where the target sample is located is depicted by the blue dotted
    circle. In Figure [5](#S2.F5 "Figure 5 ‣ 2.4.2\. Two-Stage Empirical Risk Minimization
    (TSERM) ‣ 2.4\. Unique Issue and Challenge ‣ 2\. Background ‣ Deep Learning for
    Cross-Domain Few-Shot Visual Recognition: A Survey"), (a) shows a vanilla supervised
    learning problem. It is easy to achieve ERM learning in the case of a large data
    set. The left part of (b) denotes the FSL problem, where the learning of ERM is
    not ideal when the amount of data is insufficient. The existing FSL strategy provides
    a good initialization for the target task through the different but relevant source
    task, as shown in the right part of Figure [5](#S2.F5 "Figure 5 ‣ 2.4.2\. Two-Stage
    Empirical Risk Minimization (TSERM) ‣ 2.4\. Unique Issue and Challenge ‣ 2\. Background
    ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey") (b).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '由于 $\mathcal{H}$ 的限制，我们无法优化近似误差，*即* $\mathbb{E}[R_{t}(h^{\ast}_{t},f^{\dagger})-R_{t}(h^{\dagger}_{t},f^{\dagger})]$。因此，我们的目标是优化估计误差，*即*
    $\mathbb{E}[R_{t}(\hat{h}_{t},\hat{f})-R_{t}(h^{\ast}_{t},f^{\dagger})]$。在图 [5](#S2.F5
    "Figure 5 ‣ 2.4.2\. Two-Stage Empirical Risk Minimization (TSERM) ‣ 2.4\. Unique
    Issue and Challenge ‣ 2\. Background ‣ Deep Learning for Cross-Domain Few-Shot
    Visual Recognition: A Survey") 中，实心黑色箭头表示经验风险最小化的学习。实心圆表示不同的数据分布（圆的大小表示监督信息的量，绿色和蓝色圆分别表示源领域和目标领域）。目标样本所在的分布用蓝色虚线圆表示。在图
    [5](#S2.F5 "Figure 5 ‣ 2.4.2\. Two-Stage Empirical Risk Minimization (TSERM) ‣
    2.4\. Unique Issue and Challenge ‣ 2\. Background ‣ Deep Learning for Cross-Domain
    Few-Shot Visual Recognition: A Survey") 中，(a) 显示了一个普通的监督学习问题。在数据集较大的情况下，容易实现 ERM
    学习。 (b) 的左侧部分表示 FSL 问题，当数据量不足时，ERM 学习效果不佳。现有的 FSL 策略通过不同但相关的源任务为目标任务提供了良好的初始化，如图
    [5](#S2.F5 "Figure 5 ‣ 2.4.2\. Two-Stage Empirical Risk Minimization (TSERM) ‣
    2.4\. Unique Issue and Challenge ‣ 2\. Background ‣ Deep Learning for Cross-Domain
    Few-Shot Visual Recognition: A Survey") (b) 右侧部分所示。'
- en: 'As a result of the domain gaps between the source and target datasets, a novel
    problem of CDFSL arises, as illustrated in Figure [5](#S2.F5 "Figure 5 ‣ 2.4.2\.
    Two-Stage Empirical Risk Minimization (TSERM) ‣ 2.4\. Unique Issue and Challenge
    ‣ 2\. Background ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey")(c). As such, it is evident that the CDFSL problem involves both domain
    gaps and task shifts between the source and target domains, with limited supervised
    information available in $\mathcal{D}^{t}$. This makes CDFSL have its unique challenges
    while inheriting the challenges of FSL, namely an unreliable TSERM (estimation
    error optimization) due to the following factor: the CDFSL problem is characterized
    by domain gaps and task shifts, leading to a limited correlation between the source
    and target domains, thereby restricting the shared knowledge between them. As
    a result, it becomes challenging for the model to identify the optimal function
    $f$ for task $\mathcal{T}^{t}$ with the support of $\mathcal{D}^{s}$ and $\mathcal{T}^{s}$,
    where $\mathcal{D}^{s}\neq\mathcal{D}^{t}$ and $\mathcal{T}^{s}\neq\mathcal{T}^{t}$.
    In other words, the shared knowledge between the source and target domains is
    challenging to extract.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '由于源数据集和目标数据集之间的领域差距，CDFSL出现了一个新问题，如图 [5](#S2.F5 "Figure 5 ‣ 2.4.2\. Two-Stage
    Empirical Risk Minimization (TSERM) ‣ 2.4\. Unique Issue and Challenge ‣ 2\. Background
    ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey")(c)所示。因此，很明显，CDFSL问题涉及源领域和目标领域之间的领域差距和任务转移，同时$\mathcal{D}^{t}$中可用的监督信息有限。这使得CDFSL具有其独特的挑战，同时继承了FSL的挑战，即由于以下因素导致不可靠的TSERM（估计误差优化）：CDFSL问题的特点是领域差距和任务转移，导致源领域和目标领域之间的相关性有限，从而限制了它们之间的共享知识。因此，模型在支持$\mathcal{D}^{s}$和$\mathcal{T}^{s}$的情况下识别任务$\mathcal{T}^{t}$的最优函数$f$变得具有挑战性，其中$\mathcal{D}^{s}\neq\mathcal{D}^{t}$且$\mathcal{T}^{s}\neq\mathcal{T}^{t}$。换句话说，源领域和目标领域之间的共享知识难以提取。'
- en: 2.5\. Taxonomy
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5\. 分类
- en: 'According to the above unique issue and challenge, CDFSL aims at mining as
    much shared knowledge as possible and finding the optimal $f$ for the target domain.
    Based on this consideration and to answer the question “how to transfer”, all
    the CDFSL techniques are categorized into the following four in this paper, as
    shown in Figure [6](#S2.F6 "Figure 6 ‣ 2.5\. Taxonomy ‣ 2\. Background ‣ Deep
    Learning for Cross-Domain Few-Shot Visual Recognition: A Survey"):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '根据上述独特问题和挑战，CDFSL旨在挖掘尽可能多的共享知识，并找到目标领域的最优$f$。基于这一考虑，并回答“如何转移”的问题，本文将所有CDFSL技术分类为以下四类，如图 [6](#S2.F6
    "Figure 6 ‣ 2.5\. Taxonomy ‣ 2\. Background ‣ Deep Learning for Cross-Domain Few-Shot
    Visual Recognition: A Survey")所示：'
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Instance-guided Approaches. The model learns the optimal features from more
    diverse samples by introducing a subset of instances.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实例引导的方法。通过引入实例子集，模型从更多样本中学习最优特征。
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Parameter-based Approaches. Optimizing the model parameters and excluding some
    regions of $\mathcal{H}$ where the optimal function is unlikely to exist, reduces
    the scope of $\mathcal{H}$.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于参数的方法。通过优化模型参数并排除一些$\mathcal{H}$区域中不太可能存在最优函数的区域，减少了$\mathcal{H}$的范围。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Feature Post-processing Approaches. Learning a feature function from the source
    domain and performing subsequent processing on its features. A new feature closest
    to $f^{\dagger}$ is obtained through post-processing operations.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征后处理方法。从源领域学习特征函数，并对其特征进行后续处理。通过后处理操作获得与$f^{\dagger}$最接近的新特征。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hybrid Approaches. Combining the multiple strategies from the above three categories.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 混合方法。结合上述三类中的多种策略。
- en: '![Refer to caption](img/182e0d4d9c6174aae5aea99c17ff7421.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/182e0d4d9c6174aae5aea99c17ff7421.png)'
- en: Figure 6\. Different perspectives on how the CDFSL approaches find the optimal
    features.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. CDFSL方法如何找到最优特征的不同视角。
- en: 'Accordingly, existing works can be categorized into a unified taxonomy. In
    the following sections, we will detail each category, performances, future works,
    and conclusion. The main contents of this paper are shown in Figure [7](#S2.F7
    "Figure 7 ‣ 2.5\. Taxonomy ‣ 2\. Background ‣ Deep Learning for Cross-Domain Few-Shot
    Visual Recognition: A Survey").'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，现有工作可以被分类为一个统一的分类系统。在接下来的部分中，我们将详细介绍每个类别、性能、未来工作和结论。本文的主要内容如图 [7](#S2.F7
    "Figure 7 ‣ 2.5\. Taxonomy ‣ 2\. Background ‣ Deep Learning for Cross-Domain Few-Shot
    Visual Recognition: A Survey")所示。'
- en: '![Refer to caption](img/f99092ba68fa3bbb378077bbdf1ef6f4.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f99092ba68fa3bbb378077bbdf1ef6f4.png)'
- en: Figure 7\. Outline of our survey. The main contents include the benchmarks,
    challenges, related topics, methodology, and future works of CDFSL.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. 我们调查的概述。主要内容包括CDFSL的基准、挑战、相关主题、方法论和未来工作。
- en: 3\. Approaches
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 方法
- en: 'CDFSL offers a unified solution to both cross-domain and few-shot learning
    problems. Based on the analysis of unique issue and challenges, we present a classification
    criterion for CDFSL algorithms, dividing them into four categories: instance-guided,
    parameter-based, feature post-processing, and hybrid approaches. The overview
    of CDFSL is depicted in Figure [8](#S3.F8 "Figure 8 ‣ 3\. Approaches ‣ Deep Learning
    for Cross-Domain Few-Shot Visual Recognition: A Survey").'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 'CDFSL提供了对跨领域和少样本学习问题的统一解决方案。基于对独特问题和挑战的分析，我们提出了CDFSL算法的分类标准，将其分为四类：实例指导、参数基础、特征后处理和混合方法。CDFSL的概述如图[8](#S3.F8
    "图 8 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习: 调查")所示。'
- en: '![Refer to caption](img/dc70d38154859374b50158850aa52865.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dc70d38154859374b50158850aa52865.png)'
- en: Figure 8\. An overall diagram of the CDFSL method. Firstly, the existing techniques
    pre-train the feature extractor on the source domain. Secondly, they fine-tune
    the feature extractor and train a novel recognizer on the target domain with limited
    labels. We classify existing CDFSL methods into Instance-guided, parameter-based,
    and feature post-processing.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. CDFSL方法的整体图示。首先，现有技术在源领域上预训练特征提取器。其次，它们在目标领域通过有限标签微调特征提取器并训练新的识别器。我们将现有的CDFSL方法分类为实例指导、参数基础和特征后处理。
- en: 3.1\. Instance-guided Approaches
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 实例指导的方法
- en: '![Refer to caption](img/4b7ba222d47ea0788a13216b36dc9eb4.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4b7ba222d47ea0788a13216b36dc9eb4.png)'
- en: Figure 9\. The different categories of instance-guided approaches. Instances
    are from different sources. $\theta$ means the recognizer.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. 实例指导方法的不同类别。实例来自不同来源。$\theta$ 代表识别器。
- en: 'This section presents approaches that learn the shared feature representation
    by incorporating additional valid instances from various sources, including the
    source domain, target domain, and additional domains. The diverse information
    provided by these sources offers practical guidance for finding shared features.
    For example, information from the source domain, often obtained from different
    modalities and views, expands the practical information and facilitates the learning
    of shared features. Furthermore, by incorporating information from the target
    domain, the model can better understand the target domain and generalize to it
    more easily. Information from multiple domains enables the model to learn a shared
    representation from various domains, making the learned features more generalizable.
    These approaches are illustrated in Figure [9](#S3.F9 "Figure 9 ‣ 3.1\. Instance-guided
    Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey"), and their details are presented in Table [1](#S3.T1 "Table 1 ‣ 3.1\.
    Instance-guided Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot
    Visual Recognition: A Survey").'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '本节介绍了通过结合来自各种来源的额外有效实例来学习共享特征表示的方法，包括源领域、目标领域和附加领域。这些来源提供的多样信息为寻找共享特征提供了实际指导。例如，来自源领域的信息，通常来自不同的模态和视图，扩展了实际信息，并促进了共享特征的学习。此外，通过结合来自目标领域的信息，模型可以更好地理解目标领域，并更容易地推广到该领域。来自多个领域的信息使得模型能够从各种领域中学习共享表示，从而使学习到的特征更具泛化性。这些方法在图[9](#S3.F9
    "图 9 ‣ 3.1\. 实例指导的方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习: 调查")中有所说明，其详细信息在表[1](#S3.T1 "表
    1 ‣ 3.1\. 实例指导的方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习: 调查")中呈现。'
- en: Table 1\. Representative instance-guided CDFSL approaches. ‘FG’, ‘Art’ and ‘IW’
    indicate that evaluation of Fine-grain based CDFSL (FG), Art-based CDFSL (Art)
    and imaging way-based CDFSL (IW), respectively. ‘CWUT’ means Channel-Wise Uniform
    Transformation. And ‘KBS’ represents Knowledge-Based Systems.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 代表性的实例指导CDFSL方法。‘FG’，‘Art’和‘I W’分别表示对细粒度基于CDFSL（FG）、艺术基于CDFSL（Art）和成像方式基于CDFSL（IW）的评估。‘CWUT’表示通道级均匀变换。‘KBS’代表基于知识的系统。
- en: '| Methods | Venue | Instances from | Introduced information | Loss function
    | FG | Art | IW |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 场地 | 来源实例 | 介绍的信息 | 损失函数 | FG | Art | IW |'
- en: '| TriAE (Guan et al., [2020](#bib.bib30)) | ACCV 2020 | Original data | Labels
    | $L_{2}$ |  | ✓ |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| TriAE (Guan et al., [2020](#bib.bib30)) | ACCV 2020 | 原始数据 | 标签 | $L_{2}$
    |  | ✓ |  |'
- en: '| NSAE (Liang et al., [2021](#bib.bib54)) | ICCV 2021 | Original data | Generated
    images | BSR & Log | ✓ |  | ✓ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| NSAE (Liang et al., [2021](#bib.bib54)) | ICCV 2021 | 原始数据 | 生成图像 | BSR &
    Log | ✓ |  | ✓ |'
- en: '| SET-RCL (Li et al., [2022c](#bib.bib53)) | ACM MM 2022 | Original data |
    CWUT | CE & Contrastive & Log | ✓ |  | ✓ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| SET-RCL (Li et al., [2022c](#bib.bib53)) | ACM MM 2022 | 原始数据 | CWUT | CE
    & Contrastive & Log | ✓ |  | ✓ |'
- en: '| MDKT (Li et al., [2021b](#bib.bib48)) | Neurocomputing 2021 | Original data
    | Class semantic | CE | ✓ |  |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| MDKT (Li et al., [2021b](#bib.bib48)) | Neurocomputing 2021 | 原始数据 | 类语义
    | CE | ✓ |  |  |'
- en: '| CDPSN (Gong et al., [2023](#bib.bib29)) | Scientific Reports 2023 | Original
    data | Sketch map | CE |  |  | ✓ |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| CDPSN (Gong et al., [2023](#bib.bib29)) | Scientific Reports 2023 | 原始数据
    | 草图图 | CE |  |  | ✓ |'
- en: '| ST (Liu et al., [2023](#bib.bib59)) | KBS 2023 | Original data | Transformation
    | CE | ✓ |  |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| ST (Liu et al., [2023](#bib.bib59)) | KBS 2023 | 原始数据 | 转换 | CE | ✓ |  |  |'
- en: '| DAML (Lee et al., [2022](#bib.bib46)) | ICASSP 2022 | Multiple domains |
    3 other datasets | CE | ✓ | ✓ |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| DAML (Lee et al., [2022](#bib.bib46)) | ICASSP 2022 | 多领域 | 3个其他数据集 | CE
    | ✓ | ✓ |  |'
- en: '| MCDFSL (Xu and Liu, [2022](#bib.bib118)) | arXiv 2022 | Multiple domains
    | 7 auxiliary datasets | BSR & Perceptual & Style |  |  | ✓ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| MCDFSL (Xu and Liu, [2022](#bib.bib118)) | arXiv 2022 | 多领域 | 7个辅助数据集 | BSR
    & Perceptual & Style |  |  | ✓ |'
- en: '| STARTUP (Phoo and Hariharan, [2020](#bib.bib80)) | ICLR 2021 | Target domain
    | Unlabeled target data | CE & KL & SimCLR |  |  | ✓ |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| STARTUP (Phoo and Hariharan, [2020](#bib.bib80)) | ICLR 2021 | 目标领域 | 未标记的目标数据
    | CE & KL & SimCLR |  |  | ✓ |'
- en: '| DDN (Islam et al., [2021](#bib.bib39)) | NIPS 2021 | Target domain | Unlabeled
    target data | CE |  |  | ✓ |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| DDN (Islam et al., [2021](#bib.bib39)) | NIPS 2021 | 目标领域 | 未标记的目标数据 | CE
    |  |  | ✓ |'
- en: '| DSL (Yao, [2021](#bib.bib124)) | ICLR 2022 | Target domain | Multiple targets
    | RCE & Binary KLD | ✓ |  |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| DSL (Yao, [2021](#bib.bib124)) | ICLR 2022 | 目标领域 | 多个目标 | RCE & Binary KLD
    | ✓ |  |  |'
- en: '| UD (Hu et al., [2021](#bib.bib38)) | arXiv 2021 | Target domain | Unlabeled
    target data | Log |  | ✓ |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| UD (Hu et al., [2021](#bib.bib38)) | arXiv 2021 | 目标领域 | 未标记的目标数据 | Log |  |
    ✓ |  |'
- en: 3.1.1\. Instances from Extra Information of Original Data
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 来自原始数据额外信息的实例
- en: 'Some approaches involve the use of additional information from the original
    data, such as semantic and visual information, to enhance the performance of FSL
    tasks, as shown in Figure [10](#S3.F10 "Figure 10 ‣ 3.1.1\. Instances from Extra
    Information of Original Data ‣ 3.1\. Instance-guided Approaches ‣ 3\. Approaches
    ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey"). Among
    them, some works extract this extra information through reconstructed instances,
    as depicted in the green background area of the Figure [10](#S3.F10 "Figure 10
    ‣ 3.1.1\. Instances from Extra Information of Original Data ‣ 3.1\. Instance-guided
    Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey"). For example, in (Guan et al., [2020](#bib.bib30)), a Triplet Autoencoder
    (TriAE) is utilized to learn a shared feature representation. It incorporates
    both source and target instances, and leverages semantic information as an intermediate
    bridge. In (Liang et al., [2021](#bib.bib54)), an autoencoder is used to reconstruct
    the input data, and the reconstructed data is then utilized as additional visual
    information to aid in the training process and learn the shared feature representation.
    And (Li et al., [2022c](#bib.bib53)) distills the knowledge of multiple tasks/domain-specific
    networks into a single network. This is achieved by aligning the representations
    of the single network with the task/domain-specific ones using small capacity
    adapters.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法涉及使用来自原始数据的额外信息，如语义和视觉信息，以增强FSL任务的性能，如图[10](#S3.F10 "图 10 ‣ 3.1.1\. 来自原始数据额外信息的实例
    ‣ 3.1\. 实例引导的方法 ‣ 3\. 方法 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述")所示。其中一些工作通过重构实例来提取这些额外信息，如图[10](#S3.F10
    "图 10 ‣ 3.1.1\. 来自原始数据额外信息的实例 ‣ 3.1\. 实例引导的方法 ‣ 3\. 方法 ‣ 深度学习在跨领域少样本视觉识别中的应用：综述")的绿色背景区域所示。例如，在(Guan
    et al., [2020](#bib.bib30))中，使用了三重自编码器（TriAE）来学习共享特征表示。它结合了源实例和目标实例，并利用语义信息作为中间桥梁。在(Liang
    et al., [2021](#bib.bib54))中，使用了自编码器来重构输入数据，重构数据随后作为额外的视觉信息来辅助训练过程并学习共享特征表示。而(Li
    et al., [2022c](#bib.bib53))则将多个任务/领域特定网络的知识提炼到一个单一网络中。这是通过使用小容量适配器对齐单一网络的表示与任务/领域特定网络的表示来实现的。
- en: '![Refer to caption](img/123d26929cc4a4cda76f2e52476bc551.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/123d26929cc4a4cda76f2e52476bc551.png)'
- en: Figure 10\. Instances from extra information of original data. The extra information
    can be from generation model (green area) or other modal like texts (blue part).
    Dotted lines indicate the process of additional information introduction.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图10\. 原始数据的附加信息的实例。额外信息可以来自生成模型（绿色区域）或其他模态，如文本（蓝色部分）。虚线表示附加信息引入的过程。
- en: 'Meanwhile, other works directly add additional information to the model, as
    shown in the blue part of Figure [10](#S3.F10 "Figure 10 ‣ 3.1.1\. Instances from
    Extra Information of Original Data ‣ 3.1\. Instance-guided Approaches ‣ 3\. Approaches
    ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey"). For
    instance, (Li et al., [2021b](#bib.bib48)) presents a model that integrates visual
    and semantic information to recognize target categories, and utilizes weight imprinting
    for future fine-tuning. Furthermore, in (Gong et al., [2023](#bib.bib29)), the
    original image and its corresponding sketch map are processed separately by different
    branches of the network. The features extracted from the original image are combined
    with the contour features extracted from the sketch map branch during training,
    thus improving the accuracy and generalization performance of the model. Moreover, (Liu
    et al., [2023](#bib.bib59)) proposes a task-expansion-decomposition framework
    for CD-FSL called the self-taught (ST) approach, which alleviates the problem
    of non-target guidance by constructing task-oriented metric spaces.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，其他工作直接向模型添加附加信息，如图[10](#S3.F10 "图10 ‣ 3.1.1\. 原始数据附加信息的实例 ‣ 3.1\. 实例指导方法
    ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习：一项调查")中的蓝色部分所示。例如，(Li等人，[2021b](#bib.bib48))提出了一个整合视觉和语义信息以识别目标类别的模型，并利用权重印记进行未来微调。此外，在(Gong等人，[2023](#bib.bib29))中，原始图像及其对应的草图地图分别由网络的不同分支进行处理。在训练过程中，从原始图像提取的特征与从草图地图分支提取的轮廓特征相结合，从而提高了模型的准确性和泛化性能。此外，(Liu等人，[2023](#bib.bib59))提出了一种称为自我教的（ST）方法的CD-FSL任务扩展分解框架，通过构建面向任务的度量空间缓解了非目标指导的问题。
- en: 3.1.2\. Instances from Multiple Domains
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 来自多个领域的实例
- en: 'By utilizing instances from multiple domains, a model can learn a general shared
    representation with broad generalization ability. The Domain-Agnostic Meta Learning
    (DAML) algorithm, proposed in (Lee et al., [2022](#bib.bib46)), adapts the model
    to novel classes in seen and unseen domains. In contrast, (Xu and Liu, [2022](#bib.bib118))
    introduces unlabeled data from multiple domains into the original source domain
    to transfer diverse styles, making the model more adaptable to various domains
    and styles. Moreover, most methods combine multiple strategies together with the
    multiple-domain introduction strategy, as shown in Section [3.4](#S3.SS4 "3.4\.
    Hybrid Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual
    Recognition: A Survey").'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用来自多个领域的实例，模型可以学习到具有广泛泛化能力的共享通用表示。Domain-Agnostic Meta Learning（DAML）算法，提出于(Lee等人，[2022](#bib.bib46))，使模型适应在已见和未见的领域中的新类别。相比之下，(Xu和Liu，[2022](#bib.bib118))引入了来自多个领域的未标记数据到原始源领域，以转移多样的风格，使模型更适应各种领域和风格。此外，大多数方法将多种策略与多领域引入策略结合在一起，如第[3.4](#S3.SS4
    "3.4\. 混合方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习：一项调查")节所示。
- en: 3.1.3\. Instances from Target Domain
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 来自目标领域的实例
- en: 'Approaches that leverage target domain instances aim to uncover the shared
    information between the source and target domains. Some of these approaches employ
    a teacher-student network to aid CDFSL learning. For example, in (Phoo and Hariharan,
    [2020](#bib.bib80)), (illustrated in Figure [11](#S3.F11 "Figure 11 ‣ 3.1.3\.
    Instances from Target Domain ‣ 3.1\. Instance-guided Approaches ‣ 3\. Approaches
    ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey")), a self-training
    method is proposed that utilizes unlabeled target data to improve the source domain
    representation. It is the first work to introduce the unlabeled target data into
    the training phase.  (Islam et al., [2021](#bib.bib39)) follows this setting and
    enforces consistency by comparing predictions of weakly-augmented unlabeled target
    data from a teacher network to strongly-augmented versions of the same images
    from a student network. Meanwhile, (Yao, [2021](#bib.bib124)) develops a self-supervised
    learning approach to fully leverage unlabeled target domain data.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 利用目标领域实例的方法旨在揭示源领域和目标领域之间的共享信息。这些方法中的一些采用了教师-学生网络来辅助 CDFSL 学习。例如，在 (Phoo 和 Hariharan,
    [2020](#bib.bib80))中（如图 [11](#S3.F11 "图 11 ‣ 3.1.3\. 目标领域的实例 ‣ 3.1\. 基于实例的方法 ‣
    3\. 方法 ‣ 跨领域少样本视觉识别：综述") 所示），提出了一种自我训练的方法，利用未标记的目标数据来改善源领域表示。这是第一个将未标记的目标数据引入训练阶段的工作。 (Islam
    等, [2021](#bib.bib39)) 跟随这一设置，通过将教师网络的弱增强未标记目标数据的预测与学生网络中同一图像的强增强版本进行比较来强制一致性。同时， (Yao,
    [2021](#bib.bib124)) 开发了一种自监督学习方法，以充分利用未标记的目标领域数据。
- en: '![Refer to caption](img/078cdb2d434034661e017a1b9e7c4f56.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/078cdb2d434034661e017a1b9e7c4f56.png)'
- en: Figure 11\. STARTUP (Phoo and Hariharan, [2020](#bib.bib80)) structure. The
    dotted line represents how to use the auxiliary target data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图11\. STARTUP (Phoo 和 Hariharan, [2020](#bib.bib80)) 结构。虚线表示如何使用辅助目标数据。
- en: Other works integrate all labeled target data directly into the training process.
    For instance, (Hu et al., [2021](#bib.bib38)) presents a Domain-Switch Learning
    (DSL) framework that embeds cross-domain scenarios into the training phase in
    a ”fast switching” manner using multiple target domains.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究直接将所有标记的目标数据整合到训练过程中。例如， (Hu 等, [2021](#bib.bib38)) 提出了一个领域切换学习（DSL）框架，使用多个目标领域以“快速切换”方式将跨领域场景嵌入训练阶段。
- en: 3.1.4\. Discussion and Summary
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4\. 讨论与总结
- en: 'Instance-guided strategies are chosen based on the availability of data. When
    the source domain includes extra information such as semantic and visual information,
    utilizing instances from the original source (as described in Section [3.1.1](#S3.SS1.SSS1
    "3.1.1\. Instances from Extra Information of Original Data ‣ 3.1\. Instance-guided
    Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey")) is an effective approach. However, in scenarios where extra information
    is not available, introducing instances from the target domain (as discussed in
    Section [3.1.3](#S3.SS1.SSS3 "3.1.3\. Instances from Target Domain ‣ 3.1\. Instance-guided
    Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey")) may be a better option. In cases where target data is scarce or unavailable,
    utilizing instances from multiple domains (as outlined in Section [3.1.2](#S3.SS1.SSS2
    "3.1.2\. Instances from Multiple Domains ‣ 3.1\. Instance-guided Approaches ‣
    3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A
    Survey")) can also be helpful.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 基于实例的策略是根据数据的可用性来选择的。当源领域包括额外的信息，如语义和视觉信息时，利用来自原始源的实例（如第 [3.1.1](#S3.SS1.SSS1
    "3.1.1\. 原始数据额外信息中的实例 ‣ 3.1\. 基于实例的方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别：综述") 节所述）是一种有效的方法。然而，在没有额外信息的情况下，引入目标领域的实例（如第 [3.1.3](#S3.SS1.SSS3
    "3.1.3\. 目标领域的实例 ‣ 3.1\. 基于实例的方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别：综述") 节讨论）可能是更好的选择。在目标数据稀缺或不可用的情况下，利用来自多个领域的实例（如第 [3.1.2](#S3.SS1.SSS2
    "3.1.2\. 多领域实例 ‣ 3.1\. 基于实例的方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别：综述") 节所述）也会有所帮助。
- en: 3.2\. Parameter-based Approaches
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 基于参数的方法
- en: 'Parameter-based approaches are designed to reduce the complexity of the hypothesis
    space by manipulating the model’s parameters to discover shared feature representations.
    There are three main techniques in this approach, as illustrated in Figure [12](#S3.F12
    "Figure 12 ‣ 3.2\. Parameter-based Approaches ‣ 3\. Approaches ‣ Deep Learning
    for Cross-Domain Few-Shot Visual Recognition: A Survey"): (1) Parameter freeze
    involves fixing certain model parameters, simplifying the search for shared feature
    representations, (2) In parameter selection, the most appropriate model is selected
    from a pool of models based on their parameters, and (3) Parameter Reweighting
    employs additional parameters to constrain the hypothesis space. Table [2](#S3.T2
    "Table 2 ‣ 3.2.2\. Parameter Selection ‣ 3.2\. Parameter-based Approaches ‣ 3\.
    Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey")
    provides a detailed summary of the methods that fall under this category.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 基于参数的方法旨在通过操作模型的参数来发现共享的特征表示，从而减少假设空间的复杂性。这种方法主要有三种技术，如图 [12](#S3.F12 "图 12
    ‣ 3.2\. 基于参数的方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习：综述") 所示：（1）参数冻结涉及固定某些模型参数，简化对共享特征表示的搜索，（2）在参数选择中，从模型池中根据参数选择最合适的模型，以及（3）参数重加权使用额外的参数来约束假设空间。表 [2](#S3.T2
    "表 2 ‣ 3.2.2\. 参数选择 ‣ 3.2\. 基于参数的方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习：综述") 提供了属于这一类别的方法的详细总结。
- en: '![Refer to caption](img/05bab2265a9c2048c36bd20061b845c2.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/05bab2265a9c2048c36bd20061b845c2.png)'
- en: Figure 12\. Parameter-based category. (a), (b), and (c) indicate the parameter
    freeze, parameter selection, and parameter reweighting, respectively.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12\. 基于参数的类别。(a)、(b) 和 (c) 分别表示参数冻结、参数选择和参数重加权。
- en: 3.2.1\. Parameter Freeze
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 参数冻结
- en: 'Parameter freeze is a strategy that restricts the hypothesis space’s complexity
    by fixing some model parameters. This method is usually used in meta-learning-based
    approaches, where they alternately freeze some parameters during meta-training
    and meta-testing phases. Among them, score-based meta transfer-learning (SB-MTL) (Cai
    et al., [2020](#bib.bib7)) combines transfer-learning and meta-learning by using
    a MAML-optimized feature encoder and a score-based Graph Neural Network. Some
    parameters in MAML are frozen in the training phase. And in (Wang et al., [2021](#bib.bib111)),
    a meta-encoder is alternately frozen and optimized during the inner update phase
    to learn general features. In addition, other works propose plug-and-play augmentation
    modules to constrain the hypothesis space. In these works, the core idea of (Tseng
    et al., [2020](#bib.bib101)) is to asynchronously freeze and update the proposed
    feature-wise transformation layers and the feature extractor, as shown in Figure [12](#S3.F12
    "Figure 12 ‣ 3.2\. Parameter-based Approaches ‣ 3\. Approaches ‣ Deep Learning
    for Cross-Domain Few-Shot Visual Recognition: A Survey") (a). Due to the inspiring
    of (Tseng et al., [2020](#bib.bib101)), many works have improved and enhanced
    this work. (Yalan and Jijie, [2021](#bib.bib121)) proposes a diversified feature
    transformation based on the original feature transformation layer to solve the
    CDFSL problem. And (Chen et al., [2022b](#bib.bib12)) offer two new strategies,
    FGNN (Flexible GNN) and a new hierarchical residual-like block, for the encoder
    and metric function of the metric-based network.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 参数冻结是一种通过固定一些模型参数来限制假设空间复杂性的策略。这种方法通常用于基于元学习的方法中，在这些方法中，它们在元训练和元测试阶段交替冻结某些参数。其中，基于评分的元迁移学习（SB-MTL）(Cai
    et al., [2020](#bib.bib7)) 通过使用 MAML 优化的特征编码器和基于评分的图神经网络，将迁移学习和元学习结合起来。在训练阶段，MAML
    中的一些参数被冻结。而在 (Wang et al., [2021](#bib.bib111)) 中，元编码器在内部更新阶段交替冻结和优化以学习通用特征。此外，其他研究提出了即插即用的增强模块来约束假设空间。在这些研究中，(Tseng
    et al., [2020](#bib.bib101)) 的核心思想是异步冻结和更新提出的特征变换层和特征提取器，如图 [12](#S3.F12 "图 12
    ‣ 3.2\. 基于参数的方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习：综述") (a) 所示。由于 (Tseng et al., [2020](#bib.bib101))
    的启发，许多研究改进和增强了这项工作。 (Yalan 和 Jijie, [2021](#bib.bib121)) 提出了基于原始特征变换层的多样化特征变换，以解决
    CDFSL 问题。而 (Chen et al., [2022b](#bib.bib12)) 为度量网络的编码器和度量函数提供了两种新策略：FGNN（灵活的
    GNN）和新的分层残差块。
- en: 3.2.2\. Parameter Selection
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 参数选择
- en: 'The parameter selection strategy, as depicted in Figure [12](#S3.F12 "Figure
    12 ‣ 3.2\. Parameter-based Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain
    Few-Shot Visual Recognition: A Survey") (b), seeks to identify the most appropriate
    set of parameters for the target domain to enhance performance. To achieve this,
    researchers have proposed various methods. For example, in (Tu and Pao, [2021](#bib.bib102)),
    the authors sample sub-networks by dropping neurons or feature maps, and then
    choose the most suitable sub-networks to form an ensemble for target domain learning.
    Additionally, (Lin et al., [2021](#bib.bib56)) proposes a dynamic selection mechanism
    by sequentially applying multiple state-of-the-art adaptation methods, thereby
    enabling the configuration of the most appropriate modules for the downstream
    task.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [12](#S3.F12 "Figure 12 ‣ 3.2\. Parameter-based Approaches ‣ 3\. Approaches
    ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey") (b) 所示的参数选择策略，旨在确定最适合目标领域的参数集以提高性能。为此，研究人员提出了各种方法。例如，在
    (Tu and Pao, [2021](#bib.bib102)) 中，作者通过丢弃神经元或特征图来采样子网络，然后选择最合适的子网络组成一个集成用于目标领域学习。此外，(Lin
    et al., [2021](#bib.bib56)) 提出了通过顺序应用多种最先进的适应方法来实现动态选择机制，从而为下游任务配置最合适的模块。'
- en: Table 2\. Representative parameter-based CDFSL approaches. ‘NCA’ means Neural
    Computing and Applications.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 具有代表性的基于参数的 CDFSL 方法。‘NCA’ 指神经计算与应用。
- en: '| Methods | Venue | Strategy | Parameter  operation | Loss function | FG |
    Art | IW |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 场地 | 策略 | 参数操作 | 损失函数 | FG | 艺术 | IW |'
- en: '| SB-MTL (Cai et al., [2020](#bib.bib7)) | arXiv 2020 | Parameter freeze |
    Freeze partial layers in inner loop and update all network in outer loop | CE
    |  |  | ✓ |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| SB-MTL (Cai et al., [2020](#bib.bib7)) | arXiv 2020 | 参数冻结 | 在内循环中冻结部分层，并在外循环中更新所有网络
    | CE |  |  | ✓ |'
- en: '| MPL (Wang et al., [2021](#bib.bib111)) | TNNLS 2022 | Parameter freeze |
    Freeze network in inner loop and update it in meta update | CE | ✓ | ✓ |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| MPL (Wang et al., [2021](#bib.bib111)) | TNNLS 2022 | 参数冻结 | 在内循环中冻结网络，并在元更新中更新它
    | CE | ✓ | ✓ |  |'
- en: '| FWT (Tseng et al., [2020](#bib.bib101)) | ICLR 2020 | Parameter freeze |
    Alternately update the parameters of the feature-wise transformation layers and
    backbone | CE | ✓ |  |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| FWT (Tseng et al., [2020](#bib.bib101)) | ICLR 2020 | 参数冻结 | 交替更新特征级转换层和骨干网络的参数
    | CE | ✓ |  |  |'
- en: '| DFTL (Yalan and Jijie, [2021](#bib.bib121)) | ICAICA 2021 | Parameter freeze
    | Following the training setup of (Tseng et al., [2020](#bib.bib101)), and utilize
    multiple FWT modules in each layers | CE | ✓ |  |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| DFTL (Yalan and Jijie, [2021](#bib.bib121)) | ICAICA 2021 | 参数冻结 | 遵循 (Tseng
    et al., [2020](#bib.bib101)) 的训练设置，并在每层中使用多个 FWT 模块 | CE | ✓ |  |  |'
- en: '| FGNN (Chen et al., [2022b](#bib.bib12)) | KBS 2022 | Parameter freeze | Following
    the training setup of (Tseng et al., [2020](#bib.bib101)) | Softmax | ✓ |  |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| FGNN (Chen et al., [2022b](#bib.bib12)) | KBS 2022 | 参数冻结 | 遵循 (Tseng et
    al., [2020](#bib.bib101)) 的训练设置 | Softmax | ✓ |  |  |'
- en: '| AugSelect (Tu and Pao, [2021](#bib.bib102)) | Big Data 2021 | Parameter selection
    | Select from multiple sub-network that obtained by dropping feature maps | CE
    | ✓ | ✓ |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| AugSelect (Tu and Pao, [2021](#bib.bib102)) | Big Data 2021 | 参数选择 | 从通过丢弃特征图获得的多个子网络中进行选择
    | CE | ✓ | ✓ |  |'
- en: '| MAP (Lin et al., [2021](#bib.bib56)) | arXiv 2021 | Parameter selection |
    Select from different modular adaptation pipeline | CE | ✓ | ✓ |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| MAP (Lin et al., [2021](#bib.bib56)) | arXiv 2021 | 参数选择 | 从不同的模块适应管道中进行选择
    | CE | ✓ | ✓ |  |'
- en: '| ReFine (Oh et al., [2022](#bib.bib74)) | CIKM 2022 | Parameter reweighting
    | Re-randomize the top layers of the feature extractor before fine-tuning on the
    target domain | CE |  |  | ✓ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| ReFine (Oh et al., [2022](#bib.bib74)) | CIKM 2022 | 参数重加权 | 在目标领域微调之前重新随机化特征提取器的顶层
    | CE |  |  | ✓ |'
- en: '| VDB (Yazdanpanah and Moradi, [2022](#bib.bib125)) | CVPRW 2022 | Parameter
    reweighting | Introducing the ”Visual Domain Bridge” into CNN’s Batch Normalization
    (BN) layers | CE |  |  | ✓ |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| VDB (Yazdanpanah and Moradi, [2022](#bib.bib125)) | CVPRW 2022 | 参数重加权 |
    将“视觉领域桥接”引入 CNN 的批量归一化 (BN) 层 | CE |  |  | ✓ |'
- en: '| AFGR (Sa et al., [2022](#bib.bib85)) | NCA 2022 | Parameter reweighting |
    Reweight the backbone with a residual attention module | CE | ✓ |  |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| AFGR (Sa et al., [2022](#bib.bib85)) | NCA 2022 | 参数重加权 | 用残差注意模块重新加权骨干网络
    | CE | ✓ |  |  |'
- en: '| TPA (Li et al., [2022b](#bib.bib52)) | CVPR 2022 | Parameter reweighting
    | The task-specific weights are learned to adjust model parameters | CE | ✓ |
    ✓ |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| TPA (Li et al., [2022b](#bib.bib52)) | CVPR 2022 | 参数重加权 | 学习任务特定的权重以调整模型参数
    | CE | ✓ | ✓ |  |'
- en: '| ATA (Wang and Deng, [2021](#bib.bib108)) | IJCAI 2021 | Parameter reweighting
    | Insert a plug-and play model-adaptive task augmentation module into backbone
    | CE | ✓ |  | ✓ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| ATA (Wang 和 Deng, [2021](#bib.bib108)) | IJCAI 2021 | 参数重加权 | 将一个即插即用的模型自适应任务增强模块插入骨干网络
    | CE | ✓ |  | ✓ |'
- en: '| AFA (Hu and Ma, [2022](#bib.bib36)) | ECCV 2022 | Parameter reweighting |
    Use an adversarial feature augmentation module to simulate distribution variations
    | CE & Gram-matrix | ✓ |  | ✓ |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| AFA (Hu 和 Ma, [2022](#bib.bib36)) | ECCV 2022 | 参数重加权 | 使用对抗特征增强模块来模拟分布变化
    | CE & Gram-matrix | ✓ |  | ✓ |'
- en: '| Wave-SAN (Fu et al., [2022b](#bib.bib25)) | arXiv 2022 | Parameter reweighting
    | A StyleAug module is proposed to adjust the parameter | CE & SSL & Style | ✓
    |  | ✓ |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Wave-SAN (Fu et al., [2022b](#bib.bib25)) | arXiv 2022 | 参数重加权 | 提出了一个 StyleAug
    模块来调整参数 | CE & SSL & Style | ✓ |  | ✓ |'
- en: 3.2.3\. Parameter Reweighting
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 参数重加权
- en: 'As depicted in Figure [12](#S3.F12 "Figure 12 ‣ 3.2\. Parameter-based Approaches
    ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") (c), the parameter reweighting technique optimizes the model’s performance
    for the target domain by adjusting a limited number of parameters. Various studies
    have explored this approach to address the cross-domain challenge in few-shot
    learning. For instance, (Oh et al., [2022](#bib.bib74)) resets the parameters
    that were learned on the source domain before adapting to the target data. On
    the other hand, (Yazdanpanah and Moradi, [2022](#bib.bib125)) addresses the internal
    mismatch issue in BatchNorm by introducing the ”Visual Domain Bridge” concept.
    Additionally, (Sa et al., [2022](#bib.bib85)) enhances the feature information
    by stacking a residual attention module into the feature encoder based on the
    residual network. Another study, (Li et al., [2022b](#bib.bib52)) trains task-specific
    weights from scratch on a small support set, as opposed to dynamically estimating
    them. Recent works like (Wang and Deng, [2021](#bib.bib108)) and (Hu and Ma, [2022](#bib.bib36))
    propose adversarial methods to address the domain gap in few-shot learning, where (Wang
    and Deng, [2021](#bib.bib108)) considers the worst-case problem around the source
    task distribution and (Hu and Ma, [2022](#bib.bib36)) introduces a plug-and-play
    adversarial feature augmentation (AFA) method. Finally, (Fu et al., [2022b](#bib.bib25))
    adjusts the parameters of a novel Style Augmentation (StyleAug) module to achieve
    better performance in cross-domain few-shot learning.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [12](#S3.F12 "图 12 ‣ 3.2\. 基于参数的方法 ‣ 3\. 方法 ‣ 深度学习在跨领域小样本视觉识别中的应用：综述") (c)
    所示，参数重加权技术通过调整有限数量的参数来优化模型在目标领域的性能。各种研究已探讨这种方法以应对小样本学习中的跨领域挑战。例如，(Oh et al., [2022](#bib.bib74))
    在适应目标数据之前重置了在源领域上学习的参数。另一方面，(Yazdanpanah 和 Moradi, [2022](#bib.bib125)) 通过引入“视觉领域桥”概念来解决
    BatchNorm 中的内部不匹配问题。此外，(Sa et al., [2022](#bib.bib85)) 通过在残差网络的特征编码器中叠加残差注意模块来增强特征信息。另一项研究，(Li
    et al., [2022b](#bib.bib52)) 从头开始在小支持集上训练任务特定权重，而不是动态估计它们。近期的研究如 (Wang 和 Deng,
    [2021](#bib.bib108)) 和 (Hu 和 Ma, [2022](#bib.bib36)) 提出了对抗方法来解决小样本学习中的领域差距，其中
    (Wang 和 Deng, [2021](#bib.bib108)) 考虑了源任务分布的最坏情况问题，(Hu 和 Ma, [2022](#bib.bib36))
    引入了一种即插即用的对抗特征增强 (AFA) 方法。最后，(Fu et al., [2022b](#bib.bib25)) 调整了新型 Style Augmentation
    (StyleAug) 模块的参数，以在跨领域小样本学习中获得更好的性能。
- en: 3.2.4\. Discussion and Summary
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. 讨论与总结
- en: 'The parameter freeze strategy, as discussed in Section [3.2.1](#S3.SS2.SSS1
    "3.2.1\. Parameter Freeze ‣ 3.2\. Parameter-based Approaches ‣ 3\. Approaches
    ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey"), is often
    combined with meta-learning techniques. In the meta-training phase, two pseudo-domains,
    namely the pseudo-seen and pseudo-unseen domains, are used to simulate the cross-domain
    scenario. However, it is important to note that both of these domains are derived
    from the seen domain, resulting in a relatively small domain distance between
    them. As a result, algorithms that employ this strategy may not be effective in
    addressing the distant-domain problem in cross-domain few-shot learning (CDFSL).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 参数冻结策略，如第 [3.2.1](#S3.SS2.SSS1 "3.2.1\. 参数冻结 ‣ 3.2\. 基于参数的方法 ‣ 3\. 方法 ‣ 深度学习在跨领域小样本视觉识别中的应用：综述")
    节所讨论的，通常与元学习技术结合。在元训练阶段，使用两个伪领域，即伪见领域和伪未见领域，来模拟跨领域场景。然而，需要注意的是，这两个领域均来源于已见领域，导致它们之间的领域距离相对较小。因此，采用这种策略的算法可能无法有效解决跨领域小样本学习（CDFSL）中的远域问题。
- en: 'The parameter selection strategy (Section [3.2.2](#S3.SS2.SSS2 "3.2.2\. Parameter
    Selection ‣ 3.2\. Parameter-based Approaches ‣ 3\. Approaches ‣ Deep Learning
    for Cross-Domain Few-Shot Visual Recognition: A Survey")) aims to adapt to the
    target domain by selecting the most suitable set of parameters from a pool of
    options. While this approach can be effective, it has a limited range of parameter
    sets to choose from, potentially limiting its ability to find the optimal set
    for the target domain. Moreover, some implementations of this strategy attempt
    to incorporate various techniques such as semi-supervised learning, domain adaptation,
    and fine-tuning within a single framework, resulting in a cumbersome and complex
    approach, making the framework bulky.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 参数选择策略（第[3.2.2节](#S3.SS2.SSS2 "3.2.2\. 参数选择 ‣ 3.2\. 基于参数的方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习：综述")）旨在通过从一组选项中选择最适合目标领域的参数集来适应目标领域。尽管这种方法可能有效，但其可选的参数集范围有限，可能会限制找到目标领域最佳参数集的能力。此外，该策略的一些实现尝试将半监督学习、领域适应和微调等各种技术纳入单一框架，导致方法繁琐且复杂，使得框架臃肿。
- en: 'The parameter reweighting strategy (Section [3.2.3](#S3.SS2.SSS3 "3.2.3\. Parameter
    Reweighting ‣ 3.2\. Parameter-based Approaches ‣ 3\. Approaches ‣ Deep Learning
    for Cross-Domain Few-Shot Visual Recognition: A Survey")) seeks to enhance the
    model’s generalization capability through minimal parameter adjustments. This
    approach is critical to improving the model’s performance. However, most existing
    reweighting methods utilize simple structures, which often result in limited improvements
    in terms of generalization. Therefore, further research is necessary to explore
    more complex and effective approaches to parameter reweighting in CDFSL.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 参数重标定策略（第[3.2.3节](#S3.SS2.SSS3 "3.2.3\. 参数重标定 ‣ 3.2\. 基于参数的方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习：综述")）旨在通过最小的参数调整来增强模型的泛化能力。这种方法对于提高模型性能至关重要。然而，大多数现有的重标定方法采用简单的结构，这通常导致在泛化能力方面的提升有限。因此，需要进一步研究以探索在CDFSL中更复杂且有效的参数重标定方法。
- en: 3.3\. Feature Post-processing Approaches
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 特征后处理方法
- en: 'In CDFSL, the transferable feature representation is achieved through post-processing
    of the original features, as illustrated in Figure [13](#S3.F13 "Figure 13 ‣ 3.3\.
    Feature Post-processing Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain
    Few-Shot Visual Recognition: A Survey"). The post-processing strategies include
    feature selection, feature fusion, and feature transformation. Feature selection
    involves choosing the features from multiple domains that best fit the target
    domain. Feature fusion combines multiple features to generate a generalized feature
    representation. Lastly, feature transformation adjusts the original features using
    learnable weights. Table [3](#S3.T3 "Table 3 ‣ 3.3.2\. Feature Fusion (Stacking)
    ‣ 3.3\. Feature Post-processing Approaches ‣ 3\. Approaches ‣ Deep Learning for
    Cross-Domain Few-Shot Visual Recognition: A Survey") shows the related works in
    detail.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在CDFSL中，可转移特征表示通过对原始特征进行后处理来实现，如图[13](#S3.F13 "图 13 ‣ 3.3\. 特征后处理方法 ‣ 3\. 方法
    ‣ 跨领域少样本视觉识别的深度学习：综述")所示。后处理策略包括特征选择、特征融合和特征变换。特征选择涉及从多个领域中选择最适合目标领域的特征。特征融合将多个特征结合生成一个通用的特征表示。最后，特征变换使用可学习的权重调整原始特征。表[3](#S3.T3
    "表 3 ‣ 3.3.2\. 特征融合（堆叠） ‣ 3.3\. 特征后处理方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习：综述")详细展示了相关的工作。
- en: '![Refer to caption](img/36817d1b77870e1f0ad4e34824d8717a.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/36817d1b77870e1f0ad4e34824d8717a.png)'
- en: Figure 13\. The feature post-processing categories. (a) represents the feature
    selection, in which the information closest to the shared feature is selected
    for knowledge transfer. (b) means feature fusion. Various features are stacked
    to approximate shared features. The left and right parts in (b) show the source
    of features to be merged. And (c) indicates feature transformation, *i.e*. the
    shared feature is obtained through converting the original feature and the left
    and right parts in (c) means the different convert manners.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13\. 特征后处理类别。(a) 代表特征选择，其中选择最接近共享特征的信息进行知识转移。(b) 表示特征融合。各种特征被堆叠以逼近共享特征。图(b)的左右部分显示了待合并的特征来源。而(c)
    表示特征变换，*即*通过转换原始特征获得共享特征，图(c)的左右部分表示不同的转换方式。
- en: 3.3.1\. Feature Selection
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 特征选择
- en: 'The feature selection strategy involves identifying features closest to the
    target domain for use as the optimal shared feature representation. This approach
    is often employed in conjunction with the introduction of multi-domain instances.
    The strategy first obtains multiple features from different source domains, then
    selects some of them to aid target domain adaptation. As depicted in Figure [13](#S3.F13
    "Figure 13 ‣ 3.3\. Feature Post-processing Approaches ‣ 3\. Approaches ‣ Deep
    Learning for Cross-Domain Few-Shot Visual Recognition: A Survey") (a), (Weng et al.,
    [2021](#bib.bib116)) presents a Representative Multi-Domain Feature Selection
    (RMFS) algorithm to optimize the multi-domain feature extraction and selection
    process. While (Dvornik et al., [2020](#bib.bib18)) extracts a multi-domain representation
    by training a set of feature extractors and then automatically selecting the representations
    most relevant to the target domain.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '特征选择策略涉及识别最接近目标领域的特征，以作为最佳共享特征表示。这种方法通常与多领域实例的引入一起使用。该策略首先从不同的源领域获取多个特征，然后选择其中一些特征以辅助目标领域适应。如图 [13](#S3.F13
    "Figure 13 ‣ 3.3\. Feature Post-processing Approaches ‣ 3\. Approaches ‣ Deep
    Learning for Cross-Domain Few-Shot Visual Recognition: A Survey") (a) 所示，(Weng
    et al., [2021](#bib.bib116)) 提出了一个代表性多领域特征选择 (RMFS) 算法，用于优化多领域特征提取和选择过程。而 (Dvornik
    et al., [2020](#bib.bib18)) 则通过训练一组特征提取器来提取多领域表示，然后自动选择与目标领域最相关的表示。'
- en: 3.3.2\. Feature Fusion (Stacking)
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 特征融合 (堆叠)
- en: 'Feature fusion is an approach used to enhance the generalization ability of
    models. As depicted in Figure [13](#S3.F13 "Figure 13 ‣ 3.3\. Feature Post-processing
    Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") (b), this strategy combines features from different sources or dimensions
    into a single representation to improve FSL performance on the target domain.
    Many works, influenced by (Yosinski et al., [2014](#bib.bib127)), believe that
    features from shallower layers are more transferable than those from deeper layers.
    Hence, (Adler et al., [2020](#bib.bib2)) proposed the CHEF method which unifies
    different abstraction levels of a deep neural network into one representation.
    Additionally, (Zou et al., [2021](#bib.bib141)) combined mid-level features to
    learn the discriminative information of each sample. Similarly, (Du et al., [2021](#bib.bib17))
    used a hierarchical prototype model to combine information from hierarchical memory
    into final prototype features. Unlike the fusion of shallow layer features, in (Hassani,
    [2022](#bib.bib32)), the representation of graphs is obtained by augmenting the
    graphs from sampled tasks into three views: one contextual and two geometric,
    and encoding each view with a dedicated encoder. Finally, the representations
    are aggregated into a single graph representation using an attention mechanism.
    The right part of Figure [13](#S3.F13 "Figure 13 ‣ 3.3\. Feature Post-processing
    Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") (b) shows the features from different network layers are fused, whereas
    the left part shows that features from a set of different networks are stacked.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '特征融合是一种用于增强模型泛化能力的方法。如图 [13](#S3.F13 "Figure 13 ‣ 3.3\. Feature Post-processing
    Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") (b) 所示，这种策略将来自不同来源或维度的特征组合成一个单一表示，以提高目标领域的 FSL 性能。许多研究受到了 (Yosinski
    et al., [2014](#bib.bib127)) 的影响，认为浅层的特征比深层的特征更具可迁移性。因此，(Adler et al., [2020](#bib.bib2))
    提出了 CHEF 方法，该方法将深度神经网络的不同抽象层级统一为一个表示。此外，(Zou et al., [2021](#bib.bib141)) 结合了中层特征以学习每个样本的区分信息。同样，(Du
    et al., [2021](#bib.bib17)) 使用层次化原型模型将层次记忆中的信息合并为最终的原型特征。与浅层特征的融合不同，在 (Hassani,
    [2022](#bib.bib32)) 中，图的表示通过将从采样任务中得到的图增强为三个视图：一个上下文视图和两个几何视图，并用专用编码器对每个视图进行编码。最后，使用注意力机制将表示聚合为一个单一的图表示。图 [13](#S3.F13
    "Figure 13 ‣ 3.3\. Feature Post-processing Approaches ‣ 3\. Approaches ‣ Deep
    Learning for Cross-Domain Few-Shot Visual Recognition: A Survey") (b) 的右侧显示了来自不同网络层的特征被融合，而左侧显示了来自一组不同网络的特征被堆叠。'
- en: Table 3\. Representative feature post-processing CDFSL approaches. ‘GR’ represents
    geometrical regularization.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 代表性特征后处理 CDFSL 方法。‘GR’ 代表几何正则化。
- en: '| Methods | Venue | Strategy | Feature operation | Loss function | FG | Art
    | IW |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 发表场所 | 策略 | 特征操作 | 损失函数 | FG | Art | IW |'
- en: '| RMFS (Weng et al., [2021](#bib.bib116)) | IC-NIDC 2021 | Feature selection
    | extract the multi-domain features and select from them | CE | ✓ | ✓ |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| RMFS (Weng et al., [2021](#bib.bib116)) | IC-NIDC 2021 | 特征选择 | 提取多域特征并从中选择
    | CE | ✓ | ✓ |  |'
- en: '| SUR (Dvornik et al., [2020](#bib.bib18)) | ECCV 2020 | Feature selection
    | Leverage the multi-domain feature bank to autonomously identify the most pertinent
    representations | CE | ✓ | ✓ |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| SUR (Dvornik et al., [2020](#bib.bib18)) | ECCV 2020 | 特征选择 | 利用多域特征库自主识别最相关的表示
    | CE | ✓ | ✓ |  |'
- en: '| CHEF (Adler et al., [2020](#bib.bib2)) | arXiv 2020 | Feature fusion | Accomplish
    the representation fusion through an ensemble of Hebbian learners operating on
    diverse layers of the network | CE |  |  | ✓ |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| CHEF (Adler et al., [2020](#bib.bib2)) | arXiv 2020 | 特征融合 | 通过对网络中不同层次的Hebbian学习器进行集成来完成表示融合
    | CE |  |  | ✓ |'
- en: '| MLP (Zou et al., [2021](#bib.bib141)) | ACM MM 2021 | Feature fusion | Weight
    the fusion of mid-level features and investigate a residual-prediction task |
    CE & $L_{2}$ | ✓ | ✓ |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| MLP (Zou et al., [2021](#bib.bib141)) | ACM MM 2021 | 特征融合 | 加权融合中层特征并研究残差预测任务
    | CE & $L_{2}$ | ✓ | ✓ |  |'
- en: '| HVM (Du et al., [2021](#bib.bib17)) | ICLR 2022 | Feature fusion | The mid-level
    features are weighted and fused in a hierarchical prototype model | CE & KL |  |  |
    ✓ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| HVM (Du et al., [2021](#bib.bib17)) | ICLR 2022 | 特征融合 | 中层特征在层次化原型模型中进行加权融合
    | CE & KL |  |  | ✓ |'
- en: '| TACDFSL (Zhang et al., [2022a](#bib.bib135)) | Symmetry 2022 | Feature transformation
    | Propose the adaptive feature distribution transformation | CE |  |  | ✓ |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| TACDFSL (Zhang et al., [2022a](#bib.bib135)) | Symmetry 2022 | 特征变换 | 提出自适应特征分布变换
    | CE |  |  | ✓ |'
- en: '| MemREIN (Xu et al., [2021](#bib.bib119)) | IJCAI 2022 | Feature transformation
    | Explore an instance normalization algorithm and a memorized module to transform
    the original features | CE & Contrastive | ✓ |  |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| MemREIN (Xu et al., [2021](#bib.bib119)) | IJCAI 2022 | 特征变换 | 探索实例归一化算法和记忆模块以变换原始特征
    | CE & Contrastive | ✓ |  |  |'
- en: '| RDC (Li et al., [2022a](#bib.bib49)) | CVPR 2022 | Feature transformation
    | Transform and reweight the original features through hyperbolic tangent transformation
    | CE & KL | ✓ |  | ✓ |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| RDC (Li et al., [2022a](#bib.bib49)) | CVPR 2022 | 特征变换 | 通过双曲正切变换来变换和重加权原始特征
    | CE & KL | ✓ |  | ✓ |'
- en: '| StyleAdv (Fu et al., [2023](#bib.bib26)) | arXiv 2023 | Feature transformation
    | Introducing variations to the initial style using the signed style gradients
    | CE & KL | ✓ |  | ✓ |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| StyleAdv (Fu et al., [2023](#bib.bib26)) | arXiv 2023 | 特征变换 | 引入对初始风格的变异，使用签名风格梯度
    | CE & KL | ✓ |  | ✓ |'
- en: '| LRP (Sun et al., [2021](#bib.bib93)) | ICPR 2020 | Feature transformation
    | Develop a model-agnostic explanation-guided training strategy that dynamically
    finds and emphasizes the features which are important for the predictions | CE
    | ✓ |  |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| LRP (Sun et al., [2021](#bib.bib93)) | ICPR 2020 | 特征变换 | 开发一个模型无关的解释指导训练策略，动态地寻找并强调对预测重要的特征
    | CE | ✓ |  |  |'
- en: '| BL-ES (Yuan et al., [2021](#bib.bib129)) | ICME 2021 | Feature transformation
    | An inductive graph network (IGN) is optimizaed by MPGN module, in which include
    multiple features | BCE & GR | ✓ |  | ✓ |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| BL-ES (Yuan et al., [2021](#bib.bib129)) | ICME 2021 | 特征变换 | 通过MPGN模块优化一个归纳图网络（IGN），其中包含多个特征
    | BCE & GR | ✓ |  | ✓ |'
- en: '| DeepEMD-SA (Ding and Wang, [2021](#bib.bib16)) | ISCIPT 2021 | Feature transformation
    | Employs an attention module to enable interaction between the local features
    | CE |  |  | ✓ |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| DeepEMD-SA (Ding and Wang, [2021](#bib.bib16)) | ISCIPT 2021 | 特征变换 | 使用注意力模块实现局部特征之间的互动
    | CE |  |  | ✓ |'
- en: '| FUM (Yuan et al., [2022a](#bib.bib128)) | PR 2022 | Feature transformation
    | Using a forget-update module to regulate the features | CE | ✓ | ✓ |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| FUM (Yuan et al., [2022a](#bib.bib128)) | PR 2022 | 特征变换 | 使用遗忘更新模块来调节特征
    | CE | ✓ | ✓ |  |'
- en: '| ConFeSS (Das et al., [2022](#bib.bib15)) | ICLR 2022 | Feature transformation
    | Utilizing a masking module to select relevant information that are more suited
    to target domain in the features | CE & Divergence |  |  | ✓ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| ConFeSS (Das et al., [2022](#bib.bib15)) | ICLR 2022 | 特征变换 | 利用掩码模块选择更适合目标领域的相关信息
    | CE & Divergence |  |  | ✓ |'
- en: '| TCT-GCN (Li et al., [2023](#bib.bib50)) | SSRN 2023 | Feature transformation
    | Combining the multi-levelf feature fusion and feature transform | CE | ✓ |  |
    ✓ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| TCT-GCN (Li et al., [2023](#bib.bib50)) | SSRN 2023 | 特征变换 | 结合多层次特征融合和特征变换
    | CE | ✓ |  | ✓ |'
- en: '| StabPA (Chen et al., [2022a](#bib.bib10)) | ECCV 2022 | Feature transformation
    | Transform features through learning prototypical compact and cross-domain aligned
    representations | Softmax | ✓ | ✓ |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| StabPA (Chen et al., [2022a](#bib.bib10)) | ECCV 2022 | 特征变换 | 通过学习原型紧凑的跨领域对齐表示来转换特征
    | Softmax | ✓ | ✓ |  |'
- en: 3.3.3\. Feature Transformation
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3\. 特征变换
- en: 'The feature transformation strategy reweights features to improve performance,
    as shown in Figure [13](#S3.F13 "Figure 13 ‣ 3.3\. Feature Post-processing Approaches
    ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") (c). Some methods obtain the weights through a transformation and weighting,
    *e.g*. the right part of Figure [13](#S3.F13 "Figure 13 ‣ 3.3\. Feature Post-processing
    Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") (c), while others use a learnable module, *e.g*. the left part of Figure [13](#S3.F13
    "Figure 13 ‣ 3.3\. Feature Post-processing Approaches ‣ 3\. Approaches ‣ Deep
    Learning for Cross-Domain Few-Shot Visual Recognition: A Survey") (c). For the
    former category, in (Zhang et al., [2022a](#bib.bib135)), WDMDS (Wasserstein Distance
    for Measuring Domain Shift) and MMDMDS (Maximum Mean Discrepancy for Measuring
    Domain Shift) were proposed to solve CDFSL. (Xu et al., [2021](#bib.bib119)) introduced
    the MemREIN framework which considers memorization, restitution, and instance
    normalization, *e.g*. an instance normalization algorithm is explored to alleviate
    feature dissimilarity. And (Li et al., [2022a](#bib.bib49)) minimizes task-irrelevant
    features while keeping more transferrable discriminative information by constructing
    a non-linear subspace and using a hyperbolic tangent transformation. Furthermore,
    a novel model-agnostic meta style adversarial training (StyleAdv) method together
    with a novel style adversarial attack method is proposed for CDFSL in (Fu et al.,
    [2023](#bib.bib26)).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '特征变换策略通过重新加权特征以提高性能，如图[13](#S3.F13 "Figure 13 ‣ 3.3\. Feature Post-processing
    Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") (c)所示。一些方法通过变换和加权获得权重，*例如*图[13](#S3.F13 "Figure 13 ‣ 3.3\. Feature
    Post-processing Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot
    Visual Recognition: A Survey") (c)右侧的部分，而其他方法则使用可学习模块，*例如*图[13](#S3.F13 "Figure
    13 ‣ 3.3\. Feature Post-processing Approaches ‣ 3\. Approaches ‣ Deep Learning
    for Cross-Domain Few-Shot Visual Recognition: A Survey") (c)左侧的部分。对于前者类别，在(Zhang
    et al., [2022a](#bib.bib135))中，提出了WDMDS（用于测量领域偏移的Wasserstein距离）和MMDMDS（用于测量领域偏移的最大均值差异）来解决CDFSL。(Xu
    et al., [2021](#bib.bib119))介绍了MemREIN框架，该框架考虑了记忆、恢复和实例归一化，*例如*探讨了一种实例归一化算法以缓解特征不相似性。而(Li
    et al., [2022a](#bib.bib49))通过构建非线性子空间和使用双曲正切变换来最小化任务无关特征，同时保留更多可迁移的判别信息。此外，(Fu
    et al., [2023](#bib.bib26))提出了一种新颖的模型无关的元样式对抗训练（StyleAdv）方法及一种新颖的样式对抗攻击方法用于CDFSL。'
- en: Additionally, there are methods that use a learnable module to determine the
    feature weights. For example, (Sun et al., [2021](#bib.bib93)) computes explanation
    scores for intermediate features and reweights them accordingly. (Yuan et al.,
    [2021](#bib.bib129)) acquires the weights by training a bilevel episode strategy
    (BL-ES) to weight the features. And (Ding and Wang, [2021](#bib.bib16)) employs
    an attention module upon a local-descriptor-based model called DeepEMD to enable
    interaction between the local features. Furthermore, (Yuan et al., [2022a](#bib.bib128))
    reweights features through extracting relationship embeddings using Forget-Update
    Modules (FUM). And recently, (Das et al., [2022](#bib.bib15)) employed a masking
    module to reweight features, selecting those that are more suited to the target
    domain. And a task context ransformer and graph convolutional network (TCT-GCN)
    method is proposed in (Li et al., [2023](#bib.bib50)). Lastly, some methods address
    the CDFSL problem by combining domain adaptation and few-shot learning methods.
    For instance, (Chen et al., [2022a](#bib.bib10)) proposes stabPA to learn compact,
    cross-domain aligned representations.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些方法使用可学习的模块来确定特征权重。例如，(Sun et al., [2021](#bib.bib93)) 计算中间特征的解释分数，并相应地重新加权它们。(Yuan
    et al., [2021](#bib.bib129)) 通过训练双层策略（BL-ES）来获取特征权重。而 (Ding and Wang, [2021](#bib.bib16))
    采用了一个基于局部描述符的模型 DeepEMD 上的注意力模块，以实现局部特征之间的交互。此外，(Yuan et al., [2022a](#bib.bib128))
    通过使用遗忘-更新模块（FUM）提取关系嵌入来重新加权特征。最近，(Das et al., [2022](#bib.bib15)) 使用了一个掩码模块来重新加权特征，选择那些更适合目标领域的特征。此外，(Li
    et al., [2023](#bib.bib50)) 提出了一个任务上下文变换器和图卷积网络（TCT-GCN）方法。最后，一些方法通过结合领域适应和少样本学习方法来解决
    CDFSL 问题。例如，(Chen et al., [2022a](#bib.bib10)) 提出了 stabPA，以学习紧凑的、跨领域对齐的表示。
- en: 3.3.4\. Discussion and Summary
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4\. 讨论与总结
- en: 'Feature selection strategies can be helpful in selecting the most suitable
    features for the target domain in the presence of multi-domain or auxiliary views
    data, as discussed in Section [3.3.1](#S3.SS3.SSS1 "3.3.1\. Feature Selection
    ‣ 3.3\. Feature Post-processing Approaches ‣ 3\. Approaches ‣ Deep Learning for
    Cross-Domain Few-Shot Visual Recognition: A Survey"). However, when there are
    no multiple domains available, features from a single source domain may have limited
    variability, which means selecting different features from the same source domain
    may not significantly improve the performance of FSL on the target domain.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择策略在多领域或辅助视图数据存在的情况下，有助于选择最适合目标领域的特征，如第 [3.3.1](#S3.SS3.SSS1 "3.3.1\. 特征选择
    ‣ 3.3\. 特征后处理方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习：综述") 节所讨论。然而，当没有多个领域可用时，来自单一源领域的特征可能变异性有限，这意味着从同一源领域选择不同的特征可能不会显著改善目标领域
    FSL 的性能。
- en: 'The feature fusion strategy (presented in Section [3.3.2](#S3.SS3.SSS2 "3.3.2\.
    Feature Fusion (Stacking) ‣ 3.3\. Feature Post-processing Approaches ‣ 3\. Approaches
    ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey")) aims
    to obtain features from multiple sources, either from different layers within
    a single network or from multiple networks. However, in the case of the former,
    similarities among the features from the same dataset and network may require
    effective fusion methods, while in the latter, the use of multiple networks can
    increase training costs due to the need for simultaneous training.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 特征融合策略（在第 [3.3.2](#S3.SS3.SSS2 "3.3.2\. 特征融合（堆叠） ‣ 3.3\. 特征后处理方法 ‣ 3\. 方法 ‣
    跨领域少样本视觉识别的深度学习：综述") 节中介绍）旨在从多个来源获得特征，这些来源可以是同一网络中的不同层，或者是多个网络。然而，在前者的情况下，同一数据集和网络中的特征之间的相似性可能需要有效的融合方法，而在后者的情况下，使用多个网络可能会增加训练成本，因为需要同时训练。
- en: 'Feature transformation (introduced in Section [3.3.3](#S3.SS3.SSS3 "3.3.3\.
    Feature Transformation ‣ 3.3\. Feature Post-processing Approaches ‣ 3\. Approaches
    ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey")) is a
    common approach when extra network and multi-domain data are not available. It
    involves reweighting features by assigning new parameters to them, either through
    simple transformation and weighting, or through a learnable module. However, this
    strategy only allows limited exploration of shared information as it only reweights
    the features output from the final layer.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 特征变换（在第[3.3.3节](#S3.SS3.SSS3 "3.3.3\. 特征变换 ‣ 3.3\. 特征后处理方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习综述")介绍）是一种常见的方法，当额外的网络和多领域数据不可用时使用。它涉及通过分配新参数对特征进行重新加权，可以通过简单的变换和加权，或通过可学习的模块实现。然而，这种策略仅允许有限的共享信息探索，因为它只对最终层输出的特征进行加权。
- en: 3.4\. Hybrid Approaches
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 混合方法
- en: 'Hybrid approaches in CDFSL incorporate the above menthined atrategies, the
    related technologies is listed in Table [4](#S3.T4 "Table 4 ‣ 3.4\. Hybrid Approaches
    ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey"). Combinations of instance-guided and parameter-based strategies are
    prevalent in CDFSL. For example, a parameter-efficient multi-mode modulator is
    proposed in (Liu et al., [2021](#bib.bib61)). First, the modulator is designed
    to maintain multiple modulation parameters (one for each domain) in a single network,
    thus achieving single-network multi-domain representation. Second, it divides
    the modulation parameters into the domain-specific and the domain-cooperative
    sets to explore the intra-domain information and inter-domain correlations, respectively.
    Furthermore, (Zhuo et al., [2022](#bib.bib140)) explores a novel target guided
    dynamic mixup (TGDM) framework to generate the intermediate domain images to help
    the FSL task learning on the traget domain. In addition, (Peng et al., [2020](#bib.bib77))
    learns the meta-learners by utilizing multiple domains, and the meta-learners
    are combined in the parameter space to be the Initialized parameters of a network
    used in the target domain. Besides, researchers explore the combination of feature
    post-process and and parameter-based strategies in CDFSL. (Rao et al., [2023](#bib.bib83))
    conducts style transfer-based task augmentation with feature fusion tasks from
    different tasks and styles and feature modulation module (FM). And in (Wang et al.,
    [2022a](#bib.bib109)), a feature extractor stacking (FES) is proposed to combine
    information from a backbones collection.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: CDFSL 中的混合方法结合了上述提到的策略，相关技术列在表[4](#S3.T4 "表 4 ‣ 3.4\. 混合方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习综述")中。实例引导和基于参数的策略在
    CDFSL 中很常见。例如，在(Liu et al., [2021](#bib.bib61))中提出了一种参数高效的多模式调制器。首先，调制器设计为在单个网络中保持多个调制参数（每个领域一个），从而实现单网络多领域表示。其次，它将调制参数分为领域特定集和领域协作集，以分别探索领域内信息和领域间关联。此外，(Zhuo
    et al., [2022](#bib.bib140))探索了一种新型的目标引导动态混合（TGDM）框架，以生成中间领域图像，帮助目标领域的 FSL 任务学习。此外，(Peng
    et al., [2020](#bib.bib77))通过利用多个领域来学习元学习器，这些元学习器在参数空间中结合，以作为目标领域中使用的网络的初始化参数。此外，研究人员还在
    CDFSL 中探索了特征后处理和基于参数策略的组合。(Rao et al., [2023](#bib.bib83))通过来自不同任务和风格的特征融合任务和特征调制模块（FM）进行基于风格转移的任务增强。在(Wang
    et al., [2022a](#bib.bib109))中，提出了一种特征提取器堆叠（FES）方法，以结合来自多个骨干网络的信息。
- en: Table 4\. Representative hybrid CDFSL approaches. ‘FCS’ represents ‘Frontiers
    of Computer Science’.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 代表性的混合 CDFSL 方法。“FCS”代表“计算机科学前沿”。
- en: '| Methods | Venue | Instance-guided | Feature post-process | Parameter-based
    | Loss function | FG | Art | IW |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 场地 | 实例引导 | 特征后处理 | 基于参数 | 损失函数 | FG | 艺术 | IW |'
- en: '| CosML (Peng et al., [2020](#bib.bib77)) | arXiv 2020 | Multiple domains |
    Feature fusion | ✗ | CE | ✓ |  |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| CosML (Peng et al., [2020](#bib.bib77)) | arXiv 2020 | 多领域 | 特征融合 | ✗ | CE
    | ✓ |  |  |'
- en: '| URL (Li et al., [2021a](#bib.bib51)) | ICCV 2021 | Multiple domains | ✗ |
    Parameter reweight | CE & CKA & KL | ✓ | ✓ |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| URL (Li et al., [2021a](#bib.bib51)) | ICCV 2021 | 多领域 | ✗ | 参数重新加权 | CE
    & CKA & KL | ✓ | ✓ |  |'
- en: '| Meta-FDMixup (Fu et al., [2021](#bib.bib23)) | ACM MM 2021 | Labeled target
    | Feature transformation | ✗ | CE & KL | ✓ |  |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Meta-FDMixup (Fu et al., [2021](#bib.bib23)) | ACM MM 2021 | 标记目标 | 特征变换
    | ✗ | CE & KL | ✓ |  |  |'
- en: '| Tri-M (Liu et al., [2021](#bib.bib61)) | ICCV 2021 | Multiple domains | ✗
    | Parameter reweight | CE | ✓ | ✓ |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Tri-M (Liu et al., [2021](#bib.bib61)) | ICCV 2021 | 多领域 | ✗ | 参数重新加权 | CE
    | ✓ | ✓ |  |'
- en: '| ME-D2N (Fu et al., [2022a](#bib.bib24)) | ACM MM 2022 | Labeled target |
    Feature transformation | ✗ | CE & KL | ✓ |  |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| ME-D2N (Fu et al., [2022a](#bib.bib24)) | ACM MM 2022 | 标记目标 | 特征转换 | ✗ |
    CE & KL | ✓ |  |  |'
- en: '| TL-SS (Yuan et al., [2022b](#bib.bib130)) | AAAI 2022 | Original data | ✗
    | Parameter reweight | CE & Metric | ✓ |  | ✓ |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| TL-SS (Yuan et al., [2022b](#bib.bib130)) | AAAI 2022 | 原始数据 | ✗ | 参数重加权
    | CE & 量度 | ✓ |  | ✓ |'
- en: '| TGDM (Zhuo et al., [2022](#bib.bib140)) | ACM MM 2022 | Labeled target |
    ✗ | Parameter reweight | CE | ✓ |  |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| TGDM (Zhuo et al., [2022](#bib.bib140)) | ACM MM 2022 | 标记目标 | ✗ | 参数重加权
    | CE | ✓ |  |  |'
- en: '| TAML (Rao et al., [2023](#bib.bib83)) | arXiv 2023 | Multiple domains | Future
    fusion | Parameter reweight | CE | ✓ |  | ✓ |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| TAML (Rao et al., [2023](#bib.bib83)) | arXiv 2023 | 多领域 | 未来融合 | 参数重加权 |
    CE | ✓ |  | ✓ |'
- en: '| TKD-Net (Ji et al., [2023](#bib.bib40)) | FCS 2023 | Multiple domains | Future
    fusion | ✗ | CE & KL & $L_{2}$ | ✓ |  |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| TKD-Net (Ji et al., [2023](#bib.bib40)) | FCS 2023 | 多领域 | 未来融合 | ✗ | CE
    & KL & $L_{2}$ | ✓ |  |  |'
- en: 3.4.1\. Hybrid via Loss Function
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1. 混合损失函数
- en: Several works solve CDFSL problem not only combine the above mentioned strategies
    but also with different loss function such as contrastive loss, metric loss, *etc*. (Fu
    et al., [2021](#bib.bib23)) advocates utilizing few labeled target data to guide
    the model learning, and is optimized by CE loss and KL loss. Technically, a novel
    meta-FDMixup network is proposed to extract the disentangled domain-irrelevant
    and domain-specific features with a novel disentangle module and a domain classifier.
    And (Fu et al., [2022a](#bib.bib24)) follows this setup (introduce few labeled
    target domian data) and proposes a Multi-Expert Domain Decompositional Network
    (ME-D2N) to solve CDFSL. The loss function also include CE and KL loss. (Zhang
    et al., [2022b](#bib.bib133)) proposes a Style-aware Episodic Training with Robust
    Contrastive Learning (SET-RCL) to make the learned model can achieve better adapt
    to the test tasks with domain-specific styles. And TL-SS strategy (Yuan et al.,
    [2022b](#bib.bib130)) augments multiple views of tasks and proposes a high-order
    associated encoder (HAE) to generate proper parameters and enables the encoder
    to flexibly to any unseen tasks. The loss function in this work include CE and
    a metric loss. Moreover, (Li et al., [2021a](#bib.bib51)) learns a single set
    of deep universal representations by distilling the knowledge of multiple separately
    trained networks by using multiple domains after co-aligning their features with
    the help of adapters and centered kernel alignment. It is optimized by CKA, CE,
    and KL loss. Furthermore, (Ji et al., [2023](#bib.bib40)) proposes team-knowledge
    distillation networks (TKD-Net) and explores a strategy to help the cooperation
    of multiple teachers.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究不仅结合了上述策略，还使用了不同的损失函数，如对比损失、度量损失，*等等*。 (Fu et al., [2021](#bib.bib23)) 倡导利用少量标记的目标数据来指导模型学习，并通过CE损失和KL损失进行优化。技术上，提出了一种新颖的meta-FDMixup网络，通过新颖的解缠模块和领域分类器提取解缠的领域无关和领域特定特征。而(Fu
    et al., [2022a](#bib.bib24)) 跟随这种设置（引入少量标记的目标领域数据），并提出了一个多专家领域分解网络（ME-D2N）来解决CDFSL。损失函数还包括CE和KL损失。(Zhang
    et al., [2022b](#bib.bib133)) 提出了一个风格感知的情节训练与鲁棒对比学习（SET-RCL），使得学到的模型能够更好地适应具有领域特定风格的测试任务。TL-SS策略
    (Yuan et al., [2022b](#bib.bib130)) 增强了任务的多视角，并提出了一个高阶关联编码器（HAE），以生成适当的参数，并使编码器灵活适应任何未见任务。该工作中的损失函数包括CE和量度损失。此外，(Li
    et al., [2021a](#bib.bib51)) 通过使用适配器和中心化核对齐帮助对齐特征，从多个独立训练的网络中提取单一的深层通用表示。它通过CKA、CE和KL损失进行优化。此外，(Ji
    et al., [2023](#bib.bib40)) 提出了团队知识蒸馏网络（TKD-Net），并探索了一种帮助多个教师合作的策略。
- en: 3.4.2\. Discussion and Summary
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2. 讨论与总结
- en: 'The combination of multiple strategies in CDFSL, as discussed in Section [3.4](#S3.SS4
    "3.4\. Hybrid Approaches ‣ 3\. Approaches ‣ Deep Learning for Cross-Domain Few-Shot
    Visual Recognition: A Survey"), can lead to improved performance. For example,
    the instance-guided strategy is often easily incorporated into various methods,
    and as such, is frequently combined with other approaches. However, there are
    also challenges associated with combining strategies. The combination of feature
    post-processing and parameter-based strategies can be unpredictable and may lead
    to negative transfer, making it a less frequently explored option. To achieve
    optimal results, it is essential to avoid negative transfer and carefully consider
    the combination of strategies in hybrid approaches.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在CDFSL中，正如[3.4](#S3.SS4 "3.4\. 混合方法 ‣ 3\. 方法 ‣ 跨领域少样本视觉识别的深度学习：综述")节中讨论的，结合多种策略可以提高性能。例如，实例引导策略通常容易融入各种方法，因此经常与其他方法结合。然而，结合策略也存在挑战。特征后处理和基于参数的策略的结合可能是不可预测的，并可能导致负迁移，因此它是一个较少被探索的选项。为了实现**最佳**结果，必须避免负迁移，并谨慎考虑混合方法中策略的组合。
- en: 4\. Performance
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 性能
- en: 'This section provides a comprehensive overview of the evaluation process for
    models in the field of Cross-Domain Few-Shot Learning (CDFSL). In order to evaluate
    the effectiveness of these models, we need to examine the appropriate datasets
    and benchmarks used. This is covered in Section [4.1](#S4.SS1 "4.1\. Datasets
    ‣ 4\. Performance ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") and Section [4.2](#S4.SS2 "4.2\. Benchmarks ‣ 4\. Performance ‣ Deep
    Learning for Cross-Domain Few-Shot Visual Recognition: A Survey") respectively.
    In Section [4.3](#S4.SS3 "4.3\. Performance Comparison and Analysis ‣ 4\. Performance
    ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey"), we delve
    deeper into a thorough analysis and comparison of the performance of various method
    categories in the field of CDFSL. This section provides a crucial evaluation of
    the models, highlighting the strengths and weaknesses of different approaches
    to address the challenging problem of CDFSL.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了跨领域少样本学习（CDFSL）模型评估过程的全面概述。为了评估这些模型的有效性，我们需要检查使用的适当数据集和基准。这在[4.1](#S4.SS1
    "4.1\. 数据集 ‣ 4\. 性能 ‣ 跨领域少样本视觉识别的深度学习：综述")和[4.2](#S4.SS2 "4.2\. 基准 ‣ 4\. 性能 ‣
    跨领域少样本视觉识别的深度学习：综述")节中进行了说明。在[4.3](#S4.SS3 "4.3\. 性能比较与分析 ‣ 4\. 性能 ‣ 跨领域少样本视觉识别的深度学习：综述")节中，我们**深入探讨**了CDFSL领域各种方法类别的性能的彻底分析和比较。本节提供了对模型的关键评估，突出了不同方法在解决CDFSL挑战性问题时的优点和缺点。
- en: 4.1\. Datasets
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 数据集
- en: 'The evaluation of CDFSL models is facilitated by the availability of annotated
    datasets. The comparison of various algorithms and architectures is made fair
    through the use of these datasets. The continuous growth in complexity, size,
    annotation number, and transfer difficulty of the datasets represents an ongoing
    challenge that drives the development of innovative and superior techniques. Table [5](#S4.T5
    "Table 5 ‣ 4.1\. Datasets ‣ 4\. Performance ‣ Deep Learning for Cross-Domain Few-Shot
    Visual Recognition: A Survey") presents a list of the most widely used datasets
    for the CDFSL problem, and the following sections provide an in-depth description
    of each:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: CDFSL模型的评估得益于带注释的数据集的可用性。通过使用这些数据集，使各种算法和架构的比较变得公平。数据集在复杂性、大小、注释数量和迁移难度上的持续增长代表了一个持续的挑战，推动了创新和优越技术的发展。[5](#S4.T5
    "表5 ‣ 4.1\. 数据集 ‣ 4\. 性能 ‣ 跨领域少样本视觉识别的深度学习：综述")表列出了CDFSL问题中最广泛使用的数据集，以下各节对每个数据集进行了详细描述：
- en: Table 5\. Details of datasets in CDFSL.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. CDFSL中的数据集详细信息。
- en: '| Datasets | Derived from | Number of images | Image size | Number of categories
    | Content | Fields | Reference |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 来源 | 图像数量 | 图像大小 | 类别数量 | 内容 | 领域 | 参考 |'
- en: '| miniImageNet | ImageNet | 60000 | $84\times 84$ | 100 | objects classification
    | natural scene | (Vinyals et al., [2016](#bib.bib106)) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| miniImageNet | ImageNet | 60000 | $84\times 84$ | 100 | 对象分类 | 自然场景 | (Vinyals
    et al., [2016](#bib.bib106)) |'
- en: '| tieredImageNet | ImageNet | 779165 | $84\times 84$ | 608 | objects classification
    | natural scene | (Ren et al., [2018](#bib.bib84)) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| tieredImageNet | ImageNet | 779165 | $84\times 84$ | 608 | 对象分类 | 自然场景 |
    (Ren et al., [2018](#bib.bib84)) |'
- en: '| Plantae | iNat2017 | 196613 | varying | 2101 | plants & animals classification
    | natural scene | (Van Horn et al., [2018](#bib.bib103)) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Plantae | iNat2017 | 196613 | 各异 | 2101 | 植物和动物分类 | 自然场景 | (Van Horn et al.,
    [2018](#bib.bib103)) |'
- en: '| Places | N/A | 10 million | $200\times 200$ | 400+ | scene classification
    | natural scene | (Zhou et al., [2017](#bib.bib137)) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Places | N/A | 1000万 | $200\times 200$ | 400+ | 场景分类 | 自然场景 | (Zhou et al.,
    [2017](#bib.bib137)) |'
- en: '| Stanford Cars | N/A | 16185 | varying | 196 | cars fine-grained classification
    | natural scene | (Krause et al., [2013](#bib.bib43)) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Stanford Cars | N/A | 16185 | 各异 | 196 | 汽车细粒度分类 | 自然场景 | (Krause et al.,
    [2013](#bib.bib43)) |'
- en: '| CUB | ImageNet | 11788 | $84\times 84$ | 200 | birds fine-grained classification
    | natural scene | (Wah et al., [2011](#bib.bib107)) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| CUB | ImageNet | 11788 | $84\times 84$ | 200 | 鸟类细粒度分类 | 自然场景 | (Wah et al.,
    [2011](#bib.bib107)) |'
- en: '| CropDiseases | N/A | 87000 | $256\times 256$ | 38 | crop leaves classification
    | natural scene | (Mohanty et al., [2016](#bib.bib68)) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| CropDiseases | N/A | 87000 | $256\times 256$ | 38 | 农作物叶片分类 | 自然场景 | (Mohanty
    et al., [2016](#bib.bib68)) |'
- en: '| EuroSAT | Sentinel-2 satellite | 27000 | $64\times 64$ | 10 | land classification
    | remote sensing | (Helber et al., [2019](#bib.bib33)) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| EuroSAT | Sentinel-2 卫星 | 27000 | $64\times 64$ | 10 | 土地分类 | 遥感 | (Helber
    et al., [2019](#bib.bib33)) |'
- en: '| ISIC 2018 | N/A | 11720 | $600\times 450$ | 7 | dermoscopic lesion classification
    | medical | (Tschandl et al., [2018](#bib.bib100)) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| ISIC 2018 | N/A | 11720 | $600\times 450$ | 7 | 皮肤病变分类 | 医学 | (Tschandl et
    al., [2018](#bib.bib100)) |'
- en: '| ChestX | N/A | 100K | $1024\times 1024$ | 15 | lung diseases classification
    | medical | (Wang et al., [2017](#bib.bib114)) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| ChestX | N/A | 100K | $1024\times 1024$ | 15 | 肺部疾病分类 | 医学 | (Wang et al.,
    [2017](#bib.bib114)) |'
- en: '| Omniglot | N/A | 25260 | $28\times 28$ | 1623 | characters classification
    | character | (Lake et al., [2011](#bib.bib44)) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| Omniglot | N/A | 25260 | $28\times 28$ | 1623 | 字符分类 | 字符 | (Lake et al.,
    [2011](#bib.bib44)) |'
- en: '| FGVC-Aircraft | N/A | 10200 | varying | 100 | Aircraft fine-grained classification
    | natural scene | (Maji et al., [2013](#bib.bib65)) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| FGVC-Aircraft | N/A | 10200 | 各异 | 100 | 飞机细粒度分类 | 自然场景 | (Maji et al., [2013](#bib.bib65))
    |'
- en: '| DTD | N/A | 5640 | varying | 47 | textures classification | natural scene
    | (Cimpoi et al., [2014](#bib.bib13)) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| DTD | N/A | 5640 | 各异 | 47 | 纹理分类 | 自然场景 | (Cimpoi et al., [2014](#bib.bib13))
    |'
- en: '| Quick Draw | Quick draw! | 50 million | $128\times 128$ | 345 | drawing images
    classification | Art | (Jongejan et al., [2016](#bib.bib41)) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Quick Draw | Quick draw! | 5000万 | $128\times 128$ | 345 | 手绘图像分类 | 艺术 |
    (Jongejan et al., [2016](#bib.bib41)) |'
- en: '| Fungi | N/A | 100000 | varying | 1394 | fungi fine-grained classification
    | natural scene | (Schroeder and Cui, [2018](#bib.bib86)) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Fungi | N/A | 100000 | 各异 | 1394 | 真菌细粒度分类 | 自然场景 | (Schroeder and Cui, [2018](#bib.bib86))
    |'
- en: '| VGG Flower | N/A | 8189 | varying | 102 | flowers fine-grained classification
    | natural scene | (Nilsback and Zisserman, [2008](#bib.bib73)) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| VGG Flower | N/A | 8189 | 各异 | 102 | 花卉细粒度分类 | 自然场景 | (Nilsback and Zisserman,
    [2008](#bib.bib73)) |'
- en: '| Traffic Signs | N/A | 50000 | varying | 43 | Traffic signs classification
    | natural scene | (Houben et al., [2013](#bib.bib35)) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Traffic Signs | N/A | 50000 | 各异 | 43 | 交通标志分类 | 自然场景 | (Houben et al., [2013](#bib.bib35))
    |'
- en: '| MSCOCO | N/A | 1.5 million | varying | 80 | objects classification | natural
    scene | (Lin et al., [2014](#bib.bib55)) |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| MSCOCO | N/A | 150万 | 各异 | 80 | 对象分类 | 自然场景 | (Lin et al., [2014](#bib.bib55))
    |'
- en: •
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'miniImageNet (Vinyals et al., [2016](#bib.bib106)): miniImageNet dataset consists
    of 60000 images selected from the dataset ImageNet, with a total of 100 categories.
    Each category has 600 images, and the size of each image is $84\times 84$.'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: miniImageNet (Vinyals et al., [2016](#bib.bib106))：miniImageNet 数据集由从 ImageNet
    数据集中选取的 60000 张图片组成，共有 100 个类别。每个类别有 600 张图片，每张图片的大小为 $84\times 84$。
- en: •
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'tieredImageNet (Ren et al., [2018](#bib.bib84)): tieredImageNet dataset is
    selected from the ImageNet dataset, including 34 categories, and each category
    contains 10-30 sub-categories (classes). There are 608 classes and 779165 images
    in this dataset. Each class has multiple samples of varying numbers.'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: tieredImageNet (Ren et al., [2018](#bib.bib84))：tieredImageNet 数据集从 ImageNet
    数据集中选取，包括 34 个类别，每个类别包含 10-30 个子类别（类）。该数据集有 608 个类别和 779165 张图片。每个类别有多个样本，数量各异。
- en: •
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Plantae (Van Horn et al., [2018](#bib.bib103)): Plantae dataset is one of dataset
    iNat2017\. There are 2101 categories and 196613 images in this dataset.'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Plantae (Van Horn et al., [2018](#bib.bib103))：Plantae 数据集是数据集 iNat2017 的一个子集。该数据集包含
    2101 个类别和 196613 张图片。
- en: •
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Places (Zhou et al., [2017](#bib.bib137)): Places dataset contains more than
    10 million images of 400+ unique scene categories. This dataset features 5000
    to 30000 training images in each class, which is consistent with real-world frequency
    of occurrence. The image size in this dataset is $200\times 200$.'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Places （Zhou等，[2017](#bib.bib137)）：Places数据集包含超过1000万张图像，覆盖400多个独特的场景类别。该数据集每个类别的训练图像数量从5000到30000不等，与实际世界中出现的频率一致。数据集中的图像大小为$200\times
    200$。
- en: •
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Stanford Cars (Krause et al., [2013](#bib.bib43)): The Cars dataset is a fine-grained
    classification dataset about cars. It contains 16,185 images of 196 classes of
    cars. The data is split into 8,144 training images and 8,041 testing images.'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Stanford Cars （Krause等，[2013](#bib.bib43)）：Cars数据集是一个关于汽车的细粒度分类数据集。它包含16185张196个类别的汽车图像。数据分为8144张训练图像和8041张测试图像。
- en: •
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CUB (Wah et al., [2011](#bib.bib107)): Images in CUB dataset overlap with images
    in ImageNet. It is a fine-grained classification dataset about birds that contain
    11788 images in 200 categories. The size of images in this dataset is $84\times
    84$.'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CUB （Wah等，[2011](#bib.bib107)）：CUB数据集中的图像与ImageNet中的图像有重叠。它是一个关于鸟类的细粒度分类数据集，包含11788张图像，分为200个类别。该数据集中的图像大小为$84\times
    84$。
- en: •
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CropDiseases (Mohanty et al., [2016](#bib.bib68)): The cropDiseases dataset
    consists of about 87000 RGB images of healthy and diseased crop leaves, categorized
    into 38 different classes. The total dataset is divided into an 80/20 training
    and validation set ratio. The image size in this dataset is $256\times 256$.'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CropDiseases （Mohanty等，[2016](#bib.bib68)）：CropDiseases数据集包含约87000张健康和病变作物叶片的RGB图像，分为38个不同类别。整个数据集分为80/20的训练和验证集比例。数据集中的图像大小为$256\times
    256$。
- en: •
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'EuroSAT (Helber et al., [2019](#bib.bib33)): EuroSAT is a dataset for land
    use and land cover classification. The dataset is based on Sentinel-2 satellite
    images consisting of 10 classes with in total of 27,000 labeled and geo-referenced
    images. Each class includes 2000-3000 images, and the size of these images is
    $64\times 64$.'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: EuroSAT （Helber等，[2019](#bib.bib33)）：EuroSAT是一个用于土地利用和土地覆盖分类的数据集。该数据集基于Sentinel-2卫星图像，包括10个类别，总共有27000张标注和地理参考的图像。每个类别包含2000-3000张图像，图像大小为$64\times
    64$。
- en: •
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ISIC 2018 (Tschandl et al., [2018](#bib.bib100); Codella et al., [2019](#bib.bib14)):
    ISIC 2018 dataset includes 10015 dermoscopic lesion images from 7 categories for
    training, 193 images for evaluate, and 1512 images for testing. The size of each
    image is $600\times 450$.'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ISIC 2018 （Tschandl等，[2018](#bib.bib100)；Codella等，[2019](#bib.bib14)）：ISIC 2018数据集包括10015张来自7个类别的皮肤病变图像用于训练，193张用于评估，1512张用于测试。每张图像的大小为$600\times
    450$。
- en: •
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ChestX (Wang et al., [2017](#bib.bib114)): ChestX-ray14 is currently the largest
    lung X-ray database provided by the NIH Research Institute, which contains 14
    lung diseases, and category 15 indicates no disease was found. The size of images
    in this dataset is $1024\times 1024$.'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ChestX （Wang等，[2017](#bib.bib114)）：ChestX-ray14是目前NIH研究所提供的最大肺部X射线数据库，包含14种肺部疾病，类别15表示未发现疾病。该数据集中的图像大小为$1024\times
    1024$。
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Omniglot (Lake et al., [2011](#bib.bib44)): The Omniglot dataset comprises
    1,623 handwritten characters from 50 languages, each with 20 different handwritings.
    The size of each image in this dataset is $28\times 28$.'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Omniglot （Lake等，[2011](#bib.bib44)）：Omniglot数据集包含1623个来自50种语言的手写字符，每种字符有20种不同的书写方式。数据集中每张图像的大小为$28\times
    28$。
- en: •
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FGVC-Aircraft (Maji et al., [2013](#bib.bib65)): FGVC-Aircraft dataset includes
    10200 aircraft images (102 aircraft models, 100 images per model). The image resolution
    is about 1-2 Mpixels.'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FGVC-Aircraft （Maji等，[2013](#bib.bib65)）：FGVC-Aircraft数据集包括10200张飞机图像（102种飞机模型，每种模型100张图像）。图像分辨率约为1-2百万像素。
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Describable Textures (DTD) (Cimpoi et al., [2014](#bib.bib13)): DTD is a texture
    database consisting of 5640 images, organized according to a list of 47 terms
    (categories) inspired by human perception. There are 120 images for each category.
    Image sizes range between 300x300 and 640x640.'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Describable Textures (DTD) （Cimpoi等，[2014](#bib.bib13)）：DTD是一个纹理数据库，包含5640张图像，按照人类感知启发的47个术语（类别）组织。每个类别有120张图像。图像尺寸范围在300x300到640x640之间。
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Quick Draw (Jongejan et al., [2016](#bib.bib41)): The Quick Draw Dataset is
    a collection of 50 million drawings across 345 categories, contributed by players
    of the game Quick, Draw!'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Quick Draw （Jongejan等，[2016](#bib.bib41)）：Quick Draw数据集是一个包含50百万幅图画的集合，涵盖345个类别，由游戏Quick,
    Draw!的玩家贡献。
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fungi (Schroeder and Cui, [2018](#bib.bib86)): This datasets contains 100000
    fungi images belong to 1394 different categories, which is all fungi classes that
    have been spotted by the general public in Denmark.'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Fungi (施罗德和崔, [2018](#bib.bib86))：该数据集包含 100000 张属于 1394 个不同类别的真菌图像，这些类别是丹麦公众所发现的所有真菌类。
- en: •
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VGG Flower (Nilsback and Zisserman, [2008](#bib.bib73)): VGG Flower dataset
    contains 8189 flower images belong to 102 categories. The flowers chosen to be
    flower commonly occuring in the United Kingdom. Each class consists of between
    40 and 258 images.'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VGG Flower (尼尔斯巴克和齐瑟曼, [2008](#bib.bib73))：VGG Flower 数据集包含 8189 张属于 102 个类别的花卉图像。这些花卉是选择在英国常见的花朵。每个类别包含
    40 到 258 张图像。
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Traffic Signs (Houben et al., [2013](#bib.bib35)): Traffic Signs dataset consists
    of 50,000 images of German road signs in 43 classes.'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Traffic Signs (侯本等, [2013](#bib.bib35))：Traffic Signs 数据集包含 50,000 张属于 43 个类别的德国道路标志图像。
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MSCOCO (Lin et al., [2014](#bib.bib55)): The images in MSCOCO dataset are collected
    from Flickr with 1.5 million object instances belonging to 80 classes labelled
    and localized using bounding boxes.'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MSCOCO (林等, [2014](#bib.bib55))：MSCOCO 数据集中的图像来自 Flickr，包含 150 万个物体实例，属于 80
    个类别，并通过边界框标注和定位。
- en: 4.2\. Benchmarks
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 基准
- en: This section mainly introduces the benchmarks of the CDFSL problem, including
    miniImageNet & CUB (mini-CUB), a standard fine-grained classification benchmark
    (FGCB), BSCD-FSL (Guo et al., [2020](#bib.bib31)). Besides, Meta-Dataset (Triantafillou
    et al., [2019](#bib.bib98)) also is proposed to evaluate the cross-domain problem
    in FSL. Due to the mini-CUB is included in FGCB, we mainly introduce the last
    three benchmarks.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 本节主要介绍了 CDFSL 问题的基准，包括 miniImageNet 和 CUB (mini-CUB)、一个标准的细粒度分类基准 (FGCB)、BSCD-FSL (郭等,
    [2020](#bib.bib31))。此外，Meta-Dataset (特里安塔夫卢等, [2019](#bib.bib98)) 也被提出以评估 FSL
    中的跨领域问题。由于 mini-CUB 已包含在 FGCB 中，我们主要介绍最后三个基准。
- en: FGCB. A conventional benchmark was derived for fine-grain based CDFSL (FG-CDFSL)
    in the early stage of CDFSL development. It contains five datasets including miniImageNet,
    Plantae (Van Horn et al., [2018](#bib.bib103)), Places (Zhou et al., [2017](#bib.bib137)),
    Cars (Krause et al., [2013](#bib.bib43)), and CUB (Wah et al., [2011](#bib.bib107)),
    in which we usually regard miniImageNet as the source domain and other datasets
    as the target domain. All images in this benchmark are natural images. The main
    challenge across domains for this benchmark is transferring the category information
    from coarse to fine.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: FGCB。一个传统的基准是在 CDFSL 早期阶段为细粒度 CDFSL (FG-CDFSL) 导出的。它包含五个数据集，包括 miniImageNet、Plantae (范霍恩等,
    [2018](#bib.bib103))、Places (周等, [2017](#bib.bib137))、Cars (克劳斯等, [2013](#bib.bib43))
    和 CUB (瓦赫等, [2011](#bib.bib107))，其中我们通常将 miniImageNet 视为源领域，其他数据集视为目标领域。该基准中的所有图像均为自然图像。这个基准面临的主要挑战是将类别信息从粗略转移到细粒度。
- en: BSCD-FSL (Guo et al., [2020](#bib.bib31)). As a more challenging benchmark to
    address imaging way based CDFSL (IW-CDFSL) in CDFSL, BSCD-FSL includes five datasets
    consisting of miniImageNet, CropDisease (Mohanty et al., [2016](#bib.bib68)),
    EuroSAT (Helber et al., [2019](#bib.bib33)), ISIC (Tschandl et al., [2018](#bib.bib100);
    Codella et al., [2019](#bib.bib14)), ChestX (Wang et al., [2017](#bib.bib114)).
    CropDisease is a fine-grained dataset of crop leaves and all-natural industrial
    images. EuroSAT, ISIC, and ChestX have different imaging ways with natural images.
    They are satellite images, dermatology images, and radiology images, respectively.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: BSCD-FSL (郭等, [2020](#bib.bib31))。作为一个更具挑战性的基准，BSCD-FSL 解决了基于成像方式的 CDFSL (IW-CDFSL)
    问题。BSCD-FSL 包含五个数据集，包括 miniImageNet、CropDisease (莫汉提等, [2016](#bib.bib68))、EuroSAT (赫尔伯等,
    [2019](#bib.bib33))、ISIC (齐安德尔等, [2018](#bib.bib100); 科德拉等, [2019](#bib.bib14))
    和 ChestX (王等, [2017](#bib.bib114))。CropDisease 是一个细粒度的作物叶片数据集，包含所有自然工业图像。EuroSAT、ISIC
    和 ChestX 使用不同的成像方式。它们分别为卫星图像、皮肤病图像和放射学图像。
- en: Meta-Dataset (Triantafillou et al., [2019](#bib.bib98)). Meta-Dataset is a large-scale,
    diverse benchmark for measuring various image classification models in realistic
    and challenging few-shot contexts such as CDFSL. This dataset consists of 10 publicly
    available natural image datasets, handwritten characters, and graffiti datasets
    . These datasets were chosen because they are free and easy to obtain, span a
    variety of visual concepts (natural and human-made), and vary in how fine-grained
    the class definition is. Hence, this benchmark can address fine-grain based (FG)
    and art-based CDFSL (Art) problem. And this benchmark breaks the requestment that
    source and target data from the same domain in FSL and limitations of N-way K-shot
    form tasks. And it also introduces the class imbalance in the real world, which
    means it changes the number of classes in each task and the size of training set.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: Meta-Dataset (Triantafillou et al., [2019](#bib.bib98))。Meta-Dataset 是一个大规模、多样化的基准测试，用于在现实和具有挑战性的少样本场景中测量各种图像分类模型，如
    CDFSL。该数据集包含 10 个公开可用的自然图像数据集、手写字符和涂鸦数据集。这些数据集之所以被选择，是因为它们免费且易于获取，涵盖了各种视觉概念（自然和人工），并且类别定义的细粒度程度各异。因此，该基准测试可以解决基于细粒度（FG）和基于艺术的
    CDFSL（Art）问题。同时，它打破了 FSL 中源数据和目标数据来自同一领域的要求，以及 N-way K-shot 形式任务的限制。它还引入了现实世界中的类别不平衡，即每个任务中的类别数量和训练集的大小都会发生变化。
- en: In addition to the commonly used benchmarks above, some methods adopt benchmarks
    initially designed for the domain adaptation (DA) problem. The benchmark DomainNet (Peng
    et al., [2019](#bib.bib78)) (designed to solve art-based cross-domain problem)
    is widely used in DA and comprises 6 domains, each with 345 categories of common
    objects. Additionally, the benchmark Office-Home (Venkateswara et al., [2017](#bib.bib105))
    is utilized by some studies for CDFSL, consisting of 4 domains (art, clipart,
    product, and real world) and 65 categories per domain. The benchmark is comprised
    of 15,500 images, with an average of 70 images per class and a maximum of 99 images
    per class.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述常用的基准测试外，一些方法采用了最初为领域适应（DA）问题设计的基准测试。基准测试 DomainNet （Peng et al., [2019](#bib.bib78)）（旨在解决基于艺术的跨领域问题）在
    DA 中被广泛使用，包含 6 个领域，每个领域有 345 个类别的常见对象。此外，基准测试 Office-Home （Venkateswara et al.,
    [2017](#bib.bib105)）被一些研究用于 CDFSL，包含 4 个领域（艺术、剪贴画、产品和现实世界），每个领域有 65 个类别。该基准测试包含
    15,500 张图像，每个类别平均 70 张图像，最多 99 张图像。
- en: 4.3\. Performance Comparison and Analysis
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 性能比较与分析
- en: 'This section sheds light on the comparative performance of CDFSL approaches
    from different categorizations. The standard evaluation metric used in CDFSL is
    prediction accuracy and the evaluations are typically conducted under various
    settings, including 5-way 1-shot, 5-way 5-shot, 5-way 20-shot, and 5-way 50-shot.
    As CDFSL is a subfield of FSL, many classical FSL methods can be applied directly
    to CDFSL problems. The results of these methods are shown in Table [6](#S4.T6
    "Table 6 ‣ 4.3\. Performance Comparison and Analysis ‣ 4\. Performance ‣ Deep
    Learning for Cross-Domain Few-Shot Visual Recognition: A Survey"), where it can
    be observed that, the meta-learning based methods (MatchingNet, ProtoNet, RelationNet,
    MAML) possess slightly lower performance in CDFSL due to the presence of domain
    gaps, they perform comparatively less well than simple fine-tuning transfer learning
    based methods, particularly as the value of K increases.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '本节阐明了不同分类的 CDFSL 方法的比较性能。CDFSL 中使用的标准评估指标是预测准确率，评估通常在各种设置下进行，包括 5-way 1-shot、5-way
    5-shot、5-way 20-shot 和 5-way 50-shot。由于 CDFSL 是 FSL 的一个子领域，许多经典的 FSL 方法可以直接应用于
    CDFSL 问题。这些方法的结果显示在表 [6](#S4.T6 "Table 6 ‣ 4.3\. Performance Comparison and Analysis
    ‣ 4\. Performance ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") 中，可以观察到，由于领域差距的存在，基于元学习的方法（MatchingNet、ProtoNet、RelationNet、MAML）在
    CDFSL 中的性能略低于简单的微调迁移学习方法，尤其是当 $K$ 的值增加时，它们的表现相对较差。'
- en: Table 6\. The CDFSL performance on the classical FSL approaches with ResNet10
    backbone. $K$ is the number of samples from $5$-way $K$-shot.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. 在 ResNet10 主干网络下经典 FSL 方法的 CDFSL 性能。$K$ 是从 $5$-way $K$-shot 中的样本数量。
- en: '| K | Methods | CropDiseases | EuroSAT | ISIC | ChestX | Plantae | Places |
    Cars | CUB |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| K | 方法 | 作物疾病 | EuroSAT | ISIC | 胸部 X 光 | 植物界 | 地点 | 汽车 | CUB |'
- en: '| 1 | Fine-tuning (Guo et al., [2020](#bib.bib31)) | 61.56±0.90 | 49.34±0.85
    | 30.80±0.59 | 21.88±0.38 | 33.53±0.36 | 50.87±0.48 | 29.32±0.34 | 41.98±0.41
    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 微调 (Guo et al., [2020](#bib.bib31)) | 61.56±0.90 | 49.34±0.85 | 30.80±0.59
    | 21.88±0.38 | 33.53±0.36 | 50.87±0.48 | 29.32±0.34 | 41.98±0.41 |'
- en: '| MatchingNet (Vinyals et al., [2016](#bib.bib106)) | 48.47±1.01 | 50.67±0.88
    | 29.46±0.56 | 20.91±0.30 | 32.70 ± 0.60 | 49.86±0.79 | 30.77±0.47 | 35.89±0.51
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| MatchingNet (Vinyals et al., [2016](#bib.bib106)) | 48.47±1.01 | 50.67±0.88
    | 29.46±0.56 | 20.91±0.30 | 32.70 ± 0.60 | 49.86±0.79 | 30.77±0.47 | 35.89±0.51
    |'
- en: '| RelationNet (Sung et al., [2018](#bib.bib94)) | 56.18±0.85 | 56.28±0.82 |
    29.69±0.60 | 21.94±0.42 | 33.17±0.64 | 48.64±0.85 | 29.11±0.60 | 42.44±0.77 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| RelationNet (Sung et al., [2018](#bib.bib94)) | 56.18±0.85 | 56.28±0.82 |
    29.69±0.60 | 21.94±0.42 | 33.17±0.64 | 48.64±0.85 | 29.11±0.60 | 42.44±0.77 |'
- en: '| ProtoNet (Snell et al., [2017](#bib.bib90)) | 51.22±0.50 | 52.93±0.50 | 29.20±0.30
    | 21.57±0.20 | - | - | - | - |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| ProtoNet (Snell et al., [2017](#bib.bib90)) | 51.22±0.50 | 52.93±0.50 | 29.20±0.30
    | 21.57±0.20 | - | - | - | - |'
- en: '| GNN (Garcia and Bruna, [2017](#bib.bib28)) | 64.48±1.08 | 63.69±1.03 | 32.02±0.66
    | 22.00±0.46 | 35.60±0.56 | 53.10±0.80 | 31.79±0.51 | 45.69±0.68 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| GNN (Garcia and Bruna, [2017](#bib.bib28)) | 64.48±1.08 | 63.69±1.03 | 32.02±0.66
    | 22.00±0.46 | 35.60±0.56 | 53.10±0.80 | 31.79±0.51 | 45.69±0.68 |'
- en: '| 5 | Fine-tuning | 90.64±0.54 | 81.76±0.48 | 49.68±0.36 | 26.09±0.96 | 47.40±0.36
    | 66.47±0.41 | 38.91±0.38 | 58.75±0.36 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 微调 | 90.64±0.54 | 81.76±0.48 | 49.68±0.36 | 26.09±0.96 | 47.40±0.36 |
    66.47±0.41 | 38.91±0.38 | 58.75±0.36 |'
- en: '| MatchingNet | 66.39±0.78 | 64.45±0.63 | 36.74±0.53 | 22.40±0.70 | 46.53±0.68
    | 63.16±0.77 | 38.99±0.64 | 51.37±0.77 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| MatchingNet | 66.39±0.78 | 64.45±0.63 | 36.74±0.53 | 22.40±0.70 | 46.53±0.68
    | 63.16±0.77 | 38.99±0.64 | 51.37±0.77 |'
- en: '| MAML | 78.05±0.68 | 71.70±0.72 | 40.13±0.58 | 23.48±0.96 | - | - | - | 47.20±1.10
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| MAML | 78.05±0.68 | 71.70±0.72 | 40.13±0.58 | 23.48±0.96 | - | - | - | 47.20±1.10
    |'
- en: '| RelationNet | 68.99±0.75 | 61.31±0.72 | 39.41±0.58 | 22.96±0.88 | 44.00±0.60
    | 63.32±0.76 | 37.33±0.68 | 57.77±0.69 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| RelationNet | 68.99±0.75 | 61.31±0.72 | 39.41±0.58 | 22.96±0.88 | 44.00±0.60
    | 63.32±0.76 | 37.33±0.68 | 57.77±0.69 |'
- en: '| ProtoNet | 79.72±0.67 | 73.29±0.71 | 39.57±0.57 | 24.05±1.01 | - | - | -
    | 67.00±1.00 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| ProtoNet | 79.72±0.67 | 73.29±0.71 | 39.57±0.57 | 24.05±1.01 | - | - | -
    | 67.00±1.00 |'
- en: '| GNN | 87.96±0.67 | 83.64±0.77 | 43.94±0.67 | 25.27±0.46 | 52.53±0.59 | 70.84±0.65
    | 44.28±0.63 | 62.25±0.65 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| GNN | 87.96±0.67 | 83.64±0.77 | 43.94±0.67 | 25.27±0.46 | 52.53±0.59 | 70.84±0.65
    | 44.28±0.63 | 62.25±0.65 |'
- en: '| 20 | Fine-tuning | 95.91±0.72 | 87.97±0.42 | 61.09±0.44 | 31.01±0.59 | -
    | - | - | - |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 微调 | 95.91±0.72 | 87.97±0.42 | 61.09±0.44 | 31.01±0.59 | - | - | - |
    - |'
- en: '| MatchingNet | 76.38±0.67 | 77.10±0.57 | 45.72±0.53 | 23.61±0.86 | - | - |
    - | - |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| MatchingNet | 76.38±0.67 | 77.10±0.57 | 45.72±0.53 | 23.61±0.86 | - | - |
    - | - |'
- en: '| MAML | 89.75±0.42 | 81.95±0.55 | 52.36±0.57 | 27.53±0.43 | - | - | - | -
    |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| MAML | 89.75±0.42 | 81.95±0.55 | 52.36±0.57 | 27.53±0.43 | - | - | - | -
    |'
- en: '| RelationNet | 80.45±0.64 | 74.43±0.66 | 41.77±0.49 | 26.63±0.92 | - | - |
    - | - |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| RelationNet | 80.45±0.64 | 74.43±0.66 | 41.77±0.49 | 26.63±0.92 | - | - |
    - | - |'
- en: '| ProtoNet | 88.15±0.51 | 82.27±0.57 | 49.50±0.55 | 28.21±1.15 | - | - | -
    | - |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| ProtoNet | 88.15±0.51 | 82.27±0.57 | 49.50±0.55 | 28.21±1.15 | - | - | -
    | - |'
- en: '| 50 | Fine-tuning | 97.48±0.56 | 92.00±0.56 | 67.20±0.59 | 36.79±0.53 | -
    | - | - | - |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 微调 | 97.48±0.56 | 92.00±0.56 | 67.20±0.59 | 36.79±0.53 | - | - | - |
    - |'
- en: '| MatchingNet | 58.53±0.73 | 54.44±0.67 | 54.58±0.65 | 22.12±0.88 | - | - |
    - | - |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| MatchingNet | 58.53±0.73 | 54.44±0.67 | 54.58±0.65 | 22.12±0.88 | - | - |
    - | - |'
- en: '| RelationNet | 85.08±0.53 | 74.91±0.58 | 49.32±0.51 | 28.45±1.20 | - | - |
    - | - |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| RelationNet | 85.08±0.53 | 74.91±0.58 | 49.32±0.51 | 28.45±1.20 | - | - |
    - | - |'
- en: '| ProtoNet | 90.81±0.43 | 80.48±0.57 | 51.99±0.52 | 29.32±1.12 | - | - | -
    | - |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| ProtoNet | 90.81±0.43 | 80.48±0.57 | 51.99±0.52 | 29.32±1.12 | - | - | -
    | - |'
- en: 'Besides, due to the current CDFSL approaches having various implementation
    requirements (specific datasets, various backbone, etc.) and configurations (training
    sets, learning paradigms, modules, etc.), it is impractical to compare all proposed
    CDFSL methods in a unified and fair manner. However, it is still important to
    gather and present the key details of some representative CDFSL methods, including
    their requirements, configurations, and performance highlights. To this end, we
    summarize in Table [7](#S4.T7 "Table 7 ‣ 4.3\. Performance Comparison and Analysis
    ‣ 4\. Performance ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") the performance of selected CDFSL approaches evaluated on the commonly
    used benchmarks, FGCB and BSCD-FSL. The optimal results for 1-shot and 5-shot
    are highlighted in blue and red, respectively. A comparison of the state-of-the-art
    performance of different method categories reveals that increasing the diversity
    of instances to increase the amount of shared knowledge is more effective than
    other methods that aim to mine existing shared knowledge. Hybrid methods perform
    best in FGCB, leveraging the benefits of multiple strategies in the context of
    near-domain transfer. Another promising direction in CDFSL is the integration
    of plug-and-play modules into existing FSL models, such as MatchingNet, RelationNet,
    and GNN, as illustrated in Figure [14](#S4.F14 "Figure 14 ‣ 4.3.1\. Evaluation
    for Instance-guided Approaches ‣ 4.3\. Performance Comparison and Analysis ‣ 4\.
    Performance ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey").
    Recent results indicate that these modules perform best when applied to GNN, highlighting
    GNN’s superior ability to handle CDFSL tasks compared to MatchingNet and RelationNet.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于目前的 CDFSL 方法具有各种实现要求（特定数据集、各种骨干网络等）和配置（训练集、学习范式、模块等），因此在统一和公平的基础上比较所有提出的
    CDFSL 方法是不切实际的。然而，收集和展示一些代表性 CDFSL 方法的关键细节，包括它们的要求、配置和性能亮点，仍然是重要的。为此，我们在表 [7](#S4.T7
    "表 7 ‣ 4.3\. 性能比较与分析 ‣ 4\. 性能 ‣ 跨域少样本视觉识别深度学习：综述")中总结了在常用基准 FGCB 和 BSCD-FSL 上评估的选定
    CDFSL 方法的性能。1-shot 和 5-shot 的最佳结果分别以蓝色和红色突出显示。对不同方法类别的最新性能比较显示，增加实例的多样性以增加共享知识的量比其他旨在挖掘现有共享知识的方法更有效。混合方法在
    FGCB 中表现最佳，利用了近领域转移中的多种策略的优势。CDFSL 中的另一个有前景的方向是将即插即用模块集成到现有的 FSL 模型中，如 MatchingNet、RelationNet
    和 GNN，如图 [14](#S4.F14 "图 14 ‣ 4.3.1\. 实例引导方法的评估 ‣ 4.3\. 性能比较与分析 ‣ 4\. 性能 ‣ 跨域少样本视觉识别深度学习：综述")所示。最近的结果表明，这些模块在应用于
    GNN 时表现最佳，突显了 GNN 在处理 CDFSL 任务方面相对于 MatchingNet 和 RelationNet 的卓越能力。
- en: Table 7\. The CDFSL performance of the proposed methods on BSCD-FSL and FGCB
    benchmarks. $K$ means $5$-way $K$-shot. ‘KBS’ is Knowledge-Based Systems.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7\. 提出的 CDFSL 方法在 BSCD-FSL 和 FGCB 基准上的性能。$K$ 代表 $5$-way $K$-shot。‘KBS’ 是知识基系统。
- en: '| Type | Methods | Venue | Train set | Backbone | $K$ | CropDiseases | EuroSAT
    | ISIC | ChestX | Plantae | Places | Cars | CUB | Highlight |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 方法 | 场地 | 训练集 | 骨干网络 | $K$ | CropDiseases | EuroSAT | ISIC | ChestX
    | Plantae | Places | Cars | CUB | 亮点 |'
- en: '| Instance-guided | NSAE (Liang et al., [2021](#bib.bib54)) | ICCV | miniImageNet
    | ResNet10 | 5 | 93.31±0.42 | 84.33±0.55 | 55.27±0.62 | 27.30±0.42 | 62.15±0.77
    | 73.17±0.72 | 58.30±0.75 | 71.92±0.77 | The latent noise information from the
    source domain is utilized to capture broader variations of the feature distributions.
    |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 实例引导 | NSAE (Liang et al., [2021](#bib.bib54)) | ICCV | miniImageNet | ResNet10
    | 5 | 93.31±0.42 | 84.33±0.55 | 55.27±0.62 | 27.30±0.42 | 62.15±0.77 | 73.17±0.72
    | 58.30±0.75 | 71.92±0.77 | 利用源领域的潜在噪声信息来捕捉特征分布的更广泛变化。 |'
- en: '| 20 | 98.33±0.18 | 92.34±0.35 | 67.28±0.61 | 35.70±0.47 | 77.40±0.65 | 82.50±0.59
    | 82.32±0.50 | 88.09±0.48 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 98.33±0.18 | 92.34±0.35 | 67.28±0.61 | 35.70±0.47 | 77.40±0.65 | 82.50±0.59
    | 82.32±0.50 | 88.09±0.48 |'
- en: '| 50 | 99.29±0.14 | 95.00±0.26 | 72.90±0.55 | 38.52±0.71 | 83.63±0.60 | 85.92±0.56
    | - | 91.00±0.79 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 99.29±0.14 | 95.00±0.26 | 72.90±0.55 | 38.52±0.71 | 83.63±0.60 | 85.92±0.56
    | - | 91.00±0.79 |'
- en: '| CosML (Peng et al., [2020](#bib.bib77)) | arXiv | miniImageNet Cars / Places
    | Conv-4 | 1 | - | - | - | - | 30.93±0.46 | 53.96±0.62 | 47.74±0.59 | 46.89±0.59
    | Exploring multi-domain pre-train schemes to quickly adapt the model to unseen
    domains |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| CosML (Peng et al., [2020](#bib.bib77)) | arXiv | miniImageNet 汽车 / 地点 |
    Conv-4 | 1 | - | - | - | - | 30.93±0.46 | 53.96±0.62 | 47.74±0.59 | 46.89±0.59
    | 探索多领域预训练方案以快速适应未见领域 |'
- en: '| 5 | - | - | - | - | 42.96±0.57 | 88.08±0.46 | 60.17±0.63 | 66.15±0.63 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 5 | - | - | - | - | 42.96±0.57 | 88.08±0.46 | 60.17±0.63 | 66.15±0.63 |'
- en: '| ISSNet (Xu and Liu, [2022](#bib.bib118)) | arXiv | miniImageNet other 7 datasets
    | ResNet10 | 1 | 73.40±0.86 | 64.50±0.88 | 36.06±0.69 | 23.23±0.42 | - | - | -
    | - | Transferring styles across multiple sources to broaden the distribution
    of labeled sources |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| ISSNet (Xu and Liu, [2022](#bib.bib118)) | arXiv | miniImageNet 其他 7 个数据集
    | ResNet10 | 1 | 73.40±0.86 | 64.50±0.88 | 36.06±0.69 | 23.23±0.42 | - | - | -
    | - | 在多个来源之间迁移风格，以扩大标记来源的分布 |'
- en: '| 5 | 94.10±0.41 | 83.64±0.55 | 51.82±0.67 | 28.79±0.48 | - | - | - | - |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 94.10±0.41 | 83.64±0.55 | 51.82±0.67 | 28.79±0.48 | - | - | - | - |'
- en: '| DSL (Hu et al., [2021](#bib.bib38)) | ICLR | miniImageNet target data | ResNet10
    | 1 | - | - | - | - | 41.17±0.80 | 53.16±0.88 | 37.13±0.69 | 50.15±0.80 | Incorporating
    the cross-domain scenario into the training stage by rapidly switching targets
    |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| DSL (Hu et al., [2021](#bib.bib38)) | ICLR | miniImageNet 目标数据 | ResNet10
    | 1 | - | - | - | - | 41.17±0.80 | 53.16±0.88 | 37.13±0.69 | 50.15±0.80 | 在训练阶段通过快速切换目标将跨域场景纳入训练
    |'
- en: '| 5 | - | - | - | - | 62.10±0.75 | 74.10±0.72 | 58.53±0.73 | 73.57±0.65 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 5 | - | - | - | - | 62.10±0.75 | 74.10±0.72 | 58.53±0.73 | 73.57±0.65 |'
- en: '| STARTUP (Phoo and Hariharan, [2020](#bib.bib80)) | ICLR | miniImageNet target
    data | ResNet10 | 1 | 75.93±0.80 | 63.88±0.84 | 32.66±0.60 | 23.09±0.43 | - |
    - | - | - | Self-training a source representation using unlabeled data from the
    target domain |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| STARTUP (Phoo and Hariharan, [2020](#bib.bib80)) | ICLR | miniImageNet 目标数据
    | ResNet10 | 1 | 75.93±0.80 | 63.88±0.84 | 32.66±0.60 | 23.09±0.43 | - | - | -
    | - | 使用目标域中的未标记数据进行自我训练源表示 |'
- en: '| 5 | 93.02±0.45 | 82.29±0.60 | 47.22±0.61 | 26.94±0.44 | - | - | - | - |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 93.02±0.45 | 82.29±0.60 | 47.22±0.61 | 26.94±0.44 | - | - | - | - |'
- en: '| DDA (Islam et al., [2021](#bib.bib39)) | NIPS | miniImageNet target data
    | ResNet10 | 1 | 82.14±0.78 | 73.14±0.84 | 34.66±0.58 | 23.38±0.43 | - | - | -
    | - | Propose a dynamic distillation-based approach to enhance utilize unlabeled
    target data |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| DDA (Islam et al., [2021](#bib.bib39)) | NIPS | miniImageNet 目标数据 | ResNet10
    | 1 | 82.14±0.78 | 73.14±0.84 | 34.66±0.58 | 23.38±0.43 | - | - | - | - | 提出了一种基于动态蒸馏的方法来增强对未标记目标数据的利用
    |'
- en: '| 5 | 95.54±0.38 | 89.07±0.47 | 49.36±0.59 | 28.31±0.46 | - | - | - | - |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 95.54±0.38 | 89.07±0.47 | 49.36±0.59 | 28.31±0.46 | - | - | - | - |'
- en: '| Parameter-based | SB-MTL (Cai et al., [2020](#bib.bib7)) | arXiv | miniImageNet
    | ResNet10 | 5 | 96.01±0.40 | 87.30±0.68 | 53.50±0.79 | 28.08±0.50 | - | - | -
    | - | Leveraging a first-order MAML algorithm to identify optimal initializations
    and employing a score-based GNN for prediction |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| Parameter-based | SB-MTL (Cai et al., [2020](#bib.bib7)) | arXiv | miniImageNet
    | ResNet10 | 5 | 96.01±0.40 | 87.30±0.68 | 53.50±0.79 | 28.08±0.50 | - | - | -
    | - | 利用一阶 MAML 算法确定最佳初始化，并采用基于分数的 GNN 进行预测 |'
- en: '| 20 | 99.61±0.09 | 96.53±0.28 | 70.31±0.72 | 37.70±0.57 | - | - | - | - |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 99.61±0.09 | 96.53±0.28 | 70.31±0.72 | 37.70±0.57 | - | - | - | - |'
- en: '| 50 | 99.85±0.06 | 98.37±0.18 | 78.41±0.66 | 43.04±0.66 | - | - | - | - |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 99.85±0.06 | 98.37±0.18 | 78.41±0.66 | 43.04±0.66 | - | - | - | - |'
- en: '| VDB (Yazdanpanah and Moradi, [2022](#bib.bib125)) | CVPRW | miniImageNet
    | ResNet10 | 1 | 71.98±0.82 | 63.60±0.87 | 35.32±0.65 | 22.99±0.44 | - | - | -
    | - | Propose a source-free approach through the introduction of the ”Visual Domain
    Bridge” concept, aimed at mitigating internal mismatches in BatchNorm during cross-domain
    settings |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| VDB (Yazdanpanah and Moradi, [2022](#bib.bib125)) | CVPRW | miniImageNet
    | ResNet10 | 1 | 71.98±0.82 | 63.60±0.87 | 35.32±0.65 | 22.99±0.44 | - | - | -
    | - | 提出了一种通过引入“视觉领域桥”概念来缓解跨域设置中 BatchNorm 内部不匹配的无源方法 |'
- en: '| 5 | 90.77±0.49 | 82.06±0.63 | 48.72±0.65 | 26.62±0.45 |  | - | - | - |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 90.77±0.49 | 82.06±0.63 | 48.72±0.65 | 26.62±0.45 |  | - | - | - |'
- en: '| 20 | 96.36±0.27 | 89.42±0.45 | 59.09±0.59 | 31.87±0.44 | - | - | - | - |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 96.36±0.27 | 89.42±0.45 | 59.09±0.59 | 31.87±0.44 | - | - | - | - |'
- en: '| 50 | 97.89±0.19 | 92.24±0.35 | 64.02±0.58 | 35.55±0.45 | - | - | - | - |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 97.89±0.19 | 92.24±0.35 | 64.02±0.58 | 35.55±0.45 | - | - | - | - |'
- en: '| ResNet18 | 1 | 75.46±0.76 | 67.76±0.83 | 33.22±0.58 | 22.28±0.41 | - | -
    | - | - |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| ResNet18 | 1 | 75.46±0.76 | 67.76±0.83 | 33.22±0.58 | 22.28±0.41 | - | -
    | - | - |'
- en: '| 5 | 93.11±0.42 | 85.29±0.52 | 47.48±0.61 | 25.25±0.42 | - | - | - | - |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 93.11±0.42 | 85.29±0.52 | 47.48±0.61 | 25.25±0.42 | - | - | - | - |'
- en: '| 20 | 97.61±0.21 | 91.93±0.37 | 58.89±0.59 | 29.49±0.42 | - | - | - | - |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 97.61±0.21 | 91.93±0.37 | 58.89±0.59 | 29.49±0.42 | - | - | - | - |'
- en: '| 50 | 98.40±0.16 | 93.95±0.30 | 64.23±0.58 | 32.37±0.47 | - | - | - | - |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 98.40±0.16 | 93.95±0.30 | 64.23±0.58 | 32.37±0.47 | - | - | - | - |'
- en: '| FGNN (Chen et al., [2022b](#bib.bib12)) | KBS | miniImageNet | ResNet10 |
    1 | - | - | - | - | 41.44±0.69 | 56.74±0.82 | 34.37±0.60 | 52.97±0.75 | Investigating
    instance normalization and the restitution module to enhance performance |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| FGNN (Chen et al., [2022b](#bib.bib12)) | KBS | miniImageNet | ResNet10 |
    1 | - | - | - | - | 41.44±0.69 | 56.74±0.82 | 34.37±0.60 | 52.97±0.75 | 研究实例归一化和复原模块以提升性能
    |'
- en: '| 5 | - | - | - | - | 60.81±0.66 | 76.12±0.63 | 50.19±0.69 | 71.99±0.64 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 5 | - | - | - | - | 60.81±0.66 | 76.12±0.63 | 50.19±0.69 | 71.99±0.64 |'
- en: '| MAP (Lin et al., [2021](#bib.bib56)) | arXiv | miniImageNet | ResNet10 |
    5 | 90.29±1.56 | 82.76±2.00 | 47.85±1.95 | 24.79±1.22 | 58.45±1.15 | 75.94±0.97
    | 51.64±1.16 | 67.92±1.10 | Selectively performs SOTA adaptation methods in sequence
    with modular adaptation method |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| MAP (林等，[2021](#bib.bib56)) | arXiv | miniImageNet | ResNet10 | 5 | 90.29±1.56
    | 82.76±2.00 | 47.85±1.95 | 24.79±1.22 | 58.45±1.15 | 75.94±0.97 | 51.64±1.16
    | 67.92±1.10 | 选择性地按顺序执行SOTA适应方法与模块化适应方法 |'
- en: '| 20 | 95.22±1.13 | 88.11±1.78 | 60.16±2.70 | 30.21±1.78 | - | - | - | - |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 95.22±1.13 | 88.11±1.78 | 60.16±2.70 | 30.21±1.78 | - | - | - | - |'
- en: '| HVM (Du et al., [2021](#bib.bib17)) | ICLR | miniImageNet | ResNet10 | 5
    | 87.65±0.35 | 74.88±0.45 | 42.05±0.34 | 27.15±0.45 | - | - | - | - | Introducing
    a hierarchical prototype model and a hierarchical alternative to address domain
    gaps by flexibly utilizing features at varying semantic levels |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| HVM (杜等，[2021](#bib.bib17)) | ICLR | miniImageNet | ResNet10 | 5 | 87.65±0.35
    | 74.88±0.45 | 42.05±0.34 | 27.15±0.45 | - | - | - | - | 引入了一个层次化原型模型和一个层次化替代方案，通过灵活利用不同语义级别的特征来解决领域差距
    |'
- en: '| ReFine (Oh et al., [2022](#bib.bib74)) | ICMLW | miniImageNet | ResNet10
    | 1 | 68.93±0.84 | 64.14±0.82 | 35.30±0.59 | 22.48±0.41 | - | - | - | - | Randomizing
    the fitted parameters from the source domain before adapting to target data |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| ReFine (哦等，[2022](#bib.bib74)) | ICMLW | miniImageNet | ResNet10 | 1 | 68.93±0.84
    | 64.14±0.82 | 35.30±0.59 | 22.48±0.41 | - | - | - | - | 在适应目标数据之前随机化源领域的拟合参数
    |'
- en: '| 5 | 90.75±0.49 | 82.36±0.57 | 51.68±0.63 | 26.76±0.42 | - | - | - | - |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 90.75±0.49 | 82.36±0.57 | 51.68±0.63 | 26.76±0.42 | - | - | - | - |'
- en: '| Feature post-processing | CHEF (Adler et al., [2020](#bib.bib2)) | arXiv
    | miniImageNet | ResNet18 | 5 | 86.87±0.27 | 74.15±0.27 | 41.26±0.34 | 24.72±0.14
    | - | - | - | - | Ensembling representation fusion through Hebbian learners operating
    on different layers of the network |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 特征后处理 | CHEF (阿德勒等，[2020](#bib.bib2)) | arXiv | miniImageNet | ResNet18 |
    5 | 86.87±0.27 | 74.15±0.27 | 41.26±0.34 | 24.72±0.14 | - | - | - | - | 通过在网络的不同层上操作的Hebbian学习者进行表示融合
    |'
- en: '| 20 | 94.78±0.12 | 83.31±0.14 | 54.30±0.34 | 29.71±0.27 | - | - | - | - |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 94.78±0.12 | 83.31±0.14 | 54.30±0.34 | 29.71±0.27 | - | - | - | - |'
- en: '| 50 | 96.77±0.08 | 86.55±0.15 | 60.86±0.18 | 31.25±0.20 | - | - | - | - |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 96.77±0.08 | 86.55±0.15 | 60.86±0.18 | 31.25±0.20 | - | - | - | - |'
- en: '| LRP (Sun et al., [2021](#bib.bib93)) | ICPR | miniImageNet | ResNet10 | 1
    | - | - | - | - | 34.80±0.37 | 50.59±0.46 | 29.65±0.33 | 42.44±0.41 | A training
    strategy guided by explanations is developed to identify important features |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| LRP (孙等，[2021](#bib.bib93)) | ICPR | miniImageNet | ResNet10 | 1 | - | -
    | - | - | 34.80±0.37 | 50.59±0.46 | 29.65±0.33 | 42.44±0.41 | 发展了一种由解释指导的训练策略，以识别重要特征
    |'
- en: '| 5 | - | - | - | - | 48.09±0.35 | 66.90±0.40 | 39.19±0.38 | 59.30±0.40 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 5 | - | - | - | - | 48.09±0.35 | 66.90±0.40 | 39.19±0.38 | 59.30±0.40 |'
- en: '| Confess (Das et al., [2022](#bib.bib15)) | ICLR | miniImageNet | ResNet10
    | 5 | 88.88±0.51 | 84.65±0.38 | 48.85±0.29 | 27.09±0.24 | - | - | - | - | Investigating
    a contrastive learning and feature selection system to address domain gaps between
    base and novel categories |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| Confess (达斯等，[2022](#bib.bib15)) | ICLR | miniImageNet | ResNet10 | 5 | 88.88±0.51
    | 84.65±0.38 | 48.85±0.29 | 27.09±0.24 | - | - | - | - | 研究了一个对比学习和特征选择系统，以解决基础类别和新类别之间的领域差距
    |'
- en: '| 20 | 95.34±0.48 | 90.40±0.24 | 60.10±0.33 | 33.57±0.31 | - | - | - | - |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 95.34±0.48 | 90.40±0.24 | 60.10±0.33 | 33.57±0.31 | - | - | - | - |'
- en: '| 50 | 97.56±0.43 | 92.66±0.36 | 65.34±0.45 | 39.02±0.12 | - | - | - | - |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 97.56±0.43 | 92.66±0.36 | 65.34±0.45 | 39.02±0.12 | - | - | - | - |'
- en: '| BL-ES (Yuan et al., [2021](#bib.bib129)) | ICME | miniImageNet | ResNet18
    | 5 | - | 79.78±0.83 | - | - | - | - | 50.07±0.84 | 69.63±0.88 | Proposing a bilevel
    episode strategy to train an inductive graph network of learning comparison and
    induction simultaneously |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| BL-ES (袁等，[2021](#bib.bib129)) | ICME | miniImageNet | ResNet18 | 5 | - |
    79.78±0.83 | - | - | - | - | 50.07±0.84 | 69.63±0.88 | 提出了一个双层情节策略，用于同时训练一个归纳图网络以进行学习比较和归纳
    |'
- en: '| TACDFSL (Zhang et al., [2022a](#bib.bib135)) | Symmetry | miniImageNet |
    WideResNet | 5 | 93.42±0.55 | 85.19±0.67 | 45.39±0.67 | 25.32±0.48 | - | - | -
    | - | Introducing the empirical marginal distribution measurement |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| TACDFSL (张等，[2022a](#bib.bib135)) | Symmetry | miniImageNet | WideResNet
    | 5 | 93.42±0.55 | 85.19±0.67 | 45.39±0.67 | 25.32±0.48 | - | - | - | - | 引入了经验边际分布测量
    |'
- en: '| 20 | 95.49±0.39 | 87.87±0.49 | 53.15±0.59 | 29.17±0.52 | - | - | - | - |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 95.49±0.39 | 87.87±0.49 | 53.15±0.59 | 29.17±0.52 | - | - | - | - |'
- en: '| 50 | 95.88±0.35 | 89.07±0.43 | 56.68±0.58 | 31.75±0.51 | - | - | - | - |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 95.88±0.35 | 89.07±0.43 | 56.68±0.58 | 31.75±0.51 | - | - | - | - |'
- en: '| RDC (Li et al., [2022a](#bib.bib49)) | CVPR | miniImageNet | ResNet10 | 1
    | 86.33±0.50 | 71.57±0.50 | 35.84±0.40 | 22.27±0.20 | 44.33±0.60 | 61.50±0.60
    | 39.13±0.50 | 51.20±0.50 | Minimising task-irrelevant features by constructing
    subspace |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| RDC (Li et al., [2022a](#bib.bib49)) | CVPR | miniImageNet | ResNet10 | 1
    | 86.33±0.50 | 71.57±0.50 | 35.84±0.40 | 22.27±0.20 | 44.33±0.60 | 61.50±0.60
    | 39.13±0.50 | 51.20±0.50 | 通过构建子空间来最小化与任务无关的特征 |'
- en: '| 5 | 93.55±0.30 | 84.67±0.30 | 49.06±0.30 | 25.48±0.20 | 60.63±0.40 | 74.65±0.40
    | 53.75±0.50 | 67.77±0.40 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 93.55±0.30 | 84.67±0.30 | 49.06±0.30 | 25.48±0.20 | 60.63±0.40 | 74.65±0.40
    | 53.75±0.50 | 67.77±0.40 |'
- en: '| Hybrid | FDMixup (Fu et al., [2021](#bib.bib23)) | ACM MM | miniImageNet
    | ResNet10 | 1 | 66.23±1.03 | 62.97±1.01 | 32.48±0.64 | 22.26±0.45 | 37.89±0.58
    | 53.57±0.75 | 31.14±0.51 | 46.38±0.68 | Utilizing few labeled target data to
    guide the model learning |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| Hybrid | FDMixup (Fu et al., [2021](#bib.bib23)) | ACM MM | miniImageNet
    | ResNet10 | 1 | 66.23±1.03 | 62.97±1.01 | 32.48±0.64 | 22.26±0.45 | 37.89±0.58
    | 53.57±0.75 | 31.14±0.51 | 46.38±0.68 | 利用少量标记的目标数据来指导模型学习 |'
- en: '| 5 | 87.27±0.69 | 80.48±0.79 | 44.28±0.66 | 24.52±0.44 | 54.62±0.66 | 73.42±0.65
    | 41.30±0.58 | 64.71±0.68 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 87.27±0.69 | 80.48±0.79 | 44.28±0.66 | 24.52±0.44 | 54.62±0.66 | 73.42±0.65
    | 41.30±0.58 | 64.71±0.68 |'
- en: '| TL-SS (Yuan et al., [2022b](#bib.bib130)) | AAAI | miniImageNet | ResNet10
    | 1 | - | 65.73 | - | - | - | 55.83 | 33.22 | 45.92 | Introducing a domain-irrelevant
    self-supervised learning method |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| TL-SS (Yuan et al., [2022b](#bib.bib130)) | AAAI | miniImageNet | ResNet10
    | 1 | - | 65.73 | - | - | - | 55.83 | 33.22 | 45.92 | 介绍了一种与领域无关的自监督学习方法 |'
- en: '| 5 | - | 79.36 | - | - | - | 76.33 | 49.82 | 69.16 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 5 | - | 79.36 | - | - | - | 76.33 | 49.82 | 69.16 |'
- en: '| TGDM (Zhuo et al., [2022](#bib.bib140)) | ACM MM | miniImageNet | ResNet10
    | 1 | - | - | - | - | 52.39±0.25 | 61.88±0.26 | 50.70±0.24 | 64.80±0.26 | A method
    generates an intermediate domain generation to facilitate the FSL task |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| TGDM (Zhuo et al., [2022](#bib.bib140)) | ACM MM | miniImageNet | ResNet10
    | 1 | - | - | - | - | 52.39±0.25 | 61.88±0.26 | 50.70±0.24 | 64.80±0.26 | 一种生成中间领域以促进
    FSL 任务的方法 |'
- en: '| 5 | - | - | - | - | 71.78±0.22 | 81.62±0.19 | 70.99±0.21 | 84.21±0.18 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 5 | - | - | - | - | 71.78±0.22 | 81.62±0.19 | 70.99±0.21 | 84.21±0.18 |'
- en: '| ME-D2N (Fu et al., [2022a](#bib.bib24)) | ACM MM | miniImageNet | ResNet10
    | 1 | - | - | - | - | 52.89±0.83 | 60.36±0.86 | 49.53±0.79 | 65.05±0.83 | AME-D2N
    utilizes a multi-expert learning approach to create a model |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| ME-D2N (Fu et al., [2022a](#bib.bib24)) | ACM MM | miniImageNet | ResNet10
    | 1 | - | - | - | - | 52.89±0.83 | 60.36±0.86 | 49.53±0.79 | 65.05±0.83 | AME-D2N
    利用多专家学习方法来创建模型 |'
- en: '| 5 | - | - | - | - | 72.87±0.67 | 80.45±0.62 | 69.17±0.68 | 83.17±0.56 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 5 | - | - | - | - | 72.87±0.67 | 80.45±0.62 | 69.17±0.68 | 83.17±0.56 |'
- en: 4.3.1\. Evaluation for Instance-guided Approaches
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1. 实例引导方法的评估
- en: 'Table [7](#S4.T7 "Table 7 ‣ 4.3\. Performance Comparison and Analysis ‣ 4\.
    Performance ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey")
    highlights a noticeable trend in which the performance decreases as the distance
    between the target domain and source domain increases. For instance, the results
    show a drop from 93.31% on CropDiseases to 27.30% on ChestX (5-way 5-shot). A
    comparison between (Peng et al., [2020](#bib.bib77)) and (Hu et al., [2021](#bib.bib38))
    also reveals that the former outperforms the latter on places (88.08%) and cars
    (60.17%) but underperforms on the other two datasets (42.96% and 66.15% vs. 62.10%
    and 73.57%). This discrepancy can be attributed to the difference in training
    data, as the former incorporates places and cars into the training process leading
    to overfitting on these two datasets. On the other hand, the results of (Phoo
    and Hariharan, [2020](#bib.bib80)) and (Islam et al., [2021](#bib.bib39)) on BSCD-FSL
    demonstrate that incorporating target domain data into the training process can
    improve the performance on the target domain. However, this approach works better
    for near-domain transfer than for distance-domain transfer. For example, (Islam
    et al., [2021](#bib.bib39)) showed a 4.90% improvement on CropDiseases and 7.31%
    improvement on EuroSAT but a 0.32% drop on ISIC and only a 2.22% improvement on
    ChestX when compared to the classic fine-tuning method.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 表[7](#S4.T7 "表7 ‣ 4.3\. 性能比较与分析 ‣ 4\. 性能 ‣ 跨领域少样本视觉识别：综述")突出了一个显著趋势，即随着目标领域和源领域之间距离的增加，性能下降。例如，结果显示在CropDiseases上的成绩从93.31%降至ChestX上的27.30%（5-way
    5-shot）。对比（Peng et al., [2020](#bib.bib77)）和（Hu et al., [2021](#bib.bib38)）也表明前者在地点（88.08%）和车辆（60.17%）上表现优于后者，但在其他两个数据集上表现不如后者（42.96%和66.15%
    vs. 62.10%和73.57%）。这种差异可以归因于训练数据的不同，因为前者将地点和车辆纳入训练过程，导致在这两个数据集上的过拟合。另一方面，（Phoo和Hariharan,
    [2020](#bib.bib80)）和（Islam et al., [2021](#bib.bib39)）在BSCD-FSL上的结果表明，将目标领域数据纳入训练过程可以提高目标领域的性能。然而，这种方法在近领域迁移中效果更好，而在远领域迁移中效果较差。例如，（Islam
    et al., [2021](#bib.bib39)）在CropDiseases上提高了4.90%，在EuroSAT上提高了7.31%，但在ISIC上下降了0.32%，在ChestX上仅提高了2.22%，与经典的微调方法相比。
- en: Instance-guided approaches for CDFSL are relatively simple in concept as they
    rely on adding supplementary information to enhance the model’s generalization.
    However, their effectiveness is highly dependent on the choice of information
    used in the training process. If the additional domains included in training greatly
    diverge from the target domain or the selected target domain samples are not representative,
    this can negatively affect CDFSL performance.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 针对CDFSL的实例引导方法概念相对简单，因为它们依赖于添加补充信息以增强模型的泛化能力。然而，其有效性高度依赖于训练过程中使用的信息选择。如果训练中包含的附加领域与目标领域大相径庭，或者选择的目标领域样本不具代表性，这可能会对CDFSL性能产生负面影响。
- en: '![Refer to caption](img/033e5ba97312e2b5883f3447c6e8940b.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/033e5ba97312e2b5883f3447c6e8940b.png)'
- en: Figure 14\. 1-shot (entity area) and 5-shot (slashed area) performance comparison
    of methods that propose a novel module. All methods use ResNet10 as the backbone.
    “CropD” is dataset “CropDiseases”.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图14\. 1-shot（实体区域）和5-shot（虚线区域）方法的性能比较，这些方法提出了一个新颖的模块。所有方法都使用ResNet10作为骨干网络。“CropD”是数据集“CropDiseases”。
- en: 4.3.2\. Evaluation for Parameter-based Approaches
  id: totrans-381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 基于参数的方法评估
- en: 'From the data presented in Table [7](#S4.T7 "Table 7 ‣ 4.3\. Performance Comparison
    and Analysis ‣ 4\. Performance ‣ Deep Learning for Cross-Domain Few-Shot Visual
    Recognition: A Survey"), it appears that the performance of parameter-based methods
    is generally subpar in comparison to the other two method types. Using ResNet10
    as the backbone, the results of (Cai et al., [2020](#bib.bib7)) on BSCD-FSL (5-way
    5-shot) demonstrate this trend, with scores of 96.01% (CropDiseases), 87.30% (EuroSAT),
    53.50% (ISIC), and 28.08% (ChestX). The results of other methods within this category
    are even lower. When comparing the use of ResNet10 (90.77% of CropDiseases, 82.06%
    of EuroSAT, 48.72% of ISIC, 26.62% of ChestX) and ResNet18 (93.11% of CropDiseases,
    85.29% of EuroSAT, 47.48% of ISIC, 25.25% of ChestX) as the backbone for  (Yazdanpanah
    and Moradi, [2022](#bib.bib125)) on BSCD-FSL, it is observed that while increasing
    the depth of the network enhances performance on near-domain datasets (CropDiseases,
    EuroSAT), it deteriorates performance on distant-domain datasets (ISIC, ChestX).
    As such, the best balance of near-domain and distant-domain performance is achieved
    when using ResNet10 as the backbone.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '从表格 [7](#S4.T7 "Table 7 ‣ 4.3\. Performance Comparison and Analysis ‣ 4\. Performance
    ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey") 中提供的数据来看，相比于其他两种方法类型，基于参数的方法的性能通常较差。以
    ResNet10 为骨干网络，（Cai et al., [2020](#bib.bib7)）在 BSCD-FSL（5-way 5-shot）上的结果展示了这一趋势，得分为
    96.01%（CropDiseases）、87.30%（EuroSAT）、53.50%（ISIC）和 28.08%（ChestX）。这一类别中的其他方法结果甚至更低。当比较以
    ResNet10（CropDiseases 90.77%，EuroSAT 82.06%，ISIC 48.72%，ChestX 26.62%）和 ResNet18（CropDiseases
    93.11%，EuroSAT 85.29%，ISIC 47.48%，ChestX 25.25%）作为骨干网络用于 (Yazdanpanah 和 Moradi,
    [2022](#bib.bib125)) 在 BSCD-FSL 上的表现时，可以观察到，虽然增加网络深度能提高在近域数据集（CropDiseases、EuroSAT）上的性能，但会降低在远域数据集（ISIC、ChestX）上的性能。因此，使用
    ResNet10 作为骨干网络时，近域和远域性能的最佳平衡被实现了。'
- en: 'Our analysis of Table [7](#S4.T7 "Table 7 ‣ 4.3\. Performance Comparison and
    Analysis ‣ 4\. Performance ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey") reveals that the performance of the parameter-based methods tends to
    be subpar in comparison to the first two categories of methods. The reason behind
    this is thought to be the local adjustment of network parameters by these methods
    through the use of a module to fit the new domain. Although this reduction of
    hypothesis space may appear advantageous, it actually limits the adaptation of
    the method to data distribution and hypothesis space due to the limited introduction
    of additional parameters. Therefore, parameter-based methods in CDFSL often face
    limitations in augmenting and mining shared knowledge, which makes it more challenging
    to solve the two-stage empirical risk minimization problem compared to other categories
    of methods. Thus, researchers need to explore new methods and techniques that
    can overcome these limitations and improve the performance of parameter-based
    methods in CDFSL.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对表格 [7](#S4.T7 "Table 7 ‣ 4.3\. Performance Comparison and Analysis ‣ 4\.
    Performance ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey")
    的分析揭示，基于参数的方法的性能往往低于前两类方法。这种情况的原因被认为是这些方法通过使用模块来调整网络参数以适应新领域。尽管这种减少假设空间的做法看似有利，但由于引入额外参数的有限性，实际上限制了方法对数据分布和假设空间的适应。因此，CDFSL
    中的基于参数的方法通常在增强和挖掘共享知识方面面临局限，这使得相比于其他方法类别，更难解决两阶段的经验风险最小化问题。因此，研究人员需要探索能够克服这些限制的新方法和技术，以提升基于参数的方法在
    CDFSL 中的性能。'
- en: 4.3.3\. Evaluation for Feature Post-processing Approaches
  id: totrans-384
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3. 特征后处理方法的评估
- en: 'Despite the challenge of directly comparing the performance of different feature
    post-processing methods due to the utilization of various backbones, it can still
    be noted that the performance of these approaches on distant-domain tasks may
    fall short in comparison to instance-guided methods. This was exemplified by comparing
    two representative approaches: the results of (Li et al., [2022a](#bib.bib49))
    on BSCD-FSL (93.55% for CropDiseases, 84.67% for EuroSAT, 49.06% for ISIC, and
    25.48% for ChestX) were not as good as those of (Liang et al., [2021](#bib.bib54))
    on the same benchmark (93.31% for CropDiseases, 84.33% for EuroSAT, 55.27% for
    ISIC, and 27.30% for ChestX). This trend was also reflected in the results on
    FGCB (62.15%, 73.17%, 58.30%, and 71.92% vs. 60.63%, 74.65%, 53.75%, and 67.77%).
    These observations suggest that while feature post-processing methods can still
    bring some improvement, they may not be as effective as instance-guided approaches
    in addressing the CDFSL problem.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管由于使用了不同的骨干网络，直接比较不同特征后处理方法的性能具有挑战性，但仍可以注意到这些方法在远程领域任务上的表现可能不如实例引导方法。通过比较两个代表性的方法可以说明这一点： (Li
    et al., [2022a](#bib.bib49)) 在BSCD-FSL上的结果（CropDiseases为93.55%、EuroSAT为84.67%、ISIC为49.06%、ChestX为25.48%）不如 (Liang
    et al., [2021](#bib.bib54)) 在同一基准上的结果（CropDiseases为93.31%、EuroSAT为84.33%、ISIC为55.27%、ChestX为27.30%）。这种趋势在FGCB上的结果中也得到了反映（62.15%、73.17%、58.30%和71.92%
    vs. 60.63%、74.65%、53.75%和67.77%）。这些观察结果表明，尽管特征后处理方法仍然可以带来一些改进，但在解决CDFSL问题时，它们可能不如实例引导方法有效。
- en: The comparison of the results of instance-guided and feature post-processing
    methods reveals a difference in their approach to uncovering shared knowledge
    between the source and target domains. Instance-guided methods prioritize the
    introduction of additional information during the training phase, effectively
    creating a more favorable shared feature extraction environment. On the other
    hand, feature post-processing methods aim to maximize the utilization of the limited
    shared knowledge available, which is a more restrictive approach.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 实例引导和特征后处理方法的结果比较揭示了它们在揭示源领域和目标领域之间共享知识的方式上的差异。实例引导方法优先在训练阶段引入额外信息，从而有效创建更有利的共享特征提取环境。另一方面，特征后处理方法旨在最大限度地利用有限的共享知识，这是一种更具限制性的方法。
- en: 4.3.4\. Evaluation for Hybrid Approaches
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4. 混合方法的评估
- en: Currently, there is a limited number of studies that explore hybrid methods
    in the context of CDFSL, however, our analysis of these works reveals that the
    performance of these hybrid methods in FGCB and BSCD-FSL is comparable to that
    of other methods. For instance, a study conducted in (Yuan et al., [2022b](#bib.bib130))
    produced results of 76.33%, 49.82%, and 69.16% on the Places, Cars, and CUB datasets,
    which are similar to the results of (Lin et al., [2021](#bib.bib56)) which produced
    75.94%, 51.64%, and 67.92% on the same datasets. It is important to note that
    combining strategies from different categories of methods carries a degree of
    risk, as there may be negative interactions between the different strategies.
    This highlights the high degree of precision required when matching strategies
    in hybrid methods. Ultimately, the choice of which approach to use depends on
    the specific task and available data, as well as the desired level of generalization
    and flexibility required for the model.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，关于在CDFSL背景下探索混合方法的研究数量有限，但我们对这些工作的分析表明，这些混合方法在FGCB和BSCD-FSL中的表现与其他方法相当。例如，研究 (Yuan
    et al., [2022b](#bib.bib130)) 在Places、Cars和CUB数据集上的结果分别为76.33%、49.82%和69.16%，这些结果与 (Lin
    et al., [2021](#bib.bib56)) 在相同数据集上产生的75.94%、51.64%和67.92%的结果相似。值得注意的是，将不同类别方法的策略进行组合存在一定风险，因为不同策略之间可能存在负面交互。这突显了在混合方法中匹配策略时所需的高精度。最终，使用哪种方法取决于具体任务和可用数据，以及模型所需的泛化和灵活性水平。
- en: 4.3.5\. Evaluation on Meta-Dataset
  id: totrans-389
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.5. Meta-Dataset上的评估
- en: 'The techniques tested on the Meta-Dataset (Triantafillou et al., [2019](#bib.bib98))
    utilize non-episodic training, and the evaluation results are presented in Table [8](#S4.T8
    "Table 8 ‣ 4.3.5\. Evaluation on Meta-Dataset ‣ 4.3\. Performance Comparison and
    Analysis ‣ 4\. Performance ‣ Deep Learning for Cross-Domain Few-Shot Visual Recognition:
    A Survey"). The evaluation is conducted using two setups: single-source-based
    (where the source domain is ImageNet) and multiple-sources-based (where the source
    domains are the first eight datasets). In the single source-based setting, ProtoNet,
    MAML, and Pro-MAML, serve as baselines to compare with the proposed approaches.
    The results reveal that (Dvornik et al., [2020](#bib.bib18)) achieved the best
    results on five target datasets, while (Li et al., [2022b](#bib.bib52)) attained
    the best results on the remaining five target datasets. Additionally, the findings
    indicate that deeper backbone networks, such as ResNet34 in (Li et al., [2022b](#bib.bib52)),
    tend to outperform shallower ones, like ResNet18\. The training results of multiple
    sources-based models illustrate that (Li et al., [2022b](#bib.bib52)) achieved
    the highest performance on all target datasets. This is believed to be due to
    the technique’s effective combination of multiple sources and scientifically designed
    parameter reweighting strategy. A comparison of the two setups of (Li et al.,
    [2022b](#bib.bib52)) using ResNet18 shows that the incorporation of multiple datasets
    results in significant performance improvements on eight seen datasets, but only
    a modest improvement on one unseen dataset. This indicates that introducing multiple
    domains without careful consideration may not necessarily enhance performance
    significantly. In conclusion, the proposed methods significantly enhance CDFSL
    performance relative to traditional FSL techniques, demonstrating the effectiveness
    of these methods in solving the CDFSL problem.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Meta-Dataset 上测试的技术（Triantafillou et al., [2019](#bib.bib98)）使用了非情节训练，评估结果见表[8](#S4.T8
    "表 8 ‣ 4.3.5. 在 Meta-Dataset 上的评估 ‣ 4.3. 性能比较与分析 ‣ 4. 性能 ‣ 跨领域少样本视觉识别的深度学习：综述")。评估采用了两种设置：单源设置（源领域为
    ImageNet）和多源设置（源领域为前八个数据集）。在单源设置中，ProtoNet、MAML 和 Pro-MAML 作为基线与所提出的方法进行比较。结果表明，（Dvornik
    et al., [2020](#bib.bib18)）在五个目标数据集上取得了最佳结果，而（Li et al., [2022b](#bib.bib52)）在其余五个目标数据集上取得了最佳结果。此外，结果还表明，较深的骨干网络，如（Li
    et al., [2022b](#bib.bib52)）中的 ResNet34，往往优于较浅的网络，如 ResNet18。多源设置模型的训练结果表明，（Li
    et al., [2022b](#bib.bib52)）在所有目标数据集上取得了最高的性能。这被认为是由于该技术有效结合了多源和科学设计的参数重加权策略。使用
    ResNet18 对（Li et al., [2022b](#bib.bib52)）的两种设置进行比较，结果表明，多数据集的引入在八个已见数据集上显著提高了性能，但在一个未见数据集上只有适度的提升。这表明，引入多个领域而未加以慎重考虑可能不会显著提高性能。总之，所提出的方法相较于传统
    FSL 技术显著提升了 CDFSL 性能，证明了这些方法在解决 CDFSL 问题上的有效性。
- en: Table 8\. The CDFSL performance of approaches on Meta-Dataset. $\star$ means
    the results on the seen data set (source data set).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 表8. Meta-Dataset 上方法的 CDFSL 性能。 $\star$ 表示在已见数据集（源数据集）上的结果。
- en: '|  | Single source | Multiple sources |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  | 单源 | 多源 |'
- en: '|  | ProtoNet (Snell et al., [2017](#bib.bib90)) | MAML (Finn et al., [2017](#bib.bib21))
    | Pro-MAML (Triantafillou et al., [2019](#bib.bib98)) | SUR (Dvornik et al., [2020](#bib.bib18))
    | TPA (Li et al., [2022b](#bib.bib52)) | tri-M (Liu et al., [2021](#bib.bib61))
    | RMFS (Weng et al., [2021](#bib.bib116)) | TPA (Li et al., [2022b](#bib.bib52))
    | URL (Li et al., [2021a](#bib.bib51)) |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | ProtoNet （Snell et al., [2017](#bib.bib90)） | MAML （Finn et al., [2017](#bib.bib21)）
    | Pro-MAML （Triantafillou et al., [2019](#bib.bib98)） | SUR （Dvornik et al., [2020](#bib.bib18)）
    | TPA （Li et al., [2022b](#bib.bib52)） | tri-M （Liu et al., [2021](#bib.bib61)）
    | RMFS （Weng et al., [2021](#bib.bib116)） | TPA （Li et al., [2022b](#bib.bib52)）
    | URL （Li et al., [2021a](#bib.bib51)） |'
- en: '| Backbone | ResNet18 | ResNet18 | ResNet18 | ResNet18 | ResNet18 | ResNet34
    | ResNet18 | ResNet18 | ResNet18 | ResNet18 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| Backbone | ResNet18 | ResNet18 | ResNet18 | ResNet18 | ResNet18 | ResNet34
    | ResNet18 | ResNet18 | ResNet18 | ResNet18 |'
- en: '| ImageNet | 44.5 $\pm$ 1.1$\star$ | ${32.4\pm 1.0}{\star}$ | ${47.9\pm 1.1}{\star}$
    | ${57.2\pm 1.1}{\star}$ | 59.5 $\pm$ 1.1$\star$ | ${63.7\pm 1.0}{\star}$ | ${58.6\pm
    1.0}{\star}$ | 63.1 $\pm$ 0.8$\star$ | ${59.5\pm 1.0}{\star}$ | ${58.8\pm 1.1}{\star}$
    |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | 44.5 $\pm$ 1.1$\star$ | ${32.4\pm 1.0}{\star}$ | ${47.9\pm 1.1}{\star}$
    | ${57.2\pm 1.1}{\star}$ | 59.5 $\pm$ 1.1$\star$ | ${63.7\pm 1.0}{\star}$ | ${58.6\pm
    1.0}{\star}$ | 63.1 $\pm$ 0.8$\star$ | ${59.5\pm 1.0}{\star}$ | ${58.8\pm 1.1}{\star}$
    |'
- en: '| Omniglot | $79.6\pm 1.1$ | $71.9\pm 1.2$ | $82.9\pm 0.9$ | 93.2 $\pm$ 0.8
    | $78.2\pm 1.2$ | $82.6\pm 1.1$ | $92.0\pm 0.6$ | 97.7 $\pm$ 0.5$\star$ | ${94.9\pm
    0.4}{\star}$ | ${94.5\pm 0.4}{\star}$ |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Omniglot | $79.6\pm 1.1$ | $71.9\pm 1.2$ | $82.9\pm 0.9$ | 93.2 $\pm$ 0.8
    | $78.2\pm 1.2$ | $82.6\pm 1.1$ | $92.0\pm 0.6$ | 97.7 $\pm$ 0.5$\star$ | ${94.9\pm
    0.4}{\star}$ | ${94.5\pm 0.4}{\star}$ |'
- en: '| Aircraft | $71.1\pm 0.9$ | $52.8\pm 0.9$ | $74.2\pm 0.8$ | 90.1 $\pm$ 0.8
    | $72.2\pm 1.0$ | $80.1\pm 1.0$ | $82.8\pm 0.7$ | ${65.1\pm 0.3}{\star}$ | 89.9
    $\pm$ 0.4$\star$ | ${89.4\pm 0.4}{\star}$ |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 飞机 | $71.1\pm 0.9$ | $52.8\pm 0.9$ | $74.2\pm 0.8$ | 90.1 $\pm$ 0.8 | $72.2\pm
    1.0$ | $80.1\pm 1.0$ | $82.8\pm 0.7$ | ${65.1\pm 0.3}{\star}$ | 89.9 $\pm$ 0.4$\star$
    | ${89.4\pm 0.4}{\star}$ |'
- en: '| Birds | $67.0\pm 1.0$ | $47.2\pm 1.1$ | $70.0\pm 1.0$ | 82.3 $\pm$ 0.8 |
    $74.9\pm 0.9$ | 83.4$\pm$0.8 | $75.3\pm 0.8$ | 84.1 $\pm$ 0.6$\star$ | ${81.1\pm
    0.8}{\star}$ | ${80.7\pm 0.8}{\star}$ |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 鸟类 | $67.0\pm 1.0$ | $47.2\pm 1.1$ | $70.0\pm 1.0$ | 82.3 $\pm$ 0.8 | $74.9\pm
    0.9$ | 83.4$\pm$0.8 | $75.3\pm 0.8$ | 84.1 $\pm$ 0.6$\star$ | ${81.1\pm 0.8}{\star}$
    | ${80.7\pm 0.8}{\star}$ |'
- en: '| Textures | $65.2\pm 0.8$ | $56.7\pm 0.7$ | $67.9\pm 0.8$ | $73.5\pm 0.7$
    | 77.3 $\pm$ 0.7 | 79.6$\pm$0.7 | $71.2\pm 0.8$ | ${67.5\pm 0.9}{\star}$ | 77.5
    $\pm$ 0.7$\star$ | ${77.2\pm 0.7}{\star}$ |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 纹理 | $65.2\pm 0.8$ | $56.7\pm 0.7$ | $67.9\pm 0.8$ | $73.5\pm 0.7$ | 77.3
    $\pm$ 0.7 | 79.6$\pm$0.7 | $71.2\pm 0.8$ | ${67.5\pm 0.9}{\star}$ | 77.5 $\pm$
    0.7$\star$ | ${77.2\pm 0.7}{\star}$ |'
- en: '| Quick Draw | $65.9\pm 0.9$ | $50.5\pm 1.2$ | $66.6\pm 0.9$ | 81.9 $\pm$ 1.0
    | $67.6\pm 0.9$ | $71.0\pm 0.8$ | $77.3\pm 0.7$ | 86.2 $\pm$ 0.5$\star$ | ${81.7\pm
    0.6}{\star}$ | ${82.5\pm 0.6}{\star}$ |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 快速绘制 | $65.9\pm 0.9$ | $50.5\pm 1.2$ | $66.6\pm 0.9$ | 81.9 $\pm$ 1.0 | $67.6\pm
    0.9$ | $71.0\pm 0.8$ | $77.3\pm 0.7$ | 86.2 $\pm$ 0.5$\star$ | ${81.7\pm 0.6}{\star}$
    | ${82.5\pm 0.6}{\star}$ |'
- en: '| Fungi | $40.3\pm 1.1$ | $21.0\pm 1.0$ | $42.0\pm 1.1$ | 67.9 $\pm$ 0.9 |
    $44.7\pm 1.0$ | $51.4\pm 1.2$ | $48.5\pm 1.0$ | ${62.5\pm 0.6}{\star}$ | ${66.3\pm
    0.8}{\star}$ | 68.1 $\pm$ 0.9$\star$ |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 真菌 | $40.3\pm 1.1$ | $21.0\pm 1.0$ | $42.0\pm 1.1$ | 67.9 $\pm$ 0.9 | $44.7\pm
    1.0$ | $51.4\pm 1.2$ | $48.5\pm 1.0$ | ${62.5\pm 0.6}{\star}$ | ${66.3\pm 0.8}{\star}$
    | 68.1 $\pm$ 0.9$\star$ |'
- en: '| VGG Flower | $86.9\pm 0.7$ | $70.9\pm 1.0$ | $88.5\pm 1.0$ | $88.4\pm 0.9$
    | 90.9 $\pm$ 0.6 | 94.0 $\pm$ 0.5 | $90.5\pm 0.5$ | ${86.3\pm 0.3}{\star}$ | 92.2
    $\pm$ 0.5$\star$ | ${92.0\pm 0.5}{\star}$ |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| VGG 花卉 | $86.9\pm 0.7$ | $70.9\pm 1.0$ | $88.5\pm 1.0$ | $88.4\pm 0.9$ |
    90.9 $\pm$ 0.6 | 94.0 $\pm$ 0.5 | $90.5\pm 0.5$ | ${86.3\pm 0.3}{\star}$ | 92.2
    $\pm$ 0.5$\star$ | ${92.0\pm 0.5}{\star}$ |'
- en: '| Traffic Sign | $46.5\pm 1.0$ | $34.2\pm 1.3$ | $34.2\pm 1.3$ | $67.4\pm 0.8$
    | 82.5 $\pm$ 0.8 | $81.7\pm 0.9$ | $78.0\pm 0.6$ | $73.7\pm 0.4$ | 82.8 $\pm$
    1.0 | $63.3\pm 1.2$ |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 交通标志 | $46.5\pm 1.0$ | $34.2\pm 1.3$ | $34.2\pm 1.3$ | $67.4\pm 0.8$ | 82.5
    $\pm$ 0.8 | $81.7\pm 0.9$ | $78.0\pm 0.6$ | $73.7\pm 0.4$ | 82.8 $\pm$ 1.0 | $63.3\pm
    1.2$ |'
- en: '| MSCOCO | $39.9\pm 1.1$ | $24.1\pm 1.1$ | $24.1\pm 1.1$ | $51.3\pm 1.0$ |
    59.0 $\pm$ 1.0 | $61.7\pm 0.9$ | $52.8\pm 1.1$ | $56.2\pm 0.7$ | 57.6 $\pm$ 1.0
    | $57.3\pm 1.0$ |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| MSCOCO | $39.9\pm 1.1$ | $24.1\pm 1.1$ | $24.1\pm 1.1$ | $51.3\pm 1.0$ |
    59.0 $\pm$ 1.0 | $61.7\pm 0.9$ | $52.8\pm 1.1$ | $56.2\pm 0.7$ | 57.6 $\pm$ 1.0
    | $57.3\pm 1.0$ |'
- en: '| MNIST | - | - | - | $90.8\pm 0.5$ | $93.9\pm 0.6$ | $94.6\pm 0.5$ | 96.2
    $\pm$ 0.3 | - | 96.7 $\pm$ 0.4 | $94.7\pm 0.4$ |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| MNIST | - | - | - | $90.8\pm 0.5$ | $93.9\pm 0.6$ | $94.6\pm 0.5$ | 96.2
    $\pm$ 0.3 | - | 96.7 $\pm$ 0.4 | $94.7\pm 0.4$ |'
- en: '| CIFAR 10 | - | - | - | $66.6\pm 0.8$ | 82.1 $\pm$ 0.7 | $86.0\pm 0.6$ | $75.4\pm
    0.8$ | - | 82.9 $\pm$ 0.7 | $74.2\pm 0.8$ |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR 10 | - | - | - | $66.6\pm 0.8$ | 82.1 $\pm$ 0.7 | $86.0\pm 0.6$ | $75.4\pm
    0.8$ | - | 82.9 $\pm$ 0.7 | $74.2\pm 0.8$ |'
- en: '| CIFAR 100 | - | - | - | $58.3\pm 1.0$ | 70.7 $\pm$ 0.9 | $78.3\pm 0.8$ |
    $62.0\pm 1.0$ | - | 70.4 $\pm$ 0.9 | $63.6\pm 1.0$ |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR 100 | - | - | - | $58.3\pm 1.0$ | 70.7 $\pm$ 0.9 | $78.3\pm 0.8$ |
    $62.0\pm 1.0$ | - | 70.4 $\pm$ 0.9 | $63.6\pm 1.0$ |'
- en: 5\. Future work
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 未来工作
- en: Despite significant progress in CDFSL, it continues to present unique challenges
    that require attention. As such, we outline several promising research directions
    for the future, which we discuss in terms of problem setups, applications, and
    theories, respectively.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在 CDFSL 方面取得了显著进展，但它仍然面临着独特的挑战，需要我们关注。因此，我们概述了未来几个有前景的研究方向，并分别从问题设置、应用和理论方面进行了讨论。
- en: 5.1\. Problem Setups
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 问题设置
- en: 'Active Learning-based CDFSL. In Section [2.4.3](#S2.SS4.SSS3 "2.4.3\. Unique
    Issue and Challenge ‣ 2.4\. Unique Issue and Challenge ‣ 2\. Background ‣ Deep
    Learning for Cross-Domain Few-Shot Visual Recognition: A Survey"), we discussed
    the challenge of limited shared knowledge between source and target in CDFSL,
    caused by domain gaps and task shifts, which is especially pertinent when the
    source and target domains are vastly different and the target domain data is scarce.
    To address this challenge, it is vital to find ways to expand and fully utilize
    the shared information between the source and target. Active learning (AL), which
    selects the most informative samples for labeling, has gained increasing traction
    in domain adaptation (Su et al., [2020](#bib.bib92); Ma et al., [2021](#bib.bib63))
    and few-shot learning (Boney and Ilin, [2017](#bib.bib6); Müller et al., [2022](#bib.bib71)).
    For example,(Su et al., [2020](#bib.bib92)) enhances the weights of samples with
    significant uncertainty in classification and diversity to boost the recognition
    performance of the target domain. Moreover,(Boney and Ilin, [2017](#bib.bib6))
    combines FSL and AL into FASL, a speedy and iterative platform for training text
    classification models. As AL selects the most informative data, it is well-suited
    for the CDFSL problem, as it can facilitate cross-domain and cross-task learning.
    Therefore, incorporating AL to solve the CDFSL problem is a promising avenue for
    further research.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '基于主动学习的 CDFSL。在第 [2.4.3](#S2.SS4.SSS3 "2.4.3\. Unique Issue and Challenge ‣
    2.4\. Unique Issue and Challenge ‣ 2\. Background ‣ Deep Learning for Cross-Domain
    Few-Shot Visual Recognition: A Survey") 节中，我们讨论了由于领域差距和任务转移导致的源领域和目标领域之间共享知识有限的问题，这在源领域和目标领域差异较大且目标领域数据稀缺时尤为突出。为了解决这一挑战，至关重要的是找到扩展和充分利用源领域和目标领域之间共享信息的方法。主动学习
    (AL)，即选择最有信息量的样本进行标注，已在领域适应 (Su et al., [2020](#bib.bib92); Ma et al., [2021](#bib.bib63))
    和少样本学习 (Boney and Ilin, [2017](#bib.bib6); Müller et al., [2022](#bib.bib71))
    中获得了越来越多的关注。例如，(Su et al., [2020](#bib.bib92)) 提高了在分类和多样性中存在显著不确定性的样本的权重，以提升目标领域的识别性能。此外，(Boney
    and Ilin, [2017](#bib.bib6)) 将 FSL 和 AL 结合成 FASL，一个用于训练文本分类模型的快速迭代平台。由于 AL 选择最具信息量的数据，它非常适合
    CDFSL 问题，因为它可以促进跨领域和跨任务学习。因此，将 AL 纳入解决 CDFSL 问题是一个值得进一步研究的有前途的途径。'
- en: Transductive CDFSL. Transductive inference refers to the prediction of individual
    test samples by observing specific training samples. In cases where training samples
    are limited and test samples are abundant, the category discriminant model generated
    through inductive reasoning often yields suboptimal performance. Transductive
    reasoning, on the other hand, exploits information from unlabeled test samples
    to identify clusters and enhance classification accuracy. Numerous studies have
    successfully applied transductive inference to tackle FSL problems, resulting
    in promising outcomes (Liu et al., [2018](#bib.bib60); Qiao et al., [2019](#bib.bib82);
    Singh and Jamali-Rad, [2022](#bib.bib89)). As a subfield of FSL, utilizing transductive
    inference to improve CDFSL performance is an encouraging avenue to explore.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '**传导式 CDFSL**。传导推理指通过观察特定的训练样本来预测个别测试样本。在训练样本有限而测试样本丰富的情况下，通过归纳推理生成的类别判别模型通常表现不佳。另一方面，传导推理利用未标记测试样本的信息来识别簇并提高分类准确性。许多研究成功应用传导推理解决
    FSL 问题，取得了令人鼓舞的成果 (Liu et al., [2018](#bib.bib60); Qiao et al., [2019](#bib.bib82);
    Singh and Jamali-Rad, [2022](#bib.bib89))。作为 FSL 的一个子领域，利用传导推理来提升 CDFSL 性能是一个值得探索的前景。'
- en: Incremental CDFSL. Current CDFSL methodologies are designed to tackle FSL tasks
    on the target domain but often suffer from catastrophic forgetting, leading to
    a decline in performance on the source domain. However, a good model should retain
    previous knowledge from both domains and tasks. However, an effective model must
    preserve prior knowledge from both domains and tasks. Thus, addressing catastrophic
    forgetting in CDFSL is of significant concern. Recent advancements in incremental
    learning and continuous learning have been adopted in FSL to combat task incremental
    issues (Tao et al., [2020](#bib.bib95); Zhang et al., [2021](#bib.bib132); Hersche
    et al., [2022](#bib.bib34)). For instance,(Tao et al., [2020](#bib.bib95)) stabilizes
    the network’s topology to minimize the forgetting of previous classes. In contrast,(Zhang
    et al., [2021](#bib.bib132)) solely updates the classifiers in each incremental
    session to avoid erasing the feature extractor’s knowledge. Encouraged by these
    techniques, future research in domain-incremental is also essential. Thus, the
    objective of this setup is to train the model to expand to new domains and tasks
    while maintaining performance on previous domains and tasks.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 增量 CDFSL。当前的 CDFSL 方法旨在处理目标领域的 FSL 任务，但通常会遭遇灾难性遗忘，导致源领域性能下降。然而，一个好的模型应当保留来自两个领域和任务的先前知识。然而，一个有效的模型必须保留来自两个领域和任务的先前知识。因此，解决
    CDFSL 中的灾难性遗忘是一个重要问题。近年来，增量学习和持续学习的进展已被应用于 FSL，以应对任务增量问题（Tao et al., [2020](#bib.bib95);
    Zhang et al., [2021](#bib.bib132); Hersche et al., [2022](#bib.bib34)）。例如，（Tao
    et al., [2020](#bib.bib95)）稳定了网络拓扑，以最小化对先前类别的遗忘。相反，（Zhang et al., [2021](#bib.bib132)）仅在每个增量会话中更新分类器，以避免抹去特征提取器的知识。受到这些技术的鼓舞，领域增量的未来研究也是至关重要的。因此，这种设置的目标是训练模型扩展到新领域和任务，同时保持对先前领域和任务的性能。
- en: Interpretability-guided CDFSL. Current techniques for CDFSL rely on black-box
    feature generation, which hinders understanding of which features are optimal
    for generalization and what factors influence the model’s performance. Recent
    research by (Sa et al., [2022](#bib.bib85)) introduces attention to identify the
    importance of each sample area. However, this approach still needs refinement
    for cross-domain and cross-task settings. More recently,(Yue et al., [2020](#bib.bib131);
    Teshima et al., [2020](#bib.bib97)) have introduced causal reasoning to explain
    the causal relationships between factors in FSL, rendering the model more interpretable
    and capable of acquiring shared knowledge. For example,(Yue et al., [2020](#bib.bib131))
    proposes a Structural Causal Model (SCM) to mine the causal relationships between
    pre-trained knowledge, sample features, and labels in FSL. Therefore, research
    focused on interpretability-guided feature representation is a promising direction
    to enhance the performance of CDFSL models.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性引导的 CDFSL。当前的 CDFSL 技术依赖于黑箱特征生成，这阻碍了对哪些特征对泛化最优以及哪些因素影响模型性能的理解。最近的研究（Sa et
    al., [2022](#bib.bib85)）引入了注意力机制，以识别每个样本区域的重要性。然而，这种方法在跨领域和跨任务设置中仍需改进。最近，（Yue
    et al., [2020](#bib.bib131); Teshima et al., [2020](#bib.bib97)）引入了因果推理，以解释 FSL
    中因素之间的因果关系，使模型更具可解释性，并能够获得共享知识。例如，（Yue et al., [2020](#bib.bib131)）提出了结构因果模型（SCM），以挖掘
    FSL 中预训练知识、样本特征和标签之间的因果关系。因此，专注于可解释性引导的特征表示的研究是一个有前景的方向，以提高 CDFSL 模型的性能。
- en: Multi-modal/Multi-view CDFSL. We can enhance the performance of CDFSL by incorporating
    additional modal information from different modalities, as it has been proved
    in zero-shot learning (Wang et al., [2019b](#bib.bib113)) that information from
    diverse modalities can aid in processing unseen tasks. In particular, multi-modal
    CDFSL can furnish additional insights from varying viewpoints, further enhancing
    the performance of CDFSL. Therefore, exploring multi-modal CDFSL is a promising
    research direction to pursue.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态/多视角 CDFSL。我们可以通过结合来自不同模态的附加模态信息来增强 CDFSL 的性能，因为在零样本学习中已证明，来自不同模态的信息可以帮助处理未见过的任务（Wang
    et al., [2019b](#bib.bib113)）。特别是，多模态 CDFSL 可以提供来自不同视角的额外见解，进一步提升 CDFSL 的性能。因此，探索多模态
    CDFSL 是一个值得追求的有前景的研究方向。
- en: Imbalanced CDFSL. The current CDFSL tasks assume an equitable number of labeled
    samples in various categories, which may not accurately reflect real-world scenarios.
    Nonetheless, existing research in FSL has tackled data imbalance problems using
    techniques such as data augmentation and class imbalance loss. For instance,(Chao
    and Zhang, [2021](#bib.bib9)) proposes a data augmentation method to rebalance
    the original imbalanced data, while(Zhang et al., [2020](#bib.bib134)) suggests
    a class imbalance loss to tackle the imbalance problem in FSL. Hence, such technologies
    can be adapted to address the imbalance issue in CDFSL.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡 CDFSL。目前的 CDFSL 任务假设各类别中标记样本数量相等，这可能无法准确反映现实情况。尽管如此，现有的 FSL 研究已经通过数据增强和类别不平衡损失等技术解决了数据不平衡问题。例如，（Chao
    和 Zhang，[2021](#bib.bib9)）提出了一种数据增强方法来重新平衡原始不平衡数据，而（Zhang 等，[2020](#bib.bib134)）则建议使用类别不平衡损失来解决
    FSL 中的不平衡问题。因此，这些技术可以适应于解决 CDFSL 中的不平衡问题。
- en: 5.2\. Applications
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2. 应用
- en: As CDFSL can tackle both domain and task shift problems and few-shot learning
    problems simultaneously, it has found applications in various computer vision
    (CV) fields where data is limited. This section will highlight some promising
    CDFSL applications, including detecting rare forms of cancer (Li and Niu, [2022](#bib.bib47)),
    object tracking (Bertinetto et al., [2016](#bib.bib5)), intelligent fault diagnosis (Feng
    et al., [2022](#bib.bib20)), and addressing AI algorithm bias, *etc*.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CDFSL 可以同时解决领域和任务偏移问题以及少样本学习问题，因此它已在数据有限的各种计算机视觉（CV）领域中找到了应用。本节将重点介绍一些有前景的
    CDFSL 应用，包括检测稀有癌症形式（Li 和 Niu，[2022](#bib.bib47)）、目标跟踪（Bertinetto 等，[2016](#bib.bib5)）、智能故障诊断（Feng
    等，[2022](#bib.bib20)）以及解决 AI 算法偏见，*等等*。
- en: Rare Cancer Detection. Cancer is a severe disease that requires early detection.
    The detection of rare cancers is particularly critical due to the scarcity of
    data. Several studies have employed few-shot learning to address rare cancer detection (Akinrinade
    et al., [2022](#bib.bib3); Xu et al., [2022](#bib.bib120)). However, acquiring
    a large amount of auxiliary data from the same distribution as the target data
    is often challenging, necessitating the use of CDFSL in rare cancer detection.
    CDFSL permits the utilization of auxiliary data from other domains, significantly
    relaxing the constraints on the source data in FSL, and enhancing low detection
    rates due to the paucity of medical samples. Therefore, CDFSL is a promising approach
    to overcome the challenge of rare cancer detection.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 稀有癌症检测。癌症是一种严重的疾病，需要早期检测。稀有癌症的检测尤其关键，因为数据稀缺。几项研究使用了少样本学习来解决稀有癌症检测的问题（Akinrinade
    等，[2022](#bib.bib3)；Xu 等，[2022](#bib.bib120)）。然而，获取大量与目标数据分布相同的辅助数据通常很具挑战性，这就需要在稀有癌症检测中使用CDFSL。CDFSL
    允许利用来自其他领域的辅助数据，显著放宽了 FSL 中对源数据的限制，并提高了由于医学样本稀少导致的低检测率。因此，CDFSL 是克服稀有癌症检测挑战的有前景的方法。
- en: Object Tracking. Object tracking (Yilmaz et al., [2006](#bib.bib126)) is a crucial
    computer vision task that entails predicting the location of selected objects
    in subsequent frames based on their initial locations in the first frame. This
    task closely resembles the FSL task setting, which involves classification using
    minimal data. Consequently, some researchers (Zhou et al., [2021](#bib.bib138))
    have applied FSL to object tracking. However, domain gaps frequently exist between
    auxiliary data and target data due to variations in devices and data acquisition
    methods. Existing FSL techniques have not effectively tackled these domain gaps.
    Therefore, CDFSL has emerged as a promising direction for addressing object tracking
    challenges.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 目标跟踪。目标跟踪（Yilmaz 等，[2006](#bib.bib126)）是一个关键的计算机视觉任务，需要根据第一帧中的初始位置预测后续帧中所选对象的位置。这个任务与
    FSL 任务设置非常相似，涉及使用最少的数据进行分类。因此，一些研究人员（Zhou 等，[2021](#bib.bib138)）将 FSL 应用于目标跟踪。然而，由于设备和数据采集方法的变化，辅助数据和目标数据之间经常存在领域差距。现有的
    FSL 技术尚未有效解决这些领域差距。因此，CDFSL 已成为解决目标跟踪挑战的有前景的方向。
- en: Intelligent Fault Diagnosis. Intelligent fault diagnosis (Feng et al., [2022](#bib.bib20))
    is the process of detecting machine faults at an early stage using various diagnostic
    methods. However, establishing an ideal dataset for training intelligent diagnostic
    models is a challenging task. To address this issue, (Liu et al., [2020b](#bib.bib57))
    introduced data from other domains and utilized few-shot algorithms. As a result,
    intelligent fault diagnosis represents a promising application direction for CDFSL.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 智能故障诊断。智能故障诊断（**Feng et al., [2022](#bib.bib20)**）是利用各种诊断方法在早期阶段检测机器故障的过程。然而，为智能诊断模型建立理想的数据集是一项具有挑战性的任务。为了解决这个问题，（**Liu
    et al., [2020b](#bib.bib57)**）引入了来自其他领域的数据，并利用了少样本算法。因此，智能故障诊断代表了 **CDFSL** 的一个有前景的应用方向。
- en: Solving Algorithmic Bias. AI algorithms currently rely on training data to solve
    many real-life problems. However, inherent biases in the data can be compiled
    and amplified by the algorithms. For instance, when there is less information
    about a particular group in a dataset, the algorithm trained on this data set
    may make poor predictions for that group, leading to algorithmic bias (Kleinberg
    et al., [2018](#bib.bib42)). This is a critical ethical issue in artificial intelligence.
    A good AI algorithm should reduce bias in a dataset rather than amplifying it.
    CDFSL is a potential exploration direction for addressing algorithm bias, as it
    focuses on reducing bias in datasets and generalizing to the new domains and tasks
    by addressing domain shift and task shift. Furthermore, CDFSL can help minimize
    the performance loss caused by having few samples of a specific group in datasets.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 解决算法偏见。AI 算法目前依赖训练数据来解决许多现实生活中的问题。然而，数据中的固有偏见可能会被算法编译和放大。例如，当数据集中某一特定组的信息较少时，基于该数据集训练的算法可能会对该组做出较差的预测，从而导致算法偏见（**Kleinberg
    et al., [2018](#bib.bib42)**）。这是人工智能中的一个关键伦理问题。一个好的 AI 算法应该减少数据集中的偏见，而不是放大它。**CDFSL**
    是解决算法偏见的潜在探索方向，因为它专注于减少数据集中的偏见，并通过解决领域转移和任务转移来泛化到新领域和任务。此外，**CDFSL** 可以帮助最小化由于数据集中某个特定组样本较少而造成的性能损失。
- en: 5.3\. Theories
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 理论
- en: Invariant Risk Minimization (IRM). Machine learning systems can often pick up
    all correlations present in the training data, including those that are spurious
    due to existing data biases. To ensure generalization to new environments, it
    is crucial to discard such spurious correlations that do not hold in the future.
    Invariant Risk Minimization (IRM) is a learning paradigm proposed by (Arjovsky
    et al., [2019](#bib.bib4)) that estimates nonlinear, invariant, causal predictors
    from multiple training environments to mitigate the over-reliance of machine learning
    systems on data biases. Although still in its early stages of exploration, IRM
    is crucial for CDFSL due to the migration of domains and tasks between the source
    and target domains. In CDFSL, spurious correlations learned in the source domain
    must be discarded when adapting to the target domain tasks, making the development
    of IRM important for CDFSL. By exploring IRM for CDFSL, we can significantly enhance
    the performance on the target domain in CDFSL.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 不变风险最小化（**IRM**）。机器学习系统常常会拾取训练数据中的所有相关性，包括由于数据偏差而产生的虚假相关性。为了确保对新环境的泛化，关键在于丢弃那些在未来不再成立的虚假相关性。不变风险最小化（**IRM**）是由（Arjovsky
    et al., [2019](#bib.bib4)）提出的一种学习范式，它通过从多个训练环境中估计非线性、不变的因果预测因子，以减轻机器学习系统对数据偏差的过度依赖。尽管仍处于探索的早期阶段，但由于源领域和目标领域之间任务的迁移，**IRM**
    对于 **CDFSL** 是至关重要的。在 **CDFSL** 中，必须在适应目标领域任务时丢弃在源领域中学到的虚假相关性，这使得 **IRM** 的开发对
    **CDFSL** 来说十分重要。通过探索 **IRM** 对 **CDFSL** 的应用，我们可以显著提升 **CDFSL** 中目标领域的性能。
- en: Multiple Source Domain Organization. Although some current works in CDFSL aim
    to utilize multiple source domains to improve FSL performance on the target domain,
    there is still limited theoretical research on how to effectively organize these
    source domains, including how to select and utilize them to maximize FSL performance.
    Developing relevant theoretical research in this area can greatly advance the
    application of multi-source domains in CDFSL. An excellent reference direction
    for this is provided by  (Mansour et al., [2008](#bib.bib66)), which offers theoretical
    support for organizing multi-source domains. This could lead to more rational
    and superior works on multi-source domain CDFSL.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 多源领域组织。虽然当前一些 CDFSL 的工作旨在利用多个源领域来提升目标领域上的 FSL 性能，但在如何有效组织这些源领域方面仍然存在有限的理论研究，包括如何选择和利用它们以最大化
    FSL 性能。在这一领域发展相关的理论研究可以极大地推动多源领域在 CDFSL 中的应用。关于这一点，*Mansour 等人*（[2008](#bib.bib66)）提供了一个优秀的参考方向，提供了组织多源领域的理论支持。这可能会导致更合理、更优秀的多源领域
    CDFSL 工作。
- en: Domain Generalization. The further goal of CDFSL should be not only generalize
    to a specific domain but to all domains. Theoretical research on domain generalization (Wang
    et al., [2022b](#bib.bib110)) is essential to support this goal. Utilizing this
    research, CDFSL can be transformed into a few-shot domain generalization learning
    problem, ultimately enabling models to generalize across various domains.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 领域泛化。CDFSL 的进一步目标不仅是对特定领域进行泛化，而是对所有领域进行泛化。对领域泛化的理论研究（Wang 等，[2022b](#bib.bib110)）对于支持这一目标至关重要。利用这些研究，CDFSL
    可以转化为一个少样本领域泛化学习问题，最终使模型能够跨各种领域进行泛化。
- en: 6\. Conclusion
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6. 结论
- en: Cross-domain few-shot learning (CDFSL) is a branch of few-shot learning (FSL)
    that allows models to improve FSL performance on the target domain using samples
    from other domains, thereby eliminating the constraint of the source and target
    domains being the same in FSL. It reduces the burden of gathering vast quantities
    of supervised data for various industrial applications. In this survey, we present
    a thorough and systematic review of CDFSL, beginning with the definition of supervised
    learning, naive FSL problem, and leading to the definition of CDFSL. We explore
    the similarities and distinctions between CDFSL and related topics, such as semi-supervised
    domain adaptation, unsupervised domain adaptation, domain generalization, few-shot
    learning, and multi-task learning. Furthermore, we shed light on the main challenge
    of CDFSL, which is the unreliable two-stage empirical risk minimization, and the
    difficulties of acquiring excellent shared features. We categorize different approaches
    to address these challenges as instance-guided, parameter-based, feature post-processing,
    and hybrid approaches, and examine the advantages and limitations of each one.
    We also introduce datasets and benchmarks used in CDFSL, and the performance of
    different techniques. Lastly, we discuss the future directions of CDFSL, including
    the exploration of problem setups, applications, and theories.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 跨领域少样本学习（CDFSL）是少样本学习（FSL）的一种分支，它允许模型利用来自其他领域的样本来提升目标领域上的 FSL 性能，从而消除 FSL 中源领域和目标领域必须相同的限制。这减少了为各种工业应用收集大量监督数据的负担。在本综述中，我们对
    CDFSL 进行了彻底而系统的回顾，从监督学习的定义、简单的 FSL 问题，逐步引入 CDFSL 的定义。我们探讨了 CDFSL 与相关主题（如半监督领域适应、无监督领域适应、领域泛化、少样本学习和多任务学习）之间的相似性和区别。此外，我们揭示了
    CDFSL 的主要挑战，即不可靠的两阶段经验风险最小化以及获取优秀共享特征的困难。我们将应对这些挑战的不同方法分类为实例引导、基于参数、特征后处理和混合方法，并审查了每种方法的优缺点。我们还介绍了
    CDFSL 中使用的数据集和基准，以及不同技术的性能。最后，我们讨论了 CDFSL 的未来方向，包括问题设置、应用和理论的探索。
- en: References
  id: totrans-429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Adler et al. (2020) Thomas Adler, Johannes Brandstetter, Michael Widrich, Andreas
    Mayr, David Kreil, Michael Kopp, Günter Klambauer, and Sepp Hochreiter. 2020.
    Cross-domain few-shot learning by representation fusion. *arXiv preprint arXiv:2010.06498*
    (2020).
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adler 等人（2020）*Thomas Adler, Johannes Brandstetter, Michael Widrich, Andreas
    Mayr, David Kreil, Michael Kopp, Günter Klambauer, 和 Sepp Hochreiter.* 2020. 通过表示融合实现跨领域少样本学习。*arXiv
    预印本 arXiv:2010.06498*（2020）。
- en: 'Akinrinade et al. (2022) Olusoji Akinrinade, Chunglin Du, Samuel Ajila, and
    Toluwase A Olowookere. 2022. Deep Learning and Few-Shot Learning in the Detection
    of Skin Cancer: An Overview. In *Proceedings of the Future Technologies Conference
    (FTC) 2022, Volume 1*. Springer, 275–286.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akinrinade et al. (2022) Olusoji Akinrinade, Chunglin Du, Samuel Ajila, 和 Toluwase
    A Olowookere。2022年。深度学习和少样本学习在皮肤癌检测中的应用概述。发表于*未来技术会议（FTC）2022年会论文集，第1卷*。Springer，第275–286页。
- en: Arjovsky et al. (2019) Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David
    Lopez-Paz. 2019. Invariant risk minimization. *arXiv preprint arXiv:1907.02893*
    (2019).
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky et al. (2019) Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, 和 David
    Lopez-Paz。2019年。不变风险最小化。*arXiv 预印本 arXiv:1907.02893*（2019年）。
- en: Bertinetto et al. (2016) Luca Bertinetto, João F Henriques, Jack Valmadre, Philip
    Torr, and Andrea Vedaldi. 2016. Learning feed-forward one-shot learners. *Advances
    in neural information processing systems* 29 (2016).
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertinetto et al. (2016) Luca Bertinetto, João F Henriques, Jack Valmadre, Philip
    Torr, 和 Andrea Vedaldi。2016年。学习前馈单次学习器。*神经信息处理系统进展* 29（2016年）。
- en: Boney and Ilin (2017) Rinu Boney and Alexander Ilin. 2017. Semi-supervised and
    active few-shot learning with prototypical networks. *arXiv preprint arXiv:1711.10856*
    (2017).
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boney and Ilin (2017) Rinu Boney 和 Alexander Ilin。2017年。利用原型网络进行半监督和主动少样本学习。*arXiv
    预印本 arXiv:1711.10856*（2017年）。
- en: 'Cai et al. (2020) John Cai, Bill Cai, and Sheng Mei Shen. 2020. SB-MTL: Score-based
    meta transfer-learning for cross-domain few-shot learning. *arXiv preprint arXiv:2012.01784*
    (2020).'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai et al. (2020) John Cai, Bill Cai, 和 Sheng Mei Shen。2020年。SB-MTL：基于评分的元转移学习用于跨域少样本学习。*arXiv
    预印本 arXiv:2012.01784*（2020年）。
- en: Carey and Bartlett (1978) Susan Carey and Elsa Bartlett. 1978. Acquiring a single
    new word. (1978).
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carey and Bartlett (1978) Susan Carey 和 Elsa Bartlett。1978年。获得一个新的单词。 (1978年)。
- en: Chao and Zhang (2021) Xuewei Chao and Lixin Zhang. 2021. Few-shot imbalanced
    classification based on data augmentation. *Multimedia Systems* (2021), 1–9.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chao and Zhang (2021) Xuewei Chao 和 Lixin Zhang。2021年。基于数据增强的少样本不平衡分类。*多媒体系统*（2021年），1–9。
- en: Chen et al. (2022a) Wentao Chen, Zhang Zhang, Wei Wang, Liang Wang, Zilei Wang,
    and Tieniu Tan. 2022a. Cross-Domain Cross-Set Few-Shot Learning via Learning Compact
    and Aligned Representations. In *European Conference on Computer Vision*. Springer,
    383–399.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2022a) Wentao Chen, Zhang Zhang, Wei Wang, Liang Wang, Zilei Wang,
    和 Tieniu Tan。2022a年。通过学习紧凑和对齐的表征实现跨域跨集少样本学习。发表于*欧洲计算机视觉会议*。Springer，第383–399页。
- en: Chen et al. (2019) Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang,
    and Jia-Bin Huang. 2019. A Closer Look at Few-shot Classification. In *International
    Conference on Learning Representations*.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2019) Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang,
    和 Jia-Bin Huang。2019年。对少样本分类的深入探讨。发表于*国际学习表征会议*。
- en: Chen et al. (2022b) Yu Chen, Yunan Zheng, Zhenyu Xu, Tianhang Tang, Zixin Tang,
    Jie Chen, and Yiguang Liu. 2022b. Cross-Domain Few-Shot Classification based on
    Lightweight Res2Net and Flexible GNN. *Knowledge-Based Systems* 247 (2022), 108623.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2022b) Yu Chen, Yunan Zheng, Zhenyu Xu, Tianhang Tang, Zixin Tang,
    Jie Chen, 和 Yiguang Liu。2022b年。基于轻量级Res2Net和灵活GNN的跨域少样本分类。*知识基础系统* 247（2022年），108623。
- en: Cimpoi et al. (2014) Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy
    Mohamed, and Andrea Vedaldi. 2014. Describing textures in the wild. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 3606–3613.
    https://www.robots.ox.ac.uk/ vgg/data/dtd/.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cimpoi et al. (2014) Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy
    Mohamed, 和 Andrea Vedaldi。2014年。描述野外的纹理。发表于*IEEE计算机视觉与模式识别会议论文集*。3606–3613。https://www.robots.ox.ac.uk/vgg/data/dtd/。
- en: 'Codella et al. (2019) Noel Codella, Veronica Rotemberg, Philipp Tschandl, M Emre
    Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris,
    Michael Marchetti, et al. 2019. Skin lesion analysis toward melanoma detection
    2018: A challenge hosted by the international skin imaging collaboration (isic).
    *arXiv preprint arXiv:1902.03368* (2019).'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Codella et al. (2019) Noel Codella, Veronica Rotemberg, Philipp Tschandl, M
    Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos
    Liopyris, Michael Marchetti, 等人。2019年。皮肤病变分析以检测黑色素瘤 2018：由国际皮肤成像合作组织（ISIC）主办的挑战赛。*arXiv
    预印本 arXiv:1902.03368*（2019年）。
- en: 'Das et al. (2022) Debasmit Das, Sungrack Yun, and Fatih Porikli. 2022. ConfeSS:
    A framework for single source cross-domain few-shot learning. In *International
    Conference on Learning Representations*.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Das et al. (2022) Debasmit Das, Sungrack Yun, 和 Fatih Porikli。2022年。ConfeSS：用于单一来源跨域少样本学习的框架。发表于*国际学习表征会议*。
- en: Ding and Wang (2021) Yuan Ding and Ping Wang. 2021. Reasearch on Cross Domain
    Few-shot Learning Method Based on Local Feature Association. In *2021 6th International
    Symposium on Computer and Information Processing Technology (ISCIPT)*. IEEE, 754–759.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 和 Wang (2021) Yuan Ding 和 Ping Wang. 2021. 基于局部特征关联的跨领域少样本学习方法研究. 在 *2021年第六届计算机与信息处理技术国际研讨会
    (ISCIPT)* 中. IEEE, 754–759.
- en: Du et al. (2021) Yingjun Du, Xiantong Zhen, Ling Shao, and Cees GM Snoek. 2021.
    Hierarchical Variational Memory for Few-shot Learning Across Domains. *arXiv preprint
    arXiv:2112.08181* (2021).
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等 (2021) Yingjun Du、Xiantong Zhen、Ling Shao 和 Cees GM Snoek. 2021. 用于跨领域少样本学习的分层变分记忆.
    *arXiv 预印本 arXiv:2112.08181* (2021).
- en: Dvornik et al. (2020) Nikita Dvornik, Cordelia Schmid, and Julien Mairal. 2020.
    Selecting relevant features from a multi-domain representation for few-shot classification.
    In *European Conference on Computer Vision*. Springer, 769–786.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dvornik 等 (2020) Nikita Dvornik、Cordelia Schmid 和 Julien Mairal. 2020. 从多领域表示中选择相关特征用于少样本分类.
    在 *欧洲计算机视觉会议* 中. Springer, 769–786.
- en: Fei-Fei et al. (2006) Li Fei-Fei, Robert Fergus, and Pietro Perona. 2006. One-shot
    learning of object categories. *IEEE transactions on pattern analysis and machine
    intelligence* 28, 4 (2006), 594–611.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fei-Fei 等 (2006) Li Fei-Fei、Robert Fergus 和 Pietro Perona. 2006. 单次学习物体类别. *IEEE
    计算机视觉与模式分析汇刊* 28, 4 (2006), 594–611.
- en: 'Feng et al. (2022) Yong Feng, Jinglong Chen, Jingsong Xie, Tianci Zhang, Haixin
    Lv, and Tongyang Pan. 2022. Meta-learning as a promising approach for few-shot
    cross-domain fault diagnosis: Algorithms, applications, and prospects. *Knowledge-Based
    Systems* 235 (2022), 107646.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng 等 (2022) Yong Feng、Jinglong Chen、Jingsong Xie、Tianci Zhang、Haixin Lv 和
    Tongyang Pan. 2022. 元学习作为一种有前途的少样本跨领域故障诊断方法: 算法、应用和前景. *知识驱动系统* 235 (2022), 107646.'
- en: Finn et al. (2017) Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic
    meta-learning for fast adaptation of deep networks. In *International conference
    on machine learning*. PMLR, 1126–1135.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn 等 (2017) Chelsea Finn、Pieter Abbeel 和 Sergey Levine. 2017. 模型无关的元学习用于深度网络的快速适应.
    在 *国际机器学习会议* 中. PMLR, 1126–1135.
- en: Fu and Mui (1981) King-Sun Fu and JK Mui. 1981. A survey on image segmentation.
    *Pattern recognition* 13, 1 (1981), 3–16.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 和 Mui (1981) King-Sun Fu 和 JK Mui. 1981. 图像分割的综述. *模式识别* 13, 1 (1981), 3–16.
- en: 'Fu et al. (2021) Yuqian Fu, Yanwei Fu, and Yu-Gang Jiang. 2021. Meta-fdmixup:
    Cross-domain few-shot learning guided by labeled target data. In *Proceedings
    of the 29th ACM International Conference on Multimedia*. 5326–5334.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu 等 (2021) Yuqian Fu、Yanwei Fu 和 Yu-Gang Jiang. 2021. Meta-fdmixup: 通过标记目标数据引导的跨领域少样本学习.
    在 *第29届ACM国际多媒体会议论文集* 中. 5326–5334.'
- en: 'Fu et al. (2022a) Yuqian Fu, Yu Xie, Yanwei Fu, Jingjing Chen, and Yu-Gang
    Jiang. 2022a. ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain
    Few-Shot Learning. In *Proceedings of the 30th ACM International Conference on
    Multimedia*. 6609–6617.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu 等 (2022a) Yuqian Fu、Yu Xie、Yanwei Fu、Jingjing Chen 和 Yu-Gang Jiang. 2022a.
    ME-D2N: 多专家领域分解网络用于跨领域少样本学习. 在 *第30届ACM国际多媒体会议论文集* 中. 6609–6617.'
- en: 'Fu et al. (2022b) Yuqian Fu, Yu Xie, Yanwei Fu, Jingjing Chen, and Yu-Gang
    Jiang. 2022b. Wave-SAN: Wavelet based Style Augmentation Network for Cross-Domain
    Few-Shot Learning. *arXiv preprint arXiv:2203.07656* (2022).'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu 等 (2022b) Yuqian Fu、Yu Xie、Yanwei Fu、Jingjing Chen 和 Yu-Gang Jiang. 2022b.
    Wave-SAN: 基于小波的风格增强网络用于跨领域少样本学习. *arXiv 预印本 arXiv:2203.07656* (2022).'
- en: Fu et al. (2023) Yuqian Fu, Yu Xie, Yanwei Fu, and Yu-Gang Jiang. 2023. Meta
    Style Adversarial Training for Cross-Domain Few-Shot Learning. [https://doi.org/10.48550/ARXIV.2302.09309](https://doi.org/10.48550/ARXIV.2302.09309)
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 (2023) Yuqian Fu、Yu Xie、Yanwei Fu 和 Yu-Gang Jiang. 2023. 跨领域少样本学习的Meta
    Style对抗训练. [https://doi.org/10.48550/ARXIV.2302.09309](https://doi.org/10.48550/ARXIV.2302.09309)
- en: 'Gao et al. (2022) Yipeng Gao, Lingxiao Yang, Yunmu Huang, Song Xie, Shiyong
    Li, and Wei-Shi Zheng. 2022. AcroFOD: An Adaptive Method for Cross-domain Few-shot
    Object Detection. In *European Conference on Computer Vision*. Springer, 673–690.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等 (2022) Yipeng Gao、Lingxiao Yang、Yunmu Huang、Song Xie、Shiyong Li 和 Wei-Shi
    Zheng. 2022. AcroFOD: 一种自适应的跨领域少样本物体检测方法. 在 *欧洲计算机视觉会议* 中. Springer, 673–690.'
- en: Garcia and Bruna (2017) Victor Garcia and Joan Bruna. 2017. Few-shot learning
    with graph neural networks. *arXiv preprint arXiv:1711.04043* (2017).
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garcia 和 Bruna (2017) Victor Garcia 和 Joan Bruna. 2017. 基于图神经网络的少样本学习. *arXiv
    预印本 arXiv:1711.04043* (2017).
- en: Gong et al. (2023) Yuxuan Gong, Yuqi Yue, Weidong Ji, and Guohui Zhou. 2023.
    Cross-domain few-shot learning based on pseudo-Siamese neural network. *Scientific
    Reports* 13, 1 (2023), 1427.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong 等 (2023) Yuxuan Gong、Yuqi Yue、Weidong Ji 和 Guohui Zhou. 2023. 基于伪Siamese神经网络的跨领域少样本学习.
    *科学报告* 13, 1 (2023), 1427.
- en: Guan et al. (2020) Jiechao Guan, Manli Zhang, and Zhiwu Lu. 2020. Large-scale
    cross-domain few-shot learning. In *Proceedings of the Asian Conference on Computer
    Vision*.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guan et al. (2020) Jiechao Guan, Manli Zhang, 和 Zhiwu Lu. 2020. 大规模跨域少样本学习。在
    *亚洲计算机视觉会议*。
- en: Guo et al. (2020) Yunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella,
    John R Smith, Kate Saenko, Tajana Rosing, and Rogerio Feris. 2020. A broader study
    of cross-domain few-shot learning. In *European conference on computer vision*.
    Springer, 124–141.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2020) Yunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella,
    John R Smith, Kate Saenko, Tajana Rosing, 和 Rogerio Feris. 2020. 跨域少样本学习的更广泛研究。在
    *欧洲计算机视觉会议*。Springer，124–141。
- en: Hassani (2022) Kaveh Hassani. 2022. Cross-domain few-shot graph classification.
    *arXiv preprint arXiv:2201.08265* (2022).
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassani (2022) Kaveh Hassani. 2022. 跨域少样本图分类。 *arXiv 预印本 arXiv:2201.08265* (2022)。
- en: 'Helber et al. (2019) Patrick Helber, Benjamin Bischke, Andreas Dengel, and
    Damian Borth. 2019. Eurosat: A novel dataset and deep learning benchmark for land
    use and land cover classification. *IEEE Journal of Selected Topics in Applied
    Earth Observations and Remote Sensing* 12, 7 (2019), 2217–2226. https://github.com/phelber/eurosat.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Helber et al. (2019) Patrick Helber, Benjamin Bischke, Andreas Dengel, 和 Damian
    Borth. 2019. Eurosat：用于土地利用和土地覆盖分类的新数据集和深度学习基准。 *IEEE 应用地球观测与遥感精选专题期刊* 12，7 (2019)，2217–2226。
    [https://github.com/phelber/eurosat](https://github.com/phelber/eurosat)。
- en: Hersche et al. (2022) Michael Hersche, Geethan Karunaratne, Giovanni Cherubini,
    Luca Benini, Abu Sebastian, and Abbas Rahimi. 2022. Constrained Few-Shot Class-Incremental
    Learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)*. 9057–9067.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hersche et al. (2022) Michael Hersche, Geethan Karunaratne, Giovanni Cherubini,
    Luca Benini, Abu Sebastian, 和 Abbas Rahimi. 2022. 受限的少样本类别增量学习。在 *IEEE/CVF计算机视觉与模式识别会议
    (CVPR) 会议录*。9057–9067。
- en: 'Houben et al. (2013) Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc
    Schlipsing, and Christian Igel. 2013. Detection of traffic signs in real-world
    images: The German Traffic Sign Detection Benchmark. In *The 2013 international
    joint conference on neural networks (IJCNN)*. Ieee, 1–8. https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houben et al. (2013) Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc
    Schlipsing, 和 Christian Igel. 2013. 现实世界图像中的交通标志检测：德国交通标志检测基准。在 *2013年国际神经网络联合会议
    (IJCNN)*。IEEE，1–8。 [https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign](https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign)。
- en: Hu and Ma (2022) Yanxu Hu and Andy J Ma. 2022. Adversarial Feature Augmentation
    for Cross-domain Few-Shot Classification. In *European Conference on Computer
    Vision*. Springer, 20–37.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu and Ma (2022) Yanxu Hu 和 Andy J Ma. 2022. 用于跨域少样本分类的对抗特征增强。在 *欧洲计算机视觉会议*。Springer，20–37。
- en: Hu et al. (2022) Zhengping Hu, Zijun Li, Xueyu Wang, and Saiyue Zheng. 2022.
    Unsupervised descriptor selection based meta-learning networks for few-shot classification.
    *Pattern Recognition* 122 (2022), 108304.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2022) Zhengping Hu, Zijun Li, Xueyu Wang, 和 Saiyue Zheng. 2022. 基于无监督描述符选择的元学习网络用于少样本分类。
    *模式识别* 122 (2022)，108304。
- en: 'Hu et al. (2021) Zhengdong Hu, Yifan Sun, and Yi Yang. 2021. Switch to generalize:
    Domain-switch learning for cross-domain few-shot classification. In *International
    Conference on Learning Representations*.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2021) Zhengdong Hu, Yifan Sun, 和 Yi Yang. 2021. 切换以泛化：跨域少样本分类的领域切换学习。在
    *国际学习表征会议*。
- en: Islam et al. (2021) Ashraful Islam, Chun-Fu Richard Chen, Rameswar Panda, Leonid
    Karlinsky, Rogerio Feris, and Richard J Radke. 2021. Dynamic distillation network
    for cross-domain few-shot recognition with unlabeled data. *Advances in Neural
    Information Processing Systems* 34 (2021), 3584–3595.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Islam et al. (2021) Ashraful Islam, Chun-Fu Richard Chen, Rameswar Panda, Leonid
    Karlinsky, Rogerio Feris, 和 Richard J Radke. 2021. 用于跨域少样本识别的动态蒸馏网络，支持未标记数据。 *神经信息处理系统进展*
    34 (2021)，3584–3595。
- en: 'Ji et al. (2023) Zhong Ji, Jingwei Ni, Xiyao Liu, and Yanwei Pang. 2023. Teachers
    cooperation: team-knowledge distillation for multiple cross-domain few-shot learning.
    *Frontiers of Computer Science* 17, 2 (2023), 172312.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji et al. (2023) Zhong Ji, Jingwei Ni, Xiyao Liu, 和 Yanwei Pang. 2023. 教师合作：多领域少样本学习的团队知识蒸馏。
    *计算机科学前沿* 17，2 (2023)，172312。
- en: Jongejan et al. (2016) Jonas Jongejan, Henry Rowley, Takashi Kawashima, Jongmin
    Kim, and Nick Fox-Gieg. 2016. The quick, draw!-ai experiment. *Mount View, CA,
    accessed Feb* 17, 2018 (2016), 4. https://github.com/googlecreativelab/quickdraw-dataset.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jongejan et al. (2016) Jonas Jongejan, Henry Rowley, Takashi Kawashima, Jongmin
    Kim, 和 Nick Fox-Gieg. 2016. 快速画图！- AI 实验。 *加州山景城，访问日期* 2018年2月17日 (2016)，4。 [https://github.com/googlecreativelab/quickdraw-dataset](https://github.com/googlecreativelab/quickdraw-dataset)。
- en: Kleinberg et al. (2018) Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, and
    Ashesh Rambachan. 2018. Algorithmic fairness. In *Aea papers and proceedings*,
    Vol. 108\. 22–27.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kleinberg et al. (2018) Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, 和
    Ashesh Rambachan. 2018. 算法公平性. 见 *AEA论文与会议记录*，第108卷。22–27。
- en: Krause et al. (2013) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
    2013. 3d object representations for fine-grained categorization. In *Proceedings
    of the IEEE international conference on computer vision workshops*. 554–561. http://ai.stanford.edu/j̃krause/cars/car_dataset.html.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krause et al. (2013) Jonathan Krause, Michael Stark, Jia Deng, 和 Li Fei-Fei.
    2013. 用于细粒度分类的3D对象表示. 见 *IEEE国际计算机视觉会议论文集*，554–561. http://ai.stanford.edu/j̃krause/cars/car_dataset.html。
- en: Lake et al. (2011) Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua
    Tenenbaum. 2011. One shot learning of simple visual concepts. In *Proceedings
    of the annual meeting of the cognitive science society*, Vol. 33. https://github.com/brendenlake/omniglot.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lake et al. (2011) Brenden Lake, Ruslan Salakhutdinov, Jason Gross, 和 Joshua
    Tenenbaum. 2011. 简单视觉概念的单次学习. 见 *认知科学学会年会论文集*，第33卷。https://github.com/brendenlake/omniglot。
- en: LeCun et al. (2015) Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep
    learning. *nature* 521, 7553 (2015), 436–444.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (2015) Yann LeCun, Yoshua Bengio, 和 Geoffrey Hinton. 2015. 深度学习.
    *nature* 521, 7553 (2015), 436–444。
- en: Lee et al. (2022) Wei-Yu Lee, Jheng-Yu Wang, and Yu-Chiang Frank Wang. 2022.
    Domain-Agnostic Meta-Learning for Cross-Domain Few-Shot Classification. In *ICASSP
    2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*. IEEE, 1715–1719.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2022) Wei-Yu Lee, Jheng-Yu Wang, 和 Yu-Chiang Frank Wang. 2022. 跨领域少样本分类的领域无关元学习.
    见 *ICASSP 2022-2022 IEEE国际声学、语音与信号处理会议（ICASSP）*。IEEE，1715–1719。
- en: Li and Niu (2022) Li Li and Zhendong Niu. 2022. Few-Shot Tumor Detection via
    Feature Reweighting and Knowledge Transferring. In *Proceedings of 2021 International
    Conference on Autonomous Unmanned Systems (ICAUS 2021)*. Springer, 2606–2615.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li and Niu (2022) Li Li 和 Zhendong Niu. 2022. 通过特征重标定和知识转移的少样本肿瘤检测. 见 *2021国际自主无人系统会议（ICAUS
    2021）论文集*。Springer，2606–2615。
- en: Li et al. (2021b) Mingxi Li, Ronggui Wang, Juan Yang, Lixia Xue, and Min Hu.
    2021b. Multi-domain few-shot image recognition with knowledge transfer. *Neurocomputing*
    442 (2021), 64–72.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021b) Mingxi Li, Ronggui Wang, Juan Yang, Lixia Xue, 和 Min Hu. 2021b.
    知识转移的多领域少样本图像识别. *Neurocomputing* 442 (2021), 64–72。
- en: Li et al. (2022a) Pan Li, Shaogang Gong, Chengjie Wang, and Yanwei Fu. 2022a.
    Ranking Distance Calibration for Cross-Domain Few-Shot Learning. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 9099–9108.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022a) Pan Li, Shaogang Gong, Chengjie Wang, 和 Yanwei Fu. 2022a.
    跨领域少样本学习中的排名距离校准. 见 *IEEE/CVF计算机视觉与模式识别会议论文集*，9099–9108。
- en: Li et al. (2023) Pengfang Li, Fang Liu, Licheng Jiao, Lingling Li, Puhua Chen,
    and Shuo Li. 2023. Task Context Transformer and Gcn for Few-Shot Learning of Cross-Domain.
    *Available at SSRN 4342068* (2023).
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Pengfang Li, Fang Liu, Licheng Jiao, Lingling Li, Puhua Chen,
    和 Shuo Li. 2023. 用于跨领域少样本学习的任务上下文变换器和Gcn. *可在SSRN 4342068上获得* (2023)。
- en: Li et al. (2021a) Wei-Hong Li, Xialei Liu, and Hakan Bilen. 2021a. Universal
    representation learning from multiple domains for few-shot classification. In
    *Proceedings of the IEEE/CVF International Conference on Computer Vision*. 9526–9535.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021a) Wei-Hong Li, Xialei Liu, 和 Hakan Bilen. 2021a. 来自多个领域的通用表示学习用于少样本分类.
    见 *IEEE/CVF国际计算机视觉会议论文集*，9526–9535。
- en: Li et al. (2022b) Wei-Hong Li, Xialei Liu, and Hakan Bilen. 2022b. Cross-domain
    Few-shot Learning with Task-specific Adapters. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 7161–7170.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022b) Wei-Hong Li, Xialei Liu, 和 Hakan Bilen. 2022b. 具有任务特定适配器的跨领域少样本学习.
    见 *IEEE/CVF计算机视觉与模式识别会议论文集*，7161–7170。
- en: 'Li et al. (2022c) Wei-Hong Li, Xialei Liu, and Hakan Bilen. 2022c. Universal
    Representations: A Unified Look at Multiple Task and Domain Learning. *arXiv preprint
    arXiv:2204.02744* (2022).'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022c) Wei-Hong Li, Xialei Liu, 和 Hakan Bilen. 2022c. 通用表示：对多任务和领域学习的统一视角.
    *arXiv预印本 arXiv:2204.02744* (2022)。
- en: Liang et al. (2021) Hanwen Liang, Qiong Zhang, Peng Dai, and Juwei Lu. 2021.
    Boosting the generalization capability in cross-domain few-shot learning via noise-enhanced
    supervised autoencoder. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*. 9424–9434.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2021) Hanwen Liang, Qiong Zhang, Peng Dai, 和 Juwei Lu. 2021. 通过噪声增强的监督自编码器提升跨领域少样本学习的泛化能力.
    见 *IEEE/CVF国际计算机视觉会议论文集*，9424–9434。
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft
    coco: Common objects in context. In *European conference on computer vision*.
    Springer, 740–755. https://cocodataset.org/#download.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro
    Perona, Deva Ramanan, Piotr Dollár, 和 C Lawrence Zitnick。2014。Microsoft coco：上下文中的常见对象。在
    *欧洲计算机视觉会议*。Springer，740–755。https://cocodataset.org/#download。
- en: Lin et al. (2021) Xiao Lin, Meng Ye, Yunye Gong, Giedrius Buracas, Nikoletta
    Basiou, Ajay Divakaran, and Yi Yao. 2021. Modular Adaptation for Cross-Domain
    Few-Shot Learning. *arXiv preprint arXiv:2104.00619* (2021).
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2021) Xiao Lin, Meng Ye, Yunye Gong, Giedrius Buracas, Nikoletta
    Basiou, Ajay Divakaran, 和 Yi Yao。2021。跨领域少样本学习的模块化适应。*arXiv 预印本 arXiv:2104.00619*（2021）。
- en: 'Liu et al. (2020b) Chao Liu, Chengjin Qin, Xi Shi, Zengwei Wang, Gang Zhang,
    and Yunting Han. 2020b. TScatNet: An interpretable cross-domain intelligent diagnosis
    model with antinoise and few-shot learning capability. *IEEE Transactions on Instrumentation
    and Measurement* 70 (2020), 1–10.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020b) Chao Liu, Chengjin Qin, Xi Shi, Zengwei Wang, Gang Zhang,
    和 Yunting Han。2020b。TScatNet：一种可解释的跨领域智能诊断模型，具有抗噪声和少样本学习能力。*IEEE仪器与测量学报* 70（2020），1–10。
- en: 'Liu et al. (2020a) Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen,
    Xinwang Liu, and Matti Pietikäinen. 2020a. Deep learning for generic object detection:
    A survey. *International journal of computer vision* 128 (2020), 261–318.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020a) Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen,
    Xinwang Liu, 和 Matti Pietikäinen。2020a。通用目标检测的深度学习：综述。*国际计算机视觉期刊* 128（2020），261–318。
- en: Liu et al. (2023) Xiyao Liu, Zhong Ji, Yanwei Pang, and Zhi Han. 2023. Self-taught
    cross-domain few-shot learning with weakly supervised object localization and
    task-decomposition. *Knowledge-Based Systems* (2023), 110358.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Xiyao Liu, Zhong Ji, Yanwei Pang, 和 Zhi Han。2023。自学跨领域少样本学习，结合弱监督对象定位和任务分解。*知识基础系统*（2023），110358。
- en: 'Liu et al. (2018) Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang,
    Sung Ju Hwang, and Yi Yang. 2018. Learning to propagate labels: Transductive propagation
    network for few-shot learning. *arXiv preprint arXiv:1805.10002* (2018).'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018) Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang,
    Sung Ju Hwang, 和 Yi Yang。2018。学习传播标签：少样本学习的传导传播网络。*arXiv 预印本 arXiv:1805.10002*（2018）。
- en: Liu et al. (2021) Yanbin Liu, Juho Lee, Linchao Zhu, Ling Chen, Humphrey Shi,
    and Yi Yang. 2021. A multi-mode modulator for multi-domain few-shot classification.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*.
    8453–8462.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021) Yanbin Liu, Juho Lee, Linchao Zhu, Ling Chen, Humphrey Shi,
    和 Yi Yang。2021。用于多领域少样本分类的多模态调制器。在 *IEEE/CVF 国际计算机视觉会议论文集*。8453–8462。
- en: 'Lu et al. (2020) Jiang Lu, Pinghua Gong, Jieping Ye, and Changshui Zhang. 2020.
    Learning from very few samples: A survey. *arXiv preprint arXiv:2009.02653* (2020).'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2020) Jiang Lu, Pinghua Gong, Jieping Ye, 和 Changshui Zhang。2020。从极少样本中学习：综述。*arXiv
    预印本 arXiv:2009.02653*（2020）。
- en: Ma et al. (2021) Xinhong Ma, Junyu Gao, and Changsheng Xu. 2021. Active universal
    domain adaptation. In *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*. 8968–8977.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2021) Xinhong Ma, Junyu Gao, 和 Changsheng Xu。2021。主动通用领域适应。在 *IEEE/CVF
    国际计算机视觉会议论文集*。8968–8977。
- en: 'Magnenat-Thalmann and Thalmann (2012) Nadia Magnenat-Thalmann and Daniel Thalmann.
    2012. *Image synthesis: theory and practice*. Springer Science & Business Media.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Magnenat-Thalmann 和 Thalmann (2012) Nadia Magnenat-Thalmann 和 Daniel Thalmann。2012。*图像合成：理论与实践*。Springer
    科学与商业媒体。
- en: Maji et al. (2013) Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko,
    and Andrea Vedaldi. 2013. Fine-grained visual classification of aircraft. *arXiv
    preprint arXiv:1306.5151* (2013). https://www.robots.ox.ac.uk/ vgg/data/fgvc-aircraft/.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maji et al. (2013) Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko,
    和 Andrea Vedaldi。2013。细粒度视觉分类的飞机。*arXiv 预印本 arXiv:1306.5151*（2013）。https://www.robots.ox.ac.uk/
    vgg/data/fgvc-aircraft/。
- en: Mansour et al. (2008) Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.
    2008. Domain adaptation with multiple sources. *Advances in neural information
    processing systems* 21 (2008).
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mansour et al. (2008) Yishay Mansour, Mehryar Mohri, 和 Afshin Rostamizadeh。2008。多源领域适应。*神经信息处理系统进展*
    21（2008）。
- en: Mitchell et al. (1990) Tom Mitchell, Bruce Buchanan, Gerald DeJong, Thomas Dietterich,
    Paul Rosenbloom, and Alex Waibel. 1990. Machine learning. *Annual review of computer
    science* 4, 1 (1990), 417–433.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell et al. (1990) Tom Mitchell, Bruce Buchanan, Gerald DeJong, Thomas Dietterich,
    Paul Rosenbloom, 和 Alex Waibel。1990。机器学习。*计算机科学年评* 4，1（1990），417–433。
- en: Mohanty et al. (2016) Sharada P Mohanty, David P Hughes, and Marcel Salathé.
    2016. Using deep learning for image-based plant disease detection. *Frontiers
    in plant science* 7 (2016), 1419. https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohanty 等人 (2016) Sharada P Mohanty, David P Hughes 和 Marcel Salathé. 2016.
    使用深度学习进行基于图像的植物病害检测。*植物科学前沿* 7 (2016), 1419. https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset。
- en: Mohri et al. (2018a) Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
    2018a. *Foundations of machine learning*. MIT press.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohri 等人 (2018a) Mehryar Mohri, Afshin Rostamizadeh 和 Ameet Talwalkar. 2018a.
    *机器学习的基础*. MIT 出版社。
- en: Mohri et al. (2018b) Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
    2018b. *Foundations of machine learning*. MIT press.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohri 等人 (2018b) Mehryar Mohri, Afshin Rostamizadeh 和 Ameet Talwalkar. 2018b.
    *机器学习的基础*. MIT 出版社。
- en: Müller et al. (2022) Thomas Müller, Guillermo Pérez-Torró, Angelo Basile, and
    Marc Franco-Salvador. 2022. Active Few-Shot Learning with FASL. *arXiv preprint
    arXiv:2204.09347* (2022).
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Müller 等人 (2022) Thomas Müller, Guillermo Pérez-Torró, Angelo Basile 和 Marc
    Franco-Salvador. 2022. *FASL 的主动少样本学习*. *arXiv 预印本 arXiv:2204.09347* (2022)。
- en: Nakamura and Harada (2019) Akihiro Nakamura and Tatsuya Harada. 2019. Revisiting
    fine-tuning for few-shot learning. *arXiv preprint arXiv:1910.00216* (2019).
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakamura 和 Harada (2019) Akihiro Nakamura 和 Tatsuya Harada. 2019. 重新审视少样本学习的微调。*arXiv
    预印本 arXiv:1910.00216* (2019)。
- en: Nilsback and Zisserman (2008) Maria-Elena Nilsback and Andrew Zisserman. 2008.
    Automated flower classification over a large number of classes. In *2008 Sixth
    Indian Conference on Computer Vision, Graphics & Image Processing*. IEEE, 722–729.
    https://www.robots.ox.ac.uk/ṽgg/data/flowers/102/index.html.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nilsback 和 Zisserman (2008) Maria-Elena Nilsback 和 Andrew Zisserman. 2008. 在大量类别下的自动花卉分类。在
    *2008年第六届印度计算机视觉、图形与图像处理会议* 中。IEEE, 722–729. https://www.robots.ox.ac.uk/ṽgg/data/flowers/102/index.html。
- en: 'Oh et al. (2022) Jaehoon Oh, Sungnyun Kim, Namgyu Ho, Jin-Hwa Kim, Hwanjun
    Song, and Se-Young Yun. 2022. ReFine: Re-randomization before Fine-tuning for
    Cross-domain Few-shot Learning. *arXiv preprint arXiv:2205.05282* (2022).'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Oh 等人 (2022) Jaehoon Oh, Sungnyun Kim, Namgyu Ho, Jin-Hwa Kim, Hwanjun Song
    和 Se-Young Yun. 2022. ReFine: 在微调前进行重新随机化以实现跨领域少样本学习。*arXiv 预印本 arXiv:2205.05282*
    (2022)。'
- en: Pan and Yang (2009) Sinno Jialin Pan and Qiang Yang. 2009. A survey on transfer
    learning. *IEEE Transactions on knowledge and data engineering* 22, 10 (2009),
    1345–1359.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 和 Yang (2009) Sinno Jialin Pan 和 Qiang Yang. 2009. 关于迁移学习的调查。*IEEE 知识与数据工程汇刊*
    22, 10 (2009), 1345–1359。
- en: 'Parnami and Lee (2022) Archit Parnami and Minwoo Lee. 2022. Learning from Few
    Examples: A Summary of Approaches to Few-Shot Learning. *arXiv preprint arXiv:2203.04291*
    (2022).'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parnami 和 Lee (2022) Archit Parnami 和 Minwoo Lee. 2022. 从少量样本中学习：少样本学习方法的总结。*arXiv
    预印本 arXiv:2203.04291* (2022)。
- en: Peng et al. (2020) Shuman Peng, Weilian Song, and Martin Ester. 2020. Combining
    Domain-Specific Meta-Learners in the Parameter Space for Cross-Domain Few-Shot
    Classification. *arXiv preprint arXiv:2011.00179* (2020).
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人 (2020) Shuman Peng, Weilian Song 和 Martin Ester. 2020. 在参数空间中结合特定领域的元学习者用于跨领域少样本分类。*arXiv
    预印本 arXiv:2011.00179* (2020)。
- en: Peng et al. (2019) Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko,
    and Bo Wang. 2019. Moment matching for multi-source domain adaptation. In *Proceedings
    of the IEEE/CVF international conference on computer vision*. 1406–1415.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人 (2019) Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko
    和 Bo Wang. 2019. 多源领域适应的时刻匹配。在 *IEEE/CVF 国际计算机视觉会议论文集* 中。1406–1415。
- en: Pfister et al. (2014) Tomas Pfister, James Charles, and Andrew Zisserman. 2014.
    Domain-adaptive discriminative one-shot learning of gestures. In *European Conference
    on Computer Vision*. Springer, 814–829.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pfister 等人 (2014) Tomas Pfister, James Charles 和 Andrew Zisserman. 2014. 域自适应的单次手势学习。在
    *欧洲计算机视觉会议* 中。Springer, 814–829。
- en: Phoo and Hariharan (2020) Cheng Perng Phoo and Bharath Hariharan. 2020. Self-training
    for few-shot transfer across extreme task differences. *arXiv preprint arXiv:2010.07734*
    (2020).
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phoo 和 Hariharan (2020) Cheng Perng Phoo 和 Bharath Hariharan. 2020. 极端任务差异下的自我训练少样本迁移。*arXiv
    预印本 arXiv:2010.07734* (2020)。
- en: Pourpanah et al. (2022) Farhad Pourpanah, Moloud Abdar, Yuxuan Luo, Xinlei Zhou,
    Ran Wang, Chee Peng Lim, Xi-Zhao Wang, and QM Jonathan Wu. 2022. A review of generalized
    zero-shot learning methods. *IEEE transactions on pattern analysis and machine
    intelligence* (2022).
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pourpanah 等人 (2022) Farhad Pourpanah, Moloud Abdar, Yuxuan Luo, Xinlei Zhou,
    Ran Wang, Chee Peng Lim, Xi-Zhao Wang 和 QM Jonathan Wu. 2022. 泛化零样本学习方法的综述。*IEEE
    模式分析与机器智能汇刊* (2022)。
- en: Qiao et al. (2019) Limeng Qiao, Yemin Shi, Jia Li, Yaowei Wang, Tiejun Huang,
    and Yonghong Tian. 2019. Transductive episodic-wise adaptive metric for few-shot
    learning. In *Proceedings of the IEEE/CVF international conference on computer
    vision*. 3603–3612.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao et al. (2019) Limeng Qiao, Yemin Shi, Jia Li, Yaowei Wang, Tiejun Huang
    和 Yonghong Tian. 2019. 针对少样本学习的转导式情景自适应度量。在*IEEE/CVF国际计算机视觉会议论文集*。3603–3612。
- en: Rao et al. (2023) Shuzhen Rao, Jun Huang, and Zengming Tang. 2023. Exploiting
    Style Transfer-based Task Augmentation for Cross-Domain Few-Shot Learning. [https://doi.org/10.48550/ARXIV.2301.07927](https://doi.org/10.48550/ARXIV.2301.07927)
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rao et al. (2023) Shuzhen Rao, Jun Huang 和 Zengming Tang. 2023. 利用风格迁移任务增强跨域少样本学习。[https://doi.org/10.48550/ARXIV.2301.07927](https://doi.org/10.48550/ARXIV.2301.07927)
- en: Ren et al. (2018) Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell,
    Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. 2018.
    Meta-learning for semi-supervised few-shot classification. *arXiv preprint arXiv:1803.00676*
    (2018).
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren et al. (2018) Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell,
    Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle 和 Richard S Zemel. 2018. 半监督少样本分类的元学习。*arXiv
    预印本 arXiv:1803.00676* (2018)。
- en: Sa et al. (2022) Liangbing Sa, Chongchong Yu, Xianqin Ma, Xia Zhao, and Tao
    Xie. 2022. Attentive fine-grained recognition for cross-domain few-shot classification.
    *Neural Computing and Applications* 34, 6 (2022), 4733–4746.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sa et al. (2022) Liangbing Sa, Chongchong Yu, Xianqin Ma, Xia Zhao 和 Tao Xie.
    2022. 跨域少样本分类的注意力细粒度识别。*Neural Computing and Applications* 34, 6 (2022)，4733–4746。
- en: 'Schroeder and Cui (2018) Brigit Schroeder and Yin Cui. 2018. Fgvcx fungi classification
    challenge 2018. *Available online: github. com/visipedia/fgvcx_fungi_comp (accessed
    on 14 July 2021)* (2018). https://www.kaggle.com/c/fungi-challenge-fgvc-2018.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schroeder 和 Cui (2018) Brigit Schroeder 和 Yin Cui. 2018. Fgvcx 真菌分类挑战赛 2018。*在线可用：github.
    com/visipedia/fgvcx_fungi_comp (访问于2021年7月14日)* (2018)。 https://www.kaggle.com/c/fungi-challenge-fgvc-2018。
- en: Sharma et al. (2018) Neha Sharma, Vibhor Jain, and Anju Mishra. 2018. An analysis
    of convolutional neural networks for image classification. *Procedia computer
    science* 132 (2018), 377–384.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma et al. (2018) Neha Sharma, Vibhor Jain 和 Anju Mishra. 2018. 卷积神经网络在图像分类中的分析。*Procedia
    computer science* 132 (2018)，377–384。
- en: Shu et al. (2018) Jun Shu, Zongben Xu, and Deyu Meng. 2018. Small sample learning
    in big data era. *arXiv preprint arXiv:1808.04572* (2018).
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu et al. (2018) Jun Shu, Zongben Xu 和 Deyu Meng. 2018. 大数据时代的小样本学习。*arXiv
    预印本 arXiv:1808.04572* (2018)。
- en: Singh and Jamali-Rad (2022) Anuj Singh and Hadi Jamali-Rad. 2022. Transductive
    Decoupled Variational Inference for Few-Shot Classification. *arXiv preprint arXiv:2208.10559*
    (2022).
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 和 Jamali-Rad (2022) Anuj Singh 和 Hadi Jamali-Rad. 2022. 少样本分类的转导解耦变分推断。*arXiv
    预印本 arXiv:2208.10559* (2022)。
- en: Snell et al. (2017) Jake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical
    networks for few-shot learning. *Advances in neural information processing systems*
    30 (2017).
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snell et al. (2017) Jake Snell, Kevin Swersky 和 Richard Zemel. 2017. 少样本学习的原型网络。*神经信息处理系统进展*
    30 (2017)。
- en: 'Song et al. (2023) Yisheng Song, Ting Wang, Puyu Cai, Subrota K Mondal, and
    Jyoti Prakash Sahoo. 2023. A Comprehensive Survey of Few-Shot Learning: Evolution,
    Applications, Challenges, and Opportunities. *ACM Comput. Surv.* (feb 2023). [https://doi.org/10.1145/3582688](https://doi.org/10.1145/3582688)'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2023) Yisheng Song, Ting Wang, Puyu Cai, Subrota K Mondal 和 Jyoti
    Prakash Sahoo. 2023. 少样本学习的全面调查：演变、应用、挑战和机遇。*ACM Comput. Surv.* (2023年2月)。 [https://doi.org/10.1145/3582688](https://doi.org/10.1145/3582688)
- en: Su et al. (2020) Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu
    Maji, and Manmohan Chandraker. 2020. Active adversarial domain adaptation. In
    *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*.
    739–748.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2020) Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu
    Maji 和 Manmohan Chandraker. 2020. 主动对抗域适应。在*IEEE/CVF冬季计算机视觉应用会议论文集*。739–748。
- en: Sun et al. (2021) Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, Yunqing
    Zhao, Ngai-Man Cheung, and Alexander Binder. 2021. Explanation-guided training
    for cross-domain few-shot classification. In *2020 25th International Conference
    on Pattern Recognition (ICPR)*. IEEE, 7609–7616.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2021) Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, Yunqing
    Zhao, Ngai-Man Cheung 和 Alexander Binder. 2021. 解释引导的跨域少样本分类训练。在*2020年第25届国际模式识别会议
    (ICPR)*。IEEE，7609–7616。
- en: 'Sung et al. (2018) Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS
    Torr, and Timothy M Hospedales. 2018. Learning to compare: Relation network for
    few-shot learning. In *Proceedings of the IEEE conference on computer vision and
    pattern recognition*. 1199–1208.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sung 等（2018）Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, 和
    Timothy M Hospedales。2018。学习比较：少样本学习的关系网络。见于 *IEEE 计算机视觉与模式识别会议论文集*。1199–1208。
- en: Tao et al. (2020) Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing
    Wei, and Yihong Gong. 2020. Few-Shot Class-Incremental Learning. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao 等（2020）Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei,
    和 Yihong Gong。2020。少样本类别增量学习。见于 *IEEE/CVF 计算机视觉与模式识别会议论文集（CVPR）*。
- en: Tavera et al. (2022) Antonio Tavera, Fabio Cermelli, Carlo Masone, and Barbara
    Caputo. 2022. Pixel-by-pixel cross-domain alignment for few-shot semantic segmentation.
    In *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision*. 1626–1635.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tavera 等（2022）Antonio Tavera, Fabio Cermelli, Carlo Masone, 和 Barbara Caputo。2022。逐像素跨域对齐用于少样本语义分割。见于
    *IEEE/CVF 计算机视觉应用冬季会议论文集*。1626–1635。
- en: Teshima et al. (2020) Takeshi Teshima, Issei Sato, and Masashi Sugiyama. 2020.
    Few-shot domain adaptation by causal mechanism transfer. In *International Conference
    on Machine Learning*. PMLR, 9458–9469.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Teshima 等（2020）Takeshi Teshima, Issei Sato, 和 Masashi Sugiyama。2020。通过因果机制转移进行少样本领域适应。见于
    *国际机器学习会议*。PMLR, 9458–9469。
- en: 'Triantafillou et al. (2019) Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin,
    Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky,
    Pierre-Antoine Manzagol, et al. 2019. Meta-dataset: A dataset of datasets for
    learning to learn from few examples. *arXiv preprint arXiv:1903.03096* (2019).'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Triantafillou 等（2019）Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal
    Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine
    Manzagol 等。2019。Meta-dataset: 用于从少量样本中学习的数据库数据集。*arXiv 预印本 arXiv:1903.03096*（2019）。'
- en: 'Tripuraneni et al. (2020) Nilesh Tripuraneni, Michael Jordan, and Chi Jin.
    2020. On the theory of transfer learning: The importance of task diversity. *Advances
    in Neural Information Processing Systems* 33 (2020), 7852–7862.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tripuraneni 等（2020）Nilesh Tripuraneni, Michael Jordan, 和 Chi Jin。2020。转移学习理论：任务多样性的的重要性。*神经信息处理系统进展*
    33（2020），7852–7862。
- en: Tschandl et al. (2018) Philipp Tschandl, Cliff Rosendahl, and Harald Kittler.
    2018. The HAM10000 dataset, a large collection of multi-source dermatoscopic images
    of common pigmented skin lesions. *Scientific data* 5, 1 (2018), 1–9. https://challenge.isic-archive.com/data/#2018.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tschandl 等（2018）Philipp Tschandl, Cliff Rosendahl, 和 Harald Kittler。2018。HAM10000
    数据集，一个大型多来源皮肤镜图像的常见色素性皮肤病变集合。*科学数据* 5, 1（2018），1–9。https://challenge.isic-archive.com/data/#2018。
- en: Tseng et al. (2020) Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, and Ming-Hsuan
    Yang. 2020. Cross-domain few-shot classification via learned feature-wise transformation.
    *arXiv preprint arXiv:2001.08735* (2020).
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tseng 等（2020）Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, 和 Ming-Hsuan Yang。2020。通过学习的特征变换进行跨域少样本分类。*arXiv
    预印本 arXiv:2001.08735*（2020）。
- en: Tu and Pao (2021) Pei-Cheng Tu and Hsing-Kuo Pao. 2021. A Dropout Style Model
    Augmentation for Cross Domain Few-Shot Learning. In *2021 IEEE International Conference
    on Big Data (Big Data)*. IEEE, 1138–1147.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu 和 Pao（2021）Pei-Cheng Tu 和 Hsing-Kuo Pao。2021。一种用于跨域少样本学习的 Dropout 风格模型增强。见于
    *2021 IEEE 国际大数据会议（Big Data）*。IEEE, 1138–1147。
- en: Van Horn et al. (2018) Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,
    Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. 2018.
    The inaturalist species classification and detection dataset. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 8769–8778.
    http://vllab.ucmerced.edu/ym41608/projects/CrossDomainFewShot/filelists/plantae.tar.gz.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Horn 等（2018）Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun,
    Alex Shepard, Hartwig Adam, Pietro Perona, 和 Serge Belongie。2018。inaturalist 物种分类和检测数据集。见于
    *IEEE 计算机视觉与模式识别会议论文集*。8769–8778。http://vllab.ucmerced.edu/ym41608/projects/CrossDomainFewShot/filelists/plantae.tar.gz。
- en: Vapnik (1991) Vladimir Vapnik. 1991. Principles of risk minimization for learning
    theory. *Advances in neural information processing systems* 4 (1991).
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vapnik（1991）Vladimir Vapnik。1991。学习理论的风险最小化原则。*神经信息处理系统进展* 4（1991）。
- en: Venkateswara et al. (2017) Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty,
    and Sethuraman Panchanathan. 2017. Deep hashing network for unsupervised domain
    adaptation. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*. 5018–5027.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venkateswara 等（2017）Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty 和
    Sethuraman Panchanathan。2017年。用于无监督领域适应的深度哈希网络。在*IEEE计算机视觉与模式识别会议论文集*中。5018–5027。
- en: Vinyals et al. (2016) Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan
    Wierstra, et al. 2016. Matching networks for one shot learning. *Advances in neural
    information processing systems* 29 (2016). http://vllab.ucmerced.edu/ym41608/projects/CrossDomainFewShot/filelists/mini_imagenet_full_size.tar.bz2.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 等（2016）Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra
    等。2016年。一次性学习的匹配网络。*Advances in Neural Information Processing Systems* 29 (2016)。http://vllab.ucmerced.edu/ym41608/projects/CrossDomainFewShot/filelists/mini_imagenet_full_size.tar.bz2。
- en: Wah et al. (2011) Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona,
    and Serge Belongie. 2011. The caltech-ucsd birds-200-2011 dataset. (2011). https://www.vision.caltech.edu/datasets/cub_200_2011/.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wah 等（2011）Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona 和 Serge
    Belongie。2011年。Caltech-UCSD Birds-200-2011 数据集。（2011）。https://www.vision.caltech.edu/datasets/cub_200_2011/。
- en: Wang and Deng (2021) Haoqing Wang and Zhi-Hong Deng. 2021. Cross-domain few-shot
    classification via adversarial task augmentation. *arXiv preprint arXiv:2104.14385*
    (2021).
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Deng（2021）Haoqing Wang 和 Zhi-Hong Deng。2021年。通过对抗性任务增强进行跨领域少样本分类。*arXiv
    预印本 arXiv:2104.14385* (2021)。
- en: Wang et al. (2022a) Hongyu Wang, Eibe Frank, Bernhard Pfahringer, Michael Mayo,
    and Geoffrey Holmes. 2022a. Cross-domain Few-shot Meta-learning Using Stacking.
    *arXiv preprint arXiv:2205.05831* (2022).
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2022a）Hongyu Wang, Eibe Frank, Bernhard Pfahringer, Michael Mayo 和 Geoffrey
    Holmes。2022a年。使用堆叠进行跨领域少样本元学习。*arXiv 预印本 arXiv:2205.05831* (2022)。
- en: 'Wang et al. (2022b) Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao
    Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. 2022b. Generalizing to
    unseen domains: A survey on domain generalization. *IEEE Transactions on Knowledge
    and Data Engineering* (2022).'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2022b）Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang
    Lu, Yiqiang Chen, Wenjun Zeng 和 Philip Yu。2022b年。泛化到未见领域：领域泛化的综述。*IEEE Transactions
    on Knowledge and Data Engineering* (2022)。
- en: Wang et al. (2021) Rui-Qi Wang, Xu-Yao Zhang, and Cheng-Lin Liu. 2021. Meta-prototypical
    learning for domain-agnostic few-shot recognition. *IEEE Transactions on Neural
    Networks and Learning Systems* (2021).
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2021）Rui-Qi Wang, Xu-Yao Zhang 和 Cheng-Lin Liu。2021年。用于领域无关的少样本识别的元原型学习。*IEEE
    Transactions on Neural Networks and Learning Systems* (2021)。
- en: 'Wang et al. (2019a) Wei Wang, Yujing Yang, Xin Wang, Weizheng Wang, and Ji
    Li. 2019a. Development of convolutional neural network and its application in
    image classification: a survey. *Optical Engineering* 58, 4 (2019), 040901–040901.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2019a）Wei Wang, Yujing Yang, Xin Wang, Weizheng Wang 和 Ji Li。2019a年。卷积神经网络的发展及其在图像分类中的应用：综述。*Optical
    Engineering* 58, 4 (2019), 040901–040901。
- en: 'Wang et al. (2019b) Wei Wang, Vincent W Zheng, Han Yu, and Chunyan Miao. 2019b.
    A survey of zero-shot learning: Settings, methods, and applications. *ACM Transactions
    on Intelligent Systems and Technology (TIST)* 10, 2 (2019), 1–37.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2019b）Wei Wang, Vincent W Zheng, Han Yu 和 Chunyan Miao。2019b年。零样本学习的综述：设置、方法和应用。*ACM
    Transactions on Intelligent Systems and Technology (TIST)* 10, 2 (2019), 1–37。
- en: 'Wang et al. (2017) Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi
    Bagheri, and Ronald M Summers. 2017. Chestx-ray8: Hospital-scale chest x-ray database
    and benchmarks on weakly-supervised classification and localization of common
    thorax diseases. In *Proceedings of the IEEE conference on computer vision and
    pattern recognition*. 2097–2106. https://nihcc.app.box.com/v/ChestXray-NIHCC.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2017）Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri
    和 Ronald M Summers。2017年。ChestX-ray8：医院级胸部X射线数据库及其在弱监督分类和常见胸部疾病定位上的基准。在*IEEE计算机视觉与模式识别会议论文集*中。2097–2106。https://nihcc.app.box.com/v/ChestXray-NIHCC。
- en: 'Wang et al. (2020) Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni.
    2020. Generalizing from a few examples: A survey on few-shot learning. *ACM computing
    surveys (csur)* 53, 3 (2020), 1–34.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020）Yaqing Wang, Quanming Yao, James T Kwok 和 Lionel M Ni。2020年。从少数样本中概括：少样本学习的调查。*ACM
    Computing Surveys (CSUR)* 53, 3 (2020), 1–34。
- en: Weng et al. (2021) Zhewei Weng, Chunyan Feng, Tiankui Zhang, Yutao Zhu, and
    Zeren Chen. 2021. Representative Multi-Domain Feature Selection Based Cross-Domain
    Few-Shot Classification. In *2021 7th IEEE International Conference on Network
    Intelligence and Digital Content (IC-NIDC)*. IEEE, 86–90.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng 等 (2021) Zhewei Weng, Chunyan Feng, Tiankui Zhang, Yutao Zhu, 和 Zeren Chen。2021。《基于代表性多域特征选择的跨域少样本分类》。发表于
    *2021 第七届 IEEE 国际网络智能与数字内容会议 (IC-NIDC)*。IEEE，86–90。
- en: Wu et al. (2017) Xian Wu, Kun Xu, and Peter Hall. 2017. A survey of image synthesis
    and editing with generative adversarial networks. *Tsinghua Science and Technology*
    22, 6 (2017), 660–674.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 (2017) Xian Wu, Kun Xu, 和 Peter Hall。2017。《生成对抗网络的图像合成与编辑综述》。*清华科技* 22，6
    (2017)，660–674。
- en: Xu and Liu (2022) Huali Xu and Li Liu. 2022. Cross-Domain Few-Shot Classification
    via Inter-Source Stylization. *arXiv preprint arXiv:2208.08015* (2022).
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 和 Liu (2022) Huali Xu 和 Li Liu。2022。《通过源间风格化的跨域少样本分类》。 *arXiv 预印本 arXiv:2208.08015*
    (2022)。
- en: 'Xu et al. (2021) Yi Xu, Lichen Wang, Yizhou Wang, Can Qin, Yulun Zhang, and
    Yun Fu. 2021. MemREIN: Rein the Domain Shift for Cross-Domain Few-Shot Learning.
    (2021).'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2021) Yi Xu, Lichen Wang, Yizhou Wang, Can Qin, Yulun Zhang, 和 Yun Fu。2021。《MemREIN：控制跨域少样本学习中的领域偏移》。
    (2021)。
- en: Xu et al. (2022) Zhiyuan Xu, Kai Niu, Shun Tang, Tianqi Song, Yue Rong, Wei
    Guo, and Zhiqiang He. 2022. Bone tumor necrosis rate detection in few-shot X-rays
    based on deep learning. *Computerized Medical Imaging and Graphics* 102 (2022),
    102141.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2022) Zhiyuan Xu, Kai Niu, Shun Tang, Tianqi Song, Yue Rong, Wei Guo,
    和 Zhiqiang He。2022。《基于深度学习的少样本 X 光片骨肿瘤坏死率检测》。*计算机医学影像与图形* 102 (2022)，102141。
- en: Yalan and Jijie (2021) Li Yalan and Wu Jijie. 2021. Cross-Domain Few-Shot Classification
    through Diversified Feature Transformation Layers. In *2021 IEEE International
    Conference on Artificial Intelligence and Computer Applications (ICAICA)*. IEEE,
    549–555.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yalan 和 Jijie (2021) Li Yalan 和 Wu Jijie。2021。《通过多样化特征转换层的跨域少样本分类》。发表于 *2021
    IEEE 国际人工智能与计算机应用会议 (ICAICA)*。IEEE，549–555。
- en: Yan et al. (2015) Wang Yan, Jordan Yap, and Greg Mori. 2015. Multi-task transfer
    methods to improve one-shot learning for multimedia event detection.. In *BMVC*.
    37–1.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等 (2015) Wang Yan, Jordan Yap, 和 Greg Mori。2015。《多任务迁移方法改善多媒体事件检测中的一-shot
    学习》。发表于 *BMVC*。37–1。
- en: Yang et al. (2020) Qiang Yang, Yu Zhang, Wenyuan Dai, and Sinno Jialin Pan.
    2020. *Transfer Learning*. Cambridge University Press. [https://doi.org/10.1017/9781139061773](https://doi.org/10.1017/9781139061773)
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等 (2020) Qiang Yang, Yu Zhang, Wenyuan Dai, 和 Sinno Jialin Pan。2020。*迁移学习*。剑桥大学出版社。
    [https://doi.org/10.1017/9781139061773](https://doi.org/10.1017/9781139061773)
- en: Yao (2021) Fupin Yao. 2021. Cross-domain few-shot learning with unlabelled data.
    *arXiv preprint arXiv:2101.07899* (2021).
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao (2021) Fupin Yao。2021。《使用未标记数据的跨域少样本学习》。 *arXiv 预印本 arXiv:2101.07899* (2021)。
- en: 'Yazdanpanah and Moradi (2022) Moslem Yazdanpanah and Parham Moradi. 2022. Visual
    Domain Bridge: A Source-Free Domain Adaptation for Cross-Domain Few-Shot Learning.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    2868–2877.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yazdanpanah 和 Moradi (2022) Moslem Yazdanpanah 和 Parham Moradi。2022。《视觉领域桥：一种无源领域适应的跨域少样本学习》。发表于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*。2868–2877。
- en: 'Yilmaz et al. (2006) Alper Yilmaz, Omar Javed, and Mubarak Shah. 2006. Object
    Tracking: A Survey. *ACM Comput. Surv.* 38, 4 (dec 2006), 13–es. [https://doi.org/10.1145/1177352.1177355](https://doi.org/10.1145/1177352.1177355)'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yilmaz 等 (2006) Alper Yilmaz, Omar Javed, 和 Mubarak Shah。2006。《对象跟踪：综述》。*ACM
    计算机调查* 38，4 (2006年12月)，13–es。 [https://doi.org/10.1145/1177352.1177355](https://doi.org/10.1145/1177352.1177355)
- en: Yosinski et al. (2014) Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
    2014. How transferable are features in deep neural networks? *Advances in neural
    information processing systems* 27 (2014).
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yosinski 等 (2014) Jason Yosinski, Jeff Clune, Yoshua Bengio, 和 Hod Lipson。2014。《深度神经网络中的特征可迁移性如何？》
    *神经信息处理系统进展* 27 (2014)。
- en: Yuan et al. (2022a) Minglei Yuan, Chunhao Cai, Tong Lu, Yirui Wu, Qian Xu, and
    Shijie Zhou. 2022a. A novel forget-update module for few-shot domain generalization.
    *Pattern Recognition* 129 (2022), 108704.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等 (2022a) Minglei Yuan, Chunhao Cai, Tong Lu, Yirui Wu, Qian Xu, 和 Shijie
    Zhou。2022a。《一种新颖的少样本领域泛化遗忘更新模块》。*模式识别* 129 (2022)，108704。
- en: Yuan et al. (2021) Wang Yuan, TianXue Ma, Haichuan Song, Yuan Xie, Zhizhong
    Zhang, and Lizhuang Ma. 2021. Both Comparison and Induction are Indispensable
    for Cross-Domain Few-Shot Learning. In *2021 IEEE International Conference on
    Multimedia and Expo (ICME)*. IEEE, 1–6.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等 (2021) Wang Yuan, TianXue Ma, Haichuan Song, Yuan Xie, Zhizhong Zhang,
    和 Lizhuang Ma。2021。《比较和归纳在跨域少样本学习中的不可或缺性》。发表于 *2021 IEEE 国际多媒体与博览会会议 (ICME)*。IEEE，1–6。
- en: Yuan et al. (2022b) Wang Yuan, Zhizhong Zhang, Cong Wang, Haichuan Song, Yuan
    Xie, and Lizhuang Ma. 2022b. Task-level Self-supervision for Cross-domain Few-shot
    Learning. (2022).
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2022b) Wang Yuan, Zhizhong Zhang, Cong Wang, Haichuan Song, Yuan
    Xie, 和 Lizhuang Ma. 2022b. 任务级自监督用于跨领域少样本学习。 (2022)。
- en: Yue et al. (2020) Zhongqi Yue, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua.
    2020. Interventional few-shot learning. *Advances in neural information processing
    systems* 33 (2020), 2734–2746.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yue et al. (2020) Zhongqi Yue, Hanwang Zhang, Qianru Sun, 和 Xian-Sheng Hua.
    2020. 干预少样本学习。*神经信息处理系统进展* 33 (2020), 2734–2746。
- en: Zhang et al. (2021) Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, and
    Yinghui Xu. 2021. Few-Shot Incremental Learning With Continually Evolved Classifiers.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. 12455–12464.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2021) Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, 和
    Yinghui Xu. 2021. 带有不断演化分类器的少样本增量学习。发表于*IEEE/CVF 计算机视觉与模式识别会议论文集 (CVPR)*。12455–12464。
- en: 'Zhang et al. (2022b) Ji Zhang, Jingkuan Song, Lianli Gao, and Hengtao Shen.
    2022b. Free-Lunch for Cross-Domain Few-Shot Learning: Style-Aware Episodic Training
    with Robust Contrastive Learning. In *Proceedings of the 30th ACM International
    Conference on Multimedia*. 2586–2594.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022b) Ji Zhang, Jingkuan Song, Lianli Gao, 和 Hengtao Shen. 2022b.
    跨领域少样本学习的免费午餐：风格感知的情节训练与鲁棒对比学习。发表于*第30届 ACM 国际多媒体会议论文集*。2586–2594。
- en: Zhang et al. (2020) Linbin Zhang, Caiguang Zhang, Sinong Quan, Huaxin Xiao,
    Gangyao Kuang, and Li Liu. 2020. A class imbalance loss for imbalanced object
    recognition. *IEEE Journal of Selected Topics in Applied Earth Observations and
    Remote Sensing* 13 (2020), 2778–2792.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020) Linbin Zhang, Caiguang Zhang, Sinong Quan, Huaxin Xiao,
    Gangyao Kuang, 和 Li Liu. 2020. 针对不平衡目标识别的类别不平衡损失。*IEEE 应用地球观测与遥感期刊* 13 (2020),
    2778–2792。
- en: 'Zhang et al. (2022a) Qi Zhang, Yingluo Jiang, and Zhijie Wen. 2022a. TACDFSL:
    Task Adaptive Cross Domain Few-Shot Learning. *Symmetry* 14, 6 (2022), 1097.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022a) Qi Zhang, Yingluo Jiang, 和 Zhijie Wen. 2022a. TACDFSL:
    任务自适应跨领域少样本学习。*对称性* 14, 6 (2022), 1097。'
- en: 'Zhmoginov et al. (2022) Andrey Zhmoginov, Mark Sandler, and Maksym Vladymyrov.
    2022. Hypertransformer: Model generation for supervised and semi-supervised few-shot
    learning. In *International Conference on Machine Learning*. PMLR, 27075–27098.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhmoginov et al. (2022) Andrey Zhmoginov, Mark Sandler, 和 Maksym Vladymyrov.
    2022. Hypertransformer: 用于监督和半监督少样本学习的模型生成。发表于*国际机器学习会议*。PMLR, 27075–27098。'
- en: 'Zhou et al. (2017) Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
    and Antonio Torralba. 2017. Places: A 10 million image database for scene recognition.
    *IEEE transactions on pattern analysis and machine intelligence* 40, 6 (2017),
    1452–1464. http://data.csail.mit.edu/places/places365/places365standard_easyformat.tar.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2017) Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
    和 Antonio Torralba. 2017. Places: 一个用于场景识别的千万图像数据库。*IEEE 模式分析与机器智能交易* 40, 6 (2017),
    1452–1464。 [http://data.csail.mit.edu/places/places365/places365standard_easyformat.tar](http://data.csail.mit.edu/places/places365/places365standard_easyformat.tar)'
- en: Zhou et al. (2021) Jinghao Zhou, Bo Li, Peng Wang, Peixia Li, Weihao Gan, Wei
    Wu, Junjie Yan, and Wanli Ouyang. 2021. Real-Time Visual Object Tracking via Few-Shot
    Learning. *arXiv preprint arXiv:2103.10130* (2021).
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2021) Jinghao Zhou, Bo Li, Peng Wang, Peixia Li, Weihao Gan, Wei
    Wu, Junjie Yan, 和 Wanli Ouyang. 2021. 实时视觉目标跟踪通过少样本学习。*arXiv 预印本 arXiv:2103.10130*
    (2021)。
- en: 'Zhu and Koniusz (2022) Hao Zhu and Piotr Koniusz. 2022. EASE: Unsupervised
    discriminant subspace learning for transductive few-shot learning. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 9078–9088.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu and Koniusz (2022) Hao Zhu 和 Piotr Koniusz. 2022. EASE: 用于传导性少样本学习的无监督判别子空间学习。发表于*IEEE/CVF
    计算机视觉与模式识别会议论文集*。9078–9088。'
- en: 'Zhuo et al. (2022) Linhai Zhuo, Yuqian Fu, Jingjing Chen, Yixin Cao, and Yu-Gang
    Jiang. 2022. TGDM: Target Guided Dynamic Mixup for Cross-Domain Few-Shot Learning.
    In *Proceedings of the 30th ACM International Conference on Multimedia*. 6368–6376.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhuo et al. (2022) Linhai Zhuo, Yuqian Fu, Jingjing Chen, Yixin Cao, 和 Yu-Gang
    Jiang. 2022. TGDM: 目标引导的动态混合用于跨领域少样本学习。发表于*第30届 ACM 国际多媒体会议论文集*。6368–6376。'
- en: Zou et al. (2021) Yixiong Zou, Shanghang Zhang, Jianpeng Yu, Yonghong Tian,
    and José MF Moura. 2021. Revisiting Mid-Level Patterns for Cross-Domain Few-Shot
    Recognition. In *Proceedings of the 29th ACM International Conference on Multimedia*.
    741–749.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou et al. (2021) Yixiong Zou, Shanghang Zhang, Jianpeng Yu, Yonghong Tian,
    和 José MF Moura. 2021. 重新审视中级模式用于跨领域少样本识别。发表于*第29届 ACM 国际多媒体会议论文集*。741–749。
