- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:36:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:36:12
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2310.16499] Data Optimization in Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2310.16499] 深度学习中的数据优化：一项综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.16499](https://ar5iv.labs.arxiv.org/html/2310.16499)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.16499](https://ar5iv.labs.arxiv.org/html/2310.16499)
- en: 'Data Optimization in Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的数据优化：一项综述
- en: 'Ou Wu, Rujing Yao Ou Wu is with National Center for Applied Mathematics, Tianjin
    University, Tianjin, China, 300072\. Rujing Yao is with Department of Information
    Resources Management, Nankai University, Tianjin, China, 300071\. E-mail: wuou@tju.edu.cn,
    rjyao@mail.nankai.edu.cn.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Ou Wu，Rujing Yao Ou Wu 现任职于中国天津大学应用数学国家中心，天津，300072。Rujing Yao 现任职于中国南开大学信息资源管理系，天津，300071。电子邮件：wuou@tju.edu.cn,
    rjyao@mail.nankai.edu.cn。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large-scale, high-quality data are considered an essential factor for the successful
    application of many deep learning techniques. Meanwhile, numerous real-world deep
    learning tasks still have to contend with the lack of sufficient amounts of high-quality
    data. Additionally, issues such as model robustness, fairness, and trustworthiness
    are also closely related to training data. Consequently, a huge number of studies
    in the existing literature have focused on the data aspect in deep learning tasks.
    Some typical data optimization techniques include data augmentation, logit perturbation,
    sample weighting, and data condensation. These techniques usually come from different
    deep learning divisions and their theoretical inspirations or heuristic motivations
    may seem unrelated to each other. This study aims to organize a wide range of
    existing data optimization methodologies for deep learning from the previous literature,
    and makes the effort to construct a comprehensive taxonomy for them. The constructed
    taxonomy considers the diversity of split dimensions, and deep sub-taxonomies
    are constructed for each dimension. On the basis of the taxonomy, connections
    among the extensive data optimization methods for deep learning are built in terms
    of four aspects. We probe into rendering several promising and interesting future
    directions. The constructed taxonomy and the revealed connections will enlighten
    the better understanding of existing methods and the design of novel data optimization
    techniques. Furthermore, our aspiration for this survey is to promote data optimization
    as an independent subdivision of deep learning. A curated, up-to-date list of
    resources related to data optimization in deep learning is available at [https://github.com/YaoRujing/Data-Optimization](https://github.com/YaoRujing/Data-Optimization).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模、高质量的数据被认为是许多深度学习技术成功应用的关键因素。然而，许多实际深度学习任务仍然需要应对缺乏足够高质量数据的问题。此外，模型的鲁棒性、公平性和可信性等问题也与训练数据密切相关。因此，现有文献中大量的研究集中在深度学习任务的数据方面。一些典型的数据优化技术包括数据增强、logit扰动、样本加权和数据浓缩。这些技术通常来源于不同的深度学习领域，它们的理论启发或启发式动机可能彼此看似无关。本研究旨在整理先前文献中广泛存在的数据优化方法，并努力构建一个全面的分类法。构建的分类法考虑了分割维度的多样性，并为每个维度构建了深入的子分类。基于该分类法，从四个方面建立了深度学习数据优化方法之间的联系。我们探讨了几个有前景和有趣的未来方向。构建的分类法和揭示的联系将启发对现有方法的更好理解以及新型数据优化技术的设计。此外，我们对本综述的期望是将数据优化推广为深度学习的一个独立子领域。有关深度学习数据优化的策划、最新资源列表可在[https://github.com/YaoRujing/Data-Optimization](https://github.com/YaoRujing/Data-Optimization)上获取。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep learning, data optimization, data augmentation, sample weighting, data
    perturbation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，数据优化，数据增强，样本加权，数据扰动。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Deep learning has received increasing attention in both the AI community and
    many application domains due to its superior performance in various machine-learning
    tasks in recent years. A successful application of deep learning cannot leave
    the main factors, which include a properly designed deep neural network (DNN),
    a set of high-quality training data, and a well-suited learning strategy (e.g.,
    initialization schemes for hyper-parameters). Among the main factors, training
    data is of great importance and it usually plays a decisive role in the entire
    training process [[1](#bib.bib1)]. The concept of data-centric AI is rising, which
    breaks away from the widespread model-centric perspective [[2](#bib.bib2)]. Large
    models like GPT-4 show significant potential in the direction of achieving general
    artificial intelligence (AGI). It is widely accepted that the training for large
    models requires a huge size of high-quality training data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，由于在各种机器学习任务中表现优异，深度学习在人工智能社区及许多应用领域获得了越来越多的关注。成功应用深度学习离不开主要因素，包括适当设计的深度神经网络（DNN）、一套高质量的训练数据以及合适的学习策略（例如，超参数初始化方案）。在主要因素中，训练数据非常重要，它通常在整个训练过程中发挥决定性作用[[1](#bib.bib1)]。数据中心的人工智能概念正在兴起，它打破了广泛的模型中心视角[[2](#bib.bib2)]。像GPT-4这样的大型模型在实现通用人工智能（AGI）方面显示出显著的潜力。普遍认为，大型模型的训练需要大量高质量的训练数据。
- en: 'However, most real applications lack ideal training data. Real training data
    usually encounters one or several of the nine common issues as shown in Fig. [1](#S1.F1
    "Figure 1 ‣ item (2) ‣ I Introduction ‣ Data Optimization in Deep Learning: A
    Survey"). The following six issues are directly related to training data:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数实际应用缺乏理想的训练数据。实际训练数据通常会遇到如图[1](#S1.F1 "图 1 ‣ 项 (2) ‣ I 引言 ‣ 深度学习中的数据优化：综述")所示的九个常见问题中的一个或多个。以下六个问题与训练数据直接相关：
- en: (1)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Biased distribution: This issue denotes that the distribution of the training
    data does not conform to the true distribution of the data in a learning task.
    One typical bias is class imbalance, in which the proportions of different categories
    in the training data are not identical due to reasons such as data collection
    difficulties, whereas the proportions of different categories in test data are
    identical.'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 偏倚分布：此问题表示训练数据的分布与学习任务中数据的真实分布不符。一种典型的偏倚是类别不平衡，其中训练数据中不同类别的比例由于数据收集困难等原因不相同，而测试数据中的不同类别比例则相同。
- en: (2)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Low quality: This issue corresponds to at least two scenarios. The first refers
    to data noise that either partial training samples or partial training labels
    contain noises. As for sample noises, partial samples themselves are corrupted
    by noises. Taking optical character recognition (OCR) for example, some scanned
    images may contain serious background noises. The second typical case occurs in
    multi-model/multi-view learning scenarios. Inconsistency and information missing
    may exist [[3](#bib.bib3), [4](#bib.bib4)]. For instance, the text title for an
    image may be mistakenly provided, or it may contain limited words with little
    information.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 低质量：此问题对应至少两种情况。第一种是数据噪声，即部分训练样本或标签中包含噪声。就样本噪声而言，部分样本本身受到噪声的影响。例如，在光学字符识别（OCR）中，一些扫描图像可能包含严重的背景噪声。第二种典型情况发生在多模型/多视图学习场景中。可能存在不一致和信息缺失[[3](#bib.bib3),
    [4](#bib.bib4)]。例如，图像的文本标题可能被错误提供，或者标题中包含有限的词汇，信息量较少。
- en: '![Refer to caption](img/bc525701f6cff7704b5ea3f6efb5b990.png)'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/bc525701f6cff7704b5ea3f6efb5b990.png)'
- en: 'Figure 1: Nine issues around real training data.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图1：实际训练数据的九个问题。
- en: (3)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Small size: The training size surely impacts the training performance [[5](#bib.bib5)].
    The larger the training data, the better the training performance usually being
    attained. Due to insufficient data collection budget or technique limitation,
    the training data will be relatively small for real use. Therefore, learning under
    small-size training data is a serious concern in deep learning. This study does
    not discuss the extreme cases of small size, such as few/one/zero-shot learning.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 小规模：训练规模确实影响训练性能[[5](#bib.bib5)]。训练数据越大，通常能获得的训练性能越好。由于数据收集预算不足或技术限制，实际使用中的训练数据将相对较小。因此，在小规模训练数据下的学习在深度学习中是一个严重的问题。本研究不讨论极端的小规模情况，如少样本/单样本/零样本学习。
- en: (4)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4)
- en: 'Sample redundancy: Even though large training data is expected, it does not
    mean that every datum is useful. There are still learning tasks that the training
    set contains redundant data [[6](#bib.bib6)]. Two typical cases exist. First,
    the training size is relatively large and exceeds the processing capacity of the
    available computing hardware. Second, some regions of training samples may be
    sampled excessively, and the deletion of such excessive training samples does
    not affect the training performance. In this case, sample redundancy may occur
    in certain subsets of some categories.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 样本冗余：虽然期望有大量训练数据，但这并不意味着每个数据都是有用的。训练集仍然包含冗余数据[[6](#bib.bib6)]。存在两种典型情况。首先，训练规模相对较大，超出了可用计算硬件的处理能力。其次，某些训练样本区域可能被过度采样，删除这些过度样本不会影响训练性能。在这种情况下，样本冗余可能发生在某些类别的特定子集。
- en: (5)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5)
- en: 'Lack of diversity: This issue refers to the fact that some attributes for certain
    categories concentrate excessively in the training corpus. Data diversity is also
    crucial for DNN training [[7](#bib.bib7)]. Taking object classification as an
    example, the backgrounds in images of the dog category may usually be green grass.
    However, the “dog” category is not necessarily related to green grass. The lack
    of diversity in some non-essential attributes can lead to a spurious correlation
    between some non-essential attributes and the category. This issue is similar
    to the second case of sample redundancy. Nevertheless, lack of diversity does
    not necessarily imply the presence of redundant samples.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多样性缺失：这个问题指的是某些类别的属性在训练语料中集中过度。数据多样性对于深度神经网络（DNN）训练也至关重要[[7](#bib.bib7)]。以对象分类为例，狗类图像中的背景可能通常是绿色草地。然而，“狗”类别与绿色草地并不一定相关。某些非关键属性的多样性缺失可能导致一些非关键属性与类别之间的虚假关联。这个问题类似于样本冗余的第二种情况。然而，多样性缺失并不一定意味着存在冗余样本。
- en: (6)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6)
- en: 'Distribution drift: This issue denotes that the distribution of the involved
    data varies over time. Indeed, distribution drift may occur in most real learning
    applications, as either the concept or the form (e.g., object appearances, text
    styles) of samples varies fast or slow. Concept drift [[8](#bib.bib8)] is the
    research focus in distribution drift.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分布漂移：这个问题表示所涉及数据的分布随时间变化。实际上，大多数真实学习应用中都会发生分布漂移，因为样本的概念或形式（例如对象外观、文本风格）变化快或慢。概念漂移[[8](#bib.bib8)]是分布漂移研究的重点。
- en: 'The above summary of data issues is not mutually exclusive, as there are overlaps
    among different issues. For example, small size may only occur in several categories,
    which can also be attributed to a type of biased distribution. Besides these data
    issues, there are also some other (not exhaustive) issues closely related to the
    training data:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以上数据问题总结并非相互排斥，因为不同问题之间存在重叠。例如，小规模问题可能只发生在某些类别中，这也可以归因于一种偏向性分布。除了这些数据问题，还有一些其他（不详尽列出）与训练数据密切相关的问题：
- en: (7)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7)
- en: 'Model robustness: This issue concerns the resistance ability of a DNN model
    to adversarial attacks [[9](#bib.bib9)]. Model robustness is highly important
    for applications related to health, finance, and human life. If DNN models for
    these applications are compromised by adversarial attacks, serious consequences
    may ensue.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型鲁棒性：这个问题涉及DNN模型对对抗攻击的抵抗能力[[9](#bib.bib9)]。模型鲁棒性对于健康、金融和人类生命相关的应用至关重要。如果这些应用的DNN模型受到对抗攻击，可能会带来严重后果。
- en: (8)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8)
- en: 'Fairness: This issue concerns the performance differences among different categories
    or different attributes in a learning task [[10](#bib.bib10)]. For example, the
    recognition accuracy of faces in different color groups should be at the same
    level.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 公平性：这个问题涉及学习任务中不同类别或不同属性之间的表现差异[[10](#bib.bib10)]。例如，不同肤色群体的面部识别准确率应保持在相同水平。
- en: (9)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9)
- en: Trustworthiness. This issue has emerged in recent years as deep learning has
    been gradually applied in many safety-critical applications such as autonomous
    driving and medical assistance [[11](#bib.bib11)]. This issue is closely related
    to robustness and fairness. It mainly refers to the explainability and calibration
    of DNN models.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可信度。随着深度学习逐渐应用于许多安全关键应用（如自动驾驶和医疗辅助）[[11](#bib.bib11)]，这个问题近年来逐渐浮现。它与模型的鲁棒性和公平性密切相关。主要指DNN模型的可解释性和校准性。
- en: To address the above-mentioned issues, numerous theoretical explorations have
    been conducted and tremendous new methodologies have been proposed in previous
    literature. Most of these existing methods directly optimize the involved data
    in learning rather than explore new DNN structures, which is referred to as data
    optimization for deep learning in this paper. As the listed issues belong to different
    machine learning divisions, the inspirations and focuses of these methods are
    usually distinct and seem unrelated to each other. For instance, the primary learning
    strategy for imbalanced learning (belonging to the biased distribution issue)
    is sample weighting which assigns different weights to training samples in deep
    learning training epochs. The primary manipulation for the small-size issue is
    to employ the data augmentation technique such as image resize and mixup [[12](#bib.bib12)]
    for image classification. When dealing with label noise in deep learning, one
    strategy is to identify noisy labels and then remove them during training. In
    cases where training data for certain categories lack sufficient diversity, causal
    learning is employed to break down the spurious correlations among labels and
    some irrelevant attributes such as certain backgrounds. Due to the apparent lack
    of connection, these studies typically do not mutually cite or discuss each other.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，之前的文献中进行了大量理论探索，并提出了众多新方法。其中大多数现有方法直接优化涉及的数据，而不是探索新的深度神经网络结构，本文将其称为深度学习的数据优化。由于列出的问题属于不同的机器学习领域，这些方法的灵感和重点通常各异，似乎相互之间没有关联。例如，针对不平衡学习（属于偏倚分布问题）的主要学习策略是样本加权，即在深度学习训练阶段为训练样本分配不同的权重。针对小样本问题的主要处理方法是使用数据增强技术，如图像缩放和混合[[12](#bib.bib12)]，用于图像分类。在处理深度学习中的标签噪声时，一种策略是识别噪声标签并在训练过程中将其删除。当某些类别的训练数据缺乏足够的多样性时，采用因果学习来打破标签和某些无关属性（如特定背景）之间的虚假关联。由于明显缺乏关联，这些研究通常不会互相引用或讨论。
- en: Our previous study [[13](#bib.bib13)] partially reveals that one technique,
    namely, data perturbation, has been leveraged to deal with most aforementioned
    issues. This observation illuminates us to explore the data optimization methodologies
    for those issues in a more broad view. In this study, a comprehensive review for
    a wide range of data optimization methods is conducted. First, a systematic data
    optimization taxonomy is established in terms of eight dimensions, including pipeline,
    object, technical path, and so on. Second, the intrinsic connections among some
    classical methods are explored according to four aspects, including data perception,
    application scenario, similarity/opposite, and theory. Third, theoretical studies
    are summarized for the existing data optimization techniques. Lastly, several
    future directions are presented according to our analysis.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前的研究[[13](#bib.bib13)]部分揭示了一个技术，即数据扰动，已经被用来解决大多数上述问题。这一观察启发我们从更广泛的角度探索数据优化方法。在本研究中，对各种数据优化方法进行了全面的综述。首先，建立了一个系统的数据优化分类法，涵盖了包括管道、对象、技术路径等八个维度。其次，根据数据感知、应用场景、相似性/对立性和理论等四个方面，探讨了一些经典方法之间的内在联系。第三，总结了现有数据优化技术的理论研究。最后，根据我们的分析，提出了几个未来的研究方向。
- en: The differences between our survey and existing surveys in relevant areas, including
    imbalanced learning, noisy-label learning, data augmentation, adversarial training,
    and distillation, lie in two aspects. First, this survey takes a data-centric
    view for studies from a wide range of distinct deep learning realms. Therefore,
    our focus is merely on the data optimization studies for the listed issues. Methods
    that do not belong to data optimization for the listed issues are not referred
    to in this study. Second, the split dimensions (e.g., data perception and theory)
    which facilitate the establishment of connections among seemingly unrelated methods
    are considered in our taxonomy. These dimensions are usually not referred to in
    the existing surveys.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查与现有相关领域的调查（包括不平衡学习、噪声标签学习、数据增强、对抗训练和蒸馏）的不同之处在于两个方面。首先，本调查从数据中心的角度进行研究，涉及广泛的深度学习领域。因此，我们的重点仅在于列出问题的数据优化研究。对于不属于列出问题的数据优化的方法，本研究没有涉及。其次，我们的分类法考虑了有助于建立看似无关方法之间联系的分裂维度（例如，数据感知和理论）。这些维度在现有调查中通常未被提及。
- en: The contributions of this study are summarized as follows.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的贡献总结如下。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Methodologies related to data enhancement for dealing with distinct deep learning
    issues are reviewed with a new taxonomy. To our knowledge, this is the first work
    that aims to construct a data-centric taxonomy focusing on data optimization across
    multiple deep learning divisions and applications.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 采用新的分类方法回顾了针对不同深度学习问题的数据增强相关方法。据我们所知，这是首个旨在构建一个以数据为中心的分类法，重点关注多个深度学习领域和应用的数据优化的工作。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The connections among many seemingly unrelated methods are built according to
    our constructed taxonomy. The connections can inspire researchers to design more
    potential new techniques.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据我们构建的分类法，建立了许多看似无关方法之间的联系。这些联系可以启发研究人员设计更多潜在的新技术。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Theoretical studies for data optimization are summarized and interesting future
    directions are discussed.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据优化的理论研究总结了现有成果，并讨论了有趣的未来方向。
- en: This paper is organized as follows. Section II introduces main survey studies
    related to data optimization techniques. Section III describes the main framework
    of our constructed data optimization taxonomy. Sections IV, V, VI, and VII introduce
    the details of our taxonomy. Section VIII explores the connections among different
    data optimization techniques. Section IX presents several future directions, and
    conclusions are presented in Section X.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本文组织结构如下。第二部分介绍了与数据优化技术相关的主要调查研究。第三部分描述了我们构建的数据优化分类法的主要框架。第四、第五、第六和第七部分介绍了我们分类法的详细内容。第八部分探讨了不同数据优化技术之间的联系。第九部分提出了几个未来方向，第十部分总结了结论。
- en: II Related studies
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关研究
- en: The issues listed in the previous section gradually spawn numerous independent
    research realms of deep learning. Subsequently, there have been many survey studies
    conducted for these issues. The following introduces related surveys in several
    typical research topics.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节列出的问题逐渐衍生出许多独立的深度学习研究领域。随后，针对这些问题进行了许多调查研究。以下介绍了几个典型研究主题的相关调查。
- en: Imbalanced learning. It is a hot research area in deep learning [[14](#bib.bib14)].
    He and Garcia [[15](#bib.bib15)] conducted the first comprehensive yet deep survey
    study on imbalanced learning. They explored the intrinsic characteristics of learning
    tasks incurred by imbalanced data. It is noteworthy that He and Garcia pointed
    out that an imbalanced dataset is “a high-complexity dataset with both between-class
    and within-class imbalances, multiple concepts, overlapping, noise, and a lack
    of representative data”. This statement refers to most data issues listed in Section
    I. For instance, the lack of diversity and sample redundancy can be considered
    as a lack of representativeness. Recent studies have focused on the extreme case
    of imbalanced learning, namely, long-tailed classification. Zhang et al. [[16](#bib.bib16)]
    summarized the recent developments in deep long-tailed classification. In their
    constructed taxonomy, module improvement such as a new classifier is listed as
    one of the three main techniques. In this study, module improvement is not considered,
    as it does not fall under data optimization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡学习。这是深度学习中的一个热门研究领域 [[14](#bib.bib14)]。He 和 Garcia [[15](#bib.bib15)] 进行了首个全面而深入的不平衡学习调查研究。他们探讨了不平衡数据引发的学习任务的内在特征。值得注意的是，He
    和 Garcia 指出，不平衡数据集是“一种具有高复杂性的数据集，存在类别间和类别内的不平衡、多重概念、重叠、噪声以及缺乏代表性数据”。这一说法涉及到第 I
    部分列出的多数数据问题。例如，缺乏多样性和样本冗余可以被视为代表性不足。最近的研究集中于不平衡学习的极端情况，即长尾分类。Zhang 等人 [[16](#bib.bib16)]
    总结了深度长尾分类的最新进展。在他们构建的分类法中，模块改进（如新的分类器）被列为三种主要技术之一。在本研究中，模块改进未被考虑，因为它不属于数据优化范围。
- en: Noisy-label learning. This is another research area gaining tremendous attention
    in recent years as label noise is nearly unavoidable in real learning tasks. Algan
    and Ulisory [[17](#bib.bib17)] summarized the methods in noisy-label learning
    for image classification. Song et al. [[18](#bib.bib18)] elaborately designed
    taxonomy for noisy-label learning along with three categories, including “data”,
    “objective”, and “optimization”. Their taxonomy facilitates the understanding
    of a huge number of existing techniques. Nevertheless, overlap exists between
    the three dimensions. For example, reweighting locates in the “objective” category,
    whereas learning to reweight locates in the “optimization” category. The taxonomy
    introduces in this study may aid the construction a more appropriate taxonomy
    for noisy-label learning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有噪标签学习。这是近年来获得极大关注的另一个研究领域，因为在实际学习任务中，标签噪声几乎是不可避免的。Algan 和 Ulisory [[17](#bib.bib17)]
    总结了图像分类中有噪标签学习的方法。Song 等人 [[18](#bib.bib18)] 详细设计了有噪标签学习的分类法，并将其分为“三个类别”，包括“数据”、“目标”和“优化”。他们的分类法有助于理解大量现有技术。然而，这三者之间存在重叠。例如，重加权属于“目标”类别，而学习重加权则属于“优化”类别。本研究中介绍的分类法可能有助于构建更合适的有噪标签学习分类法。
- en: Learning with small data. Big data has achieved great success in deep learning
    tasks. Meanwhile, many real learning tasks still confront with the challenge of
    small-size training data. Cao et al. [[19](#bib.bib19)] performed rigorous theoretical
    analysis for the generalization error and label complexity of learning on small
    data. They categorized the small-data learning methods into those with the Euclidean
    or non-Euclidean mean representation. Wang et al. [[20](#bib.bib20)] constructed
    a few-shot learning taxonomy with three folds, including “data”, “model”, and
    “algorithm”. Data-centric learning methods are also among the primary choices
    for few-shot learning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 小数据学习。大数据在深度学习任务中取得了巨大成功。与此同时，许多实际学习任务仍面临小规模训练数据的挑战。Cao 等人 [[19](#bib.bib19)]
    对小数据学习的泛化误差和标签复杂性进行了严格的理论分析。他们将小数据学习方法分类为具有欧几里得或非欧几里得均值表示的方法。Wang 等人 [[20](#bib.bib20)]
    构建了一个包含“三个方面”的少样本学习分类法，包括“数据”、“模型”和“算法”。以数据为中心的学习方法也是少样本学习中的主要选择之一。
- en: Concept drift. Lu et al. [[8](#bib.bib8)] investigated the learning for concept
    drift under three components, including concept drift detection, concept drift
    understanding, and concept drift adaptation. Yuan et al. [[21](#bib.bib21)] divided
    existing studies into two categories, namely, model parameter updating and model
    structure updating in concept drift adaptation. This division is from the viewpoint
    of the model. Indeed, pure data-based strategy is also employed in learning under
    concept drift. For example, Diez-Olivan et al. [[22](#bib.bib22)] leveraged data
    augmentation to fine-tune the last layer of DNNs for quickly concept drift adaptation.
    This study may motivate researchers on distribution drift to focus more on data
    optimization manners. Some early surveys can be found in [[23](#bib.bib23), [24](#bib.bib24)].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 概念漂移。Lu 等人[[8](#bib.bib8)] 研究了概念漂移的学习，涵盖三个方面，包括概念漂移检测、概念漂移理解和概念漂移适应。Yuan 等人[[21](#bib.bib21)]
    将现有研究分为两类，即概念漂移适应中的模型参数更新和模型结构更新。这一划分是从模型的角度出发的。实际上，纯数据驱动的策略也被应用于概念漂移下的学习。例如，Diez-Olivan
    等人[[22](#bib.bib22)] 利用数据增强技术对 DNN 的最后一层进行微调，以实现快速的概念漂移适应。这项研究可能会激励研究人员在分布漂移方面更多关注数据优化方式。一些早期的调查可以在[[23](#bib.bib23),
    [24](#bib.bib24)]中找到。
- en: Adversarial robustness. In many studies, model robustness is limited to adversarial
    robustness. Silva and Najafirad [[25](#bib.bib25)] explored challenges and future
    directions for model robustness in deep learning. They divided existing adversarial
    robust learning methods into three categories, including adversarial training,
    regularization, and certified defenses. Xu et al. [[26](#bib.bib26)] summarized
    the studies for model robustness on graphs. Goyal et al. [[27](#bib.bib27)] reviewed
    the adversarial defense and robustness in the NLP community. Their constructed
    taxonomy contains four categories, including adversarial training, perturbation
    control, certification, and miscellaneous. In this study, adversarial training
    is taken as a data optimization strategy, as it enhances the training set by adding
    or virtually adding new data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗鲁棒性。在许多研究中，模型鲁棒性限于对抗鲁棒性。Silva 和 Najafirad[[25](#bib.bib25)] 探索了深度学习中模型鲁棒性的挑战和未来方向。他们将现有的对抗鲁棒学习方法分为三类，包括对抗训练、正则化和认证防御。Xu
    等人[[26](#bib.bib26)] 总结了图上的模型鲁棒性研究。Goyal 等人[[27](#bib.bib27)] 综述了 NLP 领域的对抗防御和鲁棒性。他们构建的分类法包含四个类别，包括对抗训练、扰动控制、认证和其他。在这项研究中，对抗训练被视为一种数据优化策略，因为它通过添加或虚拟添加新数据来增强训练集。
- en: Fairness-aware learning. It receives increasingly attention in recent years.
    Mehrabi et al. [[28](#bib.bib28)] explored different sources of biases that can
    affect the fairness of learning models. They revealed that each of the three factors,
    namely, data, learning algorithms, and involved users may result in bias. Petrović
    et al. [[29](#bib.bib29)] pointed out that sample reweighting and adversarial
    training are two common strategies for fair machine learning.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性学习。近年来越来越受到关注。Mehrabi 等人[[28](#bib.bib28)] 探索了可能影响学习模型公平性的不同偏差来源。他们揭示了数据、学习算法和相关用户这三种因素可能导致偏差。Petrović
    等人[[29](#bib.bib29)] 指出，样本重加权和对抗训练是公平机器学习的两种常见策略。
- en: Trustworthy learning. It is the key of trustworthy AI, which aims to ensure
    that an AI system is worthy of being trusted. Trust is a complex phenomenon [[30](#bib.bib30)]
    highly related to fairness, explainability, reliability, etc. Kaur et al. [[31](#bib.bib31)]
    summarized studies on trustworthy artificial intelligence in a quite broad view.
    Wu et al. [[32](#bib.bib32)] provided an in-depth review for studies about trustworthy
    learning on graphs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 可信赖学习。它是可信 AI 的关键，旨在确保 AI 系统值得信赖。信任是一个复杂的现象[[30](#bib.bib30)]，与公平性、可解释性、可靠性等高度相关。Kaur
    等人[[31](#bib.bib31)] 从广泛的角度总结了关于可信人工智能的研究。Wu 等人[[32](#bib.bib32)] 对图上的可信赖学习研究进行了深入的回顾。
- en: There are also studies that focus on learning tasks with more than one of the
    listed data issues. For example, Fang et al. [[33](#bib.bib33)] addressed noisy-label
    learning under the long-tailed distributions of training data. Singh et al. [[34](#bib.bib34)]
    conducted an empirical study concerning fairness, adversarial robustness, and
    concept drift, simultaneously. To our knowledge, no survey study pays attention
    to the intersection of the research areas related to the listed issues. The unified
    taxonomy constructed in this survey would enlighten the study on the intersection
    of multiple research areas.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些研究关注于具有多个列出数据问题的学习任务。例如，Fang等人[[33](#bib.bib33)]解决了在长尾分布的训练数据下的噪声标签学习问题。Singh等人[[34](#bib.bib34)]进行了关于公平性、对抗鲁棒性和概念漂移的实证研究。根据我们的了解，没有任何调查研究关注与列出问题相关的研究领域交集。本调查构建的统一分类法将启发对多个研究领域交集的研究。
- en: The most similar study to this work is the survey presented by Wan et al. [[35](#bib.bib35)],
    which focuses on data optimization in computer vision. There are significant differences
    between our and Wan et al.’s study. First, the covered technical scopes of ours
    are much broader than those of Wan et al.’s study. Their study limits the scope
    merely in data selection, including resampling, subset selection, and active learning-based
    selection. Nevertheless, perturbation, weighting, dataset distillation, and augmentation
    which attempt to optimize the training data without modifying the backbone network
    are considered as the data optimization in this study. Second, the split dimensions
    of ours are quite different from those in Wan et al.’s study for the overlapped
    methods. For example, curriculum learning is divided into the resampling category,
    whereas it is divided into the weighting category in our study. Dataset distillation
    is merged into one division, namely, data pruning. In contrast, dataset distillation
    is not included in Wan et al.’s study. Lastly, additional important parts including
    data perception, connections among different paths, and theoretical investigation
    are introduced and discussed in this study. Zheng et al. [[36](#bib.bib36)] holds
    a data-centric perspective to review studies on graph machine learning, which
    is also similar in spirit with our study. Most data issues summarized in this
    study are also discussed in their study. They divided existing studies into graph
    data collection, enhancement, exploration, maintenance (for privacy and security),
    and graph operations, which are not applicable for our taxonomy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与本研究最相似的研究是Wan等人[[35](#bib.bib35)]提出的调查，该调查重点关注计算机视觉中的数据优化。我们与Wan等人的研究之间存在显著差异。首先，我们研究的技术范围比Wan等人的研究要广泛得多。他们的研究仅限于数据选择，包括重采样、子集选择和基于主动学习的选择。然而，本研究将扰动、加权、数据集蒸馏和数据增强视为数据优化，它们尝试在不修改主干网络的情况下优化训练数据。其次，我们的分类维度与Wan等人的研究中重叠方法的分类差异很大。例如，课程学习被划分到重采样类别，而在我们的研究中被划分到加权类别。数据集蒸馏被合并为一个分类，即数据修剪。相比之下，Wan等人的研究中没有包括数据集蒸馏。最后，本研究介绍和讨论了包括数据感知、不同路径之间的联系和理论研究在内的其他重要部分。Zheng等人[[36](#bib.bib36)]持数据中心的视角来回顾图机器学习的研究，这与我们的研究精神也相似。本研究总结的大多数数据问题也在他们的研究中讨论。他们将现有研究分为图数据收集、增强、探索、维护（针对隐私和安全）和图操作，这些不适用于我们的分类法。
- en: It is noteworthy that classical data pre-processing methodologies such as data
    cleaning (e.g., missing data imputation), standardization (e.g., z-score), and
    transformation (e.g., data discretization) also aim to make data better for learning.
    Considering that these methodologies are mature and mainly utilized in shallow
    learning, they are not introduced in this survey.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，经典的数据预处理方法，如数据清洗（例如，缺失数据插补）、标准化（例如，z-score）和转换（例如，数据离散化），也旨在使数据更适合学习。考虑到这些方法已经成熟并主要用于浅层学习，因此在本次调查中没有介绍。
- en: '![Refer to caption](img/4fe087664905a43890322c9566751d52.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4fe087664905a43890322c9566751d52.png)'
- en: 'Figure 2: The six split dimensions of our constructed taxonomy for data optimization.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们构建的数据优化分类法的六个分维度。
- en: III Overall of The proposed taxonomy
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 提出的分类法总体概述
- en: To ensure our constructed taxonomy well organized and comprehensive coverage
    on previous data optimization techniques about the issues listed in Section I
    as much as possible, the following principles for the design of split dimensions
    are considered.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们构建的分类法组织良好，并尽可能全面涵盖前面提到的数据优化技术，以下原则用于设计拆分维度：
- en: (1)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: The first layer of the taxonomy should consider multiple views, with each view
    corresponding to a sub-taxonomy. Most existing taxonomies for specific research
    realms adopt only a single view. In this study, only a single view is inadequate
    for systematically arranging studies from various deep learning realms.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类法的第一层应考虑多个视角，每个视角对应一个子分类法。大多数现有分类法仅采用单一视角。在这项研究中，仅有单一视角不足以系统地整理来自各个深度学习领域的研究。
- en: (2)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2)
- en: The dividing dimension should be general so as to embrace existing studies as
    much as possible. Therefore, the dimensions designed in existing taxonomies for
    specific research areas should not be directly followed. A new comprehensive taxonomy
    is required.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 划分维度应具有通用性，以尽可能涵盖现有研究。因此，不应直接遵循现有分类法为特定研究领域设计的维度。需要一个新的综合分类法。
- en: (3)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3)
- en: The new taxonomy should be compatible with existing taxonomies. That is, inconsistency
    between our and existing taxonomies is allowed. However, contradiction between
    them should be avoided.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新的分类法应与现有分类法兼容。即，允许我们的分类法与现有分类法之间存在不一致。然而，应避免它们之间的矛盾。
- en: '![Refer to caption](img/82333042bad7669980d42ed56a472982.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/82333042bad7669980d42ed56a472982.png)'
- en: 'Figure 3: The sub-taxonomy for optimization goals.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：优化目标的子分类。
- en: 'On the basis of these principles, the first layer¹¹1The fine-granularity layers
    are detailed in the succeeding sections. of our taxonomy is designed as shown
    in Fig. fig2\. This layer consists of six dimensions for data optimization as
    follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些原则，我们的分类法的第一层¹¹1细粒度层在接下来的部分中详细说明。设计如图fig2所示。该层由六个数据优化维度组成，如下所示：
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Ultimate goals. This dimension refers to the final goal of a data optimization
    method used in a deep learning task. We divide the optimization goals into five
    main aspects²²2It should be noted that these five aspects are not exhaustive and
    there are overlaps among them as revealed by the previous literature., including
    generalization, robustness, fairness, trustworthy, and efficiency.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终目标。该维度指的是用于深度学习任务的数据优化方法的最终目标。我们将优化目标分为五个主要方面²²2应注意，这五个方面并不详尽，前文文献中已揭示它们之间存在重叠，包括泛化、鲁棒性、公平性、可信度和效率。
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Application scenarios. This dimension refers to the deep learning applications
    that utilize data optimization. Nine applications are involved, including learning
    under biased distribution, noisy-label learning, learning with redundant training
    data, learning with limited training data, model safety, fairness-aware learning,
    learning under distribution drift, trustworthy learning, and learning for large
    models.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用场景。该维度指的是利用数据优化的深度学习应用。涉及九种应用，包括在偏差分布下的学习、噪声标签学习、冗余训练数据学习、有限训练数据学习、模型安全性、公平感知学习、分布漂移下的学习、可信学习以及大模型学习。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data objects. This dimension refers to the objects to optimize in the employed
    data optimization method. Most studies focus on the raw training data. There are
    also methods concentrating in other data objects such as hyper-parameters and
    meta data.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据对象。该维度指的是在所采用的数据优化方法中需要优化的对象。大多数研究集中在原始训练数据上。也有方法关注其他数据对象，如超参数和元数据。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Optimization pipeline. This dimension refers to the common steps for a concrete
    data optimization method for a deep learning procedure. We divide the pipeline
    into three common steps, namely, data perception, analysis, and optimizing.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优化流程。该维度指的是深度学习过程中具体数据优化方法的常见步骤。我们将流程分为三个常见步骤，即数据感知、分析和优化。
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Optimization techniques. This dimension refers to the employed technique paths
    in data optimization. This study summarizes five main paths, namely, data resampling,
    data augmentation, data perturbation, data weighting, and dataset pruning. Each
    path also contains sub-divisions. The introduction for this part is the focus
    of this survey.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优化技术。本维度指的是数据优化中采用的技术路径。本研究总结了五个主要路径，即数据重采样、数据增强、数据扰动、数据加权和数据集剪枝。每个路径还包含子分类。该部分的介绍是本调查的重点。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Optimization theories. This dimension refers to the theoretical analysis and
    exploration for data optimization in deep learning. We divided this dimension
    into two aspects: formulation and explanation.'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优化理论。本维度指的是对深度学习中数据优化的理论分析和探索。我们将这一维度分为两个方面：表述和解释。
- en: Section IV introduces the ultimate goals, application scenarios, and data objects.
    Sections V, VI, and VII introduces the optimization pipeline, technique, and theory,
    respectively.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第四节介绍了终极目标、应用场景和数据对象。第五、六、七节分别介绍了优化流程、技术和理论。
- en: IV Goals, scenarios, and data objects
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 目标、场景和数据对象
- en: This section introduces ultimate goals, targeted applications, and data objects.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了终极目标、目标应用和数据对象。
- en: IV-A Optimization goals
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 优化目标
- en: 'Fig. [14](#S9.F14 "Figure 14 ‣ IX-E Data optimization agent ‣ IX Future directions
    ‣ Data Optimization in Deep Learning: A Survey") describes the sub-taxonomy for
    the dimension of optimization goals, including generalization, fairness, robustness,
    trustworthiness, and efficiency.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图[14](#S9.F14 "图 14 ‣ IX-E 数据优化代理 ‣ IX 未来方向 ‣ 深度学习中的数据优化：调查") 描述了优化目标维度的子分类，包括泛化、公平性、鲁棒性、可信度和效率。
- en: Generalization is the primary optimization goal in most data optimization techniques,
    as it is almost the sole goal in most deep learning tasks. According to the generalization
    theory studied in shallow learning, generalization of a category is highly related
    to class margin, inter-class distance, and class compactness [[37](#bib.bib37)].
    A larger margin/larger inter-class distance/higher class compactness of a category
    indicates a better generalization performance on the learned model on the category.
    The data augmentation strategy that injects noise to training samples is proven
    to increase the generalization [[38](#bib.bib38)]. The implicit data augmentation
    method ISDA [[39](#bib.bib39)] actually aims to improve the class compactness³³3Some
    methods such as center loss also aim to increase the class compactness. These
    methods are considered not data optimization. of each category. Adaptive margin
    loss [[40](#bib.bib40)] also aims to improve the class compactness by perturbing
    the logits. Fujii et al. [[41](#bib.bib41)] modified the classical data augmentation
    method mixup [[12](#bib.bib12)] by considering the “between-class distance”, which
    finally increases the inter-class distance. In addition, some studies explore
    the compiling of an optimal batch in the training process of deep learning [[42](#bib.bib42)].
    The ultimate goal is also the generalization. Nevertheless, the direct goal of
    the batch compiling may consist of balance, diversity, and others.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化是大多数数据优化技术中的主要优化目标，因为它几乎是大多数深度学习任务中的唯一目标。根据浅层学习中研究的泛化理论，一个类别的泛化与类别间距、类间距离和类别紧凑性高度相关[[37](#bib.bib37)]。一个类别的边距/类别间距离/类紧凑性越大，表示在该类别上的学习模型泛化性能越好。已经证明，向训练样本注入噪声的数据增强策略可以提高泛化能力[[38](#bib.bib38)]。隐式数据增强方法ISDA[[39](#bib.bib39)]
    实际上旨在提高每个类别的类紧凑性。自适应边际损失[[40](#bib.bib40)]也通过扰动logits来提高类别紧凑性。Fujii等人[[41](#bib.bib41)]
    通过考虑“类间距离”修改了经典的数据增强方法mixup[[12](#bib.bib12)]，最终增加了类别间距离。此外，一些研究探索了在深度学习训练过程中编排最佳批次[[42](#bib.bib42)]。最终目标也是泛化。然而，批次编排的直接目标可能包括平衡、多样性等。
- en: As previously stated, fairness is also an important learning goal in many deep
    learning tasks. To combat unfairness on samples with certain attributes, techniques
    such as data augmentation [[43](#bib.bib43)], perturbation [[44](#bib.bib44)],
    and sample weighting [[45](#bib.bib45)] have been used in previous literature.
    Indeed, imbalanced learning also pursues fairness among different categories.
    A category with a small prior probability, denoted as a minor category, will receive
    more attention in the employed data optimization. For example, larger weights [[46](#bib.bib46)],
    larger degrees of data perturbation [[47](#bib.bib47)], or more augmented quantities [[48](#bib.bib48)]
    are exerted on minor categories than others.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，公平性在许多深度学习任务中也是一个重要的学习目标。为了应对具有特定属性的样本的不公平问题，已有文献中使用了数据增强[[43](#bib.bib43)]、扰动[[44](#bib.bib44)]和样本加权[[45](#bib.bib45)]等技术。实际上，不平衡学习也追求不同类别之间的公平性。具有小先验概率的类别，即次要类别，将在所采用的数据优化中受到更多关注。例如，相对于其他类别，次要类别会施加更大的权重[[46](#bib.bib46)]、更大的数据扰动程度[[47](#bib.bib47)]或更多的增强量[[48](#bib.bib48)]。
- en: Adversarial robustness is an essential goal in deep learning tasks that are
    quite sensitive to model safety. Adversarial training is usually leveraged to
    improve the adversarial robustness of a model. It can be attributed to a special
    type of data augmentation. Thus, adversarial training is actually a data optimization
    technique, which aims to improve the quality of training data such that the models
    trained on the optimized training data have better adversarial robustness.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗鲁棒性是深度学习任务中一个至关重要的目标，这些任务对模型安全性非常敏感。通常，利用对抗训练来提高模型的对抗鲁棒性。这可以归因于一种特殊类型的数据增强。因此，对抗训练实际上是一种数据优化技术，其目标是提高训练数据的质量，以便在优化后的训练数据上训练的模型具有更好的对抗鲁棒性。
- en: Trustworthiness is a goal that has recently been highly valued. Explainability
    and calibration are two crucial requirements for the trustworthiness of a deep
    learning model. Explainability mainly relies on methodologies such as feature
    attribution and causal reasoning rather than pure data optimization technique.
    Nevertheless, data optimization is widely used in model calibration. Calibration
    mainly concerns the trustworthiness of the predicted probability of a probabilistic
    model [[49](#bib.bib49)]. Liu et al. [[50](#bib.bib50)] introduced margin-aware
    label smoothing to improve the calibration of trained models. Mukhoti et al. [[51](#bib.bib51)]
    leveraged sample weighting to achieve better calibration.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 可信度是最近受到高度重视的目标。解释性和校准是深度学习模型可信度的两个关键要求。解释性主要依赖于特征归因和因果推理等方法，而非纯粹的数据优化技术。然而，数据优化在模型校准中被广泛使用。校准主要涉及概率模型的预测概率的可信度[[49](#bib.bib49)]。Liu等人[[50](#bib.bib50)]引入了边际感知标签平滑来提高训练模型的校准性。Mukhoti等人[[51](#bib.bib51)]利用样本加权来实现更好的校准。
- en: Efficiency is crucial for real applications as many learning tasks are sensitive
    to both time complexity and storage. Therefore, how to optimally reduce the redundant
    training data and remain the diverse important training data deserves further
    investigation. The time complexity can be significantly reduced after data pruning.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 效率在实际应用中至关重要，因为许多学习任务对时间复杂度和存储都很敏感。因此，如何优化地减少冗余的训练数据并保留多样且重要的训练数据值得进一步研究。数据修剪后，时间复杂度可以显著降低。
- en: '![Refer to caption](img/ca22c53e3d76b0242eefdda11ff6b2f4.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca22c53e3d76b0242eefdda11ff6b2f4.png)'
- en: 'Figure 4: The sub-taxonomy for targeted application scenarios.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：针对应用场景的子分类。
- en: IV-B Application scenarios
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 应用场景
- en: Fig. fig4 describes the sub-taxonomy for the dimension of targeted application
    scenarios. The first eight scenarios have been referred to in previous sections,
    so they are not further introduced in this subsection.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图fig4描述了针对应用场景维度的子分类。前八个场景在前面的章节中已被提及，因此在本小节中不再进一步介绍。
- en: Learning under insufficient data contains the case that the training data are
    not as diverse as possible. Data diversity affects the model generalization [[7](#bib.bib7)].
    Dunlap et al. [[52](#bib.bib52)] utilized large vision and language models to
    automatically generate visually consistent yet significantly diversified training
    data. Some studies [[53](#bib.bib53), [54](#bib.bib54)] consider that data augmentation
    is actually a widely-used technique to increase data diversity. These studies
    develop new data augmentation methods for deep learning tasks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据不足的情况下进行学习，包括训练数据不尽可能多样化的情况。数据多样性影响模型的泛化能力[[7](#bib.bib7)]。Dunlap等人[[52](#bib.bib52)]
    利用大型视觉和语言模型自动生成视觉一致但显著多样化的训练数据。一些研究[[53](#bib.bib53), [54](#bib.bib54)] 认为数据增强实际上是一种广泛使用的技术，以增加数据多样性。这些研究为深度学习任务开发了新的数据增强方法。
- en: Large models, such as large language models (LLMs), have made remarkable advancements
    in nearly each AI field. The data quality is crucial for the training or fine-tuning
    of a large model. Therefore, data optimization techniques also prevail in learning
    for large models. Yang et al. [[55](#bib.bib55)] utilized flip operation on the
    training corpus to balance the two-way translation in language pairs in their
    building of a large multilingual translation model. Liu et al. [[56](#bib.bib56)]
    applied adversarial training in both the pre-training and fine-tuning stages.
    Results show that the error rate of the trained model is also reduced.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 大型模型，如大型语言模型（LLMs），在几乎每个AI领域都取得了显著进展。数据质量对大型模型的训练或微调至关重要。因此，数据优化技术在大型模型学习中也很流行。Yang等人[[55](#bib.bib55)]
    在构建大型多语言翻译模型时，对训练语料进行翻转操作，以平衡语言对中的双向翻译。Liu等人[[56](#bib.bib56)] 在预训练和微调阶段都应用了对抗训练。结果表明，训练模型的错误率也降低了。
- en: '![Refer to caption](img/e4fe9a9692c3228fe3376a643fbc125f.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e4fe9a9692c3228fe3376a643fbc125f.png)'
- en: 'Figure 5: The sub-taxonomy for data objects.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：数据对象的子分类。
- en: IV-C Data objects
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 数据对象
- en: IV-C1 Primary objects
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 主要对象
- en: The primary objects of data optimization for deep learning are the training
    data. Some studies optimize raw samples, while some others optimize labels. There
    are also studies focusing on the data transformed by DNNs, e.g., features and
    logits. In Sections VI-B and VI-C, more details will be presented.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的数据优化主要对象是训练数据。一些研究优化原始样本，而另一些则优化标签。还有一些研究关注DNN变换后的数据，例如特征和logits。在第VI-B和VI-C节中，将提供更多细节。
- en: IV-C2 Other objects
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 其他对象
- en: 'There are also numerous studies concerning other data objects in deep learning.
    Fig. [5](#S4.F5 "Figure 5 ‣ IV-B Application scenarios ‣ IV Goals, scenarios,
    and data objects ‣ Data Optimization in Deep Learning: A Survey") lists three
    other data objects, namely, hyper-parameters, initial values, and meta data.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多研究涉及深度学习中的其他数据对象。图[5](#S4.F5 "图5 ‣ IV-B 应用场景 ‣ IV 目标、场景和数据对象 ‣ 深度学习中的数据优化：综述")
    列出了三个其他数据对象，即超参数、初始值和元数据。
- en: Hyper-parameters highly affect the final performance of trained models. They
    are determined either by grid searching in a pre-defined scope or directly being
    set as fixed values. Consequently, setting a proper searching scope or fixed initial
    values is a crucial step in DNN training.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数对训练模型的最终性能有很大影响。它们可以通过在预定义范围内的网格搜索确定，也可以直接设定为固定值。因此，设定合适的搜索范围或固定初始值是DNN训练中的关键步骤。
- en: Network initialization is also important for DNN training. Gaussian distribution-based
    initialization is the primary choice in most learning tasks. Other effective strategies
    are also investigated and applied. Glorot and Bengio [[57](#bib.bib57)] adopted
    a scaled uniform distribution for initialization which is called “Xavier” initialization.
    He et al. [[58](#bib.bib58)] proposed a robust initialization method for rectifier
    nonlinearities, which is called “Kaiming” initialization.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 网络初始化对DNN训练也很重要。基于高斯分布的初始化在大多数学习任务中是主要选择。其他有效策略也被研究和应用。Glorot和Bengio [[57](#bib.bib57)]
    采用了一种称为“Xavier”初始化的缩放均匀分布。He等人[[58](#bib.bib58)] 提出了针对整流非线性的鲁棒初始化方法，称为“Kaiming”初始化。
- en: Meta learning offers a powerful manner to optimize hyper-parameters of independent
    modules in deep learning. It relies on an unbiased meta dataset. Nevertheless,
    in most learning tasks there are no independent high-quality meta data and constructing
    a high-quality unbiased meta dataset is challenging. Su et al. [[59](#bib.bib59)]
    conducted a theoretical analysis for the compiling of a high-quality meta dataset
    from the training set. Four criteria, namely, balance, uncertainty, clean, and
    diversity, are selected in their proposed compiling method.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习提供了一种强大的方法来优化深度学习中独立模块的超参数。它依赖于一个无偏的元数据集。然而，在大多数学习任务中，并不存在独立的高质量元数据，构建高质量无偏的元数据集是具有挑战性的。Su等人[[59](#bib.bib59)]对如何从训练集中编制高质量元数据集进行了理论分析。他们提出的编制方法选择了四个标准，即平衡、不确定性、清洁和多样性。
- en: There are numerous classical studies for the optimization of the three types
    of data which are not mentioned in this study. The placing of these studies into
    data optimization for deep learning may facilitate the further development of
    the optimization for the three types of data. The focus of this survey is the
    training data. Therefore, the following parts will be limited in the scope of
    training data optimization.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究未提及三类数据优化的众多经典研究。将这些研究应用于深度学习的数据优化中，可能会促进三类数据优化的进一步发展。本调查的重点是训练数据。因此，以下部分将局限于训练数据优化的范围。
- en: '![Refer to caption](img/eaae520787f528522abc7841ffbaea80.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eaae520787f528522abc7841ffbaea80.png)'
- en: 'Figure 6: Three main steps in data optimization pipeline.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：数据优化流程的三个主要步骤。
- en: '![Refer to caption](img/65c5d1f6915b54def28a84fe6d667a2b.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/65c5d1f6915b54def28a84fe6d667a2b.png)'
- en: 'Figure 7: The sub-taxonomy for data perception.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：数据感知的子分类。
- en: V Optimization pipeline
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 优化流程
- en: 'The pipeline mainly consists of three steps, namely, data perception, analysis,
    and optimizing, as shown in Fig. [6](#S4.F6 "Figure 6 ‣ IV-C2 Other objects ‣
    IV-C Data objects ‣ IV Goals, scenarios, and data objects ‣ Data Optimization
    in Deep Learning: A Survey"). Some notations and symbols are defined as follows.
    Let $D=\{x_{i},y_{i}\}_{i=1}^{N}$ be a set of $N$ training samples, where $x_{i}$
    is the feature and $y_{i}$ is the label. Let $C$ be the number of categories and
    $N_{c}$ be the number of the samples in the $c$th category in $D$. $\pi_{c}{\rm{}}={\rm{}}N_{c}/N$
    is the proportion of the $c$th category. Let $p_{c}$ and $p(x|y=c)$ be the prior
    and the class conditional probability density for the $c$th class, respectively.
    When there is no ambiguity, $x_{i}$ represents the feature output by the last
    feature encoding layer and $u_{i}$ represents the logit vector output by the Softmax
    layer for $x_{i}$. Let $l$ be the loss for $x$. In this study, the cross-entropy
    loss is assumed and $\Theta$ represents the network parameters.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '该流程主要包括三个步骤，即数据感知、分析和优化，如图 [6](#S4.F6 "Figure 6 ‣ IV-C2 Other objects ‣ IV-C
    Data objects ‣ IV Goals, scenarios, and data objects ‣ Data Optimization in Deep
    Learning: A Survey")所示。以下是一些符号和标记的定义。设$D=\{x_{i},y_{i}\}_{i=1}^{N}$为$N$个训练样本的集合，其中$x_{i}$为特征，$y_{i}$为标签。设$C$为类别数，$N_{c}$为$D$中$c$类的样本数量。$\pi_{c}={\rm{}}N_{c}/N$为$c$类的比例。设$p_{c}$和$p(x|y=c)$分别为$c$类的先验和类别条件概率密度。当没有歧义时，$x_{i}$表示由最后的特征编码层输出的特征，而$u_{i}$表示由Softmax层为$x_{i}$输出的logit向量。设$l$为$x$的损失。在本研究中，假设采用交叉熵损失，$\Theta$表示网络参数。'
- en: V-A Data perception
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 数据感知
- en: In this study, data perception refers to all possible methods aimed at sensing
    and diagnosing the training data to capture the intrinsic data characteristics
    and patterns that affect learning performance. It serves as the first step in
    the pipeline, and an effective data optimization method cannot work well without
    accurate perception of the training corpus
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，数据感知指的是所有旨在感知和诊断训练数据的方法，以捕捉影响学习性能的内在数据特征和模式。这是流程中的第一步，没有对训练语料库的准确感知，任何有效的数据优化方法都无法发挥作用。
- en: 'Generally, data perception for training data quantifies the factors related
    to the true distribution, training data distribution, cleanliness, diversity,
    etc. We construct a sub-taxonomy for data perception in three dimensions as shown
    in Fig. [7](#S4.F7 "Figure 7 ‣ IV-C2 Other objects ‣ IV-C Data objects ‣ IV Goals,
    scenarios, and data objects ‣ Data Optimization in Deep Learning: A Survey").
    First, in terms of quantifying granularity, there are three levels, namely, sample-wise,
    category-wise, and corpus-wise. Secondly, in terms of perception types, there
    are eight divisions, namely, distribution, cleanliness, difficulty, diversity,
    balance, consistency, neighborhood, and valuation. Thirdly, in terms of quantifying
    variation, there are two divisions, namely, static and dynamics. Each of the above
    divisions and partial representative studies are introduced as follows.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，训练数据的感知量化了与真实分布、训练数据分布、清洁度、多样性等相关的因素。我们在三个维度上构建了数据感知的子分类，如图 [7](#S4.F7 "Figure
    7 ‣ IV-C2 Other objects ‣ IV-C Data objects ‣ IV Goals, scenarios, and data objects
    ‣ Data Optimization in Deep Learning: A Survey")所示。首先，在量化粒度方面，有三种级别，即样本级、类别级和语料库级。其次，在感知类型方面，有八个分支，即分布、清洁度、难度、多样性、平衡、一致性、邻域和估价。第三，在量化变化方面，有两个分支，即静态和动态。以上每个分支和部分代表性研究介绍如下。'
- en: V-A1 Perception on different granularity levels
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 不同粒度级别的感知
- en: There are three granularity levels, including sample-wise, category-wise, and
    corpus-wise.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 数据感知有三种粒度级别，包括样本级、类别级和语料库级。
- en: Sample-wise data perception. It denotes that the perceived quantities reflect
    or influence a sample’s positive/negative or trivial/important role in training.
    For example, most noisy-label learning methods employ sample-wise data perception,
    e.g., training loss [[60](#bib.bib60)] and gradient norm [[61](#bib.bib61)], to
    infer the noisy degree of a training sample.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 样本级数据感知。这意味着感知的量反映或影响样本在训练中的正面/负面或琐碎/重要作用。例如，大多数噪声标签学习方法采用样本级数据感知，例如，训练损失 [[60](#bib.bib60)]
    和梯度范数 [[61](#bib.bib61)]，以推断训练样本的噪声程度。
- en: Category-wise data perception. It denotes that the perceived quantities reflect
    or influence a category’s positive/negative or trivial/important role in training.
    In category-wise perception, the learning performance of each category is usually
    monitored to return feedback for the entire scheme [[62](#bib.bib62)]. Therefore,
    the average outcome (e.g., average loss or precision) is also used to infer a
    reasonable category-wise weight in the next training epoch [[63](#bib.bib63),
    [64](#bib.bib64)]. Another popular quantity is the category proportion ($\pi_{c}$)
    for imbalanced learning. Some studies [[65](#bib.bib65)] measure the compactness
    of a category as it reflects the generalization of the features for a category.
    Studies on/leveraging category-wise perception are fewer than sample-wise studies.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 类别级数据感知。这意味着感知的量反映或影响类别在训练中的正面/负面或琐碎/重要作用。在类别级感知中，通常监控每个类别的学习表现，以便对整个方案提供反馈 [[62](#bib.bib62)]。因此，平均结果（例如，平均损失或精度）也被用来推断下一训练轮次中的合理类别级权重 [[63](#bib.bib63),
    [64](#bib.bib64)]。另一个常见的量是用于不平衡学习的类别比例 ($\pi_{c}$)。一些研究 [[65](#bib.bib65)] 通过测量类别的紧凑性来反映类别特征的泛化。研究/利用类别级感知的数量少于样本级研究。
- en: Corpus-wise data perception. It denotes that the perceived quantities reflect
    or influence a training corpus’ positive/negative or trivial/important role in
    training. Limited studies fall into this division. Lin et al. [[66](#bib.bib66)]
    used the query score to measure the utility of a training dataset.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库级数据感知。这意味着感知的量反映或影响训练语料库的正面/负面或琐碎/重要作用。进入这个分支的研究有限。Lin等人 [[66](#bib.bib66)]
    使用查询分数来衡量训练数据集的实用性。
- en: These three levels can be used together to more comprehensively perceive the
    training data [[67](#bib.bib67)].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种级别可以结合使用，以更全面地感知训练数据 [[67](#bib.bib67)]。
- en: V-A2 Perception on different types
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 不同类型的感知
- en: 'The eight quantifying types are introduced as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 八种量化类型介绍如下：
- en: •
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Distribution. This type aims to quantify the true data distribution for a learning
    task and the training data distribution. An effective quantification of these
    two distributions is significantly beneficial for training. Nevertheless, it is
    nearly impossible to obtain a clear picture of them. Therefore, the true distribution
    is usually assumed to conform to several some basic assumptions, such as Gaussian
    distribution for each category [[39](#bib.bib39)]. For the training data distribution,
    some studies [[68](#bib.bib68), [69](#bib.bib69)] apply clustering to deduce the
    intrinsic structure of the training data. These studies concern the global distribution
    of a category. Recently, researchers have investigated local distributions of
    training samples. One typical characteristic is about the neighborhood of each
    training sample. In deep learning on graphs, the distribution of neighborhood
    samples with heterogeneous labels negatively impacts the training or prediction
    for the sample. Wang et al. [[70](#bib.bib70)] defined a label difference index
    to quantify the difference between a node and its neighborhood in a graph as follows:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分布。该类型旨在量化学习任务的真实数据分布和训练数据分布。有效量化这两个分布对于训练非常有益。然而，几乎不可能获得这两个分布的清晰图像。因此，真实分布通常假设符合一些基本假设，例如每个类别的高斯分布
    [[39](#bib.bib39)]。对于训练数据分布，一些研究 [[68](#bib.bib68), [69](#bib.bib69)] 应用聚类来推测训练数据的内在结构。这些研究关注的是一个类别的全局分布。最近，研究人员调查了训练样本的局部分布。一个典型的特征是关于每个训练样本的邻域。在图上的深度学习中，具有异质标签的邻域样本的分布对样本的训练或预测有负面影响。Wang
    等人 [[70](#bib.bib70)] 定义了一个标签差异指标来量化图中节点及其邻域之间的差异，如下所示：
- en: '|  | $LDI(x_{i})=\frac{1}{\sqrt{2}}&#124;&#124;p_{x_{i}}-p_{N_{i}}&#124;&#124;_{2},$
    |  | (1) |'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $LDI(x_{i})=\frac{1}{\sqrt{2}}&#124;&#124;p_{x_{i}}-p_{N_{i}}&#124;&#124;_{2},$
    |  | (1) |'
- en: where $p_{x_{i}}$ and $p_{N_{i}}$ are the category distributions of $x_{i}$
    and its neighborhood $N_{i}$.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $p_{x_{i}}$ 和 $p_{N_{i}}$ 分别是 $x_{i}$ 及其邻域 $N_{i}$ 的类别分布。
- en: •
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cleanliness. This type aims to identify the degree of noise in each training
    sample. This study primarily focuses on label noise, as it garners more attention
    than sample noise. There are numerous metrics for noise measurement. As illustrated
    in Fig. [7](#S4.F7 "Figure 7 ‣ IV-C2 Other objects ‣ IV-C Data objects ‣ IV Goals,
    scenarios, and data objects ‣ Data Optimization in Deep Learning: A Survey"),
    typical measures include loss-based, gradient-based, uncertainty-based, margin-based,
    and multi-training-based techniques. Samples with large losses, gradient norms,
    or uncertainties are more likely to be noisy. In margin-based measures, a small
    margin indicates a high probability of being noise. Huang et al. [[60](#bib.bib60)]
    conducted multiple training procedures to identify noisy labels.'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '清洁度。该类型旨在识别每个训练样本中的噪声程度。本研究主要关注标签噪声，因为它比样本噪声更受到关注。噪声测量有许多指标。如图 [7](#S4.F7 "Figure
    7 ‣ IV-C2 Other objects ‣ IV-C Data objects ‣ IV Goals, scenarios, and data objects
    ‣ Data Optimization in Deep Learning: A Survey") 所示，典型的测量方法包括基于损失、基于梯度、基于不确定性、基于边际和基于多训练的技术。损失、梯度范数或不确定性较大的样本更可能是噪声。在基于边际的度量中，边际较小表明是噪声的概率较高。Huang
    等人 [[60](#bib.bib60)] 进行了多次训练程序以识别噪声标签。'
- en: •
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Difficulty. This type aims to infer the degree of learning difficulty for a
    training sample or a category. The accurate measurement of learning difficulty
    for each training sample is of great importance because several deep learning
    paradigms employ adaptive learning strategies based on the level of learning difficulty.
    For instance, curriculum learning [[71](#bib.bib71)] holds the perspective that
    easy samples should receive more focus in the early training stages, while hard
    samples should be given more attention in the later stages of training. Some other
    studies [[72](#bib.bib72)] hold the opposite perspective that hard samples should
    be prioritized throughout the training procedure. As shown in Fig. [7](#S4.F7
    "Figure 7 ‣ IV-C2 Other objects ‣ IV-C Data objects ‣ IV Goals, scenarios, and
    data objects ‣ Data Optimization in Deep Learning: A Survey"), there are five
    major manners to measure learning difficulty of samples, namely, loss-based, gradient-based,
    uncertainty-based, multi-training-based, and distance-based. Obviously, the measures
    for learning difficulty are quite similar to those for cleanliness. In fact, some
    studies consider that noisy samples are those quite difficult to learn and divide
    samples into easy/medium/hard/noisy. Paul et al. [[73](#bib.bib73)] proposed the
    error l2-norm score to measure difficulty. Zhu et al. [[74](#bib.bib74)] established
    a formal definition for learning difficulty of samples inspired by the bias-variance
    trade-off theorem and proposed a new learning difficulty measures. Sorscher et
    al. [[75](#bib.bib75)] defined the cosine distance of a sample to its nearest
    cluster center as the sample’s difficulty measure and applied it in sample selection.'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '难度。这种类型旨在推测训练样本或类别的学习难度。准确测量每个训练样本的学习难度非常重要，因为一些深度学习范式根据学习难度水平采用自适应学习策略。例如，**课程学习**[[71](#bib.bib71)]的观点是，容易的样本应该在早期训练阶段得到更多关注，而难度大的样本则应在后期训练阶段得到更多关注。一些其他研究[[72](#bib.bib72)]持有相反的观点，即应在整个训练过程中优先考虑难度大的样本。如图[7](#S4.F7
    "Figure 7 ‣ IV-C2 Other objects ‣ IV-C Data objects ‣ IV Goals, scenarios, and
    data objects ‣ Data Optimization in Deep Learning: A Survey")所示，有五种主要的测量样本学习难度的方法，即基于损失的、基于梯度的、基于不确定性的、基于多次训练的和基于距离的。显然，学习难度的度量与清洁度的度量非常相似。事实上，一些研究认为，噪声样本是那些相当难以学习的样本，并将样本划分为容易/中等/困难/有噪声。Paul等人[[73](#bib.bib73)]提出了误差l2-norm分数来测量难度。Zhu等人[[74](#bib.bib74)]根据偏差-方差权衡定理建立了样本学习难度的正式定义，并提出了新的学习难度度量。Sorscher等人[[75](#bib.bib75)]定义了样本到其最近簇中心的余弦距离作为样本的难度度量，并将其应用于样本选择。'
- en: •
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Uncertainty. This type contains two sub-types, namely, aleatory uncertainty
    and epistemic uncertainty [[76](#bib.bib76)]. The former is also called data uncertainty
    and occurs when training samples are imperfect, e.g., noisy. Therefore, the cleanliness
    degree can be used as a measure of data uncertainty [[77](#bib.bib77)]. Epistemic
    uncertainty is also called model uncertainty. It appears when the learning strategy
    is imperfect. Model uncertainty can be calculated based on information entropy
    of the DNN prediction or the variance of multiple predictions output by a DNN
    with the dropout trick [[78](#bib.bib78)].
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不确定性。这种类型包含两个子类型，即**偶然不确定性**和**认识不确定性**[[76](#bib.bib76)]。前者也称为数据不确定性，当训练样本不完美时，例如有噪声时会出现。因此，数据的不洁净程度可以作为数据不确定性的度量[[77](#bib.bib77)]。认识不确定性也称为模型不确定性。当学习策略不完美时会出现。模型不确定性可以基于DNN预测的信息熵或基于使用dropout技巧的DNN的多个预测输出的方差来计算[[78](#bib.bib78)]。
- en: •
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Diversity. This type aims to identify the diversity of a subset of training
    samples. The subset is usually a category. The measurement for subset diversity
    is useful in the design of data augmentation strategy for the subset [[79](#bib.bib79)]
    and data selection [[59](#bib.bib59)]. Friedman and Dieng [[80](#bib.bib80)] leveraged
    the exponential of the Shannon entropy of the eigenvalues of a similarity matrix,
    namely, vendi score to measure diversity. Salimans et al. [[81](#bib.bib81)] utilized
    a pre-trained Inception model to measure diversity called inception score.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多样性。这种类型旨在识别训练样本子集的多样性。子集通常是一个类别。子集多样性的测量在子集的数据增强策略[[79](#bib.bib79)]和数据选择[[59](#bib.bib59)]的设计中非常有用。Friedman和Dieng[[80](#bib.bib80)]利用相似性矩阵特征值的Shannon熵的指数，即**vendi分数**来测量多样性。Salimans等人[[81](#bib.bib81)]使用预训练的Inception模型来测量多样性，这种方法称为**inception分数**。
- en: •
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Balance. This type aims to measure the balance between/within categories. The
    balance between categories belongs to global balance, while that within a category
    belongs to local balance. Global balance can be simply measured by the proportion
    of the training sample of a category. Nevertheless, our previous study [[82](#bib.bib82)]
    reveals that other factors such as variance and distance may also result in serious
    imbalance. Local balance is relatively difficult to measure. Some studies define
    local balance as attribute balance [[68](#bib.bib68)].
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平衡。这个类型旨在衡量类别之间/类别内部的平衡。类别之间的平衡属于全局平衡，而类别内部的平衡属于局部平衡。全局平衡可以通过类别训练样本的比例来简单衡量。然而，我们的前期研究[[82](#bib.bib82)]揭示了其他因素，如方差和距离，也可能导致严重的不平衡。局部平衡相对难以测量。一些研究将局部平衡定义为属性平衡[[68](#bib.bib68)]。
- en: •
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Consistency. This type aims to identify the consistency of the training dynamics
    of a training sample along the temporal or spatial dimensions. In the temporal
    dimension, the variations of the training dynamics between the previous and the
    current epochs are recorded [[83](#bib.bib83)]. In the spatial dimension, the
    differences in the training dynamics between a sample and other samples such as
    neighbors [[84](#bib.bib84)] or samples within the same category are recorded.
    A classical measure called “forgetting” [[85](#bib.bib85)] quantifies the number
    of variations in the prediction between adjacent epochs. Singh et al. [[86](#bib.bib86)]
    investigated class-wise forgetting. Maini et al. [[87](#bib.bib87)] utilized forgetting
    to distinguish among examples that are hard for distinct reasons, such as membership
    in a rare subpopulation, being mislabeled, or belonging to a complex subpopulation.
    Wang et al. [[88](#bib.bib88)] provided a comprehensive summary for sample forgetting
    in learning. Kim et al. [[89](#bib.bib89)] focused on the dynamics of each sample’s
    latent representation and measured the alignment between the latent distributions.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一致性。这个类型旨在识别训练样本在时间或空间维度上的训练动态的一致性。在时间维度上，记录了前一阶段与当前阶段训练动态的变化[[83](#bib.bib83)]。在空间维度上，记录了样本与其他样本（如邻居[[84](#bib.bib84)]）或同一类别内部样本之间的训练动态差异。一种经典的度量方法称为“遗忘”[[85](#bib.bib85)]，用来量化相邻阶段预测的变化次数。Singh等人[[86](#bib.bib86)]研究了类别遗忘。Maini等人[[87](#bib.bib87)]利用遗忘来区分由于不同原因而难以处理的例子，例如稀有子群体的成员、标记错误或属于复杂子群体。Wang等人[[88](#bib.bib88)]提供了关于学习中样本遗忘的全面总结。Kim等人[[89](#bib.bib89)]关注每个样本潜在表示的动态，并测量潜在分布之间的对齐情况。
- en: •
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Valuation. This value is usually measured by the Shapley value, which is a
    concept from the game theory [[90](#bib.bib90)]. Ghorbani and Zou firstly introduced
    Shapley value for data valuation [[91](#bib.bib91)] as follows:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 估值。这个值通常通过**Shapley值**来衡量，这是一种来自博弈论的概念[[90](#bib.bib90)]。Ghorbani和Zou首次引入了Shapley值用于数据估值[[91](#bib.bib91)]，其定义如下：
- en: '|  | $\phi(x_{i})=\sum_{S\in D-x_{i}}\frac{1}{C_{&#124;D&#124;-1}^{&#124;S&#124;}}[V(S\cup\{x_{i}\})-V(S)]$
    |  | (2) |'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\phi(x_{i})=\sum_{S\in D-x_{i}}\frac{1}{C_{&#124;D&#124;-1}^{&#124;S&#124;}}[V(S\cup\{x_{i}\})-V(S)]$
    |  | (2) |'
- en: 'where $V(\cdot)$ is the utility function of a dataset and $S$ is a subset of
    the training corpus $D$. Their values are different when modeling the clean and
    the noisy samples. Nevertheless, the calculation for the Shapley value as shown
    in Eq. ([2](#S5.E2 "In 8th item ‣ V-A2 Perception on different types ‣ V-A Data
    perception ‣ V Optimization pipeline ‣ Data Optimization in Deep Learning: A Survey"))
    is NP-hard, thereby hindering its use in real applications. Yoon et al. [[92](#bib.bib92)]
    proposed a reinforcement learning-based method for data valuation. Their inferred
    weights reflect the importance of a sample in learning, which is not equal to
    the Shapley value. Some other studies [[66](#bib.bib66), [93](#bib.bib93)] proposed
    more practical methods to approximate the Shapley value. Jiang et al. [[94](#bib.bib94)]
    established an easy-to-use and unified framework that facilitates researchers
    and practitioners to apply and compare existing data valuation algorithms. Compared
    with the aforementioned perception quantities such as cleanliness and difficulty,
    the Shapley value has a more solid theoretical basis. Therefore, establishing
    a direct connection among the Shapley value and the quantities listed above deserves
    further study.'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $V(\cdot)$ 是数据集的效用函数，而 $S$ 是训练语料库 $D$ 的一个子集。在建模干净样本和噪声样本时，它们的值是不同的。然而，如公式中所示的
    Shapley 值计算 ([2](#S5.E2 "在第8项 ‣ V-A2 对不同类型的感知 ‣ V-A 数据感知 ‣ V 优化流程 ‣ 深度学习中的数据优化：一项调查"))
    是 NP 难的，这阻碍了其在实际应用中的使用。Yoon 等人 [[92](#bib.bib92)] 提出了基于强化学习的数据估值方法。他们推断的权重反映了样本在学习中的重要性，这与
    Shapley 值不同。一些其他研究 [[66](#bib.bib66), [93](#bib.bib93)] 提出了更实际的方法来近似 Shapley 值。Jiang
    等人 [[94](#bib.bib94)] 建立了一个易于使用和统一的框架，方便研究人员和从业人员应用和比较现有的数据估值算法。与上述感知量如清洁度和难度相比，Shapley
    值具有更为坚实的理论基础。因此，建立 Shapley 值与上述量之间的直接联系值得进一步研究。
- en: This study only lists commonly used measures for data perception. Additionally,
    there are some other important measures which will be explored in our future work.
    For example, the neighborhoods for each training sample may vary as the feature
    encoding network is updated at each epoch. In shallow learning, neighborhood is
    an important information and many classical methods are based on the utilization
    of neighborhood. However, previous deep learning methodologies rarely leverage
    neighborhood information, as the computational complexity for neighborhood identification
    is high. Some studies adopt a simplified manner to construct the neighborhood.
    For example, Bahri and Heinrich Jiang [[95](#bib.bib95)] employed the logit vectors
    rather than the features to construct neighborhood. The dimension of logit vector
    is usually much smaller than the feature dimension, so the computational complexity
    is significantly reduced. It is believable that with the advancement of related
    computational techniques, neighborhood information will receive increasingly attention
    in deep learning. Some other important quantities such as problematic score [[96](#bib.bib96)]
    and data influence [[97](#bib.bib97)] in learning, which have large overlaps with
    the aforementioned quantities, also deserve further exploration.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究仅列出了用于数据感知的常用度量。此外，还有一些其他重要的度量将在我们未来的工作中探讨。例如，每个训练样本的邻域可能会随着特征编码网络在每个 epoch
    更新而有所变化。在浅层学习中，邻域是重要的信息，许多经典方法基于邻域的利用。然而，以前的深度学习方法很少利用邻域信息，因为邻域识别的计算复杂性较高。一些研究采用简化的方法来构造邻域。例如，Bahri
    和 Heinrich Jiang [[95](#bib.bib95)] 使用对数向量而不是特征来构造邻域。对数向量的维度通常远小于特征维度，因此计算复杂性显著降低。可以相信，随着相关计算技术的发展，邻域信息将在深度学习中获得越来越多的关注。其他一些重要的量，如问题分数
    [[96](#bib.bib96)] 和数据影响 [[97](#bib.bib97)]，它们与上述量有较大重叠，也值得进一步探索。
- en: If the perceived quantities are required to fed into a model, hidden representation
    for the raw quantities is favored. In meta learning-based sample weighting or
    perturbation [[98](#bib.bib98)], the weights or perturbation vectors for each
    sample are derived based on the hidden representations of the the perceived quantities.
    For example, Shu et al. [[99](#bib.bib99)] extracted the training loss for each
    sample as the input and fed it into an MLP network containing 100 hidden nodes.
    In other words, the raw training loss is represented by a 100-dimensional hidden
    vector. Zhou et al. [[67](#bib.bib67)] utilized six quantities to perceive the
    character of a training sample, including loss, margin, gradient norm, entropy
    of the Sofxmax prediction, class proportion, and average categorical loss. Likewise,
    these six quantities are also transformed into a 100-dimensional feature vector
    through an MLP network.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要将感知量输入模型，则通常会采用原始量的隐含表示。在基于元学习的样本加权或扰动中[[98](#bib.bib98)]，每个样本的权重或扰动向量是基于感知量的隐含表示得出的。例如，Shu等人[[99](#bib.bib99)]将每个样本的训练损失提取作为输入，并将其输入到包含100个隐含节点的MLP网络中。换句话说，原始训练损失被表示为一个100维的隐含向量。Zhou等人[[67](#bib.bib67)]利用六个量来感知训练样本的特征，包括损失、边际、梯度范数、Softmax预测的熵、类别比例和平均分类损失。同样，这六个量也通过MLP网络转换为100维特征向量。
- en: V-A3 Static and dynamic perception
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A3 静态与动态感知
- en: Static perception denotes that the perceived quantities remain unchanged during
    optimization, whereas dynamic perception denotes that the quantities vary.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 静态感知表示感知量在优化过程中保持不变，而动态感知表示这些量会变化。
- en: In imbalanced learning, category proportion is widely used to quantify a category.
    It belongs to static perception because this quantity remains unchanged. In noisy-label
    learning, many studies adopt a two-stage strategy in which the noisy degree of
    each training sample is measured and the degrees are used in the second training
    stage [[60](#bib.bib60)]. In this two-stage strategy, the perception for label
    noise is static.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在不平衡学习中，类别比例被广泛用于量化一个类别。这属于静态感知，因为该量保持不变。在有噪声标签的学习中，许多研究采用了两阶段策略，其中测量每个训练样本的噪声程度，并在第二阶段中使用这些程度[[60](#bib.bib60)]。在这种两阶段策略中，对标签噪声的感知是静态的。
- en: The impact of a training sample usually varies during training. Therefore, compared
    with static perception, dynamic perception is more prevailing in deep learning
    tasks. Many studies utilize training dynamics of training samples for the successive
    sample weighting or perturbation. Such training dynamics also belong to the dynamic
    perception. The training dynamics including loss, prediction, uncertainty, margin,
    and neighborhood vary at each epoch. For example, self-paced learning [[100](#bib.bib100)]
    determines the weights of each training sample according to their losses in the
    previous epoch and a varied threshold. Therefore, the weight may also vary in
    each epoch.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 训练样本的影响通常在训练过程中会有所变化。因此，与静态感知相比，动态感知在深度学习任务中更为流行。许多研究利用训练样本的训练动态进行后续的样本加权或扰动。这种训练动态也属于动态感知。训练动态包括损失、预测、不确定性、边际和邻域，在每个训练周期中都会变化。例如，自适应学习[[100](#bib.bib100)]根据每个训练样本在前一周期的损失及变化的阈值来确定权重。因此，权重在每个周期中也可能会变化。
- en: V-B Analysis on perceived quantities
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 感知量分析
- en: 'Analysis on perceived quantities contains two manners, namely, statistics and
    modeling, as shown in Fig. [6](#S4.F6 "Figure 6 ‣ IV-C2 Other objects ‣ IV-C Data
    objects ‣ IV Goals, scenarios, and data objects ‣ Data Optimization in Deep Learning:
    A Survey").'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对感知量的分析包括统计和建模两种方式，如图[6](#S4.F6 "图 6 ‣ IV-C2 其他对象 ‣ IV-C 数据对象 ‣ IV 目标、场景和数据对象
    ‣ 深度学习中的数据优化：综述")所示。
- en: '![Refer to caption](img/fb07a52d2cb83face7ea4d71ca12f4f0.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/fb07a52d2cb83face7ea4d71ca12f4f0.png)'
- en: 'Figure 8: The statistics for the forgetting numbers for training samples [[85](#bib.bib85)].'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：训练样本遗忘数量的统计[[85](#bib.bib85)]。
- en: 'Statistical analysis. Most studies employ this manner for the perceived data
    quantities. These studies considered only one or two quantities. For example,
    Toneva et al. [[85](#bib.bib85)] made a statistics for the forgetting numbers
    of training samples as shown in Fig. [8](#S5.F8 "Figure 8 ‣ V-B Analysis on perceived
    quantities ‣ V Optimization pipeline ‣ Data Optimization in Deep Learning: A Survey").
    The left figure shows the distributions of forgetting numbers for clean and noisy
    samples, while the right one shows the distributions of forgetting numbers before
    and after the noise is added. Distinct difference exists between the distributions
    of clean and noisy samples. Huang et al. [[60](#bib.bib60)] proposed a cycle training
    strategy that the model is trained from overfitting to underfitting cyclically.
    The epoch-wise loss for each training sample is recorded. Fig. [9](#S5.F9 "Figure
    9 ‣ V-B Analysis on perceived quantities ‣ V Optimization pipeline ‣ Data Optimization
    in Deep Learning: A Survey") shows the differences between the average losses
    for the noisy and clean samples. Noisy samples have larger training losses. Therefore,
    they leveraged the average loss as an indicator for noisy labels. Zhu et al. [[74](#bib.bib74)]
    proposed a cross validation-based training strategy. Multiple training losses
    are also recorded for each training sample. They revealed that the variance of
    the multiple losses for each sample is also useful in identifying noisy labels
    as shown in Fig. [10](#S5.F10 "Figure 10 ‣ V-B Analysis on perceived quantities
    ‣ V Optimization pipeline ‣ Data Optimization in Deep Learning: A Survey").'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 统计分析。大多数研究采用这种方式处理感知数据量。这些研究只考虑了一两个量。例如，Toneva 等人 [[85](#bib.bib85)] 对训练样本的遗忘次数进行了统计，如图
    [8](#S5.F8 "图 8 ‣ V-B 感知量分析 ‣ V 优化流程 ‣ 深度学习中的数据优化：综述") 所示。左侧图展示了干净样本和噪声样本的遗忘次数分布，而右侧图展示了添加噪声前后的遗忘次数分布。干净样本和噪声样本的分布存在明显差异。Huang
    等人 [[60](#bib.bib60)] 提出了一个周期性训练策略，即模型从过拟合到欠拟合进行循环训练。记录了每个训练样本的每个周期损失。如图 [9](#S5.F9
    "图 9 ‣ V-B 感知量分析 ‣ V 优化流程 ‣ 深度学习中的数据优化：综述") 所示，噪声样本的平均损失较大。因此，他们利用平均损失作为噪声标签的指标。Zhu
    等人 [[74](#bib.bib74)] 提出了基于交叉验证的训练策略。还记录了每个训练样本的多次训练损失。他们揭示了每个样本的多个损失方差在识别噪声标签方面也有用，如图
    [10](#S5.F10 "图 10 ‣ V-B 感知量分析 ‣ V 优化流程 ‣ 深度学习中的数据优化：综述") 所示。
- en: '![Refer to caption](img/306a7f80f473fda2c745f5144649b743.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/306a7f80f473fda2c745f5144649b743.png)'
- en: 'Figure 9: The statistics for loss along the training cycle rounds [[60](#bib.bib60)].'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：训练周期中的损失统计 [[60](#bib.bib60)]。
- en: '![Refer to caption](img/c0e07551408d1eb21bb3932853087e03.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c0e07551408d1eb21bb3932853087e03.png)'
- en: 'Figure 10: The statistics for the mean and the variance of the loss under the
    cross validation-based training. [[74](#bib.bib74)].'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：基于交叉验证训练的损失均值和方差统计 [[74](#bib.bib74)]。
- en: 'Modeling. This manner refers to the statistical modeling on the perceived quantities
    for training data. Arazo et al. [[101](#bib.bib101)] assumed that the traning
    loss conforms to the following distribution:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 建模。这种方式指的是对训练数据的感知量进行统计建模。Arazo 等人 [[101](#bib.bib101)] 假设训练损失符合以下分布：
- en: '|  | $p(l&#124;\alpha,\beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}l^{\alpha-1}(1-l)^{\beta-1}.$
    |  | (3) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(l\mid\alpha,\beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}l^{\alpha-1}(1-l)^{\beta-1}.$
    |  | (3) |'
- en: where $\Gamma(\cdot)$ is the Gamma function; $\alpha$ and $\beta$ are parameters
    to infer. Their values are different when modeling the clean and the noisy samples.
    Hu et al. [[102](#bib.bib102)] leveraged the Weibull mixture distribution to model
    the memorization-forgetting value of each training sample. The mixture distribution
    contains two components which suit for the clean and noisy samples, respectively.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Gamma(\cdot)$ 是 Gamma 函数；$\alpha$ 和 $\beta$ 是待推断的参数。在建模干净样本和噪声样本时，它们的值是不同的。Hu
    等人 [[102](#bib.bib102)] 利用 Weibull 混合分布来建模每个训练样本的记忆-遗忘值。该混合分布包含两个分量，分别适用于干净样本和噪声样本。
- en: Both manners are transparent and thus the entire data optimization approach
    is explainable. Nevertheless, these two divisions usually rely on appropriate
    prior distributions about the involved quantities. If the prior distributions
    are incorrect, the successive optimizing will negatively influence the model training.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方式都是透明的，因此整个数据优化方法是可以解释的。然而，这两种划分通常依赖于关于所涉及量的适当先验分布。如果先验分布不正确，后续的优化将对模型训练产生负面影响。
- en: '![Refer to caption](img/dac6575c1cb8ab9f12b3bd599ae9b3c7.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/dac6575c1cb8ab9f12b3bd599ae9b3c7.png)'
- en: 'Figure 11: The sub-taxonomy of data optimization techniques.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：数据优化技术的子分类。
- en: V-C Optimizing
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 优化
- en: The data perception and analysis act as the pre-processing for data operation.
    This step is the key processing of the entire data optimization pipeline. The
    successive section will introduce current optimization techniques in detail.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 数据感知和分析作为数据操作的预处理步骤。这个步骤是整个数据优化流程中的关键处理环节。接下来的部分将详细介绍当前的优化技术。
- en: VI Data optimization techniques
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 数据优化技术
- en: 'This section describes the most important dimension for the presented taxonomy,
    namely, data optimization techniques for deep learning. Fig. [11](#S5.F11 "Figure
    11 ‣ V-B Analysis on perceived quantities ‣ V Optimization pipeline ‣ Data Optimization
    in Deep Learning: A Survey") presents the sub-taxonomy along this dimension. We
    summarized six sub-divisions for existing data optimization techniques, including
    resampling, augmentation, perturbation, weighting, pruning, and others. It is
    noteworthy that this survey covers numerous technique/methodology divisions and
    leaves a through comparison for them as our future work. The reason lies in two
    folds. First, each division has its own merits and defects and their effectiveness
    have been verified in previous literature, so it is difficulty to judge which
    one is absolutely the best in universal learning tasks. Second, a thorough theoretical
    or empirical comparison is not a trivial task.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '本节描述了所述分类法中最重要的维度，即深度学习的数据优化技术。图 [11](#S5.F11 "Figure 11 ‣ V-B Analysis on
    perceived quantities ‣ V Optimization pipeline ‣ Data Optimization in Deep Learning:
    A Survey") 展示了该维度的子分类。我们总结了现有数据优化技术的六个子类别，包括重采样、数据增强、扰动、加权、剪枝等。值得注意的是，这项调查涵盖了众多技术/方法论的划分，并将它们的比较作为我们未来的工作。原因有两个。首先，每个划分都有其优点和缺点，并且它们的有效性已在以往文献中得到验证，因此很难判断哪一种在通用学习任务中绝对最好。其次，全面的理论或实证比较不是一项简单的任务。'
- en: VI-A Data resampling
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 数据重采样
- en: Data resampling compiles a new training set in which samples are randomly sampled
    from the raw training set. It is widely used in tasks encountering the issues,
    including biased distribution [[103](#bib.bib103)] and redundancy. This study
    summarizes two split dimensions for this division. The first dimension concerns
    the size of the sampled datasets, while the second dimension concerns the sampling
    rates.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 数据重采样编制一个新训练集，其中样本是从原始训练集中随机抽取的。它广泛应用于遇到问题的任务中，包括偏倚分布[[103](#bib.bib103)]和冗余。本研究总结了该划分的两个分维度。第一个维度涉及采样数据集的大小，而第二个维度涉及采样率。
- en: In the first dimension, resampling is divided into undersampling and oversampling.
    undersampling compiles a new training set whose size is smaller than that of the
    raw training set. Contrarily, oversampling compiles a new training set whose size
    is larger than that of the raw training set. Both manners are widely used in previous
    machine learning tasks, including imbalanced learning, bagging, and cost-sensitive
    learning. Meanwhile, tremendous theoretical studies have been conducted to explain
    the effectiveness of these two manners in both the statistics and the machine
    learning communities. Nevertheless, there is currently no consensus on which manner
    is more effective. Some studies concluded that undersampling should be the primary
    choice when dealing with imbalanced datasets [[104](#bib.bib104)]. However, some
    other studies hold the opposite view [[105](#bib.bib105)].
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个维度上，重采样被分为欠采样和过采样。欠采样编制一个新训练集，其大小小于原始训练集。相反，过采样编制一个新训练集，其大小大于原始训练集。这两种方法在以往的机器学习任务中广泛应用，包括不平衡学习、集成学习和成本敏感学习。同时，理论研究也大量开展，以解释这两种方法在统计学和机器学习社区中的有效性。然而，目前尚未就哪种方法更有效达成共识。一些研究得出结论，当处理不平衡数据集时，欠采样应作为主要选择[[104](#bib.bib104)]。然而，也有一些研究持相反观点[[105](#bib.bib105)]。
- en: 'In the second dimension, resampling is divided into uniform, proportion-based,
    importance-based, learning difficulty-based, and uncertainty-based. They are detailed
    as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二维度中，重采样分为均匀、基于比例、基于重要性、基于学习难度和基于不确定性。它们的详细说明如下：
- en: •
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Uniform sampling. This manner is quite intuitive. It treats samples definitely
    equal regardless of their distributions, location, categories, and training performances.
    In fact, in nearly all existing deep learning tasks, the batch is constructed
    by uniformly sampling from the training corpus. Kirsch et al. [[106](#bib.bib106)]
    claimed that the independent selection of a batch of samples leads to data inefficiency
    due to the correlations between samples. Some studies explore alternative sampling
    strategies. For example, Loshchilov and Hutter [[42](#bib.bib42)] proposed a rank-based
    batch selection strategy according to the training loss in previous epochs and
    samples with large losses have high probabilities to be sampled. Experiments reveal
    that this new strategy accelerates the training speed by a factor of five.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 均匀采样。这种方式非常直观。它将样本视为完全相等，无论它们的分布、位置、类别和训练表现如何。实际上，在几乎所有现有的深度学习任务中，批次都是通过从训练语料库中均匀采样构建的。Kirsch
    等人 [[106](#bib.bib106)] 声称，由于样本之间的相关性，独立选择一批样本会导致数据效率低下。一些研究探索了替代采样策略。例如，Loshchilov
    和 Hutter [[42](#bib.bib42)] 提出了基于前几轮训练损失的排名批次选择策略，损失大的样本具有较高的采样概率。实验表明，这种新策略将训练速度提高了五倍。
- en: •
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Proportion-based sampling. This manner simply assigns the total sampling rate
    for each category with its proportion ($\pi_{c}$) in the corpus. It is mainly
    used in imbalanced learning in which the minor categories are assigned with large
    sampling rates [[15](#bib.bib15)].
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于比例的采样。这种方式简单地根据每个类别在语料库中的比例 ($\pi_{c}$) 分配总的采样率。主要用于不平衡学习中，其中少数类别被分配较大的采样率
    [[15](#bib.bib15)]。
- en: •
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Importance-based sampling. This manner assigns sampling probabilities according
    to samples’ importance. In this study, the definition for importance sampling
    follows several classical studies [[107](#bib.bib107)] [[108](#bib.bib108)]. Given
    a target distribution $q(x,y)$ and a source distribution on training data $p(x,y)$,
    the importance (sampling rate) for a training sample $\{x,y\}$ in importance sampling
    is defined as
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于重要性的采样。这种方式根据样本的重要性分配采样概率。在本研究中，重要性采样的定义遵循了几项经典研究 [[107](#bib.bib107)] [[108](#bib.bib108)]。给定一个目标分布
    $q(x,y)$ 和一个训练数据上的源分布 $p(x,y)$，在重要性采样中，训练样本 $\{x,y\}$ 的重要性（采样率）定义为
- en: '|  | $w(x)=\frac{q(x,y)}{p(x,y)}.$ |  | (4) |'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $w(x)=\frac{q(x,y)}{p(x,y)}.$ |  | (4) |'
- en: 'As the target distribution is unknown, some studies [[109](#bib.bib109)] utilize
    the kernel trick to generate sampling rates. In some importance sampling studies,
    the sampling rates are not based on the probability density ration as presented
    in Eq. ([4](#S6.E4 "In 3rd item ‣ VI-A Data resampling ‣ VI Data optimization
    techniques ‣ Data Optimization in Deep Learning: A Survey")). For example, Atharopoulos
    and Fleuret [[110](#bib.bib110)] took the gradient norms of each training sample
    as their importance. These methods actually belong to learning difficulty-based
    sampling.'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于目标分布未知，一些研究 [[109](#bib.bib109)] 利用核技巧生成采样率。在一些重要性采样研究中，采样率不是基于概率密度比，如等式 ([4](#S6.E4
    "在第3项 ‣ VI-A 数据重采样 ‣ VI 数据优化技术 ‣ 深度学习中的数据优化：综述")) 所示。例如，Atharopoulos 和 Fleuret
    [[110](#bib.bib110)] 将每个训练样本的梯度范数作为其重要性。这些方法实际上属于基于学习难度的采样。
- en: •
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Learning difficulty-based sampling. This manner assigns sampling rates according
    to samples’ learning difficulties. As summarized in Section V-A2, learning difficulty
    is usually measured by loss and gradient norm. For instance, Li et al. [[61](#bib.bib61)]
    applied the gradient norm of logit vectors as the difficulty measurement. Similar
    with the study conducted by Atharopoulos and Fleuret [[110](#bib.bib110)], Johnson
    and Guestrin [[111](#bib.bib111)] proposed the O-SGD sampling method with the
    following sampling rate:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于学习难度的采样。这种方式根据样本的学习难度分配采样率。如第 V-A2 节总结所述，学习难度通常通过损失和梯度范数来衡量。例如，Li 等人 [[61](#bib.bib61)]
    将对数向量的梯度范数作为难度测量。与 Atharopoulos 和 Fleuret [[110](#bib.bib110)] 进行的研究类似，Johnson
    和 Guestrin [[111](#bib.bib111)] 提出了 O-SGD 采样方法，其采样率如下：
- en: '|  | $w(x,y)=\frac{&#124;&#124;\nabla l(x,y)&#124;&#124;}{\sum_{x}&#124;&#124;\nabla
    l(x,y)&#124;&#124;}.$ |  | (5) |'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $w(x,y)=\frac{&#124;&#124;\nabla l(x,y)&#124;&#124;}{\sum_{x}&#124;&#124;\nabla
    l(x,y)&#124;&#124;}.$ |  | (5) |'
- en: They claimed that this “importance sampling” can reduce the stochastic gradient’s
    variance and thus accelerate the training speed. Jiang et al. [[112](#bib.bib112)]
    introduced a selective back propagation strategy, in which samples with large
    losses have relative large probabilities to be selected in back propagation. Gui
    et al. [[113](#bib.bib113)] utilized sampling strategy for noisy-label learning.
    They calculated the sampling weights based on the mean loss of each example along
    the training process. The training samples with large mean losses are assigned
    low weights. Liu et al. [[114](#bib.bib114)] proposed adaptive data sampling,
    in which the sampling rates of the samples correctly classified in previous epochs
    are reduced. Xu et al. [[115](#bib.bib115)] conducted a theoretical analysis and
    concluded that inverse margin-based sampling may accelerate gradient descent in
    finite-step optimization by matching weights with the inverse margin.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 他们声称这种“重要性采样”可以减少随机梯度的方差，从而加快训练速度。Jiang等[[112](#bib.bib112)]引入了一种选择性反向传播策略，其中损失较大的样本在反向传播中具有相对较大的选择概率。Gui等[[113](#bib.bib113)]利用采样策略进行噪声标签学习。他们根据训练过程中每个样本的平均损失计算采样权重。平均损失大的训练样本被分配低权重。Liu等[[114](#bib.bib114)]提出了自适应数据采样，其中在前期正确分类的样本的采样率被降低。Xu等[[115](#bib.bib115)]进行了理论分析，并得出结论：基于逆边际的采样可能通过将权重与逆边际匹配来加速有限步骤优化中的梯度下降。
- en: •
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Uncertainty-based sampling. This manner assigns sampling rates according to
    samples’ uncertainties. It is widely used in active learning, in which a subset
    of data is sampled for human labeling [[116](#bib.bib116), [117](#bib.bib117)].
    Aljuhani et al. [[118](#bib.bib118)] presented an uncertainty-aware sampling framework
    for robust histopathology image analysis. The uncertainty is calculated by predictive
    entropy.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于不确定性的采样。这种方式根据样本的不确定性分配采样率。它广泛应用于主动学习中，其中数据的一个子集被采样进行人工标注[[116](#bib.bib116),
    [117](#bib.bib117)]。Aljuhani等[[118](#bib.bib118)]提出了一种不确定性感知的采样框架，用于鲁棒的组织病理学图像分析。不确定性是通过预测熵计算的。
- en: There are also some other sampling manners. For instance, Ting and Brochu [[119](#bib.bib119)]
    calculated the sample influence for optimal data sampling. Li and Vasconcelos [[120](#bib.bib120)]
    proposed the adversarial sampling to improve OOD detection performance of an image
    classifier. In their adversarial sampling, the sampling weights are pursued by
    maximizing the OOD loss. Wang and Wang [[121](#bib.bib121)] sampled sentences
    according to their semantic characteristics. Zhang et al. [[122](#bib.bib122)]
    sampled training data of the majority categories by considering the samples’ sensitivities.
    Samples with low sensitivities may be noisy or safe ones, while those with high
    sensitivities are borderline ones. Sun et al. [[123](#bib.bib123)] explored an
    automatic scheme for effective data resampling.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他的采样方式。例如，Ting和Brochu[[119](#bib.bib119)]计算了样本影响，以实现最佳数据采样。Li和Vasconcelos[[120](#bib.bib120)]提出了对抗性采样，以提高图像分类器的OOD检测性能。在他们的对抗性采样中，通过最大化OOD损失来追求采样权重。Wang和Wang[[121](#bib.bib121)]根据句子的语义特征进行采样。Zhang等[[122](#bib.bib122)]通过考虑样本的敏感性对多数类别的训练数据进行采样。低敏感性的样本可能是噪声或安全样本，而高敏感性的样本是边界样本。Sun等[[123](#bib.bib123)]探索了一种有效的数据重采样自动方案。
- en: VI-B Data augmentation
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 数据增强
- en: 'Data augmentation compiles a new training set in which samples (or features)
    are generated based on the raw training set or sometimes other relevant sets.
    It is a powerful tool to improve the generalization capability [[124](#bib.bib124),
    [125](#bib.bib125)] and even adversarial robustness [[126](#bib.bib126), [127](#bib.bib127)]
    of DNNs. Illuminated by related surveys on data augmentation [[128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131)], two split dimensions
    are considered, namely, sample/feature and explicit/implicit, as shown in Fig. [11](#S5.F11
    "Figure 11 ‣ V-B Analysis on perceived quantities ‣ V Optimization pipeline ‣
    Data Optimization in Deep Learning: A Survey").'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '数据增强编译了一个新的训练集，其中样本（或特征）是基于原始训练集或有时其他相关集合生成的。这是一个强大的工具，用于提高深度神经网络（DNN）的泛化能力[[124](#bib.bib124),
    [125](#bib.bib125)]，甚至对抗鲁棒性[[126](#bib.bib126), [127](#bib.bib127)]。通过相关的调查研究[[128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131)]，可以考虑两个拆分维度，即样本/特征和显式/隐式，如图[11](#S5.F11
    "Figure 11 ‣ V-B Analysis on perceived quantities ‣ V Optimization pipeline ‣
    Data Optimization in Deep Learning: A Survey")所示。'
- en: VI-B1 Sample/feature augmentation
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B1 样本/特征增强
- en: In sample augmentation, the new training set consists of generated new samples,
    while in feature augmentation, the new training set consists of generated new
    features.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在样本增强中，新训练集由生成的新样本组成，而在特征增强中，新训练集由生成的新特征组成。
- en: Sample augmentation. This division is subject to data types (e.g., image, text,
    or others). For image corpus, augmentation methods adopt noise adding, color transformation,
    geometric transformation, or other basic operations such as cropping to augment
    new images [[129](#bib.bib129)]. For texts, new samples can be generated by noise
    adding, paraphrasing, or other basic operations such as word swapping [[132](#bib.bib132)].
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 样本增强。这一分类依据数据类型（例如，图像、文本或其他）进行。对于图像数据集，增强方法采用噪声添加、颜色变换、几何变换或其他基本操作，如裁剪，以增强新图像[[129](#bib.bib129)]。对于文本，可以通过噪声添加、意译或其他基本操作，如词汇替换，生成新样本[[132](#bib.bib132)]。
- en: Feature augmentation. This division is performed on the feature space, so learning
    tasks for different data types may utilize the same or similar augmentation strategies.
    Some intuitive feature augmentation methods include adding noise, interpolating,
    or extrapolating [[133](#bib.bib133)], which are applicable for general data types,
    including both image and text data. Li et al. [[134](#bib.bib134)] revealed that
    the simply perturbing the feature embedding with Gaussian noise in training leads
    to comparable domain-generalization performance compared with the SOTA methods.
    Ye et al. [[135](#bib.bib135)] proposed novel domain-agnostic augmentation strategies
    on feature space. Cui et al. [[136](#bib.bib136)] decomposed features into the
    class-generic and the class-specific components. They generated samples by combining
    these two components for minor categories. A classical robust learning paradigm,
    namely, adversarial training, is actually a feature-wise augmentation strategy
    when it is run on the feature space [[137](#bib.bib137), [138](#bib.bib138), [139](#bib.bib139)].
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 特征增强。这一分类在特征空间进行，因此不同数据类型的学习任务可能会采用相同或类似的增强策略。一些直观的特征增强方法包括添加噪声、插值或外推[[133](#bib.bib133)]，这些方法适用于包括图像和文本数据在内的一般数据类型。Li
    等人[[134](#bib.bib134)]揭示了在训练中用高斯噪声简单扰动特征嵌入会导致与最先进的方法相当的领域泛化性能。Ye 等人[[135](#bib.bib135)]提出了在特征空间上的新颖领域无关增强策略。Cui
    等人[[136](#bib.bib136)]将特征分解为类通用组件和类特定组件。他们通过将这两个组件组合生成少数类别的样本。一种经典的鲁棒学习范式，即对抗训练，实际上是一种在特征空间上运行的特征级增强策略[[137](#bib.bib137),
    [138](#bib.bib138), [139](#bib.bib139)]。
- en: Some studies augment other data targets such as label and gradient. For example,
    Lee et al. [[140](#bib.bib140)] rotated a training image and the rotation angle
    is also used as supervised information. Elezi et al. [[141](#bib.bib141)] proposed
    a transductive label augmentation method to generate labels for unlabeled large
    set using graph transduction techniques. Some other studies [[142](#bib.bib142)]
    investigated gradient augmentation. Compared with sample/feature augmentation,
    label/gradient augmentation receives quite limited attention.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究增强了其他数据目标，例如标签和梯度。例如，Lee 等人[[140](#bib.bib140)]对训练图像进行了旋转，并将旋转角度作为监督信息使用。Elezi
    等人[[141](#bib.bib141)]提出了一种传导标签增强方法，利用图形传导技术为未标记的大型数据集生成标签。还有一些研究[[142](#bib.bib142)]探讨了梯度增强。与样本/特征增强相比，标签/梯度增强受到的关注相对较少。
- en: VI-B2 Explicit/implicit augmentation
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B2 显式/隐式增强
- en: Explicit augmentation directly generates new samples/features. Meanwhile, implicit
    augmentation conducts data augmentation only theoretically yet do not generate
    any new samples/features actually.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 显式增强直接生成新样本/特征。与此同时，隐式增强仅在理论上进行数据增强，但实际上并未生成任何新样本/特征。
- en: 'Explicit augmentation. According to the employed techniques, existing explicit
    data augmentation can be divided into basic operation, model-based (GAN, diffusion
    model), loss-optimization-based, and automatic augmentation, as described in Fig. [11](#S5.F11
    "Figure 11 ‣ V-B Analysis on perceived quantities ‣ V Optimization pipeline ‣
    Data Optimization in Deep Learning: A Survey"). They are introduced as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 显式增强。根据采用的技术，现有的显式数据增强可以分为基本操作、基于模型的（GAN、扩散模型）、基于损失优化的和自动增强，如图[11](#S5.F11 "图
    11 ‣ V-B 量的感知分析 ‣ V 优化流程 ‣ 深度学习中的数据优化：综述")所述。它们的介绍如下：
- en: •
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Basic operations. This technique is widely used in practical learning tasks
    as basic operations conform to human intuitions. The popular deep learning platforms
    such as pyTorch provide several common basic operations such as cropping, rotation,
    replacement, masking, cutout, etc. One of the most popular data augmentation method
    used for shallow learning tasks, namely, SMOTE [[143](#bib.bib143)] has been utilized
    in deep learning tasks [[144](#bib.bib144)]. Dablain et al. [[145](#bib.bib145)]
    designed more sophisticated improvement for SMOTE for deep learning tasks. Among
    the basic operations, mixup is a simple yet quite effective augmentation manner [[12](#bib.bib12),
    [146](#bib.bib146)]. It generates a new sample with a new label that does not
    belong to the raw label space.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基本操作。由于基本操作符合人类直觉，这种技术在实际学习任务中被广泛使用。流行的深度学习平台如pyTorch提供了几种常见的基本操作，如裁剪、旋转、替换、遮罩、切割等。最受欢迎的浅层学习任务数据增强方法之一，即SMOTE [[143](#bib.bib143)]，已被应用于深度学习任务 [[144](#bib.bib144)]。Dablain等人 [[145](#bib.bib145)]
    为深度学习任务设计了更复杂的SMOTE改进方法。在基本操作中，mixup是一种简单但非常有效的增强方式 [[12](#bib.bib12), [146](#bib.bib146)]。它生成一个新样本，并赋予一个不属于原始标签空间的新标签。
- en: •
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model-based augmentation. This technique generates new samples by leveraging
    independent models. There are three main schemes:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于模型的增强。这种技术通过利用独立模型生成新样本。主要有三种方案：
- en: ①
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ①
- en: GAN-based scheme. Generative adversarial network (GAN) trains a generative model
    and a discriminative model simultaneously in a well designed two-player min-max
    game [[147](#bib.bib147)]. The trained generative model can be used to generate
    new samples conforming to the distribution of the involved training data. A large
    number of variations have been designed in the previous literature [[148](#bib.bib148)].
    Mariani et al. [[149](#bib.bib149)] proposed balancing GAN for imbalanced learning
    tasks. Huang et al. [[150](#bib.bib150)] developed AugGAN for the data augmentation
    in cross domain adaptation. Yang et al. [[151](#bib.bib151)] investigated the
    GAN-based augmentation for time series.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于GAN的方案。生成对抗网络（GAN）在精心设计的双玩家最小-最大博弈中同时训练生成模型和判别模型 [[147](#bib.bib147)]。训练好的生成模型可以用于生成符合参与训练数据分布的新样本。以前的文献中设计了大量的变体 [[148](#bib.bib148)]。Mariani等人 [[149](#bib.bib149)]
    提出了用于不平衡学习任务的平衡GAN。黄等人 [[150](#bib.bib150)] 开发了AugGAN用于跨域适应中的数据增强。杨等人 [[151](#bib.bib151)]
    研究了基于GAN的时间序列增强。
- en: ②
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ②
- en: Diffusion model-based scheme. Diffusion models are a new class of generative
    models and achieve SOTA performance in many applications [[152](#bib.bib152)].
    Xiao et al. [[153](#bib.bib153)] leveraged a text-to-image stable diffusion model
    to expand the training set. Dunlap et al. [[52](#bib.bib52)] utilized large vision
    and language models to automatically generate natural language descriptions of
    a dataset’s domains and augment the training data via language-guided image editing.
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于扩散模型的方案。扩散模型是一类新的生成模型，在许多应用中取得了**SOTA**表现 [[152](#bib.bib152)]。肖等人 [[153](#bib.bib153)]
    利用文本到图像的稳定扩散模型来扩展训练集。邓拉普等人 [[52](#bib.bib52)] 利用大型视觉和语言模型自动生成数据集领域的自然语言描述，并通过语言引导的图像编辑来增强训练数据。
- en: ③
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ③
- en: Bi-transformation-based scheme. This scheme usually relies on two transformation
    models. The first model transforms a training sample into a new type of data.
    The second model transforms the new type of data into a new sample. In natural
    language processing (NLP), back-translation is a popular data augmentation technique [[154](#bib.bib154)],
    which translates the raw text sample into new texts in another language and back
    translates the new texts into a new sample in the same language with the raw sample.
    Dong et al. [[155](#bib.bib155)] proposed a new augmentation technique called
    Image-Text-Image (I2T2I) which integrates text-to-image and image-to-text (image
    captioning) models. There are also augmentation attempts about Text-Image-Text (T2I2T) [[156](#bib.bib156)]
    and Text-Text-Image (T2T2I) [[157](#bib.bib157)]. Theoretically, tri-transformation-based
    augmentation may be also applicable. We leave it our future work.
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 双重变换方案。该方案通常依赖于两个变换模型。第一个模型将训练样本转换为一种新的数据类型。第二个模型将这种新数据类型转换为新的样本。在自然语言处理（NLP）中，回译是一种流行的数据增强技术
    [[154](#bib.bib154)]，它将原始文本样本翻译成另一种语言的新文本，然后将这些新文本回译成与原始样本相同语言的新样本。Dong 等人 [[155](#bib.bib155)]
    提出了一个新的增强技术，称为图像-文本-图像（I2T2I），它集成了文本到图像和图像到文本（图像描述）模型。还有关于文本-图像-文本（T2I2T） [[156](#bib.bib156)]
    和文本-文本-图像（T2T2I） [[157](#bib.bib157)] 的增强尝试。从理论上讲，三重变换基于的增强可能也适用。我们将其留待未来的工作。
- en: •
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Loss-optimization-based augmentation. This manner generates new sample/features
    by minimizing or maximizing a defined loss with heuristic or theoretical inspirations.
    Adversarial training is a typical loss-optimization-based manner. It generates
    a new sample for $\boldsymbol{x}$ by solving the following optimization problem:'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于损失优化的增强。这种方式通过最小化或最大化定义的损失来生成新的样本/特征，具有启发性或理论性的灵感。对抗训练是一种典型的基于损失优化的方式。它通过解决以下优化问题来为
    $\boldsymbol{x}$ 生成新的样本：
- en: '|  | $\vspace{-0.03in}{\boldsymbol{x}_{\text{adv}}}=\boldsymbol{x}+\arg\mathop{\max}\limits_{\left\&#124;\boldsymbol{\delta}\right\&#124;\leq\epsilon}\ell(f(\boldsymbol{x}+\boldsymbol{\delta}),y),\vspace{-0.03in}$
    |  | (6) |'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\vspace{-0.03in}{\boldsymbol{x}_{\text{adv}}}=\boldsymbol{x}+\arg\mathop{\max}\limits_{\left\|\boldsymbol{\delta}\right\|\leq\epsilon}\ell(f(\boldsymbol{x}+\boldsymbol{\delta}),y),\vspace{-0.03in}$
    |  | (6) |'
- en: 'where $\boldsymbol{\delta}$ and $\epsilon$ are the perturbation term and bound,
    respectively. Zhou et al. [[67](#bib.bib67)] proposed anti-adversaries by solving
    the following optimization problem:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{\delta}$ 和 $\epsilon$ 分别是扰动项和界限。Zhou 等人 [[67](#bib.bib67)] 通过解决以下优化问题提出了抗对抗性的方法：
- en: '|  | $\vspace{-0.03in}{\boldsymbol{x}_{\text{anti-adv}}}=\boldsymbol{x}+\arg\mathop{\min}\limits_{\left\&#124;\boldsymbol{\delta}\right\&#124;\leq\epsilon}\ell(f(\boldsymbol{x}+\boldsymbol{\delta}),y).\vspace{-0.03in}$
    |  | (7) |'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\vspace{-0.03in}{\boldsymbol{x}_{\text{anti-adv}}}=\boldsymbol{x}+\arg\mathop{\min}\limits_{\left\|\boldsymbol{\delta}\right\|\leq\epsilon}\ell(f(\boldsymbol{x}+\boldsymbol{\delta}),y).\vspace{-0.03in}$
    |  | (7) |'
- en: Pagliardini et al. [[158](#bib.bib158)] obtained new samples by maximizing an
    uncertainty-based loss.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pagliardini 等人 [[158](#bib.bib158)] 通过最大化基于不确定性的损失获得了新的样本。
- en: •
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Automatic augmentation. This manner investigates automated data augmentation
    techniques [[159](#bib.bib159)] based on meta learning [[160](#bib.bib160)] or
    reinforcement learning [[161](#bib.bib161)]. Nishi et al. [[162](#bib.bib162)]
    proposed new automated data augmentation method and validated its usefulness in
    noisy-label learning. Some studies focus on differentiable automatic data augmentation
    which can dramatically reduces the computational complexity of existing methods [[163](#bib.bib163)].
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动增强。这种方式研究了基于元学习 [[160](#bib.bib160)] 或强化学习 [[161](#bib.bib161)] 的自动数据增强技术
    [[159](#bib.bib159)]。Nishi 等人 [[162](#bib.bib162)] 提出了新的自动数据增强方法，并验证了其在噪声标签学习中的有效性。一些研究集中在可微分的自动数据增强上，这可以显著减少现有方法的计算复杂性
    [[163](#bib.bib163)]。
- en: Implicit augmentation. Wang et al. [[39](#bib.bib39)] proposed the first implicit
    augmentation method called ISDA. It establishes a Gaussian distribution $\mathcal{N}(\boldsymbol{\mu}_{y},\Sigma_{y})$
    for each category. New samples can be generated (i.e., sampled) from its corresponding
    Gaussian distribution. An upper bound of the loss with augmented samples can then
    be derived when the number of generated samples for each training sample approaches
    to infinity. Finally, the upper bound of the loss is used for the final training
    loss. There are several variations for ISDA, such as IRDA [[164](#bib.bib164)]
    and ICDA [[165](#bib.bib165)]. Li et al. [[166](#bib.bib166)] also proposed an
    implicit data augmentation approach mainly based on heuristic inspirations.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式增强。Wang 等人 [[39](#bib.bib39)] 提出了第一个隐式增强方法 ISDA。它为每个类别建立一个高斯分布 $\mathcal{N}(\boldsymbol{\mu}_{y},\Sigma_{y})$。可以从其对应的高斯分布中生成（即采样）新的样本。当每个训练样本生成的样本数量接近无限时，可以推导出增强样本的损失上界。最后，损失的上界用于最终的训练损失。ISDA
    有几个变体，如 IRDA [[164](#bib.bib164)] 和 ICDA [[165](#bib.bib165)]。Li 等人 [[166](#bib.bib166)]
    还提出了一种主要基于启发式灵感的隐式数据增强方法。
- en: Explicit augmentation is the primary choice in data augmentation tasks. Nevertheless,
    implicit augmentation is more efficient than explicit augmentation as it does
    not actually generate new samples or features. There are also theoretical studies
    about data augmentation. A mainstream perspective is that data augmentation performs
    regularization in training [[167](#bib.bib167), [168](#bib.bib168), [169](#bib.bib169),
    [170](#bib.bib170)]. Chen et al. [[171](#bib.bib171)] conducted a probabilistic
    analysis and concluded that data augmentation can result in variance reduction
    and thus prevent overfitting according to the bias-variance theory. In fact, the
    regularizer in machine learning also reduces model variance.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 显式增强是数据增强任务的主要选择。然而，隐式增强比显式增强更有效，因为它实际上不会生成新的样本或特征。关于数据增强也有一些理论研究。一种主流观点认为，数据增强在训练中执行正则化
    [[167](#bib.bib167), [168](#bib.bib168), [169](#bib.bib169), [170](#bib.bib170)]。Chen
    等人 [[171](#bib.bib171)] 进行了概率分析，并得出结论，数据增强可以减少方差，从而防止过拟合，这符合偏差-方差理论。事实上，机器学习中的正则化器也减少了模型方差。
- en: VI-C Data perturbation
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 数据扰动
- en: 'Given a datum ${x}$ ($x$ can be the raw sample, feature, logit, label, or others),
    data perturbation will generate a perturbation $\triangle x$ such that $x^{\prime}=x+\triangle
    x$ can replace $x$ or be used as a new datum. Therefore, some data augmentation
    methods, such as adversarial perturbation, cropping, and masking, can also be
    viewed as data perturbation. In our previous work [[13](#bib.bib13)], we constructed
    a taxonomy for compensation learning, which is actually learning with perturbation.
    This study follows our previous taxonomy in [[13](#bib.bib13)] with slight improvements.
    The sub-taxonomy for data perturbation is presented in Fig. [11](#S5.F11 "Figure
    11 ‣ V-B Analysis on perceived quantities ‣ V Optimization pipeline ‣ Data Optimization
    in Deep Learning: A Survey"). Four split dimensions are considered, namely, target,
    direction, granularity, and assignment manner.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '给定一个数据 ${x}$（$x$ 可以是原始样本、特征、logit、标签或其他），数据扰动将生成一个扰动 $\triangle x$，使得 $x^{\prime}=x+\triangle
    x$ 可以替代 $x$ 或用作新数据。因此，一些数据增强方法，如对抗扰动、裁剪和遮罩，也可以视为数据扰动。在我们之前的工作 [[13](#bib.bib13)]
    中，我们构建了一个补偿学习的分类法，这实际上就是带有扰动的学习。本研究在 [[13](#bib.bib13)] 的基础上做了稍微的改进。数据扰动的子分类如图
    [11](#S5.F11 "Figure 11 ‣ V-B Analysis on perceived quantities ‣ V Optimization
    pipeline ‣ Data Optimization in Deep Learning: A Survey") 所示。考虑了四个划分维度，即目标、方向、粒度和分配方式。'
- en: VI-C1 Perturbation target
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C1 扰动目标
- en: The perturbation targets can be raw sample, feature, logit vector, label, and
    gradient.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 扰动目标可以是原始样本、特征、logit 向量、标签和梯度。
- en: •
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sample perturbation. This division adds the perturbation directly to the raw
    samples. The basic operations in data augmentation can be placed into this division.
    For instance, noise addition and masking used in image classification actually
    exert a small perturbation on the raw image.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 样本扰动。此划分直接将扰动添加到原始样本中。数据增强中的基本操作可以归入此划分。例如，图像分类中使用的噪声添加和遮罩实际上对原始图像施加了小的扰动。
- en: •
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Feature perturbation. This division adds the perturbation on the hidden features.
    Jeddi et al. [[172](#bib.bib172)] perturbed the feature space at each layer to
    increase uncertainty in the network. Their perturbation conforms to the Gaussian
    distribution. Shu et al. [[173](#bib.bib173)] designed a single network layer
    that can generate worst-case feature perturbations during training to improve
    the robustness of DNNs.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征扰动。该类别在隐藏特征上添加扰动。Jeddi 等人[[172](#bib.bib172)] 在每一层的特征空间中添加扰动，以增加网络的不确定性。他们的扰动符合高斯分布。Shu
    等人[[173](#bib.bib173)] 设计了一个单独的网络层，在训练过程中可以生成最坏情况下的特征扰动，以提高深度神经网络的鲁棒性。
- en: •
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Logit perturbation. This division adds the perturbation on the logit vectors
    in the involved DNNs. Li et al. [[47](#bib.bib47)] analyzed several classical
    learning methods such as logit adjustment [[174](#bib.bib174)], LDAM [[175](#bib.bib175)],
    and ISDA [[39](#bib.bib39)] in a unified logit perturbation viewpoint. They proposed
    a new logit perturbation method and extended it to the multi-label learning tasks [[176](#bib.bib176)].
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Logit 扰动。该类别在涉及的深度神经网络中的 logit 向量上添加扰动。Li 等人[[47](#bib.bib47)] 从统一的 logit 扰动角度分析了几种经典的学习方法，如
    logit 调整[[174](#bib.bib174)]、LDAM[[175](#bib.bib175)] 和 ISDA[[39](#bib.bib39)]。他们提出了一种新的
    logit 扰动方法，并将其扩展到多标签学习任务中[[176](#bib.bib176)]。
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Label perturbation. This division adds the perturbation on either the ground-truth
    label of the predicted label. One classical learning skill, namely, label smoothing [[177](#bib.bib177)],
    is a kind of label perturbation method. Let $C$ be the number of categories and
    $\lambda$ be a hyper-parameter. Label smoothing perturbs the label y (one-hot
    type) with the following perturbation $\triangle y=\lambda(\frac{I}{C}-y)$, where
    $I$ is a $C$-dimensional vector and its each element is 1\. A large number of
    variations have been proposed for label smoothing [[178](#bib.bib178), [179](#bib.bib179),
    [62](#bib.bib62)].
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标签扰动。该类别在真实标签或预测标签上添加扰动。一种经典的学习技巧，即标签平滑[[177](#bib.bib177)]，是一种标签扰动方法。设 $C$
    为类别数，$\lambda$ 为超参数。标签平滑通过以下扰动 $\triangle y=\lambda(\frac{I}{C}-y)$ 对标签 y（单热编码类型）进行扰动，其中
    $I$ 是一个 $C$ 维向量，且每个元素均为 1。已经提出了大量的标签平滑变体[[178](#bib.bib178), [179](#bib.bib179),
    [62](#bib.bib62)]。
- en: •
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Gradient perturbation. This division adds the perturbation directly on gradient.
    Studies on gradient perturbation are few. Orvieto et al.[[180](#bib.bib180)] proposed
    a gradient perturbation method and verified its effectiveness both theoretically
    and empirically.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 梯度扰动。该类别直接在梯度上添加扰动。关于梯度扰动的研究较少。Orvieto 等人[[180](#bib.bib180)] 提出了一种梯度扰动方法，并在理论和实证上验证了其有效性。
- en: There are also studies [[181](#bib.bib181)] which perturb other data such as
    network weights in training, which is not the focus of this study. Wang et al. [[182](#bib.bib182)]
    proposed a reward perturbation method for noisy reinforcement learning.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些研究[[181](#bib.bib181)] 通过扰动其他数据，如训练中的网络权重，这不是本研究的重点。Wang 等人[[182](#bib.bib182)]
    提出了用于噪声强化学习的奖励扰动方法。
- en: VI-C2 Perturbation direction
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C2 扰动方向
- en: Data perturbation will either increase or decrease the loss values of training
    samples in the learning process. Based on whether the loss increases or decreases,
    existing methods can be categorized as positive or negative augmentations.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 数据扰动会在学习过程中增加或减少训练样本的损失值。根据损失值是增加还是减少，现有方法可以分为正向或负向扩充。
- en: Positive perturbation. It increases the training losses of perturbed training
    samples. Obviously, adversarial perturbation belongs to positive perturbation,
    as it maximizes the training loss with the adversarial perturbations. ISDA [[39](#bib.bib39)]
    also belongs to positive perturbation as it adds positive real numbers to the
    denominator of the Softmax function.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 正向扰动。它增加了扰动训练样本的训练损失。显然，对抗扰动属于正向扰动，因为它通过对抗扰动最大化训练损失。ISDA[[39](#bib.bib39)] 也属于正向扰动，因为它在
    Softmax 函数的分母中添加了正实数。
- en: Negative perturbation. It reduces the training losses. Anti-adversarial perturbation [[67](#bib.bib67)]
    belongs to negative perturbation, as it minimizes the training loss with the adversarial
    perturbations. Bootstrapping [[183](#bib.bib183)] is a typical robust loss based
    on label perturbation. It also belongs to negative perturbation as its perturbation
    is $\triangle y=\lambda(p-y)$, where $p$ is the prediction of the current trained
    model.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 负面扰动。它减少训练损失。对抗性扰动 [[67](#bib.bib67)] 属于负面扰动，因为它通过对抗性扰动来最小化训练损失。自助法 [[183](#bib.bib183)]
    是基于标签扰动的典型鲁棒损失。它也属于负面扰动，因为其扰动为 $\triangle y=\lambda(p-y)$，其中 $p$ 是当前训练模型的预测值。
- en: Some methods increase the losses of some samples and decrease those of others
    simultaneously. For instance, the losses of noisy-label training samples may be
    reduced, while those of clean samples may be increased in label smoothing. Li
    et al. [[47](#bib.bib47)] proposed a conjecture for the relationship between loss
    increment/decrement and data augmentation.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法同时增加某些样本的损失并减少其他样本的损失。例如，在标签平滑中，噪声标签训练样本的损失可能会减少，而干净样本的损失可能会增加。Li 等人 [[47](#bib.bib47)]
    对损失增减与数据增强之间的关系提出了一个猜想。
- en: VI-C3 Perturbation granularity
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C3 扰动粒度
- en: According to perturbation granularity, existing methods can be divided into
    sample-wise, class-wise, and corpus-wise.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 根据扰动粒度，现有方法可以分为样本级、类别级和语料库级。
- en: •
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sample-wise perturbation. In this division, each training sample has its own
    perturbation and different samples usually have distinct perturbations. The aforementioned
    Bootstrapping and adversarial perturbation all belong to this division. The random
    cropping and masking also belong to this division.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 样本级扰动。在这个分类中，每个训练样本有其自身的扰动，不同的样本通常具有不同的扰动。上述的自助法和对抗性扰动都属于这个分类。随机裁剪和遮挡也属于这一分类。
- en: •
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Class-wise perturbation. In this division, all the training samples in a category
    share the same perturbation, and different categories usually have distinct perturbations.
    Benz et al. [[184](#bib.bib184)] proposed a class-wise adversarial perturbation
    method. Wang et al. [[185](#bib.bib185)] introduced class-wise logit perturbation
    for the training in semantic segmentation. Label smoothing also belongs to this
    division.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类别级扰动。在这个分类中，同一类别中的所有训练样本共享相同的扰动，而不同类别通常具有不同的扰动。Benz 等人 [[184](#bib.bib184)]
    提出了类别级对抗性扰动方法。Wang 等人 [[185](#bib.bib185)] 为语义分割训练引入了类别级逻辑扰动。标签平滑也属于这一分类。
- en: •
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Corpus-wise perturbation. In this division, all the training samples in the
    training corpus share only one perturbation. Shafahi et al. [[186](#bib.bib186)]
    pursued the universal adversarial perturbation for all the training samples, which
    has proven to be effective in various applications [[187](#bib.bib187)]. Wu et
    al. [[188](#bib.bib188)] proposed a corpus-wise logit perturbation method for
    multi-label learning tasks.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语料库级扰动。在这个分类中，训练语料库中的所有训练样本共享唯一的一个扰动。Shafahi 等人 [[186](#bib.bib186)] 研究了对所有训练样本通用的对抗性扰动，这在各种应用中已被证明是有效的
    [[187](#bib.bib187)]。Wu 等人 [[188](#bib.bib188)] 提出了用于多标签学习任务的语料库级逻辑扰动方法。
- en: VI-C4 Assignment manner
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C4 分配方式
- en: 'The perturbation variables should be assigned before or during training. As
    presented in Fig. [11](#S5.F11 "Figure 11 ‣ V-B Analysis on perceived quantities
    ‣ V Optimization pipeline ‣ Data Optimization in Deep Learning: A Survey"), there
    are four typical assignment manners to determine the perturbations.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '扰动变量应在训练前或训练期间分配。如图 [11](#S5.F11 "Figure 11 ‣ V-B Analysis on perceived quantities
    ‣ V Optimization pipeline ‣ Data Optimization in Deep Learning: A Survey") 所示，有四种典型的分配方式来确定扰动。'
- en: Rule-based assignment. In this manner, the perturbation is assigned according
    to pre-fixed rules. These rules are usually based on prior knowledge or statistical
    inspirations. In both label smoothing and Booststrapping loss, the label perturbation
    is determined according to manually defined formulas. In text classification,
    word replacement and random masking also obey rules.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的分配。在这种方式中，扰动是根据预先设定的规则分配的。这些规则通常基于先验知识或统计启发。在标签平滑和自助法损失中，标签扰动是根据手动定义的公式确定的。在文本分类中，词汇替换和随机遮挡也遵循规则。
- en: 'Regularization-based assignment. In this manner, a regularizer for the perturbation
    is usually added in the total loss. Take the logit perturbation as an example.
    A loss function with regularization for logit perturbation can be defined as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 基于正则化的分配。在这种方式下，通常在总损失中添加扰动的正则化项。以 logit 扰动为例，可以定义一个包含正则化的 logit 扰动损失函数如下：
- en: '|  | $\mathcal{L}=\sum_{i}l(\mathcal{S}(v_{i}+\triangle v_{i}),y_{i})+\lambda
    Reg(\triangle v_{i}).$ |  | (8) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\sum_{i}l(\mathcal{S}(v_{i}+\triangle v_{i}),y_{i})+\lambda
    Reg(\triangle v_{i}).$ |  | (8) |'
- en: where $\mathcal{S}$ is the Softmax function, $v_{i}$ is the logit vector for
    $x_{i}$, $\triangle v_{i}$ is the perturbation vector for $v_{i}$, and $Reg(\cdot)$
    is the regularizer. Zhou et al. [[189](#bib.bib189)] introduced a novel perturbation
    way for adversarial examples by leveraging smoothing regularization on adversarial
    perturbations. Wei et al. [[190](#bib.bib190)] took the notation that adversarial
    perturbations are temporally sparse for videos and then proposed a sparse-regularized
    adversarial perturbation method. Zhu et al. [[191](#bib.bib191)] proposed a Bayesian
    neural network with non-zero mean of Gaussian noise. The mean is actually a feature
    perturbation and inferred with $l_{2}$ regularization.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{S}$ 是 Softmax 函数，$v_{i}$ 是 $x_{i}$ 的 logit 向量，$\triangle v_{i}$
    是 $v_{i}$ 的扰动向量，$Reg(\cdot)$ 是正则化项。Zhou 等人[[189](#bib.bib189)] 通过对抗扰动上的平滑正则化引入了一种新的对抗样本扰动方式。Wei
    等人[[190](#bib.bib190)] 认为对抗扰动在视频中是时序稀疏的，并提出了一种稀疏正则化对抗扰动方法。Zhu 等人[[191](#bib.bib191)]
    提出了一个具有非零均值高斯噪声的贝叶斯神经网络。该均值实际上是特征扰动，并通过 $l_{2}$ 正则化进行推断。
- en: Loss-optimization-based assignment. This division is similar to the loss-optimization-based
    augmentation introduced in Section VI-B2\. A new loss containing the perturbations
    is defined and the perturbation is pursued by optimizing the loss. In the optimization
    procedure, only the perturbations are the variables to be optimized, while the
    model parameters are fixed.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 基于损失优化的分配。该分配类似于第 VI-B2 节中介绍的基于损失优化的增强。定义一个包含扰动的新损失，通过优化损失来追踪扰动。在优化过程中，只有扰动是需要优化的变量，而模型参数是固定的。
- en: Learning-based assignment. In this manner, the perturbation is assigned by leveraging
    a learning method. Three learning paradigms are usually applied, including self-supervised
    learning, meta learning, and reinforcement learning.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的分配。在这种方式下，扰动通过利用学习方法进行分配。通常应用三种学习范式，包括自监督学习、元学习和强化学习。
- en: •
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Self-supervised learning. This paradigm leverages self-supervised learning methodologies
    such as contrastive learning [[192](#bib.bib192)] to pursue the perturbations.
    Naseer et al. [[193](#bib.bib193)] constructed a self-supervised perturbation
    framework to optimize the feature distortion for a training image. Zhang et al. [[194](#bib.bib194)]
    proposed a generative adversarial network-based self-supervised method to generate
    EEG signals.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自监督学习。该范式利用自监督学习方法，如对比学习[[192](#bib.bib192)]，来追踪扰动。Naseer 等人[[193](#bib.bib193)]
    构建了一个自监督扰动框架，以优化训练图像的特征失真。Zhang 等人[[194](#bib.bib194)] 提出了基于生成对抗网络的自监督方法来生成脑电图信号。
- en: •
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Meta learning. This paradigm leverages meta-learning methodologies to pursue
    the perturbations using an additional meta dataset. It assumes that the perturbation
    $\triangle{x}$ (or $\triangle{y}$) for a training sample $x$ (or its label $y$)
    is determined by the representation of $x$ or factors such as training dynamics
    for $x$, which is described as follows:'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 元学习。该范式利用元学习方法通过额外的元数据集来追踪扰动。它假设训练样本 $x$（或其标签 $y$）的扰动 $\triangle{x}$（或 $\triangle{y}$）由
    $x$ 的表示或训练动态等因素决定，如下所述：
- en: '|  | $\triangle{x}=g(x,\eta(x)),$ |  | (9) |'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\triangle{x}=g(x,\eta(x)),$ |  | (9) |'
- en: where $g(\cdot)$ can be a black-box neural network such as MLP; $\eta(x)$ represents
    the training dynamics for $x$. Li et al. [[195](#bib.bib195)] applied meta learning
    to directly optimize the covariant matrix used in ISDA, which is used to calculate
    the logit perturbation. Qiao and Peng [[196](#bib.bib196)] utilized the meta learning
    to learn an independent DNN for both features and label perturbation.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $g(\cdot)$ 可以是如 MLP 这样的黑箱神经网络；$\eta(x)$ 表示 $x$ 的训练动态。Li 等人[[195](#bib.bib195)]
    采用元学习直接优化 ISDA 中使用的协方差矩阵，该矩阵用于计算 logit 扰动。Qiao 和 Peng [[196](#bib.bib196)] 利用元学习为特征和标签扰动学习一个独立的
    DNN。
- en: •
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reinforcement learning. This paradigm leverages reinforcement learning to pursue
    the perturbations without relying on additional data. Many data augmentation methods [[197](#bib.bib197),
    [198](#bib.bib198)], which also belong to data perturbation, are based on reinforcement
    learning. Giovanni et al. [[199](#bib.bib199)] leveraged deep reinforcement learning
    to automatically generate realistic attack samples that can evade detection and
    train producing hardened models. Lin et al. [[200](#bib.bib200)] formulated the
    perturbation generation as a Markov decision process and optimized it by reinforcement
    learning to generate perturbed instructions sequentially.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 强化学习。这一范式利用强化学习追求扰动，而不依赖额外的数据。许多数据增强方法[[197](#bib.bib197), [198](#bib.bib198)]，也属于数据扰动，是基于强化学习的。Giovanni等人[[199](#bib.bib199)]利用深度强化学习自动生成可以躲避检测的真实攻击样本，并训练生成强化模型。Lin等人[[200](#bib.bib200)]将扰动生成制定为马尔可夫决策过程，并通过强化学习优化以顺序生成扰动指令。
- en: Given a learning task, it is difficulty to directly judge which assignment manner
    is the most appropriate without a thorough and comprehensive understanding for
    the task. Each assignment manner has its own merits and defects.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于学习任务，很难在没有对任务进行彻底和全面了解的情况下直接判断哪种分配方式最为适宜。每种分配方式都有其自身的优点和缺陷。
- en: VI-D Data weighting
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D 数据加权
- en: Data weighting assigns a weight for each training sample in loss calculation.
    It is among the most popular data optimization techniques in many learning scenarios,
    including fraud detection [[201](#bib.bib201)], portfolio selection [[202](#bib.bib202)],
    medical diagnosis [[203](#bib.bib203)], and fairness-aware learning [[45](#bib.bib45),
    [204](#bib.bib204)]. Three dividing dimensions are considered, namely, granularity,
    dependent factor, and assignment manner for weights.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加权为每个训练样本分配一个权重用于损失计算。这是许多学习场景中最受欢迎的数据优化技术之一，包括欺诈检测[[201](#bib.bib201)]、投资组合选择[[202](#bib.bib202)]、医学诊断[[203](#bib.bib203)]和公平学习[[45](#bib.bib45),
    [204](#bib.bib204)]。考虑了三种划分维度，即，粒度、依赖因子和权重分配方式。
- en: VI-D1 Weighting granularity
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-D1 加权粒度
- en: According to the granularity of weights, existing weighting methods can be divided
    into sample-wise and category-wise. Noisy-label learning usually adopts sample-wise
    weighting methods [[205](#bib.bib205), [206](#bib.bib206)], while imbalanced learning
    usually adopts category-wise ones [[207](#bib.bib207), [46](#bib.bib46), [208](#bib.bib208)].
    Data weighting is also widely used in standard learning [[209](#bib.bib209), [210](#bib.bib210)],
    which are usually sample-wise.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 根据权重的粒度，现有的加权方法可以分为样本级和类别级。噪声标签学习通常采用样本级加权方法[[205](#bib.bib205), [206](#bib.bib206)]，而不平衡学习通常采用类别级加权方法[[207](#bib.bib207),
    [46](#bib.bib46), [208](#bib.bib208)]。数据加权也广泛用于标准学习[[209](#bib.bib209), [210](#bib.bib210)]，这些通常是样本级的。
- en: VI-D2 Dependent factor
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-D2 依赖因子
- en: Dependent factor in this study denotes the factors that are leveraged to calculate
    the sample weights. Similar with the resampling introduced in Section VI-A, three
    factor types are usually considered, namely, category proportion, importance,
    and learning difficulty. As these concepts are introduced in Section VI-A and
    quite similar procedures are adopted, these factors are not detailed in this part.
    There are an increasing number of studies employing learning difficulty-based
    weighting. They can be further summarized according to which samples are learned
    first.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中的依赖因子指的是用于计算样本权重的因素。类似于第VI-A节介绍的重采样，通常考虑三种因素类型，即，类别比例、重要性和学习难度。由于这些概念在第VI-A节已介绍且采用了相似的程序，因此此部分不再详细说明。越来越多的研究采用基于学习难度的加权。它们可以根据首先学习哪些样本进行进一步总结。
- en: As samples with larger weights than others can be considered as having priority
    in training, learning difficulty-based weighting contains three basic folds, namely,
    easy-first, hard-first, and complicated.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 由于样本权重较大的可以被认为在训练中优先考虑，基于学习难度的加权包含三个基本方面，即，先易后难、先难后易和复杂。
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Easy-first. Easy samples are given higher weights than hard ones in this fold.
    There are a huge number of easy-first weighting methods, which mainly belong to
    two paradigms: curriculum learning [[71](#bib.bib71)] and self-paced learning [[100](#bib.bib100)].
    These two paradigms assign larger weights to easy samples during the early training
    stage and gradually increase the weights of hard samples. Numerous studies have
    been conducted on the design of the weighting formulas [[211](#bib.bib211), [212](#bib.bib212),
    [213](#bib.bib213)]. Easy-first weighting is usually used in noisy-label learning.
    Extensive experiments on curriculum learning indicate that it mainly takes effects
    on noisy-label learning tasks [[214](#bib.bib214)].'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 易样本优先。在这一折中，易样本的权重高于硬样本。有大量的易样本优先加权方法，这些方法主要属于两种范式：课程学习 [[71](#bib.bib71)] 和自适应学习 [[100](#bib.bib100)]。这两种范式在早期训练阶段为易样本分配较大的权重，并逐渐增加硬样本的权重。关于加权公式的设计进行了大量研究 [[211](#bib.bib211),
    [212](#bib.bib212), [213](#bib.bib213)]。易样本优先加权通常用于带噪声标签学习。课程学习的广泛实验表明，它主要对带噪声标签的学习任务有效 [[214](#bib.bib214)]。
- en: •
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hard-first. Hard samples have higher weights than easy ones in this fold. Focal
    loss is a typical hard-first strategy [[72](#bib.bib72)]. Zhang et al. [[210](#bib.bib210)]
    also assigned large weights on hard samples. Santiagoa et al. [[215](#bib.bib215)]
    utilized the gradient norm to measure learning difficulty and exerted large weights
    on samples with large gradient norms.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 硬样本优先。在这一折中，硬样本的权重高于易样本。Focal loss 是一种典型的硬样本优先策略 [[72](#bib.bib72)]。Zhang 等人 [[210](#bib.bib210)]
    也为硬样本分配了较大的权重。Santiagoa 等人 [[215](#bib.bib215)] 利用梯度范数来测量学习难度，并对具有大梯度范数的样本施加较大的权重。
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Complicated. In some weighting methods, the easy-first or the hard-first is
    combined with other weighting inspirations. In Balanced CL [[216](#bib.bib216)](should
    be replaced), on the basis of the easy-first mode, the selection of samples has
    to be balanced under certain constraints to ensure diversity across image regions
    or categories. Therefore, Balanced CL adopts the complicated mode.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 复杂。在一些加权方法中，易样本优先或硬样本优先与其他加权启发式结合。在 Balanced CL [[216](#bib.bib216)]（应予以替换）中，在易样本优先模式的基础上，样本选择必须在一定约束下平衡，以确保图像区域或类别的多样性。因此，Balanced
    CL 采用了复杂的模式。
- en: Besides the three general ways, Zhou et al. [[217](#bib.bib217)] revealed some
    other priority types including both-ends-first and varied manners during training.
    There also other dependent factors such as misclassified cost and those reflecting
    other concerns such as fairness and confidence [[218](#bib.bib218), [219](#bib.bib219)].
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这三种一般方式外，Zhou 等人 [[217](#bib.bib217)] 还揭示了一些其他的优先级类型，包括两端优先和训练期间的多样方式。还有其他依赖因素，如误分类成本以及反映其他关注点的因素，如公平性和信心 [[218](#bib.bib218),
    [219](#bib.bib219)]。
- en: VI-D3 Assignment manner
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-D3 分配方式
- en: 'Generally, there are four manners to assign weights for training samples as
    shown in Fig. [11](#S5.F11 "Figure 11 ‣ V-B Analysis on perceived quantities ‣
    V Optimization pipeline ‣ Data Optimization in Deep Learning: A Survey").'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，样本权重的分配方式有四种，如图 [11](#S5.F11 "Figure 11 ‣ V-B Analysis on perceived quantities
    ‣ V Optimization pipeline ‣ Data Optimization in Deep Learning: A Survey")所示。'
- en: Rule-based assignment. This manner determines the sample weights according to
    theoretical or heuristic rules. For example, many methods assume that the category
    proportion is the prior probability. Consequently, the inverse of the category
    proportion is used as the weight based on the Bayesian rule. Cui et al. [[46](#bib.bib46)]
    established a theoretical framework for weight calculation based on the effective
    number theory in computation geometry. The classical Focal loss [[72](#bib.bib72)]
    heuristic defines the weight using $w=(1-p)^{\gamma}$, where $p$ is the prediction
    on the ground-truth label and $\gamma$ is a hyper-parameter. Han et al. [[220](#bib.bib220)]
    defined an uncertainty-based weighting manner for the two random samples in mixup.
    Importance weighting [[221](#bib.bib221)] is also placed in this division.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的分配。这种方式根据理论或启发式规则来确定样本权重。例如，许多方法假设类别比例是先验概率。因此，根据贝叶斯规则，类别比例的倒数被用作权重。Cui
    等人 [[46](#bib.bib46)] 基于计算几何中的有效数量理论建立了一个权重计算的理论框架。经典的 Focal loss [[72](#bib.bib72)]
    启发式定义了权重使用 $w=(1-p)^{\gamma}$，其中 $p$ 是对真实标签的预测，$\gamma$ 是超参数。Han 等人 [[220](#bib.bib220)]
    定义了一种基于不确定性的加权方式，用于 mixup 中的两个随机样本。重要性加权 [[221](#bib.bib221)] 也属于这一类别。
- en: 'Regularization-based assignment. This method defines a new loss function which
    contains a weighted loss and a regularizer ($Reg(W)$) on the weights as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 基于正则化的分配。该方法定义了一个新的损失函数，其中包含加权损失和对权重的正则化项 ($Reg(W)$)，具体如下：
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{i=1}^{N}w_{i}l(f(x_{i}),y_{i})+\lambda Reg(W),$
    |  | (10) |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\frac{1}{N}\sum_{i=1}^{N}w_{i}l(f(x_{i}),y_{i})+\lambda Reg(W),$
    |  | (10) |'
- en: where $W=\{w_{1},\cdots,w_{N}\}^{T}$ is the vector of sample weights. The classical
    self-paced learning, which mimics the mechanism of human learning from easy to
    hard gradually, is actually the regularization method defined as $Reg(W)=-|W|_{1}$ ($w_{i}\in\{0,1\}$) [[100](#bib.bib100)].
    Fan et al. [[222](#bib.bib222)] presented a new group of self-paced regularizers
    deduced from robust loss functions and further analyzed the relation between the
    presented regularizer-based optimization and half-quadratic optimization.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W=\{w_{1},\cdots,w_{N}\}^{T}$ 是样本权重的向量。经典的自适应学习，模拟了人类从易到难逐步学习的机制，实际上是一种正则化方法，定义为
    $Reg(W)=-|W|_{1}$ ($w_{i}\in\{0,1\}$) [[100](#bib.bib100)]。Fan 等人 [[222](#bib.bib222)]
    提出了从鲁棒损失函数推导出的新一组自适应正则化器，并进一步分析了所提出的正则化器基础优化与半二次优化之间的关系。
- en: 'Adversarial optimization-based assignment. This manner pursues the sample weights
    by optimizing a defined objective function, which is similar to the pursing of
    the adversarial perturbation. For instance, Gu et al. [[223](#bib.bib223)] adversarially
    learned the weights of source domain samples to align the source and target domain
    distributions by maximizing the Wasserstein distance. Yi et al. [[224](#bib.bib224)]
    defined a maximal expected loss and obtained a simple and interpretable closed-form
    solution for samples’ weights: larger weights should be given to augmented samples
    with large loss values.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗优化基础的分配。这种方式通过优化定义的目标函数来追求样本权重，这与对抗扰动的追求类似。例如，Gu 等人 [[223](#bib.bib223)] 对源域样本的权重进行了对抗性学习，通过最大化
    Wasserstein 距离来对齐源域和目标域分布。Yi 等人 [[224](#bib.bib224)] 定义了一个最大期望损失，并获得了样本权重的简单而可解释的闭式解：较大的权重应给予具有较大损失值的增强样本。
- en: Learning-based assignment. Similar with that in data perturbation, learning-based
    assignment also usually applies meta learning or reinforcement learning to infer
    the sample weights. Ren et al. [[225](#bib.bib225)] firstly introduced meta learning
    for sample weighting in imbalanced learning and noisy-label learning. Shu et al. [[99](#bib.bib99)]
    utilized an MLP network to model the relationship between samples’ characters
    and their weights, and then trained the network using meta learning. Zhao et al. [[226](#bib.bib226)]
    further proposed a probabilistic formulation for meta learning-based weighting.
    Trung et al. [[227](#bib.bib227)] leveraged meta learning to train a neural network-based
    self-paced learning for unsupervised domain adaption. Wei et al. [[228](#bib.bib228)]
    also combined meta learning and self-paced network to automatically generate a
    weighting scheme from data for cross-modal matching. Li et al. [[229](#bib.bib229)]
    proposed meta learning-based weighting for pseudo-labeled target samples in unsupervised
    domain adaptation. Meta learning requires additional meta data, whereas reinforcement
    learning does not require additional data. Zhou et al. [[230](#bib.bib230)] leveraged
    an augmentation policy network which takes a transformation and the corresponding
    augmented image as inputs to generate the loss weight of an augmented. Ge et al. [[231](#bib.bib231)]
    used a delicately designed controller network to generate sample weights and combined
    the weights with the loss of each input data to train a recommendation system.
    The controller network is optimized by reinforcement learning.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的分配。类似于数据扰动中的方法，基于学习的分配通常也应用元学习或强化学习来推断样本权重。任等人 [[225](#bib.bib225)] 首次引入了元学习用于不平衡学习和噪声标签学习中的样本加权。舒等人 [[99](#bib.bib99)]
    利用MLP网络来建模样本特征与其权重之间的关系，然后使用元学习训练网络。赵等人 [[226](#bib.bib226)] 进一步提出了一种基于元学习的概率公式。Trung等人 [[227](#bib.bib227)]
    利用元学习训练一种基于神经网络的自适应学习，用于无监督领域适应。魏等人 [[228](#bib.bib228)] 还结合了元学习和自适应网络，从数据中自动生成加权方案用于跨模态匹配。李等人 [[229](#bib.bib229)]
    提出了基于元学习的加权方法，用于无监督领域适应中的伪标签目标样本。元学习需要额外的元数据，而强化学习不需要额外的数据。周等人 [[230](#bib.bib230)]
    利用增强策略网络，该网络将变换和相应的增强图像作为输入，以生成增强的损失权重。葛等人 [[231](#bib.bib231)] 使用精心设计的控制网络来生成样本权重，并将这些权重与每个输入数据的损失结合起来，训练推荐系统。控制网络通过强化学习进行优化。
- en: Weights assignment can also be divided into static and dynamic. There are a
    few methods adopting static weighting [[46](#bib.bib46)], whereas most methods
    adopting dynamic. Fang et al. [[232](#bib.bib232)] proposed dynamic importance
    weighting to train the models.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 权重分配也可以分为静态和动态。有几种方法采用静态加权 [[46](#bib.bib46)]，而大多数方法则采用动态加权。方等人 [[232](#bib.bib232)]
    提出了动态重要性加权来训练模型。
- en: VI-E Data pruning
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-E 数据剪枝
- en: Data pruning is contrary to data augmentation. In this study, it is divided
    into dataset distillation and subset selection.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 数据剪枝与数据增强相对。在本研究中，它被分为数据集蒸馏和子集选择。
- en: VI-E1 Dataset distillation
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-E1 数据集蒸馏
- en: Dataset distillation is firstly proposed by Wang et al. [[233](#bib.bib233)]
    and it aims to synthesize a small typical training set from substantial data [[234](#bib.bib234)].
    The synthesized dataset replaces the given dataset for efficient and accurate
    data-usage for the learning task. Following the division established by Sachdeva
    and McAuley [[235](#bib.bib235)], existing data distillation methods can be placed
    in four folds.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集蒸馏首次由王等人 [[233](#bib.bib233)] 提出，旨在从大量数据中合成一个小的典型训练集 [[234](#bib.bib234)]。合成的数据集替代给定的数据集，以实现对学习任务的高效和准确的数据使用。根据Sachdeva和McAuley [[235](#bib.bib235)]
    的分类，现有的数据蒸馏方法可以分为四类。
- en: Meta-model matching-based strategy. This strategy is firstly proposed by Wang
    et al. [[233](#bib.bib233)]. It performs an inner-loop optimization for a temporal
    optimal model based on the synthesized set and an outer-loop optimization for
    a temporal subset (i.e., the synthesized set) by turns. Some recent studies discussed
    its drawbacks such as the ineffectiveness of the TBPTT-based optimization [[236](#bib.bib236)]
    and proposed new solutions such as momentum-based optimizers [[237](#bib.bib237)].
    Loo et al. [[238](#bib.bib238)] utilized the light-weight empirical neural network
    Gaussian process kernel for the inner-loop optimization and a new loss for outer-loop
    optimization. Zhou et al. [[239](#bib.bib239)] combined feature extractor in the
    distillation procedure.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 基于元模型匹配的策略。该策略由Wang等人首次提出[[233](#bib.bib233)]。它对基于合成集的临时最优模型执行内循环优化，并对临时子集（即合成集）执行外循环优化。一些近期研究讨论了其缺点，如TBPTT优化的无效性[[236](#bib.bib236)]，并提出了新的解决方案，如基于动量的优化器[[237](#bib.bib237)]。Loo等人[[238](#bib.bib238)]利用轻量级经验神经网络高斯过程核进行内循环优化，并为外循环优化提出了新的损失函数。Zhou等人[[239](#bib.bib239)]在蒸馏过程中结合了特征提取器。
- en: Gradient matching-based strategy. This strategy [[236](#bib.bib236), [240](#bib.bib240)]
    does not require to perform the inner-loop optimization as used in the meta-model
    matching-based strategy. Therefore, it is more efficient than the meta-model matching-based
    strategy. Numerous approaches have been proposed along this division. Kim et al. [[241](#bib.bib241)]
    further utilized spatial redundancy removing to accelerate the optimization process
    and gradients matching on the original dataset.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度匹配的策略。该策略[[236](#bib.bib236), [240](#bib.bib240)]不需要像元模型匹配策略那样执行内循环优化。因此，它比元模型匹配策略更高效。许多方法在这一分类下被提出。Kim等人[[241](#bib.bib241)]进一步利用空间冗余去除来加速优化过程，并在原始数据集上进行梯度匹配。
- en: Trajectory matching-based strategy. This strategy performs distillation by matching
    the training trajectories of models trained on the original and the pursued datasets [[242](#bib.bib242)].
    Cui et al. [[243](#bib.bib243)] proposed a memory-efficient method which is avaliable
    for large datasets.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 基于轨迹匹配的策略。该策略通过匹配在原始数据集和目标数据集上训练的模型的训练轨迹来执行蒸馏[[242](#bib.bib242)]。Cui等人[[243](#bib.bib243)]提出了一种适用于大数据集的内存高效方法。
- en: Distribution matching-based strategy. This strategy performs the distillation
    by directly matching the distribution of the original dataset and the pursued
    dataset [[244](#bib.bib244)]. Wang et al. [[245](#bib.bib245)] constructed a bilevel
    optimization strategy to jointly optimize a single encoder and summarize data.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分布匹配的策略。该策略通过直接匹配原始数据集和目标数据集的分布来执行蒸馏[[244](#bib.bib244)]。Wang等人[[245](#bib.bib245)]构建了一个双层优化策略来联合优化单个编码器和总结数据。
- en: There are some solutions [[246](#bib.bib246), [247](#bib.bib247), [248](#bib.bib248),
    [249](#bib.bib249), [237](#bib.bib237), [250](#bib.bib250)] which take alternative
    technical strategies. Zhou et al. [[246](#bib.bib246)] introduced reinforcement
    learning to solve the bi-level optimization in data distillation. Zhao and Bilen [[247](#bib.bib247)]
    learned a series of low-dimensional codes to generate highly informative images
    through the GAN generator.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些解决方案[[246](#bib.bib246), [247](#bib.bib247), [248](#bib.bib248), [249](#bib.bib249),
    [237](#bib.bib237), [250](#bib.bib250)]采用了替代技术策略。Zhou等人[[246](#bib.bib246)]引入了强化学习来解决数据蒸馏中的双层优化问题。Zhao和Bilen[[247](#bib.bib247)]通过GAN生成器学习了一系列低维代码来生成高信息量的图像。
- en: VI-E2 Subset selection
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-E2 子集选择
- en: 'Different from dataset distillation that generates a new training set, subset
    selection aims to select the most useful samples from the original training set [[35](#bib.bib35)].
    It does not generate new samples and can be used in any training stages that require
    to select samples from the original training set. In Fig. [11](#S5.F11 "Figure
    11 ‣ V-B Analysis on perceived quantities ‣ V Optimization pipeline ‣ Data Optimization
    in Deep Learning: A Survey"), there are two divisions, including greedy search-based
    and mathematics-based.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '不同于生成新训练集的数据集蒸馏，子集选择旨在从原始训练集中选择最有用的样本[[35](#bib.bib35)]。它不会生成新样本，可以在任何需要从原始训练集中选择样本的训练阶段使用。在图[11](#S5.F11
    "Figure 11 ‣ V-B Analysis on perceived quantities ‣ V Optimization pipeline ‣
    Data Optimization in Deep Learning: A Survey")中，有两种分类，包括基于贪婪搜索和基于数学的方法。'
- en: In the greedy search-based strategy, the utility of each training sample is
    measured, and the subset is searched based on the utility rankings. According
    to the employed measures, existing methods can be divided into four categories,
    including difficulty-based, influence-based, value-based, and confidence-based.
    Meding et al. [[251](#bib.bib251)] utilized the misclassified rate by multiple
    classifiers as the learning difficulty of a training sample to select samples.
    Feldman and Zhang [[252](#bib.bib252)] defined an influence score and a memorization
    score to measure the usefulness of a training sample. Samples with low influence
    and memorization scores are redundant and can be deleted. Birodkar et al. [[6](#bib.bib6)]
    employed clustering to select most valuable samples which are close to the cluster
    centers and delete the rest redundant ones. Northcutt et al. [[253](#bib.bib253)]
    leveraged the confidence score to prune training samples. There are also many
    studies combining the measures and active learning to select samples [[254](#bib.bib254)].
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于贪心搜索的策略中，会对每个训练样本的效用进行测量，并根据效用排名来搜索子集。根据所使用的度量方法，现有的方法可以分为四类，包括基于难度、基于影响、基于价值和基于信心的方法。Meding
    等人 [[251](#bib.bib251)] 利用多个分类器的误分类率作为训练样本的学习难度来选择样本。Feldman 和 Zhang [[252](#bib.bib252)]
    定义了一个影响评分和一个记忆评分来衡量训练样本的有用性。影响评分和记忆评分较低的样本是冗余的，可以被删除。Birodkar 等人 [[6](#bib.bib6)]
    使用聚类来选择最有价值的样本，这些样本靠近聚类中心，并删除其余的冗余样本。Northcutt 等人 [[253](#bib.bib253)] 利用信心评分来修剪训练样本。也有许多研究结合了这些度量和主动学习来选择样本
    [[254](#bib.bib254)]。
- en: Different from the greedy search strategy, some other methods seek a global
    optimal subset according to a mathematical approach. Yang et al. [[255](#bib.bib255)]
    proposed a scalable framework to iteratively extract multiple mini-batch coresets
    from larger random subsets of training data by solving a submodular cover problem.
    Mirzasoleiman et al. [[256](#bib.bib256)] defined a monotonic function for coreset
    selection and proposed a generic algorithm with approximately linear complexity.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 与贪心搜索策略不同，一些其他方法根据数学方法寻求全局最优子集。Yang 等人 [[255](#bib.bib255)] 提出了一个可扩展框架，通过解决子模块覆盖问题，迭代地从较大的随机训练数据子集中提取多个小批量核心集。Mirzasoleiman
    等人 [[256](#bib.bib256)] 为核心集选择定义了一个单调函数，并提出了一个大致线性复杂度的通用算法。
- en: VI-F Other typical techniques
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-F 其他典型技术
- en: This study lists two representative technical paths, including pure mathematical
    optimization and the combination of more than one aforementioned methods described
    in Sections VI-A to VI-E.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究列出了两条代表性的技术路径，包括纯数学优化和结合了上述一种或多种方法的技术路径，这些方法在第 VI-A 节到 VI-E 节中描述。
- en: VI-F1 Pure mathematical optimization
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-F1 纯数学优化
- en: This division refers to the manners that perform data optimization via a pure
    mathematical optimization procedure in the above-mentioned divisions.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这种划分指的是在上述划分中通过纯数学优化过程进行数据优化的方法。
- en: The first typical scenario for pure mathematical optimization is the construction
    of a small-size yet high-quality dataset from the original training set. The tasks
    involving batch construction, meta data compiling in meta learning, or dataset
    distillation usually adopt mathematical optimization. Liu et al. [[257](#bib.bib257)]
    constructed a set variance diversity-based objective function for data augmentation
    and pursued the selection for a set of augmented samples via the maximization
    of the objective function in batch construction. Joseph et al. [[258](#bib.bib258)]
    proposed a submodular optimization-based method to construct a mini-batch in DNN
    training. Significant improvements in convergence and accuracy with their constructed
    mini-batches have been observed. Su et al. [[59](#bib.bib59)] established an objective
    function for meta data compiling. The objective consists of four criterion, including
    cleanliness, diversity, balance, and informative. As introduced in Section VI-E,
    data pruning is usually performed based on pure mathematical optimization.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 纯数学优化的第一个典型场景是从原始训练集中构建一个小规模但高质量的数据集。涉及批量构建、元学习中的元数据编译或数据集蒸馏的任务通常采用数学优化。Liu等人[[257](#bib.bib257)]构建了一种基于集方差多样性的目标函数，用于数据增强，并通过最大化目标函数在批量构建中选择一组增强样本。Joseph等人[[258](#bib.bib258)]提出了一种基于子模优化的方法来构建DNN训练中的小批量。他们构建的小批量在收敛性和准确性上观察到了显著的改进。Su等人[[59](#bib.bib59)]建立了一个元数据编译的目标函数。该目标由四个标准组成，包括清洁度、多样性、平衡和信息量。如第VI-E节所述，数据剪枝通常是基于纯数学优化进行的。
- en: The second typical scenario is the regularized sample weighting or perturbation.
    The details are described in Sections VI-C4 and VI-D3\. For instance, Li et al. [[259](#bib.bib259)]
    devised a new objective function for the label perturbation strength, which can
    also reduce the Bayes error rate during training. Meister et al. [[260](#bib.bib260)]
    constructed a general form of regularization that can derive a series of label
    perturbation methods.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个典型场景是正则化样本加权或扰动。详细信息见第VI-C4和VI-D3节。例如，Li等人[[259](#bib.bib259)]设计了一种新的标签扰动强度目标函数，这也可以减少训练中的贝叶斯误差率。Meister等人[[260](#bib.bib260)]构建了一种通用的正则化形式，可以推导出一系列标签扰动方法。
- en: The third typical scenario is the constrained optimization, which embeds prior
    knowledge or conditions in data weighting, perturbation, or pruning into the constraints.
    For instance, Chai et al. [[261](#bib.bib261)] defined an optimization objective
    function with the constraints that each demographic group should have equal total
    weights in fairness-aware learning. The adversarial perturbation of multi-label
    learning is usually attained by solving constrained optimization problems [[262](#bib.bib262),
    [263](#bib.bib263), [264](#bib.bib264)]. Hu et al. [[263](#bib.bib263)] developed
    a novel loss for multi-label top-$k$ attack with the constraints that considers
    top-$k$ ranking relation among labels.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个典型场景是受限优化，它将先验知识或条件嵌入数据加权、扰动或剪枝的约束中。例如，Chai等人[[261](#bib.bib261)]定义了一个优化目标函数，约束条件是每个群体在公平学习中应具有相等的总权重。多标签学习的对抗扰动通常通过解决受限优化问题来实现[[262](#bib.bib262),
    [263](#bib.bib263), [264](#bib.bib264)]。Hu等人[[263](#bib.bib263)]开发了一种新的多标签top-$k$攻击损失，具有考虑标签之间top-$k$排名关系的约束。
- en: VI-F2 Technique combination
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-F2 技术组合
- en: Indeed, many learning algorithms do not employ a single data optimization technique.
    Instead, they combine different data optimization techniques. The following lists
    a few combination examples.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，许多学习算法并不使用单一的数据优化技术，而是结合了不同的数据优化技术。以下列举了一些组合示例。
- en: In data augmentation, many methods choose to generate samples in the first step
    and resample or reweight the samples in the second step. For instance, Cao et
    al. [[265](#bib.bib265)] dealt with grammatical error correction by using a data
    augmentation method during training and a data weighting method to automatically
    balance the importance of each kind of augmented samples. Liu et al. [[266](#bib.bib266)]
    generated new source phrases from a masked language model then sampled an aligned
    counterfactual target phrase for neural machine translation. Zang et al. [[267](#bib.bib267)]
    combined data augmentation and resampling for a long-tailed learning task.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据增强中，许多方法选择在第一步生成样本，并在第二步重新采样或加权样本。例如，Cao 等人 [[265](#bib.bib265)] 通过在训练过程中使用数据增强方法处理语法错误修正，并使用数据加权方法自动平衡每种增强样本的重要性。Liu
    等人 [[266](#bib.bib266)] 从掩蔽语言模型中生成新的源短语，然后为神经机器翻译采样对齐的反事实目标短语。Zang 等人 [[267](#bib.bib267)]
    结合了数据增强和重新采样，用于长尾学习任务。
- en: In data perturbation, different directions/granularity levels are usually combined
    in the same method. For example, adversarial perturbation belongs to the positive
    direction, while anti-adversarial perturbation belongs to the negative one. Zhao
    et al. [[268](#bib.bib268)] considered both category-wise and sample-wise factors
    to define the logit perturbation for imbalanced learning. Zhou et al. [[67](#bib.bib67)]
    combined both adversarial and anti-adversarial perturbations and theoretically
    revealed the superiority of the combination than the adversarial perturbation
    only.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据扰动中，通常将不同的方向/粒度级别结合在同一方法中。例如，对抗性扰动属于正方向，而反对抗性扰动属于负方向。Zhao 等人 [[268](#bib.bib268)]
    考虑了类别级和样本级因素来定义不平衡学习的 logit 扰动。Zhou 等人 [[67](#bib.bib67)] 结合了对抗性和反对抗性扰动，并理论上揭示了这种组合优于仅使用对抗性扰动。
- en: In data weighting, numerous methods combine it with data augmentation. Han et
    al. [[220](#bib.bib220)] combined uncertainty-based weighting and the classical
    augmentation method mixup. Chen et al. [[164](#bib.bib164)] combined effective
    number-based weighting and logit perturbation for long-tail learning tasks. In
    addition, some methods combine different granularity levels or different priority
    models. For example, Focal loss [[72](#bib.bib72)] employs both category-wise
    and sample-wise weight coefficients for each sample.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据加权中，许多方法将其与数据增强相结合。Han 等人 [[220](#bib.bib220)] 将基于不确定性的加权与经典的增强方法 mixup 结合起来。Chen
    等人 [[164](#bib.bib164)] 将有效数基加权与 logit 扰动结合用于长尾学习任务。此外，一些方法结合了不同的粒度级别或不同的优先级模型。例如，Focal
    loss [[72](#bib.bib72)] 对每个样本采用了类别级和样本级权重系数。
- en: VII Data optimization theories
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 数据优化理论
- en: There are a large amount of studies focusing on the theoretical aspects of data
    optimization. It is quite challenging to arrange existing theoretical studies
    into a clear roadmap. This study summarizes existing studies in the following
    two dimensions, including formalization and explanation.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 大量研究集中在数据优化的理论方面。将现有理论研究整理成清晰的路线图是相当具有挑战性的。本研究在形式化和解释两个维度上总结了现有研究。
- en: VII-A Formalization
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 形式化
- en: In order to theoretically analyze and understand the data optimization methods,
    it is essential to establish mathematical formulations. Statistical modeling is
    the primary tool for their formalization [[269](#bib.bib269), [270](#bib.bib270),
    [271](#bib.bib271)]. Basic assumptions are usually relied on. The most widely
    used assumptions for the statistical modeling include the following.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理论上分析和理解数据优化方法，建立数学公式是必不可少的。统计建模是其形式化的主要工具 [[269](#bib.bib269), [270](#bib.bib270),
    [271](#bib.bib271)]。通常依赖于基本假设。统计建模中最广泛使用的假设包括以下几种。
- en: •
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Gaussian distribution assumption. Many studies [[272](#bib.bib272), [29](#bib.bib29),
    [273](#bib.bib273), [274](#bib.bib274)] assume that data in each category conforms
    to a Gaussian distribution, which simplifies computation and inference compared
    to other complicated distributions [[275](#bib.bib275)].
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高斯分布假设。许多研究 [[272](#bib.bib272), [29](#bib.bib29), [273](#bib.bib273), [274](#bib.bib274)]
    假设每个类别的数据符合高斯分布，与其他复杂分布相比，这简化了计算和推断 [[275](#bib.bib275)]。
- en: •
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Equal class CPD assumption. In many learning studies [[276](#bib.bib276), [277](#bib.bib277)]
    excepting those for distribution drift, the class-conditional probability densities (CPD)
    of the training and testing sets are assumed to be identical.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相等类别CPD假设。在许多学习研究中[[276](#bib.bib276), [277](#bib.bib277)]，除了分布漂移的研究外，训练集和测试集的类别条件概率密度（CPD）被假设为相同。
- en: •
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Uniform distribution assumption. In many studies[[175](#bib.bib175), [278](#bib.bib278)],
    the distribution over categories in the testing set is assumed to be uniform.
    Some studies implicitly use this assumption by using modified losses such as the
    balanced accuracy or balanced test error [[279](#bib.bib279), [174](#bib.bib174)],
    even if the category proportions in the test corpus are not identical.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 均匀分布假设。在许多研究中[[175](#bib.bib175), [278](#bib.bib278)]，测试集中的类别分布被假设为均匀。有些研究通过使用平衡准确率或平衡测试错误等修改后的损失来隐式地应用这一假设[[279](#bib.bib279),
    [174](#bib.bib174)]，即使测试语料库中的类别比例不完全相同。
- en: •
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Linear boundary assumption. In many studies [[280](#bib.bib280), [115](#bib.bib115)],
    the decision boundary of the involved classifier is assumed to be linear. The
    decision boundary between two categories under the cross-entropy loss is linear.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性边界假设。在许多研究中[[280](#bib.bib280), [115](#bib.bib115)]，涉及的分类器的决策边界被假设为线性。在交叉熵损失下，两个类别之间的决策边界是线性的。
- en: Based on these assumptions, the data optimization problems are usually formalized
    into probabilistic, constrained optimization, or regularization-based problems.
    For example, Xu et al. [[281](#bib.bib281)] investigated importance weighting
    for covariate-shift generalization based on probabilistic analysis. Chen et al. [[282](#bib.bib282)]
    defined the classification accuracy based on posterior probability for zero-shot
    learning. Qraitem et al. [[283](#bib.bib283)] formalized a constrained linear
    program problem to investigate the effect of data resampling. Roh et al. [[284](#bib.bib284)]
    formulated a combinatorial optimization problem for the unbiased selection of
    samples in the presence of data corruption. In classical weighting paradigm such
    as SPL, data weighting is directly formalized in the optimization object consisting
    of the weighted loss and a regularizer. Zhang et al. [[285](#bib.bib285)] defined
    a re-weighted score function consisting of weighted loss and a sparsity regularization
    for causal discovery.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些假设，数据优化问题通常被形式化为概率、约束优化或基于正则化的问题。例如，Xu等人[[281](#bib.bib281)]基于概率分析研究了协变量偏移泛化的重要性加权。Chen等人[[282](#bib.bib282)]定义了基于后验概率的零样本学习分类准确率。Qraitem等人[[283](#bib.bib283)]将一个约束线性规划问题形式化以研究数据重采样的效果。Roh等人[[284](#bib.bib284)]在数据损坏的情况下，制定了一个组合优化问题用于无偏样本选择。在经典的加权范式如SPL中，数据加权直接在包含加权损失和正则化器的优化目标中进行形式化。Zhang等人[[285](#bib.bib285)]定义了一个重新加权的评分函数，包括加权损失和用于因果发现的稀疏性正则化。
- en: Jiang et al. [[286](#bib.bib286)] proposed a new adversarial perturbation generation
    method by adding a diversity-based regularization which measures the diversity
    of candidates. Hounie et al. [[287](#bib.bib287)] proposed a constrained learning
    problem for automatic data augmentation by combining conventional training loss
    and the constraints for invariance risk. Blum and Stangl [[288](#bib.bib288)]
    investigated the utility of fairness constraints in fair machine learning.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Jiang等人[[286](#bib.bib286)]提出了一种新的对抗性扰动生成方法，通过添加基于多样性的正则化来衡量候选样本的多样性。Hounie等人[[287](#bib.bib287)]通过结合传统训练损失和不变风险约束，提出了一种用于自动数据增强的约束学习问题。Blum和Stangl[[288](#bib.bib288)]研究了公平机器学习中公平性约束的效用。
- en: VII-B Explanation
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 解释
- en: Most theoretical studies on data optimization aim to explain why the existing
    methods are effective or ineffective.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数关于数据优化的理论研究旨在解释现有方法为何有效或无效。
- en: In data perception, researchers usually conducted theoretical analysis on the
    role of one typical data measure or leveraged the measure to understand the training
    process of DNNs. Doan et al. [[289](#bib.bib289)] conducted a theoretical analysis
    of catastrophic forgetting in continuous learning with neural-tangent-kernel overlap
    matrix. Chatterjee et al. [[290](#bib.bib290)] utilized the perception on gradients
    to explain the generalization of deep learning.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据感知中，研究人员通常对一种典型数据度量的作用进行理论分析，或利用该度量来理解深度神经网络的训练过程。Doan等人[[289](#bib.bib289)]通过神经切线核重叠矩阵对连续学习中的灾难性遗忘进行了理论分析。Chatterjee等人[[290](#bib.bib290)]利用对梯度的感知来解释深度学习的泛化能力。
- en: In data resampling, existing theoretical studies focus on importance sampling
    for deep learning. Katharopoulos and Fleuret [[110](#bib.bib110)] derived an estimator
    of the variance reduction achieved with importance sampling in deep learning.
    Katharopoulos and Fleuret [[291](#bib.bib291)] theoretically revealed that the
    loss value can be used as an alternative importance metric, and propose an efficient
    way to perform importance sampling for a deep model. Wang et al. [[292](#bib.bib292)]
    proposed an unweighted data sub-sampling method, and proved that the subset-model
    acquired through the method outperforms the full-set-model.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据重采样中，现有的理论研究集中于深度学习中的重要性采样。Katharopoulos 和 Fleuret[[110](#bib.bib110)]推导了通过重要性采样在深度学习中实现的方差减少的估计量。Katharopoulos
    和 Fleuret[[291](#bib.bib291)]理论上揭示了损失值可以作为一种替代的重要性度量，并提出了一种高效的深度模型重要性采样方法。Wang
    等人[[292](#bib.bib292)]提出了一种无权重数据子采样方法，并证明了通过该方法获得的子集模型优于全量模型。
- en: In data augmentation, more and more theoretical studies are performed. Dao et
    al. [[293](#bib.bib293)] established a theoretical framework for understanding
    data augmentation. According to their framework, data augmentation is approximated
    by two components, namely, first-order feature averaging and second-order variance
    regularization. Zhao et al. [[125](#bib.bib125)] defined an effective regularization
    term for adversarial data augmentation and theoretically derived it from the information
    bottleneck principle. Wu and He [[294](#bib.bib294)] investigated the theoretical
    issues for adversarial perturbations for multi-source domain adaptation. Gilmer
    et al. [[295](#bib.bib295)] also attempted to explain the adversarial samples.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据增强领域，越来越多的理论研究正在进行。Dao 等人[[293](#bib.bib293)]建立了一个理解数据增强的理论框架。根据他们的框架，数据增强被近似为两个组件，即一阶特征平均和二阶方差正则化。Zhao
    等人[[125](#bib.bib125)]定义了一个有效的对抗数据增强正则化项，并从信息瓶颈原理中理论推导出来。Wu 和 He[[294](#bib.bib294)]研究了多源领域适应中的对抗扰动的理论问题。Gilmer
    等人[[295](#bib.bib295)]也尝试解释了对抗样本。
- en: In data perturbation, most theoretical studies focus on the adversarial perturbation.
    Yi et al. [[296](#bib.bib296)] investigated the models trained by adversarial
    training on OOD data and justified that the input perturbation robust model in
    pre-training provides an initialization that generalizes well on downstream OOD
    data. Peck et al. [[297](#bib.bib297)] formally characterized adversarial perturbations
    by deriving lower bounds on the magnitudes of perturbations required to change
    the classification of neural networks. Some studies delved into the theoretical
    justification for label and logit perturbation. Xu et al. [[298](#bib.bib298)]
    analyzed the convergence SGD with label smoothing regularization and revealed
    that an appropriate LSR can help to speed up the convergence of SGD. Li et al. [[176](#bib.bib176)]
    theoretically analyzed the usefulness of logit adjustment in dealing with class
    imbalanced issues.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据扰动中，大多数理论研究集中于对抗扰动。Yi 等人[[296](#bib.bib296)]研究了在OOD数据上进行对抗训练的模型，并证明了在预训练阶段的输入扰动鲁棒模型提供了一个在下游OOD数据上能够很好地泛化的初始化。Peck
    等人[[297](#bib.bib297)]通过推导改变神经网络分类所需的扰动幅度的下界，正式描述了对抗扰动。一些研究深入探讨了标签和logit扰动的理论依据。Xu
    等人[[298](#bib.bib298)]分析了带标签平滑正则化的SGD的收敛性，并揭示了适当的LSR可以帮助加速SGD的收敛。Li 等人[[176](#bib.bib176)]理论分析了logit调整在处理类别不平衡问题中的有效性。
- en: In data weighting, Byrd and Lipton [[108](#bib.bib108)] investigated the role
    of importance in deep learning. Fang et al. [[232](#bib.bib232)] discussed the
    limitations of importance weighting and found that it suffers from a circular
    dependency. Meng et al. [[299](#bib.bib299)] analyzed the capability of the self-paced
    learning and provided an insightful interpretation of the effectiveness of several
    classical SPL variations. Weinshall et al. [[300](#bib.bib300)] proved that the
    rate of convergence of an ideal curriculum learning method is monotonically increasing
    with the learning difficulty of the training samples.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据加权中，Byrd 和 Lipton[[108](#bib.bib108)]研究了在深度学习中的重要性作用。Fang 等人[[232](#bib.bib232)]讨论了重要性加权的局限性，并发现它存在循环依赖。Meng
    等人[[299](#bib.bib299)]分析了自适应学习的能力，并对几种经典SPL变体的有效性提供了深刻的解释。Weinshall 等人[[300](#bib.bib300)]证明了理想课程学习方法的收敛速度随着训练样本学习难度的增加而单调增加。
- en: In data pruning, theoretical studies are relatively limited. Zhu et al. [[301](#bib.bib301)]
    revealed that distilled data lead to networks that are not calibratable. The reason
    lies in two folds, including a more concentrated distribution of the maximum logits
    and the loss of information that is semantically meaningful but unrelated to classification
    tasks. Dong et al. [[302](#bib.bib302)] emerged dataset distillation into the
    privacy community and theoretically revealed the connection between dataset distillation
    and differential privacy.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据剪枝方面，理论研究相对有限。Zhu 等人 [[301](#bib.bib301)] 揭示了精炼数据导致的网络无法进行校准。其原因有两个方面，包括最大
    logits 的分布更集中以及丢失了对分类任务无关但具有语义意义的信息。Dong 等人 [[302](#bib.bib302)] 将数据集精炼引入隐私社区，并理论上揭示了数据集精炼与差分隐私之间的联系。
- en: There are also studies which aim to reveal the intrinsic connections between
    two different technical paths. For instance, regularization is a widely used technique
    in deep learning [[169](#bib.bib169)], and several typical data optimization techniques
    are revealed to be a regularization method [[39](#bib.bib39), [303](#bib.bib303),
    [304](#bib.bib304)]. Therefore, intrinsic connections among these techniques can
    be established, which enlightens a better understanding of the involved technical
    paths and can envision novel inspirations or methods.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些研究旨在揭示两种不同技术路径之间的内在联系。例如，正则化是深度学习中广泛使用的技术 [[169](#bib.bib169)]，而一些典型的数据优化技术被揭示为正则化方法
    [[39](#bib.bib39), [303](#bib.bib303), [304](#bib.bib304)]。因此，可以建立这些技术之间的内在联系，这有助于更好地理解涉及的技术路径，并启发新的灵感或方法。
- en: '![Refer to caption](img/8aeb0594a225e973e6cd3cef6ac75da1.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8aeb0594a225e973e6cd3cef6ac75da1.png)'
- en: 'Figure 12: High-level connections for existing data optimization studies.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 现有数据优化研究的高层次联系。'
- en: 'TABLE I: Some data optimization methods in noisy-label learning.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 一些噪声标签学习中的数据优化方法。'
- en: '| Datasets | Resampling | Augmentation | Perturbation | Weighting | Pruning
    |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 重新采样 | 增强 | 扰动 | 权重调整 | 剪枝 |'
- en: '| CIFAR10 |  [[113](#bib.bib113)],  [[92](#bib.bib92)], [[292](#bib.bib292)]
    |  [[162](#bib.bib162)], [[305](#bib.bib305)], [[306](#bib.bib306)], [[307](#bib.bib307)]
    |  [[177](#bib.bib177)],  [[183](#bib.bib183)] |  [[100](#bib.bib100)], [[308](#bib.bib308)], [[309](#bib.bib309)], [[310](#bib.bib310)]
    |  [[311](#bib.bib311)],  [[312](#bib.bib312)], [[313](#bib.bib313)] |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR10 |  [[113](#bib.bib113)],  [[92](#bib.bib92)], [[292](#bib.bib292)]
    |  [[162](#bib.bib162)], [[305](#bib.bib305)], [[306](#bib.bib306)], [[307](#bib.bib307)]
    |  [[177](#bib.bib177)],  [[183](#bib.bib183)] |  [[100](#bib.bib100)], [[308](#bib.bib308)], [[309](#bib.bib309)], [[310](#bib.bib310)]
    |  [[311](#bib.bib311)],  [[312](#bib.bib312)], [[313](#bib.bib313)] |'
- en: '| CIFAR100 |  [[113](#bib.bib113)],  [[92](#bib.bib92)], [[292](#bib.bib292)]
    |  [[162](#bib.bib162)], [[305](#bib.bib305)], [[306](#bib.bib306)], [[307](#bib.bib307)]
    |  [[177](#bib.bib177)],  [[183](#bib.bib183)] |  [[100](#bib.bib100)], [[309](#bib.bib309)], [[310](#bib.bib310)]
    |  [[312](#bib.bib312)], [[313](#bib.bib313)] |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR100 |  [[113](#bib.bib113)],  [[92](#bib.bib92)], [[292](#bib.bib292)]
    |  [[162](#bib.bib162)], [[305](#bib.bib305)], [[306](#bib.bib306)], [[307](#bib.bib307)]
    |  [[177](#bib.bib177)],  [[183](#bib.bib183)] |  [[100](#bib.bib100)], [[309](#bib.bib309)], [[310](#bib.bib310)]
    |  [[312](#bib.bib312)], [[313](#bib.bib313)] |'
- en: '| Clothing1M |  [[292](#bib.bib292)] |  [[162](#bib.bib162)], [[307](#bib.bib307)], [[305](#bib.bib305)]
    |  [[177](#bib.bib177)],  [[183](#bib.bib183)] |  [[100](#bib.bib100)], [[310](#bib.bib310)]
    |  [[313](#bib.bib313)] |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| Clothing1M |  [[292](#bib.bib292)] |  [[162](#bib.bib162)], [[307](#bib.bib307)], [[305](#bib.bib305)]
    |  [[177](#bib.bib177)],  [[183](#bib.bib183)] |  [[100](#bib.bib100)], [[310](#bib.bib310)]
    |  [[313](#bib.bib313)] |'
- en: '| SVHN |  [[120](#bib.bib120)] |  [[314](#bib.bib314)] |  [[67](#bib.bib67)],
     [[177](#bib.bib177)],  [[183](#bib.bib183)] |  [[100](#bib.bib100)] |  [[245](#bib.bib245)]
    |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| SVHN |  [[120](#bib.bib120)] |  [[314](#bib.bib314)] |  [[67](#bib.bib67)],
     [[177](#bib.bib177)],  [[183](#bib.bib183)] |  [[100](#bib.bib100)] |  [[245](#bib.bib245)]
    |'
- en: '| WebVision |  [[315](#bib.bib315)] |  [[307](#bib.bib307)], [[305](#bib.bib305)]
    |  [[84](#bib.bib84)] |  [[308](#bib.bib308)], [[309](#bib.bib309)], [[310](#bib.bib310)]
    |  [[312](#bib.bib312)] |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| WebVision |  [[315](#bib.bib315)] |  [[307](#bib.bib307)], [[305](#bib.bib305)]
    |  [[84](#bib.bib84)] |  [[308](#bib.bib308)], [[309](#bib.bib309)], [[310](#bib.bib310)]
    |  [[312](#bib.bib312)] |'
- en: 'TABLE II: Some data optimization methods in imbalanced learning.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 一些不平衡学习中的数据优化方法。'
- en: '| Datasets | Resampling | Augmentation | Perturbation | Weighting | Dataset
    pruning |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 重新采样 | 增强 | 扰动 | 权重调整 | 数据集剪枝 |'
- en: '| CIFAR10-LT |  [[316](#bib.bib316)] |  [[316](#bib.bib316)],  [[317](#bib.bib317)]
    |  [[174](#bib.bib174)],  [[195](#bib.bib195)] |  [[72](#bib.bib72)],  [[46](#bib.bib46)]
    |  [[59](#bib.bib59)],  [[318](#bib.bib318)] |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR10-LT |  [[316](#bib.bib316)] |  [[316](#bib.bib316)],  [[317](#bib.bib317)]
    |  [[174](#bib.bib174)],  [[195](#bib.bib195)] |  [[72](#bib.bib72)],  [[46](#bib.bib46)]
    |  [[59](#bib.bib59)],  [[318](#bib.bib318)] |'
- en: '| CIFAR100-LT |  [[175](#bib.bib175)] |  [[164](#bib.bib164)],  [[317](#bib.bib317)]
    |  [[174](#bib.bib174)],  [[195](#bib.bib195)] |  [[72](#bib.bib72)],  [[46](#bib.bib46)]
    |  [[59](#bib.bib59)],  [[318](#bib.bib318)] |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR100-LT |  [[175](#bib.bib175)] |  [[164](#bib.bib164)],  [[317](#bib.bib317)]
    |  [[174](#bib.bib174)],  [[195](#bib.bib195)] |  [[72](#bib.bib72)],  [[46](#bib.bib46)]
    |  [[59](#bib.bib59)],  [[318](#bib.bib318)] |'
- en: '| iNaturallist |  [[175](#bib.bib175)] |  [[164](#bib.bib164)],  [[317](#bib.bib317)]
    |  [[174](#bib.bib174)],  [[195](#bib.bib195)] |  [[72](#bib.bib72)],  [[46](#bib.bib46)]
    |  [[59](#bib.bib59)] |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| iNaturallist |  [[175](#bib.bib175)] |  [[164](#bib.bib164)],  [[317](#bib.bib317)]
    |  [[174](#bib.bib174)],  [[195](#bib.bib195)] |  [[72](#bib.bib72)],  [[46](#bib.bib46)]
    |  [[59](#bib.bib59)] |'
- en: '| ImageNet-LT |  [[175](#bib.bib175)] |  [[164](#bib.bib164)],  [[317](#bib.bib317)]
    |  [[174](#bib.bib174)],  [[195](#bib.bib195)] |  [[72](#bib.bib72)],  [[46](#bib.bib46)]
    |  [[59](#bib.bib59)],  [[319](#bib.bib319)] |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet-LT |  [[175](#bib.bib175)] |  [[164](#bib.bib164)],  [[317](#bib.bib317)]
    |  [[174](#bib.bib174)],  [[195](#bib.bib195)] |  [[72](#bib.bib72)],  [[46](#bib.bib46)]
    |  [[59](#bib.bib59)],  [[319](#bib.bib319)] |'
- en: VIII Connections among different techniques
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 不同技术之间的联系
- en: 'The connections among different data optimizations techniques can be described
    by Fig. [12](#S7.F12 "Figure 12 ‣ VII-B Explanation ‣ VII Data optimization theories
    ‣ Data Optimization in Deep Learning: A Survey"). Four aspects, namely, perception,
    application scenarios, similarity/opposition, and theories, connect different
    methods within a technical path or across different paths.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '不同数据优化技术之间的联系可以通过图 [12](#S7.F12 "Figure 12 ‣ VII-B Explanation ‣ VII Data optimization
    theories ‣ Data Optimization in Deep Learning: A Survey") 来描述。四个方面，即感知、应用场景、相似性/对立性和理论，连接了在技术路径内或跨越不同路径的不同方法。'
- en: VIII-A Connections via data perception
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-A 通过数据感知的联系
- en: Data perception is the first (explicit or implicit) step in the data optimization
    pipeline. Methods along different technical paths introduced in Section VI may
    choose the same or similar quantities in perception. Therefore, quantities for
    data perception connect different methods. For example, many data optimization
    methods are on the basis of training loss in resampling [[320](#bib.bib320)],
    augmentation [[321](#bib.bib321)], perturbation [[47](#bib.bib47)], weighting [[299](#bib.bib299)],
    and subset selection [[322](#bib.bib322)]. Gradient is widely used in resampling [[110](#bib.bib110)],
    augmentation [[323](#bib.bib323)], perturbation [[67](#bib.bib67)], weighting [[61](#bib.bib61)],
    and dataset distillation [[236](#bib.bib236)]. Other quantities including margin
    and uncertainty are also used in different optimization techniques.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 数据感知是数据优化流程中的第一个（显式或隐式）步骤。在第 VI 节介绍的不同技术路径中的方法可能选择相同或类似的感知量。因此，数据感知的量连接了不同的方法。例如，许多数据优化方法基于训练损失，包括重新采样 [[320](#bib.bib320)]、增强 [[321](#bib.bib321)]、扰动 [[47](#bib.bib47)]、加权 [[299](#bib.bib299)]
    和子集选择 [[322](#bib.bib322)]。梯度在重新采样 [[110](#bib.bib110)]、增强 [[323](#bib.bib323)]、扰动 [[67](#bib.bib67)]、加权 [[61](#bib.bib61)]
    和数据集蒸馏 [[236](#bib.bib236)] 中广泛使用。其他量，如边距和不确定性，也用于不同的优化技术中。
- en: The utilization of the same or similar perception quantities demonstrates that
    these methods have the same or similar heuristic observations or theoretical inspirations.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 相同或类似的感知量的使用表明，这些方法具有相同或相似的启发式观察或理论灵感。
- en: VIII-B Connections via application scenarios
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B 通过应用场景的联系
- en: Most data optimization methods can be leveraged for the application scenarios
    discussed in Section IV-B.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据优化方法可以用于第 IV-B 节讨论的应用场景。
- en: One of the most focused scenarios of data optimization methods is noisy-label
    learning. Many classical methods are from resampling [[324](#bib.bib324)], augmentation,
    weighting, or perturbation. These are also dataset distillation studies for noisy-label
    datasets [[325](#bib.bib325)]. Table I shows some representative data optimization
    methods for noisy-label learning on five benchmark datasets CIFAR10 [[326](#bib.bib326)],
    CIFAR100 [[326](#bib.bib326)], Clothing1M [[327](#bib.bib327)], SVHN [[328](#bib.bib328)],
    and WebVision [[329](#bib.bib329)].
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 数据优化方法最受关注的场景之一是噪声标签学习。许多经典方法来源于重采样[[324](#bib.bib324)]、增强、加权或扰动。这些也是噪声标签数据集的
    数据提炼 研究[[325](#bib.bib325)]。表 I 显示了在五个基准数据集 CIFAR10 [[326](#bib.bib326)]、CIFAR100
    [[326](#bib.bib326)]、Clothing1M [[327](#bib.bib327)]、SVHN [[328](#bib.bib328)]
    和 WebVision [[329](#bib.bib329)] 上的噪声标签学习的一些代表性数据优化方法。
- en: Imbalanced learning is also among the most focused scenarios. Nearly all the
    listed data optimization technical paths have been used in imbalanced learning.
    Table II shows some representative data optimization methods for imbalanced learning
    on four benchmark datasets CIFAR10-LT [[46](#bib.bib46)], CIFAR100-LT [[46](#bib.bib46)],
    iNaturalist [[330](#bib.bib330)], and ImageNet-LT [[331](#bib.bib331)]. There
    are some studies employing more than one type of data optimization techniques
    such as ReMix [[316](#bib.bib316)], which combines resampling and augmentation,
    in Table II.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡学习也是最受关注的场景之一。列出的几乎所有数据优化技术路径都已应用于不平衡学习。表 II 显示了在四个基准数据集 CIFAR10-LT [[46](#bib.bib46)]、CIFAR100-LT
    [[46](#bib.bib46)]、iNaturalist [[330](#bib.bib330)] 和 ImageNet-LT [[331](#bib.bib331)]
    上的一些代表性数据优化方法。表 II 中还列出了一些采用多种数据优化技术的方法，例如 ReMix [[316](#bib.bib316)]，它结合了重采样和增强。
- en: Robust learning for adversarial attacks is another typical scenario. Karimireddy
    and Jaggi [[332](#bib.bib332)] employed resampling to design robust model in distributed
    learning. Data weighting [[210](#bib.bib210)] and dataset distillation [[333](#bib.bib333)]
    are also used in robust learning.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 针对对抗攻击的鲁棒学习是另一个典型场景。Karimireddy 和 Jaggi [[332](#bib.bib332)] 使用重采样来设计分布式学习中的鲁棒模型。数据加权
    [[210](#bib.bib210)] 和 数据提炼 [[333](#bib.bib333)] 也用于鲁棒学习。
- en: VIII-C Connections via similarity/opposition
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-C 相似性/对立性连接
- en: The similar and opposite relationships existing among the five technical paths
    are introduced in Section VI.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 存在于这五种技术路径之间的相似和对立关系在第 VI 节中介绍。
- en: Data resampling and weighting are closely related techniques, as their key steps
    are nearly the same. Therefore, in many studies on noisy-label learning and imbalanced
    learning, these two techniques are often considered as a single strategy.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 数据重采样和加权是密切相关的技术，因为它们的关键步骤几乎相同。因此，在许多噪声标签学习和不平衡学习的研究中，这两种技术通常被视为单一策略。
- en: 'Although data pruning and augmentation are opposite to each other, they have
    consistent ultimate goals in learning tasks. They are overlapped in terms of employed
    methodologies as shown in Fig. [13](#S8.F13 "Figure 13 ‣ VIII-C Connections via
    similarity/opposition ‣ VIII Connections among different techniques ‣ Data Optimization
    in Deep Learning: A Survey"). It is believable that more intrinsic connections
    can be explored for them.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据修剪和数据增强是彼此对立的，它们在学习任务中的最终目标却是一致的。它们在所采用的方法上有所重叠，如图[13](#S8.F13 "图 13 ‣ VIII-C
    相似性/对立性连接 ‣ VIII 各种技术之间的连接 ‣ 深度学习中的数据优化：综述")所示。可以相信，它们之间还可以探索更多内在的联系。
- en: '![Refer to caption](img/97f90b0a7f1097201b575823ec5ab100.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/97f90b0a7f1097201b575823ec5ab100.png)'
- en: 'Figure 13: Connection between augmentation and pruning.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：增强与修剪之间的连接。
- en: In the data resampling, weighting, and perturbation, the assignment manners
    for the sampling rate, weighting score, and perturbation variable are quite similar.
    In addition to the classical importance score, both meta learning [[334](#bib.bib334)]
    and adversarial strategy [[120](#bib.bib120)] have also been used in data resampling.
    Regularization-based manner is used in nearly all the data optimization paths
    except resampling. Due to space constraints, methods with different assignment
    manners are not summarized in a table as those in Section VIII-B.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据重采样、加权和扰动中，采样率、加权分数和扰动变量的分配方式非常相似。除了经典的重要性分数外，元学习 [[334](#bib.bib334)] 和对抗策略
    [[120](#bib.bib120)] 也已用于数据重采样。除重采样外，几乎所有的数据优化路径中都使用了基于正则化的方法。由于空间限制，具有不同分配方式的方法未在表格中总结，如第
    VIII-B 节所示。
- en: There are other opposite relationships, such as undersampling vs. oversampling,
    easy-first weighting vs. hard-first weighting, positive perturbation vs. negative
    perturbation, and explicit augmentation vs. implicit augmentation. Both methodologies
    in these opposite relationships have been demonstrated to be effective, aligning
    with the proverb “All roads lead to Rome”.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他相反的关系，如欠采样与过采样、简单优先加权与困难优先加权、正扰动与负扰动以及显式增强与隐式增强。这些相反关系中的两种方法论已被证明是有效的，与谚语“条条大路通罗马”相一致。
- en: VIII-D Connections via theory
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-D 通过理论的联系
- en: 'There are some common theoretical issues, analyses, and conclusions heavily
    influencing most data optimization techniques. They are the natural connections
    among different techniques. Several examples are listed as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一些共同的理论问题、分析和结论，这些问题和结论对大多数数据优化技术产生了重大影响。它们是不同技术之间的自然联系。以下列出了一些示例：
- en: •
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Theoretical issues in data perception. A solid theoretical basis for data perception
    in data optimization is lacking, even though most data optimization methods implicitly
    or explicitly rely on the perception for the training data. For instance, many
    methods from resampling, weighting, and perturbation are based on dividing samples
    into easy and hard. Nevertheless, there is not yet a widely accepted learning
    difficulty measure with a rigorous theoretical basis in the literature. More than
    ten types of learning difficulty measures are utilized to distinguish easy from
    hard samples in previous literature. A theoretical formulation for data perception
    is of great importance.
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据感知中的理论问题。在数据优化中，数据感知的理论基础尚不充分，尽管大多数数据优化方法在训练数据中隐含或明确地依赖于感知。例如，许多来自重采样、加权和扰动的方法都基于将样本分为简单和困难。然而，文献中尚未有一种广泛接受的具有严谨理论基础的学习难度度量方法。在之前的文献中，使用了十多种学习难度度量来区分简单样本和困难样本。数据感知的理论公式化具有重要意义。
- en: •
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Probabilistic density (ratio) estimation. Many data optimization methods, especially
    data resampling and weighting, heavily rely on the probabilistic density (ratio)
    estimation. The most representative method is the importance sampling. In learning
    difficulty-based weighting, the probabilistic density ratio, in terms of learning
    difficulty, is revealed to determine the priority mode [[217](#bib.bib217)], namely,
    easy/medium/hard-first.
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 概率密度（比率）估计。许多数据优化方法，特别是数据重采样和加权，严重依赖于概率密度（比率）估计。最具代表性的方法是重要性采样。在基于学习难度的加权中，学习难度下的概率密度比率被揭示以确定优先模式[[217](#bib.bib217)]，即，简单/中等/困难优先。
- en: •
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Regularization-based explanation. Many data optimization methods are considered
    as a type of regularization, including data augmentation and perturbation. In
    these methods, data optimization performs implicit model regularization other
    than explicit regularization that directly works on model parameters. Regularization
    is not always beneficial as over-regularization may occur. Li et al. [[335](#bib.bib335)]
    pointed out that large amount of augmented noisy data could lead to over-regularization
    and proposed an adaptive augmentation method. Adversarial training may result
    in robust overfitting [[336](#bib.bib336)].
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于正则化的解释。许多数据优化方法被视为一种正则化，包括数据增强和扰动。在这些方法中，数据优化执行的是隐式模型正则化，而不是直接作用于模型参数的显式正则化。正则化并不总是有利，因为可能会发生过度正则化。Li等人[[335](#bib.bib335)]指出，大量增强的噪声数据可能导致过度正则化，并提出了一种自适应增强方法。对抗训练可能导致稳健的过拟合[[336](#bib.bib336)]。
- en: •
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generalization bound for data optimization. Many studies choose to deduce a
    mathematical bound for the generalization risk in terms of the empirical risk
    and variables related to the data optimization. This manner can theoretically
    explain the utility of the involved data optimization. Xiao et al. [[337](#bib.bib337)]
    derived stability-based generalization bounds for stochastic gradient descent
    (SGD) on the loss with adversarial perturbations. Xu et al. [[115](#bib.bib115)]
    established a new generalization bound that reflects how importance weighting
    leads to the interplay between the empirical risk and the deviation between the
    source and target distributions.
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据优化的一般化界限。许多研究选择从经验风险和与数据优化相关的变量的角度推导出一般化风险的数学界限。这种方式可以从理论上解释所涉及的数据优化的效用。Xiao
    等人[[337](#bib.bib337)] 推导了具有对抗性扰动的随机梯度下降（SGD）的基于稳定性的泛化界限。Xu 等人[[115](#bib.bib115)]
    建立了一个新的泛化界限，反映了重要性加权如何导致经验风险与源分布和目标分布之间的偏差之间的相互作用。
- en: The progress in each of the above theoretical aspects will promote the advancement
    of many data optimization methods in different technical paths. Hopefully, this
    survey will promote the mutual understanding of the referred technical paths.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 以上每个理论方面的进展都将促进许多数据优化方法在不同技术路径上的发展。希望这项调查能促进对所提及技术路径的相互理解。
- en: IX Future directions
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 未来方向
- en: This section summarizes some research directions deserving further exploring.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 本节总结了一些值得进一步探索的研究方向。
- en: IX-A Principles of data optimization
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-A 数据优化原理
- en: 'Up till now, there has been no consensus theoretical framework that is suitable
    for all or most technical paths. There are some studies aiming to establish the
    connection between two different technical paths, such as resampling vs. weighting [[338](#bib.bib338)].
    Many open problems or controversies remain unsolved. For example, there is no
    ideal answer for which resampling strategy should be employed first: oversampling
    or undersampling? Megahed et al. [[104](#bib.bib104)] suggested that undersampling
    should be used firstly, whereas Xie et al. [[275](#bib.bib275)] demonstrated that
    oversampling is effective. Likewise, although Zhou et al. [[217](#bib.bib217)]
    provided an initial answer for the choice of easy-first and hard-first weighting
    strategies, a solid theoretical framework is still lacking in their study.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，尚无适用于所有或大多数技术路径的共识性理论框架。一些研究旨在建立两种不同技术路径之间的联系，例如重采样与加权[[338](#bib.bib338)]。许多未解决的开放问题或争议仍然存在。例如，对于应首先使用哪种重采样策略：过采样还是欠采样？Megahed
    等人[[104](#bib.bib104)] 建议首先使用欠采样，而 Xie 等人[[275](#bib.bib275)] 证明了过采样的有效性。同样，尽管
    Zhou 等人[[217](#bib.bib217)] 为简单优先和困难优先加权策略的选择提供了初步答案，但他们的研究中仍缺乏一个坚实的理论框架。
- en: Moreover, even for a single data optimization method, multiple explanations
    from different views may exist. The explanation for label smoothing is a typical
    example. At least four studies provide empirical or theoretical explanations for
    it [[298](#bib.bib298), [339](#bib.bib339), [340](#bib.bib340), [260](#bib.bib260)].
    Regarding the effectiveness of adversarial samples, some researchers have pointed
    out that adversarial samples are useful features [[270](#bib.bib270)], while some
    other researchers investigated it in terms of gradient regularization [[341](#bib.bib341)].
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，即使对于单一的数据优化方法，也可能存在来自不同视角的多种解释。标签平滑的解释就是一个典型的例子。至少有四项研究为其提供了经验或理论解释[[298](#bib.bib298),
    [339](#bib.bib339), [340](#bib.bib340), [260](#bib.bib260)]。关于对抗样本的有效性，一些研究人员指出对抗样本是有用的特征[[270](#bib.bib270)]，而另一些研究人员则从梯度正则化的角度进行研究[[341](#bib.bib341)]。
- en: Consequently, the construction of the data optimization principles is of great
    importance, as it can promote the establishing of a unified and solid theoretical
    framework which can be used to analyze and understand of each data optimization
    technical path. There have been studies on the first principle for the design
    of DNNs [[342](#bib.bib342)]. To explore the principles for data optimization,
    a unified mathematical formalization tool is required and large-scale empirical
    studies (e.g., [[343](#bib.bib343)], [[344](#bib.bib344)]) will also be helpful.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，构建数据优化原理具有重要意义，因为这可以促进建立一个统一而扎实的理论框架，用于分析和理解每种数据优化技术路径。已经有关于DNN设计第一原理的研究[[342](#bib.bib342)]。要探索数据优化的原理，需要一个统一的数学形式化工具，并且大规模的实证研究（例如，[[343](#bib.bib343)]，[[344](#bib.bib344)]）也将是有帮助的。
- en: IX-B Interpretable data optimization
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-B 可解释的数据优化
- en: Interpretable data optimization refers to the explanation for the involved data
    optimization techniques in terms of how and which aspects they affect the training
    process of DNNs. Although interpretable deep learning receives much attention
    in recent years [[345](#bib.bib345)], it focuses on DNN models other than the
    training processing in which data optimization techniques are involved. Interpretable
    data optimization is an under-explored research topic and there are limited studies
    on this topic [[346](#bib.bib346)]. The well explanation of how and which aspects
    of a data optimization method affects a specific training process is significant
    beneficial for the design or selecting of more effective optimization methods.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释的数据优化指的是对所涉及的数据优化技术的解释，包括它们如何以及哪些方面影响DNN的训练过程。尽管近年来可解释的深度学习受到了广泛关注[[345](#bib.bib345)]，但其关注的是DNN模型，而非涉及数据优化技术的训练过程。可解释的数据优化是一个尚未充分探索的研究课题，目前关于该主题的研究有限[[346](#bib.bib346)]。对数据优化方法如何以及哪些方面影响特定训练过程的详细解释对设计或选择更有效的优化方法具有重要意义。
- en: The aforementioned theoretical studies on data optimization provide partial
    explanations for the corresponding data optimization method. However, the partial
    explanations are concentrated in common aspects across different learning tasks.
    How and which aspects for the involved method on a specific learning task remain
    unexplored.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 前述关于数据优化的理论研究为相关的数据优化方法提供了部分解释。然而，这些部分解释集中在不同学习任务中的共同方面。对于特定学习任务中的具体方法，如何以及哪些方面尚未被探索。
- en: The interpretable deep learning area has raised many effective methodologies.
    Recently, researchers have attempted to introduce interpretable methodologies
    to explore the data optimization methods. Zelaya and Vladimiro [[347](#bib.bib347)]
    explored metrics to quantify the effect of some data-processing steps such as
    undersampling and data augmentation on the model performance. Hopefully, more
    and more studies on explainable data optimization appear in the future.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释的深度学习领域已经提出了许多有效的方法论。最近，研究人员尝试引入可解释的方法来探索数据优化方法。Zelaya和Vladimiro[[347](#bib.bib347)]探索了量化某些数据处理步骤（如欠采样和数据增强）对模型性能影响的指标。希望未来会有越来越多的可解释数据优化研究出现。
- en: IX-C Human-in-the-loop data optimization
  id: totrans-392
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-C 人工智能数据优化
- en: Recently, human-in-the-loop (HITL) deep learning receives increasing attention
    in the AI community [[348](#bib.bib348)]. With out human’s participants, high-quality
    training samples are not intractable to obtain. Naturally, HITL data optimization
    can also be beneficial for deep learning. Collins et al. [[349](#bib.bib349)]
    investigated HITL mixup and indicated that collating humans’ perceptions on augmented
    samples could impact model performance. Wallace et al. [[350](#bib.bib350)] proposed
    HITL adversarial generation, where human authors are guided to break models. Agarwal
    et al. [[351](#bib.bib351)] proposed Variance of Gradients (VoG) to measure samples’
    learning difficulty and ranked samples by VoG. Then, a tractable subset of the
    most difficult samples is selected for HITL auditing. Overall, research on HITL
    data optimization is in the early stage.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，人机交互（HITL）深度学习在AI社区中受到越来越多的关注[[348](#bib.bib348)]。没有人类参与，高质量的训练样本是难以获得的。因此，HITL数据优化也可以对深度学习有利。Collins等人[[349](#bib.bib349)]研究了HITL混合技术，并指出整合人类对增强样本的感知可能会影响模型性能。Wallace等人[[350](#bib.bib350)]提出了HITL对抗生成，其中人类作者被指导去破解模型。Agarwal等人[[351](#bib.bib351)]提出了梯度方差（VoG）来测量样本的学习难度，并根据VoG对样本进行排名。然后，选择一小部分最困难的样本进行HITL审计。总体而言，HITL数据优化的研究还处于早期阶段。
- en: IX-D Data optimization for new challenges
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-D 新挑战的数据优化
- en: 'New challenges are constantly emerging in deep learning applications. We take
    the following three recent challenges as examples to illustrate the future direction
    of data optimization:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习应用中，新挑战不断涌现。我们以以下三个近期挑战为例，来说明数据优化的未来方向：
- en: •
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Open-world learning. This learning scenario confronts the challenge of out-of-distribution (OOD)
    samples. Wu et al. [[352](#bib.bib352)] investigated noisy-label learning under
    the open-world setting, in which both OOD and noisy samples exist. Some other
    studies investigate cases when ODD meets imbalanced learning [[353](#bib.bib353)]
    and adversarial robustness [[354](#bib.bib354)].
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开放世界学习。这种学习场景面临分布外（OOD）样本的挑战。Wu等人[[352](#bib.bib352)]研究了开放世界设置下的噪声标签学习，其中存在OOD和噪声样本。其他一些研究探讨了当OOD遇到不平衡学习[[353](#bib.bib353)]和对抗鲁棒性[[354](#bib.bib354)]的情况。
- en: •
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Large-model training. Large models especially the large language models [[355](#bib.bib355)]
    have achieved great success in recent years. Data optimization can also take effect
    in the training of large models. Wei et al. [[356](#bib.bib356)] investigated
    the condensation of prompts and promising results are obtained. Contrarily, Jiang
    et al. [[357](#bib.bib357)] leveraged prompt augmentation to calibrate large language
    models. Many issues investigated in conventional deep learning tasks may also
    exist for large-model training, e.g., prompt valuation.
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大模型训练。近年来，大模型特别是大型语言模型[[355](#bib.bib355)]取得了巨大的成功。数据优化在大模型的训练中也能发挥作用。Wei等人[[356](#bib.bib356)]研究了提示的凝聚，并获得了有前景的结果。相反，Jiang等人[[357](#bib.bib357)]利用提示增强来校准大型语言模型。许多在传统深度学习任务中研究的问题可能在大模型训练中也存在，例如，提示评估。
- en: •
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Multi-modal learning. With the development of data sensing and collection technology,
    multi-modal data are avaliable in more and more real tasks [[358](#bib.bib358)].
    Consequently, many learning tasks are actually multi-modal learning. As each sample
    consists of raw data/features from different modalities, the data perception for
    multi-modal samples should be different from that for conventional single-modal
    samples. The data optimization methods are likewise different from conventional
    methods [[359](#bib.bib359)].
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多模态学习。随着数据感知和采集技术的发展，多模态数据在越来越多的实际任务中变得可用[[358](#bib.bib358)]。因此，许多学习任务实际上是多模态学习。由于每个样本由来自不同模态的原始数据/特征组成，因此对多模态样本的数据感知应与传统单模态样本不同。数据优化方法也不同于传统方法[[359](#bib.bib359)]。
- en: IX-E Data optimization agent
  id: totrans-402
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-E 数据优化代理
- en: Given a concrete learning task, a selection dilemma occurs for the tremendous
    data optimization techniques. There have been studies on the automatic data optimization
    such as automatic data augmentation [[287](#bib.bib287)]. Nevertheless, existing
    automatic data optimization methods still focus on a particular type of technical
    path rather than the types across different technical paths [[360](#bib.bib360),
    [361](#bib.bib361)]. A more general data optimization agent can be trained by
    iteratively training on a large number of deep learning tasks via reinforcement
    learning.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具体的学习任务，存在选择众多数据优化技术的困境。已有关于自动数据优化的研究，如自动数据增强[[287](#bib.bib287)]。然而，现有的自动数据优化方法仍然集中在某一特定技术路径上，而不是不同技术路径之间的类型[[360](#bib.bib360),
    [361](#bib.bib361)]。可以通过强化学习在大量深度学习任务上迭代训练，从而训练出一个更通用的数据优化代理。
- en: Fig. 14 shows a possible mean to construct a data optimization agent. New learning
    tasks are compiled based on existing classical tasks via operations such as noise
    adding, and class proportion re-distributing. The candidate of data optimization
    operators are from arbitrary optimization techniques or a single technique introduced
    in Section VI. A powerful data optimization agent can then be trained via reinforcement
    learning based on compiled learning tasks and their rewards.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 图14展示了一种构建数据优化代理的可能方法。新的学习任务基于现有的经典任务，通过添加噪声、重新分配类别比例等操作进行编译。数据优化操作的候选者来自于第六节介绍的任意优化技术或单一技术。然后，可以通过强化学习根据编译的学习任务及其奖励来训练一个强大的数据优化代理。
- en: '![Refer to caption](img/9683d17f02eea6dd1501ae1f89f835bc.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9683d17f02eea6dd1501ae1f89f835bc.png)'
- en: 'Figure 14: The training of a data optimization agent.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：数据优化代理的训练。
- en: X Conclusions
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: X 结论
- en: This paper aims to summarize a wide range of learning methods within an independent
    deep learning realm, namely, data optimization. A taxonomy for data optimization,
    as well as fine-granularity sub-taxonomies, is established for existing studies
    on data optimization. Connections among different methods are discussed, and potential
    future directions are presented. It is noteworthy that many classical methods,
    such as dropout, are essentially data optimization methods. In our future work,
    we will explore a more fundamental and unified viewpoint on data optimization,
    and develop a more comprehensive taxonomy to incorporate more classical methods.
    We hope that this study can inspire more researchers to gain insight into data-centric
    AI.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在总结独立深度学习领域内的一系列学习方法，即数据优化。为现有的数据优化研究建立了一个数据优化的分类法，以及细化的子分类法。讨论了不同方法之间的联系，并提出了潜在的未来方向。值得注意的是，许多经典方法，例如dropout，实际上是数据优化方法。在我们的未来工作中，我们将探索数据优化的更基础和统一的观点，并开发一个更全面的分类法，以纳入更多的经典方法。我们希望这项研究能激发更多研究人员对数据中心人工智能的深入了解。
- en: References
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. E. Whang, Y. Roh, H. Song, and J.-G. Lee, “Data collection and quality
    challenges in deep learning: A data-centric ai perspective,” *The VLDB Journal*,
    vol. 32, no. 4, pp. 791–813, 2023.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] S. E. Whang, Y. Roh, H. Song 和 J.-G. Lee，“深度学习中的数据收集和质量挑战：一种数据中心人工智能视角，”*The
    VLDB Journal*，第32卷，第4期，页码791–813，2023年。'
- en: '[2] M. H. Jarrahi, A. Memariani, and S. Guha, “The principles of data-centric
    ai,” *Communications of the ACM*, vol. 66, no. 8, pp. 84–92, 2023.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] M. H. Jarrahi, A. Memariani 和 S. Guha，“数据中心人工智能的原则，”*Communications of
    the ACM*，第66卷，第8期，页码84–92，2023年。'
- en: '[3] Y. Liang, D. Huang, C.-D. Wang, and P. S. Yu, “Multi-view graph learning
    by joint modeling of consistency and inconsistency,” *IEEE TNNLS*, pp. 1–15, 2022.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Liang, D. Huang, C.-D. Wang 和 P. S. Yu，“通过一致性和不一致性联合建模的多视图图学习，”*IEEE
    TNNLS*，页码1–15，2022年。'
- en: '[4] P. Zhu, X. Yao, Y. Wang, M. Cao, B. Hui, S. Zhao, and Q. Hu, “Latent heterogeneous
    graph network for incomplete multi-view learning,” *IEEE TMM*, vol. 25, pp. 3033–3045,
    2023.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] P. Zhu, X. Yao, Y. Wang, M. Cao, B. Hui, S. Zhao 和 Q. Hu，“用于不完整多视图学习的潜在异质图网络，”*IEEE
    TMM*，第25卷，页码3033–3045，2023年。'
- en: '[5] L. Brigato and L. Iocchi, “A close look at deep learning with small data,”
    in *ICPR*, 2021, pp. 2490–2497.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] L. Brigato 和 L. Iocchi，“深入了解小数据的深度学习，”发表于*ICPR*，2021年，页码2490–2497。'
- en: '[6] V. Birodkar, H. Mobahi, and S. Bengio, “Semantic redundancies in image-classification
    datasets: The 10% you don’t need,” *arXiv:1901.11409*, 2019.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] V. Birodkar, H. Mobahi 和 S. Bengio，“图像分类数据集中的语义冗余：你不需要的10%,” *arXiv:1901.11409*，2019年。'
- en: '[7] Y. Yu, S. Khadivi, and J. Xu, “Can data diversity enhance learning generalization?”
    in *COLING*, 2022, pp. 4933–4945.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Yu, S. Khadivi, 和 J. Xu, “数据多样性是否能增强学习的泛化能力？”在*COLING*，2022年，页码4933–4945。'
- en: '[8] J. Lu, A. Liu, F. Dong, F. Gu, J. Gama, and G. Zhang, “Learning under concept
    drift: A review,” *IEEE TKDE*, vol. 31, no. 12, pp. 2346–2363, 2018.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Lu, A. Liu, F. Dong, F. Gu, J. Gama, 和 G. Zhang, “概念漂移下的学习：综述”，*IEEE
    TKDE*，第31卷，第12期，页码2346–2363，2018年。'
- en: '[9] W. Wang, R. Wang, L. Wang, Z. Wang, and A. Ye, “Towards a robust deep neural
    network against adversarial texts: A survey,” *IEEE TKDE*, vol. 35, no. 3, pp.
    3159–3179, 2023.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] W. Wang, R. Wang, L. Wang, Z. Wang, 和 A. Ye, “朝着强健的深度神经网络对抗对抗性文本：综述”，*IEEE
    TKDE*，第35卷，第3期，页码3159–3179，2023年。'
- en: '[10] Y. Wu, L. Zhang, and X. Wu, “On convexity and bounds of fairness-aware
    classification,” in *WWW*, 2019, pp. 3356–3362.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Wu, L. Zhang, 和 X. Wu, “关于公平分类的凸性和界限”，在*WWW*，2019年，页码3356–3362。'
- en: '[11] P. Xiong, S. Buffett, S. Iqbal, P. Lamontagne, M. Mamun, and H. Molyneaux,
    “Towards a robust and trustworthy machine learning system development: An engineering
    perspective,” *JISA*, vol. 65, p. 103121, 2022.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] P. Xiong, S. Buffett, S. Iqbal, P. Lamontagne, M. Mamun, 和 H. Molyneaux,
    “朝着强健且值得信赖的机器学习系统开发迈进：工程视角”，*JISA*，第65卷，页码103121，2022年。'
- en: '[12] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical
    risk minimization,” *ICLR*, 2018.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] H. Zhang, M. Cisse, Y. N. Dauphin, 和 D. Lopez-Paz, “mixup：超越经验风险最小化”，*ICLR*，2018年。'
- en: '[13] R. Yao and O. Wu, “Compensation learning,” *arXiv:2107.11921*, 2022.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] R. Yao 和 O. Wu, “补偿学习”，*arXiv:2107.11921*，2022年。'
- en: '[14] X. Wang, L. Jing, Y. Lyu, M. Guo, J. Wang, H. Liu, J. Yu, and T. Zeng,
    “Deep generative mixture model for robust imbalance classification,” *IEEE TPAMI*,
    vol. 45, no. 3, pp. 2897–2912, 2023.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] X. Wang, L. Jing, Y. Lyu, M. Guo, J. Wang, H. Liu, J. Yu, 和 T. Zeng, “用于强健不平衡分类的深度生成混合模型”，*IEEE
    TPAMI*，第45卷，第3期，页码2897–2912，2023年。'
- en: '[15] H. He and E. A. Garcia, “Learning from imbalanced data,” *IEEE TKDE*,
    vol. 21, no. 9, pp. 1263–1284, 2009.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] H. He 和 E. A. Garcia, “从不平衡数据中学习”，*IEEE TKDE*，第21卷，第9期，页码1263–1284，2009年。'
- en: '[16] Y. Zhang, B. Kang, B. Hooi, S. Yan, and J. Feng, “Deep long-tailed learning:
    A survey,” *IEEE TPAMI*, vol. 45, no. 9, pp. 10 795–10 816, 2023.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Y. Zhang, B. Kang, B. Hooi, S. Yan, 和 J. Feng, “深度长尾学习：综述”，*IEEE TPAMI*，第45卷，第9期，页码10 795–10 816，2023年。'
- en: '[17] G. Algan and I. Ulusoy, “Image classification with deep learning in the
    presence of noisy labels: A survey,” *Knowledge-Based Systems*, vol. 215, no. 5,
    p. 106771, 2021.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] G. Algan 和 I. Ulusoy, “在存在噪声标签的情况下使用深度学习进行图像分类：综述”，*Knowledge-Based Systems*，第215卷，第5期，页码106771，2021年。'
- en: '[18] H. Song, M. Kim, D. Park, Y. Shin, and J.-G. Lee, “Learning from noisy
    labels with deep neural networks: A survey,” *IEEE TNNLS*, pp. 1–19, 2022.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] H. Song, M. Kim, D. Park, Y. Shin, 和 J.-G. Lee, “基于深度神经网络的噪声标签学习：综述”，*IEEE
    TNNLS*，页码1–19，2022年。'
- en: '[19] X. Cao, W. Bu, S. Huang, M. Zhang, I. W. Tsang, Y. S. Ong, and J. T. Kwok,
    “A survey of learning on small data,” *arXiv:2207.14443*, 2022.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] X. Cao, W. Bu, S. Huang, M. Zhang, I. W. Tsang, Y. S. Ong, 和 J. T. Kwok,
    “小数据上的学习综述”，*arXiv:2207.14443*，2022年。'
- en: '[20] Y. Wang, Q. Yao, J. Kwok, and L. M. Ni, “Generalizing from a few examples:
    A survey on few-shot learning,” *ACM computing surveys*, vol. 53, no. 3, pp. 1–34,
    2020.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Wang, Q. Yao, J. Kwok, 和 L. M. Ni, “从少量示例中泛化：关于少样本学习的综述”，*ACM computing
    surveys*，第53卷，第3期，页码1–34，2020年。'
- en: '[21] L. Yuan, H. Li, B. Xia, C. Gao, M. Liu, W. Yuan, and X. You, “Recent advances
    in concept drift adaptation methods for deep learning,” in *IJCAI*, 2022, pp.
    5654–5661.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] L. Yuan, H. Li, B. Xia, C. Gao, M. Liu, W. Yuan, 和 X. You, “深度学习的概念漂移适应方法的最新进展”，在*IJCAI*，2022年，页码5654–5661。'
- en: '[22] A. Diez-Olivan, P. Ortego, J. D. Ser, I. Landa-Torres, D. Galar, D. Camacho,
    and B. Sierra, “Adaptive dendritic cell-deep learning approach for industrial
    prognosis under changing conditions,” *IEEE TII*, vol. 17, no. 11, pp. 7760–7770,
    2021.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Diez-Olivan, P. Ortego, J. D. Ser, I. Landa-Torres, D. Galar, D. Camacho,
    和 B. Sierra, “适应性树突细胞深度学习方法用于变化条件下的工业预测”，*IEEE TII*，第17卷，第11期，页码7760–7770，2021年。'
- en: '[23] J. Gama, I. Žliobaitė, A. Bifet, M. Pechenizkiy, and A. Bouchachia, “A
    survey on concept drift adaptation,” *ACM Computing Surveys*, vol. 46, no. 4,
    pp. 1–37, 2014.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. Gama, I. Žliobaitė, A. Bifet, M. Pechenizkiy, 和 A. Bouchachia, “概念漂移适应的综述”，*ACM
    Computing Surveys*，第46卷，第4期，页码1–37，2014年。'
- en: '[24] A. S. Iwashita and J. P. Papa, “An overview on concept drift learning,”
    *IEEE Access*, vol. 7, pp. 1532–1547, 2019.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. S. Iwashita 和 J. P. Papa, “概念漂移学习综述”，*IEEE Access*，第7卷，页码1532–1547，2019年。'
- en: '[25] S. H. Silva and P. Najafirad, “Opportunities and challenges in deep learning
    adversarial robustness: A survey,” *arXiv:2007.00753*, 2020.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. H. Silva 和 P. Najafirad, “深度学习对抗鲁棒性的机遇与挑战：综述”，*arXiv:2007.00753*，2020年。'
- en: '[26] J. Xu, J. Chen, S. You, Z. Xiao, Y. Yang, and J. Lu, “Robustness of deep
    learning models on graphs: A survey,” *AI Open*, vol. 2, pp. 69–78, 2021.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. Xu, J. Chen, S. You, Z. Xiao, Y. Yang, 和 J. Lu，“图上深度学习模型的鲁棒性：综述，”*AI
    Open*，第2卷，页码69–78，2021年。'
- en: '[27] S. Goyal, S. Doddapaneni, M. M. Khapra, and B. Ravindran, “A survey of
    adversarial defenses and robustness in nlp,” *ACM Computing Surveys*, vol. 55,
    no. 14s, pp. 1–39, 2023.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Goyal, S. Doddapaneni, M. M. Khapra, 和 B. Ravindran，“自然语言处理中的对抗性防御和鲁棒性综述，”*ACM
    Computing Surveys*，第55卷，第14期，页码1–39，2023年。'
- en: '[28] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, “A survey
    on bias and fairness in machine learning,” *ACM Computing Surveys*, vol. 54, no. 6,
    pp. 1–35, 2021.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, 和 A. Galstyan，“机器学习中的偏见和公平性综述，”*ACM
    Computing Surveys*，第54卷，第6期，页码1–35，2021年。'
- en: '[29] A. Petrović, M. Nikolić, S. Radovanović, B. Delibašić, and M. Jovanović,
    “Fair: Fair adversarial instance re-weighting,” *Neurocomputing*, vol. 476, pp.
    14–37, 2022.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] A. Petrović, M. Nikolić, S. Radovanović, B. Delibašić, 和 M. Jovanović，“Fair:
    公平的对抗性实例重加权，”*Neurocomputing*，第476卷，页码14–37，2022年。'
- en: '[30] S. K. Devitt, “Trustworthiness of autonomous systems,” *Foundations of
    trusted autonomy (Studies in Systems, Decision and Control, Volume 117)*, pp.
    161–184, 2018.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] S. K. Devitt，“自主系统的可信度，”* Foundations of trusted autonomy (Studies in
    Systems, Decision and Control, Volume 117)*，页码161–184，2018年。'
- en: '[31] D. Kaur, S. Uslu, K. J. Rittichier, and A. Durresi, “Trustworthy artificial
    intelligence: A review,” *ACM Computing Surveys*, vol. 55, no. 2, pp. 1–38, 2022.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] D. Kaur, S. Uslu, K. J. Rittichier, 和 A. Durresi，“可信赖的人工智能：综述，”*ACM Computing
    Surveys*，第55卷，第2期，页码1–38，2022年。'
- en: '[32] B. Wu, Y. Bian, H. Zhang, J. Li, J. Yu, L. Chen, C. Chen, and J. Huang,
    “Trustworthy graph learning: Reliability, explainability, and privacy protection,”
    *ACM KDD*, pp. 4838–4839, 2022.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] B. Wu, Y. Bian, H. Zhang, J. Li, J. Yu, L. Chen, C. Chen, 和 J. Huang，“可信赖的图学习：可靠性、可解释性和隐私保护，”*ACM
    KDD*，页码4838–4839，2022年。'
- en: '[33] C. Fang, L. Cheng, H. Qi, and D. Zhang, “Combating noisy labels in long-tailed
    image classification,” *arXiv:2209.00273*, 2022.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] C. Fang, L. Cheng, H. Qi, 和 D. Zhang，“应对长尾图像分类中的噪声标签，”*arXiv:2209.00273*，2022年。'
- en: '[34] M. Singh, G. Ghalachyan, K. R. Varshney, and R. E. Bryant, “An empirical
    study of accuracy, fairness, explainability, distributional robustness, and adversarial
    robustness,” in *KDD Workshop*, 2021.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] M. Singh, G. Ghalachyan, K. R. Varshney, 和 R. E. Bryant，“准确性、公平性、可解释性、分布鲁棒性和对抗鲁棒性的实证研究，”在*KDD
    Workshop*，2021年。'
- en: '[35] Z. Wan, Z. Wang, C. Chung, and Z. Wang, “A survey of data optimization
    for problems in computer vision datasets,” *arXiv:2210.11717*, 2022.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Z. Wan, Z. Wang, C. Chung, 和 Z. Wang，“计算机视觉数据集中的数据优化综述，”*arXiv:2210.11717*，2022年。'
- en: '[36] X. Zheng, Y. Liu, Z. Bao, M. Fang, X. Hu, A. W.-C. Liew, and S. Pan, “Towards
    data-centric graph machine learning: Review and outlook,” *arXiv:2309.10979*,
    2023.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] X. Zheng, Y. Liu, Z. Bao, M. Fang, X. Hu, A. W.-C. Liew, 和 S. Pan，“面向数据中心的图机器学习：综述与展望，”*arXiv:2309.10979*，2023年。'
- en: '[37] Y. Luo, Y. Wong, M. Kankanhalli, and Q. Zhao, “$\mathcal{G}$-softmax:
    Improving intraclass compactness and interclass separability of features,” *IEEE
    TNNLS*, vol. 31, no. 2, pp. 685–699, 2020.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Y. Luo, Y. Wong, M. Kankanhalli, 和 Q. Zhao，“$\mathcal{G}$-softmax: 改进特征的类内紧凑性和类间可分性，”*IEEE
    TNNLS*，第31卷，第2期，页码685–699，2020年。'
- en: '[38] A. Damian, T. Ma, and J. D. Lee, “Label noise sgd provably prefers flat
    global minimizers,” in *NeurIPS*, 2021, pp. 27 449–27 461.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. Damian, T. Ma, 和 J. D. Lee，“标签噪声SGD明确定义偏好平坦全局极小化器，”在*NeurIPS*，2021年，页码27 449–27 461。'
- en: '[39] Y. Wang, X. Pan, S. Song, H. Zhang, C. Wu, and G. Huang, “Implicit semantic
    data augmentation for deep networks,” in *NeurIPS*, 2019, pp. 12 635–12 644.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Y. Wang, X. Pan, S. Song, H. Zhang, C. Wu, 和 G. Huang，“深度网络的隐式语义数据增强，”在*NeurIPS*，2019年，页码12 635–12 644。'
- en: '[40] M. Wang, Y. Zhang, and W. Deng, “Meta balanced network for fair face recognition,”
    *IEEE TPAMI*, 2021.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] M. Wang, Y. Zhang, 和 W. Deng，“公平面部识别的元平衡网络，”*IEEE TPAMI*，2021年。'
- en: '[41] S. Fujii, Y. Ishii, K. Kozuka, T. Hirakawa, T. Yamashita, and H. Fujiyoshi,
    “Data augmentation by selecting mixed classes considering distance between classes,”
    *arXiv:2209.05122*, 2022.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Fujii, Y. Ishii, K. Kozuka, T. Hirakawa, T. Yamashita, 和 H. Fujiyoshi，“通过考虑类别间距离的混合类别选择进行数据增强，”*arXiv:2209.05122*，2022年。'
- en: '[42] I. L. . F. Hutter, “Online batch selection for faster training of neural
    networks,” *ICLR Workshop*, 2016.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] I. L. . F. Hutter，“在线批量选择以加速神经网络训练，”*ICLR Workshop*，2016年。'
- en: '[43] C.-Y. Chuang and Y. Mroueh, “Fair mixup: Fairness via interpolation,”
    in *ICLR*, 2021.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] C.-Y. Chuang 和 Y. Mroueh，“公平混合：通过插值实现公平性，”在*ICLR*，2021年。'
- en: '[44] L. E. Celis, A. Mehrotra, and N. Vishnoi, “Fair classification with adversarial
    perturbations,” in *NeurIPS*, 2021, pp. 8158–8171.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] L. E. Celis, A. Mehrotra, 和 N. Vishnoi，“带有对抗扰动的公平分类，”在*NeurIPS*，2021年，页码8158–8171。'
- en: '[45] B. Yan, S. Seto, and N. Apostoloff, “Forml: Learning to reweight data
    for fairness,” *arXiv:2202.01719*, 2022.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] B. Yan, S. Seto, 和 N. Apostoloff, “Forml: 学习重新加权数据以实现公平性”，*arXiv:2202.01719*，2022年。'
- en: '[46] Y. Cui, M. Jia, T. Lin, Y. Song, and S. Belongie, “Class-balanced loss
    based on effective number of samples,” in *CVPR*, 2019, pp. 9260–9269.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Y. Cui, M. Jia, T. Lin, Y. Song, 和 S. Belongie, “基于有效样本数量的类别平衡损失”，发表于*CVPR*，2019年，第9260–9269页。'
- en: '[47] M. Li, F. Su, O. Wu, and J. Zhang, “Logit perturbation,” in *AAAI*, 2022,
    pp. 10 388–10 396.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] M. Li, F. Su, O. Wu, 和 J. Zhang, “Logit扰动”，发表于*AAAI*，2022年，第10 388–10 396页。'
- en: '[48] Y. Chen, P. Zhang, T. Kong, Y. Li, X. Zhang, L. Qi, J. Sun, and J. Jia,
    “Scale-aware automatic augmentations for object detection with dynamic training,”
    *IEEE TPAMI*, vol. 45, no. 2, pp. 2367–2383, 2023.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. Chen, P. Zhang, T. Kong, Y. Li, X. Zhang, L. Qi, J. Sun, 和 J. Jia,
    “动态训练下的尺度感知自动数据增强用于目标检测”，*IEEE TPAMI*，第45卷，第2期，第2367–2383页，2023年。'
- en: '[49] M. P. Naeini, G. F. Cooper, and M. Hauskrecht, “Obtaining well calibrated
    probabilities using bayesian binning,” in *AAAI*, 2015.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] M. P. Naeini, G. F. Cooper, 和 M. Hauskrecht, “利用贝叶斯分箱获得良好的校准概率”，发表于*AAAI*，2015年。'
- en: '[50] B. Liu, I. Ben Ayed, A. Galdran, and J. Dolz, “The devil is in the margin:
    Margin-based label smoothing for network calibration,” in *CVPR*, 2022, pp. 80–88.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] B. Liu, I. Ben Ayed, A. Galdran, 和 J. Dolz, “魔鬼在于边际：基于边际的标签平滑用于网络校准”，发表于*CVPR*，2022年，第80–88页。'
- en: '[51] J. Mukhoti, V. Kulharia, A. Sanyal, S. Golodetz, P. Torr, and P. Dokania,
    “Calibrating deep neural networks using focal loss,” in *NeurIPS*, 2020, pp. 15 288–15 299.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] J. Mukhoti, V. Kulharia, A. Sanyal, S. Golodetz, P. Torr, 和 P. Dokania,
    “使用焦点损失校准深度神经网络”，发表于*NeurIPS*，2020年，第15 288–15 299页。'
- en: '[52] L. Dunlap, A. Umino, H. Zhang, J. Yang, J. E. Gonzalez, and T. Darrell,
    “Diversify your vision datasets with automatic diffusion-based augmentation,”
    *arXiv:2305.16289*, 2023.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] L. Dunlap, A. Umino, H. Zhang, J. Yang, J. E. Gonzalez, 和 T. Darrell,
    “通过自动扩散增强多样化视觉数据集”，*arXiv:2305.16289*，2023年。'
- en: '[53] Z. Ye, Y. Dai, C. Hong, Z. Cao, and H. Lu, “Infusing definiteness into
    randomness: Rethinking composition styles for deep image matting,” *AAAI*, 2023.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Z. Ye, Y. Dai, C. Hong, Z. Cao, 和 H. Lu, “将确定性注入随机性：重新思考深度图像抠图的组成风格”，*AAAI*，2023年。'
- en: '[54] S. Yang, W. Xiao, M. Zhang, S. Guo, J. Zhao, and F. Shen, “Image data
    augmentation for deep learning: A survey,” *arXiv:2204.08610*, 2022.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] S. Yang, W. Xiao, M. Zhang, S. Guo, J. Zhao, 和 F. Shen, “深度学习中的图像数据增强：一项综述”，*arXiv:2204.08610*，2022年。'
- en: '[55] W. Yang, C. Li, J. Zhang, and C. Zong, “Bigtranslate: Augmenting large
    language models with multilingual translation capability over 100 languages,”
    *arXiv:2305.18098*, 2023.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] W. Yang, C. Li, J. Zhang, 和 C. Zong, “Bigtranslate: 增强大型语言模型的多语言翻译能力，支持100多种语言”，*arXiv:2305.18098*，2023年。'
- en: '[56] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, and J. Gao, “Adversarial
    training for large neural language models,” *arXiv:2004.08994*, 2020.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, 和 J. Gao, “针对大型神经语言模型的对抗训练”，*arXiv:2004.08994*，2020年。'
- en: '[57] X. Glorot and Y. Bengio, “Understanding the difficulty of training deep
    feedforward neural networks,” *AISTATS*, pp. 249–256, 2010.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] X. Glorot 和 Y. Bengio, “理解训练深度前馈神经网络的难度”，*AISTATS*，第249–256页，2010年。'
- en: '[58] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing
    human-level performance on imagenet classification,” *ICCV*, pp. 1026–1034, 2015.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] K. He, X. Zhang, S. Ren, 和 J. Sun, “深入探讨激活函数：超越人类水平的ImageNet分类性能”，*ICCV*，第1026–1034页，2015年。'
- en: '[59] F. Su, Y. Zhu, O. Wu, and Y. Deng, “Submodular meta data compiling for
    meta optimization,” *ECML/PKDD*, 2022.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] F. Su, Y. Zhu, O. Wu, 和 Y. Deng, “用于元优化的子模元数据编译”，*ECML/PKDD*，2022年。'
- en: '[60] J. Huang, L. Qu, R. Jia, and B. Zhao, “O2u-net: A simple noisy label detection
    approach for deep neural networks,” in *ICCV*, 2019, pp. 3326–3334.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J. Huang, L. Qu, R. Jia, 和 B. Zhao, “O2u-net: 一种简单的噪声标签检测方法用于深度神经网络”，发表于*ICCV*，2019年，第3326–3334页。'
- en: '[61] B. Li, Y. Liu, and X. Wang, “Gradient harmonized single-stage detector,”
    in *AAAI*, 2019, pp. 8577–8584.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] B. Li, Y. Liu, 和 X. Wang, “梯度协调单阶段检测器”，发表于*AAAI*，2019年，第8577–8584页。'
- en: '[62] C.-B. Zhang, P.-T. Jiang, Q. Hou, Y. Wei, Q. Han, Z. Li, and M.-M. Cheng,
    “Delving deep into label smoothing,” *IEEE TIP*, vol. 30, pp. 5984–5996, 2021.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] C.-B. Zhang, P.-T. Jiang, Q. Hou, Y. Wei, Q. Han, Z. Li, 和 M.-M. Cheng,
    “深入探讨标签平滑”，*IEEE TIP*，第30卷，第5984–5996页，2021年。'
- en: '[63] S. Sinha, H. Ohashi, and K. Nakamura, “Class-wise difficulty-balanced
    loss for solving class-imbalance,” in *ACCV*, 2020.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] S. Sinha, H. Ohashi, 和 K. Nakamura, “解决类别不平衡的类别级困难平衡损失”，发表于*ACCV*，2020年。'
- en: '[64] M. Escudero-Viñolo and A. López-Cifuentes, “Ccl: Class-wise curriculum
    learning for class imbalance problems,” in *ICIP*, 2022, pp. 1476–1480.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] M. Escudero-Viñolo 和 A. López-Cifuentes, “Ccl: 解决类别不平衡问题的类别级课程学习”，发表于*ICIP*，2022年，第1476–1480页。'
- en: '[65] X. Ning, W. Tian, F. He, X. Bai, L. Sun, and W. Li, “Hyper-sausage coverage
    function neuron model and learning algorithm for image classification,” *Pattern
    Recognition*, vol. 136, p. 109216, 2023.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] X. Ning, W. Tian, F. He, X. Bai, L. Sun, 和 W. Li，“用于图像分类的超香肠覆盖函数神经元模型和学习算法，”
    *Pattern Recognition*，第136卷，页码109216，2023年。'
- en: '[66] J. Lin, A. Zhang, M. Lécuyer, J. Li, A. Panda, and S. Sen, “Measuring
    the effect of training data on deep learning predictions via randomized experiments,”
    in *ICML*, 2022, pp. 13 468–13 504.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. Lin, A. Zhang, M. Lécuyer, J. Li, A. Panda, 和 S. Sen，“通过随机实验测量训练数据对深度学习预测的影响，”发表于
    *ICML*，2022年，页码13 468–13 504。'
- en: '[67] X. Zhou, N. Yang, and O. Wu, “Combining adversaries with anti-adversaries
    in training,” in *AAAI*, 2023.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] X. Zhou, N. Yang, 和 O. Wu，“在训练中结合对抗者与反对抗者，”发表于 *AAAI*，2023年。'
- en: '[68] K. Tang, M. Tao, J. Qi, Z. Liu, and H. Zhang, “Invariant feature learning
    for generalized long-tailed classification,” in *ECCV*, 2022, pp. 709–726.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] K. Tang, M. Tao, J. Qi, Z. Liu, 和 H. Zhang，“针对广义长尾分类的变不变特征学习，”发表于 *ECCV*，2022年，页码709–726。'
- en: '[69] S. Shrivastava, X. Zhang, S. Nagesh, and A. Parchami, “Datasetequity:
    Are all samples created equal? in the quest for equity within datasets,” in *ICCV*,
    2023, pp. 4417–4426.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] S. Shrivastava, X. Zhang, S. Nagesh, 和 A. Parchami，“数据集公平性：样本是否平等？在数据集公平性探索中，”发表于
    *ICCV*，2023年，页码4417–4426。'
- en: '[70] R. Wang, W. Xiong, Q. Hou, and O. Wu, “Tackling the imbalance for gnns,”
    in *IJCNN*, 2022.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] R. Wang, W. Xiong, Q. Hou, 和 O. Wu，“应对GNNs的不平衡，”发表于 *IJCNN*，2022年。'
- en: '[71] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,”
    in *ICML*, 2009, pp. 41–48.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Y. Bengio, J. Louradour, R. Collobert, 和 J. Weston，“课程学习，”发表于 *ICML*，2009年，页码41–48。'
- en: '[72] T. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal loss for dense
    object detection,” in *CVPR*, 2017, pp. 2999–3007.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] T. Lin, P. Goyal, R. Girshick, K. He, 和 P. Dollar，“用于密集目标检测的焦点损失，”发表于
    *CVPR*，2017年，页码2999–3007。'
- en: '[73] M. Paul, S. Ganguli, and G. K. Dziugaite, “Deep learning on a data diet:
    Finding important examples early in training,” in *NeurIPS*, 2021, pp. 20 596–20 607.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. Paul, S. Ganguli, 和 G. K. Dziugaite，“数据饮食中的深度学习：在训练早期找到重要的示例，”发表于 *NeurIPS*，2021年，页码20 596–20 607。'
- en: '[74] W. Zhu, O. Wu, F. Su, and Y. Deng, “Exploring the learning difficulty
    of data: Theory and measure,” *arXiv:2205.07427*, 2022.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] W. Zhu, O. Wu, F. Su, 和 Y. Deng，“探讨数据学习难度：理论与度量，” *arXiv:2205.07427*，2022年。'
- en: '[75] B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli, and A. S. Morcos, “Beyond
    neural scaling laws: beating power law scaling via data pruning,” in *NeurIPS*,
    2022.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli, 和 A. S. Morcos，“超越神经缩放定律：通过数据修剪击败幂律缩放，”发表于
    *NeurIPS*，2022年。'
- en: '[76] M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu, M. Ghavamzadeh,
    P. Fieguth, X. Cao, A. Khosravi, U. R. Acharya, V. Makarenkov, and S. Nahavandi,
    “A review of uncertainty quantification in deep learning: Techniques, applications
    and challenges,” *Information fusion*, vol. 76, pp. 243–297, 2021.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu, M. Ghavamzadeh,
    P. Fieguth, X. Cao, A. Khosravi, U. R. Acharya, V. Makarenkov, 和 S. Nahavandi，“深度学习中不确定性量化的综述：技术、应用与挑战，”
    *Information fusion*，第76卷，页码243–297，2021年。'
- en: '[77] D. D’souza, Z. Nussbaum, C. Agarwal, and S. Hooker, “A tale of two long
    tails,” *arXiv:2107.13098*, 2021.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] D. D’souza, Z. Nussbaum, C. Agarwal, 和 S. Hooker，“两个长尾的故事，” *arXiv:2107.13098*，2021年。'
- en: '[78] A. Kendall and Y. Gal, “What uncertainties do we need in bayesian deep
    learning for computer vision?” in *NeurIPS*, 2017, pp. 5574–5584.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] A. Kendall 和 Y. Gal，“在计算机视觉的贝叶斯深度学习中我们需要哪些不确定性？”发表于 *NeurIPS*，2017年，页码5574–5584。'
- en: '[79] A. Kumar, S. Bhattamishra, M. Bhandari, and P. Talukdar, “Submodular optimization-based
    diverse paraphrasing and its effectiveness in data augmentation,” in *NAACL*,
    2019, pp. 3609–3619.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] A. Kumar, S. Bhattamishra, M. Bhandari, 和 P. Talukdar，“基于子模块优化的多样化释义及其在数据增强中的有效性，”发表于
    *NAACL*，2019年，页码3609–3619。'
- en: '[80] D. Friedman and A. B. Dieng, “The vendi score: A diversity evaluation
    metric for machine learning,” *arXiv:2210.02410*, 2023.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] D. Friedman 和 A. B. Dieng，“Vendi分数：机器学习的多样性评估指标，” *arXiv:2210.02410*，2023年。'
- en: '[81] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen,
    “Improved techniques for training gans,” *NeurIPS*, pp. 2234–2242, 2016.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, 和 X. Chen，“改进的GAN训练技术，”
    *NeurIPS*，页码2234–2242，2016年。'
- en: '[82] O. Wu, “Rethinking class imbalance in machine learning,” *arXiv:2305.03900*,
    2023.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] O. Wu，“重新思考机器学习中的类别不平衡，” *arXiv:2305.03900*，2023年。'
- en: '[83] S. Swayamdipta, R. Schwartz, N. Lourie, Y. Wang, H. Hajishirzi, N. A.
    Smith, and Y. Choi, “Dataset cartography: Mapping and diagnosing datasets with
    training dynamics,” in *EMNLP*, 2020.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] S. Swayamdipta, R. Schwartz, N. Lourie, Y. Wang, H. Hajishirzi, N. A.
    Smith, 和 Y. Choi，“数据集制图：通过训练动态映射和诊断数据集，”发表于 *EMNLP*，2020年。'
- en: '[84] A. Iscen, J. Valmadre, A. Arnab, and C. Schmid, “Learning with neighbor
    consistency for noisy labels,” in *CVPR*, 2022, pp. 4672–4681.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] A. Iscen, J. Valmadre, A. Arnab 和 C. Schmid，“通过邻居一致性学习噪声标签”，发表于 *CVPR*，2022，页码
    4672–4681。'
- en: '[85] M. Toneva, A. Sordoni, R. T. des Combes, A. Trischler, Y. Bengio, and
    G. J. Gordon, “An empirical study of example forgetting during deep neural network
    learning,” *ICLR*, 2019.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] M. Toneva, A. Sordoni, R. T. des Combes, A. Trischler, Y. Bengio 和 G. J.
    Gordon，“深度神经网络学习过程中示例遗忘的实证研究”，*ICLR*，2019。'
- en: '[86] P. Singh, P. Mazumder, and M. A. Karim, “Attaining class-level forgetting
    in pretrained model using few samples,” in *ECCV*, 2022, pp. 433–448.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] P. Singh, P. Mazumder 和 M. A. Karim，“使用少量样本在预训练模型中实现类别级遗忘”，发表于 *ECCV*，2022，页码
    433–448。'
- en: '[87] P. Maini, S. Garg, Z. Lipton, and J. Z. Kolter, “Characterizing datapoints
    via second-split forgetting,” in *NeurIPS*, 2022, pp. 30 044–30 057.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] P. Maini, S. Garg, Z. Lipton 和 J. Z. Kolter，“通过第二次拆分遗忘来表征数据点”，发表于 *NeurIPS*，2022，页码
    30 044–30 057。'
- en: '[88] Z. Wang, E. Yang, L. Shen, and H. Huang, “A comprehensive survey of forgetting
    in deep learning beyond continual learning,” *arXiv:2307.09218*, 2023.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Z. Wang, E. Yang, L. Shen 和 H. Huang，“关于深度学习中的遗忘问题的全面调查，超越持续学习”，*arXiv:2307.09218*，2023。'
- en: '[89] T. Kim, J. Ko, s. Cho, J. Choi, and S.-Y. Yun, “Fine samples for learning
    with noisy labels,” in *NeurIPS*, 2021, pp. 24 137–24 149.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] T. Kim, J. Ko, S. Cho, J. Choi 和 S.-Y. Yun，“用于带噪声标签学习的精细样本”，发表于 *NeurIPS*，2021，页码
    24 137–24 149。'
- en: '[90] L. S. Shapley, “A value for n-person games,” in *In Contributions to the
    Theory of Games*, 1953, pp. 307–317.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] L. S. Shapley，“n人博弈的一个价值”，发表于 *In Contributions to the Theory of Games*，1953，页码
    307–317。'
- en: '[91] A. Ghorbani and J. Zou, “Data shapley: Equitable valuation of data for
    machine learning,” in *ICML*, 2019, pp. 2242–2251.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] A. Ghorbani 和 J. Zou，“Data shapley: 公平的数据价值评估”，发表于 *ICML*，2019，页码 2242–2251。'
- en: '[92] J. Yoon, S. Arik, and T. Pfister, “Data valuation using reinforcement
    learning,” in *ICML*, 2020, pp. 10 842–10 851.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Yoon, S. Arik 和 T. Pfister，“使用强化学习进行数据评估”，发表于 *ICML*，2020，页码 10 842–10 851。'
- en: '[93] Y. Bian, Y. Rong, T. Xu, J. Wu, A. Krause, and J. Huang, “Energy-based
    learning for cooperative games, with applications to valuation problems in machine
    learning,” in *ICLR*, 2022.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Y. Bian, Y. Rong, T. Xu, J. Wu, A. Krause 和 J. Huang，“用于合作博弈的基于能量的学习，及其在机器学习中的估值问题应用”，发表于
    *ICLR*，2022。'
- en: '[94] K. F. Jiang, W. Liang, J. Zou, and Y. Kwon, “Opendataval: a unified benchmark
    for data valuation,” in *NeurIPS*, 2023.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] K. F. Jiang, W. Liang, J. Zou 和 Y. Kwon，“Opendataval: 数据评估的统一基准”，发表于 *NeurIPS*，2023。'
- en: '[95] D. Bahri and H. Jiang, “Locally adaptive label smoothing improves predictive
    churn,” in *ICML*, 2021, pp. 532–542.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] D. Bahri 和 H. Jiang，“局部自适应标签平滑改善预测流失”，发表于 *ICML*，2021，页码 532–542。'
- en: '[96] C. Dong, L. Liu, and J. Shang, “Data profiling for adversarial training:
    On the ruin of problematic data,” *arXiv:2102.07437v1*, 2021.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] C. Dong, L. Liu 和 J. Shang，“对抗训练的数据分析：有问题的数据的毁灭”，*arXiv:2102.07437v1*，2021。'
- en: '[97] Z. Hammoudeh and D. Lowd, “Training data influence analysisand estimation:
    A survey,” *arXiv:2212.04612*, 2023.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Z. Hammoudeh 和 D. Lowd，“训练数据影响分析和估计：一项调查”，*arXiv:2212.04612*，2023。'
- en: '[98] Y. Wu, J. Shu, Q. Xie, Q. Zhao, and D. Meng, “Learning to purify noisy
    labels via meta soft label corrector,” in *AAAI*, 2021, pp. 10 388–10 396.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Y. Wu, J. Shu, Q. Xie, Q. Zhao 和 D. Meng，“通过元软标签校正器学习净化噪声标签”，发表于 *AAAI*，2021，页码
    10 388–10 396。'
- en: '[99] J. Shu, Q. Xie, L. Yi, Q. Zhao, S. Zhou, Z. Xu, and D. Meng, “Meta-Weight-Net:
    Learning an explicit mapping for sample weighting,” in *NeurIPS*, 2019, pp. 1917–1928.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] J. Shu, Q. Xie, L. Yi, Q. Zhao, S. Zhou, Z. Xu 和 D. Meng，“Meta-Weight-Net:
    学习样本加权的显式映射”，发表于 *NeurIPS*，2019，页码 1917–1928。'
- en: '[100] M. P. Kumar, B. Packer, and D. Koller, “Self-paced learning for latent
    variable models,” *NeurIPS*, pp. 1–9, 2010.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] M. P. Kumar, B. Packer 和 D. Koller，“针对潜变量模型的自适应学习”，*NeurIPS*，页码 1–9，2010。'
- en: '[101] E. Arazo, D. Ortego, P. Albert, N. E. O’Connor, and K. McGuinness, “Unsupervised
    label noise modeling and loss correction,” in *ICML*, 2019.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] E. Arazo, D. Ortego, P. Albert, N. E. O’Connor 和 K. McGuinness，“无监督标签噪声建模与损失校正”，发表于
    *ICML*，2019。'
- en: '[102] C. Hu, S. Yan, Z. Gao, and X. He, “Mild: Modeling the instance learning
    dynamics for learning with noisy labels,” *arXiv:2306.11560*, 2023.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] C. Hu, S. Yan, Z. Gao 和 X. He，“Mild: 处理噪声标签的实例学习动态建模”，*arXiv:2306.11560*，2023。'
- en: '[103] Y. Li and N. Vasconcelos, “Repair: Removing representation bias by dataset
    resampling,” *CVPR*, 2019.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Y. Li 和 N. Vasconcelos，“Repair: 通过数据集重采样去除表征偏差”，发表于 *CVPR*，2019。'
- en: '[104] F. M. Megahed, Y.-J. Chen, A. Megahed, Y. Ong, N. Altman, and M. Krzywinski,
    “The class imbalance problem,” *Nature Methods*, vol. 18, pp. 1270–1272, 2021.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] F. M. Megahed, Y.-J. Chen, A. Megahed, Y. Ong, N. Altman 和 M. Krzywinski，“类别不平衡问题”，*Nature
    Methods*，第 18 卷，页码 1270–1272，2021。'
- en: '[105] J. Cui, S. Liu, Z. Tian, Z. Zhong, and J. Jia, “Reslt: Residual learning
    for long-tailed recognition,” *IEEE TPAMI*, vol. 45, no. 3, pp. 3695–3706, 2023.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] J. Cui, S. Liu, Z. Tian, Z. Zhong, 和 J. Jia，“Reslt：用于长尾识别的残差学习，” *IEEE
    TPAMI*，第45卷，第3期，第3695–3706页，2023年。'
- en: '[106] A. Kirsch, J. van Amersfoort, and Y. Gal, “Batchbald: Efficient and diverse
    batch acquisition for deep bayesian active learning,” in *NeurIPS*, 2019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] A. Kirsch, J. van Amersfoort, 和 Y. Gal，“Batchbald：用于深度贝叶斯主动学习的高效多样化批量采集，”
    在 *NeurIPS*，2019年。'
- en: '[107] H. Shimodaira, “Improving predictive inference under covariate shift
    by weighting the log-likelihood function,” *Journal of statistical planning and
    inference*, 2000.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] H. Shimodaira，“通过加权对数似然函数来改善协变量偏移下的预测推断，” *统计规划与推断期刊*，2000年。'
- en: '[108] J. Byrd and Z. Lipton, “What is the effect of importance weighting in
    deep learning?” in *ICML*, 2019, pp. 872–881.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] J. Byrd 和 Z. Lipton，“重要性加权在深度学习中的效果是什么？” 在 *ICML*，2019年，第872–881页。'
- en: '[109] Q. Liu and J. Lee, “Black-box Importance Sampling,” in *AISTATS*, 2017,
    pp. 952–961.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Q. Liu 和 J. Lee，“黑箱重要性采样，” 在 *AISTATS*，2017年，第952–961页。'
- en: '[110] A. Katharopoulos and F. Fleuret, “Not all samples are created equal:
    Deep learning with importance sampling,” in *ICML*, 2018, pp. 2525–2534.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] A. Katharopoulos 和 F. Fleuret，“并非所有样本都是相同的：利用重要性采样进行深度学习，” 在 *ICML*，2018年，第2525–2534页。'
- en: '[111] T. B. Johnson and C. Guestrin, “Training deep models faster with robust,
    approximate importance sampling,” in *NeurIPS*, 2018.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] T. B. Johnson 和 C. Guestrin，“利用稳健的近似重要性采样加速深度模型训练，” 在 *NeurIPS*，2018年。'
- en: '[112] A. H. Jiang, D. L.-K. Wong, G. Zhou, D. G. Andersen, J. Dean, G. R. Ganger,
    G. Joshi, M. Kaminksy, M. Kozuch, Z. C. Lipton, and P. Pillai, “Accelerating deep
    learning by focusing on the biggest losers,” *arXiv:1910.00762*, 2019.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] A. H. Jiang, D. L.-K. Wong, G. Zhou, D. G. Andersen, J. Dean, G. R. Ganger,
    G. Joshi, M. Kaminksy, M. Kozuch, Z. C. Lipton, 和 P. Pillai，“通过关注最大损失者来加速深度学习，”
    *arXiv:1910.00762*，2019年。'
- en: '[113] X. J. Gui, W. Wang, and Z. H. Tian, “Towards understanding deep learning
    from noisy labels with small-loss criterion,” *IJCAI*, 2021.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] X. J. Gui, W. Wang, 和 Z. H. Tian，“从噪声标签中理解深度学习：小损失准则，” *IJCAI*，2021年。'
- en: '[114] H. Liu, X. Zhu, Z. Lei, and S. Z. Li, “Adaptiveface: Adaptive margin
    and sampling for face recognition,” in *CVPR*, 2019, pp. 11 947–11 956.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] H. Liu, X. Zhu, Z. Lei, 和 S. Z. Li，“Adaptiveface：用于人脸识别的自适应边界和采样，” 在
    *CVPR*，2019年，第11 947–11 956页。'
- en: '[115] D. Xu, Y. Ye, and C. Ruan, “Understanding the role of importance weighting
    for deep learning,” *arXiv:2103.15209*, 2021.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] D. Xu, Y. Ye, 和 C. Ruan，“理解重要性加权在深度学习中的作用，” *arXiv:2103.15209*，2021年。'
- en: '[116] V. Nguyen, M. Shaker, and E. Hüllermeier, “How to measure uncertainty
    in uncertainty sampling for active learning,” *Machine Learning*, vol. 111, pp.
    89–122, 2022.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] V. Nguyen, M. Shaker, 和 E. Hüllermeier，“如何衡量主动学习中的不确定性采样的不确定性，” *机器学习*，第111卷，第89–122页，2022年。'
- en: '[117] J. Mena, O. Pujol, and J. Vitrià, “A survey on uncertainty estimation
    in deep learning classification systems from a bayesian perspective,” *ACM Computing
    Surveys*, vol. 54, no. 9, pp. 1–35, 2021.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] J. Mena, O. Pujol, 和 J. Vitrià，“从贝叶斯视角对深度学习分类系统中的不确定性估计进行调查，” *ACM计算调查*，第54卷，第9期，第1–35页，2021年。'
- en: '[118] A. Aljuhani, I. Casukhela, J. Chan, D. Liebner, and R. Machiraju, “Uncertainty
    aware sampling framework of weak-label learning for histology image classification,”
    in *MICCAI*, 2022, pp. 366–376.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] A. Aljuhani, I. Casukhela, J. Chan, D. Liebner, 和 R. Machiraju，“面向组织学图像分类的弱标签学习的不确定性感知采样框架，”
    在 *MICCAI*，2022年，第366–376页。'
- en: '[119] D. Ting and E. Brochu, “Optimal subsampling with influence functions,”
    in *NeurIPS*, 2018, pp. 3650–3659.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] D. Ting 和 E. Brochu，“利用影响函数进行最优子采样，” 在 *NeurIPS*，2018年，第3650–3659页。'
- en: '[120] Y. Li and N. Vasconcelos, “Background data resampling for outlier-aware
    classification,” in *CVPR*, 2020, pp. 13 218–13 227.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Y. Li 和 N. Vasconcelos，“背景数据重采样用于异常值感知分类，” 在 *CVPR*，2020年，第13 218–13 227页。'
- en: '[121] X. Wang and Y. Wang, “Sentence-level resampling for named entity recognition,”
    in *NAACL*, 2022, pp. 2151–2165.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] X. Wang 和 Y. Wang，“用于命名实体识别的句子级重采样，” 在 *NAACL*，2022年，第2151–2165页。'
- en: '[122] J. Zhang, T. Wang, W. W. Y. Ng, S. Zhang, and C. D. Nugent, “Undersampling
    near decision boundary for imbalance problems,” in *ICMLC*, 2019, pp. 1–8.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] J. Zhang, T. Wang, W. W. Y. Ng, S. Zhang, 和 C. D. Nugent，“用于不平衡问题的决策边界附近欠采样，”
    在 *ICMLC*，2019年，第1–8页。'
- en: '[123] M. Sun, H. Dou, B. Li, J. Yan, W. Ouyang, and L. Cui, “Autosampling:
    Search for effective data sampling schedules,” in *ICML*, 2017, p. 9923–9933.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] M. Sun, H. Dou, B. Li, J. Yan, W. Ouyang, 和 L. Cui，“自动采样：寻找有效的数据采样调度，”
    在 *ICML*，2017年，第9923–9933页。'
- en: '[124] G. Li, L. Liu, G. Huang, C. Zhu, and T. Zhao, “Understanding data augmentation
    in neural machine translation: Two perspectives towards generalization,” in *EMNLP-IJCNLP*,
    2019, pp. 5689–56 958.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] G. Li, L. Liu, G. Huang, C. Zhu 和 T. Zhao，“理解神经机器翻译中的数据增广：通向泛化的两个视角，”在
    *EMNLP-IJCNLP*，2019 年，页 5689–56 958。'
- en: '[125] L. Zhao, T. Liu, X. Peng, and D. Metaxas, “Maximum-entropy adversarial
    data augmentation for improved generalization and robustness,” in *NeurIPS*, vol. 33,
    2020, pp. 14 435–14 447.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] L. Zhao, T. Liu, X. Peng 和 D. Metaxas，“最大熵对抗数据增广以改善泛化和鲁棒性，”在 *NeurIPS*，第
    33 卷，2020 年，页 14 435–14 447。'
- en: '[126] S.-A. Rebuffi, S. Gowal, D. A. Calian, F. Stimberg, O. Wiles, and T. A.
    Mann, “Data augmentation can improve robustness,” in *NeurIPS*, 2021, pp. 29 935–29 948.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] S.-A. Rebuffi, S. Gowal, D. A. Calian, F. Stimberg, O. Wiles 和 T. A.
    Mann，“数据增广可以提高鲁棒性，”在 *NeurIPS*，2021 年，页 29 935–29 948。'
- en: '[127] L. Li and M. Spratling, “Data augmentation alone can improve adversarial
    training,” in *ICLR*, 2023.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] L. Li 和 M. Spratling，“仅使用数据增广即可改善对抗训练，”在 *ICLR*，2023 年。'
- en: '[128] M. Bayer, M.-A. Kaufhold, and C. Reuter, “A survey on data augmentation
    for text classification,” *ACM Computing Surveys*, vol. 55, no. 7, pp. 1–39, 2022.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] M. Bayer, M.-A. Kaufhold 和 C. Reuter，“文本分类的数据增广综述，” *ACM Computing Surveys*，第
    55 卷，第 7 期，页 1–39，2022 年。'
- en: '[129] C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmentation
    for deep learning,” *Journal of Big Data*, vol. 6, no. 60, 2019.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] C. Shorten 和 T. M. Khoshgoftaar，“深度学习的图像数据增广综述，” *Journal of Big Data*，第
    6 卷，第 60 期，2019 年。'
- en: '[130] K. Ding, Z. Xu, H. Tong, and H. Liu, “Data augmentation for deep graph
    learning: A survey,” *ACM SIGKDD Explorations Newsletter*, vol. 24, no. 2, 2022.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] K. Ding, Z. Xu, H. Tong 和 H. Liu，“深度图学习的数据增广：综述，” *ACM SIGKDD Explorations
    Newsletter*，第 24 卷，第 2 期，2022 年。'
- en: '[131] Q. Wen, L. Sun, F. Yang, X. Song, J. Gao, X. Wang, and H. Xu, “Time series
    data augmentation for deep learning: A survey,” in *IJCAI*, 2021, pp. 1–8.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Q. Wen, L. Sun, F. Yang, X. Song, J. Gao, X. Wang 和 H. Xu，“深度学习的时间序列数据增广：综述，”在
    *IJCAI*，2021 年，页 1–8。'
- en: '[132] B. Li, Y. Hou, and W. Che, “Data augmentation approaches in natural language
    processing: A survey,” *AI Open*, vol. 3, pp. 71–90, 2022.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] B. Li, Y. Hou 和 W. Che，“自然语言处理中的数据增广方法：综述，” *AI Open*，第 3 卷，页 71–90，2022
    年。'
- en: '[133] T. DeVries and G. W. Taylor, “Dataset augmentation in feature space,”
    *ICLR Workshop*, 2017.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] T. DeVries 和 G. W. Taylor，“特征空间中的数据集增广，” *ICLR Workshop*，2017 年。'
- en: '[134] P. Li, D. Li, W. Li, S. Gong, Y. Fu, and T. M. Hospedales, “A simple
    feature augmentation for domain generalization,” in *ICCV*, 2021, pp. 8886–8895.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] P. Li, D. Li, W. Li, S. Gong, Y. Fu 和 T. M. Hospedales，“用于领域泛化的简单特征增广，”在
    *ICCV*，2021 年，页 8886–8895。'
- en: '[135] M. Ye, J. Shen, X. Zhang, P. C. Yuen, and S.-F. Chang, “Augmentation
    invariant and instance spreading feature for softmax embedding,” *IEEE TPAMI*,
    vol. 44, no. 2, pp. 924–939, 2022.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] M. Ye, J. Shen, X. Zhang, P. C. Yuen 和 S.-F. Chang，“用于 softmax 嵌入的增广不变性和实例扩展特征，”
    *IEEE TPAMI*，第 44 卷，第 2 期，页 924–939，2022 年。'
- en: '[136] P. Chu, X. Bian, S. Liu, and H. Ling, “Feature space augmentation for
    long-tailed data,” in *ECCV*, 2020, pp. 694–710.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] P. Chu, X. Bian, S. Liu 和 H. Ling，“长尾数据的特征空间增广，”在 *ECCV*，2020 年，页 694–710。'
- en: '[137] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
    deep learning models resistant to adversarial attacks,” in *ICLR*, 2018.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] A. Madry, A. Makelov, L. Schmidt, D. Tsipras 和 A. Vladu，“朝着对抗攻击抵抗的深度学习模型迈进，”在
    *ICLR*，2018 年。'
- en: '[138] T. Bai and J. Luo, “Recent advances in adversarial training for adversarial
    robustness,” in *IJCAI*, 2021, pp. 4312–4321.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] T. Bai 和 J. Luo，“对抗性训练在对抗鲁棒性方面的最新进展，”在 *IJCAI*，2021 年，页 4312–4321。'
- en: '[139] S. Lee, H. Kim, and J. Lee, “Graddiv: Adversarial robustness of randomized
    neural networks via gradient diversity regularization,” *TPAMI*, vol. 45, no. 2,
    pp. 2645–2651, 2023.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] S. Lee, H. Kim 和 J. Lee，“Graddiv：通过梯度多样性正则化提高随机神经网络的对抗鲁棒性，” *TPAMI*，第
    45 卷，第 2 期，页 2645–2651，2023 年。'
- en: '[140] H. Lee, S. J. Hwang, and J. Shin, “Self-supervised label augmentation
    via input transformations,” in *ICML*, 2020, pp. 5714–5724.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] H. Lee, S. J. Hwang 和 J. Shin，“通过输入变换进行自监督标签增广，”在 *ICML*，2020 年，页 5714–5724。'
- en: '[141] I. Elezi, A. Torcinovich, S. Vascon, and M. Pelillo, “Transductive label
    augmentation for improved deep network learning,” in *ICPR*, 2018, pp. 1432–1437.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] I. Elezi, A. Torcinovich, S. Vascon 和 M. Pelillo，“用于改进深度网络学习的传导标签增广，”在
    *ICPR*，2018 年，页 1432–1437。'
- en: '[142] F. Huang, L. Zhang, Y. Zhou, and X. Gao, “Adversarial and isotropic gradient
    augmentation for image retrieval with text feedback,” *IEEE TMM*, pp. 1–12, 2022.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] F. Huang, L. Zhang, Y. Zhou 和 X. Gao，“用于图像检索的对抗性和各向同性梯度增广，带文本反馈，” *IEEE
    TMM*，页 1–12，2022 年。'
- en: '[143] N. Chawla, K. Bowyer, L. Hall, and W. Kegelmeyer, “Smote: synthetic minority
    over-sampling technique,” *Journal of Artificial Intelligence Research*, vol. 16,
    no. 1, pp. 321–357, 2002.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] N. Chawla, K. Bowyer, L. Hall, 和 W. Kegelmeyer，“Smote: 合成少数类过采样技术，” *Journal
    of Artificial Intelligence Research*，第16卷，第1期，页码321–357，2002年。'
- en: '[144] A. Telikani, A. H. Gandomi, K.-K. R. Choo, and J. Shen, “A cost-sensitive
    deep learning-based approach for network traffic classification,” *IEEE TNSE*,
    vol. 19, no. 1, pp. 661–670, 2022.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] A. Telikani, A. H. Gandomi, K.-K. R. Choo, 和 J. Shen，“一种基于深度学习的成本敏感网络流量分类方法，”
    *IEEE TNSE*，第19卷，第1期，页码661–670，2022年。'
- en: '[145] D. Dablain, B. Krawczyk, and N. V. Chawla, “Deepsmote: Fusing deep learning
    and smote for imbalanced data,” *IEEE TNNLS*, vol. 34, no. 9, pp. 6390–6404, 2023.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] D. Dablain, B. Krawczyk, 和 N. V. Chawla，“Deepsmote: 深度学习与 smote 融合以处理不平衡数据，”
    *IEEE TNNLS*，第34卷，第9期，页码6390–6404，2023年。'
- en: '[146] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-Paz,
    and Y. Bengio, “Manifold mixup: Better representations by interpolating hidden
    states,” in *ICML*, 2019, pp. 6438–6447.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-Paz,
    和 Y. Bengio，“流形混合：通过插值隐藏状态获得更好的表示，” 见 *ICML*，2019年，页码6438–6447。'
- en: '[147] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *NeurIPS*, 2014.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络，” 见 *NeurIPS*，2014年。'
- en: '[148] J. Gui, Z. Sun, Y. Wen, D. Tao, and J. Ye, “A review on generative adversarial
    networks: Algorithms, theory, and applications,” *IEEE TKDE*, vol. 35, no. 4,
    pp. 3313–3332, 2023.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] J. Gui, Z. Sun, Y. Wen, D. Tao, 和 J. Ye，“生成对抗网络综述：算法、理论和应用，” *IEEE TKDE*，第35卷，第4期，页码3313–3332，2023年。'
- en: '[149] G. Mariani, F. Scheidegger, R. Istrate, C. Bekas, and C. Malossi, “Bagan:
    Data augmentation with balancing gan,” *arXiv:1803.09655*, 2018.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] G. Mariani, F. Scheidegger, R. Istrate, C. Bekas, 和 C. Malossi，“Bagan:
    数据增强与平衡 gan，” *arXiv:1803.09655*，2018年。'
- en: '[150] S.-W. Huang, C.-T. Lin, S.-P. Chen, Y.-Y. Wu, P.-H. Hsu, and S.-H. Lai,
    “Auggan: Cross domain adaptation with gan-based data augmentation,” in *ECCV*,
    2018.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] S.-W. Huang, C.-T. Lin, S.-P. Chen, Y.-Y. Wu, P.-H. Hsu, 和 S.-H. Lai，“Auggan:
    基于 gan 的跨领域适应数据增强，” 见 *ECCV*，2018年。'
- en: '[151] Z. Yang, Y. Li, and G. Zhou, “Ts-gan: Time-series gan for sensor-based
    health data augmentation,” *ACM TOCH*, vol. 4, no. 2, pp. 1–21, 2022.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Z. Yang, Y. Li, 和 G. Zhou，“Ts-gan: 基于时间序列的 gan 进行传感器健康数据增强，” *ACM TOCH*，第4卷，第2期，页码1–21，2022年。'
- en: '[152] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui,
    and M.-H. Yang, “Diffusion models: A comprehensive survey of methods and applications,”
    *ACM Computing Surveys*, 2023.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui,
    和 M.-H. Yang，“扩散模型：方法与应用的全面调查，” *ACM Computing Surveys*，2023年。'
- en: '[153] C. Xiao, S. X. Xu, and K. Zhang, “Multimodal data augmentation for image
    captioning using diffusion models,” *arXiv:2305.01855*, 2023.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] C. Xiao, S. X. Xu, 和 K. Zhang，“用于图像描述的多模态数据增强，基于扩散模型，” *arXiv:2305.01855*，2023年。'
- en: '[154] P. McNamee and K. Duh, “An extensive exploration of back-translation
    in 60 languages,” in *Findings of ACL*, 2023, pp. 8166–8183.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] P. McNamee 和 K. Duh，“60种语言的回译广泛探索，” 见 *Findings of ACL*，2023年，页码8166–8183。'
- en: '[155] H. Dong, J. Zhang, D. McIlwraith, and Y. Guo, “I2t2i: Learning text to
    image synthesis with textual data augmentation,” in *ICIP*, 2017, pp. 2015–2019.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] H. Dong, J. Zhang, D. McIlwraith, 和 Y. Guo，“I2t2i: 学习文本到图像合成与文本数据增强，”
    见 *ICIP*，2017年，页码2015–2019。'
- en: '[156] D. Lu, Z. Wang, T. Wang, W. Guan, H. Gao, and F. Zheng, “Set-level guidance
    attack: Boosting adversarial transferability of vision-language pre-training models,”
    in *ICCV*, 2023.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] D. Lu, Z. Wang, T. Wang, W. Guan, H. Gao, 和 F. Zheng，“集合级引导攻击：提升视觉语言预训练模型的对抗性迁移性，”
    见 *ICCV*，2023年。'
- en: '[157] Y. Yin, J. Kaddour, X. Zhang, Y. Nie, Z. Liu, L. Kong, and Q. Liu, “Ttida:
    Controllable generative data augmentation via text-to-text and text-to-image models,”
    *arXiv:2304.08821*, 2023.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Y. Yin, J. Kaddour, X. Zhang, Y. Nie, Z. Liu, L. Kong, 和 Q. Liu，“Ttida:
    通过文本到文本和文本到图像模型进行可控生成数据增强，” *arXiv:2304.08821*，2023年。'
- en: '[158] M. Pagliardini, G. Manunza, M. Jaggi, M. I. Jordan, and T. Chavdarova,
    “Improving generalization via uncertainty driven perturbations,” *arXiv:2202.05737*,
    2022.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] M. Pagliardini, G. Manunza, M. Jaggi, M. I. Jordan, 和 T. Chavdarova，“通过不确定性驱动的扰动提高泛化能力，”
    *arXiv:2202.05737*，2022年。'
- en: '[159] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment: Practical
    automated data augmentation with a reduced search space,” *arXiv:1909.13719*,
    2019.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] E. D. Cubuk, B. Zoph, J. Shlens, 和 Q. V. Le，“Randaugment: 实用的自动数据增强与减少的搜索空间，”
    *arXiv:1909.13719*，2019年。'
- en: '[160] Z. Mai, G. Hu, D. Chen, F. Shen, and H. T. Shen, “Metamixup: Learning
    adaptive interpolation policy of mixup with metalearning,” *IEEE TNNLS*, vol. 33,
    no. 7, pp. 3050–3064, 2021.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Z. Mai, G. Hu, D. Chen, F. Shen, 和 H. T. Shen, “Metamixup：通过元学习学习mixup的自适应插值策略，”
    *IEEE TNNLS*，第33卷，第7期，第3050–3064页，2021年。'
- en: '[161] T. Qin, Z. Wang, K. He, Y. Shi, Y. Gao, and D. Shen, “Automatic data
    augmentation via deep reinforcement learning for effective kidney tumor segmentation,”
    in *ICASSP*, 2020, pp. 1419–1423.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] T. Qin, Z. Wang, K. He, Y. Shi, Y. Gao, 和 D. Shen, “通过深度强化学习实现自动数据增强以提高肾脏肿瘤分割效果，”在
    *ICASSP*，2020，第1419–1423页。'
- en: '[162] K. Nishi, Y. Ding, A. Rich, and T. Hollerer, “Augmentation strategies
    for learning with noisy labels,” in *CVPR*, 2021, pp. 8022–8031.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] K. Nishi, Y. Ding, A. Rich, 和 T. Hollerer, “噪声标签学习的增强策略，”在 *CVPR*，2021，第8022–8031页。'
- en: '[163] Y. Li, G. Hu, Y. Wang, T. Hospedales, N. M. Robertson, and Y. Yang, “Differentiable
    automatic data augmentation,” in *ECCV*, A. Vedaldi, H. Bischof, T. Brox, and
    J.-M. Frahm, Eds., 2020, pp. 580–595.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Y. Li, G. Hu, Y. Wang, T. Hospedales, N. M. Robertson, 和 Y. Yang, “可微分自动数据增强，”在
    *ECCV*，A. Vedaldi, H. Bischof, T. Brox, 和 J.-M. Frahm 编辑，2020，第580–595页。'
- en: '[164] X. Chen, Y. Zhou, D. Wu, W. Zhang, Y. Zhou, B. Li, and W. Wang, “Imagine
    by reasoning: A reasoning-based implicit semantic data augmentation for long-tailed
    classification,” in *AAAI*, Online, February 2022, pp. 356–364.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] X. Chen, Y. Zhou, D. Wu, W. Zhang, Y. Zhou, B. Li, 和 W. Wang, “通过推理想象：基于推理的隐式语义数据增强用于长尾分类，”在
    *AAAI*，在线，2022年2月，第356–364页。'
- en: '[165] X. Zhou and O. Wu, “Implicit counterfactual data augmentation for deep
    neural networks,” *arXiv:2304.13431*, 2023.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] X. Zhou 和 O. Wu, “深度神经网络的隐式反事实数据增强，” *arXiv:2304.13431*，2023年。'
- en: '[166] B. Li, F. Wu, S.-N. Lim, S. Belongie, and K. Q. Weinberger, “On feature
    normalization and data augmentation,” in *CVPR*, 2021, pp. 12 383–12 392.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] B. Li, F. Wu, S.-N. Lim, S. Belongie, 和 K. Q. Weinberger, “关于特征归一化和数据增强，”在
    *CVPR*，2021，第12 383–12 392页。'
- en: '[167] D. LeJeune, R. Balestriero, H. Javadi, and R. G. Baraniuk, “Implicit
    rugosity regularization via data augmentation,” *arXiv:1905.11639*, 2019.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] D. LeJeune, R. Balestriero, H. Javadi, 和 R. G. Baraniuk, “通过数据增强实现隐式粗糙度正则化，”
    *arXiv:1905.11639*，2019年。'
- en: '[168] H. Guo, Y. Mao, and R. Zhang, “Mixup as locally linear out-of-manifold
    regularization,” *AAAI*, pp. 3714–3722, 2019.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] H. Guo, Y. Mao, 和 R. Zhang, “Mixup作为局部线性出界正则化，” *AAAI*，第3714–3722页，2019年。'
- en: '[169] C. F. G. D. Santos and J. P. Papa, “Avoiding overfitting: A survey on
    regularization methods for convolutional neural networks,” *ACM Computing Surveys*,
    vol. 54, no. 10, pp. 1–25, 2022.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] C. F. G. D. Santos 和 J. P. Papa, “避免过拟合：卷积神经网络正则化方法的综述，” *ACM计算机调查*，第54卷，第10期，第1–25页，2022年。'
- en: '[170] C.-H. Lin, C. Kaushik, E. L. Dyer, and V. Muthukumar, “The good, the
    bad and the ugly sides of data augmentation: An implicit spectral regularization
    perspective,” *arXiv:2210.05021*, 2022.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] C.-H. Lin, C. Kaushik, E. L. Dyer, 和 V. Muthukumar, “数据增强的好、坏和丑陋面：一种隐式谱正则化视角，”
    *arXiv:2210.05021*，2022年。'
- en: '[171] S. Chen, E. Dobriban, and J. Lee, “A group-theoretic framework for data
    augmentation,” *Journal of Machine Learning Research*, vol. 21, no. 1, pp. 9885–9955,
    2020.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] S. Chen, E. Dobriban, 和 J. Lee, “数据增强的群体理论框架，” *机器学习研究杂志*，第21卷，第1期，第9885–9955页，2020年。'
- en: '[172] A. Jeddi, M. J. Shafiee, M. Karg, C. Scharfenberger, and A. Wong, “Learn2perturb:
    An end-to-end feature perturbation learning to improve adversarial robustness,”
    in *CVPR*, 2020.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] A. Jeddi, M. J. Shafiee, M. Karg, C. Scharfenberger, 和 A. Wong, “Learn2perturb：一种端到端特征扰动学习以提高对抗鲁棒性，”在
    *CVPR*，2020年。'
- en: '[173] M. Shu, Z. Wu, M. Goldblum, and T. Goldstein, “Encoding robustness to
    image style via adversarial feature perturbations,” in *NeurIPS*, 2021, pp. 28 042–28 053.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] M. Shu, Z. Wu, M. Goldblum, 和 T. Goldstein, “通过对抗特征扰动编码图像风格的鲁棒性，”在 *NeurIPS*，2021，第28 042–28 053页。'
- en: '[174] A. K. Menon, S. Jayasumana, A. S. Rawat, H. Jain, A. Veit, and S. Kumar,
    “Long-tail learning via logit adjustment,” in *ICLR*, 2021.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] A. K. Menon, S. Jayasumana, A. S. Rawat, H. Jain, A. Veit, 和 S. Kumar,
    “通过logit调整进行长尾学习，”在 *ICLR*，2021年。'
- en: '[175] K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, “Learning imbalanced
    datasets with label-distribution-aware margin loss,” in *NeurIPS*, 2019, pp. 1567–1578.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] K. Cao, C. Wei, A. Gaidon, N. Arechiga, 和 T. Ma, “通过标签分布感知边际损失学习不平衡数据集，”在
    *NeurIPS*，2019，第1567–1578页。'
- en: '[176] M. Li, F. Su, O. Wu, and J. Zhang, “Class-level logit perturbation,”
    *IEEE TNNLS*, 2023.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] M. Li, F. Su, O. Wu, 和 J. Zhang, “类别级别的logit扰动，” *IEEE TNNLS*，2023年。'
- en: '[177] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *CVPR*, 2016, pp. 2818–2826.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, 和 Z. Wojna, “重新思考计算机视觉的Inception架构，”在
    *CVPR*，2016，第2818–2826页。'
- en: '[178] M. Goibert and E. Dohmatob, “Adversarial robustness via label-smoothing,”
    *arXiv:1906.11567*, 2019.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] M. Goibert 和 E. Dohmatob， “通过标签平滑提升对抗鲁棒性”，*arXiv:1906.11567*，2019年。'
- en: '[179] J. Lienen and E. Hüllermeier, “From label smoothing to label relaxation,”
    in *AAAI*, 2021, pp. 8583–8591.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] J. Lienen 和 E. Hüllermeier， “从标签平滑到标签松弛”，发表在*AAAI*，2021年，页码8583–8591。'
- en: '[180] A. Orvieto, H. Kersting, F. Proske, F. Bach, and A. Lucchi, “Anticorrelated
    noise injection for improved generalization,” in *ICML*, 2022, pp. 17 094–17 116.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] A. Orvieto, H. Kersting, F. Proske, F. Bach, 和 A. Lucchi， “用于改善泛化的反相关噪声注入”，发表在*ICML*，2022年，页码17 094–17 116。'
- en: '[181] D. Wu, S.-T. Xia, and Y. Wang, “Adversarial weight perturbation helps
    robust generalization,” in *NeurIPS*, 2020, pp. 2958–2969.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] D. Wu, S.-T. Xia, 和 Y. Wang， “对抗权重扰动有助于鲁棒泛化”，发表在*NeurIPS*，2020年，页码2958–2969。'
- en: '[182] J. Wang, Y. Liu, and B. Li, “Reinforcement learning with perturbed rewards,”
    in *AAAI*, 2020, pp. 6202–6209.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] J. Wang, Y. Liu, 和 B. Li， “扰动奖励的强化学习”，发表在*AAAI*，2020年，页码6202–6209。'
- en: '[183] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. Rabinovich,
    “Training deep neural networks on noisy labels with bootstrapping,” in *ICLR Workshop*,
    2015.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, 和 A. Rabinovich，
    “在噪声标签上训练深度神经网络的自助法”，发表在*ICLR Workshop*，2015年。'
- en: '[184] P. Benz, C. Zhang, A. Karjauv, and I. S. Kweon, “Universal adversarial
    training with class-wise perturbations,” in *ICME*, 2021, pp. 1–6.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] P. Benz, C. Zhang, A. Karjauv, 和 I. S. Kweon， “带有类别级扰动的通用对抗训练”，发表在*ICME*，2021年，页码1–6。'
- en: '[185] Y. Wang, J. Fei, H. Wang, W. Li, T. Bao, L. Wu, R. Zhao, and Y. Shen,
    “Balancing logit variation for long-tailed semantic segmentation,” in *CVPR*,
    2023, pp. 19 561–19 573.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Y. Wang, J. Fei, H. Wang, W. Li, T. Bao, L. Wu, R. Zhao, 和 Y. Shen， “长尾语义分割的对数变异平衡”，发表在*CVPR*，2023年，页码19 561–19 573。'
- en: '[186] A. Shafahi, M. Najibi, Z. Xu, J. Dickerson, L. S. Davis, and T. Goldstein,
    “Universal adversarial training,” in *CVPR*, 2017, pp. 5636–5643.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] A. Shafahi, M. Najibi, Z. Xu, J. Dickerson, L. S. Davis, 和 T. Goldstein，
    “通用对抗训练”，发表在*CVPR*，2017年，页码5636–5643。'
- en: '[187] A. Chaubey, N. Agrawal, K. Barnwal, K. K. Guliani, and P. Mehta, “Universal
    adversarial perturbations: A survey,” *arXiv:2005.08087*, 2020.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] A. Chaubey, N. Agrawal, K. Barnwal, K. K. Guliani, 和 P. Mehta， “通用对抗扰动：综述”，*arXiv:2005.08087*，2020年。'
- en: '[188] T. Wu, Q. Huang, Z. Liu, Y. Wang, and D. Lin, “Distribution-balanced
    loss for multi-label classification in long-tailed datasets,” in *ECCV*, 2020,
    pp. 162–178.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] T. Wu, Q. Huang, Z. Liu, Y. Wang, 和 D. Lin， “长尾数据集中多标签分类的分布平衡损失”，发表在*ECCV*，2020年，页码162–178。'
- en: '[189] W. Zhou, X. Hou, Y. Chen, M. Tang, X. Huang, X. Gan, and Y. Yang, “Transferable
    adversarial perturbations,” in *ECCV*, 2018, pp. 452–467.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] W. Zhou, X. Hou, Y. Chen, M. Tang, X. Huang, X. Gan, 和 Y. Yang， “可转移的对抗扰动”，发表在*ECCV*，2018年，页码452–467。'
- en: '[190] X. Wei, J. Zhu, S. Yuan, and H. Su, “Sparse adversarial perturbations
    for videos,” in *AAAI*, 2019, pp. 8973–8980.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] X. Wei, J. Zhu, S. Yuan, 和 H. Su， “视频的稀疏对抗扰动”，发表在*AAAI*，2019年，页码8973–8980。'
- en: '[191] Y. Zhu, Y. Ye, M. Li, J. Zhang, and O. Wu, “Investigating annotation
    noise for named entity recognition,” *Neural Comput. Appl.*, vol. 35, no. 1, pp.
    993–1007, 2023.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] Y. Zhu, Y. Ye, M. Li, J. Zhang, 和 O. Wu， “探讨命名实体识别中的注释噪声”，*Neural Comput.
    Appl.*，第35卷，第1期，页码993–1007，2023年。'
- en: '[192] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework
    for contrastive learning of visual representations,” in *ICML*, 2020, pp. 1597–1607.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] T. Chen, S. Kornblith, M. Norouzi, 和 G. Hinton， “视觉表征对比学习的简单框架”，发表在*ICML*，2020年，页码1597–1607。'
- en: '[193] M. Naseer, S. Khan, M. Hayat, F. S. Khan, and F. Porikli, “A self-supervised
    approach for adversarial robustness,” in *CVPR*, 2020, pp. 262–271.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] M. Naseer, S. Khan, M. Hayat, F. S. Khan, 和 F. Porikli， “自监督对抗鲁棒性的方法”，发表在*CVPR*，2020年，页码262–271。'
- en: '[194] Z. Zhang, S.-h. Zhong, and Y. Liu, “Ganser: A self-supervised data augmentation
    framework for eeg-based emotion recognition,” *IEEE TAC*, 2022.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Z. Zhang, S.-h. Zhong, 和 Y. Liu， “Ganser：一种自监督数据增强框架用于基于EEG的情感识别”，*IEEE
    TAC*，2022年。'
- en: '[195] S. Li, K. Gong, C. H. Liu, Y. Wang, F. Qiao, and X. Cheng, “Metasaug:
    Meta semantic augmentation for long-tailed visual recognition,” in *CVPR*, 2021,
    pp. 5212–5221.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] S. Li, K. Gong, C. H. Liu, Y. Wang, F. Qiao, 和 X. Cheng， “Metasaug: 针对长尾视觉识别的元语义增强”，发表在*CVPR*，2021年，页码5212–5221。'
- en: '[196] F. Qiao and X. Peng, “Uncertainty-guided model generalization to unseen
    domains,” in *CVPR*, 2021, pp. 6790–6800.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] F. Qiao 和 X. Peng， “不确定性引导的模型泛化到未见领域”，发表在*CVPR*，2021年，页码6790–6800。'
- en: '[197] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, “Autoaugment:
    Learning augmentation policies from data,” in *CVPR*, 2019.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, 和 Q. V. Le， “Autoaugment：从数据中学习增强策略”，发表在*CVPR*，2019年。'
- en: '[198] T. Niu and M. Bansal, “Automatically learning data augmentation policies
    for dialogue tasks,” in *EMNLP*, 2019.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] T. Niu 和 M. Bansal，“自动学习对话任务的数据增强策略”，发表于 *EMNLP*，2019年。'
- en: '[199] G. Apruzzese, M. Andreolini, M. Marchetti, A. Venturi, and M. Colajanni,
    “Deep reinforcement adversarial learning against botnet evasion attacks,” *IEEE
    TNSE*, vol. 17, no. 4, pp. 1975–1987, 2020.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] G. Apruzzese, M. Andreolini, M. Marchetti, A. Venturi 和 M. Colajanni，“对抗性强化学习对抗僵尸网络规避攻击”，*IEEE
    TNSE*，第17卷，第4期，第1975–1987页，2020年。'
- en: '[200] B. Lin, Y. Zhu, Y. Long, X. Liang, Q. Ye, and L. Lin, “Adversarial reinforced
    instruction attacker for robust vision-language navigation,” *IEEE TPAMI*, vol. 44,
    no. 10, pp. 7175–7189, 2022.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] B. Lin, Y. Zhu, Y. Long, X. Liang, Q. Ye 和 L. Lin，“对抗性强化指令攻击者用于鲁棒的视觉-语言导航”，*IEEE
    TPAMI*，第44卷，第10期，第7175–7189页，2022年。'
- en: '[201] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, and G. Bontempi, “Credit
    card fraud detection: A realistic modeling and a novel learning strategy,” *IEEE
    TNNLS*, vol. 29, no. 8, pp. 3784–3797, 2017.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi 和 G. Bontempi，“信用卡欺诈检测：一种现实建模和新颖的学习策略”，*IEEE
    TNNLS*，第29卷，第8期，第3784–3797页，2017年。'
- en: '[202] Y. Zhang, P. Zhao, Q. Wu, B. Li, J. Huang, and M. Tan, “Cost-sensitive
    portfolio selection via deep reinforcement learning,” *IEEE TKDE*, vol. 34, no. 1,
    pp. 236–248, 2022.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] Y. Zhang, P. Zhao, Q. Wu, B. Li, J. Huang 和 M. Tan，“通过深度强化学习进行成本敏感的投资组合选择”，*IEEE
    TKDE*，第34卷，第1期，第236–248页，2022年。'
- en: '[203] D. Gan, J. Shen, B. An, M. Xu, and N. Liu, “Integrating tanbn with cost
    sensitive classification algorithm for imbalanced data in medical diagnosis,”
    *Computers & Industrial Engineering*, vol. 140, p. 106266, 2020.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] D. Gan, J. Shen, B. An, M. Xu 和 N. Liu，“将 tanbn 与成本敏感分类算法结合用于医疗诊断中的不平衡数据”，*Computers
    & Industrial Engineering*，第140卷，第106266页，2020年。'
- en: '[204] Y. Dong, J. Ma, S. Wang, C. Chen, and J. Li, “Fairness in graph mining:
    A survey,” *IEEE TKDE*, pp. 1–22, 2023.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Y. Dong, J. Ma, S. Wang, C. Chen 和 J. Li，“图挖掘中的公平性：综述”，*IEEE TKDE*，第1–22页，2023年。'
- en: '[205] W. Wang, F. Feng, X. He, L. Nie, and T. Chua, “Denoising implicit feedback
    for recommendation,” in *WSDM*, 2021, pp. 373–381.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] W. Wang, F. Feng, X. He, L. Nie 和 T. Chua，“去噪隐式反馈推荐”，发表于 *WSDM*，2021年，第373–381页。'
- en: '[206] T. Castells, P. Weinzaepfel, and J. Revaud, “Superloss: A generic loss
    for robust curriculum learning,” in *NeurIPS*, 2020, pp. 1–12.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] T. Castells, P. Weinzaepfel, 和 J. Revaud，“Superloss: 一种通用的鲁棒课程学习损失”，发表于
    *NeurIPS*，2020年，第1–12页。'
- en: '[207] S. Zhang, Z. Li, S. Yan, X. He, , and J. Sun, “Distribution alignment:
    A unified framework for long-tail visual recognition,” in *CVPR*, 2021, pp. 2361–2370.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] S. Zhang, Z. Li, S. Yan, X. He 和 J. Sun，“分布对齐：一种用于长尾视觉识别的统一框架”，发表于 *CVPR*，2021年，第2361–2370页。'
- en: '[208] K. R. M. Fernando and C. P. Tsokos, “Dynamically weighted balanced loss:
    Class imbalanced learning and confidence calibration of deep neural networks,”
    *IEEE TNNLS*, vol. 33, no. 7, pp. 2940–2951, 2022.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] K. R. M. Fernando 和 C. P. Tsokos，“动态加权平衡损失：类别不平衡学习和深度神经网络的置信度校准”，*IEEE
    TNNLS*，第33卷，第7期，第2940–2951页，2022年。'
- en: '[209] E. Z. Liu, B. Haghgoo, A. S. Chen, A. Raghunathan, P. W. Koh, S. Sagawa,
    P. Liang, and C. Finn, “Just train twice: Improving group robustness without training
    group information,” in *ICML*, 2021, pp. 6781–6792.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] E. Z. Liu, B. Haghgoo, A. S. Chen, A. Raghunathan, P. W. Koh, S. Sagawa,
    P. Liang 和 C. Finn，“仅训练两次：在没有训练组信息的情况下提高组鲁棒性”，发表于 *ICML*，2021年，第6781–6792页。'
- en: '[210] J. Zhang, J. Zhu, G. Niu, B. Han, M. Sugiyama, and M. Kankanhalli, “Geometry-aware
    instance-reweighted adversarial training,” in *ICLR*, 2021.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] J. Zhang, J. Zhu, G. Niu, B. Han, M. Sugiyama, 和 M. Kankanhalli，“几何感知的实例重加权对抗训练”，发表于
    *ICLR*，2021年。'
- en: '[211] L. Jiang, D. Meng, T. Mitamural, and A. G. Hauptmann, “Easy samples first:
    Self-paced reranking for zero-example multimedia search,” in *ACM MM*, 2014, pp.
    547–556.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] L. Jiang, D. Meng, T. Mitamural, 和 A. G. Hauptmann，“优先考虑简单样本：零示例多媒体搜索的自适应重新排序”，发表于
    *ACM MM*，2014年，第547–556页。'
- en: '[212] L. Jiang, D. Meng, S. Yu, Z. Lan, S. Shan, and A.-G. Hauptmann, “Self-paced
    learning with diversity,” in *NeurIPS*, 2014, pp. 2078–2086.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] L. Jiang, D. Meng, S. Yu, Z. Lan, S. Shan 和 A.-G. Hauptmann，“具有多样性的自适应学习”，发表于
    *NeurIPS*，2014年，第2078–2086页。'
- en: '[213] D. Zhang, D. Meng, C. Li, L. Jiang, Q. Zhao, and J. Han, “A self-paced
    multiple-instance learning framework for co-saliency detection,” in *ICCV*, 2015,
    pp. 594–602.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] D. Zhang, D. Meng, C. Li, L. Jiang, Q. Zhao 和 J. Han，“一种自适应多实例学习框架用于共同显著性检测”，发表于
    *ICCV*，2015年，第594–602页。'
- en: '[214] P. Soviany, R. T. Ionescu, P. Rota, and N. Sebe, “Curriculum learning:
    A survey,” *IJCV*, vol. 130, no. 6, pp. 1526–1565, 2022.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] P. Soviany, R. T. Ionescu, P. Rota, 和 N. Sebe，“课程学习：综述”，*IJCV*，第130卷，第6期，第1526–1565页，2022年。'
- en: '[215] C. Santiagoa, C. Barataa, M. Sasdellib, G. Carneirob, and J. C.Nasciment,
    “Low: Training deep neural networks by learning optimal sample weights,” *Pattern
    Recognition*, vol. 110, no. 1, pp. 1–12, 2021.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] C. Santiagoa, C. Barataa, M. Sasdellib, G. Carneirob, 和 J. C. Nasciment，“Low:
    通过学习最佳样本权重训练深度神经网络，” *Pattern Recognition*，第110卷，第1期，页1–12，2021年。'
- en: '[216] P. Soviany, “Curriculum learning with diversity for supervised computer
    vision tasks,” in *ICML Workshop*, 2020.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] P. Soviany，“具有多样性的课程学习用于监督计算机视觉任务，” 见 *ICML Workshop*，2020年。'
- en: '[217] X. Zhou and O. Wu, “Which samples should be learned first: Easy or hard?”
    *IEEE TNNLS*, pp. 1–15, 2023.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] X. Zhou 和 O. Wu，“应优先学习哪些样本：简单的还是困难的？” *IEEE TNNLS*，页1–15，2023年。'
- en: '[218] W. Zhang, Y. Wang, and Y. Qiao, “Metacleaner: Learning to hallucinate
    clean representations for noisy-labeled visual recognition,” in *CVPR*, June 2019,
    pp. 7373–7382.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] W. Zhang, Y. Wang, 和 Y. Qiao，“Metacleaner: 学习为噪声标记的视觉识别生成干净表示，” 见 *CVPR*，2019年6月，页7373–7382。'
- en: '[219] C. Northcutt, L. Jiang, and I. Chuang, “Confident learning: Estimating
    uncertainty in dataset labels,” *JAIR*, vol. 70, pp. 1373–1411, 2021.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] C. Northcutt, L. Jiang, 和 I. Chuang，“自信学习：估计数据集标签的不确定性，” *JAIR*，第70卷，页1373–1411，2021年。'
- en: '[220] Z. Han, Z. Liang, F. Yang, L. Liu, L. Li, Y. Bian, P. Zhao, B. Wu, C. Zhang,
    and J. Yao, “Umix: Improving importance weighting for subpopulation shift via
    uncertainty-aware mixup,” in *NeurIPS*, 2022, pp. 37 704–37 718.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] Z. Han, Z. Liang, F. Yang, L. Liu, L. Li, Y. Bian, P. Zhao, B. Wu, C.
    Zhang, 和 J. Yao，“Umix: 通过不确定性感知混合提高子群体转移的重要性加权，” 见 *NeurIPS*，2022年，页37,704–37,718。'
- en: '[221] T. Liu and D. Tao, “Classification with noisy labels by importance reweighting,”
    *IEEE TPAMI*, vol. 38, no. 3, p. 447–461, 2023.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] T. Liu 和 D. Tao，“通过重要性重加权进行带噪声标签的分类，” *IEEE TPAMI*，第38卷，第3期，页447–461，2023年。'
- en: '[222] Y. Fan, R. He, J. Liang, and B. Hu, “Self-paced learning: An implicit
    regularization perspective,” in *AAAI*, 2017.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] Y. Fan, R. He, J. Liang, 和 B. Hu，“自适应学习：一种隐式正则化视角，” 见 *AAAI*，2017年。'
- en: '[223] X. Gu, X. Yu, Y. Yang, J. Sun, and Z. Xu, “Adversarial reweighting for
    partial domain adaptation,” in *NeurIPS*, 2021, pp. 14 860–14 872.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] X. Gu, X. Yu, Y. Yang, J. Sun, 和 Z. Xu，“部分领域适应的对抗性重加权，” 见 *NeurIPS*，2021年，页14,860–14,872。'
- en: '[224] M. Yi, L. Hou, L. Shang, X. Jiang, Q. Liu, and Z.-M. Ma, “Reweighting
    augmented samples by minimizing the maximal expected loss,” in *ICLR*, 2021.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] M. Yi, L. Hou, L. Shang, X. Jiang, Q. Liu, 和 Z.-M. Ma，“通过最小化最大期望损失来重加权增强样本，”
    见 *ICLR*，2021年。'
- en: '[225] M. Ren, W. Zeng, B. Yang, and R. Urtasun, “Learning to reweight examples
    for robust deep learning,” in *ICML*, 2018, pp. 4334–4343.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] M. Ren, W. Zeng, B. Yang, 和 R. Urtasun，“为鲁棒深度学习学习重新加权样本，” 见 *ICML*，2018年，页4334–4343。'
- en: '[226] Q. Zhao, J. Shu, X. Yuan, Z. Liu, and D. Meng, “A probabilistic formulation
    for meta-weight-net,” *IEEE TNNLS*, vol. 34, no. 3, pp. 1194–1208, 2023.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] Q. Zhao, J. Shu, X. Yuan, Z. Liu, 和 D. Meng，“Meta-weight-net的概率性公式，”
    *IEEE TNNLS*，第34卷，第3期，页1194–1208，2023年。'
- en: '[227] N. N. Trung, L. N. Van, and T. H. Nguyen, “Unsupervised domain adaptation
    for text classification via meta self-paced learning,” in *COLING*, 2022, pp.
    4741–4752.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] N. N. Trung, L. N. Van, 和 T. H. Nguyen，“通过元自适应学习进行文本分类的无监督领域适应，” 见 *COLING*，2022年，页4741–4752。'
- en: '[228] J. Wei, X. Xu, Z. Wang, and G. Wang, “Meta self-paced learning for cross-modal
    matching,” in *ACM MM*, 2021, pp. 3835–3843.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] J. Wei, X. Xu, Z. Wang, 和 G. Wang，“跨模态匹配的元自适应学习，” 见 *ACM MM*，2021年，页3835–3843。'
- en: '[229] S. Li, W. Ma, J. Zhang, C. H. Liu, J. Liang, and G. Wang, “Meta-reweighted
    regularization for unsupervised domain adaptation,” *IEEE TKDE*, vol. 35, no. 3,
    pp. 2781–2795, 2023.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] S. Li, W. Ma, J. Zhang, C. H. Liu, J. Liang, 和 G. Wang，“用于无监督领域适应的元重加权正则化，”
    *IEEE TKDE*，第35卷，第3期，页2781–2795，2023年。'
- en: '[230] F. Zhou, J. Li, C. Xie, F. Chen, L. Hong, R. Sun, and Z. Li, “Metaaugment:
    Sample-aware data augmentation policy learning,” in *AAAI*, 2021, pp. 11 097–11 105.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] F. Zhou, J. Li, C. Xie, F. Chen, L. Hong, R. Sun, 和 Z. Li，“Metaaugment:
    样本感知数据增强策略学习，” 见 *AAAI*，2021年，页11,097–11,105。'
- en: '[231] Y. Ge, M. Rahmani, A. Irissappane, J. Sepulveda, J. Caverlee, and F. Wang,
    “Automated data denoising for recommendation,” *arXiv:2305.07070*, 2023.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] Y. Ge, M. Rahmani, A. Irissappane, J. Sepulveda, J. Caverlee, 和 F. Wang，“自动化数据去噪用于推荐，”
    *arXiv:2305.07070*，2023。'
- en: '[232] T. Fang, N. Lu, G. Niu, and M. Sugiyama, “Rethinking importance weighting
    for deep learning under distribution shift,” in *NeurIPS*, 2020, pp. 11 996–12 007.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] T. Fang, N. Lu, G. Niu, 和 M. Sugiyama，“在分布变化下重新思考深度学习的重要性加权，” 见 *NeurIPS*，2020年，页11,996–12,007。'
- en: '[233] T. Wang, J. Y. Zhu, A. Torralba, and A. A. Efros, “Dataset distillation,”
    *arXiv:1811.10959*, 2018.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] T. Wang, J. Y. Zhu, A. Torralba, 和 A. A. Efros，“数据集蒸馏，” *arXiv:1811.10959*，2018年。'
- en: '[234] S. Lei and D. Tao, “A comprehensive survey of dataset distillation,”
    *arXiv:2301.05603*, 2023.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] S. Lei 和 D. Tao，“数据集蒸馏的综合调查”，*arXiv:2301.05603*，2023年。'
- en: '[235] N. Sachdeva and J. McAuley, “Data distillation: A survey,” *arXiv:2301.04272v1*,
    2023.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] N. Sachdeva 和 J. McAuley，“数据蒸馏：综述”，*arXiv:2301.04272v1*，2023年。'
- en: '[236] B. Zhao, K. R. Mopuri, and H. Bilen, “Dataset condensation with gradient
    matching,” in *ICLR*, 2021.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] B. Zhao, K. R. Mopuri 和 H. Bilen，“通过梯度匹配进行数据集浓缩”，发表于*ICLR*，2021年。'
- en: '[237] Z. Deng and O. Russakovsky, “Remember the past: Distilling datasets into
    addressable memories for neural networks,” in *NeurIPS*, 2022.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] Z. Deng 和 O. Russakovsky，“记住过去：将数据集蒸馏为神经网络可寻址的记忆”，发表于*NeurIPS*，2022年。'
- en: '[238] N. Loo, R. Hasani, A. Amini, and D. Rus, “Efficient dataset distillation
    using random feature approximation,” in *NeurIPS*, 2022.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] N. Loo, R. Hasani, A. Amini 和 D. Rus，“利用随机特征近似实现高效数据集蒸馏”，发表于*NeurIPS*，2022年。'
- en: '[239] Y. Zhou, E. Nezhadarya, and J. Ba, “Dataset distillation using neural
    feature regression,” in *NeurIPS*, 2022.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] Y. Zhou, E. Nezhadarya 和 J. Ba，“利用神经特征回归进行数据集蒸馏”，发表于*NeurIPS*，2022年。'
- en: '[240] B. Zhao and H. Bilen, “Dataset condensation with differentiable siamese
    augmentation,” in *ICML*, 2021, pp. 12 674–12 685.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] B. Zhao 和 H. Bilen，“通过可微分的Siamese增强进行数据集浓缩”，发表于*ICML*，2021年，第12 674–12 685页。'
- en: '[241] J.-H. Kim, J. Kim, S. J. Oh, S. Yun, H. Song, J. Jeong, J.-W. Ha, and
    H. O. Song, “Dataset condensation via efficient synthetic-data parameterization,”
    in *ICML*, 2022.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] J.-H. Kim, J. Kim, S. J. Oh, S. Yun, H. Song, J. Jeong, J.-W. Ha 和 H.
    O. Song，“通过高效的合成数据参数化进行数据集浓缩”，发表于*ICML*，2022年。'
- en: '[242] G. Cazenavette, T. Wang, A. Torralba, A. A. Efros, and J.-Y. Zhu, “Dataset
    distillation by matching training trajectories,” in *CVPR*, 2022.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] G. Cazenavette, T. Wang, A. Torralba, A. A. Efros 和 J.-Y. Zhu，“通过匹配训练轨迹进行数据集蒸馏”，发表于*CVPR*，2022年。'
- en: '[243] J. Cui, R. Wang, S. Si, and C.-J. Hsieh, “Scaling up dataset distillation
    to imagenet-1k with constant memory,” *arXiv:2211.10586*, 2022.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] J. Cui, R. Wang, S. Si 和 C.-J. Hsieh，“将数据集蒸馏扩展到ImageNet-1k并保持恒定内存”，*arXiv:2211.10586*，2022年。'
- en: '[244] B. Zhao and H. Bilen, “Dataset condensation with distribution matching,”
    in *WACV*, 2023.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] B. Zhao 和 H. Bilen，“通过分布匹配进行数据集浓缩”，发表于*WACV*，2023年。'
- en: '[245] K. Wang, B. Zhao, X. Peng, Z. Zhu, S. Yang, S. Wang, G. Huang, H. Bilen,
    X. Wang, and Y. You, “Cafe: Learning to condense dataset by aligning features,”
    in *CVPR*, 2022, pp. 12 196–12 205.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] K. Wang, B. Zhao, X. Peng, Z. Zhu, S. Yang, S. Wang, G. Huang, H. Bilen,
    X. Wang 和 Y. You，“Cafe：通过对齐特征学习浓缩数据集”，发表于*CVPR*，2022年，第12 196–12 205页。'
- en: '[246] X. Zhou, R. Pi, W. Zhang, Y. Lin, Z. Chen, and T. Zhang, “Probabilistic
    bilevel coreset selection,” in *ICML*, 2022, pp. 27 287–27 302.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] X. Zhou, R. Pi, W. Zhang, Y. Lin, Z. Chen 和 T. Zhang，“概率双层核心集选择”，发表于*ICML*，2022年，第27 287–27 302页。'
- en: '[247] B. Zhao and H. Bilen, “Synthesizing informative training samples with
    gan,” in *NeurIPS Workshop*, 2022.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] B. Zhao 和 H. Bilen，“利用GAN合成信息性训练样本”，发表于*NeurIPS Workshop*，2022年。'
- en: '[248] B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli, and A. S. Morcos., “Beyond
    neural scaling laws: beating power law scaling via data pruning,” in *NeurIPS*,
    2022.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli 和 A. S. Morcos，“超越神经缩放定律：通过数据剪枝击败幂律缩放”，发表于*NeurIPS*，2022年。'
- en: '[249] Z. Qin, K. Wang, Z. Zheng, J. Gu, X. Peng, D. Zhou, and Y. You, “Infobatch:
    Lossless training speed up by unbiased dynamic data pruning,” *arXiv:2303.04947*,
    2023.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] Z. Qin, K. Wang, Z. Zheng, J. Gu, X. Peng, D. Zhou 和 Y. You，“Infobatch：通过无偏动态数据剪枝实现无损训练加速”，*arXiv:2303.04947*，2023年。'
- en: '[250] S. Liu, K. Wang, X. Yang, J. Ye, and X. Wang, “Dataset distillation via
    factorization,” in *NeurIPS*, 2022.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] S. Liu, K. Wang, X. Yang, J. Ye 和 X. Wang，“通过分解进行数据集蒸馏”，发表于*NeurIPS*，2022年。'
- en: '[251] K. Meding, L. M. S. Buschoff, R. Geirhos, and F. A. Wichmann, “Trivial
    or impossible – dichotomous data difficulty masks model differences (on imagenet
    and beyond),” in *ICLR*, 2022.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] K. Meding, L. M. S. Buschoff, R. Geirhos, 和 F. A. Wichmann，“平凡还是不可能——二分数据难度掩盖模型差异（在ImageNet及其他领域）”，发表于*ICLR*，2022年。'
- en: '[252] V. Feldman and C. Zhang, “What neural networks memorize and why: Discovering
    the long tail via influence estimation,” in *NeurIPS*, 2020, pp. 2881–2891.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] V. Feldman 和 C. Zhang，“神经网络记住什么以及为何如此：通过影响估计发现长尾”，发表于*NeurIPS*，2020年，第2881–2891页。'
- en: '[253] C. G. Northcutt, T. Wu, and I. L. Chuang, “Learning with confident examples:
    Rank pruning for robust classification with noisy labels,” *arXiv:1705.01936*,
    2017.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] C. G. Northcutt, T. Wu 和 I. L. Chuang，“自信样本学习：通过排名剪枝实现带噪声标签的鲁棒分类”，*arXiv:1705.01936*，2017年。'
- en: '[254] V. Kaushal, R. Iyer, S. Kothawade, R. Mahadev, K. Doctor, and G. Ramakrishnan,
    “Learning from less data: A unified data subset selection and active learning
    framework for computer vision,” in *IEEE WACV*, 2019, pp. 1289–1299.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] V. Kaushal, R. Iyer, S. Kothawade, R. Mahadev, K. Doctor 和 G. Ramakrishnan，“从少量数据中学习：用于计算机视觉的统一数据子集选择和主动学习框架”，在
    *IEEE WACV*，2019年，第1289–1299页。'
- en: '[255] Y. Yang, H. Kang, and B. Mirzasoleiman, “Towards sustainable learning:
    Coresets for data-efficient deep learning,” in *ICML*, 2023.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] Y. Yang, H. Kang 和 B. Mirzasoleiman，“迈向可持续学习：用于数据高效深度学习的核心集”，在 *ICML*，2023年。'
- en: '[256] B. Mirzasoleiman, J. Bilmes, and J. Leskovec, “Coresets for data-efficient
    training of machine learning models,” in *ICML*, 2020, pp. 6950–6960.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] B. Mirzasoleiman, J. Bilmes 和 J. Leskovec，“用于数据高效训练机器学习模型的核心集”，在 *ICML*，2020年，第6950–6960页。'
- en: '[257] Z. Liu, H. Jin, T.-H. Wang, K. Zhou, and X. Hu, “Divaug: Plug-in automated
    data augmentation with explicit diversity maximization,” in *ICCV*, 2021, pp.
    4762–4770.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] Z. Liu, H. Jin, T.-H. Wang, K. Zhou 和 X. Hu，“Divaug：插件式自动数据增强与显式多样性最大化”，在
    *ICCV*，2021年，第4762–4770页。'
- en: '[258] K. J. Joseph, K. Singh, and V. N. Balasubramanian, “Submodular batch
    selection for training deep neural networks,” in *IJCAI*, 2019, pp. 2677–2683.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] K. J. Joseph, K. Singh 和 V. N. Balasubramanian，“用于训练深度神经网络的子模批选择”，在 *IJCAI*，2019年，第2677–2683页。'
- en: '[259] W. Li, G. Dasarathy, and V. Berisha, “Regularization via structural label
    smoothing,” in *AISTATS*, 2020, pp. 1453–1463.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] W. Li, G. Dasarathy 和 V. Berisha，“通过结构化标签平滑进行正则化”，在 *AISTATS*，2020年，第1453–1463页。'
- en: '[260] C. Meister, E. Salesky, and R. Cotterell, “Generalized entropy regularization
    or: There’s nothing special about label smoothing,” in *ACL*, 2020.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] C. Meister, E. Salesky 和 R. Cotterell，“广义熵正则化：标签平滑没有什么特别之处”，在 *ACL*，2020年。'
- en: '[261] J. Chai and X. Wang, “Fairness with adaptive weights,” in *ICML*, 2022,
    pp. 2853–2866.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] J. Chai 和 X. Wang，“带有自适应权重的公平性”，在 *ICML*，2022年，第2853–2866页。'
- en: '[262] Q. Song, H. Jin, X. Huang, and X. Hu, “Multi-label adversarial perturbations,”
    in *ICDM*.   IEEE, 2018, pp. 1242–1247.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] Q. Song, H. Jin, X. Huang 和 X. Hu，“多标签对抗扰动”，在 *ICDM*。IEEE，2018年，第1242–1247页。'
- en: '[263] S. Hu, L. Ke, X. Wang, and S. Lyu, “Tkml-ap: Adversarial attacks to top-k
    multi-label learning,” in *ICCV*, 2021, pp. 7649–7657.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] S. Hu, L. Ke, X. Wang 和 S. Lyu，“Tkml-ap：对 top-k 多标签学习的对抗攻击”，在 *ICCV*，2021年，第7649–7657页。'
- en: '[264] L. Kong, W. Luo, H. Zhang, Y. Liu, and Y. Shi, “Evolutionary multilabel
    adversarial examples: An effective black-box attack,” *IEEE TAI*, vol. 4, no. 3,
    pp. 562–572, 2023.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] L. Kong, W. Luo, H. Zhang, Y. Liu 和 Y. Shi，“进化多标签对抗样本：一种有效的黑箱攻击”，*IEEE
    TAI*，第4卷，第3期，第562–572页，2023年。'
- en: '[265] H. Cao, W. Yang, and H. T. Ng, “Mitigating exposure bias in grammatical
    error correction with data augmentation and reweighting,” in *EACL*, 2023, pp.
    2123–2135.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] H. Cao, W. Yang 和 H. T. Ng，“通过数据增强和重加权减轻语法错误修正中的暴露偏差”，在 *EACL*，2023年，第2123–2135页。'
- en: '[266] Q. Liu, M. Kusner, and P. Blunsom, “Counterfactual data augmentation
    for neural machine translation,” in *NAACL*, 2021, pp. 187–197.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] Q. Liu, M. Kusner 和 P. Blunsom，“神经机器翻译的反事实数据增强”，在 *NAACL*，2021年，第187–197页。'
- en: '[267] Y. Zang, C. Huang, and C. C. Loy, “Fasa: Feature augmentation and sampling
    adaptation for long-tailed instance segmentation,” in *ICCV*, 2021.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] Y. Zang, C. Huang 和 C. C. Loy，“Fasa：用于长尾实例分割的特征增强和采样适应”，在 *ICCV*，2021年。'
- en: '[268] Y. Zhao, W. Chen, X. Tan, K. Huang, and J. Zhu, “Adaptive logit adjustment
    loss for long-tailed visual recognition,” in *AAAI*, 2022, pp. 3472–3480.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] Y. Zhao, W. Chen, X. Tan, K. Huang 和 J. Zhu，“用于长尾视觉识别的自适应对数调整损失”，在 *AAAI*，2022年，第3472–3480页。'
- en: '[269] J.-H. Xue and P. Hall, “Why does rebalancing class-unbalanced data improve
    auc for linear discriminant analysis?” *IEEE TPAMI*, vol. 37, no. 5, pp. 1109–1112,
    2015.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] J.-H. Xue 和 P. Hall，“为什么重新平衡类别不平衡数据会提高线性判别分析的AUC？” *IEEE TPAMI*，第37卷，第5期，第1109–1112页，2015年。'
- en: '[270] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry,
    “Adversarial examples are not bugs, they are features,” in *NeurIPS*, 2019.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran 和 A. Madry，“对抗样本不是错误，它们是特征”，在
    *NeurIPS*，2019年。'
- en: '[271] D. Elreedy, A. F. Atiya, and F. Kamalov, “A theoretical distribution
    analysis of synthetic minority oversampling technique (smote) for imbalanced learning,”
    *Machine Learning*, 2023.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] D. Elreedy, A. F. Atiya 和 F. Kamalov，“合成少数类过采样技术（smote）的理论分布分析”，*机器学习*，2023年。'
- en: '[272] Y. Yang, K. Zha, Y. Chen, H. Wang, and D. Katabi, “Delving into deep
    imbalanced regression,” in *ICML)*, 2021, pp. 11 842–11 851.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] Y. Yang, K. Zha, Y. Chen, H. Wang 和 D. Katabi，“深入探讨深度不平衡回归”，在 *ICML*，2021年，第11 842–11 851页。'
- en: '[273] M. Li, Y.-m. Cheung, and Y. Lu, “Long-tailed visual recognition via gaussian
    clouded logit adjustment,” in *CVPR*, 2022, pp. 6929–6938.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] M. Li, Y.-m. Cheung, 和 Y. Lu, “通过高斯云朵逻辑调整进行长尾视觉识别，” 见 *CVPR*，2022年，第6929–6938页。'
- en: '[274] J. Ren, M. Zhang, C. Yu, and Z. Liu, “Balanced mse for imbalanced visual
    regression,” in *CVPR*, 2022, pp. 7926–7935.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] J. Ren, M. Zhang, C. Yu, 和 Z. Liu, “用于不平衡视觉回归的平衡均方误差，” 见 *CVPR*，2022年，第7926–7935页。'
- en: '[275] Y. Xie, M. Qiu, H. Zhang, L. Peng, and Z. Chen, “Gaussian distribution
    based oversampling for imbalanced data classification,” *IEEE TKDE*, vol. 32,
    no. 2, pp. 667–679, 2022.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] Y. Xie, M. Qiu, H. Zhang, L. Peng, 和 Z. Chen, “基于高斯分布的过采样用于不平衡数据分类，”
    *IEEE TKDE*，第32卷，第2期，第667–679页，2022年。'
- en: '[276] Y. Yang and Z. Xu, “Rethinking the value of labels for improving class-imbalanced
    learning,” in *NeurIPS*, 2020, pp. 19 290–19 301.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] Y. Yang 和 Z. Xu, “重新思考标签对改进类不平衡学习的价值，” 见 *NeurIPS*，2020年，第19 290–19 301页。'
- en: '[277] H. Liu, J. Z. HaoChen, A. Gaidon, and T. Ma, “Self-supervised learning
    is more robust to dataset imbalance,” *arXiv:2110.05025*, 2022.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] H. Liu, J. Z. HaoChen, A. Gaidon, 和 T. Ma, “自监督学习对数据集不平衡更具鲁棒性，” *arXiv:2110.05025*，2022年。'
- en: '[278] L. Jin, D. Lang, and N. Lei, “An optimal transport view of class-imbalanced
    visual recognition,” *International Journal of Computer Vision*, 2023.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] L. Jin, D. Lang, 和 N. Lei, “类不平衡视觉识别的最优传输视角，” *计算机视觉国际期刊*，2023年。'
- en: '[279] Q. Dong, S. Gong, and X. Zhu, “Imbalanced deep learning by minority class
    incremental rectification,” *IEEE TPAMI*, vol. 41, no. 6, pp. 1367–1381, 2019.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] Q. Dong, S. Gong, 和 X. Zhu, “通过少数类增量修正进行不平衡深度学习，” *IEEE TPAMI*，第41卷，第6期，第1367–1381页，2019年。'
- en: '[280] K. A. Wang, N. S. Chatterji, S. Haque, and T. Hashimoto, “Is importance
    weighting incompatible with interpolating classifiers?” in *ICLR*, 2022.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] K. A. Wang, N. S. Chatterji, S. Haque, 和 T. Hashimoto, “重要性加权与插值分类器是否不兼容？”
    见 *ICLR*，2022年。'
- en: '[281] R. Xu, X. Zhang, Z. Shen, T. Zhang, and P. Cui, “A theoretical analysis
    on independence-driven importance weighting for covariate-shift generalization,”
    in *ICML*, 2022, pp. 24 803–24 829.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] R. Xu, X. Zhang, Z. Shen, T. Zhang, 和 P. Cui, “关于基于独立性驱动的重要性加权的理论分析用于协变量偏移泛化，”
    见 *ICML*，2022年，第24 803–24 829页。'
- en: '[282] D. Chen, Y. Shen, H. Zhang, and P. H. Torr, “Zero-shot logit adjustment,”
    in *IJCAI*, 2022.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] D. Chen, Y. Shen, H. Zhang, 和 P. H. Torr, “零样本逻辑调整，” 见 *IJCAI*，2022年。'
- en: '[283] M. Qraitem, K. Saenko, and B. A. Plummer, “Bias mimicking: A simple sampling
    approach for bias mitigation,” in *CVPR*, 2023, pp. 20 311–20 320.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] M. Qraitem, K. Saenko, 和 B. A. Plummer, “偏差模拟：一种用于偏差缓解的简单采样方法，” 见 *CVPR*，2023年，第20 311–20 320页。'
- en: '[284] Y. Roh, K. Lee, S. Whang, and C. Suh, “Sample selection for fair and
    robust training,” in *NeurIPS*, 2021, pp. 815–827.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] Y. Roh, K. Lee, S. Whang, 和 C. Suh, “用于公平和鲁棒训练的样本选择，” 见 *NeurIPS*，2021年，第815–827页。'
- en: '[285] A. Zhang, F. Liu, W. Ma, Z. Cai, X. Wang, and T.-S. Chua, “Boosting causal
    discovery via adaptive sample reweighting,” in *ICLR*, 2023.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] A. Zhang, F. Liu, W. Ma, Z. Cai, X. Wang, 和 T.-S. Chua, “通过自适应样本重加权提升因果发现，”
    见 *ICLR*，2023年。'
- en: '[286] Y. Jang, T. Zhao, S. Hong, and H. Lee, “Adversarial defense via learning
    to generate diverse attacks,” in *ICCV*, 2019.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] Y. Jang, T. Zhao, S. Hong, 和 H. Lee, “通过学习生成多样攻击进行对抗防御，” 见 *ICCV*，2019年。'
- en: '[287] I. Hounie, L. F. O. Chamon, and A. Ribeiro, “Automatic data augmentation
    via invariance-constrained learning,” in *ICML*, 2023, pp. 13 410–13 433.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] I. Hounie, L. F. O. Chamon, 和 A. Ribeiro, “通过不变性约束学习进行自动数据增强，” 见 *ICML*，2023年，第13 410–13 433页。'
- en: '[288] A. Blum and K. Stangl, “Recovering from biased data: Can fairness constraints
    improve accuracy?” *arXiv:1912.01094*, 2019.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] A. Blum 和 K. Stangl, “从偏倚数据中恢复：公平性约束能否提高准确性？” *arXiv:1912.01094*，2019年。'
- en: '[289] T. Doan, M. Abbana Bennani, B. Mazoure, G. Rabusseau, and P. Alquier,
    “A theoretical analysis of catastrophic forgetting through the ntk overlap matrix,”
    in *AISTATS*, 2021, pp. 1072–1080.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] T. Doan, M. Abbana Bennani, B. Mazoure, G. Rabusseau, 和 P. Alquier, “通过NTK重叠矩阵对灾难性遗忘的理论分析，”
    见 *AISTATS*，2021年，第1072–1080页。'
- en: '[290] S. Chatterjee and P. Zielinski, “On the generalization mystery in deep
    learning,” *arXiv:2203.10036*, 2022.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] S. Chatterjee 和 P. Zielinski, “深度学习中的泛化谜团，” *arXiv:2203.10036*，2022年。'
- en: '[291] A. Katharopoulos and F. Fleuret, “Biased importance sampling for deep
    neural network training,” *arXiv:1706.00043*, 2017.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] A. Katharopoulos 和 F. Fleuret, “用于深度神经网络训练的偏倚重要性采样，” *arXiv:1706.00043*，2017年。'
- en: '[292] Z. Wang, H. Zhu, Z. Dong, X. He, and S.-L. Huang, “Less is better: Unweighted
    data subsampling via influence function,” in *AAAI*, 2020, pp. 6340–6347.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] Z. Wang, H. Zhu, Z. Dong, X. He, 和 S.-L. Huang, “少即是多：通过影响函数进行未加权数据子采样，”
    见 *AAAI*，2020年，第6340–6347页。'
- en: '[293] T. Dao, A. Gu, A. Ratner, V. Smith, C. De Sa, and C. Re, “A kernel theory
    of modern data augmentation,” in *ICML*, 2019, pp. 1528–1537.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] T. Dao, A. Gu, A. Ratner, V. Smith, C. De Sa, 和 C. Re，“现代数据增强的核理论，”发表于
    *ICML*，2019年，页码1528–1537。'
- en: '[294] J. Wu and J. He, “A unified framework for adversarial attacks on multi-source
    domain adaptation,” *IEEE TKDE*, pp. 1–12, 2022.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] J. Wu 和 J. He，“多源领域适应的对抗攻击统一框架，” *IEEE TKDE*，页码1–12，2022年。'
- en: '[295] J. Gilmer, N. Ford, N. Carlini, and E. Cubuk, “Adversarial examples are
    a natural consequence of test error in noise,” in *ICML*, 2019, p. 2280–2289.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] J. Gilmer, N. Ford, N. Carlini, 和 E. Cubuk，“对抗样本是噪声测试误差的自然结果，”发表于 *ICML*，2019年，页码2280–2289。'
- en: '[296] M. Yi, L. Hou, J. Sun, L. Shang, X. Jiang, Q. Liu, and Z. Ma, “Improved
    ood generalization via adversarial training and pretraing,” in *ICML*, 2021, pp.
    11 987–11 997.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] M. Yi, L. Hou, J. Sun, L. Shang, X. Jiang, Q. Liu, 和 Z. Ma，“通过对抗训练和预训练改进OOD泛化，”发表于
    *ICML*，2021年，页码11,987–11,997。'
- en: '[297] J. Peck, J. Roels, B. Goossens, and Y. Saeys, “Lower bounds on the robustness
    to adversarial perturbations,” in *NeurIPS*, 2017.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] J. Peck, J. Roels, B. Goossens, 和 Y. Saeys，“对抗扰动的鲁棒性下限，”发表于 *NeurIPS*，2017年。'
- en: '[298] Y. Xu, Y. Xu, Q. Qian, H. Li, and R. Jin, “Towards understanding label
    smoothing,” *arXiv:2006.11653*, 2017.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] Y. Xu, Y. Xu, Q. Qian, H. Li, 和 R. Jin，“理解标签平滑的方向，” *arXiv:2006.11653*，2017年。'
- en: '[299] D. Meng, Q. Zhao, and L. Jiang, “A theoretical understanding of self-paced
    learning,” *Information Sciences*, vol. 414, pp. 319–328, 2017.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] D. Meng, Q. Zhao, 和 L. Jiang，“自我节奏学习的理论理解，” *Information Sciences*，卷414，页码319–328，2017年。'
- en: '[300] D. Weinshall, G. Cohen, and D. Amir, “Curriculum learning by transfer
    learning: Theory and experiments with deep networks,” in *ICML*, 2018, pp. 5238–5246.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] D. Weinshall, G. Cohen, 和 D. Amir，“通过迁移学习进行课程学习：理论与深度网络实验，”发表于 *ICML*，2018年，页码5238–5246。'
- en: '[301] D. Zhu, B. Lei, J. Zhang, Y. Fang, R. Zhang, Y. Xie, and D. Xu, “Rethinking
    data distillation: Do not overlook calibration,” in *ICCV*, 2023.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] D. Zhu, B. Lei, J. Zhang, Y. Fang, R. Zhang, Y. Xie, 和 D. Xu，“重新思考数据蒸馏：不要忽视校准，”发表于
    *ICCV*，2023年。'
- en: '[302] T. Dong, B. Zhao, and L. Lyu, “Privacy for free: How does dataset condensation
    help privacy?” *arXiv:2206.00240*, 2022.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] T. Dong, B. Zhao, 和 L. Lyu，“免费隐私：数据集浓缩如何帮助隐私？” *arXiv:2206.00240*，2022年。'
- en: '[303] L. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng, “Revisiting knowledge
    distillation via label smoothing regularization,” in *CVPR*, 2020.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] L. Yuan, F. E. Tay, G. Li, T. Wang, 和 J. Feng，“通过标签平滑正则化重新审视知识蒸馏，”发表于
    *CVPR*，2020年。'
- en: '[304] A. D. Assis, L. C. B. Torres, L. R. G. Araújo, V. M. Hanriot, and A. P.
    Braga, “Neural networks regularization with graph-based local resampling,” *IEEE
    Access*, vol. 9, pp. 50 727–50 737, 2021.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] A. D. Assis, L. C. B. Torres, L. R. G. Araújo, V. M. Hanriot, 和 A. P.
    Braga，“基于图的局部重采样的神经网络正则化，” *IEEE Access*，卷9，页码50,727–50,737，2021年。'
- en: '[305] F. R. Cordeiro, V. Belagiannis, I. Reid, and G. Carneiro, “Propmix: Hard
    sample filtering and proportional mixup for learning with noisy labels,” in *BMVC*,
    2021.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] F. R. Cordeiro, V. Belagiannis, I. Reid, 和 G. Carneiro，“Propmix: 硬样本过滤和比例混合用于处理带噪声标签的学习，”发表于
    *BMVC*，2021年。'
- en: '[306] K. Yang, Y. Sun, J. Su, F. He, X. Tian, F. Huang, T. Zhou, and D. Tao,
    “Adversarial auto-augment with label preservation: A representation learning principle
    guided approach,” in *NeurIPS*, 2022, pp. 22 035–22 048.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] K. Yang, Y. Sun, J. Su, F. He, X. Tian, F. Huang, T. Zhou, 和 D. Tao，“带标签保留的对抗自动增强：一种基于表征学习原理的指导方法，”发表于
    *NeurIPS*，2022年，页码22,035–22,048。'
- en: '[307] J. Li, R. Socher, and S. C. Hoi, “Dividemix: Learning with noisy labels
    as semi-supervised learning,” in *ICLR*, 2020.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] J. Li, R. Socher, 和 S. C. Hoi，“Dividemix: 将带噪声标签的学习视作半监督学习，”发表于 *ICLR*，2020年。'
- en: '[308] J. Shu, X. Yuan, D. Meng, and Z. Xu, “Cmw-net: Learning a class-aware
    sample weighting mapping for robust deep learning,” *IEEE TPAMI*, vol. 45, no. 10,
    2023.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] J. Shu, X. Yuan, D. Meng, 和 Z. Xu，“Cmw-net: 学习类感知样本加权映射以实现鲁棒深度学习，” *IEEE
    TPAMI*，卷45，第10期，2023年。'
- en: '[309] Z. Zhang and T. Pfister, “Learning fast sample re-weighting without reward
    data,” in *ICCV*, 2021, pp. 725–734.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] Z. Zhang 和 T. Pfister，“无需奖励数据的快速样本重加权学习，”发表于 *ICCV*，2021年，页码725–734。'
- en: '[310] X. Wang, E. Kodirov, Y. Hua, and N. M. Robertson, “Derivative manipulation
    for general example weighting,” *arXiv:1905.11233*, 2020.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] X. Wang, E. Kodirov, Y. Hua, 和 N. M. Robertson，“用于一般示例加权的导数操控，” *arXiv:1905.11233*，2020年。'
- en: '[311] E. Yang, T. Liu, C. Deng, W. Liu, and D. Tao, “Distillhash: Unsupervised
    deep hashing by distilling data pairs,” in *CVPR*, 2019, pp. 2946–2955.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] E. Yang, T. Liu, C. Deng, W. Liu, 和 D. Tao，“Distillhash: 通过蒸馏数据对进行无监督深度哈希，”发表于
    *CVPR*，2019年，页码2946–2955。'
- en: '[312] B. Mirzasoleiman, K. Cao, and J. Leskovec, “Coresets for robust training
    of deep neural networks against noisy labels,” in *NeurIPS*, 2020, pp. 11 465–11 477.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] B. Mirzasoleiman, K. Cao, 和 J. Leskovec，“针对噪声标签的深度神经网络鲁棒训练的核心集，”发表于 *NeurIPS*，2020年，第11 465–11 477页。'
- en: '[313] S. Mindermann, J. M. Brauner, M. T. Razzak, M. Sharma, A. Kirsch, W. Xu,
    B. Höltgen, A. N. Gomez, A. Morisot, S. Farquhar, and Y. Gal, “Prioritized training
    on points that are learnable, worth learning, and not yet learnt,” in *ICML*,
    2022, pp. 15 630–15 649.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] S. Mindermann, J. M. Brauner, M. T. Razzak, M. Sharma, A. Kirsch, W.
    Xu, B. Höltgen, A. N. Gomez, A. Morisot, S. Farquhar, 和 Y. Gal，“优先训练可学习、有价值且尚未学习的点，”发表于
    *ICML*，2022年，第15 630–15 649页。'
- en: '[314] X. Hu, Y. Zeng, X. Xu, S. Zhou, and L. Liu, “Robust semi-supervised classification
    based on data augmented online elms with deep features,” *Knowledge-Based Systems*,
    vol. 229, p. 107307, 2021.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] X. Hu, Y. Zeng, X. Xu, S. Zhou, 和 L. Liu，“基于数据增强在线ELMs与深度特征的鲁棒半监督分类，”
    *知识-based Systems*，第229卷，第107307页，2021年。'
- en: '[315] S. Kim, S. Bae, and S.-Y. Yun, “Coreset sampling from open-set for fine-grained
    self-supervised learning,” in *CVPR*, 2023, pp. 7537–7547.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] S. Kim, S. Bae, 和 S.-Y. Yun，“从开放集中的核心集采样进行细粒度自监督学习，”发表于 *CVPR*，2023年，第7537–7547页。'
- en: '[316] C. Bellinger, R. Corizzo, and N. Japkowicz, “Remix: Calibrated resampling
    for class imbalance in deep learning,” *arXiv:2012.02312*, 2020.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] C. Bellinger, R. Corizzo, 和 N. Japkowicz，“Remix：用于深度学习中的类别不平衡的校准重采样，”
    *arXiv:2012.02312*，2020年。'
- en: '[317] F. Du, P. Yang, Q. Jia, F. Nan, X. Chen, and Y. Yang, “Global and local
    mixture consistency cumulative learning for long-tailed visual recognitions,”
    in *CVPR*, June 2023, pp. 15 814–15 823.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] F. Du, P. Yang, Q. Jia, F. Nan, X. Chen, 和 Y. Yang，“用于长尾视觉识别的全局与局部混合一致性累积学习，”发表于
    *CVPR*，2023年6月，第15 814–15 823页。'
- en: '[318] O. Pooladzandi, D. Davini, and B. Mirzasoleiman, “Adaptive second order
    coresets for data-efficient machine learning,” in *ICML*, 2022, pp. 17 848–17 869.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[318] O. Pooladzandi, D. Davini, 和 B. Mirzasoleiman，“用于数据高效机器学习的自适应二阶核心集，”发表于
    *ICML*，2022年，第17 848–17 869页。'
- en: '[319] G. Zhao, G. Li, Y. Qin, and Y. Yu, “Improved distribution matching for
    dataset condensation,” in *CVPR*, 2023, pp. 7856–7865.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[319] G. Zhao, G. Li, Y. Qin, 和 Y. Yu，“改进的分布匹配用于数据集浓缩，”发表于 *CVPR*，2023年，第7856–7865页。'
- en: '[320] K. Kim and H. S. Lee, “Probabilistic anchor assignment with iou prediction
    for object detection,” in *ECCV*, 2020, pp. 355–371.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[320] K. Kim 和 H. S. Lee，“基于IOU预测的概率锚点分配用于目标检测，”发表于 *ECCV*，2020年，第355–371页。'
- en: '[321] T. Takase, R. Karakida, and H. Asoh, “Self-paced data augmentation for
    training neural networks,” *Neurocomputing*, vol. 442, pp. 296–306, 2021.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[321] T. Takase, R. Karakida, 和 H. Asoh，“用于训练神经网络的自适应数据增强，” *Neurocomputing*，第442卷，第296–306页，2021年。'
- en: '[322] S. A. Siddiqui, N. Rajkumar, T. Maharaj, D. Krueger, and S. Hooker, “Metadata
    archaeology: Unearthing data subsets by leveraging training dynamics,” in *ICLR*,
    2023.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[322] S. A. Siddiqui, N. Rajkumar, T. Maharaj, D. Krueger, 和 S. Hooker，“元数据考古：通过利用训练动态挖掘数据子集，”发表于
    *ICLR*，2023年。'
- en: '[323] X. Peng, F.-Y. Wang, and L. Li, “Mixgradient: A gradient-based re-weighting
    scheme with mixup for imbalanced data streams,” *Neural Networks*, vol. 161, pp.
    525–534, 2023.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[323] X. Peng, F.-Y. Wang, 和 L. Li，“Mixgradient：一种基于梯度的重加权方案，结合mixup用于不平衡数据流，”
    *Neural Networks*，第161卷，第525–534页，2023年。'
- en: '[324] W. Xu, L. Jiang, and C. Li, “Resampling-based noise correction for crowdsourcing,”
    *Journal of Experimental & Theoretical Artificial Intelligence*, vol. 33, no. 6,
    pp. 985–999, 2020.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[324] W. Xu, L. Jiang, 和 C. Li，“基于重采样的众包噪声修正，” *实验与理论人工智能期刊*，第33卷，第6期，第985–999页，2020年。'
- en: '[325] C. Huang and S. Zhang, “Generative dataset distillation,” in *BigCom*,
    2021, pp. 212–218.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[325] C. Huang 和 S. Zhang，“生成数据集蒸馏，”发表于 *BigCom*，2021年，第212–218页。'
- en: '[326] A. Krizhevsky, “Learning multiple layers of features from tiny images.”   MIT,
    2009.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[326] A. Krizhevsky，“从小图像中学习多个层次的特征。” MIT，2009年。'
- en: '[327] T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang, “Learning from massive
    noisy labeled data for image classification,” in *CVPR*, 2015, p. 2691–2699.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[327] T. Xiao, T. Xia, Y. Yang, C. Huang, 和 X. Wang，“从大规模噪声标记数据中学习用于图像分类，”发表于
    *CVPR*，2015年，第2691–2699页。'
- en: '[328] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, “Reading
    digits in natural images with unsupervised feature learning,” in *NeurIPSW*, 2011.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[328] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, 和 A. Y. Ng，“使用无监督特征学习读取自然图像中的数字，”发表于
    *NeurIPSW*，2011年。'
- en: '[329] W. Li, L. Wang, W. Li, E. Agustsson, and L. Van Gool, “Webvision database:
    Visual learning and understanding from web data,” *arXiv:1708.02862*, 2017.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[329] W. Li, L. Wang, W. Li, E. Agustsson, 和 L. Van Gool，“Webvision 数据库：从网络数据中进行视觉学习与理解，”
    *arXiv:1708.02862*，2017年。'
- en: '[330] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam,
    P. Perona, and S. Belongie, “The inaturalist species classification and detection
    dataset,” in *CVPR*, 2018, pp. 8769–8778.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[330] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam,
    P. Perona, 和 S. Belongie，“iNaturalist物种分类和检测数据集，” 收录于 *CVPR*，2018年，页码8769–8778。'
- en: '[331] Z. Liu, Z. Miao, X. Zhan, J. Wang, B. Gong, and S. X. Yu, “Largescale
    long-tailed recognition in an open world,” in *CVPR*, 2019, p. 2537–2546.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[331] Z. Liu, Z. Miao, X. Zhan, J. Wang, B. Gong, 和 S. X. Yu，“开放世界中的大规模长尾识别，”
    收录于 *CVPR*，2019年，页码2537–2546。'
- en: '[332] S. P. Karimireddy, L. He, and M. Jaggi, “Byzantine-robust learning on
    heterogeneous datasets via bucketing,” *arXiv:2006.09365*, 2022.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[332] S. P. Karimireddy, L. He, 和 M. Jaggi，“通过分桶实现对异质数据集的拜占庭鲁棒学习，” *arXiv:2006.09365*，2022年。'
- en: '[333] N. Tsilivis, J. Su, and J. Kempe, “Can we achieve robustness from data
    alone?” *arXiv:2006.09365*, 2022.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[333] N. Tsilivis, J. Su, 和 J. Kempe，“我们能仅凭数据实现鲁棒性吗？” *arXiv:2006.09365*，2022年。'
- en: '[334] Z. Liu, P. Wei, J. Jiang, W. Cao, J. Bian, and Y. Chang, “Mesa: Boost
    ensemble imbalanced learning with meta-sampler,” in *NeurIPS*, 2020, pp. 14 463–14 474.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[334] Z. Liu, P. Wei, J. Jiang, W. Cao, J. Bian, 和 Y. Chang，“Mesa: 使用元采样器提升集成不平衡学习，”
    收录于 *NeurIPS*，2020年，页码14 463–14 474。'
- en: '[335] Y. Li, X. Liu, and F. Liu, “Adaptive noisy data augmentation for regularization
    of undirected graphical models,” *arXiv:1810.04851*, 2019.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[335] Y. Li, X. Liu, 和 F. Liu，“用于无向图模型正则化的自适应噪声数据增强，” *arXiv:1810.04851*，2019年。'
- en: '[336] C. Yu, B. Han, L. Shen, J. Yu, C. Gong, M. Gong, and T. Liu, “Understanding
    robust overfitting of adversarial training and beyond,” in *ICML*, 2022, pp. 25 595–25 610.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[336] C. Yu, B. Han, L. Shen, J. Yu, C. Gong, M. Gong, 和 T. Liu，“理解对抗训练及其以外的鲁棒过拟合，”
    收录于 *ICML*，2022年，页码25 595–25 610。'
- en: '[337] J. Xiao, Y. Fan, R. Sun, J. Wang, and Z.-Q. Luo, “Stability analysis
    and generalization bounds of adversarial training,” in *NeurIPS*, 2022, pp. 15 446–15 459.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[337] J. Xiao, Y. Fan, R. Sun, J. Wang, 和 Z.-Q. Luo，“对抗训练的稳定性分析和泛化界限，” 收录于
    *NeurIPS*，2022年，页码15 446–15 459。'
- en: '[338] J. An, L. Ying, and Y. Zhu, “Why resampling outperforms reweighting for
    correcting sampling bias with stochastic gradients,” *arXiv:2009.13447*, 2021.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[338] J. An, L. Ying, 和 Y. Zhu，“为什么重采样优于重加权来纠正带有随机梯度的采样偏差，” *arXiv:2009.13447*，2021年。'
- en: '[339] R. Müller, S. Kornblith, and G. E. Hinton, “When does label smoothing
    help?” in *NeurIPS*, 2019.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[339] R. Müller, S. Kornblith, 和 G. E. Hinton，“标签平滑何时有帮助？” 收录于 *NeurIPS*，2019年。'
- en: '[340] B. Chen, L. Ziyin, Z. Wang, and P. P. Liang, “An investigation of how
    label smoothing affects generalization,” *arXiv:2010.12648*, 2020.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[340] B. Chen, L. Ziyin, Z. Wang, 和 P. P. Liang，“标签平滑如何影响泛化的研究，” *arXiv:2010.12648*，2020年。'
- en: '[341] Z. Qian, K. Huang, Q.-F. Wang, and X.-Y. Zhang, “A survey of robust adversarial
    training in pattern recognition: Fundamental, theory, and methodologies,” *Pattern
    Recognition*, vol. 131, p. 108889, 2022.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[341] Z. Qian, K. Huang, Q.-F. Wang, 和 X.-Y. Zhang，“模式识别中鲁棒对抗训练的综述：基础、理论和方法，”
    *模式识别*，第131卷，文章编号108889，2022年。'
- en: '[342] K. H. R. Chan, Y. Yu, C. You, H. Qi, J. Wright, and Y. Ma, “Redunet:
    a white-box deep network from the principle of maximizing rate reduction,” *Journal
    of Machine Learning Research*, vol. 23, no. 1, pp. 4907–5009, 2022.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[342] K. H. R. Chan, Y. Yu, C. You, H. Qi, J. Wright, 和 Y. Ma，“Redunet: 基于最大化速率降低原理的白盒深度网络，”
    *机器学习研究杂志*，第23卷，第1期，页码4907–5009，2022年。'
- en: '[343] Y. Wen, G. Jerfel, R. Muller, M. W. Dusenberry, J. Snoek, B. Lakshminarayanan,
    and D. Tran, “Combining ensembles and data augmentation can harm your calibration,”
    in *ICLR*, 2021.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[343] Y. Wen, G. Jerfel, R. Muller, M. W. Dusenberry, J. Snoek, B. Lakshminarayanan,
    和 D. Tran，“结合集成和数据增强可能会损害你的校准，” 收录于 *ICLR*，2021年。'
- en: '[344] M. Lukasik, S. Bhojanapalli, A. K. Menon, and S. Kumar, “Does label smoothing
    mitigate label noise?” in *ICML*, 2020.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[344] M. Lukasik, S. Bhojanapalli, A. K. Menon, 和 S. Kumar，“标签平滑是否减轻标签噪声？”
    收录于 *ICML*，2020年。'
- en: '[345] M. Du, N. Liu, and X. Hu, “Techniques for interpretable machine learning,”
    *Communications of the ACM*, vol. 63, pp. 68–77, 2019.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[345] M. Du, N. Liu, 和 X. Hu，“可解释机器学习的技术，” *ACM通讯*，第63卷，页码68–77，2019年。'
- en: '[346] C. R. Pochimireddy, A. T. Siripuram, and S. S. Channappayya, “Can perceptual
    guidance lead to semantically explainable adversarial perturbations?” *IEEE J-STSP*,
    pp. 1–11, 2023.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[346] C. R. Pochimireddy, A. T. Siripuram, 和 S. S. Channappayya，“感知引导是否能导致语义上可解释的对抗扰动？”
    *IEEE J-STSP*，页码1–11，2023年。'
- en: '[347] G. Zelaya and C. Vladimiro, “Towards explaining the effects of data preprocessing
    on machine learning,” in *ICDE*, 2019, pp. 2086–2090.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[347] G. Zelaya 和 C. Vladimiro，“解释数据预处理对机器学习影响的尝试，” 收录于 *ICDE*，2019年，页码2086–2090。'
- en: '[348] E. Mosqueira-Rey, E. Hernández-Pereira, D. Alonso-Ríos, J. Bobes-Bascarán,
    and Ángel Fernández-Leal, “Human-in-the-loop machine learning: a state of the
    art,” *Journal of Machine Learning Research*, vol. 56, pp. 3005–3054, 2023.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[348] E. Mosqueira-Rey, E. Hernández-Pereira, D. Alonso-Ríos, J. Bobes-Bascarán,
    和 Ángel Fernández-Leal, “人机协作学习：现状综述，” *Journal of Machine Learning Research*，第56卷，第3005–3054页，2023年。'
- en: '[349] K. M. Collins, U. Bhatt, W. Liu, V. Piratla, I. Sucholutsky, B. Love,
    and A. Weller, “Human-in-the-loop mixup,” in *UAI*, 2023, pp. 454–464.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[349] K. M. Collins, U. Bhatt, W. Liu, V. Piratla, I. Sucholutsky, B. Love,
    和 A. Weller, “人机协作混合，” 发表在*UAI*，2023年，第454–464页。'
- en: '[350] E. Wallace, P. Rodriguez, S. Feng, I. Yamada, and J. Boyd-Graber, “Trick
    me if you can: Human-in-the-loop generation of adversarial examples for question
    answering,” *TACL*, vol. 7, pp. 387–401, 2019.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[350] E. Wallace, P. Rodriguez, S. Feng, I. Yamada, 和 J. Boyd-Graber, “如果你能的话，骗我：人机协作生成对抗样本用于问答，”
    *TACL*，第7卷，第387–401页，2019年。'
- en: '[351] C. Agarwal, D. D’souza, and S. Hooker, “Estimating example difficulty
    using variance of gradients,” in *CVPR*, 2022, pp. 10 368–10 378.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[351] C. Agarwal, D. D’souza, 和 S. Hooker, “使用梯度方差估计样本难度，” 发表在*CVPR*，2022年，第10 368–10 378页。'
- en: '[352] Z.-F. Wu, T. Wei, J. Jiang, C. Mao, M. Tang, and Y.-F. Li, “Ngc: a unified
    framework for learning with open-world noisy data,” in *ICCV*, 2021, p. 62–71.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[352] Z.-F. Wu, T. Wei, J. Jiang, C. Mao, M. Tang, 和 Y.-F. Li, “Ngc：一个用于开放世界噪声数据学习的统一框架，”
    发表在*ICCV*，2021年，第62–71页。'
- en: '[353] Z. Jiang, T. Chen, T. Chen, and Z. Wang, “Improving contrastive learning
    on imbalanced data via open-world sampling,” in *NeurIPS*, 2021, pp. 5997–6009.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[353] Z. Jiang, T. Chen, T. Chen, 和 Z. Wang, “通过开放世界采样提高不平衡数据上的对比学习，” 发表在*NeurIPS*，2021年，第5997–6009页。'
- en: '[354] T. Gokhale, S. Mishra, M. Luo, B. S. Sachdeva, and C. Baral, “Generalized
    but not robust? comparing the effects of data modification methods on out-of-domain
    generalization and adversarial robustness,” in *ACL Findings*, 2022.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[354] T. Gokhale, S. Mishra, M. Luo, B. S. Sachdeva, 和 C. Baral, “泛化但不稳健？比较数据修改方法对域外泛化和对抗稳健性的影响，”
    发表在*ACL Findings*，2022年。'
- en: '[355] W. X. Zhao, K. Zhou, and et al., “A survey of large language models,”
    *arXiv:2303.18223*, 2023.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[355] W. X. Zhao, K. Zhou, 等， “大型语言模型综述，” *arXiv:2303.18223*，2023年。'
- en: '[356] L. Wei, Z. Jiang, W. Huang, and L. Sun, “Instructiongpt-4: A 200-instruction
    paradigm for fine-tuning minigpt-4,” *arXiv:2308.12067*, 2023.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[356] L. Wei, Z. Jiang, W. Huang, 和 L. Sun, “Instructiongpt-4：用于微调minigpt-4的200条指令范式，”
    *arXiv:2308.12067*，2023年。'
- en: '[357] M. Jiang, Y. Ruan, S. Huang, S. Liao, S. Pitis, R. Grosse, and J. Ba,
    “Calibrating language models via augmented prompt ensembles,” in *ICML*, 2023.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[357] M. Jiang, Y. Ruan, S. Huang, S. Liao, S. Pitis, R. Grosse, 和 J. Ba, “通过增强的提示集校准语言模型，”
    发表在*ICML*，2023年。'
- en: '[358] T. Baltrušaitis, C. Ahuja, and L.-P. Morency, “Multimodal machine learning:
    A survey and taxonomy,” *IEEE TPAMI*, vol. 41, no. 2, pp. 423–443, 2019.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[358] T. Baltrušaitis, C. Ahuja, 和 L.-P. Morency, “多模态机器学习：综述与分类，” *IEEE TPAMI*，第41卷，第2期，第423–443页，2019年。'
- en: '[359] S. Ge, Z. Jiang, Z. Cheng, C. Wang, Y. Yin, and Q. Gu, “Learning robust
    multi-modal representation for multi-label emotion recognition via adversarial
    masking and perturbation,” in *The Web Conference*, 2023, pp. 1510–1518.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[359] S. Ge, Z. Jiang, Z. Cheng, C. Wang, Y. Yin, 和 Q. Gu, “通过对抗掩蔽和扰动学习稳健的多模态表示以进行多标签情感识别，”
    发表在*The Web Conference*，2023年，第1510–1518页。'
- en: '[360] V. A. Trinh, H. Salami Kavaki, and M. I. Mandel, “Importantaug: A data
    augmentation agent for speech,” in *ICASSP*, 2022, pp. 8592–8596.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[360] V. A. Trinh, H. Salami Kavaki, 和 M. I. Mandel, “Importantaug：一种用于语音的数据增强代理，”
    发表在*ICASSP*，2022年，第8592–8596页。'
- en: '[361] M. Li, X. Zhang, C. Thrampoulidis, J. Chen, and S. Oymak, “Autobalance:
    Optimized loss functions for imbalanced data,” in *NeurIPS*, 2021, pp. 3163–3177.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[361] M. Li, X. Zhang, C. Thrampoulidis, J. Chen, 和 S. Oymak, “Autobalance：用于不平衡数据的优化损失函数，”
    发表在*NeurIPS*，2021年，第3163–3177页。'
