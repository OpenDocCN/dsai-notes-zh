- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:48:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:48:09
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2201.12382] Deep Learning Methods for Abstract Visual Reasoning: A Survey
    on Raven’s Progressive Matrices'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2201.12382] 用于抽象视觉推理的深度学习方法：Raven''s Progressive Matrices的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2201.12382](https://ar5iv.labs.arxiv.org/html/2201.12382)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2201.12382](https://ar5iv.labs.arxiv.org/html/2201.12382)
- en: 'Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于抽象视觉推理的深度学习方法：Raven's Progressive Matrices的调查
- en: Mikołaj Małkiński and Jacek Mańdziuk Mikołaj Małkiński is a Ph.D. student at
    the Doctoral School no. 3, Warsaw University of Technology, pl. Politechniki 1,
    00-661 Warsaw, Poland, m.malkinski@mini.pw.edu.pl.Jacek Mańdziuk is with the Faculty
    of Mathematics and Information Science, Warsaw University of Technology, Koszykowa
    75, 00-662 Warsaw, Poland, mandziuk@mini.pw.edu.pl.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Mikołaj Małkiński和Jacek Mańdziuk Mikołaj Małkiński是华沙理工大学博士生，地址：Politechniki
    1, 00-661 Warsaw, Poland，电子邮件：m.malkinski@mini.pw.edu.pl。Jacek Mańdziuk在华沙理工大学数学与信息科学学院工作，地址：Koszykowa
    75, 00-662 Warsaw, Poland，电子邮件：mandziuk@mini.pw.edu.pl。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Abstract visual reasoning (AVR) domain encompasses problems solving which requires
    the ability to reason about relations among entities present in a given scene.
    While humans, generally, solve AVR tasks in a “natural” way, even without prior
    experience, this type of problems has proven difficult for current machine learning
    systems. The paper summarises recent progress in applying deep learning methods
    to solving AVR problems, as a proxy for studying machine intelligence. We focus
    on the most common type of AVR tasks—the Raven’s Progressive Matrices (RPMs)—and
    provide a comprehensive review of the learning methods and deep neural models
    applied to solve RPMs, as well as, the RPM benchmark sets. Performance analysis
    of the state-of-the-art approaches to solving RPMs leads to formulation of certain
    insights and remarks on the current and future trends in this area. We conclude
    the paper by demonstrating how real-world problems can benefit from the discoveries
    of RPM studies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象视觉推理（AVR）领域涵盖了需要推理给定场景中实体之间关系的解决问题的能力。虽然人类通常以“自然”的方式解决AVR任务，即使没有先前经验，但这类问题对当前的机器学习系统来说却是困难的。本文总结了在解决AVR问题中应用深度学习方法的最新进展，作为研究机器智能的替代方法。我们重点关注最常见的AVR任务类型——Raven's
    Progressive Matrices（RPMs）——并提供了关于解决RPMs所应用的学习方法和深度神经模型的全面回顾，以及RPM基准集。对解决RPMs的最先进方法的性能分析导致了对该领域当前和未来趋势的一些见解和评论。我们通过展示实际问题如何从RPM研究的发现中受益来结束本文。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Abstract Visual Reasoning, Deep Learning, Raven’s Progressive Matrices
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象视觉推理，深度学习，Raven’s Progressive Matrices
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Along-standing goal of human research endeavours is to understand the nature
    of intelligence. Even though existing literature points to a variety of definitions [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)], most related to this work is intelligence as
    portrayed by the ability of applying existing knowledge, skills, and past experiences
    in entirely new settings. This perspective was taken in a number of cognitive
    studies that measured intelligence (IQ) with the help of abstract visual reasoning
    (AVR) tasks [[4](#bib.bib4), [5](#bib.bib5)]. AVR problems consist of images with
    simple 2D shapes governed by underlying abstract rules. In order to solve them,
    the test-taker has to identify and often understand never-encountered abstract
    patterns and generalise them to new settings.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人类研究努力的长期目标是理解智能的本质。尽管现有文献指出了多种定义[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]，但与此工作最相关的是智能被描绘为在全新环境中应用现有知识、技能和过去经验的能力。这种观点在一些认知研究中被采用，这些研究通过抽象视觉推理（AVR）任务来衡量智能（IQ）[[4](#bib.bib4),
    [5](#bib.bib5)]。AVR问题由简单的2D形状图像组成，这些图像受潜在抽象规则的支配。为了解决这些问题，测试者必须识别并通常理解从未遇到过的抽象模式，并将其推广到新的环境中。
- en: 'Although there exists a wide range of AVR tasks [[6](#bib.bib6)], in the cognitive
    literature one of them was studied with particular attention—the Raven’s Progressive
    Matrices (RPMs) [[7](#bib.bib7), [8](#bib.bib8)] (see Fig. [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey
    on Raven’s Progressive Matrices")). RPMs were found to be highly diagnostic of
    abstract and relational reasoning abilities [[4](#bib.bib4)] and representative
    for human intelligence in general [[5](#bib.bib5)]. In light of these observations,
    recent works have started to investigate whether automatic pattern discovery algorithms
    are capable of achieving performance comparable to humans in solving RPMs.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管存在广泛的AVR任务[[6](#bib.bib6)]，但在认知文献中，有一个任务被特别关注——即费曼渐进矩阵（RPMs）[[7](#bib.bib7),
    [8](#bib.bib8)]（见图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")）。RPMs被发现对抽象和关系推理能力具有很高的诊断价值[[4](#bib.bib4)]，并且总体上代表了人类智力[[5](#bib.bib5)]。鉴于这些观察，近期的研究开始探讨自动模式发现算法是否能够在解决RPMs方面达到与人类相当的性能。'
- en: A recent stream of research devoted to developing intelligent pattern analysis
    methods employs deep learning (DL) [[9](#bib.bib9)] for discovering regularities
    in complex settings. Motivated by the impressive performance of DL methods in
    various domains [[10](#bib.bib10)] a question whether DL approaches could be effectively
    applied to solving RPMs has been posed [[11](#bib.bib11), [12](#bib.bib12)]. Since
    these seminal works, a number of approaches have been proposed which are reviewed
    and compared in this survey.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近一系列研究致力于开发智能模式分析方法，采用深度学习（DL）[[9](#bib.bib9)]在复杂环境中发现规律。受DL方法在各种领域中表现出的惊人性能的启发[[10](#bib.bib10)]，提出了一个问题：DL方法是否能有效应用于解决RPMs[[11](#bib.bib11),
    [12](#bib.bib12)]。自这些开创性研究以来，已提出了若干方法，这些方法在本综述中进行了回顾和比较。
- en: '![Refer to caption](img/4eb4eb207362aa353c0b0eaab77e6a99.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4eb4eb207362aa353c0b0eaab77e6a99.png)'
- en: 'Figure 1: RPM example. Remarkably, humans are able to intuitively solve the
    challenge even without the exact definition of the task (that is presented in
    Section [2](#S2 "2 Raven’s Progressive Matrices ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices")). The matrix is governed
    by multiple abstract patterns. Each row contains circle slices of 3 different
    colours split among columns, whereas squares have constant colour in each row.
    Moreover, positions of objects in the third column are determined by logical XOR
    applied row-wise in the case of squares and logical OR in the case of circle slices.
    The correct answer is F.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '图1：RPM示例。令人瞩目的是，人类即使没有任务的确切定义（该定义在第[2](#S2 "2 Raven’s Progressive Matrices
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices")节中呈现），也能直观地解决这个挑战。该矩阵由多个抽象模式控制。每一行包含在列中分布的3种不同颜色的圆形切片，而方形在每行中有固定颜色。此外，第三列中的对象位置通过对方形应用逐行逻辑XOR，对圆形切片应用逻辑OR来确定。正确答案是F。'
- en: '{forest}'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '{森林}'
- en: 'for tree= line width=1pt, if=level()¡2 my rounded corners, draw=linecol, ,
    edge=color=linecol, ¿=Triangle[], -¿, if level=0l sep+=.75cm, align=center, font=,
    parent anchor=south, if level=1parent anchor=south west, child anchor=north, tier=parting
    ways, align=center, font=, for descendants= child anchor=west, parent anchor=west,
    anchor=west, align=left, , if level=2 shape=coordinate, no edge, grow’=0, calign
    with current edge, xshift=10pt, for descendants= parent anchor=south west, l sep+=-15pt
    , for children= edge path= [\forestoptionedge] (!to tier=parting ways.parent anchor)
    —- (.child anchor)\forestoptionedge label; , for descendants= no edge, , , , ,
    , [Raven’s Progressive Matrices [Datasets (Section [2](#S2 "2 Raven’s Progressive
    Matrices ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s
    Progressive Matrices")) [ [1\. Sandia [[13](#bib.bib13)]] [2\. Synthetic RPMs [[14](#bib.bib14)]]
    [3\. D-set and G-set [[15](#bib.bib15)]] [4\. PGM [[12](#bib.bib12)]] [5\. RAVEN
    sets [(a) RAVEN [[16](#bib.bib16)]'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于树=线宽=1pt, 如果=层级()¡2我的圆角, 绘制=线色, , 边缘=颜色=线色, ¿=三角形[], -¿, 如果层级=0l 分隔+=.75cm,
    对齐=中心, 字体=, 父锚点=南, 如果层级=1父锚点=西南, 子锚点=北, 层级=分岔, 对齐=中心, 字体=, 对于后代=子锚点=西, 父锚点=西,
    锚点=西, 对齐=左, , 如果层级=2 形状=坐标, 无边缘, 生长'=0, 对齐与当前边缘, x偏移=10pt, 对于后代=父锚点=西南, l 分隔+=-15pt
    , 对于子项=边缘路径=[\forestoptionedge] (!到层级=分岔.父锚点)—(.子锚点)\forestoptionedge 标签; , 对于后代=无边缘,
    , , , , , [拉文进阶矩阵 [数据集 (第 [2](#S2 "2 拉文进阶矩阵 ‣ 深度学习方法用于抽象视觉推理：关于拉文进阶矩阵的调查")) [
    [1\. 桑迪亚 [[13](#bib.bib13)]] [2\. 合成 RPMs [[14](#bib.bib14)]] [3\. D-set 和 G-set [[15](#bib.bib15)]]
    [4\. PGM [[12](#bib.bib12)]] [5\. RAVEN 集 [(a) RAVEN [[16](#bib.bib16)]
- en: (b) I-RAVEN [[17](#bib.bib17)]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (b) I-RAVEN [[17](#bib.bib17)]
- en: '(c) RAVEN-FAIR [[18](#bib.bib18)]] ] ] ] [Learning methods (Section [3](#S3
    "3 Learning to solve RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning:
    A Survey on Raven’s Progressive Matrices")) [ [1\. Supervised training (e.g. [[12](#bib.bib12),
    [16](#bib.bib16)])] [2\. Auxiliary training [(a) Multi-hot (dense) encoding [[12](#bib.bib12),
    [16](#bib.bib16), [19](#bib.bib19)]'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: (c) RAVEN-FAIR [[18](#bib.bib18)]] ] ] ] [学习方法 (第 [3](#S3 "3 学习解决 RPMs ‣ 深度学习方法用于抽象视觉推理：关于拉文进阶矩阵的调查"))
    [ [1\. 监督训练 (例如 [[12](#bib.bib12), [16](#bib.bib16)])] [2\. 辅助训练 [(a) 多热 (密集)
    编码 [[12](#bib.bib12), [16](#bib.bib16), [19](#bib.bib19)]
- en: '(b) One-hot (sparse) encoding [[19](#bib.bib19)]] ] [3\. Contrastive training [[20](#bib.bib20),
    [19](#bib.bib19), [21](#bib.bib21)]] [4\. Feature Robust Abstract Reasoning [[22](#bib.bib22)]]
    [5\. Data augmentation [[19](#bib.bib19), [21](#bib.bib21)]] [6\. Disentangled [[23](#bib.bib23),
    [15](#bib.bib15), [21](#bib.bib21)]] [7\. Generative modeling [[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)]] [8\. Unsupervised learning [[28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30)]] ] ] [Deep learning models (Section [4](#S4
    "4 RPM Deep Learning Models ‣ Deep Learning Methods for Abstract Visual Reasoning:
    A Survey on Raven’s Progressive Matrices")) [ [1\. Baselines [(a) CNN + MLP [[11](#bib.bib11),
    [12](#bib.bib12), [16](#bib.bib16)]'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 独热 (稀疏) 编码 [[19](#bib.bib19)]] ] [3\. 对比训练 [[20](#bib.bib20), [19](#bib.bib19),
    [21](#bib.bib21)]] [4\. 特征稳健的抽象推理 [[22](#bib.bib22)]] [5\. 数据增强 [[19](#bib.bib19),
    [21](#bib.bib21)]] [6\. 解耦 [[23](#bib.bib23), [15](#bib.bib15), [21](#bib.bib21)]]
    [7\. 生成建模 [[24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)]]
    [8\. 无监督学习 [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]] ] ] [深度学习模型
    (第 [4](#S4 "4 RPM 深度学习模型 ‣ 深度学习方法用于抽象视觉推理：关于拉文进阶矩阵的调查")) [ [1\. 基线 [(a) CNN +
    MLP [[11](#bib.bib11), [12](#bib.bib12), [16](#bib.bib16)]
- en: (b) LSTM + MLP [[12](#bib.bib12), [16](#bib.bib16)]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LSTM + MLP [[12](#bib.bib12), [16](#bib.bib16)]
- en: (c) ResNet + MLP [[12](#bib.bib12), [16](#bib.bib16)]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: (c) ResNet + MLP [[12](#bib.bib12), [16](#bib.bib16)]
- en: (d) Context-blind [[12](#bib.bib12), [17](#bib.bib17)]] ] [2\. Relational reasoning
    networks [(a) Wild ResNet [[12](#bib.bib12)]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 上下文盲 [[12](#bib.bib12), [17](#bib.bib17)]] ] [2\. 关系推理网络 [(a) 野外 ResNet [[12](#bib.bib12)]
- en: (b) WReN [[12](#bib.bib12), [16](#bib.bib16)]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (b) WReN [[12](#bib.bib12), [16](#bib.bib16)]
- en: i. VAE-WReN [[23](#bib.bib23)]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: i. VAE-WReN [[23](#bib.bib23)]
- en: ii. ARNe [[31](#bib.bib31)]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ii. ARNe [[31](#bib.bib31)]
- en: iii. MLRN [[32](#bib.bib32)]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: iii. MLRN [[32](#bib.bib32)]
- en: (b) CoPINet [[20](#bib.bib20)]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (b) CoPINet [[20](#bib.bib20)]
- en: (c) MRNet [[18](#bib.bib18)]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (c) MRNet [[18](#bib.bib18)]
- en: (d) LEN [[22](#bib.bib22)]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (d) LEN [[22](#bib.bib22)]
- en: i. T-LEN [[22](#bib.bib22)]] ] [3\. Hierarchical networks [(a) SRAN [[17](#bib.bib17)]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: i. T-LEN [[22](#bib.bib22)]] ] [3\. 分层网络 [(a) SRAN [[17](#bib.bib17)]
- en: (b) SCL [[33](#bib.bib33)]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (b) SCL [[33](#bib.bib33)]
- en: (c) Rel-AIR [[34](#bib.bib34)]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Rel-AIR [[34](#bib.bib34)]
- en: i. Rel-Base [[34](#bib.bib34)]
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: i. Rel-Base [[34](#bib.bib34)]
- en: (d) DCNet [[35](#bib.bib35)]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (d) DCNet [[35](#bib.bib35)]
- en: (e) MXGNet [[36](#bib.bib36)]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (e) MXGNet [[36](#bib.bib36)]
- en: (f) NI [[37](#bib.bib37)]] ] ] ] ]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (f) NI [[37](#bib.bib37)]]
- en: 'Figure 2: RPM taxonomy. A list of RPM benchmarks, learning methods and DL models
    considered in this paper.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：RPM 分类。本文考虑的 RPM 基准、学习方法和深度学习模型的列表。
- en: 1.1 Motivation and scope
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 动机与范围
- en: A growing number of recent publications use RPMs as a proxy to study machine
    intelligence. Due to the increased interest in these problems among the DL community
    since the above-cited seminal papers [[11](#bib.bib11), [12](#bib.bib12)], a number
    of approaches have been proposed that vary in multiple aspects. Furthermore, several
    RPM benchmarks with particular characteristics have been recently proposed that
    allow to analyse various properties and capabilities of the tested methods. These
    methods vary in both the learning setup to solve the tasks and the model architecture.
    The latter oftentimes differs from the architectures used in other domains that
    do not require reasoning about abstract relations spanning multiple entities.
    This RPM-specific aspect imposes the use of dedicated neural components and makes
    analysis of RPM model architectures particularly interesting.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的近期出版物使用 RPM 作为研究机器智能的代理。由于上述引用的开创性论文 [[11](#bib.bib11), [12](#bib.bib12)]
    以来，深度学习社区对这些问题的兴趣增加，提出了多种在多个方面有所不同的方法。此外，最近还提出了一些具有特定特征的 RPM 基准，这些基准可以分析测试方法的各种属性和能力。这些方法在解决任务的学习设置和模型架构上有所不同。后者通常与其他不需要推理跨多个实体的抽象关系的领域所使用的架构不同。这种
    RPM 特有的方面要求使用专门的神经网络组件，并使得 RPM 模型架构的分析尤为有趣。
- en: 'This survey collates existing works on RPMs – the prevalent AVR benchmark in
    DL literature. The review is performed along all three above-mentioned angles,
    i.e. benchmark datasets, learning methods, and DL reasoning models, outlined in
    Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices").'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查汇总了关于 RPM 的现有工作——在深度学习文献中广泛使用的 AVR 基准。该综述沿着上述三个角度进行，即基准数据集、学习方法和深度学习推理模型，见图
    [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning Methods for Abstract Visual
    Reasoning: A Survey on Raven’s Progressive Matrices")。'
- en: Some works that focus on RPMs have already reported superhuman performance on
    simpler benchmarks [[15](#bib.bib15)], whereas the essence of AVR, i.e. the ability
    to generalise to novel difficult environments, remains unattained [[12](#bib.bib12)].
    Indeed, a recent work [[38](#bib.bib38)] shows that current AI systems (including
    DL methods, symbolic approaches and probabilistic program induction) still lag
    far behind the human capabilities not only in solving AVR problems, but generally,
    in broader settings of forming abstractions and analogies. In order to assess
    and better understand the “degree” of intelligence represented by the current
    DL approaches for solving RPMs, we analyse and discuss in detail numerical results
    reported in the literature.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一些专注于 RPM 的研究已经在更简单的基准测试上报告了超越人类的表现 [[15](#bib.bib15)]，然而 AVR 的本质，即对新颖困难环境的泛化能力，仍未实现
    [[12](#bib.bib12)]。实际上，最近的研究 [[38](#bib.bib38)] 表明，目前的 AI 系统（包括深度学习方法、符号方法和概率程序归纳）不仅在解决
    AVR 问题上远远落后于人类能力，而且在更广泛的抽象和类比形成设置中也如此。为了评估和更好地理解当前深度学习方法在解决 RPM 问题时所代表的“智能程度”，我们详细分析和讨论了文献中报告的数值结果。
- en: 1.2 Related work
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 相关工作
- en: Initial attempts that tackled RPMs relied on manually prepared rules and heuristics [[39](#bib.bib39),
    [40](#bib.bib40)], in some cases identified by analysing approaches of well-performing
    human solvers [[41](#bib.bib41)]. Another stream of methods that build on the
    insights from cognitive studies utilised the structure mapping theory [[42](#bib.bib42),
    [43](#bib.bib43)] to propose a set of RPM solvers that automatically discover
    relevant rules [[44](#bib.bib44), [45](#bib.bib45)]. In addition, another set
    of methods eliminates the need for structure mapping and instead focuses on visual
    similarity of the RPM elements after inducing various image transformations [[46](#bib.bib46),
    [47](#bib.bib47)], potentially with different image resolutions that facilitates
    a fractal representation of the RPMs [[48](#bib.bib48), [49](#bib.bib49)]. The
    progress made by these seminal works is comprehensively summarised in [[50](#bib.bib50)].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最初解决RPM的方法依赖于手工编写的规则和启发式方法 [[39](#bib.bib39), [40](#bib.bib40)]，在某些情况下通过分析表现良好的人工解题者的方法来识别 [[41](#bib.bib41)]。另一类方法基于认知研究的见解，利用结构映射理论 [[42](#bib.bib42),
    [43](#bib.bib43)]，提出了一组能够自动发现相关规则的RPM解题器 [[44](#bib.bib44), [45](#bib.bib45)]。此外，还有一组方法消除了结构映射的需求，转而专注于RPM元素的视觉相似性，通过引入各种图像变换 [[46](#bib.bib46),
    [47](#bib.bib47)]，可能使用不同的图像分辨率，以促进RPM的分形表示 [[48](#bib.bib48), [49](#bib.bib49)]。这些开创性工作的进展已在 [[50](#bib.bib50)]中进行了全面总结。
- en: Despite the existence of many creative approaches for solving RPMs, in recent
    days DL methods constantly prove their ability to tackle some of the most challenging
    RPM benchmarks, surpassing human performance in some problem settings. The recent
    abundance and superiority of DL methods motivated us to focus this survey exclusively
    on DL approaches. To our knowledge, such a review perspective has not been considered
    previously in the RPM literature.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管解决RPM（关系推理矩阵）的问题存在许多创造性的方法，但近年来深度学习（DL）方法不断证明其在应对一些最具挑战性的RPM基准测试方面的能力，在某些问题设置中超越了人类表现。深度学习方法的最新丰富性和优越性促使我们将本次调查专门聚焦于深度学习方法。根据我们所知，这样的审视视角在RPM文献中尚未被考虑过。
- en: While particular attention in DL literature on AVR problems is devoted to solving
    RPMs, the whole AVR domain is nowhere near limited to this task. In fact, recent
    works introduce a broad spectrum of complementary abstract visual reasoning tasks
    that allow to test different characteristics of DL approaches.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习文献中对解决RPM问题的关注特别多，但整个抽象视觉推理（AVR）领域远远不止于此。事实上，近期的研究引入了一系列互补的抽象视觉推理任务，这些任务能够测试深度学习方法的不同特征。
- en: Similarly to RPMs, the need for identifying abstract patterns is a recurring
    theme in all AVR tasks. However, depending on the specific problem, these patterns
    have to be extracted from different configurations of matrix panels and then applied
    in various contexts. In the odd one out tasks [[51](#bib.bib51), [52](#bib.bib52),
    [53](#bib.bib53)], the solver has to identify an odd element that breaks a rule
    instantiated in the remaining images. Similar idea is presented in Bongard Problems [[54](#bib.bib54)]
    where the goal is to describe a rule that is instantiated in a set of images and
    broken in a supplementary set of panels. Same-different tasks [[55](#bib.bib55)]
    present a related challenge, in which each problem instance contains two sets
    of images separated by an abstract pattern. Given a new image, the test-taker
    has to assign it to one of the two presented sets. Visual analogy problems [[56](#bib.bib56)]
    are structurally similar to RPMs and alike test the ability of making analogies
    based on abstract patterns that govern the objects and their attributes in images.
    However, in contrast to RPMs, these problems allow to test the ability of generalising
    an abstract concept from a given source domain to a different target domain. Additional
    problems test the extrapolation ability [[57](#bib.bib57)], capacity of recognizing
    abstract patterns from only few samples [[58](#bib.bib58)], or introduce numbers
    into the matrices and test the combined ability of abstract and numerical visual
    reasoning [[59](#bib.bib59)]. A comprehensive review of these emerging AVR tasks
    is conducted in [[60](#bib.bib60)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与RPMs类似，识别抽象模式的需求在所有AVR任务中都是一个反复出现的主题。然而，根据具体问题的不同，这些模式需要从不同配置的矩阵面板中提取，然后应用于各种上下文。在“奇数异常”任务[[51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53)]中，解题者必须识别出一个打破剩余图像中规则的奇数元素。类似的思想在邦加德问题[[54](#bib.bib54)]中提出，目标是描述在一组图像中实例化的规则，并在补充的面板组中被打破。相同-不同任务[[55](#bib.bib55)]提出了一个相关的挑战，其中每个问题实例包含两组图像，这两组图像由一个抽象模式分隔。给定一张新图像，测试者需要将其分配到两个呈现的集合之一。视觉类比问题[[56](#bib.bib56)]在结构上类似于RPMs，并类似地测试基于抽象模式对图像中对象及其属性进行类比的能力。然而，与RPMs不同，这些问题允许测试从给定源领域到不同目标领域的一般化抽象概念的能力。额外的问题测试外推能力[[57](#bib.bib57)]，从仅少量样本中识别抽象模式的能力[[58](#bib.bib58)]，或将数字引入矩阵并测试抽象和数值视觉推理的综合能力[[59](#bib.bib59)]。对这些新兴AVR任务的全面综述见[[60](#bib.bib60)]。
- en: Although each of the above-mentioned problems has its own specificity, we believe
    that in-depth overview of DL application to solving RPMs—a predominant AVR challenge—will
    inspire the readers to identify promising paths for solving related AVR tasks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述问题各自具有特定性，但我们相信，深入概述深度学习（DL）在解决RPMs——这一主要的自动视觉识别（AVR）挑战中的应用——将激励读者找到解决相关AVR任务的有前景的路径。
- en: 1.3 Structure
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 结构
- en: 'The rest of this work is structured as follows. In Section [2](#S2 "2 Raven’s
    Progressive Matrices ‣ Deep Learning Methods for Abstract Visual Reasoning: A
    Survey on Raven’s Progressive Matrices") we introduce the Raven’s Progressive
    Matrices, discuss their importance in measuring intelligence and describe benchmarks
    together with their automatic generation methods. In Sections [3](#S3 "3 Learning
    to solve RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey
    on Raven’s Progressive Matrices") and [4](#S4 "4 RPM Deep Learning Models ‣ Deep
    Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices") we characterise a variety of learning setups to solve RPMs and the
    related DL architectures, respectively. Current evaluations of machine intelligence
    are aggregated and analysed in Section [5](#S5 "5 Evaluating machine intelligence
    with RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s
    Progressive Matrices"). Section [6](#S6 "6 Discussion ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices") links
    the RPM-related literature with advances in other fields, presents open questions,
    potential practical applications of AVR research, and directions for future studies.
    The survey is concluded in Section [7](#S7 "7 Conclusion ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices").'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分结构如下。在第[2](#S2 "2 费尔文的渐进矩阵 ‣ 抽象视觉推理的深度学习方法：费尔文渐进矩阵的调研")节中，我们介绍了费尔文的渐进矩阵，讨论了它们在测量智力中的重要性，并描述了基准及其自动生成方法。在第[3](#S3
    "3 学习解决RPMs ‣ 抽象视觉推理的深度学习方法：费尔文渐进矩阵的调研")和第[4](#S4 "4 RPM深度学习模型 ‣ 抽象视觉推理的深度学习方法：费尔文渐进矩阵的调研")节中，我们分别描述了多种解决RPMs的学习设置和相关的深度学习架构。当前对机器智力的评估在第[5](#S5
    "5 使用RPMs评估机器智力 ‣ 抽象视觉推理的深度学习方法：费尔文渐进矩阵的调研")节中汇总和分析。第[6](#S6 "6 讨论 ‣ 抽象视觉推理的深度学习方法：费尔文渐进矩阵的调研")节将RPM相关文献与其他领域的进展联系起来，提出开放性问题、AVR研究的潜在实际应用及未来研究方向。调查在第[7](#S7
    "7 结论 ‣ 抽象视觉推理的深度学习方法：费尔文渐进矩阵的调研")节中结束。
- en: 2 Raven’s Progressive Matrices
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 费尔文的渐进矩阵
- en: The ability to solve Raven’s Progressive Matrices [[7](#bib.bib7), [8](#bib.bib8)]
    is believed to be highly correlated with human intelligence [[5](#bib.bib5)] and
    is therefore also considered as a natural measure of intelligence of advanced
    artificial reasoning systems. RPMs allow to measure both structural and abstract
    reasoning skills [[4](#bib.bib4)], which characterise human fluid / high-level
    intelligence [[61](#bib.bib61)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 解决费尔文的渐进矩阵[[7](#bib.bib7), [8](#bib.bib8)]的能力被认为与人类智力高度相关[[5](#bib.bib5)]，因此也被视为先进人工推理系统智力的自然测量。RPMs能够测量结构性和抽象推理技能[[4](#bib.bib4)]，这些技能特征表征人类的流体/高级智力[[61](#bib.bib61)]。
- en: 2.1 Problem statement
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题陈述
- en: 'Although RPMs can have variable structure, the most common matrices investigated
    in recent machine learning (ML) literature are composed of images arranged into
    two distinct parts. The first part of the matrix usually contains 8 images arranged
    in a $3\times 3$ grid, referred to as the context panels, where the bottom-right
    image is missing. The goal is to select a panel which correctly completes this
    matrix from another set of several images, referred to as the answer panels (see
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Problem statement ‣ 2 Raven’s Progressive Matrices
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices")). In order to find the correct answer the test-taker is required to
    identify a set of underlying abstract rules which govern the visual attributes
    of the matrix. Generally, these rules describe how image features differ between
    the matrix panels. These relations, as well as objects and their attributes are
    defined differently depending on the dataset.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RPMs可以具有不同的结构，但最近机器学习（ML）文献中研究的最常见矩阵由两部分组成的图像组成。矩阵的第一部分通常包含8张按$3\times 3$网格排列的图像，称为上下文面板，其中右下角的图像缺失。目标是从另一组多个图像中选择一个正确完成此矩阵的面板，这些图像称为答案面板（见图[3](#S2.F3
    "图 3 ‣ 2.1 问题陈述 ‣ 2 费尔文的渐进矩阵 ‣ 抽象视觉推理的深度学习方法：费尔文渐进矩阵的调研")）。为了找到正确的答案，测试者需要识别一组潜在的抽象规则，这些规则支配矩阵的视觉属性。这些规则通常描述了矩阵面板之间图像特征的差异。这些关系以及对象及其属性在不同的数据集中有不同的定义。
- en: '![Refer to caption](img/ebc0eb5064771ba2ae7d7e34cc94a22c.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ebc0eb5064771ba2ae7d7e34cc94a22c.png)'
- en: 'Figure 3: RPM notation. A sample RPM from the RAVEN dataset [[16](#bib.bib16)],
    which consists of two parts – the context and the answer panels. The goal is to
    complete the context with appropriate answer panel. The chosen answer panel must
    fulfil all abstract rules governing the content of the context panels. In the
    example there are three such rules applied: (1) the same set of 3 shapes is present
    in each row, (2) the shapes in a given row are of the same size, and (3) the shape
    color intensity in the third column equals the sum of color intensities in two
    previous columns. Hence, the correct answer is C.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：RPM符号。来自RAVEN数据集 [[16](#bib.bib16)]的一个样本RPM，它由两个部分组成——上下文面板和答案面板。目标是用适当的答案面板来完成上下文。所选答案面板必须符合所有支配上下文面板内容的抽象规则。在示例中，应用了三条规则：（1）每行中存在相同的3种形状，（2）给定行中的形状大小相同，以及（3）第三列中的形状颜色强度等于前两列颜色强度的总和。因此，正确答案是C。
- en: 2.2 Automatic generation of RPMs
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 RPMs的自动生成
- en: The original set of RPMs (called the Standard Progressive Matrices, SPMs) proposed
    in [[7](#bib.bib7), [8](#bib.bib8)] contains a limited number of hand-crafted
    instances, which may be insufficient for training even simple statistical pattern
    recognition models that struggle to learn from small sample size [[62](#bib.bib62),
    [63](#bib.bib63)]. Since the process of successful training of DL models usually
    requires large volumes of training data points [[64](#bib.bib64), [65](#bib.bib65)],
    there is a strong need for methods of automatic generation of AVR problems that
    could provide sufficiently large numbers of training samples.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[7](#bib.bib7), [8](#bib.bib8)]中提出的原始RPM集（称为标准渐进矩阵，SPMs）包含了有限数量的手工制作实例，这可能不足以训练即使是简单的统计模式识别模型，这些模型在学习小样本时会遇到困难 [[62](#bib.bib62),
    [63](#bib.bib63)]。由于成功训练深度学习模型通常需要大量的训练数据点 [[64](#bib.bib64), [65](#bib.bib65)]，因此迫切需要自动生成AVR问题的方法，以提供足够多的训练样本。
- en: In order to procedurally generate new RPMs, several challenges have to be addressed.
    First, the generated problems should be visually diverse to make them non-repetitive
    and appealing to the test-taker. Moreover, multiple levels of difficulty are preferred
    that allow to better estimate the reasoning capabilities of the solver. Lastly,
    the generated RPM should be valid, i.e. there should be only one answer from the
    set of possible choices that correctly completes the matrix. The task of choosing
    the correct answer should be realisable by following the Occam’s razor principle
    – the answer should be justifiable with a minimal set of rules that govern relations
    between objects and their attributes across context panels.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了程序化生成新的RPMs，必须解决几个挑战。首先，生成的问题应该在视觉上具有多样性，使其不重复且对测试者具有吸引力。此外，最好有多个难度级别，以便更好地评估解题者的推理能力。最后，生成的RPM应该是有效的，即在可能的选项集中只有一个答案能够正确完成矩阵。选择正确答案的任务应该符合奥卡姆剃刀原则——答案应该用最小的规则集来解释，这些规则支配了不同上下文面板之间对象及其属性的关系。
- en: 'Geared toward satisfying these requirements, various attempts have been made
    to design an RPM generation algorithm capable of delivering huge number of instances.
    In effect, several popular RPM datasets have been proposed which include the Sandia
    matrices [[13](#bib.bib13)], D-set and G-set from [[15](#bib.bib15)], PGM [[12](#bib.bib12)],
    RAVEN [[16](#bib.bib16)], I-RAVEN [[17](#bib.bib17)] and RAVEN-FAIR [[18](#bib.bib18)].
    Samples from these datasets are illustrated in Fig. [4](#S2.F4 "Figure 4 ‣ 2.2
    Automatic generation of RPMs ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning
    Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些要求，已经进行了各种尝试，以设计一个能够提供大量实例的RPM生成算法。实际上，已经提出了几个流行的RPM数据集，包括Sandia矩阵 [[13](#bib.bib13)]、来自 [[15](#bib.bib15)]的D集和G集、PGM [[12](#bib.bib12)]、RAVEN [[16](#bib.bib16)]、I-RAVEN [[17](#bib.bib17)]和RAVEN-FAIR [[18](#bib.bib18)]。这些数据集的样本如图 [4](#S2.F4
    "图 4 ‣ 2.2 RPMs的自动生成 ‣ 2 Raven的渐进矩阵 ‣ 抽象视觉推理的深度学习方法：对Raven渐进矩阵的综述")所示。
- en: '![Refer to caption](img/ec655acba42af0dc1d93f96e4c56e36c.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec655acba42af0dc1d93f96e4c56e36c.png)'
- en: (a) Sandia [[13](#bib.bib13)]
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Sandia [[13](#bib.bib13)]
- en: '![Refer to caption](img/659a690ecb8cf7d2b183e83276ab75c1.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/659a690ecb8cf7d2b183e83276ab75c1.png)'
- en: (b) Synthetic [[14](#bib.bib14)]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 合成的 [[14](#bib.bib14)]
- en: '![Refer to caption](img/4fe0b6d0d0c25045923a5b620fbce8cd.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4fe0b6d0d0c25045923a5b620fbce8cd.png)'
- en: (c) G-set [[15](#bib.bib15)]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: (c) G集 [[15](#bib.bib15)]
- en: '![Refer to caption](img/3c09c0b0c3fe3b8d8f1816aece56b28b.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/3c09c0b0c3fe3b8d8f1816aece56b28b.png)'
- en: (d) PGM [[12](#bib.bib12)]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (d) PGM [[12](#bib.bib12)]
- en: '![Refer to caption](img/219ff25e2fde3119285f423e65ac8b8a.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/219ff25e2fde3119285f423e65ac8b8a.png)'
- en: (e) I-RAVEN [[16](#bib.bib16), [17](#bib.bib17)]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (e) I-RAVEN [[16](#bib.bib16), [17](#bib.bib17)]
- en: 'Figure 4: RPM examples. Correct answers are marked with a green boundary. Matrices
    from RAVEN, I-RAVEN and RAVEN-FAIR differ only in the way of generating answers,
    hence only a selected matrix from the I-RAVEN dataset is shown.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：RPM示例。正确答案用绿色边框标记。RAVEN、I-RAVEN和RAVEN-FAIR的矩阵仅在生成答案的方式上有所不同，因此仅展示了I-RAVEN数据集中的一个选择矩阵。
- en: 2.2.1 Sandia matrices
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 桑迪亚矩阵
- en: 'The Sandia matrix generation process [[13](#bib.bib13)] marked the first widely-known
    attempt to expand the set of available RPM instances. Based on the analysis of
    the SPMs, the authors extracted a set of logic rules, shapes, and transformations
    that modify their attributes, which were later used to generate a large set of
    RPMs. The authors compared the quality of generated matrices to the original ones
    and found out, that although simpler instances (with 1 or 2 rules) were of similar
    difficulty to SPMs, instances with higher number of rules were generally more
    difficult than their SPMs counterparts. A sample RPM generated with the Sandia
    software is shown in Fig. [4a](#S2.F4.sf1 "In Figure 4 ‣ 2.2 Automatic generation
    of RPMs ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices").'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 桑迪亚矩阵生成过程[[13](#bib.bib13)]标志着第一次广泛知晓的扩展可用RPM实例集的尝试。根据对SPM的分析，作者提取了一组逻辑规则、形状和转换，这些规则和转换修改了它们的属性，后来用于生成大量RPM。作者将生成的矩阵质量与原始矩阵进行比较，发现尽管较简单的实例（具有1或2条规则）与SPM的难度相当，但具有更多规则的实例通常比其SPM对应物更困难。图[4a](#S2.F4.sf1
    "在图4 ‣ 2.2 RPM的自动生成 ‣ 2 拉文的渐进矩阵 ‣ 深度学习方法在抽象视觉推理中的应用：对拉文的渐进矩阵的调查")展示了使用桑迪亚软件生成的一个示例RPM。
- en: 2.2.2 Synthetic RPMs
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 合成RPM
- en: 'Another approach represents abstract RPM structure using first-order logic
    and formulates sampling restrictions that allow to construct only valid RPMs [[14](#bib.bib14)],
    such as those presented in Fig. [4b](#S2.F4.sf2 "In Figure 4 ‣ 2.2 Automatic generation
    of RPMs ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices"). Moreover, the authors
    conducted a user study to validate whether the constructed matrices differ from
    the original set of SPMs proposed in [[7](#bib.bib7)]. The experiment revealed
    that the generated problems are statistically indistinguishable from the manually
    designed matrices among a group of 24 respondents.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法使用一阶逻辑表示抽象的RPM结构，并制定采样限制，只允许构造有效的RPM[[14](#bib.bib14)]，例如图[4b](#S2.F4.sf2
    "在图4 ‣ 2.2 RPM的自动生成 ‣ 2 拉文的渐进矩阵 ‣ 深度学习方法在抽象视觉推理中的应用：对拉文的渐进矩阵的调查")中展示的那些。此外，作者进行了用户研究，以验证构造的矩阵是否与[[7](#bib.bib7)]中提出的原始SPM集不同。实验结果表明，生成的问题在24名受访者中与手工设计的矩阵在统计上没有显著差异。
- en: 2.2.3 D-set and G-set
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 D集和G集
- en: 'Another suite of automatically generated RPMs was proposed in [[15](#bib.bib15)].
    The D-set and G-set datasets have been generated based on design principles from
    prior works [[66](#bib.bib66), [5](#bib.bib5)]. The resultant matrices, with an
    example shown in Fig. [4c](#S2.F4.sf3 "In Figure 4 ‣ 2.2 Automatic generation
    of RPMs ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices"), although structurally
    similar to those from the Sandia suite, represented a different feature distribution.
    Namely, the authors of [[15](#bib.bib15)] utilised a completely different set
    of object shapes and supported full ranges for object attributes (shading, rotation
    and size) as compared to discrete values used in Sandia. This broader RPM configuration
    was realised in D-set, whereas G-set was curated to resemble the feature distributions
    found in Sandia. Thanks to synthesizing their own RPMs, the authors were able
    to use Sandia matrices as an additional evaluation dataset for verifying out-of-distribution
    generalisation of the proposed DeepIQ system in a transductive transfer learning
    setting.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一套自动生成的RPM被提出于[[15](#bib.bib15)]。D-set和G-set数据集是基于之前工作的设计原则生成的[[66](#bib.bib66),
    [5](#bib.bib5)]。生成的矩阵，以图例[4c](#S2.F4.sf3 "在图4 ‣ 2.2 自动生成RPM ‣ 2 Raven的渐进矩阵 ‣ 针对Raven渐进矩阵的深度学习方法：Raven渐进矩阵的综述")为例，虽然在结构上类似于Sandia套件的矩阵，但呈现出不同的特征分布。具体来说，[[15](#bib.bib15)]的作者使用了一套完全不同的对象形状，并支持对象属性（阴影、旋转和大小）的全范围，而不是Sandia中使用的离散值。这种更广泛的RPM配置在D-set中实现，而G-set则被策划成类似于Sandia中的特征分布。由于合成了自己的RPM，作者能够将Sandia矩阵作为额外的评估数据集，用于在传递性迁移学习设置中验证提议的DeepIQ系统的分布外泛化能力。
- en: 2.2.4 PGM
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 PGM
- en: 'Machine performance on the above-introduced datasets [[15](#bib.bib15), [13](#bib.bib13)]
    may be misleading, as the models are trained on large sets of matrices with similar
    structure to those found in the testing suite. In order to better evaluate the
    generalisation capabilities of DL models the PGM dataset [[12](#bib.bib12)] was
    proposed which arranges problems into 8 generalisation regimes with variable difficulty.
    To achieve this goal, each matrix from PGM (see example in Fig. [4d](#S2.F4.sf4
    "In Figure 4 ‣ 2.2 Automatic generation of RPMs ‣ 2 Raven’s Progressive Matrices
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices")) has an associated representation of the abstract structure, i.e. a
    set of triples $\mathcal{S}=\{[r,o,a]\ |\ r\in\mathcal{R},0\in\mathcal{O},a\in\mathcal{A}\}$,
    where $\mathcal{R}$ $=$ $\{\texttt{progression},$ $\texttt{XOR},$ $\texttt{OR},$
    $\texttt{AND},$ $\texttt{consistent union}\}$ defines the set of rules, $\mathcal{O}$
    $=$ $\{\texttt{shape},$ $\texttt{line}\}$ the set of objects and $\mathcal{A}$
    $=$ $\{\texttt{size},$ $\texttt{type},$ $\texttt{color},$ $\texttt{position},$
    $\texttt{number}\}$ the set of attributes. Based on these structures the dataset
    arranges RPM instances intro training, validation and test splits, such that both
    train and validation parts have matrices with the same structures, whereas test
    matrices are governed by relations not seen in the train and validation sets.
    Although being a perfect test bed for measuring generalisation in DL models, the
    dataset is characterised with an enormous size (each regime contains $1\,420\,000$
    RPMs, where $1.2$M belong to the training split) that is often a bottleneck for
    evaluating multiple models.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述数据集[[15](#bib.bib15), [13](#bib.bib13)]上进行的机器性能测试可能会产生误导，因为模型是在结构类似于测试套件的矩阵的大集合上训练的。为了更好地评估深度学习模型的泛化能力，提出了PGM数据集[[12](#bib.bib12)]，该数据集将问题安排到8个具有不同难度的泛化模式中。为了实现这一目标，PGM中的每个矩阵（见图例[4d](#S2.F4.sf4
    "在图4 ‣ 2.2 自动生成RPM ‣ 2 Raven的渐进矩阵 ‣ 针对Raven渐进矩阵的深度学习方法：Raven渐进矩阵的综述")）都有一个抽象结构的相关表示，即一个三元组集合$\mathcal{S}=\{[r,o,a]\
    |\ r\in\mathcal{R},0\in\mathcal{O},a\in\mathcal{A}\}$，其中$\mathcal{R}$ $=$ $\{\texttt{progression},$
    $\texttt{XOR},$ $\texttt{OR},$ $\texttt{AND},$ $\texttt{consistent union}\}$定义了规则集合，$\mathcal{O}$
    $=$ $\{\texttt{shape},$ $\texttt{line}\}$定义了对象集合，$\mathcal{A}$ $=$ $\{\texttt{size},$
    $\texttt{type},$ $\texttt{color},$ $\texttt{position},$ $\texttt{number}\}$定义了属性集合。基于这些结构，数据集将RPM实例分为训练、验证和测试集，其中训练和验证部分具有相同结构的矩阵，而测试矩阵则由训练和验证集中未见过的关系主导。尽管是一个衡量深度学习模型泛化能力的理想测试平台，但该数据集的规模巨大（每个模式包含$1\,420\,000$个RPM，其中$1.2$M属于训练集），这往往成为评估多个模型的瓶颈。
- en: 2.2.5 RAVEN
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5 RAVEN
- en: 'Another RPM dataset named RAVEN [[16](#bib.bib16)] aims to present a visually
    broader matrices with hierarchical structure (see Fig. [4e](#S2.F4.sf5 "In Figure
    4 ‣ 2.2 Automatic generation of RPMs ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning
    Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")).
    This is achieved with the help of Attributed Stochastic Image Grammar (A-SIG) [[67](#bib.bib67),
    [68](#bib.bib68), [69](#bib.bib69)]. RAVEN contains matrices belonging to 7 visual
    configurations and similarly to PGM each RPM from RAVEN has an associatied abstract
    structure. However, in this case the structure is defined as a set of pairs $\mathcal{S}$
    $=$ $\{[r,a]$ $|$ $r\in\mathcal{R},$ $a\in\mathcal{A}\}$, where $\mathcal{R}$
    $=$ $\{\texttt{constant},$ $\texttt{progression},$ $\texttt{arithmetic},$ $\texttt{distribute
    three}\}$ defines the set of rules and $\mathcal{A}$ $=$ $\{\texttt{number},$
    $\texttt{position},$ $\texttt{type},$ $\texttt{size},$ $\texttt{color}\}$ the
    set of attributes. In contrast to PGM, the dataset contains supplementary structural
    annotations thanks to A-SIG that connects visual and structural representations.
    Moreover, Zhang et al. [[16](#bib.bib16)] have carried out a human evaluation
    on RAVEN matrices which allows to better assess the performance of DL approaches.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个名为RAVEN的RPM数据集[[16](#bib.bib16)]旨在呈现具有层级结构的视觉上更广泛的矩阵（见图[4e](#S2.F4.sf5 "在图4
    ‣ 2.2 自动生成RPMs ‣ 2 RAVEN的渐进矩阵 ‣ 针对抽象视觉推理的深度学习方法：关于RAVEN的渐进矩阵的调查")）。这通过属性随机图像语法（A-SIG）[[67](#bib.bib67),
    [68](#bib.bib68), [69](#bib.bib69)]的帮助实现。RAVEN包含属于7种视觉配置的矩阵，并且类似于PGM，每个RAVEN的RPM都有一个相关的抽象结构。然而，在这种情况下，结构定义为一组对$\mathcal{S}$
    $=$ $\{[r,a]$ $|$ $r\in\mathcal{R},$ $a\in\mathcal{A}\}$，其中$\mathcal{R}$ $=$ $\{\texttt{constant},$
    $\texttt{progression},$ $\texttt{arithmetic},$ $\texttt{distribute three}\}$定义了规则集合，而$\mathcal{A}$
    $=$ $\{\texttt{number},$ $\texttt{position},$ $\texttt{type},$ $\texttt{size},$
    $\texttt{color}\}$定义了属性集合。与PGM不同的是，该数据集由于A-SIG包含了额外的结构注释，连接了视觉和结构表示。此外，Zhang等人[[16](#bib.bib16)]对RAVEN矩阵进行了人类评估，这使得能够更好地评估深度学习方法的表现。
- en: 'The dataset is composed of $42\,000$ training RPMs, and additional $2\times
    14\,000$ problems allocated for validation and testing splits, respectively. In
    comparison to PGM, RAVEN’s matrices have a few times higher average number of
    rules: 6.29 vs 1.37. At the same time, PGM is better-suited for evaluating out-of-distribution
    generalisation due to defining explicit generalisation regimes.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含$42\,000$个训练RPMs，另有$2\times 14\,000$个问题分别分配用于验证和测试拆分。与PGM相比，RAVEN的矩阵具有几倍更高的平均规则数：6.29对1.37。同时，由于定义了明确的泛化机制，PGM更适合用于评估分布外泛化。
- en: 2.2.6 I-RAVEN
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.6 I-RAVEN
- en: Although a follow-up work reported superhuman performance on RAVEN [[20](#bib.bib20)],
    it was later revealed that such impressive results may arise from a shortcut solution
    due to biased answer sets. In fact, the problem of shortcut learning is prevalent
    in visual reasoning research and was identified across multiple related problems [[70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73)]. In the case of RAVEN, a
    context-blind model—one which processes only the answer panels and discards context
    ones—was shown to achieve close to perfect performance, bypassing the need for
    discovering abstract rules that govern the matrices. It was brought to light that
    correct answer to RPMs from RAVEN may be obtained by selecting answer panel with
    the most common attributes [[17](#bib.bib17)]. In order to fix this defect, the
    I-RAVEN dataset [[17](#bib.bib17)] was proposed that generates the set of answers
    with an iterative tree-based method. Context-blind models trained on such impartial
    dataset were shown to produce classifications equivalent to random guessing [[33](#bib.bib33),
    [17](#bib.bib17)], in effect demonstrating validity of this dataset.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有后续研究报告在RAVEN上取得了超人类的表现[[20](#bib.bib20)]，但后来揭示这些令人印象深刻的结果可能源于由于偏倚的答案集而产生的捷径解法。实际上，捷径学习的问题在视觉推理研究中普遍存在，并且在多个相关问题中被识别出来[[70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73)]。在RAVEN的案例中，一种无视上下文的模型——即仅处理答案面板而忽略上下文面板的模型——被证明能够接近完美的表现，从而绕过了发现控制矩阵的抽象规则的需求。研究发现，RAVEN的RPMs的正确答案可以通过选择具有最常见属性的答案面板来获得[[17](#bib.bib17)]。为了修正这个缺陷，提出了I-RAVEN数据集[[17](#bib.bib17)]，该数据集通过迭代树基方法生成答案集。使用这种公正数据集训练的无视上下文模型被证明产生的分类等同于随机猜测[[33](#bib.bib33),
    [17](#bib.bib17)]，有效地证明了该数据集的有效性。
- en: 2.2.7 RAVEN-FAIR
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.7 RAVEN-FAIR
- en: Similarly to I-RAVEN, RAVEN-FAIR [[18](#bib.bib18)] promises to solve the problem
    of biased choice panels of the original dataset. However, as noted in the supplementary
    material of [[18](#bib.bib18), Table 4], a Context-blind ResNet model scores 17.2%
    accuracy on the proposed dataset. At the same time, the same context-blind model
    evaluated on I-RAVEN scores 12.5%, which is equal to the random guess accuracy.
    This indicates that although RAVEN-FAIR is unquestionably less biased than the
    original RAVEN, not all bias sources were mitigated. Therefore, among three RAVEN-type
    datasets, it is recommended to use the unbiased I-RAVEN when testing RPM reasoning
    models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与I-RAVEN类似，RAVEN-FAIR [[18](#bib.bib18)]承诺解决原始数据集中有偏选择面板的问题。然而，正如[[18](#bib.bib18),
    表4]的补充材料中所指出的，Context-blind ResNet模型在提出的数据集上得分为17.2%。同时，相同的context-blind模型在I-RAVEN上的得分为12.5%，这等于随机猜测的准确度。这表明，尽管RAVEN-FAIR无疑比原始RAVEN更少偏差，但并不是所有的偏差来源都得到了缓解。因此，在三种RAVEN类型的数据集中，建议在测试RPM推理模型时使用无偏的I-RAVEN。
- en: 3 Learning to solve RPMs
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 学习解决RPM
- en: After the preliminary attempts [[11](#bib.bib11), [12](#bib.bib12)], multiple
    distinct approaches have been proposed for training DL models to solve RPMs, which
    are summarised in this section. Assume that an RPM instance $\mathcal{P}=(X,y)$
    has $16$ panels divided equally into context and answer images, where $X=\{x_{i}\}_{i=1}^{16}$
    is a set of all $16$ images and $y\in\{1,\ldots,8\}$ is an index of the correct
    answer. Moreover, we denote by $X_{c}=\{x_{i}\}_{i=1}^{8}\subset X$ the set of
    context panels, and by $X_{a}=\{x_{i}\}_{i=9}^{16}=\{a_{j}\}_{j=1}^{8}\subset
    X$ the set of answer panels (a.k.a candidate or choice panels). The above number
    of $8$ answer panels holds for RPMs from Sandia suite [[13](#bib.bib13)], for
    synthetic RPMs [[14](#bib.bib14)], for matrices from PGM [[12](#bib.bib12)], RAVEN [[16](#bib.bib16)],
    I-RAVEN [[17](#bib.bib17)], and RAVEN-FAIR [[18](#bib.bib18)], whereas RPMs used
    in [[15](#bib.bib15)] have 3 possible answers less. Let us also denote by $X_{c\cup
    a_{j}}=\{x_{i}\}_{i=1}^{8}\cup\{a_{j}\}\subset X$ an RPM with the missing panel
    completed by an answer panel with index $j$.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步尝试之后 [[11](#bib.bib11), [12](#bib.bib12)]，已经提出了多种不同的方法来训练DL模型以解决RPM，这些方法在本节中总结。假设一个RPM实例$\mathcal{P}=(X,y)$有$16$个面板，均分为上下文和答案图像，其中$X=\{x_{i}\}_{i=1}^{16}$是一组所有$16$张图像，而$y\in\{1,\ldots,8\}$是正确答案的索引。此外，我们用$X_{c}=\{x_{i}\}_{i=1}^{8}\subset
    X$表示上下文面板的集合，用$X_{a}=\{x_{i}\}_{i=9}^{16}=\{a_{j}\}_{j=1}^{8}\subset X$表示答案面板的集合（也称为候选或选择面板）。上述$8$个答案面板的数量适用于Sandia套件 [[13](#bib.bib13)]、合成RPM [[14](#bib.bib14)]、PGM [[12](#bib.bib12)]、RAVEN [[16](#bib.bib16)]、I-RAVEN [[17](#bib.bib17)]和RAVEN-FAIR [[18](#bib.bib18)]的矩阵，而[[15](#bib.bib15)]中使用的RPM则少了$3$个可能的答案。我们还用$X_{c\cup
    a_{j}}=\{x_{i}\}_{i=1}^{8}\cup\{a_{j}\}\subset X$表示一个缺少面板的RPM，该面板被索引为$j$的答案面板所填补。
- en: Consider an RPM reasoning model $\mathcal{N}(X)=\{h_{j}\}_{j=1}^{8}$, that given
    the RPM panels $X$ produces embedding vectors $\{h_{j}\}_{j=1}^{8}$, one for each
    of the answer panels, where $h_{j}\in\mathbb{R}^{d}$ for $d\geq 1$. Hereinafter
    we will refer to $\{h_{j}\}_{j=1}^{8}$ as candidate embeddings. Model $\mathcal{N}$
    can be implemented as any differentiable function – concrete examples from the
    literature are discussed in the following section. Based on the provided definitions
    let us now focus on the proposed schemes for learning to solve RPMs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个RPM推理模型$\mathcal{N}(X)=\{h_{j}\}_{j=1}^{8}$，该模型在给定RPM面板$X$时生成嵌入向量$\{h_{j}\}_{j=1}^{8}$，每个答案面板对应一个，其中$h_{j}\in\mathbb{R}^{d}$，$d\geq
    1$。以后我们将$\{h_{j}\}_{j=1}^{8}$称为候选嵌入。模型$\mathcal{N}$可以实现为任何可微分函数——具体的文献示例将在下一节讨论。基于所提供的定义，让我们现在集中于学习解决RPM的建议方案。
- en: 3.1 Supervised training
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 监督训练
- en: 'In the supervised training, the model is trained to predict an index of the
    answer panel which correctly completes the matrix. For this purpose, several works
    (e.g. [[11](#bib.bib11), [12](#bib.bib12), [15](#bib.bib15), [16](#bib.bib16)])
    employ a scoring module $\psi(h)=s\in\mathbb{R}$, which produces a single logit
    $s$ for each candidate embedding. Although in practice $\psi$ is often implemented
    as a multi-layer network (e.g. MLP in [[11](#bib.bib11), [15](#bib.bib15)] or
    Relation Network [[74](#bib.bib74)] in [[12](#bib.bib12)]), we consider these
    modules as part of $\mathcal{N}$ and consider the scoring module in the form of
    a simple linear layer with learnable weights. The supervised setup gathers individual
    logits into a set $\mathcal{S}=\{s_{j}\}_{j=1}^{8}$ and converts it to a probability
    distribution over the set of possible answers for an RPM $\mathcal{P}$, with $p(\mathcal{P})=\{p(\mathcal{P})_{j}\}_{j=1}^{8}=\text{softmax}(\mathcal{S})$.
    Using the estimated probability, the scoring module $\psi$ is optimised together
    with the base network $\mathcal{N}$ with a standard cross-entropy loss function.
    That is, for a batch of RPMs $\{\mathcal{P}_{i}\}_{i=1}^{N}$, where $N$ is the
    batch size, the following objective is minimised:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督训练中，模型被训练来预测一个正确完成矩阵的答案面板的索引。为此，几项工作（例如[[11](#bib.bib11), [12](#bib.bib12),
    [15](#bib.bib15), [16](#bib.bib16)]）使用了一个评分模块 $\psi(h)=s\in\mathbb{R}$，该模块为每个候选嵌入生成一个单一的logit
    $s$。尽管在实际应用中，$\psi$ 通常实现为多层网络（例如[[11](#bib.bib11), [15](#bib.bib15)]中的MLP或[[12](#bib.bib12)]中的Relation
    Network [[74](#bib.bib74)]），我们将这些模块视为$\mathcal{N}$的一部分，并将评分模块视为具有可学习权重的简单线性层。监督设置将各个logits汇聚成一个集合
    $\mathcal{S}=\{s_{j}\}_{j=1}^{8}$ 并将其转换为RPM $\mathcal{P}$的可能答案集合上的概率分布，其中 $p(\mathcal{P})=\{p(\mathcal{P})_{j}\}_{j=1}^{8}=\text{softmax}(\mathcal{S})$。通过使用估计的概率，评分模块
    $\psi$ 与基础网络 $\mathcal{N}$ 一起通过标准交叉熵损失函数进行优化。即，对于一批RPM $\{\mathcal{P}_{i}\}_{i=1}^{N}$，其中
    $N$ 是批量大小，最小化以下目标：
- en: '|  | $\mathcal{L}^{\text{ce}}=-\frac{1}{N}\sum_{i=1}^{N}p(\mathcal{P}_{i})\log
    q(\mathcal{P}_{i})$ |  | (1) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}^{\text{ce}}=-\frac{1}{N}\sum_{i=1}^{N}p(\mathcal{P}_{i})\log
    q(\mathcal{P}_{i})$ |  | (1) |'
- en: where $q(\mathcal{P}_{i})=\text{onehot}(y_{i})$ is the one-hot encoded index
    of the correct answer for $\mathcal{P}_{i}$. The choice panel corresponding to
    the highest probability is considered as an answer chosen by the network, i.e.
    $\hat{y}=\text{argmax}_{j}\{p(\mathcal{P})_{j}\}_{j=1}^{8}$.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q(\mathcal{P}_{i})=\text{onehot}(y_{i})$ 是$\mathcal{P}_{i}$的正确答案的一热编码索引。对应最高概率的选择面板被视为网络选择的答案，即
    $\hat{y}=\text{argmax}_{j}\{p(\mathcal{P})_{j}\}_{j=1}^{8}$。
- en: 3.2 Auxiliary training
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 辅助训练
- en: In order to solve an RPM, one has to first recognise its abstract structure
    which governs the objects, attributes and relations present in the images. It
    was shown in [[12](#bib.bib12)] that training neural networks to justify their
    answer by predicting such an abstract structure leads to better performance in
    the final classification task. For this purpose Barrett et al. [[12](#bib.bib12)]
    proposed to redefine RPMs as triples $\mathcal{P}=(X,R,y)$, where $R\subset\mathcal{R}$
    defines the set of underlying abstract rules governing the RPM. The set of all
    abstract rules $\mathcal{R}$ is defined dependent on the dataset, as well as the
    maximal number of rules $n_{R}$ per RPM ($1\leq n_{R}\leq 4$ for PGM and $1\leq
    n_{R}\leq 8$ for RAVEN and its derivatives). Barrett et al. [[12](#bib.bib12)]
    proposed to encode PGM abstract rules with a multi-hot encoding and employ a rule
    prediction head $\rho(\sum_{j=1}^{8}h_{j})=\widehat{R}$, for estimating the set
    of RPM abstract rules, in the form of a vector $\widehat{R}$ of fixed length.
    The prediction $\widehat{R}$ was activated with a sigmoid function. A binary cross-entropy
    loss function $\mathcal{L}^{\text{aux}}$ was used to compare it with the ground-truth
    encoded rule representation $R$. During training a joint loss function $\mathcal{L}=\mathcal{L}^{\text{ce}}+\beta\mathcal{L}^{\text{aux}}$
    was minimised, where $\beta$ was a balancing coefficient.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决RPM问题，首先需要识别其抽象结构，该结构决定了图像中对象、属性和关系。[[12](#bib.bib12)]中的研究表明，训练神经网络通过预测这种抽象结构来解释其答案，可以提高最终分类任务的性能。为此，Barrett等人[[12](#bib.bib12)]提出将RPM重新定义为三元组$\mathcal{P}=(X,R,y)$，其中$R\subset\mathcal{R}$定义了支配RPM的潜在抽象规则集合。所有抽象规则的集合$\mathcal{R}$是依赖于数据集定义的，以及每个RPM的最大规则数$n_{R}$（PGM为$1\leq
    n_{R}\leq 4$，RAVEN及其衍生数据集为$1\leq n_{R}\leq 8$）。Barrett等人[[12](#bib.bib12)]提议使用多热编码对PGM抽象规则进行编码，并采用规则预测头$\rho(\sum_{j=1}^{8}h_{j})=\widehat{R}$，以固定长度的向量形式估计RPM抽象规则集合。预测值$\widehat{R}$通过sigmoid函数激活。使用二元交叉熵损失函数$\mathcal{L}^{\text{aux}}$将其与真实编码规则表示$R$进行比较。在训练过程中，最小化联合损失函数$\mathcal{L}=\mathcal{L}^{\text{ce}}+\beta\mathcal{L}^{\text{aux}}$，其中$\beta$是平衡系数。
- en: Similar approach was validated on the RAVEN dataset in [[16](#bib.bib16)], where
    besides the rule related auxiliary target the authors proposed another loss function
    related to the prediction of RPM structure. Surprisingly, after training models
    with these supplementary objectives, their performance deteriorated. Analogous
    conclusions were drawn in multiple follow-up works [[20](#bib.bib20), [17](#bib.bib17),
    [33](#bib.bib33), [36](#bib.bib36)] that evaluated their approaches on RAVEN and
    its derivatives. However, in [[19](#bib.bib19)] we have shown that this inferior
    performance can be overcome by replacing the rule encoding method with the sparse
    encoding—an alternative rule representation based on one-hot encoding—that provides
    a more accurate training signal.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的方法在RAVEN数据集上得到了验证[[16](#bib.bib16)]，在这些方法中，除了与规则相关的辅助目标外，作者还提出了另一种与RPM结构预测相关的损失函数。令人惊讶的是，在使用这些补充目标训练模型后，其性能却有所下降。在多个后续研究中也得出了类似的结论[[20](#bib.bib20),
    [17](#bib.bib17), [33](#bib.bib33), [36](#bib.bib36)]，这些研究评估了他们在RAVEN及其衍生数据集上的方法。然而，在[[19](#bib.bib19)]中我们展示了，通过用稀疏编码——一种基于独热编码的替代规则表示——替代规则编码方法，可以克服这种性能下降，从而提供更准确的训练信号。
- en: 3.3 Contrastive training
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 对比训练
- en: The ability to juxtapose correct and wrong answers was identified as a key component
    in adaptive problem solving across cognitive literature [[75](#bib.bib75), [76](#bib.bib76),
    [77](#bib.bib77)]. A number of works have proposed to incorporate such contrastive
    mechanisms either directly in the model architecture [[20](#bib.bib20)] or in
    the objective function [[20](#bib.bib20), [19](#bib.bib19), [21](#bib.bib21),
    [17](#bib.bib17)]. Zhang et al. [[20](#bib.bib20)] formulate an alternative to
    the supervised training setup by substituting cross-entropy with a variant of
    NCE loss that encourages contrast effects. The authors argue for shifting the
    view of solving RPMs from a classification task to ranking, where answer panels
    are ranked according to their probability of correctly completing the matrix.
    NCE-based objective functions are further utilised in the following works.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将正确和错误答案并列比较的能力被确定为认知文献中自适应问题解决的关键组成部分 [[75](#bib.bib75), [76](#bib.bib76),
    [77](#bib.bib77)]。许多研究提议将这种对比机制直接融入模型架构中 [[20](#bib.bib20)]，或融入目标函数中 [[20](#bib.bib20),
    [19](#bib.bib19), [21](#bib.bib21), [17](#bib.bib17)]。张等人 [[20](#bib.bib20)] 通过用鼓励对比效应的NCE损失变体替代交叉熵，提出了一种替代监督训练设置的方法。作者主张将解决RPMs的视角从分类任务转变为排序任务，其中答案面板根据其正确完成矩阵的概率进行排序。基于NCE的目标函数在以下工作中进一步使用。
- en: Hu et al. [[17](#bib.bib17)] employ a hierarchical model to generate embeddings
    of all possible pairs of RPM rows (and optionally columns). In effect, an embedding
    of the first two RPM rows (a so-called dominant embedding) is obtained, as well
    as embeddings of pairs of rows containing the last row completed by one of the
    answer panels (let us call it the candidate row pair embeddings). Then, an NCE-inspired
    $(N+1)$-tuplet loss [[78](#bib.bib78)] is used to maximise the similarity of the
    dominant embedding to the candidate row pair embeddings completed by the correct
    answer. At the same time, the loss function minimises the similarity of the dominant
    embedding to the candidate row pair embeddings completed by each of the wrong
    answers.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 胡等人 [[17](#bib.bib17)] 采用分层模型生成所有可能的RPM行对（可选列）的嵌入。实际上，获得了前两行RPM的嵌入（即所谓的主导嵌入），以及包含由一个答案面板完成的最后一行的行对的嵌入（我们称之为候选行对嵌入）。然后，使用NCE启发的$(N+1)$-tuplet损失 [[78](#bib.bib78)]，最大化主导嵌入与由正确答案完成的候选行对嵌入的相似性。同时，损失函数最小化主导嵌入与由每个错误答案完成的候选行对嵌入的相似性。
- en: Another work casts the problem of solving RPMs into a multi-label classification
    framework and proposes the Multi-Label Contrastive Loss [[19](#bib.bib19)] – a
    contrastive objective function which builds on the Supervised Contrastive Loss [[79](#bib.bib79)].
    The authors propose a pre-training objective which builds similar representations
    to RPMs with common abstract structure and different representations for unrelated
    RPMs. In contrast to [[17](#bib.bib17)], the proposed method considers embeddings
    of the whole RPM context completed by an answer panel instead of the pairs of
    rows. Moreover, the method is combined with auxiliary training with sparse encoding
    into a joint learning method called Multi-Label Contrastive Learning (MLCL).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个研究将解决RPMs的问题转化为多标签分类框架，并提出了多标签对比损失 [[19](#bib.bib19)]——一种基于监督对比损失 [[79](#bib.bib79)]
    的对比目标函数。作者提出了一个预训练目标，建立了类似RPM的表示，这些表示具有共同的抽象结构，以及对不相关RPM的不同表示。与 [[17](#bib.bib17)]
    相比，所提出的方法考虑了由答案面板完成的整个RPM上下文的嵌入，而不是行对。此外，该方法与稀疏编码的辅助训练结合，形成了一种称为多标签对比学习（MLCL）的联合学习方法。
- en: 'Meta-Analogical Contrastive Learning [[21](#bib.bib21)], similarly to MLCL,
    improves the efficacy of learning relational representations of AVR tasks by maximising
    similarity between analogical structural relations and minimising similarity between
    non-analogical ones. The work defines multiple analogy types: 1) intra-problem
    analogy draws an analogy between the original RPM context panels and its perturbation
    obtained via replacing randomly selected panel with noise, 2) inter-problem analogy
    collates two RPM instances with the same abstract structures but different attribute
    values and 3) non-analogy randomly shuffles the RPM context panels in order to
    break the abstract structure.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**元类比对比学习**[[21](#bib.bib21)]类似于MLCL，通过最大化类比结构关系之间的相似性和最小化非类比关系之间的相似性，提高了学习AVR任务的关系表示的效果。该工作定义了多种类比类型：1）问题内部类比，通过用噪声替换随机选择的面板，类比原始RPM上下文面板及其扰动；2）问题间类比，将两个具有相同抽象结构但不同属性值的RPM实例进行比较；3）非类比，随机打乱RPM上下文面板，以打破抽象结构。'
- en: 3.4 Learning with an optimal trajectory
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 使用最优轨迹学习
- en: In [[22](#bib.bib22)] the authors recognise distracting features—image objects
    whose attributes change at random—as the main challenge in learning to solve RPMs.
    As a possible solution Feature Robust Abstract Reasoning (FRAR) [[22](#bib.bib22)]
    is proposed, which mitigates the impact of distracting features by means of a
    carefully designed learning trajectory based on a student-teacher reinforcement
    learning approach. Moreover, the paper conducts a wide study of optimal learning
    trajectory approaches including Curriculum learning [[80](#bib.bib80)], Self-paced
    learning [[81](#bib.bib81)], Learning to teach [[82](#bib.bib82)], Hard example
    mining [[83](#bib.bib83)], Focal loss [[84](#bib.bib84)] and Mentornet-PD [[85](#bib.bib85)].
    The experiments reveal that learning to solve RPMs is most effective with the
    proposed FRAR method.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[22](#bib.bib22)]中，作者将分散注意力的特征——属性随机变化的图像对象——认定为解决RPM（图形推理问题）时的主要挑战。作为一种可能的解决方案，提出了**特征鲁棒抽象推理（FRAR）**[[22](#bib.bib22)]，该方法通过基于学生-教师强化学习方法精心设计的学习轨迹来减轻分散特征的影响。此外，论文广泛研究了包括课程学习[[80](#bib.bib80)]、自适应学习[[81](#bib.bib81)]、教学学习[[82](#bib.bib82)]、困难样本挖掘[[83](#bib.bib83)]、焦点损失[[84](#bib.bib84)]和Mentornet-PD[[85](#bib.bib85)]在内的最佳学习轨迹方法。实验结果表明，使用提出的FRAR方法解决RPM问题最为有效。
- en: '![Refer to caption](img/8356bf4cf7361816c759194ff4f0995e.png)![Refer to caption](img/70fe1d0661e08f6c144c3acb5d57e9fe.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8356bf4cf7361816c759194ff4f0995e.png)![参见说明](img/70fe1d0661e08f6c144c3acb5d57e9fe.png)'
- en: 'Figure 5: RPM augmentation. The first row presents single rows from two I-RAVEN
    matrices with configurations 2x2Grid (left) and 3x3Grid (right). Rows 2–4 demonstrate
    image-level augmentations used in [[19](#bib.bib19)], whereas the last row illustrates
    structural perturbations applied in [[21](#bib.bib21)].'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：RPM增强。第一行展示了两个I-RAVEN矩阵中单行的配置，2x2Grid（左）和3x3Grid（右）。第2至第4行演示了在[[19](#bib.bib19)]中使用的图像级增强，而最后一行则展示了在[[21](#bib.bib21)]中应用的结构性扰动。
- en: 3.5 Data augmentation
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 数据增强
- en: 'Data augmentation methods were shown to be of critical importance in applying
    DL across diverse domains [[86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88),
    [89](#bib.bib89)]. Similar conclusions were drawn in the field of AVR, where RPM
    data augmentation have proven to enhance abstract reasoning capabilities of neural
    learning models [[19](#bib.bib19), [21](#bib.bib21)]. In [[19](#bib.bib19)] it
    is proposed to use simple image transformations including horizontal/vertical
    flip, horizontal/vertical roll, shuffle 2x2/3x3, rotation and transposition, as
    illustrated in Fig. [5](#S3.F5 "Figure 5 ‣ 3.4 Learning with an optimal trajectory
    ‣ 3 Learning to solve RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning:
    A Survey on Raven’s Progressive Matrices"), and apply them consistently to all
    panels of a given RPM. The authors have shown that data augmentation boosts the
    RPM solving performance irrespective of the chosen model and training setup. The
    topic of data augmentation was explored in parallel in [[21](#bib.bib21)], where
    the authors propose to shuffle the context panels of an RPM in order to break
    its abstract structure and use such modified instances as negative pairs in contrastive
    learning or to replace selected RPM panels with noise. These two approaches mainly
    differ in that [[19](#bib.bib19)] relies on image-level transformations, whereas [[21](#bib.bib21)]
    modifies the RPM structure by rearranging or replacing its panels.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '数据增强方法在将深度学习应用于不同领域中被证明具有关键重要性 [[86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88),
    [89](#bib.bib89)]。在 AVR 领域也得出了类似的结论，其中 RPM 数据增强被证明能提高神经学习模型的抽象推理能力 [[19](#bib.bib19),
    [21](#bib.bib21)]。在 [[19](#bib.bib19)] 中，建议使用简单的图像变换，包括水平/垂直翻转、水平/垂直滚动、2x2/3x3
    混排、旋转和转置，如图 [5](#S3.F5 "Figure 5 ‣ 3.4 Learning with an optimal trajectory ‣ 3
    Learning to solve RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning:
    A Survey on Raven’s Progressive Matrices") 所示，并将其一致应用于给定 RPM 的所有面板。作者们表明，无论选择的模型和训练设置如何，数据增强都能提升
    RPM 解决性能。数据增强的主题在 [[21](#bib.bib21)] 中也有并行探索，作者们建议打乱 RPM 的上下文面板，以打破其抽象结构，并将这些修改后的实例用作对比学习中的负对，或用噪声替换选定的
    RPM 面板。这两种方法主要的不同在于 [[19](#bib.bib19)] 依赖于图像级变换，而 [[21](#bib.bib21)] 通过重新排列或替换面板来修改
    RPM 结构。'
- en: 3.6 Disentangled representations
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 解耦表示
- en: Recent studies have shown that disentangled representations are helpful for
    abstract reasoning tasks and can improve sample-efficiency [[90](#bib.bib90)].
    Specifically, it was demonstrated that a perception backbone of WReN [[12](#bib.bib12)]
    pre-trained as a disentangled Variational Autoencoder ($\beta$-VAE) [[91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93)] leads to better generalisation capabilities
    on the downstream task of solving RPMs than the same architecture trained in a
    purely supervised manner [[23](#bib.bib23)]. Moreover, autoencoders were successfully
    utilised in [[15](#bib.bib15)] for learning transferable features in simple AVR
    tasks, whereas the authors of [[21](#bib.bib21)] incorporated an autoencoder as
    part of their architecture for creating so-called generative analogies.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最新研究表明，解耦表示对抽象推理任务非常有帮助，并且可以提高样本效率 [[90](#bib.bib90)]。具体来说，有研究表明，经过解耦变分自编码器（$\beta$-VAE） [[91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93)] 预训练的 WReN 感知骨干网络 [[12](#bib.bib12)] 在解决 RPM
    任务的下游任务上，比用纯监督方式训练的相同架构具有更好的泛化能力 [[23](#bib.bib23)]。此外，自编码器在 [[15](#bib.bib15)]
    中成功用于学习简单 AVR 任务中的可迁移特征，而 [[21](#bib.bib21)] 的作者将自编码器作为他们架构的一部分，用于创建所谓的生成类比。
- en: 3.7 Generative modeling
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 生成建模
- en: 'Another work [[24](#bib.bib24)] employs variational autoencoders for the purpose
    of training an effective generative model [[94](#bib.bib94), [95](#bib.bib95),
    [96](#bib.bib96)] capable of producing probable answer panels for RPMs. The method
    combines multiple components responsible for: 1) reconstruction of an answer image
    with a variational autoencoder, 2) predicting an index of the correct answer (supervised
    training) and the representation of abstract rules (auxiliary training), and 3)
    generating a new possible answer based on the latent embedding from VAE and the
    latent embedding of the recognition pathway. Such multi-task network was shown
    to be capable of generating plausible answer panels that preserve the underlying
    abstract rules and being competitive with models trained only with the supervised
    training.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个工作[[24](#bib.bib24)]采用变分自编码器来训练一个有效的生成模型[[94](#bib.bib94), [95](#bib.bib95),
    [96](#bib.bib96)]，该模型能够为RPM生成可能的答案面板。该方法结合了多个组件，负责：1）使用变分自编码器重建答案图像，2）预测正确答案的索引（监督训练）和抽象规则的表示（辅助训练），以及3）基于来自VAE的潜在嵌入和识别路径的潜在嵌入生成新的可能答案。这样的多任务网络被证明能够生成保留基本抽象规则的合理答案面板，并与仅用监督训练训练的模型具有竞争力。
- en: An alternative RPM answer generation approach was proposed in [[26](#bib.bib26)],
    where a deep latent variable model that utilises multiple Gaussian processes was
    constructed. The resultant method was shown to be interpretable via concept-specific
    latent variables and produced high-quality RPM panels. While both works [[24](#bib.bib24),
    [26](#bib.bib26)] train generative models using RPM panels, it was also demonstrated
    that an image generator pre-trained on real-world images may as well be effective
    in producing valid RPM panels [[25](#bib.bib25)].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[26](#bib.bib26)]中提出了一种替代的RPM答案生成方法，其中构建了一个利用多个高斯过程的深度潜变量模型。结果表明，该方法通过概念特定的潜变量具有可解释性，并生成了高质量的RPM面板。虽然两项工作[[24](#bib.bib24),
    [26](#bib.bib26)]都使用RPM面板训练生成模型，但也证明了在真实世界图像上预训练的图像生成器同样可以有效地生成有效的RPM面板[[25](#bib.bib25)]。
- en: Different from the above works, the Probabilistic Abduction and Execution (PrAE)
    model [[27](#bib.bib27)] is able to construct a probabilistic RPM representation
    and use it to generate answers to RPMs with a scene inference engine. In contrast
    to the previous end-to-end generative approaches, PrAE decouples the generation
    process into a neural perception backbone and a symbolic logical reasoning engine.
    In spite of this separation, the method can be optimised end-to-end with REINFORCE [[97](#bib.bib97)]
    and showcases the applicability of neuro-symbolic approaches to solving RPMs within
    a generative process.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于上述工作，**概率性推理和执行（PrAE）模型**[[27](#bib.bib27)]能够构建一个概率性RPM表示，并使用场景推理引擎来生成RPM的答案。与之前的端到端生成方法不同，PrAE将生成过程解耦为神经感知骨干和符号逻辑推理引擎。尽管存在这种分离，但该方法可以通过REINFORCE[[97](#bib.bib97)]进行端到端优化，并展示了神经符号方法在生成过程中解决RPM问题的适用性。
- en: 3.8 Unsupervised learning
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8 无监督学习
- en: A couple of recent approaches to solving RPMs investigate whether useful representations
    can be learned in an usupervised manner. In this setup the model does not take
    into account information about the correct answer in the training process.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最近的RPM解决方法研究了是否可以以无监督的方式学习有用的表示。在这种设置中，模型在训练过程中不考虑关于正确答案的信息。
- en: Geared towards obtaining an effective RPM solving model in the unsupervised
    setting, Noisy Contrast and Decentralization (NCD) [[30](#bib.bib30)] considers
    10 rows obtained from each RPM (first and second rows together with the third
    row completed by each of the 8 answer panels). Next the method considers a binary
    classification task in which the model predicts which rows belong to RPM context
    and which are formed by completing the third row with one of the answers. During
    inference, NCD generates probabilities only for the 8 versions of the last row
    and outputs an index of the highest scoring candidate as the answer. The method
    builds upon Multi-label Classification with Pseudo Targets (MCPT) [[28](#bib.bib28)]
    proposed by the authors of NCD in their earlier work.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 针对在无监督环境中获得有效的RPM解决模型，噪声对比和去中心化（NCD）[[30](#bib.bib30)]考虑了从每个RPM获得的10行（第一和第二行以及由8个答案面板完成的第三行）。接下来，该方法考虑一个二分类任务，其中模型预测哪些行属于RPM上下文，哪些是通过用一个答案填充第三行形成的。在推理过程中，NCD仅为最后一行的8个版本生成概率，并将得分最高的候选项的索引作为答案。该方法基于NCD作者在早期工作中提出的伪目标多标签分类（MCPT）[[28](#bib.bib28)]。
- en: Another unsupervised approach, the Pairwise Relations Discriminator (PRD), is
    proposed in [[29](#bib.bib29)]. The method trains an underlying model to solve
    the task of discriminating between positive and negative pairs, similarly to other
    contrastive approaches (e.g. [[17](#bib.bib17)]). Positive pairs are formed by
    taking the first two rows from a given RPM, say $\mathcal{P}$. The negative pairs
    are obtained in several ways. First option is to pair one of the the first two
    rows from $\mathcal{P}$ with another row from a different RPM. Alternatively,
    the method randomly selects the first or the second row as $r$ and considers the
    other one as $r^{\prime}$. Then, the third panel of $r^{\prime}$ is replaced by
    one of the answers to form a negative pair with $r$. Analogously, the third row
    may be taken with its third panel replaced by the third image from $r^{\prime}$
    and considered a negative pair with $r$. Next, PRD maximises the similarity of
    elements within positive pairs and minimises the similarity of items within the
    negative pairs. To obtain an answer for $\mathcal{P}$, the method fills in the
    third row with each answer and calculates the average similarity between the completed
    row and the first two rows. The choice panel corresponding to the completed row
    with the highest similarity is considered as the answer.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种无监督的方法，即成对关系鉴别器（PRD），在[[29](#bib.bib29)]中提出。该方法训练一个基础模型来解决正负对的区分任务，类似于其他对比方法（例如[[17](#bib.bib17)]）。正对是通过从给定的RPM中取前两行形成的，假设为$\mathcal{P}$。负对则通过多种方式获得。第一种选择是将$\mathcal{P}$的前两行之一与来自不同RPM的另一行配对。另一种选择是随机选择第一或第二行作为$r$，并将另一行视为$r^{\prime}$。然后，$r^{\prime}$的第三面板被一个答案替换，从而形成一个与$r$的负对。类似地，可以将第三行与其第三面板替换为$r^{\prime}$的第三张图像，视为与$r$的负对。接下来，PRD最大化正对内元素的相似性，最小化负对内项的相似性。为了获得$\mathcal{P}$的答案，该方法用每个答案填充第三行，并计算完成的行与前两行之间的平均相似性。与具有最高相似性的完成行对应的选择面板被视为答案。
- en: 4 RPM Deep Learning Models
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4个RPM深度学习模型
- en: '![Refer to caption](img/c5c8cd165ab546077ba0c258df8f242d.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c5c8cd165ab546077ba0c258df8f242d.png)'
- en: 'Figure 6: RPM-like problem from [[11](#bib.bib11)]. The task verifies the ability
    to identify progression rule applied to triangle rotation. Correct answer is highlighted
    with a green bounding box.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：来自[[11](#bib.bib11)]的类似RPM问题。任务验证了识别应用于三角形旋转的进展规则的能力。正确答案用绿色边框突出显示。
- en: 'One of the first DL models for solving RPMs was proposed by Hoshen and Werman
    [[11](#bib.bib11)] who constructed a convolutional neural network (CNN) [[98](#bib.bib98)]
    for solving geometric pattern recognition problems, capable of generating images
    according to a specified pattern. Moreover, an additional network based on the
    same visual backbone was employed to solve problems involving rotation, reflection,
    color, size and shape of the patterns (see Fig. [6](#S4.F6 "Figure 6 ‣ 4 RPM Deep
    Learning Models ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey
    on Raven’s Progressive Matrices")). CNNs were further utilised in [[99](#bib.bib99)],
    where a generalised similarity-based approach to solving RPMs from the Sandia
    suite [[13](#bib.bib13)] (cf. Fig. [4a](#S2.F4.sf1 "In Figure 4 ‣ 2.2 Automatic
    generation of RPMs ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning Methods for
    Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")) was presented.
    The method eliminates the need for structure mapping and instead relies on feature-based
    processing using relational and non-relational features.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 解决RPMs的第一个深度学习模型之一是由Hoshen和Werman [[11](#bib.bib11)] 提出的，他们构建了一个卷积神经网络（CNN）[[98](#bib.bib98)]，用于解决几何图案识别问题，能够根据指定图案生成图像。此外，还使用了基于相同视觉骨干的额外网络来解决涉及图案的旋转、反射、颜色、大小和形状的问题（见图 [6](#S4.F6
    "图6 ‣ 4 RPM深度学习模型 ‣ 用于抽象视觉推理的深度学习方法：对Raven的渐进矩阵的调查")）。CNNs在[[99](#bib.bib99)]中进一步应用，其中介绍了一种基于相似性的通用方法来解决Sandia套件中的RPMs
    [[13](#bib.bib13)]（参见图 [4a](#S2.F4.sf1 "图4 ‣ 2.2 自动生成RPMs ‣ 2 Raven的渐进矩阵 ‣ 用于抽象视觉推理的深度学习方法：对Raven的渐进矩阵的调查")）。该方法消除了结构映射的需要，而是依赖于使用关系和非关系特征的基于特征的处理。
- en: 'Another feature-centric approach is discussed in [[15](#bib.bib15)], where
    an autoencoder with dense fully-connected layers is first trained to learn feature-based
    representations and then combined with an ensemble of shallow multilayer perceptrons
    (MLPs). On top, a scoring module is proposed that can be adjusted to the final
    downstream task. Mańdziuk and Żychowski [[15](#bib.bib15)] have shown that this
    approach facilitates transfer learning, by applying the model trained to solve
    RPMs (cf. Fig. [4c](#S2.F4.sf3 "In Figure 4 ‣ 2.2 Automatic generation of RPMs
    ‣ 2 Raven’s Progressive Matrices ‣ Deep Learning Methods for Abstract Visual Reasoning:
    A Survey on Raven’s Progressive Matrices")) to other problems with a similar input
    distribution, e.g. the odd-one-out tasks (see Fig. [7](#S4.F7 "Figure 7 ‣ 4 RPM
    Deep Learning Models ‣ Deep Learning Methods for Abstract Visual Reasoning: A
    Survey on Raven’s Progressive Matrices")). Although the above works have demonstrated
    that classic neural network architectures are capable of possessing abstract visual
    reasoning skills, the approaches were evaluated on visually-simple RPM benchmarks
    with limited possibility to verify out-of-distribution generalisation.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种以特征为中心的方法在[[15](#bib.bib15)]中讨论，其中一个具有稠密全连接层的自编码器首先被训练以学习基于特征的表示，然后与一个浅层多层感知机（MLPs）的集成结合。在此基础上，提出了一个可以根据最终下游任务进行调整的评分模块。Mańdziuk和Żychowski
    [[15](#bib.bib15)] 已经证明，这种方法通过将训练解决RPMs的模型（参见 图 [4c](#S2.F4.sf3 "图4 ‣ 2.2 自动生成RPMs
    ‣ 2 Raven的渐进矩阵 ‣ 用于抽象视觉推理的深度学习方法：对Raven的渐进矩阵的调查")）应用于具有类似输入分布的其他问题（如奇异项任务，见图 [7](#S4.F7
    "图7 ‣ 4 RPM深度学习模型 ‣ 用于抽象视觉推理的深度学习方法：对Raven的渐进矩阵的调查")），有助于迁移学习。尽管上述工作已经证明经典神经网络架构能够具备抽象视觉推理技能，但这些方法是在视觉上简单的RPM基准上评估的，验证分布外泛化的可能性有限。
- en: '![Refer to caption](img/d080ab0c98d3cfcd82ede1ef6fd81f34.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d080ab0c98d3cfcd82ede1ef6fd81f34.png)'
- en: 'Figure 7: Odd-one-out matrix. Example of an odd-one-out problem from [[15](#bib.bib15)].
    The task is to point the mismatched element out of several choices. In the example
    it is a triangle (all others are trapezoids) - marked with a red boundary.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：奇异项矩阵。来自[[15](#bib.bib15)]的奇异项问题示例。任务是指出几个选项中的不匹配元素。在示例中，这是一个三角形（其他都是梯形）-
    用红色边界标记。
- en: 'In the following sections we focus on consecutive works that considered RPMs
    with compositional structure and tackled o.o.d. generalisation challenges. A high-level
    overview of DL models proposed in these works is presented in Table [I](#S4.T1
    "TABLE I ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods for Abstract Visual
    Reasoning: A Survey on Raven’s Progressive Matrices"). Besides baseline methods,
    we categorise the followup models into two classes: relational reasoning networks
    and hierarchical networks. Models of the first type are inherently based on the
    Relation Network [[74](#bib.bib74)], whereas those of the second type, inspired
    by the hierarchical nature of RPMs, inject various structural inductive biases
    into network architectures used to solve RPMs.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '在接下来的章节中，我们将重点讨论考虑了具有组合结构的RPM并解决了o.o.d.泛化挑战的连续研究。这些研究中提出的DL模型的高级概述展示在表格 [I](#S4.T1
    "TABLE I ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods for Abstract Visual
    Reasoning: A Survey on Raven’s Progressive Matrices")中。除了基线方法，我们将后续模型分为两类：关系推理网络和层次网络。第一类模型本质上基于关系网络 [[74](#bib.bib74)]，而第二类模型受到RPM层次结构的启发，将各种结构性归纳偏置注入用于解决RPM的网络架构中。'
- en: 'TABLE I: Model overviews. A summary of key design choices adopted in RPM solving
    models. *Dataset* column shows the works in which a given (model, dataset) pair
    was included in the experiments. In the *Training setup* column, *Target cross-entropy
    loss* refers to the supervised training approaches, whereas *Meta-target binary
    cross-entropy loss* refers to the auxiliary training settings.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：模型概述。对RPM求解模型中采用的关键设计选择的总结。*数据集*列展示了给定（模型，数据集）对包含在实验中的工作。在*训练设置*列中，*目标交叉熵损失*指的是监督训练方法，而*元目标二元交叉熵损失*指的是辅助训练设置。
- en: '| Model | Dataset | Input | Design highlight | Training setup |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 数据集 | 输入 | 设计亮点 | 训练设置 |'
- en: '| Baselines [[12](#bib.bib12), [16](#bib.bib16)] | PGM [[12](#bib.bib12)] RAVEN [[16](#bib.bib16)]
    I-RAVEN [[17](#bib.bib17)] | Single panels | Embeds single panels with a shallow
    CNN or ResNet-18 and aggregates information from RPM context panels with an MLP
    or LSTM. | Target cross-entropy loss Meta-target binary cross-entropy loss |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 基线 [[12](#bib.bib12), [16](#bib.bib16)] | PGM [[12](#bib.bib12)] RAVEN [[16](#bib.bib16)]
    I-RAVEN [[17](#bib.bib17)] | 单面板 | 用浅层CNN或ResNet-18嵌入单面板，并用MLP或LSTM聚合RPM上下文面板的信息。
    | 目标交叉熵损失 元目标二元交叉熵损失 |'
- en: '| Context-blind models [[12](#bib.bib12)] | PGM [[12](#bib.bib12)] RAVEN [[33](#bib.bib33),
    [17](#bib.bib17)] I-RAVEN [[33](#bib.bib33), [17](#bib.bib17)] | Single panels
    | Reason only about answer panels and do not rely on RPM context. | Target cross-entropy
    loss |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 上下文盲模型 [[12](#bib.bib12)] | PGM [[12](#bib.bib12)] RAVEN [[33](#bib.bib33),
    [17](#bib.bib17)] I-RAVEN [[33](#bib.bib33), [17](#bib.bib17)] | 单面板 | 仅对答案面板进行推理，不依赖RPM上下文。
    | 目标交叉熵损失 |'
- en: '| Wild ResNet [[12](#bib.bib12)] | PGM [[12](#bib.bib12)] I-RAVEN [[17](#bib.bib17)]
    | Context | Uses ResNet on a stack of 9 context panels. | Target cross-entropy
    loss |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Wild ResNet [[12](#bib.bib12)] | PGM [[12](#bib.bib12)] I-RAVEN [[17](#bib.bib17)]
    | 上下文 | 在9个上下文面板堆栈上使用ResNet。 | 目标交叉熵损失 |'
- en: '| WReN [[12](#bib.bib12)] | PGM [[12](#bib.bib12)] RAVEN [[16](#bib.bib16)]
    I-RAVEN [[17](#bib.bib17)] | Single panels | Aggregates information from pairs
    of RPM context panels with the RN. | Target cross-entropy loss Meta-target binary
    cross-entropy loss |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| WReN [[12](#bib.bib12)] | PGM [[12](#bib.bib12)] RAVEN [[16](#bib.bib16)]
    I-RAVEN [[17](#bib.bib17)] | 单面板 | 与RN聚合RPM上下文面板的信息。 | 目标交叉熵损失 元目标二元交叉熵损失 |'
- en: '| VAE-WReN [[23](#bib.bib23)] | PGM [[12](#bib.bib12)] | Single panels | Replaces
    the CNN panel encoder of WReN with a disentangled $\beta$-VAE trained separately
    from the WReN model. | Target cross-entropy loss Meta-target binary cross-entropy
    loss Modified ELBO objective [[93](#bib.bib93)] |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| VAE-WReN [[23](#bib.bib23)] | PGM [[12](#bib.bib12)] | 单面板 | 用与WReN模型分开训练的解缠$\beta$-VAE替代WReN的CNN面板编码器。
    | 目标交叉熵损失 元目标二元交叉熵损失 修改的ELBO目标 [[93](#bib.bib93)] |'
- en: '| ARNe [[31](#bib.bib31)] | PGM [[31](#bib.bib31)] RAVEN [[31](#bib.bib31)]
    | Single panels | Extends the WReN model with the Transformer located between
    panel embedding component and the RN module. | Target cross-entropy loss Meta-target
    binary cross-entropy loss |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| ARNe [[31](#bib.bib31)] | PGM [[31](#bib.bib31)] RAVEN [[31](#bib.bib31)]
    | 单面板 | 在面板嵌入组件和RN模块之间扩展WReN模型，加入了Transformer。 | 目标交叉熵损失 元目标二元交叉熵损失 |'
- en: '| CoPINet [[20](#bib.bib20)] | PGM [[20](#bib.bib20)] RAVEN [[20](#bib.bib20)]
    I-RAVEN [[17](#bib.bib17)] | Single panels | Introduces an explicit permutation-invariant
    contrasting neural module for distinguishing features between answer panels. |
    Variant of NCE loss [[20](#bib.bib20)] |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| CoPINet [[20](#bib.bib20)] | PGM [[20](#bib.bib20)] RAVEN [[20](#bib.bib20)]
    I-RAVEN [[17](#bib.bib17)] | 单面板 | 引入了一个显式的置换不变对比神经模块，用于区分答案面板之间的特征。 | NCE 损失的变体 [[20](#bib.bib20)]
    |'
- en: '| LEN [[22](#bib.bib22)] | PGM [[22](#bib.bib22)] RAVEN [[22](#bib.bib22)]
    I-RAVEN [[17](#bib.bib17)] | Single panels, Context$\setminus a_{k}$ | Aggregates
    information from triples of RPM context panels and a context embedding vector
    with the RN. | Target cross-entropy loss Meta-target binary cross-entropy loss
    Feature Robust Abstract Reasoning |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| LEN [[22](#bib.bib22)] | PGM [[22](#bib.bib22)] RAVEN [[22](#bib.bib22)]
    I-RAVEN [[17](#bib.bib17)] | 单面板，背景$\setminus a_{k}$ | 从 RPM 背景面板的三元组和上下文嵌入向量中聚合信息与
    RN。 | 目标交叉熵损失 元目标二元交叉熵损失 特征鲁棒抽象推理 |'
- en: '| SRAN [[17](#bib.bib17)] | PGM [[17](#bib.bib17)] I-RAVEN [[17](#bib.bib17)]
    | Single panels, Rows, Cols, Pairs of rows Pairs of cols | Gradually aggregates
    features from panel hierarchies with a gated embedding fusion module. | ($N$+$1$)-tuplet
    loss [[78](#bib.bib78)] |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| SRAN [[17](#bib.bib17)] | PGM [[17](#bib.bib17)] I-RAVEN [[17](#bib.bib17)]
    | 单面板，行，列，行对 列对 | 使用带门嵌入融合模块逐步聚合来自面板层次结构的特征。 | ($N$+$1$)-元组损失 [[78](#bib.bib78)]
    |'
- en: '| MXGNet [[36](#bib.bib36)] | PGM [[36](#bib.bib36)] RAVEN [[36](#bib.bib36)]
    | Single panels | Reasons about inter-panel relations with multiplex graph neural
    networks. | Target cross-entropy loss Meta-target binary cross-entropy loss |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| MXGNet [[36](#bib.bib36)] | PGM [[36](#bib.bib36)] RAVEN [[36](#bib.bib36)]
    | 单面板 | 使用多重图神经网络对面板之间的关系进行推理。 | 目标交叉熵损失 元目标二元交叉熵损失 |'
- en: '| SCL [[33](#bib.bib33)] | PGM [[33](#bib.bib33), [19](#bib.bib19)] RAVEN [[33](#bib.bib33)]
    I-RAVEN [[33](#bib.bib33), [19](#bib.bib19)] | Single panels | Reasons about inter-panel
    and intra-panel relations with a neural module based on a scattering transformation.
    | Target cross-entropy loss |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| SCL [[33](#bib.bib33)] | PGM [[33](#bib.bib33), [19](#bib.bib19)] RAVEN [[33](#bib.bib33)]
    I-RAVEN [[33](#bib.bib33), [19](#bib.bib19)] | 单面板 | 使用基于散射变换的神经模块对面板间和面板内关系进行推理。
    | 目标交叉熵损失 |'
- en: '| DCNet [[35](#bib.bib35)] | PGM [[35](#bib.bib35)] RAVEN [[35](#bib.bib35)]
    | Rows Cols | Considers similarity of third row/col to the first and second ones
    and differences in the last row completed by each answer panel. | Target binary
    cross-entropy loss |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| DCNet [[35](#bib.bib35)] | PGM [[35](#bib.bib35)] RAVEN [[35](#bib.bib35)]
    | 行 列 | 考虑了第三行/列与第一和第二行的相似性以及每个答案面板完成的最后一行的差异。 | 目标二元交叉熵损失 |'
- en: '| MRNet [[18](#bib.bib18)] | PGM [[18](#bib.bib18)] RAVEN [[18](#bib.bib18)]
    RAVEN-FAIR [[18](#bib.bib18)] | Single panels | Applies separate RNs to panel
    embeddings in three different resolutions. | Weighted target binary cross-entropy
    loss |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| MRNet [[18](#bib.bib18)] | PGM [[18](#bib.bib18)] RAVEN [[18](#bib.bib18)]
    RAVEN-FAIR [[18](#bib.bib18)] | 单面板 | 对三种不同分辨率的面板嵌入应用单独的 RN。 | 加权目标二元交叉熵损失 |'
- en: '| Rel-Base [[34](#bib.bib34)] | PGM [[34](#bib.bib34)] RAVEN [[34](#bib.bib34)]
    | Single panels | Processes a stack of 9 context panel embeddings with a 1D convolution
    module. | Target cross-entropy loss |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Rel-Base [[34](#bib.bib34)] | PGM [[34](#bib.bib34)] RAVEN [[34](#bib.bib34)]
    | 单面板 | 使用 1D 卷积模块处理 9 个背景面板嵌入的堆栈。 | 目标交叉熵损失 |'
- en: '| Rel-AIR [[34](#bib.bib34)] | PGM [[34](#bib.bib34)] RAVEN [[34](#bib.bib34)]
    | Single panels | Reasons about segmented objects obtained with AIR unsupervised
    scene decomposition model. | Target cross-entropy loss Unsupervised scene decomposition
    pre-training |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Rel-AIR [[34](#bib.bib34)] | PGM [[34](#bib.bib34)] RAVEN [[34](#bib.bib34)]
    | 单面板 | 对使用 AIR 无监督场景分解模型获得的分割对象进行推理。 | 目标交叉熵损失 无监督场景分解预训练 |'
- en: '| MLRN [[32](#bib.bib32)] | PGM [[32](#bib.bib32)] | Single panels | Extends
    WReN with a Multi-Layer Relation Network, encodes the input images with a Magnitude
    Encoding and uses the LAMB optimiser. | Target cross-entropy loss |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| MLRN [[32](#bib.bib32)] | PGM [[32](#bib.bib32)] | 单面板 | 扩展了 WReN，使用多层关系网络，利用幅度编码对输入图像进行编码，并使用
    LAMB 优化器。 | 目标交叉熵损失 |'
- en: '| PrAE [[27](#bib.bib27)] | RAVEN [[27](#bib.bib27)] | Single panels | Neuro-symbolic
    approach that produces a probabilistic RPM representation and uses it to generate
    probable answer via probabilistic abduction and execution. | REINFORCE [[97](#bib.bib97)]
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| PrAE [[27](#bib.bib27)] | RAVEN [[27](#bib.bib27)] | 单面板 | 神经符号方法生成概率 RPM
    表示，并通过概率演绎和执行生成可能的答案。 | REINFORCE [[97](#bib.bib97)] |'
- en: '| NI [[37](#bib.bib37)] | PGM [[37](#bib.bib37)] | Single panels | Compositional
    model comprising reusable self-attention layers with a learnable routing mechanism.
    | Target cross-entropy loss |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| NI [[37](#bib.bib37)] | PGM [[37](#bib.bib37)] | 单面板 | 由可重用自注意力层组成的组合模型，具有可学习的路由机制。
    | 目标交叉熵损失 |'
- en: 4.1 Baselines
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基线
- en: Similarly to [[11](#bib.bib11)], baseline models for solving RPMs are formed
    by using a CNN as a visual feature extractor, followed by a reasoning module.
    The perceptual backbone often consists of 4 convolution layers with non-linearities
    in-between or a variant of ResNet [[100](#bib.bib100)] with 18, 34 or 50 layers.
    In simple baseline models [[12](#bib.bib12), [16](#bib.bib16)], the visual component
    processes each image independently. Then, the features extracted by these visual
    modules from all matrix panels are either concatenated into a single vector and
    fed into an MLP or stacked to form a time-series and processed by an LSTM [[101](#bib.bib101)].
    Nonetheless, it was shown that such typical neural architectures struggle even
    in simpler RPM generalisation regimes [[12](#bib.bib12), [16](#bib.bib16)], which
    raises the need for dedicated AVR models.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[[11](#bib.bib11)]，解决RPM的基线模型通过使用CNN作为视觉特征提取器，然后是推理模块来形成。感知骨架通常由4个卷积层和其中的非线性层组成，或者是具有18、34或50层的ResNet变体[[100](#bib.bib100)]。在简单的基线模型[[12](#bib.bib12),
    [16](#bib.bib16)]中，视觉组件独立处理每个图像。然后，这些视觉模块从所有矩阵面板提取的特征要么被连接成一个单一的向量并输入到MLP中，要么堆叠成时间序列并由LSTM[[101](#bib.bib101)]处理。然而，研究表明，这种典型的神经架构即使在更简单的RPM泛化条件下也会遇到困难[[12](#bib.bib12),
    [16](#bib.bib16)]，这突显了对专门AVR模型的需求。
- en: '![Refer to caption](img/9f30012dab1e15ee5ac64f6d984b6f03.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9f30012dab1e15ee5ac64f6d984b6f03.png)'
- en: (a) Single panel.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 单面板。
- en: '![Refer to caption](img/da40cab5a2acd1d81528308bbcd5fc0e.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/da40cab5a2acd1d81528308bbcd5fc0e.png)'
- en: (b) Single row.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 单行。
- en: '![Refer to caption](img/a561bcd122574af54333140ada56dec0.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a561bcd122574af54333140ada56dec0.png)'
- en: (c) Single col.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 单列。
- en: '![Refer to caption](img/2757597a7796fafb57c55092f7ec7939.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2757597a7796fafb57c55092f7ec7939.png)'
- en: (d) Pair of rows.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 一对行。
- en: '![Refer to caption](img/59c1fd0f110b4c70ca83f21d9eb89912.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/59c1fd0f110b4c70ca83f21d9eb89912.png)'
- en: (e) Pair of Cols.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 一对列。
- en: '![Refer to caption](img/54191e4be7499a07703e34147fea2134.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/54191e4be7499a07703e34147fea2134.png)'
- en: (f) Context$\setminus a_{k}$.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Context$\setminus a_{k}$。
- en: '![Refer to caption](img/1a48a7f6f369bf30693b726614309294.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1a48a7f6f369bf30693b726614309294.png)'
- en: (g) Context.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 上下文。
- en: 'Figure 8: RPM panel hierarchies. Each hierarchy demonstrates how panels can
    be jointly processed. For instance, when building hierarchical perceptual model
    backbones (e.g. in SRAN [[17](#bib.bib17)]), panels are stacked on top of each
    other to form a matrix of the shape $(c\times w\times h)$, where $c$ is the number
    of panels in the given hierarchy, $w$ - the image width and $h$ - the image height.
    Alternatively, the embeddings of these panels can be combined in analogous manner
    (e.g. in LEN [[22](#bib.bib22)]) and concatenated into a single vector – a representation
    of given hierarchy. The hierachy which contains 8 RPM context images without the
    remaining missing panel is denoted as “Context$\setminus a_{k}$”, whereas the
    hierarchy with full RPM context where one of the answers is placed in the bottom-right
    panel as “Context”. Additional hierarchies may include diagonal panels (e.g. [[15](#bib.bib15),
    [36](#bib.bib36)]) or random combinations (e.g. [[22](#bib.bib22), [36](#bib.bib36)]).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：RPM面板层次结构。每个层次结构展示了面板如何被联合处理。例如，在构建层次感知模型骨架（如在SRAN [[17](#bib.bib17)]中）时，面板被堆叠在一起，形成一个形状为$(c\times
    w\times h)$的矩阵，其中$c$是给定层次结构中的面板数量，$w$是图像宽度，$h$是图像高度。或者，这些面板的嵌入可以以类似的方式组合（例如，在LEN [[22](#bib.bib22)]中）并连接成一个单一的向量——给定层次结构的表示。包含8个RPM上下文图像但缺少其余面板的层次结构被称为“Context$\setminus
    a_{k}$”，而包含完整RPM上下文并且其中一个答案放在右下角面板的层次结构被称为“Context”。额外的层次结构可能包括对角线面板（例如[[15](#bib.bib15),
    [36](#bib.bib36)]）或随机组合（例如[[22](#bib.bib22), [36](#bib.bib36)]）。
- en: 4.2 Relational reasoning networks
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 关系推理网络
- en: Seminal works that introduced challenging RPM reasoning benchmarks (PGM [[12](#bib.bib12)]
    and RAVEN [[16](#bib.bib16)]) have shown that baseline DL models generally lack
    the relational reasoning capability that is inherent to the AVR domain. Similar
    observations were noted in other (non AVR) problems involving relational reasoning
    that tried to tackle both artificial [[102](#bib.bib102), [74](#bib.bib74)] and
    real-world [[103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106),
    [107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111)] challenges. With the aim of equipping neural modules with
    relational reasoning abilities, Santoro et al. [[74](#bib.bib74)] proposed the
    Relation Network (RN) – a simple neural module for relational reasoning. RN arranges
    a set of objects $O$ into pairs (where an object $o\in O$ can be generally represented
    by any vector) and summarises the whole set of objects into a single descriptive
    representation with
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 引入挑战性 RPM 推理基准（PGM [[12](#bib.bib12)] 和 RAVEN [[16](#bib.bib16)]) 的开创性工作表明，基线
    DL 模型通常缺乏 AVR 领域固有的关系推理能力。在其他涉及关系推理的问题中也观察到了类似的现象，这些问题尝试解决人工 [[102](#bib.bib102),
    [74](#bib.bib74)] 和现实世界 [[103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105),
    [106](#bib.bib106), [107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109),
    [110](#bib.bib110), [111](#bib.bib111)] 挑战。为了使神经模块具备关系推理能力，Santoro 等人 [[74](#bib.bib74)]
    提出了关系网络 (RN)——一个用于关系推理的简单神经模块。RN 将一组对象 $O$ 排列成对（其中一个对象 $o\in O$ 通常可以由任何向量表示），并将整个对象集总结为一个单一的描述性表示，其中
- en: '|  | $RN(O)=f_{\phi}(\sum_{i,j}g_{\theta}(o_{i},o_{j}))$ |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | $RN(O)=f_{\phi}(\sum_{i,j}g_{\theta}(o_{i},o_{j}))$ |  |'
- en: where $f_{\phi}$ and $g_{\theta}$ are functions typically implemented as MLPs.
    Santoro et al. [[74](#bib.bib74)] successfully applied RN to problems from diverse
    domains including VQA [[102](#bib.bib102)], text-based question answering [[104](#bib.bib104)]
    and reasoning about dynamic physical systems [[74](#bib.bib74)]. The module was
    later extended in several ways, e.g. to recurrent version capable of sequential
    relational reasoning [[112](#bib.bib112)], and its learning capacity was enhanced
    by stacking multiple layers [[113](#bib.bib113)]. As a follow-up to [[74](#bib.bib74)],
    in [[12](#bib.bib12)] the RN was integrated as part of an end-to-end architecture
    for solving RPMs – the Wild Relation Network (WReN). The model outperformed baselines
    by a significant margin and sparked interest within the AVR community in building
    models that employ RN.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f_{\phi}$ 和 $g_{\theta}$ 是通常实现为MLP的函数。Santoro 等人 [[74](#bib.bib74)] 成功地将
    RN 应用到了包括 VQA [[102](#bib.bib102)]、基于文本的问答 [[104](#bib.bib104)] 和动态物理系统推理 [[74](#bib.bib74)]
    等多种领域的问题中。该模块后来以多种方式进行了扩展，例如，扩展到能够进行序列关系推理的递归版本 [[112](#bib.bib112)]，并通过堆叠多层增强了其学习能力
    [[113](#bib.bib113)]。作为对 [[74](#bib.bib74)] 的后续工作，在 [[12](#bib.bib12)] 中，RN 被整合为解决
    RPM 的端到端架构的一部分——Wild Relation Network (WReN)。该模型显著超越了基线，并在 AVR 社区中引起了对构建使用 RN
    的模型的兴趣。
- en: Multiple attempts have been made to either extend the originally proposed WReN
    model or to incorporate the RN into another end-to-end architecture. Steenbrugge
    et al. [[23](#bib.bib23)] introduced the VAE-WReN, which replaced the CNN backbone
    of WReN with a disentangled variational autoencoder [[91](#bib.bib91), [92](#bib.bib92),
    [93](#bib.bib93)] and showed improved generalisation when tested on PGM. The RN
    part of WReN was further extended to a multi-layer version [[113](#bib.bib113)].
    The Multi-Layer Relation Network (MLRN) combined with $\ell_{2}$-regularization,
    Magnitude Encoding and the LAMB optimiser [[114](#bib.bib114)] presented close
    to perfect performance in neutral PGM regime, however, its performance was worse
    than that of VAE-WReN in other generalisation regimes, suggesting that the model
    was prone to overfitting.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 已经进行了多次尝试以扩展原始提出的 WReN 模型或将 RN 纳入到另一个端到端架构中。Steenbrugge 等人 [[23](#bib.bib23)]
    引入了 VAE-WReN，它将 WReN 的 CNN 主干替换为解耦变分自编码器 [[91](#bib.bib91), [92](#bib.bib92),
    [93](#bib.bib93)]，并在 PGM 测试中显示出改进的泛化性能。WReN 的 RN 部分进一步扩展到多层版本 [[113](#bib.bib113)]。结合
    $\ell_{2}$-正则化、Magnitude Encoding 和 LAMB 优化器 [[114](#bib.bib114)] 的 Multi-Layer
    Relation Network (MLRN) 在中性 PGM 领域表现接近完美，但在其他泛化领域的表现则不如 VAE-WReN，表明该模型容易过拟合。
- en: Inspired by advances in psychology, which suggest that attention mechanisms
    play a crucial role in human visual reasoning capabilities, the Attention Relation
    Network (ARNe) was proposed in [[31](#bib.bib31)]. ARNe builds on WReN and equips
    it with attention mechanism borrowed from the Transformer [[115](#bib.bib115)].
    However, the increased model complexity was not fully justified, as the model
    demonstrated small improvements on PGM and performed worse than baselines on RAVEN.
    Nonetheless, Hahne et al. [[31](#bib.bib31)] have shown that after increasing
    RAVEN dataset size 5-fold, ARNe performance increased substantially, which suggests
    that the proposed attention mechanism for solving AVR tasks is promising, despite
    being inefficient w.r.t. the sample size. Possible directions for future work
    in this area may involve further research on incorporating the attention mechanism,
    which is additionally motivated by the impressive performance of attention-based
    models in other domains, e.g. [[115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117),
    [118](#bib.bib118)].
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 受心理学进展的启发，研究表明注意力机制在人的视觉推理能力中发挥着关键作用，因此提出了注意力关系网络（ARNe） [[31](#bib.bib31)]。ARNe
    基于 WReN，并配备了借自 Transformer 的注意力机制 [[115](#bib.bib115)]。然而，模型复杂度的增加并未得到充分的理由，因为该模型在
    PGM 上的改进很小，并且在 RAVEN 上表现不如基线模型。尽管如此，Hahne 等人 [[31](#bib.bib31)] 证明，在将 RAVEN 数据集规模扩大
    5 倍后，ARNe 的性能显著提高，这表明尽管在样本规模方面效率不高，但用于解决 AVR 任务的注意力机制是有前景的。未来工作的可能方向可能涉及对注意力机制的进一步研究，这一点也受到注意力基础模型在其他领域表现优异的激励，例如
    [[115](#bib.bib115)，[116](#bib.bib116)，[117](#bib.bib117)，[118](#bib.bib118)]。
- en: In the original WReN proposal [[12](#bib.bib12)], the relational module operated
    on pairs of panel embeddings. However, as described in [[5](#bib.bib5)], the rules
    in RPMs are inherently applied row- or column-wise. Therefore, some attempts have
    been made to incorporate this structural bias in the RN module. Benny et al. [[18](#bib.bib18)]
    proposed the MRNet, a model which first generates panel embeddings in three different
    resolutions and then processes them with distinct RN modules that consider triples
    of embeddings from each RPM row and column, respectively. In a similar spirit, Zheng
    et al. [[22](#bib.bib22)] introduced the Logic Embedding Network (LEN) – a model
    with several improvements to WReN in the context of solving RPMs. Firstly, the
    approach arranges RPM context panel embeddings into triples instead of pairs.
    The triples include row- and column-wise combinations (similarly to [[18](#bib.bib18)],
    although, only a single resolution is considered) processed by a neural module
    $g$ with parameters $\theta_{1}$. In contrast to [[18](#bib.bib18)], LEN additionally
    processes the remaining possible panel embedding combinations with a parallel
    module of the same structure as $g$, but with different parameters $\theta_{2}$.
    Moreover, with each triple a representation of the whole RPM context is concatenated.
    This representation is obtained with a supplementary CNN that processes a stack
    of 8 context panels. This structural inductive bias allows to reason about the
    relations between multiple panels more accurately and forms a basis for a group
    of hierarchical models discussed in the following section.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的 WReN 提案中 [[12](#bib.bib12)]，关系模块对面板嵌入进行操作。然而，如 [[5](#bib.bib5)] 中所述，RPMs
    中的规则本质上是按行或列应用的。因此，一些尝试已经被提出以将这种结构偏差融入到 RN 模块中。Benny 等人 [[18](#bib.bib18)] 提出了
    MRNet，这是一种首先生成三种不同分辨率的面板嵌入，然后用不同的 RN 模块处理这些嵌入的三元组，每个模块分别考虑来自每个 RPM 行和列的嵌入。类似地，郑等人
    [[22](#bib.bib22)] 介绍了逻辑嵌入网络（LEN）——一种在解决 RPMs 上对 WReN 进行了几项改进的模型。首先，该方法将 RPM 上下文面板嵌入安排为三元组而非对。三元组包括按行和列组合（类似于
    [[18](#bib.bib18)]，不过只考虑单一分辨率），由带有参数 $\theta_{1}$ 的神经模块 $g$ 处理。与 [[18](#bib.bib18)]
    相比，LEN 还使用与 $g$ 相同结构但参数 $\theta_{2}$ 不同的并行模块处理剩余的可能面板嵌入组合。此外，每个三元组与整个 RPM 上下文的表示进行拼接。这种表示是通过一个附加的
    CNN 获得的，该 CNN 处理 8 个上下文面板的堆叠。这种结构性归纳偏差使得对多个面板之间关系的推理更加准确，并为下一节讨论的一组层次模型奠定了基础。
- en: At the intersection of relational reasoning and hierarchical networks, another
    model—the Contrastive Perceptual Inference Network (CoPINet) [[20](#bib.bib20)]—is
    located. It is a permutation-invariant approach with an explicit contrastive mechanism
    that helps to distinguish the correct RPM answer. CoPINet first extracts independent
    panel embeddings with a visual backbone and then iteratively applies the contrastive
    module capable of discovering features that differentiate among a set of choices.
    In contrast to RN that considers pairs of feature embeddings, CoPINet’s contrasting
    mechanism collates each object representation with an aggregated representation
    of all remaining objects. The initial object representation is obtained by summing
    outputs of the visual backbone for each image along RPM’s rows and columns, respectively.
    Next, these representations are iteratively refined using the contrast module.
    Although CoPINet was shown to possess impressive reasoning ability on matrices
    from RAVEN [[20](#bib.bib20)], it was later discovered that the model performs
    sub-par on the balanced version of this dataset (I-RAVEN) [[17](#bib.bib17)].
    Therefore, additional validation of CoPINet’s architecture in other AVR problems
    sets an interesting avenue for future work.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在关系推理和层级网络的交汇处，存在另一种模型——对比感知推理网络（CoPINet）[[20](#bib.bib20)]。这是一种具有显式对比机制的置换不变方法，有助于区分正确的RPM答案。CoPINet首先通过视觉主干提取独立的面板嵌入，然后迭代地应用对比模块，能够发现区分一组选择的特征。与考虑特征嵌入对的RN不同，CoPINet的对比机制将每个对象表示与所有剩余对象的汇总表示进行比较。初始对象表示通过对每张图像沿RPM的行和列分别求和视觉主干的输出获得。接下来，这些表示通过对比模块进行迭代优化。虽然CoPINet在RAVEN的矩阵上展示了令人印象深刻的推理能力[[20](#bib.bib20)]，但后来发现该模型在该数据集的平衡版本（I-RAVEN）上表现不佳[[17](#bib.bib17)]。因此，进一步验证CoPINet在其他AVR问题上的架构为未来工作提供了一个有趣的方向。
- en: 4.3 Hierarchical networks
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 层级网络
- en: 'In multiple areas where DL models thrive, it has often been beneficial to incorporate
    domain-specific knowledge about the problem structure into the network architecture.
    Notable examples include making CNNs translation-invariant in problems with 2D
    images [[119](#bib.bib119)] or rotation-equivariant for spherical images [[120](#bib.bib120)].
    As demonstrated by the LEN model [[22](#bib.bib22)], such inductive biases are
    also helpful in solving RPMs, e.g. by specifying in what manner panel representations
    should be processed. Figure [8](#S4.F8 "Figure 8 ‣ 4.1 Baselines ‣ 4 RPM Deep
    Learning Models ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey
    on Raven’s Progressive Matrices") summarises the most common hierarchies utilised
    by RPM DL models. While LEN exploited a single row (Fig. [8b](#S4.F8.sf2 "In Figure
    8 ‣ 4.1 Baselines ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices")) and a single column
    (Fig. [8c](#S4.F8.sf3 "In Figure 8 ‣ 4.1 Baselines ‣ 4 RPM Deep Learning Models
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices")) hierarchies, respectively together with the 8 context panels (Fig. [8f](#S4.F8.sf6
    "In Figure 8 ‣ 4.1 Baselines ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")), subsequent
    DL approaches relied on additional techniques.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '在深度学习模型表现突出的多个领域，将特定领域的知识融入网络架构往往是有益的。显著的例子包括使CNN在2D图像问题中具有平移不变性[[119](#bib.bib119)]，或使其在球面图像中具有旋转等变性[[120](#bib.bib120)]。如LEN模型[[22](#bib.bib22)]所示，这种归纳偏差在解决RPM问题时也很有帮助，例如通过指定面板表示应该如何处理。图[8](#S4.F8
    "Figure 8 ‣ 4.1 Baselines ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")总结了RPM深度学习模型最常用的层级结构。LEN分别利用了单行（图[8b](#S4.F8.sf2
    "In Figure 8 ‣ 4.1 Baselines ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")）和单列（图[8c](#S4.F8.sf3
    "In Figure 8 ‣ 4.1 Baselines ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")）层级结构以及8个上下文面板（图[8f](#S4.F8.sf6
    "In Figure 8 ‣ 4.1 Baselines ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")），而后续的深度学习方法依赖于额外的技术。'
- en: 'While solving RPMs, it is often required not only to identify a set of rules
    that govern a single row/column but also to subsequently find an analogous set
    of rules applied to another row/column from the same matrix. Motivated by this
    observation, the Stratified Rule-Aware Network (SRAN) [[17](#bib.bib17)] devotes
    particular attention to pairs of rows (Fig. [8d](#S4.F8.sf4 "In Figure 8 ‣ 4.1
    Baselines ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods for Abstract Visual
    Reasoning: A Survey on Raven’s Progressive Matrices")) and pairs of columns (Fig. [8e](#S4.F8.sf5
    "In Figure 8 ‣ 4.1 Baselines ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")). From
    each such pair that includes 6 images, the panels are stacked on top of each other
    and processed by a dedicated convolutional pathway. SRAN gradually aggregates
    representations from consecutive hierarchies (single panel, single row/col, pair
    of rows/cols) using a gated embedding fusion module realised by an MLP.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决RPM问题时，通常不仅需要识别一个单独行/列的规则集，还要随之找到应用于同一矩阵中另一行/列的类似规则集。受此观察启发，分层规则感知网络（SRAN）[[17](#bib.bib17)]特别关注行对（图[8d](#S4.F8.sf4
    "在图8 ‣ 4.1 基准 ‣ 4 RPM 深度学习模型 ‣ 抽象视觉推理的深度学习方法：Raven进阶矩阵的调查")）和列对（图[8e](#S4.F8.sf5
    "在图8 ‣ 4.1 基准 ‣ 4 RPM 深度学习模型 ‣ 抽象视觉推理的深度学习方法：Raven进阶矩阵的调查")）。从每对包含6张图像的对中，面板被堆叠在一起，并通过专用的卷积路径处理。SRAN使用通过MLP实现的门控嵌入融合模块，逐渐聚合来自连续层次（单个面板、单行/列、行/列对）的表示。
- en: Such a gradual processing of latent features was also shown to be beneficial
    in the Scattering Compositional Learner (SCL) model [[33](#bib.bib33)]. Similarly
    to SRAN, SCL first computes embeddings for each RPM panel. Then, the matrix is
    iteratively completed by one of the answer panels. In each step, the model computes
    a joint representation of all 9 context images. The obtained embedding is finally
    fed to a scoring module which produces a probablity of correctness of the selected
    answer. In contrast to SRAN, SCL replaces MLP components with a scattering transformation
    that splits the input into multiple groups, applies the same neural module to
    each group, and merges the outputs into a single embedding. The approach shares
    some ideas with group convolution [[121](#bib.bib121)], ResNeXt [[122](#bib.bib122)]
    and Modular Networks [[123](#bib.bib123)].
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这种逐渐处理潜在特征的方法在散射组合学习器（SCL）模型[[33](#bib.bib33)]中也被证明是有益的。与SRAN类似，SCL首先为每个RPM面板计算嵌入。然后，矩阵通过一个答案面板进行迭代补全。在每一步，模型计算所有9张上下文图像的联合表示。得到的嵌入最终被输入到评分模块中，产生所选答案的正确性概率。与SRAN不同，SCL用散射变换替换了MLP组件，这种变换将输入拆分成多个组，对每个组应用相同的神经模块，并将输出合并为一个单一的嵌入。该方法与组卷积[[121](#bib.bib121)]、ResNeXt[[122](#bib.bib122)]和模块化网络[[123](#bib.bib123)]有一些相似的思路。
- en: Processing order similar to SCL was employed in Rel-Base and Rel-AIR models [[34](#bib.bib34)]
    that also start with building embeddings independently for each matrix panel.
    The models differ from SCL with the choice of the encoder network – instead of
    a combination of CNN and the scattering transformation they use shallow ResNet.
    Next, both models aggregate 9 context embeddings. However, in contrast to SCL,
    these representations are processed in Rel-Base/AIR models with a simple 1D convolution
    rather than the scattering transformation. In this view, Rel-Base and Rel-AIR
    relate to WReN which instead of 1D convolution uses more computationally expensive
    RN. Although the models proposed by Spratley et al. [[34](#bib.bib34)] are structurally
    similar, they operate on different inputs. Rel-Base processes original matrix
    panels, whereas Rel-AIR employs an unsupervised scene decomposition pre-processing
    step with the Attend-Infer-Repeat (AIR) [[124](#bib.bib124)] neural model. AIR
    decomposes each panel into several object slots that are used to compute a panel
    embedding. This allows to disentangle single objects from the overall scene and
    simplifies the process of discovering relations between them. Explicit scene representation
    is additionally proven to be useful in [[27](#bib.bib27)], where it is represented
    in a probabilistic manner. The authors combine this abstract scene representation
    with a scene inference engine to produce plausible answers to RPMs with a neuro-symbolic
    approach.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与 SCL 相似的处理顺序被用于 Rel-Base 和 Rel-AIR 模型[[34](#bib.bib34)]，这些模型也首先为每个矩阵面板独立构建嵌入。这些模型与
    SCL 的不同之处在于编码器网络的选择——它们使用的是浅层 ResNet，而不是 CNN 和散射变换的组合。接下来，这两个模型聚合了 9 个上下文嵌入。然而，与
    SCL 不同的是，这些表示在 Rel-Base/AIR 模型中通过简单的 1D 卷积处理，而不是散射变换。从这个角度来看，Rel-Base 和 Rel-AIR
    与 WReN 相关，后者使用比 1D 卷积更为计算密集的 RN。尽管 Spratley 等人[[34](#bib.bib34)] 提出的模型在结构上相似，但它们处理的输入不同。Rel-Base
    处理原始矩阵面板，而 Rel-AIR 使用无监督的场景分解预处理步骤以及 Attend-Infer-Repeat (AIR)[[124](#bib.bib124)]
    神经模型。AIR 将每个面板分解为几个对象槽，用于计算面板嵌入。这允许将单个对象与整体场景分离，并简化了发现它们之间关系的过程。显式的场景表示在[[27](#bib.bib27)]中也被证明是有用的，其中场景表示以概率方式呈现。作者将这种抽象的场景表示与场景推理引擎相结合，以神经符号方法生成对
    RPM 的合理答案。
- en: In [[35](#bib.bib35)], similarly to SRAN, the authors further build on the importance
    of instantiating RPM rules row- and column-wise and propose a Dual-Contrast Network
    (DCNet). The model uses a rule contrast module that compares representations of
    the completed third row with the first and second row representations (resp. for
    columns) already present in the RPM. DCNet additionally incorporates a contrast
    module similar to that of CoPINet, which increases relative differences between
    the candidate embeddings. The model training algorithm resembles that of SRAN,
    where the similarity of the representation of the first two rows/columns from
    the RPM context to embeddings of row/column pairs including the correct answer
    panel is maximised, while the similarity to embeddings of row/column pairs with
    wrong answer panels is minimised.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[35](#bib.bib35)]中，与 SRAN 相似，作者进一步强调了逐行和逐列实例化 RPM 规则的重要性，并提出了一种双重对比网络 (DCNet)。该模型使用一个规则对比模块，将完成的第三行的表示与
    RPM 中已存在的第一行和第二行的表示（分别针对列）进行比较。DCNet 还结合了类似于 CoPINet 的对比模块，这增加了候选嵌入之间的相对差异。该模型的训练算法类似于
    SRAN，其中最大化 RPM 上下文中前两行/列的表示与包括正确答案面板的行/列对嵌入的相似性，同时最小化与错误答案面板的行/列对嵌入的相似性。
- en: RPM hierarchies were further exploited in [[36](#bib.bib36)], where the authors
    propose MXGNet – a deep graph neural network for solving AVR problems. In contrast
    to the already described approaches where structural inductive biases were hand-crafted,
    MXGNet builds on an adaptive mechanism which automatically selects key problem
    hierarchies depending on the task at hand. This is achieved by employing $\ell_{1}$-regularised
    gating variables that measure to which extent the panel subsets contribute to
    the model performance. The authors found out that indeed single row/column hierarchies
    are crucial for high model performance in solving RPMs.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: RPM 层次结构在[[36](#bib.bib36)]中得到了进一步的应用，作者提出了 MXGNet——一种用于解决 AVR 问题的深度图神经网络。与之前描述的手工构造结构性归纳偏置的方法不同，MXGNet
    基于一种自适应机制，自动选择根据任务需要的关键问题层次结构。这是通过使用 $\ell_{1}$-正则化的门控变量来实现的，这些变量衡量面板子集对模型性能的贡献程度。作者发现，单一的行/列层次结构对于在解决
    RPM 问题时实现高模型性能确实是至关重要的。
- en: Another unique approach was presented in [[37](#bib.bib37)], where the authors
    propose a method of dynamic inference with Neural Interpreters (NI). The model
    is composed of several reusable self-attention blocks with a learnable routing
    mechanism. The method is inspired by the design of programming languages and utilizes
    concepts analogous to scripts, functions, variables, and an interpreter. This
    perspective facilitates compositional reasoning, where model blocks can be reused
    across tasks. Another design highlight of the model lies in the input processing
    module – the model splits each RPM panel into smaller patches instead of taking
    the whole images as inputs. This division reduces input dimensionality, which
    allows to apply even computationally expensive modules – the authors employ the
    attention module, which computes pair-wise interactions between input elements
    (image pixels). The approach of splitting an image into patches was initially
    proposed in the Vision Transformer (ViT) [[117](#bib.bib117)], which Rahaman et al.
    [[37](#bib.bib37)] adapted to solve RPMs from PGM.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种独特的方法在 [[37](#bib.bib37)]中提出，作者提出了一种使用神经解释器（NI）的动态推理方法。该模型由多个可重用的自注意力块组成，具有可学习的路由机制。该方法受到编程语言设计的启发，利用了类似于脚本、函数、变量和解释器的概念。这一视角促进了组合推理，其中模型块可以在任务之间重复使用。模型的另一个设计亮点在于输入处理模块——模型将每个
    RPM 面板分割成更小的补丁，而不是将整个图像作为输入。这种分割减少了输入维度，使得即使是计算开销大的模块也可以应用——作者使用了注意力模块，该模块计算输入元素（图像像素）之间的成对交互。图像分割成补丁的方法最初在
    Vision Transformer (ViT) [[117](#bib.bib117)] 中提出，Rahaman 等人 [[37](#bib.bib37)]
    将其调整用于解决 PGM 的 RPM 问题。
- en: 'TABLE II: PGM accuracy. Accuracy in all regimes of the PGM dataset [[12](#bib.bib12)]
    arranged in the ascending order by score on the test set of the Neutral regime.
    The Held-out Attribute Pairs regime is denoted as H.O. A.P., Held-out Triple Pairs
    as H.O. T.P., Held-out Triples as H.O. Triples, Held-out Attribute line-type as
    H.O. L-T and Held-out Attribute shape-colour as H.O. S-C.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE II: PGM 精度。PGM 数据集的所有模式的精度 [[12](#bib.bib12)] 按照在中性模式测试集上的得分升序排列。Held-out
    Attribute Pairs 模式记作 H.O. A.P.，Held-out Triple Pairs 记作 H.O. T.P.，Held-out Triples
    记作 H.O. Triples，Held-out Attribute line-type 记作 H.O. L-T，Held-out Attribute shape-colour
    记作 H.O. S-C。'
- en: '| Method | Accuracy (%) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Method | Accuracy (%) |'
- en: '| Neutral | Interpolation | H.O. A.P. | H.O. T.P. | H.O. Triples | H.O. L-T
    | H.O. S-C | Extrapolation |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Neutral | Interpolation | H.O. A.P. | H.O. T.P. | H.O. Triples | H.O. L-T
    | H.O. S-C | Extrapolation |'
- en: '| Val. | Test. | Val. | Test. | Val. | Test. | Val. | Test. | Val. | Test.
    | Val. | Test. | Val. | Test. | Val. | Test. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Val. | Test. | Val. | Test. | Val. | Test. | Val. | Test. | Val. | Test.
    | Val. | Test. | Val. | Test. | Val. | Test. |'
- en: '| Context-blind ResNet [[12](#bib.bib12)] | - | 22.4 | - | - | - | - | - |
    - | - | - | - | - | - | - | - | - |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Context-blind ResNet [[12](#bib.bib12)] | - | 22.4 | - | - | - | - | - |
    - | - | - | - | - | - | - | - | - |'
- en: '| CNN MLP [[12](#bib.bib12)] | - | 33.0 | - | - | - | - | - | - | - | - | -
    | - | - | - | - | - |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| CNN MLP [[12](#bib.bib12)] | - | 33.0 | - | - | - | - | - | - | - | - | -
    | - | - | - | - | - |'
- en: '| CNN LSTM [[12](#bib.bib12)] | - | 35.8 | - | - | - | - | - | - | - | - |
    - | - | - | - | - | - |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| CNN LSTM [[12](#bib.bib12)] | - | 35.8 | - | - | - | - | - | - | - | - |
    - | - | - | - | - | - |'
- en: '| ResNet-50 [[12](#bib.bib12)] | - | 42.0 | - | - | - | - | - | - | - | - |
    - | - | - | - | - | - |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 [[12](#bib.bib12)] | - | 42.0 | - | - | - | - | - | - | - | - |
    - | - | - | - | - | - |'
- en: '| NCD [[30](#bib.bib30)] | - | 47.6 | - | 47.0 | - | - | - | - | - | - | -
    | - | - | - | - | 24.9 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| NCD [[30](#bib.bib30)] | - | 47.6 | - | 47.0 | - | - | - | - | - | - | -
    | - | - | - | - | 24.9 |'
- en: '| Wild-ResNet [[12](#bib.bib12)] | - | 48.0 | - | - | - | - | - | - | - | -
    | - | - | - | - | - | - |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Wild-ResNet [[12](#bib.bib12)] | - | 48.0 | - | - | - | - | - | - | - | -
    | - | - | - | - | - | - |'
- en: '| CoPINet [[20](#bib.bib20)] | - | 56.4 | - | - | - | - | - | - | - | - | -
    | - | - | - | - | - |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| CoPINet [[20](#bib.bib20)] | - | 56.4 | - | - | - | - | - | - | - | - | -
    | - | - | - | - | - |'
- en: '| WReN $\beta=0$ [[12](#bib.bib12)] | 63.0 | 62.6 | 79.0 | 64.4 | 46.7 | 27.2
    | 63.9 | 41.9 | 63.4 | 19.0 | 59.5 | 14.4 | 59.1 | 12.5 | 69.3 | 17.2 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| WReN $\beta=0$ [[12](#bib.bib12)] | 63.0 | 62.6 | 79.0 | 64.4 | 46.7 | 27.2
    | 63.9 | 41.9 | 63.4 | 19.0 | 59.5 | 14.4 | 59.1 | 12.5 | 69.3 | 17.2 |'
- en: '| VAE-WReN $\beta=4$ [[23](#bib.bib23)] | 64.8 | 64.2 | - | - | 70.1 | 36.8
    | 64.6 | 43.6 | 59.5 | 24.6 | - | - | - | - | - | - |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| VAE-WReN $\beta=4$ [[23](#bib.bib23)] | 64.8 | 64.2 | - | - | 70.1 | 36.8
    | 64.6 | 43.6 | 59.5 | 24.6 | - | - | - | - | - | - |'
- en: '| MXGNet $\beta=0$ [[36](#bib.bib36)] | 67.1 | 66.7 | 74.2 | 65.4 | 68.3 |
    33.6 | 67.1 | 43.3 | 63.7 | 19.9 | 60.1 | 16.7 | 68.5 | 16.6 | 69.1 | 18.9 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| MXGNet $\beta=0$ [[36](#bib.bib36)] | 67.1 | 66.7 | 74.2 | 65.4 | 68.3 |
    33.6 | 67.1 | 43.3 | 63.7 | 19.9 | 60.1 | 16.7 | 68.5 | 16.6 | 69.1 | 18.9 |'
- en: '| LEN $\beta=0$ [[22](#bib.bib22)] | - | 68.1 | - | - | - | - | - | - | - |
    - | - | - | - | - | - | - |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| LEN $\beta=0$ [[22](#bib.bib22)] | - | 68.1 | - | - | - | - | - | - | - |
    - | - | - | - | - | - | - |'
- en: '| DCNet [[35](#bib.bib35)] | - | 68.6 | - | 59.7 | - | - | - | - | - | - |
    - | - | - | - | - | 17.8 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| DCNet [[35](#bib.bib35)] | - | 68.6 | - | 59.7 | - | - | - | - | - | - |
    - | - | - | - | - | 17.8 |'
- en: '| T-LEN $\beta=0$ [[22](#bib.bib22)] | - | 70.3 | - | - | - | - | - | - | -
    | - | - | - | - | - | - | - |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| T-LEN $\beta=0$ [[22](#bib.bib22)] | - | 70.3 | - | - | - | - | - | - | -
    | - | - | - | - | - | - | - |'
- en: '| SCL MLCL [[19](#bib.bib19)] | 71.0 | 71.1 | 93.2 | 70.9 | 79.7 | 66.0 | 86.1
    | 71.7 | 84.0 | 22.1 | 86.1 | 16.1 | 94.1 | 12.8 | 83.0 | 21.9 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| SCL MLCL [[19](#bib.bib19)] | 71.0 | 71.1 | 93.2 | 70.9 | 79.7 | 66.0 | 86.1
    | 71.7 | 84.0 | 22.1 | 86.1 | 16.1 | 94.1 | 12.8 | 83.0 | 21.9 |'
- en: '| SRAN [[17](#bib.bib17)] | - | 71.3 | - | - | - | - | - | - | - | - | - |
    - | - | - | - | - |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| SRAN [[17](#bib.bib17)] | - | 71.3 | - | - | - | - | - | - | - | - | - |
    - | - | - | - | - |'
- en: '| ViT [[37](#bib.bib37)] | 73.3 | 72.7 | 89.9 | 67.7 | 69.4 | 34.1 | 67.6 |
    44.1 | 73.8 | 15.9 | - | - | - | - | 92.2 | 16.4 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| ViT [[37](#bib.bib37)] | 73.3 | 72.7 | 89.9 | 67.7 | 69.4 | 34.1 | 67.6 |
    44.1 | 73.8 | 15.9 | - | - | - | - | 92.2 | 16.4 |'
- en: '| WReN $\beta=10$ [[12](#bib.bib12)] | 77.2 | 76.9 | 92.3 | 67.4 | 73.4 | 51.7
    | 74.5 | 56.3 | 80.0 | 20.1 | 78.1 | 16.4 | 85.2 | 13.0 | 93.6 | 15.5 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| WReN $\beta=10$ [[12](#bib.bib12)] | 77.2 | 76.9 | 92.3 | 67.4 | 73.4 | 51.7
    | 74.5 | 56.3 | 80.0 | 20.1 | 78.1 | 16.4 | 85.2 | 13.0 | 93.6 | 15.5 |'
- en: '| NI [[37](#bib.bib37)] | 77.3 | 77.0 | 87.9 | 70.5 | 69.5 | 36.6 | 68.6 |
    45.2 | 79.9 | 20.0 | - | - | - | - | 91.8 | 19.4 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| NI [[37](#bib.bib37)] | 77.3 | 77.0 | 87.9 | 70.5 | 69.5 | 36.6 | 68.6 |
    45.2 | 79.9 | 20.0 | - | - | - | - | 91.8 | 19.4 |'
- en: '| WReN $\beta=10$ + TM[[22](#bib.bib22)] | - | 77.8 | - | - | - | - | - | -
    | - | - | - | - | - | - | - | - |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| WReN $\beta=10$ + TM[[22](#bib.bib22)] | - | 77.8 | - | - | - | - | - | -
    | - | - | - | - | - | - | - | - |'
- en: '| LEN $\beta=10$ [[22](#bib.bib22)] | - | 82.3 | - | - | - | - | - | - | -
    | - | - | - | - | - | - | - |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| LEN $\beta=10$ [[22](#bib.bib22)] | - | 82.3 | - | - | - | - | - | - | -
    | - | - | - | - | - | - | - |'
- en: '| T-LEN $\beta=10$ [[22](#bib.bib22)] | - | 84.1 | - | - | - | - | - | - |
    - | - | - | - | - | - | - | - |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| T-LEN $\beta=10$ [[22](#bib.bib22)] | - | 84.1 | - | - | - | - | - | - |
    - | - | - | - | - | - | - | - |'
- en: '| Rel-Base [[34](#bib.bib34)] | - | 85.5 | - | - | - | - | - | - | - | - |
    - | - | - | - | - | 22.1 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Rel-Base [[34](#bib.bib34)] | - | 85.5 | - | - | - | - | - | - | - | - |
    - | - | - | - | - | 22.1 |'
- en: '| SCL CE [[19](#bib.bib19)] | 86.2 | 85.6 | 91.2 | 55.8 | 56.4 | 40.8 | 78.2
    | 64.5 | 78.6 | 27.0 | 87.6 | 15.1 | 96.9 | 12.7 | 96.3 | 17.3 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| SCL CE [[19](#bib.bib19)] | 86.2 | 85.6 | 91.2 | 55.8 | 56.4 | 40.8 | 78.2
    | 64.5 | 78.6 | 27.0 | 87.6 | 15.1 | 96.9 | 12.7 | 96.3 | 17.3 |'
- en: '| LEN $\beta=10$ + TM[[22](#bib.bib22)] | - | 85.8 | - | - | - | - | - | -
    | - | - | - | - | - | - | - | - |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| LEN $\beta=10$ + TM[[22](#bib.bib22)] | - | 85.8 | - | - | - | - | - | -
    | - | - | - | - | - | - | - | - |'
- en: '| SCL AUX-dense [[19](#bib.bib19)] | 87.6 | 87.1 | 97.9 | 56.0 | 88.6 | 79.6
    | 88.7 | 76.6 | 88.1 | 23.0 | 87.9 | 14.1 | 98.3 | 12.6 | 99.1 | 19.8 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| SCL AUX-dense [[19](#bib.bib19)] | 87.6 | 87.1 | 97.9 | 56.0 | 88.6 | 79.6
    | 88.7 | 76.6 | 88.1 | 23.0 | 87.9 | 14.1 | 98.3 | 12.6 | 99.1 | 19.8 |'
- en: '| SCL AUX-sparse [[19](#bib.bib19)] | 87.4 | 87.1 | 88.1 | 54.1 | 80.8 | 63.6
    | 77.5 | 64.0 | 86.0 | 30.8 | 93.5 | 17.0 | 98.0 | 12.7 | 96.9 | 17.5 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| SCL AUX-sparse [[19](#bib.bib19)] | 87.4 | 87.1 | 88.1 | 54.1 | 80.8 | 63.6
    | 77.5 | 64.0 | 86.0 | 30.8 | 93.5 | 17.0 | 98.0 | 12.7 | 96.9 | 17.5 |'
- en: '| ARNe $\beta=10$ [[31](#bib.bib31)] | - | 88.2 | - | - | - | - | - | - | -
    | - | - | - | - | - | 98.9 | 17.8 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| ARNe $\beta=10$ [[31](#bib.bib31)] | - | 88.2 | - | - | - | - | - | - | -
    | - | - | - | - | - | 98.9 | 17.8 |'
- en: '| T-LEN $\beta=10$ + TM[[22](#bib.bib22)] | - | 88.9 | - | - | - | - | - |
    - | - | - | - | - | - | - | - | - |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| T-LEN $\beta=10$ + TM[[22](#bib.bib22)] | - | 88.9 | - | - | - | - | - |
    - | - | - | - | - | - | - | - | - |'
- en: '| SCL [[33](#bib.bib33)] | - | 88.9 | - | - | - | - | - | - | - | - | - | -
    | - | - | - | - |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| SCL [[33](#bib.bib33)] | - | 88.9 | - | - | - | - | - | - | - | - | - | -
    | - | - | - | - |'
- en: '| MXGNet $\beta=10$ [[36](#bib.bib36)] | 89.9 | 89.6 | 91.5 | 84.6 | 81.9 |
    69.3 | 78.1 | 64.2 | 80.5 | 20.2 | 85.2 | 16.8 | 89.2 | 15.6 | 94.3 | 18.4 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| MXGNet $\beta=10$ [[36](#bib.bib36)] | 89.9 | 89.6 | 91.5 | 84.6 | 81.9 |
    69.3 | 78.1 | 64.2 | 80.5 | 20.2 | 85.2 | 16.8 | 89.2 | 15.6 | 94.3 | 18.4 |'
- en: '| MRNet [[18](#bib.bib18)] | - | 93.4 | - | 68.1 | - | 38.4 | - | 55.3 | -
    | 25.9 | - | 30.1 | - | 16.9 | - | 19.2 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| MRNet [[18](#bib.bib18)] | - | 93.4 | - | 68.1 | - | 38.4 | - | 55.3 | -
    | 25.9 | - | 30.1 | - | 16.9 | - | 19.2 |'
- en: '| MLRN [[32](#bib.bib32)] | - | 98.0 | - | 57.8 | - | - | - | - | - | - | -
    | - | - | - | - | 14.9 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| MLRN [[32](#bib.bib32)] | - | 98.0 | - | 57.8 | - | - | - | - | - | - | -
    | - | - | - | - | 14.9 |'
- en: 5 Evaluating machine intelligence with RPMs
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估机器智能与RPMs
- en: In spite of many attempts at creating efficient models for solving RPMs, current
    approaches described in the previous section still struggle in more demanding
    benchmark setups. In this section, we summarise the main quantitative results
    of the discussed models on PGM, RAVEN and I-RAVEN datasets. Moreover, we highlight
    the most challenging setups for the current models and contrast their performance
    with the number of trainable parameters.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管进行了多次尝试来创建高效的模型以解决RPM问题，但上一节中描述的当前方法在更具挑战性的基准设置中仍面临困难。在本节中，我们总结了在PGM、RAVEN和I-RAVEN数据集上讨论的模型的主要定量结果。此外，我们还强调了当前模型面临的最具挑战性的设置，并对比了它们的性能和可训练参数的数量。
- en: 5.1 Results on PGM
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 PGM 结果
- en: 'We start by discussing the results on PGM. Table [II](#S4.T2 "TABLE II ‣ 4.3
    Hierarchical networks ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods for
    Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices") compares
    performance of all discussed approaches based on the accuracy results reported
    in the respective papers. Firstly, it can be seen that the majority of presented
    approaches weren’t evaluated in all PGM regimes. Namely, out of 32 methods shown
    in the table, only 9 were tested in each regime (4 unique models: WReN, MXGNet,
    SCL, MRNet, trained with different setups) and only 8 other methods were evaluated
    in at least one regime other than Neutral.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先讨论 PGM 的结果。表 [II](#S4.T2 "TABLE II ‣ 4.3 Hierarchical networks ‣ 4 RPM
    Deep Learning Models ‣ Deep Learning Methods for Abstract Visual Reasoning: A
    Survey on Raven’s Progressive Matrices") 比较了所有讨论的方法在各自论文中报告的准确率结果。首先，可以看到大多数展示的方法并未在所有
    PGM 条件下进行评估。即在表中展示的 32 种方法中，仅有 9 种在每个条件下进行了测试（4 种独特模型：WReN、MXGNet、SCL、MRNet，使用不同的设置训练），且仅有
    8 种其他方法在至少一个 Neutral 之外的条件下进行了评估。'
- en: This observation raises a concern that, in practice, the PGM dataset is not
    utilized in a way it was intended for. Barrett et al. [[12](#bib.bib12)] defined
    the PGM benchmark as a tool for measuring generalisation performance across different
    regimes, whereas less than half of existing methods were evaluated on regimes
    other than Neutral. In the Neutral regime, all dataset splits (train/val/test)
    contain RPMs with objects, attributes and rules sampled from the same underlying
    distributions. As a consequence, the regime measures the capacity of the algorithm
    of understanding what an RPM is, rather than its ability to generalise to novel
    settings represented by the remaining regimes.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这一观察提出了一个问题，即在实践中，PGM 数据集并没有按照其预期的方式使用。Barrett 等人 [[12](#bib.bib12)] 将 PGM 基准定义为用于衡量不同条件下泛化性能的工具，而现有方法中不到一半的在
    Neutral 之外的条件下进行了评估。在 Neutral 条件下，所有数据集划分（训练/验证/测试）包含的 RPMs 具有从相同基本分布中采样的对象、属性和规则。因此，这一条件衡量的是算法理解
    RPM 的能力，而不是其在其他条件下的泛化能力。
- en: Although this limited evaluation of various works is partially explained by
    the extensive dataset size which is the main bottleneck for measuring generalisation,
    our beliefs are in line with the PGM authors that, actually, the performance across
    different regimes is what should be compared. The accuracy in the Neutral regime
    is only an initial indicator of the model reasoning capability, which can often
    be misleading as demonstrated by variation in the MLRN performance – the model
    achieves near perfect accuracy in the Neutral regime, while completely failing
    in the Extrapolation split. At the same time it performs subpar when compared
    to other models—potentially weaker when judging by Neutral regime scores—in the
    Interpolation regime. We believe that the ultimate goal of developing DL methods
    for solving AVR problems is not to obtain well-performing models on existing benchmarks,
    but rather to search for higher level approaches tackling generalisation that
    could be transferred to other domains.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管各种工作的有限评估部分原因可以归因于数据集规模庞大，这也是衡量泛化能力的主要瓶颈，但我们与 PGM 作者的观点一致，认为实际应比较不同条件下的性能。Neutral
    条件下的准确率仅是模型推理能力的初步指标，这常常可能具有误导性，如 MLRN 性能的变化所示——模型在 Neutral 条件下的准确率接近完美，但在 Extrapolation
    划分中完全失败。同时，与其他模型相比，其在 Interpolation 条件下的表现也较差——在 Neutral 条件下评分较低的情况下，表现可能较弱。我们相信，开发用于解决
    AVR 问题的深度学习方法的终极目标不是在现有基准上获得表现良好的模型，而是寻求更高级的方法来处理泛化问题，并能迁移到其他领域。
- en: 'TABLE III: PGM mean accuracy. Average accuracy measured on the test split of
    models that were evaluated in each PGM regime, sorted in increasing order.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: PGM 平均准确率。对每个 PGM 条件下评估的模型在测试划分上的平均准确率进行排序。'
- en: '| Method | Mean test accuracy (%) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 平均测试准确率（%） |'
- en: '| WReN $\beta=0$ [[12](#bib.bib12)] | 32.4 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| WReN $\beta=0$ [[12](#bib.bib12)] | 32.4 |'
- en: '| MXGNet $\beta=0$ [[36](#bib.bib36)] | 35.1 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| MXGNet $\beta=0$ [[36](#bib.bib36)] | 35.1 |'
- en: '| WReN $\beta=10$ [[12](#bib.bib12)] | 39.7 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| WReN $\beta=10$ [[12](#bib.bib12)] | 39.7 |'
- en: '| SCL CE [[19](#bib.bib19)] | 39.8 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| SCL CE [[19](#bib.bib19)] | 39.8 |'
- en: '| SCL AUX-sparse [[19](#bib.bib19)] | 43.3 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| SCL AUX-sparse [[19](#bib.bib19)] | 43.3 |'
- en: '| MRNet [[18](#bib.bib18)] | 43.4 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| MRNet [[18](#bib.bib18)] | 43.4 |'
- en: '| SCL MLCL [[19](#bib.bib19)] | 44.1 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| SCL MLCL [[19](#bib.bib19)] | 44.1 |'
- en: '| SCL AUX-dense [[19](#bib.bib19)] | 46.1 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| SCL AUX-dense [[19](#bib.bib19)] | 46.1 |'
- en: '| MXGNet $\beta=10$ [[36](#bib.bib36)] | 47.3 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| MXGNet $\beta=10$ [[36](#bib.bib36)] | 47.3 |'
- en: 'A closer look at the models’ performance in different regimes reveals that
    there isn’t a single method that performs superior across all regimes. On the
    contrary, each method seems to possess specific generalisation capabilities. The
    average accuracy of the models that were evaluated in all PGM regimes is summarised
    in Table [III](#S5.T3 "TABLE III ‣ 5.1 Results on PGM ‣ 5 Evaluating machine intelligence
    with RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s
    Progressive Matrices"). Best result is achieved by MXGNet trained using auxiliary
    training with $\beta=10$. The SCL model achieves competitive results with various
    training setups. The third model, MRNet is a close runner-up to SCL. The WReN
    model performs worst. However, since only a handful of the proposed methods were
    evaluated in each regime, it is difficult to ultimately choose a model with the
    best generalisation performance.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '更深入地观察模型在不同领域的表现可以发现，没有一种方法在所有领域中都表现最佳。相反，每种方法似乎都具有特定的泛化能力。所有PGM领域中评估的模型的平均准确率总结在表[III](#S5.T3
    "TABLE III ‣ 5.1 Results on PGM ‣ 5 Evaluating machine intelligence with RPMs
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices")中。最佳结果由使用$\beta=10$进行辅助训练的MXGNet取得。SCL模型在各种训练设置中都表现出竞争力。第三个模型MRNet紧随SCL。WReN模型表现最差。然而，由于在每个领域中仅评估了少数提出的方法，因此最终选择具有最佳泛化性能的模型仍然困难。'
- en: '![Refer to caption](img/4b9b816ce900dca86c9b1de7d3b9cff0.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4b9b816ce900dca86c9b1de7d3b9cff0.png)'
- en: 'Figure 9: PGM regime difficulty. Mean accuracy on test (Test.) and validation
    (Val.) splits and their difference (Diff.) for the methods that were evaluated
    in each regime (listed in Table [III](#S5.T3 "TABLE III ‣ 5.1 Results on PGM ‣
    5 Evaluating machine intelligence with RPMs ‣ Deep Learning Methods for Abstract
    Visual Reasoning: A Survey on Raven’s Progressive Matrices")).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '图9：PGM领域难度。测试（Test.）和验证（Val.）分割的平均准确率及其差异（Diff.），适用于在每个领域中评估的方法（见表[III](#S5.T3
    "TABLE III ‣ 5.1 Results on PGM ‣ 5 Evaluating machine intelligence with RPMs
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices")）。'
- en: 'Given the results of 9 models from Table [III](#S5.T3 "TABLE III ‣ 5.1 Results
    on PGM ‣ 5 Evaluating machine intelligence with RPMs ‣ Deep Learning Methods for
    Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices") that were
    evaluated in each PGM regime, we compare in Fig. [9](#S5.F9 "Figure 9 ‣ 5.1 Results
    on PGM ‣ 5 Evaluating machine intelligence with RPMs ‣ Deep Learning Methods for
    Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices") the levels
    of difficulty of individual regimes. It can be easily noticed that the regimes
    can be categorised into three distinct groups, depending on their difficulty.
    Firstly, there is the Neutral regime, where by definition the test split has the
    same distribution as the validation split. In this conventional setup, the performance
    on validation split is an accurate indicator of the results on the test split.
    This, however, is no longer the case in the remaining regimes. In the Interpolation,
    Held-out Triple Pairs and Held-out Attribute Pairs regimes, the accuracy of the
    models noticeably decreases between validation and test splits (by as large as
    34.7% in case of MXGNet $\beta=0$ in Held-out Attribute Pairs regime, see Table [II](#S4.T2
    "TABLE II ‣ 4.3 Hierarchical networks ‣ 4 RPM Deep Learning Models ‣ Deep Learning
    Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")).
    Generalisation performance practically diminishes in the remaining regimes, i.e. Held-out
    Triples, Extrapolation, Held-out Line-Type and Held-out Shape-Color, where some
    models even present the behaviour on the test set indistinguishable from random
    guessing. These observations demonstrate that although current approaches present
    satisfactory results in the Neutral regime, all other, more demanding regimes,
    remain a challenge. We believe that future works should detour from pushing the
    limits of i.i.d. generalisation and instead explicitly focus on building models
    capable of generalising to out-of-distribution samples.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '根据表 [III](#S5.T3 "TABLE III ‣ 5.1 Results on PGM ‣ 5 Evaluating machine intelligence
    with RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s
    Progressive Matrices") 中 9 个模型在每个 PGM 环境下的结果，我们在图 [9](#S5.F9 "Figure 9 ‣ 5.1 Results
    on PGM ‣ 5 Evaluating machine intelligence with RPMs ‣ Deep Learning Methods for
    Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices") 中比较了各个环境的难度水平。可以明显地看到，这些环境可以根据其难度分为三组。首先是中性环境，根据定义，测试集与验证集具有相同的分布。在这种传统设置下，验证集上的表现是测试集结果的准确指示。然而，在其余环境中情况已不再如此。在插值、保留三元组对和保留属性对环境中，模型的准确率在验证集与测试集之间显著下降（在
    MXGNet $\beta=0$ 的保留属性对环境中下降高达 34.7%，见表 [II](#S4.T2 "TABLE II ‣ 4.3 Hierarchical
    networks ‣ 4 RPM Deep Learning Models ‣ Deep Learning Methods for Abstract Visual
    Reasoning: A Survey on Raven’s Progressive Matrices")）。在剩余环境中，即保留三元组、外推、保留线型和保留形状-颜色中，泛化性能几乎消失，一些模型甚至在测试集上的表现与随机猜测无法区分。这些观察结果表明，尽管当前方法在中性环境下表现令人满意，但其他更具挑战性的环境仍然是一个难题。我们认为，未来的工作应避免仅仅推动
    i.i.d. 泛化的极限，而应明确专注于构建能够对分布外样本进行泛化的模型。'
- en: 'TABLE IV: RAVEN vs I-RAVEN accuracy. Mean accuracy on the test splits for all
    configurations of both RAVEN [[16](#bib.bib16)] and I-RAVEN [[17](#bib.bib17)]
    datasets. The models are arranged according to their score on I-RAVEN and then
    on RAVEN in ascending order. The results of the best reported configuration are
    presented for each model. ²²footnotemark: 2MRNet was evaluated on RAVEN-FAIR.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：RAVEN 与 I-RAVEN 的准确率。所有 RAVEN [[16](#bib.bib16)] 和 I-RAVEN [[17](#bib.bib17)]
    数据集配置在测试集上的平均准确率。模型根据其在 I-RAVEN 上的得分以及在 RAVEN 上的得分按升序排列。每个模型呈现了最佳报告配置的结果。²²脚注：2MRNet
    在 RAVEN-FAIR 上进行评估。
- en: '| Method | Test accuracy (%) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 测试准确率（%） |'
- en: '| RAVEN | I-RAVEN |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| RAVEN | I-RAVEN |'
- en: '| CNN LSTM + DRT [[16](#bib.bib16)] | 14.0 | - |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| CNN LSTM + DRT [[16](#bib.bib16)] | 14.0 | - |'
- en: '| WReN + DRT [[16](#bib.bib16)] | 15.0 | - |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| WReN + DRT [[16](#bib.bib16)] | 15.0 | - |'
- en: '| ARNe [[31](#bib.bib31)] | 19.7 | - |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| ARNe [[31](#bib.bib31)] | 19.7 | - |'
- en: '| MCPT [[28](#bib.bib28)] | 28.5 | - |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| MCPT [[28](#bib.bib28)] | 28.5 | - |'
- en: '| WReN-Tag-Aux [[20](#bib.bib20)] | 34.0 | - |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| WReN-Tag-Aux [[20](#bib.bib20)] | 34.0 | - |'
- en: '| CNN MLP [[16](#bib.bib16)] | 37.0 | - |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| CNN MLP [[16](#bib.bib16)] | 37.0 | - |'
- en: '| CNN MLP + DRT [[16](#bib.bib16)] | 39.4 | - |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| CNN MLP + DRT [[16](#bib.bib16)] | 39.4 | - |'
- en: '| PRD [[29](#bib.bib29)] | 50.7 | - |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| PRD [[29](#bib.bib29)] | 50.7 | - |'
- en: '| ResNet-18 [[28](#bib.bib28)] | 77.2 | - |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 [[28](#bib.bib28)] | 77.2 | - |'
- en: '| LEN + TM [[22](#bib.bib22)] | 78.3 | - |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| LEN + TM [[22](#bib.bib22)] | 78.3 | - |'
- en: '| MXGNet [[36](#bib.bib36)] | 83.9 | - |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| MXGNet [[36](#bib.bib36)] | 83.9 | - |'
- en: '| ²²footnotemark: 2MRNet [[18](#bib.bib18)] | 84.0 | - |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| ²²脚注：2MRNet [[18](#bib.bib18)] | 84.0 | - |'
- en: '| ResNet-50 + pre-train [[28](#bib.bib28)] | 86.3 | - |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 + pre-train [[28](#bib.bib28)] | 86.3 | - |'
- en: '| Rel-Base [[34](#bib.bib34)] | 91.7 | - |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Rel-Base [[34](#bib.bib34)] | 91.7 | - |'
- en: '| CoPINet + AL [[21](#bib.bib21)] | 93.5 | - |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| CoPINet + AL [[21](#bib.bib21)] | 93.5 | - |'
- en: '| DCNet [[35](#bib.bib35)] | 93.6 | - |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| DCNet [[35](#bib.bib35)] | 93.6 | - |'
- en: '| CoPINet + ACL [[21](#bib.bib21)] | 93.7 | - |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| CoPINet + ACL [[21](#bib.bib21)] | 93.7 | - |'
- en: '| Rel-AIR [[34](#bib.bib34)] | 94.1 | - |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Rel-AIR [[34](#bib.bib34)] | 94.1 | - |'
- en: '| Context-blind ResNet [[17](#bib.bib17)] | 71.9 | 12.2 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Context-blind ResNet [[17](#bib.bib17)] | 71.9 | 12.2 |'
- en: '| Context-blind SCL [[33](#bib.bib33)] | 94.2 | 12.2 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Context-blind SCL [[33](#bib.bib33)] | 94.2 | 12.2 |'
- en: '| Context-blind CoPINet [[17](#bib.bib17)] | 94.2 | 14.2 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Context-blind CoPINet [[17](#bib.bib17)] | 94.2 | 14.2 |'
- en: '| CNN LSTM [[16](#bib.bib16), [17](#bib.bib17)] | 13.1 | 18.9 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| CNN LSTM [[16](#bib.bib16), [17](#bib.bib17)] | 13.1 | 18.9 |'
- en: '| WReN [[16](#bib.bib16), [17](#bib.bib17)] | 14.7 | 23.8 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| WReN [[16](#bib.bib16), [17](#bib.bib17)] | 14.7 | 23.8 |'
- en: '| ResNet-18 [[16](#bib.bib16), [17](#bib.bib17)] | 53.4 | 40.3 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 [[16](#bib.bib16), [17](#bib.bib17)] | 53.4 | 40.3 |'
- en: '| ResNet-18 + DRT [[16](#bib.bib16), [17](#bib.bib17)] | 59.6 | 40.4 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 + DRT [[16](#bib.bib16), [17](#bib.bib17)] | 59.6 | 40.4 |'
- en: '| LEN [[22](#bib.bib22), [17](#bib.bib17)] | 72.9 | 41.4 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| LEN [[22](#bib.bib22), [17](#bib.bib17)] | 72.9 | 41.4 |'
- en: '| Wild ResNet [[17](#bib.bib17)] | - | 44.3 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Wild ResNet [[17](#bib.bib17)] | - | 44.3 |'
- en: '| CoPINet [[20](#bib.bib20), [17](#bib.bib17)] | 91.4 | 46.1 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| CoPINet [[20](#bib.bib20), [17](#bib.bib17)] | 91.4 | 46.1 |'
- en: '| NCD [[30](#bib.bib30)] | 37.0 | 48.2 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| NCD [[30](#bib.bib30)] | 37.0 | 48.2 |'
- en: '| CoPINet MLCL+DA [[19](#bib.bib19)] | - | 57.1 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| CoPINet MLCL+DA [[19](#bib.bib19)] | - | 57.1 |'
- en: '| SRAN [[17](#bib.bib17)] | - | 60.8 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| SRAN [[17](#bib.bib17)] | - | 60.8 |'
- en: '| SRAN MLCL+DA [[19](#bib.bib19)] | - | 73.3 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| SRAN MLCL+DA [[19](#bib.bib19)] | - | 73.3 |'
- en: '| ²²footnotemark: 2MRNet [[18](#bib.bib18)] | - | 86.8 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| ²²footnotemark: 2MRNet [[18](#bib.bib18)] | - | 86.8 |'
- en: '| SCL [[33](#bib.bib33)] | 91.6 | 95.0 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| SCL [[33](#bib.bib33)] | 91.6 | 95.0 |'
- en: '| SCL MLCL+DA [[19](#bib.bib19)] | - | 96.8 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| SCL MLCL+DA [[19](#bib.bib19)] | - | 96.8 |'
- en: '| Human [[16](#bib.bib16)] | 84.4 | - |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Human [[16](#bib.bib16)] | 84.4 | - |'
- en: 5.2 Results on (I-)RAVEN
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 (I-)RAVEN上的结果
- en: 'Besides PGM, the methods for solving RPMs are often evaluated on the RAVEN
    dataset and its derivatives. The aggregated accuracy scores of the discussed approaches
    on test splits of both RAVEN and I-RAVEN are shown in Table [2](#footnotex2 "footnote
    2 ‣ TABLE IV ‣ 5.1 Results on PGM ‣ 5 Evaluating machine intelligence with RPMs
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices"). Firstly, it can be seen that the upper part of the table contains
    methods that were evaluated only on RAVEN. However, due to the hidden bias in
    the answer generation algorithm [[17](#bib.bib17)] the results reported on this
    dataset are inconclusive and can be misleading. This was demonstrated by remarkable
    performance of context-blind models, on the one hand, and by significant (45.3
    p.p.) drop of accuracy of CoPINet on the balanced dataset (91.4% on RAVEN vs 46.1%
    on I-RAVEN), on the other hand. Therefore, we strongly advocate for evaluating
    existing and future DL approaches on the unbiased version of the dataset.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '除了PGM，解决RPM问题的方法通常会在RAVEN数据集及其衍生品上进行评估。表[2](#footnotex2 "footnote 2 ‣ TABLE
    IV ‣ 5.1 Results on PGM ‣ 5 Evaluating machine intelligence with RPMs ‣ Deep Learning
    Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")中展示了讨论方法在RAVEN和I-RAVEN测试分割上的聚合准确性评分。首先，可以看出表的上半部分包含仅在RAVEN上评估的方法。然而，由于答案生成算法中的隐含偏差[[17](#bib.bib17)]，该数据集上报告的结果是不确定的，可能具有误导性。这一观点得到了对上下文盲模型显著性能的证明，以及CoPINet在平衡数据集上准确率显著下降（RAVEN上为91.4%，I-RAVEN上为46.1%）的支持。因此，我们强烈建议对现有和未来的DL方法在数据集的无偏版本上进行评估。'
- en: Best results on I-RAVEN are achieved by SCL, a model which also performed very
    well on PGM. This suggests that SCL is, overall, the best-performing model for
    solving RPMs. This claim is further strengthened in the following section which
    sheds light on the relation between model performance and the number of its parameters.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的I-RAVEN结果由SCL模型实现，该模型在PGM上的表现也非常出色。这表明，SCL总体上是解决RPM问题的**最佳**模型。接下来的部分进一步揭示了模型性能与其参数数量之间的关系，从而**进一步加强**了这一说法。
- en: Detailed performance results on particular RAVEN and I-RAVEN configurations
    are presented in the *supplementary material*.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 对特定RAVEN和I-RAVEN配置的详细性能结果展示在*补充材料*中。
- en: 'TABLE V: Model size. Number of trainable parameters based on model’s open-source
    implementation. C-B stands for Context-blind. ²²footnotemark: 2In C-B ResNet-18
    we have changed the number of input channels from 16 to 8 in the provided implementation.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V：模型大小。基于模型开源实现的可训练参数数量。C-B 代表 Context-blind。²²footnotemark: 2 在 C-B ResNet-18
    中，我们已将输入通道数量从 16 更改为 8。'
- en: '| Model | # params | GitHub repository |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | # 参数 | GitHub 仓库 |'
- en: '| SCL [[33](#bib.bib33)] | 137,286 | [dhh1995/SCL](https://github.com/dhh1995/SCL)
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| SCL [[33](#bib.bib33)] | 137,286 | [dhh1995/SCL](https://github.com/dhh1995/SCL)
    |'
- en: '| CNN LSTM [[16](#bib.bib16)] | 143,960 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| CNN LSTM [[16](#bib.bib16)] | 143,960 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
- en: '| CNN MLP [[16](#bib.bib16)] | 299,400 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| CNN MLP [[16](#bib.bib16)] | 299,400 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
- en: '| CNN LSTM + DRT [[16](#bib.bib16)] | 804,628 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| CNN LSTM + DRT [[16](#bib.bib16)] | 804,628 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
- en: '| WReN [[12](#bib.bib12)] | 1,216,173 | [Fen9/WReN](https://github.com/Fen9/WReN)
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| WReN [[12](#bib.bib12)] | 1,216,173 | [Fen9/WReN](https://github.com/Fen9/WReN)
    |'
- en: '| Rel-Base [[34](#bib.bib34)] | 1,226,673 | [SvenShade/Rel-AIR](https://github.com/SvenShade/Rel-AIR)
    |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Rel-Base [[34](#bib.bib34)] | 1,226,673 | [SvenShade/Rel-AIR](https://github.com/SvenShade/Rel-AIR)
    |'
- en: '| CoPINet [[20](#bib.bib20)] | 1,685,949 | [WellyZhang/CoPINet](https://github.com/WellyZhang/CoPINet)
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| CoPINet [[20](#bib.bib20)] | 1,685,949 | [WellyZhang/CoPINet](https://github.com/WellyZhang/CoPINet)
    |'
- en: '| Rel-AIR [[34](#bib.bib34)] | 1,948,644 | [SvenShade/Rel-AIR](https://github.com/SvenShade/Rel-AIR)
    |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Rel-AIR [[34](#bib.bib34)] | 1,948,644 | [SvenShade/Rel-AIR](https://github.com/SvenShade/Rel-AIR)
    |'
- en: '| CNN MLP + DRT [[16](#bib.bib16)] | 2,054,724 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| CNN MLP + DRT [[16](#bib.bib16)] | 2,054,724 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
- en: '| LEN [[22](#bib.bib22)] | 5,520,673 | [zkcys001/distracting_feature](https://github.com/zkcys001/distracting_feature)
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| LEN [[22](#bib.bib22)] | 5,520,673 | [zkcys001/distracting_feature](https://github.com/zkcys001/distracting_feature)
    |'
- en: '| DCNet [[35](#bib.bib35)] | 5,833,025 | [visiontao/dcnet](https://github.com/visiontao/dcnet)
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| DCNet [[35](#bib.bib35)] | 5,833,025 | [visiontao/dcnet](https://github.com/visiontao/dcnet)
    |'
- en: '| NCD [[30](#bib.bib30)] | 11,177,025 | [visiontao/ncd](https://github.com/visiontao/ncd)
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| NCD [[30](#bib.bib30)] | 11,177,025 | [visiontao/ncd](https://github.com/visiontao/ncd)
    |'
- en: '| ²²footnotemark: 2C-B ResNet-18 [[16](#bib.bib16)] | 11,474,342 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| ²²footnotemark: 2C-B ResNet-18 [[16](#bib.bib16)] | 11,474,342 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
- en: '| ResNet-18 [[16](#bib.bib16)] | 11,499,430 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 [[16](#bib.bib16)] | 11,499,430 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
- en: '| ResNet-18 + DRT [[16](#bib.bib16)] | 13,254,754 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 + DRT [[16](#bib.bib16)] | 13,254,754 | [WellyZhang/RAVEN](https://github.com/WellyZhang/RAVEN)
    |'
- en: '| MRNet [[18](#bib.bib18)] | 19,531,841 | [yanivbenny/MRNet](https://github.com/yanivbenny/MRNet)
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| MRNet [[18](#bib.bib18)] | 19,531,841 | [yanivbenny/MRNet](https://github.com/yanivbenny/MRNet)
    |'
- en: '| SRAN [[17](#bib.bib17)] | 44,030,217 | [husheng12345/SRAN](https://github.com/husheng12345/SRAN)
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| SRAN [[17](#bib.bib17)] | 44,030,217 | [husheng12345/SRAN](https://github.com/husheng12345/SRAN)
    |'
- en: <svg   height="395.2" overflow="visible" version="1.1" width="483.94"><g transform="translate(0,395.2)
    matrix(1 0 0 -1 0 0) translate(32.26,0) translate(0,9.2)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g clip-path="url(#pgfcp1)"><g stroke="#000000" fill="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 49.46 30.65)"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 109.53 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 173.06 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 236.6 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 300.13 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 360.21 30.65)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 115.82 11.29)"><foreignobject
    width="191.85" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PGM
    neutral test accuracy (%)</foreignobject></g><g stroke="#000000" fill="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 41.11 39.61)"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 105.96)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 172.31)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 238.66)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 305.01)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 27.22 371.35)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(0.0 1.0 -1.0 0.0 18.21 111.66)"><foreignobject
    width="198.83" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RAVEN
    mean test accuracy (%)</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -32.26 150.45)"><foreignobject width="380.01" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">CNN MLP [[12](#bib.bib12), [16](#bib.bib16)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 -27.5 71.16)"><foreignobject
    width="388.28" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CNN
    LSTM [[12](#bib.bib12), [16](#bib.bib16)]</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 74.36 103.01)"><foreignobject
    width="354.84" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">WReN [[12](#bib.bib12),
    [16](#bib.bib16)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 157.9 266.23)"><foreignobject width="222.69" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">LEN [[22](#bib.bib22)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 -21.68 344.19)"><foreignobject
    width="247.41" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CoPINet [[20](#bib.bib20)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 139.92 361.44)"><foreignobject
    width="230.07" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DCNet [[35](#bib.bib35)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 223.66 303.05)"><foreignobject
    width="220.16" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MRNet [[18](#bib.bib18)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 160.44 328.6)"><foreignobject
    width="245.56" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Rel-Base [[34](#bib.bib34)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 80.15 180.3)"><foreignobject
    width="247.95" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NCD [[30](#bib.bib30)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 238.02 354.8)"><foreignobject
    width="213.66" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SCL [[33](#bib.bib33)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 70.48 352.1)"><foreignobject
    width="60.08" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">#
    params</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 332.2)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0.1 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 312.83)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0.3
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 293.46)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1.2 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 274.09)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1.2
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 254.73)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1.7 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 235.36)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5.5
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 215.99)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">5.8 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 196.63)"><foreignobject
    width="41.9" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">11.2
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 177.26)"><foreignobject width="41.9" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">19.5 M</foreignobject></g></g></g></svg>
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="395.2" overflow="visible" version="1.1" width="483.94"><g transform="translate(0,395.2)
    matrix(1 0 0 -1 0 0) translate(32.26,0) translate(0,9.2)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g clip-path="url(#pgfcp1)"><g stroke="#000000" fill="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 49.46 30.65)"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 109.53 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 173.06 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 236.6 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 300.13 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 360.21 30.65)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 115.82 11.29)"><foreignobject
    width="191.85" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PGM
    中性测试准确率 (%)</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 41.11 39.61)"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 105.96)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 172.31)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 238.66)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 305.01)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 27.22 371.35)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(0.0 1.0 -1.0 0.0 18.21 111.66)"><foreignobject
    width="198.83" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RAVEN
    平均测试准确率 (%)</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -32.26 150.45)"><foreignobject width="380.01" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">CNN MLP [[12](#bib.bib12), [16](#bib.bib16)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 -27.5 71.16)"><foreignobject
    width="388.28" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CNN
    LSTM [[12](#bib.bib12), [16](#bib.bib16)]</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 74.36 103.01)"><foreignobject
    width="354.84" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">WReN [[12](#bib.bib12),
    [16](#bib.bib16)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 157.9 266.23)"><foreignobject width="222.69" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">LEN [[22](#bib.bib22)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 -21.68 344.19)"><foreignobject
    width="247.41" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CoPINet [[20](#bib.bib20)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 139.92 361.44)"><foreignobject
    width="230.07" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DCNet [[35](#bib.bib35)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 223.66 303.05)"><foreignobject
    width="220.16" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MRNet [[18](#bib.bib18)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 160.44 328
- en: (a) PGM – RAVEN.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PGM – RAVEN.
- en: '<svg version="1.1" width="645.49" height="395.2" overflow="visible"><g transform="translate(0,395.2)
    scale(1,-1)"><g transform="translate(0,395.2) scale(1, -1)"><foreignobject width="645.49"
    height="395.2" overflow="visible"><svg height="395.2" overflow="visible" version="1.1"
    width="645.49"><g transform="translate(0,395.2) matrix(1 0 0 -1 0 0) translate(28.11,0)
    translate(0,9.2)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g clip-path="url(#pgfcp22)"><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 49.46 30.65)"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 109.53 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 173.06 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 236.6 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 300.13 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 360.21 30.65)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 115.82 11.29)"><foreignobject
    width="191.85" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PGM
    neutral test accuracy (%)</foreignobject></g><g stroke="#000000" fill="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 41.11 39.61)"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 105.96)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 172.31)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 238.66)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 305.01)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 27.22 371.35)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(0.0 1.0 -1.0 0.0 18.21 106.85)"><foreignobject
    width="208.44" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">I-RAVEN
    mean test accuracy (%)</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -28.11 90.4)"><foreignobject width="389.51" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">CNN LSTM [[12](#bib.bib12), [17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 261.31 119.93)"><foreignobject
    width="356.07" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">WReN [[12](#bib.bib12),
    [17](#bib.bib17)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 247.01 161.73)"><foreignobject width="323.51" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">LEN [[22](#bib.bib22), [17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 89.73 203.86)"><foreignobject
    width="348.23" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CoPINet [[17](#bib.bib17),
    [20](#bib.bib20)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 170.28 259.26)"><foreignobject width="218.27" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SRAN [[17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 217.3 312.06)"><foreignobject
    width="220.16" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MRNet [[18](#bib.bib18)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 80.15 184.29)"><foreignobject
    width="247.95" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NCD [[30](#bib.bib30)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 118.48 356.13)"><foreignobject
    width="213.66" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SCL [[33](#bib.bib33)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 70.48 352.1)"><foreignobject
    width="60.08" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">#
    params</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 332.2)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0.1 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 312.83)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1.2
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 293.46)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1.7 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 274.09)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5.5
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 254.73)"><foreignobject width="41.9" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">11.2 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 235.36)"><foreignobject
    width="41.9" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">19.5
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 215.99)"><foreignobject width="41.9" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">44.0 M</foreignobject></g></g></g></svg></foreignobject></g><g
    transform="translate(0,395.2) scale(1, -1)"><foreignobject width="645.49" height="395.2"
    overflow="visible">²²footnotemark: 2</foreignobject></g></g></svg>'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ²²脚注标记：2
- en: (b) PGM – I-RAVEN.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: (b) PGM – I-RAVEN.
- en: '<svg version="1.1" width="678.01" height="395.2" overflow="visible"><g transform="translate(0,395.2)
    scale(1,-1)"><g transform="translate(0,395.2) scale(1, -1)"><foreignobject width="678.01"
    height="395.2" overflow="visible"><svg height="395.2" overflow="visible" version="1.1"
    width="678.01"><g transform="translate(0,395.2) matrix(1 0 0 -1 0 0) translate(60.47,0)
    translate(0,9.2)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g clip-path="url(#pgfcp39)"><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 49.46 30.65)"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 109.53 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 173.06 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 236.6 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 300.13 30.65)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 360.21 30.65)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 113.1 11.29)"><foreignobject
    width="198.83" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RAVEN
    mean test accuracy (%)</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 41.11 39.61)"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\displaystyle{0}$</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 105.96)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{20}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 172.31)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{40}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 238.66)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{60}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.17 305.01)"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{80}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 27.22 371.35)"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{100}$</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(0.0 1.0 -1.0 0.0 18.21 106.85)"><foreignobject
    width="208.44" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">I-RAVEN
    mean test accuracy (%)</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 -56.86 90.4)"><foreignobject width="353.61" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">CNN LSTM [[16](#bib.bib16), [17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 32.8 158.08)"><foreignobject
    width="341.39" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ResNet-18 [[16](#bib.bib16),
    [17](#bib.bib17)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 145.91 64.86)"><foreignobject width="270.82" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">C-B ResNet-18 [[17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 56.29 208.17)"><foreignobject
    width="390.97" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ResNet-18
    + DRT [[16](#bib.bib16), [17](#bib.bib17)]</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 -60.47 129.88)"><foreignobject
    width="320.17" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">WReN [[16](#bib.bib16),
    [17](#bib.bib17)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 294.03 178.31)"><foreignobject width="323.51" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">LEN [[22](#bib.bib22), [17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 140.56 237.03)"><foreignobject
    width="348.23" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CoPINet [[17](#bib.bib17),
    [20](#bib.bib20)]</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 183.09 101.35)"><foreignobject width="261.9" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">C-B CoPINet [[17](#bib.bib17)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 203.32 312.34)"><foreignobject
    width="220.16" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MRNet [[18](#bib.bib18)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 46.48 184.29)"><foreignobject
    width="247.95" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NCD [[30](#bib.bib30)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 127.06 356.13)"><foreignobject
    width="213.66" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SCL [[33](#bib.bib33)]</foreignobject></g><g
    stroke="#000000" fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 70.48 352.1)"><foreignobject
    width="60.08" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">#
    params</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 332.2)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0.1 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 312.83)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1.2
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 293.46)"><foreignobject width="34.98" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1.7 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 274.09)"><foreignobject
    width="34.98" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">5.5
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 254.73)"><foreignobject width="41.9" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">11.2 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 235.36)"><foreignobject
    width="41.9" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">11.5
    M</foreignobject></g><g stroke="#000000" fill="#000000" transform="matrix(1.0
    0.0 0.0 1.0 104.31 215.99)"><foreignobject width="41.9" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">13.3 M</foreignobject></g><g stroke="#000000"
    fill="#000000" transform="matrix(1.0 0.0 0.0 1.0 104.31 196.63)"><foreignobject
    width="41.9" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">19.5
    M</foreignobject></g></g></g></svg></foreignobject></g><g transform="translate(0,395.2)
    scale(1, -1)"><foreignobject width="678.01" height="395.2" overflow="visible">²²footnotemark:
    2</foreignobject></g></g></svg>'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ²²脚注标记：2
- en: (c) RAVEN – I-RAVEN.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: (c) RAVEN – I-RAVEN。
- en: 'Figure 10: Size vs accuracy. Model performance evaluated on PGM, RAVEN and
    I-RAVEN against the number of its trainable parameters. The plots present only
    those models which were evaluated in the literature on the respective pairs of
    datasets. C-B stands for Context-blind. ²²footnotemark: 2MRNet was evaluated on
    RAVEN-FAIR instead of I-RAVEN.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：大小与准确率。模型性能在PGM、RAVEN和I-RAVEN上根据其可训练参数数量进行评估。这些图表仅展示了在相关数据集对中已被文献评估的模型。C-B代表上下文盲。²²脚注标记：2MRNet在RAVEN-FAIR上进行评估，而非I-RAVEN。
- en: 5.3 Model sizes
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 模型大小
- en: 'To the best of our knowledge, in the existing literature on DL methods for
    solving RPMs, the size of the models—measured in the number of parameters—was
    never discussed. However, some models, e.g. MRNet [[18](#bib.bib18)] or SRAN [[17](#bib.bib17)]
    use deep visual backbones based on ResNet [[100](#bib.bib100)] which results in
    huge number of trainable parameters. The numbers of parameters of all discussed
    models with published open-source implementations are compared in Table [V](#S5.T5
    "TABLE V ‣ 5.2 Results on (I-)RAVEN ‣ 5 Evaluating machine intelligence with RPMs
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices"). Model sizes range from hundreds of thousands up to tens of millions,
    with the largest one (SRAN) having around 320 times more parameters than the smallest
    one (SCL).'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '据我们了解，在现有关于解决RPM的深度学习方法的文献中，从未讨论过模型的大小——以参数数量来衡量。然而，一些模型，例如MRNet [[18](#bib.bib18)]或SRAN [[17](#bib.bib17)]，使用了基于ResNet [[100](#bib.bib100)]的深度视觉骨干网，导致了大量的可训练参数。所有讨论的模型中具有公开源代码实现的模型参数数量在表[V](#S5.T5
    "TABLE V ‣ 5.2 Results on (I-)RAVEN ‣ 5 Evaluating machine intelligence with RPMs
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices")中进行了比较。模型大小从数十万到数千万不等，其中最大模型（SRAN）的参数数量约为最小模型（SCL）的320倍。'
- en: 'Although a large number of trainable parameters is often necessary in various
    real-world problems, e.g. in computer vision [[125](#bib.bib125), [126](#bib.bib126)]
    or natural language processing [[127](#bib.bib127), [128](#bib.bib128)] where
    practically unlimited supply of the training data exists, in the context of RPMs
    this is no longer the case. Figure [2](#footnotex11 "footnote 2 ‣ Figure 10 ‣
    5.2 Results on (I-)RAVEN ‣ 5 Evaluating machine intelligence with RPMs ‣ Deep
    Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices") shows that in RPM benchmarks the best results are achieved by the model
    with the smallest number of parameters – SCL. Although a bit surprising, this
    observation shows that a relatively small model can effectively learn to solve
    RPMs with competitive generalisation ability (cf. Table [III](#S5.T3 "TABLE III
    ‣ 5.1 Results on PGM ‣ 5 Evaluating machine intelligence with RPMs ‣ Deep Learning
    Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")).
    In other words, these results prove that in the case of RPMs, large (in terms
    of the number of parameters) visual backbones (e.g. deep ResNets) are not necessary,
    and instead, the existence of efficient parameter-sharing modules is crucial.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管在各种现实世界问题中，例如计算机视觉 [[125](#bib.bib125), [126](#bib.bib126)]或自然语言处理 [[127](#bib.bib127),
    [128](#bib.bib128)]中，经常需要大量的可训练参数，因为几乎可以获得无限量的训练数据，但在RPM的背景下情况并非如此。图[2](#footnotex11
    "footnote 2 ‣ Figure 10 ‣ 5.2 Results on (I-)RAVEN ‣ 5 Evaluating machine intelligence
    with RPMs ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s
    Progressive Matrices")显示，在RPM基准测试中，最佳结果是由参数数量最少的模型——SCL获得的。尽管这有些令人惊讶，但这一观察结果表明，相对较小的模型能够有效地学习解决RPM，并具有竞争力的泛化能力（参见表[III](#S5.T3
    "TABLE III ‣ 5.1 Results on PGM ‣ 5 Evaluating machine intelligence with RPMs
    ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive
    Matrices")）。换句话说，这些结果证明了在RPM的情况下，大型（从参数数量的角度看）视觉骨干网（例如深度ResNet）并非必需，相反，存在高效的参数共享模块是至关重要的。'
- en: While this is yet to be verified in other AVR tasks, we hypothesize that due
    to their visual similarity (e.g. lack of texture and presence of 2D greyscale
    shapes), analogous architectural decisions should be made when designing AVR machine
    solvers in general.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这一点尚待在其他AVR任务中验证，但我们推测，由于它们的视觉相似性（例如缺乏纹理和存在二维灰度形状），在设计AVR机器求解器时应做出类似的建筑决策。
- en: 6 Discussion
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: The initial reason for introducing RPMs into DL literature, was to use these
    problems as a proxy for estimating machine intelligence. It turned out, however,
    that the ability of spatial and abstract reasoning is also crucial for the development
    of intelligent systems in various other settings [[129](#bib.bib129)]. Consequently,
    methods proposed in the context of RPMs are oftentimes relevant in other research
    and practical contexts. In this section, we link the discussed approaches to advancements
    in other fields and highlight the main unsolved challenges and open questions
    left for investigation in future work.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 将RPM（Relation Prediction Method）引入深度学习文献的初衷是将这些问题作为估计机器智能的代理。然而，事实证明，空间和抽象推理的能力在各种其他环境中也对智能系统的发展至关重要[[129](#bib.bib129)]。因此，在RPM背景下提出的方法在其他研究和实际应用背景中往往具有相关性。在本节中，我们将讨论的方法与其他领域的进展联系起来，并突出未来工作中主要未解决的挑战和开放问题。
- en: 6.1 Seeds and fruits of RPM research
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 RPM研究的种子与成果
- en: Relation Network, the fundamental component of discussed models, e.g. WReN,
    LEN or MLRN, has already demonstrated its usefulness in multiple tasks. In [[130](#bib.bib130)]
    the authors consider a 3D human pose estimation problem [[131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133)] and propose a DL algorithm to tackle this challenge. The designed
    method employs an RN to capture relations among different body parts. Similarly,
    in the context of semantic segmentation [[134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)], the idea of capturing long-distance spatial relationships
    between entities using RN is further explored. It is shown that a relational reasoning
    component can be used to augment CNN feature maps by exploiting both channel-wise
    and spatial feature relations [[137](#bib.bib137)]. Such incorporation of the
    global context is a recurring theme in works that exploit RNs (and is also explored
    in other neural modules [[138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141)]). RN was further applied to model spatio-temporal interactions
    between human actors, objects and scene elements in the context of action recognition [[142](#bib.bib142)],
    to train an effective image recognition model in a contrastive self-supervised
    setting [[143](#bib.bib143)], or to provide a mechanism for relational reasoning
    over structured representations for a deep reinforcement learning agent [[144](#bib.bib144)].
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 关系网络（Relation Network），作为所讨论模型（如WReN、LEN或MLRN）的基本组成部分，已经在多个任务中展示了其有效性。在[[130](#bib.bib130)]中，作者考虑了一个三维人体姿态估计问题[[131](#bib.bib131)、[132](#bib.bib132)、[133](#bib.bib133)]，并提出了一种深度学习算法来应对这一挑战。该方法设计中采用了RN来捕捉不同身体部位之间的关系。同样，在语义分割的背景下[[134](#bib.bib134)、[135](#bib.bib135)、[136](#bib.bib136)]，使用RN捕捉实体之间的远距离空间关系的想法得到了进一步探索。研究表明，关系推理组件可以通过利用通道和空间特征关系来增强CNN特征图[[137](#bib.bib137)]。这种全球上下文的融入是利用RN的工作的一个反复出现的主题（也在其他神经模块中得到了探索[[138](#bib.bib138)、[139](#bib.bib139)、[140](#bib.bib140)、[141](#bib.bib141)]）。RN进一步应用于建模人类行为者、物体和场景元素之间的时空互动，以进行动作识别[[142](#bib.bib142)]，在对比自监督设置中训练有效的图像识别模型[[143](#bib.bib143)]，或为结构化表示的关系推理提供机制，以用于深度强化学习代理[[144](#bib.bib144)]。
- en: In order to successfully solve RPMs and learn to formulate analogies, multiple
    discussed works have employed various forms of contrastive mechanisms either directly
    in the model architecture [[20](#bib.bib20), [35](#bib.bib35)] or in the objective
    function [[20](#bib.bib20), [19](#bib.bib19), [21](#bib.bib21)]. Contrastive approaches [[145](#bib.bib145),
    [146](#bib.bib146), [147](#bib.bib147)] are especially useful for self-supervised
    learning, where the availability of labelled data is scarce. The importance of
    such methods was already demonstrated in the context of computer vision [[148](#bib.bib148),
    [149](#bib.bib149), [143](#bib.bib143)], natural language processing [[150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152)], speech recognition [[153](#bib.bib153),
    [154](#bib.bib154), [155](#bib.bib155)], or reinforcement learning [[156](#bib.bib156),
    [157](#bib.bib157), [158](#bib.bib158)].
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功解决 RPM 并学会制定类比，多项讨论的工作采用了各种形式的对比机制，或者直接在模型架构中 [[20](#bib.bib20), [35](#bib.bib35)]，或者在目标函数中
    [[20](#bib.bib20), [19](#bib.bib19), [21](#bib.bib21)]。对比方法 [[145](#bib.bib145),
    [146](#bib.bib146), [147](#bib.bib147)] 对于自监督学习尤其有用，其中标记数据的可用性稀缺。此类方法的重要性已经在计算机视觉
    [[148](#bib.bib148), [149](#bib.bib149), [143](#bib.bib143)]，自然语言处理 [[150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152)]，语音识别 [[153](#bib.bib153), [154](#bib.bib154),
    [155](#bib.bib155)] 或强化学习 [[156](#bib.bib156), [157](#bib.bib157), [158](#bib.bib158)]
    的背景下得到了证明。
- en: RPM solving methods are often directly applicable, after minor adjustments only,
    to related abstract reasoning tasks. RN was found to be competitive to other models
    in solving arithmetic visual reasoning tasks [[59](#bib.bib59)], while WReN-based
    models were found to be one of the top performing methods in solving the visual
    analogy problems [[56](#bib.bib56)] and abstract reasoning matrices structurally
    similar to RPMs [[90](#bib.bib90)].
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: RPM 求解方法通常在仅做少量调整后，直接适用于相关的抽象推理任务。RN 在解决算术视觉推理任务时发现与其他模型具有竞争力 [[59](#bib.bib59)]，而基于
    WReN 的模型被发现是解决视觉类比问题 [[56](#bib.bib56)] 和结构上类似于 RPM 的抽象推理矩阵 [[90](#bib.bib90)]
    的最佳方法之一。
- en: '![Refer to caption](img/e1b237606f30c8440d3667c222a80cd5.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e1b237606f30c8440d3667c222a80cd5.png)'
- en: (a) RAVEN, $i\in\{0,1,\ldots,6\}$
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: (a) RAVEN，$i\in\{0,1,\ldots,6\}$
- en: '![Refer to caption](img/fe36157da9c525d662dd07b35031bed9.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fe36157da9c525d662dd07b35031bed9.png)'
- en: (b) PGM, $i\in\{0,2,\ldots,12\}$
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: (b) PGM，$i\in\{0,2,\ldots,12\}$
- en: 'Figure 11: Low-sample regime. The plots present the performance of two models
    trained on splits of a) RAVEN and b) PGM datasets. The split sizes equal to $N/2^{i}$,
    where $N$ denotes the size of the original dataset, which equals to 42,000 for
    RAVEN and 1,200,000 for PGM. The results were reported in [[35](#bib.bib35)].'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：低样本情况。图表展示了两个模型在 a) RAVEN 和 b) PGM 数据集分割上的表现。分割大小等于 $N/2^{i}$，其中 $N$ 表示原始数据集的大小，对
    RAVEN 为 42,000，对 PGM 为 1,200,000。结果见 [[35](#bib.bib35)]。
- en: 6.2 Challenges and open problems
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 挑战与开放问题
- en: 'Even though the discussed works have embarked on a quest to measure machine
    intelligence by means of evaluating their performance on RPM benchmarks, some
    may oppose the validity of this path. When solving an RPM, a human solver is often
    faced with a task that he/she has not encountered before, which tests the ability
    of adaptive problem solving. Contrary to humans, the majority of current DL approaches
    use thousands [[16](#bib.bib16)] or millions [[12](#bib.bib12)] training samples
    beforehand. In addition, the performance of these models rapidly deteriorates
    when the size of the training corpora decreases, as illustrated in Fig. [11](#S6.F11
    "Figure 11 ‣ 6.1 Seeds and fruits of RPM research ‣ 6 Discussion ‣ Deep Learning
    Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices").
    Conversely, humans are able not only to grasp the concepts after familiarising
    themselves with just a few examples, but also to extrapolate knowledge gained
    when solving simple matrices to more advanced ones. Given this notable contrast
    it is worth to advocate the search for efficient methods for solving RPMs without
    access to huge training sets. We are convinced that it is worthwhile to pursue
    certain emerging pathways discussed in this paper that are explicitly designed
    to probe DL algorithms in few-shot learning setups [[58](#bib.bib58), [159](#bib.bib159)].'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管讨论中的工作已经开始通过评估其在RPM基准测试上的表现来测量机器智能，但仍有人可能反对这种方法的有效性。在解决RPM时，人类解题者通常面对的是之前未曾遇到的任务，这测试了其适应性问题解决能力。与人类相反，当前大多数DL方法在训练之前使用了数千[[16](#bib.bib16)]或数百万[[12](#bib.bib12)]个样本。此外，如图[11](#S6.F11
    "Figure 11 ‣ 6.1 Seeds and fruits of RPM research ‣ 6 Discussion ‣ Deep Learning
    Methods for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")所示，这些模型的性能在训练语料库规模减少时迅速恶化。相反，人类不仅能够在熟悉少量示例后掌握概念，还能够将解决简单矩阵时获得的知识外推到更复杂的矩阵中。鉴于这种显著的对比，值得倡导在没有大规模训练集的情况下寻找解决RPM的高效方法。我们相信，探索本文讨论的某些新兴途径是值得的，这些途径专门设计用于在少样本学习设置中测试DL算法[[58](#bib.bib58),
    [159](#bib.bib159)]。'
- en: Another way of ensuring that the developed pattern analysis algorithms are benchmarked
    similarly to humans in new environments is to follow the perspective of the seminal
    PGM paper [[12](#bib.bib12)]. By explicitly defining various generalisation regimes
     Barrett et al. [[12](#bib.bib12)] allowed to directly measure generalisation
    (performance in new settings) of DL methods for solving RPMs. However, as of today
    no effective method capable of achieving human-like performance in all regimes
    was proposed. In fact, existing approaches seem to possess specific generalisation
    abilities that are rather a side effect than a deliberate choice. While recent
    work [[57](#bib.bib57)] shows that neural models can generalise in AVR tasks that
    focus on extrapolation, which constitutes one of the most demanding PGM regimes,
    there is still a long way to construct a universal learning system that would
    excel in all regimes.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种确保开发的模式分析算法在新环境中与人类相似的基准测试的方法是遵循开创性的PGM论文[[12](#bib.bib12)]的观点。通过明确界定各种泛化模式，Barrett等人[[12](#bib.bib12)]
    使得能够直接测量DL方法在解决RPM时的泛化能力（在新环境中的表现）。然而，至今尚未提出能够在所有模式下实现类人表现的有效方法。事实上，现有的方法似乎具有特定的泛化能力，这更像是副作用而非刻意选择。虽然近期的研究[[57](#bib.bib57)]表明神经模型在专注于外推的AVR任务中能够进行泛化，这构成了最具挑战性的PGM模式之一，但构建一个在所有模式下都表现优异的通用学习系统仍然任重道远。
- en: Another key characteristic that differentiates humans from current AI systems
    is the ability to solve various types of AVR problems with limited training. While
    humans are known to be able to generalise and transfer knowledge between problems,
    such property has only been briefly demonstrated by the existing DL methods. Some
    works that evaluate DL approaches on the RAVEN dataset show that models trained
    on one configuration may learn to build abstractions useful for solving matrices
    belonging to other configurations [[16](#bib.bib16), [34](#bib.bib34), [27](#bib.bib27)].
    In [[15](#bib.bib15)], an RPM solving model was shown to be adaptable to solving
    odd-one-out tasks that consisted of similar input images. While the above examples
    indicate, to some extent, the ability of DL algorithms to i.i.d. generalisation
    in the context of AVR, knowledge reuse between multiple distinct AVR problems
    remains to be investigated in future work.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 区分人类与当前AI系统的另一个关键特征是解决各种类型AVR问题的能力，且只需有限的训练。尽管已知人类能够在问题之间进行泛化和知识转移，但现有的深度学习方法仅在短暂的时间内展示了这种属性。一些在RAVEN数据集上评估深度学习方法的工作表明，训练于某一配置的模型可能学习构建对解决属于其他配置的矩阵有用的抽象
    [[16](#bib.bib16), [34](#bib.bib34), [27](#bib.bib27)]。在 [[15](#bib.bib15)] 中，展示了一个RPM解决模型能够适应解决由类似输入图像组成的奇异任务。虽然上述例子在一定程度上表明了深度学习算法在AVR背景下的独立同分布泛化能力，但多个不同AVR问题之间的知识重用仍需在未来的工作中进一步研究。
- en: 7 Conclusion
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This paper discusses recent progress in applying DL methods to solving RPMs,
    summarises various methods of learning to solve these tasks, reviews the existing
    RPM benchmark sets, and categorises the DL models employed in this field. Also,
    by aggregating results of recently published methods, it brings to attention the
    most challenging aspects of RPM problems, which to this day remain primarily unsolved.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了将深度学习方法应用于解决RPM的最新进展，总结了各种学习解决这些任务的方法，回顾了现有的RPM基准数据集，并分类了该领域中使用的深度学习模型。此外，通过汇总最近发表的方法的结果，它突出了RPM问题中最具挑战性的方面，这些方面至今仍主要未解决。
- en: The paper argues that while RPMs were initially proposed as a task for measuring
    human intelligence and were later employed as a proxy for estimating machine intelligence,
    they additionally offer a comprehensible playground for developing and testing
    abstract and relational reasoning approaches. Viewed from this perspective, advancements
    in RPM research are applicable to a broad spectrum of other domains where spatial
    and abstract reasoning skills are required.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 论文指出，尽管RPM最初被提议作为衡量人类智力的任务，并随后作为估计机器智力的替代工具使用，但它们还提供了一个易于理解的实验场所，用于开发和测试抽象及关系推理方法。从这一角度看，RPM研究的进展可以应用于广泛的其他领域，在这些领域中需要空间和抽象推理技能。
- en: Even though the task of solving RPMs has seen a wide interest within the DL
    community in recent years, its core challenges remain unattained. We hope that
    by collating the advances in methods for solving RPMs, this survey will stimulate
    progress in future AVR research.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近年来解决RPM的任务在深度学习社区中引起了广泛关注，但其核心挑战仍未得到解决。我们希望通过汇总解决RPM的方法进展，这项调查将刺激未来AVR研究的进展。
- en: References
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Detterman and Sternberg [1986] D. K. Detterman and R. J. Sternberg, *What is
    intelligence?: Contemporary viewpoints on its nature and definition*.   Ablex,
    1986.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Detterman和Sternberg [1986] D. K. Detterman和R. J. Sternberg, *什么是智力?: 关于其本质和定义的当代观点*。
    Ablex, 1986.'
- en: Legg et al. [2007] S. Legg, M. Hutter *et al.*, “A collection of definitions
    of intelligence,” *Frontiers in Artificial Intelligence and applications*, vol.
    157, p. 17, 2007.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Legg等 [2007] S. Legg, M. Hutter *等*, “智力的定义集合，” *人工智能与应用前沿*, vol. 157, p. 17,
    2007.
- en: 'Hernández-Orallo [2017] J. Hernández-Orallo, *The measure of all minds: evaluating
    natural and artificial intelligence*.   Cambridge University Press, 2017.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hernández-Orallo [2017] J. Hernández-Orallo, *所有智力的度量: 评估自然和人工智力*。 剑桥大学出版社,
    2017.'
- en: Snow et al. [1984] R. E. Snow, P. C. Kyllonen, and B. Marshalek, “The topography
    of ability and learning correlations,” *Advances in the psychology of human intelligence*,
    vol. 2, no. S 47, p. 103, 1984.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snow等 [1984] R. E. Snow, P. C. Kyllonen, 和 B. Marshalek, “能力和学习相关性的地形图，” *人类智力心理学的进展*,
    vol. 2, no. S 47, p. 103, 1984.
- en: 'Carpenter et al. [1990] P. A. Carpenter, M. A. Just, and P. Shell, “What one
    intelligence test measures: a theoretical account of the processing in the Raven
    Progressive Matrices Test.” *Psychological review*, vol. 97, no. 3, p. 404, 1990.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carpenter 等 [1990] P. A. Carpenter, M. A. Just 和 P. Shell，“**一个智力测试测量的内容**：关于拉文渐进矩阵测试中的处理的理论说明。”
    *心理学评论*，第97卷，第3期，第404页，1990年。
- en: 'Hofstadter et al. [1979] D. R. Hofstadter *et al.*, *Gödel, Escher, Bach: an
    eternal golden braid*.   Basic books New York, 1979, vol. 13.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hofstadter 等 [1979] D. R. Hofstadter *等*，*哥德尔、艾舍尔、巴赫：一个永恒的金色编织*。 Basic books
    纽约，1979年，第13卷。
- en: 'Raven [1936] J. C. Raven, “Mental tests used in genetic studies: The performance
    of related individuals on tests mainly educative and mainly reproductive,” *Master’s
    thesis, University of London*, 1936.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raven [1936] J. C. Raven，“**在遗传研究中使用的心理测验**：相关个体在主要教育性和主要生殖性测试上的表现，” *伦敦大学硕士论文*，1936年。
- en: Raven and Court [1998] J. C. Raven and J. H. Court, *Raven’s progressive matrices
    and vocabulary scales*.   Oxford pyschologists Press Oxford, England, 1998.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raven 和 Court [1998] J. C. Raven 和 J. H. Court，*拉文的渐进矩阵和词汇量表*。 Oxford psychologists
    Press 牛津，英格兰，1998年。
- en: LeCun et al. [2015] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*,
    vol. 521, no. 7553, pp. 436–444, 2015.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等 [2015] Y. LeCun, Y. Bengio 和 G. Hinton，“**深度学习**，” *自然*，第521卷，第7553期，第436–444页，2015年。
- en: Raghu and Schmidt [2020] M. Raghu and E. Schmidt, “A survey of deep learning
    for scientific discovery,” *arXiv preprint arXiv:2003.11755*, 2020.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raghu 和 Schmidt [2020] M. Raghu 和 E. Schmidt，“**深度学习在科学发现中的调查**，” *arXiv预印本
    arXiv:2003.11755*，2020年。
- en: Hoshen and Werman [2017] D. Hoshen and M. Werman, “IQ of Neural Networks,” *arXiv
    preprint arXiv:1710.01692*, 2017.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoshen 和 Werman [2017] D. Hoshen 和 M. Werman，“**神经网络的IQ**，” *arXiv预印本 arXiv:1710.01692*，2017年。
- en: Barrett et al. [2018] D. Barrett, F. Hill, A. Santoro, A. Morcos, and T. Lillicrap,
    “Measuring abstract reasoning in neural networks,” in *International Conference
    on Machine Learning*.   PMLR, 2018, pp. 511–520.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barrett 等 [2018] D. Barrett, F. Hill, A. Santoro, A. Morcos 和 T. Lillicrap，“**神经网络中的抽象推理测量**，”在
    *国际机器学习会议*。 PMLR，2018年，第511–520页。
- en: 'Matzen et al. [2010] L. E. Matzen, Z. O. Benz, K. R. Dixon, J. Posey, J. K.
    Kroger, and A. E. Speed, “Recreating raven’s: Software for systematically generating
    large numbers of raven-like matrix problems with normed properties,” *Behavior
    research methods*, vol. 42, no. 2, pp. 525–541, 2010.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matzen 等 [2010] L. E. Matzen, Z. O. Benz, K. R. Dixon, J. Posey, J. K. Kroger
    和 A. E. Speed，“**重建拉文**：一种用于系统生成大量类似拉文的矩阵问题的软件，具有规范化属性，” *行为研究方法*，第42卷，第2期，第525–541页，2010年。
- en: Wang and Su [2015] K. Wang and Z. Su, “Automatic generation of raven’s progressive
    matrices,” in *Twenty-Fourth International Joint Conference on Artificial Intelligence*,
    2015.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Su [2015] K. Wang 和 Z. Su，“**拉文的渐进矩阵**的自动生成，”在 *第二十四届国际人工智能联合会议*，2015年。
- en: 'Mańdziuk and Żychowski [2019] J. Mańdziuk and A. Żychowski, “DeepIQ: A Human-Inspired
    AI System for Solving IQ Test Problems,” in *2019 International Joint Conference
    on Neural Networks (IJCNN)*.   IEEE, 2019, pp. 1–8.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mańdziuk 和 Żychowski [2019] J. Mańdziuk 和 A. Żychowski，“**DeepIQ**: 一个受人类启发的AI系统，用于解决IQ测试问题，”在
    *2019年国际神经网络联合会议 (IJCNN)*。 IEEE，2019年，第1–8页。'
- en: 'Zhang et al. [2019a] C. Zhang, F. Gao, B. Jia, Y. Zhu, and S.-C. Zhu, “Raven:
    A dataset for relational and analogical visual reasoning,” in *Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition*, 2019, pp. 5317–5327.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 [2019a] C. Zhang, F. Gao, B. Jia, Y. Zhu 和 S.-C. Zhu，“**Raven**: 一个用于关系和类比视觉推理的数据集，”在
    *IEEE计算机视觉与模式识别会议论文集*，2019年，第5317–5327页。'
- en: Hu et al. [2021] S. Hu, Y. Ma, X. Liu, Y. Wei, and S. Bai, “Stratified rule-aware
    network for abstract visual reasoning,” in *AAAI Conference on Artificial Intelligence
    (AAAI)*, 2021.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 [2021] S. Hu, Y. Ma, X. Liu, Y. Wei 和 S. Bai，“**分层规则感知网络**用于抽象视觉推理，”在 *AAAI人工智能会议*，2021年。
- en: Benny et al. [2021] Y. Benny, N. Pekar, and L. Wolf, “Scale-localized abstract
    reasoning,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2021, pp. 12 557–12 565.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benny 等 [2021] Y. Benny, N. Pekar 和 L. Wolf，“**尺度局部化抽象推理**，”在 *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第12,557–12,565页。
- en: Małkiński and Mańdziuk [2020] M. Małkiński and J. Mańdziuk, “Multi-label contrastive
    learning for abstract visual reasoning,” *arXiv preprint arXiv:2012.01944*, 2020.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Małkiński 和 Mańdziuk [2020] M. Małkiński 和 J. Mańdziuk，“**用于抽象视觉推理的多标签对比学习**，”
    *arXiv预印本 arXiv:2012.01944*，2020年。
- en: Zhang et al. [2019b] C. Zhang, B. Jia, F. Gao, Y. Zhu, H. Lu, and S.-C. Zhu,
    “Learning perceptual inference by contrasting,” in *Advances in Neural Information
    Processing Systems*, 2019, pp. 1075–1087.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 [2019b] C. Zhang, B. Jia, F. Gao, Y. Zhu, H. Lu 和 S.-C. Zhu，“通过对比学习感知推理，”
    见于 *Advances in Neural Information Processing Systems*，2019，第1075–1087页。
- en: Kim et al. [2020] Y. Kim, J. Shin, E. Yang, and S. J. Hwang, “Few-shot visual
    reasoning with meta-analogical contrastive learning,” *Advances in Neural Information
    Processing Systems*, vol. 33, 2020.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 [2020] Y. Kim, J. Shin, E. Yang 和 S. J. Hwang，“通过元类比对比学习进行少样本视觉推理，” *Advances
    in Neural Information Processing Systems*，第33卷，2020。
- en: Zheng et al. [2019] K. Zheng, Z.-J. Zha, and W. Wei, “Abstract reasoning with
    distracting features,” in *Advances in Neural Information Processing Systems*,
    2019, pp. 5842–5853.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等 [2019] K. Zheng, Z.-J. Zha 和 W. Wei，“带有干扰特征的抽象推理，” 见于 *Advances in Neural
    Information Processing Systems*，2019，第5842–5853页。
- en: Steenbrugge et al. [2018] X. Steenbrugge, S. Leroux, T. Verbelen, and B. Dhoedt,
    “Improving generalization for abstract reasoning tasks using disentangled feature
    representations,” *arXiv preprint arXiv:1811.04784*, 2018.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Steenbrugge 等 [2018] X. Steenbrugge, S. Leroux, T. Verbelen 和 B. Dhoedt，“通过解缠特征表示提高抽象推理任务的泛化能力，”
    *arXiv 预印本 arXiv:1811.04784*，2018。
- en: Pekar et al. [2020] N. Pekar, Y. Benny, and L. Wolf, “Generating correct answers
    for progressive matrices intelligence tests,” in *Advances in Neural Information
    Processing Systems*, vol. 33, 2020, pp. 7390–7400.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pekar 等 [2020] N. Pekar, Y. Benny 和 L. Wolf，“为进阶矩阵智力测试生成正确答案，” 见于 *Advances
    in Neural Information Processing Systems*，第33卷，2020年，第7390–7400页。
- en: Hua and Kunda [2020] T. Hua and M. Kunda, “Modeling gestalt visual reasoning
    on raven’s progressive matrices using generative image inpainting techniques.”
    in *CogSci*, 2020.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hua 和 Kunda [2020] T. Hua 和 M. Kunda，“利用生成图像修补技术建模 Raven 进阶矩阵上的整体视觉推理。” 见于 *CogSci*，2020。
- en: Shi et al. [2021] F. Shi, B. Li, and X. Xue, “Raven’s progressive matrices completion
    with latent gaussian process priors,” in *Proceedings of the AAAI Conference on
    Artificial Intelligence*, vol. 35, no. 11, 2021, pp. 9612–9620.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等 [2021] F. Shi, B. Li 和 X. Xue，“带有潜在高斯过程先验的 Raven 进阶矩阵完成，” 见于 *Proceedings
    of the AAAI Conference on Artificial Intelligence*，第35卷，第11期，2021年，第9612–9620页。
- en: Zhang et al. [2021] C. Zhang, B. Jia, S.-C. Zhu, and Y. Zhu, “Abstract spatial-temporal
    reasoning via probabilistic abduction and execution,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 9736–9746.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 [2021] C. Zhang, B. Jia, S.-C. Zhu 和 Y. Zhu，“通过概率性归纳和执行进行抽象时空推理，” 见于
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*，2021，第9736–9746页。
- en: Zhuo and Kankanhalli [2020] T. Zhuo and M. Kankanhalli, “Solving raven’s progressive
    matrices with neural networks,” *arXiv preprint arXiv:2002.01646*, 2020.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuo 和 Kankanhalli [2020] T. Zhuo 和 M. Kankanhalli，“使用神经网络解决 Raven 的进阶矩阵，” *arXiv
    预印本 arXiv:2002.01646*，2020。
- en: Kiat et al. [2020] N. Q. W. Kiat, D. Wang, and M. Jamnik, “Pairwise relations
    discriminator for unsupervised raven’s progressive matrices,” *arXiv preprint
    arXiv:2011.01306*, 2020.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiat 等 [2020] N. Q. W. Kiat, D. Wang 和 M. Jamnik，“用于无监督 Raven 进阶矩阵的成对关系判别器，”
    *arXiv 预印本 arXiv:2011.01306*，2020。
- en: Zhuo et al. [2021] T. Zhuo, Q. Huang, and M. Kankanhalli, “Unsupervised abstract
    reasoning for raven’s problem matrices,” *IEEE Transactions on Image Processing*,
    vol. 30, pp. 8332–8341, 2021.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuo 等 [2021] T. Zhuo, Q. Huang 和 M. Kankanhalli，“用于 Raven 问题矩阵的无监督抽象推理，” *IEEE
    Transactions on Image Processing*，第30卷，第8332–8341页，2021。
- en: Hahne et al. [2019] L. Hahne, T. Lüddecke, F. Wörgötter, and D. Kappel, “Attention
    on abstract visual reasoning,” *arXiv preprint arXiv:1911.05990*, 2019.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hahne 等 [2019] L. Hahne, T. Lüddecke, F. Wörgötter 和 D. Kappel，“关注抽象视觉推理，” *arXiv
    预印本 arXiv:1911.05990*，2019。
- en: Jahrens and Martinetz [2020] M. Jahrens and T. Martinetz, “Solving raven’s progressive
    matrices with multi-layer relation networks,” in *2020 International Joint Conference
    on Neural Networks (IJCNN)*.   IEEE, 2020, pp. 1–6.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jahrens 和 Martinetz [2020] M. Jahrens 和 T. Martinetz，“使用多层关系网络解决 Raven 的进阶矩阵，”
    见于 *2020 International Joint Conference on Neural Networks (IJCNN)*。 IEEE，2020，第1–6页。
- en: 'Wu et al. [2020] Y. Wu, H. Dong, R. Grosse, and J. Ba, “The Scattering Compositional
    Learner: Discovering Objects, Attributes, Relationships in Analogical Reasoning,”
    *arXiv preprint arXiv:2007.04212*, 2020.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 [2020] Y. Wu, H. Dong, R. Grosse 和 J. Ba，“散射组合学习者：在类比推理中发现对象、属性、关系，” *arXiv
    预印本 arXiv:2007.04212*，2020。
- en: Spratley et al. [2020] S. Spratley, K. Ehinger, and T. Miller, “A closer look
    at generalisation in raven,” in *Computer Vision – ECCV 2020*.   Springer, 2020,
    pp. 601–616.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spratley 等 [2020] S. Spratley, K. Ehinger 和 T. Miller，“对 Raven 中泛化的更深入观察，” 见于
    *Computer Vision – ECCV 2020*。 Springer，2020，第601–616页。
- en: Zhuo and Kankanhalli [2021] T. Zhuo and M. Kankanhalli, “Effective abstract
    reasoning with dual-contrast network,” in *International Conference on Learning
    Representations (ICLR)*, 2021.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuo 和 Kankanhalli [2021] T. Zhuo 和 M. Kankanhalli, “使用双重对比网络的有效抽象推理，” 收录于 *国际学习表征会议（ICLR）*，2021
    年。
- en: Wang et al. [2020] D. Wang, M. Jamnik, and P. Lio, “Abstract diagrammatic reasoning
    with multiplex graph networks,” in *International Conference on Learning Representations
    (ICLR)*, 2020.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2020] D. Wang, M. Jamnik, 和 P. Lio, “利用多重图网络进行抽象图示推理，” 收录于 *国际学习表征会议（ICLR）*，2020
    年。
- en: Rahaman et al. [2021] N. Rahaman, M. W. Gondal, S. Joshi, P. Gehler, Y. Bengio,
    F. Locatello, and B. Schölkopf, “Dynamic inference with neural interpreters,”
    *Advances in Neural Information Processing Systems*, vol. 34, 2021.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahaman 等 [2021] N. Rahaman, M. W. Gondal, S. Joshi, P. Gehler, Y. Bengio, F.
    Locatello, 和 B. Schölkopf, “使用神经解释器进行动态推理，” *神经信息处理系统进展*，第 34 卷，2021 年。
- en: Mitchell [2021] M. Mitchell, “Abstraction and analogy-making in artificial intelligence,”
    *arXiv preprint arXiv:2102.10717*, 2021.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell [2021] M. Mitchell, “人工智能中的抽象与类比推理，” *arXiv 预印本 arXiv:2102.10717*，2021
    年。
- en: Evans [1964] T. G. Evans, “A heuristic program to solve geometric-analogy problems,”
    in *Proceedings of the April 21-23, 1964, spring joint computer conference*, 1964,
    pp. 327–338.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evans [1964] T. G. Evans, “一个解决几何类比问题的启发式程序，” 收录于 *1964 年 4 月 21-23 日春季联合计算机会议论文集*，1964
    年，第 327–338 页。
- en: 'Foundalis [2006] H. E. Foundalis, “Phaeaco: A cognitive architecture inspired
    by bongard’s problems.” PhD dissertation, Indiana University, 2006.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foundalis [2006] H. E. Foundalis, “Phaeaco：一种受 Bongard 问题启发的认知架构。” 印第安纳大学博士学位论文，2006
    年。
- en: Strannegård et al. [2013] C. Strannegård, S. Cirillo, and V. Ström, “An anthropomorphic
    method for progressive matrix problems,” *Cognitive Systems Research*, vol. 22,
    pp. 35–46, 2013.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strannegård 等 [2013] C. Strannegård, S. Cirillo, 和 V. Ström, “一种用于渐进矩阵问题的人性化方法，”
    *认知系统研究*，第 22 卷，第 35–46 页，2013 年。
- en: Gentner [1980] D. Gentner, “The structure of analogical models in science.”
    BOLT BERANEK AND NEWMAN INC CAMBRIDGE MA, Tech. Rep., 1980.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gentner [1980] D. Gentner, “科学中类比模型的结构。” BOLT BERANEK AND NEWMAN INC CAMBRIDGE
    MA, 技术报告，1980 年。
- en: Falkenhainer et al. [1986] B. Falkenhainer, K. D. Forbus, and D. Gentner, *The
    structure-mapping engine*.   Department of Computer Science, University of Illinois
    at Urbana-Champaign, 1986, vol. 1275.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falkenhainer 等 [1986] B. Falkenhainer, K. D. Forbus, 和 D. Gentner, *结构映射引擎*。伊利诺伊大学厄本那-香槟分校计算机科学系，1986
    年，第 1275 卷。
- en: Lovett et al. [2007] A. Lovett, K. Forbus, and J. Usher, “Analogy with qualitative
    spatial representations can simulate solving raven’s progressive matrices,” in
    *Proceedings of the Annual Meeting of the Cognitive Science Society*, vol. 29,
    no. 29, 2007.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lovett 等 [2007] A. Lovett, K. Forbus, 和 J. Usher, “使用定性空间表征的类比可以模拟解决 Raven 渐进矩阵，”
    收录于 *认知科学学会年会论文集*，第 29 卷，第 29 期，2007 年。
- en: Lovett et al. [2010] ——, “A structure-mapping model of raven’s progressive matrices,”
    in *Proceedings of the Annual Meeting of the Cognitive Science Society*, vol. 32,
    no. 32, 2010.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lovett 等 [2010] ——, “Raven 渐进矩阵的结构映射模型，” 收录于 *认知科学学会年会论文集*，第 32 卷，第 32 期，2010
    年。
- en: 'Kunda et al. [2010] M. Kunda, K. McGreggor, and A. Goel, “Taking a look (literally!)
    at the raven’s intelligence test: Two visual solution strategies,” in *Proceedings
    of the Annual Meeting of the Cognitive Science Society*, vol. 32, no. 32, 2010.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kunda 等 [2010] M. Kunda, K. McGreggor, 和 A. Goel, “从视觉上看 Raven 智力测试（字面意思！）：两种视觉解决策略，”
    收录于 *认知科学学会年会论文集*，第 32 卷，第 32 期，2010 年。
- en: Kunda et al. [2012] ——, “Reasoning on the raven’s advanced progressive matrices
    test with iconic visual representations,” in *Proceedings of the Annual Meeting
    of the Cognitive Science Society*, vol. 34, no. 34, 2012.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kunda 等 [2012] ——, “使用图标视觉表征在 Raven 高级渐进矩阵测试中的推理，” 收录于 *认知科学学会年会论文集*，第 34 卷，第
    34 期，2012 年。
- en: McGreggor et al. [2010] K. McGreggor, M. Kunda, and A. Goel, “A fractal analogy
    approach to the raven’s test of intelligence,” in *Workshops at the Twenty-Fourth
    AAAI Conference on Artificial Intelligence*, 2010.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McGreggor 等 [2010] K. McGreggor, M. Kunda, 和 A. Goel, “一种分形类比方法用于 Raven 智力测试，”
    收录于 *第二十四届 AAAI 人工智能会议研讨会*，2010 年。
- en: McGreggor and Goel [2014] K. McGreggor and A. Goel, “Confident reasoning on
    raven’s progressive matrices tests,” in *Proceedings of the AAAI Conference on
    Artificial Intelligence*, vol. 28, no. 1, 2014.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McGreggor 和 Goel [2014] K. McGreggor 和 A. Goel, “对 Raven 的渐进矩阵测试的自信推理，” 收录于
    *AAAI 人工智能会议论文集*，第 28 卷，第 1 期，2014 年。
- en: 'Hernández-Orallo et al. [2016] J. Hernández-Orallo, F. Martínez-Plumed, U. Schmid,
    M. Siebers, and D. L. Dowe, “Computer models solving intelligence test problems:
    Progress and implications,” *Artificial Intelligence*, vol. 230, pp. 74–107, 2016.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hernández-Orallo et al. [2016] J. Hernández-Orallo, F. Martínez-Plumed, U. Schmid,
    M. Siebers, 和 D. L. Dowe, “计算机模型解决智能测试问题：进展与影响”，*人工智能*，第230卷，页码74–107，2016年。
- en: Gardner and Richards [2006] M. Gardner and D. Richards, *The colossal book of
    short puzzles and problems*.   Norton, 2006.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gardner 和 Richards [2006] M. Gardner 和 D. Richards, *短谜题和问题的大百科全书*。Norton，2006年。
- en: 'Ruiz [2011] P. E. Ruiz, “Building and solving odd-one-out classification problems:
    A systematic approach,” *Intelligence*, vol. 39, no. 5, pp. 342–350, 2011.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruiz [2011] P. E. Ruiz, “构建和解决找出异类分类问题：一种系统的方法”，*智力*，第39卷，第5期，页码342–350，2011年。
- en: 'Smets and Vreeken [2011] K. Smets and J. Vreeken, “The odd one out: Identifying
    and characterising anomalies,” in *Proceedings of the 2011 SIAM international
    conference on data mining*.   SIAM, 2011, pp. 804–815.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smets 和 Vreeken [2011] K. Smets 和 J. Vreeken, “找出异类：识别和特征异常”，发表于*2011年SIAM国际数据挖掘会议*。SIAM，2011年，页码804–815。
- en: Bongard [1968] M. M. Bongard, “The recognition problem,” Foreign Technology
    Div Wright-Patterson AFB Ohio, Tech. Rep., 1968.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bongard [1968] M. M. Bongard, “识别问题”，外国技术部门 Wright-Patterson AFB Ohio, 技术报告，1968年。
- en: Fleuret et al. [2011] F. Fleuret, T. Li, C. Dubout, E. K. Wampler, S. Yantis,
    and D. Geman, “Comparing machines and humans on a visual categorization test,”
    *Proceedings of the National Academy of Sciences*, vol. 108, no. 43, pp. 17 621–17 625,
    2011.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fleuret et al. [2011] F. Fleuret, T. Li, C. Dubout, E. K. Wampler, S. Yantis,
    和 D. Geman, “在视觉分类测试中比较机器和人类”，*美国国家科学院院刊*，第108卷，第43期，页码17 621–17 625，2011年。
- en: Hill et al. [2019] F. Hill, A. Santoro, D. Barrett, A. Morcos, and T. Lillicrap,
    “Learning to Make Analogies by Contrasting Abstract Relational Structure,” in
    *International Conference on Learning Representations (ICLR)*, 2019.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hill et al. [2019] F. Hill, A. Santoro, D. Barrett, A. Morcos, 和 T. Lillicrap,
    “通过对比抽象关系结构来学习类比”，发表于*国际学习表征会议 (ICLR)*，2019年。
- en: Webb et al. [2020] T. Webb, Z. Dulberg, S. Frankland, A. Petrov, R. O’Reilly,
    and J. Cohen, “Learning representations that support extrapolation,” in *International
    Conference on Machine Learning*.   PMLR, 2020, pp. 10 136–10 146.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Webb et al. [2020] T. Webb, Z. Dulberg, S. Frankland, A. Petrov, R. O’Reilly,
    和 J. Cohen, “学习支持外推的表征”，发表于*国际机器学习会议*。PMLR，2020年，页码10 136–10 146。
- en: Chollet [2019] F. Chollet, “On the measure of intelligence,” *arXiv preprint
    arXiv:1911.01547*, 2019.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet [2019] F. Chollet, “关于智能的度量”，*arXiv预印本 arXiv:1911.01547*，2019年。
- en: 'Zhang et al. [2020] W. Zhang, C. Zhang, Y. Zhu, and S.-C. Zhu, “Machine Number
    Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning,”
    in *AAAI Conference on Artificial Intelligence (AAAI)*, 2020.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2020] W. Zhang, C. Zhang, Y. Zhu, 和 S.-C. Zhu, “机器数感：用于抽象和关系推理的视觉算术问题数据集”，发表于*AAAI人工智能会议
    (AAAI)*，2020年。
- en: Małkiński and Mańdziuk [2022] M. Małkiński and J. Mańdziuk, “A review of emerging
    research directions in abstract visual reasoning,” *arXiv preprint arXiv:2202.10284*,
    2022.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Małkiński 和 Mańdziuk [2022] M. Małkiński 和 J. Mańdziuk, “抽象视觉推理新兴研究方向综述”，*arXiv预印本
    arXiv:2202.10284*，2022年。
- en: Jaeggi et al. [2008] S. M. Jaeggi, M. Buschkuehl, J. Jonides, and W. J. Perrig,
    “Improving fluid intelligence with training on working memory,” *Proceedings of
    the National Academy of Sciences*, vol. 105, no. 19, pp. 6829–6833, 2008.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaeggi et al. [2008] S. M. Jaeggi, M. Buschkuehl, J. Jonides, 和 W. J. Perrig,
    “通过工作记忆训练提高流体智力”，*美国国家科学院院刊*，第105卷，第19期，页码6829–6833，2008年。
- en: Jain and Chandrasekaran [1982] A. K. Jain and B. Chandrasekaran, “39 dimensionality
    and sample size considerations in pattern recognition practice,” *Handbook of
    statistics*, vol. 2, pp. 835–855, 1982.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 和 Chandrasekaran [1982] A. K. Jain 和 B. Chandrasekaran, “模式识别实践中的维度和样本大小考虑”，*统计手册*，第2卷，页码835–855，1982年。
- en: 'Raudys et al. [1991] S. J. Raudys, A. K. Jain *et al.*, “Small sample size
    effects in statistical pattern recognition: Recommendations for practitioners,”
    *IEEE Transactions on pattern analysis and machine intelligence*, vol. 13, no. 3,
    pp. 252–264, 1991.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raudys et al. [1991] S. J. Raudys, A. K. Jain *等*，“统计模式识别中的小样本效应：对从业者的建议”，*IEEE模式分析与机器智能汇刊*，第13卷，第3期，页码252–264，1991年。
- en: Sun et al. [2017] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, “Revisiting
    unreasonable effectiveness of data in deep learning era,” in *Proceedings of the
    IEEE international conference on computer vision*, 2017, pp. 843–852.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2017] C. Sun, A. Shrivastava, S. Singh, 和 A. Gupta, “重审数据在深度学习时代的不合理有效性”，发表于*IEEE国际计算机视觉会议*，2017年，页码843–852。
- en: Mahajan et al. [2018] D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri,
    Y. Li, A. Bharambe, and L. Van Der Maaten, “Exploring the limits of weakly supervised
    pretraining,” in *Proceedings of the European Conference on Computer Vision (ECCV)*,
    2018, pp. 181–196.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahajan 等人 [2018] D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri,
    Y. Li, A. Bharambe, 和 L. Van Der Maaten, “探索弱监督预训练的极限，” 在 *欧洲计算机视觉会议 (ECCV) 论文集*，2018年，第181–196页。
- en: 'Ragni and Neubert [2014] M. Ragni and S. Neubert, “Analyzing raven’s intelligence
    test: Cognitive model, demand, and complexity,” in *Computational Approaches to
    Analogical Reasoning: Current Trends*.   Springer, 2014, pp. 351–370.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ragni 和 Neubert [2014] M. Ragni 和 S. Neubert, “分析渡鸦智力测试：认知模型、需求和复杂性，” 在 *类比推理的计算方法：当前趋势*。   Springer,
    2014年，第351–370页。
- en: Fu [1974] K. S. Fu, *Syntactic methods in pattern recognition*.   Elsevier,
    1974.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu [1974] K. S. Fu, *模式识别中的句法方法*。   Elsevier, 1974。
- en: Zhu and Mumford [2007] S.-C. Zhu and D. Mumford, *A stochastic grammar of images*.   Now
    Publishers Inc, 2007.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 和 Mumford [2007] S.-C. Zhu 和 D. Mumford, *图像的随机语法*。   Now Publishers Inc,
    2007。
- en: Lin et al. [2009] L. Lin, T. Wu, J. Porway, and Z. Xu, “A stochastic graph grammar
    for compositional object representation and recognition,” *Pattern Recognition*,
    vol. 42, no. 7, pp. 1297–1307, 2009.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 [2009] L. Lin, T. Wu, J. Porway, 和 Z. Xu, “用于组成对象表示和识别的随机图语法，” *模式识别*，第42卷，第7期，第1297–1307页，2009年。
- en: 'Agrawal et al. [2018] A. Agrawal, D. Batra, D. Parikh, and A. Kembhavi, “Don’t
    just assume; look and answer: Overcoming priors for visual question answering,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2018, pp. 4971–4980.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agrawal 等人 [2018] A. Agrawal, D. Batra, D. Parikh, 和 A. Kembhavi, “不要仅仅假设；观察并回答：克服视觉问答中的先验假设，”
    在 *IEEE 计算机视觉与模式识别会议论文集*，2018年，第4971–4980页。
- en: D’Amour et al. [2020] A. D’Amour, K. Heller, D. Moldovan, B. Adlam, B. Alipanahi,
    A. Beutel, C. Chen, J. Deaton, J. Eisenstein, M. D. Hoffman *et al.*, “Underspecification
    presents challenges for credibility in modern machine learning,” *arXiv preprint
    arXiv:2011.03395*, 2020.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D’Amour 等人 [2020] A. D’Amour, K. Heller, D. Moldovan, B. Adlam, B. Alipanahi,
    A. Beutel, C. Chen, J. Deaton, J. Eisenstein, M. D. Hoffman *等人*, “不足表述对现代机器学习中的可信度提出挑战，”
    *arXiv 预印本 arXiv:2011.03395*, 2020。
- en: Geirhos et al. [2020] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel,
    M. Bethge, and F. A. Wichmann, “Shortcut learning in deep neural networks,” *Nature
    Machine Intelligence*, vol. 2, no. 11, pp. 665–673, 2020.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geirhos 等人 [2020] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel,
    M. Bethge, 和 F. A. Wichmann, “深度神经网络中的快捷学习，” *自然机器智能*，第2卷，第11期，第665–673页，2020年。
- en: 'Dancette et al. [2021] C. Dancette, R. Cadene, D. Teney, and M. Cord, “Beyond
    question-based biases: Assessing multimodal shortcut learning in visual question
    answering,” *arXiv preprint arXiv:2104.03149*, 2021.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dancette 等人 [2021] C. Dancette, R. Cadene, D. Teney, 和 M. Cord, “超越基于问题的偏差：评估视觉问答中的多模态快捷学习，”
    *arXiv 预印本 arXiv:2104.03149*, 2021。
- en: Santoro et al. [2017] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu,
    P. Battaglia, and T. Lillicrap, “A simple neural network module for relational
    reasoning,” in *Advances in neural information processing systems*, 2017, pp.
    4967–4976.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santoro 等人 [2017] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu,
    P. Battaglia, 和 T. Lillicrap, “用于关系推理的简单神经网络模块，” 在 *神经信息处理系统进展*，2017年，第4967–4976页。
- en: 'Gentner [1983] D. Gentner, “Structure-mapping: A theoretical framework for
    analogy,” *Cognitive science*, vol. 7, no. 2, pp. 155–170, 1983.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gentner [1983] D. Gentner, “结构映射：类比的理论框架，” *认知科学*，第7卷，第2期，第155–170页，1983年。
- en: 'Hofstadter [1995] D. R. Hofstadter, *Fluid concepts and creative analogies:
    Computer models of the fundamental mechanisms of thought.*   Basic books, 1995.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hofstadter [1995] D. R. Hofstadter, *流动的概念与创造性的类比：思维基本机制的计算模型*。   Basic books,
    1995。
- en: Smith and Gentner [2014] L. Smith and D. Gentner, “The role of difference-detection
    in learning contrastive categories,” in *Proceedings of the Annual Meeting of
    the Cognitive Science Society*, vol. 36, 2014.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith 和 Gentner [2014] L. Smith 和 D. Gentner, “差异检测在学习对比类别中的作用，” 在 *认知科学学会年会论文集*，第36卷，2014年。
- en: Sohn [2016] K. Sohn, “Improved deep metric learning with multi-class n-pair
    loss objective,” in *Proceedings of the 30th International Conference on Neural
    Information Processing Systems*, 2016, pp. 1857–1865.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohn [2016] K. Sohn, “改进的深度度量学习与多类 n-pair 损失目标，” 在 *第30届国际神经信息处理系统会议论文集*，2016年，第1857–1865页。
- en: Khosla et al. [2020] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola,
    A. Maschinot, C. Liu, and D. Krishnan, “Supervised contrastive learning,” in *Advances
    in Neural Information Processing Systems*, vol. 33, 2020, pp. 18 661–18 673.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khosla 等 [2020] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola,
    A. Maschinot, C. Liu 和 D. Krishnan，“*监督对比学习*”，见于 *神经信息处理系统进展*，第33卷，2020，页 18 661–18 673。
- en: Bengio et al. [2009] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum
    learning,” in *Proceedings of the 26th annual international conference on machine
    learning*, 2009, pp. 41–48.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等 [2009] Y. Bengio, J. Louradour, R. Collobert 和 J. Weston，“*课程学习*”，见于
    *第26届国际机器学习大会论文集*，2009，页 41–48。
- en: Kumar et al. [2010] M. P. Kumar, B. Packer, and D. Koller, “Self-paced learning
    for latent variable models.” in *Advances in Neural Information Processing Systems*,
    vol. 1, 2010, p. 2.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等 [2010] M. P. Kumar, B. Packer 和 D. Koller，“*自定进度学习用于潜变量模型*”，见于 *神经信息处理系统进展*，第1卷，2010，页
    2。
- en: Borko and Putnam [1996] H. Borko and R. T. Putnam, “Learning to teach,” in *International
    Conference on Learning Representations (ICLR)*, 1996.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borko 和 Putnam [1996] H. Borko 和 R. T. Putnam，“*学习教学*”，见于 *学习表示国际会议 (ICLR)*，1996。
- en: Malisiewicz et al. [2011] T. Malisiewicz, A. Gupta, and A. A. Efros, “Ensemble
    of exemplar-svms for object detection and beyond,” in *2011 International conference
    on computer vision*.   IEEE, 2011, pp. 89–96.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malisiewicz 等 [2011] T. Malisiewicz, A. Gupta 和 A. A. Efros，“*用于目标检测及其他的示例-SVM集成*”，见于
    *2011年国际计算机视觉大会*。 IEEE，2011，页 89–96。
- en: Lin et al. [2017] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal
    loss for dense object detection,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 2980–2988.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 [2017] T.-Y. Lin, P. Goyal, R. Girshick, K. He 和 P. Dollár，“*密集目标检测的焦点损失*”，见于
    *IEEE国际计算机视觉会议论文集*，2017，页 2980–2988。
- en: 'Jiang et al. [2018] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei,
    “Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted
    labels,” in *International Conference on Machine Learning*.   PMLR, 2018, pp.
    2304–2313.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等 [2018] L. Jiang, Z. Zhou, T. Leung, L.-J. Li 和 L. Fei-Fei，“*Mentornet:
    基于数据驱动的课程学习用于深度神经网络处理受损标签*”，见于 *国际机器学习大会*。 PMLR，2018，页 2304–2313。'
- en: Ko et al. [2015] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, “Audio augmentation
    for speech recognition,” in *Sixteenth Annual Conference of the International
    Speech Communication Association*, 2015.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ko 等 [2015] T. Ko, V. Peddinti, D. Povey 和 S. Khudanpur，“*语音识别的音频增强*”，见于 *国际语音通信协会第十六届年会*，2015。
- en: Shorten and Khoshgoftaar [2019] C. Shorten and T. M. Khoshgoftaar, “A survey
    on image data augmentation for deep learning,” *Journal of Big Data*, vol. 6,
    no. 1, pp. 1–48, 2019.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shorten 和 Khoshgoftaar [2019] C. Shorten 和 T. M. Khoshgoftaar，“*深度学习的图像数据增强综述*”，*大数据杂志*，第6卷，第1期，页
    1–48，2019。
- en: 'Wei and Zou [2019] J. Wei and K. Zou, “EDA: Easy data augmentation techniques
    for boosting performance on text classification tasks,” in *Proceedings of the
    2019 Conference on Empirical Methods in Natural Language Processing and the 9th
    International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*.   Hong
    Kong, China: Association for Computational Linguistics, Nov. 2019, pp. 6382–6388.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 和 Zou [2019] J. Wei 和 K. Zou，“*EDA: 用于提升文本分类任务性能的简单数据增强技术*”，见于 *2019年自然语言处理经验方法会议暨第九届国际自然语言处理联合会议（EMNLP-IJCNLP）*。
    香港，中国：计算语言学协会，2019年11月，页 6382–6388。'
- en: 'Wen et al. [2020] Q. Wen, L. Sun, F. Yang, X. Song, J. Gao, X. Wang, and H. Xu,
    “Time series data augmentation for deep learning: A survey,” *arXiv preprint arXiv:2002.12478*,
    2020.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等 [2020] Q. Wen, L. Sun, F. Yang, X. Song, J. Gao, X. Wang 和 H. Xu，“*深度学习的时间序列数据增强：综述*”，*arXiv
    预印本 arXiv:2002.12478*，2020。
- en: van Steenkiste et al. [2019] S. van Steenkiste, F. Locatello, J. Schmidhuber,
    and O. Bachem, “Are disentangled representations helpful for abstract visual reasoning?”
    in *Advances in Neural Information Processing Systems*, 2019, pp. 14 245–14 258.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Steenkiste 等 [2019] S. van Steenkiste, F. Locatello, J. Schmidhuber 和 O.
    Bachem，“*解耦表示对抽象视觉推理是否有帮助？*” 见于 *神经信息处理系统进展*，2019，页 14 245–14 258。
- en: Kingma and Welling [2013] D. P. Kingma and M. Welling, “Auto-encoding variational
    bayes,” in *International Conference on Learning Representations (ICLR)*, 2013.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Welling [2013] D. P. Kingma 和 M. Welling，“*自动编码变分贝叶斯*”，见于 *学习表示国际会议
    (ICLR)*，2013。
- en: 'Higgins et al. [2017] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot,
    M. Botvinick, S. Mohamed, and A. Lerchner, “beta-vae: Learning basic visual concepts
    with a constrained variational framework,” in *International Conference on Learning
    Representations (ICLR)*, 2017.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Higgins 等人 [2017] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick,
    S. Mohamed, 和 A. Lerchner，“beta-vae：使用约束变分框架学习基本视觉概念”，见于 *国际学习表征会议（ICLR）*，2017
    年。
- en: Burgess et al. [2018] C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters,
    G. Desjardins, and A. Lerchner, “Understanding disentangling in $\beta$-vae,”
    *arXiv preprint arXiv:1804.03599*, 2018.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burgess 等人 [2018] C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters,
    G. Desjardins, 和 A. Lerchner，“理解 $\beta$-vae 中的解耦”，*arXiv 预印本 arXiv:1804.03599*，2018
    年。
- en: Goodfellow et al. [2014] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances
    in Neural Information Processing Systems*, vol. 27, 2014.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等人 [2014] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络”，见于 *神经信息处理系统进展*，第 27 卷，2014 年。
- en: 'Creswell et al. [2018] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran,
    B. Sengupta, and A. A. Bharath, “Generative adversarial networks: An overview,”
    *IEEE Signal Processing Magazine*, vol. 35, no. 1, pp. 53–65, 2018.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Creswell 等人 [2018] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta,
    和 A. A. Bharath，“生成对抗网络：概述”，*IEEE 信号处理杂志*，第 35 卷，第 1 期，第 53–65 页，2018 年。
- en: 'Wang et al. [2021] Z. Wang, Q. She, and T. E. Ward, “Generative adversarial
    networks in computer vision: A survey and taxonomy,” *ACM Comput. Surv.*, vol. 54,
    no. 2, Feb. 2021.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2021] Z. Wang, Q. She, 和 T. E. Ward，“计算机视觉中的生成对抗网络：调查与分类”，*ACM 计算机调查*，第
    54 卷，第 2 期，2021 年 2 月。
- en: Williams [1992] R. J. Williams, “Simple statistical gradient-following algorithms
    for connectionist reinforcement learning,” *Machine learning*, vol. 8, no. 3,
    pp. 229–256, 1992.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams [1992] R. J. Williams，“用于连接主义强化学习的简单统计梯度跟随算法”，*机器学习*，第 8 卷，第 3 期，第
    229–256 页，1992 年。
- en: LeCun et al. [1990] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E.
    Howard, W. E. Hubbard, and L. D. Jackel, “Handwritten digit recognition with a
    back-propagation network,” in *Advances in neural information processing systems*,
    1990, pp. 396–404.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等人 [1990] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard,
    W. E. Hubbard, 和 L. D. Jackel，“用反向传播网络进行手写数字识别”，见于 *神经信息处理系统进展*，1990 年，第 396–404
    页。
- en: Mekik et al. [2018] C. S. Mekik, R. Sun, and D. Y. Dai, “Similarity-based reasoning,
    raven’s matrices, and general intelligence.” in *IJCAI*, 2018, pp. 1576–1582.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mekik 等人 [2018] C. S. Mekik, R. Sun, 和 D. Y. Dai，“基于相似性的推理、乌鸦矩阵和一般智能”，见于 *IJCAI*，2018
    年，第 1576–1582 页。
- en: He et al. [2016] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
    for image recognition,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2016, pp. 770–778.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2016] K. He, X. Zhang, S. Ren, 和 J. Sun，“深度残差学习用于图像识别”，见于 *IEEE 计算机视觉与模式识别会议论文集*，2016
    年，第 770–778 页。
- en: Hochreiter and Schmidhuber [1997] S. Hochreiter and J. Schmidhuber, “Long short-term
    memory,” *Neural computation*, vol. 9, no. 8, pp. 1735–1780, 1997.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber [1997] S. Hochreiter 和 J. Schmidhuber，“长短期记忆”，*神经计算*，第
    9 卷，第 8 期，第 1735–1780 页，1997 年。
- en: 'Johnson et al. [2017] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei,
    C. Lawrence Zitnick, and R. Girshick, “CLEVR: A diagnostic dataset for compositional
    language and elementary visual reasoning,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 2901–2910.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Johnson 等人 [2017] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei,
    C. Lawrence Zitnick, 和 R. Girshick，“CLEVR: 一个用于组合语言和基础视觉推理的诊断数据集”，见于 *IEEE 计算机视觉与模式识别会议论文集*，2017
    年，第 2901–2910 页。'
- en: Malinowski and Fritz [2014] M. Malinowski and M. Fritz, “A multi-world approach
    to question answering about real-world scenes based on uncertain input,” *Advances
    in neural information processing systems*, vol. 27, pp. 1682–1690, 2014.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malinowski 和 Fritz [2014] M. Malinowski 和 M. Fritz，“基于不确定输入的现实场景问答的多世界方法”，*神经信息处理系统进展*，第
    27 卷，第 1682–1690 页，2014 年。
- en: 'Weston et al. [2015] J. Weston, A. Bordes, S. Chopra, A. M. Rush, B. van Merriënboer,
    A. Joulin, and T. Mikolov, “Towards ai-complete question answering: A set of prerequisite
    toy tasks,” *arXiv preprint arXiv:1502.05698*, 2015.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weston 等人 [2015] J. Weston, A. Bordes, S. Chopra, A. M. Rush, B. van Merriënboer,
    A. Joulin, 和 T. Mikolov，“走向 AI 完备的问答系统：一组前提玩具任务”，*arXiv 预印本 arXiv:1502.05698*，2015
    年。
- en: 'Antol et al. [2015] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
    Zitnick, and D. Parikh, “VQA: Visual question answering,” in *Proceedings of the
    IEEE international conference on computer vision*, 2015, pp. 2425–2433.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Antol 等人 [2015] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick,
    和 D. Parikh, “VQA：视觉问答，” 见于 *IEEE 国际计算机视觉会议论文集*，2015 年，第 2425–2433 页。
- en: Gao et al. [2015] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu, “Are
    you talking to a machine? dataset and methods for multilingual image question
    answering,” in *Proceedings of the 28th International Conference on Neural Information
    Processing Systems - Volume 2*, ser. NIPS’15, 2015, p. 2296–2304.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 [2015] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, 和 W. Xu, “你在跟机器说话吗？多语言图像问答的数据集和方法，”
    见于 *第 28 届神经信息处理系统国际会议论文集 - 第 2 卷*，系列 NIPS’15，2015 年，第 2296–2304 页。
- en: 'Ren et al. [2015a] M. Ren, R. Kiros, and R. Zemel, “Image question answering:
    A visual semantic embedding model and a new dataset,” in *Advances in Neural Information
    Processing Systems*, vol. 1, no. 2, 2015, p. 5.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人 [2015a] M. Ren, R. Kiros, 和 R. Zemel, “图像问答：一种视觉语义嵌入模型及一个新数据集，” 见于 *神经信息处理系统进展*，第
    1 卷，第 2 期，2015 年，第 5 页。
- en: Ren et al. [2015b] ——, “Exploring models and data for image question answering,”
    in *Advances in Neural Information Processing Systems*, vol. 28, 2015.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人 [2015b] ——, “探索图像问答的模型和数据，” 见于 *神经信息处理系统进展*，第 28 卷，2015 年。
- en: 'Krishna et al. [2017] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
    S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma *et al.*, “Visual genome: Connecting
    language and vision using crowdsourced dense image annotations,” *International
    journal of computer vision*, vol. 123, no. 1, pp. 32–73, 2017.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna 等人 [2017] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
    S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma *等*，“视觉基因组：通过众包密集图像注释连接语言和视觉，”
    *计算机视觉国际期刊*，第 123 卷，第 1 期，第 32–73 页，2017 年。
- en: 'Hudson and Manning [2019] D. A. Hudson and C. D. Manning, “GQA: A new dataset
    for real-world visual reasoning and compositional question answering,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 6700–6709.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hudson 和 Manning [2019] D. A. Hudson 和 C. D. Manning, “GQA：一个用于真实世界视觉推理和组合问答的新数据集，”
    见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2019 年，第 6700–6709 页。
- en: 'Teney et al. [2020] D. Teney, P. Wang, J. Cao, L. Liu, C. Shen, and A. van den
    Hengel, “V-PROM: A benchmark for visual reasoning using visual progressive matrices,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 34, no. 07,
    2020, pp. 12 071–12 078.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Teney 等人 [2020] D. Teney, P. Wang, J. Cao, L. Liu, C. Shen, 和 A. van den Hengel,
    “V-PROM：用于视觉推理的视觉渐进矩阵基准测试，” 见于 *AAAI 人工智能会议论文集*，第 34 卷，第 07 期，2020 年，第 12,071–12,078
    页。
- en: Palm et al. [2018] R. Palm, U. Paquet, and O. Winther, “Recurrent relational
    networks,” *Advances in Neural Information Processing Systems*, vol. 31, pp. 3368–3378,
    2018.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Palm 等人 [2018] R. Palm, U. Paquet, 和 O. Winther, “递归关系网络，” *神经信息处理系统进展*，第 31
    卷，第 3368–3378 页，2018 年。
- en: Jahrens and Martinetz [2019] M. Jahrens and T. Martinetz, “Multi-layer relation
    networks for relational reasoning,” in *Proceedings of the 2nd International Conference
    on Applications of Intelligent Systems*, 2019, pp. 1–5.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jahrens 和 Martinetz [2019] M. Jahrens 和 T. Martinetz, “用于关系推理的多层关系网络，” 见于 *第
    2 届智能系统应用国际会议论文集*，2019 年，第 1–5 页。
- en: 'You et al. [2020] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli,
    X. Song, J. Demmel, K. Keutzer, and C.-J. Hsieh, “Large batch optimization for
    deep learning: Training bert in 76 minutes,” in *International Conference on Learning
    Representations*, 2020.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You 等人 [2020] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X.
    Song, J. Demmel, K. Keutzer, 和 C.-J. Hsieh, “深度学习的大批量优化：76 分钟内训练 BERT，” 见于 *国际学习表示会议*，2020
    年。
- en: Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in
    *Advances in neural information processing systems*, 2017, pp. 5998–6008.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人 [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. u. Kaiser, 和 I. Polosukhin, “注意力机制就是你所需的全部，” 见于 *神经信息处理系统进展*，2017
    年，第 5998–6008 页。
- en: 'Dong et al. [2018] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-recurrence
    sequence-to-sequence model for speech recognition,” in *2018 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE, 2018,
    pp. 5884–5888.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人 [2018] L. Dong, S. Xu, 和 B. Xu, “Speech-transformer：一种无递归的序列到序列模型用于语音识别，”
    见于 *2018 IEEE 国际声学、语音与信号处理会议 (ICASSP)*。IEEE，2018 年，第 5884–5888 页。
- en: 'Dosovitskiy et al. [2021] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
    X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
    and N. Houlsby, “An image is worth 16x16 words: Transformers for image recognition
    at scale,” in *International Conference on Learning Representations*, 2021.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dosovitskiy 等人 [2021] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
    X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
    和 N. Houlsby, “An image is worth 16x16 words: Transformers for image recognition
    at scale,” 在 *International Conference on Learning Representations*, 2021。'
- en: 'Jaegle et al. [2021] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman,
    and J. Carreira, “Perceiver: General perception with iterative attention,” in
    *Proceedings of the 38th International Conference on Machine Learning*, ser. Proceedings
    of Machine Learning Research, vol. 139.   PMLR, 18–24 Jul 2021, pp. 4651–4664.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jaegle 等人 [2021] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman,
    和 J. Carreira, “Perceiver: General perception with iterative attention,” 在 *Proceedings
    of the 38th International Conference on Machine Learning*, ser. Proceedings of
    Machine Learning Research, vol. 139。PMLR, 2021年7月18–24日, pp. 4651–4664。'
- en: 'Kayhan and Gemert [2020] O. S. Kayhan and J. C. v. Gemert, “On translation
    invariance in cnns: Convolutional layers can exploit absolute spatial location,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 14 274–14 285.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kayhan 和 Gemert [2020] O. S. Kayhan 和 J. C. v. Gemert, “On translation invariance
    in cnns: Convolutional layers can exploit absolute spatial location,” 在 *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020,
    pp. 14,274–14,285。'
- en: Cohen et al. [2018] T. S. Cohen, M. Geiger, J. Köhler, and M. Welling, “Spherical
    CNNs,” in *International Conference on Learning Representations*, 2018.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen 等人 [2018] T. S. Cohen, M. Geiger, J. Köhler, 和 M. Welling, “Spherical
    CNNs,” 在 *International Conference on Learning Representations*, 2018。
- en: Krizhevsky et al. [2012] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet
    classification with deep convolutional neural networks,” *Advances in neural information
    processing systems*, vol. 25, pp. 1097–1105, 2012.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等人 [2012] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “Imagenet
    classification with deep convolutional neural networks,” *Advances in neural information
    processing systems*, vol. 25, pp. 1097–1105, 2012。
- en: Xie et al. [2017] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated
    residual transformations for deep neural networks,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 1492–1500.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人 [2017] S. Xie, R. Girshick, P. Dollár, Z. Tu, 和 K. He, “Aggregated residual
    transformations for deep neural networks,” 在 *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 1492–1500。
- en: 'Kirsch et al. [2018] L. Kirsch, J. Kunze, and D. Barber, “Modular networks:
    Learning to decompose neural computation,” in *Advances in Neural Information
    Processing Systems*, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
    and R. Garnett, Eds., vol. 31, 2018.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kirsch 等人 [2018] L. Kirsch, J. Kunze, 和 D. Barber, “Modular networks: Learning
    to decompose neural computation,” 在 *Advances in Neural Information Processing
    Systems*, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, 和
    R. Garnett 编, vol. 31, 2018。'
- en: 'Eslami et al. [2016] S. M. A. Eslami, N. Heess, T. Weber, Y. Tassa, D. Szepesvari,
    k. kavukcuoglu, and G. E. Hinton, “Attend, infer, repeat: Fast scene understanding
    with generative models,” in *Advances in Neural Information Processing Systems*,
    vol. 29, 2016.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Eslami 等人 [2016] S. M. A. Eslami, N. Heess, T. Weber, Y. Tassa, D. Szepesvari,
    k. kavukcuoglu, 和 G. E. Hinton, “Attend, infer, repeat: Fast scene understanding
    with generative models,” 在 *Advances in Neural Information Processing Systems*,
    vol. 29, 2016。'
- en: Goyal et al. [2021] P. Goyal, M. Caron, B. Lefaudeux, M. Xu, P. Wang, V. Pai,
    M. Singh, V. Liptchinsky, I. Misra, A. Joulin *et al.*, “Self-supervised pretraining
    of visual features in the wild,” *arXiv preprint arXiv:2103.01988*, 2021.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal 等人 [2021] P. Goyal, M. Caron, B. Lefaudeux, M. Xu, P. Wang, V. Pai, M.
    Singh, V. Liptchinsky, I. Misra, A. Joulin *等人*, “Self-supervised pretraining
    of visual features in the wild,” *arXiv 预印本 arXiv:2103.01988*, 2021。
- en: 'Kolesnikov et al. [2020] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung,
    S. Gelly, and N. Houlsby, “Big transfer (bit): General visual representation learning,”
    in *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part V 16*.   Springer, 2020, pp. 491–507.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kolesnikov 等人 [2020] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung,
    S. Gelly, 和 N. Houlsby, “Big transfer (bit): General visual representation learning,”
    在 *Computer Vision–ECCV 2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23日至28日，论文集，第五部分 16*。Springer,
    2020, pp. 491–507。'
- en: Radford et al. [2019] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
    “Language models are unsupervised multitask learners,” *OpenAI blog*, vol. 1,
    no. 8, p. 9, 2019.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 [2019] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, 和 I. Sutskever,
    “Language models are unsupervised multitask learners,” *OpenAI blog*, vol. 1,
    no. 8, p. 9, 2019。
- en: Brown et al. [2020] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
    A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,
    T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen,
    E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,
    I. Sutskever, and D. Amodei, “Language models are few-shot learners,” in *Advances
    in Neural Information Processing Systems*, vol. 33, 2020, pp. 1877–1901.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 [2020] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
    A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G.
    Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse,
    M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,
    A. Radford, I. Sutskever 和 D. Amodei，“语言模型是少样本学习者”，在 *神经信息处理系统进展*，第 33 卷，2020
    年，第 1877–1901 页。
- en: 'Zhu et al. [2020] Y. Zhu, T. Gao, L. Fan, S. Huang, M. Edmonds, H. Liu, F. Gao,
    C. Zhang, S. Qi, Y. N. Wu *et al.*, “Dark, beyond deep: A paradigm shift to cognitive
    ai with humanlike common sense,” *Engineering*, vol. 6, no. 3, pp. 310–345, 2020.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 [2020] Y. Zhu, T. Gao, L. Fan, S. Huang, M. Edmonds, H. Liu, F. Gao, C.
    Zhang, S. Qi, Y. N. Wu *等*，“黑暗，超越深度：认知 AI 的范式转变与类人常识”，*工程*，第 6 卷，第 3 期，第 310–345
    页，2020 年。
- en: Park and Kwak [2018] S. Park and N. Kwak, “3d human pose estimation with relational
    networks,” in *BMVC*, 2018.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 和 Kwak [2018] S. Park 和 N. Kwak，“带关系网络的 3D 人体姿态估计”，在 *BMVC*，2018 年。
- en: 'Sarafianos et al. [2016] N. Sarafianos, B. Boteanu, B. Ionescu, and I. A. Kakadiaris,
    “3d human pose estimation: A review of the literature and analysis of covariates,”
    *Computer Vision and Image Understanding*, vol. 152, pp. 1–20, 2016.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarafianos 等 [2016] N. Sarafianos, B. Boteanu, B. Ionescu 和 I. A. Kakadiaris，“3D
    人体姿态估计：文献综述及协变量分析”，*计算机视觉与图像理解*，第 152 卷，第 1–20 页，2016 年。
- en: 'Chen et al. [2020a] Y. Chen, Y. Tian, and M. He, “Monocular human pose estimation:
    A survey of deep learning-based methods,” *Computer Vision and Image Understanding*,
    vol. 192, p. 102897, 2020.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2020a] Y. Chen, Y. Tian 和 M. He，“单目人体姿态估计：深度学习方法综述”，*计算机视觉与图像理解*，第 192
    卷，第 102897 页，2020 年。
- en: 'Zheng et al. [2020] C. Zheng, W. Wu, T. Yang, S. Zhu, C. Chen, R. Liu, J. Shen,
    N. Kehtarnavaz, and M. Shah, “Deep learning-based human pose estimation: A survey,”
    *arXiv preprint arXiv:2012.13392*, 2020.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等 [2020] C. Zheng, W. Wu, T. Yang, S. Zhu, C. Chen, R. Liu, J. Shen, N.
    Kehtarnavaz 和 M. Shah，“基于深度学习的人体姿态估计：综述”，*arXiv 预印本 arXiv:2012.13392*，2020 年。
- en: Garcia-Garcia et al. [2018] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez,
    P. Martinez-Gonzalez, and J. Garcia-Rodriguez, “A survey on deep learning techniques
    for image and video semantic segmentation,” *Applied Soft Computing*, vol. 70,
    pp. 41–65, 2018.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garcia-Garcia 等 [2018] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez,
    P. Martinez-Gonzalez 和 J. Garcia-Rodriguez，“深度学习技术在图像和视频语义分割中的应用综述”，*应用软计算*，第
    70 卷，第 41–65 页，2018 年。
- en: Hao et al. [2020] S. Hao, Y. Zhou, and Y. Guo, “A brief survey on semantic segmentation
    with deep learning,” *Neurocomputing*, vol. 406, pp. 302–321, 2020.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao 等 [2020] S. Hao, Y. Zhou 和 Y. Guo，“深度学习语义分割简要综述”，*神经计算*，第 406 卷，第 302–321
    页，2020 年。
- en: 'Minaee et al. [2021] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz,
    and D. Terzopoulos, “Image segmentation using deep learning: A survey,” *IEEE
    Transactions on Pattern Analysis and Machine Intelligence*, 2021.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee 等 [2021] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz
    和 D. Terzopoulos，“使用深度学习的图像分割：综述”，*IEEE 模式分析与机器智能汇刊*，2021 年。
- en: Mou et al. [2019] L. Mou, Y. Hua, and X. X. Zhu, “A relation-augmented fully
    convolutional network for semantic segmentation in aerial scenes,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 12 416–12 425.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mou 等 [2019] L. Mou, Y. Hua 和 X. X. Zhu，“用于航空场景语义分割的关系增强全卷积网络”，在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2019
    年，第 12,416–12,425 页。
- en: Battaglia et al. [2016] P. Battaglia, R. Pascanu, M. Lai, D. Jimenez Rezende,
    and k. kavukcuoglu, “Interaction networks for learning about objects, relations
    and physics,” in *Advances in Neural Information Processing Systems*, vol. 29,
    2016.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Battaglia 等 [2016] P. Battaglia, R. Pascanu, M. Lai, D. Jimenez Rezende 和 K.
    Kavukcuoglu，“用于学习对象、关系和物理的交互网络”，在 *神经信息处理系统进展*，第 29 卷，2016 年。
- en: Wang et al. [2018] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural
    networks,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2018, pp. 7794–7803.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2018] X. Wang, R. Girshick, A. Gupta 和 K. He，“非局部神经网络”，在 *IEEE 计算机视觉与模式识别会议论文集*，2018
    年，第 7794–7803 页。
- en: Hu et al. [2018] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei, “Relation networks
    for object detection,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 3588–3597.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2018] H. Hu, J. Gu, Z. Zhang, J. Dai, 和 Y. Wei, “用于目标检测的关系网络，” 在 *IEEE
    计算机视觉与模式识别会议论文集* 中，2018年，第3588–3597页。
- en: Chen et al. [2019] Y. Chen, M. Rohrbach, Z. Yan, Y. Shuicheng, J. Feng, and
    Y. Kalantidis, “Graph-based global reasoning networks,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019, pp. 433–442.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2019] Y. Chen, M. Rohrbach, Z. Yan, Y. Shuicheng, J. Feng, 和 Y. Kalantidis,
    “基于图的全局推理网络，” 在 *IEEE/CVF 计算机视觉与模式识别会议论文集* 中，2019年，第433–442页。
- en: Sun et al. [2018] C. Sun, A. Shrivastava, C. Vondrick, K. Murphy, R. Sukthankar,
    and C. Schmid, “Actor-centric relation network,” in *Proceedings of the European
    Conference on Computer Vision (ECCV)*, 2018, pp. 318–334.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2018] C. Sun, A. Shrivastava, C. Vondrick, K. Murphy, R. Sukthankar,
    和 C. Schmid, “以演员为中心的关系网络，” 在 *欧洲计算机视觉会议（ECCV）论文集* 中，2018年，第318–334页。
- en: Patacchiola and Storkey [2020] M. Patacchiola and A. J. Storkey, “Self-supervised
    relational reasoning for representation learning,” in *Advances in Neural Information
    Processing Systems*, vol. 33, 2020, pp. 4003–4014.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patacchiola 和 Storkey [2020] M. Patacchiola 和 A. J. Storkey, “用于表示学习的自监督关系推理，”
    在 *神经信息处理系统进展* 中，第33卷，2020年，第4003–4014页。
- en: Zambaldi et al. [2018] V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li,
    I. Babuschkin, K. Tuyls, D. Reichert, T. Lillicrap, E. Lockhart *et al.*, “Deep
    reinforcement learning with relational inductive biases,” in *International Conference
    on Learning Representations*, 2018.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zambaldi 等人 [2018] V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li, I. Babuschkin,
    K. Tuyls, D. Reichert, T. Lillicrap, E. Lockhart *等人*，“具有关系归纳偏差的深度强化学习，” 在 *学习表示国际会议*
    中，2018年。
- en: 'Gutmann and Hyvärinen [2010] M. Gutmann and A. Hyvärinen, “Noise-contrastive
    estimation: A new estimation principle for unnormalized statistical models,” in
    *Proceedings of the Thirteenth International Conference on Artificial Intelligence
    and Statistics*.   JMLR Workshop and Conference Proceedings, 2010, pp. 297–304.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gutmann 和 Hyvärinen [2010] M. Gutmann 和 A. Hyvärinen, “噪声对比估计：一种新的非标准统计模型估计原理，”
    在 *第十三届人工智能与统计国际会议论文集* 中。 JMLR 研讨会和会议论文集，2010年，第297–304页。
- en: Hyvärinen and Morioka [2016] A. Hyvärinen and H. Morioka, “Unsupervised feature
    extraction by time-contrastive learning and nonlinear ica,” in *Advances in Neural
    Information Processing Systems*, vol. 29, 2016.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hyvärinen 和 Morioka [2016] A. Hyvärinen 和 H. Morioka, “通过时间对比学习和非线性独立成分分析进行无监督特征提取，”
    在 *神经信息处理系统进展* 中，第29卷，2016年。
- en: Oord et al. [2018] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning
    with contrastive predictive coding,” *arXiv preprint arXiv:1807.03748*, 2018.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oord 等人 [2018] A. v. d. Oord, Y. Li, 和 O. Vinyals, “利用对比预测编码进行表示学习，” *arXiv
    预印本 arXiv:1807.03748*，2018年。
- en: Chen et al. [2020b] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple
    framework for contrastive learning of visual representations,” in *International
    conference on machine learning*.   PMLR, 2020, pp. 1597–1607.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2020b] T. Chen, S. Kornblith, M. Norouzi, 和 G. Hinton, “一种简单的视觉表示对比学习框架，”
    在 *国际机器学习会议* 中。 PMLR，2020年，第1597–1607页。
- en: He et al. [2020] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast
    for unsupervised visual representation learning,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2020, pp. 9729–9738.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2020] K. He, H. Fan, Y. Wu, S. Xie, 和 R. Girshick, “用于无监督视觉表示学习的动量对比，”
    在 *IEEE/CVF 计算机视觉与模式识别会议论文集* 中，2020年，第9729–9738页。
- en: Mikolov et al. [2013] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
    J. Dean, “Distributed representations of words and phrases and their compositionality,”
    in *Advances in Neural Information Processing Systems*, vol. 26, 2013.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等人 [2013] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, 和 J. Dean,
    “词汇和短语的分布式表示及其组合性，” 在 *神经信息处理系统进展* 中，第26卷，2013年。
- en: Saunshi et al. [2019] N. Saunshi, O. Plevrakis, S. Arora, M. Khodak, and H. Khandeparkar,
    “A theoretical analysis of contrastive unsupervised representation learning,”
    in *International Conference on Machine Learning*.   PMLR, 2019, pp. 5628–5637.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saunshi 等人 [2019] N. Saunshi, O. Plevrakis, S. Arora, M. Khodak, 和 H. Khandeparkar,
    “对比无监督表示学习的理论分析，” 在 *国际机器学习会议* 中。 PMLR，2019年，第5628–5637页。
- en: Klein and Nabi [2020] T. Klein and M. Nabi, “Contrastive self-supervised learning
    for commonsense reasoning,” in *Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics*, jul 2020, pp. 7517–7523.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Klein 和 Nabi [2020] T. Klein 和 M. Nabi，“用于常识推理的对比自监督学习”，*第 58 届计算语言学协会年会会议论文集*，2020
    年 7 月，第 7517–7523 页。
- en: 'Schneider et al. [2019] S. Schneider, A. Baevski, R. Collobert, and M. Auli,
    “wav2vec: Unsupervised Pre-Training for Speech Recognition,” in *Proc. Interspeech*,
    2019, pp. 3465–3469.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schneider 等 [2019] S. Schneider, A. Baevski, R. Collobert 和 M. Auli，“wav2vec:
    无监督预训练用于语音识别”，*Interspeech 会议论文集*，2019，第 3465–3469 页。'
- en: Kreuk et al. [2020] F. Kreuk, J. Keshet, and Y. Adi, “Self-supervised contrastive
    learning for unsupervised phoneme segmentation,” in *Proc. Interspeech*, 2020.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kreuk 等 [2020] F. Kreuk, J. Keshet 和 Y. Adi，“用于无监督音素分割的自监督对比学习”，*Interspeech
    会议论文集*，2020。
- en: 'Al-Tahan and Mohsenzadeh [2021] H. Al-Tahan and Y. Mohsenzadeh, “Clar: Contrastive
    learning of auditory representations,” in *International Conference on Artificial
    Intelligence and Statistics*.   PMLR, 2021, pp. 2530–2538.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Al-Tahan 和 Mohsenzadeh [2021] H. Al-Tahan 和 Y. Mohsenzadeh，“Clar: 听觉表征的对比学习”，*国际人工智能与统计会议*。PMLR，2021，第
    2530–2538 页。'
- en: Kipf et al. [2020] T. Kipf, E. van der Pol, and M. Welling, “Contrastive learning
    of structured world models,” in *International Conference on Learning Representations*,
    2020.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf 等 [2020] T. Kipf, E. van der Pol 和 M. Welling，“结构化世界模型的对比学习”，*国际学习表示会议*，2020。
- en: 'Laskin et al. [2020] M. Laskin, A. Srinivas, and P. Abbeel, “Curl: Contrastive
    unsupervised representations for reinforcement learning,” in *International Conference
    on Machine Learning*.   PMLR, 2020, pp. 5639–5650.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Laskin 等 [2020] M. Laskin, A. Srinivas 和 P. Abbeel，“Curl: 用于强化学习的对比无监督表征”，*国际机器学习会议*。PMLR，2020，第
    5639–5650 页。'
- en: Liu et al. [2021] G. Liu, C. Zhang, L. Zhao, T. Qin, J. Zhu, L. Jian, N. Yu,
    and T.-Y. Liu, “Return-based contrastive representation learning for reinforcement
    learning,” in *International Conference on Learning Representations*, 2021.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2021] G. Liu, C. Zhang, L. Zhao, T. Qin, J. Zhu, L. Jian, N. Yu 和 T.-Y.
    Liu，“基于回报的对比表征学习用于强化学习”，*国际学习表示会议*，2021。
- en: 'Nie et al. [2020] W. Nie, Z. Yu, L. Mao, A. B. Patel, Y. Zhu, and A. Anandkumar,
    “Bongard-logo: A new benchmark for human-level concept learning and reasoning,”
    *Advances in Neural Information Processing Systems*, vol. 33, 2020.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nie 等 [2020] W. Nie, Z. Yu, L. Mao, A. B. Patel, Y. Zhu 和 A. Anandkumar，“Bongard-logo:
    一种用于人类级别概念学习和推理的新基准”，*神经信息处理系统进展*，第 33 卷，2020。'
- en: '[Detailed results]'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[详细结果]'
- en: 'Additional results on particular configurations from RAVEN [[16](#bib.bib16)]
    and I-RAVEN [[17](#bib.bib17)] datasets are provided in Table [VI](#A0.T6 "TABLE
    VI ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s
    Progressive Matrices") and Table [VII](#A0.T7 "TABLE VII ‣ Deep Learning Methods
    for Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices"), respectively.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '来自 RAVEN [[16](#bib.bib16)] 和 I-RAVEN [[17](#bib.bib17)] 数据集的特定配置的附加结果见表 [VI](#A0.T6
    "TABLE VI ‣ Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven’s
    Progressive Matrices") 和表 [VII](#A0.T7 "TABLE VII ‣ Deep Learning Methods for
    Abstract Visual Reasoning: A Survey on Raven’s Progressive Matrices")。'
- en: 'TABLE VI: RAVEN accuracy. Accuracy on the test split of the RAVEN dataset [[16](#bib.bib16)].
    Mean denotes the mean accuracy for all configurations. The Left-Right configuration
    is denoted as L-R, Up-Down as U-D, Out-InCenter as O-IC and Out-InGrid as O-IG.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：RAVEN 准确率。RAVEN 数据集的测试拆分上的准确率 [[16](#bib.bib16)]。均值表示所有配置的均值准确率。Left-Right
    配置表示为 L-R，上下表示为 U-D，Out-InCenter 表示为 O-IC，Out-InGrid 表示为 O-IG。
- en: '| Method | Test accuracy (%) |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 测试准确率 (%) |'
- en: '| Mean | Center | 2x2Grid | 3x3Grid | L-R | U-D | O-IC | O-IG |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| 均值 | 中心 | 2x2 网格 | 3x3 网格 | L-R | U-D | O-IC | O-IG |'
- en: '| CNN LSTM [[16](#bib.bib16)] | 13.1 | 13.2 | 14.1 | 13.7 | 12.8 | 12.4 | 12.2
    | 13.0 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| CNN LSTM [[16](#bib.bib16)] | 13.1 | 13.2 | 14.1 | 13.7 | 12.8 | 12.4 | 12.2
    | 13.0 |'
- en: '| CNN LSTM + DRT [[16](#bib.bib16)] | 14.0 | 14.3 | 15.1 | 14.1 | 13.8 | 13.2
    | 14.0 | 13.3 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| CNN LSTM + DRT [[16](#bib.bib16)] | 14.0 | 14.3 | 15.1 | 14.1 | 13.8 | 13.2
    | 14.0 | 13.3 |'
- en: '| WReN [[16](#bib.bib16)] | 14.7 | 13.1 | 28.6 | 28.3 | 7.5 | 6.3 | 8.4 | 10.6
    |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| WReN [[16](#bib.bib16)] | 14.7 | 13.1 | 28.6 | 28.3 | 7.5 | 6.3 | 8.4 | 10.6
    |'
- en: '| WReN + DRT [[16](#bib.bib16)] | 15.0 | 15.4 | 23.3 | 29.5 | 7.0 | 8.4 | 8.9
    | 12.4 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| WReN + DRT [[16](#bib.bib16)] | 15.0 | 15.4 | 23.3 | 29.5 | 7.0 | 8.4 | 8.9
    | 12.4 |'
- en: '| ARNe [[31](#bib.bib31)] | 19.7 | - | - | - | - | - | - | - |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| ARNe [[31](#bib.bib31)] | 19.7 | - | - | - | - | - | - | - |'
- en: '| MCPT [[28](#bib.bib28)] | 28.5 | 35.9 | 26.0 | 27.2 | 29.3 | 27.4 | 33.1
    | 20.7 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| MCPT [[28](#bib.bib28)] | 28.5 | 35.9 | 26.0 | 27.2 | 29.3 | 27.4 | 33.1
    | 20.7 |'
- en: '| WReN-Tag-Aux [[20](#bib.bib20)] | 34.0 | 58.4 | 38.9 | 37.7 | 21.6 | 19.7
    | 38.8 | 22.6 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| WReN-Tag-Aux [[20](#bib.bib20)] | 34.0 | 58.4 | 38.9 | 37.7 | 21.6 | 19.7
    | 38.8 | 22.6 |'
- en: '| NCD [[30](#bib.bib30)] | 37.0 | 45.5 | 35.5 | 39.5 | 34.9 | 33.4 | 40.3 |
    30.0 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| NCD [[30](#bib.bib30)] | 37.0 | 45.5 | 35.5 | 39.5 | 34.9 | 33.4 | 40.3 |
    30.0 |'
- en: '| CNN MLP [[16](#bib.bib16)] | 37.0 | 33.6 | 30.3 | 33.5 | 39.4 | 41.3 | 43.2
    | 37.5 |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| CNN MLP [[16](#bib.bib16)] | 37.0 | 33.6 | 30.3 | 33.5 | 39.4 | 41.3 | 43.2
    | 37.5 |'
- en: '| CNN MLP + DRT [[16](#bib.bib16)] | 39.4 | 37.3 | 30.1 | 34.6 | 45.5 | 45.5
    | 45.9 | 37.5 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| CNN MLP + DRT [[16](#bib.bib16)] | 39.4 | 37.3 | 30.1 | 34.6 | 45.5 | 45.5
    | 45.9 | 37.5 |'
- en: '| PRD [[29](#bib.bib29)] | 50.7 | 74.6 | 38.7 | 34.9 | 60.8 | 60.3 | 62.5 |
    23.4 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| PRD [[29](#bib.bib29)] | 50.7 | 74.6 | 38.7 | 34.9 | 60.8 | 60.3 | 62.5 |
    23.4 |'
- en: '| ResNet-18 [[16](#bib.bib16)] | 53.4 | 52.8 | 41.9 | 44.3 | 58.8 | 60.2 |
    63.2 | 53.1 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 [[16](#bib.bib16)] | 53.4 | 52.8 | 41.9 | 44.3 | 58.8 | 60.2 |
    63.2 | 53.1 |'
- en: '| ResNet-18 + DRT [[16](#bib.bib16)] | 59.6 | 58.1 | 46.5 | 50.4 | 65.8 | 67.1
    | 69.1 | 60.1 |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 + DRT [[16](#bib.bib16)] | 59.6 | 58.1 | 46.5 | 50.4 | 65.8 | 67.1
    | 69.1 | 60.1 |'
- en: '| LEN [[22](#bib.bib22)] | 72.9 | 80.2 | 57.5 | 62.1 | 73.5 | 81.2 | 84.4 |
    71.5 |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| LEN [[22](#bib.bib22)] | 72.9 | 80.2 | 57.5 | 62.1 | 73.5 | 81.2 | 84.4 |
    71.5 |'
- en: '| ResNet-18 [[28](#bib.bib28)] | 77.2 | 72.8 | 57.0 | 62.7 | 91.0 | 89.6 |
    88.4 | 78.9 |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 [[28](#bib.bib28)] | 77.2 | 72.8 | 57.0 | 62.7 | 91.0 | 89.6 |
    88.4 | 78.9 |'
- en: '| LEN + TM [[22](#bib.bib22)] | 78.3 | 82.3 | 58.5 | 64.3 | 87.0 | 85.5 | 88.9
    | 81.9 |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| LEN + TM [[22](#bib.bib22)] | 78.3 | 82.3 | 58.5 | 64.3 | 87.0 | 85.5 | 88.9
    | 81.9 |'
- en: '| MXGNet [[36](#bib.bib36)] | 83.9 | - | - | - | - | - | - | - |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| MXGNet [[36](#bib.bib36)] | 83.9 | - | - | - | - | - | - | - |'
- en: '| MRNet [[18](#bib.bib18)] | 84.0 | - | - | - | - | - | - | - |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| MRNet [[18](#bib.bib18)] | 84.0 | - | - | - | - | - | - | - |'
- en: '| ResNet-50 + pre-train [[28](#bib.bib28)] | 86.3 | 89.5 | 66.6 | 68.0 | 97.9
    | 98.2 | 96.6 | 87.2 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 + pre-train [[28](#bib.bib28)] | 86.3 | 89.5 | 66.6 | 68.0 | 97.9
    | 98.2 | 96.6 | 87.2 |'
- en: '| CoPINet [[20](#bib.bib20)] | 91.4 | 95.1 | 77.5 | 78.9 | 99.1 | 99.7 | 98.5
    | 91.4 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| CoPINet [[20](#bib.bib20)] | 91.4 | 95.1 | 77.5 | 78.9 | 99.1 | 99.7 | 98.5
    | 91.4 |'
- en: '| SCL [[33](#bib.bib33)] | 91.6 | 98.1 | 91.0 | 82.5 | 96.8 | 96.5 | 96.0 |
    80.1 |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| SCL [[33](#bib.bib33)] | 91.6 | 98.1 | 91.0 | 82.5 | 96.8 | 96.5 | 96.0 |
    80.1 |'
- en: '| Rel-Base [[34](#bib.bib34)] | 91.7 | 97.6 | 85.9 | 86.9 | 93.5 | 96.5 | 97.6
    | 83.8 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| Rel-Base [[34](#bib.bib34)] | 91.7 | 97.6 | 85.9 | 86.9 | 93.5 | 96.5 | 97.6
    | 83.8 |'
- en: '| CoPINet + AL [[21](#bib.bib21)] | 93.5 | 98.6 | 80.5 | 83.2 | 99.7 | 99.8
    | 99.4 | 93.3 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| CoPINet + AL [[21](#bib.bib21)] | 93.5 | 98.6 | 80.5 | 83.2 | 99.7 | 99.8
    | 99.4 | 93.3 |'
- en: '| DCNet [[35](#bib.bib35)] | 93.6 | 97.8 | 81.7 | 86.7 | 99.8 | 99.8 | 99.0
    | 91.5 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| DCNet [[35](#bib.bib35)] | 93.6 | 97.8 | 81.7 | 86.7 | 99.8 | 99.8 | 99.0
    | 91.5 |'
- en: '| CoPINet + ACL [[21](#bib.bib21)] | 93.7 | 98.4 | 81.0 | 84.0 | 99.7 | 99.8
    | 99.4 | 93.9 |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| CoPINet + ACL [[21](#bib.bib21)] | 93.7 | 98.4 | 81.0 | 84.0 | 99.7 | 99.8
    | 99.4 | 93.9 |'
- en: '| Rel-AIR [[34](#bib.bib34)] | 94.1 | 99.0 | 92.4 | 87.1 | 98.7 | 97.9 | 98.0
    | 85.3 |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| Rel-AIR [[34](#bib.bib34)] | 94.1 | 99.0 | 92.4 | 87.1 | 98.7 | 97.9 | 98.0
    | 85.3 |'
- en: '| Context-blind ResNet [[17](#bib.bib17)] | 71.9 | - | - | - | - | - | - |
    - |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| Context-blind ResNet [[17](#bib.bib17)] | 71.9 | - | - | - | - | - | - |
    - |'
- en: '| Context-blind SCL [[33](#bib.bib33)] | 94.2 | - | - | - | - | - | - | - |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| Context-blind SCL [[33](#bib.bib33)] | 94.2 | - | - | - | - | - | - | - |'
- en: '| Context-blind CoPINet [[17](#bib.bib17)] | 94.2 | - | - | - | - | - | - |
    - |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| Context-blind CoPINet [[17](#bib.bib17)] | 94.2 | - | - | - | - | - | - |
    - |'
- en: '| Human [[16](#bib.bib16)] | 84.4 | 95.4 | 81.8 | 79.6 | 86.4 | 81.8 | 86.4
    | 81.8 |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| Human [[16](#bib.bib16)] | 84.4 | 95.4 | 81.8 | 79.6 | 86.4 | 81.8 | 86.4
    | 81.8 |'
- en: 'TABLE VII: I-RAVEN accuracy. Accuracy on the test split of the I-RAVEN dataset [[17](#bib.bib17)].
    Mean denotes the mean accuracy for all configurations. The Left-Right configuration
    is denoted as L-R, Up-Down as U-D, Out-InCenter as O-IC and Out-InGrid as O-IG.
    ²²footnotemark: 2MRNet was evaluated on RAVEN-FAIR in [[18](#bib.bib18)].'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE VII: I-RAVEN 精度。I-RAVEN 数据集 [[17](#bib.bib17)] 的测试分割上的精度。Mean 表示所有配置的平均精度。Left-Right
    配置表示为 L-R，Up-Down 为 U-D，Out-InCenter 为 O-IC，Out-InGrid 为 O-IG。²²脚注：2MRNet 在 [[18](#bib.bib18)]
    中在 RAVEN-FAIR 上进行了评估。'
- en: '| Method | Test accuracy (%) |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| Method | Test accuracy (%) |'
- en: '| Mean | Center | 2x2Grid | 3x3Grid | L-R | U-D | O-IC | O-IG |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| Mean | Center | 2x2Grid | 3x3Grid | L-R | U-D | O-IC | O-IG |'
- en: '| CNN LSTM [[17](#bib.bib17)] | 18.9 | 26.2 | 16.7 | 15.1 | 14.6 | 16.5 | 21.9
    | 21.1 |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| CNN LSTM [[17](#bib.bib17)] | 18.9 | 26.2 | 16.7 | 15.1 | 14.6 | 16.5 | 21.9
    | 21.1 |'
- en: '| WReN [[17](#bib.bib17)] | 23.8 | 29.4 | 26.8 | 23.5 | 21.9 | 21.4 | 22.5
    | 21.5 |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| WReN [[17](#bib.bib17)] | 23.8 | 29.4 | 26.8 | 23.5 | 21.9 | 21.4 | 22.5
    | 21.5 |'
- en: '| ResNet-18 [[17](#bib.bib17)] | 40.3 | 44.7 | 29.3 | 27.9 | 51.2 | 47.4 |
    46.2 | 35.8 |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 [[17](#bib.bib17)] | 40.3 | 44.7 | 29.3 | 27.9 | 51.2 | 47.4 |
    46.2 | 35.8 |'
- en: '| ResNet-18 + DRT [[17](#bib.bib17)] | 40.4 | 46.5 | 28.8 | 27.3 | 50.1 | 49.8
    | 46.0 | 34.2 |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 + DRT [[17](#bib.bib17)] | 40.4 | 46.5 | 28.8 | 27.3 | 50.1 | 49.8
    | 46.0 | 34.2 |'
- en: '| LEN [[17](#bib.bib17)] | 41.4 | 56.4 | 31.7 | 29.7 | 44.2 | 44.2 | 52.1 |
    31.7 |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| LEN [[17](#bib.bib17)] | 41.4 | 56.4 | 31.7 | 29.7 | 44.2 | 44.2 | 52.1 |
    31.7 |'
- en: '| Wild ResNet [[17](#bib.bib17)] | 44.3 | 50.9 | 33.1 | 30.8 | 53.1 | 52.6
    | 50.9 | 38.7 |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| Wild ResNet [[17](#bib.bib17)] | 44.3 | 50.9 | 33.1 | 30.8 | 53.1 | 52.6
    | 50.9 | 38.7 |'
- en: '| CoPINet [[17](#bib.bib17)] | 46.1 | 54.4 | 36.8 | 31.9 | 51.9 | 52.5 | 52.2
    | 42.8 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| CoPINet [[17](#bib.bib17)] | 46.1 | 54.4 | 36.8 | 31.9 | 51.9 | 52.5 | 52.2
    | 42.8 |'
- en: '| NCD [[30](#bib.bib30)] | 48.2 | 60.0 | 31.2 | 30.0 | 58.9 | 57.2 | 62.4 |
    39.0 |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| NCD [[30](#bib.bib30)] | 48.2 | 60.0 | 31.2 | 30.0 | 58.9 | 57.2 | 62.4 |
    39.0 |'
- en: '| CoPINet MLCL+DA [[19](#bib.bib19)] | 57.1 | - | - | - | - | - | - | - |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| CoPINet MLCL+DA [[19](#bib.bib19)] | 57.1 | - | - | - | - | - | - | - |'
- en: '| SRAN [[17](#bib.bib17)] | 60.8 | 78.2 | 50.1 | 42.4 | 70.1 | 70.3 | 68.2
    | 46.3 |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| SRAN [[17](#bib.bib17)] | 60.8 | 78.2 | 50.1 | 42.4 | 70.1 | 70.3 | 68.2
    | 46.3 |'
- en: '| SRAN MLCL+DA [[19](#bib.bib19)] | 73.3 | - | - | - | - | - | - | - |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| SRAN MLCL+DA [[19](#bib.bib19)] | 73.3 | - | - | - | - | - | - | - |'
- en: '| ²²footnotemark: 2MRNet [[18](#bib.bib18)] | 86.8 | 97.0 | 72.7 | 69.5 | 98.7
    | 98.9 | 97.6 | 73.3 |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| ²²footnotemark: 2MRNet [[18](#bib.bib18)] | 86.8 | 97.0 | 72.7 | 69.5 | 98.7
    | 98.9 | 97.6 | 73.3 |'
- en: '| SCL [[33](#bib.bib33)] | 95.0 | 99.0 | 96.2 | 89.5 | 97.9 | 97.1 | 97.6 |
    87.7 |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| SCL [[33](#bib.bib33)] | 95.0 | 99.0 | 96.2 | 89.5 | 97.9 | 97.1 | 97.6 |
    87.7 |'
- en: '| SCL MLCL+DA [[19](#bib.bib19)] | 96.8 | - | - | - | - | - | - | - |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| SCL MLCL+DA [[19](#bib.bib19)] | 96.8 | - | - | - | - | - | - | - |'
- en: '| Context-blind SCL [[33](#bib.bib33)] | 12.2 | - | - | - | - | - | - | - |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| Context-blind SCL [[33](#bib.bib33)] | 12.2 | - | - | - | - | - | - | - |'
- en: '| Context-blind ResNet [[17](#bib.bib17)] | 12.2 | - | - | - | - | - | - |
    - |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| Context-blind ResNet [[17](#bib.bib17)] | 12.2 | - | - | - | - | - | - |
    - |'
- en: '| Context-blind CoPINet [[17](#bib.bib17)] | 14.2 | - | - | - | - | - | - |
    - |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| Context-blind CoPINet [[17](#bib.bib17)] | 14.2 | - | - | - | - | - | - |
    - |'
