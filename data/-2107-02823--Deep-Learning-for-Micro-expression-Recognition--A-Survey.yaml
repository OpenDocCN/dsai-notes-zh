- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:53:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:53:03
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2107.02823] Deep Learning for Micro-expression Recognition: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2107.02823] 深度学习在微表情识别中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2107.02823](https://ar5iv.labs.arxiv.org/html/2107.02823)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2107.02823](https://ar5iv.labs.arxiv.org/html/2107.02823)
- en: 'Deep Learning for Micro-expression Recognition: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在微表情识别中的应用：综述
- en: 'Yante Li, Jinsheng Wei, Yang Liu, Janne Kauttonen and Guoying Zhao * represents
    the corresponding author. Y. Li, J. Wei, Y. Liu, G. Zhao are with the Center for
    Machine Vision and Signal Analysis, University of Oulu, Oulu, FI-90014, Finland.
    E-mail: firstname.lastname@oulu.fi J. Wei is with the School of Telecommunications
    and Information Engineering, Nanjing University of Posts and Telecommunications,
    Nanjing 210003 China; Email: 2018010217@njupt.edu.cn. J. Kauttonen is with the
    School of Digital Business, Haaga-Helia University of Applied Sciences, Helsinki,
    FI-00520, Finland; Email: Janne.Kauttonen@haaga-helia.fi.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yante Li、Jinsheng Wei、Yang Liu、Janne Kauttonen 和 Guoying Zhao * 表示通讯作者。Y. Li、J.
    Wei、Y. Liu、G. Zhao 皆为芬兰奥卢大学机器视觉与信号分析中心的成员，地址：FI-90014，Oulu，芬兰。电子邮件：firstname.lastname@oulu.fi。J.
    Wei 为南京邮电大学通信与信息工程学院的成员，地址：210003，南京，中国；电子邮件：2018010217@njupt.edu.cn。J. Kauttonen
    为赫尔辛基应用科技大学数字商务学院的成员，地址：FI-00520，赫尔辛基，芬兰；电子邮件：Janne.Kauttonen@haaga-helia.fi。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Micro-expressions (MEs) are involuntary facial movements revealing people’s
    hidden feelings in high-stake situations and have practical importance in various
    fields. Early methods for Micro-expression Recognition (MER) are mainly based
    on traditional features. Recently, with the success of Deep Learning (DL) in various
    tasks, neural networks have received increasing interest in MER. Different from
    macro-expressions, MEs are spontaneous, subtle, and rapid facial movements, leading
    to difficult data collection and annotation, thus publicly available datasets
    are usually small-scale. Currently, various DL approaches have been proposed to
    solve the ME issues and improve MER performance. In this survey, we provide a
    comprehensive review of deep MER and define a new taxonomy for the field encompassing
    all aspects of MER based on DL, including datasets, each step of the deep MER
    pipeline, and performance comparisons of the most influential methods. The basic
    approaches and advanced developments are summarized and discussed for each aspect.
    Additionally, we conclude the remaining challenges and potential directions for
    the design of robust MER systems. Finally, ethical considerations in MER are discussed.
    To the best of our knowledge, this is the first survey of deep MER methods, and
    this survey can serve as a reference point for future MER research.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 微表情（MEs）是无意识的面部运动，能够在高风险情境中揭示人们的隐藏情感，并在多个领域具有实际重要性。早期的微表情识别（MER）方法主要基于传统特征。近年来，随着深度学习（DL）在各种任务中的成功，神经网络在MER中的应用受到越来越多的关注。与宏观表情不同，微表情是自发的、细微的和快速的面部运动，这使得数据收集和标注变得困难，因此公开可用的数据集通常规模较小。目前，已经提出了各种深度学习方法来解决微表情问题并提高MER性能。在这项综述中，我们对深度MER进行了全面回顾，并定义了一个新的分类体系，涵盖了基于深度学习的MER领域的所有方面，包括数据集、深度MER流程的每一个步骤，以及最具影响力的方法的性能比较。我们总结并讨论了每个方面的基本方法和先进发展。此外，我们总结了设计强健MER系统的剩余挑战和潜在方向。最后，讨论了MER中的伦理考虑。据我们所知，这是对深度MER方法的首次综述，这项综述可以作为未来MER研究的参考点。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Micro-expression recognition, Deep learning, Micro-expression dataset, Survey.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 微表情识别，深度学习，微表情数据集，综述。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Facial expression (FE) is one of the most powerful and universal means for human
    communication, which is highly associated with human mental states, attitudes,
    and intentions. Besides ordinary FEs (also known as macro-expressions) that we
    see daily, emotions can also be expressed in a special format of Micro-expressions
    (MEs) under certain conditions. MEs are FEs revealing people’s hidden feelings
    in high-stake situations when people try to conceal their true feelings [[1](#bib.bib1)].
    Different from macro-expressions, MEs are spontaneous, subtle, and rapid (1/25
    to 1/3 second) facial movements reacting to emotional stimulus [[2](#bib.bib2),
    [3](#bib.bib3)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 面部表情（FE）是人类交流中最强大且最普遍的手段之一，与人类的心理状态、态度和意图高度相关。除了我们每天看到的普通面部表情（也称为宏观表情），情感在特定条件下还可以以微表情（MEs）的特殊形式表达。ME是在高风险情况下揭示人们隐藏感受的面部表情，当人们试图掩饰真实情感时
    [[1](#bib.bib1)]。与宏观表情不同，ME是自发的、微妙的和快速的（1/25到1/3秒）面部运动，反应于情感刺激 [[2](#bib.bib2),
    [3](#bib.bib3)]。
- en: 'The ME phenomenon was firstly discovered by Haggard and Isaacs [[4](#bib.bib4)]
    in 1966\. Three years later, Ekman and Friesen also declared the finding of MEs
    [[5](#bib.bib5)] during examining psychiatric patient’s videos for lie detection.
    In the following years, Ekman et al.  continued ME research and developed the
    Facial Action Coding System (FACS) [[6](#bib.bib6)] and Micro Expression Training
    Tool (METT) [[7](#bib.bib7)]. Specifically, FACS breaks down FEs into individual
    components of muscle movement, called Action Units (AUs) [[6](#bib.bib6)]. AU
    analysis can effectively resolve the ambiguity issue to represent individual expression
    and increase Facial Expression Recognition (FER) performance [[8](#bib.bib8)].
    Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning for Micro-expression
    Recognition: A Survey") shows the example of micro- and macro-expressions as well
    as activated AUs in each FE. On the other hand, METT is helpful for increasing
    people’s emotional awareness. It can promote manual ME detection performance which
    provides a potential chance to build reliable ME datasets.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ME现象最初由Haggard和Isaacs于1966年发现 [[4](#bib.bib4)]。三年后，Ekman和Friesen在检查精神病患者视频以检测谎言时，也宣布发现了ME
    [[5](#bib.bib5)]。在接下来的几年中，Ekman等人继续研究ME，并开发了面部动作编码系统（FACS） [[6](#bib.bib6)] 和微表情训练工具（METT）
    [[7](#bib.bib7)]。具体来说，FACS将面部表情分解为肌肉运动的单独组成部分，称为动作单元（AUs） [[6](#bib.bib6)]。AU分析可以有效解决表示个体表情的模糊性问题，并提高面部表情识别（FER）性能
    [[8](#bib.bib8)]。图[1](#S1.F1 "图 1 ‣ 1 引言 ‣ 深度学习在微表情识别中的应用综述")展示了微表情和宏观表情的示例，以及每个FE中激活的AUs。另一方面，METT有助于提高人们的情感意识。它可以促进手动ME检测性能，为建立可靠的ME数据集提供潜在机会。
- en: '![Refer to caption](img/a4cc143073913d256643fd8464f85f73.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a4cc143073913d256643fd8464f85f73.png)'
- en: 'Figure 1: Examples of micro-expressions in CASME II [[9](#bib.bib9)] and macro-expressions
    in MMI [[10](#bib.bib10)], as well as the active AUs. The red arrow represents
    the muscle movement direction. AU4, AU6, AU7, AU9, AU12, AU15, and AU25 represent
    brow lowerer, cheek raise, lids tight, nose wrinkle, lip corner puller, lip corner
    depressor, and lips part, respectively.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：CASME II [[9](#bib.bib9)] 中的微表情和MMI [[10](#bib.bib10)] 中的宏观表情示例，以及活动的AUs。红色箭头表示肌肉运动方向。AU4、AU6、AU7、AU9、AU12、AU15和AU25分别代表降眉、颊部上升、眼睑紧闭、鼻子皱纹、嘴角上扬、嘴角下垂和嘴唇分开。
- en: MER is the task of classifying ME clips into various emotion categories. In
    each ME clip, the frame starting facial movements is denoted as the onset frame,
    while the end frame is the offset frame. The frame with the largest intensity
    is the apex frame. Like FER, MER also classifies facial images/sequences into
    categories such as anger, surprise, and happiness. However, MER is more challenging
    as spontaneous MEs are involuntary, subtle, and fleeting. In addition, MEs can
    also be impacted by emotional context and cultural background [[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)]. Therefore, it is difficult to collect and
    annotate ME data, leading to small-scale ME datasets and existing methods are
    incapable of dealing with subtleness and fleetness.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MER（情感识别任务）是将ME（微表情）片段分类到各种情感类别中。在每个ME片段中，启动面部运动的帧称为开始帧，而结束帧称为结束帧。具有最大强度的帧为顶点帧。像FER（面部表情识别）一样，MER也将面部图像/序列分类为愤怒、惊讶和幸福等类别。然而，MER更具挑战性，因为自发的ME是无意识的、微妙的和短暂的。此外，ME还可能受到情感背景和文化背景的影响
    [[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]。因此，收集和标注ME数据非常困难，导致ME数据集规模较小，现有方法无法处理其微妙性和短暂性。
- en: 'MER has drawn increasing interest recently due to its practical importance
    in many human-computer interaction systems. The first spontaneous MER research
    can be traced to Pfister et al.’s work [[14](#bib.bib14)] which utilized a Local
    Binary Pattern from Three Orthogonal Planes (LBP-TOP) [[15](#bib.bib15)] on the
    first public spontaneous ME dataset: SMIC [[16](#bib.bib16)]. Following the work
    of [[15](#bib.bib15)], various approaches based on appearance and geometry features
     [[17](#bib.bib17), [18](#bib.bib18)] were proposed for improving the performance
    of MER.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其在许多人机交互系统中的实际重要性，MER 最近引起了越来越多的关注。首次自发的 MER 研究可以追溯到 Pfister 等人 [[14](#bib.bib14)]
    的工作，该工作利用了三正交平面的局部二值模式（LBP-TOP）[[15](#bib.bib15)]，并应用于第一个公开的自发 ME 数据集：SMIC [[16](#bib.bib16)]。在
    [[15](#bib.bib15)] 的工作之后，提出了各种基于外观和几何特征的方法 [[17](#bib.bib17), [18](#bib.bib18)]，以提高
    MER 的性能。
- en: In recent years, with the advance of Deep Learning (DL) and its successful extensions
    on object detection [[19](#bib.bib19)], human tracking [[20](#bib.bib20), [21](#bib.bib21)],
    image retrieval [[22](#bib.bib22), [23](#bib.bib23)] and FER [[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26)], researchers have started to exploit MER with
    DL. Although MER with DL becomes challenging because of the limited ME samples
    and low intensity, great progress on MER has been made through designing effective
    shallow networks, exploring Generative Adversarial Net (GAN) [[27](#bib.bib27)]
    and so on. Currently, DL-based MER has achieved the state-of-the-art performance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着深度学习（DL）的进步及其在目标检测 [[19](#bib.bib19)]、人类跟踪 [[20](#bib.bib20), [21](#bib.bib21)]、图像检索
    [[22](#bib.bib22), [23](#bib.bib23)] 和面部表情识别（FER） [[24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26)] 等领域的成功应用，研究人员开始利用 DL 来探索 MER。尽管由于 ME 样本有限且强度低，使得基于 DL 的 MER
    变得具有挑战性，但通过设计有效的浅层网络、探索生成对抗网络（GAN）[[27](#bib.bib27)]等方法，在 MER 上取得了显著进展。目前，基于 DL
    的 MER 已达到最先进的性能。
- en: In this survey, we review the research on MER by DL since 2016 when the DL technology
    was firstly adopted in MER. Due to the page limitation, the representative works
    published in well-known journals and conferences, such as IEEE TPAMI, IEEE TAC,
    IEEE TIP, and ACM MM are specifically discussed. The ordinary FER approaches and
    MER with traditional learning methods are not considered in this survey. Although
    a few MER surveys have discussed the historical evolution and algorithmic pipelines
    for MER [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33)], they mainly focus on traditional methods
    and only introduce some recent DL approaches. The DL-based MER has not been discussed
    systematically and specifically. As far as we know, this is the first survey of
    the DL-based MER. Different from previous surveys, we analyze the strengths and
    shortcomings of dynamic network inputs which are important for MER based on DL.
    Furthermore, the network blocks, architectures, training strategies, and losses
    are discussed and summarized in detail and future research directions are identified.
    The goal of this survey is to provide a DL-based MER dictionary that can serve
    as a reference point for future MER research.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们回顾了自 2016 年 DL 技术首次应用于 MER 以来基于 DL 的 MER 研究。由于篇幅限制，我们特别讨论了在知名期刊和会议上发表的代表性工作，如
    IEEE TPAMI、IEEE TAC、IEEE TIP 和 ACM MM。本调查未考虑普通 FER 方法和基于传统学习方法的 MER。虽然已有一些 MER
    调查讨论了 MER 的历史演变和算法流程 [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33)]，但它们主要集中在传统方法上，仅介绍了一些近期的 DL 方法。基于 DL 的 MER
    尚未被系统地和具体地讨论。据我们所知，这是首个基于 DL 的 MER 调查。与以往调查不同，我们分析了动态网络输入的优缺点，这对于基于 DL 的 MER 非常重要。此外，网络模块、架构、训练策略和损失函数也进行了详细讨论和总结，并确定了未来的研究方向。本调查的目标是提供一个基于
    DL 的 MER 词典，作为未来 MER 研究的参考点。
- en: '![Refer to caption](img/1a3d225eb9c50d4f1532bb770ec13264.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1a3d225eb9c50d4f1532bb770ec13264.png)'
- en: 'Figure 2: Examples of ME samples in ME datasets for MER.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：用于 MER 的 ME 数据集中的 ME 样本示例。
- en: 'This paper is organized as follows: Section [2](#S2 "2 Datasets ‣ Deep Learning
    for Micro-expression Recognition: A Survey") introduces spontaneous ME datasets.
    Section [3](#S3 "3 A taxonomy for MER based on DL ‣ Deep Learning for Micro-expression
    Recognition: A Survey") presents the taxonomy we defined for MER based on DL.
    Section [4](#S4 "4 Inputs ‣ Deep Learning for Micro-expression Recognition: A
    Survey") discusses the various inputs for deep MER. Section [5](#S5 "5 Deep networks
    for MER ‣ Deep Learning for Micro-expression Recognition: A Survey") provides
    a detailed review of neural networks for MER. The evaluation matrix, protocol,
    and the performance of representative DL-based MER are described in Section [6](#S6
    "6 Experiments ‣ Deep Learning for Micro-expression Recognition: A Survey"). Section
    [7](#S7 "7 Challenges AND FUTURE DIRECTIONS ‣ Deep Learning for Micro-expression
    Recognition: A Survey") summarizes current challenges and potential study directions.
    Finally, Section 8 discusses the ethical considerations.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文组织如下：第[2](#S2 "2 数据集 ‣ 微表情识别的深度学习：综述")节介绍自发 ME 数据集。第[3](#S3 "3 基于深度学习的 MER
    分类 ‣ 微表情识别的深度学习：综述")节介绍了我们为 MER 基于深度学习定义的分类。第[4](#S4 "4 输入 ‣ 微表情识别的深度学习：综述")节讨论了深度
    MER 的各种输入。第[5](#S5 "5 深度网络用于 MER ‣ 微表情识别的深度学习：综述")节提供了关于 MER 的神经网络的详细回顾。评估矩阵、协议和代表性的基于深度学习的
    MER 性能在第[6](#S6 "6 实验 ‣ 微表情识别的深度学习：综述")节中描述。第[7](#S7 "7 挑战与未来方向 ‣ 微表情识别的深度学习：综述")节总结了当前的挑战和潜在的研究方向。最后，第
    8 节讨论了伦理考量。
- en: 'TABLE I: Spontaneous Datasets for MER'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: MER 的自发数据集'
- en: '| Database | Resolution | Facial size | Frame rate | Samples | subjects | Expression
    | AU | Apex | Eth | Env |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 数据库 | 分辨率 | 面部大小 | 帧率 | 样本 | 受试者 | 表情 | AU | Apex | Eth | Env |'
- en: '| SMIC HS/NIR/VIS [[16](#bib.bib16)] | $640\times 480$ | $190\times 230$ |
    100/25/25 | 164/71/71 | 16/8/8 | Pos (51) Neg (70) Sur (43) / Pos (28) Neg (23)
    Sur (20) / Pos (28) Neg (24) Sur (19) | $\circ$ | $\circ$ | 3 | L |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| SMIC HS/NIR/VIS [[16](#bib.bib16)] | $640\times 480$ | $190\times 230$ |
    100/25/25 | 164/71/71 | 16/8/8 | Pos (51) Neg (70) Sur (43) / Pos (28) Neg (23)
    Sur (20) / Pos (28) Neg (24) Sur (19) | $\circ$ | $\circ$ | 3 | L |'
- en: '| CASME [[34](#bib.bib34)] | $640\times 480$ $1280\times 720$ | $150\times
    90$ | 60 | 195 | 35 | Hap (5) Dis (88) Sad (6) Con (3) Fea (2) Ten (28) Sur (20)
    Rep (40) | $\bullet$ | $\bullet$ | 1 | L |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| CASME [[34](#bib.bib34)] | $640\times 480$ $1280\times 720$ | $150\times
    90$ | 60 | 195 | 35 | Hap (5) Dis (88) Sad (6) Con (3) Fea (2) Ten (28) Sur (20)
    Rep (40) | $\bullet$ | $\bullet$ | 1 | L |'
- en: '| CASME II [[9](#bib.bib9)] | $640\times 480$ | $250\times 340$ | 200 | 247
    | 35 |  Hap (33) Sur (25) Dis (60) Rep (27) Oth (102) | $\bullet$ | $\bullet$
    | 1 | L |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| CASME II [[9](#bib.bib9)] | $640\times 480$ | $250\times 340$ | 200 | 247
    | 35 |  Hap (33) Sur (25) Dis (60) Rep (27) Oth (102) | $\bullet$ | $\bullet$
    | 1 | L |'
- en: '| CAS(ME)²  [[35](#bib.bib35)] | $640\times 480$ | - | 30 | Macro 300 Micro
    57 | 22 | Hap (51) Neg (70) Sur (43) Oth (19) | $\bullet$ | $\bullet$ | 1 | L
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| CAS(ME)²  [[35](#bib.bib35)] | $640\times 480$ | - | 30 | Macro 300 Micro
    57 | 22 | Hap (51) Neg (70) Sur (43) Oth (19) | $\bullet$ | $\bullet$ | 1 | L
    |'
- en: '| SAMM [[36](#bib.bib36)] | $2040\times 1088$ | $400\times 400$ | 200 | 159
    | 32 | Hap (24) Ang (20) Sur (13) Dis (8) Fea (7) Sad (3) Oth (84) | $\bullet$
    | $\bullet$ | 13 | L |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| SAMM [[36](#bib.bib36)] | $2040\times 1088$ | $400\times 400$ | 200 | 159
    | 32 | Hap (24) Ang (20) Sur (13) Dis (8) Fea (7) Sad (3) Oth (84) | $\bullet$
    | $\bullet$ | 13 | L |'
- en: '| MEVIEW [[37](#bib.bib37)] | $720\times 1280$ | - | 25 | 31 | 16 | Hap (6)
    Ang (2) Sur (9) Dis (1) Fea (3) Unc (13) Con(6) | $\bullet$ | $\circ$ | - | W
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| MEVIEW [[37](#bib.bib37)] | $720\times 1280$ | - | 25 | 31 | 16 | Hap (6)
    Ang (2) Sur (9) Dis (1) Fea (3) Unc (13) Con(6) | $\bullet$ | $\circ$ | - | W
    |'
- en: '| MMEW [[32](#bib.bib32)] | $1920\times 1080$ | $400\times 400$ | 90 | 300
    | 36 | Hap (36) Ang (8) Sur (80) Dis (72) Fea (16) Sad (13) Oth (102) | $\bullet$
    | $\bullet$ | 1 | L |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| MMEW [[32](#bib.bib32)] | $1920\times 1080$ | $400\times 400$ | 90 | 300
    | 36 | Hap (36) Ang (8) Sur (80) Dis (72) Fea (16) Sad (13) Oth (102) | $\bullet$
    | $\bullet$ | 1 | L |'
- en: '| Composite ME [[38](#bib.bib38)] | $640\times 480$ $1280\times 720$ $720\times
    1280$ | $150\times 90$ $250\times 340$ $400\times 400$ | 200 | 442 | 68 | Pos
    (109), Neg (250), and Sur (83) | $\circ\bullet$ | $\circ\bullet$ | 13 | L |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 复合 ME [[38](#bib.bib38)] | $640\times 480$ $1280\times 720$ $720\times 1280$
    | $150\times 90$ $250\times 340$ $400\times 400$ | 200 | 442 | 68 | Pos (109),
    Neg (250), 和 Sur (83) | $\circ\bullet$ | $\circ\bullet$ | 13 | L |'
- en: '| Compound ME [[39](#bib.bib39)] | $640\times 480$ $1280\times 720$ $720\times
    1280$ | $150\times 90$ | 200 | 1050 | 90 | Neg (233) Pos (82) Sur (70) PS (74)
    N S (236) PN (197) NN (158) | $\circ\bullet$ | $\circ\bullet$ | 13 | L |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 复合 ME [[39](#bib.bib39)] | $640\times 480$ $1280\times 720$ $720\times 1280$
    | $150\times 90$ | 200 | 1050 | 90 | Neg (233) Pos (82) Sur (70) PS (74) N S (236)
    PN (197) NN (158) | $\circ\bullet$ | $\circ\bullet$ | 13 | L |'
- en: '¹ Eth: Ethnicity; Env : Environment.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '¹ Eth: 种族; Env : 环境。'
- en: '² Pos: Positive; Neg: Negative; Sur: Surprise; Hap: Happiness; Dis: Disgust;
    Rep: Repression; Ang: Anger; Fea: Fear; Sad: Sadness; Con: Contempt; Unc: Unclear;
    Oth: Others; PS: Positively surprise; NS Negatively surprise; PN: Positively negative;
    NN: Negatively negative; L:Laboratory; W:In the wild.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '² Pos: 积极；Neg: 消极；Sur: 惊讶；Hap: 快乐；Dis: 厌恶；Rep: 压抑；Ang: 生气；Fea: 恐惧；Sad: 伤心；Con:
    蔑视；Unc: 不明确；Oth: 其他；PS: 积极惊讶；NS: 消极惊讶；PN: 积极消极；NN: 消极消极；L: 实验室；W: 野外。'
- en: ³ $\circ$ represents unlabeled; $\bullet$ represents labeled and - represents
    unknown
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ³ $\circ$ 代表未标记；$\bullet$ 代表已标记；- 代表未知
- en: 2 Datasets
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数据集
- en: 'Different from macro-expressions which can be easily captured in our daily
    life, MEs are involuntary brief FEs, particularly occurring under high stake situations.
    Four early databases appeared continuously around 2010: Canal9 [[40](#bib.bib40)],
    York-DDT [[41](#bib.bib41)], Polikvsky’s database [[41](#bib.bib41)] and USF-HD
    [[42](#bib.bib42)]. However, Canal9 and York-DDT are not aimed for ME research.
    Polikvsky’s database and USF-HD include only posed MEs which are collected by
    asking participants to intentionally pose or mimic a micro movement. The posed
    expressions contradict with the spontaneous nature of MEs. Currently, these databases
    are not used anymore for MER. In the recent years, several spontaneous ME databases
    were created, including: SMIC [[16](#bib.bib16)] and its extended version SMIC-E,
    CASME [[34](#bib.bib34)], CASME II [[9](#bib.bib9)], CAS(ME)² [[35](#bib.bib35)],
    SAMM [[36](#bib.bib36)], and micro-and-macro expression warehouse (MMEW) [[32](#bib.bib32)].
    In this survey, we focus on the spontaneous datasets.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们日常生活中容易捕捉到的宏观表情不同，MEs（微表情）是无意识的短暂FE（面部表情），尤其是在高风险情境下出现。四个早期数据库在2010年左右相继出现：Canal9 [[40](#bib.bib40)]、York-DDT
    [[41](#bib.bib41)]、Polikvsky的数据库 [[41](#bib.bib41)] 和 USF-HD [[42](#bib.bib42)]。然而，Canal9
    和 York-DDT 并不是专门用于ME研究的。Polikvsky的数据库和USF-HD仅包含通过要求参与者有意展示或模仿微小动作所收集的摆拍表情。摆拍表情与MEs的自发性质相矛盾。目前，这些数据库已不再用于ME研究。近年来，创建了几个自发ME数据库，包括：SMIC [[16](#bib.bib16)]
    及其扩展版本SMIC-E、CASME [[34](#bib.bib34)]、CASME II [[9](#bib.bib9)]、CAS(ME)² [[35](#bib.bib35)]、SAMM [[36](#bib.bib36)]
    和 微表情与宏观表情仓库（MMEW） [[32](#bib.bib32)]。在这次调查中，我们重点关注自发数据集。
- en: In a general ME dataset collection procedure, participants are asked to keep
    a poker face while watching video clips to induce spontaneous MEs. The video clips
    are selected according to previous psychological studies, which can elicit strong
    emotions. Commonly, a high-speed camera is utilized to record facial videos. After
    one participant watched a video clip, he/she fills in a self-report questionnaire
    to report his/her true feelings about the video clip. As well, considering cultural
    backgrounds may have an impact on MEs [[43](#bib.bib43)], participants from different
    ethnicities could be recruited [[36](#bib.bib36)] for the potential study of cultural
    impact on MEs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般的ME数据集收集过程中，参与者被要求在观看视频片段时保持面无表情，以引发自发ME。这些视频片段是根据先前的心理学研究选择的，能够引发强烈的情绪。通常，使用高速摄像机录制面部视频。在一个参与者观看完视频片段后，他/她填写自我报告问卷，报告他/她对视频片段的真实感受。此外，考虑到文化背景可能对ME产生影响
    [[43](#bib.bib43)]，可以招募不同民族的参与者 [[36](#bib.bib36)] 以研究文化对ME的潜在影响。
- en: 'Since the MEs are subtle and rapid, annotators are usually trained with FACS
    and certified facial action unit coders are employed to detect the MEs in the
    facial videos. The FACS helps people look precisely at the facial movements to
    make ME detection reliable. Specifically, when the duration of the facial action
    unit is less than 0.5s, the clip is regarded as a ME clip. The MEs are annotated
    into discrete categories. In SMIC [[16](#bib.bib16)], the emotions are labeled
    as ‘positive’, ‘negative’, and ‘surprise’ according to the participants’ self-reports.
    However, mixed emotions may be induced while the participants watch one video
    clip. Annotations based on the general emotion reported after watching the video,
    which usually allows one emotion, are not accurate. To this end, several datasets,
    such as CASME [[34](#bib.bib34)] and CASME II [[9](#bib.bib9)], consider AUs,
    self-reports, and the watched video clips to label the MEs. When there are ambiguities
    and conflicts in the emotion annotation, the emotion is annotated as ‘others’.
    Furthermore, to alleviate annotation bias caused by an individual annotator, the
    ME annotations are always carried out through cross-validation by multiple annotators.
    The specific details of datasets are introduced as followings:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于情感表达微妙且迅速，标注员通常接受FACS培训，并雇用认证的面部动作单元编码员来检测面部视频中的情感表达。FACS帮助人们精确观察面部运动，使情感表达检测更可靠。具体来说，当面部动作单元的持续时间少于0.5秒时，该片段被视为情感表达片段。情感表达被标注为离散类别。在SMIC
    [[16](#bib.bib16)]中，情感根据参与者的自我报告被标记为“正面”、“负面”和“惊讶”。然而，当参与者观看一个视频片段时，可能会引发混合情感。基于观看视频后的一般情感报告的标注（通常允许一种情感）并不准确。为此，CASME
    [[34](#bib.bib34)] 和CASME II [[9](#bib.bib9)]等多个数据集考虑了面部动作单元、自我报告和观看的视频片段来标记情感表达。当情感标注存在歧义和冲突时，该情感标注为“其他”。此外，为了减轻个别标注员造成的标注偏差，情感表达标注总是通过多个标注员的交叉验证来进行。数据集的具体细节如下：
- en: 'SMIC [[16](#bib.bib16)] is consisted of three subsets: SMIC-HS, SMIC-VIS and
    SMIC-NIR. SMIC-VIS and SMIC-NIR contain 71 samples recorded by normal speed cameras
    with 25 fps of visual (VIS) and near-inferred light range (NIR), respectively.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SMIC [[16](#bib.bib16)] 由三个子集组成：SMIC-HS、SMIC-VIS和SMIC-NIR。SMIC-VIS和SMIC-NIR包含71个样本，由正常速度的摄像机记录，分别使用25
    fps的可视光（VIS）和近红外光（NIR）。
- en: CASME [[34](#bib.bib34)] contains spontaneous 159 ME clips from 19 subjects
    including frames from onset to offset. The emotions were labeled partly based
    on AUs and also taking account of participants’ self-reports and the content of
    the video episodes. Besides the onset and offset, the apex frames are also labeled.
    The shortcoming of CASME is the imbalanced sample distribution among classes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: CASME [[34](#bib.bib34)] 包含来自19名受试者的159个自发情感表达片段，包括从开始到结束的帧。情感标注部分基于面部动作单元，同时考虑了参与者的自我报告和视频片段的内容。除了开始和结束，顶峰帧也被标注。CASME的不足之处在于类别间样本分布不均。
- en: CASME II [[9](#bib.bib9)] is an improved version of the CASME dataset. Samples
    in CASME II are increased to 247 MEs from 26 subjects and they are recorded by
    high-speed camera at 200 fps with face sizes cropped to $280\times 340$. Thus,
    it has a greater temporal and spatial resolution, compared with CASME.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: CASME II [[9](#bib.bib9)] 是CASME数据集的改进版。CASME II中的样本数量增加到247个情感表达样本，来自26名受试者，记录时使用了200
    fps的高速摄像机，面部尺寸裁剪至$280\times 340$。因此，与CASME相比，它具有更高的时间和空间分辨率。
- en: CAS(ME)² [[35](#bib.bib35)] consists of spontaneous macro- and micro-expressions
    elicited from 22 subjects. CAS(ME)² has samples with longer durations which makes
    it suitable for ME spotting. Compared to the above datasets, the samples in CAS(ME)²
    were recorded with a relatively low frame rate in a relatively small number of
    ME samples, which makes it unsuitable for DL approaches.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CAS(ME)² [[35](#bib.bib35)] 由22名受试者的自发宏观和微观表情组成。CAS(ME)²具有较长持续时间的样本，使其适合于情感表达检测。与上述数据集相比，CAS(ME)²的样本记录帧率较低，样本数量相对较少，这使其不适合深度学习方法。
- en: SAMM [[36](#bib.bib36)] collects 159 ME samples from 32 participants. The samples
    were collected by a gray-scale camera at 200 fps in controlled lighting conditions
    to prevent flickering. Unlike previous datasets that lack ethnic diversity, the
    participants are from 13 different ethnicities.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: SAMM [[36](#bib.bib36)] 收集了来自32名参与者的159个情感表达样本。这些样本由灰度摄像机在200 fps的受控光照条件下采集，以防止闪烁。与缺乏民族多样性的先前数据集不同，参与者来自13个不同的民族。
- en: MEVIEW [[37](#bib.bib37)] is in-the-wild ME dataset. The samples in MEVIEW are
    collected from poker games and TV interviews on the Internet. In total, 31 videos
    from 16 individuals were annotated in the dataset and the average length of videos
    is three seconds.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: MEVIEW [[37](#bib.bib37)] 是一个真实场景中的微表情数据集。MEVIEW 中的样本来自互联网的扑克游戏和电视采访。数据集中总共有
    16 人的 31 个视频被标注，视频的平均长度为三秒。
- en: MMEW [[32](#bib.bib32)] contains 300 ME and 900 macro-expression samples acted
    out by the same participants with a larger resolution ($1920\times 1080$ pixels).
    MEs and macro-expressions in MMEW were annotated to the same emotion classes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: MMEW [[32](#bib.bib32)] 包含 300 个微表情和 900 个宏表情样本，由相同的参与者表演，分辨率更高（$1920\times
    1080$ 像素）。MMEW 中的微表情和宏表情被标注到相同的情感类别。
- en: 'The composite dataset [[38](#bib.bib38)] is proposed by the 2nd Micro-Expression
    Grand Challenge (MEGC2019). The composite dataset merges samples from three spontaneous
    facial ME datasets: CASME II [[9](#bib.bib9)], SAMM [[36](#bib.bib36)], and SMIC-HS
    [[16](#bib.bib16)]. This is to facilitate the evaluation of newly developed methods.
    As the annotations in the three datasets vary hugely, the composite dataset unifies
    emotion labels in all three datasets. The emotion labels are re-annotated as positive,
    negative, and surprise.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 复合数据集 [[38](#bib.bib38)] 由第二届微表情大奖赛（MEGC2019）提出。复合数据集合并了来自三个自发面部微表情数据集的样本：CASME
    II [[9](#bib.bib9)]、SAMM [[36](#bib.bib36)] 和 SMIC-HS [[16](#bib.bib16)]。这样做是为了便于评估新开发的方法。由于这三个数据集中的标注差异巨大，复合数据集统一了所有三个数据集中的情感标签。情感标签被重新标注为正面、负面和惊讶。
- en: 'The compound micro-expression dataset (CMED) [[39](#bib.bib39), [44](#bib.bib44)]
    is constructed by combining MEs from the CASME, CASME II, CAS(ME)², SMIC-HS, and
    SAMM datasets. Specifically, the MEs are divided into basic and compound emotional
    categories, as shown in Table [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ Deep Learning
    for Micro-expression Recognition: A Survey"). Psychological studies demonstrate
    that there are usually complex expressions in daily life. Multiple emotions co-exist
    in one FE, termed as “compound expressions” [[44](#bib.bib44)]. Compound expression
    analysis reflects more complex mental states and more abundant human facial emotions.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '复合微表情数据集（CMED） [[39](#bib.bib39), [44](#bib.bib44)] 通过将来自 CASME、CASME II、CAS(ME)²、SMIC-HS
    和 SAMM 数据集的微表情合并构建。具体而言，微表情被分为基本和复合情感类别，如表 [I](#S1.T1 "TABLE I ‣ 1 Introduction
    ‣ Deep Learning for Micro-expression Recognition: A Survey") 所示。心理学研究表明，日常生活中通常存在复杂的表情。多个情感共存于一个面部表情中，称为“复合表情”
    [[44](#bib.bib44)]。复合表情分析反映了更复杂的心理状态和更丰富的人类面部情感。'
- en: 'The specific comparisons of the ME datasets are shown in Table [I](#S1.T1 "TABLE
    I ‣ 1 Introduction ‣ Deep Learning for Micro-expression Recognition: A Survey")
    and example samples are shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Deep Learning for Micro-expression Recognition: A Survey"). Although MEVIEW
    collects MEs in the wild, the number of ME samples is too small to learn robust
    ME features. The state-of-the-art approaches are commonly tested on the SMIC-HS,
    CASME [[34](#bib.bib34)], CASME II [[9](#bib.bib9)], and SAMM databases. As some
    emotions are difficult to trigger, such as fear and contempt, these categories
    have only a few samples and are not enough for learning. In most practical experiments,
    only the emotion categories with more than 10 samples are considered. Recently,
    the composite dataset is popular, because it can verify the generalization ability
    of the method on datasets with different natures. For further increasing the MER
    performance, MMEW collected micro- and macro-expressions from the same subjects
    which may be helpful for further cross-modal research.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '微表情数据集的具体比较见表 [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ Deep Learning for Micro-expression
    Recognition: A Survey")，示例样本见图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning
    for Micro-expression Recognition: A Survey")。尽管 MEVIEW 收集了真实场景中的微表情，但微表情样本的数量太少，无法学习稳健的微表情特征。最先进的方法通常在
    SMIC-HS、CASME [[34](#bib.bib34)]、CASME II [[9](#bib.bib9)] 和 SAMM 数据库上测试。由于某些情感如恐惧和蔑视难以引发，这些类别只有少量样本，不足以进行学习。在大多数实际实验中，仅考虑样本数量超过
    10 的情感类别。最近，复合数据集变得流行，因为它可以验证方法在具有不同性质的数据集上的泛化能力。为了进一步提高微表情识别的性能，MMEW 收集了来自相同受试者的微表情和宏表情，这可能有助于进一步的跨模态研究。'
- en: <svg   height="1" overflow="visible" version="1.1" width="1"><g transform="translate(0,1)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="0" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">\Tree</foreignobject>
    <foreignobject width="8.3" height="49.81" transform="matrix(1 0 0 -1 0 16.6)"
    overflow="visible">|  |
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="1" overflow="visible" version="1.1" width="1"><g transform="translate(0,1)
    matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignobject
    width="0" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">\Tree</foreignobject>
    <foreignobject width="8.3" height="49.81" transform="matrix(1 0 0 -1 0 16.6)"
    overflow="visible">|  |
- en: '|  |</foreignobject> <foreignobject width="-1.54" height="0" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">[45](#bib.bib45)[46](#bib.bib46)[47](#bib.bib47)[48](#bib.bib48)[49](#bib.bib49)[50](#bib.bib50)[51](#bib.bib51)[52](#bib.bib52)[48](#bib.bib48)[53](#bib.bib53)[54](#bib.bib54)[55](#bib.bib55),
    [56](#bib.bib56)[57](#bib.bib57)[58](#bib.bib58)[59](#bib.bib59)[60](#bib.bib60)[61](#bib.bib61)[58](#bib.bib58),
    [27](#bib.bib27)</foreignobject> <foreignobject width="40.55" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| [62](#bib.bib62)[17](#bib.bib17), [63](#bib.bib63)[64](#bib.bib64)
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  |</foreignobject> <foreignobject width="-1.54" height="0" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">[45](#bib.bib45)[46](#bib.bib46)[47](#bib.bib47)[48](#bib.bib48)[49](#bib.bib49)[50](#bib.bib50)[51](#bib.bib51)[52](#bib.bib52)[48](#bib.bib48)[53](#bib.bib53)[54](#bib.bib54)[55](#bib.bib55),
    [56](#bib.bib56)[57](#bib.bib57)[58](#bib.bib58)[59](#bib.bib59)[60](#bib.bib60)[61](#bib.bib61)[58](#bib.bib58),
    [27](#bib.bib27)</foreignobject> <foreignobject width="40.55" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| [62](#bib.bib62)[17](#bib.bib17), [63](#bib.bib63)[64](#bib.bib64)
    |'
- en: '| [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67) |</foreignobject> <foreignobject
    width="-2.31" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[59](#bib.bib59),
    [64](#bib.bib64), [68](#bib.bib68), [69](#bib.bib69)[61](#bib.bib61), [70](#bib.bib70),
    [58](#bib.bib58), [71](#bib.bib71), [72](#bib.bib72)[73](#bib.bib73)[74](#bib.bib74)[75](#bib.bib75)[76](#bib.bib76),
    [77](#bib.bib77)[78](#bib.bib78)</foreignobject> <foreignobject width="41.7" height="24.91"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| [79](#bib.bib79)[80](#bib.bib80)[60](#bib.bib60)[81](#bib.bib81)[82](#bib.bib82)
    |</foreignobject>  <foreignobject width="43.63" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| [83](#bib.bib83)[84](#bib.bib84)[71](#bib.bib71)
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67) |</foreignobject> <foreignobject
    width="-2.31" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[59](#bib.bib59),
    [64](#bib.bib64), [68](#bib.bib68), [69](#bib.bib69)[61](#bib.bib61), [70](#bib.bib70),
    [58](#bib.bib58), [71](#bib.bib71), [72](#bib.bib72)[73](#bib.bib73)[74](#bib.bib74)[75](#bib.bib75)[76](#bib.bib76),
    [77](#bib.bib77)[78](#bib.bib78)</foreignobject> <foreignobject width="41.7" height="24.91"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| [79](#bib.bib79)[80](#bib.bib80)[60](#bib.bib60)[81](#bib.bib81)[82](#bib.bib82)
    |</foreignobject>  <foreignobject width="43.63" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">| [83](#bib.bib83)[84](#bib.bib84)[71](#bib.bib71)
    |'
- en: '| [85](#bib.bib85) |</foreignobject>  <foreignobject width="47.46" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| [86](#bib.bib86), [76](#bib.bib76)[81](#bib.bib81)[78](#bib.bib78)[83](#bib.bib83)[60](#bib.bib60)[87](#bib.bib87),
    [88](#bib.bib88), [61](#bib.bib61)[89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91)
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| [85](#bib.bib85) |</foreignobject>  <foreignobject width="47.46" height="49.81"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">| [86](#bib.bib86), [76](#bib.bib76)[81](#bib.bib81)[78](#bib.bib78)[83](#bib.bib83)[60](#bib.bib60)[87](#bib.bib87),
    [88](#bib.bib88), [61](#bib.bib61)[89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91)
    |'
- en: '| [92](#bib.bib92) |</foreignobject> <foreignobject width="-6.92" height="0"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[64](#bib.bib64), [93](#bib.bib93),
    [74](#bib.bib74)[94](#bib.bib94), [88](#bib.bib88), [87](#bib.bib87)[95](#bib.bib95),
    [96](#bib.bib96), [97](#bib.bib97)[98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100),
    [101](#bib.bib101)[102](#bib.bib102)[103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105)[106](#bib.bib106),
    [107](#bib.bib107)[108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110)[111](#bib.bib111)[112](#bib.bib112),
    [113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115), [105](#bib.bib105),
    [116](#bib.bib116)</foreignobject><g stroke="#000000" fill="#000000" color="#000000"><foreignobject
    width="-2.69" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[58](#bib.bib58)[117](#bib.bib117)[76](#bib.bib76)[91](#bib.bib91)[109](#bib.bib109)[87](#bib.bib87),
    [74](#bib.bib74)[118](#bib.bib118), [119](#bib.bib119), [69](#bib.bib69)[120](#bib.bib120),
    [121](#bib.bib121), [122](#bib.bib122)[123](#bib.bib123)[124](#bib.bib124), [125](#bib.bib125)[126](#bib.bib126),
    [127](#bib.bib127), [128](#bib.bib128)[129](#bib.bib129), [91](#bib.bib91)</foreignobject></g></g></svg>![Refer
    to caption](img/69e108b8a9d56412a08a729c1304a9f3.png)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '| [92](#bib.bib92) |</foreignobject> <foreignobject width="-6.92" height="0"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[64](#bib.bib64), [93](#bib.bib93),
    [74](#bib.bib74)[94](#bib.bib94), [88](#bib.bib88), [87](#bib.bib87)[95](#bib.bib95),
    [96](#bib.bib96), [97](#bib.bib97)[98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100),
    [101](#bib.bib101)[102](#bib.bib102)[103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105)[106](#bib.bib106),
    [107](#bib.bib107)[108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110)[111](#bib.bib111)[112](#bib.bib112),
    [113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115), [105](#bib.bib105),
    [116](#bib.bib116)</foreignobject><g stroke="#000000" fill="#000000" color="#000000"><foreignobject
    width="-2.69" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">[58](#bib.bib58)[117](#bib.bib117)[76](#bib.bib76)[91](#bib.bib91)[109](#bib.bib109)[87](#bib.bib87),
    [74](#bib.bib74)[118](#bib.bib118), [119](#bib.bib119), [69](#bib.bib69)[120](#bib.bib120),
    [121](#bib.bib121), [122](#bib.bib122)[123](#bib.bib123)[124](#bib.bib124), [125](#bib.bib125)[126](#bib.bib126),
    [127](#bib.bib127), [128](#bib.bib128)[129](#bib.bib129), [91](#bib.bib91)</foreignobject></g></g></svg>![参见说明](img/69e108b8a9d56412a08a729c1304a9f3.png)'
- en: 'Figure 3: Taxonomy for MER based on deep learning. The studies cited on the
    branches are example approaches discussed in this paper. The future directions
    and corresponding approaches are shown on the right side. The future directions
    are annotated in brackets.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于深度学习的MER分类法。分支上引用的研究是本文讨论的示例方法。未来的方向和相应的方法显示在右侧，未来的方向用括号标注。
- en: 3 A taxonomy for MER based on DL
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于深度学习的MER分类法
- en: 'Fig. 3 shows a taxonomy we summarize for MER based on DL, built along the important
    components including input and network. As the ME sequences have subtle movements
    and limited samples, different inputs have big impacts on MER performance. Thus
    the input plays an important role in MER. Firstly, the inputs need to be pre-processed
    for training a robust network. The specific pre-processing approaches and the
    strengths and shortcomings of various input modalities are discussed in Section
    [4](#S4 "4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey").
    Then, the networks introduced in Section [5](#S5 "5 Deep networks for MER ‣ Deep
    Learning for Micro-expression Recognition: A Survey") are utilized to discriminate
    between MEs. A common MER network can be described from four aspects: block, architecture,
    training strategy, and loss. Firstly, we introduce the special blocks designed
    to solve the ME challenges. Then, we describe the architecture in terms of single-stream,
    multi-stream, cascaded networks, and multi-task learning. Finally, the training
    strategies and loss functions for training networks are discussed. The future
    directions are annotated on the right side of Fig. 3\. All the methods discussed
    in this survey are face-based MER with DL.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '图3展示了我们基于深度学习（DL）为微表情识别（MER）总结的分类法，涵盖了输入和网络等重要组件。由于微表情序列具有细微的运动和有限的样本，不同的输入对MER性能有很大影响。因此，输入在MER中扮演了重要角色。首先，输入需要经过预处理，以训练出一个稳健的网络。具体的预处理方法以及各种输入模态的优缺点在第[4节](#S4
    "4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey")中进行了讨论。然后，第[5节](#S5
    "5 Deep networks for MER ‣ Deep Learning for Micro-expression Recognition: A Survey")中介绍的网络被用于区分微表情。一个常见的MER网络可以从四个方面来描述：模块、架构、训练策略和损失。首先，我们介绍了为解决微表情挑战而设计的特殊模块。接着，我们描述了在单流、多流、级联网络和多任务学习方面的架构。最后，讨论了用于训练网络的训练策略和损失函数。图3右侧标注了未来的方向。本次综述讨论的所有方法都是基于面部的深度学习MER。'
- en: 4 Inputs
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 输入
- en: 4.1 Pre-processing
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 预处理
- en: Like ordinary FEs, pre-processing involving face detection and alignment is
    required for robust MER. Compared with common FEs, MEs have low intensity, short
    duration, and small-scale datasets making MER more difficult. Therefore, besides
    traditional pre-processing steps, motion magnification, temporal normalization,
    regions-of-interest, and data augmentation have also been undertaken for better
    MER performance.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通的FE相比，MER（面部表情识别）的稳健性要求进行面部检测和对齐的预处理。与常见的FE相比，ME具有低强度、短持续时间和小规模的数据集，使得MER更加困难。因此，除了传统的预处理步骤，还进行了运动放大、时间归一化、感兴趣区域和数据增强，以提高MER的性能。
- en: 4.1.1 Face detection and registration
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 面部检测和注册
- en: For processing MEs, face detection which removes the background and gets the
    facial region is the first step. One of the most widely used algorithms for face
    detection is Viola-Jones [[45](#bib.bib45)] based on a cascade of weak classifiers.
    However, this method can not deal with large pose variations and occlusions. Matsugu et
    al.  [[46](#bib.bib46)] firstly adopted CNN network for face detection with a
    rule-based algorithm, which is robust to translation, scale, and pose. Recently,
    face detectors based on DL have been utilized in popular open source libraries,
    such as dlib and OpenCV.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 处理ME的第一步是面部检测，去除背景并获取面部区域。最广泛使用的面部检测算法之一是基于弱分类器级联的Viola-Jones[[45](#bib.bib45)]。然而，这种方法无法处理大姿势变化和遮挡。Matsugu等人[[46](#bib.bib46)]首次采用基于规则的CNN网络进行面部检测，该算法对平移、缩放和姿势具有鲁棒性。最近，基于深度学习的面部检测器已在流行的开源库中得到应用，例如dlib和OpenCV。
- en: Since spontaneous MEs involve muscle movements of low intensity, even little
    pose variations and movements may heavily affect MER performance. To this end,
    face registration is crucial for MER. It aligns the detected faces onto a reference
    face to handle varying head-pose issues for successful MER. Currently, one of
    the most used facial registration methods is the Active Shape Models (ASM) [[47](#bib.bib47)]
    encoding both geometry and intensity information. Then, the Active Appearance
    Models (AAM) [[48](#bib.bib48)] is presented for matching any face with any expression
    rapidly. With the fast development of DL, deep networks with cascaded regression
    [[49](#bib.bib49)] have become the state-of-the-art methods for face alignment
    due to their excellent performances.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自发的ME涉及低强度的肌肉运动，即使是很小的姿势变化和动作也可能严重影响MER的性能。为此，面部注册对于MER至关重要。它将检测到的面部对齐到参考面部，以处理不同头部姿势的问题，从而实现成功的MER。目前，最常用的面部注册方法之一是主动形状模型（ASM）[[47](#bib.bib47)]，它编码了几何和强度信息。随后，主动外观模型（AAM）[[48](#bib.bib48)]被提出，用于快速匹配任何面部与任何表情。随着深度学习的快速发展，具有级联回归的深度网络[[49](#bib.bib49)]由于其出色的性能，已成为面部对齐的最先进方法。
- en: 4.1.2 Motion magnification
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 运动放大
- en: One challenge for MER is that the facial movements of MEs are too subtle to
    be distinguished. Therefore, motion magnification is important to enhance the
    ME intensity level. One of the commonly used methods is the Eulerian Video Magnification
    method (EVM) [[52](#bib.bib52)]. For MEs, the EVM is applied for facial motion
    magnification [[17](#bib.bib17)]. EVM magnifies either motion or color content
    across two consecutive frames in videos. However, a larger motion amplification
    level leads to a larger scale of motion amplification, which causes bigger artifacts.
    Different from EVM considering local magnification, Global Lagrangian Motion Magnification
    (GLMM) [[130](#bib.bib130)] was proposed for consistently tracking and exaggerating
    the FEs and global displacements across a whole video. Furthermore, the learning-based
    motion magnification [[53](#bib.bib53)] was firstly used in ME magnification by
    Lei et al.  [[89](#bib.bib89)] through extracting shape representations from the
    intermediate layers of networks. Compared with the traditional methods, the shape
    representations from the intermediate layers introduce less noise.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MER（面部表情识别）的一个挑战是，ME（微表情）的面部动作过于微妙，难以区分。因此，运动放大对增强ME的强度水平至关重要。常用的方法之一是欧拉视频放大法（EVM）[[52](#bib.bib52)]。对于ME，EVM被应用于面部动作放大[[17](#bib.bib17)]。EVM在视频的两个连续帧之间放大动作或颜色内容。然而，较大的动作放大级别会导致更大规模的运动放大，从而产生更大的伪影。不同于EVM考虑局部放大，全球拉格朗日运动放大法（GLMM）[[130](#bib.bib130)]被提出用于一致地跟踪和夸大整个视频中的FE（面部表情）和全局位移。此外，基于学习的运动放大[[53](#bib.bib53)]首次由Lei等人[[89](#bib.bib89)]应用于ME放大，通过从网络的中间层提取形状表示。与传统方法相比，中间层的形状表示引入了较少的噪声。
- en: 4.1.3 Temporal Normalization (TN)
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 时间归一化（TN）
- en: 'Besides the low intensity, the short and varied duration also increases the
    difficulty for robust MER. This problem is especially serious when the videos
    are filmed with relatively low frame rate. To solve this issue, the Temporal Interpolation
    Model[[50](#bib.bib50)] (TIM) was introduced to interpolate all ME sequences into
    the same specified length based on path graph between the frames. There are three
    strengths of applying TIM: 1) up-sampling ME clips with too few frames; 2) more
    stable features can be expected with a unified clip length; 3) extending ME clips
    to long sequences and sub-sampling to short clips for data augmentation. Additionally,
    CNN-based temporal interpolation [[51](#bib.bib51)] have been proposed to solve
    complex scenarios in reality.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了低强度，短暂且多变的持续时间也增加了对稳健的MER的难度。当视频以相对低的帧率拍摄时，这个问题尤其严重。为了解决这个问题，引入了基于路径图的时间插值模型[[50](#bib.bib50)]（TIM），将所有ME序列插值为相同指定长度。应用TIM有三个优点：1）提升帧较少的ME片段；2）预期具有统一片段长度的更稳定特征；3）将ME片段扩展为长序列，并对其进行子采样以用于数据增强。此外，还提出了基于CNN的时间插值[[51](#bib.bib51)]来解决现实中的复杂场景。
- en: '![Refer to caption](img/6110fd5e3da0970c6dae548f57d4fc5c.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6110fd5e3da0970c6dae548f57d4fc5c.png)'
- en: 'Figure 4: Examples of regions of interest. (a) Equal block; (b) FACS-based
    RoIs [[55](#bib.bib55)]; (c) RoIs Masked eye and cheek [[93](#bib.bib93)]; (d)
    Eye and mouth [[131](#bib.bib131)]; (e) Difference-based ME datasets [[132](#bib.bib132)];
    (f) landmark-based local regions [[133](#bib.bib133)].'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：感兴趣区域的示例。 (a) 相等的区块; (b) 基于FACS的RoIs [[55](#bib.bib55)];  (c) 眼睛和脸颊 [[93](#bib.bib93)];
    (d) 眼睛和嘴巴 [[131](#bib.bib131)]; (e) 基于差异的ME数据集 [[132](#bib.bib132)];  (f) 基于基准点的局部区域
    [[133](#bib.bib133)]。
- en: 4.1.4 Regions of interest (RoIs)
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 感兴趣区域（RoIs）
- en: FEs are formulated by basic facial movements [[134](#bib.bib134), [6](#bib.bib6)],
    which correspond to specific facial muscles and relate to different facial regions.
    In other words, not all facial regions contribute equally to FER. Especially for
    MEs, the MEs only trigger specific small regions, as MEs involve subtle facial
    movements. Moreover, the empirical experience and quantitative analysis in [[135](#bib.bib135)]
    found that the outliers such as eyeglass have a seriously negative impact on the
    performance of MER. Therefore, it is important to suppress the influence of outliers.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: FEs由基本面部动作组成[[134](#bib.bib134)，[6](#bib.bib6)]，对应特定面部肌肉，并与不同面部区域相关。换句话说，并非所有面部区域对FER的贡献都相同。特别是对于MEs，MEs仅能触发特定的小区域，因为MEs涉及细微的面部动作。此外，[135](#bib.bib135)的经验经验和定量分析发现，例如眼镜等异常值严重影响MER的性能。因此，抑制异常值的影响是很重要的。
- en: 'Some studies alleviate the influence of regions without useful information
    by extracting features on the RoIs [[93](#bib.bib93)]. Several MER approaches
    [[17](#bib.bib17), [136](#bib.bib136)] divided the entire face into several equal
    blocks for better describing local changes (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.3
    Temporal Normalization (TN) ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for
    Micro-expression Recognition: A Survey") (a)). Davison et al.  [[55](#bib.bib55),
    [56](#bib.bib56)] selected RoIs from the face based on the FACS [[6](#bib.bib6)],
    shown in Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.3 Temporal Normalization (TN) ‣ 4.1 Pre-processing
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (b). In
    addition, to eliminate the noise caused by the eye blinking and motion-less regions,
    Le et al.[[93](#bib.bib93)] proposed to mask the eye and cheek regions for each
    image (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.3 Temporal Normalization (TN) ‣ 4.1
    Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A
    Survey") (c)). However, the motion of eyes has a big contribution to MER under
    certain situations, e.g. lid tighten refers to negative emotion. In work [[131](#bib.bib131)],
    Liong et al.  utilized the eyes and mouth regions for MER, as shown in Fig. [4](#S4.F4
    "Figure 4 ‣ 4.1.3 Temporal Normalization (TN) ‣ 4.1 Pre-processing ‣ 4 Inputs
    ‣ Deep Learning for Micro-expression Recognition: A Survey") (d). Besides, Xia et
    al. [[132](#bib.bib132)] found that the regions around the eyes, nose, and mouth
    are mostly active for MEs and can be chosen as RoIs through analyzing difference
    heat maps of ME datasets, as shown in Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.3 Temporal
    Normalization (TN) ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (e). Furthermore, Xie et al. [[58](#bib.bib58)] and Li et
    al. [[133](#bib.bib133)] proposed to extract features on small facial blocks located
    by facial landmarks (see Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.3 Temporal Normalization
    (TN) ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (f)). In this way, the dimension of learning space can be drastically
    reduced and helpful for deep model learning on small ME datasets.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '一些研究通过在感兴趣区域（RoIs）上提取特征来减轻没有有用信息区域的影响[[93](#bib.bib93)]。一些微表情识别（MER）方法[[17](#bib.bib17),
    [136](#bib.bib136)]将整个面部分为若干相等的块，以更好地描述局部变化（见图[4](#S4.F4 "Figure 4 ‣ 4.1.3 Temporal
    Normalization (TN) ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (a)）。Davison 等人 [[55](#bib.bib55), [56](#bib.bib56)] 基于面部动作编码系统（FACS）[[6](#bib.bib6)]
    从面部选择了感兴趣区域，见图[4](#S4.F4 "Figure 4 ‣ 4.1.3 Temporal Normalization (TN) ‣ 4.1 Pre-processing
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (b)。此外，为了消除由眼睛眨动和静止区域造成的噪声，Le
    等人[[93](#bib.bib93)] 提出了对每张图像的眼睛和脸颊区域进行遮罩（见图[4](#S4.F4 "Figure 4 ‣ 4.1.3 Temporal
    Normalization (TN) ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (c)）。然而，在某些情况下，眼睛的运动对MER有很大贡献，例如眼睑紧闭指示负面情绪。在工作[[131](#bib.bib131)]中，Liong
    等人利用眼睛和嘴部区域进行MER，如图[4](#S4.F4 "Figure 4 ‣ 4.1.3 Temporal Normalization (TN) ‣
    4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (d)所示。此外，Xia 等人[[132](#bib.bib132)] 发现眼睛、鼻子和嘴巴周围的区域在微表情中最为活跃，可以通过分析ME数据集的差异热图来选择这些区域作为感兴趣区域，如图[4](#S4.F4
    "Figure 4 ‣ 4.1.3 Temporal Normalization (TN) ‣ 4.1 Pre-processing ‣ 4 Inputs
    ‣ Deep Learning for Micro-expression Recognition: A Survey") (e)所示。此外，Xie 等人[[58](#bib.bib58)]
    和 Li 等人[[133](#bib.bib133)] 提出了在由面部标志点定位的小面部块上提取特征（见图[4](#S4.F4 "Figure 4 ‣ 4.1.3
    Temporal Normalization (TN) ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for
    Micro-expression Recognition: A Survey") (f)）。通过这种方式，可以大幅减少学习空间的维度，有助于在小型ME数据集上进行深度模型学习。'
- en: 4.1.5 Data augmentation
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5 数据增强
- en: 'The main challenge for MER with DL is the small-scale ME datasets. The current
    ME datasets are too limited to train a robust DL model from scratch, therefore
    data augmentation is necessary. The common way for data augmentation is random
    crop and rotation in terms of the spatial domain. Xia et al. augmented MEs through
    magnifying MEs with multiple ratios [[60](#bib.bib60)]. Fig. [5](#S4.F5 "Figure
    5 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for
    Micro-expression Recognition: A Survey") (a) and (b) show the examples of magnified
    ME apex frames with different ratios on the basis of EVM [[52](#bib.bib52)] and
    learning-based magnification [[53](#bib.bib53)], respectively. Additionally, Generative
    Adversarial Network (GAN) [[137](#bib.bib137)] can augment data by producing synthetic
    images. Xie et al. [[58](#bib.bib58)] introduced the AU Intensity Controllable
    GAN (AU-ICGAN) to synthesize subtle MEs. As Fig. [5](#S4.F5 "Figure 5 ‣ 4.1.5
    Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (d) shows, the ME sequences with continuous AU intensity
    can be synthesized through [[58](#bib.bib58)]. Yu et al. [[27](#bib.bib27)] proposed
    a Identity-aware and Capsule-Enhanced Generative Adversarial Network (ICE-GAN)
    to complete the ME synthesis and recognition tasks. ICE-GAN outperformed the winner
    of MEGC2019 by 7$\%$, demonstrating the effectiveness of GAN for ME augmentation
    and recognition. The synthesized images corresponding to different emotions are
    shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (c). Besides,
    Liong et al. [[138](#bib.bib138)] utilized conditional GAN to generate optical-flow
    images to improve the MER accuracy based on computed optical flow. For ME clips,
    sub-sampling MEs from extended ME sequences through TIM can augment ME sequences [[61](#bib.bib61)].'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '对于使用深度学习的微表情识别（MER），主要挑战在于小规模的微表情数据集。当前的微表情数据集过于有限，无法从头开始训练一个强健的深度学习模型，因此需要进行数据增强。数据增强的常见方法是在空间域中进行随机裁剪和旋转。Xia
    等人通过使用多个比例放大微表情来扩增数据[[60](#bib.bib60)]。图[5](#S4.F5 "Figure 5 ‣ 4.1.5 Data augmentation
    ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (a)和(b)分别展示了基于EVM [[52](#bib.bib52)]和学习型放大[[53](#bib.bib53)]的不同比例放大的微表情顶点帧的示例。此外，生成对抗网络（GAN）[[137](#bib.bib137)]可以通过生成合成图像来增强数据。Xie
    等人[[58](#bib.bib58)]引入了AU强度可控GAN（AU-ICGAN）来合成细微的微表情。图[5](#S4.F5 "Figure 5 ‣ 4.1.5
    Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (d)显示了通过[[58](#bib.bib58)]合成的具有连续AU强度的微表情序列。Yu 等人[[27](#bib.bib27)]提出了一种身份感知和胶囊增强的生成对抗网络（ICE-GAN），以完成微表情合成和识别任务。ICE-GAN的表现比MEGC2019的冠军高出7$\%$，展示了GAN在微表情增强和识别中的有效性。图[5](#S4.F5
    "Figure 5 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning
    for Micro-expression Recognition: A Survey") (c)展示了对应于不同情绪的合成图像。此外，Liong 等人[[138](#bib.bib138)]利用条件GAN生成光流图像，以基于计算的光流提高MER准确性。对于微表情剪辑，可以通过TIM从扩展的微表情序列中进行子采样以增强微表情序列[[61](#bib.bib61)]。'
- en: '![Refer to caption](img/3481bdd8aae98979687d886972e517d3.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3481bdd8aae98979687d886972e517d3.png)'
- en: 'Figure 5: Examples of magnified and synthesized MEs.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：放大和合成微表情的示例。
- en: '![Refer to caption](img/6d99e44617d9348160bd863b4c958d81.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6d99e44617d9348160bd863b4c958d81.png)'
- en: 'Figure 6: Examples of various inputs.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：各种输入的示例。
- en: 4.2 Input modality
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 输入模态
- en: 'Since the MEs have low intensity, short duration, and limited data, it is challenging
    to recognize MEs based on DL and the MER performance varies with different inputs.
    In this section, we describe the various ME inputs and summarize their strengths
    and shortcomings, as shown in Table [II](#S4.T2 "TABLE II ‣ 4.2.2 Dynamic image
    sequence ‣ 4.2 Input modality ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey").'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '由于微表情具有低强度、短时长和数据有限等特点，基于深度学习识别微表情具有挑战性，且MER的表现因不同输入而异。在本节中，我们描述了各种微表情输入，并总结了它们的优缺点，如表[II](#S4.T2
    "TABLE II ‣ 4.2.2 Dynamic image sequence ‣ 4.2 Input modality ‣ 4 Inputs ‣ Deep
    Learning for Micro-expression Recognition: A Survey")所示。'
- en: 4.2.1 Static image
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 静态图像
- en: 'For FER, a large volume of existing studies are conducted on static images
    without temporal information due to the availability of the massive facial images
    online and the convenience of data processing. Inspired by efficient FER with
    static images, some researchers [[64](#bib.bib64), [68](#bib.bib68)] explored
    the MER based on the apex frame with the largest intensity of facial movement
    among all frames (See Fig. [6](#S4.F6 "Figure 6 ‣ 4.1.5 Data augmentation ‣ 4.1
    Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A
    Survey") (a)). Li et al. [[59](#bib.bib59)] studied the contribution of the apex
    frame and verified that DL can achieve good MER performance with the single apex
    frame. Furthermore, the research of Sun et al.  [[69](#bib.bib69)] showed that
    the apex frame-based methods can effectively utilize the massive static images
    in macro-expression databases [[69](#bib.bib69)] and obtain better performance
    than onset-apex-offset sequences and the whole videos.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 FER，大量现有研究基于静态图像进行，因为网上有大量面部图像可用，且数据处理方便。受到静态图像 FER 的启发，一些研究者 [[64](#bib.bib64),
    [68](#bib.bib68)] 探索了基于所有帧中面部运动强度最大的顶点帧的 MER（见图 [6](#S4.F6 "Figure 6 ‣ 4.1.5 Data
    augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (a)）。Li 等人 [[59](#bib.bib59)] 研究了顶点帧的贡献，并验证了 DL 可以通过单一顶点帧实现良好的
    MER 性能。此外，Sun 等人 [[69](#bib.bib69)] 的研究表明，基于顶点帧的方法可以有效利用宏表达数据库中的大量静态图像 [[69](#bib.bib69)]，并比起点-顶点-终点序列和整段视频获得更好的性能。'
- en: Apex spotting is one of the key components for building a robust MER system
    based on apex frames. Patel et al. [[62](#bib.bib62)] computed the motion amplitude
    of optical flow shifted over time to locate the onset, apex, and offset frames
    of MEs, while other works [[17](#bib.bib17), [63](#bib.bib63)] exploited feature
    differences to detect MEs in long videos. However, optical flow-based approaches
    required complicated feature operation and the feature contrast-based methods
    ignored ME dynamic information. Different from above methods estimating the facial
    change in the spatio-temporal domain, Li et al. [[64](#bib.bib64)] proposed to
    locate the apex frame in rapid ME clips through exploring the information in the
    frequency domain which clearly describes the rate of change. Furthermore, SMEConvNet
    [[65](#bib.bib65)] firstly adopted CNN for ME spotting and a feature matrix processing
    was proposed for locating the apex frame in long videos. Following SMEConvNet,
    various CNN-based ME spotting methods [[66](#bib.bib66), [67](#bib.bib67)] were
    proposed. In general, the performance of CNN-based spotting method is limited
    because of the small-scale ME datasets and mixed macro- and micro-expressions
    clips in long videos. Further studies on reliable spotting methods are required
    in the future.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 顶点检测是基于顶点帧构建稳健 MER 系统的关键组成部分之一。Patel 等人 [[62](#bib.bib62)] 计算了光流的运动幅度来定位 ME
    的起点、顶点和终点，而其他工作 [[17](#bib.bib17), [63](#bib.bib63)] 则利用特征差异来检测长视频中的 ME。然而，基于光流的方法需要复杂的特征操作，而基于特征对比的方法忽略了
    ME 的动态信息。不同于上述方法在时空域中估计面部变化，Li 等人 [[64](#bib.bib64)] 提出了通过探索频域中的信息来定位快速 ME 片段中的顶点帧，频域信息清晰地描述了变化速率。此外，SMEConvNet
    [[65](#bib.bib65)] 首次采用 CNN 进行 ME 检测，并提出了一种特征矩阵处理方法来定位长视频中的顶点帧。继 SMEConvNet 之后，提出了各种基于
    CNN 的 ME 检测方法 [[66](#bib.bib66), [67](#bib.bib67)]。一般来说，由于 ME 数据集规模小以及长视频中混合的宏观和微观表情片段，基于
    CNN 的检测方法性能有限。未来需要进一步研究可靠的检测方法。
- en: 4.2.2 Dynamic image sequence
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 动态图像序列
- en: As the facial movements are subtle in the spatial domain, while change fast
    in the temporal domain, the temporal dynamics along the video sequences are essential
    in improving the MER performance. In this subsection, we describe the various
    dynamic inputs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于面部动作在空间域中较为细微，而在时间域中变化快速，因此视频序列中的时间动态对于提升 MER 性能至关重要。在本小节中，我们描述了各种动态输入。
- en: 'Sequence. Most ME researches utilize consecutive frames in video clips [[139](#bib.bib139),
    [54](#bib.bib54), [140](#bib.bib140)], as shown in Fig. [6](#S4.F6 "Figure 6 ‣
    4.1.5 Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (b). With the success of 3D CNN [[141](#bib.bib141)] and
    Recurrent Neural Network (RNN) [[142](#bib.bib142)] in video analysis [[143](#bib.bib143),
    [144](#bib.bib144)], MER based on sequence [[107](#bib.bib107), [61](#bib.bib61),
    [70](#bib.bib70), [58](#bib.bib58), [71](#bib.bib71), [72](#bib.bib72), [145](#bib.bib145)]
    is developed that considers the spatial and temporal information simultaneously.
    However, the computation cost is relatively high and the complex model tends to
    overfit the small-scale training data.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '序列。大多数微表情研究利用视频剪辑中的连续帧[[139](#bib.bib139), [54](#bib.bib54), [140](#bib.bib140)]，如图
    [6](#S4.F6 "Figure 6 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs
    ‣ Deep Learning for Micro-expression Recognition: A Survey") (b) 所示。随着3D CNN [[141](#bib.bib141)]
    和递归神经网络 (RNN) [[142](#bib.bib142)] 在视频分析中的成功[[143](#bib.bib143), [144](#bib.bib144)]，基于序列的
    MER [[107](#bib.bib107), [61](#bib.bib61), [70](#bib.bib70), [58](#bib.bib58),
    [71](#bib.bib71), [72](#bib.bib72), [145](#bib.bib145)] 得到了发展，它同时考虑了空间和时间信息。然而，计算成本相对较高，复杂的模型倾向于过拟合小规模的训练数据。'
- en: 'Frame aggregation. MEs are mostly collected with a high-speed camera (e.g. 200
    fps) to capture the rapid subtle changes. Liong et al. discovered that there is
    redundant information in ME clips recorded with high-speed cameras [[146](#bib.bib146)].
    The redundancy could decrease the performance of MER. The experimental results
    of [[146](#bib.bib146)] demonstrate that the onset, apex, and offset frames can
    provide enough spatial and temporal information to ME classification. Liong et
    al. [[73](#bib.bib73)] extracted features on onset and apex frames for MER, as
    shown in  Fig. [6](#S4.F6 "Figure 6 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (c). Furthermore,
    in order to avoid apex frame spotting, Liu et al. [[74](#bib.bib74)] and Kumar et
    al. [[75](#bib.bib75)] designed simple strategies to select aggregated frames
    automatically.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '帧聚合。微表情通常使用高速摄像机（例如200 fps）来捕捉快速的微小变化。Liong 等人发现，使用高速摄像机记录的微表情片段中存在冗余信息 [[146](#bib.bib146)]。冗余信息可能会降低
    MER 的性能。 [[146](#bib.bib146)] 的实验结果表明，起始、顶点和结束帧可以提供足够的空间和时间信息用于微表情分类。Liong 等人
    [[73](#bib.bib73)] 在起始和顶点帧上提取特征用于 MER，如图 [6](#S4.F6 "Figure 6 ‣ 4.1.5 Data augmentation
    ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (c) 所示。此外，为了避免顶点帧检测，Liu 等人 [[74](#bib.bib74)] 和 Kumar 等人 [[75](#bib.bib75)]
    设计了简单的策略来自动选择聚合帧。'
- en: 'Image with dynamic information. Image with dynamic information [[147](#bib.bib147)]
    is a standard image that holds the dynamics of an entire video sequence in a single
    instance. The dynamic image generated by using the rank pooling algorithm has
    been successfully used in MER [[76](#bib.bib76), [93](#bib.bib93), [77](#bib.bib77),
    [148](#bib.bib148)] to summarize the subtle dynamics and appearance in an image.
    Similar to dynamic images, active images [[78](#bib.bib78)] encapsulated the spatial
    and temporal information of a video sequence into a single instance through estimating
    and accumulating the change of each pixel component (See Fig. [6](#S4.F6 "Figure
    6 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for
    Micro-expression Recognition: A Survey") (d)).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '动态信息图像。动态信息图像 [[147](#bib.bib147)] 是一种标准图像，它在一个实例中包含整个视频序列的动态。使用秩池化算法生成的动态图像已经成功应用于
    MER [[76](#bib.bib76), [93](#bib.bib93), [77](#bib.bib77), [148](#bib.bib148)]，以总结图像中的微妙动态和外观。类似于动态图像，主动图像
    [[78](#bib.bib78)] 通过估计和累积每个像素组件的变化，将视频序列的空间和时间信息封装到一个实例中（见图 [6](#S4.F6 "Figure
    6 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for
    Micro-expression Recognition: A Survey") (d)）。'
- en: 'TABLE II: The comparisons of inputs for MER'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: MER 输入的比较'
- en: '| Input modality | Strength | Shortcoming |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 输入模式 | 优势 | 缺点 |'
- en: '| Static | Efficient; Take advantage of massive facial images | Require magnification
    and apex detection Without temporal information |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 静态 | 高效；利用大量面部图像 | 需要放大和顶点检测 缺乏时间信息 |'
- en: '| Dynamic | Sequence | Process directly | Not efficient; Information redundancy
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 动态 | 序列 | 直接处理 | 效率低；信息冗余 |'
- en: '| Frame aggregation | Efficiently leverage key temporal information | Require
    apex detection |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 帧聚合 | 高效地利用关键时间信息 | 需要顶点检测 |'
- en: '| Image with dynamic information | Efficiently embed spatio-temporal information
    | Require dynamic information computation |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 带有动态信息的图像 | 高效嵌入时空信息 | 需要动态信息计算 |'
- en: '| Optical flow | Remove identity to some degree; Movement considered | Optical
    flow computation is necessary |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 光流 | 在一定程度上去除身份特征；考虑运动 | 光流计算是必要的 |'
- en: '| Combination | Explore spatial and temporal information | High computation
    cost |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 组合 | 探索空间和时间信息 | 高计算成本 |'
- en: 'Optical flow. The motion between ME frames contributes important information
    for ME recognition. Optical flow approximates the local image motion, which has
    been verified to be helpful for motion representation [[149](#bib.bib149)]. It
    specifies the magnitude and direction of pixel motion in a given sequence of images
    with a two-dimension vector field (horizontal and vertical optical flows). In
    recent years, several novel methodologies have been presented to improve optical
    flow techniques [[79](#bib.bib79), [150](#bib.bib150), [151](#bib.bib151), [80](#bib.bib80),
    [152](#bib.bib152)], such as Farnebäck’s [[80](#bib.bib80)], Lucas-Kanade [[79](#bib.bib79)],
    TV-L1 [[152](#bib.bib152)], FlowNet [[82](#bib.bib82)], as shown in Fig. [6](#S4.F6
    "Figure 6 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning
    for Micro-expression Recognition: A Survey") (e). Currently, many MER approaches
    utilize optical flow to represent the micro-facial movement and reduce the identity
    characteristic [[60](#bib.bib60), [99](#bib.bib99), [153](#bib.bib153)]. Researches [[60](#bib.bib60),
    [99](#bib.bib99)] indicated that optical flow-based methods always outperform
    appearance-based methods. To further capture the subtle facial changes, multiple
    works [[81](#bib.bib81), [106](#bib.bib106), [91](#bib.bib91)] extracted features
    on computed optical flows on the onset and mid-frame/apex in horizontal and vertical
    directions separately.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '光流。ME 帧之间的运动提供了 ME 识别的重要信息。光流近似局部图像运动，已被验证对运动表示有帮助 [[149](#bib.bib149)]。它通过二维向量场（水平和垂直光流）指定给定图像序列中像素运动的大小和方向。近年来，提出了几种新颖的方法来改进光流技术 [[79](#bib.bib79),
    [150](#bib.bib150), [151](#bib.bib151), [80](#bib.bib80), [152](#bib.bib152)]，例如
    Farnebäck’s [[80](#bib.bib80)], Lucas-Kanade [[79](#bib.bib79)], TV-L1 [[152](#bib.bib152)],
    FlowNet [[82](#bib.bib82)]，如图 [6](#S4.F6 "Figure 6 ‣ 4.1.5 Data augmentation ‣
    4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (e) 所示。目前，许多 MER 方法利用光流来表示微表情运动，并减少身份特征 [[60](#bib.bib60), [99](#bib.bib99),
    [153](#bib.bib153)]。研究 [[60](#bib.bib60), [99](#bib.bib99)] 指出，基于光流的方法总是优于基于外观的方法。为了进一步捕捉细微的面部变化，多个研究 [[81](#bib.bib81),
    [106](#bib.bib106), [91](#bib.bib91)] 在计算的光流上提取了在水平和垂直方向上的起始和中间帧/顶点的特征。'
- en: 4.2.3 Input combination
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 输入组合
- en: 'Considering the strengths of apex frame and dynamic image sequences, some works
    [[99](#bib.bib99), [71](#bib.bib71), [83](#bib.bib83), [84](#bib.bib84)] analyze
    multiple inputs to learn features from different cues in ME videos. Specifically,
    in Liu et al.’s work [[83](#bib.bib83)], the apex frames and optical flow are
    utilized to extract static-spatial and temporal features, respectively. Besides
    the above modalities, Song et al. [[99](#bib.bib99)] added local facial regions
    of the apex frame as inputs to embed the relationship of individual facial regions
    for increasing the robustness of MER. In addition, Sun et al. [[84](#bib.bib84)]
    employed optical flow and sequences for fully exploring the temporal ME information.
    Recently, inspired by the successful application of landmarks in facial analysis
    (See Fig. [6](#S4.F6 "Figure 6 ‣ 4.1.5 Data augmentation ‣ 4.1 Pre-processing
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (f)),
    Kumar et al. [[85](#bib.bib85)] proposed to fuse the landmark graph and optical
    flow to enhance the discriminative ability of ME repression. Currently, the approaches
    with multiple inputs achieve the best MER performance through leveraging as much
    as ME information on limited ME datasets.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑到峰值帧和动态图像序列的优点，一些工作 [[99](#bib.bib99), [71](#bib.bib71), [83](#bib.bib83),
    [84](#bib.bib84)] 分析了多种输入以从ME视频中的不同线索中学习特征。具体而言，在刘等人的工作 [[83](#bib.bib83)] 中，利用峰值帧和光流分别提取静态空间特征和时间特征。除了上述模态，宋等人
    [[99](#bib.bib99)] 将峰值帧的局部面部区域作为输入，以嵌入个体面部区域的关系，从而提高MER的稳健性。此外，孙等人 [[84](#bib.bib84)]
    采用光流和序列来充分探索时间ME信息。最近，受到面部分析中地标成功应用的启发（见图 [6](#S4.F6 "Figure 6 ‣ 4.1.5 Data augmentation
    ‣ 4.1 Pre-processing ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (f)），库马尔等人 [[85](#bib.bib85)] 提出了融合地标图和光流，以增强ME表征的区分能力。目前，利用多种输入的方法通过在有限的ME数据集上利用尽可能多的ME信息，取得了最佳的MER性能。'
- en: 4.3 Discussion
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 讨论
- en: 'In summary, the input is one of the key components to guarantee robust MER.
    The various ME inputs have different strengths and shortcomings. The comparisons
    of inputs are shown in Table [II](#S4.T2 "TABLE II ‣ 4.2.2 Dynamic image sequence
    ‣ 4.2 Input modality ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '总之，输入是保证稳健MER的关键组件之一。各种ME输入具有不同的优点和缺点。输入的比较见表 [II](#S4.T2 "TABLE II ‣ 4.2.2
    Dynamic image sequence ‣ 4.2 Input modality ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey")。'
- en: The input pre-processing is the first step in the MER system. Besides common
    face pre-processing approaches (face detection and registration), motion magnification,
    RoIs, and TIM also play important roles for robust MER, due to the subtle and
    rapid characteristics of MEs. Current motion magnification approaches always introduce
    noises and artifacts. More effective motion magnification approaches should be
    explored. Furthermore, considering the small-scale ME datasets are far from enough
    to train a robust deep model, data augmentation is necessary for MER. In the future,
    studying more robust GAN-based ME generation approaches is a promising research
    direction.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 输入预处理是MER系统的第一步。除了常见的人脸预处理方法（人脸检测和注册），运动放大、感兴趣区域（RoIs）和时间序列信息（TIM）在稳健MER中也发挥重要作用，因为MEs的特征微妙而快速。目前的运动放大方法常常引入噪声和伪影。应探索更有效的运动放大方法。此外，考虑到小规模的ME数据集远远不足以训练一个稳健的深度模型，数据增强对MER来说是必要的。未来，研究更稳健的基于GAN的ME生成方法是一个有前景的研究方向。
- en: Regarding the static input, the apex-based MER can reduce computational complexity
    and take advantage of the massive FEs to resolve the small-dataset issue in some
    degree. But, magnification is necessary since all the temporal information is
    dropped in single apex-based methods and the motion intensity is still low in
    the apex frames. Moreover, as the apex label is absent in some ME datasets, the
    performance of apex-based MER severely relies on the apex detection algorithm.
    Currently, the apex frame detection in long videos is still challenging. The end-to-end
    framework for apex frame detection and MER needs to be further studied.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 关于静态输入，基于峰值的MER可以降低计算复杂度，并利用大量的特征提取器在一定程度上解决小数据集问题。但是，由于所有时间信息在单一的基于峰值的方法中被丢弃，并且峰值帧中的运动强度仍然较低，因此放大是必要的。此外，由于一些ME数据集中缺少峰值标签，因此基于峰值的MER性能严重依赖于峰值检测算法。目前，长视频中的峰值帧检测仍然具有挑战性。需要进一步研究端到端的峰值帧检测和MER框架。
- en: Compared with the static image, the dynamic input is able to leverage spatial
    and temporal information for robust MER. The simplest dynamic input is ME sequence
    which doesn’t require extra operations. However, there is redundancy in ME sequences,
    and the complexity of the deep model is relatively high and tends to overfit on
    small-scale ME datasets. To solve the problem of redundancy, frame aggregation
    cascading multiple key frames is utilized. Besides, the dynamic image improves
    the computation efficiency through embedding the temporal and spatial information
    to a still image. It can simultaneously consider spatial and temporal information
    in one image without challenging apex frame detection. Furthermore, optical flow
    is widely used for MER as the optical flow describes the motions and removes the
    identity in some degree. However, most of the current optical flow-based MER methods
    are based on traditional optical flow, which is not end-to-end. In the future,
    more DL-based optical flow extraction can be further researched.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与静态图像相比，动态输入能够利用空间和时间信息进行稳健的MER。最简单的动态输入是ME序列，它不需要额外的操作。然而，ME序列中存在冗余，并且深度模型的复杂性相对较高，容易在小规模ME数据集上过拟合。为了解决冗余问题，采用了帧聚合级联多个关键帧。此外，动态图像通过将时间和空间信息嵌入静态图像来提高计算效率。它可以在一幅图像中同时考虑空间和时间信息，而不需要挑战顶点帧检测。此外，光流被广泛用于MER，因为光流描述了运动，并在一定程度上去除了身份。然而，目前大多数基于光流的MER方法是基于传统光流的，而不是端到端的。未来，可以进一步研究更多基于DL的光流提取。
- en: In addition, combining various inputs is the inevitable trend to fully explore
    spatial and temporal information and leverage the merits of various inputs. Correspondingly,
    the combined inputs also inherit the shortcomings of the inputs. However, the
    multiple inputs could be complementary in some degree. So far, the method with
    various inputs has achieved the best performance. Considering the success of multiple
    inputs and limited ME samples, more combined modalities, such as optical flow,
    key frames, and landmarks can be promising research directions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，结合各种输入是充分探索空间和时间信息并利用各种输入优点的不可避免趋势。因此，组合输入也继承了这些输入的缺点。然而，多种输入在某种程度上可以互补。目前，具有各种输入的方法已实现了最佳性能。考虑到多种输入的成功和有限的ME样本，更加多样化的模态，如光流、关键帧和地标，可以成为有前途的研究方向。
- en: '![Refer to caption](img/c9a52c0d2d2d1d09ab961a745d48b775.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c9a52c0d2d2d1d09ab961a745d48b775.png)'
- en: 'Figure 7: Special blocks: (a) Residual block [[154](#bib.bib154)] (b) Inception
    module[[155](#bib.bib155)]; (c) RCN [[60](#bib.bib60)]; (d) Spatial attention
    of CBAM [[87](#bib.bib87)] (e) Channel attention of CBAM [[87](#bib.bib87)]; (f)
    Capsule module [[156](#bib.bib156)]; (g) Spatio-temporal attention [[94](#bib.bib94)];
    (h) GCN [[78](#bib.bib78)].'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：特殊模块：(a) 残差块 [[154](#bib.bib154)] (b) Inception模块[[155](#bib.bib155)]; (c)
    RCN [[60](#bib.bib60)]; (d) CBAM的空间注意力 [[87](#bib.bib87)] (e) CBAM的通道注意力 [[87](#bib.bib87)];
    (f) Capsule模块 [[156](#bib.bib156)]; (g) 时空注意力 [[94](#bib.bib94)]; (h) GCN [[78](#bib.bib78)]。
- en: 5 Deep networks for MER
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 深度网络用于MER
- en: Convolutional Neural Networks (CNNs) have shown excellent performances for various
    computer vision tasks, such as action recognition [[157](#bib.bib157)] and FER
    [[24](#bib.bib24)]. In general, for image classification, CNNs employ two dimensional
    convolutional kernels (denoted as 2D CNN) to leverage spatial context across the
    height and width of the images to make predictions. Compared with 2D CNN, CNNs
    with three-dimensional convolutional kernels (denoted as 3D CNN) are verified
    more effective for exploring spatio-temporal information of videos [[158](#bib.bib158)].
    3D CNN can take advantage of spatio-temporal information to improve the performance
    but comes with a computational cost because of the increased number of parameters.
    Moreover, the 3D CNN only can deal with videos with the fixed length due to the
    pre-defined kernels. Recurrent Neural Network (RNN) [[142](#bib.bib142)] was proposed
    to process the time series data with various duration. Furthermore, Long Short-Term
    Memory (LSTM) was developed to settle the vanishing gradient problem that can
    be encountered when training RNNs.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）在各种计算机视觉任务中表现出色，如动作识别[[157](#bib.bib157)]和面部表情识别（FER）[[24](#bib.bib24)]。一般来说，对于图像分类，CNNs使用二维卷积核（表示为2D
    CNN）来利用图像的高度和宽度上的空间上下文进行预测。与2D CNN相比，具有三维卷积核的CNNs（表示为3D CNN）在探索视频的时空信息方面被验证更为有效[[158](#bib.bib158)]。3D
    CNN可以利用时空信息来提高性能，但由于参数数量增加，计算成本也随之上升。此外，由于预定义的卷积核，3D CNN只能处理固定长度的视频。递归神经网络（RNN）[[142](#bib.bib142)]被提出用于处理具有不同时长的时间序列数据。此外，长短期记忆（LSTM）被开发用来解决在训练RNN时可能遇到的梯度消失问题。
- en: Unlike common video-based classification problems, for the recognition of subtle,
    fleeting, and involuntary MEs, various DL approaches have been proposed to boost
    MER performance. In this section, we introduced the approaches in the view of
    special blocks, network architecture, training strategy, and loss.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与常见的视频分类问题不同，为了识别微妙、短暂和非自愿的MEs，提出了各种深度学习方法来提升MER性能。在本节中，我们从特殊模块、网络结构、训练策略和损失的角度介绍这些方法。
- en: 5.1 Network block
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 网络模块
- en: 'In terms of solving the two main ME challenges: overfitting on small-scale
    ME datasets and low intensity of MEs, various effective network blocks have been
    utilized and designed, such as ResNet family with residual modules [[154](#bib.bib154),
    [159](#bib.bib159), [160](#bib.bib160)], and Inception module [[155](#bib.bib155)].
    In this subsection, we introduce the special network blocks utilized for MER improvement.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决两个主要ME挑战方面：小规模ME数据集的过拟合和ME的低强度，已经利用和设计了各种有效的网络模块，如具有残差模块的ResNet系列[[154](#bib.bib154),
    [159](#bib.bib159), [160](#bib.bib160)]和Inception模块[[155](#bib.bib155)]。在本小节中，我们介绍了用于提高MER的特殊网络模块。
- en: 'For the challenge of small-scale datasets, recent researches [[154](#bib.bib154)]
    demonstrate that residual blocks with shortcut connections (shown in Fig. [7](#S4.F7
    "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (a)) achieves easy optimization and reduces the effect of the vanishing
    gradient problem. Multiple MER works [[104](#bib.bib104), [161](#bib.bib161),
    [162](#bib.bib162), [89](#bib.bib89), [163](#bib.bib163)] employed residual blocks
    for robust recognition on small-scale ME datasets. Instead of directly applying
    the shortcut connection, [[164](#bib.bib164)] further designed a convolutionable
    shortcut to learn the important residual information and AffectiveNet [[165](#bib.bib165)]
    introduced an MFL module learning the low- and high-level feature parallelly to
    increase the discriminative capability between the inter and intra-class variations.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '对于小规模数据集的挑战，最近的研究[[154](#bib.bib154)]表明，具有快捷连接的残差块（如图[7](#S4.F7 "Figure 7 ‣
    4.3 Discussion ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A
    Survey") (a)所示）实现了容易优化，并减少了梯度消失问题的影响。多个MER研究[[104](#bib.bib104), [161](#bib.bib161),
    [162](#bib.bib162), [89](#bib.bib89), [163](#bib.bib163)]采用残差块来在小规模ME数据集上进行稳健的识别。除了直接应用快捷连接外，[[164](#bib.bib164)]进一步设计了一个可卷积的快捷连接来学习重要的残差信息，而AffectiveNet[[165](#bib.bib165)]引入了一个MFL模块，平行学习低级和高级特征，以增强类内和类间变异的区分能力。'
- en: 'Since the fully connected layer requires lots of parameters which makes it
    prone to extreme loss explosion and overfitting [[166](#bib.bib166)], the Inception
    module [[167](#bib.bib167)] aggregates different sizes of filters to compute multi-scale
    spatial information and assembles $1\times 1\times 1$ convolutional filters to
    reduce the dimension and parameter, as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3
    Discussion ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey")
    (b). Multiple works [[81](#bib.bib81), [91](#bib.bib91)] utilized the Inception
    module for efficient MER. Inspired by the Inception structure, a Hybrid Feature
    (HyFeat) block [[168](#bib.bib168), [77](#bib.bib77), [78](#bib.bib78)] was proposed
    to preserve the domain knowledge features for expressive regions of MEs and enrich
    features of edge variations through using different scaled convolutional filters.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '由于全连接层需要大量参数，这使得它容易出现极端的损失爆炸和过拟合 [[166](#bib.bib166)]，Inception 模块 [[167](#bib.bib167)]
    聚合了不同大小的滤波器来计算多尺度空间信息，并组装了 $1\times 1\times 1$ 卷积滤波器以降低维度和参数，如图 Fig. [7](#S4.F7
    "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (b) 所示。多个工作 [[81](#bib.bib81), [91](#bib.bib91)] 利用 Inception 模块实现高效的
    MER。受到 Inception 结构的启发，提出了一种混合特征（HyFeat）块 [[168](#bib.bib168), [77](#bib.bib77),
    [78](#bib.bib78)]，旨在保留 MEs 表达区域的领域知识特征，并通过使用不同尺度的卷积滤波器来丰富边缘变化的特征。'
- en: 'Furthermore, considering the fact that CNN with more convolutional layers has
    stronger representation ability, but easy to overfit on small-scale datasets,
    paper [[60](#bib.bib60)] and [[169](#bib.bib169)] introduced Recurrent Convolutional
    Network (RCN) which achieved a shallow architecture though recurrent connections,
    as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs ‣ Deep Learning
    for Micro-expression Recognition: A Survey") (c).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，考虑到具有更多卷积层的 CNN 具有更强的表示能力，但在小规模数据集上容易过拟合，论文 [[60](#bib.bib60)] 和 [[169](#bib.bib169)]
    引入了递归卷积网络（RCN），该网络通过递归连接实现了浅层结构，如图 Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (c) 所示。'
- en: 'On the other hand, MEs perform as the combination of multiple facial movements.
    The latent semantic information among subtle facial changes contributes important
    information for MER performance. Recent researches illustrate that the Graph Convolutional
    Network (GCN) is effective to model these semantic relationships and can be leveraged
    for face analysis tasks, as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (h). Inspired
    by the successful application of GCN in FER, [[89](#bib.bib89), [58](#bib.bib58),
    [90](#bib.bib90), [91](#bib.bib91)] developed the GCN for MER to further improve
    the performance by modeling the relationship between the local facial movements.
    Lei et al. [[89](#bib.bib89), [92](#bib.bib92)] built graphs on the RoIs along
    facial landmarks contributing information to subtle MEs. The TCN residual blocks
    [[170](#bib.bib170), [89](#bib.bib89)] and transformer [[171](#bib.bib171), [92](#bib.bib92)]
    were applied for reasoning the relationships of RoIs. On the other hand, as the
    FE analysis can be benefited from the knowledge of AUs and FACS, the works [[58](#bib.bib58),
    [90](#bib.bib90), [91](#bib.bib91)] built graph on AU-level representations to
    boost the MER performance by inferring the AU relationship.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，MEs 作为多种面部动作的组合进行表现。细微的面部变化中的潜在语义信息为 MER 性能贡献了重要信息。近期研究表明，图卷积网络（GCN）在建模这些语义关系方面是有效的，并且可以用于面部分析任务，如图
    Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (h) 所示。受到 GCN 在 FER 中成功应用的启发，[[89](#bib.bib89), [58](#bib.bib58),
    [90](#bib.bib90), [91](#bib.bib91)] 开发了用于 MER 的 GCN，通过建模局部面部动作之间的关系来进一步提高性能。Lei
    等人 [[89](#bib.bib89), [92](#bib.bib92)] 在面部标志的 RoIs 上构建图，提供了对细微 MEs 的信息。TCN 残差块
    [[170](#bib.bib170), [89](#bib.bib89)] 和 transformer [[171](#bib.bib171), [92](#bib.bib92)]
    被应用于推理 RoIs 的关系。另一方面，由于 FE 分析可以从 AUs 和 FACS 的知识中受益，工作 [[58](#bib.bib58), [90](#bib.bib90),
    [91](#bib.bib91)] 在 AU 级别的表示上构建了图，通过推断 AU 关系来提升 MER 性能。'
- en: 'Besides graph, Capsule Neural Network (CapsNet) [[156](#bib.bib156)] was employed
    to explore part-whole relationships on face to promote MER performance through
    better model hierarchical relationships by routing procedure [[68](#bib.bib68),
    [83](#bib.bib83), [27](#bib.bib27)], as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3
    Discussion ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey")
    (f).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '除了图卷积，胶囊神经网络（CapsNet）[[156](#bib.bib156)] 被用来探索面部的部分-整体关系，通过路由过程[[68](#bib.bib68),
    [83](#bib.bib83), [27](#bib.bib27)] 提升MER性能，如图 [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion
    ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey") (f)所示。'
- en: In addition, since MEs have specific muscular activations on the face, MEs are
    related with local regional changes [[172](#bib.bib172)]. Therefore, it is crucial
    to highlight the representation on RoIs [[8](#bib.bib8), [113](#bib.bib113)].
    Several approaches [[103](#bib.bib103), [173](#bib.bib173), [174](#bib.bib174),
    [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177)] have shown the benefit
    of enhancing spatial encoding with attention module.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于微表情（MEs）在面部具有特定的肌肉激活，微表情与局部区域变化相关[[172](#bib.bib172)]。因此，突出RoIs上的表示至关重要[[8](#bib.bib8),
    [113](#bib.bib113)]。若干方法[[103](#bib.bib103), [173](#bib.bib173), [174](#bib.bib174),
    [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177)] 已经展示了通过注意力模块增强空间编码的好处。
- en: 'Except for spatial information, the temporal change also plays an important
    role for MER. As MEs have rapid changes, the frames have unequal contribution
    to MER. Wang et al. [[94](#bib.bib94)] explored a global spatial and temporal
    attention module (GAM) based on the non-local network [[178](#bib.bib178)] to
    encode wider spatial and temporal information to capture local high-level semantic
    information, as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs
    ‣ Deep Learning for Micro-expression Recognition: A Survey") (g).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '除了空间信息，时间变化在微表情识别（MER）中也发挥着重要作用。由于微表情有快速变化，帧对MER的贡献是不均等的。王等[[94](#bib.bib94)]
    探索了一种基于非本地网络[[178](#bib.bib178)]的全局空间和时间注意力模块（GAM），以编码更广泛的空间和时间信息，捕捉局部高级语义信息，如图
    [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (g)所示。'
- en: 'Moreover, Yao et al. [[179](#bib.bib179)] learned the weights of each feature
    channel adaptively through adding squeezeand-and-excitation blocks. Additionally,
    recent works [[87](#bib.bib87), [88](#bib.bib88), [61](#bib.bib61), [180](#bib.bib180)]
    encoded the spatio-temporal and channel attention simultaneously to further boost
    the representational power of MEs. Specifically, CBAMNet [[87](#bib.bib87)] presented
    a convolutional block attention module (CBAM) cascading the spatial attention
    module (see Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs ‣ Deep Learning
    for Micro-expression Recognition: A Survey") (d)) and channel attention module
    (see Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs ‣ Deep Learning for
    Micro-expression Recognition: A Survey") (e)).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，姚等[[179](#bib.bib179)]通过添加挤压和激励块（squeeze-and-excitation blocks）自适应地学习了每个特征通道的权重。此外，最近的工作[[87](#bib.bib87),
    [88](#bib.bib88), [61](#bib.bib61), [180](#bib.bib180)]同时编码了时空和通道注意力，以进一步提升微表情的表示能力。具体而言，CBAMNet
    [[87](#bib.bib87)] 提出了一个卷积块注意力模块（CBAM），级联了空间注意力模块（见图 [7](#S4.F7 "Figure 7 ‣ 4.3
    Discussion ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition: A Survey")
    (d)）和通道注意力模块（见图 [7](#S4.F7 "Figure 7 ‣ 4.3 Discussion ‣ 4 Inputs ‣ Deep Learning
    for Micro-expression Recognition: A Survey") (e)）。'
- en: '![Refer to caption](img/1392597ca9f53d7a987ce0ec0b3c9373.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1392597ca9f53d7a987ce0ec0b3c9373.png)'
- en: 'Figure 8: (a) GAM based on single stream [[94](#bib.bib94)]; (b) CNN cascaded
    with LSTM [[114](#bib.bib114)]; (c) TSCNN [[99](#bib.bib99)] based on three-stream
    network; (d) A dual-stream multi-task network incorporating gender detection designed
    by GEME [[76](#bib.bib76)]; (e) CNN cascaded with GCNs on the basis of AU-feature
    graph [[58](#bib.bib58)]; (f) MER based on knowledge distillation [[69](#bib.bib69)];
    and (g) The general concept of single-stream, multi-stream, cascaded networks,
    multi-task learning, and the training strategy of transferring knowledge.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '图8: (a) 基于单流的GAM[[94](#bib.bib94)]; (b) 与LSTM级联的CNN[[114](#bib.bib114)]; (c)
    基于三流网络的TSCNN [[99](#bib.bib99)]; (d) GEME设计的包含性别检测的双流多任务网络[[76](#bib.bib76)];
    (e) 基于AU特征图的GCNs与CNN级联[[58](#bib.bib58)]; (f) 基于知识蒸馏的MER[[69](#bib.bib69)]; (g)
    单流、多流、级联网络、多任务学习的通用概念以及知识转移的训练策略。'
- en: In summary, due to the special characteristics of MEs, many DL-based methods
    designed special blocks to extract discriminative ME representations from the
    latent semantic information. Recent MER researches indicate that attention and
    graph blocks are effective to model the semantic relationships. Current GCN-based
    MER are always based on the local facial regions and AU labels. In the future,
    more compact and concise representation, such as landmark location, can be further
    developed for efficient MER. Moreover, the transformer [[171](#bib.bib171)] has
    been verified effectively on modeling the relationship. For future MER research,
    transformers can be further applied to model the relationships between facial
    landmarks, AUs, RoIs and frames to enhance ME representation. On the other hand,
    other special blocks [[60](#bib.bib60)] targeted at learning discriminative ME
    features with less parameters to avoid overfitting. In the future, more efficient
    blocks should be studied to dig subtle ME movements on limited ME datasets.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，由于ME的特殊特性，许多基于DL的方法设计了特殊模块来从潜在语义信息中提取判别性ME表示。近期MER研究表明，注意力和图块对建模语义关系是有效的。目前的GCN-based
    MER通常基于局部面部区域和AU标签。未来可以进一步开发更紧凑和简洁的表示，例如地标位置，以提高MER效率。此外，transformer[[171](#bib.bib171)]在建模关系方面已被有效验证。对于未来的MER研究，transformer可以进一步应用于建模面部地标、AU、RoI和帧之间的关系，以增强ME表示。另一方面，其他特殊模块[[60](#bib.bib60)]旨在学习具有较少参数的判别性ME特征，以避免过拟合。未来应研究更高效的模块，以发掘有限ME数据集中的细微ME运动。
- en: 5.2 Network architecture
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 网络架构
- en: 'Besides designing special blocks for discriminative ME representation, the
    way of combining the blocks is also very important. The current network architecture
    of MER methods can be classified to five categories: single-stream, multi-stream,
    cascaded, multi-task learning and transfer learning. In this section, we will
    discuss the details of the five network architectures.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除了设计用于判别性ME表示的特殊模块外，组合这些模块的方式也非常重要。目前，MER方法的网络架构可以分为五类：单流、多流、级联、多任务学习和迁移学习。本节将详细讨论这五种网络架构。
- en: 5.2.1 Single-stream networks
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 单流网络
- en: Typical deep MER methods adopt single CNN with individual input [[181](#bib.bib181)].
    The apex frame, optical flow images and dynamic images are common inputs for single-stream
    2D CNNs, while single-stream 3D CNNs extract the spatial and temporal features
    from ME sequences directly. Considering the limited ME samples are far from enough
    to train a robust deep network, multiple works designed single-stream shallow
    CNNs for MER [[182](#bib.bib182), [140](#bib.bib140), [183](#bib.bib183)]. Belaiche et
    al. [[161](#bib.bib161)] achieved a shallow network through deleting multiple
    convolutional layers of the deep network Resnet. Zhao et al.  [[44](#bib.bib44)]
    proposed a 6-layer CNN in which the input is followed by an $1\times 1$ convolutional
    layer to increase the non-linear representation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的深层MER方法采用具有独立输入的单一CNN[[181](#bib.bib181)]。顶点帧、光流图像和动态图像是单流2D CNN的常见输入，而单流3D
    CNN直接从ME序列中提取空间和时间特征。考虑到有限的ME样本远不能训练出一个稳健的深层网络，许多工作设计了单流浅层CNN用于MER[[182](#bib.bib182),
    [140](#bib.bib140), [183](#bib.bib183)]。Belaiche等人[[161](#bib.bib161)]通过删除深层网络Resnet的多个卷积层实现了一个浅层网络。Zhao等人[[44](#bib.bib44)]提出了一个6层CNN，其中输入后跟一个$1\times
    1$卷积层以增加非线性表示。
- en: Besides designing shallow networks, many studies [[64](#bib.bib64), [74](#bib.bib74),
    [93](#bib.bib93)] fine-tuned deep networks pre-trained on large face datasets
    to avoid the overfitting problem. Li et al. [[64](#bib.bib64)] firstly adopted
    the 16-layer VGG-FACE model pre-trained on VGG-FACE dataset [[184](#bib.bib184)]
    for MER. Following [[64](#bib.bib64)], the MER with Resnet50, SEnet50 and VGG19
    pre-trained on Imagenet was explored in [[93](#bib.bib93)]. The results illustrate
    that VGG surpasses other architectures regarding the MER topic and is good at
    distinguishing the complex hidden information in data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 除了设计浅层网络外，许多研究[[64](#bib.bib64), [74](#bib.bib74), [93](#bib.bib93)]对在大型面部数据集上预训练的深层网络进行了微调，以避免过拟合问题。Li等人[[64](#bib.bib64)]首次采用了在VGG-FACE数据集[[184](#bib.bib184)]上预训练的16层VGG-FACE模型用于MER。根据[[64](#bib.bib64)]，MER还探索了基于Imagenet预训练的Resnet50、SEnet50和VGG19[[93](#bib.bib93)]。结果表明，在MER主题上，VGG超越了其他架构，擅长区分数据中的复杂隐藏信息。
- en: All of above works are based on 2D CNN with image input, while several works
    employed single 3D CNN to directly extract the spatial and temporal features from
    ME sequences. GAM [[94](#bib.bib94)], MERANet [[88](#bib.bib88)] and CBAMNet [[87](#bib.bib87)]
    combined attention modules to 3D CNN to enhance the representation in spatial
    and temporal dimensions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有工作均基于2D CNN图像输入，而一些工作则采用了单一3D CNN直接从ME序列中提取空间和时间特征。GAM [[94](#bib.bib94)]、MERANet
    [[88](#bib.bib88)]和CBAMNet [[87](#bib.bib87)]将注意力模块与3D CNN相结合，以增强空间和时间维度的表示。
- en: 5.2.2 Multi-stream network
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 多流网络
- en: 'Single stream is a basic model structure and only extracts features from the
    single view of MEs. However, MEs have subtle movements and limited samples, the
    single view is not able to provide sufficient information. As we discussed in
    Section [4.2](#S4.SS2 "4.2 Input modality ‣ 4 Inputs ‣ Deep Learning for Micro-expression
    Recognition: A Survey"), the various inputs from different views is able to effectively
    explore spatial and temporal information. Thus, the multi-stream network is adopted
    in MER to learn features from multiple inputs. The multi-stream structure allows
    the network extracting multi-view features through multi-path networks, as shown
    in Fig. [8](#S5.F8 "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep
    Learning for Micro-expression Recognition: A Survey") (g). In general, multi-stream
    networks can be classified to networks with the same blocks, different blocks
    and handcrafted features.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '单流是基本的模型结构，仅从单一视角提取特征。然而，由于MEs具有微妙的运动和有限的样本，单一视角无法提供足够的信息。正如我们在第[4.2](#S4.SS2
    "4.2 Input modality ‣ 4 Inputs ‣ Deep Learning for Micro-expression Recognition:
    A Survey")节中讨论的那样，来自不同视角的各种输入能够有效地探索空间和时间信息。因此，多流网络被应用于MER中，以从多个输入中学习特征。多流结构允许网络通过多路径网络提取多视角特征，如图[8](#S5.F8
    "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (g)所示。总体而言，多流网络可以分为具有相同块、不同块和手工特征的网络。'
- en: '*Multi-stream networks with the same blocks.* The Optical Flow Features from
    Apex frame Network (OFF-ApexNet) [[95](#bib.bib95)] and Dual-stream shallow network
    (DSSN) [[96](#bib.bib96)] built the dual-stream CNN for MER based on optical flow
    extracted from onset and apex. Furthermore, Liong et al. [[138](#bib.bib138)]
    extended OFF-ApexNet to multiple streams with various optical flow components
    as input data. The multi-stream CNN with optical flow [[86](#bib.bib86)] and Three-Stream
    CNN (TSCNN) [[98](#bib.bib98), [99](#bib.bib99)] designed three-stream CNN models
    for MER with three kinds of inputs (See Fig. [8](#S5.F8 "Figure 8 ‣ 5.1 Network
    block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (c)). Specifically, the former one utilized apex frame, optical flow
    and the apex frame masked by the optical flow threshold, while the latter approach
    employed the apex frames, optical flow between onset, apex, and offset frames
    to investigate the information of the static spatial, dynamic temporal and local
    information. In addition, She et al. [[102](#bib.bib102)] proposed a four-stream
    model considering three RoIs and global regions as each stream to explore the
    local and global information. Besides multi-stream 2D CNNs, 3DFCNN [[100](#bib.bib100)],
    SETFNet [[179](#bib.bib179)] and [[97](#bib.bib97)] applied 3D flow-based CNNs
    for video-based MER consisting of multiple sub-streams to extract features from
    frame sequences and optical flow, or RoIs.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*相同块的多流网络。* Apex框架网络（OFF-ApexNet）[[95](#bib.bib95)]和双流浅层网络（DSSN）[[96](#bib.bib96)]基于从开始和顶点提取的光流构建了用于MER的双流CNN。此外，Liong等人[[138](#bib.bib138)]将OFF-ApexNet扩展到多个流，使用各种光流组件作为输入数据。具有光流的多流CNN
    [[86](#bib.bib86)]和三流CNN (TSCNN) [[98](#bib.bib98), [99](#bib.bib99)] 设计了用于MER的三流CNN模型，输入包括三种类型的数据（见图[8](#S5.F8
    "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (c)）。具体而言，第一个方法利用了顶点帧、光流和由光流阈值遮罩的顶点帧，而后者的方法则采用了顶点帧、开始、顶点和结束帧之间的光流来探究静态空间、动态时间和局部信息。此外，She等人[[102](#bib.bib102)]提出了一种四流模型，考虑了三个RoI和全局区域作为每个流，以探索局部和全局信息。除了多流2D
    CNN之外，3DFCNN [[100](#bib.bib100)]、SETFNet [[179](#bib.bib179)]和[[97](#bib.bib97)]应用了基于3D流的CNN，用于视频基础的MER，包括多个子流，用于从帧序列和光流或RoI中提取特征。'
- en: '*Multi-stream networks with different blocks.* For enhancing the ME feature
    representation, some works [[106](#bib.bib106), [165](#bib.bib165), [102](#bib.bib102),
    [103](#bib.bib103), [107](#bib.bib107)] investigated the combination of different
    convolutions. Liong et al.  designed a Shallow Triple Stream Three-dimensional
    CNN (STSTNet) [[106](#bib.bib106)] adopting multiple 2D CNN with different kernels.
    Instead of utilizing different kernels, AffectiveNet [[165](#bib.bib165)] constructed
    a four-path network with four different receptive fields (RF) to obtain multi-scale
    features for better describing subtle MEs. On the other hand, Landmark Relations
    with Graph Attention Convolutional Network (LR-GACNN) [[75](#bib.bib75)] and MER-GCN
    [[90](#bib.bib90)] built two-stream graph networks to explore relationships between
    landmark points and the local patches, and AUs and sequence, respectively. Furthermore,
    [[103](#bib.bib103)] and [[107](#bib.bib107)] integrated 2D CNN and 3D CNN to
    extract spatio-temporal information.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*具有不同模块的多流网络。* 为了增强ME特征表示，一些研究[[106](#bib.bib106), [165](#bib.bib165), [102](#bib.bib102),
    [103](#bib.bib103), [107](#bib.bib107)]探讨了不同卷积的组合。Liong等人设计了一种浅层三流三维CNN（STSTNet）[[106](#bib.bib106)]，采用多个具有不同卷积核的2D
    CNN。AffectiveNet [[165](#bib.bib165)]则构建了一个具有四种不同感受野（RF）的四路径网络，以获得多尺度特征，从而更好地描述细微的MEs。另一方面，Landmark
    Relations与图注意力卷积网络（LR-GACNN）[[75](#bib.bib75)]和MER-GCN [[90](#bib.bib90)]构建了两个流图网络，以探索地标点和局部补丁之间的关系，以及AUs和序列之间的关系。此外，[[103](#bib.bib103)]和[[107](#bib.bib107)]集成了2D
    CNN和3D CNN，以提取时空信息。'
- en: '*Multi-stream networks with handcrafted features.* Since the subtle facial
    movements of MEs are highly related to face textures, the handcrafted features
    for low-level representation also plays an important role in MER. Multiple works
    [[108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110)] combined deep features
    and handcrafted features to leverage the low-level and high-level information
    for robust MER. Specifically, in the works [[108](#bib.bib108)] and [[110](#bib.bib110)],
    the CNN features on apex frame and LBP-TOP were concatenated to represent MEs.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*具有手工特征的多流网络。* 由于MEs的细微面部运动与面部纹理高度相关，因此低级表示的手工特征在MER中也发挥了重要作用。多个研究[[108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110)]将深度特征和手工特征结合起来，以利用低级和高级信息来实现稳健的MER。具体而言，在研究[[108](#bib.bib108)]和[[110](#bib.bib110)]中，顶点帧上的CNN特征和LBP-TOP被串联起来以表示MEs。'
- en: 5.2.3 Cascaded network
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 级联网络
- en: 'Cascaded network combines various modules for different tasks sequentially
    to construct an effective network, as shown in Fig. [8](#S5.F8 "Figure 8 ‣ 5.1
    Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression Recognition:
    A Survey") (g). Recent FE studies [[24](#bib.bib24)] demonstrate that learning
    a hierarchy of features gradually filters out the information unrelated to expressions.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '级联网络将各种模块按顺序结合以构建有效的网络，如图[8](#S5.F8 "Figure 8 ‣ 5.1 Network block ‣ 5 Deep
    networks for MER ‣ Deep Learning for Micro-expression Recognition: A Survey")
    (g)所示。最近的FE研究[[24](#bib.bib24)]表明，学习特征的层次结构可以逐步过滤掉与表情无关的信息。'
- en: 'Inspired by the FE studies [[24](#bib.bib24)], for further exploring the temporal
    information of MEs, Nistor et al. [[111](#bib.bib111)] cascaded CNN and RNN to
    extract features from individual frames of the sequence and capture the facial
    evolution during the sequence, respectively. Furthermore, Bai et al. [[114](#bib.bib114)]
    and Zhi et al. [[116](#bib.bib116)] combined CNN with LSTMs in series to deal
    with ME samples with various duration directly, as shown in Fig. [8](#S5.F8 "Figure
    8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (b). Besides, in order to explore the AU semantics in
    MEs, Xie et al. proposed an AU-assisted Graph Attention Convolutional Network
    (AU-GACN) [[58](#bib.bib58)] cascading 3D CNN and GCN to infer MEs based on AU
    features (see Fig. [8](#S5.F8 "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks
    for MER ‣ Deep Learning for Micro-expression Recognition: A Survey") (f)).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '受到FE研究的启发[[24](#bib.bib24)]，为了进一步探索MEs的时间信息，Nistor等人[[111](#bib.bib111)]将CNN和RNN级联，用于从序列的各个帧中提取特征，并分别捕捉序列中的面部演变。此外，Bai等人[[114](#bib.bib114)]和Zhi等人[[116](#bib.bib116)]将CNN与LSTMs串联，以直接处理不同持续时间的ME样本，如图[8](#S5.F8
    "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (b)所示。此外，为了探索MEs中的AU语义，Xie等人提出了一种AU辅助的图注意力卷积网络（AU-GACN）[[58](#bib.bib58)]，将3D
    CNN与GCN级联，以基于AU特征推断MEs（见图[8](#S5.F8 "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks
    for MER ‣ Deep Learning for Micro-expression Recognition: A Survey") (f)）。'
- en: In addition, multiple MER works combined multi-stream and cascaded structure
    to further explore the multi-view series information. VGGFace2+LSTM [[114](#bib.bib114)],
    Temporal Facial Micro-Variation Network (TFMVN) [[113](#bib.bib113)] and MER with
    Ternary Attentions (MERTA) [[101](#bib.bib101)] developed three stream VGGNets
    followed by LSTMs to extract multi-view spatio-temporal features. Different from
    above works, Khor et al.  [[112](#bib.bib112)] proposed an Enriched Long-term
    Recurrent Convolutional Network (ELRCN) adding one VGG+LSTM path with channel-wise
    stacking inputs for spatial enrichment. Besides, AT-Net [[104](#bib.bib104)] and
    SHCFNet [[105](#bib.bib105)] extracted spatial and temporal features by CNN and
    LSTM from the apex frame and optical-flow in parallel and concatenated them together
    to represent MEs.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，多个 MER 工作结合了多流和级联结构，以进一步探索多视角系列信息。VGGFace2+LSTM [[114](#bib.bib114)]、时间面部微变网络（TFMVN）
    [[113](#bib.bib113)] 和带三元注意力的 MER（MERTA） [[101](#bib.bib101)] 发展了三个流的 VGGNets，随后使用
    LSTM 提取多视角时空特征。与上述工作不同，Khor 等人 [[112](#bib.bib112)] 提出了一个丰富的长时记忆卷积网络（ELRCN），增加了一个
    VGG+LSTM 路径，并使用通道级堆叠输入来进行空间丰富。此外，AT-Net [[104](#bib.bib104)] 和 SHCFNet [[105](#bib.bib105)]
    从顶点帧和光流中并行提取空间和时间特征，并将其串联在一起，以表示 MEs。
- en: 5.2.4 Multi-task network
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 多任务网络
- en: 'Most existing works for MER focus on learning features that are sensitive to
    expressions. However, MEs in the real world are intertwined with various factors,
    such as subject identity and AUs. The approaches aiming at a single MER task are
    incapable of making full use of the information on face. To address this issue,
    several multi-task learning-based MER approaches have been subsequently developed
    for better MER [[117](#bib.bib117), [76](#bib.bib76)]. Firstly, Li et al. [[117](#bib.bib117)]
    developed a multi-task network combining facial landmarks detection and optical
    flow extraction to refine the optical flow features for MER with SVM. Following
    [[117](#bib.bib117)], several end-to-end deep multi-task networks leveraging different
    side tasks were proposed. GEnder-based ME recognition (GEME) [[76](#bib.bib76)]
    designed a dual-stream multi-task network incorporating gender detection task
    with MER (see Fig [8](#S5.F8 "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks for
    MER ‣ Deep Learning for Micro-expression Recognition: A Survey") (d)), while feature
    refinement [[185](#bib.bib185)] and MER-auGCN [[91](#bib.bib91)] simultaneously
    detected AUs and MEs and further aggregated AU representation into ME representation.
    On the other hand, considering that a common feature representation can be learned
    from multiple tasks, Hu et al. [[109](#bib.bib109)] formulated MER as a multi-task
    classification problem in which each category classification can be regarded as
    one-against-all pairwise classification problem.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '目前大多数 MER 工作集中于学习对表情敏感的特征。然而，现实世界中的 MEs 受到各种因素的交织，例如主体身份和 AUs。旨在解决单一 MER 任务的方法无法充分利用面部信息。为了解决这个问题，随后开发了几种基于多任务学习的
    MER 方法，以实现更好的 MER [[117](#bib.bib117), [76](#bib.bib76)]。首先，Li 等人 [[117](#bib.bib117)]
    开发了一个多任务网络，结合了面部关键点检测和光流提取，以通过 SVM 提炼光流特征用于 MER。随后 [[117](#bib.bib117)]，提出了几个端到端的深度多任务网络，利用不同的附加任务。基于性别的
    ME 识别（GEME） [[76](#bib.bib76)] 设计了一个双流多任务网络，将性别检测任务与 MER 结合起来（见图 [8](#S5.F8 "Figure
    8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (d)），同时特征精炼 [[185](#bib.bib185)] 和 MER-auGCN [[91](#bib.bib91)]
    同时检测 AUs 和 MEs，并进一步将 AU 表示聚合到 ME 表示中。另一方面，考虑到可以从多个任务中学习到共同的特征表示，Hu 等人 [[109](#bib.bib109)]
    将 MER 公式化为一个多任务分类问题，其中每个类别分类可以视为一对一的对抗分类问题。'
- en: 'In summary, the network architecture can be roughly divided into single-stream,
    multi-stream, cascaded networks, and multi-task learning, as shown in Fig. [8](#S5.F8
    "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (g). Single stream is the simple basic model architecture.
    However, single-stream networks only consider the single view of MEs. To further
    leverage the ME information, the multi-stream network is proposed to learn features
    from multiple views for robust MER. Moreover, since learning a hierarchy of features
    can gradually filter out the information unrelated to expressions, the network
    cascades various modules, such as LSTMs and GCNs, sequentially to construct an
    effective MER network. In the future, more effective modules should be combined
    in multi-stream and cascaded ways to further boost the MER performance.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，网络架构大致可以分为单流、多流、级联网络和多任务学习，如图[8](#S5.F8 "Figure 8 ‣ 5.1 Network block
    ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression Recognition: A
    Survey") (g)所示。单流是最简单的基本模型架构。然而，单流网络只考虑ME的单一视角。为了进一步利用ME信息，提出了多流网络，从多个视角学习特征，以实现稳健的MER。此外，由于学习特征层次结构可以逐渐过滤掉与表情无关的信息，因此网络级联了各种模块，如LSTM和GCN，按顺序构建一个有效的MER网络。在未来，应该以多流和级联方式结合更多有效的模块，以进一步提升MER性能。'
- en: In terms of the tasks, multiple task learning [[186](#bib.bib186)] can share
    knowledge among tasks, introducing extra information and a low risk of overfitting
    in each task. Currently, most ME research only studied the contribution of landmarks
    detection, gender classification, and AU detection. Other tasks, such as face
    recognition and eye gaze tracking may also introduce useful knowledge for MER.
    Exploring and taking advantage of more face related-tasks is a practical way to
    further improve MER performance.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在任务方面，多任务学习[[186](#bib.bib186)]可以在任务之间共享知识，引入额外的信息并降低每个任务的过拟合风险。目前，大多数ME研究仅研究了地标检测、性别分类和AU检测的贡献。其他任务，如面部识别和眼动追踪，也可能为MER引入有用的知识。探索并利用更多面部相关任务是进一步提升MER性能的实用方法。
- en: 5.3 Training strategy
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 训练策略
- en: 'As we discussed before, DL-based MER suffers from a lack of adequate data.
    It is almost impossible to train a reliable deep model from scratch. Currently,
    there are large-scale FE datasets with labels. Leveraging the FE datasets by special
    training strategy, such as fine-tuning [[99](#bib.bib99)], knowledge distillation [[69](#bib.bib69)],
    and domain adaptation [[122](#bib.bib122)], is a reasonable way to solve the problem
    of a small amount of data. The knowledge of a pre-trained model for a related
    task can be transferred to MER to boost performance. The training strategy of
    transferring knowledge is shown in Fig [8](#S5.F8 "Figure 8 ‣ 5.1 Network block
    ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression Recognition: A
    Survey") (g).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们之前讨论的那样，基于深度学习（DL）的MER面临着数据不足的问题。几乎不可能从头开始训练一个可靠的深度模型。目前，已经有带标签的大规模FE数据集。通过特殊的训练策略利用FE数据集，例如微调[[99](#bib.bib99)]、知识蒸馏[[69](#bib.bib69)]和领域适应[[122](#bib.bib122)]，是解决数据量少问题的合理方法。可以将预训练模型在相关任务上的知识迁移到MER中，以提升性能。知识迁移的训练策略如图[8](#S5.F8
    "Figure 8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (g)所示。'
- en: Fine-tuning ME datasets on pre-trained models is widely used in MER [[99](#bib.bib99),
    [59](#bib.bib59), [87](#bib.bib87), [74](#bib.bib74)]. Patel et al. [[187](#bib.bib187)]
    provided two models pre-trained on ImageNet dataset and FE datasets, respectively.
    The feature selection method was also adopted to improve the model’s performance.
    It was found that features captured from the FE datasets performed better in terms
    of accuracy, as it is more similar to the ME datasets than object/face datasets.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练模型上对微表情（ME）数据集进行微调是一种广泛使用的MER方法[[99](#bib.bib99), [59](#bib.bib59), [87](#bib.bib87),
    [74](#bib.bib74)]。Patel 等人[[187](#bib.bib187)]提供了两个分别在ImageNet数据集和FE数据集上预训练的模型。特征选择方法也被采用以提高模型的性能。研究发现，来自FE数据集的特征在准确性方面表现更好，因为它比对象/面部数据集与ME数据集更相似。
- en: 'Besides fine-tuning, another effective transfer learning strategy is knowledge
    distillation. Knowledge distillation achieves small and fast networks through
    leveraging information from pre-trained high-capacity networks. Sun et al. [[69](#bib.bib69)]
    utilized Fitnets [[188](#bib.bib188)] to guide the shallow network learning for
    MER by mimicking the intermediate features of the deep network pre-trained for
    macro-expression recognition and AU detection, as shown in Fig [8](#S5.F8 "Figure
    8 ‣ 5.1 Network block ‣ 5 Deep networks for MER ‣ Deep Learning for Micro-expression
    Recognition: A Survey") (e). However, the appearances of MEs and macro-expressions
    are different due to the different intensity of facial movements. Thus, mimicking
    the macro-expression representation directly is not reasonable. Instead, SAAT
    [[121](#bib.bib121)] transferred attention on the style aggregated MEs generated
    by CycleGAN [[189](#bib.bib189)].'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除了微调，另一种有效的迁移学习策略是知识蒸馏。知识蒸馏通过利用预训练的高容量网络的信息来实现小而快速的网络。Sun等人[[69](#bib.bib69)]利用Fitnets[[188](#bib.bib188)]指导浅层网络学习MER，方法是模仿为宏观表情识别和AU检测预训练的深层网络的中间特征，如图[8](#S5.F8
    "图 8 ‣ 5.1 网络块 ‣ 5 深度网络用于MER ‣ 微表情识别的深度学习：综述") (e)所示。然而，由于面部运动的强度不同，ME和宏观表情的表现不同。因此，直接模仿宏观表情表示是不合理的。相反，SAAT[[121](#bib.bib121)]将注意力转移到由CycleGAN[[189](#bib.bib189)]生成的样式聚合ME上。
- en: In addition, domain adaptation methods can obtain domain invariant representations
    by embedding domain adaptation in the pipeline of deep learning. In [[120](#bib.bib120)],
    [[122](#bib.bib122)], and [[190](#bib.bib190)], the gap between the MEs and macro-expressions
    was narrowed down by domain adaption based on adversarial learning strategy.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，领域适应方法可以通过将领域适应嵌入深度学习的管道中来获得领域不变的表示。在[[120](#bib.bib120)]、[[122](#bib.bib122)]和[[190](#bib.bib190)]中，通过基于对抗学习策略的领域适应缩小了ME和宏观表情之间的差距。
- en: In general, fine-tuning is most widely used in MER. To further effectively transfer
    meaningful information from massive FEs, knowledge distillation and domain adaptation
    are also applied to MER by distilling knowledge and extracting domain invariant
    representations, respectively. However, the domain adaptation with adversarial
    learning increases the learning complexity. There are significant differences
    both spatially and temporally between macro-expressions and MEs, therefore, directly
    transferring the knowledge is not able to fully leverage the macro-expression
    information. Considering that the facial muscle movements are consistent between
    MEs and macro-expressions, the attention and AUs can be further studied for transfer
    learning in future ME research. Moreover, semi-supervised and unsupervised learning
    could also be further developed to take advantage of unlabeled facial images.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，微调在MER中使用最广泛。为了更有效地从大量FEs中转移有意义的信息，还通过知识蒸馏和领域适应分别将知识蒸馏和提取领域不变表示应用于MER。然而，带有对抗学习的领域适应增加了学习复杂性。宏观表情和ME之间在空间和时间上存在显著差异，因此，直接转移知识无法充分利用宏观表情信息。考虑到ME和宏观表情之间面部肌肉运动的一致性，未来ME研究中可以进一步研究注意力和AUs以进行迁移学习。此外，半监督和无监督学习也可以进一步发展，以利用未标记的面部图像。
- en: 5.4 Loss functions
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 损失函数
- en: Different from traditional methods, where the feature extraction and classification
    are independent, deep networks can perform end-to-end classification through loss
    functions by penalizing the deviation between the predicted and true labels during
    training. Most MER works directly applied the commonly used softmax cross-entropy
    loss [[123](#bib.bib123)]. The softmax loss is typically good at correctly classifying
    known categories. However, in practical classification tasks, the unknown samples
    need to be classified. Therefore, in order to obtain better-generalized ability,
    the inter-class difference and intra-class variation should be further optimized
    and reduced, respectively, especially for subtle and limited MEs. The metric learning
    techniques, such as contrastive loss [[124](#bib.bib124)] and triplet loss [[125](#bib.bib125)],
    was developed to ensure intra-class compactness and inter-class separability through
    measuring the relative distances between inputs. Xia et al. [[122](#bib.bib122)]
    adopted an adversarial learning approach and triplet loss with inequality regularization
    to converge the output of MicroNet efficiently. However, metric learning loss
    usually requires effective sample mining strategies for robust recognition performance.
    Metric learning alone is not enough for learning a discriminative metric space
    for MEs. Intensive experiments demonstrate that importing a large margin on softmax
    loss can increase the inter-class difference. Lalitha et al. [[191](#bib.bib191)]
    and Li et al. [[59](#bib.bib59)] combined softmax cross-entropy loss and center
    loss [[192](#bib.bib192)] to increase the compactness of intra-class variations
    and separable inter-class differences through penalizing the distance between
    deep features and their corresponding class centers.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统方法不同，传统方法中特征提取和分类是独立的，而深度网络通过损失函数可以进行端到端的分类，通过在训练过程中惩罚预测标签与真实标签之间的偏差来实现。大多数MER工作直接应用了常用的softmax交叉熵损失[[123](#bib.bib123)]。softmax损失通常擅长正确分类已知类别。然而，在实际分类任务中，需要对未知样本进行分类。因此，为了获得更好的泛化能力，需要进一步优化和减少类间差异和类内变化，特别是对于细微和有限的MEs。度量学习技术，如对比损失[[124](#bib.bib124)]和三元组损失[[125](#bib.bib125)]，通过测量输入之间的相对距离来确保类内紧凑性和类间可分性。Xia等人[[122](#bib.bib122)]采用了对抗学习方法和带不等式正则化的三元组损失，以高效地收敛MicroNet的输出。然而，度量学习损失通常需要有效的样本挖掘策略以获得稳健的识别性能。仅靠度量学习不足以为MEs学习一个具有区分性的度量空间。大量实验表明，在softmax损失上引入大边距可以增加类间差异。Lalitha等人[[191](#bib.bib191)]和Li等人[[59](#bib.bib59)]结合了softmax交叉熵损失和中心损失[[192](#bib.bib192)]，通过惩罚深度特征与其对应类别中心之间的距离，以增加类内变化的紧凑性和可分离的类间差异。
- en: Some special MEs are difficult to trigger, thus leading to data imbalance. Multiple
    MER works [[163](#bib.bib163), [76](#bib.bib76), [60](#bib.bib60), [91](#bib.bib91)]
    utilized the Focal loss to overcome the imbalance challenge by adding a factor
    to put more focus on misclassified and hard samples which are difficult to recognize.
    Moreover, MER-auGCN [[91](#bib.bib91)] designed an adaptive factor with the Focal
    loss to balance the proportion of the negative and positive samples in a given
    training batch.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一些特殊的MEs很难触发，从而导致数据不平衡。多个MER工作[[163](#bib.bib163), [76](#bib.bib76), [60](#bib.bib60),
    [91](#bib.bib91)]利用Focal损失通过添加一个因子来更多关注误分类和难以识别的样本，从而克服不平衡挑战。此外，MER-auGCN[[91](#bib.bib91)]设计了一个带有Focal损失的自适应因子，以平衡给定训练批次中负样本和正样本的比例。
- en: In summary, MER suffers from high intra-class variation, low inter-class differences,
    and imbalanced distribution because of the low intensity and spontaneous characteristics
    of MEs. Currently, most MER approaches are based on the basic softmax cross-entropy
    loss, but others utilized the triplet loss, center loss, or focal loss to encourage
    inter-class separability, intra-class compactness, and balanced learning. In the
    future, exploring more effective loss functions to learn discriminative representation
    for MEs can be a promising research direction. Considering the low intensity of
    facial movements leading to low inter-class differences, better metric space and
    larger margin loss for MER should be further studied. Recently, various methods
    [[193](#bib.bib193)] have been proposed for the classification of imbalanced long-tail
    distribution data. ME research can leverage the ideas for long-tail data to improve
    the MER performance.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，由于 ME 的低强度和自发特性，MER 面临着高类内变异、低类间差异和不平衡分布的问题。目前，大多数 MER 方法基于基本的 softmax
    交叉熵损失，但也有一些方法使用了三元组损失、中心损失或焦点损失，以促进类间分离、类内紧凑性和平衡学习。未来，探索更有效的损失函数以学习 ME 的判别表示可能是一个有前途的研究方向。考虑到面部动作的低强度导致低类间差异，MER
    应进一步研究更好的度量空间和更大的边际损失。最近，已经提出了各种方法 [[193](#bib.bib193)] 用于不平衡长尾分布数据的分类。ME 研究可以借鉴长尾数据的思想，以提升
    MER 的性能。
- en: 5.5 Discussion
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 讨论
- en: MEs are involuntary, subtle, and brief facial movements. How to extract high-level
    discriminative representations on limited subtle ME samples is the main challenge
    for robust MER with DL. In order to extract discriminative ME representation,
    various blocks have been designed based on exploring the special characteristics
    of MEs with less parameters, such as the attention module and capsule module.
    In the future, more effective blocks, such as attention, GCN and transformer,
    should be further developed for MER performance improvement. On the other hand,
    considering the limited ME samples, more efficient blocks should be studied to
    learn discriminative ME features with less parameters for avoiding overfitting
    on small-scale ME datasets.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: MEs 是无意识的、微妙的且短暂的面部动作。如何在有限的微妙 ME 样本上提取高层次的判别表示是使用深度学习进行鲁棒 MER 的主要挑战。为了提取具有判别性的
    ME 表示，已经设计了各种模块，这些模块基于对 ME 特殊特征的探索，参数较少，如注意力模块和胶囊模块。未来，应该进一步开发更有效的模块，如注意力机制、GCN
    和变换器，以提升 MER 的性能。另一方面，考虑到 ME 样本的有限性，应研究更高效的模块，以便在较小规模的 ME 数据集上学习具有判别性的 ME 特征，从而避免过拟合。
- en: In terms of the network architecture, compared with basic single stream networks,
    multi-stream networks can extract features from multi-view inputs to provide more
    information for MER. On the other hand, the cascaded network combines various
    modules for different tasks sequentially to construct an effective network and
    gradually filter out the information unrelated to MEs. Considering the strengths
    of multi-stream and cascaded networks, multi-stream cascaded networks have been
    developed to boost the MER performance further. In the future, exploring multi-stream
    cascaded networks combined with various efficient blocks is a promising research
    direction for MER. In addition, the multi-task learning framework achieves robust
    MER through leveraging information from related tasks. Multi-task learning is
    able to make use of more available information on the face. Current MER explored
    gender classification, landmark detection, and AU detection to take advantage
    of existing information as much as possible. In the future, more relevant tasks,
    such as identity classification and age estimation, could be studied.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络架构方面，与基本的单流网络相比，多流网络可以从多视角输入中提取特征，为 MER 提供更多信息。另一方面，级联网络将各种模块按顺序组合以完成不同任务，从而构建一个有效的网络，并逐步筛选掉与
    ME 无关的信息。考虑到多流网络和级联网络的优势，已经开发出多流级联网络，以进一步提升 MER 的性能。未来，探索结合各种高效模块的多流级联网络是 MER
    研究的一个有前途的方向。此外，多任务学习框架通过利用相关任务的信息实现了鲁棒的 MER。多任务学习能够充分利用面部上可用的更多信息。目前的 MER 研究探索了性别分类、地标检测和
    AU 检测，以尽可能多地利用现有信息。未来可以研究更多相关任务，如身份分类和年龄估计。
- en: Fine-tuning is widely used in MER. Recent research [[69](#bib.bib69)] illustrated
    that borrowing information from large FE datasets through knowledge distillation
    and domain adaptation can achieve promising performance. For future ME research,
    how to effectively leverage massive face images will be a focus. Besides, the
    semi-supervised learning [[194](#bib.bib194)] and unsupervised learning [[195](#bib.bib195)]
    could be promising research directions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 微调在MER中被广泛使用。近期研究[[69](#bib.bib69)]表明，通过知识蒸馏和领域适应从大型特征提取（FE）数据集中借用信息可以取得令人满意的性能。未来的ME研究将关注如何有效利用大量面部图像。此外，半监督学习[[194](#bib.bib194)]和无监督学习[[195](#bib.bib195)]可能是有前景的研究方向。
- en: For the losses, most DL-based MER employs the basic softmax cross-entropy loss.
    Several works utilized the metric learning loss and margin loss to increase the
    compactness of intra-class variations and separable inter-class differences. Furthermore,
    since the ME datasets are imbalanced, multiple works aimed to boost MER performance
    through Focal loss. However, current MER methods just employed the losses designed
    for common tasks, such as image classification and face recognition. MER is a
    special task due to the ME characteristics (low intensity and imbalanced small-scale
    ME datasets), effective losses aimed for MER should be explored in the future.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于损失，大多数基于深度学习的情感识别（MER）采用了基本的softmax交叉熵损失。一些研究利用了度量学习损失和边距损失来增加类别内部变化的紧凑性和类别之间差异的可分性。此外，由于情感表达（ME）数据集不平衡，多项工作旨在通过焦点损失（Focal
    loss）提升MER性能。然而，目前的MER方法只是采用了为常见任务（如图像分类和人脸识别）设计的损失函数。由于MER是一个特殊任务（低强度和不平衡的小规模ME数据集），未来应该探索针对MER的有效损失函数。
- en: 6 Experiments
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验
- en: 6.1 Evaluation matrix
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 评估指标
- en: The common evaluation metrics for MER are accuracy and F1-score. In general,
    the accuracy metric measures the ratio of correct predictions over the total evaluated
    samples. However, the accuracy is susceptible to bias data. F1-score solves the
    bias problem by considering the total True Positives (TP), False Positives (FP)
    and False Negatives (FN) to reveal the true classification performance.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: MER的常见评估指标是准确率和F1分数。通常，准确率指标衡量的是正确预测的比例与总评估样本数的比值。然而，准确率容易受到偏差数据的影响。F1分数通过考虑总的真正例（TP）、假正例（FP）和假负例（FN），来解决偏差问题，从而揭示真实的分类性能。
- en: For the composited dataset which combines multiple datasets leading to severe
    data imbalance, Unweighted F1-score (UF1) and Unweighted Average Recall (UAR)
    are utilized to measure the performance of various methods. UF1 is also known
    as macro-averaged F1-score which is determined by averaging the per-class F1-scores.
    UF1 provides equal emphasis on rare classes in imbalanced multi-class settings.
    UAR is defined as the average accuracy of each class divided by the number of
    classes without consideration of samples per class. UAR can reduce the bias caused
    by class imbalance and is known as balanced accuracy.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于合成数据集，即结合了多个数据集导致严重数据不平衡的情况，使用了未加权的F1分数（UF1）和未加权的平均召回率（UAR）来衡量各种方法的性能。UF1也称为宏平均F1分数，它是通过对每个类别的F1分数取平均来确定的。UF1在不平衡的多类别设置中对稀有类别给予了相等的重视。UAR定义为每个类别的平均准确率除以类别数量，不考虑每个类别的样本数。UAR可以减少由于类别不平衡而导致的偏差，并且被称为平衡准确率。
- en: 'TABLE III: MER on SMIC, CASME, CASME II, SAMM, and CMED datasets.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：SMIC、CASME、CASME II、SAMM和CMED数据集上的MER。
- en: '| Dataset | Method | Year | Pre-p. | Input | Network architecture | Block |
    Pre-train | Protocol | Cate. | F1 | ACC (%) |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | 年份 | 预处理 | 输入 | 网络架构 | 块 | 预训练 | 协议 | 类别 | F1 | 准确率 (%) |'
- en: '| SMIC | TSCNN [[99](#bib.bib99)] | 2019 | E, R | OF+Apex | 3S-CNN | - | FER2013
    [[196](#bib.bib196)] | LOSO | 3 | 0.7236 | 72.74 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| SMIC | TSCNN [[99](#bib.bib99)] | 2019 | E, R | OF+Apex | 3S-CNN | - | FER2013
    [[196](#bib.bib196)] | LOSO | 3 | 0.7236 | 72.74 |'
- en: '| DIKD [[69](#bib.bib69)] | 2020 | - | Apex | CNN+KD+SVM | - | - | LOSO | 3
    | 0.71 | 76.06 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| DIKD [[69](#bib.bib69)] | 2020 | - | Apex | CNN+KD+SVM | - | - | LOSO | 3
    | 0.71 | 76.06 |'
- en: '| MTMNet [[122](#bib.bib122)] | 2020 | - | Onset-Apex | 2S-CNN+DA+GAN | RES
    | CK+ [[197](#bib.bib197)],MMI [[10](#bib.bib10)], Oulu-CASIA [[198](#bib.bib198)]
    | LOSO | 3 | 0.744 | 76.0 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| MTMNet [[122](#bib.bib122)] | 2020 | - | Onset-Apex | 2S-CNN+DA+GAN | RES
    | CK+ [[197](#bib.bib197)], MMI [[10](#bib.bib10)], Oulu-CASIA [[198](#bib.bib198)]
    | LOSO | 3 | 0.744 | 76.0 |'
- en: '| MiMaNet [[190](#bib.bib190)] | 2021 | T | Apex+sequence | 2S-CNN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI[[10](#bib.bib10)] | LOSO | 3 | 0.778 | 78.6 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| MiMaNet [[190](#bib.bib190)] | 2021 | T | Apex+sequence | 2S-CNN+DA | RES
    | CK+ [[197](#bib.bib197)], MMI [[10](#bib.bib10)] | LOSO | 3 | 0.778 | 78.6 |'
- en: '| DSTAN[[180](#bib.bib180)] | 2021 | T | OF+sequence | 2S-CNN+LSTM+SVM | Attention
    | - | LOSO | 3 | 0.78 | 77 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| DSTAN[[180](#bib.bib180)] | 2021 | T | OF+sequence | 2S-CNN+LSTM+SVM | Attention
    | - | LOSO | 3 | 0.78 | 77 |'
- en: '| AMAN[[177](#bib.bib177)] | 2022 | E,T | sequence | CNN | Attention | FER2013
    [[196](#bib.bib196)] | LOSO | 3 | 0.77 | 79.87 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| AMAN[[177](#bib.bib177)] | 2022 | E,T | sequence | CNN | Attention | FER2013
    [[196](#bib.bib196)] | LOSO | 3 | 0.77 | 79.87 |'
- en: '| CASME | TSCNN [[99](#bib.bib99)] | 2019 | E,R | OF+Apex | 3S-CNN | - | FER2013
    [[196](#bib.bib196)] | LOSO | 4 | 0.7270 | 73.88 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| CASME | TSCNN [[99](#bib.bib99)] | 2019 | E,R | OF+Apex | 3S-CNN | - | FER2013
    [[196](#bib.bib196)] | LOSO | 4 | 0.7270 | 73.88 |'
- en: '| DIKD [[69](#bib.bib69)] | 2020 | - | Apex | CNN+KD+SVM | RES | - | LOSO |
    4 | 0.77 | 81.80 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| DIKD [[69](#bib.bib69)] | 2020 | - | Apex | CNN+KD+SVM | RES | - | LOSO |
    4 | 0.77 | 81.80 |'
- en: '| AffectiveNet [[165](#bib.bib165)] | 2020 | E | DI | 4S-CNN | MFL | - | LOSO
    | 4 | - | 72.64 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| AffectiveNet [[165](#bib.bib165)] | 2020 | E | DI | 4S-CNN | MFL | - | LOSO
    | 4 | - | 72.64 |'
- en: '| DSTAN[[180](#bib.bib180)] | 2021 | T | OF+sequence | 2S-CNN+LSTM+SVM | Attention
    | - | LOSO | 4 | 0.75 | 78 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| DSTAN[[180](#bib.bib180)] | 2021 | T | OF+sequence | 2S-CNN+LSTM+SVM | Attention
    | - | LOSO | 4 | 0.75 | 78 |'
- en: '| CASME II | OFF-ApexNet [[95](#bib.bib95)] | 2019 | - | OF | 2S-CNN | - |
    - | LOSO | 3 | 0.8697 | 88.28 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| CASME II | OFF-ApexNet [[95](#bib.bib95)] | 2019 | - | OF | 2S-CNN | - |
    - | LOSO | 3 | 0.8697 | 88.28 |'
- en: '| TSCNN [[99](#bib.bib99)] | 2019 | E, R | OF+Apex | 3S-CNN | - | FER2013 [[196](#bib.bib196)]
    | LOSO | 5 | 0.807 | 80.97 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| TSCNN [[99](#bib.bib99)] | 2019 | E, R | OF+Apex | 3S-CNN | - | FER2013 [[196](#bib.bib196)]
    | LOSO | 5 | 0.807 | 80.97 |'
- en: '| STSTNet [[106](#bib.bib106)] | 2019 | E | OF | 3S-3DCNN | - | - | LOSO |
    3 | 0.8382 | 86.86 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| STSTNet [[106](#bib.bib106)] | 2019 | E | OF | 3S-3DCNN | - | - | LOSO |
    3 | 0.8382 | 86.86 |'
- en: '| Graph-TCN [[89](#bib.bib89)] | 2020 | L, R | Apex | TCN+GCN | Graph | - |
    LOSO | 5 | 0.7246 | 73.98 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Graph-TCN [[89](#bib.bib89)] | 2020 | L, R | Apex | TCN+GCN | Graph | - |
    LOSO | 5 | 0.7246 | 73.98 |'
- en: '| SMA-STN [[74](#bib.bib74)] | 2020 | - | Snippet | CNN | Attention | WIDER
    FACE[[199](#bib.bib199)] | LOSO | 5 | 0.7946 | 82.59 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| SMA-STN [[74](#bib.bib74)] | 2020 | - | Snippet | CNN | Attention | WIDER
    FACE[[199](#bib.bib199)] | LOSO | 5 | 0.7946 | 82.59 |'
- en: '| GEME [[76](#bib.bib76)] | 2021 | - | DI | 2S-CNN+ML | RES | - | LOSO | 5
    | 0.7354 | 75.20 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| GEME [[76](#bib.bib76)] | 2021 | - | DI | 2S-CNN+ML | RES | - | LOSO | 5
    | 0.7354 | 75.20 |'
- en: '| MiMaNet [[190](#bib.bib190)] | 2021 | T | Apex+sequence | 2S-CNN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI[[10](#bib.bib10)] | LOSO | 5 | 0.759 | 79.9 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| MiMaNet [[190](#bib.bib190)] | 2021 | T | Apex+sequence | 2S-CNN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI[[10](#bib.bib10)] | LOSO | 5 | 0.759 | 79.9 |'
- en: '| LR-GACNN [[75](#bib.bib75)] | 2021 | E | OF+Landmark | 2S-GACNN | Graph |
    - | LOSO | 5 | 0.7090 | 81.30 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| LR-GACNN [[75](#bib.bib75)] | 2021 | E | OF+Landmark | 2S-GACNN | Graph |
    - | LOSO | 5 | 0.7090 | 81.30 |'
- en: '| DSTAN[[180](#bib.bib180)] | 2021 | T | OF+sequence | 2S-CNN+LSTM+SVM | Attention
    | - | LOSO | 5 | 0.73 | 75 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| DSTAN[[180](#bib.bib180)] | 2021 | T | OF+sequence | 2S-CNN+LSTM+SVM | Attention
    | - | LOSO | 5 | 0.73 | 75 |'
- en: '| AMAN[[177](#bib.bib177)] | 2022 | E,T | sequence | CNN | Attention | FER2013
    [[196](#bib.bib196)] | LOSO | 5 | 0.71 | 75.40 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| AMAN[[177](#bib.bib177)] | 2022 | E,T | sequence | CNN | Attention | FER2013
    [[196](#bib.bib196)] | LOSO | 5 | 0.71 | 75.40 |'
- en: '| SAMM | DIKD [[69](#bib.bib69)] | 2020 | - | Apex | CNN+KD+SVM | - | - | LOSO
    | 4 | 0.83 | 86.74 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| SAMM | DIKD [[69](#bib.bib69)] | 2020 | - | Apex | CNN+KD+SVM | - | - | LOSO
    | 4 | 0.83 | 86.74 |'
- en: '| SMA-STN [[74](#bib.bib74)] | 2020 | - | Snippet | CNN | - | WIDER FACE[[199](#bib.bib199)]
    | LOSO | 5 | 0.7033 | 77.20 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| SMA-STN [[74](#bib.bib74)] | 2020 | - | Snippet | CNN | - | WIDER FACE[[199](#bib.bib199)]
    | LOSO | 5 | 0.7033 | 77.20 |'
- en: '| MTMNet [[122](#bib.bib122)] | 2020 | - | Onset-Apex | 2S-CNN+GAN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI [[10](#bib.bib10)], Oulu-CASIA [[198](#bib.bib198)]
    | LOSO | 5 | 0.736 | 74.1 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| MTMNet [[122](#bib.bib122)] | 2020 | - | Onset-Apex | 2S-CNN+GAN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI [[10](#bib.bib10)], Oulu-CASIA [[198](#bib.bib198)]
    | LOSO | 5 | 0.736 | 74.1 |'
- en: '| MiMaNet [[190](#bib.bib190)] | 2021 | T | Apex+sequence | 2S-CNN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI[[10](#bib.bib10)] | LOSO | 5 | 0.764 | 76.7 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| MiMaNet [[190](#bib.bib190)] | 2021 | T | Apex+sequence | 2S-CNN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI[[10](#bib.bib10)] | LOSO | 5 | 0.764 | 76.7 |'
- en: '| LR-GACNN [[75](#bib.bib75)] | 2021 | E | OF+Landmark | 2S-GACNN | - | - |
    LOSO | 5 | 0.8279 | 88.24 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| LR-GACNN [[75](#bib.bib75)] | 2021 | E | OF+Landmark | 2S-GACNN | - | - |
    LOSO | 5 | 0.8279 | 88.24 |'
- en: '| GRAPH-AU [[92](#bib.bib92)] | 2021 | L | Apex | 2S-CNN+GCN | Graph, Transformer
    | - | LOSO | 5 | 0.7045 | 74.26 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| GRAPH-AU [[92](#bib.bib92)] | 2021 | L | Apex | 2S-CNN+GCN | Graph, Transformer
    | - | LOSO | 5 | 0.7045 | 74.26 |'
- en: '| AMAN[[177](#bib.bib177)] | 2022 | E,T | sequence | CNN | Attention | FER2013
    [[196](#bib.bib196)] | LOSO | 5 | 0.67 | 68.85 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| AMAN[[177](#bib.bib177)] | 2022 | E,T | sequence | CNN | Attention | FER2013
    [[196](#bib.bib196)] | LOSO | 5 | 0.67 | 68.85 |'
- en: '| CMED | Shallow CNN[[44](#bib.bib44)] | 2020 | E | OF | CNN | - | - | LOSO
    | 7 | 0.6353 | 66.06 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| CMED | Shallow CNN[[44](#bib.bib44)] | 2020 | E | OF | CNN | - | - | LOSO
    | 7 | 0.6353 | 66.06 |'
- en: '¹ Pre-p.: Pre-processing; E:EVM; R: RoI; T: Temporal normalization ; L: Learning-based
    magnification.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '¹ Pre-p.: 预处理；E: EVM；R: RoI；T: 时间归一化；L: 基于学习的放大。'
- en: '² OF: Optical flow; DI: Dynamic image.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '² OF: 光流；DI: 动态图像。'
- en: '³ nS-CNN: n-stream CNN; ML: Multi-task learning; DA: Domain adaption; KD: Knowledge
    distillation.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '³ nS-CNN: n-stream CNN；ML: 多任务学习；DA: 领域适应；KD: 知识蒸馏。'
- en: '⁴ Cate: Category; F1: F1-score; ACC: Accuracy; RES: Residual block.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '⁴ Cate: 类别；F1: F1 分数；ACC: 准确率；RES: 残差块。'
- en: 6.2 Model evaluation protocols
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 模型评估协议
- en: Cross-validation is the widely utilized protocol for evaluating the MER performance.
    In cross-validation, the dataset is splitted into multiple folds and the training
    and testing were evaluated on different folds. It regards a fair verification
    and prevents overfitting on the small-scale ME datasets. In the MER field, cross-validation
    includes leave-one-subject-out (LOSO), leave-one-video-out (LOVO), and K-Fold
    cross-validations.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是评估 MER 性能的广泛使用协议。在交叉验证中，数据集被分成多个折叠，训练和测试在不同的折叠上进行评估。它提供了公平的验证，防止了小规模 ME
    数据集上的过拟合。在 MER 领域，交叉验证包括留一受试者法 (LOSO)、留一视频法 (LOVO) 和 K 折交叉验证。
- en: In LOSO, every subject is taken as a test set in turn and the other subjects
    as the training data. This kind of subject-independent protocol can avoid subject
    bias and evaluate the generalization performance of various algorithms. LOSO is
    the most popular cross-validation in MER.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LOSO 中，每个受试者轮流作为测试集，其他受试者作为训练数据。这种独立于受试者的协议可以避免受试者偏差，评估各种算法的泛化性能。LOSO 是 MER
    中最受欢迎的交叉验证方法。
- en: The LOVO takes each sample as the validation unit which enables more training
    data and alleviates the overfitting to some degree. However, it is not subject-independent,
    thus it can not well evaluate the generalization capability. Another problem is
    that the test number of LOVO is the sample size which may lead to huge time cost,
    not suitable for deep learning.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: LOVO 将每个样本作为验证单元，这样可以使用更多的训练数据，并在一定程度上减轻过拟合。然而，它不是独立于受试者的，因此不能很好地评估泛化能力。另一个问题是
    LOVO 的测试数量是样本量，这可能导致巨大的时间成本，不适合深度学习。
- en: For K-fold cross-validation, the original samples are randomly partitioned into
    k equal-sized parts. Each part is taken as a test set in turn and the rest are
    the training data. Thus, the number of cross-validation tests is K. In practice,
    the evaluation time can be greatly reduced by setting an appropriate K. The typical
    K values are 5 or 10.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 K 折交叉验证，原始样本被随机划分为 k 个大小相等的部分。每个部分轮流作为测试集，其他部分作为训练数据。因此，交叉验证测试的数量为 K。在实际应用中，通过设置适当的
    K 值，可以大大减少评估时间。典型的 K 值为 5 或 10。
- en: Since the MEs have small-scale datasets, the experiments on MER do not have
    reliable validation datasets. According to the released codes, some works [[120](#bib.bib120)]
    utilized the test datasets as the validation datasets directly and reserved the
    best epoch results on each fold as the final results. As the data is limited,
    even only two samples for some subjects, the final MER results will be greatly
    improved by regarding the test data as the validation data. According to [[169](#bib.bib169)],
    compared to the experiments based on the same epoch on all of the folds, the results
    can be increased by more than 10$\%$ by testing on the test datasets. But, in
    practice, the test data is unknown and it is not reasonable to reserve the best
    epoch results on each fold of the test data as the final results.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 MEs 具有小规模数据集，MER 上的实验没有可靠的验证数据集。根据发布的代码，一些工作 [[120](#bib.bib120)] 直接将测试数据集用作验证数据集，并将每个折叠中的最佳时期结果保留作为最终结果。由于数据有限，某些受试者甚至只有两个样本，通过将测试数据视为验证数据，可以大大提高最终的
    MER 结果。根据 [[169](#bib.bib169)]，与基于所有折叠上相同时期的实验相比，通过在测试数据集上测试，结果可以提高超过 10$\%$。但实际上，测试数据是未知的，将每个折叠的最佳时期结果保留作为最终结果是不合理的。
- en: 'TABLE IV: MER on the Composite dataset (MECG2019)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：复合数据集上的 MER（MECG2019）
- en: '| Method | Year | Pre-p. | Input | Network architecture | Block | Pre-train
    | Protocol | Cate. | UF1 | UAR |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 预处理 | 输入 | 网络架构 | 块 | 预训练 | 协议 | 类别 | UF1 | UAR |'
- en: '| NMER  [[120](#bib.bib120)] | 2019 | E, R | OF | CNN+DA | - | - | LOSO | 3
    | 0.7885 | 0.7824 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| NMER [[120](#bib.bib120)] | 2019 | E, R | OF | CNN+DA | - | - | LOSO | 3
    | 0.7885 | 0.7824 |'
- en: '| Dual-Inception [[81](#bib.bib81)] | 2019 | - | OF | 2S-CNN | Inception |
    - | LOSO | 3 | 0.7322 | 0.7278 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 双重Inception [[81](#bib.bib81)] | 2019 | - | OF | 2S-CNN | Inception | - |
    LOSO | 3 | 0.7322 | 0.7278 |'
- en: '| ICE-GAN [[27](#bib.bib27)] | 2020 | GAN | Apex | CNN+GAN | Capsule | ImageNet
    | LOSO | 3 | 0.845 | 0.841 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| ICE-GAN [[27](#bib.bib27)] | 2020 | GAN | Apex | CNN+GAN | Capsule | ImageNet
    | LOSO | 3 | 0.845 | 0.841 |'
- en: '| MTMNet [[122](#bib.bib122)] | 2020 | - | Onset-Apex | 2S-CNN+GAN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI [[10](#bib.bib10)], Oulu-CASIA [[198](#bib.bib198)]
    | LOSO | 3 | 0.864 | 0.857 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| MTMNet [[122](#bib.bib122)] | 2020 | - | Onset-Apex | 2S-CNN+GAN+DA | RES
    | CK+ [[197](#bib.bib197)], MMI [[10](#bib.bib10)], Oulu-CASIA [[198](#bib.bib198)]
    | LOSO | 3 | 0.864 | 0.857 |'
- en: '| FR [[185](#bib.bib185)] | 2021 | - | OF | 2S-CNN+ML | Inception | - | LOSO
    | 3 | 0.7838 | 0.7832 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| FR [[185](#bib.bib185)] | 2021 | - | OF | 2S-CNN+ML | Inception | - | LOSO
    | 3 | 0.7838 | 0.7832 |'
- en: '| MiMaNet [[190](#bib.bib190)] | 2021 | T | Apex+sequence | 2S-CNN+DA | RES
    | CK+ [[197](#bib.bib197)],MMI[[10](#bib.bib10)] | LOSO | 3 | 0.883 | 0.876 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| MiMaNet [[190](#bib.bib190)] | 2021 | T | Apex+sequence | 2S-CNN+DA | RES
    | CK+ [[197](#bib.bib197)], MMI[[10](#bib.bib10)] | LOSO | 3 | 0.883 | 0.876 |'
- en: '| GRAPH-AU [[92](#bib.bib92)] | 2021 | L | Apex | 2S-CNN+GCN | Graph, Transformer
    | - | LOSO | 3 | 0.7914 | 0.7933 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| GRAPH-AU [[92](#bib.bib92)] | 2021 | L | Apex | 2S-CNN+GCN | 图，Transformer
    | - | LOSO | 3 | 0.7914 | 0.7933 |'
- en: '| BDCNN [[136](#bib.bib136)] | 2022 | L | OF | 4S-CNN | - | - | LOSO | 3 |
    0.8509 | 0.8500 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| BDCNN [[136](#bib.bib136)] | 2022 | L | OF | 4S-CNN | - | - | LOSO | 3 |
    0.8509 | 0.8500 |'
- en: '¹ Pre-p.: Pre-processing; E: EVM; R: RoI; T: Temporal normalization ; L: Learning-based
    magnification.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '¹ Pre-p.: 预处理; E: EVM; R: RoI; T: 时间归一化; L: 基于学习的放大。'
- en: '² OF: Optical flow; DI: Dynamic image.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '² OF: 光流; DI: 动态图像。'
- en: '³ nS-CNN: n-stream CNN; ML: Multi-task learning; DA: Domain adaption; KD: Knowledge
    distillation'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '³ nS-CNN: n-stream CNN; ML: 多任务学习; DA: 领域适应; KD: 知识蒸馏'
- en: '⁴ Cate.: Category; RES: Residual block.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '⁴ Cate.: 类别; RES: 残差块。'
- en: 'Tables [III](#S6.T3 "TABLE III ‣ 6.1 Evaluation matrix ‣ 6 Experiments ‣ Deep
    Learning for Micro-expression Recognition: A Survey") and [IV](#S6.T4 "TABLE IV
    ‣ 6.2 Model evaluation protocols ‣ 6 Experiments ‣ Deep Learning for Micro-expression
    Recognition: A Survey") list the reported performance of representative recent
    work of DL-based MER on popular ME datasets. As we discussed before, the evaluation
    protocol is varying and the practical training rule of each paper is ambiguous,
    we can not directly make a conclusion that which method performs best for MER.
    But, from the experimental results, the general trends of MER can be found.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [III](#S6.T3 "TABLE III ‣ 6.1 Evaluation matrix ‣ 6 Experiments ‣ Deep Learning
    for Micro-expression Recognition: A Survey") 和 [IV](#S6.T4 "TABLE IV ‣ 6.2 Model
    evaluation protocols ‣ 6 Experiments ‣ Deep Learning for Micro-expression Recognition:
    A Survey") 列出了在流行的微表情数据集上基于深度学习的微表情识别（MER）代表性近期工作的报告性能。如前所述，评估协议各异且每篇论文的实际训练规则模糊，我们不能直接得出哪种方法在MER中表现最佳的结论。但从实验结果中，可以发现MER的一般趋势。'
- en: For the input, in general, the combined inputs can provide promising results
    on all of the datasets [[99](#bib.bib99), [180](#bib.bib180), [75](#bib.bib75)].
    This is because the different input modalities can contribute information from
    different views. On the basis of various input modalities, we can explore useful
    information on limited ME samples to the greatest extent. Since the combined inputs
    is a good choice for robust MER, the multi-stream network is recommended to learn
    effective representations from various inputs [[99](#bib.bib99), [180](#bib.bib180),
    [75](#bib.bib75)]. In contrast to the combined inputs, the sole sequence performs
    worse [[177](#bib.bib177)], due to the limited information and redundancy.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入，一般来说，组合输入可以在所有数据集上提供有希望的结果 [[99](#bib.bib99), [180](#bib.bib180), [75](#bib.bib75)]。这是因为不同的输入模态可以从不同的视角提供信息。在各种输入模态的基础上，我们可以在有限的微表情样本上最大程度地探索有用的信息。由于组合输入是进行鲁棒性MER的良好选择，因此建议多流网络从各种输入中学习有效的表示
    [[99](#bib.bib99), [180](#bib.bib180), [75](#bib.bib75)]。相比之下，单一序列表现较差 [[177](#bib.bib177)]，这是由于信息有限和冗余。
- en: 'Besides, from Tables [III](#S6.T3 "TABLE III ‣ 6.1 Evaluation matrix ‣ 6 Experiments
    ‣ Deep Learning for Micro-expression Recognition: A Survey") and [IV](#S6.T4 "TABLE
    IV ‣ 6.2 Model evaluation protocols ‣ 6 Experiments ‣ Deep Learning for Micro-expression
    Recognition: A Survey"), it can be seen that the learning strategy including fine-tuning
    [[74](#bib.bib74)], domain adaptation [[122](#bib.bib122), [190](#bib.bib190)]
    and knowledge distillation [[69](#bib.bib69)] can achieve state-of-the-art results
    on both the individual datasets and the composite dataset. This could be explained
    that the limited ME sample is the main challenge for MER and leveraging other
    related data sources is a reasonable and effective solution. In the future, domain
    adaption and knowledge distillation should be further researched to boost MER
    performance.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，从表格 [III](#S6.T3 "TABLE III ‣ 6.1 Evaluation matrix ‣ 6 Experiments ‣ Deep
    Learning for Micro-expression Recognition: A Survey") 和 [IV](#S6.T4 "TABLE IV
    ‣ 6.2 Model evaluation protocols ‣ 6 Experiments ‣ Deep Learning for Micro-expression
    Recognition: A Survey") 可以看出，包括微调 [[74](#bib.bib74)]、领域适应 [[122](#bib.bib122),
    [190](#bib.bib190)] 和知识蒸馏 [[69](#bib.bib69)] 的学习策略在单独数据集和综合数据集上均能取得最先进的结果。这可以解释为有限的ME样本是MER的主要挑战，利用其他相关数据源是一种合理且有效的解决方案。未来，领域适应和知识蒸馏应进一步研究，以提升MER性能。'
- en: In some latest works [[92](#bib.bib92), [75](#bib.bib75), [89](#bib.bib89)],
    the GCN becomes a mainstream choice for MER and shows promising performance. Currently,
    the spatio-temporal graph representation combined with GCNs obtains more attention
    in MER studies. The possible reason is that the landmark and AU information are
    helpful and effective for locating and representing the facial muscle movements.
    However, the small-sample ME datasets limit the ability of graph representation.
    The combination of transfer learning and graph should be a promising direction
    for future ME studies.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些最新的研究 [[92](#bib.bib92), [75](#bib.bib75), [89](#bib.bib89)] 中，GCN已成为MER的主流选择，并表现出有前景的性能。目前，结合GCN的时空图表示在MER研究中获得了更多关注。可能的原因是地标和AU信息对于定位和表示面部肌肉运动是有帮助且有效的。然而，小样本ME数据集限制了图表示的能力。将迁移学习与图结合应是未来ME研究的一个有前途的方向。
- en: 7 Challenges AND FUTURE DIRECTIONS
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 挑战与未来方向
- en: MER has a wide range of potential applications in various fields, such as psychological
    disorders, education, business negotiation, and security control. More specific
    descriptions about applications of MER are introduced in APPENDIX A. Although
    MER could facilitate society in various fields, there are many challenges. In
    this section, we discuss the challenges and future directions of MER.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: MER在心理障碍、教育、商务谈判和安全控制等各个领域具有广泛的潜在应用。关于MER应用的更具体描述请见附录A。虽然MER可以在各个领域促进社会发展，但也面临许多挑战。在这一部分，我们探讨了MER的挑战和未来方向。
- en: 7.1 Dealing with small-scale dataset
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 处理小规模数据集
- en: Deep learning is a data-driven method, and successful training requires various
    large-scale data. Recent studies indicated that annotation bias, emotional contexts,
    and cultural backgrounds could affect the ME perception [[200](#bib.bib200)].
    They may mislead the model training and finally cause misclassification. Unfortunately,
    existing ME datasets are far from enough for training a robust model. To this
    end, more diverse ME datasets should be collected. Besides, effective deep-based
    data augmentation approaches should be further developed for ME analysis to avoid
    over-fitting. Semi-supervised and unsupervised learning could also be potential
    solutions.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一种数据驱动的方法，成功的训练需要各种大规模的数据。近期研究表明，注释偏差、情感背景和文化背景可能影响ME的感知 [[200](#bib.bib200)]。这些因素可能会误导模型训练，最终导致误分类。不幸的是，现有的ME数据集远远不足以训练一个强健的模型。为此，应收集更多多样化的ME数据集。此外，还应进一步开发有效的基于深度学习的数据增强方法，以避免过拟合。半监督和无监督学习也可能是潜在的解决方案。
- en: In addition, some emotions, such as fear, are challenging to be evoked and collected.
    The data imbalance causes the network to be biased towards classes in the majority.
    Therefore, effective imbalanced losses are needed.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些情感，如恐惧，难以引发和收集。数据不平衡导致网络偏向于多数类。因此，需要有效的不平衡损失。
- en: 7.2 3D ME sequence
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 3D ME序列
- en: Currently, the main focus of MER is based on the 2D domain because of the data
    prevalence in the relevant modalities including images and videos. Although significant
    progress has been made for MER in recent years, most existing MER algorithms based
    on 2D facial images and sequences can not solve the challenging problems of illumination
    and pose variations in real-world applications. Recent research about FE analysis
    illustrates that the above issues can be addressed through 3D facial data [[201](#bib.bib201)].
    Inherent characteristics of 3D face make facial recognition robust to lighting
    and pose variations. Moreover, 3D geometry information may include important features
    for FER and provide more data for better training. Thanks to the benefits of 3D
    faces and the technological development of 3D scanning, MER based on 3D sequence
    could be a promising research direction. Special 3D blocks, such as 3D Graph and
    Transformer, should be studied in 3D MER.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，MER的主要焦点基于2D领域，因为相关模态（包括图像和视频）中数据的普遍性。尽管近年来MER取得了显著进展，但大多数基于2D面部图像和序列的现有MER算法仍无法解决实际应用中的光照和姿态变化等挑战性问题。关于FE分析的最新研究表明，可以通过3D面部数据[[201](#bib.bib201)]来解决这些问题。3D面部的固有特性使面部识别对光照和姿态变化具有较强的鲁棒性。此外，3D几何信息可能包含对FER重要的特征，并提供更多数据以便更好的训练。由于3D面部的优势和3D扫描技术的发展，基于3D序列的MER可能是一个有前景的研究方向。应在3D
    MER中研究特定的3D块，如3D图形和Transformer。
- en: 7.3 AU analysis in MEs
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 AU分析在面部表情中的应用
- en: MEs reveal people’s hidden emotions in high-stake situations [[202](#bib.bib202),
    [3](#bib.bib3)] and have various applications such as clinic diagnosis and national
    security. However, ME interpretation suffers ambiguities  [[55](#bib.bib55)],
    e.g., the inner brow raiser may refer to surprise or sad. The FACS [[6](#bib.bib6)]
    has been verified to be effective for resolving the ambiguity issue. In FACS,
    action units (AUs) are defined as the basic facial movements, working as the building
    blocks to formulate multiple FEs [[6](#bib.bib6)]. Furthermore, the criteria for
    AU and FE correspondence is defined in FACS manual. Encoding AUs has been verified
    to benefit the MER [[69](#bib.bib69), [58](#bib.bib58), [203](#bib.bib203)] through
    embedding AU features. In the future, the relationship between AUs and MEs can
    be further explored to improve MER.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 面部表情（MEs）在高风险情况下揭示了人们的隐藏情感[[202](#bib.bib202), [3](#bib.bib3)]，并在临床诊断和国家安全等多个领域具有广泛应用。然而，面部表情解释存在歧义[[55](#bib.bib55)]，例如，内眉提升可能表示惊讶或悲伤。FACS
    [[6](#bib.bib6)]已被验证在解决歧义问题上是有效的。在FACS中，动作单位（AUs）被定义为基本的面部动作，作为构建多个面部表情（FEs）的基础构件[[6](#bib.bib6)]。此外，FACS手册中定义了AU与FE的对应标准。编码AU已被验证通过嵌入AU特征对MER有益[[69](#bib.bib69),
    [58](#bib.bib58), [203](#bib.bib203)]。未来，可以进一步探讨AU与MEs之间的关系，以改善MER。
- en: 7.4 Multi-modal MER
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 多模态MER
- en: One of the MER challenges is that the low-intensity and small-scale ME datasets
    provide very limited information for robust MER. Recent research demonstrated
    that utilizing multiple modalities can provide complementary information and enhance
    classification robustness. Different emotional expressions can produce different
    changes in autonomic activity, e.g. fear leads to increased heart rate and decreased
    skin temperature. Thus, the physiological signal can be utilized to incorporate
    complementary information for further improving MER. Besides, in recent years,
    new micro-gesture datasets [[204](#bib.bib204)] had been proposed. The micro-gesture
    is body movements that are elicited when hidden expressions are triggered in unconstrained
    situations. The hidden emotional states can be reflected through micro-gestures.
    How to combine multiple modalities to enhance MER performance is an important
    future direction. Lightweight multi-stream networks should be developed to learn
    multi-view ME information effectively and efficiently.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: MER面临的挑战之一是低强度和小规模的ME数据集提供的信息非常有限，难以进行稳健的MER。最近的研究表明，利用多模态可以提供互补信息并增强分类的鲁棒性。不同的情感表达可以在自主活动上产生不同的变化，例如，恐惧会导致心率增加和皮肤温度下降。因此，可以利用生理信号来结合互补信息，以进一步改善MER。此外，近年来提出了新的微表情数据集[[204](#bib.bib204)]。微表情是当隐藏的情感在非约束情况中被触发时产生的身体动作。隐藏的情感状态可以通过微表情反映出来。如何结合多种模态来增强MER性能是未来的重要方向。应开发轻量级的多流网络，以有效地学习多视角的ME信息。
- en: 7.5 The explainanty of MER based on DL
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 基于深度学习的MER解释性
- en: The neural network is a brain-inspired model developed by neurobiologists and
    psychologists to test the computational analog of neurons [[205](#bib.bib205)].
    Naturally, it could be a tool to verify the theory in other disciplines, such
    as psychology, to enhance psychological and human communication study. In addition,
    the current DL is a “black box” algorithm [[205](#bib.bib205)] and focuses on
    learning features and recognizing patterns by updating the weights of networks.
    The interpretation and understanding of the inside DL process can get experts
    from cross disciplines involved in the internal state analysis and therefore facilitate
    building interpretable and reliable deep models.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一个受大脑启发的模型，由神经生物学家和心理学家开发，用于测试神经元的计算类比[[205](#bib.bib205)]。自然，它可以成为验证其他学科理论的工具，如心理学，以增强心理学和人际沟通研究。此外，目前的深度学习（DL）是一个“黑箱”算法[[205](#bib.bib205)]，专注于通过更新网络的权重来学习特征和识别模式。对DL内部过程的解释和理解可以让跨学科的专家参与内部状态分析，从而有助于构建可解释和可靠的深度模型。
- en: 7.6 MEs in realistic situations
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6 现实情况中的MEs
- en: Currently, most existing MER researches focus on classifying the basic MEs collected
    in controlled environments from the frontal view without any head movements, illumination
    variations or occlusion. However, it is almost impossible to reproduce such strict
    conditions in real-world applications. The approaches based on the constrained
    settings usually do not generalize well to videos recorded in-the-wild environment.
    Practical and robust algorithms for recognizing MEs in realistic situations with
    pose changes and illumination variations should be developed in the future.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数现有的MER研究集中在分类从受控环境中收集的基本MEs，这些环境中的视频通常是正面视角，没有任何头部移动、光照变化或遮挡。然而，在现实应用中几乎不可能再现如此严格的条件。基于受限设置的方法通常对在自然环境中录制的视频不具备良好的泛化能力。未来应开发在具有姿态变化和光照变化的实际情况中识别MEs的实用和鲁棒的算法。
- en: Moreover, most ME researches assume that there are just MEs in a video clip.
    However, in real life, MEs can appear with macro-expressions. Future studies should
    explore deep-based ME spotting methods to detect and distinguish the micro- and
    macro-expressions when they occur at the same time. Analyzing the macro and micro-expressions
    simultaneously would be helpful to understand people’s intentions and feelings
    in reality more accurately.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大多数ME（微表情）研究假设视频片段中只有MEs。然而，在现实生活中，MEs可能会伴随宏观表情出现。未来的研究应探索基于深度学习的ME检测方法，以在同时出现时检测和区分微表情和宏观表情。同步分析宏观和微观表情有助于更准确地理解人们在现实中的意图和感受。
- en: 8 Ethical considerations
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 伦理考虑
- en: As discussed above, MEs can help reveal people’s hidden feelings in high-stake
    situations and have practical applications in various fields, such as medical
    treatment and interrogations. MER, like many other computer vision and machine
    learning tasks, could be misused, especially when used in surveillance with predatory
    data collection practices [[206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208),
    [209](#bib.bib209)] Therefore, ethical issues should be considered.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，MEs可以帮助揭示人们在高风险情境中的隐藏情感，并在各种领域中具有实际应用，如医疗治疗和审讯。ME识别（MER）像许多其他计算机视觉和机器学习任务一样，可能会被滥用，特别是在具有掠夺性数据收集实践的监控中[[206](#bib.bib206),
    [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209)]。因此，应考虑伦理问题。
- en: Privacy and data protection is the primarily and frequently discussed ethical
    issue in machine learning. For MER, the critical privacy concern is the privacy
    of personal data. Currently, data protection laws are well established to regulate
    data privacy, for example, the EU General Data Protection Regulation (GDPR) [[210](#bib.bib210)].
    The legislation defined rules for the protection of personal data, including international
    data protection agreements, privacy shields, transfer of participant names, record
    data, etc. In the research community, consent forms concerning data collection,
    processing, and sharing need to be signed when collecting ME data. In practical
    applications, consent forms should also be considered to regulate the usage, as
    people’s faces are present in the recorded images/videos with sensitive and biometric
    information that may be misused beyond the intended purpose. Pilot studies aim
    to remove sensitive information like identity while preserving facial properties
    [[211](#bib.bib211)], which could be further explored in MER.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私和数据保护是机器学习中最主要和最常讨论的伦理问题。对于MER而言，关键的隐私问题是个人数据的隐私。目前，数据保护法律已建立完善，例如，欧盟《通用数据保护条例》（GDPR）[[210](#bib.bib210)]。该立法定义了个人数据保护的规则，包括国际数据保护协议、隐私保护措施、参与者姓名的传输、记录数据等。在研究社区中，收集ME数据时需要签署关于数据收集、处理和共享的同意书。在实际应用中，同意书也应考虑到，以规范使用，因为人们的面部出现在记录的图像/视频中，其中包含可能被滥用的敏感和生物特征信息。试点研究旨在在保留面部特征的同时去除身份等敏感信息[[211](#bib.bib211)]，这可以在MER中进一步探索。
- en: Moreover, questions of reliability in MER systems are further pointed out together
    with privacy and data protection [[212](#bib.bib212)]. Results of a deep learning-based
    MER system usually depend on the quality of training data, which are difficult
    to ascertain because of possible data biases. Transparency of data and models
    should be aware and well-studied.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MER系统中的可靠性问题以及隐私和数据保护问题被进一步指出[[212](#bib.bib212)]。基于深度学习的MER系统的结果通常取决于训练数据的质量，而这些数据的质量难以确定，因为可能存在数据偏差。数据和模型的透明度应引起重视并进行充分研究。
- en: 9 Acknowledgement
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 致谢
- en: This work was supported by the Academy of Finland for Academy Professor project
    EmotionAI (grants 336116, 345122), by Ministry of Education and Culture of Finland
    for AI forum project and Infotech Oulu.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了芬兰科学院Academy Professor项目EmotionAI（资助编号336116, 345122）、芬兰教育与文化部AI论坛项目以及Infotech
    Oulu的支持。
- en: References
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] P. Ekman, “Darwin, deception, and facial expression,” *Annals of the New
    York Academy of Sciences*, vol. 1000, no. 1, pp. 205–221, 2003.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] P. Ekman, “达尔文、欺骗与面部表情，”*纽约科学院年刊*，第1000卷，第1期，205–221页，2003年。'
- en: '[2] P. Ekman and W. Friesen, “Constants across cultures in the face and emotion,”
    *Personality and Social Psychology*, vol. 17, no. 2, pp. 124–129, 1971.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] P. Ekman 和 W. Friesen, “面部和情感中的跨文化恒定性，”*个性与社会心理学*，第17卷，第2期，124–129页，1971年。'
- en: '[3] P. Ekman, “Lie catching and microexpressions,” *The philosophy of deception*,
    pp. 118–133, 2009.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] P. Ekman, “识别谎言与微表情，”*欺骗的哲学*，118–133页，2009年。'
- en: '[4] E. A. Haggard and K. S. Isaacs, “Micromomentary facial expressions as indicators
    of ego mechanisms in psychotherapy,” in *Methods of research in psychotherapy*.   Springer,
    1966, pp. 154–165.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] E. A. Haggard 和 K. S. Isaacs, “微瞬面部表情作为心理治疗中自我机制的指标，”收录于*心理治疗研究方法*。Springer，1966年，154–165页。'
- en: '[5] P. Ekman and W. V. Friesen, “Nonverbal leakage and clues to deception,”
    *Psychiatry*, vol. 32, no. 1, pp. 88–106, 1969.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] P. Ekman 和 W. V. Friesen, “非语言泄漏和欺骗线索，”*精神病学*，第32卷，第1期，88–106页，1969年。'
- en: '[6] W. V. Friesen and P. Ekman, “Facial action coding system: a technique for
    the measurement of facial movement,” *Palo Alto*, vol. 3, 1978.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] W. V. Friesen 和 P. Ekman, “面部动作编码系统：一种测量面部运动的技术，”*帕洛阿尔托*，第3卷，1978年。'
- en: '[7] P. Ekman, “Microexpression training tool (METT),” 2002.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] P. Ekman, “微表情训练工具（METT）”，2002年。'
- en: '[8] X. Niu, H. Han, S. Yang, Y. Huang, and S. Shan, “Local relationship learning
    with person-specific shape regularization for facial action unit detection,” in
    *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2019, pp. 11 917–11 926.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] X. Niu, H. Han, S. Yang, Y. Huang, 和 S. Shan, “基于个人特定形状正则化的局部关系学习用于面部动作单位检测，”收录于*IEEE计算机视觉与模式识别会议论文集*，2019年，11 917–11 926页。'
- en: '[9] W. J. Yan, X. Li, S. J. Wang, G. Zhao, Y. J. Liu, Y. H. Chen, and X. Fu,
    “CASME II: an improved spontaneous micro-expression database and the baseline
    evaluation,” *Plos One*, vol. 9, no. 1, p. e86041, 2014.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] W. J. Yan, X. Li, S. J. Wang, G. Zhao, Y. J. Liu, Y. H. Chen, 和 X. Fu，“CASME
    II：改进的自发性微表情数据库及基线评估，” *公共科学图书馆 ONE*，第9卷，第1期，页e86041，2014年。'
- en: '[10] M. Pantic, M. Valstar, R. Rademaker, and L. Maat, “Web-based database
    for facial expression analysis,” in *2005 IEEE international conference on multimedia
    and Expo*.   IEEE, 2005, pp. 5–pp.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. Pantic, M. Valstar, R. Rademaker, 和 L. Maat，“用于面部表情分析的基于网络的数据库，” 见于
    *2005年IEEE国际多媒体与博览会*。 IEEE，2005年，页5–页。'
- en: '[11] W. Merghani, A. K. Davison, and M. H. Yap, “A review on facial micro-expressions
    analysis: datasets, features and metrics,” *arXiv preprint arXiv:1805.02397*,
    2018.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] W. Merghani, A. K. Davison, 和 M. H. Yap，“面部微表情分析综述：数据集、特征和指标，” *arXiv预印本arXiv:1805.02397*，2018年。'
- en: '[12] C. Crivelli and A. J. Fridlund, “Inside-out: From basic emotions theory
    to the behavioral ecology view,” *Journal of Nonverbal Behavior*, vol. 43, no. 2,
    pp. 161–194, 2019.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] C. Crivelli 和 A. J. Fridlund，“从内到外：从基础情感理论到行为生态学视角，” *非言语行为期刊*，第43卷，第2期，页161–194，2019年。'
- en: '[13] P. M. Niedenthal, M. Rychlowska, F. Zhao, and A. Wood, “Historical migration
    patterns shape contemporary cultures of emotion,” *Perspectives on Psychological
    Science*, vol. 14, no. 4, pp. 560–573, 2019.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] P. M. Niedenthal, M. Rychlowska, F. Zhao, 和 A. Wood，“历史迁徙模式塑造当代情感文化，”
    *心理科学视角*，第14卷，第4期，页560–573，2019年。'
- en: '[14] T. Pfister, X. Li, G. Zhao, and M. Pietikäinen, “Differentiating spontaneous
    from posed facial expressions within a generic facial expression recognition framework,”
    in *International Conference Computer Vision*.   Springer, 2011, pp. 1449–1456.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] T. Pfister, X. Li, G. Zhao, 和 M. Pietikäinen，“在通用面部表情识别框架中区分自发和伪造面部表情，”
    见于 *国际计算机视觉会议*。 Springer，2011年，页1449–1456。'
- en: '[15] X. Li, T. Pfister, X. Huang, G. Zhao, and M. Pietikainen, “A spontaneous
    micro-expression database: Inducement, collection and baseline,” in *Proceedings
    of IEEE Conference Workshops on Automatic Face Gesture Recognition*, 2013, pp.
    1–6.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] X. Li, T. Pfister, X. Huang, G. Zhao, 和 M. Pietikainen，“自发微表情数据库：诱发、收集及基线，”
    见于 *IEEE自动面部表情识别与分析会议论文集*，2013年，页1–6。'
- en: '[16] X. Li, T. Pfister, X. Huang, G. Zhao, and M. Pietikäinen, “A spontaneous
    micro-expression database: Inducement, collection and baseline,” in *2013 10th
    IEEE International Conference and Workshops on Automatic Face and Gesture Recognition
    (FG)*.   IEEE, 2013, pp. 1–6.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] X. Li, T. Pfister, X. Huang, G. Zhao, 和 M. Pietikäinen，“自发微表情数据库：诱发、收集及基线，”
    见于 *2013年第10届IEEE国际自动面部与手势识别会议（FG）*。 IEEE，2013年，页1–6。'
- en: '[17] X. Li, X. Hong, A. Moilanen, X. Huang, T. Pfister, G. Zhao, and M. Pietikainen,
    “Towards reading hidden emotions: A comparative study of spontaneous micro-expression
    spotting and recognition methods,” *IEEE Transactions on Affective Computing*,
    vol. 9, no. 4, pp. 563 – 577, 2018.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] X. Li, X. Hong, A. Moilanen, X. Huang, T. Pfister, G. Zhao, 和 M. Pietikainen，“探索隐藏情感：自发微表情检测与识别方法的比较研究，”
    *IEEE情感计算学报*，第9卷，第4期，页563–577，2018年。'
- en: '[18] J. Wei, G. Lu, and J. Yan, “A comparative study on movement feature in
    different directions for micro-expression recognition,” *Neurocomputing*, vol.
    449, pp. 159–171, 2021.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Wei, G. Lu, 和 J. Yan，“微表情识别中不同方向运动特征的比较研究，” *神经计算*，第449卷，页159–171，2021年。'
- en: '[19] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietikäinen,
    “Deep learning for generic object detection: A survey,” *International Journal
    of Computer Vision*, vol. 128, no. 2, pp. 261–318, 2020.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, 和 M. Pietikäinen，“通用物体检测的深度学习：综述，”
    *国际计算机视觉期刊*，第128卷，第2期，页261–318，2020年。'
- en: '[20] A. Brunetti, D. Buongiorno, G. F. Trotta, and V. Bevilacqua, “Computer
    vision and deep learning techniques for pedestrian detection and tracking: A survey,”
    *Neurocomputing*, vol. 300, pp. 17–33, 2018.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Brunetti, D. Buongiorno, G. F. Trotta, 和 V. Bevilacqua，“计算机视觉和深度学习技术在行人检测和跟踪中的应用：综述，”
    *神经计算*，第300卷，页17–33，2018年。'
- en: '[21] Y. Liu, F. Yang, C. Zhong, Y. Tao, B. Dai, and M. Yin, “Visual tracking
    via salient feature extraction and sparse collaborative model,” *AEU-International
    Journal of Electronics and Communications*, vol. 87, pp. 134–143, 2018.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. Liu, F. Yang, C. Zhong, Y. Tao, B. Dai, 和 M. Yin，“通过显著特征提取和稀疏协同模型进行视觉跟踪，”
    *AEU-国际电子与通信期刊*，第87卷，页134–143，2018年。'
- en: '[22] Z. Li, Y. Li, Y. Gao, and Y. Liu, “Fast cross-scenario clothing retrieval
    based on indexing deep features,” in *Pacific Rim Conference on Multimedia*.   Springer,
    2016, pp. 107–118.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Z. Li, Y. Li, Y. Gao 和 Y. Liu，“基于索引深度特征的快速跨场景服装检索，” 见 *太平洋地区多媒体会议*。 Springer，2016年，页码107–118。'
- en: '[23] Z. Li, W. Tian, Y. Li, Z. Kuang, and Y. Liu, “A more effective method
    for image representation: Topic model based on latent dirichlet allocation,” in
    *2015 14th International Conference on Computer-Aided Design and Computer Graphics
    (CAD/Graphics)*.   IEEE, 2015, pp. 143–148.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Z. Li, W. Tian, Y. Li, Z. Kuang 和 Y. Liu，“一种更有效的图像表示方法：基于潜在狄利克雷分配的主题模型，”
    见 *2015年第14届国际计算机辅助设计与计算机图形学会议（CAD/Graphics）*。 IEEE，2015年，页码143–148。'
- en: '[24] S. Li and W. Deng, “Deep facial expression recognition: A survey,” *IEEE
    Transactions on Affective Computing*, 2020.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Li 和 W. Deng，“深度面部表情识别：综述，” *IEEE情感计算汇刊*，2020年。'
- en: '[25] Y. Liu, X. Zhang, J. Zhou, and L. Fu, “Sg-dsn: A semantic graph-based
    dual-stream network for facial expression recognition,” *Neurocomputing*, vol.
    462, pp. 320–330, 2021.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Liu, X. Zhang, J. Zhou 和 L. Fu，“Sg-dsn：一种基于语义图的双流网络用于面部表情识别，” *Neurocomputing*，第462卷，页码320–330，2021年。'
- en: '[26] Y. Liu, X. Zhang, J. Kauttonen, and G. Zhao, “Uncertain label correction
    via auxiliary action unit graphs for facial expression recognition,” *arXiv preprint
    arXiv:2204.11053*, 2022.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Y. Liu, X. Zhang, J. Kauttonen 和 G. Zhao，“通过辅助动作单元图进行不确定标签校正，用于面部表情识别，”
    *arXiv预印本 arXiv:2204.11053*，2022年。'
- en: '[27] J. Yu, C. Zhang, Y. Song, and W. Cai, “ICE-GAN: Identity-aware and capsule-enhanced
    gan for micro-expression recognition and synthesis,” *arXiv preprint arXiv:2005.04370*,
    2020.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] J. Yu, C. Zhang, Y. Song 和 W. Cai，“ICE-GAN：具有身份感知和胶囊增强的生成对抗网络用于微表情识别和合成，”
    *arXiv预印本 arXiv:2005.04370*，2020年。'
- en: '[28] H.-X. Xie, L. Lo, H.-H. Shuai, and W.-H. Cheng, “An overview of facial
    micro-expression analysis: Data, methodology and challenge,” *arXiv preprint arXiv:2012.11307*,
    2020.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] H.-X. Xie, L. Lo, H.-H. Shuai 和 W.-H. Cheng，“面部微表情分析概述：数据、方法论和挑战，” *arXiv预印本
    arXiv:2012.11307*，2020年。'
- en: '[29] K. M. Goh, C. H. Ng, L. L. Lim, and U. U. Sheikh, “Micro-expression recognition:
    an updated review of current trends, challenges and solutions,” *The Visual Computer*,
    vol. 36, no. 3, pp. 445–468, 2020.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] K. M. Goh, C. H. Ng, L. L. Lim 和 U. U. Sheikh，“微表情识别：对当前趋势、挑战和解决方案的更新综述，”
    *The Visual Computer*，第36卷，第3期，页码445–468，2020年。'
- en: '[30] M. Takalkar, M. Xu, Q. Wu, and Z. Chaczko, “A survey: facial micro-expression
    recognition,” *Multimedia Tools and Applications*, vol. 77, no. 15, pp. 19 301–19 325,
    2018.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] M. Takalkar, M. Xu, Q. Wu 和 Z. Chaczko，“综述：面部微表情识别，” *Multimedia Tools
    and Applications*，第77卷，第15期，页码19 301–19 325，2018年。'
- en: '[31] Y.-H. Oh, J. See, A. C. Le Ngo, R. C.-W. Phan, and V. M. Baskaran, “A
    survey of automatic facial micro-expression analysis: databases, methods, and
    challenges,” *Frontiers in psychology*, vol. 9, p. 1128, 2018.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Y.-H. Oh, J. See, A. C. Le Ngo, R. C.-W. Phan 和 V. M. Baskaran，“自动面部微表情分析的综述：数据库、方法和挑战，”
    *Frontiers in Psychology*，第9卷，页码1128，2018年。'
- en: '[32] X. Ben, Y. Ren, J. Zhang, S.-J. Wang, K. Kpalma, W. Meng, and Y.-J. Liu,
    “Video-based facial micro-expression analysis: A survey of datasets, features
    and algorithms,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2021.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] X. Ben, Y. Ren, J. Zhang, S.-J. Wang, K. Kpalma, W. Meng 和 Y.-J. Liu，“基于视频的面部微表情分析：数据集、特征和算法综述，”
    *IEEE模式分析与机器智能汇刊*，2021年。'
- en: '[33] L. Zhou, X. Shao, and Q. Mao, “A survey of micro-expression recognition,”
    *Image and Vision Computing*, vol. 105, p. 104043, 2021.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] L. Zhou, X. Shao 和 Q. Mao，“微表情识别综述，” *图像与视觉计算*，第105卷，页码104043，2021年。'
- en: '[34] W. Yan, Q. Wu, Y. Liu, S. Wang, and X. Fu, “CASME database: A dataset
    of spontaneous micro-expressions collected from neutralized faces,” in *Proceedings
    of IEEE Conference Automatic Face Gesture Recognition*, 2013, pp. 1–7.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] W. Yan, Q. Wu, Y. Liu, S. Wang 和 X. Fu，“CASME数据库：从中性面孔中收集的自发微表情数据集，” 见
    *IEEE自动面部与手势识别会议论文集*，2013年，页码1–7。'
- en: '[35] F. Qu, S.-J. Wang, W.-J. Yan, H. Li, S. Wu, and X. Fu, “Cas (me) ²: A
    database for spontaneous macro-expression and micro-expression spotting and recognition,”
    *IEEE Transactions on Affective Computing*, vol. 9, no. 4, pp. 424–436, 2017.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] F. Qu, S.-J. Wang, W.-J. Yan, H. Li, S. Wu 和 X. Fu，“Cas (me)²：用于自发宏表情和微表情检测与识别的数据库，”
    *IEEE情感计算汇刊*，第9卷，第4期，页码424–436，2017年。'
- en: '[36] A. K. Davison, C. Lansley, N. Costen, K. Tan, and M. H. Yap, “SAMM: A
    spontaneous micro-facial movement dataset,” *IEEE Transactions on Affective Computing*,
    vol. 9, no. 1, pp. 116–129, 2018.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] A. K. Davison, C. Lansley, N. Costen, K. Tan 和 M. H. Yap，“SAMM：一个自发微面部运动数据集，”
    *IEEE情感计算汇刊*，第9卷，第1期，页码116–129，2018年。'
- en: '[37] P. Husák, J. Cech, and J. Matas, “Spotting facial micro-expressions “in
    the wild”,” in *22nd Computer Vision Winter Workshop (RETZ)*, 2017.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] P. 胡萨克, J. 切赫, 和 J. 马塔斯, “在‘野外’检测面部微表情，” 见于 *第22届计算机视觉冬季研讨会（RETZ）*, 2017年。'
- en: '[38] J. See, Y. Hoon, J. Li, X. Hong, and S. Wang, “MEGC 2019–the second facial
    micro-expressions grand challenge,” in *2019 14th IEEE International Conference
    on Automatic Face and Gesture Recognition (FG)*.   IEEE, 2019, pp. 1–5.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] J. 斯, Y. 训, J. 李, X. 宏, 和 S. 王, “MEGC 2019——第二届面部微表情大挑战，” 见于 *2019年第14届IEEE国际自动面部与姿态识别会议（FG）*。
    IEEE, 2019年，第1–5页。'
- en: '[39] Y. Zhao and J. Xu, “A convolutional neural network for compound micro-expression
    recognition,” *Sensors*, vol. 19, no. 24, p. 5553, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Y. 赵 和 J. 许, “用于复合微表情识别的卷积神经网络，” *传感器*, 第19卷，第24期，第5553页，2019年。'
- en: '[40] A. Vinciarelli, A. Dielmann, S. Favre, and H. Salamin, “Canal9: A database
    of political debates for analysis of social interactions,” in *2009 3rd International
    Conference on Affective Computing and Intelligent Interaction and Workshops*.   IEEE,
    2009, pp. 1–4.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. 温恰雷利, A. 迪尔曼, S. 法弗, 和 H. 萨拉敏, “Canal9: 一个用于分析社会互动的政治辩论数据库，” 见于 *2009年第3届国际情感计算与智能交互会议及研讨会*。
    IEEE, 2009年，第1–4页。'
- en: '[41] S. Polikovsky, Y. Kameda, and Y. Ohta, “Facial micro-expressions recognition
    using high speed camera and 3D-gradient descriptor,” 2009.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. 波利科夫斯基, Y. 亀田, 和 Y. 大田, “使用高速相机和3D梯度描述符的面部微表情识别，” 2009年。'
- en: '[42] M. Shreve, S. Godavarthy, D. Goldgof, and S. Sarkar, “Macro- and micro-expression
    spotting in long videos using spatio-temporal strain,” in *Proceedings of IEEE
    Conference Workshops on Automatic Face and Gesture Recognition*, 2011, pp. 51–56.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. 谢夫, S. 戈达瓦尔提, D. 戈尔德戈夫, 和 S. 萨卡尔, “利用时空应变在长视频中检测宏观和微观表情，” 见于 *IEEE自动面部与姿态识别会议论文集*,
    2011年，第51–56页。'
- en: '[43] M. Zhang, Q. Fu, Y.-H. Chen, and X. Fu, “Emotional context modulates micro-expression
    processing as reflected in event-related potentials,” *PsyCh journal*, vol. 7,
    no. 1, pp. 13–24, 2018.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] M. 张, Q. 富, Y.-H. 陈, 和 X. 富, “情感背景调节微表情处理，反映在事件相关电位中，” *心理学期刊*, 第7卷，第1期，第13–24页，2018年。'
- en: '[44] Y. Zhao and J. Xu, “Compound micro-expression recognition system,” in
    *2020 International Conference on Intelligent Transportation, Big Data and Smart
    City (ICITBS)*.   IEEE, 2020, pp. 728–733.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. 赵 和 J. 许, “复合微表情识别系统，” 见于 *2020国际智能交通、大数据与智慧城市会议（ICITBS）*。 IEEE, 2020年，第728–733页。'
- en: '[45] P. Viola, M. Jones *et al.*, “Robust real-time object detection,” *International
    journal of computer vision*, vol. 4, no. 34-47, p. 4, 2001.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] P. 维奥拉, M. 琼斯 *等*, “鲁棒实时目标检测，” *计算机视觉国际期刊*, 第4卷，第34-47期，第4页，2001年。'
- en: '[46] M. Matsugu, K. Mori, Y. Mitari, and Y. Kaneda, “Subject independent facial
    expression recognition with robust face detection using a convolutional neural
    network,” *Neural Networks*, vol. 16, no. 5-6, pp. 555–559, 2003.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] M. 松久, K. 森, Y. 美田里, 和 Y. 兼田, “使用卷积神经网络的主观无关面部表情识别与鲁棒人脸检测，” *神经网络*, 第16卷，第5-6期，第555–559页，2003年。'
- en: '[47] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham, “Active shape
    models-their training and application,” *Computer vision and image understanding*,
    vol. 61, no. 1, pp. 38–59, 1995.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] T. F. 库特斯, C. J. 泰勒, D. H. 库珀, 和 J. 格雷厄姆, “主动形状模型——它们的训练和应用，” *计算机视觉与图像理解*,
    第61卷，第1期，第38–59页，1995年。'
- en: '[48] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active appearance models,”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 23, no. 6,
    pp. 681–685, 2001.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] T. F. 库特斯, G. J. 爱德华兹, 和 C. J. 泰勒, “主动外观模型，” *IEEE模式分析与机器智能汇刊*, 第23卷，第6期，第681–685页，2001年。'
- en: '[49] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection and alignment
    using multitask cascaded convolutional networks,” *IEEE Signal Processing Letters*,
    vol. 23, no. 10, pp. 1499–1503, 2016.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] K. 张, Z. 张, Z. 李, 和 Y. 乔, “使用多任务级联卷积网络的联合人脸检测和对齐，” *IEEE信号处理快报*, 第23卷，第10期，第1499–1503页，2016年。'
- en: '[50] Z. Zhou, G. Zhao, and M. Pietikäinen, “Towards a practical lipreading
    system,” in *CVPR 2011*.   IEEE, 2011, pp. 137–144.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Z. 周, G. 赵, 和 M. 皮耶蒂凯宁, “迈向实用的唇读系统，” 见于 *CVPR 2011*。 IEEE, 2011年，第137–144页。'
- en: '[51] S. Niklaus and F. Liu, “Context-aware synthesis for video frame interpolation,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2018, pp. 1701–1710.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] S. 尼克劳斯 和 F. 刘, “用于视频帧插值的上下文感知合成，” 见于 *IEEE计算机视觉与模式识别会议论文集*, 2018年，第1701–1710页。'
- en: '[52] H. Wu, E. Shih, E. Shih, J. Guttag, and W. Freeman, “Eulerian video magnification
    for revealing subtle changes in the world,” *ACM Transactions on Graphics*, vol. 31,
    no. 4, pp. 1–8, 2012.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] H. Wu, E. Shih, E. Shih, J. Guttag 和 W. Freeman，“欧拉视频放大用于揭示世界中的微小变化，”
    *ACM图形学交易*，第31卷，第4期，页码 1–8，2012年。'
- en: '[53] T.-H. Oh, R. Jaroensri, C. Kim, M. Elgharib, F. Durand, W. T. Freeman,
    and W. Matusik, “Learning-based video motion magnification,” in *Proceedings of
    the European Conference on Computer Vision (ECCV)*, 2018, pp. 633–648.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] T.-H. Oh, R. Jaroensri, C. Kim, M. Elgharib, F. Durand, W. T. Freeman
    和 W. Matusik，“基于学习的视频运动放大，” 在 *欧洲计算机视觉会议（ECCV）论文集*，2018年，页码 633–648。'
- en: '[54] G. Zhao and M. Pietikainen, “Dynamic texture recognition using local binary
    patterns with an application to facial expressions,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol. 29, no. 6, pp. 915–928, 2007.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] G. Zhao 和 M. Pietikainen，“使用局部二进制模式的动态纹理识别及其在面部表情中的应用，” *IEEE 模式分析与机器智能交易*，第29卷，第6期，页码
    915–928，2007年。'
- en: '[55] A. Davison, W. Merghani, C. Lansley, C.-C. Ng, and M. H. Yap, “Objective
    micro-facial movement detection using facs-based regions and baseline evaluation,”
    in *2018 13th IEEE international conference on automatic face and gesture recognition
    (FG)*.   IEEE, 2018, pp. 642–649.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] A. Davison, W. Merghani, C. Lansley, C.-C. Ng 和 M. H. Yap，“基于FACS的区域和基线评估的客观微面部运动检测，”
    在 *2018年第13届IEEE国际自动面部和手势识别会议（FG）*。 IEEE，2018年，页码 642–649。'
- en: '[56] W. Merghani and M. H. Yap, “Adaptive mask for region-based facial micro-expression
    recognition,” in *2020 15th IEEE International Conference on Automatic Face and
    Gesture Recognition (FG)*.   IEEE Computer Society, 2020, pp. 428–433.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] W. Merghani 和 M. H. Yap，“用于区域基础面部微表情识别的自适应掩模，” 在 *2020年第15届IEEE国际自动面部和手势识别会议（FG）*。
    IEEE计算机学会，2020年，页码 428–433。'
- en: '[57] M. Shreve, J. Brizzi, S. Fefilatyev, T. Luguev, D. Goldgof, and S. Sarkar,
    “Automatic expression spotting in videos,” *Image and Vision Computing*, vol. 32,
    no. 8, pp. 476–486, 2014.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] M. Shreve, J. Brizzi, S. Fefilatyev, T. Luguev, D. Goldgof 和 S. Sarkar，“视频中的自动表情检测，”
    *图像与视觉计算*，第32卷，第8期，页码 476–486，2014年。'
- en: '[58] H.-X. Xie, L. Lo, H.-H. Shuai, and W.-H. Cheng, “AU-assisted graph attention
    convolutional network for micro-expression recognition,” in *Proceedings of the
    28th ACM International Conference on Multimedia*, 2020, pp. 2871–2880.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] H.-X. Xie, L. Lo, H.-H. Shuai 和 W.-H. Cheng，“基于AU的图卷积网络用于微表情识别，” 在 *第28届ACM国际多媒体会议论文集*，2020年，页码
    2871–2880。'
- en: '[59] Y. Li, X. Huang, and G. Zhao, “Joint local and global information learning
    with single apex frame detection for micro-expression recognition,” *IEEE Transactions
    on Image Processing*, vol. 30, pp. 249–263, 2020.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Y. Li, X. Huang 和 G. Zhao，“基于单个顶点帧检测的局部和全局信息联合学习用于微表情识别，” *IEEE 图像处理交易*，第30卷，页码
    249–263，2020年。'
- en: '[60] Z. Xia, X. Hong, X. Gao, X. Feng, and G. Zhao, “Spatiotemporal recurrent
    convolutional networks for recognizing spontaneous micro-expressions,” *IEEE Transaction
    on Multimedia*, 2019.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Z. Xia, X. Hong, X. Gao, X. Feng 和 G. Zhao，“用于识别自发性微表情的时空递归卷积网络，” *IEEE
    多媒体交易*，2019年。'
- en: '[61] Y. Li, X. Huang, and G. Zhao, “Micro-expression action unit detection
    with spatial and channel attention,” *Neurocomputing*, 2021.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Y. Li, X. Huang 和 G. Zhao，“具有空间和通道注意力的微表情动作单位检测，” *神经计算*，2021年。'
- en: '[62] D. Patel, G. Zhao, and M. Pietikäinen, “Spatiotemporal integration of
    optical flow vectors for micro-expression detection,” in *International Conference
    Advanced Concepts Intelligence Vision System*.   Springer, 2015, pp. 369–380.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] D. Patel, G. Zhao 和 M. Pietikäinen，“用于微表情检测的光流向量时空集成，” 在 *国际高级概念智能视觉系统会议*。
     Springer，2015年，页码 369–380。'
- en: '[63] Y. Han, B. Li, Y.-K. Lai, and Y.-J. Liu, “CFD: A collaborative feature
    difference method for spontaneous micro-expression spotting,” in *2018 25th IEEE
    International Conference on Image Processing (ICIP)*.   IEEE, 2018, pp. 1942–1946.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Y. Han, B. Li, Y.-K. Lai 和 Y.-J. Liu，“CFD：一种用于自发性微表情识别的协作特征差异方法，” 在 *2018年第25届IEEE国际图像处理会议（ICIP）*。
    IEEE，2018年，页码 1942–1946。'
- en: '[64] Y. Li, X. Huang, and G. Zhao, “Can micro-expression be recognized based
    on single apex frame?” in *IEEE International Conference Image Processing*.   IEEE,
    2018, pp. 3094–3098.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Y. Li, X. Huang 和 G. Zhao，“微表情可以基于单个顶点帧被识别吗？” 在 *IEEE国际图像处理会议*。 IEEE，2018年，页码
    3094–3098。'
- en: '[65] Z. Zhang, T. Chen, H. Meng, G. Liu, and X. Fu, “Smeconvnet: A convolutional
    neural network for spotting spontaneous facial micro-expression from long videos,”
    *IEEE Access*, vol. 6, pp. 71 143–71 151, 2018.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Z. Zhang, T. Chen, H. Meng, G. Liu 和 X. Fu，“Smeconvnet: 用于在长视频中捕捉自发面部微表情的卷积神经网络。”*IEEE
    Access*，第6卷，页码71 143–71 151，2018年。'
- en: '[66] T.-K. Tran, Q.-N. Vo, X. Hong, X. Li, and G. Zhao, “Micro-expression spotting:
    A new benchmark,” *Neurocomputing*, vol. 443, pp. 356–368, 2021.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] T.-K. Tran, Q.-N. Vo, X. Hong, X. Li 和 G. Zhao，“微表情捕捉：一个新的基准。”*神经计算*，第443卷，页码356–368，2021年。'
- en: '[67] H. Pan, L. Xie, and Z. Wang, “Local bilinear convolutional neural network
    for spotting macro-and micro-expression intervals in long video sequences,” in
    *2020 15th IEEE International Conference on Automatic Face and Gesture Recognition
    (FG)*.   IEEE Computer Society, 2020, pp. 343–347.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] H. Pan, L. Xie 和 Z. Wang，“用于在长视频序列中捕捉大/微表情间隔的本地双线性卷积神经网络。”出自*2020年第15届IEEE国际自动脸部与手势识别会议
    (FG)*。IEEE计算机学会，2020年，页码343–347。'
- en: '[68] N. V. Quang, J. Chun, and T. Tokuyama, “Capsulenet for micro-expression
    recognition,” in *2019 14th IEEE International Conference on Automatic Face and
    Gesture Recognition (FG)*.   IEEE, 2019, pp. 1–7.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] N. V. Quang, J. Chun 和 T. Tokuyama，“Capsulenet用于微表情识别。”出自*2019年第14届IEEE国际自动脸部与手势识别会议
    (FG)*。IEEE，2019年，页码1–7。'
- en: '[69] B. Sun, S. Cao, D. Li, J. He, and L. Yu, “Dynamic micro-expression recognition
    using knowledge distillation,” *IEEE Transactions on Affective Computing*, 2020.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] B. Sun, S. Cao, D. Li, J. He 和 L. Yu，“使用知识蒸馏的动态微表情识别。”*IEEE情感计算交易*，2020年。'
- en: '[70] R. Zhi, H. Xu, M. Wan, and T. Li, “Combining 3D convolutional neural networks
    with transfer learning by supervised pre-training for facial micro-expression
    recognition,” *IEICE Transactions on Information and Systems*, vol. 102, no. 5,
    pp. 1054–1064, 2019.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] R. Zhi, H. Xu, M. Wan 和 T. Li，“结合3D卷积神经网络和监督预训练的迁移学习用于面部微表情识别。”*IEICE信息与系统交易*，第102卷，第5期，页码1054–1064，2019年。'
- en: '[71] D. Kim, W. J. Baddar, and Y. M. Ro, “Micro-expression recognition with
    expression-state constrained spatio-temporal feature representations,” in *Proceedings
    of the 24th ACM International Conference on Multimedia*, 2016, pp. 382–386.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] D. Kim, W. J. Baddar 和 Y. M. Ro, “基于表情状态约束时空特征表示的微表情识别。”出自*第24届ACM国际多媒体会议论文集*，2016年，页码382–386。'
- en: '[72] J. Li, Y. Wang, J. See, and W. Liu, “Micro-expression recognition based
    on 3D flow convolutional neural network,” *Pattern Analysis and Applications*,
    pp. 1–9, 2018.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] J. Li, Y. Wang, J. See 和 W. Liu，“基于3D流卷积神经网络的微表情识别。”*模式分析与应用*，页码1–9，2018年。'
- en: '[73] S.-T. Liong and K. Wong, “Micro-expression recognition using apex frame
    with phase information,” in *2017 Asia-Pacific Signal and Information Processing
    Association Annual Summit and Conference (APSIPA ASC)*.   IEEE, 2017, pp. 534–537.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] S.-T. Liong 和 K. Wong，“利用具有相位信息的顶点帧进行微表情识别。”出自*2017年亚太信号和信息处理协会年度峰会暨会议*。IEEE，2017年，页码534–537。'
- en: '[74] J. Liu, W. Zheng, and Y. Zong, “SMA-STN: Segmented movement-attending
    spatiotemporal network for micro-expression recognition,” *arXiv preprint arXiv:2010.09342*,
    2020.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. Liu, W. Zheng 和 Y. Zong，“SMA-STN: 分段运动关注时空网络用于微表情识别。”*arXiv预印本 arXiv:2010.09342*，2020年。'
- en: '[75] A. J. R. Kumar and B. Bhanu, “Micro-expression classification based on
    landmark relations with graph attention convolutional network,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops*,
    June 2021, pp. 1511–1520.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] A. J. R. Kumar 和 B. Bhanu, “基于图注意力卷积网络的微表情分类。”出自*IEEE/CVF计算机视觉与模式识别（CVPR）研讨会论文集*，2021年6月，页码1511–1520。'
- en: '[76] X. Nie, M. A. Takalkar, M. Duan, H. Zhang, and M. Xu, “GEME: Dual-stream
    multi-task gender-based micro-expression recognition,” *Neurocomputing*, vol.
    427, pp. 13–28, 2021.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] X. Nie, M. A. Takalkar, M. Duan, H. Zhang 和 M. Xu，“GEME: 基于双流多任务性别的微表情识别。”*神经计算*，第427卷，页码13–28，2021年。'
- en: '[77] M. Verma, S. K. Vipparthi, G. Singh, and S. Murala, “Learnet: Dynamic
    imaging network for micro expression recognition,” *IEEE Transactions on Image
    Processing*, vol. 29, pp. 1618–1627, 2019.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] M. Verma, S. K. Vipparthi, G. Singh 和 S. Murala，“Learnet: 用于微表情识别的动态成像网络。”*IEEE图像处理交易*，第29卷，页码1618–1627，2019年。'
- en: '[78] M. Verma, S. K. Vipparthi, and G. Singh, “Non-linearities improve originet
    based on active imaging for micro expression recognition,” in *2020 International
    Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2020, pp. 1–8.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Verma, S. K. Vipparthi 和 G. Singh，“非线性改进的用于微表情识别的流网络。”出自*2020年国际神经网络联合会议
    (IJCNN)*。IEEE，2020年，页码1–8。'
- en: '[79] B. D. Lucas, “Generalized image matching by the method of differences.”
    1986.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] B. D. Lucas，“通过差异法进行广义图像匹配。”1986年。'
- en: '[80] G. Farnebäck, “Two-frame motion estimation based on polynomial expansion,”
    in *Scandinavian conference on Image analysis*.   Springer, 2003, pp. 363–370.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] G. Farnebäck，“基于多项式展开的双帧运动估计”，收录于*北欧图像分析会议*。Springer，2003年，第363-370页。'
- en: '[81] L. Zhou, Q. Mao, and L. Xue, “Dual-inception network for cross-database
    micro-expression recognition,” in *2019 14th IEEE International Conference on
    Automatic Face and Gesture Recognition (FG)*.   IEEE, 2019, pp. 1–5.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] L. Zhou，Q. Mao，和L. Xue，“跨数据库微表情识别的双重拓扑网络”，收录于*2019年第14届IEEE国际自动人脸与动作识别会议（FG）*，IEEE，2019年，第1-5页。'
- en: '[82] D. Alexey, F. Philipp, I. H. Philip, H. Caner, G. Vladimir, V. Patrick,
    C. Daniel, and B. Thomas, “Flownet: Learning optical flow with convolutional networks,”
    in *Proceedings of IEEE International Conference of Computing Vision*, 2015, pp.
    2758–2766.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] D. Alexey，F. Philipp，I. H. Philip，H. Caner，G. Vladimir，V. Patrick，C. Daniel，和B. Thomas，“Flownet：使用卷积网络学习光流”，收录于*
    IEEE国际计算机视觉大会论文集*，2015年，第2758-2766页。'
- en: '[83] N. Liu, X. Liu, Z. Zhang, X. Xu, and T. Chen, “Offset or onset frame:
    A multi-stream convolutional neural network with capsulenet module for micro-expression
    recognition,” in *2020 5th International Conference on Intelligent Informatics
    and Biomedical Sciences (ICIIBMS)*.   IEEE, 2020, pp. 236–240.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] N. Liu，X. Liu，Z. Zhang，X. Xu，和T. Chen，“偏移或起始帧：一种用于微表情识别的多流卷积神经网络与胶囊网络模块”，收录于*2020年第5届智能信息学与生物医学科学国际会议（ICIIBMS）*，IEEE，2020年，第236-240页。'
- en: '[84] B. Sun, S. Cao, J. He, and L. Yu, “Two-stream attention-aware network
    for spontaneous micro-expression movement spotting,” in *2019 IEEE 10th International
    Conference on Software Engineering and Service Science (ICSESS)*.   IEEE, 2019,
    pp. 702–705.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] B. Sun，S. Cao，J. He，和L. Yu，“用于自发微表情运动检测的双流注意力网络”，收录于*2019年IEEE第10届软件工程和服务科学国际会议（ICSESS）*，IEEE，2019年，第702-705页。'
- en: '[85] A. J. R. Kumar and B. Bhanu, “Micro-expression classification based on
    landmark relations with graph attention convolutional network,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 1511–1520.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] A. J. R. Kumar和B. Bhanu，“基于图注意力卷积网络的关键点关系的微表情分类”，收录于* IEEE/CVF计算机视觉和模式识别会议论文集*，2021年，第1511-1520页。'
- en: '[86] J. Liu, K. Li, B. Song, and L. Zhao, “A multi-stream convolutional neural
    network for micro-expression recognition using optical flow and EVM,” *arXiv preprint
    arXiv:2011.03756*, 2020.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. Liu，K. Li，B. Song，和L. Zhao，“一种使用光流和EVM的多流卷积神经网络进行微表情识别的方法”，*arXiv预印本arXiv:2011.03756*，2020年。'
- en: '[87] B. Chen, Z. Zhang, N. Liu, Y. Tan, X. Liu, and T. Chen, “Spatiotemporal
    convolutional neural network with convolutional block attention module for micro-expression
    recognition,” *Information*, vol. 11, no. 8, p. 380, 2020.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] B. Chen，Z. Zhang，N. Liu，Y. Tan，X. Liu，和T. Chen，“应用卷积块注意模块的时空卷积神经网络进行微表情识别”，*Information*，卷11，期8，第380页，2020年。'
- en: '[88] V. R. Gajjala, S. P. T. Reddy, S. Mukherjee, and S. R. Dubey, “MERANet:
    Facial micro-expression recognition using 3D residual attention network,” *arXiv
    preprint arXiv:2012.04581*, 2020.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] V. R. Gajjala，S. P. T. Reddy，S. Mukherjee，和S. R. Dubey，“使用3D残差注意力网络的面部微表情识别（MERANet）”，*arXiv预印本arXiv:2012.04581*，2020年。'
- en: '[89] L. Lei, J. Li, T. Chen, and S. Li, “A novel Graph-TCN with a graph structured
    representation for micro-expression recognition,” in *Proceedings of the 28th
    ACM International Conference on Multimedia*, 2020, pp. 2237–2245.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] L. Lei，J. Li，T. Chen，和S. Li，“一种带有图结构表示的微表情识别的新型Graph-TCN”，收录于*第28届ACM国际多媒体会议论文集*，2020年，第2237-2245页。'
- en: '[90] L. Lo, H.-X. Xie, H.-H. Shuai, and W.-H. Cheng, “MER-GCN: Micro-expression
    recognition based on relation modeling with graph convolutional networks,” in
    *2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)*.   IEEE,
    2020, pp. 79–84.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] L. Lo，H.-X. Xie，H.-H. Shuai，和W.-H. Cheng，“基于图卷积网络的关系建模的微表情识别（MER-GCN）”，收录于*2020年IEEE多媒体信息处理和检索会议（MIPR）*，IEEE，2020年，第79-84页。'
- en: '[91] L. Zhou, Q. Mao, and M. Dong, “Objective class-based micro-expression
    recognition through simultaneous action unit detection and feature aggregation,”
    *arXiv preprint arXiv:2012.13148*, 2020.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] L. Zhou，Q. Mao，和M. Dong，“通过同时动作单元检测和特征聚合进行客观类基微表情识别”，*arXiv预印本arXiv:2012.13148*，2020年。'
- en: '[92] L. Lei, T. Chen, S. Li, and J. Li, “Micro-expression recognition based
    on facial graph representation learning and facial action unit fusion,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops*,
    June 2021, pp. 1571–1580.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] L. Lei, T. Chen, S. Li, 和 J. Li，“基于面部图表示学习和面部动作单元融合的微表情识别”，发表于*IEEE/CVF计算机视觉与模式识别会议（CVPR）研讨会论文集*，2021年6月，页码
    1571–1580。'
- en: '[93] T. T. Q. Le, T.-K. Tran, and M. Rege, “Dynamic image for micro-expression
    recognition on region-based framework,” in *2020 IEEE 21st International Conference
    on Information Reuse and Integration for Data Science (IRI)*.   IEEE, 2020, pp.
    75–81.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] T. T. Q. Le, T.-K. Tran, 和 M. Rege，“基于区域框架的微表情识别动态图像”，发表于*2020 IEEE第21届信息重用与整合数据科学国际会议（IRI）*。
    IEEE, 2020, 页码 75–81。'
- en: '[94] Y. Wang, H. Ma, X. Xing, and Z. Pan, “Eulerian motion based 3Dcnn architecture
    for facial micro-expression recognition,” in *International Conference on Multimedia
    Modeling*.   Springer, 2020, pp. 266–277.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Y. Wang, H. Ma, X. Xing, 和 Z. Pan，“基于欧拉运动的 3Dcnn 架构用于面部微表情识别”，发表于*多媒体建模国际会议*。
    Springer, 2020, 页码 266–277。'
- en: '[95] Y. Gan, S. Liong, W. Yau, Y. Huang, and L. Tan, “Off-apexnet on micro-expression
    recognition system,” *Signal Processing: Image Communication*, vol. 74, pp. 129–139,
    2019.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Y. Gan, S. Liong, W. Yau, Y. Huang, 和 L. Tan，“Off-apexnet 在微表情识别系统中的应用”，*信号处理：图像通信*，第74卷，页码
    129–139, 2019。'
- en: '[96] H.-Q. Khor, J. See, S.-T. Liong, R. C. Phan, and W. Lin, “Dual-stream
    shallow networks for facial micro-expression recognition,” in *2019 IEEE International
    Conference on Image Processing (ICIP)*.   IEEE, 2019, pp. 36–40.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] H.-Q. Khor, J. See, S.-T. Liong, R. C. Phan, 和 W. Lin，“用于面部微表情识别的双流浅层网络”，发表于*2019
    IEEE国际图像处理会议（ICIP）*。 IEEE, 2019, 页码 36–40。'
- en: '[97] H. Yan and L. Li, “Micro-expression recognition using enriched two stream
    3D convolutional network,” in *Proceedings of the 4th International Conference
    on Computer Science and Application Engineering*, 2020, pp. 1–5.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] H. Yan 和 L. Li，“使用丰富的双流 3D 卷积网络进行微表情识别”，发表于*第4届计算机科学与应用工程国际会议论文集*，2020,
    页码 1–5。'
- en: '[98] K. Li, Y. Zong, B. Song, J. Zhu, J. Shi, W. Zheng, and L. Zhao, “Three-stream
    convolutional neural network for micro-expression recognition,” *Australian Journal
    of Intelligent Information Processing System*, vol. 15, no. 3, pp. 41–48, 2019.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] K. Li, Y. Zong, B. Song, J. Zhu, J. Shi, W. Zheng, 和 L. Zhao，“用于微表情识别的三流卷积神经网络”，*澳大利亚智能信息处理系统期刊*，第15卷，第3期，页码
    41–48, 2019。'
- en: '[99] B. Song, K. Li, Y. Zong, J. Zhu, W. Zheng, J. Shi, and L. Zhao, “Recognizing
    spontaneous micro-expression using a three-stream convolutional neural network,”
    *IEEE Access*, vol. 7, pp. 184 537–184 551, 2019.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] B. Song, K. Li, Y. Zong, J. Zhu, W. Zheng, J. Shi, 和 L. Zhao，“使用三流卷积神经网络识别自发微表情”，*IEEE
    Access*，第7卷，页码 184 537–184 551, 2019。'
- en: '[100] J. Li, Y. Wang, J. See, and W. Liu, “Micro-expression recognition based
    on 3D flow convolutional neural network,” *Pattern Analysis and Applications*,
    vol. 22, no. 4, pp. 1331–1339, 2019.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] J. Li, Y. Wang, J. See, 和 W. Liu，“基于 3D 流卷积神经网络的微表情识别”，*模式分析与应用*，第22卷，第4期，页码
    1331–1339, 2019。'
- en: '[101] B. Yang, J. Cheng, Y. Yang, B. Zhang, and J. Li, “MERTA: micro-expression
    recognition with ternary attentions,” *Multimedia Tools and Applications*, vol. 80,
    no. 11, pp. 1–16, 2021.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] B. Yang, J. Cheng, Y. Yang, B. Zhang, 和 J. Li，“MERTA：基于三元注意力的微表情识别”，*多媒体工具与应用*，第80卷，第11期，页码
    1–16, 2021。'
- en: '[102] W. She, Z. Lv, J. Taoi, M. Niu *et al.*, “Micro-expression recognition
    based on multiple aggregation networks,” in *2020 Asia-Pacific Signal and Information
    Processing Association Annual Summit and Conference (APSIPA ASC)*.   IEEE, 2020,
    pp. 1043–1047.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] W. She, Z. Lv, J. Taoi, M. Niu *等*，“基于多重聚合网络的微表情识别”，发表于*2020亚太信号与信息处理协会年度峰会暨会议（APSIPA
    ASC）*。 IEEE, 2020, 页码 1043–1047。'
- en: '[103] L. Wang, J. Jia, and N. Mao, “Micro-expression recognition based on 2D
    -3D cnn,” in *2020 39th Chinese Control Conference (CCC)*.   IEEE, 2020, pp. 3152–3157.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] L. Wang, J. Jia, 和 N. Mao，“基于 2D -3D cnn 的微表情识别”，发表于*2020年第39届中国控制会议（CCC）*。
    IEEE, 2020, 页码 3152–3157。'
- en: '[104] M. Peng, C. Wang, T. Bi, Y. Shi, X. Zhou, and T. Chen, “A novel apex-time
    network for cross-dataset micro-expression recognition,” in *2019 8th International
    Conference on Affective Computing and Intelligent Interaction (ACII)*.   IEEE,
    2019, pp. 1–6.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] M. Peng, C. Wang, T. Bi, Y. Shi, X. Zhou, 和 T. Chen，“一种新颖的顶点时间网络用于跨数据集微表情识别”，发表于*2019年第8届情感计算与智能交互国际会议（ACII）*。
    IEEE, 2019, 页码 1–6。'
- en: '[105] J. Huang, X. Zhao, and L. Zheng, “Shcfnet on micro-expression recognition
    system,” in *2020 13th International Congress on Image and Signal Processing,
    BioMedical Engineering and Informatics (CISP-BMEI)*.   IEEE, 2020, pp. 163–168.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] J. Huang, X. Zhao 和 L. Zheng，“Shcfnet 在微表情识别系统中的应用”，收录于 *2020年第13届图像与信号处理、生物医学工程与信息学国际大会（CISP-BMEI）*。
    IEEE，2020年，第163–168页。'
- en: '[106] S. Liong, Y. Gan, J. See, H. Khor, and Y. Huang, “Shallow triple stream
    three-dimensional cnn (ststnet) for micro-expression recognition,” in *2019 14th
    IEEE International Conference on Automatic Face and Gesture Recognition (FG)*.   IEEE,
    2019, pp. 1–5.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] S. Liong, Y. Gan, J. See, H. Khor 和 Y. Huang，“用于微表情识别的浅层三流三维 CNN (ststnet)”，收录于
    *2019年第14届IEEE国际自动面部和姿态识别大会（FG）*。 IEEE，2019年，第1–5页。'
- en: '[107] C. Wu and F. Guo, “TSNN: Three-stream combining 2D and 3D convolutional
    neural network for micro-expression recognition,” *IEEJ Transactions on Electrical
    and Electronic Engineering*, vol. 16, no. 1, pp. 98–107, 2021.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] C. Wu 和 F. Guo，“TSNN：结合 2D 和 3D 卷积神经网络的三流微表情识别”，*IEEJ 电气与电子工程学报*，第16卷，第1期，第98–107页，2021年。'
- en: '[108] M. A. Takalkar, M. Xu, and Z. Chaczko, “Manifold feature integration
    for micro-expression recognition,” *Multimedia Systems*, vol. 26, no. 5, pp. 535–551,
    2020.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] M. A. Takalkar, M. Xu 和 Z. Chaczko，“用于微表情识别的流形特征融合”，*多媒体系统*，第26卷，第5期，第535–551页，2020年。'
- en: '[109] C. Hu, D. Jiang, H. Zou, X. Zuo, and Y. Shu, “Multi-task micro-expression
    recognition combining deep and handcrafted features,” in *2018 24th International
    Conference on Pattern Recognition (ICPR)*.   IEEE, 2018, pp. 946–951.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C. Hu, D. Jiang, H. Zou, X. Zuo 和 Y. Shu，“结合深度特征和手工特征的多任务微表情识别”，收录于 *2018年第24届国际模式识别大会（ICPR）*。
    IEEE，2018年，第946–951页。'
- en: '[110] H. Pan, L. Xie, Z. Lv, J. Li, and Z. Wang, “Hierarchical support vector
    machine for facial micro-expression recognition,” *Multimedia Tools and Applications*,
    vol. 79, no. 41, pp. 31 451–31 465, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] H. Pan, L. Xie, Z. Lv, J. Li 和 Z. Wang，“用于面部微表情识别的分层支持向量机”，*多媒体工具与应用*，第79卷，第41期，第31,451–31,465页，2020年。'
- en: '[111] S. C. Nistor, “Multi-staged training of deep neural networks for micro-expression
    recognition,” in *2020 IEEE 14th International Symposium on Applied Computational
    Intelligence and Informatics (SACI)*.   IEEE, 2020, pp. 000 029–000 034.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] S. C. Nistor，“用于微表情识别的深度神经网络的多阶段训练”，收录于 *2020年第14届IEEE应用计算智能与信息学国际研讨会（SACI）*。
    IEEE，2020年，第000,029–000,034页。'
- en: '[112] H.-Q. Khor, J. See, R. C. W. Phan, and W. Lin, “Enriched long-term recurrent
    convolutional network for facial micro-expression recognition,” in *2018 13th
    IEEE International Conference on Automatic Face and Gesture Recognition (FG)*.   IEEE,
    2018, pp. 667–674.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] H.-Q. Khor, J. See, R. C. W. Phan 和 W. Lin，“用于面部微表情识别的丰富长短期递归卷积网络”，收录于
    *2018年第13届IEEE国际自动面部和姿态识别大会（FG）*。 IEEE，2018年，第667–674页。'
- en: '[113] M. Zhang, Z. Huan, and L. Shang, “Micro-expression recognition using
    micro-variation boosted heat areas,” in *Chinese Conference on Pattern Recognition
    and Computer Vision (PRCV)*.   Springer, 2020, pp. 531–543.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] M. Zhang, Z. Huan 和 L. Shang，“使用微变化增强热区域的微表情识别”，收录于 *中国模式识别与计算机视觉大会（PRCV）*。
    Springer，2020年，第531–543页。'
- en: '[114] M. Bai and R. Goecke, “Investigating lstm for micro-expression recognition,”
    in *Companion Publication of the 2020 International Conference on Multimodal Interaction*,
    2020, pp. 7–11.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] M. Bai 和 R. Goecke，“调查 LSTM 在微表情识别中的应用”，收录于 *2020 年国际多模态交互会议附刊*，2020年，第7–11页。'
- en: '[115] D. Y. Choi and B. C. Song, “Facial micro-expression recognition using
    two-dimensional landmark feature maps,” *IEEE Access*, vol. 8, pp. 121 549–121 563,
    2020.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] D. Y. Choi 和 B. C. Song，“使用二维地标特征图进行面部微表情识别”，*IEEE Access*，第8卷，第121,549–121,563页，2020年。'
- en: '[116] R. Zhi, M. Liu, H. Xu, and M. Wan, “Facial micro-expression recognition
    using enhanced temporal feature-wise model,” in *Cyberspace Data and Intelligence,
    and Cyber-Living, Syndrome, and Health*.   Springer, 2019, pp. 301–311.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] R. Zhi, M. Liu, H. Xu 和 M. Wan，“使用增强的时间特征模型进行面部微表情识别”，收录于 *网络空间数据与智能、网络生活、综合症及健康*。
    Springer，2019年，第301–311页。'
- en: '[117] Q. Li, S. Zhan, L. Xu, and C. Wu, “Facial micro-expression recognition
    based on the fusion of deep learning and enhanced optical flow,” *Multimedia Tools
    and Applications*, vol. 78, no. 20, pp. 29 307–29 322, 2019.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Q. Li, S. Zhan, L. Xu 和 C. Wu，“基于深度学习与增强光流融合的面部微表情识别”，*多媒体工具与应用*，第78卷，第20期，第29,307–29,322页，2019年。'
- en: '[118] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *arXiv preprint arXiv:1503.02531*, 2015.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] G. Hinton, O. Vinyals 和 J. Dean，“提炼神经网络中的知识”，*arXiv 预印本 arXiv:1503.02531*，2015年。'
- en: '[119] N. Komodakis and S. Zagoruyko, “Paying more attention to attention: improving
    the performance of convolutional neural networks via attention transfer,” in *International
    Conference on Learning Representations (ICLR)*, 2017.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] N. Komodakis 和 S. Zagoruyko, “更加关注注意力：通过注意力转移提高卷积神经网络的性能，” 见于*国际学习表征会议（ICLR）*，2017年。'
- en: '[120] Y. Liu, H. Du, L. Zheng, and T. Gedeon, “A neural micro-expression recognizer,”
    in *2019 14th IEEE International Conference on Automatic Face and Gesture Recognition
    (FG)*.   IEEE, 2019, pp. 1–4.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Y. Liu, H. Du, L. Zheng, 和 T. Gedeon, “一个神经微表情识别器，” 见于*2019年第14届 IEEE
    国际自动面部与手势识别会议（FG）*。 IEEE, 2019, 第1–4页。'
- en: '[121] L. Zhou, Q. Mao, and L. Xue, “Cross-database micro-expression recognition:
    a style aggregated and attention transfer approach,” in *2019 IEEE International
    Conference on Multimedia and Expo Workshops (ICMEW)*.   IEEE, 2019, pp. 102–107.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] L. Zhou, Q. Mao, 和 L. Xue, “跨数据库微表情识别：一种风格聚合和注意力转移的方法，” 见于*2019 IEEE
    国际多媒体与博览会研讨会（ICMEW）*。 IEEE, 2019, 第102–107页。'
- en: '[122] B. Xia, W. Wang, S. Wang, and E. Chen, “Learning from macro-expression:
    a micro-expression recognition framework,” in *Proceedings of the 28th ACM International
    Conference on Multimedia*, 2020, pp. 2936–2944.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] B. Xia, W. Wang, S. Wang, 和 E. Chen, “从宏观表情中学习：一个微表情识别框架，” 见于*第28届 ACM
    国际多媒体会议论文集*，2020年，第2936–2944页。'
- en: '[123] D. M. Kline and V. L. Berardi, “Revisiting squared-error and cross-entropy
    functions for training neural network classifiers,” *Neural Computing and Applications*,
    vol. 14, no. 4, pp. 310–318, 2005.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] D. M. Kline 和 V. L. Berardi, “重新审视用于训练神经网络分类器的平方误差和交叉熵函数，” *神经计算与应用*，第14卷，第4期，第310–318页，2005年。'
- en: '[124] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction by learning
    an invariant mapping,” in *IEEE Computing Society Conference on Computer Vision
    and Pattern Recognition*, vol. 2.   IEEE, 2006, pp. 1735–1742.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] R. Hadsell, S. Chopra, 和 Y. LeCun, “通过学习不变映射进行维度减少，” 见于*IEEE 计算机视觉与模式识别会议*，第2卷。
    IEEE, 2006年，第1735–1742页。'
- en: '[125] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified embedding
    for face recognition and clustering,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2015, pp. 815–823.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] F. Schroff, D. Kalenichenko, 和 J. Philbin, “Facenet：用于面部识别和聚类的统一嵌入，”
    见于*IEEE 计算机视觉与模式识别会议论文集*，2015年，第815–823页。'
- en: '[126] W. Liu, Y. Wen, Z. Yu, and M. Yang, “Large-margin softmax loss for convolutional
    neural networks.” in *International Conference on Machine Learning (ICML)*, vol. 2,
    2016, p. 7.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] W. Liu, Y. Wen, Z. Yu, 和 M. Yang, “卷积神经网络的大边距软最大损失。” 见于*国际机器学习会议（ICML）*，第2卷，2016年，第7页。'
- en: '[127] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular
    margin loss for deep face recognition,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 4690–4699.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] J. Deng, J. Guo, N. Xue, 和 S. Zafeiriou, “Arcface：用于深度面部识别的加性角度边距损失，”
    见于*IEEE/CVF 计算机视觉与模式识别会议论文集*，2019年，第4690–4699页。'
- en: '[128] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu,
    “Cosface: Large margin cosine loss for deep face recognition,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2018, pp.
    5265–5274.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, 和 W. Liu,
    “Cosface：深度面部识别的大边距余弦损失，” 见于*IEEE 计算机视觉与模式识别会议论文集*，2018年，第5265–5274页。'
- en: '[129] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
    dense object detection,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 2980–2988.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] T.-Y. Lin, P. Goyal, R. Girshick, K. He, 和 P. Dollár, “用于密集对象检测的焦点损失，”
    见于*IEEE 国际计算机视觉会议论文集*，2017年，第2980–2988页。'
- en: '[130] A. C. Le Ngo, A. Johnston, R. C.-W. Phan, and J. See, “Micro-expression
    motion magnification: Global lagrangian vs. local eulerian approaches,” in *2018
    13th IEEE international conference on automatic face and gesture recognition (FG)*.   IEEE,
    2018, pp. 650–656.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] A. C. Le Ngo, A. Johnston, R. C.-W. Phan, 和 J. See, “微表情运动放大：全局拉格朗日方法与局部欧拉方法，”
    见于*2018年第13届 IEEE 国际自动面部与手势识别会议（FG）*。 IEEE, 2018, 第650–656页。'
- en: '[131] S.-T. Liong, J. See, R. C.-W. Phan, K. Wong, and S.-W. Tan, “Hybrid facial
    regions extraction for micro-expression recognition system,” *Journal of Signal
    Processing Systems*, vol. 90, no. 4, pp. 601–617, 2018.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] S.-T. Liong, J. See, R. C.-W. Phan, K. Wong, 和 S.-W. Tan, “用于微表情识别系统的混合面部区域提取，”
    *信号处理系统杂志*，第90卷，第4期，第601–617页，2018年。'
- en: '[132] Z. Xia, X. Hong, X. Gao, X. Feng, and G. Zhao, “Spatiotemporal recurrent
    convolutional networks for recognizing spontaneous micro-expressions,” *IEEE Transactions
    on Multimedia*, vol. PP, pp. 1–1, 2019.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Z. Xia, X. Hong, X. Gao, X. Feng, 和 G. Zhao, “用于识别自发微表情的时空递归卷积网络，” *IEEE
    多媒体交易*, vol. PP, pp. 1–1, 2019。'
- en: '[133] J. Li, C. Soladie, and R. Seguier, “Ltp-ml: Micro-expression detection
    by recognition of local temporal pattern of facial movements,” in *2018 13th IEEE
    international conference on automatic face and gesture recognition (FG)*.   IEEE,
    2018, pp. 634–641.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] J. Li, C. Soladie, 和 R. Seguier, “Ltp-ml: 通过识别面部运动的局部时间模式进行微表情检测，” 在
    *2018年第13届IEEE国际自动脸部和手势识别会议 (FG)*。 IEEE, 2018, pp. 634–641。'
- en: '[134] E. L. Rosenberg and P. Ekman, *What the face reveals: Basic and applied
    studies of spontaneous expression using the facial action coding system (FACS)*.   Oxford
    University Press, 2020.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] E. L. Rosenberg 和 P. Ekman, *面部揭示的秘密：使用面部动作编码系统 (FACS) 的自发表情基本与应用研究*。
    牛津大学出版社, 2020。'
- en: '[135] X. Huang, G. Zhao, W. Zheng, and M. Pietikäinen, “Towards a dynamic expression
    recognition system under facial occlusion,” *Pattern Recognition Letters*, vol. 33,
    no. 16, pp. 2181–2191, 2012.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] X. Huang, G. Zhao, W. Zheng, 和 M. Pietikäinen, “在面部遮挡下的动态表情识别系统，” *模式识别快报*,
    vol. 33, no. 16, pp. 2181–2191, 2012。'
- en: '[136] B. Chen, K.-H. Liu, Y. Xu, Q.-Q. Wu, and J.-F. Yao, “Block division convolutional
    network with implicit deep features augmentation for micro-expression recognition,”
    *IEEE Transactions on Multimedia*, 2022.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] B. Chen, K.-H. Liu, Y. Xu, Q.-Q. Wu, 和 J.-F. Yao, “具有隐式深层特征增强的块划分卷积网络用于微表情识别，”
    *IEEE 多媒体交易*, 2022。'
- en: '[137] F. Zhang, T. Zhang, Q. Mao, and C. Xu, “Joint pose and expression modeling
    for facial expression recognition,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2018, pp. 3359–3368.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] F. Zhang, T. Zhang, Q. Mao, 和 C. Xu, “面部表情识别的联合姿势和表情建模，” 在 *IEEE计算机视觉与模式识别会议论文集*,
    2018, pp. 3359–3368。'
- en: '[138] S.-T. Liong, Y. Gan, D. Zheng, S.-M. Li, H.-X. Xu, H.-Z. Zhang, R.-K.
    Lyu, and K.-H. Liu, “Evaluation of the spatio-temporal features and gan for micro-expression
    recognition system,” *Journal of Signal Processing Systems*, pp. 1–21, 2020.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] S.-T. Liong, Y. Gan, D. Zheng, S.-M. Li, H.-X. Xu, H.-Z. Zhang, R.-K.
    Lyu, 和 K.-H. Liu, “评估时空特征和GAN用于微表情识别系统，” *信号处理系统期刊*, pp. 1–21, 2020。'
- en: '[139] S.-J. W. et al., “Micro-expression recognition using color spaces,” *IEEE
    Transactions on Image Processing*, vol. 24, no. 12, pp. 6034–6047, 2015.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] S.-J. W. 等, “使用色彩空间的微表情识别，” *IEEE 图像处理交易*, vol. 24, no. 12, pp. 6034–6047,
    2015。'
- en: '[140] V. Mayya, R. M. Pai, and M. M. Pai, “Combining temporal interpolation
    and dcnn for faster recognition of micro-expressions in video sequences,” in *2016
    International Conference on Advances in Computing, Communications and Informatics
    (ICACCI)*.   IEEE, 2016, pp. 699–703.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] V. Mayya, R. M. Pai, 和 M. M. Pai, “结合时间插值和DCNN以加快视频序列中微表情的识别，” 在 *2016年国际计算、通信与信息学进展会议
    (ICACCI)*。 IEEE, 2016, pp. 699–703。'
- en: '[141] S. Ji, W. Xu, M. Yang, and K. Yu, “3D convolutional neural networks for
    human action recognition,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 35, no. 1, pp. 221–231, 2012.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] S. Ji, W. Xu, M. Yang, 和 K. Yu, “用于人类动作识别的3D卷积神经网络，” *IEEE模式分析与机器智能交易*,
    vol. 35, no. 1, pp. 221–231, 2012。'
- en: '[142] L. R. Medsker and L. Jain, “Recurrent neural networks,” *Design and Applications*,
    vol. 5, 2001.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] L. R. Medsker 和 L. Jain, “递归神经网络，” *设计与应用*, vol. 5, 2001。'
- en: '[143] A. Ullah, J. Ahmad, K. Muhammad, M. Sajjad, and S. W. Baik, “Action recognition
    in video sequences using deep bi-directional lstm with cnn features,” *IEEE access*,
    vol. 6, pp. 1155–1166, 2017.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] A. Ullah, J. Ahmad, K. Muhammad, M. Sajjad, 和 S. W. Baik, “使用深度双向LSTM与CNN特征的视频序列动作识别，”
    *IEEE Access*, vol. 6, pp. 1155–1166, 2017。'
- en: '[144] H. Yang, C. Yuan, B. Li, Y. Du, J. Xing, W. Hu, and S. J. Maybank, “Asymmetric
    3D convolutional neural networks for action recognition,” *Pattern recognition*,
    vol. 85, pp. 1–12, 2019.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] H. Yang, C. Yuan, B. Li, Y. Du, J. Xing, W. Hu, 和 S. J. Maybank, “用于动作识别的非对称3D卷积神经网络，”
    *模式识别*, vol. 85, pp. 1–12, 2019。'
- en: '[145] M. Peng, C. Wang, Y. Gao, T. Bi, T. Chen, Y. Shi, and X.-D. Zhou, “Recognizing
    micro-expression in video clip with adaptive key-frame mining,” *arXiv preprint
    arXiv:2009.09179*, 2020.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] M. Peng, C. Wang, Y. Gao, T. Bi, T. Chen, Y. Shi, 和 X.-D. Zhou, “通过自适应关键帧挖掘识别视频剪辑中的微表情，”
    *arXiv 预印本 arXiv:2009.09179*, 2020。'
- en: '[146] S. Liong, J. See, K. S. Wong, and R. C. W. Phan, “Less is more: Micro-expression
    recognition from video using apex frame,” *Signal Processing: Image Communication*,
    vol. 62, pp. 82–92, 2018.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] S. Liong, J. See, K. S. Wong, 和 R. C. W. Phan，“少即是多：使用顶点帧从视频中识别微表情，”
    *信号处理：图像通信*，第62卷，第82–92页，2018年。'
- en: '[147] H. Bilen, B. Fernando, E. Gavves, A. Vedaldi, and S. Gould, “Dynamic
    image networks for action recognition,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016, pp. 3034–3042.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] H. Bilen, B. Fernando, E. Gavves, A. Vedaldi, 和 S. Gould，“用于动作识别的动态图像网络，”在
    *IEEE 计算机视觉与模式识别会议论文集*，2016年，第3034–3042页。'
- en: '[148] H. Bilen, B. Fernando, E. Gavves, and A. Vedaldi, “Action recognition
    with dynamic image networks,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 40, no. 12, pp. 2799–2813, 2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] H. Bilen, B. Fernando, E. Gavves, 和 A. Vedaldi，“动态图像网络进行动作识别，” *IEEE
    模式分析与机器智能学报*，第40卷，第12期，第2799–2813页，2017年。'
- en: '[149] J. L. Barron, D. J. Fleet, and S. S. Beauchemin, “Performance of optical
    flow techniques,” *International journal of computer vision*, vol. 12, no. 1,
    pp. 43–77, 1994.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] J. L. Barron, D. J. Fleet, 和 S. S. Beauchemin，“光流技术的性能，” *国际计算机视觉期刊*，第12卷，第1期，第43–77页，1994年。'
- en: '[150] B. K. Horn and B. G. Schunck, “Determining optical flow,” *Artificial
    intelligence*, vol. 17, no. 1-3, pp. 185–203, 1981.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] B. K. Horn 和 B. G. Schunck，“确定光流，” *人工智能*，第17卷，第1-3期，第185–203页，1981年。'
- en: '[151] T. Senst, V. Eiselein, and T. Sikora, “Robust local optical flow for
    feature tracking,” *IEEE Transactions on Circuits and Systems for Video Technology*,
    vol. 22, no. 9, pp. 1377–1387, 2012.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] T. Senst, V. Eiselein, 和 T. Sikora，“用于特征跟踪的鲁棒局部光流，” *IEEE 视频技术电路与系统学报*，第22卷，第9期，第1377–1387页，2012年。'
- en: '[152] A. Wedel, T. Pock, C. Zach, H. Bischof, and D. Cremers, “An improved
    algorithm for TV-L1 optical flow,” in *Statistical and geometrical approaches
    to visual motion analysis*.   Springer, 2009, pp. 23–45.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] A. Wedel, T. Pock, C. Zach, H. Bischof, 和 D. Cremers，“改进的 TV-L1 光流算法，”在
    *统计与几何方法在视觉运动分析中的应用*。 Springer，2009年，第23–45页。'
- en: '[153] B. Allaert, I. R. Ward, I. M. Bilasco, C. Djeraba, and M. Bennamoun,
    “Optical flow techniques for facial expression analysis: Performance evaluation
    and improvements,” *arXiv preprint arXiv:1904.11592*, 2019.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] B. Allaert, I. R. Ward, I. M. Bilasco, C. Djeraba, 和 M. Bennamoun，“面部表情分析的光流技术：性能评估和改进，”
    *arXiv 预印本 arXiv:1904.11592*，2019年。'
- en: '[154] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2016, pp. 770–778.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习，”在 *IEEE 计算机视觉与模式识别会议论文集*，2016年，第770–778页。'
- en: '[155] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, “Inception-v4, inception-resnet
    and the impact of residual connections on learning,” in *Proceedings of the AAAI
    Conference on Artificial Intelligence*, vol. 31, 2017.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] C. Szegedy, S. Ioffe, V. Vanhoucke, 和 A. Alemi，“Inception-v4、Inception-ResNet
    和残差连接对学习的影响，”在 *AAAI 人工智能会议论文集*，第31卷，2017年。'
- en: '[156] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between capsules,”
    *arXiv preprint arXiv:1710.09829*, 2017.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] S. Sabour, N. Frosst, 和 G. E. Hinton，“胶囊之间的动态路由，” *arXiv 预印本 arXiv:1710.09829*，2017年。'
- en: '[157] P. Pareek and A. Thakkar, “A survey on video-based human action recognition:
    recent updates, datasets, challenges, and applications,” *Artificial Intelligence
    Review*, vol. 54, no. 3, pp. 2259–2322, 2021.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] P. Pareek 和 A. Thakkar，“基于视频的人类动作识别调查：最近更新、数据集、挑战和应用，” *人工智能评论*，第54卷，第3期，第2259–2322页，2021年。'
- en: '[158] K. Hara, H. Kataoka, and Y. Satoh, “Can spatiotemporal 3D cnns retrace
    the history of 2D cnns and imagenet?” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2018, pp. 6546–6555.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] K. Hara, H. Kataoka, 和 Y. Satoh，“时空3D CNNs 能否回溯2D CNNs 和 ImageNet 的历史？”在
    *IEEE 计算机视觉与模式识别会议论文集*，2018年，第6546–6555页。'
- en: '[159] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 1492–1500.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] S. Xie, R. Girshick, P. Dollár, Z. Tu, 和 K. He，“用于深度神经网络的聚合残差变换，”在 *IEEE
    计算机视觉与模式识别会议论文集*，2017年，第1492–1500页。'
- en: '[160] Z. Wu, C. Shen, and A. Van Den Hengel, “Wider or deeper: Revisiting the
    resnet model for visual recognition,” *Pattern Recognition*, vol. 90, pp. 119–133,
    2019.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Z. Wu, C. Shen, 和 A. Van Den Hengel，“更宽还是更深：重新审视用于视觉识别的 ResNet 模型，” *模式识别*，第90卷，第119–133页，2019年。'
- en: '[161] R. Belaiche, Y. Liu, C. Migniot, D. Ginhac, and F. Yang, “Cost-effective
    cnns for real-time micro-expression recognition,” *Applied Sciences*, vol. 10,
    no. 14, p. 4959, 2020.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] R. Belaiche, Y. Liu, C. Migniot, D. Ginhac, 和 F. Yang, “用于实时微表情识别的高性价比CNNs，”
    *Applied Sciences*，第10卷，第14期，第4959页，2020年。'
- en: '[162] J. Wen, W. Yang, L. Wang, W. Wei, S. Tan, and Y. Wu, “Cross-database
    micro expression recognition based on apex frame optical flow and multi-head self-attention,”
    in *International Symposium on Parallel Architectures, Algorithms and Programming*.   Springer,
    2020, pp. 128–139.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] J. Wen, W. Yang, L. Wang, W. Wei, S. Tan, 和 Y. Wu, “基于顶点帧光流和多头自注意力的跨数据库微表情识别，”
    在 *国际并行架构、算法与编程研讨会*。 Springer，2020年，第128–139页。'
- en: '[163] Z. Lai, R. Chen, J. Jia, and Y. Qian, “Real-time micro-expression recognition
    based on resnet and atrous convolutions,” *Journal of Ambient Intelligence and
    Humanized Computing*, pp. 1–12, 2020.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Z. Lai, R. Chen, J. Jia, 和 Y. Qian, “基于ResNet和空洞卷积的实时微表情识别，” *环境智能与人性化计算杂志*，第1–12页，2020年。'
- en: '[164] G. Chinnappa and M. K. Rajagopal, “Residual attention network for deep
    face recognition using micro-expression image analysis,” *Journal of Ambient Intelligence
    and HumanizedComputing*, pp. 1–14, 2021.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] G. Chinnappa 和 M. K. Rajagopal, “用于深度人脸识别的残差注意力网络，通过微表情图像分析，” *环境智能与人性化计算杂志*，第1–14页，2021年。'
- en: '[165] M. Verma, S. K. Vipparthi, and G. Singh, “Affectivenet: Affective-motion
    feature learning for micro expression recognition,” *IEEE MultiMedia*, 2020.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] M. Verma, S. K. Vipparthi, 和 G. Singh, “Affectivenet: 用于微表情识别的情感运动特征学习，”
    *IEEE MultiMedia*，2020年。'
- en: '[166] W. Li, F. Abtahi, and Z. Zhu, “Action unit detection with region adaptation,
    multi-labeling learning and optimal temporal fusing,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2017, pp. 1841–1850.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] W. Li, F. Abtahi, 和 Z. Zhu, “通过区域适配、多标签学习和最优时间融合进行动作单元检测，” 在 *IEEE计算机视觉与模式识别会议论文集*，2017年，第1841–1850页。'
- en: '[167] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2015, pp.
    1–9.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, 和 A. Rabinovich, “通过卷积深入研究，” 在 *IEEE计算机视觉与模式识别会议论文集*，2015年，第1–9页。'
- en: '[168] M. Verma, S. K. Vipparthi, and G. Singh, “Hinet: Hybrid inherited feature
    learning network for facial expression recognition,” *IEEE Letters of the Computer
    Society*, vol. 2, no. 4, pp. 36–39, 2019.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] M. Verma, S. K. Vipparthi, 和 G. Singh, “Hinet: 用于面部表情识别的混合继承特征学习网络，”
    *IEEE Letters of the Computer Society*，第2卷，第4期，第36–39页，2019年。'
- en: '[169] Z. Xia, W. Peng, H.-Q. Khor, X. Feng, and G. Zhao, “Revealing the invisible
    with model and data shrinking for composite-database micro-expression recognition,”
    *IEEE Transactions on Image Processing*, vol. 29, pp. 8590–8605, 2020.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Z. Xia, W. Peng, H.-Q. Khor, X. Feng, 和 G. Zhao, “通过模型和数据收缩揭示隐形信息，用于复合数据库微表情识别，”
    *IEEE Transactions on Image Processing*，第29卷，第8590–8605页，2020年。'
- en: '[170] S. Bai, J. Z. Kolter, and V. Koltun, “An empirical evaluation of generic
    convolutional and recurrent networks for sequence modeling,” *arXiv preprint arXiv:1803.01271*,
    2018.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] S. Bai, J. Z. Kolter, 和 V. Koltun, “对通用卷积和递归网络进行序列建模的实证评估，” *arXiv预印本arXiv:1803.01271*，2018年。'
- en: '[171] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and
    D. Tran, “Image transformer,” in *International Conference on Machine Learning*.   PMLR,
    2018, pp. 4055–4064.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, 和
    D. Tran, “图像变换器，” 在 *国际机器学习大会*。 PMLR，2018年，第4055–4064页。'
- en: '[172] D. Acharya, Z. Huang, D. Pani Paudel, and L. Van Gool, “Covariance pooling
    for facial expression recognition,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition Workshops*, 2018, pp. 367–374.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] D. Acharya, Z. Huang, D. Pani Paudel, 和 L. Van Gool, “用于面部表情识别的协方差池化，”
    在 *IEEE计算机视觉与模式识别会议研讨会论文集*，2018年，第367–374页。'
- en: '[173] M. A. Takalkar, S. Thuseethan, S. Rajasegarar, Z. Chaczko, M. Xu, and
    J. Yearwood, “LGAttNet: Automatic micro-expression detection using dual-stream
    local and global attentions,” *Knowledge-Based Systems*, vol. 212, p. 106566,
    2021.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] M. A. Takalkar, S. Thuseethan, S. Rajasegarar, Z. Chaczko, M. Xu, 和 J.
    Yearwood, “LGAttNet: 使用双流局部和全局注意力的自动微表情检测，” *Knowledge-Based Systems*，第212卷，第106566页，2021年。'
- en: '[174] M. Bai, “Detection of micro-expression recognition based on spatio-temporal
    modelling and spatial attention,” in *Proceedings of the 2020 International Conference
    on Multimodal Interaction*, 2020, pp. 703–707.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] M. Bai，“基于时空建模和空间注意力的微表情识别检测”，见于 *2020国际多模态互动会议论文集*，2020年，第703–707页。'
- en: '[175] M. F. Hashmi, B. Ashish, V. Sharma, A. G. Keskar, N. D. Bokde, J. H.
    Yoon, and Z. W. Geem, “LARNet: Real-time detection of facial micro expression
    using lossless attention residual network,” *Sensors*, vol. 21, no. 4, p. 1098,
    2021.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] M. F. Hashmi, B. Ashish, V. Sharma, A. G. Keskar, N. D. Bokde, J. H.
    Yoon, 和 Z. W. Geem，“LARNet：使用无损注意力残差网络的实时面部微表情检测”， *传感器*，第21卷，第4期，第1098页，2021年。'
- en: '[176] Y. Su, J. Zhang, J. Liu, and G. Zhai, “Key facial components guided micro-expression
    recognition based on first and second-order motion,” in *2021 IEEE International
    Conference on Multimedia and Expo (ICME)*.   IEEE, 2021, pp. 1–6.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Y. Su, J. Zhang, J. Liu, 和 G. Zhai，“基于一阶和二阶运动的关键面部组件引导的微表情识别”，见于 *2021
    IEEE国际多媒体与博览会（ICME）*。 IEEE，2021年，第1–6页。'
- en: '[177] M. Wei, W. Zheng, Y. Zong, X. Jiang, C. Lu, and J. Liu, “A novel micro-expression
    recognition approach using attention-based magnification-adaptive networks,” in
    *ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*.   IEEE, 2022, pp. 2420–2424.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] M. Wei, W. Zheng, Y. Zong, X. Jiang, C. Lu, 和 J. Liu，“一种基于注意力放大自适应网络的新型微表情识别方法”，见于
    *ICASSP 2022-2022 IEEE国际声学、语音与信号处理会议（ICASSP）*。 IEEE，2022年，第2420–2424页。'
- en: '[178] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2018, pp. 7794–7803.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] X. Wang, R. Girshick, A. Gupta, 和 K. He，“非本地神经网络”，见于 *IEEE计算机视觉与模式识别会议论文集*，2018年，第7794–7803页。'
- en: '[179] L. Yao, X. Xiao, R. Cao, F. Chen, and T. Chen, “Three stream 3D cnn with
    se block for micro-expression recognition,” in *2020 International Conference
    on Computer Engineering and Application (ICCEA)*, 2020, pp. 439–443.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] L. Yao, X. Xiao, R. Cao, F. Chen, 和 T. Chen，“用于微表情识别的三流3D CNN与SE块”，见于
    *2020国际计算机工程与应用会议（ICCEA）*，2020年，第439–443页。'
- en: '[180] Y. Wang, Y. Huang, C. Liu, X. Gu, D. Yang, S. Wang, and B. Zhang, “Micro
    expression recognition via dual-stream spatiotemporal attention network,” *Journal
    of Healthcare Engineering*, vol. 2021, 2021.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Y. Wang, Y. Huang, C. Liu, X. Gu, D. Yang, S. Wang, 和 B. Zhang，“通过双流时空注意力网络进行微表情识别”，
    *医疗工程杂志*，第2021卷，2021年。'
- en: '[181] X. Li, G. Wei, J. Wang, and Y. Zhou, “Multi-scale joint feature network
    for micro-expression recognition,” *Computational Visual Media*, pp. 1–11, 2021.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] X. Li, G. Wei, J. Wang, 和 Y. Zhou，“用于微表情识别的多尺度联合特征网络”， *计算视觉媒体*，第1–11页，2021年。'
- en: '[182] Y. Gan and S.-T. Liong, “Bi-directional vectors from apex in cnn for
    micro-expression recognition,” in *2018 IEEE 3rd International Conference on Image,
    Vision and Computing (ICIVC)*.   IEEE, 2018, pp. 168–172.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Y. Gan 和 S.-T. Liong，“用于微表情识别的来自CNN顶点的双向向量”，见于 *2018 IEEE第三届国际图像、视觉与计算会议（ICIVC）*。
    IEEE，2018年，第168–172页。'
- en: '[183] P. Gupta, “MERASTC: Micro-expression recognition using effective feature
    encodings and 2d convolutional neural network,” *IEEE Transactions on Affective
    Computing*, 2021.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] P. Gupta，“MERASTC：使用有效特征编码和二维卷积神经网络的微表情识别”， *IEEE情感计算学报*，2021年。'
- en: '[184] O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition,”
    in *British Machine Vision Conference*, 2015.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] O. M. Parkhi, A. Vedaldi, 和 A. Zisserman，“深度面部识别”，见于 *英国机器视觉会议*，2015年。'
- en: '[185] L. Zhou, Q. Mao, X. Huang, F. Zhang, and Z. Zhang, “Feature refinement:
    An expression-specific feature learning and fusion method for micro-expression
    recognition,” *arXiv preprint arXiv:2101.04838*, 2021.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] L. Zhou, Q. Mao, X. Huang, F. Zhang, 和 Z. Zhang，“特征细化：用于微表情识别的表情特定特征学习与融合方法”，
    *arXiv预印本 arXiv:2101.04838*，2021年。'
- en: '[186] Y. Zhang and Q. Yang, “A survey on multi-task learning,” *IEEE Transactions
    on Knowledge and Data Engineering*, 2021.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Y. Zhang 和 Q. Yang，“多任务学习综述”， *IEEE知识与数据工程学报*，2021年。'
- en: '[187] D. Patel, X. Hong, and G. Zhao, “Selective deep features for micro-expression
    recognition,” in *Proceedings of IEEE International Conference Pattern Recognition*,
    2017, pp. 2258–2263.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] D. Patel, X. Hong, 和 G. Zhao，“用于微表情识别的选择性深度特征”，见于 *IEEE国际模式识别会议论文集*，2017年，第2258–2263页。'
- en: '[188] A. Romero, N. Ballas, E. K. Samira, A. Chassang, C. Gatta, and B. Yoshua,
    “Fitnets: Hints for thin deep nets,” *Proceedings of International Conference
    on Learning Representations (ICLR)*, 2015.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] A. Romero, N. Ballas, E. K. Samira, A. Chassang, C. Gatta, 和 B. Yoshua，“Fitnets：细深网络的提示，”*国际学习表征会议（ICLR）论文集*，2015年。'
- en: '[189] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *Proceedings of the
    IEEE international conference on computer vision*, 2017, pp. 2223–2232.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] J.-Y. Zhu, T. Park, P. Isola, 和 A. A. Efros，“使用循环一致对抗网络进行未配对图像到图像的转换，”见于*IEEE国际计算机视觉会议论文集*，2017年，页码2223–2232。'
- en: '[190] B. Xia and S. Wang, “Micro-expression recognition enhanced by macro-expression
    from spatial-temporal domain,” in *Proceedings of the Thirtieth International
    Joint Conference on Artificial Intelligence*, 2021, pp. 1186–1193.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] B. Xia 和 S. Wang，“通过时空领域中的宏表达增强微表情识别，”见于*第30届国际人工智能联合会议论文集*，2021年，页码1186–1193。'
- en: '[191] S. Lalitha and K. Thyagharajan, “Micro-facial expression recognition
    based on deep-rooted learning algorithm,” *arXiv preprint arXiv:2009.05778*, 2020.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] S. Lalitha 和 K. Thyagharajan，“基于深度学习算法的微面部表情识别，”*arXiv预印本arXiv:2009.05778*，2020年。'
- en: '[192] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, “A discriminative feature learning
    approach for deep face recognition,” in *Proceedings of European Conference Computer
    Vision*.   Springer, 2016, pp. 499–515.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Y. Wen, K. Zhang, Z. Li, 和 Y. Qiao，“一种用于深度人脸识别的辨别特征学习方法，”见于*欧洲计算机视觉会议论文集*。
    斯普林格，2016年，页码499–515。'
- en: '[193] Y. Li, T. Wang, B. Kang, S. Tang, C. Wang, J. Li, and J. Feng, “Overcoming
    classifier imbalance for long-tail object detection with balanced group softmax,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 10 991–11 000.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Y. Li, T. Wang, B. Kang, S. Tang, C. Wang, J. Li, 和 J. Feng，“通过平衡组Softmax克服分类器不平衡以进行长尾目标检测，”见于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页码10 991–11 000。'
- en: '[194] J. E. Van Engelen and H. H. Hoos, “A survey on semi-supervised learning,”
    *Machine Learning*, vol. 109, no. 2, pp. 373–440, 2020.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] J. E. Van Engelen 和 H. H. Hoos，“关于半监督学习的调查，”*机器学习*，第109卷，第2期，页码373–440，2020年。'
- en: '[195] X. Wang, X. Wang, and Y. Ni, “Unsupervised domain adaptation for facial
    expression recognition using generative adversarial networks,” *Computational
    Intelligence and Neuroscience*, vol. 2018, 2018.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] X. Wang, X. Wang, 和 Y. Ni，“使用生成对抗网络的无监督领域适应进行面部表情识别，”*计算智能与神经科学*，第2018卷，2018年。'
- en: '[196] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza, B. Hamner,
    W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee *et al.*, “Challenges in representation
    learning: A report on three machine learning contests,” in *International conference
    on Neural Information Processing*.   Springer, 2013, pp. 117–124.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza, B.
    Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee *等*，“表示学习中的挑战：三场机器学习竞赛的报告，”见于*国际神经信息处理会议*。
    斯普林格，2013年，页码117–124。'
- en: '[197] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews,
    “The extended cohn-kanade dataset (CK+): A complete dataset for action unit and
    emotion-specified expression,” in *2010 IEEE Computer society Conference on Computer
    Vision and Pattern Recognition workshops*.   IEEE, 2010, pp. 94–101.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, 和 I. Matthews，“扩展的Cohn-Kanade数据集（CK+）：一个完整的动作单位和情感特定表情数据集，”见于*2010
    IEEE计算机学会计算机视觉与模式识别研讨会*。 IEEE，2010年，页码94–101。'
- en: '[198] G. Zhao, X. Huang, M. Taini, S. Z. Li, and M. PietikäInen, “Facial expression
    recognition from near-infrared videos,” *Image and Vision Computing*, vol. 29,
    no. 9, pp. 607–619, 2011.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] G. Zhao, X. Huang, M. Taini, S. Z. Li, 和 M. PietikäInen，“从近红外视频中识别面部表情，”*图像与视觉计算*，第29卷，第9期，页码607–619，2011年。'
- en: '[199] S. Yang, P. Luo, C.-C. Loy, and X. Tang, “Wider face: A face detection
    benchmark,” in *Proceedings of the IEEE*, 2016, pp. 5525–5533.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] S. Yang, P. Luo, C.-C. Loy, 和 X. Tang，“Wider face：一个人脸检测基准，”见于*IEEE会议论文集*，2016年，页码5525–5533。'
- en: '[200] J. Li, Z. Dong, S. Lu, S.-J. Wang, W.-J. Yan, Y. Ma, Y. Liu, C. Huang,
    and X. Fu, “Cas (me) 3: A third generation facial spontaneous micro-expression
    database with depth information and high ecological validity,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2022.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] J. Li, Z. Dong, S. Lu, S.-J. Wang, W.-J. Yan, Y. Ma, Y. Liu, C. Huang,
    和 X. Fu，“Cas (me) 3：第三代面部自发微表情数据库，包含深度信息和高生态有效性，”*IEEE模式分析与机器智能汇刊*，2022年。'
- en: '[201] G. Sandbach, S. Zafeiriou, M. Pantic, and L. Yin, “Static and dynamic
    3D facial expression recognition: A comprehensive survey,” *Image and Vision Computing*,
    vol. 30, no. 10, pp. 683–697, 2012.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] G. Sandbach, S. Zafeiriou, M. Pantic 和 L. Yin, “静态和动态3D面部表情识别：综合调查”，*《图像与视觉计算》*，第30卷，第10期，第683–697页，2012年。'
- en: '[202] P. Ekman and W. Friesen, “Nonverbal leakage and clues to deception,”
    *Study Interpers*, vol. 32, pp. 88–106, 1969.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] P. Ekman 和 W. Friesen, “非语言泄露与欺骗线索”，*《Study Interpers》*，第32卷，第88–106页，1969年。'
- en: '[203] Y. Chen and J. Joo, “Understanding and mitigating annotation bias in
    facial expression recognition,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 14 980–14 991.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Y. Chen 和 J. Joo, “理解和缓解面部表情识别中的标注偏差”，在 *《IEEE/CVF国际计算机视觉会议论文集》*，2021年，第14,980–14,991页。'
- en: '[204] H. Chen, X. Liu, X. Li, H. Shi, and G. Zhao, “Analyze spontaneous gestures
    for emotional stress state recognition: A micro-gesture dataset and analysis with
    deep learning,” in *2019 14th IEEE International Conference on Automatic Face
    and Gesture Recognition (FG)*.   IEEE, 2019, pp. 1–8.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] H. Chen, X. Liu, X. Li, H. Shi 和 G. Zhao, “分析自发手势以识别情感压力状态：一个微手势数据集和深度学习分析”，在
    *《2019年第14届IEEE国际自动面部和姿态识别会议（FG）》*。IEEE, 2019，第1–8页。'
- en: '[205] A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik,
    A. Barbado, S. García, S. Gil-López, D. Molina, R. Benjamins *et al.*, “Explainable
    artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges
    toward responsible ai,” *Information fusion*, vol. 58, pp. 82–115, 2020.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik,
    A. Barbado, S. García, S. Gil-López, D. Molina, R. Benjamins *等*，“可解释的人工智能（XAI）：概念、分类、机遇和面向负责任AI的挑战”，*《信息融合》*，第58卷，第82–115页，2020年。'
- en: '[206] M. Kosinski, “Facial recognition technology can expose political orientation
    from naturalistic facial images,” *Scientific reports*, vol. 11, no. 1, pp. 1–7,
    2021.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] M. Kosinski, “面部识别技术可以从自然面部图像中揭示政治取向”，*《科学报告》*，第11卷，第1期，第1–7页，2021年。'
- en: '[207] S. Zuboff, *The age of surveillance capitalism: The fight for a human
    future at the new frontier of power: Barack Obama’s books of 2019*.   Profile
    books, 2019.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] S. Zuboff, *《监控资本主义时代：在权力新前沿争取人类未来：巴拉克·奥巴马的2019年书籍》*。Profile books, 2019。'
- en: '[208] I. D. Raji, T. Gebru, M. Mitchell, J. Buolamwini, J. Lee, and E. Denton,
    “Saving face: Investigating the ethical concerns of facial recognition auditing,”
    in *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society*, 2020,
    pp. 145–151.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] I. D. Raji, T. Gebru, M. Mitchell, J. Buolamwini, J. Lee 和 E. Denton,
    “挽救面子：调查面部识别审计的伦理问题”，在 *《AAAI/ACM人工智能、伦理和社会会议论文集》*，2020年，第145–151页。'
- en: '[209] S. Oviatt, “Technology as infrastructure for dehumanization: three hundred
    million people with the same face,” in *Proceedings of the 2021 International
    Conference on Multimodal Interaction*, 2021, pp. 278–287.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] S. Oviatt, “技术作为非人化基础设施：三亿人有着相同的面孔”，在 *《2021年国际多模态交互会议论文集》*，2021年，第278–287页。'
- en: '[210] P. Voigt and A. Von dem Bussche, “The eu general data protection regulation
    (gdpr),” *A Practical Guide, 1st Ed., Cham: Springer International Publishing*,
    vol. 10, no. 3152676, pp. 10–5555, 2017.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] P. Voigt 和 A. Von dem Bussche, “欧盟通用数据保护条例（GDPR）”，*《实用指南，第1版，Cham: Springer
    International Publishing》*，第10卷，第3152676期，第10–5555页，2017年。'
- en: '[211] H. Proença, “The uu-net: Reversible face de-identification for visual
    surveillance video footage,” *IEEE Transactions on Circuits and Systems for Video
    Technology*, vol. 32, no. 2, pp. 496–509, 2021.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] H. Proença, “UU-Net：可逆面部去身份化用于视觉监控视频”，*《IEEE电路与系统视频技术汇刊》*，第32卷，第2期，第496–509页，2021年。'
- en: '[212] B. C. Stahl, “Ethical issues of ai,” in *Artificial Intelligence for
    a Better Future*.   Springer, 2021, pp. 35–53.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] B. C. Stahl, “人工智能的伦理问题”，在 *《为了更美好未来的人工智能》*。Springer, 2021，第35–53页。'
