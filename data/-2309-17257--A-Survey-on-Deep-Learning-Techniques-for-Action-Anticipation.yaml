- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:36:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:36:55'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2309.17257] A Survey on Deep Learning Techniques for Action Anticipation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2309.17257] 行为预测深度学习技术综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.17257](https://ar5iv.labs.arxiv.org/html/2309.17257)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.17257](https://ar5iv.labs.arxiv.org/html/2309.17257)
- en: A Survey on Deep Learning Techniques for Action Anticipation
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行为预测深度学习技术综述
- en: Zeyun Zhong, Manuel Martin, Michael Voit, Juergen Gall, Jürgen Beyerer Zeyun
    Zhong, Jürgen Beyerer are with the Karlsruhe Institute of Technology (KIT), Germany.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zeyun Zhong, Manuel Martin, Michael Voit, Juergen Gall, Jürgen Beyerer Zeyun
    Zhong, Jürgen Beyerer 隶属于德国卡尔斯鲁厄理工学院（KIT）。
- en: 'E-mail: firstname.lastname@kit.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'E-mail: firstname.lastname@kit.edu'
- en: Zeyun Zhong, Manuel Martin, Michael Voit, Jürgen Beyerer are with the Fraunhofer
    Institute of Optronics, System Technologies and Image Exploitation IOSB, Germany.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Zeyun Zhong, Manuel Martin, Michael Voit, Jürgen Beyerer 隶属于德国弗劳恩霍夫光电学、系统技术与图像利用研究所（IOSB）。
- en: 'E-mail: firstname.lastname@iosb.fraunhofer.de'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 'E-mail: firstname.lastname@iosb.fraunhofer.de'
- en: Juergen Gall is with the University of Bonn and the Lamarr Institute for Machine
    Learning and Artificial Intelligence, Germany.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Juergen Gall 隶属于波恩大学和德国拉玛尔机器学习与人工智能研究所。
- en: 'E-mail: gall@iai.uni-bonn.de'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 'E-mail: gall@iai.uni-bonn.de'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The ability to anticipate possible future human actions is essential for a wide
    range of applications, including autonomous driving and human-robot interaction.
    Consequently, numerous methods have been introduced for action anticipation in
    recent years, with deep learning-based approaches being particularly popular.
    In this work, we review the recent advances of action anticipation algorithms
    with a particular focus on daily-living scenarios. Additionally, we classify these
    methods according to their primary contributions and summarize them in tabular
    form, allowing readers to grasp the details at a glance. Furthermore, we delve
    into the common evaluation metrics and datasets used for action anticipation and
    provide future directions with systematical discussions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 预测未来人类行为的能力对广泛应用至关重要，包括自动驾驶和人机交互。因此，近年来提出了许多用于行为预测的方法，特别是基于深度学习的方法尤为流行。在这项工作中，我们回顾了行为预测算法的最新进展，特别关注日常生活场景。此外，我们根据这些方法的主要贡献对其进行分类，并以表格形式总结，使读者能够一目了然。此外，我们还深入探讨了用于行为预测的常见评估指标和数据集，并提供了系统化的未来方向讨论。
- en: 'Index Terms:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: action anticipation, activities of daily living, video understanding, deep learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 行为预测、日常生活活动、视频理解、深度学习。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Compared to human action recognition and early action recognition, where the
    entire or part of an action is observable, action anticipation aims to predict
    a future action without observing any part of it, as displayed in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Techniques for Action Anticipation").
    Anticipating possible future daily-living actions is one of the most important
    tasks for human machine cooperation and robotic assistance, e.g., to offer a hand
    at the right time or to generate a proactive dialog to provide more natural interactions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类行为识别和早期行为识别相比，后者观察到整个或部分行为，行为预测旨在在未观察到任何部分的情况下预测未来的行为，如图 [1](#S1.F1 "图 1
    ‣ 1 引言 ‣ 行为预测深度学习技术综述")所示。预测可能的未来日常生活行为是人机合作和机器人辅助中最重要的任务之一，例如，在适当的时间提供帮助或生成主动对话以提供更自然的互动。
- en: As the future actions are often not deterministic, this tends to be significantly
    more challenging than the traditional action recognition task, where today’s well-honed
    discriminative models [[1](#bib.bib1), [2](#bib.bib2)] perform very well. The
    predictability of actions varies based on the nature of the tasks involved, as
    shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Learning
    Techniques for Action Anticipation"). In the realm of predefined processes with
    less variability, such as industrial processes and medical operations [[3](#bib.bib3)],
    actions tend to be highly predictable due to the strict adherence to established
    protocols and guidelines. On the other hand, predefined processes with large variability,
    like cooking, introduce a degree of unpredictability as they involve subjective
    decision-making and varying ingredients, which can influence the final outcome.
    Actions stimulated by the environment or other people, exemplified by opening
    the door when the doorbell rings, exhibit moderate predictability, as they are
    influenced by external cues, but the responses are often ingrained through habitual
    patterns. Lastly, spontaneous behavior represents the highest level of unpredictability,
    as it arises without explicit stimuli or established patterns, making it challenging
    to anticipate.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于未来的动作通常不是确定性的，这往往比传统的动作识别任务更具挑战性，而今天的成熟**判别模型** [[1](#bib.bib1), [2](#bib.bib2)]
    表现非常出色。动作的可预测性根据涉及的任务性质而有所不同，如图[2](#S1.F2 "图 2 ‣ 1 引言 ‣ 关于动作预测的深度学习技术综述")所示。在变异性较小的预定义过程中，如工业过程和医疗操作
    [[3](#bib.bib3)]，由于严格遵守既定的协议和指南，动作通常是高度可预测的。另一方面，变异性较大的预定义过程，如烹饪，引入了不确定性，因为这些过程涉及主观决策和不同的原料，这些因素会影响最终结果。由环境或他人刺激的动作，如听到门铃响时开门，表现出中等的可预测性，因为这些动作受外部提示的影响，但回应通常是通过习惯模式形成的。最后，自发行为代表了最高级别的不可预测性，因为它在没有明确刺激或既定模式的情况下产生，使得预测变得困难。
- en: Recently, the computer vision research community has shown significant interest
    in addressing this challenging task. In line with action recognition, anticipation
    approaches began with predictions based on a single video frame [[4](#bib.bib4)]
    and tended to use longer temporal context in more recent works [[5](#bib.bib5),
    [6](#bib.bib6)]. In addition to utilizing a longer action history, many approaches
    are exploring the incorporation of multiple modalities beyond the raw RGB video
    frames to further improve the anticipatory ability. These include optical flow,
    which contains motion information, as well as objects present in the scene, among
    others.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，计算机视觉研究社区对解决这一挑战性任务表现出了极大的兴趣。与动作识别相一致，预测方法最初基于单帧视频 [[4](#bib.bib4)]，而最近的工作则倾向于使用更长的时间上下文
    [[5](#bib.bib5), [6](#bib.bib6)]。除了利用更长的动作历史，许多方法正在探索融合多种模态的信息，以进一步提高预测能力。这些模态包括包含运动信息的光流，以及场景中的物体等。
- en: '![Refer to caption](img/1718b2ac9b69fea627b01a544512e523.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1718b2ac9b69fea627b01a544512e523.png)'
- en: 'Figure 1: The action anticipation task aims to anticipate future actions before
    they happen, whereas action recognition and early action recognition require the
    observation of complete and partial actions, respectively. The example frames
    are from the EpicKitchens [[7](#bib.bib7)] dataset.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 动作预测任务旨在预测未来的动作，而动作识别和早期动作识别分别需要观察完整动作和部分动作。示例帧来自 EpicKitchens [[7](#bib.bib7)]
    数据集。'
- en: '![Refer to caption](img/e089cbdaf73eb5c7e4bf5ea15f99fd0b.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e089cbdaf73eb5c7e4bf5ea15f99fd0b.png)'
- en: 'Figure 2: Predictability of actions.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 动作的可预测性。'
- en: Apart from the aforementioned action recognition and early action recognition
    task, there are several other tasks in the literature that are related to action
    anticipation, such as action segmentation [[8](#bib.bib8)], temporal/spatio-temporal
    action detection [[9](#bib.bib9)], video prediction [[10](#bib.bib10)], trajectory
    prediction [[11](#bib.bib11)], and motion prediction [[12](#bib.bib12)]. The definitions
    of action anticipation and its related tasks in computer vision are listed in
    Table [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ A Survey on Deep Learning Techniques
    for Action Anticipation"). In contrast to action anticipation, the related tasks
    are concerned with recognizing and understanding actions that have already occurred
    or are in progress. Despite these differences, these tasks complement each other,
    with action recognition and localization providing foundational components for
    action anticipation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述的动作识别和早期动作识别任务，文献中还有几个与动作预测相关的任务，如动作分割[[8](#bib.bib8)]、时间/时空动作检测[[9](#bib.bib9)]、视频预测[[10](#bib.bib10)]、轨迹预测[[11](#bib.bib11)]和动作预测[[12](#bib.bib12)]。计算机视觉中动作预测及其相关任务的定义列在表[I](#S1.T1
    "TABLE I ‣ 1 Introduction ‣ A Survey on Deep Learning Techniques for Action Anticipation")中。与动作预测不同，相关任务关注于识别和理解已经发生或正在进行的动作。尽管存在这些差异，这些任务是互补的，其中动作识别和定位为动作预测提供了基础组件。
- en: 'TABLE I: Definition of action anticipation and its related tasks.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 动作预测及相关任务的定义。'
- en: '| Task | Definition |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 定义 |'
- en: '| --- | --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Action Anticipation | Anticipates one or multiple future actions before they
    happen. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 动作预测 | 在动作发生之前预测一个或多个未来动作。 |'
- en: '| Action Recognition | Categorizes actions in a video or image sequence given
    the full observation. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 动作识别 | 根据完整的观察对视频或图像序列中的动作进行分类。 |'
- en: '| Early Action Recognition | Categorizes actions in a video or image sequence
    given the partial observation. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 早期动作识别 | 根据部分观察对视频或图像序列中的动作进行分类。 |'
- en: '| Action Segmentation | Categorizes actions for every frame of the video. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 动作分割 | 对视频的每一帧中的动作进行分类。 |'
- en: '| Temporal Action Detection | Identifies and localizes actions within a video
    by predicting the start and end times of actions. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 时间动作检测 | 通过预测动作的开始和结束时间来识别和定位视频中的动作。'
- en: '| Spatio-temporal Action Detection | Identifies and localizes actions in both
    spatial and temporal dimensions within a video. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 时空动作检测 | 在视频中的空间和时间维度上识别和定位动作。 |'
- en: '| Video Prediction | Predicts future frames given a set of video frames. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 视频预测 | 给定一组视频帧，预测未来的帧。 |'
- en: '| Trajectory Prediction | Estimates the future trajectory or path of objects
    or agents in a scene. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 轨迹预测 | 估计场景中物体或代理的未来轨迹或路径。 |'
- en: '| Motion Prediction | Predicts changes in human pose. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 动作预测 | 预测人体姿态的变化。 |'
- en: 1.1 Application Domains
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 应用领域
- en: While exploring the plausible future is well studied in other fields, such as
    weather forecasting [[13](#bib.bib13)] and stock price prediction [[14](#bib.bib14)],
    researchers have only recently become heavily active in exploring solutions in
    the computer vision community. As such, action anticipation approaches have been
    applied for diverse applications across various domains in recent years, such
    as industrial applications [[15](#bib.bib15)], robotics [[16](#bib.bib16)], advanced
    driver assistance systems [[17](#bib.bib17)], autonomous driving [[18](#bib.bib18)],
    and more.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在天气预报[[13](#bib.bib13)]和股票价格预测[[14](#bib.bib14)]等其他领域，探索可能的未来已经得到很好的研究，但研究人员最近才在计算机视觉领域积极探索相关解决方案。因此，近年来动作预测方法已经在工业应用[[15](#bib.bib15)]、机器人技术[[16](#bib.bib16)]、高级驾驶员辅助系统[[17](#bib.bib17)]、自动驾驶[[18](#bib.bib18)]等多个领域得到了广泛应用。
- en: In the industrial domain like robotized warehouses, action anticipation approaches
    can be used to infer the intention of workers to improve efficiency and safety [[15](#bib.bib15),
    [19](#bib.bib19)]. In the field of human-robot-interaction, in contrast to passive
    service, e.g., providing service only after being spoken to, robots are expected
    to provide proactive service that initiates an interaction at an early stage.
    In this context, the ability of accurately predicting the start of an interaction [[20](#bib.bib20)]
    or the object that the user is going to interact with [[16](#bib.bib16), [21](#bib.bib21),
    [22](#bib.bib22)] enables robots to respond naturally and intuitively to human
    actions, improving collaboration. Anticipation can also find application in generating
    notifications for our daily routines, such as reminding us to turn off lights
    before leaving a room or the stove after cooking [[23](#bib.bib23)]. Moreover,
    the safety of users, especially care recipients, stands to be significantly improved
    if robots can assess the risk of the current environment [[24](#bib.bib24)]. In
    driving scenarios, anticipating a dangerous maneuver beforehand can alert drivers
    before they perform the maneuver and can also give driver assistance systems more
    time to avoid or prepare for the danger [[17](#bib.bib17)]. In autonomous driving,
    intelligent vehicles rely on predictive abilities to anticipate the actions of
    other road users in urban environments, particularly pedestrians at crosswalks [[18](#bib.bib18)]
    or potential traffic accidents [[25](#bib.bib25)], ensuring safe and efficient
    navigation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在像机器人化仓库这样的工业领域，动作预测方法可以用来推断工人的意图，从而提高效率和安全性[[15](#bib.bib15), [19](#bib.bib19)]。在人机交互领域，与被动服务（例如，仅在被问及后才提供服务）相比，机器人预计要提供主动服务，在早期阶段主动发起互动。在这种背景下，准确预测互动开始[[20](#bib.bib20)]或用户将要互动的对象[[16](#bib.bib16),
    [21](#bib.bib21), [22](#bib.bib22)]使机器人能够自然直观地响应人类行为，从而改善协作。预测还可以应用于生成日常生活的通知，例如在离开房间前提醒我们关灯，或在烹饪后关闭炉子[[23](#bib.bib23)]。此外，如果机器人能够评估当前环境的风险，用户，特别是照顾对象的安全性将显著提高[[24](#bib.bib24)]。在驾驶场景中，提前预测危险操作可以在司机执行操作前警示他们，并且可以给驾驶辅助系统更多时间来避免或准备应对危险[[17](#bib.bib17)]。在自动驾驶中，智能车辆依靠预测能力来预见城市环境中其他道路使用者的行为，特别是在斑马线上的行人[[18](#bib.bib18)]或潜在的交通事故[[25](#bib.bib25)]，以确保安全高效的导航。
- en: 1.2 Review Scope and Terminology
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 复习范围与术语
- en: There are several surveys in the literature that discuss action anticipation
    as part of their scope [[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)]. However, these surveys
    do not provide a full overview of the action anticipation literature. While [[27](#bib.bib27),
    [28](#bib.bib28), [30](#bib.bib30)] consider several topics and action anticipation
    is only briefly discussed among other topics, [[29](#bib.bib29), [31](#bib.bib31)]
    focus on the specific challenges of egocentric vision. [[26](#bib.bib26)], being
    one of the earlier works, primarily delves into methods that are not deep learning-based.
    In this survey, our objective is to review and classify the research field of
    action anticipation in general. This includes an overview of the current state-of-the-art
    approaches as well as datasets and evaluation metrics. We conclude our review
    by identifying future research challenges and by summarizing our findings.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中有几个调查研究讨论了动作预测作为其范围的一部分[[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)]。然而，这些调查并未提供动作预测文献的完整概述。虽然[[27](#bib.bib27),
    [28](#bib.bib28), [30](#bib.bib30)]考虑了几个主题，且动作预测仅在其他主题中简要讨论，而[[29](#bib.bib29),
    [31](#bib.bib31)]专注于自我中心视觉的特定挑战。作为早期的工作之一，[[26](#bib.bib26)]主要深入探讨了非深度学习基础的方法。在这项调查中，我们的目标是总体回顾和分类动作预测的研究领域。这包括对当前最先进方法的概述，以及数据集和评估指标。我们通过识别未来的研究挑战并总结我们的发现来结束我们的回顾。
- en: Although classical learning approaches, such as models based on the Markovian
    assumption, including Probabilistic Suffix Tree (PST) [[32](#bib.bib32), [33](#bib.bib33)],
    Hidden Markov Model (HMM) [[17](#bib.bib17)], Conditional Random Fields (CRFs) [[16](#bib.bib16)],
    Markov Decision Process (MDP) [[34](#bib.bib34)], and other statistical methods [[35](#bib.bib35),
    [36](#bib.bib36)], have been widely used in the literature, we put our focus on
    deep learning techniques and how they have been extended or applied to action
    anticipation, leaving the classical approaches outside the scope of the present
    review. In this context, the terms action anticipation, action prediction, and
    action forecasting are used interchangeably. Note that we do not explicitly differentiate
    between the terms action and activity in this survey, since these two terms are
    commonly used interchangeably in the literature.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管经典的学习方法，如基于马尔可夫假设的模型，包括概率后缀树（PST）[[32](#bib.bib32), [33](#bib.bib33)], 隐马尔可夫模型（HMM）[[17](#bib.bib17)],
    条件随机场（CRFs）[[16](#bib.bib16)], 马尔可夫决策过程（MDP）[[34](#bib.bib34)]以及其他统计方法[[35](#bib.bib35),
    [36](#bib.bib36)], 在文献中被广泛使用，我们将重点放在深度学习技术及其在动作预测中的扩展或应用上，将经典方法排除在本综述的范围之外。在这种情况下，动作预测、动作预报和动作预测这些术语是可以互换使用的。请注意，在本综述中我们没有明确区分“动作”和“活动”这两个术语，因为这两个术语在文献中通常是互换使用的。
- en: 2 Problem Statement
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题陈述
- en: 'In Section [2.1](#S2.SS1 "2.1 Video feature encoding ‣ 2 Problem Statement
    ‣ A Survey on Deep Learning Techniques for Action Anticipation"), we introduce
    video feature encoding, a foundational step for action anticipation. Based on
    the predicted time horizon, action anticipation approaches can be grouped into
    two categories: short-term anticipation approaches and long-term anticipation
    approaches. Short-term approaches usually operate on subsymbolic sensory data
    and predict actions a few seconds into the future. Conversely, long-term approaches
    may utilize action history with a higher abstraction level to predict a sequence
    of future actions (with their durations) up to several minutes into the future.
    Below, we present the detailed task definitions commonly used in the literature
    for both categories in Section [2.2](#S2.SS2 "2.2 Short-term anticipation ‣ 2
    Problem Statement ‣ A Survey on Deep Learning Techniques for Action Anticipation")
    and Section [2.3](#S2.SS3 "2.3 Long-term anticipation ‣ 2 Problem Statement ‣
    A Survey on Deep Learning Techniques for Action Anticipation"), respectively.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[2.1节](#S2.SS1 "2.1 视频特征编码 ‣ 2 问题陈述 ‣ 动作预测深度学习技术综述")中，我们介绍了视频特征编码，这是动作预测的基础步骤。根据预测的时间范围，动作预测方法可以分为两类：短期预测方法和长期预测方法。短期方法通常在子符号传感器数据上操作，并预测未来几秒的动作。相反，长期方法可能利用具有更高抽象级别的动作历史来预测未来几分钟内的一系列动作（及其持续时间）。下面，我们在第[2.2节](#S2.SS2
    "2.2 短期预测 ‣ 2 问题陈述 ‣ 动作预测深度学习技术综述")和第[2.3节](#S2.SS3 "2.3 长期预测 ‣ 2 问题陈述 ‣ 动作预测深度学习技术综述")中分别介绍了文献中常用的详细任务定义。
- en: 2.1 Video feature encoding
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 视频特征编码
- en: Untrimmed videos are typically used to evaluate action anticipation approaches.
    They can be as long as several minutes, and contain several actions. It is therefore
    difficult to directly input the entire video to a visual encoder for feature extraction
    due to the limits of computational resources. A common strategy for video representation
    is to partition the video into equally sized temporal intervals called snippets,
    and then apply a pre-trained feature extractor over each snippet.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 未剪辑的视频通常用于评估动作预测方法。这些视频可能长达几分钟，并包含多个动作。因此，由于计算资源的限制，直接将整个视频输入视觉编码器以提取特征是困难的。一种常见的视频表示策略是将视频划分为大小相等的时间间隔，称为片段，然后在每个片段上应用预训练的特征提取器。
- en: More formally, let the observed video segment be denoted by $V$ that contains
    $T$ frames $\{x_{1},x_{2},\dotsc,x_{T}\}$, corresponding to $i$ actions $\{a^{1},a^{2},\dotsc,a^{i}\}$,
    where $i$ is usually much smaller than $T$. Then, the video segment is broken
    down into $N$ snippets {$V_{1},V_{2},\dotsc,V_{N}$}, with each snippet $V_{k}$
    containing $n$ video frames $\{x_{k1},x_{k2},\dotsc,x_{kn}\}$, where the exact
    number of frames $n$ depends on the frame rate of the videos and the used feature
    extractor. Afterwards, a feature extractor is applied on these snippets to extract
    a sequence of video representations {$z_{1},z_{2},\dotsc,z_{N}$}.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地，让观察到的视频片段用$V$表示，其中包含$T$帧$\{x_{1},x_{2},\dotsc,x_{T}\}$，对应于$i$个动作$\{a^{1},a^{2},\dotsc,a^{i}\}$，其中$i$通常远小于$T$。然后，视频片段分解成$N$个片段{$V_{1},V_{2},\dotsc,V_{N}$}，每个片段$V_{k}$包含$n$视频帧$\{x_{k1},x_{k2},\dotsc,x_{kn}\}$，其中帧数$n$取决于视频的帧率和所使用的特征提取器。然后，在这些片段上应用特征提取器以提取视频表示的序列{$z_{1},z_{2},\dotsc,z_{N}$}。
- en: Common extractors range from frame-level spatial models, such as VGG16 [[37](#bib.bib37)],
    ResNet50 [[38](#bib.bib38)], TSN [[39](#bib.bib39)] and ViT [[40](#bib.bib40)],
    to snippet-level spatiotemporal models, such as I3D [[41](#bib.bib41)], Two-stream [[42](#bib.bib42)],
    R(2+1)D [[43](#bib.bib43)], MViT [[44](#bib.bib44)], and Swin [[2](#bib.bib2)].
    The features extracted from pre-trained encoders, which are typically trained
    for trimmed action recognition tasks, are not necessarily suitable for action
    anticipation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的提取器包括帧级空间模型，如VGG16 [[37](#bib.bib37)]，ResNet50 [[38](#bib.bib38)]，TSN [[39](#bib.bib39)]和ViT
    [[40](#bib.bib40)]，以及片段级时空模型，如I3D [[41](#bib.bib41)]，Two-stream [[42](#bib.bib42)]，R(2+1)D
    [[43](#bib.bib43)]，MViT [[44](#bib.bib44)]和Swin [[2](#bib.bib2)]。从预训练的编码器中提取的特征通常用于修剪动作识别任务，并不一定适用于动作预测。
- en: 2.2 Short-term anticipation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 短期预测
- en: '![Refer to caption](img/1065cb18dfae6d2bf3ab2111dcfad2c0.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1065cb18dfae6d2bf3ab2111dcfad2c0.png)'
- en: 'Figure 3: Category of the action anticipation task. Short-term anticipation
    aims to predict actions at several future time steps, whereas the long-term task
    aims to predict subsequent actions (along with their durations) without adhering
    to fixed time steps.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：动作预测任务的类别。短期预测旨在预测未来几个时间步的动作，而长期任务旨在预测随后的动作（以及它们的持续时间），而不是固定的时间步长。
- en: Most short-term anticipation approaches follow the setup defined in [[4](#bib.bib4),
    [45](#bib.bib45), [7](#bib.bib7)]. Approaches tackling the short-term anticipation
    task typically operate on the sequence of video representations extracted by a
    pre-trained feature extractor, as introduced in Section [2.1](#S2.SS1 "2.1 Video
    feature encoding ‣ 2 Problem Statement ‣ A Survey on Deep Learning Techniques
    for Action Anticipation"). Taking these representations with the same temporal
    spacing as input, short-term approaches predict a few representations $M$ time
    steps into the future $\{\hat{z}_{N+1},\hat{z}_{N+2},\dotsc,\hat{z}_{N+M}\}$,
    which are then classified into actions $\{\hat{a}_{N+1},\hat{a}_{N+2},\dotsc,\hat{a}_{N+M}\}$,
    as illustrated in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Short-term anticipation ‣
    2 Problem Statement ‣ A Survey on Deep Learning Techniques for Action Anticipation").
    The number of time steps into the future $M$ depends on the anticipation protocol
    and the temporal spacing of the video representations. For instance, $M$ would
    be 4 if the temporal spacing were set at 0.25 seconds and the chosen protocol
    aimed to predict an action in 1 second.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数短期预测方法遵循[[4](#bib.bib4), [45](#bib.bib45), [7](#bib.bib7)]定义的设置。处理短期预测任务的方法通常在由预先训练的特征提取器提取的视频表示序列上操作，如[2.1](#S2.SS1
    "2.1 视频特征编码 ‣ 2 问题陈述 ‣ 深度学习技术对动作预测的调查")节介绍的那样。采用相同时间间隔的这些表示作为输入，短期方法预测未来$M$时间步的一些表示$\{\hat{z}_{N+1},\hat{z}_{N+2},\dotsc,\hat{z}_{N+M}\}$，然后将其分类为动作$\{\hat{a}_{N+1},\hat{a}_{N+2},\dotsc,\hat{a}_{N+M}\}$，如图[3](#S2.F3
    "图3 ‣ 2.2 短期预测 ‣ 2 问题陈述 ‣ 深度学习技术对动作预测的调查")所示。未来时间步$M$的数量取决于预测协议和视频表示的时间间隔。例如，如果时间间隔设置为0.25秒，所选协议旨在1秒内预测一个动作，则$M$将为4。
- en: 2.3 Long-term anticipation
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 长期预测
- en: Parallel research addresses the long-term anticipation task [[46](#bib.bib46),
    [47](#bib.bib47)]. The goal is to anticipate the category (and the duration) of
    future actions for a given time horizon, which can extend up to several minutes,
    as illustrated in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Short-term anticipation ‣ 2
    Problem Statement ‣ A Survey on Deep Learning Techniques for Action Anticipation").
    In contrast to short-term approaches, long-term approaches are not confined to
    using subsymbolic video representations as their input. Certain methods leverage
    past actions, represented as $\{a^{1},a^{2},\dotsc,a^{i}\}$, to directly forecast
    a sequence of future actions, $\{\hat{a}^{i+1},\hat{a}^{i+2},\dotsc,\hat{a}^{i+j}\}$ [[46](#bib.bib46),
    [48](#bib.bib48), [49](#bib.bib49)]. These past actions could either be based
    on actual observations or derived from an action segmentation technique [[50](#bib.bib50),
    [51](#bib.bib51)]. Approaches that tackle long-term anticipation typically rely
    on fully-labeled data, i.e., sequences labeled with all future actions and their
    durations. Notably, some methods operate within a weakly supervised setting, using
    only a few fully labeled sequences and primarily sequences where only the next
    action is labeled [[52](#bib.bib52)].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 平行研究解决了长期预测任务[[46](#bib.bib46), [47](#bib.bib47)]。目标是在给定的时间范围内预测未来动作的类别（和持续时间），该时间范围可以延续到几分钟，如图[3](#S2.F3
    "图 3 ‣ 2.2 短期预测 ‣ 2 问题陈述 ‣ 基于深度学习的动作预测技术综述")所示。与短期方法不同，长期方法不受限于使用子符号视频表示作为输入。某些方法利用过去的动作，表示为$\{a^{1},a^{2},\dotsc,a^{i}\}$，以直接预测未来动作序列$\{\hat{a}^{i+1},\hat{a}^{i+2},\dotsc,\hat{a}^{i+j}\}$[[46](#bib.bib46),
    [48](#bib.bib48), [49](#bib.bib49)]。这些过去的动作可以基于实际观察，也可以从动作分割技术中推导得出[[50](#bib.bib50),
    [51](#bib.bib51)]。处理长期预测的方法通常依赖于完全标注的数据，即标注了所有未来动作及其持续时间的序列。值得注意的是，一些方法在弱监督环境下操作，仅使用少量完全标注的序列以及主要是仅标注了下一个动作的序列[[52](#bib.bib52)]。
- en: 3 Methods
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 种方法
- en: Although the predictive capability of machines could enable many real-world
    applications in robotics and manufacturing, developing an algorithm to anticipate
    the future action of the user is challenging. Humans rely on extensive knowledge
    accumulated over their lifetime to infer what will happen next. However, how machines
    can gain such knowledge remains an open question. In this section, we describe
    the methods that are trying to tackle such a challenging task. To our knowledge,
    the current literature does not provide a specific taxonomy to classify action
    anticipation models. In this review, we therefore classify the existing methods
    into five groups, according to the specific problem they addressed, as shown in
    Fig. [4](#S3.F4 "Figure 4 ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for
    Action Anticipation").
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器的预测能力可能会在机器人和制造领域启用许多实际应用，但开发一种预测用户未来动作的算法仍具有挑战性。人类依赖于一生积累的大量知识来推测接下来会发生什么。然而，机器如何获得这种知识仍然是一个未解之谜。在本节中，我们描述了尝试解决这一挑战性任务的方法。据我们所知，当前文献未提供特定的分类法来对动作预测模型进行分类。因此，在本综述中，我们将现有的方法分为五类，依据它们所解决的具体问题，如图[4](#S3.F4
    "图 4 ‣ 3 种方法 ‣ 基于深度学习的动作预测技术综述")所示。
- en: '![Refer to caption](img/4a4a8401c8e7ca435429e8292b5db551.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/4a4a8401c8e7ca435429e8292b5db551.png)'
- en: 'Figure 4: Classification of action anticipation methods.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：动作预测方法的分类。
- en: Leveraging the inherent temporal dynamics of videos as a learning signal, some
    methods build self-supervised frameworks to train the anticipation models (Section [3.1](#S3.SS1
    "3.1 Leveraging temporal dynamics as a learning signal ‣ 3 Methods ‣ A Survey
    on Deep Learning Techniques for Action Anticipation")). Early methods anticipate
    future actions based on the representation of a single past frame, ignoring all
    temporal dynamics. To accurately predict the actions in the future, the evolution
    of the past actions should be analyzed and summarized. Consequently, recent approaches
    use various architectures to encode the long-term history, including recurrent
    and transformer-based models (Section [3.2](#S3.SS2 "3.2 Utilizing long-term temporal
    history ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation")).
    Some methods attempt to derive and utilize additional modalities¹¹1We refer to
    both different sensory data (e.g., RGB and audio) and different feature representations
    (e.g., RGB and optical flow) as modalities for the sake of emphasizing the difference
    in data representations. to improve the anticipative performance, such as presence
    of objects in the scene, object bounding boxes, and optical flow, as they may
    complement the raw RGB video frames. In addition to the data acquired from the
    input video, other information sources, e.g., personalization, the topology of
    the action sequences, or the underlying intention, may also contribute to improved
    future prediction (Section [3.3](#S3.SS3 "3.3 Narrowing anticipation space by
    using additional information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation")). As the vast majority of action anticipation models
    are deterministic, they do not deal with the uncertainty of the future. To address
    this issue, several authors proposed modeling uncertainty for future prediction
    (Section [3.4](#S3.SS4 "3.4 Incorporating uncertainty ‣ 3 Methods ‣ A Survey on
    Deep Learning Techniques for Action Anticipation")). Most action anticipation
    methods in the literature operate on the representation-level, i.e., future video
    representations are first anticipated and then categorized to actions, as illustrated
    in the upper part of Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Short-term anticipation ‣
    2 Problem Statement ‣ A Survey on Deep Learning Techniques for Action Anticipation").
    In contrast, some methods predict future actions directly based on the observed
    action sequences (Section [3.5](#S3.SS5 "3.5 Modeling at a more conceptual level
    ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation")),
    which may provide a clean analysis of action dependencies and better explainability.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 利用视频固有的时间动态作为学习信号，一些方法构建了自我监督框架来训练预测模型（第[3.1节](#S3.SS1 "3.1 Leveraging temporal
    dynamics as a learning signal ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation")）。早期的方法基于单个过去帧的表示来预测未来动作，忽略了所有时间动态。为了准确预测未来的动作，应分析和总结过去动作的演变。因此，最近的方法使用各种架构来编码长期历史，包括递归和基于变压器的模型（第[3.2节](#S3.SS2
    "3.2 Utilizing long-term temporal history ‣ 3 Methods ‣ A Survey on Deep Learning
    Techniques for Action Anticipation")）。一些方法试图推导和利用额外的模态¹¹1我们将不同的感官数据（例如，RGB和音频）以及不同的特征表示（例如，RGB和光流）称为模态，以强调数据表示之间的差异。来提高预测性能，例如场景中的物体存在、物体边界框和光流，因为它们可能补充原始的RGB视频帧。除了从输入视频中获得的数据外，其他信息来源，例如个性化、动作序列的拓扑结构或潜在意图，也可能有助于提高未来预测（第[3.3节](#S3.SS3
    "3.3 Narrowing anticipation space by using additional information ‣ 3 Methods
    ‣ A Survey on Deep Learning Techniques for Action Anticipation")）。由于绝大多数动作预测模型是确定性的，它们不处理未来的不确定性。为了解决这个问题，几位作者提出了对未来预测建模不确定性的方法（第[3.4节](#S3.SS4
    "3.4 Incorporating uncertainty ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation")）。文献中的大多数动作预测方法在表示层面上操作，即首先预测未来的视频表示，然后将其分类为动作，如图[3](#S2.F3
    "Figure 3 ‣ 2.2 Short-term anticipation ‣ 2 Problem Statement ‣ A Survey on Deep
    Learning Techniques for Action Anticipation")上部所示。相比之下，一些方法直接基于观察到的动作序列预测未来动作（第[3.5节](#S3.SS5
    "3.5 Modeling at a more conceptual level ‣ 3 Methods ‣ A Survey on Deep Learning
    Techniques for Action Anticipation")），这可能提供对动作依赖关系的清晰分析和更好的可解释性。
- en: Table [II](#S3.T2 "TABLE II ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation") shows an overview of the most relevant methods, ordered
    in chronological order. We characterize these approaches by their neural network
    architecture, datasets used for training and testing, and technical details, including
    modalities fed into the individual model, the fusion strategy (if any), incorporation
    of external knowledge, the abstraction level on which a prediction is made, and
    whether the model is addressing the long-term or short-term anticipation tasks
    introduced in Section [2](#S2 "2 Problem Statement ‣ A Survey on Deep Learning
    Techniques for Action Anticipation"). Although most short-term methods can perform
    long-term anticipation by iteratively applying the prediction module, we characterize
    each method based on the experiments performed in the original work only. Moreover,
    we provide the information whether the model was trained end-to-end, or if it
    relied on pre-extracted features. Learning from features allows to save computational
    resources during training. On the other hand, the ability to learn or fine-tune
    a task-speciﬁc representation end-to-end may be beneficial depending on the specific
    task. Furthermore, pre-extracted features are not available in a real-life streaming
    scenario, so models trained from features should also consider the impact of feature
    extraction at application time. Note that the taxonomy is not mutually exclusive,
    as some methods can be classified into several categories since they address multiple
    goals. For instance, [[48](#bib.bib48), [53](#bib.bib53)] address both action-level
    prediction and prediction uncertainty. We specify the category of these models
    according to their main contribution.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [II](#S3.T2 "TABLE II ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for
    Action Anticipation") 显示了最相关方法的概览，按时间顺序排列。我们通过神经网络架构、用于训练和测试的数据集以及技术细节来表征这些方法，包括输入单个模型的模态、融合策略（如有）、外部知识的引入、预测所依据的抽象层次，以及模型是否处理第
    [2](#S2 "2 Problem Statement ‣ A Survey on Deep Learning Techniques for Action
    Anticipation") 节中介绍的长期或短期预测任务。虽然大多数短期方法可以通过迭代应用预测模块来执行长期预测，但我们仅根据原始工作的实验对每种方法进行表征。此外，我们提供了模型是否进行了端到端训练的信息，或者是否依赖于预提取的特征。从特征中学习可以在训练过程中节省计算资源。另一方面，根据具体任务，能够端到端地学习或微调任务特定表示可能是有益的。此外，在真实的流媒体场景中，预提取的特征不可用，因此从特征中训练的模型还应考虑特征提取在应用时的影响。请注意，分类法并不是互斥的，因为一些方法可以被归类为多个类别，因为它们涉及多个目标。例如，[[48](#bib.bib48),
    [53](#bib.bib53)] 涉及动作级预测和预测不确定性。我们根据这些模型的主要贡献指定其类别。
- en: 'TABLE II: Summary of most relevant action anticipation models. The following
    abbreviations are used in the table. Architecture: Transformer (TF), Non-local
    Block (NLB). Modalities: Objects (O), Bounding Boxes (BB), Motion (M), Action
    Classes (A), Audio (Au), Gaze (G), Hand Mask (H), Human-Object-Interaction (HOI),
    Time (T). Fusion: Score Fusion (score), Late feature fusion (late), Mid-level
    feature fusion (mid). EK: external knowledge. LT: Long-term anticipation. E2E:
    End-to-end. Because of space constraints, we have omitted the inclusion of object
    detectors such as Fast R-CNN [[54](#bib.bib54)] and Mask R-CNN [[55](#bib.bib55)]
    as feature extractors for approaches that rely on object or human-object interaction
    modalities.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 主要动作预测模型的总结。表中使用了以下缩写。架构：Transformer (TF)，非本地块 (NLB)。模态：对象 (O)，边界框 (BB)，运动
    (M)，动作类别 (A)，音频 (Au)，注视 (G)，手部遮罩 (H)，人-物体互动 (HOI)，时间 (T)。融合：分数融合 (score)，后期特征融合
    (late)，中层特征融合 (mid)。EK：外部知识。LT：长期预测。E2E：端到端。由于空间限制，我们省略了像 Fast R-CNN [[54](#bib.bib54)]
    和 Mask R-CNN [[55](#bib.bib55)] 这样的物体检测器作为依赖于物体或人-物体互动模态的方法的特征提取器。'
- en: '| Method | Year | Feature extractor | Future predictor | Datasets | Details
    |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 特征提取器 | 未来预测器 | 数据集 | 详细信息 |  |'
- en: '| Modalities | Fusion | EK | Abst. | LT | E2E |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 模态 | 融合 | EK | 抽象 | LT | E2E |'
- en: '| Leveraging temporal dynamics as a learning signal |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 利用时间动态作为学习信号 |'
- en: '| Vondrick et al. [[4](#bib.bib4)] | 2016 | AlexNet[[56](#bib.bib56)] | MLP
    | [[57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59)] | RGB | – | ✗ | feat.
    | ✗ | ✓ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Vondrick 等人 [[4](#bib.bib4)] | 2016 | AlexNet[[56](#bib.bib56)] | MLP | [[57](#bib.bib57),
    [58](#bib.bib58), [59](#bib.bib59)] | RGB | – | ✗ | 特征 | ✗ | ✓ |'
- en: '| Zeng et al. [[60](#bib.bib60)] | 2017 | ResNet[[38](#bib.bib38)] | MLP |
    [[57](#bib.bib57), [59](#bib.bib59)] | RGB | – | ✗ | feat. | ✗ | ✓ |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Zeng 等人 [[60](#bib.bib60)] | 2017 | ResNet[[38](#bib.bib38)] | MLP | [[57](#bib.bib57),
    [59](#bib.bib59)] | RGB | – | ✗ | 特征 | ✗ | ✓ |'
- en: '| RED [[45](#bib.bib45)] | 2017 | VGG[[37](#bib.bib37)],TS[[61](#bib.bib61)],
    | LSTM | [[62](#bib.bib62), [57](#bib.bib57), [59](#bib.bib59)] | RGB,M | mid
    | ✗ | feat. | ✗ | ✗ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| RED [[45](#bib.bib45)] | 2017 | VGG[[37](#bib.bib37)],TS[[61](#bib.bib61)],
    | LSTM | [[62](#bib.bib62), [57](#bib.bib57), [59](#bib.bib59)] | RGB,M | mid
    | ✗ | feat. | ✗ | ✗ |'
- en: '| Zhong et al. [[63](#bib.bib63)] | 2018 | ResNet [[38](#bib.bib38)] | LSTM
    | [[57](#bib.bib57), [59](#bib.bib59)] | RGB | – | ✗ | feat. | ✗ | ✓ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Zhong et al. [[63](#bib.bib63)] | 2018 | ResNet [[38](#bib.bib38)] | LSTM
    | [[57](#bib.bib57), [59](#bib.bib59)] | RGB | – | ✗ | feat. | ✗ | ✓ |'
- en: '| Tran et al. [[64](#bib.bib64)] | 2021 | I3D [[41](#bib.bib41)] | – | [[7](#bib.bib7)]
    | RGB | – | ✗ | feat. | ✗ | ✗ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Tran et al. [[64](#bib.bib64)] | 2021 | I3D [[41](#bib.bib41)] | – | [[7](#bib.bib7)]
    | RGB | – | ✗ | feat. | ✗ | ✗ |'
- en: '| Fernando et al. [[65](#bib.bib65)] | 2021 | R(2+1)D [[43](#bib.bib43)] |
    FC | [[7](#bib.bib7), [66](#bib.bib66)] | RGB,O,M | score | ✗ | feat.,act. | ✗
    | ✓ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Fernando et al. [[65](#bib.bib65)] | 2021 | R(2+1)D [[43](#bib.bib43)] |
    FC | [[7](#bib.bib7), [66](#bib.bib66)] | RGB,O,M | score | ✗ | feat.,act. | ✗
    | ✓ |'
- en: '| AVT [[67](#bib.bib67)] | 2021 | TSN [[39](#bib.bib39)],ViT [[40](#bib.bib40)]
    | TF | [[7](#bib.bib7), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)]
    | RGB,O | score | ✗ | feat. | ✗ | ✓ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| AVT [[67](#bib.bib67)] | 2021 | TSN [[39](#bib.bib39)],ViT [[40](#bib.bib40)]
    | TF | [[7](#bib.bib7), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)]
    | RGB,O | score | ✗ | feat. | ✗ | ✓ |'
- en: '| DCR [[71](#bib.bib71)] | 2022 | TSN [[39](#bib.bib39)],TSM [[72](#bib.bib72)]
    | LSTM,TF | [[7](#bib.bib7), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)]
    | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| DCR [[71](#bib.bib71)] | 2022 | TSN [[39](#bib.bib39)],TSM [[72](#bib.bib72)]
    | LSTM,TF | [[7](#bib.bib7), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)]
    | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | MViT [[44](#bib.bib44)] | TF | [[7](#bib.bib7),
    [68](#bib.bib68), [69](#bib.bib69)] | RGB | – | ✗ | feat. | ✗ | ✗ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| RAFTformer [[73](#bib.bib73)] | 2023 | MViT [[44](#bib.bib44)] | TF | [[7](#bib.bib7),
    [68](#bib.bib68), [69](#bib.bib69)] | RGB | – | ✗ | feat. | ✗ | ✗ |'
- en: '| Utilizing long-term temporal history |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 利用长期时间历史 |'
- en: '| Gammulle et al. [[74](#bib.bib74)] | 2019 | ResNet [[38](#bib.bib38)] | LSTM
    | [[70](#bib.bib70), [66](#bib.bib66)] | RGB,A | mid | ✗ | feat. | ✓ | ✗ |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Gammulle et al. [[74](#bib.bib74)] | 2019 | ResNet [[38](#bib.bib38)] | LSTM
    | [[70](#bib.bib70), [66](#bib.bib66)] | RGB,A | mid | ✗ | feat. | ✓ | ✗ |'
- en: '| TRN [[75](#bib.bib75)] | 2019 | TS [[61](#bib.bib61)] | LSTM | [[59](#bib.bib59),
    [62](#bib.bib62)] | RGB,M | mid | ✗ | feat. | ✗ | ✗ |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| TRN [[75](#bib.bib75)] | 2019 | TS [[61](#bib.bib61)] | LSTM | [[59](#bib.bib59),
    [62](#bib.bib62)] | RGB,M | mid | ✗ | feat. | ✗ | ✗ |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | TSN [[39](#bib.bib39)],I3D [[41](#bib.bib41)]
    | NLB,LSTM | [[7](#bib.bib7), [70](#bib.bib70), [66](#bib.bib66)] | RGB,O,BB,M
    | score | ✗ | feat. | ✓ | ✗ |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | TSN [[39](#bib.bib39)],I3D [[41](#bib.bib41)]
    | NLB,LSTM | [[7](#bib.bib7), [70](#bib.bib70), [66](#bib.bib66)] | RGB,O,BB,M
    | score | ✗ | feat. | ✓ | ✗ |'
- en: '| TTPP [[76](#bib.bib76)] | 2020 | VGG[[37](#bib.bib37)],TS[[61](#bib.bib61)]
    | TF,MLP | [[62](#bib.bib62), [57](#bib.bib57), [59](#bib.bib59)] | RGB,M | mid
    | ✗ | feat. | ✗ | ✗ |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| TTPP [[76](#bib.bib76)] | 2020 | VGG[[37](#bib.bib37)],TS[[61](#bib.bib61)]
    | TF,MLP | [[62](#bib.bib62), [57](#bib.bib57), [59](#bib.bib59)] | RGB,M | mid
    | ✗ | feat. | ✗ | ✗ |'
- en: '| LAP [[77](#bib.bib77)] | 2020 | TS [[61](#bib.bib61)] | GRU | [[59](#bib.bib59),
    [62](#bib.bib62)] | RGB,M | mid | ✗ | feat. | ✗ | ✗ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| LAP [[77](#bib.bib77)] | 2020 | TS [[61](#bib.bib61)] | GRU | [[59](#bib.bib59),
    [62](#bib.bib62)] | RGB,M | mid | ✗ | feat. | ✗ | ✗ |'
- en: '| ImagineRNN [[78](#bib.bib78)] | 2021 | TSN [[39](#bib.bib39)] | LSTM | [[7](#bib.bib7),
    [69](#bib.bib69)] | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ImagineRNN [[78](#bib.bib78)] | 2021 | TSN [[39](#bib.bib39)] | LSTM | [[7](#bib.bib7),
    [69](#bib.bib69)] | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
- en: '| LSTR [[79](#bib.bib79)] | 2021 | TS [[61](#bib.bib61)] | TF | [[59](#bib.bib59),
    [62](#bib.bib62)] | RGB,M | mid | ✗ | feat. | ✗ | ✗ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| LSTR [[79](#bib.bib79)] | 2021 | TS [[61](#bib.bib61)] | TF | [[59](#bib.bib59),
    [62](#bib.bib62)] | RGB,M | mid | ✗ | feat. | ✗ | ✗ |'
- en: '| OadTR [[80](#bib.bib80)] | 2021 | TS [[61](#bib.bib61)] | TF | [[62](#bib.bib62),
    [59](#bib.bib59)] | RGB,M | mid | ✗ | feat. | ✗ | ✗ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| OadTR [[80](#bib.bib80)] | 2021 | TS [[61](#bib.bib61)] | TF | [[62](#bib.bib62),
    [59](#bib.bib59)] | RGB,M | mid | ✗ | feat. | ✗ | ✗ |'
- en: '| HRO [[81](#bib.bib81)] | 2022 | TSN [[39](#bib.bib39)] | GRU | [[7](#bib.bib7),
    [69](#bib.bib69)] | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| HRO [[81](#bib.bib81)] | 2022 | TSN [[39](#bib.bib39)] | GRU | [[7](#bib.bib7),
    [69](#bib.bib69)] | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
- en: '| MeMViT [[6](#bib.bib6)] | 2022 | MViT [[44](#bib.bib44)] | – | [[68](#bib.bib68)]
    | RGB | – | ✗ | feat. | ✗ | ✓ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| MeMViT [[6](#bib.bib6)] | 2022 | MViT [[44](#bib.bib44)] | – | [[68](#bib.bib68)]
    | RGB | – | ✗ | feat. | ✗ | ✓ |'
- en: '| FUTR [[82](#bib.bib82)] | 2022 | I3D [[41](#bib.bib41)] | TF | [[70](#bib.bib70),
    [66](#bib.bib66)] | RGB,M | mid | ✗ | feat. | ✓ | ✗ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| FUTR [[82](#bib.bib82)] | 2022 | I3D [[41](#bib.bib41)] | TF | [[70](#bib.bib70),
    [66](#bib.bib66)] | RGB,M | mid | ✗ | feat. | ✓ | ✗ |'
- en: '| TeSTra [[83](#bib.bib83)] | 2022 | TS [[61](#bib.bib61)],TSN [[39](#bib.bib39)]
    | TF | [[59](#bib.bib59), [68](#bib.bib68)] | RGB,M | mid | ✗ | feat. | ✗ | ✗
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| TeSTra [[83](#bib.bib83)] | 2022 | TS [[61](#bib.bib61)],TSN [[39](#bib.bib39)]
    | TF | [[59](#bib.bib59), [68](#bib.bib68)] | RGB,M | mid | ✗ | feat. | ✗ | ✗
    |'
- en: '| Multi-modal fusion |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 多模态融合 |'
- en: '| Zhou et al. [[84](#bib.bib84)] | 2015 | VGG [[37](#bib.bib37)],TS [[42](#bib.bib42)]
    | MLP | [[85](#bib.bib85), [84](#bib.bib84)] | RGB,O,M | late | ✗ | feat. | ✗
    | ✗ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Zhou et al. [[84](#bib.bib84)] | 2015 | VGG [[37](#bib.bib37)],TS [[42](#bib.bib42)]
    | MLP | [[85](#bib.bib85), [84](#bib.bib84)] | RGB,O,M | late | ✗ | feat. | ✗
    | ✗ |'
- en: '| Mahmud et al. [[86](#bib.bib86)] | 2017 | C3D [[87](#bib.bib87)] | LSTM,MLP
    | [[88](#bib.bib88), [89](#bib.bib89)] | O,M | late | ✗ | feat. | ✗ | ✗ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Mahmud et al. [[86](#bib.bib86)] | 2017 | C3D [[87](#bib.bib87)] | LSTM,MLP
    | [[88](#bib.bib88), [89](#bib.bib89)] | O,M | late | ✗ | feat. | ✗ | ✗ |'
- en: '| Shen et al. [[90](#bib.bib90)] | 2018 | [[91](#bib.bib91)],SSD [[92](#bib.bib92)]
    | LSTM | [[93](#bib.bib93), [94](#bib.bib94)] | O,BB,G,H | late | ✗ | feat. |
    ✗ | ✗ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Shen et al. [[90](#bib.bib90)] | 2018 | [[91](#bib.bib91)],SSD [[92](#bib.bib92)]
    | LSTM | [[93](#bib.bib93), [94](#bib.bib94)] | O,BB,G,H | late | ✗ | feat. |
    ✗ | ✗ |'
- en: '| Liang et al. [[95](#bib.bib95)] | 2019 | [[96](#bib.bib96)] | LSTM | [[97](#bib.bib97)]
    | RGB,HOI | late | ✗ | feat. | ✗ | ✓ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Liang et al. [[95](#bib.bib95)] | 2019 | [[96](#bib.bib96)] | LSTM | [[97](#bib.bib97)]
    | RGB,HOI | late | ✗ | feat. | ✗ | ✓ |'
- en: '| RULSTM [[98](#bib.bib98)] | 2019 | TSN [[39](#bib.bib39)] | LSTM | [[7](#bib.bib7),
    [69](#bib.bib69)] | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| RULSTM [[98](#bib.bib98)] | 2019 | TSN [[39](#bib.bib39)] | LSTM | [[7](#bib.bib7),
    [69](#bib.bib69)] | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
- en: '| FHOI [[99](#bib.bib99)] | 2020 | I3D [[41](#bib.bib41)],CSN [[100](#bib.bib100)]
    | FC | [[7](#bib.bib7), [69](#bib.bib69)] | RGB,O | score | ✗ | feat. | ✗ | ✓
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| FHOI [[99](#bib.bib99)] | 2020 | I3D [[41](#bib.bib41)],CSN [[100](#bib.bib100)]
    | FC | [[7](#bib.bib7), [69](#bib.bib69)] | RGB,O | score | ✗ | feat. | ✗ | ✓
    |'
- en: '| Ego-OMG [[101](#bib.bib101)] | 2021 | CSN [[87](#bib.bib87)],GCN [[102](#bib.bib102)]
    | LSTM | [[7](#bib.bib7)] | RGB | score | ✗ | feat. | ✗ | ✗ |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Ego-OMG [[101](#bib.bib101)] | 2021 | CSN [[87](#bib.bib87)],GCN [[102](#bib.bib102)]
    | LSTM | [[7](#bib.bib7)] | RGB | score | ✗ | feat. | ✗ | ✗ |'
- en: '| Zatsarynna et al. [[103](#bib.bib103)] | 2021 | TSN [[39](#bib.bib39)] |
    TCN | [[7](#bib.bib7), [68](#bib.bib68)] | RGB,O,M | late | ✗ | feat. | ✗ | ✗
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Zatsarynna et al. [[103](#bib.bib103)] | 2021 | TSN [[39](#bib.bib39)] |
    TCN | [[7](#bib.bib7), [68](#bib.bib68)] | RGB,O,M | late | ✗ | feat. | ✗ | ✗
    |'
- en: '| Roy et al. [[104](#bib.bib104)] | 2021 | I3D [[41](#bib.bib41)] | TF | [[7](#bib.bib7),
    [70](#bib.bib70), [66](#bib.bib66)] | RGB,HOI,M | score | ✗ | feat. | ✗ | ✗ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Roy et al. [[104](#bib.bib104)] | 2021 | I3D [[41](#bib.bib41)] | TF | [[7](#bib.bib7),
    [70](#bib.bib70), [66](#bib.bib66)] | RGB,HOI,M | score | ✗ | feat. | ✗ | ✗ |'
- en: '| AFFT [[105](#bib.bib105)] | 2023 | TSN [[39](#bib.bib39)],Swin [[106](#bib.bib106)]
    | TF | [[68](#bib.bib68), [69](#bib.bib69)] | RGB,O,M,Au | mid | ✗ | feat. | ✗
    | ✗ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| AFFT [[105](#bib.bib105)] | 2023 | TSN [[39](#bib.bib39)],Swin [[106](#bib.bib106)]
    | TF | [[68](#bib.bib68), [69](#bib.bib69)] | RGB,O,M,Au | mid | ✗ | feat. | ✗
    | ✗ |'
- en: '| Conditioning on extra information |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 额外信息条件 |'
- en: '| S-RNN [[107](#bib.bib107)] | 2016 | [[108](#bib.bib108)] | GNN,LSTM | [[108](#bib.bib108)]
    | RGB,O,BB | mid | ✗ | feat. | ✗ | ✗ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| S-RNN [[107](#bib.bib107)] | 2016 | [[108](#bib.bib108)] | GNN,LSTM | [[108](#bib.bib108)]
    | RGB,O,BB | mid | ✗ | feat. | ✗ | ✗ |'
- en: '| DR²N [[109](#bib.bib109)] | 2019 | S3D [[110](#bib.bib110)] | GNN,GRU | [[111](#bib.bib111)]
    | RGB | – | ✗ | feat.,act. | ✗ | ✓ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| DR²N [[109](#bib.bib109)] | 2019 | S3D [[110](#bib.bib110)] | GNN,GRU | [[111](#bib.bib111)]
    | RGB | – | ✗ | feat.,act. | ✗ | ✓ |'
- en: '| Farha et al. [[112](#bib.bib112)] | 2020 | I3D [[41](#bib.bib41)] | TCN,GRU
    | [[70](#bib.bib70), [66](#bib.bib66)] | RGB,M | mid | ✗ | feat. | ✓ | ✗ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Farha et al. [[112](#bib.bib112)] | 2020 | I3D [[41](#bib.bib41)] | TCN,GRU
    | [[70](#bib.bib70), [66](#bib.bib66)] | RGB,M | mid | ✗ | feat. | ✓ | ✗ |'
- en: '| Camporese et al. [[113](#bib.bib113)] | 2020 | TSN [[39](#bib.bib39)] | LSTM
    | [[7](#bib.bib7), [69](#bib.bib69)] | RGB,O,M | score | ✓ | feat. | ✗ | ✗ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Camporese et al. [[113](#bib.bib113)] | 2020 | TSN [[39](#bib.bib39)] | LSTM
    | [[7](#bib.bib7), [69](#bib.bib69)] | RGB,O,M | score | ✓ | feat. | ✗ | ✗ |'
- en: '| RESTEP [[114](#bib.bib114)] | 2021 | I3D [[41](#bib.bib41)] | ConvGRU | [[111](#bib.bib111)]
    | RGB | – | ✗ | feat. | ✗ | ✗ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| RESTEP [[114](#bib.bib114)] | 2021 | I3D [[41](#bib.bib41)] | ConvGRU | [[111](#bib.bib111)]
    | RGB | – | ✗ | feat. | ✗ | ✗ |'
- en: '| A-ACT [[115](#bib.bib115)] | 2022 | TSN [[39](#bib.bib39)],I3D [[41](#bib.bib41)]
    | TF,MLP | [[66](#bib.bib66), [70](#bib.bib70), [7](#bib.bib7)] | RGB,O,M | score
    | ✗ | feat.,act. | ✗ | ✗ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| A-ACT [[115](#bib.bib115)] | 2022 | TSN [[39](#bib.bib39)],I3D [[41](#bib.bib41)]
    | TF,MLP | [[66](#bib.bib66), [70](#bib.bib70), [7](#bib.bib7)] | RGB,O,M | score
    | ✗ | feat.,act. | ✗ | ✗ |'
- en: '| Abst. goal [[116](#bib.bib116)] | 2022 | TSN [[39](#bib.bib39)] | GRU | [[7](#bib.bib7),
    [68](#bib.bib68), [69](#bib.bib69)] | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 抽象目标 [[116](#bib.bib116)] | 2022 | TSN [[39](#bib.bib39)] | GRU | [[7](#bib.bib7),
    [68](#bib.bib68), [69](#bib.bib69)] | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
- en: '| Anticipatr [[117](#bib.bib117)] | 2022 | I3D [[41](#bib.bib41)] | TF | [[70](#bib.bib70),
    [66](#bib.bib66), [69](#bib.bib69), [7](#bib.bib7)] | RGB | – | ✗ | feat. | ✓
    | ✗ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Anticipatr [[117](#bib.bib117)] | 2022 | I3D [[41](#bib.bib41)] | TF | [[70](#bib.bib70),
    [66](#bib.bib66), [69](#bib.bib69), [7](#bib.bib7)] | RGB | – | ✗ | feat. | ✓
    | ✗ |'
- en: '| Mascaro et al. [[118](#bib.bib118)] | 2023 | SlowFast [[1](#bib.bib1)] |
    MLP,VAE,TF | [[47](#bib.bib47)] | RGB | – | ✗ | feat. | ✓ | ✗ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Mascaro 等 [[118](#bib.bib118)] | 2023 | SlowFast [[1](#bib.bib1)] | MLP,VAE,TF
    | [[47](#bib.bib47)] | RGB | – | ✗ | feat. | ✓ | ✗ |'
- en: '| AntGPT [[119](#bib.bib119)] | 2023 | ViT [[40](#bib.bib40)] | TF | [[47](#bib.bib47),
    [7](#bib.bib7), [69](#bib.bib69)] | RGB | – | ✓ | act. | ✓ | ✗ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| AntGPT [[119](#bib.bib119)] | 2023 | ViT [[40](#bib.bib40)] | TF | [[47](#bib.bib47),
    [7](#bib.bib7), [69](#bib.bib69)] | RGB | – | ✓ | act. | ✓ | ✗ |'
- en: '| Incorporating uncertainty |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 融入不确定性'
- en: '| Schydlo et al. [[21](#bib.bib21)] | 2018 | [[120](#bib.bib120)] | LSTM |
    [[108](#bib.bib108)] | P | – | ✗ | feat. | ✗ | ✗ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Schydlo 等 [[21](#bib.bib21)] | 2018 | [[120](#bib.bib120)] | LSTM | [[108](#bib.bib108)]
    | P | – | ✗ | feat. | ✗ | ✗ |'
- en: '| Farha et al. [[48](#bib.bib48)] | 2019 | – | GRU | [[70](#bib.bib70), [66](#bib.bib66)]
    | A,T | mid | ✗ | act. | ✓ | ✗ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Farha 等 [[48](#bib.bib48)] | 2019 | – | GRU | [[70](#bib.bib70), [66](#bib.bib66)]
    | A,T | mid | ✗ | act. | ✓ | ✗ |'
- en: '| APP-VAE [[53](#bib.bib53)] | 2019 | – | LSTM,VAE | [[66](#bib.bib66), [121](#bib.bib121)]
    | A,T | mid | ✗ | act. | ✓ | ✗ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| APP-VAE [[53](#bib.bib53)] | 2019 | – | LSTM,VAE | [[66](#bib.bib66), [121](#bib.bib121)]
    | A,T | mid | ✗ | act. | ✓ | ✗ |'
- en: '| Ng et al. [[122](#bib.bib122)] | 2020 | I3D [[41](#bib.bib41)] | GRU | [[123](#bib.bib123),
    [66](#bib.bib66), [89](#bib.bib89), [70](#bib.bib70)] | RGB | – | ✗ | feat. |
    ✓ | ✗ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Ng 等 [[122](#bib.bib122)] | 2020 | I3D [[41](#bib.bib41)] | GRU | [[123](#bib.bib123),
    [66](#bib.bib66), [89](#bib.bib89), [70](#bib.bib70)] | RGB | – | ✗ | feat. |
    ✓ | ✗ |'
- en: '| AGG [[124](#bib.bib124)] | 2020 | – | TCN,MLP,GAN | [[123](#bib.bib123),
    [70](#bib.bib70), [121](#bib.bib121)] | A | – | ✗ | act. | ✓ | ✗ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| AGG [[124](#bib.bib124)] | 2020 | – | TCN,MLP,GAN | [[123](#bib.bib123),
    [70](#bib.bib70), [121](#bib.bib121)] | A | – | ✗ | act. | ✓ | ✗ |'
- en: '| Zhao et al. [[49](#bib.bib49)] | 2020 | – | LSTM,GAN | [[66](#bib.bib66),
    [70](#bib.bib70), [7](#bib.bib7)] | A,T | mid | ✗ | act. | ✓ | ✗ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Zhao 等 [[49](#bib.bib49)] | 2020 | – | LSTM,GAN | [[66](#bib.bib66), [70](#bib.bib70),
    [7](#bib.bib7)] | A,T | mid | ✗ | act. | ✓ | ✗ |'
- en: '| Modeling at a more conceptual level |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 在更高层次上建模'
- en: '| Farha et al. [[46](#bib.bib46)] | 2018 | – | CNN,GRU | [[70](#bib.bib70),
    [66](#bib.bib66)] | A,T | mid | ✗ | act. | ✓ | ✗ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Farha 等 [[46](#bib.bib46)] | 2018 | – | CNN,GRU | [[70](#bib.bib70), [66](#bib.bib66)]
    | A,T | mid | ✗ | act. | ✓ | ✗ |'
- en: '| Miech et al. [[125](#bib.bib125)] | 2019 | R(2+1)D [[43](#bib.bib43)] | FC
    | [[7](#bib.bib7), [66](#bib.bib66), [126](#bib.bib126)] | RGB | – | ✗ | feat.,act.
    | ✗ | ✗ |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Miech 等 [[125](#bib.bib125)] | 2019 | R(2+1)D [[43](#bib.bib43)] | FC | [[7](#bib.bib7),
    [66](#bib.bib66), [126](#bib.bib126)] | RGB | – | ✗ | feat.,act. | ✗ | ✗ |'
- en: '| Ke et al. [[127](#bib.bib127)] | 2019 | I3D [[41](#bib.bib41)] | TCN | [[7](#bib.bib7),
    [70](#bib.bib70), [66](#bib.bib66)] | RGB,T | mid | ✗ | act. | ✓ | ✗ |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Ke 等 [[127](#bib.bib127)] | 2019 | I3D [[41](#bib.bib41)] | TCN | [[7](#bib.bib7),
    [70](#bib.bib70), [66](#bib.bib66)] | RGB,T | mid | ✗ | act. | ✓ | ✗ |'
- en: '| Zhang et al. [[128](#bib.bib128)] | 2020 | TSN [[39](#bib.bib39)] | LSTM,MLP
    | [[7](#bib.bib7)] | RGB,O,M | score | ✗ | feat.,act. | ✗ | ✗ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等 [[128](#bib.bib128)] | 2020 | TSN [[39](#bib.bib39)] | LSTM,MLP |
    [[7](#bib.bib7)] | RGB,O,M | score | ✗ | feat.,act. | ✗ | ✗ |'
- en: '| ProActive [[129](#bib.bib129)] | 2022 | – | TF | [[66](#bib.bib66), [126](#bib.bib126),
    [121](#bib.bib121)] | A,T | mid | ✓ | act. | ✓ | ✗ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ProActive [[129](#bib.bib129)] | 2022 | – | TF | [[66](#bib.bib66), [126](#bib.bib126),
    [121](#bib.bib121)] | A,T | mid | ✓ | act. | ✓ | ✗ |'
- en: 3.1 Leveraging temporal dynamics as a learning signal
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 利用时间动态作为学习信号
- en: Utilizing temporal dynamics via a self-supervised loss. One promising way to
    equip machines with the ability to infer future user actions is to use the abundantly
    available unlabeled videos on the Internet. These videos are economically feasible
    to obtain at massive scales and contain rich signals, particularly the inherent
    temporal ordering of frames. Some models leverage the temporal ordering and are
    trained by predicting the next video frame or motion image [[130](#bib.bib130)].
    However, the pixel space is high dimensional and extremely variable [[10](#bib.bib10)],
    which can pose additional challenges to the anticipation task. Consequently, many
    approaches consider the visual representation of future frames as their prediction
    target and learn to anticipate future representations by maximizing the correlation [[65](#bib.bib65),
    [131](#bib.bib131)] or minimizing the distance between predicted and actual future
    frame representations, e.g., in the form of a contrastive loss [[132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137)] or a simple MSE loss [[4](#bib.bib4), [45](#bib.bib45), [63](#bib.bib63),
    [67](#bib.bib67)].
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自监督损失利用时间动态。一种有前途的方法是利用互联网大量存在的未标记视频来赋予机器推断未来用户行为的能力。这些视频在大规模获取上经济可行，且包含丰富的信号，特别是帧的固有时间顺序。一些模型利用时间顺序，通过预测下一个视频帧或运动图像进行训练[[130](#bib.bib130)]。然而，像素空间高维且极其多变[[10](#bib.bib10)]，这可能给预测任务带来额外挑战。因此，许多方法将未来帧的视觉表示作为预测目标，并通过最大化相关性[[65](#bib.bib65),
    [131](#bib.bib131)]或最小化预测与实际未来帧表示之间的距离（例如，以对比损失的形式）[[132](#bib.bib132), [133](#bib.bib133),
    [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136), [137](#bib.bib137)]，或简单的MSE损失[[4](#bib.bib4),
    [45](#bib.bib45), [63](#bib.bib63), [67](#bib.bib67)]来学习预测未来的表示。
- en: Video representations capture the semantic information about actions and can
    be automatically computed using a pre-trained feature extractor, making them scalable
    to unlabeled videos. Building upon this idea, an initial work [[4](#bib.bib4)]
    built a deep regression network that takes a single frame as input and anticipates
    the future frame representation of a pre-trained AlexNet [[56](#bib.bib56)] (last
    hidden layer (fc7)). A single frame, however, does not provide much information
    about the past. To tackle this limitation, Zhong and Zheng [[63](#bib.bib63)]
    developed a two-stream network comprising a spatial stream and a temporal stream,
    following the learning objective from [[4](#bib.bib4)] (i.e., an MSE loss). While
    the spatial stream encoded the spatial context of the recent frame using a ResNet [[38](#bib.bib38)],
    the temporal stream summarized the historical information using a Long Short-Term
    Memory unit (LSTM [[138](#bib.bib138)]). To address the limited long-term modeling
    ability of LSTMs, the authors proposed employing a compact representation computed
    by a fully connected layer over all hidden states of past time steps. With the
    encoded spatial and temporal context, a fully connected layer was then employed
    to predict future representations. Through the incorporation of temporal information,
    this approach surpassed [[4](#bib.bib4)].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 视频表示捕捉了关于动作的语义信息，可以使用预训练的特征提取器自动计算，使其可以扩展到未标记视频。基于这一想法，初步工作[[4](#bib.bib4)]建立了一个深度回归网络，该网络以单帧作为输入，预测预训练AlexNet[[56](#bib.bib56)]（最后隐藏层(fc7)）的未来帧表示。然而，单帧并不能提供关于过去的信息。为了应对这一限制，Zhong和Zheng[[63](#bib.bib63)]开发了一个包括空间流和时间流的双流网络，遵循[[4](#bib.bib4)]的学习目标（即MSE损失）。空间流使用ResNet[[38](#bib.bib38)]对最近帧的空间上下文进行编码，而时间流则使用长短期记忆单元（LSTM[[138](#bib.bib138)]）总结历史信息。为了应对LSTM有限的长期建模能力，作者提出使用一个全连接层计算所有过去时间步的隐藏状态的紧凑表示。通过编码的空间和时间上下文，接着使用全连接层来预测未来的表示。通过整合时间信息，该方法超越了[[4](#bib.bib4)]。
- en: Another line of research is based on contrastive learning [[132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136)].
    The essence of the contrastive objective is to bring positive pairs closer together
    while distancing them from a large set of negative pairs. The definition of positive
    and negative pairs varies across these works. The Contrastive Predictive Coding
    (CPC) [[132](#bib.bib132)] and Dense Predictive Coding (DPC) [[133](#bib.bib133)]
    approaches share a common conceptual foundation, where their learning objectives
    were to predict coarse clip-level and fine-grained spatiotemporal region representations
    of future clips, given an observed sequence of clips for context, respectively.
    Han et al. [[134](#bib.bib134)] extended the principles of DPC by introducing
    a memory bank composed of learnable vectors to account for the non-deterministic
    nature of predicting the future. Suris et al. [[135](#bib.bib135)] integrated
    the hyperbolic geometry, specifically the Poincaré ball model, into the DPC framework
    to discern which features can be predicted from the data. Hyperbolic geometry,
    akin to a continuous tree [[139](#bib.bib139)], encodes hierarchical structures
    and facilitates predictions at multiple levels of abstraction. In cases of high
    confidence, specific actions are predicted, while in uncertain scenarios, higher-level
    abstractions are chosen. Zatsarynna et al. [[136](#bib.bib136)] proposed a composite
    loss for unintentional action prediction, which consists of a temporal contrastive
    loss that encourages proximity of neighboring video clips in the embedding space,
    and a pair-wise ordering loss that accounts for the relative order of video clips.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 另一条研究方向基于对比学习[[132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136)]。对比目标的本质是将正样本对拉近，同时将其与大量负样本对保持距离。正样本对和负样本对的定义在这些研究中各不相同。对比预测编码（CPC）[[132](#bib.bib132)]和密集预测编码（DPC）[[133](#bib.bib133)]方法具有共同的概念基础，它们的学习目标分别是预测未来剪辑的粗略剪辑级和细粒度时空区域表示，前提是给定一个观察到的剪辑序列作为上下文。Han等人[[134](#bib.bib134)]通过引入由可学习向量组成的记忆库，扩展了DPC的原理，以考虑预测未来的非确定性特征。Suris等人[[135](#bib.bib135)]将双曲几何，特别是庞加莱球模型，整合到DPC框架中，以辨别数据中可以预测的特征。双曲几何类似于一个连续的树[[139](#bib.bib139)]，它编码了层次结构，并促进了多个层次的抽象预测。在高置信度的情况下，预测特定动作，而在不确定的情况下，选择更高层次的抽象。Zatsarynna等人[[136](#bib.bib136)]提出了一种复合损失用于无意动作预测，其中包括一种时间对比损失，鼓励嵌入空间中相邻视频剪辑的接近，以及一种成对排序损失，考虑视频剪辑的相对顺序。
- en: 'In contrast to aforementioned approaches, where the classifier is trainable
    while the main part of the models remains fixed after pre-training, some approaches
    have demonstrated the effectiveness of fine-tuning on the target dataset for action
    anticipation [[45](#bib.bib45), [71](#bib.bib71), [131](#bib.bib131), [137](#bib.bib137)],
    especially for datasets where positive video segments containing ongoing actions
    are a very small part of the entire dataset. For instance, Gao et al. [[45](#bib.bib45)]
    proposed an encoder-decoder network (RED) based on LSTMs. Taking continuous steps
    of historical visual representations as input, the network output a sequence of
    anticipated future representations, which were then processed by a classification
    network for action classification. The architecture was trained in a two-stage
    process. In the first stage, self-supervised training using an MSE loss was performed
    on all training videos in the dataset. In the second stage, the initialized network
    was optimized with full supervision on positive samples from the videos. Moreover,
    the authors introduced a reinforcement learning module to optimize the network
    on the sequence level. For further details, please refer to Section [3.3.2](#S3.SS3.SSS2
    "3.3.2 Conditioning on extra information ‣ 3.3 Narrowing anticipation space by
    using additional information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation"). Similarly, Xu et al. [[71](#bib.bib71)] performed order-aware
    pre-training for the adopted transformer-based reasoning model [[140](#bib.bib140)],
    which proved beneficial for the final anticipation performance. Specifically,
    the observed video segment was sent into the transformer without positional encoding.
    The cosine similarity between transformer output tokens and the positional encoding
    was then computed and transformed into a probability using Softmax. To supervise
    the training, the authors proposed a similarity based on Gaussian affinity [[141](#bib.bib141)].
    The similarity $s_{p,q}$ of positional encoding at time $p$ and the frame feature
    at time $q$ is measured as:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述方法不同，某些方法展示了在目标数据集上进行微调的有效性[[45](#bib.bib45)、[71](#bib.bib71)、[131](#bib.bib131)、[137](#bib.bib137)]，特别是对于那些包含正向视频片段的整个数据集非常少的情况。例如，Gao
    等人[[45](#bib.bib45)]提出了一种基于 LSTM 的编码-解码网络（RED）。该网络以历史视觉表示的连续步骤作为输入，输出一系列预测的未来表示，然后由分类网络进行动作分类。该架构采用两阶段训练过程。在第一阶段，使用均方误差（MSE）损失对数据集中所有训练视频进行自监督训练。在第二阶段，利用正样本对初始化的网络进行完全监督优化。此外，作者引入了一个强化学习模块，以在序列层面优化网络。有关更多详细信息，请参见第[3.3.2](#S3.SS3.SSS2
    "3.3.2 Conditioning on extra information ‣ 3.3 Narrowing anticipation space by
    using additional information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation")节。类似地，Xu 等人[[71](#bib.bib71)]对所采用的基于变换器的推理模型[[140](#bib.bib140)]进行了顺序感知预训练，这对最终的预测性能证明了有益。具体来说，将观察到的视频片段输入到没有位置编码的变换器中。然后计算变换器输出标记与位置编码之间的余弦相似度，并通过
    Softmax 转换为概率。为了监督训练，作者提出了一种基于高斯亲和力的相似度[[141](#bib.bib141)]。位置编码在时间 $p$ 和帧特征在时间
    $q$ 的相似度 $s_{p,q}$ 被测量为：
- en: '|  | $s_{p,q}=\mathrm{exp}\biggl{(}-\frac{(p-q)^{2}}{\sigma^{2}}\biggr{)},$
    |  | (1) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{p,q}=\mathrm{exp}\biggl{(}-\frac{(p-q)^{2}}{\sigma^{2}}\biggr{)},$
    |  | (1) |'
- en: where $\sigma$ represents the bandwidth of the Gaussian and is typically set
    to a constant empirically.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma$ 代表高斯的带宽，通常根据经验设置为一个常数。
- en: Some approaches chose to incorporate a self-supervised loss in the training
    objective, often combined with a cross-entropy loss for action classification.
    Tran et al. [[64](#bib.bib64)] proposed supervising the training of the anticipation
    model by minimizing the distance between past and future feature maps, along with
    a cross-entropy loss. To address dynamics in videos, the authors introduced an
    attentional pooling operator to transform feature maps to location-agnostic representations
    for computing the squared loss. Fernando and Herath [[65](#bib.bib65)] proposed
    three novel similarity measures to supervise the training, maximizing the correlation
    between the predicted and actual future representations. They argued that commonly
    used similarity measures such as L2 distance and cosine similarity are not ideal,
    as they are either unbounded or discard the magnitude. Girdhar and Grauman [[67](#bib.bib67)]
    proposed a GPT2-based causal anticipation model [[142](#bib.bib142)] and employed
    the MSE loss for supervising intermediate future predictions. In contrast to predicting
    the next frame feature using the causal attention mask as done in [[67](#bib.bib67)],
    Girase et al. [[73](#bib.bib73)] proposed using a generalized form of masking-based
    self-supervision to predict shuffled future features, allowing for exponentially
    more variations than the vanilla causal masking scheme.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法选择在训练目标中加入自监督损失，通常与用于动作分类的交叉熵损失结合。Tran 等人[[64](#bib.bib64)] 提出了通过最小化过去和未来特征图之间的距离来监督预期模型的训练，并结合交叉熵损失。为了处理视频中的动态，作者引入了注意力池化操作符，将特征图转换为位置无关的表示，以计算平方损失。Fernando
    和 Herath [[65](#bib.bib65)] 提出了三种新颖的相似性度量来监督训练，最大化预测和实际未来表示之间的相关性。他们认为，常用的相似性度量如
    L2 距离和余弦相似度并不理想，因为它们要么无界，要么忽略了幅度。Girdhar 和 Grauman [[67](#bib.bib67)] 提出了基于 GPT2
    的因果预期模型[[142](#bib.bib142)]，并使用 MSE 损失来监督中间未来预测。与使用因果注意力掩码预测下一个帧特征的方法不同[[67](#bib.bib67)]，Girase
    等人 [[73](#bib.bib73)] 提出了使用掩码基础自监督的广义形式来预测打乱的未来特征，这允许比原始因果掩码方案更多的变化。
- en: Exploring the underlying dynamics. Some approaches relied on the adversarial
    training framework [[143](#bib.bib143)] to learn the underlying dynamics of videos [[60](#bib.bib60),
    [144](#bib.bib144), [124](#bib.bib124), [49](#bib.bib49)]. For instance, Zeng et al. [[60](#bib.bib60)]
    formulated the anticipation task as an inverse reinforcement learning (IRL) problem,
    where the goal was to imitate the behavior of natural sequences that are treated
    as expert demonstrations. More specifically, they leveraged the Generative Adversarial
    Imitation Learning framework [[145](#bib.bib145)] to bypass the exhaustive state-action
    pair visits, since both the state (frame) and action (transition) space were high-dimensional
    and continuous. In this framework, a discriminator, which aimed to distinguish
    the generated sequence from expert’s, and a generator (policy), guided by the
    discriminator to move toward expert-like regions, were jointly optimized during
    training.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 探索潜在的动态。一些方法依赖于对抗训练框架[[143](#bib.bib143)]来学习视频的潜在动态[[60](#bib.bib60), [144](#bib.bib144),
    [124](#bib.bib124), [49](#bib.bib49)]。例如，Zeng 等人 [[60](#bib.bib60)] 将预期任务表述为逆向强化学习（IRL）问题，目标是模仿自然序列的行为，这些序列被视为专家演示。更具体地说，他们利用生成对抗模仿学习框架[[145](#bib.bib145)]
    来绕过全面的状态-动作对访问，因为状态（帧）和动作（过渡）空间都是高维的连续的。在这个框架中，一个判别器旨在区分生成的序列和专家的序列，生成器（策略）在判别器的指导下朝着专家类似的区域移动，在训练过程中共同优化。
- en: 3.2 Utilizing long-term temporal history
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 利用长期时间历史
- en: Most actions exhibit dependencies on preceding actions. These preceding trigger
    actions may not necessarily be confined to the immediate past but can extend back
    to the long-term history. Consequently, many methods aim to exploit the information
    contained in the long-term history.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数动作表现出对前序动作的依赖。这些前序触发动作不一定仅限于最近的过去，还可以追溯到较长的历史。因此，许多方法旨在利用长期历史中的信息。
- en: Recurrent networks. Having an internal memory, recurrent networks, including
    LSTMs [[138](#bib.bib138)] and GRUs [[146](#bib.bib146)], are often adopted for
    exploiting the sequential action history [[45](#bib.bib45), [98](#bib.bib98),
    [75](#bib.bib75), [77](#bib.bib77)]. Qi et al. [[147](#bib.bib147)] employed GRUs
    for a recursive sequence prediction task. To tackle the challenge of error accumulation,
    they proposed a self-regulated learning framework that regulated the intermediate
    representation using a contrastive loss and a dynamic reweighing mechanism to
    attend to informative frames in the observed content. Inspired by the human visual
    cognitive system, where individuals often deduce current actions by envisioning
    future scenes concurrently [[148](#bib.bib148), [149](#bib.bib149)], Xu et al. [[75](#bib.bib75)]
    proposed the temporal recurrent network (TRN) to simultaneously perform online
    action detection and anticipation of the immediate future. This predicted future
    data was then harnessed to enhance the present action detection. While initially
    crafted for online action detection, TRN’s predictive capabilities led to its
    application in action anticipation. This concept of incorporating predicted future
    information for improving detection performance has further been followed by later
    works [[77](#bib.bib77), [80](#bib.bib80)]. Instead of using simply fixed future
    temporal ranges as in TRN, Qu et al. [[77](#bib.bib77)] argued that the optimal
    supplementary features should be obtained from distinct temporal ranges. They
    introduced an adaptive feature sampling strategy. More specifically, they initially
    inferred the distribution of the ongoing action progression by leveraging past
    frame representations. To sample a progression, they employed the Gumbel-Softmax
    trick [[150](#bib.bib150)], a technique that emulates one-hot vectors derived
    from categorical distributions. Subsequently, an equidistant sampling strategy
    was employed to acquire the desired supplementary features, encompassing both
    preceding and anticipated future features. The size of the sampling range was
    contingent upon the current progression of actions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 循环网络。具备内部记忆的循环网络，包括 LSTMs [[138](#bib.bib138)] 和 GRUs [[146](#bib.bib146)]，常用于利用序列化的动作历史
    [[45](#bib.bib45), [98](#bib.bib98), [75](#bib.bib75), [77](#bib.bib77)]。Qi 等人
    [[147](#bib.bib147)] 在递归序列预测任务中采用了 GRUs。为了解决误差积累的问题，他们提出了一种自我调节学习框架，该框架使用对比损失和动态重加权机制来调节中间表示，从而关注观察内容中的信息帧。受人类视觉认知系统的启发，人们常常通过同时想象未来场景来推断当前动作
    [[148](#bib.bib148), [149](#bib.bib149)]，Xu 等人 [[75](#bib.bib75)] 提出了时间递归网络（TRN），以同时进行在线动作检测和对即时未来的预期。这些预测的未来数据随后被用于增强当前的动作检测。尽管
    TRN 最初是为在线动作检测而设计的，其预测能力使得它被应用于动作预期。这种通过引入预测的未来信息来提高检测性能的概念，后来被进一步的研究所采用 [[77](#bib.bib77),
    [80](#bib.bib80)]。与 TRN 中简单使用固定未来时间范围不同，Qu 等人 [[77](#bib.bib77)] 认为最佳的补充特征应该来自不同的时间范围。他们引入了一种自适应特征采样策略。更具体地说，他们首先通过利用过去的帧表示来推断正在进行的动作进展的分布。为了采样进展，他们采用了
    Gumbel-Softmax 技巧 [[150](#bib.bib150)]，这是一种模拟从分类分布中得到的一热向量的技术。随后，采用等距采样策略来获取所需的补充特征，包括前期和预期未来的特征。采样范围的大小取决于当前的动作进展。
- en: Incorporating an external memory module. It is well known that recurrent networks
    struggle to model long-term dependencies due to their sequential nature. To enhance
    the long-term modeling ability of common recurrent architectures such as LSTMs,
    external memory modules were incorporated in [[74](#bib.bib74), [81](#bib.bib81)].
    Gammulle et al. [[74](#bib.bib74)] proposed using external neural memory networks
    to store the knowledge of the whole dataset. Specifically, they made use of two
    types of historical information, the observed frames and the corresponding action
    labels. When an input stimulus, i.e., a query vector generated by a read operation
    based on the input was present, the memory network generated an output based on
    the knowledge that persisted in the memory and updated itself with a write operation.
    By iteratively feeding the concatenated sequential outputs of each separate memory
    network of two information sources to an LSTM, a sequence of future action predictions
    was generated. Unlike in [[74](#bib.bib74)], the external memory module in Liu
    and Lam [[81](#bib.bib81)] (HRO) was fixed, i.e., it will not be updated when
    processing a sequence during inference. Furthermore, the memory module in HRO
    was used for predicting the representation at the next time step, rather than
    enhancing the representation at the current time step. To further regularize the
    learning process and improve the anticipation performance, the authors employed
    a contrastive learning paradigm, inspired by [[78](#bib.bib78)], and a transition
    layer to bridge the gap between the semantics of the past and the future.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 引入外部记忆模块。众所周知，由于递归网络的顺序特性，它们在建模长期依赖关系方面存在困难。为了增强常见递归架构（如LSTM）的长期建模能力，外部记忆模块被引入[[74](#bib.bib74),
    [81](#bib.bib81)]。Gammulle等人[[74](#bib.bib74)]提出使用外部神经记忆网络来存储整个数据集的知识。具体来说，他们利用了两种类型的历史信息：观察到的帧和对应的动作标签。当存在输入刺激，即基于输入生成的查询向量时，记忆网络根据存在于记忆中的知识生成输出，并通过写操作更新自己。通过将每个记忆网络的两个信息源的串联顺序输出迭代地输入到LSTM中，生成了一系列未来的动作预测。与[[74](#bib.bib74)]中的方法不同，Liu和Lam[[81](#bib.bib81)]（HRO）中的外部记忆模块是固定的，即在推理过程中处理序列时不会被更新。此外，HRO中的记忆模块用于预测下一个时间步骤的表示，而不是增强当前时间步骤的表示。为了进一步规范学习过程并提高预期性能，作者采用了对比学习范式（受[[78](#bib.bib78)]的启发）和一个过渡层，以弥合过去和未来语义之间的差距。
- en: Most anticipation models are designed to expect pre-extracted features as inputs,
    in order to leverage a longer temporal history while limiting computational cost
    during training. However, this also eliminates the benefit of fine-tuning a task-specific
    representation compared to end-to-end approaches, where feature extractors are
    also optimized during back-propagation. To train a fully end-to-end model and
    benefit from a longer action history without hitting the computation or memory
    bottlenecks, Wu et al. [[6](#bib.bib6)] proposed processing videos in an online
    fashion and caching memory. Videos were divided into consecutive clips and processed
    sequentially. Intermediate results, i.e., the key and value vectors of earlier
    time steps, were cached in the memory module, so that the query vector at the
    current time step had access not only to the recent past frames within the same
    clip but also to the past frames in earlier clips. This design differs from the
    aforementioned memory-based methods in that it does not contain any learnable
    parameters and thus no knowledge about the dataset.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数预期模型设计为接受预提取特征作为输入，以利用较长的时间历史，同时限制训练过程中的计算成本。然而，这也消除了与端到端方法相比，微调任务特定表示的好处，在端到端方法中，特征提取器在反向传播过程中也会被优化。为了训练一个完全端到端的模型，并在不遇到计算或内存瓶颈的情况下受益于更长的动作历史，Wu等人[[6](#bib.bib6)]提出了以在线方式处理视频并缓存记忆。视频被划分为连续的片段并按顺序处理。中间结果，即早期时间步骤的键和值向量，被缓存到记忆模块中，以便当前时间步骤的查询向量不仅可以访问同一片段内的最近帧，还可以访问早期片段中的过去帧。这种设计不同于上述基于记忆的方法，因为它不包含任何可学习的参数，因此没有关于数据集的知识。
- en: Attending the temporal history. In tackling the challenge of sequentially modeling
    recurrent architectures, numerous approaches have emerged, aiming to encapsulate
    the action history using a parallel methodology. This involves processing all
    past frames simultaneously rather than sequentially. Notably, the attention mechanism [[140](#bib.bib140)]
    has garnered substantial interest within these approaches, leveraging its widespread
    application and remarkable performance in both natural language processing and
    computer vision domains. Wang et al. [[76](#bib.bib76)] proposed a transformer-style
    architecture which generated a query vector from the last observed frame feature,
    and key and value vectors from all other past frame features. The query vector
    extracted relevant information from the past history based on the attention score,
    forming an aggregated representation. A prediction module consisting of MLPs was
    then leveraged to iteratively predict future features and actions based on the
    aggregated representation. Sener et al. [[5](#bib.bib5)] represented recent observation
    with different temporal extent and long-range past history at various granularities.
    Non-local blocks [[151](#bib.bib151), [152](#bib.bib152)] were used to aggregate
    short-term (recent) and long-term (spanning) features via attention. Each recent
    feature was coupled with all spanning representations. Outputs were combined via
    score fusion, i.e., averaging multiple predictions, to predict the final result.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 处理时间历史。在解决顺序建模递归架构的挑战时，出现了许多方法，旨在通过并行方法封装动作历史。这涉及到同时处理所有过去的帧，而不是顺序处理。其中，注意力机制 [[140](#bib.bib140)]
    在这些方法中引起了广泛关注，利用其在自然语言处理和计算机视觉领域的广泛应用和显著表现。Wang 等人 [[76](#bib.bib76)] 提出了一个类似于变压器的架构，该架构从最后观察到的帧特征中生成查询向量，从所有其他过去帧特征中生成键和值向量。查询向量基于注意力得分从过去历史中提取相关信息，形成一个聚合表示。然后，利用由MLPs组成的预测模块，根据聚合表示迭代预测未来特征和动作。Sener 等人 [[5](#bib.bib5)]
    用不同的时间范围和多种粒度表示最近的观察和长期过去的历史。通过注意力使用非局部块 [[151](#bib.bib151), [152](#bib.bib152)]
    来聚合短期（最近）和长期（跨越）特征。每个最近的特征与所有跨越的表示相结合。输出通过得分融合，即平均多个预测，来预测最终结果。
- en: 'Reducing complexity of self-attention. The self-attention operation in transformer
    models calculates the dot product between each pair of tokens in the input sequence,
    whose complexity is $O(N^{2})$, where $N$ is the length of the sequence. Recently,
    there has been a targeted focus to reduce the complexity to $O(N)$ in large methods
    so that transformer models can scale to long sequences. Low-rank approximations
    factorize the attention matrix into two lower-rank matrices [[153](#bib.bib153),
    [154](#bib.bib154)]. Query-based cross-attention mechanisms compress longer-term
    input into a fixed-size representation [[155](#bib.bib155), [156](#bib.bib156)].
    Following the idea of query-based cross-attention design, Xu et al. [[79](#bib.bib79)]
    proposed a two-stage memory compression design to compress and abstract the long-term
    memory into a latent representation of fixed length, which has empirically been
    found to lead to better performance. Moreover, the authors employed a short window
    of recent frames to perform self-attention and cross-attention operations on the
    extracted long-term memory. In case of action anticipation, the short-term memory
    was concatenated with a sequence of learnable tokens, responsible for future predictions.
    Following Xu et al. [[79](#bib.bib79)], Zhao and Krähenbühl [[83](#bib.bib83)]
    reformulated the cross-attention in the first memory compression stage through
    the lens of a kernel [[157](#bib.bib157)]. In this reformulation, the attention
    can be seen as applying a kernel smoother to the inputs, where the kernel scores
    represent input similarities. They applied two kinds of temporal smoothing kernel:
    a box kernel and a Laplace kernel. The resulting streaming attention reuses much
    of the computation from frame to frame, and only has a constant caching and computing
    overhead.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 降低自注意力的复杂性。变压器模型中的自注意力操作计算输入序列中每对标记之间的点积，其复杂度为$O(N^{2})$，其中$N$是序列的长度。最近，已经有针对性地关注将复杂度降低到$O(N)$的大型方法，以使变压器模型能够扩展到长序列。低秩近似将注意力矩阵分解为两个较低秩的矩阵[[153](#bib.bib153),
    [154](#bib.bib154)]。基于查询的交叉注意力机制将长期输入压缩为固定大小的表示[[155](#bib.bib155), [156](#bib.bib156)]。沿着基于查询的交叉注意力设计的思路，Xu
    等[[79](#bib.bib79)]提出了一个两阶段记忆压缩设计，将长期记忆压缩并抽象为固定长度的潜在表示，经验上发现这会带来更好的性能。此外，作者使用了一个短时间窗口的最近帧来对提取的长期记忆进行自注意力和交叉注意力操作。在动作预测的情况下，短期记忆与一系列可学习的标记进行连接，这些标记负责未来的预测。沿着Xu
    等[[79](#bib.bib79)]的思路，赵和Krähenbühl[[83](#bib.bib83)]通过核的视角[[157](#bib.bib157)]重新构造了第一个记忆压缩阶段的交叉注意力。在这种重新构造中，注意力可以被看作是对输入应用核平滑，其中核分数表示输入相似性。他们应用了两种时间平滑核：盒子核和拉普拉斯核。最终的流式注意力在帧与帧之间重用了大量计算，并且只有一个常数的缓存和计算开销。
- en: 3.3 Narrowing anticipation space by using additional information
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 通过使用额外信息来缩小预测空间
- en: While vision based systems are the de-facto standard for action recognition
    and anticipation [[98](#bib.bib98), [67](#bib.bib67)], using other additional
    supporting input modalities like optical flow features [[39](#bib.bib39), [41](#bib.bib41),
    [158](#bib.bib158)] or knowledge about objects and sounds in the scene [[159](#bib.bib159),
    [158](#bib.bib158), [105](#bib.bib105)] has shown to be beneficial [[160](#bib.bib160)].
    To properly fuse the different modalities, various fusion strategies have been
    exploited for action anticipation in recent years, which are described in Section [3.3.1](#S3.SS3.SSS1
    "3.3.1 Multi-modal fusion ‣ 3.3 Narrowing anticipation space by using additional
    information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation").
    Moreover, additional useful information such as personalization (Section [3.3.2](#S3.SS3.SSS2
    "3.3.2 Conditioning on extra information ‣ 3.3 Narrowing anticipation space by
    using additional information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation")) could also be incorporated in the system to further
    support anticipation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于视觉的系统是动作识别和预测的事实标准[[98](#bib.bib98), [67](#bib.bib67)]，但使用其他额外的支持输入模态如光流特征[[39](#bib.bib39),
    [41](#bib.bib41), [158](#bib.bib158)]或场景中的物体和声音知识[[159](#bib.bib159), [158](#bib.bib158),
    [105](#bib.bib105)]已被证明是有益的[[160](#bib.bib160)]。为了正确融合不同的模态，近年来已利用各种融合策略进行动作预测，这些策略在第[3.3.1](#S3.SS3.SSS1
    "3.3.1 Multi-modal fusion ‣ 3.3 Narrowing anticipation space by using additional
    information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation")节中有所描述。此外，个性化等额外有用的信息（第[3.3.2](#S3.SS3.SSS2
    "3.3.2 Conditioning on extra information ‣ 3.3 Narrowing anticipation space by
    using additional information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation")节）也可以被纳入系统中以进一步支持预测。
- en: 3.3.1 Multi-modal fusion
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 多模态融合
- en: Input modalities. While action anticipation methods typically take the original
    video frames as the default input modality, many methods leverage higher level
    modalities from the original RGB frames, such as optical flow [[161](#bib.bib161)],
    object features (usually depending on an object detector such as [[54](#bib.bib54),
    [55](#bib.bib55)]), human-object-interaction [[162](#bib.bib162), [104](#bib.bib104),
    [163](#bib.bib163)], and human-scene-interaction [[95](#bib.bib95)]. For egocentric
    videos, the camera wearer’s trajectory [[164](#bib.bib164)], hand trajectory [[99](#bib.bib99)],
    eye gaze [[69](#bib.bib69)], and environment affordance [[165](#bib.bib165)] have
    been additionally used.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输入模态。虽然动作预测方法通常将原始视频帧作为默认输入模态，但许多方法利用原始RGB帧中的高级模态，如光流[[161](#bib.bib161)]、物体特征（通常依赖于物体检测器，如[[54](#bib.bib54),
    [55](#bib.bib55)]）、人-物体互动[[162](#bib.bib162), [104](#bib.bib104), [163](#bib.bib163)]和人-场景互动[[95](#bib.bib95)]。对于自我中心视频，额外使用了摄像机佩戴者的轨迹[[164](#bib.bib164)]、手部轨迹[[99](#bib.bib99)]、眼睛注视[[69](#bib.bib69)]和环境适应性[[165](#bib.bib165)]。
- en: '![Refer to caption](img/afb7690c3e9f4c828d7c5ac9df9a3264.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/afb7690c3e9f4c828d7c5ac9df9a3264.png)'
- en: (a) Score fusion.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 分数融合。
- en: '![Refer to caption](img/6559f16b0cc5d677fbe8496a420dc9df.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6559f16b0cc5d677fbe8496a420dc9df.png)'
- en: (b) Late feature fusion.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 晚期特征融合。
- en: '![Refer to caption](img/f767661504ed5f0da194f9d15dbdbefb.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f767661504ed5f0da194f9d15dbdbefb.png)'
- en: (c) Mid-level feature fusion.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 中层特征融合。
- en: 'Figure 5: Three typical fusion strategies for multi-modal action anticipation.
    Feature extractors are omitted for brevity. Examples from the EpicKitchens [[7](#bib.bib7)]
    dataset and AFFT [[105](#bib.bib105)].'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：多模态动作预测的三种典型融合策略。为了简洁，特征提取器被省略。示例来自EpicKitchens[[7](#bib.bib7)]数据集和AFFT[[105](#bib.bib105)]。
- en: For instance, Shen et al. [[90](#bib.bib90)] proposed using gaze to extract
    events, as gaze moving in/out of a certain object most probably indicates the
    occurrence/ending of a certain activity, and constructed a two-stream network
    driven by the gaze events. The asynchronous stream modeled inter-relationship
    between different events with a Hawkes process model [[166](#bib.bib166)], whereas
    the synchronous stream extracted frame-wise hand and gaze features which were
    temporally weighted by an attention module based on the gaze events. Liu et al. [[99](#bib.bib99)]
    explicitly incorporated intentional hand movements as an anticipatory representation
    of actions. They jointly modeled and predicted hand trajectories, interaction
    hotspots and labels of future actions. Similarly, Dessalene et al. [[101](#bib.bib101)]
    exploited hand and object segmentation masks and used the time to contact between
    hands and objects as a representation for the anticipation task. Long-term temporal
    semantic relations between actions were modeled using a graph convolutional network
    (GCN [[102](#bib.bib102)]) and an LSTM. Future actions were predicted based on
    the graph representations along with appearance features. Roy and Fernando [[104](#bib.bib104)]
    introduced human-object interaction as an anticipatory representation and represented
    it by cross-correlation of visual features of every pairwise human-object pairs
    in the scene. A standard encoder-decoder transformer was then leveraged for temporal
    feature aggregation and future action generation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Shen 等人[[90](#bib.bib90)] 提出了使用凝视来提取事件，因为凝视进出某一物体很可能表示某一活动的发生或结束，并构建了一个由凝视事件驱动的双流网络。异步流通过
    Hawkes 过程模型[[166](#bib.bib166)] 建模了不同事件之间的关系，而同步流提取了逐帧的手部和凝视特征，这些特征通过一个基于凝视事件的注意力模块进行时间加权。Liu
    等人[[99](#bib.bib99)] 明确地将意图手部运动作为动作的预期表示。他们联合建模并预测了手部轨迹、交互热点和未来动作的标签。类似地，Dessalene
    等人[[101](#bib.bib101)] 利用手部和物体分割掩码，并使用手部与物体之间的接触时间作为预期任务的表示。长期时间语义关系通过图卷积网络 (GCN
    [[102](#bib.bib102)]) 和 LSTM 进行了建模。基于图形表示及外观特征预测未来动作。Roy 和 Fernando [[104](#bib.bib104)]
    引入了人机交互作为预期表示，并通过场景中每对人机对的视觉特征的互相关来表示它。然后利用标准的编码器-解码器 Transformer 进行时间特征聚合和未来动作生成。
- en: 'Score fusion. Consistent with most multi-modal action recognition models [[39](#bib.bib39),
    [41](#bib.bib41)], anticipation models typically use score fusion, as displayed
    in Fig. [5(a)](#S3.F5.sf1 "In Figure 5 ‣ 3.3.1 Multi-modal fusion ‣ 3.3 Narrowing
    anticipation space by using additional information ‣ 3 Methods ‣ A Survey on Deep
    Learning Techniques for Action Anticipation"), to fuse different modalities. While
    averaging using fixed weights, including simple averaging [[5](#bib.bib5), [104](#bib.bib104)]
    and weighted averaging [[67](#bib.bib67)], showed already superior results over
    the uni-modal baseline, Furnari et al. [[98](#bib.bib98)] showed that assigning
    each modality with dynamical importance for the final prediction is particularly
    beneficial for anticipating egocentric actions. They employed a two-LSTMs-based
    encoder-decoder architecture (RU-LSTM) for individual modalities. Taking three
    distinct types of information into account: appearance, motion, and object presence
    in the scene, they built three branches and proposed a fusion module consisting
    of fully connected layers to generate attention scores for each modality. It is
    noteworthy that the authors also published the pre-extracted features²²2[https://github.com/fpv-iplab/rulstm](https://github.com/fpv-iplab/rulstm)
    of Epic-Kitchens [[7](#bib.bib7), [167](#bib.bib167)] and EGTEA Gaze+ [[69](#bib.bib69)],
    which were used in many approaches on action anticipation [[5](#bib.bib5), [103](#bib.bib103),
    [105](#bib.bib105)]. Osman et al. [[168](#bib.bib168)] further extended this work
    by incorporating a second stream operating at different frame rates, inspired
    by [[1](#bib.bib1)].'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 评分融合。与大多数多模态动作识别模型一致[[39](#bib.bib39), [41](#bib.bib41)]，预期模型通常使用评分融合，如图[5(a)](#S3.F5.sf1
    "在图5 ‣ 3.3.1 多模态融合 ‣ 3.3 通过使用额外信息来缩小预期空间 ‣ 3 方法 ‣ 动作预期的深度学习技术综述")所示，以融合不同的模态。虽然使用固定权重的平均方法，包括简单平均[[5](#bib.bib5),
    [104](#bib.bib104)]和加权平均[[67](#bib.bib67)]，已经显示出比单模态基线更优的结果，Furnari 等[[98](#bib.bib98)]展示了为最终预测分配每种模态的动态重要性对于预测自我中心动作特别有利。他们采用了一种基于两个LSTM的编码器-解码器架构（RU-LSTM）来处理各个模态。考虑到三种不同类型的信息：外观、运动和场景中的物体存在，他们构建了三个分支，并提出了一个包含全连接层的融合模块，以生成每种模态的注意力分数。值得注意的是，作者还发布了Epic-Kitchens[[7](#bib.bib7),
    [167](#bib.bib167)]和EGTEA Gaze+[[69](#bib.bib69)]的预提取特征²²2[https://github.com/fpv-iplab/rulstm](https://github.com/fpv-iplab/rulstm)，这些特征在许多动作预期方法中得到了应用[[5](#bib.bib5),
    [103](#bib.bib103), [105](#bib.bib105)]。Osman 等[[168](#bib.bib168)]进一步扩展了这项工作，加入了一个在不同帧率下运行的第二个流，灵感来自[[1](#bib.bib1)]。
- en: Feature fusion. There exists another line of multi-modal work which focuses
    on feature fusion, i.e., modalities get fused before decisions are made, including
    late fusion [[103](#bib.bib103), [86](#bib.bib86)] and mid-level fusion [[105](#bib.bib105),
    [82](#bib.bib82), [48](#bib.bib48)]. Similar to score-fusion-based methods, approaches
    based on a late fusion strategy first anticipate representations containing future
    action patterns, and then fuse representations of different modalities with a
    specific fusion module (refer to Fig. [5(b)](#S3.F5.sf2 "In Figure 5 ‣ 3.3.1 Multi-modal
    fusion ‣ 3.3 Narrowing anticipation space by using additional information ‣ 3
    Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation")). Mahmud et al. [[86](#bib.bib86)]
    proposed a network consisting of three branches to jointly learn both the future
    action label and the starting time using object features and motion-based action
    features. To align the three branches, a fully-connected layer was added on top
    of the concatenated outputs of all branches. Building upon the work of Furnari et al. [[98](#bib.bib98)],
    Zatsarynna et al. [[103](#bib.bib103)] considered the same three modalities in
    their work. They used temporal convolutional networks (TCN [[169](#bib.bib169)])
    to predict future representations independently for each input modality. Subsequently,
    the three independent representations were concatenated and fused via fully-connected
    layers. Final predictions were made based on the resulting fused representation.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 特征融合。另一类多模态工作集中于特征融合，即在做出决策之前融合模态，包括后期融合[[103](#bib.bib103), [86](#bib.bib86)]和中层融合[[105](#bib.bib105),
    [82](#bib.bib82), [48](#bib.bib48)]。类似于基于得分融合的方法，采用后期融合策略的方法首先预测包含未来动作模式的表示，然后使用特定的融合模块融合不同模态的表示（参见图[5(b)](#S3.F5.sf2
    "图5 ‣ 3.3.1 多模态融合 ‣ 3.3 通过使用额外信息来缩小预测空间 ‣ 3 方法 ‣ 关于动作预测的深度学习技术调查")）。Mahmud等人[[86](#bib.bib86)]提出了一个由三个分支组成的网络，利用对象特征和基于运动的动作特征共同学习未来动作标签和开始时间。为了对齐这三个分支，在所有分支的连接输出上方添加了一个全连接层。在Furnari等人[[98](#bib.bib98)]的工作基础上，Zatsarynna等人[[103](#bib.bib103)]在他们的工作中考虑了相同的三种模态。他们使用时间卷积网络（TCN
    [[169](#bib.bib169)]）独立预测每个输入模态的未来表示。随后，这三个独立的表示被连接并通过全连接层进行融合。最终的预测是基于结果融合表示进行的。
- en: Unlike the late fusion strategy used in [[103](#bib.bib103)], mid-level strategies
    usually first fuse different modalities without explicitly taking the temporal
    information into account, and then anticipate the next action with an additional
    anticipation module, as shown in Fig. [5(c)](#S3.F5.sf3 "In Figure 5 ‣ 3.3.1 Multi-modal
    fusion ‣ 3.3 Narrowing anticipation space by using additional information ‣ 3
    Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation"). While
    aggregating the concatenated multi-modal features with a fully-connected layer [[82](#bib.bib82),
    [48](#bib.bib48)] is a standard feature fusion strategy, Zhong et al. [[105](#bib.bib105)]
    built a transformer-based fusion module with a modality-agnostic token, similar
    to the [cls] token in vision transformers (ViT [[40](#bib.bib40)]). They achieved
    superior results compared to a score-fusion-based counterpart. In addition to
    the three modalities presented by Furnari et al. [[98](#bib.bib98)], i.e., appearance,
    motion, and object presence, they also incorporated an audio modality. However,
    it was demonstrated that audio provides comparatively less informative cues compared
    to the visual modalities. To predict the next action, the GPT-based anticipation
    module from [[67](#bib.bib67)] was used.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 与[[103](#bib.bib103)]中使用的后期融合策略不同，中层策略通常首先融合不同的模态，而不显式考虑时间信息，然后使用额外的预测模块来预测下一个动作，如图[5(c)](#S3.F5.sf3
    "图5 ‣ 3.3.1 多模态融合 ‣ 3.3 通过使用额外信息来缩小预测空间 ‣ 3 方法 ‣ 关于动作预测的深度学习技术调查")所示。虽然使用全连接层[[82](#bib.bib82),
    [48](#bib.bib48)]聚合连接的多模态特征是一种标准的特征融合策略，Zhong等人[[105](#bib.bib105)]构建了一个基于变换器的融合模块，使用模态无关的标记，类似于视觉变换器（ViT
    [[40](#bib.bib40)]）中的[cls]标记。他们的结果优于基于得分融合的方法。除了Furnari等人[[98](#bib.bib98)]提出的三种模态，即外观、运动和对象存在，他们还加入了音频模态。然而，研究表明，与视觉模态相比，音频提供的信息较少。为了预测下一个动作，使用了[[67](#bib.bib67)]中的基于GPT的预测模块。
- en: 3.3.2 Conditioning on extra information
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 基于额外信息的条件设置
- en: Personalization. Zhou and Berg [[84](#bib.bib84)] explored two simple tasks
    related to temporal prediction in egocentric videos of everyday activities, pairwise
    temporal ordering and future video selection. They showed that personalization
    to a particular individual or environment provides significantly increased performance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化。Zhou 和 Berg [[84](#bib.bib84)] 探索了与日常活动的自我中心视频中的时间预测相关的两个简单任务，即成对的时间排序和未来视频选择。他们表明，对特定个体或环境的个性化显著提高了性能。
- en: Prediction time. As the recurrent approaches suffer from increasing inference
    time and error accumulation when predicting longer action sequences, some methods
    relied on parallel decoding by learning a sequence of action queries [[80](#bib.bib80),
    [82](#bib.bib82), [117](#bib.bib117)]. However, the number of predictable future
    actions was also limited to the number of action queries used in the training
    process. Ke et al. [[127](#bib.bib127)] took another approach and chose to condition
    on a time variable representing the prediction time. Specifically, they transformed
    the prediction time to a time representation, and concatenated it with the original
    inputs forming time-conditioned observations. Their model was therefore capable
    of anticipating a future action at arbitrary and variable time horizons in a one-shot
    fashion. Building upon a similar concept, Anticipatr [[117](#bib.bib117)] utilized
    a linear layer to convert learnable action queries, along with the anticipation
    duration, into time-conditioned queries. It employed a two-stage learning approach.
    In the first stage, a segment encoder is trained to predict the set of future
    action labels. In the second stage, a video encoder and an anticipation decoder
    were trained for making the final decision based on the input of the segment encoder.
    Anticipatr demonstrated state-of-the-art performance across multiple benchmarks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 预测时间。由于递归方法在预测较长的动作序列时面临推理时间增加和错误累积的问题，一些方法依赖于通过学习一系列动作查询进行并行解码 [[80](#bib.bib80),
    [82](#bib.bib82), [117](#bib.bib117)]。然而，可预测的未来动作数量也受限于训练过程中使用的动作查询数量。Ke 等 [[127](#bib.bib127)]
    采取了另一种方法，选择以表示预测时间的时间变量为条件。具体来说，他们将预测时间转化为时间表示，并将其与原始输入拼接，形成时间条件观察。因此，他们的模型能够以一次性方式在任意和可变的时间范围内预测未来动作。在类似概念的基础上，Anticipatr
    [[117](#bib.bib117)] 利用线性层将可学习的动作查询与预期持续时间转换为时间条件查询。它采用了两阶段的学习方法。在第一阶段，训练了一个片段编码器来预测未来动作标签的集合。在第二阶段，训练了一个视频编码器和一个预期解码器，以根据片段编码器的输入做出最终决策。Anticipatr
    在多个基准测试中展示了最先进的性能。
- en: 'Sequence-level information. A natural expectation of action anticipation is
    to make the correct anticipation as early as possible. However, a cross-entropy
    loss would not capture sequence-level distinctions, as it is calculated at each
    step to output higher confident scores on the ground truth category, and no sequence-level
    information is involved. Gao et al. [[45](#bib.bib45)] therefore proposed a reinforcement
    learning module to give high rewards to sequences that correspond to early correct
    predictions. Following a similar idea, Ng and Fernando [[122](#bib.bib122)] modified
    the cross-entropy loss between the prediction and the ground truth, taking the
    following considerations into account: 1) shorter observations contain less information
    and should therefore contribute less to the overall loss, and 2) a correct predicted
    action in the near future is important for predictions far into the future and
    should be emphasized by the loss function. Empirical results demonstrated the
    efficacy of the modified loss. Drawing inspiration from the human ability to infer
    the past based on the future, certain approaches [[112](#bib.bib112), [115](#bib.bib115)]
    have introduced a cycle consistency module. This module predicts past activities
    using the projected future and has demonstrated improved outcomes in comparison
    to its counterpart lacking the consistency module. Similarly, Fosco et al. [[170](#bib.bib170)]
    introduced the Event Transition Matrix (ETM) to incorporate statistical regularities
    into models. The ETM is computed from action labels in an untrimmed video dataset
    and represents the likelihood that a particular action is preceded or followed
    by any other action in the set. By incorporating ETM, the model can directly leverage
    this explicit representation of event transition likelihoods without having to
    learn it from scratch.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 序列级信息。行动预测的自然期望是尽早做出正确预测。然而，交叉熵损失不能捕捉序列级别的区别，因为它在每一步都计算，以对地面真实类别输出更高的置信度分数，并且不涉及序列级别的信息。因此，Gao
    等人[[45](#bib.bib45)] 提出了一个强化学习模块，为与早期正确预测对应的序列提供高奖励。基于类似的思路，Ng 和 Fernando [[122](#bib.bib122)]
    修改了预测与地面真实之间的交叉熵损失，考虑了以下因素：1）较短的观察包含的信息较少，因此应对整体损失的贡献较小；2）在不久的将来正确预测的行为对远期预测很重要，应通过损失函数加以强调。实证结果证明了修改损失函数的有效性。受到人类根据未来推测过去的能力的启发，某些方法[[112](#bib.bib112),
    [115](#bib.bib115)] 引入了一个周期一致性模块。该模块使用预测的未来来预测过去的活动，并与缺乏一致性模块的对照组相比，显示了改进的结果。同样，Fosco
    等人[[170](#bib.bib170)] 引入了事件转移矩阵（ETM）以将统计规律纳入模型。ETM 是从未剪辑的视频数据集中计算得出的，表示特定行为被任何其他行为之前或之后的可能性。通过引入
    ETM，模型可以直接利用这种事件转移可能性的明确表示，而无需从头学习。
- en: Semantic meaning of actions. An important aspect to consider when anticipating
    human actions is the future’s uncertainty since multiple actions may occur. However,
    most approaches used one-hot encoded labels to supervise the learning process,
    which may prevent the models from capturing the actual data distribution. Addressing
    this issue, Camporese et al. [[113](#bib.bib113)] proposed injecting semantic
    meaning of actions, e.g., in form of the similarity of word embeddings like GloVe [[171](#bib.bib171)],
    into the model via label smoothing techniques, which broadens the set of possible
    futures.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 行动的语义意义。在预测人类行为时，一个重要的考虑因素是未来的不确定性，因为可能发生多种行为。然而，大多数方法使用一热编码标签来监督学习过程，这可能会阻止模型捕捉到实际的数据分布。为了解决这个问题，Camporese
    等人[[113](#bib.bib113)] 提出了通过标签平滑技术将行动的语义意义（例如，通过类似 GloVe [[171](#bib.bib171)]
    的词嵌入的相似性）注入模型，从而扩展了可能的未来集合。
- en: Spatio-temporal graph structure. When humans interact with objects, the interactions
    take place in both space and time. Modeling such a spatio-temporal graph structure,
    where the nodes of the graph typically represent the problem components and the
    edges capture their spatio-temporal interactions, has attracted much attention
    in recent years [[107](#bib.bib107), [162](#bib.bib162), [109](#bib.bib109)].
    While the concept is clear, the way to construct the graph and to update the node
    features is challenging. Jain et al. [[107](#bib.bib107)] employed multiple LSTMs
    for updating corresponding nodes or edges based on the concatenated message features.
    Qi et al. [[162](#bib.bib162)] updated the node features based on the sum of weighted
    messages, where the weights, i.e., the adjacency matrix, were also estimated and
    updated during optimization. Similar to [[162](#bib.bib162)], Sun et al. [[109](#bib.bib109)]
    estimated attention scores during optimization to aggregate all messages within
    a time step for the virtual nodes similar to a Graph Attention Network [[172](#bib.bib172)],
    which were then used to update node features in the next step. To anticipate the
    actor’s behavior, a GRU [[146](#bib.bib146)] was used for temporal modeling. By
    doing so, the spatial and temporal dependencies were isolated from each other.
    Li et al. [[114](#bib.bib114)] argued that such a setting may not be able to fully
    capture the spatial and temporal information due to the ignorance of their co-occurrence
    and strong correlations. Addressing this issue, they proposed a spatio-temporal
    relational network (STRN) by broadening the Convolutional Gated Recurrent Units
    (ConvGRU) [[173](#bib.bib173)], which can capture the spatial along with the temporal
    context.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 时空图结构。当人类与物体互动时，这些互动发生在空间和时间上。建模这样一个时空图结构，其中图的节点通常表示问题组件，边缘捕捉它们的时空互动，近年来引起了广泛关注 [[107](#bib.bib107),
    [162](#bib.bib162), [109](#bib.bib109)]。虽然概念很清楚，但构建图和更新节点特征的方式具有挑战性。Jain等人 [[107](#bib.bib107)]
    使用多个LSTM来更新基于连接的消息特征的相应节点或边缘。Qi等人 [[162](#bib.bib162)] 基于加权消息的总和来更新节点特征，其中权重，即邻接矩阵，也在优化过程中被估计和更新。类似于 [[162](#bib.bib162)]，Sun等人 [[109](#bib.bib109)]
    在优化过程中估计注意力分数，以在时间步内聚合所有消息，类似于图注意力网络 [[172](#bib.bib172)]，然后这些分数被用于更新下一步的节点特征。为了预测行为者的行为，使用了GRU [[146](#bib.bib146)]
    进行时间建模。通过这样做，空间和时间依赖性被相互隔离。Li等人 [[114](#bib.bib114)] 认为这种设置可能无法完全捕捉空间和时间信息，因为忽略了它们的共现和强相关性。为了解决这个问题，他们提出了一个时空关系网络（STRN），通过扩展卷积门控递归单元（ConvGRU） [[173](#bib.bib173)]，可以同时捕捉空间和时间背景。
- en: '![Refer to caption](img/1d6690c1de057364e67bbbe782156c26.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1d6690c1de057364e67bbbe782156c26.png)'
- en: 'Figure 6: Abstraction level of activities.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：活动的抽象级别。
- en: The underlying intention. To get something done, humans perform a sequence of
    actions dictated by an intention [[174](#bib.bib174)]. It is therefore advantageous
    to understand human intention to anticipate how a person would act in the future.
    Following this idea, some works [[175](#bib.bib175), [116](#bib.bib116), [118](#bib.bib118),
    [119](#bib.bib119)] built a top-down framework (refer to Fig. [6](#S3.F6 "Figure
    6 ‣ 3.3.2 Conditioning on extra information ‣ 3.3 Narrowing anticipation space
    by using additional information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation")) to explicitly infer the intention and use the intention
    to constrain the variability of future actions. Specifically, Roy and Fernando [[175](#bib.bib175)]
    used a stacked LSTM to derive the latent goal from the observed visual representations,
    and exploited the concepts of goal closeness and goal consistency to guide the
    predictions. Goal closeness suggests actions should align with the latent goal,
    while goal consistency maintains this alignment over a sequence. Mascaro et al. [[118](#bib.bib118)]
    proposed estimating human intention with an MLP Mixer [[176](#bib.bib176)]. By
    taking the past actions, the estimated intention, as well as a latent vector sampled
    from a latent distribution based on the reparameterization trick from VAE as input,
    a transformer decoder was used to generate a sequence of future actions. Zhao et al. [[119](#bib.bib119)]
    proposed first recognizing human actions and then feeding the recognized actions
    as discretized labels to the ChatGPT [[177](#bib.bib177)] to get both the goal
    and future actions. Zatsarynna and Gall [[178](#bib.bib178)] integrated goal estimation
    into a multi-task framework, demonstrating improved performance compared to the
    counterpart that lacks goal estimation.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在意图。为了完成某件事，人类会执行由意图决定的一系列动作[[174](#bib.bib174)]。因此，理解人类意图可以帮助预测一个人未来的行为。基于这一思想，一些研究[[175](#bib.bib175),
    [116](#bib.bib116), [118](#bib.bib118), [119](#bib.bib119)] 建立了一个自上而下的框架（参见图[6](#S3.F6
    "Figure 6 ‣ 3.3.2 Conditioning on extra information ‣ 3.3 Narrowing anticipation
    space by using additional information ‣ 3 Methods ‣ A Survey on Deep Learning
    Techniques for Action Anticipation")），以明确推断意图并利用该意图来限制未来动作的变异性。具体来说，Roy 和 Fernando
    [[175](#bib.bib175)] 使用堆叠 LSTM 从观察到的视觉表示中推导出潜在目标，并利用目标接近度和目标一致性的概念来指导预测。目标接近度表明动作应与潜在目标对齐，而目标一致性则在整个序列中保持这种对齐。Mascaro
    等[[118](#bib.bib118)] 提出了使用 MLP Mixer [[176](#bib.bib176)] 来估计人类意图。通过将过去的动作、估计的意图以及基于
    VAE 重新参数化技巧的潜在分布中的潜在向量作为输入，使用变换器解码器生成未来动作序列。Zhao 等[[119](#bib.bib119)] 提出了首先识别人类动作，然后将识别出的动作作为离散标签输入到
    ChatGPT [[177](#bib.bib177)] 中，以获取目标和未来动作。Zatsarynna 和 Gall [[178](#bib.bib178)]
    将目标估计集成到多任务框架中，展示了相比于缺乏目标估计的对照组的改进性能。
- en: 3.4 Incorporating uncertainty
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 纳入不确定性
- en: Predicting future actions is inherently uncertain. Given an observed video segment
    containing an ongoing action, multiple actions could be possible to be the next
    action following the observed one. This uncertainty becomes even larger if we
    are going to predict far into the future. Therefore, it may be beneficial to model
    the underlying uncertainty, allowing to capture different possible future actions.
    However, action prediction is mostly treated as a classification problem and optimized
    using the cross-entropy loss, suffering from overly high resemblance to dominant
    ground truth while suppressing other reasonable possibilities [[179](#bib.bib179)].
    Moreover, approaches that are optimized using the mean square error tend to produce
    the mean of the modes [[4](#bib.bib4), [180](#bib.bib180)]. Therefore, some approaches
    output multiple possible future actions to be able to capture the underlying uncertainty.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 预测未来动作本质上是不确定的。给定一个包含正在进行的动作的观察视频片段，多个动作可能是观察到的动作之后的下一个动作。如果我们预测较远的未来，这种不确定性会变得更大。因此，建模潜在的不确定性可能是有益的，这样可以捕捉到不同的可能未来动作。然而，动作预测大多被视为分类问题，并使用交叉熵损失进行优化，这样会导致与主流真实情况的相似性过高，同时抑制其他合理的可能性[[179](#bib.bib179)]。此外，使用均方误差优化的方法往往产生模式的均值[[4](#bib.bib4),
    [180](#bib.bib180)]。因此，一些方法输出多个可能的未来动作，以捕捉潜在的不确定性。
- en: 'Generating multiple outputs with multiple rules. To address the non-deterministic
    nature of future prediction, Vondrick et al. [[4](#bib.bib4)] extended their regression
    network to support multiple outputs by training a mixture of $K$ networks, where
    each mixture was trained to predict one of the modes in the future. As only one
    of the possible futures is provided in the dataset and it is unknown to which
    of the $K$ mixtures each data sample belongs, they overcame these issues by alternating
    between two steps: 1) optimizing the network while keeping the mixture assignment
    fixed, and 2) re-estimating the mixture assignment with the new network weights.
    During inference, the most likely action class is selected by marginalizing over
    the probability distributions of all modes. Following a similar idea, Piergiovanni et al. [[124](#bib.bib124)]
    proposed a differentiable grammar model which learns a set of production rules,
    being able to generate multiple candidate future sequences that follow a similar
    distribution of sequences seen during training. To avoid enumerating all possible
    rules which have exponential growth in time, they introduced adversarial learning
    for the grammar model, allowing for much more memory- and computationally-efficient
    learning without such enumeration.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多重规则生成多个输出。为了应对未来预测的非确定性，Vondrick 等人 [[4](#bib.bib4)] 扩展了他们的回归网络，通过训练 $K$
    个网络的混合来支持多个输出，每个混合被训练来预测未来的其中一种模式。由于数据集中只提供了可能的未来之一，而且不知道每个数据样本属于 $K$ 个混合中的哪一个，他们通过两步交替解决了这些问题：1)
    在保持混合分配固定的情况下优化网络，2) 用新的网络权重重新估计混合分配。在推理过程中，通过对所有模式的概率分布进行边际化，选择最可能的动作类别。按照类似的思路，Piergiovanni
    等人 [[124](#bib.bib124)] 提出了一个可微分语法模型，该模型学习一组生成规则，能够生成多个候选未来序列，这些序列遵循训练期间看到的序列的类似分布。为了避免列举所有可能的规则（这些规则的增长是指数级的），他们引入了对抗学习来改进语法模型，使学习过程更加高效，无需进行这种列举。
- en: Generating multiple outputs by sampling from the learned distribution. Schydlo et al. [[21](#bib.bib21)]
    took human body pose features observed over three time steps as input into an
    encoder-decoder LSTM network to generate future action sequences. By sampling
    from the action probability distribution at each time step and using beam search,
    they made a final selection from a pool of action candidates during inference.
    Similarly, Farha and Gall [[48](#bib.bib48)] introduced a framework that predicts
    all subsequent actions and corresponding durations in a stochastic manner. In
    their framework, an action model similar to the one proposed in [[46](#bib.bib46)]
    and a time model were trained to predict the probability distribution of the future
    action label and duration, respectively. While the action model is trained using
    the cross-entropy loss, the durations were modeled with a Gaussian distribution
    and optimized with the negative log likelihood. At test time, a future action
    label and its duration were sampled from the learned distributions. Long-term
    predictions were achieved by feeding the predicted action segment to the model
    recursively.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从学习到的分布中采样生成多个输出。Schydlo 等人 [[21](#bib.bib21)] 将在三个时间步观察到的人体姿态特征作为输入传入编码器-解码器
    LSTM 网络，以生成未来的动作序列。通过在每个时间步从动作概率分布中采样，并使用束搜索，他们在推理过程中从一组动作候选中做出了最终选择。类似地，Farha
    和 Gall [[48](#bib.bib48)] 引入了一个框架，以随机的方式预测所有后续动作及其相应的持续时间。在他们的框架中，类似于 [[46](#bib.bib46)]
    中提出的动作模型和时间模型被训练来预测未来动作标签和持续时间的概率分布。动作模型使用交叉熵损失进行训练，而持续时间则通过高斯分布进行建模，并用负对数似然进行优化。在测试时，从学习到的分布中采样未来的动作标签及其持续时间。通过递归地将预测的动作片段输入模型，实现了长期预测。
- en: Zhao and Wildes [[49](#bib.bib49)] addressed the same task, i.e., joint anticipation
    of long-term action labels and their corresponding times, by using Conditional
    Adversarial Generative Networks. Unlike Farha and Gall [[48](#bib.bib48)], they
    treated both action labels and time as discrete data, formatting them as one-hot
    vectors. This approach was inspired by research on recommendation systems, which
    found that modeling time with a categorical representation and optimizing with
    cross-entropy consistently outperformed the use of real-valued modeling [[181](#bib.bib181)].
    After projecting these vectors into higher-dimensional continuous spaces and concatenating
    them, they were fed into a seq2seq generator [[182](#bib.bib182)] to compute future
    action labels and their corresponding times. To enable differentiable sampling
    and generate future sequences with both quality and diversity during training,
    the Gumbel-Softmax relaxation technique [[150](#bib.bib150)], which mimics one-hot
    vectors from categorical distributions, and a normalized distance regularizer [[183](#bib.bib183)],
    which encourages diversity, were adopted. A ConvNet classifier was used as the
    discriminator to enable adversarial training of the generator.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao和Wildes[[49](#bib.bib49)]通过使用条件对抗生成网络解决了相同的任务，即联合预测长期动作标签及其对应时间。与Farha和Gall[[48](#bib.bib48)]不同，他们将动作标签和时间都视为离散数据，并将其格式化为独热编码向量。这种方法受到推荐系统研究的启发，研究发现用类别表示建模时间并用交叉熵优化，比使用实值建模更具优势[[181](#bib.bib181)]。在将这些向量投射到更高维的连续空间并将其连接后，它们被送入seq2seq生成器[[182](#bib.bib182)]以计算未来的动作标签及其对应时间。为了在训练过程中实现可微采样，并生成具有质量和多样性的未来序列，采用了Gumbel-Softmax松弛技术[[150](#bib.bib150)]，该技术模仿了类别分布中的独热向量，以及规范化距离正则化器[[183](#bib.bib183)]，鼓励多样性。使用ConvNet分类器作为判别器，实现了生成器的对抗训练。
- en: Mehrasa et al. [[53](#bib.bib53)] proposed using a recurrent variational auto-encoder [[184](#bib.bib184)]
    (VAE) to capture the distribution over the times and categories of action sequences.
    To address the limitation that a fixed prior distribution of the latent variable
    (typically $\mathcal{N}(0,I)$ in VAE models) might ignore temporal dependencies
    present between actions, the authors developed a time-varying prior. At test time,
    a latent code was sampled from the learned prior distribution, and this code was
    then used to infer the probability distributions of the action class and its corresponding
    time.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Mehrasa等人[[53](#bib.bib53)]提议使用递归变分自编码器[[184](#bib.bib184)]（VAE）来捕捉动作序列的时间和类别分布。为了解决固定的潜变量先验分布（通常是VAE模型中的$\mathcal{N}(0,I)$）可能忽略动作之间存在的时间依赖性的问题，作者们开发了一个时间变化的先验。在测试时，从学习到的先验分布中采样一个潜在代码，并利用该代码推断动作类别及其对应时间的概率分布。
- en: 3.5 Modeling at a more conceptual level
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 更概念层面的建模
- en: Most action anticipation methods introduced in previous sections operate on
    the feature-level, i.e., the anticipation module receives a single or a sequence
    of feature vectors representing the observed video, and then predicts the future
    feature vectors that are further categorized to actions by a classifier. Some
    approaches go in the other direction. They first recognize the ongoing action(s)
    and then infer future actions directly. The intuition behind these approaches
    is also straightforward. As some actions are dependent on specific trigger actions,
    we can directly predict the next action, if one of the trigger actions has been
    observed. For example, if we have observed the action take glass, it is more likely
    to see an future action drink water. Instead of processing on feature-level, these
    approaches utilize action-level observations and model action dependencies directly.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 前面介绍的大多数动作预测方法都在特征层面上操作，即预测模块接收一个或一系列表示观察到视频的特征向量，然后预测未来的特征向量，这些特征向量由分类器进一步分类为动作。一些方法则采取了相反的方向。他们首先识别正在进行的动作，然后直接推断未来的动作。这些方法背后的直觉也很简单。由于某些动作依赖于特定的触发动作，如果已经观察到其中一个触发动作，我们可以直接预测下一个动作。例如，如果我们观察到的动作是拿杯子，那么未来的动作更可能是喝水。这些方法没有在特征层面上处理，而是利用动作层面的观察直接建模动作之间的依赖性。
- en: Markov assumption. To this end, some methods make a Markov assumption on the
    sequence of performed actions and use a linear layer to model the transition between
    the past and the next action [[125](#bib.bib125), [128](#bib.bib128), [65](#bib.bib65)].
    The linear weights of the transition model can be interpreted as conveying the
    importance of each past action class for the anticipation. By analyzing the linear
    weights and the confidence vector of the current action, we can easily interpret
    which action class is most responsible for the anticipation and diagnose the source
    of false predictions, i.e., whether the false prediction is due to the recognition
    model or due to the learned transitional model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫假设。为此，一些方法对执行的行动序列做出马尔可夫假设，并使用线性层来建模过去和下一个行动之间的过渡[[125](#bib.bib125), [128](#bib.bib128),
    [65](#bib.bib65)]。过渡模型的线性权重可以解释为传达每个过去行动类别对预测的重要性。通过分析线性权重和当前行动的置信度向量，我们可以轻松解释哪个行动类别对预测最有责任，并诊断错误预测的来源，即错误预测是由于识别模型还是由于学习到的过渡模型。
- en: Long-term anticipation. While the history of feature vectors contains rich semantic
    information, the action-level event history provides a different perspective yet
    perhaps a better analysis of the evolution of past actions, allowing to create
    a model for predicting future events occurring farther into the future [[74](#bib.bib74),
    [53](#bib.bib53)]. Consequently, many approaches utilize such action-level history
    to perform long-term anticipation [[46](#bib.bib46), [49](#bib.bib49), [53](#bib.bib53)].
    For instance, Farha et al. [[46](#bib.bib46)] introduced two methods for long-term
    action anticipation. One was based on an RNN model, which output the remaining
    length of the current action, the next action class, and its length. The long-term
    prediction was conducted recursively, i.e., observations were combined with the
    current prediction to produce the next prediction. Another method was based on
    a CNN model, which output a sequence of future actions in the form of a matrix
    in one single step.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 长期预测。虽然特征向量的历史包含丰富的语义信息，但行动级事件历史提供了不同的视角，可能更好地分析了过去行动的演变，从而创建了一个预测未来更远事件的模型[[74](#bib.bib74),
    [53](#bib.bib53)]。因此，许多方法利用这种行动级历史进行长期预测[[46](#bib.bib46), [49](#bib.bib49), [53](#bib.bib53)]。例如，Farha等人[[46](#bib.bib46)]提出了两种长期行动预测的方法。一种基于RNN模型，输出当前行动的剩余长度、下一个行动类别及其长度。长期预测是递归进行的，即将观察结果与当前预测结合以产生下一个预测。另一种方法基于CNN模型，它在一步中输出一个未来行动序列的矩阵。
- en: Treating actions as events. A majority of data generated via human activities,
    e.g., running, cooking, etc., can be represented as a sequence of actions over
    a continuous time. These actions vary in their start and completion times depending
    on the user and the surrounding environment. Therefore, these continuous-time
    action sequences can be viewed as sparse events (action categories and their temporal
    occurrence), and modeled with classical event modeling methods such as temporal
    point processes (TPPs) [[185](#bib.bib185), [166](#bib.bib166)]. TPPs have shown
    a significant promise in modeling a variety of continuous-time sequences in healthcare [[186](#bib.bib186)],
    finance [[187](#bib.bib187)], education [[188](#bib.bib188)], and social networks [[189](#bib.bib189)].
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 将行动视为事件。通过人类活动（例如跑步、烹饪等）生成的大多数数据可以表示为连续时间上的一系列行动。这些行动的开始和完成时间会根据用户和周围环境的不同而有所变化。因此，这些连续时间的行动序列可以被视为稀疏事件（行动类别及其时间发生情况），并可以通过经典的事件建模方法如时间点过程（TPPs）[[185](#bib.bib185),
    [166](#bib.bib166)]进行建模。TPPs在建模各种连续时间序列方面显示出了显著的潜力，例如在医疗保健[[186](#bib.bib186)]、金融[[187](#bib.bib187)]、教育[[188](#bib.bib188)]和社交网络[[189](#bib.bib189)]领域。
- en: 'In recent years, there are also some attempts to apply TPPs to action prediction.
    An initial effort in this direction [[35](#bib.bib35)] adopted the Poisson process [[185](#bib.bib185)]
    as a key technique for activity inter-arrival time modeling. Notice that inter-arrival
    time differs from the absolute activity occurrence time in that it stands for
    the time duration between consecutive activities. To support a stochastic modeling
    of time, the authors pre-defined a Gaussian Process prior for the intensity function
    and learned its parameters, i.e., the mean and variance for certain actions and
    times, from training data for every action. As TPPs can also be described using
    a counting process $C(t)$, the physical meaning of the intensity function is the
    rate of event arrivals:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，也有一些尝试将TPPs应用于动作预测。在这个方向上的初步努力 [[35](#bib.bib35)] 采用了泊松过程 [[185](#bib.bib185)]
    作为活动间隔时间建模的关键技术。注意，间隔时间与绝对活动发生时间不同，它表示的是连续活动之间的时间持续。为了支持时间的随机建模，作者为强度函数预定义了高斯过程先验，并从每个动作的训练数据中学习其参数，即某些动作和时间的均值和方差。由于TPPs也可以用计数过程
    $C(t)$ 来描述，因此强度函数的物理意义是事件到达的速率：
- en: '|  | $\mathrm{P}\bigl{(}dC(t)=1\bigr{)}=\lambda^{*}(t),$ |  | (2) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{P}\bigl{(}dC(t)=1\bigr{)}=\lambda^{*}(t),$ |  | (2) |'
- en: where $dC(t)=C(t+dt)-C(t)$ and ^∗ denotes a dependence on history. Then, they
    performed testing through importance sampling. As opposed to the constant intensity
    function in a Poisson process, an extension of the initial work calculated the
    intensity parameter dynamically from past observations [[53](#bib.bib53)]. Their
    model also supports stochastic time and action predictions via sampling latent
    variables through a Variational Auto-Encoder (VAE). Gupta and Bedathur [[129](#bib.bib129)]
    proposed learning the distribution of actions in a continuous-time action sequence
    using temporal normalizing flows (NF) [[190](#bib.bib190), [191](#bib.bib191)]
    conditioned on the dynamics of the sequence and the action features, e.g., minimum
    completion time. Such a flow-based formulation facilitates closed-form and faster
    sampling as well as more accurate predictions than the intensity-based models [[191](#bib.bib191),
    [192](#bib.bib192)]. In addition to action prediction, the authors also addressed
    goal prediction and sequence generation given the goal.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $dC(t)=C(t+dt)-C(t)$ 和 ^∗ 表示对历史的依赖。然后，他们通过重要性采样进行了测试。与泊松过程中的常数强度函数相比，初步工作的扩展动态地计算了从过去观测中获得的强度参数 [[53](#bib.bib53)]。他们的模型还通过变分自编码器（VAE）采样潜在变量来支持随机时间和动作预测。Gupta
    和 Bedathur [[129](#bib.bib129)] 提出了使用时间归一化流（NF） [[190](#bib.bib190)， [191](#bib.bib191)]
    学习连续时间动作序列中的动作分布，该分布以序列的动态和动作特征（如最短完成时间）为条件。这样的流式公式比基于强度的模型 [[191](#bib.bib191)，
    [192](#bib.bib192)] 提供了封闭形式和更快的采样，以及更准确的预测。除了动作预测，作者还讨论了给定目标的目标预测和序列生成。
- en: 4 Evaluation Datasets and Metrics
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估数据集与指标
- en: 4.1 Datasets
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: The algorithms for action anticipation are evaluated on a wide range of datasets
    that are temporally labeled with the corresponding action. To better differentiate
    current datasets, we consider several characteristics as follows.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 动作预测算法在广泛的数据集上进行评估，这些数据集按时间标记了相应的动作。为了更好地区分当前数据集，我们考虑了以下几个特征。
- en: 'TABLE III: Representative benchmark datasets for human action anticipation.
    The following abbreviations are used in the table. Type: Activities of daily living
    (ADL). Sensor modality: Depth (D), Au (Audio). Comp. act.: Composite activities.
    Conc. act.: Concurrent activities. Spont.: Spontaneity.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：用于人类动作预测的代表性基准数据集。表中使用了以下缩写。类型：日常生活活动（ADL）。传感器模态：深度（D），音频（Au）。复合活动：Composite
    activities。并发活动：Concurrent activities。自发性：Spontaneity。
- en: '| Dataset | Year | Type | View | Multi-view | Comp. act. | Conc. act. | Spont.
    | Hours | Segments | Classes | Sensor modality |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | 类型 | 视角 | 多视角 | 复合活动 | 并发活动 | 自发性 | 小时 | 分段 | 类别 | 传感器模态 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| TV-I [[57](#bib.bib57)] | 2010 | Movie | Shooting | ✗ | ✗ | ✗ | Medium |
    – | 300 | 4 | RGB |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| TV-I [[57](#bib.bib57)] | 2010 | 电影 | 拍摄 | ✗ | ✗ | ✗ | 中等 | – | 300 | 4 |
    RGB |'
- en: '| 50Salads [[70](#bib.bib70)] | 2013 | Cooking | Top-view | ✗ | ✓ | ✗ | Medium
    | 4.5 | 966 | 17 | RGB |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 50Salads [[70](#bib.bib70)] | 2013 | 烹饪 | 顶视图 | ✗ | ✓ | ✗ | 中等 | 4.5 | 966
    | 17 | RGB |'
- en: '| CAD-120 [[108](#bib.bib108)] | 2013 | ADL | Shooting | ✗ | ✓ | ✗ | Low |
    0.57 | 120 | 12 | RGB, D |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| CAD-120 [[108](#bib.bib108)] | 2013 | ADL | 拍摄 | ✗ | ✓ | ✗ | 低 | 0.57 | 120
    | 12 | RGB, D |'
- en: '| Breakfast [[66](#bib.bib66)] | 2014 | Cooking | Shooting | ✓ | ✓ | ✗ | Medium
    | 77 | 11.2K | 48 | RGB |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 早餐 [[66](#bib.bib66)] | 2014 | 烹饪 | 拍摄 | ✓ | ✓ | ✗ | 中等 | 77 | 11.2K | 48
    | RGB |'
- en: '| THUMOS14 [[59](#bib.bib59)] | 2014 | Web | Shooting | ✗ | ✗ | ✗ | Medium
    | 20 | 6.3K | 20 | RGB |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| THUMOS14 [[59](#bib.bib59)] | 2014 | 网络 | 拍摄 | ✗ | ✗ | ✗ | 中等 | 20 | 6.3K
    | 20 | RGB |'
- en: '| Charades [[123](#bib.bib123)] | 2016 | ADL | Shooting | ✗ | ✗ | ✓ | Low |
    82.3 | 66.5K | 157 | RGB |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Charades [[123](#bib.bib123)] | 2016 | ADL | 拍摄 | ✗ | ✗ | ✓ | 低 | 82.3 |
    66.5K | 157 | RGB |'
- en: '| TVSeries [[62](#bib.bib62)] | 2016 | Movie | Shooting | ✗ | ✗ | ✗ | Medium
    | 16 | 6.2K | 30 | RGB |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| TVSeries [[62](#bib.bib62)] | 2016 | 电影 | 拍摄 | ✗ | ✗ | ✗ | 中等 | 16 | 6.2K
    | 30 | RGB |'
- en: '| EGTEA Gaze+[[69](#bib.bib69)] | 2018 | Cooking | Egocentric | ✗ | ✗ | ✗ |
    Medium | 28 | 10.3K | 106 | RGB, Gaze |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| EGTEA Gaze+[[69](#bib.bib69)] | 2018 | 烹饪 | 自我中心 | ✗ | ✗ | ✗ | 中等 | 28 |
    10.3K | 106 | RGB, Gaze |'
- en: '| EpicKitchens-50[[7](#bib.bib7)] | 2018 | Cooking | Egocentric | ✗ | ✗ | Few
    | High | 55 | 39.6K | 2,513 | RGB, Au |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| EpicKitchens-50[[7](#bib.bib7)] | 2018 | 烹饪 | 自我中心 | ✗ | ✗ | 少量 | 高 | 55
    | 39.6K | 2,513 | RGB, Au |'
- en: '| AVA [[111](#bib.bib111)] | 2018 | Movie | Shooting | ✗ | ✗ | ✓ | Medium |
    107.5 | 38.6K | 60 | RGB |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| AVA [[111](#bib.bib111)] | 2018 | 电影 | 拍摄 | ✗ | ✗ | ✓ | 中等 | 107.5 | 38.6K
    | 60 | RGB |'
- en: '| EpicKitchens-100 [[68](#bib.bib68)] | 2020 | Cooking | Egocentric | ✗ | ✗
    | ✓ | High | 100 | 90K | 3,807 | RGB, Au |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| EpicKitchens-100 [[68](#bib.bib68)] | 2020 | 烹饪 | 自我中心 | ✗ | ✗ | ✓ | 高 |
    100 | 90K | 3,807 | RGB, Au |'
- en: '| Ego4D [[47](#bib.bib47)] | 2022 | ADL | Egocentric | ✗ | ✗ | ✓ | High | 243
    | – | 4,756 | RGB, Au |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Ego4D [[47](#bib.bib47)] | 2022 | ADL | 自我中心 | ✗ | ✗ | ✓ | 高 | 243 | – |
    4,756 | RGB, Au |'
- en: '| Assembly101 [[193](#bib.bib193)] | 2022 | ADL | Multi-view | ✓ | ✓ | ✗ |
    Medium | 513 | 1M | 1,380 | RGB, Gray, Pose |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Assembly101 [[193](#bib.bib193)] | 2022 | ADL | 多视角 | ✓ | ✓ | ✗ | 中等 | 513
    | 1M | 1,380 | RGB, Gray, Pose |'
- en: 'Spontaneous behavior: Activities in real-world scenarios are performed naturally,
    yet many existing datasets involve subjects following some scripts or certain
    activities, which may oversimplify the reality.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 自发行为：现实场景中的活动自然发生，但许多现有数据集涉及受试者按照某些脚本或特定活动进行，这可能过于简化现实。
- en: 'View-point: Although certain datasets, such as EpicKichens [[7](#bib.bib7),
    [68](#bib.bib68)] and Ego4D [[47](#bib.bib47)], provide recordings from an egocentric
    viewpoint, the majority of the utilized action anticipation datasets are derived
    from a third-person perspective where the subjects are typically centrally positioned.
    Conversely, datasets captured by automated monitoring systems employing fixed
    cameras may exhibit subjects that are off-center, occluded, or partially outside
    the field of view. Given the need for methods to exhibit robustness in the face
    of viewpoint variations, we also provide the information if samples captured from
    diverse perspectives are included within benchmark datasets.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 视角：尽管某些数据集，如EpicKitchens [[7](#bib.bib7), [68](#bib.bib68)]和Ego4D [[47](#bib.bib47)]，提供了自我中心视角的录制，大多数用于动作预测的数据集源自第三人称视角，其中受试者通常居中。相反，由固定摄像头拍摄的自动监控系统数据集可能会展示偏离中心、遮挡或部分在视野外的受试者。鉴于需要应对视角变化的鲁棒性方法，我们还提供了基准数据集中是否包含从不同视角捕获的样本的信息。
- en: 'Composite activities: Certain elementary activities can compose complex activities,
    forming a scenario as illustrated in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3.2 Conditioning
    on extra information ‣ 3.3 Narrowing anticipation space by using additional information
    ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation").
    For instance, cutting bread, spreading butter, and eating at a table may indicate
    a having breakfast scenario. It is therefore advantageous to include these composite
    activities in the dataset as well.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 复合活动：某些基本活动可以组成复杂活动，形成场景，如图 [6](#S3.F6 "Figure 6 ‣ 3.3.2 Conditioning on extra
    information ‣ 3.3 Narrowing anticipation space by using additional information
    ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation")所示。例如，切面包、涂黄油和在桌子上吃饭可能表示早餐场景。因此，将这些复合活动纳入数据集也是有利的。
- en: 'Concurrent activities: Simultaneous performance of activities, such as reading
    and listening to music, is common in daily life. Incorporating concurrent activities
    within the dataset enhances its reflection of real-world scenarios.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 同时活动：日常生活中同时进行多种活动（如阅读和听音乐）很常见。在数据集中融入同时活动，可以更好地反映现实场景。
- en: Table [III](#S4.T3 "TABLE III ‣ 4.1 Datasets ‣ 4 Evaluation Datasets and Metrics
    ‣ A Survey on Deep Learning Techniques for Action Anticipation") presents a comprehensive
    overview, comparing the essential attributes of prominent public untrimmed video
    datasets commonly employed for assessing action anticipation methods. This comparison
    encompasses the aforementioned criteria in conjunction with dataset sizes. Subsequently,
    we provide an in-depth exploration of these datasets, delving into their specific
    details.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [III](#S4.T3 "TABLE III ‣ 4.1 Datasets ‣ 4 Evaluation Datasets and Metrics
    ‣ A Survey on Deep Learning Techniques for Action Anticipation") 提供了一个全面的概述，比较了用于评估动作预测方法的主要公共未修剪视频数据集的基本属性。此比较涵盖了前述标准以及数据集的大小。随后，我们将深入探讨这些数据集，*详细了解*它们的具体细节。
- en: 4.1.1 Web & Movie datasets
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 网络与电影数据集
- en: 'Several widely used benchmark datasets are collected from web or movies [[57](#bib.bib57),
    [59](#bib.bib59), [62](#bib.bib62), [111](#bib.bib111)]. The TV-Interaction dataset [[57](#bib.bib57)]
    contains 300 videos clips with human interactions. These videos are categorized
    into 4 interaction categories: handshake, high five, hug, and kiss, and annotated
    with the upper body of people, discrete head orientation and interaction. THUMOS14 [[59](#bib.bib59)]
    is a popular benchmark for temporal action detection and anticipation [[45](#bib.bib45),
    [76](#bib.bib76), [80](#bib.bib80), [83](#bib.bib83)]. It contains over 20 hours
    of sport videos annotated with 20 actions. Since the training set contains only
    trimmed videos that cannot be used to train action anticipation models, prior
    works usually use the validation set (including 3K action instances in 200 untrimmed
    videos) for training and the test set (including 3.3K action instances in 213
    untrimmed videos) for evaluation. TVSeries [[62](#bib.bib62)] contains video footage
    from six popular TV series, about 150 minutes for each and about 16 hours in total.
    The dataset totally includes 30 realistic, everyday actions (e.g., pick up, open
    door, drink, etc.), and every action occurs at least 50 times in the dataset.
    TVSeries contains many unconstrained perspectives and a wide variety of backgrounds.
    AVA [[111](#bib.bib111)] provides audio-visual annotations for about 15 minute
    long movie clips. The AVA Action subset contains 430 videos split into 235 for
    training, 64 for validation, and 131 for testing, covering 60 atomic action classes.
    The videos are annotated in a 1 second interval.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 几个广泛使用的基准数据集来自网络或电影 [[57](#bib.bib57), [59](#bib.bib59), [62](#bib.bib62), [111](#bib.bib111)]。TV-Interaction
    数据集 [[57](#bib.bib57)] 包含 300 个具有人体互动的视频片段。这些视频被分类为 4 种互动类别：握手、高五、拥抱和亲吻，并注释了人的上半身、头部姿态和互动。THUMOS14 [[59](#bib.bib59)]
    是一个流行的时间动作检测和预测基准数据集 [[45](#bib.bib45), [76](#bib.bib76), [80](#bib.bib80), [83](#bib.bib83)]。它包含超过
    20 小时的运动视频，标注了 20 种动作。由于训练集仅包含修剪过的视频，不能用于训练动作预测模型，先前的工作通常使用验证集（包括 200 个未修剪视频中的
    3K 动作实例）进行训练，使用测试集（包括 213 个未修剪视频中的 3.3K 动作实例）进行评估。TVSeries [[62](#bib.bib62)]
    包含来自六个流行电视剧的视频素材，每个视频约 150 分钟，总时长约 16 小时。该数据集总共包括 30 种真实的日常动作（如捡起、开门、喝水等），每种动作在数据集中至少出现
    50 次。TVSeries 包含许多不受限制的视角和各种背景。AVA [[111](#bib.bib111)] 为约 15 分钟长的电影片段提供了视听注释。AVA
    Action 子集包含 430 个视频，分为 235 个用于训练，64 个用于验证，131 个用于测试，覆盖 60 种原子动作类别。视频以 1 秒的间隔进行标注。
- en: 4.1.2 Cooking activities
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 烹饪活动
- en: 'One popular category of datasets in this domain contains videos of cooking
    activities: Epic-Kitchens [[7](#bib.bib7), [167](#bib.bib167)], EGTEA Gaze+ [[69](#bib.bib69)],
    Breakfast [[66](#bib.bib66)], and 50Salads [[70](#bib.bib70)]. Among these, Epic-Kitchens
    and EGTEA Gaze+ are egocentric (first-person) datasets. The videos are recorded
    with a wearable camera that captures the scene directly in front of the user,
    in which only hands are visible in the center of the camera view. EpicKitchens-100
    consists of 700 long unscripted videos of cooking activities totalling 100 hours.
    It contains 90K action annotations, 97 verbs, and 300 nouns. Considering all unique
    (verb, noun) pairs in the public training set yields 3,806 unique actions. EpicKitchens-55
    is an earlier version of the EpicKitchens-100 with 39,596 segments labeled with
    125 verbs, 352 nouns, and 2,513 combinations (actions), totalling 55 hours. EGTEA
    Gaze+ contains 28 hours of videos including 10.3K action annotations, 19 verbs,
    51 nouns, and 106 unique actions.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域，一个受欢迎的数据集类别包含了烹饪活动的视频：Epic-Kitchens [[7](#bib.bib7), [167](#bib.bib167)]、EGTEA Gaze+ [[69](#bib.bib69)]、Breakfast [[66](#bib.bib66)]
    和 50Salads [[70](#bib.bib70)]。其中，Epic-Kitchens 和 EGTEA Gaze+ 是以自我为中心（第一人称）的数据集。这些视频是通过可穿戴摄像头录制的，直接捕捉用户面前的场景，其中只有手在摄像头视野的中央可见。EpicKitchens-100
    包含 700 个长时间的非脚本化烹饪活动视频，总计 100 小时。它包含 90K 动作注释、97 个动词和 300 个名词。考虑到公共训练集中的所有唯一（动词，名词）对，得出
    3,806 个独特的动作。EpicKitchens-55 是 EpicKitchens-100 的早期版本，包含 39,596 个标记了 125 个动词、352
    个名词和 2,513 种组合（动作）的片段，总计 55 小时。EGTEA Gaze+ 包含 28 小时的视频，包括 10.3K 动作注释、19 个动词、51
    个名词和 106 个独特的动作。
- en: Breakfast [[66](#bib.bib66)], and 50Salads [[70](#bib.bib70)] contain cooking
    activities like making breakfast, or preparing salads. Subjects are asked to prepare
    a specific recipe in each video. The activities are therefore performed without
    hesitation or mistakes (reduced spontaneity). Moreover, the datasets lack the
    presence of other activities that are irrelevant to cooking, e.g. drinking water,
    but often occur in real-life. The Breakfast dataset comprises 1,712 videos of
    52 different individuals making breakfast in 18 different kitchens, totalling
    77 hours. Every video is categorized into one of the 10 activities related to
    breakfast preparation. The videos are annotated by 48 fine-grained actions. The
    50Salads dataset comprises 50 top-view videos of 25 people preparing a salad.
    The dataset contains over 4 hours of RGB-D video data, annotated with 17 fine-grained
    action labels and 3 high-level activities.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Breakfast [[66](#bib.bib66)] 和 50Salads [[70](#bib.bib70)] 包含了如制作早餐或准备沙拉等烹饪活动。受试者被要求在每个视频中准备一个特定的食谱。因此，这些活动的执行没有犹豫或错误（减少了自发性）。此外，这些数据集缺乏与烹饪无关的其他活动的出现，例如饮水，但这些活动在现实生活中经常发生。Breakfast
    数据集包括 1,712 个视频，涉及 52 名不同的个人在 18 个不同的厨房中制作早餐，总计 77 小时。每个视频被分类为与早餐准备相关的 10 种活动中的一种。视频经过
    48 个细粒度动作的注释。50Salads 数据集包括 25 人准备沙拉的 50 个顶视角视频。该数据集包含超过 4 小时的 RGB-D 视频数据，注释有
    17 个细粒度动作标签和 3 个高级活动。
- en: 4.1.3 Other daily living activities
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 其他日常生活活动
- en: This section presents datasets of other activities of daily living (ADL) that
    are not focussed on cooking. CAD-120 [[108](#bib.bib108)] is a small-sized dataset
    (about 60K frames in total). This dataset comprises 120 RGB-D action videos covering
    12 daily activities, which are captured using the Kinect sensor. Action videos
    are performed by 4 subjects following a script in different rooms. The Charades [[123](#bib.bib123)]
    dataset is recorded by hundreds of people in their private homes following strict
    scripts. It has 7,985 videos for training and 1,863 videos for testing. The dataset
    is collected in 15 types of indoor scenes, involves interactions with 46 object
    classes and has a vocabulary of 30 verbs leading to 157 action classes. On average
    there are 6.8 actions per video.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了其他日常生活（ADL）活动的数据集，这些活动并不专注于烹饪。CAD-120 [[108](#bib.bib108)] 是一个小型数据集（总计约
    60K 帧）。该数据集包含 120 个 RGB-D 动作视频，涵盖了 12 种日常活动，这些视频是使用 Kinect 传感器捕捉的。动作视频由 4 位受试者在不同房间内按照脚本进行。Charades [[123](#bib.bib123)]
    数据集是由数百人在其私人住宅中按照严格脚本录制的。该数据集有 7,985 个用于训练的视频和 1,863 个用于测试的视频。数据集收集于 15 种室内场景，涉及与
    46 类对象的互动，并且具有 30 个动词的词汇，形成 157 个动作类别。每个视频平均有 6.8 个动作。
- en: Ego4D [[47](#bib.bib47)] is the most extensive daily-life egocentric video dataset,
    currently available to research. The forecasting benchmark from Ego4D consists
    of 120 hours of annotated videos from 53 different scenarios. The annotations
    provided contain 478 noun types and 115 verb types, with a total amount of 4756
    action classes among training and validation set. The number of training and validation
    samples has doubled in the second version.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Ego4D [[47](#bib.bib47)] 是目前可供研究的最广泛的日常生活自我中心视频数据集。Ego4D的预测基准包括来自53种不同场景的120小时标注视频。提供的标注包含478种名词和115种动词类型，训练集和验证集共包含4756个动作类别。第二版中，训练和验证样本数量翻了一倍。
- en: The Assembly101 [[193](#bib.bib193)] dataset features 4321 videos of participants
    assembling and disassembling 101 toy vehicles. With no fixed guidelines, this
    led to varied action sequences, mistakes, and corrections. Assembly101, unique
    for its multi-view recordings, has 8 static and 4 egocentric cameras. It contains
    513 hours of footage with over 100K coarse and 1M fine-grained segments, categorized
    into 1380 fine-grained and 202 coarse classes. The static cameras record in RGB,
    while the egocentric ones are in monochrome.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Assembly101 [[193](#bib.bib193)] 数据集包含4321段参与者组装和拆解101种玩具车的视频。由于没有固定的指导方针，这导致了各种不同的动作序列、错误和修正。Assembly101以其多视角录制而独特，拥有8个静态和4个自我中心摄像头。它包含513小时的录像，涵盖了超过10万条粗略和100万条精细分段，分类为1380个精细类别和202个粗略类别。静态摄像头记录RGB视频，而自我中心摄像头则为单色。
- en: 4.2 Evluation metrics
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估指标
- en: Top-k accuracy is widely used to evaluate the overall performance of action
    anticipation methods. It is computed by checking if the ground truth label is
    among the top-k predictions. Accounting for the class imbalance in a long-tail
    distribution, many methods tend to use class-aware measures such as Class Mean
    Top-k Accuracy to evaluate the performance. Note that some works [[194](#bib.bib194),
    [98](#bib.bib98)] refer to this metric as Mean Top-k Recall. In the case of long-term
    anticipation, where predictions for a large number of frames are made, some works [[46](#bib.bib46),
    [127](#bib.bib127), [5](#bib.bib5), [82](#bib.bib82)] report the class mean top-1
    accuracy of the predicted frames (Mean over Classes (MoC)) as the evaluation metric.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k准确率被广泛用于评估动作预测方法的整体表现。通过检查真实标签是否在前k个预测中来计算。考虑到长尾分布中的类别不平衡，许多方法倾向于使用类感知度量，如类均值Top-k准确率来评估性能。注意，某些研究[[194](#bib.bib194),
    [98](#bib.bib98)]将此指标称为均值Top-k召回率。在长时间预测的情况下，即对大量帧进行预测，一些研究[[46](#bib.bib46),
    [127](#bib.bib127), [5](#bib.bib5), [82](#bib.bib82)]报告了预测帧的类别均值Top-1准确率（类别均值（MoC））作为评估指标。
- en: In the case of multiple step predictions, the Mean Average Precision (mAP) [[121](#bib.bib121)]
    and its augmented version Calibrated Average Precision (cAP) [[62](#bib.bib62)]
    are widely used. After average precision (AP) is calculated for each action class
    based on the following equation,
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在多步骤预测的情况下，均值平均精度（mAP）[[121](#bib.bib121)]及其增强版本校准平均精度（cAP）[[62](#bib.bib62)]被广泛使用。计算每个动作类别的平均精度（AP）基于以下公式，
- en: '|  | $\mathrm{AP}=\frac{\sum_{p}\mathrm{Prec}(p)\times\mathrm{I}(p)}{\sum\mathrm{TP}},$
    |  | (3) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{AP}=\frac{\sum_{p}\mathrm{Prec}(p)\times\mathrm{I}(p)}{\sum\mathrm{TP}},$
    |  | (3) |'
- en: 'where I($p$) is equal to 1 if frame $p$ is a true positive (TP) and 0 otherwise,
    the mean average precision is then computed by taking the mean over all action
    classes. Calibrated average precision is an augmented version of the mAP, accounting
    for the class imbalance phenomenon. It uses a parameter $w$, which is the ratio
    between negative frames and positive frames in the calculation of precision, so
    that the average precision is calculated as if there were an equal amount of positive
    and negative frames:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 其中I($p$)等于1，如果帧$p$是一个真正的正例（TP），否则为0，均值平均精度通过对所有动作类别取均值得到。校准平均精度是mAP的增强版本，考虑了类别不平衡现象。它使用参数$w$，这是在计算精度时负帧与正帧的比例，从而使得平均精度的计算假设正负帧数量相等：
- en: '|  | $\displaystyle\mathrm{cPrec}$ | $\displaystyle=\frac{\mathrm{TP}}{\mathrm{TP}+\frac{\mathrm{FP}}{w}}$
    |  | (4) |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{cPrec}$ | $\displaystyle=\frac{\mathrm{TP}}{\mathrm{TP}+\frac{\mathrm{FP}}{w}}$
    |  | (4) |'
- en: '|  | $\displaystyle\mathrm{cAP}$ | $\displaystyle=\frac{\sum_{p}\mathrm{cPrec}(p)\times\mathrm{I}(p)}{\sum\mathrm{TP}}.$
    |  | (5) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{cAP}$ | $\displaystyle=\frac{\sum_{p}\mathrm{cPrec}(p)\times\mathrm{I}(p)}{\sum\mathrm{TP}}.$
    |  | (5) |'
- en: As treating predictions for each future time-step independently when calculating
    accuracy does not account for the sequential nature of the prediction task where
    the order of predictions is important, Edit Distance (ED), computed as the Damerau-Levenshtein
    distance [[195](#bib.bib195), [196](#bib.bib196)], is employed to evaluate the
    predicted action sequences [[47](#bib.bib47)]. The goal of this measure is to
    assess performance in a way which is robust to some error in the predicted order
    of future actions. A predicted verb/noun is considered ”correct” if it matches
    the ground truth verb label at a specific timestep. The allowed operations to
    compute the edit distance are insertions, deletions, substitutions and transpositions
    of any two predicted actions. The lower the ED is the more similar are the anticipated
    sequences to the ground-truth.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在计算准确性时将每个未来时间步的预测独立处理并没有考虑预测任务的序列性质，预测的顺序很重要，因此采用了 Edit Distance (ED)，计算为
    Damerau-Levenshtein 距离 [[195](#bib.bib195), [196](#bib.bib196)]，来评估预测的动作序列 [[47](#bib.bib47)]。此度量的目标是以对未来动作预测顺序的某些错误具有鲁棒性的方式评估性能。如果预测的动词/名词与特定时间步的真实动词标签匹配，则认为预测是“正确的”。计算编辑距离允许的操作包括插入、删除、替换和任意两个预测动作的交换。ED
    值越低，预期序列与真实情况的相似度越高。
- en: 5 Benchmark Protocols and Results
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 基准协议和结果
- en: 5.1 Short-term anticipation
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 短期预测
- en: 'TABLE IV: Comparison of the state-of-the-art methods addressing predictions
    at multiple timestamps on TVSeries [[62](#bib.bib62)] (mean cAP %) and THUMOS-14 [[59](#bib.bib59)]
    (mAP %). The optimal performance in each column within every block is indicated
    in bold, while the supreme overall performance in each column is further denoted
    by an underscore.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：比较了针对多个时间戳的预测的最先进方法在 TVSeries [[62](#bib.bib62)]（平均 cAP %）和 THUMOS-14 [[59](#bib.bib59)]（mAP
    %）上的表现。每个块中每列的最佳性能用粗体表示，而每列中的总体最佳性能进一步用下划线表示。
- en: '| Method | Year | Back- bone | Pre-train | TVSeries | THUMOS14 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 主干 | 预训练 | TVSeries | THUMOS14 |'
- en: '| 1.0 | 2.0 | Avg. | 1.0 | 2.0 | Avg. |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 | 2.0 | 平均 | 1.0 | 2.0 | 平均 |'
- en: '| ED [[45](#bib.bib45)] | 2017 | VGG | IN1K | 68.8 | 66.7 | 68.7 | – | – |
    – |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| ED [[45](#bib.bib45)] | 2017 | VGG | IN1K | 68.8 | 66.7 | 68.7 | – | – |
    – |'
- en: '| RED [[45](#bib.bib45)] | 2017 | 70.2 | 66.8 | 69.4 | – | – | – |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| RED [[45](#bib.bib45)] | 2017 | 70.2 | 66.8 | 69.4 | – | – | – |'
- en: '| TTPP[[76](#bib.bib76)] | 2020 | 71.6 | 69.3 | 71.3 | – | – | – |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| TTPP[[76](#bib.bib76)] | 2020 | 71.6 | 69.3 | 71.3 | – | – | – |'
- en: '| ED [[45](#bib.bib45)] | 2017 | TS | ANet1.3 | 74.6 | 71.0 | 74.5 | 36.8 |
    31.6 | 36.6 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| ED [[45](#bib.bib45)] | 2017 | TS | ANet1.3 | 74.6 | 71.0 | 74.5 | 36.8 |
    31.6 | 36.6 |'
- en: '| RED [[45](#bib.bib45)] | 2017 | 75.5 | 71.2 | 75.1 | 37.5 | 32.1 | 37.5 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| RED [[45](#bib.bib45)] | 2017 | 75.5 | 71.2 | 75.1 | 37.5 | 32.1 | 37.5 |'
- en: '| TRN [[75](#bib.bib75)] | 2019 | 75.9 | 72.3 | 75.7 | 39.1 | 34.3 | 38.9 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| TRN [[75](#bib.bib75)] | 2019 | 75.9 | 72.3 | 75.7 | 39.1 | 34.3 | 38.9 |'
- en: '| TTPP[[76](#bib.bib76)] | 2020 | 77.6 | 74.9 | 77.9 | 41.0 | 37.3 | 40.9 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| TTPP[[76](#bib.bib76)] | 2020 | 77.6 | 74.9 | 77.9 | 41.0 | 37.3 | 40.9 |'
- en: '| LAP [[77](#bib.bib77)] | 2020 | 78.9 | 75.5 | 78.7 | 43.2 | 37.0 | 42.6 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| LAP [[77](#bib.bib77)] | 2020 | 78.9 | 75.5 | 78.7 | 43.2 | 37.0 | 42.6 |'
- en: '| OadTR[[80](#bib.bib80)] | 2021 | 78.2 | 74.3 | 77.8 | 46.8 | 41.1 | 45.9
    |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| OadTR[[80](#bib.bib80)] | 2021 | 78.2 | 74.3 | 77.8 | 46.8 | 41.1 | 45.9
    |'
- en: '| LSTR [[79](#bib.bib79)] | 2021 | – | – | 80.8 | – | – | 50.1 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| LSTR [[79](#bib.bib79)] | 2021 | – | – | 80.8 | – | – | 50.1 |'
- en: '| TeSTra [[83](#bib.bib83)] | 2022 |  |  | – | – | – | 55.7 | 47.8 | 55.3 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| TeSTra [[83](#bib.bib83)] | 2022 |  |  | – | – | – | 55.7 | 47.8 | 55.3 |'
- en: '| TTPP [[76](#bib.bib76)] | 2020 |  |  | – | – | – | 43.6 | 38.7 | 42.8 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| TTPP [[76](#bib.bib76)] | 2020 |  |  | – | – | – | 43.6 | 38.7 | 42.8 |'
- en: '| LSTR [[79](#bib.bib79)] | 2021 |  |  | – | – | – | 53.3 | 45.7 | 52.6 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| LSTR [[79](#bib.bib79)] | 2021 |  |  | – | – | – | 53.3 | 45.7 | 52.6 |'
- en: '| OadTR[[80](#bib.bib80)] | 2021 | TS | K400 | 80.1 | 75.7 | 79.1 | 54.6 |
    46.8 | 53.5 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| OadTR[[80](#bib.bib80)] | 2021 | TS | K400 | 80.1 | 75.7 | 79.1 | 54.6 |
    46.8 | 53.5 |'
- en: '| TeSTra [[83](#bib.bib83)] | 2022 |  |  | – | – | – | 57.4 | 48.9 | 56.8 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| TeSTra [[83](#bib.bib83)] | 2022 |  |  | – | – | – | 57.4 | 48.9 | 56.8 |'
- en: 'Short-term action anticipation can be broadly classified into two types: future
    action predictions at multiple timestamps and single future action prediction.
    The former frequently employs datasets that incorporate third-person perspectives,
    such as TVSeries [[62](#bib.bib62)] and THUMOS14 [[59](#bib.bib59)] (Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 TVSeries and THUMOS14 ‣ 5.1 Short-term anticipation ‣ 5 Benchmark Protocols
    and Results ‣ A Survey on Deep Learning Techniques for Action Anticipation")),
    while the latter is typically evaluated using egocentric datasets such as EpicKitchens [[68](#bib.bib68)]
    (Section [5.1.2](#S5.SS1.SSS2 "5.1.2 EpicKitchens-100 ‣ 5.1 Short-term anticipation
    ‣ 5 Benchmark Protocols and Results ‣ A Survey on Deep Learning Techniques for
    Action Anticipation")).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 短期动作预测大致可以分为两种类型：在多个时间戳的未来动作预测和单一未来动作预测。前者常常使用包含第三人称视角的数据集，如TVSeries [[62](#bib.bib62)]和THUMOS14 [[59](#bib.bib59)]（第[5.1.1节](#S5.SS1.SSS1
    "5.1.1 TVSeries and THUMOS14 ‣ 5.1 Short-term anticipation ‣ 5 Benchmark Protocols
    and Results ‣ A Survey on Deep Learning Techniques for Action Anticipation")），而后者通常使用以自我为中心的数据集进行评估，如EpicKitchens [[68](#bib.bib68)]（第[5.1.2节](#S5.SS1.SSS2
    "5.1.2 EpicKitchens-100 ‣ 5.1 Short-term anticipation ‣ 5 Benchmark Protocols
    and Results ‣ A Survey on Deep Learning Techniques for Action Anticipation")）。
- en: 5.1.1 TVSeries and THUMOS14
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 TVSeries和THUMOS14
- en: Table [IV](#S5.T4 "TABLE IV ‣ 5.1 Short-term anticipation ‣ 5 Benchmark Protocols
    and Results ‣ A Survey on Deep Learning Techniques for Action Anticipation") showcases
    the performance of approaches that address predictions at multiple timestamps,
    with results detailed at times 1.0 and 2.0 seconds. Comprehensive results spanning
    0.25 to 2.0 seconds can be found in the supplementary material. The chosen evaluation
    metrics for TVSeries and THUMOS14 are mean calibrated average precision (mcAP)
    and mean average precision (mAP), respectively, as detailed in Section [4.2](#S4.SS2
    "4.2 Evluation metrics ‣ 4 Evaluation Datasets and Metrics ‣ A Survey on Deep
    Learning Techniques for Action Anticipation"). The backbones adopted by the methods
    range extensively from pre-trained ImageNet-1K (IN1K) models like VGG-16 [[37](#bib.bib37)]
    to two-stream video models [[61](#bib.bib61)] (TS), pre-trained either on ActivityNet [[126](#bib.bib126)]
    (ANet1.3) or Kinetics [[41](#bib.bib41)] (K400). Within these two-stream models,
    the spatial and temporal subnetworks typically employ ResNet [[38](#bib.bib38)]
    and BN-Inception [[197](#bib.bib197)] separately.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [IV](#S5.T4 "TABLE IV ‣ 5.1 Short-term anticipation ‣ 5 Benchmark Protocols
    and Results ‣ A Survey on Deep Learning Techniques for Action Anticipation") 展示了在多个时间戳进行预测的方法的表现，结果详细列在1.0秒和2.0秒时。0.25秒到2.0秒的综合结果可以在补充材料中找到。TVSeries和THUMOS14选择的评估指标分别是均值校准平均精度（mcAP）和均值平均精度（mAP），详见第[4.2节](#S4.SS2
    "4.2 Evluation metrics ‣ 4 Evaluation Datasets and Metrics ‣ A Survey on Deep
    Learning Techniques for Action Anticipation")。这些方法采用的骨干网广泛涵盖了从预训练的ImageNet-1K（IN1K）模型如VGG-16 [[37](#bib.bib37)]到两流视频模型 [[61](#bib.bib61)]
    (TS)，这些模型分别在ActivityNet [[126](#bib.bib126)] (ANet1.3)或Kinetics [[41](#bib.bib41)]
    (K400)上进行预训练。在这些两流模型中，空间和时间子网络通常分别采用ResNet [[38](#bib.bib38)]和BN-Inception [[197](#bib.bib197)]。
- en: Considering results on TVSeries, TTPP [[76](#bib.bib76)], which utilizes VGG-16
    as the backbone, offers the best performance, boasting an average mcAP of 71.3%.
    When transitioning to two-stream models pre-trained on ActivityNet, performance
    generally improves. For instance, LSTR [[79](#bib.bib79)], which only provides
    an average score, surpasses others with an mcAP of 80.8%. Pre-training with a
    more extensive dataset such as Kinetics enhances performance further, as demonstrated
    by OadTR [[80](#bib.bib80)], which reports a 1.3% improvement in the average score
    ($77.8\%\rightarrow 79.1\%$). Parallel observations can be made on THUMOS14\.
    The state-of-the-art method TeSTra [[83](#bib.bib83)] achieves an average mAP
    of 55.3%, when pre-trained on ActivityNet, and exhibits an increase of 1.5% when
    Kinetics is used for pre-training. Additionally, we observe that transformer-based
    approaches, such as OadTR, LSTR, and TeSTra, demonstrate superior results compared
    to non-transformer-based approaches.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到TVSeries的结果，利用VGG-16作为骨干网的TTPP [[76](#bib.bib76)] 提供了最佳性能，平均mcAP为71.3%。转向在ActivityNet上预训练的两流模型时，性能通常会有所提升。例如，仅提供平均分数的LSTR [[79](#bib.bib79)]
    超过了其他模型，mcAP为80.8%。在Kinetics这样更大数据集上的预训练进一步提升了性能，如OadTR [[80](#bib.bib80)]，其报告的平均分数提高了1.3%（$77.8\%\rightarrow
    79.1\%$）。在THUMOS14中也可以做出类似的观察。最先进的方法TeSTra [[83](#bib.bib83)] 在ActivityNet上预训练时，平均mAP为55.3%，而在Kinetics预训练时提高了1.5%。此外，我们观察到，基于变换器的方法，如OadTR、LSTR和TeSTra，相较于非变换器方法表现更佳。
- en: 5.1.2 EpicKitchens-100
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 EpicKitchens-100
- en: 'TABLE V: Comparison of state-of-the-art methods on the validation and test
    set of EpicKitchens-100 [[68](#bib.bib68)] in terms of mean top-5 recall (%).
    The optimal performance in each column within every block is indicated in bold.
    Additional modalities to the RGB modality: Objects (O), Bounding Boxes (BB), Motion
    (M), Audio (Au).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：在 EpicKitchens-100 [[68](#bib.bib68)] 的验证集和测试集上，最先进方法的平均 top-5 召回率（%）的比较。每个块中每列的最佳性能以粗体显示。附加模态：RGB
    模态的其他模态包括：对象（O），边界框（BB），运动（M），音频（Au）。
- en: '|  | Method | Year | Addl. modality | Backbone | Init. | E2E | Overall |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 年份 | 附加模态 | 骨干网络 | 初始化 | E2E | 总体 |'
- en: '| Verb | Noun | Act. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| Verb | Noun | Act. |'
- en: '| Val | RULSTM [[98](#bib.bib98)] | 2019 | – | TSN | IN1K | ✗ | 27.5 | 29.0
    | 13.3 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| Val | RULSTM [[98](#bib.bib98)] | 2019 | – | TSN | IN1K | ✗ | 27.5 | 29.0
    | 13.3 |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | – | TSN | IN1K | ✗ | 24.2 | 29.8 | 13.0
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | – | TSN | IN1K | ✗ | 24.2 | 29.8 | 13.0
    |'
- en: '| AVT [[67](#bib.bib67)] | 2021 | – | TSN | IN1K | ✗ | 27.2 | 30.7 | 13.6 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| AVT [[67](#bib.bib67)] | 2021 | – | TSN | IN1K | ✗ | 27.2 | 30.7 | 13.6 |'
- en: '| AVT [[67](#bib.bib67)] | 2021 | – | ViT-B | IN21K | ✓ | 30.2 | 31.7 | 14.9
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| AVT [[67](#bib.bib67)] | 2021 | – | ViT-B | IN21K | ✓ | 30.2 | 31.7 | 14.9
    |'
- en: '| MeMViT [[6](#bib.bib6)] | 2022 | – | MViT-B | K400 | ✓ | 32.8 | 33.2 | 15.1
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| MeMViT [[6](#bib.bib6)] | 2022 | – | MViT-B | K400 | ✓ | 32.8 | 33.2 | 15.1
    |'
- en: '| MeMViT [[6](#bib.bib6)] | 2022 | – | MViT-L | K700 | ✓ | 32.2 | 37.0 | 17.7
    |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| MeMViT [[6](#bib.bib6)] | 2022 | – | MViT-L | K700 | ✓ | 32.2 | 37.0 | 17.7
    |'
- en: '| DCR [[71](#bib.bib71)] | 2022 | – | TSM | K400 | ✗ | 32.6 | 32.7 | 16.1 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| DCR [[71](#bib.bib71)] | 2022 | – | TSM | K400 | ✗ | 32.6 | 32.7 | 16.1 |'
- en: '| TeSTra [[83](#bib.bib83)] | 2022 | – | TSN | IN1K | ✗ | 26.8 | 36.2 | 17.0
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| TeSTra [[83](#bib.bib83)] | 2022 | – | TSN | IN1K | ✗ | 26.8 | 36.2 | 17.0
    |'
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | – | MViT-B | K400 | ✗ | 33.3 | 35.5
    | 17.6 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| RAFTformer [[73](#bib.bib73)] | 2023 | – | MViT-B | K400 | ✗ | 33.3 | 35.5
    | 17.6 |'
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | – | MViT-B | K700 | ✗ | 33.7 | 37.1
    | 18.0 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| RAFTformer [[73](#bib.bib73)] | 2023 | – | MViT-B | K700 | ✗ | 33.7 | 37.1
    | 18.0 |'
- en: '|  | RULSTM [[98](#bib.bib98)] | 2019 | O,M | TSN | IN1K | ✗ | 27.8 | 30.8
    | 14.0 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | RULSTM [[98](#bib.bib98)] | 2019 | O,M | TSN | IN1K | ✗ | 27.8 | 30.8
    | 14.0 |'
- en: '|  | TempAgg [[5](#bib.bib5)] | 2020 | O,M,BB | TSN | IN1K | ✗ | 23.2 | 31.4
    | 14.7 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | TempAgg [[5](#bib.bib5)] | 2020 | O,M,BB | TSN | IN1K | ✗ | 23.2 | 31.4
    | 14.7 |'
- en: '|  | AVT [[67](#bib.bib67)] | 2021 | O | ViT-B | IN21K | ✓ | 28.2 | 32.0 |
    15.9 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | AVT [[67](#bib.bib67)] | 2021 | O | ViT-B | IN21K | ✓ | 28.2 | 32.0 |
    15.9 |'
- en: '|  | DCR [[71](#bib.bib71)] | 2022 | O | TSM | K400 | ✗ | – | – | 18.3 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | DCR [[71](#bib.bib71)] | 2022 | O | TSM | K400 | ✗ | – | – | 18.3 |'
- en: '|  | TeSTra [[83](#bib.bib83)] | 2022 | M | TSN | IN1K | ✗ | 30.8 | 35.8 |
    17.6 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | TeSTra [[83](#bib.bib83)] | 2022 | M | TSN | IN1K | ✗ | 30.8 | 35.8 |
    17.6 |'
- en: '|  | AFFT [[105](#bib.bib105)] | 2023 | O,M | TSN | IN1K | ✗ | 21.3 | 32.7
    | 16.4 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | AFFT [[105](#bib.bib105)] | 2023 | O,M | TSN | IN1K | ✗ | 21.3 | 32.7
    | 16.4 |'
- en: '|  | AFFT [[105](#bib.bib105)] | 2023 | O,M,Au | Swin-B | K400 | ✗ | 22.8 |
    34.6 | 18.5 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | AFFT [[105](#bib.bib105)] | 2023 | O,M,Au | Swin-B | K400 | ✗ | 22.8 |
    34.6 | 18.5 |'
- en: '| Test | RULSTM [[98](#bib.bib98)] | 2019 | O,M | TSN | IN1K | ✗ | 25.3 | 26.7
    | 11.2 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Test | RULSTM [[98](#bib.bib98)] | 2019 | O,M | TSN | IN1K | ✗ | 25.3 | 26.7
    | 11.2 |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | O,M,BB | TSN | IN1K | ✗ | 21.8 | 30.6 |
    12.6 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | O,M,BB | TSN | IN1K | ✗ | 21.8 | 30.6 |
    12.6 |'
- en: '| AVT [[67](#bib.bib67)] | 2021 | O | ViT-B | IN21K | ✓ | 25.6 | 28.8 | 12.6
    |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| AVT [[67](#bib.bib67)] | 2021 | O | ViT-B | IN21K | ✓ | 25.6 | 28.8 | 12.6
    |'
- en: '| TCN-TBN [[103](#bib.bib103)] | 2021 | O,M | TBN | IN1K | ✗ | 21.5 | 26.8
    | 11.0 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| TCN-TBN [[103](#bib.bib103)] | 2021 | O,M | TBN | IN1K | ✗ | 21.5 | 26.8
    | 11.0 |'
- en: '| Abst. goal [[116](#bib.bib116)] | 2022 | O,M | TSN | IN1K | ✗ | 31.4 | 30.1
    | 14.3 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Abst. goal [[116](#bib.bib116)] | 2022 | O,M | TSN | IN1K | ✗ | 31.4 | 30.1
    | 14.3 |'
- en: '| AFFT [[105](#bib.bib105)] | 2023 | O,M,Au | Swin-B | K400 | ✗ | 20.7 | 31.8
    | 14.9 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| AFFT [[105](#bib.bib105)] | 2023 | O,M,Au | Swin-B | K400 | ✗ | 20.7 | 31.8
    | 14.9 |'
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | – | MViT-B | K400 | ✗ | 27.3 | 32.8
    | 14.0 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| RAFTformer [[73](#bib.bib73)] | 2023 | – | MViT-B | K400 | ✗ | 27.3 | 32.8
    | 14.0 |'
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | – | MViT-B | K700 | ✗ | 27.4 | 34.0
    | 14.7 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| RAFTformer [[73](#bib.bib73)] | 2023 | – | MViT-B | K700 | ✗ | 27.4 | 34.0
    | 14.7 |'
- en: Table [V](#S5.T5 "TABLE V ‣ 5.1.2 EpicKitchens-100 ‣ 5.1 Short-term anticipation
    ‣ 5 Benchmark Protocols and Results ‣ A Survey on Deep Learning Techniques for
    Action Anticipation") provides a detailed study of various state-of-the-art methods
    on the validation and test sets of EpicKitchens-100 [[68](#bib.bib68)]. It shows
    the overall results, while results for unseen kitchen and tail classes are available
    in the supplementary material. Each of these benchmarks is evaluated based on
    the top-5 recall for verbs, nouns, and actions across all classes. The primary
    metric used to rank the methods is accentuated. If the anticipation approach employs
    a backbone fine-tuned for feature extraction, it is marked with a check mark in
    the column titled E2E in Table [V](#S5.T5 "TABLE V ‣ 5.1.2 EpicKitchens-100 ‣
    5.1 Short-term anticipation ‣ 5 Benchmark Protocols and Results ‣ A Survey on
    Deep Learning Techniques for Action Anticipation"). We also report the modalities
    used by each method, providing a comprehensive analysis of the potential gains
    of multi-modal methods and facilitating a fair comparison across different approaches.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [V](#S5.T5 "TABLE V ‣ 5.1.2 EpicKitchens-100 ‣ 5.1 Short-term anticipation
    ‣ 5 Benchmark Protocols and Results ‣ A Survey on Deep Learning Techniques for
    Action Anticipation") 对 EpicKitchens-100 [[68](#bib.bib68)] 的验证集和测试集上的各种先进方法进行了详细研究。它展示了整体结果，而未见过的厨房和尾部类别的结果则在补充材料中提供。每个基准是基于所有类别中动词、名词和动作的
    top-5 召回率进行评估的。用于排名方法的主要指标被突出显示。如果预测方法使用了为特征提取微调的主干网络，它将在表 [V](#S5.T5 "TABLE V
    ‣ 5.1.2 EpicKitchens-100 ‣ 5.1 Short-term anticipation ‣ 5 Benchmark Protocols
    and Results ‣ A Survey on Deep Learning Techniques for Action Anticipation") 中的
    E2E 列中标记一个对勾。我们还报告了每种方法使用的模态，提供了对多模态方法潜在收益的综合分析，并促进了不同方法之间的公平比较。
- en: When examining the validation set, RAFTformer and MeMViT show the best overall
    performance with a single RGB modality. Specifically, RAFTformer, equipped with
    the MViT-B backbone and initialized with the Kinetics-700 [[198](#bib.bib198)]
    (K700) dataset, exhibits superior performance. Several modalities are typically
    employed for egocentric vision following [[98](#bib.bib98)], including RGB, object
    presence (O), and optical flow (M). Some methods extend this set of modalities
    to include additional features like interacting hand-object bounding boxes (BB) [[5](#bib.bib5)]
    and audio (Au) [[105](#bib.bib105)]. Multi-modal incorporation proves to be highly
    effective for action anticipation. The fusion of these different modalities plays
    a significant role, as seen in the case of AFFT [[105](#bib.bib105)], which surpasses
    the earlier state-of-the-art, RULSTM [[98](#bib.bib98)], significantly on the
    validation set using the exact same features ($14.0\rightarrow 16.4$). Regarding
    backbones, Transformer-based models like ViT, MViT, and Swin consistently outperform
    models such as TSN or TSM [[72](#bib.bib72)], particularly when additional modalities
    are used, and end-to-end training is applied. For the initialization datasets,
    larger datasets like ImageNet-21K [[199](#bib.bib199)] (IN21K), Kinetics-400 (K400),
    Kinetics-700 (K700) generally deliver better performance than those initialized
    with the smaller ImageNet-1K (IN1K) dataset.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证集上，RAFTformer 和 MeMViT 在单一 RGB 模态下表现最佳。具体来说，RAFTformer 配备了 MViT-B 主干网络，并使用
    Kinetics-700 [[198](#bib.bib198)] (K700) 数据集进行初始化，展示了卓越的性能。通常，针对自我中心视觉会使用几种模态，如
    RGB、物体存在 (O) 和光流 (M) [[98](#bib.bib98)]。一些方法扩展了这些模态，增加了诸如交互手-物体边界框 (BB) [[5](#bib.bib5)]
    和音频 (Au) [[105](#bib.bib105)] 等附加特征。多模态结合对于动作预测非常有效。不同模态的融合起着重要作用，例如 AFFT [[105](#bib.bib105)]，在验证集上使用完全相同的特征
    ($14.0\rightarrow 16.4$) 显著超越了早期的先进技术 RULSTM [[98](#bib.bib98)]。在主干网络方面，基于 Transformer
    的模型如 ViT、MViT 和 Swin 一直优于 TSN 或 TSM [[72](#bib.bib72)] 等模型，尤其是在使用附加模态和应用端到端训练时。对于初始化数据集，像
    ImageNet-21K [[199](#bib.bib199)] (IN21K)、Kinetics-400 (K400)、Kinetics-700 (K700)
    等较大的数据集通常比使用较小的 ImageNet-1K (IN1K) 数据集初始化的性能更佳。
- en: In the context of the test set, only peer-reviewed results from approaches using
    standard training sets are included. Results originating from the EpicKitchens-Challenge
    that lack peer-reviewed validation are excluded. The challenge submissions often
    use ensembles of various methods or combine the training and validation set for
    training. AFFT with the Swin-B backbone achieves the highest score, while the
    RAFTformer model shows similar performance using only RGB as modality.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集的背景下，仅包括使用标准训练集的经过同行评审的结果。来自EpicKitchens-Challenge的结果如果没有同行评审验证，则被排除。挑战提交通常使用各种方法的集合或将训练集和验证集合并进行训练。使用
    Swin-B 主干网络的 AFFT 达到最高分，而仅使用 RGB 作为模态的 RAFTformer 模型表现相似。
- en: 5.2 Long-term anticipation
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 长期预测
- en: 'TABLE VI: Benchmark of long-term action anticipation on Breakfast [[66](#bib.bib66)]
    and 50Salads [[70](#bib.bib70)] in terms of mean over classes (%). For details
    on the font coding, please refer to Table [IV](#S5.T4 "TABLE IV ‣ 5.1 Short-term
    anticipation ‣ 5 Benchmark Protocols and Results ‣ A Survey on Deep Learning Techniques
    for Action Anticipation").'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：在 Breakfast [[66](#bib.bib66)] 和 50Salads [[70](#bib.bib70)] 上的长期行动预测基准，按照类别均值（%）。有关字体编码的详细信息，请参见表
    [IV](#S5.T4 "TABLE IV ‣ 5.1 短期预测 ‣ 5 基准协议和结果 ‣ 深度学习技术在行动预测中的调查")。
- en: '| Input Type | Backbone | Methods | Year | Breakfast $\beta$ ($\alpha$ = 0.3)
    | 50Salads $\beta$ ($\alpha$ = 0.3) |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 输入类型 | 主干网络 | 方法 | 年份 | Breakfast $\beta$ ($\alpha$ = 0.3) | 50Salads $\beta$
    ($\alpha$ = 0.3) |'
- en: '| 0.1 | 0.2 | 0.3 | 0.5 | 0.1 | 0.2 | 0.3 | 0.5 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 | 0.2 | 0.3 | 0.5 | 0.1 | 0.2 | 0.3 | 0.5 |'
- en: '| GT. label | – | RNN [[46](#bib.bib46)] | 2018 | 61.45 | 50.25 | 44.90 | 41.75
    | 44.19 | 29.51 | 19.96 | 10.38 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| GT. 标签 | – | RNN [[46](#bib.bib46)] | 2018 | 61.45 | 50.25 | 44.90 | 41.75
    | 44.19 | 29.51 | 19.96 | 10.38 |'
- en: '| CNN [[46](#bib.bib46)] | 2018 | 60.32 | 50.14 | 45.18 | 40.51 | 37.36 | 24.78
    | 20.78 | 14.05 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| CNN [[46](#bib.bib46)] | 2018 | 60.32 | 50.14 | 45.18 | 40.51 | 37.36 | 24.78
    | 20.78 | 14.05 |'
- en: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 65.95 | 55.94 | 49.14 | 44.23 |
    46.40 | 34.80 | 25.24 | 13.84 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 65.95 | 55.94 | 49.14 | 44.23 |
    46.40 | 34.80 | 25.24 | 13.84 |'
- en: '| UAAA [[48](#bib.bib48)] (avg.) | 2019 | 51.25 | 42.94 | 38.33 | 33.07 | 33.15
    | 24.65 | 18.84 | 14.34 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| UAAA [[48](#bib.bib48)] (平均) | 2019 | 51.25 | 42.94 | 38.33 | 33.07 | 33.15
    | 24.65 | 18.84 | 14.34 |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 67.40 | 56.10 | 47.40 | 41.50 | 44.80 |
    32.70 | 23.50 | 15.30 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | 67.40 | 56.10 | 47.40 | 41.50 | 44.80 |
    32.70 | 23.50 | 15.30 |'
- en: '| Zhao et al. [[49](#bib.bib49)] (avg.) | 2020 | 74.14 | 71.32 | 65.30 | 52.38
    | 46.13 | 36.37 | 33.10 | 19.45 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al. [[49](#bib.bib49)] (平均) | 2020 | 74.14 | 71.32 | 65.30 | 52.38
    | 46.13 | 36.37 | 33.10 | 19.45 |'
- en: '| Pred. label | Fisher | RNN [[46](#bib.bib46)] | 2018 | 21.64 | 20.02 | 19.73
    | 19.21 | 30.77 | 17.19 | 14.79 | 9.77 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 预测标签 | Fisher | RNN [[46](#bib.bib46)] | 2018 | 21.64 | 20.02 | 19.73 | 19.21
    | 30.77 | 17.19 | 14.79 | 9.77 |'
- en: '| CNN [[46](#bib.bib46)] | 2018 | 22.44 | 20.12 | 19.69 | 18.76 | 29.14 | 20.14
    | 17.46 | 10.86 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| CNN [[46](#bib.bib46)] | 2018 | 22.44 | 20.12 | 19.69 | 18.76 | 29.14 | 20.14
    | 17.46 | 10.86 |'
- en: '| UAAA [[48](#bib.bib48)] (avg.) | 2019 | 19.14 | 17.18 | 17.38 | 14.98 | 28.04
    | 17.95 | 14.77 | 12.06 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| UAAA [[48](#bib.bib48)] (平均) | 2019 | 19.14 | 17.18 | 17.38 | 14.98 | 28.04
    | 17.95 | 14.77 | 12.06 |'
- en: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 22.75 | 20.44 | 19.64 | 19.75 |
    35.12 | 27.05 | 22.05 | 15.59 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 22.75 | 20.44 | 19.64 | 19.75 |
    35.12 | 27.05 | 22.05 | 15.59 |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 23.00 | 20.00 | 19.90 | 18.60 | 32.30 |
    25.50 | 22.70 | 17.10 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | 23.00 | 20.00 | 19.90 | 18.60 | 32.30 |
    25.50 | 22.70 | 17.10 |'
- en: '| Piergiovanni et al. [[124](#bib.bib124)] | 2020 | - | - | - | - | 40.70 |
    40.10 | 26.40 | 19.20 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Piergiovanni et al. [[124](#bib.bib124)] | 2020 | - | - | - | - | 40.70 |
    40.10 | 26.40 | 19.20 |'
- en: '|  | I3D | TempAgg [[5](#bib.bib5)] | 2020 | 39.50 | 34.10 | 31.00 | 27.90
    | - | - | - | - |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | I3D | TempAgg [[5](#bib.bib5)] | 2020 | 39.50 | 34.10 | 31.00 | 27.90
    | - | - | - | - |'
- en: '| Features | Fisher | CNN [[46](#bib.bib46)] | 2018 | 17.72 | 16.87 | 15.48
    | 14.09 | - | - | - | - |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | Fisher | CNN [[46](#bib.bib46)] | 2018 | 17.72 | 16.87 | 15.48 | 14.09
    | - | - | - | - |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 19.50 | 17.00 | 15.60 | 15.10 | 30.60 |
    22.50 | 19.10 | 11.20 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | 19.50 | 17.00 | 15.60 | 15.10 | 30.60 |
    22.50 | 19.10 | 11.20 |'
- en: '|  | I3D | TempAgg [[5](#bib.bib5)] | 2020 | 30.40 | 26.30 | 23.80 | 21.20
    | - | - | - | - |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | I3D | TempAgg [[5](#bib.bib5)] | 2020 | 30.40 | 26.30 | 23.80 | 21.20
    | - | - | - | - |'
- en: '|  | Cycle Cons[[112](#bib.bib112)] | 2020 | 29.66 | 27.37 | 25.58 | 25.20
    | 34.39 | 23.70 | 18.95 | 15.89 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | Cycle Cons [[112](#bib.bib112)] | 2020 | 29.66 | 27.37 | 25.58 | 25.20
    | 34.39 | 23.70 | 18.95 | 15.89 |'
- en: '|  | A-ACT [[115](#bib.bib115)] | 2022 | 30.80 | 28.30 | 26.10 | 25.80 | 35.70
    | 25.30 | 20.10 | 16.30 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | A-ACT [[115](#bib.bib115)] | 2022 | 30.80 | 28.30 | 26.10 | 25.80 | 35.70
    | 25.30 | 20.10 | 16.30 |'
- en: '|  | FUTR [[82](#bib.bib82)] | 2022 | 32.27 | 29.88 | 27.49 | 25.87 | 35.15
    | 24.86 | 24.22 | 15.26 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | FUTR [[82](#bib.bib82)] | 2022 | 32.27 | 29.88 | 27.49 | 25.87 | 35.15
    | 24.86 | 24.22 | 15.26 |'
- en: '|  | Anticipatr [[117](#bib.bib117)] | 2022 | 39.90 | 35.70 | 32.10 | 29.40
    | 42.80 | 42.30 | 28.50 | 23.60 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  | Anticipatr [[117](#bib.bib117)] | 2022 | 39.90 | 35.70 | 32.10 | 29.40
    | 42.80 | 42.30 | 28.50 | 23.60 |'
- en: The current long-term action anticipation approaches are primarily evaluated
    on the Breakfast [[66](#bib.bib66)] and 50Salads [[70](#bib.bib70)] datasets using
    mean over classes accuracy (see Sec. [4.2](#S4.SS2 "4.2 Evluation metrics ‣ 4
    Evaluation Datasets and Metrics ‣ A Survey on Deep Learning Techniques for Action
    Anticipation")), averaged over future timestamps within a defined anticipation
    duration. Actions are typically predicted after observing the first part ($\alpha$)
    of a video, with benchmarks from[[46](#bib.bib46)] setting $\alpha$ at 0.2 or
    0.3\. Predictions then span segments $\beta$ of the entire video, with $\beta=\{0.1,0.2,0.3,0.5\}$.
    Due to space limits, only results with $\alpha=0.3$ are shown in Table [VI](#S5.T6
    "TABLE VI ‣ 5.2 Long-term anticipation ‣ 5 Benchmark Protocols and Results ‣ A
    Survey on Deep Learning Techniques for Action Anticipation"). Complete results
    are in the supplementary material. Notably, the evaluation protocol of Anticipatr
    differs slightly, as they assessed predictions over $\beta$ portions of the remaining
    video segments. We therefore mark their results in Table [VI](#S5.T6 "TABLE VI
    ‣ 5.2 Long-term anticipation ‣ 5 Benchmark Protocols and Results ‣ A Survey on
    Deep Learning Techniques for Action Anticipation") with gray font. Additionally,
    we observe that the results of FUTR [[82](#bib.bib82)] on 50Salads deviate slightly
    from those reported on the official GitHub repository.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的长期动作预测方法主要在Breakfast [[66](#bib.bib66)]和50Salads [[70](#bib.bib70)]数据集上进行评估，使用类别均值准确率（见第[4.2节](#S4.SS2
    "4.2 Evluation metrics ‣ 4 Evaluation Datasets and Metrics ‣ A Survey on Deep
    Learning Techniques for Action Anticipation")），并在定义的预测持续时间内对未来时间戳进行平均。通常，在观察到视频的第一部分（$\alpha$）后预测动作，基准来自[[46](#bib.bib46)]，将$\alpha$设置为0.2或0.3。然后，预测覆盖整个视频的片段$\beta$，其中$\beta=\{0.1,0.2,0.3,0.5\}$。由于篇幅限制，表[VI](#S5.T6
    "TABLE VI ‣ 5.2 Long-term anticipation ‣ 5 Benchmark Protocols and Results ‣ A
    Survey on Deep Learning Techniques for Action Anticipation")中仅显示了$\alpha=0.3$的结果。完整结果见补充材料。值得注意的是，Anticipatr的评估协议略有不同，因为他们评估了剩余视频片段的$\beta$部分的预测。因此，我们在表[VI](#S5.T6
    "TABLE VI ‣ 5.2 Long-term anticipation ‣ 5 Benchmark Protocols and Results ‣ A
    Survey on Deep Learning Techniques for Action Anticipation")中用灰色字体标记了他们的结果。此外，我们还观察到FUTR [[82](#bib.bib82)]在50Salads上的结果与官方GitHub仓库中报告的结果略有偏差。
- en: 'Three primary types of inputs are commonly utilized: ground truth video segmentations,
    frame-level features such as Fisher vectors [[200](#bib.bib200)] or I3D features [[41](#bib.bib41)],
    and video segmentations inferred from these features through segmentation models
    such as [[50](#bib.bib50)]. Using ground truth annotations facilitates a comprehensive
    analysis of the prediction capabilities of anticipation models. However, employing
    raw features or predicted segmentations reveals the real performance, and thus
    introduces more challenges as potential errors are carried forward into future
    predictions.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的三种主要输入类型是：真实标注的视频分割、帧级特征，如Fisher向量[[200](#bib.bib200)]或I3D特征[[41](#bib.bib41)]，以及通过[[50](#bib.bib50)]等分割模型从这些特征中推断出的分割。使用真实标注有助于全面分析预期模型的预测能力。然而，使用原始特征或预测的分割可以揭示实际表现，因此引入了更多挑战，因为潜在的错误会延续到未来的预测中。
- en: Findings from the Breakfast dataset reveal that methods using ground truth action
    observations as input deliver superior results compared to those utilizing features
    or predicted video segmentations. This suggests that accurately recognizing past
    actions is crucial for future action anticipation. Furthermore, models using I3D
    features exhibit significantly better outcomes than those leveraging Fisher vectors,
    indicating the beneficial role of strong backbones. Different from Breakfast,
    on the 50Salads dataset, the drop in accuracy of models using segmentation predictions
    compared to those using perfect observations is surprisingly small. This can be
    partially attributed to the strong performance of the segmentation decoder [[50](#bib.bib50)]
    and the pronounced inter-class dependencies of the 50Salads dataset, which facilitates
    the learning of coherent action sequences [[46](#bib.bib46)].
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Breakfast 数据集的发现表明，使用真实动作观察作为输入的方法相比于利用特征或预测视频分割的方法，结果更为优越。这表明准确识别过去的动作对于未来动作预测至关重要。此外，使用
    I3D 特征的模型显示出比利用 Fisher 向量的模型显著更好的结果，这表明强大的骨干网络的有益作用。与 Breakfast 数据集不同，在 50Salads
    数据集中，使用分割预测的模型的准确率下降与使用完美观察的模型相比出奇地小。这部分归因于分割解码器的强大性能[[50](#bib.bib50)]和 50Salads
    数据集中的类间依赖关系明显，这有助于学习连贯的动作序列[[46](#bib.bib46)]。
- en: Moreover, earlier approaches that are based on the predicted video segmentations
    considerably outperform the direct prediction from features. For instance, Temporal
    Agg. [[5](#bib.bib5)] gains increases of up to +7% on Breakfast using I3D features
    and +6% on 50Salads using Fisher vectors with $\alpha=0.3$ and $\beta=0.5$. Direct
    future prediction from features poses a challenge as it requires the model to
    recognize observed actions and extract relevant information for future anticipation
    simultaneously. Conversely, approaches based on predicted video segmentations
    decouple these tasks, employing a powerful decoding model to recognize observed
    actions and confining the future predictor to capture context over action classes
    rather than frame-wise features. However, separating the understanding of the
    past and the anticipation of future also leads to several disadvantages. First,
    the temporal action segmentation model is not optimized for the anticipation task.
    Second, any mistakes in the temporal action segmentation will be propagated and
    affect the anticipation results. Finally, since ground truth action labels are
    one-hotted vectors, they contain less information than the spatiotemporal features [[112](#bib.bib112)].
    When the semantic information contained in the features is effectively utilized,
    approaches that directly make predictions from these features may yield better
    performance.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，基于预测视频分割的早期方法显著优于直接从特征预测。例如，Temporal Agg. [[5](#bib.bib5)] 在使用 I3D 特征的 Breakfast
    数据集上提升了多达 +7%，在使用 Fisher 向量的 50Salads 数据集上提升了 +6%，其中 $\alpha=0.3$ 和 $\beta=0.5$。从特征直接进行未来预测面临挑战，因为这要求模型同时识别观察到的动作并提取相关信息以进行未来预测。相反，基于预测视频分割的方法将这些任务解耦，采用强大的解码模型识别观察到的动作，并将未来预测器限制在捕捉动作类别的上下文，而非逐帧特征。然而，将过去的理解与未来的预测分离也带来了若干缺点。首先，时间动作分割模型并未针对预测任务进行优化。其次，时间动作分割中的任何错误都会被传播并影响预测结果。最后，由于真实动作标签是独热编码向量，它们包含的信息少于时空特征[[112](#bib.bib112)]。当特征中包含的语义信息得到有效利用时，直接从这些特征中进行预测的方法可能会取得更好的表现。
- en: 6 Research Challenges and Future Directions
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 研究挑战与未来方向
- en: Despite the abundance of current action anticipation approaches and the substantial
    advancements that have been achieved in this field, there is still potential for
    enhancing state-of-the-art algorithms. In the following discussion, we address
    the research challenges and suggest several promising directions for future research.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管当前动作预测方法种类繁多，并且在这一领域取得了显著进展，但仍有提升最先进算法的潜力。在接下来的讨论中，我们将探讨研究挑战，并提出一些未来研究的有前景方向。
- en: 'Expanding dataset coverage: Current anticipation methods are mainly trained
    and evaluated on datasets with a very restricted scene context and perspective,
    such as Breakfast [[66](#bib.bib66)] and EpicKitchens [[68](#bib.bib68)]. The
    limitations of these datasets restrict the ability of these methods to generalize
    to other scenarios. To make the anticipation methods applicable in our everyday
    life, the development of more comprehensive datasets is crucial. The optimal datasets
    should cover a wide range of daily life scenarios and include diverse actions
    across different cultures, geographies, and social contexts. While Ego4D [[47](#bib.bib47)]
    is a very good example for diversity, it is limited to egocentric views. Studying
    anticipation across different tasks and geographic locations is a promising research
    directions, which has recently been studied in the context of egocentric action
    recognition [[201](#bib.bib201)]. Additionally, more complex scenarios involving
    interactions with various objects or activities with long-term dependencies should
    be included. Since actions do not occur in isolation but are influenced by the
    environment in which they take place, the context, such as location, time, or
    objects present, should also be provided along with the publication of the dataset,
    and be exploited by the anticipation methods. However, labeling a dataset can
    be a labor-intensive process, especially for action anticipation datasets that
    contain natural, untrimmed, long videos. In response to this challenge, unsupervised
    approaches, not necessarily restricted to short-term anticipation, may present
    a promising direction. Furthermore, synthetic datasets, like those generated from
    games [[202](#bib.bib202)], could be of interest.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展数据集覆盖范围：目前的预测方法主要在场景上下文和视角非常有限的数据集上进行训练和评估，如 Breakfast [[66](#bib.bib66)]
    和 EpicKitchens [[68](#bib.bib68)]。这些数据集的局限性限制了这些方法在其他场景中的泛化能力。为了使预测方法在日常生活中适用，开发更全面的数据集至关重要。理想的数据集应覆盖广泛的日常生活场景，并包含跨文化、地域和社会背景的多样化行为。虽然
    Ego4D [[47](#bib.bib47)] 是一个多样性的很好的例子，但它仅限于自我中心的视角。研究不同任务和地理位置下的预测是一个有前景的研究方向，最近在自我中心动作识别的背景下进行了研究 [[201](#bib.bib201)]。此外，还应包括涉及与各种物体交互或具有长期依赖的复杂场景。由于动作不是孤立发生的，而是受其发生环境的影响，因此在数据集发布时应提供上下文信息，如位置、时间或存在的物体，并由预测方法加以利用。然而，为数据集标注可能是一个劳动密集的过程，特别是对于包含自然、未剪辑长视频的动作预测数据集。针对这一挑战，无监督方法（不一定局限于短期预测）可能是一个有前途的方向。此外，像从游戏中生成的合成数据集 [[202](#bib.bib202)]
    也可能引起兴趣。
- en: 'Exploiting language models: Many actions follow a specific order (see Fig. [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Learning Techniques for Action Anticipation")).
    If we identify the preceding actions, we could predict the subsequent action without
    needing to predict the next frame feature and classify it as an action. Furthermore,
    we often adhere to daily routines. For instance, we might wake up at 7 o’clock,
    head to the washroom, and then prepare and eat breakfast. Learning such routines
    or action patterns is akin to our predictive abilities based on the experiences
    we have gathered since childhood. In this context, transferring the knowledge
    harnessed by large language models (LLMs) like ChatGPT [[177](#bib.bib177)] to
    action anticipation is a fascinating avenue to explore, as LLMs are trained on
    extensive text corpora which encapsulate such data, and demonstrate remarkable
    zero-shot capability in effectively tackling multiple NLP or vision-centric tasks [[203](#bib.bib203),
    [119](#bib.bib119)].'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 利用语言模型：许多动作遵循特定的顺序（见图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep
    Learning Techniques for Action Anticipation")）。如果我们能识别前面的动作，就可以在不需要预测下一帧特征并将其分类为动作的情况下预测后续动作。此外，我们通常遵循日常例行活动。例如，我们可能在7点钟醒来，前往洗手间，然后准备和吃早餐。学习这些例行活动或动作模式类似于我们基于从小到大的经验进行预测的能力。在这种背景下，将大型语言模型（LLMs）如
    ChatGPT [[177](#bib.bib177)] 的知识迁移到动作预测中是一个引人入胜的方向，因为 LLMs 在广泛的文本语料库上进行训练，这些数据包含了这样的信息，并展示了在有效处理多个
    NLP 或视觉任务上的显著零样本能力 [[203](#bib.bib203), [119](#bib.bib119)]。
- en: 'Personalization: Current approaches are subject-agnostic, treating all individuals
    as having uniform preferences and behaviors. However, it is evident that each
    individual possesses unique tendencies, preferences, and patterns of behavior.
    Adapting anticipation models to individuals is another promising research direction.
    In particular, if the personal context, e.g., on vacation, is known.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化：当前的方法是主体无关的，将所有个体视为具有统一的偏好和行为。然而，很明显，每个人都有独特的倾向、偏好和行为模式。将预测模型适应于个体是另一个有前景的研究方向。特别是如果知道个人背景，例如在度假时。
- en: 'Addressing uncertainty with probabilistic generative models: Generative models,
    including GANs [[143](#bib.bib143)], VAEs [[184](#bib.bib184)], Normalizing Flows [[190](#bib.bib190)],
    and Diffusion Models [[204](#bib.bib204), [205](#bib.bib205)], offer a promising
    avenue for addressing uncertainty in action anticipation. These models possess
    the capability to generate diverse potential future actions given a current context,
    which can help to provide a better understanding of the range of possible actions.
    Methods for quantifying and incorporating this uncertainty into the decision-making
    process could be explored in the future.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 通过概率生成模型解决不确定性：生成模型，包括GANs[[143](#bib.bib143)], VAEs[[184](#bib.bib184)], 正则化流[[190](#bib.bib190)]和扩散模型[[204](#bib.bib204),
    [205](#bib.bib205)]，提供了一个有前途的途径来解决动作预测中的不确定性。这些模型具有在当前上下文中生成多样潜在未来动作的能力，这可以帮助更好地理解可能的动作范围。未来可以探索量化和将这种不确定性纳入决策过程的方法。
- en: 'Real-time action anticipation: Although vision-based forecasting systems are
    typically designed for real-time deployment on autonomous entities such as self-driving
    cars and robots, they are predominantly evaluated in offline settings where inference
    latency is overlooked. To facilitate real-world applications, anticipation methods
    should be optimized and assessed within an online setting where latency plays
    a critical role [[206](#bib.bib206), [207](#bib.bib207), [73](#bib.bib73)].'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 实时动作预测：虽然基于视觉的预测系统通常设计用于自动驾驶汽车和机器人等自主实体的实时部署，但它们主要在离线环境中评估，其中推理延迟被忽视。为了促进实际应用，预测方法应在在线环境中优化和评估，其中延迟发挥着关键作用[[206](#bib.bib206),
    [207](#bib.bib207), [73](#bib.bib73)]。
- en: 'Multi-person action anticipation: Existing methods mainly focus on single-person
    scenarios, but many real-world situations involve multiple people. Multi-person
    action forecasting in a video sequence has therefore surged as an intriguing topic.
    To address this, object and person detection can be incorporated, following the
    works of [[109](#bib.bib109), [114](#bib.bib114)].'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 多人动作预测：现有方法主要集中在单人场景上，但许多现实世界情况涉及多个人。因此，多人动作预测在视频序列中成为了一个引人注目的话题。为此，可以结合物体和人物检测，参考[[109](#bib.bib109),
    [114](#bib.bib114)]的工作。
- en: 7 Conclusion
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'In this survey, we provide a systematic overview of over 50 action anticipation
    methods pertinent to daily-living scenarios. We examined these methods from three
    perspectives: the specific research question each method addresses, a description
    of each method, and its contributions over previous work. We introduced a taxonomy
    to organize the different methods based on their main contributions. Moreover,
    we have provided a comparative summary of these methods in a tabular format to
    allow readers to identify low-level details at a glance. We have also introduced
    commonly used datasets and metrics, and presented results on several standard
    benchmarks. Lastly, we have offered insights in the form of future research directions.
    In conclusion, action anticipation is a captivating and relatively recent research
    topic that is garnering increasing attention within the community and holds value
    for numerous intelligent decision-making systems. While considerable progress
    has been made, there remains a vast scope for improvement in action anticipation
    using deep learning techniques.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们系统地概述了50多种与日常生活场景相关的动作预测方法。我们从三个角度审视了这些方法：每种方法所解决的具体研究问题、每种方法的描述以及它们相对于之前工作的贡献。我们引入了一个分类法来组织不同的方法，基于它们的主要贡献。此外，我们还以表格格式提供了这些方法的比较总结，方便读者一目了然地识别低级细节。我们还介绍了常用的数据集和指标，并在几个标准基准上展示了结果。最后，我们以未来研究方向的形式提供了见解。总之，动作预测是一个引人入胜且相对较新的研究主题，在社区中受到越来越多的关注，并对众多智能决策系统具有价值。尽管已经取得了相当大的进展，但在使用深度学习技术进行动作预测方面仍有广阔的改进空间。
- en: Acknowledgments
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by the JuBot project which was made possible by funding
    from the Carl-Zeiss-Foundation. Juergen Gall has been supported by the Deutsche
    Forschungsgemeinschaft (DFG, German Research Foundation) GA 1927/4-2 (FOR 2535
    Anticipating Human Behavior) and the ERC Consolidator Grant FORHUE (101044724).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了 JuBot 项目的资助，感谢 Carl-Zeiss-Foundation 的资助。Juergen Gall 得到了德国研究基金会（DFG）的支持，GA
    1927/4-2（FOR 2535 预测人类行为）以及 ERC Consolidator Grant FORHUE（101044724）的资助。
- en: References
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] C. Feichtenhofer, H. Fan, J. Malik, and K. He, “Slowfast networks for video
    recognition,” in *ICCV*, 2019, pp. 6202–6211.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] C. Feichtenhofer, H. Fan, J. Malik 和 K. He，“用于视频识别的 SlowFast 网络”，发表于 *ICCV*，2019年，页码6202–6211。'
- en: '[2] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, “Video swin
    transformer,” in *CVPR*, 2022, pp. 3202–3211.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin 和 H. Hu，“视频 Swin Transformer”，发表于
    *CVPR*，2022年，页码3202–3211。'
- en: '[3] P. Philipp, *Über die Formalisierung und Analyse medizinischer Prozesse
    im Kontext von Expertenwissen und künstlicher Intelligenz*.   KIT Scientific Publishing,
    2023.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] P. Philipp，*关于医疗过程在专家知识和人工智能背景下的形式化和分析*。 KIT Scientific Publishing，2023年。'
- en: '[4] C. Vondrick, H. Pirsiavash, and A. Torralba, “Anticipating visual representations
    from unlabeled video,” in *CVPR*, 2016, pp. 98–106.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] C. Vondrick, H. Pirsiavash 和 A. Torralba， “从未标记视频中预测视觉表示”，发表于 *CVPR*，2016年，页码98–106。'
- en: '[5] F. Sener, D. Singhania, and A. Yao, “Temporal Aggregate Representations
    for Long-Range Video Understanding,” in *ECCV*, 2020.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] F. Sener, D. Singhania 和 A. Yao，“用于长距离视频理解的时间聚合表示”，发表于 *ECCV*，2020年。'
- en: '[6] C.-Y. Wu, Y. Li, K. Mangalam, H. Fan, B. Xiong, J. Malik, and C. Feichtenhofer,
    “MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term
    Video Recognition,” in *CVPR*, 2022.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] C.-Y. Wu, Y. Li, K. Mangalam, H. Fan, B. Xiong, J. Malik 和 C. Feichtenhofer，“MeMViT：用于高效长期视频识别的记忆增强多尺度视觉
    Transformer”，发表于 *CVPR*，2022年。'
- en: '[7] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price *et al.*, “Scaling egocentric vision:
    The epic-kitchens dataset,” in *ECCV*, 2018, pp. 720–736.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price *等*，“扩展自我中心视觉：Epic-Kitchens 数据集”，发表于
    *ECCV*，2018年，页码720–736。'
- en: '[8] G. Ding, F. Sener, and A. Yao, “Temporal action segmentation: An analysis
    of modern technique,” *arXiv preprint arXiv:2210.10352*, 2022.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] G. Ding, F. Sener 和 A. Yao，“时间动作分割：现代技术的分析”，*arXiv 预印本 arXiv:2210.10352*，2022年。'
- en: '[9] E. Vahdani and Y. Tian, “Deep learning-based action detection in untrimmed
    videos: A survey,” *TPAMI*, 2022.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] E. Vahdani 和 Y. Tian，“基于深度学习的未剪裁视频动作检测：综述”，*TPAMI*，2022年。'
- en: '[10] S. Oprea, P. Martinez-Gonzalez, A. Garcia-Garcia, J. A. Castro-Vargas,
    S. Orts-Escolano, J. Garcia-Rodriguez, and A. Argyros, “A Review on Deep Learning
    Techniques for Video Prediction,” *TPAMI*, no. 6, 2020.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S. Oprea, P. Martinez-Gonzalez, A. Garcia-Garcia, J. A. Castro-Vargas,
    S. Orts-Escolano, J. Garcia-Rodriguez 和 A. Argyros，“视频预测的深度学习技术综述”，*TPAMI*，第6期，2020年。'
- en: '[11] A. Rudenko, L. Palmieri, M. Herman, K. M. Kitani, D. M. Gavrila, and K. O.
    Arras, “Human motion trajectory prediction: A survey,” *The International Journal
    of Robotics Research*, vol. 39, no. 8, pp. 895–935, 2020.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Rudenko, L. Palmieri, M. Herman, K. M. Kitani, D. M. Gavrila 和 K. O.
    Arras，“人类运动轨迹预测：综述”，*The International Journal of Robotics Research*，第39卷，第8期，页码895–935，2020年。'
- en: '[12] K. Lyu, H. Chen, Z. Liu, B. Zhang, and R. Wang, “3d human motion prediction:
    A survey,” *Neurocomputing*, vol. 489, pp. 345–365, 2022.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] K. Lyu, H. Chen, Z. Liu, B. Zhang 和 R. Wang，“3D 人体运动预测：综述”，*Neurocomputing*，第489卷，页码345–365，2022年。'
- en: '[13] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo, “Convolutional
    lstm network: A machine learning approach for precipitation nowcasting,” in *NeurIPS*,
    vol. 28, 2015.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong 和 W.-c. Woo，“卷积 LSTM
    网络：一种用于降水现在预报的机器学习方法”，发表于 *NeurIPS*，第28卷，2015年。'
- en: '[14] J. K. MacKie-Mason, A. V. Osepayshvili, D. M. Reeves, and M. P. Wellman,
    “Price prediction strategies for market-based scheduling,” in *International Conference
    on Automated Planning and Scheduling*, 2004.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. K. MacKie-Mason, A. V. Osepayshvili, D. M. Reeves 和 M. P. Wellman，“市场调度的价格预测策略”，发表于
    *国际自动化规划与调度会议*，2004年。'
- en: '[15] T. Petković, D. Puljiz, I. Marković, and B. Hein, “Human Intention Estimation
    based on Hidden Markov Model Motion Validation for Safe Flexible Robotized Warehouses,”
    *Robotics and Computer-Integrated Manufacturing*, 2019.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] T. Petković, D. Puljiz, I. Marković 和 B. Hein，“基于隐马尔可夫模型运动验证的人类意图估计，用于安全灵活的机器人化仓库”，*Robotics
    and Computer-Integrated Manufacturing*，2019年。'
- en: '[16] H. S. Koppula and A. Saxena, “Anticipating Human Activities Using Object
    Affordances for Reactive Robotic Response,” *TPAMI*, no. 1, 2016.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] H. S. Koppula 和 A. Saxena，“使用对象功能预测人类活动以便机器人响应，”发表于*TPAMI*，第1期，2016年。'
- en: '[17] A. Jain, H. S. Koppula, B. Raghavan, S. Soh, and A. Saxena, “Car that
    Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models,”
    in *ICCV*, 2015.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Jain, H. S. Koppula, B. Raghavan, S. Soh, 和 A. Saxena，“在你之前知道的汽车：通过学习时间驾驶模型预测机动，”发表于*ICCV*，2015年。'
- en: '[18] A. Rasouli, I. Kotseruba, and J. K. Tsotsos, “Pedestrian Action Anticipation
    using Contextual Feature Fusion in Stacked RNNs,” in *BMVC*, 2019.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Rasouli, I. Kotseruba, 和 J. K. Tsotsos，“使用上下文特征融合在堆叠RNN中进行行人动作预测，”发表于*BMVC*，2019年。'
- en: '[19] E. Alati, L. Mauro, V. Ntouskos, and F. Pirri, “Help by predicting what
    to do,” in *ICIP*, 2019.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] E. Alati, L. Mauro, V. Ntouskos, 和 F. Pirri，“通过预测要做的事情提供帮助，”发表于*ICIP*，2019年。'
- en: '[20] K. Ito, Q. Kong, S. Horiguchi, T. Sumiyoshi, and K. Nagamatsu, “Anticipating
    the Start of User Interaction for Service Robot in the Wild,” in *ICRA*, 2020.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] K. Ito, Q. Kong, S. Horiguchi, T. Sumiyoshi, 和 K. Nagamatsu，“预测用户互动开始时间以服务机器人在实际环境中，”发表于*ICRA*，2020年。'
- en: '[21] P. Schydlo, M. Rakovic, L. Jamone, and J. Santos-Victor, “Anticipation
    in human-robot cooperation: A recurrent neural network approach for multiple action
    sequences prediction,” in *ICRA*, 2018, pp. 5909–5914.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] P. Schydlo, M. Rakovic, L. Jamone, 和 J. Santos-Victor，“人机合作中的预测：一种用于多个动作序列预测的递归神经网络方法，”发表于*ICRA*，2018年，第5909–5914页。'
- en: '[22] C.-M. Huang, S. Andrist, A. Sauppé, and B. Mutlu, “Using gaze patterns
    to predict task intent in collaboration,” *Frontiers in psychology*, vol. 6, p.
    1049, 2015.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] C.-M. Huang, S. Andrist, A. Sauppé, 和 B. Mutlu，“利用注视模式预测协作中的任务意图，”发表于*Frontiers
    in psychology*，第6卷，第1049页，2015年。'
- en: '[23] B. Soran, A. Farhadi, and L. Shapiro, “Generating notifications for missing
    actions: Don’t forget to turn the lights off!” in *ICCV*, 2015, pp. 4669–4677.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] B. Soran, A. Farhadi, 和 L. Shapiro，“生成缺失动作的通知：别忘了关灯！”发表于*ICCV*，2015年，第4669–4677页。'
- en: '[24] K.-H. Zeng, S.-H. Chou, F.-H. Chan, J. C. Niebles, and M. Sun, “Agent-Centric
    Risk Assessment: Accident Anticipation and Risky Region Localization,” in *CVPR*,
    2017.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] K.-H. Zeng, S.-H. Chou, F.-H. Chan, J. C. Niebles, 和 M. Sun，“以代理为中心的风险评估：事故预测和危险区域定位，”发表于*CVPR*，2017年。'
- en: '[25] T. Suzuki, H. Kataoka, Y. Aoki, and Y. Satoh, “Anticipating traffic accidents
    with adaptive loss and large-scale incident db,” in *CVPR*, 2018, pp. 3521–3529.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] T. Suzuki, H. Kataoka, Y. Aoki, 和 Y. Satoh，“通过自适应损失和大规模事件数据库预测交通事故，”发表于*CVPR*，2018年，第3521–3529页。'
- en: '[26] N. P. Trong, H. Nguyen, K. Kazunori, and B. Le Hoai, “A comprehensive
    survey on human activity prediction,” in *ICCSA*, 2017, pp. 411–425.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] N. P. Trong, H. Nguyen, K. Kazunori, 和 B. Le Hoai，“人类活动预测的综合调查，”发表于*ICCSA*，2017年，第411–425页。'
- en: '[27] A. Rasouli, “Deep learning for vision-based prediction: A survey,” *arXiv
    preprint arXiv:2007.00095*, 2020.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Rasouli，“基于视觉的深度学习预测：综述，”发表于*arXiv preprint arXiv:2007.00095*，2020年。'
- en: '[28] Y. Kong and Y. Fu, “Human action recognition and prediction: A survey,”
    *IJCV*, vol. 130, no. 5, pp. 1366–1401, 2022.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Kong 和 Y. Fu，“人类动作识别与预测：综述，”发表于*IJCV*，第130卷，第5期，第1366–1401页，2022年。'
- en: '[29] I. Rodin, A. Furnari, D. Mavroeidis, and G. M. Farinella, “Predicting
    the future from first person (egocentric) vision: A survey,” *Computer Vision
    and Image Understanding*, vol. 211, p. 103252, 2021.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] I. Rodin, A. Furnari, D. Mavroeidis, 和 G. M. Farinella，“从第一人称（自我中心）视觉预测未来：综述，”发表于*Computer
    Vision and Image Understanding*，第211卷，第103252页，2021年。'
- en: '[30] X. Hu, J. Dai, M. Li, C. Peng, Y. Li, and S. Du, “Online human action
    detection and anticipation in videos: A survey,” *Neurocomputing*, vol. 491, pp.
    395–413, 2022.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] X. Hu, J. Dai, M. Li, C. Peng, Y. Li, 和 S. Du，“视频中的在线人类动作检测与预测：综述，”发表于*Neurocomputing*，第491卷，第395–413页，2022年。'
- en: '[31] C. Plizzari, G. Goletto, A. Furnari, S. Bansal, F. Ragusa, G. M. Farinella,
    D. Damen, and T. Tommasi, “An outlook into the future of egocentric vision,” *arXiv
    preprint arXiv:2308.07123*, 2023.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] C. Plizzari, G. Goletto, A. Furnari, S. Bansal, F. Ragusa, G. M. Farinella,
    D. Damen, 和 T. Tommasi，“对自我中心视觉未来的展望，”发表于*arXiv preprint arXiv:2308.07123*，2023年。'
- en: '[32] K. Li, J. Hu, and Y. Fu, “Modeling Complex Temporal Composition of Actionlets
    for Activity Prediction,” in *ECCV*, vol. 7572, 2012, pp. 286–299.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] K. Li, J. Hu, 和 Y. Fu，“动作片段复杂时间组合的建模用于活动预测，”发表于*ECCV*，第7572卷，2012年，第286–299页。'
- en: '[33] K. Li and Y. Fu, “Prediction of Human Activity by Discovering Temporal
    Sequence Patterns,” *TPAMI*, vol. 36, no. 8, pp. 1644–1657, 2014.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] K. Li 和 Y. Fu，“通过发现时间序列模式预测人类活动，”发表于*TPAMI*，第36卷，第8期，第1644–1657页，2014年。'
- en: '[34] N. Rhinehart and K. M. Kitani, “First-person activity forecasting with
    online inverse reinforcement learning,” in *ICCV*, 2017, pp. 3696–3705.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] N. Rhinehart 和 K. M. Kitani，“使用在线逆强化学习进行第一人称活动预测，”发表于*ICCV*，2017年，页码3696–3705。'
- en: '[35] T. Mahmud, M. Hasan, A. Chakraborty, and A. K. Roy-Chowdhury, “A poisson
    process model for activity forecasting,” in *ICIP*, 2016, pp. 3339–3343.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] T. Mahmud, M. Hasan, A. Chakraborty 和 A. K. Roy-Chowdhury，“用于活动预测的泊松过程模型，”发表于*ICIP*，2016年，页码3339–3343。'
- en: '[36] S. Qi, S. Huang, P. Wei, and S.-C. Zhu, “Predicting Human Activities Using
    Stochastic Grammar,” in *ICCV*, 2017.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Qi, S. Huang, P. Wei 和 S.-C. Zhu，“使用随机语法预测人类活动，”发表于*ICCV*，2017年。'
- en: '[37] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *ICLR*, 2015.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] K. Simonyan 和 A. Zisserman，“用于大规模图像识别的非常深卷积网络，”发表于*ICLR*，2015年。'
- en: '[38] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] K. He, X. Zhang, S. Ren 和 J. Sun，“深度残差学习用于图像识别，”发表于*CVPR*，2016年，页码770–778。'
- en: '[39] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool,
    “Temporal Segment Networks: Towards Good Practices for Deep Action Recognition,”
    in *ECCV*, 2016.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang 和 L. Van Gool，“时间段网络：深入深度动作识别的良好实践，”发表于*ECCV*，2016年。'
- en: '[40] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,
    “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,”
    in *ICLR*, 2021.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit 和 N. Houlsby，“一张图片值16x16个词：大规模图像识别的Transformer，”发表于*ICLR*，2021年。'
- en: '[41] J. Carreira and A. Zisserman, “Quo Vadis, Action Recognition? A New Model
    and the Kinetics Dataset,” in *CVPR*, 2017.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Carreira 和 A. Zisserman，“行动识别，何去何从？新模型与Kinetics数据集，”发表于*CVPR*，2017年。'
- en: '[42] K. Simonyan and A. Zisserman, “Two-stream convolutional networks for action
    recognition in videos,” in *NeurIPS*, vol. 27, 2014.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] K. Simonyan 和 A. Zisserman，“用于视频中的动作识别的双流卷积网络，”发表于*NeurIPS*，第27卷，2014年。'
- en: '[43] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri, “A closer
    look at spatiotemporal convolutions for action recognition,” in *CVPR*, 2018,
    pp. 6450–6459.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun 和 M. Paluri，“对时空卷积进行更详细的审视用于动作识别，”发表于*CVPR*，2018年，页码6450–6459。'
- en: '[44] Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and C. Feichtenhofer,
    “Mvitv2: Improved multiscale vision transformers for classification and detection,”
    in *CVPR*, 2022, pp. 4804–4814.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik 和 C. Feichtenhofer，“Mvitv2：改进的多尺度视觉变换器用于分类和检测，”发表于*CVPR*，2022年，页码4804–4814。'
- en: '[45] J. Gao, Z. Yang, and R. Nevatia, “RED: Reinforced Encoder-Decoder Networks
    for Action Anticipation,” in *BMVC*, 2017.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] J. Gao, Z. Yang 和 R. Nevatia，“RED：用于动作预测的强化编码器-解码器网络，”发表于*BMVC*，2017年。'
- en: '[46] Y. A. Farha, A. Richard, and J. Gall, “When will you do what? - Anticipating
    Temporal Occurrences of Activities,” in *CVPR*, 2018.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Y. A. Farha, A. Richard 和 J. Gall，“你什么时候做什么？- 预测活动的时间发生，”发表于*CVPR*，2018年。'
- en: '[47] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar *et al.*,
    “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” in *CVPR*, 2022.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar *等*，“Ego4D：3,000小时自视角视频中的全球旅行，”发表于*CVPR*，2022年。'
- en: '[48] Y. Abu Farha and J. Gall, “Uncertainty-Aware Anticipation of Activities,”
    in *ICCV Workshop*, 2019.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. Abu Farha 和 J. Gall，“不确定性感知活动预测，”发表于*ICCV Workshop*，2019年。'
- en: '[49] H. Zhao and R. P. Wildes, “On diverse asynchronous activity anticipation,”
    in *ECCV*, 2020, pp. 781–799.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] H. Zhao 和 R. P. Wildes，“多样化异步活动预测，”发表于*ECCV*，2020年，页码781–799。'
- en: '[50] A. Richard, H. Kuehne, and J. Gall, “Weakly supervised action learning
    with rnn based fine-to-coarse modeling,” in *CVPR*, 2017, pp. 754–763.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] A. Richard, H. Kuehne 和 J. Gall，“基于RNN的细到粗建模的弱监督动作学习，”发表于*CVPR*，2017年，页码754–763。'
- en: '[51] Y. A. Farha and J. Gall, “Ms-tcn: Multi-stage temporal convolutional network
    for action segmentation,” in *CVPR*, 2019, pp. 3575–3584.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. A. Farha 和 J. Gall，“Ms-tcn：用于动作分割的多阶段时间卷积网络，”发表于*CVPR*，2019年，页码3575–3584。'
- en: '[52] H. Zhang, F. Chen, and A. Yao, “Weakly-supervised dense action anticipation,”
    in *BMVC*, 2021.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] H. Zhang, F. Chen 和 A. Yao，“弱监督密集动作预测，”发表于*BMVC*，2021年。'
- en: '[53] N. Mehrasa, A. A. Jyothi, T. Durand, J. He, L. Sigal, and G. Mori, “A
    Variational Auto-Encoder Model for Stochastic Point Processes,” in *CVPR*, 2019.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] N. Mehrasa, A. A. Jyothi, T. Durand, J. He, L. Sigal 和 G. Mori，“用于随机点过程的变分自编码器模型，”发表于*CVPR*，2019年。'
- en: '[54] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” in *NeurIPS*, vol. 28, 2015.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] S. Ren, K. He, R. Girshick, 和 J. Sun，“Faster R-CNN：向实时目标检测迈进，使用区域提议网络”，发表于
    *NeurIPS*，第28卷，2015年。'
- en: '[55] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *ICCV*,
    2017, pp. 2961–2969.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] K. He, G. Gkioxari, P. Dollár, 和 R. Girshick，“Mask R-CNN”，发表于 *ICCV*，2017年，第2961–2969页。'
- en: '[56] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NeurIPS*, 2012.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton，“使用深度卷积神经网络进行 ImageNet 分类”，发表于
    *NeurIPS*，2012年。'
- en: '[57] A. Patron-Perez, M. Marszalek, A. Zisserman, and I. Reid, “High five:
    Recognising human interactions in tv shows.” in *BMVC*, vol. 1, no. 2, 2010, p. 33.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] A. Patron-Perez, M. Marszalek, A. Zisserman, 和 I. Reid，“High five：识别电视节目中的人际互动”，发表于
    *BMVC*，第1卷，第2期，2010年，第33页。'
- en: '[58] H. Pirsiavash and D. Ramanan, “Detecting activities of daily living in
    first-person camera views,” in *CVPR*, 2012, pp. 2847–2854.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] H. Pirsiavash 和 D. Ramanan，“检测第一人称视角中的日常活动”，发表于 *CVPR*，2012年，第2847–2854页。'
- en: '[59] Y.-G. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev, M. Shah, and
    R. Sukthankar, “Thumos challenge: Action recognition with a large number of classes,”
    2014.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Y.-G. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev, M. Shah, 和 R.
    Sukthankar，“Thumos挑战：大量类别的动作识别”，2014年。'
- en: '[60] K.-H. Zeng, W. B. Shen, D.-A. Huang, M. Sun, and J. Carlos Niebles, “Visual
    forecasting by imitating dynamics in natural sequences,” in *ICCV*, 2017, pp.
    2999–3008.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] K.-H. Zeng, W. B. Shen, D.-A. Huang, M. Sun, 和 J. Carlos Niebles，“通过模仿自然序列中的动态进行视觉预测”，发表于
    *ICCV*，2017年，第2999–3008页。'
- en: '[61] Y. Xiong, L. Wang, Z. Wang, B. Zhang, H. Song, W. Li, D. Lin, Y. Qiao,
    L. Van Gool, and X. Tang, “Cuhk & ethz & siat submission to activitynet challenge
    2016,” *arXiv preprint arXiv:1608.00797*, 2016.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Y. Xiong, L. Wang, Z. Wang, B. Zhang, H. Song, W. Li, D. Lin, Y. Qiao,
    L. Van Gool, 和 X. Tang，“CUHK & ETHZ & SIAT 提交至 ActivityNet 挑战赛 2016”，*arXiv 预印本
    arXiv:1608.00797*，2016年。'
- en: '[62] R. D. Geest, E. Gavves, A. Ghodrati, Z. Li, C. Snoek, and T. Tuytelaars,
    “Online action detection,” in *ECCV*, 2016, pp. 269–284.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] R. D. Geest, E. Gavves, A. Ghodrati, Z. Li, C. Snoek, 和 T. Tuytelaars，“在线动作检测”，发表于
    *ECCV*，2016年，第269–284页。'
- en: '[63] Y. Zhong and W.-S. Zheng, “Unsupervised learning for forecasting action
    representations,” in *ICIP*, 2018, pp. 1073–1077.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Y. Zhong 和 W.-S. Zheng，“用于预测动作表征的无监督学习”，发表于 *ICIP*，2018年，第1073–1077页。'
- en: '[64] V. Tran, Y. Wang, Z. Zhang, and M. Hoai, “Knowledge distillation for human
    action anticipation,” in *ICIP*, 2021, pp. 2518–2522.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] V. Tran, Y. Wang, Z. Zhang, 和 M. Hoai，“用于人类动作预测的知识蒸馏”，发表于 *ICIP*，2021年，第2518–2522页。'
- en: '[65] B. Fernando and S. Herath, “Anticipating human actions by correlating
    past with the future with Jaccard similarity measures,” in *CVPR*, 2021.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] B. Fernando 和 S. Herath，“通过将过去与未来相关联来预测人类动作，使用 Jaccard 相似度量”，发表于 *CVPR*，2021年。'
- en: '[66] H. Kuehne, A. Arslan, and T. Serre, “The Language of Actions: Recovering
    the Syntax and Semantics of Goal-Directed Human Activities,” in *CVPR*, 2014.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] H. Kuehne, A. Arslan, 和 T. Serre，“动作的语言：恢复以目标为导向的人类活动的句法和语义”，发表于 *CVPR*，2014年。'
- en: '[67] R. Girdhar and K. Grauman, “Anticipative Video Transformer,” in *ICCV*,
    2021.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] R. Girdhar 和 K. Grauman，“预见性视频变换器”，发表于 *ICCV*，2021年。'
- en: '[68] D. Damen, H. Doughty, G. M. Farinella, A. Furnari, E. Kazakos, J. Ma,
    D. Moltisanti, J. Munro, T. Perrett, W. Price *et al.*, “Rescaling egocentric
    vision: Collection, pipeline and challenges for epic-kitchens-100,” *IJCV*, pp.
    1–23, 2022.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] D. Damen, H. Doughty, G. M. Farinella, A. Furnari, E. Kazakos, J. Ma,
    D. Moltisanti, J. Munro, T. Perrett, W. Price *等*，“重新标定自我中心视觉：Epic-Kitchens-100的收集、管道和挑战”，*IJCV*，第1–23页，2022年。'
- en: '[69] Y. Li, M. Liu, and J. M. Rehg, “In the eye of beholder: Joint learning
    of gaze and actions in first person video,” in *ECCV*, 2018, pp. 619–635.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Y. Li, M. Liu, 和 J. M. Rehg，“在旁观者眼中：第一人称视频中注视和动作的联合学习”，发表于 *ECCV*，2018年，第619–635页。'
- en: '[70] S. Stein and S. J. McKenna, “Combining embedded accelerometers with computer
    vision for recognizing food preparation activities,” in *UbiComp*, 2013, pp. 729–738.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] S. Stein 和 S. J. McKenna，“将嵌入式加速度计与计算机视觉相结合以识别食品准备活动”，发表于 *UbiComp*，2013年，第729–738页。'
- en: '[71] X. Xu, Y.-L. Li, and C. Lu, “Learning To Anticipate Future With Dynamic
    Context Removal,” in *CVPR*, 2022.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] X. Xu, Y.-L. Li, 和 C. Lu，“学习通过动态上下文去除来预测未来”，发表于 *CVPR*，2022年。'
- en: '[72] J. Lin, C. Gan, and S. Han, “Tsm: Temporal shift module for efficient
    video understanding,” in *ICCV*, 2019, pp. 7083–7093.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] J. Lin, C. Gan, 和 S. Han，“TSM：高效视频理解的时间移动模块”，发表于 *ICCV*，2019年，第7083–7093页。'
- en: '[73] H. Girase, N. Agarwal, C. Choi, and K. Mangalam, “Latency matters: Real-time
    action forecasting transformer,” in *CVPR*, 2023, pp. 18 759–18 769.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] H. Girase, N. Agarwal, C. Choi, 和 K. Mangalam，“延迟很重要：实时动作预测变换器”，发表于 *CVPR*，2023年，第18,759–18,769页。'
- en: '[74] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “Forecasting future
    action sequences with neural memory networks,” in *BMVC*, 2019.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] H. Gammulle, S. Denman, S. Sridharan 和 C. Fookes，“使用神经记忆网络预测未来动作序列，” 发表在*BMVC*，2019年。'
- en: '[75] M. Xu, M. Gao, Y.-T. Chen, L. S. Davis, and D. J. Crandall, “Temporal
    recurrent networks for online action detection,” in *ICCV*, 2019, pp. 5532–5541.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] M. Xu, M. Gao, Y.-T. Chen, L. S. Davis 和 D. J. Crandall，“用于在线动作检测的时间递归网络，”
    发表在*ICCV*，2019年，第5532–5541页。'
- en: '[76] W. Wang, X. Peng, Y. Su, Y. Qiao, and J. Cheng, “TTPP: Temporal Transformer
    with Progressive Prediction for Efficient Action Anticipation,” *Neurocomputing*,
    2020.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] W. Wang, X. Peng, Y. Su, Y. Qiao 和 J. Cheng，“TTPP：具有渐进预测的时间变换器用于高效的动作预测，”
    *Neurocomputing*，2020年。'
- en: '[77] S. Qu, G. Chen, D. Xu, J. Dong, F. Lu, and A. Knoll, “Lap-net: Adaptive
    features sampling via learning action progression for online action detection,”
    *arXiv preprint arXiv:2011.07915*, 2020.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] S. Qu, G. Chen, D. Xu, J. Dong, F. Lu 和 A. Knoll，“Lap-net：通过学习动作进程进行自适应特征采样的在线动作检测，”
    *arXiv preprint arXiv:2011.07915*，2020年。'
- en: '[78] Y. Wu, L. Zhu, X. Wang, Y. Yang, and F. Wu, “Learning to Anticipate Egocentric
    Actions by Imagination,” *TIP*, 2021.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Y. Wu, L. Zhu, X. Wang, Y. Yang 和 F. Wu，“通过想象学习预测自我中心动作，” *TIP*，2021年。'
- en: '[79] M. Xu, Y. Xiong, H. Chen, X. Li, W. Xia, Z. Tu, and S. Soatto, “Long short-term
    transformer for online action detection,” in *NeurIPS*, vol. 34, 2021, pp. 1086–1099.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] M. Xu, Y. Xiong, H. Chen, X. Li, W. Xia, Z. Tu 和 S. Soatto，“用于在线动作检测的长短期变换器，”
    发表在*NeurIPS*，第34卷，2021年，第1086–1099页。'
- en: '[80] X. Wang, S. Zhang, Z. Qing, Y. Shao, Z. Zuo, C. Gao, and N. Sang, “Oadtr:
    Online action detection with transformers,” in *ICCV*, 2021.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] X. Wang, S. Zhang, Z. Qing, Y. Shao, Z. Zuo, C. Gao 和 N. Sang，“Oadtr：使用变换器进行在线动作检测，”
    发表在*ICCV*，2021年。'
- en: '[81] T. Liu and K.-M. Lam, “A Hybrid Egocentric Activity Anticipation Framework
    via Memory-Augmented Recurrent and One-shot Representation Forecasting,” in *CVPR*,
    2022.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] T. Liu 和 K.-M. Lam，“通过记忆增强递归和一次性表示预测的混合自我中心活动预测框架，” 发表在*CVPR*，2022年。'
- en: '[82] D. Gong, J. Lee, M. Kim, S. J. Ha, and M. Cho, “Future Transformer for
    Long-term Action Anticipation,” in *CVPR*, 2022.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] D. Gong, J. Lee, M. Kim, S. J. Ha 和 M. Cho，“用于长期动作预测的未来变换器，” 发表在*CVPR*，2022年。'
- en: '[83] Y. Zhao and P. Krähenbühl, “Real-time online video detection with temporal
    smoothing transformers,” in *ECCV*, 2022, pp. 485–502.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. Zhao 和 P. Krähenbühl，“使用时间平滑变换器的实时在线视频检测，” 发表在*ECCV*，2022年，第485–502页。'
- en: '[84] Y. Zhou and T. L. Berg, “Temporal perception and prediction in ego-centric
    video,” in *ICCV*, 2015, pp. 4498–4506.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Y. Zhou 和 T. L. Berg，“自我中心视频中的时间感知与预测，” 发表在*ICCV*，2015年，第4498–4506页。'
- en: '[85] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101 human actions
    classes from videos in the wild,” *arXiv preprint arXiv:1212.0402*, 2012.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] K. Soomro, A. R. Zamir 和 M. Shah，“UCF101：一个包含101个人类动作类别的野外视频数据集，” *arXiv
    preprint arXiv:1212.0402*，2012年。'
- en: '[86] T. Mahmud, M. Hasan, and A. K. Roy-Chowdhury, “Joint prediction of activity
    labels and starting times in untrimmed videos,” in *ICCV*, 2017, pp. 5773–5782.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] T. Mahmud, M. Hasan 和 A. K. Roy-Chowdhury，“在未裁剪视频中联合预测活动标签和起始时间，” 发表在*ICCV*，2017年，第5773–5782页。'
- en: '[87] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
    spatiotemporal features with 3d convolutional networks,” in *ICCV*, 2015, pp.
    4489–4497.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] D. Tran, L. Bourdev, R. Fergus, L. Torresani 和 M. Paluri，“使用3D卷积网络学习时空特征，”
    发表在*ICCV*，2015年，第4489–4497页。'
- en: '[88] S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T. Lee, S. Mukherjee,
    J. Aggarwal, H. Lee, L. Davis *et al.*, “A large-scale benchmark dataset for event
    recognition in surveillance video,” in *CVPR*, 2011, pp. 3153–3160.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T. Lee, S. Mukherjee,
    J. Aggarwal, H. Lee 和 L. Davis *等*，“用于监控视频事件识别的大规模基准数据集，” 发表在*CVPR*，2011年，第3153–3160页。'
- en: '[89] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele, “A database for fine
    grained activity detection of cooking activities,” in *CVPR*, 2012, pp. 1194–1201.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] M. Rohrbach, S. Amin, M. Andriluka 和 B. Schiele，“一个用于精细化活动检测的烹饪活动数据库，”
    发表在*CVPR*，2012年，第1194–1201页。'
- en: '[90] Y. Shen, B. Ni, Z. Li, and N. Zhuang, “Egocentric activity prediction
    via event modulated attention,” in *ECCV*, 2018, pp. 197–212.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Y. Shen, B. Ni, Z. Li 和 N. Zhuang，“通过事件调制注意力进行自我中心活动预测，” 发表在*ECCV*，2018年，第197–212页。'
- en: '[91] X. Zhu, X. Jia, and K.-Y. K. Wong, “Pixel-level hand detection with shape-aware
    structured forests,” in *ACCV*, 2015, pp. 64–78.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] X. Zhu, X. Jia 和 K.-Y. K. Wong， “像素级手部检测与形状感知结构森林，” 发表在*ACCV*，2015年，第64–78页。'
- en: '[92] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single shot multibox detector,” in *ECCV*, 2016, pp. 21–37.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu 和 A. C. Berg，“SSD：单次多框检测器，”
    发表在*ECCV*，2016年，第21–37页。'
- en: '[93] A. Fathi, X. Ren, and J. M. Rehg, “Learning to recognize objects in egocentric
    activities,” in *CVPR*, 2011, pp. 3281–3288.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] A. Fathi, X. Ren, 和 J. M. Rehg，“学习在自我中心活动中识别物体，”发表于 *CVPR*，2011年，第3281–3288页。'
- en: '[94] A. Fathi, Y. Li, and J. M. Rehg, “Learning to recognize daily actions
    using gaze,” in *ECCV*, 2012, pp. 314–327.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] A. Fathi, Y. Li, 和 J. M. Rehg，“通过注视学习识别日常动作，”发表于 *ECCV*，2012年，第314–327页。'
- en: '[95] J. Liang, L. Jiang, J. C. Niebles, A. G. Hauptmann, and L. Fei-Fei, “Peeking
    Into the Future: Predicting Future Person Activities and Locations in Videos,”
    in *CVPR*, 2019.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] J. Liang, L. Jiang, J. C. Niebles, A. G. Hauptmann, 和 L. Fei-Fei，“窥探未来：预测视频中未来的人物活动和位置，”发表于
    *CVPR*，2019年。'
- en: '[96] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-decoder
    with atrous separable convolution for semantic image segmentation,” in *ECCV*,
    2018, pp. 801–818.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, 和 H. Adam，“带有空洞可分离卷积的编码器-解码器用于语义图像分割，”发表于
    *ECCV*，2018年，第801–818页。'
- en: '[97] G. Awad, A. A. Butt, K. Curtis, Y. Lee, J. Fiscus, A. Godil, D. Joy, A. Delgado,
    A. F. Smeaton, Y. Graham *et al.*, “Trecvid 2018: Benchmarking video activity
    detection, video captioning and matching, video storytelling linking and video
    search,” in *TRECVID*, 2018.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] G. Awad, A. A. Butt, K. Curtis, Y. Lee, J. Fiscus, A. Godil, D. Joy, A.
    Delgado, A. F. Smeaton, Y. Graham *等*，“Trecvid 2018：视频活动检测、视频标题生成和匹配、视频叙事链接和视频搜索的基准测试，”发表于
    *TRECVID*，2018年。'
- en: '[98] A. Furnari and G. Farinella, “What Would You Expect? Anticipating Egocentric
    Actions With Rolling-Unrolling LSTMs and Modality Attention,” in *ICCV*, 2019.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] A. Furnari 和 G. Farinella，“你会期待什么？通过滚动-展开 LSTM 和模态注意力预测自我中心动作，”发表于 *ICCV*，2019年。'
- en: '[99] M. Liu, S. Tang, Y. Li, and J. M. Rehg, “Forecasting Human-Object Interaction:
    Joint Prediction of Motor Attention and Actions in First Person Video,” in *ECCV*,
    2020.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] M. Liu, S. Tang, Y. Li, 和 J. M. Rehg，“预测人类与物体的交互：在第一人称视频中对运动注意力和动作的联合预测，”发表于
    *ECCV*，2020年。'
- en: '[100] D. Tran, H. Wang, L. Torresani, and M. Feiszli, “Video classification
    with channel-separated convolutional networks,” in *ICCV*, 2019, pp. 5552–5561.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] D. Tran, H. Wang, L. Torresani, 和 M. Feiszli，“使用通道分离卷积网络进行视频分类，”发表于 *ICCV*，2019年，第5552–5561页。'
- en: '[101] E. Dessalene, C. Devaraj, M. Maynord, C. Fermuller, and Y. Aloimonos,
    “Forecasting action through contact representations from first person video,”
    *TPAMI*, 2021.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] E. Dessalene, C. Devaraj, M. Maynord, C. Fermuller, 和 Y. Aloimonos，“通过来自第一人称视频的接触表示预测动作，”
    *TPAMI*，2021年。'
- en: '[102] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *ICLR*, 2017.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] T. N. Kipf 和 M. Welling，“带图卷积网络的半监督分类，”发表于 *ICLR*，2017年。'
- en: '[103] O. Zatsarynna, Y. A. Farha, and J. Gall, “Multi-Modal Temporal Convolutional
    Network for Anticipating Actions in Egocentric Videos,” in *CVPR Workshop*, 2021.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] O. Zatsarynna, Y. A. Farha, 和 J. Gall，“用于预测自我中心视频中的动作的多模态时间卷积网络，”发表于
    *CVPR Workshop*，2021年。'
- en: '[104] D. Roy and B. Fernando, “Action Anticipation Using Pairwise Human-Object
    Interactions and Transformers,” *TIP*, 2021.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] D. Roy 和 B. Fernando，“使用配对的人类-物体交互和变换器进行动作预测，” *TIP*，2021年。'
- en: '[105] Z. Zhong, D. Schneider, M. Voit, R. Stiefelhagen, and J. Beyerer, “Anticipative
    feature fusion transformer for multi-modal action anticipation,” in *WACV*, 2023.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Z. Zhong, D. Schneider, M. Voit, R. Stiefelhagen, 和 J. Beyerer，“用于多模态动作预测的预期特征融合变换器，”发表于
    *WACV*，2023年。'
- en: '[106] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
    “Swin transformer: Hierarchical vision transformer using shifted windows,” in
    *CVPR*, 2021, pp. 10 012–10 022.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, 和 B. Guo，“Swin
    变换器：使用移位窗口的层次化视觉变换器，”发表于 *CVPR*，2021年，第10 012–10 022页。'
- en: '[107] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena, “Structural-rnn: Deep
    learning on spatio-temporal graphs,” in *CVPR*, 2016, pp. 5308–5317.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] A. Jain, A. R. Zamir, S. Savarese, 和 A. Saxena，“Structural-rnn: 深度学习在时空图上的应用，”发表于
    *CVPR*，2016年，第5308–5317页。'
- en: '[108] H. S. Koppula, R. Gupta, and A. Saxena, “Learning human activities and
    object affordances from rgb-d videos,” *IJRR*, vol. 32, no. 8, pp. 951–970, 2013.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] H. S. Koppula, R. Gupta, 和 A. Saxena，“从 RGB-D 视频中学习人类活动和物体可用性，” *IJRR*，第32卷，第8期，第951–970页，2013年。'
- en: '[109] C. Sun, A. Shrivastava, C. Vondrick, R. Sukthankar, K. Murphy, and C. Schmid,
    “Relational Action Forecasting,” in *CVPR*, 2019.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C. Sun, A. Shrivastava, C. Vondrick, R. Sukthankar, K. Murphy, 和 C. Schmid，“关系动作预测，”发表于
    *CVPR*，2019年。'
- en: '[110] S. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy, “Rethinking spatiotemporal
    feature learning for video understanding,” in *ECCV*, 2018.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] S. Xie, C. Sun, J. Huang, Z. Tu, 和 K. Murphy，“重新思考视频理解中的时空特征学习，”发表于 *ECCV*，2018年。'
- en: '[111] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan,
    G. Toderici, S. Ricco, R. Sukthankar *et al.*, “Ava: A video dataset of spatio-temporally
    localized atomic visual actions,” in *CVPR*, 2018, pp. 6047–6056.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan,
    G. Toderici, S. Ricco 和 R. Sukthankar *等*，“Ava：一个时空局部化原子视觉动作的视频数据集”，在 *CVPR*，2018年，页6047–6056。'
- en: '[112] Y. Abu Farha, Q. Ke, B. Schiele, and J. Gall, “Long-term anticipation
    of activities with cycle consistency,” in *GCPR*, 2020, pp. 159–173.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Y. Abu Farha, Q. Ke, B. Schiele 和 J. Gall，“通过循环一致性进行长期活动预测”，在 *GCPR*，2020年，页159–173。'
- en: '[113] G. Camporese, P. Coscia, A. Furnari, G. M. Farinella, and L. Ballan,
    “Knowledge distillation for action anticipation via label smoothing,” in *ICPR*,
    2020, pp. 3312–3319.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] G. Camporese, P. Coscia, A. Furnari, G. M. Farinella 和 L. Ballan，“通过标签平滑进行动作预测的知识蒸馏”，在
    *ICPR*，2020年，页3312–3319。'
- en: '[114] Y. Li, P. Wang, and C.-Y. Chan, “Restep into the future: relational spatio-temporal
    learning for multi-person action forecasting,” *IEEE Transactions on Multimedia*,
    2021.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Y. Li, P. Wang 和 C.-Y. Chan，“迈向未来：用于多人的关系时空学习预测”，*IEEE Transactions on
    Multimedia*，2021。'
- en: '[115] A. Gupta, J. Liu, L. Bo, A. K. Roy-Chowdhury, and T. Mei, “A-act: Action
    anticipation through cycle transformations,” *arXiv preprint arXiv:2204.00942*,
    2022.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] A. Gupta, J. Liu, L. Bo, A. K. Roy-Chowdhury 和 T. Mei，“A-act: 通过循环变换进行动作预测”，*arXiv
    preprint arXiv:2204.00942*，2022。'
- en: '[116] D. Roy and B. Fernando, “Predicting the next action by modeling the abstract
    goal,” *arXiv preprint arXiv:2209.05044*, 2022.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] D. Roy 和 B. Fernando，“通过建模抽象目标预测下一个动作”，*arXiv preprint arXiv:2209.05044*，2022。'
- en: '[117] M. Nawhal, A. A. Jyothi, and G. Mori, “Rethinking learning approaches
    for long-term action anticipation,” in *ECCV*, 2022, pp. 558–576.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] M. Nawhal, A. A. Jyothi 和 G. Mori，“重新思考长期动作预测的学习方法”，在 *ECCV*，2022年，页558–576。'
- en: '[118] E. V. Mascaró, H. Ahn, and D. Lee, “Intention-conditioned long-term human
    egocentric action anticipation,” in *WACV*, 2023, pp. 6048–6057.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] E. V. Mascaró, H. Ahn 和 D. Lee，“基于意图的长期人类自我中心动作预测”，在 *WACV*，2023年，页6048–6057。'
- en: '[119] Q. Zhao, C. Zhang, S. Wang, C. Fu, N. Agarwal, K. Lee, and C. Sun, “Antgpt:
    Can large language models help long-term action anticipation from videos?” *arXiv
    preprint arXiv:2307.16368*, 2023.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Q. Zhao, C. Zhang, S. Wang, C. Fu, N. Agarwal, K. Lee 和 C. Sun，“Antgpt：大型语言模型能否帮助从视频中进行长期动作预测？”
    *arXiv preprint arXiv:2307.16368*，2023。'
- en: '[120] J. Sung, C. Ponce, B. Selman, and A. Saxena, “Unstructured human activity
    detection from rgbd images,” in *ICRA*, 2012, pp. 842–849.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] J. Sung, C. Ponce, B. Selman 和 A. Saxena，“从RGBD图像中检测无结构人类活动”，在 *ICRA*，2012年，页842–849。'
- en: '[121] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori, and L. Fei-Fei,
    “Every moment counts: Dense detailed labeling of actions in complex videos,” *IJCV*,
    vol. 126, no. 2, pp. 375–389, 2018.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori 和 L. Fei-Fei，“每一刻都很重要：复杂视频中动作的密集详细标注”，*IJCV*，第126卷，第2期，页375–389，2018。'
- en: '[122] Y. B. Ng and B. Fernando, “Forecasting future action sequences with attention:
    a new approach to weakly supervised action forecasting,” *TIP*, vol. 29, pp. 8880–8891,
    2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Y. B. Ng 和 B. Fernando，“通过注意力预测未来动作序列：一种新的弱监督动作预测方法”，*TIP*，第29卷，页8880–8891，2020。'
- en: '[123] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and A. Gupta,
    “Hollywood in homes: Crowdsourcing data collection for activity understanding,”
    in *ECCV*, 2016, pp. 510–526.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev 和 A. Gupta，“好莱坞在家：众包数据收集用于活动理解”，在
    *ECCV*，2016年，页510–526。'
- en: '[124] A. Piergiovanni, A. Angelova, A. Toshev, and M. S. Ryoo, “Adversarial
    generative grammars for human activity prediction,” in *ECCV*, 2020, pp. 507–523.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] A. Piergiovanni, A. Angelova, A. Toshev 和 M. S. Ryoo，“用于人类活动预测的对抗生成语法”，在
    *ECCV*，2020年，页507–523。'
- en: '[125] A. Miech, I. Laptev, J. Sivic, H. Wang, L. Torresani, and D. Tran, “Leveraging
    the Present to Anticipate the Future in Videos,” in *CVPR Workshop*, 2019.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] A. Miech, I. Laptev, J. Sivic, H. Wang, L. Torresani 和 D. Tran，“利用现在来预测视频中的未来”，在
    *CVPR Workshop*，2019。'
- en: '[126] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Carlos Niebles, “Activitynet:
    A large-scale video benchmark for human activity understanding,” in *CVPR*, 2015,
    pp. 961–970.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] F. Caba Heilbron, V. Escorcia, B. Ghanem 和 J. Carlos Niebles，“Activitynet：用于人类活动理解的大规模视频基准”，在
    *CVPR*，2015年，页961–970。'
- en: '[127] Q. Ke, M. Fritz, and B. Schiele, “Time-Conditioned Action Anticipation
    in One Shot,” in *CVPR*, Jun. 2019.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Q. Ke, M. Fritz 和 B. Schiele，“一次性时间条件下的动作预测”，在 *CVPR*，2019年6月。'
- en: '[128] T. Zhang, W. Min, Y. Zhu, Y. Rui, and S. Jiang, “An egocentric action
    anticipation framework via fusing intuition and analysis,” in *ACMMM*, 2020, pp.
    402–410.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] T. Zhang, W. Min, Y. Zhu, Y. Rui, 和 S. Jiang，“通过融合直觉与分析的自我中心动作预期框架，”
    见 *ACMMM*，2020年，第402–410页。'
- en: '[129] V. Gupta and S. Bedathur, “Proactive: Self-attentive temporal point process
    flows for activity sequences,” in *KDD*, 2022, pp. 496–504.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] V. Gupta 和 S. Bedathur，“Proactive: 用于活动序列的自注意力时间点过程流，” 见 *KDD*，2022年，第496–504页。'
- en: '[130] C. Rodriguez, B. Fernando, and H. Li, “Action anticipation by predicting
    future dynamic images,” in *ECCV Workshop*, 2018.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] C. Rodriguez, B. Fernando, 和 H. Li，“通过预测未来动态图像进行动作预期，” 见 *ECCV Workshop*，2018年。'
- en: '[131] I. Teeti, R. S. Bhargav, V. Singh, A. Bradley, B. Banerjee, and F. Cuzzolin,
    “Temporal dino: A self-supervised video strategy to enhance action prediction,”
    *arXiv preprint arXiv:2308.04589*, 2023.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] I. Teeti, R. S. Bhargav, V. Singh, A. Bradley, B. Banerjee, 和 F. Cuzzolin，“Temporal
    dino: 一种自监督视频策略以增强动作预测，” *arXiv 预印本 arXiv:2308.04589*，2023年。'
- en: '[132] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with contrastive
    predictive coding,” *arXiv preprint arXiv:1807.03748*, 2018.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] A. v. d. Oord, Y. Li, 和 O. Vinyals，“通过对比预测编码进行表示学习，” *arXiv 预印本 arXiv:1807.03748*，2018年。'
- en: '[133] T. Han, W. Xie, and A. Zisserman, “Video representation learning by dense
    predictive coding,” in *ICCV Workshop*, 2019, pp. 0–0.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] T. Han, W. Xie, 和 A. Zisserman，“通过密集预测编码进行视频表示学习，” 见 *ICCV Workshop*，2019年，第0–0页。'
- en: '[134] ——, “Memory-augmented dense predictive coding for video representation
    learning,” in *ECCV*, 2020, pp. 312–329.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] ——，“用于视频表示学习的记忆增强密集预测编码，” 见 *ECCV*，2020年，第312–329页。'
- en: '[135] D. Surís, R. Liu, and C. Vondrick, “Learning the predictability of the
    future,” in *CVPR*, 2021, pp. 12 607–12 617.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] D. Surís, R. Liu, 和 C. Vondrick，“学习未来的可预测性，” 见 *CVPR*，2021年，第12 607–12 617页。'
- en: '[136] O. Zatsarynna, Y. A. Farha, and J. Gall, “Self-supervised learning for
    unintentional action prediction,” in *GCPR*, 2022, pp. 429–444.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] O. Zatsarynna, Y. A. Farha, 和 J. Gall，“用于无意动作预测的自监督学习，” 见 *GCPR*，2022年，第429–444页。'
- en: '[137] R. Tan, M. De Lange, M. Iuzzolino, B. A. Plummer, K. Saenko, K. Ridgeway,
    and L. Torresani, “Multiscale video pretraining for long-term activity forecasting,”
    *arXiv preprint arXiv:2307.12854*, 2023.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] R. Tan, M. De Lange, M. Iuzzolino, B. A. Plummer, K. Saenko, K. Ridgeway,
    和 L. Torresani，“用于长期活动预测的多尺度视频预训练，” *arXiv 预印本 arXiv:2307.12854*，2023年。'
- en: '[138] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] S. Hochreiter 和 J. Schmidhuber，“长短期记忆，” *神经计算*，第9卷，第8期，第1735–1780页，1997年。'
- en: '[139] M. Nickel and D. Kiela, “Poincaré embeddings for learning hierarchical
    representations,” *NeurIPS*, vol. 30, 2017.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] M. Nickel 和 D. Kiela，“用于学习层次表示的庞加莱嵌入，” *NeurIPS*，第30卷，2017年。'
- en: '[140] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is All you Need,” in *NeurIPS*, 2017.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“Attention is All you Need，” 见 *NeurIPS*，2017年。'
- en: '[141] M. Hayat, S. Khan, S. W. Zamir, J. Shen, and L. Shao, “Gaussian affinity
    for max-margin class imbalanced learning,” in *ICCV*, 2019, pp. 6469–6479.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] M. Hayat, S. Khan, S. W. Zamir, J. Shen, 和 L. Shao，“用于最大边际类别不平衡学习的高斯亲和力，”
    见 *ICCV*，2019年，第6469–6479页。'
- en: '[142] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *et al.*,
    “Language models are unsupervised multitask learners,” *OpenAI blog*, vol. 1,
    no. 8, p. 9, 2019.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *等*，“语言模型是无监督多任务学习者，”
    *OpenAI 博客*，第1卷，第8期，第9页，2019年。'
- en: '[143] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *NeurIPS*, vol. 27,
    2014.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络，” 见 *NeurIPS*，第27卷，2014年。'
- en: '[144] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “Predicting the
    future: A jointly learnt model for action anticipation,” in *ICCV*, 2019, pp.
    5562–5571.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] H. Gammulle, S. Denman, S. Sridharan, 和 C. Fookes，“预测未来：用于动作预期的联合学习模型，”
    见 *ICCV*，2019年，第5562–5571页。'
- en: '[145] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in *NeurIPS*,
    2016.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] J. Ho 和 S. Ermon，“生成对抗模仿学习，” 见 *NeurIPS*，2016年。'
- en: '[146] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
    gated recurrent neural networks on sequence modeling,” *arXiv preprint arXiv:1412.3555*,
    2014.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] J. Chung, C. Gulcehre, K. Cho, 和 Y. Bengio，“门控递归神经网络在序列建模上的实证评估，” *arXiv
    预印本 arXiv:1412.3555*，2014年。'
- en: '[147] Z. Qi, S. Wang, C. Su, L. Su, Q. Huang, and Q. Tian, “Self-regulated
    learning for egocentric video activity anticipation,” *TPAMI*, 2021.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Z. Qi, S. Wang, C. Su, L. Su, Q. Huang, 和 Q. Tian，“用于自我中心视频活动预测的自我调节学习，”
    *TPAMI*，2021年。'
- en: '[148] A. Bubic, D. Y. Von Cramon, and R. I. Schubotz, “Prediction, cognition
    and the brain,” *Frontiers in human neuroscience*, vol. 4, p. 1094, 2010.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] A. Bubic, D. Y. Von Cramon, 和 R. I. Schubotz，“预测、认知与大脑”， *Frontiers in
    human neuroscience*，第4卷，第1094页，2010年。'
- en: '[149] A. Clark, “Whatever next? predictive brains, situated agents, and the
    future of cognitive science,” *Behavioral and brain sciences*, vol. 36, no. 3,
    pp. 181–204, 2013.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] A. Clark，“接下来会怎样？预测性大脑、情境代理与认知科学的未来”， *Behavioral and brain sciences*，第36卷，第3期，第181–204页，2013年。'
- en: '[150] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with gumbel-softmax,”
    in *ICLR*, 2017.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] E. Jang, S. Gu, 和 B. Poole，“使用Gumbel-Softmax的分类重参数化”，发表于 *ICLR*，2017年。'
- en: '[151] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,”
    in *CVPR*, 2018, pp. 7794–7803.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] X. Wang, R. Girshick, A. Gupta, 和 K. He，“非局部神经网络”，发表于 *CVPR*，2018年，第7794–7803页。'
- en: '[152] C.-Y. Wu, C. Feichtenhofer, H. Fan, K. He, P. Krahenbuhl, and R. Girshick,
    “Long-term feature banks for detailed video understanding,” in *CVPR*, 2019, pp.
    284–293.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] C.-Y. Wu, C. Feichtenhofer, H. Fan, K. He, P. Krahenbuhl, 和 R. Girshick，“用于详细视频理解的长期特征库”，发表于
    *CVPR*，2019年，第284–293页。'
- en: '[153] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos,
    P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser *et al.*, “Rethinking attention
    with performers,” in *ICLR*, 2021.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos,
    P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser *等*，“用执行者重新思考注意力”，发表于 *ICLR*，2021年。'
- en: '[154] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer: Self-attention
    with linear complexity,” *arXiv preprint arXiv:2006.04768*, 2020.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] S. Wang, B. Z. Li, M. Khabsa, H. Fang, 和 H. Ma，“Linformer：具有线性复杂度的自注意力”，
    *arXiv preprint arXiv:2006.04768*，2020年。'
- en: '[155] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap, “Compressive
    transformers for long-range sequence modelling,” in *ICLR*, 2020.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] J. W. Rae, A. Potapenko, S. M. Jayakumar, 和 T. P. Lillicrap，“用于长距离序列建模的压缩Transformer”，发表于
    *ICLR*，2020年。'
- en: '[156] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira,
    “Perceiver: General perception with iterative attention,” in *ICML*, 2021, pp.
    4651–4664.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, 和 J. Carreira，
    “Perceiver: 一种具有迭代注意力的通用感知”，发表于 *ICML*，2021年，第4651–4664页。'
- en: '[157] Y.-H. H. Tsai, S. Bai, M. Yamada, L.-P. Morency, and R. Salakhutdinov,
    “Transformer dissection: An unified understanding for transformer’s attention
    via the lens of kernel,” in *EMNLP*, 2019.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Y.-H. H. Tsai, S. Bai, M. Yamada, L.-P. Morency, 和 R. Salakhutdinov，“Transformer解剖：通过内核视角对Transformer注意力的统一理解”，发表于
    *EMNLP*，2019年。'
- en: '[158] E. Kazakos, A. Nagrani, A. Zisserman, and D. Damen, “EPIC-Fusion: Audio-Visual
    Temporal Binding for Egocentric Action Recognition,” in *ICCV*, 2019.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] E. Kazakos, A. Nagrani, A. Zisserman, 和 D. Damen，“EPIC-Fusion：用于自我中心动作识别的视听时间绑定”，发表于
    *ICCV*，2019年。'
- en: '[159] A. Furnari, S. Battiato, K. Grauman, and G. M. Farinella, “Next-active-object
    prediction from egocentric videos,” *Journal of Visual Communication and Image
    Representation*, pp. 401–411, 2017.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] A. Furnari, S. Battiato, K. Grauman, 和 G. M. Farinella，“来自自我中心视频的下一活动对象预测”，
    *Journal of Visual Communication and Image Representation*，第401–411页，2017年。'
- en: '[160] Z. Sun, Q. Ke, H. Rahmani, M. Bennamoun, G. Wang, and J. Liu, “Human
    action recognition from various data modalities: A review,” *TPAMI*, 2022.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Z. Sun, Q. Ke, H. Rahmani, M. Bennamoun, G. Wang, 和 J. Liu，“从各种数据模态中进行人类动作识别：综述”，
    *TPAMI*，2022年。'
- en: '[161] C. Zach, T. Pock, and H. Bischof, “A duality based approach for realtime
    tv-l 1 optical flow,” in *Joint pattern recognition symposium*, 2007, pp. 214–223.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] C. Zach, T. Pock, 和 H. Bischof，“基于对偶性的实时TV-L 1光流方法”，发表于 *Joint pattern
    recognition symposium*，2007年，第214–223页。'
- en: '[162] S. Qi, W. Wang, B. Jia, J. Shen, and S.-C. Zhu, “Learning Human-Object
    Interactions by Graph Parsing Neural Networks,” in *ECCV*, 2018.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] S. Qi, W. Wang, B. Jia, J. Shen, 和 S.-C. Zhu，“通过图解析神经网络学习人类-物体交互”，发表于
    *ECCV*，2018年。'
- en: '[163] D. Roy, R. Rajendiran, and B. Fernando, “Interaction visual transformer
    for egocentric action anticipation,” *arXiv preprint arXiv:2211.14154*, 2022.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] D. Roy, R. Rajendiran, 和 B. Fernando，“用于自我中心动作预测的交互视觉Transformer”， *arXiv
    preprint arXiv:2211.14154*，2022年。'
- en: '[164] H. S. Park, J.-J. Hwang, Y. Niu, and J. Shi, “Egocentric future localization,”
    in *CVPR*, 2016, pp. 4697–4705.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] H. S. Park, J.-J. Hwang, Y. Niu, 和 J. Shi，“自我中心未来定位”，发表于 *CVPR*，2016年，第4697–4705页。'
- en: '[165] T. Nagarajan, Y. Li, C. Feichtenhofer, and K. Grauman, “Ego-topo: Environment
    affordances from egocentric video,” in *CVPR*, 2020, pp. 163–172.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] T. Nagarajan, Y. Li, C. Feichtenhofer, 和 K. Grauman，“Ego-topo：从自我中心视频中获取环境可用性”，发表于
    *CVPR*，2020年，第163–172页。'
- en: '[166] A. G. Hawkes, “Spectra of some self-exciting and mutually exciting point
    processes,” *Biometrika*, vol. 58, no. 1, pp. 83–90, 1971.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] A. G. Hawkes，“一些自激发和互激发点过程的谱”， *Biometrika*，第58卷，第1期，第83–90页，1971年。'
- en: '[167] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price *et al.*, “The epic-kitchens dataset:
    Collection, challenges and baselines,” *TPAMI*, vol. 43, no. 11, pp. 4125–4141,
    2020.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price *等*，“Epic-Kitchens 数据集：收集、挑战和基线，”*TPAMI*，第
    43 卷，第 11 期，第 4125–4141 页，2020 年。'
- en: '[168] N. Osman, G. Camporese, P. Coscia, and L. Ballan, “Slowfast rolling-unrolling
    lstms for action anticipation in egocentric videos,” in *ICCV Workshop*, 2021,
    pp. 3437–3445.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] N. Osman, G. Camporese, P. Coscia 和 L. Ballan，“用于自我中心视频的慢快滚动展开 LSTM 进行动作预测，”发表于
    *ICCV Workshop*，2021 年，第 3437–3445 页。'
- en: '[169] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager, “Temporal
    convolutional networks for action segmentation and detection,” in *CVPR*, 2017,
    pp. 156–165.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] C. Lea, M. D. Flynn, R. Vidal, A. Reiter 和 G. D. Hager，“用于动作分割和检测的时间卷积网络，”发表于
    *CVPR*，2017 年，第 156–165 页。'
- en: '[170] C. L. Fosco, S. Jin, E. Josephs, and A. Oliva, “Leveraging temporal context
    in low representational power regimes,” in *CVPR*, June 2023, pp. 10 693–10 703.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] C. L. Fosco, S. Jin, E. Josephs 和 A. Oliva，“在低表示能力的环境中利用时间上下文，”发表于 *CVPR*，2023
    年 6 月，第 10 693–10 703 页。'
- en: '[171] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for
    word representation,” in *EMNLP*, 2014, pp. 1532–1543.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] J. Pennington, R. Socher 和 C. D. Manning，“Glove：全球词向量表示，”发表于 *EMNLP*，2014
    年，第 1532–1543 页。'
- en: '[172] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio,
    “Graph attention networks,” in *ICLR*, 2018.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Lio 和 Y. Bengio，“图注意力网络，”发表于
    *ICLR*，2018 年。'
- en: '[173] N. Ballas, L. Yao, C. Pal, and A. Courville, “Delving deeper into convolutional
    networks for learning video representations,” in *ICLR*, 2016.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] N. Ballas, L. Yao, C. Pal 和 A. Courville，“深入探讨卷积网络以学习视频表示，”发表于 *ICLR*，2016
    年。'
- en: '[174] A. W. Kruglanski and E. Szumowska, “Habitual behavior is goal-driven,”
    *Perspectives on Psychological Science*, 2020.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] A. W. Kruglanski 和 E. Szumowska，“习惯行为是目标驱动的，”*心理科学视角*，2020 年。'
- en: '[175] D. Roy and B. Fernando, “Action anticipation using latent goal learning,”
    in *WACV*, 2022, pp. 2745–2753.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] D. Roy 和 B. Fernando，“利用潜在目标学习进行动作预测，”发表于 *WACV*，2022 年，第 2745–2753 页。'
- en: '[176] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner,
    J. Yung, A. Steiner, D. Keysers, J. Uszkoreit *et al.*, “Mlp-mixer: An all-mlp
    architecture for vision,” in *NeurIPS*, vol. 34, 2021, pp. 24 261–24 272.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner,
    J. Yung, A. Steiner, D. Keysers 和 J. Uszkoreit *等*，“Mlp-mixer：一种全 MLP 结构的视觉架构，”发表于
    *NeurIPS*，第 34 卷，2021 年，第 24 261–24 272 页。'
- en: '[177] OpenAI, “Chatgpt: Optimizing language models for dialogue,” 2022.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] OpenAI，“ChatGPT：优化对话的语言模型，”2022 年。'
- en: '[178] O. Zatsarynna and J. Gall, “Action anticipation with goal consistency,”
    in *ICIP*, 2023.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] O. Zatsarynna 和 J. Gall，“具有目标一致性的动作预测，”发表于 *ICIP*，2023 年。'
- en: '[179] B. Dai, S. Fidler, R. Urtasun, and D. Lin, “Towards diverse and natural
    image descriptions via a conditional gan,” in *ICCV*, 2017, pp. 2970–2979.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] B. Dai, S. Fidler, R. Urtasun 和 D. Lin，“通过条件 GAN 实现多样化和自然的图像描述，”发表于 *ICCV*，2017
    年，第 2970–2979 页。'
- en: '[180] M. Mathieu, C. Couprie, and Y. LeCun, “Deep multi-scale video prediction
    beyond mean square error,” in *ICLR*, 2016.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] M. Mathieu, C. Couprie 和 Y. LeCun，“超越均方误差的深度多尺度视频预测，”发表于 *ICLR*，2016
    年。'
- en: '[181] Y. Li, N. Du, and S. Bengio, “Time-dependent representation for neural
    event sequence prediction,” *arXiv preprint arXiv:1708.00065*, 2017.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Y. Li, N. Du 和 S. Bengio，“神经事件序列预测的时间依赖表示，”*arXiv 预印本 arXiv:1708.00065*，2017
    年。'
- en: '[182] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *NeurIPS*, 2014.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] I. Sutskever, O. Vinyals 和 Q. V. Le，“使用神经网络的序列到序列学习，”发表于 *NeurIPS*，2014
    年。'
- en: '[183] D. Yang, S. Hong, Y. Jang, T. Zhao, and H. Lee, “Diversity-sensitive
    conditional generative adversarial networks,” in *ICLR*, 2019.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] D. Yang, S. Hong, Y. Jang, T. Zhao 和 H. Lee，“多样性敏感的条件生成对抗网络，”发表于 *ICLR*，2019
    年。'
- en: '[184] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in *ICLR*,
    2014.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] D. P. Kingma 和 M. Welling，“自动编码变分贝叶斯，”发表于 *ICLR*，2014 年。'
- en: '[185] J. F. C. Kingman, *Poisson processes*.   Clarendon Press, 1992, vol. 3.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] J. F. C. Kingman，*泊松过程*。Clarendon Press，1992 年，第 3 卷。'
- en: '[186] M.-A. Rizoiu, L. Xie, S. Sanner, M. Cebrian, H. Yu, and P. Van Hentenryck,
    “Expecting to be hip: Hawkes intensity processes for social media popularity,”
    in *WWW*, 2017.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] M.-A. Rizoiu, L. Xie, S. Sanner, M. Cebrian, H. Yu 和 P. Van Hentenryck，“期待时尚：用于社交媒体流行度的
    Hawkes 强度过程，”发表于 *WWW*，2017 年。'
- en: '[187] E. Bacry, I. Mastromatteo, and J.-F. Muzy, “Hawkes processes in finance,”
    *Market Microstructure and Liquidity*, 2015.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] E. Bacry, I. Mastromatteo 和 J.-F. Muzy，“金融中的 Hawkes 过程，”*市场微观结构与流动性*，2015
    年。'
- en: '[188] M. Yao, S. Zhao, S. Sahebi, and R. Feyzi Behnagh, “Stimuli-sensitive
    hawkes processes for personalized student procrastination modeling,” in *WWW*,
    2021, pp. 1562–1573.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] M. Yao, S. Zhao, S. Sahebi, 和 R. Feyzi Behnagh, “针对个性化学生拖延建模的刺激敏感霍克斯过程，”发表于
    *WWW*，2021年，第1562–1573页。'
- en: '[189] Q. Zhao, M. A. Erdogdu, H. Y. He, A. Rajaraman, and J. Leskovec, “Seismic:
    A self-exciting point process model for predicting tweet popularity,” in *KDD*,
    2015, pp. 1513–1522.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Q. Zhao, M. A. Erdogdu, H. Y. He, A. Rajaraman, 和 J. Leskovec, “Seismic:
    用于预测推文受欢迎程度的自激点过程模型，”发表于 *KDD*，2015年，第1513–1522页。'
- en: '[190] D. Rezende and S. Mohamed, “Variational inference with normalizing flows,”
    in *ICML*, 2015, pp. 1530–1538.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] D. Rezende 和 S. Mohamed, “使用正则化流的变分推断，”发表于 *ICML*，2015年，第1530–1538页。'
- en: '[191] N. Mehrasa, R. Deng, M. O. Ahmed, B. Chang, J. He, T. Durand, M. Brubaker,
    and G. Mori, “Point process flows,” *arXiv preprint arXiv:1910.08281*, 2019.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] N. Mehrasa, R. Deng, M. O. Ahmed, B. Chang, J. He, T. Durand, M. Brubaker,
    和 G. Mori, “点过程流，” *arXiv预印本 arXiv:1910.08281*，2019年。'
- en: '[192] O. Shchur, M. Biloš, and S. Günnemann, “Intensity-free learning of temporal
    point processes,” in *ICLR*, 2020.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] O. Shchur, M. Biloš, 和 S. Günnemann, “无强度的时间点过程学习，”发表于 *ICLR*，2020年。'
- en: '[193] F. Sener, D. Chatterjee, D. Shelepov, K. He, D. Singhania, R. Wang, and
    A. Yao, “Assembly101: A large-scale multi-view video dataset for understanding
    procedural activities,” in *CVPR*, 2022, pp. 21 096–21 106.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] F. Sener, D. Chatterjee, D. Shelepov, K. He, D. Singhania, R. Wang, 和
    A. Yao, “Assembly101: 一个用于理解程序性活动的大规模多视角视频数据集，”发表于 *CVPR*，2022年，第21 096–21 106页。'
- en: '[194] A. Furnari, S. Battiato, and G. M. Farinella, “Leveraging Uncertainty
    to Rethink Loss Functions and Evaluation Measures for Egocentric Action Anticipation,”
    in *ECCV Workshop*, 2018.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] A. Furnari, S. Battiato, 和 G. M. Farinella, “利用不确定性重新思考损失函数和评估措施用于自我中心动作预期，”发表于
    *ECCV Workshop*，2018年。'
- en: '[195] F. J. Damerau, “A technique for computer detection and correction of
    spelling errors,” *Communications of the ACM*, vol. 7, no. 3, pp. 171–176, 1964.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] F. J. Damerau, “计算机拼写错误检测与修正技术，” *ACM通讯*，第7卷，第3期，第171–176页，1964年。'
- en: '[196] V. I. Levenshtein *et al.*, “Binary codes capable of correcting deletions,
    insertions, and reversals,” in *Soviet physics doklady*, vol. 10, no. 8, 1966,
    pp. 707–710.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] V. I. Levenshtein *等人*，“能够纠正删除、插入和反转的二进制码，”发表于 *苏联物理学报告*，第10卷，第8期，1966年，第707–710页。'
- en: '[197] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” in *ICML*, 2015, pp. 448–456.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] S. Ioffe 和 C. Szegedy, “批量归一化：通过减少内部协变量偏移加速深度网络训练，”发表于 *ICML*，2015年，第448–456页。'
- en: '[198] J. Carreira, E. Noland, C. Hillier, and A. Zisserman, “A short note on
    the kinetics-700 human action dataset,” *arXiv preprint arXiv:1907.06987*, 2019.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] J. Carreira, E. Noland, C. Hillier, 和 A. Zisserman, “关于 Kinetics-700
    人类动作数据集的简短说明，” *arXiv预印本 arXiv:1907.06987*，2019年。'
- en: '[199] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *CVPR*, 2009, pp. 248–255.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, 和 L. Fei-Fei, “Imagenet:
    一个大规模层次化图像数据库，”发表于 *CVPR*，2009年，第248–255页。'
- en: '[200] [https://bitbucket.org/doneata/fv4a/src/master/](https://bitbucket.org/doneata/fv4a/src/master/).'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] [https://bitbucket.org/doneata/fv4a/src/master/](https://bitbucket.org/doneata/fv4a/src/master/)。'
- en: '[201] C. Plizzari, T. Perrett, B. Caputo, and D. Damen, “What can a cook in
    italy teach a mechanic in india? action recognition generalisation over scenarios
    and locations,” in *ICCV*, 2023.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] C. Plizzari, T. Perrett, B. Caputo, 和 D. Damen, “意大利的厨师能教印度的机械师什么？情境和位置上的动作识别泛化，”发表于
    *ICCV*，2023年。'
- en: '[202] A. Roitberg, D. Schneider, A. Djamal, C. Seibold, S. Reiß, and R. Stiefelhagen,
    “Let’s play for action: Recognizing activities of daily living by learning from
    life simulation video games,” in *IROS*, 2021, pp. 8563–8569.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] A. Roitberg, D. Schneider, A. Djamal, C. Seibold, S. Reiß, 和 R. Stiefelhagen,
    “让我们为动作而玩：通过学习生活模拟视频游戏来识别日常活动，”发表于 *IROS*，2021年，第8563–8569页。'
- en: '[203] G. Chen, Y.-D. Zheng, J. Wang, J. Xu, Y. Huang, J. Pan, Y. Wang, Y. Wang,
    Y. Qiao, T. Lu *et al.*, “Videollm: Modeling video sequence with large language
    models,” *arXiv preprint arXiv:2305.13292*, 2023.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] G. Chen, Y.-D. Zheng, J. Wang, J. Xu, Y. Huang, J. Pan, Y. Wang, Y. Wang,
    Y. Qiao, T. Lu *等人*，“Videollm: 使用大规模语言模型建模视频序列，” *arXiv预印本 arXiv:2305.13292*，2023年。'
- en: '[204] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    in *NeurIPS*, vol. 33, 2020, pp. 6840–6851.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] J. Ho, A. Jain, 和 P. Abbeel, “去噪扩散概率模型，”发表于 *NeurIPS*，第33卷，2020年，第6840–6851页。'
- en: '[205] D. Liu, Q. Li, A. Dinh, T. Jiang, M. Shah, and C. Xu, “Diffusion action
    segmentation,” in *ICCV*, 2023.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] D. Liu, Q. Li, A. Dinh, T. Jiang, M. Shah, 和 C. Xu, “扩散动作分割，”发表于 *ICCV*，2023年。'
- en: '[206] M. Li, Y.-X. Wang, and D. Ramanan, “Towards streaming perception,” in
    *ECCV*, 2020, pp. 473–488.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] M. Li, Y.-X. Wang 和 D. Ramanan，“迈向流式感知，”见于*ECCV*，2020年，第473–488页。'
- en: '[207] A. Furnari and G. M. Farinella, “Towards streaming egocentric action
    anticipation,” in *ICPR*, 2022, pp. 1250–1257.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] A. Furnari 和 G. M. Farinella，“迈向流式自我中心动作预判，”见于*ICPR*，2022年，第1250–1257页。
    |'
- en: '| ![[Uncaptioned image]](img/96f59c6a2a2d68aa1bc64965825f1bfa.png) | Zeyun
    Zhong received the B.Eng. degree in process-, energy- and environmental engineering
    from Hannover University of Applied Sciences and Arts, in 2018, and the M.Sc.
    in mechatronics from Leibniz University Hannover, in 2021\. He is currently working
    toward the Ph.D. degree at the Department of Informatics, Karlsruhe Institute
    of Technology. His main research interests include deep learning, video understanding,
    action recognition, and anticipation. |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图像]](img/96f59c6a2a2d68aa1bc64965825f1bfa.png) | Zeyun Zhong 于2018年在汉诺威应用科学与艺术大学获得过程、能源和环境工程的工学学士学位，并于2021年在汉诺威大学获得机电一体化的硕士学位。他目前在卡尔斯鲁厄理工学院信息学系攻读博士学位。他的主要研究兴趣包括深度学习、视频理解、动作识别和预判。
    |'
- en: '| ![[Uncaptioned image]](img/7e8c9902cc6fdc1c7c94a8c2c8c46165.png) | Manuel
    Martin studied computer science at the Karlsruhe Institute of Technology (KIT)
    obtaining his diploma in 2013\. He received his Ph.D. from KIT in 2023 for his
    work on 3D human body pose estimation and activity recognition of drivers in automated
    vehicles. He is now senior scientist in the group Perceptual User Interfaces at
    Fraunhofer IOSB, Karlsruhe, continuing his work on machine learning based occupant
    monitoring systems. |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图像]](img/7e8c9902cc6fdc1c7c94a8c2c8c46165.png) | Manuel Martin 在卡尔斯鲁厄理工学院（KIT）学习计算机科学，并于2013年获得学位。他于2023年在KIT获得博士学位，研究方向为3D人体姿态估计和自动驾驶车辆中驾驶员的活动识别。他现在是弗劳恩霍夫IOSB卡尔斯鲁厄的感知用户界面组的高级科学家，继续从事基于机器学习的乘员监测系统的研究。
    |'
- en: '| ![[Uncaptioned image]](img/96be0b1cd4bb95cb6dea06869f3284c0.png) | Michael
    Voit studied computer science at the former University of Karlsruhe (TH), now
    Karlsruhe Institute of Technology (KIT), obtaining his diploma in 2004\. He received
    his Ph.D. from KIT in 2012 for estimating the visual focus of attention of people
    by monitoring them with multiple overhead cameras distributed in a room. He is
    now head of the research group Perceptual User Interfaces at Fraunhofer IOSB,
    Karlsruhe, continuing his work on camera-based human machine interaction techniques.
    |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图像]](img/96be0b1cd4bb95cb6dea06869f3284c0.png) | Michael Voit 在前卡尔斯鲁厄大学（TH），现在的卡尔斯鲁厄理工学院（KIT）学习计算机科学，并于2004年获得学位。他于2012年在KIT获得博士学位，研究通过监控房间内多个天花板摄像机来估计人们的视觉注意力焦点。他现在是弗劳恩霍夫IOSB卡尔斯鲁厄的感知用户界面研究组组长，继续从事基于摄像机的人机交互技术的研究。
    |'
- en: '| ![[Uncaptioned image]](img/da15fc854403b99bf8d38575f65d72f2.png) | Juergen
    Gall obtained his B.Sc. and his Masters degree in mathematics from the University
    of Wales Swansea (2004) and from the University of Mannheim (2005). In 2009, he
    obtained a Ph.D. in computer science from the Saarland University and the Max
    Planck Institut für Informatik. He was a postdoctoral researcher at the Computer
    Vision Laboratory, ETH Zurich, from 2009 until 2012 and senior research scientist
    at the Max Planck Institute for Intelligent Systems in Tübingen from 2012 until
    2013\. Since 2013, he is professor at the University of Bonn and member of the
    Lamarr Institute for Machine Learning and Artificial Intelligence. |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图像]](img/da15fc854403b99bf8d38575f65d72f2.png) | Juergen Gall 从威尔士大学斯旺西分校（2004年）和曼海姆大学（2005年）获得数学学士和硕士学位。2009年，他在萨尔大学和马克斯·普朗克计算机科学研究所获得计算机科学博士学位。他在2009年至2012年期间是ETH苏黎世计算机视觉实验室的博士后研究员，并在2012年至2013年期间担任图宾根马克斯·普朗克智能系统研究所的高级研究科学家。自2013年起，他担任波恩大学教授，并成为拉玛尔机器学习与人工智能研究所的成员。
    |'
- en: '| ![[Uncaptioned image]](img/8df0840672c864ec74942e018bf6891c.png) | Jürgen
    Beyerer studied electrical engineering at the University of Karlsruhe, obtaining
    his diploma in 1989\. In 1994, he obtained his Ph.D. from the Institute for Measurement
    and Control Systems (MRT) at the same university. Post his doctorate, he established
    a research group on automatic visual inspection and image processing at MRT. From
    1999 to 2004, he led the Hottinger Systems GmbH in Mannheim and served as the
    deputy managing director for Hottinger Maschinenbau GmbH. Since 2004, he is professor
    at the Karlsruhe Institute of Technology and concurrently the head of the Fraunhofer
    Institute of Optronics, System Technologies and Image Exploitation (IOSB). |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图像]](img/8df0840672c864ec74942e018bf6891c.png) | 於尔根·贝耶雷尔（Jürgen Beyerer）在卡尔斯鲁厄大学学习电气工程，并于1989年获得学位。1994年，他从同一大学的测量与控制系统研究所（MRT）获得博士学位。博士毕业后，他在MRT成立了一个自动视觉检查和图像处理的研究小组。从1999年到2004年，他领导了曼海姆的霍廷格系统有限公司（Hottinger
    Systems GmbH），并担任霍廷格机械制造有限公司（Hottinger Maschinenbau GmbH）的副总经理。自2004年以来，他担任卡尔斯鲁厄理工学院的教授，同时担任弗劳恩霍夫光电学、系统技术和图像利用研究所（IOSB）的负责人。
    |'
- en: 8 Supplementary Material
  id: totrans-527
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 补充材料
- en: While Tables [IV](#S5.T4 "TABLE IV ‣ 5.1 Short-term anticipation ‣ 5 Benchmark
    Protocols and Results ‣ A Survey on Deep Learning Techniques for Action Anticipation")-[VI](#S5.T6
    "TABLE VI ‣ 5.2 Long-term anticipation ‣ 5 Benchmark Protocols and Results ‣ A
    Survey on Deep Learning Techniques for Action Anticipation") show the results
    for a subset of the evaluation protocols, we provide the comprehensive results
    in Tables [VII](#S8.T7 "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep
    Learning Techniques for Action Anticipation")-[XI](#S8.T11 "TABLE XI ‣ 8 Supplementary
    Material ‣ A Survey on Deep Learning Techniques for Action Anticipation"). These
    results encompass both short-term and long-term action anticipation, covering
    both third-person and egocentric perspectives.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然表格 [IV](#S5.T4 "TABLE IV ‣ 5.1 短期预测 ‣ 5 基准协议和结果 ‣ 关于行动预测的深度学习技术综述")-[VI](#S5.T6
    "TABLE VI ‣ 5.2 长期预测 ‣ 5 基准协议和结果 ‣ 关于行动预测的深度学习技术综述") 显示了评估协议子集的结果，我们在表格 [VII](#S8.T7
    "TABLE VII ‣ 8 补充材料 ‣ 关于行动预测的深度学习技术综述")-[XI](#S8.T11 "TABLE XI ‣ 8 补充材料 ‣ 关于行动预测的深度学习技术综述")
    中提供了全面的结果。这些结果涵盖了短期和长期的行动预测，包括第三人称和自我中心的视角。
- en: 'TABLE VII: Comparison of the state-of-the-art methods on TVSeries [[62](#bib.bib62)]
    in terms of mean cAP (%). The optimal performance in each column within every
    block is indicated in bold, while the supreme overall performance in each column
    is further denoted by an underscore.'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：在 TVSeries [[62](#bib.bib62)] 上对最先进方法的比较，按平均 cAP (%) 排列。每个区块内每列的最佳性能用粗体标出，而每列的最高整体性能则用下划线进一步标记。
- en: '| Method | Year | Backbone | Pre-train | mcAP@Time predicted into the future
    (seconds) |  |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 骨干网络 | 预训练 | mcAP@时间预测未来（秒） |  |'
- en: '| 0.25 | 0.5 | 0.75 | 1.0 | 1.25 | 1.5 | 1.75 | 2.0 | Avg. |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| 0.25 | 0.5 | 0.75 | 1.0 | 1.25 | 1.5 | 1.75 | 2.0 | 平均值 |'
- en: '| ED [[45](#bib.bib45)] | 2017 | VGG | IN1K | 71.0 | 70.6 | 69.9 | 68.8 | 68.0
    | 67.4 | 67.0 | 66.7 | 68.7 |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| ED [[45](#bib.bib45)] | 2017 | VGG | IN1K | 71.0 | 70.6 | 69.9 | 68.8 | 68.0
    | 67.4 | 67.0 | 66.7 | 68.7 |'
- en: '| RED [[45](#bib.bib45)] | 2017 | 71.2 | 71.0 | 70.6 | 70.2 | 69.2 | 68.5 |
    67.5 | 66.8 | 69.4 |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| RED [[45](#bib.bib45)] | 2017 | 71.2 | 71.0 | 70.6 | 70.2 | 69.2 | 68.5 |
    67.5 | 66.8 | 69.4 |'
- en: '| TTPP[[76](#bib.bib76)] | 2020 | 72.7 | 72.3 | 71.9 | 71.6 | 71.3 | 70.9 |
    69.9 | 69.3 | 71.3 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| TTPP[[76](#bib.bib76)] | 2020 | 72.7 | 72.3 | 71.9 | 71.6 | 71.3 | 70.9 |
    69.9 | 69.3 | 71.3 |'
- en: '| ED [[45](#bib.bib45)] | 2017 | TS | ANet1.3 | 78.5 | 78.0 | 76.3 | 74.6 |
    73.7 | 72.7 | 71.7 | 71.0 | 74.5 |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| ED [[45](#bib.bib45)] | 2017 | TS | ANet1.3 | 78.5 | 78.0 | 76.3 | 74.6 |
    73.7 | 72.7 | 71.7 | 71.0 | 74.5 |'
- en: '| RED [[45](#bib.bib45)] | 2017 | 79.2 | 78.7 | 77.1 | 75.5 | 74.2 | 73.0 |
    72.0 | 71.2 | 75.1 |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| RED [[45](#bib.bib45)] | 2017 | 79.2 | 78.7 | 77.1 | 75.5 | 74.2 | 73.0 |
    72.0 | 71.2 | 75.1 |'
- en: '| TRN [[75](#bib.bib75)] | 2019 | 79.9 | 78.4 | 77.1 | 75.9 | 74.9 | 73.9 |
    73.0 | 72.3 | 75.7 |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| TRN [[75](#bib.bib75)] | 2019 | 79.9 | 78.4 | 77.1 | 75.9 | 74.9 | 73.9 |
    73.0 | 72.3 | 75.7 |'
- en: '| TTPP[[76](#bib.bib76)] | 2020 | 81.2 | 80.3 | 79.3 | 77.6 | 76.9 | 76.7 |
    76.0 | 74.9 | 77.9 |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| TTPP[[76](#bib.bib76)] | 2020 | 81.2 | 80.3 | 79.3 | 77.6 | 76.9 | 76.7 |
    76.0 | 74.9 | 77.9 |'
- en: '| LAP [[77](#bib.bib77)] | 2020 | 82.6 | 81.3 | 80.0 | 78.9 | 77.9 | 77.1 |
    76.3 | 75.5 | 78.7 |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| LAP [[77](#bib.bib77)] | 2020 | 82.6 | 81.3 | 80.0 | 78.9 | 77.9 | 77.1 |
    76.3 | 75.5 | 78.7 |'
- en: '| OadTR[[80](#bib.bib80)] | 2021 | 81.9 | 80.6 | 79.4 | 78.2 | 77.1 | 76.0
    | 75.2 | 74.3 | 77.8 |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| OadTR[[80](#bib.bib80)] | 2021 | 81.9 | 80.6 | 79.4 | 78.2 | 77.1 | 76.0
    | 75.2 | 74.3 | 77.8 |'
- en: '| LSTR [[79](#bib.bib79)] | 2021 | – | – | – | – | – | – | – | – | 80.8 |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| LSTR [[79](#bib.bib79)] | 2021 | – | – | – | – | – | – | – | – | 80.8 |'
- en: '| OadTR[[80](#bib.bib80)] | 2021 | TS | K400 | 84.1 | 82.6 | 81.3 | 80.1 |
    78.9 | 77.7 | 76.7 | 75.7 | 79.1 |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| OadTR[[80](#bib.bib80)] | 2021 | TS | K400 | 84.1 | 82.6 | 81.3 | 80.1 |
    78.9 | 77.7 | 76.7 | 75.7 | 79.1 |'
- en: TVSeries [[62](#bib.bib62)] & THUMOS14 [[59](#bib.bib59)]. TVSeries contains
    six popular TV series including 30 realistic, everyday actions. THUMOS14 contains
    sport videos depicting 20 actions. Both datasets employ an anticipation protocol
    to predict actions at various future timestamps, ranging from 0.25 to 2.0 seconds.
    For evaluation, mean calibrated average precision (mcAP) is used for TVSeries
    and mean average precision (mAP) for THUMOS14\. Comprehensive benchmark results
    and average performance across all timestamps can be found in Table [VII](#S8.T7
    "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep Learning Techniques for
    Action Anticipation") and Table [VIII](#S8.T8 "TABLE VIII ‣ 8 Supplementary Material
    ‣ A Survey on Deep Learning Techniques for Action Anticipation"), respectively.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: TVSeries [[62](#bib.bib62)] 和 THUMOS14 [[59](#bib.bib59)]。TVSeries 包含六个受欢迎的电视剧，其中包括30个现实的日常动作。THUMOS14
    包含描绘20种动作的运动视频。这两个数据集都采用预期协议来预测未来不同时间戳的动作，时间范围从0.25到2.0秒。评估时，TVSeries 使用平均校准精度
    (mcAP)，而 THUMOS14 使用平均精度 (mAP)。详细的基准结果和所有时间戳的平均性能可以在表 [VII](#S8.T7 "TABLE VII
    ‣ 8 Supplementary Material ‣ A Survey on Deep Learning Techniques for Action Anticipation")
    和表 [VIII](#S8.T8 "TABLE VIII ‣ 8 Supplementary Material ‣ A Survey on Deep Learning
    Techniques for Action Anticipation") 中找到。
- en: 'EpicKitchens-100 [[68](#bib.bib68)] consists of long unscripted videos of cooking
    activities totalling 100 hours. Table [IX](#S8.T9 "TABLE IX ‣ 8 Supplementary
    Material ‣ A Survey on Deep Learning Techniques for Action Anticipation") outlines
    three performance benchmarks: overall, unseen kitchen, and tail classes. Each
    of these benchmarks is evaluated based on the top-5 recall for verbs, nouns, and
    actions across all classes. The primary metric used to rank the methods is accentuated.
    If the anticipation approach employs a backbone fine-tuned for feature extraction,
    it is marked with a check mark in the column titled E2E.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: EpicKitchens-100 [[68](#bib.bib68)] 包含了总计100小时的烹饪活动的长时间非脚本化视频。表 [IX](#S8.T9
    "TABLE IX ‣ 8 Supplementary Material ‣ A Survey on Deep Learning Techniques for
    Action Anticipation") 列出了三个性能基准：整体，未见过的厨房，以及尾部类别。每个基准都是根据所有类别中的动词、名词和动作的前五名召回率进行评估的。用于排名的方法的主要指标被突出显示。如果预期方法采用了针对特征提取进行微调的骨干网络，它将在标题为
    E2E 的列中标记一个对勾。
- en: Breakfast [[66](#bib.bib66)] & 50Salads [[70](#bib.bib70)]. The Breakfast dataset
    features videos of various individuals preparing breakfast in distinct kitchens,
    while the 50Salads dataset consists of top-view RGB-D video footage capturing
    individuals making salads. Evaluation of these datasets is conducted using mean-over-classes
    accuracy, averaged across future timestamps within a specified anticipation duration.
    For evaluation, a fraction ($\alpha$) of a complete video is observed, with the
    objective of predicting actions in the subsequent video. Typically, the observation
    ratio $\alpha$ is set to 0.2 or 0.3\. Action predictions are then made in $\beta$
    segments of the full video, where $\beta$ can be $\{0.1,0.2,0.3,0.5\}$. It is
    important to note that the evaluation protocol of Anticipatr deviates from previous
    works. Instead of the typical approach, they measure the predictions at $\beta$
    of the residual video segments. We thus show the results of Anticipatr in Table [X](#S8.T10
    "TABLE X ‣ 8 Supplementary Material ‣ A Survey on Deep Learning Techniques for
    Action Anticipation") and Table [XI](#S8.T11 "TABLE XI ‣ 8 Supplementary Material
    ‣ A Survey on Deep Learning Techniques for Action Anticipation") in gray font
    since the results are not comparable to previous works.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: Breakfast [[66](#bib.bib66)] 和 50Salads [[70](#bib.bib70)]。Breakfast 数据集包含了不同厨房中各种个体准备早餐的视频，而
    50Salads 数据集包含了俯视角度的 RGB-D 视频片段，捕捉了个体制作沙拉的过程。这些数据集的评估是使用跨未来时间戳的均值准确率进行的，平均计算在指定的预期时间段内。评估时，观察到完整视频的一部分
    ($\alpha$)，目的是预测后续视频中的动作。通常，观察比例 $\alpha$ 设置为0.2或0.3。然后在完整视频的 $\beta$ 段中进行动作预测，其中
    $\beta$ 可以是 $\{0.1,0.2,0.3,0.5\}$。需要注意的是，Anticipatr 的评估协议与以前的工作有所不同。与典型方法不同，他们测量的是剩余视频段的
    $\beta$ 的预测结果。因此，我们在表 [X](#S8.T10 "TABLE X ‣ 8 Supplementary Material ‣ A Survey
    on Deep Learning Techniques for Action Anticipation") 和表 [XI](#S8.T11 "TABLE XI
    ‣ 8 Supplementary Material ‣ A Survey on Deep Learning Techniques for Action Anticipation")
    中以灰色字体显示 Anticipatr 的结果，因为这些结果无法与之前的工作进行比较。
- en: 'TABLE VIII: Comparison of the state-of-the-art methods on THUMOS14 [[59](#bib.bib59)]
    in terms of mAP (%). For details on the font coding, please refer to Table [VII](#S8.T7
    "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep Learning Techniques for
    Action Anticipation").'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VIII: 关于THUMOS14 [[59](#bib.bib59)]上先进方法的比较，按mAP（%）排序。关于字体编码的详细信息，请参考表 [VII](#S8.T7
    "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep Learning Techniques for
    Action Anticipation")。'
- en: '| Method | Year | Backbone | Pre-train | mAP@Time predicted into the future
    (seconds) |  |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 主干网络 | 预训练 | 未来预测的mAP（秒） |  |'
- en: '| 0.25 | 0.5 | 0.75 | 1.0 | 1.25 | 1.5 | 1.75 | 2.0 | Avg. |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| 0.25 | 0.5 | 0.75 | 1.0 | 1.25 | 1.5 | 1.75 | 2.0 | 平均 |'
- en: '| ED [[45](#bib.bib45)] | 2017 | TS | ANet1.3 | 43.8 | 40.9 | 38.7 | 36.8 |
    34.6 | 33.9 | 32.5 | 31.6 | 36.6 |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| ED [[45](#bib.bib45)] | 2017 | TS | ANet1.3 | 43.8 | 40.9 | 38.7 | 36.8 |
    34.6 | 33.9 | 32.5 | 31.6 | 36.6 |'
- en: '| RED [[45](#bib.bib45)] | 2017 | 45.3 | 42.1 | 39.6 | 37.5 | 35.8 | 34.4 |
    33.2 | 32.1 | 37.5 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| RED [[45](#bib.bib45)] | 2017 | 45.3 | 42.1 | 39.6 | 37.5 | 35.8 | 34.4 |
    33.2 | 32.1 | 37.5 |'
- en: '| TRN [[75](#bib.bib75)] | 2019 | 45.1 | 42.4 | 40.7 | 39.1 | 37.7 | 36.4 |
    35.3 | 34.3 | 38.9 |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| TRN [[75](#bib.bib75)] | 2019 | 45.1 | 42.4 | 40.7 | 39.1 | 37.7 | 36.4 |
    35.3 | 34.3 | 38.9 |'
- en: '| TTPP [[76](#bib.bib76)] | 2020 | 45.9 | 43.7 | 42.4 | 41.0 | 39.9 | 39.4
    | 37.9 | 37.3 | 40.9 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| TTPP [[76](#bib.bib76)] | 2020 | 45.9 | 43.7 | 42.4 | 41.0 | 39.9 | 39.4
    | 37.9 | 37.3 | 40.9 |'
- en: '| LAP [[77](#bib.bib77)] | 2020 | 49.0 | 47.4 | 45.3 | 43.2 | 41.3 | 39.7 |
    38.3 | 37.0 | 42.6 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| LAP [[77](#bib.bib77)] | 2020 | 49.0 | 47.4 | 45.3 | 43.2 | 41.3 | 39.7 |
    38.3 | 37.0 | 42.6 |'
- en: '| OadTR [[80](#bib.bib80)] | 2021 | 50.2 | 49.3 | 48.1 | 46.8 | 45.3 | 43.9
    | 42.4 | 41.1 | 45.9 |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| OadTR [[80](#bib.bib80)] | 2021 | 50.2 | 49.3 | 48.1 | 46.8 | 45.3 | 43.9
    | 42.4 | 41.1 | 45.9 |'
- en: '| LSTR [[79](#bib.bib79)] | 2021 | – | – | – | – | – | – | – | – | 50.1 |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| LSTR [[79](#bib.bib79)] | 2021 | – | – | – | – | – | – | – | – | 50.1 |'
- en: '| TeSTra [[83](#bib.bib83)] | 2022 | 64.7 | 61.8 | 58.7 | 55.7 | 53.2 | 51.1
    | 49.2 | 47.8 | 55.3 |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| TeSTra [[83](#bib.bib83)] | 2022 | 64.7 | 61.8 | 58.7 | 55.7 | 53.2 | 51.1
    | 49.2 | 47.8 | 55.3 |'
- en: '| TTPP [[76](#bib.bib76)] | 2020 | TS | K400 | 46.8 | 45.5 | 44.6 | 43.6 |
    41.9 | 41.1 | 40.4 | 38.7 | 42.8 |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| TTPP [[76](#bib.bib76)] | 2020 | TS | K400 | 46.8 | 45.5 | 44.6 | 43.6 |
    41.9 | 41.1 | 40.4 | 38.7 | 42.8 |'
- en: '| LSTR [[79](#bib.bib79)] | 2021 | 60.4 | 58.6 | 56.0 | 53.3 | 50.9 | 48.9
    | 47.1 | 45.7 | 52.6 |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| LSTR [[79](#bib.bib79)] | 2021 | 60.4 | 58.6 | 56.0 | 53.3 | 50.9 | 48.9
    | 47.1 | 45.7 | 52.6 |'
- en: '| OadTR [[80](#bib.bib80)] | 2021 | 59.8 | 58.5 | 56.6 | 54.6 | 52.6 | 50.5
    | 48.6 | 46.8 | 53.5 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| OadTR [[80](#bib.bib80)] | 2021 | 59.8 | 58.5 | 56.6 | 54.6 | 52.6 | 50.5
    | 48.6 | 46.8 | 53.5 |'
- en: '| TeSTra [[83](#bib.bib83)] | 2022 | 66.2 | 63.5 | 60.5 | 57.4 | 54.8 | 52.6
    | 50.5 | 48.9 | 56.8 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| TeSTra [[83](#bib.bib83)] | 2022 | 66.2 | 63.5 | 60.5 | 57.4 | 54.8 | 52.6
    | 50.5 | 48.9 | 56.8 |'
- en: 'TABLE IX: Comparison of state-of-the-art methods on the validation and test
    set of EpicKitchens-100 in terms of mean top-5 recall (%). The optimal performance
    in each column within every block is indicated in bold. Modalities: Objects (O),
    Bounding Boxes (BB), Motion (M), Audio (Au).'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IX: 关于EpicKitchens-100的验证和测试集上先进方法的比较，按平均Top-5召回率（%）排序。每个块中每列的最佳性能用粗体标出。模态：对象（O），边界框（BB），动作（M），音频（Au）。'
- en: '|  | Method | Year | Modality | Backbone | Init. | E2E | Overall | Unseen Kitchen
    | Tail Classes |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 年份 | 模态 | 主干网络 | 初始化 | E2E | 总体 | 未见厨房 | 尾部类别 |'
- en: '| Verb | Noun | Act. |  Verb | Noun | Act. |  Verb | Noun | Act. |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| 动词 | 名词 | 动作 |  动词 | 名词 | 动作 |  动词 | 名词 | 动作 |'
- en: '| Val | RULSTM [[98](#bib.bib98)] | 2019 | RGB | TSN | IN1K | ✗ | 27.5 | 29.0
    | 13.3 |  – | – | – |  – | – | – |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| Val | RULSTM [[98](#bib.bib98)] | 2019 | RGB | TSN | IN1K | ✗ | 27.5 | 29.0
    | 13.3 |  – | – | – |  – | – | – |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | RGB | TSN | IN1K | ✗ | 24.2 | 29.8 | 13.0
    |  27.0 | 23.0 | 12.2 |  16.2 | 22.9 | 10.4 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | RGB | TSN | IN1K | ✗ | 24.2 | 29.8 | 13.0
    |  27.0 | 23.0 | 12.2 |  16.2 | 22.9 | 10.4 |'
- en: '| AVT [[67](#bib.bib67)] | 2021 | RGB | TSN | IN1K | ✗ | 27.2 | 30.7 | 13.6
    |  – | – | – |  – | – | – |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| AVT [[67](#bib.bib67)] | 2021 | RGB | TSN | IN1K | ✗ | 27.2 | 30.7 | 13.6
    |  – | – | – |  – | – | – |'
- en: '| AVT [[67](#bib.bib67)] | 2021 | RGB | ViT-B | IN21K | ✓ | 30.2 | 31.7 | 14.9
    |  – | – | – |  – | – | – |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| AVT [[67](#bib.bib67)] | 2021 | RGB | ViT-B | IN21K | ✓ | 30.2 | 31.7 | 14.9
    |  – | – | – |  – | – | – |'
- en: '| MeMViT [[6](#bib.bib6)] | 2022 | RGB | MViT-B | K400 | ✓ | 32.8 | 33.2 |
    15.1 |  27.5 | 21.7 | 9.8 |  26.3 | 27.4 | 13.2 |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| MeMViT [[6](#bib.bib6)] | 2022 | RGB | MViT-B | K400 | ✓ | 32.8 | 33.2 |
    15.1 |  27.5 | 21.7 | 9.8 |  26.3 | 27.4 | 13.2 |'
- en: '| MeMViT [[6](#bib.bib6)] | 2022 | RGB | MViT-L | K700 | ✓ | 32.2 | 37.0 |
    17.7 |  28.6 | 27.4 | 15.2 |  25.3 | 31.0 | 15.5 |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| MeMViT [[6](#bib.bib6)] | 2022 | RGB | MViT-L | K700 | ✓ | 32.2 | 37.0 |
    17.7 |  28.6 | 27.4 | 15.2 |  25.3 | 31.0 | 15.5 |'
- en: '| DCR [[71](#bib.bib71)] | 2022 | RGB | TSM | K400 | ✗ | 32.6 | 32.7 | 16.1
    |  – | – | – |  – | – | – |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| DCR [[71](#bib.bib71)] | 2022 | RGB | TSM | K400 | ✗ | 32.6 | 32.7 | 16.1
    |  – | – | – |  – | – | – |'
- en: '| TeSTra [[83](#bib.bib83)] | 2022 | RGB | TSN | IN1K | ✗ | 26.8 | 36.2 | 17.0
    |  27.1 | 30.1 | 13.3 |  19.3 | 28.6 | 13.7 |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| TeSTra [[83](#bib.bib83)] | 2022 | RGB | TSN | IN1K | ✗ | 26.8 | 36.2 | 17.0
    |  27.1 | 30.1 | 13.3 |  19.3 | 28.6 | 13.7 |'
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | RGB | MViT-B | K400 + IN1K | ✗ | 33.3
    | 35.5 | 17.6 |  – | – | – |  – | – | – |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| RAFTformer [[73](#bib.bib73)] | 2023 | RGB | MViT-B | K400 + IN1K | ✗ | 33.3
    | 35.5 | 17.6 |  – | – | – |  – | – | – |'
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | RGB | MViT-B | K700 | ✗ | 33.7 | 37.1
    | 18.0 |  – | – | – |  – | – | – |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| RAFTformer [[73](#bib.bib73)] | 2023 | RGB | MViT-B | K700 | ✗ | 33.7 | 37.1
    | 18.0 |  – | – | – |  – | – | – |'
- en: '|  | RULSTM [[98](#bib.bib98)] | 2019 | RGB,O,M | TSN | IN1K | ✗ | 27.8 | 30.8
    | 14.0 |  28.8 | 27.2 | 14.2 |  19.8 | 22.0 | 11.1 |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '|  | RULSTM [[98](#bib.bib98)] | 2019 | RGB,O,M | TSN | IN1K | ✗ | 27.8 | 30.8
    | 14.0 |  28.8 | 27.2 | 14.2 |  19.8 | 22.0 | 11.1 |'
- en: '|  | TempAgg [[5](#bib.bib5)] | 2020 | RGB,O,M,BB | TSN | IN1K | ✗ | 23.2 |
    31.4 | 14.7 |  28.0 | 26.2 | 14.5 |  14.5 | 22.5 | 11.8 |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '|  | TempAgg [[5](#bib.bib5)] | 2020 | RGB,O,M,BB | TSN | IN1K | ✗ | 23.2 |
    31.4 | 14.7 |  28.0 | 26.2 | 14.5 |  14.5 | 22.5 | 11.8 |'
- en: '|  | AVT [[67](#bib.bib67)] | 2021 | RGB,O | ViT-B | IN21K | ✓ | 28.2 | 32.0
    | 15.9 |  29.5 | 23.9 | 11.9 |  21.1 | 25.8 | 14.1 |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '|  | AVT [[67](#bib.bib67)] | 2021 | RGB,O | ViT-B | IN21K | ✓ | 28.2 | 32.0
    | 15.9 |  29.5 | 23.9 | 11.9 |  21.1 | 25.8 | 14.1 |'
- en: '|  | DCR [[71](#bib.bib71)] | 2022 | RGB,O | TSM | K400 | ✗ | – | – | 18.3
    |  – | – | 14.7 |  – | – | 15.8 |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '|  | DCR [[71](#bib.bib71)] | 2022 | RGB,O | TSM | K400 | ✗ | – | – | 18.3
    |  – | – | 14.7 |  – | – | 15.8 |'
- en: '|  | TeSTra [[83](#bib.bib83)] | 2022 | RGB,M | TSN | IN1K | ✗ | 30.8 | 35.8
    | 17.6 |  29.6 | 26.0 | 12.8 |  23.2 | 29.2 | 14.2 |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '|  | TeSTra [[83](#bib.bib83)] | 2022 | RGB,M | TSN | IN1K | ✗ | 30.8 | 35.8
    | 17.6 |  29.6 | 26.0 | 12.8 |  23.2 | 29.2 | 14.2 |'
- en: '|  | AFFT [[105](#bib.bib105)] | 2023 | RGB,O,M | TSN | IN1K | ✗ | 21.3 | 32.7
    | 16.4 |  24.1 | 25.5 | 13.6 |  13.2 | 25.8 | 14.3 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '|  | AFFT [[105](#bib.bib105)] | 2023 | RGB,O,M | TSN | IN1K | ✗ | 21.3 | 32.7
    | 16.4 |  24.1 | 25.5 | 13.6 |  13.2 | 25.8 | 14.3 |'
- en: '|  | AFFT [[105](#bib.bib105)] | 2023 | RGB,O,M,Au | Swin-B | K400 | ✗ | 22.8
    | 34.6 | 18.5 |  24.8 | 26.4 | 15.5 |  15.0 | 27.7 | 16.2 |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '|  | AFFT [[105](#bib.bib105)] | 2023 | RGB,O,M,Au | Swin-B | K400 | ✗ | 22.8
    | 34.6 | 18.5 |  24.8 | 26.4 | 15.5 |  15.0 | 27.7 | 16.2 |'
- en: '| Test | RULSTM [[98](#bib.bib98)] | 2019 | RGB,O,M | TSN | IN1K | ✗ | 25.3
    | 26.7 | 11.2 |  19.4 | 26.9 | 9.7 |  17.6 | 16.0 | 7.9 |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| Test | RULSTM [[98](#bib.bib98)] | 2019 | RGB,O,M | TSN | IN1K | ✗ | 25.3
    | 26.7 | 11.2 |  19.4 | 26.9 | 9.7 |  17.6 | 16.0 | 7.9 |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | RGB,O,M,BB | TSN | IN1K | ✗ | 21.8 | 30.6
    | 12.6 |  17.9 | 27.0 | 10.5 |  13.6 | 20.6 | 8.9 |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | RGB,O,M,BB | TSN | IN1K | ✗ | 21.8 | 30.6
    | 12.6 |  17.9 | 27.0 | 10.5 |  13.6 | 20.6 | 8.9 |'
- en: '| AVT [[67](#bib.bib67)] | 2021 | RGB,O | ViT-B | IN21K | ✓ | 25.6 | 28.8 |
    12.6 |  20.9 | 22.3 | 8.8 |  19.0 | 22.0 | 10.1 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| AVT [[67](#bib.bib67)] | 2021 | RGB,O | ViT-B | IN21K | ✓ | 25.6 | 28.8 |
    12.6 |  20.9 | 22.3 | 8.8 |  19.0 | 22.0 | 10.1 |'
- en: '| TCN-TBN [[103](#bib.bib103)] | 2021 | RGB,O,M | TBN | IN1K | ✗ | 21.5 | 26.8
    | 11.0 |  20.8 | 28.3 | 12.2 |  13.2 | 15.4 | 7.2 |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| TCN-TBN [[103](#bib.bib103)] | 2021 | RGB,O,M | TBN | IN1K | ✗ | 21.5 | 26.8
    | 11.0 |  20.8 | 28.3 | 12.2 |  13.2 | 15.4 | 7.2 |'
- en: '| Abst. goal [[116](#bib.bib116)] | 2022 | RGB,O,M | TSN | IN1K | ✗ | 31.4
    | 30.1 | 14.3 |  31.4 | 35.6 | 17.3 |  – | – | – |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| Abst. goal [[116](#bib.bib116)] | 2022 | RGB,O,M | TSN | IN1K | ✗ | 31.4
    | 30.1 | 14.3 |  31.4 | 35.6 | 17.3 |  – | – | – |'
- en: '| AFFT [[105](#bib.bib105)] | 2023 | RGB,O,M,Au | Swin-B | K400 | ✗ | 20.7
    | 31.8 | 14.9 |  16.2 | 27.7 | 12.1 |  13.4 | 23.8 | 11.8 |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| AFFT [[105](#bib.bib105)] | 2023 | RGB,O,M,Au | Swin-B | K400 | ✗ | 20.7
    | 31.8 | 14.9 |  16.2 | 27.7 | 12.1 |  13.4 | 23.8 | 11.8 |'
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | RGB | MViT-B | K400 + IN1K | ✗ | 27.3
    | 32.8 | 14.0 |  – | – | – |  – | – | – |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| RAFTformer [[73](#bib.bib73)] | 2023 | RGB | MViT-B | K400 + IN1K | ✗ | 27.3
    | 32.8 | 14.0 |  – | – | – |  – | – | – |'
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | RGB | MViT-B | K700 | ✗ | 27.4 | 34.0
    | 14.7 |  – | – | – |  – | – | – |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| RAFTformer [[73](#bib.bib73)] | 2023 | RGB | MViT-B | K700 | ✗ | 27.4 | 34.0
    | 14.7 |  – | – | – |  – | – | – |'
- en: 'TABLE X: Benchmark of long-term action anticipation on Breakfast [[66](#bib.bib66)]
    in terms of mean over classes (%). For details on the font coding, please refer
    to Table [VII](#S8.T7 "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep
    Learning Techniques for Action Anticipation").'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE X: 对于 Breakfast [[66](#bib.bib66)] 长期动作预测的基准测试（按类别的均值百分比）。有关字体编码的详细信息，请参阅表 [VII](#S8.T7
    "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep Learning Techniques for
    Action Anticipation")。'
- en: '| Input Type | Backbone | Methods | Year | $\beta$ ($\alpha$ = 0.2) | $\beta$
    ($\alpha$ = 0.3) |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| Input Type | Backbone | Methods | Year | $\beta$ ($\alpha$ = 0.2) | $\beta$
    ($\alpha$ = 0.3) |'
- en: '| 0.1 | 0.2 | 0.3 | 0.5 | 0.1 | 0.2 | 0.3 | 0.5 |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 | 0.2 | 0.3 | 0.5 | 0.1 | 0.2 | 0.3 | 0.5 |'
- en: '| GT. label | – | RNN [[46](#bib.bib46)] | 2018 | 60.35 | 50.44 | 45.28 | 40.42
    | 61.45 | 50.25 | 44.90 | 41.75 |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| GT. label | – | RNN [[46](#bib.bib46)] | 2018 | 60.35 | 50.44 | 45.28 | 40.42
    | 61.45 | 50.25 | 44.90 | 41.75 |'
- en: '| CNN [[46](#bib.bib46)] | 2018 | 57.97 | 49.12 | 44.03 | 39.26 | 60.32 | 50.14
    | 45.18 | 40.51 |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| CNN [[46](#bib.bib46)] | 2018 | 57.97 | 49.12 | 44.03 | 39.26 | 60.32 | 50.14
    | 45.18 | 40.51 |'
- en: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 64.46 | 56.27 | 50.15 | 43.99 |
    65.95 | 55.94 | 49.14 | 44.23 |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 64.46 | 56.27 | 50.15 | 43.99 |
    65.95 | 55.94 | 49.14 | 44.23 |'
- en: '| UAAA [[48](#bib.bib48)] (avg.) | 2019 | 50.39 | 41.71 | 37.79 | 32.78 | 51.25
    | 42.94 | 38.33 | 33.07 |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| UAAA [[48](#bib.bib48)] (平均) | 2019 | 50.39 | 41.71 | 37.79 | 32.78 | 51.25
    | 42.94 | 38.33 | 33.07 |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 65.50 | 55.50 | 46.80 | 40.10 | 67.40 |
    56.10 | 47.40 | 41.50 |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | 65.50 | 55.50 | 46.80 | 40.10 | 67.40 |
    56.10 | 47.40 | 41.50 |'
- en: '| Zhao et al. [[49](#bib.bib49)] (avg.) | 2020 | 72.22 | 62.40 | 56.22 | 45.95
    | 74.14 | 71.32 | 65.30 | 52.38 |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| Zhao 等人 [[49](#bib.bib49)] (平均) | 2020 | 72.22 | 62.40 | 56.22 | 45.95 |
    74.14 | 71.32 | 65.30 | 52.38 |'
- en: '| Pred. label | Fisher | RNN [[46](#bib.bib46)] | 2018 | 18.11 | 17.20 | 15.94
    | 15.81 | 21.64 | 20.02 | 19.73 | 19.21 |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| 预测标签 | Fisher | RNN [[46](#bib.bib46)] | 2018 | 18.11 | 17.20 | 15.94 | 15.81
    | 21.64 | 20.02 | 19.73 | 19.21 |'
- en: '| CNN [[46](#bib.bib46)] | 2018 | 17.90 | 16.35 | 15.37 | 14.54 | 22.44 | 20.12
    | 19.69 | 18.76 |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| CNN [[46](#bib.bib46)] | 2018 | 17.90 | 16.35 | 15.37 | 14.54 | 22.44 | 20.12
    | 19.69 | 18.76 |'
- en: '| UAAA [[48](#bib.bib48)] (avg.) | 2019 | 15.69 | 14.00 | 13.30 | 12.95 | 19.14
    | 17.18 | 17.38 | 14.98 |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| UAAA [[48](#bib.bib48)] (平均) | 2019 | 15.69 | 14.00 | 13.30 | 12.95 | 19.14
    | 17.18 | 17.38 | 14.98 |'
- en: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 18.41 | 17.21 | 16.42 | 15.84 |
    22.75 | 20.44 | 19.64 | 19.75 |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 18.41 | 17.21 | 16.42 | 15.84 |
    22.75 | 20.44 | 19.64 | 19.75 |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 18.80 | 16.90 | 16.50 | 15.40 | 23.00 |
    20.00 | 19.90 | 18.60 |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | 18.80 | 16.90 | 16.50 | 15.40 | 23.00 |
    20.00 | 19.90 | 18.60 |'
- en: '|  | I3D | TempAgg [[5](#bib.bib5)] | 2020 | 37.40 | 31.20 | 30.00 | 26.10
    | 39.50 | 34.10 | 31.00 | 27.90 |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '|  | I3D | TempAgg [[5](#bib.bib5)] | 2020 | 37.40 | 31.20 | 30.00 | 26.10
    | 39.50 | 34.10 | 31.00 | 27.90 |'
- en: '| Features | Fisher | CNN [[46](#bib.bib46)] | 2018 | 12.78 | 11.62 | 11.21
    | 10.27 | 17.72 | 16.87 | 15.48 | 14.09 |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | Fisher | CNN [[46](#bib.bib46)] | 2018 | 12.78 | 11.62 | 11.21 | 10.27
    | 17.72 | 16.87 | 15.48 | 14.09 |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 15.60 | 13.10 | 12.10 | 11.10 | 19.50 |
    17.00 | 15.60 | 15.10 |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | 15.60 | 13.10 | 12.10 | 11.10 | 19.50 |
    17.00 | 15.60 | 15.10 |'
- en: '|  | I3D | TempAgg [[5](#bib.bib5)] | 2020 | 24.20 | 21.10 | 20.00 | 18.10
    | 30.40 | 26.30 | 23.80 | 21.20 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '|  | I3D | TempAgg [[5](#bib.bib5)] | 2020 | 24.20 | 21.10 | 20.00 | 18.10
    | 30.40 | 26.30 | 23.80 | 21.20 |'
- en: '|  | Cycle Cons[[112](#bib.bib112)] | 2020 | 25.88 | 23.42 | 22.42 | 21.54
    | 29.66 | 27.37 | 25.58 | 25.20 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '|  | Cycle Cons [[112](#bib.bib112)] | 2020 | 25.88 | 23.42 | 22.42 | 21.54
    | 29.66 | 27.37 | 25.58 | 25.20 |'
- en: '|  | A-ACT [[115](#bib.bib115)] | 2022 | 26.70 | 24.30 | 23.20 | 21.70 | 30.80
    | 28.30 | 26.10 | 25.80 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '|  | A-ACT [[115](#bib.bib115)] | 2022 | 26.70 | 24.30 | 23.20 | 21.70 | 30.80
    | 28.30 | 26.10 | 25.80 |'
- en: '|  | FUTR [[82](#bib.bib82)] | 2022 | 27.70 | 24.55 | 22.83 | 22.04 | 32.27
    | 29.88 | 27.49 | 25.87 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '|  | FUTR [[82](#bib.bib82)] | 2022 | 27.70 | 24.55 | 22.83 | 22.04 | 32.27
    | 29.88 | 27.49 | 25.87 |'
- en: '|  | Anticipatr [[117](#bib.bib117)] | 2022 | 37.40 | 32.00 | 30.30 | 28.60
    | 39.90 | 35.70 | 32.10 | 29.40 |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '|  | Anticipatr [[117](#bib.bib117)] | 2022 | 37.40 | 32.00 | 30.30 | 28.60
    | 39.90 | 35.70 | 32.10 | 29.40 |'
- en: 'TABLE XI: Benchmark of long-term action anticipation on 50 Salads [[70](#bib.bib70)]
    in terms of mean over classes (%). For details on the font coding, please refer
    to Table [VII](#S8.T7 "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep
    Learning Techniques for Action Anticipation").'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE XI: 关于50 Salads [[70](#bib.bib70)] 的长期动作预测基准，以类别平均值（%）表示。有关字体编码的详细信息，请参见表
    [VII](#S8.T7 "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep Learning
    Techniques for Action Anticipation")。'
- en: '| Input Type | Backbone | Methods | Year | $\beta$ ($\alpha$ = 0.2) | $\beta$
    ($\alpha$ = 0.3) |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| 输入类型 | 主干网络 | 方法 | 年份 | $\beta$ ($\alpha$ = 0.2) | $\beta$ ($\alpha$ = 0.3)
    |'
- en: '| 0.1 | 0.2 | 0.3 | 0.5 | 0.1 | 0.2 | 0.3 | 0.5 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 | 0.2 | 0.3 | 0.5 | 0.1 | 0.2 | 0.3 | 0.5 |'
- en: '| GT. label | – | RNN [[46](#bib.bib46)] | 2018 | 42.30 | 31.19 | 25.22 | 16.82
    | 44.19 | 29.51 | 19.96 | 10.38 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| GT. 标签 | – | RNN [[46](#bib.bib46)] | 2018 | 42.30 | 31.19 | 25.22 | 16.82
    | 44.19 | 29.51 | 19.96 | 10.38 |'
- en: '| CNN [[46](#bib.bib46)] | 2018 | 36.08 | 27.62 | 21.43 | 15.48 | 37.36 | 24.78
    | 20.78 | 14.05 |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| CNN [[46](#bib.bib46)] | 2018 | 36.08 | 27.62 | 21.43 | 15.48 | 37.36 | 24.78
    | 20.78 | 14.05 |'
- en: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 45.12 | 33.23 | 27.59 | 17.27 |
    46.40 | 34.80 | 25.24 | 13.84 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 45.12 | 33.23 | 27.59 | 17.27 |
    46.40 | 34.80 | 25.24 | 13.84 |'
- en: '| UAAA [[48](#bib.bib48)] (avg.) | 2019 | 34.95 | 28.05 | 24.08 | 15.41 | 33.15
    | 24.65 | 18.84 | 14.34 |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| UAAA [[48](#bib.bib48)] (平均) | 2019 | 34.95 | 28.05 | 24.08 | 15.41 | 33.15
    | 24.65 | 18.84 | 14.34 |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 47.20 | 34.60 | 30.50 | 19.10 | 44.80 |
    32.70 | 23.50 | 15.30 |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | 47.20 | 34.60 | 30.50 | 19.10 | 44.80 |
    32.70 | 23.50 | 15.30 |'
- en: '| Zhao et al. [[49](#bib.bib49)] (avg.) | 2020 | 46.63 | 35.62 | 31.91 | 21.37
    | 46.13 | 36.37 | 33.10 | 19.45 |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| Zhao 等人 [[49](#bib.bib49)] (平均) | 2020 | 46.63 | 35.62 | 31.91 | 21.37 |
    46.13 | 36.37 | 33.10 | 19.45 |'
- en: '| Pred. label | Fisher | RNN [[46](#bib.bib46)] | 2018 | 30.06 | 25.43 | 18.74
    | 13.49 | 30.77 | 17.19 | 14.79 | 9.77 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| 预测标签 | Fisher | RNN [[46](#bib.bib46)] | 2018 | 30.06 | 25.43 | 18.74 | 13.49
    | 30.77 | 17.19 | 14.79 | 9.77 |'
- en: '| CNN [[46](#bib.bib46)] | 2018 | 21.24 | 19.03 | 15.98 | 9.87 | 29.14 | 20.14
    | 17.46 | 10.86 |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| CNN [[46](#bib.bib46)] | 2018 | 21.24 | 19.03 | 15.98 | 9.87 | 29.14 | 20.14
    | 17.46 | 10.86 |'
- en: '| UAAA [[48](#bib.bib48)] (avg.) | 2019 | 23.56 | 19.48 | 18.01 | 12.78 | 28.04
    | 17.95 | 14.77 | 12.06 |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| UAAA [[48](#bib.bib48)] (平均) | 2019 | 23.56 | 19.48 | 18.01 | 12.78 | 28.04
    | 17.95 | 14.77 | 12.06 |'
- en: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 32.51 | 27.61 | 21.26 | 15.99 |
    35.12 | 27.05 | 22.05 | 15.59 |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 32.51 | 27.61 | 21.26 | 15.99 |
    35.12 | 27.05 | 22.05 | 15.59 |'
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 32.70 | 26.30 | 21.90 | 15.60 | 32.30 |
    25.50 | 22.70 | 17.10 |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| TempAgg [[5](#bib.bib5)] | 2020 | 32.70 | 26.30 | 21.90 | 15.60 | 32.30 |
    25.50 | 22.70 | 17.10 |'
- en: '| Piergiovanni et al. [[124](#bib.bib124)] | 2020 | 40.40 | 33.70 | 25.40 |
    20.90 | 40.70 | 40.10 | 26.40 | 19.20 |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| Piergiovanni 等 [[124](#bib.bib124)] | 2020 | 40.40 | 33.70 | 25.40 | 20.90
    | 40.70 | 40.10 | 26.40 | 19.20 |'
- en: '| Features | Fisher | TempAgg [[5](#bib.bib5)] | 2020 | 25.50 | 19.90 | 18.20
    | 15.10 | 30.60 | 22.50 | 19.10 | 11.20 |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | Fisher | TempAgg [[5](#bib.bib5)] | 2020 | 25.50 | 19.90 | 18.20 | 15.10
    | 30.60 | 22.50 | 19.10 | 11.20 |'
- en: '|  | I3D | Cycle Cons. [[112](#bib.bib112)] | 2020 | 34.76 | 28.41 | 21.82
    | 15.25 | 34.39 | 23.70 | 18.95 | 15.89 |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '|  | I3D | Cycle Cons. [[112](#bib.bib112)] | 2020 | 34.76 | 28.41 | 21.82
    | 15.25 | 34.39 | 23.70 | 18.95 | 15.89 |'
- en: '|  | A-ACT [[115](#bib.bib115)] | 2022 | 35.40 | 29.60 | 22.50 | 16.10 | 35.70
    | 25.30 | 20.10 | 16.30 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '|  | A-ACT [[115](#bib.bib115)] | 2022 | 35.40 | 29.60 | 22.50 | 16.10 | 35.70
    | 25.30 | 20.10 | 16.30 |'
- en: '|  | FUTR [[82](#bib.bib82)] | 2022 | 39.55 | 27.54 | 23.31 | 17.77 | 35.15
    | 24.86 | 24.22 | 15.26 |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '|  | FUTR [[82](#bib.bib82)] | 2022 | 39.55 | 27.54 | 23.31 | 17.77 | 35.15
    | 24.86 | 24.22 | 15.26 |'
- en: '|  | Anticipatr [[117](#bib.bib117)] | 2022 | 41.10 | 35.00 | 27.60 | 27.30
    | 42.80 | 42.30 | 28.50 | 23.60 |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '|  | Anticipatr [[117](#bib.bib117)] | 2022 | 41.10 | 35.00 | 27.60 | 27.30
    | 42.80 | 42.30 | 28.50 | 23.60 |'
