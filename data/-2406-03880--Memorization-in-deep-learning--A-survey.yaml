- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:31:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:31:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2406.03880] Memorization in deep learning: A survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2406.03880] 深度学习中的记忆：调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.03880](https://ar5iv.labs.arxiv.org/html/2406.03880)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.03880](https://ar5iv.labs.arxiv.org/html/2406.03880)
- en: 'Memorization in deep learning: A survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的记忆：调查
- en: 'Jiaheng Wei, Yanjun Zhang, Leo Yu Zhang,  Ming Ding,  Chao Chen,  Kok-Leong
    Ong, Jun Zhang,  Yang Xiang Manuscript received June 7, 2024.Jiaheng Wei, Chao
    Chen, and Kok-Leong Ong are with the School of Accounting, Information System
    and Supply Chain, RMIT University, Melbourne, VIC 3001, Australia (e-mail: s3986349@student.rmit.edu.au;
    chao.chen@rmit.edu.au; kok-leong.ong2@rmit.edu.au).Yanjun Zhang is with the School
    of Computer Science, University of Technology Sydney, Sydney, NSW 2007, Australia
    (e-mail: Yanjun.Zhang@uts.edu.au).Leo Yu Zhang is with the School of Information
    and Communication Technology, Griffith University, Brisbane, QLD 4215, Australia
    (e-mail: leo.zhang@griffith.edu.au).Ming Ding is with Data61, CSIRO, Sydney, NSW
    2015, Australia (e-mail: ming.ding@data61.csiro.au).Jun Zhang and Yang Xiang are
    with the School of Science, Computing and Engineering Technologies, Swinburne
    University of Technology, Melbourne, VIC 3122, Australia (e-mail: junzhang@swin.edu.au;
    yxiang@swin.edu.au). [0009-0003-7180-4268](https://orcid.org/0009-0003-7180-4268
    "ORCID identifier") [0000-0001-5611-3483](https://orcid.org/0000-0001-5611-3483
    "ORCID identifier") [0000-0001-9330-2662](https://orcid.org/0000-0001-9330-2662
    "ORCID identifier") [0000-0002-3690-0321](https://orcid.org/0000-0002-3690-0321
    "ORCID identifier") [0000-0003-1355-3870](https://orcid.org/0000-0003-1355-3870
    "ORCID identifier") [0000-0003-4688-7674](https://orcid.org/0000-0003-4688-7674%0A
    "ORCID identifier") [0000-0002-2189-7801](https://orcid.org/0000-0002-2189-7801
    "ORCID identifier") [0000-0001-5252-0831](https://orcid.org/0000-0001-5252-0831
    "ORCID identifier")'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Jiaheng Wei, Yanjun Zhang, Leo Yu Zhang, Ming Ding, Chao Chen, Kok-Leong Ong,
    Jun Zhang, Yang Xiang 论文收到日期：2024年6月7日。Jiaheng Wei、Chao Chen 和 Kok-Leong Ong 隶属于澳大利亚墨尔本RMIT大学会计、信息系统与供应链学院（电子邮件：s3986349@student.rmit.edu.au;
    chao.chen@rmit.edu.au; kok-leong.ong2@rmit.edu.au）。Yanjun Zhang 隶属于澳大利亚悉尼科技大学计算机科学学院（电子邮件：Yanjun.Zhang@uts.edu.au）。Leo
    Yu Zhang 隶属于澳大利亚布里斯班格里菲斯大学信息与通信技术学院（电子邮件：leo.zhang@griffith.edu.au）。Ming Ding
    隶属于澳大利亚悉尼CSIRO的Data61（电子邮件：ming.ding@data61.csiro.au）。Jun Zhang 和 Yang Xiang 隶属于澳大利亚墨尔本斯威本科技大学科学、计算与工程技术学院（电子邮件：junzhang@swin.edu.au;
    yxiang@swin.edu.au）。 [0009-0003-7180-4268](https://orcid.org/0009-0003-7180-4268
    "ORCID identifier") [0000-0001-5611-3483](https://orcid.org/0000-0001-5611-3483
    "ORCID identifier") [0000-0001-9330-2662](https://orcid.org/0000-0001-9330-2662
    "ORCID identifier") [0000-0002-3690-0321](https://orcid.org/0000-0002-3690-0321
    "ORCID identifier") [0000-0003-1355-3870](https://orcid.org/0000-0003-1355-3870
    "ORCID identifier") [0000-0003-4688-7674](https://orcid.org/0000-0003-4688-7674%0A
    "ORCID identifier") [0000-0002-2189-7801](https://orcid.org/0000-0002-2189-7801
    "ORCID identifier") [0000-0001-5252-0831](https://orcid.org/0000-0001-5252-0831
    "ORCID identifier")
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep Learning (DL) powered by Deep Neural Networks (DNNs) has revolutionized
    various domains, yet understanding the intricacies of DNN decision-making and
    learning processes remains a significant challenge. Recent investigations have
    uncovered an interesting memorization phenomenon in which DNNs tend to memorize
    specific details from examples rather than learning general patterns, affecting
    model generalization, security, and privacy. This raises critical questions about
    the nature of generalization in DNNs and their susceptibility to security breaches.
    In this survey, we present a systematic framework to organize memorization definitions
    based on the generalization and security/privacy domains and summarize memorization
    evaluation methods at both the example and model levels. Through a comprehensive
    literature review, we explore DNN memorization behaviors and their impacts on
    security and privacy. We also introduce privacy vulnerabilities caused by memorization
    and the phenomenon of forgetting and explore its connection with memorization.
    Furthermore, we spotlight various applications leveraging memorization and forgetting
    mechanisms, including noisy label learning, privacy preservation, and model enhancement.
    This survey offers the first-in-kind understanding of memorization in DNNs, providing
    insights into its challenges and opportunities for enhancing AI development while
    addressing critical ethical concerns.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由深度神经网络（DNNs）驱动的深度学习（DL）已经革新了多个领域，但理解DNN决策和学习过程的复杂性仍然是一个重大挑战。近期的研究揭示了一个有趣的记忆现象，即DNN倾向于记住示例中的具体细节，而不是学习一般模式，这影响了模型的泛化、安全性和隐私。这引发了关于DNN泛化本质及其对安全漏洞的易感性的重要问题。在这项调查中，我们提出了一个系统框架，根据泛化和安全/隐私领域组织记忆定义，并总结了在示例和模型层面上的记忆评估方法。通过全面的文献综述，我们探索了DNN记忆行为及其对安全和隐私的影响。我们还介绍了由记忆引发的隐私漏洞以及遗忘现象，并探讨了其与记忆的关系。此外，我们重点介绍了利用记忆和遗忘机制的各种应用，包括噪声标签学习、隐私保护和模型增强。这项调查提供了DNN记忆的首个全面理解，洞察了其挑战和提升AI发展的机会，同时解决了关键的伦理问题。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep learning, deep neural networks, memorization phenomenon, forgetting phenomenon,
    privacy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、深度神经网络、记忆现象、遗忘现象、隐私。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: In the development of artificial intelligence (AI), deep learning (DL) has emerged
    as an effective solution for various complex tasks like text generation [[1](#bib.bib1)],
    speech translation [[2](#bib.bib2)], etc. Deep neural network (DNN) as the main
    model architecture has been widely used in numerous innovative applications such
    as autonomous vehicles [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)] and medical
    diagnosis [[6](#bib.bib6), [7](#bib.bib7)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能（AI）的发展中，深度学习（DL）已成为解决各种复杂任务如文本生成[[1](#bib.bib1)]、语音翻译[[2](#bib.bib2)]等的有效方案。深度神经网络（DNN）作为主要的模型架构，在诸如自动驾驶[[3](#bib.bib3)、[4](#bib.bib4)、[5](#bib.bib5)]和医学诊断[[6](#bib.bib6)、[7](#bib.bib7)]等众多创新应用中得到了广泛使用。
- en: '![Refer to caption](img/9ad7d875d03906d68abb8d3672cf3711.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9ad7d875d03906d68abb8d3672cf3711.png)'
- en: (a)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/bd0d52b1f91031c4f2b4367e98b3d312.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bd0d52b1f91031c4f2b4367e98b3d312.png)'
- en: (b)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 1: The Direct Memorization Effect. In (a), we use an image generator
    to describe memorization. The upper part demonstrates the memorization effect
    and the lower part represents the common generation. For (b), the memorization
    effect has two different levels: Example Memorization and Model Memorization.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：直接记忆效应。在（a）中，我们使用图像生成器来描述记忆。上部分展示了记忆效应，下部分表示常见的生成情况。对于（b），记忆效应有两个不同的层次：示例记忆和模型记忆。
- en: However, it is still challenging to understand how DNNs make decisions and what
    they learn from the training data. Though researchers believe DNNs can learn patterns
    in the training data to achieve success in assigned tasks, a recent study found
    that DNNs are able to memorize the entire randomly labeled training dataset [[8](#bib.bib8)],
    which illustrates that properties of the model family, or the regularization techniques
    fail to explain why large neural networks generalize well. DNNs may memorize particular
    features from training data instead of learning patterns to perform specific tasks.
    This attracts the community to explore the memorization mechanism and prompts
    researchers to rethink the generalization in DNNs. Additionally, this memorization
    phenomenon raises concerns about the security of AI because of potential privacy
    leakage risks and vulnerability against malicious attacks. Furthermore, the training
    dataset collected from the real world may contain significant noise and bias,
    and memorized data in DNNs may keep the noise and bias, impairing the usability
    and fairness of the models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，理解DNN如何做出决策以及它们从训练数据中学到了什么仍然具有挑战性。尽管研究人员认为DNN可以通过学习训练数据中的模式来成功完成指定任务，但最近的一项研究发现，DNN能够记住整个随机标记的训练数据集[[8](#bib.bib8)]，这表明模型家族的属性或正则化技术未能解释为何大型神经网络能够很好地泛化。DNN可能记住了训练数据中的特定特征，而不是学习模式以执行特定任务。这吸引了学术界探索记忆机制，并促使研究人员重新思考DNN的泛化。此外，这种记忆现象引发了对AI安全性的担忧，因为可能存在隐私泄露风险和对恶意攻击的脆弱性。此外，从现实世界中收集的训练数据可能包含显著的噪声和偏差，而DNN中记忆的数据可能保留噪声和偏差，从而影响模型的可用性和公平性。
- en: 'So far, numerous papers have found the memorization effects that neural networks
    may memorize some training data in training with gradient descent [[9](#bib.bib9),
    [10](#bib.bib10), [8](#bib.bib8), [11](#bib.bib11), [12](#bib.bib12)]. Current
    memorization studies mainly focus on two domains: the behaviors in standard training
    and the security/privacy risks. We summarize explicit memorization definitions
    in literature based on generalization and security/privacy. However, there is
    a lack of a widely adopted definition for memorization, making describing and
    discussing the memorization concept challenging. Many relevant works provide inconsistent,
    sometimes contradictory, definitions of memorization. Especially, many works directly
    apply the word "memorization" as the synonymous words of "learning" and "fitting".
    Thus, we adopt the following terms for facilitating discussion: Memorization Learning
    refers to DNNs learning specific details or particular features of examples, while
    common Pattern Learning indicates DNNs learning the common patterns or generalized
    features of the data distribution.In Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ I
    Introduction ‣ Memorization in deep learning: A survey"), we use a large language
    model to illustrate memorization learning and pattern learning. We use the word
    "generalization" to define the model performance on the new, unseen data. Suppose
    there is no extra explanation, all terms like "memorization", "memorization effect",
    and "memorization phenomenon" point to memorization learning. Moreover, we think
    pattern learning and memorization learning together constitute the learning path
    of DNNs.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，大量论文发现了神经网络可能在梯度下降训练中记住一些训练数据的记忆效应[[9](#bib.bib9), [10](#bib.bib10), [8](#bib.bib8),
    [11](#bib.bib11), [12](#bib.bib12)]。当前的记忆研究主要集中在两个领域：标准训练中的行为和安全/隐私风险。我们根据泛化和安全/隐私总结了文献中的明确记忆定义。然而，缺乏广泛采用的记忆定义，使得描述和讨论记忆概念变得具有挑战性。许多相关工作提供了不一致、有时矛盾的记忆定义。尤其是，许多工作将“记忆”一词直接用作“学习”和“拟合”的同义词。因此，我们采用以下术语以便于讨论：记忆学习指DNN学习示例的特定细节或特征，而常见模式学习指DNN学习数据分布的共同模式或泛化特征。在图[1(a)](#S1.F1.sf1
    "在图1 ‣ I 引言 ‣ 深度学习中的记忆：综述")中，我们使用大型语言模型来说明记忆学习和模式学习。我们用“泛化”一词来定义模型在新的、未见过的数据上的表现。如果没有额外的解释，所有术语如“记忆”、“记忆效应”和“记忆现象”都指记忆学习。此外，我们认为模式学习和记忆学习共同构成了DNN的学习路径。
- en: '![Refer to caption](img/b81d839cb98b9b1107badd97640856bb.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b81d839cb98b9b1107badd97640856bb.png)'
- en: 'Figure 2: Paper Structure.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：论文结构。
- en: 'Moreover, memorization is a complex concept that requires us to consider it
    at various levels. In our opinion, memorization learning and pattern learning
    operate at a feature level. However, understanding the features of neural networks
    directly is exceedingly difficult for humans. Hence, we mainly study memorization
    at the example level and model level as illustrated in Figure  [1(b)](#S1.F1.sf2
    "In Figure 1 ‣ I Introduction ‣ Memorization in deep learning: A survey").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，记忆是一个复杂的概念，需要我们从不同层次进行考虑。在我们看来，记忆学习和模式学习是在特征层面上进行的。然而，直接理解神经网络的特征对人类来说极其困难。因此，我们主要研究示例层面和模型层面的记忆，如图 [1(b)](#S1.F1.sf2
    "在图1 ‣ I 引言 ‣ 深度学习中的记忆：综述")所示。
- en: 'Intuitively, example memorization and model memorization indicate the objects
    of study are examples and models. Consequently, memorization concepts at different
    levels inspire distinct memorization evaluation methods. Example memorization
    evaluation tries to ensure if an example is memorized including differential evaluation
    and probabilistic evaluation. On the other hand, model memorization evaluation
    measures how much models memorize or the memorization ability of models. We summarize
    various approaches to three main methods: noisy label evaluation, recurrence evaluation,
    and extraction evaluation.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上看，示例记忆和模型记忆指的是研究对象是示例和模型。因此，不同层次的记忆概念启发了不同的记忆评估方法。示例记忆评估试图确保一个示例是否被记住，包括差异评估和概率评估。另一方面，模型记忆评估则衡量模型记住了多少内容或模型的记忆能力。我们将各种方法总结为三种主要方法：噪声标签评估、重复评估和提取评估。
- en: After the definitions and evaluation methods, we systematically review related
    literature. For memorization behaviors in standard training, existing studies
    investigate the relationships between the memorization effect and training data,
    training stages, model architecture, overfitting, regularization, and other factors.
    One study [[13](#bib.bib13), [11](#bib.bib11)] provides an interesting conclusion
    that memorization learning improves the generalization of models because the memorization
    of rare and atypical examples actually contributes to the generalization performance
    of similar rare subgroups, which is adverse to some early opinions. Additionally,
    some evidence [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)] shows overfitting
    is not responsible for memorization. Memorization is a persistent process in training.
    For security/privacy risks, the memorized particular features become multiple
    risk sources like membership inference risks and extraction risks, enabling attackers
    to exploit the memorization mechanism to invade privacy and violate the security
    rules of DNNs. In contrast, some risks like adversary attack risks are not obviously
    related to the memorization mechanism.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义和评估方法之后，我们系统地回顾了相关文献。对于标准训练中的记忆行为，现有研究探讨了记忆效应与训练数据、训练阶段、模型架构、过拟合、正则化等因素之间的关系。一项研究 [[13](#bib.bib13),
    [11](#bib.bib11)] 提供了一个有趣的结论，即记忆学习提高了模型的泛化能力，因为对稀有和非典型示例的记忆实际上有助于类似稀有子组的泛化表现，这与一些早期观点相悖。此外，一些证据 [[14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)] 表明过拟合并不是记忆的原因。记忆是训练中的一个持久过程。对于安全/隐私风险，记住的特定特征成为多种风险来源，如成员推断风险和提取风险，使攻击者能够利用记忆机制入侵隐私并违反深度神经网络的安全规则。相比之下，一些风险如对抗攻击风险与记忆机制的关系并不明显。
- en: On a related aspect, the forgetting phenomenon is closely connected to the memorization
    effect. Thus, we also discuss and review the forgetting effect. We explore useful
    forgetting definitions and evaluation methods and summarize relevant forgetting
    phenomenon studies.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在相关方面，遗忘现象与记忆效应密切相关。因此，我们还讨论和回顾了遗忘效应。我们探讨了有用的遗忘定义和评估方法，并总结了相关的遗忘现象研究。
- en: Additionally, we also review numerous applications utilizing the memorization
    and forgetting mechanisms. These applications like noisy label learning, example
    enhancement, privacy audit and protection, memorization architecture, and model
    editing, take advantage of different properties of memorization.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还回顾了众多利用记忆和遗忘机制的应用。这些应用如噪声标签学习、示例增强、隐私审计和保护、记忆架构和模型编辑，利用了记忆的不同特性。
- en: 'In summary, we attempt to organize the memorization definitions and evaluation
    methods and review relevant literature, aiming to build a scientific and effective
    framework and help the readers understand the memorization mechanism and its influence
    on model training and system security. Additionally, we also explore the forgetting
    phenomenon and illustrate some potential applications of the memorization and
    forgetting mechanisms. We hope this survey can help the research community have
    a general understanding of the memorization phenomenon. The key contributions
    of this survey are summarized as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们尝试组织记忆定义和评估方法，并回顾相关文献，旨在建立一个科学有效的框架，帮助读者理解记忆机制及其对模型训练和系统安全的影响。此外，我们还探讨了遗忘现象，并说明了记忆和遗忘机制的一些潜在应用。我们希望这项调查能够帮助研究社区对记忆现象有一个总体了解。该调查的主要贡献总结如下：
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Organizing definitions. We propose a framework to organize all existing memorization
    definitions and evaluation methods. We also explain the scope and limitations
    of these definitions and evaluation methods.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 组织定义。我们提出了一个框架来组织所有现有的记忆定义和评估方法。我们还解释了这些定义和评估方法的范围和局限性。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Comprehensive review. We review relevant memorization studies on its behaviors
    in the standard training and security/privacy risks. Moreover, we also investigate
    its connection with the forgetting studies and some possible applications.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合回顾。我们回顾了相关的记忆研究，关注其在标准训练和安全/隐私风险中的表现。此外，我们还调查了它与遗忘研究的联系以及一些可能的应用。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Discussion. In this survey, we thoroughly discuss the memorization mechanism
    and how memorization effects can boost other relevant technologies.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 讨论。在这项调查中，我们深入讨论了记忆机制以及记忆效果如何促进其他相关技术。
- en: 'The survey is organized into the following sections, each focusing on a different
    aspect of memorization in deep learning as we present in Figure [2](#S1.F2 "Figure
    2 ‣ I Introduction ‣ Memorization in deep learning: A survey"). Section [II](#S2
    "II Memorization Definition ‣ Memorization in deep learning: A survey") provides
    existing memorization definitions and Section [III](#S3 "III Memorization Evaluations
    ‣ Memorization in deep learning: A survey") lists the memorization evaluation
    methods based on various levels. Section [IV](#S4 "IV Memorization in DNN Training
    ‣ Memorization in deep learning: A survey") delves into the memorization behaviors,
    presenting how memorization affects each training component and its relationship
    with overfitting, data augmentation, and regularization technology. Section [V](#S5
    "V Underlying Risks of Memorization Learning ‣ Memorization in deep learning:
    A survey") presents a review of memorization-associated risks that memorized particular
    features enhance privacy risks. Section [VI](#S6 "VI Forgetting Research ‣ Memorization
    in deep learning: A survey") explores the forgetting phenomenon, which is the
    opposite of memorization. Section [VII](#S7 "VII Application ‣ Memorization in
    deep learning: A survey") demonstrates the underlying application of the memorization
    effects including the noisy label learning, example enhancement technology, privacy
    audit and protection, and memorization architecture. Section [VIII](#S8 "VIII
    Discussion and Future Research ‣ Memorization in deep learning: A survey") comprehensively
    discusses the memorization phenomenon’s influence on standard training and security/privacy
    risks and how it enlightens and explains other technologies or phenomena.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '该调查分为以下几个部分，每个部分关注深度学习中记忆的不同方面，如图 [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Memorization
    in deep learning: A survey")所示。第[II](#S2 "II Memorization Definition ‣ Memorization
    in deep learning: A survey")部分提供了现有的记忆定义，第[III](#S3 "III Memorization Evaluations
    ‣ Memorization in deep learning: A survey")部分列出了基于不同层级的记忆评估方法。第[IV](#S4 "IV Memorization
    in DNN Training ‣ Memorization in deep learning: A survey")部分深入探讨了记忆行为，展示了记忆如何影响每个训练组件以及与过拟合、数据增强和正则化技术的关系。第[V](#S5
    "V Underlying Risks of Memorization Learning ‣ Memorization in deep learning:
    A survey")部分回顾了记忆相关的风险，即记忆特定特征会增加隐私风险。第[VI](#S6 "VI Forgetting Research ‣ Memorization
    in deep learning: A survey")部分探讨了遗忘现象，它是记忆的对立面。第[VII](#S7 "VII Application ‣ Memorization
    in deep learning: A survey")部分展示了记忆效果的潜在应用，包括噪声标签学习、示例增强技术、隐私审计和保护以及记忆架构。第[VIII](#S8
    "VIII Discussion and Future Research ‣ Memorization in deep learning: A survey")部分全面讨论了记忆现象对标准训练和安全/隐私风险的影响，以及它如何启发和解释其他技术或现象。'
- en: '![Refer to caption](img/a4b281e9f4039711192e303a3b24c5ba.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a4b281e9f4039711192e303a3b24c5ba.png)'
- en: 'Figure 3: Memorization Definitions and Evaluations.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：记忆化定义和评估。
- en: 'TABLE I: Main Memorization Definitions'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：主要记忆化定义
- en: '| Domain | Name | Reference | Research Question | Description |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 名称 | 参考文献 | 研究问题 | 描述 |'
- en: '| Generalization | Label Memorization | Feldman et al. 2020 [[13](#bib.bib13),
    [11](#bib.bib11)] | Studying memorization effect of long-tailed examples. | This
    definition provides a universal understanding of memorization and distinguishes
    memorization learning and pattern learning effectively. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 泛化 | 标签记忆化 | Feldman等人 2020 [[13](#bib.bib13), [11](#bib.bib11)] | 研究长尾样本的记忆化效果。
    | 这个定义提供了对记忆化的通用理解，并有效区分了记忆学习和模式学习。 |'
- en: '|  | Exact Memorization | Tirumala et al. 2022 [[16](#bib.bib16)] | Studying
    underlying training and memorization dynamics of very large language models. |
    The exact memorization actually represents accuracy that cannot identify memorization
    learning in the language model. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | 精确记忆化 | Tirumala等人 2022 [[16](#bib.bib16)] | 研究非常大规模语言模型的基础训练和记忆化动态。 |
    精确记忆化实际上代表了无法在语言模型中识别记忆学习的准确性。 |'
- en: '|  | Counterfactual Memorization | Zhang et al. 2021 [[17](#bib.bib17)] | Studying
    counterfactual memorization in language models. | This concept extends label memorization
    to unsupervised tasks. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | 反事实记忆化 | Zhang等人 2021 [[17](#bib.bib17)] | 研究语言模型中的反事实记忆化。 | 这个概念将标签记忆化扩展到无监督任务。
    |'
- en: '|  | Benign Memorization | Anagnostidis et al. 2023 [[18](#bib.bib18)] | Studying
    learned features with data augmentation. | Benign memorization describes DNNs
    can learn useful features on the randomly labeled dataset with data augmentation
    technology. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | 良性记忆化 | Anagnostidis等人 2023 [[18](#bib.bib18)] | 研究数据增强下的学习特征。 | 良性记忆化描述了DNN可以在随机标记的数据集上使用数据增强技术学习有用的特征。
    |'
- en: '|  | Corrupt Label Memorization based on Neural Collapse | Nguyen et al. 2023 [[19](#bib.bib19)]
    | Studying how corrupt label data impacts neural collapse. | The definition attempts
    to explain the influence of corrupt label data in neural collapse. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于神经崩溃的标签腐败记忆化 | Nguyen等人 2023 [[19](#bib.bib19)] | 研究标签腐败数据如何影响神经崩溃。
    | 该定义尝试解释标签腐败数据在神经崩溃中的影响。 |'
- en: '| Security and Privacy | Unintended Memorization | Carlini et al. 2019 [[14](#bib.bib14)]
    | Studying unintended memorization in training. | Unintended memorization definites
    the features that are irrelevant to the main task but memorized by DNNs. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 安全与隐私 | 无意记忆化 | Carlini等人 2019 [[14](#bib.bib14)] | 研究训练中的无意记忆化。 | 无意记忆化定义了与主要任务无关但被DNN记忆的特征。
    |'
- en: '|  | $k$-Eidetic Memorization | Carlini et al. 2021 [[9](#bib.bib9)] | Studying
    privacy leakage in language models. | This memorization definition helps analyze
    the possibly memorized data based on repetition times. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $k$-超忆记忆化 | Carlini等人 2021 [[9](#bib.bib9)] | 研究语言模型中的隐私泄露。 | 这个记忆化定义有助于基于重复次数分析可能记忆的数据。
    |'
- en: II Memorization Definition
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 记忆化定义
- en: 'Memorization is a vague and abstract concept, and difficult to obverse during
    the training of neural networks. Thus, previous studies did not provide a clear
    and uniform definition. Based on relevant research, we find that the motivations
    for studying the memorization phenomenon are its impact on generalization and
    the concerns about privacy and security risks. In this section, we outline existing
    definitions of memorization within the contexts of the generalization domain and
    security domain as Table [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Memorization in
    deep learning: A survey") and Figure [3](#S1.F3 "Figure 3 ‣ I Introduction ‣ Memorization
    in deep learning: A survey").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '记忆化是一个模糊和抽象的概念，在神经网络训练过程中难以观察。因此，之前的研究未能提供明确且统一的定义。基于相关研究，我们发现研究记忆化现象的动机是其对泛化的影响以及对隐私和安全风险的关注。在本节中，我们概述了在泛化领域和安全领域内对记忆化的现有定义，如表[I](#S1.T1
    "TABLE I ‣ I Introduction ‣ Memorization in deep learning: A survey")和图[3](#S1.F3
    "Figure 3 ‣ I Introduction ‣ Memorization in deep learning: A survey")所示。'
- en: II-A Memorization Definitions in Generalization Domain
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 泛化领域中的记忆化定义
- en: II-A1 Label Memorization
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 标签记忆化
- en: Intuitively, there would exist an obvious disparity when evaluating a data point
    on a model between the model memorizing the data point and not. Feldman [[13](#bib.bib13)]
    introduces the label memorization concept to describe the disparity in supervised
    learning tasks. Label memorization differentially defines what memorizing a label
    of a point in the dataset means.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，当在模型上评估数据点时，记住数据点的模型和不记住数据点的模型之间会存在明显的差异。Feldman [[13](#bib.bib13)] 引入了标签记忆的概念，用于描述监督学习任务中的这种差异。标签记忆对数据集中一个点的标签记忆的定义有所不同。
- en: Definition 1
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1
- en: Label Memorization for Supervised Tasks. Given a training algorithm $A$ that
    maps a training dataset $D$ to a trained model $f$, the amount of memorization
    by $A$ on example $(x_{i},y_{i})\in D$ is defined as
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 监督任务中的标签记忆。给定一个训练算法 $A$，它将训练数据集 $D$ 映射到一个训练模型 $f$，$A$ 在示例 $(x_{i},y_{i})\in
    D$ 上的记忆量定义为
- en: '|  |  |  | (1) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (1) |'
- en: where $D^{\backslash i}$ denotes the dataset $D$ with $(x_{i},y_{i})$ removed.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D^{\backslash i}$ 表示从数据集 $D$ 中移除 $(x_{i},y_{i})$ 后的数据集。
- en: This definition provides a universal understanding of memorization and distinguishes
    generalized examples effectively. The definition actually approaches the nature
    of memorization that memorized examples cannot rely on generalized features.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这一定义提供了对记忆的普遍理解，并有效区分了广义示例。实际上，这一定义接近于记忆的本质，即被记忆的示例不能依赖于广义特征。
- en: II-A2 Exact Memorization
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 精确记忆
- en: Exact memorization proposed by Tirumala et al. [[16](#bib.bib16)], is used to
    perform a large-scale study of the dynamics of memorization over training. Additionally,
    this definition is only applied to the language models.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Tirumala 等人 [[16](#bib.bib16)] 提出的精确记忆，用于大规模研究训练过程中的记忆动态。此外，这一定义仅应用于语言模型。
- en: Definition 2
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2
- en: 'Exact Memorization. Let $V$ denote the vocabulary size. Let $C$ denote a set
    of contexts, which can be thought of as a list of tuples $(s,y)$ where $s$ is
    an input context (incomplete block of text) and $y$ is the index of the ground
    truth token in the vocabulary that completes the block of text. Let $S$ denote
    the set of input contexts, and let $f:S\rightarrow\mathbb{R}^{V}$ denote a language
    model. A context $c=(s,y)\in C$ is memorized if $\arg\max(f(s))=y$. For a given
    set of contexts $C$ (i.e., a given training dataset), the proportion of memorized
    contexts can be represented as:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 精确记忆。设 $V$ 为词汇表大小。设 $C$ 为上下文集合，可以将其视为一系列元组 $(s,y)$，其中 $s$ 是输入上下文（不完整的文本块），$y$
    是词汇表中完成文本块的真实标记的索引。设 $S$ 为输入上下文集合，$f:S\rightarrow\mathbb{R}^{V}$ 为语言模型。如果上下文 $c=(s,y)\in
    C$ 被记住，则有 $\arg\max(f(s))=y$。对于给定的上下文集合 $C$（即给定的训练数据集），记忆上下文的比例可以表示为：
- en: '|  | $\displaystyle\text{mem(f)}=\frac{\sum_{(s,y)\in C}1\{\arg\max(f(s))=y\}}{&#124;C&#124;}.$
    |  | (2) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{mem(f)}=\frac{\sum_{(s,y)\in C}1\{\arg\max(f(s))=y\}}{|C|}.$
    |  | (2) |'
- en: Based on the formula, we know that the exact memorization actually represents
    accuracy since it just measures the average number that predicted token matches
    the ground truth token in the contexts. Thus, this definition is not related to
    the memorization phenomenon and cannot describe the memorization.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 基于公式，我们知道精确记忆实际上表示准确率，因为它只是衡量预测标记与上下文中的真实标记匹配的平均次数。因此，这一定义与记忆现象无关，不能描述记忆现象。
- en: II-A3 Counterfactual Memorization
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 反事实记忆
- en: Counterfactual memorization is extended from label memorization to unsupervised
    tasks. Zhang et al. [[17](#bib.bib17)] introduce the definition, applying it to
    quantify the episodic memorization in language models.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实记忆从标签记忆扩展到无监督任务。Zhang 等人 [[17](#bib.bib17)] 介绍了这一定义，并将其应用于量化语言模型中的情景记忆。
- en: Definition 3
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3
- en: Counterfactual Memorization. Given a training algorithm $A$ that maps a training
    dataset $D$ to a trained model $f$, and a measure $M$ which measures the performance
    of $x_{i}$ on $f$, the amount of memorization by $A$ on example $x_{i}\in D$ measured
    with $M$ is defined as
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实记忆。给定一个训练算法 $A$，它将训练数据集 $D$ 映射到一个训练模型 $f$，以及一个测量 $M$，用来衡量 $x_{i}$ 在 $f$ 上的表现，那么
    $A$ 在示例 $x_{i}\in D$ 上的记忆量，由 $M$ 测量定义为
- en: '|  |  |  | (3) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (3) |'
- en: where $M$ can be per-token accuracy that $f$ predicts the next token based on
    the preceding tokens, then measures the 0-1 loss.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M$ 可以是每个标记的准确率，即 $f$ 根据前面的标记预测下一个标记，然后测量 0-1 损失。
- en: Basically, counterfactual memorization is a universal version of label memorization
    and this differential memorization definition can empirically evaluate memorization
    in various tasks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，对抗记忆是标签记忆的通用版本，这种差分记忆定义可以实证地评估各种任务中的记忆情况。
- en: II-A4 Benign Memorization
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A4 良性记忆
- en: Benign memorization describes the phenomenon that neural networks can learn
    useful features on the randomly labeled dataset with data augmentation technology [[18](#bib.bib18)].
    This work regards the general neural network structure as an encoder-projector
    pair and trains the pair on an augmented noisy dataset. If the accuracy of $k$NN
    probing at the embedding vectors of the encoder increases over probing at initialization,
    this is benign memorization.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 良性记忆描述了神经网络能够在随机标记的数据集上通过数据增强技术学习有用特征的现象 [[18](#bib.bib18)]。这项工作将通用神经网络结构视为编码器-投影器对，并在增强的噪声数据集上训练这一对。如果
    $k$NN 在编码器嵌入向量上的探测准确率相较于初始化时有所提高，则称之为良性记忆。
- en: Definition 4
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4
- en: Benign Memorization. Here are two datasets, $D:={(x_{i},y_{i})}^{n}_{i=1}$ denotes
    the original clean dataset and $\tilde{D}={(x_{i},\tilde{y}_{i})}^{n}_{i=1}$ its
    randomly labeled version. We call an encoder-projector pair $(h_{\phi*},g_{\psi*})$
    a memorization of $\tilde{D}$, if $f_{*}$ perfectly fits $\tilde{D}$. Moreover,
    we call $(h_{\phi*},g_{\psi*})$ a malign memorization if additionally, probing
    of $h_{\phi*}$ on $D$ does not improve over probing at initialization. On the
    contrary, we call $(h_{\phi*},g_{\psi*})$ a benign memorization of $\tilde{D}$
    if probing of $h_{\phi*}$ on $D$ outperforms probing at initialization.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 良性记忆。这里有两个数据集，$D:={(x_{i},y_{i})}^{n}_{i=1}$ 表示原始干净数据集，而 $\tilde{D}={(x_{i},\tilde{y}_{i})}^{n}_{i=1}$
    是其随机标记版本。如果 $f_{*}$ 完美地拟合了 $\tilde{D}$，我们称编码器-投影器对 $(h_{\phi*},g_{\psi*})$ 为 $\tilde{D}$
    的记忆。此外，如果在 $D$ 上对 $h_{\phi*}$ 的探测没有比初始化时更好，我们称 $(h_{\phi*},g_{\psi*})$ 为恶性记忆。相反，如果在
    $D$ 上对 $h_{\phi*}$ 的探测优于初始化探测，我们称 $(h_{\phi*},g_{\psi*})$ 为 $\tilde{D}$ 的良性记忆。
- en: This definition focuses on the generalization performance when training on randomly
    labeled datasets. Benign memorization occurs if the encoder learns generalized
    features. Therefore, this memorization definition is auxiliary to explain noisy
    label learning rather than a general memorization definition.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 该定义专注于在随机标记数据集上的训练时的泛化性能。如果编码器学习了通用特征，则发生良性记忆。因此，这一记忆定义是用于解释噪声标签学习的辅助定义，而不是通用记忆定义。
- en: II-A5 Corrupt Label Memorization based on Neural Collapse
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A5 基于神经崩溃的标签腐蚀记忆
- en: Empirical evidence indicates that the memorization of noisy data points may
    lead to degradation (dilation) of the neural collapse. Nguyen et al. [[19](#bib.bib19)]
    purpose memorization-dilation model and define memorization based on neural collapse
    under corrupt label training data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 实证证据表明，噪声数据点的记忆可能导致神经崩溃的退化（膨胀）。Nguyen 等人 [[19](#bib.bib19)] 提出了记忆-膨胀模型，并在腐蚀标签训练数据下定义了记忆。
- en: Definition 5
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 5
- en: Corrupt Label Memorization based on Neural Collapse. For a given and labeled
    dataset $D$ with label noise $\eta$ and $K$ categories, if $f$ is a feature extractor,
    denoting the feature representations $f(x_{i}^{k})$ of the example $x_{i}^{k}$
    by $h_{i}^{k}$. Under neural collapse, any $h_{i}^{k}$ will collapse to a single
    feature representation $h^{k}$. We denote the set of corrupted instances of class
    $k$ by $[\tilde{I}^{k}]$. Memorization can be defined as
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于神经崩溃的标签腐蚀记忆。对于给定的带标签数据集 $D$，其中包含标签噪声 $\eta$ 和 $K$ 个类别，如果 $f$ 是一个特征提取器，将样本
    $x_{i}^{k}$ 的特征表示 $f(x_{i}^{k})$ 表示为 $h_{i}^{k}$。在神经崩溃下，任何 $h_{i}^{k}$ 都会崩溃为单一特征表示
    $h^{k}$。我们用 $[\tilde{I}^{k}]$ 表示类别 $k$ 的被腐蚀实例集合。记忆可以定义为
- en: '|  | $\displaystyle\text{mem}:=\sum\limits_{k=1}^{K}\sum\limits_{i\in[\tilde{I}^{k}]}&#124;&#124;h_{i}^{k}-h_{*}^{k}&#124;&#124;$
    |  | (4) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{mem}:=\sum\limits_{k=1}^{K}\sum\limits_{i\in[\tilde{I}^{k}]}&#124;&#124;h_{i}^{k}-h_{*}^{k}&#124;&#124;$
    |  | (4) |'
- en: where $h_{*}^{k}$ denotes the mean of (unseen) test instances belonging to class
    $k$.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{*}^{k}$ 表示属于类别 $k$ 的（未见过的）测试实例的均值。
- en: The DNN under neural collapse intends to map examples with the same ground truth
    label to a single representation due to the similarity in input features. Therefore,
    instances of the same ground truth but with randomly corrupted labels lack predictable
    characteristics, making it challenging for the network to distinguish and separate
    them in a manner that can be generalized effectively. Thus, when the network is
    still able to successfully separate such instances, it indicates that the network
    has memorized the feature representations of the corrupted instances present in
    the training set. This definition expresses the memorization of noisy examples
    but only applies to the problem domain of neural collapse. The scope of the definition
    is limited.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经崩溃下，DNN意图将具有相同真实标签的示例映射到单一表示，因为输入特征相似。因此，具有相同真实标签但标签随机损坏的实例缺乏可预测特征，使得网络难以以有效的方式区分和分离它们。因此，当网络仍能成功分离这些实例时，说明网络已经记住了训练集中存在的损坏实例的特征表示。这个定义表达了对噪声示例的记忆，但仅适用于神经崩溃的问题领域。定义的范围有限。
- en: II-B Memorization Definitions in Security Domain
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 安全领域中的记忆定义
- en: II-B1 Unintended Memorization
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 无意记忆
- en: Unintended memorization mainly serves to privacy concerns. The concept was first
    proposed by Carlini et al. [[14](#bib.bib14)] when they found that LLMs may memorize
    some sensitive information like social-security numbers unintentionally. Generally,
    such memorization is unnecessary for achieving generalization and they give a
    simple unintended memorization definition.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 无意记忆主要与隐私问题相关。这个概念最早由Carlini等人提出[[14](#bib.bib14)]，他们发现LLMs可能会无意中记住一些敏感信息，如社会保障号码。通常，这种记忆对实现泛化没有必要，他们给出了一个简单的无意记忆定义。
- en: Definition 6
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义6
- en: Unintended Memorization. Unintended memorization occurs when trained neural
    networks may expose the presence of out-of-distribution training data and the
    training data is irrelevant to the learning task and definitely does not contribute
    to improving model accuracy.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 无意记忆。当训练的神经网络可能暴露出分布外训练数据的存在时，且这些训练数据与学习任务无关，显然不会有助于提高模型准确性时，就会发生无意记忆。
- en: Compared to the differential memorization definitions, the unintended definition
    focuses specifically on the memorization of out-of-distribution and sensitive
    data. These data can also be considered as secrets, as they should not be revealed
    or disclosed by the trained neural networks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 与差异记忆定义相比，无意记忆定义特别关注分布外和敏感数据的记忆。这些数据也可以被视为秘密，因为它们不应被训练的神经网络揭示或泄露。
- en: II-B2 $k$-Eidetic Memorization
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 $k$-Eidetic记忆
- en: Carlini et al. [[9](#bib.bib9)] introduces the concept of $k$-Eidetic Memorization
    for language tasks. The parameter $k$ represents the count of distinct training
    examples that contain a specific string.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Carlini等人[[9](#bib.bib9)]为语言任务引入了$k$- Eidetic记忆的概念。参数$k$表示包含特定字符串的不同训练示例的数量。
- en: Definition 7
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义7
- en: $k$-Eidetic Memorization. A string $s$ is $k$-eidetic memorized (for $k\geq
    1$) by an LM $f_{\theta}$ if $s$ appears in at most $k$ examples in the training
    data $X:|{x\in X:s\subseteq x}|\leq k$ and the $s$ is extractable from the LM
    $f_{\theta}$ with a prefix $c$ which satisfies
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: $k$-Eidetic记忆。如果字符串$s$在训练数据$X$中出现次数不超过$k$（$k\geq 1$），且$s$可以从LM $f_{\theta}$中提取，前缀$c$满足
- en: '|  | $\displaystyle s\leftarrow\arg\max\limits_{s^{\prime}:&#124;s^{\prime}&#124;=N}f_{\theta}(s^{\prime}&#124;c)$
    |  | (5) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s\leftarrow\arg\max\limits_{s^{\prime}:&#124;s^{\prime}&#124;=N}f_{\theta}(s^{\prime}&#124;c)$
    |  | (5) |'
- en: where $f_{\theta}(s^{\prime}|c)$ is the likelihood of an entire sequence $s^{\prime}$
    with length $N$.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$f_{\theta}(s^{\prime}|c)$是长度为$N$的整个序列$s^{\prime}$的可能性。
- en: This memorization definition helps figure out the possibly memorized strings
    based on repetition times in LM. If $k$ is large, the memorized string may be
    common knowledge like the zip code of a particular city. But when $k$ is very
    small, the memorized string could be harmful like accidentally exposing a personal
    phone number. The $k$-Eidetic Memorization is also concerned with privacy but
    utilizes the repetition times as a parameter to identify common knowledge memorization
    and harmful unintended memorization in language tasks.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个记忆定义帮助确定基于LM中重复次数的可能记忆字符串。如果$k$很大，记忆字符串可能是像特定城市的邮政编码这样的常识。但当$k$非常小时，记忆字符串可能会有害，比如意外暴露个人电话号码。$k$-eidetic记忆也关注隐私，但利用重复次数作为参数来识别语言任务中的常识记忆和有害的非故意记忆。
- en: III Memorization Evaluations
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 记忆评估
- en: 'Memorization evaluation basically follows different memorization levels to
    identify the existence of memorization and its influence. Figure [3](#S1.F3 "Figure
    3 ‣ I Introduction ‣ Memorization in deep learning: A survey") demonstrates the
    methods of memorization evaluation and associated memorization definitions.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '记忆评估基本上遵循不同的记忆水平来识别记忆的存在及其影响。图[3](#S1.F3 "Figure 3 ‣ I Introduction ‣ Memorization
    in deep learning: A survey")展示了记忆评估的方法和相关的记忆定义。'
- en: III-A Example Memorization Evaluation
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 示例记忆评估
- en: Example memorization focuses on individual example memorization, which means
    checking if any example has been memorized by neural networks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 示例记忆集中在单个示例的记忆上，即检查神经网络是否记住了任何示例。
- en: III-A1 Differential Memorization Evaluation
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 差分记忆评估
- en: 'When example memorization happens, the model’s outputs on the memorized data
    between the model training with it and without it will produce a large gap. This
    is ’leave-one-out’ memorization or differential memorization that we present in
    Defintion [1](#Thmdefinition1 "Definition 1 ‣ II-A1 Label Memorization ‣ II-A
    Memorization Definitions in Generalization Domain ‣ II Memorization Definition
    ‣ Memorization in deep learning: A survey") and Definition [3](#Thmdefinition3
    "Definition 3 ‣ II-A3 Counterfactual Memorization ‣ II-A Memorization Definitions
    in Generalization Domain ‣ II Memorization Definition ‣ Memorization in deep learning:
    A survey"). The measurement can be called the memorization score and the formula
    is the same as the definitions.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '当示例记忆发生时，模型在记忆数据上的输出在模型训练有此数据和没有此数据之间会产生较大差距。这就是我们在定义[1](#Thmdefinition1 "Definition
    1 ‣ II-A1 Label Memorization ‣ II-A Memorization Definitions in Generalization
    Domain ‣ II Memorization Definition ‣ Memorization in deep learning: A survey")和定义[3](#Thmdefinition3
    "Definition 3 ‣ II-A3 Counterfactual Memorization ‣ II-A Memorization Definitions
    in Generalization Domain ‣ II Memorization Definition ‣ Memorization in deep learning:
    A survey")中呈现的“留一法”记忆或差分记忆。该测量可以称为记忆分数，公式与定义相同。'
- en: The memorization score for supervised tasks is
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 监督任务的记忆分数是
- en: '|  |  |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: and the memorization score for unsupervised tasks is
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督任务的记忆分数是
- en: '|  |  |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: The memorization score quantifies the performance gap on a single example when
    the example is included and excluded from the training dataset. A notably large
    performance gap indicates that other examples cannot provide useful features for
    the data example and the model has to memorize it. Additionally, this measurement
    may require more computation resources. Jiang et al. [[20](#bib.bib20)] provide
    a simplified method to use multiple large subsets to replace the held-out dataset
    which saves the evaluation cost.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆分数量化了在训练数据集中包含和不包含某个示例时的性能差距。显著大的性能差距表明其他示例无法为数据示例提供有用特征，模型必须记住它。此外，这种测量可能需要更多的计算资源。姜等人[[20](#bib.bib20)]提供了一种简化的方法，使用多个大的子集来替代保留的数据集，从而节省评估成本。
- en: III-A2 Probabilistic Memorization Evaluation
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 概率记忆评估
- en: Probabilistic memorization depends on the differences in model outputs of memorized
    and generalized examples. There may exist multiple techniques to capture the differences
    but the most relevant method is the membership inference attack.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 概率记忆依赖于记忆和泛化示例的模型输出差异。可能存在多种技术来捕捉这些差异，但最相关的方法是成员推断攻击。
- en: This kind of attack aims to determine whether a data point belongs to the training
    dataset. The success of the attack cannot rely on the generalized feature of examples
    because these features are common for the entire data distribution. Therefore,
    the membership inference attack focuses on the particular or unique features that
    models memorize. In other words, data points that the model has memorized during
    training are more likely to be correctly identified as belonging to the training
    dataset in membership inference attacks. Though there is no obvious quantitative
    research to prove such a relationship and no formal definition, some works [[21](#bib.bib21),
    [22](#bib.bib22)] tacitly approve the relationship and adopt membership inference
    attack to measure memorization. Thus, it is possible to use membership inference
    attacks to evaluate model memorization indirectly. It is noted that some membership
    inference methods have high false positive rates which cannot exactly measure
    memorization [[23](#bib.bib23), [24](#bib.bib24)].
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这种攻击旨在确定一个数据点是否属于训练数据集。攻击的成功不能依赖于样本的泛化特征，因为这些特征对整个数据分布来说是普遍存在的。因此，成员推断攻击关注模型记住的特定或独特特征。换句话说，在成员推断攻击中，模型在训练过程中记住的数据点更可能被正确识别为属于训练数据集。尽管没有明显的定量研究来证明这种关系，也没有正式的定义，但一些工作[[21](#bib.bib21),
    [22](#bib.bib22)] 默许了这种关系，并采用成员推断攻击来衡量记忆。因此，可能利用成员推断攻击间接评估模型记忆。需要注意的是，一些成员推断方法具有较高的假阳性率，这不能准确测量记忆[[23](#bib.bib23),
    [24](#bib.bib24)]。
- en: 'The typical work is Likelihood Ratio Attack (LiRA) [[24](#bib.bib24)]. The
    core idea behind LiRA is similar to Definition [1](#Thmdefinition1 "Definition
    1 ‣ II-A1 Label Memorization ‣ II-A Memorization Definitions in Generalization
    Domain ‣ II Memorization Definition ‣ Memorization in deep learning: A survey"),
    which involves evaluating membership inference risks by leveraging likelihood
    ratios. LiRA aims to assess whether a given data point is a member of the training
    dataset by computing the likelihood ratio based on the model’s predictions of
    the data point when the training dataset includes and excludes it. The original
    formula can be demonstrated as'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '典型的工作是**似然比攻击**（LiRA）[[24](#bib.bib24)]。LiRA 的核心思想类似于定义 [1](#Thmdefinition1
    "Definition 1 ‣ II-A1 Label Memorization ‣ II-A Memorization Definitions in Generalization
    Domain ‣ II Memorization Definition ‣ Memorization in deep learning: A survey")，即通过利用似然比评估成员推断风险。LiRA
    的目的是通过计算似然比，来评估给定数据点是否属于训练数据集，这个计算基于数据点在训练数据集包含和排除情况下的模型预测。原始公式可以表示为'
- en: '|  | $\Lambda(f,(x_{i},y_{i}))=\frac{p(f&#124;\mathbb{Q}_{in}(x_{i},y_{i}))}{p(f&#124;\mathbb{Q}_{out}(x_{i},y_{i}))}$
    |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Lambda(f,(x_{i},y_{i}))=\frac{p(f\mid \mathbb{Q}_{in}(x_{i},y_{i}))}{p(f\mid
    \mathbb{Q}_{out}(x_{i},y_{i}))}$ |  |'
- en: where $\mathbb{Q}_{in}(x_{i},y_{i})$ and $\mathbb{Q}_{out}(x_{i},y_{i})$ represents
    the distribution of models trained on the training dataset with and without the
    data point $(x_{i},y_{i})$ and $p$ is the probability density function over $f$
    under the distribution of model parameters $\mathbb{Q}$. The similarity in the
    core idea highlights the connection between membership inference risks and memorization
    evaluation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{Q}_{in}(x_{i},y_{i})$ 和 $\mathbb{Q}_{out}(x_{i},y_{i})$ 分别表示在训练数据集中包含和不包含数据点
    $(x_{i},y_{i})$ 时模型的分布，$p$ 是在模型参数分布 $\mathbb{Q}$ 下的 $f$ 的概率密度函数。核心思想的相似性突显了成员推断风险和记忆评估之间的联系。
- en: Moreover, there may exist other techniques based on probability that can be
    used to estimate memorization. Nevertheless, relevant techniques need careful
    validation and confirmation that they can really reflect the memorization effect.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还可能存在其他基于概率的技术可以用来估计记忆。然而，相关技术需要经过仔细验证和确认，确保它们确实能反映记忆效果。
- en: III-B Example Memorization Influence Score
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 示例记忆影响评分
- en: The influence score represents how an individual memorized example impacts model
    generalization performance.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 影响评分表示一个单独记忆的样本对模型泛化性能的影响。
- en: Now, we know some methods to evaluate example memorization but we are also curious
    about how memorized examples influence model generalization. To measure the impact
    of an individual memorized example on generalization, the influence score based
    on the memorization score has been proposed [[11](#bib.bib11)]. Generally, the
    influence score of a training example $(x_{i},y_{i})$ against a test example $(x_{j}^{\prime},y_{j}^{\prime})$
    under a supervised task can be defined as
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道一些评估例子记忆的方法，但我们也好奇被记忆的例子如何影响模型的泛化。为了测量单个记忆例子对泛化的影响，基于记忆分数的影响分数已被提出 [[11](#bib.bib11)]。通常，在监督任务下，训练例子
    $(x_{i},y_{i})$ 对测试例子 $(x_{j}^{\prime},y_{j}^{\prime})$ 的影响分数可以定义为
- en: '|  |  |  | (6) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (6) |'
- en: Similarly, the influence score for unsupervised tasks [[17](#bib.bib17)] can
    be defined as
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，无监督任务的影响分数 [[17](#bib.bib17)] 可以定义为
- en: '|  |  |  | (7) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (7) |'
- en: In the corresponding work [[11](#bib.bib11), [17](#bib.bib17)], they find a
    direct positive correlation between memorization scores and influence scores,
    and these examples are almost atypical. This observation proves rare and memorized
    examples provide particular features for their subpopulation generalization. Moreover,
    it’s worth noting that not all memorized examples contribute high influence scores
    because some memorized examples can be regarded as noisy examples and some are
    so rare that even no test examples belong to the corresponding subpopulation.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在相关工作 [[11](#bib.bib11), [17](#bib.bib17)]中，他们发现记忆分数和影响分数之间存在直接的正相关，这些例子几乎是非典型的。这一观察结果证明了稀有且被记忆的例子为其子群体泛化提供了特定的特征。此外，值得注意的是，并非所有被记忆的例子都会产生高影响分数，因为一些被记忆的例子可以被视为噪声例子，还有一些稀有到没有测试例子属于对应的子群体。
- en: '![Refer to caption](img/e2b5f86fb96c48fd43a2fb4c4545ba21.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e2b5f86fb96c48fd43a2fb4c4545ba21.png)'
- en: 'Figure 4: Demonstration of the Long-tailed Examples.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：长尾例子的演示。
- en: III-C Model Memorization Evaluation
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 模型记忆评估
- en: Model memorization cares about the role of neural networks, concerning how much
    memorization exists in models and the memorization capacity and ability of models.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 模型记忆关注神经网络的角色，涉及模型中存在多少记忆以及模型的记忆能力和能力。
- en: III-C1 Noisy Label Memorization Evaluation
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 噪声标签记忆评估
- en: Noisy label memorization evaluation actually is not used to measure model memorization
    but is a valuable method to build memorization baselines compared to other properties
    of the model. Depending on the fact that noisy label examples have no shared class-level
    features and patterns, the model has to memorize all of these noisy label examples.
    Thus, many works utilize noisy label examples as known memorization. Arpit et
    al. [[25](#bib.bib25)] mix the noisy label examples with normal examples to study
    the learning dynamics during training. They use the ratio of noisy label examples
    in the training dataset to represent memorization. Another work [[26](#bib.bib26)]
    is studying the memorization effect in adversarial training, utilizing the randomly
    labeled adversarial examples. Additionally, Maini et al. [[15](#bib.bib15)] attempt
    to employ noisy label examples to localize the memorization in the neural network.
    Hence, noisy label memorization evaluation is a common method to investigate the
    relationships between memorization and other factors.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声标签记忆评估实际上不是用来测量模型记忆的，而是用来建立记忆基线的一种有价值的方法，相对于模型的其他特性。由于噪声标签例子没有共享的类级特征和模式，模型必须记忆所有这些噪声标签例子。因此，许多工作利用噪声标签例子作为已知的记忆。Arpit
    等人 [[25](#bib.bib25)] 将噪声标签例子与正常例子混合，以研究训练过程中的学习动态。他们使用训练数据集中噪声标签例子的比例来表示记忆。另一项工作 [[26](#bib.bib26)]
    正在研究对抗训练中的记忆效果，利用随机标记的对抗例子。此外，Maini 等人 [[15](#bib.bib15)] 尝试利用噪声标签例子来定位神经网络中的记忆。因此，噪声标签记忆评估是一种常见的方法，用于研究记忆与其他因素之间的关系。
- en: III-C2 Recurrence Memorization Evaluation
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 复发记忆评估
- en: Recurrence memorization evaluation refers to the probability that neural networks
    can generate or extract specific marked examples put in the training dataset to
    measure the memorization tendency and ability of the model. Obviously, the selection
    of marked examples mainly affects the memorization evaluation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 复发记忆评估指的是神经网络能够生成或提取训练数据集中标记的特定例子的概率，以测量模型的记忆倾向和能力。显然，标记例子的选择主要影响记忆评估。
- en: 'Carlini et al. [[14](#bib.bib14)] employ random sequences to evaluate unintended
    memorization (Definition [6](#Thmdefinition6 "Definition 6 ‣ II-B1 Unintended
    Memorization ‣ II-B Memorization Definitions in Security Domain ‣ II Memorization
    Definition ‣ Memorization in deep learning: A survey")) in language models depending
    on this evaluation method. Specifically, they build the canary sequences which
    consist of two parts. The first part is like "the random number is" and the second
    part is just random numbers. Consequently, they create a metric called exposure
    index based on the log-perplexity,'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 'Carlini 等人 [[14](#bib.bib14)] 使用随机序列来评估语言模型中非预期的记忆（定义 [6](#Thmdefinition6 "Definition
    6 ‣ II-B1 Unintended Memorization ‣ II-B Memorization Definitions in Security
    Domain ‣ II Memorization Definition ‣ Memorization in deep learning: A survey")），具体是依赖于此评估方法。他们构建了由两部分组成的金丝雀序列。第一部分类似于“随机数是”，第二部分则是随机数字。因此，他们基于对数困惑度创建了一种称为曝光指数的度量。'
- en: '|  | $\mathbf{Px}_{f}(x_{1},\cdots,x_{n})=\sum_{i=1}^{N}(-\log_{2}\mathbf{Pr}(x_{i}&#124;f(x_{1},\cdots,x_{i-1}))),$
    |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{Px}_{f}(x_{1},\cdots,x_{n})=\sum_{i=1}^{N}(-\log_{2}\mathbf{Pr}(x_{i}\mid
    f(x_{1},\cdots,x_{i-1}))),$ |  |'
- en: where $f$ is the language model, and $x_{1},\cdots,x_{n}$ represents the input
    sequence. The perplexity measures how “surprised” the model is to see a given
    value. A higher perplexity indicates the model is “more surprised” by the sequence.
    Therefore, the exposure index measures the likelihood of data sequences. The evaluation
    follows confirming the canary sequence inserted into the training dataset, training,
    and then applying the exposure index to gain the probability of the canary sequence
    reproduction. The exposure index of the canary sequence may represent the model
    memorization ability.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f$ 是语言模型，而 $x_{1},\cdots,x_{n}$ 代表输入序列。困惑度衡量模型对给定值的“惊讶”程度。较高的困惑度表明模型对该序列的“惊讶”程度更高。因此，曝光指数衡量数据序列的可能性。评估过程包括确认插入训练数据集的金丝雀序列，进行训练，然后应用曝光指数以获取金丝雀序列的重现概率。金丝雀序列的曝光指数可能代表模型的记忆能力。
- en: Additionally, employing other types of examples like atypical examples instead
    of random examples may disclose other properties of model memorization. This requires
    further studies.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，采用其他类型的示例，如非典型示例，而非随机示例，可能会揭示模型记忆的其他特性。这需要进一步研究。
- en: III-C3 Extraction Memorization Evaluation
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C3 提取记忆评估
- en: Extraction memorization evaluation applies the extraction or inversion approaches
    to empirically evaluate the model memorization by producing all extractable examples
    and identifying those in the training dataset. This method attempts to provide
    a lower bound of model memorization. However, it is noted that not all extracted
    examples are identical to corresponding training examples because memorization
    works on the feature scale. Some extractable examples can be regarded as the representatives
    of generalized examples. It requires good metrics to ensure extractable examples
    are really memorized.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 提取记忆评估采用提取或反转方法，通过生成所有可提取的示例并识别训练数据集中的示例来实证评估模型的记忆。这种方法试图提供模型记忆的下界。然而，需要注意的是，并非所有提取的示例都与相应的训练示例完全相同，因为记忆是在特征尺度上进行的。一些可提取的示例可以被视为泛化示例的代表。这需要良好的度量标准来确保可提取示例确实被记忆。
- en: 'An effective work [[9](#bib.bib9)] applying this method attempts to extract
    training data from large language models and find examples with small $k$ in $k$-Eidetic
    Memorization (Definition [7](#Thmdefinition7 "Definition 7 ‣ II-B2 𝑘-Eidetic Memorization
    ‣ II-B Memorization Definitions in Security Domain ‣ II Memorization Definition
    ‣ Memorization in deep learning: A survey")). They generate a lot of text with
    GPT-2 and pick text with the highest memorization probability, validating the
    memorization on picked text manually via Internet search.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '一项有效的工作 [[9](#bib.bib9)] 应用此方法试图从大型语言模型中提取训练数据，并找到 $k$-Eidetic Memorization（定义 [7](#Thmdefinition7
    "Definition 7 ‣ II-B2 𝑘-Eidetic Memorization ‣ II-B Memorization Definitions in
    Security Domain ‣ II Memorization Definition ‣ Memorization in deep learning:
    A survey")）中 $k$ 较小的示例。它们使用 GPT-2 生成大量文本，并挑选出记忆概率最高的文本，通过互联网搜索手动验证挑选文本的记忆情况。'
- en: This evaluation method may require more resources but perhaps additionally assist
    researchers in understanding what models memorize.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这种评估方法可能需要更多资源，但或许能额外帮助研究人员理解模型记住了什么。
- en: IV Memorization in DNN Training
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 深度神经网络训练中的记忆
- en: 'A primary motivation behind research on memorization is to explore its impact
    and what role it plays in DNN training. In this section, we will provide a comprehensive
    review of memorization research in the DNN training framework. Table [II](#S4.T2
    "TABLE II ‣ IV Memorization in DNN Training ‣ Memorization in deep learning: A
    survey") demonstrates the main relevant works in this area.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '研究记忆的主要动机是探索其影响及其在深度神经网络（DNN）训练中扮演的角色。在本节中，我们将对DNN训练框架中的记忆研究进行全面回顾。表[II](#S4.T2
    "TABLE II ‣ IV Memorization in DNN Training ‣ Memorization in deep learning: A
    survey")展示了该领域的主要相关工作。'
- en: 'TABLE II: Main Works about Memorization in DNN Training'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：DNN训练中记忆的主要工作
- en: '| Main Topic | Reference | Background Task | Main Eval. Method | Main Findings
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 主要话题 | 参考文献 | 背景任务 | 主要评估方法 | 主要发现 |'
- en: '| Memorization Mechanism | Zhang et al., 2017 [[8](#bib.bib8)] | Supervised
    |  |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 记忆机制 | Zhang等, 2017 [[8](#bib.bib8)] | 有监督 |  |  |'
- en: '| Classification Task | Noisy Label Memorization Evaluation | DNNs can memorize
    randomly labeled datasets that traditional approaches fail to explain generalization.
    |  |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 分类任务 | 噪声标签记忆评估 | DNN可以记住随机标记的数据集，而传统方法无法解释其泛化能力。 |  |  |'
- en: '| Memorization about Data | Feldmen et al., 2020 [[13](#bib.bib13), [11](#bib.bib11)]
    | Supervised |  |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 关于数据的记忆 | Feldmen等, 2020 [[13](#bib.bib13), [11](#bib.bib11)] | 有监督 |  |  |'
- en: '| Classification Task | Differential Memorization Evaluation | Propose the
    long tail theory that memorization of long-tailed examples is crucial for achieving
    close-to-optimal generalization error. |  |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 分类任务 | 差分记忆评估 | 提出了长尾理论，认为对长尾示例的记忆对实现接近最优的泛化误差至关重要。 |  |  |'
- en: '|  | Hacohen et al., 2020 [[27](#bib.bib27)] | Various Tasks | / | Different
    neural networks memorize data in different orders. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | Hacohen等, 2020 [[27](#bib.bib27)] | 各种任务 | / | 不同的神经网络以不同的顺序记忆数据。 |'
- en: '|  | Zhang et al., 2021 [[17](#bib.bib17)] | Unsupervised Language |  |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | Zhang等, 2021 [[17](#bib.bib17)] | 无监督语言 |  |  |'
- en: '| Generative Task | Differential emorization Evaluation | High memorization
    examples are generally unconventional texts. |  |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 生成任务 | 差分记忆评估 | 高记忆示例通常是非传统文本。 |  |  |'
- en: '|  | Lee et al., 2022 [[28](#bib.bib28)] | Unsupervised Language |  |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | Lee等, 2022 [[28](#bib.bib28)] | 无监督语言 |  |  |'
- en: '| Generative Task | Probabilistic Memorization Evaluation | Deduplicated datasets
    make less memorization. |  |  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 生成任务 | 概率记忆评估 | 去重数据集的记忆较少。 |  |  |'
- en: '|  | Carlini et al., 2023 [[29](#bib.bib29)] | Unsupervised Language |  |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | Carlini等, 2023 [[29](#bib.bib29)] | 无监督语言 |  |  |'
- en: '| Generative Task | Extraction Memorization Evaluation | Repeated examples
    have a high probability of being extracted. |  |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 生成任务 | 提取记忆评估 | 重复示例有较高的被提取概率。 |  |  |'
- en: '| Memorzation about Training Stage | Arpit et al., 2018 [[25](#bib.bib25)]
    | Supervised |  |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 关于训练阶段的记忆 | Arpit等, 2018 [[25](#bib.bib25)] | 有监督 |  |  |'
- en: '| Classification Task | Noisy Label Memorization Evaluation | Learning simple
    patterns is prior to remembering noise data in the early training stage. |  |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 分类任务 | 噪声标签记忆评估 | 学习简单模式优先于记住噪声数据，特别是在早期训练阶段。 |  |  |'
- en: '|  | Maennel et al., 2020 [[30](#bib.bib30)] | Supervised |  |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | Maennel等, 2020 [[30](#bib.bib30)] | 有监督 |  |  |'
- en: '| Classification Task | Noisy Label Memorization Evaluation | An alignment
    between the principal components of network parameters and data takes place when
    training with random labels in the early training stage. |  |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 分类任务 | 噪声标签记忆评估 | 在早期训练阶段，当用随机标签进行训练时，网络参数和数据的主成分之间存在对齐。 |  |  |'
- en: '| Memorization about Architecture | Stephenson et al., 2021 [[31](#bib.bib31)]
    | Supervised |  |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 关于架构的记忆 | Stephenson等, 2021 [[31](#bib.bib31)] | 有监督 |  |  |'
- en: '| Classification Task | Mean Field |  |  |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 分类任务 | 平均场 |  |  |  |'
- en: '| Theoretic Geometric Analysis | Memorization predominately occurs in the deeper
    layers. |  |  |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 理论几何分析 | 记忆主要发生在更深层次。 |  |  |  |'
- en: '|  | Maini et al., 2023 [[15](#bib.bib15)] | Supervised |  |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | Maini等, 2023 [[15](#bib.bib15)] | 有监督 |  |  |'
- en: '| Classification Task | Noisy Label Memorization Evaluation | Memorization
    exists in a small set of neurons in various layers of the model. |  |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 分类任务 | 噪声标签记忆评估 | 记忆存在于模型的不同层中的少量神经元中。 |  |  |'
- en: '|  | Geva et al., 2021 [[32](#bib.bib32)] | Unsupervised Language Generative
    Task | / | Feed-forward layers in Transformer are key-value memories. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | Geva等, 2021 [[32](#bib.bib32)] | 无监督语言生成任务 | / | Transformer中的前馈层是键值记忆。
    |'
- en: '| Memorization about Overfitting | Tirumala et al., 2022 [[16](#bib.bib16)]
    | Unsupervised Language |  |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 过拟合的记忆 | Tirumala 等人, 2022 [[16](#bib.bib16)] | 无监督语言 |  |  |'
- en: '| Generative Task | Exact Memorization | Larger models can memorize a larger
    portion of the data before over-fitting |  |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 生成任务 | 精确记忆 | 更大的模型可以在过拟合之前记住更多的数据 |  |  |'
- en: '| Memorization about DA and Regularization | Anagnostidis et al., 2023 [[18](#bib.bib18)]
    | Supervised |  |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| DA 和正则化的记忆 | Anagnostidis 等人, 2023 [[18](#bib.bib18)] | 监督学习 |  |  |'
- en: '| Classification Task | $k$NN Probe | Even randomly labeled datasets with DA
    could lead to highly useful features. |  |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 分类任务 | $k$NN 探测 | 即使是具有 DA 的随机标记数据集也可能导致非常有用的特征。 |  |  |'
- en: '|  | Li et al., 2023 [[22](#bib.bib22)] | Supervised |  |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | Li 等人, 2023 [[22](#bib.bib22)] | 监督学习 |  |  |'
- en: '| Classification Task | Probabilistic Memorization Evaluation | Trivial data
    augmentation technologies can mitigate memorization. |  |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 分类任务 | 概率记忆评估 | 微不足道的数据增强技术可以减轻记忆。 |  |  |'
- en: IV-A Exploring Memorization Mechanism
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 探索记忆机制
- en: 'In some early studies [[33](#bib.bib33)], researchers believed that the memorization
    effect was not necessary for learning. Generally, the generalization of DNNs means
    networks can learn and recognize common patterns hidden within the input data.
    These common patterns consist of shared features among similar data examples.
    When DNNs learn these common patterns, they exhibit the ability to generalize,
    thereby demonstrating their capacity to perform well on new, unseen data beyond
    the training dataset. In contrast, memorization means networks memorize input
    examples rather than patterns which results in overfitting. However, as Zhang
    et al. [[8](#bib.bib8)] find that DNNs can easily fit the random labeled dataset,
    the traditional statistical learning theory like VC dimension [[34](#bib.bib34)],
    Rademacher complexity [[35](#bib.bib35)], and uniform stability [[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38)] cannot explain the generalization of DNNs.
    It is well known that DNNs cannot correctly classify the randomly labeled dataset
    based on the common patterns of the data distribution, thus DNNs have to memorize
    the entire dataset. However, the memorization mechanism in the DNNs remains unclear
    and vague. Therefore, two important and interesting questions arise: why DNNs
    memorize data in the standard training process, and how the memorization mechanism
    operates. This attracts the machine learning community to explore the memorization
    effect.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些早期研究中[[33](#bib.bib33)]，研究人员认为记忆效应对学习并非必要。通常，DNNs 的泛化能力意味着网络可以学习并识别输入数据中隐藏的常见模式。这些常见模式由相似数据示例之间的共享特征组成。当
    DNNs 学习这些常见模式时，它们表现出泛化能力，从而在训练数据集之外的新数据上表现良好。相比之下，记忆意味着网络记住输入示例而不是模式，这会导致过拟合。然而，正如
    Zhang 等人[[8](#bib.bib8)]发现，DNNs 可以轻松拟合随机标记的数据集，传统的统计学习理论如 VC 维度[[34](#bib.bib34)]、Rademacher
    复杂度[[35](#bib.bib35)]和均匀稳定性[[36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)]无法解释
    DNNs 的泛化。众所周知，DNNs 不能基于数据分布的常见模式正确分类随机标记的数据集，因此 DNNs 必须记住整个数据集。然而，DNNs 中的记忆机制仍然不清楚且模糊。因此，出现了两个重要而有趣的问题：为什么
    DNNs 在标准训练过程中记住数据，以及记忆机制如何运作。这吸引了机器学习界对记忆效应的探索。
- en: IV-B Memorization and Data Training
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 记忆与数据训练
- en: In studying overfitting [[39](#bib.bib39)], researchers have found that DNNs
    may memorize data. In exploring the memorization phenomenon, understanding the
    relationship between data distribution and memorization tendency, orders, as well
    as how the memorization mechanism affects training performance, is an important
    step.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究过拟合[[39](#bib.bib39)]时，研究人员发现 DNNs 可能记住数据。在探索记忆现象时，了解数据分布与记忆倾向、顺序之间的关系，以及记忆机制如何影响训练性能是一个重要步骤。
- en: 'The real-world natural data distributions are generally long-tailed [[40](#bib.bib40)]
    and almost all practical datasets are sampled from the real world. Considering
    this, Feldman et al. [[13](#bib.bib13), [11](#bib.bib11)] propose the long tail
    theory. This theory suggests that long-tailed examples as illustrated in Figure [4](#S3.F4
    "Figure 4 ‣ III-B Example Memorization Influence Score ‣ III Memorization Evaluations
    ‣ Memorization in deep learning: A survey") are prone to be memorized. Moreover,
    memorizing these long-tailed examples is crucial for achieving close-to-optimal
    generalization errors in long-tailed data distributions because rare and atypical
    instances can provide necessary generalization. They further validate the theory
    by evaluating examples based on the memorization score (Definition [1](#Thmdefinition1
    "Definition 1 ‣ II-A1 Label Memorization ‣ II-A Memorization Definitions in Generalization
    Domain ‣ II Memorization Definition ‣ Memorization in deep learning: A survey")).
    The results illustrate that examples with high memorization scores are more atypical.
    Thus, compared to typical examples, atypical examples are more likely to be memorized
    by DNNs. When removing examples with high memorization scores, the generalization
    errors increase. This theory also has empirical evidence in language tasks [[12](#bib.bib12)].
    Jiang et al. [[20](#bib.bib20)] develop the consistency score (C-score) based
    on memorization score (Definition [1](#Thmdefinition1 "Definition 1 ‣ II-A1 Label
    Memorization ‣ II-A Memorization Definitions in Generalization Domain ‣ II Memorization
    Definition ‣ Memorization in deep learning: A survey")), which can be applied
    in larger datasets. The C-score aims to measure the per-instance generalization.
    Their result demonstrates that a more atypical example has a lower C-score which
    provides convincing evidence for the long tail theory. Zhang et al. [[17](#bib.bib17)]
    extend the memorization score to unsupervised learning and propose counterfactual
    memorization (Definition [3](#Thmdefinition3 "Definition 3 ‣ II-A3 Counterfactual
    Memorization ‣ II-A Memorization Definitions in Generalization Domain ‣ II Memorization
    Definition ‣ Memorization in deep learning: A survey")) to evaluate text datasets.
    They discover that high memorization examples are generally unconventional texts
    such as all-capital letters, structured formats, and multilingual texts. In contrast,
    low memorization examples are generally templated documents with many near-duplicate
    copies in the training data. The tendency also corresponds to the long tail theory [[11](#bib.bib11)].'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '现实世界中的自然数据分布通常是长尾型的[[40](#bib.bib40)]，几乎所有实际数据集都是从现实世界中采样的。考虑到这一点，Feldman等人[[13](#bib.bib13),
    [11](#bib.bib11)]提出了长尾理论。该理论表明，如图[4](#S3.F4 "Figure 4 ‣ III-B Example Memorization
    Influence Score ‣ III Memorization Evaluations ‣ Memorization in deep learning:
    A survey")所示，长尾型示例更容易被记住。此外，记住这些长尾型示例对于在长尾数据分布中实现接近最佳的泛化误差至关重要，因为稀有和不典型的实例能够提供必要的泛化。他们进一步通过基于记忆评分（定义[1](#Thmdefinition1
    "Definition 1 ‣ II-A1 Label Memorization ‣ II-A Memorization Definitions in Generalization
    Domain ‣ II Memorization Definition ‣ Memorization in deep learning: A survey")）评估示例来验证该理论。结果表明，具有高记忆评分的示例更为不典型。因此，与典型示例相比，不典型示例更容易被深度神经网络（DNNs）记住。当去除具有高记忆评分的示例时，泛化误差会增加。该理论在语言任务中也有实证证据[[12](#bib.bib12)]。Jiang等人[[20](#bib.bib20)]基于记忆评分（定义[1](#Thmdefinition1
    "Definition 1 ‣ II-A1 Label Memorization ‣ II-A Memorization Definitions in Generalization
    Domain ‣ II Memorization Definition ‣ Memorization in deep learning: A survey")）开发了一种一致性评分（C-score），可以应用于更大的数据集。C-score旨在测量每个实例的泛化能力。他们的结果表明，更不典型的示例具有较低的C-score，这为长尾理论提供了有力的证据。Zhang等人[[17](#bib.bib17)]将记忆评分扩展到无监督学习中，并提出了反事实记忆（定义[3](#Thmdefinition3
    "Definition 3 ‣ II-A3 Counterfactual Memorization ‣ II-A Memorization Definitions
    in Generalization Domain ‣ II Memorization Definition ‣ Memorization in deep learning:
    A survey")）来评估文本数据集。他们发现，高记忆示例通常是非常规文本，如全大写字母、结构化格式和多语言文本。相对而言，低记忆示例通常是具有许多近似重复副本的模板化文档。这一趋势也与长尾理论相符[[11](#bib.bib11)]。'
- en: Moreover, a recent research [[15](#bib.bib15)] indicates that the DNNs cannot
    identify noisy examples from atypical examples empirically. When removing memorization-associated
    neurons, DNNs cannot classify the noisy examples and the generalization performance
    also reduces on the noise-mixed dataset. Additionally, for the same memorized
    example, the memorization path is distinct in repeated independent training experiments [[41](#bib.bib41)].
    This may represent that DNNs can select various particular features to uniquely
    identify the same example. Furthermore, the learning order of clean examples has
    observable consistency in similar architectures [[27](#bib.bib27)]. However, when
    training DNNs on the same dataset with randomly shuffled labels, they find that
    different models memorize the data in a different order. This finding also suggests
    that memorization learning has various possible paths.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最近的研究[[15](#bib.bib15)] 指出，DNNs 从经验上无法识别噪声示例与非典型示例。当去除与记忆相关的神经元时，DNNs 无法对噪声示例进行分类，且在噪声混合数据集上的泛化性能也下降。此外，对于相同的记忆示例，在重复的独立训练实验中，记忆路径是不同的[[41](#bib.bib41)]。这可能表示DNNs可以选择不同的特征来唯一识别相同的示例。此外，清洁示例的学习顺序在类似架构中具有明显的一致性[[27](#bib.bib27)]。然而，当在相同数据集上训练DNNs且标签随机打乱时，他们发现不同的模型以不同的顺序记住数据。这一发现也表明记忆学习具有多种可能路径。
- en: Summary. Atypical and noisy examples as long-tailed examples lack representativity
    in datasets, thus, DNNs are prone to memorize these long-tailed examples to minimize
    the training loss. This explains why and what DNNs memorize.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。非典型和噪声示例作为长尾示例在数据集中缺乏代表性，因此，DNNs更倾向于记住这些长尾示例以最小化训练损失。这解释了DNNs为什么以及记住了什么。
- en: IV-C Memorization and Repetition
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 记忆与重复
- en: Intuitively, DNNs tend to memorize duplicated examples. Zhang et al. [[17](#bib.bib17)]
    believe most memorization criteria strongly correlate with the number of example
    occurrences in the training, and language models will capture common memorization
    such as familiar phrases, public knowledge, or templated texts. Moreover, deduplicated
    datasets reduce the memorization frequency and improve generalization [[28](#bib.bib28)].
    From the perspective of the extraction task, repeated examples have a high probability
    of being extracted [[29](#bib.bib29)].
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，DNNs 倾向于记住重复的示例。张等人[[17](#bib.bib17)] 认为，大多数记忆标准与训练中示例出现的次数有很强的关联，语言模型将捕捉常见的记忆，如熟悉的短语、公共知识或模板化文本。此外，去重数据集减少了记忆频率并提高了泛化能力[[28](#bib.bib28)]。从提取任务的角度来看，重复的示例具有较高的被提取的概率[[29](#bib.bib29)]。
- en: 'One study [[9](#bib.bib9)] links repetition times and memorization, proposing
    $k$-Eidetic Memorization (Definition [7](#Thmdefinition7 "Definition 7 ‣ II-B2
    𝑘-Eidetic Memorization ‣ II-B Memorization Definitions in Security Domain ‣ II
    Memorization Definition ‣ Memorization in deep learning: A survey")), where $k$
    relates to the number of occurrences for one example. They apply this definition
    to the language model extraction task and investigate GPT-2\. For extractable
    large $k$ examples, they include common knowledge like city names or high-frequency
    words, and complex text such as the entire text of the MIT public license because
    the license may occur thousands of times in the training dataset. However, GPT-2
    also memorizes some low-frequent examples with small $k$ like contact information
    and valid URLs.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '一项研究[[9](#bib.bib9)] 将重复次数与记忆联系起来，提出了$k$-情景记忆（定义[7](#Thmdefinition7 "Definition
    7 ‣ II-B2 𝑘-Eidetic Memorization ‣ II-B Memorization Definitions in Security Domain
    ‣ II Memorization Definition ‣ Memorization in deep learning: A survey")），其中$k$与一个示例的出现次数相关。他们将这一定义应用于语言模型提取任务，并研究了GPT-2。对于可提取的大$k$示例，他们包括了像城市名称或高频词这样的常识，以及复杂文本，如MIT公共许可证的整个文本，因为许可证可能在训练数据集中出现数千次。然而，GPT-2也记住了一些低频示例，如联系信息和有效的URLs。'
- en: Therefore, in practical environments, example repetition is an influence factor
    of memorization. Nevertheless, the long tail theory [[13](#bib.bib13), [11](#bib.bib11)]
    tells us that the long-tailed examples are prone to be memorized and these long-tailed
    examples are low-frequent in the distribution. This requires systematically evaluating
    memorization factors.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在实际环境中，示例重复是记忆的一个影响因素。尽管如此，长尾理论[[13](#bib.bib13), [11](#bib.bib11)] 告诉我们，长尾示例容易被记住，而这些长尾示例在分布中是低频的。这需要系统地评估记忆因素。
- en: Summary. DNNs tend to prioritize the memorization of repeated data. However,
    memorization learning is limited by multiple factors. There is currently no unified
    framework to describe the impact of data on memorization.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。深度神经网络（DNN）倾向于优先记忆重复的数据。然而，记忆学习受到多种因素的限制。目前尚无统一的框架来描述数据对记忆的影响。
- en: IV-D Memorization and Training Stage
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 记忆与训练阶段
- en: Researchers have discovered that DNN training has a critical early learning
    stage [[42](#bib.bib42), [43](#bib.bib43)], during which model performance increases
    rapidly. Then with the truth that DNNs typically minimize loss in the final stage
    of training [[8](#bib.bib8), [44](#bib.bib44)], it is reasonable to believe that
    pattern learning and memorization learning dominate different training stages.
    Therefore, investigating and explaining the memorization dynamic during training
    stages constitutes a valuable research topic.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现，DNN训练在早期学习阶段具有关键意义[[42](#bib.bib42), [43](#bib.bib43)]，此阶段模型性能迅速提高。再加上DNN通常在训练的最终阶段最小化损失[[8](#bib.bib8),
    [44](#bib.bib44)]，因此合理认为模式学习和记忆学习主导了不同的训练阶段。因此，研究和解释训练阶段中的记忆动态是一个有价值的研究课题。
- en: Due to the difficulty in separating the memorization learning from the generalization,
    it is possible to explore how DNN learns by using a training dataset containing
    a mixture of normal and noisy examples in certain proportions. Arpit et al. [[25](#bib.bib25)]
    utilize the method and find that DNNs tend to prioritize learning patterns even
    in noisy datasets, as evidenced by high validation accuracy in the early training
    stage. Subsequently, DNNs begin to directly memorize noisy examples, leading to
    a rapid drop in validation accuracy. Maennel et al. [[30](#bib.bib30)] obverse
    an alignment between the principal components of network parameters and data takes
    place when training with random labels in the early training stage. However, the
    misalignment scores gradually increase during the later training stage.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于很难将记忆学习与泛化分开，可以通过使用包含正常和噪声示例的训练数据集来探索DNN的学习方式。Arpit等人[[25](#bib.bib25)]使用这种方法发现，DNN即使在噪声数据集中也倾向于优先学习模式，这在早期训练阶段通过高验证准确率得到了证明。随后，DNN开始直接记忆噪声示例，导致验证准确率迅速下降。Maennel等人[[30](#bib.bib30)]观察到，当在早期训练阶段使用随机标签进行训练时，网络参数的主成分与数据之间存在对齐。然而，错误对齐分数在后期训练阶段逐渐增加。
- en: Another perspective [[31](#bib.bib31), [41](#bib.bib41)] based on analyzing
    the gradient variation explains the phenomenon. In the early learning phase, the
    gradients from noisy examples contribute minimally to the total gradient because
    inconsistent gradient information may counteract each other, and those shared
    patterns of the same class are consistent, facilitating quick updates and promoting
    pattern learning. Similarly, applying a new detection method called ’variance
    of gradients’ (VoG) [[45](#bib.bib45)], the examples with lower VoG in the early
    training stage are more typical compared to the examples in the later training
    stage. Combining this with the long tail theory [[13](#bib.bib13)], it may be
    inferred that pattern learning dominates the early training stage.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种基于梯度变化分析的观点[[31](#bib.bib31), [41](#bib.bib41)]解释了这一现象。在早期学习阶段，来自噪声示例的梯度对总梯度的贡献最小，因为不一致的梯度信息可能会相互抵消，而同一类别的共享模式是一致的，有助于快速更新并促进模式学习。同样，应用一种新的检测方法称为“梯度方差”（VoG）[[45](#bib.bib45)]，在早期训练阶段具有较低VoG的示例相比于较晚训练阶段的示例更为典型。结合长尾理论[[13](#bib.bib13)]，可以推测模式学习主导了早期训练阶段。
- en: Summary. There is sufficient evidence to demonstrate that pattern learning dominates
    the early training stage, while memorization learning mainly occupies the relatively
    later training stage. One reasonable explanation is the subpopulation with the
    same pattern can contribute effective gradients during the early learning stage.
    Conversely, atypical examples and noisy examples contain conflicting gradient
    information that cannot be effectively learned during early training.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。有充分的证据表明，模式学习主导了早期训练阶段，而记忆学习则主要占据了相对较晚的训练阶段。一个合理的解释是，具有相同模式的子群体在早期学习阶段能够提供有效的梯度。相反，非典型示例和噪声示例包含的梯度信息存在冲突，这些信息在早期训练阶段无法有效学习。
- en: IV-E Memorization and Model Architecture
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 记忆与模型架构
- en: Different network layers experience diverse learning dynamics. In exploring
    the functions of DNN layers, a study on transferability [[46](#bib.bib46)] suggests
    that shallow layers’ features appear to be general and applicable to other tasks
    or datasets. However, deep layers tend to learn task-correlated features. This
    illustrates that different layers learn distinct features, with shallow layers
    in DNNs being more prone to learn patterns while deep layers specialize.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的网络层经历不同的学习动态。在探讨DNN层的功能时，一项关于迁移性的研究[[46](#bib.bib46)]表明，浅层的特征似乎具有一般性，并适用于其他任务或数据集。然而，深层则倾向于学习与任务相关的特征。这说明不同的层学习不同的特征，其中DNN中的浅层更容易学习模式，而深层则更具专业性。
- en: Some subsequent studies have contributed to enhancing the reliability of this
    viewpoint. Using Singular Vector Canonical Correlation Analysis (SVCCA), Raghu
    et al. [[47](#bib.bib47)] compare layers across time and observe their convergence
    starting from the shallow layers. Morcos et al. [[48](#bib.bib48)] develop projection-weighted
    Canonical Correlation Analysis (CCA) based on SVCCA. With multiple networks, they
    demonstrate that generalized networks converge to more similar representations.
    Specifically, they note that at shallow layers, all networks converged to equally
    similar solutions. Intuitively, this indicates that the shallow layers learn common
    patterns. However, at deep layers, groups of generalizing networks converged to
    substantially more similar solutions compared to groups of memorizing networks.
    This indicates that each memorizing network memorizes the training data using
    different strategies. Ansuini et al. [[49](#bib.bib49)] employ the Intrinsic Dimensionality
    (ID) of data representations, i.e. the minimal number of parameters needed to
    describe a representation. In their study on the utility of different layers,
    they find across layers, the ID initially increases and then progressively decreases
    in the final layers. Remarkably, they observed that the ID of the last hidden
    layer could predict classification on the test set.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一些后续研究有助于提高这一观点的可靠性。Raghu等人[[47](#bib.bib47)]使用奇异向量典型相关分析（SVCCA）比较不同时间的层，并观察到从浅层开始的收敛。Morcos等人[[48](#bib.bib48)]基于SVCCA开发了投影加权典型相关分析（CCA）。他们通过多个网络展示了通用网络收敛到更相似的表示。具体来说，他们注意到在浅层，所有网络收敛到相似的解决方案。这直观地表明，浅层学习了共同的模式。然而，在深层，通用网络组收敛到比记忆网络组更相似的解决方案。这表明每个记忆网络使用不同的策略来记忆训练数据。Ansuini等人[[49](#bib.bib49)]使用数据表示的内在维度（ID），即描述表示所需的最小参数数量。在他们对不同层的效用进行的研究中，他们发现各层的ID最初增加，然后在最终层逐渐减少。值得注意的是，他们观察到最后一个隐藏层的ID可以预测测试集上的分类。
- en: 'Clearly, the specialization of deep layers connects to memorization learning.
    Therefore, a reasonable inference is that the memorization of training examples
    occurs in the last (or final few) layers of a deep network. Stephenson et al. [[31](#bib.bib31)],
    employing replica-based mean field theoretic geometric analysis method, believe
    memorization mainly occurs in the deeper layers due to decreasing object manifolds’
    radius and dimension, whereas previous layers are minimally affected. In the experiment,
    if rewinding the parameters of the final convolutional layer to an earlier epoch,
    the generalization of the model can achieve a similar performance to the early-stopped
    model. Moreover, Anagnostidis et al. [[18](#bib.bib18)] employ the $k$NN probing
    to evaluate feature learning of each network layer, utilizing embedding vectors
    of each training example from each layer and correct labels to build $k$NN models.
    They find that embedding vectors of test examples produced from shallow layers
    achieve non-trivial $k$NN accuracy with the randomly labeled training dataset
    and data augmentation. They term this phenomenon benign memorization (Definition [4](#Thmdefinition4
    "Definition 4 ‣ II-A4 Benign Memorization ‣ II-A Memorization Definitions in Generalization
    Domain ‣ II Memorization Definition ‣ Memorization in deep learning: A survey")).
    However, the $k$NN probing accuracies drop significantly in the deep layers, indicating
    that DNN fits the random labels. This suggests that only the very last layers
    are used for memorization, while previous layers encode generalized features that
    remain largely unaffected by the label noise. Furthermore, another work [[30](#bib.bib30)]
    explains the training data will align the principal components of network parameters
    at the earlier layers when trained with random labels and later layers become
    specialized.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '显然，深层的专业化与记忆学习有关。因此，一个合理的推断是，训练示例的记忆发生在深度网络的最后（或最后几）层。Stephenson 等人[[31](#bib.bib31)]采用基于副本的均场理论几何分析方法，认为记忆主要发生在更深的层中，因为物体流形的半径和维度在减少，而前面的层受到的影响最小。在实验中，如果将最后卷积层的参数回滚到早期的时代，模型的泛化性能可以达到类似于早期停止模型的效果。此外，Anagnostidis
    等人[[18](#bib.bib18)] 采用 $k$NN 探测来评估每个网络层的特征学习，利用每个训练示例的嵌入向量和正确标签来构建 $k$NN 模型。他们发现，来自浅层的测试示例的嵌入向量在随机标记的训练数据集和数据增强下获得了非平凡的
    $k$NN 准确性。他们称这种现象为良性记忆（定义 [4](#Thmdefinition4 "Definition 4 ‣ II-A4 Benign Memorization
    ‣ II-A Memorization Definitions in Generalization Domain ‣ II Memorization Definition
    ‣ Memorization in deep learning: A survey")）。然而，$k$NN 探测准确性在深层显著下降，表明深度神经网络适应了随机标签。这表明只有最后几层用于记忆，而前面的层编码了普遍的特征，这些特征在标签噪声的影响下大多保持不变。此外，另一项工作[[30](#bib.bib30)]解释了当使用随机标签训练时，训练数据将对齐网络参数的主要组件于早期层，而后来的层则变得专门化。'
- en: However, the latest work conducted by Maini et al. [[15](#bib.bib15)] reveals
    that memorization of a classification task exists in a small set of neurons in
    various layers of the model, and the layers that contribute to example memorization
    are, not the final layers. In their experiment, they use a noisy dataset to train
    DNNs. Subsequently, they apply technologies known as layer retraining and layer
    rewinding to eliminate memorization within individual layers. Finally, they validate
    the memorization effect (i.e. accuracy of noisy examples in the training dataset)
    on modified models. The unexpected finding is that the memorization effect still
    persists in the model, which proves various layers contribute to the memorization.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Maini 等人[[15](#bib.bib15)] 进行的最新研究表明，分类任务的记忆存在于模型的各个层中的一小部分神经元中，而有助于示例记忆的层并不是最后的层。在他们的实验中，他们使用了一个噪声数据集来训练深度神经网络（DNNs）。随后，他们应用了称为层重训练和层回滚的技术来消除单个层中的记忆。最后，他们在修改后的模型上验证了记忆效应（即训练数据集中噪声示例的准确性）。意外的发现是，模型中的记忆效应仍然存在，这证明了各个层都对记忆有所贡献。
- en: 'Furthermore, researchers also investigate the inherent functions of layers
    regarding memorization. An interesting work [[50](#bib.bib50)] attempts to demonstrate
    if only the memorization function can provide generalization. The author builds
    a network of lookup tables and finds deep look-up table network exhibits generalization
    in the Binary-MNIST task. Moreover, this model reproduces a crucial finding with
    neural networks: memorization can provide generalization with depth, though it
    is doubtful that the model can work on more complex tasks. Additionally, Zhang
    et al. [[51](#bib.bib51)] investigate whether different trained networks tend
    to demonstrate the constant function (memorization) or the identity function (generalization)
    and they empirically find that different architectures exhibit strikingly different
    complex biases.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究人员还调查了层的固有记忆功能。一个有趣的研究 [[50](#bib.bib50)] 尝试展示仅凭记忆功能是否能提供泛化。作者构建了一个查找表网络，发现深度查找表网络在
    Binary-MNIST 任务中表现出泛化能力。此外，该模型重现了神经网络中的一个重要发现：记忆可以通过深度提供泛化，尽管该模型是否能处理更复杂的任务仍存疑。此外，Zhang
    等人 [[51](#bib.bib51)] 调查了不同训练网络是否倾向于表现出恒定函数（记忆）或身份函数（泛化），他们实证发现，不同架构展现出显著不同的复杂偏差。
- en: As Transformers [[52](#bib.bib52)] achieve a big success in various tasks, people
    are also interested in memorization of Transformers and large language models.
    Sukhbaatar et al. [[53](#bib.bib53)] augment the self-attention layers with persistent
    memory vectors and find this plays a similar role as the feed-forward layer. Moreover,
    Geva et al. [[32](#bib.bib32)] directly point out that feed-forward layers are
    key-value memories, where each key correlates with textual patterns in the training
    examples, and each value induces a distribution over the output vocabulary. It
    should be noted here that this kind of key-value memory combines pattern learning
    and memorization learning. They show that feed-forward layers act as pattern detectors
    over the input across all layers and learned patterns are human-interpretable.
    Additionally, Dai et al. [[54](#bib.bib54)] introduce the concept of knowledge
    neurons that express factual knowledge and propose a knowledge attribution method
    to identify the neurons via a fill-in-the-blank cloze task in BERT. They find
    that the activation of such knowledge neurons is positively correlated to the
    expression of their corresponding facts.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Transformers [[52](#bib.bib52)] 在各种任务中取得巨大成功，人们对 Transformers 和大型语言模型的记忆功能也产生了兴趣。Sukhbaatar
    等人 [[53](#bib.bib53)] 通过持久记忆向量增强了自注意力层，并发现这在一定程度上类似于前馈层的作用。此外，Geva 等人 [[32](#bib.bib32)]
    直接指出，前馈层是键值记忆，其中每个键与训练示例中的文本模式相关联，每个值则对输出词汇表产生一个分布。需要指出的是，这种键值记忆结合了模式学习和记忆学习。他们展示了前馈层如何在所有层上作为模式检测器，且学习到的模式是人类可解释的。此外，Dai
    等人 [[54](#bib.bib54)] 引入了表达事实知识的知识神经元概念，并提出了一种知识归属方法，通过 BERT 中的填空克漏字任务来识别这些神经元。他们发现，这些知识神经元的激活与其对应事实的表达呈正相关。
- en: Summary. Functions of different layers in neural networks vary significantly.
    While many works prove deep layers specialize, memorization location still requires
    more exploration. For Transformers, researchers have found that feed-forward layers
    are key-value memories but the memory is not only the result of memorization learning.
    Among these key-value memories, we cannot ensure they are all task-correlated.
    Some irrelevant and unexpected details could also be stored in them. Further research
    is needed to investigate the memorization mechanism associated with the model
    architecture.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。神经网络中不同层的功能差异显著。虽然许多研究证明了深层专门化，但记忆位置仍需进一步探索。对于 Transformers，研究人员发现前馈层是键值记忆，但记忆不仅仅是记忆学习的结果。在这些键值记忆中，我们不能确保它们全部与任务相关。一些无关且意外的细节也可能被存储其中。需要进一步研究与模型架构相关的记忆机制。
- en: IV-F Memorization and Overfitting
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F 记忆与过拟合
- en: Overfitting is a common phenomenon in deep learning which represents that a
    model learns the training data so well that it captures not only the underlying
    patterns but also the particular features in the data. This causes the model to
    fail in generalizing effectively to new, unseen data. Thus, early research commonly
    held the opinion that overfitting was responsible for memorization. However, contemporary
    studies [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)] provide evidence
    supporting the persistence of memorization throughout the training process. Memorization
    does not necessarily lead to overfitting.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是深度学习中的一种常见现象，它表示模型学习训练数据非常好，以至于不仅捕捉到数据中的潜在模式，还捕捉到数据中的特定特征。这导致模型在对新数据进行有效泛化时失败。因此，早期研究普遍认为过拟合是记忆的原因。然而，现代研究[[14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)]提供了支持记忆在整个训练过程中持续存在的证据。记忆不一定导致过拟合。
- en: 'Based on the long tail theory [[13](#bib.bib13), [11](#bib.bib11)], we understand
    that memorizing atypical examples contributes to generalization. In contrast,
    overfitting will enlarge the generalization error while training loss decreases.
    Some recent works suggest that even for DNNs without a significant train-test
    gap, memorization still exists [[9](#bib.bib9), [16](#bib.bib16)]. Additionally,
    the privacy risks (Evaluation [III-A2](#S3.SS1.SSS2 "III-A2 Probabilistic Memorization
    Evaluation ‣ III-A Example Memorization Evaluation ‣ III Memorization Evaluations
    ‣ Memorization in deep learning: A survey")) also imply the underlying memorization.
    It is known that overfitting is not necessary for successful membership inference
    attacks [[55](#bib.bib55), [56](#bib.bib56)]. Furthermore, when a neural network
    is trained in a training dataset mixed with clean examples and less noisy examples,
    both the accuracy of noisy examples and that of clean examples exhibit concurrent
    improvement [[15](#bib.bib15)]. Overfitting is a phenomenon of training observed
    in the later stages of training. For individual training examples, DNNs may memorize
    them while learning patterns in the early training stage. Thus, memorization does
    not necessarily require overfitting.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '基于长尾理论[[13](#bib.bib13), [11](#bib.bib11)]，我们了解到记忆不典型示例有助于泛化。相比之下，过拟合会扩大泛化误差，而训练损失减少。一些近期的研究表明，即使对于没有显著训练-测试差距的深度神经网络（DNN），记忆仍然存在[[9](#bib.bib9),
    [16](#bib.bib16)]。此外，隐私风险（评估 [III-A2](#S3.SS1.SSS2 "III-A2 Probabilistic Memorization
    Evaluation ‣ III-A Example Memorization Evaluation ‣ III Memorization Evaluations
    ‣ Memorization in deep learning: A survey")）也暗示了潜在的记忆。已知过拟合并不是成功进行成员推断攻击的必要条件[[55](#bib.bib55),
    [56](#bib.bib56)]。此外，当神经网络在包含干净示例和噪声较少的示例的训练数据集上训练时，噪声示例和干净示例的准确率都会同时提高[[15](#bib.bib15)]。过拟合是训练后期观察到的一种现象。对于个别训练示例，DNNs可能在早期训练阶段就记住了它们，同时学习模式。因此，记忆并不一定需要过拟合。'
- en: Another interesting phenomenon is benign overfitting. The phenomenon means that
    even after overlearning training data, DNNs still can generalize well [[57](#bib.bib57),
    [58](#bib.bib58)]. This theory believes overparameterized DNNs can generalize
    to the majority of the data distribution using simple paths, and memorize mislabeled
    and irregular data using complex paths. These components do not interfere, making
    such overfitting benign. One explanation [[59](#bib.bib59)] believes that overfitting
    becomes benign when the signal-to-noise ratio satisfies a certain condition. In
    simple terms, benign overfitting requires sufficient signals in the dataset. Thus,
    benign overfitting may involve less memorization, yet there is insufficient evidence
    to illustrate their relationship.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的现象是良性过拟合。该现象意味着即使在过度学习训练数据后，DNNs 仍然可以很好地泛化[[57](#bib.bib57), [58](#bib.bib58)]。这一理论认为，过参数化的
    DNNs 可以使用简单路径对大部分数据分布进行泛化，并使用复杂路径记忆标记错误和不规则数据。这些组件不会相互干扰，使得这种过拟合是良性的。一种解释[[59](#bib.bib59)]认为，当信号与噪声比满足某一条件时，过拟合变得良性。简单来说，良性过拟合要求数据集中有足够的信号。因此，良性过拟合可能涉及较少的记忆，但目前尚无充分证据说明它们之间的关系。
- en: Summary. Overfitting as a training phenomenon does not have a strong relationship
    with memorization. In the context of overfitting, memorization is necessary but
    not sufficient.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。作为训练现象，过拟合与记忆之间的关系并不强。在过拟合的背景下，记忆是必要的，但并不充分。
- en: IV-G Memorization and Data Augmentation, Regularization
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-G 记忆与数据增强、正则化
- en: Data augmentation and regularization are widespread techniques used in training
    neural networks. Therefore, it is necessary to study the impact of these practices
    on memorization.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强和正则化是训练神经网络中广泛使用的技术。因此，有必要研究这些实践对记忆化的影响。
- en: General Data Augmentation
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通用数据增强
- en: Data Augmentation is a pivotal strategy used to expand the original training
    dataset by introducing a variety of artificially generated examples. Generally,
    the primary objective of this technique is to enrich example representations based
    on corresponding semantic features. Multiple representations can help DNNs perform
    well on unseen examples, thereby improving the generalization. Presently, various
    technologies exist to implement data augmentation, but here we focus on trivial
    data augmentation, which means fundamental transformations applied to the original
    training dataset. Trivial data augmentation depends on specific data formats.
    For instance, in image processing, these transformations could include rotations,
    flips, zooms, and color variations. In natural language processing, techniques
    might encompass synonym replacement or back-translation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是一种关键策略，通过引入各种人工生成的示例来扩展原始训练数据集。通常，这种技术的主要目标是基于相应的语义特征丰富示例表示。多重表示可以帮助深度神经网络在未见过的示例上表现良好，从而提高泛化能力。目前，存在各种技术来实现数据增强，但这里我们关注简单的数据增强，即对原始训练数据集应用的基本变换。简单的数据增强依赖于特定的数据格式。例如，在图像处理领域，这些变换可能包括旋转、翻转、缩放和颜色变化。在自然语言处理领域，技术可能包括同义词替换或回译。
- en: In related works, one early study [[60](#bib.bib60)] demonstrates trivial data
    augmentation can reduce the risks of membership inference attacks, thereby diminishing
    memorization. Utilizing recent memorization evaluation methods, Li et al. [[22](#bib.bib22)]
    study the memorization effect of multiple data augmentation. They measure the
    memorization evaluation results by membership inference and demonstrate trivial
    data augmentation technologies significantly mitigate memorization. However, for
    advanced data augmentation technologies, further research on the memorization
    effect is still required. Another work [[18](#bib.bib18)] measures memorization
    based on $k$NN probing and they find that $k$NN probing accuracy of the embedding
    vectors increases with data augmentation under random label training datasets
    and clean training datasets. Moreover, they observe that learning under complete
    label noise with data augmentation still leads to highly useful features in the
    shallow layers, explaining it as augmented datasets increasing the effective size
    of the dataset beyond the capacity of networks. This supports that data augmentation
    mitigates memorization. However, the mechanism of how data augmentation impacts
    memorization still needs to be explored and systematically evaluated.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在相关工作中，一项早期研究[[60](#bib.bib60)]表明，简单的数据增强可以降低会员推断攻击的风险，从而减少记忆化。利用最新的记忆化评估方法，Li
    等人[[22](#bib.bib22)]研究了多种数据增强的记忆化效果。他们通过会员推断来测量记忆化评估结果，并证明简单的数据增强技术显著缓解了记忆化。然而，对于高级数据增强技术，仍需进一步研究其记忆化效果。另一项工作[[18](#bib.bib18)]基于$k$NN探测来测量记忆化，他们发现，在随机标签训练数据集和干净训练数据集下，嵌入向量的$k$NN探测准确率随着数据增强而增加。此外，他们观察到，在完全标签噪声下进行数据增强学习仍会在浅层中产生非常有用的特征，解释为增强的数据集增加了超出网络容量的有效数据集大小。这支持数据增强能够减轻记忆化。然而，数据增强如何影响记忆化的机制仍需探索和系统评估。
- en: Regularization
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 正则化
- en: Regularizers like weight decay and dropout are the standard tools in theory
    and practice to mitigate overfitting in the training of neural networks. We know
    that regularizers help constrain the learning process to a specific subset of
    the hypothesis space with manageable complexity. In the work of Zhang et al. [[8](#bib.bib8)],
    explicit regularizers can prevent model memorization under random label learning,
    and help the model improve generalization. However, regularization is neither
    necessary nor by itself sufficient for controlling generalization errors. Then
    the research conducted by Arpit et al. [[25](#bib.bib25)] reproduces a similar
    result as Zhang et al. [[8](#bib.bib8)] and finds dropout is best at hindering
    memorization without reducing the model’s ability to learn. This also responds
    to the work of location memorization [[15](#bib.bib15)], in which finds memorization
    exists in a small set of neurons in various layers of the model. It seems under
    random label training, explicit regularizers can mitigate memorization by dropping
    or constraining neurons, but it is not clear to understand how regularizers influence
    atypical example memorization in standard training.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化器如权重衰减和丢弃是理论和实践中用于减轻神经网络训练中过拟合的标准工具。我们知道正则化器有助于将学习过程限制在具有可管理复杂性的假设空间的特定子集内。在
    Zhang 等人[[8](#bib.bib8)] 的工作中，显式正则化器可以在随机标签学习下防止模型记忆，并帮助模型提高泛化能力。然而，正则化既不是控制泛化误差的必要条件，也不是唯一的充分条件。随后，Arpit
    等人[[25](#bib.bib25)] 进行的研究重现了 Zhang 等人[[8](#bib.bib8)] 的类似结果，并发现丢弃在抑制记忆的同时不会降低模型的学习能力。这也回应了位置记忆的工作[[15](#bib.bib15)]，该工作发现记忆存在于模型不同层中的一小部分神经元中。在随机标签训练下，显式正则化器可以通过丢弃或限制神经元来减轻记忆，但尚不清楚正则化器如何影响标准训练中的非典型例子记忆。
- en: Summary. Data augmentation and regularization are standard tools in training
    neural networks and help improve model generalization. In related works, both
    tools mitigate memorization under random label training, but we do not know if
    they hinder learning long-tailed examples in standard training. This could be
    a research direction in the future.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。数据增强和正则化是训练神经网络中的标准工具，并帮助提高模型的泛化能力。在相关工作中，这两种工具在随机标签训练下减轻了记忆，但我们不知道它们是否会阻碍标准训练中长尾例子的学习。这可能是未来的研究方向。
- en: IV-H Memorization and Other Factors
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-H 记忆与其他因素
- en: Capacity
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 容量
- en: 'The model capacity is related to model memorization. Generally, models with
    larger sizes can memorize more data than smaller ones [[29](#bib.bib29)]. Additionally,
    early work has shown that overparameterized neural networks can directly memorize
    randomly labeled modern datasets [[8](#bib.bib8)]. However, we cannot easily think
    larger capacity leads to more memorization because training data plays an important
    role. The effective capacity of networks cannot directly explain memorization
    and generalization [[25](#bib.bib25)]. Naturally, a question arises: what happens
    if the training dataset size far exceeds the model’s capacity? Data augmentation
    can create this condition, and Anagnostidis et al. [[18](#bib.bib18)] find that
    even the randomly labeled dataset with data augmentation exceeding the model capacity
    can produce effective patterns in the model. Nevertheless, related topics still
    require further research.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 模型容量与模型记忆有关。一般来说，较大规模的模型可以记住比小规模模型更多的数据[[29](#bib.bib29)]。此外，早期工作已显示过参数化的神经网络可以直接记住随机标记的现代数据集[[8](#bib.bib8)]。然而，我们不能轻易认为更大的容量导致更多的记忆，因为训练数据在其中扮演了重要角色。网络的有效容量不能直接解释记忆和泛化[[25](#bib.bib25)]。自然地，问题出现了：如果训练数据集的大小远远超过模型的容量会发生什么？数据增强可以创造这种条件，Anagnostidis
    等人[[18](#bib.bib18)] 发现，即使是容量超过模型的随机标记数据集通过数据增强也能在模型中产生有效的模式。然而，相关主题仍需进一步研究。
- en: Loss
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 损失
- en: The loss function is an important component of neural network training. Thus,
    it must influence the memorization dynamics of models. Patel et al. [[61](#bib.bib61)]
    propose robust log loss (RLL) which can prevent model overfitting on the randomly
    labeled data. However, no further studies have explored how different types of
    loss functions affect model memorization.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是神经网络训练中的重要组成部分。因此，它必须影响模型的记忆动态。Patel 等人[[61](#bib.bib61)] 提出了鲁棒对数损失（RLL），它可以防止模型在随机标记数据上的过拟合。然而，目前尚无进一步的研究探讨不同类型的损失函数如何影响模型记忆。
- en: Learning Rate
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 学习率
- en: Learning rate is an essential hyperparameter in neural network training. Li
    et al. [[62](#bib.bib62)] believe that a small learning rate model easily learns
    details, while a large learning rate helps capture patterns. They demonstrate
    this by adding a small patch to CIFAR10 images that are immediately memorizable
    by a model with a small initial learning rate but ignored by the model with a
    large learning rate until after annealing.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是神经网络训练中的一个重要超参数。Li 等人[[62](#bib.bib62)]认为，小学习率模型容易学习细节，而大学习率有助于捕捉模式。他们通过在
    CIFAR10 图像上添加一个小补丁来演示这一点，该补丁被具有小初始学习率的模型立即记忆，但被具有大学习率的模型忽略，直到退火之后。
- en: Data Format
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据格式
- en: The data format may affect memorization during training, particularly for language
    tasks. Kharitonov et al. [[63](#bib.bib63)] find the size of the subword vocabulary
    learned by Byte-Pair Encoding (BPE) greatly affects both ability and tendency
    of standard Transformer models to memorize training data. Larger subword vocabulary
    and shorter input sequences result in strong memorization of Transformer models.
    The underlying reason could be that complex subwords weaken the patterns in the
    data distribution. Thus, the input data format likewise impacts memorization.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 数据格式可能会影响训练过程中的记忆，特别是在语言任务中。Kharitonov 等人[[63](#bib.bib63)]发现，Byte-Pair Encoding
    (BPE) 学到的子词词汇大小极大地影响了标准 Transformer 模型记忆训练数据的能力和倾向。较大的子词词汇和较短的输入序列导致 Transformer
    模型强烈记忆。其潜在原因可能是复杂的子词削弱了数据分布中的模式。因此，输入数据格式也会影响记忆。
- en: Summary. Many other factors may facilitate or hinder memorization. Specifically,
    large models tend to memorize due to an excessive number of parameters. Moreover,
    a small learning rate can promote memorization. Additionally, loss functions and
    data format have impacts on model memorization but we still require further studies.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 概述。许多其他因素可能促进或阻碍记忆。具体而言，大型模型由于参数过多而倾向于记忆。此外，小学习率可以促进记忆。另外，损失函数和数据格式对模型记忆有影响，但我们仍需进一步研究。
- en: V Underlying Risks of Memorization Learning
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 记忆学习的潜在风险
- en: 'In previous sections, DNNs have been shown the feature of memorizing training
    data, and this property may cause various security risks. This section undertakes
    exploration and synthesis of the impact of memorization on typical threats and
    defenses in DNNs. We summarize the main literature related to the risks of memorization
    learning in Table [III](#S5.T3 "TABLE III ‣ V Underlying Risks of Memorization
    Learning ‣ Memorization in deep learning: A survey") and plot Figure [5](#S5.F5
    "Figure 5 ‣ V Underlying Risks of Memorization Learning ‣ Memorization in deep
    learning: A survey") to demonstrate these risks.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '在前面的部分中，深度神经网络（DNN）已显示出记忆训练数据的特征，这种属性可能会引发各种安全风险。本节将探讨和综合记忆对 DNN 中典型威胁和防御的影响。我们在表[III](#S5.T3
    "TABLE III ‣ V Underlying Risks of Memorization Learning ‣ Memorization in deep
    learning: A survey")中总结了与记忆学习风险相关的主要文献，并绘制了图[5](#S5.F5 "Figure 5 ‣ V Underlying
    Risks of Memorization Learning ‣ Memorization in deep learning: A survey")以展示这些风险。'
- en: '![Refer to caption](img/82f74e31176b70b1f31dc3a395996a23.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/82f74e31176b70b1f31dc3a395996a23.png)'
- en: 'Figure 5: Underlying Risks of Memorization.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 记忆的潜在风险。'
- en: 'TABLE III: Main Works about Underlying Risks of Memorization Learning'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 记忆学习潜在风险的主要工作'
- en: '| Main Topic | Reference | Background Task | Main Eval. Method | Main Findings
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 主要主题 | 参考文献 | 背景任务 | 主要评估方法 | 主要发现 |'
- en: '| Memorization about Membership Inference Risks | Leino et al., 2020 [[64](#bib.bib64)]
    | Supervised Classification Task | / | Propose a new membership inference attack
    based on memorized features. |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 关于成员推断风险的记忆 | Leino 等，2020 [[64](#bib.bib64)] | 有监督分类任务 | / | 提出了一种基于记忆特征的新成员推断攻击。
    |'
- en: '|  | Carlini et al., 2022 [[24](#bib.bib24)] | Various Tasks | / | Propose
    a new membership inference attack ’LiRA’ utilizing the memorization effect. |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | Carlini 等，2022 [[24](#bib.bib24)] | 各种任务 | / | 提出了一种利用记忆效应的新成员推断攻击‘LiRA’。
    |'
- en: '|  | Carlini et al., 2022 [[21](#bib.bib21)] | Supervised Classification Task
    | Probabilistic Memorization Evaluation | Removing the vulnerable outlier points
    may threaten inner previously-safe points on the same attack. |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | Carlini 等，2022 [[21](#bib.bib21)] | 有监督分类任务 | 概率记忆评估 | 删除易受攻击的离群点可能会威胁到同一攻击下的内部先前安全点。
    |'
- en: '| Memorization about Extraction Risks | Carlini et al., 2019 [[14](#bib.bib14)]
    | Unsupervised Language Generative Task | Recurrence Memorization Evaluation |
    Introduce a memorization exposure metric to measure unintended memorization. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 关于提取风险的记忆 | Carlini et al., 2019 [[14](#bib.bib14)] | 无监督语言生成任务 | 重现记忆评估
    | 引入记忆暴露度度量以衡量无意的记忆。 |'
- en: '|  | Carlini et al., 2021 [[9](#bib.bib9)] | Unsupervised Language Generative
    Task | Extraction Memorization Evaluation | An adversary can perform a training
    data extraction attack to recover individual training examples by querying the
    large language model. |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | Carlini et al., 2021 [[9](#bib.bib9)] | 无监督语言生成任务 | 提取记忆评估 | 对手可以通过查询大语言模型执行训练数据提取攻击，从而恢复单个训练样本。
    |'
- en: '|  | Carlini et al., 2023 [[29](#bib.bib29)] | Unsupervised Language Generative
    Task | Extraction Memorization Evaluation | Describe three log-linear relationships
    that how model capacity, duplication times, and the number of tokens impact memorization
    of LMs. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | Carlini et al., 2023 [[29](#bib.bib29)] | 无监督语言生成任务 | 提取记忆评估 | 描述模型容量、复制次数和令牌数量如何影响语言模型的记忆的三种对数线性关系。
    |'
- en: '| Memorization about Poisoning Risks | Zhang et al., 2017 [[8](#bib.bib8)]
    | Supervised Classification Task | Noisy Label Memorization Evaluation | DNNs
    can memorize randomly labeled datasets that traditional approaches fail to explain
    generalization. |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 关于中毒风险的记忆 | Zhang et al., 2017 [[8](#bib.bib8)] | 有监督分类任务 | 噪声标签记忆评估 | 深度神经网络可以记忆随机标记的数据集，这些数据集传统方法无法解释泛化。
    |'
- en: '|  | Nguyen et al., 2023 [[19](#bib.bib19)] | Supervised Classification Task
    | Noisy Label Memorization Evaluation | Mislabeled examples may degrade the neural
    collapse and damage model generalization |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | Nguyen et al., 2023 [[19](#bib.bib19)] | 有监督分类任务 | 噪声标签记忆评估 | 标签错误的样本可能会恶化神经崩溃并损害模型泛化。
    |'
- en: '|  | Maini et al., 2023 [[15](#bib.bib15)] | Supervised Classification Task
    | Noisy Label Memorization Evaluation | Drop memorization-associated neurons,
    mislabeled examples cannot be effectively classified. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | Maini et al., 2023 [[15](#bib.bib15)] | 有监督分类任务 | 噪声标签记忆评估 | 删除与记忆相关的神经元，标签错误的样本无法有效分类。
    |'
- en: '| Memorization about Adversarial Risks | Li et al., 2023 [[22](#bib.bib22)]
    | Supervised Classification Task | Probabilistic Memorization Evaluation | In
    adversarial training, adversarial examples are very atypical and prone to be memorized.
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 关于对抗风险的记忆 | Li et al., 2023 [[22](#bib.bib22)] | 有监督分类任务 | 概率记忆评估 | 在对抗训练中，对抗样本非常异常，容易被记忆。
    |'
- en: '|  | Xu et al., 2023 [[65](#bib.bib65)] | Supervised Classification Task |
    Differential Memorization Evaluation | Memorizing atypical samples hardly improve
    their adversarial robustness. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | Xu et al., 2023 [[65](#bib.bib65)] | 有监督分类任务 | 差异记忆评估 | 记忆异常样本几乎不会提高其对抗鲁棒性。
    |'
- en: '|  | Dong et al., 2022 [[26](#bib.bib26)] | Supervised Classification Task
    | Noisy Label Memorization Evaluation | Memorization in adversarial training could
    result in robust overfitting. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | Dong et al., 2022 [[26](#bib.bib26)] | 有监督分类任务 | 噪声标签记忆评估 | 对抗训练中的记忆可能导致鲁棒过拟合。
    |'
- en: V-A Memorization and Membership Inference Risks
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 记忆和成员推断风险
- en: A membership inference attack is a representative privacy inference attack and
    seeks to address the query if a specific instance belongs to the training dataset [[66](#bib.bib66)].
    In the machine learning setting, the membership inference adversary is typically
    given access to a model’s predictions with varying granularity, ranging from the
    complete confidence vector to the label corresponding to the highest confidence
    score. It is established that the memorization phenomenon entails the memorization
    of training data points by DNNs, thereby implying that memorized data bears substantial
    risks for membership inference.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 成员推断攻击是一种代表性的隐私推断攻击，旨在解决一个特定实例是否属于训练数据集的查询 [[66](#bib.bib66)]。在机器学习环境中，成员推断对手通常可以访问模型的预测，粒度从完整的置信度向量到对应于最高置信度得分的标签。已经确定，记忆现象涉及深度神经网络对训练数据点的记忆，这意味着记忆数据对成员推断具有实质性风险。
    |
- en: Indeed, prior work has demonstrated that the membership inference risks associated
    with training data exhibit significant non-uniformity. According to empirical
    results, typical data points have lower membership inference risks than those
    atypical data examples and outliers [[24](#bib.bib24), [11](#bib.bib11), [64](#bib.bib64),
    [67](#bib.bib67)]. These findings imply memorization is highly corresponding or
    even results in high membership inference risks. However, no direct quantitative
    investigation has been identified to establish the precise relationship between
    memorization and membership inference risks. In practice, this relationship has
    been implicitly approved [[21](#bib.bib21), [24](#bib.bib24), [64](#bib.bib64)].
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，先前的工作已经表明，与训练数据相关的成员推断风险表现出显著的不均匀性。根据实证结果，典型数据点的成员推断风险低于那些非典型数据示例和离群值 [[24](#bib.bib24),
    [11](#bib.bib11), [64](#bib.bib64), [67](#bib.bib67)]。这些发现暗示记忆与成员推断风险高度相关，甚至导致高风险。然而，尚未确定直接的定量研究来建立记忆与成员推断风险之间的精确关系。在实践中，这种关系已经被隐含地认可 [[21](#bib.bib21),
    [24](#bib.bib24), [64](#bib.bib64)]。
- en: 'Depending on the relationship between memorization and membership inference
    attack, Carlini et al. [[21](#bib.bib21)] find the privacy onion effect. The effect
    can be defined: when removing the most vulnerable data under a specific privacy
    attack and retraining a model on only the previously safe data, a new set of examples
    in turn becomes vulnerable to the same privacy attack. This phenomenon may indicate
    that even after removing those memorized data points, the model still memorizes
    relatively atypical data examples in the remaining training dataset. This observation
    intuitively underscores membership inference risks are highly associated with
    memorization and proves that memorization is relative.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 根据记忆与成员推断攻击之间的关系，Carlini 等人 [[21](#bib.bib21)] 发现了隐私洋葱效应。该效应可以定义为：在特定隐私攻击下，移除最脆弱的数据并仅在之前安全的数据上重新训练模型后，一组新的示例反过来变得容易受到相同的隐私攻击。这一现象可能表明，即使在移除那些记忆数据点之后，模型仍然会记住剩余训练数据集中相对非典型的数据示例。这一观察直观地强调了成员推断风险与记忆高度相关，并证明了记忆是相对的。
- en: Utilizing the memorization effect, researchers investigate more threatening
    membership inference attacks. Leino et al. [[64](#bib.bib64)] attempt to exploit
    features of memorized examples that are predictive only for the training data
    but not the sampling distribution. They capture differences in memorization learning
    data and pattern learning data and build a confident binary logistic classifier
    to infer membership. Another work is Likelihood Ratio Attack (LiRA) [[24](#bib.bib24)],
    it depends on the leave-one-out method as memorization score definition and utilizes
    differences of model outputs that training with and without the training example
    to do membership inference. These attacks directly exhibit that memorized examples
    have higher privacy risks than generalized examples.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 利用记忆效应，研究人员调查了更具威胁性的成员推断攻击。Leino 等人 [[64](#bib.bib64)] 试图利用那些仅对训练数据有效而对采样分布无效的记忆示例特征。他们捕捉了记忆学习数据与模式学习数据之间的差异，并构建了一个有信心的二元逻辑回归分类器来推断成员资格。另一项工作是似然比攻击
    (LiRA) [[24](#bib.bib24)]，它依赖于 leave-one-out 方法作为记忆分数定义，并利用训练与不训练该示例的模型输出差异来进行成员推断。这些攻击直接表明，记忆示例的隐私风险高于泛化示例。
- en: Summary. While no direct quantitative research has yet proven the relationship
    between memorization and membership inference attacks, nearly all relevant existing
    studies suggest a strong correspondence between memorization and membership inference
    risks and even indicate a causal relationship. Additionally, it is imperative
    to explore novel inference risks and mitigation strategies that are contingent
    upon the memorization effect.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。虽然尚无直接的定量研究证明记忆与成员推断攻击之间的关系，但几乎所有相关的现有研究都表明记忆与成员推断风险之间存在强烈的对应关系，甚至指出了因果关系。此外，探索依赖于记忆效应的新型推断风险和缓解策略是至关重要的。
- en: V-B Memorization and Inversion/Extraction Risks
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 记忆和反转/提取风险
- en: The adversary in an inversion/extraction attack attempts to rebuild or extract
    training examples by leveraging gradients or models. The attack obviously threatens
    the privacy of machine learning as the acquired training examples inherently unveil
    sensitive information. Based on current knowledge, the generalized features embedded
    in the gradient or model parameters cannot facilitate the precise reconstruction
    or extraction of training examples because these features are common. Consequently,
    the underlying reasons for the inversion/extraction risk potentially come from
    the memorization phenomenon.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在反演/提取攻击中，攻击者试图通过利用梯度或模型来重建或提取训练例子。攻击显然威胁到机器学习的隐私，因为获得的训练例子本质上揭示了敏感信息。根据现有知识，嵌入在梯度或模型参数中的广义特征无法促进训练例子的精确重建或提取，因为这些特征是常见的。因此，反演/提取风险的根本原因可能来自记忆现象。
- en: Related work mainly analyses the memorization effect concerning the extraction
    risk of language tasks [[14](#bib.bib14), [9](#bib.bib9), [29](#bib.bib29)]. Carlini
    et al. [[14](#bib.bib14)] introduce a memorization exposure metric utilizing canary
    sequences and log-perplexity. Subsequently, they establish that successful extraction
    becomes feasible when the level of memorization exposure surpasses a threshold.
    Conversely, extraction remains unsuccessful below this threshold. Consequently,
    it can be inferred that memorized examples carry a substantial risk of extraction.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 相关工作主要分析了记忆效果与语言任务提取风险的关系[[14](https://example.org/bib.bib14), [9](https://example.org/bib.bib9),
    [29](https://example.org/bib.bib29)]。Carlini等人[[14](https://example.org/bib.bib14)]引入了一种利用金丝雀序列和对数困惑度的记忆暴露度量。随后，他们建立了当记忆暴露度超过某个阈值时，成功提取变得可行的结论。相反，低于该阈值时提取则会失败。因此，可以推断，记忆的例子存在显著的提取风险。
- en: Another work [[9](#bib.bib9)] directly demonstrates the performance of their
    proposed extraction attack applied to GPT-2\. The researchers generate an extensive
    dataset through unconditional sampling from the model and employ diverse metrics
    to identify examples exhibiting high memorization likelihood. Consequently, they
    find the extraction result actually consists of trivial memorization and atypical
    information. We can explain the outcome originating from two distinct inversion/extraction
    mechanisms. The first mechanism rebuilds representations of common knowledge based
    on patterns. Another mechanism extracts atypical and individual examples exactly
    depending on memorized data. If we consider the two through the lens of privacy,
    the former mechanism supports the main task of DNNs while the latter apparently
    breaks privacy.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项工作[[9](https://example.org/bib.bib9)]直接展示了他们提出的提取攻击在GPT-2上的表现。研究人员通过模型的无条件采样生成了一个广泛的数据集，并采用多种度量方法来识别显示出高记忆可能性的例子。因此，他们发现提取结果实际上由琐碎的记忆和非典型信息组成。我们可以从两种不同的反演/提取机制来解释这个结果。第一种机制基于模式重建常识的表示。另一种机制则是准确地依赖于记忆数据提取非典型和个体例子。如果我们从隐私的角度考虑这两种机制，前者支持深度神经网络的主要任务，而后者显然破坏了隐私。
- en: Moreover, if we acknowledge the robust correlation between memorization and
    the inversion/extraction risk, the measurement outcomes of inversion/extraction
    risk can be regarded as an empirical lower-bound of memorization [[29](#bib.bib29)].
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们承认记忆和反演/提取风险之间的强相关性，则反演/提取风险的测量结果可以被视为记忆的经验下限[[29](https://example.org/bib.bib29)]。
- en: 'Summary. Previous studies indicate the vulnerability of inversion/extraction
    risks is highly relevant to the memorization effect within models. Despite the
    absence of direct experimental evidence, we attempt to reveal two inversion/extraction
    mechanisms: one where the attack reconstructs representations of common knowledge
    through generalized patterns, and another where it precisely extracts exceptional
    and individual examples using memorized data. Consequently, we deduce that the
    memorization effect constitutes the foundational cause of privacy risk in inversion/extraction
    attacks. Furthermore, the outcomes of such attacks can serve as a lower-bound
    approximation of model memorization.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。先前的研究表明，反演/提取风险的脆弱性与模型中的记忆效果高度相关。尽管缺乏直接的实验证据，我们尝试揭示两种反演/提取机制：一种是攻击通过广义模式重建常识的表示，另一种是通过记忆数据精确提取特殊和个体例子。因此，我们推测记忆效果构成了反演/提取攻击中隐私风险的根本原因。此外，此类攻击的结果可以作为模型记忆的下限近似值。
- en: V-C Memorization and Poisoning Risks
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 记忆和中毒风险
- en: Poisoning attacks target breaking model availability. Specifically, adversaries
    attempt to degrade model performance on all examples (i.e. untargeted poisoning
    attack) or specific classes or examples (i.e. targeted poisoning attack), even
    examples with particular features (i.e. backdoor attack). Common poisoning techniques
    include data manipulation, called data poisoning, and model corruption, called
    model poisoning. As model poisoning is generally used in distribution machine
    learning systems, we mainly discuss data poisoning including label manipulation
    and input noise corruption.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 中毒攻击的目标是破坏模型的可用性。具体而言，对手试图降低模型对所有样本（即非目标中毒攻击）或特定类别或样本（即目标中毒攻击），甚至具有特定特征的样本（即后门攻击）的性能。常见的中毒技术包括数据操控，称为数据中毒，以及模型损坏，称为模型中毒。由于模型中毒通常用于分布式机器学习系统，我们主要讨论数据中毒，包括标签操控和输入噪声损坏。
- en: Randomly labeled examples cannot be classified to the assigned noisy class based
    on pattern learning due to the absence of highly shared features. However, based
    on empirical results [[8](#bib.bib8)], we know that DNNs can minimize the training
    loss of randomly labeled datasets and achieve almost perfect accuracy. Therefore,
    randomly labeled examples are memorized and we can infer that data poisoning via
    label manipulation depends on the memorization effect. Arpit et al. [[25](#bib.bib25)]
    provides more effective evidence that model performance reduction is quantitatively
    corresponding to the proportion of mislabeled examples. Additionally, a recent
    study [[15](#bib.bib15)] finds if drop memorization-associated neurons, mislabeled
    examples cannot be effectively classified. However, they also demonstrate atypical
    examples and noisy examples are hard to identify for DNNs and generalization may
    be damaged by dropping memorization-associated neurons. Memorization-dilation [[19](#bib.bib19)]
    based on neural collapse provides an explanation that mislabeled examples may
    degrade the neural collapse and damage model generalization.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 随机标记的样本由于缺乏高度共享的特征，无法基于模式学习被分类到分配的噪声类。然而，根据经验结果[[8](#bib.bib8)]，我们知道深度神经网络（DNNs）可以最小化随机标记数据集的训练损失，并达到几乎完美的准确率。因此，随机标记的样本被记忆，我们可以推断，通过标签操控的数据中毒依赖于记忆效应。Arpit
    等人[[25](#bib.bib25)]提供了更有效的证据，表明模型性能的下降与误标记样本的比例在数量上是相关的。此外，最近的研究[[15](#bib.bib15)]发现，如果去除与记忆相关的神经元，误标记的样本无法有效分类。然而，他们也表明，异常样本和噪声样本对于DNNs而言难以识别，并且去除记忆相关神经元可能会损害泛化能力。基于神经崩溃的记忆扩展[[19](#bib.bib19)]提供了一种解释，说明误标记样本可能会降低神经崩溃并损害模型的泛化能力。
- en: Another technology of data poisoning is adding random noise to the input. As
    noise increases, we can infer the inputs may gradually transform from typical
    examples to atypical examples then full noise. Concurrently, DNNs have been forced
    to minimize the loss so models may boost the memorization of such inputs.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒的另一种技术是向输入中添加随机噪声。随着噪声的增加，我们可以推断，输入可能逐渐从典型样本转变为非典型样本，然后完全被噪声覆盖。同时，DNNs被迫最小化损失，因此模型可能会增强对这种输入的记忆。
- en: Summary. When an adversary launches a poisoning attack with mislabeled data,
    neural networks would memorize these data to minimize the loss. Thus, memorization
    is an adaptive process and the vulnerability to poisoning attack comes from the
    DNNs training framework.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。当对手发起数据中毒攻击时，神经网络会记忆这些数据以最小化损失。因此，记忆是一个适应性过程，对中毒攻击的脆弱性来自于DNNs的训练框架。
- en: V-D Memorization and Adversarial Risks
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 记忆与对抗风险
- en: The adversarial attack employs adversarial noise on inputs to drive examples
    approaching the decision boundary and achieving the maximum loss. Generally, the
    adversarial noise is generated by gradient ascending [[68](#bib.bib68), [69](#bib.bib69)].
    An effective defense strategy is adversarial training [[68](#bib.bib68)] which
    means directly training on the adversarial examples and this method provides a
    lower-bound robustness guarantee.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击在输入上施加对抗噪声，以使样本接近决策边界并达到最大损失。一般来说，对抗噪声是通过梯度上升生成的[[68](#bib.bib68), [69](#bib.bib69)]。一种有效的防御策略是对抗训练[[68](#bib.bib68)]，即直接对对抗样本进行训练，这种方法提供了下界鲁棒性保证。
- en: In spite of the absence of relevant studies on memorization and adversarial
    attacks, we can infer that the memorization effect is not the source of adversarial
    vulnerability because existing work [[70](#bib.bib70)] believes adversarial vulnerability
    derives from non-robust features.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管缺乏有关记忆化和对抗攻击的相关研究，我们可以推断记忆化效应并不是对抗脆弱性的根源，因为现有的研究[[70](#bib.bib70)]认为对抗脆弱性源于非鲁棒特征。
- en: Many works [[65](#bib.bib65), [22](#bib.bib22), [26](#bib.bib26)] investigate
    memorization in adversarial training. Li et al. [[22](#bib.bib22)] find adversarial
    examples are very atypical and Schmidt et al. [[71](#bib.bib71)] demonstrate the
    complexity of adversarial examples can be significantly larger than normal examples
    in standard learning, thus DNNs memorize them during adversarial training making
    DNNs more vulnerable to privacy attacks. Another work [[65](#bib.bib65)] exhibits
    that memorizing atypical examples hardly helps adversarial robustness and when
    memorizing some harmful atypical examples that share similar features with a “wrong"
    class, the boundary becomes blurred, and this damages robustness. This phenomenon
    may be explained by robust overfitting that one-hot labels can be inappropriate
    and some adversarial examples should be given low confidence [[72](#bib.bib72)].
    Researchers [[26](#bib.bib26)] are also curious about randomly labeled dataset
    performance in adversarial training and they find PGD-AT [[68](#bib.bib68)] fails
    to converge while TRADES [[73](#bib.bib73)] still reaches nearly 100% training
    accuracy. However, they believe DNNs have sufficient capacity to memorize adversarial
    examples of training data with completely random labels, but the convergence depends
    on the AT algorithms. Next, they analyze the gradients of the two different adversarial
    training methods and recognize the gradient of PGD-AT performs large variance,
    making it fail to converge. Moreover, they study robust overfitting and put it
    down to excessive memorization of one-hot labels breaking the robust decision
    boundary.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究[[65](#bib.bib65), [22](#bib.bib22), [26](#bib.bib26)]探讨了对抗训练中的记忆化。Li 等人[[22](#bib.bib22)]发现对抗样本非常不典型，而
    Schmidt 等人[[71](#bib.bib71)]展示了对抗样本的复杂性可能远大于标准学习中的正常样本，因此 DNN 在对抗训练过程中记忆了这些样本，使得
    DNN 更容易受到隐私攻击。另一项研究[[65](#bib.bib65)]显示，记忆不典型样本几乎无法帮助对抗鲁棒性，当记忆一些有害的不典型样本，这些样本与“错误”类别共享相似特征时，边界变得模糊，这会损害鲁棒性。这种现象可能可以通过鲁棒过拟合来解释，即一热标签可能不合适，一些对抗样本应给予低置信度[[72](#bib.bib72)]。研究人员[[26](#bib.bib26)]也对随机标记数据集在对抗训练中的表现感到好奇，他们发现
    PGD-AT[[68](#bib.bib68)] 无法收敛，而 TRADES[[73](#bib.bib73)] 仍能达到接近 100% 的训练准确率。然而，他们认为
    DNN 具有足够的能力来记忆具有完全随机标签的对抗样本，但收敛性取决于对抗训练算法。接下来，他们分析了两种不同对抗训练方法的梯度，并认识到 PGD-AT 的梯度表现出较大的方差，使其无法收敛。此外，他们研究了鲁棒过拟合，并将其归因于过度记忆一热标签破坏了鲁棒决策边界。
- en: Summary. It seems that the memorization effect is not responsible for adversarial
    vulnerability. In the adversarial training, due to the hardness of examples, most
    adversarial examples will be memorized increasing privacy leakage. Particularly,
    memorizing examples sharing similar features with a “wrong" class or excessive
    memorization of one-hot labels may corrupt the decision boundary.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。看起来记忆化效应与对抗脆弱性无关。在对抗训练中，由于样本的难度，大多数对抗样本会被记忆，从而增加隐私泄露。特别是，记忆那些与“错误”类别共享相似特征的样本或过度记忆一热标签可能会破坏决策边界。
- en: V-E Memorization and Differential Privacy
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E 记忆化与差分隐私
- en: 'Differential Privacy [[74](#bib.bib74)] (DP) is a commonly employed strategy
    for defending against privacy attacks, which aims to guarantee indistinguishability
    between various data points. In particular, DP ensures that the trained model
    remains largely unchanged when any single example is removed from the training
    set. Within the framework of $(\epsilon,\delta)$-DP setting, DP comprises two
    key components: gradient clipping and the application of noise. Gradient clipping
    restricts the gradients of each example to a predefined boundary, reducing disparities
    in their gradient magnitudes. This facilitates the standardization of gradients
    in terms of magnitude and mitigates the memorization effect. The phenomenon has
    been observed in some cases [[75](#bib.bib75), [24](#bib.bib24)]. Additionally,
    the random noise application also promotes example memorization reduction. When
    models are trained on examples mixed with random noise, the features carried by
    long-tailed examples or atypical features will be diluted, leading the models
    to mainly learn typical patterns. Moreover, neural networks may memorize artificial
    random noise [[13](#bib.bib13), [11](#bib.bib11), [9](#bib.bib9)]. Because we
    always observe that effective DP measures hurt model generalization. Corresponding,
    from a privacy standpoint, we can understand that DP operates by safeguarding
    privacy through the prevention of atypical feature memorization. While a comprehensive
    analysis of DP from a memory perspective is currently lacking, some researchers
    agree that DP can limit example memorization supported by empirical results [[24](#bib.bib24),
    [64](#bib.bib64), [14](#bib.bib14)].'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私[[74](#bib.bib74)]（DP）是一种常用的防御隐私攻击的策略，其目标是保证不同数据点之间的不可区分性。特别是，DP确保在训练集中移除任何单个示例时，训练后的模型保持基本不变。在$(\epsilon,\delta)$-DP设置框架下，DP包括两个关键组件：梯度裁剪和噪声应用。梯度裁剪将每个示例的梯度限制在预定义的边界内，从而减少它们梯度幅度的差异。这有助于在幅度上标准化梯度，并减轻记忆效应。这种现象在一些案例中已经被观察到[[75](#bib.bib75),
    [24](#bib.bib24)]。此外，随机噪声的应用也促进了示例记忆的减少。当模型在混有随机噪声的示例上进行训练时，长尾示例或非典型特征所携带的特征会被稀释，从而使模型主要学习典型模式。此外，神经网络可能会记住人工随机噪声[[13](#bib.bib13),
    [11](#bib.bib11), [9](#bib.bib9)]。因为我们总是观察到有效的DP措施会损害模型的泛化能力。因此，从隐私角度来看，我们可以理解DP通过防止对非典型特征的记忆来保护隐私。虽然目前从记忆角度对DP的全面分析尚缺乏，但一些研究者同意DP可以通过实证结果限制示例记忆[[24](#bib.bib24),
    [64](#bib.bib64), [14](#bib.bib14)]。
- en: Summary. DP serves as an effective measure to alleviate the issue of example
    memorization in neural networks, achieved through gradient clipping and the introduction
    of noise. Nevertheless, the efficacy of DP may be guaranteed by reducing the memorization
    of atypical features and increasing the learning of random noise.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。差分隐私（DP）作为缓解神经网络中示例记忆问题的有效措施，通过梯度裁剪和噪声引入来实现。然而，DP的有效性可能通过减少对非典型特征的记忆和增加对随机噪声的学习来保证。
- en: V-F Memorization and Other Risks
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-F 记忆与其他风险
- en: As DNNs have been widely applied in various social scenarios, the public gradually
    shifted its focus from system performance to additional attributes such as model
    fairness [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80)], interpretability [[81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [84](#bib.bib84), [85](#bib.bib85)], and others. These additional attributes potentially
    exhibit a strong relationship with the phenomenon of memorization, consequently
    leading to additional risks. In terms of fairness, DNNs may inadvertently learn
    societal biases present in the training data from the real world [[86](#bib.bib86)].
    Such biases come from unbalanced subgroups or sensitive attributes, which correlates
    with the long-tailed theory [[13](#bib.bib13)]. This indicates the significant
    role of memorization in the risk of unfairness. Furthermore, certain approaches [[87](#bib.bib87),
    [88](#bib.bib88)] aimed at promoting fairness employ techniques like data augmentation
    and weight reassignment, which may amplify privacy leakage via memorization learning.
    Regarding interpretability, the public views uninterpretable models and predictions
    as uncontrolled risks. Memorization study can help mitigate this risk. Additionally,
    the phenomenon of memorization is also associated with certain risks of violation
    of intellectual property rights or copyrights.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度神经网络（DNNs）在各种社会场景中的广泛应用，公众逐渐将关注点从系统性能转移到如模型公平性[[76](#bib.bib76), [77](#bib.bib77),
    [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)]、可解释性[[81](#bib.bib81), [82](#bib.bib82),
    [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85)]等附加属性上。这些附加属性可能与记忆现象有强烈的关系，从而带来额外的风险。在公平性方面，DNNs可能无意中学习到训练数据中存在的社会偏见[[86](#bib.bib86)]。这些偏见源于不平衡的子群体或敏感属性，与长尾理论相关[[13](#bib.bib13)]。这表明记忆在公平性风险中的重要作用。此外，一些旨在促进公平性的措施[[87](#bib.bib87),
    [88](#bib.bib88)]采用了数据增强和权重重新分配等技术，这可能通过记忆学习放大隐私泄露。关于可解释性，公众将不可解释的模型和预测视为无法控制的风险。记忆研究可以帮助减轻这一风险。此外，记忆现象还与侵犯知识产权或版权的某些风险相关联。
- en: Summary. We believe that the memorization effect has strong relationships with
    fairness, interpretability, and other risks. However, this area of research remains
    largely unexplored.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。我们认为记忆效应与公平性、可解释性和其他风险有强烈关系。然而，这一研究领域仍然未被充分探索。
- en: VI Forgetting Research
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 遗忘研究
- en: 'Forgetting is the opposite of memorization. Generally, neural networks may
    encounter difficulties in continual learning because the learning capacity of
    networks is not infinite. During iterative training, as networks train on new
    examples, they tend to forget learned features or information from previous examples,
    as shown in Figure [6](#S6.F6 "Figure 6 ‣ VI Forgetting Research ‣ Memorization
    in deep learning: A survey"). This phenomenon is known as catastrophic forgetting [[89](#bib.bib89),
    [90](#bib.bib90)]. Variations in data distributions cause models to converge to
    different optimal points. Although there are some methods to overcome this phenomenon [[91](#bib.bib91),
    [90](#bib.bib90), [89](#bib.bib89), [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94),
    [91](#bib.bib91)], we still lack an understanding of forgetting especially as
    an opposite of memorization. We may be curious about what information will be
    forgotten, how the forgetting effect impacts model performance and privacy, and
    its relationship with memorization. The main works are presented in Table [IV](#S6.T4
    "TABLE IV ‣ VI Forgetting Research ‣ Memorization in deep learning: A survey").'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '遗忘是记忆的对立面。一般来说，神经网络在持续学习中可能会遇到困难，因为网络的学习能力不是无限的。在迭代训练过程中，随着网络对新示例的训练，它们往往会遗忘从之前示例中学到的特征或信息，如图[6](#S6.F6
    "Figure 6 ‣ VI Forgetting Research ‣ Memorization in deep learning: A survey")所示。这种现象被称为灾难性遗忘[[89](#bib.bib89),
    [90](#bib.bib90)]。数据分布的变化导致模型收敛到不同的最优点。尽管有一些方法可以克服这种现象[[91](#bib.bib91), [90](#bib.bib90),
    [89](#bib.bib89), [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [91](#bib.bib91)]，但我们对遗忘的理解仍然不足，特别是作为记忆的对立面。我们可能对将会遗忘哪些信息、遗忘效应如何影响模型性能和隐私、以及其与记忆的关系感到好奇。主要工作见表[IV](#S6.T4
    "TABLE IV ‣ VI Forgetting Research ‣ Memorization in deep learning: A survey")。'
- en: 'TABLE IV: Main Works about Forgetting'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：关于遗忘的主要工作
- en: '| Main Topic | Reference | Background Task | Main Eval. Method | Main Findings
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 主要主题 | 参考文献 | 背景任务 | 主要评估方法 | 主要发现 |'
- en: '| Forgetting about Data | Toneva et al. 2019 [[95](#bib.bib95)] | Supervised
    Classification Task | Forgetting and Learning Event | Atypical examples and noisy
    examples are prone to be forgotten. |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 关于数据的遗忘 | Toneva 等 2019 [[95](#bib.bib95)] | 监督分类任务 | 遗忘和学习事件 | 不典型样本和噪声样本容易被遗忘。
    |'
- en: '|  | Maini et al. 2022 [[96](#bib.bib96)] | Supervised Classification Task
    | Second-Split Forgetting Time | In fine-tune, noisy examples are forgotten quickly
    and seemingly atypical examples are forgotten slowly. |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | Maini 等 2022 [[96](#bib.bib96)] | 监督分类任务 | 第二次拆分遗忘时间 | 在微调中，噪声样本被迅速遗忘，而似乎不典型的样本被缓慢遗忘。
    |'
- en: '| Forgetting about Privacy | Jagielski et al. 2022 [[67](#bib.bib67)] | Various
    Tasks | Probabilistic Memorization Evaluation | Standard image, speech, and language
    models empirically do forget examples over time. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 关于隐私的遗忘 | Jagielski 等 2022 [[67](#bib.bib67)] | 各种任务 | 概率记忆评估 | 标准图像、语音和语言模型在经验上确实会随着时间的推移遗忘样本。
    |'
- en: '![Refer to caption](img/cf09ca8180cfc702b948943444b36390.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cf09ca8180cfc702b948943444b36390.png)'
- en: 'Figure 6: Demonstration of the Forgetting Phenomenon.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 遗忘现象的演示。'
- en: VI-A Forgetting Definition and Evaluation
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 遗忘定义和评估
- en: VI-A1 Forgetting Definition based on Accuracy
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A1 基于准确性的遗忘定义
- en: 'Toneva et al. [[95](#bib.bib95)] propose the forgetting and learning event:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Toneva 等 [[95](#bib.bib95)] 提出了遗忘和学习事件：
- en: Definition 8
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 8
- en: Forgetting and Learning Event. For supervised classification task, given an
    example $\mathbf{x}_{i}$, the predicted label for example $\mathbf{x}_{i}$ obtained
    after $t$ steps of SGD is $\hat{y}_{i}^{t}=\arg\max_{k}p(y_{ik}|\mathbf{x}_{i};\theta^{t})$
    and accuracy is $acc_{i}^{t}=\vmathbb{1}_{\hat{y}^{t}_{i}=y_{i}}$. Therefore,
    the forgetting event is that example $i$ is misclassified at step $t+1$ after
    having been correctly classified at step $t$ (i.e. $acc^{t}_{i}>acc^{t+1}_{i}$).
    Conversely, a learning event has occurred if $acc^{t}_{i}<acc^{t+1}_{i}$.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 遗忘和学习事件。对于监督分类任务，给定样本 $\mathbf{x}_{i}$，在 $t$ 步 SGD 之后获得的样本 $\mathbf{x}_{i}$
    的预测标签是 $\hat{y}_{i}^{t}=\arg\max_{k}p(y_{ik}|\mathbf{x}_{i};\theta^{t})$，准确率是
    $acc_{i}^{t}=\vmathbb{1}_{\hat{y}^{t}_{i}=y_{i}}$。因此，遗忘事件是样本 $i$ 在步骤 $t+1$ 时被错误分类，而在步骤
    $t$ 时被正确分类（即 $acc^{t}_{i}>acc^{t+1}_{i}$）。相反，如果 $acc^{t}_{i}<acc^{t+1}_{i}$，则发生了学习事件。
- en: Following the forgetting and learning event definitions, Maini et al. [[96](#bib.bib96)]
    definite First-Split Learning Time (FSLT) to demonstrate the first epoch that
    model learns an example and Second-Split Forgetting Time (SSFT) to describe the
    forgetting time in the fine-tuned stage.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 根据遗忘和学习事件的定义，Maini 等 [[96](#bib.bib96)] 定义了第一次拆分学习时间（FSLT）以展示模型学习样本的第一次时期，以及第二次拆分遗忘时间（SSFT）以描述微调阶段的遗忘时间。
- en: Definition 9
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 9
- en: First-Split Learning Time. For $\{\mathbf{x_{i}},y_{i}\}\in D_{A}$, learning
    time is defined as the earliest epoch during the training of a classifier $f$
    on $D_{A}$ after which it is always classified correctly, i.e.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次拆分学习时间。对于 $\{\mathbf{x_{i}},y_{i}\}\in D_{A}$，学习时间定义为训练分类器 $f$ 在 $D_{A}$
    上时的最早时期，此时期之后它始终被正确分类，即
- en: '|  | $\displaystyle FSLT_{i}=\arg\min_{t^{*}}(\hat{y}_{i,(A)}^{t}=y_{i},\forall
    t\geq t^{*})\ \forall\{\mathbf{x_{i}},y_{i}\}\in D_{A},$ |  | (8) |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle FSLT_{i}=\arg\min_{t^{*}}(\hat{y}_{i,(A)}^{t}=y_{i},\forall
    t\geq t^{*})\ \forall\{\mathbf{x_{i}},y_{i}\}\in D_{A},$ |  | (8) |'
- en: where $t$ denotes epoch, $A$ is the pre-training stage and $D_{A}$ is the training
    dataset. The $f_{A}$ represents the trained model with 100% training accuracy
    on the $D_{A}$.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t$ 表示时期，$A$ 是预训练阶段，$D_{A}$ 是训练数据集。$f_{A}$ 代表在 $D_{A}$ 上具有 100% 训练准确率的训练模型。
- en: Definition 10
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 10
- en: Second-Split Forgetting Time. Let $\hat{y}_{i,(A\rightarrow B)}^{t}$ to denote
    the prediction of example $\{\mathbf{x_{i}},y_{i}\}\in D_{A}$ after training $f_{(A\rightarrow
    B)}$ for $t$ epochs on $D_{B}$. Then, for $\{\mathbf{x_{i}},y_{i}\}\in D_{A}$
    forgetting time is defined as the earliest epoch after which it is never classified
    correctly, i.e.,
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次拆分遗忘时间。令 $\hat{y}_{i,(A\rightarrow B)}^{t}$ 表示在 $D_{B}$ 上训练 $f_{(A\rightarrow
    B)}$ $t$ 个时期后，样本 $\{\mathbf{x_{i}},y_{i}\}\in D_{A}$ 的预测。然后，对于 $\{\mathbf{x_{i}},y_{i}\}\in
    D_{A}$，遗忘时间定义为从该时期之后它再也不能被正确分类的最早时期，即
- en: '|  | $\displaystyle SSFT_{i}=\arg\min_{t^{*}}(\hat{y}_{i,(A\rightarrow B)}^{t}\neq
    y_{i},\forall t\geq t^{*})\ \forall\{\mathbf{x_{i}},y_{i}\}\in D_{A},$ |  | (9)
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle SSFT_{i}=\arg\min_{t^{*}}(\hat{y}_{i,(A\rightarrow B)}^{t}\neq
    y_{i},\forall t\geq t^{*})\ \forall\{\mathbf{x_{i}},y_{i}\}\in D_{A},$ |  | (9)
    |'
- en: where $D_{B}$ is a held-out split dataset (without $\{\mathbf{x_{i}},y_{i}\}$)
    of $D_{A}$, $f_{(A\rightarrow B)}$ is initialized by $f_{A}$.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D_{B}$ 是一个从 $D_{A}$ 中保留的拆分数据集（不包含 $\{\mathbf{x_{i}},y_{i}\}$），$f_{(A\rightarrow
    B)}$ 由 $f_{A}$ 初始化。
- en: These definitions can be used to measure forgetting in supervised classification
    tasks. Based on the nature of forgetting, i.e. learned features have been lost,
    it is reasonable to obverse forgetting depending on the accuracy.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这些定义可以用于测量监督分类任务中的遗忘。根据遗忘的性质，即学习到的特征已丢失，根据准确性观察遗忘是合理的。
- en: VI-A2 Forgetting Definition based on Membership Inference Attack
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A2 基于成员推断攻击的遗忘定义
- en: Jagielski et al. [[67](#bib.bib67)] measure the ratio of forgetting based on
    a membership inference attack.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Jagielski 等人 [[67](#bib.bib67)] 基于成员推断攻击测量遗忘比率。
- en: Definition 11
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 11
- en: Rate of Forgetting. A training algorithm $\mathcal{T}$ is said to $(\mathcal{A},\alpha,k)$-forget
    a training example $z$ if, $k$ steps after $z$ is last used in $\mathcal{T}$,
    a privacy attack $\mathcal{A}$ achieves no higher than success rate $\alpha$.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 遗忘率。如果训练算法 $\mathcal{T}$ 在 $z$ 最后一次用于 $\mathcal{T}$ 后 $k$ 步，隐私攻击 $\mathcal{A}$
    达到的成功率不高于 $\alpha$，则称训练算法 $\mathcal{T}$ 对训练示例 $z$ 进行 $(\mathcal{A},\alpha,k)$-遗忘。
- en: We know that the membership inference attack relies on particular features,
    thus, the reduced risk could be regarded as forgetting. This is also an effective
    definition to describe forgetting.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，成员推断攻击依赖于特定特征，因此，降低的风险可以视为遗忘。这也是描述遗忘的有效定义。
- en: VI-B Forgetting Phenomenon
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 遗忘现象
- en: Some existing studies provide interesting evidence to understand the forgetting
    phenomenon. Toneva et al. [[95](#bib.bib95)] study example forgetting during DNN
    learning and use accuracy as the metric to define the example forgetting event,
    i.e. an example that has been correctly classified becomes misclassified. They
    find that generalized examples are unforgettable (always correctly classified)
    but atypical examples and noisy examples are prone to be forgotten. This is connected
    with the related memorization findings [[13](#bib.bib13), [11](#bib.bib11)] that
    forgetting occurs on those memorized examples. Even with some intermediate forgetting
    events, DNNs still can correctly classify all training examples and finally achieve
    100% training accuracy, indicating memorization is a challenging but forced learning
    stage. Another finding is removing a part of these generalized/unforgettable examples
    will not damage the model generalization performance. This may be explained as
    DNNs do not need to repeatedly learn common patterns.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 一些现有研究提供了有趣的证据来理解遗忘现象。Toneva 等人 [[95](#bib.bib95)] 研究了 DNN 学习过程中的示例遗忘，并使用准确率作为度量来定义示例遗忘事件，即一个已经正确分类的示例变成了错误分类。他们发现泛化示例是难以遗忘的（始终被正确分类），但非典型示例和嘈杂示例容易被遗忘。这与相关的记忆发现
    [[13](#bib.bib13), [11](#bib.bib11)] 相关，即遗忘发生在那些被记忆的示例上。即使有一些中间遗忘事件，DNN 仍然能够正确分类所有训练示例，并最终达到
    100% 的训练准确率，这表明记忆是一个具有挑战性但被迫经历的学习阶段。另一个发现是，移除这些泛化/难以遗忘的示例不会损害模型的泛化性能。这可以解释为 DNN
    不需要重复学习常见模式。
- en: 'Following this research, Maini et al. [[96](#bib.bib96)] propose second-split
    forgetting time (SSFT) to track the epoch (if any) after which an original training
    example is forgotten as the network is fine-tuned on a randomly held-out partition
    of the data. In the fine-tuned stage, they demonstrate that noisy examples are
    forgotten quickly and seemingly atypical examples are forgotten slowly, while
    typical examples are never forgotten. Tirumala et al. [[16](#bib.bib16)] employ
    Definition [2](#Thmdefinition2 "Definition 2 ‣ II-A2 Exact Memorization ‣ II-A
    Memorization Definitions in Generalization Domain ‣ II Memorization Definition
    ‣ Memorization in deep learning: A survey") to measure the single-injected validation
    dataset forgetting dynamics and find the exact memorization from a higher point
    gradually drops to the forgetting baseline as the number of epoch increases. The
    forgetting baseline may represent generalization.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项研究之后，Maini 等人 [[96](#bib.bib96)] 提出了第二次遗忘时间（SSFT）来追踪在网络在随机保留的数据分区上微调后的原始训练样本被遗忘的时期（如果有的话）。在微调阶段，他们展示了嘈杂的样本被迅速遗忘，而看似非典型的样本被缓慢遗忘，而典型的样本则从未被遗忘。Tirumala
    等人 [[16](#bib.bib16)] 使用定义 [2](#Thmdefinition2 "Definition 2 ‣ II-A2 Exact Memorization
    ‣ II-A Memorization Definitions in Generalization Domain ‣ II Memorization Definition
    ‣ Memorization in deep learning: A survey") 测量单一注入验证数据集的遗忘动态，并发现从更高点的精确记忆逐渐下降到遗忘基线，随着周期数的增加。遗忘基线可能代表泛化。'
- en: Jagielski et al. [[67](#bib.bib67)] focus on the privacy risk associated with
    forgetting. They utilize the membership inference probability to evaluate forgetting
    and believe that the size of the training dataset, repetitions, and hardness mainly
    influence forgetting. Examples used early in model training may be more robust
    to privacy attacks, while repeated examples are harder to forget. During the forgetting
    phase, the membership inference probability of typical examples is still around
    50% which is lower than the inference risk of atypical examples. This finding
    corresponds to previous studies. They also illustrate that non-convexity and deterministic
    SGD can prevent forgetting. Another piece of evidence is the variation in local
    data distribution of every batch of data leads optimization techniques to converge
    to different local optimal points. Therefore, some lack-of-representativeness
    information may be lost.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Jagielski 等 [[67](#bib.bib67)] 关注与遗忘相关的隐私风险。他们利用成员推断概率来评估遗忘，并认为训练数据集的大小、重复次数和难度主要影响遗忘。模型训练早期使用的示例可能对隐私攻击更具鲁棒性，而重复示例则更难遗忘。在遗忘阶段，典型示例的成员推断概率仍然约为50%，低于非典型示例的推断风险。这一发现与之前的研究相符。他们还表明，非凸性和确定性SGD可以防止遗忘。另一个证据是，每批数据的局部数据分布变化导致优化技术收敛到不同的局部最优点。因此，可能会丢失一些代表性不足的信息。
- en: Summary. In the single-task learning scenario, memorized examples based on the
    view of performance metrics are easy to forget. However, the low performance does
    not mean all features of these memorized examples have been forgotten because
    they still pose high inference risks compared to generalized examples.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。在单任务学习场景中，基于性能指标的记忆示例容易被遗忘。然而，低性能并不意味着这些记忆示例的所有特征都被遗忘，因为与泛化示例相比，它们仍然具有较高的推断风险。
- en: VII Application
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 应用
- en: 'Utilizing the memorization and forgetting effects of neural networks, researchers
    have developed various applications in several scenarios. We organize these applications
    in Tabel [V](#S7.T5 "TABLE V ‣ VII Application ‣ Memorization in deep learning:
    A survey").'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '利用神经网络的记忆和遗忘效应，研究人员在多个场景中开发了各种应用。我们将这些应用整理在表 [V](#S7.T5 "TABLE V ‣ VII Application
    ‣ Memorization in deep learning: A survey")中。'
- en: 'TABLE V: Related Application about Memorization and Forgetting'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 关于记忆和遗忘的相关应用'
- en: '| Application | Associated Memorization or Forgetting Effect | Technology |
    Reference |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 相关的记忆或遗忘效应 | 技术 | 参考 |'
- en: '| Noisy Label Learning | DNNs are prior to learning patterns in the early training
    stage. | Noise Control Scheduler | Han et al., 2018 [[97](#bib.bib97)]; Yao et
    al., 2020 [[98](#bib.bib98)] |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 噪声标签学习 | DNNs 在早期训练阶段优先学习模式。 | 噪声控制调度器 | Han 等，2018 [[97](#bib.bib97)]; Yao
    等，2020 [[98](#bib.bib98)] |'
- en: '|  |  | Regularization | Liu et al., 2020 [[99](#bib.bib99)]; Xia et al., 2020 [[100](#bib.bib100)]
    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 正则化 | Liu 等，2020 [[99](#bib.bib99)]; Xia 等，2020 [[100](#bib.bib100)]
    |'
- en: '|  | Pre-training on random labels can learn label-irrelevant generalized features.
    | Random Labels Pre-training | Pondenkandath et al., 2018 [[101](#bib.bib101)];
    Maennel et al., 2020 [[30](#bib.bib30)] |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | 在随机标签上的预训练可以学习与标签无关的泛化特征。 | 随机标签预训练 | Pondenkandath 等，2018 [[101](#bib.bib101)];
    Maennel 等，2020 [[30](#bib.bib30)] |'
- en: '| Example Enhancement | Long-tailed examples are prone to be memorized. | Example
    Reweighting | Zhou et al., 2022 [[102](#bib.bib102)]; Xu et al., 2023 [[65](#bib.bib65)];
    Zhang et al., 2023 [[103](#bib.bib103)] |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 示例增强 | 长尾示例容易被记住。 | 示例重加权 | Zhou 等，2022 [[102](#bib.bib102)]; Xu 等，2023 [[65](#bib.bib65)];
    Zhang 等，2023 [[103](#bib.bib103)] |'
- en: '| Privacy Audit and Protection | Memorization cause inference and extraction
    risks | Membership Inference and Extraction Attacks | Carlini et al., 2021 [[9](#bib.bib9)];
    Carlini et al., 2022 [[24](#bib.bib24)] |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 隐私审计和保护 | 记忆引发推断和提取风险 | 成员推断和提取攻击 | Carlini 等，2021 [[9](#bib.bib9)]; Carlini
    等，2022 [[24](#bib.bib24)] |'
- en: '|  |  | Memorization Suppression | Maini et al., 2023 [[15](#bib.bib15)] |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 记忆抑制 | Maini 等，2023 [[15](#bib.bib15)] |'
- en: '|  | The privacy of forgotten examples are relatively guaranteed. | Unlearning
    Technology | Zhu et al., 2020 [[104](#bib.bib104)] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  | 忘记的示例的隐私相对有保障。 | 反学习技术 | Zhu 等，2020 [[104](#bib.bib104)] |'
- en: '| Memorization Architecture | Explicit memorization helps specific task performance.
    | Explicit Memorization Structure | Khandelwal et al., 2019 [[105](#bib.bib105)];
    Yogatama et al., 2021 [[106](#bib.bib106)]; Guu et al., 2020 [[107](#bib.bib107)];
    Lewis et al., 2020 [[108](#bib.bib108)]; Lewis et al., 2020 [[109](#bib.bib109)];
    Wu et al., 2022 [[110](#bib.bib110)] |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 记忆架构 | 显式记忆有助于特定任务的性能。 | 显式记忆结构 | Khandelwal等人，2019[[105](#bib.bib105)]；Yogatama等人，2021[[106](#bib.bib106)]；Guu等人，2020[[107](#bib.bib107)]；Lewis等人，2020[[108](#bib.bib108)]；Lewis等人，2020[[109](#bib.bib109)]；Wu等人，2022[[110](#bib.bib110)]
    |'
- en: '|  | DNNs could be viewed as databases or knowledge-bases | DNN Database |
    Tay et al., 2022 [[111](#bib.bib111)] |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  | DNN可以被视为数据库或知识库 | DNN数据库 | Tay等人，2022[[111](#bib.bib111)] |'
- en: '| Model Editing | Language models memorize a lot of facts. | Memorization Neurons
    Modification | Dai et al., 2021 [[112](#bib.bib112)]; De Cao et al., 2021 [[113](#bib.bib113)];
    Mitchell et al., 2021 [[114](#bib.bib114)]; Meng et al., 2022 [[115](#bib.bib115),
    [116](#bib.bib116)]; Gupta et al., 2024 [[117](#bib.bib117)]. |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 模型编辑 | 语言模型记忆了大量事实。 | 记忆神经元修改 | Dai等人，2021[[112](#bib.bib112)]；De Cao等人，2021[[113](#bib.bib113)]；Mitchell等人，2021[[114](#bib.bib114)]；Meng等人，2022[[115](#bib.bib115)、[116](#bib.bib116)]；Gupta等人，2024[[117](#bib.bib117)]。
    |'
- en: VII-A Noisy Label Learning
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 噪声标签学习
- en: Deep learning with noisy labels is challenging, as neural networks have powerful
    memorization abilities that can completely memorize all noisy labels in the later
    training stage. However, this memorization phenomenon is utilizable.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 带有噪声标签的深度学习具有挑战性，因为神经网络在后期训练阶段具有强大的记忆能力，可以完全记忆所有噪声标签。然而，这种记忆现象是可利用的。
- en: Specifically, one effective method for learning with noisy labels involves selecting
    potentially clean learning examples in each iteration for training [[118](#bib.bib118),
    [119](#bib.bib119), [120](#bib.bib120), [98](#bib.bib98)]. In example selection,
    it is challenging to choose a reasonable threshold based on example loss to drop
    underlying noisy examples because dropping too many examples may lose some useful
    features and lead to lower accuracy [[119](#bib.bib119)]. According to previous
    studies [[25](#bib.bib25)], we know that DNNs usually learn easy patterns before
    overfitting the noisy examples, and the pattern learning period can be regarded
    as the early learning stage. Thus, in the early stage of noisy label learning,
    it is not necessary to drop a large number of examples and the threshold could
    be loose. As training epochs increase, the dropping rate will also increase to
    avoid noisy label memorization. Therefore, it requires to define a scheduler used
    to control the example selection. Han et al. [[119](#bib.bib119)] propose a novel
    pre-defined scheduler. The scheduler is a non-increasing function and is controlled
    by noise level and current epoch. At the start of training, the scheduler would
    not drop any examples, but as the training continues, the dropping rate would
    increase. However, this pre-defined scheduler may not be "optimal" and is limited
    to specific tasks and datasets. Yao et al. [[98](#bib.bib98)] improve the scheduler
    as an AutoML problem to conduct a search, which outperforms previous works.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，一种有效的噪声标签学习方法是在每次迭代中选择潜在的干净学习样本用于训练[[118](#bib.bib118)、[119](#bib.bib119)、[120](#bib.bib120)、[98](#bib.bib98)]。在样本选择过程中，基于样本损失选择合理的阈值来剔除潜在的噪声样本是一项挑战，因为剔除过多样本可能会丢失一些有用的特征，从而导致准确率下降[[119](#bib.bib119)]。根据以往研究[[25](#bib.bib25)]，我们知道深度神经网络通常会在过拟合噪声样本之前学习简单的模式，模式学习阶段可以看作是早期学习阶段。因此，在噪声标签学习的早期阶段，不必剔除大量样本，阈值可以设置得较宽松。随着训练周期的增加，剔除率也会增加，以避免噪声标签的记忆。因此，需要定义一个调度器来控制样本选择。Han等人[[119](#bib.bib119)]提出了一种新颖的预定义调度器。该调度器是一个非递增函数，由噪声水平和当前训练周期控制。在训练开始时，调度器不会剔除任何样本，但随着训练的继续，剔除率会增加。然而，这种预定义的调度器可能并非“最优”，并且仅限于特定的任务和数据集。Yao等人[[98](#bib.bib98)]将调度器改进为AutoML问题进行搜索，取得了优于以往工作的效果。
- en: Loss modification and regularization technology represent another approach to
    learning with noisy labels. Existing works [[99](#bib.bib99), [100](#bib.bib100)]
    attempt to control noise during the early-learning stage. Liu et al. [[99](#bib.bib99)]
    develop early-learning regularization (ELR). This regularization item can facilitate
    learning from clean examples by prioritizing pattern learning and restraining
    noise in the training dataset by maximizing the inner product between the model
    output and the targets. Specifically, the regularization item can diminish noisy
    examples’ effect on the gradient, implicitly preventing the memorization of wrong
    labels. Modifying the gradient update based on the differences between pattern-associated
    neurons and memorization-associated neurons can provide a similar regularization
    effect. Xia et al. [[100](#bib.bib100)] find parameters have different tendencies
    that some respond for fitting clean labels as critical parameters and some for
    memorization as non-critical parameters. It is possible to assess the parameter
    tendency in each iteration based on the inner product between the value of the
    parameters and the gradient with respect to the parameters. Then, performing positive
    normal gradient updates on critical parameters and applying weight decay only
    to non-critical parameters mitigates noise learning during the early learning
    stage. If the minimum classification error is achieved on the validation dataset,
    the training should be stopped early.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 损失修改和正则化技术代表了另一种处理噪声标签的学习方法。现有的研究[[99](#bib.bib99), [100](#bib.bib100)]尝试在早期学习阶段控制噪声。Liu等人[[99](#bib.bib99)]开发了早期学习正则化（ELR）。该正则化项通过优先考虑模式学习并通过最大化模型输出与目标之间的内积来抑制训练数据集中的噪声，从而促进从干净样本中学习。具体而言，该正则化项可以减小噪声样本对梯度的影响，隐含地防止错误标签的记忆。基于模式相关神经元和记忆相关神经元之间的差异来修改梯度更新可以提供类似的正则化效果。Xia等人[[100](#bib.bib100)]发现参数具有不同的趋势，一些参数响应于拟合干净标签，作为关键参数，而一些参数则用于记忆，作为非关键参数。可以在每次迭代中基于参数的值与梯度之间的内积来评估参数趋势。然后，对关键参数执行正向梯度更新，并仅对非关键参数应用权重衰减，可以在早期学习阶段减轻噪声学习。如果在验证数据集上达到最小分类误差，应提前停止训练。
- en: Another way to utilize the memorization effect in noisy label learning is pre-training,
    expecting that models can learn label-irrelevant useful features. Basically, the
    data distribution always has some low-level and label-irrelevant features. For
    instance, in colorful image classification, the data distribution contains colorful
    low-level features compared to the gray in random noise. One previous study [[101](#bib.bib101)]
    empirically proves that performing unsupervised pre-training in the form of training
    for classification with random labels may boost initialized training speed and
    improve the generalization performance. However, these label-irrelevant features
    are all low-level generalized features and can be easily learned in the normal
    training process. Maennel et al. [[30](#bib.bib30)] demonstrate that the pre-training
    on random labels may pose a positive effect or negative effect on downstream tasks.
    They show that aligned shallow layers improve the performance of downstream tasks.
    However, the neural activations at the deep layers may drop abruptly and permanently
    on downstream tasks due to specialization, which may impair downstream task performance.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在噪声标签学习中利用记忆效应的方法是预训练，期望模型可以学习与标签无关的有用特征。基本上，数据分布总是包含一些低级别和与标签无关的特征。例如，在彩色图像分类中，与随机噪声中的灰色相比，数据分布包含了彩色的低级别特征。一项早期的研究[[101](#bib.bib101)]通过实验证明，以随机标签进行分类训练的无监督预训练可能会加速初始化训练速度并提高泛化性能。然而，这些与标签无关的特征都是低级别的广泛特征，可以在正常训练过程中轻松学习。Maennel等人[[30](#bib.bib30)]展示了随机标签上的预训练可能对下游任务产生正面或负面效果。他们表明，对齐的浅层能够提高下游任务的性能。然而，深层的神经激活可能会由于专业化而急剧且永久性地下降，从而影响下游任务的性能。
- en: VII-B Example Enhancement
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 示例增强
- en: Data distribution is not uniform in the real environment. Atypical examples
    with low frequencies in the data distribution always exist and cause models to
    memorize them instead of learning patterns based on the long tail theory [[11](#bib.bib11),
    [13](#bib.bib13)]. Some existing research applies example reweighting technology
    to handle these long-tailed examples.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分布在实际环境中是不均匀的。数据分布中总是存在低频的非典型示例，这些示例会导致模型记住它们，而不是基于长尾理论学习模式[[11](#bib.bib11),
    [13](#bib.bib13)]。一些现有的研究采用了示例重加权技术来处理这些长尾示例。
- en: In self-supervised long-tailed representation learning, learning long-tailed
    examples is generally challenging. Zhou et al. [[102](#bib.bib102)] employ the
    memorization effect to boost the performance of contrastive learning on tail examples.
    Specifically, they attempt to identify tail examples and apply heavier augmentation,
    consistently improving the performance of these examples. Due to the high computational
    cost associated with tracking memorization using memorization scores [[11](#bib.bib11)].
    They tend to trace the historical losses of each example as memorization clues.
    Then, they construct the normalization of momentum losses which indicates the
    memorization level of examples. Based on the memorization level, a stronger information
    discrepancy between views will be constructed to emphasize the importance of tail
    examples. Their results demonstrate that the method is effective.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在自监督的长尾表征学习中，学习长尾示例通常是具有挑战性的。Zhou等人[[102](#bib.bib102)]利用记忆效应来提升对尾部示例的对比学习性能。具体来说，他们尝试识别尾部示例，并应用更强的增强方法，一致性地提高这些示例的性能。由于使用记忆评分跟踪记忆的高计算成本[[11](#bib.bib11)]，他们倾向于追踪每个示例的历史损失作为记忆线索。然后，他们构建了动量损失的归一化，这表明示例的记忆水平。基于记忆水平，将构建更强的信息差异以强调尾部示例的重要性。他们的结果表明，这种方法是有效的。
- en: For adversarial training, Xu et al. [[65](#bib.bib65)] discover that memorizing
    atypical examples is only effective in improving DNN’s accuracy on clean atypical
    examples but hardly improves their adversarial robustness. Moreover, fitting some
    atypical adversarial examples even damages the model’s robustness. They believe
    some atypical examples share similar features with a wrong class and become harmful
    in the context of adversarial training. This may uncover the key differences between
    traditional standard training and adversarial training. Motivated by their findings,
    they propose an algorithm called Benign Adversarial Training (BAT), which can
    mitigate the negative influence of memorizing those harmful atypical examples,
    simultaneously preserving the model’s ability to learn those useful/benign atypical
    examples. In BAT, the core is the example reweighting technology that assigns
    the harmful atypical examples a small weight to reduce the negative effect. Additionally,
    harmful atypical examples can be detected by the high memorization score and high
    misclassification confidence in the standard training. Another related work [[103](#bib.bib103)]
    also agrees the atypical examples may hurt DNN’s robustness. They similarly employ
    example reweighting technology to reassign example update weights. Their method
    actually builds a $k$NN structure to measure the example typicalness. This structure
    has been called the codebook and trained concurrently with the classifier. The
    codebook training is actually clustering based on distance, finding the central
    points approaching the nearest embedding vectors. These central points could be
    regarded as the most typical embedding vectors. Then, depending on the distance
    between input batch embedding and central vectors in the codebook, it is effective
    in identifying the atypical examples. Subsequently, the distances can be utilized
    as weights to enhance atypical example learning in standard training and reduce
    atypical example learning in adversarial training.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抗训练方面，Xu等人[[65](#bib.bib65)]发现，记忆非典型示例仅在提高DNN对干净非典型示例的准确性方面有效，但几乎不会提高其对抗鲁棒性。此外，拟合一些非典型对抗示例甚至会损害模型的鲁棒性。他们认为一些非典型示例与错误类别具有相似特征，在对抗训练中会变得有害。这可能揭示了传统标准训练和对抗训练之间的关键区别。受到他们发现的启发，他们提出了一种名为“良性对抗训练”（BAT）的算法，该算法可以减轻记忆这些有害非典型示例的负面影响，同时保持模型学习那些有用/良性非典型示例的能力。在BAT中，核心是示例重加权技术，它为有害非典型示例分配较小的权重，以减少负面效应。此外，通过在标准训练中高记忆分数和高误分类置信度可以检测到有害的非典型示例。另一项相关工作[[103](#bib.bib103)]也同意非典型示例可能会伤害DNN的鲁棒性。他们同样采用示例重加权技术来重新分配示例更新权重。他们的方法实际上建立了一个$k$NN结构来测量示例的典型性。这个结构被称为代码本，并与分类器同时训练。代码本训练实际上是基于距离的聚类，找到接近最近嵌入向量的中心点。这些中心点可以视为最典型的嵌入向量。然后，根据输入批次嵌入与代码本中中心向量之间的距离，有效地识别非典型示例。随后，可以利用这些距离作为权重，在标准训练中增强非典型示例学习，并在对抗训练中减少非典型示例学习。
- en: VII-C Privacy Audit and Protection
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 隐私审计与保护
- en: With a deeper understanding of the memorization phenomenon, some works [[75](#bib.bib75),
    [9](#bib.bib9), [24](#bib.bib24), [29](#bib.bib29), [14](#bib.bib14)] have demonstrated
    that memorized particular features become sources of risk for several attacks.
    Thus, researchers can apply the memorization effects to audit security risks and
    develop novel defense strategies.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对记忆现象的更深入理解，一些研究[[75](#bib.bib75), [9](#bib.bib9), [24](#bib.bib24), [29](#bib.bib29),
    [14](#bib.bib14)]已经证明，记忆中的特定特征成为多种攻击的风险源。因此，研究人员可以利用记忆效应来审计安全风险，并开发新的防御策略。
- en: 'In section [V](#S5 "V Underlying Risks of Memorization Learning ‣ Memorization
    in deep learning: A survey"), we have introduced memorization-associated risks.
    For membership inference attacks, it is possible to utilize the memorized particular
    features of examples to achieve high true-positive rates at low false-positive
    rates [[24](#bib.bib24)]. Moreover, extractable training examples also should
    depend on those particular features [[9](#bib.bib9)]. However, adversely, we actually
    can employ these attacks as memorization audit tools to help model compliance.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[V](#S5 "V 记忆学习的潜在风险 ‣ 深度学习中的记忆：调研")节中，我们介绍了与记忆相关的风险。对于成员推断攻击，可以利用记忆中的特定特征来在低假阳性率下实现高真实阳性率[[24](#bib.bib24)]。此外，可提取的训练示例也应依赖于这些特定特征[[9](#bib.bib9)]。然而，相反地，我们实际上可以将这些攻击作为记忆审计工具，帮助模型遵守规范。
- en: Additionally, mitigating the memorization effect may reduce the corresponding
    risks. Maini et al. [[15](#bib.bib15)] propose a novel dropout technology. This
    technology utilizes the memorization effect to guide the specific neurons to memorize
    the specific details of examples, while other neurons work for pattern learning.
    When the neural network drops these memorization neurons, those memorized particular
    features are dropped along with the neurons. Another promising strategy is applying
    the forgetting mechanism to unlearning technology. Private features of long-tailed
    examples could be forgotten under continuous learning with tail-changing data
    distribution. This means the new data distribution after removing the target unlearning
    example could not provide similar features for the target unlearning example,
    and features of the example will be gradually forgotten because parameters related
    to the example may be updated. Zhu et al. [[104](#bib.bib104)] apply the forgetting
    phenomenon without retraining to update a Transformer on the new integrated dataset
    without stale knowledge. However, it is still unsure if we can control the forgetting
    process and achieve the expected unlearning effect.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，减轻记忆效应可能会降低相关风险。Maini 等人 [[15](#bib.bib15)] 提出了一个新颖的 dropout 技术。该技术利用记忆效应来引导特定的神经元记忆示例的特定细节，而其他神经元则用于模式学习。当神经网络丢弃这些记忆神经元时，那些记忆的特定特征也会随着神经元一起丢弃。另一个有前景的策略是将遗忘机制应用于非学习技术。长尾示例的私有特征可以在连续学习过程中随着尾部变化的数据分布而被遗忘。这意味着在移除目标非学习示例后的新数据分布中，无法为目标非学习示例提供相似的特征，示例的特征将逐渐被遗忘，因为与示例相关的参数可能会被更新。Zhu
    等人 [[104](#bib.bib104)] 应用遗忘现象，在不重新训练的情况下更新 Transformer，以适应新的集成数据集而没有过时的知识。然而，我们是否可以控制遗忘过程并实现预期的非学习效果仍不确定。
- en: VII-D Memorization Architecture
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-D 记忆架构
- en: Although memorization leads to privacy risks, we may require the memorization
    ability of networks. Specifically, memorizing or caching common input and output
    can improve inference speed. Networks or large language models (LLMs) also can
    function as an information retrieval system. Despite this, some external memory
    structures like $k$NN and key-value memory could serve as additional components
    to improve task performance.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管记忆可能导致隐私风险，但我们可能需要网络的记忆能力。具体来说，记忆或缓存常见的输入和输出可以提高推理速度。网络或大型语言模型（LLMs）也可以作为信息检索系统。尽管如此，一些外部记忆结构，如
    $k$NN 和键值记忆，可以作为额外的组件来提高任务性能。
- en: Khandelwal et al. [[105](#bib.bib105)] attempt to combine the $k$NN and language
    model. They find that applying $k$NN as an external memorization structure can
    improve performance. Qualitatively, this kind of model is particularly helpful
    in predicting rare patterns that allow rare features to be memorized explicitly
    rather than implicitly in model parameters. Search based on similarity is better
    than predicting the next word in the long tail. Moreover, they also extend this
    method to the machine translation [[121](#bib.bib121)] and achieve non-trivial
    performance. Yogatama et al. [[106](#bib.bib106)] modify this approach by a gating
    mechanism and context compression retrieval.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: Khandelwal 等人 [[105](#bib.bib105)] 尝试将 $k$NN 和语言模型结合起来。他们发现，将 $k$NN 作为外部记忆结构应用可以提高性能。从定性上看，这种模型特别有助于预测稀有模式，使稀有特征能够被明确地记忆，而不是隐式地存在于模型参数中。基于相似性的搜索优于预测长尾中的下一个词。此外，他们还将这种方法扩展到机器翻译
    [[121](#bib.bib121)] 并取得了非平凡的性能。Yogatama 等人 [[106](#bib.bib106)] 通过门控机制和上下文压缩检索修改了这种方法。
- en: Additionally, the explicit key-value memory can also improve the effectiveness
    of inference. In the context of Transformers, Févry et al. [[122](#bib.bib122)]
    and Verga et al. [[123](#bib.bib123)] apply similar methods to install an external
    key-value memory to store entities and facts. With this explicit memorization,
    models can achieve higher performance in question-answering tasks.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，显式的键值记忆也可以提高推理的有效性。在 Transformer 的背景下，Févry 等人 [[122](#bib.bib122)] 和 Verga
    等人 [[123](#bib.bib123)] 应用类似的方法来安装外部键值记忆，以存储实体和事实。通过这种显式记忆，模型可以在问答任务中实现更高的性能。
- en: For information retrieval, REALM [[107](#bib.bib107)], MARGE [[108](#bib.bib108)],
    and RAG [[109](#bib.bib109)] apply knowledge retrieval in pre-training. REALM [[107](#bib.bib107)]
    augments language model pretraining with a knowledge retriever, which allows the
    model to retrieve documents from a large corpus, used during pre-training, fine-tuning,
    and inference. Next, MARGE [[108](#bib.bib108)] is trained by self-supervising
    the reconstruction of the target text. This process first involves retrieving
    a set of related texts (in many languages) and then maximizing their likelihood
    of generating the original. RAG [[109](#bib.bib109)] combines parametric memory
    and non-memory for language generation. The parametric memory is a pre-trained
    seq2seq model and the non-parametric memory is a dense vector index of Wikipedia.
    These pre-training models all gain non-trivial results in the downstream tasks
    like question-answering tasks. Regarding the latest studies, Wu et al. [[110](#bib.bib110)]
    envision language models that can simply read and memorize new data at inference
    time, thus acquiring new knowledge immediately. By using attention, a model can
    simply memorize facts (e.g. function definitions) by storing them as key-value
    pairs in long-term memory. Then, it can retrieve those facts later by creating
    a query that attends to them. In this case, attention acts as a form of information
    retrieval, allowing the model to look up facts that it has seen previously. Thus,
    they propose a Transformer model with $k$NN-augmented attention that unifies attention
    and retrieval. Their experiment demonstrates that an approximate $k$NN lookup
    into a non-differentiable memory of recent key-value pairs improves language modeling
    across various benchmarks and tasks.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 对于信息检索，REALM [[107](#bib.bib107)], MARGE [[108](#bib.bib108)], 和 RAG [[109](#bib.bib109)]
    在预训练中应用知识检索。REALM [[107](#bib.bib107)] 通过知识检索器增强语言模型预训练，使模型能够从大型语料库中检索文档，这些文档在预训练、微调和推理中都被使用。接下来，MARGE [[108](#bib.bib108)]
    通过自我监督目标文本的重建进行训练。这个过程首先涉及检索一组相关文本（用多种语言），然后最大化它们生成原始文本的可能性。RAG [[109](#bib.bib109)]
    将参数记忆和非参数记忆结合用于语言生成。参数记忆是预训练的seq2seq模型，而非参数记忆是维基百科的稠密向量索引。这些预训练模型在下游任务如问答任务中都取得了显著结果。关于最新研究，Wu等人 [[110](#bib.bib110)]
    设想了能够在推理时简单地读取和记忆新数据的语言模型，从而立即获取新知识。通过使用注意力机制，模型可以简单地将事实（例如函数定义）作为键值对存储在长期记忆中，然后可以通过创建关注这些事实的查询来检索这些事实。在这种情况下，注意力机制充当了一种信息检索形式，允许模型查找以前见过的事实。因此，他们提出了一种具有$k$NN增强注意力的Transformer模型，将注意力和检索统一起来。他们的实验表明，对最近键值对的非可微记忆进行近似$k$NN查找，在各种基准和任务中改善了语言建模。
- en: A more interesting work directly regards Transformer memory as a differentiable
    search index (DSI) [[111](#bib.bib111)]. All information about the corpus is encoded
    in the parameters of a Transformer. In other words, a DSI model answers queries
    directly using only its parameters, dramatically simplifying the whole retrieval
    process. At inference time, the trained model takes as input a text query and
    outputs the id of the correlated document.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 一项更有趣的工作直接将Transformer记忆视为可微分搜索索引（DSI） [[111](#bib.bib111)]。所有关于语料库的信息都被编码在Transformer的参数中。换句话说，DSI模型直接使用其参数回答查询，大大简化了整个检索过程。在推理时，训练好的模型将文本查询作为输入，并输出相关文档的ID。
- en: VII-E Model Editing
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-E 模型编辑
- en: Due to the computational burden of training large language models (LLMs) and
    the requirement of updating information in LLMs, researchers attempt to directly
    edit LLMs neurons to update facts [[112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117)]. LLMs learn a variety
    of facts about the world during pre-training and these facts are stored in model
    weights [[124](#bib.bib124)]. Specifically, the MLP weights actually serve as
    key-value memories [[125](#bib.bib125), [126](#bib.bib126), [115](#bib.bib115)].
    Thus, editing these neurons to update facts becomes a practical approach.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练大型语言模型（LLMs）的计算负担以及更新LLMs中信息的需求，研究人员尝试直接编辑LLMs神经元以更新事实 [[112](#bib.bib112),
    [113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116),
    [117](#bib.bib117)]。LLMs在预训练过程中学习了关于世界的各种事实，这些事实存储在模型权重中 [[124](#bib.bib124)]。具体来说，MLP权重实际上作为键值记忆 [[125](#bib.bib125),
    [126](#bib.bib126), [115](#bib.bib115)]。因此，编辑这些神经元以更新事实成为一种实际的方法。
- en: Dai et al. [[112](#bib.bib112)] first identify knowledge-containing neurons
    in a model using integrated gradients [[127](#bib.bib127)] and then modify the
    selected neurons to edit facts in a model. Specifically, they focus on evaluating
    BERT’s performance on the fill-in-the-blank cloze task. In this task, they introduce
    a technique called knowledge attribution, aiming to find the neurons in BERT that
    represent specific facts. Their analysis demonstrates a positive correlation between
    the activation of these identified ’knowledge neurons’ and the accurate expression
    of the corresponding facts. De Cao et al. [[113](#bib.bib113)] and Mitchell et
    al. [[114](#bib.bib114)] train a hypernetwork that predicts the new weights of
    the model being edited. This method modifies a fact rapidly without affecting
    the rest of the knowledge.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: Dai 等人 [[112](#bib.bib112)] 首先使用集成梯度 [[127](#bib.bib127)] 识别模型中的知识包含神经元，然后修改选定的神经元以编辑模型中的事实。具体来说，他们重点评估
    BERT 在填空克漏字任务中的表现。在这个任务中，他们引入了一种叫做知识归属的技术，旨在找出 BERT 中表示特定事实的神经元。他们的分析表明，这些被识别出的“知识神经元”的激活与相应事实的准确表达之间存在正相关。De
    Cao 等人 [[113](#bib.bib113)] 和 Mitchell 等人 [[114](#bib.bib114)] 训练了一个超网络来预测正在编辑模型的新权重。这种方法可以快速修改一个事实，而不影响其他知识。
- en: 'Meng et al. [[115](#bib.bib115), [116](#bib.bib116)] develop two "locating
    and editing" technologies: Rank-One Model Editing (ROME) and Mass-Editing Memory
    In a Transformer (MEMIT). They create a method for causal intervention to identify
    the activation of neurons crucial for a model’s factual predictions. Then, they
    directly update particular "knowledge-containing" components of the model without
    requiring to train additional models. Additionally, this approach is applicable
    to any transformer-based LLM. Subsequently, Gupta et al. [[117](#bib.bib117)]
    build a unifying conceptual framework for ROME and MEMIT following the preservation-memorization
    objective of model editing. During the editing process, this approach preserves
    the representations of certain selected vectors while memorizing the representations
    of new factual information. In summary, model editing demonstrates a good prospect
    of memorization utilization.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: Meng 等人 [[115](#bib.bib115), [116](#bib.bib116)] 开发了两种“定位和编辑”技术：Rank-One 模型编辑
    (ROME) 和变压器中的大规模编辑记忆 (MEMIT)。他们创建了一种因果干预的方法，以识别对模型事实预测至关重要的神经元的激活。然后，他们直接更新模型中特定的“知识包含”组件，而不需要训练额外的模型。此外，这种方法适用于任何基于变压器的
    LLM。随后，Gupta 等人 [[117](#bib.bib117)] 构建了一个统一的概念框架，用于 ROME 和 MEMIT，以遵循模型编辑的保存-记忆目标。在编辑过程中，这种方法保留了某些选定向量的表示，同时记忆新的事实信息的表示。总之，模型编辑展示了记忆利用的良好前景。
- en: VIII Discussion and Future Research
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 讨论与未来研究
- en: The memorization effect of DNN is an ongoing field with significant implications
    for the interpretability, generalization, and security of AI. In this section,
    we will discuss existing research findings and possible future research directions.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 的记忆效应是一个不断发展的领域，对 AI 的可解释性、泛化能力和安全性具有重要意义。在这一部分，我们将讨论现有的研究发现和可能的未来研究方向。
- en: VIII-A Memorization and Forgetting Mechanism
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-A 记忆与遗忘机制
- en: 'The memorization and forgetting mechanism remains unclear and confusing. However,
    based on existing studies, we have known some memorization and forgetting truths
    on the classification task:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆和遗忘机制仍然不清楚且令人困惑。然而，基于现有研究，我们已知在分类任务中一些记忆和遗忘的真相：
- en: •
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Standard training framework always leads DNNs to the minimal loss [[44](#bib.bib44)];
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标准训练框架总是将 DNNs 导向最小损失 [[44](#bib.bib44)];
- en: •
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DNNs can memorize common modern training datasets, even when the dataset is
    randomly labeled [[8](#bib.bib8)];
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DNNs 能够记忆常见的现代训练数据集，即使数据集是随机标记的 [[8](#bib.bib8)];
- en: •
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Long-tailed examples that lack representation in the data distribution like
    atypical examples and noisy examples are prone to be memorized [[13](#bib.bib13),
    [11](#bib.bib11)];
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在数据分布中缺乏代表性的长尾样本，如非典型样本和噪声样本，容易被记忆 [[13](#bib.bib13), [11](#bib.bib11)];
- en: •
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DNNs cannot identify atypical examples and noisy examples in training [[15](#bib.bib15)];
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DNNs 无法识别训练中的非典型样本和噪声样本 [[15](#bib.bib15)];
- en: •
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DNNs tend to prioritize the memorization of repeated data [[17](#bib.bib17),
    [28](#bib.bib28), [29](#bib.bib29)];
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DNNs 倾向于优先记忆重复的数据 [[17](#bib.bib17), [28](#bib.bib28), [29](#bib.bib29)];
- en: •
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DNNs have a critical early learning stage where pattern learning takes domination [[25](#bib.bib25)];
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DNNs 在早期学习阶段有一个关键的学习阶段，在这个阶段模式学习占据主导地位 [[25](#bib.bib25)];
- en: •
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Memorization appears to be confined to a limited set of neurons across various
    layers in DNNs [[15](#bib.bib15)];
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记忆似乎局限于深度神经网络中各层的有限神经元集合 [[15](#bib.bib15)];
- en: •
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Memorization is not responsible for overfitting [[56](#bib.bib56), [55](#bib.bib55),
    [15](#bib.bib15)];
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记忆并不负责过拟合 [[56](#bib.bib56), [55](#bib.bib55), [15](#bib.bib15)];
- en: •
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Long-tailed examples are prioritized to be forgotten, and noisy examples are
    forgotten more quickly than atypical examples [[96](#bib.bib96), [95](#bib.bib95)].
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长尾样本被优先忘记，而噪声样本比非典型样本更快地被遗忘 [[96](#bib.bib96), [95](#bib.bib95)]。
- en: According to these observations, it is reasonable to infer that, at least in
    the context of classification tasks, the memorization phenomenon is a property
    of the standard DNN gradient descent training framework. Specifically, minimizing
    the loss leads to the memorization effect. For instance, when a neural network
    has been fed some data during training, some parameters are activated and updated.
    The network may gradually build several mapping paths of common patterns, and
    each input example passes these mapping paths and then gains a probability score
    for every class. These probability scores constitute a probability vector and
    the predicted label will be the class with the maximum score. However, long-tailed
    examples cannot gain reasonable scores on all of these mapping paths because they
    have no strong patterns compared to more generalized examples. Consequently, the
    network may build a unique mapping path for each long-tailed example to minimize
    the loss, and this is the memorization phenomenon.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些观察结果，可以合理推测，至少在分类任务的背景下，记忆现象是标准DNN梯度下降训练框架的一个特性。具体来说，最小化损失会导致记忆效应。例如，当神经网络在训练过程中接收到一些数据时，一些参数会被激活并更新。网络可能会逐渐建立几个常见模式的映射路径，每个输入样本会经过这些映射路径，然后为每个类别获得一个概率分数。这些概率分数组成一个概率向量，预测标签将是分数最高的类别。然而，长尾样本无法在这些映射路径上获得合理的分数，因为它们相对于更通用的样本没有强模式。因此，网络可能会为每个长尾样本建立一个独特的映射路径以最小化损失，这就是记忆现象。
- en: 'Considering training with the batch stochastic gradient descent method, in
    the beginning, the network may randomly create unique mapping paths for each example
    to fit the labels. Subsequently, because of inherent patterns of data distribution,
    some of these mapping paths gradually align. The alignment can be regarded as
    the pattern extraction and may correlate with the early learning phenomenon [[43](#bib.bib43),
    [42](#bib.bib42), [25](#bib.bib25)] because the alignment effect decreases the
    loss of representative examples in the data distribution. This alignment precedes
    memorization because early gradients represent the direction that can most effectively
    reduce the loss. Additionally, long-tailed examples may also participate in the
    alignment but not align well, which let these examples experience forgetting events [[95](#bib.bib95)]
    (Definion [8](#Thmdefinition8 "Definition 8 ‣ VI-A1 Forgetting Definition based
    on Accuracy ‣ VI-A Forgetting Definition and Evaluation ‣ VI Forgetting Research
    ‣ Memorization in deep learning: A survey")), i.e. some long-tailed examples could
    be correctly classified at previous steps but misclassified at later steps. Simultaneously,
    the network could employ some extra capacity or parameters to memorize some particular
    features of long-tailed examples that are not aligned well in the early learning
    stage to reduce the loss. This is evidenced by concurrent improvements in accuracy
    for randomly labeled examples and clean examples [[15](#bib.bib15)]. This also
    explains why memorization does not depend on overfitting. Moreover, this indicates
    memorization learning and pattern learning are not totally discrete and contrary.
    They imply the difficulty of pattern extraction on examples. The more challenging
    the pattern extraction, the more apparent the tendency to be memorized. After
    the alignment phase, the network will prioritize memorizing long-tailed examples,
    which can lead to close-to-optimal generalization error based on the long tail
    theory [[13](#bib.bib13), [11](#bib.bib11)]. After memorizing of long-tailed examples,
    the network may exhibit the best generalization performance. However, if the training
    continues, diminishing the loss becomes challenging. The network may develop unique
    paths for all examples, causing the predicted vector to closely approach the label
    vector, even resulting in zero training error. This phenomenon is referred to
    as neural collapse [[44](#bib.bib44)], where each example in the same class collapses
    to the same representation. For architecture, unique memorization mapping paths
    may require only a few parameters across layers because these paths are not based
    on pattern recognition. Therefore, it is reasonable to observe that memorization
    appears to be confined to a limited set of neurons across various layers in DNNs [[15](#bib.bib15)].
    Furthermore, if we take into account forgetting, these memorized examples become
    highly unstable. This instability arises because even if a small part of the associated
    parameters has been updated, these examples are likely to be misclassified. This
    phenomenon explains why long-tailed examples are particularly prone to being forgotten [[96](#bib.bib96)].
    Meanwhile, there may remain some unchanged associated parameters that pose privacy
    risks [[67](#bib.bib67)].'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑使用批量随机梯度下降法进行训练时，最开始，网络可能会随机为每个示例创建独特的映射路径以适应标签。随后，由于数据分布的固有模式，这些映射路径中的一些会逐渐对齐。这种对齐可以被视为模式提取，可能与早期学习现象相关[[43](#bib.bib43),
    [42](#bib.bib42), [25](#bib.bib25)]，因为对齐效应减少了数据分布中代表性示例的损失。这种对齐优先于记忆，因为早期梯度表示最有效地减少损失的方向。此外，长尾示例也可能参与对齐，但对齐效果不好，这使得这些示例经历遗忘事件[[95](#bib.bib95)]（定义[8](#Thmdefinition8
    "Definition 8 ‣ VI-A1 Forgetting Definition based on Accuracy ‣ VI-A Forgetting
    Definition and Evaluation ‣ VI Forgetting Research ‣ Memorization in deep learning:
    A survey")），即某些长尾示例可能在早期步骤被正确分类，但在后续步骤被误分类。同时，网络可能会利用一些额外的容量或参数来记忆长尾示例中在早期学习阶段对齐不佳的特定特征，以减少损失。这通过随机标记示例和干净示例的准确率同步提高得到了证实[[15](#bib.bib15)]。这也解释了为什么记忆不依赖于过拟合。此外，这表明记忆学习和模式学习并不是完全离散和相反的。它们暗示了示例上模式提取的难度。模式提取越具挑战性，越明显地倾向于被记忆。在对齐阶段之后，网络将优先记忆长尾示例，这可能导致基于长尾理论的接近最优泛化误差[[13](#bib.bib13),
    [11](#bib.bib11)]。在记忆长尾示例之后，网络可能会展示最佳的泛化性能。然而，如果继续训练，减少损失将变得具有挑战性。网络可能会为所有示例发展独特的路径，使得预测向量接近标签向量，甚至导致零训练误差。这种现象被称为神经崩溃[[44](#bib.bib44)]，即同一类别中的每个示例崩溃到相同的表示。对于架构来说，独特的记忆映射路径可能只需在各层之间使用少量参数，因为这些路径不是基于模式识别。因此，观察到记忆似乎仅限于DNNs[[15](#bib.bib15)]中各层的有限神经元集是合理的。此外，如果考虑到遗忘，这些记忆的示例会变得非常不稳定。这种不稳定性是因为即使相关参数的少量部分已被更新，这些示例也可能会被误分类。这种现象解释了为什么长尾示例特别容易被遗忘[[96](#bib.bib96)]。与此同时，可能会有一些未改变的相关参数，这些参数存在隐私风险[[67](#bib.bib67)]。'
- en: Certainly, our theoretic model of memorization and forgetting in the classification
    task is an assumption. Further experimentation and empirical evidence are required
    to fully explain the memorization phenomenon. Understanding the memorization mechanism
    carries significant implications for enhancing the interpretability of DNNs. To
    effectively understand this mechanism, it is crucial to describe the spatiotemporal
    memorization process. In terms of training periods, the primary objectives include
    characterizing memorization in different stages and investigating whether the
    memorization phenomenon constitutes a form of overlearning. Concerning neural
    network components, it becomes essential to quantitatively explain the distribution
    of memorization across layers or components and evaluate whether certain neurons
    exhibit a tendency for memorizing examples. Additionally, it is important to differentiate
    between memorization learning and pattern learning neurons. Moreover, different
    training frameworks may have distinct memorization phenomena, particularly for
    unsupervised tasks and multiple-task scenarios.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们在分类任务中关于记忆和遗忘的理论模型仅仅是一种假设。要全面解释记忆现象，还需要进一步的实验和实证证据。理解记忆机制对增强深度神经网络（DNNs）的可解释性具有重要意义。为了有效理解这一机制，描述时空记忆过程至关重要。在训练周期方面，主要目标包括描述不同阶段的记忆情况，并调查记忆现象是否构成过度学习。在神经网络组件方面，定量解释记忆在层或组件之间的分布变得至关重要，并评估某些神经元是否表现出记忆示例的倾向。此外，区分记忆学习和模式学习神经元也很重要。此外，不同的训练框架可能会有不同的记忆现象，特别是在无监督任务和多任务场景下。
- en: VIII-B Memorization and Forgetting for Training Discussion
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B 训练讨论中的记忆与遗忘
- en: Data
  id: totrans-358
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据
- en: DNNs offer significant advantages in processing complex real-world data such
    as images and text compared to traditional machine learning methods [[128](#bib.bib128),
    [52](#bib.bib52)]. A notable observation is that DNNs can effectively extract
    common features or patterns from the data distribution. However, controlling this
    extraction process is challenging, and some uncommon yet useful features may not
    be learned well [[13](#bib.bib13), [11](#bib.bib11)]. At the feature level, out-of-distribution
    features and rare but useful features are both less representative. This may explain
    why memorization learning cannot identify atypical examples and noisy examples.
    Moreover, we may rethink how to describe complex data in reality to keep features
    balanced. This encourages us to contemplate aspects such as data dimension, granularity [[129](#bib.bib129)],
    and distribution [[130](#bib.bib130), [131](#bib.bib131)] to enhance the performance
    of DNNs. Additionally, the size of the training dataset probably does not serve
    as the sole determining factor for task performance [[132](#bib.bib132)].
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统机器学习方法相比，DNNs在处理复杂的现实数据（如图像和文本）方面具有显著优势[[128](https://example.org/bib.bib128),
    [52](https://example.org/bib.bib52)]。一个显著的观察是，DNNs能够有效地从数据分布中提取共同特征或模式。然而，控制这一提取过程具有挑战性，一些不常见但有用的特征可能无法很好地学习[[13](https://example.org/bib.bib13),
    [11](https://example.org/bib.bib11)]。在特征层面，分布外特征和稀有但有用的特征都较少具有代表性。这可能解释了为什么记忆学习无法识别非典型示例和噪声示例。此外，我们可能需要重新思考如何描述现实中的复杂数据，以保持特征平衡。这促使我们考虑数据维度、粒度[[129](https://example.org/bib.bib129)]和分布[[130](https://example.org/bib.bib130),
    [131](https://example.org/bib.bib131)]等方面，以提升DNNs的性能。此外，训练数据集的大小可能并不是任务性能的唯一决定因素[[132](https://example.org/bib.bib132)]。
- en: Training Framework
  id: totrans-360
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练框架
- en: It is understood that the stochastic gradient descent method will aggressively
    minimize the loss function until reaching extreme mathematical conditions such
    as neural collapse [[44](#bib.bib44)]. However, the extreme conditions may not
    meet our requirements, and even potentially introduce further challenges. From
    this perspective, the loss function really matters and decides the learning direction.
    The challenge lies in the fact that loss functions may not always accurately measure
    the true loss associated with the assigned task. For instance, in a classification
    task, a model with minimal loss may have poor generalization performance due to
    overfitting. Therefore, the memorization and forgetting effect may serve as adaptive
    solutions to address this conflict.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 已知随机梯度下降方法会积极地最小化损失函数，直至达到极端的数学条件，例如神经崩溃[[44](#bib.bib44)]。然而，这些极端条件可能不符合我们的要求，甚至可能引入更多挑战。从这个角度来看，损失函数确实很重要，并决定了学习的方向。挑战在于损失函数可能并不总是准确衡量分配任务的真实损失。例如，在分类任务中，一个损失最小的模型可能因过拟合而具有较差的泛化性能。因此，记忆和遗忘效应可能作为适应性解决方案来解决这一冲突。
- en: Architecture
  id: totrans-362
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 架构
- en: The impact of neural network architecture details on the memorization phenomenon
    remains unclear. Different layers within the architecture may assume distinct
    roles, with certain layers potentially exhibiting a preference for memorization.
    Viewing the architecture in terms of layer depth, deeper layers may tend to learn
    more specialized features [[46](#bib.bib46)] although these features are not completely
    for memorization. Specialized features still retain patterns, whereas memorized
    features may lack patterns and serve primarily to mark data. Therefore, deeper
    layers do not function for memorization [[15](#bib.bib15)]. Regarding the size
    of networks, the memorization tendency also correlates with the size of the training
    dataset. A larger model trained on a small dataset may lead to significant overfitting
    and a strong inclination toward memorization. Conversely, larger datasets, which
    contain more diverse patterns, tend to reduce the preference for memorization
    but may increase the probability of underfitting.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构细节对记忆现象的影响仍不清楚。架构中的不同层可能承担不同的角色，某些层可能会表现出记忆的倾向。从层的深度来看，更深的层可能倾向于学习更多的专门化特征[[46](#bib.bib46)]，尽管这些特征并不完全用于记忆。专门化特征仍保留模式，而记忆特征可能缺乏模式，主要用于标记数据。因此，更深的层不会用于记忆[[15](#bib.bib15)]。关于网络的规模，记忆倾向也与训练数据集的规模相关。一个在小数据集上训练的大模型可能导致显著的过拟合和强烈的记忆倾向。相反，包含更多多样化模式的大数据集往往会减少记忆的倾向，但可能增加欠拟合的概率。
- en: Tasks
  id: totrans-364
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 任务
- en: Memorization and forgetting manifest differently across various tasks. Presently,
    most memorization studies focus on the classification task, where the memorization
    phenomenon entails the utilization of a small set of parameters to uniquely mark
    examples. The classification task, being a dimension reduction task, can apply
    this way to minimize the loss. However, for other tasks such as generative tasks,
    the dynamics differ. Obviously, the generative task could be a dimension increment
    task like GAN [[133](#bib.bib133)], Diffusion model [[134](#bib.bib134)], and
    GPT [[135](#bib.bib135)], which aim to learn a target data distribution. Thus,
    the generalization of generative models refers to the model’s ability to produce
    accurate, relevant, and coherent outputs to cover the target data distribution.
    From this viewpoint, the memorization phenomenon in the generative task could
    be significantly different from the classification task. For instance, generative
    models may use enormous parameters to memorize almost all features of those long-tailed
    examples. This phenomenon has been demonstrated in some works [[9](#bib.bib9),
    [10](#bib.bib10)]. Furthermore, multiple-task learning and continuous learning
    are also distinct. A famous phenomenon called catastrophic forgetting [[90](#bib.bib90),
    [91](#bib.bib91), [89](#bib.bib89)] means that neural networks are hard to retain
    learned knowledge on multiple and dynamic data distributions. As the data distribution
    shifts based on sub-tasks, and the model learning capacity is limited, separating
    learned features becomes challenging. In such scenarios, memorization may have
    a very beneficial effect in preserving learned features.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆和遗忘在不同任务中表现各异。目前，大多数记忆研究集中于分类任务，其中记忆现象涉及使用一小组参数来唯一标记样本。分类任务作为一种维度减少任务，可以采用这种方式来最小化损失。然而，对于生成任务等其他任务，动态情况则有所不同。显然，生成任务可能是类似GAN [[133](#bib.bib133)]、扩散模型 [[134](#bib.bib134)]
    和 GPT [[135](#bib.bib135)] 这样需要学习目标数据分布的维度增加任务。因此，生成模型的泛化能力指的是模型产生准确、相关且一致的输出以覆盖目标数据分布的能力。从这个角度来看，生成任务中的记忆现象可能与分类任务显著不同。例如，生成模型可能使用大量参数来记住几乎所有长尾样本的特征。这一现象在一些研究中已有所证明 [[9](#bib.bib9),
    [10](#bib.bib10)]。此外，多任务学习和持续学习也是不同的。一种被称为灾难性遗忘的著名现象 [[90](#bib.bib90), [91](#bib.bib91),
    [89](#bib.bib89)] 表明神经网络很难在多个和动态数据分布上保留学习到的知识。由于数据分布根据子任务而变化，且模型学习能力有限，分离学习到的特征变得具有挑战性。在这种情况下，记忆可能在保存学习到的特征方面发挥非常有益的作用。
- en: VIII-C Memorization and Forgetting for Privacy and Security Discussion
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-C 隐私和安全讨论中的记忆与遗忘
- en: Privacy Leakage
  id: totrans-367
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 隐私泄露
- en: Privacy leakage is a common problem in neural networks. Direct privacy leakage
    includes the inference attack and the inversion/extraction attack we mentioned
    before. According to some empirical evidence [[24](#bib.bib24), [67](#bib.bib67)],
    the vulnerability of privacy leakage often arises from memorization. For membership
    inference attacks, unique features of examples are heavily exploited rather than
    representative features that generally improve false-positive rates [[24](#bib.bib24)].
    It is known that examples sharing patterns but not in the training set are very
    easily inferred as membership. This protects the privacy of those representative
    examples. Conversely, memorized long-tailed examples are more vulnerable. In inversion/extraction
    attacks, the adversary can generally produce representative data based on patterns.
    However, the representative data could be common knowledge and not private. The
    real and particular data belonging to the training dataset is more valuable for
    the adversary. Due to varied behaviors of memorization in various tasks, generative
    models are more vulnerable from the perspective of memorization because the generative
    models may be required to memorize more long-tailed examples to fit the target
    data distribution. The memorization process likely contains most features of certain
    examples, allowing these examples to be reconstructed in a lossless manner under
    inversion/extraction attacks. Some empirical results have illustrated that memorization
    is the source of extraction [[10](#bib.bib10), [9](#bib.bib9)]. Qualitatively,
    the memorization effect indeed poses risks of privacy leakage. Therefore, mitigating
    the memorization effect may reduce risks associated with inference and inversion
    attacks. Nonetheless, we acknowledge that memorization somehow contributes to
    task performance [[13](#bib.bib13), [11](#bib.bib11)]. Considering a trade-off
    framework could be beneficial.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私泄露是神经网络中的一个常见问题。直接的隐私泄露包括我们之前提到的推断攻击和反演/提取攻击。根据一些实证证据 [[24](#bib.bib24), [67](#bib.bib67)]，隐私泄露的脆弱性通常来源于记忆。对于成员推断攻击，样本的独特特征被大量利用，而不是通常能提高假阳性率的代表性特征 [[24](#bib.bib24)]。已知那些共享模式但不在训练集中的样本很容易被推断为成员，这保护了那些代表性样本的隐私。相反，记忆中的长尾样本则更为脆弱。在反演/提取攻击中，对手通常可以根据模式生成代表性数据。然而，这些代表性数据可能是常识而非私人信息。真正属于训练数据集的特定数据对对手更有价值。由于记忆在各种任务中的行为差异，从记忆的角度来看，生成模型更容易受到攻击，因为生成模型可能需要记住更多的长尾样本以适应目标数据分布。记忆过程可能包含某些样本的大部分特征，使得这些样本在反演/提取攻击下能够以无损的方式重构。一些实证结果表明，记忆是提取的来源 [[10](#bib.bib10),
    [9](#bib.bib9)]。从定性分析来看，记忆效应确实带来了隐私泄露的风险。因此，减轻记忆效应可能有助于降低与推断和反演攻击相关的风险。然而，我们也认识到记忆在某种程度上有助于任务性能 [[13](#bib.bib13),
    [11](#bib.bib11)]。考虑一个权衡框架可能会有益。
- en: Malicious Attacks
  id: totrans-369
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 恶意攻击
- en: Poisoning, backdoor, and adversarial attacks are typical malicious attacks.
    These attacks can directly disable networks or embed malicious triggers to mislead
    models. Due to the absence of specific threat evaluation, we only conduct some
    hypothetical discussions. Suppose the attack just modifies the training data without
    optimization and targets disabling networks like label flip and random noise.
    In such cases, the model fails because it cannot learn correct patterns following
    the gradient direction and has to memorize them. Therefore, the memorization phenomenon
    is an adaptive process. For induced attacks without optimization [[136](#bib.bib136)],
    these attacks can install malicious triggers in networks, the triggers could be
    learned via pattern learning or memorization learning. Specifically, this depends
    on the feature distribution of triggers. If the backdoor feature is long-tailed,
    here applying memorization, otherwise it is pattern learning. Finally, some malicious
    attacks are based on optimization, the adversary can submit artificial features
    or gradients to the networks [[68](#bib.bib68)]. The synthetic features or gradients
    are out of the standard training framework, so it is challenging to discuss the
    memorization effect under this condition. This requires further studies.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 中毒、后门和对抗攻击是典型的恶意攻击。这些攻击可以直接使网络瘫痪或嵌入恶意触发器来误导模型。由于缺乏具体的威胁评估，我们仅进行一些假设讨论。假设攻击只是修改训练数据而不进行优化，并且针对像标签翻转和随机噪声这样的网络禁用情况。在这种情况下，模型失败是因为它无法按照梯度方向学习正确的模式，只能记住这些模式。因此，记忆现象是一种适应性过程。对于没有优化的诱发攻击[[136](#bib.bib136)]，这些攻击可以在网络中安装恶意触发器，这些触发器可以通过模式学习或记忆学习进行学习。具体而言，这取决于触发器的特征分布。如果后门特征是长尾的，则应用记忆，否则是模式学习。最后，一些恶意攻击基于优化，对手可以向网络提交人工特征或梯度[[68](#bib.bib68)]。这些合成特征或梯度超出了标准训练框架，因此在这种情况下讨论记忆效应具有挑战性。这需要进一步的研究。
- en: Forgetting Guarantee
  id: totrans-371
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 遗忘保证
- en: From the perspective of privacy, the forgetting phenomenon actually provides
    the privacy guarantee by ensuring that previously learned particular features
    of examples will be forgotten in later stages of training without prompt repetition [[16](#bib.bib16)].
    Therefore, the input order of examples may also impact the example privacy. Forgetting
    also corresponds to machine unlearning technology. At the example level, typical
    examples may only provide generalized patterns. Unlearning these examples is not
    valid because other examples also provide similar features. However, for long-tailed
    examples, after removing these examples from the training dataset, the model will
    gradually forget particular features of them during constant training and the
    corresponding privacy risk is simultaneously reducing [[67](#bib.bib67)]. However,
    the privacy onion effect [[21](#bib.bib21)] is another problem. This effect indicates
    that the removal of some most vulnerable data could improve the threat of other
    vulnerable data. Consequently, it is uncertain how the forgetting phenomenon and
    relevant unlearning technologies quantitatively reduce privacy risks.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 从隐私的角度来看，遗忘现象实际上通过确保之前学习的特定示例特征在训练的后期阶段会被遗忘而提供隐私保障，而无需立即重复[[16](#bib.bib16)]。因此，示例的输入顺序也可能影响示例隐私。遗忘也对应于机器遗忘技术。在示例级别，典型示例可能只提供概括的模式。遗忘这些示例是不有效的，因为其他示例也提供了类似的特征。然而，对于长尾示例，在从训练数据集中删除这些示例后，模型会在持续训练过程中逐渐遗忘它们的特定特征，相应的隐私风险也会同时减少[[67](#bib.bib67)]。然而，隐私洋葱效应[[21](#bib.bib21)]是另一个问题。该效应表明，移除一些最脆弱的数据可能会提高其他脆弱数据的威胁。因此，遗忘现象及相关的遗忘技术如何定量减少隐私风险仍不确定。
- en: Memorization Inhibition
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 记忆抑制
- en: Considering the underlying privacy risks of memorization, we could apply some
    technologies or strategies to inhibit memorization. In relevant technologies,
    data augmentation and regularization can help inhibit memorization and improve
    generalization. Data augmentation enhances the patterns within the data, particularly
    for those long-tailed examples. Regarding regularizers, weight decay technology
    restrains the feature space and prevents extreme parameters used in memorization,
    while the dropout strategy could randomly drop memorization activation. Moreover,
    it’s feasible to guide memorization to specific neurons and drop them when testing [[15](#bib.bib15)].
    However, this technology may drop some features of atypical examples. Despite
    this, we can also follow the memorization mechanism to propose some new regularizers
    to mitigate the memorization effect. Differential privacy is another effective
    tool. The added noise would cover some features that tend to be memorized, potentially
    causing the networks to memorize the added noise rather than unique features.
    However, this may result in performance degradation. Thus, while memorization
    inhibition can protect privacy, it may simultaneously impair generalization performance.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到记忆的潜在隐私风险，我们可以应用一些技术或策略来抑制记忆。在相关技术中，数据增强和正则化可以帮助抑制记忆并提高泛化能力。数据增强提升了数据中的模式，特别是对于那些长尾样本。关于正则化器，权重衰减技术限制了特征空间，防止了在记忆中使用极端参数，而丢弃策略则可以随机丢弃记忆激活。此外，可以引导记忆到特定的神经元，并在测试时丢弃它们[[15](#bib.bib15)]。然而，这项技术可能会丢弃一些非典型样本的特征。尽管如此，我们仍然可以遵循记忆机制，提出一些新的正则化器来减轻记忆效应。差分隐私是另一种有效的工具。添加的噪声会遮盖一些倾向于被记住的特征，可能导致网络记住添加的噪声而非独特特征。然而，这可能会导致性能下降。因此，虽然记忆抑制可以保护隐私，但它可能同时损害泛化性能。
- en: Threat Evaluation
  id: totrans-375
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 威胁评估
- en: The perspective of memorization and forgetting offers a novel and interesting
    entry point to re-evaluate existing threat and defense strategies. The memorization
    and forgetting mechanism can enhance the interpretability of certain threats,
    thereby deeply understanding existing threats. From this perspective, we could
    propose new solutions or enhance existing defense methods. Additionally, this
    viewpoint can assist in uncovering previously unknown threats. It is essential
    to systematically assess how memorization and forgetting influence these threats
    and defense methods.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆与遗忘的视角为重新评估现有威胁和防御策略提供了一个新颖且有趣的切入点。记忆与遗忘机制可以增强某些威胁的可解释性，从而深入理解现有威胁。从这一视角出发，我们可以提出新的解决方案或增强现有防御方法。此外，这一观点还可以帮助揭示以前未知的威胁。系统地评估记忆和遗忘如何影响这些威胁和防御方法是至关重要的。
- en: VIII-D Application Discussion
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-D 应用讨论
- en: As memorization is an innate feature of DNNs, we can apply the advantages of
    the memorization effect to assist in some specific tasks and mitigate the disadvantages
    in some scenarios.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 由于记忆是深度神经网络的固有特征，我们可以利用记忆效应的优势来协助某些特定任务，并在某些场景中减轻其劣势。
- en: In positive terms, large language models directly benefit from memorization
    abilities in tasks like question-answer. Memorization can serve as an additional
    structure to cache representations [[105](#bib.bib105), [121](#bib.bib121)] or
    key-value pairs, improving network speed and performance [[122](#bib.bib122),
    [123](#bib.bib123)]. Furthermore, networks can function as databases or knowledge
    bases with the memorization effect [[111](#bib.bib111)]. Additionally, the memorization
    of LLMs can be modified directly to update facts. LLMs learn a variety of facts
    about the world during pretraining and these facts are stored in model weights [[124](#bib.bib124)],
    suggesting that MLP weights act as key-value memories [[125](#bib.bib125), [126](#bib.bib126),
    [115](#bib.bib115)]. Thus, editing memorization neurons for injecting new information
    has been a popular technology called model editing to update facts which sidesteps
    the computational burden associated with training a wholly new model [[137](#bib.bib137),
    [113](#bib.bib113), [112](#bib.bib112), [115](#bib.bib115), [116](#bib.bib116),
    [114](#bib.bib114), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [139](#bib.bib139), [141](#bib.bib141)].
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 从积极的角度来看，大型语言模型在如问答任务等方面直接受益于记忆能力。记忆可以作为缓存表示[[105](#bib.bib105), [121](#bib.bib121)]或键值对的额外结构，从而提高网络速度和性能[[122](#bib.bib122),
    [123](#bib.bib123)]。此外，网络可以通过记忆效应作为数据库或知识库[[111](#bib.bib111)]。此外，LLM的记忆可以直接修改以更新事实。LLM在预训练过程中学习了关于世界的各种事实，这些事实被存储在模型权重中[[124](#bib.bib124)]，这表明MLP权重充当了键值记忆[[125](#bib.bib125),
    [126](#bib.bib126), [115](#bib.bib115)]。因此，编辑记忆神经元以注入新信息已成为一种流行的技术，称为模型编辑，用于更新事实，避免了训练完全新模型的计算负担[[137](#bib.bib137),
    [113](#bib.bib113), [112](#bib.bib112), [115](#bib.bib115), [116](#bib.bib116),
    [114](#bib.bib114), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [139](#bib.bib139), [141](#bib.bib141)]。
- en: Indirectly, the memorization phenomenon can be employed to filter noisy examples
    and atypical examples, which benefits example enhancement [[102](#bib.bib102),
    [65](#bib.bib65), [103](#bib.bib103)] and noisy data learning [[97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [30](#bib.bib30)].
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 间接地，记忆现象可以用来过滤噪声示例和非典型示例，这有利于示例增强[[102](#bib.bib102), [65](#bib.bib65), [103](#bib.bib103)]和噪声数据学习[[97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [30](#bib.bib30)]。
- en: However, the memorization effect also raises privacy concerns. We may utilize
    the memorization effect to audit privacy [[29](#bib.bib29), [9](#bib.bib9)] or
    mitigate memorization to ensure compliance [[15](#bib.bib15)]. Additionally, the
    forgetting effect may provide some privacy guarantee and correlate with unlearning
    technology.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，记忆效应也引发了隐私问题。我们可以利用记忆效应来审计隐私[[29](#bib.bib29), [9](#bib.bib9)]，或减轻记忆以确保合规[[15](#bib.bib15)]。此外，遗忘效应可能提供一些隐私保证，并与遗忘技术相关联。
- en: IX Conclusion
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 结论
- en: This survey based on the memorization effect provides a detailed exploration
    of a pivotal concept in DNNs. It begins by discussing the memorization definitions
    in the generalization domain and security and privacy domain. The survey then
    provides relevant measurements of memorization at different levels. Next, we discuss
    how memorization influences DNN training including data distribution, training
    stage, model structure, and other factors. After that, we review related studies
    on underlying privacy and security risks that correlate with the memorization
    effect. Following this, we also review the studies about the forgetting effect
    because forgetting is the opposite of memorization. Subsequently, this survey
    discusses related applications to the memorization effect or are highly associated
    with memorization functions. Lastly, we discuss possible memorization and forgetting
    mechanisms, attempt to understand memorization and forgetting impacts, and suggest
    further research. In this survey, to the furthest extent, we collect and present
    the main literature about the memorization and forgetting effect of DNNs, organizing
    relevant works in a comprehensive framework.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述基于记忆效应，提供了对深度神经网络（DNNs）一个关键概念的详细探索。它从讨论一般化领域以及安全和隐私领域中的记忆定义开始。随后，综述提供了不同层次上的记忆相关测量。接下来，我们探讨了记忆如何影响DNN训练，包括数据分布、训练阶段、模型结构及其他因素。之后，我们回顾了与记忆效应相关的基础隐私和安全风险的研究。接着，我们也回顾了遗忘效应的研究，因为遗忘是记忆的对立面。随后，本综述讨论了与记忆效应相关的应用或与记忆功能高度相关的应用。最后，我们讨论了可能的记忆和遗忘机制，尝试理解记忆和遗忘的影响，并提出进一步的研究建议。在本综述中，我们尽可能地收集和呈现了有关DNNs记忆和遗忘效应的主要文献，并将相关工作组织在一个全面的框架中。
- en: In this review, we highlight that memorization and forgetting effects are features
    of DNNs. These effects have deep impacts on the performance, fairness, explainability,
    accountability, and privacy of DNNs. Therefore, we should develop the ability
    to control, manage, and utilize the effects, leading to highly usable and trustworthy
    neural networks.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在本综述中，我们强调记忆和遗忘效应是DNNs的特征。这些效应对DNNs的性能、公平性、可解释性、责任性和隐私产生深远影响。因此，我们应该培养控制、管理和利用这些效应的能力，从而实现高度可用和值得信赖的神经网络。
- en: References
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat *et al.*, “Gpt-4 technical report,” *arXiv
    preprint arXiv:2303.08774*, 2023.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D.
    Almeida, J. Altenschmidt, S. Altman, S. Anadkat *等*，“Gpt-4技术报告，” *arXiv预印本arXiv:2303.08774*，2023年。'
- en: '[2] A. S. Dhanjal and W. Singh, “A comprehensive survey on automatic speech
    recognition using neural networks,” *Multimedia Tools and Applications*, vol. 83,
    no. 8, pp. 23 367–23 412, 2024.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. S. Dhanjal 和 W. Singh，“基于神经网络的自动语音识别的全面综述，” *多媒体工具与应用*，第83卷，第8期，pp.
    23 367–23 412，2024年。'
- en: '[3] Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing of deep-neural-network-driven
    autonomous cars,” in *Proceedings of the 40th international conference on software
    engineering*, 2018, pp. 303–314.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest：深度神经网络驱动的自动驾驶汽车的自动化测试，”发表于
    *第40届国际软件工程会议论文集*，2018年，pp. 303–314。'
- en: '[4] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin,
    W. Wang, L. Lu, X. Jia, Q. Liu, J. Dai, Y. Qiao, and H. Li, “Planning-oriented
    autonomous driving,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, June 2023, pp. 17 853–17 862.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin,
    W. Wang, L. Lu, X. Jia, Q. Liu, J. Dai, Y. Qiao, 和 H. Li，“面向规划的自动驾驶，”发表于 *IEEE/CVF计算机视觉与模式识别会议论文集（CVPR）*，2023年6月，pp.
    17 853–17 862。'
- en: '[5] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin,
    W. Wang *et al.*, “Planning-oriented autonomous driving,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023, pp. 17 853–17 862.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin,
    W. Wang *等*，“面向规划的自动驾驶，”发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2023年，pp. 17 853–17 862。'
- en: '[6] M. Li, B. Lin, Z. Chen, H. Lin, X. Liang, and X. Chang, “Dynamic graph
    enhanced contrastive learning for chest x-ray report generation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023,
    pp. 3334–3343.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. Li, B. Lin, Z. Chen, H. Lin, X. Liang, and X. Chang, “动态图增强对比学习用于胸部X光报告生成，”发表于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2023年，pp. 3334–3343。'
- en: '[7] T. Tu, A. Palepu, M. Schaekermann, K. Saab, J. Freyberg, R. Tanno, A. Wang,
    B. Li, M. Amin, N. Tomasev *et al.*, “Towards conversational diagnostic ai,” *arXiv
    preprint arXiv:2401.05654*, 2024.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] T. 图，A. 帕莱普，M. 沙克曼，K. 萨布，J. 弗雷伯格，R. 谭诺，A. 王，B. 李，M. 阿敏，N. 托马谢夫 *等*，“朝着对话式诊断
    AI 的方向发展”，*arXiv 预印本 arXiv:2401.05654*，2024年。'
- en: '[8] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understanding
    deep learning requires rethinking generalization,” in *International Conference
    on Learning Representations*, 2017.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] C. 张，S. 本吉奥，M. 哈特，B. 雷赫特，和 O. 维尼亚尔斯，“理解深度学习需要重新思考泛化”，发表于*国际学习表征会议*，2017年。'
- en: '[9] N. Carlini, F. Tramèr, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee,
    A. Roberts, T. Brown, D. Song, Ú. Erlingsson, A. Oprea, and C. Raffel, “Extracting
    Training Data from Large Language Models,” in *30th USENIX Security Symposium
    (USENIX Security 21)*, 2021, pp. 2633–2650.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] N. 卡尔尼，F. 特拉梅尔，E. 华莱士，M. 贾吉尔斯基，A. 赫伯特-沃斯，K. 李，A. 罗伯茨，T. 布朗，D. 宋，Ú. 埃尔林松，A.
    奥普雷亚，和 C. 拉费尔，“从大规模语言模型中提取训练数据”，发表于*第30届USENIX安全研讨会（USENIX Security 21）*，2021年，页码
    2633–2650。'
- en: '[10] N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramèr, B. Balle,
    D. Ippolito, and E. Wallace, “Extracting Training Data from Diffusion Models,”
    in *32nd USENIX Security Symposium (USENIX Security 23)*, 2023, pp. 5253–5270.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] N. 卡尔尼，J. 海耶斯，M. 纳斯尔，M. 贾吉尔斯基，V. 塞瓦格，F. 特拉梅尔，B. 巴勒，D. 伊波利托，和 E. 华莱士，“从扩散模型中提取训练数据”，发表于*第32届USENIX安全研讨会（USENIX
    Security 23）*，2023年，页码 5253–5270。'
- en: '[11] V. Feldman and C. Zhang, “What Neural Networks Memorize and Why: Discovering
    the Long Tail via Influence Estimation,” in *Advances in Neural Information Processing
    Systems*, vol. 33.   Curran Associates, Inc., 2020, pp. 2881–2891.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] V. 费尔德曼 和 C. 张，“神经网络记住了什么以及为什么：通过影响估计发现长尾”，发表于*神经信息处理系统进展*，第33卷。   Curran
    Associates, Inc.，2020年，页码 2881–2891。'
- en: '[12] X. Zheng and J. Jiang. An Empirical Study of Memorization in NLP. [Online].
    Available: http://arxiv.org/abs/2203.12171'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] X. 郑 和 J. 蒋。“NLP 中记忆的实证研究”。[在线]。可用： http://arxiv.org/abs/2203.12171'
- en: '[13] V. Feldman, “Does learning require memorization? a short tale about a
    long tail,” in *Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory
    of Computing*, ser. STOC 2020.   New York, NY, USA: Association for Computing
    Machinery, Jun. 2020, pp. 954–959.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] V. 费尔德曼，“学习是否需要记忆？关于长尾的短篇故事”，发表于*第52届ACM SIGACT理论计算机科学年会论文集*，系列 STOC 2020。   纽约，NY，美国：计算机协会，2020年6月，页码
    954–959。'
- en: '[14] N. Carlini, C. Liu, Ú. Erlingsson, J. Kos, and D. Song, “The Secret Sharer:
    Evaluating and Testing Unintended Memorization in Neural Networks,” 2019.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] N. 卡尔尼，C. 刘，Ú. 埃尔林松，J. 科斯，和 D. 宋，“秘密分享者：评估和测试神经网络中的非预期记忆”，2019年。'
- en: '[15] P. Maini, M. C. Mozer, H. Sedghi, Z. C. Lipton, J. Z. Kolter, and C. Zhang,
    “Can Neural Network Memorization Be Localized?” Jul. 2023.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] P. 曼尼，M. C. 莫泽，H. 塞赫吉，Z. C. 利普顿，J. Z. 科尔特，和 C. 张，“神经网络记忆能否局部化？”2023年7月。'
- en: '[16] K. Tirumala, A. Markosyan, L. Zettlemoyer, and A. Aghajanyan, “Memorization
    Without Overfitting: Analyzing the Training Dynamics of Large Language Models,”
    *Advances in Neural Information Processing Systems*, vol. 35, pp. 38 274–38 290,
    Dec. 2022.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] K. 提鲁马拉，A. 马尔科斯扬，L. 泽特尔莫耶，和 A. 阿赫贾尼扬，“无过拟合的记忆：分析大规模语言模型的训练动态”，*神经信息处理系统进展*，第35卷，页码
    38 274–38 290，2022年12月。'
- en: '[17] C. Zhang, D. Ippolito, K. Lee, M. Jagielski, F. Tramèr, and N. Carlini,
    “Counterfactual Memorization in Neural Language Models,” Dec. 2021.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] C. 张，D. 伊波利托，K. 李，M. 贾吉尔斯基，F. 特拉梅尔，和 N. 卡尔尼，“神经语言模型中的反事实记忆”，2021年12月。'
- en: '[18] S. Anagnostidis, G. Bachmann, L. Noci, and T. Hofmann, “The Curious Case
    of Benign Memorization,” in *The Eleventh International Conference on Learning
    Representations*, Sep. 2022.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. 阿纳格诺斯蒂迪斯，G. 巴赫曼，L. 诺西，和 T. 霍夫曼，“良性记忆的奇怪案例”，发表于*第十一届国际学习表征会议*，2022年9月。'
- en: '[19] D. A. Nguyen, R. Levie, J. Lienen, G. Kutyniok, and E. Hüllermeier, “Memorization-Dilation:
    Modeling Neural Collapse Under Label Noise,” Apr. 2023.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] D. A. 阮，R. 莱维，J. 利能，G. 库提纽克，和 E. 胡勒迈耶，“记忆-膨胀：在标签噪声下建模神经崩溃”，2023年4月。'
- en: '[20] Z. Jiang, C. Zhang, K. Talwar, and M. C. Mozer, “Characterizing Structural
    Regularities of Labeled Data in Overparameterized Models,” in *Proceedings of
    the 38th International Conference on Machine Learning*.   PMLR, Jul. 2021, pp.
    5034–5044.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Z. 姜，C. 张，K. 塔尔瓦尔，和 M. C. 莫泽，“在过参数模型中表征标记数据的结构规律”，发表于*第38届国际机器学习会议论文集*。   PMLR，2021年7月，页码
    5034–5044。'
- en: '[21] N. Carlini, M. Jagielski, C. Zhang, N. Papernot, A. Terzis, and F. Tramer,
    “The Privacy Onion Effect: Memorization is Relative,” *Advances in Neural Information
    Processing Systems*, vol. 35, pp. 13 263–13 276, Dec. 2022.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] N. Carlini, M. Jagielski, C. Zhang, N. Papernot, A. Terzis, 和 F. Tramer,
    “隐私洋葱效应：记忆是相对的”，*神经信息处理系统进展*，第35卷，页码13,263–13,276，2022年12月。'
- en: '[22] X. Li, Q. Li, Z. Hu, and X. Hu, “On the Privacy Effect of Data Enhancement
    via the Lens of Memorization,” Feb. 2023.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] X. Li, Q. Li, Z. Hu, 和 X. Hu, “从记忆的角度看数据增强的隐私效应”，2023年2月。'
- en: '[23] S. Rezaei and X. Liu, “On the Difficulty of Membership Inference Attacks,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2021, pp. 7892–7900.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Rezaei 和 X. Liu, “关于成员推断攻击的难度”，在*IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，页码7892–7900。'
- en: '[24] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer, “Membership
    Inference Attacks From First Principles,” Apr. 2022.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, 和 F. Tramer, “从第一原理出发的成员推断攻击”，2022年4月。'
- en: '[25] D. Arpit, S. Jastrzębski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal,
    T. Maharaj, A. Fischer, A. Courville, Y. Bengio, and S. Lacoste-Julien, “A Closer
    Look at Memorization in Deep Networks,” in *Proceedings of the 34th International
    Conference on Machine Learning*.   PMLR, Jul. 2017, pp. 233–242.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] D. Arpit, S. Jastrzębski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal,
    T. Maharaj, A. Fischer, A. Courville, Y. Bengio, 和 S. Lacoste-Julien, “深度网络中的记忆研究”，在*第34届国际机器学习会议论文集*。PMLR，2017年7月，页码233–242。'
- en: '[26] Y. Dong, K. Xu, X. Yang, T. Pang, Z. Deng, H. Su, and J. Zhu, “Exploring
    Memorization in Adversarial Training,” Mar. 2022.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Y. Dong, K. Xu, X. Yang, T. Pang, Z. Deng, H. Su, 和 J. Zhu, “探索对抗训练中的记忆”，2022年3月。'
- en: '[27] G. Hacohen, L. Choshen, and D. Weinshall, “Let’s Agree to Agree: Neural
    Networks Share Classification Order on Real Datasets,” in *Proceedings of the
    37th International Conference on Machine Learning*.   PMLR, Nov. 2020, pp. 3950–3960.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] G. Hacohen, L. Choshen, 和 D. Weinshall, “达成共识：神经网络在真实数据集上共享分类顺序”，在*第37届国际机器学习会议论文集*。PMLR，2020年11月，页码3950–3960。'
- en: '[28] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch,
    and N. Carlini, “Deduplicating Training Data Makes Language Models Better,” in
    *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers)*, S. Muresan, P. Nakov, and A. Villavicencio, Eds.   Dublin,
    Ireland: Association for Computational Linguistics, May 2022, pp. 8424–8445.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch,
    和 N. Carlini, “去重训练数据使语言模型更好”，在*第60届计算语言学协会年会论文集（第1卷：长篇论文）*，S. Muresan, P. Nakov,
    和 A. Villavicencio 编辑。都柏林，爱尔兰：计算语言学协会，2022年5月，页码8424–8445。'
- en: '[29] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang,
    “Quantifying Memorization Across Neural Language Models,” Mar. 2023.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, 和 C. Zhang,
    “量化神经语言模型中的记忆”，2023年3月。'
- en: '[30] H. Maennel, I. M. Alabdulmohsin, I. O. Tolstikhin, R. Baldock, O. Bousquet,
    S. Gelly, and D. Keysers, “What Do Neural Networks Learn When Trained With Random
    Labels?” in *Advances in Neural Information Processing Systems*, vol. 33.   Curran
    Associates, Inc., 2020, pp. 19 693–19 704.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] H. Maennel, I. M. Alabdulmohsin, I. O. Tolstikhin, R. Baldock, O. Bousquet,
    S. Gelly, 和 D. Keysers, “神经网络在随机标签训练时学到了什么？” 在*神经信息处理系统进展*，第33卷。Curran Associates,
    Inc.，2020年，页码19,693–19,704。'
- en: '[31] C. Stephenson, S. Padhy, A. Ganesh, Y. Hui, H. Tang, and S. Chung, “On
    the geometry of generalization and memorization in deep neural networks,” May
    2021.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] C. Stephenson, S. Padhy, A. Ganesh, Y. Hui, H. Tang, 和 S. Chung, “深度神经网络中的泛化与记忆的几何学”，2021年5月。'
- en: '[32] M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer Feed-Forward
    Layers Are Key-Value Memories. [Online]. Available: http://arxiv.org/abs/2012.14913'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] M. Geva, R. Schuster, J. Berant, 和 O. Levy. Transformer前馈层是键值记忆。[在线]。可用：
    http://arxiv.org/abs/2012.14913'
- en: '[33] D. Krueger*, N. Ballas*, S. Jastrzebski*, D. Arpit*, M. S. Kanwal, T. Maharaj,
    E. Bengio, A. Fischer, and A. Courville, “Deep Nets Don’t Learn via Memorization,”
    Feb. 2017.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] D. Krueger*, N. Ballas*, S. Jastrzebski*, D. Arpit*, M. S. Kanwal, T.
    Maharaj, E. Bengio, A. Fischer, 和 A. Courville, “深度网络不会通过记忆学习”，2017年2月。'
- en: '[34] V. N. Vapnik, “Adaptive and learning systems for signal processing communications,
    and control,” *Statistical learning theory*, 1998.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] V. N. Vapnik, “信号处理、通信和控制的自适应与学习系统”，*统计学习理论*，1998年。'
- en: '[35] P. L. Bartlett and S. Mendelson, “Rademacher and gaussian complexities:
    Risk bounds and structural results,” in *International Conference on Computational
    Learning Theory*.   Springer, 2001, pp. 224–240.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] P. L. Bartlett 和 S. Mendelson， “Rademacher和高斯复杂性：风险界限与结构性结果，” 发表在*计算学习理论国际会议*。Springer，2001年，第224–240页。'
- en: '[36] S. Mukherjee, P. Niyogi, T. Poggio, and R. Rifkin, “Statistical learning:
    Stability is sufficient for generalization and necessary and sufficient for consistency
    of empirical risk minimization,” 2002.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Mukherjee, P. Niyogi, T. Poggio, 和 R. Rifkin， “统计学习：稳定性足以实现泛化，并且对经验风险最小化的一致性既必要又充分，”
    2002年。'
- en: '[37] O. Bousquet and A. Elisseeff, “Stability and generalization,” *The Journal
    of Machine Learning Research*, vol. 2, pp. 499–526, 2002.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] O. Bousquet 和 A. Elisseeff， “稳定性与泛化，” *机器学习研究杂志*，第2卷，第499–526页，2002年。'
- en: '[38] T. Poggio, R. Rifkin, S. Mukherjee, and P. Niyogi, “General conditions
    for predictivity in learning theory,” *Nature*, vol. 428, no. 6981, pp. 419–422,
    2004.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] T. Poggio, R. Rifkin, S. Mukherjee, 和 P. Niyogi， “学习理论中的预测性一般条件，” *自然*，第428卷，第6981期，第419–422页，2004年。'
- en: '[39] I. Goodfellow, Y. Bengio, and A. Courville, *Deep learning*, 2016.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] I. Goodfellow, Y. Bengio, 和 A. Courville， *深度学习*，2016年。'
- en: '[40] W. J. Reed, “The Pareto, Zipf and other power laws,” *Economics Letters*,
    vol. 74, no. 1, pp. 15–19, Dec. 2001.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] W. J. Reed， “帕累托、齐夫和其他幂律，” *经济学通讯*，第74卷，第1期，第15–19页，2001年12月。'
- en: '[41] J. Gu and V. Tresp, “Neural Network Memorization Dissection,” Nov. 2019.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Gu 和 V. Tresp， “神经网络记忆剖析，” 2019年11月。'
- en: '[42] J. Frankle, D. J. Schwab, and A. S. Morcos, “The Early Phase of Neural
    Network Training,” in *International Conference on Learning Representations*,
    Sep. 2019.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. Frankle, D. J. Schwab, 和 A. S. Morcos， “神经网络训练的早期阶段，” 发表在*国际学习表征会议*，2019年9月。'
- en: '[43] A. Achille, M. Rovere, and S. Soatto, “Critical learning periods in deep
    networks,” in *International Conference on Learning Representations*, 2018.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] A. Achille, M. Rovere, 和 S. Soatto， “深度网络中的关键学习阶段，” 发表在*国际学习表征会议*，2018年。'
- en: '[44] V. Papyan, X. Han, and D. L. Donoho, “Prevalence of neural collapse during
    the terminal phase of deep learning training,” *Proceedings of the National Academy
    of Sciences*, vol. 117, no. 40, pp. 24 652–24 663, 2020.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] V. Papyan, X. Han, 和 D. L. Donoho， “深度学习训练终期的神经崩溃普遍性，” *美国国家科学院院刊*，第117卷，第40期，第24 652–24 663页，2020年。'
- en: '[45] C. Agarwal, D. D’souza, and S. Hooker, “Estimating Example Difficulty
    Using Variance of Gradients,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022, pp. 10 368–10 378.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C. Agarwal, D. D’souza, 和 S. Hooker， “利用梯度方差估计样本难度，” 发表在*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，第10 368–10 378页。'
- en: '[46] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are
    features in deep neural networks?” in *Advances in Neural Information Processing
    Systems*, vol. 27.   Curran Associates, Inc., 2014.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Yosinski, J. Clune, Y. Bengio, 和 H. Lipson， “深度神经网络中的特征可转移性如何？” 发表在*神经信息处理系统进展*，第27卷。Curran
    Associates, Inc.，2014年。'
- en: '[47] M. Raghu, J. Gilmer, J. Yosinski, and J. Sohl-Dickstein, “SVCCA: Singular
    Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability,”
    in *Advances in Neural Information Processing Systems*, vol. 30.   Curran Associates,
    Inc., 2017.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] M. Raghu, J. Gilmer, J. Yosinski, 和 J. Sohl-Dickstein， “SVCCA：用于深度学习动态和可解释性的奇异向量典型相关分析，”
    发表在*神经信息处理系统进展*，第30卷。Curran Associates, Inc.，2017年。'
- en: '[48] A. Morcos, M. Raghu, and S. Bengio, “Insights on representational similarity
    in neural networks with canonical correlation,” in *Advances in Neural Information
    Processing Systems*, vol. 31.   Curran Associates, Inc., 2018.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. Morcos, M. Raghu, 和 S. Bengio， “通过典型相关分析揭示神经网络中的表征相似性，” 发表在*神经信息处理系统进展*，第31卷。Curran
    Associates, Inc.，2018年。'
- en: '[49] A. Ansuini, A. Laio, J. H. Macke, and D. Zoccolan, “Intrinsic dimension
    of data representations in deep neural networks,” in *Advances in Neural Information
    Processing Systems*, vol. 32.   Curran Associates, Inc., 2019.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] A. Ansuini, A. Laio, J. H. Macke, 和 D. Zoccolan， “深度神经网络中数据表示的内在维度，” 发表在*神经信息处理系统进展*，第32卷。Curran
    Associates, Inc.，2019年。'
- en: '[50] S. Chatterjee, “Learning and Memorization,” in *Proceedings of the 35th
    International Conference on Machine Learning*.   PMLR, Jul. 2018, pp. 755–763.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Chatterjee， “学习与记忆，” 发表在*第35届国际机器学习会议论文集*。PMLR，2018年7月，第755–763页。'
- en: '[51] C. Zhang, S. Bengio, M. Hardt, M. C. Mozer, and Y. Singer, “Identity Crisis:
    Memorization and Generalization under Extreme Overparameterization,” Jan. 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] C. Zhang, S. Bengio, M. Hardt, M. C. Mozer, 和 Y. Singer， “身份危机：极端过参数化下的记忆与泛化，”
    2020年1月。'
- en: '[52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is All you Need,” in *Advances in Neural
    Information Processing Systems*, vol. 30.   Curran Associates, Inc., 2017.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“注意力机制就是你所需要的一切，”发表于 *神经信息处理系统进展*，第30卷。 Curran Associates,
    Inc.，2017年。'
- en: '[53] S. Sukhbaatar, E. Grave, G. Lample, H. Jegou, and A. Joulin. Augmenting
    Self-attention with Persistent Memory. [Online]. Available: http://arxiv.org/abs/1907.01470'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] S. Sukhbaatar, E. Grave, G. Lample, H. Jegou, 和 A. Joulin. 增强自注意力与持久记忆。
    [在线]. 可用: http://arxiv.org/abs/1907.01470'
- en: '[54] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei. Knowledge Neurons
    in Pretrained Transformers. [Online]. Available: http://arxiv.org/abs/2104.08696'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, 和 F. Wei. 预训练 Transformer 中的知识神经元。
    [在线]. 可用: http://arxiv.org/abs/2104.08696'
- en: '[55] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, “Privacy Risk in Machine
    Learning: Analyzing the Connection to Overfitting,” in *2018 IEEE 31st Computer
    Security Foundations Symposium (CSF)*, Jul. 2018, pp. 268–282.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] S. Yeom, I. Giacomelli, M. Fredrikson, 和 S. Jha，“机器学习中的隐私风险：分析与过拟合的关联，”发表于
    *2018年IEEE第31届计算机安全基础研讨会 (CSF)*，2018年7月，第268–282页。'
- en: '[56] Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang, H. Tang, C. A. Gunter,
    and K. Chen, “Understanding Membership Inferences on Well-Generalized Learning
    Models,” Feb. 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang, H. Tang, C. A. Gunter,
    和 K. Chen，“理解对良好泛化学习模型的成员推断，” 2018年2月。'
- en: '[57] P. L. Bartlett, P. M. Long, G. Lugosi, and A. Tsigler, “Benign overfitting
    in linear regression,” *Proceedings of the National Academy of Sciences*, vol.
    117, no. 48, pp. 30 063–30 070, Dec. 2020.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] P. L. Bartlett, P. M. Long, G. Lugosi, 和 A. Tsigler，“线性回归中的良性过拟合，” *国家科学院院刊*，第117卷，第48期，第30 063–30 070页，2020年12月。'
- en: '[58] Z. Li, Z.-H. Zhou, and A. Gretton, “Towards an Understanding of Benign
    Overfitting in Neural Networks,” Jun. 2021.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Z. Li, Z.-H. Zhou, 和 A. Gretton，“理解神经网络中的良性过拟合，” 2021年6月。'
- en: '[59] Y. Cao, Z. Chen, M. Belkin, and Q. Gu, “Benign Overfitting in Two-layer
    Convolutional Neural Networks,” *Advances in Neural Information Processing Systems*,
    vol. 35, pp. 25 237–25 250, Dec. 2022.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Y. Cao, Z. Chen, M. Belkin, 和 Q. Gu，“两层卷积神经网络中的良性过拟合，” *神经信息处理系统进展*，第35卷，第25 237–25 250页，2022年12月。'
- en: '[60] A. Sablayrolles, M. Douze, C. Schmid, and H. Jégou, “D\’ej\‘a Vu: An empirical
    evaluation of the memorization properties of ConvNets,” Sep. 2018.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] A. Sablayrolles, M. Douze, C. Schmid, 和 H. Jégou，“D\’ej\‘a Vu：对 ConvNets
    记忆特性的实证评估，” 2018年9月。'
- en: '[61] D. Patel and P. S. Sastry, “Memorization in Deep Neural Networks: Does
    the Loss Function Matter?” in *Advances in Knowledge Discovery and Data Mining*,
    ser. Lecture Notes in Computer Science, K. Karlapalem, H. Cheng, N. Ramakrishnan,
    R. K. Agrawal, P. K. Reddy, J. Srivastava, and T. Chakraborty, Eds.   Cham: Springer
    International Publishing, 2021, pp. 131–142.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] D. Patel 和 P. S. Sastry，“深度神经网络中的记忆：损失函数是否重要？”发表于 *知识发现与数据挖掘进展*，系列 计算机科学讲义笔记，K.
    Karlapalem, H. Cheng, N. Ramakrishnan, R. K. Agrawal, P. K. Reddy, J. Srivastava,
    和 T. Chakraborty 编辑。 查姆：Springer 国际出版，2021年，第131–142页。'
- en: '[62] Y. Li, C. Wei, and T. Ma, “Towards Explaining the Regularization Effect
    of Initial Large Learning Rate in Training Neural Networks,” in *Advances in Neural
    Information Processing Systems*, vol. 32.   Curran Associates, Inc., 2019.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Y. Li, C. Wei, 和 T. Ma，“解释训练神经网络时初始大学习率的正则化效应，”发表于 *神经信息处理系统进展*，第32卷。
    Curran Associates, Inc.，2019年。'
- en: '[63] E. Kharitonov, M. Baroni, and D. Hupkes, “How BPE Affects Memorization
    in Transformers,” Dec. 2021.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] E. Kharitonov, M. Baroni, 和 D. Hupkes，“BPE 如何影响 Transformer 中的记忆，” 2021年12月。'
- en: '[64] K. Leino and M. Fredrikson, “Stolen Memories: Leveraging Model Memorization
    for Calibrated {}White-Box{} Membership Inference,” in *29th USENIX Security Symposium
    (USENIX Security 20)*, 2020, pp. 1605–1622.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] K. Leino 和 M. Fredrikson，“被窃取的记忆：利用模型记忆进行校准的{}白盒{}成员推断，”发表于 *第29届 USENIX
    安全研讨会 (USENIX Security 20)*，2020年，第1605–1622页。'
- en: '[65] H. Xu, X. Liu, W. Wang, Z. Liu, A. K. Jain, and J. Tang, “How does the
    Memorization of Neural Networks Impact Adversarial Robust Models?” in *Proceedings
    of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, ser.
    KDD ’23.   New York, NY, USA: Association for Computing Machinery, Aug. 2023,
    pp. 2801–2812.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] H. Xu, X. Liu, W. Wang, Z. Liu, A. K. Jain, 和 J. Tang，“神经网络的记忆如何影响对抗鲁棒模型？”发表于
    *第29届 ACM SIGKDD 知识发现与数据挖掘会议论文集*，系列 KDD ’23。 纽约，NY，美国：计算机协会，2023年8月，第2801–2812页。'
- en: '[66] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership Inference
    Attacks Against Machine Learning Models,” in *2017 IEEE Symposium on Security
    and Privacy (SP)*, pp. 3–18.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] R. Shokri, M. Stronati, C. Song 和 V. Shmatikov，“对抗机器学习模型的成员推断攻击”，见于 *2017
    年 IEEE 安全与隐私研讨会 (SP)*，第 3–18 页。'
- en: '[67] M. Jagielski, O. Thakkar, F. Tramer, D. Ippolito, K. Lee, N. Carlini,
    E. Wallace, S. Song, A. G. Thakurta, N. Papernot, and C. Zhang, “Measuring Forgetting
    of Memorized Training Examples,” in *The Eleventh International Conference on
    Learning Representations*, Sep. 2022.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] M. Jagielski, O. Thakkar, F. Tramer, D. Ippolito, K. Lee, N. Carlini,
    E. Wallace, S. Song, A. G. Thakurta, N. Papernot 和 C. Zhang，“测量记忆化训练样本的遗忘”，见于
    *第十一届国际学习表示大会*，2022 年 9 月。'
- en: '[68] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards Deep
    Learning Models Resistant to Adversarial Attacks.” [Online]. Available: https://arxiv.org/abs/1706.06083v4'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] A. Madry, A. Makelov, L. Schmidt, D. Tsipras 和 A. Vladu，“朝着抵御对抗攻击的深度学习模型迈进。”
    [在线]. 可用: https://arxiv.org/abs/1706.06083v4'
- en: '[69] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and Harnessing
    Adversarial Examples.” [Online]. Available: https://arxiv.org/abs/1412.6572v3'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] I. J. Goodfellow, J. Shlens 和 C. Szegedy，“解释和利用对抗样本。” [在线]. 可用: https://arxiv.org/abs/1412.6572v3'
- en: '[70] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry,
    “Adversarial Examples Are Not Bugs, They Are Features.” [Online]. Available: http://arxiv.org/abs/1905.02175'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran 和 A. Madry，“对抗样本不是错误，它们是特征。”
    [在线]. 可用: http://arxiv.org/abs/1905.02175'
- en: '[71] L. Schmidt, S. Santurkar, D. Tsipras, K. Talwar, and A. Madry, “Adversarially
    Robust Generalization Requires More Data,” in *Advances in Neural Information
    Processing Systems*, vol. 31.   Curran Associates, Inc., 2018.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] L. Schmidt, S. Santurkar, D. Tsipras, K. Talwar 和 A. Madry，“对抗性鲁棒泛化需要更多数据”，见于
    *神经信息处理系统进展*，第 31 卷。Curran Associates, Inc.，2018 年。'
- en: '[72] L. Rice, E. Wong, and Z. Kolter, “Overfitting in adversarially robust
    deep learning,” in *Proceedings of the 37th International Conference on Machine
    Learning*.   PMLR, Nov. 2020, pp. 8093–8104.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] L. Rice, E. Wong 和 Z. Kolter，“对抗性鲁棒深度学习中的过拟合”，见于 *第 37 届国际机器学习大会论文集*。PMLR，2020
    年 11 月，第 8093–8104 页。'
- en: '[73] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. E. Ghaoui, and M. Jordan, “Theoretically
    Principled Trade-off between Robustness and Accuracy,” in *Proceedings of the
    36th International Conference on Machine Learning*.   PMLR, pp. 7472–7482. [Online].
    Available: https://proceedings.mlr.press/v97/zhang19p.html'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. E. Ghaoui 和 M. Jordan，“理论上原则的鲁棒性与准确性权衡”，见于
    *第 36 届国际机器学习大会论文集*。PMLR，第 7472–7482 页。[在线]. 可用: https://proceedings.mlr.press/v97/zhang19p.html'
- en: '[74] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar,
    and L. Zhang, “Deep Learning with Differential Privacy,” in *Proceedings of the
    2016 ACM SIGSAC Conference on Computer and Communications Security*, pp. 308–318\.
    [Online]. Available: http://arxiv.org/abs/1607.00133'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar
    和 L. Zhang，“具有差分隐私的深度学习”，见于 *2016 年 ACM SIGSAC 计算机与通信安全会议论文集*，第 308–318 页。[在线].
    可用: http://arxiv.org/abs/1607.00133'
- en: '[75] O. Thakkar, S. Ramaswamy, R. Mathews, and F. Beaufays, “Understanding
    Unintended Memorization in Federated Learning,” Jun. 2020.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] O. Thakkar, S. Ramaswamy, R. Mathews 和 F. Beaufays，“理解联邦学习中的无意记忆”，2020
    年 6 月。'
- en: '[76] S. Caton and C. Haas, “Fairness in machine learning: A survey,” *ACM Computing
    Surveys*, 2020.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] S. Caton 和 C. Haas，“机器学习中的公平性：综述”，*ACM 计算机调查*，2020 年。'
- en: '[77] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness through
    awareness,” in *Proceedings of the 3rd innovations in theoretical computer science
    conference*, 2012, pp. 214–226.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] C. Dwork, M. Hardt, T. Pitassi, O. Reingold 和 R. Zemel，“通过意识实现公平性”，见于
    *第 3 届理论计算机科学创新会议论文集*，2012 年，第 214–226 页。'
- en: '[78] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian,
    “Certifying and removing disparate impact,” in *proceedings of the 21th ACM SIGKDD
    international conference on knowledge discovery and data mining*, 2015, pp. 259–268.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger 和 S. Venkatasubramanian，“认证和消除差异影响”，见于
    *第 21 届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*，2015 年，第 259–268 页。'
- en: '[79] M. Hardt, E. Price, and N. Srebro, “Equality of opportunity in supervised
    learning,” *Advances in neural information processing systems*, vol. 29, 2016.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] M. Hardt, E. Price 和 N. Srebro，“监督学习中的机会平等”，*神经信息处理系统进展*，第 29 卷，2016 年。'
- en: '[80] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, “Learning fair
    representations,” in *International conference on machine learning*.   PMLR, 2013,
    pp. 325–333.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, 和 C. Dwork，“学习公平表示”，在 *国际机器学习会议*
    中。 PMLR，2013年，第325–333页。'
- en: '[81] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
    “Grad-cam: Visual explanations from deep networks via gradient-based localization,”
    in *Proceedings of the IEEE international conference on computer vision*, 2017,
    pp. 618–626.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, 和 D. Batra，“Grad-CAM：通过基于梯度的定位从深度网络中生成可视解释”，在
    *IEEE国际计算机视觉会议论文集*，2017年，第618–626页。'
- en: '[82] K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside convolutional
    networks: Visualising image classification models and saliency maps,” *arXiv preprint
    arXiv:1312.6034*, 2013.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] K. Simonyan, A. Vedaldi, 和 A. Zisserman，“深入卷积网络：可视化图像分类模型和显著性图”，*arXiv预印本
    arXiv:1312.6034*，2013年。'
- en: '[83] R. C. Fong and A. Vedaldi, “Interpretable explanations of black boxes
    by meaningful perturbation,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 3429–3437.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] R. C. Fong 和 A. Vedaldi，“通过有意义的扰动对黑箱进行可解释性解释”，在 *IEEE国际计算机视觉会议论文集*，2017年，第3429–3437页。'
- en: '[84] R. Dwivedi, D. Dave, H. Naik, S. Singhal, R. Omer, P. Patel, B. Qian,
    Z. Wen, T. Shah, G. Morgan *et al.*, “Explainable ai (xai): Core ideas, techniques,
    and solutions,” *ACM Computing Surveys*, vol. 55, no. 9, pp. 1–33, 2023.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] R. Dwivedi, D. Dave, H. Naik, S. Singhal, R. Omer, P. Patel, B. Qian,
    Z. Wen, T. Shah, G. Morgan *等*，“可解释人工智能（XAI）：核心理念、技术与解决方案，” *ACM Computing Surveys*，第55卷，第9期，第1–33页，2023年。'
- en: '[85] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi,
    “A survey of methods for explaining black box models,” *ACM computing surveys
    (CSUR)*, vol. 51, no. 5, pp. 1–42, 2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, 和 D. Pedreschi，“解释黑箱模型的方法综述，”
    *ACM computing surveys (CSUR)*，第51卷，第5期，第1–42页，2018年。'
- en: '[86] F. Tramer, V. Atlidakis, R. Geambasu, D. Hsu, J.-P. Hubaux, M. Humbert,
    A. Juels, and H. Lin, “Fairtest: Discovering unwarranted associations in data-driven
    applications,” in *2017 IEEE European Symposium on Security and Privacy (EuroS&P)*.   IEEE,
    2017, pp. 401–416.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] F. Tramer, V. Atlidakis, R. Geambasu, D. Hsu, J.-P. Hubaux, M. Humbert,
    A. Juels, 和 H. Lin，“Fairtest：发现数据驱动应用中的不合理关联”，在 *2017 IEEE欧洲安全与隐私研讨会（EuroS&P）*
    中。 IEEE，2017年，第401–416页。'
- en: '[87] X. Gao, J. Zhai, S. Ma, C. Shen, Y. Chen, and Q. Wang, “FairNeuron: Improving
    deep neural network fairness with adversary games on selective neurons,” in *Proceedings
    of the 44th International Conference on Software Engineering*, ser. ICSE ’22.   Association
    for Computing Machinery, pp. 921–933\. [Online]. Available: https://dl.acm.org/doi/10.1145/3510003.3510087'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] X. Gao, J. Zhai, S. Ma, C. Shen, Y. Chen, 和 Q. Wang，“FairNeuron：通过对选择性神经元的对抗游戏提高深度神经网络的公平性”，在
    *第44届国际软件工程会议论文集* 中，ICSE ’22系列。计算机协会，第921–933页。 [在线]。可用: https://dl.acm.org/doi/10.1145/3510003.3510087'
- en: '[88] G. Barone, A. Cunchala, and R. Nunez. Increasing Fairness in Classification
    of Out of Distribution Data for Facial Recognition. [Online]. Available: http://arxiv.org/abs/2404.03876'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] G. Barone, A. Cunchala, 和 R. Nunez. 提高人脸识别中的数据分布外分类的公平性。 [在线]。可用: http://arxiv.org/abs/2404.03876'
- en: '[89] H. Ritter, A. Botev, and D. Barber, “Online Structured Laplace Approximations
    For Overcoming Catastrophic Forgetting,” May 2018.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] H. Ritter, A. Botev, 和 D. Barber，“用于克服灾难性遗忘的在线结构化拉普拉斯近似”，2018年5月。'
- en: '[90] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A.
    Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath,
    D. Kumaran, and R. Hadsell, “Overcoming catastrophic forgetting in neural networks,”
    *Proceedings of the National Academy of Sciences*, vol. 114, no. 13, pp. 3521–3526,
    Mar. 2017.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A.
    A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C.
    Clopath, D. Kumaran, 和 R. Hadsell，“克服神经网络中的灾难性遗忘”，*国家科学院院刊*，第114卷，第13期，第3521–3526页，2017年3月。'
- en: '[91] C. Shao and Y. Feng, “Overcoming Catastrophic Forgetting beyond Continual
    Learning: Balanced Training for Neural Machine Translation,” in *Proceedings of
    the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, S. Muresan, P. Nakov, and A. Villavicencio, Eds.   Dublin, Ireland:
    Association for Computational Linguistics, May 2022, pp. 2023–2036.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] C. Shao 和 Y. Feng，“超越持续学习克服灾难性遗忘：神经机器翻译的平衡训练”，在 *第60届计算语言学协会年会（第一卷：长篇论文）*
    中，S. Muresan, P. Nakov, 和 A. Villavicencio 编。都柏林，爱尔兰：计算语言学协会，2022年5月，第2023–2036页。'
- en: '[92] T. L. Hayes, K. Kafle, R. Shrestha, M. Acharya, and C. Kanan, “REMIND
    Your Neural Network to Prevent Catastrophic Forgetting,” in *Computer Vision –
    ECCV 2020*, ser. Lecture Notes in Computer Science, A. Vedaldi, H. Bischof, T. Brox,
    and J.-M. Frahm, Eds.   Cham: Springer International Publishing, 2020, pp. 466–483.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] T. L. Hayes, K. Kafle, R. Shrestha, M. Acharya, 和 C. Kanan, “提醒你的神经网络以防止灾难性遗忘，”发表于*计算机视觉
    – ECCV 2020*，系列计算机科学讲义，A. Vedaldi, H. Bischof, T. Brox, 和 J.-M. Frahm编辑。   沙姆：Springer国际出版社，2020年，第466–483页。'
- en: '[93] H. Liu, Y. Yang, and X. Wang, “Overcoming Catastrophic Forgetting in Graph
    Neural Networks,” *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 35, no. 10, pp. 8653–8661, May 2021.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] H. Liu, Y. Yang, 和 X. Wang, “克服图神经网络中的灾难性遗忘，”*AAAI人工智能会议论文集*，第35卷，第10期，第8653–8661页，2021年5月。'
- en: '[94] Z. Li and D. Hoiem, “Learning without Forgetting,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, vol. 40, no. 12, pp. 2935–2947,
    Dec. 2018.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Z. Li 和 D. Hoiem, “无遗忘学习，”*IEEE模式分析与机器智能学报*，第40卷，第12期，第2935–2947页，2018年12月。'
- en: '[95] M. Toneva, A. Sordoni, R. T. des Combes, A. Trischler, Y. Bengio, and
    G. J. Gordon, “An Empirical Study of Example Forgetting during Deep Neural Network
    Learning,” Nov. 2019.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] M. Toneva, A. Sordoni, R. T. des Combes, A. Trischler, Y. Bengio, 和 G.
    J. Gordon, “深度神经网络学习中的示例遗忘实证研究，”2019年11月。'
- en: '[96] P. Maini, S. Garg, Z. Lipton, and J. Z. Kolter, “Characterizing Datapoints
    via Second-Split Forgetting,” *Advances in Neural Information Processing Systems*,
    vol. 35, pp. 30 044–30 057, Dec. 2022.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] P. Maini, S. Garg, Z. Lipton, 和 J. Z. Kolter, “通过二次分裂遗忘来表征数据点，”*神经信息处理系统进展*，第35卷，第30 044–30 057页，2022年12月。'
- en: '[97] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. W. Tsang, and M. Sugiyama,
    “Co-teaching: Robust training of deep neural networks with extremely noisy labels,”
    in *Proceedings of the 32nd International Conference on Neural Information Processing
    Systems*, ser. NIPS’18.   Red Hook, NY, USA: Curran Associates Inc., Dec. 2018,
    pp. 8536–8546.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. W. Tsang, 和 M. Sugiyama,
    “共同教学：对具有极度嘈杂标签的深度神经网络进行稳健训练，”发表于*第32届神经信息处理系统国际会议论文集*，系列NIPS’18。 纽约州红钩：Curran
    Associates Inc.，2018年12月，第8536–8546页。'
- en: '[98] Q. Yao, H. Yang, B. Han, G. Niu, and J. T.-Y. Kwok, “Searching to Exploit
    Memorization Effect in Learning with Noisy Labels,” in *Proceedings of the 37th
    International Conference on Machine Learning*.   PMLR, Nov. 2020, pp. 10 789–10 798.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Q. Yao, H. Yang, B. Han, G. Niu, 和 J. T.-Y. Kwok, “搜索利用记忆效应以应对嘈杂标签学习，”发表于*第37届国际机器学习会议论文集*。
    PMLR，2020年11月，第10 789–10 798页。'
- en: '[99] S. Liu, J. Niles-Weed, N. Razavian, and C. Fernandez-Granda, “Early-Learning
    Regularization Prevents Memorization of Noisy Labels,” in *Advances in Neural
    Information Processing Systems*, vol. 33.   Curran Associates, Inc., 2020, pp.
    20 331–20 342.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] S. Liu, J. Niles-Weed, N. Razavian, 和 C. Fernandez-Granda, “早期学习正则化防止嘈杂标签的记忆，”发表于*神经信息处理系统进展*，第33卷。
    Curran Associates, Inc.，2020年，第20 331–20 342页。'
- en: '[100] X. Xia, T. Liu, B. Han, C. Gong, N. Wang, Z. Ge, and Y. Chang, “Robust
    early-learning: Hindering the memorization of noisy labels,” in *International
    Conference on Learning Representations*, Oct. 2020.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] X. Xia, T. Liu, B. Han, C. Gong, N. Wang, Z. Ge, 和 Y. Chang, “稳健的早期学习：阻碍嘈杂标签的记忆，”发表于*国际学习表征会议*，2020年10月。'
- en: '[101] V. Pondenkandath, M. Alberti, S. Puran, R. Ingold, and M. Liwicki, “Leveraging
    Random Label Memorization for Unsupervised Pre-Training,” Nov. 2018.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] V. Pondenkandath, M. Alberti, S. Puran, R. Ingold, 和 M. Liwicki, “利用随机标签记忆进行无监督预训练，”2018年11月。'
- en: '[102] Z. Zhou, J. Yao, Y.-F. Wang, B. Han, and Y. Zhang, “Contrastive Learning
    with Boosted Memorization,” in *Proceedings of the 39th International Conference
    on Machine Learning*.   PMLR, Jun. 2022, pp. 27 367–27 377.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Z. Zhou, J. Yao, Y.-F. Wang, B. Han, 和 Y. Zhang, “增强记忆的对比学习，”发表于*第39届国际机器学习会议论文集*。
    PMLR，2022年6月，第27 367–27 377页。'
- en: '[103] J. Zhang, Y. Hong, and Q. Zhao, “Memorization Weights for Instance Reweighting
    in Adversarial Training,” *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 37, no. 9, pp. 11 228–11 236, Jun. 2023.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] J. Zhang, Y. Hong, 和 Q. Zhao, “用于对抗训练的实例重加权的记忆权重，”*AAAI人工智能会议论文集*，第37卷，第9期，第11 228–11 236页，2023年6月。'
- en: '[104] C. Zhu, A. S. Rawat, M. Zaheer, S. Bhojanapalli, D. Li, F. Yu, and S. Kumar,
    “Modifying Memories in Transformer Models,” Dec. 2020.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] C. Zhu, A. S. Rawat, M. Zaheer, S. Bhojanapalli, D. Li, F. Yu, 和 S. Kumar,
    “在变换器模型中修改记忆，”2020年12月。'
- en: '[105] U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis, “Generalization
    through Memorization: Nearest Neighbor Language Models,” in *International Conference
    on Learning Representations*, Sep. 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, 和 M. Lewis，“通过记忆进行泛化：最近邻语言模型，”发表于*国际表示学习会议*，2019年9月。'
- en: '[106] D. Yogatama, C. de Masson d’Autume, and L. Kong, “Adaptive Semiparametric
    Language Models,” *Transactions of the Association for Computational Linguistics*,
    vol. 9, pp. 362–373, Apr. 2021.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] D. Yogatama, C. de Masson d’Autume, 和 L. Kong，“自适应半参数语言模型，”*计算语言学协会会刊*，第9卷，第362–373页，2021年4月。'
- en: '[107] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, “Retrieval Augmented
    Language Model Pre-Training,” in *Proceedings of the 37th International Conference
    on Machine Learning*.   PMLR, Nov. 2020, pp. 3929–3938.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] K. Guu, K. Lee, Z. Tung, P. Pasupat, 和 M. Chang，“检索增强语言模型预训练，”发表于*第37届国际机器学习会议论文集*。
    PMLR，2020年11月，第3929–3938页。'
- en: '[108] M. Lewis, M. Ghazvininejad, G. Ghosh, A. Aghajanyan, S. Wang, and L. Zettlemoyer,
    “Pre-training via Paraphrasing,” in *Advances in Neural Information Processing
    Systems*, vol. 33.   Curran Associates, Inc., 2020, pp. 18 470–18 481.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] M. Lewis, M. Ghazvininejad, G. Ghosh, A. Aghajanyan, S. Wang, 和 L. Zettlemoyer，“通过同义改写进行预训练，”发表于*神经信息处理系统进展*，第33卷。Curran
    Associates, Inc.，2020年，第18 470–18 481页。'
- en: '[109] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler,
    M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks,” in *Advances in Neural Information
    Processing Systems*, vol. 33.   Curran Associates, Inc., 2020, pp. 9459–9474.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H.
    Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, 和 D. Kiela，“用于知识密集型NLP任务的检索增强生成，”发表于*神经信息处理系统进展*，第33卷。Curran
    Associates, Inc.，2020年，第9459–9474页。'
- en: '[110] Y. Wu, M. N. Rabe, D. Hutchins, and C. Szegedy, “Memorizing Transformers,”
    Mar. 2022.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Y. Wu, M. N. Rabe, D. Hutchins, 和 C. Szegedy，“记忆变换器，”2022年3月。'
- en: '[111] Y. Tay, V. Tran, M. Dehghani, J. Ni, D. Bahri, H. Mehta, Z. Qin, K. Hui,
    Z. Zhao, J. Gupta, T. Schuster, W. W. Cohen, and D. Metzler, “Transformer Memory
    as a Differentiable Search Index,” *Advances in Neural Information Processing
    Systems*, vol. 35, pp. 21 831–21 843, Dec. 2022.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Y. Tay, V. Tran, M. Dehghani, J. Ni, D. Bahri, H. Mehta, Z. Qin, K. Hui,
    Z. Zhao, J. Gupta, T. Schuster, W. W. Cohen, 和 D. Metzler，“变换器记忆作为可微分的搜索索引，”*神经信息处理系统进展*，第35卷，第21 831–21 843页，2022年12月。'
- en: '[112] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei, “Knowledge neurons
    in pretrained transformers,” *arXiv preprint arXiv:2104.08696*, 2021.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, 和 F. Wei，“预训练变换器中的知识神经元，”*arXiv预印本
    arXiv:2104.08696*，2021年。'
- en: '[113] N. De Cao, W. Aziz, and I. Titov, “Editing factual knowledge in language
    models,” *arXiv preprint arXiv:2104.08164*, 2021.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] N. De Cao, W. Aziz, 和 I. Titov，“编辑语言模型中的事实知识，”*arXiv预印本 arXiv:2104.08164*，2021年。'
- en: '[114] E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning, “Fast model
    editing at scale,” *arXiv preprint arXiv:2110.11309*, 2021.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] E. Mitchell, C. Lin, A. Bosselut, C. Finn, 和 C. D. Manning，“大规模快速模型编辑，”*arXiv预印本
    arXiv:2110.11309*，2021年。'
- en: '[115] K. Meng, D. Bau, A. Andonian, and Y. Belinkov, “Locating and editing
    factual associations in gpt,” *Advances in Neural Information Processing Systems*,
    vol. 35, pp. 17 359–17 372, 2022.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] K. Meng, D. Bau, A. Andonian, 和 Y. Belinkov，“在GPT中定位和编辑事实关联，”*神经信息处理系统进展*，第35卷，第17 359–17 372页，2022年。'
- en: '[116] K. Meng, A. S. Sharma, A. Andonian, Y. Belinkov, and D. Bau, “Mass-editing
    memory in a transformer,” *arXiv preprint arXiv:2210.07229*, 2022.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] K. Meng, A. S. Sharma, A. Andonian, Y. Belinkov, 和 D. Bau，“在变换器中大规模编辑记忆，”*arXiv预印本
    arXiv:2210.07229*，2022年。'
- en: '[117] A. Gupta, D. Sajnani, and G. Anumanchipalli. A unified framework for
    model editing. [Online]. Available: http://arxiv.org/abs/2403.14236'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] A. Gupta, D. Sajnani, 和 G. Anumanchipalli. 一个统一的模型编辑框架。[在线]. 可用： http://arxiv.org/abs/2403.14236'
- en: '[118] X. Yu, B. Han, J. Yao, G. Niu, I. Tsang, and M. Sugiyama, “How does disagreement
    help generalization against label corruption?” in *Proceedings of the 36th International
    Conference on Machine Learning*, ser. Proceedings of Machine Learning Research,
    K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97.   PMLR, 09–15 Jun 2019, pp.
    7164–7173\. [Online]. Available: https://proceedings.mlr.press/v97/yu19b.html'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] X. Yu, B. Han, J. Yao, G. Niu, I. Tsang, 和 M. Sugiyama，“不一致性如何有助于对抗标签污染的泛化？”发表于*第36届国际机器学习会议论文集*，K.
    Chaudhuri 和 R. Salakhutdinov 编， vol. 97。PMLR，2019年6月09–15日，第7164–7173页。[在线]. 可用：
    https://proceedings.mlr.press/v97/yu19b.html'
- en: '[119] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. W. Tsang, and M. Sugiyama,
    “Co-teaching: Robust training of deep neural networks with extremely noisy labels,”
    in *Proceedings of the 32nd International Conference on Neural Information Processing
    Systems*, ser. NIPS’18.   Red Hook, NY, USA: Curran Associates Inc., Dec. 2018,
    pp. 8536–8546.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. W. Tsang, 和 M. Sugiyama,
    “Co-teaching：具有极端噪声标签的深度神经网络的稳健训练，” 见于 *第32届国际神经信息处理系统会议论文集*，系列：NIPS’18。Red Hook,
    NY, USA: Curran Associates Inc.，2018年12月，第8536–8546页。'
- en: '[120] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei, “MentorNet: Learning
    Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels,” in
    *Proceedings of the 35th International Conference on Machine Learning*.   PMLR,
    Jul. 2018, pp. 2304–2313.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, 和 L. Fei-Fei, “MentorNet：为具有损坏标签的非常深度神经网络学习数据驱动的课程，”
    见于 *第35届国际机器学习会议论文集*。PMLR，2018年7月，第2304–2313页。'
- en: '[121] U. Khandelwal, A. Fan, D. Jurafsky, L. Zettlemoyer, and M. Lewis, “Nearest
    Neighbor Machine Translation,” in *International Conference on Learning Representations*,
    Oct. 2020.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] U. Khandelwal, A. Fan, D. Jurafsky, L. Zettlemoyer, 和 M. Lewis, “最近邻机器翻译，”
    见于 *国际表示学习会议*，2020年10月。'
- en: '[122] T. Févry, L. Baldini Soares, N. FitzGerald, E. Choi, and T. Kwiatkowski,
    “Entities as Experts: Sparse Memory Access with Entity Supervision,” in *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*,
    B. Webber, T. Cohn, Y. He, and Y. Liu, Eds.   Online: Association for Computational
    Linguistics, Nov. 2020, pp. 4937–4951.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] T. Févry, L. Baldini Soares, N. FitzGerald, E. Choi, 和 T. Kwiatkowski,
    “实体作为专家：通过实体监督的稀疏记忆访问，” 见于 *2020年自然语言处理实证方法会议（EMNLP）论文集*，B. Webber, T. Cohn, Y.
    He, 和 Y. Liu 编辑。在线：计算语言学协会，2020年11月，第4937–4951页。'
- en: '[123] P. Verga, H. Sun, L. B. Soares, and W. W. Cohen, “Facts as Experts: Adaptable
    and Interpretable Neural Memory over Symbolic Knowledge,” Jul. 2020.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] P. Verga, H. Sun, L. B. Soares, 和 W. W. Cohen, “事实作为专家：对符号知识的可适应和可解释的神经记忆，”
    2020年7月。'
- en: '[124] F. Petroni, T. Rocktäschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller,
    and S. Riedel, “Language models as knowledge bases?” *arXiv preprint arXiv:1909.01066*,
    2019.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] F. Petroni, T. Rocktäschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller,
    和 S. Riedel, “语言模型作为知识库？” *arXiv预印本 arXiv:1909.01066*，2019年。'
- en: '[125] M. Geva, R. Schuster, J. Berant, and O. Levy, “Transformer feed-forward
    layers are key-value memories,” *arXiv preprint arXiv:2012.14913*, 2020.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] M. Geva, R. Schuster, J. Berant, 和 O. Levy, “Transformer前馈层作为键值记忆，” *arXiv预印本
    arXiv:2012.14913*，2020年。'
- en: '[126] M. Geva, A. Caciularu, K. R. Wang, and Y. Goldberg, “Transformer feed-forward
    layers build predictions by promoting concepts in the vocabulary space,” *arXiv
    preprint arXiv:2203.14680*, 2022.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] M. Geva, A. Caciularu, K. R. Wang, 和 Y. Goldberg, “Transformer前馈层通过提升词汇空间中的概念来构建预测，”
    *arXiv预印本 arXiv:2203.14680*，2022年。'
- en: '[127] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep
    networks,” in *International conference on machine learning*.   PMLR, 2017, pp.
    3319–3328.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] M. Sundararajan, A. Taly, 和 Q. Yan, “深度网络的公理属性，” 见于 *国际机器学习会议*。PMLR，2017年，第3319–3328页。'
- en: '[128] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classification
    with Deep Convolutional Neural Networks,” in *Advances in Neural Information Processing
    Systems*, vol. 25.   Curran Associates, Inc., 2012.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行ImageNet分类，”
    见于 *神经信息处理系统进展*，第25卷。Curran Associates, Inc.，2012年。'
- en: '[129] Y. Cui, Z. Gu, D. Mahajan, L. van der Maaten, S. Belongie, and S.-N.
    Lim, “Measuring Dataset Granularity,” Dec. 2019.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Y. Cui, Z. Gu, D. Mahajan, L. van der Maaten, S. Belongie, 和 S.-N. Lim,
    “测量数据集粒度，” 2019年12月。'
- en: '[130] P. Chu, X. Bian, S. Liu, and H. Ling, “Feature Space Augmentation for
    Long-Tailed Data,” in *Computer Vision – ECCV 2020*, ser. Lecture Notes in Computer
    Science, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds.   Cham: Springer
    International Publishing, 2020, pp. 694–710.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] P. Chu, X. Bian, S. Liu, 和 H. Ling, “针对长尾数据的特征空间扩增，” 见于 *计算机视觉 – ECCV
    2020*，系列：计算机科学讲义，A. Vedaldi, H. Bischof, T. Brox, 和 J.-M. Frahm 编辑。Cham: Springer国际出版社，2020年，第694–710页。'
- en: '[131] S. Zhang, Z. Li, S. Yan, X. He, and J. Sun, “Distribution Alignment:
    A Unified Framework for Long-Tail Visual Recognition,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 2361–2370.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] S. Zhang, Z. Li, S. Yan, X. He, 和 J. Sun, “分布对齐：一个统一的长尾视觉识别框架，” 见于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，第2361–2370页。'
- en: '[132] A. Althnian, D. AlSaeed, H. Al-Baity, A. Samha, A. B. Dris, N. Alzakari,
    A. Abou Elwafa, and H. Kurdi, “Impact of Dataset Size on Classification Performance:
    An Empirical Evaluation in the Medical Domain,” *Applied Sciences*, vol. 11, no. 2,
    p. 796, Jan. 2021.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] A. Althnian, D. AlSaeed, H. Al-Baity, A. Samha, A. B. Dris, N. Alzakari,
    A. Abou Elwafa, 和 H. Kurdi，“数据集规模对分类性能的影响：医学领域的实证评估”，*应用科学*，第11卷，第2期，第796页，2021年1月。'
- en: '[133] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial networks,” *Communications
    of the ACM*, vol. 63, no. 11, pp. 139–144, 2020.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络”，*ACM通讯*，第63卷，第11期，第139–144页，2020年。'
- en: '[134] P. Dhariwal and A. Nichol, “Diffusion Models Beat GANs on Image Synthesis,”
    in *Advances in Neural Information Processing Systems*, vol. 34.   Curran Associates,
    Inc., 2021, pp. 8780–8794.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] P. Dhariwal 和 A. Nichol，“扩散模型在图像合成方面超越GANs”，发表于*神经信息处理系统进展*，第34卷。Curran
    Associates, Inc.，2021年，第8780–8794页。'
- en: '[135] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving
    Language Understanding by Generative Pre-Training.”'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] A. Radford, K. Narasimhan, T. Salimans, 和 I. Sutskever，“通过生成预训练提升语言理解”。'
- en: '[136] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted Backdoor Attacks
    on Deep Learning Systems Using Data Poisoning,” Dec. 2017.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] X. Chen, C. Liu, B. Li, K. Lu, 和 D. Song，“利用数据中毒对深度学习系统进行定向后门攻击”，2017年12月。'
- en: '[137] A. Sinitsin, V. Plokhotnyuk, D. Pyrkin, S. Popov, and A. Babenko, “Editable
    neural networks,” *arXiv preprint arXiv:2004.00345*, 2020.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] A. Sinitsin, V. Plokhotnyuk, D. Pyrkin, S. Popov, 和 A. Babenko，“可编辑神经网络”，*arXiv预印本
    arXiv:2004.00345*，2020年。'
- en: '[138] E. Mitchell, C. Lin, A. Bosselut, C. D. Manning, and C. Finn, “Memory-based
    model editing at scale,” in *International Conference on Machine Learning*.   PMLR,
    2022, pp. 15 817–15 831.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] E. Mitchell, C. Lin, A. Bosselut, C. D. Manning, 和 C. Finn，“大规模基于记忆的模型编辑”，发表于*国际机器学习会议*。PMLR，2022年，第15,817–15,831页。'
- en: '[139] A. Gupta, S. Baskaran, and G. Anumanchipalli. Rebuilding rome: Resolving
    model collapse during sequential model editing. [Online]. Available: http://arxiv.org/abs/2403.07175'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] A. Gupta, S. Baskaran, 和 G. Anumanchipalli，“重建罗马：解决序列模型编辑中的模型崩溃”，[在线]。可用链接：http://arxiv.org/abs/2403.07175'
- en: '[140] P. Hase, M. Bansal, B. Kim, and A. Ghandeharioun, “Does localization
    inform editing? surprising differences in causality-based localization vs. knowledge
    editing in language models,” *Advances in Neural Information Processing Systems*,
    vol. 36, 2024.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] P. Hase, M. Bansal, B. Kim, 和 A. Ghandeharioun，“定位是否影响编辑？基于因果性的定位与语言模型中的知识编辑之间的惊人差异”，*神经信息处理系统进展*，第36卷，2024年。'
- en: '[141] Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang.
    Editing large language models: Problems, methods, and opportunities. [Online].
    Available: http://arxiv.org/abs/2305.13172'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, 和 N. Zhang，“编辑大型语言模型：问题、方法和机会”，[在线]。可用链接：http://arxiv.org/abs/2305.13172'
