- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:09:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 20:09:00'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1705.03645] A Survey of Deep Learning Methods for Relation Extraction'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1705.03645] 深度学习方法综述：关系抽取'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1705.03645](https://ar5iv.labs.arxiv.org/html/1705.03645)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1705.03645](https://ar5iv.labs.arxiv.org/html/1705.03645)
- en: A Survey of Deep Learning Methods for Relation Extraction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习方法综述：关系抽取
- en: Shantanu Kumar
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Shantanu Kumar
- en: Indian Institute of Technology Delhi
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 印度德里理工学院
- en: ee1130798@iitd.ac.in
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ee1130798@iitd.ac.in
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Abstract
- en: Relation Extraction is an important subtask of Information Extraction which
    has the potential of employing deep learning (DL) models with the creation of
    large datasets using distant supervision. In this review, we compare the contributions
    and pitfalls of the various DL models that have been used for the task, to help
    guide the path ahead.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 关系抽取是信息抽取的重要子任务，有可能利用深度学习（DL）模型通过远程监督创建大型数据集。在本综述中，我们比较了用于该任务的各种DL模型的贡献和缺陷，以指导未来的发展道路。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 Introduction
- en: Information Extraction (IE) is a task of natural language processing that involves
    extracting structured information, that can be interpreted easily by a machine
    or a program, from plain unstructured text. Since the Internet is filled with
    huge amounts of data in the form of text, IE systems are extremely important.
    They can extract meaningful facts from this text, which can then be used for applications
    like search and QA. Knowledge-bases like Freebase (Bollacker et al., [2008](#bib.bib2))
    and DBpedia (Auer et al., [2007](#bib.bib1)) which are a source for useful information
    are far from complete and can be extended using such systems. Information Extraction
    itself is a huge task consisting of several subtasks like named-entity-recognition,
    relation extraction, event extraction etc. In this review, we specifically focus
    on deep learning methods used for the subtask of relation extraction.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 信息抽取（IE）是自然语言处理的任务，涉及从纯文本中提取结构化信息，这些信息可以被机器或程序轻松解释。由于互联网上充斥着大量文本数据，IE系统非常重要。它们可以从这些文本中提取有意义的事实，然后用于搜索和问答等应用。像Freebase（Bollacker等人，[2008](#bib.bib2)）和DBpedia（Auer等人，[2007](#bib.bib1)）这样的知识库是有用信息的来源，但远未完善，可以使用这些系统进行扩展。信息抽取本身是一个包含多个子任务的巨大任务，如命名实体识别、关系抽取、事件抽取等。在本综述中，我们特别关注用于关系抽取子任务的深度学习方法。
- en: IE can be done in unsupervised or semi-supervised domain, in the form of OpenIE,
    where we do not have any predefined ontology or relation classes and we extract
    facts from the data along with the relation phrases. In the supervised domain,
    the relation extraction and classification tasks specifically refers to the classification
    of an entity pair to a set of known relations, using documents containing mentions
    of the entity pair. The RE task refers to predicting whether a given document
    contains a relation or not for the pair, modeled as a binary classification. Relation
    classification refers to predicting which relation class out of a given ontology
    does that document point to, given that it does contain a relation (modeled as
    a multi-class classification problem). The two tasks can be combined by making
    a multi-class classification problem with an extra NoRelation class.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 信息抽取（IE）可以在无监督或半监督领域中进行，以OpenIE的形式进行，其中我们没有任何预定义的本体论或关系类别，我们从数据中提取事实以及关系短语。在监督领域中，关系抽取和分类任务特指将实体对分类到一组已知关系中，使用包含实体对提及的文档。关系抽取任务是指预测给定文档是否包含一个关系对（建模为二元分类）。关系分类是指预测文档指向给定本体中的哪个关系类别，假设文档确实包含一个关系（建模为多类分类问题）。这两个任务可以通过将多类分类问题与额外的NoRelation类结合来进行。
- en: Traditional non deep learning methods for relation extraction typically work
    in the supervised paradigm. They can be divided into two classes which are feature
    based methods and kernel based methods. In both these methods, the extracted features
    and elaborately-designed kernels use pre-existing NLP systems which result in
    errors of the various modules accumulating downstream. Also, the manually constructed
    features may not capture all the relevant information that is required. This need
    to manual engineer features is removed by moving into the domain of deep learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的非深度学习方法通常在监督范式中进行关系抽取。它们可以分为两类：基于特征的方法和基于核方法。在这两种方法中，提取的特征和精心设计的核函数使用预先存在的自然语言处理系统，这导致各模块的错误在下游累积。此外，手动构建的特征可能无法捕捉到所需的所有相关信息。进入深度学习领域后，这种需要手动工程特征的需求被消除。
- en: Supervised techniques for machine learning require large amount of training
    data for learning. Using hand annotated datasets for relation extraction takes
    huge time and effort to make the datasets. Mintz et al. ([2009](#bib.bib11)) proposed
    a distant supervision method for producing large amount of training data by aligning
    KB facts with texts. Such large datasets allow for learning more complex models
    for the task like convolutional neural networks. The noise present in datasets
    generated through distant supervision also require special ways of modeling the
    problem like Multi-Instance Learning as discussed in the subsequent sections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的监督技术需要大量的训练数据来学习。使用手工标注的数据集进行关系抽取需要大量时间和精力来制作这些数据集。Mintz等人（[2009](#bib.bib11)）提出了一种远程监督方法，通过将知识库事实与文本对齐来生成大量的训练数据。这样大规模的数据集允许学习更复杂的模型，例如卷积神经网络。通过远程监督生成的数据集中存在的噪音也需要特殊的建模方式来解决问题，如后续章节中讨论的多实例学习。
- en: 2 Datasets
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数据集
- en: 2.1 Supervised Training
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 监督训练
- en: The early works on relation extraction using deep learning employed supervised
    training datasets that were previously used by non deep learning models. These
    datasets required intensive human annotation which meant that the data contained
    high quality tuples with little to no noise. But human annotation can be time-consuming,
    as a result of which these datasets were generally small. Both of the datasets
    mentioned below contain data samples in which the document sentence is already
    labeled with named entities of interest and the relation class expressed between
    the entity pair is to be predicted.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习进行关系抽取的早期作品采用了之前非深度学习模型使用过的监督训练数据集。这些数据集需要进行大量人工标注，这意味着数据中包含了高质量的元组，几乎没有噪音。但是人工标注可能非常耗时，因此这些数据集通常较小。下面提到的两个数据集都包含数据样本，其中文档句子已经标记了感兴趣的命名实体，并且需要预测实体对之间表达的关系类别。
- en: ACE 2005 dataset
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ACE 2005数据集
- en: The Automatic Content Extraction dataset contains 599 documents related to news
    and email and contains relations that are divided into 7 major types. Out of these,
    6 major relation types contain enough instances (average of 700 instances per
    relation type) and are used for training and testing.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自动内容提取数据集包含599篇与新闻和电子邮件相关的文档，并包含分为7种主要类型的关系。其中6种主要关系类型包含足够数量的实例（每种关系类型平均700个实例），并用于训练和测试。
- en: SemEval-2010 Task 8 dataset
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: SemEval-2010任务8数据集
- en: This dataset is a freely available dataset by Hendrickx et al. ([2009](#bib.bib5))
    which contains 10,717 samples which are divided as 8,000 for training and 2,717
    for testing. It contains 9 relation types which are ordered relations. The directionality
    of the relations effectively doubles the number of relations, since an entity
    pair is believed to be correctly labeled only if the order is also correct. The
    final dataset thus has 19 relation classes (2 $\times$ 9 + 1 for Other class).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由Hendrickx等人（[2009](#bib.bib5)）提供的免费数据集，包含10,717个样本，其中8,000个用于训练，2,717个用于测试。它包含9种关系类型，这些是有序关系。由于关系的方向性，关系的数量有效地翻倍，因为只有当实体对的顺序也正确时，才认为标记正确。因此，最终数据集具有19种关系类别（2
    $\times$ 9 + 1表示其他类别）。
- en: 2.2 Distant Supervision
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 远程监督
- en: To avoid the laborious task of manually building datasets for relation extraction,
    Mintz et al. ([2009](#bib.bib11)) proposed a distant supervision approach for
    automatically generating large amounts of training data. They aligned documents
    with known KBs, using the assumption that if a relation exists between an entity
    pair in the KB, then every document containing the mention of the entity pair
    would express that relation. It can easily be realised that this distant supervision
    assumption is a very strong assumption and that every document containing the
    entity pair mention may not express the relation between the pair. Eg. For the
    tuple (Microsoft, Founder_of, Microsoft) in the database and the document “Bill
    Gates’s turn to philanthropy was linked to the antitrust problems Microsoft had
    in the U.S. and the European union”, the document does not express the relation
    Founder_of even though it contains both the entities.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免手动构建关系提取数据集的繁琐任务，Mintz等人（[2009](#bib.bib11)）提出了一种远程监督方法，用于自动生成大量训练数据。他们将文档与已知的知识库对齐，假设如果知识库中存在实体对之间的关系，那么每个包含该实体对提及的文档都会表达该关系。很容易意识到，这种远程监督假设是一个非常强的假设，并且每个包含实体对提及的文档可能并不表达实体对之间的关系。例如，对于数据库中的元组（Microsoft,
    Founder_of, Microsoft）和文档“比尔·盖茨转向慈善事业与微软在美国和欧盟的反垄断问题有关”，该文档尽管包含了两个实体，但并没有表达Founder_of关系。
- en: To alleviate this problem and reduce the noise, Riedel et al. ([2010](#bib.bib14))
    relaxed the distant supervision assumption by modeling the problem as a multi-instance
    learning problem (described in the subsequent section). The dataset they used
    is the most popular dataset used in subsequent works building on distant supervision
    for relation extraction. This dataset was formed by aligning Freebase relations
    with the New York Times corpus (NYT). Entity mentions were found in the documents
    using the Stanford named entity tagger, and are further matched to the names of
    Freebase entities. There are 53 possible relation classes including a special
    relation NA which indicates there is no relation between the entity pair. The
    training data contains 522,611 sentences, 281,270 entity pairs and 18,252 relational
    facts. The testing set contains 172,448 sentences, 96,678 entity pairs and 1,950
    relational facts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这个问题并减少噪声，Riedel等人（[2010](#bib.bib14)）通过将问题建模为多实例学习问题来放宽远程监督假设（在随后的部分中描述）。他们使用的数据集是后续基于远程监督进行关系提取工作的最流行数据集。该数据集是通过将Freebase关系与纽约时报语料库（NYT）对齐形成的。在文档中使用斯坦福命名实体标注器找到实体提及，并进一步与Freebase实体名称匹配。共有53种可能的关系类别，包括一个特殊的NA关系，表示实体对之间没有关系。训练数据包含522,611个句子、281,270个实体对和18,252个关系事实。测试集包含172,448个句子、96,678个实体对和1,950个关系事实。
- en: The evaluation for this dataset is usually done by comparing the extracted facts
    against the entries in Freebase. However, since Freebase is not a complete KB,
    the evaluation scheme is affected by false negatives that undermine the performance
    of the models. For a comparative study, however, the evaluation scheme works alright.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集的评估通常是通过将提取的事实与Freebase中的条目进行比较。然而，由于Freebase不是一个完整的知识库，评估方案受到假阴性影响，从而削弱了模型的性能。然而，对于比较研究来说，评估方案还是有效的。
- en: 3 Basic Concepts
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基本概念
- en: The following section talks about some basic concepts that are common across
    most deep learning models for relation extraction.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节讨论一些在大多数关系提取深度学习模型中常见的基本概念。
- en: 3.1 Word Embeddings
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 词嵌入
- en: Word embeddings (Mikolov et al., [2013](#bib.bib10); Pennington et al., [2014](#bib.bib13))
    are a form of distributional representations for the words in a vocabulary, where
    each word is expressed as a vector in a low dimensional space (low w.r.t to the
    size of the vocabulary). Word embeddings aim to capture the syntactic and semantic
    information about the word. They are learnt using unsupervised methods over large
    unlabeled text corpora. They are implemented using an embedding matrix $E\in\mathbb{R}^{|V|\times
    d_{w}}$, where $d_{w}$ is the dimensionality of the embedding space and $|V|$
    is the size of the vocabulary.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入（Mikolov等人，[2013](#bib.bib10)；Pennington等人，[2014](#bib.bib13)）是一种分布式表示形式，其中每个单词在一个低维空间中表示为一个向量（相对于词汇表的大小而言较低维）。词嵌入旨在捕捉单词的句法和语义信息。它们使用无监督方法在大规模未标记文本语料库上学习。它们使用嵌入矩阵$E\in\mathbb{R}^{|V|\times
    d_{w}}$实现，其中$d_{w}$是嵌入空间的维度，$|V|$是词汇表的大小。
- en: 3.2 Positional Embeddings
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 位置信息嵌入
- en: In the relation extraction task, along with word embeddings, the input to the
    model also usually encodes the relative distance of each word from the entities
    in the sentence, with the help of positional embeddings (as introduced by Zeng
    et al. ([2014](#bib.bib20))). This helps the network to keep track of how close
    each word is to each entity. The idea is that words closer to the target entities
    usually contain more useful information regarding the relation class. The positional
    embeddings comprise of the relative distance of the current word from the entities.
    For example, in the sentence “Bill_Gates is the founder of Microsoft.”, the relative
    distance of the word “founder” to head entity “Bill_Gates” is 3 and tail entity
    “Microsoft” is -2\. The distance are then encoded in a $d_{p}$ dimensional embedding.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在关系抽取任务中，除了词嵌入外，模型的输入通常还编码每个词与句子中实体的相对距离，借助位置嵌入（如 Zeng 等人 ([2014](#bib.bib20))
    介绍的）。这有助于网络跟踪每个词与每个实体的距离。其思想是，距离目标实体较近的词通常包含更多有关关系类别的有用信息。位置嵌入包括当前词与实体的相对距离。例如，在句子
    “Bill_Gates is the founder of Microsoft.” 中，词 “founder” 对主实体 “Bill_Gates” 的相对距离为
    3，对尾实体 “Microsoft” 的相对距离为 -2。然后这些距离被编码在一个 $d_{p}$ 维的嵌入中。
- en: Finally, the overall sentence $x$ can expressed as a sequence of vectors $x=\{w_{1},w_{2},...,w_{m}\}$
    where every word $w_{i}\in\mathbb{R}^{d}$ and $d=d_{w}+2\times d_{p}$.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，整体句子 $x$ 可以表示为向量序列 $x=\{w_{1},w_{2},...,w_{m}\}$，其中每个词 $w_{i}\in\mathbb{R}^{d}$，且
    $d=d_{w}+2\times d_{p}$。
- en: 3.3 Convolutional Neural Networks
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 卷积神经网络
- en: For encoding the sentences further, deep learning models for relation extraction
    usually use convolutional neural network layers to capture n-gram level features,
    similar to Collobert et al. ([2011](#bib.bib3)). The convolutional layer operates
    as follows. Given an input sentences $x$ as a sequence of vectors $x=\{w_{1},w_{2},...,w_{m}\},w_{i}\in\mathbb{R}^{d}$,
    if $l$ is the window size for the convolutional layer kernel, then the vector
    for the $i$-th window ($q_{i}\in\mathbb{R}^{(d\times l)}$) is formed by concatenating
    the input vectors for that window,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步编码句子，关系抽取的深度学习模型通常使用卷积神经网络层来捕捉 n-gram 级别的特征，这与 Collobert 等人 ([2011](#bib.bib3))
    的方法类似。卷积层的操作如下。给定输入句子 $x$ 为一系列向量 $x=\{w_{1},w_{2},...,w_{m}\},w_{i}\in\mathbb{R}^{d}$，如果
    $l$ 是卷积层内核的窗口大小，则第 $i$ 个窗口的向量 ($q_{i}\in\mathbb{R}^{(d\times l)}$) 通过连接该窗口的输入向量来形成，
- en: '|  | $q_{i}=w_{i:i+l-1};(1\leq i\leq m-l+1)$ |  | (1) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $q_{i}=w_{i:i+l-1};(1\leq i\leq m-l+1)$ |  | (1) |'
- en: A single convolutional kernel would then consist of a weight vector $W\in\mathbb{R}^{(d\times
    l)}$ and a bias $b\in\mathbb{R}$, and the output for the $i$-th window is computed
    as,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 单个卷积核将由一个权重向量 $W\in\mathbb{R}^{(d\times l)}$ 和一个偏置 $b\in\mathbb{R}$ 组成，第 $i$
    个窗口的输出计算为，
- en: '|  | $p_{i}=f(W^{\prime}q_{i}+b)$ |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{i}=f(W^{\prime}q_{i}+b)$ |  | (2) |'
- en: where $f$ is the activation function. Hence the output of the convolutional
    kernel $p$ would be of the shape $p\in\mathbb{R}^{(m-l+1)}$. A convolutional layer
    can consist of $d_{c}$ convolutional kernels which would make the output of the
    convolutional layer of the shape $\mathbb{R}^{d_{c}\times(m-l+1)}$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f$ 是激活函数。因此，卷积核 $p$ 的输出形状为 $p\in\mathbb{R}^{(m-l+1)}$。一个卷积层可以包含 $d_{c}$
    个卷积核，这样卷积层的输出形状将为 $\mathbb{R}^{d_{c}\times(m-l+1)}$。
- en: '![Refer to caption](img/21cfb59a665d4c2df17451e8cb3e692f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/21cfb59a665d4c2df17451e8cb3e692f.png)'
- en: 'Figure 1: Encoder structure with Word and Positional Embeddings followed by
    Convolutional Layer. (Sourced from (Lin et al., [2016](#bib.bib8)))'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：具有词嵌入和位置嵌入的编码器结构，后跟卷积层。（来源于 (Lin 等人, [2016](#bib.bib8)))
- en: '| Model |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |'
- en: '&#124; Multi-instance &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多实例 &#124;'
- en: '&#124; Learning &#124;'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习 &#124;'
- en: '|'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Word &#124;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 词汇 &#124;'
- en: '&#124; Embeddings &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 嵌入 &#124;'
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Positional &#124;'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 位置 &#124;'
- en: '&#124; Embeddings &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 嵌入 &#124;'
- en: '|'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Additional &#124;'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 额外 &#124;'
- en: '&#124; Lexical &#124;'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 词汇 &#124;'
- en: '&#124; Features &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征 &#124;'
- en: '| Max Pooling |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 最大池化 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Liu et al. ([2013](#bib.bib9)) | No | Random | No | Yes | No |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 ([2013](#bib.bib9)) | 否 | 随机 | 否 | 是 | 否 |'
- en: '| Zeng et al. ([2014](#bib.bib20)) | No | Pretrained |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Zeng 等人 ([2014](#bib.bib20)) | 否 | 预训练 |'
- en: '&#124; Yes &#124;'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '&#124; (Not Trained) &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （未训练） &#124;'
- en: '| Yes | Yes |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 是 |'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Nguyen and &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Nguyen 和 &#124;'
- en: '&#124; Grishman (2015) &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Grishman (2015) &#124;'
- en: '| No | Word2Vec | Yes | No | Yes |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 否 | Word2Vec | 是 | 否 | 是 |'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PCNN &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PCNN &#124;'
- en: '&#124; Zeng et al. ([2015](#bib.bib19)) &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Zeng 等 ([2015](#bib.bib19)) &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Yes &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '&#124; (1 sentence &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (1 句子 &#124;'
- en: '&#124; per bag) &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 每个包) &#124;'
- en: '| Word2Vec | Yes | No |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Word2Vec | 是 | 否 |'
- en: '&#124; Yes &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '&#124; (Piecewise in &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (分段在 &#124;'
- en: '&#124; a sentence) &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一个句子) &#124;'
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PCNN + Att &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PCNN + Att &#124;'
- en: '&#124; Lin et al. ([2016](#bib.bib8)) &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Lin 等 ([2016](#bib.bib8)) &#124;'
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Yes &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '&#124; (Attention weighted &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (注意力加权 &#124;'
- en: '&#124; sum over bag) &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在包上求和) &#124;'
- en: '| Word2Vec | Yes | No |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Word2Vec | 是 | 否 |'
- en: '&#124; Yes &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '&#124; (Piecewise &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (分段 &#124;'
- en: '&#124; and Full) &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和完整) &#124;'
- en: '|'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MIMLCNN &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MIMLCNN &#124;'
- en: '&#124; Jiang et al. ([2016](#bib.bib7)) &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Jiang 等 ([2016](#bib.bib7)) &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Yes &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '&#124; (Max of each &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (每个 &#124;'
- en: '&#124; feature over bag) &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征的最大值) &#124;'
- en: '| Word2Vec | Yes | No |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Word2Vec | 是 | 否 |'
- en: '&#124; Yes &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '&#124; (Cross sentence &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (跨句子 &#124;'
- en: '&#124; in a bag) &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在包中) &#124;'
- en: '|'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 1: Summary of features used in the various models for relation extraction
    using CNNs'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：使用 CNN 进行关系抽取的各种模型中使用的特征总结
- en: 4 Supervised learning with CNNs
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 使用 CNN 的监督学习
- en: The early works using deep learning for relation extraction worked in the supervised
    training paradigm with the hand-annotated corpus mentioned previously. These model
    tried to assigned a relation class label to each sentence containing a mention
    of the entity pair in focus, by modeling the problem as a multi-class classification
    problem.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 早期使用深度学习进行关系抽取的工作采用了之前提到的手工标注的语料库进行监督训练。这些模型尝试将关系类别标签分配给每个包含关注的实体对提及的句子，通过将问题建模为多类分类问题。
- en: 4.1 Simple CNN model  (Liu et al., [2013](#bib.bib9))
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 简单的 CNN 模型  (Liu 等，[2013](#bib.bib9))
- en: This work is perhaps the earliest work that tries to use a CNN to automatically
    learn features instead of hand-craft features. It builds an end-to-end network
    which first encodes the input sentence using word vectors and lexical features,
    which is followed by a convolutional kernel layer, a single layer neural network
    and a softmax output layer to give a probability distribution over all the relation
    classes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作可能是最早尝试使用 CNN 自动学习特征而不是手工制作特征的研究。它构建了一个端到端的网络，首先使用词向量和词汇特征对输入句子进行编码，然后通过卷积核层、单层神经网络和
    softmax 输出层来提供所有关系类别的概率分布。
- en: The model uses synonym vectors instead of word vectors, by assigning a single
    vector to each synonym class rather than giving every individual word a vector.
    However, it fails to exploit the real representational power of word embeddings.
    The embeddings are not trained in an unsupervised fashion on the corpus, but randomly
    assigned to each synonym class. Further, the model also tries to incorporate some
    lexical features using word lists, POS lists and entity type lists. It is found
    that this model outperforms the state-of-the-art kernel-based model at the time
    on the ACE 2005 dataset by 9 points of F-score. There were several improvements
    that could be made in this model, but as primary step it worked as a proof of
    concept that deep learning models could perform as good or even better than the
    rigorously engineered feature-based or kernel-based models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用同义词向量而不是词向量，通过为每个同义词类别分配一个向量，而不是为每个单独的词分配一个向量。然而，它未能利用词嵌入的真正表示能力。这些嵌入没有在语料库上以无监督的方式进行训练，而是随机分配给每个同义词类别。此外，该模型还尝试使用词汇表、词性标签列表和实体类型列表来结合一些词汇特征。研究发现，该模型在
    ACE 2005 数据集上比当时最先进的基于核的方法提高了 9 个 F 分数点。虽然该模型还有多个改进之处，但作为初步步骤，它证明了深度学习模型可以表现得与精心设计的基于特征或基于核的模型一样好，甚至更好。
- en: 4.2 CNN model with max-pooling  (Zeng et al., [2014](#bib.bib20))
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 带有最大池化的 CNN 模型  (Zeng 等，[2014](#bib.bib20))
- en: Similar to the previous model, this paper used a CNN for encoding the sentence
    level features. But unlike the previous paper, they used word embeddings that
    were pre-trained on a large unlabeled corpus. The paper was also the first work
    that used Positional Embeddings described in the earlier section, which were adapted
    as standard in all subsequent RE deep learning models. This model also used lexical
    level features like information about the nouns in the sentence and the WordNet
    hypernyms of the nouns.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的模型类似，本文使用CNN对句子级特征进行编码。但与之前的论文不同，他们使用了在大规模未标注语料库上预训练的词嵌入。本文也是首次使用前面描述的**位置嵌入**的工作，这些嵌入被标准化并应用于所有后续的关系提取深度学习模型。该模型还使用了词汇级特征，如句子中的名词信息以及名词的WordNet上位词。
- en: One important contribution of this model was the use of a max-pooling layer
    over the output of the convolutional network. The output of the convolutional
    layer $Z\in\mathbb{R}^{d_{c}\times(m-l+1)}$ is dependent on the size of the input
    sentence $m$. To make this output independent of $m$ and to capture most useful
    feature in each dimension of the feature vector across the entire sentence, it
    was motivated to use a max operation that would collapse $Z$ to $Z^{\prime}\in\mathbb{R}^{d_{c}}$.
    Hence, the the dimension of $Z^{\prime}$ is no longer related to the sentence
    length $m$. The model was shown to outperform SVM and MaxEnt based models that
    used a variety of lexical features. Their ablation study also showed that the
    Positonal Embeddings gave almost a 9% improvement in their F-score.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的重要贡献之一是对卷积网络输出应用了最大池化层。卷积层的输出$Z\in\mathbb{R}^{d_{c}\times(m-l+1)}$依赖于输入句子的大小$m$。为了使该输出独立于$m$并捕捉句子中每个维度的最有用特征，采用了最大操作将$Z$压缩为$Z^{\prime}\in\mathbb{R}^{d_{c}}$。因此，$Z^{\prime}$的维度不再与句子长度$m$相关。该模型被证明优于使用各种词汇特征的SVM和MaxEnt基模型。他们的消融研究还表明，位置嵌入在F-score上提高了近9%。
- en: 4.3 CNN with multi-sized window kernels  (Nguyen and Grishman, [2015](#bib.bib12))
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 使用多尺寸窗口核的CNN（Nguyen 和 Grishman，[2015](#bib.bib12)）
- en: This work was one of the last works in supervised domain for relation extraction
    which built upon the works of Liu et al. ([2013](#bib.bib9)) and Zeng et al. ([2014](#bib.bib20)).
    The model completely gets rid of exterior lexical features to enrich the representation
    of the input sentence and lets the CNN learn the required features itself. Their
    architecture is similar to Zeng et al. ([2014](#bib.bib20)) consisting of word
    and positional embeddings followed by convolution and max-pooling. Additionally,
    they also incorporate convolutional kernels of varying window sizes to capture
    wider ranges of n-gram features. By experimenting with different iteration, they
    find that using kernels with 2-3-4-5 window lengths, gives them the best performance.
    The authors also initialize the word embedding matrix using pre-trained word embeddings
    trained with word2vec (Mikolov et al., [2013](#bib.bib10)), which gives them a
    significant boost over random vectors and static-word2vec vectors.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作是监督领域中关系提取的最后几项工作之一，建立在Liu等人（[2013](#bib.bib9)）和Zeng等人（[2014](#bib.bib20)）的工作基础上。该模型完全去除了外部词汇特征，以丰富输入句子的表示，并让CNN自行学习所需特征。他们的架构类似于Zeng等人（[2014](#bib.bib20)），包括词和位置嵌入，然后是卷积和最大池化。此外，他们还结合了不同窗口大小的卷积核，以捕捉更广泛的n-gram特征。通过实验不同的迭代，他们发现使用2-3-4-5窗口长度的卷积核可以获得最佳性能。作者还使用预训练的word2vec（Mikolov等人，[2013](#bib.bib10)）初始化词嵌入矩阵，这比随机向量和静态word2vec向量提供了显著提升。
- en: 5 Multi-instance learning models with distant supervision
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 多实例学习模型与远程监督
- en: As mentioned previously, Riedel et al. ([2010](#bib.bib14)) relaxed the distant
    supervision assumption by modeling the task as a multi-instance learning problem,
    so that they could exploit the large training data created by distant supervision
    while being robust to the noise in the labels. Multi-instance learning is a form
    of supervised learning where a label is given to a bag of instances, rather than
    a single instance. In the context of RE, every entity pair defines a bag and the
    bag consists of all the sentences that contain the mention of the entity pair.
    Instead of giving a relation class label to every sentence, a label is instead
    given to each bag of the relation entity. Riedel et al. ([2010](#bib.bib14)) model
    this using the assumption that ”if a relation exists between an entity pair, then
    at least one document in the bag for the entity pair must reflect that relation”.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，Riedel 等人 ([2010](#bib.bib14)) 通过将任务建模为多实例学习问题来放宽远程监督的假设，从而能够利用远程监督创建的大量训练数据，同时对标签中的噪声具有鲁棒性。多实例学习是一种监督学习形式，其中标签赋予给一组实例的包，而不是单个实例。在关系抽取的背景下，每个实体对定义一个包，包含所有包含实体对提及的句子。不是为每个句子赋予关系类标签，而是为每个关系实体的包赋予标签。Riedel
    等人 ([2010](#bib.bib14)) 使用这一假设建模，即“如果实体对之间存在关系，则包中至少一个文档必须反映该关系”。
- en: 5.1 Piecewise Convolutional Neural Networks  (Zeng et al., [2015](#bib.bib19))
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 分段卷积神经网络 (Zeng 等人，[2015](#bib.bib19))
- en: The PCNNs model uses the multi-instance learning paradigm, with a neural network
    model to build a relation extractor using distant supervision data. The neural
    network architecture is similar to the models by (Zeng et al., [2014](#bib.bib20))
    and (Nguyen and Grishman, [2015](#bib.bib12)) discussed previously, with one important
    contribution of piecewise max-pooling across the sentence. The authors claim that
    the max-pooling layer drastically reduces the size of the hidden layer and is
    also not sufficient to capture the structure between the entities in the sentence.
    This can be avoided by max-pooling in different segments of the sentence instead
    of the entire sentence. It is claimed that every sentence can naturally be divided
    into 3 segments based on the positions of the 2 entities in focus. By doing a
    piecewise max-pool within each of the segments, we get a richer representation
    while still maintaining a vector that is independent of the input sentences length.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: PCNNs 模型采用多实例学习范式，使用神经网络模型基于远程监督数据构建关系提取器。神经网络的架构类似于之前讨论过的 (Zeng 等人，[2014](#bib.bib20))
    和 (Nguyen 和 Grishman，[2015](#bib.bib12)) 的模型，其中一个重要贡献是句子中的分段最大池化。作者声称，最大池化层显著减小了隐藏层的大小，但仍然不足以捕捉句子中实体之间的结构。这可以通过在句子的不同段落中进行最大池化来避免。据称，每个句子根据焦点实体的位置可以自然地分为3个段落。在每个段落内进行分段最大池化，我们可以获得更丰富的表示，同时仍然保持与输入句子长度无关的向量。
- en: One of the drawbacks in this model which is later addressed in future models
    is the way in which the multi-instance problem was set in the loss function. The
    paper defined the loss for training of the model as follows. Given $T$ bags of
    documents with each bag containing $q_{i}$ documents and having the label $y_{i}$,
    $i=1,2..,T$, the neural network gives the probability of extracting relation $r$
    from document $j$ of bag $i$, $d_{i}^{j}$ denoted as,
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的一个缺点是多实例问题在损失函数中的设置方式，这在后续模型中得到了解决。文中定义了模型训练的损失函数如下。给定 $T$ 个文档包，每个包含 $q_{i}$
    个文档并具有标签 $y_{i}$，$i=1,2..,T$，神经网络给出从包 $i$ 的文档 $j$ 中提取关系 $r$ 的概率，$d_{i}^{j}$ 表示为，
- en: '|  | $p(r&#124;d_{i}^{j},\theta);j=1,2,...,q_{i}$ |  | (3) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(r&#124;d_{i}^{j},\theta);j=1,2,...,q_{i}$ |  | (3) |'
- en: where $\theta$ is the weight parameters of the neural network. Then the loss
    is given as,
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $\theta$ 是神经网络的权重参数。然后损失函数定义为，
- en: '|  | $J(\theta)=\sum_{i=1}^{T}\text{log}p(y_{i}&#124;d_{i}^{j^{*}},\theta)$
    |  | (4) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\theta)=\sum_{i=1}^{T}\text{log}p(y_{i}&#124;d_{i}^{j^{*}},\theta)$
    |  | (4) |'
- en: '|  | $j^{*}=\text{arg}\text{max}_{j}p(y_{i}&#124;d_{i}^{j},\theta);j=1,2...,q_{i}$
    |  | (5) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $j^{*}=\text{arg}\text{max}_{j}p(y_{i}&#124;d_{i}^{j},\theta);j=1,2...,q_{i}$
    |  | (5) |'
- en: Thus, since the method assumes that “atleast one document in the bag expresses
    the relation of the entity pair” it uses only the one most-likely document for
    the entity pair during the training and prediction stage. This means that the
    model is neglecting large amounts of useful data and information that is expressed
    by the other sentences in the bag. Even though not all the sentences in the bag
    express the correct relation between the entity pair, using only a single sentence
    is a very hard constraint. This issue is addressed in the subsequent works.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于该方法假设“至少一个包中的文档表达了实体对之间的关系”，在训练和预测阶段仅使用实体对的最有可能的文档。这意味着该模型忽略了大量包中其他句子表达的有用数据和信息。尽管不是所有包中的句子都表达了实体对之间的正确关系，但仅使用一个句子是一个非常严格的限制。这个问题在后续的工作中得到了解决。
- en: The PCNNs model with Multi-instance learning is shown to outperform the traditional
    non deep learning models like the distant-supervision based model by Mintz et al.
    ([2009](#bib.bib11)), the multi instance learning method MultiR proposed by Hoffmann
    et al. ([2011](#bib.bib6)) and the multi-instance multi-label model MIML by Surdeanu
    et al. ([2012](#bib.bib15)), on the dataset by Riedel et al. ([2010](#bib.bib14))
    (Figure 3). The results are further discussed in the later section. Their ablation
    study also shows the advantages of using PCNNs over CNNs and Multi-instance learning
    over traditional learning, which both add incrementally to the model as shown
    in Figure 2.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: PCNNs模型通过多实例学习显示出优于传统非深度学习模型，如Mintz等人（[2009](#bib.bib11)）基于远程监督的模型，Hoffmann等人（[2011](#bib.bib6)）提出的多实例学习方法MultiR，以及Surdeanu等人（[2012](#bib.bib15)）提出的多实例多标签模型MIML，在Riedel等人（[2010](#bib.bib14)）的数据集上（见图3）。这些结果在后文进一步讨论。他们的消融研究还显示出PCNNs相对于CNNs和传统学习的优势，两者都逐步添加到模型中，如图2所示。
- en: '![Refer to caption](img/1a9141871ed5fa29ec0ff262182fdb20.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1a9141871ed5fa29ec0ff262182fdb20.png)'
- en: 'Figure 2: Effect of piecewise max pooling and multi-instance learning. (Sourced
    from (Zeng et al., [2015](#bib.bib19)))'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：分段最大池化和多实例学习的效果。（来源自(Zeng等人，[2015](#bib.bib19)））
- en: 5.2 Selective Attention over Instances  (Lin et al., [2016](#bib.bib8))
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 选择性注意力跨实例（Lin等人，[2016](#bib.bib8)）
- en: To address the shortcoming of the previous model which only used the one most-relevant
    sentence from the bag, Lin et al. ([2016](#bib.bib8)) used an attention mechanism
    over all the instances in the bag for the multi-instance problem. In this model,
    each sentence $d_{i}^{j}$ of bag $i$ is first encoded into a vector representation,
    $r_{i}^{j}$ using a PCNN or a CNN, as defined previously. Then the final vector
    representation for the bag of sentences is found by taking an attention-weighted
    average of all the sentence vectors ($r_{i}^{j},j=1,2...q_{i}$) in the bag. The
    model computes a weight $\alpha_{j}$ for each instance $d_{i}^{j}$ of bag $i$.
    These $\alpha$ values are dynamic in the sense that they are different for each
    bag and depend on the relation type and the document instance. The final feature
    vector for the bag of sentences is given as,
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决之前模型仅使用包中最相关句子的缺点，Lin等人（[2016](#bib.bib8)）针对多实例问题使用了对袋中所有实例的注意力机制。在该模型中，袋中的每个句子$d_{i}^{j}$首先通过PCNN或CNN编码成向量表示$r_{i}^{j}$，如前所述。然后通过对袋中所有句子向量（$r_{i}^{j},
    j=1,2...q_{i}$）进行加权平均来找到袋中句子的最终向量表示。模型为袋中的每个实例$d_{i}^{j}$计算权重$\alpha_{j}$。这些$\alpha$值在每个包中是动态的，取决于关系类型和文档实例。袋中句子的最终特征向量如下定义：
- en: '|  | $r_{i}=\sum_{j=1}^{q_{i}}\alpha_{j}r_{i}^{j}$ |  | (6) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '-   |  | $r_{i}=\sum_{j=1}^{q_{i}}\alpha_{j}r_{i}^{j}$ |  | (6) |'
- en: When the loss is found using this attention weighted representation of all the
    instances in the bag, the model is able to inherently identify the important sentences
    from the noisy ones and all the information in the bag is utilized to make the
    relation class prediction.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用这种关注加权表示袋中所有实例的损失时，模型能够固有地识别重要句子和噪声句子，利用袋中的所有信息来进行关系类别预测。
- en: It can also be observed that the ‘only-one most likely sentence’ model used
    in the PCNN paper is a special case of the selective attention procedure where
    $\alpha_{j^{*}}=1$ for only $j^{*}$ as defined by equation (5) and all the remaining
    $\alpha$ values are zero (hard attention). It is shown that using this selective
    attention procedure significantly improves the precision recall curve of both
    the CNN and the PCNN models. The model is able to predict the correct relations
    with higher confidence as it able to gather evidence over multiple sentences in
    the bag.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以观察到，PCNN 论文中使用的“只有一个最可能的句子”模型是选择性注意程序的特例，其中 $\alpha_{j^{*}}=1$ 仅对 $j^{*}$
    如公式 (5) 定义，所有其他 $\alpha$ 值为零（硬注意）。研究表明，使用这种选择性注意程序显著提高了 CNN 和 PCNN 模型的精确度召回曲线。模型能够以更高的置信度预测正确的关系，因为它能够在包中的多个句子上收集证据。
- en: 5.3 Multi-instance Multi-label CNNs  (Jiang et al., [2016](#bib.bib7))
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 多实例多标签 CNNs (Jiang et al., [2016](#bib.bib7))
- en: The authors of this paper address the information loss problem in Zeng et al.
    ([2015](#bib.bib19)) by using a cross-document max-pooling layer. Like in the
    attention model, they first find a vector representation, $r_{i}^{j}$ for each
    sentence $d_{i}^{j}$ of bag $i$. Then the final vector representation for the
    bag of sentences is found by taking a dimension wise max of the sentence vectors
    ($r_{i}^{j},j=1,2...q_{i}$). The final feature vector for the bag of sentences
    is given as,
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者通过使用跨文档最大池化层解决了 Zeng 等人 ([2015](#bib.bib19)) 的信息丢失问题。与注意力模型类似，他们首先为包 $i$
    的每个句子 $d_{i}^{j}$ 找到一个向量表示 $r_{i}^{j}$。然后通过对句子向量 ($r_{i}^{j},j=1,2...q_{i}$) 进行维度最大值来找到句子包的最终向量表示。句子包的最终特征向量表示为，
- en: '|  | $(r_{i})_{k}=\max_{j=1,2...q_{i}}(r_{i}^{j})_{k};k=1,2...D$ |  | (7) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $(r_{i})_{k}=\max_{j=1,2...q_{i}}(r_{i}^{j})_{k};k=1,2...D$ |  | (7) |'
- en: where $r_{i}^{j},r_{i}\in\mathbb{R}^{D}$. This allows each feature in the final
    feature vector to come from the most prominent document for that feature, rather
    than the entire feature vector coming from the overall most-prominent document.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r_{i}^{j},r_{i}\in\mathbb{R}^{D}$。这使得最终特征向量中的每个特征都来自于该特征最突出的文档，而不是整个特征向量都来自于整体最突出的文档。
- en: The paper also address the issue of multi-label in relation extraction. Up until
    now, all models predicted a single relation class for an entity pair. But it is
    likely that the same entity pair can have multiply relations (called overlapping
    relations) which are supported by different documents. For example, (Steve_Jobs,
    Founded, Apple) and (Steve_Jobs, CEO_of, Apple) are both valid relations between
    the same entity pair (Steve_Jobs, Apple) which may be supported by different sentences.
    The authors modify the architecture to have sigmoid activation functions instead
    of softmax activations in the final layer, which would mean that the network predicts
    a probability for each relation class independently, rather than predicting a
    probability distribution over the relations. The loss for training the model is
    then defined as,
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 论文还讨论了多标签在关系抽取中的问题。到目前为止，所有模型都为实体对预测一个单一的关系类别。但同一实体对可能有多个关系（称为重叠关系），这些关系由不同文档支持。例如，(Steve_Jobs,
    Founded, Apple) 和 (Steve_Jobs, CEO_of, Apple) 都是同一实体对 (Steve_Jobs, Apple) 的有效关系，可能由不同的句子支持。作者将架构修改为在最终层使用
    sigmoid 激活函数，而不是 softmax 激活函数，这意味着网络会独立预测每个关系类别的概率，而不是预测关系的概率分布。训练模型的损失定义为，
- en: '|  | $J(\theta)=\sum_{i=1}^{T}\sum_{r=1}^{R}y_{r}^{i}\text{log}p_{r}^{i}+(1-y_{r}^{i})\text{log}(1-p_{r}^{i})$
    |  | (8) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\theta)=\sum_{i=1}^{T}\sum_{r=1}^{R}y_{r}^{i}\text{log}p_{r}^{i}+(1-y_{r}^{i})\text{log}(1-p_{r}^{i})$
    |  | (8) |'
- en: where $R$ is the number of relation classes, $p_{r}^{i}$ is probability for
    bag $i$ to have relation $r$ as predicted by the network and $y_{r}^{i}$ is a
    binary label if bag $i$ had relation $r$ or not.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R$ 是关系类别的数量，$p_{r}^{i}$ 是网络预测包 $i$ 具有关系 $r$ 的概率，$y_{r}^{i}$ 是一个二元标签，表示包
    $i$ 是否具有关系 $r$。
- en: The MIMLCNN model is able to improve performance of the PCNN and CNN models
    like the selective attention mechanism, as it is able to exploit the information
    across multiple documents in the bag, by using the most prominent document for
    each feature. The results are discuss further in the next section.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: MIMLCNN 模型能够像选择性注意机制一样提高 PCNN 和 CNN 模型的性能，因为它能够利用包中多个文档的信息，通过对每个特征使用最突出的文档。结果将在下一节进一步讨论。
- en: 6 Results
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结果
- en: Figure 3 summarizes the results of the various multi-instance learning models
    applied on the distant supervision dataset created by Riedel et al. ([2010](#bib.bib14)).
    It shows the results for 3 non deep learning models namely Mintz (Mintz et al.,
    [2009](#bib.bib11)), MultiR (Hoffmann et al., [2011](#bib.bib6)) and MIML (Surdeanu
    et al., [2012](#bib.bib15)). We also see the performance of the deep learning
    models discussed in the previous sections.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图3总结了应用于Riedel等人（[2010](#bib.bib14)）创建的远程监督数据集的各种多实例学习模型的结果。它展示了3个非深度学习模型的结果，即Mintz（Mintz
    et al., [2009](#bib.bib11)）、MultiR（Hoffmann et al., [2011](#bib.bib6)）和MIML（Surdeanu
    et al., [2012](#bib.bib15)）。我们还可以看到前述部分讨论的深度学习模型的表现。
- en: It is observed that the all the deep learning models perform significantly better
    than the non deep learning models. Using the Multi-instance Multi-label (MIMLCNN)
    mechanism with the CNN model further improves the curve over the PCNN model. However,
    the selective attention mechanism applied over the PCNN model gives the best performance
    out of all the models. It is interesting to note the increase in performance in
    the PCNN curve to the PCNN+Att curve as compared to the MIMLCNN curve. Since the
    attention mechanism is a soft-selection mechanism, it works out to be more robust
    and able to exploit information across the sentences more effectively, than even
    the cross-document max mechanism used in MIMLCNN.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到所有深度学习模型的表现明显优于非深度学习模型。使用多实例多标签（MIMLCNN）机制的CNN模型进一步改善了PCNN模型的曲线。然而，应用于PCNN模型的选择性注意机制在所有模型中表现最佳。值得注意的是，相较于MIMLCNN曲线，PCNN曲线到PCNN+Att曲线的性能提升是有趣的。由于注意机制是一种软选择机制，它比MIMLCNN中使用的跨文档最大机制更具鲁棒性，能够更有效地利用句子之间的信息。
- en: '![Refer to caption](img/cdff2566174c15fd672225abd9dd0549.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cdff2566174c15fd672225abd9dd0549.png)'
- en: 'Figure 3: Results for Multi-instance learning models. (Sourced from (Lin et al.,
    [2016](#bib.bib8)) and (Jiang et al., [2016](#bib.bib7)))'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：多实例学习模型的结果。（来源于（Lin et al., [2016](#bib.bib8)）和（Jiang et al., [2016](#bib.bib7)））
- en: 7 Concluding Remarks
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: With the introduction of distant supervision for relation extraction by Mintz
    et al. ([2009](#bib.bib11)), modeling the task as Multi-instance problem has been
    widely adopted. Using this mechanism also provides enough data for deep learning
    models to be trained in the multi-instance setting which accommodates for the
    labeling noise in the data. Successive works have tried to handle the noise and
    distant supervision assumption with mechanisms like selective attention over document
    instances and cross-document max pooling, which have shown to increase performance.
    Some very recent works in the field also try to exploit the interaction between
    the relations by exploiting relation paths (Zeng et al., [2016](#bib.bib21)) and
    relation class ties (Ye et al., [2016](#bib.bib17)) to improve the performance
    further. For example relations like Father_of and Mother_of can be exploited to
    extract instance for Spouse_of.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Mintz等人（[2009](#bib.bib11)）引入的远程监督用于关系抽取，将任务建模为多实例问题已被广泛采用。使用这种机制还提供了足够的数据以在多实例设置中训练深度学习模型，这可以容纳数据中的标签噪声。后续的工作尝试通过选择性注意机制和跨文档最大池化等机制来处理噪声和远程监督假设，这些机制已显示出提升性能的效果。该领域一些非常新的工作也尝试通过利用关系路径（Zeng
    et al., [2016](#bib.bib21)）和关系类联系（Ye et al., [2016](#bib.bib17)）来进一步提升性能。例如，可以利用Father_of和Mother_of等关系来提取Spouse_of的实例。
- en: However these improvements only work on the training and inference methods of
    the model. As far as the deep learning aspect is concerned, the CNN or PCNN architecture
    used to encode the sentences is same across all these works. It is surprising
    to note that no work for the task of relation extraction has tried to use Recurrent
    Neural Networks (RNNs) for encoding the sentences in place of the CNNs (to the
    best of our knowledge). RNNs and LSTMs intuitively fit more naturally to natural
    language tasks. Even though NLP literature does not support a clear distinction
    between the domains where CNNs or RNNs perform better, recent works have shown
    that each provide complementary information for text classification tasks (Yin
    et al., [2017](#bib.bib18)). Where RNNs perform well on document-level sentiment
    classification (Tang et al., [2015](#bib.bib16)), some works have shown CNNs to
    outperform LSTMs on language modeling tasks (Dauphin et al., [2016](#bib.bib4)).
    Future works for relation extraction can thus definitely try to experiment with
    using LSTMs for encoding sentence and relations.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些改进仅适用于模型的训练和推断方法。就深度学习方面而言，用于编码句子的CNN或PCNN架构在所有这些工作中都是相同的。令人惊讶的是，据我们所知，没有任何关系抽取任务的工作尝试使用递归神经网络（RNNs）来替代CNNs进行句子编码。RNNs和LSTMs直观上更适合自然语言任务。尽管NLP文献并未明确区分CNNs和RNNs在哪些领域表现更好，但近期的研究表明，每种方法在文本分类任务中提供了互补的信息（Yin
    et al., [2017](#bib.bib18)）。RNNs在文档级情感分类中表现良好（Tang et al., [2015](#bib.bib16)），而一些研究则表明CNNs在语言建模任务中优于LSTMs（Dauphin
    et al., [2016](#bib.bib4)）。因此，未来的关系抽取工作可以尝试使用LSTMs进行句子和关系的编码实验。
- en: References
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Auer et al. (2007) Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann,
    Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open
    data. In The semantic web, Springer, pages 722–735.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Auer et al. (2007) Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann,
    Richard Cyganiak, and Zachary Ives. 2007. Dbpedia：开放数据网络的核心。发表于《语义网》，Springer，第722–735页。
- en: 'Bollacker et al. (2008) Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
    Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database
    for structuring human knowledge. In SIGMOD ’08. AcM, pages 1247–1250.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bollacker et al. (2008) Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge,
    and Jamie Taylor. 2008. Freebase：一个协作创建的图数据库，用于结构化人类知识。发表于SIGMOD ’08。AcM，第1247–1250页。
- en: Collobert et al. (2011) Ronan Collobert, Jason Weston, Léon Bottou, Michael
    Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing
    (almost) from scratch. Journal of Machine Learning Research 12(Aug):2493–2537.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collobert et al. (2011) Ronan Collobert, Jason Weston, Léon Bottou, Michael
    Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. 从头开始的自然语言处理。机器学习研究杂志 12（8月）：2493–2537。
- en: Dauphin et al. (2016) Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier.
    2016. Language modeling with gated convolutional networks. arXiv preprint arXiv:1612.08083
    .
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dauphin et al. (2016) Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier.
    2016. 使用门控卷积网络的语言建模。arXiv预印本arXiv:1612.08083。
- en: 'Hendrickx et al. (2009) Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
    Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano,
    and Stan Szpakowicz. 2009. Semeval-2010 task 8: Multi-way classification of semantic
    relations between pairs of nominals. In Proceedings of the Workshop on Semantic
    Evaluations. ACL, pages 94–99.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrickx et al. (2009) Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
    Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano,
    and Stan Szpakowicz. 2009. Semeval-2010任务8：名词对之间语义关系的多重分类。发表于《语义评估研讨会论文集》。ACL，第94–99页。
- en: 'Hoffmann et al. (2011) Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer,
    and Daniel S Weld. 2011. Knowledge-based weak supervision for information extraction
    of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association
    for Computational Linguistics: Human Language Technologies-Volume 1. ACL, pages
    541–550.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann et al. (2011) Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer,
    and Daniel S Weld. 2011. 基于知识的弱监督用于重叠关系的信息抽取。发表于《第49届计算语言学协会年会：人类语言技术-第1卷论文集》。ACL，第541–550页。
- en: 'Jiang et al. (2016) Xiaotian Jiang, Quan Wang, Peng Li, and Bin Wang. 2016.
    Relation extraction with multi-instance multi-label convolutional neural networks.
    In Proceedings of COLING 2016, the 26th International Conference on Computational
    Linguistics: Technical Papers. pages 1471–1480.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2016) Xiaotian Jiang, Quan Wang, Peng Li, and Bin Wang. 2016.
    使用多实例多标签卷积神经网络进行关系抽取。发表于COLING 2016，第26届国际计算语言学会议：技术论文集，第1471–1480页。
- en: Lin et al. (2016) Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong
    Sun. 2016. Neural relation extraction with selective attention over instances.
    In Proceedings of ACL. volume 1, pages 2124–2133.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人（2016）Yankai Lin，Shiqi Shen，Zhiyuan Liu，Huanbo Luan和Maosong Sun。2016年。具有实例选择性注意力的神经关系抽取。在ACL会议论文集上的卷1，页面2124–2133。
- en: Liu et al. (2013) ChunYang Liu, WenBo Sun, WenHan Chao, and Wanxiang Che. 2013.
    Convolution neural network for relation extraction. In International Conference
    on Advanced Data Mining and Applications. Springer, pages 231–242.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2013）ChunYang Liu，WenBo Sun，WenHan Chao和Wanxiang Che。2013年。用于关系抽取的卷积神经网络。在国际高级数据挖掘与应用会议上。Springer，页面231–242。
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013. Distributed representations of words and phrases and their
    compositionality. In Advances in neural information processing systems. pages
    3111–3119.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等人（2013）Tomas Mikolov，Ilya Sutskever，Kai Chen，Greg S Corrado和Jeff Dean。2013年。词和短语的分布式表示及其组合性。在神经信息处理系统的进展中。页面3111–3119。
- en: 'Mintz et al. (2009) Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
    2009. Distant supervision for relation extraction without labeled data. In Proceedings
    of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
    Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume
    2. ACL, pages 1003–1011.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mintz 等人（2009）Mike Mintz，Steven Bills，Rion Snow和Dan Jurafsky。2009年。远程监督关系抽取而不使用标记数据。在ACL年会和AFNLP国际联合会议的论文集上。ACL，页面1003–1011。
- en: 'Nguyen and Grishman (2015) Thien Huu Nguyen and Ralph Grishman. 2015. Relation
    extraction: Perspective from convolutional neural networks. In Proceedings of
    NAACL-HLT. pages 39–48.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen和Grishman（2015）Thien Huu Nguyen和Ralph Grishman。2015年。关系抽取：来自卷积神经网络的视角。在NAACL-HLT的会议论文集上。页面39–48。
- en: 'Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher D.
    Manning. 2014. [Glove: Global vectors for word representation](http://www.aclweb.org/anthology/D14-1162).
    In Empirical Methods in Natural Language Processing (EMNLP). pages 1532–1543.
    [http://www.aclweb.org/anthology/D14-1162](http://www.aclweb.org/anthology/D14-1162).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pennington 等人（2014）Jeffrey Pennington，Richard Socher和Christopher D. Manning。2014年。[Glove：全局词向量表示](http://www.aclweb.org/anthology/D14-1162)。在自然语言处理的经验方法（EMNLP）上。页面1532–1543。[http://www.aclweb.org/anthology/D14-1162](http://www.aclweb.org/anthology/D14-1162)。
- en: Riedel et al. (2010) Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010.
    Modeling relations and their mentions without labeled text. In Joint European
    Conference on Machine Learning and Knowledge Discovery in Databases. Springer,
    pages 148–163.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riedel 等人（2010）Sebastian Riedel，Limin Yao和Andrew McCallum。2010年。在没有标记文本的情况下建模关系及其提及。在联合欧洲机器学习和数据库知识发现会议上。Springer，页面148–163。
- en: Surdeanu et al. (2012) Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
    Christopher D Manning. 2012. Multi-instance multi-label learning for relation
    extraction. In Proceedings of the 2012 joint conference on empirical methods in
    natural language processing and computational natural language learning. ACL,
    pages 455–465.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Surdeanu 等人（2012）Mihai Surdeanu，Julie Tibshirani，Ramesh Nallapati和Christopher
    D Manning。2012年。关系抽取的多实例多标签学习。在2012年联合会议上的ACL，页面455–465。
- en: Tang et al. (2015) Duyu Tang, Bing Qin, and Ting Liu. 2015. Document modeling
    with gated recurrent neural network for sentiment classification. In EMNLP. pages
    1422–1432.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人（2015）Duyu Tang，Bing Qin和Ting Liu。2015年。用门控循环神经网络进行情感分类的文档建模。在EMNLP上。页面1422–1432。
- en: Ye et al. (2016) Hai Ye, Wenhan Chao, and Zhunchen Luo. 2016. Jointly extracting
    relations with class ties via effective deep ranking. arXiv preprint arXiv:1612.07602
    .
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等人（2016）Hai Ye，Wenhan Chao和Zhunchen Luo。2016年。通过有效的深度排名共同提取具有类关系的关系。arXiv预印本arXiv:1612.07602。
- en: Yin et al. (2017) Wenpeng Yin, Katharina Kann, Mo Yu, and Hinrich Schütze. 2017.
    Comparative study of cnn and rnn for natural language processing. arXiv preprint
    arXiv:1702.01923 .
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等人（2017）Wenpeng Yin，Katharina Kann，Mo Yu和Hinrich Schütze。2017年。卷积神经网络和循环神经网络在自然语言处理中的比较研究。arXiv预印本arXiv:1702.01923。
- en: Zeng et al. (2015) Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant
    supervision for relation extraction via piecewise convolutional neural networks.
    In EMNLP. pages 1753–1762.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等人（2015）Daojian Zeng，Kang Liu，Yubo Chen和Jun Zhao。2015年。通过分段卷积神经网络进行远程监督关系抽取。在EMNLP上。页面1753–1762。
- en: Zeng et al. (2014) Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, Jun Zhao,
    et al. 2014. Relation classification via convolutional deep neural network. In
    COLING. pages 2335–2344.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等人（2014）Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, Jun Zhao 等人。2014年。通过卷积深度神经网络进行关系分类。发表在
    COLING 会议上。第 2335–2344 页。
- en: Zeng et al. (2016) Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2016.
    Incorporating relation paths in neural relation extraction. arXiv preprint arXiv:1609.07479
    .
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等人（2016）Wenyuan Zeng, Yankai Lin, Zhiyuan Liu 和 Maosong Sun。2016年。将关系路径融入神经关系提取。arXiv
    预印本 arXiv:1609.07479。
