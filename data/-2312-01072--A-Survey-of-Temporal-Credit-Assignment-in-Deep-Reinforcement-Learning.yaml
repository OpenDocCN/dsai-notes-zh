- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:35:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:35:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2312.01072] A Survey of Temporal Credit Assignment in Deep Reinforcement Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2312.01072] 深度强化学习中的时间信用分配调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.01072](https://ar5iv.labs.arxiv.org/html/2312.01072)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.01072](https://ar5iv.labs.arxiv.org/html/2312.01072)
- en: A Survey of Temporal Credit Assignment
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间信用分配调查
- en: in Deep Reinforcement Learning
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度强化学习中
- en: \nameEduardo Pignatelli \emaile.pignatelli@ucl.ac.uk
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \nameEduardo Pignatelli \emaile.pignatelli@ucl.ac.uk
- en: \addrUniversity College London \AND\nameJohan Ferret \emailjferret@google.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \addrUniversity College London \AND\nameJohan Ferret \emailjferret@google.com
- en: \addrGoogle DeepMind \AND\nameMatthieu Geist \emailmfgeist@google.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \addrGoogle DeepMind \AND\nameMatthieu Geist \emailmfgeist@google.com
- en: \addrGoogle DeepMind \AND\nameThomas Mesnard \emailmesnard@google.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \addrGoogle DeepMind \AND\nameThomas Mesnard \emailmesnard@google.com
- en: \addrGoogle DeepMind \AND\nameHado van Hasselt \emailhado@google.com
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \addrGoogle DeepMind \AND\nameHado van Hasselt \emailhado@google.com
- en: \addrGoogle DeepMind \AND\nameLaura Toni \emaill.toni@ucl.ac.uk
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \addrGoogle DeepMind \AND\nameLaura Toni \emaill.toni@ucl.ac.uk
- en: \addrUniversity College London
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \addrUniversity College London
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The Credit Assignment Problem (CAP) refers to the longstanding challenge of
    RL agents to associate actions with their long-term consequences. Solving the
    CAP is a crucial step towards the successful deployment of RL in the real world
    since most decision problems provide feedback that is noisy, delayed, and with
    little or no information about the causes. These conditions make it hard to distinguish
    serendipitous outcomes from those caused by informed decision-making. However,
    the mathematical nature of credit and the CAP remains poorly understood and defined.
    In this survey, we review the state of the art of Temporal Credit Assignment (CA)
    in deep RL. We propose a unifying formalism for credit that enables equitable
    comparisons of state of the art algorithms and improves our understanding of the
    trade-offs between the various methods. We cast the CAP as the problem of learning
    the influence of an action over an outcome from a finite amount of experience.
    We discuss the challenges posed by delayed effects, transpositions, and a lack
    of action influence, and analyse how existing methods aim to address them. Finally,
    we survey the protocols to evaluate a credit assignment method, and suggest ways
    to diagnoses the sources of struggle for different credit assignment methods.
    Overall, this survey provides an overview of the field for new-entry practitioners
    and researchers, it offers a coherent perspective for scholars looking to expedite
    the starting stages of a new study on the CAP, and it suggests potential directions
    for future research.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 信用分配问题（CAP）指的是强化学习（RL）代理将动作与其长期后果关联起来的长期挑战。解决CAP是成功在现实世界中部署RL的关键步骤，因为大多数决策问题的反馈是噪声大、延迟且几乎没有关于原因的信息。这些条件使得很难区分偶然结果和由有信息的决策导致的结果。然而，信用和CAP的数学性质仍然理解和定义不清。在本调查中，我们回顾了深度RL中时间信用分配（CA）的最新进展。我们提出了一种统一的信用形式，使得能够对先进算法进行公平比较，并改善我们对各种方法之间权衡的理解。我们将CAP视为从有限经验中学习一个动作对结果的影响的问题。我们讨论了延迟效应、转置以及缺乏动作影响带来的挑战，并分析了现有方法如何解决这些问题。最后，我们调查了评估信用分配方法的协议，并建议了诊断不同信用分配方法困难来源的方法。总体而言，本调查为新入行的从业者和研究人员提供了领域概述，为希望加快新研究起步阶段的学者提供了连贯的视角，并建议了未来研究的潜在方向。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'RL is poised to impact many real world problems that require sequential decision
    making, such as strategy (Silver et al.,, [2016](#bib.bib165), [2018](#bib.bib166);
    Schrittwieser et al.,, [2020](#bib.bib158); Anthony et al.,, [2020](#bib.bib6);
    Vinyals et al.,, [2019](#bib.bib198); Perolat et al.,, [2022](#bib.bib133)) and
    arcade video games (Mnih et al.,, [2013](#bib.bib117), [2015](#bib.bib118); Badia
    et al.,, [2020](#bib.bib13); Wurman et al.,, [2022](#bib.bib209)), climate control
    (Wang and Hong,, [2020](#bib.bib203)), energy management (Gao,, [2014](#bib.bib53)),
    car driving (Filos et al.,, [2020](#bib.bib48)) and stratospheric balloon navigation (Bellemare
    et al.,, [2020](#bib.bib24)), designing circuits (Mirhoseini et al.,, [2020](#bib.bib115)),
    cybersecurity (Nguyen and Reddi,, [2021](#bib.bib123)), robotics (Kormushev et al.,,
    [2013](#bib.bib93)), or physics (Degrave et al.,, [2022](#bib.bib38)). One fundamental
    mechanism allowing RL agents to succeed in these scenarios is their ability to
    evaluate the *influence* of their actions over outcomes – e.g., a win, a loss,
    a particular event, a payoff. Often, these outcomes are consequences of isolated
    decisions taken in a very remote past: actions can have long-term effects. The
    problem of learning to associate actions with distant, future outcomes is known
    as the temporal Credit Assignment Problem (CAP): to distribute the credit of success
    among the multitude of decisions involved (Minsky,, [1961](#bib.bib114)). Overall,
    the *influence* that an action has on an outcome represents *knowledge* in the
    form of *associations* between actions and outcomes (Sutton et al.,, [2011](#bib.bib183);
    Zhang et al.,, [2020](#bib.bib216)). These associations constitute the scaffolding
    that agencies can use to deduce, reason, improve and act to address decision-making
    problems and ultimately improve on their data efficiency.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）有望影响许多需要顺序决策的现实世界问题，如策略（Silver et al., [2016](#bib.bib165), [2018](#bib.bib166);
    Schrittwieser et al., [2020](#bib.bib158); Anthony et al., [2020](#bib.bib6);
    Vinyals et al., [2019](#bib.bib198); Perolat et al., [2022](#bib.bib133)）和街机视频游戏（Mnih
    et al., [2013](#bib.bib117), [2015](#bib.bib118); Badia et al., [2020](#bib.bib13);
    Wurman et al., [2022](#bib.bib209)），气候控制（Wang and Hong, [2020](#bib.bib203)），能源管理（Gao,
    [2014](#bib.bib53)），汽车驾驶（Filos et al., [2020](#bib.bib48)）和平流层气球导航（Bellemare et
    al., [2020](#bib.bib24)），电路设计（Mirhoseini et al., [2020](#bib.bib115)），网络安全（Nguyen
    and Reddi, [2021](#bib.bib123)），机器人技术（Kormushev et al., [2013](#bib.bib93)），或物理学（Degrave
    et al., [2022](#bib.bib38)）。使强化学习代理在这些场景中成功的一个基本机制是它们评估其行为对结果的*影响*的能力——例如，胜利、失败、特定事件、回报。通常，这些结果是很久以前做出的孤立决策的后果：行动可能具有长期效果。学习将行动与遥远的未来结果相关联的问题称为时间信用分配问题（CAP）：在涉及的众多决策中分配成功的信用（Minsky,
    [1961](#bib.bib114)）。总体而言，一个行动对结果的*影响*代表了以*关联*形式存在的*知识*（Sutton et al., [2011](#bib.bib183);
    Zhang et al., [2020](#bib.bib216)）。这些关联构成了代理可以用来推断、推理、改进和采取行动以解决决策问题并最终提高数据效率的支撑结构。
- en: 'Solving the CAP is paramount since most decision problems have two important
    characteristics: they take a *long time to complete*, and they seldom provide
    immediate feedback, but often *with delay* and little insight as to which actions
    caused it. These conditions produce environments where the feedback signal is
    weak, noisy, or deceiving, and the ability to separate serendipitous outcomes
    from those caused by informed decision-making becomes a hard challenge. Furthermore,
    as these environments grow in complexity with the aim to scale to real-world tasks
    (Rahmandad et al.,, [2009](#bib.bib145); Luoma et al.,, [2017](#bib.bib107)),
    the actions taken by an agent affect an increasingly vanishing part of the outcome.
    In these conditions, it becomes challenging to learn value functions that accurately
    represent the *influence* of an action, and to be able to distinguish and order
    the relative long-term values of different actions. In fact, canonical Deep Reinforcement
    Learning (Deep RL) solutions to *control* are often brittle to the hyperparameter
    choice (Henderson et al.,, [2018](#bib.bib69)), inelastic to generalise zero-shot
    to different tasks (Kirk et al.,, [2023](#bib.bib89)), prone to overfitting (Behzadan
    and Hsu,, [2019](#bib.bib23); Wang et al.,, [2022](#bib.bib201)), and sample-inefficient
    (Ye et al.,, [2021](#bib.bib212); Kapturowski et al.,, [2023](#bib.bib86)). Overall,
    building a solid foundation of knowledge that can unlock solutions to complex
    problems beyond those already solved calls for better CA techniques (Mesnard et al.,,
    [2021](#bib.bib112)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 解决CAP至关重要，因为大多数决策问题有两个重要特点：它们*完成的时间很长*，而且通常不会提供即时反馈，往往*延迟*并且很少提供哪些行动导致反馈的见解。这些条件产生了反馈信号弱、噪声大或具有欺骗性的环境，使得将偶然结果与由知情决策造成的结果区分开来成为一项艰巨的挑战。此外，随着这些环境复杂性的增长，旨在扩展到实际任务（Rahmandad
    等，[2009](#bib.bib145)；Luoma 等，[2017](#bib.bib107)），代理采取的行动影响着日益减少的结果部分。在这些条件下，学习能够准确表示*影响*的价值函数，并区分和排序不同行动的相对长期价值变得具有挑战性。实际上，经典的深度强化学习（Deep
    RL）*控制*解决方案通常对超参数选择非常脆弱（Henderson 等，[2018](#bib.bib69)），在不同任务的零样本泛化上表现不佳（Kirk
    等，[2023](#bib.bib89)），容易过拟合（Behzadan 和 Hsu，[2019](#bib.bib23)；Wang 等，[2022](#bib.bib201)），并且样本效率低（Ye
    等，[2021](#bib.bib212)；Kapturowski 等，[2023](#bib.bib86)）。总体而言，建立能够解决复杂问题的知识基础，超越当前已解决的问题，呼唤更好的CA技术（Mesnard
    等，[2021](#bib.bib112)）。
- en: In the current state of RL, action values are a key proxy for *action influence*.
    Values actualise a return by synthesising statistics of the *future* into properties
    of the *present*. Recently, the advent of Deep RL (Arulkumaran et al.,, [2017](#bib.bib9))
    granted access to new avenues to express credit through values, either by using
    memory (Goyal et al.,, [2019](#bib.bib56); Hung et al.,, [2019](#bib.bib75)),
    associative memory (Hung et al.,, [2019](#bib.bib75); [Ferret et al., 2021a,](#bib.bib46)
    ; Raposo et al.,, [2021](#bib.bib146)), counterfactuals (Mesnard et al.,, [2021](#bib.bib112)),
    planning (Edwards et al.,, [2018](#bib.bib40); Goyal et al.,, [2019](#bib.bib56);
    van Hasselt et al.,, [2021](#bib.bib190)) or by meta-learning (Xu et al.,, [2018](#bib.bib211);
    Houthooft et al.,, [2018](#bib.bib72); Oh et al.,, [2020](#bib.bib126); Xu et al.,,
    [2020](#bib.bib210); Zahavy et al.,, [2020](#bib.bib214)). The research on CAP
    is now fervent, and with a rapidly growing corpus of works.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的强化学习（RL）状态下，行动值是*行动影响*的重要代理。值通过将*未来*的统计数据合成为*现在*的属性，从而实现回报。最近，深度强化学习（Arulkumaran
    等，[2017](#bib.bib9)）的出现提供了通过值表达信用的新途径，无论是通过使用记忆（Goyal 等，[2019](#bib.bib56)；Hung
    等，[2019](#bib.bib75)），联想记忆（Hung 等，[2019](#bib.bib75)；[Ferret et al., 2021a,](#bib.bib46)；Raposo
    等，[2021](#bib.bib146)），反事实（Mesnard 等，[2021](#bib.bib112)），规划（Edwards 等，[2018](#bib.bib40)；Goyal
    等，[2019](#bib.bib56)；van Hasselt 等，[2021](#bib.bib190)）或通过元学习（Xu 等，[2018](#bib.bib211)；Houthooft
    等，[2018](#bib.bib72)；Oh 等，[2020](#bib.bib126)；Xu 等，[2020](#bib.bib210)；Zahavy
    等，[2020](#bib.bib214)）。对CAP的研究现在非常热烈，并且相关工作迅速增长。
- en: Motivation.
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动机。
- en: 'Despite its central role, there is little discussion on the precise mathematical
    nature of credit. While these proxies are sufficient to unlock solutions to complex
    tasks, it remains unclear where to draw the line between a generic measure of
    action influence and *credit*. Existing works focus on partial aspects or sub-problems
    (Hung et al.,, [2019](#bib.bib75); Arjona-Medina et al.,, [2019](#bib.bib7); Arumugam
    et al.,, [2021](#bib.bib10)) and not all works refer to the CAP explicitly in
    their text (Andrychowicz et al.,, [2017](#bib.bib5); Nota et al.,, [2021](#bib.bib124);
    Goyal et al.,, [2019](#bib.bib56)), despite their findings providing relevant
    contributions to address the problem. The resulting literature is fragmented and
    lacks a space to connect recent works and put their efforts in perspective for
    the future. The field still holds open questions:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其中心作用很大，但对于credit的确切数学性质的讨论很少。虽然这些代理能够解锁复杂任务的解决方案，但到底在一般行动影响的测度和*credit*之间划定何种界限，这仍不清楚。现有作品侧重于部分方面或子问题（Hung
    et al.,，[2019](#bib.bib75)；Arjona-Medina et al.,，[2019](#bib.bib7)；Arumugam et al.,，[2021](#bib.bib10)），并非所有作品在文中显式提及CAP（Andrychowicz
    et al.,，[2017](#bib.bib5)；Nota et al.,，[2021](#bib.bib124)；Goyal et al.,，[2019](#bib.bib56)），尽管他们的发现对于解决问题提供了相关的贡献。由此得到的文献是零碎的，而缺乏一个空间来连接最近的作品并将他们的努力放在未来的视角之中。该领域仍有待解决的问题：
- en: Q1.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q1.
- en: What is the *credit* of an action? How is it different from an action value?
    And what is the CAP? What in words, and what in mathematics?
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个行动的*credit*是什么？它与行动价值有什么不同？CAP是什么？用言语和数学分别怎么表述？
- en: Q2.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q2.
- en: How do agents learn to *assign* credit? What are the main methods in the literature
    and how can they be organised?
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理如何学习*分配*credit？文献中主要的方法有哪些，如何进行组织？
- en: Q3.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q3.
- en: How can we *evaluate* whether a method is improving on a challenge? How can
    we monitor advancements?
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何*评估*一种方法是否在挑战中有所改进？我们如何监测进展？
- en: Goals.
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 目标。
- en: Here, we propose potential answers to these questions and set out to realign
    the fundamental issue raised by Minsky, ([1961](#bib.bib114)) to the Deep RL framework.
    Our main goal is to provide an overview of the field to new-entry practitioners
    and researchers, and, for scholars looking to develop the field further, to put
    the heterogeneous set of works into a comprehensive, coherent perspective. Lastly,
    we aim to reconnect works whose findings are relevant for CAP, but that do not
    refer to it directly. To the best of our knowledge, the work by Ferret, ([2022](#bib.bib45),
    Chapter 4) is the only effort in this direction, and the literature offers no
    explicit surveys on the temporal CA problem in Deep RL.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提出了对这些问题的潜在答案，并着手重新调整明斯基提出的基本问题（[1961](#bib.bib114)）至深度强化学习框架中。我们的主要目标是为新进入从业者和研究人员提供该领域的概述，并为希望进一步发展该领域的学者，将各种作品放入一个全面、连贯的视角中。最后，我们旨在重新连接那些与CAP有关的发现，但并不直接提及它的作品。就我们所知，Ferret的作品（[2022](#bib.bib45)，第四章）是朝这个方向的唯一努力，而文献中并没有明确针对深度强化学习中的时间CAP问题的调查。
- en: Scope.
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 范围。
- en: The survey focuses on temporal CA in single-agent Deep RL, and the problems
    of (i) quantifying the influence of an action mathematically and formalising a
    mathematical objective for the CA problem (ii) defining its challenges, and categorising
    the existing methods to learn the quantities above, (iii) defining a suitable
    evaluation protocol to monitor the advancement of the field. We do not discuss
    *structural* CA in Deep Neural Networks (DNNs), that is, the problem of assigning
    credit or blame to individual parameters of a DNN (Schmidhuber,, [2015](#bib.bib156);
    Balduzzi et al.,, [2015](#bib.bib16)). We also do not discuss CA in multi-agent
    RL, that is, to ascertain which agents are responsible for creating good reinforcement
    signals (Chang et al.,, [2003](#bib.bib30); Foerster et al.,, [2018](#bib.bib51)).
    When credit (assignment) is used without any preceding adjective, we always refer
    to *temporal* credit (assignment). In particular, with the adjective temporal
    we refer to fact that “each ultimate success is associated with a vast number
    of internal decisions”(Minsky,, [1961](#bib.bib114)) and that these decisions,
    together with states and rewards, are arranged to form a temporal sequence.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该调查专注于单一智能体深度强化学习中的时间信用分配，主要问题包括：（i）数学量化行动的影响并形式化时间信用分配问题的数学目标，（ii）定义其挑战，分类现有的方法以学习上述量，（iii）定义适当的评估协议以监控领域的发展。我们不讨论深度神经网络（DNNs）中的*结构性*信用分配，即将信用或责备分配给DNN的各个参数（Schmidhuber,,
    [2015](#bib.bib156); Balduzzi et al.,, [2015](#bib.bib16)）。我们也不讨论多智能体强化学习中的信用分配，即确定哪些智能体负责生成良好的强化信号（Chang
    et al.,, [2003](#bib.bib30); Foerster et al.,, [2018](#bib.bib51)）。当使用“信用（分配）”而没有任何前缀形容词时，我们总是指*时间性*信用（分配）。特别是，带有时间性形容词我们指的是“每一个*终极*成功都与大量的内部决策相关联”（Minsky,,
    [1961](#bib.bib114)），这些决策与状态和奖励一起排列形成一个时间序列。
- en: The survey focuses on Deep RL. In surveying existing formalisms and methods,
    we only look at the Deep RL literature, and when proposing new ones, we tailor
    them to Deep RL theories and applications. We exclude from the review methods
    specifically designed to solve decision problems with linear or tabular RL, as
    they do not bode well for scaling to complex problems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 该调查专注于深度强化学习。在调查现有的形式化方法时，我们仅查看深度强化学习的文献，并在提出新方法时将其调整为深度强化学习的理论和应用。我们排除了专门设计用于解决线性或表格强化学习决策问题的方法，因为它们不适合扩展到复杂问题。
- en: Outline.
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 大纲。
- en: 'We address [Q1.](#S1.I1.i1 "item Q1\. ‣ Motivation. ‣ 1 Introduction ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning"), [Q2.](#S1.I1.i2
    "item Q2\. ‣ Motivation. ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") and [Q3.](#S1.I1.i3 "item Q3\. ‣ Motivation.
    ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") in the three major sections of the manuscript. Respectively:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在手稿的三个主要部分中讨论了[Q1.](#S1.I1.i1 "item Q1\. ‣ Motivation. ‣ 1 Introduction ‣
    A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")，[Q2.](#S1.I1.i2
    "item Q2\. ‣ Motivation. ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")和[Q3.](#S1.I1.i3 "item Q3\. ‣ Motivation. ‣ 1
    Introduction ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")。具体：
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Section [4](#S4 "4 Quantifying action influences ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning") addresses [Q1.](#S1.I1.i1 "item Q1\.
    ‣ Motivation. ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment in Deep
    Reinforcement Learning"), proposing a definition of credit and the CAP and providing
    a survey of action influence measures.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[4节](#S4 "4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")讨论了[Q1.](#S1.I1.i1 "item Q1\. ‣ Motivation. ‣
    1 Introduction ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")，提出了信用和CAP的定义，并提供了行动影响度量的调查。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Section [5](#S5 "5 The challenges to assign credit in Deep RL ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning") and Section [6](#S6
    "6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") address [Q2.](#S1.I1.i2 "item Q2\. ‣ Motivation.
    ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"), respectively discussing the key challenges to solving the CAP and
    the existing methods to assign credit.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[5节](#S5 "5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")和第[6节](#S6 "6 Methods to assign
    credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")分别讨论了[Q2.](#S1.I1.i2 "item Q2\. ‣ Motivation. ‣ 1 Introduction ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning")，即解决CAP的关键挑战和现有的信用分配方法。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Section [7](#S7 "7 Evaluating credit ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") answers [Q3.](#S1.I1.i3 "item Q3\. ‣ Motivation.
    ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"), reviewing the problem setup, the metrics and the evaluation protocols
    to monitor advancements in the field.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[7](#S7 "7 Evaluating credit ‣ A Survey of Temporal Credit Assignment in Deep
    Reinforcement Learning")节回答了[Q3.](#S1.I1.i3 "item Q3\. ‣ Motivation. ‣ 1 Introduction
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")，回顾了问题设置、指标和评估协议，以监测该领域的进展。
- en: 'For each question, we contribute by: (a) systematising *existing works* into
    a simpler, coherent space; (b) discussing it, and (c) synthesising our perspective
    into a unifying formalism. Table [1](#S1.T1 "Table 1 ‣ Outline. ‣ 1 Introduction
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning") outlines
    the suggested reading flow according to the type of reader.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个问题，我们的贡献包括：(a) 将*现有工作*系统化为一个更简单、一致的空间；(b) 进行讨论，和 (c) 将我们的观点综合成一个统一的形式。表[1](#S1.T1
    "Table 1 ‣ Outline. ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")概述了根据读者类型的建议阅读流程。
- en: '| Reader type | Suggested Flow |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 读者类型 | 建议流程 |'
- en: '| --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Specialised CA scholar | [1](#S1 "1 Introduction ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning") $\rightarrow$ [2](#S2 "2 Related Work
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning") $\rightarrow$
    [4](#S4 "4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") $\rightarrow$ [5](#S5 "5 The challenges to assign
    credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") $\rightarrow$ [6](#S6 "6 Methods to assign credit in Deep RL ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning") $\rightarrow$ [7](#S7
    "7 Evaluating credit ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 专门从事CA的学者 | [1](#S1 "1 Introduction ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") $\rightarrow$ [2](#S2 "2 Related Work ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning") $\rightarrow$ [4](#S4
    "4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in Deep
    Reinforcement Learning") $\rightarrow$ [5](#S5 "5 The challenges to assign credit
    in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")
    $\rightarrow$ [6](#S6 "6 Methods to assign credit in Deep RL ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning") $\rightarrow$ [7](#S7 "7 Evaluating
    credit ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")
    |'
- en: '| --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| RL researcher | [1](#S1 "1 Introduction ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") $\rightarrow$ [4](#S4 "4 Quantifying action influences
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning") $\rightarrow$
    [5](#S5 "5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning") $\rightarrow$ [6](#S6 "6 Methods to
    assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") $\rightarrow$ [7](#S7 "7 Evaluating credit ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning") |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 强化学习研究人员 | [1](#S1 "1 Introduction ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") $\rightarrow$ [4](#S4 "4 Quantifying action influences
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning") $\rightarrow$
    [5](#S5 "5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning") $\rightarrow$ [6](#S6 "6 Methods to
    assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") $\rightarrow$ [7](#S7 "7 Evaluating credit ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning") |'
- en: '| --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Deep Learning researcher | [1](#S1 "1 Introduction ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning") $\rightarrow$ [3](#S3 "3 Notation
    and Background ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") $\rightarrow$ [4](#S4 "4 Quantifying action influences ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning") $\rightarrow$ [5](#S5
    "5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") $\rightarrow$ [6](#S6 "6 Methods to assign credit
    in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")
    $\rightarrow$ [7](#S7 "7 Evaluating credit ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习研究人员 | [1](#S1 "1 Introduction ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") $\rightarrow$ [3](#S3 "3 Notation and Background
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning") $\rightarrow$
    [4](#S4 "4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") $\rightarrow$ [5](#S5 "5 The challenges to assign
    credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") $\rightarrow$ [6](#S6 "6 Methods to assign credit in Deep RL ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning") $\rightarrow$ [7](#S7
    "7 Evaluating credit ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") |'
- en: '| Practitioner (applied researcher) | [6](#S6 "6 Methods to assign credit in
    Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")
    $\rightarrow$ [4.4](#S4.SS4 "4.4 The credit assignment problem ‣ 4 Quantifying
    action influences ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") $\rightarrow$ [3](#S3 "3 Notation and Background ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning") |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 实践者（应用研究者） | [6](#S6 "6 深度RL中分配信用的方法 ‣ 深度强化学习中时间信用分配的调查") $\rightarrow$ [4.4](#S4.SS4
    "4.4 信用分配问题 ‣ 4 量化行动影响 ‣ 深度强化学习中时间信用分配的调查") $\rightarrow$ [3](#S3 "3 符号和背景 ‣ 深度强化学习中时间信用分配的调查")
    |'
- en: '| Proposing a new CA method | [7](#S7 "7 Evaluating credit ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning") $\rightarrow$ [6](#S6 "6 Methods
    to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") $\rightarrow$ [2](#S2 "2 Related Work ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning") $\rightarrow$ [4](#S4 "4 Quantifying
    action influences ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 提出一种新的CA方法 | [7](#S7 "7 评估信用 ‣ 深度强化学习中时间信用分配的调查") $\rightarrow$ [6](#S6 "6
    深度RL中分配信用的方法 ‣ 深度强化学习中时间信用分配的调查") $\rightarrow$ [2](#S2 "2 相关工作 ‣ 深度强化学习中时间信用分配的调查")
    $\rightarrow$ [4](#S4 "4 量化行动影响 ‣ 深度强化学习中时间信用分配的调查") |'
- en: 'Table 1: Suggested flow of reading by type of reader to support the outline
    in Section [1](#S1.SS0.SSS0.Px4 "Outline. ‣ 1 Introduction ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning"). Numbers represent section
    numbers.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：建议的阅读流程按读者类型来支持第[1](#S1.SS0.SSS0.Px4 "大纲 ‣ 1 引言 ‣ 深度强化学习中时间信用分配的调查")节中的大纲。数字代表章节编号。
- en: 2 Related Work
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Three existing works stand out for proposing a better understanding of the CAP
    explicitly. Ferret, ([2022](#bib.bib45), Chapter 4) designs a conceptual framework
    to unify and study credit assignment methods. The chapter proposes a general formalism
    for a range of credit assignment functions and discusses their characteristics
    and general desiderata. Unlike Ferret, ([2022](#bib.bib45), Chapter 4), we survey
    potential formalisms for a mathematical definition of credit (Section [4](#S4
    "4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in Deep
    Reinforcement Learning")); in view of the new formalism, we propose an alternative
    view of the methods to assign credit (Section [6](#S6 "6 Methods to assign credit
    in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")),
    and an evaluation protocol to measure future advancements in the field. Arumugam
    et al., ([2021](#bib.bib10)) analyses the CAP from an information theoretic perspective.
    The work focuses on the notion of *information sparsity* to clarify the role of
    credit in solving sparse reward problems in RL. Despite the work questioning what
    credit is mathematically, it does not survey existing material, and it does not
    provide a framework that can unify existing approaches to represent credit under
    a single formalism. Harutyunyan et al., ([2019](#bib.bib67)) propose a principled
    method to measure the credit of an action. However, the study does not aim to
    survey existing methods to *measure* credit, the methods to *assign* credit, and
    the methods to evaluate a credit assignment method, and does not aim to organise
    them into a cohesive synthesis.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 三项现有工作突出了对CAP的更好理解。Ferret, ([2022](#bib.bib45), 第4章) 设计了一个概念框架来统一和研究信用分配方法。该章节提出了一种通用形式化方法，涵盖了一系列的信用分配函数，并讨论了它们的特性和一般要求。与Ferret,
    ([2022](#bib.bib45), 第4章)不同，我们调查了信用的数学定义的潜在形式化方法（第[4](#S4 "4 量化行动影响 ‣ 深度强化学习中时间信用分配的调查")节）；基于新的形式化方法，我们提出了一种对信用分配方法的替代观点（第[6](#S6
    "6 深度RL中分配信用的方法 ‣ 深度强化学习中时间信用分配的调查")节），以及一个评估协议来衡量该领域未来的进展。Arumugam等人, ([2021](#bib.bib10))
    从信息理论的角度分析了CAP。该工作关注*信息稀疏性*的概念，以澄清信用在解决RL中的稀疏奖励问题中的作用。尽管该工作质疑了信用的数学定义，但它没有调查现有的材料，也没有提供一个能够统一现有方法的框架来用单一形式化方法表示信用。Harutyunyan等人,
    ([2019](#bib.bib67)) 提出了一个有原则的方法来测量行动的信用。然而，该研究并不旨在调查现有的方法来*测量*信用、分配信用的方法，以及评估信用分配方法的方法，也没有旨在将它们组织成一个连贯的综合体。
- en: The literature offers also surveys on related topics. We discuss them in Appendix [A](#S1a
    "A Further related works ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") to preserve the fluidity of the manuscript.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中还提供了相关主题的综述。为了保持稿件的连贯性，我们在附录[A](#S1a "进一步相关工作 ‣ 深度强化学习中的时间信用分配综述")中讨论了这些内容。
- en: As a result, none of these works position CAP in a single space that enables
    thorough discussion, assessment and critique. Instead, we propose a formalism
    that unifies the existing *quantities* that represent the influence of an action
    (Section [4](#S4 "4 Quantifying action influences ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning")). Based on this, we can analyse the
    advantages and limitations of existing measures of action influence. The resulting
    framework provides a way to gather the variety of existing *methods* that learn
    these quantities from experience (Section [6](#S6 "6 Methods to assign credit
    in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")),
    and to monitor the advancements in solving the CAP.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，这些工作没有将CAP定位于一个能够进行全面讨论、评估和批判的单一空间。相反，我们提出了一种形式化方法，将现有的*量*统一起来，这些量表示行动的影响（第[4](#S4
    "4 量化行动影响 ‣ 深度强化学习中的时间信用分配综述")节）。基于此，我们可以分析现有行动影响测量的优缺点。最终框架提供了一种方式来汇集从经验中学习这些*方法*的各种现有方法（第[6](#S6
    "6 深度强化学习中的信用分配方法 ‣ 深度强化学习中的时间信用分配综述")节），并监测CAP解决方案的进展。
- en: 3 Notation and Background
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 符号和背景
- en: Here we introduce the notation and background that we will follow in the rest
    of the paper.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍了在本文其余部分中将遵循的符号和背景。
- en: Notations.
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 符号。
- en: 'We use calligraphic characters to denote sets and the corresponding lowercases
    to denote their elements, for example, $x\in\mathcal{X}$. For a measurable space
    $(\mathcal{X},\Sigma)$, we denote the set of probability measures over $\mathcal{X}$
    with $\Delta({\mathcal{X}})$. We use an uppercase letter $X$ to indicate a random
    variable, and the notation $\mathbb{P}_{X}$ to denote its distribution over the
    sample set $\mathcal{X}$, for example, $\mathbb{P}_{X}:\mathcal{X}\rightarrow\Delta({\mathcal{X}})$.
    When we mention a random event $X$ (for example, a random action) we refer to
    a random draw of a specific value $x\in\mathcal{X}$ from its distribution $\mathbb{P}_{X}$
    and we write, $X\sim\mathbb{P}_{X}$. When a distribution is clear from the context,
    we omit it from the subscript and write $\mathbb{P}(X)$ instead of $\mathbb{P}_{X}(X)$.
    We use $\mathbbm{1}_{\mathcal{Y}}(x)$ for the indicator function that maps an
    element $x\in\mathcal{X}$ to $1$ if $x\in\mathcal{Y}\subset\mathcal{X}$ and $0$
    otherwise. We use $\mathbb{R}$ to denote the set of real numbers and $\mathbb{B}=\{0,1\}$
    to denote the Boolean domain. We use $\ell_{\infty}(x)=\lVert x\rVert_{\infty}=\sup_{i}|x_{i}|$
    to denote the $\ell$-infinity norm of a vector $x$ of components $x_{i}$. We write
    the Kullback-Leibler divergence between two discrete probability distributions
    $\mathbb{P}_{P}(X)$ and $\mathbb{P}_{Q}(X)$ with sample space $\mathcal{X}$ as:
    $D_{KL}(\mathbb{P}_{P}(X)||\mathbb{P}_{Q}(X))=\sum_{x\in\mathcal{X}}[\mathbb{P}_{P}(x)\log({\mathbb{P}_{P}(x)}/{\mathbb{P}_{Q}(x)})]$.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用花体字母表示集合，使用相应的小写字母表示其元素，例如，$x\in\mathcal{X}$。对于一个可测空间$(\mathcal{X},\Sigma)$，我们用$\Delta({\mathcal{X}})$表示$\mathcal{X}$上的概率测度集合。我们用大写字母$X$表示随机变量，用符号$\mathbb{P}_{X}$表示其在样本集$\mathcal{X}$上的分布，例如，$\mathbb{P}_{X}:\mathcal{X}\rightarrow\Delta({\mathcal{X}})$。当我们提到一个随机事件$X$（例如，一个随机动作）时，我们指的是从其分布$\mathbb{P}_{X}$中随机抽取一个特定值$x\in\mathcal{X}$，我们写作$X\sim\mathbb{P}_{X}$。当上下文中分布已明确时，我们省略下标中的分布，写作$\mathbb{P}(X)$而不是$\mathbb{P}_{X}(X)$。我们使用$\mathbbm{1}_{\mathcal{Y}}(x)$表示指示函数，该函数将元素$x\in\mathcal{X}$映射到$1$（如果$x\in\mathcal{Y}\subset\mathcal{X}$）或$0$（否则）。我们用$\mathbb{R}$表示实数集合，$\mathbb{B}=\{0,1\}$表示布尔域。我们使用$\ell_{\infty}(x)=\lVert
    x\rVert_{\infty}=\sup_{i}|x_{i}|$表示一个向量$x$的$\ell$-无穷范数，其中$x_{i}$为其分量。我们写出两个离散概率分布$\mathbb{P}_{P}(X)$和$\mathbb{P}_{Q}(X)$在样本空间$\mathcal{X}$上的Kullback-Leibler散度为：$D_{KL}(\mathbb{P}_{P}(X)||\mathbb{P}_{Q}(X))=\sum_{x\in\mathcal{X}}[\mathbb{P}_{P}(x)\log({\mathbb{P}_{P}(x)}/{\mathbb{P}_{Q}(x)})]$。
- en: Reinforcement Learning.
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 强化学习。
- en: We consider the problem of learning by interacting with an environment. A program
    (the agent) interacts with an environment by making decisions (actions). The action
    is the agent’s interface with the environment. Before each action, the agent may
    observe part of the environment and take suitable actions. The action changes
    the state of the environment. After each action, the agent may perceive a feedback
    signal (the reward). The goal of the agent is to learn a rule of behaviour (the
    policy) that maximises the expected sum of rewards.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑通过与环境交互来学习的问题。一个程序（代理）通过做出决策（动作）与环境互动。动作是代理与环境的接口。在每个动作之前，代理可以观察环境的一部分并采取适当的行动。动作改变了环境的状态。在每个动作之后，代理可以感知反馈信号（奖励）。代理的目标是学习一种行为规则（策略），以最大化奖励的期望总和。
- en: MDPs.
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MDPs。
- en: 'MDPs formalise decision-making problems. This survey focuses on the most common
    MDP settings for Deep RL. Formally, a discounted MDP (Howard,, [1960](#bib.bib73);
    Puterman,, [2014](#bib.bib140)) is defined by a tuple $\mathcal{M}=\left(\mathcal{S},\mathcal{A},R,\mu,\gamma\right)$.
    $\mathcal{S}$ is a finite set of states (the state space) and $\mathcal{A}$ is
    a finite set of actions (the action space). $R:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{R}$
    is a deterministic, bounded reward function that maps a state-action pair to a
    scalar reward $r\in[r_{min},r_{max}]=\mathcal{R}$. $\gamma\in[0,1]$ is a discount
    factor and $\mu:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})$ is
    a transition kernel, which maps a state-action pair to probabilities over states.
    We refer to an arbitrary state $s\in\mathcal{S}$ with $s$, an action $a\in\mathcal{A}$
    with $a$ and a reward $r\in[r_{min},r_{max}]$ with $r$. Given a state-action tuple
    $(s,a)$, the probability of the next random state $S_{t+1}$ being $s^{\prime}$
    depends on a state-transition distribution: $\mathbb{P}_{\mu}(S_{t+1}=s^{\prime}|S_{t}=s,A_{t}=a)=\mu(s^{\prime}|s,a),\forall
    s,s^{\prime}\in\mathcal{S}$. We refer to $S_{t}$ as the random state at time $t$.
    The probability of the action $a$ depends on the agent’s policy, which is a stationary
    mapping $\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})$, from a state to a probability
    distribution over actions.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MDPs 形式化了决策问题。本调查专注于深度强化学习中最常见的 MDP 设置。正式地，折扣 MDP（Howard，[1960](#bib.bib73);
    Puterman，[2014](#bib.bib140)）由一个元组 $\mathcal{M}=\left(\mathcal{S},\mathcal{A},R,\mu,\gamma\right)$
    定义。$\mathcal{S}$ 是一个有限状态集（状态空间），$\mathcal{A}$ 是一个有限动作集（动作空间）。$R:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{R}$
    是一个确定性、有界的奖励函数，将状态-动作对映射到标量奖励 $r\in[r_{min},r_{max}]=\mathcal{R}$。$\gamma\in[0,1]$
    是一个折扣因子，$\mu:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})$ 是一个转移核，将状态-动作对映射到状态的概率。我们将任意状态
    $s\in\mathcal{S}$ 称为 $s$，将动作 $a\in\mathcal{A}$ 称为 $a$，将奖励 $r\in[r_{min},r_{max}]$
    称为 $r$。给定一个状态-动作元组 $(s,a)$，下一个随机状态 $S_{t+1}$ 为 $s^{\prime}$ 的概率依赖于状态转移分布：$\mathbb{P}_{\mu}(S_{t+1}=s^{\prime}|S_{t}=s,A_{t}=a)=\mu(s^{\prime}|s,a),\forall
    s,s^{\prime}\in\mathcal{S}$。我们将 $S_{t}$ 称为时间 $t$ 的随机状态。动作 $a$ 的概率依赖于代理的策略，策略是一个从状态到动作概率分布的平稳映射
    $\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})$。
- en: These settings give rise to a discrete-time, stateless (Markovian), Random Process
    (RP) with the additional notions of actions to represent decisions and rewards
    for a feedback signal. Given an initial state distribution $\mathbb{P}_{\mu_{0}}(S_{0})$,
    the process begins with a random state $s_{0}\sim\mathbb{P}_{\mu_{0}}$. Starting
    from $s_{0}$, at each time $t$ the agent interacts with the environment by choosing
    an action $A_{t}\sim\mathbb{P}_{\pi}(\cdot|s_{t})$, observing the reward $r_{t}\sim
    R_{t}(S_{t},A_{t})$ and the next state $s_{t+1}\sim\mathbb{P}_{\mu}$. If a state
    $s_{t}$ is also an absorbing state ($s\in\overline{\mathcal{S}}\subset\mathcal{S}$),
    the MDP transitions to the same state $s_{t}$ with probability $1$ and reward
    $0$, and we say that the episode terminates. We refer to the union of each temporal
    transition $(s_{t},a_{t},r_{t},s_{t+1})$ as a trajectory or episode $d=\{s_{t},a_{t},r_{t},:0\leq
    t\leq T\}$, where $T$ is the horizon of the episode.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置产生了一个离散时间、无状态（马尔可夫的）随机过程（RP），并引入了动作来表示决策和奖励作为反馈信号。给定初始状态分布 $\mathbb{P}_{\mu_{0}}(S_{0})$，过程以随机状态
    $s_{0}\sim\mathbb{P}_{\mu_{0}}$ 开始。从 $s_{0}$ 开始，在每个时间 $t$，代理通过选择一个动作 $A_{t}\sim\mathbb{P}_{\pi}(\cdot|s_{t})$
    与环境交互，观察奖励 $r_{t}\sim R_{t}(S_{t},A_{t})$ 和下一个状态 $s_{t+1}\sim\mathbb{P}_{\mu}$。如果状态
    $s_{t}$ 也是一个吸收状态 ($s\in\overline{\mathcal{S}}\subset\mathcal{S}$)，MDP 以概率 $1$
    和奖励 $0$ 转移到相同的状态 $s_{t}$，我们称该情节终止。我们将每个时间转移 $(s_{t},a_{t},r_{t},s_{t+1})$ 的联合称为轨迹或情节
    $d=\{s_{t},a_{t},r_{t},:0\leq t\leq T\}$，其中 $T$ 是情节的时间范围。
- en: We mostly consider episodic settings where the probability of ending in an absorbing
    state in finite time is $1$, resulting in the random horizon $T$. We consider
    discrete action spaces $\mathcal{A}=\{a_{i}:1\leq i\leq n\}$ only.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要考虑 episodic 设置，其中在有限时间内到达吸收状态的概率为$1$，从而导致随机时间跨度$T$。我们仅考虑离散动作空间$\mathcal{A}=\{a_{i}:1\leq
    i\leq n\}$。
- en: A trajectory is also a random variable in the space of all trajectories $\mathcal{D}=(\mathcal{S}\times\mathcal{A}\times\mathcal{R})^{T}$,
    and its distribution is the joint of all of its components $\mathbb{P}_{D}(D)=\mathbb{P}_{A,S,R}(s_{0},a_{1},r_{1},\ldots,s_{T})$.
    Given an MDP $\mathcal{M}=(\mathcal{S},\mathcal{A},R,\mu,\gamma)$ and fixing a
    policy $\pi$ produces a Markov Process (MP) $\mathcal{M}^{\pi}$ and induces a
    distribution over trajectory $\mathbb{P}_{\mu,\pi}(D)$. Therefore, we refer to
    a random trajectory that starts at $k$ and ends at $T$ also as a sequence of random
    decisions $D_{k}=\{X_{k},\ldots,X_{t},\ldots,X_{T-1},S_{T}\}$.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹也是所有轨迹空间$\mathcal{D}=(\mathcal{S}\times\mathcal{A}\times\mathcal{R})^{T}$中的随机变量，其分布是所有组件的联合分布$\mathbb{P}_{D}(D)=\mathbb{P}_{A,S,R}(s_{0},a_{1},r_{1},\ldots,s_{T})$。给定一个MDP
    $\mathcal{M}=(\mathcal{S},\mathcal{A},R,\mu,\gamma)$并固定一个策略$\pi$产生一个马尔可夫过程（MP）$\mathcal{M}^{\pi}$并诱导出轨迹的分布$\mathbb{P}_{\mu,\pi}(D)$。因此，我们也将从$k$开始到$T$结束的随机轨迹称为一系列随机决策$D_{k}=\{X_{k},\ldots,X_{t},\ldots,X_{T-1},S_{T}\}$。
- en: We refer to the return random variable $Z_{t}$, as the sum of discounted rewards
    from time $t$ to the end of the episode, $Z_{t}=\sum_{t=k}^{T}\gamma^{k-t}R(S_{k},A_{k})$.
    The control objective of an RL problem is to find a policy $\pi^{*}$ that maximises
    the expected return,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们把返回的随机变量$Z_{t}$称为从时间$t$到整个 episode 结束的折扣奖励的总和，即$Z_{t}=\sum_{t=k}^{T}\gamma^{k-t}R(S_{k},A_{k})$。强化学习问题的控制目标是找到一个策略$\pi^{*}$，以最大化期望回报，
- en: '|  | $\displaystyle\pi^{*}\in\mathop{\mathrm{argmax}}_{\pi}\mathbb{E}_{\mu,\pi}\left[\sum_{t=0}^{T}\gamma^{t}R(S_{t},A_{t})\right]=\mathbb{E}\left[Z_{0}\right].$
    |  | (1) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\pi^{*}\in\mathop{\mathrm{argmax}}_{\pi}\mathbb{E}_{\mu,\pi}\left[\sum_{t=0}^{T}\gamma^{t}R(S_{t},A_{t})\right]=\mathbb{E}\left[Z_{0}\right].$
    |  | (1) |'
- en: Partially-Observable MDPs (POMDPs).
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 部分可观察马尔可夫决策过程（POMDPs）。
- en: POMDPs are MDPs in which agents do not get to observe a true state of the environment,
    but only a transformation of it, and are specified with an additional tuple $\left\langle\mathcal{O},\mu_{O}\right\rangle$,
    where $\mathcal{O}$ is an observation space, and $\mu_{O}:\mathcal{S}\rightarrow\Delta({\mathcal{O}})$
    is an observation kernel, that maps the true environment state to observation
    probabilities. Because transitioning between observations is not Markovian, policies
    are a mapping from partial *trajectories*, which we denote as histories, to actions.
    Histories are sequences of transitions $h_{t}=\{O_{0}\}\cup\{A_{k},R_{k},O_{k+1}:0<k<t-1\}\in(\mathcal{O}\times\mathcal{A}\times\mathcal{R})^{t}=\mathcal{H}$.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 部分可观察马尔可夫决策过程（POMDPs）是代理无法观察环境的真实状态，只能观察其转化形式的MDP，并通过一个额外的元组$\left\langle\mathcal{O},\mu_{O}\right\rangle$来指定，其中$\mathcal{O}$是观察空间，$\mu_{O}:\mathcal{S}\rightarrow\Delta({\mathcal{O}})$是观察核，将真实环境状态映射到观察概率。由于观察之间的过渡不是马尔可夫的，策略是从部分*轨迹*（我们称之为历史）到行动的映射。历史是状态转移序列$h_{t}=\{O_{0}\}\cup\{A_{k},R_{k},O_{k+1}:0<k<t-1\}\in(\mathcal{O}\times\mathcal{A}\times\mathcal{R})^{t}=\mathcal{H}$。
- en: Generalised Policy Iteration (GPI).
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 泛化策略迭代（GPI）。
- en: 'We now introduce the concept of value functions. The state value function of
    a policy $\pi$ is the expected return of the policy from state $s_{t}$, $v^{\pi}(s)=\mathbb{E}_{\pi,\mu}[Z_{t}|S_{t}=s]$.
    The action-value function (or Q-function) of a policy $\pi$ is the expected return
    of the policy from state $s_{t}$ if the agent takes $a_{t}$, $q^{\pi}(s,a)=\mathbb{E}_{\pi,\mu}[Z_{t}|S_{t}=s,A_{t}=a]$.
    Policy Evaluation (PE) is then the process that maps a policy $\pi$ to its value
    function. A canonical PE procedure starts from an arbitrary value function $V_{0}$
    and iteratively applies the Bellman operator, $\mathcal{T}$, such that:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们介绍价值函数的概念。策略$\pi$的状态价值函数是从状态$s_{t}$开始的策略的期望回报，$v^{\pi}(s)=\mathbb{E}_{\pi,\mu}[Z_{t}|S_{t}=s]$。策略$\pi$的动作-价值函数（或Q函数）是从状态$s_{t}$开始，如果代理采取$a_{t}$的策略的期望回报，$q^{\pi}(s,a)=\mathbb{E}_{\pi,\mu}[Z_{t}|S_{t}=s,A_{t}=a]$。策略评估（PE）是将策略$\pi$映射到其价值函数的过程。一个经典的PE过程从任意的价值函数$V_{0}$开始，并迭代应用Bellman算子$\mathcal{T}$，使得：
- en: '|  | $\displaystyle\hat{v}^{\pi}_{k+1}(S_{t})$ | $\displaystyle=\mathcal{T}^{\pi}[\hat{v}^{\pi}_{k}(S_{t})]:=\mathbb{E}_{\pi,\mu}\left[R(S_{t},A_{t})+\gamma\hat{v}_{k}(S_{t+1})\right],$
    |  | (2) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{v}^{\pi}_{k+1}(S_{t})$ | $\displaystyle=\mathcal{T}^{\pi}[\hat{v}^{\pi}_{k}(S_{t})]:=\mathbb{E}_{\pi,\mu}\left[R(S_{t},A_{t})+\gamma\hat{v}_{k}(S_{t+1})\right],$
    |  | (2) |'
- en: 'where $\hat{v}_{k}$ denotes the value approximation at iteration $k$, $A_{t}\sim\mathbb{P}_{\pi}(\cdot|S_{t})$,
    and $S_{t+1}\sim\mathbb{P}_{\pi,\mu}(\cdot|S_{t},A_{t})$. The Bellman operator
    is a $\gamma$-contraction in the $\ell_{\infty}$ and the $\ell_{2}$ norms, and
    its fixed point is the value of the policy $\pi$. Hence, successive applications
    of the Bellman operator improve prediction accuracy because the current value
    gets closer to the true value of the policy. We refer to the PE as the prediction
    objective (Sutton and Barto,, [2018](#bib.bib181)). Policy improvement maps a
    policy $\pi$ to an improved policy:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\hat{v}_{k}$表示迭代$k$时的值近似，$A_{t}\sim\mathbb{P}_{\pi}(\cdot|S_{t})$，以及$S_{t+1}\sim\mathbb{P}_{\pi,\mu}(\cdot|S_{t},A_{t})$。Bellman算子在$\ell_{\infty}$和$\ell_{2}$范数下是一个$\gamma$-收缩，其不动点是策略$\pi$的值。因此，连续应用Bellman算子会提高预测精度，因为当前值越来越接近策略的真实值。我们将PE称为预测目标（Sutton和Barto，[2018](#bib.bib181)）。策略改进将策略$\pi$映射到改进策略：
- en: '|  | $\displaystyle\pi_{k+1}(a&#124;S)=\mathcal{G}[\pi_{k},S]=\mathbbm{1}_{\{a\}}(\mathop{\mathrm{argmax}}_{u\in\mathcal{A}}\left[R(S,u)+\gamma
    v_{k}(S^{\prime})\right])=\mathbbm{1}_{\{a\}}(\mathop{\mathrm{argmax}}_{u\in\mathcal{A}}\left[q_{k}(S,u)\right]).$
    |  | (3) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\pi_{k+1}(a&#124;S)=\mathcal{G}[\pi_{k},S]=\mathbbm{1}_{\{a\}}(\mathop{\mathrm{argmax}}_{u\in\mathcal{A}}\left[R(S,u)+\gamma
    v_{k}(S^{\prime})\right])=\mathbbm{1}_{\{a\}}(\mathop{\mathrm{argmax}}_{u\in\mathcal{A}}\left[q_{k}(S,u)\right]).$
    |  | (3) |'
- en: We refer to GPI as a general method to solve the control problem (Sutton and
    Barto,, [2018](#bib.bib181)) deriving from the composition of PE and Policy Improvement
    (PI). In particular, we refer to the algorithm that alternates an arbitrary number
    $k$ of PE steps and a PI step as Modified Policy Iteration (MPI) (Puterman and
    Shin,, [1978](#bib.bib141); Scherrer et al.,, [2015](#bib.bib155)). For $k=1$,
    MPI recovers Value Iteration, while for $k\rightarrow+\infty$, it recovers Policy
    Iteration. For any value of $k\in[1,+\infty]$, and under mild assumptions, MPI
    converges to an optimal policy (Puterman,, [2014](#bib.bib140)).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将GPI称为一种解决控制问题的通用方法（Sutton和Barto，[2018](#bib.bib181)），其来源于PE和策略改进（PI）的组合。特别地，我们将交替执行任意数量$k$的PE步骤和一个PI步骤的算法称为改进策略迭代（MPI）（Puterman和Shin，[1978](#bib.bib141)；Scherrer等，[2015](#bib.bib155)）。当$k=1$时，MPI恢复值迭代，而当$k\rightarrow+\infty$时，它恢复策略迭代。对于任何$k\in[1,+\infty]$的值，在温和假设下，MPI收敛到最优策略（Puterman，[2014](#bib.bib140)）。
- en: In Deep RL we parameterise a policy using a neural network with parameters set
    $\theta$ and denote the distribution over action as $\pi(a|s,\theta)$. We apply
    the same reasoning for value functions, with parameters set $\phi$, which leads
    to $v(s,\phi)$ and $q(s,a,\phi)$ for the state and action value functions respectively.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度强化学习中，我们使用参数集$\theta$的神经网络来参数化策略，并将动作的分布表示为$\pi(a|s,\theta)$。我们对价值函数采用相同的推理，参数集为$\phi$，这导致状态值函数$v(s,\phi)$和动作值函数$q(s,a,\phi)$。
- en: 4 Quantifying action influences
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 量化行动影响
- en: 'We start by answering [Q1.](#S1.I1.i1 "item Q1\. ‣ Motivation. ‣ 1 Introduction
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning"), which
    aims to address the problem of *what* to measure, when referring to credit. Since
    Minsky, ([1961](#bib.bib114)) raised the Credit Assignment Problem (CAP), a multitude
    of works paraphrased his words:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先回答[Q1.](#S1.I1.i1 "item Q1\. ‣ Motivation. ‣ 1 Introduction ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning")，其目的是解决*什么*需要衡量的问题，即在提及信用时。自Minsky（[1961](#bib.bib114)）提出信用分配问题（CAP）以来，许多研究对他的观点进行了阐述：
- en: '-'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '-'
- en: “The problem of how to incorporate knowledge” and “given an outcome, how relevant
    were past decisions?” (Harutyunyan et al.,, [2019](#bib.bib67)),
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “如何融入知识的问题”和“给定结果，过去的决策有多相关？”（Harutyunyan等，[2019](#bib.bib67)），
- en: '-'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '-'
- en: “Is concerned with identifying the contribution of past actions on observed
    future outcomes” (Arumugam et al.,, [2021](#bib.bib10)),
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “关注于识别过去行动对观察到的未来结果的贡献”（Arumugam等，[2021](#bib.bib10)），
- en: '-'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '-'
- en: “The problem of measuring an action’s influence on future rewards” (Mesnard
    et al.,, [2021](#bib.bib112)),
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “测量行动对未来奖励影响的问题”（Mesnard等，[2021](#bib.bib112)），
- en: '-'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '-'
- en: “An agent must assign credit or blame for the rewards it obtains to past states
    and actions” (Chelu et al.,, [2022](#bib.bib31)),
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “代理必须将其获得的奖励归因于过去的状态和行动”（Chelu等，[2022](#bib.bib31)），
- en: '-'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '-'
- en: “The challenge of matching observed outcomes in the future to decisions made
    in the past” (Venuto et al.,, [2022](#bib.bib195)),
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “将未来观察到的结果与过去做出的决策匹配的挑战”（Venuto等，[2022](#bib.bib195)），
- en: '-'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '-'
- en: “Given an observed outcome, how much did previous actions contribute to its
    realization?” (Ferret,, [2022](#bib.bib45), Chapter 4.1).
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “给定一个观察到的结果，之前的行动对其实现贡献了多少？”（Ferret,, [2022](#bib.bib45)，第4.1章）。
- en: These descriptions converge to Minsky’s original question and show agreement
    in the literature on an informal notion of credit. In this introduction, we propose
    to reflect on the different metrics that exist in literature to quantify it. We
    generalise the idea of action value, which often only refers to $q$-values, to
    that of action influence, which describes a broader range of metrics used to quantify
    the credit of an action. While we do not provide a definitive answer on what credit
    *should* be, we review how different works in the existing RL literature have
    characterised it. We now start from developing an intuition of the notion of credit.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些描述汇聚了Minsky的原始问题，并显示了文献中对非正式信用概念的一致性。在本引言中，我们建议反思文献中存在的不同度量标准来量化信用。我们将行动价值的概念（通常仅指$q$-值）推广到行动影响的概念，后者描述了用于量化行动信用的更广泛的度量标准。虽然我们未提供关于信用*应当*是什么的最终答案，但我们回顾了现有RL文献中不同的工作如何对其进行描述。我们现在开始发展对信用概念的直观理解。
- en: Consider Figure [1](#S4.F1 "Figure 1 ‣ 4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning"), inspired to both
    Figure 1 of Harutyunyan et al., ([2019](#bib.bib67)) and to the umbrella problem
    in Osband et al., ([2020](#bib.bib127)). The action taken at $x_{0}$ determines
    the return of the episode by itself. From the point of view of *control*, any
    policy that always takes $a^{\prime}$ in $x_{0}$ (i.e., $\pi^{*}\in\Pi^{*}:\pi^{*}(a^{\prime}|x_{0})=1$),
    and then any other action afterwards, is an optimal policy. From the CAP point
    of view, some optimal actions, namely those after the first one, do not *actually*
    contribute to optimal returns. Indeed, alternative actions still produce optimal
    returns and contribute equally to each other to achieve the goal, so their credit
    is equal. We can see that, in addition to optimality, credit not only identifies
    optimal actions but informs them of how *necessary* they are to achieve an outcome
    of interest.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图 [1](#S4.F1 "Figure 1 ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")，它的灵感来自于Harutyunyan等人的图1（[2019](#bib.bib67)）和Osband等人的伞形问题（[2020](#bib.bib127)）。在$x_{0}$处采取的行动本身决定了这一回合的回报。从*控制*的角度看，任何在$x_{0}$处总是采取$a^{\prime}$（即$\pi^{*}\in\Pi^{*}:\pi^{*}(a^{\prime}|x_{0})=1$）然后再采取其他任何行动的策略都是最优策略。从CAP的角度来看，一些最优行动，即首个行动之后的那些，并不*真正*对最优回报产生贡献。实际上，替代行动仍然产生最优回报，并且相互之间贡献相等，因此它们的信用是相等的。我们可以看到，除了最优性之外，信用不仅识别最优行动，还告知这些行动在实现感兴趣的结果时的*必要性*。
- en: '![Refer to caption](img/88c2d43a69c8d5f505c92ba51d07bb49.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/88c2d43a69c8d5f505c92ba51d07bb49.png)'
- en: 'Figure 1: A simplified MDP to develop an intuition of credit. The agent starts
    at $x_{0}$, and can choose between two actions, $a^{\prime}$ and $a^{\prime\prime}$
    in each state; the reward is $1$ when reaching the upper, solid red square, and
    $0$ otherwise. The first action determines the outcome alone.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一个简化的MDP用于开发对信用的直观理解。智能体从$x_{0}$开始，在每个状态下可以选择两种行动，$a^{\prime}$和$a^{\prime\prime}$；当到达上方的实心红色方块时奖励为$1$，否则为$0$。首个行动单独决定结果。
- en: From the example, we can deduce that credit evaluates actions for their potential
    to influence an outcome. The resulting CAP is the problem of estimating the influence
    of an action over an outcome from experimental data and describes a pure association
    between them.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中，我们可以推断出，信用评估行动对结果的潜在影响。最终的CAP是估计从实验数据中行动对结果的影响的问题，并描述了它们之间的纯粹关联。
- en: Why solving the CAP?
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么要解决CAP？
- en: Action evaluation is a cornerstone of RL. In fact, solving a control problem
    often involves running a GPI scheme. Here, the influence of an action drives learning
    for it suggests a possible direction to improve the policy. For example, the action-value
    plays that role in Equation ([3](#S3.E3 "In Generalised Policy Iteration (GPI).
    ‣ 3 Notation and Background ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")). It follows that the quality of the measure of influence fundamentally
    impacts the quality of the policy improvement. Low quality evaluations can lead
    the policy to diverge from the optimal one, hinder learning, and slow down progress
    (Sutton and Barto,, [2018](#bib.bib181); van Hasselt et al.,, [2018](#bib.bib188)).
    On the contrary, high quality evaluations provide accurate, robust and reliable
    signals that foster convergence, sample-efficiency and low variance. While simple
    evaluations are enough for specialised experiments, the real world is a complex
    blend of multiple, sometimes hierarchical tasks. In these cases, the optimal value
    changes from one task to another and these simple evaluations do not bode well
    to adapt to general problem solving. Yet, the causal structure that underlies
    the real word is shared among all tasks and the modularity of its causal mechanisms
    is often a valuable property to incorporate. In these conditions, learning to
    assign credit in one environment becomes a lever to assign credit in another ([Ferret
    et al., 2021a,](#bib.bib46) ), and ultimately makes learning faster, more accurate
    and more efficient. For these reasons, and because an optimal policy only requires
    discovering one single optimal trajectory, credit stores knowledge beyond that
    expressed by optimal behaviours alone, and solving the control problem is not
    sufficient to solve the CAP, with the former being an underspecification of the
    latter.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 行动评估是强化学习的基石。事实上，解决控制问题通常涉及运行一个GPI方案。在这里，行动的影响推动了学习，因为它建议了一个可能的改进策略的方向。例如，行动价值在方程式中扮演了这个角色（[3](#S3.E3
    "在广义策略迭代（GPI）中。 ‣ 3 符号和背景 ‣ 深度强化学习中的时间信用分配概述")）。由此可见，影响度量的质量从根本上影响了策略改进的质量。低质量的评估可能导致策略偏离最佳策略，阻碍学习，减缓进展（Sutton和Barto，
    [2018](#bib.bib181); van Hasselt等， [2018](#bib.bib188)）。相反，高质量的评估提供了准确、可靠的信号，促进了收敛、样本效率和低方差。虽然简单的评估足以应对专业实验，但现实世界是多个有时是层次化任务的复杂混合体。在这些情况下，最佳值从一个任务变到另一个任务，这些简单的评估对于适应一般问题解决并不乐观。然而，现实世界中的因果结构在所有任务中都是共享的，其因果机制的模块性往往是一个值得纳入的宝贵特性。在这些条件下，在一个环境中学习分配信用成为在另一个环境中分配信用的杠杆（[Ferret等，2021a,](#bib.bib46)），并最终使学习更快、更准确、更高效。由于这些原因，并且因为一个最佳策略只需要发现一个单一的最佳轨迹，信用存储了超出最佳行为所表达的知识，而解决控制问题不足以解决CAP，前者是后者的不足描述。
- en: 4.1 Are all action values, *credit*?
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 所有的行动价值，*信用*吗？
- en: As we stated earlier, most Deep RL algorithms use some form of *action influence*
    to evaluate the impacts of an action on an outcome. This is a fundamental requirement
    to rank actions and select the optimal one to solve complex tasks. For example,
    many model-free methods use the *state-action value* function $q^{\pi}(s,a)$ to
    evaluate actions (Mnih et al.,, [2015](#bib.bib118); van Hasselt et al.,, [2016](#bib.bib189)),
    where actions contribute as much as the expected return they achieve at termination
    of the episode. Advantage Learning (AL) (Baird,, [1999](#bib.bib15); Mnih et al.,,
    [2016](#bib.bib116); [Wang et al., 2016b,](#bib.bib204) , Chapter 5) uses the
    *advantage* function $A^{\pi}(s_{t},a_{t})=q^{\pi}(s_{t},a_{t})-v^{\pi}(s_{t})$
    ¹¹1To be consistent with the RL literature we abuse notation and denote the advantage
    with a capital letter $A^{\pi}$ despite not being random and being the same symbol
    of the action $A_{t}$. to measure credit, while other works study the effects
    of the *action-gap* (Farahmand,, [2011](#bib.bib44); Bellemare et al.,, [2016](#bib.bib27);
    [Vieillard et al., 2020b,](#bib.bib197) ) on it, that is the relative difference
    between the expected return of the best action and that of another action, usually
    the second best. Action influence is also a key ingredient of actor-critic and
    policy gradient methods (Lillicrap et al.,, [2015](#bib.bib101); Mnih et al.,,
    [2016](#bib.bib116); [Wang et al., 2016a,](#bib.bib202) ), where the policy gradient
    is proportional to $\mathbb{E}_{\mu,\pi}[A^{\pi}(s,a)\nabla\log\pi(A|s)]$, with
    $A^{\pi}(s,a)$ estimating the influence of the action $A$.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所述，大多数深度强化学习算法使用某种形式的*行动影响*来评估行动对结果的影响。这是对行动进行排名并选择最佳行动以解决复杂任务的基本要求。例如，许多无模型方法使用*状态-行动值*函数
    $q^{\pi}(s,a)$ 来评估行动（Mnih et al., [2015](#bib.bib118); van Hasselt et al., [2016](#bib.bib189)），其中行动的贡献等同于它们在回合结束时所获得的预期回报。优势学习（AL）（Baird，[1999](#bib.bib15);
    Mnih et al., [2016](#bib.bib116); [Wang et al., 2016b,](#bib.bib204) ，第5章）使用*优势*函数
    $A^{\pi}(s_{t},a_{t})=q^{\pi}(s_{t},a_{t})-v^{\pi}(s_{t})$ ¹¹为了与强化学习文献保持一致，尽管优势函数不是随机的并且与行动
    $A_{t}$ 使用相同的符号，我们仍然使用大写字母 $A^{\pi}$ 来表示。来衡量信用，而其他研究则研究*行动差距*（Farahmand，[2011](#bib.bib44);
    Bellemare et al., [2016](#bib.bib27); [Vieillard et al., 2020b,](#bib.bib197)）的影响，即最佳行动的预期回报与另一个行动，通常是第二最佳行动的预期回报之间的相对差异。行动影响也是演员-评论家和策略梯度方法（Lillicrap
    et al., [2015](#bib.bib101); Mnih et al., [2016](#bib.bib116); [Wang et al., 2016a,](#bib.bib202)）的关键组成部分，其中策略梯度与
    $\mathbb{E}_{\mu,\pi}[A^{\pi}(s,a)\nabla\log\pi(A|s)]$ 成正比，其中 $A^{\pi}(s,a)$ 估计行动
    $A$ 的影响。
- en: 'These proxies are sufficient to select optimal actions and unlock solutions
    to complex tasks (Silver et al.,, [2018](#bib.bib166); [Wang et al., 2016b,](#bib.bib204)
    ; Kapturowski et al.,, [2019](#bib.bib87); Badia et al.,, [2020](#bib.bib13);
    [Ferret et al., 2021b,](#bib.bib47) ). However, while these works explicitly refer
    to the action influence as a measure of credit, the term is not formally defined
    and it remains unclear where to draw the line between *credit* and other quantities.
    Key questions arise: What is the difference between these quantities and credit?
    Do they actually represent credit as originally formulated by Minsky, ([1961](#bib.bib114))?
    If so, under what conditions do they do? Without a clear definition of what to
    measure, we do not have an appropriate quantity to target when designing an algorithm
    to solve the CAP. More importantly, we do not have an appropriate quantity to
    use as a single source of truth and term of reference to measure the accuracy
    of other metrics of action influence, and how well they approximate credit. To
    fill this gap, we proceed as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代理足以选择最佳行动并解锁复杂任务的解决方案（Silver et al., [2018](#bib.bib166); [Wang et al., 2016b,](#bib.bib204);
    Kapturowski et al., [2019](#bib.bib87); Badia et al., [2020](#bib.bib13); [Ferret
    et al., 2021b,](#bib.bib47)）。然而，虽然这些工作明确将行动影响作为信用的衡量标准，但这个术语没有被正式定义，并且尚不清楚如何区分*信用*和其他量。关键问题出现了：这些量和信用之间有什么区别？它们是否真正代表了Minsky（[1961](#bib.bib114)）最初提出的信用？如果是，那么在什么条件下它们才会这样？没有明确的测量定义，我们在设计解决CAP的算法时没有合适的目标量。更重要的是，我们没有合适的量作为单一的真相源和参考标准来衡量其他行动影响度量的准确性，以及它们对信用的近似程度。为填补这一空白，我们将按照以下步骤进行：
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [4.2](#S4.SS2 "4.2 What is a goal? ‣ 4 Quantifying action influences
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning") formalises
    what is a goal or an outcome: what we evaluate the action for;'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [4.2](#S4.SS2 "4.2 什么是目标？ ‣ 4 量化行动影响 ‣ 深度强化学习中的时间信用分配调查") 形式化了什么是目标或结果：我们评估行动的标准；
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Section [4.3](#S4.SS3 "4.3 What is an assignment? ‣ 4 Quantifying action influences
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning") unifies
    existing functions under the same formalism;
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[4.3节](#S4.SS3 "4.3 What is an assignment? ‣ 4 Quantifying action influences
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")将现有的功能统一在相同的形式下；
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Section [4.4](#S4.SS4 "4.4 The credit assignment problem ‣ 4 Quantifying action
    influences ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")
    formalises the CAP following this definition.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[4.4节](#S4.SS4 "4.4 The credit assignment problem ‣ 4 Quantifying action influences
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")按照这一定义对CAP进行了形式化。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Section [4.5](#S4.SS5 "4.5 Existing assignment functions ‣ 4 Quantifying action
    influences ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")
    analyses how different works interpreted and quantified action influences and
    reviews them.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[4.5节](#S4.SS5 "4.5 Existing assignment functions ‣ 4 Quantifying action influences
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")分析了不同研究如何解释和量化行动影响，并对其进行了回顾。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Section [4.6](#S4.SS6 "4.6 Discussion ‣ 4 Quantifying action influences ‣ A
    Survey of Temporal Credit Assignment in Deep Reinforcement Learning") distils
    the properties that existing measure of action influence.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[4.6节](#S4.SS6 "4.6 Discussion ‣ 4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning")提炼了现有行动影响度量的特性。
- en: We suggest the reader only interested in the final formalism to directly skip
    to Section [4.4](#S4.SS4 "4.4 The credit assignment problem ‣ 4 Quantifying action
    influences ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning"),
    and to come back to the next sections to understand the motivation behind it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议读者如果只对最终的形式化内容感兴趣，可以直接跳到第[4.4节](#S4.SS4 "4.4 The credit assignment problem
    ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning")，然后再回到后续章节，以理解其背后的动机。
- en: 4.2 What is a goal?
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 什么是目标？
- en: Because credit measures the influence of an action upon achieving a certain
    goal, to define credit formally we must be able to describe *goals* formally,
    and without a clear understanding of what constitutes one, an agent cannot construct
    a learning signal to evaluate its actions. Goal is a synonym for *purpose*, which
    we can informally describe as a performance to meet or a prescription to follow.
    Defining a goal rigorously allows to make the relationship between the action
    and the goal explicit (Ferret,, [2022](#bib.bib45), Chapter 4) and enables the
    agent to decompose complex behaviour into elementary ones in a compositional (Sutton
    et al.,, [1999](#bib.bib184); Bacon et al.,, [2017](#bib.bib12)), and possibly
    hierarchical way (Flet-Berliac,, [2019](#bib.bib50); Pateria et al.,, [2021](#bib.bib129);
    Hafner et al.,, [2022](#bib.bib65)). This idea is at the foundation of many CA
    methods (Sutton et al.,, [1999](#bib.bib184), [2011](#bib.bib183); [Schaul et al.,
    2015a,](#bib.bib153) ; Andrychowicz et al.,, [2017](#bib.bib5); Harutyunyan et al.,,
    [2019](#bib.bib67); Bacon et al.,, [2017](#bib.bib12); Smith et al.,, [2018](#bib.bib170);
    Riemer et al.,, [2018](#bib.bib149); Bagaria and Konidaris,, [2019](#bib.bib14);
    Harutyunyan et al.,, [2018](#bib.bib68); Klissarov and Precup,, [2021](#bib.bib91)).
    We proceed with a formal definition of goals in the next paragraph, and review
    how these goals are *represented* in seminal works on CA in the one after. This
    will lay the foundation for a unifying notion of credit later in Sections [4.3](#S4.SS3
    "4.3 What is an assignment? ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning").
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于信用衡量的是一个动作对实现某一目标的影响，为了正式定义信用，我们必须能够正式描述*目标*，如果对目标的构成没有清晰的理解，代理就无法构建学习信号来评估其动作。目标是*目的*的同义词，我们可以非正式地将其描述为要达成的绩效或要遵循的规定。严谨地定义目标可以明确动作与目标之间的关系（Ferret，，[2022](#bib.bib45)，第4章），并使代理能够以组成的（Sutton
    et al.,，[1999](#bib.bib184)；Bacon et al.,，[2017](#bib.bib12)），并可能是分层的方式（Flet-Berliac，，[2019](#bib.bib50)；Pateria
    et al.,，[2021](#bib.bib129)；Hafner et al.,，[2022](#bib.bib65)）将复杂行为分解为基本行为。这一思想是许多CA方法的基础（Sutton
    et al.,，[1999](#bib.bib184)，[2011](#bib.bib183)；[Schaul et al., 2015a,](#bib.bib153)；Andrychowicz
    et al.,，[2017](#bib.bib5)；Harutyunyan et al.,，[2019](#bib.bib67)；Bacon et al.,，[2017](#bib.bib12)；Smith
    et al.,，[2018](#bib.bib170)；Riemer et al.,，[2018](#bib.bib149)；Bagaria and Konidaris，，[2019](#bib.bib14)；Harutyunyan
    et al.,，[2018](#bib.bib68)；Klissarov and Precup，，[2021](#bib.bib91)）。我们将在下一段中对目标进行正式定义，并回顾这些目标在经典CA研究中的*表现形式*。这将为后续[4.3节](#S4.SS3
    "4.3 What is an assignment? ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")中统一的信用概念奠定基础。
- en: Defining goals.
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义目标。
- en: 'To define goals formally we adopt the *reward hypothesis*, which posits:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正式定义目标，我们采用*奖励假设*，该假设提出：
- en: That all of what we mean by goals and purposes can be well thought of as maximization
    of the expected value of the cumulative sum of a received scalar signal (reward).
    (Sutton,, [2004](#bib.bib180)).
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们所说的目标和目的可以很好地被认为是对接收的标量信号（奖励）累积和期望值的最大化。（Sutton, [2004](#bib.bib180)）。
- en: 'Here, the goal is defined as the *behaviour* that results from the process
    of maximising the return. The reward hypothesis has been further advanced by later
    studies (Abel et al.,, [2021](#bib.bib1); Pitis,, [2019](#bib.bib135); Shakerinava
    and Ravanbakhsh,, [2022](#bib.bib163); Bowling et al.,, [2023](#bib.bib28)). In
    the following text, we employ the goal definition in Bowling et al., ([2023](#bib.bib28)),
    which we report hereafter:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，目标被定义为从最大化回报的过程中产生的*行为*。奖励假设已被后来的研究进一步发展（Abel et al., [2021](#bib.bib1);
    Pitis, [2019](#bib.bib135); Shakerinava 和 Ravanbakhsh, [2022](#bib.bib163); Bowling
    et al., [2023](#bib.bib28)）。在以下文本中，我们使用 Bowling et al., ([2023](#bib.bib28)) 中的目标定义，如下所述：
- en: Definition 1  (Goal).
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1 （目标）。
- en: Given a distribution of finite histories $\mathbb{P}(H),\forall H\in\mathcal{H}$,
    we define a *goal* as a partial ordering over $\mathbb{P}(H)$, and for all $h,h^{\prime}\in\mathcal{H}$
    we write $h\succsim h^{\prime}$ to indicate that $h$ is preferred to $h^{\prime}$
    or that the two are indifferently preferred.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 给定有限历史的分布 $\mathbb{P}(H),\forall H\in\mathcal{H}$，我们将*目标*定义为 $\mathbb{P}(H)$
    上的部分排序，并且对于所有 $h,h^{\prime}\in\mathcal{H}$，我们写作 $h\succsim h^{\prime}$ 来表示 $h$
    比 $h^{\prime}$ 更受偏好或两者同样被偏好。
- en: Here, $H$ is a random history in the set of all histories $\mathcal{H}$ as described
    in Section [3](#S3 "3 Notation and Background ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning"), and $\mathbb{P}(H)$ is an unknown distribution
    over histories, different from that induced by the policy and the environment.
    An agent behaviour and an environment then induce a new distribution over histories
    and we obtain $\mathbb{P}_{\mu,\pi}(H)$ as described in Section [3](#S3 "3 Notation
    and Background ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"). This in turn allows to define a partial ordering over policies, rather
    than histories, and we write analogously $\pi\succsim\pi^{\prime}$ to indicate
    the preference. For the Markov Reward Theorem (Bowling et al.,, [2023](#bib.bib28),
    Theorem 4.1) and under mild conditions (Bowling et al.,, [2023](#bib.bib28)),
    there exist a deterministic, Markov reward function²²2We omit the transition dependent
    discounting for the sake of conciseness and because not relevant to our problem.
    The reader can consult Pitis, ([2019](#bib.bib135)); White, ([2017](#bib.bib207))
    for details. $R:\mathcal{O}\times\mathcal{A}\rightarrow[0,1]$ such that the maximisation
    of the expected sum of rewards is consistent with the preference relation over
    policies.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$H$ 是所有历史集合 $\mathcal{H}$ 中的一个随机历史，如第[3](#S3 "3 Notation and Background
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")节所述，而
    $\mathbb{P}(H)$ 是一个未知的历史分布，区别于由策略和环境引起的分布。一个代理行为和环境随后会引发一个新的历史分布，我们得到 $\mathbb{P}_{\mu,\pi}(H)$，如第[3](#S3
    "3 Notation and Background ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")节所述。这反过来允许在策略上定义部分排序，而不是历史上，我们类似地写作 $\pi\succsim\pi^{\prime}$ 来表示偏好。对于马尔可夫奖励定理（Bowling
    et al., [2023](#bib.bib28), 定理 4.1）和在轻微条件下（Bowling et al., [2023](#bib.bib28)），存在一个确定性的马尔可夫奖励函数²²2为了简洁起见，我们省略了与过渡相关的折扣，因为与我们的问题无关。读者可以参阅
    Pitis, ([2019](#bib.bib135)); White, ([2017](#bib.bib207)) 以获取详细信息。 $R:\mathcal{O}\times\mathcal{A}\rightarrow[0,1]$，使得奖励的期望总和的最大化与策略上的偏好关系一致。
- en: Subjective and objective goals.
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 主观目标和客观目标。
- en: The Markov Reward Theorem holds both in the case of the preferences being defined
    internally by the agent itself – this is the case of intrinsic motivation (Piaget
    et al.,, [1952](#bib.bib134); Chentanez et al.,, [2004](#bib.bib35); Barto et al.,,
    [2004](#bib.bib21); Singh et al.,, [2009](#bib.bib167); Barto,, [2013](#bib.bib19);
    Colas et al.,, [2022](#bib.bib37)) – and in case they originate from an external
    entity, such as agent-designer. In the first case, the agent doing the maximising
    is the same as the one holding the ordering over policies, and we refer to the
    corresponding goal as a subjective goal. In the second case, an agent designer
    or an unknown, non-observable entity holds the ordering and a separate learning
    agent is the one pursuing the optimisation process. We refer to a goal as an objective
    goal in this latter case. These settings usually correspond to the distinction
    between goals and sub-goals in the literature (Liu et al.,, [2022](#bib.bib104)).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫奖励定理在两种情况下成立：一是偏好由智能体自身内部定义——即内在动机的情况（Piaget 等人，[1952](#bib.bib134)；Chentanez
    等人，[2004](#bib.bib35)；Barto 等人，[2004](#bib.bib21)；Singh 等人，[2009](#bib.bib167)；Barto，[2013](#bib.bib19)；Colas
    等人，[2022](#bib.bib37)）——二是偏好来源于外部实体，如智能体设计师。在第一种情况下，进行最大化的智能体与持有政策排序的智能体是相同的，我们将相应的目标称为主观目标。在第二种情况下，智能体设计师或未知的、不可观察的实体持有排序，另一个学习智能体负责追求优化过程。我们将这种情况的目标称为客观目标。这些设置通常对应于文献中的目标和子目标之间的区分（Liu
    等人，[2022](#bib.bib104)）。
- en: Outcomes.
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 结果。
- en: A particularly interesting use of goals for CA is in hindsight (Andrychowicz
    et al.,, [2017](#bib.bib5)). Here the agent acts with a goal in mind, but it evaluates
    a trajectory as if *a* reward function – one different from the original one –
    was maximised in the current trajectory. We discuss the benefits of these methods
    in Section [6.4](#S6.SS4 "6.4 Conditioning in hindsight ‣ 6 Methods to assign
    credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"). When this is the case, we use the term outcome to indicate a realised
    goal in hindsight. In particular, given a history $H\sim\mathbb{P}_{\mu,\pi}(H)$,
    there exist a deterministic, Markov reward function $R$ that is maximal in $H$.
    We refer to the corresponding $H$ as an outcome. For example, consider a trajectory
    $h$ that ends in a certain state $s$. There exist a Markov reward function that
    outputs always $0$ and $1$ only when the $s$ is the final state of $h$. We refer
    to $h$ as an *outcome*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在CA中，目标的一个特别有趣的应用是在事后（Andrychowicz 等人，[2017](#bib.bib5)）。在这里，智能体以某个目标为导向行动，但它评估一个轨迹，仿佛*一个*奖励函数——与原始的不同——在当前轨迹中被最大化。我们在第[6.4](#S6.SS4
    "6.4 Conditioning in hindsight ‣ 6 Methods to assign credit in Deep RL ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning")节讨论了这些方法的好处。当情况是这样时，我们使用“结果”一词来表示事后的实现目标。特别地，给定一个历史$H\sim\mathbb{P}_{\mu,\pi}(H)$，存在一个在$H$中最大化的确定性马尔可夫奖励函数$R$。我们将相应的$H$称为结果。例如，考虑一个以某个状态$s$结束的轨迹$h$。存在一个马尔可夫奖励函数，总是输出$0$，仅在$s$是$h$的最终状态时输出$1$。我们将$h$称为*结果*。
- en: In other words, this way of defining goals or outcomes corresponds to defining
    a task to solve, which in turn can be expressed through a reward function with
    the characteristics described above. Vice-versa, the reward function can *encode*
    a task. When credit is assigned with respect to a particular goal or outcome,
    it then evaluates the influence of an action to solving a particular task. As
    discussed above, this is key to decompose and recompose complex behaviours and
    the definition aligns with that of other disciplines, such as psychology where
    a goal …is a cognitive representation of something that is possible in the future
    (Elliot and Fryer,, [2008](#bib.bib41)) or philosophy, where representations do
    not merely read the world as it is but they express *preferences* over something
    that is possible in the future (Hoffman,, [2016](#bib.bib71); Prakash et al.,,
    [2021](#bib.bib137); Le Lan et al.,, [2022](#bib.bib99)).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这种定义目标或结果的方式对应于定义一个待解决的任务，这可以通过具有上述特征的奖励函数来表达。反之，奖励函数可以*编码*一个任务。当与特定目标或结果相关的信用被分配时，它评估某个行动对解决特定任务的影响。如上所述，这对于分解和重新组合复杂行为至关重要，并且这种定义与其他学科的定义一致，例如心理学中目标……是对未来可能发生的事情的认知表征（Elliot
    和 Fryer，[2008](#bib.bib41)）或哲学中，表征不仅仅是读取世界的状态，而是对未来可能发生的事情表达*偏好*（Hoffman，[2016](#bib.bib71)；Prakash
    等人，[2021](#bib.bib137)；Le Lan 等人，[2022](#bib.bib99)）。
- en: Representing goals and outcomes.
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表示目标和结果。
- en: However, expressing the relation between actions and goals explicitly, that
    is, when the function that returns the credit of an action has a goal as an input,
    raises the problem of how to represent a goal for computational purposes. This
    is important because among the CA methods that define goals explicitly (Sutton
    et al.,, [2011](#bib.bib183); [Schaul et al., 2015a,](#bib.bib153) ; Andrychowicz
    et al.,, [2017](#bib.bib5); Rauber et al.,, [2019](#bib.bib147); Harutyunyan et al.,,
    [2019](#bib.bib67); Tang and Kucukelbir,, [2021](#bib.bib185); Arulkumaran et al.,,
    [2022](#bib.bib8); Chen et al.,, [2021](#bib.bib33)), not many use the rigour
    of a general-purpose definition of goal such as that in Bowling et al., ([2023](#bib.bib28)).
    In these works, the goal-representation space, which we denote as $\psi\in\Psi$,
    is arbitrarily chosen to represent specific features of a trajectory. It denotes
    an object, rather than a performance or a prescription to meet. For example, a
    goal-representation $\psi$ can be a state (Sutton et al.,, [2011](#bib.bib183);
    Andrychowicz et al.,, [2017](#bib.bib5)) and $\psi\in\Psi=\mathcal{S}$. It can
    be a specific observation (Nair et al.,, [2018](#bib.bib120)) with $\psi\in\Psi=\mathcal{O}$.
    Alternatively, it can be an abstract feature vector (Mesnard et al.,, [2021](#bib.bib112))
    that reports on some characteristics of a history, and we have $\psi\in\Psi=\mathbb{R}^{d}$
    with $d$ is the dimensionality of the vector. Even, a goal can be represented
    by a natural language instruction (Luketina et al.,, [2019](#bib.bib106)) and
    $\psi\in\Psi=\mathbb{R}^{d}$ be the embedding of that piece of text. A goal can
    be represented by a scalar $\psi\in\Psi=\mathbb{R}$ (Chen et al.,, [2021](#bib.bib33))
    that indicates a specific return to achieve, or even a full command (Schmidhuber,,
    [2019](#bib.bib157)), that is a return to achieve is a specific window of time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当明确表达动作与目标之间的关系时，即当返回动作信用的函数以目标作为输入时，就会出现如何为计算目的表示目标的问题。这一点很重要，因为在明确定义目标的CA方法中（Sutton等，[2011](#bib.bib183)；[Schaul等，2015a](#bib.bib153)；Andrychowicz等，[2017](#bib.bib5)；Rauber等，[2019](#bib.bib147)；Harutyunyan等，[2019](#bib.bib67)；Tang和Kucukelbir，[2021](#bib.bib185)；Arulkumaran等，[2022](#bib.bib8)；Chen等，[2021](#bib.bib33)），并不是很多使用像Bowling等（[2023](#bib.bib28)）中的通用定义目标的严谨性。在这些工作中，目标表示空间，我们表示为$\psi\in\Psi$，是任意选择以表示轨迹的特定特征。它表示一个对象，而不是一个表现或一个需要满足的处方。例如，目标表示$\psi$可以是一个状态（Sutton等，[2011](#bib.bib183)；Andrychowicz等，[2017](#bib.bib5)），且$\psi\in\Psi=\mathcal{S}$。它可以是一个特定的观察（Nair等，[2018](#bib.bib120)），且$\psi\in\Psi=\mathcal{O}$。或者，它可以是一个抽象的特征向量（Mesnard等，[2021](#bib.bib112)），报告历史的一些特征，我们有$\psi\in\Psi=\mathbb{R}^{d}$，其中$d$是向量的维度。甚至，目标可以通过自然语言指令表示（Luketina等，[2019](#bib.bib106)），且$\psi\in\Psi=\mathbb{R}^{d}$是该文本片段的嵌入。一个目标可以通过一个标量表示$\psi\in\Psi=\mathbb{R}$（Chen等，[2021](#bib.bib33)），指示需要实现的特定回报，或者甚至通过一个完整的命令表示（Schmidhuber，[2019](#bib.bib157)），即需要实现的回报是特定时间窗口内的回报。
- en: While these representations are all useful heuristics, they lack formal rigour
    and leave space to ambiguities. For example, saying that the goal is a state might
    mean that visiting the state at the end of the trajectory is the goal or that
    visiting it in the middle of it is the goal. This is often not formally defined
    and what is the reward function that corresponds to that specific representation
    of a goal is not always clear. In the following text, when surveying a method
    or a metric that specifies a goal, we refer to the specific goal representation
    used in the work and make an effort to detail what is the reward function that
    underpins that goal representation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些表示法都是有用的启发式方法，但它们缺乏正式的严谨性，并且存在歧义的空间。例如，说目标是一个状态可能意味着在轨迹结束时访问该状态是目标，或者在轨迹中间访问该状态是目标。这通常没有形式化定义，并且与该目标表示法对应的奖励函数也并不总是清楚。在以下文本中，当调查指定目标的方法或度量时，我们会参考工作中使用的具体目标表示法，并努力详细说明支撑该目标表示法的奖励函数是什么。
- en: 4.3 What is an *assignment*?
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 什么是*任务*？
- en: 'Having established a formalism for goals and outcomes we are now ready to describe
    credit formally and we proceed with a formalism that unifies the existing measures
    of action influence. We first describe a generic function that generalises most
    CAs, and then proceed to formalise the CAP. Overall, this formulation provides
    a term of reference for the quantities described in Section [4.5](#S4.SS5 "4.5
    Existing assignment functions ‣ 4 Quantifying action influences ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning"). We now formalise
    an *assignment*:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立了目标和结果的形式化之后，我们现在准备正式描述信用，并且我们将采用一种统一现有行动影响度量的形式化方法。我们首先描述一个概括大多数CAs的通用函数，然后继续形式化CAP。总体而言，这种公式为第[4.5节](#S4.SS5
    "4.5 现有分配函数 ‣ 4 量化行动影响 ‣ 深度强化学习中的时间信用分配调查")中描述的量提供了参考项。我们现在形式化一个*分配*：
- en: Definition 2  (Assignment).
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2  （分配）。
- en: 'Consider an action $a\in\mathcal{A}$, a goal $g\in\mathcal{G}$, and a context
    $c\in\mathcal{C}$ representing some experience data. We use the term *assignment
    function* or simply *assignment* to denote a function $\mathcal{K}$ that maps
    a context, an action and an outcome to a quantity $y\in\mathcal{Y}$, which we
    refer to as the influence of the action:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个行动 $a\in\mathcal{A}$，一个目标 $g\in\mathcal{G}$，以及一个表示某些经验数据的背景 $c\in\mathcal{C}$。我们使用术语*分配函数*或简单的*分配*来表示一个函数
    $\mathcal{K}$，它将一个背景、一个行动和一个结果映射到一个量 $y\in\mathcal{Y}$，我们称之为行动的影响：
- en: '|  | $\displaystyle K:\mathcal{C}\times\mathcal{A}\times\mathcal{G}\rightarrow\mathcal{Y}.$
    |  | (4) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K:\mathcal{C}\times\mathcal{A}\times\mathcal{G}\rightarrow\mathcal{Y}.$
    |  | (4) |'
- en: Here, a context $c\in\mathcal{C}$ represents some input data and can be arbitrarily
    chosen depending on the assignment in question. A context must hold information
    about the present, for example, the current state or the current observation;
    it may contain information about the past, for example, the sequence of past decisions
    occurred until now for a POMDP; to evaluate the current action, it must contain
    information about what future actions will be taken in-potentia, for example by
    specifying a policy to follow when $a\in\mathcal{A}$ is not taken, or a fixed
    trajectory, in which case the current action is evaluated in hindsight (Andrychowicz
    et al.,, [2017](#bib.bib5)). We provide further details to contexts in Appendix [B](#S2a
    "B Further details on contexts ‣ A Survey of Temporal Credit Assignment in Deep
    Reinforcement Learning").
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，背景 $c\in\mathcal{C}$ 代表一些输入数据，可以根据具体任务随意选择。背景必须包含关于现在的信息，例如当前状态或当前观察；它还可能包含关于过去的信息，例如直到现在的POMDP中的过去决策序列；为了评估当前的行动，它必须包含关于未来将采取的行动的信息，例如通过指定在
    $a\in\mathcal{A}$ 未被采取时要遵循的策略，或者一个固定的轨迹，在这种情况下，当前行动在事后被评估（Andrychowicz et al.,
    [2017](#bib.bib5)）。我们在附录[B](#S2a "B 进一步细节 ‣ 深度强化学习中的时间信用分配调查")中提供了有关背景的更多细节。
- en: In the general case, the action influence is a random variable $Y\in\mathcal{Y}\subset\mathbb{R}^{d}$.
    This is the case, for example, of the action-value distribution (Bellemare et al.,,
    [2017](#bib.bib25)) as described in Equation [10](#S4.E10 "In Distributional values
    ‣ 4.5 Existing assignment functions ‣ 4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning"), where the action
    influence is defined over the full distribution of returns. However, most methods
    extract some scalar measures of the full influence distribution, such as expectations
    (Watkins,, [1989](#bib.bib205)), and the action influence becomes a scalar $y\in\mathbb{R}$.
    In the following text, we mostly consider scalar forms of the influence $\mathcal{Y}=\mathbb{R}$
    as these represent the majority of the existing formulations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般情况下，行动影响是一个随机变量 $Y\in\mathcal{Y}\subset\mathbb{R}^{d}$。例如，行动价值分布（Bellemare
    et al., [2017](#bib.bib25)）就是这种情况，如方程[10](#S4.E10 "在分布值 ‣ 4.5 现有分配函数 ‣ 4 量化行动影响
    ‣ 深度强化学习中的时间信用分配调查")中所述，其中行动影响在回报的整个分布上定义。然而，大多数方法从完整的影响分布中提取一些标量度量，例如期望（Watkins,
    [1989](#bib.bib205)），而行动影响变成一个标量 $y\in\mathbb{R}$。在以下文本中，我们主要考虑影响的标量形式 $\mathcal{Y}=\mathbb{R}$，因为这些代表了现有公式的多数情况。
- en: In practice, an *assignment* provides a single mathematical form to talk about
    the multitude of ways to quantify action influence that are used in the literature.
    It takes an action $a\in\mathcal{A}$, some contextual data $c\in\mathcal{C}$ and
    a goal $g\in\mathcal{G}$ and maps it to some measure of action influence. While
    maintaining the same mathematical form, different assignments can return different
    values of action influence and steer the improvement in different directions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，*assignment* 提供了一种单一的数学形式来讨论文献中用于量化行动影响的各种方法。它接受一个行动 `$a\in\mathcal{A}$`、一些上下文数据
    `$c\in\mathcal{C}$` 和一个目标 `$g\in\mathcal{G}$`，并将其映射到某种行动影响的度量上。在保持相同数学形式的同时，不同的
    assignment 可以返回不同的行动影响值，并引导改进朝着不同的方向发展。
- en: 'Equation ([4](#S4.E4 "In Definition 2 (Assignment). ‣ 4.3 What is an assignment?
    ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning")) also resembles the General Value Function (GVF)
    (Sutton et al.,, [2011](#bib.bib183)), where the influence $y=q^{\pi}(s,a,g)$
    is the expected return of the policy $\pi$ when taking action $a$ in state $s$,
    with respect a goal $g$. However, in GVFs: (i) $y$is an *action value* and does
    not generalise other forms of action influence; the goal is an MDP state $g\in\mathcal{S}$
    and does not generalise to our notion of goals in Section [4.2](#S4.SS2 "4.2 What
    is a goal? ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning"); the function only considers forward predictions
    and does not generalise to evaluating an action in hindsight (Andrychowicz et al.,,
    [2017](#bib.bib5)). Table [2](#S4.T2 "Table 2 ‣ 4.5 Existing assignment functions
    ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning") page [2](#S4.T2 "Table 2 ‣ 4.5 Existing assignment
    functions ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") contains further details on the comparison and
    further specifies the relationship between the most common functions and their
    corresponding assignment.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 ([4](#S4.E4 "In Definition 2 (Assignment). ‣ 4.3 What is an assignment? ‣
    4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in Deep
    Reinforcement Learning")) 还类似于一般价值函数 (GVF) (Sutton et al., [2011](#bib.bib183))，其中影响
    `$y=q^{\pi}(s,a,g)$` 是策略 `$π$` 在状态 `$s$` 下采取行动 `$a$` 时，相对于目标 `$g$` 的期望回报。然而，在
    GVFs 中：(i) `$y$` 是 *行动价值*，并不泛化其他形式的行动影响；目标是一个 MDP 状态 `$g\in\mathcal{S}$`，并不泛化到我们在第
    [4.2 节](#S4.SS2 "4.2 What is a goal? ‣ 4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning") 中提出的目标概念；该函数仅考虑前向预测，而不泛化为对行动的事后评估
    (Andrychowicz et al., [2017](#bib.bib5))。表 [2](#S4.T2 "Table 2 ‣ 4.5 Existing
    assignment functions ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning") 第 [2](#S4.T2 "Table 2 ‣ 4.5
    Existing assignment functions ‣ 4 Quantifying action influences ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning") 页包含了更多关于比较的细节，并进一步说明了最常见函数及其对应
    assignment 之间的关系。
- en: 4.4 The credit assignment problem
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 信用分配问题
- en: The generality of the assignment formalism reflects the great heterogeneity
    of action influence metrics, which we review later in Section [4.5](#S4.SS5 "4.5
    Existing assignment functions ‣ 4 Quantifying action influences ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning"). This heterogeneity
    shows that, even if most studies agree on an intuitive notion of credit, they
    diverge in practice on how to quantify credit mathematically. Having unified the
    existing assignments in the previous section, we now proceed to formalise the
    CAP analogously. This allows us to put the existing methods into a coherent perspective
    as a guarantee for a fair comparison, and to maintain the heterogeneity of the
    existing measures of action influence.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: assignment 形式的通用性反映了行动影响度量的巨大异质性，我们将在第[4.5节](#S4.SS5 "4.5 Existing assignment
    functions ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")中进一步回顾。这种异质性表明，即使大多数研究对信用的直观概念达成一致，在如何以数学方式量化信用方面实践中仍然存在分歧。在上一节中统一了现有的
    assignments 后，我们现在类似地对 CAP 进行形式化。这使我们能够将现有方法放在一个连贯的视角下，以保证公平比较，并保持现有行动影响度量的异质性。
- en: We cast the CAP as the problem of approximating a measure of action influence
    from experience. We assume standard model-free, Deep RL settings and consider
    an assignment represented as a neural network $k:\mathcal{C}\times\mathcal{A}\times\mathcal{G}\times\Phi\rightarrow\mathbb{R}$
    with parameters $\phi\in\Phi=\mathbb{R}^{n}$ that can be used to approximate the
    credit of the actions. This usually represents the critic, or the value function
    of a RL algorithm. In addition, we admit a stochastic function to represent the
    policy, also in the form of a neural network $f:\mathcal{S}\times\Theta\rightarrow\Delta(\mathcal{A})$,
    with parameters set $\theta\in\Theta=\mathbb{R}^{m}$. We assume that $n\ll|\mathcal{S}|\times|\mathcal{A}|$
    and $m\ll|\mathcal{S}|\times|\mathcal{A}|$ and note that often subsets of parameters
    are shared among the two functions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将CAP视为从经验中逼近动作影响度量的问题。我们假设标准的无模型深度强化学习设置，并考虑一个表示为神经网络的分配 $k:\mathcal{C}\times\mathcal{A}\times\mathcal{G}\times\Phi\rightarrow\mathbb{R}$，其参数为
    $\phi\in\Phi=\mathbb{R}^{n}$，可用于逼近动作的信用。这通常代表评论家或强化学习算法的价值函数。此外，我们接受一个随机函数来表示策略，形式也为神经网络
    $f:\mathcal{S}\times\Theta\rightarrow\Delta(\mathcal{A})$，参数集为 $\theta\in\Theta=\mathbb{R}^{m}$。我们假设
    $n\ll|\mathcal{S}|\times|\mathcal{A}|$ 和 $m\ll|\mathcal{S}|\times|\mathcal{A}|$，并注意到通常两个函数之间会共享参数子集。
- en: We further assume that the agent has access to a set of experiences $\mathcal{D}$
    and that it can sample from it according to a distribution $D\sim\mathbb{P}_{C}$.
    This can be a pre-compiled set of external demonstrations, where $\mathbb{P}_{C}(D)=\mathcal{U}(D)$,
    or an MDP, where $\mathbb{P}_{C}=\mathbb{P}_{\mu,\pi}(D)$, or even a fictitious
    model of an MDP $\mathbb{P}_{C}=\mathbb{P}_{\widetilde{\mu},\pi}(D)$, where $\widetilde{\mu}$
    is a function internal to the agent, of the same form of $\mu$. These are also
    mild assumptions as they correspond to, respectively, offline settings, online
    settings, and model-based settings where the model is learned. We detail these
    settings in Appendix [B](#S2a "B Further details on contexts ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning"). We now define the CAP formally.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步假设代理可以访问一组经验 $\mathcal{D}$，并且可以根据分布 $D\sim\mathbb{P}_{C}$ 从中抽样。这可以是一个预先编译的外部演示集，其中
    $\mathbb{P}_{C}(D)=\mathcal{U}(D)$，也可以是一个MDP，其中 $\mathbb{P}_{C}=\mathbb{P}_{\mu,\pi}(D)$，甚至是一个虚拟的MDP模型
    $\mathbb{P}_{C}=\mathbb{P}_{\widetilde{\mu},\pi}(D)$，其中 $\widetilde{\mu}$ 是一个与
    $\mu$ 形式相同的内部函数。这些也是温和的假设，因为它们分别对应于离线设置、在线设置和模型学习的基于模型的设置。我们在附录 [B](#S2a "B 进一步的背景细节
    ‣ 深度强化学习中时间信用分配的调查")中详细说明了这些设置。我们现在正式定义CAP。
- en: Definition 3  (The credit assignment problem).
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3（信用分配问题）。
- en: 'Consider an MDP $\mathcal{M}$, a goal $g\in\mathcal{G}$, and a set of experience
    $c\in\mathcal{C}$. Consider an arbitrary assignment $K\in\mathcal{K}$ as described
    in Equation ([4](#S4.E4 "In Definition 2 (Assignment). ‣ 4.3 What is an assignment?
    ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning")). Given a parameterised function $\widetilde{K}:\mathcal{C}\times\mathcal{A}\times\mathcal{G}\times\Phi\rightarrow\mathbb{R}$
    with parameters set $\phi\in\Phi\subset\mathcal{R}^{n}$, we refer to the *Credit
    Assignment Problem* as the problem of finding the set of parameters $\phi\in\Phi$
    such that:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个MDP $\mathcal{M}$、一个目标 $g\in\mathcal{G}$ 和一组经验 $c\in\mathcal{C}$。考虑一个如公式 ([4](#S4.E4
    "在定义2（分配）。 ‣ 4.3 什么是分配？ ‣ 4 量化动作影响 ‣ 深度强化学习中时间信用分配的调查"))中所述的任意分配 $K\in\mathcal{K}$。给定一个参数化函数
    $\widetilde{K}:\mathcal{C}\times\mathcal{A}\times\mathcal{G}\times\Phi\rightarrow\mathbb{R}$
    和参数集 $\phi\in\Phi\subset\mathcal{R}^{n}$，我们将*信用分配问题*称为寻找参数集 $\phi\in\Phi$ 的问题，使得：
- en: '|  | $\displaystyle\widetilde{K}(c,a,g,\phi)=K(c,a,g),\quad\forall s\in\mathcal{S},\forall
    a\in\mathcal{A}.$ |  | (5) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widetilde{K}(c,a,g,\phi)=K(c,a,g),\quad\forall s\in\mathcal{S},\forall
    a\in\mathcal{A}.$ |  | (5) |'
- en: 'Different choices of action influence have a great impact on the hardness of
    the problem. In particular, there is a trade-off between:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的动作影响选择对问题的难度有很大影响。特别是，在以下之间存在权衡：
- en: (a)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: how effective the chosen measure of influence is to inform the direction of
    the policy improvement,
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择的影响度量在告知政策改进方向上的有效性如何，
- en: (b)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: how easy it is to learn that function from experience.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从经验中学习这个函数有多么简单。
- en: For example, using causal influence (Janzing et al.,, [2013](#bib.bib79)) as
    a measure of action influence makes the CAP hard to solve in practice. The reason
    is that discovering causal mechanisms from associations alone is notoriously challenging
    (Pearl,, [2009](#bib.bib131); Bareinboim et al.,, [2022](#bib.bib17)) and pure
    causal relationships are rarely observed in nature (Pearl et al.,, [2000](#bib.bib132))
    but in specific experimental conditions. However, causal knowledge is reliable,
    robust to changes in the experience collected and effective, and causal mechanisms
    can be invariant to changes in the goal. On the contrary, $q$-values are easier
    to learn as they represent a measure of statistical correlation between state-actions
    and outcomes, but their knowledge is limited to the bare minimum necessary to
    solve a control problem. Which quantity to use in each specific instance or each
    specific problem is still subject of investigation in the literature. Ideally,
    we should aim for the most effective measure of influence that can be learned
    with the least amount of experience.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用因果影响（Janzing 等，[2013](#bib.bib79)）作为行动影响的度量会使CAP在实际应用中很难解决。原因在于仅通过关联发现因果机制
    notoriously challenging（Pearl，[2009](#bib.bib131)；Bareinboim 等，[2022](#bib.bib17)），而纯粹的因果关系在自然界中很少观察到（Pearl
    等，[2000](#bib.bib132)），而且通常是在特定实验条件下。然而，因果知识是可靠的，对经验变化具有鲁棒性，并且有效，因果机制可以对目标变化保持不变。相反，$q$-值更容易学习，因为它们表示状态-行动与结果之间的统计相关度量，但它们的知识仅限于解决控制问题所需的最低限度。每个具体实例或每个具体问题中使用哪种量仍然是文献中的研究课题。理想情况下，我们应该寻求最有效的影响度量，该度量能够以最少的经验进行学习。
- en: 4.5 Existing assignment functions
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 现有任务函数
- en: Assignment Action influence Context Action Goal State-action-value $q^{\pi}(s,a)$
    $s\in\mathcal{S}$ $a\in\mathcal{A}$ $g\in\mathbb{R}$ Advantage $q^{\pi}(s,a)-v(s)$
    $s\in\mathcal{S}$ $a\in\mathcal{A}$ $g\in\mathbb{R}$ General $q$-value function
    $q^{\pi}(s,a,g)$ $s\in\mathcal{S}$ $a\in\mathcal{A}$ $g\in\mathcal{S}$ Distributional
    action-value $Q^{\pi}(s,a)$ $s\in\mathcal{S}$ $a\in\mathcal{A}$ $g\in\{0,\ldots,n\}$
    Distributional advantage $D_{KL}(Q^{\pi}(s,a)||V^{\pi}(s,a))$ $s\in\mathcal{S}$
    $a\in\mathcal{A}$ $g\in\{0,\ldots,n\}$ Hindsight advantage $1-\frac{\pi(A_{t}|s)}{\mathbb{P}_{D}(A_{t}|s_{t},Z_{t})}Z_{t}$
    $s\in\mathcal{S},h_{T}\in\mathcal{H}$ $a\in h$ $g\in\mathbb{R}$ Counterfactual
    advantage $\mathbb{P}_{D}(A_{t}=a|S_{t}=s,F_{t}=f)q(s,a,f)$ $s\in\mathcal{S}$
    $a\in h$ $g\in\mathbb{R}$ Posterior value $\sum_{t=0}^{T}\mathbb{P}_{\mu,\pi}(U_{t}=u|h_{t})v^{\pi}(o_{t},x_{t})$
    $o\in\mathcal{O},b\in\mathbb{R}^{d},\pi$ $A\sim\pi$ $g\in\mathbb{R}$ Policy-conditioned
    value $q(s,a,\pi)$ $s\in\mathcal{S},\pi\in\Pi$ $a\in\mathcal{A}$ $g\in\mathbb{R}$
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 任务行为影响 上下文 行动 目标 状态-行动-值 $q^{\pi}(s,a)$ $s\in\mathcal{S}$ $a\in\mathcal{A}$
    $g\in\mathbb{R}$ 优势 $q^{\pi}(s,a)-v(s)$ $s\in\mathcal{S}$ $a\in\mathcal{A}$ $g\in\mathbb{R}$
    一般 $q$-值函数 $q^{\pi}(s,a,g)$ $s\in\mathcal{S}$ $a\in\mathcal{A}$ $g\in\mathcal{S}$
    分布式行动值 $Q^{\pi}(s,a)$ $s\in\mathcal{S}$ $a\in\mathcal{A}$ $g\in\{0,\ldots,n\}$
    分布式优势 $D_{KL}(Q^{\pi}(s,a)||V^{\pi}(s,a))$ $s\in\mathcal{S}$ $a\in\mathcal{A}$
    $g\in\{0,\ldots,n\}$ 事后优势 $1-\frac{\pi(A_{t}|s)}{\mathbb{P}_{D}(A_{t}|s_{t},Z_{t})}Z_{t}$
    $s\in\mathcal{S},h_{T}\in\mathcal{H}$ $a\in h$ $g\in\mathbb{R}$ 反事实优势 $\mathbb{P}_{D}(A_{t}=a|S_{t}=s,F_{t}=f)q(s,a,f)$
    $s\in\mathcal{S}$ $a\in h$ $g\in\mathbb{R}$ 后验值 $\sum_{t=0}^{T}\mathbb{P}_{\mu,\pi}(U_{t}=u|h_{t})v^{\pi}(o_{t},x_{t})$
    $o\in\mathcal{O},b\in\mathbb{R}^{d},\pi$ $A\sim\pi$ $g\in\mathbb{R}$ 策略条件值 $q(s,a,\pi)$
    $s\in\mathcal{S},\pi\in\Pi$ $a\in\mathcal{A}$ $g\in\mathbb{R}$
- en: 'Table 2: A list of the most common action-*influences* and their assignment
    functions in the Deep RL literature analysed in this survey. For each function,
    the table specifies the influence, the context representation, the action, and
    the goal representation of the corresponding assignment function $K\in\mathcal{K}$.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：深度强化学习文献中最常见的行动-*影响*及其任务函数的列表。对于每个函数，表格指定了相应任务函数$K\in\mathcal{K}$的影响、上下文表示、行动和目标表示。
- en: We now survey the most important assignment functions from the literature and
    their corresponding measure of action influence. The following list is not exhaustive,
    but rather it is representative of the limitations of existing credit formalisms.
    For brevity, and without loss of generality, we omit functions that do not explicitly
    evaluate actions (for example, state-values), but we notice that it is still possible
    to reinterpret an assignment to a state as an assignment to a set of actions for
    it affects all the actions that led to that state.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在调查文献中最重要的赋值函数及其对应的行动影响度量。以下列表并非详尽无遗，而是代表了现有信用形式的局限性。为了简洁起见，且不失一般性，我们省略了未明确评估行动的函数（例如，状态值），但我们注意到，仍然可以将对状态的赋值重新解释为对一组行动的赋值，因为它影响到所有导致该状态的行动。
- en: State-action values
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 状态-行动值
- en: '(Shannon,, [1950](#bib.bib164); Schultz,, [1967](#bib.bib161); Michie,, [1963](#bib.bib113);
    Watkins,, [1989](#bib.bib205)) are a hallmark of RL, and are described by the
    following expression:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: (Shannon,, [1950](#bib.bib164); Schultz,, [1967](#bib.bib161); Michie,, [1963](#bib.bib113);
    Watkins,, [1989](#bib.bib205)) 是强化学习的一个标志，描述如下表达式：
- en: '|  | $\displaystyle q^{\pi}(s,a)$ | $\displaystyle=\mathbb{E}_{\mu,\pi}[Z_{t}&#124;S_{t}=s,A_{t}=a].$
    |  | (6) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q^{\pi}(s,a)$ | $\displaystyle=\mathbb{E}_{\mu,\pi}[Z_{t}\mid
    S_{t}=s,A_{t}=a].$ |  | (6) |'
- en: 'Here, the context $c$ is a state $s\in\mathcal{S}$ in the case of MDPs or a
    history $h\in\mathcal{H}$ for a POMDP. The $q$-function quantifies the credit
    of an action by the expected return of the action in the context. $q$-values are
    among the simplest way to quantify credit and offer a basic mechanism to solve
    control problems. However, while $q$-functions offer solid theoretical guarantees
    in tabular RL, they can be unstable in Deep RL. When paired with bootstrapping
    and off-policy learning, q-values are well known to diverge from the optimal solution
    (Sutton and Barto,, [2018](#bib.bib181)). van Hasselt et al., ([2018](#bib.bib188))
    provides empirical evidence of the phenomenon, investigating the relationship
    between divergence and performance, and how different variables affect divergence.
    In particular, the work showed that the Deep Q-Network (DQN) (Mnih et al.,, [2015](#bib.bib118))
    is not guaranteed to converge to the optimal $q$-function. The divergence rate
    on both evaluation and control problems increases depending on specific mechanisms,
    such as the amount of bootstrapping, or the amount of prioritisation of updates
    ([Schaul et al., 2015b,](#bib.bib154) ). An additional problem arises in GPI schemes
    used to solve control problems. While during evaluation the policy is fixed, here
    the policy continuously changes, and it becomes more challenging to track the
    target of the update while converging to it, as the change of policy make the
    problem appear non-stationary from the point of view of the value estimation.
    This is because the policy changes, but there is no signal that informs the policy
    evaluation about the change. To mitigate the issue, many methods either use a
    fixed network as an evaluation target (Mnih et al.,, [2015](#bib.bib118)), they
    perform Polyak averaging of the target network (Haarnoja et al.,, [2018](#bib.bib64)),
    or they clip the gradient update to a maximum cap (Schulman et al.,, [2017](#bib.bib160)).
    To further support the idea, theoretical and empirical evidence (Bellemare et al.,,
    [2016](#bib.bib27)) shows that the $q$-function is inconsistent: for any suboptimal
    action $a$, the optimal value function $q^{*}(s,a)$ describes the value of a non-stationary
    policy, which selects a different action $\pi^{*}(s)$ (rather than $a$) at each
    visit of $s$. The inconsistency of $q$-values for suboptimal actions has also
    been shown empirically. Schaul et al., ([2022](#bib.bib152)) measured the per-state
    policy change $W(\pi,\pi^{\prime}|s)=\sum_{a\in\mathcal{A}}|\pi(a|s)-\pi^{\prime}(a|s)|$
    for several Atari 2600 games Arcade Learning Environment (ALE) (Bellemare et al.,,
    [2013](#bib.bib26)), and showed that the action-gap (the difference of value between
    the best and the second best action) undergoes brutal changes despite the agent
    maintaining a constant value of expected returns. In practice, Deep RL algorithms
    often use $q$-targets to approximate the $q$-value, for example, $n$-step targets
    (Sutton and Barto,, [2018](#bib.bib181), Chapter 7), or $\lambda$-returns (Watkins,,
    [1989](#bib.bib205); Jaakkola et al.,, [1993](#bib.bib76); Sutton and Barto,,
    [2018](#bib.bib181), Chapter 12). However, we consider them as *methods*, rather
    than quantities to measure credit, since the $q$-value is the quantity to which
    the function approximator converges. For this reason we discuss them in Section [6.1](#S6.SS1
    "6.1 Time as a heuristic ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning").'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，背景 $c$ 是 MDPs 中的状态 $s\in\mathcal{S}$ 或 POMDP 中的历史 $h\in\mathcal{H}$。$q$-函数通过在背景下的预期回报来量化一个动作的信用。$q$-值是量化信用的最简单方法之一，并提供了解决控制问题的基本机制。然而，尽管
    $q$-函数在表格型强化学习中提供了坚实的理论保证，它们在深度强化学习中可能会不稳定。当与自举和离策略学习结合时，$q$-值众所周知会偏离最优解（Sutton
    和 Barto，[2018](#bib.bib181)）。van Hasselt 等人（[2018](#bib.bib188)）提供了这种现象的实证证据，研究了偏离与性能之间的关系，以及不同变量如何影响偏离。特别是，研究表明深度
    Q 网络（DQN）（Mnih 等人，[2015](#bib.bib118)）并不保证收敛到最优 $q$-函数。无论是在评估还是控制问题中，偏离率都会根据特定机制的不同而增加，例如自举的数量或更新的优先级（[Schaul
    等人，2015b](#bib.bib154)）。在用于解决控制问题的 GPI 方案中，还会出现一个额外的问题。虽然在评估过程中策略是固定的，但在这里策略会不断变化，而在收敛到目标时，跟踪更新的目标变得更加困难，因为策略的变化使问题在价值估计的角度看起来像是非平稳的。这是因为策略发生了变化，但没有信号通知策略评估变化。为了缓解这一问题，许多方法要么使用固定网络作为评估目标（Mnih
    等人，[2015](#bib.bib118)），要么对目标网络执行 Polyak 平均（Haarnoja 等人，[2018](#bib.bib64)），或将梯度更新剪辑到最大值（Schulman
    等人，[2017](#bib.bib160)）。进一步支持这一观点的理论和实证证据（Bellemare 等人，[2016](#bib.bib27)）表明 $q$-函数是不一致的：对于任何次优动作
    $a$，最优值函数 $q^{*}(s,a)$ 描述了一个非平稳策略的价值，该策略在每次访问 $s$ 时选择不同的动作 $\pi^{*}(s)$（而不是 $a$）。次优动作的
    $q$-值的不一致性也在实证中得到了证明。Schaul 等人（[2022](#bib.bib152)）测量了多个 Atari 2600 游戏 Arcade
    Learning Environment (ALE)（Bellemare 等人，[2013](#bib.bib26)）的每状态策略变化 $W(\pi,\pi^{\prime}|s)=\sum_{a\in\mathcal{A}}|\pi(a|s)-\pi^{\prime}(a|s)|$，并显示尽管代理保持了预期回报的恒定值，动作间隙（最佳动作和第二佳动作之间的价值差异）却经历了剧烈变化。在实践中，深度强化学习算法通常使用
    $q$-目标来近似 $q$-值，例如，$n$-步目标（Sutton 和 Barto，[2018](#bib.bib181)，第 7 章），或 $\lambda$-返回（Watkins，[1989](#bib.bib205)；Jaakkola
    等人，[1993](#bib.bib76)；Sutton 和 Barto，[2018](#bib.bib181)，第 12 章）。然而，我们将它们视为 *方法*，而不是衡量信用的量，因为
    $q$-值是函数逼近器所收敛的量。基于此原因，我们在第 [6.1](#S6.SS1 "6.1 Time as a heuristic ‣ 6 Methods
    to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") 节中讨论它们。
- en: Advantage
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优势函数
- en: (Baird,, [1999](#bib.bib15)) measures, in a given state, the difference between
    the q-value of an action and the value of its state
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: (Baird,, [1999](#bib.bib15)) 测量在给定状态下，某个动作的 q-值与其状态值之间的差异
- en: '|  | $\displaystyle A^{\pi}(s,a)=q^{\pi}(s,a)-v^{\pi}(s).$ |  | (7) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A^{\pi}(s,a)=q^{\pi}(s,a)-v^{\pi}(s).$ |  | (7) |'
- en: Here, the context $c$ is the same as in Equation ([6](#S4.E6 "In State-action
    values ‣ 4.5 Existing assignment functions ‣ 4 Quantifying action influences ‣
    A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")). Because
    $v^{\pi}(s)=\sum_{a\in\mathcal{A}}q(s,a)\mathbb{P}_{\pi}(a)$ and $A^{\pi}(s,a)=q^{\pi}(s,a)-\mathbb{E}_{\pi}[q^{\pi}(s,a)]$,
    the advantage function measures action influence by the amount an action is better
    than average, that is, if $A^{\pi}(s,a)>0$. As also shown in Bellemare et al.,
    ([2016](#bib.bib27)), using the advantage to quantify credit can increase the
    action-gap, the value difference between the optimal and the second-best action.
    Empirical evidence has shown the consistent benefits of advantage over q-values
    (Baird,, [1999](#bib.bib15); [Wang et al., 2016b,](#bib.bib204) ; Bellemare et al.,,
    [2016](#bib.bib27); Schulman et al.,, [2016](#bib.bib159)), most probably due
    to its regularisation effects ([Vieillard et al., 2020b,](#bib.bib197) ; [Vieillard
    et al., 2020a,](#bib.bib196) ; [Ferret et al., 2021a,](#bib.bib46) ). On the other
    hand, when estimated directly and not by composing state and state-action values,
    for example in Pan et al., ([2022](#bib.bib128)), advantage does not permit bootstrapping.
    This is because advantage lacks an absolute measure of action influence, and only
    maintains one that is relative to the other possible actions. Overall, in canonical
    benchmarks for both evaluation ([Wang et al., 2016b,](#bib.bib204) ) and control
    (Bellemare et al.,, [2013](#bib.bib26)), advantage has been shown to improve over
    $q$-values ([Wang et al., 2016b,](#bib.bib204) ). In particular, policy evaluation
    experiences faster convergence in large action spaces because the state-value
    $v^{\pi}(s)$ can hold information that is shared between multiple actions. For
    control, it improves the score over several Atari 2600 games compared to both
    double $q$-learning (van Hasselt et al.,, [2016](#bib.bib189)) and prioritised
    experience replay ([Schaul et al., 2015b,](#bib.bib154) ).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，上下文 $c$ 与公式 ([6](#S4.E6 "在状态-动作值 ‣ 4.5 现有的分配函数 ‣ 4 量化动作影响 ‣ 深度强化学习中的时间信用分配调研"))
    中相同。由于 $v^{\pi}(s)=\sum_{a\in\mathcal{A}}q(s,a)\mathbb{P}_{\pi}(a)$ 和 $A^{\pi}(s,a)=q^{\pi}(s,a)-\mathbb{E}_{\pi}[q^{\pi}(s,a)]$，优势函数通过动作比平均水平更好的程度来衡量动作影响，即如果
    $A^{\pi}(s,a)>0$。正如 Bellemare 等人 ([2016](#bib.bib27)) 所示，使用优势来量化信用可以增加动作差距，即最优动作与次优动作之间的价值差异。实证证据表明，优势相较于
    q-值有一致的好处 (Baird,, [1999](#bib.bib15); [Wang 等人, 2016b,](#bib.bib204) ; Bellemare
    等人,, [2016](#bib.bib27); Schulman 等人,, [2016](#bib.bib159))，这很可能归因于其正则化效果 ([Vieillard
    等人, 2020b,](#bib.bib197) ; [Vieillard 等人, 2020a,](#bib.bib196) ; [Ferret 等人, 2021a,](#bib.bib46)
    )。另一方面，当直接估计而非通过组合状态和值-动作值时，例如在 Pan 等人 ([2022](#bib.bib128)) 中，优势不允许引导。这是因为优势缺乏对动作影响的绝对测量，只保持相对于其他可能动作的相对测量。总体而言，在经典基准测试中，无论是评估
    ([Wang 等人, 2016b,](#bib.bib204)) 还是控制 (Bellemare 等人,, [2013](#bib.bib26))，优势都显示出相较于
    $q$-值的改进 ([Wang 等人, 2016b,](#bib.bib204))。特别是在大动作空间中，策略评估因状态值 $v^{\pi}(s)$ 可以包含多个动作共享的信息而实现更快的收敛。对于控制，相较于双重
    $q$-学习 (van Hasselt 等人,, [2016](#bib.bib189)) 和优先经验回放 ([Schaul 等人, 2015b,](#bib.bib154))，在多个
    Atari 2600 游戏中的得分有所提高。
- en: GVFs
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GVFs
- en: '(Sutton et al.,, [2011](#bib.bib183); [Schaul et al., 2015a,](#bib.bib153)
    ) are a set of q-value functions that predict returns with respect to multiple
    reward functions:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: (Sutton 等人,, [2011](#bib.bib183); [Schaul 等人, 2015a,](#bib.bib153) ) 是一组 q-值函数，用于预测相对于多个奖励函数的回报：
- en: '|  | $\displaystyle q^{\pi,R}(s,a)=\{\forall R\in\mathcal{R}:\mathbb{E}_{\mu,\pi}\left[\sum_{t}^{T}R(S_{t},A_{t})&#124;S_{t}=s,A_{t}=a\right]\},$
    |  | (8) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q^{\pi,R}(s,a)=\{\forall R\in\mathcal{R}:\mathbb{E}_{\mu,\pi}\left[\sum_{t}^{T}R(S_{t},A_{t})\mid
    S_{t}=s,A_{t}=a\right]\},$ |  | (8) |'
- en: where $R$ is a pseudo-reward function and $\mathcal{R}$ is an arbitrary, pre-defined
    set of reward functions. Notice that we omit the pseudo-termination and pseudo-discounting
    terms that appear in their original formulation (Sutton et al.,, [2011](#bib.bib183))
    to maintain the focus on credit assignment. The context $c$ is the same of $q$-values
    and advantage, and the goal that the pseudo-reward represents is to reach a specific
    state $g=s\in\mathcal{S}$. When first introduced (Sutton et al.,, [2011](#bib.bib183)),
    the idea of GVFs stemmed from the observation that canonical value functions are
    limit to only a single task at the same time. Solving a new task would require
    learning a values function ex-novo. By maintaining multiple assignment functions
    at the same time, one for each goal, GVFs can instantly quantify the influence
    of an action with respect to multiple goals simultaneously. However, while GVFs
    maintain multiple assignments, the goal is still not an explicit input of the
    value function. Instead, it is left implicit and each assignment serves the ultimate
    goal to maximise a different pseudo-reward function (Sutton et al.,, [2011](#bib.bib183)).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R$ 是一个伪奖励函数，$\mathcal{R}$ 是一个任意的、预定义的奖励函数集合。注意我们省略了原始公式中出现的伪终止和伪折扣项（Sutton
    等，[2011](#bib.bib183)），以保持对信用分配的关注。上下文 $c$ 与 $q$-值和优势相同，伪奖励所代表的目标是达到一个特定的状态 $g=s\in\mathcal{S}$。在首次提出时（Sutton
    等，[2011](#bib.bib183)），GVF 的思想源于观察到经典值函数仅限于同时处理单一任务。解决新任务需要从头学习一个值函数。通过同时维护多个分配函数，每个目标一个，GVF
    可以即时量化一个动作对多个目标的影响。然而，尽管 GVF 保持了多个分配，目标仍然不是值函数的显式输入。相反，它被保留为隐式，每个分配服务于最大化不同的伪奖励函数（Sutton
    等，[2011](#bib.bib183)）。
- en: Universal Value Functions Approximators (UVFAs)
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通用值函数逼近器（UVFAs）
- en: '([Schaul et al., 2015a,](#bib.bib153) ) scale GVFs to Deep RL and advance their
    idea further by conflating these multiple assignment functions into a single one,
    represented as a deep neural network. Here, unlike for state-action values and
    GVFs, the goal is an explicit input of the assignment:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ([Schaul 等，2015a,](#bib.bib153)) 将 GVF 扩展到深度强化学习，并通过将这些多个分配函数合并为一个表示为深度神经网络的函数进一步推进他们的想法。在这里，与状态-动作值和
    GVF 不同，目标是分配的显式输入：
- en: '|  | $\displaystyle q^{\pi}(s,a,g)=\mathbb{E}_{\mu,\pi}[Z_{t}&#124;S_{t}=s,A_{t}=a,G_{t}=g],$
    |  | (9) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q^{\pi}(s,a,g)=\mathbb{E}_{\mu,\pi}[Z_{t}|S_{t}=s,A_{t}=a,G_{t}=g],$
    |  | (9) |'
- en: The action influence here is measured with respect to a goal explicitly. This
    allows to leverage the generalisation capacity of deep neural networks and to
    generalise not only over space of states but also over that of goals.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的动作影响是相对于一个明确的目标来衡量的。这使得可以利用深度神经网络的泛化能力，不仅在状态空间上进行泛化，也在目标空间上进行泛化。
- en: Distributional values
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分布值
- en: '(Jaquette,, [1973](#bib.bib80); Sobel,, [1982](#bib.bib171); White,, [1988](#bib.bib206);
    Bellemare et al.,, [2017](#bib.bib25)) consider the full return distribution $Z_{t}$
    instead of the expected value:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: (Jaquette，[1973](#bib.bib80); Sobel，[1982](#bib.bib171); White，[1988](#bib.bib206);
    Bellemare 等，[2017](#bib.bib25)) 考虑了完整的回报分布 $Z_{t}$ 而不是期望值：
- en: '|  | $\displaystyle Q^{\pi}(s,a)$ | $\displaystyle=\mathbb{P}_{\mu,\pi}(Z_{t}&#124;S_{t}=s,A_{t}=a),$
    |  | (10) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{\pi}(s,a)$ | $\displaystyle=\mathbb{P}_{\mu,\pi}(Z_{t}|S_{t}=s,A_{t}=a),$
    |  | (10) |'
- en: where $\mathbb{P}_{\mu,\pi}(Z_{t})$ is the probability of achieving a certain
    return $\mathbb{P}_{\mu,\pi}(Z_{t}=z)$ with $z\in\mathbb{R}$ and $Q^{\pi}(s,a)$
    maps a state-action pair to the distribution over returns. Notice that we use
    uppercase $Q$ to denote the value distribution and the lowercase $q$ for its expectation
    (Equation ([6](#S4.E6 "In State-action values ‣ 4.5 Existing assignment functions
    ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning"))). To translate the idea into a practical algorithm,
    Bellemare et al., ([2017](#bib.bib25)) proposes a discretised version of the value
    distribution by projecting $\mathbb{P}_{\mu,\pi}(Z_{t})$ on a finite support $\mathcal{C}=\{0\leq
    i\leq C\}$. The discretised value distribution then becomes $Q^{\pi}(s,a)=\mathbb{P}_{C}(Z_{t}|S_{t}=s,A_{t}=a)$,
    where $\mathbb{P}_{C}$ is a categorical Bernoulli denoting the quantised version
    of the value distribution $Z_{t}$ in the sample space $\mathcal{C}$, and describes
    the probability that a return $\mathbb{P}(Z_{t}=c),\forall c\in\mathcal{C}$ happens.
    Here, the context $c$ is the current MDP state and the goal is to achieve a policy
    that maximises the value distribution under the Wasserstein metric (Bellemare
    et al.,, [2017](#bib.bib25)). Notice that while the optimal expected value function
    $q^{*}(s,a)$ is unique, in general there are many optimal value distributions.
    Experimental evidence (Bellemare et al.,, [2017](#bib.bib25)) suggests that distributional
    values provide a better quantification of the action influence, leading to superior
    results in well known benchmarks for control (Bellemare et al.,, [2013](#bib.bib26)).
    However, it is yet not clear why distributional values improve over their expected
    counterparts. One hypothesis is that predicting for multiple goals works as an
    auxiliary task (Jaderberg et al.,, [2017](#bib.bib77)), which often lead to better
    performance. Another hypothesis, is that the distributional Bellman optimality
    operator proposed in Bellemare et al., ([2017](#bib.bib25)) produces a smoother
    optimisation problem, but the evidence remains weak or inconclusive (Sun et al.,,
    [2022](#bib.bib174)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbb{P}_{\mu,\pi}(Z_{t})$ 是实现某一回报 $\mathbb{P}_{\mu,\pi}(Z_{t}=z)$ 的概率，$z\in\mathbb{R}$，$Q^{\pi}(s,a)$
    将状态-动作对映射到回报的分布。请注意，我们使用大写的 $Q$ 来表示值分布，而小写的 $q$ 表示其期望（方程 ([6](#S4.E6 "状态-动作值 ‣
    4.5 现有分配函数 ‣ 4 量化动作影响 ‣ 深度强化学习中的时间信用分配综述"))）。为了将这一理念转化为实际算法，Bellemare 等人（[2017](#bib.bib25)）提出了一种通过将
    $\mathbb{P}_{\mu,\pi}(Z_{t})$ 投影到有限支持 $\mathcal{C}=\{0\leq i\leq C\}$ 上的离散化版本。离散化后的值分布变为
    $Q^{\pi}(s,a)=\mathbb{P}_{C}(Z_{t}|S_{t}=s,A_{t}=a)$，其中 $\mathbb{P}_{C}$ 是一个分类伯努利分布，表示样本空间
    $\mathcal{C}$ 中的量化值分布 $Z_{t}$，并描述了回报 $\mathbb{P}(Z_{t}=c),\forall c\in\mathcal{C}$
    发生的概率。在这里，背景 $c$ 是当前的 MDP 状态，目标是实现一个在 Wasserstein 度量下最大化值分布的策略（Bellemare 等人，[2017](#bib.bib25)）。请注意，虽然最优期望值函数
    $q^{*}(s,a)$ 是唯一的，但一般来说，存在许多最优值分布。实验证据（Bellemare 等人，[2017](#bib.bib25)）表明，分布值提供了更好的动作影响量化，从而在控制的知名基准测试中取得了优越的结果（Bellemare
    等人，[2013](#bib.bib26)）。然而，尚不清楚为什么分布值会优于其期望对应物。一个假设是，为多个目标进行预测作为辅助任务（Jaderberg
    等人，[2017](#bib.bib77)），这通常会带来更好的性能。另一个假设是，Bellemare 等人（[2017](#bib.bib25)）提出的分布贝尔曼最优性算子产生了一个更平滑的优化问题，但证据仍然薄弱或不确定（Sun
    等人，[2022](#bib.bib174)）。
- en: Distributional advantage
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分布优势
- en: '(Arumugam et al.,, [2021](#bib.bib10)) proposes a probabilistic equivalent
    of the expected advantage:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: （Arumugam 等人，[2021](#bib.bib10)）提出了一种期望优势的概率等效表示：
- en: '|  | $\displaystyle A^{\pi}(s,a)=D_{KL}(Q^{\pi}(s,a)&#124;&#124;V^{\pi}(s)),$
    |  | (11) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A^{\pi}(s,a)=D_{KL}(Q^{\pi}(s,a)&#124;&#124;V^{\pi}(s)),$
    |  | (11) |'
- en: and borrows the properties of both distributional values and the expected advantage.
    Intuitively, Equation ([11](#S4.E11 "In Distributional advantage ‣ 4.5 Existing
    assignment functions ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")) measures how much the value
    distribution for a given state-action pair changes relative to the distribution
    for the particular state only, marginalising over all actions. The relationship
    between the two distributions can then be interpreted as the distributional analogue
    of Equation ([7](#S4.E7 "In Advantage ‣ 4.5 Existing assignment functions ‣ 4
    Quantifying action influences ‣ A Survey of Temporal Credit Assignment in Deep
    Reinforcement Learning")), where the two quantities appear in their expectation
    instead. The biggest drawback of this measure of action influence is that it is
    only treated in theory and there is no empirical evidence that supports distributional
    advantage as a useful proxy for credit in practice.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 并借鉴了分布值和期望优势的属性。直观地，方程 ([11](#S4.E11 "在分布优势 ‣ 4.5 现有分配函数 ‣ 4 量化行动影响 ‣ 深度强化学习中的时间信用分配调查"))
    测量了对于给定的状态-行动对，值分布相对于仅特定状态的分布的变化，边际化所有行动。然后可以将这两个分布之间的关系解释为方程 ([7](#S4.E7 "在优势
    ‣ 4.5 现有分配函数 ‣ 4 量化行动影响 ‣ 深度强化学习中的时间信用分配调查")) 的分布类比，其中两个量的期望值取代了原来的量。这种行动影响的度量最大的问题是它仅在理论中处理，缺乏支持分布优势作为实际信用代理的经验证据。
- en: Hindsight advantage
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 事后优势
- en: '(Harutyunyan et al.,, [2019](#bib.bib67)) stems from conditioning the action
    influence on future states or returns. The return-conditional hindsight advantage
    function can be written as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: （Harutyunyan 等人，[2019](#bib.bib67)）源于对未来状态或回报的行动影响的条件化。回报条件的事后优势函数可以写作如下：
- en: '|  | $\displaystyle A^{\pi}(s,a,z)=1-\frac{\mathbb{P}_{\pi}(A_{t}=a&#124;S_{t}=s)}{\mathbb{P}_{D}(A_{t}=a&#124;S_{t}=s,Z_{t}=z)}z.$
    |  | (12) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A^{\pi}(s,a,z)=1-\frac{\mathbb{P}_{\pi}(A_{t}=a&#124;S_{t}=s)}{\mathbb{P}_{D}(A_{t}=a&#124;S_{t}=s,Z_{t}=z)}z.$
    |  | (12) |'
- en: Here $A^{\pi}(s,a,z)$ denotes the return-conditional advantage and $\mathbb{P}_{D}(a_{t}|S_{t}=s,Z_{t}=z)$
    is the return-conditional hindsight distribution that describes the probability
    to take action $a$ in $s$, given than we observe return $z$ at the end of the
    episode. The context $c$ is the same for $q$-values and advantage and the goal
    is a specific value of the return $Z=z$. The idea of hindsight – initially presented
    in Andrychowicz et al., ([2017](#bib.bib5)) – is that even if the trajectory does
    not provide useful information for the main goal, it can be revisited as if the
    goal was the outcomes just achieved. Hindsight advantage brings this idea to the
    extreme and rather than evaluating only for a pre-defined set of goals such as
    in Andrychowicz et al., ([2017](#bib.bib5)), it evaluates for every experienced
    state or return. Here, the action influence is quantified by that proportion of
    return determined by the ratio in Equation ([12](#S4.E12 "In Hindsight advantage
    ‣ 4.5 Existing assignment functions ‣ 4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning")). To develop an
    intuition of it, if the action $a$ leads to the return $z$ with probability $1$
    such that $\mathbb{P}_{D}(A_{t}=a|S_{t}=s,Z_{t}=z)=1$, but the behaviour policy
    $\pi$ takes $a$ with probability $0$, the credit of the action $a$ is $0$. There
    exist also a state-conditional formulation rather than a return-conditional one,
    and we refer to Harutyunyan et al., ([2019](#bib.bib67)) for details on it to
    keep the description concise.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $A^{\pi}(s,a,z)$ 表示回报条件的优势，$\mathbb{P}_{D}(a_{t}|S_{t}=s,Z_{t}=z)$ 是回报条件的事后分布，描述了在我们观察到在回合结束时的回报
    $z$ 的情况下，在 $s$ 中采取行动 $a$ 的概率。上下文 $c$ 对 $q$-值和优势是相同的，目标是回报 $Z=z$ 的特定值。事后思维的想法——最初由
    Andrychowicz 等人 ([2017](#bib.bib5)) 提出——是，即使轨迹未提供对主要目标有用的信息，它也可以被重新审视，好像目标是刚刚实现的结果一样。事后优势将这一思想推向极端，而不是仅对预定义的目标集进行评估（如
    Andrychowicz 等人 ([2017](#bib.bib5))），它对每个经历的状态或回报进行评估。这里，行动影响通过方程 ([12](#S4.E12
    "在事后优势 ‣ 4.5 现有分配函数 ‣ 4 量化行动影响 ‣ 深度强化学习中的时间信用分配调查")) 中的比例来量化。为了形成直观认识，如果行动 $a$
    以概率 $1$ 导致回报 $z$，即 $\mathbb{P}_{D}(A_{t}=a|S_{t}=s,Z_{t}=z)=1$，但行为策略 $\pi$ 以概率
    $0$ 采取 $a$，则行动 $a$ 的信用为 $0$。也存在状态条件的表述，而不是回报条件的表述，详细信息请参见 Harutyunyan 等人 ([2019](#bib.bib67))，以保持描述的简洁。
- en: Future-conditional advantage
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 未来条件优势
- en: '(Mesnard et al.,, [2021](#bib.bib112)) generalises hindsight advantage to use
    an arbitrary property of the future:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: (Mesnard et al., [2021](#bib.bib112)) 将回顾优势推广到使用未来的任意属性：
- en: '|  | $\displaystyle A^{\pi}(s,a,f)=\mathbb{P}_{D}(A_{t}=a&#124;S_{t}=s,F_{t}=f)q(s,a,f),$
    |  | (13) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A^{\pi}(s,a,f)=\mathbb{P}_{D}(A_{t}=a \mid S_{t}=s,F_{t}=f)q(s,a,f),$
    |  | (13) |'
- en: with $F_{t}=\psi(d_{t})$ being some random property of the future trajectory
    $d_{t}$ that starts at time $t$ and ends at the random horizon $T$; $q(s,a,f)=\mathbb{E}_{\mu,\pi}[Z_{t}|S_{t}=s,F_{t}=f,A_{t}=a]$
    denotes the future-conditioned state-action value function. Notice that you can
    derive the hindsight advantage by setting $\psi=z$.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $F_{t}=\psi(d_{t})$ 是未来轨迹 $d_{t}$ 的某种随机属性，该轨迹从时间 $t$ 开始，到随机终点 $T$ 结束；$q(s,a,f)=\mathbb{E}_{\mu,\pi}[Z_{t}|S_{t}=s,F_{t}=f,A_{t}=a]$
    表示未来条件下的状态-动作价值函数。注意，你可以通过设置 $\psi=z$ 来推导回顾优势。
- en: Counterfactual advantage
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 反事实优势
- en: '(Mesnard et al.,, [2021](#bib.bib112)) proposes a specific choice of $F$ such
    that $F$ is independent from the current action. This produces a future-conditional
    advantage that factorises the influence of an action in two components: the contribution
    deriving from the intervention itself (the action) and the luck represented by
    all the components not under control of the agent at the time $t$, such as fortuitous
    outcomes of the state-transition dynamics, exogenous reward noise, or future actions.
    The form is the same that in Equation [13](#S4.E13 "In Future-conditional advantage
    ‣ 4.5 Existing assignment functions ‣ 4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning"), with the additional
    condition that $A_{t}\perp F_{t}$ and we have $\mathbb{E}_{U}[D_{KL}(\mathbb{P}(A_{t}|S_{t}=s)||\mathbb{P}(A_{t}|S_{t}=s,F_{t}=f)]=0$.
    The main intuition behind counterfactual advantage is the following. While to
    compute counterfactuals we need access to a model of the environment, in model-free
    settings we can still compute all the relevant information $u$ that does not depend
    on this model. Once learned, a model of $u$ can then represents a valid baseline
    to compute counterfactuals in a model-free way. To stay in the scope of this section,
    we detail how to learn this quantity in Section [6.4](#S6.SS4 "6.4 Conditioning
    in hindsight ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning").'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: (Mesnard et al., [2021](#bib.bib112)) 提出了一个特定的 $F$ 选择，使得 $F$ 与当前动作独立。这产生了一种未来条件优势，将动作的影响分解为两个组件：来自干预本身（动作）的贡献和在时间
    $t$ 时不受代理控制的所有组件所代表的运气，例如状态转移动态的偶然结果、外生奖励噪声或未来动作。其形式与方程 [13](#S4.E13 "In Future-conditional
    advantage ‣ 4.5 Existing assignment functions ‣ 4 Quantifying action influences
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning") 相同，附加条件是
    $A_{t}\perp F_{t}$ 且我们有 $\mathbb{E}_{U}[D_{KL}(\mathbb{P}(A_{t}|S_{t}=s)||\mathbb{P}(A_{t}|S_{t}=s,F_{t}=f)]=0$。反事实优势的主要直觉如下。虽然计算反事实需要访问环境模型，但在无模型设置中，我们仍然可以计算所有不依赖于此模型的相关信息
    $u$。一旦学习到，$u$ 的模型可以作为计算反事实的有效基准，以无模型的方式进行。为了保持在本节的范围内，我们在第 [6.4](#S6.SS4 "6.4
    Conditioning in hindsight ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning") 节详细说明了如何学习这个量。
- en: Posterior value functions
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 后验价值函数
- en: '(Nota et al.,, [2021](#bib.bib124)) reflects on partial-observability and proposes
    a characterisation of hindsight advantage bespoke to POMDPs. The intuition behind
    Posterior Value Functions (PVFs) is that the evaluated action accounts only for
    a small portion of the variance of returns. The majority of the variance is often
    due to the part of the trajectory that still has to happen. For this reason, incorporating
    in the baseline information usually unavailable after the time when the action
    was taken could have a greater impact in reducing the variance of the policy gradient
    estimator. PVFs focus on the variance of a future-conditional baseline (Mesnard
    et al.,, [2021](#bib.bib112)) caused by the partial observability. Nota et al.,
    ([2021](#bib.bib124)) factorises a state $s$ into an observable component $o$
    and an non-observable one $u$, and formalises the PVF as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: (Nota et al., [2021](#bib.bib124)) 反映了部分可观测性，并提出了针对 POMDPs 的回顾优势的特征化。后验价值函数（PVFs）的直觉是，评估的动作只占回报方差的一小部分。大部分方差通常是由于轨迹中仍需发生的部分。因此，结合在动作采取后通常不可用的基线信息，可能对减少策略梯度估计器的方差有更大的影响。PVFs
    关注由于部分可观测性导致的未来条件基线（Mesnard et al., [2021](#bib.bib112)）的方差。Nota et al., ([2021](#bib.bib124))
    将状态 $s$ 分解为一个可观测组件 $o$ 和一个不可观测组件 $u$，并将 PVF 形式化如下：
- en: '|  | $\displaystyle v_{t}^{\pi}(h_{t})=\sum_{u\in\mathcal{U}}\mathbb{P}_{\mu,\pi}(U_{t}=u&#124;h_{t})v^{\pi}(o_{t},u_{t})$
    |  | (14) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle v_{t}^{\pi}(h_{t})=\sum_{u\in\mathcal{U}}\mathbb{P}_{\mu,\pi}(U_{t}=u&#124;h_{t})v^{\pi}(o_{t},u_{t})$
    |  | (14) |'
- en: 'where $u\in\mathcal{U}$ is the non-observable component of $s_{t}$ such that
    $s=\{u,o\}$. Notice that this method is not taking actions into account. However,
    it is trivial to derive the corresponding Posterior Action-Value Function (PAVF)
    as: $q_{t}^{\pi}(h_{t},a)=R(s_{t},a_{t})+v_{t}^{\pi}(h_{t})$.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $u\in\mathcal{U}$ 是 $s_{t}$ 的不可观察部分，使得 $s=\{u,o\}$。请注意，这种方法没有考虑动作。然而，推导相应的后验动作值函数（PAVF）是简单的：$q_{t}^{\pi}(h_{t},a)=R(s_{t},a_{t})+v_{t}^{\pi}(h_{t})$。
- en: Policy-conditioned values
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于策略的值
- en: '(Harb et al.,, [2020](#bib.bib66); Faccio et al.,, [2021](#bib.bib43)) are
    value functions that include the policy as an input. For example, a policy-conditioned
    state-action value has the form:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: (Harb 等人，[2020](#bib.bib66)；Faccio 等人，[2021](#bib.bib43)) 是包括策略作为输入的值函数。例如，基于策略的状态-动作值具有以下形式：
- en: '|  | $\displaystyle q(s,\pi,a)=\mathbb{E}_{\mu,\pi}[Z_{t}&#124;S_{t}=s,\pi_{i}=\pi,A_{t}=a],$
    |  | (15) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q(s,\pi,a)=\mathbb{E}_{\mu,\pi}[Z_{t}&#124;S_{t}=s,\pi_{i}=\pi,A_{t}=a],$
    |  | (15) |'
- en: where $\pi_{i}\in\Pi$ denotes the policy from which to sample the actions that
    follow $A_{t}$. Here, the context $c$ is union of the current MDP state and the
    policy $\pi$, and the goal is to maximise the MDP return, which is unknown to
    the agent. The main difference with state-action values is that, all else being
    equal, $q(s,\pi,a,g)$ produces different values *instantly* when $\pi$ varies,
    since $\pi$ is now an explicit input, where $q^{\pi}(s,a,g)$ requires a full PE
    procedure instead. Using the policy as an input raises the problem of representing
    a policy in a way that can be fed to a neural network. Harb et al., ([2020](#bib.bib66))
    and Faccio et al., ([2021](#bib.bib43)) propose two methods to represent a policy.
    To keep our attention on the CAP, we refer to their works for further details
    on possible ways to represent a policy (Harb et al.,, [2020](#bib.bib66); Faccio
    et al.,, [2021](#bib.bib43)). Here we limit to convey that the problem of representing
    a policy has been already raised in the literature.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\pi_{i}\in\Pi$ 表示从中采样的策略，该策略决定了 $A_{t}$ 后续的动作。在这里，背景 $c$ 是当前 MDP 状态和策略 $\pi$
    的联合，目标是最大化 MDP 回报，而这一点对智能体来说是未知的。与状态-动作值的主要区别在于，所有其他条件相同的情况下，当 $\pi$ 变化时，$q(s,\pi,a,g)$
    会*立即*产生不同的值，因为 $\pi$ 现在是一个显式输入，而 $q^{\pi}(s,a,g)$ 则需要完整的 PE 过程。使用策略作为输入会引发一个问题，即如何以可以传递给神经网络的方式表示策略。Harb
    等人（[2020](#bib.bib66)）和 Faccio 等人（[2021](#bib.bib43)）提出了两种表示策略的方法。为了集中注意力在 CAP
    上，我们参阅了他们的工作以获取有关表示策略的进一步细节（Harb 等人，[2020](#bib.bib66)；Faccio 等人，[2021](#bib.bib43)）。在这里我们仅传达了在文献中已经提出了表示策略的问题。
- en: 4.6 Discussion
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 讨论
- en: Name Explicitness Recursivity Future-dependent Causality State-action value
    $\circ$ $\bullet$ $\circ$ $\circ$ Advantage $\circ$ $\bullet$ $\circ$ $\circ$
    GVFs/UVFAs $\bullet$ $\bullet$ $\circ$ $\circ$ Distributional action-value <math
    style="background-color:#F2F2F2;" alttext="\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{}
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 名称 明确性 递归性 未来依赖 因果关系 状态-动作值 $\circ$ $\bullet$ $\circ$ $\circ$ 优势 $\circ$ $\bullet$
    $\circ$ $\circ$ GVFs/UVFAs $\bullet$ $\bullet$ $\circ$ $\circ$ 分布式动作值 <math style="background-color:#F2F2F2;"
    alttext="\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{ \leavevmode\hbox
    to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower 0.0pt\hbox
    to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{}
- en: '{{}{{}}}{{}{}}{}{{}{}}'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '{{}{{}}}{{}{}}{}{{}{}}'
- en: '{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}}'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}}'
- en: '{}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}"
    display="inline"><semantics ><mtable rowspacing="0pt"  ><mtr ><mtd columnalign="left"
    ><mtext ><svg height="1" overflow="visible" version="1.1" width="6.92" ><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1
    0 0)"  ><g clip-path="url(#pgfcp1)" ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)"
    ><foreignobject height="6.15" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></mtd></mtr><mtr
    ><mtd columnalign="left" ><mo mathbackground="#FFFFFF" >∘</mo></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix ><matrixrow ><ci ><mtext ><svg height="1" overflow="visible"
    version="1.1" width="6.92" ><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,1) matrix(1 0 0 -1 0 0)"  ><g clip-path="url(#pgfcp1)"
    ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)" ><foreignobject height="6.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{} {{}{{}}}{{}{}}{}{{}{}} {{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}} {}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}</annotation></semantics></math>
    $\bullet$ $\circ$ $\circ$ Distributional advantage <math style="background-color:#FFFFFF;"
    alttext="\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{ \leavevmode\hbox
    to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower 0.0pt\hbox
    to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{}'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个复杂的数学表达式，包含一些符号和格式设定。由于该文本主要是数学标记和图形描述，翻译可能不适用。
- en: '{{}{{}}}{{}{}}{}{{}{}}'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '{{}{{}}}{{}{}}{}{{}{}}'
- en: '{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}}'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.'
- en: '{}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}"
    display="inline"><semantics ><mtable rowspacing="0pt"  ><mtr ><mtd columnalign="left"
    ><mtext ><svg height="1" overflow="visible" version="1.1" width="6.92" ><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1
    0 0)"  ><g clip-path="url(#pgfcp2)" ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)"
    ><foreignobject height="6.15" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></mtd></mtr><mtr
    ><mtd columnalign="left" ><mo mathbackground="#FFFFFF" >∘</mo></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix ><matrixrow ><ci ><mtext ><svg height="1" overflow="visible"
    version="1.1" width="6.92" ><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,1) matrix(1 0 0 -1 0 0)"  ><g clip-path="url(#pgfcp2)"
    ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)" ><foreignobject height="6.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{} {{}{{}}}{{}{}}{}{{}{}} {{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}} {}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}</annotation></semantics></math>
    $\circ$ $\circ$ $\bullet$ Hindsight advantage <math style="background-color:#F2F2F2;"
    alttext="\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{ \leavevmode\hbox
    to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower 0.0pt\hbox
    to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{}'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '{}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}"
    display="inline"><semantics ><mtable rowspacing="0pt"  ><mtr ><mtd columnalign="left"
    ><mtext ><svg height="1" overflow="visible" version="1.1" width="6.92" ><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1
    0 0)"  ><g clip-path="url(#pgfcp2)" ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)"
    ><foreignobject height="6.15" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></mtd></mtr><mtr
    ><mtd columnalign="left" ><mo mathbackground="#FFFFFF" >∘</mo></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix ><matrixrow ><ci ><mtext ><svg height="1" overflow="visible"
    version="1.1" width="6.92" ><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,1) matrix(1 0 0 -1 0 0)"  ><g clip-path="url(#pgfcp2)"
    ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)" ><foreignobject height="6.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{} {{}{{}}}{{}{}}{}{{}{}} {{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}} {}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}</annotation></semantics></math>
    $\circ$ $\circ$ $\bullet$ 回顾优势 <math style="background-color:#F2F2F2;" alttext="\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{} {{}{{}}}{{}{}}{}{{}{}} {{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}} {}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}</annotation></semantics></math>'
- en: '{{}{{}}}{{}{}}{}{{}{}}'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '{{}{{}}}{{}{}}{}{{}{}}'
- en: '{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}}'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}}'
- en: '{}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}"
    display="inline"><semantics ><mtable rowspacing="0pt"  ><mtr ><mtd columnalign="left"
    ><mtext ><svg height="1" overflow="visible" version="1.1" width="6.92" ><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1
    0 0)"  ><g clip-path="url(#pgfcp3)" ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)"
    ><foreignobject height="6.15" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></mtd></mtr><mtr
    ><mtd columnalign="left" ><mo mathbackground="#FFFFFF" >∘</mo></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix ><matrixrow ><ci ><mtext ><svg height="1" overflow="visible"
    version="1.1" width="6.92" ><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,1) matrix(1 0 0 -1 0 0)"  ><g clip-path="url(#pgfcp3)"
    ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)" ><foreignobject height="6.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{} {{}{{}}}{{}{}}{}{{}{}} {{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}} {}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}</annotation></semantics></math>
    $\circ$ <math   style="background-color:#F2F2F2;" alttext="\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{}'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '{}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}"
    display="inline"><semantics ><mtable rowspacing="0pt"  ><mtr ><mtd columnalign="left"
    ><mtext ><svg height="1" overflow="visible" version="1.1" width="6.92" ><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1
    0 0)"  ><g clip-path="url(#pgfcp3)" ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)"
    ><foreignobject height="6.15" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></mtd></mtr><mtr
    ><mtd columnalign="left" ><mo mathbackground="#FFFFFF" >∘</mo></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix ><matrixrow ><ci ><mtext ><svg height="1" overflow="visible"
    version="1.1" width="6.92" ><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,1) matrix(1 0 0 -1 0 0)"  ><g clip-path="url(#pgfcp3)"
    ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)" ><foreignobject height="6.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{} {{}{{}}}{{}{}}{}{{}{}} {{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}} {}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}</annotation></semantics></math>
    $\circ$ <math   style="background-color:#F2F2F2;" alttext="\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{}'
- en: '{{}{{}}}{{}{}}{}{{}{}}'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '{{}{{}}}{{}{}}{}{{}{}}'
- en: '{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}}'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}}'
- en: '{}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}"
    display="inline"><semantics ><mtable rowspacing="0pt"  ><mtr ><mtd columnalign="left"
    ><mtext ><svg height="1" overflow="visible" version="1.1" width="6.92" ><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1
    0 0)"  ><g clip-path="url(#pgfcp4)" ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)"
    ><foreignobject height="6.15" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></mtd></mtr><mtr
    ><mtd columnalign="left" ><mo mathbackground="#FFFFFF" >∘</mo></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix ><matrixrow ><ci ><mtext ><svg height="1" overflow="visible"
    version="1.1" width="6.92" ><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,1) matrix(1 0 0 -1 0 0)"  ><g clip-path="url(#pgfcp4)"
    ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)" ><foreignobject height="6.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{} {{}{{}}}{{}{}}{}{{}{}} {{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}} {}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}</annotation></semantics></math>
    $\circ$ Counterfactual advantage <math style="background-color:#FFFFFF;" alttext="\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{}'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: $\circ$ 反事实优势
- en: '{{}{{}}}{{}{}}{}{{}{}}'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '{{}{{}}}{{}{}}{}{{}{}}'
- en: '{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}}'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}}'
- en: '{}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}"
    display="inline"><semantics ><mtable rowspacing="0pt"  ><mtr ><mtd columnalign="left"
    ><mtext ><svg height="1" overflow="visible" version="1.1" width="6.92" ><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1
    0 0)"  ><g clip-path="url(#pgfcp5)" ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)"
    ><foreignobject height="6.15" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></mtd></mtr><mtr
    ><mtd columnalign="left" ><mo mathbackground="#FFFFFF" >∘</mo></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix ><matrixrow ><ci ><mtext ><svg height="1" overflow="visible"
    version="1.1" width="6.92" ><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,1) matrix(1 0 0 -1 0 0)"  ><g clip-path="url(#pgfcp5)"
    ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)" ><foreignobject height="6.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{} {{}{{}}}{{}{}}{}{{}{}} {{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}} {}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}</annotation></semantics></math>
    $\circ$ <math   style="background-color:#FFFFFF;" alttext="\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{}'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '{}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}"
    display="inline"><semantics ><mtable rowspacing="0pt"  ><mtr ><mtd columnalign="left"
    ><mtext ><svg height="1" overflow="visible" version="1.1" width="6.92" ><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1
    0 0)"  ><g clip-path="url(#pgfcp5)" ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)"
    ><foreignobject height="6.15" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></mtd></mtr><mtr
    ><mtd columnalign="left" ><mo mathbackground="#FFFFFF" >∘</mo></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix ><matrixrow ><ci ><mtext ><svg height="1" overflow="visible"
    version="1.1" width="6.92" ><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,1) matrix(1 0 0 -1 0 0)"  ><g clip-path="url(#pgfcp5)"
    ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)" ><foreignobject height="6.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{} {{}{{}}}{{}{}}{}{{}{}} {{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}} {}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}</annotation></semantics></math>
    $\circ$ <math   style="background-color:#FFFFFF;" alttext="\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{}'
- en: '{{}{{}}}{{}{}}{}{{}{}}'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '{{}{{}}}{{}{}}{}{{}{}}'
- en: '{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}}'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '{{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}}'
- en: '{}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}"
    display="inline"><semantics ><mtable rowspacing="0pt"  ><mtr ><mtd columnalign="left"
    ><mtext ><svg height="1" overflow="visible" version="1.1" width="6.92" ><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1
    0 0)"  ><g clip-path="url(#pgfcp6)" ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)"
    ><foreignobject height="6.15" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></mtd></mtr><mtr
    ><mtd columnalign="left" ><mo mathbackground="#FFFFFF" >∘</mo></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix ><matrixrow ><ci ><mtext ><svg height="1" overflow="visible"
    version="1.1" width="6.92" ><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,1) matrix(1 0 0 -1 0 0)"  ><g clip-path="url(#pgfcp6)"
    ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)" ><foreignobject height="6.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{} {{}{{}}}{{}{}}{}{{}{}} {{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}} {}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}</annotation></semantics></math>
    $\bullet$ Posterior value $\circ$ $\circ$ $\bullet$ $\circ$ Observation-action
    value $\circ$ $\circ$ $\circ$ $\circ$ Policy-conditioned value $\circ$ $\bullet$
    $\bullet$ $\circ$'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '{}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}"
    display="inline"><semantics ><mtable rowspacing="0pt"  ><mtr ><mtd columnalign="left"
    ><mtext ><svg height="1" overflow="visible" version="1.1" width="6.92" ><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1
    0 0)"  ><g clip-path="url(#pgfcp6)" ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)"
    ><foreignobject height="6.15" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></mtd></mtr><mtr
    ><mtd columnalign="left" ><mo mathbackground="#FFFFFF" >∘</mo></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix ><matrixrow ><ci ><mtext ><svg height="1" overflow="visible"
    version="1.1" width="6.92" ><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,1) matrix(1 0 0 -1 0 0)"  ><g clip-path="url(#pgfcp6)"
    ><g transform="matrix(1.0 0.0 0.0 1.0 0 0)" ><foreignobject height="6.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="6.92" >$\bullet$</foreignobject></g></g></g></svg></mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\mathbin{\vphantom{\circ}\text{\ooalign{\leavevmode\hbox{
    \leavevmode\hbox to5pt{\vbox to0pt{\pgfpicture\makeatletter\hbox{\hskip 0.0pt\lower
    0.0pt\hbox to0.0pt{\pgfsys@beginscope\pgfsys@invoke{ }\definecolor{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@rgb@stroke{0}{0}{0}\pgfsys@invoke{
    }\pgfsys@color@rgb@fill{0}{0}{0}\pgfsys@invoke{ }\pgfsys@setlinewidth{0.4pt}\pgfsys@invoke{
    }\nullfont\hbox to0.0pt{{}{}{}{}\pgfsys@moveto{0.0pt}{0.0pt}\pgfsys@lineto{0.0pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@lineto{5.00002pt}{0.0pt}\pgfsys@closepath\pgfsys@clipnext\pgfsys@discardpath\pgfsys@invoke{
    }{{{}{}{{}}{} {{}{{}}}{{}{}}{}{{}{}} {{{{}}\pgfsys@beginscope\pgfsys@invoke{ }\pgfsys@transformcm{1.0}{0.0}{0.0}{1.0}{0.0pt}{0.0pt}\pgfsys@invoke{
    }\hbox{{\leavevmode\hbox{\hskip 0.0pt\hbox{\set@color{$\bullet$}}}}}\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope}}}} {}{}{}{}\hss}\pgfsys@discardpath\pgfsys@invoke{\lxSVG@closescope
    }\pgfsys@endscope\hss}}\lxSVG@closescope\endpgfpicture}}}\cr$\circ$\cr}}}</annotation></semantics></math>
    $\bullet$ 后验值 $\circ$ $\circ$ $\bullet$ $\circ$ 观测-行动值 $\circ$ $\circ$ $\circ$
    $\circ$ 策略条件值 $\circ$ $\bullet$ $\bullet$ $\circ$'
- en: 'Table 3: A list of the most common action-*influences* and their assignment
    functions in the Deep RL literature analysed in this survey, and the properties
    they respect. Respectively, empty circles, half circles and bullets indicate that
    the property is not respected, that it is only partially respected, and it is
    fully respected. See Sections [4.5](#S4.SS5 "4.5 Existing assignment functions
    ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning") and [4.6](#S4.SS6 "4.6 Discussion ‣ 4 Quantifying
    action influences ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") for details.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：本调查分析的深度 RL 文献中最常见的行为-*影响*及其分配函数的列表，以及它们遵循的属性。空心圆、半圆和实心圆点分别表示该属性不被遵循、仅部分遵循和完全遵循。详情请参见章节[4.5](#S4.SS5
    "4.5 现有分配函数 ‣ 4 量化行为影响 ‣ 深度强化学习中的时间信用分配调查")和[4.6](#S4.SS6 "4.6 讨论 ‣ 4 量化行为影响 ‣
    深度强化学习中的时间信用分配调查")。
- en: The sheer variety of assignment functions described above leads to an equally
    broad range of metrics to quantify action influence and what is the best assignment
    function for a specific problem remains an open question. While we do not provide
    a definitive answer to the question of which properties are necessary or sufficient
    for an assignment function to output a satisfactory measure of credit, we set
    out to draw attention to the problem by abstracting out some of the properties
    that the metrics above share or lack. We identify the following properties of
    an assignment function and summarise our analysis in Table [3](#S4.T3 "Table 3
    ‣ 4.6 Discussion ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning").
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提到的各种分配函数导致了同样广泛的度量范围，以量化行为影响，并且什么是特定问题的最佳分配函数仍然是一个悬而未决的问题。虽然我们没有提供关于哪些属性对于分配函数输出满意的信用度量是必要或充分的明确答案，但我们旨在通过抽象出这些度量共享或缺乏的一些属性来引起对这个问题的关注。我们识别了分配函数的以下属性，并在表[3](#S4.T3
    "表 3 ‣ 4.6 讨论 ‣ 4 量化行为影响 ‣ 深度强化学习中的时间信用分配调查")中总结了我们的分析。
- en: Explicitness.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 明确性。
- en: We use the term explicitness when the goal appears as an explicit input of the
    assignment and it is not left implicit or inferred from experience. Using the
    goal as an input allows assigning credit for multiple goals at the same time.
    The decision problem can then more easily be broken down into subroutines that
    are both independent from each other and independently useful to achieve some
    superior goal $g$. Overall, explicitness allows incorporating more knowledge because
    the assignment spans each goals without losing information about others. This
    is the case, for example, of UVFAs, hindsight advantages, and future conditional
    advantages. As discussed in the previous section, distributional values can also
    be interpreted as explicitly assigning credit for each atom of the quantised return
    distribution, which is why we only partially consider them having this property
    in Table [3](#S4.T3 "Table 3 ‣ 4.6 Discussion ‣ 4 Quantifying action influences
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning"). Likewise,
    hindsight and future-conditional advantage, while not conditioning on a goal explicitly,
    can be interpreted as conditioning the influence on sub-goals that are states
    or returns, and future statistics, respectively. For this reason we consider them
    as partially being explicit assignments.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标作为分配的明确输入出现，而不是隐含或从经验中推断时，我们使用“明确性”这一术语。将目标作为输入可以同时为多个目标分配信用。决策问题可以更容易地被分解成彼此独立且对实现某个更高级目标$g$独立有用的子程序。总体而言，明确性允许纳入更多的知识，因为分配涵盖每个目标而不丢失关于其他目标的信息。例如，UVFAs、事后优势和未来条件优势就是这种情况。如前一节所讨论的，分布值也可以被解释为明确地为量化返回分布的每个原子分配信用，这就是为什么我们在表[3](#S4.T3
    "表 3 ‣ 4.6 讨论 ‣ 4 量化行为影响 ‣ 深度强化学习中的时间信用分配调查")中仅部分考虑它们具有这种属性的原因。同样，尽管事后和未来条件优势没有明确地以目标为条件，但可以解释为将影响条件化为子目标（状态或返回）和未来统计数据。因此，我们认为它们在某种程度上是明确的分配。
- en: Recursivity.
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 递归性。
- en: 'We use the term Recursivity to characterise the ability of an assignment function
    to support bootstrapping (Sutton and Barto,, [2018](#bib.bib181)). When an assignment
    is Markovian, it also respects a relationship of the type: $K(c_{t+1},a_{t+1},g)=f(K(c_{t},a_{t},g))$,
    where $f$ projects the influence from the time $t$ to $t+1$. For example, $q$-values
    can be written as: $q^{\pi}(s_{t+1},a_{t+1},g)=R(s_{t},a_{t})+\gamma q^{\pi}(s_{t},a_{t},g)$.
    The possibility to learn an action influence by bootstrapping provides key advantages.
    Theoretically, bootstrapping reduces the variance of the estimation at the cost
    of a bias (Sutton and Barto,, [2018](#bib.bib181)). In practice, bootstrapping
    is often necessary in Deep RL when the length of the episode for certain environments
    makes full Monte-Carlo estimations intractable due to computational and memory
    constraints. Often, when an assignment supports bootstrapping it also provides
    an absolute measure of influence of the action, opposed to a relative one. For
    example, the advantage produces a measure of influence that is relative to all
    the other possible actions. Indeed we can write $A^{\pi}(s,a)=q^{\pi}(s,a)-v^{\pi}(s)$
    or $A^{\pi}(s,a)=q^{\pi}(s,a)-\mathbb{E}_{a^{\prime}\sim\pi}\left[q^{\pi}(s,a^{\prime})\right]$.
    In fact, when estimated directly (Pan et al.,, [2022](#bib.bib128)) and not as
    the difference between $q(s,a)$ and $v(s)$, it cannot be learnt via bootstrapping,
    and one must obtain complete episodes to have unbiased samples of the return.
    This is often not advised as it increases the variance of the estimate of the
    return. At the same time, advantage also produces a measure of influence that
    is relative to all the other possible actions. On the other hand, $q$-values,
    which provide a measure of influence that does not vary if that of other actions
    in the same state do, support bootstrapping. Overall, both approaches to quantify
    influence have their pros and cons, and the main benefit of recursivity is to
    allow bootstrapping.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用术语递归性来描述赋值函数支持引导学习的能力（Sutton 和 Barto，[2018](#bib.bib181)）。当赋值是马尔可夫的，它也尊重一种关系：$K(c_{t+1},a_{t+1},g)=f(K(c_{t},a_{t},g))$，其中
    $f$ 将影响从时间 $t$ 投射到 $t+1$。例如，$q$-值可以写作：$q^{\pi}(s_{t+1},a_{t+1},g)=R(s_{t},a_{t})+\gamma
    q^{\pi}(s_{t},a_{t},g)$。通过引导学习来学习动作的影响提供了关键优势。从理论上讲，引导学习以偏差为代价减少了估计的方差（Sutton
    和 Barto，[2018](#bib.bib181)）。在实践中，当某些环境的回合长度使得完整的蒙特卡罗估计由于计算和内存限制而不可行时，引导学习通常是必要的。通常，当赋值支持引导学习时，它还提供了一个绝对的动作影响度量，而不是相对的。例如，优势产生的影响度量是相对于所有其他可能动作的。实际上，我们可以写作
    $A^{\pi}(s,a)=q^{\pi}(s,a)-v^{\pi}(s)$ 或 $A^{\pi}(s,a)=q^{\pi}(s,a)-\mathbb{E}_{a^{\prime}\sim\pi}\left[q^{\pi}(s,a^{\prime})\right]$。实际上，当直接估计（Pan
    等人，[2022](#bib.bib128)）而不是作为 $q(s,a)$ 和 $v(s)$ 之间的差异时，它不能通过引导学习获得，必须获得完整的回合才能得到无偏的回报样本。这通常不被推荐，因为它增加了回报估计的方差。同时，优势还产生了一个相对于所有其他可能动作的影响度量。另一方面，$q$-值提供了一个在同一状态下其他动作的影响不变的度量，支持引导学习。总体而言，量化影响的这两种方法各有利弊，递归性的主要好处是允许引导学习。
- en: Future-dependent.
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 依赖于未来。
- en: 'We use the term future-dependent for assignments that take as input information
    about what actions will be or have been taken after the time $t$ at which the
    action $A_{t}$ is evaluated. This is a key ingredient of many evaluations because
    the influence of the current action over a goal depends also on what happens after
    the action. For example, picking up a key is not meaningful if the policy does
    not lead to open the door afterwards, and the action to grab the key becomes irrelevant.
    Actions can be specified in-potentia, for example by specifying a policy to follow
    after the action. This is the case of policy-conditioned value function, whose
    benefit is to explicitly condition the value function on the policy such that,
    if the policy changes, but the action remains the same, the influence of the action
    changes instantly. Actions can also be specified in realisation. This is the case,
    for example, of hindsight evaluations (Andrychowicz et al.,, [2017](#bib.bib5))
    such as the hindsight advantage, the counterfactual advantage, and the PVF where
    actions are evaluated considering the full trajectory just collected. However,
    these functions only consider features of the future: the hindsight advantage
    considers only the final state or the final return of a trajectory; the counterfactual
    advantage considers some action-independent features of the future; the posterior
    value function considers only the non-observable components. Because futures are
    not considered fully, we consider these functions as only partially specifying
    the future. Furthermore, while state-action value functions, the advantage and
    their distributional counterparts specify a policy in principle, that information
    is not an explicit input of the assignment, but only left implicit. In practice,
    in Deep RL, if the policy changes, these assignment would not change their estimates.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用“未来依赖”这一术语来描述那些以时间 $t$ 后的行动信息作为输入的任务，这些信息用于评估行动 $A_{t}$。这是许多评估中的关键成分，因为当前行动对目标的影响也取决于行动后的情况。例如，拿起钥匙如果政策没有导致随后开门，那这一行动就没有意义，抓钥匙的行为也变得无关紧要。行动可以在潜在状态下指定，例如通过指定一个在行动之后跟随的政策。这就是策略条件价值函数的情况，其优点是将价值函数明确地条件化在策略上，使得如果策略发生变化，但行动保持不变，行动的影响会立即改变。行动也可以在实现中指定。例如，事后评估（Andrychowicz
    et al., [2017](#bib.bib5)）就是这样的情况，如事后优势、反事实优势和PVF，其中行动是考虑到刚刚收集的完整轨迹来评估的。然而，这些函数仅考虑未来的特征：事后优势只考虑轨迹的最终状态或最终回报；反事实优势考虑一些与行动无关的未来特征；后验价值函数只考虑不可观察的部分。由于未来没有被完全考虑，我们认为这些函数只是部分地指定了未来。此外，虽然状态-行动价值函数、优势及其分布对手从原则上指定了一个策略，但这些信息不是任务的显式输入，而是隐含的。实际上，在深度强化学习中，如果政策发生变化，这些任务将不会改变其估计。
- en: Causality.
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 因果关系。
- en: 'We refer to a causal assignment when the influence that it produces is also
    a measure of causal influence (Janzing et al.,, [2013](#bib.bib79)). For example,
    the counterfactual advantage proposes an interpretation closer to causality of
    the action influence by factorising the influence of an action in two. The first
    factor includes only the non-controllable components of the trajectory (e.g.,
    exogenous reward noise, stochasticity of the state-transition dynamics, stochasticity
    in the observation kernel), or those not under direct control of the agent at
    time $t$, such as future actions. The second factor includes only the effects
    of the action alone. The interpretation is that, while the latter is due causation,
    the former is only due to fortuitous correlations. This vicinity to causality
    theory exists despite the counterfactual advantage not being a satisfactory measure
    of causal influence as described in Janzing et al., ([2013](#bib.bib79)). Distributional
    advantage in Equation [11](#S4.E11 "In Distributional advantage ‣ 4.5 Existing
    assignment functions ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning") can also be interpreted as
    containing elements of causality. In fact, we have that the expectation of the
    advantage over states and actions is the Conditional Mutual Information (CMI)
    between the policy and the return, conditioned on the state-transition dynamics:
    $\mathbb{E}_{\mu,\pi}[D_{KL}(Q^{\pi}(s,a)||V^{\pi}(s))]=\mathcal{I}(\mathbb{P}_{\pi}(A|S=s);Z|\mathbb{P}_{\mu}(S))$.
    The CMI (with its limitations (Janzing et al.,, [2013](#bib.bib79))) is a known
    measure of causal influence.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提到因果分配时，所产生的影响也被视为因果影响的度量（Janzing et al., [2013](#bib.bib79)）。例如，对比因果优势提出了一种解释，靠近因果性，将动作的影响通过分解为两个因素。第一个因素仅包含轨迹中的不可控成分（例如，外生奖励噪声、状态转移动态的随机性、观察核的随机性），或者那些在时间$t$时不受代理直接控制的因素，如未来的动作。第二个因素仅包含动作本身的效应。其解释是，虽然后者由于因果关系产生，而前者仅由于偶然的相关性。尽管对比因果优势在Janzing等（[2013](#bib.bib79)）中描述的因果影响度量上并不令人满意，但这种接近因果性的理论依然存在。在方程[11](#S4.E11
    "在分布优势 ‣ 4.5 现有分配函数 ‣ 4 量化动作影响 ‣ 深度强化学习中的时间信用分配调查")中的分布优势也可以解释为包含因果性元素。实际上，我们有对状态和动作优势的期望是策略与回报之间的条件互信息（CMI），以状态转移动态为条件：$\mathbb{E}_{\mu,\pi}[D_{KL}(Q^{\pi}(s,a)||V^{\pi}(s))]=\mathcal{I}(\mathbb{P}_{\pi}(A|S=s);Z|\mathbb{P}_{\mu}(S))$。CMI（有其局限性（Janzing
    et al., [2013](#bib.bib79)））是一个已知的因果影响度量。
- en: Overall, these properties define some characteristics of an assignment, each
    one bringing positive and negative aspects. Explicitness allows to maintain the
    influence of an action with respect to multiple goals at the same time, promoting
    the reuse of information and a compositional onset of behaviour. Recursivity ensures
    that the influence can be learned via bootstrapping, an essential component of
    many RL methods. Future-dependency separates assignments by whether they include
    information about the future actions. Finally, causality has the benefit to filter
    out the spurious correlation and provides clearer signals to the policy improvement.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这些属性定义了分配的一些特征，每个特征都有正面和负面方面。明确性允许同时维持一个动作对多个目标的影响，促进信息的重用和行为的组合性开始。递归性确保影响可以通过引导学习，这对于许多强化学习方法来说是一个基本组件。未来依赖性将分配区分为是否包含有关未来动作的信息。最后，因果性具有过滤虚假相关性的好处，并为策略改进提供了更清晰的信号。
- en: 4.7 Summary
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 总结
- en: 'In this section, we addressed [Q1.](#S1.I1.i1 "item Q1\. ‣ Motivation. ‣ 1
    Introduction ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")
    and discussed the problem to quantify action influences. In Section [4.1](#S4.SS1
    "4.1 Are all action values, credit? ‣ 4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning") we formalised our
    question: “How do different works quantify action influences?” and “Are these
    quantities satisfactory measures of credit?”. We proceeded to answer the questions.
    In Section [4.2](#S4.SS2 "4.2 What is a goal? ‣ 4 Quantifying action influences
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning") we formalised
    the concept of *outcome* as some arbitrary function of a given history. In Section [4.3](#S4.SS3
    "4.3 What is an assignment? ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning") we defined the assignment function
    as a function that returns a measure of action influence. In Section [4.4](#S4.SS4
    "4.4 The credit assignment problem ‣ 4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning") we used this definition
    to formalise the CAP as the problem of learning a measure of action influence
    from experience. We refer to the set of protocols of this learning process as
    a credit assignment *method*. In Section [4.5](#S4.SS5 "4.5 Existing assignment
    functions ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") we surveyed existing measures of action influence
    from literature, detailed the intuition behind them, their advantages and drawbacks.
    Finally, in Section [4.6](#S4.SS6 "4.6 Discussion ‣ 4 Quantifying action influences
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning") we discussed
    how these measures of action influence relate to each other, the properties that
    they share and those that are more rare in literature, but still promising for
    future advancements. In the next section, we proceed to address [Q2.](#S1.I1.i2
    "item Q2\. ‣ Motivation. ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning"), by describing the challenges that arise from
    solving the CAP in Section [5](#S5 "5 The challenges to assign credit in Deep
    RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning") and
    surveying the methods to solve the CAP in Section [6](#S6 "6 Methods to assign
    credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning").'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了[Q1.](#S1.I1.i1 "item Q1\. ‣ Motivation. ‣ 1 Introduction ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning")并讨论了量化行动影响的问题。在第[4.1节](#S4.SS1
    "4.1 Are all action values, credit? ‣ 4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning")中，我们明确了我们的研究问题：“不同的工作如何量化行动影响？”以及“这些量化指标是否是满意的归因度量？”我们继续回答这些问题。在第[4.2节](#S4.SS2
    "4.2 What is a goal? ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")中，我们将*结果*的概念形式化为给定历史的某种任意函数。在第[4.3节](#S4.SS3
    "4.3 What is an assignment? ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")中，我们将归因函数定义为返回行动影响度量的函数。在第[4.4节](#S4.SS4
    "4.4 The credit assignment problem ‣ 4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning")中，我们利用这个定义将CAP形式化为从经验中学习行动影响度量的问题。我们将这一学习过程的协议集合称为归因*方法*。在第[4.5节](#S4.SS5
    "4.5 Existing assignment functions ‣ 4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning")中，我们回顾了文献中现有的行动影响度量，详细介绍了它们的直觉、优点和缺点。最后，在第[4.6节](#S4.SS6
    "4.6 Discussion ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning")中，我们讨论了这些行动影响度量如何相互关联，它们共享的属性以及在文献中较少见但仍有望为未来发展带来启示的属性。在下一节中，我们将继续探讨[Q2.](#S1.I1.i2
    "item Q2\. ‣ Motivation. ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")，通过描述在第[5节](#S5 "5 The challenges to assign credit
    in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")中解决CAP所面临的挑战，并在第[6节](#S6
    "6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")中回顾解决CAP的方法。
- en: 5 The challenges to assign credit in Deep RL
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 深度强化学习中的归因挑战
- en: Having clarified what are the measure of action influence available in literature,
    we now look at the challenges that arise to learn them and, together with Section [6](#S6
    "6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning"), answer [Q2.](#S1.I1.i2 "item Q2\. ‣ Motivation.
    ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"). The challenges proposed below provide a perspective to understand
    the principal directions of development of the methods to assign credit and to
    classify the sheer variety of methods that have been proposed. These challenges
    are often independent of the choice of action influence and apply to all of them.
    However, solving the CAP with a measure of influence or another will impact the
    prominence of each challenge. For example, hindsight methods deal better with
    sparsity compared to q-values, and so do GVFs. Overall, the following challenges
    help identify the major, outstanding research questions around devising a method
    to assign credit.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在阐明文献中可用的行为影响测量之后，我们现在来看学习这些测量所面临的挑战，并且结合第 [6](#S6 "6 Methods to assign credit
    in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")
    节，回答 [Q2.](#S1.I1.i2 "item Q2\. ‣ Motivation. ‣ 1 Introduction ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")。以下提出的挑战提供了理解分配奖励方法主要发展方向的视角，并分类各种已提出的方法。这些挑战通常与行为影响的选择无关，并且适用于所有这些方法。然而，用某种影响度量来解决
    CAP 将影响每个挑战的显著性。例如，事后方法在处理稀疏性方面表现更好，相比之下，q-values 和 GVFs 也是如此。总体而言，以下挑战有助于识别围绕设计分配奖励方法的主要研究问题。
- en: 'The current literature identifies the following sub-problems to assign credit:
    (a) delayed rewards(Raposo et al.,, [2021](#bib.bib146); Hung et al.,, [2019](#bib.bib75);
    Arjona-Medina et al.,, [2019](#bib.bib7); Chelu et al.,, [2022](#bib.bib31)):
    reward collection happens long after the action that determined it, causing its
    influence to be perceived as faint; (b) sparse rewards(Arjona-Medina et al.,,
    [2019](#bib.bib7); Seo et al.,, [2019](#bib.bib162); Chen and Lin,, [2020](#bib.bib34);
    Chelu et al.,, [2022](#bib.bib31)): the reward function is zero everywhere, and
    rarely spikes, causing uninformative TD errors; (c) partial observability(Harutyunyan
    et al.,, [2019](#bib.bib67)): where the agent does not hold perfect information
    about the current state; (d) high variance(Harutyunyan et al.,, [2019](#bib.bib67);
    Mesnard et al.,, [2021](#bib.bib112); van Hasselt et al.,, [2021](#bib.bib190))of
    the optimisation process; (e) the resort to time as a heuristic to determine the
    credit of an action (Harutyunyan et al.,, [2019](#bib.bib67); Raposo et al.,,
    [2021](#bib.bib146)): (f) the lack of counterfactual CA (Harutyunyan et al.,,
    [2019](#bib.bib67); Foerster et al.,, [2018](#bib.bib51); Mesnard et al.,, [2021](#bib.bib112);
    Buesing et al.,, [2019](#bib.bib29); van Hasselt et al.,, [2021](#bib.bib190));
    (g) slow convergence(Arjona-Medina et al.,, [2019](#bib.bib7)).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 当前文献识别了分配奖励的以下子问题：（a）延迟奖励（Raposo 等，[2021](#bib.bib146)；Hung 等，[2019](#bib.bib75)；Arjona-Medina
    等，[2019](#bib.bib7)；Chelu 等，[2022](#bib.bib31)）：奖励的收集发生在确定该奖励的行为之后很久，使得其影响被感知为微弱；（b）稀疏奖励（Arjona-Medina
    等，[2019](#bib.bib7)；Seo 等，[2019](#bib.bib162)；Chen 和 Lin，[2020](#bib.bib34)；Chelu
    等，[2022](#bib.bib31)）：奖励函数在所有地方都为零，并且很少出现突增，导致信息量不足的 TD 错误；（c）部分可观察性（Harutyunyan
    等，[2019](#bib.bib67)）：代理无法获取当前状态的完美信息；（d）高方差（Harutyunyan 等，[2019](#bib.bib67)；Mesnard
    等，[2021](#bib.bib112)；van Hasselt 等，[2021](#bib.bib190)）的优化过程；（e）将时间作为启发式方法来确定行为的信用（Harutyunyan
    等，[2019](#bib.bib67)；Raposo 等，[2021](#bib.bib146)）；（f）缺乏反事实 CA（Harutyunyan 等，[2019](#bib.bib67)；Foerster
    等，[2018](#bib.bib51)；Mesnard 等，[2021](#bib.bib112)；Buesing 等，[2019](#bib.bib29)；van
    Hasselt 等，[2021](#bib.bib190)）；（g）收敛缓慢（Arjona-Medina 等，[2019](#bib.bib7)）。
- en: 'While these issues are all very relevant to CAP, their classification is also
    tailored to control problems. Some of these are described by the use of a particular
    solution, such as [(e)](#S5.I1.i5 "item (e) ‣ 5 The challenges to assign credit
    in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning"),
    or the lack thereof, like [(f)](#S5.I1.i6 "item (f) ‣ 5 The challenges to assign
    credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"), rather than by a characteristic of the decision or of the optimisation
    problem. Here, we systematise these issues and transfer them to the CAP. We identify
    three principal characteristics of MDPs, which we refer to as dimensions of the
    MDP: depth, density and breadth (see Figure [2](#S5.F2 "Figure 2 ‣ 5 The challenges
    to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")). Challenges to CA emerge when pathological conditions on depth, density,
    and breadth produce specific phenomena that mask the learning signal to be unreliable,
    inaccurate, or insufficient to correctly reinforce an action. We now detail these
    three dimensions and the corresponding challenges that arise.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些问题与CAP（信用分配问题）非常相关，但它们的分类也针对控制问题进行量身定制。其中一些通过特定解决方案进行描述，例如[(e)](#S5.I1.i5
    "项 (e) ‣ 5 深度强化学习中的信用分配挑战 ‣ 深度强化学习中的时间信用分配调查")，或者缺乏这种解决方案，如[(f)](#S5.I1.i6 "项
    (f) ‣ 5 深度强化学习中的信用分配挑战 ‣ 深度强化学习中的时间信用分配调查")，而不是通过决策或优化问题的特征来描述。在这里，我们对这些问题进行了系统化，并将其转移到CAP中。我们确定了MDP（马尔科夫决策过程）的三个主要特征，我们称之为MDP的维度：深度、密度和广度（见图[2](#S5.F2
    "图 2 ‣ 5 深度强化学习中的信用分配挑战 ‣ 深度强化学习中的时间信用分配调查")）。当深度、密度和广度的病态条件产生特定现象，使得学习信号显得不可靠、不准确或不足以正确强化一个行动时，挑战就会出现。我们现在详细说明这三个维度及其相应的挑战。
- en: '![Refer to caption](img/1620a0811db939ca46f3d9f66977a34a.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1620a0811db939ca46f3d9f66977a34a.png)'
- en: (a) Depth of the MDP.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MDP的深度。
- en: '![Refer to caption](img/8754aed238056129634abe7f5b7d4a6b.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8754aed238056129634abe7f5b7d4a6b.png)'
- en: (b) Density of the MDP.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MDP的密度。
- en: '![Refer to caption](img/072a998570c976165a00f96bf8e22277.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/072a998570c976165a00f96bf8e22277.png)'
- en: (c) Breadth of the MDP.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: (c) MDP的广度。
- en: 'Figure 2: Visual intuition of the three challenges to temporal CA and their
    respective set of solutions, using the graph analogy. Nodes and arrows represent,
    respectively, MDP states and actions. Blue nodes and arrows denote the current
    episode. Black ones show states that could have potentially been visited, but
    have not. Square nodes denote goals. Forward arrows (pointing right) represent
    environment interactions, whereas backward arrows (pointing left) denote credit
    propagation via state-action back-ups. From top left: ([2(a)](#S5.F2.sf1 "In Figure
    2 ‣ 5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning")) the temporal distance between the
    accountable action and the target state requires propagating credit deep back
    in time; ([2(b)](#S5.F2.sf2 "In Figure 2 ‣ 5 The challenges to assign credit in
    Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning"))
    considering any state as a target increases the density of possible associations
    and reduces information sparsity; and finally, ([2(c)](#S5.F2.sf3 "In Figure 2
    ‣ 5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")) the breadth of possible pathways leading to
    the target state.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用图形类比对时间信用分配的三个挑战及其相应解决方案的视觉直观。节点和箭头分别表示MDP状态和动作。蓝色节点和箭头表示当前情节。黑色节点和箭头显示可能已经访问但尚未访问的状态。方形节点表示目标。前向箭头（指向右）表示环境交互，而后向箭头（指向左）表示通过状态-动作备份传播的信用。从左上角开始：（[2(a)](#S5.F2.sf1
    "在图 2 ‣ 5 深度强化学习中的信用分配挑战 ‣ 深度强化学习中的时间信用分配调查")）负责的行动和目标状态之间的时间距离需要深度传播信用；（[2(b)](#S5.F2.sf2
    "在图 2 ‣ 5 深度强化学习中的信用分配挑战 ‣ 深度强化学习中的时间信用分配调查")）将任何状态视为目标会增加可能关联的密度并减少信息稀疏性；最后，（[2(c)](#S5.F2.sf3
    "在图 2 ‣ 5 深度强化学习中的信用分配挑战 ‣ 深度强化学习中的时间信用分配调查")）指向目标状态的可能路径的广度。
- en: 5.1 Delayed effects due to high MDP depth
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 由于MDP深度较高导致的延迟效果
- en: We refer to the depth of an MDP as the number of temporal steps that intervene
    between an highly influential action and an outcome. When this happens, we refer
    to the action as a remote action, and the outcome as a delayed outcome. When outcomes
    are delayed, the increase of temporal distance often corresponds to a combinatorial
    increase of possible alternative futures and the paths to get to them. In these
    conditions, recognising which action was responsible for the outcome is harder
    since the space of possible associations is very large. We identify two main reasons
    for an outcome to be delayed, depending on whether the decision after the remote
    action influences the outcome or not.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将MDP的深度定义为在一个高度影响的行动与一个结果之间的时间步骤数量。当这种情况发生时，我们称该行动为远程行动，结果为延迟结果。当结果被延迟时，时间距离的增加通常对应于可能的替代未来及其路径的组合性增加。在这些条件下，识别哪个行动对结果负责更困难，因为可能的关联空间非常大。我们识别出两种主要原因导致结果被延迟，取决于远程行动后的决策是否影响结果。
- en: The first is that the success of the action is not immediate but requires a
    sequence of actions to be performed afterwards, which causes the causal chain
    to be long. This issue originates from the typical hierarchical structure of many
    MDPs, where the agent must first perform a sequence of actions to reach a subjective
    sub-goal, and then perform another sequence to reach another. These behaviours
    can then be composed to reach the final, objective goal and solve the assigned
    task. When this happens, agents must be able to assign credit to the individual
    actions that are responsible for the objective goal, while still being able to
    select sub-goals along the way and assign credit to action for their ability to
    reach the subjective sub-goal. The key-to-door task (Hung et al.,, [2019](#bib.bib75))
    is a good example of this phenomenon, where the agent must first collect a key,
    to be able to open a door later. Here, one decision (opening the door) is contingent
    upon a previous one (collecting the key), and the credit of the latter is delayed
    until the former is performed.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，行动的成功不是立刻显现的，而是需要一系列后续行动，这导致因果链条较长。这个问题源自许多MDP的典型层次结构，其中代理首先必须执行一系列动作以达到一个主观子目标，然后再执行另一系列动作以达到另一个子目标。这些行为可以组合起来以实现最终的客观目标并解决分配的任务。当这种情况发生时，代理必须能够为那些对最终目标负有责任的个体动作分配功劳，同时仍能在过程中选择子目标并为动作在达到主观子目标方面的能力分配功劳。关键开门任务（Hung
    et al., [2019](#bib.bib75)）是这种现象的一个很好的例子，其中代理必须首先收集一个钥匙，然后才能打开门。在这里，一个决策（开门）依赖于之前的决策（收集钥匙），而后者的功劳会被延迟到前者完成之后。
- en: The second reason why outcomes can be delayed is that they might only be observed
    after a long time horizon since the decisions taken after the remote action do
    not influence the outcome significantly. This issue originates from behavioural
    psychology and is known as the delayed reinforcement problem (Lattal,, [2010](#bib.bib98)),
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个导致结果可能被延迟的原因是，可能需要较长时间才能观察到这些结果，因为在远程行动之后做出的决策对结果的影响不大。这个问题源自行为心理学，被称为延迟强化问题（Lattal,,
    [2010](#bib.bib98)）。
- en: Reinforcement is delayed whenever there is a period of time between the response
    producing the reinforcer and its subsequent delivery. (Lattal,, [2010](#bib.bib98))
  id: totrans-247
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每当在产生强化物的反应和其后续交付之间存在一段时间时，强化就会被延迟。（Lattal,, [2010](#bib.bib98)）
- en: One can find references to the same phenomenon in RL as long-term CA (Ma et al.,,
    [2021](#bib.bib108); Raposo et al.,, [2021](#bib.bib146); Hung et al.,, [2019](#bib.bib75))
    or long-term consequences (Barto,, [1997](#bib.bib18); Vinyals et al.,, [2017](#bib.bib199);
    Hung et al.,, [2019](#bib.bib75)), long-horizon tasks (Gupta et al.,, [2019](#bib.bib62);
    Arumugam et al.,, [2021](#bib.bib10)), or delayed rewards (Hung et al.,, [2019](#bib.bib75);
    Arjona-Medina et al.,, [2019](#bib.bib7)). The main challenge with delayed reinforcements
    is in being able to ignore the series of irrelevant decisions that are encountered
    between the remote action and the delayed outcome, focus on the actions that are
    responsible for the outcome, and assign credit accordingly. This is a key requirement
    because most CA methods rely on temporal recency as a heuristic to assign credit
    (Klopf,, [1972](#bib.bib92); Sutton,, [1988](#bib.bib177); Mahmood et al.,, [2015](#bib.bib109);
    Sutton et al.,, [2016](#bib.bib182); [Jiang et al., 2021a,](#bib.bib81) ). When
    this is the case, the actions in the proximity of achieving the goal are reinforced,
    even if not actually being responsible for the outcome (only the remote action
    is), but only happen to be temporally close to the outcome.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在RL中，可以找到与长期CA相关的相同现象（Ma et al., [2021](#bib.bib108); Raposo et al., [2021](#bib.bib146);
    Hung et al., [2019](#bib.bib75)）或长期后果（Barto, [1997](#bib.bib18); Vinyals et al.,
    [2017](#bib.bib199); Hung et al., [2019](#bib.bib75)），长期任务（Gupta et al., [2019](#bib.bib62);
    Arumugam et al., [2021](#bib.bib10)），或者延迟奖励（Hung et al., [2019](#bib.bib75); Arjona-Medina
    et al., [2019](#bib.bib7)）。延迟强化的主要挑战在于能够忽略在远程行动和延迟结果之间遇到的一系列无关决策，专注于那些对结果负责的行动，并相应地分配奖励。这是一个关键要求，因为大多数CA方法依赖于时间上的接近性作为分配奖励的启发式方法（Klopf,
    [1972](#bib.bib92); Sutton, [1988](#bib.bib177); Mahmood et al., [2015](#bib.bib109);
    Sutton et al., [2016](#bib.bib182); [Jiang et al., 2021a,](#bib.bib81)）。在这种情况下，即使行动实际上并不对结果负责（只有远程行动才负责），但如果它们恰好在时间上接近结果，它们也会受到强化。
- en: In practice, the prevalence of delayed effects can manifest as a lack of progress
    in training, but it is often hard to isolate the impact of delayed effects from
    the other features of the environment that hinder learning without appropriate
    experimental conditions being set. For example, consider the key-to-door environments
    introduced above, where the agent has to collect a key that opens a door and then
    navigate to a certain position of a grid. The effects of picking up the key are
    delayed until the target square is reached, which, in turn, is contingent upon
    opening the door. In these conditions it is hard to disentangle the problem of
    exploring the right combination with that of learning that the event of grabbing
    the key is necessary to obtain that combination. We discuss more in detail how
    to diagnose this in Section [7](#S7 "7 Evaluating credit ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning") and the relationship between
    CAP and exploration in Section [5.4](#S5.SS4 "5.4 Relationship with the exploration
    problem ‣ 5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning"). To solve the challenge, a
    subset of studies on the CAP has focused on this issue, and have proposed methods
    that can deal specifically with delayed effects either by using memory (Hung et al.,,
    [2019](#bib.bib75); Arjona-Medina et al.,, [2019](#bib.bib7); [Ferret et al.,
    2021a,](#bib.bib46) ; Ren et al.,, [2022](#bib.bib148); Raposo et al.,, [2021](#bib.bib146)),
    re-weighing updates (Sutton et al.,, [2016](#bib.bib182); Chelu et al.,, [2022](#bib.bib31)),
    or meta-learning (Xu et al.,, [2018](#bib.bib211); Badia et al.,, [2020](#bib.bib13);
    Kapturowski et al.,, [2022](#bib.bib85); Flennerhag et al.,, [2021](#bib.bib49);
    Oh et al.,, [2020](#bib.bib126)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，延迟效应的普遍存在可能表现为训练进展的缺乏，但在没有设置适当实验条件的情况下，通常很难将延迟效应的影响与环境中其他阻碍学习的特征分离。例如，考虑上述引入的钥匙-门环境，在这种环境中，智能体必须收集一个能打开门的钥匙，然后导航到网格的某个位置。拾取钥匙的效果会延迟到目标方格被到达，这又依赖于门的开启。在这些条件下，很难将探索正确组合的问题与学习到抓取钥匙事件是获得该组合所需的事件的问题分开。我们在第[7](#S7
    "7 Evaluating credit ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")节中详细讨论了如何诊断这一问题，以及在第[5.4](#S5.SS4 "5.4 Relationship with the exploration
    problem ‣ 5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")节中CAP与探索之间的关系。为了解决这一挑战，一些关于CAP的研究集中于这一问题，并提出了可以专门处理延迟效应的方法，这些方法通过使用记忆（Hung
    et al., [2019](#bib.bib75); Arjona-Medina et al., [2019](#bib.bib7); [Ferret et
    al., 2021a,](#bib.bib46); Ren et al., [2022](#bib.bib148); Raposo et al., [2021](#bib.bib146)）、重新加权更新（Sutton
    et al., [2016](#bib.bib182); Chelu et al., [2022](#bib.bib31)）或元学习（Xu et al.,
    [2018](#bib.bib211); Badia et al., [2020](#bib.bib13); Kapturowski et al., [2022](#bib.bib85);
    Flennerhag et al., [2021](#bib.bib49); Oh et al., [2020](#bib.bib126)）来处理。
- en: 5.2 Low action influence due to low MDP density
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 由于低MDP密度导致的低动作影响
- en: If delayed effects are characterised by a large temporal distance between an
    action and the outcome it causes, MDP sparsity derives from a lack of influence
    between them. This is substantially different from delayed effects, where actions
    can cause outcomes very frequently, except with delay. Here, actions have little
    impact on the probability to achieve a given goal neither now, nor far in the
    future, and it does not matter what the agent does, the outcome will still be
    the same. We identify two main reasons why this happens.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如果延迟效应的特征是动作和其造成的结果之间的时间距离较大，MDP稀疏性源于它们之间缺乏影响。这与延迟效应有所不同，在延迟效应中，动作可以非常频繁地导致结果，只是有延迟。在这里，动作对实现给定目标的概率几乎没有影响，无论现在还是将来，智能体所做的事情都不会改变结果。我们确定了两个主要原因导致这种情况的发生。
- en: The first one is a high stochasticity of the state-transition distribution,
    which is characterised by a high entropy of the state-transition distribution
    $\mathcal{H}(\mathbb{P}_{\mu})$ and/or the reward function $\mathcal{H}(\mathbb{P}(R))$.
    When this happens, actions hardly affect the future states of the trajectory.
    The agent is unable to make predictions with high confidence, and therefore cannot
    select actions that are likely to lead to the goal.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原因是状态转移分布的高随机性，其特点是状态转移分布$\mathcal{H}(\mathbb{P}_{\mu})$和/或奖励函数$\mathcal{H}(\mathbb{P}(R))$的高熵。当发生这种情况时，动作几乎不影响轨迹的未来状态。智能体无法以高信心进行预测，因此无法选择可能导致目标的动作。
- en: The second reason is the low goal density. This is the canonical case of reward
    sparsity in RL, where the goal is only achievable in a small subset of the state
    space, or for a specific sequence of actions.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是目标密度低。这是 RL 中奖励稀疏性的典型情况，其中目标仅在状态空间的一个小子集内可达，或者对于特定的动作序列才可达。
- en: Formally, we can measure the *lack of influence* using the notion of information
    sparsity (Arumugam et al.,, [2021](#bib.bib10)) of an MDP.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，我们可以使用 MDP 的信息稀疏性（Arumugam 等人，[2021](#bib.bib10)）来衡量*影响缺乏*。
- en: Definition 4  (MDP sparsity).
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4  （MDP 稀疏性）。
- en: 'An MDP is $\varepsilon$-information sparse if:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 MDP 是 $\varepsilon$-信息稀疏的，则：
- en: '|  | $\displaystyle\max_{\pi\in\Pi}\,\mathbb{E}_{\mu,\pi}[D_{KL}(P_{\pi,\mu}(Z&#124;s,a)&#124;&#124;P_{\pi,\mu}(Z&#124;s))]\leq\varepsilon,$
    |  | (16) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\pi\in\Pi}\,\mathbb{E}_{\mu,\pi}[D_{KL}(P_{\pi,\mu}(Z\vert
    s,a)\vert\vert P_{\pi,\mu}(Z\vert s))]\leq\varepsilon,$ |  | (16) |'
- en: where $\mathbb{E}_{\mu,\pi}$ denotes the expectation over the stationary distribution
    induced by the policy and the state-transition dynamics. The information sparsity
    of an MDP is the maximum information gain that can be obtained by an agent, represented
    by a policy $\pi$, by knowing which immediate action is played. When the information
    gain is low everywhere, and only concentrated in a small subset of decisions,
    CA methods often struggle to assign credit, because the probability of the outcome
    occurring is low, and there is rarely a signal to propagate. Here, exploration
    also plays a key role. Indeed, to acquire knowledge (the CAP), the underlying
    set of associations between actions an outcome must be discovered first (exploration).
    Often, this is faced by artificially improving the learning signal, either by
    reward shaping (Ng et al.,, [1999](#bib.bib122); Zou et al.,, [2019](#bib.bib220);
    Hu et al.,, [2020](#bib.bib74)), using auxiliary goals (Sutton et al.,, [2011](#bib.bib183);
    [Schaul et al., 2015a,](#bib.bib153) ), or selecting goals in hindsight (Rauber
    et al.,, [2019](#bib.bib147); Andrychowicz et al.,, [2017](#bib.bib5); Harutyunyan
    et al.,, [2019](#bib.bib67); Tang and Kucukelbir,, [2021](#bib.bib185)).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbb{E}_{\mu,\pi}$ 表示由策略和状态转移动态引起的平稳分布下的期望。MDP 的信息稀疏性是指代理通过知道立即执行的动作而获得的最大信息增益，代理由策略
    $\pi$ 表示。当信息增益处处都很低，并且仅集中在少量决策中时，CA 方法往往难以分配奖励，因为结果发生的概率较低，并且很少有信号可以传播。在这里，探索也发挥了关键作用。实际上，为了获取知识（CAP），必须首先发现动作与结果之间的基本关联集合（探索）。通常，这需要通过奖励塑造（Ng
    等人，[1999](#bib.bib122)；Zou 等人，[2019](#bib.bib220)；Hu 等人，[2020](#bib.bib74)）、使用辅助目标（Sutton
    等人，[2011](#bib.bib183)；[Schaul 等人，2015a,](#bib.bib153)）或回顾性选择目标（Rauber 等人，[2019](#bib.bib147)；Andrychowicz
    等人，[2017](#bib.bib5)；Harutyunyan 等人，[2019](#bib.bib67)；Tang 和 Kucukelbir，[2021](#bib.bib185)）来人工提高学习信号。
- en: 5.3 Low action influence due to high MDP breadth
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 由于 MDP 广度大而导致的动作影响低
- en: 'We refer to the *breadth* of an MDP by the expected number of possible alternative
    routes that can lead to a given outcome. To provide an intuition of how it affects
    CA we borrow that of *transpositions* from game theory, in particular from chess.
    A transposition is an alternative sequence of actions and states that results
    in the same final result. In RL, given a trajectory $h$, we call transposition
    another trajectory $h^{\prime}$ that produces the same outcome $\psi(h)=\psi(h^{\prime})$.
    We formalise the concept using the notion of the null space of a policy (Schaul
    et al.,, [2022](#bib.bib152)). Given a policy $\pi$, its null space is the subspace
    of policies with the same expected value:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过期望的可能替代路线数量来描述 MDP 的*广度*，这些路线可以导致给定结果。为了提供它如何影响 CA 的直觉，我们借鉴了来自博弈论的*换位*，特别是来自象棋的换位。换位是一种结果相同的替代动作和状态序列。在
    RL 中，给定一个轨迹 $h$，我们称另一个轨迹 $h^{\prime}$ 为换位，如果它产生相同的结果 $\psi(h)=\psi(h^{\prime})$。我们使用策略的零空间的概念来形式化这一概念（Schaul
    等人，[2022](#bib.bib152)）。给定一个策略 $\pi$，它的零空间是具有相同期望值的策略子空间：
- en: '|  | $\displaystyle\text{Null}(\pi):=\overline{\Pi}\subseteq\Pi:v(\overline{\pi})=v(\pi),\quad\forall\pi,\overline{\pi}_{2}\in\overline{\Pi}.$
    |  | (17) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Null}(\pi):=\overline{\Pi}\subseteq\Pi:v(\overline{\pi})=v(\pi),\quad\forall\pi,\overline{\pi}_{2}\in\overline{\Pi}.$
    |  | (17) |'
- en: Definition 5  (Transposition).
  id: totrans-262
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 5  （换位）。
- en: Given a reference policy $\pi$, and its null space $Null(\pi)$, consider a random
    sample $\overline{\pi}\in Null(\pi)$. We refer to *transposition* as any unique
    trajectory drawn following $\overline{\pi}$.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个参考策略 $\pi$ 及其零空间 $Null(\pi)$，考虑一个随机样本 $\overline{\pi}\in Null(\pi)$。我们将*换位*定义为任何唯一的轨迹，它是根据
    $\overline{\pi}$ 绘制的。
- en: An optimal set of transpositions is then the set of transpositions for the set
    of optimal policies $\Pi^{*}$.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 最优转置集是指针对最优策略集 $\Pi^{*}$ 的转置集合。
- en: We refer to this as transposition because it is a form of permutation of the
    decisions in the trajectory, which still produce the same results. Because many
    optimal pathways exist, there is no one key pathway that is responsible for the
    outcome, and not bottleneck decision that the agent has to make necessarily to
    achieve the goal. When this happens, the influence of these actions is low because
    credit is diluted over many alternative routes.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其称为转置，因为它是一种轨迹中决策的排列形式，仍然产生相同的结果。由于存在许多最优路径，没有任何一条关键路径对结果负有责任，也没有代理必须做出的瓶颈决策来实现目标。当这种情况发生时，这些动作的影响较小，因为信用被稀释到许多替代路径上。
- en: This challenge occurs when there exist a high number of combinations of states
    and actions that can lead to the same outcome. When only one pathway is found,
    its credit is confounded as high, when in fact it is not, because many other combinations
    lead to the same outcome. Deep RL algorithms often struggle to find all transpositions,
    because they often stop exploring already when a single solution (one optimal
    pathway) is found. To mitigate the problem, different studies experimented with
    propagating credit to experiences beyond the current trajectory using memory (van
    Hasselt et al.,, [2021](#bib.bib190); [Jiang et al., 2021c,](#bib.bib84) ), world
    models (Chelu et al.,, [2020](#bib.bib32)) that imagine proceeding backwards (Edwards
    et al.,, [2018](#bib.bib40); Goyal et al.,, [2019](#bib.bib56); Nair et al.,,
    [2020](#bib.bib121); Buesing et al.,, [2019](#bib.bib29); Lu et al.,, [2020](#bib.bib105);
    Zhang et al.,, [2020](#bib.bib216); Lai et al.,, [2020](#bib.bib96)), or that
    plan forward (Sutton,, [1990](#bib.bib178)).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在大量状态和动作组合可以导致相同结果时，就会出现此挑战。当仅找到一条路径时，其信用被混淆为高，尽管实际上并非如此，因为许多其他组合也会导致相同的结果。深度强化学习算法往往难以找到所有的转置，因为它们在找到一个解决方案（一个最优路径）时通常就停止探索。为了缓解这个问题，不同的研究尝试通过使用记忆（van
    Hasselt et al.,, [2021](#bib.bib190); [Jiang et al., 2021c,](#bib.bib84)）、世界模型（Chelu
    et al.,, [2020](#bib.bib32)）进行反向模拟（Edwards et al.,, [2018](#bib.bib40); Goyal
    et al.,, [2019](#bib.bib56); Nair et al.,, [2020](#bib.bib121); Buesing et al.,,
    [2019](#bib.bib29); Lu et al.,, [2020](#bib.bib105); Zhang et al.,, [2020](#bib.bib216);
    Lai et al.,, [2020](#bib.bib96)）或前向规划（Sutton,, [1990](#bib.bib178)）。
- en: 5.4 Relationship with the exploration problem
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 与探索问题的关系
- en: The challenges above refer to the CAP alone and try to isolate independent components
    of an MDP that affect the CAP. Before concluding the section, we discuss the relationship
    between these three challenges and the exploration problem (Amin et al.,, [2021](#bib.bib4)).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 上述挑战仅涉及CAP，并尝试隔离影响CAP的MDP的独立组件。在总结本节之前，我们讨论这三种挑战与探索问题之间的关系（Amin et al.,, [2021](#bib.bib4)）。
- en: 'Exploration and CA are two cornerstones of RL. Exploration is the problem of
    discovering temporal sequences of states, actions and rewards with the purpose
    of acquiring new information and expanding the pool of viable ways to act in an
    unknown environment (Jiang et al.,, [2023](#bib.bib82)). For example, consider
    a key-to-door environment, where the agent need to pick up a key, which opens
    a door, behind which lies a reward. In this environment, Exploration discovers
    the combination of actions and states that visits the key, grabs it, goes to the
    door and opens it. The discovery is often not the result of informed decision-making,
    the agent does not know that the key opens a door and this happens often only
    by chance³³3Or, rather, by the laws of the exploration algorithm.. On the other
    hand, CA is tasked with consuming the full set of experiences acquired by Exploration,
    with the purpose of associating elements of it: an state-action and an outcome.
    Associations remain alive for a certain period of time until superseded by others
    or due to extinction (Thorndike,, [1898](#bib.bib186); Pavlov,, [1927](#bib.bib130);
    Skinner,, [1937](#bib.bib169)). In the example above, one can think of CA as remembering
    – learning – that the key opens a door, in order to reuse this association in
    the future. Unlike with exploration, this behaviour is not the result of chance
    anymore, but of informed-decision making powered by ability to forecast the effects
    of the action (Sutton et al.,, [2011](#bib.bib183)).'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 探索和CA是强化学习（RL）的两个基石。探索是发现状态、动作和奖励的时间序列的问题，目的是获取新信息并扩展在未知环境中采取行动的可行方式的池子（Jiang
    et al., [2023](#bib.bib82)）。例如，考虑一个钥匙到门的环境，其中代理需要拿到一把钥匙，这把钥匙可以打开一扇门，门后面有一个奖励。在这个环境中，探索发现了访问钥匙、拿起钥匙、走到门前并打开它的动作和状态的组合。这个发现通常不是由知情决策做出的，代理并不知道钥匙可以打开门，这通常只是偶然发生的³³3或者说，正如探索算法的规律所决定的那样。另一方面，CA的任务是利用探索获得的全部经验，目的是将其元素关联起来：一个状态-动作和一个结果。关联会在一定时间内保持活跃，直到被其他关联取代或因灭绝而消失（Thorndike,
    [1898](#bib.bib186); Pavlov, [1927](#bib.bib130); Skinner, [1937](#bib.bib169)）。在上面的例子中，可以将CA视为记住—学习—钥匙可以打开门，以便将来重用这个关联。与探索不同，这种行为不再是偶然的结果，而是基于预测动作效果的知情决策（Sutton
    et al., [2011](#bib.bib183)）。
- en: While exploration and CA can be studied independently under particular experimental
    conditions (see Section [7](#S7 "7 Evaluating credit ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning")), they depend on each other when solving
    control problems, where the agent relies on a combination of both. The relationship
    between the two is particularly evident in sparse MDP, where the action influence
    is low. In these conditions, from the CA point of view, actions struggle to have
    any influence on the outcome. From the point of view of exploration, this is often
    described in different terms as the sparse reward problem (Ladosz et al.,, [2022](#bib.bib95)).
    Indeed, when rewards are particularly sparse (Ladosz et al.,, [2022](#bib.bib95)),
    exploration struggles to discover rewards. In support of this connection, studies
    on CA often start from sparse rewards setups, for example, in Andrychowicz et al.,
    ([2017](#bib.bib5)); Arumugam et al., ([2021](#bib.bib10)); Edwards et al., ([2018](#bib.bib40)).
    On one hand, this connection makes it often hard to disentangle the impacts of
    CA and exploration on solving the overall RL problem. On the other hand, it highlights
    that the two problems are interdependent in the most common control settings and
    making claims on one or the other requires particular care.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管探索和CA可以在特定实验条件下独立研究（见第[7](#S7 "7 Evaluating credit ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning)节），但在解决控制问题时，它们是相互依赖的，代理依赖于两者的结合。两者之间的关系在稀疏MDP中尤为明显，在这些条件下，动作的影响较低。在这些条件下，从CA的角度来看，动作很难对结果产生任何影响。从探索的角度来看，这通常被不同的术语描述为稀疏奖励问题（Ladosz
    et al., [2022](#bib.bib95)）。确实，当奖励特别稀疏时（Ladosz et al., [2022](#bib.bib95)），探索很难发现奖励。支持这种联系的研究通常从稀疏奖励设置开始，例如在Andrychowicz
    et al., ([2017](#bib.bib5)); Arumugam et al., ([2021](#bib.bib10)); Edwards et
    al., ([2018](#bib.bib40))中。另一方面，这种联系使得很难分开CA和探索在解决整体RL问题上的影响。另一方面，它突显了这两个问题在最常见的控制设置中的相互依赖性，对其中任何一个问题的主张都需要特别小心。
- en: To conclude, while CA and exploration are orthogonal problems and can be studied
    independently, what data to use to learn credit is fundamental to solve control
    problem, and that is where the two problems connect.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 总结起来，尽管CA（信用分配）和探索是正交问题，可以独立研究，但用于学习信用的数据是解决控制问题的基础，这也是这两个问题的交汇点。
- en: 5.5 Summary
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 总结
- en: 'In this section, we have identified the challenges that arise from solving
    the CAP. These challenges include delayed rewards, sparse rewards, partial observability,
    high variance, the resort to time as a heuristic, lack of counterfactual CA, and
    sample efficiency. We have systematised these issues as challenges that emerge
    from specific properties of the decision problem, which we refer to as dimensions
    of the MDP: depth, density, and breadth. Challenges to CA emerge when pathological
    conditions on these dimensions produce specific phenomena that mask the learning
    signal to be unreliable, inaccurate, or insufficient to correctly reinforce an
    action. We have provided an intuition of this classification with the aid of graphs
    and proceeded to detail each challenge. Finally, we discussed the connection between
    the CAP and the exploration problem, encouraging to take particular care to disentangle
    the contribution of each of them when making claims on one or the other.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们已识别出解决CAP（信用分配问题）所面临的挑战。这些挑战包括延迟奖励、稀疏奖励、部分可观测性、高方差、依赖时间作为启发式方法、缺乏对抗性CA以及样本效率。我们将这些问题系统化为源自决策问题特定属性的挑战，我们称之为MDP（马尔可夫决策过程）的维度：深度、密度和广度。当这些维度上的病态条件产生特定现象，掩盖了学习信号，使其不可靠、不准确或不足以正确强化一个动作时，CA的挑战就会出现。我们利用图示对这种分类提供了直观理解，并详细描述了每个挑战。最后，我们讨论了CAP与探索问题的联系，建议在对一个或另一个做出声明时特别注意区分它们的贡献。
- en: With these challenges in mind, we now proceed to review the state of the art
    in CA, and discuss the methods that have been proposed to address them.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这些挑战，我们现在继续回顾CA的最新进展，并讨论为解决这些问题而提出的方法。
- en: 6 Methods to assign credit in Deep RL
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度强化学习中的6种信用分配方法
- en: Following the definition of CAP in Section [4.4](#S4.SS4 "4.4 The credit assignment
    problem ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") a credit assignment method is an algorithm to
    approximate action influence from a finite amount of experience. In this section,
    we present a list of credit assignment methods that focuses on Deep RL. Our classification
    aims to identify the principal directions of development around credit assignment
    algorithms, that is, to minimise the intersection between each class of methods.
    Our intent is to understand the density around each set of approaches, to locate
    the branches suggesting the most promising results, and to draw a trend of the
    latest findings. This can be helpful to both the researchers on the CAP who want
    to have a bigger picture of the current state of the art, to general RL practitioners
    and research engineers to identify the most promising methods to use in their
    applications, and to the part of the scientific community that focuses on different
    problems, but that can benefit from the insights on CA. We define a CA method
    according to how it specifies three elements.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 根据第[4.4节](#S4.SS4 "4.4 信用分配问题 ‣ 4 量化动作影响 ‣ 深度强化学习中时间信用分配调查")中的CAP定义，信用分配方法是一个算法，用于从有限经验中近似动作影响。在本节中，我们介绍了一系列专注于深度强化学习的信用分配方法。我们的分类旨在识别围绕信用分配算法的主要发展方向，即最小化每种方法类别之间的交集。我们的目的是理解每组方法的密度，定位出最具前景的分支，并绘制最新发现的趋势。这对于希望了解当前最新状态的CAP研究人员、希望识别最有前景方法以应用于其应用的通用RL实践者和研究工程师，以及关注不同问题但可以从CA见解中受益的科学界成员都很有帮助。我们根据方法如何指定三个元素来定义CA方法。
- en: (a)
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: The measure of action influence, thus the assignment function $K$. This usual
    an approximation of one of the quantities discussed in Section [4.5](#S4.SS5 "4.5
    Existing assignment functions ‣ 4 Quantifying action influences ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning"), for example, an $n$-step
    return target in place of the full target.
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动作影响度量，即分配函数$K$。这通常是对第[4.5节](#S4.SS5 "4.5 现有分配函数 ‣ 4 量化动作影响 ‣ 深度强化学习中时间信用分配调查")中讨论的某些量的近似，例如，用$n$步返回目标代替完整目标。
- en: (b)
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: The protocol that the method uses to approximate $K$ from the experience $\mathcal{D}$.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方法用来从经验 $\mathcal{D}$ 逼近 $K$ 的协议。
- en: (c)
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: The mechanism it uses to collect the experience, which we refer to as the contextual
    distribution (see Appendix [B](#S2a "B Further details on contexts ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning") for more details).⁴⁴4To
    enhance the flow of the manuscript, we formalise contextual distributions in Appendix [B](#S2a
    "B Further details on contexts ‣ A Survey of Temporal Credit Assignment in Deep
    Reinforcement Learning"), and since they are intuitive concepts, we describe them
    in words when surveying the methods.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它用来收集经验的机制，我们称之为上下文分布（更多细节请参见附录 [B](#S2a "B Further details on contexts ‣ A
    Survey of Temporal Credit Assignment in Deep Reinforcement Learning")）。⁴⁴4为了提高稿件的流畅性，我们在附录 [B](#S2a
    "B Further details on contexts ‣ A Survey of Temporal Credit Assignment in Deep
    Reinforcement Learning")中形式化了上下文分布，由于这些是直观的概念，我们在调查方法时用文字描述它们。
- en: 'This provides consistency with the framework just proposed, and allows categorising
    each method by the heuristics that it uses to assign credit. Therefore, for each
    method, we report the three elements described above. We identify the following
    categories:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这与刚提出的框架保持一致，并允许按其用于分配信用的启发式方法对每种方法进行分类。因此，对于每种方法，我们报告上述三个元素。我们识别出以下类别：
- en: '1.'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Methods using time contiguity as a heuristic (Section [6.1](#S6.SS1 "6.1 Time
    as a heuristic ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")).
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用时间连贯性作为启发式方法的手段（第 [6.1](#S6.SS1 "6.1 Time as a heuristic ‣ 6 Methods to assign
    credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")节）。
- en: '2.'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Those decomposing returns into per-timestep utilities (Section [6.2](#S6.SS2
    "6.2 Decomposing return contributions ‣ 6 Methods to assign credit in Deep RL
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")).
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 那些将回报分解为每个时间步效用的方法（第 [6.2](#S6.SS2 "6.2 Decomposing return contributions ‣ 6
    Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning")节）。
- en: '3.'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Those conditioning on predefined goals explicitly (Section [6.3](#S6.SS3 "6.3
    Conditioning on a predefined set of goals ‣ 6 Methods to assign credit in Deep
    RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")).
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 那些明确条件化于预定义目标的方法（第 [6.3](#S6.SS3 "6.3 Conditioning on a predefined set of goals
    ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")节）。
- en: '4.'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Methods conditioning the present on future outcomes in hindsight (Section [6.4](#S6.SS4
    "6.4 Conditioning in hindsight ‣ 6 Methods to assign credit in Deep RL ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning")).
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以未来结果为后见之明对当前进行条件化的方法（第 [6.4](#S6.SS4 "6.4 Conditioning in hindsight ‣ 6 Methods
    to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")节）。
- en: '5.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Modelling trajectories as sequences (Section [6.5](#S6.SS5 "6.5 Modelling transitions
    as sequences ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning")).
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将轨迹建模为序列（第 [6.5](#S6.SS5 "6.5 Modelling transitions as sequences ‣ 6 Methods
    to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")节）。
- en: '6.'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Those planning or learning backwards from an outcome (Section [6.6](#S6.SS6
    "6.6 Planning and learning backwards ‣ 6 Methods to assign credit in Deep RL ‣
    A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")).
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 那些从结果进行反向规划或学习的方法（第 [6.6](#S6.SS6 "6.6 Planning and learning backwards ‣ 6 Methods
    to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")节）。
- en: '7.'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: Meta-learning different proxies for credit (Section [6.7](#S6.SS7 "6.7 Meta-learning
    proxies for credit ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")).
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 针对信用进行元学习的不同代理（第 [6.7](#S6.SS7 "6.7 Meta-learning proxies for credit ‣ 6 Methods
    to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")节）。
- en: Note that, we do not claim that this list of methods is exhaustive. Rather,
    as for Section [4.5](#S4.SS5 "4.5 Existing assignment functions ‣ 4 Quantifying
    action influences ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"), this taxonomy is representative of the main approaches to assign credit,
    and a tool to understand the current state of the art in the field. We are keen
    to receive feedback on missing methods from the list to improve further revisions
    of the manuscript.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们并不声称这份方法清单是详尽无遗的。相反，如第 [4.5](#S4.SS5 "4.5 Existing assignment functions
    ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning")节所示，这个分类法代表了分配信用的主要方法，并且是理解该领域当前技术水平的工具。我们希望收到有关遗漏方法的反馈，以进一步改进稿件的修订。
- en: To simplify the reading, we group the classes into subsections, and format each
    method into its own paragraph. Each method contains a brief description of the
    intuition, how it is employed to assign credit, and a specification of the context,
    the action value it measures, and the way it learns that quantity from experience.
    We now proceed to describe the methods, which we also summarise in  [4](#S6.T4
    "Table 4 ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning").
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化阅读，我们将类分组到子章节中，并将每种方法格式化为单独的段落。每种方法都包含直观描述、如何用来分配信用的说明，以及上下文的规范、所测量的行动价值和从经验中学习该数量的方法。我们现在开始描述这些方法，我们也在[4](#S6.T4
    "Table 4 ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning")中进行了总结。
- en: Publication Method Class Depth Density Breadth Klopf, ([1972](#bib.bib92)) ET
    Time $\bullet$ $\circ$ $\circ$ Sutton et al., ([2016](#bib.bib182)) ETD Time $\bullet$
    $\circ$ $\circ$ Baird, ([1999](#bib.bib15)) AL Time $\circ$ $\circ$ $\bullet$
    Pan et al., ([2022](#bib.bib128)) DAE Time $\circ$ $\circ$ $\bullet$ [Ferret et al.,
    2021b](#bib.bib47) SAIL Time $\circ$ $\bullet$ $\bullet$ Hung et al., ([2019](#bib.bib75))
    TVT Return decomposition $\bullet$ $\circ$ $\circ$ Arjona-Medina et al., ([2019](#bib.bib7))
    RUDDER Return decomposition $\bullet$ $\circ$ $\circ$ [Ferret et al., 2021a](#bib.bib46)
    SECRET Return decomposition $\bullet$ $\bullet$ $\circ$ Ren et al., ([2022](#bib.bib148))
    RRD Return decomposition $\bullet$ $\circ$ $\circ$ Raposo et al., ([2021](#bib.bib146))
    SR Return decomposition $\bullet$ $\circ$ $\circ$ Sutton et al., ([2011](#bib.bib183))
    GVF Auxiliary goals $\circ$ $\bullet$ $\circ$ [Schaul et al., 2015a](#bib.bib153)
    UVFA Auxiliary goals $\circ$ $\bullet$ $\circ$ Andrychowicz et al., ([2017](#bib.bib5))
    HER Future-conditioning $\circ$ $\bullet$ $\circ$ Rauber et al., ([2019](#bib.bib147))
    HPG Future-conditioning $\circ$ $\bullet$ $\circ$ Harutyunyan et al., ([2019](#bib.bib67))
    HCA Future-conditioning $\circ$ $\bullet$ $\circ$ Schmidhuber, ([2019](#bib.bib157))
    UDRL Future-conditioning $\circ$ $\bullet$ $\circ$ Mesnard et al., ([2021](#bib.bib112))
    CCA Future-conditioning $\circ$ $\bullet$ $\bullet$ Nota et al., ([2021](#bib.bib124))
    PPG Future-conditioning $\circ$ $\bullet$ $\bullet$ Venuto et al., ([2022](#bib.bib195))
    PGIF Future-conditioning $\circ$ $\bullet$ $\circ$ Buesing et al., ([2019](#bib.bib29))
    CBPS Future-conditioning $\circ$ $\bullet$ $\bullet$ Janner et al., ([2021](#bib.bib78))
    TT Sequence modelling $\circ$ $\bullet$ $\circ$ Chen et al., ([2021](#bib.bib33))
    DT Sequence modelling $\circ$ $\bullet$ $\circ$ Zheng et al., ([2022](#bib.bib217))
    ODT Sequence modelling $\circ$ $\bullet$ $\circ$ Furuta et al., ([2022](#bib.bib52))
    GDT Sequence modelling $\circ$ $\bullet$ $\circ$ Goyal et al., ([2019](#bib.bib56))
    Recall traces Backward planning $\circ$ $\bullet$ $\bullet$ Edwards et al., ([2018](#bib.bib40))
    FBRL Backward planning $\circ$ $\bullet$ $\bullet$ Nair et al., ([2020](#bib.bib121))
    TRASS Backward planning $\circ$ $\bullet$ $\bullet$ Wang et al., ([2021](#bib.bib200))
    ROMI Backward planning $\circ$ $\bullet$ $\bullet$ Lai et al., ([2020](#bib.bib96))
    BMPO Backward planning $\circ$ $\bullet$ $\bullet$ van Hasselt et al., ([2021](#bib.bib190))
    ET($\lambda$) Learning predecessors $\bullet$ $\circ$ $\bullet$ Xu et al., ([2018](#bib.bib211))
    MG Meta-Learning $\bullet$ $\circ$ $\circ$ Xu et al., ([2020](#bib.bib210)) FRODO
    Meta-Learning $\bullet$ $\circ$ $\circ$ Yin et al., ([2023](#bib.bib213)) Distr.
    MG Meta-Learning $\bullet$ $\circ$ $\circ$
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 发表方法 类别 深度 密度 广度 Klopf, ([1972](#bib.bib92)) ET 时间 $\bullet$ $\circ$ $\circ$
    Sutton 等, ([2016](#bib.bib182)) ETD 时间 $\bullet$ $\circ$ $\circ$ Baird, ([1999](#bib.bib15))
    AL 时间 $\circ$ $\circ$ $\bullet$ Pan 等, ([2022](#bib.bib128)) DAE 时间 $\circ$ $\circ$
    $\bullet$ [Ferret 等, 2021b](#bib.bib47) SAIL 时间 $\circ$ $\bullet$ $\bullet$ Hung
    等, ([2019](#bib.bib75)) TVT 回报分解 $\bullet$ $\circ$ $\circ$ Arjona-Medina 等, ([2019](#bib.bib7))
    RUDDER 回报分解 $\bullet$ $\circ$ $\circ$ [Ferret 等, 2021a](#bib.bib46) SECRET 回报分解
    $\bullet$ $\bullet$ $\circ$ Ren 等, ([2022](#bib.bib148)) RRD 回报分解 $\bullet$ $\circ$
    $\circ$ Raposo 等, ([2021](#bib.bib146)) SR 回报分解 $\bullet$ $\circ$ $\circ$ Sutton
    等, ([2011](#bib.bib183)) GVF 辅助目标 $\circ$ $\bullet$ $\circ$ [Schaul 等, 2015a](#bib.bib153)
    UVFA 辅助目标 $\circ$ $\bullet$ $\circ$ Andrychowicz 等, ([2017](#bib.bib5)) HER 未来条件化
    $\circ$ $\bullet$ $\circ$ Rauber 等, ([2019](#bib.bib147)) HPG 未来条件化 $\circ$ $\bullet$
    $\circ$ Harutyunyan 等, ([2019](#bib.bib67)) HCA 未来条件化 $\circ$ $\bullet$ $\circ$
    Schmidhuber, ([2019](#bib.bib157)) UDRL 未来条件化 $\circ$ $\bullet$ $\circ$ Mesnard
    等, ([2021](#bib.bib112)) CCA 未来条件化 $\circ$ $\bullet$ $\bullet$ Nota 等, ([2021](#bib.bib124))
    PPG 未来条件化 $\circ$ $\bullet$ $\bullet$ Venuto 等, ([2022](#bib.bib195)) PGIF 未来条件化
    $\circ$ $\bullet$ $\circ$ Buesing 等, ([2019](#bib.bib29)) CBPS 未来条件化 $\circ$ $\bullet$
    $\bullet$ Janner 等, ([2021](#bib.bib78)) TT 序列建模 $\circ$ $\bullet$ $\circ$ Chen
    等, ([2021](#bib.bib33)) DT 序列建模 $\circ$ $\bullet$ $\circ$ Zheng 等, ([2022](#bib.bib217))
    ODT 序列建模 $\circ$ $\bullet$ $\circ$ Furuta 等, ([2022](#bib.bib52)) GDT 序列建模 $\circ$
    $\bullet$ $\circ$ Goyal 等, ([2019](#bib.bib56)) 召回痕迹 向后规划 $\circ$ $\bullet$ $\bullet$
    Edwards 等, ([2018](#bib.bib40)) FBRL 向后规划 $\circ$ $\bullet$ $\bullet$ Nair 等,
    ([2020](#bib.bib121)) TRASS 向后规划 $\circ$ $\bullet$ $\bullet$ Wang 等, ([2021](#bib.bib200))
    ROMI 向后规划 $\circ$ $\bullet$ $\bullet$ Lai 等, ([2020](#bib.bib96)) BMPO 向后规划 $\circ$
    $\bullet$ $\bullet$ van Hasselt 等, ([2021](#bib.bib190)) ET($\lambda$) 学习前驱 $\bullet$
    $\circ$ $\bullet$ Xu 等, ([2018](#bib.bib211)) MG 元学习 $\bullet$ $\circ$ $\circ$
    Xu 等, ([2020](#bib.bib210)) FRODO 元学习 $\bullet$ $\circ$ $\circ$ Yin 等, ([2023](#bib.bib213))
    Distr. MG 元学习 $\bullet$ $\circ$ $\circ$
- en: 'Table 4: List of the most representative algorithms for CA classified by the
    CA challenge they aim to address. For each method we report the publication that
    proposed it, the class we assigned to it, and whether it is designed to address
    each challenge described in Section [5](#S5 "5 The challenges to assign credit
    in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning").
    Hollow circles mean that the method does not address the challenge and the full
    circle represents the opposite.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：按CA挑战分类的最具代表性的CA算法列表。对于每种方法，我们报告了提出该方法的出版物、我们分配给它的类别以及它是否旨在解决第[5](#S5 "5
    The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")节中描述的每个挑战。空心圆表示该方法未解决该挑战，实心圆则表示相反。
- en: 6.1 Time as a heuristic
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 时间作为启发式方法
- en: 'One common way to assign credit is to use time contiguity as a proxy for causality:
    an action is as influential as it is temporally close to the outcome. This means
    that, regardless of the action being an actual cause of the outcome, if the action
    and the outcome appear temporally close in the same trajectory, the action is
    assigned high credit. At their foundation, there is TD learning (Sutton,, [1988](#bib.bib177)),
    which we describe below.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的信用分配方法是使用时间连续性作为因果关系的代理：一个动作的影响力与其在时间上接近结果的程度相关。这意味着，不论动作是否是结果的实际原因，如果动作和结果在同一轨迹上时间上接近，则该动作会被赋予较高的信用。其基础是TD学习（Sutton，[1988](#bib.bib177)），我们将在下文中描述。
- en: TD learning
  id: totrans-304
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: TD学习
- en: (Sutton,, [1984](#bib.bib176), [1988](#bib.bib177); Sutton and Barto,, [2018](#bib.bib181))
    iteratively updates an initial guess of the value function according to the differences
    between expected and observed outcomes. More specifically, the agent starts with
    an initial guess of values, acts in the environment, observes returns, and aligns
    the current guess with the observed return. The difference between the expected
    return and the observed one is the TD error.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: （Sutton，[1984](#bib.bib176)，[1988](#bib.bib177)；Sutton和Barto，[2018](#bib.bib181)）根据期望结果和观察结果之间的差异，迭代更新价值函数的初始猜测。更具体地说，代理从初始价值猜测开始，在环境中进行操作，观察回报，并将当前猜测与观察到的回报对齐。期望回报和观察回报之间的差异是TD误差。
- en: 'When the temporal distance between the goal and the action is high – a premise
    at the base of the CAP– it is often impractical to observe very far rewards. As
    time grows, so does the variance of the observed outcome, due to intrinsic stochasticity
    in the environment dynamics, the reward function, or the policy. To mitigate the
    issue, TD methods often replace the theoretical measure of influence with a an
    approximation: the TD target. Instead of updating the current guess on the observed
    return, these methods use the sum of discounted rewards observed for arbitrary
    $n$ steps, plus their current value estimate at the last step observed. This is
    referred to as boostrapping (Sutton and Barto,, [2018](#bib.bib181)) and allows
    to write the current action influence as a function of a future one. As a consequence,
    the TD target is what drives the learning process. In GPI schemes, the value function
    is updated to approximate the target, and not the theoretical action influence
    measure behind it, even if eventually it may converge to it (Sutton and Barto,,
    [2018](#bib.bib181); van Hasselt et al.,, [2018](#bib.bib188), Chapter 11.3).
    Since policy improvement uses the current approximation of the value to update
    the policy, future behaviour are shaped according to it.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标与动作之间的时间距离较大时——这是CAP的基础前提——观察到非常远的奖励通常是不切实际的。随着时间的推移，由于环境动态、奖励函数或策略中的固有随机性，观察结果的方差也会增加。为了解决这个问题，TD方法通常用一个近似值：TD目标来代替理论影响度的测量。这些方法不是根据观察到的回报更新当前猜测，而是使用在任意$n$步中观察到的折扣奖励之和，加上在最后一步观察到的当前值估计。这被称为引导（Sutton和Barto，[2018](#bib.bib181)），并允许将当前动作的影响写成未来动作的函数。因此，TD目标是驱动学习过程的关键。在GPI方案中，价值函数被更新以逼近目标，而不是其背后的理论动作影响度测量，尽管最终它可能会收敛到该理论值（Sutton和Barto，[2018](#bib.bib181)；van
    Hasselt等，[2018](#bib.bib188)，第11.3章）。由于策略改进使用当前的价值近似值来更新策略，因此未来行为会根据它进行调整。
- en: 'We separate the methods in this category in three subgroups: those specifically
    designed around the advantage function, those re-weighing updates, and those assigning
    credit to sets of temporally extended courses of actions.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这一类别的方法分为三个子组：那些专门围绕优势函数设计的方法，那些重新加权更新的方法，以及那些将信用分配给一系列时间扩展的动作过程的方法。
- en: 6.1.1 Advantage-based approaches
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 基于优势的方法
- en: The first subset of methods use some form of advantage (see Section [4.5](#S4.SS5
    "4.5 Existing assignment functions ‣ 4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning")) as a measure of
    action influence but still uses time as a heuristic to learn it.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类方法使用某种形式的优势（见第[4.5节](#S4.SS5 "4.5 Existing assignment functions ‣ 4 Quantifying
    action influences ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")）作为行动影响的度量，但仍然使用时间作为启发式方法来学习它。
- en: Policy Gradient (PG) and Actor-Critic (AC)
  id: totrans-310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 策略梯度（PG）和演员-评论员（AC）
- en: methods with a baseline function (Sutton and Barto,, [2018](#bib.bib181), Chapter 13)
    approximate advantage to measure action influence when using the value function
    as a baseline. In fact, the policy gradient is proportional to $\mathbb{E}_{\mu,\pi}[(Q^{\pi}(s,a)-b(s))\nabla\log\pi(A|s)]$
    and if we choose $v(s)$ as our baseline $b(s)$, we get $\mathbb{E}_{\mu,\pi}[(A^{\pi}(s,a))\nabla\log\pi(A|s)]$
    because $q^{\pi}(s,a)-v^{\pi}(s,a)=A^{\pi}(s,a)$. The use an action-independent
    baseline function usually helps to reduce the variance of the policy gradients,
    while maintaining an unbiased estimate of it (Sutton and Barto,, [2018](#bib.bib181)).
    What function to use as a baseline is the subject of major studies and different
    choices of baselines often yield methods that go beyond using time as a heuristic
    (Harutyunyan et al.,, [2019](#bib.bib67); Mesnard et al.,, [2021](#bib.bib112);
    Nota et al.,, [2021](#bib.bib124); Mesnard et al.,, [2023](#bib.bib111)).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 带有基线函数的方法（Sutton and Barto,, [2018](#bib.bib181), 第13章）在使用价值函数作为基线时，通过近似优势来衡量行动影响。实际上，策略梯度与$\mathbb{E}_{\mu,\pi}[(Q^{\pi}(s,a)-b(s))\nabla\log\pi(A|s)]$成正比，如果我们选择$v(s)$作为我们的基线$b(s)$，则得到$\mathbb{E}_{\mu,\pi}[(A^{\pi}(s,a))\nabla\log\pi(A|s)]$，因为$q^{\pi}(s,a)-v^{\pi}(s,a)=A^{\pi}(s,a)$。使用与动作无关的基线函数通常有助于减少策略梯度的方差，同时保持其无偏估计（Sutton
    and Barto,, [2018](#bib.bib181)）。选择什么函数作为基线是主要的研究课题，不同的基线选择通常会产生超越使用时间作为启发式的方法的结果（Harutyunyan
    et al., [2019](#bib.bib67)；Mesnard et al., [2021](#bib.bib112)；Nota et al., [2021](#bib.bib124)；Mesnard
    et al., [2023](#bib.bib111)）。
- en: Advantage Learning (AL)
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优势学习（AL）
- en: '(Baird,, [1999](#bib.bib15)) also uses time as a proxy for causality. However,
    rather than using $q$-values as a proxy for credit, AL uses the advantage $A_{t}=q^{\pi}(s_{t},a_{t})-v(s_{t})$,
    which, despite not being a measure of causal influence, it improves on the q-value
    by providing a measure of the relative importance of the action. There are many
    instances of AL in the Deep RL literature. Duelling Deep Q-Network (DQN) ([Wang
    et al., 2016b,](#bib.bib204) ) improves on DQN by replacing the q-value with the
    advantage as a proxy for credit. In these methods the action influence is measured
    by the advantage:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: (Baird,, [1999](#bib.bib15))也使用时间作为因果关系的代理。然而，与使用$q$-值作为信用代理不同，AL使用优势$A_{t}=q^{\pi}(s_{t},a_{t})-v(s_{t})$，尽管这不是因果影响的度量，但它通过提供动作的相对重要性度量来改善q-值。Deep
    RL文献中有许多AL的实例。对抗深度Q网络（DQN）（[Wang et al., 2016b](#bib.bib204)）通过用优势替代q-值作为信用代理来改进DQN。在这些方法中，行动影响通过优势来衡量：
- en: '|  | $\displaystyle K(c,a,g)=A^{\pi}(s,a).$ |  | (18) |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K(c,a,g)=A^{\pi}(s,a).$ |  | (18) |'
- en: The context $c$ is an MDP state $c=s\in\mathcal{S}$, the action is the greedy
    action with respect to the current advantage estimation, and the goal is the maximum
    expected return of an optimal policy $g=z^{*}\in\mathbb{R}$. The advantage, and
    its effects of using it as a proxy for credit, has been further investigated by
    measuring the action-gap (Bellemare et al.,, [2016](#bib.bib27)), the difference
    between the highest and the second-highest action value.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文$c$是MDP状态$c=s\in\mathcal{S}$，动作是相对于当前优势估计的贪婪动作，目标是最优策略的最大期望回报$g=z^{*}\in\mathbb{R}$。优势及其作为信用代理的效果已经通过测量行动间隙（Bellemare
    et al., [2016](#bib.bib27)），即最高和第二高动作值之间的差异进一步研究。
- en: Direct Advantage Estimation (DAE)
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 直接优势估计（DAE）
- en: (Pan et al.,, [2022](#bib.bib128)) exploits the idea that $\mathbb{E}_{\pi}[A^{\pi}(s,a)]=0$
    to estimate the advantage directly from data and not as the usual difference $A^{\pi}(s,a)=q^{\pi}(s,a)-v^{\pi}(s)$.
    Because this method does not approximate $q$ or $v$ as an intermediate step to
    $A\pi$, DAE is limited to full Monte-Carlo returns. This way of estimating the
    advantage has the further drawback of not allowing bootstrapping anymore. In the
    canonical way of estimating advantage, $q^{\pi}(s,a)$ or $v^{\pi}(s)$ can tell
    how valuable the current state is, which is necessary to bootstrap the following
    values. However, since we do not have estimations of neither $q^{\pi}(s,a)$ or
    $v^{\pi}(s)$, but only of $A^{\pi}(s,a)$ directly, we cannot learn the advantage
    function with bootstrapping. The influence of an action in DAE is the same as
    for canonical advantage learning.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: （Pan 等人，[2022](#bib.bib128)）利用 $\mathbb{E}_{\pi}[A^{\pi}(s,a)]=0$ 的思想直接从数据中估计优势，而不是像通常那样通过
    $A^{\pi}(s,a)=q^{\pi}(s,a)-v^{\pi}(s)$ 计算。由于这种方法不将 $q$ 或 $v$ 作为中间步骤来近似 $A^{\pi}$，DAE
    限制于完整的蒙特卡罗回报。这种估计优势的方法还有一个额外的缺点，即不再允许引导。按照传统方式估计优势时，$q^{\pi}(s,a)$ 或 $v^{\pi}(s)$
    可以告诉我们当前状态的价值，这是引导后续值所必需的。然而，由于我们没有 $q^{\pi}(s,a)$ 或 $v^{\pi}(s)$ 的估计，而只有直接的 $A^{\pi}(s,a)$，我们不能通过引导来学习优势函数。在
    DAE 中，动作的影响与传统的优势学习相同。
- en: Self-Imitation Advantage Estimation (SAIL)
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自我模仿优势估计（SAIL）
- en: ([Ferret et al., 2021b,](#bib.bib47) ) combines AL with self-imitation learning
    into an off-policy algorithm that increases the action-gap (Bellemare et al.,,
    [2016](#bib.bib27)). As for DAE, it differs from previous methods by the protocol
    it uses to estimate the advantage from experience and combines advantage learning
    with self-imitation learning (Oh et al.,, [2018](#bib.bib125)).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: （[Ferret 等人，2021b，](#bib.bib47)）将主动学习与自我模仿学习结合成一种离线策略算法，增加了动作间隔（Bellemare 等人，[2016](#bib.bib27)）。至于
    DAE，它通过用于从经验中估计优势的协议与之前的方法不同，并将优势学习与自我模仿学习结合起来（Oh 等人，[2018](#bib.bib125)）。
- en: 6.1.2 Re-weighing updates and compound targets
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 重新加权更新和复合目标
- en: The second subset of methods in this category re-weighs temporal updates according
    to some heuristics. Re-weighing updates can be useful to emphasise or de-emphasise
    important states or actions.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 本类别的第二类方法根据一些启发式规则重新加权时间更新。重新加权更新可以用于强调或减轻重要状态或动作的影响。
- en: Eligibility Traces (ET)
  id: totrans-322
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 资格迹（ET）
- en: '(Klopf,, [1972](#bib.bib92); Singh and Sutton,, [1996](#bib.bib168); [Precup,
    2000a,](#bib.bib138) ; Geist et al.,, [2014](#bib.bib55); Mousavi et al.,, [2017](#bib.bib119))
    credit the long-term impact of actions on future rewards by keeping track of the
    influence of past actions on the agent’s future reward. Specifically, an eligibility
    trace (Sutton and Barto,, [2018](#bib.bib181), Chapter 12) is a function that
    assigns a weight to each state-action pair, based on the recency of the state-action
    pair. A trace spikes every time a state-action is visited and decays exponentially
    over time until the next visit. There are several types of eligibility traces,
    depending on the law of decay of the trace, for example, Klopf, ([1972](#bib.bib92));
    Singh and Sutton, ([1996](#bib.bib168)). Overall, they often admit two equivalent
    implementations: the forward and the backward view, which differ both in the context
    and the action value that they measure. For the reasons described in Section [1](#S1.SS0.SSS0.Px3
    "Scope. ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"), we focus on their Deep RL formulation, which mostly implements their
    backward view. Deep Q$(\lambda)$-Network (DQ$(\lambda)$N) (Mousavi et al.,, [2017](#bib.bib119))
    implement eligibility traces on top of a DQN (Mnih et al.,, [2015](#bib.bib118)).
    In ETs with deep function approximation, the eligibility trace is a vector $e\in\mathbb{R}^{d}$
    with the same number of components $d$ as the parameters of the DNN, and the action
    influence is measured by $q$-value with parameters set $\theta\in\mathbb{R}^{d}$:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: (Klopf，， [1972](#bib.bib92)；Singh 和 Sutton，， [1996](#bib.bib168)；[Precup, 2000a,](#bib.bib138)；Geist
    等， [2014](#bib.bib55)；Mousavi 等， [2017](#bib.bib119)) 通过跟踪过去动作对代理未来奖励的影响，强调了动作对未来奖励的长期影响。具体来说，资格迹（Sutton
    和 Barto，， [2018](#bib.bib181)，第12章）是一个函数，它根据状态-动作对的近期程度为每个状态-动作对分配一个权重。每次访问状态-动作对时，迹会激增，并随着时间的推移指数衰减直到下次访问。根据迹的衰减规律，资格迹有几种类型，例如，Klopf，（[1972](#bib.bib92)）；Singh
    和 Sutton，（[1996](#bib.bib168)）。总体而言，它们通常有两种等效的实现：前向视角和后向视角，它们在上下文和测量的动作值上有所不同。由于[1](#S1.SS0.SSS0.Px3
    "Scope. ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")节中描述的原因，我们关注它们的深度RL公式，它主要实现了它们的后向视角。深度Q$(\lambda)$-网络（DQ$(\lambda)$N）（Mousavi
    等， [2017](#bib.bib119)）在DQN（Mnih 等， [2015](#bib.bib118)）之上实现了资格迹。在具有深度函数逼近的ET中，资格迹是一个向量$e\in\mathbb{R}^{d}$，其分量数$d$与DNN的参数相同，动作影响通过$q$-值测量，参数集为$\theta\in\mathbb{R}^{d}$：
- en: '|  | $\displaystyle K(c,a,g)=q^{\pi}(s,a,\theta).$ |  | (19) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K(c,a,g)=q^{\pi}(s,a,\theta).$ |  | (19) |'
- en: 'The context $c$ is an MDP state $c=s\in\mathcal{S}$, the action is either sampled
    from the policy for on-policy methods (e.g., SARSA($\lambda$)) or arbitrarily
    chosen for off-policy methods (e.g., Q($\lambda$)), and the goal is the maximum
    expected return of an optimal policy $g=z^{*}\in\mathbb{R}$. The ET information
    is embedded in the parameters $\theta$ since they are updated according to $\theta\leftarrow\theta+\delta
    e$. $\delta$ is a TD error and $e$ is the eligibility trace, incremented at each
    update by the value gradient (Sutton and Barto,, [2018](#bib.bib181), Chapter 12):
    $e\leftarrow\gamma\lambda e+\nabla_{\theta}q^{\pi}(s,a)$. Notice the dependence
    on $Z_{t}$, which underlines that the method is a backward method. Successive
    works advanced on the idea of ETs, and proposed different updates for the eligibility
    vector (Singh and Sutton,, [1996](#bib.bib168); van Hasselt and Sutton,, [2015](#bib.bib191);
    [Precup, 2000a,](#bib.bib138) ).'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文$c$是一个MDP状态$c=s\in\mathcal{S}$，动作要么从策略中采样（如，SARSA($\lambda$)）用于在策略方法，要么为离策略方法（如，Q($\lambda$)）任意选择，目标是最优策略的最大期望回报$g=z^{*}\in\mathbb{R}$。ET信息嵌入在参数$\theta$中，因为它们根据$\theta\leftarrow\theta+\delta
    e$进行更新。$\delta$是TD误差，$e$是资格迹，每次更新时由值梯度增加（Sutton 和 Barto，， [2018](#bib.bib181)，第12章）：$e\leftarrow\gamma\lambda
    e+\nabla_{\theta}q^{\pi}(s,a)$。注意到对$Z_{t}$的依赖，这表明该方法是一个向后方法。随后的研究推进了ET的思想，并提出了不同的资格向量更新方法（Singh
    和 Sutton，， [1996](#bib.bib168)；van Hasselt 和 Sutton，， [2015](#bib.bib191)；[Precup,
    2000a,](#bib.bib138)）。
- en: Emphatic Temporal Differences (ETDs)
  id: totrans-326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 强调时间差分（ETDs）
- en: (Sutton et al.,, [2016](#bib.bib182); Mahmood et al.,, [2015](#bib.bib109);
    [Jiang et al., 2021b,](#bib.bib83) ) improves on the instabilities of ET by re-weighing
    state-action updates. The re-weighing is based on the emphatic trace, a per-parameter
    value that encodes the degree of bootstrapping of a state. The intuition behind
    ETDs is that states with high (low) uncertainty – states whose estimates result
    from heavy (soft) bootstrapping – are less (more) reliable. The main adaptation
    of the algorithm to Deep RL is by [Jiang et al., 2021b](#bib.bib83) , who propose
    the Windowed Emphatic TD ($\lambda$) (WETD) algorithm that measures the emphatic
    trace in temporal window of $n$ steps. The influence of an action in WETD is the
    same as for any other ET, but the trace itself is different, and measures the
    amount of bootstrapping of the current estimate. ETDs provide an additional mechanism
    to re-weigh updates, the interest function $i:\mathcal{S}\rightarrow[0,\infty)$.
    By emphasising or de-emphasising the interest on a state, the interest function
    can be a helpful tool to encode the influence of the actions that had led to that
    state. Because hand-crafting an interest function requires to important human
    effort, and the result might be suboptimal,Klissarov et al., ([2022](#bib.bib90))
    proposes a method to learn and adapt the interest function at each update using
    meta-gradients. Improvements on both discrete control, such as ALE, and on continuous
    control problems, such as MuJoCo (Todorov et al.,, [2012](#bib.bib187)), suggests
    that the interest function can be helpful to assign credit faster and more accurately.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: (Sutton 等人, [2016](#bib.bib182); Mahmood 等人, [2015](#bib.bib109); [Jiang 等人,
    2021b,](#bib.bib83) ) 通过重新加权状态-动作更新来改善 ET 的不稳定性。重新加权基于强调性迹，这是一种每个参数的值，用于编码状态的自举程度。ETD
    的直觉是，高（低）不确定性的状态——估计结果来源于大量（少量）自举的状态——不那么（更）可靠。该算法对深度强化学习的主要适应由 [Jiang 等人, 2021b](#bib.bib83)
    提出，他们提出了测量在 $n$ 步时间窗口内的强调性迹的 Windowed Emphatic TD ($\lambda$)（WETD）算法。WETD 中一个动作的影响与其他
    ET 相同，但迹本身不同，测量当前估计的自举量。ETD 提供了一种额外的机制来重新加权更新，即兴趣函数 $i:\mathcal{S}\rightarrow[0,\infty)$。通过强调或减轻对状态的兴趣，兴趣函数可以成为编码导致该状态的动作影响的有用工具。由于手动设计兴趣函数需要大量人工努力，并且结果可能不尽如人意，Klissarov
    等人 ([2022](#bib.bib90)) 提出了一个方法，通过元梯度在每次更新时学习和调整兴趣函数。在离散控制（如 ALE）和连续控制问题（如 MuJoCo
    (Todorov 等人, [2012](#bib.bib187))）上的改进表明，兴趣函数可以帮助更快、更准确地分配奖励。
- en: Selective Credit Assignment (SCA)
  id: totrans-328
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择性奖励分配 (SCA)
- en: (Chelu et al.,, [2022](#bib.bib31)) generalises the idea of re-weighting TD
    updates to a generic weighting function.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: (Chelu 等人, [2022](#bib.bib31)) 将重新加权 TD 更新的思想推广到一个通用加权函数。
- en: 6.1.3 Assigning credit to temporally extended actions
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 将奖励分配给时间延续的动作
- en: The third and last subset of methods in this category assigns credit to temporally
    extended actions rather than an action primitive.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别中的第三种和最后一种方法将奖励分配给时间延续的动作，而不是基本动作。
- en: The option framework
  id: totrans-332
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选项框架
- en: (Sutton et al.,, [1999](#bib.bib184); [Precup, 2000b,](#bib.bib139) ) stems
    from the intuition that it is often convenient to engage into strategical choices
    at a higher level of abstractions. In fact, options (or skills in another branch
    of literature (Haarnoja et al.,, [2017](#bib.bib63); Eysenbach et al.,, [2018](#bib.bib42)))
    generalise the concept of action. An option represents temporally extended courses
    of actions that an agent can select, and assign credit to, to produce a specific
    behaviour. Formally, an option is a triple ($\pi$, $\beta$, $\mathcal{S}^{\pi}$)
    where $\pi$ is a policy, $\beta:\mathcal{H}\rightarrow\mathbb{B}$ is a *termination
    condition function* indicating when to cease using the option, and $\mathcal{S}^{\pi}\subset\mathcal{S}$
    is an *initiation set* that determines when the option is available if $s\in\mathcal{S}^{\pi}$
    or not available for use otherwise. The initiation set is often relaxed to the
    whole state space and $\mathcal{S}^{\pi}=\mathcal{S}$. For example, in a key-to-door
    environment, such as MiniGrid (Chevalier-Boisvert et al.,, [2018](#bib.bib36))
    or MiniHack (Samvelyan et al.,, [2021](#bib.bib151)) the agent might select the
    option pick up the key, followed by open the door. Each of this macro-action requires
    a policy to be executed. For example, pick up the key requires to select the actions
    that lead to have key in front before grabbing it. We refer to Sutton et al.,
    ([1999](#bib.bib184), Section 2) for more details on the framework and the execution
    of an option in an MDP.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: （Sutton等，[1999](#bib.bib184)；[Precup, 2000b,](#bib.bib139)）源于这样的直觉：在较高层次的抽象中进行战略选择通常是方便的。事实上，选项（或在其他文献中的技能（Haarnoja等，[2017](#bib.bib63)；Eysenbach等，[2018](#bib.bib42)））扩展了行动的概念。一个选项代表了一个代理可以选择并赋予信用的时间延续的行动过程，以产生特定的行为。形式上，一个选项是一个三元组（$\pi$，$\beta$，$\mathcal{S}^{\pi}$），其中$\pi$是策略，$\beta:\mathcal{H}\rightarrow\mathbb{B}$是*终止条件函数*，指示何时停止使用该选项，$\mathcal{S}^{\pi}\subset\mathcal{S}$是*启动集*，它决定了当$s\in\mathcal{S}^{\pi}$时该选项是否可用，否则不可用。启动集通常被放宽到整个状态空间，$\mathcal{S}^{\pi}=\mathcal{S}$。例如，在一个钥匙-门环境中，例如MiniGrid（Chevalier-Boisvert等，[2018](#bib.bib36)）或MiniHack（Samvelyan等，[2021](#bib.bib151)），代理可能选择选项“捡起钥匙”，然后是“打开门”。每一个宏动作都需要执行策略。例如，捡起钥匙需要选择导致钥匙在前面的位置的动作，然后才能抓取它。有关框架和在MDP中执行选项的更多细节，请参见Sutton等（[1999](#bib.bib184)，第2节）。
- en: 'Yet, the biggest obstacle to learning options is how to choose the option set.
    The majority of works has focused on finding sub-goals (Liu et al.,, [2022](#bib.bib104))
    and learning sub-policies to achieve them. Often in literature these or options
    themselves are pre-specified, which is inflexible as they have to be specified
    for each single task. For this reason, most advancements of the option framework
    to Deep RL focus on how to discover the options set from experimental data. Overall,
    with the option framework, credit is assigned at two levels: at the level of the
    sub-policy (often referred to as intra-option level) and at the extra-option level,
    that is to choose which option to follow. We review works about learning options
    next, and dedicate a separate section to auxiliary goal-conditioning in Section [6.3](#S6.SS3
    "6.3 Conditioning on a predefined set of goals ‣ 6 Methods to assign credit in
    Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning").'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，学习选项的最大障碍是如何选择选项集。大多数研究集中在寻找子目标（刘等， [2022](#bib.bib104)）以及学习实现这些子目标的子策略上。在文献中，这些选项或子策略通常是预先指定的，这种方式缺乏灵活性，因为它们必须为每个单独的任务指定。由于这个原因，大多数选项框架在深度强化学习中的进展都集中在如何从实验数据中发现选项集。总体而言，使用选项框架时，信用分配在两个层次上进行：在子策略层次（通常称为内部选项层次）和外部选项层次，即选择跟随哪个选项。接下来，我们回顾有关学习选项的研究，并在第[6.3](#S6.SS3
    "6.3 Conditioning on a predefined set of goals ‣ 6 Methods to assign credit in
    Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")节中专门讨论辅助目标条件。
- en: The option-critic architecture
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选项-评论架构
- en: '(Bacon et al.,, [2017](#bib.bib12)) scales options to Deep RL and mirrors the
    actor-critic architecture but with options rather than actions. The option-critic
    architecture allows to learn both at the intra-option level together with the
    corresponding termination function, and the policy over them simultaneously. The
    option executes using the call-and-return model. Starting from a state $s$, the
    agent picks an option $\omega$ according to its policy over options $\pi_{\Omega}$.
    This option then determines the primitive action selection process through the
    intra-policy $\pi_{\omega}$ until the option termination function $\beta$ signals
    to stop. Learning options, and assigning credit to its actions, is then possible
    using the intra-option policy gradient and the termination gradient theorems (Bacon
    et al.,, [2017](#bib.bib12)), which define the gradient (thus the corresponding
    update) for all three elements of the learning process: the option $\omega\in\Omega$,
    their termination function $\beta(s)$ and the policy over options $\pi_{\Omega}$.
    Here, the context is a state $s\in\mathcal{S}$, the actions to assign credit to
    are both the intra-option action $a\in\mathcal{A}$ and the option $\omega\in\Omega$,
    and the goal is to maximise the return. Overall, learning with the option-critic
    architecture does not require a special training methodology but allows any method
    used for actor critics.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: (Bacon 等人，[2017](#bib.bib12)) 将选项扩展到深度强化学习，并反映了演员-评论家架构，但使用的是选项而非动作。选项-评论家架构允许在内选项级别与相应的终止函数一起学习，同时学习策略。选项通过调用和返回模型执行。从状态
    $s$ 开始，智能体根据其在选项上的策略 $\pi_{\Omega}$ 选择一个选项 $\omega$。该选项随后通过内策略 $\pi_{\omega}$
    确定原始动作选择过程，直到选项终止函数 $\beta$ 发出停止信号。然后可以使用内选项策略梯度和终止梯度定理（Bacon 等人，[2017](#bib.bib12)）学习选项并分配其动作的奖励，这些定理定义了学习过程的三个元素的梯度（即相应的更新）：选项
    $\omega\in\Omega$、其终止函数 $\beta(s)$ 和选项策略 $\pi_{\Omega}$。在这里，上下文是状态 $s\in\mathcal{S}$，分配奖励的动作包括内选项动作
    $a\in\mathcal{A}$ 和选项 $\omega\in\Omega$，目标是最大化回报。总体而言，使用选项-评论家架构进行学习不需要特殊的训练方法，但允许使用任何用于演员-评论家的方法。
- en: Hierarchical option-critics
  id: totrans-337
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分层选项批评
- en: (Riemer et al.,, [2018](#bib.bib149)) extend the previous results from learning
    options at only two levels (intra and extra options) to learning options at multiple
    hierarchical levels of resolution. However, the hierarchical option-critic architecture
    only generalises to a fixed number of hierarchical levels, which cannot be changed
    during or after training. Since Riemer et al., ([2018](#bib.bib149)) generalises
    Bacon et al., ([2017](#bib.bib12)), they propose a generalisation of both the
    intra-option policy gradient and the termination gradient.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: (Riemer 等人，[2018](#bib.bib149)) 将先前仅在两个层级（内层和外层选项）中的学习选项结果扩展到多个层级的分辨率学习选项。然而，分层选项批评架构仅能推广到固定数量的分层级别，这在训练期间或训练后不能更改。由于
    Riemer 等人（[2018](#bib.bib149)）扩展了 Bacon 等人（[2017](#bib.bib12)）的方法，他们提出了对内选项策略梯度和终止梯度的泛化。
- en: Flexible option learning
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 灵活的选项学习
- en: (Klissarov and Precup,, [2021](#bib.bib91)) improves on the previous methods
    by assigning credit to all options *simultaneously*, rather than a single option
    at a time – the one currently used – while learning in hierarchical settings.
    The main contribution of the work to discovering/learning options is the theoretical
    formulation that makes the above possible. The intuition stems from the way importance
    sampling (Hesterberg,, [1995](#bib.bib70); Sutton et al.,, [2014](#bib.bib175);
    [Precup, 2000a,](#bib.bib138) ) re-weighs off-policy actions using the probabilities
    of the action under the behavioural and that under the target policy. Credit is
    then assigned proportionally to the probability of choosing an option that has
    not been taken.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: (Klissarov 和 Precup，[2021](#bib.bib91)) 通过对所有选项*同时*分配奖励来改进了先前的方法，而不是一次对一个当前使用的选项进行分配。这项工作对发现/学习选项的主要贡献是理论上的公式，使上述成为可能。直觉来自于重要性抽样（Hesterberg，[1995](#bib.bib70)；Sutton
    等人，[2014](#bib.bib175)；[Precup, 2000a,](#bib.bib138)）使用行为政策和目标政策下的动作概率重新加权离策略动作。奖励则按未选择选项的概率进行分配。
- en: Summary.
  id: totrans-341
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 总结。
- en: Overall, the methods that use time as a heuristic directly stem from transferring
    RL concepts to Deep RL, and represent a baseline for the methods that we review
    in the next sections. These methods use either $q$-values or advantages as a measure
    of action influence by actively interacting with an environment (see Section [B](#S2a
    "B Further details on contexts ‣ A Survey of Temporal Credit Assignment in Deep
    Reinforcement Learning")). Time contiguity is the strong bias that distinguishes
    them from others. Recency is used as a proxy for credit and a substitute for the
    causal strength of the relationship between actions and outcomes. While this is
    a reasonable assumption in many cases and allows solving quite complex problems,
    this is not always true. In fact, one of the reasons for which the CAP has not
    taken-off until recent years is that environments were quite simple to solve from
    the CA point of view. This was necessary because the CAP would have overcast the
    others and it wouldn’t have been possible to solve them. Indeed, CA methods do
    not usually shine in classical benchmark but they also do not degrade learning.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，使用时间作为启发式方法的方法直接源于将强化学习（RL）概念转移到深度强化学习（Deep RL），并且代表了我们在接下来的章节中回顾的方法的基准。这些方法使用
    $q$-值或优势作为行动影响的衡量标准，通过与环境的积极互动（见第[B](#S2a "B Further details on contexts ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning)节）来实现。时间连续性是将它们与其他方法区分开的强偏差。近期性被用作信用的代理和行动与结果之间因果关系强度的替代。虽然在许多情况下这是一个合理的假设，并且允许解决相当复杂的问题，但这并非总是如此。事实上，CAP直到最近几年才取得进展的原因之一是从CA角度来看环境相当简单。这是必要的，因为CAP会掩盖其他方法，从而无法解决它们。的确，CA方法在经典基准测试中通常并不突出，但它们也不会降低学习效果。
- en: Especially in conditions of delayed effects, either of a hierarchical nature
    of in the case (key-door) or plain temporal delay (distractors), these methods
    are not the best choice, either because they do not scale well to Deep RL, or
    because they don’t deal with the challenge directly. They are also not designed
    to handle the sparsity of the action influence and do not perform as well as those
    that address these challenges directly. Except AL, which uses the action-gap as
    a proxy for credit, these methods do not incentivise the agent to find multiple
    pathways to the same goal. Today, the research on these methods is not very active,
    as they have been superseded by methods that address the challenges of CAP directly.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在延迟效应的条件下，无论是层次性（关键-门）还是纯时间延迟（干扰项）的情况，这些方法都不是最佳选择，原因要么是它们在深度强化学习中扩展性较差，要么是它们没有直接处理挑战。它们也没有设计来处理行动影响的稀疏性，表现也不如那些直接应对这些挑战的方法。除AL外，AL使用行动间隔作为信用的代理，这些方法并没有激励代理找到多个通向相同目标的路径。今天，这些方法的研究并不活跃，因为它们已经被直接解决CAP挑战的方法所取代。
- en: 6.2 Decomposing return contributions
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 回报贡献的分解
- en: A line of research that aims to overcome the limitation of time-based methods
    in addressing the delayed effect challenge focuses on decomposing returns into
    per-timestep contributions. These methods are heavily based on the idea of reward
    shaping (Ng et al.,, [1999](#bib.bib122)), and often construct a dual decision
    problem in which the future expected reward is $0$ because there are no delayed
    rewards. This reward function then acts as a measure of action influence. They
    interpret the CAP as a redistribution problem. Given an observed return at termination,
    the value of each action in the trajectory is re-distributed to the time-steps
    that it influenced based on what already happened, and not on what will happen,
    such as in the case of forward methods. While introducing a new mechanism to learn
    how to assign credit, these methods still use TD errors, $\delta_{t}=q^{\pi}(s_{t},a_{t})-q^{\pi}(s_{t-1},a_{t-1})$,
    and an action is as creditable as the difference in expected returns between two
    contiguous time steps. We now review the main methods in this category.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列旨在克服基于时间的方法在处理延迟效应挑战中的局限性的研究集中在将回报分解为每个时间步的贡献。这些方法主要基于奖励塑形的思想（Ng et al.,
    [1999](#bib.bib122)），并且通常构建一个双重决策问题，其中未来的预期奖励为 $0$，因为没有延迟奖励。该奖励函数随后作为行动影响的衡量标准。他们将CAP解释为一个重新分配问题。给定一个在终止时观察到的回报，轨迹中每个行动的值被重新分配到它所影响的时间步上，这基于已经发生的事情，而不是将要发生的事情，例如前向方法的情况。尽管引入了一个新的机制来学习如何分配信用，这些方法仍然使用TD误差，$\delta_{t}=q^{\pi}(s_{t},a_{t})-q^{\pi}(s_{t-1},a_{t-1})$，并且一个行动的信用程度等于两个连续时间步之间预期回报的差异。我们现在回顾这一类别中的主要方法。
- en: Temporal Value Transport (TVT)
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 时间值传输（TVT）
- en: '(Hung et al.,, [2019](#bib.bib75)) uses an external memory system to contain
    the loss of action influence due to time. The memory mechanism is based on the
    Differentiable Neural Computer (DNC) (Grefenstette et al.,, [2015](#bib.bib59);
    Graves et al.,, [2016](#bib.bib58)), a neural network then reads and writes events
    to an external memory matrix. To write, state-action-reward triples are projected
    to a lower dimensional space, and processed by the DNC. During training, this
    works as a trigger: when a specific state-action pair is read from memory, it
    is associated with the current one, transporting the value – credit – from the
    present to the remote state. To read, the state-action-reward is reconstructed
    from the latent code. During inference, this act as a proxy for credit, and by
    pointing to past state-action-reward triple that is highly correlated with the
    current return, it measures the influence of the action.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: (Hung et al., [2019](#bib.bib75)) 使用外部记忆系统来容纳因时间导致的动作影响损失。该记忆机制基于可微分神经计算机（DNC）（Grefenstette
    et al., [2015](#bib.bib59); Graves et al., [2016](#bib.bib58)），一个神经网络然后将事件读写到外部记忆矩阵。写入时，状态-动作-奖励三元组被投影到一个低维空间，并由DNC处理。在训练期间，这起到了触发器的作用：当从记忆中读取特定的状态-动作对时，它与当前状态关联，将值——信用——从当前状态传递到远程状态。读取时，从潜在代码中重构状态-动作-奖励。在推理过程中，这作为信用的代理，并通过指向与当前回报高度相关的过去状态-动作-奖励三元组来衡量动作的影响。
- en: Return Decomposition for Delayer Rewards (RUDDER)
  id: totrans-348
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 延迟奖励的回报分解（RUDDER）
- en: (Arjona-Medina et al.,, [2019](#bib.bib7)) stems from the intuition that, if
    we can construct a reward function that redistributes the rewards collected in
    a trajectory such that the expected future reward is zero, we obtain an instantaneous
    signal that immediately informs the agent about future rewards. In practice, a
    function $g(s,a)$ outputs the sum of discounter rewards of a trajectory $d$, including
    the past, present and future rewards. The difference between its output at two
    consecutive time steps represents the influence of the action.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: (Arjona-Medina et al., [2019](#bib.bib7)) 的出发点是，如果我们能构造一个重新分配在轨迹中收集的奖励的奖励函数，使得期望未来奖励为零，我们将得到一个即时信号，立即通知代理未来的奖励。实际上，函数
    $g(s,a)$ 输出轨迹 $d$ 的折扣奖励总和，包括过去、现在和未来的奖励。其在两个连续时间步骤的输出之间的差异代表了动作的影响。
- en: '|  | $\displaystyle K(c,a,g)=g(s_{t-1},a_{t-1})-g(s_{t},a_{t})=R^{*}(s,a),$
    |  | (20) |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K(c,a,g)=g(s_{t-1},a_{t-1})-g(s_{t},a_{t})=R^{*}(s,a),$
    |  | (20) |'
- en: where $R^{*}(s,a)$ is the reward function of $\mathcal{M}^{*}$. The context
    $c$ is a history $h=\{o_{t},a_{t},r_{t}:0\leq t\leq T\}$ from the assigned MDP,
    the action is an action from the trajectory $a\in h$, and the goal is the maximum
    expected return of an optimal policy $g=z^{*}\in\mathbb{R}$.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R^{*}(s,a)$ 是 $\mathcal{M}^{*}$ 的奖励函数。上下文 $c$ 是从指定的MDP中得到的历史 $h=\{o_{t},a_{t},r_{t}:0\leq
    t\leq T\}$，动作是轨迹中的一个动作 $a\in h$，目标是最优策略的最大期望回报 $g=z^{*}\in\mathbb{R}$。
- en: Self-Attentional Credit Assignment for Transfer (SECRET)
  id: totrans-352
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自注意力信用分配（SECRET）
- en: '([Ferret et al., 2021a,](#bib.bib46) ) uses a causal Transformer-like architecture (Vaswani
    et al.,, [2017](#bib.bib194)) with a self-attention mechanism (Lin et al.,, [2017](#bib.bib103))
    in the standalone supervised task of reconstructing the sequence of rewards from
    observations and actions. It then views attention weights over past state-action
    pairs as credit for the generated rewards. This was shown to help with long-term
    CA in a way that transfers to novel tasks when trained over a distribution of
    tasks. We can write its measure of action influence as follows:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ([Ferret et al., 2021a,](#bib.bib46)) 使用了类似Transformer的因果架构（Vaswani et al.,
    [2017](#bib.bib194)）和自注意力机制（Lin et al., [2017](#bib.bib103)），在从观测和动作中重建奖励序列的独立监督任务中。然后，它将过去状态-动作对上的注意力权重视为生成奖励的信用。这被证明有助于长期的信用分配，并在训练时能转移到新任务上。我们可以将其动作影响的度量写为如下形式：
- en: '|  | $\displaystyle K(c,a,g)=\sum_{t=1}^{T}\mathbbm{1}\{S_{t}=s,A_{t}=a\}\sum_{i=t}^{T}\alpha_{t\leftarrow
    i}R(s_{i},a_{i}).$ |  | (21) |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K(c,a,g)=\sum_{t=1}^{T}\mathbbm{1}\{S_{t}=s,A_{t}=a\}\sum_{i=t}^{T}\alpha_{t\leftarrow
    i}R(s_{i},a_{i}).$ |  | (21) |'
- en: Here, $\alpha_{t\leftarrow i}$ is the attention weight on $(o_{i},a_{i})$ when
    predicting the reward $r_{j}$. Also here, the context is a history $h$, the action
    is an action from the trajectory $a\in h$, and the goal is the maximum expected
    return of an optimal policy $g=z^{*}\in\mathbb{R}$.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\alpha_{t\leftarrow i}$ 是预测奖励 $r_{j}$ 时对 $(o_{i},a_{i})$ 的注意权重。这里的上下文是历史
    $h$，动作是轨迹中的一个动作 $a\in h$，目标是最优策略的最大期望回报 $g=z^{*}\in\mathbb{R}$。
- en: Randomised Return Decomposition (RRD)
  id: totrans-356
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 随机回报分解（RRD）
- en: (Ren et al.,, [2022](#bib.bib148)) advances the idea of return decomposition
    presented by Arjona-Medina et al., ([2019](#bib.bib7)) further. The method assumes
    that a small set of subsequences that compose a trajectory is responsible for
    the terminal reward (or the return). A reward model is then trained to predict
    the episodic return, given the subset of transitions that are randomly sampled
    from the trajectory. Because this method proposes the same formulation as Arjona-Medina
    et al., ([2019](#bib.bib7)), its action influence is $R^{*}(s,a)$, with the difference
    that Randomised Return Decomposition (RRD) directly optimises for the reward function.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: (Ren等人，[2022](#bib.bib148)) 进一步发展了Arjona-Medina等人（[2019](#bib.bib7)）提出的回报分解思想。该方法假设组成一个轨迹的小子序列集负责终端奖励（或回报）。然后训练一个奖励模型来预测给定从轨迹中随机采样的转换子集的情节回报。因为该方法提出了与Arjona-Medina等人（[2019](#bib.bib7)）相同的公式，其行动影响为$R^{*}(s,a)$，不同之处在于随机回报分解（RRD）直接优化奖励函数。
- en: Synthetic returns (SR)
  id: totrans-358
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 合成回报（SR）
- en: '(Raposo et al.,, [2021](#bib.bib146)) assume only one state-action to be responsible
    for the terminal reward. They propose a form of state pairs association where
    the earlier state (the operant) is a leading indicator of the reward obtained
    in the later one (the reinforcer). The association model is learned using a form
    of episodic memory. Each entry in the memory buffer, which holds the states visited
    in the current episode, is associated with a reward – the synthetic reward – via
    supervised learning. At training time, this allows propagating credit directly
    from the reinforcer to the operant at-a-distance: without local temporal difference.
    At inference time, when this reward model is accurately learned, each time the
    operant is observed, the synthetic reward model spikes, indicating a creditable
    state-action pair. Here the synthetic reward acts as a measure of causal influence,
    and we write:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: (Raposo等人，[2021](#bib.bib146)) 假设只有一个状态-动作对负责终端奖励。他们提出了一种状态对关联的形式，其中较早的状态（操作员）是后一个状态（强化器）获得奖励的前导指标。关联模型使用一种形式的情节记忆进行学习。记忆缓冲区中的每个条目（保存当前情节中访问的状态）通过监督学习与奖励——合成奖励——相关联。在训练时，这允许直接从强化器到操作员的远程传播信用：没有局部时间差。在推断时，当这个奖励模型准确学习时，每当观察到操作员时，合成奖励模型会激增，表明这是一个值得归功的状态-动作对。这里，合成奖励作为因果影响的衡量标准，我们写：
- en: '|  | $\displaystyle K(c,a,g)=q^{\pi}(s,a)+c(s).$ |  | (22) |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K(c,a,g)=q^{\pi}(s,a)+c(s).$ |  | (22) |'
- en: Here $c(s)$ is the synthetic reward function and it is trained with value regression
    on the loss $||r_{t}-g(s_{t})\sum_{k=0}^{t-1}c(s_{t})-b(s_{t})||^{2}$, where $g(s_{t})$
    and $b(s_{t})$ are auxiliary neural networks optimised together with $c$. As for
    Arjona-Medina et al., ([2019](#bib.bib7)), the context $c$ is a history $h$ from
    the assigned MDP, the action is an action from the trajectory $a\in h$, and the
    goal is the maximum expected return of an optimal policy $g=z^{*}\in\mathbb{R}$.
    This method is, however, stable only within a narrow range of hyperparameters
    and assumes that only one single action is to be credited.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$c(s)$ 是合成奖励函数，它通过在损失 $||r_{t}-g(s_{t})\sum_{k=0}^{t-1}c(s_{t})-b(s_{t})||^{2}$
    上进行值回归来训练，其中 $g(s_{t})$ 和 $b(s_{t})$ 是与 $c$ 一起优化的辅助神经网络。与Arjona-Medina等人（[2019](#bib.bib7)）相同，上下文
    $c$ 是从指定MDP中获得的历史 $h$，动作是轨迹中的一个动作 $a\in h$，目标是最优策略的最大期望回报 $g=z^{*}\in\mathbb{R}$。然而，该方法仅在狭窄的超参数范围内稳定，并且假设仅有一个动作需要被归功。
- en: Summary.
  id: totrans-362
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要。
- en: This set of work arises from the direct interpretation of the CAP as a redistribution
    problem, and they are the first to report this connection explicitly. These methods
    assign credit backward, that is, based on what has already happened, rather than
    on what they predict will happen, as in the case of forward methods. Despite their
    measure of action influence not being a satisfactory quantification of credit
    (see Section [4.3](#S4.SS3 "4.3 What is an assignment? ‣ 4 Quantifying action
    influences ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")),
    the resulting methods are explicitly designed to address the delayed effect challenge,
    and therefore perform more robustly in tasks that require long-term CA. One key
    drawback of these methods is their lack of a foundational theory. Today, research
    to improve on the depth of CA is still ongoing, with the temporal coherence of
    behaviour over long time-spans, which humans conceptualise as strategies, being
    the main target. These models are still slow to learn and often unstable to different
    environments and hyperparameters.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列工作源于对 CAP 的直接解释作为重新分配问题，并首次明确报告了这一联系。这些方法向后分配信用，即基于已发生的情况，而不是基于他们预测将发生的情况，如前向方法中的情况。尽管他们的动作影响度量未能令人满意地量化信用（见[4.3节](#S4.SS3
    "4.3 What is an assignment? ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")），但结果方法显然旨在解决延迟效应挑战，因此在需要长期
    CA 的任务中表现更为稳健。这些方法的一个关键缺陷是缺乏基础理论。今天，改进 CA 深度的研究仍在进行中，人们将行为在长时间跨度上的时间一致性（即人们概念化为策略的东西）作为主要目标。这些模型仍然学习缓慢，并且在不同的环境和超参数下往往不稳定。
- en: 6.3 Conditioning on a predefined set of goals
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 在预定义目标集上的条件化
- en: If in the previous section we saw advancements on the distributivity aspect,
    the methods in this category are the first methods to evaluate actions for their
    ability to achieve multiple goals explicitly. They do so by conditioning the value
    function on a goal and then using the resulting value function to evaluate actions.
    The intuition behind them is that the agent’s knowledge about the future can be
    decomposed into more elementary associations between states and goals. What distinguishes
    these methods from the ones that follow is that the set of goals they consider
    is objective and thus predefined, and the agent is not allowed to choose a subjective
    one and learn from it. We now describe the two most influential methods in this
    category.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在前一节中我们看到的是分配性方面的进展，那么本类别中的方法是第一个明确评估动作实现多个目标能力的方法。它们通过以目标为条件的值函数来评估动作，直观上认为代理对未来的知识可以分解为状态和目标之间的更基本的关联。这些方法与后续方法的区别在于，它们考虑的目标集是客观的，因此是预定义的，代理不能选择主观目标并从中学习。我们现在描述这一类别中最有影响力的两种方法。
- en: General Value Functions (GVFs)
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一般值函数（GVFs）
- en: '(Sutton et al.,, [2011](#bib.bib183)) stem from the idea that knowledge about
    the world can be expressed in the form of predictions and decomposed into independent
    ones. These predictions can then be organised hierarchically to solve more complex
    problems. While GVFs carry many modifications to the canonical value, we focus
    on its goal-conditioning for the purpose of this review, which is also its foundational
    idea. As described in Section [4.5](#S4.SS5 "4.5 Existing assignment functions
    ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning"), GVFs conditions the action value on a goal to express
    the expected return with respect to the reward function that the goal induces.
    In their original formulation (Sutton et al.,, [2011](#bib.bib183)), GVFs are
    a set of value functions, one for each goal. The goal is any object in a predefined
    goal set of MDP states $g\in\mathcal{S}$, and the resulting measure of action
    influence is the following:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: （Sutton 等，[2011](#bib.bib183)）源于这样一种观点：关于世界的知识可以以预测的形式表达，并分解为独立的预测。这些预测可以按层次组织，以解决更复杂的问题。虽然
    GVFs 对经典值进行了许多修改，但我们在本综述中重点关注其目标条件化，这是其基础思想。正如[4.5节](#S4.SS5 "4.5 Existing assignment
    functions ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")中所述，GVFs 将动作值以目标为条件，以表达相对于目标引发的奖励函数的期望回报。在其原始表述（Sutton
    等，[2011](#bib.bib183)）中，GVFs 是一组值函数，每个目标对应一个。目标是预定义的 MDP 状态集合中的任何对象 $g\in\mathcal{S}$，由此得到的动作影响度量如下：
- en: '|  | $\displaystyle K(c,a,g)=q^{\pi}(s,a,g),$ |  | (23) |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K(c,a,g)=q^{\pi}(s,a,g),$ |  | (23) |'
- en: that is the $q$-function with respect to the goal-conditioned reward function
    $R(s,a,g)$, which is $0$ everywhere, and $1$ when the goal is achieved $\psi(d)=g$.
    Because GVFs evaluate an action for what it is going to happen in the future,
    GVFs are forward methods, and interpret the CAP as a prediction problem, “What
    is the expected return of this action, given that I am going to achieve this goal?”.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相对于目标条件奖励函数 $R(s,a,g)$ 的 $q$-函数，该函数在所有地方为 $0$，当目标 $\psi(d)=g$ 达成时为 $1$。由于GVFs评估的是未来可能发生的动作，因此GVFs属于前向方法，并将CAP解释为预测问题，“考虑到我将要实现这个目标，这个动作的期望回报是多少？”。
- en: Universal Value Functions Approximators (UVFAs)
  id: totrans-370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通用价值函数逼近器（UVFAs）
- en: ([Schaul et al., 2015a,](#bib.bib153) ) scale-up the idea of GVFs to a large
    set of goals, by using a single value function to amortise all of them. One major
    benefit of UVFAs over GVFs is that they are readily applicable to Deep RL by simply
    adding the goal as an input to the value function approximator. This allows the
    agent to learn end-to-end with bootstrapping and allows for exploiting a shared
    prediction structure across different states and goals. Since they derive from
    GVFs, UVFA share most of their characteristics. The context is an MDP state $s\in\mathcal{S}$;
    the goal is still any object in a predefined goal set of states, $g\in\mathcal{S}$,
    and the credit of an action is the expected return of the reward function induced
    by the goal (see Equation ([23](#S6.E23 "In General Value Functions (GVFs) ‣ 6.3
    Conditioning on a predefined set of goals ‣ 6 Methods to assign credit in Deep
    RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning"))).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ([Schaul et al., 2015a,](#bib.bib153)) 将GVFs的理念扩展到一个大的目标集合，通过使用单一的价值函数来平摊所有目标。UVFAs相比于GVFs的一个主要优势是，它们可以通过简单地将目标作为输入添加到价值函数逼近器中，从而轻松应用于深度强化学习。这使得智能体可以通过引导学习端到端，并允许在不同状态和目标之间利用共享的预测结构。由于UVFAs源自GVFs，它们共享大部分特性。上下文是MDP状态
    $s\in\mathcal{S}$；目标仍然是预定义目标状态集合中的任何对象 $g\in\mathcal{S}$，动作的信用是由目标引导的奖励函数的期望回报（参见方程 ([23](#S6.E23
    "In General Value Functions (GVFs) ‣ 6.3 Conditioning on a predefined set of goals
    ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning"))）。
- en: Summary.
  id: totrans-372
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Both GVFs and UVFAs are forward methods that condition the value function on
    a goal to express the expected return of the reward function that the goal induces.
    Their interpretation of credit is still linked to the idea of temporal contiguity
    described in Section [6.1](#S6.SS1 "6.1 Time as a heuristic ‣ 6 Methods to assign
    credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"), and they still suffer from the same drawbacks and limitations. In
    particular, they do not explicitly handle the delayed effect challenge, and their
    performance in the corresponding tasks suffers the same correlation problems as
    the ones described in Section [6.1](#S6.SS1 "6.1 Time as a heuristic ‣ 6 Methods
    to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"). However, by conditioning the value function on a goal, they provide
    a way to extract some signal from the environment even when the action influence
    is low. As described in Section [4.2](#S4.SS2 "4.2 What is a goal? ‣ 4 Quantifying
    action influences ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"), using the goal as an explicit input to the assignment allows to: (a)
    maintain knowledge about multiple goals at the same time and (b) to be aware of
    the objective, which in turn sets out to eventually build an internal metric describing
    the distance from achieving the goal. For example, if the agent’s goal is to achieve
    a minimum return of $1$,it becomes possible to verify if and when that is achieved.
    This is not a possibility when goals are implicit, for example, to maximise the
    return. Which is the maximum return? Without an answer to the question it is hard
    to verify that the goal is achieved. Finally, as for the previous categories,
    most of these methods are value-based methods with a greedy actor.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: GVFs 和 UVFAs 都是前向方法，它们将价值函数条件化于一个目标，以表达目标所引发的奖励函数的期望回报。它们对信用的解释仍然与第[6.1](#S6.SS1
    "6.1 Time as a heuristic ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning")节中描述的时间连续性的理念相关，并且仍然存在相同的缺陷和局限性。特别是，它们没有明确处理延迟效应问题，并且在相应任务中的表现受到与第[6.1](#S6.SS1
    "6.1 Time as a heuristic ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning")节中描述的相同相关性问题的影响。然而，通过将价值函数条件化于一个目标，它们提供了一种即使在行动影响力较低时也能从环境中提取信号的方法。如第[4.2](#S4.SS2
    "4.2 What is a goal? ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")节所述，使用目标作为分配的显式输入可以：(a) 同时维护对多个目标的知识，(b)
    意识到目标，这反过来有助于最终建立描述距离实现目标的内部度量。例如，如果代理的目标是实现最低回报$1，那么可以验证是否以及何时实现了这一目标。当目标是隐含时，例如，最大化回报，这种可能性是不存在的。哪个是最大回报？没有问题的答案，很难验证目标是否已实现。最后，像之前的类别一样，这些方法大多数是基于价值的方法，并且有一个贪婪的行为者。
- en: 6.4 Conditioning in hindsight
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 事后条件化
- en: 'The methods in this category are characterised by the idea of re-evaluating
    the action influence according to what the agent achieved, rather than what it
    was supposed to achieve. They exploit the richness of the temporal stream and
    flexibility of the definition of outcomes to produce a higher number of decision-outcome
    pairs, which results in a higher number of associations and overall a denser signal
    to learn credit for. To produce more associations, their credit formulation is
    inherently goal-explicit, and they learn credit in hindsight. However, unlike
    goal-conditional values (Section [6.3](#S6.SS3 "6.3 Conditioning on a predefined
    set of goals ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning")), they do not act on a pre-defined
    objective set of goals. Rather, they collect a trajectory and re-examine it with
    a different goal in mind. The goal they consider is an outcome in the trajectory
    just collected, and therefore the success distribution, albeit still sparse when
    considering a single goal, is denser when considering all possible goals, allowing
    for a richer learning signal. In practice, this translates into additional training
    data for the neural network that learns the credit model. For these reasons, these
    methods work backward and adopt the interpretation of the CAP as a redistribution
    problem. Notice, however, that there is a difference between backward and hindsight.
    The former refers to the fact that the credit is assigned based on what happened
    in the past after the action has been taken. The latter refers to the fact that
    the interactive experience is re-purposed: while a goal was the original intent
    of the agent, credit is assigned to one that is actually experienced in the trajectory.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的方法的特点是根据代理实际取得的结果而不是预期结果重新评估动作的影响。它们利用时间流的丰富性和结果定义的灵活性来产生更多的决策-结果对，从而生成更多的关联，整体上提供了一个更密集的学习信号。为了产生更多的关联，它们的信用公式本质上是目标明确的，并且它们在事后学习信用。然而，与目标条件值（第[6.3](#S6.SS3
    "6.3 Conditioning on a predefined set of goals ‣ 6 Methods to assign credit in
    Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")节）不同，它们并不基于预定义的目标集合。相反，它们收集一个轨迹，并以不同的目标重新审视它。它们考虑的目标是刚收集到的轨迹中的一个结果，因此成功分布虽然在考虑单一目标时仍然稀疏，但在考虑所有可能目标时更为密集，从而提供了更丰富的学习信号。实际上，这意味着为学习信用模型的神经网络提供了额外的训练数据。由于这些原因，这些方法向后工作，并采用CAP的重新分配问题的解释。然而，请注意，向后和事后之间是有区别的。前者指的是在动作完成后基于过去发生的事情来分配信用。后者指的是交互体验被重新利用：虽然目标是代理的原始意图，但信用被分配给在轨迹中实际经历的目标。
- en: We separate the methods in this category into three subgroups. Those that re-label
    past experience under a different perspective, such as achieving a different goal
    than the one the agent started; this increases the amount of data to learn. Those
    that condition the action evaluation with statistics of the future during training,
    which becomes an explicit performance request at inference time. Those that use
    hindsight to expose actions irrelevant to achieving the goal via counterfactual
    reasoning.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这一类别的方法分为三个子组。那些在不同的视角下重新标记过去经验的方法，例如实现与代理最初设定目标不同的目标，这增加了学习的数据量。那些在训练期间用未来的统计数据来条件化动作评估的方法，这在推理时成为一个明确的性能要求。那些使用事后推理来通过反事实推理暴露与实现目标无关的动作的方法。
- en: 6.4.1 Relabelling experience
  id: totrans-377
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1 重新标记经验
- en: Hindsight Experience Replay (HER)
  id: totrans-378
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 事后经验回放（HER）
- en: '(Andrychowicz et al.,, [2017](#bib.bib5)) stems from the problem of learning
    in sparse rewards environments, which is an example of low action influence. The
    method develops on the following intuition: despite a trajectory not being optimal,
    there is some useful information in that experience to enhance the learning process.
    Hindsight Experience Replay (HER) brings together the UVFA approach to assign
    credit and the experience replay technique (Lin,, [1992](#bib.bib102)) to re-examine
    trajectories. After collecting a set of trajectories from the environment, the
    agent stores not only the return for the goal that was originally pursued but
    also for a subset of other pre-defined goals. This is usually described as a process
    of relabelling the experience with respect to different goals. We refer to this
    process of re-examining a trajectory collected with a prior goal in mind and evaluating
    it according to the actually realised outcome as hindsight conditioning, which
    is also the main innovation that HER brings to the CAP. Notice that the original
    goal is important because the trajectory is collected with a policy that aims
    to maximise the return for that specific goal. However, in HER the goal set is
    still predefined, which is a key insight exploited by other methods, such as Hindsight
    Credit Assignment (HCA) and Upside-Down RL (UDRL) to increase the autonomy of
    the agent. HER uses the goal-conditioned $q$-values described in Section [6.3](#S6.SS3
    "6.3 Conditioning on a predefined set of goals ‣ 6 Methods to assign credit in
    Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")
    to measure action influence:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: （Andrychowicz 等人，[2017](#bib.bib5)）源于稀疏奖励环境中的学习问题，这是低动作影响的一个例子。该方法基于以下直觉：尽管轨迹可能不是最优的，但在这种经验中存在一些有用的信息来增强学习过程。事后经验回放（HER）将
    UVFA 方法用于分配奖励与经验回放技术（Lin，[1992](#bib.bib102)）结合起来，以重新审视轨迹。在从环境中收集一组轨迹后，代理不仅存储了原本追求目标的回报，还存储了其他预定义目标的回报。这通常被描述为根据不同目标重新标记经验的过程。我们将这一过程称为事后条件化，这也是
    HER 带给 CAP 的主要创新。注意，原始目标很重要，因为轨迹是通过旨在最大化该特定目标回报的策略收集的。然而，在 HER 中，目标集仍然是预定义的，这是其他方法（如事后信用分配（HCA）和上下颠倒强化学习（UDRL））用来提高代理自主性的关键见解。HER
    使用第 [6.3](#S6.SS3 "6.3 Conditioning on a predefined set of goals ‣ 6 Methods to
    assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning") 节中描述的目标条件 $q$-值来衡量动作影响：
- en: '|  | $\displaystyle K(c,a,g)=q^{\pi}(s,a,g).$ |  | (24) |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K(c,a,g)=q^{\pi}(s,a,g).$ |  | (24) |'
- en: Here the context $c$ is a history $h=\{o_{t},a_{t},r_{t}:0\leq t\leq T\}$ from
    the assigned MDP, the action is an action from the trajectory $a\in h$, and the
    outcome is the final state of the trajectory $s_{T}$.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的上下文 $c$ 是一个来自指定 MDP 的历史 $h=\{o_{t},a_{t},r_{t}:0\leq t\leq T\}$，动作是轨迹中的一个动作
    $a\in h$，结果是轨迹的最终状态 $s_{T}$。
- en: Hindsight Policy Gradient (HPG)
  id: totrans-382
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 事后政策梯度（HPG）
- en: '(Rauber et al.,, [2019](#bib.bib147)) transfers the findings of HER to policy
    gradient setting. Since HER is limited to off-policy learning with experience
    replay (Lin,, [1992](#bib.bib102)), this effectively allows to extend the concept
    of hindsight to iterative settings. Instead of updating the policy based on the
    actual reward received, Hindsight Policy Gradient (HPG) updates the policy based
    on the hindsight reward, which is calculated based on the new goals that were
    defined using HER. The main difference with HER is that in HPGs, both the critic
    and the actor are conditioned on the additional goal. This results in a goal-conditioned
    policy $\pi(A|S=s,G=g)$, describing the probability of taking an action, given
    the current state and a realised outcome. The action influence is the advantage
    formulation of the hindsight policy gradients:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: （Rauber 等人，[2019](#bib.bib147)）将 HER 的发现转移到政策梯度设置中。由于 HER 限于使用经验回放的离线学习（Lin，[1992](#bib.bib102)），这有效地允许将事后的概念扩展到迭代设置中。事后政策梯度（HPG）不是基于实际获得的奖励更新策略，而是基于使用
    HER 定义的新目标计算的事后奖励更新策略。与 HER 的主要区别在于，HPG 中的批评者和演员都以额外目标为条件。这导致一个目标条件的策略 $\pi(A|S=s,G=g)$，描述了在当前状态和实际结果的条件下采取某个动作的概率。动作影响是事后政策梯度的优势公式：
- en: '|  | $\displaystyle K(c,a,g)=q^{\pi}(s,a,g)-v^{\pi}(s,g),$ |  | (25) |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K(c,a,g)=q^{\pi}(s,a,g)-v^{\pi}(s,g),$ |  | (25) |'
- en: where $q^{\pi}(s,a,g)$ and $v^{\pi}(s,g)$ are the goal-conditioned value functions.
    Here the context $c$ is a history $h=\{o_{t},a_{t},r_{t}:0\leq t\leq T\}$, the
    goal is arbitrarily sampled from a goal set, $g\in\mathcal{G}$. Like HER, HPG
    is tailored to tasks with low action influence and it is shown to be effective
    in sparse reward settings. Overall, HER and HPG are the first completed work to
    talk about hindsight as the re-examination of outcomes for CA. Their solution
    is not particularly interesting for the CAP as they do not cast their problem
    as a connect the finding to the CAP explicitly. However, they are key precursors
    of the methods that we review next, which instead provide novel and reusable developments
    for CAP specifically.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q^{\pi}(s,a,g)$ 和 $v^{\pi}(s,g)$ 是目标条件值函数。这里上下文 $c$ 是历史 $h=\{o_{t},a_{t},r_{t}:0\leq
    t\leq T\}$，目标是从目标集合中任意抽取的 $g\in\mathcal{G}$。类似于HER，HPG 适用于动作影响较低的任务，并且在稀疏奖励设置中被证明是有效的。总体而言，HER
    和 HPG 是首次讨论事后视角作为信用分配重新审视结果的完整工作。尽管它们的解决方案对于CAP并不特别有趣，因为它们没有明确将问题与CAP联系起来，但它们是我们接下来审阅的方法的关键前导，这些方法则专门为CAP提供了新颖且可重用的发展。
- en: 6.4.2 Conditioning on the future
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2 对未来的条件化
- en: Hindsight Credit Assignment (HCA)
  id: totrans-387
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 事后信用分配（HCA）
- en: Traditional reinforcement learning algorithms often struggle with credit assignment
    as they rely solely on foresight. These methods operate under the assumption that
    we lack knowledge of what occurs beyond a given time step, making accurate credit
    assignment challenging, especially in intricate environments. (Harutyunyan et al.,,
    [2019](#bib.bib67)), on the other hand, centres on utilising hindsight information,
    acknowledging that credit assignment and learning typically take place after the
    agent completes its current trajectory. This approach enables us to leverage this
    additional data to refine the learning of critical variables necessary for credit
    assignment.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的强化学习算法在信用分配方面常常面临挑战，因为它们完全依赖于前瞻。这些方法假设我们对给定时间步之后发生的事情缺乏了解，因此准确的信用分配在复杂环境中尤其困难。另一方面，（Harutyunyan
    et al., [2019](#bib.bib67)）则侧重于利用事后信息，承认信用分配和学习通常发生在代理完成当前轨迹之后。这种方法使我们能够利用这些额外数据来改进对信用分配所需关键变量的学习。
- en: (Harutyunyan et al.,, [2019](#bib.bib67)) introduces a new family of algorithms
    known as ”Hindsight Credit Assignment” (HCA). HCA algorithms explicitly assign
    credit to past actions based on the likelihood of those actions leading to the
    observed outcome. This is achieved by comparing a learned hindsight distribution
    over actions, conditioned by a future state or return, with the policy that generated
    the trajectory.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: （Harutyunyan et al., [2019](#bib.bib67)）介绍了一种称为“事后信用分配”（HCA）的新算法系列。HCA算法明确地根据这些行动导致观察到的结果的可能性来分配过去行动的信用。这是通过比较基于未来状态或回报的事后分布与生成轨迹的策略来实现的。
- en: 'More precisely, the hindsight distribution, $h(a|s_{t},\pi,g)$ is the likelihood
    of an action $a$, given the outcome $g$ experienced in the trajectory $d\sim\mathbb{P}_{\mu,\pi}(D|S_{0}=s,a_{t}\sim\pi)$.
    In practice, Harutyunyan et al., ([2019](#bib.bib67)) consider two classes of
    outcomes: states and returns. We refer to the algorithms that derive from these
    two classes of goals as state-HCA and return-HCA. For state-HCA, the context $c$
    is the current state $s_{t}$ at time $t$; the outcome is a future state in the
    trajectory $s_{t^{\prime}}\in d$ where $t^{\prime}>t$; the credit is the ratio
    between the state-conditional hindsight distribution and the policy $\frac{h_{t}(a|s_{t},s_{t}^{\prime})}{\pi(a|s_{t})}$.
    For return-HCA, the context $c$ is the current state $s_{t}$ at time $t$; the
    outcome is the observed return $Z_{t}$; the credit is the ratio between the return-conditional
    hindsight distribution and the policy $1-\frac{\pi(a|s_{t})}{h_{t}(a|s_{t},Z_{t})}$.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，事后分布 $h(a|s_{t},\pi,g)$ 是给定轨迹 $d\sim\mathbb{P}_{\mu,\pi}(D|S_{0}=s,a_{t}\sim\pi)$
    中结果 $g$ 的行动 $a$ 的可能性。在实践中，Harutyunyan et al., ([2019](#bib.bib67)) 考虑了两类结果：状态和回报。我们将从这两类目标衍生的算法称为状态-HCA
    和回报-HCA。对于状态-HCA，上下文 $c$ 是时间 $t$ 的当前状态 $s_{t}$；结果是在轨迹 $s_{t^{\prime}}\in d$ 中的未来状态，其中
    $t^{\prime}>t$；信用是状态条件事后分布与策略之间的比率 $\frac{h_{t}(a|s_{t},s_{t}^{\prime})}{\pi(a|s_{t})}$。对于回报-HCA，上下文
    $c$ 是时间 $t$ 的当前状态 $s_{t}$；结果是观察到的回报 $Z_{t}$；信用是回报条件事后分布与策略之间的比率 $1-\frac{\pi(a|s_{t})}{h_{t}(a|s_{t},Z_{t})}$。
- en: 'For example, return-HCA measures the influence of an action with the hindsight
    advantage described in Section [4](#S4 "4 Quantifying action influences ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning"):'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，return-HCA 通过第 [4](#S4 "4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning")节中描述的回顾优势来衡量一个动作的影响：
- en: '|  | $\displaystyle K(c,a,g)=1-\frac{\pi(A_{t}&#124;S_{t}=s_{t})}{\mathbb{P}_{\mu,\pi}(A_{t}&#124;S_{t}=s_{t},Z_{t}=z_{t})}z_{t}.$
    |  | (26) |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K(c,a,g)=1-\frac{\pi(A_{t}&#124;S_{t}=s_{t})}{\mathbb{P}_{\mu,\pi}(A_{t}&#124;S_{t}=s_{t},Z_{t}=z_{t})}z_{t}.$
    |  | (26) |'
- en: The resulting ratio provides a measure of how crucial a particular action was
    in achieving the outcome. A ratio deviating further from 1 indicates a greater
    impact (positive or negative) of that action on the outcome.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 结果比率提供了一个特定动作在实现结果中的重要性的度量。偏离 1 越远的比率表示该动作对结果的影响（正面或负面）越大。
- en: To compute the hindsight distribution, HCA algorithms employ a technique related
    to importance sampling. Importance sampling estimates the expected value of a
    function under one distribution (the hindsight distribution) using samples from
    another distribution (the policy distribution). In the context of HCA, importance
    sampling weights are determined based on the likelihood of the agent taking each
    action in the trajectory, given the hindsight state compared to the likelihood
    of the policy for that same action. Once the hindsight distribution is computed,
    HCA algorithms can be used to update the agent’s policy and value function. One
    approach involves using the hindsight distribution to reweight the agent’s experience.
    This means the agent will learn more from actions that were more likely to have
    contributed to the observed outcome.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算回顾分布，HCA 算法采用了一种与重要性采样相关的技术。重要性采样利用来自另一分布（策略分布）的样本来估计在一个分布（回顾分布）下函数的期望值。在
    HCA 的背景下，重要性采样权重基于在回顾状态下代理执行每个动作的可能性，比较该动作的政策的可能性来确定。一旦计算出回顾分布，HCA 算法可以用于更新代理的策略和价值函数。一个方法是使用回顾分布来重新加权代理的经验。这意味着代理将从那些更可能对观察到的结果产生影响的动作中学到更多。
- en: Besides advancing the idea of hindsight, (Harutyunyan et al.,, [2019](#bib.bib67))
    carries one novelty:the possibility to drop the typical policy evaluation settings,
    where the goal is to learn a value function by the repeated application of the
    Bellman expectation backup. Instead, action values are defined as a measure of
    the likelihood that the action and the outcome appear together in the trajectory,
    and are a precursor of the sequence modelling techniques described in the next
    section (Section [6.5](#S6.SS5 "6.5 Modelling transitions as sequences ‣ 6 Methods
    to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 除了推进回顾的概念，（Harutyunyan et al.,, [2019](#bib.bib67)）还具有一个新颖之处：可以放弃典型的策略评估设置，在这些设置中，目标是通过重复应用
    Bellman 期望备份来学习一个价值函数。相反，动作值被定义为动作和结果在轨迹中共同出现的可能性度量，并且是下一节（第 [6.5](#S6.SS5 "6.5
    Modelling transitions as sequences ‣ 6 Methods to assign credit in Deep RL ‣ A
    Survey of Temporal Credit Assignment in Deep Reinforcement Learning")节）中描述的序列建模技术的前导。
- en: Upside-Down RL (UDRL)
  id: totrans-396
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Upside-Down RL (UDRL)
- en: (Schmidhuber,, [2019](#bib.bib157); Srivastava et al.,, [2019](#bib.bib172);
    Ashley et al.,, [2022](#bib.bib11); Štrupl et al.,, [2022](#bib.bib173)) is another
    implementation of the idea to condition on the properties of the future. The intuition
    behind UDRL is that rather than conditioning returns on actions, which is the
    case of the methods in Section [6.1](#S6.SS1 "6.1 Time as a heuristic ‣ 6 Methods
    to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"), we can invert the dependency and condition actions on returns instead.
    This allows using returns as input, and inferring the action distribution that
    would achieve that return. The action distribution is approximated using a neural
    network, the behaviour policy, that is trained via maximum likelihood estimation
    using trajectories collected online from the environment. In UDRL the context
    is a completed trajectory $d$; the outcome is a command that achieves the return
    $Z_{k}$ in $H=T-k$ time-steps, which we denote as $g=(Z_{k},H)$; the credit of
    an action $a$ is its probability according to the behaviour function, $\pi(a|s,g)$.
    In addition to HCA, UDRL also conditions the return to be achieved in a specific
    time span. The goal that results from achieving a desired return in a specific
    time span is called command (Schmidhuber,, [2019](#bib.bib157)).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: (Schmidhuber, [2019](#bib.bib157); Srivastava et al., [2019](#bib.bib172); Ashley
    et al., [2022](#bib.bib11); Štrupl et al., [2022](#bib.bib173))是另一种基于未来属性进行条件化的实现方式。UDRL的直觉是，与其将回报条件化于动作（这是第[6.1节](#S6.SS1
    "6.1 Time as a heuristic ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning")的方法所做的），不如将依赖关系反转，将动作条件化于回报。这允许使用回报作为输入，并推断出实现该回报的动作分布。动作分布使用神经网络行为策略进行近似，该网络通过最大似然估计进行训练，使用从环境中在线收集的轨迹。在UDRL中，上下文是完整的轨迹$d$；结果是一个在$H=T-k$时间步中实现回报$Z_{k}$的命令，我们将其表示为$g=(Z_{k},H)$；动作$a$的信用是根据行为函数$\pi(a|s,g)$的概率。除了HCA，UDRL还将回报条件化于特定时间跨度内实现的要求。实现特定时间跨度内所期望回报的目标称为命令(Schmidhuber,
    [2019](#bib.bib157))。
- en: Posterior Policy Gradients (PPGs)
  id: totrans-398
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 事后策略梯度 (PPGs)
- en: '(Nota et al.,, [2021](#bib.bib124)) further the idea of hindsight to provide
    lower-variance, future-conditioned baselines for policy gradient methods. At the
    base of PPG there is a novel value estimator, the PVF. The intuition behind PVFs
    is that in POMDPs the state value is not a valid baseline because the true state
    is hidden from the agent, and the observation cannot provide as a sufficient statistic
    for the return. However, after a full episode, the agent has more information
    to calculate a better, a posteriori guess of the state value at earlier states
    in the trajectory. Nota et al., ([2021](#bib.bib124)) refers to the family of
    possible a posteriori estimations of the state value as the PVF. Formally, a PVF
    decomposes a state into its current observation $o_{t}$, and some hidden state
    that is not observable and typically unknown $b_{t}$. The value of a state can
    then be written as the expected observation-action value function over the possible
    non-observable states $u_{T}\in\mathcal{U}=\mathbb{R}^{d}$. The action influence
    of a PPG is quantified by the expression:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: (Nota et al., [2021](#bib.bib124))进一步发展了事后视角的理念，为策略梯度方法提供了低方差的、未来条件的基准。在PPG的基础上，有一个新颖的价值评估器，即PVF。PVF的直觉是，在POMDPs中，状态价值不是有效的基准，因为真实状态对代理隐藏，而观察不能作为回报的充分统计量。然而，在完整的回合之后，代理有更多信息来计算对轨迹中早期状态的更好、事后的状态价值估计。Nota
    et al., ([2021](#bib.bib124))将状态价值可能的事后估计的家族称为PVF。形式上，PVF将状态分解为其当前观察$o_{t}$和一些不可观察且通常未知的隐藏状态$b_{t}$。然后，可以将状态的价值写成对可能的不可观察状态$u_{T}\in\mathcal{U}=\mathbb{R}^{d}$的期望观察-动作价值函数。PPG的动作影响通过以下表达式来量化：
- en: '|  | $\displaystyle K(c,a,g)=\mathop{\mathbb{E}}_{u\in\mathcal{U}}\left[\mathbb{P}(u_{t}=u&#124;h_{t})v(o_{t},u_{t})\right].$
    |  | (27) |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K(c,a,g)=\mathop{\mathbb{E}}_{u\in\mathcal{U}}\left[\mathbb{P}(u_{t}=u&#124;h_{t})v(o_{t},u_{t})\right].$
    |  | (27) |'
- en: Here, the context is a history $h_{t}$, the action is the current action and
    the goal is the return distribution $Z_{t}$. In practice, PVF advances HCA by
    learning which statistics of the trajectory, $\psi(d)$, are useful to assign credit,
    rather than specifying it objectively as a state or a return.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，上下文是历史$h_{t}$，动作是当前动作，目标是回报分布$Z_{t}$。在实际应用中，PVF通过学习轨迹的哪些统计量$\psi(d)$对分配信用有用，来推进HCA，而不是将其客观地指定为状态或回报。
- en: Policy Gradient Incorporating the Future (PGIF)
  id: totrans-402
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 未来纳入策略梯度 (PGIF)
- en: (Venuto et al.,, [2022](#bib.bib195)) implements the same idea of PVFs, but
    proposes a different method to learn the posterior. They propose an information
    bottleneck (a variational autoencoder) between the prior and the posterior value
    function to approximate the posterior. This encourages the agent to learn only
    the useful non-observable features. Finally, they theoretically link their formulation
    to the idea of teacher forcing (Williams and Zipser,, [1989](#bib.bib208); Lamb
    et al.,, [2016](#bib.bib97); Goyal et al.,, [2017](#bib.bib57)) in recurrent neural
    networks.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: (Venuto et al., [2022](#bib.bib195)) 实现了相同的 PVF 思路，但提出了一种不同的方法来学习后验。他们提出了一种信息瓶颈（变分自编码器）在先验和后验价值函数之间以近似后验。这鼓励智能体仅学习有用的不可观测特征。最后，他们将他们的理论公式与递归神经网络中的教师强迫（Williams
    and Zipser, [1989](#bib.bib208); Lamb et al., [2016](#bib.bib97); Goyal et al.,
    [2017](#bib.bib57)）的思想联系起来。
- en: 6.4.3 Exposing irrelevant factors
  id: totrans-404
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.3 揭示无关因素
- en: Future-Conditional Policy Gradient (FC-PG)
  id: totrans-405
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 未来条件策略梯度（FC-PG）
- en: For being data efficient, credit assignment methods need to disentangle the
    effects of a given action of the agent from the effects of external factors and
    subsequent actions. External factors in reinforcement learning are any factors
    that affect the state of the environment or the agent’s reward, but are outside
    of the agent’s control. This can include things like the actions of other agents
    in the environment, changes in the environment state due to natural processes
    or events. These factors can make credit assignment difficult because they can
    obscure the relationship between the agent’s actions and its rewards.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高数据效率，信用分配方法需要将智能体的某一行为的效果与外部因素和随后的行为的效果区分开。强化学习中的外部因素是任何影响环境状态或智能体奖励的因素，但超出了智能体的控制范围。这可以包括环境中其他智能体的行为、因自然过程或事件导致的环境状态变化等。这些因素可能使信用分配变得困难，因为它们可能会模糊智能体行为与奖励之间的关系。
- en: Mesnard et al., ([2021](#bib.bib112)) proposes to get inspiration from the counterfactuals
    from causality theory to improve credit assignment in model-free reinforcement
    learning. The key idea is to condition value functions on future events, and learn
    to extract relevant information from a trajectory. Relevant information here corresponds
    to all information that is predictive of the return while being independent of
    the agent’s action at time $t$. This allows the agent to separate the effect of
    its own actions, the skills, from the effect of external factors and subsequent
    actions, the luck, which will enable refined credit assignment and therefore faster
    and more stable learning.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: Mesnard et al., ([2021](#bib.bib112)) 提出了从因果理论中的反事实中汲取灵感，以改善无模型强化学习中的信用分配。关键思想是将价值函数条件化于未来事件，并学会从轨迹中提取相关信息。这里的相关信息对应于所有能够预测回报的，同时与时间
    $t$ 上的智能体行为无关的信息。这使得智能体能够将自身行为的影响，即技能，与外部因素和随后的行为，即运气，的影响分开，从而实现精细的信用分配，从而加速并稳定学习。
- en: It shows that these algorithms are provably lower variance than vanilla policy
    gradient, and develops valid, practical variants that avoid the potential bias
    from conditioning on future information. One variant explicitly tries to remove
    information from the hindsight conditioning that depends on the current action
    while the second variant avoids the potential bias from conditioning on future
    information thanks to a technique related to important sampling. Counterfactual
    Credit Assignment (CCA) currently provides some of the best results on the CAP.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 它表明，这些算法在理论上比原始策略梯度具有更低的方差，并开发了有效、实用的变体，避免了对未来信息条件化可能带来的偏差。一个变体明确试图从事后条件化中去除与当前行为相关的信息，而第二个变体通过与重要采样相关的技术避免了对未来信息条件化的潜在偏差。反事实信用分配（CCA）目前在
    CAP 上提供了一些最佳结果。
- en: Counterfactually-Guided Policy Search (CGPS)
  id: totrans-409
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 反事实引导策略搜索（CGPS）
- en: (Buesing et al.,, [2019](#bib.bib29)) is a precursor of Mesnard et al., ([2021](#bib.bib112))
    in a model-based setting, where the hindsight statistic is known a priori, rather
    than learned from experience. These are represented as a Structural Causal Model
    (SCM) and actual experience is combined with counterfactual queries to perform
    off-policy evaluation.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: (Buesing et al., [2019](#bib.bib29)) 是 Mesnard et al., ([2021](#bib.bib112))
    在模型基础设置中的前身，其中事后统计量是已知的，而不是从经验中学习得来的。这些被表示为结构性因果模型（SCM），实际经验与反事实查询相结合以执行非策略评估。
- en: Summary.
  id: totrans-411
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 总结。
- en: While this line of research brings many independent novelties, the most relevant
    for the scope of this section is the idea of hindsight conditioning, which can
    be summarised by the intuition to revisit past estimations with additional information
    about the future. The evidence provided by these studies suggests that hindsight
    conditioning provides great benefit in terms of CA, while the only requirement
    on the overall RL strategy is to employ an actor-critic algorithm, which we consider
    a mild assumption. Overall, learning in hindsight improves on delayed effects,
    especially those induced by the hierarchical structure of the decision process,
    despite not targeting it directly. The methods improve the ability to solve decision
    problems where the action influence is overall very low. Finally, some of these
    methods (Mesnard et al.,, [2021](#bib.bib112); Buesing et al.,, [2019](#bib.bib29))
    also incentivise the discovery of multiple pathways to the same goal, by identifying
    decisions that are irrelevant to the outcome, resulting in the fact that any of
    them can be taken without affecting the outcome.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这一研究方向带来了许多独立的创新，但本节最相关的概念是事后条件，这可以通过重新审视过去估计的直觉来总结，附加了关于未来的信息。这些研究提供的证据表明，事后条件在
    CA 方面提供了巨大的好处，而对整体 RL 策略的唯一要求是采用演员-评论家算法，我们认为这是一个温和的假设。总体而言，事后学习改进了延迟效应，尤其是由决策过程的层级结构引起的效应，尽管没有直接针对它。这些方法提高了解决整体动作影响非常低的决策问题的能力。最后，这些方法中的一些
    (Mesnard et al., [2021](#bib.bib112); Buesing et al., [2019](#bib.bib29)) 还鼓励发现通向相同目标的多条路径，通过识别与结果无关的决策，导致任何决策都可以采取而不影响结果。
- en: 6.5 Modelling transitions as sequences
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 作为序列建模的过渡
- en: The methods in this category are based on the observation that RL can be seen
    as a sequence modelling problem. Their main idea is to transfer the successes
    of sequence modelling in Natural Language Processing (NLP) to improve RL. On a
    high level they all share the same assumption that a sequence in RL is a sequence
    of transitions $(s,a,r)$, and they differ in either how to model the sequence,
    the problem they solve, or the specific method they transfer from NLP. Another
    common characteristic is that they often learn from offline datasets, which is
    a limitation that is not shared by the other methods in this section.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的方法基于观察到的 RL 可以被视为序列建模问题。它们的主要思想是将序列建模在自然语言处理 (NLP) 中的成功转移到 RL 上。在高层次上，它们都共享相同的假设，即
    RL 中的序列是过渡序列 $(s,a,r)$，并且它们在建模序列的方式、解决的问题或从 NLP 转移的具体方法上有所不同。另一个共同的特征是它们通常从离线数据集中学习，这是一种本节中其他方法没有的限制。
- en: Trajectory Transformers (TTs)
  id: totrans-415
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 轨迹变换器（TTs）
- en: '(Janner et al.,, [2021](#bib.bib78)) implement a decoder-only (Radford et al.,,
    [2018](#bib.bib143), [2019](#bib.bib144)) transformer (Vaswani et al.,, [2017](#bib.bib194))
    to model the sequence of transitions. TTs learn from an observational stream of
    data, composed of expert demonstrations resulting in an offline RL training protocol.
    The main idea of TTs is to model the next token in the sequence, which is a composed
    by next state, the next action, and the resulting reward. This enables planning,
    which TTs exploit to plan via beam search. Notice that, for any of these paradigms,
    if the sequence model is autoregressive – the next prediction depends only on
    the past history, but since a full episode is available, the future-conditioned
    probabilities are still well-defined, and also TTs can condition on the future.
    In TTs the action influence is the product between the action probability according
    to the demonstration dataset and its $q$-value:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: (Janner et al., [2021](#bib.bib78)) 实现了一种仅解码器的 (Radford et al., [2018](#bib.bib143),
    [2019](#bib.bib144)) 变换器 (Vaswani et al., [2017](#bib.bib194)) 来建模过渡序列。 TTs 从由专家演示组成的观察数据流中学习，从而形成离线
    RL 训练协议。 TTs 的主要思想是建模序列中的下一个标记，该标记由下一个状态、下一个动作和结果奖励组成。这使得规划成为可能，TTs 利用该规划通过束搜索进行规划。请注意，对于这些范式中的任何一个，如果序列模型是自回归的——下一个预测仅依赖于过去的历史，但由于完整的情节是可用的，未来条件的概率仍然是明确定义的，同时
    TTs 也可以对未来进行条件设置。在 TTs 中，动作影响是根据演示数据集的动作概率与其 $q$-值的乘积：
- en: '|  | $\displaystyle K(c,a,g)=\mathbb{P}_{\theta}(A_{t}=a_{t}&#124;Z_{t}=z_{t})q^{\pi}(s,a).$
    |  | (28) |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K(c,a,g)=\mathbb{P}_{\theta}(A_{t}=a_{t}&#124;Z_{t}=z_{t})q^{\pi}(s,a).$
    |  | (28) |'
- en: Here, the context $c$ is an MDP state $c=s\in\mathcal{S}$, the action is arbitrarily
    selected, and the goal is the return distribution $\mathbb{P}(Z)$.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，上下文 $c$ 是一个MDP状态 $c=s\in\mathcal{S}$，动作是任意选择的，目标是返回分布 $\mathbb{P}(Z)$。
- en: Decision Transformers (DTs)
  id: totrans-419
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 决策变换器（DTs）
- en: (Chen et al.,, [2021](#bib.bib33)) proceed on the same lines as TTs but ground
    the problem in learning, rather than planning. DTs interpret a sequence as a list
    of $(s_{t},a_{t},Z_{t})$ triples, where $Z_{t}$ is the discounted sum of rewards
    from $t$ to the end of the episode. They then use a decoder-only transformer to
    learn a model of the actor that takes the current state and the return as input
    and outputs a distribution over actions. In addition, they optionally learn a
    model of the critic as well, which takes the current state and each action in
    the distribution to output the value of each action. The sequences are sampled
    from expert or semi-expert demonstrations, and the model is trained to maximise
    the likelihood of the actions taken by the expert. From the perspective of CA,
    TTs are equivalent to DTs, and they share the same limitation in that they struggle
    to assign credit accurately to experience beyond that of the offline dataset.
    Furthermore, like HCA (Harutyunyan et al.,, [2019](#bib.bib67)), DTs bring more
    than one novelty to RL. Besides modelling the likelihood of the next token, they
    also use returns as input to the model, resulting in a form of future conditioning.
    However, for CA and this section, we are only interested in their idea of sequence
    modelling, and we will not discuss the other novelties.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: (陈等人，[2021](#bib.bib33)) 继续沿用TTs的方法，但将问题基础放在学习而非规划上。DTs将一个序列解释为 $(s_{t},a_{t},Z_{t})$
    三元组，其中 $Z_{t}$ 是从 $t$ 到一集结束的折扣奖励总和。他们使用仅解码器的变换器来学习演员的模型，该模型以当前状态和返回作为输入，并输出一个关于动作的分布。此外，他们还可选地学习评论员的模型，该模型以当前状态和分布中的每个动作作为输入，以输出每个动作的值。这些序列是从专家或半专家的演示中采样的，模型经过训练以最大化专家采取的动作的似然性。从CA的角度来看，TTs等同于DTs，它们具有相同的局限性，即难以准确地将信用分配到离线数据集之外的经验。此外，像HCA（Harutyunyan
    et al., [2019](#bib.bib67)）一样，DTs为RL带来了不止一种新颖性。除了建模下一个标记的可能性外，他们还将返回作为模型的输入，导致一种未来条件化的形式。然而，对于CA和本节，我们仅对他们的序列建模思想感兴趣，其他新颖性将不予讨论。
- en: Online Decision Transformers (ODTs)
  id: totrans-421
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在线决策变换器（ODTs）
- en: (Zheng et al.,, [2022](#bib.bib217)) extend DT to fine-tuning from experience
    collected online, to overcome the limitations of learning from offline data only.
    To adapt the notion of exploration to sequence modelling, they propose to pre-train
    on the offline dataset, and then employ maximum entropy exploration and collect
    data outside the support of the offline dataset. Finally, they learn off-policy
    using a replay buffer composed of full trajectories. From the perspective of CAP,
    the only difference with DTs is that they learn from a different source of experience.
    While DTs collect contextual data from a static dataset of demonstrations, ODTs
    do it by interacting with an environment (see also Appendix [B](#S2a "B Further
    details on contexts ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")) at the stage of fine-tuning. Fine-tuning online allows to increase
    the scale of the data available to learn from, and later evidence (Lee et al.,,
    [2022](#bib.bib100)) shows that this brings to better performance, and to suggests
    promising scaling laws.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: (郑等人，[2022](#bib.bib217)) 将决策变换器（DT）扩展到从在线收集的经验进行微调，以克服仅从离线数据学习的局限性。为了将探索的概念适应序列建模，他们建议先在离线数据集上进行预训练，然后采用最大熵探索并收集离线数据集支持之外的数据。最后，他们使用由完整轨迹组成的重放缓冲区进行离策略学习。从CAP的角度来看，与DT的唯一区别是他们从不同的经验来源学习。虽然DT从静态演示数据集中收集上下文数据，但ODT通过与环境交互来实现这一点（另见附录[B](#S2a
    "B Further details on contexts ‣ A Survey of Temporal Credit Assignment in Deep
    Reinforcement Learning")）在微调阶段。在线微调可以增加可学习的数据规模，后来证据（Lee et al., [2022](#bib.bib100)）显示这带来了更好的性能，并且表明有前景的扩展规律。
- en: Generalised Decision Transformers (GDTs)
  id: totrans-423
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 广义决策变换器（GDTs）
- en: (Furuta et al.,, [2022](#bib.bib52)) generalise DT to model quantities beyond
    the return, the same way FC-PG generalises HCA to model quantities beyond the
    states and returns. Furuta et al., ([2022](#bib.bib52)) introduce the idea of
    hindsight information, defined as the information content of a function of the
    trajectory, that is, the self-information of an outcome. In theory, this formulation
    is similar to that of PVF, and the two threads of research arrive at similar conclusions
    from different perspectives. However, in practice, this degenerates to using states
    or returns. To measure action influence, the difference with its DT predecessor
    is that the goal $g$ us now self-information of an arbitrary goal.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: (Furuta et al., [2022](#bib.bib52)) 将 DT 推广到建模超出回报的量，就像 FC-PG 将 HCA 推广到建模超出状态和回报的量一样。Furuta
    et al. ([2022](#bib.bib52)) 引入了事后信息的概念，定义为轨迹函数的信息内容，即结果的自信息。从理论上讲，这种表述类似于 PVF
    的表述，这两条研究线索从不同的角度得出了类似的结论。然而，在实践中，这会退化为使用状态或回报。为了测量动作的影响，与其 DT 前身的区别在于目标 $g$ 现在是任意目标的自信息。
- en: Summary.
  id: totrans-425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要。
- en: Sequence modelling in RL transfers the advances of sequence modelling in NLP
    to the RL setting. The main idea for the purpose of CA is to measure credit by
    estimating the probability of the next action (or the next token), conditioned
    on the context and the goal, according to an offline dataset of expert trajectories
    (Chen et al.,, [2021](#bib.bib33); Janner et al.,, [2021](#bib.bib78)), with online
    fine-tuning (Lee et al.,, [2022](#bib.bib100)). Their development has a similar
    pattern to that of hindsight methods and progressively generalises to more complex
    settings, such as online learning (Zheng et al.,, [2022](#bib.bib217)) and more
    general outcomes (Furuta et al.,, [2022](#bib.bib52)). Overall, they are a promising
    direction for CA, especially for their ability to scale to large datasets. It
    is not clear how these methods position with respect to the CA challenges described
    in Section [5](#S5 "5 The challenges to assign credit in Deep RL ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning"), for the lack of experimentation
    on tasks that explicitly stress the agent’s ability to assign credit. However,
    for their vicinity to future-conditioned methods, they bear some of the same advantages
    and also share some limitations. In particular, for their ability to define outcomes
    in hindsight, regardless of an objective learning signal, they bode well in tasks
    with low action influence.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: RL 中的序列建模将 NLP 中序列建模的进展转移到 RL 环境中。CA 的主要思想是通过估计下一个动作（或下一个标记）的概率，根据上下文和目标，利用离线专家轨迹数据集（Chen
    et al., [2021](#bib.bib33); Janner et al., [2021](#bib.bib78)），并通过在线微调（Lee et
    al., [2022](#bib.bib100)）。它们的发展模式类似于事后方法，并逐步推广到更复杂的设置，如在线学习（Zheng et al., [2022](#bib.bib217)）和更一般的结果（Furuta
    et al., [2022](#bib.bib52)）。总体而言，它们是 CA 的一个有前景的方向，特别是由于其扩展到大数据集的能力。目前尚不清楚这些方法如何与第
    [5](#S5 "5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning") 节中描述的 CA 挑战相关，因为缺乏对明确考验代理分配信用能力的任务的实验。然而，由于它们与未来条件方法的相似性，它们具有一些相同的优点，也存在一些限制。特别是，对于在没有客观学习信号的情况下定义事后结果的能力，它们在动作影响较低的任务中表现良好。
- en: 6.6 Planning and learning backwards
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 规划与反向学习
- en: The methods in this category extend CA to potential predecessor decisions that
    have not been taken, but could have led to the same outcome (Chelu et al.,, [2020](#bib.bib32)).
    The main intuition behind these methods is that, in environments with low action
    influence, influential actions are rare, and when a goal is achieved the agent
    should use that event to extract as much information as possible to assign credit
    to relevant decisions. We divide the section into two major sub-categories, depending
    on whether the agent identifies predecessor states by planning with an inverse
    model, or by learning relevant statistics without it.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的方法将 CA 扩展到可能的前任决策，这些决策没有被采取，但本可以导致相同的结果（Chelu et al., [2020](#bib.bib32)）。这些方法的主要直觉是，在动作影响较低的环境中，有影响的动作是稀少的，当目标达成时，代理应利用这一事件提取尽可能多的信息，以便将信用分配到相关决策上。我们将本节分为两个主要子类别，取决于代理是否通过与逆向模型规划识别前任状态，或通过不使用逆向模型学习相关统计数据。
- en: 6.6.1 Planning backwards
  id: totrans-429
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.6.1 反向规划
- en: Recall traces
  id: totrans-430
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 回顾痕迹
- en: (Goyal et al.,, [2019](#bib.bib56)) combine model-free updates from Section [6.1](#S6.SS1
    "6.1 Time as a heuristic ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning") with learning a backward
    model of the environment. A backward model $\mu^{-1}(S_{t-1}|S_{t}=s,A_{t-1}=a)$
    describes the probability of a state $S_{t-1}$ being the predecessor of another
    state $S_{t}$, given that the backward action $A_{t=1}$ was taken. The backward
    action is sampled from a backward policy, $\pi_{b}(a_{t-1}|s_{t})$, which predicts
    the previous action, and a backward dynamics. By autoregressively sampling from
    the backward policy and dynamics, the agent can cross the MDP backwards, starting
    from a final state, $S_{T}$, up until a starting state, $S_{0}$ to produce a new
    history, recall trace. This allows the agent to collect experience that always
    leads to a certain state, $s_{T}$, but that does so from different starting points,
    discovering multiple pathways to the same goal. By performing behaviour cloning
    on the recall trace, Formally, the agent alternates between steps of GPI via model-free
    updates and steps of behaviour cloning on trajectories collected via the backward
    model (trajectories are reversed to match the forward arrow of time before cloning).
    This is a key step towards solving the CAP as it allows propagating credit to
    decisions that have not been taken but could have led to the same outcome without
    interacting with the environment directly. Recall-traces measure the influence
    of an action by its $q$-value, but differ from any other method using the same
    action influence because the contextual data is produced via backward crossing.
    The goal is the expected return.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: (Goyal 等人，[2019](#bib.bib56)) 将第 [6.1](#S6.SS1 "6.1 Time as a heuristic ‣ 6
    Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning") 节中的无模型更新与学习环境的反向模型结合起来。反向模型 $\mu^{-1}(S_{t-1}|S_{t}=s,A_{t-1}=a)$
    描述了在采取反向动作 $A_{t=1}$ 的情况下，状态 $S_{t-1}$ 是另一个状态 $S_{t}$ 的前身的概率。反向动作是从反向策略 $\pi_{b}(a_{t-1}|s_{t})$
    中抽样的，该策略预测之前的动作，以及反向动态。通过从反向策略和动态中自回归抽样，智能体可以从最终状态 $S_{T}$ 向后推演 MDP，直到起始状态 $S_{0}$，以生成新的历史记录、回忆轨迹。这使得智能体可以从不同的起点收集经验，这些经验总是导致某个特定状态
    $s_{T}$，从而发现通往相同目标的多条路径。通过对回忆轨迹进行行为克隆，形式上，智能体在通过无模型更新的 GPI 步骤和通过反向模型收集的轨迹上进行行为克隆的步骤之间交替进行（轨迹被反转以匹配时间的正向箭头，然后进行克隆）。这是解决
    CAP 的关键步骤，因为它允许将信用传播到那些尚未做出但可能导致相同结果的决策，而无需直接与环境互动。回忆轨迹通过其 $q$-值测量动作的影响，但与使用相同动作影响的任何其他方法不同，因为上下文数据是通过反向推演产生的。目标是期望回报。
- en: Forward-Backward RL (FBRL)
  id: totrans-432
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 前向-后向 RL（FBRL）
- en: Edwards et al., ([2018](#bib.bib40)) is a concurrent work to Goyal et al., ([2019](#bib.bib56))
    that also learns a backward model to train on imagined reversed trajectories,
    and provides further evidence of the method’s benefits.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: Edwards 等人（[2018](#bib.bib40)）是 Goyal 等人（[2019](#bib.bib56)）的同期工作，也学习了反向模型以训练想象的反向轨迹，并进一步证明了该方法的好处。
- en: Time-Reversal as Self Supervision (TRASS)
  id: totrans-434
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 时间反转自监督（TRASS）
- en: (Nair et al.,, [2020](#bib.bib121)) follows on the same idea of learning a backward
    state-transition dynamics model. However, compared to FBRL and recall traces,
    TRASS require the ability to access a simulator of the environment that can reset
    to a subset of high reward states. This is a fairly restrictive assumption compared
    to FBRL and recall traces, which resort to the more canonical GPI to find the
    states of interest. Overall, Nair et al., ([2020](#bib.bib121)) provide additional
    evidence that, if a goal is achieved, it is beneficial to assign credit to state-action
    pairs discovered by crossing the MDP backwards from the time the goal is achieved.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: (Nair 等人，[2020](#bib.bib121)) 继承了学习反向状态转换动态模型的相同理念。然而，与 FBRL 和回忆轨迹相比，TRASS 需要能够访问一个环境模拟器，该模拟器可以重置到一组高奖励状态。与依赖于更经典的
    GPI 来找到感兴趣状态的 FBRL 和回忆轨迹相比，这是一种相当严格的假设。总体而言，Nair 等人（[2020](#bib.bib121)）提供了额外的证据，表明如果一个目标已实现，将信用分配给从目标实现时向后推演
    MDP 发现的状态-动作对是有益的。
- en: Reverse Offline Model-based Imagination (ROMI)
  id: totrans-436
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 反向离线模型基础的想象（ROMI）
- en: (Wang et al.,, [2021](#bib.bib200)) proceeds on the same line but focuses on
    offline settings, emphasising that, all else being equal, backward models enable
    better generalisation than forward ones, as they start directly from high-value
    states from which the return can only diminish.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: （Wang 等人，[2021](#bib.bib200)）在相同的方向上继续，但专注于离线设置，强调在其他条件相同的情况下，后向模型比前向模型更能实现更好的泛化，因为它们直接从高价值状态开始，而回报只能减少。
- en: Bidirectional Model-based Policy Optimisation (BMPO)
  id: totrans-438
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 双向模型优化策略（BMPO）
- en: (Lai et al.,, [2020](#bib.bib96)) learns both a forward and a backward model
    of the state-transition dynamics and proves that BMPO achieves a lower return
    error bound than pure forward or pure backward methods. Overall, this method allows
    both to assign credit directly via direct policy search.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: （Lai 等人，[2020](#bib.bib96)）学习了状态转移动态的前向和后向模型，并证明了 BMPO 实现了比纯前向或纯后向方法更低的回报误差界限。总体而言，这种方法允许通过直接策略搜索直接分配信用。
- en: The relationship between forward and backward planning
  id: totrans-440
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 前向和后向规划之间的关系
- en: is the subject of further investigations (van Hasselt et al.,, [2019](#bib.bib193);
    Chelu et al.,, [2020](#bib.bib32)). van Hasselt et al., ([2019](#bib.bib193))
    provides empirical evidence suggesting that assigning credit from hypothetical
    transitions, that is, when trajectories are sampled from a forward model, improves
    the overall efficiency in control problems. This remarks the difference between
    assigning credit in a model-free fashion, where the model is only used to generate
    fictitious trajectories and assigning credit via search, where the model is used
    to look-ahead or look-behind. Chelu et al., ([2020](#bib.bib32)) and van Hasselt
    et al., ([2019](#bib.bib193)) further show that backward planning provides even
    greater benefits than forward planning when the state-transition dynamics are
    stochastic.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 这是进一步研究的主题（van Hasselt 等人，[2019](#bib.bib193); Chelu 等人，[2020](#bib.bib32)）。van
    Hasselt 等人（[2019](#bib.bib193)）提供了实证证据，表明从假设过渡中分配信用，即当轨迹从前向模型中采样时，可以提高控制问题的总体效率。这指出了在无模型方式中分配信用与通过搜索分配信用之间的区别，其中模型仅用于生成虚拟轨迹，而通过搜索分配信用则使用模型进行前瞻或回顾。Chelu
    等人（[2020](#bib.bib32)）和 van Hasselt 等人（[2019](#bib.bib193)）进一步显示，当状态转移动态是随机时，回顾规划提供的好处甚至比前瞻规划更大。
- en: 6.6.2 Learning predecessors
  id: totrans-442
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.6.2 学习前驱
- en: Expected Eligibility Trace (ET($\lambda$))
  id: totrans-443
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 期望资格迹（ET($\lambda$)）
- en: '(van Hasselt et al.,, [2021](#bib.bib190)) provide a model-free alternative
    to backward planning that assigns credit to potential predecessors decisions of
    the outcome: decisions that have been taken in the past, but have not in the last
    episode. The main idea is to weight the action value by its expected eligibility
    trace, that is, the instantaneous trace (see Section [6.1](#S6.SS1 "6.1 Time as
    a heuristic ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning")), but in expectation over the random
    trajectory, defined by the policy and the state-transition dynamics. The Deep
    RL implementation of ET($\lambda$) considers the expected trace upon the action
    value representation – usually the last layer of a neural network value approximator.
    Like for other ETs algorithms, ET($\lambda$) measures action influence using the
    $q$-value of the decision and encodes the information of the trace in the parameters
    of the function approximator. In this case the authors interpret the value network
    as a composition of a non-linear representation function $\phi(s)$ and a linear
    value function $v(s_{t})=w\top\phi(s)$. The expected trace $e(s)=E\phi(s)$ is
    then the result of applying a second linear operator $E$ on the representation.
    $e(s)$ is then trained to minimise the expected $\ell_{2}$ norm between the current
    estimation of $e(s)$ and the instantaneous trace.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: (van Hasselt 等人，[2021](#bib.bib190)) 提供了一种无模型的替代方法，与传统的反向规划不同，这种方法将信用分配给可能的前任决策：这些决策是在过去做出的，但在最后一集没有做出的。主要思想是通过其期望的资格迹来加权动作价值，即瞬时迹（见第
    [6.1](#S6.SS1 "6.1 时间作为启发式 ‣ 6 深度强化学习中的信用分配方法 ‣ 深度强化学习中的时间信用分配综述") 节），但在政策和状态转移动态定义的随机轨迹的期望下。ET($\lambda$)
    的深度强化学习实现考虑了在动作价值表示上的期望迹——通常是神经网络价值逼近器的最后一层。与其他 ETs 算法类似，ET($\lambda$) 通过决策的 $q$-值来测量动作的影响，并将迹的信息编码到函数逼近器的参数中。在这种情况下，作者将价值网络解释为非线性表示函数
    $\phi(s)$ 和线性价值函数 $v(s_{t})=w\top\phi(s)$ 的组合。期望迹 $e(s)=E\phi(s)$ 是将第二个线性算子 $E$
    应用到表示上得到的结果。$e(s)$ 然后被训练以最小化当前对 $e(s)$ 的估计与瞬时迹之间的期望 $\ell_{2}$ 范数。
- en: Summary.
  id: totrans-445
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要。
- en: In this section, we have reviewed the main methods that enhance CA by extending
    it to decisions that have not been taken, but could have led to the same outcome.
    The intuition behind them is that, in tasks where the action influence is low,
    creditable actions are rare findings, and when this happens the agent can use
    that occurrence to extract as much information as possible. One set of methods
    does so by learning inverse models of the state-transition dynamics and walking
    backwards from the outcome. Chelu et al., ([2020](#bib.bib32)); van Hasselt et al.,
    ([2019](#bib.bib193)) further analyse the conditions in which backward planning
    is beneficial. Another set of methods exploits the idea of eligibility traces
    and keeps a measure of the marginal state-action probability to assign credit
    to actions that could have led to the same outcome. Overall, these methods are
    designed to thrive in tasks where the action influence is low. Also, for their
    ability to start from a high-value state, backward planning methods can find a
    higher number of optimal transpositions, and therefore provide a less biased estimate
    of the credit of a state-action pair. On the other hand, none of these methods
    addresses delayed effects directly, and, albeit not explicitly tested on these,
    they are not the best choice in those circumstances.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了通过将信用扩展到尚未采取但可能导致相同结果的决策的主要方法。其背后的直觉是，在行动影响较低的任务中，值得信赖的行动是稀有的发现，当这种情况发生时，代理可以利用这种情况尽可能多地提取信息。一组方法通过学习状态转移动态的逆模型并从结果向后推演来实现这一点。Chelu
    等人（[2020](#bib.bib32)）；van Hasselt 等人（[2019](#bib.bib193)）进一步分析了反向规划有益的条件。另一组方法利用了资格迹的思想，并保持对边际状态-动作概率的度量，以将信用分配给可能导致相同结果的动作。总体而言，这些方法旨在在行动影响较低的任务中取得成功。此外，由于它们能够从高价值状态开始，反向规划方法可以找到更多的最优变换，从而提供对状态-动作对的信用的较少偏差的估计。另一方面，这些方法没有直接解决延迟效应，尽管没有在这些情况下明确测试，但它们不是这些情况的最佳选择。
- en: 6.7 Meta-learning proxies for credit
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 信用的元学习代理
- en: Often, control methods show brittleness to the choice of hyperparameters of
    the RL problem, for example, the number of steps to look-ahead in bootstrapping,
    what discount factor to use, or meta-parameters specific to the method at hand.
    How to select these meta-parameters is an accurate balance that depends on the
    task, the algorithm, and the objective of the agent. Because a policy improvement
    step is as valuable as accurate is the current estimation of action influence,
    these issues also transfer to CA. The methods in this category assign credit by
    meta-learning these meta-parameters, which changes the measure of action influence,
    for example, by optimising the $\lambda$ of ET. The nature of these methods is
    quite different from that of the others we surveyed earlier. For this reason,
    it is sometimes difficult to analyse using the usual framework, and we present
    them differently, by describing their main idea, and the way they are implemented
    in Deep RL.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 控制方法通常对 RL 问题的超参数选择表现出脆弱性，例如，引导中的前瞻步数、使用的折扣因子，或特定方法的元参数。这些元参数的选择是一个准确的平衡，取决于任务、算法和代理的目标。由于策略改进步骤的价值与当前动作影响估计的准确性同等重要，这些问题也会转移到
    CA 中。该类别的方法通过元学习这些元参数来分配信用，这改变了动作影响的度量，例如，通过优化 ET 的 $\lambda$。这些方法的性质与我们早期调查的其他方法有很大不同。因此，有时很难使用常规框架进行分析，我们将它们以不同的方式呈现，通过描述其主要思想和在
    Deep RL 中的实现方式。
- en: Meta Gradient (MG) RL
  id: totrans-449
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Meta Gradient (MG) RL
- en: '(Xu et al.,, [2018](#bib.bib211)) remarks how different CA methods underlie
    different choices of the target, and proposes to answer the question: “*Which
    target results in the best performance?*”. The method interprets the target as
    a parametric, differentiable function that can be used and modified by the agent
    to guide its behaviour to achieve the highest returns. In particular, Meta-Gradients
    consider the $\lambda$-return (Sutton,, [1988](#bib.bib177)) target, for it can
    generalise the choice of many targets (Schulman et al.,, [2016](#bib.bib159)),
    and learn its meta-parameters, the bootstrapping parameter $\lambda$ and the discount
    factor $\gamma$, via online cross validation (Sutton,, [1992](#bib.bib179)). In
    this review, we are interested in their formulation of the evaluation problem
    with differentiable, non-linear function approximators. After collecting a batch
    of trajectories, the agent computes the $\lambda$-return for each trajectory with
    the current guess of parameters and meta-parameters. One can then calculate either
    the gradient of the parameters, or the meta-gradient of the meta-parameters, and
    perform updates accordingly. In Meta-gradient RL, the action influence is akin
    to that of the methods based on the ET, but the hyperparameters of the TD ($\lambda$)
    are meta-optimised. Zheng et al., ([2018](#bib.bib219)) also propose to learn
    the target online but they consider the value of the learned target as an intrinsic
    reward. Since their introduction, meta-gradients have been applied to different
    meta-parameters.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: (Xu et al., [2018](#bib.bib211)) 指出，不同的 CA 方法在选择目标时存在差异，并提出了回答以下问题的建议：“*哪个目标能够实现最佳表现？*”。该方法将目标解释为一个可用于引导代理行为以实现最高回报的参数化、可微分函数，并可以由代理进行修改。特别地，Meta-Gradients
    考虑了 $\lambda$-return (Sutton, [1988](#bib.bib177)) 目标，因为它可以泛化许多目标的选择 (Schulman
    et al., [2016](#bib.bib159))，并通过在线交叉验证 (Sutton, [1992](#bib.bib179)) 学习其元参数，即引导参数
    $\lambda$ 和折扣因子 $\gamma$。在这次回顾中，我们对他们在可微非线性函数逼近器下的评价问题的公式化感兴趣。在收集一批轨迹后，代理使用当前参数和元参数的猜测计算每个轨迹的
    $\lambda$-return。然后可以计算参数的梯度或元参数的元梯度，并相应地进行更新。在 Meta-gradient RL 中，动作影响类似于基于 ET
    的方法，但 TD ($\lambda$) 的超参数被进行元优化。Zheng et al., ([2018](#bib.bib219)) 还提出了在线学习目标的建议，但他们将学习到的目标视为内在奖励。自其提出以来，元梯度已被应用于不同的元参数。
- en: Flexible Reinforcement Objective Discovered Online (FRODO)
  id: totrans-451
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 灵活强化目标在线发现 (FRODO)
- en: (Xu et al.,, [2020](#bib.bib210)) scales up meta-gradients by parameterising
    the target with a neural network and learning its meta-parameters online.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: (Xu et al., [2020](#bib.bib210)) 通过用神经网络对目标进行参数化并在线学习其元参数来扩展了元梯度。
- en: Distributional meta-gradient
  id: totrans-453
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分布式元梯度
- en: (Yin et al.,, [2023](#bib.bib213)) combines the idea of meta-gradients with
    distributional RL (Bellemare et al.,, [2017](#bib.bib25)), and learn the meta-parameters
    of the target by meta-learning the distribution of the returns.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: (Yin等人，[2023](#bib.bib213)) 将元梯度的思想与分布式RL（Bellemare等人，[2017](#bib.bib25)）结合起来，通过元学习回报分布来学习目标的元参数。
- en: Zheng et al., ([2020](#bib.bib218)) meta-learn intrinsic rewards across multiple
    lifetimes, where it is clearer that the intrinsic reward itself is a proxy for
    credit.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: Zheng等人（[2020](#bib.bib218)）在多个生命周期中进行内在奖励的元学习，其中内在奖励本身更清楚地充当了信用的代理。
- en: Summary.
  id: totrans-456
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 概述。
- en: Overall these methods assign credit to actions by applying canonical RL algorithms
    to a meta-learning goal. The goal can come in the form of an update target (Xu
    et al.,, [2018](#bib.bib211); Zheng et al.,, [2018](#bib.bib219); Xu et al.,,
    [2020](#bib.bib210)), a full return distribution (Yin et al.,, [2023](#bib.bib213)),
    or a reward function (Zheng et al.,, [2020](#bib.bib218)). Since these methods
    are not explicitly designed for CA, it is not clear what their performance is
    with respect the challenges described in Section [5](#S5 "5 The challenges to
    assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"). However, despite the lack of diagnostic experiments that stress specific
    aspects of credit assignment, these methods have been shown to perform well in
    complex RL tasks, such as the ALE (Bellemare et al.,, [2013](#bib.bib26)).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这些方法通过将经典RL算法应用于元学习目标来分配行动的信用。这个目标可以是更新目标（Xu等人，[2018](#bib.bib211)；Zheng等人，[2018](#bib.bib219)；Xu等人，[2020](#bib.bib210)）、完整的回报分布（Yin等人，[2023](#bib.bib213)），或者奖励函数（Zheng等人，[2020](#bib.bib218)）。由于这些方法并非专门为CA设计，因此尚不清楚它们在与第[5](#S5
    "5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")节中描述的挑战相关的情况下的表现如何。然而，尽管缺乏针对信用分配特定方面的诊断实验，这些方法在复杂的RL任务中表现良好，例如ALE（Bellemare等人，[2013](#bib.bib26)）。
- en: 7 Evaluating credit
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 评估信用
- en: The aim of this section is to survey the state of the art in the metrics, the
    tasks and the evaluation protocols to evaluate a CA algorithm. Like accurate evaluation
    is fundamental to RL agents to improve their policy, evaluating a method is fundamental
    to our research to monitor if and how the field is advancing. We discuss the main
    components of the evaluation procedure, the tasks, the performance metrics and
    the protocol in the three subsections, respectively. We start with the metrics
    and protocol altogether, as they should be as agnostic as possible to the considered
    task.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目的是调查评估CA算法的度量标准、任务和评估协议的最新进展。正如准确评估对强化学习（RL）代理改善其策略至关重要一样，评估一种方法对我们的研究也至关重要，以监测该领域是否以及如何进展。我们将分别讨论评估程序的主要组成部分、任务、性能指标和协议。我们从度量标准和协议开始，因为它们应该尽可能与所考虑的任务无关。
- en: 7.1 Metrics
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 度量标准
- en: We categorise the existing metrics to evaluate a CA method in two main classes.
    The first class uses the metrics already used for control problems. These mostly
    aim to assess the agent’s ability to make optimal decisions, but they do not explicitly
    measure the accuracy of the action influence. The second class aims to assess
    the quality of an assignment directly, and usually aggregates metrics over the
    course of the RL training procedure. We now proceed to describe the two classes
    of metrics.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将现有的CA方法评估度量标准分为两大类。第一类使用已经用于控制问题的度量标准。这些度量标准主要旨在评估代理做出最佳决策的能力，但它们并不明确测量动作影响的准确性。第二类旨在直接评估分配的质量，通常在RL训练过程中汇总度量标准。接下来我们将描述这两类度量标准。
- en: 7.1.1 Metrics borrowed from control
  id: totrans-462
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1 从控制中借用的度量标准
- en: Bias, variance and contraction rate.
  id: totrans-463
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 偏差、方差和收敛率。
- en: The first, intuitive, obvious proxy to assess the quality of credit assignment
    methods is the performance in suitable *control* problems. Formally, we refer
    to the bias, variance and contraction rate of the policy improvement operator
    described by Rowland et al., ([2020](#bib.bib150)), which we now recall. For the
    evaluation operator described in ([2](#S3.E2 "In Generalised Policy Iteration
    (GPI). ‣ 3 Notation and Background ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning")), we can specify these quantities as follows. Notice
    that these metrics are not applicable to all the methods, either some of the variables
    cannot be accessed or because operators they act on are not formally defined for
    the method in question.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，直观且明显的代理指标来评估信用分配方法的质量是其在合适的*控制*问题中的表现。正式地，我们参考Rowland等人描述的策略改进算子的偏差、方差和收敛率（[2020](#bib.bib150)），我们现在回顾一下。对于在([2](#S3.E2
    "在广义策略迭代（GPI）中。 ‣ 3 符号和背景 ‣ 深度强化学习中时间信用分配的综述"))中描述的评估算子，我们可以如下指定这些量。注意，这些指标并不适用于所有方法，因为某些变量无法访问，或者因为它们作用的算子在所讨论的方法中没有形式上定义。
- en: '|  | $\displaystyle\Gamma$ | $\displaystyle=\sup_{s\in\mathcal{S}}\frac{&#124;&#124;\mathcal{T}V^{\pi}(s)-\mathcal{T}V^{\prime\pi}(s)&#124;&#124;_{\infty}}{&#124;&#124;V^{\pi}(s)-V^{\prime\pi}(s)&#124;&#124;_{\infty}}$
    |  | (29) |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Gamma$ | $\displaystyle=\sup_{s\in\mathcal{S}}\frac{&#124;&#124;\mathcal{T}V^{\pi}(s)-\mathcal{T}V^{\prime\pi}(s)&#124;&#124;_{\infty}}{&#124;&#124;V^{\pi}(s)-V^{\prime\pi}(s)&#124;&#124;_{\infty}}$
    |  | (29) |'
- en: is the contraction rate and describes, how fast the operator converges to its
    fixed point, if it does so, and thus how efficient it is. Here $V^{\pi}(s)$ and
    $V^{\prime\pi}(s)$ are two estimates of the value of a state.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 是收缩率，并描述了算子收敛到其固定点的速度，如果它确实收敛，并且因此它的效率。这里$V^{\pi}(s)$和$V^{\prime\pi}(s)$是状态的两个值估计。
- en: 'If $\mathcal{T}$ is contractive, that is, if $\forall\mathcal{T}:\Gamma<1$,
    there exist a fixed-point bias of $\mathcal{T}$ is given by:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$\mathcal{T}$是收缩的，即，如果$\forall\mathcal{T}:\Gamma<1$，则$\mathcal{T}$的固定点偏差由以下公式给出：
- en: '|  | $\displaystyle\xi$ | $\displaystyle=&#124;&#124;V^{\pi}(s)-V^{*\pi}(s)&#124;&#124;_{2},$
    |  | (30) |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\xi$ | $\displaystyle=&#124;&#124;V^{\pi}(s)-V^{*\pi}(s)&#124;&#124;_{2},$
    |  | (30) |'
- en: where $\hat{V}^{\pi}(s)$ is the true, unique fixed point of $\mathcal{T}$, whose
    existence is guaranteed by $\Gamma<1$. For every evaluation operator $\mathcal{T}$
    there is an update rule $\Lambda:\mathbb{R}^{|\mathcal{S}|}\times\mathcal{D}\rightarrow\mathbb{R}$
    that takes as input a value estimate and a trajectory and outputs an update for
    the value. $\Lambda$ has a variance
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\hat{V}^{\pi}(s)$是$\mathcal{T}$的真实唯一固定点，其存在性由$\Gamma<1$保证。对于每个评估算子$\mathcal{T}$，都有一个更新规则$\Lambda:\mathbb{R}^{|\mathcal{S}|}\times\mathcal{D}\rightarrow\mathbb{R}$，它以值估计和轨迹作为输入，并输出值的更新。$\Lambda$有一个方差
- en: '|  | $\displaystyle\nu$ | $\displaystyle=\mathbb{E}_{\mu,\pi}[&#124;&#124;\Lambda[V(s),D]-\mathcal{T}V(s)&#124;&#124;_{2}^{2}].$
    |  | (31) |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nu$ | $\displaystyle=\mathbb{E}_{\mu,\pi}[&#124;&#124;\Lambda[V(s),D]-\mathcal{T}V(s)&#124;&#124;_{2}^{2}].$
    |  | (31) |'
- en: These three quantities are usually in a trade-off (Rowland et al.,, [2020](#bib.bib150)).
    Indeed, many (if not all) studies on credit assignment (Hung et al.,, [2019](#bib.bib75);
    Mesnard et al.,, [2021](#bib.bib112); Ren et al.,, [2022](#bib.bib148); Raposo
    et al.,, [2021](#bib.bib146)) report the empirical return and its variance. Because
    the contraction rate is often harder to calculate, an alternative metric is the
    time-to-performance, which evaluates the number of interactions necessary to reach
    a given performance. These mostly aim at showing improvement in sample efficiency
    and/or asymptotical performance. While useful, this is often not enough to assess
    the quality of credit assignment, as superior returns can be the result of better
    exploration, better optimisation, better representation learning, luck (as per
    the environment dynamics’ stochasticity) or of a combination of such factors.
    Nonetheless, when the only difference between two RL algorithms lies in how credit
    is assigned, and this is not confounded by factors aforementioned, it is generally
    safe to attribute improvements to superior credit, given that the improvements
    are statistically significant (Henderson et al.,, [2018](#bib.bib69); Agarwal
    et al.,, [2021](#bib.bib2)). Notice that this metrics can only be applied to measure
    of influence that result from the fixed point iterations.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个量通常处于权衡关系中（Rowland 等，[2020](#bib.bib150)）。事实上，许多（如果不是全部）关于信用分配的研究（Hung 等，[2019](#bib.bib75)；Mesnard
    等，[2021](#bib.bib112)；Ren 等，[2022](#bib.bib148)；Raposo 等，[2021](#bib.bib146)）报告了经验回报及其方差。由于收敛速率通常较难计算，一种替代指标是时间到性能，它评估达到给定性能所需的互动次数。这些指标主要旨在展示样本效率和/或渐近性能的改进。尽管有用，但这通常不足以评估信用分配的质量，因为更高的回报可能是更好的探索、更好的优化、更好的表征学习、运气（根据环境动态的随机性）或这些因素的组合的结果。然而，当两个
    RL 算法之间唯一的差异在于如何分配信用，并且没有受到前述因素的混淆时，通常可以将改进归因于更优的信用分配，前提是改进具有统计学意义（Henderson 等，[2018](#bib.bib69)；Agarwal
    等，[2021](#bib.bib2)）。注意，这些指标只能应用于衡量固定点迭代产生的影响。
- en: Task completion rate.
  id: totrans-472
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 任务完成率。
- en: 'A related, but more precise set of metrics is the task completion rate. Given
    a budget of trials, the task completion rate measures the frequency of success,
    that is the number of times the task was solved over the total number of trials.
    Considering completion rates instead of bias, variance and trade-off is useful
    as it alleviates another issue of these performance metrics: there is no distinction
    between easy-to-optimise rewards and hard-to-optimise rewards. We will illustrate
    that on the Key-to-Apple-to-Door task. Due to the stochasticity from the Apple
    phase, it is generally impossible to distinguish performance on apple picking
    (easy-to-optimise rewards) and door opening (hard-to-optimise rewards, that superior
    credit assignment methods should make easier to obtain over time). However, notice
    that this clarity in reporting credit comes at a cost. In fact, these kind of
    metrics necessitate expert knowledge about the task at hand, but are more precise
    than performance metrics. They often suffer from the same confounders as performance
    metrics.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关但更精确的指标是任务完成率。给定一个试验预算，任务完成率衡量成功的频率，即任务解决的次数与总试验次数的比率。考虑完成率而不是偏差、方差和权衡是有用的，因为它缓解了这些性能指标的另一个问题：没有区分易于优化的奖励和难以优化的奖励。我们将在
    Key-to-Apple-to-Door 任务中说明这一点。由于 Apple 阶段的随机性，通常无法区分苹果采摘（易于优化的奖励）和开门（难以优化的奖励，更优的信用分配方法应使其随时间更易获得）的表现。然而，注意到这种信用报告的清晰度是有代价的。事实上，这些指标需要对当前任务有专家知识，但比性能指标更精确。它们通常受与性能指标相同的混淆因素的影响。
- en: Value error.
  id: totrans-474
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 值错误。
- en: As the value (resp. action-value) function is at the heart of many credit assignment
    methods, another proxy to the quality of the credit is the quality of value estimation,
    which can be estimated from the distribution of TD errors (Andrychowicz et al.,,
    [2017](#bib.bib5); Rauber et al.,, [2019](#bib.bib147); Arjona-Medina et al.,,
    [2019](#bib.bib7)). A drawback of the expected TD error is that it can be misleading.
    When an algorithm does not fully converge, for example, because of a sparse reward
    function, it can happen that the value error is very low. This is because the
    current policy never visits a state with reward different from zero, and the value
    function collapses to always return zero. This only applies to CA methods that
    do not circumvent the value function altogether.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 由于值（即行动值）函数是许多信用分配方法的核心，另一种衡量信用质量的代理是值估计的质量，这可以通过TD误差的分布来估计（Andrychowicz 等，[2017](#bib.bib5)；Rauber
    等，[2019](#bib.bib147)；Arjona-Medina 等，[2019](#bib.bib7)）。期望的TD误差的一个缺点是它可能具有误导性。例如，当算法没有完全收敛时，例如由于稀疏的奖励函数，值误差可能非常低。这是因为当前的策略从未访问过奖励非零的状态，值函数收敛于始终返回零。这仅适用于那些不完全规避值函数的CA方法。
- en: 7.1.2 Bespoke metrics for credit assignments
  id: totrans-476
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2 针对信用分配的定制指标
- en: We now review metrics that measure the quality of individual credit assignments,
    that is how well actions are mapped to corresponding outcomes, or how well outcomes
    are redistributed to past actions. Usually, these metrics are calculated in hindsight,
    after outcomes have been observed.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在回顾衡量单个信用分配质量的指标，即行动与相应结果的映射程度，或结果如何重新分配给过去的行动。通常，这些指标是在观察到结果之后进行事后计算的。
- en: Using knowledge about the causal structure.
  id: totrans-478
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用关于因果结构的知识。
- en: Given expert knowledge about the causal structure of the task at hand, that
    is which actions cause which outcomes, one can leverage it and compare it to the
    corresponding credit assignments, which approximate such cause and effect relationships.
    We give several examples from the literature. In Delayed Catch, Raposo et al.,
    ([2021](#bib.bib146)) look at the estimated credit for actions that lead to catches
    and the end-of-episode reward that only depends on catches. They do the same on
    the Atari game Skiing, which is a more complex environment but is similar in the
    sense that only getting between poles grants rewards at the end of episode. [Ferret
    et al., 2021a](#bib.bib46) adopt a similar approach and look at the estimated
    credit given to actions responsible for trigger switches in the Triggers environment,
    which contribute alone to the end-of-episode reward. Arjona-Medina et al., ([2019](#bib.bib7))
    look at redistributions of RUDDER on several tasks including the Atari 2600 game
    Bowling.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对任务的因果结构进行专家知识，即哪些行动会导致哪些结果，可以利用这些知识并将其与对应的信用分配进行比较，这些分配近似于因果关系。我们从文献中给出几个例子。在延迟捕捉中，Raposo
    等人（[2021](#bib.bib146)）查看了导致捕捉的行动的估计信用和仅依赖于捕捉的回合结束奖励。他们在Atari游戏滑雪中做了相同的事情，虽然环境更复杂，但在奖励方面类似，即只有在杆子之间移动才会在回合结束时获得奖励。[Ferret
    等人，2021a](#bib.bib46)采用了类似的方法，查看了在触发器环境中触发开关的行动的估计信用，这些行动单独贡献于回合结束的奖励。Arjona-Medina
    等人（[2019](#bib.bib7)）查看了在多个任务中，包括Atari 2600游戏保龄球上的RUDDER的重新分配。
- en: Counterfactual simulation.
  id: totrans-480
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 反事实模拟。
- en: A natural approach, which is nonetheless seldom explored in the literature,
    is counterfactual simulation. On a high-level, it consists in asking what would
    have happened if actions that are credited for particular outcomes had been replaced
    by another action. This is close to the notion of hindsight advantage.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 一种自然的方法，尽管在文献中很少被探讨，就是反事实模拟。从高层次上看，它包括询问如果将被认为是特定结果的行动替换为另一个行动会发生什么。这接近于事后洞察的概念。
- en: Comparing to actual values of the estimated quantity.
  id: totrans-482
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与估计量的实际值进行比较。
- en: This only applies to methods whose credit assignments are mathematically grounded,
    in the sense that they are the empirical approximations of well-defined quantities.
    In general, one can leverage extra compute and the ability to reset a simulator
    to arbitrary states to obtain accurate estimations of the underlying quantity,
    and compare it to the actual, resource-constrained quantity used as credit assignment.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅适用于那些在数学上有依据的信用分配方法，即它们是定义明确的量的经验近似值。一般而言，可以利用额外的计算资源和将模拟器重置为任意状态的能力，以获得底层量的准确估计，并将其与实际的、受资源限制的用于信用分配的量进行比较。
- en: 7.2 Tasks
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 任务
- en: 'In what follows, we present environments that we think are most relevant to
    evaluate credit assignment methods and individual credit assignments. The most
    significant tasks are those that present all the three challenges to assign credit:
    delayed rewards, transpositions and sparsity of the influence. This often corresponds
    to experiments that have reward delay, high marginal entropy of the reward, and
    partial observability. To benchmark explicit credit assignment methods, we additionally
    need to be able to recover the ground truth influence of actions w.r.t. given
    outcomes, or we can use our knowledge of the environment and develop more subjective
    measures.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示了我们认为最相关的环境，用于评估信用分配方法和个体信用分配。最重要的任务是那些展示所有三种信用分配挑战的任务：延迟奖励、转置和影响稀疏性。这通常对应于具有奖励延迟、高边际熵奖励和部分可观察性的实验。为了基准明确的信用分配方法，我们还需要能够恢复与给定结果相关的实际行动影响，或者我们可以利用对环境的知识，开发更主观的度量。
- en: 7.2.1 Diagnostic tasks
  id: totrans-486
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1 诊断任务
- en: Diagnostic tasks are useful as sanity checks for RL agents and present the advantage
    to be ran rather quickly, compared to complex environments with visual input that
    may imply several millions of samples before agents manage to solve the task at
    hand. Notice that these tasks may not be representative of the performance of
    the method at scale, but provide a useful signal to diagnose the behaviour of
    the algorithm in the challenges described in Section [5](#S5 "5 The challenges
    to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"). Sometimes, the same environment can represent both a diagnostic task
    and an experiment at scale simply by changing the space of the observations or
    the action space.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 诊断任务作为 RL 代理的理智检查非常有用，与可能需要数百万样本才能解决任务的复杂视觉输入环境相比，这些任务可以比较快速地运行。注意，这些任务可能无法代表方法在大规模上的表现，但提供了有用的信号以诊断算法在第[5](#S5
    "5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")节中描述的挑战中的行为。有时，通过更改观察空间或动作空间，相同的环境可以同时代表诊断任务和大规模实验。
- en: We first present chain-like environments, that can be represented graphically
    by a chain (environments a to c), and then a grid-like environment (environment
    d), that has more natural grid representations for both the environment and the
    state.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先展示链状环境，这些环境可以通过链的图形表示（环境a到c），然后是网格状环境（环境d），它具有更自然的网格表示，适用于环境和状态。
- en: a) Aliasing chain.
  id: totrans-489
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: a) 别名链。
- en: The aliasing chain (introduced in Harutyunyan et al., ([2019](#bib.bib67)) as
    Delayed Effect) is an environment whose outcome depends only on the first action.
    A series of perceptually aliased and zero-reward states follow this first action
    and an outcome is observed at the end of the chain ($+1$ or $-1$ depending on
    the binary first action).
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 别名链（Harutyunyan et al., [2019](#bib.bib67) 介绍为延迟效应）是一个环境，其结果仅依赖于第一次行动。第一次行动之后跟随一系列感知上类似且奖励为零的状态，并且在链的末尾观察到结果（$+1$或$-1$，取决于二元第一次行动）。
- en: b) Discounting chain.
  id: totrans-491
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: b) 折扣链。
- en: The discounting chain (Osband et al.,, [2020](#bib.bib127)) is an environment
    in which a first action leads to a series of states with inconsequential decisions
    with a final reward that is either $1$ or $1+\epsilon$, and a variable length.
    It highlights issues with the discounting horizon.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣链（Osband et al., [2020](#bib.bib127)）是一个环境，其中第一次行动导致一系列无关紧要的状态，并以一个最终奖励结束，奖励为$1$或$1+\epsilon$，长度可变。它突出了折扣视野的问题。
- en: c) Ambiguous bandit.
  id: totrans-493
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: c) 模糊强盗。
- en: 'The ambiguous bandit (Harutyunyan et al.,, [2019](#bib.bib67)) is a variant
    of a two-armed bandit problem. The agent is given two actions: one that transitions
    to a state with a slightly more advantageous Gaussian distribution over rewards
    with probability $1-\epsilon$, and another that does so with probability $\epsilon$.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊强盗（Harutyunyan et al., [2019](#bib.bib67)）是一个变种的二臂强盗问题。代理有两个行动：一个以概率$1-\epsilon$过渡到奖励上有稍微更有利的高斯分布的状态，另一个以概率$\epsilon$这样做。
- en: d) Triggers.
  id: totrans-495
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: d) 触发器。
- en: 'Triggers ([Ferret et al., 2021a,](#bib.bib46) ) is a family of environments
    and corresponding discrete control tasks that are suited for the quantitative
    analysis of the credit assignment abilities of RL algorithms. Each environment
    is a bounded square-shaped 2D gridworld where the agent collects rewards that
    are conditioned on the previous activation of all the triggers of the map. Collecting
    all triggers turns the negative value of rewards into positive and this knowledge
    can be exploited to assess proper credit assignment: the actions of collecting
    triggers appear natural to be credited. The environments are procedurally generated:
    when requesting a new environment, a random layout is drawn according to the input
    specifications.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: Triggers ([Ferret et al., 2021a,](#bib.bib46)) 是一系列环境及其对应的离散控制任务，适合用于定量分析 RL
    算法的信用分配能力。每个环境是一个有限的方形 2D 网格世界，其中代理收集的奖励取决于之前所有触发器的激活情况。收集所有触发器会将负奖励转化为正奖励，这些知识可以被利用来评估适当的信用分配：收集触发器的动作自然地被归因。环境是程序生成的：当请求新的环境时，会根据输入规格绘制一个随机布局。
- en: 7.2.2 Tasks at scale
  id: totrans-497
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2 大规模任务
- en: In the following, we present higher-dimension benchmarks for agents equipped
    with credit assignment capabilities.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们将介绍配备信用分配能力的代理的高维基准测试。
- en: Atari.
  id: totrans-499
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Atari.
- en: 'The Arcade Learning Environment (Bellemare et al.,, [2013](#bib.bib26)) (ALE)
    is an emulator in which RL agents compete to reach the highest scores on $56$
    classic Atari games. We list the ones we deem interesting for temporal credit
    assignment assessment due to delayed rewards, which were first highlighted by Arjona-Medina
    et al., ([2019](#bib.bib7)). Bowling: like in real-life bowling, the agent must
    throw a bowling ball at pins, while ideally curving the ball so that it can clear
    all pins in one throw. The agent experiences rewards with a high delay, at the
    end of all rolls (between $2$ and $4$ depending on the number of strikes achieved).
    Venture: the agent must enter a room, collect a treasure and shoot monsters. Shooting
    monsters only give rewards after the treasure was collected, and there is no in-game
    reward for collecting it. Seaquest: the agent controls a submarine and must sink
    enemy submarines. To reach higher scores, the agent has to additionally rescue
    divers that only provide reward once the submarine lacks oxygen and surfaces to
    replenish it. Solaris: the agent controls a spaceship that earns points by hunting
    enemy spaceships. These shooting phases are followed by the choice of the next
    zone to explore on a high-level map, which conditions future reward. Skiing: the
    agent controls a skier that has to go between poles while going down the slope.
    The agent gets no reward until reaching the bottom of the slope, at which time
    it receives a reward proportional to the pairs of poles it went through, which
    makes for long-term credit assignment.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: Arcade Learning Environment (Bellemare et al., [2013](#bib.bib26))（ALE）是一个模拟器，其中
    RL 代理竞赛以在 $56$ 款经典 Atari 游戏中获得最高分。我们列出了那些由于延迟奖励而被认为对时间信用分配评估有趣的游戏，这些延迟奖励最早由 Arjona-Medina
    et al., ([2019](#bib.bib7)) 提出。保龄球：与现实中的保龄球类似，代理必须将保龄球投向瓶子，同时理想情况下使球曲线，以便一次性击倒所有瓶子。代理在所有投球结束后（根据击倒的瓶子数量在
    $2$ 到 $4$ 之间）会经历高延迟奖励。冒险：代理必须进入一个房间，收集一个宝藏并击败怪物。击败怪物只有在宝藏被收集后才会给予奖励，并且收集宝藏本身没有游戏内奖励。海底探险：代理控制一艘潜水艇，必须击沉敌方潜艇。为了获得更高的分数，代理还需要救援潜水员，而潜水员只在潜艇缺氧并浮出水面以补充氧气时才会提供奖励。太阳神：代理控制一艘宇宙飞船，通过猎杀敌方宇宙飞船来获得积分。这些射击阶段之后是选择在高级地图上探索下一个区域，这将影响未来的奖励。滑雪：代理控制一个滑雪者，必须在下坡时通过标杆。代理在到达坡底之前没有奖励，到达坡底时，奖励与通过的标杆对数成正比，这使得信用分配具有长期性。
- en: VizDoom.
  id: totrans-501
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: VizDoom.
- en: VizDoom (Kempka et al.,, [2016](#bib.bib88)) is a suite of partially observable
    3D tasks based on the classical Doom video game, a first-person shooter. As mentioned
    before, it is an interesting sandbox for credit assignment because it optionally
    provides high-level information such as labelled game objects, depth as well as
    a top-view minimap representation; all of which can be used for approximate optimally
    efficient credit assignment algorithms.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: VizDoom (Kempka et al., [2016](#bib.bib88)) 是一组基于经典射击游戏 Doom 的部分可观察 3D 任务。正如前面提到的，它是一个有趣的信用分配沙箱，因为它可以选择性地提供高级信息，如标记的游戏对象、深度以及顶视图迷你地图表示；所有这些都可以用于近似最优高效的信用分配算法。
- en: BoxWorld.
  id: totrans-503
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: BoxWorld.
- en: BoxWorld (Zambaldi et al.,, [2018](#bib.bib215)) is a family of environments
    that shares similarities with Triggers, while being more challenging. Environments
    are also procedurally-generated square-shaped 2D gridworlds with discrete controls.
    The goal is to reach a gem, which requires going through a series of boxes protected
    by locks that can only be opened with keys of the same colour while avoiding distractor
    boxes. The relations between keys and locks can be utilised to assess assigned
    credit since the completion of the task (as well as intermediate rewards for opening
    locks) depends on the collection of the right keys.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: BoxWorld (Zambaldi et al., [2018](#bib.bib215)) 是一系列环境，与 Triggers 有相似之处，但更具挑战性。这些环境也是程序生成的方形二维网格世界，具有离散控制。目标是到达一个宝石，这需要穿过一系列被锁保护的箱子，这些锁只能用相同颜色的钥匙打开，同时避免分散注意力的箱子。钥匙和锁之间的关系可以用来评估分配的信用，因为任务的完成（以及打开锁的中间奖励）取决于收集正确的钥匙。
- en: Sokoban.
  id: totrans-505
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 箱子世界。
- en: Sokoban (Racanière et al.,, [2017](#bib.bib142)) is a family of environments
    that is similar to the two previous ones. The agent must push boxes to intended
    positions on the grid while avoiding dead-end situations (for instance, if a block
    is stuck against walls on two sides, it cannot be moved anymore). While there
    is no definite criterion to identify decisive actions, actions that lead to dead-ends
    are known and can be exploited to assess the quality of credit assignment.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: Sokoban (Racanière et al., [2017](#bib.bib142)) 是一个类似于前两者的环境系列。代理必须将箱子推到网格上的预定位置，同时避免死胡同情况（例如，如果一个箱子被困在两侧的墙壁之间，则无法再移动）。虽然没有确定的标准来识别决定性动作，但已知会导致死胡同的动作可以被利用来评估信用分配的质量。
- en: DeepMind Lab.
  id: totrans-507
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DeepMind Lab。
- en: 'DeepMind Lab (Beattie et al.,, [2016](#bib.bib22)) (DMLab) is a suite of partially
    observable 3D tasks with rich visual input. We identify several tasks that might
    be of interest to assess credit assignment capabilities, some of which were used
    in recent work. Keys-Doors: the agent navigates to keys that open doors (identified
    by their shared colour) so that it can get to an absorbing state represented by
    a cake. [Ferret et al., 2021a](#bib.bib46) consider a harder variant of the task
    where collecting keys is not directly rewarded anymore and feedback is delayed
    until opening doors. Keys-Apples-Doors: Hung et al., ([2019](#bib.bib75)) consider
    an extended version of the previous task. The agent still has to collect a key,
    but after a fixed duration a distractor phase begins in which it can only collect
    small rewards from apples, and finally the agent must find and open a door with
    the key it got in the initial phase. To solve the task, the agent has to learn
    the correlation or causation link between the key and the door, which is made
    hard because of the extended temporal distance between the two events and of the
    distractor phase. Deferred Effects: the agent navigates between two rooms, the
    first one of which contains apples that give low rewards, while the other contains
    cakes that give high rewards but is entirely in the dark. The agent can turn the
    light on by reaching the switch in the first room, but it gets an immediate negative
    reward for it. In the end, the most successful policy is to activate the switch
    regardless of the immediate cost so that a maximum number of cakes can be collected
    in the second room before the time limit.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 'DeepMind Lab (Beattie et al., [2016](#bib.bib22)) (DMLab) 是一套部分可观察的 3D 任务，具有丰富的视觉输入。我们识别了几项可能有兴趣评估信用分配能力的任务，其中一些在近期工作中被使用。Keys-Doors:
    代理导航到可以打开门的钥匙（由其共享颜色标识），以便到达由蛋糕表示的吸收状态。[Ferret et al., 2021a](#bib.bib46) 考虑了任务的一个更困难的变体，其中收集钥匙不再直接得到奖励，反馈被延迟到打开门时。Keys-Apples-Doors:
    Hung et al., ([2019](#bib.bib75)) 考虑了前述任务的扩展版本。代理仍需收集钥匙，但在固定时间后开始一个干扰阶段，此阶段仅能从苹果中获得小奖励，最后代理必须找到并用最初阶段获得的钥匙打开一扇门。为了解决任务，代理必须学习钥匙和门之间的相关性或因果关系，这由于两事件之间的时间距离和干扰阶段而变得困难。延迟效果：代理在两个房间之间导航，第一个房间包含给出低奖励的苹果，而第二个房间包含给出高奖励的蛋糕，但完全黑暗。代理可以通过到达第一个房间的开关来打开灯，但会立即收到负奖励。最终，最成功的策略是激活开关，无论立即成本如何，以便在时间限制之前在第二个房间收集最多的蛋糕。'
- en: 7.3 Protocol
  id: totrans-509
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 协议
- en: Online evaluation.
  id: totrans-510
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在线评估。
- en: The most standard approach is to evaluate the quality of credit assignment methods
    and individual credit assignments along the RL training procedure. As the policy
    changes, the credit assignments change since the effect of actions depends on
    subsequent actions (which are dictated by the policy). One can dynamically track
    the quality of credit assignments and that of the credit assignment method using
    the metrics developed in the previous section. For the credit assignment method,
    since it requires a dataset of interaction, one can consider using the most trajectories
    produced by the agent. An advantage of this approach is that it allows evaluating
    the evolution of the credit assignment quality along the RL training, with an
    evolving policy and resulting dynamics. Also, since the goal of credit assignment
    is to help turn feedback into improvements, it makes sense to evaluate it in the
    context of said improvements. While natural, online evaluation means one has little
    control over the data distribution of the evaluation. This is problematic because
    it is generally hard to disentangle credit quality from the nature of the trajectories
    it is evaluated on. A corollary is that outcomes that necessitate precise exploration
    (which can be the outcomes for which agents would benefit most from accurate credit
    assignment) might not be explored.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 最标准的方法是评估信用分配方法和个别信用分配在RL训练过程中的质量。随着策略的变化，信用分配也会改变，因为行动的效果取决于后续的行动（这些行动由策略决定）。可以使用上一节开发的指标动态跟踪信用分配的质量和信用分配方法的质量。对于信用分配方法，由于它需要一个交互数据集，可以考虑使用代理生成的最多轨迹。这种方法的一个优点是，它允许在RL训练过程中评估信用分配质量的演变，包括不断变化的策略和结果动态。此外，由于信用分配的目标是帮助将反馈转化为改进，因此在改进的背景下评估它是有意义的。虽然自然，在线评估意味着对评估的数据分布几乎没有控制。这是有问题的，因为通常很难将信用质量与其评估的轨迹性质区分开来。一个推论是，需要精确探索的结果（可能是代理从准确信用分配中获益最多的结果）可能不会被探索。
- en: Offline evaluation.
  id: totrans-512
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 离线评估。
- en: An alternative is to consider offline evaluation. It requires a dataset of interactions,
    either collected before or during the RL training. Credit assignments and the
    credit assignment method then use the parameters learned during the RL training
    while being evaluated on the offline data. As the policy in the offline data is
    generally not the latest policy from the online training, offline evaluation is
    better suited for policy-conditioned credit assignment or (to some extent) trajectory-conditioned
    credit assignment. Indeed, other forms of credit assignment are specific to a
    single policy, and evaluating these on data generated from another policy would
    not be accurate. An important advantage of offline evaluation is that it alleviates
    the impact of exploration, as one controls the data distribution credit is evaluated
    on.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是考虑离线评估。它需要一个交互数据集，无论是训练前还是训练期间收集的。信用分配和信用分配方法在离线数据上进行评估时使用在RL训练期间学到的参数。由于离线数据中的策略通常不是来自在线训练的最新策略，离线评估更适合用于策略条件下的信用分配或（在某种程度上）轨迹条件下的信用分配。确实，其他形式的信用分配是特定于单一策略的，在来自其他策略的数据上评估这些形式的信用分配将不准确。离线评估的一个重要优点是它缓解了探索的影响，因为可以控制信用评估的数据分布。
- en: 8 Closing, discussion and open challenges
  id: totrans-514
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论、讨论和开放挑战
- en: The CAP is the problem to approximate the causal influence of an action from
    a finite amount of experience, and it is of critical importance to deploy RL agents
    into the real world that are effective, general, safe and interpretable. However,
    there is a misalignment in the current literature on what credit means in words
    and how it is formalised. In this survey, we put the basis to reconcile this gap
    by reviewing the state of the art of the temporal CAP in Deep RL, focusing on
    three major questions.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: CAP 是从有限经验中近似行动的因果影响的问题，对于将RL代理部署到现实世界中，使其有效、通用、安全和可解释至关重要。然而，目前文献中对信用的定义和形式化存在不一致。在本调查中，我们通过回顾深度RL中时间CAP的最新进展，为弥合这一差距奠定了基础，重点关注三个主要问题。
- en: 8.1 Summary
  id: totrans-516
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 摘要
- en: Overall, we observed tree major fronts of development around the CAP.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们观察到CAP的发展主要集中在三个前沿。
- en: The first front concerns the problems of how to quantify action influence. In
    Section [4](#S4 "4 Quantifying action influences ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning") we addressed [Q1.](#S1.I1.i1 "item
    Q1\. ‣ Motivation. ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning") and analysed the quantities that the existing works
    use to represent the influence of an action. In Section [4.1](#S4.SS1 "4.1 Are
    all action values, credit? ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning") we unified these measures of
    action influence in the assignment class. In Sections [4.5](#S4.SS5 "4.5 Existing
    assignment functions ‣ 4 Quantifying action influences ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning") and [4.3](#S4.SS3 "4.3 What
    is an assignment? ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning") we showed that the existing literature
    agrees on an intuition of credit as causal influence, but that it does not translate
    that well into mathematics and none of the current quantities are satisfactory
    measures of causal influence. As a consequence, we proposed a set of principles
    that we suggest a measure of action influence should respect to represent credit.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 首要问题涉及如何量化行动的影响。在第[4节](#S4 "4 量化行动影响 ‣ 深度强化学习中的时序信用分配调查")中，我们讨论了[Q1.](#S1.I1.i1
    "条目 Q1\. ‣ 动机 ‣ 1 引言 ‣ 深度强化学习中的时序信用分配调查")并分析了现有工作中用来表示行动影响的量度。在第[4.1节](#S4.SS1
    "4.1 所有行动值都是信用吗？ ‣ 4 量化行动影响 ‣ 深度强化学习中的时序信用分配调查")中，我们统一了这些行动影响的量度。在第[4.5节](#S4.SS5
    "4.5 现有分配函数 ‣ 4 量化行动影响 ‣ 深度强化学习中的时序信用分配调查")和第[4.3节](#S4.SS3 "4.3 什么是分配？ ‣ 4 量化行动影响
    ‣ 深度强化学习中的时序信用分配调查")中，我们展示了现有文献对信用作为因果影响的直觉达成了一致，但这种直觉在数学表达上并不那么理想，目前的量度也都无法令人满意地衡量因果影响。因此，我们提出了一套原则，建议一种行动影响量度应当遵循这些原则，以准确表示信用。
- en: 'The second front aims to address the question of how to learn an action influence
    from experience and concerns the methods to assign credit. In Section [5](#S5
    "5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning") we looked at the challenges that arise from learn
    these measures of action influence and, together with Section [6](#S6 "6 Methods
    to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"), answered [Q2.](#S1.I1.i2 "item Q2\. ‣ Motivation. ‣ 1 Introduction
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning"). We
    first reviewed the most common obstacles to learning already identified in literature
    and realigned them to our newly developed formalism. We identified three dimensions
    of an MDP, depth, breadth and density and described pathological conditions on
    each of them that hinder the CA. In Section [6](#S6 "6 Methods to assign credit
    in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")
    we defined a CA as an algorithm whose aim is to approximate a measure of action
    influence from a finite amount of experience. We categorised methods into those
    that: (i) use temporal contiguity as a proxy for causal influence; (ii) decompose
    the total return into smaller per-timestep contributions; (iii) condition the
    present on information about the future using the idea of hindsight; (iv) use
    sequence modelling and represent action influence as the likelihood of action
    to follow a state and predict an outcome; (v) learn to imagine backward transitions
    that always start at a key state and propagate back to the state that could generate
    them; (vi) meta-learn action influence measures.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个方向旨在解决如何从经验中学习行动影响的问题，并涉及信用分配的方法。在第[5](#S5 "5 The challenges to assign credit
    in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")节中，我们研究了学习这些行动影响度量所带来的挑战，并与第[6](#S6
    "6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")节一起回答了[Q2.](#S1.I1.i2 "item Q2\. ‣ Motivation.
    ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")。我们首先回顾了文献中已识别出的最常见障碍，并将其重新调整到我们新开发的形式化理论中。我们确定了MDP的三个维度：深度、广度和密度，并描述了每个维度上的病态条件，这些条件妨碍了CA。在第[6](#S6
    "6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")节中，我们定义了CA作为一种旨在从有限经验中近似行动影响度量的算法。我们将方法分为以下几类：(i)
    使用时间连贯性作为因果影响的代理；(ii) 将总回报分解为较小的每步贡献；(iii) 使用回顾的思想以未来信息为条件对当前进行条件化；(iv) 使用序列建模，将行动影响表示为状态下行动的可能性并预测结果；(v)
    学习想象向后转移，这些转移总是从关键状态开始，并向后传播到可能生成这些转移的状态；(vi) 元学习行动影响度量。
- en: 'Finally, the third research front deals with how to evaluate quantities and
    methods to assign credit and aims to provide an unbiased estimation of the progress
    in the field. In Section [7](#S7 "7 Evaluating credit ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning") we addressed [Q3.](#S1.I1.i3 "item
    Q3\. ‣ Motivation. ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning") and analysed how current methods evaluate their
    performance and how we can monitor future advancements. We highlighted that current
    benchmarks are not fit for the purpose. Diagnostic benchmarks do not isolate the
    specific CAP challenges identified in Section [5](#S5 "5 The challenges to assign
    credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning"): delayed effects, transpositions and sparsity. We explained that benchmarks
    at scale often cannot disentangle the CAP from the exploration problem and it
    becomes hard to understand whether a method is advancing one problem or another.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第三个研究方向涉及如何评估数量和方法来分配信用，并旨在提供对该领域进展的无偏估计。在第[7](#S7 "7 Evaluating credit ‣
    A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")节中，我们讨论了[Q3.](#S1.I1.i3
    "item Q3\. ‣ Motivation. ‣ 1 Introduction ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")，分析了当前方法如何评估其性能以及我们如何监控未来的进展。我们强调了当前基准测试不适合这个目的。诊断基准测试没有孤立出在第[5](#S5
    "5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")节中确定的特定CAP挑战：延迟效应、置换和稀疏性。我们解释说，大规模的基准测试通常不能将CAP与探索问题区分开来，因此很难理解某个方法是在推进一个问题还是另一个问题。
- en: 8.2 Discussion and open challenges
  id: totrans-521
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 讨论与开放挑战
- en: As this survey suggest, the work in the field is now fervent and the number
    of studies in a bullish trend, with many works showing substantial gains in control
    problems only by – to the best of our current knowledge – advancing on the CAP
    alone (Bellemare et al.,, [2017](#bib.bib25); van Hasselt et al.,, [2021](#bib.bib190);
    Edwards et al.,, [2018](#bib.bib40); Mesnard et al.,, [2021](#bib.bib112), [2023](#bib.bib111)).
    We observed that the take-off of CA research in the broader area of RL research
    is only recent. Most probably the reason of this only recent take-off is to be
    found in the hierarchy of problems in the broader RL field. The tasks considered
    in earlier Deep RL research were simple from the CA point of view because otherwise
    it would have not been possible to solve them. Using tasks where assigning credit
    is hard would have – and probably still do, e.g., Küttler et al., ([2020](#bib.bib94))
    – obfuscate other problems that it was necessary to solve before solving the CAP.
    For example, adding the CAP on the top of scaling RL to high-dimensional observations
    (Arulkumaran et al.,, [2017](#bib.bib9)) or dealing with large action spaces (Dulac-Arnold
    et al.,, [2015](#bib.bib39); van Hasselt and Wiering,, [2009](#bib.bib192)) would
    have, most likely, concealed any evidence of progress for the underlying challenges.
    This is also why CA methods do not usually shine in classical benchmark (Bellemare
    et al.,, [2013](#bib.bib26)) and peer reviews are often hard on these works.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这项调查所示，该领域的工作现在非常活跃，研究数量呈现出看涨趋势，许多工作仅通过——根据我们目前的知识——在CAP上取得了显著的控制问题进展（Bellemare
    et al., [2017](#bib.bib25); van Hasselt et al., [2021](#bib.bib190); Edwards et
    al., [2018](#bib.bib40); Mesnard et al., [2021](#bib.bib112), [2023](#bib.bib111)）。我们观察到，CA研究在更广泛的RL研究领域中的起步仅为近期。造成这一近期起步的原因很可能在于更广泛RL领域中的问题层次。早期深度RL研究中考虑的任务从CA的角度看较为简单，否则将无法解决这些任务。使用难以分配信用的任务会——并且可能仍然会，例如，Küttler
    et al., ([2020](#bib.bib94))——掩盖在解决CAP之前必须解决的其他问题。例如，将CAP添加到将RL扩展到高维观察（Arulkumaran
    et al., [2017](#bib.bib9)）或处理大规模动作空间（Dulac-Arnold et al., [2015](#bib.bib39);
    van Hasselt and Wiering, [2009](#bib.bib192)）的顶部，很可能会掩盖任何进展的证据。这也是为什么CA方法在经典基准（Bellemare
    et al., [2013](#bib.bib26)）中通常不显眼且同行评审常对这些工作严格的原因。
- en: On this background, the CAP still hold open question and there is still much
    discussion required to consider the problem solved. In particular, the following
    observations describe our positions with respect to this survey.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种背景下，CAP仍然存在未解的问题，还需要更多讨论才能认为问题得到解决。特别是，以下观察描述了我们对这项调查的立场。
- en: Aligning future works to a common problem definition.
  id: totrans-524
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将未来的工作与共同的问题定义对齐。
- en: The lack of a review since its conception (Minsky,, [1961](#bib.bib114)) and
    the rapid advancements produced a fragmented landscape of definitions for action
    influence, an ambiguity in the meaning of credit assignment, a misalignment between
    the general intuition and its practical quantification, and a general lack of
    coherence in the principal directions of the works. While this diversity being
    beneficial for the diversification of the research, it is also detrimental to
    comparing the same works, as their starting point and their aim only intersect
    in a few places. Future works aiming to propose a new CA method should clarify
    these preliminary concepts. Answers to “What is the choice of the measure of action
    influence? Why the choice? What is the method to learn that influence from experience?
    How is it evaluated?” would be good a starting point.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 自从其构思以来缺乏综述（Minsky, [1961](#bib.bib114)），以及快速进展产生了一个碎片化的行动影响定义景观，信用分配意义的模糊性，整体直觉与实际量化之间的错位，以及主要方向上的一般缺乏一致性。虽然这种多样性对研究的多样化有利，但也不利于对相同工作的比较，因为它们的起点和目标仅在少数地方相交。未来旨在提出新CA方法的工作应澄清这些初步概念。回答“行动影响的测量选择是什么？为什么选择？从经验中学习这种影响的方法是什么？如何评估？”将是一个好的起点。
- en: Characterising credit.
  id: totrans-526
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 特征化信用。
- en: “What is the *minimum* set of properties that a measure of action influence
    should respect to inform control? What the more desirable ones?”. This question
    remains unanswered, with some ideas in Ferret, ([2022](#bib.bib45), Chapter 4),
    and we still need to understand what characterises a proper measure of credit.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: “衡量行动影响的测量应尊重哪些*最小*属性以告知控制？哪些是更理想的？”这个问题仍然没有答案，一些想法在Ferret, ([2022](#bib.bib45),
    第4章)中提出，我们仍然需要理解什么特征是适当的信用测量。
- en: Causality.
  id: totrans-528
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 因果关系。
- en: 'The relationship between CA and causality is underexplored but in a small subset
    of works (Mesnard et al.,, [2021](#bib.bib112); Pitis et al.,, [2020](#bib.bib136);
    Buesing et al.,, [2019](#bib.bib29)). The literature lacks a clear and complete
    formalisms that casts the CAP as a problem of causal discovery. Investigating
    this connection and formalising a measure of action influence that is also a satisfactory
    measure of causal influence would help better understand the effects of choosing
    a measure of action influence over another. Overall, we need a better understanding
    of the connection between CA and causality: what happens when credit is a strict
    measure of causal influence? How do current algorithms perform with respect to
    this measure? Can we devise an algorithm that exploits a causal measure of influence?'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: CA与因果关系之间的关系尚未被充分探索，但在一些小范围的研究中有所涉及（Mesnard等，[2021](#bib.bib112)；Pitis等，[2020](#bib.bib136)；Buesing等，[2019](#bib.bib29)）。现有文献缺乏明确和完整的形式化方法，将CAP视为因果发现的问题。研究这一联系并形式化一个动作影响的度量，同时使其成为一个令人满意的因果影响度量，将有助于更好地理解选择某种动作影响度量而非其他度量的效果。总体而言，我们需要更好地理解CA与因果关系之间的联系：当信用是严格的因果影响度量时会发生什么？当前算法在这一度量下的表现如何？我们能否设计出利用因果影响度量的算法？
- en: Optimal credit.
  id: totrans-530
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最优信用。
- en: Many works refer to optimal credit or to assigning credit optimally but it is
    unclear what that formally means. “When is credit optimal?” remains unanswered.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究提到最优信用或最优分配信用，但尚不清楚这在形式上是什么意思。“信用何时最优？”仍然没有答案。
- en: Combining benefits from different methods.
  id: totrans-532
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 综合不同方法的优势。
- en: Method conditioning on the future currently show superior results with respect
    to methods in other categories. These promising methods includes hindsight (Section [6.4](#S6.SS4
    "6.4 Conditioning in hindsight ‣ 6 Methods to assign credit in Deep RL ‣ A Survey
    of Temporal Credit Assignment in Deep Reinforcement Learning")), sequence modelling
    (Section [6.5](#S6.SS5 "6.5 Modelling transitions as sequences ‣ 6 Methods to
    assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")) and backward learning and planning methods (Section [6.6](#S6.SS6
    "6.6 Planning and learning backwards ‣ 6 Methods to assign credit in Deep RL ‣
    A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")). However,
    while hindsight methods are advancing fast, sequence modelling and backward planning
    methods are underinvestigated. We need a better understanding of the connection
    between these two worlds, which could potentially lead to even better ways of
    assigning credit. Could there be a connection between these methods? What are
    the effects of combining backward planning methods with more satisfactory measures
    of influence, for example, with CCA?
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，未来条件化的方法在与其他类别的方法相比时显示出更好的结果。这些有前景的方法包括回顾（第[6.4](#S6.SS4 "6.4 Conditioning
    in hindsight ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit
    Assignment in Deep Reinforcement Learning")节）、序列建模（第[6.5](#S6.SS5 "6.5 Modelling
    transitions as sequences ‣ 6 Methods to assign credit in Deep RL ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning")节）和向后学习与规划方法（第[6.6](#S6.SS6
    "6.6 Planning and learning backwards ‣ 6 Methods to assign credit in Deep RL ‣
    A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")节）。然而，尽管回顾方法进展迅速，但序列建模和向后规划方法的研究仍不够充分。我们需要更好地理解这两个领域之间的联系，这可能会导致更好的分配信用的方法。是否存在这些方法之间的联系？将向后规划方法与更令人满意的影响度量结合使用，例如与CCA结合，会有什么效果？
- en: Benchmarking.
  id: totrans-534
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准测试。
- en: The benchmarks currently used review a CA method (Chevalier-Boisvert et al.,,
    [2018](#bib.bib36); Bellemare et al.,, [2013](#bib.bib26); Samvelyan et al.,,
    [2021](#bib.bib151)) (see Section [7.2](#S7.SS2 "7.2 Tasks ‣ 7 Evaluating credit
    ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")) are
    often borrowed from control problems. This creates two problems. On one hand,
    these benchmarks cannot isolate the issues caused by each of the challenges reviewed
    in Section [5](#S5 "5 The challenges to assign credit in Deep RL ‣ A Survey of
    Temporal Credit Assignment in Deep Reinforcement Learning"). This makes it hard
    to understand which challenge of the CAP the method is improving, and it is not
    clear which methods to combine to achieve a unison advancement. On the other hand,
    to acquire knowledge (the CAP), the underlying set of associations between actions
    an outcome must be first discovered (see Section [5.4](#S5.SS4 "5.4 Relationship
    with the exploration problem ‣ 5 The challenges to assign credit in Deep RL ‣
    A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")). Because
    solving control requires to solve both the underlying CAP and exploration problem,
    it is not always clear if one or the other is responsible for an improvement.
    On a complementary note, CA methods are often evaluated in actor-critic settings
    (Harutyunyan et al.,, [2019](#bib.bib67); Mesnard et al.,, [2021](#bib.bib112)),
    which adds additional layers of complexity. This, together with other accessories
    unnecessary to validate a new algorithm, can obfuscate the contribution of the
    credit mechanism to the overall RL success. As a consequence, literature lacks
    a fair comparison among all the methods and it is not clear how all the methods
    in Section [6](#S6 "6 Methods to assign credit in Deep RL ‣ A Survey of Temporal
    Credit Assignment in Deep Reinforcement Learning") behave with respect to each
    other against the same set of benchmarks. Overall, the lack of a comprehensive
    understanding of the state of the art leads to a poor signal to direct future
    research. We call for new, community-driven single set of benchmarks that disentangle
    the CAP from the exploration problem and isolate the challenges described in Section [5](#S5
    "5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning"). How to disentangle the CAP and the exploration
    problem? How to isolate each challenge? Shall we evaluate in value-based settings
    and would the raking between the methods be consistent with an evaluation in actor-critic
    settings? These questions are still unanswered.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 目前使用的基准测试回顾了CA方法（Chevalier-Boisvert等，[2018](#bib.bib36)；Bellemare等，[2013](#bib.bib26)；Samvelyan等，[2021](#bib.bib151)）（见第[7.2节](#S7.SS2
    "7.2 Tasks ‣ 7 Evaluating credit ‣ A Survey of Temporal Credit Assignment in Deep
    Reinforcement Learning")），这些基准测试通常借用自控制问题。这带来了两个问题。一方面，这些基准测试无法孤立第[5节](#S5 "5
    The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")中回顾的每个挑战所导致的问题。这使得很难理解CA方法正在改善哪个挑战，也不清楚应该结合哪些方法来实现统一的进展。另一方面，为了获得知识（CAP），必须首先发现动作和结果之间的基本关联（见第[5.4节](#S5.SS4
    "5.4 Relationship with the exploration problem ‣ 5 The challenges to assign credit
    in Deep RL ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement Learning")）。由于解决控制问题需要同时解决基本的CAP和探索问题，因此不总是清楚哪个问题对改进负责。补充说明的是，CA方法通常在演员-评论家设置下进行评估（Harutyunyan等，[2019](#bib.bib67)；Mesnard等，[2021](#bib.bib112)），这增加了额外的复杂性。这与验证新算法不必要的其他附加因素一起，可能会混淆信用机制对整体RL成功的贡献。因此，文献中缺乏对所有方法的公平比较，也不清楚第[6节](#S6
    "6 Methods to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")中的所有方法在相同基准集下的表现如何。总体而言，对现有技术状态缺乏全面了解导致了对未来研究方向的信号不足。我们呼吁社区驱动的新基准集，以将CAP与探索问题区分开来，并孤立第[5节](#S5
    "5 The challenges to assign credit in Deep RL ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")中描述的挑战。如何区分CAP和探索问题？如何孤立每个挑战？我们是否应该在基于价值的设置中进行评估，这种评估方法与演员-评论家设置中的评估结果一致吗？这些问题仍然没有答案。
- en: Reproducibility.
  id: totrans-536
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 可重复性。
- en: Many works propose open source code, but experiments are often not reproducible,
    their code is hard to read, hard to run and hard to understand. Making code public
    is not enough and cannot be considered open-source if it is not easily usable.
    Other than public, open source code should be accessible, documented, easy to
    run, and accompanied by continuous support to questions and issues that may arise
    from its later usage. We need future research to acquire more rigour in the way
    to publish, present, and support the code that accompanies scientific publications.
    In particular, we need (i) a formalised, shared and broadly agreed standard that
    is not necessarily a new standard; (ii) for new studies to adhere to this standard,
    and (iii) for publishers to review accompanying code at least as thoroughly as
    when reviewing scientific manuscripts.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 许多工作提供开源代码，但实验通常无法重现，代码难以阅读、运行和理解。公开代码还不够，如果代码不易用，则不能被视为开源。除了公开，开源代码应可访问、有文档支持、易于运行，并且伴随对后续使用可能出现的问题和问题的持续支持。我们需要未来的研究在发布、展示和支持随科学出版物附带的代码方面更具严谨性。特别是，我们需要（i）一个正式化、共享且广泛认可的标准，这不一定是一个新的标准；（ii）新研究应遵守该标准；（iii）出版商在审查附带代码时应至少与审查科学手稿一样彻底。
- en: Monitoring advancements.
  id: totrans-538
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 监测进展。
- en: The community lacks a database containing comprehensive, curated results of
    each baseline. Currently, baselines are often re-run when a new method is proposed.
    This can potentially lead to comparisons that are unfair both because the baselines
    could be suboptimal (e.g., in the hyperparameters choice, training regime) and
    their reproduction could be not faithful (e.g., in translating the mathematics
    into code). When these conditions are not met, it is not clear whether a new method
    is advancing the field because it assigns credit better or because of misaligned
    baselines. We call for new, community-driven database holding the latest evaluations
    of each baseline. The evaluation should be driven by the authors and the authors
    be responsible for its results. When such a database will be available, new publications
    should run be tested against the same benchmarks but they not re-run previous
    baselines and rather refer to the curated results stored in the database.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 社区缺乏包含每个基线综合整理结果的数据库。目前，当提出新方法时，基线往往会被重新运行。这可能导致不公平的比较，因为基线可能存在次优情况（例如，超参数选择、训练模式）且其重现性可能不准确（例如，将数学翻译成代码）。当这些条件未满足时，不清楚新方法是否在推动领域进展，因为它是否更好地分配了信用，还是由于基线的不匹配。我们呼吁建立一个新的社区驱动数据库，保存每个基线的最新评估。评估应由作者主导，并且作者应对其结果负责。未来的出版物应在相同的基准下进行测试，但不应重新运行以前的基线，而应参考存储在数据库中的整理结果。
- en: Peer reviewing CA works.
  id: totrans-540
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 同行评审CA工作。
- en: As a consequence of the issues identified above, and because CA methods do not
    usually shine in classical benchmarks (Bellemare et al.,, [2013](#bib.bib26)),
    peer reviews often do not have the tools to capture the novelties of a method
    and its improvements. On one hand, we need a clear evaluation protocol, including
    a shared benchmark and leaderboard to facilitate peer reviews. On the other hand,
    peer reviews must steer away from using tools and metrics that would be used for
    control, and use those appropriate for the CAP instead.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述问题的影响，并且因为CA方法在经典基准测试中通常表现不佳（Bellemare et al., [2013](#bib.bib26)），同行评审往往没有工具来捕捉方法的创新和改进。一方面，我们需要一个明确的评估协议，包括共享的基准和排行榜，以便于同行评审。另一方面，同行评审必须避免使用控制所需的工具和指标，而应使用适用于CAP的工具和指标。
- en: Lack of priors and foundation models.
  id: totrans-542
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缺乏先验和基础模型。
- en: Most of the CA methods start to learn credit from scratch, without any prior
    knowledge but the one held by the initialisation pattern of its underlying network.
    This represents a main obstacle to making CA efficient because at each new learning
    phase even elementary associations must be learned from scratch. In contrasts,
    when facing a new tasks, humans often rely on their prior knowledge to determine
    the influence of an action. In the current state of the art, the use of priors
    to assign credit more efficiently is overlooked. Viceversa, the relevance of the
    CAP and the use of more advanced methods for CA (Mesnard et al.,, [2021](#bib.bib112),
    [2023](#bib.bib111); Edwards et al.,, [2018](#bib.bib40); van Hasselt et al.,,
    [2021](#bib.bib190)) is often underestimated for the development of foundation
    models in RL.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 CA 方法从头开始学习信用，没有任何先验知识，只有其底层网络的初始化模式所持有的知识。这是使 CA 高效的主要障碍，因为在每个新的学习阶段，甚至基本的关联也必须从头开始学习。相比之下，人类在面对新任务时，通常依赖其先前的知识来确定行动的影响。在当前的技术水平下，更高效地使用先验来分配信用被忽视了。相反，CAP
    的相关性和更先进的 CA 方法的使用（Mesnard 等，[2021](#bib.bib112)，[2023](#bib.bib111)；Edwards 等，[2018](#bib.bib40)；van
    Hasselt 等，[2021](#bib.bib190)）在 RL 基础模型的发展中往往被低估了。
- en: 8.3 Conclusions
  id: totrans-544
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 结论
- en: To conclude, in this survey, we have set out to formally settle the CAP in Deep
    RL. The resulting material does not aim to solve the CAP, but rather proposes
    a unifying framework that enables a fair comparison among the methods that assign
    credit and organises existing material to expedite the starting stages of new
    studies. Where the literature lacks answers, we identify the gaps and organise
    them in a list of challenges. We kindly encourage the research community to join
    in solving these challenges in a shared effort and we hope that the material collected
    in this manuscript can be a helpful resource to both inform future advancements
    in the field and inspire new applications in the real world.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在这项调查中，我们旨在正式解决深度 RL 中的 CAP。所得材料并不旨在解决 CAP，而是提出了一个统一的框架，使得在分配信用的方法之间能够公平比较，并整理现有材料以加速新研究的初期阶段。对于文献中缺乏的答案，我们识别了这些空白，并将其组织成挑战列表。我们诚恳地鼓励研究社区共同努力解决这些挑战，希望本手稿中收集的材料能为未来的领域进展提供有用资源，并激发现实世界中的新应用。
- en: Acknowledgments
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢
- en: 'E.P. was supported by the Engineering and Physical Sciences Research Council
    (EPSRC) [grant number: EP/R513143/1]. The authors thank Sephora Madjiheurem for
    the insightful discussions in the early stages of the manuscript.'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: E.P. 得到了工程与物理科学研究委员会（EPSRC）[资助编号：EP/R513143/1]的支持。作者感谢 Sephora Madjiheurem 在手稿早期阶段的深刻讨论。
- en: References
  id: totrans-548
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abel et al., (2021) Abel, D., Dabney, W., Harutyunyan, A., Ho, M. K., Littman,
    M., Precup, D., and Singh, S. (2021). On the expressivity of markov reward. In
    Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors, Advances
    in Neural Information Processing Systems.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abel 等，（2021）Abel, D., Dabney, W., Harutyunyan, A., Ho, M. K., Littman, M.,
    Precup, D., and Singh, S. (2021). Markov 奖励的表现力。见 Beygelzimer, A., Dauphin, Y.,
    Liang, P., and Vaughan, J. W. 编辑的《神经信息处理系统进展》。
- en: Agarwal et al., (2021) Agarwal, R., Schwarzer, M., Castro, P. S., Courville,
    A. C., and Bellemare, M. (2021). Deep reinforcement learning at the edge of the
    statistical precipice. Advances in neural information processing systems, 34:29304–29320.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等，（2021）Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C.,
    and Bellemare, M. (2021). 统计悬崖边缘的深度强化学习。《神经信息处理系统进展》，34:29304–29320。
- en: 'Al-Emran, (2015) Al-Emran, M. (2015). Hierarchical reinforcement learning:
    a survey. International journal of computing and digital systems, 4(02).'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Emran，（2015）Al-Emran, M. (2015). 层次强化学习：综述。《国际计算与数字系统期刊》，4(02)。
- en: Amin et al., (2021) Amin, S., Gomrokchi, M., Satija, H., van Hoof, H., and Precup,
    D. (2021). A survey of exploration methods in reinforcement learning. arXiv preprint
    arXiv:2109.00157.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amin 等，（2021）Amin, S., Gomrokchi, M., Satija, H., van Hoof, H., and Precup,
    D. (2021). 强化学习中的探索方法综述。arXiv 预印本 arXiv:2109.00157。
- en: Andrychowicz et al., (2017) Andrychowicz, M., Wolski, F., Ray, A., Schneider,
    J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O., and Zaremba,
    W. (2017). Hindsight experience replay. In Advances in neural information processing
    systems, volume 30.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrychowicz 等，（2017）Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,
    R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O., and Zaremba, W. (2017).
    事后经验重放。在《神经信息处理系统进展》，第 30 卷。
- en: Anthony et al., (2020) Anthony, T., Eccles, T., Tacchetti, A., Kramár, J., Gemp,
    I., Hudson, T., Porcel, N., Lanctot, M., Pérolat, J., Everett, R., et al. (2020).
    Learning to play no-press diplomacy with best response policy iteration. Advances
    in Neural Information Processing Systems, 33:17987–18003.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthony 等人 (2020) Anthony, T., Eccles, T., Tacchetti, A., Kramár, J., Gemp,
    I., Hudson, T., Porcel, N., Lanctot, M., Pérolat, J., Everett, R., 等 (2020). 学习通过最佳响应策略迭代玩无压外交。在《神经信息处理系统进展》中，第
    33 卷：17987–18003。
- en: 'Arjona-Medina et al., (2019) Arjona-Medina, J. A., Gillhofer, M., Widrich,
    M., Unterthiner, T., Brandstetter, J., and Hochreiter, S. (2019). Rudder: Return
    decomposition for delayed rewards. Advances in Neural Information Processing Systems,
    32.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjona-Medina 等人 (2019) Arjona-Medina, J. A., Gillhofer, M., Widrich, M., Unterthiner,
    T., Brandstetter, J., 和 Hochreiter, S. (2019). Rudder：延迟奖励的回报分解。在《神经信息处理系统进展》中，第
    32 卷。
- en: 'Arulkumaran et al., (2022) Arulkumaran, K., Ashley, D. R., Schmidhuber, J.,
    and Srivastava, R. K. (2022). All you need is supervised learning: From imitation
    learning to meta-rl with upside down rl. arXiv preprint arXiv:2202.11960.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arulkumaran 等人 (2022) Arulkumaran, K., Ashley, D. R., Schmidhuber, J., 和 Srivastava,
    R. K. (2022). 你所需的只是监督学习：从模仿学习到上下颠倒的元强化学习。arXiv 预印本 arXiv:2202.11960。
- en: 'Arulkumaran et al., (2017) Arulkumaran, K., Deisenroth, M. P., Brundage, M.,
    and Bharath, A. A. (2017). Deep reinforcement learning: A brief survey. IEEE Signal
    Processing Magazine, 34(6):26–38.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arulkumaran 等人 (2017) Arulkumaran, K., Deisenroth, M. P., Brundage, M., 和 Bharath,
    A. A. (2017). 深度强化学习：简要调查。IEEE 信号处理杂志，34(6)：26–38。
- en: Arumugam et al., (2021) Arumugam, D., Henderson, P., and Bacon, P.-L. (2021).
    An information-theoretic perspective on credit assignment in reinforcement learning.
    CoRR, abs/2103.06224.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arumugam 等人 (2021) Arumugam, D., Henderson, P., 和 Bacon, P.-L. (2021). 从信息理论的角度看强化学习中的信用分配。CoRR,
    abs/2103.06224。
- en: Ashley et al., (2022) Ashley, D. R., Arulkumaran, K., Schmidhuber, J., and Srivastava,
    R. K. (2022). Learning relative return policies with upside-down reinforcement
    learning. arXiv preprint arXiv:2202.12742.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashley 等人 (2022) Ashley, D. R., Arulkumaran, K., Schmidhuber, J., 和 Srivastava,
    R. K. (2022). 使用上下颠倒的强化学习学习相对回报策略。arXiv 预印本 arXiv:2202.12742。
- en: Bacon et al., (2017) Bacon, P.-L., Harb, J., and Precup, D. (2017). The option-critic
    architecture. In Proceedings of the AAAI conference on artificial intelligence,
    volume 31.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bacon 等人 (2017) Bacon, P.-L., Harb, J., 和 Precup, D. (2017). 选项-评论架构。在 AAAI
    人工智能会议论文集中，第 31 卷。
- en: 'Badia et al., (2020) Badia, A. P., Piot, B., Kapturowski, S., Sprechmann, P.,
    Vitvitskyi, A., Guo, Z. D., and Blundell, C. (2020). Agent57: Outperforming the
    atari human benchmark. In International Conference on Machine Learning, pages
    507–517\. PMLR.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Badia 等人 (2020) Badia, A. P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi,
    A., Guo, Z. D., 和 Blundell, C. (2020). Agent57：超越 Atari 人类基准。在国际机器学习会议上，第 507–517
    页。PMLR。
- en: Bagaria and Konidaris, (2019) Bagaria, A. and Konidaris, G. (2019). Option discovery
    using deep skill chaining. In International Conference on Learning Representations.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagaria 和 Konidaris (2019) Bagaria, A. 和 Konidaris, G. (2019). 使用深度技能链发现选项。在国际学习表征会议上。
- en: Baird, (1999) Baird, L. C. I. (1999). Reinforcement Learning Through Gradient
    Descent. PhD thesis, US Air Force Academy.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baird (1999) Baird, L. C. I. (1999). 通过梯度下降进行强化学习。博士论文，美国空军学院。
- en: 'Balduzzi et al., (2015) Balduzzi, D., Vanchinathan, H., and Buhmann, J. (2015).
    Kickback cuts backprop’s red-tape: Biologically plausible credit assignment in
    neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
    volume 29.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balduzzi 等人 (2015) Balduzzi, D., Vanchinathan, H., 和 Buhmann, J. (2015). Kickback
    减少了反向传播的繁文缛节：神经网络中的生物学上合理的信用分配。在 AAAI 人工智能会议论文集中，第 29 卷。
- en: Bareinboim et al., (2022) Bareinboim, E., Correa, J. D., Ibeling, D., and Icard,
    T. (2022). On Pearl’s Hierarchy and the Foundations of Causal Inference, page
    507–556. Association for Computing Machinery, New York, NY, USA, 1 edition.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bareinboim 等人 (2022) Bareinboim, E., Correa, J. D., Ibeling, D., 和 Icard, T.
    (2022). 论 Pearl 的层次结构和因果推断的基础，第 507–556 页。计算机协会，纽约，美国，第 1 版。
- en: Barto, (1997) Barto, A. G. (1997). Reinforcement learning. In Neural systems
    for control, pages 7–30\. Elsevier.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barto (1997) Barto, A. G. (1997). 强化学习。在《神经系统控制》中，第 7–30 页。Elsevier。
- en: Barto, (2013) Barto, A. G. (2013). Intrinsic motivation and reinforcement learning.
    Intrinsically motivated learning in natural and artificial systems, pages 17–47.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barto (2013) Barto, A. G. (2013). 内在动机和强化学习。在《自然和人工系统中的内在动机学习》中，第 17–47 页。
- en: Barto and Mahadevan, (2003) Barto, A. G. and Mahadevan, S. (2003). Recent advances
    in hierarchical reinforcement learning. Discrete event dynamic systems, 13(1):41–77.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barto 和 Mahadevan，(2003) Barto, A. G. 和 Mahadevan, S. (2003). 分层强化学习的最新进展。离散事件动态系统，13(1):41–77。
- en: Barto et al., (2004) Barto, A. G., Singh, S., Chentanez, N., et al. (2004).
    Intrinsically motivated learning of hierarchical collections of skills. In Proceedings
    of the 3rd International Conference on Development and Learning, volume 112, page 19\.
    Citeseer.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barto 等，(2004) Barto, A. G., Singh, S., Chentanez, N., 等. (2004). 内在动机驱动的分层技能集合学习。在第三届国际发展与学习大会论文集，第112卷，第19页。Citeseer。
- en: Beattie et al., (2016) Beattie, C., Leibo, J. Z., Teplyashin, D., Ward, T.,
    Wainwright, M., Küttler, H., Lefrancq, A., Green, S., Valdés, V., Sadik, A., et al.
    (2016). Deepmind lab. arXiv preprint arXiv:1612.03801.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beattie 等，(2016) Beattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright,
    M., Küttler, H., Lefrancq, A., Green, S., Valdés, V., Sadik, A., 等. (2016). Deepmind
    实验室。arXiv 预印本 arXiv:1612.03801。
- en: Behzadan and Hsu, (2019) Behzadan, V. and Hsu, W. (2019). Adversarial exploitation
    of policy imitation. arXiv preprint arXiv:1906.01121.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Behzadan 和 Hsu，(2019) Behzadan, V. 和 Hsu, W. (2019). 政策模仿的对抗性利用。arXiv 预印本 arXiv:1906.01121。
- en: Bellemare et al., (2020) Bellemare, M. G., Candido, S., Castro, P. S., Gong,
    J., Machado, M. C., Moitra, S., Ponda, S. S., and Wang, Z. (2020). Autonomous
    navigation of stratospheric balloons using reinforcement learning. Nature, 588(7836):77–82.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等，(2020) Bellemare, M. G., Candido, S., Castro, P. S., Gong, J., Machado,
    M. C., Moitra, S., Ponda, S. S., 和 Wang, Z. (2020). 使用强化学习对平流层气球进行自主导航。自然，588(7836):77–82。
- en: Bellemare et al., (2017) Bellemare, M. G., Dabney, W., and Munos, R. (2017).
    A distributional perspective on reinforcement learning. In International Conference
    on Machine Learning, pages 449–458\. Proceedings of Machine Learning Research.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等，(2017) Bellemare, M. G., Dabney, W., 和 Munos, R. (2017). 强化学习的分布视角。在国际机器学习大会，页449–458。机器学习研究论文集。
- en: 'Bellemare et al., (2013) Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling,
    M. (2013). The arcade learning environment: An evaluation platform for general
    agents. Journal of Artificial Intelligence Research, 47:253–279.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等，(2013) Bellemare, M. G., Naddaf, Y., Veness, J., 和 Bowling, M. (2013).
    游戏机学习环境：通用代理的评估平台。人工智能研究杂志，47:253–279。
- en: 'Bellemare et al., (2016) Bellemare, M. G., Ostrovski, G., Guez, A., Thomas,
    P., and Munos, R. (2016). Increasing the action gap: New operators for reinforcement
    learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等，(2016) Bellemare, M. G., Ostrovski, G., Guez, A., Thomas, P., 和
    Munos, R. (2016). 增加行动差距：强化学习的新操作符。在 AAAI 人工智能大会论文集，第30卷。
- en: Bowling et al., (2023) Bowling, M., Martin, J. D., Abel, D., and Dabney, W.
    (2023). Settling the reward hypothesis. In Krause, A., Brunskill, E., Cho, K.,
    Engelhardt, B., Sabato, S., and Scarlett, J., editors, Proceedings of the 40th
    International Conference on Machine Learning, volume 202 of Proceedings of Machine
    Learning Research, pages 3003–3020\. PMLR.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bowling 等，(2023) Bowling, M., Martin, J. D., Abel, D., 和 Dabney, W. (2023).
    确定奖励假设。在 Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., 和 Scarlett,
    J. 编者，《第40届国际机器学习大会论文集》，第202卷，机器学习研究论文集，页3003–3020。PMLR。
- en: 'Buesing et al., (2019) Buesing, L., Weber, T., Zwols, Y., Heess, N., Racaniere,
    S., Guez, A., and Lespiau, J.-B. (2019). Woulda, coulda, shoulda: Counterfactually-guided
    policy search. In International Conference on Learning Representations.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buesing 等，(2019) Buesing, L., Weber, T., Zwols, Y., Heess, N., Racaniere, S.,
    Guez, A., 和 Lespiau, J.-B. (2019). 应该、可能、会：基于反事实指导的政策搜索。在国际学习表示大会。
- en: 'Chang et al., (2003) Chang, Y.-H., Ho, T., and Kaelbling, L. (2003). All learning
    is local: Multi-agent learning in global reward games. Advances in neural information
    processing systems, 16.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等，(2003) Chang, Y.-H., Ho, T., 和 Kaelbling, L. (2003). 所有学习都是局部的：全球奖励游戏中的多智能体学习。神经信息处理系统进展，第16卷。
- en: Chelu et al., (2022) Chelu, V., Borsa, D., Precup, D., and van Hasselt, H. (2022).
    Selective credit assignment. arXiv preprint arXiv:2202.09699.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chelu 等，(2022) Chelu, V., Borsa, D., Precup, D., 和 van Hasselt, H. (2022). 选择性信用分配。arXiv
    预印本 arXiv:2202.09699。
- en: Chelu et al., (2020) Chelu, V., Precup, D., and van Hasselt, H. P. (2020). Forethought
    and hindsight in credit assignment. In Larochelle, H., Ranzato, M., Hadsell, R.,
    Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems,
    volume 33, pages 2270–2281\. Curran Associates, Inc.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chelu 等，(2020) Chelu, V., Precup, D., 和 van Hasselt, H. P. (2020). 信用分配中的前瞻与回顾。在
    Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., 和 Lin, H. 编者，《神经信息处理系统进展》，第33卷，页2270–2281。Curran
    Associates, Inc.
- en: 'Chen et al., (2021) Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A.,
    Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. (2021). Decision transformer:
    Reinforcement learning via sequence modeling. In Advances in Neural Information
    Processing Systems, volume 34, pages 15084–15097.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2021年）Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin,
    M., Abbeel, P., Srinivas, A., 和 Mordatch, I.（2021年）。决策变换器：通过序列建模的强化学习。发表于神经信息处理系统进展，第34卷，第15084–15097页。
- en: Chen and Lin, (2020) Chen, Z. and Lin, M. (2020). Self-imitation learning in
    sparse reward settings. arXiv preprint arXiv:2010.06962.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen和Lin（2020年）Chen, Z. 和 Lin, M.（2020年）。稀疏奖励设置中的自我模仿学习。arXiv预印本 arXiv:2010.06962。
- en: Chentanez et al., (2004) Chentanez, N., Barto, A., and Singh, S. (2004). Intrinsically
    motivated reinforcement learning. Advances in neural information processing systems,
    17.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chentanez等人（2004年）Chentanez, N., Barto, A., 和 Singh, S.（2004年）。内在动机强化学习。神经信息处理系统进展，第17卷。
- en: Chevalier-Boisvert et al., (2018) Chevalier-Boisvert, M., Willems, L., and Pal,
    S. (2018). Minimalistic gridworld environment for openai gym. [https://github.com/maximecb/gym-minigrid](https://github.com/maximecb/gym-minigrid).
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chevalier-Boisvert等人（2018年）Chevalier-Boisvert, M., Willems, L., 和 Pal, S.（2018年）。开源AI
    Gym的极简网格世界环境。[https://github.com/maximecb/gym-minigrid](https://github.com/maximecb/gym-minigrid)。
- en: 'Colas et al., (2022) Colas, C., Karch, T., Sigaud, O., and Oudeyer, P.-Y. (2022).
    Autotelic agents with intrinsically motivated goal-conditioned reinforcement learning:
    a short survey. Journal of Artificial Intelligence Research, 74:1159–1199.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colas等人（2022年）Colas, C., Karch, T., Sigaud, O., 和 Oudeyer, P.-Y.（2022年）。具有内在动机的目标条件强化学习的自我目的性代理：简要调查。人工智能研究杂志，74:1159–1199。
- en: Degrave et al., (2022) Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey,
    B., Carpanese, F., Ewalds, T., Hafner, R., Abdolmaleki, A., de Las Casas, D.,
    et al. (2022). Magnetic control of tokamak plasmas through deep reinforcement
    learning. Nature, 602(7897):414–419.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Degrave等人（2022年）Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B.,
    Carpanese, F., Ewalds, T., Hafner, R., Abdolmaleki, A., de Las Casas, D., 等（2022年）。通过深度强化学习对托卡马克等离子体的磁控。自然，602(7897):414–419。
- en: Dulac-Arnold et al., (2015) Dulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag,
    P., Lillicrap, T., Hunt, J., Mann, T., Weber, T., Degris, T., and Coppin, B. (2015).
    Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dulac-Arnold等人（2015年）Dulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag,
    P., Lillicrap, T., Hunt, J., Mann, T., Weber, T., Degris, T., 和 Coppin, B.（2015年）。大离散动作空间中的深度强化学习。arXiv预印本
    arXiv:1512.07679。
- en: Edwards et al., (2018) Edwards, A. D., Downs, L., and Davidson, J. C. (2018).
    Forward-backward reinforcement learning. arXiv preprint arXiv:1803.10227.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Edwards等人（2018年）Edwards, A. D., Downs, L., 和 Davidson, J. C.（2018年）。前向-后向强化学习。arXiv预印本
    arXiv:1803.10227。
- en: Elliot and Fryer, (2008) Elliot, A. J. and Fryer, J. W. (2008). The goal construct
    in psychology. In Handbook of motivation science, volume 18, pages 235–250.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elliot和Fryer（2008年）Elliot, A. J. 和 Fryer, J. W.（2008年）。心理学中的目标构建。发表于《动机科学手册》，第18卷，第235–250页。
- en: 'Eysenbach et al., (2018) Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S.
    (2018). Diversity is all you need: Learning skills without a reward function.
    arXiv preprint arXiv:1802.06070.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eysenbach等人（2018年）Eysenbach, B., Gupta, A., Ibarz, J., 和 Levine, S.（2018年）。多样性就是你所需要的一切：无需奖励函数的技能学习。arXiv预印本
    arXiv:1802.06070。
- en: Faccio et al., (2021) Faccio, F., Kirsch, L., and Schmidhuber, J. (2021). Parameter-based
    value functions. In International Conference on Learning Representations.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faccio等人（2021年）Faccio, F., Kirsch, L., 和 Schmidhuber, J.（2021年）。基于参数的价值函数。发表于国际学习表征会议。
- en: Farahmand, (2011) Farahmand, A.-m. (2011). Action-gap phenomenon in reinforcement
    learning. Advances in Neural Information Processing Systems, 24.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Farahmand（2011年）Farahmand, A.-m.（2011年）。强化学习中的行动差距现象。神经信息处理系统进展，第24卷。
- en: 'Ferret, (2022) Ferret, J. (2022). On Actions that Matter: Credit Assignment
    and Interpretability in Reinforcement Learning. PhD thesis, Université de Lille.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferret（2022年）Ferret, J.（2022年）。重要的行动：强化学习中的信用分配和可解释性。博士论文，里尔大学。
- en: (46) Ferret, J., Marinier, R., Geist, M., and Pietquin, O. (2021a). Self-attentional
    credit assignment for transfer in reinforcement learning. In Proceedings of the
    Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI’20.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (46) Ferret, J., Marinier, R., Geist, M., 和 Pietquin, O.（2021a年）。自注意力的信用分配在强化学习中的转移。发表于第二十九届国际人工智能联合会议，IJCAI’20。
- en: (47) Ferret, J., Pietquin, O., and Geist, M. (2021b). Self-imitation advantage
    learning. In AAMAS 2021-20th International Conference on Autonomous Agents and
    Multiagent Systems.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (47) Ferret, J., Pietquin, O., 和 Geist, M.（2021b年）。自我模仿优势学习。发表于AAMAS 2021-第20届国际自主代理与多智能体系统会议。
- en: Filos et al., (2020) Filos, A., Tigkas, P., McAllister, R., Rhinehart, N., Levine,
    S., and Gal, Y. (2020). Can autonomous vehicles identify, recover from, and adapt
    to distribution shifts? In International Conference on Machine Learning, pages
    3145–3153\. PMLR.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Filos等人（2020）Filos, A., Tigkas, P., McAllister, R., Rhinehart, N., Levine, S.,
    和Gal, Y.（2020）。自主车辆能否识别、恢复并适应分布变化？在国际机器学习会议，页3145–3153。PMLR。
- en: Flennerhag et al., (2021) Flennerhag, S., Schroecker, Y., Zahavy, T., van Hasselt,
    H., Silver, D., and Singh, S. (2021). Bootstrapped meta-learning. arXiv preprint
    arXiv:2109.04504.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flennerhag等人（2021）Flennerhag, S., Schroecker, Y., Zahavy, T., van Hasselt, H.,
    Silver, D., 和Singh, S.（2021）。自举元学习。arXiv预印本 arXiv:2109.04504。
- en: Flet-Berliac, (2019) Flet-Berliac, Y. (2019). The promise of hierarchical reinforcement
    learning. The Gradient, 9.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flet-Berliac（2019）Flet-Berliac, Y.（2019）。层次化强化学习的承诺。《Gradient》，9。
- en: Foerster et al., (2018) Foerster, J., Farquhar, G., Afouras, T., Nardelli, N.,
    and Whiteson, S. (2018). Counterfactual multi-agent policy gradients. In Proceedings
    of the AAAI conference on artificial intelligence, volume 32.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foerster等人（2018）Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., 和Whiteson,
    S.（2018）。反事实多智能体策略梯度。在AAAI人工智能会议论文集，卷32。
- en: Furuta et al., (2022) Furuta, H., Matsuo, Y., and Gu, S. S. (2022). Generalized
    decision transformer for offline hindsight information matching. In International
    Conference on Learning Representations.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Furuta等人（2022）Furuta, H., Matsuo, Y., 和Gu, S. S.（2022）。用于离线回顾信息匹配的广义决策变换器。在国际学习表示会议。
- en: Gao, (2014) Gao, J. (2014). Machine learning applications for data center optimization.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao（2014）Gao, J.（2014）。数据中心优化的机器学习应用。
- en: García et al., (2015) García, J., Fern, and o Fernández (2015). A comprehensive
    survey on safe reinforcement learning. Journal of Machine Learning Research, 16(42):1437–1480.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: García等人（2015）García, J., Fern, 和Fernández（2015）。关于安全强化学习的综合调查。《机器学习研究杂志》，16(42):1437–1480。
- en: 'Geist et al., (2014) Geist, M., Scherrer, B., et al. (2014). Off-policy learning
    with eligibility traces: a survey. J. Mach. Learn. Res., 15(1):289–333.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geist等人（2014）Geist, M., Scherrer, B., 等（2014）。带有资格迹的离策略学习：综述。《机器学习研究杂志》，15(1):289–333。
- en: 'Goyal et al., (2019) Goyal, A., Brakel, P., Fedus, W., Singhal, S., Lillicrap,
    T., Levine, S., Larochelle, H., and Bengio, Y. (2019). Recall traces: Backtracking
    models for efficient reinforcement learning. In International Conference on Learning
    Representations.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal等人（2019）Goyal, A., Brakel, P., Fedus, W., Singhal, S., Lillicrap, T., Levine,
    S., Larochelle, H., 和Bengio, Y.（2019）。回忆痕迹：高效强化学习的回溯模型。在国际学习表示会议。
- en: 'Goyal et al., (2017) Goyal, A., Sordoni, A., Côté, M.-A., Ke, N. R., and Bengio,
    Y. (2017). Z-forcing: Training stochastic recurrent networks. Advances in neural
    information processing systems, 30.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goyal等人（2017）Goyal, A., Sordoni, A., Côté, M.-A., Ke, N. R., 和Bengio, Y.（2017）。Z-forcing:
    训练随机递归网络。神经信息处理系统进展，30。'
- en: Graves et al., (2016) Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka,
    I., Grabska-Barwińska, A., Colmenarejo, S. G., Grefenstette, E., Ramalho, T.,
    Agapiou, J., et al. (2016). Hybrid computing using a neural network with dynamic
    external memory. Nature, 538(7626):471–476.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves等人（2016）Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I.,
    Grabska-Barwińska, A., Colmenarejo, S. G., Grefenstette, E., Ramalho, T., Agapiou,
    J., 等（2016）。使用动态外部记忆的神经网络混合计算。《自然》，538(7626):471–476。
- en: Grefenstette et al., (2015) Grefenstette, E., Hermann, K. M., Suleyman, M.,
    and Blunsom, P. (2015). Learning to transduce with unbounded memory. Advances
    in neural information processing systems, 28.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grefenstette等人（2015）Grefenstette, E., Hermann, K. M., Suleyman, M., 和Blunsom,
    P.（2015）。学习使用无界记忆进行转导。神经信息处理系统进展，28。
- en: 'Grinsztajn et al., (2021) Grinsztajn, N., Ferret, J., Pietquin, O., Geist,
    M., et al. (2021). There is no turning back: A self-supervised approach for reversibility-aware
    reinforcement learning. Advances in Neural Information Processing Systems, 34:1898–1911.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grinsztajn等人（2021）Grinsztajn, N., Ferret, J., Pietquin, O., Geist, M., 等（2021）。没有回头路：一种自监督方法用于可逆性强化学习。神经信息处理系统进展，34:1898–1911。
- en: Guez et al., (2020) Guez, A., Viola, F., Weber, T., Buesing, L., Kapturowski,
    S., Precup, D., Silver, D., and Heess, N. (2020). Value-driven hindsight modelling.
    Advances in Neural Information Processing Systems, 33:12499–12509.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guez等人（2020）Guez, A., Viola, F., Weber, T., Buesing, L., Kapturowski, S., Precup,
    D., Silver, D., 和Heess, N.（2020）。价值驱动的回顾建模。神经信息处理系统进展，33:12499–12509。
- en: 'Gupta et al., (2019) Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman,
    K. (2019). Relay policy learning: Solving long-horizon tasks via imitation and
    reinforcement learning. arXiv preprint arXiv:1910.11956.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等（2019）Gupta, A., Kumar, V., Lynch, C., Levine, S., 和 Hausman, K.（2019）。《继电策略学习：通过模仿和强化学习解决长期任务》。arXiv
    预印本 arXiv:1910.11956。
- en: Haarnoja et al., (2017) Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017).
    Reinforcement learning with deep energy-based policies. In International conference
    on machine learning, pages 1352–1361\. PMLR.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haarnoja 等（2017）Haarnoja, T., Tang, H., Abbeel, P., 和 Levine, S.（2017）。《具有深度能量基政策的强化学习》。在国际机器学习会议，页码
    1352–1361。PMLR。
- en: 'Haarnoja et al., (2018) Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
    (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning
    with a stochastic actor. In International Conference on Machine Learning, pages
    1861–1870\. Proceedings of Machine Learning Research.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haarnoja 等（2018）Haarnoja, T., Zhou, A., Abbeel, P., 和 Levine, S.（2018）。《软演员-评论家：具有随机演员的离线最大熵深度强化学习》。国际机器学习大会，页码
    1861–1870。机器学习研究论文集。
- en: Hafner et al., (2022) Hafner, D., Lee, K.-H., Fischer, I., and Abbeel, P. (2022).
    Deep hierarchical planning from pixels. Advances in Neural Information Processing
    Systems, 35:26091–26104.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner 等（2022）Hafner, D., Lee, K.-H., Fischer, I., 和 Abbeel, P.（2022）。《从像素中进行深度层次规划》。神经信息处理系统进展，35:26091–26104。
- en: Harb et al., (2020) Harb, J., Schaul, T., Precup, D., and Bacon, P.-L. (2020).
    Policy evaluation networks. arXiv preprint arXiv:2002.11833.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harb 等（2020）Harb, J., Schaul, T., Precup, D., 和 Bacon, P.-L.（2020）。《策略评估网络》。arXiv
    预印本 arXiv:2002.11833。
- en: Harutyunyan et al., (2019) Harutyunyan, A., Dabney, W., Mesnard, T., Gheshlaghi Azar,
    M., Piot, B., Heess, N., van Hasselt, H. P., Wayne, G., Singh, S., Precup, D.,
    et al. (2019). Hindsight credit assignment. Advances in neural information processing
    systems, 32.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harutyunyan 等（2019）Harutyunyan, A., Dabney, W., Mesnard, T., Gheshlaghi Azar,
    M., Piot, B., Heess, N., van Hasselt, H. P., Wayne, G., Singh, S., Precup, D.,
    等（2019）。《事后信用分配》。神经信息处理系统进展，32。
- en: Harutyunyan et al., (2018) Harutyunyan, A., Vrancx, P., Bacon, P.-L., Precup,
    D., and Nowe, A. (2018). Learning with options that terminate off-policy. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 32.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harutyunyan 等（2018）Harutyunyan, A., Vrancx, P., Bacon, P.-L., Precup, D., 和
    Nowe, A.（2018）。《使用终止的选项进行学习的离策略》。在 AAAI 人工智能会议论文集中，第 32 卷。
- en: Henderson et al., (2018) Henderson, P., Islam, R., Bachman, P., Pineau, J.,
    Precup, D., and Meger, D. (2018). Deep reinforcement learning that matters. In
    Proceedings of the AAAI conference on artificial intelligence, volume 32.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson 等（2018）Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup,
    D., 和 Meger, D.（2018）。《有意义的深度强化学习》。在 AAAI 人工智能会议论文集中，第 32 卷。
- en: Hesterberg, (1995) Hesterberg, T. (1995). Weighted average importance sampling
    and defensive mixture distributions. Technometrics, 37(2):185–194.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hesterberg（1995）Hesterberg, T.（1995）。《加权平均重要性采样与防御性混合分布》。Technometrics, 37(2):185–194。
- en: Hoffman, (2016) Hoffman, D. D. (2016). The interface theory of perception. Current
    Directions in Psychological Science, 25(3):157–161.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffman（2016）Hoffman, D. D.（2016）。《感知的界面理论》。心理科学当前方向，25(3):157–161。
- en: Houthooft et al., (2018) Houthooft, R., Chen, Y., Isola, P., Stadie, B., Wolski,
    F., Jonathan Ho, O., and Abbeel, P. (2018). Evolved policy gradients. Advances
    in Neural Information Processing Systems, 31.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houthooft 等（2018）Houthooft, R., Chen, Y., Isola, P., Stadie, B., Wolski, F.,
    Jonathan Ho, O., 和 Abbeel, P.（2018）。《进化策略梯度》。神经信息处理系统进展，31。
- en: Howard, (1960) Howard, R. A. (1960). Dynamic programming and Markov processes.
    John Wiley.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard（1960）Howard, R. A.（1960）。《动态规划与马尔可夫过程》。John Wiley。
- en: 'Hu et al., (2020) Hu, Y., Wang, W., Jia, H., Wang, Y., Chen, Y., Hao, J., Wu,
    F., and Fan, C. (2020). Learning to utilize shaping rewards: A new approach of
    reward shaping. Advances in Neural Information Processing Systems, 33:15931–15941.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2020）Hu, Y., Wang, W., Jia, H., Wang, Y., Chen, Y., Hao, J., Wu, F., 和
    Fan, C.（2020）。《学习利用塑造奖励：一种新的奖励塑造方法》。神经信息处理系统进展，33:15931–15941。
- en: Hung et al., (2019) Hung, C.-C., Lillicrap, T., Abramson, J., Wu, Y., Mirza,
    M., Carnevale, F., Ahuja, A., and Wayne, G. (2019). Optimizing agent behavior
    over long time scales by transporting value. Nature Communications, 10(1):5223.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hung 等（2019）Hung, C.-C., Lillicrap, T., Abramson, J., Wu, Y., Mirza, M., Carnevale,
    F., Ahuja, A., 和 Wayne, G.（2019）。《通过价值传输优化智能体行为的长期时间尺度》。Nature Communications,
    10(1):5223。
- en: Jaakkola et al., (1993) Jaakkola, T., Jordan, M., and Singh, S. (1993). Convergence
    of stochastic iterative dynamic programming algorithms. Advances in neural information
    processing systems, 6.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaakkola 等 (1993) Jaakkola, T., Jordan, M., 和 Singh, S. (1993). 随机迭代动态规划算法的收敛性。神经信息处理系统进展，第6卷。
- en: Jaderberg et al., (2017) Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul,
    T., Leibo, J. Z., Silver, D., and Kavukcuoglu, K. (2017). Reinforcement learning
    with unsupervised auxiliary tasks. In International Conference on Learning Representations.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg 等 (2017) Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo,
    J. Z., Silver, D., 和 Kavukcuoglu, K. (2017). 带有无监督辅助任务的强化学习。见国际学习表示会议。
- en: Janner et al., (2021) Janner, M., Li, Q., and Levine, S. (2021). Offline reinforcement
    learning as one big sequence modeling problem. In Advances in Neural Information
    Processing Systems.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janner 等 (2021) Janner, M., Li, Q., 和 Levine, S. (2021). 离线强化学习作为一个大规模序列建模问题。见神经信息处理系统进展。
- en: Janzing et al., (2013) Janzing, D., Balduzzi, D., Grosse-Wentrup, M., and Schölkopf,
    B. (2013). Quantifying causal influences. The Annals Of Statistics, pages 2324–2358.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janzing 等 (2013) Janzing, D., Balduzzi, D., Grosse-Wentrup, M., 和 Schölkopf,
    B. (2013). 量化因果影响。《统计年鉴》，第2324–2358页。
- en: 'Jaquette, (1973) Jaquette, S. C. (1973). Markov decision processes with a new
    optimality criterion: Discrete time. The Annals of Statistics, 1(3):496–505.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaquette (1973) Jaquette, S. C. (1973). 具有新最优标准的马尔可夫决策过程：离散时间。《统计年鉴》，1(3):496–505。
- en: (81) Jiang, M., Grefenstette, E., and Rocktäschel, T. (2021a). Prioritized level
    replay. In International Conference on Machine Learning, pages 4940–4950\. Proceedings
    of Machine Learning Research.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (81) Jiang, M., Grefenstette, E., 和 Rocktäschel, T. (2021a). 优先级层级回放。见国际机器学习会议，第4940–4950页。机器学习研究论文集。
- en: Jiang et al., (2023) Jiang, M., Rocktäschel, T., and Grefenstette, E. (2023).
    General intelligence requires rethinking exploration. Royal Society Open Science,
    10(6):230539.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2023) Jiang, M., Rocktäschel, T., 和 Grefenstette, E. (2023). 通用智能需要重新思考探索。皇家学会开放科学，10(6):230539。
- en: (83) Jiang, R., Zahavy, T., Xu, Z., White, A., Hessel, M., Blundell, C., and
    van Hasselt, H. (2021b). Emphatic algorithms for deep reinforcement learning.
    In Meila, M. and Zhang, T., editors, Proceedings of the 38th International Conference
    on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages
    5023–5033\. PMLR.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (83) Jiang, R., Zahavy, T., Xu, Z., White, A., Hessel, M., Blundell, C., 和 van
    Hasselt, H. (2021b). 深度强化学习的情感算法。见 Meila, M. 和 Zhang, T. 编辑, 《第38届国际机器学习会议论文集》，《机器学习研究论文集》第139卷，第5023–5033页。PMLR。
- en: '(84) Jiang, Z., Zhang, T., Kirk, R., Rocktäschel, T., and Grefenstette, E.
    (2021c). Graph backup: Data efficient backup exploiting markovian data. In Deep
    RL Workshop NeurIPS 2021.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (84) Jiang, Z., Zhang, T., Kirk, R., Rocktäschel, T., 和 Grefenstette, E. (2021c).
    图备份：利用马尔可夫数据的数据高效备份。见Deep RL Workshop NeurIPS 2021。
- en: Kapturowski et al., (2022) Kapturowski, S., Campos, V., Jiang, R., Rakićević,
    N., van Hasselt, H., Blundell, C., and Badia, A. P. (2022). Human-level atari
    200x faster. arXiv preprint arXiv:2209.07550.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapturowski 等 (2022) Kapturowski, S., Campos, V., Jiang, R., Rakićević, N.,
    van Hasselt, H., Blundell, C., 和 Badia, A. P. (2022). 人类水平的Atari游戏速度提升200倍。arXiv预印本
    arXiv:2209.07550。
- en: Kapturowski et al., (2023) Kapturowski, S., Campos, V., Jiang, R., Rakicevic,
    N., van Hasselt, H., Blundell, C., and Badia, A. P. (2023). Human-level atari
    200x faster. In The Eleventh International Conference on Learning Representations.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapturowski 等 (2023) Kapturowski, S., Campos, V., Jiang, R., Rakicevic, N.,
    van Hasselt, H., Blundell, C., 和 Badia, A. P. (2023). 人类水平的Atari游戏速度提升200倍。见第十一届国际学习表示会议。
- en: Kapturowski et al., (2019) Kapturowski, S., Ostrovski, G., Quan, J., Munos,
    R., and Dabney, W. (2019). Recurrent experience replay in distributed reinforcement
    learning. In International conference on learning representations.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapturowski 等 (2019) Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., 和
    Dabney, W. (2019). 分布式强化学习中的递归经验回放。见国际学习表示会议。
- en: 'Kempka et al., (2016) Kempka, M., Wydmuch, M., Runc, G., Toczek, J., and Jaśkowski,
    W. (2016). Vizdoom: A doom-based ai research platform for visual reinforcement
    learning. In 2016 IEEE conference on computational intelligence and games (CIG),
    pages 1–8\. IEEE.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kempka 等 (2016) Kempka, M., Wydmuch, M., Runc, G., Toczek, J., 和 Jaśkowski,
    W. (2016). Vizdoom：基于《毁灭战士》的视觉强化学习AI研究平台。见2016 IEEE计算智能与游戏会议（CIG），第1–8页。IEEE。
- en: Kirk et al., (2023) Kirk, R., Zhang, A., Grefenstette, E., and Rocktäschel,
    T. (2023). A survey of zero-shot generalisation in deep reinforcement learning.
    Journal of Artificial Intelligence Research, 76:201–264.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirk 等 (2023) Kirk, R., Zhang, A., Grefenstette, E., 和 Rocktäschel, T. (2023).
    深度强化学习中零-shot泛化的调查。人工智能研究杂志，76:201–264。
- en: Klissarov et al., (2022) Klissarov, M., Fakoor, R., Mueller, J., Asadi, K.,
    Kim, T., and Smola, A. (2022). Adaptive interest for emphatic reinforcement learning.
    In Decision Awareness in Reinforcement Learning Workshop at ICML 2022.
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Klissarov 等（2022）Klissarov, M., Fakoor, R., Mueller, J., Asadi, K., Kim, T.,
    and Smola, A. (2022). 适应性兴趣用于同情强化学习。发表于 ICML 2022 会议中的决策意识强化学习研讨会。
- en: Klissarov and Precup, (2021) Klissarov, M. and Precup, D. (2021). Flexible option
    learning. Advances in Neural Information Processing Systems, 34:4632–4646.
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Klissarov 和 Precup（2021）Klissarov, M. 和 Precup, D. (2021). 灵活的选项学习。《神经信息处理系统进展》，34:4632–4646。
- en: 'Klopf, (1972) Klopf, A. H. (1972). Brain function and adaptive systems: a heterostatic
    theory. Technical Report 133, Air Force Cambridge Research Laboratories. Special
    Reports, Bedford, Massachusets.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Klopf（1972）Klopf, A. H. (1972). 大脑功能与自适应系统：一种异质静态理论。技术报告 133，空军剑桥研究实验室。特别报告，马萨诸塞州贝德福德。
- en: 'Kormushev et al., (2013) Kormushev, P., Calinon, S., and Caldwell, D. G. (2013).
    Reinforcement learning in robotics: Applications and real-world challenges. Robotics,
    2(3):122–148.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kormushev 等（2013）Kormushev, P., Calinon, S., and Caldwell, D. G. (2013). 机器人中的强化学习：应用及实际挑战。《机器人学》，2(3):122–148。
- en: Küttler et al., (2020) Küttler, H., Nardelli, N., Miller, A., Raileanu, R.,
    Selvatici, M., Grefenstette, E., and Rocktäschel, T. (2020). The nethack learning
    environment. Advances in Neural Information Processing Systems, 33:7671–7684.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Küttler 等（2020）Küttler, H., Nardelli, N., Miller, A., Raileanu, R., Selvatici,
    M., Grefenstette, E., 和 Rocktäschel, T. (2020). Nethack 学习环境。《神经信息处理系统进展》，33:7671–7684。
- en: 'Ladosz et al., (2022) Ladosz, P., Weng, L., Kim, M., and Oh, H. (2022). Exploration
    in deep reinforcement learning: A survey. Information Fusion, 85:1–22.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ladosz 等（2022）Ladosz, P., Weng, L., Kim, M., and Oh, H. (2022). 深度强化学习中的探索：综述。《信息融合》，85:1–22。
- en: Lai et al., (2020) Lai, H., Shen, J., Zhang, W., and Yu, Y. (2020). Bidirectional
    model-based policy optimization. In International Conference on Machine Learning,
    pages 5618–5627\. PMLR.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai 等（2020）Lai, H., Shen, J., Zhang, W., and Yu, Y. (2020). 双向基于模型的策略优化。发表于国际机器学习会议，页面
    5618–5627\. PMLR。
- en: 'Lamb et al., (2016) Lamb, A. M., Goyal, A., Zhang, Y., Zhang, S., Courville,
    A. C., and Bengio, Y. (2016). Professor forcing: A new algorithm for training
    recurrent networks. Advances in neural information processing systems, 29.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lamb 等（2016）Lamb, A. M., Goyal, A., Zhang, Y., Zhang, S., Courville, A. C.,
    和 Bengio, Y. (2016). 教授强迫：一种新的递归网络训练算法。《神经信息处理系统进展》，29。
- en: Lattal, (2010) Lattal, K. A. (2010). Delayed reinforcement of operant behavior.
    Journal of the Experimental Analysis of Behavior, 93(1):129–139.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lattal（2010）Lattal, K. A. (2010). 操作行为的延迟强化。《实验行为分析期刊》，93(1):129–139。
- en: Le Lan et al., (2022) Le Lan, C., Tu, S., Oberman, A., Agarwal, R., and Bellemare,
    M. G. (2022). On the generalization of representations in reinforcement learning.
    In International Conference on Artificial Intelligence and Statistics, pages 4132–4157\.
    PMLR.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le Lan 等（2022）Le Lan, C., Tu, S., Oberman, A., Agarwal, R., and Bellemare, M.
    G. (2022). 强化学习中表征的泛化问题。发表于国际人工智能与统计会议，页面 4132–4157\. PMLR。
- en: Lee et al., (2022) Lee, K.-H., Nachum, O., Yang, M., Lee, L., Freeman, D., Xu,
    W., Guadarrama, S., Fischer, I., Jang, E., Michalewski, H., et al. (2022). Multi-game
    decision transformers. arXiv preprint arXiv:2205.15241.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2022）Lee, K.-H., Nachum, O., Yang, M., Lee, L., Freeman, D., Xu, W., Guadarrama,
    S., Fischer, I., Jang, E., Michalewski, H., 等（2022）。多游戏决策变换器。arXiv 预印本 arXiv:2205.15241。
- en: Lillicrap et al., (2015) Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess,
    N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Continuous control
    with deep reinforcement learning. arXiv preprint arXiv:1509.02971.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lillicrap 等（2015）Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
    T., Tassa, Y., Silver, D., 和 Wierstra, D. (2015). 使用深度强化学习的连续控制。arXiv 预印本 arXiv:1509.02971。
- en: Lin, (1992) Lin, L.-J. (1992). Self-improving reactive agents based on reinforcement
    learning, planning and teaching. Machine learning, 8(3):293–321.
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin（1992）Lin, L.-J. (1992). 基于强化学习、规划和教学的自我改进反应型代理。《机器学习》，8(3):293–321。
- en: Lin et al., (2017) Lin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou,
    B., and Bengio, Y. (2017). A structured self-attentive sentence embedding. In
    International Conference on Learning Representations.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2017）Lin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou, B.,
    和 Bengio, Y. (2017). 结构化自注意句子嵌入。发表于国际学习表征会议。
- en: 'Liu et al., (2022) Liu, M., Zhu, M., and Zhang, W. (2022). Goal-conditioned
    reinforcement learning: Problems and solutions. arXiv preprint arXiv:2201.08299.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2022）Liu, M., Zhu, M., and Zhang, W. (2022). 目标条件强化学习：问题与解决方案。arXiv 预印本
    arXiv:2201.08299。
- en: Lu et al., (2020) Lu, C., Huang, B., Wang, K., Hernández-Lobato, J. M., Zhang,
    K., and Schölkopf, B. (2020). Sample-efficient reinforcement learning via counterfactual-based
    data augmentation. arXiv preprint arXiv:2012.09092.
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人 (2020) Lu, C., Huang, B., Wang, K., Hernández-Lobato, J. M., Zhang, K.,
    和 Schölkopf, B. (2020). 通过反事实数据增强的样本高效强化学习. arXiv 预印本 arXiv:2012.09092。
- en: Luketina et al., (2019) Luketina, J., Nardelli, N., Farquhar, G., Foerster,
    J., Andreas, J., Grefenstette, E., Whiteson, S., and Rocktäschel, T. (2019). A
    survey of reinforcement learning informed by natural language. In Proceedings
    of the Twenty-Eighth International Joint Conference on Artificial Intelligence,
    IJCAI 2019, August 10-16 2019, Macao, China., volume 57, pages 6309–6317\. AAAI
    Press (Association for the Advancement of Artificial Intelligence).
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luketina 等人 (2019) Luketina, J., Nardelli, N., Farquhar, G., Foerster, J., Andreas,
    J., Grefenstette, E., Whiteson, S., 和 Rocktäschel, T. (2019). 自然语言启发的强化学习调查. 在第二十八届国际人工智能联合会议
    IJCAI 2019 会议录中，2019 年 8 月 10-16 日，澳门，中国，第 57 卷，第 6309–6317 页。AAAI出版社（人工智能进步协会）。
- en: Luoma et al., (2017) Luoma, J., Ruutu, S., King, A. W., and Tikkanen, H. (2017).
    Time delays, competitive interdependence, and firm performance. Strategic Management
    Journal, 38(3):506–525.
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luoma 等人 (2017) Luoma, J., Ruutu, S., King, A. W., 和 Tikkanen, H. (2017). 时间延迟、竞争依赖性与公司绩效.
    《战略管理杂志》，38(3):506–525。
- en: Ma et al., (2021) Ma, M., D’Oro, P., Bengio, Y., and Bacon, P.-L. (2021). Long-term
    credit assignment via model-based temporal shortcuts. In Deep RL Workshop NeurIPS
    2021.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等人 (2021) Ma, M., D’Oro, P., Bengio, Y., 和 Bacon, P.-L. (2021). 通过基于模型的时间短路进行长期信用分配.
    在深度 RL 研讨会 NeurIPS 2021 中。
- en: Mahmood et al., (2015) Mahmood, A. R., Yu, H., White, M., and Sutton, R. S.
    (2015). Emphatic temporal-difference learning. arXiv preprint arXiv:1507.01569.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahmood 等人 (2015) Mahmood, A. R., Yu, H., White, M., 和 Sutton, R. S. (2015).
    强调时间差分学习. arXiv 预印本 arXiv:1507.01569。
- en: Mendonca et al., (2019) Mendonca, M. R., Ziviani, A., and Barreto, A. M. (2019).
    Graph-based skill acquisition for reinforcement learning. ACM Computing Surveys
    (CSUR), 52(1):1–26.
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mendonca 等人 (2019) Mendonca, M. R., Ziviani, A., 和 Barreto, A. M. (2019). 基于图的强化学习技能获取.
    《ACM计算调查 (CSUR)》，52(1):1–26。
- en: Mesnard et al., (2023) Mesnard, T., Chen, W., Saade, A., Tang, Y., Rowland,
    M., Weber, T., Lyle, C., Gruslys, A., Valko, M., Dabney, W., Ostrovski, G., Moulines,
    E., and Munos, R. (2023). Quantile credit assignment. In Krause, A., Brunskill,
    E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J., editors, Proceedings
    of the 40th International Conference on Machine Learning, volume 202 of Proceedings
    of Machine Learning Research, pages 24517–24531\. PMLR.
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesnard 等人 (2023) Mesnard, T., Chen, W., Saade, A., Tang, Y., Rowland, M., Weber,
    T., Lyle, C., Gruslys, A., Valko, M., Dabney, W., Ostrovski, G., Moulines, E.,
    和 Munos, R. (2023). 分位数信用分配. 在 Krause, A., Brunskill, E., Cho, K., Engelhardt,
    B., Sabato, S., 和 Scarlett, J. 编辑的《第 40 届国际机器学习会议录》中，第 202 卷，机器学习研究会会议论文集，第 24517–24531
    页。PMLR。
- en: Mesnard et al., (2021) Mesnard, T., Weber, T., Viola, F., Thakoor, S., Saade,
    A., Harutyunyan, A., Dabney, W., Stepleton, T. S., Heess, N., Guez, A., et al.
    (2021). Counterfactual credit assignment in model-free reinforcement learning.
    In International Conference on Machine Learning, pages 7654–7664\. Proceedings
    of Machine Learning Research.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesnard 等人 (2021) Mesnard, T., Weber, T., Viola, F., Thakoor, S., Saade, A.,
    Harutyunyan, A., Dabney, W., Stepleton, T. S., Heess, N., Guez, A., 等 (2021).
    无模型强化学习中的反事实信用分配. 在国际机器学习会议中，第 7654–7664 页。机器学习研究会会议论文集。
- en: Michie, (1963) Michie, D. (1963). Experiments on the mechanization of game-learning
    part i. characterization of the model and its parameters. The Computer Journal,
    6(3):232–236.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michie (1963) Michie, D. (1963). 游戏学习机制化实验 I 部分. 模型及其参数的特征描述. 《计算机杂志》，6(3):232–236。
- en: Minsky, (1961) Minsky, M. (1961). Steps toward artificial intelligence. Proceedings
    of the IRE, 49(1):8–30.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minsky (1961) Minsky, M. (1961). 朝向人工智能的步骤. 《IRE 会议录》，49(1):8–30。
- en: Mirhoseini et al., (2020) Mirhoseini, A., Goldie, A., Yazgan, M., Jiang, J.,
    Songhori, E., Wang, S., Lee, Y.-J., Johnson, E., Pathak, O., Bae, S., et al. (2020).
    Chip placement with deep reinforcement learning. arXiv preprint arXiv:2004.10746.
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirhoseini 等人 (2020) Mirhoseini, A., Goldie, A., Yazgan, M., Jiang, J., Songhori,
    E., Wang, S., Lee, Y.-J., Johnson, E., Pathak, O., Bae, S., 等 (2020). 使用深度强化学习的芯片布局.
    arXiv 预印本 arXiv:2004.10746。
- en: Mnih et al., (2016) Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
    T., Harley, T., Silver, D., and Kavukcuoglu, K. (2016). Asynchronous methods for
    deep reinforcement learning. In International conference on machine learning,
    pages 1928–1937\. PMLR.
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等人 (2016) Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T.,
    Harley, T., Silver, D., 和 Kavukcuoglu, K. (2016). 用于深度强化学习的异步方法. 在国际机器学习会议中，第
    1928–1937 页。PMLR。
- en: Mnih et al., (2013) Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou,
    I., Wierstra, D., and Riedmiller, M. (2013). Playing atari with deep reinforcement
    learning. In Advances in Neural Information Processing Systems, Deep Learning
    Workshop.
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等 (2013) Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou,
    I., Wierstra, D., 和 Riedmiller, M. (2013). 通过深度强化学习玩 Atari 游戏。在神经信息处理系统进展，深度学习研讨会。
- en: Mnih et al., (2015) Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
    J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski,
    G., et al. (2015). Human-level control through deep reinforcement learning. Nature,
    518(7540):529–533.
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等 (2015) Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J.,
    Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G.,
    等 (2015). 通过深度强化学习实现人类级控制。自然，第518卷第7540期，第529–533页。
- en: Mousavi et al., (2017) Mousavi, S. S., Schukat, M., Howley, E., and Mannion,
    P. (2017). Applying q ($\lambda$)-learning in deep reinforcement learning to play
    atari games. In AAMAS Adaptive Learning Agents (ALA) Workshop, pages 1–6.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mousavi 等 (2017) Mousavi, S. S., Schukat, M., Howley, E., 和 Mannion, P. (2017).
    在深度强化学习中应用 q ($\lambda$)-学习以玩 Atari 游戏。在 AAMAS 自适应学习代理（ALA）研讨会，第1–6页。
- en: Nair et al., (2018) Nair, A. V., Pong, V., Dalal, M., Bahl, S., Lin, S., and
    Levine, S. (2018). Visual reinforcement learning with imagined goals. Advances
    in neural information processing systems, 31.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair 等 (2018) Nair, A. V., Pong, V., Dalal, M., Bahl, S., Lin, S., 和 Levine,
    S. (2018). 带有想象目标的视觉强化学习。神经信息处理系统进展，第31卷。
- en: 'Nair et al., (2020) Nair, S., Babaeizadeh, M., Finn, C., Levine, S., and Kumar,
    V. (2020). Trass: Time reversal as self-supervision. In 2020 IEEE International
    Conference on Robotics and Automation (ICRA), pages 115–121\. IEEE.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair 等 (2020) Nair, S., Babaeizadeh, M., Finn, C., Levine, S., 和 Kumar, V. (2020).
    Trass：时间反转作为自我监督。在2020 IEEE 国际机器人与自动化会议（ICRA），第115–121页。IEEE。
- en: 'Ng et al., (1999) Ng, A. Y., Harada, D., and Russell, S. (1999). Policy invariance
    under reward transformations: Theory and application to reward shaping. In Icml,
    volume 99, pages 278–287\. Citeseer.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng 等 (1999) Ng, A. Y., Harada, D., 和 Russell, S. (1999). 奖励变换下的政策不变性：理论及其在奖励塑造中的应用。在
    Icml，第99卷，第278–287页。Citeseer。
- en: Nguyen and Reddi, (2021) Nguyen, T. T. and Reddi, V. J. (2021). Deep reinforcement
    learning for cyber security. IEEE Transactions on Neural Networks and Learning
    Systems.
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 和 Reddi (2021) Nguyen, T. T. 和 Reddi, V. J. (2021). 用于网络安全的深度强化学习。IEEE
    神经网络与学习系统汇刊。
- en: 'Nota et al., (2021) Nota, C., Thomas, P., and Silva, B. C. D. (2021). Posterior
    value functions: Hindsight baselines for policy gradient methods. In Meila, M.
    and Zhang, T., editors, Proceedings of the 38th International Conference on Machine
    Learning, volume 139 of Proceedings of Machine Learning Research, pages 8238–8247\.
    PMLR.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nota 等 (2021) Nota, C., Thomas, P., 和 Silva, B. C. D. (2021). 后验价值函数：政策梯度方法的事后基准。在
    Meila, M. 和 Zhang, T. 编者，《第38届国际机器学习大会论文集》，机器学习研究论文集第139卷，第8238–8247页。PMLR。
- en: Oh et al., (2018) Oh, J., Guo, Y., Singh, S., and Lee, H. (2018). Self-imitation
    learning. In International Conference on Machine Learning, pages 3878–3887\. PMLR.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh 等 (2018) Oh, J., Guo, Y., Singh, S., 和 Lee, H. (2018). 自我模仿学习。在国际机器学习大会，第3878–3887页。PMLR。
- en: Oh et al., (2020) Oh, J., Hessel, M., Czarnecki, W. M., Xu, Z., van Hasselt,
    H. P., Singh, S., and Silver, D. (2020). Discovering reinforcement learning algorithms.
    In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors,
    Advances in Neural Information Processing Systems, volume 33, pages 1060–1070\.
    Curran Associates, Inc.
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh 等 (2020) Oh, J., Hessel, M., Czarnecki, W. M., Xu, Z., van Hasselt, H. P.,
    Singh, S., 和 Silver, D. (2020). 发现强化学习算法。在 Larochelle, H., Ranzato, M., Hadsell,
    R., Balcan, M., 和 Lin, H. 编者，《神经信息处理系统进展》，第33卷，第1060–1070页。Curran Associates,
    Inc.
- en: Osband et al., (2020) Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener,
    E., Saraiva, A., McKinney, K., Lattimore, T., Szepesvári, C., Singh, S., Van Roy,
    B., Sutton, R., Silver, D., and van Hasselt, H. (2020). Behaviour suite for reinforcement
    learning. In International Conference on Learning Representations.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osband 等 (2020) Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E.,
    Saraiva, A., McKinney, K., Lattimore, T., Szepesvári, C., Singh, S., Van Roy,
    B., Sutton, R., Silver, D., 和 van Hasselt, H. (2020). 强化学习行为套件。在国际学习表征会议。
- en: Pan et al., (2022) Pan, H.-R., Gürtler, N., Neitz, A., and Schölkopf, B. (2022).
    Direct advantage estimation. Advances in Neural Information Processing Systems,
    35:11869–11880.
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等 (2022) Pan, H.-R., Gürtler, N., Neitz, A., 和 Schölkopf, B. (2022). 直接优势估计。神经信息处理系统进展，第35卷，第11869–11880页。
- en: 'Pateria et al., (2021) Pateria, S., Subagdja, B., Tan, A.-h., and Quek, C.
    (2021). Hierarchical reinforcement learning: A comprehensive survey. ACM Computing
    Surveys (CSUR), 54(5):1–35.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pateria 等人 (2021) Pateria, S., Subagdja, B., Tan, A.-h., 和 Quek, C. (2021).
    分层强化学习：全面调查。ACM 计算调查（CSUR），54(5):1–35.
- en: Pavlov, (1927) Pavlov, P. I. (1927). Conditioned Reflexes. Oxford University
    Press, London, UK.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavlov (1927) Pavlov, P. I. (1927). 条件反射。牛津大学出版社，伦敦，英国.
- en: 'Pearl, (2009) Pearl, J. (2009). Causal inference in statistics: An overview.
    Statistics Surveys, 3:96–146.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearl (2009) Pearl, J. (2009). 统计学中的因果推断：概述。统计调查，3:96–146.
- en: 'Pearl et al., (2000) Pearl, J. et al. (2000). Models, reasoning and inference.
    Cambridge, UK: CambridgeUniversityPress, 19(2):3.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearl 等人 (2000) Pearl, J. 等 (2000). 模型、推理与推断。剑桥，英国：剑桥大学出版社，19(2):3.
- en: Perolat et al., (2022) Perolat, J., De Vylder, B., Hennes, D., Tarassov, E.,
    Strub, F., de Boer, V., Muller, P., Connor, J. T., Burch, N., Anthony, T., et al.
    (2022). Mastering the game of stratego with model-free multiagent reinforcement
    learning. Science, 378(6623):990–996.
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perolat 等人 (2022) Perolat, J., De Vylder, B., Hennes, D., Tarassov, E., Strub,
    F., de Boer, V., Muller, P., Connor, J. T., Burch, N., Anthony, T., 等 (2022).
    使用无模型多智能体强化学习掌握 Stratego 游戏。科学，378(6623):990–996.
- en: Piaget et al., (1952) Piaget, J., Cook, M., et al. (1952). The origins of intelligence
    in children, volume 8. International Universities Press New York.
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piaget 等人 (1952) Piaget, J., Cook, M., 等 (1952). 儿童智力的起源，第 8 卷。国际大学出版社，纽约.
- en: 'Pitis, (2019) Pitis, S. (2019). Rethinking the discount factor in reinforcement
    learning: A decision theoretic approach. In Proceedings of the AAAI Conference
    on Artificial Intelligence, volume 33, pages 7949–7956.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pitis (2019) Pitis, S. (2019). 在强化学习中重新思考折扣因子：一种决策理论方法。在 AAAI 人工智能会议论文集，卷 33，第
    7949–7956 页.
- en: Pitis et al., (2020) Pitis, S., Creager, E., and Garg, A. (2020). Counterfactual
    data augmentation using locally factored dynamics. Advances in Neural Information
    Processing Systems, 33:3976–3990.
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pitis 等人 (2020) Pitis, S., Creager, E., 和 Garg, A. (2020). 使用局部因子动态的反事实数据增强。神经信息处理系统进展，33:3976–3990.
- en: Prakash et al., (2021) Prakash, C., Stephens, K. D., Hoffman, D. D., Singh,
    M., and Fields, C. (2021). Fitness beats truth in the evolution of perception.
    Acta Biotheoretica, 69:319–341.
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prakash 等人 (2021) Prakash, C., Stephens, K. D., Hoffman, D. D., Singh, M., 和
    Fields, C. (2021). 在感知进化中适应性超越真实性。Acta Biotheoretica, 69:319–341.
- en: (138) Precup, D. (2000a). Eligibility traces for off-policy policy evaluation.
    Computer Science Department Faculty Publication Series, page 80.
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (138) Precup, D. (2000a). 离策略策略评估的资格跟踪。计算机科学系教职员工出版系列，第 80 页.
- en: (139) Precup, D. (2000b). Temporal abstraction in reinforcement learning. PhD
    thesis, University of Massachusetts Amherst.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (139) Precup, D. (2000b). 强化学习中的时间抽象。博士论文，马萨诸塞大学阿默斯特分校.
- en: 'Puterman, (2014) Puterman, M. L. (2014). Markov decision processes: discrete
    stochastic dynamic programming. John Wiley & Sons.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puterman (2014) Puterman, M. L. (2014). 马尔可夫决策过程：离散随机动态规划。John Wiley & Sons.
- en: Puterman and Shin, (1978) Puterman, M. L. and Shin, M. C. (1978). Modified policy
    iteration algorithms for discounted markov decision problems. Management Science,
    24(11):1127–1137.
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puterman 和 Shin (1978) Puterman, M. L. 和 Shin, M. C. (1978). 折扣马尔可夫决策问题的修改策略迭代算法。管理科学，24(11):1127–1137.
- en: Racanière et al., (2017) Racanière, S., Weber, T., Reichert, D., Buesing, L.,
    Guez, A., Jimenez Rezende, D., Puigdomènech Badia, A., Vinyals, O., Heess, N.,
    Li, Y., et al. (2017). Imagination-augmented agents for deep reinforcement learning.
    Advances in neural information processing systems, 30.
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Racanière 等人 (2017) Racanière, S., Weber, T., Reichert, D., Buesing, L., Guez,
    A., Jimenez Rezende, D., Puigdomènech Badia, A., Vinyals, O., Heess, N., Li, Y.,
    等 (2017). 用于深度强化学习的想象增强代理。神经信息处理系统进展，30.
- en: Radford et al., (2018) Radford, A., Narasimhan, K., Salimans, T., Sutskever,
    I., et al. (2018). Improving language understanding by generative pre-training.
    OpenAI blog.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 (2018) Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.,
    等 (2018). 通过生成预训练提高语言理解。OpenAI 博客.
- en: Radford et al., (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    Sutskever, I., et al. (2019). Language models are unsupervised multitask learners.
    OpenAI blog, 1(8):9.
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever,
    I., 等 (2019). 语言模型是无监督的多任务学习者。OpenAI 博客, 1(8):9.
- en: Rahmandad et al., (2009) Rahmandad, H., Repenning, N., and Sterman, J. (2009).
    Effects of feedback delay on learning. System Dynamics Review, 25(4):309–338.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahmandad 等人 (2009) Rahmandad, H., Repenning, N., 和 Sterman, J. (2009). 反馈延迟对学习的影响。系统动力学评论，25(4):309–338.
- en: Raposo et al., (2021) Raposo, D., Ritter, S., Santoro, A., Wayne, G., Weber,
    T., Botvinick, M., van Hasselt, H., and Song, F. (2021). Synthetic returns for
    long-term credit assignment. CoRR.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raposo 等人 (2021) Raposo, D., Ritter, S., Santoro, A., Wayne, G., Weber, T.,
    Botvinick, M., van Hasselt, H., 和 Song, F. (2021). 用于长期信用分配的合成回报. CoRR。
- en: Rauber et al., (2019) Rauber, P., Ummadisingu, A., Mutz, F., and Schmidhuber,
    J. (2019). Hindsight policy gradients. In International Conference on Learning
    Representations.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rauber 等人 (2019) Rauber, P., Ummadisingu, A., Mutz, F., 和 Schmidhuber, J. (2019).
    事后策略梯度. 载于国际学习表征会议。
- en: Ren et al., (2022) Ren, Z., Guo, R., Zhou, Y., and Peng, J. (2022). Learning
    long-term reward redistribution via randomized return decomposition. In International
    Conference on Learning Representations.
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人 (2022) Ren, Z., Guo, R., Zhou, Y., 和 Peng, J. (2022). 通过随机回报分解学习长期奖励重分配.
    载于国际学习表征会议。
- en: Riemer et al., (2018) Riemer, M., Liu, M., and Tesauro, G. (2018). Learning
    abstract options. Advances in neural information processing systems, 31.
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riemer 等人 (2018) Riemer, M., Liu, M., 和 Tesauro, G. (2018). 学习抽象选项. 神经信息处理系统进展,
    31。
- en: Rowland et al., (2020) Rowland, M., Dabney, W., and Munos, R. (2020). Adaptive
    trade-offs in off-policy learning. In International Conference on Artificial Intelligence
    and Statistics, pages 34–44\. PMLR.
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rowland 等人 (2020) Rowland, M., Dabney, W., 和 Munos, R. (2020). 离策略学习中的自适应权衡.
    载于国际人工智能与统计学会议, 页码 34–44. PMLR。
- en: 'Samvelyan et al., (2021) Samvelyan, M., Kirk, R., Kurin, V., Parker-Holder,
    J., Jiang, M., Hambro, E., Petroni, F., Kuttler, H., Grefenstette, E., and Rocktäschel,
    T. (2021). Minihack the planet: A sandbox for open-ended reinforcement learning
    research. In Thirty-fifth Conference on Neural Information Processing Systems
    Datasets and Benchmarks Track (Round 1).'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Samvelyan 等人 (2021) Samvelyan, M., Kirk, R., Kurin, V., Parker-Holder, J.,
    Jiang, M., Hambro, E., Petroni, F., Kuttler, H., Grefenstette, E., 和 Rocktäschel,
    T. (2021). Minihack 地球: 一个用于开放式强化学习研究的沙盒. 载于第三十五届神经信息处理系统会议数据集和基准跟踪 (第一轮)。'
- en: Schaul et al., (2022) Schaul, T., Barreto, A., Quan, J., and Ostrovski, G. (2022).
    The phenomenon of policy churn. arXiv preprint arXiv:2206.00730.
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaul 等人 (2022) Schaul, T., Barreto, A., Quan, J., 和 Ostrovski, G. (2022).
    策略波动现象. arXiv 预印本 arXiv:2206.00730。
- en: (153) Schaul, T., Horgan, D., Gregor, K., and Silver, D. (2015a). Universal
    value function approximators. In Bach, F. and Blei, D., editors, Proceedings of
    the 32nd International Conference on Machine Learning, volume 37 of Proceedings
    of Machine Learning Research, pages 1312–1320, Lille, France. PMLR.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (153) Schaul, T., Horgan, D., Gregor, K., 和 Silver, D. (2015a). 通用价值函数近似. 载于
    Bach, F. 和 Blei, D. 编辑, 第32届国际机器学习会议论文集, 机器学习研究论文集第37卷, 页码 1312–1320, 法国里尔. PMLR。
- en: (154) Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2015b). Prioritized
    experience replay. arXiv preprint arXiv:1511.05952.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (154) Schaul, T., Quan, J., Antonoglou, I., 和 Silver, D. (2015b). 优先经验回放. arXiv
    预印本 arXiv:1511.05952。
- en: Scherrer et al., (2015) Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner,
    B., and Geist, M. (2015). Approximate modified policy iteration and its application
    to the game of tetris. Journal of Machine Learning Research, 16(49):1629–1676.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scherrer 等人 (2015) Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B.,
    和 Geist, M. (2015). 近似修改策略迭代及其在俄罗斯方块游戏中的应用. 机器学习研究杂志, 16(49):1629–1676。
- en: 'Schmidhuber, (2015) Schmidhuber, J. (2015). Deep learning in neural networks:
    An overview. Neural Networks, 61:85–117.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schmidhuber (2015) Schmidhuber, J. (2015). 神经网络中的深度学习: 概述. 神经网络, 61:85–117。'
- en: 'Schmidhuber, (2019) Schmidhuber, J. (2019). Reinforcement learning upside down:
    Don’t predict rewards–just map them to actions. arXiv preprint arXiv:1912.02875.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schmidhuber (2019) Schmidhuber, J. (2019). 反向强化学习: 不预测回报——仅将其映射到动作. arXiv 预印本
    arXiv:1912.02875。'
- en: Schrittwieser et al., (2020) Schrittwieser, J., Antonoglou, I., Hubert, T.,
    Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel,
    T., et al. (2020). Mastering atari, go, chess and shogi by planning with a learned
    model. Nature, 588(7839):604–609.
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schrittwieser 等人 (2020) Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan,
    K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T.,
    等. (2020). 通过规划与学习模型掌握 Atari、围棋、国际象棋和将棋. 自然, 588(7839):604–609。
- en: Schulman et al., (2016) Schulman, J., Moritz, P., Levine, S., Jordan, M., and
    Abbeel, P. (2016). High-dimensional continuous control using generalized advantage
    estimation. In Proceedings of the International Conference on Learning Representations
    (ICLR).
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等人 (2016) Schulman, J., Moritz, P., Levine, S., Jordan, M., 和 Abbeel,
    P. (2016). 使用广义优势估计的高维连续控制. 载于国际学习表征会议 (ICLR) 论文集。
- en: Schulman et al., (2017) Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
    and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint
    arXiv:1707.06347.
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman等人，（2017）Schulman, J., Wolski, F., Dhariwal, P., Radford, A., 和 Klimov,
    O.（2017）。近端策略优化算法。arXiv预印本 arXiv:1707.06347。
- en: Schultz, (1967) Schultz, D. G. (1967). State functions and linear control systems.
    McGraw-Hill Book Company.
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schultz，（1967）Schultz, D. G.（1967）。状态函数和线性控制系统。McGraw-Hill书籍公司。
- en: Seo et al., (2019) Seo, M., Vecchietti, L. F., Lee, S., and Har, D. (2019).
    Rewards prediction-based credit assignment for reinforcement learning with sparse
    binary rewards. IEEE Access, 7:118776–118791.
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seo等人，（2019）Seo, M., Vecchietti, L. F., Lee, S., 和 Har, D.（2019）。基于奖励预测的稀疏二元奖励强化学习的信用分配。IEEE
    Access，7:118776–118791。
- en: Shakerinava and Ravanbakhsh, (2022) Shakerinava, M. and Ravanbakhsh, S. (2022).
    Utility theory for sequential decision making. In International Conference on
    Machine Learning, pages 19616–19625\. PMLR.
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shakerinava和Ravanbakhsh，（2022）Shakerinava, M. 和 Ravanbakhsh, S.（2022）。用于序列决策的效用理论。载于《国际机器学习大会》，第19616–19625页。PMLR。
- en: Shannon, (1950) Shannon, C. E. (1950). Programming a computer for playing chess.
    The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science,
    41(314):256–275.
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shannon，（1950）Shannon, C. E.（1950）。为计算机编程以进行国际象棋游戏。《伦敦、爱丁堡和都柏林哲学杂志及科学杂志》，41(314):256–275。
- en: Silver et al., (2016) Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre,
    L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,
    V., Lanctot, M., et al. (2016). Mastering the game of go with deep neural networks
    and tree search. nature, 529(7587):484–489.
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver等人，（2016）Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,
    Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V.,
    Lanctot, M., 等（2016）。利用深度神经网络和树搜索掌握围棋。自然，529(7587):484–489。
- en: Silver et al., (2018) Silver, D., Hubert, T., Schrittwieser, J., Antonoglou,
    I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al.
    (2018). A general reinforcement learning algorithm that masters chess, shogi,
    and go through self-play. Science, 362(6419):1140–1144.
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver等人，（2018）Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,
    M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., 等（2018）。一种通用的强化学习算法，通过自我对弈掌握国际象棋、将棋和围棋。科学，362(6419):1140–1144。
- en: Singh et al., (2009) Singh, S., Lewis, R. L., and Barto, A. G. (2009). Where
    do rewards come from. In Proceedings of the annual conference of the cognitive
    science society, pages 2601–2606\. Cognitive Science Society.
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh等人，（2009）Singh, S., Lewis, R. L., 和 Barto, A. G.（2009）。奖励来自哪里。载于《认知科学学会年会论文集》，第2601–2606页。认知科学学会。
- en: Singh and Sutton, (1996) Singh, S. P. and Sutton, R. S. (1996). Reinforcement
    learning with replacing eligibility traces. Machine learning, 22(1):123–158.
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh和Sutton，（1996）Singh, S. P. 和 Sutton, R. S.（1996）。使用替代资格迹的强化学习。机器学习，22(1):123–158。
- en: 'Skinner, (1937) Skinner, B. F. (1937). Two types of conditioned reflex: A reply
    to konorski and miller. The Journal of General Psychology, 16(1):272–279.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skinner，（1937）Skinner, B. F.（1937）。两种类型的条件反射：对Konorski和Miller的回应。一般心理学杂志，16(1):272–279。
- en: Smith et al., (2018) Smith, M., Hoof, H., and Pineau, J. (2018). An inference-based
    policy gradient method for learning options. In International Conference on Machine
    Learning, pages 4703–4712\. PMLR.
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith等人，（2018）Smith, M., Hoof, H., 和 Pineau, J.（2018）。一种基于推断的策略梯度方法用于学习选项。载于《国际机器学习大会》，第4703–4712页。PMLR。
- en: Sobel, (1982) Sobel, M. J. (1982). The variance of discounted markov decision
    processes. Journal of Applied Probability, 19(4):794–802.
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sobel，（1982）Sobel, M. J.（1982）。折扣马尔可夫决策过程的方差。应用概率杂志，19(4):794–802。
- en: Srivastava et al., (2019) Srivastava, R. K., Shyam, P., Mutz, F., Jaskowski,
    W., and Schmidhuber, J. (2019). Training agents using upside-down reinforcement
    learning. CoRR, abs/1912.02877.
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava等人，（2019）Srivastava, R. K., Shyam, P., Mutz, F., Jaskowski, W., 和
    Schmidhuber, J.（2019）。使用倒置强化学习训练智能体。CoRR，abs/1912.02877。
- en: Štrupl et al., (2022) Štrupl, M., Faccio, F., Ashley, D. R., Schmidhuber, J.,
    and Srivastava, R. K. (2022). Upside-down reinforcement learning can diverge in
    stochastic environments with episodic resets. arXiv preprint arXiv:2205.06595.
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Štrupl等人，（2022）Štrupl, M., Faccio, F., Ashley, D. R., Schmidhuber, J., 和 Srivastava,
    R. K.（2022）。倒置强化学习在带有情节重置的随机环境中可能会发散。arXiv预印本 arXiv:2205.06595。
- en: Sun et al., (2022) Sun, K., Jiang, B., and Kong, L. (2022). How does value distribution
    in distributional reinforcement learning help optimization? arXiv preprint arXiv:2209.14513.
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等人，（2022）Sun, K., Jiang, B., 和 Kong, L.（2022）。在分布式强化学习中，价值分布如何帮助优化？arXiv预印本
    arXiv:2209.14513。
- en: Sutton et al., (2014) Sutton, R., Mahmood, A. R., Precup, D., and Hasselt, H.
    (2014). A new q (lambda) with interim forward view and monte carlo equivalence.
    In International Conference on Machine Learning, pages 568–576\. PMLR.
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton et al., (2014) Sutton, R., Mahmood, A. R., Precup, D., 和 Hasselt, H.
    (2014). 一种新的 Q (lambda) 与临时前向视图及蒙特卡罗等效。在国际机器学习会议，页码 568–576\. PMLR。
- en: Sutton, (1984) Sutton, R. S. (1984). Temporal credit assignment in reinforcement
    learning. PhD thesis, University of Massachusetts.
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton, (1984) Sutton, R. S. (1984). 强化学习中的时间信用分配。博士论文，马萨诸塞大学。
- en: Sutton, (1988) Sutton, R. S. (1988). Learning to predict by the methods of temporal
    differences. Machine learning, 3:9–44.
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton, (1988) Sutton, R. S. (1988). 通过时间差分方法学习预测。《机器学习》，3:9–44。
- en: Sutton, (1990) Sutton, R. S. (1990). Integrated architectures for learning,
    planning, and reacting based on approximating dynamic programming. In Machine
    learning proceedings 1990, pages 216–224\. Elsevier.
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton, (1990) Sutton, R. S. (1990). 基于近似动态规划的学习、规划和反应的集成架构。在 1990 年机器学习会议，页码
    216–224\. Elsevier。
- en: 'Sutton, (1992) Sutton, R. S. (1992). Adapting bias by gradient descent: An
    incremental version of delta-bar-delta. In AAAI, pages 171–176\. Citeseer.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton, (1992) Sutton, R. S. (1992). 通过梯度下降适应偏差：一种增量版的 delta-bar-delta。在 AAAI，页码
    171–176\. Citeseer。
- en: Sutton, (2004) Sutton, R. S. (2004). The reward hypothesis. [http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html](http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html).
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton, (2004) Sutton, R. S. (2004). 奖励假设。 [http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html](http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html)。
- en: 'Sutton and Barto, (2018) Sutton, R. S. and Barto, A. G. (2018). Reinforcement
    Learning: an introduction. MIT Press, 2nd edition.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton and Barto, (2018) Sutton, R. S. 和 Barto, A. G. (2018). 《强化学习：导论》。MIT
    Press，第 2 版。
- en: Sutton et al., (2016) Sutton, R. S., Mahmood, A. R., and White, M. (2016). An
    emphatic approach to the problem of off-policy temporal-difference learning. The
    Journal of Machine Learning Research, 17(1):2603–2631.
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton et al., (2016) Sutton, R. S., Mahmood, A. R., 和 White, M. (2016). 解决偏政策时间差分学习问题的感性方法。《机器学习研究期刊》，17(1):2603–2631。
- en: 'Sutton et al., (2011) Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski,
    P. M., White, A., and Precup, D. (2011). Horde: A scalable real-time architecture
    for learning knowledge from unsupervised sensorimotor interaction. In The 10th
    International Conference on Autonomous Agents and Multiagent Systems-Volume 2,
    pages 761–768.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton et al., (2011) Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski,
    P. M., White, A., 和 Precup, D. (2011). Horde：一种可扩展的实时架构，用于从无监督传感运动交互中学习知识。在第 10
    届自主代理和多智能体系统国际会议-第 2 卷，页码 761–768。
- en: 'Sutton et al., (1999) Sutton, R. S., Precup, D., and Singh, S. (1999). Between
    mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning.
    Artificial intelligence, 112(1-2):181–211.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton et al., (1999) Sutton, R. S., Precup, D., 和 Singh, S. (1999). 在 mdps
    和 semi-mdps 之间：强化学习中的时间抽象框架。《人工智能》，112(1-2):181–211。
- en: Tang and Kucukelbir, (2021) Tang, Y. and Kucukelbir, A. (2021). Hindsight expectation
    maximization for goal-conditioned reinforcement learning. In International Conference
    on Artificial Intelligence and Statistics, pages 2863–2871\. PMLR.
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang and Kucukelbir, (2021) Tang, Y. 和 Kucukelbir, A. (2021). 用于目标条件强化学习的事后期望最大化。在国际人工智能与统计会议，页码
    2863–2871\. PMLR。
- en: 'Thorndike, (1898) Thorndike, E. L. (1898). Animal intelligence: An experimental
    study of the associative processes in animals. American Journal of Psychology,
    2(4).'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thorndike, (1898) Thorndike, E. L. (1898). 动物智力：动物联想过程的实验研究。《美国心理学杂志》，2(4)。
- en: 'Todorov et al., (2012) Todorov, E., Erez, T., and Tassa, Y. (2012). Mujoco:
    A physics engine for model-based control. In International conference on intelligent
    robots and systems, pages 5026–5033\. IEEE.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Todorov et al., (2012) Todorov, E., Erez, T., 和 Tassa, Y. (2012). Mujoco：用于基于模型控制的物理引擎。在国际智能机器人与系统会议，页码
    5026–5033\. IEEE。
- en: van Hasselt et al., (2018) van Hasselt, H., Doron, Y., Strub, F., Hessel, M.,
    Sonnerat, N., and Modayil, J. (2018). Deep reinforcement learning and the deadly
    triad. arXiv preprint arXiv:1812.02648.
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Hasselt et al., (2018) van Hasselt, H., Doron, Y., Strub, F., Hessel, M.,
    Sonnerat, N., 和 Modayil, J. (2018). 深度强化学习与致命三联体。arXiv 预印本 arXiv:1812.02648。
- en: van Hasselt et al., (2016) van Hasselt, H., Guez, A., and Silver, D. (2016).
    Deep reinforcement learning with double q-learning. In Proceedings of the AAAI
    conference on artificial intelligence, volume 30.
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Hasselt et al., (2016) van Hasselt, H., Guez, A., 和 Silver, D. (2016). 使用双重
    Q 学习的深度强化学习。在 AAAI 人工智能会议论文集中，第 30 卷。
- en: van Hasselt et al., (2021) van Hasselt, H., Madjiheurem, S., Hessel, M., Silver,
    D., Barreto, A., and Borsa, D. (2021). Expected eligibility traces. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 35, pages 9997–10005.
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Hasselt 等 (2021) van Hasselt, H., Madjiheurem, S., Hessel, M., Silver, D.,
    Barreto, A., 和 Borsa, D. (2021). 期望资格迹。在《AAAI 人工智能会议论文集》，第 35 卷，第 9997–10005 页。
- en: van Hasselt and Sutton, (2015) van Hasselt, H. and Sutton, R. S. (2015). Learning
    to predict independent of span. arXiv preprint arXiv:1508.04582.
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Hasselt 和 Sutton (2015) van Hasselt, H. 和 Sutton, R. S. (2015). 学习预测与跨度无关。arXiv
    预印本 arXiv:1508.04582。
- en: van Hasselt and Wiering, (2009) van Hasselt, H. and Wiering, M. A. (2009). Using
    continuous action spaces to solve discrete problems. In 2009 International Joint
    Conference on Neural Networks, pages 1149–1156\. IEEE.
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Hasselt 和 Wiering (2009) van Hasselt, H. 和 Wiering, M. A. (2009). 使用连续动作空间解决离散问题。在
    2009 年国际神经网络联合会议， 第 1149–1156 页。IEEE。
- en: van Hasselt et al., (2019) van Hasselt, H. P., Hessel, M., and Aslanides, J.
    (2019). When to use parametric models in reinforcement learning? Advances in Neural
    Information Processing Systems, 32.
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Hasselt 等 (2019) van Hasselt, H. P., Hessel, M., 和 Aslanides, J. (2019).
    何时在强化学习中使用参数化模型？《神经信息处理系统进展》，32。
- en: Vaswani et al., (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all
    you need. Advances in neural information processing systems, 30.
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., 和 Polosukhin, I. (2017). 注意力机制是你需要的一切。《神经信息处理系统进展》，30。
- en: Venuto et al., (2022) Venuto, D., Lau, E., Precup, D., and Nachum, O. (2022).
    Policy gradients incorporating the future. In International Conference on Learning
    Representations.
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venuto 等 (2022) Venuto, D., Lau, E., Precup, D., 和 Nachum, O. (2022). 融入未来的策略梯度。在国际学习表征会议。
- en: '(196) Vieillard, N., Kozuno, T., Scherrer, B., Pietquin, O., Munos, R., and
    Geist, M. (2020a). Leverage the average: an analysis of kl regularization in reinforcement
    learning. Advances in Neural Information Processing Systems, 33:12163–12174.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vieillard 等 (2020a) Vieillard, N., Kozuno, T., Scherrer, B., Pietquin, O., Munos,
    R., 和 Geist, M. (2020a). 利用平均值：强化学习中 kl 正则化的分析。《神经信息处理系统进展》，33:12163–12174。
- en: (197) Vieillard, N., Pietquin, O., and Geist, M. (2020b). Munchausen reinforcement
    learning. Advances in Neural Information Processing Systems, 33:4235–4246.
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vieillard 等 (2020b) Vieillard, N., Pietquin, O., 和 Geist, M. (2020b). 孟乔森强化学习。《神经信息处理系统进展》，33:4235–4246。
- en: 'Vinyals et al., (2019) Vinyals, O., Babuschkin, I., Chung, J., Mathieu, M.,
    Jaderberg, M., Czarnecki, W., Dudzik, A., Huang, A., Georgiev, P., Powell, R.,
    Ewalds, T., Horgan, D., Kroiss, M., Danihelka, I., Agapiou, J., Oh, J., Dalibard,
    V., Choi, D., Sifre, L., Sulsky, Y., Vezhnevets, S., Molloy, J., Cai, T., Budden,
    D., Paine, T., Gulcehre, C., Wang, Z., Pfaff, T., Pohlen, T., Yogatama, D., Cohen,
    J., McKinney, K., Smith, O., Schaul, T., Lillicrap, T., Apps, C., Kavukcuoglu,
    K., Hassabis, D., and Silver, D. (2019). AlphaStar: Mastering the Real-Time Strategy
    Game StarCraft II. [https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/).'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vinyals 等 (2019) Vinyals, O., Babuschkin, I., Chung, J., Mathieu, M., Jaderberg,
    M., Czarnecki, W., Dudzik, A., Huang, A., Georgiev, P., Powell, R., Ewalds, T.,
    Horgan, D., Kroiss, M., Danihelka, I., Agapiou, J., Oh, J., Dalibard, V., Choi,
    D., Sifre, L., Sulsky, Y., Vezhnevets, S., Molloy, J., Cai, T., Budden, D., Paine,
    T., Gulcehre, C., Wang, Z., Pfaff, T., Pohlen, T., Yogatama, D., Cohen, J., McKinney,
    K., Smith, O., Schaul, T., Lillicrap, T., Apps, C., Kavukcuoglu, K., Hassabis,
    D., 和 Silver, D. (2019). AlphaStar: 精通实时战略游戏《星际争霸 II》。[https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)。'
- en: 'Vinyals et al., (2017) Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P.,
    Vezhnevets, A. S., Yeo, M., Makhzani, A., Küttler, H., Agapiou, J., Schrittwieser,
    J., et al. (2017). Starcraft ii: A new challenge for reinforcement learning. arXiv
    preprint arXiv:1708.04782.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 等 (2017) Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets,
    A. S., Yeo, M., Makhzani, A., Küttler, H., Agapiou, J., Schrittwieser, J., 等 (2017).
    《星际争霸 II》：强化学习的新挑战。arXiv 预印本 arXiv:1708.04782。
- en: Wang et al., (2021) Wang, J., Li, W., Jiang, H., Zhu, G., Li, S., and Zhang,
    C. (2021). Offline reinforcement learning with reverse model-based imagination.
    Advances in Neural Information Processing Systems, 34:29420–29432.
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2021) Wang, J., Li, W., Jiang, H., Zhu, G., Li, S., 和 Zhang, C. (2021).
    基于逆模型的离线强化学习。《神经信息处理系统进展》，34:29420–29432。
- en: Wang et al., (2022) Wang, T. T., Gleave, A., Belrose, N., Tseng, T., Miller,
    J., Dennis, M. D., Duan, Y., Pogrebniak, V., Levine, S., and Russell, S. (2022).
    Adversarial policies beat professional-level go ais. arXiv preprint arXiv:2211.00241.
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人，(2022) Wang, T. T., Gleave, A., Belrose, N., Tseng, T., Miller, J., Dennis,
    M. D., Duan, Y., Pogrebniak, V., Levine, S., 和 Russell, S. (2022). 对抗策略击败职业级围棋AI。arXiv预印本arXiv:2211.00241。
- en: (202) Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K.,
    and de Freitas, N. (2016a). Sample efficient actor-critic with experience replay.
    In International Conference on Learning Representations.
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (202) Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K.,
    和 de Freitas, N. (2016a). 样本高效的演员-评论家方法与经验回放。在国际学习表征会议上。
- en: 'Wang and Hong, (2020) Wang, Z. and Hong, T. (2020). Reinforcement learning
    for building controls: The opportunities and challenges. Applied Energy, 269:115036.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang和Hong，(2020) Wang, Z. 和 Hong, T. (2020). 建筑控制的强化学习：机遇与挑战。应用能源，269:115036。
- en: (204) Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., and Freitas,
    N. (2016b). Dueling network architectures for deep reinforcement learning. In
    International conference on machine learning, pages 1995–2003\. PMLR.
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (204) Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., 和 Freitas,
    N. (2016b). 对抗网络架构用于深度强化学习。在国际机器学习会议，页面1995–2003。PMLR。
- en: Watkins, (1989) Watkins, C. J. C. H. (1989). Learning from delayed rewards.
    PhD thesis, King’s College, Cambridge, United Kingdom.
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watkins，(1989) Watkins, C. J. C. H. (1989). 从延迟奖励中学习。博士论文，剑桥大学国王学院，英国。
- en: 'White, (1988) White, D. J. (1988). Mean, variance, and probabilistic criteria
    in finite markov decision processes: A review. Journal of Optimization Theory
    and Applications, 56:1–29.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White，(1988) White, D. J. (1988). 有限马尔可夫决策过程中的均值、方差和概率标准：综述。优化理论与应用杂志，56:1–29。
- en: White, (2017) White, M. (2017). Unifying task specification in reinforcement
    learning. In International Conference on Machine Learning, pages 3742–3750\. PMLR.
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White，(2017) White, M. (2017). 强化学习中任务规范的统一。在国际机器学习会议，页面3742–3750。PMLR。
- en: Williams and Zipser, (1989) Williams, R. J. and Zipser, D. (1989). A learning
    algorithm for continually running fully recurrent neural networks. Neural computation,
    1(2):270–280.
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams和Zipser，(1989) Williams, R. J. 和 Zipser, D. (1989). 一种用于持续运行的完全递归神经网络的学习算法。神经计算，1(2):270–280。
- en: Wurman et al., (2022) Wurman, P. R., Barrett, S., Kawamoto, K., MacGlashan,
    J., Subramanian, K., Walsh, T. J., Capobianco, R., Devlic, A., Eckert, F., Fuchs,
    F., et al. (2022). Outracing champion gran turismo drivers with deep reinforcement
    learning. Nature, 602(7896):223–228.
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wurman等人，(2022) Wurman, P. R., Barrett, S., Kawamoto, K., MacGlashan, J., Subramanian,
    K., Walsh, T. J., Capobianco, R., Devlic, A., Eckert, F., Fuchs, F., 等. (2022).
    用深度强化学习超越冠军级Gran Turismo赛车手。自然，602(7896):223–228。
- en: Xu et al., (2020) Xu, Z., van Hasselt, H. P., Hessel, M., Oh, J., Singh, S.,
    and Silver, D. (2020). Meta-gradient reinforcement learning with an objective
    discovered online. Advances in Neural Information Processing Systems, 33:15254–15264.
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等人，(2020) Xu, Z., van Hasselt, H. P., Hessel, M., Oh, J., Singh, S., 和 Silver,
    D. (2020). 带有在线发现目标的元梯度强化学习。神经信息处理系统进展，33:15254–15264。
- en: Xu et al., (2018) Xu, Z., van Hasselt, H. P., and Silver, D. (2018). Meta-gradient
    reinforcement learning. Advances in neural information processing systems, 31.
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等人，(2018) Xu, Z., van Hasselt, H. P., 和 Silver, D. (2018). 元梯度强化学习。神经信息处理系统进展，31。
- en: Ye et al., (2021) Ye, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y. (2021).
    Mastering atari games with limited data. Advances in Neural Information Processing
    Systems, 34:25476–25488.
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye等人，(2021) Ye, W., Liu, S., Kurutach, T., Abbeel, P., 和 Gao, Y. (2021). 用有限数据掌握Atari游戏。神经信息处理系统进展，34:25476–25488。
- en: Yin et al., (2023) Yin, H., YAN, S., and Xu, Z. (2023). Distributional meta-gradient
    reinforcement learning. In The Eleventh International Conference on Learning Representations.
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin等人，(2023) Yin, H., YAN, S., 和 Xu, Z. (2023). 分布式元梯度强化学习。在第十一届国际学习表征会议上。
- en: Zahavy et al., (2020) Zahavy, T., Xu, Z., Veeriah, V., Hessel, M., Oh, J., van
    Hasselt, H., Silver, D., and Singh, S. (2020). Self-tuning deep reinforcement
    learning. arXiv preprint arXiv:2002.12928.
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zahavy等人，(2020) Zahavy, T., Xu, Z., Veeriah, V., Hessel, M., Oh, J., van Hasselt,
    H., Silver, D., 和 Singh, S. (2020). 自调节深度强化学习。arXiv预印本arXiv:2002.12928。
- en: Zambaldi et al., (2018) Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li,
    Y., Babuschkin, I., Tuyls, K., Reichert, D., Lillicrap, T., Lockhart, E., et al.
    (2018). Relational deep reinforcement learning. arXiv preprint arXiv:1806.01830.
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zambaldi等人，(2018) Zambaldi, V., Raposo, D., Santoro, A., Bapst, V., Li, Y.,
    Babuschkin, I., Tuyls, K., Reichert, D., Lillicrap, T., Lockhart, E., 等. (2018).
    关系型深度强化学习。arXiv预印本arXiv:1806.01830。
- en: Zhang et al., (2020) Zhang, S., Veeriah, V., and Whiteson, S. (2020). Learning
    retrospective knowledge with reverse reinforcement learning. Advances in Neural
    Information Processing Systems, 33:19976–19987.
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2020) Zhang, S., Veeriah, V., 和 Whiteson, S. (2020). 通过反向强化学习学习回顾性知识。神经信息处理系统进展，第33卷，第19976–19987页。
- en: Zheng et al., (2022) Zheng, Q., Zhang, A., and Grover, A. (2022). Online decision
    transformer. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G.,
    and Sabato, S., editors, Proceedings of the 39th International Conference on Machine
    Learning, volume 162 of Proceedings of Machine Learning Research, pages 27042–27059\.
    PMLR.
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 (2022) Zheng, Q., Zhang, A., 和 Grover, A. (2022). 在线决策变换器。见 Chaudhuri,
    K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., 和 Sabato, S., 编辑，《第39届国际机器学习会议论文集》，机器学习研究论文集第162卷，第27042–27059页。PMLR。
- en: Zheng et al., (2020) Zheng, Z., Oh, J., Hessel, M., Xu, Z., Kroiss, M., van
    Hasselt, H., Silver, D., and Singh, S. (2020). What can learned intrinsic rewards
    capture? In International Conference on Machine Learning, pages 11436–11446\.
    PMLR.
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 (2020) Zheng, Z., Oh, J., Hessel, M., Xu, Z., Kroiss, M., van Hasselt,
    H., Silver, D., 和 Singh, S. (2020). 学到的内在奖励能捕捉什么？见国际机器学习会议论文集，第11436–11446页。PMLR。
- en: Zheng et al., (2018) Zheng, Z., Oh, J., and Singh, S. (2018). On learning intrinsic
    rewards for policy gradient methods. Advances in Neural Information Processing
    Systems, 31.
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 (2018) Zheng, Z., Oh, J., 和 Singh, S. (2018). 关于政策梯度方法的内在奖励学习。神经信息处理系统进展，第31卷。
- en: Zou et al., (2019) Zou, H., Ren, T., Yan, D., Su, H., and Zhu, J. (2019). Reward
    shaping via meta-learning. arXiv preprint arXiv:1901.09330.
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人 (2019) Zou, H., Ren, T., Yan, D., Su, H., 和 Zhu, J. (2019). 通过元学习进行奖励塑造。arXiv
    预印本 arXiv:1901.09330。
- en: A Further related works
  id: totrans-769
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步相关工作
- en: The literature also offers surveys on related topics. Liu et al., ([2022](#bib.bib104))
    review challenges and solutions of Goal-Conditioned Reinforcement Learning (GCRL),
    and Colas et al., ([2022](#bib.bib37)) follow to extend GCRL to Intrinsically
    Motivated Goal Exploration Process (IMGEP). Both these works are relevant for
    they generalise RL to multiple goals, but while goal-conditioning is a key ingredient
    of further arguments (see Section [4.2](#S4.SS2 "4.2 What is a goal? ‣ 4 Quantifying
    action influences ‣ A Survey of Temporal Credit Assignment in Deep Reinforcement
    Learning")), GCRL does not aim to address CAP directly. Barto and Mahadevan, ([2003](#bib.bib20));
    Al-Emran, ([2015](#bib.bib3)); Mendonca et al., ([2019](#bib.bib110)); Flet-Berliac,
    ([2019](#bib.bib50)); Pateria et al., ([2021](#bib.bib129)) survey Hierarchical
    Reinforcement Learning (HRL). HRL breaks down a long-term task into a hierarchical
    set of smaller sub-tasks, where each sub-task can be interpreted as an independent
    goal. However, despite sub-tasks providing intermediate, mid-way feedback that
    reduces the overall delay of effects that characterises the CAP, these works on
    HRL are limited to investigate the CAP only by decomposing the problem into smaller
    ones. Even in these cases, for example in the case of temporally abstract actions
    (Sutton et al.,, [1999](#bib.bib184)), sub-tasks either are not always well defined,
    or they require strong domain knowledge that might hinder generalisation.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中还提供了相关主题的综述。Liu 等人 ([2022](#bib.bib104)) 回顾了目标条件强化学习 (GCRL) 的挑战和解决方案，而 Colas
    等人 ([2022](#bib.bib37)) 则进一步将 GCRL 扩展到内在激励目标探索过程 (IMGEP)。这两项工作都相关，因为它们将 RL 推广到多个目标，但虽然目标条件化是进一步论证的关键成分（见第
    [4.2](#S4.SS2 "4.2 什么是目标？ ‣ 4 量化行动影响 ‣ 深度强化学习中的时间信用分配调查") 节），GCRL 并不直接旨在解决 CAP。Barto
    和 Mahadevan ([2003](#bib.bib20))；Al-Emran ([2015](#bib.bib3))；Mendonca 等人 ([2019](#bib.bib110))；Flet-Berliac
    ([2019](#bib.bib50))；Pateria 等人 ([2021](#bib.bib129)) 对层次化强化学习 (HRL) 进行了调查。HRL
    将长期任务分解为一组层次化的小任务，其中每个小任务可以被视为一个独立的目标。然而，尽管小任务提供了中间的反馈，减少了特征 CAP 的整体延迟，但这些关于 HRL
    的工作仅限于通过将问题分解为更小的问题来研究 CAP。即使在这些情况下，例如在时间抽象行动的情况下 (Sutton 等人，[1999](#bib.bib184))，小任务要么不总是明确定义，要么需要强大的领域知识，这可能会阻碍推广。
- en: B Further details on contexts
  id: totrans-771
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步的背景细节
- en: A contextual distribution defines a general mechanism to collect the contextual
    data $c$ (experience). For example, it can be a set of predefined demonstration,
    an MDP to actively query by interaction, or imaginary rollouts produced by an
    internal world model. This is a key ingredient of each method, together with its
    choice of action influence and the protocol to learn that from experience. Two
    algorithms can use the same action influence measure (e.g., (Klopf,, [1972](#bib.bib92))
    and (Goyal et al.,, [2019](#bib.bib56))), but specify different contextual distributions,
    resulting in two separate, often very different methods.
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文分布定义了一个通用机制来收集上下文数据 $c$（经验）。例如，它可以是一个预定义的演示集，一个通过交互主动查询的MDP，或由内部世界模型产生的虚拟回合。这是每种方法的关键组成部分，结合其行动影响的选择和从经验中学习的协议。两个算法可以使用相同的行动影响度量（例如，（Klopf,,
    [1972](#bib.bib92)）和（Goyal et al.,, [2019](#bib.bib56)）），但指定不同的上下文分布，从而导致两种不同的、通常非常不同的方法。
- en: Formally, we represent a context as a distribution over some contextual data
    $C\sim\mathbb{P}_{C}(C)$, where $C$ is the context, and $\mathbb{P}_{C}$ is the
    distribution induced by a specific choice of source. Our main reference for the
    classification of contextual distributions is the ladder of causality (Pearl,,
    [2009](#bib.bib131); Bareinboim et al.,, [2022](#bib.bib17)), seeing, doing, imagining,
    and we define our three classes accordingly.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，我们将上下文表示为某些上下文数据的分布 $C\sim\mathbb{P}_{C}(C)$，其中 $C$ 是上下文，$\mathbb{P}_{C}$
    是由特定的源选择引起的分布。我们对上下文分布分类的主要参考是因果层次（Pearl,, [2009](#bib.bib131); Bareinboim et
    al.,, [2022](#bib.bib17)），即观察、操作、想象，我们据此定义了我们的三类。
- en: Observational distributions
  id: totrans-774
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 观察性分布
- en: 'are distributions over a predefined set of data, and we denote it with $\mathbb{P}_{obs}(C)$.
    Here, the agent has only access to passive set of experience collection from a
    (possibly unknown) environment. It cannot intervene or affect the environment
    in any way, but it must learn from the data that is available: it cannot explore.
    This is the typical case of offline CA methods or methods that learn from demonstrations
    (Chen et al.,, [2021](#bib.bib33)), where the context is a fixed dataset of trajectories.
    The agent can sample from $\mathbb{P}_{obs}$ uniformly at random or with forms
    of prioritisation ([Schaul et al., 2015b,](#bib.bib154) ; [Jiang et al., 2021a,](#bib.bib81)
    ). Observational distributions allow assigning credit efficiently and safely since
    they do not require direct interactions with the environment and can ignore the
    burden of either waiting for the environment to respond or getting stuck into
    irreversible states (Grinsztajn et al.,, [2021](#bib.bib60)). However, they can
    be limited both in the amount of information they can provide and in the overall
    coverage of the space of associations between actions and outcomes, often failing
    to generalise to unobserved associations (Kirk et al.,, [2023](#bib.bib89)).'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个预定义数据集上的分布，我们用 $\mathbb{P}_{obs}(C)$ 表示。这里，代理只能访问来自（可能未知的）环境的被动经验集合。它不能以任何方式干预或影响环境，但必须从可用的数据中学习：它不能进行探索。这是离线CA方法或从演示中学习的方法（Chen
    et al.,, [2021](#bib.bib33)）的典型情况，其中上下文是一个固定的轨迹数据集。代理可以从 $\mathbb{P}_{obs}$ 中均匀随机抽样或以优先级形式抽样（[Schaul
    et al., 2015b,](#bib.bib154)；[Jiang et al., 2021a,](#bib.bib81)）。观察性分布允许高效且安全地分配信用，因为它们不需要直接与环境互动，可以忽略等待环境响应或陷入不可逆状态的负担（Grinsztajn
    et al.,, [2021](#bib.bib60)）。然而，它们在提供信息的数量和对行动与结果之间关联空间的整体覆盖方面可能有限，往往难以推广到未观察到的关联（Kirk
    et al.,, [2023](#bib.bib89)）。
- en: Interactive distributions
  id: totrans-776
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 交互性分布
- en: 'are distributions defined by active interactions with an environment, and we
    denote them with $\mathbb{P}_{\mu,\pi}$. Here, the agent can actively intervene
    to control the environment through the policy, which defines a distribution over
    trajectories, $D\sim\mathbb{P}_{\mu,\pi}$. This is the typical case of model-free,
    online CA methods (Arjona-Medina et al.,, [2019](#bib.bib7); Harutyunyan et al.,,
    [2019](#bib.bib67)), where the source is the interface of interaction between
    the agent and the environment. Interactive distributions allow the agent to make
    informed decisions about which experience to collect (Amin et al.,, [2021](#bib.bib4))
    because the space of associations between actions and outcomes is under the direct
    control of the agent: they allow exploration. One interesting use of these distributions
    is to define outcomes in hindsight, that is, by unrolling the policy in the environment
    with a prior objective and then considering a different goal from the resulting
    trajectory (Andrychowicz et al.,, [2017](#bib.bib5)). Interactive distributions
    provide greater information than observational ones but may be more expensive
    to query, they do not allow to specify all queries, such as starting from a specific
    state or crossing the MDP backwards, and they might lead to irreversible outcomes
    with safety concerns (García et al.,, [2015](#bib.bib54)).'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 是由与环境的主动互动定义的分布，我们用$\mathbb{P}_{\mu,\pi}$表示。在这里，代理可以通过策略主动干预以控制环境，该策略定义了轨迹的分布$D\sim\mathbb{P}_{\mu,\pi}$。这是无模型的在线CA方法的典型情况（Arjona-Medina
    et al., [2019](#bib.bib7); Harutyunyan et al., [2019](#bib.bib67)），其中源是代理与环境之间的互动接口。互动分布允许代理对收集哪些经验做出知情决策（Amin
    et al., [2021](#bib.bib4)），因为动作与结果之间的关联空间在代理的直接控制下：它们允许探索。这些分布的一个有趣用途是从事后定义结果，即通过在环境中展开策略以实现先前目标，然后从结果轨迹中考虑不同的目标（Andrychowicz
    et al., [2017](#bib.bib5)）。互动分布提供比观察性分布更多的信息，但可能查询成本更高，它们不允许指定所有查询，如从特定状态开始或逆向穿越MDP，并且可能导致有安全隐患的不可逆结果（García
    et al., [2015](#bib.bib54)）。
- en: Hypothetical distributions
  id: totrans-778
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 假设分布
- en: are distributions defined by functions internal to the agent, and we denote
    them with $\mathbb{P}_{\widetilde{\mu},\pi}$, where $\widetilde{\mu}$ is the agent’s
    internal state-transition dynamic function (learned). They represent potential
    scenarios, futures or pasts, that do not correspond to actual data collected from
    the real environment. The agent can query the space of associations surgically
    and explore a broader space of possible outcomes for a given action without having
    to interact with the environment. In short, it can imagine a hypothetical scenario,
    and reason about what would have happened if the agent had taken a different action.
    Hypothetical distributions enable counterfactual reasoning, that is, to reason
    about what would have happened if the agent had taken a different action in a
    given situation. Crucially, they allow navigating the MDP independently of the
    arrow of time, and, for example, pause the process of generating a trajectory,
    revert to a previous state, and then continue the trajectory from that point.
    However, they can produce a paradoxical situation in which the agent explores
    a region of space with high uncertainty, but relies on a world model that, because
    of that uncertainty is not very accurate (Guez et al.,, [2020](#bib.bib61)).
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 是由代理内部函数定义的分布，我们用$\mathbb{P}_{\widetilde{\mu},\pi}$表示，其中$\widetilde{\mu}$是代理的内部状态转移动态函数（学习得到的）。它们代表潜在的情景、未来或过去，不对应于从真实环境中收集的实际数据。代理可以对关联空间进行精确查询，并探索给定动作的更广泛可能结果，而无需与环境交互。简言之，它可以想象一个假设情景，并推理如果代理采取了不同的行动会发生什么。假设分布支持反事实推理，即推理如果代理在特定情况下采取了不同的行动会发生什么。关键是，它们允许在不受时间箭头影响的情况下导航MDP，例如，可以暂停生成轨迹的过程，回到之前的状态，然后从那个点继续轨迹。然而，它们可能会产生一种矛盾的情况，即代理在一个高不确定性的空间中探索，但依赖于由于这种不确定性而不够准确的世界模型（Guez
    et al., [2020](#bib.bib61)）。
- en: B.1 Representing a context
  id: totrans-780
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 表示上下文
- en: 'Since equation ([4](#S4.E4 "In Definition 2 (Assignment). ‣ 4.3 What is an
    assignment? ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment
    in Deep Reinforcement Learning")) includes a context as an input a natural question
    arises, “How to represent contexts?”. Recall that the purpose of the context is
    to two-fold: a)to unambiguously determine the current present as much as possible,
    and b) to convey information about the distribution of actions that will be taken
    after the action we aim to evaluate. Section [4.3](#S4.SS3 "4.3 What is an assignment?
    ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning") details the reasons of the choice. In many action
    influence measures (see Section [4.5](#S4.SS5 "4.5 Existing assignment functions
    ‣ 4 Quantifying action influences ‣ A Survey of Temporal Credit Assignment in
    Deep Reinforcement Learning")), such as $q$-values or advantage, the context is
    only the state of an MDP, or a history if we are solving a POMDP instead. In this
    case representing the context is the problem of representing a state, which is
    widely discussed in literature. Notice that this is not about learning a state
    representation, but rather about specifying the shape of a state when constructing
    and defining an MDP or an observation and an action for a POMDP. These portion
    of the input addresses the first purpose of a context.'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 由于方程式（[4](#S4.E4 "在定义 2（分配）。 ‣ 4.3 什么是分配？ ‣ 4 量化行动影响 ‣ 深度强化学习中的时间信用分配概述")）将上下文作为输入，因此自然会产生一个问题：“如何表示上下文？”请记住，上下文的目的有两个方面：a）尽可能明确地确定当前状态，以及
    b）传达关于在我们旨在评估的行动之后将采取的行动分布的信息。第 [4.3](#S4.SS3 "4.3 什么是分配？ ‣ 4 量化行动影响 ‣ 深度强化学习中的时间信用分配概述")
    节详细说明了选择的原因。在许多行动影响度量（见第 [4.5](#S4.SS5 "4.5 现有分配函数 ‣ 4 量化行动影响 ‣ 深度强化学习中的时间信用分配概述")
    节），例如 $q$-值或优势，上下文仅是 MDP 的状态，或者如果我们解决的是 POMDP，则是历史。在这种情况下，表示上下文就是表示状态的问题，这在文献中有广泛讨论。请注意，这不是关于学习状态表示，而是关于在构建和定义
    MDP 或 POMDP 的观察和行动时指定状态的形状。这些输入部分解决了上下文的第一个目的。
- en: To fulfil its second function the context may contain additional objects and
    here we discuss only the documented cases, rather than proposing a theoretical
    generalisation. When the additional input is a policy (Harb et al.,, [2020](#bib.bib66);
    Faccio et al.,, [2021](#bib.bib43)), then the problem turns to how to represent
    that specific object. In the specific case of a policy Harb et al., ([2020](#bib.bib66))
    and Faccio et al., ([2021](#bib.bib43)) propose two different methods of representing
    a policy. In other cases, future actions are specified using a full trajectory,
    or a feature of it, and in this case the evaluation happens in hindsight. As for
    policies, the problem turns to representing this additional portion of the context.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现其第二个功能，上下文可能包含附加对象，这里我们只讨论已记录的案例，而不是提出理论上的概括。当附加输入是一个政策（Harb 等，[2020](#bib.bib66);
    Faccio 等，[2021](#bib.bib43)）时，问题转变为如何表示该特定对象。在政策的具体情况下，Harb 等（[2020](#bib.bib66)）和
    Faccio 等（[2021](#bib.bib43)）提出了两种不同的政策表示方法。在其他情况下，未来的行动通过完整的轨迹或其特征来指定，此时评估发生在事后。至于政策，问题则转为如何表示上下文中的这一附加部分。
