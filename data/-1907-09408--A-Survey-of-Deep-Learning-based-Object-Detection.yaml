- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:05:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:05:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1907.09408] A Survey of Deep Learning-based Object Detection'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1907.09408] 深度学习基础的目标检测综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1907.09408](https://ar5iv.labs.arxiv.org/html/1907.09408)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1907.09408](https://ar5iv.labs.arxiv.org/html/1907.09408)
- en: A Survey of Deep Learning-based Object Detection
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习基础的目标检测综述
- en: 'Licheng Jiao,  Fan Zhang, Fang Liu,  Shuyuan Yang,  Lingling Li,  Zhixi Feng, 
    and Rong Qu Key Laboratory of Intelligent Perception and Image Understanding of
    Ministry of Education, International Research Center for Intelligent Perception
    and Computation, Joint International Research Laboratory of Intelligent Perception
    and Computation, School of Artificial Intelligence, Xidian University, Xian, Shaanxi
    Province 710071, China e-mail: (lchjiao@mail.xidian.edu.cn).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Licheng Jiao, Fan Zhang, Fang Liu, Shuyuan Yang, Lingling Li, Zhixi Feng 和 Rong
    Qu 关键实验室：教育部智能感知与图像理解重点实验室，国际智能感知与计算研究中心，智能感知与计算联合国际研究实验室，西安电子科技大学人工智能学院，中国陕西省西安710071，电子邮件：（lchjiao@mail.xidian.edu.cn）。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Object detection is one of the most important and challenging branches of computer
    vision, which has been widely applied in people’s life, such as monitoring security,
    autonomous driving and so on, with the purpose of locating instances of semantic
    objects of a certain class. With the rapid development of deep learning networks
    for detection tasks, the performance of object detectors has been greatly improved.
    In order to understand the main development status of object detection pipeline,
    thoroughly and deeply, in this survey, we first analyze the methods of existing
    typical detection models and describe the benchmark datasets. Afterwards and primarily,
    we provide a comprehensive overview of a variety of object detection methods in
    a systematic manner, covering the one-stage and two-stage detectors. Moreover,
    we list the traditional and new applications. Some representative branches of
    object detection are analyzed as well. Finally, we discuss the architecture of
    exploiting these object detection methods to build an effective and efficient
    system and point out a set of development trends to better follow the state-of-the-art
    algorithms and further research.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是计算机视觉中最重要和最具挑战性的分支之一，已广泛应用于人们的生活中，如安全监控、自动驾驶等，其目的是定位特定类别的语义对象实例。随着深度学习网络在检测任务中的快速发展，目标检测器的性能得到了极大提高。为了深入了解目标检测管道的主要发展状态，在本综述中，我们首先分析了现有典型检测模型的方法，并描述了基准数据集。之后，我们主要以系统化的方式提供了各种目标检测方法的全面概述，涵盖了一阶段和二阶段检测器。此外，我们列出了传统和新兴应用，并分析了一些具有代表性的目标检测分支。最后，我们讨论了利用这些目标检测方法构建有效且高效系统的架构，并指出了一系列发展趋势，以更好地跟踪最先进的算法并进一步研究。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Classification, deep learning, localization, object detection, typical pipelines.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 分类、深度学习、定位、目标检测、典型管道。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Object detection has been attracting increasing amounts of attention in recent
    years due to its wide range of applications and recent technological breakthroughs.
    This task is under extensive investigation in both academia and real world applications,
    such as monitoring security, autonomous driving, transportation surveillance,
    drone scene analysis, and robotic vision. Among many factors and efforts that
    lead to the fast evolution of object detection techniques, notable contributions
    should be attributed to the development of deep convolution neural networks and
    GPUs computing power. At present, deep learning model has been widely adopted
    in the whole field of computer vision, including general object detection and
    domain-specific object detection. Most of the state-of-the-art object detectors
    utilize deep learning networks as their backbone and detection network to extract
    features from input images (or videos), classification and localization respectively.
    Object detection is a computer technology related to computer vision and image
    processing which deals with detecting instances of semantic objects of a certain
    class (such as humans, buildings, or cars) in digital images and videos. Well-researched
    domains of object detection include multi-categories detection, edge detection,
    salient object detection, pose detection, scene text detection, face detection,
    and pedestrian detection etc. As an important part of scene understanding, object
    detection has been widely used in many fields of modern life, such as security
    field, military field, transportation field, medical field and life field. Furthermore,
    many benchmarks have played an important role in object detection field so far,
    such as Caltech [[1](#bib.bib1)], KITTI [[2](#bib.bib2)], ImageNet [[3](#bib.bib3)],
    PASCAL VOC [[4](#bib.bib4)], MS COCO [[5](#bib.bib5)], and Open Images V5 [[6](#bib.bib6)].
    In ECCV VisDrone 2018 contest, organizers have released a novel drone platform-based
    dataset [[7](#bib.bib7)] which contains a large amount of images and videos.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对象检测因其广泛的应用范围和近期的技术突破而越来越受到关注。该任务在学术界和实际应用中都在进行广泛研究，例如监控安全、自动驾驶、交通监控、无人机场景分析和机器人视觉。在推动对象检测技术快速发展的众多因素和努力中，值得注意的贡献应归因于深度卷积神经网络和GPU计算能力的发展。目前，深度学习模型已被广泛应用于计算机视觉领域，包括一般对象检测和领域特定对象检测。大多数最先进的对象检测器利用深度学习网络作为其骨干网络和检测网络，从输入图像（或视频）中提取特征，进行分类和定位。对象检测是一种与计算机视觉和图像处理相关的计算机技术，处理在数字图像和视频中检测某一类别（如人类、建筑物或汽车）的语义对象实例。经过深入研究的对象检测领域包括多类别检测、边缘检测、显著对象检测、姿态检测、场景文本检测、人脸检测和行人检测等。作为场景理解的重要组成部分，对象检测已广泛应用于现代生活的许多领域，如安全领域、军事领域、交通领域、医疗领域和生活领域。此外，到目前为止，许多基准在对象检测领域发挥了重要作用，如Caltech
    [[1](#bib.bib1)]、KITTI [[2](#bib.bib2)]、ImageNet [[3](#bib.bib3)]、PASCAL VOC [[4](#bib.bib4)]、MS
    COCO [[5](#bib.bib5)]和Open Images V5 [[6](#bib.bib6)]。在ECCV VisDrone 2018竞赛中，组织者发布了一个基于无人机平台的全新数据集
    [[7](#bib.bib7)]，该数据集包含大量图像和视频。
- en: $\bullet$ Two kinds of object detectors
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 两种对象检测器
- en: Pre-existing domain-specific image object detectors usually can be divided into
    two categories, the one is two-stage detector, the most representative one, Faster
    R-CNN [[8](#bib.bib8)]. The other is one-stage detector, such as YOLO [[9](#bib.bib9)],
    SSD [[10](#bib.bib10)]. Two-stage detectors have high localization and object
    recognition accuracy, whereas the one-stage detectors achieve high inference speed.
    The two stages of two-stage detectors can be divided by RoI (Region of Interest)
    pooling layer. For instance, in Faster R-CNN, the first stage, called RPN, a Region
    Proposal Network, proposes candidate object bounding boxes. The second stage,
    features are extracted by RoIPool (RoI Pooling) operation from each candidate
    box for the following classification and bounding-box regression tasks [[11](#bib.bib11)].
    Fig.1 (a) shows the basic architecture of two-stage detectors. Furthermore, the
    one-stage detectors propose predicted boxes from input images directly without
    region proposal step, thus they are time efficient and can be used for real-time
    devices. Fig.1 (b) exhibits the basic architecture of one-stage detectors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的领域特定图像目标检测器通常可以分为两类，一类是两阶段检测器，最具代表性的是 Faster R-CNN [[8](#bib.bib8)]。另一类是单阶段检测器，如
    YOLO [[9](#bib.bib9)]、SSD [[10](#bib.bib10)]。两阶段检测器具有较高的定位和目标识别准确性，而单阶段检测器则实现了较高的推断速度。两阶段检测器的两个阶段可以通过
    RoI（兴趣区域）池化层来划分。例如，在 Faster R-CNN 中，第一个阶段是 RPN（区域建议网络），它提出候选目标边界框。第二个阶段，通过 RoIPool（RoI
    池化）操作从每个候选框中提取特征，用于后续的分类和边界框回归任务 [[11](#bib.bib11)]。图1 (a) 显示了两阶段检测器的基本结构。此外，单阶段检测器直接从输入图像中提出预测框，无需区域提议步骤，因此它们具有时间效率高的优点，可用于实时设备。图1
    (b) 展示了单阶段检测器的基本结构。
- en: $\bullet$ Contributions
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 贡献
- en: This survey focuses on describing and analyzing deep learning based object detection
    task. The existing surveys always cover a series of domain of general object detection
    and may not contain state-of-the-art methods which provide some novel solutions
    and newly directions of these tasks due of the rapid development of computer vision
    research.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述重点描述和分析基于深度学习的目标检测任务。现有的综述通常涵盖一般目标检测的系列领域，可能不包含提供一些新颖解决方案和这些任务的新方向的最先进方法，因为计算机视觉研究的快速发展。
- en: (1) This paper lists very novel solutions proposed recently but neglects to
    discuss the basics so that readers can see the cutting edge of the field more
    easily.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 本文列出了最近提出的非常新颖的解决方案，但忽略了基础知识的讨论，以便读者能够更容易地看到该领域的前沿。
- en: (2) Moreover, different from previous object detection surveys, this paper systematically
    and comprehensively reviews deep learning based object detection methods and most
    importantly the up to date detection solutions and a set of significant research
    trends as well.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 此外，与以往的目标检测综述不同，本文系统全面地回顾了基于深度学习的目标检测方法，最重要的是更新的检测解决方案以及一系列重要的研究趋势。
- en: (3) This survey is featured by in-depth analysis and discussion in various aspects,
    many of which, to the best of our knowledge, are the first time in this field.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 本综述以对各个方面进行深入分析和讨论为特色，我们认为其中许多方面在该领域是首次出现的。
- en: Above all, it is our intention to provide an overview how different deep learning
    methods are used rather than a full summary of all related papers. To get into
    this field, we recommend readers refer to [[12](#bib.bib12)] [[13](#bib.bib13)]
    [[14](#bib.bib14)] for more details of early methods.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，我们的意图是提供不同深度学习方法的概述，而不是对所有相关论文的全面总结。为了深入了解该领域，我们建议读者参考 [[12](#bib.bib12)]
    [[13](#bib.bib13)] [[14](#bib.bib14)] 以获取早期方法的更多细节。
- en: The rest of this paper is organized as follows. Object detectors need a powerful
    backbone network to extract rich features. This paper discusses backbone networks
    in section 2 below. As is known to all, the typical pipelines of domain-specific
    image detectors act as basics and milestone of the task. In section 3, this paper
    elaborates the most representative and pioneering deep learning-based approaches
    proposed before June 2019\. Section 4 describes common used datasets and metrics.
    Section 5 systematically explains the analysis of general object detection methods.
    Section 6 details five typical fields and several popular branches of object detection.
    The development trend is summarized in section 7.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。对象检测器需要一个强大的骨干网络来提取丰富的特征。本文在下面的第2节讨论了骨干网络。众所周知，特定领域图像检测器的典型流程是任务的基础和里程碑。第3节详细阐述了2019年6月之前提出的最具代表性和开创性的基于深度学习的方法。第4节描述了常用的数据集和指标。第5节系统地解释了通用对象检测方法的分析。第6节详细介绍了五个典型领域和若干流行的对象检测分支。第7节总结了发展趋势。
- en: '![Refer to caption](img/84e047b6ba77b62920da15cd58b7a793.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/84e047b6ba77b62920da15cd58b7a793.png)'
- en: 'Figure 1: (a) exhibits the basic architecture of two-stage detectors, which
    consists of region proposal network to feed region proposals into classifier and
    regressor. (b) shows the basic architecture of one-stage detectors, which predicts
    bounding boxes from input images directly. Yellow cubes are a series of convolutional
    layers (called a block) with the same resolution in backbone network, because
    of down-sampling operation after one block, the size of the following cubes gradually
    becoming small. Thick blue cubes are a series of convolutional layers contain
    one or more convolutional layers. The flat blue cube demonstrates the RoI pooling
    layer which generates feature maps for objects of the same size.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1： (a) 展示了两阶段检测器的基本架构，它由区域提议网络组成，将区域提议输入分类器和回归器。 (b) 展示了一阶段检测器的基本架构，它直接从输入图像中预测边界框。黄色立方体是一系列具有相同分辨率的卷积层（称为一个块），由于在一个块后的下采样操作，后续立方体的大小逐渐变小。厚蓝色立方体是一系列包含一个或多个卷积层的卷积层。平坦的蓝色立方体表示RoI池化层，它为相同大小的对象生成特征图。
- en: II Backbone networks
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 骨干网络
- en: Backbone network is acting as the basic feature extractor for object detection
    task which takes images as input and outputs feature maps of the corresponding
    input image. Most of backbone networks for detection are the network for classification
    task taking out the last fully connected layers. The improved version of basic
    classification network is also available. For instance, Lin et al. [[15](#bib.bib15)]
    add or subtract layers or replace some layers with special designed layers. To
    better meet specific requirements, some works [[9](#bib.bib9)] [[16](#bib.bib16)]
    utilize the newly designed backbone for feature extraction.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 骨干网络作为对象检测任务的基本特征提取器，接受图像作为输入，并输出对应输入图像的特征图。大多数检测骨干网络是分类任务的网络，通过去除最后的全连接层来实现的。基本分类网络的改进版本也可用。例如，Lin等人[[15](#bib.bib15)]添加或减去层或用特殊设计的层替换一些层。为了更好地满足特定要求，一些工作[[9](#bib.bib9)]
    [[16](#bib.bib16)]利用新设计的骨干网络进行特征提取。
- en: Towards different requirements about accuracy vs. efficiency, people can choose
    deeper and densely connected backbones, like ResNet [[11](#bib.bib11)], ResNeXt
    [[17](#bib.bib17)], AmoebaNet [[18](#bib.bib18)] or lightweight backbones like
    MobileNet [[19](#bib.bib19)], ShuffleNet [[20](#bib.bib20)], SqueezeNet [[21](#bib.bib21)],
    Xception [[22](#bib.bib22)], MobileNetV2 [[23](#bib.bib23)]. When applied to mobile
    devices, lightweight backbones can meet the requirements. Wang et al. [[24](#bib.bib24)]
    propose a novel real-time object detection system by combining PeleeNet with SSD
    [[10](#bib.bib10)] and optimizing the architecture for fast processing speed.
    In order to meet the needs of high precision and more accurate applications, complex
    backbones are needed. On the other hand, real-time acquirements like video or
    webcam require not only high processing speed but high accuracy [[9](#bib.bib9)],
    which need well-designed backbone to adapt to the detection architecture and make
    a trade-off between speed and accuracy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 针对不同的精度与效率需求，人们可以选择更深且密集连接的骨干网络，如ResNet [[11](#bib.bib11)]、ResNeXt [[17](#bib.bib17)]、AmoebaNet
    [[18](#bib.bib18)]，或像MobileNet [[19](#bib.bib19)]、ShuffleNet [[20](#bib.bib20)]、SqueezeNet
    [[21](#bib.bib21)]、Xception [[22](#bib.bib22)]、MobileNetV2 [[23](#bib.bib23)]这样的轻量级骨干网络。当应用于移动设备时，轻量级骨干网络可以满足要求。Wang等人
    [[24](#bib.bib24)] 提出了一个新颖的实时物体检测系统，通过将PeleeNet与SSD [[10](#bib.bib10)] 结合，并优化架构以提高处理速度。为了满足高精度和更准确应用的需求，需要复杂的骨干网络。另一方面，实时要求如视频或网络摄像头不仅需要高处理速度，还需要高精度
    [[9](#bib.bib9)]，这需要设计良好的骨干网络以适应检测架构，并在速度和精度之间进行权衡。
- en: To explore more competitive detecting accuracy, deeper and densely connected
    backbone is adopted to replace the shallower and sparse connected counterpart.
    He et al. [[11](#bib.bib11)] utilize ResNet [[25](#bib.bib25)] rather than VGG
    [[26](#bib.bib26)] to capture rich features which is adopted in Faster R-CNN [[8](#bib.bib8)]
    for further accuracy gain because of its high capacity.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索更具竞争力的检测精度，采用更深且密集连接的骨干网络替代较浅且稀疏连接的网络。He等人 [[11](#bib.bib11)] 使用ResNet [[25](#bib.bib25)]
    而非VGG [[26](#bib.bib26)] 来捕捉丰富特征，这在Faster R-CNN [[8](#bib.bib8)] 中被采用以进一步提升精度，因为其具有高容量。
- en: The newly high performance classification networks can improve precision and
    reduce the complexity of object detection task. This is an effective way to further
    improve network performance because the backbone network acts as a feature extractor.
    As is known to all, the quality of features determines the upper bound of network
    performance, thus it is an important step that needs further exploration. Please
    refer to [[27](#bib.bib27)] for more details.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 新兴的高性能分类网络可以提高精度并减少物体检测任务的复杂性。这是一种有效的方式来进一步提升网络性能，因为骨干网络作为特征提取器。众所周知，特征的质量决定了网络性能的上限，因此这是一个需要进一步探索的重要步骤。更多细节请参考
    [[27](#bib.bib27)]。
- en: III Typical baselines
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 典型基准
- en: With the development of deep learning and the continuous improvement of computing
    power, great progress has been made in the field of general object detection.
    When the first CNN-based object detector R-CNN was proposed, a series of significant
    contributions have been made which promote the development of general object detection
    by a large margin. We introduce some representative object detection architectures
    for beginners to get started in this domain.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的发展和计算能力的不断提升，一般物体检测领域取得了巨大进展。当第一个基于CNN的物体检测器R-CNN被提出时，产生了一系列显著的贡献，这些贡献大大推动了通用物体检测的发展。我们介绍了一些代表性的物体检测架构，以帮助初学者入门这一领域。
- en: III-A Two-stage Detectors
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 两阶段检测器
- en: III-A1 R-CNN
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 R-CNN
- en: R-CNN is a region based CNN detector. As Girshick et al. [[28](#bib.bib28)]
    propose R-CNN which can be used in object detection tasks, their works are the
    first to show that a CNN could lead to dramatically higher object detection performance
    on PASCAL VOC datasets [[4](#bib.bib4)] than those systems based on simpler HOG-like
    features. Deep learning method is verified effective and efficient in the field
    of object detection.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN是基于区域的CNN检测器。正如Girshick等人 [[28](#bib.bib28)] 提出的R-CNN可以用于物体检测任务，他们的工作首次展示了CNN在PASCAL
    VOC数据集 [[4](#bib.bib4)] 上能够显著提高物体检测性能，相比于基于简单HOG-like特征的系统。深度学习方法在物体检测领域被验证为有效且高效的。
- en: R-CNN detector consists of four modules. The first module generates category-independent
    region proposals. The second module extracts a fixed-length feature vector from
    each region proposal. The third module is a set of class-specific linear SVMs
    to classify the objects in one image. The last module is a bounding-box regressor
    for precisely bounding-box prediction. For detailed, first, to generate region
    proposals, the authors adopt selective search method. Then, a CNN is used to extract
    a 4096-dimensional feature vector from each region proposal. Because the fully
    connected layer needs input vectors of fixed length, the region proposal features
    should have the same size. The authors adopt a fixed ${227}\times{227}$ pixel
    as the input size of CNN. As we know, the objects in various images have different
    size and aspect ratio, which makes the region proposals extracted by the first
    module different in size. Regardless of the size or aspect ratio of the candidate
    region, the authors warp all pixels in a tight bounding box around it to the required
    size ${227}\times{227}$. The feature extraction network consists of five convolutional
    layers and two fully connected layers. And all CNN parameters are shared across
    all categories. Each category trains category-independent SVM which does not share
    parameters between different SVMs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN探测器由四个模块组成。第一个模块生成类别无关的区域提议。第二个模块从每个区域提议中提取固定长度的特征向量。第三个模块是一组特定类别的线性SVM，用于对图像中的对象进行分类。最后一个模块是一个边界框回归器，用于精确预测边界框。具体而言，首先，生成区域提议时，作者采用选择性搜索方法。然后，使用CNN从每个区域提议中提取一个4096维的特征向量。由于全连接层需要固定长度的输入向量，因此区域提议特征应具有相同的大小。作者采用固定的${227}\times{227}$像素作为CNN的输入大小。众所周知，不同图像中的对象具有不同的大小和纵横比，这使得第一模块提取的区域提议大小不同。无论候选区域的大小或纵横比如何，作者将所有像素在其周围的紧密边界框内变形为所需的大小${227}\times{227}$。特征提取网络由五个卷积层和两个全连接层组成，并且所有CNN参数在所有类别之间共享。每个类别训练类别无关的SVM，不同SVM之间的参数不共享。
- en: 'Pre-training on larger dataset followed by fine-tuning on the specified dataset
    is a good training method for deep convolutional neural networks to achieve fast
    convergence. First, Girshick et al. [[28](#bib.bib28)] pre-train the CNN on a
    large scale dataset (ImageNet classification dataset [[3](#bib.bib3)]). The last
    fully connected layer is replaced by the CNN’s ImageNet specific 1000-way classification
    layer. The next step is to use SGD (stochastic gradient descent) to fine-tune
    the CNN parameters on the warped proposal windows. The last fully connected layer
    is a (N+1)-way classification layer (N: object classes, 1: background) which is
    randomly initialized.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在更大的数据集上预训练，然后在指定的数据集上微调，是深度卷积神经网络实现快速收敛的良好训练方法。首先，Girshick等人[[28](#bib.bib28)]在大规模数据集（ImageNet分类数据集[[3](#bib.bib3)]）上预训练CNN。最后一个全连接层被替换为CNN的ImageNet特定1000类分类层。下一步是使用SGD（随机梯度下降）微调CNN参数，针对变形提议窗口。最后一个全连接层是一个（N+1）类分类层（N：对象类别，1：背景），其初始化为随机状态。
- en: When setting positive examples and negative examples the authors divide into
    two situations. The first is to define the IoU (intersection over union) overlap
    threshold as 0.5 in the process of fine-tuning. Below the threshold, region proposals
    are defined as negatives while above it object proposals are defined as positives.
    As well, the object proposals whose maximum IoU overlap with a ground-truth class
    are assigned to the ground-truth box. Another situation is to set parameters when
    training SVM. In contrast, only the ground-truth boxes are taken as positive examples
    for their respective classes and proposals have less than 0.3 IoU overlap with
    all ground-truth instances of one class as a negative proposal for that class.
    These proposals with overlap between 0.5 and 1 and they are not ground truth,
    which expand the number of positive examples by approximately ${30}\times$. Therefore
    such a big set can avoid overfitting during fine-tuning process effectively.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在设定正负样本时，作者将其分为两种情况。第一种是在微调过程中将IoU（交并比）重叠阈值定义为0.5。在阈值以下，区域提议被定义为负样本，而在阈值以上，对象提议被定义为正样本。此外，与真实类别的最大IoU重叠的对象提议会被分配给真实框。另一种情况是在训练SVM时设置参数。相比之下，只有真实框被作为其各自类别的正样本，而与该类别所有真实实例IoU重叠小于0.3的提议被视为负样本。这些提议的重叠在0.5到1之间，并且它们不是真实的，这样可以将正样本的数量扩展约${30}\times$。因此，这样一个大集合可以有效地避免微调过程中的过拟合。
- en: III-A2 Fast R-CNN
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 Fast R-CNN
- en: R-CNN proposed a year later, Ross Girshick [[29](#bib.bib29)] proposed a faster
    version of R-CNN, called Fast R-CNN [[29](#bib.bib29)]. Because R-CNN performs
    a ConvNet forward pass for each region proposal without sharing computation, R-CNN
    takes a long time on SVMs classification. Fast R-CNN extracts features from an
    entire input image and then passes the region of interest (RoI) pooling layer
    to get the fixed size features as the input of the following classification and
    bounding box regression fully connected layers. The features are extracted from
    the entire image once and are sent to CNN for classification and localization
    at a time. Compared to R-CNN which inputs each region proposals to CNN, a large
    amount of time can be saved for CNN processing and large disk storage to store
    a great deal of features can be saved either in Fast R-CNN. As mentioned above,
    training R-CNN is a multi-stage process which covers pre-training stage, fine-tuning
    stage, SVMs classification stage and bounding box regression stage. Fast R-CNN
    is a one-stage end-to-end training process using a multi-task loss on each labeled
    RoI to jointly train the network.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN 于一年后提出，Ross Girshick [[29](#bib.bib29)] 提出了一个更快的 R-CNN 版本，称为 Fast R-CNN
    [[29](#bib.bib29)]。由于 R-CNN 对每个区域提议都执行一次 ConvNet 前向传递且没有共享计算，R-CNN 在 SVM 分类上花费了较长时间。Fast
    R-CNN 从整个输入图像中提取特征，然后通过区域兴趣 (RoI) 池化层获取固定大小的特征作为后续分类和边界框回归全连接层的输入。特征从整个图像中提取一次，并同时发送到
    CNN 进行分类和定位。与 R-CNN 将每个区域提议输入 CNN 相比，Fast R-CNN 可以节省大量 CNN 处理时间，并节省存储大量特征所需的大量磁盘存储。如上所述，训练
    R-CNN 是一个多阶段过程，包括预训练阶段、微调阶段、SVM 分类阶段和边界框回归阶段。Fast R-CNN 是一个一阶段的端到端训练过程，使用多任务损失对每个标记的
    RoI 进行联合训练。
- en: Another improvement is that Fast R-CNN uses a RoI pooling layer to extract a
    fixed size feature map from region proposals of different size. This operation
    with no need of warping regions and reserves the spatial information of features
    of region proposals. For fast detection, the author uses truncated SVD which accelerates
    the forward pass of computing the fully connected layers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个改进是 Fast R-CNN 使用 RoI 池化层从不同大小的区域提议中提取固定大小的特征图。这个操作无需扭曲区域，并保留了区域提议特征的空间信息。为了快速检测，作者使用了截断
    SVD，加快了全连接层的前向传递计算。
- en: Experiment results showed that Fast R-CNN had 66.9% mAP while R-CNN of 66.0%
    on PASCAL VOC 2007 dataset [[4](#bib.bib4)]. Training time dropped to 9.5 hours
    as compared to R-CNN with 84h, 9 times faster. For test rate (s/image), Fast R-CNN
    with truncated SVD (0.32s) was ${213}\times$ faster than R-CNN (47s). These experiments
    were carried out on an Nvidia K40 GPU, which demonstrated that Fast R-CNN did
    accelerate object detection process.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果显示，Fast R-CNN 在 PASCAL VOC 2007 数据集上的 mAP 为 66.9%，而 R-CNN 为 66.0% [[4](#bib.bib4)]。训练时间从
    R-CNN 的 84 小时减少到 9.5 小时，速度快了 9 倍。对于测试率 (s/image)，使用截断 SVD 的 Fast R-CNN (0.32s)
    比 R-CNN (47s) 快 ${213}\times$。这些实验是在 Nvidia K40 GPU 上进行的，证明了 Fast R-CNN 确实加速了物体检测过程。
- en: III-A3 Faster R-CNN
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 Faster R-CNN
- en: Three months after Fast R-CNN was proposed, Faster R-CNN [[8](#bib.bib8)] further
    improves the region-based CNN baseline. Fast R-CNN uses selective search to propose
    RoI, which is slow and needs the same running time as the detection network. Faster
    R-CNN replaces it with a novel RPN (region proposal network) that is a fully convolutional
    network to efficiently predict region proposals with a wide range of scales and
    aspect ratios. RPN accelerates the generating speed of region proposals because
    it shares fully-image convolutional features and a common set of convolutional
    layers with the detection network. The procedure is simplified in Fig.3 (b). Furthermore,
    a novel method for different sized object detection is that multi-scale anchors
    are used as reference. The anchors can greatly simplify the process of generating
    various sized region proposals with no need of multiple scales of input images
    or features. On the outputs (feature maps) of the last shared convolutional layer,
    sliding a fixed size window (${3}\times{3}$), the center point of each feature
    window is relative to a point of the original input image which is the center
    point of k (${3}\times{3}$) anchor boxes. The authors define anchor boxes have
    3 different scales and 3 aspect ratios. The region proposal is parameterized relative
    to a reference anchor box. Then they measure the distance between predicted box
    and its corresponding ground truth box to optimize the location of the predicted
    box.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在Fast R-CNN提出三个月后，Faster R-CNN [[8](#bib.bib8)] 进一步改进了基于区域的CNN基线。Fast R-CNN使用选择性搜索来提议RoI，这种方法较慢，需要与检测网络相同的运行时间。Faster
    R-CNN用一种新颖的RPN（区域提议网络）替代了它，RPN是一个全卷积网络，可以高效地预测具有广泛尺度和纵横比的区域提议。由于RPN与检测网络共享全图卷积特征和一组公共卷积层，它加速了区域提议的生成速度。该过程在图3（b）中进行了简化。此外，一种用于不同尺寸目标检测的新方法是使用多尺度锚点作为参考。锚点可以大大简化生成各种尺寸区域提议的过程，无需多尺度的输入图像或特征。在最后共享卷积层的输出（特征图）上，滑动一个固定大小的窗口（${3}\times{3}$），每个特征窗口的中心点相对于原始输入图像的一个点，即k（${3}\times{3}$）锚框的中心点。作者定义锚框具有3种不同的尺度和3种纵横比。区域提议相对于参考锚框进行参数化。然后，他们测量预测框与其对应的真实框之间的距离，以优化预测框的位置。
- en: Experiments indicated that Faster R-CNN has greatly improved both precision
    and detection efficiency. On PASCAL VOC 2007 test set, Faster R-CNN achieved mAP
    of 69.9% as compared to Fast R-CNN of 66.9% with shared convolutional computations.
    As well, total running time of Faster R-CNN (198ms) was nearly 10 times lower
    than Fast R-CNN (1830ms) with the same VGG [[26](#bib.bib26)] backbone, and processing
    rate was 5fps vs. 0.5fps.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，Faster R-CNN在精度和检测效率上都有了显著提高。在PASCAL VOC 2007测试集上，Faster R-CNN达到了69.9%的mAP，而Fast
    R-CNN为66.9%，并且共享了卷积计算。此外，Faster R-CNN（198ms）的总运行时间比Fast R-CNN（1830ms）低近10倍，使用相同的VGG
    [[26](#bib.bib26)]骨干网，处理速率为5fps对比0.5fps。
- en: III-A4 Mask R-CNN
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 Mask R-CNN
- en: Mask R-CNN [[11](#bib.bib11)] is an extending work to Faster R-CNN mainly for
    instance segmentation task. Regardless of the adding parallel mask branch, Mask
    R-CNN can be seen a more accurate object detector. He et al. use Faster R-CNN
    with a ResNet [[25](#bib.bib25)]-FPN [[15](#bib.bib15)] (feature pyramid network,
    a backbone extracts RoI features from different levels of the feature pyramid
    according to their scale) backbone to extract features achieves excellent accuracy
    and processing speed. FPN contains a bottom-up pathway and a top-down pathway
    with lateral connections. The bottom-up pathway is a backbone ConvNet which computes
    a feature hierarchy consisting of feature maps at several scales with a scaling
    step of 2\. The top-down pathway produces higher resolution features by upsampling
    spatially coarser, but semantically stronger, feature maps from higher pyramid
    levels. At the beginning, the top pyramid feature maps are captured by the output
    of the last convolutional layer of the bottom-up pathway. Each lateral connection
    merges feature maps of the same spatial size from the bottom-up pathway and the
    top-down pathway. While the dimensions of feature maps are different, the ${1}\times{1}$
    convolutional layer can change the dimension. Once undergoing a lateral connection
    operation, there will form a new pyramid level and predictions are made independently
    on each level. Because higher-resolution feature maps are important for detecting
    small objects while lower-resolution feature maps are rich in semantic information,
    feature pyramid network extracts significant features.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN [[11](#bib.bib11)] 是对 Faster R-CNN 的扩展，主要用于实例分割任务。除了增加的并行掩膜分支外，Mask
    R-CNN 被认为是一个更精确的目标检测器。He 等人使用了带有 ResNet [[25](#bib.bib25)]-FPN [[15](#bib.bib15)]（特征金字塔网络，一种骨干网络从特征金字塔的不同层级中提取
    RoI 特征，根据其尺度）骨干网的 Faster R-CNN 来提取特征，达到了出色的准确率和处理速度。FPN 包含一个自下而上的路径和一个自上而下的路径，并有横向连接。自下而上的路径是一个骨干
    ConvNet，它计算一个包含几个尺度特征图的特征层次，缩放步长为 2。自上而下的路径通过从较高金字塔层中上采样空间上较粗但语义更强的特征图来生成更高分辨率的特征。最开始，顶层金字塔特征图由自下而上的路径的最后一个卷积层的输出捕获。每个横向连接合并自下而上的路径和自上而下的路径中相同空间尺寸的特征图。虽然特征图的尺寸不同，但
    ${1}\times{1}$ 卷积层可以改变尺寸。经过横向连接操作后，会形成一个新的金字塔层级，并在每个层级上独立进行预测。由于高分辨率的特征图对于检测小物体很重要，而低分辨率的特征图富含语义信息，因此特征金字塔网络提取了重要特征。
- en: Another way to improve accuracy is to replace RoI pooling with RoIAlign to extract
    a small feature map from each RoI, as shown in Fig. 2\. Traditional RoI pooling
    quantizes floating-number in two steps to get approximate feature values in each
    bin. First, quantization is applied to calculate the coordinates of each RoI on
    feature maps, given the coordinates of RoIs in the input images and down sampling
    stride. Then RoI feature maps are divided into bins to generate feature maps at
    the same size, which is also quantized during the process. These two quantization
    operations cause misalignments between the RoI and the extracted features. To
    address this, at those two steps, RoIAlign avoids any quantization of the RoI
    boundaries or bins. First it computes the floating-number of the coordinates of
    each RoI feature map followed by a bilinear interpolation operation to compute
    the exact values of the features at four regularly sampled locations in each RoI
    bin. Then it aggregates the results using max or average pooling to get values
    of each bin. Fig. 2 is an example of RoIAlign operation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种提高准确度的方法是用 RoIAlign 替代 RoI pooling，从每个 RoI 中提取一个小特征图，如图 2 所示。传统的 RoI pooling
    通过两个步骤来量化浮点数，以获得每个 bin 中的近似特征值。首先，应用量化来计算特征图上每个 RoI 的坐标，给定输入图像中的 RoI 坐标和下采样步幅。然后
    RoI 特征图被划分为 bins，以生成相同大小的特征图，在此过程中也会进行量化。这两个量化操作会导致 RoI 与提取的特征之间的对齐错误。为了解决这个问题，在这两个步骤中，RoIAlign
    避免了对 RoI 边界或 bins 的任何量化。首先，它计算每个 RoI 特征图的浮点坐标，然后通过双线性插值操作来计算每个 RoI bin 中四个规则采样位置的特征的确切值。接着，它使用最大池化或平均池化来汇总结果，以获取每个
    bin 的值。图 2 是 RoIAlign 操作的一个示例。
- en: Experiments showed that with the above two improvements the precision got promotion.
    Using ResNet-FPN backbone improved 1.7 points box AP and RoIAlign operation improved
    1.1 points box AP on MS COCO detection dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，通过以上两项改进，精度得到了提升。使用 ResNet-FPN 骨干网使得框 AP 提升了 1.7 个点，RoIAlign 操作在 MS COCO
    检测数据集上提高了 1.1 个点的框 AP。
- en: '![Refer to caption](img/ca57e243bd451b0a20f0369a32deb49c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca57e243bd451b0a20f0369a32deb49c.png)'
- en: 'Figure 2: RoIAlign operation. The first step calculates floating number coordinates
    of an object in the feature map. Next step utilizes bilinear interpolation to
    compute the exact values of the features at four regularly sampled locations in
    the separated bin.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：RoIAlign操作。第一步计算特征图中物体的浮点坐标。下一步利用双线性插值计算在分隔的bin中四个规律采样位置的特征的精确值。
- en: III-B One-stage Detectors
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 一阶段检测器
- en: III-B1 YOLO
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 YOLO
- en: YOLO [[9](#bib.bib9)] (you only look once) is a one-stage object detector proposed
    by Redmon et al. after Faster R-CNN [[8](#bib.bib8)]. The main contribution is
    real-time detection of full images and webcam. Firstly, it is due to this pipeline
    only predicts less than 100 bounding boxes per image while Fast R-CNN using selective
    search predicts 2000 region proposals per image. Secondly, YOLO frames detection
    as a regression problem, so a unified architecture can extract features from input
    images straightly to predict bounding boxes and class probabilities. YOLO network
    runs at 45 frames per second with no batch processing on a Titan X GPU as compared
    to Fast R-CNN at 0.5fps and Faster R-CNN at 7fps.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO [[9](#bib.bib9)]（you only look once）是Redmon等人在Faster R-CNN [[8](#bib.bib8)]之后提出的一阶段目标检测器。其主要贡献是实时检测完整图像和摄像头。首先，由于该流程每张图像仅预测不到100个边界框，而Fast
    R-CNN使用选择性搜索预测每张图像2000个区域提议。其次，YOLO将检测框架视为回归问题，因此统一的架构可以直接从输入图像中提取特征以预测边界框和类别概率。YOLO网络在Titan
    X GPU上以每秒45帧的速度运行，而Fast R-CNN为0.5fps，Faster R-CNN为7fps。
- en: YOLO pipeline first divides the input image into an ${S}\times{S}$ grid, where
    a grid cell is responsible to detect the object whose center falls into. The confidence
    score is obtained by multiplying two parts, where $P(object)$ denotes the probability
    of the box containing an object and IOU (intersection over union) shows how accurate
    the box containing that object. Each grid cell predicts B bounding boxes $(x,y,w,h)$
    and confidence scores for them and C-dimension conditional class probabilities
    for C categories. The feature extraction network contains 24 convolutional layers
    followed by 2 fully connected layers. When pre-training on ImageNet dataset, the
    authors use the first 20 convolutional layers and an average pooling layer followed
    by a fully connected layer. For detection, the whole network is used for better
    performance. In order to get fine-grained visual information to improve detection
    precision, in detection stage double the input resolution of ${224}\times{224}$
    in pre-training stage.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO流程首先将输入图像划分为${S}\times{S}$网格，其中每个网格单元负责检测其中心落入的物体。置信度得分通过两个部分的乘积获得，其中$P(object)$表示框中包含物体的概率，IOU（交并比）显示框中物体的准确度。每个网格单元预测B个边界框$(x,y,w,h)$及其置信度得分，并为C个类别提供C维条件类别概率。特征提取网络包含24个卷积层，后跟2个全连接层。在ImageNet数据集上进行预训练时，作者使用前20个卷积层和一个平均池化层，然后是一个全连接层。为了获得细粒度的视觉信息以提高检测精度，在检测阶段，将预训练阶段的输入分辨率${224}\times{224}$加倍。
- en: The experiments showed that YOLO was not good at accurate localization and localization
    error was the main component of prediction error. Fast R-CNN makes many background
    false positives mistakes while YOLO is 3 times less than it. Training and testing
    on PASCAL VOC dataset, YOLO achieved 63.4% mAP with 45 fps as compared to Fast
    R-CNN (70.0% mAP, 0.5fps) and Faster R-CNN (73.2% mAP, 7fps).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 实验显示，YOLO在准确定位方面表现不佳，定位误差是预测误差的主要组成部分。Fast R-CNN在背景中产生了许多误报，而YOLO比其少3倍。在PASCAL
    VOC数据集上进行训练和测试时，YOLO实现了63.4%的mAP和45 fps，而Fast R-CNN为70.0%的mAP和0.5fps，Faster R-CNN为73.2%的mAP和7fps。
- en: III-B2 YOLOv2
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 YOLOv2
- en: YOLOv2 [[30](#bib.bib30)] is a second version of YOLO [[9](#bib.bib9)], which
    adopts a series of design decisions from past works with novel concepts to improve
    YOLO’s speed and precision.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv2 [[30](#bib.bib30)]是YOLO [[9](#bib.bib9)]的第二个版本，采用了一系列过去工作的设计决策和新颖概念，以提高YOLO的速度和精度。
- en: Batch Normalization. Fixed distribution of inputs to a ConvNet layer would have
    positive consequences for the layers. It is impractical to normalize the entire
    training set because the optimization step uses stochastic gradient descent. Since
    SGD uses mini-batches during training, each mini-batch produces estimates of the
    mean and variance of each activation. Computing the mean and variance value of
    the mini-batch of size $m$, then normalize the activations of number $m$ to have
    mean zero and variance 1\. Finally, the elements of each mini-batch are sampled
    from the same distribution. This operation can be seen as a BN layer [[31](#bib.bib31)]
    which outputs activations with the same distribution. YOLOv2 adds a BN layer ahead
    of each convolutional layer which accelerates the network to get convergence and
    helps regularize the model. Batch normalization gets more than 2% improvement
    in mAP.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化。将输入的固定分布应用到卷积网络层会对层产生积极的影响。由于优化步骤使用随机梯度下降，规范化整个训练集是不切实际的。由于SGD在训练过程中使用小批量，因此每个小批量都会生成每个激活的均值和方差的估计值。计算小批量的均值和方差，然后将大小为$m$的激活值归一化，使其均值为零，方差为1。最后，每个小批量的元素从相同的分布中抽样。这个操作可以看作是一个BN层[[31](#bib.bib31)]，它输出具有相同分布的激活值。YOLOv2在每个卷积层之前添加了一个BN层，这加快了网络的收敛速度并帮助正则化模型。批量归一化使mAP提高了超过2%。
- en: High Resolution Classifier. In YOLO backbone, the classifier adopts an input
    resolution of ${224}\times{224}$ then increases the resolution to 448 for detection.
    This process needs the network adjust to a new resolution inputs when switches
    to object detection task. To address this, YOLOv2 adds a fine-tuning process to
    the classification network at ${448}\times{448}$ for 10 epochs on ImageNet dataset
    which increases the mAP at 4%.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 高分辨率分类器。在YOLO骨干网络中，分类器采用${224}\times{224}$的输入分辨率，然后将分辨率提高到448进行检测。这个过程需要网络在切换到目标检测任务时调整为新的分辨率输入。为了解决这个问题，YOLOv2在${448}\times{448}$的分类网络上添加了一个微调过程，在ImageNet数据集上进行10个周期的训练，这将mAP提高了4%。
- en: Convolutional with Anchor Boxes. In original YOLO networks, coordinates of predicted
    boxes are directly generated by fully connected layers. Faster R-CNN uses anchor
    boxes as reference to generate offsets with predicted boxes. YOLOv2 adopts this
    prediction mechanism and firstly removes fully connected layers. Then it predicts
    class and objectness for every anchor box. This operation increases 7% recall
    while mAP decreases 0.3%.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 带锚框的卷积。在原始YOLO网络中，预测框的坐标是由全连接层直接生成的。Faster R-CNN使用锚框作为参考来生成与预测框的偏移量。YOLOv2采用这种预测机制，并首先去除了全连接层。然后，它为每个锚框预测类别和物体性。这个操作使召回率提高了7%，而mAP减少了0.3%。
- en: Predicting the size and aspect ratio of anchor boxes using dimension clusters.
    In Faster R-CNN, the size and aspect ratio of anchor boxes is identified empirically.
    For easier learning to predict good detections, YOLOv2 uses K-means clustering
    on the training set bounding boxes to automatically get good priors. Using dimension
    clusters along with directly predicting the bounding box center location improves
    YOLO by almost 5% over the above version with anchor boxes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用维度聚类预测锚框的尺寸和宽高比。在Faster R-CNN中，锚框的尺寸和宽高比是通过经验确定的。为了更容易地学习预测良好的检测，YOLOv2在训练集的边界框上使用K-means聚类自动获取良好的先验。使用维度聚类以及直接预测边界框中心位置，将YOLO的性能提高了近5%。
- en: Fine-Grained Features. For localizing smaller objects, high-resolution feature
    maps can provide useful information. Similar to the identity mappings in ResNet,
    YOLOv2 concatenates the higher resolution features with the low resolution features
    by stacking adjacent features into different channels which gives a modest 1%
    performance increase.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度特征。为了定位较小的物体，高分辨率特征图可以提供有用的信息。类似于ResNet中的身份映射，YOLOv2通过将相邻的特征堆叠到不同的通道中，将高分辨率特征与低分辨率特征进行拼接，这使性能提高了1%。
- en: Multi-Scale Training. For networks to be robust to run on images of different
    sizes, every 10 batches the network randomly chooses a new image dimension size
    from $\{320,352,...,608\}$. This means the same network can predict detections
    at different resolutions. At high resolution detection, YOLOv2 achieves 78.6%
    mAP and 40fps as compared to YOLO with 63.4% mAP and 45fps on VOC 2007.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度训练。为了使网络对不同尺寸的图像具有鲁棒性，每10个批次，网络会随机选择一个新的图像尺寸，来自$\{320,352,...,608\}$。这意味着相同的网络可以在不同分辨率下预测检测。高分辨率检测中，YOLOv2在VOC
    2007上达到78.6%的mAP和40fps，而YOLO为63.4%的mAP和45fps。
- en: As well, YOLOv2 proposes a new classification backbone namely Darknet-19 with
    19 convolutional layers and 5 max-pooling layers which requires less operations
    to process an image yet achieves high accuracy. The more competitive YOLOv2 version
    has 78.6% mAP and 40fps as compared to Faster R-CNN with ResNet backbone of 76.4%
    mAP and 5fps, and SSD500 has 76.8% mAP and 19fps. As mentioned above, YOLOv2 can
    achieve high detecting precision while high processing rate which benefit from
    7 main improvements and a new backbone.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，YOLOv2 提出了一个新的分类骨干网络，即 Darknet-19，具有 19 层卷积层和 5 层最大池化层，处理图像所需的操作较少，同时实现了高精度。更具竞争力的
    YOLOv2 版本具有 78.6% 的 mAP 和 40fps，而 Faster R-CNN 与 ResNet 骨干网络的 mAP 为 76.4% 和 5fps，SSD500
    的 mAP 为 76.8% 和 19fps。如上所述，YOLOv2 通过 7 项主要改进和一个新的骨干网络，实现了高检测精度和高处理速度。
- en: III-B3 YOLOv3
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 YOLOv3
- en: YOLOv3 [[32](#bib.bib32)] is an improved version of YOLOv2\. First, YOLOv3 uses
    multi-label classification (independent logistic classifiers) to adapt to more
    complex datasets containing many overlapping labels. Second, YOLOv3 utilizes three
    different scale feature maps to predict the bounding box. The last convolutional
    layer predicts a 3-d tensor encoding class predictions, objectness, and bounding
    box. Third, YOLOv3 proposes a deeper and robust feature extractor, called Darknet-53,
    inspired by ResNet.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv3 [[32](#bib.bib32)] 是 YOLOv2 的改进版。首先，YOLOv3 使用多标签分类（独立逻辑回归分类器）来适应包含许多重叠标签的更复杂数据集。其次，YOLOv3
    利用三种不同尺度的特征图来预测边界框。最后一层卷积层预测一个 3-d 张量，编码类别预测、物体性和边界框。第三，YOLOv3 提出了一个更深且更强大的特征提取器，称为
    Darknet-53，灵感来自于 ResNet。
- en: According to results of experiments on MS COCO dataset, YOLOv3 (AP:33%) performs
    on par with the SSD variant (DSSD513:AP:33.2%) under MS COCO metrics yet 3 times
    faster than DSSD while quite a bit behind RetinaNet [[33](#bib.bib33)] (AP:40.8%).
    But uses the “old” detection metric of mAP at IOU= 0.5 (or ${AP}_{50}$), YOLOv3
    can achieve 57.9% mAP as compared to DSSD513 of 53.3% and RetinaNet of 61.1%.
    Due to the advantages of multi-scale predictions, YOLOv3 can detect small objects
    even more but has comparatively worse performance on medium and larger sized objects.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 根据在 MS COCO 数据集上的实验结果，YOLOv3（AP:33%）在 MS COCO 指标下的表现与 SSD 变体（DSSD513:AP:33.2%）相当，但速度比
    DSSD 快三倍，而远远落后于 RetinaNet [[33](#bib.bib33)]（AP:40.8%）。不过，使用的是旧的检测指标 mAP（IOU=
    0.5，或 ${AP}_{50}$），YOLOv3 能达到 57.9% 的 mAP，相比之下 DSSD513 为 53.3%，RetinaNet 为 61.1%。由于多尺度预测的优势，YOLOv3
    能检测更多的小物体，但在中型和大型物体上的表现相对较差。
- en: 'TABLE I: AP scores (%) on the MS COCO dataset,${AP}_{S}$:AP of small objects,
    ${AP}_{M}$:AP of medium objects, ${AP}_{L}$:AP of large objects'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：MS COCO 数据集上的 AP 分数（%），${AP}_{S}$：小物体的 AP，${AP}_{M}$：中型物体的 AP，${AP}_{L}$：大型物体的
    AP
- en: '| Model | ${AP}_{S}$ | ${AP}_{M}$ | ${AP}_{L}$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Model | ${AP}_{S}$ | ${AP}_{M}$ | ${AP}_{L}$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| DSSD513 | 13.0 | 35.4 | 51.1 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| DSSD513 | 13.0 | 35.4 | 51.1 |'
- en: '| RetinaNet | 24.1 | 44.2 | 51.2 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| RetinaNet | 24.1 | 44.2 | 51.2 |'
- en: III-B4 SSD
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B4 SSD
- en: SSD [[10](#bib.bib10)], a single-shot detector for multiple categories within
    one-stage which directly predicts category scores and box offsets for a fixed
    set of default bounding boxes of different scales at each location in several
    feature maps with different scales, as shown in Fig.4 (a). The default bounding
    boxes have different aspect ratios and scales in each feature map. In different
    feature maps, the scale of default bounding boxes is computed with regularly space
    between the highest layer and the lowest layer where each specific feature map
    learns to be responsive to the particular scale of the objects. For each default
    box, it predicts both the offsets and the confidences for all object categories.
    Fig.3 (c) shows the method. At training time, matching these default bounding
    boxes to ground truth boxes where the matched default boxes as positive examples
    and the rest as negatives. For the large amount of default boxes are negatives,
    the authors adopt hard negative mining using the highest confidence loss for each
    default box then pick the top ones to make the ratio between the negatives and
    positives at most 3:1\. As well, the authors implement data augmentation which
    is proved an effective way to enhance precision by a large margin.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: SSD [[10](#bib.bib10)] 是一种单阶段的多类别单次检测器，它直接预测类别分数和框偏移量，对于不同尺度的一组默认边界框，在多个尺度的特征图中的每个位置进行预测，如图
    4（a）所示。默认边界框在每个特征图中具有不同的长宽比和尺度。在不同的特征图中，默认边界框的尺度与最高层和最低层之间的规律间距计算，每个特定特征图学习对物体特定尺度的响应。对于每个默认框，它预测所有物体类别的偏移量和置信度。图
    3（c）展示了该方法。在训练时，将这些默认边界框与真实框匹配，其中匹配的默认框作为正样本，其余作为负样本。由于大量默认框是负样本，作者采用了硬负样本挖掘，使用每个默认框的最高置信度损失，然后挑选出前几个，以使负样本与正样本的比例最多为
    3:1。此外，作者实现了数据增强，这被证明是一种有效的提高精度的方法。
- en: 'Experiments showed that SSD512 had a competitive result on both mAP and speed
    with VGG-16 [[26](#bib.bib26)] backbone. SSD512 (input image size: ${512}\times{512}$)
    achieved mAP of 81.6% on PASCAL VOC 2007 test set and 80.0% on PASCAL VOC 2012
    test set as compared to Faster R-CNN (78.8%, 75.9%) and YOLO (VOC2012: 57.9%).
    On MS COCO DET dataset, SSD512 was better than Faster R-CNN under all evaluation
    criteria.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '实验表明，SSD512 在 mAP 和速度上都与 VGG-16 [[26](#bib.bib26)] 主干网络具有竞争力。SSD512（输入图像尺寸：${512}\times{512}$）在
    PASCAL VOC 2007 测试集上获得了 81.6% 的 mAP，在 PASCAL VOC 2012 测试集上获得了 80.0%，相比之下，Faster
    R-CNN 分别为 78.8% 和 75.9%，YOLO（VOC2012: 57.9%）。在 MS COCO DET 数据集上，SSD512 在所有评估标准下均优于
    Faster R-CNN。'
- en: '![Refer to caption](img/bf46b920b7dc183a2e246c8b41e064f5.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/bf46b920b7dc183a2e246c8b41e064f5.png)'
- en: 'Figure 3: Four methods utilize features for different sized object prediction.
    (a) Using an image pyramid to build a feature pyramid. Features are computed on
    each of the image scales independently, which is slow. (b) Detection systems [[8](#bib.bib8)]
    [[29](#bib.bib29)] use only single scale features (the outputs of the last convolutional
    layer) for faster detection. (c) Predicting each of the pyramidal feature hierarchy
    from a ConvNet as if it is a image pyramid like SSD [[10](#bib.bib10)]. (d) Feature
    Pyramid Network (FPN) [[15](#bib.bib15)] is fast like (b) and (c), but more accurate.
    In this figure, the feature graph is represented by a gray-filled quadrilateral.
    The head network is represented by a blue rectangle.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：四种方法利用特征进行不同尺寸物体的预测。（a）使用图像金字塔来构建特征金字塔。特征在每个图像尺度上独立计算，这很慢。（b）检测系统 [[8](#bib.bib8)]
    [[29](#bib.bib29)] 仅使用单一尺度特征（最后一层卷积层的输出）以加快检测速度。（c）像 SSD [[10](#bib.bib10)] 一样，将
    ConvNet 的每个金字塔特征层级预测为图像金字塔。（d）特征金字塔网络（FPN）[[15](#bib.bib15)] 像（b）和（c）一样速度快，但更为准确。在此图中，特征图用灰色填充的四边形表示。头网络用蓝色矩形表示。
- en: III-B5 DSSD
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B5 DSSD
- en: DSSD [[34](#bib.bib34)] (Deconvolutional Single Shot Detector) is a modified
    version of SSD (Single Shot Detector) which adds prediction module and deconvolution
    module also adopts ResNet-101 as backbone. The architecture of DSSD is shown in
    Fig.4 (b). For prediction module, Fu et al. add a residual block to each predicting
    layer, then do element-wise addition of the outputs of prediction layer and residual
    block. Deconvolution module increases the resolution of feature maps to strengthen
    features. Each deconvolution layer followed by a prediction module is to predict
    a variety of objects with different sizes. At training process, first the authors
    pre-train ResNet-101 based backbone network on the ILSVRC CLS-LOC dataset, then
    use ${321}\times{321}$ inputs or ${513}\times{513}$ inputs training the original
    SSD model on detection dataset. Finally, they train the deconvolution module freezing
    all the weights of SSD module.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: DSSD [[34](#bib.bib34)]（反卷积单次检测器）是SSD（单次检测器）的一个修改版本，增加了预测模块和反卷积模块，同时采用ResNet-101作为主干网络。DSSD的架构如图4（b）所示。对于预测模块，Fu等人向每个预测层添加了一个残差块，然后对预测层和残差块的输出进行逐元素相加。反卷积模块提高了特征图的分辨率以增强特征。每个反卷积层后面跟着一个预测模块，用于预测不同尺寸的各种对象。在训练过程中，首先作者在ILSVRC
    CLS-LOC数据集上对基于ResNet-101的主干网络进行预训练，然后使用${321}\times{321}$输入或${513}\times{513}$输入在检测数据集上训练原始SSD模型。最后，他们训练反卷积模块，同时冻结SSD模块的所有权重。
- en: Experiments on both PASCAL VOC dataset and MS COCO dataset showed the effectiveness
    of DSSD513 model, while the added prediction module and deconvolution module brought
    2.2% enhancement on PASCAL VOC 2007 test dataset.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在PASCAL VOC数据集和MS COCO数据集上的实验显示了DSSD513模型的有效性，而添加的预测模块和反卷积模块在PASCAL VOC 2007测试数据集上带来了2.2%的提升。
- en: '![Refer to caption](img/fe16e0362c16eda4613a9a4ab7fd9a98.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fe16e0362c16eda4613a9a4ab7fd9a98.png)'
- en: 'Figure 4: Networks of SSD, DSSD and RetinaNet on residual network. (a) The
    blue modules are the layers added in SSD framework whose resolution gradually
    drop because of down sampling. In SSD the prediction layer is acting on fused
    features of different levels. Head module consists of a series of convolutional
    layers followed by several classification layers and localization layers. (b)
    The red modules are the layers added in DSSD framework denoting deconvolution
    operation. In DSSD, the prediction layer is following every deconvolution module.
    (c) RetinaNet utilizes ResNet-FPN as its backbone network, which generates 5 level
    feature pyramid (P3-P7) corresponding to C3-C7 (the feature map of conv3-conv7
    respectively) to predict different sized objects.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：SSD、DSSD和基于残差网络的RetinaNet的网络结构。 (a) 蓝色模块是SSD框架中添加的层，其分辨率由于下采样而逐渐降低。在SSD中，预测层作用于不同层次的融合特征。头部模块由一系列卷积层和若干分类层以及定位层组成。
    (b) 红色模块是DSSD框架中添加的层，表示反卷积操作。在DSSD中，预测层跟随每个反卷积模块。 (c) RetinaNet利用ResNet-FPN作为其主干网络，生成5级特征金字塔（P3-P7），对应C3-C7（分别为conv3-conv7的特征图），以预测不同尺寸的对象。
- en: III-B6 RetinaNet
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B6 RetinaNet
- en: RetinaNet [[33](#bib.bib33)] is a one-stage object detector with focal loss
    as classification loss function proposed by Lin et al. [[33](#bib.bib33)] in February
    2018\. The architecture of RetinaNet is shown in Fig.4 (c). R-CNN is a typical
    two-stage object detector. The first stage generates a sparse set of region proposals
    and the second stage classifies each candidate location. Owing to the first stage
    filters out the majority of negative locations, two-stage object detectors can
    achieve higher precision than one-stage detectors which propose a dense set of
    candidate locations. The main reason is the extreme foreground-background class
    imbalance when one-stage detectors train networks to get convergence. So the authors
    propose a loss function, called focal loss, which can down-weight the loss assigned
    to well-classified or easy examples. Focal loss concentrates on the hard training
    examples and avoids the vast number of easy negative examples overwhelming the
    detector during training. RetinaNet inherits the fast speed of previous one-stage
    detectors while greatly overcomes the disadvantage of one-stage detectors difficult
    to train unbalanced positive and negative examples.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: RetinaNet [[33](#bib.bib33)]是一种单阶段目标检测器，采用focal loss作为分类损失函数，由Lin等人[[33](#bib.bib33)]于2018年2月提出。RetinaNet的架构如图4（c）所示。R-CNN是一种典型的两阶段目标检测器。第一阶段生成一组稀疏的区域提议，第二阶段对每个候选位置进行分类。由于第一阶段筛选出了大多数负样本，两阶段目标检测器能够实现比单阶段检测器更高的精度，因为单阶段检测器提议了一组密集的候选位置。主要原因是单阶段检测器在训练网络时面临极端的前景-背景类别不平衡。因此，作者提出了一种叫做focal
    loss的损失函数，可以减少分配给分类良好或容易的样本的损失。Focal loss专注于困难的训练样本，避免了大量简单负样本在训练期间淹没检测器。RetinaNet继承了先前单阶段检测器的快速速度，同时大大克服了单阶段检测器在训练不平衡正负样本时的困难。
- en: Experiments showed that RetinaNet with ResNet-101-FPN backbone got 39.1% AP
    as compared to DSSD513 of 33.2% AP on MS COCO test-dev dataset. With ResNeXt-101-FPN,
    it made 40.8% AP far surpassing DSSD513\. RetinaNet improved the detection precision
    on small and medium objects by a large margin.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，使用ResNet-101-FPN骨干网的RetinaNet在MS COCO test-dev数据集上的AP为39.1%，相比之下，DSSD513的AP为33.2%。使用ResNeXt-101-FPN时，其AP达到了40.8%，远超DSSD513。RetinaNet显著提高了对小型和中型物体的检测精度。
- en: III-B7 M2Det
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B7 M2Det
- en: To meet a large variety of scale variation across object instances, Zhao et
    al. [[35](#bib.bib35)] propose a multi-level feature pyramid network (MLFPN) constructing
    more effective feature pyramids. The authors adopt three steps to obtain final
    enhanced feature pyramids. First, like FPN, multi-level features extracted from
    multiple layers in the backbone are fused as the base feature. Second, the base
    feature is fed into a block, composing of alternating joint Thinned U-shape Modules
    and Feature Fusion Modules, and obtains the decoder layers of TUM as the features
    for next step. Finally, a feature pyramid containing multi-level features is constructed
    by integrating the decoder layers of equivalent scale. So far, features with multi-scale
    and multi-level are prepared. The remaining part is to follow the SSD architecture
    to obtain bounding box localization and classification results in an end-to-end
    manner.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应物体实例间的大范围尺度变化，Zhao等人[[35](#bib.bib35)]提出了一种多级特征金字塔网络（MLFPN），构建了更有效的特征金字塔。作者采用了三步来获得最终的增强特征金字塔。首先，像FPN一样，从骨干网的多个层提取的多级特征被融合为基础特征。其次，将基础特征输入一个由交替的Thinned
    U形模块和特征融合模块组成的块，并获得TUM的解码器层作为下一步的特征。最后，通过整合等效尺度的解码器层来构建包含多级特征的特征金字塔。目前，多尺度和多级的特征已准备就绪。剩下的部分是按照SSD架构以端到端的方式获得边界框定位和分类结果。
- en: For M2Det is a one-stage detector, it achieves AP of 41.0 at speed of 11.8 FPS
    with single-scale inference strategy and AP of 44.2 with multi-scale inference
    strategy utilizing VGG-16 on COCO test-dev set. It outperforms RetinaNet800 (Res101-FPN
    as backbone) by 0.9% with single-scale inference strategy, but is twice slower
    than RetinaNet800.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于M2Det作为一种单阶段检测器，它在使用VGG-16进行COCO test-dev数据集测试时，单尺度推理策略下达到了41.0的AP，速度为11.8
    FPS，而多尺度推理策略下达到了44.2的AP。它在单尺度推理策略下比RetinaNet800（以Res101-FPN为骨干网）提高了0.9%，但比RetinaNet800慢了两倍。
- en: III-B8 RefineDet
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B8 RefineDet
- en: The whole network of RefineDet [[36](#bib.bib36)] contains two inter-connected
    modules, the anchors refinement module and the object detection module. These
    two modules are connected by a transfer connection block to transfer and enhance
    features from the former module to better predict objects in the latter module.
    The training process is in an end-to-end way, conducted by three stages, preprocessing,
    detection (two inter-connected modules), and NMS.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: RefineDet [[36](#bib.bib36)]的整个网络包含两个相互连接的模块：锚点细化模块和对象检测模块。这两个模块通过一个传输连接块连接，以从前一个模块传输和增强特征，从而更好地预测后一个模块中的对象。训练过程是端到端的，通过三个阶段进行：预处理、检测（两个相互连接的模块）和NMS。
- en: Classical one-stage detectors such as SSD, YOLO, RetinaNet all use one-step
    regression method to obtain the final results. The authors find that use two-step
    cascaded regression method can better predict hard detected objects, especially
    for small objects and provide more accurate locations of objects.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的一阶段检测器，如SSD、YOLO和RetinaNet，都是使用一步回归方法来获得最终结果。作者发现，使用两步级联回归方法可以更好地预测难以检测的对象，尤其是小物体，并提供更准确的对象位置。
- en: III-C Latest Detectors
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 最新检测器
- en: III-C1 Relation Networks for Object Detection
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 用于对象检测的关系网络
- en: Hu et al. [[37](#bib.bib37)] propose an adapted attention module for object
    detection called object relation module which considers the interaction between
    different targets in an image including their appearance feature and geometry
    information. This object relation module is added in the head of detector before
    two fully connected layers to get enhanced features for accurate classification
    and localization of objects. The relation module not only feeds enhanced features
    into classifier and regressor, but replaces NMS post-processing step which gains
    higher accuracy than NMS. By using Faster R-CNN, FPN and DCN as the backbone network
    on the COCO test-dev dataset, adding the relationship module increases the accuracy
    by 0.2, 0.6, and 0.2, respectively.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 胡等人[[37](#bib.bib37)]提出了一种适应性注意力模块，称为对象关系模块，该模块考虑了图像中不同目标之间的相互作用，包括它们的外观特征和几何信息。这个对象关系模块被添加在检测器的头部，在两个全连接层之前，以获得增强的特征，用于准确分类和定位对象。关系模块不仅将增强的特征输入到分类器和回归器中，而且替代了NMS后处理步骤，获得了比NMS更高的准确性。通过在COCO
    test-dev数据集上使用Faster R-CNN、FPN和DCN作为主干网络，添加关系模块分别提高了0.2、0.6和0.2的准确率。
- en: III-C2 DCNv2
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 DCNv2
- en: For learning to adapt to geometric variation reflected in the effective spatial
    support region of targets, deformable convolutional networks (DCN) [[38](#bib.bib38)]
    was proposed by Dai et al. Regular ConvNets can only focus on features of fixed
    square size (according to the kernel), thus the receptive field does not properly
    cover the entire pixel of a target object to represent it. The deformable ConvNets
    can produce deformable kernel and the offset from the initial convolution kernel
    (of fixed size) are learned from the networks. Deformable RoI Pooling can also
    adapt to part location for objects with different shapes. On COCO test-dev set,
    DCNv1 achieves significant accuracy improvement, which is almost 4% higher than
    three plain ConvNets. The best mean average-precision result under the strict
    COCO evaluation criteria (mAP @[0.5:0.95] ) is 37.5%.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习适应目标在有效空间支持区域中反映的几何变化，Dai等人提出了可变形卷积网络（DCN）[[38](#bib.bib38)]。常规卷积网络只能关注固定大小的特征（根据卷积核），因此感受野无法正确覆盖目标对象的整个像素以进行表示。可变形卷积网络可以生成可变形的卷积核，并且从初始卷积核（固定大小）中学习偏移量。可变形RoI池化也可以适应具有不同形状的对象的部分位置。在COCO
    test-dev数据集上，DCNv1实现了显著的准确性提升，比三种普通卷积网络高出近4%。在严格的COCO评估标准下（mAP @[0.5:0.95]），最佳的平均精度结果为37.5%。
- en: Deformable ConvNets v2 [[39](#bib.bib39)] utilizes more deformable convolutional
    layers than DCNv1 (from only the convolutional layers in the conv5 stage to all
    the convolutional layers in the conv3-conv5 stages) to replace the regular convolutional
    layers. All the deformable layers are modulated by a learnable scalar, which obviously
    enhance the deformable effect and accuracy. The authors adopt feature mimicking
    to further improve detection accuracy by incorporating a feature mimic loss on
    the per-RoI features of DCN to be similar to good features extracted from cropped
    images. DCNv2 achieves 45.3% mAP under COCO evaluation criteria on the COCO 2017
    test-dev set, while DCNv1 with 41.7% and regular Faster R-CNN with 40.1% on ResNext-101
    backbone. On other strong backbones, DCNv2 surpasses DCNv1 by $3\%-5\%$ mAP and
    regular Faster R-CNN by $5\%-8\%$.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Deformable ConvNets v2 [[39](#bib.bib39)]相比DCNv1（从仅卷积层在conv5阶段到所有卷积层在conv3-conv5阶段）使用了更多的可变形卷积层来替代常规卷积层。所有可变形层都由可学习的标量进行调节，这显著增强了可变形效果和准确性。作者采用特征模仿进一步提高检测准确性，通过在DCN的每个RoI特征上引入特征模仿损失，使其类似于从裁剪图像中提取的良好特征。DCNv2在COCO
    2017 test-dev集上根据COCO评估标准达到了45.3%的mAP，而DCNv1为41.7%，常规Faster R-CNN在ResNext-101骨干网上的mAP为40.1%。在其他强大骨干网上，DCNv2超越了DCNv1
    $3\%-5\%$ mAP，超越了常规Faster R-CNN $5\%-8\%$。
- en: III-C3 NAS-FPN
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C3 NAS-FPN
- en: In recent days, the authors from Google Brain adopt neural architecture search
    to find some new feature pyramid architecture, named NAS-FPN [[18](#bib.bib18)],
    consisting of both top-down and bottom-up connections to fuse features with a
    variety of different scales. By repeating FPN architecture N times then concatenating
    them into a large architecture during the search, the high level feature layers
    pick arbitrary level features for them to imitate. All of the highest accuracy
    architectures have the connection between high resolution input feature maps and
    output feature layers, which indicate that it is necessary to generate high resolution
    features for small targets detection. Stacking more pyramid networks, adding feature
    dimension, adopting high capacity architecture all increase detection accuracy
    by a large margin.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Google Brain的作者采用神经架构搜索来寻找一些新的特征金字塔架构，命名为NAS-FPN [[18](#bib.bib18)]，包括自上而下和自下而上的连接，以融合不同尺度的特征。在搜索过程中，通过重复FPN架构N次然后将其连接成一个大型架构，高层特征层可以选择任意级别的特征进行模拟。所有具有最高精度的架构都包含高分辨率输入特征图与输出特征层之间的连接，这表明生成高分辨率特征对于小目标检测是必要的。堆叠更多的金字塔网络、增加特征维度和采用高容量架构都大幅提高了检测准确率。
- en: Experiments showed that adopting ResNet-50 as backbone of 256 feature dimension,
    on the COCO test-dev dataset, the mAP of NAS-FPN exceeded the original FPN by
    2.9%. The superlative configuration of NAS-FPN utilized AmoebaNet as backbone
    network and stacked 7 FPN of 384 feature dimension, which achieved 48.0% on COCO
    test-dev.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，在COCO test-dev数据集上，采用256特征维度的ResNet-50作为骨干网，NAS-FPN的mAP超过了原始FPN 2.9%。NAS-FPN的最佳配置采用AmoebaNet作为骨干网，并堆叠了7个384特征维度的FPN，在COCO
    test-dev上达到了48.0%。
- en: In conclusion, the typical baselines enhance accuracy by extracting richer features
    of objects and adopting multi-level and multi-scale features for different sized
    object detection. To achieve higher speed and precision, the one-stage detectors
    utilize newly designed loss function to filter out easy samples which drops the
    number of proposal targets by a large margin. To address geometric variation,
    adopting deformable convolution layers is an effective way. Modeling the relationship
    between different objects in an image is also necessary to improve performance.
    Detection results on MS COCO test-dev dataset of the above typical baselines are
    listed on table 2.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，典型的基线通过提取更丰富的对象特征和采用多级别、多尺度特征来提升准确性，以应对不同尺寸的目标检测。为了实现更高的速度和精度，一阶段检测器利用新设计的损失函数来过滤简单样本，从而大幅减少提案目标的数量。为了解决几何变异问题，采用可变形卷积层是一种有效的方法。建模图像中不同对象之间的关系也是提升性能的必要步骤。表2列出了上述典型基线在MS
    COCO test-dev数据集上的检测结果。
- en: 'TABLE II: Detection results on the MS COCO test-dev dataset of some typical
    baselines. AP, ${AP}_{50}$ , ${AP}_{75}$ scores (%). ${AP}_{S}$:AP of small objects,
    ${AP}_{M}$:AP of medium objects, ${AP}_{L}$:AP of large objects. *DCNv2+Faster
    R-CNN models are trained on the 118k images of the COCO 2017 train set.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE II: MS COCO 测试开发数据集上一些典型基线的检测结果。AP, ${AP}_{50}$ , ${AP}_{75}$ 分数（%）。${AP}_{S}$:小物体的
    AP，${AP}_{M}$:中等物体的 AP，${AP}_{L}$:大物体的 AP。*DCNv2+Faster R-CNN 模型在 COCO 2017 训练集的
    118k 图像上进行训练。'
- en: '| Method | Data | Backbone | AP | ${AP}_{50}$ | ${AP}_{75}$ | ${AP}_{S}$ |
    ${AP}_{M}$ | ${AP}_{L}$ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 数据 | 骨干网络 | AP | ${AP}_{50}$ | ${AP}_{75}$ | ${AP}_{S}$ | ${AP}_{M}$
    | ${AP}_{L}$ |'
- en: '| Fast R-CNN[[29](#bib.bib29)] | train | VGG-16 | 19.7 | 35.9 | ${-}$ | ${-}$
    | ${-}$ | ${-}$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Fast R-CNN[[29](#bib.bib29)] | train | VGG-16 | 19.7 | 35.9 | ${-}$ | ${-}$
    | ${-}$ | ${-}$ |'
- en: '| Faster R-CNN[[8](#bib.bib8)] | trainval | VGG-16 | 21.9 | 42.7 | ${-}$ |
    ${-}$ | ${-}$ | ${-}$ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Faster R-CNN[[8](#bib.bib8)] | trainval | VGG-16 | 21.9 | 42.7 | ${-}$ |
    ${-}$ | ${-}$ | ${-}$ |'
- en: '| OHEM[[40](#bib.bib40)] | trainval | VGG-16 | 22.6 | 42.5 | 22.2 | 5.0 | 23.7
    | 37.9 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| OHEM[[40](#bib.bib40)] | trainval | VGG-16 | 22.6 | 42.5 | 22.2 | 5.0 | 23.7
    | 37.9 |'
- en: '| ION[[41](#bib.bib41)] | train | VGG-16 | 23.6 | 43.2 | 23.6 | 6.4 | 24.1
    | 38.3 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| ION[[41](#bib.bib41)] | train | VGG-16 | 23.6 | 43.2 | 23.6 | 6.4 | 24.1
    | 38.3 |'
- en: '| OHEM++[[40](#bib.bib40)] | trainval | VGG-16 | 25.5 | 45.9 | 26.1 | 7.4 |
    27.7 | 40.3 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| OHEM++[[40](#bib.bib40)] | trainval | VGG-16 | 25.5 | 45.9 | 26.1 | 7.4 |
    27.7 | 40.3 |'
- en: '| R-FCN[[42](#bib.bib42)] | trainval | ResNet-101 | 29.9 | 51.9 | - | 10.8
    | 32.8 | 45.0 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| R-FCN[[42](#bib.bib42)] | trainval | ResNet-101 | 29.9 | 51.9 | - | 10.8
    | 32.8 | 45.0 |'
- en: '| CoupleNet[[43](#bib.bib43)] | trainval | ResNet-101 | 34.4 | 54.8 | 37.2
    | 13.4 | 38.1 | 52.0 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| CoupleNet[[43](#bib.bib43)] | trainval | ResNet-101 | 34.4 | 54.8 | 37.2
    | 13.4 | 38.1 | 52.0 |'
- en: '| Faster R-CNN G-RMI[[44](#bib.bib44)] | ${-}$ | Inception-ResNet-v2 | 34.7
    | 55.5 | 36.7 | 13.5 | 38.1 | 52.0 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Faster R-CNN G-RMI[[44](#bib.bib44)] | ${-}$ | Inception-ResNet-v2 | 34.7
    | 55.5 | 36.7 | 13.5 | 38.1 | 52.0 |'
- en: '| Faster R-CNN+++[[25](#bib.bib25)] | trainval | ResNet-101-C4 | 34.9 | 55.7
    | 37.4 | 15.6 | 38.7 | 50.9 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Faster R-CNN+++[[25](#bib.bib25)] | trainval | ResNet-101-C4 | 34.9 | 55.7
    | 37.4 | 15.6 | 38.7 | 50.9 |'
- en: '| Faster R-CNN w FPN[[15](#bib.bib15)] | trainval35k | ResNet-101-FPN | 36.2
    | 59.1 | 39.0 | 18.2 | 39.0 | 48.2 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Faster R-CNN w FPN[[15](#bib.bib15)] | trainval35k | ResNet-101-FPN | 36.2
    | 59.1 | 39.0 | 18.2 | 39.0 | 48.2 |'
- en: '| Faster R-CNN w TDM[[45](#bib.bib45)] | trainval | Inception-ResNet-v2-TDM
    | 36.8 | 57.7 | 39.2 | 16.2 | 39.8 | 52.1 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Faster R-CNN w TDM[[45](#bib.bib45)] | trainval | Inception-ResNet-v2-TDM
    | 36.8 | 57.7 | 39.2 | 16.2 | 39.8 | 52.1 |'
- en: '| Deformable R-FCN[[38](#bib.bib38)] | trainval | Aligned-Inception-ResNet
    | 37.5 | 58.0 | 40.8 | 19.4 | 40.1 | 52.5 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Deformable R-FCN[[38](#bib.bib38)] | trainval | 对齐的 Inception-ResNet | 37.5
    | 58.0 | 40.8 | 19.4 | 40.1 | 52.5 |'
- en: '| ${{umd}_{-}}$det[[46](#bib.bib46)] | trainval | ResNet-101 | 40.8 | 62.4
    | 44.9 | 23.0 | 43.4 | 53.2 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ${{umd}_{-}}$det[[46](#bib.bib46)] | trainval | ResNet-101 | 40.8 | 62.4
    | 44.9 | 23.0 | 43.4 | 53.2 |'
- en: '| Cascade R-CNN[[47](#bib.bib47)] | trainval35k | ResNet-101-FPN | 42.8 | 62.1
    | 46.3 | 23.7 | 45.5 | 55.2 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Cascade R-CNN[[47](#bib.bib47)] | trainval35k | ResNet-101-FPN | 42.8 | 62.1
    | 46.3 | 23.7 | 45.5 | 55.2 |'
- en: '| SNIP[[48](#bib.bib48)] | trainval35k | DPN-98 | 45.7 | 67.3 | 51.1 | 29.3
    | 48.8 | 57.1 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| SNIP[[48](#bib.bib48)] | trainval35k | DPN-98 | 45.7 | 67.3 | 51.1 | 29.3
    | 48.8 | 57.1 |'
- en: '| Fitness-NMS[[49](#bib.bib49)] | trainval35k | ResNet-101 | 41.8 | 60.9 |
    44.9 | 21.5 | 45.0 | 57.5 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Fitness-NMS[[49](#bib.bib49)] | trainval35k | ResNet-101 | 41.8 | 60.9 |
    44.9 | 21.5 | 45.0 | 57.5 |'
- en: '| Mask R-CNN[[11](#bib.bib11)] | trainval35k | ResNeXt-101 | 39.8 | 62.3 |
    43.4 | 22.1 | 43.2 | 51.2 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Mask R-CNN[[11](#bib.bib11)] | trainval35k | ResNeXt-101 | 39.8 | 62.3 |
    43.4 | 22.1 | 43.2 | 51.2 |'
- en: '| DCNv2+Faster R-CNN[[39](#bib.bib39)] | train118k* | ResNet-101 | 44.8 | 66.3
    | 48.8 | 24.4 | 48.1 | 59.6 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| DCNv2+Faster R-CNN[[39](#bib.bib39)] | train118k* | ResNet-101 | 44.8 | 66.3
    | 48.8 | 24.4 | 48.1 | 59.6 |'
- en: '| G-RMI[[44](#bib.bib44)] | trainval32k | Ensemble of Five Models | 41.6 |
    61.9 | 45.4 | 23.9 | 43.5 | 54.9 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| G-RMI[[44](#bib.bib44)] | trainval32k | 五模型集成 | 41.6 | 61.9 | 45.4 | 23.9
    | 43.5 | 54.9 |'
- en: '| YOLOv2[[30](#bib.bib30)] | trainval35k | DarkNet-53 | 33.0 | 57.9 | 34.4
    | 18.3 | 35.4 | 41.9 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| YOLOv2[[30](#bib.bib30)] | trainval35k | DarkNet-53 | 33.0 | 57.9 | 34.4
    | 18.3 | 35.4 | 41.9 |'
- en: '| YOLOv3[[32](#bib.bib32)] | trainval35k | DarkNet-19 | 21.6 | 44.0 | 19.2
    | 5.0 | 22.4 | 35.5 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| YOLOv3[[32](#bib.bib32)] | trainval35k | DarkNet-19 | 21.6 | 44.0 | 19.2
    | 5.0 | 22.4 | 35.5 |'
- en: '| ${SSD300}^{*}$[[10](#bib.bib10)] | trainval35k | VGG-16 | 25.1 | 43.1 | 25.8
    | 6.6 | 22.4 | 35.5 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| ${SSD300}^{*}$[[10](#bib.bib10)] | trainval35k | VGG-16 | 25.1 | 43.1 | 25.8
    | 6.6 | 22.4 | 35.5 |'
- en: '| RON384+++[[50](#bib.bib50)] | trainval | VGG-16 | 27.4 | 49.5 | 27.1 | ${-}$
    | ${-}$ | ${-}$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| RON384+++[[50](#bib.bib50)] | trainval | VGG-16 | 27.4 | 49.5 | 27.1 | ${-}$
    | ${-}$ | ${-}$ |'
- en: '| SSD321[[34](#bib.bib34)] | trainval35k | ResNet-101 | 28.0 | 45.4 | 29.3
    | 6.2 | 28.3 | 49.3 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| SSD321[[34](#bib.bib34)] | trainval35k | ResNet-101 | 28.0 | 45.4 | 29.3
    | 6.2 | 28.3 | 49.3 |'
- en: '| DSSD321[[34](#bib.bib34)] | trainval35k | ResNet-101 | 28.0 | 46.1 | 29.2
    | 7.4 | 28.1 | 47.6 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| DSSD321[[34](#bib.bib34)] | trainval35k | ResNet-101 | 28.0 | 46.1 | 29.2
    | 7.4 | 28.1 | 47.6 |'
- en: '| SSD512*[[10](#bib.bib10)] | trainval35k | VGG-16 | 28.8 | 48.5 | 30.3 | 10.9
    | 31.8 | 43.5 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| SSD512*[[10](#bib.bib10)] | trainval35k | VGG-16 | 28.8 | 48.5 | 30.3 | 10.9
    | 31.8 | 43.5 |'
- en: '| SSD513[[34](#bib.bib34)] | trainval35k | ResNet-101 | 31.2 | 50.4 | 33.3
    | 10.2 | 34.5 | 49.8 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| SSD513[[34](#bib.bib34)] | trainval35k | ResNet-101 | 31.2 | 50.4 | 33.3
    | 10.2 | 34.5 | 49.8 |'
- en: '| DSSD513[[34](#bib.bib34)] | trainval35k | ResNet-101 | 33.2 | 53.3 | 35.2
    | 13.0 | 35.4 | 51.1 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| DSSD513[[34](#bib.bib34)] | trainval35k | ResNet-101 | 33.2 | 53.3 | 35.2
    | 13.0 | 35.4 | 51.1 |'
- en: '| RetinaNet500[[33](#bib.bib33)] | trainval35k | ResNet-101 | 34.4 | 53.1 |
    36.8 | 14.7 | 38.5 | 49.1 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| RetinaNet500[[33](#bib.bib33)] | trainval35k | ResNet-101 | 34.4 | 53.1 |
    36.8 | 14.7 | 38.5 | 49.1 |'
- en: '| RetinaNet800[[33](#bib.bib33)] | trainval35k | ResNet-101-FPN | 39.1 | 59.1
    | 42.3 | 21.8 | 42.7 | 50.2 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| RetinaNet800[[33](#bib.bib33)] | trainval35k | ResNet-101-FPN | 39.1 | 59.1
    | 42.3 | 21.8 | 42.7 | 50.2 |'
- en: '| M2Det512[[35](#bib.bib35)] | trainval35k | VGG-16 | 37.6 | 56.6 | 40.5 |
    18.4 | 43.4 | 51.2 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| M2Det512[[35](#bib.bib35)] | trainval35k | VGG-16 | 37.6 | 56.6 | 40.5 |
    18.4 | 43.4 | 51.2 |'
- en: '| M2Det512[[35](#bib.bib35)] | trainval35k | ResNet-101 | 38.8 | 59.4 | 41.7
    | 20.5 | 43.9 | 53.4 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| M2Det512[[35](#bib.bib35)] | trainval35k | ResNet-101 | 38.8 | 59.4 | 41.7
    | 20.5 | 43.9 | 53.4 |'
- en: '| M2Det800[[35](#bib.bib35)] | trainval35k | VGG-16 | 41.0 | 59.7 | 45.0 |
    22.1 | 46.5 | 53.8 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| M2Det800[[35](#bib.bib35)] | trainval35k | VGG-16 | 41.0 | 59.7 | 45.0 |
    22.1 | 46.5 | 53.8 |'
- en: '| RefineDet320[[36](#bib.bib36)] | trainval35k | VGG-16 | 29.4 | 49.2 | 31.3
    | 10.0 | 32.0 | 44.4 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| RefineDet320[[36](#bib.bib36)] | trainval35k | VGG-16 | 29.4 | 49.2 | 31.3
    | 10.0 | 32.0 | 44.4 |'
- en: '| RefineDet512[[36](#bib.bib36)] | trainval35k | VGG-16 | 33.0 | 54.5 | 35.5
    | 16.3 | 36.3 | 44.3 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| RefineDet512[[36](#bib.bib36)] | trainval35k | VGG-16 | 33.0 | 54.5 | 35.5
    | 16.3 | 36.3 | 44.3 |'
- en: '| RefineDet320[[36](#bib.bib36)] | trainval35k | ResNet-101 | 32.0 | 51.4 |
    34.2 | 10.5 | 34.7 | 50.4 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| RefineDet320[[36](#bib.bib36)] | trainval35k | ResNet-101 | 32.0 | 51.4 |
    34.2 | 10.5 | 34.7 | 50.4 |'
- en: '| RefineDet512[[36](#bib.bib36)] | trainval35k | ResNet-101 | 36.4 | 57.5 |
    39.5 | 16.6 | 39.9 | 51.4 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| RefineDet512[[36](#bib.bib36)] | trainval35k | ResNet-101 | 36.4 | 57.5 |
    39.5 | 16.6 | 39.9 | 51.4 |'
- en: '| RefineDet320+[[36](#bib.bib36)] | trainval35k | VGG-16 | 35.2 | 56.1 | 37.7
    | 19.5 | 37.2 | 47.0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| RefineDet320+[[36](#bib.bib36)] | trainval35k | VGG-16 | 35.2 | 56.1 | 37.7
    | 19.5 | 37.2 | 47.0 |'
- en: '| RefineDet512+[[36](#bib.bib36)] | trainval35k | VGG-16 | 37.6 | 58.7 | 40.8
    | 22.7 | 40.3 | 48.3 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| RefineDet512+[[36](#bib.bib36)] | trainval35k | VGG-16 | 37.6 | 58.7 | 40.8
    | 22.7 | 40.3 | 48.3 |'
- en: '| RefineDet320+[[36](#bib.bib36)] | trainval35k | ResNet-101 | 38.6 | 59.9
    | 41.7 | 21.1 | 41.7 | 52.3 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| RefineDet320+[[36](#bib.bib36)] | trainval35k | ResNet-101 | 38.6 | 59.9
    | 41.7 | 21.1 | 41.7 | 52.3 |'
- en: '| RefineDet512+[[36](#bib.bib36)] | trainval35k | ResNet-101 | 41.8 | 62.9
    | 45.7 | 25.6 | 45.1 | 54.1 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| RefineDet512+[[36](#bib.bib36)] | trainval35k | ResNet-101 | 41.8 | 62.9
    | 45.7 | 25.6 | 45.1 | 54.1 |'
- en: '| CornerNet512[[51](#bib.bib51)] | trainval35k | Hourglass | 40.5 | 57.8 |
    45.3 | 20.8 | 44.8 | 56.7 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| CornerNet512[[51](#bib.bib51)] | trainval35k | Hourglass | 40.5 | 57.8 |
    45.3 | 20.8 | 44.8 | 56.7 |'
- en: '| NAS-FPN[[18](#bib.bib18)] | trainval35k | RetinaNet | 45.4 | - | - | - |
    - | - |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| NAS-FPN[[18](#bib.bib18)] | trainval35k | RetinaNet | 45.4 | - | - | - |
    - | - |'
- en: '| NAS-FPN[[18](#bib.bib18)] | trainval35k | AmoebaNet | 48.0 | - | - | - |
    - | - |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| NAS-FPN[[18](#bib.bib18)] | trainval35k | AmoebaNet | 48.0 | - | - | - |
    - | - |'
- en: IV Datasets and metrics
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 数据集和指标
- en: Detecting an object has to state that an object belongs to a specified class
    and locate it in the image. The localization of an object is typically represented
    by a bounding box as shown in Fig. 5\. Using challenging datasets as benchmark
    is significant in many areas of research, because they are able to draw a standard
    comparison between different algorithms and set goals for solutions. Early algorithms
    focused on face detection using various ad hoc datasets. Later, more realistic
    and challenging face detection datasets were created. Another popular challenge
    is the detection of pedestrians for which several datasets have been created.
    The Caltech Pedestrian Dataset [[1](#bib.bib1)] contains 350,000 labeled instances
    with bounding boxes. General object detection datasets like PASCAL VOC [[4](#bib.bib4)],
    MS COCO [[5](#bib.bib5)], ImageNet-loc [[3](#bib.bib3)] are the mainstream benchmarks
    of object detection task. The official metrics are mainly adopted to measure the
    performance of detectors with corresponding dataset.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测需要确定一个对象属于指定类别并在图像中定位它。对象的定位通常通过一个边界框来表示，如图 5 所示。使用具有挑战性的数据集作为基准在许多研究领域中具有重要意义，因为它们能够在不同算法之间进行标准化比较并为解决方案设定目标。早期的算法专注于使用各种临时数据集进行人脸检测。后来，创建了更现实且具有挑战性的人脸检测数据集。另一个流行的挑战是行人检测，为此创建了几个数据集。Caltech
    行人数据集 [[1](#bib.bib1)] 包含 350,000 个带有边界框的标记实例。像 PASCAL VOC [[4](#bib.bib4)]、MS
    COCO [[5](#bib.bib5)]、ImageNet-loc [[3](#bib.bib3)] 这样的通用对象检测数据集是对象检测任务的主流基准。官方指标主要用于测量检测器在相应数据集上的性能。
- en: '![Refer to caption](img/9668e3b6f38765a50ed9a7b49d70e96c.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9668e3b6f38765a50ed9a7b49d70e96c.png)'
- en: 'Figure 5: The first two lines are examples from the MS COCO dataset [[5](#bib.bib5)].
    The images show three different types of images sampled in the dataset, including
    iconic objects, iconic scenes and non-iconic objects. In addition, the last two
    lines are annotated sample images from the PASCAL VOC dataset [[4](#bib.bib4)].'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：前两行是来自 MS COCO 数据集的示例 [[5](#bib.bib5)]。这些图像显示了数据集中采样的三种不同类型的图像，包括标志性对象、标志性场景和非标志性对象。此外，最后两行是来自
    PASCAL VOC 数据集 [[4](#bib.bib4)] 的标注示例图像。
- en: '![Refer to caption](img/6aebf54066bb0e1f76d671f62a4c63cf.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6aebf54066bb0e1f76d671f62a4c63cf.png)'
- en: 'Figure 6: A drone-based image with bounding box and category labels of objects.
    Image from VisDrone 2018 dataset [[7](#bib.bib7)].'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：一张带有边界框和类别标签的无人机拍摄的图像。图像来自 VisDrone 2018 数据集 [[7](#bib.bib7)]。
- en: IV-A PASCAL VOC dataset
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A PASCAL VOC 数据集
- en: IV-A1 Dataset
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 数据集
- en: For the detection of basic object categories, a multi-year effort from 2005
    to 2012 was devoted to the creation and maintenance of a series of benchmark datasets
    that were widely adopted. The PASCAL VOC datasets [[4](#bib.bib4)] contain 20
    object categories (in VOC2007, such as person, bicycle, bird, bottle, dog, etc.)
    spread over 11,000 images. The 20 categories can be considered as 4 main branches-vehicles,
    animals, household objects and people. Some of them increase semantic specificity
    of the output, such as car and motorbike, different types of vehicle, but not
    look similar. In addition, the visually similar classes increase the difficulty
    of detection, e.g. “dog” vs. “cat”. Over 27,000 object instance bounding boxes
    are labeled, of which almost 7,000 have detailed segmentations. Imbalanced datasets
    exist in the VOC2007 dataset, while the class “person” is definitely the biggest
    one, which is nearly 20 times more than the smallest class “sheep” in the training
    set. This problem is widespread in the surrounding scene and how can detectors
    solve this well? Another issue is viewpoint, such as, front, rear, left, right
    and unspecified, the detectors need to treat different viewpoints separately.
    Some annotated examples are showed in the last two lines of Fig. 5.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基础对象类别的检测，从 2005 年到 2012 年，进行了多年努力，创建和维护了一系列广泛采用的基准数据集。PASCAL VOC 数据集 [[4](#bib.bib4)]
    包含 20 个对象类别（在 VOC2007 中，例如人、自行车、鸟、瓶子、狗等），分布在 11,000 张图像中。这 20 个类别可以被认为是 4 个主要分支——车辆、动物、家庭物品和人。一些类别增加了输出的语义特异性，如汽车和摩托车，不同类型的车辆，但外观不相似。此外，视觉上相似的类别增加了检测难度，例如“狗”与“猫”。超过
    27,000 个对象实例的边界框被标注，其中近 7,000 个具有详细的分割。VOC2007 数据集中存在类别不平衡的问题，其中“人”类别无疑是最大的，几乎是训练集中最小类别“羊”的
    20 倍。这个问题在周围场景中普遍存在，检测器如何解决这个问题？另一个问题是视角，例如前、后、左、右和未指定，检测器需要分别处理不同的视角。图 5 的最后两行显示了一些标注示例。
- en: IV-A2 Metric
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 指标
- en: For the VOC2007 criteria, the interpolated average precision (Salton and McGill
    1986) was used to evaluate both classification and detection. It is designed to
    penalize the algorithm for missing object instances, for duplicate detections
    of one instance, and for false positive detections.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 VOC2007 标准，使用了插值平均精度（Salton 和 McGill 1986）来评估分类和检测。它旨在对漏检对象实例、对一个实例的重复检测和假阳性检测进行惩罚。
- en: '|  | $Recall(t)=\frac{\sum_{ij}{1[s_{ij}\geq{t}]}z_{ij}}{N}$ |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $Recall(t)=\frac{\sum_{ij}{1[s_{ij}\geq{t}]}z_{ij}}{N}$ |  |'
- en: '|  | $Precision(t)=\frac{\sum_{ij}{1[s_{ij}\geq{t}]}z_{ij}}{\sum_{ij}{1[s_{ij}\geq{t}]}}$
    |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | $Precision(t)=\frac{\sum_{ij}{1[s_{ij}\geq{t}]}z_{ij}}{\sum_{ij}{1[s_{ij}\geq{t}]}}$
    |  |'
- en: where $t$ is threshold to judge the IoU between predicted box and ground truth
    box. In VOC metric, $t$ is set to 0.5\. $i$ is the index of the i-th image while
    $j$ is the index of the j-th object. $N$ is the number of predicted boxes. The
    indicator function $1[s_{ij}\geq{t}]=1$ if $s_{ij}\geq{t}$ is true, 0 otherwise.
    If one detection is matched to a ground truth box according to the threshold criteria,
    it will be seen as a true positive result.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t$ 是判断预测框与真实框之间 IoU 的阈值。在 VOC 指标中，$t$ 设置为 0.5。$i$ 是第 i 张图像的索引，而 $j$ 是第 j
    个对象的索引。$N$ 是预测框的数量。如果 $s_{ij}\geq{t}$ 为真，则指示函数 $1[s_{ij}\geq{t}]=1$，否则为 0。如果一个检测根据阈值标准匹配到一个真实框，则会被视为真正的正例结果。
- en: For a given task and class, the precision/recall curve is computed from a method’s
    ranked output. Recall is defined as the proportion of all positive examples ranked
    above a given rank. Precision is the proportion of all examples above that rank
    which are from the positive class. The mean average precision across all categories
    is the ultimate results.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的任务和类别，精度/召回率曲线是从方法的排名输出中计算得出的。召回率定义为所有正例中排名高于给定排名的比例。精度是指高于该排名的所有例子中属于正类的比例。所有类别的平均精度是最终结果。
- en: IV-B MS COCO benchmark
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B MS COCO 基准
- en: IV-B1 Dataset
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 数据集
- en: The Microsoft Common Objects in Context (MS COCO) dataset [[5](#bib.bib5)] for
    detecting and segmenting objects found in everyday life in their natural environments
    contains 91 common object categories with 82 of them having more than 5,000 labeled
    instances. These categories cover the 20 categories in PASCAL VOC dataset. In
    total the dataset has 2,500,000 labeled instances in 328,000 images. MS COCO dataset
    also pays attention to varied viewpoints and all objects of it are in natural
    environments which gives us rich contextual information.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 微软的常见对象数据集（MS COCO）[[5](#bib.bib5)]用于检测和分割日常生活中自然环境中的物体，包含91个常见物体类别，其中82个类别拥有超过5,000个标记实例。这些类别涵盖了PASCAL
    VOC数据集中的20个类别。数据集总共有2,500,000个标记实例和328,000张图像。MS COCO数据集还关注不同的视角，所有物体都处于自然环境中，这为我们提供了丰富的上下文信息。
- en: In contrast to the popular ImageNet dataset [[3](#bib.bib3)], COCO has fewer
    categories but more instances per category. The dataset is also significantly
    larger in the number of instances per category (27k on average) than the PASCAL
    VOC datasets [[4](#bib.bib4)] (about 10 more times less than MS COCO dataset)
    and ImageNet object detection dataset (1k) [[3](#bib.bib3)]. MS COCO contains
    considerably more object instances per image (7.7) as compared to PASCAL VOC (2.3)
    and ImageNet (3.0). Furthermore, MS COCO dataset contains 3.5 categories per image
    as compared to PASCAL (1.4) and ImageNet (1.7) on average. In addition, 10% images
    in MS COCO have only one category, while in ImageNet and PASCAL VOC all have more
    than 60% of images contain a single object category. As we know, small objects
    need more contextual reasoning to recognize. Images among MS COCO dataset are
    rich in contextual information. The biggest class is also the “person”, nearly
    800,000 instances, while the smallest class is “hair driver”, about 600 instances
    in the whole dataset. Another small class is “hair brush” whose number is nearly
    800\. Except for 20 classes with many or few instances, the number of instances
    in the remaining 71 categories is roughly the same. Three typical categories of
    images in MS COCO dataset are showed in the first two lines of Fig. 5.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于流行的ImageNet数据集[[3](#bib.bib3)]，COCO的类别较少，但每个类别的实例更多。与PASCAL VOC数据集[[4](#bib.bib4)]（大约是MS
    COCO数据集的10倍）和ImageNet物体检测数据集（1k）[[3](#bib.bib3)]相比，该数据集每个类别的实例数量（平均27k）显著更大。MS
    COCO包含的每张图像中的物体实例数量（7.7）明显多于PASCAL VOC（2.3）和ImageNet（3.0）。此外，MS COCO数据集每张图像包含的类别数（平均3.5）也高于PASCAL（1.4）和ImageNet（1.7）。此外，MS
    COCO中的10%图像只有一个类别，而ImageNet和PASCAL VOC中的图像中有60%以上包含一个物体类别。众所周知，小物体需要更多的上下文推理来识别。MS
    COCO数据集中的图像信息丰富。最大类别是“人”，有近800,000个实例，而最小类别是“发夹”，整个数据集中约有600个实例。另一个小类别是“发刷”，其数量接近800。除去20个实例数量多或少的类别，其余71个类别的实例数量大致相同。图5的前两行显示了MS
    COCO数据集中的三个典型图像类别。
- en: IV-B2 Metric
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 指标
- en: MS COCO metric is under a strict manner and thoroughly judge the performance
    of detections. The threshold in PASCAL VOC is set to a single value, 0.5, but
    is belong to [0.5,0.95] with an interval 0.05 that is 10 values to calculate the
    mean average precision in MS COCO. Apart from that, the special average precision
    for small, medium and large objects are calculated separately to measure the performance
    of the detector in detecting targets of different sizes.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: MS COCO指标在严格的标准下全面评估检测性能。PASCAL VOC中的阈值设置为一个固定值0.5，而MS COCO中的阈值范围为[0.5,0.95]，步长为0.05，即10个值来计算平均精度。此外，还单独计算小型、中型和大型物体的特殊平均精度，以衡量检测器在检测不同尺寸目标方面的性能。
- en: IV-C ImageNet benchmark
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C ImageNet 基准
- en: IV-C1 Dataset
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 数据集
- en: Challenging datasets can encourage a step forward of vision tasks and practical
    applications. Another important large-scale benchmark dataset is ImageNet dataset
    [[3](#bib.bib3)]. The ILSVRC task of object detection evaluates the ability of
    an algorithm to name and localize all instances of all target objects present
    in an image. ILSVRC2014 has 200 object classes and nearly 450k training images,
    20k validation images and 40k test images. More comparisons with PASCAL VOC are
    in Table 3.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 富有挑战性的数据集可以推动视觉任务和实际应用的发展。另一个重要的大规模基准数据集是ImageNet数据集 [[3](#bib.bib3)]。ILSVRC的物体检测任务评估算法命名和定位图像中所有目标物体实例的能力。ILSVRC2014有200个物体类别和近45万张训练图像，2万张验证图像和4万张测试图像。与PASCAL
    VOC的更多比较见表3。
- en: 'TABLE III: comparison between ILSVRC object detection dataset and PASCAL VOC
    dataset'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：ILSVRC物体检测数据集与PASCAL VOC数据集的比较
- en: '| Dataset | Classes | Fully annotated training images | Training objects |
    Val images | Val objects | Annotated obj/im |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类别 | 完全标注的训练图像 | 训练物体 | 验证图像 | 验证物体 | 每张图像的标注物体 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| PASCAL VOC | 20 | 5717 | 13609 | 5823 | 15787 | 2.7 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| PASCAL VOC | 20 | 5717 | 13609 | 5823 | 15787 | 2.7 |'
- en: '| ILSVRC | 200 | 60658 | 478807 | 20121 | 55501 | 2.8 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| ILSVRC | 200 | 60658 | 478807 | 20121 | 55501 | 2.8 |'
- en: IV-C2 Metric
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 指标
- en: 'The PASCAL VOC metric uses the threshold t = 0.5\. However, for small objects
    even deviations of a few pixels would be unacceptable according to this threshold.
    ImageNet uses a loosen threshold calculated as:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: PASCAL VOC指标使用阈值 t = 0.5。然而，对于小型物体，即使是几个像素的偏差也会根据该阈值被认为是不可接受的。ImageNet使用了一个较宽松的阈值，计算公式为：
- en: '|  | $t=min(0.5,\frac{wh}{(w+10)(h+10)})$ |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $t=min(0.5,\frac{wh}{(w+10)(h+10)})$ |  |'
- en: where $w$ and $h$ are width and height of a ground truth box respectively. This
    threshold allows for the annotation to extend up to 5 pixels on average in each
    direction around the object.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $w$ 和 $h$ 分别是地面真实框的宽度和高度。这个阈值允许每个方向上的标注在物体周围扩展最多5像素。
- en: IV-D VisDrone2018 benchmark
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D VisDrone2018基准测试
- en: Last year, a new dataset consists of images and videos captured by drones, called
    VisDrone2018 [[7](#bib.bib7)], a large-scale visual object detection and tracking
    benchmark dataset. This dataset aims at advancing visual understanding tasks on
    the drone platform. The images and video sequences in the benchmark were captured
    over various urban/suburban areas of 14 different cities across China from north
    to south. Specifically, VisDrone2018 consists of 263 video clips and 10,209 images
    (no overlap with video clips) with rich annotations, including object bounding
    boxes, object categories, occlusion, truncation ratios, etc. This benchmark has
    more than 2.5 million annotated instances in 179,264 images/video frames.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 去年，一个由无人机拍摄的图像和视频组成的新数据集，称为VisDrone2018 [[7](#bib.bib7)]，这是一个大规模的视觉物体检测和跟踪基准测试数据集。该数据集旨在推动无人机平台上的视觉理解任务。基准中的图像和视频序列是在中国南北14个不同城市的各种城市/郊区地区拍摄的。具体而言，VisDrone2018包含263个视频片段和10,209张图像（与视频片段不重叠），具有丰富的注释，包括物体边界框、物体类别、遮挡、截断比例等。这个基准测试在179,264张图像/视频帧中有超过250万的标注实例。
- en: Being the larger such dataset ever published, the benchmark enables extensive
    evaluation and investigation of visual analysis algorithms on the drone platform.
    VisDrone2018 has a large amount of small objects, such as dense cars, pedestrians
    and bicycles, which will cause difficult detection about certain categories. Moreover,
    a large proportion of the images in this dataset have more than 20 objects per
    image, 82.4% in training set, and the average number of objects per image is 54
    in 6471 images of training set. This dataset contains dark night scenes so the
    brightness of these images lower than those in day time, which complicates the
    correct detection of small and dense objects, as shown in Fig. 6\. This dataset
    adopts MS COCO metric.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 作为发布的最大数据集之一，这个基准测试能够对无人机平台上的视觉分析算法进行广泛的评估和研究。VisDrone2018包含大量小型物体，如密集的汽车、行人和自行车，这会导致某些类别的检测困难。此外，该数据集中大比例的图像每张图像中有超过20个物体，在训练集中占82.4%，训练集的6471张图像中每张图像的平均物体数量为54。该数据集包含了黑暗的夜晚场景，因此这些图像的亮度低于白天，这使得对小型和密集物体的正确检测更加复杂，如图6所示。该数据集采用了MS
    COCO指标。
- en: IV-E Open Images V5
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 开放图像 V5
- en: IV-E1 Dataset
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-E1 数据集
- en: Open Images [[6](#bib.bib6)] is a dataset of 9.2M images annotated with image-level
    labels, object bounding boxes, object segmentation masks, and visual relationships.
    Open Images V5 contains a total of 16M bounding boxes for 600 object classes on
    1.9M images, which makes it the largest existing dataset with object location
    annotations. First, the boxes in this dataset have been largely manually drawn
    by professional annotators (Google-internal annotators) to ensure accuracy and
    consistency. Second, the images in it are very diverse and mostly contain complex
    scenes with several objects (8.3 per image on average). Third, this dataset offers
    visual relationship annotations, indicating pairs of objects in particular relations
    (e.g. ”woman playing guitar”, ”beer on table”). In total it has 329 relationship
    triplets with 391,073 samples. Fourth, V5 provides segmentation masks for 2.8M
    object instances in 350 classes. Segmentation masks mark the outline of objects,
    which characterizes their spatial extent to a much higher level of detail. Finally,
    the dataset is annotated with 36.5M image-level labels spanning 19,969 classes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Open Images [[6](#bib.bib6)] 是一个包含920万张图像的数据集，标注了图像级标签、对象边界框、对象分割掩码和视觉关系。Open
    Images V5包含了总计1600万的边界框，覆盖600个对象类别，涉及190万张图像，使其成为现存最大的数据集，提供了对象位置的注释。首先，该数据集中的边界框主要由专业注释员（Google内部注释员）手动绘制，以确保准确性和一致性。其次，其中的图像非常多样化，大多包含复杂的场景和多个对象（每张图像平均8.3个）。第三，该数据集提供了视觉关系注释，指示了特定关系中的对象对（例如“女人弹吉他”，“桌上的啤酒”）。总共包含329个关系三元组，391,073个样本。第四，V5为350个类别的280万个对象实例提供了分割掩码。分割掩码标记对象的轮廓，详细描述了其空间范围。最后，该数据集被标注了3650万个图像级标签，涵盖19,969个类别。
- en: IV-E2 Metric
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-E2 指标
- en: On the basis of PASCAL VOC 2012 mAP evaluation metric, Kuznetsova et al. propose
    several modifications to consider thoroughly of some important aspects of the
    Open Images Dataset. First, for fair evaluation, the unannotated classes are ignored
    to avoid wrongly counted as false negatives. Second, if an object belongs to a
    class and a subclass, an object detection model should give a detection result
    for each of the relevant classes. The absence of one of these classes would be
    considered a false negative in that class. Third, in Open Images Dataset, there
    exists group-of boxes which contain a group of (more than one which are occluding
    each other or physically touching) object instances but unknown a single object
    localization inside them. If a detection inside a group-of box and the intersection
    of the detection and the box divided by the area of the detection is larger than
    0.5, the detection will be counted as a true positive. Multiple correct detections
    inside the same group-of box only count one valid true positive.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 基于PASCAL VOC 2012的mAP评估指标，Kuznetsova等人提出了几项修改，以全面考虑Open Images数据集的一些重要方面。首先，为了公平评估，忽略未标注的类别，以避免误算为假阴性。其次，如果一个对象属于一个类别和一个子类别，则对象检测模型应为每个相关类别提供检测结果。缺少其中一个类别将被视为该类别的假阴性。第三，在Open
    Images数据集中，存在包含一组（相互遮挡或物理接触的多个）对象实例的“组框”，但未知组内单个对象的定位。如果检测在“组框”内，且检测与框的交集除以检测区域的比率大于0.5，则该检测将被计为真正的正例。多个正确的检测在同一“组框”内仅计为一个有效的真正正例。
- en: IV-F Pedestrian detection datasets
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F 行人检测数据集
- en: Table 4 and table 5 list the comparison between several people detection benchmarks
    and pedestrian detection datasets, respectively.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 表4和表5列出了几个行人检测基准和行人检测数据集的比较。
- en: 'TABLE IV: Comparison of person detection benchmarks,* Images in EuroCity Persons
    benchmark have day and night collections, which use ”/” to split the number of
    day and night. Table information from Markus Braun et al. IEEE TPAMI2019[[52](#bib.bib52)]'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表IV：行人检测基准的比较，*EuroCity Persons基准中的图像具有白天和夜晚的集合，使用“/”分隔白天和夜晚的数量。表格信息来自Markus
    Braun等人 IEEE TPAMI2019[[52](#bib.bib52)]*
- en: '| Dataset | countries | cities | seasons | images | pedestrians | resolution
    | weather | train-cal-test-split(%) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 国家 | 城市 | 季节 | 图像 | 行人 | 分辨率 | 天气 | 训练-验证-测试拆分(%) |'
- en: '| Caltech[[1](#bib.bib1)] | 1 | 1 | 1 | 249884 | 289395 | ${640}\times{480}$
    | dry | 50-0-50 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 加州理工学院[[1](#bib.bib1)] | 1 | 1 | 1 | 249884 | 289395 | ${640}\times{480}$
    | 干燥 | 50-0-50 |'
- en: '| KITTI[[2](#bib.bib2)] | 1 | 1 | 1 | 14999 | 9400 | ${1240}\times{376}$ |
    dry | 50-0-50 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| KITTI[[2](#bib.bib2)] | 1 | 1 | 1 | 14999 | 9400 | ${1240}\times{376}$ |
    干燥 | 50-0-50 |'
- en: '| CityPersons[[53](#bib.bib53)] | 3 | 27 | 3 | 5000 | 31514 | ${2048}\times{1024}$
    | dry | 60-10-30 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| CityPersons[[53](#bib.bib53)] | 3 | 27 | 3 | 5000 | 31514 | ${2048}\times{1024}$
    | 干燥 | 60-10-30 |'
- en: '| TDC[[54](#bib.bib54)] | 1 | 1 | 1 | 14674 | 8919 | ${2048}\times{1024}$ |
    dry | 71-8-21 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| TDC[[54](#bib.bib54)] | 1 | 1 | 1 | 14674 | 8919 | ${2048}\times{1024}$ |
    干燥 | 71-8-21 |'
- en: '| EuroCity Persons[[52](#bib.bib52)] | 12 | 31 | 4 | 40217/7118* | 183004/35309*
    | ${1920}\times{1024}$ | dry, wet | 60-10-30 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| EuroCity Persons[[52](#bib.bib52)] | 12 | 31 | 4 | 40217/7118* | 183004/35309*
    | ${1920}\times{1024}$ | 干燥，湿润 | 60-10-30 |'
- en: 'TABLE V: Comparison of pedestrian detection datasets. The 3rd, 4th, 5th are
    training set. The 6th, 7th, 8th are test set. Table information from Piotr et
    al. IEEE TPAMI2012 [[1](#bib.bib1)]'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：行人检测数据集的比较。第3、第4、第5列为训练集。第6、第7、第8列为测试集。表格信息来源于Piotr等人，IEEE TPAMI2012 [[1](#bib.bib1)]。
- en: '| Dataset | imaging setup | pedestrians | neg. images | pos. images | pedestrians
    | neg. images | pos. images |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 成像设置 | 行人 | 负样本图像 | 正样本图像 | 行人 | 负样本图像 | 正样本图像 |'
- en: '| Caltech[[1](#bib.bib1)] | mobile | 192k | 61k | 67k | 155k | 56k | 65k |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Caltech[[1](#bib.bib1)] | 移动 | 192k | 61k | 67k | 155k | 56k | 65k |'
- en: '| INRIA[[55](#bib.bib55)] | photo | 1208 | 1218 | 614 | 566 | 453 | 288 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| INRIA[[55](#bib.bib55)] | 照片 | 1208 | 1218 | 614 | 566 | 453 | 288 |'
- en: '| ETH[[56](#bib.bib56)] | mobile | 2388 | - | 499 | 12k | - | 1804 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| ETH[[56](#bib.bib56)] | 移动 | 2388 | - | 499 | 12k | - | 1804 |'
- en: '| TUD-Brussels[[57](#bib.bib57)] | mobile | 1776 | 218 | 1092 | 1498 | - |
    508 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| TUD-Brussels[[57](#bib.bib57)] | 移动 | 1776 | 218 | 1092 | 1498 | - | 508
    |'
- en: '| Daimler-DB[[58](#bib.bib58)] | mobile | 192k | 61k | 67k | 155k | 56k | 65k
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Daimler-DB[[58](#bib.bib58)] | 移动 | 192k | 61k | 67k | 155k | 56k | 65k |'
- en: V Analysis of general image object detection methods
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 一般图像目标检测方法分析
- en: Deep neural network based object detection pipelines have four steps in general,
    image pre-processing, feature extraction, classification and localization, post-processing.
    Firstly, raw images from the dataset can’t be fed into the network directly. Therefore,
    we need to resize them to any special sizes and make them clearer, such as enhancing
    brightness, color, contrast. Data augmentation is also available to meet some
    requirements, such as flipping, rotation, scaling, cropping, translation, adding
    Gaussian noise. In addition, GANs [[59](#bib.bib59)] (generative adversarial networks)
    can generate new images to enrich the diversity of input according to people’s
    needs. For more details about data augmentation, please refer to [[60](#bib.bib60)]
    for more details. Secondly, feature extraction is a key step for further detection.
    The feature quality directly determines the upper bound of subsequent tasks like
    classification and localization. Thirdly, the detector head is responsible to
    propose and refine bounding box concluding classification scores and bounding
    box coordinates. Fig. 1 illustrates the basic procedure of the second and the
    third step. At last, the post-processing step deletes any weak detecting results.
    For example, NMS is a widely used method in which the highest scoring object deletes
    its nearby objects with inferior classification scores.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度神经网络的目标检测管道一般包括四个步骤：图像预处理、特征提取、分类与定位、后处理。首先，数据集中原始图像不能直接输入网络。因此，我们需要将图像调整为特定的尺寸并使其更清晰，比如增强亮度、颜色、对比度。数据增强也可以满足一些需求，例如翻转、旋转、缩放、裁剪、平移、添加高斯噪声。此外，生成对抗网络（GANs）[[59](#bib.bib59)]可以根据需求生成新图像，以丰富输入的多样性。有关数据增强的更多细节，请参考[[60](#bib.bib60)]。其次，特征提取是进一步检测的关键步骤。特征质量直接决定了后续任务如分类和定位的上限。第三，检测头负责提出和优化边界框，包括分类分数和边界框坐标。图1展示了第二步和第三步的基本过程。最后，后处理步骤删除任何弱检测结果。例如，NMS是一种广泛使用的方法，其中得分最高的对象会删除其附近得分较低的对象。
- en: To obtain precise detection results, there exists several methods can be used
    alone or in combination with other methods.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得准确的检测结果，存在多种方法可以单独使用或与其他方法结合使用。
- en: V-A Enhanced features
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 增强特征
- en: Extracting effective features from input images is a vital prerequisite for
    further accurate classification and localization steps. To fully utilize the output
    feature maps of consecutive backbone layers, Lin et al. [[15](#bib.bib15)] aim
    to extract richer features by dividing them into different levels to detect objects
    of different sizes, as shown in Fig. 3 (d). Some works [[11](#bib.bib11)] [[33](#bib.bib33)]
    [[61](#bib.bib61)] [[62](#bib.bib62)] utilize FPN as their multi-level feature
    pyramid backbone. Furthermore, a series of improved FPN [[18](#bib.bib18)] [[35](#bib.bib35)]
    [[63](#bib.bib63)] enriching features for detection task. Kim et al. [[64](#bib.bib64)]
    propose a parallel feature pyramid (FP) network (PFPNet), where the FP is constructed
    by widening the network width instead of increasing the network depth. The additional
    feature transformation operation is to generate a pool of feature maps with different
    sizes, which yields the feature maps with similar levels of semantic abstraction
    across the scales. Li et al. [[65](#bib.bib65)] concatenate features from different
    layers with different scales and then generates new feature pyramid to feed into
    multibox detectors predicting the final detection results. Chen et al. [[66](#bib.bib66)]
    introduce WeaveNet which iteratively weaves context information from adjacent
    scales together to enable more sophisticated context reasoning. Zheng et al. [[67](#bib.bib67)]
    extend better context information for the shallow layers of one-stage detector
    [[10](#bib.bib10)].
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入图像中提取有效特征是进一步准确分类和定位步骤的关键前提。为了充分利用连续骨干层的输出特征图，Lin等人[[15](#bib.bib15)]旨在通过将特征图划分为不同的层次以检测不同尺寸的物体，从而提取更丰富的特征，如图3
    (d)所示。一些研究[[11](#bib.bib11)] [[33](#bib.bib33)] [[61](#bib.bib61)] [[62](#bib.bib62)]利用FPN作为其多层次特征金字塔骨干。此外，一系列改进的FPN[[18](#bib.bib18)]
    [[35](#bib.bib35)] [[63](#bib.bib63)]丰富了检测任务中的特征。Kim等人[[64](#bib.bib64)]提出了一种并行特征金字塔（FP）网络（PFPNet），其中FP通过加宽网络宽度而不是增加网络深度来构建。额外的特征变换操作生成具有不同大小的特征图池，这样可以在不同尺度上产生具有类似语义抽象层次的特征图。Li等人[[65](#bib.bib65)]将来自不同层次的不同尺度的特征进行拼接，然后生成新的特征金字塔，以供多框检测器预测最终检测结果。Chen等人[[66](#bib.bib66)]引入了WeaveNet，它通过迭代地将来自相邻尺度的上下文信息编织在一起，以实现更复杂的上下文推理。Zheng等人[[67](#bib.bib67)]为单阶段检测器[[10](#bib.bib10)]的浅层扩展了更好的上下文信息。
- en: Semantic relationships between different objects or regions of an image can
    help detect occluded and small objects. Bae et al. [[68](#bib.bib68)] utilize
    the combined and high-level semantic features for object classification and localization
    which combine the multi-region features stage by stage. Zhang et al. [[36](#bib.bib36)]
    combine a semantic segmentation branch and a global activation module to enrich
    the semantics of object detection features within a typical deep detector. Scene
    contextual relations [[69](#bib.bib69)] can provide some useful information for
    accurate visual recognition. Liu et al. [[70](#bib.bib70)] adopt scene contextual
    information to further improve accuracy. Modeling relations between objects can
    help object detection. Singh et al. [[71](#bib.bib71)] process context regions
    around the ground-truth object on an appropriate scale. Hu et al. [[37](#bib.bib37)]
    propose a relation module that processes a set of objects simultaneously considering
    both appearance and geometry features through interaction. Mid-level semantic
    properties of objects can benefit object detection containing visual attributes
    [[72](#bib.bib72)].
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 不同物体或图像区域之间的语义关系可以帮助检测被遮挡的小物体。Bae等人[[68](#bib.bib68)]利用结合的高级语义特征进行物体分类和定位，这些特征逐步结合了多区域特征。Zhang等人[[36](#bib.bib36)]将语义分割分支和全局激活模块结合在一起，以丰富典型深度检测器中物体检测特征的语义。场景上下文关系[[69](#bib.bib69)]可以为准确的视觉识别提供有用的信息。Liu等人[[70](#bib.bib70)]采用场景上下文信息进一步提高准确性。建模物体之间的关系可以帮助物体检测。Singh等人[[71](#bib.bib71)]在适当的尺度上处理地面真值物体周围的上下文区域。Hu等人[[37](#bib.bib37)]提出了一个关系模块，该模块同时处理一组物体，考虑到外观和几何特征的互动。物体的中级语义属性可以有助于包含视觉属性的物体检测[[72](#bib.bib72)]。
- en: Attention mechanism is an effective method for networks focusing on the most
    significant region part. Some typical works [[73](#bib.bib73)][[74](#bib.bib74)][[75](#bib.bib75)][[76](#bib.bib76)][[77](#bib.bib77)][[78](#bib.bib78)][[79](#bib.bib79)]
    focus on attention mechanism so as to capture more useful features what detecting
    objects need. Kong et al. [[80](#bib.bib80)] design an architecture combining
    both global attention and local reconfigurations to gather task-oriented features
    across different spatial locations and scales.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制是网络关注最重要区域部分的有效方法。一些典型的工作[[73](#bib.bib73)][[74](#bib.bib74)][[75](#bib.bib75)][[76](#bib.bib76)][[77](#bib.bib77)][[78](#bib.bib78)][[79](#bib.bib79)]
    关注注意机制以捕获检测对象所需的更多有用特征。Kong等[[80](#bib.bib80)] 设计了一种结合全局注意力和局部重新配置的架构，以在不同空间位置和尺度上收集面向任务的特征。
- en: Fully utilizing the effective region of one object can promote the accuracy.
    Original ConvNets only focus on features of fixed square size (according to the
    kernel), thus the receptive field does not properly cover the entire pixel of
    a target object to represent it well. The deformable ConvNets can produce deformable
    kernel and the offset from the initial convolution kernel (of fixed size) are
    learned from the networks. Deformable RoI Pooling can also adapt to part location
    for objects with different shapes. In [[38](#bib.bib38)] [[39](#bib.bib39)], network
    weights and sampling locations jointly determine the effective support region.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 充分利用对象的有效区域可以提高准确性。原始的ConvNets仅关注固定方形尺寸（根据内核）的特征，因此感受野未能很好地覆盖目标对象的整个像素以进行良好的表示。可变形ConvNets可以生成可变形内核，并从网络中学习与初始卷积内核（固定大小）的偏移量。可变形RoI
    Pooling还可以适应不同形状对象的部分位置。在[[38](#bib.bib38)] [[39](#bib.bib39)]中，网络权重和采样位置共同决定有效支持区域。
- en: Above all, richer and proper representations of an object can promote detection
    accuracy remarkably. Brain-inspired mechanism is a powerful way to further improve
    detection performance.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，更丰富和适当的对象表示可以显著提高检测准确性。受脑机制启发的机制是一种有效的方式来进一步提升检测性能。
- en: V-B Increasing localization accuracy
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 提高定位准确性
- en: Localization and classification are two missions of object detection. Under
    object detection evaluation metrics, the precision of localization is a vital
    measurable indicator, thus increasing localization accuracy can promote detection
    performance remarkably. Designing a novel loss function to measure the accuracy
    of predicted boxes is an effective way to increase localization accuracy. Considering
    intersection over union (IoU) is the most commonly used evaluation metric of object
    detection, estimating regression quality can judge the IoU between predicted bounding
    box and its corresponding assignment ground truth box. For two bounding boxes,
    IoU can be calculated as the intersection area divided by the union area.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 定位和分类是对象检测的两个任务。在对象检测评估指标下，定位的精确度是一个重要的可测量指标，因此提高定位准确性可以显著提升检测性能。设计一种新颖的损失函数来衡量预测框的准确性是一种有效的方法来提高定位准确性。考虑到交并比（IoU）是对象检测中最常用的评估指标，估计回归质量可以判断预测边界框与其对应的真实框之间的IoU。对于两个边界框，IoU可以计算为交集面积除以并集面积。
- en: '|  | $IoU=\frac{bbox\cap{gt}}{bbox\cup{gt}}$ |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $IoU=\frac{bbox\cap{gt}}{bbox\cup{gt}}$ |  |'
- en: A typical work [[81](#bib.bib81)] adopts IoU loss to measure the degree of accuracy
    the network predicting. This loss function is robust to varied shapes and scales
    of different objects and can converge well in a short time. Rezatofighi et al.
    [[82](#bib.bib82)] incorporate generalized IoU as a loss function and a new metric
    into existing object detection pipeline which makes a consistent improvement than
    the original smooth L1 loss counterpart. Tychsen et al. [[49](#bib.bib49)] adopt
    a novel bounding box regression loss for localization branch. IoU loss in this
    research considers the intersection over union between predicted box and assigned
    ground truth box which is higher than a preset threshold but not concludes only
    the highest one. He et al. [[83](#bib.bib83)] propose a novel bounding box regression
    loss for learning bounding box localization and transformation variance together.
    He et al. [[84](#bib.bib84)] introduce a novel bounding box regression loss which
    has a strong connection to localization accuracy. Pang et al. [[63](#bib.bib63)]
    propose a novel balanced L1 Loss to further improve localization accuracy. Cabriel
    et al. [[85](#bib.bib85)] present Axially Localized Detection method to achieve
    a very high localization precision at the cellular level.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一项典型的工作[[81](#bib.bib81)]采用IoU损失来测量网络预测的准确度。这种损失函数对不同物体的形状和尺度变化具有鲁棒性，并且可以在短时间内良好收敛。Rezatofighi等人[[82](#bib.bib82)]将广义IoU作为损失函数和新指标纳入现有的目标检测流程，使其相较于原始的平滑L1损失函数有了持续的改进。Tychsen等人[[49](#bib.bib49)]为定位分支采用了一种新颖的边界框回归损失。此研究中的IoU损失考虑了预测框和分配的真实框之间的交并比，其值高于预设阈值，但不只考虑最高的一个。何等人[[83](#bib.bib83)]提出了一种新颖的边界框回归损失，用于同时学习边界框定位和变换方差。何等人[[84](#bib.bib84)]引入了一种新颖的边界框回归损失，与定位准确度有很强的关联。庞等人[[63](#bib.bib63)]提出了一种新型平衡L1损失，以进一步提高定位准确度。Cabriel等人[[85](#bib.bib85)]提出了轴向定位检测方法，以在细胞层面实现非常高的定位精度。
- en: In general, researchers design new loss function of localization branch to make
    the retained predictions more accurate.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，研究人员设计新的定位分支损失函数，以使保留的预测更为准确。
- en: V-C Solving negatives-positives imbalance issue
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 解决负样本与正样本不平衡问题
- en: In the first stage, that networks produce proposals and filter out a large number
    of negative samples are mainly well designed steps of two-stage detectors. When
    feed into the detector the proposal bounding boxes belong to a sparse set. However,
    in a one-stage detector, the network has no steps to filter out bad samples, thus
    the dense sample sets are difficult to train. The proportion of positive and negative
    samples is extremely unbalanced as well. The typical solution is hard negative
    mining [[86](#bib.bib86)]. The popularized hard mining methods OHEM [[40](#bib.bib40)]
    can help drive the focus towards hard samples. Liu et al. [[10](#bib.bib10)] adopt
    hard negative mining method which sorts all of the negative samples using the
    highest confidence loss for each pre-defined boxes and picking the top ones to
    make the ratio between the negative and positive samples at most 3:1\. Considering
    hard samples is more effective to improve the detection performance when training
    an object detector. Pang et al. [[63](#bib.bib63)] propose a novel hard mining
    method called IoU-balanced sampling. Yu et al. [[87](#bib.bib87)] concentrate
    on real-time requirements.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，网络生成提议并过滤掉大量负样本是两个阶段检测器的主要设计步骤。当提议边界框输入检测器时，它们属于一个稀疏集合。然而，在单阶段检测器中，网络没有步骤来过滤掉不良样本，因此密集的样本集合难以训练。正样本和负样本的比例也极为不平衡。典型的解决方案是困难负样本挖掘[[86](#bib.bib86)]。流行的困难挖掘方法OHEM[[40](#bib.bib40)]可以帮助将重点集中在困难样本上。刘等人[[10](#bib.bib10)]采用困难负样本挖掘方法，通过对每个预定义框的负样本使用最高置信度损失进行排序，挑选出前几个样本，使负样本和正样本的比例最多为3:1。考虑困难样本在训练目标检测器时更有效地提高检测性能。庞等人[[63](#bib.bib63)]提出了一种称为IoU平衡采样的新型困难挖掘方法。于等人[[87](#bib.bib87)]专注于实时需求。
- en: Another effective way is adding some items in classification loss function.
    Lin et al. [[33](#bib.bib33)] propose a loss function, called focal loss, which
    can down-weight the loss assigned to well-classified or easy examples, focusing
    on the hard training examples and avoiding the vast number of easy negative examples
    that overwhelm the detector during training. Chen et al. [[88](#bib.bib88)] consider
    designing a novel ranking task to replace the conventional classification task
    and a newly Average-Precision loss for this task, which can alleviate the extreme
    negative-positive class imbalance issue remarkably.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有效的方法是在分类损失函数中添加一些项目。林等人[[33](#bib.bib33)]提出了一种称为焦点损失的损失函数，可以降低对分类良好或易样本分配的损失，聚焦于难的训练样本，并避免训练期间轻松负样本的大量数量压倒探测器。陈等人[[88](#bib.bib88)]考虑设计一项新的排名任务来替代传统的分类任务，以及针对该任务的新的平均精确损失，可以显著缓解极端的负正类别不平衡问题。
- en: V-D Improving post-processing NMS methods
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 改进后处理 NMS 方法
- en: Only one detected object can be successfully matched to a ground truth object
    which will be preserved as a result, while others matched to it are classified
    as duplicate. NMS (non-maximum suppression) is a heuristic method which selects
    only the object of the highest classification score, otherwise the object will
    be ignored. Hu et al. [[37](#bib.bib37)] use the intermediate results produced
    by relation module to better determine which object will be saved while it does
    not need NMS. NMS considers the classification score but the localization confidence
    is absent, which causes less accurate in deleting weak results. Jiang et al. [[89](#bib.bib89)]
    propose IoU-Net learning to predict the IoU between each detected bounding box
    and the matched ground-truth. Because of its consideration of localization confidence,
    it improves the NMS method by preserving accurately localized bounding boxes.
    Tychsen et al. [[49](#bib.bib49)] present a novel fitness NMS method which considers
    both greater estimated IoU overlap and classification score of predicted bounding
    boxes. Liu et al. [[90](#bib.bib90)] propose adaptive-NMS which applies a dynamic
    suppression threshold to an instance decided by the target density. Bodla et al.
    [[46](#bib.bib46)] adopt an improved NMS method without any extra training and
    is simple to implement. He et al. [[84](#bib.bib84)] further improve soft-NMS
    method. Jan et al. [[91](#bib.bib91)] feed network score maps resulting from NMS
    at multiple IoU thresholds. Hosang et al. [[92](#bib.bib92)] design a novel ConvNets
    which does NMS directly without a subsequent post-processing step. Yu et al. [[87](#bib.bib87)]
    utilize the final feature map to filter out easy samples so the network concentrates
    on hard samples.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 仅成功匹配到一个检测到的对象与一个保存的地面真相对象，而其他与之匹配的对象被分类为重复。NMS（非最大抑制）是一种启发式方法，仅选择具有最高分类得分的对象，否则对象将被忽略。胡等人[[37](#bib.bib37)]利用关系模块产生的中间结果来更好地确定将保留的对象，而不需要NMS。NMS考虑了分类得分，但忽略了定位置信度，这导致删除弱结果时不够准确。姜等人[[89](#bib.bib89)]提出了IoU-Net学习，以预测每个检测到的边界框与匹配的基准真值之间的IoU。由于它考虑到了定位置信度，它通过准确保留定位的边界框来改进NMS方法。Tychsen等[[49](#bib.bib49)]提出了一种新颖的适应性NMS方法，该方法同时考虑了预测边界框的更大的估计IoU重叠和分类得分。刘等人[[90](#bib.bib90)]提出了自适应NMS，该方法对由目标密度决定的实例应用动态抑制阈值。Bodla等人[[46](#bib.bib46)]采用了一种改进的NMS方法，无需额外的训练且易于实现。何等人[[84](#bib.bib84)]进一步改进了软NMS方法。Jan等人[[91](#bib.bib91)]在多个IoU阈值下向网络分数映射提供NMS的结果。Hosang等人[[92](#bib.bib92)]设计了一种新颖的ConvNets，它直接执行NMS而无需后续的后处理步骤。于等人[[87](#bib.bib87)]利用最后的特征图来过滤出易样本，使网络集中于难样本。
- en: V-E Combining one-stage and two-stage detectors to make good results
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E 结合一阶段和两阶段的检测器以获得良好的结果
- en: In general, pre-existing object detectors are divided into two categories, the
    one is two-stage detector, the representative one, Faster R-CNN [[8](#bib.bib8)].
    The other is one-stage detector, such as YOLO [[9](#bib.bib9)], SSD [[10](#bib.bib10)].
    Two-stage detectors have high localization and object recognition precision, while
    one-stage detectors achieve high inference and test speed. The two stages of two-stage
    detectors are divided by ROI (Region of Interest) pooling layer. In Faster R-CNN
    detector, the first stage, called RPN, a Region Proposal Network, proposes candidate
    object bounding boxes. The second stage, the network extracts features using RoIPool
    from each candidate box and performs classification and bounding-box regression.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，现有的物体检测器分为两类，一类是两阶段检测器，代表性的是Faster R-CNN [[8](#bib.bib8)]。另一类是一阶段检测器，如YOLO
    [[9](#bib.bib9)] 和SSD [[10](#bib.bib10)]。两阶段检测器具有较高的定位和物体识别精度，而一阶段检测器则实现了较高的推理和测试速度。两阶段检测器的两个阶段由ROI（感兴趣区域）池化层分隔。在Faster
    R-CNN检测器中，第一个阶段称为RPN（区域提议网络），提出候选物体边界框。第二个阶段，网络使用RoIPool从每个候选框中提取特征，并进行分类和边界框回归。
- en: To fully inherit the advantages of one-stage and two-stage detectors while overcoming
    their disadvantages, Zhang et al. [[36](#bib.bib36)] present a novel RefineDet
    which achieves better accuracy than two-stage detectors and maintains comparable
    efficiency of one-stage detectors.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分继承一阶段和两阶段检测器的优点，同时克服它们的缺点，Zhang等人[[36](#bib.bib36)] 提出了新型的RefineDet，它比两阶段检测器具有更好的准确性，同时保持了与一阶段检测器相当的效率。
- en: V-F Complicated scene solutions
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-F 复杂场景解决方案
- en: Object detection always meets some challenges like small objects hard to detect
    and heavy occluded situation. Due to low resolution and noisy representation,
    detecting small objects is a very hard problem. Object detection pipelines [[10](#bib.bib10)]
    [[33](#bib.bib33)] detect small objects through learning representations of objects
    at multiple scales. Some works [[93](#bib.bib93)][[94](#bib.bib94)][[95](#bib.bib95)]
    improve detection accuracy on the basis of [[10](#bib.bib10)]. Li et al. [[96](#bib.bib96)]
    utilize GAN model in which generator transfer perceived poor representations of
    the small objects to super-resolved ones that are similar enough to real large
    objects to fool a competing discriminator. This makes the representation of small
    objects similar to the large one thus improves accuracy without heavy computing
    cost. Some methods [[47](#bib.bib47)][[97](#bib.bib97)] improve detection accuracy
    of small objects by enhancing IoU thresholds to train multiple localization modules.
    Hu et al. [[98](#bib.bib98)] adopt feature fusion to better detect small faces
    which is produced by image pyramid. Xu et al. [[99](#bib.bib99)] fuse high level
    features with rich semantic information and low level features via Deconvolution
    Fusion Block to enhance representation of small objects.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测常常面临一些挑战，如小物体难以检测和重度遮挡情况。由于分辨率低和表示噪声，检测小物体是一个非常困难的问题。目标检测管道[[10](#bib.bib10)]
    [[33](#bib.bib33)] 通过学习多尺度物体表示来检测小物体。一些工作[[93](#bib.bib93)][[94](#bib.bib94)][[95](#bib.bib95)]
    在[[10](#bib.bib10)]的基础上提高了检测精度。Li等人[[96](#bib.bib96)] 使用GAN模型，其中生成器将小物体的较差表示转化为足够类似于真实大物体的超分辨率表示，以欺骗竞争的判别器。这使得小物体的表示类似于大物体，从而提高了准确性而不需要大量计算成本。一些方法[[47](#bib.bib47)][[97](#bib.bib97)]
    通过增强IoU阈值来训练多个定位模块，从而提高小物体的检测精度。Hu等人[[98](#bib.bib98)] 采用特征融合来更好地检测由图像金字塔产生的小脸。Xu等人[[99](#bib.bib99)]
    通过反卷积融合块将高层次特征与丰富的语义信息和低层次特征融合，以增强小物体的表示。
- en: Target occlusion is another difficult problem in the field of object detection.
    Wang et al. [[100](#bib.bib100)] improve the recall of face detection problem
    in the occluded case without speed decay. Wang et al. [[101](#bib.bib101)] propose
    a novel bounding box regression loss specifically designed for crowd scenes, called
    repulsion loss. Zhang et al. [[102](#bib.bib102)] present a newly designed occlusion-aware
    R-CNN (OR-CNN) to improve the detection accuracy in the crowd. Baqu et al. [[103](#bib.bib103)]
    combine Convolutional Neural Nets and Conditional Random Fields that model potential
    occlusions.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 目标遮挡是目标检测领域中的另一个难题。Wang等人[[100](#bib.bib100)] 提高了在遮挡情况下人脸检测的召回率，而不会导致速度下降。Wang等人[[101](#bib.bib101)]
    提出了专门为人群场景设计的新型边界框回归损失，称为排斥损失。Zhang等人[[102](#bib.bib102)] 提出了新设计的遮挡感知R-CNN（OR-CNN），以提高人群中的检测精度。Baqu等人[[103](#bib.bib103)]
    结合了卷积神经网络和建模潜在遮挡的条件随机场。
- en: '![Refer to caption](img/bca5aa6a46b4f03acd8d027a4f240bd4.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/bca5aa6a46b4f03acd8d027a4f240bd4.png)'
- en: 'Figure 7: To meet various scales of objects issue, there are three ways. (a)
    multiple scaled images detector trains each of them. (b) multiple sized filters
    separately act on the same sized image. (c) multiple pre-defined boxes are the
    reference of predicted boxes.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：为了应对各种尺寸的对象问题，有三种方法。 (a) 多尺度图像检测器训练每一种图像。 (b) 多种尺寸的滤波器分别作用于相同尺寸的图像。 (c)
    多个预定义框作为预测框的参考。
- en: As for the size of different objects in a dataset varies greatly, to address
    it, there are three commonly used methods. Firstly, input images are resized at
    multiple specified scales and feature maps are computed for each scale, called
    multi-scale training. Typical examples [[29](#bib.bib29)][[48](#bib.bib48)][[104](#bib.bib104)][[105](#bib.bib105)]
    use this method. Singh et al. [[71](#bib.bib71)] adaptively sample regions from
    multiple scales of an image pyramid, conditioned on the image content. Secondly,
    researchers use convolutional filters of multiple scales on the feature maps.
    For instance, in [[106](#bib.bib106)], models of different aspect ratios are trained
    separately using different filter sizes (such as ${5}\times{7}$ and ${7}\times{5}$
    ). Thirdly, pre-defined anchors with multi-scales and multiple aspect ratios are
    reference boxes of the predicted bounding boxes. Faster R-CNN [[8](#bib.bib8)]
    and SSD [[10](#bib.bib10)] use reference box in two-stage and one-stage detectors
    for the first time, respectively. Fig. 7 is a schematic diagram of the above three
    cases.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集中不同对象的尺寸差异很大，为了解决这一问题，有三种常用的方法。首先，将输入图像调整到多个指定的尺度，并为每个尺度计算特征图，这被称为多尺度训练。典型的例子[[29](#bib.bib29)][[48](#bib.bib48)][[104](#bib.bib104)][[105](#bib.bib105)]使用了这种方法。Singh
    等人[[71](#bib.bib71)]根据图像内容自适应地从图像金字塔的多个尺度中采样区域。其次，研究人员在特征图上使用多尺度的卷积滤波器。例如，在[[106](#bib.bib106)]中，使用不同滤波器尺寸（如
    ${5}\times{7}$ 和 ${7}\times{5}$）分别训练不同纵横比的模型。第三，预定义的多尺度和多纵横比的锚点作为预测边界框的参考框。Faster
    R-CNN [[8](#bib.bib8)] 和 SSD [[10](#bib.bib10)] 分别首次在两阶段和单阶段检测器中使用了参考框。图 7 是上述三种情况的示意图。
- en: '![Refer to caption](img/401d3f46bea99240c9a9692761474e42.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/401d3f46bea99240c9a9692761474e42.png)'
- en: 'Figure 8: An anchor-based architecture require heuristics to determine which
    size level anchors are responsible for what scale range of objects.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：基于锚点的架构需要启发式方法来确定哪个尺寸级别的锚点负责哪些范围的对象。
- en: V-G anchor-free
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-G 无锚点
- en: While there are constellation anchor-based object detectors being mainstream
    method which contain both one-stage and two-stage detectors making significant
    performance improvements, such as SSD, Faster R-CNN, YOLOv2, YOLOv3, they still
    suffer some drawbacks.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于锚点的物体检测器（包括一阶段和两阶段检测器）在性能上取得了显著改进，如 SSD、Faster R-CNN、YOLOv2、YOLOv3，它们仍然存在一些缺陷。
- en: (1) The pre-defined anchor boxes have a set of hand-crafted scales and aspect
    ratios which are sensitive to dataset and affect the detection performance by
    a large margin.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 预定义的锚框具有一组手工设置的尺度和纵横比，这些尺度和纵横比对数据集非常敏感，并且会大幅度影响检测性能。
- en: (2) The scales and aspect ratios of pre-defined anchor boxes are kept fixed
    during training, thus the next step can’t get adaptively adjust boxes. Meanwhile,
    detectors have trouble handling objects of all sizes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 预定义锚框的尺度和纵横比在训练过程中保持固定，因此接下来的步骤无法自适应地调整框。同时，检测器在处理所有尺寸的对象时遇到困难。
- en: (3) For densely place anchor boxes to achieve high recall, especially on large-scale
    dataset, the computation cost and memory requirements bring huge overhead during
    processing procedure.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 为了实现高召回率，尤其是在大规模数据集上，密集放置的锚框在处理过程中会带来巨大的计算成本和内存需求。
- en: (4) Most of pre-defined anchors are negative samples, which causes great imbalance
    between positive and negative sample during training.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 大多数预定义的锚点是负样本，这在训练过程中导致正负样本之间的不平衡。
- en: To address that, recently a series of anchor-free methods [[51](#bib.bib51)]
    [[61](#bib.bib61)] [[62](#bib.bib62)] [[107](#bib.bib107)][[108](#bib.bib108)]
    [[109](#bib.bib109)] [[110](#bib.bib110)] [[111](#bib.bib111)] [[112](#bib.bib112)]
    [[113](#bib.bib113)] are proposed. CenterNet [[108](#bib.bib108)] locates the
    center point, top-left and bottom-right point of an object. Tian et al. [[61](#bib.bib61)]
    propose a localization method which is based on the four distance values between
    the predicted center point and four sides of a bounding box. It is still a novel
    direction for further research.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，最近提出了一系列无锚点的方法[[51](#bib.bib51)] [[61](#bib.bib61)] [[62](#bib.bib62)]
    [[107](#bib.bib107)][[108](#bib.bib108)] [[109](#bib.bib109)] [[110](#bib.bib110)]
    [[111](#bib.bib111)] [[112](#bib.bib112)] [[113](#bib.bib113)]。CenterNet[[108](#bib.bib108)]定位对象的中心点、左上角和右下角点。Tian等人[[61](#bib.bib61)]提出了一种定位方法，该方法基于预测的中心点与边界框四边之间的四个距离值。这仍然是一个进一步研究的新方向。
- en: V-H Training from scratch
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-H 从头训练
- en: Almost all of the state-of-the-art detectors utilize off-the-shelf classification
    backbone pre-trained on large scale classification dataset [[3](#bib.bib3)] as
    their initial parameter set then fine-tune parameters to adapt to the new detection
    task. Another way to implement training procedure is that all parameters are assigned
    from scratch. Zhu et al. [[114](#bib.bib114)] train detector from scratch thus
    do not need pre-trained classification backbone because of stable and predictable
    gradient brought by batch normalization operation. Some works [[115](#bib.bib115)]
    [[116](#bib.bib116)] [[117](#bib.bib117)] [[118](#bib.bib118)] train object detectors
    from scratch by dense layer-wise connections.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有最先进的检测器都利用在大规模分类数据集上预训练的现成分类骨干网络[[3](#bib.bib3)]作为其初始参数集，然后对参数进行微调，以适应新的检测任务。另一种实现训练过程的方法是所有参数从头开始分配。Zhu等人[[114](#bib.bib114)]从头开始训练检测器，因此不需要预训练的分类骨干网络，因为批量归一化操作带来了稳定且可预测的梯度。一些工作[[115](#bib.bib115)]
    [[116](#bib.bib116)] [[117](#bib.bib117)] [[118](#bib.bib118)]通过密集层级连接从头开始训练目标检测器。
- en: V-I Designing new architecture
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-I 设计新架构
- en: Because of different propose of classification and localization task, there
    exists a gap between classification network and detection architecture. Localization
    needs fine-grained representations of objects while classification needs high
    semantic information. Li et al. [[16](#bib.bib16)] propose a newly designed object
    detection architecture to specially focus on detection task which maintains high
    spatial resolution in deeper layers and does not need to pre-train on large scale
    classification dataset.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分类任务和定位任务的不同目的，分类网络和检测架构之间存在差距。定位需要对象的细粒度表示，而分类需要高语义信息。Li等人[[16](#bib.bib16)]提出了一种新设计的目标检测架构，专门关注检测任务，保持较深层的高空间分辨率，并且不需要在大规模分类数据集上进行预训练。
- en: The two-stage detectors are always slower than one-stage detectors. By studying
    the structure of two-stage network, researchers find two-stage detectors like
    Faster R-CNN and R-FCN have a heavy head which slows it down. Li et al. [[119](#bib.bib119)]
    present a light head two-stage detector to keep time efficiency.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 两阶段检测器通常比单阶段检测器慢。通过研究两阶段网络的结构，研究人员发现像Faster R-CNN和R-FCN这样的两阶段检测器具有一个沉重的头部，从而减慢了速度。Li等人[[119](#bib.bib119)]提出了一种轻量化头部的两阶段检测器，以保持时间效率。
- en: V-J Speeding up detection
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-J 加快检测速度
- en: For limited computing power and memory resource such as mobile devices, real-time
    devices, webcam, automatic driving encourage research into efficient detection
    architecture design. The most typical real-time detector is the [[9](#bib.bib9)]
    [[30](#bib.bib30)] [[32](#bib.bib32)] series and [[10](#bib.bib10)] [[34](#bib.bib34)]
    and their improved architecture [[66](#bib.bib66)] [[67](#bib.bib67)] [[95](#bib.bib95)]
    [[120](#bib.bib120)]. Some methods [[24](#bib.bib24)] [[87](#bib.bib87)] [[121](#bib.bib121)]
    [[122](#bib.bib122)] [[123](#bib.bib123)] [[124](#bib.bib124)] are aim to reach
    real-time detection.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算能力和内存资源有限的设备，如移动设备、实时设备、网络摄像头和自动驾驶，推动了对高效检测架构设计的研究。最典型的实时检测器是[[9](#bib.bib9)]
    [[30](#bib.bib30)] [[32](#bib.bib32)]系列和[[10](#bib.bib10)] [[34](#bib.bib34)]及其改进架构[[66](#bib.bib66)]
    [[67](#bib.bib67)] [[95](#bib.bib95)] [[120](#bib.bib120)]。一些方法[[24](#bib.bib24)]
    [[87](#bib.bib87)] [[121](#bib.bib121)] [[122](#bib.bib122)] [[123](#bib.bib123)]
    [[124](#bib.bib124)]旨在实现实时检测。
- en: V-K Achieving Fast and Accurate Detections
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-K 实现快速准确的检测
- en: The best object detector needs both high efficiency and high accuracy which
    is the ultimate goal of this task. Lin et al. [[33](#bib.bib33)] aim to surpass
    the accuracy of existing two-stage detectors while maintain fast speed. Zhou et
    al. [[125](#bib.bib125)] combine an accurate (but slow) detector and a fast (but
    less accurate) detector adaptively determining whether an image is easy or hard
    to detect and choosing an appropriate detector to detect it. Liu et al. [[126](#bib.bib126)]
    build a fast and accurate detector by strengthening lightweight network features
    using receptive fields block.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳目标检测器需要高效和高准确性，这是该任务的**终极目标**。Lin等人[[33](#bib.bib33)]旨在超越现有双阶段检测器的准确性，同时保持快速速度。Zhou等人[[125](#bib.bib125)]结合了一个准确（但较慢）检测器和一个快速（但较不准确）检测器，自适应地判断图像是否容易或难以检测，并选择合适的检测器进行检测。Liu等人[[126](#bib.bib126)]通过使用感受野块来增强轻量级网络特征，构建了一个快速且准确的检测器。
- en: VI Applications and branches
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 应用与分支
- en: VI-A Typical application areas
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 典型应用领域
- en: Object detection has been widely used in some fields to assist people to complete
    some tasks, such as security field, military field, transportation field, medical
    field and life field. We describe the typical and recent methods utilized in these
    fields in detail.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测已被广泛应用于某些领域，以帮助人们完成任务，例如安全领域、军事领域、交通领域、医疗领域和生活领域。我们详细描述了这些领域中典型和最新的方法。
- en: VI-A1 Security field
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A1 安全领域
- en: The most well known applications in the security field are face detection, pedestrian
    detection, fingerprint identification, fraud detection, anomaly detection etc.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在安全领域中最著名的应用包括人脸检测、行人检测、指纹识别、欺诈检测、异常检测等。
- en: $\bullet$ Face detection aims at detecting people faces in an image, as shown
    in Fig. 9\. Because of extreme poses, illumination and resolution variations,
    face detection is still a difficult mission. Many works focus on precise detector
    designing. Ranjan et al. [[127](#bib.bib127)] learn correlated tasks (face detection,
    facial landmarks localization, head pose estimation and gender recognition) simultaneously
    to boost the performance of individual tasks. He et al. [[128](#bib.bib128)] propose
    a novel Wasserstein convolutional neural network approach to learn invariant features
    between near-infrared (NIR) and visual (VIS) face images. Designing appropriate
    loss functions can enhance discriminative power of DCNNs based large-scale face
    recognition. The cosine-based softmax losses [[129](#bib.bib129)][[130](#bib.bib130)][[131](#bib.bib131)][[132](#bib.bib132)]
    achieve great success in deep learning based face recognition. Deng et al. [[133](#bib.bib133)]
    propose an Additive Angular Margin Loss (ArcFace) to get highly discriminative
    features for face recognition. Guo et al. [[134](#bib.bib134)] give a fuzzy sparse
    auto-encoder framework for single image per person face recognition. Please refer
    to [[135](#bib.bib135)] for more details.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 人脸检测旨在检测图像中的人脸，如图9所示。由于极端的姿势、光照和分辨率变化，人脸检测仍然是一项困难的任务。许多工作集中在精确的检测器设计上。Ranjan等人[[127](#bib.bib127)]同时学习相关任务（人脸检测、面部关键点定位、头部姿势估计和性别识别）以提升各个任务的性能。He等人[[128](#bib.bib128)]提出了一种新颖的Wasserstein卷积神经网络方法，用于学习近红外（NIR）和可见光（VIS）人脸图像之间的不变特征。设计合适的损失函数可以增强基于DCNN的大规模人脸识别的判别能力。基于余弦的softmax损失[[129](#bib.bib129)][[130](#bib.bib130)][[131](#bib.bib131)][[132](#bib.bib132)]在深度学习的人脸识别中取得了巨大成功。Deng等人[[133](#bib.bib133)]提出了加性角度边距损失（ArcFace），以获得高度区分的特征用于人脸识别。Guo等人[[134](#bib.bib134)]提供了一个模糊稀疏自编码框架，用于每人单张图像的人脸识别。有关更多详细信息，请参阅[[135](#bib.bib135)]。
- en: $\bullet$ Pedestrian detection focuses on detecting pedestrians in the natural
    scenes. Braun et al. [[52](#bib.bib52)] release an EuroCity Persons dataset containing
    pedestrians, cyclists and other riders in urban traffic scenes. Complexity-aware
    cascaded pedestrian detectors [[136](#bib.bib136)][[137](#bib.bib137)][[138](#bib.bib138)]
    devote to real time pedestrian detection. Please refer to a survey [[139](#bib.bib139)]
    for more details.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 行人检测专注于在自然场景中检测行人。Braun等人[[52](#bib.bib52)]发布了一个EuroCity Persons数据集，包含城市交通场景中的行人、自行车骑士和其他骑车者。复杂度感知的级联行人检测器[[136](#bib.bib136)][[137](#bib.bib137)][[138](#bib.bib138)]致力于实时行人检测。有关更多详细信息，请参阅调研[[139](#bib.bib139)]。
- en: $\bullet$ Anomaly detection plays a significant role in fraud detection, climate
    analysis, and healthcare monitoring. Existing anomaly detection techniques [[140](#bib.bib140)][[141](#bib.bib141)][[142](#bib.bib142)][[143](#bib.bib143)]
    analyze the data on a point-wise basis. To point the expert analysts to the interesting
    regions (anomalies) of the data, Barz et al. [[144](#bib.bib144)] propose a novel
    unsupervised method called “Maximally Divergent Intervals” (MDI), which searches
    for contiguous intervals of time and regions in space.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 异常检测在欺诈检测、气候分析和健康监测中发挥着重要作用。现有的异常检测技术 [[140](#bib.bib140)][[141](#bib.bib141)][[142](#bib.bib142)][[143](#bib.bib143)]
    是基于逐点分析数据的。为了引导专家分析师关注数据中的有趣区域（异常），Barz 等人 [[144](#bib.bib144)] 提出了一个新的无监督方法，称为“最大分歧区间”（MDI），该方法搜索时间和空间中连续的区间。
- en: '![Refer to caption](img/e9c89e6530739b1e8d865b5aa99052b6.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e9c89e6530739b1e8d865b5aa99052b6.png)'
- en: 'Figure 9: A challenging densely tiny human faces detection results. Image from
    Hu et al. [[98](#bib.bib98)].'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 具有挑战性的密集小型人脸检测结果。图片来自 Hu 等人 [[98](#bib.bib98)]。'
- en: VI-A2 Military field
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A2 军事领域
- en: In military field, remote sensing object detection, topographic survey, flyer
    detection, etc. are representative applications.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在军事领域，遥感物体检测、地形测绘、传单检测等是代表性应用。
- en: $\bullet$ Remote sensing object detection aims at detecting objects on remote
    sensing images or videos, which meets some challenges. Firstly, the extreme large
    input size but small targets makes the existing object detection procedure too
    slow for practical use and too hard to detect. Secondly, the massive and complex
    backgrounds cause serious false detection. To address these issues, researchers
    adopt the method of data fusion. Due to the lack of information and small deviation,
    which caused great inaccuracy, they focused on the detection of small targets.
    Remote sensing images have some characteristics far from natural images, thus
    strong pipelines such as Faster R-CNN, FCN, SSD, YOLO can’t transfer well to the
    new data domain. Designing remote sensing dataset adapted detectors remains a
    research hot spot in this domain.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 遥感物体检测旨在检测遥感图像或视频中的物体，但面临一些挑战。首先，极大的输入尺寸但小目标使得现有的物体检测过程在实际使用中过于缓慢且难以检测。其次，大量复杂的背景导致严重的误检。为了解决这些问题，研究人员采用了数据融合的方法。由于信息不足和小偏差导致的巨大不准确性，他们专注于小目标的检测。遥感图像具有与自然图像截然不同的特性，因此像
    Faster R-CNN、FCN、SSD、YOLO 等强大的检测管道无法很好地迁移到新数据领域。设计适应遥感数据集的检测器仍然是该领域的研究热点。
- en: Cheng et al. [[145](#bib.bib145)] propose a CNN-based Remote Sensing Image (RSI)
    object detection model dealing with the rotation problem by designing a rotation-invariant
    layer. Zhang et al. [[146](#bib.bib146)] present a rotation and scaling robust
    structure to address lacking rotation and scaling invariance in RSI object detection.
    Li et al. [[147](#bib.bib147)] raise a rotatable region proposal network and a
    rotatable detection network considering the orientation of vehicles. Deng et al.
    [[148](#bib.bib148)] put forward an accurate-vehicle-proposal-network (AVPN) for
    small object detection. Audebert et al. [[149](#bib.bib149)] utilize accurate
    semantic segmentation results to obtain detection of vehicles. Li et al. [[150](#bib.bib150)]
    address large range of resolutions of ships (ranging from dozens of pixels to
    thousands) issue in ship detection. Pang et al. [[151](#bib.bib151)] propose a
    real-time remote sensing method. Pei et al. [[152](#bib.bib152)] present a deep
    learning framework on synthetic aperture radar (SAR) automatic target recognition.
    Long et al. [[153](#bib.bib153)] concentrate on automatically and accurately locating
    objects. Shahzad et al. [[154](#bib.bib154)] propose a novel framework containing
    automatic labeling and recurrent neural network for detection.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Cheng 等人 [[145](#bib.bib145)] 提出了一个基于 CNN 的遥感图像（RSI）物体检测模型，通过设计一个旋转不变层来处理旋转问题。Zhang
    等人 [[146](#bib.bib146)] 提出了一个旋转和缩放鲁棒结构，以解决 RSI 物体检测中缺乏旋转和缩放不变性的问题。Li 等人 [[147](#bib.bib147)]
    提出了一个可旋转区域提议网络和一个可旋转检测网络，考虑到车辆的方向。Deng 等人 [[148](#bib.bib148)] 提出了一个用于小物体检测的精确车辆提议网络（AVPN）。Audebert
    等人 [[149](#bib.bib149)] 利用准确的语义分割结果来检测车辆。Li 等人 [[150](#bib.bib150)] 解决了船舶检测中船舶分辨率范围广泛（从几十像素到几千像素）的问题。Pang
    等人 [[151](#bib.bib151)] 提出了一个实时遥感方法。Pei 等人 [[152](#bib.bib152)] 提出了一个基于合成孔径雷达（SAR）的深度学习框架用于自动目标识别。Long
    等人 [[153](#bib.bib153)] 专注于自动和准确地定位物体。Shahzad 等人 [[154](#bib.bib154)] 提出了一个包含自动标注和递归神经网络的检测新框架。
- en: Typical methods [[155](#bib.bib155)][[156](#bib.bib156)][[157](#bib.bib157)][[158](#bib.bib158)][[159](#bib.bib159)][[160](#bib.bib160)][[161](#bib.bib161)][[162](#bib.bib162)][[163](#bib.bib163)][[164](#bib.bib164)][[165](#bib.bib165)]
    all utilize deep neural networks to achieve detection task on remote sensing datasets.
    NWPU VHR-10 [[166](#bib.bib166)], HRRSD [[146](#bib.bib146)], DOTA [[167](#bib.bib167)],
    DLR 3K Munich [[168](#bib.bib168)] and VEDAI [[169](#bib.bib169)] are remote sensing
    object detection benchmarks. We recommend readers refer to [[170](#bib.bib170)]
    for more details on remote sensing object detection.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的方法 [[155](#bib.bib155)][[156](#bib.bib156)][[157](#bib.bib157)][[158](#bib.bib158)][[159](#bib.bib159)][[160](#bib.bib160)][[161](#bib.bib161)][[162](#bib.bib162)][[163](#bib.bib163)][[164](#bib.bib164)][[165](#bib.bib165)]
    都利用深度神经网络在遥感数据集上实现检测任务。NWPU VHR-10 [[166](#bib.bib166)]、HRRSD [[146](#bib.bib146)]、DOTA
    [[167](#bib.bib167)]、DLR 3K Munich [[168](#bib.bib168)] 和 VEDAI [[169](#bib.bib169)]
    是遥感目标检测的基准测试。我们建议读者参考 [[170](#bib.bib170)] 以获取有关遥感目标检测的更多详细信息。
- en: VI-A3 Transportation field
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A3 交通领域
- en: As we known that, license plate recognition, automatic driving and traffic sign
    recognition etc. greatly facilitate people’s life.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，车牌识别、自动驾驶和交通标志识别等技术极大地便利了人们的生活。
- en: $\bullet$ With the popularity of cars, license plate recognition is required
    in tracking crime, residential access, traffic violations tracking etc. Edge information,
    mathematical morphology, texture features, sliding concentric windows, connected
    component analysis etc. can bring license plate recognition system more robust
    and stable. Recently, deep learning-based methods [[171](#bib.bib171)][[172](#bib.bib172)][[173](#bib.bib173)][[174](#bib.bib174)][[175](#bib.bib175)]
    provide a variety of solutions for license plate recognition. Please refer to
    [[176](#bib.bib176)] for more details.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 随着汽车的普及，车牌识别在犯罪追踪、住宅访问、交通违规追踪等方面变得必不可少。边缘信息、数学形态学、纹理特征、滑动同心窗口、连通组件分析等可以使车牌识别系统更为稳健和稳定。最近，基于深度学习的方法
    [[171](#bib.bib171)][[172](#bib.bib172)][[173](#bib.bib173)][[174](#bib.bib174)][[175](#bib.bib175)]
    为车牌识别提供了多种解决方案。更多细节请参考 [[176](#bib.bib176)]。
- en: $\bullet$ An autonomous vehicle (AV) needs an accurate perception of its surroundings
    to operate reliably. The perception system of an AV normally employs machine learning
    (e.g., deep learning) and transforms sensory data into semantic information which
    enables autonomous driving. Object detection is a fundamental function of this
    perception system. 3D object detection methods involve a third dimension that
    reveals more detailed object’s size and location information, which are divided
    into three categories, monocular, point-cloud and fusion. First, monocular image
    based methods predict 2D bounding boxes on the image then extrapolate them to
    3D, which lacks explicit depth information so limits the accuracy of localization.
    Second, point-cloud based methods project point clouds into a 2D image to process
    or generate a 3D representation of the point cloud directly in a voxel structure,
    where the former loses information and the latter is time consuming. Third, fusion
    based methods fuse both front view images and point clouds to generate a robust
    detection, which represent state-of-the-art detectors while computationally expensive.
    Recently, Lu et al. [[177](#bib.bib177)] utilize a novel architecture contains
    3D convolutions and RNNs to achieve centimeter-level localization accuracy in
    different real-world driving scenarios. Song et al. [[178](#bib.bib178)] release
    a 3D car instance understanding benchmark for autonomous driving. Banerjee et
    al. [[179](#bib.bib179)] utilize sensor fusion to obtain better features. Please
    refer to a recently published survey [[180](#bib.bib180)] for more details.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 一辆自主车辆（AV）需要对其周围环境有准确的感知以可靠地操作。AV的感知系统通常采用机器学习（例如，深度学习），将传感器数据转换为语义信息，从而实现自主驾驶。目标检测是该感知系统的一个基本功能。3D目标检测方法涉及第三维度，可以揭示更详细的物体尺寸和位置信息，这些方法分为三类：单目、点云和融合。首先，基于单目图像的方法在图像上预测2D边界框，然后将其外推到3D，这缺乏明确的深度信息，因此限制了定位的准确性。其次，基于点云的方法将点云投影到2D图像中进行处理，或直接在体素结构中生成3D点云表示，其中前者会丢失信息，后者则耗时。第三，基于融合的方法结合前视图像和点云生成稳健的检测，这些方法代表了最先进的检测器，但计算成本较高。最近，Lu等人[[177](#bib.bib177)]利用一种包含3D卷积和RNN的新架构，在不同的现实世界驾驶场景中实现了厘米级的定位精度。Song等人[[178](#bib.bib178)]发布了一个用于自主驾驶的3D汽车实例理解基准。Banerjee等人[[179](#bib.bib179)]利用传感器融合来获得更好的特征。有关更多细节，请参考最近发布的调查报告[[180](#bib.bib180)]。
- en: $\bullet$ Both unmanned vehicles and autonomous driving systems need to solve
    the problem of traffic sign recognition. For the sake of safety and obeying the
    rules, real-time accurate traffic sign recognition assists in driving by acquiring
    the temporal and spatial information of the potential signs. Deep learning methods
    [[181](#bib.bib181)][[182](#bib.bib182)][[183](#bib.bib183)][[184](#bib.bib184)][[185](#bib.bib185)][[186](#bib.bib186)][[187](#bib.bib187)]
    solve this problem with high performance.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 无人驾驶车辆和自主驾驶系统都需要解决交通标志识别的问题。为了安全和遵守规则，实时准确的交通标志识别通过获取潜在标志的时间和空间信息来辅助驾驶。深度学习方法[[181](#bib.bib181)][[182](#bib.bib182)][[183](#bib.bib183)][[184](#bib.bib184)][[185](#bib.bib185)][[186](#bib.bib186)][[187](#bib.bib187)]以高性能解决了这一问题。
- en: VI-A4 Medical field
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A4 医疗领域
- en: In medical field, medical image detection, cancer detection, disease detection,
    skin disease detection and healthcare monitoring etc. have become a means of supplementary
    medical treatments.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗领域，医学图像检测、癌症检测、疾病检测、皮肤病检测和健康监测等已成为补充医疗治疗的一种手段。
- en: $\bullet$ Computer Aided Diagnosis (CAD) systems can help doctors classify different
    types of cancer. In detail, after an appropriate acquisition of the images, the
    fundamental steps carried out by a CAD framework can be identified as image segmentation,
    feature extraction, classification and object detection. Due to significant individual
    differences, data scarcity and privacy, there usually exists data distribution
    difference between source domain and target domain. A domain adaptation framework
    [[188](#bib.bib188)] is needed for medical image detection.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 计算机辅助诊断（CAD）系统可以帮助医生分类不同类型的癌症。具体而言，在适当获取图像后，CAD框架所执行的基本步骤可以识别为图像分割、特征提取、分类和物体检测。由于个体差异、数据稀缺和隐私问题，源领域和目标领域之间通常存在数据分布差异。医学图像检测需要一个领域自适应框架[[188](#bib.bib188)]。
- en: $\bullet$ Li et al. [[77](#bib.bib77)] incorporate the attention mechanism in
    CNN for glaucoma detection and establish a large-scale attention-based glaucoma
    dataset. Liu et al. [[189](#bib.bib189)] design a bidirectional recurrent neural
    network (RNN) with long short-term memory (LSTM) to detect DNA modifications called
    DeepMod. Schubert et al. [[190](#bib.bib190)] propose cellular morphology neural
    networks (CMNs) for automated neuron reconstruction and automated detection of
    synapses. Codella et al. [[191](#bib.bib191)] organize a challenge of skin lesion
    analysis toward melanoma detection. Please refer to two representative surveys
    [[192](#bib.bib192)] [[193](#bib.bib193)] for more details.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Li 等人 [[77](#bib.bib77)] 在 CNN 中结合了注意力机制用于青光眼检测，并建立了一个大规模基于注意力的青光眼数据集。Liu
    等人 [[189](#bib.bib189)] 设计了一种具有长短期记忆（LSTM）的双向递归神经网络（RNN），用于检测 DNA 修改，称为 DeepMod。Schubert
    等人 [[190](#bib.bib190)] 提出了细胞形态神经网络（CMNs），用于自动化神经元重建和突触自动检测。Codella 等人 [[191](#bib.bib191)]
    组织了一个皮肤病变分析挑战，以检测黑色素瘤。有关更多详细信息，请参阅两个代表性综述 [[192](#bib.bib192)] [[193](#bib.bib193)]。
- en: VI-A5 Life field
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A5 生活领域
- en: In life field, intelligent home, commodity detection, event detection, pattern
    detection, image caption generation, rain/shadow detection, species identification
    etc. are the most representative applications.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在生活领域中，智能家居、商品检测、事件检测、模式检测、图像标题生成、雨/阴影检测、物种识别等是最具代表性的应用。
- en: $\bullet$ On densely packed scenes like retail shelf displays, Goldman et al.
    [[194](#bib.bib194)] propose a novel precise object detector and release a new
    SKU-110K dataset to meet this challenge.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 在像零售货架展示这样的密集场景中，Goldman 等人 [[194](#bib.bib194)] 提出了一个新颖的精确目标检测器，并发布了一个新的
    SKU-110K 数据集以应对这一挑战。
- en: $\bullet$ Event detection aims to discover real-world events from the Internet
    such as festivals, talks, protests, natural disasters, elections. With the popularity
    of social media and its new characters, the data type of which are more diverse
    than before. Multi-domain event detection (MED) provides comprehensive descriptions
    of events. Yang et al. [[195](#bib.bib195)] present an event detection framework
    to dispose multi-domain data. Wang et al. [[196](#bib.bib196)] incorporate online
    social interaction features by constructing affinity graphs for event detection
    tasks. Schinas et al. [[197](#bib.bib197)] design a multimodal graph-based system
    to detect events from 100 million photos/videos. Please refer to a survey [[198](#bib.bib198)]
    for more details.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 事件检测旨在从互联网发现现实世界的事件，如节日、讲座、抗议、自然灾害和选举。随着社交媒体的普及及其新特性，数据类型比以往更为多样化。多领域事件检测（MED）提供了事件的综合描述。Yang
    等人 [[195](#bib.bib195)] 提出了一个事件检测框架以处理多领域数据。Wang 等人 [[196](#bib.bib196)] 通过构建亲和图来融入在线社交互动特征，用于事件检测任务。Schinas
    等人 [[197](#bib.bib197)] 设计了一个多模态图形系统，从 1 亿张照片/视频中检测事件。有关更多详细信息，请参阅综述 [[198](#bib.bib198)]。
- en: $\bullet$ Pattern detection always meet some challenges such as, scene occlusion,
    pose variation, varying illumination and sensor noise. To better address repeated
    pattern or periodic structure detection, researches design strong baselines in
    both 2D images [[199](#bib.bib199)] [[200](#bib.bib200)] and 3D point clouds [[201](#bib.bib201)]
    [[202](#bib.bib202)] [[203](#bib.bib203)][[204](#bib.bib204)][[205](#bib.bib205)][[206](#bib.bib206)][[207](#bib.bib207)][[208](#bib.bib208)][[209](#bib.bib209)][[210](#bib.bib210)][[211](#bib.bib211)][[212](#bib.bib212)].
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 模式检测总是面临一些挑战，例如场景遮挡、姿势变化、光照变化和传感器噪声。为了更好地解决重复模式或周期结构检测的问题，研究人员在 2D
    图像 [[199](#bib.bib199)] [[200](#bib.bib200)] 和 3D 点云 [[201](#bib.bib201)] [[202](#bib.bib202)]
    [[203](#bib.bib203)][[204](#bib.bib204)][[205](#bib.bib205)][[206](#bib.bib206)][[207](#bib.bib207)][[208](#bib.bib208)][[209](#bib.bib209)][[210](#bib.bib210)][[211](#bib.bib211)][[212](#bib.bib212)]
    中设计了强大的基准。
- en: $\bullet$ Image caption generation means that computers automatically generate
    a caption for a given image. The most important part is to capture semantic information
    of images and express it to natural languages. Image captioning needs to connect
    computer vision and natural language processing technologies, which is a great
    challenge task. To address this issue, multimodal embedding, encoder–decoder frameworks,
    attention mechanism [[75](#bib.bib75)] [[213](#bib.bib213)], and reinforcement
    learning [[214](#bib.bib214)] [[215](#bib.bib215)] are widely adopted in this
    field. Yao et al. [[216](#bib.bib216)] introduce a new design to explore the connections
    between objects by constructing Graph Convolutional Networks and Long Short-Term
    Memory (dubbed as GCN-LSTM) architecture. This framework integrates both semantic
    and spatial object relationships. Apart from LSTM (long short term memory)-based
    methods, deep convolutional networks based method [[217](#bib.bib217)] is verified
    effective and efficient. Please refer to a survey [[218](#bib.bib218)] for more
    details.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 图像标题生成意味着计算机自动为给定的图像生成标题。最重要的部分是捕捉图像的语义信息并将其表达为自然语言。图像标题生成需要连接计算机视觉和自然语言处理技术，这是一项巨大的挑战。为了解决这个问题，多模态嵌入、编码器–解码器框架、注意力机制[[75](#bib.bib75)]
    [[213](#bib.bib213)]以及强化学习[[214](#bib.bib214)] [[215](#bib.bib215)]在这一领域被广泛采用。姚等人[[216](#bib.bib216)]介绍了一种新设计，通过构建图卷积网络和长短期记忆（称为GCN-LSTM）架构来探索对象之间的联系。该框架整合了语义和空间对象关系。除了基于LSTM（长短期记忆）的方法，基于深度卷积网络的方法[[217](#bib.bib217)]也被验证为有效且高效。有关更多细节，请参阅综述[[218](#bib.bib218)]。
- en: $\bullet$ Yang et al. [[219](#bib.bib219)] present a novel rain model accompany
    with a deep learning architecture to address rain detection in a single image.
    Hu et al. [[220](#bib.bib220)] analyze the spatial image context in a direction-aware
    manner and design a novel deep neural network to detect shadow. Accurate species
    identification is the basis for taxonomic research, a recently work [[221](#bib.bib221)]
    introduces a deep learning method for species identification.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 杨等人[[219](#bib.bib219)]提出了一种新型雨模型，并配有深度学习架构，以解决单幅图像中的雨检测问题。胡等人[[220](#bib.bib220)]以方向感知的方式分析空间图像上下文，并设计了一种新型深度神经网络来检测阴影。准确的物种识别是分类研究的基础，最近的一项工作[[221](#bib.bib221)]介绍了一种用于物种识别的深度学习方法。
- en: VI-B Object detection branches
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 对象检测分支
- en: Object detection has a wide range of application scenarios. The research of
    this domain contains a large variety of branches. We describe some representative
    branches in this part.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对象检测有广泛的应用场景。该领域的研究包含了各种分支。在这一部分，我们描述了一些代表性的分支。
- en: VI-B1 Weakly supervised object detection
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B1 弱监督对象检测
- en: Weakly supervised object detection (WSOD) aims at utilizing a few fully annotated
    images (supervision) to detect a large amount of non-fully annotated ones. Traditionally
    models are learnt from images labelled only with the object class and not the
    object bounding box. Annotating a bounding box for each object in large datasets
    is expensive, laborious and impractical. Weakly supervised learning relies on
    incomplete annotated training data to learn detection models.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督对象检测（WSOD）旨在利用少量完全标注的图像（监督）来检测大量未完全标注的图像。传统上，模型是从仅标记了对象类别而没有标记对象边界框的图像中学习的。为大数据集中的每个对象标注边界框既昂贵又繁琐且不切实际。弱监督学习依赖于不完全标注的训练数据来学习检测模型。
- en: Weakly supervised deep detection network in [[222](#bib.bib222)] is a representative
    framework for weakly supervised object detection. Context information [[223](#bib.bib223)],
    instance classifier refinement [[224](#bib.bib224)] and image segmentation [[225](#bib.bib225)][[226](#bib.bib226)]
    are adopted to tackle hardly optimized problems. Yang et al. [[227](#bib.bib227)]
    show that the action depicted in the image can provide strong cues about the location
    of the associated object. Wan et al. [[228](#bib.bib228)] design a min-entropy
    latent model optimized with a recurrent learning algorithm for weakly supervised
    object detection. Tang et al. [[229](#bib.bib229)] utilize an iterative procedure
    to generate proposal clusters and learn refined instance classifiers, which makes
    the network concentrate on the whole object rather than part of it. Cao et al.
    [[230](#bib.bib230)] design a novel feedback convolutional neural network for
    weakly supervised object localization. Wan et al. [[231](#bib.bib231)] present
    continuation multiple instance learning to alleviate the non-convexity problem
    in WSOD.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[222](#bib.bib222)]中，弱监督深度检测网络是弱监督目标检测的代表性框架。上下文信息[[223](#bib.bib223)]、实例分类器优化[[224](#bib.bib224)]和图像分割[[225](#bib.bib225)][[226](#bib.bib226)]被采用来解决难以优化的问题。杨等人[[227](#bib.bib227)]展示了图像中描绘的动作可以提供关于相关物体位置的强线索。万等人[[228](#bib.bib228)]设计了一个通过递归学习算法优化的最小熵潜在模型，用于弱监督目标检测。唐等人[[229](#bib.bib229)]利用迭代过程生成提议簇并学习优化的实例分类器，使网络集中在整个物体上，而不是它的一部分。曹等人[[230](#bib.bib230)]设计了一个新颖的反馈卷积神经网络用于弱监督目标定位。万等人[[231](#bib.bib231)]提出了持续多实例学习，以缓解WSOD中的非凸性问题。
- en: VI-B2 Salient object detection
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B2 显著物体检测
- en: Salient object detection utilizes deep neural network to predict saliency scores
    of image regions and obtain accurate saliency maps, as shown in Fig. 10\. Salient
    object detection networks usually need to aggregate multi-level features of backbone
    network. For fast speed without accuracy dropping, Wu et al. [[232](#bib.bib232)]
    present that discarding the shallower layer features can achieve fast speed and
    the deeper layer features are sufficient to obtain precisely salient map. Liu
    et al. [[233](#bib.bib233)] expand the role of pooling in convolutional neural
    networks. Wang et al. [[234](#bib.bib234)] utilize fixation prediction to detect
    salient objects. Wang et al. [[235](#bib.bib235)] adopt recurrent fully convolutional
    networks and incorporate saliency prior knowledge for accurate salient object
    detection. Feng et al. [[236](#bib.bib236)] design an attentive feedback module
    to better explore the structure of objects.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 显著物体检测利用深度神经网络预测图像区域的显著性分数并获得准确的显著性图，如图 10 所示。显著物体检测网络通常需要聚合骨干网络的多层特征。为了在不降低精度的情况下提高速度，吴等人[[232](#bib.bib232)]提出丢弃浅层特征可以实现快速速度，而深层特征足以获得精确的显著图。刘等人[[233](#bib.bib233)]扩展了卷积神经网络中池化的作用。王等人[[234](#bib.bib234)]利用注视预测来检测显著物体。王等人[[235](#bib.bib235)]采用递归全卷积网络，并结合显著性先验知识进行准确的显著物体检测。冯等人[[236](#bib.bib236)]设计了一个注意力反馈模块，以更好地探索物体的结构。
- en: Video salient object detection datasets [[237](#bib.bib237)][[238](#bib.bib238)][[239](#bib.bib239)][[240](#bib.bib240)][[241](#bib.bib241)][[242](#bib.bib242)][[243](#bib.bib243)]
    provide benchmarks for video salient object detection, and existing good algorithms
    [[244](#bib.bib244)] [[245](#bib.bib245)] [[238](#bib.bib238)] [[241](#bib.bib241)]
    [[246](#bib.bib246)] [[247](#bib.bib247)] [[248](#bib.bib248)] [[249](#bib.bib249)]
    [[250](#bib.bib250)] [[251](#bib.bib251)] [[252](#bib.bib252)][[253](#bib.bib253)][[254](#bib.bib254)][[255](#bib.bib255)]
    devote to the development of this field.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 视频显著物体检测数据集[[237](#bib.bib237)][[238](#bib.bib238)][[239](#bib.bib239)][[240](#bib.bib240)][[241](#bib.bib241)][[242](#bib.bib242)][[243](#bib.bib243)]提供了视频显著物体检测的基准，现有的优秀算法[[244](#bib.bib244)]
    [[245](#bib.bib245)] [[238](#bib.bib238)] [[241](#bib.bib241)] [[246](#bib.bib246)]
    [[247](#bib.bib247)] [[248](#bib.bib248)] [[249](#bib.bib249)] [[250](#bib.bib250)]
    [[251](#bib.bib251)] [[252](#bib.bib252)][[253](#bib.bib253)][[254](#bib.bib254)][[255](#bib.bib255)]致力于该领域的发展。
- en: '![Refer to caption](img/e3c636645b819532da6deb99bd644209.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e3c636645b819532da6deb99bd644209.png)'
- en: 'Figure 10: Some examples from the salient object detection datasets. (a), (c)
    are images, (b), (d) ground truth. Image from Liu et al. [[233](#bib.bib233)]
    and Wu et al. [[232](#bib.bib232)].'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：来自显著物体检测数据集的一些示例。(a)、(c)是图像，(b)、(d)是地面真值。图像来自刘等人[[233](#bib.bib233)]和吴等人[[232](#bib.bib232)]。
- en: VI-B3 Highlight detection
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B3 高亮检测
- en: Highlight detection is to retrieve a moment in a short video clip that captures
    a user’s primary attention or interest, which can accelerate browsing many videos,
    enhance social video sharing and facilitate video recommendation. Typical highlight
    detectors [[256](#bib.bib256)] [[257](#bib.bib257)] [[258](#bib.bib258)] [[259](#bib.bib259)]
    [[260](#bib.bib260)] [[261](#bib.bib261)] are domain-specific for they are tailored
    to a category of videos. All object detection tasks require a large amount of
    manual annotation data and highlight detection is no exception. Xiong et al. [[262](#bib.bib262)]
    propose a weakly supervised method on shorter user-generated videos to address
    this issue.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 高亮检测是从短视频片段中提取捕捉用户主要注意力或兴趣的时刻，这可以加速浏览多个视频，增强社交视频分享并促进视频推荐。典型的高亮检测器 [[256](#bib.bib256)]
    [[257](#bib.bib257)] [[258](#bib.bib258)] [[259](#bib.bib259)] [[260](#bib.bib260)]
    [[261](#bib.bib261)] 是特定领域的，因为它们是针对特定类别的视频量身定制的。所有对象检测任务都需要大量的人工标注数据，高亮检测也不例外。Xiong
    等人 [[262](#bib.bib262)] 提出了一个在较短用户生成视频上进行弱监督的方法来解决这一问题。
- en: VI-B4 Edge detection
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B4 边缘检测
- en: Edge detection aims at extracting object boundaries and perceptually salient
    edges from images, which is important to a series of higher level vision tasks
    like segmentation, object detection and recognition. Edge detection meets some
    challenges. First, the edges of various scales in an image need both object-level
    boundaries and useful local region details. Second, convolutional layers of different
    levels are specialized to predict different parts of the final detection, thus
    each layer in CNN should be trained by proper layer-specific supervision. To address
    these issues, He et al. [[263](#bib.bib263)] propose a Bi-Directional Cascade
    Network to let one layer supervised by labeled edges while adopt dilated convolution
    to generate multi-scale features. Liu et al. [[264](#bib.bib264)] present an accurate
    edge detector which utilizes richer convolutional features.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘检测旨在从图像中提取对象边界和感知显著边缘，这对一系列高级视觉任务如分割、对象检测和识别非常重要。边缘检测面临一些挑战。首先，图像中的不同尺度的边缘需要同时考虑对象级边界和有用的局部区域细节。其次，不同层级的卷积层专门用于预测最终检测的不同部分，因此
    CNN 中的每一层应通过适当的层级特定监督进行训练。为了解决这些问题，He 等人 [[263](#bib.bib263)] 提出了一个双向级联网络，使一层由标注边缘进行监督，同时采用扩张卷积生成多尺度特征。Liu
    等人 [[264](#bib.bib264)] 提出了一个准确的边缘检测器，利用了更丰富的卷积特征。
- en: VI-B5 Text detection
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B5 文本检测
- en: Text detection aims to identify text regions of given images or videos which
    is also an important prerequisite for many computer vision tasks, such as classification,
    video analysis. There have been many successful commercial optical character recognition
    (OCR) systems for internet content and documentary texts recognition. The detection
    of text in natural scenes remains a challenge due to complex situations such as
    blurring, uneven lighting, perspective distortion, various orientation. Some typical
    works [[265](#bib.bib265)][[266](#bib.bib266)][[267](#bib.bib267)] focus on horizontal
    or nearly horizontal text detection. Recently, researchers find that arbitrary-oriented
    text detection [[268](#bib.bib268)][[269](#bib.bib269)][[270](#bib.bib270)][[271](#bib.bib271)][[272](#bib.bib272)]
    is a direction that needs to pay attention to. In general, deep learning based
    scene text detection methods can be classified into two categories. The first
    category takes scene text as a type of general object, following the general object
    detection paradigm and locating scene text by text box regression. These methods
    have difficulties to deal with the large aspect ratios and arbitrary-orientation
    of scene text. The second one directly segments text regions, but mostly requires
    complicated post-processing step. Usually, some methods in this category mainly
    involve two steps, segmentation (generating text prediction maps) and geometric
    approaches (for inclined proposals), which is time-consuming. In addition, in
    order to obtain the desired orientation of text boxes, some methods require complex
    post-processing step, so it’s not as efficient as those architectures that are
    directly based on detection networks.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 文本检测旨在识别给定图像或视频中的文本区域，这也是许多计算机视觉任务（如分类、视频分析）的重要前提。已经有许多成功的商业光学字符识别（OCR）系统用于互联网内容和文献文本的识别。然而，在自然场景中检测文本仍然是一个挑战，因为存在模糊、不均匀的光照、透视失真和各种方向等复杂情况。一些典型的工作
    [[265](#bib.bib265)][[266](#bib.bib266)][[267](#bib.bib267)] 专注于水平或近水平文本的检测。最近，研究人员发现任意方向的文本检测
    [[268](#bib.bib268)][[269](#bib.bib269)][[270](#bib.bib270)][[271](#bib.bib271)][[272](#bib.bib272)]
    是一个值得关注的方向。一般来说，基于深度学习的场景文本检测方法可以分为两类。第一类将场景文本视为一般对象，遵循一般目标检测范式，通过文本框回归来定位场景文本。这些方法在处理大纵横比和任意方向的场景文本时存在困难。第二类直接对文本区域进行分割，但大多数需要复杂的后处理步骤。通常，这类方法主要包括两个步骤：分割（生成文本预测图）和几何方法（用于倾斜提议），这非常耗时。此外，为了获得所需的文本框方向，一些方法需要复杂的后处理步骤，因此效率不如那些直接基于检测网络的架构。
- en: Lyu et al. [[271](#bib.bib271)] combine the ideas of the two categories above
    avoiding their shortcomings by locating corner points of text bounding boxes and
    dividing text regions in relative positions to detect scene text, which can handle
    long oriented text and only need a simple NMS post-processing step. Ma et al.
    [[272](#bib.bib272)] develop a novel rotation-based approach and an end-to-end
    text detection system in which Rotation Region Proposal Networks (RRPN) generate
    inclined proposals with text orientation angle information.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Lyu 等人 [[271](#bib.bib271)] 结合了上述两类方法的思想，通过定位文本边界框的角点并将文本区域按相对位置划分来检测场景文本，从而避免了它们的缺陷。这种方法能够处理长且有方向的文本，只需一个简单的
    NMS 后处理步骤。Ma 等人 [[272](#bib.bib272)] 开发了一种新型的基于旋转的方法和端到端的文本检测系统，其中旋转区域提议网络（RRPN）生成带有文本方向角信息的倾斜提议。
- en: VI-B6 Multi-domain object detection
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B6 多领域目标检测
- en: Domain-specific detectors always achieve high detection performance on the specified
    dataset. So as to get a universal detector which is capable of working on various
    image domains, recently many works focus on training a multi-domain detector while
    do not require prior knowledge of the newly domain of interest. Wang et al. [[273](#bib.bib273)]
    propose a universal detector which utilizes a new domain-attention mechanism working
    on a variety of image domains (human faces, traffic signs and medical CT images)
    without prior knowledge of the domain of interest. Wang et al. [[273](#bib.bib273)]
    release a newly established universal object detection benchmark consisting of
    11 diverse datasets to better meet the challenges of generalization in different
    domains.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 特定领域的检测器在指定的数据集上总是能取得高检测性能。为了获得能够在各种图像领域上工作的通用检测器，最近许多工作集中在训练一个多领域检测器，而不需要对新领域有先验知识。Wang
    等人 [[273](#bib.bib273)] 提出了一个通用检测器，它利用一种新的领域注意机制，能够在多种图像领域（如人脸、交通标志和医学 CT 图像）上工作，而无需先验领域知识。Wang
    等人 [[273](#bib.bib273)] 发布了一个新建立的通用目标检测基准，包含 11 个多样化的数据集，以更好地应对不同领域的泛化挑战。
- en: To learn a universal representation of vision, Bilen et al. [[274](#bib.bib274)]
    add domain-specific BN (batch normalization) layers to a multi-domain shared network.
    Rebuffi et al. [[275](#bib.bib275)] propose adapter residual modules which achieve
    a high degree of parameter sharing while maintaining or even improving the accuracy
    of domain-specific representations. Rebuffi et al. [[275](#bib.bib275)] introduce
    the Visual Decathlon Challenge, a benchmark contains ten very different visual
    domains. Inspired by transfer learning, Rebuffi et al. [[276](#bib.bib276)] empirically
    study efficient parameterizations and outperform traditional fine-tuning techniques.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习视觉的通用表示，Bilen 等人 [[274](#bib.bib274)] 向多领域共享网络中添加了领域特定的 BN（批量归一化）层。Rebuffi
    等人 [[275](#bib.bib275)] 提出了适配器残差模块，这些模块在保持或甚至提高领域特定表示的准确性的同时，实现了高度的参数共享。Rebuffi
    等人 [[275](#bib.bib275)] 引入了视觉十项挑战，这是一个包含十个非常不同视觉领域的基准。受转移学习的启发，Rebuffi 等人 [[276](#bib.bib276)]
    通过实证研究高效的参数化，并超越了传统的微调技术。
- en: Another requirement for multi-domain object detection is to reduce annotation
    costs. Object detection datasets need heavily annotation works which is time consuming
    and mechanical. Transferring pre-trained models from label-rich domains to label-poor
    datasets can solve label-poor detection works. One way is to use unsupervised
    domain adaptation methods to tackle dataset bias problems. In recent years, researchers
    have adopted adversarial learning to align the source and target distribution
    of samples. Chen et al. [[277](#bib.bib277)] utilize Faster R-CNN with a domain
    classifier trained to distinguish source and target samples, like adversarial
    learning, where the feature extractor learns to deceive the domain classifier.
    Saito et al. [[278](#bib.bib278)] propose a weak alignment model to focus on similarity
    between different images from domains with large discrepancy rather than aligning
    images that are globally dissimilar. Only in the source domain manual annotations
    are available, which can be addressed by using Unsupervised Domain Adaptation
    methods. Haupmann et al. [[279](#bib.bib279)] propose an Unsupervised Domain Adaptation
    method which models both intra-class and inter-class domain discrepancy.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 多领域目标检测的另一个要求是降低标注成本。目标检测数据集需要大量的标注工作，这些工作既耗时又机械。将预训练模型从标注丰富的领域转移到标注稀少的数据集可以解决标注稀缺的问题。一个方法是使用无监督领域适应方法来处理数据集偏差问题。近年来，研究人员采用了对抗学习来对齐源域和目标域样本的分布。Chen
    等人 [[277](#bib.bib277)] 使用了 Faster R-CNN 和一个领域分类器进行训练，以区分源域和目标域样本，类似于对抗学习，其中特征提取器学习欺骗领域分类器。Saito
    等人 [[278](#bib.bib278)] 提出了一个弱对齐模型，专注于具有较大差异的领域中不同图像之间的相似性，而不是对齐全局上不相似的图像。只有在源域中提供了手动标注，这可以通过使用无监督领域适应方法来解决。Haupmann
    等人 [[279](#bib.bib279)] 提出了一个无监督领域适应方法，建模了类内和类间的领域差异。
- en: VI-B7 Object detection in videos
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B7 视频中的目标检测
- en: Object detection in videos aims at detecting objects in videos, which brings
    additional challenges due to degraded image qualities such as motion blur and
    video defocus, leading to unstable classifications for the same object across
    video. Video detectors [[280](#bib.bib280)][[281](#bib.bib281)][[282](#bib.bib282)][[283](#bib.bib283)][[284](#bib.bib284)][[285](#bib.bib285)][[286](#bib.bib286)][[287](#bib.bib287)][[288](#bib.bib288)][[289](#bib.bib289)]
    exploit temporal contexts to meet this challenge. Some static detectors [[280](#bib.bib280)][[281](#bib.bib281)][[282](#bib.bib282)][[283](#bib.bib283)]
    first detect objects in each frame then check them by linking detections of the
    same object in neighbor frames. Due to object motion, the same object in neighbor
    frames may not have a large overlap. On the other hand, the predicted object movements
    are not accurate enough to link neighbor frames. Tang et al. [[290](#bib.bib290)]
    propose an architecture which links objects in the same frame instead of neighboring
    frames to address it.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 视频中的目标检测旨在检测视频中的对象，由于图像质量退化（如运动模糊和视频失焦），这带来了额外的挑战，导致同一对象在视频中的分类不稳定。视频检测器[[280](#bib.bib280)][[281](#bib.bib281)][[282](#bib.bib282)][[283](#bib.bib283)][[284](#bib.bib284)][[285](#bib.bib285)][[286](#bib.bib286)][[287](#bib.bib287)][[288](#bib.bib288)][[289](#bib.bib289)]利用时间上下文来应对这一挑战。一些静态检测器[[280](#bib.bib280)][[281](#bib.bib281)][[282](#bib.bib282)][[283](#bib.bib283)]首先在每一帧中检测对象，然后通过链接相邻帧中的同一对象的检测结果来检查它们。由于对象运动，相邻帧中的同一对象可能没有大的重叠。另一方面，预测的对象移动不够准确，无法链接相邻帧。Tang等人[[290](#bib.bib290)]提出了一种架构，通过在同一帧内链接对象而不是相邻帧来解决这一问题。
- en: VI-B8 Point clouds 3D object detection
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B8 点云3D对象检测
- en: Compared to image based detection, LiDAR point cloud provides reliable depth
    information that can be used to accurately locate objects and characterize their
    shapes. In autonomous navigation, autonomous driving, housekeeping robots and
    augmented/virtual reality applications, LiDAR point cloud based 3D object detection
    plays an important role. Point cloud based 3D object detection meets some challenges,
    the sparsity of LiDAR point clouds, highly variable point density, non-uniform
    sampling of the 3D space, effective range of the sensors, occlusion, and the relative
    pose variation. Engelcke et al. [[291](#bib.bib291)] first propose sparse convolutional
    layers and L1 regularization for efficient large-scale processing of 3D data.
    Qi et al. [[292](#bib.bib292)] raise an end-to-end deep neural network called
    PointNet, which learns point-wise features directly from point clouds. Qi et al.
    [[293](#bib.bib293)] improve PointNet which learns local structures at different
    scales. Zhou et al. [[294](#bib.bib294)] close the gap between RPN and point set
    feature learning for 3D detection task. Zhou et al. [[294](#bib.bib294)] present
    a generic end-to-end 3D detection framework called VoxelNet, which learns a discriminative
    feature representation from point clouds and predicts accurate 3D bounding boxes
    simultaneously.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于图像的检测相比，LiDAR点云提供了可靠的深度信息，可以用于准确定位对象和描述其形状。在自主导航、自主驾驶、家政机器人以及增强/虚拟现实应用中，基于LiDAR点云的3D对象检测扮演了重要角色。点云基础的3D对象检测面临一些挑战，包括LiDAR点云的稀疏性、高度变化的点密度、3D空间的非均匀采样、传感器的有效范围、遮挡和相对姿态变化。Engelcke等人[[291](#bib.bib291)]首次提出了稀疏卷积层和L1正则化，用于高效的大规模3D数据处理。Qi等人[[292](#bib.bib292)]提出了一种名为PointNet的端到端深度神经网络，直接从点云中学习点级特征。Qi等人[[293](#bib.bib293)]改进了PointNet，使其能够在不同尺度下学习局部结构。Zhou等人[[294](#bib.bib294)]缩小了RPN和点集特征学习之间的差距，以完成3D检测任务。Zhou等人[[294](#bib.bib294)]提出了一种通用的端到端3D检测框架，名为VoxelNet，它从点云中学习区分性的特征表示，并同时预测准确的3D边界框。
- en: In autonomous driving application, Chen et al. [[295](#bib.bib295)] perform
    3D object detection from a single monocular image. Chen et al. [[296](#bib.bib296)]
    take both LiDAR point cloud and RGB images as input then predict oriented 3D bounding
    boxes for high-accuracy 3D object detection. Example 3D detection result is shown
    in Fig. 11.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在自主驾驶应用中，Chen等人[[295](#bib.bib295)]从单张单目图像中执行3D对象检测。Chen等人[[296](#bib.bib296)]同时采用LiDAR点云和RGB图像作为输入，然后预测定向的3D边界框以实现高精度的3D对象检测。图11展示了示例3D检测结果。
- en: '![Refer to caption](img/480162e21b113023c43c5fc6901c1f3e.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/480162e21b113023c43c5fc6901c1f3e.png)'
- en: 'Figure 11: Example 3D detection result from the KITTI validation set projected
    onto an image. Image from Vishwanath A. Sindagi et al. [[297](#bib.bib297)].'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：从KITTI验证集投影到图像上的3D检测结果示例。图像来自Vishwanath A. Sindagi等人[[297](#bib.bib297)]。
- en: VI-B9 2D, 3D pose detection
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B9 2D，3D姿态检测
- en: Human pose detection aims at estimating the 2D or 3D pose location of the body
    joints and defining pose classes then returning the average pose of the top scoring
    class, as shown in Fig. 12\. Typical 2D human pose estimation methods [[298](#bib.bib298)][[299](#bib.bib299)][[300](#bib.bib300)][[301](#bib.bib301)][[302](#bib.bib302)][[303](#bib.bib303)][[304](#bib.bib304)]
    utilize deep CNN architectures. Rogez et al. [[305](#bib.bib305)] propose an end-to-end
    architecture for joint 2D and 3D human pose estimation in natural images which
    predicts 2D and 3D poses of multiple people simultaneously. Benefit by full-body
    3D pose, it can recover body part locations in cases of occlusion between different
    targets. Human pose estimation approaches can be divided into two categories,
    one-stage and multi-stage methods. The best performing methods [[306](#bib.bib306)][[11](#bib.bib11)][[307](#bib.bib307)][[308](#bib.bib308)]
    typically base on one-stage backbone networks. The most representative multi-stage
    methods are convolutional pose machine [[309](#bib.bib309)], Hourglass network
    [[300](#bib.bib300)], and MSPN [[310](#bib.bib310)].
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态检测旨在估计身体关节的2D或3D姿态位置，并定义姿态类别，然后返回得分最高类别的平均姿态，如图12所示。典型的2D人体姿态估计方法[[298](#bib.bib298)][[299](#bib.bib299)][[300](#bib.bib300)][[301](#bib.bib301)][[302](#bib.bib302)][[303](#bib.bib303)][[304](#bib.bib304)]利用深度CNN架构。Rogez等人[[305](#bib.bib305)]提出了一种端到端架构，用于自然图像中的2D和3D人体姿态估计，同时预测多人的2D和3D姿态。通过全身3D姿态的好处，它可以在不同目标之间出现遮挡的情况下恢复身体部位的位置。人体姿态估计方法可以分为两类，一阶段和多阶段方法。表现最好的方法[[306](#bib.bib306)][[11](#bib.bib11)][[307](#bib.bib307)][[308](#bib.bib308)]通常基于一阶段骨干网络。最具代表性的多阶段方法是卷积姿态机[[309](#bib.bib309)]、Hourglass网络[[300](#bib.bib300)]和MSPN[[310](#bib.bib310)]。
- en: '![Refer to caption](img/04d3b657426ca5dfc67ef6497eec08ba.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/04d3b657426ca5dfc67ef6497eec08ba.png)'
- en: 'Figure 12: Some examples of multi-person pose estimation. Image from Chen et
    al. [[306](#bib.bib306)].'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：多人人体姿态估计的一些示例。图片来源于Chen等人[[306](#bib.bib306)]。
- en: VI-B10 Fine-Grained Visual Recognition
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B10 精细视觉识别
- en: Fine-grained recognition aims to identify an exact category of objects in each
    basic-level category, such as identifying the species of a bird, or the model
    of an aircraft. This task is quite challenging because the visual differences
    between the categories are small and can be easily overwhelmed by those caused
    by factors such as pose, viewpoint, and location of the object in the image. To
    generalize across viewpoints, Krause et al. [[311](#bib.bib311)] utilize 3D object
    representations on the level of both local feature appearance and location. Lin
    et al. [[312](#bib.bib312)] introduce bilinear models that consists of two feature
    extractors (two CNN streams). The outputs of these two feature extractors are
    multiplied using outer product at each location of the image and then pooled to
    obtain an image descriptor. He et al. [[313](#bib.bib313)] introduce a fine-grained
    discriminative localization method via saliency-guided Faster R-CNN. After that,
    He et al. [[314](#bib.bib314)] propose a weakly supervised discriminative localization
    approach (WSDL) for fast fine-grained image classification. Classical datasets
    [[315](#bib.bib315)] [[316](#bib.bib316)] provide useful information on some interesting
    categories. Please refer to a survey [[317](#bib.bib317)] for more details.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 精细识别旨在识别每个基本类别中的准确对象类别，例如识别鸟的种类或飞机的型号。这个任务相当具有挑战性，因为类别之间的视觉差异很小，容易被姿态、视角和图像中物体的位置等因素造成的差异所掩盖。为了跨视角进行泛化，Krause等人[[311](#bib.bib311)]利用了局部特征外观和位置的3D对象表示。Lin等人[[312](#bib.bib312)]引入了由两个特征提取器（两个CNN流）组成的双线性模型。这两个特征提取器的输出通过外积在图像的每个位置相乘，然后进行池化以获得图像描述符。He等人[[313](#bib.bib313)]通过显著性引导的Faster
    R-CNN提出了一种精细化判别定位方法。之后，He等人[[314](#bib.bib314)]提出了一种弱监督判别定位方法（WSDL），用于快速精细图像分类。经典数据集[[315](#bib.bib315)]
    [[316](#bib.bib316)]提供了有关一些有趣类别的有用信息。更多细节请参见综述[[317](#bib.bib317)]。
- en: VII Conclusions and trends
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论与趋势
- en: VII-A Conclusions
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 结论
- en: With the continuous upgrading of powerful computing equipment, object detection
    technology based on deep learning has been developed rapidly. In order to deploy
    on more accurate applications, the need for high precision real-time systems is
    becoming more and more urgent. Since achieving high accuracy and efficiency detectors
    is the ultimate goal of this task, researchers have developed a series of directions
    such as, constructing new architecture, extracting rich features, exploiting good
    representations, improving processing speed, training from scratch, anchor-free
    methods, solving sophisticated scene issues (small objects, occluded objects),
    combining one-stage and two-stage detectors to make good results, improving post-processing
    NMS method, solving negatives-positives imbalance issue, increasing localization
    accuracy, enhancing classification confidence. With the increasingly powerful
    object detectors in security field, military field, transportation field, medical
    field, and life field, the application of object detection is gradually extensive.
    In addition, a variety of branches in detection domain arise. Although the achievement
    of this domain has been effective recently, there is still much room for further
    development.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 随着强大计算设备的不断升级，基于深度学习的目标检测技术得到了快速发展。为了在更精确的应用上部署，对高精度实时系统的需求变得越来越迫切。由于实现高准确度和高效率的检测器是该任务的**终极目标**，研究人员已经开发了一系列方向，如构建新架构、提取丰富特征、利用良好表示、提高处理速度、从头训练、无锚方法、解决复杂场景问题（小目标、遮挡目标）、结合单阶段和两阶段检测器以获得良好结果、改进后处理NMS方法、解决负正样本不平衡问题、提高定位准确性、增强分类置信度。随着安全领域、军事领域、交通领域、医疗领域和生活领域中目标检测器的不断强大，目标检测的应用逐渐广泛。此外，检测领域还出现了多种分支。尽管这一领域最近取得了有效成果，但仍有很大的发展空间。
- en: VII-B Trends
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 趋势
- en: VII-B1 Combining one-stage and two-stage detectors
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B1 结合单阶段和两阶段检测器
- en: On the one hand, the two-stage detectors have a densely tailing process to obtain
    as many as reference boxes, which is time consuming and inefficient. To address
    this issue, researchers are required to eliminate so much redundancy while maintaining
    high accuracy. On the other hand, the one-stage detectors achieve fast processing
    speed which have been used successfully in real-time applications. Although fast,
    the lower accuracy is still a bottleneck for high precision requirements. How
    to combine the advantages of both one-stage and two-stage detectors remains a
    big challenge.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，两阶段检测器有一个密集的尾部过程来获得尽可能多的参考框，这既耗时又低效。为了解决这个问题，研究人员需要在保持高准确度的同时消除多余的部分。另一方面，单阶段检测器实现了快速处理速度，已成功应用于实时应用。尽管速度快，但较低的准确性仍然是高精度要求的瓶颈。如何结合单阶段和两阶段检测器的优势仍然是一个重大挑战。
- en: VII-B2 Video object detection
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B2 视频目标检测
- en: In video object detection, motion blur, video defocus, motion target ambiguity,
    intense target movements, small targets, occlusion and truncation etc. make it
    difficult for this task to achieve good performance in real life scene and remote
    sensing scene. Delving into moving goals and more complex source data such as
    video is one of the key points for future research.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在视频目标检测中，运动模糊、视频失焦、运动目标模糊、目标剧烈运动、小目标、遮挡和截断等使得这一任务在现实场景和遥感场景中难以取得良好性能。深入研究移动目标和更复杂的源数据如视频是未来研究的关键点之一。
- en: VII-B3 Efficient post-processing methods
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B3 高效的后处理方法
- en: In the three (for one-stage detectors) or four (for two-stage detectors) stage
    detection procedure, post-processing is an initial step for the final results.
    On most of the detection metrics, only the highest prediction result of one object
    can be send to the metric program to calculate accuracy score. The post-processing
    methods like NMS and its improvements may eliminate well located but high classification
    confidence objects, which is detrimental to the accuracy of the measurement. Exploiting
    more efficient and accurate post-processing method is another direction for object
    detection domain.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在三阶段（对于单阶段检测器）或四阶段（对于两阶段检测器）检测过程中，后处理是最终结果的初始步骤。在大多数检测指标中，只有一个对象的最高预测结果可以发送到指标程序以计算准确度评分。像NMS及其改进这样的后处理方法可能会消除定位良好但分类置信度高的对象，这对测量的准确性是有害的。开发更高效、更准确的后处理方法是目标检测领域的另一个方向。
- en: VII-B4 Weakly supervised object detection methods
  id: totrans-326
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B4 弱监督目标检测方法
- en: Utilizing high proportion labelled images only with object class but not with
    object bounding box to replace a large amount of fully annotated images to train
    the network is of high efficiency and easy to get. Weakly supervised object detection
    (WSOD) aims at utilizing a few fully annotated images (supervision) to detect
    a large amount of non-fully annotated ones. Therefore developing WSOD methods
    is a significant problem for further study.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 利用仅有目标类别而没有目标边界框的高比例标记图像来替代大量完全标注的图像以训练网络是高效且易于获得的。弱监督目标检测（WSOD）旨在利用少量完全标注的图像（监督）来检测大量非完全标注的图像。因此，开发
    WSOD 方法是进一步研究的一个重要问题。
- en: VII-B5 Multi-domain object detection
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B5 多领域目标检测
- en: Domain-specific detectors always achieve high detection performance on the specified
    dataset. So as to get a universal detector which is capable of working on various
    image domains, multi-domain detectors can solve this problem without prior knowledge
    of new domain. Domain transfer is a challenging mission for further study.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 特定领域的检测器总是在指定的数据集上表现出色。为了获得一个能够在各种图像领域中工作的通用检测器，多领域检测器可以在没有新领域先验知识的情况下解决这个问题。领域迁移是一个需要进一步研究的挑战性任务。
- en: VII-B6 3D object detection
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B6 3D 目标检测
- en: With the advent of 3D sensors and diverse applications of 3D understanding,
    3D object detection gradually becomes a hot research direction. Compared to 2D
    image based detection, LiDAR point cloud provides reliable depth information that
    can be used to accurately locate objects and characterize their shapes. LiDAR
    enables accurate localization of objects in the 3D space. Object detection techniques
    based on LiDAR data often outperform the 2D counterparts as well.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 3D 传感器的出现和 3D 理解的多样化应用，3D 目标检测逐渐成为一个热门研究方向。与基于 2D 图像的检测相比，LiDAR 点云提供了可靠的深度信息，可以用于准确定位对象并描述其形状。LiDAR
    使得对象在 3D 空间中的准确定位成为可能。基于 LiDAR 数据的目标检测技术通常也优于 2D 对应方法。
- en: VII-B7 Salient object detection
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B7 突出目标检测
- en: Salient object detection (SOD) aims at highlighting salient object regions in
    images. Video object detection is to classify and locate objects of interest in
    a continuous scene. SOD is driven by and applied to a widely spectrum of object-level
    applications in various areas. Given salient object regions of interest in each
    frame can assist accurate object detection in videos. Therefore, for high-level
    recognition task and challenging detection task, highlighting target detection
    is a crucial preliminary process.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 突出目标检测（SOD）的目标是强调图像中的显著目标区域。视频目标检测旨在对连续场景中的感兴趣目标进行分类和定位。SOD 由广泛的目标级应用驱动，并被应用于各种领域。每一帧中的显著目标区域有助于在视频中准确检测目标。因此，对于高级识别任务和具有挑战性的检测任务，突出目标检测是一个至关重要的初步过程。
- en: VII-B8 Unsupervised object detection
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B8 无监督目标检测
- en: Supervised methods are time consuming and inefficient in training process, which
    need well annotated dataset used for supervision information. Annotating a bounding
    box for each object in large datasets is expensive, laborious and impractical.
    Developing automatic annotation technology to release human annotation work is
    a promising trend for unsupervised object detection. Unsupervised object detection
    is a future research direction for intelligent detection mission.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督方法在训练过程中耗时且效率低下，需要良好标注的数据集来提供监督信息。为大型数据集中的每个目标标注边界框既昂贵又费力，不切实际。开发自动标注技术以减少人工标注工作是无监督目标检测的一个有前景的趋势。无监督目标检测是智能检测任务的未来研究方向。
- en: VII-B9 Multi-task learning
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B9 多任务学习
- en: Aggregating multi-level features of backbone network is a significant way to
    improve detection performance. Furthermore, performing multiple computer vision
    tasks simultaneously such as object detection, semantic segmentation, instance
    segmentation, edge detection, highlight detection can enhance performance of separate
    task by a large margin because of richer information. Adopting multi-task learning
    is a good way to aggregate multiple tasks in a network, and it presents great
    challenges to researchers to maintain processing speed and improve accuracy as
    well.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合骨干网络的多级特征是提高检测性能的重要方法。此外，同时执行多个计算机视觉任务，如目标检测、语义分割、实例分割、边缘检测、突出检测，可以通过丰富的信息大幅提升单独任务的性能。采用多任务学习是将多个任务聚合到网络中的一种好方法，并且对研究人员提出了保持处理速度和提高准确性的巨大挑战。
- en: VII-B10 Multi-source information assistance
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B10 多源信息辅助
- en: Due to the popularity of social media and the development of big data technology,
    multi-source information becomes easy to access. Many social media information
    can provide both pictures and descriptions of them in textual form, which can
    help detection task. Fusing multi-source information is an emerging research direction
    with the development of various technologies.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 由于社交媒体的普及和大数据技术的发展，多源信息变得易于获取。许多社交媒体信息不仅提供图片，还提供文字描述，这有助于检测任务。随着各种技术的发展，融合多源信息成为一个新兴的研究方向。
- en: VII-B11 Constructing terminal object detection system
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B11 构建终端目标检测系统
- en: From the cloud to the terminal, the terminalization of artificial intelligence
    can help people deal with mass information and solve problems better and faster.
    With the emergence of lightweight networks, terminal detectors are developed into
    more efficient and reliable devices with broad application scenarios. The chip
    detection network based on FPGA will make real-time application possible.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 从云端到终端，人工智能的终端化可以帮助人们更好、更快地处理大量信息并解决问题。随着轻量级网络的出现，终端检测器发展成为更高效、更可靠的设备，具有广泛的应用场景。基于FPGA的芯片检测网络将使实时应用成为可能。
- en: VII-B12 Medical imaging and diagnosis
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B12 医学成像与诊断
- en: FDA (U.S. Food and Drug Administration) is promoting “AI-based Medical Devices”.
    In April 2018, FDA first approved an artificial intelligence software called IDx-DR,
    a diabetic retinopathy detector with an accuracy of more than 87.4%. For customers,
    the combination of image recognition systems and mobile devices can make cell
    phone a powerful family diagnostic tool. This direction is full of challenges
    and expectations.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: FDA（美国食品药品监督管理局）正在推动“基于AI的医疗设备”。2018年4月，FDA首次批准了一款名为IDx-DR的人工智能软件，这是一种糖尿病视网膜病变检测器，其准确率超过87.4%。对于用户而言，图像识别系统与移动设备的结合可以使手机成为一个强大的家庭诊断工具。这个方向充满了挑战和期望。
- en: VII-B13 Advanced medical biometrics
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B13 高级医疗生物特征识别
- en: Utilizing deep neural network, researchers began to study and measure atypical
    risk factors that had been difficult to quantify previously. Using neural networks
    to analyze retinal images and speech patterns may help identify the risk of heart
    disease. In the near future, medical biometrics will be used for passive monitoring.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 利用深度神经网络，研究人员开始研究和测量以前难以量化的非典型风险因素。使用神经网络分析视网膜图像和语音模式可能有助于识别心脏病的风险。在不久的将来，医疗生物特征识别将用于被动监测。
- en: VII-B14 Remote sensing airborne and real-time detection
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B14 遥感空中和实时检测
- en: Both military and agricultural fields require accurate analysis of remote sensing
    images. Automated detection software and integrated hardware will bring unprecedented
    development to these fields. Loading deep learning based object detection system
    to SoC (System on Chip) realizes real-time high-altitude detection.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 军事和农业领域都需要对遥感图像进行准确分析。自动化检测软件和集成硬件将为这些领域带来前所未有的发展。将基于深度学习的目标检测系统加载到SoC（系统芯片）上，实现了实时高空检测。
- en: VII-B15 GAN based detector
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B15 基于GAN的检测器
- en: Deep learning based systems always require large amounts of data for training,
    whereas Generative Adversarial Network is a powerful structure to generate fake
    images. How much you need, how much it can produce. Mixing the real world scene
    and simulated data generated by GAN trains object detector to make the detector
    grow more robust and obtain stronger generalization ability.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的系统通常需要大量数据进行训练，而生成对抗网络（GAN）是一种强大的生成假图像的结构。你需要多少，它就能产生多少。将真实世界场景与GAN生成的模拟数据混合训练目标检测器，使检测器变得更加稳健，获得更强的泛化能力。
- en: The research of object detection still needs further study. We hope that deep
    learning methods will make breakthroughs in the near future.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测的研究仍需进一步探索。我们希望深度学习方法在不久的将来能够取得突破。
- en: Acknowledgment
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Thanks to the scholars involved in this paper. This paper quotes the research
    literature of several scholars. Without the help and inspiration of the research
    results of all scholars, it would be difficult for me to complete the writing
    of this paper.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢参与本论文的学者们。本论文引用了几位学者的研究文献。如果没有所有学者的研究成果提供的帮助和启发，我很难完成论文的写作。
- en: We would like to express our gratitude to all those who helped us during the
    writing of this thesis.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要向在本论文写作过程中给予我们帮助的所有人表示感谢。
- en: Reference
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] P. Dollar, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detection:
    An evaluation of the state of the art,” IEEE Transactions on Pattern Analysis
    and Machine Intelligence, vol. 34, pp. 743–761, April 2012.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] P. Dollar, C. Wojek, B. Schiele, 和 P. Perona，“行人检测：对现状的评估”，《IEEE 模式分析与机器智能汇刊》，第34卷，第743–761页，2012年4月。'
- en: '[2] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in 2012 IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 3354–3361, June 2012.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Geiger, P. Lenz, 和 R. Urtasun，“我们准备好进行自动驾驶了吗？KITTI 视觉基准套件”，收录于2012 IEEE计算机视觉与模式识别大会，第3354–3361页，2012年6月。'
- en: '[3] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “Imagenet large
    scale visual recognition challenge,” International Journal of Computer Vision,
    vol. 115, pp. 211–252, Dec 2015.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, 和 L. Fei-Fei，“Imagenet 大规模视觉识别挑战”，《计算机视觉国际期刊》，第115卷，第211–252页，2015年12月。'
- en: '[4] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes (voc) challenge,” International Journal of Computer
    Vision, vol. 88, pp. 303–338, Jun 2010.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, 和 A. Zisserman，“PASCAL
    视觉物体类别（VOC）挑战”，《计算机视觉国际期刊》，第88卷，第303–338页，2010年6月。'
- en: '[5] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in Computer Vision
    – ECCV 2014 (D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, eds.), (Cham),
    pp. 740–755, Springer International Publishing, 2014.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    和 C. L. Zitnick，“Microsoft COCO：上下文中的常见物体”，收录于《计算机视觉 – ECCV 2014》（D. Fleet, T.
    Pajdla, B. Schiele, 和 T. Tuytelaars 编），（Cham），第740–755页，Springer International
    Publishing，2014年。'
- en: '[6] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset,
    S. Kamali, S. Popov, M. Malloci, T. Duerig, and V. Ferrari, “The open images dataset
    v4: Unified image classification, object detection, and visual relationship detection
    at scale,” arXiv:1811.00982, 2018.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset,
    S. Kamali, S. Popov, M. Malloci, T. Duerig, 和 V. Ferrari，“开放图像数据集 v4：统一的图像分类、物体检测和视觉关系检测”，arXiv:1811.00982，2018年。'
- en: '[7] P. Zhu, L. Wen, X. Bian, H. Ling, and Q. Hu, “Vision meets drones: A challenge,”
    arXiv preprint arXiv:1804.07437, 2018.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] P. Zhu, L. Wen, X. Bian, H. Ling, 和 Q. Hu，“视觉与无人机的结合：一个挑战”，arXiv 预印本 arXiv:1804.07437，2018年。'
- en: '[8] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” IEEE Transactions on Pattern
    Analysis and Machine Intelligence, vol. 39, pp. 1137–1149, June 2017.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] S. Ren, K. He, R. Girshick, 和 J. Sun，“Faster R-CNN：基于区域提议网络的实时物体检测”，《IEEE
    模式分析与机器智能汇刊》，第39卷，第1137–1149页，2017年6月。'
- en: '[9] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in 2016 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), pp. 779–788, June 2016.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] J. Redmon, S. Divvala, R. Girshick, 和 A. Farhadi，“你只需看一次：统一的实时物体检测”，收录于2016
    IEEE计算机视觉与模式识别大会（CVPR），第779–788页，2016年6月。'
- en: '[10] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single shot multibox detector,” in Computer Vision – ECCV 2016 (B. Leibe,
    J. Matas, N. Sebe, and M. Welling, eds.), (Cham), pp. 21–37, Springer International
    Publishing, 2016.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, 和 A. C.
    Berg，“SSD：单次检测多框器”，收录于《计算机视觉 – ECCV 2016》（B. Leibe, J. Matas, N. Sebe, 和 M. Welling
    编），（Cham），第21–37页，Springer International Publishing，2016年。'
- en: '[11] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in 2017
    IEEE International Conference on Computer Vision (ICCV), pp. 2980–2988, Oct 2017.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] K. He, G. Gkioxari, P. Dollár, 和 R. Girshick，“Mask R-CNN”，收录于2017 IEEE
    国际计算机视觉大会（ICCV），第2980–2988页，2017年10月。'
- en: '[12] A. Khan, A. Sohail, U. Zahoora, and A. S. Qureshi, “A survey of the recent
    architectures of deep convolutional neural networks,” arXiv preprint arXiv:1901.06032,
    2019.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Khan, A. Sohail, U. Zahoora, 和 A. S. Qureshi，“深度卷积神经网络的近期架构综述”，arXiv
    预印本 arXiv:1901.06032，2019年。'
- en: '[13] Z. Zou, Z. Shi, Y. Guo, and J. Ye, “Object detection in 20 years: A survey,”
    CoRR, vol. abs/1905.05055, 2019.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Z. Zou, Z. Shi, Y. Guo, 和 J. Ye，“20年来的物体检测：综述”，CoRR，第abs/1905.05055卷，2019年。'
- en: '[14] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietikäinen,
    “Deep learning for generic object detection: A survey,” arXiv preprint arXiv:1809.02165,
    2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, 和 M. Pietikäinen，“通用物体检测的深度学习：综述”，arXiv
    预印本 arXiv:1809.02165，2018年。'
- en: '[15] T. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in 2017 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 936–944, July 2017.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] T. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, 和 S. Belongie, “用于物体检测的特征金字塔网络，”
    发表在2017年IEEE计算机视觉与模式识别会议（CVPR）论文集中，pp. 936–944, 2017年7月。'
- en: '[16] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun, “Detnet: A backbone
    network for object detection,” arXiv preprint arXiv:1804.06215, 2018.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, 和 J. Sun, “Detnet: 用于物体检测的骨干网络，”
    arXiv预印本 arXiv:1804.06215, 2018年。'
- en: '[17] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 1492–1500, 2017.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Xie, R. Girshick, P. Dollár, Z. Tu, 和 K. He, “深度神经网络的聚合残差变换，” 发表在IEEE计算机视觉与模式识别会议论文集中，pp.
    1492–1500, 2017年。'
- en: '[18] G. Ghiasi, T.-Y. Lin, R. Pang, and Q. V. Le, “Nas-fpn: Learning scalable
    feature pyramid architecture for object detection,” arXiv preprint arXiv:1904.07392,
    2019.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] G. Ghiasi, T.-Y. Lin, R. Pang, 和 Q. V. Le, “Nas-fpn: 学习可扩展的特征金字塔架构用于物体检测，”
    arXiv预印本 arXiv:1904.07392, 2019年。'
- en: '[19] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision
    applications,” arXiv preprint arXiv:1704.04861, 2017.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M.
    Andreetto, 和 H. Adam, “Mobilenets: 高效的卷积神经网络用于移动视觉应用，” arXiv预印本 arXiv:1704.04861,
    2017年。'
- en: '[20] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufflenet: An extremely efficient
    convolutional neural network for mobile devices,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 6848–6856, 2018.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] X. Zhang, X. Zhou, M. Lin, 和 J. Sun, “Shufflenet: 一种极其高效的卷积神经网络用于移动设备，”
    发表在IEEE计算机视觉与模式识别会议论文集中，pp. 6848–6856, 2018年。'
- en: '[21] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer,
    “Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model
    size,” arXiv preprint arXiv:1602.07360, 2016.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, 和 K. Keutzer,
    “Squeezenet: 具有50倍更少参数和不到0.5 MB模型大小的Alexnet级准确度，” arXiv预印本 arXiv:1602.07360, 2016年。'
- en: '[22] F. Chollet, “Xception: Deep learning with depthwise separable convolutions,”
    in Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 1251–1258, 2017.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] F. Chollet, “Xception: 使用深度可分离卷积的深度学习，” 发表在IEEE计算机视觉与模式识别会议论文集中，pp. 1251–1258,
    2017年。'
- en: '[23] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2:
    Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 4510–4520, 2018.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, 和 L.-C. Chen, “Mobilenetv2:
    倒置残差和线性瓶颈，” 发表在IEEE计算机视觉与模式识别会议论文集中，pp. 4510–4520, 2018年。'
- en: '[24] R. J. Wang, X. Li, and C. X. Ling, “Pelee: A real-time object detection
    system on mobile devices,” in Advances in Neural Information Processing Systems,
    pp. 1963–1972, 2018.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] R. J. Wang, X. Li, 和 C. X. Ling, “Pelee: 一种移动设备上的实时物体检测系统，” 发表在神经信息处理系统进展论文集中，pp.
    1963–1972, 2018年。'
- en: '[25] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in 2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 770–778, June 2016.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] K. He, X. Zhang, S. Ren, 和 J. Sun, “用于图像识别的深度残差学习，” 发表在2016年IEEE计算机视觉与模式识别会议（CVPR）论文集中，pp.
    770–778, 2016年6月。'
- en: '[26] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” arXiv preprint arXiv:1409.1556, 2014.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深的卷积网络，” arXiv预印本 arXiv:1409.1556,
    2014年。'
- en: '[27] W. Rawat and Z. Wang, “Deep convolutional neural networks for image classification:
    A comprehensive review,” Neural computation, vol. 29, no. 9, pp. 2352–2449, 2017.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] W. Rawat 和 Z. Wang, “用于图像分类的深度卷积神经网络：综合评审，” 神经计算，卷29，第9期，pp. 2352–2449,
    2017年。'
- en: '[28] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in 2014 IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 580–587, June 2014.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] R. Girshick, J. Donahue, T. Darrell, 和 J. Malik, “用于精确物体检测和语义分割的丰富特征层次结构，”
    发表在2014年IEEE计算机视觉与模式识别会议论文集中，pp. 580–587, 2014年6月。'
- en: '[29] R. Girshick, “Fast r-cnn,” in 2015 IEEE International Conference on Computer
    Vision (ICCV), pp. 1440–1448, Dec 2015.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] R. Girshick, “Fast r-cnn，” 发表在2015年IEEE国际计算机视觉会议（ICCV）论文集中，pp. 1440–1448,
    2015年12月。'
- en: '[30] J. Redmon and A. Farhadi, “Yolo9000: Better, faster, stronger,” in 2017
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6517–6525,
    July 2017.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Redmon 和 A. Farhadi，“Yolo9000: 更好、更快、更强”，发表于2017 IEEE计算机视觉与模式识别会议（CVPR），第6517–6525页，2017年7月。'
- en: '[31] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” arXiv preprint arXiv:1502.03167,
    2015.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Ioffe 和 C. Szegedy，“批量归一化：通过减少内部协变量偏移加速深度网络训练”，arXiv预印本 arXiv:1502.03167，2015年。'
- en: '[32] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” CoRR,
    vol. abs/1804.02767, 2018.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Redmon 和 A. Farhadi，“Yolov3: 递增改进”，CoRR，卷abs/1804.02767，2018年。'
- en: '[33] T. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for dense
    object detection,” in 2017 IEEE International Conference on Computer Vision (ICCV),
    pp. 2999–3007, Oct 2017.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] T. Lin, P. Goyal, R. Girshick, K. He, 和 P. Dollár，“密集对象检测的焦点损失”，发表于2017
    IEEE国际计算机视觉会议（ICCV），第2999–3007页，2017年10月。'
- en: '[34] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, “Dssd: Deconvolutional
    single shot detector,” arXiv preprint arXiv:1701.06659, 2017.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, 和 A. C. Berg，“Dssd: 反卷积单次检测器”，arXiv预印本
    arXiv:1701.06659，2017年。'
- en: '[35] Q. Zhao, T. Sheng, Y. Wang, Z. Tang, Y. Chen, L. Cai, and H. Ling, “M2det:
    A single-shot object detector based on multi-level feature pyramid network,” arXiv
    preprint arXiv:1811.04533, 2018.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Q. Zhao, T. Sheng, Y. Wang, Z. Tang, Y. Chen, L. Cai, 和 H. Ling，“M2det:
    基于多级特征金字塔网络的单次对象检测器”，arXiv预印本 arXiv:1811.04533，2018年。'
- en: '[36] S. Zhang, L. Wen, X. Bian, Z. Lei, and S. Z. Li, “Single-shot refinement
    neural network for object detection,” in Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 4203–4212, 2018.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Zhang, L. Wen, X. Bian, Z. Lei, 和 S. Z. Li，“用于对象检测的单次修正神经网络”，发表于《IEEE计算机视觉与模式识别会议论文集》，第4203–4212页，2018年。'
- en: '[37] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei, “Relation networks for object
    detection,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 3588–3597, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] H. Hu, J. Gu, Z. Zhang, J. Dai, 和 Y. Wei，“用于对象检测的关系网络”，发表于《IEEE计算机视觉与模式识别会议论文集》，第3588–3597页，2018年。'
- en: '[38] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, “Deformable
    convolutional networks,” in Proceedings of the IEEE international conference on
    computer vision, pp. 764–773, 2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, 和 Y. Wei，“可变形卷积网络”，发表于《IEEE国际计算机视觉会议论文集》，第764–773页，2017年。'
- en: '[39] X. Zhu, H. Hu, S. Lin, and J. Dai, “Deformable convnets v2: More deformable,
    better results,” arXiv preprint arXiv:1811.11168, 2018.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] X. Zhu, H. Hu, S. Lin, 和 J. Dai，“可变形卷积网络 v2: 更加可变形，效果更佳”，arXiv预印本 arXiv:1811.11168，2018年。'
- en: '[40] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based object
    detectors with online hard example mining,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 761–769, 2016.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. Shrivastava, A. Gupta, 和 R. Girshick，“通过在线困难样本挖掘训练基于区域的对象检测器”，发表于《IEEE计算机视觉与模式识别会议论文集》，第761–769页，2016年。'
- en: '[41] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, “Inside-outside
    net: Detecting objects in context with skip pooling and recurrent neural networks,”
    in Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 2874–2883, 2016.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Bell, C. Lawrence Zitnick, K. Bala, 和 R. Girshick，“Inside-outside net:
    通过跳跃池化和递归神经网络在上下文中检测对象”，发表于《IEEE计算机视觉与模式识别会议论文集》，第2874–2883页，2016年。'
- en: '[42] J. Dai, Y. Li, K. He, and J. Sun, “R-fcn: Object detection via region-based
    fully convolutional networks,” in Advances in neural information processing systems,
    pp. 379–387, 2016.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. Dai, Y. Li, K. He, 和 J. Sun，“R-fcn: 基于区域的全卷积网络进行对象检测”，发表于《神经信息处理系统进展》，第379–387页，2016年。'
- en: '[43] Y. Zhu, C. Zhao, J. Wang, X. Zhao, Y. Wu, and H. Lu, “Couplenet: Coupling
    global structure with local parts for object detection,” in Proceedings of the
    IEEE International Conference on Computer Vision, pp. 4126–4134, 2017.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Y. Zhu, C. Zhao, J. Wang, X. Zhao, Y. Wu, 和 H. Lu，“Couplenet: 将全局结构与局部部分结合进行对象检测”，发表于《IEEE国际计算机视觉会议论文集》，第4126–4134页，2017年。'
- en: '[44] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer,
    Z. Wojna, Y. Song, S. Guadarrama, et al., “Speed/accuracy trade-offs for modern
    convolutional object detectors,” in Proceedings of the IEEE conference on computer
    vision and pattern recognition, pp. 7310–7311, 2017.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer,
    Z. Wojna, Y. Song, S. Guadarrama 等，“现代卷积对象检测器的速度/准确性权衡”，发表于《IEEE计算机视觉与模式识别会议论文集》，第7310–7311页，2017年。'
- en: '[45] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta, “Beyond skip connections:
    Top-down modulation for object detection,” arXiv preprint arXiv:1612.06851, 2016.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Shrivastava, R. Sukthankar, J. Malik, 和 A. Gupta，“超越跳跃连接：用于对象检测的自上而下调制”，arXiv预印本
    arXiv:1612.06851，2016年。'
- en: '[46] N. Bodla, B. Singh, R. Chellappa, and L. S. Davis, “Soft-nms–improving
    object detection with one line of code,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 5561–5569, 2017.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] N. Bodla, B. Singh, R. Chellappa, 和 L. S. Davis，“Soft-NMS–通过一行代码改进对象检测”，在IEEE国际计算机视觉会议论文集中，pp.
    5561–5569，2017年。'
- en: '[47] Z. Cai and N. Vasconcelos, “Cascade r-cnn: Delving into high quality object
    detection,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 6154–6162, 2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Z. Cai 和 N. Vasconcelos，“Cascade R-CNN: 深入高质量对象检测”，在IEEE计算机视觉与模式识别会议论文集中，pp.
    6154–6162，2018年。'
- en: '[48] B. Singh and L. S. Davis, “An analysis of scale invariance in object detection
    snip,” in Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 3578–3587, 2018.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] B. Singh 和 L. S. Davis，“对象检测剪切中的尺度不变性分析”，在IEEE计算机视觉与模式识别会议论文集中，pp. 3578–3587，2018年。'
- en: '[49] L. Tychsen-Smith and L. Petersson, “Improving object localization with
    fitness nms and bounded iou loss,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 6877–6885, 2018.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] L. Tychsen-Smith 和 L. Petersson，“通过适应性NMS和有界IOU损失改善对象定位”，在IEEE计算机视觉与模式识别会议论文集中，pp.
    6877–6885，2018年。'
- en: '[50] T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, and Y. Chen, “Ron: Reverse connection
    with objectness prior networks for object detection,” in Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 5936–5944, 2017.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, 和 Y. Chen，“Ron: 具有对象性先验网络的反向连接用于对象检测”，在IEEE计算机视觉与模式识别会议论文集中，pp.
    5936–5944，2017年。'
- en: '[51] H. Law and J. Deng, “Cornernet: Detecting objects as paired keypoints,”
    in Proceedings of the European Conference on Computer Vision (ECCV), pp. 734–750,
    2018.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] H. Law 和 J. Deng，“Cornernet: 将对象检测为配对关键点”，在欧洲计算机视觉会议（ECCV）论文集中，pp. 734–750，2018年。'
- en: '[52] M. Braun, S. Krebs, F. Flohr, and D. Gavrila, “Eurocity persons: A novel
    benchmark for person detection in traffic scenes,” IEEE Transactions on Pattern
    Analysis and Machine Intelligence, pp. 1–1, 2019.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. Braun, S. Krebs, F. Flohr, 和 D. Gavrila，“Eurocity persons: 交通场景中人员检测的新基准”，IEEE模式分析与机器智能汇刊，pp.
    1–1，2019年。'
- en: '[53] S. Zhang, R. Benenson, and B. Schiele, “Citypersons: A diverse dataset
    for pedestrian detection,” in Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 3213–3221, 2017.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] S. Zhang, R. Benenson, 和 B. Schiele，“Citypersons: 一个多样化的行人检测数据集”，在IEEE计算机视觉与模式识别会议论文集中，pp.
    3213–3221，2017年。'
- en: '[54] X. Li, F. Flohr, Y. Yang, H. Xiong, M. Braun, S. Pan, K. Li, and D. M.
    Gavrila, “A new benchmark for vision-based cyclist detection,” in 2016 IEEE Intelligent
    Vehicles Symposium (IV), pp. 1028–1033, IEEE, 2016.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] X. Li, F. Flohr, Y. Yang, H. Xiong, M. Braun, S. Pan, K. Li, 和 D. M. Gavrila，“基于视觉的骑行者检测的新基准”，在2016年IEEE智能车辆研讨会（IV）上，pp.
    1028–1033，IEEE，2016年。'
- en: '[55] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,”
    in international Conference on computer vision & Pattern Recognition (CVPR’05),
    vol. 1, pp. 886–893, IEEE Computer Society, 2005.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] N. Dalal 和 B. Triggs，“用于人体检测的方向梯度直方图”，在国际计算机视觉与模式识别会议（CVPR’05）上，vol. 1,
    pp. 886–893，IEEE计算机学会，2005年。'
- en: '[56] A. Ess, B. Leibe, and L. Van Gool, “Depth and appearance for mobile scene
    analysis,” in 2007 IEEE 11th International Conference on Computer Vision, pp. 1–8,
    IEEE, 2007.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] A. Ess, B. Leibe, 和 L. Van Gool，“用于移动场景分析的深度与外观”，在2007年IEEE第11届国际计算机视觉会议上，pp.
    1–8，IEEE，2007年。'
- en: '[57] C. Wojek, S. Walk, and B. Schiele, “Multi-cue onboard pedestrian detection,”
    in 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 794–801,
    IEEE, 2009.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] C. Wojek, S. Walk, 和 B. Schiele，“多线索车载行人检测”，在2009年IEEE计算机视觉与模式识别会议上，pp.
    794–801，IEEE，2009年。'
- en: '[58] M. Enzweiler and D. M. Gavrila, “Monocular pedestrian detection: Survey
    and experiments,” IEEE transactions on pattern analysis and machine intelligence,
    vol. 31, no. 12, pp. 2179–2195, 2008.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] M. Enzweiler 和 D. M. Gavrila，“单目行人检测：综述与实验”，IEEE模式分析与机器智能汇刊，vol. 31, no.
    12, pp. 2179–2195，2008年。'
- en: '[59] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in Advances in neural
    information processing systems, pp. 2672–2680, 2014.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络”，在神经信息处理系统进展中，pp. 2672–2680，2014年。'
- en: '[60] B. Zoph, E. D. Cubuk, G. Ghiasi, T. Lin, J. Shlens, and Q. V. Le, “Learning
    data augmentation strategies for object detection,” CoRR, vol. abs/1906.11172,
    2019.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] B. Zoph, E. D. Cubuk, G. Ghiasi, T. Lin, J. Shlens 和 Q. V. Le, “学习数据增强策略用于物体检测”，CoRR，卷abs/1906.11172，2019年。'
- en: '[61] Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage
    object detection,” arXiv preprint arXiv:1904.01355, 2019.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Z. Tian, C. Shen, H. Chen 和 T. He, “Fcos: 完全卷积单阶段物体检测”，arXiv预印本 arXiv:1904.01355，2019年。'
- en: '[62] T. Kong, F. Sun, H. Liu, Y. Jiang, and J. Shi, “Foveabox: Beyond anchor-based
    object detector,” arXiv preprint arXiv:1904.03797, 2019.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] T. Kong, F. Sun, H. Liu, Y. Jiang 和 J. Shi, “Foveabox: 超越基于锚点的物体检测器”，arXiv预印本
    arXiv:1904.03797，2019年。'
- en: '[63] J. Pang, K. Chen, J. Shi, H. Feng, W. Ouyang, and D. Lin, “Libra r-cnn:
    Towards balanced learning for object detection,” arXiv preprint arXiv:1904.02701,
    2019.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] J. Pang, K. Chen, J. Shi, H. Feng, W. Ouyang 和 D. Lin, “Libra r-cnn: 迈向平衡学习的物体检测”，arXiv预印本
    arXiv:1904.02701，2019年。'
- en: '[64] S.-W. Kim, H.-K. Kook, J.-Y. Sun, M.-C. Kang, and S.-J. Ko, “Parallel
    feature pyramid network for object detection,” in Proceedings of the European
    Conference on Computer Vision (ECCV), pp. 234–250, 2018.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] S.-W. Kim, H.-K. Kook, J.-Y. Sun, M.-C. Kang 和 S.-J. Ko, “用于物体检测的并行特征金字塔网络”，发表于《欧洲计算机视觉会议论文集》（ECCV），第234–250页，2018年。'
- en: '[65] Z. Li and F. Zhou, “Fssd: feature fusion single shot multibox detector,”
    arXiv preprint arXiv:1712.00960, 2017.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Z. Li 和 F. Zhou, “Fssd: 特征融合单次多框检测器”，arXiv预印本 arXiv:1712.00960，2017年。'
- en: '[66] Y. Chen, J. Li, B. Zhou, J. Feng, and S. Yan, “Weaving multi-scale context
    for single shot detector,” arXiv preprint arXiv:1712.03149, 2017.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Y. Chen, J. Li, B. Zhou, J. Feng 和 S. Yan, “编织多尺度上下文用于单次检测器”，arXiv预印本
    arXiv:1712.03149，2017年。'
- en: '[67] L. Zheng, C. Fu, and Y. Zhao, “Extend the shallow part of single shot
    multibox detector via convolutional neural network,” in Tenth International Conference
    on Digital Image Processing (ICDIP 2018), vol. 10806, p. 1080613, International
    Society for Optics and Photonics, 2018.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] L. Zheng, C. Fu 和 Y. Zhao, “通过卷积神经网络扩展单次多框检测器的浅层部分”，发表于第十届数字图像处理国际会议（ICDIP
    2018），卷10806，第1080613页，国际光学与光子学学会，2018年。'
- en: '[68] S.-H. Bae, “Object detection based on region decomposition and assembly,”
    arXiv preprint arXiv:1901.08225, 2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] S.-H. Bae, “基于区域分解与组合的物体检测”，arXiv预印本 arXiv:1901.08225，2019年。'
- en: '[69] E. Barnea and O. Ben-Shahar, “On the utility of context (or the lack thereof)
    for object detection,” CoRR, vol. abs/1711.05471, 2017.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] E. Barnea 和 O. Ben-Shahar, “关于上下文（或其缺乏）对物体检测的效用”，CoRR，卷abs/1711.05471，2017年。'
- en: '[70] Y. Liu, R. Wang, S. Shan, and X. Chen, “Structure inference net: Object
    detection using scene-level context and instance-level relationships,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6985–6994,
    2018.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Y. Liu, R. Wang, S. Shan 和 X. Chen, “结构推断网络：使用场景级上下文和实例级关系进行物体检测”，发表于《IEEE计算机视觉与模式识别会议论文集》，第6985–6994页，2018年。'
- en: '[71] B. Singh, M. Najibi, and L. S. Davis, “Sniper: Efficient multi-scale training,”
    in Advances in Neural Information Processing Systems, pp. 9310–9320, 2018.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] B. Singh, M. Najibi 和 L. S. Davis, “Sniper: 高效的多尺度训练”，发表于《神经信息处理系统进展》，第9310–9320页，2018年。'
- en: '[72] K. Liang, H. Chang, B. Ma, S. Shan, and X. Chen, “Unifying visual attribute
    learning with object recognition in a multiplicative framework,” IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 41, pp. 1747–1760, July 2019.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] K. Liang, H. Chang, B. Ma, S. Shan 和 X. Chen, “在乘法框架中统一视觉属性学习与物体识别”，《IEEE模式分析与机器智能汇刊》，卷41，第1747–1760页，2019年7月。'
- en: '[73] C. Zhang and J. Kim, “Object detection with location-aware deformable
    convolution and backward attention filtering,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 9452–9461, 2019.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] C. Zhang 和 J. Kim, “具有位置感知的可变形卷积和反向注意力过滤的物体检测”，发表于《IEEE计算机视觉与模式识别会议论文集》，第9452–9461页，2019年。'
- en: '[74] D. Yoo, S. Park, J.-Y. Lee, A. S. Paek, and I. So Kweon, “Attentionnet:
    Aggregating weak directions for accurate object detection,” in Proceedings of
    the IEEE International Conference on Computer Vision, pp. 2659–2667, 2015.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] D. Yoo, S. Park, J.-Y. Lee, A. S. Paek 和 I. So Kweon, “Attentionnet: 聚合弱方向以实现准确的物体检测”，发表于《IEEE计算机视觉国际会议论文集》，第2659–2667页，2015年。'
- en: '[75] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel,
    and Y. Bengio, “Show, attend and tell: Neural image caption generation with visual
    attention,” arXiv preprint arXiv:1502.03044, 2015.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel
    和 Y. Bengio, “展示、关注与讲述：带有视觉注意力的神经图像标题生成”，arXiv预印本 arXiv:1502.03044，2015年。'
- en: '[76] J. Ba, V. Mnih, and K. Kavukcuoglu, “Multiple object recognition with
    visual attention,” arXiv preprint arXiv:1412.7755, 2014.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] J. Ba, V. Mnih, 和 K. Kavukcuoglu，“具有视觉注意力的多对象识别”，arXiv 预印本 arXiv:1412.7755，2014
    年。'
- en: '[77] L. Li, M. Xu, X. Wang, L. Jiang, and H. Liu, “Attention based glaucoma
    detection: A large-scale database and cnn model,” arXiv preprint arXiv:1903.10831,
    2019.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] L. Li, M. Xu, X. Wang, L. Jiang, 和 H. Liu，“基于注意力的青光眼检测：大规模数据库和 CNN 模型”，arXiv
    预印本 arXiv:1903.10831，2019 年。'
- en: '[78] K. Hara, M.-Y. Liu, O. Tuzel, and A.-m. Farahmand, “Attentional network
    for visual object detection,” arXiv preprint arXiv:1702.01478, 2017.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] K. Hara, M.-Y. Liu, O. Tuzel, 和 A.-m. Farahmand，“用于视觉对象检测的注意力网络”，arXiv
    预印本 arXiv:1702.01478，2017 年。'
- en: '[79] S. Chaudhari, G. Polatkan, R. Ramanath, and V. Mithal, “An attentive survey
    of attention models,” CoRR, vol. abs/1904.02874, 2019.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] S. Chaudhari, G. Polatkan, R. Ramanath, 和 V. Mithal，“注意力模型的详细调查”，CoRR，卷
    abs/1904.02874，2019 年。'
- en: '[80] T. Kong, F. Sun, C. Tan, H. Liu, and W. Huang, “Deep feature pyramid reconfiguration
    for object detection,” in Proceedings of the European Conference on Computer Vision
    (ECCV), pp. 169–185, 2018.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] T. Kong, F. Sun, C. Tan, H. Liu, 和 W. Huang，“用于对象检测的深度特征金字塔重配置”，欧洲计算机视觉会议（ECCV）论文集，第
    169–185 页，2018 年。'
- en: '[81] J. Yu, Y. Jiang, Z. Wang, Z. Cao, and T. Huang, “Unitbox: An advanced
    object detection network,” in Proceedings of the 24th ACM international conference
    on Multimedia, pp. 516–520, ACM, 2016.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] J. Yu, Y. Jiang, Z. Wang, Z. Cao, 和 T. Huang，“Unitbox：一种先进的对象检测网络”，第 24
    届 ACM 国际多媒体会议论文集，第 516–520 页，ACM，2016 年。'
- en: '[82] H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese,
    “Generalized intersection over union: A metric and a loss for bounding box regression,”
    arXiv preprint arXiv:1902.09630, 2019.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, 和 S. Savarese，“广义交并比：边界框回归的度量和损失”，arXiv
    预印本 arXiv:1902.09630，2019 年。'
- en: '[83] Y. He, C. Zhu, J. Wang, M. Savvides, and X. Zhang, “Bounding box regression
    with uncertainty for accurate object detection,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2888–2897, 2019.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. He, C. Zhu, J. Wang, M. Savvides, 和 X. Zhang，“带有不确定性的边界框回归，用于精确的对象检测”，IEEE
    计算机视觉与模式识别会议论文集，第 2888–2897 页，2019 年。'
- en: '[84] Y. He, X. Zhang, M. Savvides, and K. Kitani, “Softer-nms: Rethinking bounding
    box regression for accurate object detection,” arXiv preprint arXiv:1809.08545,
    2018.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Y. He, X. Zhang, M. Savvides, 和 K. Kitani，“Softer-nms：重新思考边界框回归以实现准确的对象检测”，arXiv
    预印本 arXiv:1809.08545，2018 年。'
- en: '[85] C. Cabriel, N. Bourg, P. Jouchet, G. Dupuis, C. Leterrier, A. Baron, M.-A.
    Badet-Denisot, B. Vauzeilles, E. Fort, and S. Lévêque-Fort, “Combining 3d single
    molecule localization strategies for reproducible bioimaging,” Nature Communications,
    vol. 10, no. 1, p. 1980, 2019.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] C. Cabriel, N. Bourg, P. Jouchet, G. Dupuis, C. Leterrier, A. Baron, M.-A.
    Badet-Denisot, B. Vauzeilles, E. Fort, 和 S. Lévêque-Fort，“结合 3D 单分子定位策略用于可重复的生物成像”，Nature
    Communications，卷 10，第 1 期，第 1980 页，2019 年。'
- en: '[86] M. Bucher, S. Herbin, and F. Jurie, “Hard negative mining for metric learning
    based zero-shot classification,” in European Conference on Computer Vision, pp. 524–531,
    Springer, 2016.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] M. Bucher, S. Herbin, 和 F. Jurie，“基于度量学习的困难负例挖掘用于零样本分类”，欧洲计算机视觉会议，第 524–531
    页，Springer，2016 年。'
- en: '[87] H. Yu, Z. Zhang, Z. Qin, H. Wu, D. Li, J. Zhao, and X. Lu, “Loss rank
    mining: A general hard example mining method for real-time detectors,” in 2018
    International Joint Conference on Neural Networks (IJCNN), pp. 1–8, IEEE, 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] H. Yu, Z. Zhang, Z. Qin, H. Wu, D. Li, J. Zhao, 和 X. Lu，“损失排名挖掘：一种通用的困难示例挖掘方法，用于实时检测器”，2018
    年国际联合神经网络会议（IJCNN），第 1–8 页，IEEE，2018 年。'
- en: '[88] K. Chen, J. Li, W. Lin, J. See, J. Wang, L. Duan, Z. Chen, C. He, and
    J. Zou, “Towards accurate one-stage object detection with ap-loss,” arXiv preprint
    arXiv:1904.06373, 2019.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] K. Chen, J. Li, W. Lin, J. See, J. Wang, L. Duan, Z. Chen, C. He, 和 J.
    Zou，“朝向准确的单阶段对象检测与 ap-loss”，arXiv 预印本 arXiv:1904.06373，2019 年。'
- en: '[89] B. Jiang, R. Luo, J. Mao, T. Xiao, and Y. Jiang, “Acquisition of localization
    confidence for accurate object detection,” in Proceedings of the European Conference
    on Computer Vision (ECCV), pp. 784–799, 2018.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] B. Jiang, R. Luo, J. Mao, T. Xiao, 和 Y. Jiang，“获取定位置信心以实现准确的对象检测”，欧洲计算机视觉会议（ECCV）论文集，第
    784–799 页，2018 年。'
- en: '[90] S. Liu, D. Huang, and Y. Wang, “Adaptive nms: Refining pedestrian detection
    in a crowd,” arXiv preprint arXiv:1904.03629, 2019.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] S. Liu, D. Huang, 和 Y. Wang，“自适应 nms：在拥挤人群中细化行人检测”，arXiv 预印本 arXiv:1904.03629，2019
    年。'
- en: '[91] J. Hosang, R. Benenson, and B. Schiele, “A convnet for non-maximum suppression,”
    in German Conference on Pattern Recognition, pp. 192–204, Springer, 2016.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] J. Hosang, R. Benenson, 和 B. Schiele，“用于非最大抑制的卷积网络”，德国模式识别会议，第 192–204
    页，Springer，2016 年。'
- en: '[92] J. Hosang, R. Benenson, and B. Schiele, “Learning non-maximum suppression,”
    in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 4507–4515, 2017.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Hosang, R. Benenson, 和 B. Schiele, “学习非极大值抑制，” 发表在《IEEE计算机视觉与模式识别会议论文集》，第4507–4515页，2017年。'
- en: '[93] J. Jeong, H. Park, and N. Kwak, “Enhancement of ssd by concatenating feature
    maps for object detection,” arXiv preprint arXiv:1705.09587, 2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Jeong, H. Park, 和 N. Kwak, “通过连接特征图增强SSD以进行目标检测，” arXiv预印本 arXiv:1705.09587，2017年。'
- en: '[94] W. Xiang, D.-Q. Zhang, H. Yu, and V. Athitsos, “Context-aware single-shot
    detector,” in 2018 IEEE Winter Conference on Applications of Computer Vision (WACV),
    pp. 1784–1793, IEEE, 2018.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] W. Xiang, D.-Q. Zhang, H. Yu, 和 V. Athitsos, “上下文感知的单次检测器，” 发表在2018年IEEE计算机视觉应用冬季会议（WACV），第1784–1793页，IEEE，2018年。'
- en: '[95] G. Cao, X. Xie, W. Yang, Q. Liao, G. Shi, and J. Wu, “Feature-fused ssd:
    fast detection for small objects,” in Ninth International Conference on Graphic
    and Image Processing (ICGIP 2017), vol. 10615, p. 106151E, International Society
    for Optics and Photonics, 2018.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] G. Cao, X. Xie, W. Yang, Q. Liao, G. Shi, 和 J. Wu, “特征融合SSD：小物体的快速检测，”
    发表在第九届国际图形与图像处理会议（ICGIP 2017），第10615卷，第106151E页，国际光学与光子学学会，2018年。'
- en: '[96] J. Li, X. Liang, Y. Wei, T. Xu, J. Feng, and S. Yan, “Perceptual generative
    adversarial networks for small object detection,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 1222–1230, 2017.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] J. Li, X. Liang, Y. Wei, T. Xu, J. Feng, 和 S. Yan, “用于小物体检测的感知生成对抗网络，”
    发表在《IEEE计算机视觉与模式识别会议论文集》，第1222–1230页，2017年。'
- en: '[97] W. Liu, S. Liao, W. Hu, X. Liang, and X. Chen, “Learning efficient single-stage
    pedestrian detectors by asymptotic localization fitting,” in Proceedings of the
    European Conference on Computer Vision (ECCV), pp. 618–634, 2018.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] W. Liu, S. Liao, W. Hu, X. Liang, 和 X. Chen, “通过渐近定位拟合学习高效的单阶段行人检测器，”
    发表在《欧洲计算机视觉会议（ECCV）会议论文集》，第618–634页，2018年。'
- en: '[98] P. Hu and D. Ramanan, “Finding tiny faces,” in Proceedings of the IEEE
    conference on computer vision and pattern recognition, pp. 951–959, 2017.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] P. Hu 和 D. Ramanan, “寻找微小面部，” 发表在《IEEE计算机视觉与模式识别会议论文集》，第951–959页，2017年。'
- en: '[99] M. Xu, L. Cui, P. Lv, X. Jiang, J. Niu, B. Zhou, and M. Wang, “Mdssd:
    Multi-scale deconvolutional single shot detector for small objects,” arXiv preprint
    arXiv:1805.07009, 2018.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] M. Xu, L. Cui, P. Lv, X. Jiang, J. Niu, B. Zhou, 和 M. Wang, “Mdssd：用于小物体的多尺度反卷积单次检测器，”
    arXiv预印本 arXiv:1805.07009，2018年。'
- en: '[100] J. Wang, Y. Yuan, and G. Yu, “Face attention network: an effective face
    detector for the occluded faces,” arXiv preprint arXiv:1711.07246, 2017.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] J. Wang, Y. Yuan, 和 G. Yu, “面部注意力网络：有效的遮挡面部检测器，” arXiv预印本 arXiv:1711.07246，2017年。'
- en: '[101] X. Wang, T. Xiao, Y. Jiang, S. Shao, J. Sun, and C. Shen, “Repulsion
    loss: Detecting pedestrians in a crowd,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 7774–7783, 2018.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] X. Wang, T. Xiao, Y. Jiang, S. Shao, J. Sun, 和 C. Shen, “排斥损失：在人群中检测行人，”
    发表在《IEEE计算机视觉与模式识别会议论文集》，第7774–7783页，2018年。'
- en: '[102] S. Zhang, L. Wen, X. Bian, Z. Lei, and S. Z. Li, “Occlusion-aware r-cnn:
    detecting pedestrians in a crowd,” in Proceedings of the European Conference on
    Computer Vision (ECCV), pp. 637–653, 2018.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] S. Zhang, L. Wen, X. Bian, Z. Lei, 和 S. Z. Li, “遮挡感知r-cnn：在人群中检测行人，”
    发表在《欧洲计算机视觉会议（ECCV）会议论文集》，第637–653页，2018年。'
- en: '[103] P. Baqué, F. Fleuret, and P. Fua, “Deep occlusion reasoning for multi-camera
    multi-target detection,” in Proceedings of the IEEE International Conference on
    Computer Vision, pp. 271–279, 2017.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] P. Baqué, F. Fleuret, 和 P. Fua, “多摄像头多目标检测的深度遮挡推理，” 发表在《IEEE国际计算机视觉会议论文集》，第271–279页，2017年。'
- en: '[104] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep
    convolutional networks for visual recognition,” IEEE transactions on pattern analysis
    and machine intelligence, vol. 37, no. 9, pp. 1904–1916, 2015.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] K. He, X. Zhang, S. Ren, 和 J. Sun, “深度卷积网络中的空间金字塔池化用于视觉识别，” 《IEEE模式分析与机器智能事务》，第37卷，第9期，第1904–1916页，2015年。'
- en: '[105] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun,
    “Overfeat: Integrated recognition, localization and detection using convolutional
    networks,” arXiv preprint arXiv:1312.6229, 2013.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, 和 Y. LeCun, “Overfeat：使用卷积网络进行集成识别、定位和检测，”
    arXiv预印本 arXiv:1312.6229，2013年。'
- en: '[106] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, “Object
    detection with discriminatively trained part-based models,” IEEE transactions
    on pattern analysis and machine intelligence, vol. 32, no. 9, pp. 1627–1645, 2009.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, 和 D. Ramanan, “使用判别训练的基于部分的模型进行目标检测，”
    IEEE 模式分析与机器智能汇刊，vol. 32, no. 9, 第 1627–1645 页，2009。'
- en: '[107] H. Law, Y. Teng, O. Russakovsky, and J. Deng, “Cornernet-lite: Efficient
    keypoint based object detection,” arXiv preprint arXiv:1904.08900, 2019.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] H. Law, Y. Teng, O. Russakovsky, 和 J. Deng, “Cornernet-lite: 高效的基于关键点的目标检测，”
    arXiv 预印本 arXiv:1904.08900, 2019。'
- en: '[108] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, “Centernet: Keypoint
    triplets for object detection,” arXiv preprint arXiv:1904.08189, 2019.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, 和 Q. Tian, “Centernet: 关键点三元组用于目标检测，”
    arXiv 预印本 arXiv:1904.08189, 2019。'
- en: '[109] J. Wang, K. Chen, S. Yang, C. C. Loy, and D. Lin, “Region proposal by
    guided anchoring,” arXiv preprint arXiv:1901.03278, 2019.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] J. Wang, K. Chen, S. Yang, C. C. Loy, 和 D. Lin, “通过引导锚点进行区域提议，” arXiv
    预印本 arXiv:1901.03278, 2019。'
- en: '[110] X. Zhou, J. Zhuo, and P. Krähenbühl, “Bottom-up object detection by grouping
    extreme and center points,” arXiv preprint arXiv:1901.08043, 2019.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] X. Zhou, J. Zhuo, 和 P. Krähenbühl, “通过分组极值和中心点进行自下而上的目标检测，” arXiv 预印本
    arXiv:1901.08043, 2019。'
- en: '[111] X. Zhou, D. Wang, and P. Krähenbühl, “Objects as points,” CoRR, vol. abs/1904.07850,
    2019.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] X. Zhou, D. Wang, 和 P. Krähenbühl, “将物体视为点，” CoRR, vol. abs/1904.07850,
    2019。'
- en: '[112] S. Chen, J. Li, C. Yao, W. Hou, S. Qin, W. Jin, and X. Tang, “Dubox:
    No-prior box objection detection via residual dual scale detectors,” arXiv preprint
    arXiv:1904.06883, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] S. Chen, J. Li, C. Yao, W. Hou, S. Qin, W. Jin, 和 X. Tang, “Dubox: 无先验框目标检测通过残差双尺度检测器，”
    arXiv 预印本 arXiv:1904.06883, 2019。'
- en: '[113] C. Zhu, Y. He, and M. Savvides, “Feature selective anchor-free module
    for single-shot object detection,” arXiv preprint arXiv:1903.00621, 2019.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] C. Zhu, Y. He, 和 M. Savvides, “用于单次检测的特征选择锚点无模块，” arXiv 预印本 arXiv:1903.00621,
    2019。'
- en: '[114] R. Zhu, S. Zhang, X. Wang, L. Wen, H. Shi, L. Bo, and T. Mei, “Scratchdet:
    Training single-shot object detectors from scratch,” in Proceedings of the IEEE
    conference on computer vision and pattern recognition, 2019.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] R. Zhu, S. Zhang, X. Wang, L. Wen, H. Shi, L. Bo, 和 T. Mei, “Scratchdet:
    从头开始训练单次检测目标检测器，” 在 IEEE 计算机视觉与模式识别会议论文集中，2019。'
- en: '[115] Z. Shen, Z. Liu, J. Li, Y.-G. Jiang, Y. Chen, and X. Xue, “Dsod: Learning
    deeply supervised object detectors from scratch,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 1919–1927, 2017.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Z. Shen, Z. Liu, J. Li, Y.-G. Jiang, Y. Chen, 和 X. Xue, “Dsod: 从头开始学习深度监督的目标检测器，”
    在 IEEE 国际计算机视觉会议论文集中，第 1919–1927 页，2017。'
- en: '[116] Z. Shen, H. Shi, R. Feris, L. Cao, S. Yan, D. Liu, X. Wang, X. Xue, and
    T. S. Huang, “Learning object detectors from scratch with gated recurrent feature
    pyramids,” arXiv preprint arXiv:1712.00886, 2017.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Z. Shen, H. Shi, R. Feris, L. Cao, S. Yan, D. Liu, X. Wang, X. Xue, 和
    T. S. Huang, “通过门控递归特征金字塔从头开始学习目标检测器，” arXiv 预印本 arXiv:1712.00886, 2017。'
- en: '[117] Y. Li, J. Li, W. Lin, and J. Li, “Tiny-dsod: Lightweight object detection
    for resource-restricted usages,” arXiv preprint arXiv:1807.11013, 2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Y. Li, J. Li, W. Lin, 和 J. Li, “Tiny-dsod: 适用于资源受限用途的轻量级目标检测，” arXiv
    预印本 arXiv:1807.11013, 2018。'
- en: '[118] Z. Shen, Z. Liu, J. Li, Y.-G. Jiang, Y. Chen, and X. Xue, “Object detection
    from scratch with deep supervision,” arXiv preprint arXiv:1809.09294, 2018.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Z. Shen, Z. Liu, J. Li, Y.-G. Jiang, Y. Chen, 和 X. Xue, “通过深度监督从头开始的目标检测，”
    arXiv 预印本 arXiv:1809.09294, 2018。'
- en: '[119] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun, “Light-head r-cnn:
    In defense of two-stage object detector,” arXiv preprint arXiv:1711.07264, 2017.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, 和 J. Sun, “Light-head r-cnn:
    为两阶段目标检测器辩护，” arXiv 预印本 arXiv:1711.07264, 2017。'
- en: '[120] A. Womg, M. J. Shafiee, F. Li, and B. Chwyl, “Tiny ssd: A tiny single-shot
    detection deep convolutional neural network for real-time embedded object detection,”
    in 2018 15th Conference on Computer and Robot Vision (CRV), pp. 95–101, IEEE,
    2018.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] A. Womg, M. J. Shafiee, F. Li, 和 B. Chwyl, “Tiny ssd: 一种用于实时嵌入式目标检测的小型单次检测深度卷积神经网络，”
    在 2018 第十五届计算机与机器人视觉会议（CRV），第 95–101 页，IEEE，2018。'
- en: '[121] L. Tychsen-Smith and L. Petersson, “Denet: Scalable real-time object
    detection with directed sparse sampling,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 428–436, 2017.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] L. Tychsen-Smith 和 L. Petersson, “Denet: 具有定向稀疏采样的可扩展实时目标检测，” 在 IEEE
    国际计算机视觉会议论文集中，第 428–436 页，2017。'
- en: '[122] S. Tripathi, G. Dane, B. Kang, V. Bhaskaran, and T. Nguyen, “Lcdet: Low-complexity
    fully-convolutional neural networks for object detection in embedded systems,”
    in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    Workshops, pp. 94–103, 2017.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] S. Tripathi, G. Dane, B. Kang, V. Bhaskaran 和 T. Nguyen， “Lcdet: 用于嵌入式系统的低复杂度全卷积神经网络进行物体检测，”
    见于 《IEEE计算机视觉与模式识别会议》研讨会论文集，第94–103页，2017年。'
- en: '[123] Y. Lee, H. Kim, E. Park, X. Cui, and H. Kim, “Wide-residual-inception
    networks for real-time object detection,” in 2017 IEEE Intelligent Vehicles Symposium
    (IV), pp. 758–764, IEEE, 2017.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Y. Lee, H. Kim, E. Park, X. Cui 和 H. Kim， “宽残差 inception 网络用于实时物体检测，”
    见于 2017 IEEE智能车辆研讨会（IV），第758–764页，IEEE，2017年。'
- en: '[124] Q. Li, S. Jin, and J. Yan, “Mimicking very efficient network for object
    detection,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 6356–6364, 2017.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Q. Li, S. Jin 和 J. Yan， “模拟非常高效的网络用于物体检测，” 见于 《IEEE计算机视觉与模式识别会议论文集》，第6356–6364页，2017年。'
- en: '[125] H.-Y. Zhou, B.-B. Gao, and J. Wu, “Adaptive feeding: Achieving fast and
    accurate detections by adaptively combining object detectors,” in Proceedings
    of the IEEE International Conference on Computer Vision, pp. 3505–3513, 2017.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] H.-Y. Zhou, B.-B. Gao 和 J. Wu， “自适应喂养: 通过自适应结合物体检测器实现快速而准确的检测，” 见于 《IEEE国际计算机视觉会议》论文集，第3505–3513页，2017年。'
- en: '[126] S. Liu, D. Huang, et al., “Receptive field block net for accurate and
    fast object detection,” in Proceedings of the European Conference on Computer
    Vision (ECCV), pp. 385–400, 2018.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] S. Liu, D. Huang 等， “感受野块网络用于准确而快速的物体检测，” 见于 欧洲计算机视觉会议（ECCV）论文集，第385–400页，2018年。'
- en: '[127] R. Ranjan, V. M. Patel, and R. Chellappa, “Hyperface: A deep multi-task
    learning framework for face detection, landmark localization, pose estimation,
    and gender recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
    vol. 41, pp. 121–135, Jan 2019.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] R. Ranjan, V. M. Patel 和 R. Chellappa， “Hyperface: 用于人脸检测、标志点定位、姿态估计和性别识别的深度多任务学习框架，”
    《IEEE模式分析与机器智能学报》，第41卷，第121–135页，2019年1月。'
- en: '[128] R. He, X. Wu, Z. Sun, and T. Tan, “Wasserstein cnn: Learning invariant
    features for nir-vis face recognition,” IEEE Transactions on Pattern Analysis
    and Machine Intelligence, vol. 41, pp. 1761–1773, July 2019.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] R. He, X. Wu, Z. Sun 和 T. Tan， “Wasserstein cnn: 学习不变特征以进行近红外-可见光人脸识别，”
    《IEEE模式分析与机器智能学报》，第41卷，第1761–1773页，2019年7月。'
- en: '[129] X. Zhang, R. Zhao, Y. Qiao, X. Wang, and H. Li, “Adacos: Adaptively scaling
    cosine logits for effectively learning deep face representations,” arXiv preprint
    arXiv:1905.00292, 2019.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] X. Zhang, R. Zhao, Y. Qiao, X. Wang 和 H. Li， “Adacos: 自适应缩放余弦 logits
    有效学习深度人脸表示，” arXiv 预印本 arXiv:1905.00292，2019年。'
- en: '[130] Y. Liu, H. Li, and X. Wang, “Rethinking feature discrimination and polymerization
    for large-scale recognition,” arXiv preprint arXiv:1710.00870, 2017.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Y. Liu, H. Li 和 X. Wang， “重新思考大规模识别中的特征区分和聚合，” arXiv 预印本 arXiv:1710.00870，2017年。'
- en: '[131] R. Ranjan, C. D. Castillo, and R. Chellappa, “L2-constrained softmax
    loss for discriminative face verification,” arXiv preprint arXiv:1703.09507, 2017.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] R. Ranjan, C. D. Castillo 和 R. Chellappa， “L2约束软最大损失用于判别性人脸验证，” arXiv
    预印本 arXiv:1703.09507，2017年。'
- en: '[132] F. Wang, X. Xiang, J. Cheng, and A. L. Yuille, “Normface: l 2 hypersphere
    embedding for face verification,” in Proceedings of the 25th ACM international
    conference on Multimedia, pp. 1041–1049, ACM, 2017.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] F. Wang, X. Xiang, J. Cheng 和 A. L. Yuille， “Normface: 用于人脸验证的 l 2 超球面嵌入，”
    见于 第25届ACM国际多媒体会议论文集，第1041–1049页，ACM，2017年。'
- en: '[133] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular
    margin loss for deep face recognition,” arXiv preprint arXiv:1801.07698, 2018.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] J. Deng, J. Guo, N. Xue 和 S. Zafeiriou， “Arcface: 用于深度人脸识别的加性角度边距损失，”
    arXiv 预印本 arXiv:1801.07698，2018年。'
- en: '[134] Y. Guo, L. Jiao, S. Wang, S. Wang, and F. Liu, “Fuzzy sparse autoencoder
    framework for single image per person face recognition,” IEEE transactions on
    cybernetics, vol. 48, no. 8, pp. 2402–2415, 2017.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Y. Guo, L. Jiao, S. Wang, S. Wang 和 F. Liu， “用于单张人脸图像识别的模糊稀疏自编码框架，” 《IEEE控制论学报》，第48卷，第8期，第2402–2415页，2017年。'
- en: '[135] M. Wang and W. Deng, “Deep face recognition: A survey,” arXiv preprint
    arXiv:1804.06655, 2018.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] M. Wang 和 W. Deng， “深度人脸识别: 一项综述，” arXiv 预印本 arXiv:1804.06655，2018年。'
- en: '[136] Z. Cai, M. J. Saberian, and N. Vasconcelos, “Learning complexity-aware
    cascades for pedestrian detection,” IEEE Transactions on Pattern Analysis and
    Machine Intelligence, pp. 1–1, 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Z. Cai, M. J. Saberian 和 N. Vasconcelos， “学习复杂度感知级联用于行人检测，” 《IEEE模式分析与机器智能学报》，第1–1页，2019年。'
- en: '[137] M. J. Saberian and N. Vasconcelos, “Learning optimal embedded cascades,”
    IEEE transactions on pattern analysis and machine intelligence, vol. 34, no. 10,
    pp. 2005–2018, 2012.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] M. J. Saberian 和 N. Vasconcelos, “学习最优嵌入级联，” IEEE transactions on pattern
    analysis and machine intelligence, vol. 34, no. 10, pp. 2005–2018, 2012。'
- en: '[138] P. Dollár, R. Appel, S. Belongie, and P. Perona, “Fast feature pyramids
    for object detection,” IEEE transactions on pattern analysis and machine intelligence,
    vol. 36, no. 8, pp. 1532–1545, 2014.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] P. Dollár, R. Appel, S. Belongie 和 P. Perona, “用于目标检测的快速特征金字塔，” IEEE
    transactions on pattern analysis and machine intelligence, vol. 36, no. 8, pp.
    1532–1545, 2014。'
- en: '[139] A. Brunetti, D. Buongiorno, G. F. Trotta, and V. Bevilacqua, “Computer
    vision and deep learning techniques for pedestrian detection and tracking: A survey,”
    Neurocomputing, vol. 300, pp. 17–33, 2018.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] A. Brunetti, D. Buongiorno, G. F. Trotta 和 V. Bevilacqua, “用于行人检测和跟踪的计算机视觉和深度学习技术：综述，”
    Neurocomputing, vol. 300, pp. 17–33, 2018。'
- en: '[140] S. Liu, M. Yamada, N. Collier, and M. Sugiyama, “Change-point detection
    in time-series data by relative density-ratio estimation,” Neural Networks, vol. 43,
    pp. 72–83, 2013.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] S. Liu, M. Yamada, N. Collier 和 M. Sugiyama, “通过相对密度比估计进行时间序列数据的变化点检测，”
    Neural Networks, vol. 43, pp. 72–83, 2013。'
- en: '[141] P. Senin, J. Lin, X. Wang, T. Oates, S. Gandhi, A. P. Boedihardjo, C. Chen,
    and S. Frankenstein, “Grammarviz 3.0: Interactive discovery of variable-length
    time series patterns,” ACM Transactions on Knowledge Discovery from Data (TKDD),
    vol. 12, no. 1, p. 10, 2018.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] P. Senin, J. Lin, X. Wang, T. Oates, S. Gandhi, A. P. Boedihardjo, C.
    Chen 和 S. Frankenstein, “Grammarviz 3.0: 可变长度时间序列模式的交互式发现，” ACM Transactions on
    Knowledge Discovery from Data (TKDD), vol. 12, no. 1, p. 10, 2018。'
- en: '[142] M. Jiang, A. Beutel, P. Cui, B. Hooi, S. Yang, and C. Faloutsos, “A general
    suspiciousness metric for dense blocks in multimodal data,” in 2015 IEEE International
    Conference on Data Mining, pp. 781–786, IEEE, 2015.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] M. Jiang, A. Beutel, P. Cui, B. Hooi, S. Yang 和 C. Faloutsos, “多模态数据中密集块的一般可疑性度量，”
    在 2015 IEEE International Conference on Data Mining, pp. 781–786, IEEE, 2015。'
- en: '[143] E. Wu, W. Liu, and S. Chawla, “Spatio-temporal outlier detection in precipitation
    data,” in International Workshop on Knowledge Discovery from Sensor Data, pp. 115–133,
    Springer, 2008.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] E. Wu, W. Liu 和 S. Chawla, “降水数据中的时空异常检测，” 在 International Workshop on
    Knowledge Discovery from Sensor Data, pp. 115–133, Springer, 2008。'
- en: '[144] B. Barz, E. Rodner, Y. G. Garcia, and J. Denzler, “Detecting regions
    of maximal divergence for spatio-temporal anomaly detection,” IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 41, pp. 1088–1101, May 2019.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] B. Barz, E. Rodner, Y. G. Garcia 和 J. Denzler, “检测时空异常的最大发散区域，” IEEE
    Transactions on Pattern Analysis and Machine Intelligence, vol. 41, pp. 1088–1101,
    May 2019。'
- en: '[145] G. Cheng, P. Zhou, and J. Han, “Learning rotation-invariant convolutional
    neural networks for object detection in vhr optical remote sensing images,” IEEE
    Transactions on Geoscience and Remote Sensing, vol. 54, no. 12, pp. 7405–7415,
    2016.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] G. Cheng, P. Zhou 和 J. Han, “学习旋转不变的卷积神经网络用于 vhr 光学遥感图像中的目标检测，” IEEE
    Transactions on Geoscience and Remote Sensing, vol. 54, no. 12, pp. 7405–7415,
    2016。'
- en: '[146] Y. Zhang, Y. Yuan, Y. Feng, and X. Lu, “Hierarchical and robust convolutional
    neural network for very high-resolution remote sensing object detection,” IEEE
    Transactions on Geoscience and Remote Sensing, 2019.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Y. Zhang, Y. Yuan, Y. Feng 和 X. Lu, “层次化且鲁棒的卷积神经网络用于超高分辨率遥感目标检测，” IEEE
    Transactions on Geoscience and Remote Sensing, 2019。'
- en: '[147] Q. Li, L. Mou, Q. Xu, Y. Zhang, and X. X. Zhu, “R³-net: A deep network
    for multioriented vehicle detection in aerial images and videos,” IEEE Transactions
    on Geoscience and Remote Sensing, 2019.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Q. Li, L. Mou, Q. Xu, Y. Zhang 和 X. X. Zhu, “R³-net: 一种用于航空图像和视频中多方向车辆检测的深度网络，”
    IEEE Transactions on Geoscience and Remote Sensing, 2019。'
- en: '[148] Z. Deng, H. Sun, S. Zhou, J. Zhao, and H. Zou, “Toward fast and accurate
    vehicle detection in aerial images using coupled region-based convolutional neural
    networks,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote
    Sensing, vol. 10, no. 8, pp. 3652–3664, 2017.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Z. Deng, H. Sun, S. Zhou, J. Zhao 和 H. Zou, “通过耦合区域卷积神经网络实现快速而准确的航空图像车辆检测，”
    IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,
    vol. 10, no. 8, pp. 3652–3664, 2017。'
- en: '[149] N. Audebert, B. Le Saux, and S. Lefèvre, “Segment-before-detect: Vehicle
    detection and classification through semantic segmentation of aerial images,”
    Remote Sensing, vol. 9, no. 4, p. 368, 2017.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] N. Audebert, B. Le Saux 和 S. Lefèvre, “检测前分割：通过语义分割的航空图像进行车辆检测和分类，” Remote
    Sensing, vol. 9, no. 4, p. 368, 2017。'
- en: '[150] Q. Li, L. Mou, Q. Liu, Y. Wang, and X. X. Zhu, “Hsf-net: Multiscale deep
    feature embedding for ship detection in optical remote sensing imagery,” IEEE
    Transactions on Geoscience and Remote Sensing, no. 99, pp. 1–15, 2018.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Q. Li, L. Mou, Q. Liu, Y. Wang, 和 X. X. Zhu, “Hsf-net: 多尺度深度特征嵌入用于光学遥感图像中的船舶检测，”
    IEEE地球科学与遥感汇刊, no. 99, pp. 1–15, 2018。'
- en: '[151] J. Pang, C. Li, J. Shi, Z. Xu, and H. Feng, “R²-cnn: Fast tiny object
    detection in large-scale remote sensing images,” IEEE Transactions on Geoscience
    and Remote Sensing, 2019.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] J. Pang, C. Li, J. Shi, Z. Xu, 和 H. Feng, “R²-cnn: 大规模遥感图像中快速微小目标检测，”
    IEEE地球科学与遥感汇刊, 2019。'
- en: '[152] J. Pei, Y. Huang, W. Huo, Y. Zhang, J. Yang, and T.-S. Yeo, “Sar automatic
    target recognition based on multiview deep learning framework,” IEEE Transactions
    on Geoscience and Remote Sensing, vol. 56, no. 4, pp. 2196–2210, 2017.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] J. Pei, Y. Huang, W. Huo, Y. Zhang, J. Yang, 和 T.-S. Yeo, “基于多视角深度学习框架的SAR自动目标识别，”
    IEEE地球科学与遥感汇刊, vol. 56, no. 4, pp. 2196–2210, 2017。'
- en: '[153] Y. Long, Y. Gong, Z. Xiao, and Q. Liu, “Accurate object localization
    in remote sensing images based on convolutional neural networks,” IEEE Transactions
    on Geoscience and Remote Sensing, vol. 55, no. 5, pp. 2486–2498, 2017.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Y. Long, Y. Gong, Z. Xiao, 和 Q. Liu, “基于卷积神经网络的遥感图像中准确的目标定位，” IEEE地球科学与遥感汇刊,
    vol. 55, no. 5, pp. 2486–2498, 2017。'
- en: '[154] M. Shahzad, M. Maurer, F. Fraundorfer, Y. Wang, and X. X. Zhu, “Buildings
    detection in vhr sar images using fully convolution neural networks,” IEEE transactions
    on geoscience and remote sensing, vol. 57, no. 2, pp. 1100–1116, 2018.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] M. Shahzad, M. Maurer, F. Fraundorfer, Y. Wang, 和 X. X. Zhu, “使用全卷积神经网络进行VHR
    SAR图像中的建筑物检测，” IEEE地球科学与遥感汇刊, vol. 57, no. 2, pp. 1100–1116, 2018。'
- en: '[155] F. Zhang, B. Du, L. Zhang, and M. Xu, “Weakly supervised learning based
    on coupled convolutional neural networks for aircraft detection,” IEEE Transactions
    on Geoscience and Remote Sensing, vol. 54, no. 9, pp. 5553–5563, 2016.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] F. Zhang, B. Du, L. Zhang, 和 M. Xu, “基于耦合卷积神经网络的弱监督学习用于飞机检测，” IEEE地球科学与遥感汇刊,
    vol. 54, no. 9, pp. 5553–5563, 2016。'
- en: '[156] J. Han, D. Zhang, G. Cheng, L. Guo, and J. Ren, “Object detection in
    optical remote sensing images based on weakly supervised learning and high-level
    feature learning,” IEEE Transactions on Geoscience and Remote Sensing, vol. 53,
    no. 6, pp. 3325–3337, 2014.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] J. Han, D. Zhang, G. Cheng, L. Guo, 和 J. Ren, “基于弱监督学习和高级特征学习的光学遥感图像目标检测，”
    IEEE地球科学与遥感汇刊, vol. 53, no. 6, pp. 3325–3337, 2014。'
- en: '[157] Q. Li, Y. Wang, Q. Liu, and W. Wang, “Hough transform guided deep feature
    extraction for dense building detection in remote sensing images,” in 2018 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    pp. 1872–1876, IEEE, 2018.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Q. Li, Y. Wang, Q. Liu, 和 W. Wang, “Hough变换引导的深度特征提取用于遥感图像中密集建筑物检测，”
    在2018年IEEE国际声学、语音与信号处理会议（ICASSP）上，pp. 1872–1876, IEEE, 2018。'
- en: '[158] L. Mou and X. X. Zhu, “Vehicle instance segmentation from aerial image
    and video using a multitask learning residual fully convolutional network,” IEEE
    Transactions on Geoscience and Remote Sensing, no. 99, pp. 1–13, 2018.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] L. Mou 和 X. X. Zhu, “利用多任务学习残差全卷积网络从航空图像和视频中进行车辆实例分割，” IEEE地球科学与遥感汇刊,
    no. 99, pp. 1–13, 2018。'
- en: '[159] X. Chen, S. Xiang, C.-L. Liu, and C.-H. Pan, “Vehicle detection in satellite
    images by hybrid deep convolutional neural networks,” IEEE Geoscience and remote
    sensing letters, vol. 11, no. 10, pp. 1797–1801, 2014.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] X. Chen, S. Xiang, C.-L. Liu, 和 C.-H. Pan, “通过混合深度卷积神经网络在卫星图像中进行车辆检测，”
    IEEE地球科学与遥感快报, vol. 11, no. 10, pp. 1797–1801, 2014。'
- en: '[160] N. Ammour, H. Alhichri, Y. Bazi, B. Benjdira, N. Alajlan, and M. Zuair,
    “Deep learning approach for car detection in uav imagery,” Remote Sensing, vol. 9,
    no. 4, p. 312, 2017.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] N. Ammour, H. Alhichri, Y. Bazi, B. Benjdira, N. Alajlan, 和 M. Zuair,
    “深度学习方法用于无人机图像中的汽车检测，” 遥感, vol. 9, no. 4, p. 312, 2017。'
- en: '[161] S. Wang, M. Wang, S. Yang, and L. Jiao, “New hierarchical saliency filtering
    for fast ship detection in high-resolution sar images,” IEEE Transactions on Geoscience
    and Remote Sensing, vol. 55, no. 1, pp. 351–362, 2016.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] S. Wang, M. Wang, S. Yang, 和 L. Jiao, “新型分层显著性滤波用于高分辨率SAR图像中的快速船舶检测，”
    IEEE地球科学与遥感汇刊, vol. 55, no. 1, pp. 351–362, 2016。'
- en: '[162] W. Ma, Q. Guo, Y. Wu, W. Zhao, X. Zhang, and L. Jiao, “A novel multi-model
    decision fusion network for object detection in remote sensing images,” Remote
    Sensing, vol. 11, no. 7, p. 737, 2019.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] W. Ma, Q. Guo, Y. Wu, W. Zhao, X. Zhang, 和 L. Jiao, “一种新型的多模型决策融合网络用于遥感图像中的目标检测，”
    遥感, vol. 11, no. 7, p. 737, 2019。'
- en: '[163] R. Dong, D. Xu, J. Zhao, L. Jiao, and J. An, “Sig-nms-based faster r-cnn
    combining transfer learning for small target detection in vhr optical remote sensing
    imagery,” IEEE Transactions on Geoscience and Remote Sensing, 2019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] R. Dong, D. Xu, J. Zhao, L. Jiao 和 J. An，“基于SIG-NMS的Faster R-CNN结合迁移学习，用于VHR光学遥感图像中的小目标检测，”《IEEE地球科学与遥感学报》，2019年。'
- en: '[164] C. Chen, C. He, C. Hu, H. Pei, and L. Jiao, “A deep neural network based
    on an attention mechanism for sar ship detection in multiscale and complex scenarios,”
    IEEE Access, 2019.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] C. Chen, C. He, C. Hu, H. Pei 和 L. Jiao，“一种基于注意力机制的深度神经网络，用于多尺度和复杂场景下的SAR船舶检测，”《IEEE
    Access》，2019年。'
- en: '[165] H. Zhu, P. Zhang, L. Wang, X. Zhang, and L. Jiao, “A multiscale object
    detection approach for remote sensing images based on mse-densenet and the dynamic
    anchor assignment,” Remote Sensing Letters, vol. 10, no. 10, pp. 959–967, 2019.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] H. Zhu, P. Zhang, L. Wang, X. Zhang 和 L. Jiao，“一种基于MSE-Densenet和动态锚点分配的多尺度对象检测方法，”《遥感快报》，第10卷，第10期，第959–967页，2019年。'
- en: '[166] G. Cheng, J. Han, P. Zhou, and L. Guo, “Multi-class geospatial object
    detection and geographic image classification based on collection of part detectors,”
    ISPRS Journal of Photogrammetry and Remote Sensing, vol. 98, pp. 119–132, 2014.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] G. Cheng, J. Han, P. Zhou 和 L. Guo，“基于部分检测器集合的多类地理空间对象检测和地理图像分类，”《ISPRS摄影测量与遥感学报》，第98卷，第119–132页，2014年。'
- en: '[167] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo,
    and L. Zhang, “Dota: A large-scale dataset for object detection in aerial images,”
    in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 3974–3983, 2018.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo
    和 L. Zhang，“DOTA：一个用于航空图像中对象检测的大规模数据集，”发表于IEEE计算机视觉与模式识别会议论文集，第3974–3983页，2018年。'
- en: '[168] K. Liu and G. Mattyus, “Fast multiclass vehicle detection on aerial images,”
    IEEE Geoscience and Remote Sensing Letters, vol. 12, no. 9, pp. 1938–1942, 2015.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] K. Liu 和 G. Mattyus，“快速多类车辆检测在航空图像中，”《IEEE地球科学与遥感通讯》，第12卷，第9期，第1938–1942页，2015年。'
- en: '[169] S. Razakarivony and F. Jurie, “Vehicle detection in aerial imagery: A
    small target detection benchmark,” Journal of Visual Communication and Image Representation,
    vol. 34, pp. 187–203, 2016.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] S. Razakarivony 和 F. Jurie，“航空图像中的车辆检测：一个小目标检测基准，”《视觉通信与图像表示学报》，第34卷，第187–203页，2016年。'
- en: '[170] G. Cheng and J. Han, “A survey on object detection in optical remote
    sensing images,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 117,
    pp. 11–28, 2016.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] G. Cheng 和 J. Han，“关于光学遥感图像中对象检测的调查，”《ISPRS摄影测量与遥感学报》，第117卷，第11–28页，2016年。'
- en: '[171] P. Shivakumara, D. Tang, M. Asadzadehkaljahi, T. Lu, U. Pal, and M. H.
    Anisi, “Cnn-rnn based method for license plate recognition,” CAAI Transactions
    on Intelligence Technology, vol. 3, no. 3, pp. 169–175, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] P. Shivakumara, D. Tang, M. Asadzadehkaljahi, T. Lu, U. Pal 和 M. H. Anisi，“基于Cnn-rnn的车牌识别方法，”《CAAI智能技术学报》，第3卷，第3期，第169–175页，2018年。'
- en: '[172] M. Sarfraz and M. J. Ahmed, “An approach to license plate recognition
    system using neural network,” in Exploring Critical Approaches of Evolutionary
    Computation, pp. 20–36, IGI Global, 2019.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] M. Sarfraz 和 M. J. Ahmed，“使用神经网络的车牌识别系统方法，”发表于《探索进化计算的关键方法》，第20–36页，IGI
    Global，2019年。'
- en: '[173] H. Li, P. Wang, and C. Shen, “Toward end-to-end car license plate detection
    and recognition with deep neural networks,” IEEE Transactions on Intelligent Transportation
    Systems, no. 99, pp. 1–11, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] H. Li, P. Wang 和 C. Shen，“利用深度神经网络实现端到端车牌检测和识别，”《IEEE智能交通系统学报》，第99期，第1–11页，2018年。'
- en: '[174] J. Qian and B. Qu, “Fast license plate recognition method based on competitive
    neural network,” in 2018 3rd International Conference on Communications, Information
    Management and Network Security (CIMNS 2018), Atlantis Press, 2018.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] J. Qian 和 B. Qu，“基于竞争神经网络的快速车牌识别方法，”发表于2018年第三届国际通信、信息管理与网络安全会议（CIMNS
    2018），Atlantis Press，2018年。'
- en: '[175] R. Laroca, E. Severo, L. A. Zanlorensi, L. S. Oliveira, G. R. Gonçalves,
    W. R. Schwartz, and D. Menotti, “A robust real-time automatic license plate recognition
    based on the yolo detector,” in 2018 International Joint Conference on Neural
    Networks (IJCNN), pp. 1–10, IEEE, 2018.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] R. Laroca, E. Severo, L. A. Zanlorensi, L. S. Oliveira, G. R. Gonçalves,
    W. R. Schwartz 和 D. Menotti，“基于YOLO检测器的鲁棒实时自动车牌识别，”发表于2018年国际神经网络联合会议（IJCNN），第1–10页，IEEE，2018年。'
- en: '[176] A. S. Nair, S. Raju, K. Harikrishnan, and A. Mathew, “A survey of techniques
    for license plate detection and recognition,” i-manager’s Journal on Image Processing,
    vol. 5, no. 1, p. 25, 2018.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] A. S. Nair, S. Raju, K. Harikrishnan 和 A. Mathew，“车牌检测与识别技术综述，”《i-manager
    图像处理期刊》，第5卷，第1期，页码 25，2018年。'
- en: '[177] W. Lu, Y. Zhou, G. Wan, S. Hou, S. Song, and B. A. D. B. U. ADU, “L3-net:
    Towards learning based lidar localization for autonomous driving,” in Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6389–6398,
    2019.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] W. Lu, Y. Zhou, G. Wan, S. Hou, S. Song 和 B. A. D. B. U. ADU，“L3-net:
    面向自动驾驶的基于学习的激光雷达定位，”在IEEE计算机视觉与模式识别会议论文集中，页码 6389–6398，2019年。'
- en: '[178] X. Song, P. Wang, D. Zhou, R. Zhu, C. Guan, Y. Dai, H. Su, H. Li, and
    R. Yang, “Apollocar3d: A large 3d car instance understanding benchmark for autonomous
    driving,” arXiv preprint arXiv:1811.12222, 2018.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] X. Song, P. Wang, D. Zhou, R. Zhu, C. Guan, Y. Dai, H. Su, H. Li 和 R.
    Yang，“Apollocar3d: 一种大型 3D 车辆实例理解基准，用于自动驾驶，”arXiv 预印本 arXiv:1811.12222，2018年。'
- en: '[179] K. Banerjee, D. Notz, J. Windelen, S. Gavarraju, and M. He, “Online camera
    lidar fusion and object detection on hybrid data for autonomous driving,” in 2018
    IEEE Intelligent Vehicles Symposium (IV), pp. 1632–1638, IEEE, 2018.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] K. Banerjee, D. Notz, J. Windelen, S. Gavarraju 和 M. He，“在线相机激光雷达融合和混合数据的物体检测，用于自动驾驶，”在2018年IEEE智能车辆研讨会（IV）上，页码
    1632–1638，IEEE，2018年。'
- en: '[180] E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby, and A. Mouzakitis,
    “A survey on 3d object detection methods for autonomous driving applications,”
    IEEE Transactions on Intelligent Transportation Systems, 2019.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby 和 A. Mouzakitis，“关于自动驾驶应用的
    3D 物体检测方法的综述，”《IEEE 智能交通系统学报》，2019年。'
- en: '[181] J. Li and Z. Wang, “Real-time traffic sign recognition based on efficient
    cnns in the wild,” IEEE Transactions on Intelligent Transportation Systems, no. 99,
    pp. 1–10, 2018.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] J. Li 和 Z. Wang，“基于高效 CNN 的实时交通标志识别，”《IEEE 智能交通系统学报》，第99期，页码 1–10，2018年。'
- en: '[182] T. Moritani, Y. Otsubo, and T. Arinaga, “Traffic sign recognition system,”
    Jan. 9 2018. US Patent 9,865,165.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] T. Moritani, Y. Otsubo 和 T. Arinaga，“交通标志识别系统，”2018年1月9日，美国专利 9,865,165。'
- en: '[183] S. Khalid, N. Muhammad, and M. Sharif, “Automatic measurement of the
    traffic sign with digital segmentation and recognition,” IET Intelligent Transport
    Systems, vol. 13, no. 2, pp. 269–279, 2018.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] S. Khalid, N. Muhammad 和 M. Sharif，“基于数字分割和识别的交通标志自动测量，”《IET 智能交通系统》，第13卷，第2期，页码
    269–279，2018年。'
- en: '[184] Á. Arcos-García, J. A. Álvarez-García, and L. M. Soria-Morillo, “Deep
    neural network for traffic sign recognition systems: An analysis of spatial transformers
    and stochastic optimisation methods,” Neural Networks, vol. 99, pp. 158–165, 2018.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Á. Arcos-García, J. A. Álvarez-García 和 L. M. Soria-Morillo，“用于交通标志识别系统的深度神经网络：空间变换器和随机优化方法的分析，”《神经网络》，第99卷，页码
    158–165，2018年。'
- en: '[185] D. Li, D. Zhao, Y. Chen, and Q. Zhang, “Deepsign: Deep learning based
    traffic sign recognition,” in 2018 international joint conference on neural networks
    (IJCNN), pp. 1–6, IEEE, 2018.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] D. Li, D. Zhao, Y. Chen 和 Q. Zhang，“Deepsign: 基于深度学习的交通标志识别，”在2018年国际神经网络联合会议（IJCNN）上，页码
    1–6，IEEE，2018年。'
- en: '[186] B.-X. Wu, P.-Y. Wang, Y.-T. Yang, and J.-I. Guo, “Traffic sign recognition
    with light convolutional networks,” in 2018 IEEE International Conference on Consumer
    Electronics-Taiwan (ICCE-TW), pp. 1–2, IEEE, 2018.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] B.-X. Wu, P.-Y. Wang, Y.-T. Yang 和 J.-I. Guo，“使用轻量卷积网络进行交通标志识别，”在2018年IEEE国际消费电子展台湾（ICCE-TW）上，页码
    1–2，IEEE，2018年。'
- en: '[187] S. Zhou, W. Liang, J. Li, and J.-U. Kim, “Improved vgg model for road
    traffic sign recognition,” Computers, Materials and Continua, vol. 57, pp. 11–24,
    2018.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] S. Zhou, W. Liang, J. Li 和 J.-U. Kim，“改进的 VGG 模型用于道路交通标志识别，”《计算机、材料与连续介质》，第57卷，页码
    11–24，2018年。'
- en: '[188] Z. Li, M. Dong, S. Wen, X. Hu, P. Zhou, and Z. Zeng, “Clu-cnns: Object
    detection for medical images,” Neurocomputing, vol. 350, pp. 53–59, 2019.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Z. Li, M. Dong, S. Wen, X. Hu, P. Zhou 和 Z. Zeng，“Clu-cnns: 医学图像中的物体检测，”《神经计算》，第350卷，页码
    53–59，2019年。'
- en: '[189] Q. Liu, L. Fang, G. Yu, D. Wang, C.-L. Xiao, and K. Wang, “Detection
    of dna base modifications by deep recurrent neural network on oxford nanopore
    sequencing data,” Nature Communications, vol. 10, no. 1, p. 2449, 2019.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Q. Liu, L. Fang, G. Yu, D. Wang, C.-L. Xiao 和 K. Wang，“通过深度递归神经网络检测 DNA
    碱基修饰，基于牛津纳米孔测序数据，”《自然通讯》，第10卷，第1期，页码 2449，2019年。'
- en: '[190] P. J. Schubert, S. Dorkenwald, M. Januszewski, V. Jain, and J. Kornfeld,
    “Learning cellular morphology with neural networks,” Nature Communications, vol. 10,
    no. 1, p. 2736, 2019.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] P. J. Schubert, S. Dorkenwald, M. Januszewski, V. Jain 和 J. Kornfeld，“利用神经网络学习细胞形态，”《自然通讯》，第10卷，第1期，页码
    2736，2019年。'
- en: '[191] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S. W.
    Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler, et al., “Skin lesion analysis
    toward melanoma detection: A challenge at the 2017 international symposium on
    biomedical imaging (isbi), hosted by the international skin imaging collaboration
    (isic),” in 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI
    2018), pp. 168–172, IEEE, 2018.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S.
    W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler 等, “皮肤病变分析以检测黑色素瘤：2017年国际生物医学成像研讨会（ISBI）挑战，由国际皮肤成像合作组织（ISIC）主办,”
    在2018年IEEE第15届国际生物医学成像研讨会（ISBI 2018）上, pp. 168–172, IEEE, 2018.'
- en: '[192] S. Naji, H. A. Jalab, and S. A. Kareem, “A survey on skin detection in
    colored images,” Artificial Intelligence Review, pp. 1–47, 2018.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] S. Naji, H. A. Jalab, 和 S. A. Kareem, “彩色图像中的肤色检测综述,” Artificial Intelligence
    Review, pp. 1–47, 2018.'
- en: '[193] F. Altaf, S. Islam, N. Akhtar, and N. K. Janjua, “Going deep in medical
    image analysis: Concepts, methods, challenges and future directions,” arXiv preprint
    arXiv:1902.05655, 2019.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] F. Altaf, S. Islam, N. Akhtar, 和 N. K. Janjua, “深入医学图像分析：概念、方法、挑战和未来方向,”
    arXiv 预印本 arXiv:1902.05655, 2019.'
- en: '[194] E. Goldman, R. Herzig, A. Eisenschtat, O. Ratzon, I. Levi, J. Goldberger,
    and T. Hassner, “Precise detection in densely packed scenes,” CoRR, vol. abs/1904.00853,
    2019.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] E. Goldman, R. Herzig, A. Eisenschtat, O. Ratzon, I. Levi, J. Goldberger,
    和 T. Hassner, “在密集场景中的精确检测,” CoRR, vol. abs/1904.00853, 2019.'
- en: '[195] Z. Yang, Q. Li, L. Wenyin, and J. Lv, “Shared multi-view data representation
    for multi-domain event detection,” IEEE Transactions on Pattern Analysis and Machine
    Intelligence, pp. 1–1, 2019.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Z. Yang, Q. Li, L. Wenyin, 和 J. Lv, “多域事件检测的共享多视角数据表示,” IEEE Transactions
    on Pattern Analysis and Machine Intelligence, pp. 1–1, 2019.'
- en: '[196] Y. Wang, H. Sundaram, and L. Xie, “Social event detection with interaction
    graph modeling,” in Proceedings of the 20th ACM international conference on Multimedia,
    pp. 865–868, ACM, 2012.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Y. Wang, H. Sundaram, 和 L. Xie, “通过互动图建模进行社交事件检测,” 在第20届ACM国际多媒体会议论文集中,
    pp. 865–868, ACM, 2012.'
- en: '[197] M. Schinas, S. Papadopoulos, G. Petkos, Y. Kompatsiaris, and P. A. Mitkas,
    “Multimodal graph-based event detection and summarization in social media streams,”
    in Proceedings of the 23rd ACM international conference on Multimedia, pp. 189–192,
    ACM, 2015.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] M. Schinas, S. Papadopoulos, G. Petkos, Y. Kompatsiaris, 和 P. A. Mitkas,
    “社交媒体流中的多模态图基事件检测和总结,” 在第23届ACM国际多媒体会议论文集中, pp. 189–192, ACM, 2015.'
- en: '[198] M. Hasan, M. A. Orgun, and R. Schwitter, “A survey on real-time event
    detection from the twitter data stream,” Journal of Information Science, vol. 44,
    no. 4, pp. 443–463, 2018.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] M. Hasan, M. A. Orgun, 和 R. Schwitter, “实时事件检测综述，来自Twitter数据流,” Journal
    of Information Science, vol. 44, no. 4, pp. 443–463, 2018.'
- en: '[199] O. Teboul, I. Kokkinos, L. Simon, P. Koutsourakis, and N. Paragios, “Shape
    grammar parsing via reinforcement learning,” in CVPR 2011, pp. 2273–2280, IEEE,
    2011.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] O. Teboul, I. Kokkinos, L. Simon, P. Koutsourakis, 和 N. Paragios, “通过强化学习进行形状语法解析,”
    在CVPR 2011, pp. 2273–2280, IEEE, 2011.'
- en: '[200] P. Zhao, T. Fang, J. Xiao, H. Zhang, Q. Zhao, and L. Quan, “Rectilinear
    parsing of architecture in urban environment,” in 2010 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition, pp. 342–349, IEEE, 2010.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] P. Zhao, T. Fang, J. Xiao, H. Zhang, Q. Zhao, 和 L. Quan, “城市环境中建筑物的直线解析,”
    在2010 IEEE计算机协会计算机视觉与模式识别会议上, pp. 342–349, IEEE, 2010.'
- en: '[201] S. Friedman and I. Stamos, “Online detection of repeated structures in
    point clouds of urban scenes for compression and registration,” International
    journal of computer vision, vol. 102, no. 1-3, pp. 112–128, 2013.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] S. Friedman 和 I. Stamos, “城市场景点云中重复结构的在线检测用于压缩和配准,” International Journal
    of Computer Vision, vol. 102, no. 1-3, pp. 112–128, 2013.'
- en: '[202] C.-H. Shen, S.-S. Huang, H. Fu, and S.-M. Hu, “Adaptive partitioning
    of urban facades,” in ACM Transactions on Graphics (TOG), vol. 30, p. 184, ACM,
    2011.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] C.-H. Shen, S.-S. Huang, H. Fu, 和 S.-M. Hu, “城市立面自适应分割,” 在ACM Transactions
    on Graphics (TOG), vol. 30, p. 184, ACM, 2011.'
- en: '[203] G. Schindler, P. Krishnamurthy, R. Lublinerman, Y. Liu, and F. Dellaert,
    “Detecting and matching repeated patterns for automatic geo-tagging in urban environments,”
    in 2008 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–7, IEEE,
    2008.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] G. Schindler, P. Krishnamurthy, R. Lublinerman, Y. Liu, 和 F. Dellaert,
    “检测和匹配重复模式用于城市环境中的自动地理标记,” 在2008 IEEE计算机视觉与模式识别会议上, pp. 1–7, IEEE, 2008.'
- en: '[204] C. Wu, J.-M. Frahm, and M. Pollefeys, “Detecting large repetitive structures
    with salient boundaries,” in European conference on computer vision, pp. 142–155,
    Springer, 2010.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] C. Wu, J.-M. Frahm, 和 M. Pollefeys, “具有显著边界的大型重复结构检测,” 在欧洲计算机视觉会议论文集中,
    pp. 142–155, Springer, 2010.'
- en: '[205] P. Müller, G. Zeng, P. Wonka, and L. Van Gool, “Image-based procedural
    modeling of facades,” in ACM Transactions on Graphics (TOG), vol. 26, p. 85, ACM,
    2007.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] P. Müller, G. Zeng, P. Wonka, 和 L. Van Gool, “基于图像的立面程序建模”，发表于 ACM 图形学交易（TOG），第26卷，第85页，ACM，2007年。'
- en: '[206] O. Barinova, V. Lempitsky, E. Tretiak, and P. Kohli, “Geometric image
    parsing in man-made environments,” in European conference on computer vision,
    pp. 57–70, Springer, 2010.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] O. Barinova, V. Lempitsky, E. Tretiak, 和 P. Kohli, “人造环境中的几何图像解析”，发表于欧洲计算机视觉会议论文集，第57–70页，Springer，2010年。'
- en: '[207] M. Kozinski, R. Gadde, S. Zagoruyko, G. Obozinski, and R. Marlet, “A
    mrf shape prior for facade parsing with occlusions,” in Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 2820–2828, 2015.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] M. Kozinski, R. Gadde, S. Zagoruyko, G. Obozinski, 和 R. Marlet, “用于处理遮挡的外立面解析的
    mrf 形状先验”，发表于 IEEE 计算机视觉与模式识别会议论文集，第2820–2828页，2015年。'
- en: '[208] A. Cohen, A. G. Schwing, and M. Pollefeys, “Efficient structured parsing
    of facades using dynamic programming,” in Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 3206–3213, 2014.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] A. Cohen, A. G. Schwing, 和 M. Pollefeys, “使用动态规划高效结构化解析立面”，发表于 IEEE 计算机视觉与模式识别会议论文集，第3206–3213页，2014年。'
- en: '[209] S. Gandy, B. Recht, and I. Yamada, “Tensor completion and low-n-rank
    tensor recovery via convex optimization,” Inverse Problems, vol. 27, no. 2, p. 025010,
    2011.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] S. Gandy, B. Recht, 和 I. Yamada, “通过凸优化进行张量补全和低秩张量恢复”，逆问题，第27卷，第2期，第025010页，2011年。'
- en: '[210] E. J. Candès, X. Li, Y. Ma, and J. Wright, “Robust principal component
    analysis?,” Journal of the ACM (JACM), vol. 58, no. 3, p. 11, 2011.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] E. J. Candès, X. Li, Y. Ma, 和 J. Wright, “鲁棒主成分分析？”，ACM 学报（JACM），第58卷，第3期，第11页，2011年。'
- en: '[211] J. Liu, P. Musialski, P. Wonka, and J. Ye, “Tensor completion for estimating
    missing values in visual data,” IEEE transactions on pattern analysis and machine
    intelligence, vol. 35, no. 1, pp. 208–220, 2012.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] J. Liu, P. Musialski, P. Wonka, 和 J. Ye, “用于估计视觉数据中缺失值的张量补全”，IEEE 图案分析与机器智能学报，第35卷，第1期，第208–220页，2012年。'
- en: '[212] J. Liu, E. Psarakis, Y. Feng, and I. Stamos, “A kronecker product model
    for repeated pattern detection on 2d urban images,” IEEE Transactions on Pattern
    Analysis and Machine Intelligence, pp. 1–1, 2018.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] J. Liu, E. Psarakis, Y. Feng, 和 I. Stamos, “用于2D城市图像的重复模式检测的 Kronecker
    积模型”，IEEE 图案分析与机器智能学报，第1–1页，2018年。'
- en: '[213] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang,
    “Bottom-up and top-down attention for image captioning and visual question answering,”
    in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 6077–6086, 2018.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, 和 L.
    Zhang, “图像描述和视觉问答的自下而上与自上而下注意力”，发表于 IEEE 计算机视觉与模式识别会议论文集，第6077–6086页，2018年。'
- en: '[214] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural
    image caption generator,” in Proceedings of the IEEE conference on computer vision
    and pattern recognition, pp. 3156–3164, 2015.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] O. Vinyals, A. Toshev, S. Bengio, 和 D. Erhan, “展示与讲述：一种神经图像描述生成器”，发表于
    IEEE 计算机视觉与模式识别会议论文集，第3156–3164页，2015年。'
- en: '[215] J. Gu, J. Cai, G. Wang, and T. Chen, “Stack-captioning: Coarse-to-fine
    learning for image captioning,” in Thirty-Second AAAI Conference on Artificial
    Intelligence, 2018.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] J. Gu, J. Cai, G. Wang, 和 T. Chen, “Stack-captioning：图像描述的粗到细学习”，发表于第三十二届
    AAAI 人工智能大会，2018年。'
- en: '[216] T. Yao, Y. Pan, Y. Li, and T. Mei, “Exploring visual relationship for
    image captioning,” in Proceedings of the European Conference on Computer Vision
    (ECCV), pp. 684–699, 2018.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] T. Yao, Y. Pan, Y. Li, 和 T. Mei, “探索图像描述的视觉关系”，发表于欧洲计算机视觉会议（ECCV）论文集，第684–699页，2018年。'
- en: '[217] J. Aneja, A. Deshpande, and A. G. Schwing, “Convolutional image captioning,”
    in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 5561–5570, 2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] J. Aneja, A. Deshpande, 和 A. G. Schwing, “卷积图像描述”，发表于 IEEE 计算机视觉与模式识别会议论文集，第5561–5570页，2018年。'
- en: '[218] S. Bai and S. An, “A survey on automatic image caption generation,” Neurocomputing,
    vol. 311, pp. 291–304, 2018.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] S. Bai 和 S. An, “自动图像描述生成的综述”，Neurocomputing，第311卷，第291–304页，2018年。'
- en: '[219] W. Yang, R. T. Tan, J. Feng, J. Liu, S. Yan, and Z. Guo, “Joint rain
    detection and removal from a single image with contextualized deep networks,”
    IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2019.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] W. Yang, R. T. Tan, J. Feng, J. Liu, S. Yan, 和 Z. Guo, “通过上下文化深度网络进行单幅图像的联合雨滴检测与去除”，IEEE
    图案分析与机器智能学报，第1–1页，2019年。'
- en: '[220] X. Hu, C. Fu, L. Zhu, J. Qin, and P. Heng, “Direction-aware spatial context
    features for shadow detection and removal,” IEEE Transactions on Pattern Analysis
    and Machine Intelligence, pp. 1–1, 2019.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] X. 胡, C. 傅, L. 朱, J. 秦, 和 P. 亨, “用于阴影检测和去除的方向感知空间上下文特征，” IEEE模式分析与机器智能汇刊,
    第1页–第1页, 2019年。'
- en: '[221] J. Wäldchen and P. Mäder, “Machine learning for image based species identification,”
    Methods in Ecology and Evolution, vol. 9, no. 11, pp. 2216–2225, 2018.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] J. 瓦尔德钦 和 P. 马德尔, “基于图像的物种识别的机器学习，” 生态学与进化方法, 第9卷, 第11期, 第2216页–第2225页,
    2018年。'
- en: '[222] H. Bilen and A. Vedaldi, “Weakly supervised deep detection networks,”
    in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 2846–2854, 2016.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] H. 比伦 和 A. 维达尔迪, “弱监督深度检测网络，” 在IEEE计算机视觉与模式识别会议论文集中, 第2846页–第2854页, 2016年。'
- en: '[223] V. Kantorov, M. Oquab, M. Cho, and I. Laptev, “Contextlocnet: Context-aware
    deep network models for weakly supervised localization,” in European Conference
    on Computer Vision, pp. 350–365, Springer, 2016.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] V. 坎托罗夫, M. 奥夸布, M. 乔, 和 I. 拉普捷夫, “Contextlocnet: 面向弱监督定位的上下文感知深度网络模型，”
    在欧洲计算机视觉会议, 第350页–第365页, 斯普林格, 2016年。'
- en: '[224] P. Tang, X. Wang, X. Bai, and W. Liu, “Multiple instance detection network
    with online instance classifier refinement,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2843–2851, 2017.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] P. 唐, X. 王, X. 白, 和 W. 刘, “具有在线实例分类器精炼的多实例检测网络，” 在IEEE计算机视觉与模式识别会议论文集中,
    第2843页–第2851页, 2017年。'
- en: '[225] A. Diba, V. Sharma, A. Pazandeh, H. Pirsiavash, and L. Van Gool, “Weakly
    supervised cascaded convolutional networks,” in Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 914–922, 2017.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] A. 迪巴, V. 香玛, A. 帕赞德赫, H. 皮尔西亚瓦什, 和 L. 范·古尔, “弱监督级联卷积网络，” 在IEEE计算机视觉与模式识别会议论文集中,
    第914页–第922页, 2017年。'
- en: '[226] Y. Li, L. Liu, C. Shen, and A. van den Hengel, “Image co-localization
    by mimicking a good detector’s confidence score distribution,” in European Conference
    on Computer Vision, pp. 19–34, Springer, 2016.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] Y. 李, L. 刘, C. 沈, 和 A. 范登·亨格尔, “通过模仿良好检测器的置信度分布进行图像共同定位，” 在欧洲计算机视觉会议,
    第19页–第34页, 斯普林格, 2016年。'
- en: '[227] Z. Yang, D. Mahajan, D. Ghadiyaram, R. Nevatia, and V. Ramanathan, “Activity
    driven weakly supervised object detection,” arXiv preprint arXiv:1904.01665, 2019.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] Z. 杨, D. 马哈詹, D. 加迪亚拉姆, R. 内瓦蒂亚, 和 V. 拉曼纳坦, “活动驱动的弱监督目标检测，” arXiv 预印本
    arXiv:1904.01665, 2019年。'
- en: '[228] F. Wan, P. Wei, Z. Han, J. Jiao, and Q. Ye, “Min-entropy latent model
    for weakly supervised object detection,” IEEE Transactions on Pattern Analysis
    and Machine Intelligence, pp. 1–1, 2019.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] F. 万, P. 魏, Z. 韩, J. 焦, 和 Q. 叶, “用于弱监督目标检测的最小熵潜变量模型，” IEEE模式分析与机器智能汇刊,
    第1页–第1页, 2019年。'
- en: '[229] P. Tang, X. Wang, S. Bai, W. Shen, X. Bai, W. Liu, and A. L. Yuille,
    “Pcl: Proposal cluster learning for weakly supervised object detection,” IEEE
    Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] P. 唐, X. 王, S. 白, W. 申, X. 白, W. 刘, 和 A. L. 尤伊尔, “Pcl: 提案聚类学习用于弱监督目标检测，”
    IEEE模式分析与机器智能汇刊, 第1页–第1页, 2018年。'
- en: '[230] C. Cao, Y. Huang, Y. Yang, L. Wang, Z. Wang, and T. Tan, “Feedback convolutional
    neural network for visual localization and segmentation,” IEEE Transactions on
    Pattern Analysis and Machine Intelligence, vol. 41, pp. 1627–1640, July 2019.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] C. 曹, Y. 黄, Y. 杨, L. 王, Z. 王, 和 T. 谭, “用于视觉定位和分割的反馈卷积神经网络，” IEEE模式分析与机器智能汇刊,
    第41卷, 第1627页–第1640页, 2019年7月。'
- en: '[231] F. Wan, C. Liu, W. Ke, X. Ji, J. Jiao, and Q. Ye, “C-mil: Continuation
    multiple instance learning for weakly supervised object detection,” arXiv preprint
    arXiv:1904.05647, 2019.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] F. 万, C. 刘, W. 柯, X. 季, J. 焦, 和 Q. 叶, “C-mil: 连续多实例学习用于弱监督目标检测，” arXiv
    预印本 arXiv:1904.05647, 2019年。'
- en: '[232] Z. Wu, L. Su, and Q. Huang, “Cascaded partial decoder for fast and accurate
    salient object detection,” arXiv preprint arXiv:1904.08739, 2019.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] Z. 吴, L. 苏, 和 Q. 黄, “用于快速准确显著物体检测的级联部分解码器，” arXiv 预印本 arXiv:1904.08739,
    2019年。'
- en: '[233] J.-J. Liu, Q. Hou, M.-M. Cheng, J. Feng, and J. Jiang, “A simple pooling-based
    design for real-time salient object detection,” arXiv preprint arXiv:1904.09569,
    2019.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] J.-J. 刘, Q. 侯, M.-M. 程, J. 冯, 和 J. 江, “一种基于池化的实时显著物体检测简单设计，” arXiv 预印本
    arXiv:1904.09569, 2019年。'
- en: '[234] W. Wang, J. Shen, X. Dong, A. Borji, and R. Yang, “Inferring salient
    objects from human fixations,” IEEE Transactions on Pattern Analysis and Machine
    Intelligence, pp. 1–1, 2019.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] W. 王, J. 申, X. 董, A. 博尔吉, 和 R. 杨, “从人类注视点推断显著物体，” IEEE模式分析与机器智能汇刊, 第1页–第1页,
    2019年。'
- en: '[235] L. Wang, L. Wang, H. Lu, P. Zhang, and X. Ruan, “Salient object detection
    with recurrent fully convolutional networks,” IEEE Transactions on Pattern Analysis
    and Machine Intelligence, vol. 41, pp. 1734–1746, July 2019.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] L. 王, L. 王, H. 陆, P. 张, 和 X. 阮, “基于递归全卷积网络的显著目标检测，”《IEEE模式分析与机器智能汇刊》，第41卷，页1734–1746，2019年7月。'
- en: '[236] M. Feng, H. Lu, and E. Ding, “Attentive feedback network for boundary-aware
    salient object detection,” in Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 1623–1632, 2019.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] M. 冯, H. 陆, 和 E. 丁, “用于边界感知显著目标检测的注意力反馈网络，”在《IEEE计算机视觉与模式识别会议论文集》中，页1623–1632，2019年。'
- en: '[237] D.-P. Fan, W. Wang, M.-M. Cheng, and J. Shen, “Shifting more attention
    to video salient object detection,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 8554–8564, 2019.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] D.-P. 范, W. 王, M.-M. 程, 和 J. 沈, “将更多关注转向视频显著目标检测，”在《IEEE计算机视觉与模式识别会议论文集》中，页8554–8564，2019年。'
- en: '[238] H. Kim, Y. Kim, J.-Y. Sim, and C.-S. Kim, “Spatiotemporal saliency detection
    for video sequences based on random walk with restart,” IEEE Transactions on Image
    Processing, vol. 24, no. 8, pp. 2552–2564, 2015.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] H. 金, Y. 金, J.-Y. 辛, 和 C.-S. 金, “基于随机游走重启的视频序列时空显著性检测，”《IEEE图像处理汇刊》，第24卷，第8期，页2552–2564，2015年。'
- en: '[239] F. Li, T. Kim, A. Humayun, D. Tsai, and J. M. Rehg, “Video segmentation
    by tracking many figure-ground segments,” in Proceedings of the IEEE International
    Conference on Computer Vision, pp. 2192–2199, 2013.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] F. 李, T. 金, A. 胡马云, D. 蔡, 和 J. M. 瑞赫, “通过跟踪多个背景分割的视频分割，”在《IEEE国际计算机视觉会议论文集》中，页2192–2199，2013年。'
- en: '[240] J. Li, C. Xia, and X. Chen, “A benchmark dataset and saliency-guided
    stacked autoencoders for video-based salient object detection,” IEEE Transactions
    on Image Processing, vol. 27, no. 1, pp. 349–364, 2017.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] J. 李, C. 夏, 和 X. 陈, “一个基准数据集和用于视频显著目标检测的显著性引导堆叠自编码器，”《IEEE图像处理汇刊》，第27卷，第1期，页349–364，2017年。'
- en: '[241] Z. Liu, J. Li, L. Ye, G. Sun, and L. Shen, “Saliency detection for unconstrained
    videos using superpixel-level graph and spatiotemporal propagation,” IEEE transactions
    on circuits and systems for video technology, vol. 27, no. 12, pp. 2527–2542,
    2016.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] Z. 刘, J. 李, L. 叶, G. 孙, 和 L. 沈, “基于超像素级图和时空传播的无约束视频显著性检测，”《IEEE视频技术电路与系统汇刊》，第27卷，第12期，页2527–2542，2016年。'
- en: '[242] P. Ochs, J. Malik, and T. Brox, “Segmentation of moving objects by long
    term video analysis,” IEEE transactions on pattern analysis and machine intelligence,
    vol. 36, no. 6, pp. 1187–1200, 2013.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] P. 奥赫斯, J. 马利克, 和 T. 布罗克斯, “通过长期视频分析的运动物体分割，”《IEEE模式分析与机器智能汇刊》，第36卷，第6期，页1187–1200，2013年。'
- en: '[243] W. Wang, J. Shen, and L. Shao, “Consistent video saliency using local
    gradient flow optimization and global refinement,” IEEE Transactions on Image
    Processing, vol. 24, no. 11, pp. 4185–4196, 2015.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] W. 王, J. 沈, 和 L. 邵, “使用局部梯度流优化和全局细化的一致视频显著性，”《IEEE图像处理汇刊》，第24卷，第11期，页4185–4196，2015年。'
- en: '[244] C. Chen, S. Li, Y. Wang, H. Qin, and A. Hao, “Video saliency detection
    via spatial-temporal fusion and low-rank coherency diffusion,” IEEE Transactions
    on Image Processing, vol. 26, no. 7, pp. 3156–3170, 2017.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] C. 陈, S. 李, Y. 王, H. 秦, 和 A. 郝, “通过空间-时间融合和低秩一致性扩散的视频显著性检测，”《IEEE图像处理汇刊》，第26卷，第7期，页3156–3170，2017年。'
- en: '[245] Y. Chen, W. Zou, Y. Tang, X. Li, C. Xu, and N. Komodakis, “Scom: Spatiotemporal
    constrained optimization for salient object detection,” IEEE Transactions on Image
    Processing, vol. 27, no. 7, pp. 3345–3357, 2018.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] Y. 陈, W. 邹, Y. 唐, X. 李, C. 徐, 和 N. 科莫达基斯, “Scom: 用于显著目标检测的时空约束优化，”《IEEE图像处理汇刊》，第27卷，第7期，页3345–3357，2018年。'
- en: '[246] S. Li, B. Seybold, A. Vorobyov, X. Lei, and C.-C. Jay Kuo, “Unsupervised
    video object segmentation with motion-based bilateral networks,” in Proceedings
    of the European Conference on Computer Vision (ECCV), pp. 207–223, 2018.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] S. 李, B. 赛博尔德, A. 沃罗比约夫, X. 雷, 和 C.-C. 杰伊·郭, “基于运动的双边网络的无监督视频目标分割，”在《欧洲计算机视觉会议论文集》中，页207–223，2018年。'
- en: '[247] Z. Liu, X. Zhang, S. Luo, and O. Le Meur, “Superpixel-based spatiotemporal
    saliency detection,” IEEE transactions on circuits and systems for video technology,
    vol. 24, no. 9, pp. 1522–1540, 2014.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] Z. 刘, X. 张, S. 罗, 和 O. 勒梅, “基于超像素的时空显著性检测，”《IEEE视频技术电路与系统汇刊》，第24卷，第9期，页1522–1540，2014年。'
- en: '[248] H. Song, W. Wang, S. Zhao, J. Shen, and K.-M. Lam, “Pyramid dilated deeper
    convlstm for video salient object detection,” in Proceedings of the European Conference
    on Computer Vision (ECCV), pp. 715–731, 2018.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] H. Song, W. Wang, S. Zhao, J. Shen 和 K.-M. Lam, “用于视频显著性目标检测的金字塔膨胀更深卷积LSTM,”
    在欧洲计算机视觉会议（ECCV）论文集, 第715–731页, 2018年。'
- en: '[249] Y. Tang, W. Zou, Z. Jin, Y. Chen, Y. Hua, and X. Li, “Weakly supervised
    salient object detection with spatiotemporal cascade neural networks,” IEEE Transactions
    on Circuits and Systems for Video Technology, 2018.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] Y. Tang, W. Zou, Z. Jin, Y. Chen, Y. Hua 和 X. Li, “基于时空级联神经网络的弱监督显著性目标检测,”
    IEEE 视频技术电路与系统汇刊, 2018年。'
- en: '[250] W. Wang, J. Shen, and L. Shao, “Video salient object detection via fully
    convolutional networks,” IEEE Transactions on Image Processing, vol. 27, no. 1,
    pp. 38–49, 2017.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] W. Wang, J. Shen 和 L. Shao, “通过全卷积网络的视频显著性目标检测,” IEEE 图像处理汇刊, 第27卷，第1期，第38–49页,
    2017年。'
- en: '[251] W.-C. Tu, S. He, Q. Yang, and S.-Y. Chien, “Real-time salient object
    detection with a minimum spanning tree,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2334–2342, 2016.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] W.-C. Tu, S. He, Q. Yang 和 S.-Y. Chien, “通过最小生成树进行实时显著性目标检测,” 在IEEE计算机视觉与模式识别会议论文集,
    第2334–2342页, 2016年。'
- en: '[252] W. Wang, J. Shen, and F. Porikli, “Saliency-aware geodesic video object
    segmentation,” in Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 3395–3402, 2015.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] W. Wang, J. Shen 和 F. Porikli, “显著性感知的测地视频目标分割,” 在IEEE计算机视觉与模式识别会议论文集,
    第3395–3402页, 2015年。'
- en: '[253] T. Xi, W. Zhao, H. Wang, and W. Lin, “Salient object detection with spatiotemporal
    background priors for video,” IEEE Transactions on Image Processing, vol. 26,
    no. 7, pp. 3425–3436, 2016.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] T. Xi, W. Zhao, H. Wang 和 W. Lin, “基于时空背景先验的视频显著性目标检测,” IEEE 图像处理汇刊,
    第26卷，第7期，第3425–3436页, 2016年。'
- en: '[254] J. Zhang, S. Sclaroff, Z. Lin, X. Shen, B. Price, and R. Mech, “Minimum
    barrier salient object detection at 80 fps,” in Proceedings of the IEEE international
    conference on computer vision, pp. 1404–1412, 2015.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] J. Zhang, S. Sclaroff, Z. Lin, X. Shen, B. Price 和 R. Mech, “80 fps的最小障碍显著性目标检测,”
    在IEEE国际计算机视觉会议论文集, 第1404–1412页, 2015年。'
- en: '[255] F. Zhou, S. Bing Kang, and M. F. Cohen, “Time-mapping using space-time
    saliency,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 3358–3365, 2014.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] F. Zhou, S. Bing Kang 和 M. F. Cohen, “使用时空显著性进行时间映射,” 在IEEE计算机视觉与模式识别会议论文集,
    第3358–3365页, 2014年。'
- en: '[256] M. Sun, A. Farhadi, and S. Seitz, “Ranking domain-specific highlights
    by analyzing edited videos,” in European conference on computer vision, pp. 787–802,
    Springer, 2014.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] M. Sun, A. Farhadi 和 S. Seitz, “通过分析编辑过的视频对领域特定亮点进行排名,” 在欧洲计算机视觉会议, 第787–802页,
    Springer, 2014年。'
- en: '[257] T. Yao, T. Mei, and Y. Rui, “Highlight detection with pairwise deep ranking
    for first-person video summarization,” in Proceedings of the IEEE conference on
    computer vision and pattern recognition, pp. 982–990, 2016.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] T. Yao, T. Mei 和 Y. Rui, “通过成对深度排名进行第一人称视频总结中的亮点检测,” 在IEEE计算机视觉与模式识别会议论文集,
    第982–990页, 2016年。'
- en: '[258] H. Yang, B. Wang, S. Lin, D. Wipf, M. Guo, and B. Guo, “Unsupervised
    extraction of video highlights via robust recurrent auto-encoders,” in Proceedings
    of the IEEE international conference on computer vision, pp. 4633–4641, 2015.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] H. Yang, B. Wang, S. Lin, D. Wipf, M. Guo 和 B. Guo, “通过鲁棒递归自编码器进行无监督的视频亮点提取,”
    在IEEE国际计算机视觉会议论文集, 第4633–4641页, 2015年。'
- en: '[259] W. Liu, T. Mei, Y. Zhang, C. Che, and J. Luo, “Multi-task deep visual-semantic
    embedding for video thumbnail selection,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 3707–3715, 2015.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] W. Liu, T. Mei, Y. Zhang, C. Che 和 J. Luo, “用于视频缩略图选择的多任务深度视觉语义嵌入,” 在IEEE计算机视觉与模式识别会议论文集,
    第3707–3715页, 2015年。'
- en: '[260] R. Panda, A. Das, Z. Wu, J. Ernst, and A. K. Roy-Chowdhury, “Weakly supervised
    summarization of web videos,” in Proceedings of the IEEE International Conference
    on Computer Vision, pp. 3657–3666, 2017.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] R. Panda, A. Das, Z. Wu, J. Ernst 和 A. K. Roy-Chowdhury, “弱监督的网页视频总结,”
    在IEEE国际计算机视觉会议论文集, 第3657–3666页, 2017年。'
- en: '[261] D. Potapov, M. Douze, Z. Harchaoui, and C. Schmid, “Category-specific
    video summarization,” in European conference on computer vision, pp. 540–555,
    Springer, 2014.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] D. Potapov, M. Douze, Z. Harchaoui 和 C. Schmid, “类别特定的视频总结,” 在欧洲计算机视觉会议,
    第540–555页, Springer, 2014年。'
- en: '[262] B. Xiong, Y. Kalantidis, D. Ghadiyaram, and K. Grauman, “Less is more:
    Learning highlight detection from video duration,” arXiv preprint arXiv:1903.00859,
    2019.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] B. Xiong, Y. Kalantidis, D. Ghadiyaram, 和 K. Grauman， “少即是多：从视频时长中学习高光检测，”
    arXiv预印本 arXiv:1903.00859，2019年。'
- en: '[263] J. He, S. Zhang, M. Yang, Y. Shan, and T. Huang, “Bi-directional cascade
    network for perceptual edge detection,” arXiv preprint arXiv:1902.10903, 2019.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] J. He, S. Zhang, M. Yang, Y. Shan, 和 T. Huang， “用于感知边缘检测的双向级联网络，” arXiv预印本
    arXiv:1902.10903，2019年。'
- en: '[264] Y. Liu, M. Cheng, X. Hu, J. Bian, L. Zhang, X. Bai, and J. Tang, “Richer
    convolutional features for edge detection,” IEEE Transactions on Pattern Analysis
    and Machine Intelligence, pp. 1–1, 2018.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] Y. Liu, M. Cheng, X. Hu, J. Bian, L. Zhang, X. Bai, 和 J. Tang， “更丰富的卷积特征用于边缘检测，”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*，第1–1页，2018年。'
- en: '[265] X. Ren, Y. Zhou, J. He, K. Chen, X. Yang, and J. Sun, “A convolutional
    neural network-based chinese text detection algorithm via text structure modeling,”
    IEEE Transactions on Multimedia, vol. 19, no. 3, pp. 506–518, 2016.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] X. Ren, Y. Zhou, J. He, K. Chen, X. Yang, 和 J. Sun， “基于卷积神经网络的中文文本检测算法，通过文本结构建模，”
    *IEEE Transactions on Multimedia*，第19卷，第3期，第506–518页，2016年。'
- en: '[266] M. Liao, B. Shi, X. Bai, X. Wang, and W. Liu, “Textboxes: A fast text
    detector with a single deep neural network,” in Thirty-First AAAI Conference on
    Artificial Intelligence, 2017.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] M. Liao, B. Shi, X. Bai, X. Wang, 和 W. Liu， “Textboxes：一种使用单个深度神经网络的快速文本检测器，”
    见 *第31届AAAI人工智能会议*，2017年。'
- en: '[267] D. Bazazian, R. Gomez, A. Nicolaou, L. Gomez, D. Karatzas, and A. D.
    Bagdanov, “Improving text proposals for scene images with fully convolutional
    networks,” arXiv preprint arXiv:1702.05089, 2017.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] D. Bazazian, R. Gomez, A. Nicolaou, L. Gomez, D. Karatzas, 和 A. D. Bagdanov，
    “利用全卷积网络改进场景图像的文本提案，” arXiv预印本 arXiv:1702.05089，2017年。'
- en: '[268] Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu, and X. Bai, “Multi-oriented
    text detection with fully convolutional networks,” in Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 4159–4167, 2016.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu, 和 X. Bai， “使用全卷积网络的多方向文本检测，”
    见 *IEEE计算机视觉与模式识别会议论文集*，第4159–4167页，2016年。'
- en: '[269] C. Yao, X. Bai, N. Sang, X. Zhou, S. Zhou, and Z. Cao, “Scene text detection
    via holistic, multi-channel prediction,” arXiv preprint arXiv:1606.09002, 2016.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] C. Yao, X. Bai, N. Sang, X. Zhou, S. Zhou, 和 Z. Cao， “通过整体多通道预测进行场景文本检测，”
    arXiv预印本 arXiv:1606.09002，2016年。'
- en: '[270] T. He, W. Huang, Y. Qiao, and J. Yao, “Accurate text localization in
    natural image with cascaded convolutional text network,” arXiv preprint arXiv:1603.09423,
    2016.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] T. He, W. Huang, Y. Qiao, 和 J. Yao， “使用级联卷积文本网络在自然图像中进行精确文本定位，” arXiv预印本
    arXiv:1603.09423，2016年。'
- en: '[271] P. Lyu, C. Yao, W. Wu, S. Yan, and X. Bai, “Multi-oriented scene text
    detection via corner localization and region segmentation,” in Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7553–7563,
    2018.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] P. Lyu, C. Yao, W. Wu, S. Yan, 和 X. Bai， “通过角点定位和区域分割进行多方向场景文本检测，” 见
    *IEEE计算机视觉与模式识别会议论文集*，第7553–7563页，2018年。'
- en: '[272] J. Ma, W. Shao, H. Ye, L. Wang, H. Wang, Y. Zheng, and X. Xue, “Arbitrary-oriented
    scene text detection via rotation proposals,” IEEE Transactions on Multimedia,
    vol. 20, no. 11, pp. 3111–3122, 2018.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] J. Ma, W. Shao, H. Ye, L. Wang, H. Wang, Y. Zheng, 和 X. Xue， “通过旋转提案进行任意方向场景文本检测，”
    *IEEE Transactions on Multimedia*，第20卷，第11期，第3111–3122页，2018年。'
- en: '[273] X. Wang, Z. Cai, D. Gao, and N. Vasconcelos, “Towards universal object
    detection by domain attention,” arXiv preprint arXiv:1904.04402, 2019.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] X. Wang, Z. Cai, D. Gao, 和 N. Vasconcelos， “通过领域注意力朝向通用对象检测，” arXiv预印本
    arXiv:1904.04402，2019年。'
- en: '[274] H. Bilen and A. Vedaldi, “Universal representations: The missing link
    between faces, text, planktons, and cat breeds,” arXiv preprint arXiv:1701.07275,
    2017.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] H. Bilen 和 A. Vedaldi， “通用表示：面孔、文本、浮游生物和猫品种之间的缺失联系，” arXiv预印本 arXiv:1701.07275，2017年。'
- en: '[275] S.-A. Rebuffi, H. Bilen, and A. Vedaldi, “Learning multiple visual domains
    with residual adapters,” in Advances in Neural Information Processing Systems,
    pp. 506–516, 2017.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] S.-A. Rebuffi, H. Bilen, 和 A. Vedaldi， “通过残差适配器学习多个视觉域，” 见 *神经信息处理系统进展*，第506–516页，2017年。'
- en: '[276] S.-A. Rebuffi, H. Bilen, and A. Vedaldi, “Efficient parametrization of
    multi-domain deep neural networks,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 8119–8127, 2018.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] S.-A. Rebuffi, H. Bilen, 和 A. Vedaldi， “多域深度神经网络的高效参数化，” 见 *IEEE计算机视觉与模式识别会议论文集*，第8119–8127页，2018年。'
- en: '[277] Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool, “Domain adaptive
    faster r-cnn for object detection in the wild,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 3339–3348, 2018.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] Y. Chen, W. Li, C. Sakaridis, D. Dai, 和 L. Van Gool，“用于野外目标检测的领域自适应 Faster
    R-CNN”，发表于 IEEE 计算机视觉与模式识别会议论文集，pp. 3339–3348，2018年。'
- en: '[278] K. Saito, Y. Ushiku, T. Harada, and K. Saenko, “Strong-weak distribution
    alignment for adaptive object detection,” arXiv preprint arXiv:1812.04798, 2018.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] K. Saito, Y. Ushiku, T. Harada, 和 K. Saenko，“用于自适应目标检测的强弱分布对齐”，arXiv
    预印本 arXiv:1812.04798，2018年。'
- en: '[279] A. Haupmann, G. Kang, L. Jiang, and Y. Yang, “Contrastive adaptation
    network for unsupervised domain adaptation,” 2019.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] A. Haupmann, G. Kang, L. Jiang, 和 Y. Yang，“用于无监督领域自适应的对比适应网络”，2019年。'
- en: '[280] W. Han, P. Khorrami, T. L. Paine, P. Ramachandran, M. Babaeizadeh, H. Shi,
    J. Li, S. Yan, and T. S. Huang, “Seq-nms for video object detection,” arXiv preprint
    arXiv:1602.08465, 2016.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] W. Han, P. Khorrami, T. L. Paine, P. Ramachandran, M. Babaeizadeh, H.
    Shi, J. Li, S. Yan, 和 T. S. Huang，“用于视频目标检测的 Seq-nms”，arXiv 预印本 arXiv:1602.08465，2016年。'
- en: '[281] C. Feichtenhofer, A. Pinz, and A. Zisserman, “Detect to track and track
    to detect,” in Proceedings of the IEEE International Conference on Computer Vision,
    pp. 3038–3046, 2017.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] C. Feichtenhofer, A. Pinz, 和 A. Zisserman，“检测以跟踪，跟踪以检测”，发表于 IEEE 国际计算机视觉会议论文集，pp.
    3038–3046，2017年。'
- en: '[282] K. Kang, W. Ouyang, H. Li, and X. Wang, “Object detection from video
    tubelets with convolutional neural networks,” in Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 817–825, 2016.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] K. Kang, W. Ouyang, H. Li, 和 X. Wang，“通过卷积神经网络从视频小段中进行目标检测”，发表于 IEEE
    计算机视觉与模式识别会议论文集，pp. 817–825，2016年。'
- en: '[283] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang, Z. Wang,
    R. Wang, X. Wang, et al., “T-cnn: Tubelets with convolutional neural networks
    for object detection from videos,” IEEE Transactions on Circuits and Systems for
    Video Technology, vol. 28, no. 10, pp. 2896–2907, 2017.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang, Z. Wang,
    R. Wang, X. Wang 等，“T-cnn：使用卷积神经网络的视频目标检测小段”，IEEE 视频技术电路与系统汇刊，第28卷，第10期，pp. 2896–2907，2017年。'
- en: '[284] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang, “Object
    detection in videos with tubelet proposal networks,” in Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 727–735, 2017.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, 和 X. Wang，“使用小段提议网络的视频目标检测”，发表于
    IEEE 计算机视觉与模式识别会议论文集，pp. 727–735，2017年。'
- en: '[285] X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei, “Deep feature flow for
    video recognition,” in Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pp. 2349–2358, 2017.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] X. Zhu, Y. Xiong, J. Dai, L. Yuan, 和 Y. Wei，“用于视频识别的深度特征流”，发表于 IEEE 计算机视觉与模式识别会议论文集，pp.
    2349–2358，2017年。'
- en: '[286] X. Zhu, Y. Wang, J. Dai, L. Yuan, and Y. Wei, “Flow-guided feature aggregation
    for video object detection,” in Proceedings of the IEEE International Conference
    on Computer Vision, pp. 408–417, 2017.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] X. Zhu, Y. Wang, J. Dai, L. Yuan, 和 Y. Wei，“用于视频目标检测的流引导特征聚合”，发表于 IEEE
    国际计算机视觉会议论文集，pp. 408–417，2017年。'
- en: '[287] L. Wang, W. Ouyang, X. Wang, and H. Lu, “Visual tracking with fully convolutional
    networks,” in Proceedings of the IEEE international conference on computer vision,
    pp. 3119–3127, 2015.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] L. Wang, W. Ouyang, X. Wang, 和 H. Lu，“基于全卷积网络的视觉跟踪”，发表于 IEEE 国际计算机视觉会议论文集，pp.
    3119–3127，2015年。'
- en: '[288] G. Bertasius, L. Torresani, and J. Shi, “Object detection in video with
    spatiotemporal sampling networks,” in Proceedings of the European Conference on
    Computer Vision (ECCV), pp. 331–346, 2018.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] G. Bertasius, L. Torresani, 和 J. Shi，“具有时空采样网络的视频目标检测”，发表于 欧洲计算机视觉会议（ECCV）论文集，pp.
    331–346，2018年。'
- en: '[289] F. Xiao and Y. Jae Lee, “Video object detection with an aligned spatial-temporal
    memory,” in Proceedings of the European Conference on Computer Vision (ECCV),
    pp. 485–501, 2018.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] F. Xiao 和 Y. Jae Lee，“具有对齐时空记忆的视频目标检测”，发表于 欧洲计算机视觉会议（ECCV）论文集，pp. 485–501，2018年。'
- en: '[290] P. Tang, C. Wang, X. Wang, W. Liu, W. Zeng, and J. Wang, “Object detection
    in videos by high quality object linking,” IEEE Transactions on Pattern Analysis
    and Machine Intelligence, pp. 1–1, 2019.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] P. Tang, C. Wang, X. Wang, W. Liu, W. Zeng, 和 J. Wang，“通过高质量目标链接进行视频中的目标检测”，IEEE
    模式分析与机器智能汇刊，pp. 1–1，2019年。'
- en: '[291] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, and I. Posner, “Vote3deep:
    Fast object detection in 3d point clouds using efficient convolutional neural
    networks,” in 2017 IEEE International Conference on Robotics and Automation (ICRA),
    pp. 1355–1361, IEEE, 2017.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong 和 I. Posner，“Vote3deep：使用高效卷积神经网络在3D点云中进行快速物体检测，”在2017年IEEE国际机器人与自动化会议（ICRA）中，第1355–1361页，IEEE，2017年。'
- en: '[292] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 652–660, 2017.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] C. R. Qi, H. Su, K. Mo 和 L. J. Guibas，“Pointnet：用于3D分类和分割的点集深度学习，”在《IEEE计算机视觉与模式识别会议论文集》中，第652–660页，2017年。'
- en: '[293] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
    feature learning on point sets in a metric space,” in Advances in Neural Information
    Processing Systems, pp. 5099–5108, 2017.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] C. R. Qi, L. Yi, H. Su 和 L. J. Guibas，“Pointnet++：在度量空间中的点集上进行深层次的层次特征学习，”在《神经信息处理系统进展》中，第5099–5108页，2017年。'
- en: '[294] Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud
    based 3d object detection,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 4490–4499, 2018.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] Y. Zhou 和 O. Tuzel，“Voxelnet：基于点云的3D物体检测的端到端学习，”在《IEEE计算机视觉与模式识别会议论文集》中，第4490–4499页，2018年。'
- en: '[295] X. Chen, K. Kundu, Z. Zhang, H. Ma, S. Fidler, and R. Urtasun, “Monocular
    3d object detection for autonomous driving,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2147–2156, 2016.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] X. 陈, K. Kundu, Z. Zhang, H. Ma, S. Fidler 和 R. Urtasun，“用于自动驾驶的单目3D物体检测，”在《IEEE计算机视觉与模式识别会议论文集》中，第2147–2156页，2016年。'
- en: '[296] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object detection
    network for autonomous driving,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 1907–1915, 2017.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] X. 陈, H. Ma, J. Wan, B. Li 和 T. Xia，“用于自动驾驶的多视角3D物体检测网络，”在《IEEE计算机视觉与模式识别会议论文集》中，第1907–1915页，2017年。'
- en: '[297] V. A. Sindagi, Y. Zhou, and O. Tuzel, “Mvx-net: Multimodal voxelnet for
    3d object detection,” arXiv preprint arXiv:1904.01649, 2019.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] V. A. Sindagi, Y. Zhou 和 O. Tuzel，“Mvx-net：用于3D物体检测的多模态体素网，”arXiv预印本
    arXiv:1904.01649，2019年。'
- en: '[298] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person 2d
    pose estimation using part affinity fields,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 7291–7299, 2017.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] Z. Cao, T. Simon, S.-E. Wei 和 Y. Sheikh，“实时多人的2D姿态估计使用部件亲和场，”在《IEEE计算机视觉与模式识别会议论文集》中，第7291–7299页，2017年。'
- en: '[299] A. Bulat and G. Tzimiropoulos, “Human pose estimation via convolutional
    part heatmap regression,” in European Conference on Computer Vision, pp. 717–732,
    Springer, 2016.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] A. Bulat 和 G. Tzimiropoulos，“通过卷积部件热图回归进行人体姿态估计，”在《欧洲计算机视觉会议》中，第717–732页，Springer，2016年。'
- en: '[300] A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for human
    pose estimation,” in European Conference on Computer Vision, pp. 483–499, Springer,
    2016.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] A. Newell, K. Yang 和 J. Deng，“用于人体姿态估计的堆叠沙漏网络，”在《欧洲计算机视觉会议》中，第483–499页，Springer，2016年。'
- en: '[301] X. Chen and A. L. Yuille, “Articulated pose estimation by a graphical
    model with image dependent pairwise relations,” in Advances in neural information
    processing systems, pp. 1736–1744, 2014.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] X. 陈 和 A. L. Yuille，“通过具有图像依赖的成对关系的图形模型进行姿态估计，”在《神经信息处理系统进展》中，第1736–1744页，2014年。'
- en: '[302] A. Toshev and C. Szegedy, “Deeppose: Human pose estimation via deep neural
    networks,” in Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 1653–1660, 2014.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] A. Toshev 和 C. Szegedy，“Deeppose：通过深度神经网络进行人体姿态估计，”在《IEEE计算机视觉与模式识别会议论文集》中，第1653–1660页，2014年。'
- en: '[303] X. Fan, K. Zheng, Y. Lin, and S. Wang, “Combining local appearance and
    holistic view: Dual-source deep neural networks for human pose estimation,” in
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 1347–1355, 2015.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] X. Fan, K. Zheng, Y. Lin 和 S. Wang，“结合局部外观和整体视图：用于人体姿态估计的双源深度神经网络，”在《IEEE计算机视觉与模式识别会议论文集》中，第1347–1355页，2015年。'
- en: '[304] W. Ouyang, X. Chu, and X. Wang, “Multi-source deep learning for human
    pose estimation,” in Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pp. 2329–2336, 2014.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] W. Ouyang, X. Chu 和 X. Wang，“用于人体姿态估计的多源深度学习，”在《IEEE计算机视觉与模式识别会议论文集》中，第2329–2336页，2014年。'
- en: '[305] G. Rogez, P. Weinzaepfel, and C. Schmid, “Lcr-net++: Multi-person 2d
    and 3d pose detection in natural images,” IEEE Transactions on Pattern Analysis
    and Machine Intelligence, pp. 1–1, 2019.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] G. Rogez, P. Weinzaepfel, 和 C. Schmid， “LCR-Net++：自然图像中的多人人体2D和3D姿态检测”，《IEEE模式分析与机器智能汇刊》，第1–1页，2019年。'
- en: '[306] Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, and J. Sun, “Cascaded pyramid
    network for multi-person pose estimation,” in Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 7103–7112, 2018.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, 和 J. Sun， “用于多人人体姿态估计的级联金字塔网络”，发表于《IEEE计算机视觉与模式识别会议论文集》，第7103–7112页，2018年。'
- en: '[307] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tompson, C. Bregler,
    and K. Murphy, “Towards accurate multi-person pose estimation in the wild,” in
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 4903–4911, 2017.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tompson, C. Bregler,
    和 K. Murphy， “朝着准确的野外多人人体姿态估计”，发表于《IEEE计算机视觉与模式识别会议论文集》，第4903–4911页，2017年。'
- en: '[308] B. Xiao, H. Wu, and Y. Wei, “Simple baselines for human pose estimation
    and tracking,” in Proceedings of the European Conference on Computer Vision (ECCV),
    pp. 466–481, 2018.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] B. Xiao, H. Wu, 和 Y. Wei， “人类姿态估计和跟踪的简单基线”，发表于《欧洲计算机视觉会议（ECCV）论文集》，第466–481页，2018年。'
- en: '[309] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh, “Convolutional pose
    machines,” in Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 4724–4732, 2016.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] S.-E. Wei, V. Ramakrishna, T. Kanade, 和 Y. Sheikh， “卷积姿态机器”，发表于《IEEE计算机视觉与模式识别会议论文集》，第4724–4732页，2016年。'
- en: '[310] W. Li, Z. Wang, B. Yin, Q. Peng, Y. Du, T. Xiao, G. Yu, H. Lu, Y. Wei,
    and J. Sun, “Rethinking on multi-stage networks for human pose estimation,” arXiv
    preprint arXiv:1901.00148, 2019.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] W. Li, Z. Wang, B. Yin, Q. Peng, Y. Du, T. Xiao, G. Yu, H. Lu, Y. Wei,
    和 J. Sun， “重新思考用于人体姿态估计的多阶段网络”，arXiv预印本 arXiv:1901.00148，2019年。'
- en: '[311] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3d object representations
    for fine-grained categorization,” in The IEEE International Conference on Computer
    Vision (ICCV) Workshops, June 2013.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] J. Krause, M. Stark, J. Deng, 和 L. Fei-Fei， “用于细粒度分类的3D对象表示”，发表于《IEEE计算机视觉国际会议（ICCV）研讨会》，2013年6月。'
- en: '[312] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear cnn models for fine-grained
    visual recognition,” in Proceedings of the IEEE international conference on computer
    vision, pp. 1449–1457, 2015.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] T.-Y. Lin, A. RoyChowdhury, 和 S. Maji， “用于细粒度视觉识别的双线性CNN模型”，发表于《IEEE国际计算机视觉会议论文集》，第1449–1457页，2015年。'
- en: '[313] X. He, Y. Peng, and J. Zhao, “Fine-grained discriminative localization
    via saliency-guided faster r-cnn,” in Proceedings of the 25th ACM international
    conference on Multimedia, pp. 627–635, ACM, 2017.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] X. He, Y. Peng, 和 J. Zhao， “通过显著性引导的快速 R-CNN 实现细粒度的区分性定位”，发表于《第25届ACM国际多媒体会议论文集》，第627–635页，ACM，2017年。'
- en: '[314] X. He, Y. Peng, and J. Zhao, “Fast fine-grained image classification
    via weakly supervised discriminative localization,” IEEE Transactions on Circuits
    and Systems for Video Technology, vol. 29, no. 5, pp. 1394–1407, 2018.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] X. He, Y. Peng, 和 J. Zhao， “通过弱监督的区分性定位实现快速细粒度图像分类”，《IEEE视频技术电路与系统汇刊》，第29卷，第5期，第1394–1407页，2018年。'
- en: '[315] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li, “Novel dataset for
    fine-grained image categorization: Stanford dogs,” in Proc. CVPR Workshop on Fine-Grained
    Visual Categorization (FGVC), vol. 2, 2011.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] A. Khosla, N. Jayadevaprakash, B. Yao, 和 F.-F. Li， “用于细粒度图像分类的新数据集：斯坦福犬”，发表于《CVPR细粒度视觉分类（FGVC）研讨会论文集》，第2卷，2011年。'
- en: '[316] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi, “Fine-grained
    visual classification of aircraft,” arXiv preprint arXiv:1306.5151, 2013.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, 和 A. Vedaldi， “飞机的细粒度视觉分类”，arXiv预印本
    arXiv:1306.5151，2013年。'
- en: '[317] B. Zhao, J. Feng, X. Wu, and S. Yan, “A survey on deep learning-based
    fine-grained object classification and semantic segmentation,” International Journal
    of Automation and Computing, vol. 14, no. 2, pp. 119–135, 2017.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] B. Zhao, J. Feng, X. Wu, 和 S. Yan， “基于深度学习的细粒度对象分类和语义分割综述”，《自动化与计算国际期刊》，第14卷，第2期，第119–135页，2017年。'
