- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:05:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:05:40
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1907.09475] Deep Reinforcement Learning for Clinical Decision Support: A Brief
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1907.09475] 深度强化学习在临床决策支持中的应用：简要综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1907.09475](https://ar5iv.labs.arxiv.org/html/1907.09475)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1907.09475](https://ar5iv.labs.arxiv.org/html/1907.09475)
- en: \theorembodyfont\theoremheaderfont\theorempostheader
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \theorembodyfont\theoremheaderfont\theorempostheader
- en: ': \theoremsep'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ': \theoremsep'
- en: \jmlryear \jmlrworkshop
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \jmlryear \jmlrworkshop
- en: 'Deep Reinforcement Learning for Clinical Decision Support: A Brief Survey'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习在临床决策支持中的应用：简要综述
- en: \NameSiqi Liu \EmailE0272316@u.nus.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \NameSiqi Liu \EmailE0272316@u.nus.edu
- en: \addrNUS Graduate School for Integrative Sciences and Engineering
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \addrNUS 集成科学与工程研究生院
- en: National University of Singapore
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 新加坡国立大学
- en: 21 Lower Kent Ridge Rd    Singapore 119077    \NameKee Yuan Ngiam \Emailkee_yuan_ngiam@nuhs.edu.sg
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 21 Lower Kent Ridge Rd    新加坡 119077    \NameKee Yuan Ngiam \Emailkee_yuan_ngiam@nuhs.edu.sg
- en: \addrDepartment of General Surgery
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \addr普通外科系
- en: National University Hospital
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 国家大学医院
- en: 5 Lower Kent Ridge Rd    Singapore 119074    \NameMengling Feng \Emailephfm@nus.edu.sg
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 5 Lower Kent Ridge Rd    新加坡 119074    \NameMengling Feng \Emailephfm@nus.edu.sg
- en: \addrSaw Swee Hock School of Public Health
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \addrSaw Swee Hock 公共卫生学院
- en: National University of Singapore
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 新加坡国立大学
- en: 12 Science Drive 2    Singapore 117549
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 12 Science Drive 2    新加坡 117549
- en: Abstract
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Owe to the recent advancements in Artificial Intelligence especially deep learning,
    many data-driven decision support systems have been implemented to facilitate
    medical doctors in delivering personalized care. We focus on the deep reinforcement
    learning (DRL) models in this paper. DRL models have demonstrated human-level
    or even superior performance in the tasks of computer vision and game playings,
    such as Go and Atari game. However, the adoption of deep reinforcement learning
    techniques in clinical decision optimization is still rare. We here present the
    first survey that summarizes reinforcement learning algorithms with Deep Neural
    Networks (DNN) on clinical decision support. We also discuss some case studies,
    where different DRL algorithms were applied to address various clinical challenges.
    We further compare and contrast the advantages and limitations of various DRL
    algorithms and present a preliminary guide on how to choose the appropriate DRL
    algorithm for particular clinical applications.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于近期人工智能，尤其是深度学习的进步，许多数据驱动的决策支持系统已被实施，以帮助医生提供个性化护理。本文重点关注深度强化学习（DRL）模型。DRL模型在计算机视觉和游戏玩法（如围棋和Atari游戏）的任务中表现出了人类级别甚至更优的性能。然而，深度强化学习技术在临床决策优化中的应用仍然很少。我们在这里呈现了第一个综述，总结了在临床决策支持中使用深度神经网络（DNN）的强化学习算法。我们还讨论了一些案例研究，其中不同的DRL算法被应用于解决各种临床挑战。我们进一步比较和对比了各种DRL算法的优缺点，并提供了一个初步指南，说明如何为特定的临床应用选择合适的DRL算法。
- en: 1 Introduction
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The effective combination of deep learning (deep neural networks) and reinforcement
    learning technique, named Deep Reinforcement Learning (DRL), is initially invented
    for intelligent game playing(Mnih et al., [2013](#bib.bib40); Silver et al., [2016](#bib.bib53))
    and has later emerged as an effective method to solve complicated control problems
    with large-scale, high-dimensional state and action spaces(Mnih et al., [2015](#bib.bib41)).
    The great applications of deep reinforcement learning to these domains have relied
    on the knowledge of the underlying processes (e.g., the rules of the game).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（深度神经网络）与强化学习技术的有效结合，名为深度强化学习（DRL），最初是为智能游戏玩法而发明的（Mnih et al., [2013](#bib.bib40);
    Silver et al., [2016](#bib.bib53)），后来作为解决具有大规模、高维状态和动作空间的复杂控制问题的有效方法（Mnih et al.,
    [2015](#bib.bib41)）而出现。这些领域中深度强化学习的巨大应用依赖于对基础过程的知识（例如，游戏规则）。
- en: 'In the healthcare domain, the clinical process is very dynamic. The ’rules’
    for making clinical decisions are usually unclear(Marik, [2015](#bib.bib38)).
    For instance, let us consider the following clinical decision-making questions:
    will the patient benefit from the organ transplant; under what condition the transplant
    will become a better option; what are the best medications and dosages to prescribe
    after the transplantation. The decisions from those questions should dedicate
    to the individual patient’s condition. In order to find the optimal decisions
    to those questions, conducting randomized clinical trials (RCTs) are usually the
    choice. However, RCTs can be unpractical and infeasible in certain clinical conditions.
    Therefore, starting from analyzing the observational data becomes an alternative.
    With the improvement of data collection and advancement in DRL technologies, we
    see great potentials in DRL-based decision support system to optimize treatment
    recommendations.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健领域，临床过程非常动态。做出临床决策的‘规则’通常不明确（Marik，[2015](#bib.bib38)）。例如，考虑以下临床决策问题：患者是否会从器官移植中受益；在什么条件下移植将成为更好的选择；移植后最好的药物和剂量是什么。这些问题的决策应依据个体患者的状况。为了找到这些问题的最佳决策，进行随机临床试验（RCTs）通常是选择。然而，在某些临床条件下，RCTs可能是不切实际和不可行的。因此，从分析观察数据开始成为一种替代方案。随着数据收集的改进和DRL技术的发展，我们看到DRL基于的决策支持系统在优化治疗推荐方面具有巨大潜力。
- en: Technical Significance
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术意义
- en: This survey paper summarizes and discusses major types of DRL algorithms that
    have been applied to clinical applications to provide clinical decision support.
    Furthermore, we discuss the trade-offs and assumptions for these DRL algorithms.
    This paper aims to serve as a guideline to assist our audience to pick the appropriate
    DRL models for their particular clinical applications. To the best of our knowledge,
    this is the first survey paper on DRL for treatment recommendation or clinical
    decision support.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查论文总结并讨论了已应用于临床决策支持的主要深度强化学习（DRL）算法类型。此外，我们还讨论了这些DRL算法的权衡和假设。本文旨在作为指南，帮助读者选择适合其特定临床应用的DRL模型。据我们所知，这是第一篇关于治疗推荐或临床决策支持的DRL调查论文。
- en: Clinical Relevance
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 临床相关性
- en: DRL is proven to achieve the human-level capacity for learning complex sequential
    decisions in specific domains, such as video game, board game, and autonomous
    control. While in healthcare, DRL has not been widely applied for clinical applications
    yet. In this paper, we survey major DRL algorithms that provide sequential decision
    support in the clinical domain. We believe that learning from the vast amount
    of collected electronic health record (EHR) data, DRL is capable of extracting
    and summarizing the knowledge and experience needed to optimize the treatments
    for new patients. DRL also has the potential to expand our understanding of the
    current clinical system by automatically exploring various treatment options and
    estimate their possible outcomes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）已被证明能够在特定领域如视频游戏、棋盘游戏和自主控制中实现人类水平的复杂序列决策学习。然而，在医疗保健领域，DRL尚未广泛应用于临床应用。本文对提供临床领域序列决策支持的主要DRL算法进行了调查。我们相信，通过从大量收集的电子健康记录（EHR）数据中学习，DRL能够提取和总结优化新患者治疗所需的知识和经验。DRL还具有通过自动探索各种治疗选项并估计其可能结果，扩展我们对当前临床系统的理解的潜力。
- en: Structure
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结构
- en: This paper surveys the reported cases on the applications of DRL algorithms
    for clinical decision support. In part 2, we first introduce the basic concepts
    of RL and DRL. Then we summarize the main types of DRL algorithms. In part 3,
    we present a few clinical applications that use various DRL algorithms. In part
    4, we discuss how to choose among various DRL algorithms. Finally, in part 5,
    we investigate the challenges and the remedies of using DRL for clinical decision
    support.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本文调查了DRL算法在临床决策支持中的应用案例。在第2部分，我们首先介绍了强化学习（RL）和深度强化学习（DRL）的基本概念。然后我们总结了主要的DRL算法类型。在第3部分，我们介绍了一些使用各种DRL算法的临床应用。在第4部分，我们讨论了如何在各种DRL算法中进行选择。最后，在第5部分，我们研究了使用DRL进行临床决策支持的挑战及其解决办法。
- en: 2 Fundamentals of Reinforcement Learning
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习基础
- en: Reinforcement learning (RL)(Sutton et al., [1998](#bib.bib56)) is a goal-oriented
    learning tool wherein an agent or a decision maker learns a policy to optimize
    a long-term reward by interacting with the environment. At each step, an RL agent
    gets evaluative feedback about the performance of its action, allowing it to improve
    the performance of subsequent actions(Kiumarsi et al., [2018](#bib.bib33)). Mathematically,
    this sequential decision-making process is called the Markov Decision Process
    (MDP)(Howard, [1960](#bib.bib25)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）（Sutton等，[1998](#bib.bib56)）是一种目标导向的学习工具，其中代理或决策者通过与环境互动来学习优化长期奖励的策略。在每一步，RL代理会收到关于其行动表现的评估反馈，从而改进后续行动的表现（Kiumarsi等，[2018](#bib.bib33)）。从数学上讲，这一序列决策过程被称为马尔可夫决策过程（MDP）（Howard，[1960](#bib.bib25)）。
- en: 2.1 MDP Formulation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 MDP公式
- en: 'A Markov decision process is defined by 4 major components:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程由四个主要组件定义：
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A state space $S$: at each time t, the environment is in state $s_{t}\in S$'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 状态空间$S$：在每个时间$t$，环境处于状态$s_{t}\in S$。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'An action space $A$: at each time $t$, the agent takes action $a_{t}\in A$,
    which influence the next state, $s_{t+1}$'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行动空间$A$：在每个时间$t$，代理采取行动$a_{t}\in A$，这会影响下一个状态$s_{t+1}$。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A transition function $P(s_{t+1}\>\lvert\>s_{t},a_{t})$: the probability of
    the next state given the current state and action, which represent an environment
    for the agent to interact.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转移函数$P(s_{t+1}\>\lvert\>s_{t},a_{t})$：在给定当前状态和行动的情况下，下一状态的概率，表示代理与环境互动的方式。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A reward function $r(s_{t},a_{t})\in\mathbb{R}$: the observed feedback given
    the state-action pair $(s_{t},a_{t})$.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励函数$r(s_{t},a_{t})\in\mathbb{R}$：给定状态-行动对$(s_{t},a_{t})$的观察反馈。
- en: In a clinical context, we consider an agent as a clinician. We can think the
    state as the wellbeing/condition of a patient. The state of the patients can depend
    on his demographics (e.g., age, gender, ethnicity, etc.), longitudinal and physiological
    measurements (e.g., lab test, vital signs, medical image reports, etc.) and some
    other clinical features. An action is a treatment that clinicians act to the patient
    (e.g., prescription of certain medication, ordering of surgical procedures, etc.).
    The transition function $P(s_{t+1}\>\lvert\>s_{t},a_{t})$ can be view as the patient’s
    own biological system, that given the current wellbeing and the intervention,
    the patient will enter the next time step $s_{t+1}$. If the wellbeing is improved,
    we assign a reward to the agent, and we penalize the agent if the patient’s condition
    gets worse or stay stagnant after the intervention.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在临床背景下，我们将一个代理视为临床医生。我们可以把状态视为患者的健康/状况。患者的状态可能依赖于其人口统计特征（如年龄、性别、民族等）、纵向和生理测量（如实验室检查、生命体征、医学影像报告等）以及其他一些临床特征。一个行动是临床医生对患者采取的治疗（如某种药物的处方、手术程序的安排等）。转移函数$P(s_{t+1}\>\lvert\>s_{t},a_{t})$可以视为患者自身的生物系统，即在当前健康状况和干预下，患者将进入下一个时间步骤$s_{t+1}$。如果健康状况有所改善，我们会给代理分配奖励；如果患者的状况在干预后变差或停滞不前，我们会惩罚代理。
- en: The objective of the reinforcement learning agent(Sutton and Barto, [2018](#bib.bib55))
    is defined as the expected total reward along with a trajectory $\tau$ that follow
    the distribution $p(\tau)$, i.e., a mapping from state to action $\pi(a\>\lvert\>s)$,
    that maximizes the expected accumulated reward.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习代理的目标（Sutton和Barto，[2018](#bib.bib55)）被定义为沿着轨迹$\tau$的期望总奖励，该轨迹遵循分布$p(\tau)$，即从状态到行动的映射$\pi(a\>\lvert\>s)$，最大化期望的累计奖励。
- en: '|  | $\pi^{*}=arg\max\underbrace{E_{\tau\sim p(\tau)}[\sum\limits_{t}\gamma^{t}r(s_{t},a_{t})]}\limits_{J}$
    |  | (1) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}=arg\max\underbrace{E_{\tau\sim p(\tau)}[\sum\limits_{t}\gamma^{t}r(s_{t},a_{t})]}\limits_{J}$
    |  | (1) |'
- en: where $p(\tau)$ is the probability distribution over state-action trajectories,
    and $\gamma$ is a discounted factor.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$p(\tau)$是状态-行动轨迹上的概率分布，$\gamma$是折扣因子。
- en: The expected long term reward noted as $J$ in equation (1), is the objective
    that we want to maximize w.r.t. a policy $\pi(a\>\lvert\>s)$. The optimal policy
    $\pi^{*}$ is the one that maximizes the objective $J$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程（1）中记作$J$的期望长期奖励是我们希望根据策略$\pi(a\>\lvert\>s)$最大化的目标。最优策略$\pi^{*}$是最大化目标$J$的策略。
- en: To evaluate how ’good or bad’ a state or an action is, we define Q-function
    and value-function in equation (2) and (3) respectively.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估状态或行动的‘好’或‘坏’，我们在方程（2）和（3）中分别定义了Q函数和价值函数。
- en: '|  | $Q^{\pi}(s_{t},a_{t})=\sum_{t^{\prime}=t}^{T}E_{\pi}[r(s_{t^{\prime}},a_{t^{\prime}})\>\lvert\>s_{t},a_{t}]$
    |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{\pi}(s_{t},a_{t})=\sum_{t^{\prime}=t}^{T}E_{\pi}[r(s_{t^{\prime}},a_{t^{\prime}})\>\lvert\>s_{t},a_{t}]$
    |  | (2) |'
- en: '|  | $V^{\pi}(s_{t})=E_{a_{t}\sim\pi(a_{t},s_{t})}[Q^{\pi}(s_{t},a_{t})]$ |  |
    (3) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $V^{\pi}(s_{t})=E_{a_{t}\sim\pi(a_{t},s_{t})}[Q^{\pi}(s_{t},a_{t})]$ |  |
    (3) |'
- en: Q-function $Q^{\pi}(s_{t},a_{t})$ is the conditional expectation of cumulative
    reward given the current state-action pair $(s_{t},a_{t})$, whereas the value
    $V^{\pi}(s_{t})$ is the expectation of Q-value at $(s_{t},a_{t})$ and take action
    $a_{t}$ according to the policy $\pi$.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Q函数 $Q^{\pi}(s_{t},a_{t})$ 是在给定当前状态-动作对 $(s_{t},a_{t})$ 的条件下，累计奖励的期望，而价值 $V^{\pi}(s_{t})$
    是在 $(s_{t},a_{t})$ 处的Q值的期望，并根据策略 $\pi$ 采取动作 $a_{t}$。
- en: If we know the actual Q-function or value-function for all possible states and
    actions in an environment, an optimal policy $\pi^{*}$ will be easy to derive
    from equation (1). An agent at each state $s_{t}$ will pick an action if it maximizes
    the Q-function or the value-function.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道环境中所有可能状态和动作的实际Q函数或价值函数，从方程（1）中推导出一个最优策略 $\pi^{*}$ 将变得很简单。在每个状态 $s_{t}$
    下，智能体将选择一个能最大化Q函数或价值函数的动作。
- en: 2.2 Deep Reinforcement Learning
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 深度强化学习
- en: Deep Reinforcement Learning (DRL) adopts deep neural networks (DNN)(Cireşan
    et al., [2012](#bib.bib11)) to replace tabular representations for $Q(s_{t},a_{t})$,
    $V(s_{t})$ or $\pi$ in traditional RL. For complex environments with a large number
    of states, tabular representations are not feasible due to the large-scale storage
    and high computation requirements (Curse of Dimensionality).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）采用深度神经网络（DNN）（Cireşan et al., [2012](#bib.bib11)）来替代传统RL中的 $Q(s_{t},a_{t})$、$V(s_{t})$
    或 $\pi$ 的表格表示。对于状态数量众多的复杂环境，表格表示由于存储规模大和计算要求高（维度诅咒）而不可行。
- en: Different DRL algorithms use DNN in various ways. For instance, in policy gradient
    RL(Sutton et al., [2000](#bib.bib57)), DNN can be used to optimize a policy $\pi(\theta)$,
    where $\theta$ is the parameters (weights) in DNN. The input to the DNN is the
    current state, and the output is an action. By taking the gradient of DNN, at
    each time step, the parameters $\theta$ in the DNN are updated and thus the policy
    is improved.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的DRL算法以不同方式使用DNN。例如，在策略梯度RL（Sutton et al., [2000](#bib.bib57)）中，DNN可以用来优化策略
    $\pi(\theta)$，其中 $\theta$ 是DNN中的参数（权重）。DNN的输入是当前状态，输出是一个动作。通过对DNN进行梯度计算，在每个时间步，DNN中的参数
    $\theta$ 会被更新，从而改进策略。
- en: In value-based RL(Kaelbling et al., [1996](#bib.bib29)), DNN can be used to
    approximate the value function or the Q-function. For the value function, DNN
    take the current state $(s_{t})$ as the input and the output is the approximated
    value $\hat{V}^{\pi}(s_{t})$ given a policy $\pi$. whereas for Q-function approximation,
    DNN take the state-action pair $(s_{t},a_{t})$ as the input and output is the
    $\hat{Q}_{\phi}(s_{t},a_{t})$ value, where $\phi$ is the parameter in DNN for
    value-based RL. In model-based RL(Doya et al., [2002](#bib.bib13)), DNN is used
    to approximate the transition function $p_{\phi}(s_{t+1}\>\lvert\>s_{t},a_{t})$.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于价值的RL（Kaelbling et al., [1996](#bib.bib29)）中，DNN可以用来逼近价值函数或Q函数。对于价值函数，DNN以当前状态
    $(s_{t})$ 作为输入，输出是给定策略 $\pi$ 的逼近值 $\hat{V}^{\pi}(s_{t})$。而对于Q函数的逼近，DNN以状态-动作对
    $(s_{t},a_{t})$ 作为输入，输出是 $\hat{Q}_{\phi}(s_{t},a_{t})$ 值，其中 $\phi$ 是价值基于RL的DNN中的参数。在基于模型的RL（Doya
    et al., [2002](#bib.bib13)）中，DNN用于逼近转移函数 $p_{\phi}(s_{t+1}\>\lvert\>s_{t},a_{t})$。
- en: We will discuss the architecture for different DRL in details in section 2.4\.
    In order to have a clearer understanding of DRL, let us first discuss the key
    steps in RL algorithms, then we will introduce main types of RL algorithms, including
    policy gradient RL, valued-based RL, actor-critic RL(Grondman et al., [2012](#bib.bib17)),
    model-based RL and some other extensions of RL algorithms. (From this point onward,
    all the RL algorithms we discuss in this paper refer to RL with DNN architecture
    (DRL). We will use the term ’RL’ and ’DRL’ interchangeably, while they both refer
    to Deep RL.)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第2.4节详细讨论不同深度强化学习（DRL）的架构。为了更清楚地理解DRL，首先我们将讨论RL算法中的关键步骤，然后介绍主要的RL算法类型，包括策略梯度RL、基于价值的RL、演员-评论家RL（Grondman
    et al., [2012](#bib.bib17)）、基于模型的RL以及一些其他RL算法的扩展。（从现在开始，我们讨论的所有RL算法均指具有深度神经网络（DNN）架构的RL（DRL）。我们将‘RL’和‘DRL’交替使用，两者都指深度RL。）
- en: 2.3 Key Steps in Reinforcement Learning
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 强化学习中的关键步骤
- en: 'Most RL algorithms can be broken down into 3 steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数强化学习（RL）算法可以分解为3个步骤：
- en: Step 1\. Sample collection
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤1\. 样本收集
- en: Step 2\. Evaluate objective
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2\. 评估目标
- en: Step 3\. Improve policy
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步。改进策略
- en: We take policy gradient RL as an example. In step 1, the RL algorithm interacts
    with the environment and generate sample (state-action pairs) by following an
    unlearned policy (The initialization can be random). The set of state-action pairs
    are sequential and form a trajectory $\tau$. The RL algorithm can generate multiple
    trajectories in step 1\. In the next step, the RL algorithm will try to evaluate
    how good or bad are those collected trajectories by calculating the objective
    function $J$ (cumulative reward). Then in step 3, the RL algorithm will try to
    maximize the objective $J$ by increasing the chance of visiting those highly rewarded
    trajectories and reducing the chance of visiting low rewarded trajectories, and
    this is called policy update. The agent will then go back to step 1 to generate
    new samples according to the update policy and will repeat step 2 and 3 to update
    the policy. In this way, the policy is improved over time by ’trial and error’.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以策略梯度RL为例。在第1步中，RL算法与环境互动，并通过遵循一个未经学习的策略生成样本（状态-动作对）（初始化可以是随机的）。这些状态-动作对是顺序的，形成一个轨迹$\tau$。RL算法可以在第1步中生成多个轨迹。在下一步中，RL算法将尝试通过计算目标函数$J$（累计奖励）来评估这些收集到的轨迹的好坏。然后在第3步中，RL算法将通过增加访问那些高奖励轨迹的机会和减少访问低奖励轨迹的机会来尝试最大化目标$J$，这称为策略更新。然后，代理将返回第1步，根据更新后的策略生成新的样本，并重复第2步和第3步来更新策略。通过这种方式，策略通过‘试错法’随着时间的推移得到改进。
- en: Most RL algorithm follows this three-step structure, but the three steps are
    not equally important in different RL algorithms. So, which RL algorithm works
    the best for an application is a context-dependent question. We will discuss this
    question in more details in section 4\. For now, let us go through the cost in
    the three steps briefly. In step 1, if we collect data samples from a real physical
    system, then data generation would be expensive because the data collection is
    in real time. In contrast, if we use simulators to generate samples, like those
    in Atari games(Mnih et al., [2013](#bib.bib40)), then samples are much easier
    to obtain and step 1 would probably not be a concern in this case.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数RL算法遵循这种三步结构，但这三步在不同的RL算法中并不是同等重要的。因此，哪个RL算法最适合某个应用是一个依赖于上下文的问题。我们将在第4节中更详细地讨论这个问题。目前，让我们简要了解这三步的成本。在第1步，如果我们从真实的物理系统中收集数据样本，那么数据生成将是昂贵的，因为数据收集是实时进行的。相反，如果我们使用模拟器生成样本，比如那些在Atari游戏中（Mnih
    et al., [2013](#bib.bib40)），那么样本更容易获得，在这种情况下，第1步可能不会成为问题。
- en: For step 2, if the application uses policy gradient to improve policy, where
    the policy can be improved by trial and error, then the objective $J$ will be
    the sum of a few rewards from those trials. However, for model-based RL, if the
    model is fitted using DNN, then fitting the model will be expensive that we need
    to make sure the fitted model converges and close to the actual model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第2步，如果应用使用策略梯度来改进策略，其中策略可以通过试错法改进，那么目标$J$将是这些试验中的几个奖励的总和。然而，对于基于模型的RL，如果模型使用DNN进行拟合，那么拟合模型将很昂贵，我们需要确保拟合的模型收敛且接近实际模型。
- en: In step 3, policy gradient RL will only require us to compute the gradient and
    apply the gradient to the policy. It is still relatively simple. However, for
    model-based RL with DNN, we will need to do the backpropagation for a vast number
    of time steps to improve the policy.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步，策略梯度RL只需计算梯度并将其应用于策略，这仍然相对简单。然而，对于使用DNN的基于模型的RL，我们需要对大量时间步骤进行反向传播，以改进策略。
- en: When it comes to real-world applications, especially in clinical applications,
    data generation is often a big concern because it is in real time, and many of
    the observational data (from Electronic Health Records) are private that we do
    not readily have access. We also do not wish to estimate values by trials and
    error, because it is often unethical and costly. Therefore, when we choose the
    RL algorithms, we need to be aware of those constraints.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到现实世界应用，尤其是在临床应用中，数据生成往往是一个大问题，因为它是实时的，许多观察数据（来自电子健康记录）是私密的，我们无法轻易获取。我们也不希望通过试错法来估计值，因为这通常是不道德且成本高昂的。因此，当我们选择RL算法时，需要注意这些限制。
- en: In this paper, we will first focus on discussing the main types of RL algorithms
    that were applied for clinical decision support, while only briefly cover one
    RL algorithm which is rarely applied in clinical applications. We will also discuss
    the underlying reason why specific algorithms are not so "popular" compared to
    others in the healthcare domain.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将首先讨论应用于临床决策支持的主要类型的强化学习（RL）算法，同时仅简要介绍一种在临床应用中很少使用的RL算法。我们还将探讨为何某些特定算法在医疗领域中相比其他算法不那么“流行”的根本原因。
- en: 2.4 Types of Reinforcement Learning Algorithms
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 强化学习算法的类型
- en: Policy Gradient Reinforcement Learning
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 策略梯度强化学习
- en: Under section 2.3, we discussed the three-step structure of general RL algorithm.
    Policy Gradient RL follows this three-step structure. In step 1, the policy gradient
    RL first rolls out a random policy $\pi(\theta)$ to the environment and generate
    trajectories under the policy. The environment is the transition function that
    does not need to specify in this setting. Because we are not interested to learn
    the exact transition function in policy gradient RL. After a few trials, we collect
    a set of trajectories, and we come to step 2 to evaluate the reward $J(\theta)$.
    It can be approximated by samples from trajectories that are collected in step
    1\. $J(\theta)\approx\frac{1}{N}\sum\limits_{i}\sum\limits_{t}r(s_{i,t},a_{i,t})$
    In step 3, we’ll apply gradient to the expected long term reward $J(\theta)$ and
    update the policy $\pi_{\theta}$, where new $\theta^{\prime}$ is set to $\theta+\alpha\nabla_{\theta}J(\theta)$.Then
    we need to sample new state-action pairs from the updated policy $\theta^{\prime}$
    in step 1 and do the optimization of policy step by step. We call this on-policy(Krstic
    et al., [1995](#bib.bib34)), because every time the policy changes, we need to
    sample new data under the new policy. In contrast to on-policy, off-policy applies
    when we can improve the policy without generating new samples from the policy.
    Therefore, policy gradient RL is an on-policy RL algorithm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2.3节中，我们讨论了通用RL算法的三步结构。策略梯度RL遵循这一三步结构。在第一步中，策略梯度RL首先将随机策略$\pi(\theta)$应用于环境中，并在该策略下生成轨迹。环境是一个转移函数，在此设置中无需具体指定。因为我们不希望在策略梯度RL中学习确切的转移函数。经过几次试验后，我们收集了一组轨迹，然后进入第二步评估奖励$J(\theta)$。它可以通过从第一步中收集的轨迹样本来近似。$J(\theta)\approx\frac{1}{N}\sum\limits_{i}\sum\limits_{t}r(s_{i,t},a_{i,t})$
    在第三步中，我们对期望的长期奖励$J(\theta)$应用梯度并更新策略$\pi_{\theta}$，其中新的$\theta^{\prime}$设置为$\theta+\alpha\nabla_{\theta}J(\theta)$。然后，我们需要在第一步中从更新后的策略$\theta^{\prime}$中抽样新的状态-动作对，并逐步进行策略优化。我们称之为on-policy（Krstic等，[1995](#bib.bib34)），因为每次策略发生变化时，我们都需要在新策略下采样新数据。与on-policy相对，off-policy适用于我们可以在不从策略生成新样本的情况下改进策略。因此，策略梯度RL是一种on-policy
    RL算法。
- en: In policy gradient RL, DNN is used to construct a policy in step 3, where the
    input to the DNN is state and output is an action. Figure 1 is the MDP diagram
    of policy gradient RL. By taking a gradient of $J(\theta)$ and update the weights
    in DNN, the policy is learned accordingly. In the clinical context, policy gradient
    RL is not as ’popular’ compared to other RL algorithms. The underlying reason
    for this may lie on the fact that it is an on-policy algorithm that needs to collect
    data iteratively based on a new policy. The algorithm is learned by ’trial and
    error.’ Most clinical applications cannot afford the cost of collecting real-time
    clinical data. For instance, to learn the optimal clinical decisions of medication
    dosage for sepsis patients, it would be unethical to do trial and error and will
    also be time-consuming. However, policy gradient RL is still popular under other
    domains, such as robot control and computer board game, where the environment
    is a simulator that can afford for the trial and error.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略梯度RL中，第3步使用深度神经网络（DNN）构建策略，其中DNN的输入是状态，输出是动作。图1是策略梯度RL的MDP图。通过对$J(\theta)$进行梯度更新DNN中的权重，策略得以相应学习。在临床背景下，策略梯度RL相比其他RL算法不那么“流行”。其根本原因可能在于它是一种on-policy算法，需要基于新策略迭代地收集数据。该算法通过“试错法”进行学习。大多数临床应用无法承受收集实时临床数据的成本。例如，为了学习针对脓毒症患者的最佳药物剂量临床决策，进行试错法既不符合伦理，也非常耗时。然而，策略梯度RL在其他领域仍然很受欢迎，比如机器人控制和计算机棋盘游戏，在这些领域中，环境是可以承担试错法的模拟器。
- en: '![Refer to caption](img/f2b3b736714b33918954289256d373db.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f2b3b736714b33918954289256d373db.png)'
- en: 'Figure 1: MDP for policy gradient RL algorithm'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：策略梯度RL算法的MDP
- en: Value-based Reinforcement Learning
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于值的强化学习
- en: Value-based RL tries to estimate the value function or Q-function for certain
    state and action. The Q-function in equation (2) is also known as bellman equation.
    Value-based RL tries to evaluate either value function $V^{\pi}(s_{t})$ or Q-function
    $Q^{\pi}(s_{t},a_{t})$ from the conditional cumulative reward using DNN in step
    2\. While for value function, the input to the DNN is state $s_{t}$, and output
    is $V^{\pi}(s_{t})$; For Q-function, the input to the DNN is state-action pair
    $(s_{t},a_{t})$, and output is $Q^{\pi}(s_{t},a_{t})$. The DNN is trained to optimize
    the Mean Squared Error (MSE) given as $L(\phi)=\frac{1}{2}\left\lVert\hat{V}_{\phi}^{\pi}(s_{t})-y_{t+1}\right\rVert^{2}$,
    where $y_{t+1}$ is called target value. $y_{t+1}=\max\limits_{a_{t+1}}\gamma Q^{\pi}(s_{t+1},a_{t+1})$
    for value function and $y_{t+1}=r(s_{t},a_{t})+\gamma\max\limits_{a_{t+1}}Q^{\pi}(s_{t+1},a_{t+1})$
    for Q-function. This optimization can be done with various Gradient Descent methods
    in deep learning, such as Stochastic Gradient Descent (SGD)(Bottou, [2010](#bib.bib8)).
    So DNN is implemented in step 2 of the RL algorithm since it’s evaluating the
    objective $J(\theta)$ that is estimated by Q-function or value function. In step
    3, value-based RL update the policy $\pi$ to $\pi^{\prime}$ only if the action
    taken from $\pi^{\prime}$ resulted in the maximum for Q-function.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于值的 RL 尝试估计某一状态和动作的值函数或 Q 函数。方程 (2) 中的 Q 函数也被称为贝尔曼方程。基于值的 RL 尝试使用 DNN 在第 2
    步中从条件累积奖励来评估值函数 $V^{\pi}(s_{t})$ 或 Q 函数 $Q^{\pi}(s_{t},a_{t})$。对于值函数，DNN 的输入是状态
    $s_{t}$，输出是 $V^{\pi}(s_{t})$；对于 Q 函数，DNN 的输入是状态-动作对 $(s_{t},a_{t})$，输出是 $Q^{\pi}(s_{t},a_{t})$。DNN
    的训练目标是优化均方误差 (MSE)，表示为 $L(\phi)=\frac{1}{2}\left\lVert\hat{V}_{\phi}^{\pi}(s_{t})-y_{t+1}\right\rVert^{2}$，其中
    $y_{t+1}$ 被称为目标值。值函数的 $y_{t+1}=\max\limits_{a_{t+1}}\gamma Q^{\pi}(s_{t+1},a_{t+1})$，而
    Q 函数的 $y_{t+1}=r(s_{t},a_{t})+\gamma\max\limits_{a_{t+1}}Q^{\pi}(s_{t+1},a_{t+1})$。这项优化可以使用深度学习中的各种梯度下降方法来完成，例如随机梯度下降
    (SGD)（Bottou, [2010](#bib.bib8)）。因此，DNN 被实现于 RL 算法的第 2 步，因为它评估了由 Q 函数或值函数估计的目标
    $J(\theta)$。在第 3 步中，基于值的 RL 仅在从 $\pi^{\prime}$ 采取的动作导致 Q 函数最大值时更新策略 $\pi$ 为 $\pi^{\prime}$。
- en: '|  | $\pi^{\prime}(a_{t},s_{t})=\begin{cases}1,&amp;\text{if }a_{t}=arg\max_{a_{t}}Q^{\pi}(s_{t},a_{t}),\\
    0,&amp;\text{otherwise}\end{cases}$ |  | (4) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{\prime}(a_{t},s_{t})=\begin{cases}1,&amp;\text{如果 }a_{t}=arg\max_{a_{t}}Q^{\pi}(s_{t},a_{t}),\\
    0,&amp;\text{否则}\end{cases}$ |  | (4) |'
- en: These two algorithms are called Fitted Value Iteration (FVI)(Munos and Szepesvári,
    [2008](#bib.bib44)) and Fitted Q Iteration (FQI)(Riedmiller, [2005](#bib.bib51))
    respectively. A special case for FQI is called Q-learning(Watkins and Dayan, [1992](#bib.bib64)),
    wherein step 1 we only take one tuple of sample $(s_{t},a_{t},s_{t+1},r_{t})$
    and use that for Q-function approximation. Wewe use one gradient step to do parameter
    update, then go back to step 1 to collect one more tuple of the new sample and
    do the optimization iteratively. The term ’Q-learning’ also sometimes refers to
    general value-based RL in some literature.(Arulkumaran et al., [2017](#bib.bib4))
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种算法分别被称为 Fitted Value Iteration (FVI)（Munos 和 Szepesvári, [2008](#bib.bib44)）和
    Fitted Q Iteration (FQI)（Riedmiller, [2005](#bib.bib51)）。FQI 的一个特殊情况被称为 Q-learning（Watkins
    和 Dayan, [1992](#bib.bib64)），在第 1 步中我们仅使用一个样本元组 $(s_{t},a_{t},s_{t+1},r_{t})$
    来进行 Q 函数的近似。我们使用一步梯度更新来更新参数，然后返回第 1 步以收集一个新的样本元组并迭代优化。术语“Q-learning”在某些文献中也有时指一般的基于值的
    RL（Arulkumaran et al., [2017](#bib.bib4)）。
- en: FVI, FQI, and Q-learning work well for off-policy samples and do not have a
    high-variance as in policy gradient RL(Arulkumaran et al., [2017](#bib.bib4)).
    However, the limitation is that they often not converge for non-linear function
    approximation (such as DNN). Besides, the samples collected from Q-learning are
    temporal sequential; thus samples are highly correlated(Arulkumaran et al., [2017](#bib.bib4)).
    Then the network might only find the optimal local solution. Another issue with
    Q-learning is that its target value $y_{t+1}$ is estimated using one tuple sample
    and iterate over multiple one-step estimations. It makes the target value very
    unstable.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: FVI、FQI 和 Q-learning 对于离策略样本表现良好，并且不像策略梯度 RL 那样具有高方差（Arulkumaran et al., [2017](#bib.bib4)）。然而，其限制在于它们通常不能在非线性函数逼近（如
    DNN）中收敛。此外，从 Q-learning 收集的样本是时间序列的，因此样本高度相关（Arulkumaran et al., [2017](#bib.bib4)）。这样网络可能仅找到局部最优解。Q-learning
    的另一个问题是其目标值 $y_{t+1}$ 是通过一个样本元组来估计的，并且需要进行多次一步估计。这使得目标值非常不稳定。
- en: Lin ([1992](#bib.bib36)) mitigated the problem of highly correlated samples
    using a new component named ’replay buffer’ with Q-learning. Q-learning with replay
    buffer sample a batch of tuples $(s_{t},a_{t},s_{t+1},r_{t})$ and use the batch
    to do a one-step gradient for parameter update and go back to the buffer to collect
    another batch of tuples. In this way, samples are no longer correlated, and multiple
    samples in the batch also ensure low gradient variance. To mitigate the problem
    of the unstable target value, ’target network’ is introduced by Mnih et al. ([2016](#bib.bib42)),
    where the target value $y_{t+1}$ is estimated by a fixed value and only gets updated
    after a few iterations of learning. Combining the two components, ’replay buffer’
    and ’target network’ with Q-learning, Mnih et al. ([2015](#bib.bib41)) defined
    a new RL algorithm Deep Q-Network (DQN).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Lin ([1992](#bib.bib36)) 使用一种名为“重放缓冲区”的新组件来缓解样本高度相关的问题，并结合 Q-learning。带有重放缓冲区的
    Q-learning 会从缓冲区中抽取一批元组 $(s_{t},a_{t},s_{t+1},r_{t})$，利用这一批数据进行一次梯度更新参数，然后返回缓冲区收集另一批元组。这样，样本之间不再相关，批次中的多个样本也确保了低梯度方差。为了缓解目标值不稳定的问题，Mnih
    等人 ([2016](#bib.bib42)) 引入了“目标网络”，在这种方法中，目标值 $y_{t+1}$ 由一个固定值估计，只有在经过几次学习迭代后才会更新。将“重放缓冲区”和“目标网络”这两个组件与
    Q-learning 结合，Mnih 等人 ([2015](#bib.bib41)) 定义了一种新的强化学习算法 Deep Q-Network (DQN)。
- en: However, DQN is also not perfect. It has the same limitation as Q-learning that
    it often overestimate Q-function (Thrun and Schwartz, [1993](#bib.bib59)). The
    overestimation is due to ’$\max$’ term in the estimated target value function
    $y_{t+1}=r(s_{t},a_{t})+\gamma\max\limits_{a_{t+1}}Q^{\pi}(s_{t+1},a_{t+1})$,
    which is affected by noise during data collection(van Hasselt, [2011](#bib.bib61)).
    One possible method to mitigate this issue is to use Double Q-Learning (Double
    DQN) where (Hasselt, [2010](#bib.bib20)) implemented two DNNs to learn two Q-functions.
    They used one of them to evaluate the Q-value and let the other one choose the
    action. The goal of having two Q-functions is to de-correlate the noise in both
    action and Q-function.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，DQN 也并非完美。它与 Q-learning 存在相同的限制，即经常高估 Q 函数 (Thrun and Schwartz, [1993](#bib.bib59))。这种高估是由于估计目标值函数
    $y_{t+1}=r(s_{t},a_{t})+\gamma\max\limits_{a_{t+1}}Q^{\pi}(s_{t+1},a_{t+1})$ 中的
    ‘$\max$’ 项，该项在数据收集过程中受噪声影响 (van Hasselt, [2011](#bib.bib61))。一种可能的解决方法是使用 Double
    Q-Learning (Double DQN)，其中 (Hasselt, [2010](#bib.bib20)) 实现了两个 DNN 来学习两个 Q 函数。他们使用其中一个来评估
    Q 值，并让另一个选择动作。使用两个 Q 函数的目标是去相关化动作和 Q 函数中的噪声。
- en: Value-based RL is commonly applied in clinical applications, and we will see
    more examples from section 3 Clinical applications on DRL.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 基于值的强化学习通常应用于临床应用中，我们将在第 3 节“临床应用”中看到更多例子。
- en: Actor-Critic Reinforcement Learning
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Actor-Critic 强化学习
- en: Actor-critic RL(Heess et al., [2015](#bib.bib22)) is a combination of policy
    gradient RL and value-based RL. In step 1, we sample state-action pairs from policy
    just like those on the policy gradient RL, but this time, the state-action pairs
    are generated by a DNN (actor) that act according to the policy $\pi_{\theta}$.
    Then, in step 2, we fit the value $\hat{V}_{\phi}^{\pi}(s_{t})=\sum_{i=t}^{T}E_{\pi_{\theta}}[r(s_{i},a_{i})\>\lvert\>s_{t}]$
    to sampled reward sums using another DNN (critic), the input to DNN is state s,
    and the output is $\hat{V}^{\pi}(s)$. We evaluate a new term named ’advantage’(Baird III,
    [1993](#bib.bib7); Harmon and Baird III, [1996](#bib.bib19)) $\hat{A}^{\pi}(s_{i},a_{i})$,
    which is defined as $\hat{A}^{\pi}(s_{i},a_{i})=r(s_{i},a_{i})+\hat{V}_{\phi}^{\pi}(s^{\prime})-\hat{V}_{\phi}^{\pi}(s)$.
    where $\hat{V}_{\phi}^{\pi}(s^{\prime})$ is the estimated value for the next state
    $s^{\prime}$. Advantage function tells us how much better is that action than
    the average action in the state according to the estimated value function. Then
    in step 3, we take the gradient of the cumulative advantage $\hat{A}^{\pi}(s_{i},a_{i})$
    along the trajectory and update $\theta$ in the actor DNN to learn a better policy.
    The critic DNN can be optimized by supervised regression, $L(\phi)=\frac{1}{2}\left\lVert\hat{V}_{\phi}^{\pi}(s_{t})-y_{t}\right\rVert^{2}$,
    where $y_{t}$ can be estimated by either Monte Carlo estimation of reward along
    the trajectory, or it can be estimated by bootstrap method. The Actor-critic RL
    is an off-policy algorithm, but an alternative version can also be an on-policy
    algorithm. The only difference is in step 1, instead of collecting a batch of
    trajectories, we only collect one trajectory and update the policy to generate
    new samples from the updated policy. Again, the on-policy will not be suitable
    to implement in real-time clinical applications; thus one of the applications
    discussed in this paper(Wang et al., [2018](#bib.bib62)) utilized the off-policy
    actor-critic RL algorithm.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critic RL（Heess 等，[2015](#bib.bib22)）是策略梯度 RL 和基于价值的 RL 的结合。在第一步，我们从策略中采样状态-动作对，就像策略梯度
    RL 中那样，但这次，状态-动作对是由根据策略 $\pi_{\theta}$ 行动的 DNN（actor）生成的。然后，在第二步，我们用另一个 DNN（critic）拟合值
    $\hat{V}_{\phi}^{\pi}(s_{t})=\sum_{i=t}^{T}E_{\pi_{\theta}}[r(s_{i},a_{i})\>\lvert\>s_{t}]$，DNN
    的输入是状态 s，输出是 $\hat{V}^{\pi}(s)$。我们评估一个新的术语’advantage’（Baird III，[1993](#bib.bib7);
    Harmon 和 Baird III，[1996](#bib.bib19)）$\hat{A}^{\pi}(s_{i},a_{i})$，其定义为 $\hat{A}^{\pi}(s_{i},a_{i})=r(s_{i},a_{i})+\hat{V}_{\phi}^{\pi}(s^{\prime})-\hat{V}_{\phi}^{\pi}(s)$，其中
    $\hat{V}_{\phi}^{\pi}(s^{\prime})$ 是下一个状态 $s^{\prime}$ 的估计值。优势函数告诉我们，根据估计的价值函数，该动作比状态中的平均动作好多少。然后，在第三步，我们沿着轨迹计算累积优势
    $\hat{A}^{\pi}(s_{i},a_{i})$ 的梯度，并更新 actor DNN 中的 $\theta$ 以学习更好的策略。critic DNN
    可以通过监督回归优化，$L(\phi)=\frac{1}{2}\left\lVert\hat{V}_{\phi}^{\pi}(s_{t})-y_{t}\right\rVert^{2}$，其中
    $y_{t}$ 可以通过沿轨迹的蒙特卡洛奖励估计，也可以通过自举方法估计。Actor-critic RL 是一种离策略算法，但替代版本也可以是在线策略算法。唯一的区别是在第一步中，不是收集一批轨迹，而是仅收集一个轨迹并更新策略以从更新的策略生成新样本。同样，在线策略不适合在实时临床应用中实现；因此，本文讨论的应用之一（Wang
    等，[2018](#bib.bib62)）利用了离策略 actor-critic RL 算法。
- en: Model-based Reinforcement Learning
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于模型的强化学习
- en: All the above discussed RL algorithms are all model-free RL, and in model-free
    RL we assume we do not know about the exact transition function $p(s_{t+1}\>\lvert\>s_{t},a_{t})$.
    So given the current state and action pair, we do not know what the real next
    state is. The model-free RL does not attempt to learn the transition function
    explicitly but bypass it by sampling from the environment. Knowing the right transition
    function or the environment will always be helpful. Moreover, in certain situations,
    we do know about the transition function, such as a simple board game where we
    design the rules ourselves. For clinical applications, most of the time we are
    not sure about the exact transition function, but we do know a bit about the dynamics
    of the environment. For instance, clinicians generally know that after treating
    a sick patient with appropriate medication dosage, the patient will gradually
    recover from a sick state to a healthy state. Even if we do not know the full
    picture of the environment, we can still propose several models to estimate the
    real transition function (environment) and optimize it from there. It is called
    model-based RL(Doya et al., [2002](#bib.bib13)). There are various models we can
    use to estimate the transition function, such as Gaussian process(GP)(Deisenroth
    and Rasmussen, [2011](#bib.bib12); Rasmussen, [2003](#bib.bib50)), DNN, Gaussian
    mixture models (GMM)(Chernova and Veloso, [2007](#bib.bib10)), and so on. With
    DNN model-based RL, the input to the DNN is state-action pair $(s_{t},a_{t})$
    and output is $s_{t+1}$. The DNN is implemented in step 2 for the transition function.
    In contrast to DNN as the model for the environment, GP is very data efficient.
    GP can produce reasonably decent predictions of the next state using few data
    samples. It is useful in the clinical context since most of the clinical application
    suffers from the data deficiency problem. However, the limitation for GP is that
    it has troubles when the actual transition function is non-smooth. Moreover, GP
    can be slow if the number of samples is vast and in high dimensional space. It
    is the opposite of DNN, wherein DNN larger the number of samples, the more accurate
    prediction in general. So in clinical context when the input state is medical
    images (very high dimensional), DNN will be more suitable for model-based RL than
    GP.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 上述讨论的所有强化学习算法都是无模型强化学习，在无模型强化学习中，我们假设我们不知道确切的转移函数 $p(s_{t+1}\>\lvert\>s_{t},a_{t})$。因此，给定当前的状态和动作对，我们不知道真实的下一个状态是什么。无模型强化学习并不试图显式地学习转移函数，而是通过从环境中采样来绕过它。了解正确的转移函数或环境总是有帮助的。此外，在某些情况下，我们确实知道转移函数，例如在设计我们自己规则的简单棋盘游戏中。在临床应用中，大多数情况下我们不确定确切的转移函数，但我们对环境的动态有一定了解。例如，临床医生通常知道，在用适当药物剂量治疗生病的患者后，患者将逐渐从生病状态恢复到健康状态。即使我们不了解环境的全貌，我们仍然可以提出若干模型来估计真实的转移函数（环境）并从中进行优化。这被称为基于模型的强化学习（Doya
    等， [2002](#bib.bib13)）。我们可以使用各种模型来估计转移函数，例如高斯过程（GP）（Deisenroth 和 Rasmussen， [2011](#bib.bib12)；Rasmussen，
    [2003](#bib.bib50)），深度神经网络（DNN），高斯混合模型（GMM）（Chernova 和 Veloso， [2007](#bib.bib10)）等。使用
    DNN 的基于模型的强化学习中，DNN 的输入是状态-动作对 $(s_{t},a_{t})$，输出是 $s_{t+1}$。DNN 在步骤 2 中用于转移函数。与
    DNN 作为环境模型不同，GP 在数据效率方面非常高。GP 可以使用少量数据样本产生相当不错的下一个状态预测。这在临床背景下非常有用，因为大多数临床应用都面临数据不足的问题。然而，GP
    的限制在于，当实际转移函数不平滑时，它会遇到问题。此外，当样本数量庞大且维度较高时，GP 可能会很慢。与 DNN 相反，DNN 在样本数量增加时，预测通常更准确。因此，在临床背景下，当输入状态是医学图像（非常高维）时，DNN
    将比 GP 更适合基于模型的强化学习。
- en: Other Extensions of Reinforcement Learning
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 强化学习的其他扩展
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hierarchical Reinforcement Learning
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层次化强化学习
- en: When the learning task is massive, and we have seen several different state-action
    spaces in RL with several sub-optimal policies, it is intuitive to sequence the
    sub-space and try to obtain the optimal policy for global space. Hierarchical
    RL(Kulkarni et al., [2016](#bib.bib35)) generally contains a two-level structure.
    The lower level is just like the policies we trained in general RL algorithms
    that try to suggest an action given $(s_{t},a_{t})$. In the same time, there exists
    a higher level network where the ’meta-policy’ trains to select which of these
    lower level policies to apply over a trajectory. Hierarchical RL has the advantage
    of learning faster global optimal policy compared to policies randomly initiated,
    and it transfers the knowledge learned from past tasks from lower level policies.
    In the clinical setting, the state-action space can be huge due to the complex
    behavior of human interactions. Therefore, applying the Hierarchical RL is a very
    natural choice for clinical applications. However, the architecture for Hierarchical
    RL is more complicated to train, and inappropriate transfer learning can result
    in ’negative transfer’(Pan and Yang, [2010](#bib.bib47)) where the final policy
    is not necessarily better than lower level policies.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当学习任务非常庞大，并且我们在RL中看到了几个不同的状态-动作空间以及几个次优策略时，将子空间进行排序并尝试获取全局空间的最佳策略是直观的。层次化RL（Kulkarni
    等，[2016](#bib.bib35)）通常包含一个两层结构。较低层级就像我们在一般RL算法中训练的策略，尝试在给定 $(s_{t},a_{t})$ 的情况下建议一个动作。同时，存在一个较高层级网络，其中“元策略”训练以选择在整个轨迹上应用这些较低层级策略中的哪一个。与随机初始化的策略相比，层次化RL具有更快学习全局最优策略的优势，并且能够将从过去任务中学到的知识从较低层级策略中转移。在临床环境中，由于人际互动的复杂行为，状态-动作空间可能非常庞大。因此，将层次化RL应用于临床应用是一个非常自然的选择。然而，层次化RL的架构更复杂，训练起来更困难，不恰当的迁移学习可能会导致“负迁移”（Pan
    和 Yang，[2010](#bib.bib47)），即最终策略未必比较低层级策略更优。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recurrent Reinforcement Learning
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 递归强化学习
- en: A fundamental limiting assumption of Markov decision processes is the Markov
    property (full observation of MDP), which is rarely satisfied in real-world problems.
    In medical applications, it is unlikely that a patient’s full clinical state will
    be measured. It is known as a Partially Observable Markov Decision Process (POMDP)
    problem(Kaelbling et al., [1998](#bib.bib30)). A POMDP has a 4-tuple $(S,A,R,O)$,
    where $O$ are observations. Classic DQN is only useful if the observations reflect
    the underlying state.(Hausknecht and Stone, [2015](#bib.bib21)). Hausknecht and
    Stone ([2015](#bib.bib21)) proposed an extension to DQN network to deal with POMDP
    problem, where the first fully connected layer of DQN is replaced with Long Short
    Term Memory (LSTM)(Hochreiter and Schmidhuber, [1997](#bib.bib23)). This new RL
    algorithm is called Deep Recurrent Q-Network (DRQN)(Hausknecht and Stone, [2015](#bib.bib21)).
    Their algorithm showed that it could integrate information successfully through
    time and could replicate DQN’s performance on standard Atari games with a setting
    of POMDP for the game screen.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程的一个基本限制假设是马尔可夫性质（MDP的完全观测），这一点在实际问题中很少得到满足。在医学应用中，患者的完整临床状态不太可能被测量。这被称为部分可观测马尔可夫决策过程（POMDP）问题（Kaelbling
    等，[1998](#bib.bib30)）。一个POMDP具有一个4元组 $(S,A,R,O)$，其中 $O$ 是观测值。经典DQN只有在观测值能够反映潜在状态时才有用。（Hausknecht
    和 Stone，[2015](#bib.bib21)）。Hausknecht 和 Stone ([2015](#bib.bib21)) 提出了对DQN网络的扩展，以解决POMDP问题，其中DQN的第一个全连接层被长短期记忆（LSTM）（Hochreiter
    和 Schmidhuber，[1997](#bib.bib23)）替代。这种新的RL算法称为深度递归Q网络（DRQN）（Hausknecht 和 Stone，[2015](#bib.bib21)）。他们的算法表明，DRQN能够成功地通过时间整合信息，并能够在POMDP设置下复制DQN在标准Atari游戏中的表现。
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inverse Reinforcement Learning
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 逆强化学习
- en: To learn most of the standard RL algorithm in clinical applications, we would
    design the reward function by hand, but we do not know what the real reward is.
    This reward design is very vulnerable to if it is misspecified. Inverse RL is
    an algorithm that we can infer the correct reward function from expert demonstrations
    without having to program the reward by hand.(Ghavamzadeh et al., [2015](#bib.bib15);
    Abbeel and Ng, [2004](#bib.bib1); Ng et al., [2000](#bib.bib46)) Alternative to
    Inverse RL is to learn them directly from the behaviors from experts, and this
    often refers to imitation learning(Schaal, [1999](#bib.bib52)). However, one limitation
    for imitation learning is that the experts may have different capabilities and
    prone to be imperfect(Wang et al., [2018](#bib.bib62)); learning from experts
    may only result in sub-optimal policies. So generally in Inverse RL, we are given
    state-action space and a sub-optimal policy in a dynamic model. The goal of Inverse
    RL is to recover the right reward function. Then we can use the learned reward
    function to get a new policy which is better than the sub-optimal policies. The
    reward can be learned by DNN where the input is state-action pairs $(s,a)$ produced
    by a sub-optimal policy $\pi^{\#}$, then the output of the DNN is a reward $r_{\Phi}(s,a)$,
    where $\Phi$ is the parameters to the DNN that we would learn through backpropagation.
    Later after we obtained $r_{\Phi}(s,a)$, we can use the new reward function to
    plan for better policy and hopefully the optimal policy $\pi^{*}$. In the clinical
    context, it is important to reason about what the clinicians are trying to achieve
    and what they think is essential.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了学习大多数标准RL算法在临床应用中的用法，我们通常需要手动设计奖励函数，但我们不知道真正的奖励是什么。这种奖励设计非常脆弱，如果设计错误则会受到影响。逆向RL是一种可以从专家演示中推断正确奖励函数的算法，无需手动编程奖励（Ghavamzadeh
    et al., [2015](#bib.bib15); Abbeel and Ng, [2004](#bib.bib1); Ng et al., [2000](#bib.bib46)）。与逆向RL的替代方法是直接从专家的行为中学习，这通常称为模仿学习（Schaal,
    [1999](#bib.bib52)）。然而，模仿学习的一个限制是专家可能具有不同的能力，并且可能不完美（Wang et al., [2018](#bib.bib62)）；从专家那里学习可能只会导致次优策略。因此，在逆向RL中，我们通常给定状态-动作空间和动态模型中的次优策略。逆向RL的目标是恢复正确的奖励函数。然后，我们可以使用学习到的奖励函数来获得比次优策略更好的新策略。奖励可以通过DNN进行学习，其中输入是由次优策略$\pi^{\#}$生成的状态-动作对$(s,a)$，然后DNN的输出是奖励$r_{\Phi}(s,a)$，其中$\Phi$是通过反向传播学习的DNN参数。之后，我们获得$r_{\Phi}(s,a)$后，可以使用新的奖励函数来规划更好的策略，希望得到最终的最优策略$\pi^{*}$。在临床背景下，理解临床医生试图实现的目标和他们认为重要的内容是至关重要的。
- en: 3 Deep Reinforcement Learning for Clinical Applications
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 临床应用中的深度强化学习
- en: Recent research(Prasad et al., [2017](#bib.bib48); Nemati et al., [2016](#bib.bib45);
    Tseng et al., [2017](#bib.bib60)) have demonstrated that DRL can be used to provide
    treatment recommendation and clinical decision support for various applications
    with different types of data sources, ranging from EHR, online disease database,
    and genetic data, and so on. The applications include medication/fluid choice,
    dosage for patients with acute or chronic disease, settings and duration of mechanical
    ventilation, and constructing clinical motifs from clinical notes. Table 1 summarizes
    all the clinical application papers discussed in this survey, in particular highlighting
    the distinct sub-type of DRL methods and different clinical problems. As discussed
    in the earlier section, the policy gradient RL is an on-line algorithm that does
    not fit with most clinical applications. Therefore, value-based RL algorithms
    were popular in clinical applications.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究（Prasad et al., [2017](#bib.bib48); Nemati et al., [2016](#bib.bib45);
    Tseng et al., [2017](#bib.bib60)）已证明，DRL可以用于为各种应用提供治疗建议和临床决策支持，这些应用涉及不同类型的数据来源，包括电子健康记录、在线疾病数据库和基因数据等。应用包括药物/液体选择、急性或慢性病患者的剂量、机械通气的设置和持续时间，以及从临床记录中构建临床模式。表1总结了本调查中讨论的所有临床应用论文，特别强调了DRL方法的不同子类型和不同的临床问题。如前面所讨论的，策略梯度RL是一种在线算法，不适合大多数临床应用。因此，基于价值的RL算法在临床应用中更为流行。
- en: For each application in this section, we will investigate what RL algorithms
    the author used, the formulation of context-based MDP $(S,A,R)$, and finally the
    performance of the RL. We first discuss the applications that used value-based
    RL, followed by actor-critic RL, model-based RL, hierarchical RL, recurrent RL
    and finally inverse RL. For value-based RL, we further divide the application
    based on the sub-types of value-based RL algorithms, which contain Fitted Q Iteration,
    DQN, and double DQN.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中的每个应用中，我们将调查作者使用了哪些强化学习（RL）算法、基于上下文的马尔可夫决策过程（MDP） $(S,A,R)$ 的构建方式，以及最终的RL性能。我们首先讨论使用基于价值的RL的应用，其次是演员-评论员RL、基于模型的RL、层次RL、递归RL，最后是逆向RL。对于基于价值的RL，我们进一步根据基于价值的RL算法的子类型进行分类，这些子类型包括Fitted
    Q Iteration、DQN和双重DQN。
- en: 3.1 Value-based Reinforcement Learning
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于价值的强化学习
- en: Fitted Q Iteration
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Fitted Q Iteration
- en: Weaning of Mechanical Ventilation
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 机械通气的撤离
- en: Weaning patients in the intensive care unit from mechanical ventilation (MV)
    is often haphazard and inefficient. With this regard, Prasad et al. ([2017](#bib.bib48))
    used an off-policy Fitted Q Iteration (FQI) algorithms to determine ICU strategies
    for the MV administration, they aimed to develop a decision support tool that
    could leverage available patient information in the data-rich ICU setting to alert
    clinicians when a patient was ready for initiation of weaning, and to recommend
    a personalized treatment protocol. They used MIMIC-III database to extract 8860
    admissions from 8182 unique adult patients undergoing invasive ventilation for
    more than 24 hours.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在重症监护病房（ICU）中从机械通气（MV）撤离患者通常是随意且效率低下的。在这方面，Prasad等人（[2017](#bib.bib48)）使用了一个离线Fitted
    Q Iteration (FQI) 算法来确定MV管理的ICU策略，他们的目标是开发一个决策支持工具，可以利用数据丰富的ICU环境中的可用患者信息，提醒临床医生患者何时准备好开始撤离，并推荐个性化的治疗方案。他们使用了MIMIC-III数据库，提取了8182名独特成人患者中进行超过24小时侵入性通气的8860次入院记录。
- en: They included features like patients’ demographic characteristics, pre-existing
    conditions, co-morbidities and time-varying vital signs. They preprocessed the
    lab measurements and vitals using Gaussian Process (GP) to impute missing values;
    this could ensure more precise policy estimation. The state $s_{t}$ was a 32-dimensional
    feature vector. The action was designed as a 2-dimensional vector with the 1st
    dimension of on/off MV and the 2nd dimension for four different levels of dosage
    for sedation. Reward $r_{t}$ with each state encompassed both time-into-ventilation
    and physiological stability, whereas reward would increase with stable vitals
    and successful extubation, but would penalize on adverse events (reintubation)
    and additional ventilator hour. Comparing to the actual policy implemented at
    the Hospital of University of Pennsylvania, HUP, their learned policy matched
    85% of actual policy.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 他们包括了患者的人口统计特征、既往病史、共病情况和随时间变化的生命体征。他们使用高斯过程（GP）对实验室测量数据和生命体征进行预处理，以填补缺失值；这可以确保更精确的政策估计。状态
    $s_{t}$ 是一个32维的特征向量。动作被设计为一个二维向量，其中第一个维度表示开/关机械通气，第二个维度表示四种不同的镇静剂量级别。每个状态的奖励 $r_{t}$
    包括了通气时间和生理稳定性，奖励会因生命体征稳定和成功拔管而增加，但会因不良事件（如再插管）和额外的通气小时而受到惩罚。与宾夕法尼亚大学医院（HUP）实施的实际政策相比，他们学习的政策与实际政策吻合了85%。
- en: Optimal Heparin Dosing in ICU
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ICU中的最佳肝素剂量
- en: Mismanagement of certain drugs can drive up costs by unnecessarily extending
    hospital length of stay, and place patients at risk. Unfractionated Heparin(UH)
    is one example of such drugs that overdosing leads to increased risk of bleeding
    and underdosing leads to increased risk of clot formation. RL is particularly
    well-suited for the medication dosing problem given the sequential nature of clinical
    treatment. Nemati et al. ([2016](#bib.bib45)) trained an RL medication dosing
    agent to learn a dosing policy that maximized the overall fraction of time given
    patient stays within his/her therapeutic activated partial thromboplastin time
    (aPTT). They used the MIMIC-II database and extracted 4470 patients who received
    an intravenous heparin infusion at some point during their ICU stay with a time
    window of 48 hours. Variables included demographics, lab measurements and severity
    scores(Glasgow Coma Score(GCS), daily Sequential Organ Failure Assessment(SOFA)score).
    A state was constructed using the features and estimated by discriminative hidden
    Markov model (DHMM)(Kapadia, [1998](#bib.bib32)). Whereas the discrete action
    was using discretized heparin values from six quantile intervals, and the reward
    was designed according to the aPPT feedback. Given the feedback, a decision was
    made to increase, decrease or maintain the heparin dosage until the next aPTT
    measure. $r_{t}=\frac{2}{1+e^{-(aPTT_{t}-60)}}-\frac{2}{1+e^{-(aPTT_{t}-100)}}-1$.
    This function assigned a maximal reward of one when a patient’s aPTT value was
    within the therapeutic window which rapidly diminished towards a minimal reward
    of -1 as the distance from the therapeutic window increases.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 不当使用某些药物可能会因不必要地延长住院时间而增加成本，并使患者面临风险。未分级肝素（UH）就是一个例子，过量使用会增加出血风险，而用量不足则会增加血栓形成的风险。鉴于临床治疗的顺序特性，RL特别适合药物剂量问题。Nemati等人（[2016](#bib.bib45)）训练了一个RL药物剂量代理，以学习一种剂量策略，最大化患者在其治疗性活化部分凝血酮时间（aPTT）范围内的整体时间比例。他们使用了MIMIC-II数据库，提取了4470名在ICU住院期间某个时点接受静脉肝素输注的患者，时间窗口为48小时。变量包括人口统计学信息、实验室测量和严重程度评分（格拉斯哥昏迷评分（GCS）、每日顺序器官衰竭评估（SOFA）评分）。使用特征构建了状态，并由判别隐马尔可夫模型（DHMM）（Kapadia，[1998](#bib.bib32)）进行估计。离散动作是使用来自六个分位区间的离散肝素值，奖励则根据aPPT反馈设计。根据反馈，决定增加、减少或维持肝素剂量，直到下次aPTT测量。
    $r_{t}=\frac{2}{1+e^{-(aPTT_{t}-60)}}-\frac{2}{1+e^{-(aPTT_{t}-100)}}-1$。这个函数在患者的aPTT值在治疗窗口内时分配了最大奖励1，随着距离治疗窗口的增加，奖励迅速减少到最小值-1。
- en: The performance of the optimal policy was tested by comparing the accumulated
    reward from a clinician and the trained RL agent. On average and consistently
    over time, the RL algorithm resulted in the best long-term performance by following
    the recommendations of the agent.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略的性能通过比较临床医生和训练的RL代理的累计奖励来测试。平均而言，并且随着时间的推移，RL算法通过遵循代理的建议获得了最佳的长期表现。
- en: Deep Q Network
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q网络
- en: Extract Clinical Concepts from Free Clinical Text
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从自由临床文本中提取临床概念
- en: 'Extracting relevant clinical concepts from a free clinical text is a critical
    first step for diagnosis inference. Ling et al. ([2017](#bib.bib37)) proposed
    to use DQN to learns clinical concepts from external evidence (Wikipedia: Signs
    and Symptoms Section and MayoClinic: Symptoms section). They used TREC CDS dataset(Simpson
    et al., [2014](#bib.bib54)) to conducted their experiments. This dataset contains
    30 topics, where each topic is a medical free-text that described a patient’s
    situation with a diagnosis. MetaMap(Aronson, [2006](#bib.bib3)) extracted clinical
    concepts from TREC CD5 and external articles. The state contained two vectors:
    current clinical concepts, as well as candidate concepts from external articles:
    the more similar between the two vectors, the higher state value, would be. The
    action space was discrete that included action to accept or reject candidate concepts.
    The reward was designed in a way such that it was high when candidate concepts
    were more relevant than current concepts for a patient’s diagnosis. DQN was trained
    to optimize the reward function that measured the accuracy of the candidate clinical
    concepts. Their preliminary experiments on the TREC CDS dataset demonstrated the
    effectiveness of DQN over various non-reinforcement learning based baselines.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从自由临床文本中提取相关的临床概念是诊断推理的关键第一步。Ling 等人 ([2017](#bib.bib37)) 提出使用 DQN 从外部证据中学习临床概念（维基百科：体征和症状部分和MayoClinic：症状部分）。他们使用了
    TREC CDS 数据集（Simpson 等，[2014](#bib.bib54)）来进行实验。该数据集包含30个主题，每个主题都是描述患者情况和诊断的医学自由文本。MetaMap(Aronson，[2006](#bib.bib3))
    从 TREC CD5 和外部文章中提取临床概念。状态包含两个向量：当前的临床概念以及来自外部文章的候选概念：两个向量之间的相似性越高，状态值越高。动作空间是离散的，包括接受或拒绝候选概念的动作。奖励设计的方式是当候选概念比当前概念对于患者的诊断更相关时，奖励较高。DQN
    被训练来优化奖励函数，该函数度量候选临床概念的准确性。他们在 TREC CDS 数据集上的初步实验证明了 DQN 在各种非强化学习基线上的有效性。
- en: Symptom Checking 1.0
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 症状检查 1.0
- en: 'To facilitate self-diagnosis, Tang et al. ([2016](#bib.bib58)) proposed symptom
    checking systems. A symptom checker first asked a sequence of questions regarding
    the condition of a patient. Patients would then provide a sequence of answers
    according to the questions. Then the symptom checker tried to make a diagnosis
    based on the Q&A. Tang et al. proposed an RL based ensemble model to train this
    symptom checker. They implemented 11 DQN models, while each model represented
    an anatomical part of a human body, such as head, neck, arm, etc. Those model
    were trained independently of each other. For each anatomical model, the state
    s was one-hot encoded based on a symptom. (i.e., if the symptom were a headache,
    then only the anatomical model representing head would have $s=1$, while other
    models would have $s=0$). The action was discrete and had two types: inquiry and
    diagnosis. If the maximum Q-value corresponded to the action of inquiry, the symptom
    checker would continue to ask the next question. If the maximum Q-value corresponded
    to the action of diagnosis. The symptom checker would give a diagnosis and terminate.
    The reward was designed as a scalar. The agent would receive the reward when it
    could correctly predict the disease with a limited number of inquiries. They applied
    the DQN algorithms on a simulated disease dataset, and the result showed that
    symptom checker could imitate the behavior of inquiry and diagnosis as those performed
    by doctors.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便自诊断，Tang 等人 ([2016](#bib.bib58)) 提出了症状检查系统。症状检查程序首先询问有关患者状况的一系列问题。然后，患者会根据问题提供一系列答案。然后，症状检查程序尝试根据问答结果进行诊断。Tang
    等人提出了一个基于 RL 的集成模型来训练这个症状检查程序。他们实现了11个 DQN 模型，每个模型代表人体的一个解剖部位，如头部、颈部、手臂等。这些模型彼此独立地进行训练。对于每个解剖模型，状态
    s 被根据症状进行 one-hot 编码（即，如果症状是头痛，则只有表示头部的解剖模型具有 $s=1$，其他模型具有 $s=0$）。动作是离散的，有两种类型：查询和诊断。如果最大的
    Q 值对应于查询动作，症状检查程序将继续询问下一个问题。如果最大的 Q 值对应于诊断动作，则症状检查程序将给出一个诊断并终止。奖励被设计为一个标量。当代理能够在有限的查询次数内正确预测出疾病时，代理将获得奖励。他们在一个模拟疾病数据集上应用了
    DQN 算法，结果显示症状检查程序可以模仿医生进行询问和诊断的行为。
- en: Double DQN
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Double DQN
- en: Sepsis Treatment 1.0
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 脓毒症治疗 1.0
- en: Raghu et al. ([2017](#bib.bib49)). was one of the first ones to directly discuss
    the application of DRL to healthcare problems. They used the Sepsis subset of
    the MIMIC-III dataset and chose to define the action space as consisting of Vasopressors
    and IV fluid. They grouped drugs doses into four bins consisting of varying amounts
    of each drug. Q-values were frequently overestimated in practice, leading to incorrect
    predictions and poor policies. Thus the authors solved this problem with a Double
    DQN(Wang et al., [2015](#bib.bib63)). They also employed dueling deep Q Network
    to separate value and advantage streams, where the value represented the quality
    of the current state, and the advantage represented the quality of the chosen
    action. The reward function was clinically motivated based on the SOFA score which
    measures organ failure. They demonstrated that using continuous state space modeling,
    the found policies could reduce patient mortality in the hospital by 1.8% to 3.6%.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Raghu 等人 ([2017](#bib.bib49)) 是最早直接讨论将深度强化学习（DRL）应用于医疗保健问题的研究之一。他们使用了 MIMIC-III
    数据集的脓毒症子集，并选择将动作空间定义为包括血管收缩药和静脉液体。他们将药物剂量分为四个不同的区间，每个区间包含不同数量的药物。在实际应用中，Q 值经常被高估，导致预测不准确和策略不佳。因此，作者通过双重
    DQN（Wang 等人，[2015](#bib.bib63)）解决了这一问题。他们还采用了对决深度 Q 网络，以将价值流和优势流分开，其中价值表示当前状态的质量，优势表示所选动作的质量。奖励函数是以
    SOFA 评分为临床动机的，该评分用于衡量器官衰竭。他们证明，通过使用连续状态空间建模，所找到的策略能够将医院患者的死亡率降低 1.8% 到 3.6%。
- en: 3.2 Actor-critic Reinforcement Learning
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 行为者-评论家强化学习
- en: Optimal medical prescription in ICU
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ICU 中的最佳医疗处方
- en: 'Wang et al. ([2018](#bib.bib62)) adopted the Actor-critic RL algorithms to
    find optimal medical prescriptions to patients with various diseases. They experimented
    with MIMIC-III database and extracted 22,865 admissions. Features used for state
    construction included demographics, vitals, lab results, etc. Action space was
    180 distinct ATC codes. Wang et al. not just implemented classic actor-critic
    RL. Instead, they combined both RL with Supervised learning (SL) in the actor-network.
    The objective function $J(\theta)$ was evaluated as the linear combination of
    objective function for both RL and SL in the equation: $J(\theta)=(1-\epsilon)J_{RL}(\theta)+\epsilon(-J_{SL}(\theta))$,
    where $\epsilon$ was a hyperparameters that ranges from 0 to 1, and it used to
    balance the RL and SL. $J_{RL}(\theta)$ was objective in actor-network, while
    $J_{SL}(\theta)$ was evaluated as the cross-entropy loss between the predicted
    treatment and prescriptions given by the doctor.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 ([2018](#bib.bib62)) 采用了行为者-评论家强化学习算法，以寻找针对各种疾病患者的最佳医疗处方。他们在 MIMIC-III
    数据库上进行了实验，提取了 22,865 次住院记录。用于状态构建的特征包括人口统计数据、生命体征、实验室结果等。动作空间为 180 个不同的 ATC 代码。Wang
    等人不仅仅实现了经典的行为者-评论家强化学习。相反，他们将强化学习（RL）与监督学习（SL）相结合，应用于行为者网络。目标函数 $J(\theta)$ 被评估为
    RL 和 SL 目标函数的线性组合，公式为：$J(\theta)=(1-\epsilon)J_{RL}(\theta)+\epsilon(-J_{SL}(\theta))$，其中
    $\epsilon$ 是一个范围从 0 到 1 的超参数，用于平衡 RL 和 SL。$J_{RL}(\theta)$ 是行为者网络中的目标函数，而 $J_{SL}(\theta)$
    被评估为预测治疗与医生给出的处方之间的交叉熵损失。
- en: They applied gradient ascent to the objective function w.r.t. $\theta$ in the
    actor-network, and tried different $\epsilon$ value for RL-SL balancing. Besides,
    they also incorporated an LSTM network to improve performance in the partial observed
    MDP (POMDP). The state $s$ was replaced by summarizing the entire historical observations
    with $c_{t}=f(o_{1},o_{2},\dots,o_{t})$ and $c_{t}$ was used as state for both
    actor and critic networks. Their experiments showed that the proposed network
    could detect good medication treatment automatically.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 他们对行为者网络中的目标函数关于 $\theta$ 进行了梯度上升，并尝试了不同的 $\epsilon$ 值以平衡 RL 和 SL。此外，他们还引入了
    LSTM 网络，以改善部分观测的 MDP（POMDP）中的性能。状态 $s$ 被替换为通过 $c_{t}=f(o_{1},o_{2},\dots,o_{t})$
    总结整个历史观察，$c_{t}$ 被用作行为者和评论家网络的状态。他们的实验表明，所提出的网络可以自动检测出良好的药物治疗。
- en: 3.3 Model-based Reinforcement Learning
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于模型的强化学习
- en: Radiation dose fraction for lung cancer
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 肺癌的放射剂量分割
- en: Tseng et al. ([2017](#bib.bib60)) implemented model-based RL to train a dose
    escalation policy for patients received radiotherapy with lung cancer. They included
    114 patients in the RL design and first trained a DNN to estimate the transition
    function $p(s_{t+1}\>\lvert\>s_{t},a_{t})$. The loss function for the DNN model
    aimed to minimize the difference between the expectation of Q-values from the
    estimated trajectory with the observed values. After the construction of the transition
    function, Tseng et al. applied DQN to learn an optimal policy for the dose in
    thoracic irradiation that trade-off between the local control (LC) and the risk
    for radiation-induced pneumonitis (RP). The reward for the network was designed
    as a trade-off between encouraging improved LC and attempting to suppress RP.
    State of DQN was defined as a combination of 9 features, including cytokines,
    PET radiomics, and dose features. The action was designed as the dose per fraction.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Tseng 等人 ([2017](#bib.bib60)) 实施了基于模型的强化学习（RL）来训练用于接受肺癌放疗的患者的剂量递增策略。他们在 RL 设计中包括了
    114 名患者，并首先训练了一个深度神经网络（DNN）来估计过渡函数 $p(s_{t+1}\>\lvert\>s_{t},a_{t})$。DNN 模型的损失函数旨在最小化从估计轨迹中得到的
    Q 值期望与观察值之间的差异。在过渡函数构建完成后，Tseng 等人应用了深度 Q 网络（DQN）来学习一种最优策略，用于在胸部照射中权衡局部控制（LC）和放射性肺炎（RP）的风险。网络的奖励设计为在鼓励改进
    LC 和抑制 RP 之间进行权衡。DQN 的状态定义为 9 个特征的组合，包括细胞因子、PET 放射组学和剂量特征。动作被设计为每次分割的剂量。
- en: Since the construction of the transition function would require a large amount
    of data. The authors implemented drop-off in the DNN in transition function to
    avoid overfitting. Besides, the authors implemented a Generative Adversarial Networks
    (GAN)(Goodfellow et al., [2014](#bib.bib16)) to simulate more data to mitigate
    the data deficiency problem. The simulated data from GAN was also fed to the transition
    function DNN to training. The proposed (Model-based RL) network showed a promising
    result that it could suggest similar treatment dose compared with clinicians.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于构建过渡函数需要大量数据，作者在 DNN 中实施了丢弃技术以避免过拟合。此外，作者还实现了生成对抗网络（GAN）（Goodfellow 等人，[2014](#bib.bib16)）来模拟更多数据以缓解数据不足的问题。来自
    GAN 的模拟数据也被输入到过渡函数 DNN 进行训练。所提出的（基于模型的 RL）网络显示出有希望的结果，能够建议与临床医生类似的治疗剂量。
- en: 3.4 Hierarchical Reinforcement Learning
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 层次化强化学习
- en: Symptom Checking 2.0
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 症状检查 2.0
- en: The main idea from Kao et al. ([2018](#bib.bib31))’s work was to imitate a group
    of doctors with different expertise who jointly diagnose a patient. Since a patient
    could only accept an inquiry from one doctor at a time, a meta-policy was required
    to appoint doctors, in turn, to inquire to the patient. The meta-policy came from
    a higher level network. At each step, the meta-policy was responsible for appointing
    an anatomical-part model to perform a symptom inquiry for a disease prediction.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Kao 等人 ([2018](#bib.bib31)) 的研究主要思想是模仿一组具有不同专长的医生共同诊断患者。由于患者每次只能接受一个医生的询问，因此需要一个元策略来轮流指派医生向患者进行询问。这个元策略来自于一个更高层级的网络。在每一步中，元策略负责指派一个解剖部位模型进行症状询问以预测疾病。
- en: In Kao et al’s paper, the first hierarchy level was a master agent M. The master
    M possesses its action space $A_{M}$ and policy $\pi_{M}$. In this level, the
    action space $A_{M}$ equaled $P$, the set of anatomical parts. At step t, the
    master agent entered state $s_{t}$, and it picked an action $a_{M_{t}}$ from $A_{M}$
    according to its policy $\pi_{M}$. The second level of hierarchy consists of anatomical
    models $m_{p}$. If the master acted $a^{M}$, the task would be delegated to the
    anatomical model $m_{p}=m_{a^{M}}$. Once the model $m_{p}$ was selected, the actual
    action $a_{t}\in A$ was then performed according to the policy of $m_{p}$, denoted
    as $\pi_{m_{p}}$.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kao 等人的论文中，第一级层次是一个主代理 M。主代理 M 拥有其行动空间 $A_{M}$ 和策略 $\pi_{M}$。在这一层级中，行动空间 $A_{M}$
    等于解剖部位集合 $P$。在第 t 步，主代理进入状态 $s_{t}$，并根据其策略 $\pi_{M}$ 从 $A_{M}$ 中选择一个行动 $a_{M_{t}}$。第二层级由解剖模型
    $m_{p}$ 组成。如果主代理执行了 $a^{M}$，任务将委托给解剖模型 $m_{p}=m_{a^{M}}$。一旦选择了模型 $m_{p}$，则根据模型
    $m_{p}$ 的策略 $\pi_{m_{p}}$ 执行实际行动 $a_{t}\in A$。
- en: Based on this structure, Kao et al. trained an online symptom checkers on simulated
    data from SymCAT symptom disease database for self-diagnosis of health-related
    ailments. It was the improved version of Symptom Checking 1.0, where Kao et al.
    added another layer of DQN on top of anatomical models in Symptom Checking 1.0
    as a master agent. Both the anatomical models and the master model applied DQN
    to pick the action that maximized the Q-value. Their result showed that the proposed
    Hierarchical RL algorithm significantly improved the accuracy of symptom checking
    over traditional systems.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一结构，Kao 等人训练了一个在线症状检查器，使用来自 SymCAT 症状疾病数据库的模拟数据进行健康相关疾病的自我诊断。这是 Symptom Checking
    1.0 的改进版本，其中 Kao 等人在 Symptom Checking 1.0 中的解剖模型之上添加了另一个 DQN 层作为主智能体。解剖模型和主模型都使用
    DQN 来选择最大化 Q 值的动作。他们的结果显示，提出的层次强化学习算法显著提高了症状检查的准确性，相较于传统系统。
- en: 3.5 Recurrent Reinforcement Learning
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 递归强化学习
- en: Sepsis Treatment 2.0
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 脓毒症治疗 2.0
- en: 'Futoma et al. ([2018](#bib.bib14)) proposed a new extension to DRQN architecture
    with a multi-output Gaussian process to train an agent to learn the optimal treatment
    for sepsis patients. They collected data from private database in the Duke University
    health system with 9,255 sepsis patients and their 165 features (including demographics,
    longitudinal physiological variables, medication, etc. ), and followed up in 30
    days. Actions were discrete values consisting of 3 common treatment for sepsis
    patients: antibiotics, vasopressors, and IV fluids. The reward was sparsely coded.
    The reward at every non-terminal time point was 0\. A reward was +10 at the end
    of a trajectory if the patient survives; and -10 if the patient dead. They investigated
    the effect of replacing fully connected neural network layers with LSTM layers
    in the DQN architecture. The optimized policy for sepsis treatment could reduce
    patient mortality by as much as 8.2% from an overall baseline mortality rate of
    13.3%.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Futoma 等人 ([2018](#bib.bib14)) 提出了对 DRQN 架构的新扩展，使用多输出高斯过程来训练智能体学习针对脓毒症患者的最佳治疗方法。他们从杜克大学健康系统的私人数据库中收集了
    9,255 名脓毒症患者的数据及其 165 个特征（包括人口统计数据、长期生理变量、药物等），并进行了 30 天的跟踪。动作是离散值，包括 3 种常见的脓毒症治疗：抗生素、血管加压药和静脉输液。奖励被稀疏编码。在每个非终止时间点的奖励为
    0\。如果患者生存，轨迹结束时奖励为 +10；如果患者死亡，则奖励为 -10。他们研究了在 DQN 架构中用 LSTM 层替代全连接神经网络层的效果。优化的脓毒症治疗策略可以将患者的死亡率从整体基线死亡率
    13.3% 降低多达 8.2%。
- en: 3.6 Inverse Reinforcement Learning
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 逆强化学习
- en: Diabetes Treatment
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病治疗
- en: In previous papers, the reward function was approximated by the heuristic. However,
    the appropriateness of the reward function could not be validated. In recent studies
    on RL, inverse RL has been proposed to estimate the reward function of experts
    from their behavior data. There have been some papers that focus on Inverse RL(Ng
    et al., [2000](#bib.bib46); Abbeel and Ng, [2004](#bib.bib1)). However, to the
    best of our knowledge, there was not any paper that implemented DNN based Inverse
    RL algorithm for clinical decision support. For non-DNN based Inverse RL, Asoh
    et al. ([2013](#bib.bib5)) implemented an Inverse RL in the Bayesian framework
    and used Markov chain Monte Carlo sampling(Ghavamzadeh et al., [2015](#bib.bib15))
    to learn the reward function in a clinical context. They applied the Inverse RL
    to learn the reward function for diabetic treatments with a uniform prior. The
    drug prescription data was private with the University of Tokyo Hospital. The
    state was discrete and defined as the severity of diabetes (’Normal’, ’Medium’,
    ’Severe’). They used MCMC-sampling and derived the reward for the 3 states as
    $r=(0.01,0.98,0.01)$. The reward showed ’medium’-level diabetes patients have
    the highest value. It seemed to contradict with the current clinical understanding
    that the reward should have the highest value for ’normal’-level diabetic patients.
    Asoh et al. explained that 65% of diabetic patients from their database were already
    in the ’medium’ condition. Therefore, keeping the patients in the ’medium’ state
    might be the best effort from clinicians.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的论文中，奖励函数是通过启发式方法进行近似的。然而，奖励函数的适用性无法得到验证。在近期的强化学习（RL）研究中，逆向RL被提议用于从专家的行为数据中估计奖励函数。有一些论文关注逆向RL（Ng等，[2000](#bib.bib46)；Abbeel和Ng，[2004](#bib.bib1)）。然而，据我们所知，还没有论文实现基于DNN的逆向RL算法用于临床决策支持。对于非DNN基础的逆向RL，Asoh等（[2013](#bib.bib5)）在贝叶斯框架中实现了逆向RL，并使用马尔可夫链蒙特卡洛采样（Ghavamzadeh等，[2015](#bib.bib15)）来学习临床背景中的奖励函数。他们应用逆向RL学习糖尿病治疗的奖励函数，使用了均匀先验。药物处方数据与东京大学医院保持私密。状态是离散的，并定义为糖尿病的严重程度（‘正常’，‘中等’，‘严重’）。他们使用MCMC采样，并为3个状态推导了奖励$r=(0.01,0.98,0.01)$。奖励表明‘中等’级别的糖尿病患者具有最高的价值。这似乎与当前临床理解相矛盾，因为奖励应该对‘正常’级别的糖尿病患者具有最高的价值。Asoh等解释说，他们数据库中的65%的糖尿病患者已经处于‘中等’状态。因此，将患者保持在‘中等’状态可能是临床医生的最佳努力。
- en: Despite very few clinical application implemented Inverse RL, we believe that
    inverse RL is a valuable topic, and it will benefit the applications in the clinical
    context. We are not only optimizing the policy by mimicking experts behavior,
    but we are also very keen to train the policy such that it can automatically figure
    out what are the treatments that clinicians think important.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管逆向强化学习（Inverse RL）在临床应用中实施的案例非常少，但我们相信逆向RL是一个有价值的主题，它将有利于临床环境中的应用。我们不仅通过模仿专家的行为来优化策略，而且我们也非常希望训练策略，使其能够自动识别临床医生认为重要的治疗方法。
- en: 4 How to Choose among Reinforcement Learning Algorithms
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 如何在强化学习算法中进行选择
- en: 'There is no unique answer to this question. Choice of RL algorithm will depend
    on the actual application. Below is a list of trade-offs and assumptions to consider:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个问题没有唯一的答案。RL算法的选择将取决于实际应用。以下是需要考虑的权衡和假设列表：
- en: 'Sample efficiency: Sample efficiency refers to how many samples are required
    to make the algorithm converge. If the algorithm is on-policy, such as policy
    gradient RL, then it would take more samples. In contrast, value-based RL and
    model-based RL are off-policy algorithms, so that fewer samples are required for
    training. Actor-critic RL algorithm is in between value-based RL and policy gradient
    RL. Given different sample efficiency, it does not mean we should always choose
    the one that requires fewer samples. For specific applications where samples are
    easily generated (i.e., symptom checker 1.0 used simulator to generate data),
    the wall-clock time for model training may be more important than the number of
    samples required. In such cases, on-policy RL algorithms might be preferred because
    they are generally faster to converge than off-policy RL algorithms.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sample efficiency**（样本效率）: **Sample efficiency** 指的是算法收敛所需的样本数量。如果算法是在线策略的，比如策略梯度
    RL，那么它需要更多的样本。相比之下，基于价值的 RL 和基于模型的 RL 是离线策略算法，因此需要较少的样本进行训练。演员-评论家 RL 算法介于基于价值的
    RL 和策略梯度 RL 之间。鉴于不同的样本效率，这并不意味着我们总是应选择所需样本较少的算法。对于样本易于生成的特定应用（即，症状检查器 1.0 使用模拟器生成数据），模型训练的实际时间可能比所需样本数量更为重要。在这种情况下，在线策略
    RL 算法可能更受青睐，因为它们通常比离线策略 RL 算法更快收敛。'
- en: 'Convergence: Policy gradient performs gradient ascent on the objective function,
    and it is guaranteed for convergence. Value-based RL minimizes the "Bellman error"
    of fit, but in the worst case, it is not guaranteed to converge to anything in
    the nonlinear cases. Whereas for model-based RL algorithm, the model minimizes
    the error of fit and the model is guaranteed to converge. However, a better model
    is not equivalent to better policy.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**Convergence**（收敛性）: 策略梯度对目标函数执行梯度上升，并且保证收敛。基于价值的 RL 最小化“Bellman error”（贝尔曼误差），但在最坏情况下，在非线性情况下并不保证收敛到任何结果。而对于基于模型的
    RL 算法，模型最小化拟合误差，模型是保证收敛的。然而，更好的模型并不等于更好的策略。'
- en: 'Episodic/infinite horizon: Episodic means the there is an end-point for a state-action
    trajectory. For instance, in the disease symptom checker application, an agent
    continuously searches for the symptom, an episode is over when the agent found
    the disease. Episodic is often assumed by policy gradient methods and also assumed
    by some model-based RL methods. Infinite horizon means there is no clear endpoint
    for a trajectory. The time step for the trajectory can go to infinity, but there
    will be some point where the distribution of the state action pairs remain stable
    and not changing anymore. We refer this as stationary distribution. Most of the
    applications we discuss in this paper are episodic and have clear end-points(i.e.,
    mortality, diagnosis of disease). The episodic assumption is usually assumed by
    pure policy gradient RL and some model-based RL algorithms. Whereas we observed
    value-based RL algorithm also works decently with many clinical applications in
    this paper.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '** episodic / infinite horizon**: **episodic**（阶段性）意味着状态-动作轨迹有一个结束点。例如，在疾病症状检查应用中，代理会持续搜索症状，当代理找到疾病时，一集就结束了。**episodic**
    通常由策略梯度方法假设，也由一些基于模型的 RL 方法假设。**infinite horizon**（无限期）意味着轨迹没有明确的终点。轨迹的时间步可以无限延续，但会有某个点使得状态-动作对的分布保持稳定，不再变化。我们称之为**stationary
    distribution**（平稳分布）。我们在本文讨论的大多数应用是**episodic**的，并且有明确的结束点（即死亡率、疾病诊断）。**episodic**
    假设通常被纯策略梯度 RL 和一些基于模型的 RL 算法假设。而我们观察到基于价值的 RL 算法在本文中的许多临床应用中也表现良好。'
- en: Fully observed/ partially observed MDP When the MDP is fully observed, all the
    main RL algorithms can be applied. Whereas for partially observed MDP, one possible
    solution is to use recurrent RL, such as LSTM based DRQN algorithm, to aggregate
    all historical observation as the belief state. In real time, most of the clinical
    applications are POMDP; we are only able to represent a patient’s state by their
    physiological features. For the methods working in POMDPs, the approach of maintaining
    a belief state is widely used besides RNN. The belief state is the posterior distribution
    over latent state based on the historical incomplete and noisy observations. McAllister
    and Rasmussen ([2017](#bib.bib39)) illustrated a particular case where partial
    observability is due to additive Gaussian white noise on the unobserved state.
    Then belief can be used to filter the noisy observations. Igl et al. ([2018](#bib.bib26))
    proposed a deep variational reinforcement learning (DVRL) algorithm that used
    a DNN to directly output belief state from noisy observations. They showed their
    algorithm outperformed recurrent RL on inferring the actual belief state.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 完全观察/部分观察的MDP 当MDP是完全观察时，所有主要的强化学习算法都可以应用。对于部分观察的MDP，一种可能的解决方案是使用递归强化学习，例如基于LSTM的DRQN算法，将所有历史观察汇总为信念状态。在实际情况中，大多数临床应用是POMDP；我们只能通过患者的生理特征来表示患者的状态。对于在POMDP中工作的算法，除了RNN，保持信念状态的方法被广泛使用。信念状态是基于历史不完整和嘈杂观察的潜在状态的后验分布。McAllister和Rasmussen
    ([2017](#bib.bib39)) 说明了一个特定的情况，其中部分可观察性是由于对未观察状态的加性高斯白噪声。然后，信念可以用来过滤嘈杂的观察。Igl等人
    ([2018](#bib.bib26)) 提出了一个深度变分强化学习（DVRL）算法，该算法使用DNN直接从嘈杂的观察中输出信念状态。他们显示了他们的算法在推断实际信念状态方面优于递归强化学习。
- en: 5 Challenges and Remedies
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个挑战及解决办法
- en: Learning from limited observational data The applications of Deep RL in the
    clinical setting is very different from the case for the Atari game, where one
    can repeat the game many many times and play out all possible scenarios to allow
    the Deep RL agent to learn the optimal policy. In the clinical setting, the Deep
    RL agent requires to learn from a limited set of data and intervention variations
    that were collected. It is known as the POMDP problem. Therefore, for clinical
    applications, the improved policy learned by the RL agent is often not the optimal
    policy. As discussed in the last section, this problem can be addressed by using
    a recurrent structure such as LSTM(Futoma et al., [2018](#bib.bib14)) or by inferring
    and maintaining a belief state with DNN(Igl et al., [2018](#bib.bib26)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从有限观察数据中学习 深度强化学习在临床环境中的应用与在Atari游戏中的情况非常不同，在后者中可以重复游戏很多次，并播放所有可能的场景，以使深度强化学习代理学习最佳策略。在临床环境中，深度强化学习代理需要从有限的数据和干预变异中学习。这被称为POMDP问题。因此，对于临床应用，强化学习代理学到的改进策略通常不是最佳策略。如前一节所讨论，这个问题可以通过使用递归结构如LSTM(Futoma等，[2018](#bib.bib14))或通过使用DNN推断和保持信念状态(Igl等，[2018](#bib.bib26))来解决。
- en: Definition of state action, reward space for clinical applications Finding appropriate
    representation of the state, action, and reward function is challenging in the
    clinical context(Futoma et al., [2018](#bib.bib14)). One needs to define the reward
    function to balance the trade-offs between short-term improvement and long-term
    success. Take sepsis patients in ICU as an example. Periodic improvements in blood
    pressure may not lead to improvements in patients’ outcome. However, if we solely
    focus on patients’ outcome (survival or death) as the reward, this will result
    in a very long sequence of learning without any feedbacks for the agent. While
    the good news is for some RL algorithms, such as Inverse RL, we do not need to
    design the reward by hand. It can be approximated using DNN, and we might even
    train a reward that is closer to the actual reward than hand-crafted reward.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 临床应用中的状态、动作、奖励空间的定义 在临床环境中，找到适当的状态、动作和奖励函数的表示是具有挑战性的(Futoma等，[2018](#bib.bib14))。需要定义奖励函数以平衡短期改进和长期成功之间的权衡。以ICU中的脓毒症患者为例。血压的周期性改善可能不会导致患者结果的改善。然而，如果我们仅将患者的结果（生存或死亡）作为奖励，这将导致一个非常长的学习序列，而代理没有任何反馈。好消息是对于一些强化学习算法，例如逆强化学习，我们不需要手动设计奖励。可以使用DNN进行近似，我们甚至可能训练出比手工设计的奖励更接近实际奖励的奖励。
- en: Performance Benchmarking In other domain, such as video games, the successful
    implementation of DQN on Atari game has attracted the great interest of researches
    in this field Mnih et al. ([2013](#bib.bib40)). For instance, DeepMind applied
    actor-critic RL for their video game ’StarCraft II’(Alghanem et al., [2018](#bib.bib2)).
    Microsoft developed an open source environment for researchers to test on their
    video game ’Malmo’(Johnson et al., [2016b](#bib.bib28)). All these successful
    implementations now are serving as the benchmarks for any new applications of
    RL in video games. However, in the healthcare domain, the benchmark is absent
    due to the lack of many successful applications. We observed that the majority
    of the existing RL healthcare applications utilized the MIMIC EHR database Johnson
    et al. ([2016a](#bib.bib27)). Thus, we plan to build a set benchmark using the
    MIMIC data for the application of Deep RL for clinical decision support in ICUs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 性能基准 在其他领域，例如视频游戏，DQN 在 Atari 游戏上的成功实施引起了该领域研究人员的极大兴趣 Mnih 等人 ([2013](#bib.bib40))。例如，DeepMind
    对其视频游戏《星际争霸 II》应用了演员-评论家强化学习（Alghanem 等人，[2018](#bib.bib2)）。微软为研究人员开发了一个开源环境，以便在他们的视频游戏《Malmo》上进行测试（Johnson
    等人，[2016b](#bib.bib28)）。所有这些成功的实施现在都作为任何新应用于视频游戏的强化学习的基准。然而，在医疗保健领域，由于缺乏许多成功的应用，基准尚不存在。我们观察到，现有的多数强化学习医疗保健应用利用了
    MIMIC EHR 数据库 Johnson 等人（[2016a](#bib.bib27)）。因此，我们计划使用 MIMIC 数据建立一套基准，以应用于临床决策支持的深度强化学习在
    ICU 中的应用。
- en: Exploration/ Exploitation
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 探索/开发
- en: The fundamental dilemma for RL is exploration versus exploitation. If an agent
    knows to take certain actions would result in good reward, how can the agent decide
    whether to attempt new behaviors to discover ones with higher reward (Exploration)
    or continue to do the best thing it knows so far (Exploitation). Exploitation
    means to do things we know will yield the highest reward, whereas exploration
    means to do things we have not done before, but in the hopes of getting an even
    higher reward. Exploration can be challenging in clinical settings due to ethics
    and treatment safety considerations. One paradigm for exploration-exploitation
    balancing is to use $\epsilon$-greedy search to explore random action with a probability
    of $\epsilon\in[0,1]$. The higher the value of $\epsilon$, the more ’open-minded’
    an agent will be to explore an arbitrary action. Alternatives for choosing exploration
    or exploitation include optimistic exploration(Auer et al., [2002](#bib.bib6)),
    Thompson sampling(Chapelle and Li, [2011](#bib.bib9)) and information gain(Mohamed
    and Rezende, [2015](#bib.bib43); Houthooft et al., [2016](#bib.bib24)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的基本困境是探索与开发。如果一个智能体知道某些行为会带来良好的奖励，那么智能体如何决定是尝试新的行为以发现更高奖励（探索），还是继续做目前所知的最佳行为（开发）。开发意味着做我们知道会带来最高奖励的事情，而探索则是做我们以前没有做过的事情，但希望能获得更高的奖励。在临床环境中，探索可能会因伦理和治疗安全考虑而变得具有挑战性。平衡探索与开发的一种范式是使用
    $\epsilon$-贪婪搜索，以概率 $\epsilon\in[0,1]$ 探索随机行为。$\epsilon$ 值越高，智能体对探索任意行为的态度就越“开放”。选择探索或开发的替代方法包括乐观探索（Auer
    等人，[2002](#bib.bib6)）、汤普森采样（Chapelle 和 Li，[2011](#bib.bib9)）和信息增益（Mohamed 和 Rezende，[2015](#bib.bib43);
    Houthooft 等人，[2016](#bib.bib24)）。
- en: Data deficiency and Data Quality Almost all deep learning models have the problem
    of data deficiency in healthcare applications. Though there exist available public
    database, the smaller medical institution often lack sufficient data to fit a
    good deep learning model relying on their local database. Possible solutions include
    using GAN based models to generate data from similar distribution(Tseng et al.,
    [2017](#bib.bib60)), or use transfer learning(Haarnoja et al., [2017](#bib.bib18))
    to pre-train the DNN model from larger datasets and later apply it to a smaller
    hospital/institutional clinical data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数据缺乏与数据质量 几乎所有深度学习模型在医疗应用中都有数据缺乏的问题。尽管存在可用的公共数据库，但较小的医疗机构通常缺乏足够的数据来建立一个良好的深度学习模型，依赖于他们的本地数据库。可能的解决方案包括使用基于
    GAN 的模型从类似分布中生成数据（Tseng 等人，[2017](#bib.bib60)），或者使用迁移学习（Haarnoja 等人，[2017](#bib.bib18)）从较大的数据集预训练
    DNN 模型，然后将其应用于较小的医院/机构临床数据。
- en: 'Table 1: Deep Reinforcement Learning Applications for Clinical Decision Support'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 深度强化学习在临床决策支持中的应用'
- en: '| Application Context | Task/Treatment | Method | Data | Reference |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 应用背景 | 任务/治疗 | 方法 | 数据 | 参考文献 |'
- en: '| Weaning of Mechanical Ventilation | On/Off Mechanical Ventilation, Sedation
    dosage | Fitted Q Iteration | MIMIC III | (Prasad et al., [2017](#bib.bib48))
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 机械通气的断奶 | 开/关机械通气，镇静剂量 | 拟合 Q 迭代 | MIMIC III | （Prasad 等，[2017](#bib.bib48)）
    |'
- en: '| Optimal Heparin Dosing in ICU | Medication Dosage | Fitted Q Iteration |
    MIMIC II | (Nemati et al., [2016](#bib.bib45)) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| ICU 中的最佳肝素剂量 | 药物剂量 | 拟合 Q 迭代 | MIMIC II | （Nemati 等，[2016](#bib.bib45)）
    |'
- en: '| Concept extraction from Free Clinical Text | Extract diagnoses and clinical
    concept | DQN | TREC CDS | (Ling et al., [2017](#bib.bib37)) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 从自由临床文本中提取概念 | 提取诊断和临床概念 | DQN | TREC CDS | （Ling 等，[2017](#bib.bib37)） |'
- en: '| Symptom Checking 1.0 | imitate behavior of inquiry/diagnosis | DQN | Simulated
    Data | (Tang et al., [2016](#bib.bib58)) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 症状检查 1.0 | 模拟询问/诊断行为 | DQN | 模拟数据 | （Tang 等，[2016](#bib.bib58)） |'
- en: '| Symptom Checking 2.0 | imitate behavior of inquiry/diagnosis | Hierarchical
    RL | SymCAT (Simulated) | (Kao et al., [2018](#bib.bib31)) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 症状检查 2.0 | 模拟询问/诊断行为 | 分层 RL | SymCAT（模拟） | （Kao 等，[2018](#bib.bib31)） |'
- en: '| Sepsis Treatment 1.0 | Vasopressors, IV fluid | Double DQN | MIMIC III |
    (Raghu et al., [2017](#bib.bib49)) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 败血症治疗 1.0 | 升压药，静脉输液 | 双重 DQN | MIMIC III | （Raghu 等，[2017](#bib.bib49)）
    |'
- en: '| Sepsis Treatment 2.0 | Vasopressors, IV fluid , antibiotics | MPG+ DRQN |
    Duke University health system (private) | (Futoma et al., [2018](#bib.bib14))
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 败血症治疗 2.0 | 升压药，静脉输液，抗生素 | MPG+ DRQN | 杜克大学健康系统（私人） | （Futoma 等，[2018](#bib.bib14)）
    |'
- en: '| Optimal medical prescription in ICU | Medication choice | Actor-critic RLL
    | MIMIC III | (Wang et al., [2018](#bib.bib62)) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| ICU 中的最佳医疗处方 | 药物选择 | 演员-评论员 RLL | MIMIC III | （Wang 等，[2018](#bib.bib62)）
    |'
- en: '| Radiation dose fraction for lung cancer | Chemotherapy, Medication type and
    dosage | Model-based RL | CIBMTR registry | (Tseng et al., [2017](#bib.bib60))
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 肺癌的放射剂量分割 | 化疗，药物类型和剂量 | 基于模型的 RL | CIBMTR 注册库 | （Tseng 等，[2017](#bib.bib60)）
    |'
- en: '| Diabetes Treatment | Investigate reward for patient’s state | Inverse RL
    (Non-DNN) | University of Tokuo Hospital (private) | (Asoh et al., [2013](#bib.bib5))
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 糖尿病治疗 | 调查患者状态的奖励 | 逆向 RL（非 DNN） | 东京都大学医院（私人） | （Asoh 等，[2013](#bib.bib5)）
    |'
- en: References
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abbeel and Ng [2004] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning
    via inverse reinforcement learning. In *Proceedings of the twenty-first international
    conference on Machine learning*, page 1\. ACM, 2004.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbeel 和 Ng [2004] Pieter Abbeel 和 Andrew Y Ng。通过逆向强化学习进行学徒学习。见于 *第二十一届国际机器学习会议论文集*，第1页。ACM，2004。
- en: Alghanem et al. [2018] Basel Alghanem et al. Asynchronous advantage actor-critic
    agent for starcraft ii. *arXiv preprint arXiv:1807.08217*, 2018.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alghanem 等 [2018] Basel Alghanem 等。用于星际争霸 II 的异步优势演员-评论员代理。*arXiv 预印本 arXiv:1807.08217*，2018。
- en: 'Aronson [2006] Alan R Aronson. Metamap: Mapping text to the umls metathesaurus.
    *Bethesda, MD: NLM, NIH, DHHS*, 1:26, 2006.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aronson [2006] Alan R Aronson。Metamap: 将文本映射到 UML 语义网络。*Bethesda, MD: NLM,
    NIH, DHHS*，1:26，2006。'
- en: Arulkumaran et al. [2017] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage,
    and Anil Anthony Bharath. A brief survey of deep reinforcement learning. *arXiv
    preprint arXiv:1708.05866*, 2017.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arulkumaran 等 [2017] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage
    和 Anil Anthony Bharath。深度强化学习的简要综述。*arXiv 预印本 arXiv:1708.05866*，2017。
- en: Asoh et al. [2013] Hideki Asoh, Masanori Shiro1 Shotaro Akaho, Toshihiro Kamishima,
    Koiti Hasida, Eiji Aramaki, and Takahide Kohro. An application of inverse reinforcement
    learning to medical records of diabetes treatment. In *ECMLPKDD2013 workshop on
    reinforcement learning with generalized feedback*, 2013.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asoh 等 [2013] Hideki Asoh, Masanori Shiro1 Shotaro Akaho, Toshihiro Kamishima,
    Koiti Hasida, Eiji Aramaki 和 Takahide Kohro。将逆向强化学习应用于糖尿病治疗的医疗记录。见于 *ECMLPKDD2013
    强化学习与广义反馈工作坊*，2013。
- en: Auer et al. [2002] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time
    analysis of the multiarmed bandit problem. *Machine learning*, 47(2-3):235–256,
    2002.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Auer 等 [2002] Peter Auer, Nicolo Cesa-Bianchi 和 Paul Fischer。多臂赌博机问题的有限时间分析。*机器学习*，47(2-3):235–256，2002。
- en: Baird III [1993] Leemon C Baird III. Advantage updating. Technical report, WRIGHT
    LAB WRIGHT-PATTERSON AFB OH, 1993.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baird III [1993] Leemon C Baird III。优势更新。技术报告，WRIGHT LAB WRIGHT-PATTERSON AFB
    OH，1993。
- en: Bottou [2010] Léon Bottou. Large-scale machine learning with stochastic gradient
    descent. In *Proceedings of COMPSTAT’2010*, pages 177–186\. Springer, 2010.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bottou [2010] Léon Bottou。大规模机器学习与随机梯度下降。见于 *COMPSTAT’2010 会议录*，第177–186页。Springer，2010。
- en: Chapelle and Li [2011] Olivier Chapelle and Lihong Li. An empirical evaluation
    of thompson sampling. In *Advances in neural information processing systems*,
    pages 2249–2257, 2011.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chapelle 和 Li [2011] Olivier Chapelle 和 Lihong Li。汤普森采样的实证评估。在*神经信息处理系统进展*，页码
    2249–2257，2011。
- en: Chernova and Veloso [2007] Sonia Chernova and Manuela Veloso. Confidence-based
    policy learning from demonstration using gaussian mixture models. In *Proceedings
    of the 6th international joint conference on Autonomous agents and multiagent
    systems*, page 233\. ACM, 2007.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chernova 和 Veloso [2007] Sonia Chernova 和 Manuela Veloso。基于置信度的演示学习策略使用高斯混合模型。在*第六届国际自主智能体与多智能体系统联合会议论文集*，页码
    233。ACM，2007。
- en: Cireşan et al. [2012] Dan Cireşan, Ueli Meier, and Jürgen Schmidhuber. Multi-column
    deep neural networks for image classification. *arXiv preprint arXiv:1202.2745*,
    2012.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cireşan 等 [2012] Dan Cireşan、Ueli Meier 和 Jürgen Schmidhuber。用于图像分类的多列深度神经网络。*arXiv
    预印本 arXiv:1202.2745*，2012。
- en: 'Deisenroth and Rasmussen [2011] Marc Deisenroth and Carl E Rasmussen. Pilco:
    A model-based and data-efficient approach to policy search. In *Proceedings of
    the 28th International Conference on machine learning (ICML-11)*, pages 465–472,
    2011.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deisenroth 和 Rasmussen [2011] Marc Deisenroth 和 Carl E Rasmussen。Pilco：一种基于模型的数据高效策略搜索方法。在*第
    28 届国际机器学习会议（ICML-11）*，页码 465–472，2011。
- en: Doya et al. [2002] Kenji Doya, Kazuyuki Samejima, Ken-ichi Katagiri, and Mitsuo
    Kawato. Multiple model-based reinforcement learning. *Neural computation*, 14(6):1347–1369,
    2002.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doya 等 [2002] Kenji Doya、Kazuyuki Samejima、Ken-ichi Katagiri 和 Mitsuo Kawato。基于多模型的强化学习。*Neural
    computation*，14(6):1347–1369，2002。
- en: Futoma et al. [2018] Joseph Futoma, Anthony Lin, Mark Sendak, Armando Bedoya,
    Meredith Clement, Cara O’Brien, and Katherine Heller. Learning to treat sepsis
    with multi-output gaussian process deep recurrent q-networks. 2018.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Futoma 等 [2018] Joseph Futoma、Anthony Lin、Mark Sendak、Armando Bedoya、Meredith
    Clement、Cara O’Brien 和 Katherine Heller。使用多输出高斯过程深度递归 Q 网络学习治疗脓毒症。2018。
- en: 'Ghavamzadeh et al. [2015] Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau,
    Aviv Tamar, et al. Bayesian reinforcement learning: A survey. *Foundations and
    Trends® in Machine Learning*, 8(5-6):359–483, 2015.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghavamzadeh 等 [2015] Mohammad Ghavamzadeh、Shie Mannor、Joelle Pineau、Aviv Tamar
    等。贝叶斯强化学习：一项调查。*机器学习基础与趋势®*，8(5-6):359–483，2015。
- en: Goodfellow et al. [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. In *Advances in neural information processing systems*, pages
    2672–2680, 2014.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 [2014] Ian Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David
    Warde-Farley、Sherjil Ozair、Aaron Courville 和 Yoshua Bengio。生成对抗网络。在*神经信息处理系统进展*，页码
    2672–2680，2014。
- en: 'Grondman et al. [2012] Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and
    Robert Babuska. A survey of actor-critic reinforcement learning: Standard and
    natural policy gradients. *IEEE Transactions on Systems, Man, and Cybernetics,
    Part C (Applications and Reviews)*, 42(6):1291–1307, 2012.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grondman 等 [2012] Ivo Grondman、Lucian Busoniu、Gabriel AD Lopes 和 Robert Babuska。演员-评论员强化学习调查：标准和自然策略梯度。*IEEE
    系统、人类和控制论会刊 C 部分（应用与评论）*，42(6):1291–1307，2012。
- en: Haarnoja et al. [2017] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey
    Levine. Reinforcement learning with deep energy-based policies. In *Proceedings
    of the 34th International Conference on Machine Learning-Volume 70*, pages 1352–1361\.
    JMLR. org, 2017.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haarnoja 等 [2017] Tuomas Haarnoja、Haoran Tang、Pieter Abbeel 和 Sergey Levine。基于深度能量模型的强化学习。在*第
    34 届国际机器学习会议第 70 卷*，页码 1352–1361。JMLR. org，2017。
- en: Harmon and Baird III [1996] Mance E Harmon and Leemon C Baird III. Multi-player
    residual advantage learning with general function approximation. *Wright Laboratory,
    WL/AACF, Wright-Patterson Air Force Base, OH*, pages 45433–7308, 1996.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harmon 和 Baird III [1996] Mance E Harmon 和 Leemon C Baird III。多玩家残差优势学习与通用函数逼近。*Wright
    Laboratory, WL/AACF, Wright-Patterson Air Force Base, OH*，页码 45433–7308，1996。
- en: Hasselt [2010] Hado V Hasselt. Double q-learning. In *Advances in Neural Information
    Processing Systems*, pages 2613–2621, 2010.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasselt [2010] Hado V Hasselt。双重 Q 学习。在*神经信息处理系统进展*，页码 2613–2621，2010。
- en: Hausknecht and Stone [2015] Matthew Hausknecht and Peter Stone. Deep recurrent
    q-learning for partially observable mdps. In *2015 AAAI Fall Symposium Series*,
    2015.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hausknecht 和 Stone [2015] Matthew Hausknecht 和 Peter Stone。用于部分可观测 MDPs 的深度递归
    Q 学习。在*2015 AAAI 秋季研讨会系列*，2015。
- en: Heess et al. [2015] Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap,
    Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic
    value gradients. In *Advances in Neural Information Processing Systems*, pages
    2944–2952, 2015.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heess 等人 [2015] Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap,
    Tom Erez, 和 Yuval Tassa。通过随机值梯度学习连续控制策略。收录于 *Advances in Neural Information Processing
    Systems*，页码 2944–2952，2015。
- en: Hochreiter and Schmidhuber [1997] Sepp Hochreiter and Jürgen Schmidhuber. Long
    short-term memory. *Neural computation*, 9(8):1735–1780, 1997.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber [1997] Sepp Hochreiter 和 Jürgen Schmidhuber。长短期记忆。*Neural
    computation*，9(8):1735–1780，1997。
- en: 'Houthooft et al. [2016] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip
    De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration.
    In *Advances in Neural Information Processing Systems*, pages 1109–1117, 2016.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houthooft 等人 [2016] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip
    De Turck, 和 Pieter Abbeel。VIME：变分信息最大化探索。收录于 *Advances in Neural Information Processing
    Systems*，页码 1109–1117，2016。
- en: Howard [1960] Ronald A Howard. Dynamic programming and markov processes. 1960.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard [1960] Ronald A Howard。动态规划与马尔可夫过程。1960。
- en: Igl et al. [2018] Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and
    Shimon Whiteson. Deep variational reinforcement learning for pomdps. *arXiv preprint
    arXiv:1806.02426*, 2018.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Igl 等人 [2018] Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, 和 Shimon
    Whiteson。用于 POMDPs 的深度变分强化学习。*arXiv preprint arXiv:1806.02426*，2018。
- en: Johnson et al. [2016a] Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman
    Li-wei, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony
    Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database.
    *Scientific data*, 3:160035, 2016a.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等人 [2016a] Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei,
    Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony
    Celi, 和 Roger G Mark。Mimic-iii，一个可以自由访问的重症监护数据库。*Scientific data*，3:160035，2016a。
- en: Johnson et al. [2016b] Matthew Johnson, Katja Hofmann, Tim Hutton, and David
    Bignell. The malmo platform for artificial intelligence experimentation. In *IJCAI*,
    pages 4246–4247, 2016b.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等人 [2016b] Matthew Johnson, Katja Hofmann, Tim Hutton, 和 David Bignell。用于人工智能实验的
    Malmo 平台。收录于 *IJCAI*，页码 4246–4247，2016b。
- en: 'Kaelbling et al. [1996] Leslie Pack Kaelbling, Michael L Littman, and Andrew W
    Moore. Reinforcement learning: A survey. *Journal of artificial intelligence research*,
    4:237–285, 1996.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaelbling 等人 [1996] Leslie Pack Kaelbling, Michael L Littman, 和 Andrew W Moore。强化学习：一个综述。*Journal
    of artificial intelligence research*，4:237–285，1996。
- en: Kaelbling et al. [1998] Leslie Pack Kaelbling, Michael L Littman, and Anthony R
    Cassandra. Planning and acting in partially observable stochastic domains. *Artificial
    intelligence*, 101(1-2):99–134, 1998.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaelbling 等人 [1998] Leslie Pack Kaelbling, Michael L Littman, 和 Anthony R Cassandra。在部分可观察的随机领域中进行规划和行动。*Artificial
    intelligence*，101(1-2):99–134，1998。
- en: Kao et al. [2018] Hao-Cheng Kao, Kai-Fu Tang, and Edward Y Chang. Context-aware
    symptom checking for disease diagnosis using hierarchical reinforcement learning.
    In *Thirty-Second AAAI Conference on Artificial Intelligence*, 2018.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kao 等人 [2018] Hao-Cheng Kao, Kai-Fu Tang, 和 Edward Y Chang。基于层次强化学习的上下文感知症状检查用于疾病诊断。收录于
    *Thirty-Second AAAI Conference on Artificial Intelligence*，2018。
- en: Kapadia [1998] Sadik Kapadia. *Discriminative training of hidden Markov models*.
    PhD thesis, Citeseer, 1998.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapadia [1998] Sadik Kapadia。*Discriminative training of hidden Markov models*。博士学位论文，Citeseer，1998。
- en: 'Kiumarsi et al. [2018] Bahare Kiumarsi, Kyriakos G Vamvoudakis, Hamidreza Modares,
    and Frank L Lewis. Optimal and autonomous control using reinforcement learning:
    A survey. *IEEE transactions on neural networks and learning systems*, 29(6):2042–2062,
    2018.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiumarsi 等人 [2018] Bahare Kiumarsi, Kyriakos G Vamvoudakis, Hamidreza Modares,
    和 Frank L Lewis。使用强化学习的最优和自主控制：一个综述。*IEEE transactions on neural networks and
    learning systems*，29(6):2042–2062，2018。
- en: Krstic et al. [1995] Miroslav Krstic, Ioannis Kanellakopoulos, Petar V Kokotovic,
    et al. *Nonlinear and adaptive control design*, volume 222. Wiley New York, 1995.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krstic 等人 [1995] Miroslav Krstic, Ioannis Kanellakopoulos, Petar V Kokotovic
    等。*Nonlinear and adaptive control design*，第 222 卷。Wiley New York，1995。
- en: 'Kulkarni et al. [2016] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi,
    and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal
    abstraction and intrinsic motivation. In *Advances in neural information processing
    systems*, pages 3675–3683, 2016.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulkarni 等人 [2016] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, 和 Josh
    Tenenbaum。层次深度强化学习：整合时间抽象和内在动机。收录于 *Advances in neural information processing
    systems*，页码 3675–3683，2016。
- en: Lin [1992] Long-Ji Lin. Self-improving reactive agents based on reinforcement
    learning, planning and teaching. *Machine learning*, 8(3-4):293–321, 1992.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin [1992] Long-Ji Lin。基于强化学习、规划和教学的自我改进反应性代理。*Machine learning*，8(3-4):293–321，1992。
- en: 'Ling et al. [2017] Yuan Ling, Sadid A Hasan, Vivek Datla, Ashequl Qadir, Kathy
    Lee, Joey Liu, and Oladimeji Farri. Diagnostic inferencing via improving clinical
    concept extraction with deep reinforcement learning: A preliminary study. In *Machine
    Learning for Healthcare Conference*, pages 271–285, 2017.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling 等 [2017] Yuan Ling、Sadid A Hasan、Vivek Datla、Ashequl Qadir、Kathy Lee、Joey
    Liu 和 Oladimeji Farri. 通过改进临床概念提取进行诊断推理的深度强化学习：初步研究。在 *医疗保健机器学习会议* 中，页面 271–285，2017年。
- en: Marik [2015] PE Marik. The demise of early goal-directed therapy for severe
    sepsis and septic shock. *Acta Anaesthesiologica Scandinavica*, 59(5):561–567,
    2015.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marik [2015] PE Marik. 严重脓毒症和脓毒性休克早期目标导向治疗的衰退。*斯堪的纳维亚麻醉学杂志*，59(5)：561–567，2015年。
- en: McAllister and Rasmussen [2017] Rowan McAllister and Carl Edward Rasmussen.
    Data-efficient reinforcement learning in continuous state-action gaussian-pomdps.
    In *Advances in Neural Information Processing Systems*, pages 2040–2049, 2017.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McAllister 和 Rasmussen [2017] Rowan McAllister 和 Carl Edward Rasmussen. 在 *神经信息处理系统的进展*
    中，数据高效的连续状态-动作高斯POMDP中的强化学习。页面 2040–2049，2017年。
- en: Mnih et al. [2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
    Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep
    reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等 [2013] Volodymyr Mnih、Koray Kavukcuoglu、David Silver、Alex Graves、Ioannis
    Antonoglou、Daan Wierstra 和 Martin Riedmiller. 使用深度强化学习玩 Atari 游戏。*arXiv 预印本 arXiv:1312.5602*，2013年。
- en: Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A
    Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K
    Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement
    learning. *Nature*, 518(7540):529, 2015.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等 [2015] Volodymyr Mnih、Koray Kavukcuoglu、David Silver、Andrei A Rusu、Joel
    Veness、Marc G Bellemare、Alex Graves、Martin Riedmiller、Andreas K Fidjeland、Georg
    Ostrovski 等。通过深度强化学习实现人类水平的控制。*自然*，518(7540)：529，2015年。
- en: Mnih et al. [2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
    Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous
    methods for deep reinforcement learning. In *International conference on machine
    learning*, pages 1928–1937, 2016.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等 [2016] Volodymyr Mnih、Adria Puigdomenech Badia、Mehdi Mirza、Alex Graves、Timothy
    Lillicrap、Tim Harley、David Silver 和 Koray Kavukcuoglu. 深度强化学习的异步方法。在 *国际机器学习会议*
    中，页面 1928–1937，2016年。
- en: Mohamed and Rezende [2015] Shakir Mohamed and Danilo Jimenez Rezende. Variational
    information maximisation for intrinsically motivated reinforcement learning. In
    *Advances in neural information processing systems*, pages 2125–2133, 2015.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohamed 和 Rezende [2015] Shakir Mohamed 和 Danilo Jimenez Rezende. 用于内在动机强化学习的变分信息最大化。在
    *神经信息处理系统的进展* 中，页面 2125–2133，2015年。
- en: Munos and Szepesvári [2008] Rémi Munos and Csaba Szepesvári. Finite-time bounds
    for fitted value iteration. *Journal of Machine Learning Research*, 9(May):815–857,
    2008.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Munos 和 Szepesvári [2008] Rémi Munos 和 Csaba Szepesvári. 拟合值迭代的有限时间界限。*机器学习研究杂志*，9（5月）：815–857，2008年。
- en: 'Nemati et al. [2016] Shamim Nemati, Mohammad M Ghassemi, and Gari D Clifford.
    Optimal medication dosing from suboptimal clinical examples: A deep reinforcement
    learning approach. In *2016 38th Annual International Conference of the IEEE Engineering
    in Medicine and Biology Society (EMBC)*, pages 2978–2981\. IEEE, 2016.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nemati 等 [2016] Shamim Nemati、Mohammad M Ghassemi 和 Gari D Clifford. 从亚优临床示例中获得的最佳药物剂量：一种深度强化学习方法。在
    *2016年第38届IEEE医学与生物工程国际会议（EMBC）* 中，页面 2978–2981。IEEE，2016年。
- en: Ng et al. [2000] Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse
    reinforcement learning. In *Icml*, volume 1, page 2, 2000.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng 等 [2000] Andrew Y Ng、Stuart J Russell 等。逆强化学习的算法。在 *Icml* 中，第 1 卷，第 2 页，2000年。
- en: Pan and Yang [2010] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning.
    *IEEE Transactions on knowledge and data engineering*, 22(10):1345–1359, 2010.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 和 Yang [2010] Sinno Jialin Pan 和 Qiang Yang. 转移学习综述。*IEEE 知识与数据工程学报*，22(10)：1345–1359，2010年。
- en: Prasad et al. [2017] Niranjani Prasad, Li-Fang Cheng, Corey Chivers, Michael
    Draugelis, and Barbara E Engelhardt. A reinforcement learning approach to weaning
    of mechanical ventilation in intensive care units. *arXiv preprint arXiv:1704.06300*,
    2017.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prasad 等 [2017] Niranjani Prasad、Li-Fang Cheng、Corey Chivers、Michael Draugelis
    和 Barbara E Engelhardt. 一种用于重症监护病房机械通气脱离的强化学习方法。*arXiv 预印本 arXiv:1704.06300*，2017年。
- en: Raghu et al. [2017] Aniruddh Raghu, Matthieu Komorowski, Leo Anthony Celi, Peter
    Szolovits, and Marzyeh Ghassemi. Continuous state-space models for optimal sepsis
    treatment-a deep reinforcement learning approach. *arXiv preprint arXiv:1705.08422*,
    2017.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raghu 等人 [2017] Aniruddh Raghu, Matthieu Komorowski, Leo Anthony Celi, Peter
    Szolovits, 和 Marzyeh Ghassemi。连续状态空间模型用于最佳脓毒症治疗——一种深度强化学习方法。 *arXiv 预印本 arXiv:1705.08422*，2017年。
- en: Rasmussen [2003] Carl Edward Rasmussen. Gaussian processes in machine learning.
    In *Summer School on Machine Learning*, pages 63–71\. Springer, 2003.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasmussen [2003] Carl Edward Rasmussen。机器学习中的高斯过程。载于 *机器学习暑期学校*，第63–71页。Springer，2003年。
- en: Riedmiller [2005] Martin Riedmiller. Neural fitted q iteration–first experiences
    with a data efficient neural reinforcement learning method. In *European Conference
    on Machine Learning*, pages 317–328. Springer, 2005.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riedmiller [2005] Martin Riedmiller。神经拟合Q迭代——数据高效神经强化学习方法的初步经验。载于 *欧洲机器学习大会*，第317–328页。Springer，2005年。
- en: Schaal [1999] Stefan Schaal. Is imitation learning the route to humanoid robots?
    *Trends in cognitive sciences*, 3(6):233–242, 1999.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaal [1999] Stefan Schaal。模仿学习是否是类人机器人发展的途径？ *认知科学趋势*，3(6)：233–242，1999年。
- en: Silver et al. [2016] David Silver, Aja Huang, Chris J Maddison, Arthur Guez,
    Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
    Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural
    networks and tree search. *nature*, 529(7587):484, 2016.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver 等人 [2016] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent
    Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
    Panneershelvam, Marc Lanctot, 等。使用深度神经网络和树搜索掌握围棋游戏。 *自然*，529(7587)：484，2016年。
- en: Simpson et al. [2014] Matthew S Simpson, Ellen M Voorhees, and William Hersh.
    Overview of the trec 2014 clinical decision support track. Technical report, LISTER
    HILL NATIONAL CENTER FOR BIOMEDICAL COMMUNICATIONS BETHESDA MD, 2014.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simpson 等人 [2014] Matthew S Simpson, Ellen M Voorhees, 和 William Hersh。TREC
    2014临床决策支持轨道概述。技术报告，LISTER HILL NATIONAL CENTER FOR BIOMEDICAL COMMUNICATIONS
    BETHESDA MD，2014年。
- en: 'Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. *Reinforcement
    learning: An introduction*. MIT press, 2018.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto [2018] Richard S Sutton 和 Andrew G Barto。 *强化学习：导论*。MIT出版社，2018年。
- en: Sutton et al. [1998] Richard S Sutton, Andrew G Barto, et al. *Introduction
    to reinforcement learning*, volume 135. MIT press Cambridge, 1998.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 等人 [1998] Richard S Sutton, Andrew G Barto, 等。 *强化学习导论*，第135卷。MIT出版社，1998年。
- en: Sutton et al. [2000] Richard S Sutton, David A McAllester, Satinder P Singh,
    and Yishay Mansour. Policy gradient methods for reinforcement learning with function
    approximation. In *Advances in neural information processing systems*, pages 1057–1063,
    2000.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 等人 [2000] Richard S Sutton, David A McAllester, Satinder P Singh, 和 Yishay
    Mansour。用于函数逼近的强化学习中的策略梯度方法。载于 *神经信息处理系统进展*，第1057–1063页，2000年。
- en: 'Tang et al. [2016] Kai-Fu Tang, Hao-Cheng Kao, Chun-Nan Chou, and Edward Y
    Chang. Inquire and diagnose: Neural symptom checking ensemble using deep reinforcement
    learning. In *Proceedings of NIPS Workshop on Deep Reinforcement Learning*, 2016.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人 [2016] Kai-Fu Tang, Hao-Cheng Kao, Chun-Nan Chou, 和 Edward Y Chang。询问和诊断：使用深度强化学习的神经症状检查集成。载于
    *NIPS深度强化学习研讨会论文集*，2016年。
- en: Thrun and Schwartz [1993] Sebastian Thrun and Anton Schwartz. Issues in using
    function approximation for reinforcement learning. In *Proceedings of the 1993
    Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum*, 1993.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thrun 和 Schwartz [1993] Sebastian Thrun 和 Anton Schwartz。在使用函数逼近进行强化学习中的问题。载于
    *1993年连接主义模型暑期学校论文集，Hillsdale, NJ. Lawrence Erlbaum*，1993年。
- en: Tseng et al. [2017] Huan-Hsin Tseng, Yi Luo, Sunan Cui, Jen-Tzung Chien, Randall K
    Ten Haken, and Issam El Naqa. Deep reinforcement learning for automated radiation
    adaptation in lung cancer. *Medical physics*, 44(12):6690–6705, 2017.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tseng 等人 [2017] Huan-Hsin Tseng, Yi Luo, Sunan Cui, Jen-Tzung Chien, Randall
    K Ten Haken, 和 Issam El Naqa。用于肺癌的深度强化学习自动辐射适应。 *医学物理*，44(12)：6690–6705，2017年。
- en: van Hasselt [2011] Hado Philip van Hasselt. *Insights in reinforcement learning*.
    Hado van Hasselt, 2011.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Hasselt [2011] Hado Philip van Hasselt。 *强化学习中的洞察*。Hado van Hasselt，2011年。
- en: Wang et al. [2018] Lu Wang, Wei Zhang, Xiaofeng He, and Hongyuan Zha. Supervised
    reinforcement learning with recurrent neural network for dynamic treatment recommendation.
    In *Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining*, pages 2447–2456\. ACM, 2018.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2018] Lu Wang, Wei Zhang, Xiaofeng He, 和 Hongyuan Zha。用于动态治疗推荐的监督强化学习与递归神经网络。载于
    *第24届ACM SIGKDD国际知识发现与数据挖掘大会论文集*，第2447–2456页。ACM，2018年。
- en: Wang et al. [2015] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc
    Lanctot, and Nando De Freitas. Dueling network architectures for deep reinforcement
    learning. *arXiv preprint arXiv:1511.06581*, 2015.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2015] 祖宇·王、汤姆·绍尔、马泰奥·赫塞尔、哈多·范·哈塞尔、马克·兰克托和南多·德·弗雷塔斯。深度强化学习中的对抗网络架构。*arXiv
    预印本 arXiv:1511.06581*，2015年。
- en: Watkins and Dayan [1992] Christopher JCH Watkins and Peter Dayan. Q-learning.
    *Machine learning*, 8(3-4):279–292, 1992.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watkins and Dayan [1992] 克里斯托弗·JCH·沃特金斯和彼得·戴扬。Q学习。*机器学习*，8(3-4)：279–292，1992年。
