- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:44:04'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:44:04'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2210.05866] Deep Learning for Iris Recognition: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2210.05866] 深度学习在虹膜识别中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2210.05866](https://ar5iv.labs.arxiv.org/html/2210.05866)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2210.05866](https://ar5iv.labs.arxiv.org/html/2210.05866)
- en: 'Deep Learning for Iris Recognition: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在虹膜识别中的应用：综述
- en: 'Kien Nguyen Queensland University of TechnologyAustralia [nguyentk@qut.edu.au](mailto:nguyentk@qut.edu.au)
    ,  Hugo Proença University of Beira Interior, IT: Instituto de TelecomunicaçõesPortugal
    [hugomcp@di.ubi.pt](mailto:hugomcp@di.ubi.pt)  and  Fernando Alonso-Fernandez
    Halmstad UniversitySweden [feralo@hh.se](mailto:feralo@hh.se)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kien Nguyen，昆士兰科技大学，澳大利亚 [nguyentk@qut.edu.au](mailto:nguyentk@qut.edu.au)，Hugo
    Proença，贝拉内里奥大学，IT: 通讯学院，葡萄牙 [hugomcp@di.ubi.pt](mailto:hugomcp@di.ubi.pt)，以及
    Fernando Alonso-Fernandez，哈尔姆斯塔大学，瑞典 [feralo@hh.se](mailto:feralo@hh.se)'
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: ABSTRACT
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this survey, we provide a comprehensive review of more than 200 papers,
    technical reports, and GitHub repositories published over the last 10 years on
    the recent developments of deep learning techniques for iris recognition, covering
    broad topics on algorithm designs, open-source tools, open challenges, and emerging
    research. First, we conduct a comprehensive analysis of deep learning techniques
    developed for two main sub-tasks in iris biometrics: segmentation and recognition.
    Second, we focus on deep learning techniques for the robustness of iris recognition
    systems against presentation attacks and via human-machine pairing. Third, we
    delve deep into deep learning techniques for forensic application, especially
    in post-mortem iris recognition. Fourth, we review open-source resources and tools
    in deep learning techniques for iris recognition. Finally, we highlight the technical
    challenges, emerging research trends, and outlook for the future of deep learning
    in iris recognition.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项综述中，我们提供了对过去10年中超过200篇论文、技术报告和GitHub仓库的全面回顾，这些文献涉及了深度学习技术在虹膜识别中的最新进展，涵盖了算法设计、开源工具、开放挑战和新兴研究等广泛主题。首先，我们对深度学习技术在虹膜生物特征识别中的两个主要子任务——分割和识别——进行了全面分析。其次，我们关注了深度学习技术在虹膜识别系统对抗伪造攻击和人机配对中的鲁棒性。第三，我们深入探讨了深度学习技术在法医学应用中的作用，特别是在尸检后的虹膜识别中。第四，我们回顾了虹膜识别中深度学习技术的开源资源和工具。最后，我们突出了技术挑战、新兴研究趋势以及对虹膜识别中深度学习未来的展望。
- en: 'Iris Recognition, Deep Learning, Neural Networks^†^†ccs: Security and privacy Biometrics'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '虹膜识别、深度学习、神经网络^†^†ccs: 安全与隐私 生物特征识别'
- en: 1\. Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 引言
- en: The human iris is a sight organ that controls the amount of light reaching the
    retina, by changing the size of the pupil. The texture of the iris is fully developed
    before birth, its minutiae do not depend on genotype, it stays relatively stable
    across lifetime (except for disease- and normal aging-related biological changes),
    and it may even be used for forensic identification shortly after subject’s death (Muron
    and Pospisil, [2000](#bib.bib111); Daugman, [2016](#bib.bib37); Trokielewicz et al.,
    [2019](#bib.bib171)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人眼虹膜是一个调节到达视网膜光线量的器官，通过改变瞳孔的大小来实现。虹膜的纹理在出生前就已完全发育，其细节不依赖于基因型，终生相对稳定（除疾病和正常老化相关的生物变化外），甚至可以在受试者去世后不久用于法医识别（Muron和Pospisil，[2000](#bib.bib111)；Daugman，[2016](#bib.bib37)；Trokielewicz等，[2019](#bib.bib171)）。
- en: 'In terms of its information theory-related properties, the iris texture has
    an extremely high randotypic randomness, and is stable (permanent) over time,
    providing an exceptionally high entropy per mm.² that justifies its higher discriminating
    power, when compared to other biometric modalities (e.g., face or fingerprint).
    The iris’ collectability is another feature of interest and has been the subject
    of discussion over the last years: while it can be acquired using commercial off-the-shelf
    (COTS) hardware, either handheld or stationary, data can be even collected from
    at-a-distance, up to tens of meters away from the subjects (Nguyen et al., [2017a](#bib.bib112)).
    Even though commercial visible-light (RGB) cameras are able to image the iris,
    the near infrared-based (NIR) sensing dominates in most applications, due to a
    better visibility of iris texture for darker eyes, rich in melanin pigment, which
    is characterized by lower light absorption in NIR spectrum compared to shorter
    wavelengths. In addition, NIR wavelengths are barely perceivable by the human
    eye, which augment users’ comfort, and avoids pupil contraction/dilation that
    would appear under visible light.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 就其信息理论相关属性而言，虹膜纹理具有极高的随机性，并且随着时间的推移稳定（持久），每平方毫米的熵值极高，这也解释了其比其他生物识别方式（例如面部或指纹）更高的区分能力。虹膜的采集性是另一个值得关注的特性，并且在过去几年中一直是讨论的主题：虽然可以使用商业现成（COTS）硬件进行采集，无论是手持的还是固定的，但数据甚至可以从距离被试者数十米远的地方进行收集（Nguyen
    et al., [2017a](#bib.bib112)）。尽管商业可见光（RGB）摄像机能够成像虹膜，但近红外（NIR）传感器在大多数应用中占主导地位，因为对于黑色眼睛中丰富的黑色素，虹膜纹理在NIR光谱中的可见性更好，这种光谱的光吸收较短波长光少。此外，NIR波长几乎无法被人眼感知，这增加了用户的舒适度，并避免了在可见光下出现的瞳孔收缩/扩张现象。
- en: A seminal work by John Daugman brought to the community the Gabor filtering-based
    approach that became the dominant approach for iris recognition (Daugman, [1993](#bib.bib35),
    [2007](#bib.bib36), [2021](#bib.bib38)). Even though subsequent solutions to iris
    image encoding and matching appeared, the IrisCodes approach is still dominant
    due to its ability to effectively search in massive databases with a minimal probability
    of false matches, at extreme time performance. By considering binary words, pairs
    of signatures are matched using XOR parallel-bit logic at lightening speed, enabling
    millions of comparisons/second per processing core. Also, most of the methods
    that outperformed the original techniques in terms of effectiveness do not work
    under the *one-shot learning* paradigm, assume multiple observations of each class
    to obtain appropriate decision boundaries, and - most importantly - have encoding/matching
    steps with time complexity that forbid their use in large environments (in particular,
    for *all-against-all* settings).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰·道格曼的开创性工作带来了基于Gabor滤波的方法，这种方法成为虹膜识别的主导方法（Daugman, [1993](#bib.bib35), [2007](#bib.bib36),
    [2021](#bib.bib38)）。尽管后续的虹膜图像编码和匹配解决方案出现了，但由于其在极端时间性能下能够有效地在大规模数据库中搜索且假匹配概率极低，IrisCodes方法仍然占主导地位。通过考虑二进制字，使用XOR并行位逻辑以闪电般的速度匹配签名对，从而实现每个处理核心每秒数百万次比较。此外，大多数在效果上超越原始技术的方法并不适用于*一次性学习*范式，假设每类有多次观察以获得适当的决策边界，并且
    - 最重要的是 - 具有时间复杂度的编码/匹配步骤使其在大型环境中无法使用（特别是*全对全*设置）。
- en: In short, Daugman’s algorithm encodes the iris image into a binary sequence
    of 2,048 bits by filtering the iris image with a family of Gabor kernels. The
    varying pupil size is rectified by the Cartesian-to-polar coordinate system transformation,
    to end up with an image representation of canonical size, guarantying identical
    structure of the iris code independently of the iris and pupil size. This makes
    possible to use the Hamming Distance (HD) to measure the similarity between two
    iris codes (Daugman, [2021](#bib.bib38)). Its low false match rate at acceptable
    false non-match rates is the key factor behind the success of global-scale iris
    recognition installments, such as the national person identification and border
    security program Aadhaar program in India (with over 1.2 billion pairs of irises
    enrolled) (Unique Identification Authority of India, [2021](#bib.bib175)), the
    Homeland Advanced Recognition Technology (HART) in the US (up to 500 million identities)
    (Planet Biometrics, [2017](#bib.bib129)), or the NEXUS system, designed to speed
    up border crossings for low-risk and pre-approved travelers moving between Canada
    and the US.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Daugman的算法通过用一系列Gabor核对虹膜图像进行滤波，将虹膜图像编码为2,048位的二进制序列。通过笛卡尔到极坐标系的转换来校正不同的瞳孔大小，最终得到一个标准大小的图像表示，确保虹膜编码的结构在不同的虹膜和瞳孔大小下保持一致。这使得可以使用汉明距离（HD）来测量两个虹膜编码之间的相似性（Daugman，[2021](#bib.bib38)）。其在接受的假阳性率下的低假匹配率是全球范围内虹膜识别系统成功的关键因素，例如印度的国家个人识别和边境安全项目Aadhaar（注册了超过12亿对虹膜）（Unique
    Identification Authority of India，[2021](#bib.bib175)），美国的家园先进识别技术（HART）（多达5亿个身份）（Planet
    Biometrics，[2017](#bib.bib129)），或NEXUS系统，旨在加快低风险和预先批准的旅行者在加拿大和美国之间的边境通行。
- en: 'Deep learning-based methods, in particular using various Convolutional Neural
    Network architectures, have been driving remarkable improvements in many computer
    vision applications over the last decade. In terms of biometrics technologies,
    it’s not surprising that iris recognition has also seen an increasing adoption
    of purely data-driven approaches at all stages of the recognition pipeline: from
    preprocessing (such as off-axis gaze correction), segmentation, encoding to matching.
    Interestingly, however, the impact of deep learning on the various stages of iris
    recognition pipeline is uneven. One of the primary goals of this survey paper
    is to assess where deep learning helped in achieving highly performance and more
    secure systems, and which procedures did not benefit from more complex modeling.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的方法，特别是使用各种卷积神经网络架构，在过去十年中推动了许多计算机视觉应用的显著进步。在生物特征技术方面，虹膜识别也在识别流程的各个阶段——从预处理（如偏轴视线校正）、分割、编码到匹配——越来越多地采用了纯数据驱动的方法，这并不令人意外。然而，深度学习对虹膜识别流程各个阶段的影响是不均匀的。本调查论文的主要目标之一是评估深度学习在哪些方面有助于实现高性能和更安全的系统，以及哪些过程没有从更复杂的建模中受益。
- en: 'The remainder of the paper is structured as follows. Section [2](#S2 "2\. Deep
    Learning-Based Iris Segmentation ‣ Deep Learning for Iris Recognition: A Survey")
    and  [3](#S3 "3\. Deep Learning-Based Iris Recognition ‣ Deep Learning for Iris
    Recognition: A Survey") review the application of deep learning in two main stages
    of the recognition pipeline: segmentation and recognition (encoding and comparison).
    Section [4](#S4 "4\. Deep Learning-Based Iris Presentation Attack Detection ‣
    Deep Learning for Iris Recognition: A Survey") and  [5](#S5 "5\. Deep Learning-Based
    Forensic Iris Recognition ‣ Deep Learning for Iris Recognition: A Survey") analyze
    the state of the art of deep learning-based approaches in two applications: Presentation
    Attack Detection (PAD) and Forensic. Section [6](#S6 "6\. Human-Machine Pairing
    to Improve Deep Learning-Based Iris Recognition ‣ Deep Learning for Iris Recognition:
    A Survey") investigates how human and machine can pair to improve deep learning
    based iris recognition. Section [7](#S7 "7\. Recognition in Less Controlled Environments:
    Iris/Periocular Analysis ‣ Deep Learning for Iris Recognition: A Survey") focuses
    on approaches in less controlled environments of iris and periocular analysis.
    Section [8](#S8 "8\. Open-Source Deep Learning-Based Iris Recognition Tools ‣
    Deep Learning for Iris Recognition: A Survey") reviews public resources and tools
    available in the deep learning based iris recognition domain. Section [9](#S9
    "9\. Emerging Research Directions ‣ Deep Learning for Iris Recognition: A Survey")
    focuses on the future of deep learning for iris recognition with discussion on
    emerging research directions in different aspects of iris analysis. The paper
    in concluded in Section [10](#S10 "10\. Conclusions ‣ Deep Learning for Iris Recognition:
    A Survey").'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分结构如下。第[2](#S2 "2\. 基于深度学习的虹膜分割 ‣ 深度学习在虹膜识别中的应用：综述")和[3](#S3 "3\. 基于深度学习的虹膜识别
    ‣ 深度学习在虹膜识别中的应用：综述")节回顾了深度学习在识别管道的两个主要阶段中的应用：分割和识别（编码和比较）。第[4](#S4 "4\. 基于深度学习的虹膜展示攻击检测
    ‣ 深度学习在虹膜识别中的应用：综述")和[5](#S5 "5\. 基于深度学习的法医虹膜识别 ‣ 深度学习在虹膜识别中的应用：综述")节分析了基于深度学习的方法在两个应用中的前沿：展示攻击检测（PAD）和法医识别。第[6](#S6
    "6\. 人机配对以提升基于深度学习的虹膜识别 ‣ 深度学习在虹膜识别中的应用：综述")节探讨了人机如何配对以提升基于深度学习的虹膜识别。第[7](#S7
    "7\. 在控制较少的环境下的识别：虹膜/眼周分析 ‣ 深度学习在虹膜识别中的应用：综述")节关注虹膜和眼周分析中控制较少的环境下的方法。第[8](#S8
    "8\. 开源基于深度学习的虹膜识别工具 ‣ 深度学习在虹膜识别中的应用：综述")节回顾了在深度学习基于虹膜识别领域的公共资源和工具。第[9](#S9 "9\.
    新兴研究方向 ‣ 深度学习在虹膜识别中的应用：综述")节关注深度学习在虹膜识别中的未来，讨论了虹膜分析不同方面的新兴研究方向。第[10](#S10 "10\.
    结论 ‣ 深度学习在虹膜识别中的应用：综述")节对论文进行了总结。
- en: 2\. Deep Learning-Based Iris Segmentation
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 基于深度学习的虹膜分割
- en: 'The segmentation of the iris is seen as an extremely challenging problem. As
    illustrated in Fig. [1](#S2.F1 "Figure 1 ‣ 2\. Deep Learning-Based Iris Segmentation
    ‣ Deep Learning for Iris Recognition: A Survey"), segmenting the iris involves
    essentially three tasks: detect and parameterize the inner (pupillary) and outer
    (scleric) biological boundaries of the iris and also to locally discriminate between
    the noise-free/noisy regions inside the iris ring, which should be subsequently
    used in the feature encoding and matching processes.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虹膜分割被视为一个极具挑战性的问题。如图[1](#S2.F1 "图 1 ‣ 2\. 基于深度学习的虹膜分割 ‣ 深度学习在虹膜识别中的应用：综述")所示，虹膜分割实质上涉及三个任务：检测和参数化虹膜的内（瞳孔）和外（巩膜）生物边界，以及在虹膜环内局部区分噪声干扰/无噪声区域，这些区域随后应在特征编码和匹配过程中使用。
- en: This problem has motivated numerous research works for decades. From the pioneering
    integro-differential operator (Daugman, [1993](#bib.bib35)) up to subsequent handcrafted
    techniques based in active contours and neural networks (e.g., (He et al., [2009](#bib.bib69)), (Proença,
    [2010](#bib.bib134)), (Shah and Ross, [2009](#bib.bib148)) and (Vatsa et al.,
    [2008](#bib.bib176))) a long road has been traveled in this problem. Regardless
    an obvious evolution in the effectiveness of such techniques, they all face particular
    difficulties in case of heavily degraded data. Images are frequently motion-blurred,
    poor focused, partially occluded and off-angle. Additionally, in case of visible
    light data, severe reflections from the environments surrounding the subjects
    are visible, and even augment the difficulties of the segmentation task.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年来，这个问题一直激发了无数的研究工作。从开创性的整合-微分算子（Daugman，[1993](#bib.bib35)）到后来基于主动轮廓和神经网络的手工技术（例如，(He
    et al.，[2009](#bib.bib69))，(Proença，[2010](#bib.bib134))，(Shah和Ross，[2009](#bib.bib148))和(Vatsa
    et al.，[2008](#bib.bib176))），在解决这个问题上已经经过了很长的道路。尽管这些技术的效果明显改进，但在数据严重退化的情况下，它们都面临着特殊困难。图像经常出现运动模糊、焦点不准、部分遮挡和偏角。此外，在可见光数据的情况下，还会出现来自主体周围环境的严重反射，甚至增加了分割任务的难度。
- en: 'Recently, as in many other computer vision tasks, DL-based frameworks have
    been advocated as providing consistent advances over the state-of-the-art for
    the iris segmentation problem, with numerous models being proposed. A cohesive
    perspective of the most relevant recent DL-based methods is given in Table [1](#S2.T1
    "Table 1 ‣ 2\. Deep Learning-Based Iris Segmentation ‣ Deep Learning for Iris
    Recognition: A Survey"), with the techniques appearing in chronographic (and then
    alphabetical) order. The type of data each model aims to handle is given in the
    ”*Data*” column, along with the datasets where the corresponding experiments were
    carried out and a summary of the main characteristics of each proposal (”*Features*”
    column). Here, considering that models were empirically validated in completely
    heterogeneous ways and using very different metrics, we decided not to include
    the summary performance of each model/solution.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，在很多其他计算机视觉任务中，基于深度学习的框架被认为相比最新技术提供了一致的进展，对虹膜分割问题有很多模型被提出。在表格[1](#S2.T1 "表1
    ‣ 2\. 基于深度学习的虹膜分割 ‣ 虹膜识别的深度学习：调查")中，给出了最近相关的基于深度学习的方法的整体观点，按时间顺序（然后按字母顺序）列出了这些方法。每个模型旨在处理的数据类型在“*数据*”列中给出，以及进行相应实验的数据集，并列出了每个提案的主要特征的摘要（“*特征*”列）。考虑到模型在完全异构的方式下经验验证，并使用非常不同的指标，我们决定不包括每个模型/解决方案的综合性能摘要。
- en: <svg   height="168.43" overflow="visible" version="1.1" width="442.05"><g transform="translate(0,168.43)
    matrix(1 0 0 -1 0 0) translate(128.51,0) translate(0,79.29)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -69 -31.5)" fill="#000000"
    stroke="#000000"><foreignobject width="138" height="63" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">![Refer to caption](img/f29d6e98b2a84ce57df5e582022676cd.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -74.04 -41.79)" fill="#000000" stroke="#000000"><foreignobject
    width="148.09" height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Scleric
    Boundary Parameterization</foreignobject></g><g transform="matrix(1.0 0.0 0.0
    1.0 86.16 -42.49)" fill="#FFFFFF" stroke="#FFFFFF" color="#FFFFFF"><foreignobject
    width="4.84" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -100.16 -56.51)" fill="#000000" stroke="#000000"><foreignobject
    width="122.39" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Noise-free
    Texture Detection</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 33.01
    -56.27)" fill="#000000" stroke="#000000" color="#000000"><foreignobject width="4.84"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -99.58 46.79)" fill="#000000" stroke="#000000"><foreignobject
    width="159.79" height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Pupillary
    Boundary Parameterization</foreignobject></g><g transform="matrix(1.0 0.0 0.0
    1.0 72.38 46.09)" fill="#000000" stroke="#000000" color="#000000"><foreignobject
    width="4.84" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 157.54 -29.5)" fill="#000000" stroke="#000000"><foreignobject
    width="118" height="59" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/effc3c2666ee5f9be482169720f82465.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 27.84 69.38)" fill="#000000" stroke="#000000"><foreignobject width="181.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Iris Segmentation
    Main Tasks</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 161.66 -42.73)"
    fill="#000000" stroke="#000000"><foreignobject width="110.55" height="6.73" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Dimensionless Noise-Free</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 184.81 -53.55)" fill="#000000" stroke="#000000"><foreignobject
    width="63.71" height="8.5" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Representation</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.72 12.63)" fill="#FFFFFF" stroke="#FFFFFF"
    color="#FFFFFF"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 94.04 12.63)" fill="#000000" stroke="#000000" color="#000000"><foreignobject
    width="4.84" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 102.53 13.33)" fill="#000000" stroke="#000000"
    color="#000000"><foreignobject width="7.53" height="6.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">+</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 122.22 13.33)" fill="#000000" stroke="#000000" color="#000000"><foreignobject
    width="7.53" height="6.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">+</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 133.41 12.63)" fill="#000000" stroke="#000000"
    color="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="168.43" overflow="visible" version="1.1" width="442.05"><g transform="translate(0,168.43)
    matrix(1 0 0 -1 0 0) translate(128.51,0) translate(0,79.29)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -69 -31.5)" fill="#000000"
    stroke="#000000"><foreignobject width="138" height="63" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">![请参阅标题](img/f29d6e98b2a84ce57df5e582022676cd.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -74.04 -41.79)" fill="#000000" stroke="#000000"><foreignobject
    width="148.09" height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">教士边界参数化</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 86.16 -42.49)" fill="#FFFFFF" stroke="#FFFFFF"
    color="#FFFFFF"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -100.16 -56.51)" fill="#000000" stroke="#000000"><foreignobject width="122.39"
    height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">无噪声纹理检测</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 33.01 -56.27)" fill="#000000" stroke="#000000"
    color="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -99.58 46.79)" fill="#000000" stroke="#000000"><foreignobject width="159.79"
    height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">瞳孔边界参数化</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 72.38 46.09)" fill="#000000" stroke="#000000"
    color="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 157.54 -29.5)" fill="#000000" stroke="#000000"><foreignobject width="118"
    height="59" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![请参阅标题](img/effc3c2666ee5f9be482169720f82465.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 27.84 69.38)" fill="#000000" stroke="#000000"><foreignobject
    width="181.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">虹膜分割主要任务</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 161.66 -42.73)" fill="#000000" stroke="#000000"><foreignobject
    width="110.55" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">无量纲无噪声</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 184.81 -53.55)" fill="#000000" stroke="#000000"><foreignobject
    width="63.71" height="8.5" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">表示</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.72 12.63)" fill="#FFFFFF" stroke="#FFFFFF"
    color="#FFFFFF"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 94.04 12.63)" fill="#000000" stroke="#000000" color="#000000"><foreignobject
    width="4.84" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 102.53 13.33)" fill="#000000" stroke="#000000"
    color="#000000"><foreignobject width="7.53" height="6.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">+</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 122.22 13.33)" fill="#000000" stroke="#000000" color="#000000"><foreignobject
    width="7.53" height="6.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">+</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 133.41 12.63)" fill="#000000" stroke="#000000"
    color="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
- en: 'Figure 1. Three main tasks typically associated to *iris segmentation*: 1)
    parameterization of the pupillary (inner) boundary; 2) parameterization of the
    scleric (outer) boundary; and 3) discrimination between the unoccluded (noise-free)
    and occluded (noisy) regions inside the iris ring. Such pieces of information
    are further used to obtain dimensionless polar representations of the iris texture,
    where feature extraction methods typically operate.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. 通常与*虹膜分割*相关的三个主要任务：1) 瞳孔（内）边界的参数化；2) 巩膜（外）边界的参数化；以及 3) 区分虹膜环内的非遮挡（无噪声）区域和遮挡（有噪声）区域。这些信息进一步用于获取虹膜纹理的无量纲极坐标表示，特征提取方法通常在此处操作。
- en: Schlett *et* al. (Schlett et al., [2018](#bib.bib145)) provided a multi-spectral
    analysis to improve iris segmentation accuracy in visible wavelengths by preprocessing
    data before the actual segmentation phase, extracting multiple spectral components
    in form of RGB color channels. Even though this approach does propose a DL-based
    framework, the different versions of the input could be easily used to feed DL-based
    models, and augment the robustness to non-ideal data. Chen *et* al. (Chen et al.,
    [2019a](#bib.bib23)) used CNNs that include dense blocks, referred to as a dense-fully
    convolutional network (DFCN), where the encoder part consists of dense blocks,
    and the decoder counterpart obtains the segmentation masks via transpose convolutions.
    Hofbauer *et* al. (Hofbauer et al., [2019](#bib.bib73)) parameterize the iris
    boundaries based on segmentation maps yielding from a CNN, using a a cascaded
    architecture with four RefineNet units, each directly connecting to one Residual
    net. Huynh *et* al. (Huynh et al., [2019](#bib.bib77)) discriminate between three
    distinct eye regions with a DL model, and removes incorrect areas with heuristic
    filters. The proposed architecture is based on the encoder-decoder model, with
    depth-wise convolutions used to reduce the computational cost. Roughly at the
    same time, Li *et* al. (Li et al., [2021](#bib.bib95)) described the *Interleaved
    Residual U-Net* model for semantic segmentation and iris mask synthesis. In this
    work, unsupervised techniques (K-means clustering) were used to create intermediary
    pictorial representations of the ocular region, from where saliency points deemed
    to belong to the iris boundaries were found. Kerrigan *et* al. (Kerrigan et al.,
    [2019](#bib.bib86)) assessed the performance of four different convolutional architectures
    designed for semantic segmentation. Two of these models were based in dilated
    convolutions, as proposed by Yu and Koltun (Y. and K., [2016](#bib.bib189)). Wu
    and Zhao (Wu and Zhao, [2019](#bib.bib187)) described the Dense U-Net model, that
    combines dense layers to the U-Net network. The idea is to take advantage of the
    reduced set of parameters of the dense U-Net, while keeping the semantic segmentation
    capabilities of U-Net. The proposed model integrates dense connectivity into U-Net
    contraction and expansion paths. Compared with traditional CNNs, this model is
    claimed to reduce learning redundancy and enhance information flow, while keeping
    controlled the number of parameters of the model. Wei *et* al. (Zhang et al.,
    [2019](#bib.bib206)) suggested to perform *dilated convolutions*, which is claimed
    to obtain more consistent global features. In this setting, convolutional kernels
    are not continuous, with zero-values being artificially inserted between each
    non-zero position, increasing the receptive field without augmenting the number
    of parameters of the model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Schlett *等*（Schlett et al., [2018](#bib.bib145)）提供了一种多光谱分析方法，通过在实际分割阶段之前对数据进行预处理，提取RGB颜色通道的多光谱成分，从而提高可见光波段的虹膜分割精度。尽管这种方法提出了一种基于DL的框架，但输入的不同版本可以轻松地用于喂给DL模型，并增强对非理想数据的鲁棒性。Chen
    *等*（Chen et al., [2019a](#bib.bib23)）使用了包括密集块的CNN，称为密集全卷积网络（DFCN），其中编码器部分由密集块组成，而解码器部分通过转置卷积获取分割掩膜。Hofbauer
    *等*（Hofbauer et al., [2019](#bib.bib73)）基于来自CNN的分割图，参数化虹膜边界，使用具有四个RefineNet单元的级联结构，每个单元直接连接到一个残差网络。Huynh
    *等*（Huynh et al., [2019](#bib.bib77)）通过DL模型区分三种不同的眼部区域，并用启发式滤波器去除错误区域。所提出的架构基于编码器-解码器模型，使用深度卷积来降低计算成本。大致在同一时间，Li
    *等*（Li et al., [2021](#bib.bib95)）描述了用于语义分割和虹膜掩膜合成的*交错残差U-Net*模型。在这项工作中，使用无监督技术（K均值聚类）创建眼部区域的中间图像表示，从中找到被认为属于虹膜边界的显著点。Kerrigan
    *等*（Kerrigan et al., [2019](#bib.bib86)）评估了四种不同卷积架构在语义分割中的表现。这些模型中的两个基于扩张卷积，如Yu和Koltun （Y.
    and K., [2016](#bib.bib189)）提出的。Wu和Zhao（Wu and Zhao, [2019](#bib.bib187)）描述了Dense
    U-Net模型，该模型将密集层与U-Net网络结合。其想法是利用Dense U-Net参数集的减少，同时保持U-Net的语义分割能力。所提出的模型将密集连接集成到U-Net的收缩和扩展路径中。与传统CNN相比，该模型被称为减少学习冗余并增强信息流，同时控制模型的参数数量。Wei
    *等*（Zhang et al., [2019](#bib.bib206)）建议执行*扩张卷积*，这被认为可以获得更一致的全局特征。在这种设置下，卷积核不连续，在每个非零位置之间人为插入零值，从而增加感受野而不增加模型的参数数量。
- en: 'Table 1. Cohesive comparison of the most relevant DL-based iris segmentation
    methods (NIR: *near-infrared*; VW: *visible wavelength*). Methods are listed in
    chronological (and then alphabetical) order.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1. 相关 DL 基于虹膜分割方法的综合比较（NIR: *近红外*; VW: *可见波长*）。方法按时间顺序（然后按字母顺序）列出。'
- en: '| Method | Year | Data | Datasets | Features |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 数据 | 数据集 | 特性 |'
- en: '| NIR | VW |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| NIR | VW |'
- en: '| Schlett *et* al. (Schlett et al., [2018](#bib.bib145)) | 2018 | ✗ | ✓ | MobBIO
    | Preprocessing (combines different possibilities of the input RGB channels) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 施莱特 *等* (Schlett et al., [2018](#bib.bib145)) | 2018 | ✗ | ✓ | MobBIO | 预处理（结合输入
    RGB 通道的不同可能性） |'
- en: '| Trokielewicz and Czajka (Trokielewicz and Czajka, [2018](#bib.bib167)) |
    2018 | ✓ | ✓ | Warsaw-Post-Mortem v1.0 | Fine-tuned CNN (SegNet) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 特罗基列维奇和查伊卡 (Trokielewicz and Czajka, [2018](#bib.bib167)) | 2018 | ✓ | ✓
    | Warsaw-Post-Mortem v1.0 | 微调 CNN (SegNet) |'
- en: '| Chen *et* al. (Chen et al., [2019a](#bib.bib23)) | 2019 | ✓ | ✓ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | Dense CNN |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 陈 *等* (Chen et al., [2019a](#bib.bib23)) | 2019 | ✓ | ✓ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | Dense CNN |'
- en: '| Hofbauer *et* al. (Hofbauer et al., [2019](#bib.bib73)) | 2019 | ✓ | ✗ |
    IITD, CASIA-Irisv4-Interval, ND-Iris-0405 | Cascaded architecture of four RefineNet,
    each connecting to one Residual net |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 霍夫鲍尔 *等* (Hofbauer et al., [2019](#bib.bib73)) | 2019 | ✓ | ✗ | IITD, CASIA-Irisv4-Interval,
    ND-Iris-0405 | 四个 RefineNet 的级联架构，每个连接到一个 Residual net |'
- en: '| Huynh *et* al. (Huynh et al., [2019](#bib.bib77)) | 2019 | ✓ | ✗ | OpenEDS
    | MobileNetV2 + heuristic filtering postproc. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 黄 *等* (Huynh et al., [2019](#bib.bib77)) | 2019 | ✓ | ✗ | OpenEDS | MobileNetV2
    + 启发式过滤后处理 |'
- en: '| Li *et* al. (Anisetti et al., [2019](#bib.bib8)) | 2019 | ✓ | ✗ | CASIA-Iris-Thousand
    | Faster-R-CNN (ROI detection) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 李 *等* (Anisetti et al., [2019](#bib.bib8)) | 2019 | ✓ | ✗ | CASIA-Iris-Thousand
    | Faster-R-CNN (ROI 检测) |'
- en: '| Kerrigan *et* al. (Kerrigan et al., [2019](#bib.bib86)) | 2019 | ✓ | ✓ |
    CASIA-Irisv4-Interval, BioSec, ND-Iris-0405, UBIRIS.v2, Warsaw-Post-Mortem v2.0,
    ND-TWINS-2009-2010 | Resent + Segnet (with dilated convolutions) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 凯里根 *等* (Kerrigan et al., [2019](#bib.bib86)) | 2019 | ✓ | ✓ | CASIA-Irisv4-Interval,
    BioSec, ND-Iris-0405, UBIRIS.v2, Warsaw-Post-Mortem v2.0, ND-TWINS-2009-2010 |
    Resent + Segnet（带膨胀卷积） |'
- en: '| Wu and Zhao (Wu and Zhao, [2019](#bib.bib187)) | 2019 | ✓ | ✓ | CASIA-Irisv4-Interval,
    UBIRIS.v2 | Dense-U-Net (dense layers + U-Net) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 吴和赵 (Wu and Zhao, [2019](#bib.bib187)) | 2019 | ✓ | ✓ | CASIA-Irisv4-Interval,
    UBIRIS.v2 | Dense-U-Net (密集层 + U-Net) |'
- en: '| Wei *et* al. (Zhang et al., [2019](#bib.bib206)) | 2019 | ✓ | ✓ | CASIA-Iris4-Interval,
    ND-IRIS-0405, UBIRIS.v2 | U-Net with dilated convolutions |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 魏 *等* (Zhang et al., [2019](#bib.bib206)) | 2019 | ✓ | ✓ | CASIA-Iris4-Interval,
    ND-IRIS-0405, UBIRIS.v2 | 带膨胀卷积的 U-Net |'
- en: '| Fang and Czajka (Fang and Czajka, [2020](#bib.bib51)) | 2020 | ✓ | ✓ | ND-Iris-0405,
    CASIA, BATH, BioSec, UBIRIS, Warsaw-Post-Mortem v1.0 & v2.0 | Fine-tuned CC-Net (Mishra
    et al., [2019](#bib.bib107)) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 方和查伊卡 (Fang and Czajka, [2020](#bib.bib51)) | 2020 | ✓ | ✓ | ND-Iris-0405,
    CASIA, BATH, BioSec, UBIRIS, Warsaw-Post-Mortem v1.0 & v2.0 | 微调 CC-Net (Mishra
    et al., [2019](#bib.bib107)) |'
- en: '| Ganeva and Myasnikov (Ganeeva and Myasnikov, [2020](#bib.bib56)) | 2020 |
    ✓ | ✗ | MMU | U-Net, LinkNet, and FC-DenseNet (performance comparison) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 加涅娃和米亚斯尼科夫 (Ganeeva and Myasnikov, [2020](#bib.bib56)) | 2020 | ✓ | ✗ | MMU
    | U-Net, LinkNet 和 FC-DenseNet（性能比较） |'
- en: '| Jalilian *et* al. (Jalilian et al., [2020](#bib.bib80)) | 2020 | ✓ | ✗ |  |
    RefineNet + morphological postprocessing |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 贾利安 *等* (Jalilian et al., [2020](#bib.bib80)) | 2020 | ✓ | ✗ |  | RefineNet
    + 形态学后处理 |'
- en: '| Sardar *et* al. (Sardar et al., [2020](#bib.bib143)) | 2020 | ✓ | ✓ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | Squeeze-Expand module + active learning (interactive segmentation)
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 萨达尔 *等* (Sardar et al., [2020](#bib.bib143)) | 2020 | ✓ | ✓ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | Squeeze-Expand 模块 + 主动学习（交互式分割） |'
- en: '| Trokielewicz *et* al. (Trokielewicz et al., [2020](#bib.bib173)) | 2020 |
    ✓ | ✓ | ND-Iris-0405, CASIA, BATH, BioSec, UBIRIS, Warsaw-Post-Mortem v1.0 & v2.0
    | Fined-tuned SegNet (Badrinarayanan et al., [2017](#bib.bib10)) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 特罗基列维奇 *等* (Trokielewicz et al., [2020](#bib.bib173)) | 2020 | ✓ | ✓ | ND-Iris-0405,
    CASIA, BATH, BioSec, UBIRIS, Warsaw-Post-Mortem v1.0 & v2.0 | 微调 SegNet (Badrinarayanan
    et al., [2017](#bib.bib10)) |'
- en: '| Wang et al (Wang et al., [2020b](#bib.bib179)) | 2020 | ✓ | ✓ | CASIA-Iris-M1-S1/S2/S3,
    MICHE-I | Hourglass network |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 王等 (Wang et al., [2020b](#bib.bib179)) | 2020 | ✓ | ✓ | CASIA-Iris-M1-S1/S2/S3,
    MICHE-I | Hourglass 网络 |'
- en: '| Wang *et* al. (Wang et al., [2020a](#bib.bib177)) | 2020 | ✓ | ✓ | CASIA-v4-Distance,
    UBIRIS.v2, MICHE-I | U-Net + multi-task attention net + postproc. (probabilistic
    masks priors & thresholding) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 王 *等* (Wang et al., [2020a](#bib.bib177)) | 2020 | ✓ | ✓ | CASIA-v4-Distance,
    UBIRIS.v2, MICHE-I | U-Net + 多任务注意力网 + 后处理（概率掩模先验 & 阈值处理） |'
- en: '| Li *et* al. (Li et al., [2021](#bib.bib95)) | 2021 | ✓ | ✗ | CASIA-Iris-Thousand
    | IRU-Net network |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 李 *等* (Li et al., [2021](#bib.bib95)) | 2021 | ✓ | ✗ | CASIA-Iris-Thousand
    | IRU-Net 网络 |'
- en: '| Wang *et* al. (Wang et al., [2021](#bib.bib185)) | 2021 | ✗ | ✓ | Online
    Video Streams and Internet Videos | U-Net and Squeezenet to iris segmentation
    and detect eye closure |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 王*等*人（Wang et al.，[2021](#bib.bib185)） | 2021 | ✗ | ✓ | 在线视频流和互联网视频 | 使用
    U-Net 和 Squeezenet 进行虹膜分割和眼睛闭合检测 |'
- en: '| Kuehlkamp et al. (Kuehlkamp et al., [2022](#bib.bib92)) | 2022 | ✓ | ✓ |
    ND-Iris-0405, CASIA, BATH, BioSec, UBIRIS, Warsaw-Post-Mortem v2.0 | Fined-tuning
    of Mask-RCNN architecture |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Kuehlkamp *等*人（Kuehlkamp et al.，[2022](#bib.bib92)） | 2022 | ✓ | ✓ | ND-Iris-0405,
    CASIA, BATH, BioSec, UBIRIS, Warsaw-Post-Mortem v2.0 | Mask-RCNN 架构的精细调整 |'
- en: More recently, Ganeva and Myasnikov (Ganeeva and Myasnikov, [2020](#bib.bib56))
    compared the effectiveness of three convolutional neural network architectures
    (U-Net, LinkNet, and FC- DenseNet), determining the optimal parameterization for
    each one. Jalilian *et* al. (Jalilian et al., [2020](#bib.bib80)) introduced a
    scheme to compensate for texture deformations caused by the off-angle distortions,
    re-projecting the off-angle images back to frontal view. The used architecture
    is a variant of RefineNet (Lin et al., [2016](#bib.bib97)), which provides high-resolution
    prediction, while preserving the boundary information (required for parameterization
    purposes).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Ganeva 和 Myasnikov（Ganeeva and Myasnikov，[2020](#bib.bib56)）比较了三种卷积神经网络架构（U-Net、LinkNet
    和 FC-DenseNet）的有效性，确定了每种架构的最佳参数化。Jalilian *等*人（Jalilian et al.，[2020](#bib.bib80)）提出了一种补偿因角度畸变引起的纹理变形的方案，将角度畸变图像重新投影回正视图。使用的架构是
    RefineNet（Lin et al.，[2016](#bib.bib97)）的一个变体，提供了高分辨率预测，同时保留了边界信息（用于参数化目的）。
- en: The idea of interactive learning for iris segmentation was suggested by Sardar
    *et* al. (Sardar et al., [2020](#bib.bib143)), describing an interactive variant
    of U-Net that includes Squeeze Expand modules. Trokielewicz *et* al. (Trokielewicz
    et al., [2020](#bib.bib173)) used DL-based iris segmentation models to extract
    highly irregular iris texture areas in post-mortem iris images. They used a pre-trained
    SegNet model, fine-tuned with a database of cadaver iris images. Wang *et* al. (Wang
    et al., [2020b](#bib.bib179)) (further extended in (Wang et al., [2019b](#bib.bib180)))
    described a lightweight deep convolutional neural network specifically designed
    for iris segmentation of degraded images acquired by handheld devices. The proposed
    approach jointly obtains the segmentation mask and parameterized pupillary/limbic
    boundaries of the iris.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 用于虹膜分割的互动学习理念由 Sardar *等*人（Sardar et al.，[2020](#bib.bib143)）提出，描述了一种包含 Squeeze
    Expand 模块的 U-Net 互动变体。Trokielewicz *等*人（Trokielewicz et al.，[2020](#bib.bib173)）使用基于深度学习的虹膜分割模型来提取尸检虹膜图像中的高度不规则虹膜纹理区域。他们使用了一个经过预训练的
    SegNet 模型，并用尸体虹膜图像数据库进行了微调。王*等*人（Wang et al.，[2020b](#bib.bib179)）（在（Wang et al.，[2019b](#bib.bib180)）中进一步扩展）描述了一种专门为手持设备获取的退化图像设计的轻量级深度卷积神经网络。该方法联合获取虹膜的分割掩码和参数化的瞳孔/角膜边界。
- en: Observing that edge-based information is extremely sensitive to be obtained
    in degraded data, Li *et* al. (Anisetti et al., [2019](#bib.bib8)) presented an
    hybrid method that combines edge-based information to deep learning frameworks.
    A compacted Faster R-CNN-like architecture was used to roughly detect the eye
    and define the initial region of interest, from where the pupil is further located
    using a Gaussian mixture model. Wang *et* al. (Wang et al., [2021](#bib.bib185))
    trained a deep convolutional neural network(DCNN) that automatically extracts
    the iris and pupil pixels of each eye from input images. This work combines the
    power of U-Net and SqueezeNet to obtain a compact CNN suitable for real time mobile
    applications. Finally, Wang *et* al. (Wang et al., [2020a](#bib.bib177)) parameterize
    both the iris mask and the inner/outer iris boundaries jointly, by actively modeling
    such information into a unified multi-task network.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到边缘信息在退化数据中极其敏感，Li *等*人（Anisetti et al.，[2019](#bib.bib8)）提出了一种结合边缘信息和深度学习框架的混合方法。使用了类似
    Faster R-CNN 的紧凑架构来大致检测眼睛并定义初始兴趣区域，然后通过高斯混合模型进一步定位瞳孔。王*等*人（Wang et al.，[2021](#bib.bib185)）训练了一个深度卷积神经网络（DCNN），自动提取输入图像中每只眼睛的虹膜和瞳孔像素。这项工作结合了
    U-Net 和 SqueezeNet 的优势，获得了适用于实时移动应用的紧凑 CNN。最后，王*等*人（Wang et al.，[2020a](#bib.bib177)）通过将这些信息建模到一个统一的多任务网络中，联合参数化虹膜掩码和内/外虹膜边界。
- en: A final word is given to *segmentation-less* techniques. Assuming that the accurate
    segmentation of the iris boundaries is one of the hardest phases of the whole
    recognition chain and the main source for recognition errors, some recent works
    have been proposing to perform biometrics recognition in non-segmented or roughly
    segmented data (Proença and Neves, [2017](#bib.bib133))(Proença and Neves, [2019](#bib.bib136)).
    Here, the idea is to use the remarkable discriminating power of DL-frameworks
    to perceive the agreeing patterns between pairs of images, even on such *segmentation-less*
    representations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要提到的是*无分割*技术。假设虹膜边界的准确分割是整个识别链中最困难的阶段之一，并且是识别错误的主要来源，一些近期的研究提出在未分割或粗略分割的数据中进行生物特征识别（Proença和Neves，[2017](#bib.bib133)）（Proença和Neves，[2019](#bib.bib136)）。在这里，思想是利用深度学习框架的显著区分能力来感知图像对之间的一致模式，即使在*无分割*表示中也是如此。
- en: 3\. Deep Learning-Based Iris Recognition
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 基于深度学习的虹膜识别
- en: 3.1\. Deep Learning Models as a Feature Extractor
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 深度学习模型作为特征提取器
- en: <svg   height="158.59" overflow="visible" version="1.1" width="442.05"><g transform="translate(0,158.59)
    matrix(1 0 0 -1 0 0) translate(128.51,0) translate(0,79.29)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.16 55.6)" fill="#000000"
    stroke="#000000"><foreignobject width="203.56" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">DL-based Feature Representation</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -78.84 -34.5)" fill="#000000" stroke="#000000"><foreignobject
    width="138" height="69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/6fe2772c84903e1f3319d6c908feb205.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -64.72 -42.73)" fill="#000000" stroke="#000000"><foreignobject width="110.55"
    height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Dimensionless
    Noise-Free</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -41.56 -53.55)"
    fill="#000000" stroke="#000000"><foreignobject width="63.71" height="8.5" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Representation</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 245.39 -48.58)" fill="#000000" stroke="#000000"><foreignobject width="48.59"
    height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Feature Set</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 258.8 -22.21)" fill="#000000" stroke="#000000"><foreignobject
    width="17.84" height="44.42" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><mathalttext="\left.\begin{bmatrix}f_{1}\\
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="158.59" overflow="visible" version="1.1" width="442.05"><g transform="translate(0,158.59)
    matrix(1 0 0 -1 0 0) translate(128.51,0) translate(0,79.29)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.16 55.6)" fill="#000000"
    stroke="#000000"><foreignobject width="203.56" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">基于深度学习的特征表示</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -78.84 -34.5)" fill="#000000" stroke="#000000"><foreignobject width="138"
    height="69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![参见说明](img/6fe2772c84903e1f3319d6c908feb205.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -64.72 -42.73)" fill="#000000" stroke="#000000"><foreignobject
    width="110.55" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">无量纲噪声自由</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -41.56 -53.55)" fill="#000000" stroke="#000000"><foreignobject
    width="63.71" height="8.5" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">表示</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 245.39 -48.58)" fill="#000000" stroke="#000000"><foreignobject
    width="48.59" height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">特征集</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 258.8 -22.21)" fill="#000000" stroke="#000000"><foreignobject
    width="17.84" height="44.42" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><mathalttext="\left.\begin{bmatrix}f_{1}\\
- en: f_{2}\\
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: f_{2}\\
- en: \vdots\\
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: f_{t}\\
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: f_{t}\\
- en: \end{bmatrix}\right." display="inline"><semantics ><mrow ><mo >[</mo><mtable
    rowspacing="0pt" ><mtr ><mtd ><msub ><mi mathsize="90%" >f</mi><mn mathsize="90%"
    >1</mn></msub></mtd></mtr><mtr ><mtd ><msub ><mi mathsize="90%" >f</mi><mn mathsize="90%"
    >2</mn></msub></mtd></mtr><mtr ><mtd ><mi mathsize="90%" mathvariant="normal"
    >⋮</mi></mtd></mtr><mtr ><mtd ><msub ><mi mathsize="90%" >f</mi><mi mathsize="90%"
    >t</mi></msub></mtd></mtr></mtable><mo >]</mo></mrow><annotation-xml encoding="MathML-Content"
    ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix ><matrixrow ><apply><csymbol
    cd="ambiguous" >subscript</csymbol><ci>𝑓</ci><cn type="integer" >1</cn></apply></matrixrow><matrixrow
    ><apply><csymbol cd="ambiguous" >subscript</csymbol><ci>𝑓</ci><cn type="integer"
    >2</cn></apply></matrixrow><matrixrow ><ci>⋮</ci></matrixrow><matrixrow ><apply><csymbol
    cd="ambiguous" >subscript</csymbol><ci>𝑓</ci><ci>𝑡</ci></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\left.\begin{bmatrix}f_{1}\\ f_{2}\\ \vdots\\ f_{t}\\
    \end{bmatrix}\right.</annotation></semantics></math></foreignobject></g></g></svg>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \end{bmatrix}\right." display="inline"><semantics ><mrow ><mo >[</mo><mtable
    rowspacing="0pt" ><mtr ><mtd ><msub ><mi mathsize="90%" >f</mi><mn mathsize="90%"
    >1</mn></msub></mtd></mtr><mtr ><mtd ><msub ><mi mathsize="90%" >f</mi><mn mathsize="90%"
    >2</mn></msub></mtd></mtr><mtr ><mtd ><mi mathsize="90%" mathvariant="normal"
    >⋮</mi></mtd></mtr><mtr ><mtd ><msub ><mi mathsize="90%" >f</mi><mi mathsize="90%"
    >t</mi></msub></mtd></mtr></mtable><mo >]</mo></mrow><annotation-xml encoding="MathML-Content"
    ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix ><matrixrow ><apply><csymbol
    cd="ambiguous" >subscript</csymbol><ci>𝑓</ci><cn type="integer" >1</cn></apply></matrixrow><matrixrow
    ><apply><csymbol cd="ambiguous" >subscript</csymbol><ci>𝑓</ci><cn type="integer"
    >2</cn></apply></matrixrow><matrixrow ><ci>⋮</ci></matrixrow><matrixrow ><apply><csymbol
    cd="ambiguous" >subscript</csymbol><ci>𝑓</ci><ci>𝑡</ci></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\left.\begin{bmatrix}f_{1}\\ f_{2}\\ \vdots\\ f_{t}\\
    \end{bmatrix}\right.</annotation></semantics></math></foreignobject></g></g></svg>
- en: 'Figure 2. The main task of DL-based iris feature extraction: given a dimensionless
    representation of the iris data, obtain its compact and representative representation
    - the feature set - that is further used in the classification phase.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. 基于深度学习的虹膜特征提取的主要任务：给定虹膜数据的无量纲表示，获得其紧凑且具有代表性的表示——特征集——这在分类阶段进一步使用。
- en: 'As illustrated in Fig. [2](#S3.F2 "Figure 2 ‣ 3.1\. Deep Learning Models as
    a Feature Extractor ‣ 3\. Deep Learning-Based Iris Recognition ‣ Deep Learning
    for Iris Recognition: A Survey"), the idea here is to analyze a dimensionless
    representation of the iris data and produce a feature vector that lies in a hyperspace
    (embedding) where recognition is carried out.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [2](#S3.F2 "Figure 2 ‣ 3.1\. Deep Learning Models as a Feature Extractor
    ‣ 3\. Deep Learning-Based Iris Recognition ‣ Deep Learning for Iris Recognition:
    A Survey") 所示，这里的思路是分析虹膜数据的无量纲表示，并生成一个位于超空间（嵌入）中的特征向量，在此空间中进行识别。'
- en: In this context, Boyd *et* el. (Boyd et al., [2019](#bib.bib16)) explored five
    different sets of weights for the popular ResNet50 architecture to test if iris-specific
    feature extractors perform better than models trained for general tasks. Minaee
    *et* al. (Minaee et al., [2016](#bib.bib106)) studied the application of deep
    features extracted from VGG-Net for iris recognition, having authors observed
    that the resulting features can be well transferred to biometric recognition.
    Luo *et* al. (Luo et al., [2021](#bib.bib103)) described a DL model with spatial
    attention and channel attention mechanisms, that are directly inserted into the
    feature extraction module. Also, a co-attention mechanism adaptively fuses features
    to obtain representative iris-periocular features. Hafner *et* al. (Hafner et al.,
    [2021](#bib.bib66)) adapted the classical Daugman’s pipeline, using convolutional
    neural networks to function as feature extractors. The DenseNet-201 architecture
    outperformed its competitors achieving state-of- the-art results both in the open
    and close world settings. Menotti *et* al. (Menotti et al., [2015](#bib.bib105))
    assessed how DL-based feature representations can be used in spoofing detection,
    observing that spoofing detection systems based on CNNs can be robust to attacks
    already known and adapted, with little effort, to image-based attacks that are
    yet to come.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Boyd *等人* (Boyd et al., [2019](#bib.bib16)) 探索了流行的 ResNet50 架构的五组不同权重，以测试特定于虹膜的特征提取器是否比为一般任务训练的模型表现更好。Minaee
    *等人* (Minaee et al., [2016](#bib.bib106)) 研究了从 VGG-Net 中提取的深度特征在虹膜识别中的应用，作者观察到这些特征可以很好地迁移到生物识别任务中。Luo
    *等人* (Luo et al., [2021](#bib.bib103)) 描述了一个具有空间注意力和通道注意力机制的深度学习模型，这些机制被直接插入到特征提取模块中。此外，一个共注意力机制自适应地融合特征，以获得具有代表性的虹膜-周边特征。Hafner
    *等人* (Hafner et al., [2021](#bib.bib66)) 采用了经典的 Daugman 流程，使用卷积神经网络作为特征提取器。DenseNet-201
    架构在开放世界和封闭世界环境中都表现出色，达到了最先进的结果。Menotti *等人* (Menotti et al., [2015](#bib.bib105))
    评估了基于深度学习的特征表示如何在欺骗检测中使用，观察到基于 CNN 的欺骗检测系统对已知攻击具有鲁棒性，并且能够以较少的努力适应未来的基于图像的攻击。
- en: Yang *et* al. (Yang et al., [2021](#bib.bib197)) generated multi-level spatially
    corresponding feature representations by an encoder-decoder structure. Also, a
    spatial attention feature fusion module was used to ensemble the resulting features
    more effectively. Chen *et* al. (Chen et al., [2020](#bib.bib24)) addressed the
    large-scale recognition problem and described an optimized center loss function
    (tight center) to attenuate the insufficient discriminating power of the cross-entropy
    function. Nguyen *et* al. (Nguyen et al., [2017b](#bib.bib113)) explored the performance
    of state-of-the-art pre-trained CNNs on iris recognition, concluding that off-the-shelf
    CNN generic features are also extremely good at representing iris images, effectively
    extracting discriminative visual features and achieving promising results. Zhao
    *et* al. (Zhao et al., [2019](#bib.bib208)) proposed a method based on the capsule
    network architecture, where a modified routing algorithm based on the dynamic
    routing between two capsule layers was described, with three pre-trained models
    (VGG16, InceptionV3, and ResNet50) extracting the primary iris features. Next,
    a convolution capsule replaces the full connection capsule to reduce the number
    of parameters. Wang and Kumar (Wang and Kumar, [2019](#bib.bib181)) introduced
    the concept of *residual feature* for iris recognition. They described a residual
    network learning procedure with offline triplets selection and dilated convolutional
    kernels.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Yang *等人* (Yang et al., [2021](#bib.bib197)) 通过编码器-解码器结构生成了多层级的空间对应特征表示。此外，使用了空间注意力特征融合模块，以更有效地集成结果特征。Chen
    *等人* (Chen et al., [2020](#bib.bib24)) 解决了大规模识别问题，并描述了一种优化的中心损失函数（紧凑中心），以减轻交叉熵函数的不足判别能力。Nguyen
    *等人* (Nguyen et al., [2017b](#bib.bib113)) 探索了最先进的预训练 CNN 在虹膜识别中的性能，得出结论认为现成的
    CNN 通用特征在表示虹膜图像方面也非常出色，有效提取了判别性视觉特征并取得了良好的结果。Zhao *等人* (Zhao et al., [2019](#bib.bib208))
    提出了基于胶囊网络架构的方法，其中描述了一种基于两个胶囊层之间动态路由的修改路由算法，并使用三个预训练模型（VGG16、InceptionV3 和 ResNet50）提取主要虹膜特征。接下来，一个卷积胶囊取代了全连接胶囊，以减少参数数量。Wang
    和 Kumar (Wang and Kumar, [2019](#bib.bib181)) 引入了虹膜识别的*残差特征*概念。他们描述了一种带有离线三元组选择和扩张卷积核的残差网络学习程序。
- en: 'Other works have addressed the extraction of appropriate feature representations
    in multi-biometrics settings: Damer *et* al. (Damer et al., [2019](#bib.bib33))
    propose to jointly extract multi-biometric representations within a single DNN.
    Unlike previous solutions that create independent representations from each biometric
    modality, they create these representations from multi-modality (face and iris),
    multi-instance (iris left and right), and multi- presentation (two face samples),
    which can be seen as a fusion at the data level policy. Finally, concerned about
    the difficulty of performing reliable recognition in hand-held devices, Odinokikh
    *et* al. (Odinokikh et al., [2019](#bib.bib122)) combined the advantages of handcrafted
    feature extractors and advanced deep learning techniques. The model utilizes shallow
    and deep feature representations in combination with characteristics describing
    the environment, to reduce the intra-subject variations expected in this kind
    of environments.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究已经探讨了在多生物特征设置中提取适当特征表示的方法：Damer *et* al.（Damer et al., [2019](#bib.bib33)）提议在单一DNN中联合提取多生物特征表示。与以往从每个生物特征模态中创建独立表示的方法不同，他们从多模态（面部和虹膜）、多实例（左侧和右侧虹膜）以及多样本（两个面部样本）中创建这些表示，这可以视为数据层面的融合策略。最后，考虑到在手持设备上执行可靠识别的困难，Odinokikh
    *et* al.（Odinokikh et al., [2019](#bib.bib122)）结合了手工特征提取器和先进深度学习技术的优点。该模型利用浅层和深层特征表示，结合描述环境的特征，以减少这种环境中预期的主体内变异。
- en: 3.2\. Deep Learning-based Iris Matching Strategies
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 基于深度学习的虹膜匹配策略
- en: 'The existing matching strategies can be categorized into three categories:
    (1) using conventional classifiers, such as SVM, RF, and Sparse Representation;
    (2) softmax-based losses; and (3) pairwise-based losses. A cohesive perspective
    of the most relevant recent DL-based methods is given in Table 2, with the techniques
    appearing in chronographic (and then alphabetical) order'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的匹配策略可以分为三类：（1）使用传统分类器，如SVM、RF和稀疏表示；（2）基于softmax的损失；（3）基于对的损失。表2提供了最相关的近期基于深度学习的方法的凝练视角，技术按时间顺序（然后按字母顺序）排列。
- en: 3.2.1\. Conventional classifiers
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 传统分类器
- en: Various researchers have been using deep learning networks designed and pre-trained
    on the ImageNet dataset to extract iris feature representations, followed by a
    conventional classifier such as SVM, RF, Sparse Representation, etc. (Nguyen et al.,
    [2017b](#bib.bib113); Boyd et al., [2019](#bib.bib16); Boyd et al., [2020b](#bib.bib19)).
    The key benefit of these approaches is the simplicity of “plug and play”, where
    proven and pre-trained deep learning networks inherited from large-scale computer
    vision challenges are widely available and ready to be used (Nguyen et al., [2017b](#bib.bib113)).
    Another benefit is that there is no need for large scale iris image datasets to
    train these networks because they have already been trained on such large-scale
    datasets as ImageNet. Considering these networks usually contain hundreds of layers
    and millions of parameters, and require millions of images to train, using pre-trained
    networks is extremely beneficial.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 各种研究人员已使用在ImageNet数据集上设计和预训练的深度学习网络来提取虹膜特征表示，随后使用传统分类器，如SVM、RF、稀疏表示等（Nguyen
    et al., [2017b](#bib.bib113)；Boyd et al., [2019](#bib.bib16)；Boyd et al., [2020b](#bib.bib19)）。这些方法的关键好处是“即插即用”的简单性，其中从大规模计算机视觉挑战中继承来的经过验证和预训练的深度学习网络广泛可用并准备使用（Nguyen
    et al., [2017b](#bib.bib113)）。另一个好处是，无需大规模的虹膜图像数据集来训练这些网络，因为它们已经在如ImageNet这样的超大规模数据集上进行过训练。考虑到这些网络通常包含数百层和数百万个参数，并需要数百万张图像进行训练，使用预训练网络极为有利。
- en: 3.2.2\. Iris Classification Networks
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 虹膜分类网络
- en: Iris classification networks couple deep learning architectures with a family
    of softmax-based losses to classify an iris image into a list of known identities.
    Coupling a softmax loss with a backbone network enables training the backbone
    network in an end-to-end manner via popular optimization strategies such as back-propagation
    and steepest gradient decent. Compared to the conventional classifier approaches,
    the DL-based backbones in this category are learnable directly from the iris data,
    allowing them to better represent the iris. The key benefit is that it is similar
    to a generic image classification task, hence all designs and algorithms in the
    generic image classification task can be trivially applied with the iris image
    data. Typical examples of these iris classification networks are (Gangwar and
    Joshi, [2016](#bib.bib57); Boyd et al., [2019](#bib.bib16)). However, these softmax-based
    networks require the iris in the test image be known in the identity classes in
    the training set, which means the networks must be re-trained whenever a new class
    (*i.e.* a new identity) is added. Gangwar *et al.* proposed two backbone networks
    (*i.e.* DeepIrisNet-A and DeepIrisNet-B) followed by a softmax loss for the iris
    recognition task (Gangwar and Joshi, [2016](#bib.bib57)). Later, they proposed
    another backbone network, but still followed by a softmax loss to classify one
    normalized iris image into a pre-defined list of identity (Gangwar et al., [2019](#bib.bib58)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虹膜分类网络将深度学习架构与一系列基于 softmax 的损失函数结合，以将虹膜图像分类到已知身份列表中。将 softmax 损失与主干网络结合使得通过常用的优化策略如反向传播和最陡梯度下降，能够以端到端的方式训练主干网络。与传统的分类器方法相比，这类基于
    DL 的主干网络可以直接从虹膜数据中学习，使其更好地表示虹膜。关键好处是，它类似于通用图像分类任务，因此通用图像分类任务中的所有设计和算法都可以简单地应用于虹膜图像数据。这些虹膜分类网络的典型示例有（Gangwar
    和 Joshi，[2016](#bib.bib57)；Boyd 等，[2019](#bib.bib16)）。然而，这些基于 softmax 的网络要求测试图像中的虹膜在训练集中已知的身份类中，这意味着每当添加新类别（*即*
    新身份）时，网络必须重新训练。Gangwar *等人* 提出了两个主干网络（*即* DeepIrisNet-A 和 DeepIrisNet-B），随后使用
    softmax 损失进行虹膜识别任务（Gangwar 和 Joshi，[2016](#bib.bib57)）。后来，他们提出了另一个主干网络，但仍然跟随 softmax
    损失将一个标准化的虹膜图像分类到预定义的身份列表中（Gangwar 等，[2019](#bib.bib58)）。
- en: 'Backbone Network Architectures: A wide range of backbone network architectures
    have been borrowed from generic image classification for the iris recognition
    task due to their similarity.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 主干网络架构：由于其相似性，虹膜识别任务中借用了广泛的通用图像分类主干网络架构。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'AlexNet: AlexNet is the most primitive and been shown as least accurate for
    iris recognition compared to others (Boyd et al., [2020a](#bib.bib17); Nguyen
    et al., [2017b](#bib.bib113)).'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: AlexNet：与其他方法相比，AlexNet 是最原始的且被证明在虹膜识别中准确性最低（Boyd 等，[2020a](#bib.bib17)；Nguyen
    等，[2017b](#bib.bib113)）。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VGG: Boyd *et al.* (Boyd et al., [2020a](#bib.bib17)), Nguyen *et al.* (Nguyen
    et al., [2017b](#bib.bib113)) and Minaee *et al.* (Minaee et al., [2016](#bib.bib106))
    all experimented VGG16 .'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VGG：Boyd *等人*（Boyd 等，[2020a](#bib.bib17)）、Nguyen *等人*（Nguyen 等，[2017b](#bib.bib113)）和
    Minaee *等人*（Minaee 等，[2016](#bib.bib106)）都实验了 VGG16。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ResNet: ResNet with its variants are the most popular backbone network architecture.
    Nguyen *et al.* experimented ResNet152 (Nguyen et al., [2017b](#bib.bib113)).
    Boyd *et al.* experimented three variants ResNet18, ResNet50 and ResNet152 in
    their post-mortem iris classification task (Boyd et al., [2020a](#bib.bib17)).'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ResNet：ResNet 及其变体是最受欢迎的主干网络架构。Nguyen *等人* 实验了 ResNet152（Nguyen 等，[2017b](#bib.bib113)）。Boyd
    *等人* 在他们的尸检虹膜分类任务中实验了三个变体 ResNet18、ResNet50 和 ResNet152（Boyd 等，[2020a](#bib.bib17)）。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inception: Zhao *et al.* employed capsule network based on the InceptionV3
    architecture (Zhao et al., [2019](#bib.bib208)).'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Inception：Zhao *等人* 基于 InceptionV3 架构使用了胶囊网络（Zhao 等，[2019](#bib.bib208)）。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'EfficientNet: Hsiao *et al.* (Hsiao and Fan, [2021](#bib.bib75)) employed EfficientNet
    to extract iris features.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: EfficientNet：Hsiao *等人*（Hsiao 和 Fan，[2021](#bib.bib75)）使用 EfficientNet 提取虹膜特征。
- en: 3.2.3\. Iris Similarity Networks
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 虹膜相似度网络
- en: 'Iris similarity networks couple deep learning architectures with a family of
    pairwise-based losses to learn a metric representing how similar or dissimilar
    two iris images are without knowing their identities. The pairwise loss aims to
    pull images of the same iris closer and push images of different irises away in
    the similarity distance space. Different to the iris classification networks which
    only operate in an identification mode on a pre-defined identity list, iris similarity
    networks operate across both verification and identification modes with an open
    set of identities (Zhao and Kumar, [2017b](#bib.bib210)). Typical examples of
    these iris similarity networks are (Liu et al., [2016b](#bib.bib98); Zhao and
    Kumar, [2017b](#bib.bib210); Wang and Kumar, [2019](#bib.bib181); Nguyen et al.,
    [2020](#bib.bib114); Jalilian et al., [2022](#bib.bib81)). There are three key
    benefits of these networks: (i) verification and identification: iris similarity
    networks operate across both verification and identification modes; (ii) open
    set of identities: iris similarity networks operate on an open set of identities;
    and (iii) explicit reflection: iris similarity networks directly and explicitly
    reflect what we want to achieve, *i.e.,* small distances between irises of the
    same subject and larger distances between irises of different subjects.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 虹膜相似性网络将深度学习架构与一系列基于成对的损失函数结合起来，以学习表示两张虹膜图像的相似或不相似的度量，而无需知道其身份。成对损失旨在将相同虹膜的图像拉近，而将不同虹膜的图像推远于相似性距离空间。不同于仅在预定义身份列表上进行识别模式操作的虹膜分类网络，虹膜相似性网络在开放身份集合中操作，包括验证和识别模式（赵和库马，[2017b](#bib.bib210)）。这些虹膜相似性网络的典型例子有（刘等人，[2016b](#bib.bib98)；赵和库马，[2017b](#bib.bib210)；王和库马，[2019](#bib.bib181)；阮等人，[2020](#bib.bib114)；贾利安等人，[2022](#bib.bib81)）。这些网络有三个主要优点：（i）验证和识别：虹膜相似性网络在验证和识别模式下均可操作；（ii）开放身份集合：虹膜相似性网络在开放身份集合中操作；（iii）明确反映：虹膜相似性网络直接且明确地反映我们想要实现的目标，*即，*相同主体的虹膜之间距离较小，而不同主体的虹膜之间距离较大。
- en: 'Pairwise loss: Nianfeng *et al.* (Liu et al., [2016b](#bib.bib98)) proposed
    a pairwise network, which accepts two input images and directly outputs a similarity
    score. They designed a pairwise layer which accepts two input images and encodes
    their features via a backbone network. The backbone network is trained iteratively
    to minimize the dissimilarity distance between genuine pairs (pairs of the same
    identity) and maximize the dissimilarity distance between impostor pairs (pairs
    of the different identities).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 成对损失：年丰*等人*（刘等人，[2016b](#bib.bib98)）提出了一种成对网络，该网络接收两张输入图像并直接输出相似性分数。他们设计了一个成对层，该层接收两张输入图像并通过骨干网络编码其特征。骨干网络通过迭代训练以最小化真实配对（相同身份的配对）之间的不相似距离，并最大化冒充配对（不同身份的配对）之间的不相似距离。
- en: 'Triplet loss: Since the pairwise network is trained with separate genuine and
    impostor pairs, it may not converge well, which has been proven in the face recognition
    (Schroff et al., [2015](#bib.bib146)). Rather than using one pair of two images
    to update the training as in the pairwise loss for each training iteration, the
    triplet loss employs a triplet of three images: an anchor image, a positive image
    with the same identity and a negative image with a different identity (Schroff
    et al., [2015](#bib.bib146)). The backbone network is trained to simultaneously
    minimize the similarity distance between the positive and the anchor images and
    maximize the distance between the negative and the anchor images. Tailored for
    iris images, Zhao *et al.* (Zhao and Kumar, [2017b](#bib.bib210); Wang and Kumar,
    [2019](#bib.bib181); Zhao and Kumar, [2019](#bib.bib212)) proposed Extended Triplet
    Loss (EPL) to incorporate a bit-shifting operation to deal with rotation in the
    normalized iris images. Nguyen *et al.* also employed the ETL for their iris recognition
    network (Nguyen et al., [2020](#bib.bib114), [2022](#bib.bib116)). Kuehlkamp *et
    al.* (Kuehlkamp et al., [2022](#bib.bib92)) proposed to improve the generic triplet
    loss function for iris recognition by forcing the distance to be positive (through
    the use of a sigmoid output layer), and adding a logarithmic penalty to the error.
    This modification allows the network to learn even when the difference between
    samples is negative and converge faster. Yan *et al.* (Yan et al., [2021](#bib.bib196))
    extended the generic triplet loss to batch triplet loss, in which the triplet
    loss is calculated over a batch of $S$ subjects and $K$ images for each subject.
    Performing batch triplet loss is usually expected to have smooth loss function.
    Yang *et al.* (Yang et al., [2021](#bib.bib197)) improved triplet selection method
    for training by Batch Hard (Yuan et al., [2020](#bib.bib198)).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 三元组损失：由于配对网络使用不同的真实对和假冒对进行训练，因此可能无法很好地收敛，这在面部识别中已经得到证明（Schroff 等，[2015](#bib.bib146)）。与在每次训练迭代中使用一对两张图像来更新训练的配对损失不同，三元组损失使用三张图像的三元组：一个锚图像，一个具有相同身份的正样本图像和一个具有不同身份的负样本图像（Schroff
    等，[2015](#bib.bib146)）。骨干网络被训练以同时最小化正样本图像与锚图像之间的相似度距离，并最大化负样本图像与锚图像之间的距离。专门针对虹膜图像，赵*等*（赵和库马，[2017b](#bib.bib210)；王和库马，[2019](#bib.bib181)；赵和库马，[2019](#bib.bib212)）提出了扩展三元组损失（EPL），以结合位移操作处理标准化虹膜图像中的旋转。Nguyen
    *等* 也使用了 ETL 进行虹膜识别网络（Nguyen 等，[2020](#bib.bib114)，[2022](#bib.bib116)）。Kuehlkamp
    *等*（Kuehlkamp 等，[2022](#bib.bib92)）提出通过强制距离为正（通过使用 sigmoid 输出层）并向误差添加对数惩罚来改进通用三元组损失函数。这一修改允许网络在样本之间的差异为负时仍能学习并更快收敛。Yan
    *等*（Yan 等，[2021](#bib.bib196)）将通用三元组损失扩展到批量三元组损失，其中三元组损失是在 $S$ 个主体和每个主体的 $K$ 张图像上计算的。进行批量三元组损失通常期望具有平滑的损失函数。Yang
    *等*（Yang 等，[2021](#bib.bib197)）通过批量硬（Yuan 等，[2020](#bib.bib198)）改进了训练的三元组选择方法。
- en: 'Backbone Network Architectures: Different to the classification iris networks,
    similarity iris networks are usually designed with their own network architectures
    and are usually much “shallower” than the classification counterparts.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 骨干网络架构：与分类虹膜网络不同，相似度虹膜网络通常设计有其自身的网络架构，并且通常比分类网络要“浅”得多。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FCN: All similarity iris networks employ Fully Convolutional Networks (FCNs)
    instead of CNNs. Compared to CNNs, FCNs (Long et al., [2015](#bib.bib101)) do
    not have fully connected layer, allowing the output map to preserve the original
    spatial information. This is important to iris recognition since the output map
    can preserve spatial correspondence with the original input image (Zhao and Kumar,
    [2017b](#bib.bib210); Nguyen et al., [2020](#bib.bib114)), thus enabling pixel-to-pixel
    matching. Zhao *et al.* (Zhao and Kumar, [2017b](#bib.bib210)) proposed a FCN
    architecture with 3 convolutional layers, followed by activation and pooling layers.
    Outputs of convolutional layers are up-sampled to the original input image size.
    The up-samples features are stacked and convolved by another convolutional layer
    to generate a 2-dimension features with the same size as the input image. Later,
    they extended the backbone network with dilated convolutions (Wang and Kumar,
    [2019](#bib.bib181)). Yan *et al.* (Yan et al., [2021](#bib.bib196)) employed
    a ResNet architecture and fine-tuned it with the triplet loss. Kuehlkamp *et al.*
    only used a part of the ResNet architecture.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FCN：所有相似性虹膜网络都使用完全卷积网络（FCNs）而不是CNNs。与CNNs相比，FCNs（Long et al., [2015](#bib.bib101)）没有完全连接层，使输出图能够保留原始空间信息。这对虹膜识别非常重要，因为输出图能够保留与原始输入图像的空间对应关系（Zhao
    and Kumar, [2017b](#bib.bib210); Nguyen et al., [2020](#bib.bib114)），从而实现像素到像素的匹配。Zhao
    *et al.*（Zhao and Kumar, [2017b](#bib.bib210)）提出了一种具有3个卷积层的FCN架构，后面跟有激活和池化层。卷积层的输出被上采样到原始输入图像的大小。上采样的特征被堆叠并通过另一个卷积层进行卷积，以生成与输入图像大小相同的二维特征。后来，他们用膨胀卷积扩展了骨干网络（Wang
    and Kumar, [2019](#bib.bib181)）。Yan *et al.*（Yan et al., [2021](#bib.bib196)）采用了ResNet架构并用三元组损失进行了微调。Kuehlkamp
    *et al.* 仅使用了ResNet架构的一部分。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'NAS: Nguyen *et al.* (Nguyen et al., [2020](#bib.bib114)) proposed to learn
    the network architecture directly from data rather than hand-designing it or using
    generic-image-classification architectures. They proposed a differential Neural
    Architecture Search (NAS) approach that models the architecture design process
    as a bi-level constrained optimization approach. This approach is not only able
    to search for the optimal network which achieves the best possible performance,
    but it can also impose constraints on resources such as model size or number of
    computational operations.'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NAS：Nguyen *et al.*（Nguyen et al., [2020](#bib.bib114)）提议直接从数据中学习网络架构，而不是手动设计或使用通用的图像分类架构。他们提出了一种差分神经架构搜索（NAS）方法，该方法将架构设计过程建模为双层约束优化方法。这种方法不仅能够搜索出实现最佳性能的最优网络，还可以对资源如模型大小或计算操作数量施加约束。
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Complex-valued: Observing that there is an intrinsic difference between the
    iris texture and generic object-based images where the iris texture is stochastic
    without consistent shapes, edges, or semantic structure, Nguyen *et al.* (Nguyen
    et al., [2022](#bib.bib116)) argued the network architecture has to be better
    tailored to incorporate domain-specific knowledge in order to reach the full potential
    in the iris recognition setting. Another observation that they made is a majority
    of well-known handcrafted features such as IrisCode (Daugman, [2007](#bib.bib36))
    transformed iris texture image into a complex-valued representation first, then
    further encoded the complex-valued representation to get a final representation.
    They proposed to use fully complex-valued networks rather than popular real-valued
    networks. Complex-valued backbone networks better retain the phase, are more invariant
    to multi-scale, multi-resolution and multi-orientation, have solid correspondence
    with the classic Gabor wavelets (Tygert et al., [2016](#bib.bib174)), hence are
    much better suited to iris recognition than their real-valued counterparts.'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 复数值：观察到虹膜纹理与通用物体图像之间存在内在差异，其中虹膜纹理是随机的，没有一致的形状、边缘或语义结构，Nguyen *et al.*（Nguyen
    et al., [2022](#bib.bib116)）认为网络架构必须更好地结合领域特定知识，以充分发挥虹膜识别的潜力。他们还观察到，许多著名的手工特征，如IrisCode（Daugman,
    [2007](#bib.bib36)），首先将虹膜纹理图像转换为复数值表示，然后进一步编码该复数值表示以获得最终表示。他们提议使用完全复数值的网络，而不是流行的实数值网络。复数值的骨干网络更好地保留了相位，对多尺度、多分辨率和多方向具有更好的不变性，与经典的Gabor小波（Tygert
    et al., [2016](#bib.bib174)）有很好的对应关系，因此比实数值网络更适合虹膜识别。
- en: 'Table 2. Cohesive comparison of the most relevant DL-based iris recognition
    methods (NIR: *near-infrared*; VW: *visible wavelength*). Methods are listed in
    chronological (and then alphabetical) order.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '表2. 最相关的基于深度学习的虹膜识别方法的综合比较（NIR: *近红外*；VW: *可见光波长*）。方法按时间顺序（然后按字母顺序）列出。'
- en: '| Category | Method | Year | Data | Datasets | Features |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 年份 | 数据 | 数据集 | 特征 |'
- en: '| NIR | VW |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| NIR | VW |'
- en: '| Conventional classifiers | Menotti etat. (Menotti et al., [2015](#bib.bib105))
    | 2015 | ✓ | ✓ | Biosec, LivDet-2013-Warsaw, MobBIOfake | Shallow CNNs + SVM for Spoofing
    Detection |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 传统分类器 | Menotti et al. (Menotti et al., [2015](#bib.bib105)) | 2015 | ✓ |
    ✓ | Biosec, LivDet-2013-Warsaw, MobBIOfake | 浅层 CNN + SVM 进行欺骗检测 |'
- en: '| Minaee et al. (Minaee et al., [2016](#bib.bib106)) | 2016 | ✓ | ✗ | CASIA-Iris-Thousand,
    IITD | VGG + SVM |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Minaee et al. (Minaee et al., [2016](#bib.bib106)) | 2016 | ✓ | ✗ | CASIA-Iris-Thousand,
    IITD | VGG + SVM |'
- en: '| Nguyen et al.(Nguyen et al., [2017b](#bib.bib113)) | 2017 | ✓ | ✗ | ND-CrossSensor-2013,
    CASIA-Iris-Thousand | AlexNet, VGG, Google Inception, ResNet, DenseNet + SVM |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Nguyen et al.(Nguyen et al., [2017b](#bib.bib113)) | 2017 | ✓ | ✗ | ND-CrossSensor-2013,
    CASIA-Iris-Thousand | AlexNet, VGG, Google Inception, ResNet, DenseNet + SVM |'
- en: '| Boyd et al.(Boyd et al., [2019](#bib.bib16)) | 2019 | ✓ | ✓ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | ResNet50 + SVM |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Boyd et al.(Boyd et al., [2019](#bib.bib16)) | 2019 | ✓ | ✓ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | ResNet50 + SVM |'
- en: '| Boyd et al.(Boyd et al., [2020b](#bib.bib19)) | 2020 | ✓ | ✓ | DCMEO1, Warsaw
    | AlexNet, ResNet, VGG, DenseNet + Cosine, Euclidean, MSE |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Boyd et al.(Boyd et al., [2020b](#bib.bib19)) | 2020 | ✓ | ✓ | DCMEO1, Warsaw
    | AlexNet, ResNet, VGG, DenseNet + 余弦, 欧氏, MSE |'
- en: '| Hafner et al.(Hafner et al., [2021](#bib.bib66)) | 2021 | ✓ | ✗ | CASIA-Iris-Thousand
    | ResNet101 + DenseNet-201 + Cosine Similarity |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Hafner et al.(Hafner et al., [2021](#bib.bib66)) | 2021 | ✓ | ✗ | CASIA-Iris-Thousand
    | ResNet101 + DenseNet-201 + 余弦相似度 |'
- en: '| Classification Networks | Gangwar et al.(Gangwar and Joshi, [2016](#bib.bib57))
    | 2016 | ✓ | ✗ | ND-IRIS-0405, ND-CrossSensor-2013 | DeepIrisNet |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 分类网络 | Gangwar et al.(Gangwar 和 Joshi, [2016](#bib.bib57)) | 2016 | ✓ | ✗
    | ND-IRIS-0405, ND-CrossSensor-2013 | DeepIrisNet |'
- en: '| Gangwar et al.(Gangwar et al., [2019](#bib.bib58)) | 2019 | ✓ | ✓ | ND-IRIS-0405,
    UBIRIS.v2, MICHE-I, CASIA-Irisv4-Interval | DeepIrisNetV2 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Gangwar et al.(Gangwar et al., [2019](#bib.bib58)) | 2019 | ✓ | ✓ | ND-IRIS-0405,
    UBIRIS.v2, MICHE-I, CASIA-Irisv4-Interval | DeepIrisNetV2 |'
- en: '| Odinokikh et al.(Odinokikh et al., [2019](#bib.bib122)) | 2019 | ✓ | ✗ |
    CASIA-Iris-M1-S2, CASIA-Iris-M1-S3, Iris-Mobile | Feature Fusion + Softmax |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Odinokikh et al.(Odinokikh et al., [2019](#bib.bib122)) | 2019 | ✓ | ✗ |
    CASIA-Iris-M1-S2, CASIA-Iris-M1-S3, Iris-Mobile | 特征融合 + Softmax |'
- en: '| Zhao et al.(Zhao et al., [2019](#bib.bib208)) | 2019 | ✓ | ✗ | JluIrisV3.1,
    JluIrisV4, CASIA-Irisv4-Lamp | Capsule network + Softmax |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al.(Zhao et al., [2019](#bib.bib208)) | 2019 | ✓ | ✗ | JluIrisV3.1,
    JluIrisV4, CASIA-Irisv4-Lamp | 胶囊网络 + Softmax |'
- en: '| Chen et al. (Chen et al., [2020](#bib.bib24)) | 2020 | ✓ | ✗ | ND-IRIS-0405,
    CASIA-Iris-Thousand, IITD cross sensor | T-Center loss |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al. (Chen et al., [2020](#bib.bib24)) | 2020 | ✓ | ✗ | ND-IRIS-0405,
    CASIA-Iris-Thousand, IITD 跨传感器 | T-Center 损失 |'
- en: '| Luo et al.(Luo et al., [2021](#bib.bib103)) | 2021 | ✓ | ✗ | ND-IRIS-0405, CASIA-Iris-Thousand
    | Attention + Softmax Loss + Center Loss |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Luo et al.(Luo et al., [2021](#bib.bib103)) | 2021 | ✓ | ✗ | ND-IRIS-0405,
    CASIA-Iris-Thousand | 注意力 + Softmax 损失 + 中心损失 |'
- en: '| Similarity Networks | Nianfeng et al.(Liu et al., [2016b](#bib.bib98)) |
    2016 | ✓ | ✗ | Q-FIRE, CASIA-Cross-Sensor | DeepIris |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 相似度网络 | Nianfeng et al.(Liu et al., [2016b](#bib.bib98)) | 2016 | ✓ | ✗ |
    Q-FIRE, CASIA-Cross-Sensor | DeepIris |'
- en: '| Zhao et al.(Zhao and Kumar, [2017b](#bib.bib210)) | 2017 | ✓ | ✓ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | UniNet (FeatNet+MaskNet) + Extended Triplet Loss |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al.(Zhao 和 Kumar, [2017b](#bib.bib210)) | 2017 | ✓ | ✓ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | UniNet (FeatNet+MaskNet) + 扩展三元组损失 |'
- en: '| Damer et al.(Damer et al., [2019](#bib.bib33)) | 2019 | ✓ | ✗ | Biosecure,
    CASIA-Iris-Thousand/Lamp/Interval | Inception + Triplet Loss |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Damer et al.(Damer et al., [2019](#bib.bib33)) | 2019 | ✓ | ✗ | Biosecure,
    CASIA-Iris-Thousand/Lamp/Interval | Inception + 三元组损失 |'
- en: '| Wang et al.(Wang and Kumar, [2019](#bib.bib181)) | 2019 | ✓ | ✓ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | FeatNet + Dilated Convolution + Extended Triplet Loss |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al.(Wang 和 Kumar, [2019](#bib.bib181)) | 2019 | ✓ | ✓ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | FeatNet + 膨胀卷积 + 扩展三元组损失 |'
- en: '| Zhao et al.(Zhao and Kumar, [2019](#bib.bib212)) | 2019 | ✓ | ✗ | ND-Iris-0405,
    Casia-Irisv4-Distance, IITD | FeatNet + Mask RCNN + Extended Triplet Loss |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al.(Zhao 和 Kumar, [2019](#bib.bib212)) | 2019 | ✓ | ✗ | ND-Iris-0405,
    Casia-Irisv4-Distance, IITD | FeatNet + Mask RCNN + 扩展三元组损失 |'
- en: '| Nguyen et al.(Nguyen et al., [2020](#bib.bib114)) | 2020 | ✓ | ✓ | CASIA-v4-Distance,
    UBIRIS.v2, ND-CrossSensor-2013 | Constrained Design Backbone + Extended Triplet
    Loss |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Nguyen et al.(Nguyen et al., [2020](#bib.bib114)) | 2020 | ✓ | ✓ | CASIA-v4-Distance,
    UBIRIS.v2, ND-CrossSensor-2013 | 约束设计骨干 + 扩展三元组损失 |'
- en: '| Yan et al. (Yan et al., [2021](#bib.bib196)) | 2021 | ✓ | ✗ | CASIA-Iris-Thousand
    | Spatial Feature Reconstruction + Triplet Loss |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Yan et al.（Yan et al.，[2021](#bib.bib196)） | 2021 | ✓ | ✗ | CASIA-Iris-Thousand
    | 空间特征重建 + 三元组损失 |'
- en: '| Yang et al.(Yang et al., [2021](#bib.bib197)) | 2021 | ✓ | ✗ | CASIA-Irisv4-Thousand,
    CASIA-Irisv4-Distance, IITD | Dual Spatial Attention Network + Batch Hard |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al.（Yang et al.，[2021](#bib.bib197)） | 2021 | ✓ | ✗ | CASIA-Irisv4-Thousand,
    CASIA-Irisv4-Distance, IITD | 双空间注意网络 + 批量硬 |'
- en: '| Nguyen et al.(Nguyen et al., [2022](#bib.bib116)) | 2022 | ✓ | ✓ | ND-CrossSensor-2013,
    CASIA-Iris-Thousand, UBIRIS.v2 | Complex-valued Backbone + Extended Triplet Loss
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Nguyen et al.（Nguyen et al.，[2022](#bib.bib116)） | 2022 | ✓ | ✓ | ND-CrossSensor-2013,
    CASIA-Iris-Thousand, UBIRIS.v2 | 复数值骨干网 + 扩展三元组损失 |'
- en: '| Kuehlkamp et al. (Kuehlkamp et al., [2022](#bib.bib92)) | 2022 | ✓ | ✓ |
    DCMEO1, DCMEO2, Warsaw-Post-Mortem v2.0 | ResNet + Triplet Loss |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Kuehlkamp et al.（Kuehlkamp et al.，[2022](#bib.bib92)） | 2022 | ✓ | ✓ | DCMEO1,
    DCMEO2, Warsaw-Post-Mortem v2.0 | ResNet + 三元组损失 |'
- en: 3.3\. End-to-end Joint Iris Segmentation+Recognition Networks
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 端到端联合虹膜分割+识别网络
- en: Almost all existing approaches perform segmentation and normalization to transform
    an input image to a normalized rectangular 2D representation before recognition
    as this simplifies the representation learning. As segmentation and recognition
    may require a separate network themselves, this would cause redundancy in both
    computation and training, further slowing down an DL-based iris recognition approach.
    Several researchers have looked at approaches to perform end-to-end networks.
    One category is to perform segmentation-less recognition. Another category is
    to jointly learn segmentation and recognition using an unified network via multi-task
    learning.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有现有的方法都进行分割和归一化，以将输入图像转换为归一化的矩形二维表示，因为这简化了表示学习。由于分割和识别可能需要单独的网络，这会导致计算和训练上的冗余，进一步减缓基于深度学习的虹膜识别方法的速度。一些研究人员探讨了执行端到端网络的方法。一种方法是执行无需分割的识别。另一种方法是通过多任务学习使用统一的网络联合学习分割和识别。
- en: 'Segmentation-less: These approaches feed the cropped iris images directly into
    a deep learning network to extract features. For example, Kuehlkamp *et al.* (Kuehlkamp
    et al., [2022](#bib.bib92)) used Mask R-CNN for semantic segmentation and fed
    the cropped iris region directly into a ResNet50 to extract features. Similarly,
    Chen *et al.* (Chen et al., [2019b](#bib.bib25)) also fed the cropped iris images
    directly into a DenseNet. Rather than feeding the cropped iris images directly,
    Proenca *et al.* transformed the cropped region (which is detected by SSD) into
    a polar representation first, then fed the polar representation into the VGG19
    for extracting features (Proença and Neves, [2019](#bib.bib136)).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 无需分割：这些方法直接将裁剪后的虹膜图像输入深度学习网络以提取特征。例如，Kuehlkamp *et al.*（Kuehlkamp et al.，[2022](#bib.bib92)）使用了
    Mask R-CNN 进行语义分割，并将裁剪后的虹膜区域直接输入 ResNet50 以提取特征。类似地，Chen *et al.*（Chen et al.，[2019b](#bib.bib25)）也将裁剪后的虹膜图像直接输入
    DenseNet。与直接输入裁剪后的虹膜图像不同，Proenca *et al.* 首先将裁剪区域（由 SSD 检测）转换为极坐标表示，然后将极坐标表示输入
    VGG19 以提取特征（Proença and Neves，[2019](#bib.bib136)）。
- en: 'Multi-task: Segmentation and recognition can be jointly learned with one unified
    network. This paves a way for multi-task learning. However, segmentation and recognition
    may require different number of layers, hence research is required to perform
    using different intermediate layers for each task. To the best knowledge, there
    does not exist any approach to explore this direction.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务：可以通过一个统一的网络联合学习分割和识别。这为多任务学习铺平了道路。然而，分割和识别可能需要不同数量的层，因此需要研究以不同的中间层执行每个任务。据我所知，目前尚未存在探索此方向的方法。
- en: 4\. Deep Learning-Based Iris Presentation Attack Detection
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 基于深度学习的虹膜呈现攻击检测
- en: In parallel to the popularity of biometrics, the security of these systems against
    attacks has become of paramount importance. The most common attack is a Presentation
    Attack (PA), which refers to presenting a fake sample to the sensor. The goal
    can be either to impersonate somebody else identity (also known as Impostor Attack
    Presentation), or to conceal the own identity (also known as Concealer Attack
    Presentation). Via impostor attacks, a person could also enroll fraudulently,
    allowing a continuous manipulation of the system. The previous acronyms and terms
    in italics correspond to the vocabulary recommended in the series of ISO/IEC 30107
    standards of the ISO/IEC Subcommittee 37 (SC37) on Biometrics (technology — Biometric
    presentation attack detection — Part 1: Framework, [2016](#bib.bib164)), which
    we will follow in the rest of this section. Presentation Attack Instruments (PAI)
    used to carry out impostor attacks are typically generated from bona fide images
    of an iris from an individual who has legitimate access to the system. The iris
    is printed on a piece of paper (printout attack) or displayed on a screen (replay
    attack) and then presented to the sensor. The iris of deceased individuals can
    also be used as PAI, since the texture remains intact for some hours (Trokielewicz
    et al., [2018](#bib.bib170)). Theoretically, it would be possible to print a genuine
    iris texture into a contact lens as well, although this has not been successfully
    demonstrated yet (Boyd et al., [2020a](#bib.bib17)). Concealer attacks, on the
    other hand, are commonly done via textured contact lenses that obscure or alter
    properties of the eye (such as color) to prevent the system from identifying the
    user. Synthetic iris images (Yadav et al., [2019a](#bib.bib192)) not belonging
    to any specific identity could be used for similar purposes. Concealers can also
    present their legitimate iris, but in a way not expected by the system, e.g. closing
    eyelids as much as possible, looking to the sides (off-axis gaze), rotating the
    head, etc.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与生物特征识别的普及同步，这些系统的安全性在面对攻击时变得至关重要。最常见的攻击是**呈现攻击（PA）**，指的是向传感器展示伪造样本。攻击的目标可以是冒充他人身份（也称为**冒充者攻击呈现**），或是隐藏自己的身份（也称为**隐蔽者攻击呈现**）。通过冒充者攻击，一个人还可能进行欺诈性注册，从而持续操控系统。前面提到的缩写和斜体术语对应于ISO/IEC生物特征识别技术（技术
    — 生物特征呈现攻击检测 — 第1部分：框架，[2016](#bib.bib164)）的ISO/IEC第37分委员会（SC37）系列标准中推荐的词汇，我们将在本节剩余部分遵循这些术语。用于进行冒充者攻击的**呈现攻击工具（PAI）**通常来源于具有合法系统访问权限的个体的虹膜真实图像。虹膜会被打印在纸张上（打印攻击）或在屏幕上显示（重播攻击），然后呈现给传感器。已故个体的虹膜也可以作为PAI使用，因为纹理在几个小时内保持完好（Trokielewicz等，[2018](#bib.bib170)）。理论上，也可以将真实的虹膜纹理打印到隐形眼镜上，尽管这尚未成功展示（Boyd等，[2020a](#bib.bib17)）。另一方面，**隐蔽者攻击**通常通过纹理隐形眼镜进行，这些隐形眼镜遮蔽或改变眼睛的某些属性（如颜色），以防止系统识别用户。合成虹膜图像（Yadav等，[2019a](#bib.bib192)）也可用于类似目的。隐蔽者还可以以系统未预期的方式呈现其合法虹膜，例如尽量闭合眼睑、侧视（非轴线注视）、旋转头部等。
- en: 'Two challenges of PAs is that they happen outside the physical limits of the
    system, and they do not require specific knowledge of its inner workings, or any
    technical knowledge at all. Thus, if no properly tackled, they can derail public
    perception of even the most reliable biometric modality. It is even more critical
    if authentication is done without any supervision. Presentation Attack Detection
    (PAD) methods to counteract such attacks can be done (Galbally and Gomez-Barrero,
    [2016](#bib.bib55)): $i$) at the hardware (or sensor) level, using additional
    illuminators or sensors that detect intrinsic properties of a living eye or responses
    to external stimuli (like pupil contraction or reflection), or $ii$) at the software
    level, using only the footprint of the PA (if any) left in the same images captured
    with the standard sensor that will be employed for authentication. Software-based
    techniques are in principle less expensive and intrusive, since they do not demand
    extra hardware, and they will be the focus of this section.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: PAs 的两个挑战是它们发生在系统的物理限制之外，并且不需要对其内部工作原理或任何技术知识有特定了解。因此，如果没有得到适当处理，它们可能会影响公众对即使是最可靠的生物识别方式的看法。如果在没有任何监督的情况下进行身份验证，这一点尤为重要。应对此类攻击的展示攻击检测（PAD）方法可以在（Galbally
    和 Gomez-Barrero，[2016](#bib.bib55)）：$i$) 硬件（或传感器）级别，使用额外的光源或传感器来检测活体眼睛的固有特性或对外部刺激（如瞳孔收缩或反射）的反应，或
    $ii$) 软件级别，仅使用在进行身份验证时所用的标准传感器捕捉的图像中留下的（如果有的话）PA 的痕迹。基于软件的技术原则上成本更低且侵入性更小，因为它们不需要额外的硬件，本节将以这些技术为重点。
- en: 'Two comprehensive surveys on PAD are (Czajka and Bowyer, [2018](#bib.bib31))
    (2018) and (Boyd et al., [2020a](#bib.bib17)) (2020). While DL techniques were
    residual in the 2018 survey, they rose in popularity thereafter. We build this
    section upon the latest survey and summarize the most important developments in
    DL-PAD since it was published (Table [3](#S4.T3 "Table 3 ‣ 4.5\. Open Research
    Questions in Iris PAD ‣ 4\. Deep Learning-Based Iris Presentation Attack Detection
    ‣ Deep Learning for Iris Recognition: A Survey")). A descriptive summary of the
    datasets employed is given later in Section [8](#S8 "8\. Open-Source Deep Learning-Based
    Iris Recognition Tools ‣ Deep Learning for Iris Recognition: A Survey"). The aim
    of PAD is to classify an image either as a bona fide or an attack presentation,
    so it is usually modeled as a two-class classification task. Typical strategies
    mimic the trend of the previous section when applying DL to iris recognition:
    either a CNN backbone is used to extract features that will feed a conventional
    classifier, or the network is trained end-to-end to do the classification itself.
    Some hybrid methods also combine traditional hand-crafted with deep-learned features.
    In the same manner, the network may be initialized e.g. on the ImageNet dataset
    to take advantage of such large generic corpus, since available iris PAD data
    is more scarce. Another strategy also employed widely in the PAD literature is
    to use adversarial networks, where a GAN (Goodfellow et al., [2014](#bib.bib61))
    is trained to generate synthetic iris images that the discriminator must use to
    detect attack samples.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '关于 PAD 的两个综合调查分别是（Czajka 和 Bowyer，[2018](#bib.bib31)）（2018）和（Boyd 等，[2020a](#bib.bib17)）（2020）。尽管
    DL 技术在 2018 年的调查中处于残余状态，但其后迅速流行。我们基于最新的调查，概述了自发布以来 DL-PAD 的最重要进展（表 [3](#S4.T3
    "Table 3 ‣ 4.5\. Open Research Questions in Iris PAD ‣ 4\. Deep Learning-Based
    Iris Presentation Attack Detection ‣ Deep Learning for Iris Recognition: A Survey")）。后续第
    [8](#S8 "8\. Open-Source Deep Learning-Based Iris Recognition Tools ‣ Deep Learning
    for Iris Recognition: A Survey") 节对所用数据集进行了描述性总结。PAD 的目标是将图像分类为真实的或攻击的展示，因此通常被建模为一个两类分类任务。典型的策略模仿了前一节在应用
    DL 于虹膜识别时的趋势：要么使用 CNN 主干提取特征，然后将其输入到传统分类器中，要么训练网络端到端地进行分类。一些混合方法还结合了传统手工设计的特征和深度学习特征。同样，网络可以例如在
    ImageNet 数据集上进行初始化，以利用这样的通用大数据集，因为现有的虹膜 PAD 数据较为稀缺。另一个在 PAD 文献中广泛采用的策略是使用对抗网络，其中
    GAN（Goodfellow 等，[2014](#bib.bib61)）被训练生成合成虹膜图像，判别器必须使用这些图像来检测攻击样本。'
- en: 4.1\. CNNs for Feature Extraction
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 用于特征提取的 CNN
- en: Since each layer of a CNN represents a different level of abstraction, Fang
    et al. (Fang et al., [2020a](#bib.bib45)) fused the features from the last four
    convolutional layers of two models (VGG16, MobileNetv3-small). The features are
    projected to a lower dimensional space by PCA and either concatenated for classification
    with SVM (feature fusion) or the classification scores of each level combined
    (score fusion). Using two databases of printouts and textured contact lenses,
    the method showed superiority over the use of the different layers individually,
    or the feature vector from the next-to-last layer of the networks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CNN 的每一层表示不同的抽象层次，Fang 等（Fang 等，[2020a](#bib.bib45)）融合了两个模型（VGG16，MobileNetv3-small）的最后四层卷积层的特征。这些特征通过
    PCA 投影到较低维度的空间中，并通过 SVM 进行分类（特征融合）或将每一层的分类分数组合（分数融合）。在包含打印件和纹理隐形眼镜的两个数据库上，该方法表现出优于单独使用不同层或网络倒数第二层特征向量的效果。
- en: 4.2\. End-to-end Classification Networks
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 端到端分类网络
- en: Arora and Bathia (Arora and Bhatia, [2020](#bib.bib9)) trained a CNN with 10
    convolutional layers to detect contact lenses and printouts. Rather than using
    the entire image, the network is trained on patches from all parts of the iris
    image. The system showed superior performance compared to state-of-the-art methods
    which at that time, according to the paper, were mostly based on hand-crafted
    features.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Arora 和 Bathia（Arora 和 Bhatia，[2020](#bib.bib9)）训练了一个具有 10 层卷积层的 CNN 来检测隐形眼镜和打印件。网络并非使用整个图像，而是对虹膜图像的各个部分进行训练。该系统显示出优于当时大多数基于手工特征的最先进方法的性能。
- en: Focusing on embedded low-power devices, Peng et al. (Peng et al., [2020](#bib.bib127))
    adopted a Lite Anti-attack Iris Location Network (LAILNet) based on three dense
    blocks featuring depthwise separable convolutions to reduce the number of parameters.
    The algorithm demonstrated very good performance on three databases with printouts,
    synthetic irises, contact lenses and artificial plastic eyes.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 针对嵌入式低功耗设备，Peng 等（Peng 等，[2020](#bib.bib127)）采用了基于三个密集块的 Lite Anti-attack Iris
    Location Network（LAILNet），这些密集块具有深度可分离卷积以减少参数数量。该算法在包含打印件、合成虹膜、隐形眼镜和人造塑料眼的三个数据库上表现出非常好的性能。
- en: Also focusing on mobiles, Fang et al. (Fang et al., [2021b](#bib.bib46); Fang
    et al., [2020b](#bib.bib49)) used MobileNetv3-small. The contribution lies in
    the division of the normalized iris image into overlapped micro-stripes which
    are fed individually, and a decision reached by majority voting. The claimed advantages
    are that the classifier is forced to focus on the iris/sclera boundaries (given
    by their exact micro-stripes), the input dimensionality is lower and the amount
    of samples is higher (reducing overfitting), and the impact of imprecise segmentation
    is alleviated. Using three databases with contact lenses and printouts, the paper
    featured an extensive experimentation with cross-database, cross-sensor, and cross-attack
    setting.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 同样关注移动设备，Fang 等（Fang 等，[2021b](#bib.bib46)；Fang 等，[2020b](#bib.bib49)）使用了 MobileNetv3-small。贡献在于将标准化虹膜图像划分为重叠的微条纹，并单独输入，通过多数投票得出决策。声称的优点是分类器被迫关注虹膜/巩膜边界（由其精确的微条纹给出），输入维度更低，样本量更高（减少过拟合），并且减轻了不精确分割的影响。使用包含隐形眼镜和打印件的三个数据库，本文进行了广泛的实验，包括跨数据库、跨传感器和跨攻击设置。
- en: Sharma and Ross (Sharma and Ross, [2020](#bib.bib151)) proposed D-NetPAD, based
    on DenseNet121, chosen due to benefits such as maximum flow of information given
    by dense connections to all subsequent layers, or fewer parameters compared to
    counterparts like ResNet or VGG. The PAI included printouts, artificial eye, cosmetic
    contacts, kindle replay, and transparent dome on print, with experiments substantiating
    the effectiveness of the method on cross-PAI, cross-sensor and cross-database
    scenarios.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Sharma 和 Ross（Sharma 和 Ross，[2020](#bib.bib151)）提出了基于 DenseNet121 的 D-NetPAD，之所以选择
    DenseNet121 是因为其密集连接带来了最大的信息流，或者相较于 ResNet 或 VGG 等对手，参数更少。PAI 包括打印件、人造眼、化妆用隐形眼镜、Kindle
    重放和打印件上的透明圆顶，实验证明该方法在跨-PAI、跨传感器和跨数据库场景下的有效性。
- en: Chen and Ross (Chen and Ross, [2021](#bib.bib20)) proposed an explainable attention-guided
    detector (AG-PAD). To do so, the feature maps of a DenseNet121 were fed into two
    modules that independently capture inter-channel and inter-spatial feature dependencies.
    The outputs were then fused via element-wise sum to capture complementary attention
    features from both channel and spatial dimensions. With three datasets containing
    colored contact lenses, artificial eyes (Van Dyke/Doll fake eyes), printouts,
    and textured contact lenses, the attention modules are shown to improve accuracy
    over the baseline network. Using heatmap visualization, it is also shown that
    the attention modules force the network to attend to the annular iris textural
    region which, intuitively, plays a vital role for PAD.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 陈和罗斯（Chen and Ross, [2021](#bib.bib20)）提出了一种可解释的注意力引导检测器（AG-PAD）。为此，将 DenseNet121
    的特征图输入到两个模块中，这些模块分别捕捉通道间和空间间的特征依赖关系。然后通过元素逐一求和将输出融合，以捕捉来自通道和空间维度的互补注意力特征。使用包含彩色隐形眼镜、人工眼（Van
    Dyke/假眼）、印刷物和纹理隐形眼镜的三个数据集，注意力模块显示出比基线网络更高的准确性。通过热图可视化，还显示注意力模块使网络关注于环形虹膜纹理区域，这直观上对
    PAD 起着至关重要的作用。
- en: Spatial attention was also explored by Fang et al. (Fang et al., [2021c](#bib.bib47)).
    To find local regions that contribute the most to make accurate decisions and
    capture pixel/patch-level cues, they proposed an attention-based pixel-wise binary
    supervision (A-PBS) method. To capture different levels of abstraction, they perform
    multi-scale fusion by adding spatial attention modules to feature maps from three
    levels of a DenseNet backbone. Using six datasets with textured lenses and printouts,
    they outperformed previous state-of-the-art including scenarios with unknown attacks,
    sensors, and databases.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 空间注意力也被方等（Fang et al., [2021c](#bib.bib47)）探索了。为了找到对做出准确决策贡献最大的局部区域并捕捉像素/补丁级的线索，他们提出了一种基于注意力的像素级二进制监督（A-PBS）方法。为了捕捉不同层次的抽象，他们通过在
    DenseNet 主干网的三个层级的特征图中添加空间注意力模块来进行多尺度融合。使用六个包含纹理镜片和印刷物的数据集，他们在包括未知攻击、传感器和数据库的场景下超越了之前的最先进技术。
- en: Given the difficulty of collecting iris PAD data, most databases contain, at
    most, a few hundred subjects. To address this, Fang et al. (Fang et al., [2021d](#bib.bib48))
    studied data augmentation techniques that modify position, scale or illumination.
    Using three architectures (ResNet50, VGG16, MobileNetv3-small) and three databases
    with printouts and textured contact lenses, they found that data augmentation
    improves PAD performance significantly, but each technique has a positive role
    on a particular dataset or CNN. They also explored the selection of augmentation
    techniques, finding, again, no consensus regarding the best combination, which
    was attributed to differences in capture environment, subject population, scale
    of the different datasets or imbalance between bona fide and attack samples.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于收集虹膜 PAD 数据的困难，大多数数据库最多只包含几百个样本。为了解决这个问题，方等（Fang et al., [2021d](#bib.bib48)）研究了通过修改位置、尺度或光照的数据增强技术。通过使用三种架构（ResNet50、VGG16、MobileNetv3-small）和三个包含印刷物和纹理隐形眼镜的数据库，他们发现数据增强显著提升了
    PAD 性能，但每种技术在特定数据集或 CNN 上具有积极作用。他们还探讨了数据增强技术的选择，发现对于最佳组合没有共识，这归因于捕捉环境、样本群体、不同数据集的规模或真实样本与攻击样本之间的不平衡。
- en: Gupta et al. (Gupta et al., [2021](#bib.bib64)) proposed MVANet, with 5 convolutional
    layers and 3 branches of fully connected layers. They addressed the challenge
    of unseen databases, sensors, and imaging environment on textured contact lenses
    detection. The size of each layer of MVANet is different, thus capturing different
    features. They used three databases, each one captured in different settings (indoor/outdoor,
    different times of the day, varying weather, fixed/mobile sensors, etc.), with
    MVANET trained in one database at a time and tested on the other two. As baseline,
    they fine-tuned three popular CNNs (VGG16, ResNet18, DenseNet) initialized on
    ImageNet. The proposed network is shown to perform consistently better and more
    uniformly on the test databases than the baseline approaches.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Gupta 等人（Gupta 等人, [2021](#bib.bib64)）提出了 MVANet，具有 5 个卷积层和 3 个全连接层分支。他们解决了在纹理接触镜检测中面临的未见数据库、传感器和成像环境的挑战。MVANet
    的每一层大小不同，从而捕获不同的特征。他们使用了三个数据库，每个数据库在不同的设置（室内/室外、一天中的不同时间、不同的天气、固定/移动传感器等）中捕获，MVANet
    在一个数据库上训练，并在其他两个数据库上进行测试。作为基准，他们对在 ImageNet 上初始化的三种流行 CNN（VGG16、ResNet18、DenseNet）进行了微调。结果显示，所提出的网络在测试数据库上的性能一致且更均匀地优于基准方法。
- en: Sharma and Ross (Sharma and Ross, [2021](#bib.bib152)) studied the viability
    of Optical Coherence Tomography (OCT). OCT provides a cross-sectional view of
    the eye, whereas traditional NIR or VW imaging provides 2D textural data. The
    PAIs considered are artificial eyes (Van Dyke eyes) and cosmetic lenses, evaluated
    on three different CNNs (VGG19, ResNet50, DenseNet121). By both intra- (known
    PAs) and cross-attack (unknown PAs) scenarios, OCT is determined as a viable solution,
    although hardware cost is still a limiting factor. Indeed, OCT outperforms NIR
    and VW in the intra-attack scenario, while NIR generalizes better to unseen PAs.
    Cosmetic lenses also appear to be more difficult to detect than artificial eyes
    with any modality. Via heatmaps, it is seen as well that the fixation regions
    are different for each imaging modality and for each PAI, which could be a source
    of complementarity.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Sharma 和 Ross（Sharma 和 Ross, [2021](#bib.bib152)）研究了光学相干断层扫描（OCT）的可行性。OCT 提供了眼睛的横截面视图，而传统的
    NIR 或 VW 成像提供的是 2D 纹理数据。考虑的 PAI 包括人工眼（Van Dyke 眼）和化妆镜片，在三种不同的 CNN（VGG19、ResNet50、DenseNet121）上进行评估。通过已知
    PAs 的内部攻击和未知 PAs 的交叉攻击场景，OCT 被确定为一种可行的解决方案，尽管硬件成本仍然是一个限制因素。确实，OCT 在内部攻击场景中优于 NIR
    和 VW，而 NIR 对未见 PAs 的泛化能力更强。化妆镜片似乎比人工眼更难被任何一种模式检测到。通过热图也可以看到，每种成像模式和每种 PAI 的固定区域不同，这可能是互补性的来源。
- en: Zhang et al. (Zhang et al., [2021](#bib.bib200)) proposed a Weighted Region
    Network (WRN) to detect cosmetic lenses that includes a local attention Weight
    Network (for evaluating the discriminating information of different regions) and
    a global classification Region Network (for characterizing global features). Such
    strategy considers both the entire image and the attention effect by assigning
    different weights to regions. The mentioned networks are applied to a VGG16 backbone.
    The reported results showed improved performance compared to the state-of-the-art
    over three different databases.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等人（Zhang 等人, [2021](#bib.bib200)）提出了一种加权区域网络（WRN）来检测化妆镜片，其中包括一个局部注意力权重网络（用于评估不同区域的区分信息）和一个全球分类区域网络（用于表征全局特征）。这种策略考虑了整个图像和通过分配不同权重的区域的注意效果。提到的网络被应用于
    VGG16 主干。报告的结果显示，与三种不同数据库上的最先进技术相比，性能有所提高。
- en: The works by Agarwal et al. (Agarwal et al., [2022b](#bib.bib3), [a](#bib.bib2))
    evaluated the detection of contact lenses. In (Agarwal et al., [2022b](#bib.bib3)),
    they trained a siamese CNN of 5 convolutional layers on two different inputs (the
    original image and its CLAHE version), which are then combined by weighted score
    fusion of the softmax layer. Adding a processed version of the raw image attempts
    to enhance the feature extraction capabilities of the CNN. A similar strategy
    is followed in (Agarwal et al., [2022a](#bib.bib2)), but here they used a siamese
    contraction-expansion CNN, and the processed image is a edge-enhanced image obtained
    via Histogram of Oriented Gradients (HOG). Another difference was the use of feature-level
    fusion of the next-to-last CNN feature vectors, testing different strategies (vector
    addition, multiplication, concatenation and distance). The papers employed several
    databases, with an extensive protocol including unseen subjects, environments
    (indoor vs outdoor) and databases (sensors) that showcases the strength of the
    solutions against cross-domain changes. The methods also showed superiority against
    popular CNN models (VGG16, ResNet18, DenseNet) and the popular LBP and HOG hand-crafted
    features.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Agarwal 等人（Agarwal et al., [2022b](#bib.bib3), [a](#bib.bib2)）的研究评估了隐形眼镜的检测。在（Agarwal
    et al., [2022b](#bib.bib3)）中，他们对两个不同输入（原始图像及其 CLAHE 版本）训练了一个五层卷积的孪生 CNN，然后通过 softmax
    层的加权分数融合这两个输入。添加处理过的原始图像旨在增强 CNN 的特征提取能力。在（Agarwal et al., [2022a](#bib.bib2)）中采用了类似的策略，但他们使用了一个孪生的收缩-扩展
    CNN，并且处理后的图像是通过梯度方向直方图（HOG）获得的边缘增强图像。另一种不同之处在于使用了接近最后一层 CNN 特征向量的特征级融合，测试了不同的策略（向量加法、乘法、拼接和距离）。这些论文使用了多个数据库，包含了未见过的对象、环境（室内与室外）和数据库（传感器）的广泛协议，展示了这些解决方案在跨领域变化中的优势。这些方法还在流行的
    CNN 模型（VGG16、ResNet18、DenseNet）以及流行的 LBP 和 HOG 手工特征中表现出优越性。
- en: Gautam et al. (Gautam et al., [2022](#bib.bib60)) proposed a Deep Supervised
    Class Encoding (DSCE) approach consisting of an Autoencoder that exploits class
    information, and minimizes simultaneously the reconstruction and classification
    errors during training. Three datasets were used, containing textured lenses,
    printouts and synthetic images, showing superiority over a variety of hand-crafted
    and deep-learned features.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Gautam 等人（Gautam et al., [2022](#bib.bib60)）提出了一种深度监督类别编码（DSCE）方法，该方法包括一个利用类别信息的自编码器，在训练过程中同时最小化重建和分类错误。他们使用了三个数据集，包括纹理镜片、打印件和合成图像，显示出在多种手工制作和深度学习特征中的优越性。
- en: 'Tapia et al.(Tapia et al., [2022](#bib.bib163)) used a two-stages serial architecture
    based on a modified MobiletNetv2\. A first network was trained to only distinguish
    two classes (bona fide vs attack). If it votes bona fide, the image is sent to
    a second network trained to classify it among three or four classes (bona fide
    or a different type of PAI: contact lenses, printout, or cadaver). Four databases
    were combined to obtain a super-set with the different PAIs, and class-weights
    were also incorporated into the loss to compensate imbalance. The paper applied
    contrast enhancement (CLAHE), and an aggressive data augmentation (rotation, blurring,
    contrast change, Gaussian noise, edge enhancement, image region dropout, etc.).
    They tested two image sizes, 224$\times$224 and 448$\times$448, observing that
    the extra detail of a higher resolution image results in more effective features.
    The paper also carried out leave-one-out PAI tests for open-set evaluation, showing
    robustness in detecting unknown attacks.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Tapia 等人（Tapia et al., [2022](#bib.bib163)）使用了基于修改版 MobiletNetv2 的两阶段串联架构。第一个网络仅被训练用来区分两个类别（真实与攻击）。如果它投票为真实，则图像会被送到第二个网络，该网络被训练用来将图像分类为三类或四类（真实或其他类型的伪影：隐形眼镜、打印件或尸体）。他们将四个数据库结合起来，得到一个包含不同伪影的超集，并将类别权重也融入到损失函数中，以补偿不平衡。论文应用了对比度增强（CLAHE），以及激进的数据增强（旋转、模糊、对比度变化、高斯噪声、边缘增强、图像区域丢弃等）。他们测试了两种图像尺寸，224$\times$224
    和 448$\times$448，观察到更高分辨率图像的额外细节能产生更有效的特征。论文还进行了留一法伪影测试，以进行开放集评估，显示出检测未知攻击的鲁棒性。
- en: 4.3\. Hybrid Methods
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 混合方法
- en: Choudhary et al. (Choudhary et al., [2022b](#bib.bib28), [a](#bib.bib27)) applied
    a Friedman test-based selection method to identify the best features of a set
    of hand-crafted and deep-learned ones. Each feature method feeds a SVM classifier,
    and the scores of the individual SVMs are fused via weighted sum. A preliminary
    version of (Choudhary et al., [2022a](#bib.bib27)) without feature selection appeared
    in (Choudhary et al., [2021](#bib.bib26)). The databases of (Choudhary et al.,
    [2022b](#bib.bib28)) include a medley of different PA (printouts, synthetic irises,
    artificial eyeballs, etc.), although the feature selection and classification
    methods are trained and evaluated separately on each database. The authors observed
    a saturation after a certain number of features are combined, and a superiority
    of the score-level fusion over other methods such as majority voting, feature-level
    fusion, and rank-level fusion. The work (Choudhary et al., [2022a](#bib.bib27)),
    on the other hand, concentrated on textured contact lenses attack, with an extensive
    set of evaluations including single sensor, cross-sensor and combined sensor experiments.
    Apart from the generic live vs attack scenario, it also reports binary and ternary
    classification across the different types of real (normal iris, soft lens) and
    fake (textured) classes. Naturally, the cross-sensor error is larger compared
    to single-sensor, and the combined sensor error is also observed to be slightly
    larger. The latter is attributed to the larger intraclass variation created when
    images from different sensors are combined. In any case, an improvement of performance
    over previous works with the three datasets employed is observed after the proposed
    feature selection and score-level fusion method.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Choudhary 等人（Choudhary et al., [2022b](#bib.bib28), [a](#bib.bib27)）应用了基于 Friedman
    测试的选择方法来识别一组手工制作和深度学习特征中的最佳特征。每种特征方法都输入到 SVM 分类器中，个别 SVM 的分数通过加权和进行融合。没有进行特征选择的
    Choudhary 等人（Choudhary et al., [2022a](#bib.bib27)）的初步版本出现在 Choudhary 等人（Choudhary
    et al., [2021](#bib.bib26)）。Choudhary 等人（Choudhary et al., [2022b](#bib.bib28)）的数据库包括各种
    PA（打印件、合成虹膜、人工眼球等），尽管特征选择和分类方法是在每个数据库上单独训练和评估的。作者观察到，当组合的特征数量达到一定程度后，出现了饱和现象，并且在得分级融合上优于其他方法，如多数投票、特征级融合和排名级融合。另一方面，Choudhary
    等人（Choudhary et al., [2022a](#bib.bib27)）集中在纹理接触镜攻击上，进行了广泛的评估，包括单传感器、跨传感器和组合传感器实验。除了通用的实时与攻击场景外，还报告了不同类型的真实（正常虹膜、软镜）和假（纹理）类别的二分类和三分类。自然地，与单传感器相比，跨传感器的错误更大，组合传感器的错误也略微增大。后者归因于当来自不同传感器的图像被组合时产生的较大类内变异。无论如何，经过所提出的特征选择和得分级融合方法后，在使用的三个数据集上观察到了性能的提升。
- en: 4.4\. Adversarial Networks
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 对抗网络
- en: Generative methods have been used by some approaches, either to use the trained
    discriminator for iris PAD, or to generate synthetic samples and augment under-represented
    classes. In this direction, Yadav and Ross (Yadav and Ross, [2021](#bib.bib194))
    proposed CIT-GAN (Cyclic Image Translation Generative Adversarial Network) for
    multi-domain style transfer to generate synthetic samples of several PAIs (cosmetic
    contact lenses, printed eyes, artificial eyes and kindle-display attack). To do
    so, image translation is driven by a Styling Network that learns style characteristics
    of each given domain. It also employs a Convolutional Autoencoder in the generator
    for image-to-image style translation, which takes a domain label as input along
    with an image. This is different than previous works of the same authors (Yadav
    et al., [2020](#bib.bib193), [2019a](#bib.bib192)) which employed the traditional
    generator/discriminator approach driven by a noise vector. Different PAD methods
    using hand-crafted (BSIF, DESIST) and deep features (VGG16, D-NetPAD, AlexNet)
    were evaluated, demonstrating that they can be improved by adding synthetically
    generated data. The quality of synthetic images is also superior to a competing
    generative method (Star-GAN v2), measured via FID score distributions.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法已经使用了生成方法，要么是使用训练好的判别器进行虹膜PAD，要么是生成合成样本并增强不足表示的类别。在这方面，Yadav和Ross（Yadav
    and Ross, [2021](#bib.bib194)）提出了CIT-GAN（循环图像翻译生成对抗网络），用于多领域风格迁移，生成多种PAI（化妆隐形眼镜、打印眼、人工眼和Kindle显示攻击）的合成样本。为此，图像翻译由一个学习每个给定领域风格特征的Styling
    Network驱动。它还在生成器中使用了卷积自编码器进行图像到图像的风格翻译，该编码器将领域标签和图像作为输入。这与同一作者之前的工作（Yadav et al.,
    [2020](#bib.bib193), [2019a](#bib.bib192)）不同，后者采用了由噪声向量驱动的传统生成器/判别器方法。评估了使用手工制作（BSIF，DESIST）和深度特征（VGG16，D-NetPAD，AlexNet）的不同PAD方法，结果表明通过添加合成生成的数据可以改进这些方法。合成图像的质量也优于竞争的生成方法（Star-GAN
    v2），通过FID分数分布进行衡量。
- en: 4.5\. Open Research Questions in Iris PAD
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. Iris PAD中的开放研究问题
- en: One of the open research issues is to design robust iris PAD methods with cross-sensor
    and cross-database capabilities, so they generalize to unseen imaging conditions.
    Attackers are constantly developing new attack methodologies to circumvent PAD
    systems, so an even more important issue is unseen PAIs (i.e. cross-PAI capabilities)
    (Sharma and Selwal, [2021](#bib.bib150)). Great results have been achieved on
    detecting known attack types (known as closed-set recognition), although cross-database
    evaluation (training in one database an testing in others) still appears as a
    difficult challenge due to changes in sensors, acquisition environments, or subjects.
    Moreover, generalizing to attacks that are unknown at the time of training (open-set
    recognition) is even a greater challenge for state-of-the-art methods (Fang et al.,
    [2021b](#bib.bib46)). Part of the problem lies into the limited size of existing
    databases, which is an issue for data-hungry DL approaches. Some solutions, as
    studied by some of the methods above, are data augmentation by geometric or illumination
    modifications (Fang et al., [2021d](#bib.bib48)), or creating additional synthetic
    data via generative methods (Yadav and Ross, [2021](#bib.bib194)). Human-aided
    DL training is another promising avenue. Indeed, humans and machines cooperating
    in vision tasks is not new, and this strategy is finding its way into DL as well
    (Boyd et al., [2021](#bib.bib18), [2022](#bib.bib15)). For example, Boyd et al.
    (Boyd et al., [2022](#bib.bib15)) analyzed the utility of human judgement about
    salient regions of images to improve generalization of DL models. Asked about
    regions that humans deem important for their decision about an image, the work
    proposed to transform the training data to incorporate such opinions, demonstrating
    an improvement in accuracy and generalization in leave-one-attack-type-out scenarios.
    In a similar work, Boyd et al. (Boyd et al., [2021](#bib.bib18)) incorporated
    annotated saliency maps into the loss function to penalize large differences with
    human judgement.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一个开放的研究问题是设计具有跨传感器和跨数据库能力的鲁棒虹膜防伪方法，以便它们能适应未见过的成像条件。攻击者不断开发新的攻击方法以绕过防伪系统，因此，未见过的伪造攻击（即跨伪造攻击能力）（Sharma
    和 Selwal，[2021](#bib.bib150)）是一个更为重要的问题。尽管在检测已知攻击类型（即封闭集识别）方面取得了很好的结果，但由于传感器、采集环境或受试者的变化，跨数据库评估（在一个数据库中训练，在其他数据库中测试）仍然是一个困难的挑战。此外，对于在训练时未知的攻击进行泛化（开放集识别）对最先进的方法而言是更大的挑战（Fang
    等，[2021b](#bib.bib46)）。问题的一部分在于现有数据库的规模有限，这对数据需求量大的深度学习方法来说是一个问题。一些解决方案，如上述方法所研究的，通过几何或光照修改进行数据增强（Fang
    等，[2021d](#bib.bib48)），或通过生成方法创建额外的合成数据（Yadav 和 Ross，[2021](#bib.bib194)）。人机协作的深度学习训练是另一个有前景的方向。实际上，人类和机器在视觉任务中的合作并不新鲜，这一策略也正在深度学习中找到自己的路径（Boyd
    等，[2021](#bib.bib18)，[2022](#bib.bib15)）。例如，Boyd 等（Boyd 等，[2022](#bib.bib15)）分析了人类对图像显著区域的判断的效用，以提高深度学习模型的泛化能力。通过询问人类认为对图像决策重要的区域，研究提出将训练数据转化为包含这些意见的方式，从而在留一攻击类型的情境中展示了准确性和泛化的提升。在类似的工作中，Boyd
    等（Boyd 等，[2021](#bib.bib18)）将注释的显著性图纳入损失函数，以惩罚与人类判断的较大差异。
- en: Recently, concerns have emerged about the observed bias of DL methods that leads
    to discriminatory performance differences based on the user´s demographics, with
    face biometrics being the most talked-about and many companies and authorities
    banning its use (Jain et al., [2021](#bib.bib79)). Obviously, this issue appears
    in iris PAD as well, as addressed by Fang et al. (Fang et al., [2021e](#bib.bib50)).
    Using three baselines based on hand-crafted and DL approaches and a database of
    contact lenses, the authors showed a significant difference in the performance
    between male and female samples. In dealing with this phenomenon, examination
    of biases towards eye color or race are another directions worthwhile to consider.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，关于深度学习方法存在的偏见问题引发了关注，这种偏见导致基于用户人口统计特征的歧视性表现差异，面部生物特征识别是讨论最多的领域，许多公司和机构已禁止其使用（Jain
    等，[2021](#bib.bib79)）。显然，这个问题在虹膜防伪（PAD）中也存在，正如 Fang 等（Fang 等，[2021e](#bib.bib50)）所讨论的那样。利用基于手工特征和深度学习方法的三种基线以及一个隐形眼镜数据库，作者显示了男性和女性样本之间的性能差异。处理这一现象时，研究对眼睛颜色或种族的偏见也是值得考虑的方向。
- en: Some elements considered as PAIs in this section, such as cosmetic lenses, may
    be worn normally by users without the purpose of fooling the biometric system,
    as it is the case of facial retouching via make-up, digital beautification or
    augmented reality (Hedman et al., [2021](#bib.bib70)). This poses the question
    of whether it is possible to use such images for authentication, while diminishing
    the effect in the recognition performance. Suggested alternatives have been to
    detect and match portions of live iris tissue still visible (Parzianello and Czajka,
    [2022](#bib.bib126)) or incorporate ocular information of the surrounding area
    (Alonso-Fernandez and Bigun, [2016](#bib.bib5)). Unfortunately, in iris biometrics,
    recognition with textured contact lenses remains a hard problem to solve.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中一些被视为PAIs的元素，例如化妆镜片，用户可能会正常佩戴，而非为了欺骗生物识别系统，就像通过化妆、数字美化或增强现实进行面部修饰一样（Hedman等，[2021](#bib.bib70)）。这引发了一个问题：是否可以使用这些图像进行身份验证，同时减少对识别性能的影响。建议的替代方案是检测和匹配仍然可见的活体虹膜组织部分（Parzianello和Czajka，[2022](#bib.bib126)），或结合周围区域的眼部信息（Alonso-Fernandez和Bigun，[2016](#bib.bib5)）。不幸的是，在虹膜生物识别中，带有纹理隐形眼镜的识别仍然是一个难题。
- en: 'Another under-researched task is iris PAD in the visible spectrum. The majority
    of studies and datasets (Section [8](#S8 "8\. Open-Source Deep Learning-Based
    Iris Recognition Tools ‣ Deep Learning for Iris Recognition: A Survey")) employ
    near-infrared illumination and specific iris close-up sensors. However, in some
    environments such as mobile or distant capture, such sensing is not guaranteed
    (Nigam et al., [2015](#bib.bib120)).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个尚未充分研究的任务是可见光谱中的虹膜PAD。大多数研究和数据集（第[8](#S8 "8\. 开源深度学习虹膜识别工具 ‣ 深度学习虹膜识别综述")节）使用近红外照明和特定的虹膜近拍传感器。然而，在某些环境下，如移动或远距离拍摄，这种传感并不保证（Nigam等，[2015](#bib.bib120)）。
- en: 'Table 3. Cohesive comparison of the most relevant DL-based iris Presentation
    Attack Detection methods after the surveys (Czajka and Bowyer, [2018](#bib.bib31);
    Boyd et al., [2020a](#bib.bib17)) (NIR: *near-infrared*; VW: *visible wavelength*).
    Methods are listed in chronological (and then alphabetical) order.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '表3. 调查后最相关的基于DL的虹膜呈现攻击检测方法的综合比较（Czajka和Bowyer，[2018](#bib.bib31)；Boyd等，[2020a](#bib.bib17)）（NIR:
    *近红外*；VW: *可见光波长*）。方法按时间顺序（然后按字母顺序）列出。'
- en: '| Category | Method | Year | Data | Datasets | Features |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方法 | 年份 | 数据 | 数据集 | 特征 |'
- en: '| NIR | VW |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| NIR | VW |'
- en: '| Feature Extraction | Fang et al. (Fang et al., [2020a](#bib.bib45)) | 2020
    | ✓ | ✗ | LivDet-2017 (IIITD-WVU, ND- CLD) | VGG16, MobileNetv3-small (multi-layer
    features) + PCA + SVM |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 特征提取 | Fang et al. (Fang et al., [2020a](#bib.bib45)) | 2020 | ✓ | ✗ | LivDet-2017
    (IIITD-WVU, ND-CLD) | VGG16, MobileNetv3-small（多层特征）+ PCA + SVM |'
- en: '| End-to-end Training | Arora and Bathia (Arora and Bhatia, [2020](#bib.bib9))
    | 2020 | ✓ | ✗ | LivDet-2017 (IIITD-WVU) | CNN with patch input |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 端到端训练 | Arora and Bathia (Arora and Bhatia, [2020](#bib.bib9)) | 2020 | ✓
    | ✗ | LivDet-2017 (IIITD-WVU) | 带有补丁输入的CNN |'
- en: '|  | Peng et al. (Peng et al., [2020](#bib.bib127)) | 2020 | ✓ | ✗ | IPITRT,
    CASIA-Iris-v4, CASIA-Iris-Fake | LAILNet lightweight CNN |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | Peng et al. (Peng et al., [2020](#bib.bib127)) | 2020 | ✓ | ✗ | IPITRT,
    CASIA-Iris-v4, CASIA-Iris-Fake | LAILNet 轻量级CNN |'
- en: '|  | Sharma and Ross (Sharma and Ross, [2020](#bib.bib151)) | 2020 | ✓ | ✗
    | Proprietary, LivDet-2017 (IIITD-WVU, ND-CLD, Warsaw, Clarkson) | DenseNet121
    pre-trained on ImageNet |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | Sharma and Ross (Sharma and Ross, [2020](#bib.bib151)) | 2020 | ✓ | ✗
    | 专有, LivDet-2017 (IIITD-WVU, ND-CLD, Warsaw, Clarkson) | 在ImageNet上预训练的DenseNet121
    |'
- en: '|  | Chen and Ross (Chen and Ross, [2021](#bib.bib20)) | 2021 | ✓ | ✗ | JHU-APL,
    LivDet-2017 (Warsaw, ND-CLD) | DenseNet121 pre-trained on ImageNet + AG-PAD channel
    and spatial attention |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | Chen and Ross (Chen and Ross, [2021](#bib.bib20)) | 2021 | ✓ | ✗ | JHU-APL,
    LivDet-2017 (Warsaw, ND-CLD) | 在ImageNet上预训练的DenseNet121 + AG-PAD通道和空间注意力 |'
- en: '|  | Fang et al. (Fang et al., [2021b](#bib.bib46)) | 2021 | ✓ | ✗ | LivDet-2017
    (IIITD-WVU, ND-CLD), ND-CLD-15, | MobileNetv3-small with micro-stripes |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | Fang et al. (Fang et al., [2021b](#bib.bib46)) | 2021 | ✓ | ✗ | LivDet-2017
    (IIITD-WVU, ND-CLD), ND-CLD-15 | MobileNetv3-small带微条纹 |'
- en: '|  | Fang et al. (Fang et al., [2021c](#bib.bib47)) | 2021 | ✓ | ✗ | LivDet-2017
    (IIITD-WVU, ND-CLD, Clarkson), ND-CLD-13, ND-CLD-15, IIITD-CLI | DenseNet + A-PBS
    spatial attention |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | Fang et al. (Fang et al., [2021c](#bib.bib47)) | 2021 | ✓ | ✗ | LivDet-2017
    (IIITD-WVU, ND-CLD, Clarkson), ND-CLD-13, ND-CLD-15, IIITD-CLI | DenseNet + A-PBS空间注意力
    |'
- en: '|  | Fang et al. (Fang et al., [2021d](#bib.bib48)) | 2021 | ✓ | ✗ | LivDet-2017
    (IIITD-WVU, ND-CLD, Clarkson) | ResNet50, VGG16, MobileNetv3-small |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | Fang et al. (Fang et al., [2021d](#bib.bib48)) | 2021 | ✓ | ✗ | LivDet-2017
    (IIITD-WVU, ND-CLD, Clarkson) | ResNet50, VGG16, MobileNetv3-small |'
- en: '|  | Gupta et al. (Gupta et al., [2021](#bib.bib64)) | 2021 | ✓ | ✗ | MUIPA,
    UnMIPA, IIITD-CLI | CNN with multi-branch classification |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | Gupta et al. (Gupta et al., [2021](#bib.bib64)) | 2021 | ✓ | ✗ | MUIPA,
    UnMIPA, IIITD-CLI | 多分支分类的 CNN |'
- en: '|  | Sharma and Ross (Sharma and Ross, [2021](#bib.bib152)) | 2021 | ✓ | ✓
    | OCT, NIR and VW images | VGG19, ResNet50, DenseNet121 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | Sharma and Ross (Sharma and Ross, [2021](#bib.bib152)) | 2021 | ✓ | ✓
    | OCT, NIR 和 VW 图像 | VGG19，ResNet50，DenseNet121 |'
- en: '|  | Zhang et al. (Zhang et al., [2021](#bib.bib200)) | 2021 | ✓ | ✗ | ND-CLD-13,
    CASIA-Iris-Fake, IF-VE | VGG16 + WRN local attention and global classification
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | Zhang et al. (Zhang et al., [2021](#bib.bib200)) | 2021 | ✓ | ✗ | ND-CLD-13,
    CASIA-Iris-Fake, IF-VE | VGG16 + WRN 局部注意力和全局分类 |'
- en: '|  | Agarwal et al. (Agarwal et al., [2022a](#bib.bib2)) | 2022 | ✓ | ✗ | MUIPA,
    UnMIPA, IIITD-CLI, LivDet-2017 (IIITD-WVU), ND-PSID | Siamese contraction-expansion
    CNN, feature fusion |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | Agarwal et al. (Agarwal et al., [2022a](#bib.bib2)) | 2022 | ✓ | ✗ | MUIPA,
    UnMIPA, IIITD-CLI, LivDet-2017 (IIITD-WVU), ND-PSID | Siamese contraction-expansion
    CNN, 特征融合 |'
- en: '|  | Agarwal et al. (Agarwal et al., [2022b](#bib.bib3)) | 2022 | ✓ | ✗ | MUIPA,
    UnMIPA, IIITD-CLI, LivDet-2017 (IIITD-WVU), ND-PSID, NDIris3D | Siamese CNN, score
    fusion |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | Agarwal et al. (Agarwal et al., [2022b](#bib.bib3)) | 2022 | ✓ | ✗ | MUIPA,
    UnMIPA, IIITD-CLI, LivDet-2017 (IIITD-WVU), ND-PSID, NDIris3D | Siamese CNN，得分融合
    |'
- en: '|  | Gautam et al. (Gautam et al., [2022](#bib.bib60)) | 2022 | ✓ | ✗ | SYN,
    IIITD-CLI, IIITD-IS | Autoencoder with reconstruction and classification loss
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | Gautam et al. (Gautam et al., [2022](#bib.bib60)) | 2022 | ✓ | ✗ | SYN,
    IIITD-CLI, IIITD-IS | 自编码器与重建和分类损失 |'
- en: '|  | Tapia et al. (Tapia et al., [2022](#bib.bib163)) | 2022 | ✓ | ✓ | LivDet-2020,
    Iris-CL1, Warsaw-Post-Mortem v3.0 | MobileNetv2, data augmentation, class-weights
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | Tapia et al. (Tapia et al., [2022](#bib.bib163)) | 2022 | ✓ | ✓ | LivDet-2020,
    Iris-CL1, Warsaw-Post-Mortem v3.0 | MobileNetv2，数据增强，类别权重 |'
- en: '| Hybrid Methods | Choudhary et al. (Choudhary et al., [2022b](#bib.bib28))
    | 2022 | ✓ | ✗ | IIITD-CLI, ND-CLD-13, CASIA, LivDet-2017 (IIITD-WVU, ND-CLD,
    Clarkson) | MBISF (domain-specific filters), SIFT, Haralick, DenseNet, VGG8 +
    SVM classification |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 混合方法 | Choudhary et al. (Choudhary et al., [2022b](#bib.bib28)) | 2022 |
    ✓ | ✗ | IIITD-CLI, ND-CLD-13, CASIA, LivDet-2017 (IIITD-WVU, ND-CLD, Clarkson)
    | MBISF（领域特定滤波器），SIFT，Haralick，DenseNet，VGG8 + SVM 分类 |'
- en: '|  | Choudhary et al. (Choudhary et al., [2022a](#bib.bib27)) | 2022 | ✓ |
    ✗ | IIITD-CLI, ND-CLD-13, LivDet-2017 (Clarkson) | MBSIF (generic filters), MBSIF
    (domain-specific filters), SIFT, LBPV, DAISY, DenseNet121 + SVM classification
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | Choudhary et al. (Choudhary et al., [2022a](#bib.bib27)) | 2022 | ✓ |
    ✗ | IIITD-CLI, ND-CLD-13, LivDet-2017 (Clarkson) | MBSIF（通用滤波器），MBSIF（领域特定滤波器），SIFT，LBPV，DAISY，DenseNet121
    + SVM 分类 |'
- en: '| Adversarial Networks | Yadav and Ross (Yadav and Ross, [2021](#bib.bib194))
    | 2021 | ✓ | ✗ | Casia-Iris-Fake, Berc-iris-fake, ND-CLD-15, LivDet-2017, MSU-IrisPA-01
    | BSIF, DESIST, VGG16, D-NetPAD, AlexNet |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 对抗网络 | Yadav and Ross (Yadav and Ross, [2021](#bib.bib194)) | 2021 | ✓ |
    ✗ | Casia-Iris-Fake, Berc-iris-fake, ND-CLD-15, LivDet-2017, MSU-IrisPA-01 | BSIF,
    DESIST, VGG16, D-NetPAD, AlexNet |'
- en: 5\. Deep Learning-Based Forensic Iris Recognition
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 基于深度学习的法医虹膜识别
- en: Iris recognition has become the next biometric mode (in addition to face, fingerprints
    and palmprints) considered for large-scale forensic applications (FBI Criminal
    Justice Information Services (CJIS) Division, [2021](#bib.bib53)), and coincides
    in time with discoveries made in recent years about possibility to employ iris
    in recognition of deceased subjects. This includes both matching of iris patterns
    acquired a few hours after death with those with longer PMIs (Post-Mortem Intervals),
    ranging from days (Sauerwein et al., [2017](#bib.bib144); Bolme et al., [2016](#bib.bib13);
    Trokielewicz et al., [2016b](#bib.bib169), [a](#bib.bib168)) to several weeks
    after demise (Trokielewicz et al., [2019](#bib.bib171); Boyd et al., [2020b](#bib.bib19)),
    as well as matching patterns acquired before death with those collected post-mortem
    (Sansola, [2015](#bib.bib142)).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 虹膜识别已成为除面部、指纹和掌纹外，考虑用于大规模法医应用的下一种生物识别模式（FBI 犯罪司法信息服务 (CJIS) 部门，[2021](#bib.bib53)），并且与近年来关于使用虹膜识别已故对象的发现相吻合。这包括将死亡几小时后获取的虹膜图案与具有较长死亡后间隔（PMI）的图案进行匹配，PMI
    范围从几天（Sauerwein et al., [2017](#bib.bib144); Bolme et al., [2016](#bib.bib13);
    Trokielewicz et al., [2016b](#bib.bib169), [a](#bib.bib168)）到死亡几周后（Trokielewicz
    et al., [2019](#bib.bib171); Boyd et al., [2020b](#bib.bib19)），以及将死亡前获取的图案与尸检后收集的图案进行匹配（Sansola,
    [2015](#bib.bib142)）。
- en: '![Refer to caption](img/2ded5af25ce7ae3cdd407bcb301e247d.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/2ded5af25ce7ae3cdd407bcb301e247d.png)'
- en: (a)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/8c6e2401cb6d939a61d6bde783a8dcb0.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/8c6e2401cb6d939a61d6bde783a8dcb0.png)'
- en: (b)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/51dd785471e9aa2755919ddfc026bde1.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/51dd785471e9aa2755919ddfc026bde1.png)'
- en: (c)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 3. Post-mortem iris recognition and visualization: (a) a good-quality
    post-mortem iris image; (b) top to bottom: deep learning-based detection of iris
    annulus, specular highlights and decomposition-induced wrinkles; (c) segmentation
    results presented to a human examiner along with an overlaid heatmap visualizing
    regions judged as salient by the matching algorithm. Source: (Kuehlkamp et al.,
    [2022](#bib.bib92))'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. 死后虹膜识别与可视化：（a）优质的死后虹膜图像；（b）从上到下：基于深度学习的虹膜环带、镜面高光和分解引起的皱纹的检测；（c）将分割结果呈现给人工检查员，并叠加一个热图，展示匹配算法判断为显著的区域。来源：（Kuehlkamp
    等，[2022](#bib.bib92)）
- en: 'Due to decomposition changes to the eye tissues, post-mortem iris images differ
    significantly from live iris images and rarely meet ISO/IEC 29794-6 quality requirements,
    as shown in Fig. [3](#S5.F3 "Figure 3 ‣ 5\. Deep Learning-Based Forensic Iris
    Recognition ‣ Deep Learning for Iris Recognition: A Survey")(a). The challenges
    are related to appropriate detection of places when cornea dries and generates
    irregular and large specular highlights, as well regions where iris muscle furrows
    show up when the eyeball dehydrates. This is where DL-based methods may win over
    hand crafted approaches, as the latter usually make strong assumptions about anatomy
    of the iris appearance, not possible to be predicted for eyes undergoing random
    decomposition processes. Trokielewicz et al. proposed the first known to us iris
    recognition method designed specifically to cadaver irises (Trokielewicz et al.,
    [2020](#bib.bib172), [2020](#bib.bib173)). It incorporates SegNet-based segmenter
    and Siamese networks-based feature extractor, both trained in a domain-specific
    way solely on post-mortem iris samples. An interesting element of this approach
    is that segmetation incorporates two models: one trained with “fine” ground truth
    masks, marking all details associated with eye decomposition, and “coarse” model,
    aiming at detecting iris annulus and eyelids, as in classical iris recognition
    approaches. This allowed to apply a standard “rubber sheet” iris images normalization
    based on “coarse” masks, and at the same time exclude decomposition-driven artifacts
    from encoding, marked by the “fine” mask. Kuehlkamp et al. (Kuehlkamp et al.,
    [2022](#bib.bib92)) in addition to detecting post-mortem deformations, as shown
    in Fig. [3](#S5.F3 "Figure 3 ‣ 5\. Deep Learning-Based Forensic Iris Recognition
    ‣ Deep Learning for Iris Recognition: A Survey")(c), they also proposed a human-interpretable
    visualization of a classification process. The visualization is based on Class
    Activation Mapping mechanism (Zhou et al., [2016](#bib.bib213)) and highlights
    salient features used by the classifier in its judgment. This novelty in iris
    recognition algorithms may help human examiners to locate iris regions that should
    be carefully inspected, or to verify the algorithm’s decision.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '由于眼组织的分解变化，死后虹膜图像与活体虹膜图像有显著差异，并且很少符合 ISO/IEC 29794-6 质量要求，如图 [3](#S5.F3 "Figure
    3 ‣ 5\. Deep Learning-Based Forensic Iris Recognition ‣ Deep Learning for Iris
    Recognition: A Survey")(a) 所示。挑战与适当检测眼角膜干燥后产生的不规则且较大的镜面高光以及眼球脱水后虹膜肌肉皱纹的区域有关。这是深度学习（DL）方法可能胜过手工方法的地方，因为后者通常对虹膜的解剖结构做出强假设，而这种结构在眼睛经历随机分解过程时无法预测。Trokielewicz
    等提出了我们所知的第一个专门为尸体虹膜设计的虹膜识别方法（Trokielewicz 等，[2020](#bib.bib172)，[2020](#bib.bib173)）。该方法包括基于
    SegNet 的分割器和基于 Siamese 网络的特征提取器，两者都是专门在死后虹膜样本上训练的。该方法的一个有趣元素是分割包含两个模型：一个是用“细致”地面真实标记训练的，标记所有与眼部分解相关的细节；另一个是“粗略”模型，旨在检测虹膜环带和眼睑，类似于经典的虹膜识别方法。这使得基于“粗略”标记的标准“橡皮布”虹膜图像标准化成为可能，同时从编码中排除了由“细致”标记标记的分解驱动的伪影。Kuehlkamp
    等（Kuehlkamp 等，[2022](#bib.bib92)）除了检测死后变形，如图 [3](#S5.F3 "Figure 3 ‣ 5\. Deep Learning-Based
    Forensic Iris Recognition ‣ Deep Learning for Iris Recognition: A Survey")(c)
    所示，他们还提出了分类过程的人工可解释可视化。该可视化基于类激活映射机制（Zhou 等，[2016](#bib.bib213)），突出显示分类器在其判断中使用的显著特征。这种虹膜识别算法的新颖性可能有助于人工检查员定位需要仔细检查的虹膜区域，或验证算法的决策。'
- en: 6\. Human-Machine Pairing to Improve Deep Learning-Based Iris Recognition
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 人机配对以改善基于深度学习的虹膜识别
- en: 'Iris recognition is usually associated with automatic, solely machine-based
    and rapid biometric means. It has been changing in the recent decade due to constantly
    increasing ubiquitousness of iris recognition, especially owing to large governmental
    applications such as (Unique Identification Authority of India, [2021](#bib.bib175))
    or FBI’s Next Generation Identification System (NGI) gradually replacing the Integrated
    Automated Fingerprint Identification System (IAFIS) (FBI Criminal Justice Information
    Services (CJIS) Division, [2021](#bib.bib53)). This combined with unique identification
    power of iris whetted the appetite to apply this technique to identification problems
    normally reserved for fingerprints and face: forensics, lost subjects search or
    post-mortem identification. To have the legal power, however, the judgment about
    samples originating or not from the same eye conclusion must be confirmed by a
    trained human expert. And here is the place where DL-based iris image processing
    may play a useful role.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 虹膜识别通常与自动化、完全基于机器且快速的生物识别手段相关联。由于虹膜识别的普及性日益增加，尤其是由于大型政府应用（例如印度独特身份认证局，[2021](#bib.bib175)）或FBI的下一代身份识别系统（NGI）逐步取代集成自动指纹识别系统（IAFIS）（FBI刑事司法信息服务（CJIS）部门，[2021](#bib.bib53)），这一点在最近十年发生了变化。虹膜的独特识别能力激发了将该技术应用于通常仅限于指纹和面部识别的识别问题：法医鉴定、失踪人员搜寻或尸检鉴定的兴趣。然而，为了具备法律效力，关于样本是否来源于同一只眼睛的判断必须由经过培训的人工专家确认。在这里，基于深度学习的虹膜图像处理可能发挥有用的作用。
- en: Trokielewicz *et al.* compared iris images in post-mortem iris recognition between
    humans and machines. They investigated which iris image regions humans and machines
    mainly attend to compare a pair of images. The machine-based attention maps are
    generated by Grad-CAM to highlight the regions that contribute the most to the
    deep learning model’s prediction. The human-based attention maps are learned by
    tracking the gaze as the human is looking around the screen that display iris
    image pairs and recording the regions where the human spend most time on. Interestingly
    while humans and machines tend to focus on a limited number of iris areas, however,
    the region, appearance and density of these areas between humans and machines
    are different. As salient regions proposed by the deep learning model and identified
    from human eye gaze do not overlap in general, the computer-added visual cues
    may potentially constitute a valuable addition to the forensic examiner’s expertise,
    as it can highlight important discriminatory regions that the human expert might
    miss in their proceedings. This human-machine pairing is important as human subjects
    can provide an incorrect decision even despite spending quite sometime observing
    many iris regions (NIST, [2021](#bib.bib121)). In addition, there has been a body
    of research showing that humans and machines do not perform similarly well under
    different conditions (Stark et al., [2010](#bib.bib155); Chen et al., [2016](#bib.bib21);
    Moreira et al., [2019](#bib.bib109)). For example, Moreira *et al.* also showed
    that machines can outperform humans in healthy easy iris image pairs; however,
    humans outperform machines in disease-affected iris image pairs (Moreira et al.,
    [2019](#bib.bib109)). Human-machine pairing will improve deep learning based iris
    recognition.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Trokielewicz *等* 比较了人类和机器在尸检虹膜识别中的虹膜图像。他们调查了人类和机器在比较一对图像时主要关注的虹膜图像区域。机器基于的注意力图由Grad-CAM生成，以突出对深度学习模型预测贡献最大的区域。人类基于的注意力图通过跟踪人类在查看显示虹膜图像对的屏幕时的视线并记录人类花费最多时间的区域来学习。有趣的是，尽管人类和机器往往集中在有限数量的虹膜区域上，但这些区域在人类和机器之间的外观和密度却有所不同。由于深度学习模型提出的显著区域和人眼注视识别的区域通常不重叠，计算机添加的视觉提示可能会对法医检查员的专业知识构成有价值的补充，因为它可以突出人类专家在处理过程中可能遗漏的重要区分区域。人机配对很重要，因为人类受试者即使花费相当多的时间观察许多虹膜区域，也可能提供不正确的决定（NIST，[2021](#bib.bib121)）。此外，有研究表明，人类和机器在不同条件下的表现不同（Stark等，[2010](#bib.bib155)；Chen等，[2016](#bib.bib21)；Moreira等，[2019](#bib.bib109)）。例如，Moreira
    *等* 还表明，机器在健康的容易识别的虹膜图像对中可以超越人类；然而，在疾病影响的虹膜图像对中，人类表现优于机器（Moreira等，[2019](#bib.bib109)）。人机配对将改善基于深度学习的虹膜识别。
- en: '7\. Recognition in Less Controlled Environments: Iris/Periocular Analysis'
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 在较少受控环境中的识别：虹膜/周边眼部分析
- en: Rooted in the seminal work due to Park *et* al. (Park et al., [2011](#bib.bib125)),
    efforts have been paid to the development of human recognition methods that -
    apart the iris - also consider information in the vicinity of the eye to infer
    the identity. This is a relatively recent topic, termed as *periocular recognition*.
    The rationale is that the periocular region represents a trade-off between the
    face and the iris. Periocular biometrics has been claimed to be particularly useful
    in environments that produce poor quality data (e.g., visual surveillance). Recently,
    as in the case of iris, several DL-based solutions have been proposed.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Park *等*（Park et al., [2011](#bib.bib125)）的开创性工作，已经付出了努力来发展人脸识别方法，这些方法除了虹膜外，还考虑了眼睛附近的信息以推断身份。这是一个相对较新的主题，称为*眼周识别*。其原理是眼周区域代表了面部与虹膜之间的权衡。眼周生物识别在产生低质量数据的环境中（例如视觉监控）被认为特别有用。最近，就像虹膜识别一样，提出了几种基于深度学习的解决方案。
- en: Hernandez-Diaz *et* al. (Hernandez-Diaz et al., [2018](#bib.bib71)) tested the
    suitability of off-the-shelf CNN architectures to the periocular recognition task,
    observing that albeit such networks are optimized to classify generic objects,
    their features still can be effectively transferred to the periocular domain.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Hernandez-Diaz *等*（Hernandez-Diaz et al., [2018](#bib.bib71)）测试了现成的CNN架构在眼周识别任务中的适用性，观察到尽管这些网络被优化用于分类通用对象，它们的特征仍然可以有效地转移到眼周领域。
- en: In the visual surveillance context, Kim *et* al. (Kim et al., [2018](#bib.bib89))
    infer subjects identities based either in loose/tight regions-of-interest, depending
    of the perceived image quality. Hwang and Lee (Hwang and Lee, [2020](#bib.bib78))prevents
    the loss of mid-level features and dynamically selects the most important features
    for classification. Luo *et* al. (Luo et al., [2021](#bib.bib103)) used self-attention
    channel and spatial mechanisms into the feature encoding module of a CNN, in order
    to obtain the most discriminative features of the iris and periocular regions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉监控的背景下，Kim *等*（Kim et al., [2018](#bib.bib89)）根据图像质量的感知，基于松散/紧凑的感兴趣区域来推断被试的身份。Hwang和Lee（Hwang
    and Lee, [2020](#bib.bib78)）防止了中级特征的丢失，并动态选择最重要的特征用于分类。Luo *等*（Luo et al., [2021](#bib.bib103)）在CNN的特征编码模块中使用了自注意力通道和空间机制，以获得虹膜和眼周区域的最具区分性的特征。
- en: Jung *et* al. (Jung et al., [2020](#bib.bib83))’s work is based in the concept
    of label smoothing regularization (LSR). Having as main goal to reduce the intra-class
    variability, they described a so-called Generalized LSR (GLSR) by learning a pre-task
    network prediction that is claimed to improve the permanence of the obtained periocular
    features. Having similar purposes, Zanlorensi *et* al. (Zanlorensi et al., [2020](#bib.bib199))
    described a preprocessing step based in generative networks able to compensate
    for the typical data variations in visual surveillance environments. Nie *et*
    al. (Nie et al., [2014](#bib.bib119)) applied convolutional restricted Boltzmann
    machines to the periocular recognition problem. Starting from a set of genuine
    pairs that are used as a constraint, a Mahalanobis distance-metric is learned.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Jung *等*（Jung et al., [2020](#bib.bib83)）的工作基于标签平滑正则化（LSR）概念。其主要目标是减少类内变异性，他们描述了一种所谓的广义LSR（GLSR），通过学习一个前任务网络预测，声称可以改善所获得的眼周特征的持久性。Zanlorensi
    *等*（Zanlorensi et al., [2020](#bib.bib199)）有类似的目的，描述了一种基于生成网络的预处理步骤，能够补偿视觉监控环境中的典型数据变化。Nie
    *等*（Nie et al., [2014](#bib.bib119)）将卷积限制玻尔兹曼机应用于眼周识别问题。从一组用于约束的真实对开始，学习了马氏距离度量。
- en: Obtaining auxiliary (e.g., soft biometrics) has been seen as an interesting
    direction for compensating the lack of image quality. Zhao and Kumar (Zhao and
    Kumar, [2018](#bib.bib211)) incorporate an attention model into a DL-architecture
    to emphasize the most important regions in the periocular data. The same authors (Zhao
    and Kumar, [2017a](#bib.bib209)) described a semantics-assisted CNN framework
    to infer comprehensive periocular features. The whole model is composed of different
    networks, trained upon ID and semantic (e.g., gender, ethnicity) data, that are
    fused at the score and prediction levels. Similarly, Talreja *et* al. (Talreja
    et al., [2022](#bib.bib159)) described a multi-branch CNN framework that predicts
    simultaneously soft biometrics and ID labels, which are finally fused into the
    final response.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 获取辅助信息（例如，软生物特征）被认为是补偿图像质量不足的一个有趣方向。Zhao和Kumar（Zhao and Kumar，[2018](#bib.bib211)）在DL架构中加入了注意力模型，以强调眼周数据中最重要的区域。同样的作者（Zhao
    and Kumar，[2017a](#bib.bib209)）描述了一个语义辅助的CNN框架，用于推断综合的眼周特征。整个模型由不同的网络组成，这些网络在ID和语义（例如，性别、种族）数据上进行训练，并在得分和预测层面进行融合。类似地，Talreja
    *et* al.（Talreja et al., [2022](#bib.bib159)）描述了一个多分支CNN框架，该框架同时预测软生物特征和ID标签，最终将这些标签融合成最终的响应。
- en: With regard to cross-spectral settings, Hernandez-Diaz *et* al. (Hernandez-Diaz
    et al., [2020](#bib.bib72)) used conditional GANs (CGANs) to convert periocular
    images between domains, that are further fed to intra-domain off-the-self frameworks.
    Sharma *et* el. (Sharma et al., [2014](#bib.bib149)) described a shallow neural
    architecture where each model learns the data features in each spectrum. Then,
    at a subsequent phase, all models are jointly fine tuned, to learn the cross-spectral
    variability and correspondence features.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 关于跨光谱设置，Hernandez-Diaz *et* al.（Hernandez-Diaz et al., [2020](#bib.bib72)）使用条件GAN（CGANs）在不同领域之间转换眼周图像，然后将这些图像输入到域内的现成框架中。Sharma
    *et* el.（Sharma et al., [2014](#bib.bib149)）描述了一种浅层神经架构，其中每个模型学习每个光谱中的数据特征。然后，在随后的阶段，所有模型一起微调，以学习跨光谱变异性和对应特征。
- en: Finally, several works have attempted to faithfully fuse the scores/responses
    from iris and periocular data. Wang and Kumar (Wang and Kumar, [2021](#bib.bib182))
    used periocular features to adaptively match iris data acquired in less constrained
    conditions. Their framework incorporates such discriminative information using
    a multilayer perceptron network. Zhang *et* al. (Zhang et al., [2018](#bib.bib204))
    described a DL-model that exploits complementary information from the iris and
    the periocular regions, that applies *maxout* units to obtain compact representations
    for each modality and then fuses the discriminative features of the modalities
    through weighted concatenation. In an opposite direction, Proença and Neves (Proença
    and Neves, [2018](#bib.bib135)) argued that the periocular recognition performance
    is optimized when the components inside the ocular globe (the iris and the sclera)
    are simply discarded.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一些研究试图真实地融合虹膜和眼周数据的得分/响应。Wang和Kumar（Wang and Kumar，[2021](#bib.bib182)）利用眼周特征来自适应匹配在不太受限条件下获取的虹膜数据。他们的框架使用多层感知机网络来结合这些区分性信息。Zhang
    *et* al.（Zhang et al., [2018](#bib.bib204)）描述了一种利用虹膜和眼周区域的互补信息的DL模型，该模型应用*maxout*单元为每种模式获得紧凑的表示，然后通过加权连接融合这些模式的区分性特征。相反，Proença和Neves（Proença
    and Neves，[2018](#bib.bib135)）认为，当眼球内部的组件（虹膜和巩膜）被简单地丢弃时，眼周识别性能会得到优化。
- en: 'Table 4. Summary of datasets used in the DL-based iris segmentation and recognition
    methods of Tables [1](#S2.T1 "Table 1 ‣ 2\. Deep Learning-Based Iris Segmentation
    ‣ Deep Learning for Iris Recognition: A Survey") and [2](#S3.T2 "Table 2 ‣ 3.2.3\.
    Iris Similarity Networks ‣ 3.2\. Deep Learning-based Iris Matching Strategies
    ‣ 3\. Deep Learning-Based Iris Recognition ‣ Deep Learning for Iris Recognition:
    A Survey") (NIR: *near-infrared*; VW: *visible wavelength*).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '表4. 用于表格[1](#S2.T1 "Table 1 ‣ 2\. Deep Learning-Based Iris Segmentation ‣ Deep
    Learning for Iris Recognition: A Survey")和[2](#S3.T2 "Table 2 ‣ 3.2.3\. Iris Similarity
    Networks ‣ 3.2\. Deep Learning-based Iris Matching Strategies ‣ 3\. Deep Learning-Based
    Iris Recognition ‣ Deep Learning for Iris Recognition: A Survey")中基于DL的虹膜分割和识别方法所用数据集的总结（NIR:
    *近红外*；VW: *可见光波长*）。'
- en: '| Name | Data | Size | # IDs | # Samples | # Sessions | Features |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 数据 | 大小 | # ID | # 样本 | # 会话 | 特征 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| BATH (Monro et al., [2007](#bib.bib108)) | NIR | 1280$\times$960 | 1600 |
    16000 | 1 | High quality images |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| BATH (Monro et al., [2007](#bib.bib108)) | NIR | 1280$\times$960 | 1600 |
    16000 | 1 | 高质量图像 |'
- en: '| BioSec (Fierrez et al., [2007](#bib.bib54)) | NIR | 640$\times$480 | 400
    | 3200 | 2 | Office environment |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| BioSec (Fierrez et al., [2007](#bib.bib54)) | NIR | 640$\times$480 | 400
    | 3200 | 2 | 办公环境 |'
- en: '| Biosecure (Ortega et al., [2010](#bib.bib124)) | NIR | 640$\times$480 | 1334
    | 2668 | 2 | Office environment |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Biosecure (Ortega et al., [2010](#bib.bib124)) | NIR | 640$\times$480 | 1334
    | 2668 | 2 | 办公环境 |'
- en: '| CASIA-Cross-Sensor (Xiao et al., [2013](#bib.bib188)) | NIR | n/a | 700 |
    21000 | 1 | Multi-sensor, multi-distance (12-30cm, 3-5m) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| CASIA-Cross-Sensor (Xiao et al., [2013](#bib.bib188)) | NIR | 不适用 | 700 |
    21000 | 1 | 多传感器，多距离（12-30cm, 3-5m） |'
- en: '| CASIA-Iris-Distance (Dong et al., [2009](#bib.bib41)) | NIR | 2352$\times$1728
    | 284 | 2567 | 1 | Distant acquisition |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| CASIA-Iris-Distance (Dong et al., [2009](#bib.bib41)) | NIR | 2352$\times$1728
    | 284 | 2567 | 1 | 远距离采集 |'
- en: '| CASIA-Iris-Interval (Ma et al., [2003](#bib.bib104)) | NIR | 320$\times$280
    | 395 | 2639 | 2 | High quality images |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| CASIA-Iris-Interval (Ma et al., [2003](#bib.bib104)) | NIR | 320$\times$280
    | 395 | 2639 | 2 | 高质量图像 |'
- en: '| CASIA-Iris-Lamp (Wei et al., [2007](#bib.bib186)) | NIR | 640$\times$480
    | 819 | 16212 | 1 | Non-linear deformation |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| CASIA-Iris-Lamp (Wei et al., [2007](#bib.bib186)) | NIR | 640$\times$480
    | 819 | 16212 | 1 | 非线性变形 |'
- en: '| CASIA-Iris-M1-S1 (Zhang et al., [2015](#bib.bib205)) | NIR | 1920$\times$1080
    | 140 | 1400 | 1 | Mobile device |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| CASIA-Iris-M1-S1 (Zhang et al., [2015](#bib.bib205)) | NIR | 1920$\times$1080
    | 140 | 1400 | 1 | 移动设备 |'
- en: '| CASIA-Iris-M1-S2 (Zhang et al., [2016b](#bib.bib203)) | NIR | 1968$\times$1024
    | 400 | 6000 | 1 | Mobile device, multi-distance (20,25,30cm) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| CASIA-Iris-M1-S2 (Zhang et al., [2016b](#bib.bib203)) | NIR | 1968$\times$1024
    | 400 | 6000 | 1 | 移动设备，多距离（20,25,30cm） |'
- en: '| CASIA-Iris-M1-S3 (Zhang et al., [2018](#bib.bib204)) | NIR | 1920$\times$1920
    | 720 | 3600 | 1 | Mobile device |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| CASIA-Iris-M1-S3 (Zhang et al., [2018](#bib.bib204)) | NIR | 1920$\times$1920
    | 720 | 3600 | 1 | 移动设备 |'
- en: '| CASIA-Iris-Thousand (Zhang et al., [2010](#bib.bib201)) | NIR | 640$\times$480
    | 2000 | 20000 | 1 | High quality images |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| CASIA-Iris-Thousand (Zhang et al., [2010](#bib.bib201)) | NIR | 640$\times$480
    | 2000 | 20000 | 1 | 高质量图像 |'
- en: '| DCME01 (Boyd et al., [2020b](#bib.bib19)) | NIR, VW | n/a | 254 | 621 | 1-9
    | - |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| DCME01 (Boyd et al., [2020b](#bib.bib19)) | NIR, VW | 不适用 | 254 | 621 | 1-9
    | - |'
- en: '| DCME02 (Kuehlkamp et al., [2022](#bib.bib92)) | NIR | n/a | 259 | 5770 |
    1-53 | - |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| DCME02 (Kuehlkamp et al., [2022](#bib.bib92)) | NIR | 不适用 | 259 | 5770 |
    1-53 | - |'
- en: '| IITD (Kumar and Passi, [2010](#bib.bib93)) | NIR | 320$\times$240 | 224 |
    1120 | 1 | Varying quality |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| IITD (Kumar and Passi, [2010](#bib.bib93)) | NIR | 320$\times$240 | 224 |
    1120 | 1 | 质量变化 |'
- en: '| Iris-Mobile (Odinokikh et al., [2019](#bib.bib122)) | NIR | n/a | 750 | 22966
    | n/a | Mobile device, indoor & outdoor |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Iris-Mobile (Odinokikh et al., [2019](#bib.bib122)) | NIR | 不适用 | 750 | 22966
    | 不适用 | 移动设备，室内及室外 |'
- en: '| JluIrisV3.1 (Zhao et al., [2019](#bib.bib208)) | NIR | 640$\times$480 | 120
    | 1780 | n/a | - |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| JluIrisV3.1 (Zhao et al., [2019](#bib.bib208)) | NIR | 640$\times$480 | 120
    | 1780 | 不适用 | - |'
- en: '| JluIrisV4 (Zhao et al., [2019](#bib.bib208)) | NIR | 640$\times$480 | 172
    | 114904 | n/a | - |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| JluIrisV4 (Zhao et al., [2019](#bib.bib208)) | NIR | 640$\times$480 | 172
    | 114904 | 不适用 | - |'
- en: '| LivDet-2013-Warsaw (Czajka, [2013](#bib.bib29)) | NIR | 640$\times$480 |
    284 | 1667 | 1 | High quality images |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| LivDet-2013-Warsaw (Czajka, [2013](#bib.bib29)) | NIR | 640$\times$480 |
    284 | 1667 | 1 | 高质量图像 |'
- en: '| MICHE-I (De Marsico et al., [2015](#bib.bib39)) | VW | var. | 184 | 3732
    | 2 | Three mobile devices |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| MICHE-I (De Marsico et al., [2015](#bib.bib39)) | VW | 变化 | 184 | 3732 |
    2 | 三个移动设备 |'
- en: '| MMU | NIR | 320$\times$240 | 92 | 460 | 1 | High quality images |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| MMU | NIR | 320$\times$240 | 92 | 460 | 1 | 高质量图像 |'
- en: '| MobBIOfake (Sequeira et al., [2014](#bib.bib147)) | VW | 300$\times$200 |
    200 | 1600 | 1 | With a handheld device |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| MobBIOfake (Sequeira et al., [2014](#bib.bib147)) | VW | 300$\times$200 |
    200 | 1600 | 1 | 手持设备 |'
- en: '| ND-CrossSensor-2013 (Xiao et al., [2013](#bib.bib188)) | NIR | 640$\times$480
    | 1352 | 146550 | 27 | Multi-sensor |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| ND-CrossSensor-2013 (Xiao et al., [2013](#bib.bib188)) | NIR | 640$\times$480
    | 1352 | 146550 | 27 | 多传感器 |'
- en: '| ND-Iris-0405 (Phillips et al., [2010](#bib.bib128)) | NIR | 640$\times$480
    | 712 | 64980 | 1 | Varying quality |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| ND-Iris-0405 (Phillips et al., [2010](#bib.bib128)) | NIR | 640$\times$480
    | 712 | 64980 | 1 | 质量变化 |'
- en: '| ND-TWINS-2009-2010 | VW | n/a | 435 | 24050 | n/a | Facial pictures frontal,
    3/4 and side views. Indoor & outdoor |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| ND-TWINS-2009-2010 | VW | 不适用 | 435 | 24050 | 不适用 | 正面、三分之一和侧面人脸图像。室内及室外
    |'
- en: '| OpenEDS (Garbin et al., [2019](#bib.bib59)) | NIR | 640$\times$400 | 304
    | 356649 | 1 | From head-mounted VR glasses |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| OpenEDS (Garbin et al., [2019](#bib.bib59)) | NIR | 640$\times$400 | 304
    | 356649 | 1 | 头戴式 VR 眼镜 |'
- en: '| Q-FIRE (Johnson et al., [2010](#bib.bib82)) | NIR | var. | 390 | 586560 |
    2 | Iris/face Videos, various distances and quality |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Q-FIRE (Johnson et al., [2010](#bib.bib82)) | NIR | 变化 | 390 | 586560 | 2
    | 虹膜/面部视频，各种距离和质量 |'
- en: '| UBIRIS.v1 (Proença and Alexandre, [2005](#bib.bib131)) | VW | 800$\times$600
    | 241 | 1877 | 2 | Several noise factors |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| UBIRIS.v1 (Proença and Alexandre, [2005](#bib.bib131)) | VW | 800$\times$600
    | 241 | 1877 | 2 | 多种噪声因素 |'
- en: '| UBIRIS.v2 (Proenca et al., [2010](#bib.bib132)) | VW | 400$\times$300 | 522
    | 11102 | 2 | Distant acquisition, on the move |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| UBIRIS.v2 (Proenca et al., [2010](#bib.bib132)) | VW | 400$\times$300 | 522
    | 11102 | 2 | 远程采集，移动中 |'
- en: '| Warsaw (Boyd et al., [2020b](#bib.bib19)) | NIR, VW | n/a | 157 | 4866 |
    1-13 | - |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Warsaw (Boyd et al., [2020b](#bib.bib19)) | NIR, VW | 不适用 | 157 | 4866 |
    1-13 | - |'
- en: '| Warsaw-Post-Mortem v1.0 (Trokielewicz et al., [2016a](#bib.bib168)) | NIR,
    VW | var. | 34 | 1330 | 2-3 | Deceased persons, 5-7h to 17 days postmortem |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Warsaw-Post-Mortem v1.0 (Trokielewicz et al., [2016a](#bib.bib168)) | NIR,
    VW | 变化 | 34 | 1330 | 2-3 | 死者，死后5-7小时至17天 |'
- en: '| Warsaw-Post-Mortem v2.0 (Trokielewicz et al., [2019](#bib.bib171)) | NIR,
    VW | var. | 73 | 2987 | 1-13 | Deceased persons |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Warsaw-Post-Mortem v2.0 (Trokielewicz et al., [2019](#bib.bib171)) | NIR,
    VW | 变化 | 73 | 2987 | 1-13 | 死者 |'
- en: 8\. Open-Source Deep Learning-Based Iris Recognition Tools
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 开源深度学习基础的虹膜识别工具
- en: Here we summarize the main properties of the datasets employed by the methods
    of the previous sections for DL-based iris segmentation, recognition and PAD.
    We also describe available open-source software code for these tasks, and other
    relevant tools.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了上一节中用于基于深度学习的虹膜分割、识别和演示攻击检测的方法所使用的数据集的主要属性。我们还描述了这些任务的开放源代码软件及其他相关工具。
- en: 8.1\. Data Sources
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 数据来源
- en: 'Table [4](#S7.T4 "Table 4 ‣ 7\. Recognition in Less Controlled Environments:
    Iris/Periocular Analysis ‣ Deep Learning for Iris Recognition: A Survey") gives
    the technical details of the datasets used in the segmentation and recognition
    methods of Tables [1](#S2.T1 "Table 1 ‣ 2\. Deep Learning-Based Iris Segmentation
    ‣ Deep Learning for Iris Recognition: A Survey") and [2](#S3.T2 "Table 2 ‣ 3.2.3\.
    Iris Similarity Networks ‣ 3.2\. Deep Learning-based Iris Matching Strategies
    ‣ 3\. Deep Learning-Based Iris Recognition ‣ Deep Learning for Iris Recognition:
    A Survey"). Table [5](#S8.T5 "Table 5 ‣ 8.1\. Data Sources ‣ 8\. Open-Source Deep
    Learning-Based Iris Recognition Tools ‣ Deep Learning for Iris Recognition: A
    Survey") does the same for the iris PAD methods of Table [3](#S4.T3 "Table 3 ‣
    4.5\. Open Research Questions in Iris PAD ‣ 4\. Deep Learning-Based Iris Presentation
    Attack Detection ‣ Deep Learning for Iris Recognition: A Survey"). We show the
    main properties (spectrum, image size, identities, images, sessions) and relevant
    features. Only the datasets of the methods reported in previous section are presented.
    Since we focus on the most recent developments, we consider that such approach
    provides the most relevant datasets for each task. Of course, the list of available
    datasets after decades of iris research is much longer (Omelina et al., [2021](#bib.bib123)).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 表[4](#S7.T4 "表 4 ‣ 7\. 在控制条件较少的环境中的识别：虹膜/眼周分析 ‣ 基于深度学习的虹膜识别：综述")给出了表[1](#S2.T1
    "表 1 ‣ 2\. 基于深度学习的虹膜分割 ‣ 基于深度学习的虹膜识别：综述")和[2](#S3.T2 "表 2 ‣ 3.2.3\. 虹膜相似性网络 ‣
    3.2\. 基于深度学习的虹膜匹配策略 ‣ 3\. 基于深度学习的虹膜识别 ‣ 基于深度学习的虹膜识别：综述")中所使用的数据集的技术细节。表[5](#S8.T5
    "表 5 ‣ 8.1\. 数据来源 ‣ 8\. 开源深度学习基础的虹膜识别工具 ‣ 基于深度学习的虹膜识别：综述")对表[3](#S4.T3 "表 3 ‣
    4.5\. 虹膜演示攻击中的开放研究问题 ‣ 4\. 基于深度学习的虹膜演示攻击检测 ‣ 基于深度学习的虹膜识别：综述")中的虹膜PAD方法做了相同的处理。我们展示了主要属性（光谱、图像尺寸、身份、图像、会话）和相关特征。仅展示了上一节中报告的方法的数据集。由于我们关注的是最新进展，我们认为这种方法提供了每个任务最相关的数据集。当然，经过数十年的虹膜研究，现有数据集的列表要长得多（Omelina
    et al., [2021](#bib.bib123)）。
- en: 'A first observation is the dominance of near infrared (NIR) over the visible
    (VW) spectrum, which should not be surprising, since NIR is regarded as most suitable
    for iris analysis. However, research-wise, many segmentation and recognition studies
    (Tables [1](#S2.T1 "Table 1 ‣ 2\. Deep Learning-Based Iris Segmentation ‣ Deep
    Learning for Iris Recognition: A Survey"), [2](#S3.T2 "Table 2 ‣ 3.2.3\. Iris
    Similarity Networks ‣ 3.2\. Deep Learning-based Iris Matching Strategies ‣ 3\.
    Deep Learning-Based Iris Recognition ‣ Deep Learning for Iris Recognition: A Survey"))
    use VW images, pushed by the success of challenging databases such as MICHE and
    UBIRIS. On the contrary, the VW modality in iris PAD research is residual (Table [3](#S4.T3
    "Table 3 ‣ 4.5\. Open Research Questions in Iris PAD ‣ 4\. Deep Learning-Based
    Iris Presentation Attack Detection ‣ Deep Learning for Iris Recognition: A Survey")),
    a tendency also observed in pre-DL research (Czajka and Bowyer, [2018](#bib.bib31);
    Boyd et al., [2020a](#bib.bib17)).'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '首先观察到的是近红外（NIR）光谱对可见光（VW）光谱的主导地位，这不应令人惊讶，因为NIR被认为最适合虹膜分析。然而，研究中许多分割和识别研究（表[1](#S2.T1
    "Table 1 ‣ 2\. Deep Learning-Based Iris Segmentation ‣ Deep Learning for Iris
    Recognition: A Survey")， [2](#S3.T2 "Table 2 ‣ 3.2.3\. Iris Similarity Networks
    ‣ 3.2\. Deep Learning-based Iris Matching Strategies ‣ 3\. Deep Learning-Based
    Iris Recognition ‣ Deep Learning for Iris Recognition: A Survey")）使用VW图像，这是由于MICHE和UBIRIS等挑战性数据库的成功。相反，虹膜PAD研究中的VW模态是残余的（表[3](#S4.T3
    "Table 3 ‣ 4.5\. Open Research Questions in Iris PAD ‣ 4\. Deep Learning-Based
    Iris Presentation Attack Detection ‣ Deep Learning for Iris Recognition: A Survey")），这一趋势在DL前的研究中也有观察到（Czajka和Bowyer，[2018](#bib.bib31)；Boyd等，[2020a](#bib.bib17)）。'
- en: 'When it comes to the types of Presentation Attack Instruments (PAIs) employed
    in iris PAD databases, they can be categorized into:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 关于虹膜PAD数据库中使用的展示攻击工具（PAIs）的类型，可以将其分类为：
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'PP: paper printout of a real iris image, i.e. from a live person'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'PP: 实际虹膜图像的纸质打印输出，即来自活体人'
- en: •
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'PPD: paper printout of a real iris image with a transparent 3D plastic eye
    dome on top'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'PPD: 实际虹膜图像的纸质打印输出，上面有一个透明的3D塑料眼罩'
- en: •
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CLL: textured contact lenses worn by a live person'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CLL: 由活体人佩戴的纹理隐形眼镜'
- en: •
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CLP: textured contact lenses on printout (either a printout of a CLL image,
    or a printout of a real iris image with a textured contact lens placed on top)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CLP: 纹理隐形眼镜的打印输出（可以是CLL图像的打印输出，也可以是实际虹膜图像上放置了纹理隐形眼镜的打印输出）'
- en: •
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RA: replay attack, i.e. a real iris image shown on a display'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RA: 回放攻击，即在显示器上展示的实际虹膜图像'
- en: •
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'AE: artificial eyeball (plastic eyes of two different types: Van Dyke Eyes,
    with higher iris quality details, and Scary eyes, plastic fake eyes with a simple
    pattern on the iris region)'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'AE: 人工眼球（两种不同类型的塑料眼睛：Van Dyke眼睛，具有更高的虹膜质量细节，以及Scary眼睛，虹膜区域有简单图案的假眼）'
- en: •
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'AEC: artificial eyeball with a textured contact lens on top'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'AEC: 人工眼球，上面有纹理隐形眼镜'
- en: •
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SY: synthetic iris, i.e. an image created via generative methods'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SY: 合成虹膜，即通过生成方法创建的图像'
- en: •
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'PM: postmortem iris, i.e. an image acquired from cadaver eyes'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'PM: 尸体虹膜，即从尸体眼睛中获取的图像'
- en: 'These PAIs mostly entail presenting the mentioned instrument to the iris sensor,
    which then captures an image of the artifact. An exception is “SY”, which directly
    produces a synthetic digital image, although such image could be used as base
    to, for example, PP, PPD, RA, or AE attacks. In Table [5](#S8.T5 "Table 5 ‣ 8.1\.
    Data Sources ‣ 8\. Open-Source Deep Learning-Based Iris Recognition Tools ‣ Deep
    Learning for Iris Recognition: A Survey"), it can be seen that CLL (textured lenses
    live) and PP (paper printouts) largely dominates as the most popular PAIs on the
    existing databases, and consequently, on the related research (Table [3](#S4.T3
    "Table 3 ‣ 4.5\. Open Research Questions in Iris PAD ‣ 4\. Deep Learning-Based
    Iris Presentation Attack Detection ‣ Deep Learning for Iris Recognition: A Survey")).
    CLP (textured lenses on printout) also appears in many studies, driven by the
    wide use of the LivDet-2017-IIITD-WVU set, which includes such PAI. CASIA-Iris-Fake,
    which contains AE (artificial eyes) and SY (synthetic irises) also appears in
    a few studies. Other attacks that one may expect on the digital era, such as RA
    (replay), however, are residual in datasets and recent studies.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '这些PAIs主要包括将提到的仪器放置在虹膜传感器前，然后传感器捕捉到该物体的图像。例外的是“SY”，它直接生成合成数字图像，虽然这种图像可以作为基础，例如用于PP、PPD、RA或AE攻击。在表[5](#S8.T5
    "Table 5 ‣ 8.1\. Data Sources ‣ 8\. Open-Source Deep Learning-Based Iris Recognition
    Tools ‣ Deep Learning for Iris Recognition: A Survey")中，可以看出CLL（纹理镜片实时）和PP（纸质打印件）在现有数据库中占主导地位，因此在相关研究中也占据重要地位（表[3](#S4.T3
    "Table 3 ‣ 4.5\. Open Research Questions in Iris PAD ‣ 4\. Deep Learning-Based
    Iris Presentation Attack Detection ‣ Deep Learning for Iris Recognition: A Survey")）。CLP（打印件上的纹理镜片）也出现在许多研究中，这得益于LivDet-2017-IIITD-WVU数据集的广泛使用，该数据集包含这样的PAI。包含AE（人造眼睛）和SY（合成虹膜）的CASIA-Iris-Fake也出现在一些研究中。然而，数字时代可能出现的其他攻击，如RA（重放攻击），在数据集和最新研究中仍然存在。'
- en: 'Table 5. Summary of datasets used in the DL-based iris Presentation Attack
    Detection methods of Table [3](#S4.T3 "Table 3 ‣ 4.5\. Open Research Questions
    in Iris PAD ‣ 4\. Deep Learning-Based Iris Presentation Attack Detection ‣ Deep
    Learning for Iris Recognition: A Survey") (NIR: *near-infrared*; VW: *visible
    wavelength*). The type of PAIs (second column) are PP: paper printout, PPD: paper
    printout with plastic dome, CLL: textured contact lenses (live), CLP: textured
    contact lenses (printout), RA: replay attack (display), AE: artificial eyeball,
    AEC: artificial eyeball with textured contact lens, SY: synthetic iris, PM: postmortem
    iris. TTP (next to last column) indicates the existence of a training/test split.
    The features (last column) are MS: multi-sensor, ME: multi-environment (e.g. indoor/outdoor,
    light variability, mobile environment, etc.), UPAI: unseen PAIs in the test set.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '表5. 表总结了表[3](#S4.T3 "Table 3 ‣ 4.5\. Open Research Questions in Iris PAD ‣
    4\. Deep Learning-Based Iris Presentation Attack Detection ‣ Deep Learning for
    Iris Recognition: A Survey")中所使用的数据集（NIR: *近红外*；VW: *可见光波长*）。PAIs的类型（第二列）包括PP:
    纸质打印件，PPD: 带塑料圆顶的纸质打印件，CLL: 纹理隐形眼镜（实时），CLP: 纹理隐形眼镜（打印件），RA: 重放攻击（显示），AE: 人造眼球，AEC:
    带纹理隐形眼镜的人造眼球，SY: 合成虹膜，PM: 死后虹膜。TTP（倒数第二列）表示是否存在训练/测试分割。特征（最后一列）包括MS: 多传感器，ME:
    多环境（如室内/室外、光照变化、移动环境等），UPAI: 测试集中的未见PAIs。'
- en: '| Name | PAIs | Data | Size | # IDs | # Samples | TTP | Features |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | PAIs | 数据 | 大小 | # IDs | # 样本 | TTP | 特征 |'
- en: '| live | fake | live | fake | total |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| live | fake | live | fake | total |'
- en: '| CASIA-Iris-Fake (Sun et al., [2014](#bib.bib156)) | PP, CLL, AE, SY | NIR
    | 640$\times$480 | 1000 | 815 | 6000 | 4120 | 10240 |  |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| CASIA-Iris-Fake (Sun et al., [2014](#bib.bib156)) | PP, CLL, AE, SY | NIR
    | 640$\times$480 | 1000 | 815 | 6000 | 4120 | 10240 |  |  |'
- en: '| IF-VE (Zhang et al., [2021](#bib.bib200)) | CLL | NIR | n/a | 200 | 200 |
    25000 | 25000 | 50000 | ✓ | MS, ME |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| IF-VE (Zhang et al., [2021](#bib.bib200)) | CLL | NIR | n/a | 200 | 200 |
    25000 | 25000 | 50000 | ✓ | MS, ME |'
- en: '| IPITRT (Peng et al., [2020](#bib.bib127)) | PP | NIR | var. | 58 | n/a |
    1800 | 551 | 2351 |  | ME |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| IPITRT (Peng et al., [2020](#bib.bib127)) | PP | NIR | var. | 58 | n/a |
    1800 | 551 | 2351 |  | ME |'
- en: '| IIITD-CLI (Kohli et al., [2013](#bib.bib90)) | CLL | NIR | 640$\times$480
    | 202 | n/a | n/a | n/a | 6570 | ✓ | MS |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| IIITD-CLI (Kohli et al., [2013](#bib.bib90)) | CLL | NIR | 640$\times$480
    | 202 | n/a | n/a | n/a | 6570 | ✓ | MS |'
- en: '| IIITD-IS³  (Gupta et al., [2014](#bib.bib65)) | PP, CLP | NIR | 640$\times$480
    | 202 | n/a | 0 | 4848 | 4848 |  | MS |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| IIITD-IS³ (Gupta et al., [2014](#bib.bib65)) | PP, CLP | NIR | 640$\times$480
    | 202 | n/a | 0 | 4848 | 4848 |  | MS |'
- en: '| LivDet-2017 (Yambay et al., [2017](#bib.bib195)) |  |  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| LivDet-2017 (Yambay et al., [2017](#bib.bib195)) |  |  |'
- en: '| -Clarkson | PP, CLL | NIR | 640$\times$480 | 50 | n/a | 3954 | 4141 | 8095
    | ✓ | UPAI (additional patterned lenses) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| -Clarkson | PP, CLL | NIR | 640$\times$480 | 50 | n/a | 3954 | 4141 | 8095
    | ✓ | UPAI（附加图案镜片） |'
- en: '| -IIITD-WVU¹ | PP, CLL, CLP | NIR | 640$\times$480 | n/a | n/a | 2952 | 4507
    | 7459 | ✓ | MS, ME, UPAI (additional patterned lenses) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| -IIITD-WVU¹ | PP, CLL, CLP | NIR | 640$\times$480 | n/a | n/a | 2952 | 4507
    | 7459 | ✓ | MS, ME, UPAI (附加图案镜头) |'
- en: '| -ND-CLD² | CLL | NIR | 640$\times$480 | n/a | n/a | 2400 | 2400 | 4800 |
    ✓ | UPAI (additional patterned lenses) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| -ND-CLD² | CLL | NIR | 640$\times$480 | n/a | n/a | 2400 | 2400 | 4800 |
    ✓ | UPAI (附加图案镜头) |'
- en: '| -Warsaw | PP | NIR | 640$\times$480 | 457 | 446 | 5168 | 6845 | 12013 | ✓
    | MS |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| -Warsaw | PP | NIR | 640$\times$480 | 457 | 446 | 5168 | 6845 | 12013 | ✓
    | MS |'
- en: '| LivDet-2020 (Das et al., [2020](#bib.bib34)) | PP, PPD, CLL, CLP, RA, AE,
    AEC, PM | NIR | 640$\times$480 | n/a | n/a | 5331 | 7101 | 12432 |  | MS |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| LivDet-2020 (Das 等, [2020](#bib.bib34)) | PP, PPD, CLL, CLP, RA, AE, AEC,
    PM | NIR | 640$\times$480 | n/a | n/a | 5331 | 7101 | 12432 |  | MS |'
- en: '| Iris-CL1 (Tapia et al., [2022](#bib.bib163)) | PP | NIR | var. | n/a | n/a
    | n/a | 1800 | n/a |  | MS |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| Iris-CL1 (Tapia 等, [2022](#bib.bib163)) | PP | NIR | var. | n/a | n/a | n/a
    | 1800 | n/a |  | MS |'
- en: '| JHU-APL (Chen and Ross, [2021](#bib.bib20)) | CLL, AE | NIR | n/a | n/a |
    n/a | 7191 | 7214 | 14405 |  | ME |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| JHU-APL (Chen 和 Ross, [2021](#bib.bib20)) | CLL, AE | NIR | n/a | n/a | n/a
    | 7191 | 7214 | 14405 |  | ME |'
- en: '| MSU-IrisPA-01 (Yadav et al., [2019a](#bib.bib192)) | PP, CLL, RA, AE | NIR
    | 640$\times$480 | n/a | n/a | 1343 | 2523 |  |  |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| MSU-IrisPA-01 (Yadav 等, [2019a](#bib.bib192)) | PP, CLL, RA, AE | NIR | 640$\times$480
    | n/a | n/a | 1343 | 2523 |  |  |  |'
- en: '| MUIPA (Yadav et al., [2018](#bib.bib191)) | PP, CLL | NIR | 640$\times$480
    | 70 | 70 | n/a | n/a | 10296 |  | ME |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| MUIPA (Yadav 等, [2018](#bib.bib191)) | PP, CLL | NIR | 640$\times$480 | 70
    | 70 | n/a | n/a | 10296 |  | ME |'
- en: '| ND-CLD-13 (Doyle et al., [2013](#bib.bib43)) | CLL | NIR | 640$\times$480
    | 330 | n/a | 3400 | 1700 | 5100 | ✓ | MS |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| ND-CLD-13 (Doyle 等, [2013](#bib.bib43)) | CLL | NIR | 640$\times$480 | 330
    | n/a | 3400 | 1700 | 5100 | ✓ | MS |'
- en: '| ND-CLD-15²  (Doyle and Bowyer, [2015](#bib.bib42)) | CLL | NIR | 640$\times$480
    | n/a | n/a | 4800 | 2500 | 7300 | ✓ | MS |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| ND-CLD-15² (Doyle 和 Bowyer, [2015](#bib.bib42)) | CLL | NIR | 640$\times$480
    | n/a | n/a | 4800 | 2500 | 7300 | ✓ | MS |'
- en: '| NDIris3D (Fang et al., [2021a](#bib.bib52)) | CLL | NIR | 640$\times$480
    | 176 | 176 | 3458 | 3392 | 6850 |  | MS |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| NDIris3D (Fang 等, [2021a](#bib.bib52)) | CLL | NIR | 640$\times$480 | 176
    | 176 | 3458 | 3392 | 6850 |  | MS |'
- en: '| ND-PSID⁴  (Czajka et al., [2019](#bib.bib32)) | CLL | NIR | 640$\times$480
    | 238 | 238 | 3132 | 2664 | 5796 |  |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| ND-PSID⁴ (Czajka 等, [2019](#bib.bib32)) | CLL | NIR | 640$\times$480 | 238
    | 238 | 3132 | 2664 | 5796 |  |  |'
- en: '| UnMIPA (Yadav et al., [2019b](#bib.bib190)) | CLL | NIR | 640$\times$480
    | 162 | 162 | 9319 | 9387 | 18706 |  | MS, ME |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| UnMIPA (Yadav 等, [2019b](#bib.bib190)) | CLL | NIR | 640$\times$480 | 162
    | 162 | 9319 | 9387 | 18706 |  | MS, ME |'
- en: '| Warsaw-Post-Mortem v3.0 (Trokielewicz et al., [2020](#bib.bib173)) | PM |
    NIR, VW | var. | 0 | 79 | 0 | 1879 | 1879 |  | MS |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Warsaw-Post-Mortem v3.0 (Trokielewicz 等, [2020](#bib.bib173)) | PM | NIR,
    VW | var. | 0 | 79 | 0 | 1879 | 1879 |  | MS |'
- en: '| ¹ Contains IIITD-CLI and IIITD-IS |  |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| ¹ 包含 IIITD-CLI 和 IIITD-IS |  |  |'
- en: '| ² Iris-LivDet-2017-ND-CLD is a subset of ND-CLD-15 |  |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| ² Iris-LivDet-2017-ND-CLD 是 ND-CLD-15 的一个子集 |  |  |'
- en: '| ³ IIITD-IS images are printouts of IIITD-CLI captured with a iris scanner
    and a flatbed scanner |  |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| ³ IIITD-IS 图像是使用虹膜扫描仪和扫描仪扫描的 IIITD-CLI 打印件 |  |  |'
- en: '| ⁴ ND-PSID is a subset of ND-CLD-15 |  |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| ⁴ ND-PSID 是 ND-CLD-15 的一个子集 |  |  |'
- en: 8.2\. Software Tools
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 软件工具
- en: The availability of DL-based tools for iris biometrics has been scarce for years,
    specially for PAD (Fang et al., [2021a](#bib.bib52)). In the following, we provide
    a short description of peer-reviewed references with associated source code (link
    included in the paper, or easily found on the websites of the authors or dedicated
    sites such as www.paperswithcode.com). We describe (in this order) tools for segmentation,
    recognition and PAD. For each type, the references are then presented in cronological
    order.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，基于深度学习的虹膜生物识别工具一直稀缺，尤其是在 PAD 方面 (Fang 等, [2021a](#bib.bib52))。接下来，我们提供了同行评审的参考文献简要描述，附带源代码链接（文献中包含链接，或在作者网站或专门网站如
    www.paperswithcode.com 上可以轻松找到）。我们按顺序描述了分割、识别和 PAD 工具。每种类型的参考文献按照时间顺序呈现。
- en: 8.2.1\. Segmentation
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1\. 分割
- en: Lozej et al. (Lozej et al., [2018](#bib.bib102)) released their end-to-end DL
    model based on the U-Net architecture (Ronneberger et al., [2015](#bib.bib140)).
    The model was trained and evaluated with a small set of 200 annotated iris images
    from CASIA database. The authors also explored the impact of the model depth and
    the use of batch normalization layers.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Lozej 等 (Lozej 等, [2018](#bib.bib102)) 发布了基于 U-Net 架构 (Ronneberger 等, [2015](#bib.bib140))
    的端到端深度学习模型。该模型使用来自 CASIA 数据库的 200 张标注虹膜图像进行训练和评估。作者还探讨了模型深度和批量归一化层的影响。
- en: Kerrigan et al. (Kerrigan et al., [2019](#bib.bib86)) released the code and
    models of Iris-recognition-OTS-DNN, a set of four architectures based on off-the-shelf
    CNNs trained for iris segmentation (two VGG-16 with dilated convolutions, one
    ResNet with dilated kernels, and one SegNet encoder/decoder). Training databases
    included CASIA-Irisv4-Interval, ND-Iris-0405, Warsaw-Post-Mortem v2.0 and ND-TWINS-2009-2010,
    whereas testing data came from ND-Iris-0405 (disjoint subject), BioSec and UBIRIS.v2.
    Results showed that the DL solutions evaluated outperform traditional segmentation
    techniques, e.g. Hough transform or integro-differential operators. It was also
    seen that each test dataset had a method that performs best, with UBIRIS obtaining
    the worst performance. This should not come as a surprise, since it contains VW
    images with high variability taking distantly with a digital camera, whereas the
    other two are from close-up NIR iris sensors in controlled environments.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Kerrigan等人（Kerrigan et al., [2019](#bib.bib86)）发布了Iris-recognition-OTS-DNN的代码和模型，这是一组基于现成CNN的四种架构，用于虹膜分割（两个带膨胀卷积的VGG-16，一个带膨胀核的ResNet，以及一个SegNet编码器/解码器）。训练数据库包括CASIA-Irisv4-Interval、ND-Iris-0405、Warsaw-Post-Mortem
    v2.0和ND-TWINS-2009-2010，而测试数据来自ND-Iris-0405（不同的受试者）、BioSec和UBIRIS.v2。结果显示，评估的深度学习解决方案优于传统的分割技术，例如霍夫变换或积分-微分算子。还发现每个测试数据集都有表现最佳的方法，UBIRIS的表现最差。这不足为奇，因为它包含了用数码相机远距离拍摄的高变异性VW图像，而其他两个数据集则来自受控环境下的近距离NIR虹膜传感器。
- en: Wang et al. (Wang et al., [2020a](#bib.bib177)) released the code and models
    of their high-efficiency segmentation approach, IrisParseNet. A multi-task attention
    network was first applied to simultaneously predict the iris mask, pupil mask
    and iris outer boundary. Then, from the predicted masks and outer boundary, a
    parameterization of the iris boundaries was calculated. The solution is complete,
    in the sense that the mask (including light reflections and occlusions) and the
    parameterized inner and outer iris boundaries are jointly achieved.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Wang等人（Wang et al., [2020a](#bib.bib177)）发布了他们高效分割方法IrisParseNet的代码和模型。首先应用了多任务注意力网络，同时预测虹膜掩码、瞳孔掩码和虹膜外边界。然后，根据预测的掩码和外边界计算虹膜边界的参数化。这一解决方案是完整的，即掩码（包括光反射和遮挡）以及参数化的内外虹膜边界都是共同实现的。
- en: 'More recently, authors from the same group presented IrisSegBenchmark (Wang
    and Sun, [2020](#bib.bib178)), an open iris segmentation evaluation benchmark
    where they implemented six different CNN architectures, including Fully Convolutional
    Networks (FCN) (Long et al., [2015](#bib.bib101)), Deeplab V1,V2,V3 (Chen et al.,
    [2017](#bib.bib22)), ParseNet (Liu et al., [2016a](#bib.bib99)), PSPNet (Zhao
    et al., [2017](#bib.bib207)), SegNet (Badrinarayanan et al., [2017](#bib.bib10)),
    and U-Net (Ronneberger et al., [2015](#bib.bib140)). The methods were evaluated
    on CASIA-Irisv4-Distance, MICHE-I and UBIRIS.v2. As in (Kerrigan et al., [2019](#bib.bib86)),
    results showed that the best method depends on the database, being: ParseNet for
    CASIA (NIR data), DeeplabV3 for MICHE (VW images from mobile devices), and U-Net
    for UBIRIS (VW images from a digital camera). In this case, however, the three
    tests databases behaved approximately equal, since they all contain difficult
    distant data. CASIA showed a slightly better accuracy, suggesting that NIR data
    may be easier to segment. Traditional, non-DL methods were also evaluated, concluding
    that DL-based segmentation achieves superior accuracy.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，同一组的作者提出了IrisSegBenchmark（Wang and Sun, [2020](#bib.bib178)），这是一个开放的虹膜分割评估基准，其中他们实现了六种不同的CNN架构，包括全卷积网络（FCN）（Long
    et al., [2015](#bib.bib101)）、Deeplab V1、V2、V3（Chen et al., [2017](#bib.bib22)）、ParseNet（Liu
    et al., [2016a](#bib.bib99)）、PSPNet（Zhao et al., [2017](#bib.bib207)）、SegNet（Badrinarayanan
    et al., [2017](#bib.bib10)）和U-Net（Ronneberger et al., [2015](#bib.bib140)）。这些方法在CASIA-Irisv4-Distance、MICHE-I和UBIRIS.v2上进行了评估。如（Kerrigan
    et al., [2019](#bib.bib86)）所示，结果表明最佳方法取决于数据库，分别为：CASIA（NIR数据）的ParseNet、MICHE（来自移动设备的VW图像）的DeeplabV3，以及UBIRIS（来自数码相机的VW图像）的U-Net。然而，在这种情况下，这三个测试数据库的表现大致相同，因为它们都包含困难的远距离数据。CASIA表现出稍好的准确性，表明NIR数据可能更容易分割。传统的非深度学习方法也进行了评估，结论是基于深度学习的分割方法具有更高的准确性。
- en: Banerjee et al. (Banerjee et al., [2022](#bib.bib11)) released the code of their
    V-Net architecture, designed to overcome some drawbacks of U-Net, such as instability
    to tackle iris segmentation or tendency to overfit. A pre-processing stage on
    the YCrCb and HSV spaces was also added to detect salient regions and aid detection
    of iris boundaries. The method was evaluated on the difficult UBIRIS.v2 VW dataset.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Banerjee 等人（Banerjee et al., [2022](#bib.bib11)）发布了他们的 V-Net 架构代码，该架构旨在克服 U-Net
    的一些缺陷，例如在处理虹膜分割时的不稳定性或过拟合倾向。还在 YCrCb 和 HSV 空间中添加了一个预处理阶段，以检测显著区域并帮助识别虹膜边界。该方法在困难的
    UBIRIS.v2 VW 数据集上进行了评估。
- en: 8.2.2\. Recognition
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2. 识别
- en: The code of the DL method ThirdEye was released by Ahmad and Fuller (Ahmad and
    Fuller, [2019](#bib.bib4)), based on a ResNet-50 trained with triplet loss. Authors
    directly used segmented images without normalization to a rectangular 2D representation,
    arguing that such step may be counterproductive in unconstrained images. The model
    was evaluated on the ND-0405, IITD and UBIRIS.v2 datasets.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Ahmad 和 Fuller（Ahmad and Fuller, [2019](#bib.bib4)）发布了 DL 方法 ThirdEye 的代码，该方法基于经过三元组损失训练的
    ResNet-50。作者直接使用了未归一化为矩形 2D 表示的分割图像，认为在无约束图像中这样的步骤可能适得其反。该模型在 ND-0405、IITD 和 UBIRIS.v2
    数据集上进行了评估。
- en: 'The models of Boyd et al. (Boyd et al., [2019](#bib.bib16)) for recognition
    have been also released, based on a ResNet-50 with different weight initialization
    techniques, comprising: from scratch (random), off-the-shelf ImageNet (general-purpose
    vision weights), off-the shelf VGGFace2 (face recognition weights), fine-tuned
    ImageNet weights, and fine-tuned VGGFace2 weights. Both ImageNet and VGGFace2
    are very large datasets with millions of images, and face images contain the iris
    region. Thus, using these datasets as initialization may be beneficial for iris
    recognition, where available training data is in the order of hundreds of thousand
    images only. This strategy has been followed e.g. in ocular soft-biometrics as
    well (Alonso-Fernandez et al., [2021](#bib.bib7)). The observed optimal strategy
    is indeed to fine-tune an off-the-shelf set of weights to the iris recognition
    domain, be general-purpose or face recognition weights.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Boyd 等人（Boyd et al., [2019](#bib.bib16)）的识别模型也已发布，基于具有不同权重初始化技术的 ResNet-50，包括：从头开始（随机），现成的
    ImageNet（通用视觉权重），现成的 VGGFace2（人脸识别权重），微调的 ImageNet 权重和微调的 VGGFace2 权重。ImageNet
    和 VGGFace2 都是包含数百万张图像的大型数据集，人脸图像包含虹膜区域。因此，使用这些数据集作为初始化可能对虹膜识别有益，而可用的训练数据只有几十万张图像。这种策略也在眼部软生物识别中得到了应用（Alonso-Fernandez
    et al., [2021](#bib.bib7)）。观察到的最佳策略确实是对现成的权重集进行微调以适应虹膜识别领域，无论是通用还是人脸识别权重。
- en: 8.2.3\. Segmentation and Recognition Packages
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.3. 分割和识别包
- en: A complete package comprising segmentation and feature encoding was provided
    by Tann et al.(Tann et al., [2019](#bib.bib162)). The segmentator is based on
    a Fully Convolutional Network (FCN), but encoding is based on hand-crafted Gabor
    filters (Daugman, [2007](#bib.bib36)). Evaluation was done on CASIA-Irisv4-Interval
    and IITD.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: Tann 等人（Tann et al., [2019](#bib.bib162)）提供了一个完整的包含分割和特征编码的包。分割器基于全卷积网络（FCN），但编码基于手工制作的
    Gabor 滤波器（Daugman, [2007](#bib.bib36)）。在 CASIA-Irisv4-Interval 和 IITD 上进行了评估。
- en: In forensic investigation for diseased eyes and post-mortem samples, Czajka
    (Czajka, [2021](#bib.bib30)) also released a complete package combining segmentation
    and feature encoding. The models are based on previous efforts of the author and
    co-workers, comprising a SegNet (Trokielewicz et al., [2020](#bib.bib173)) and
    a CCNet (Mishra et al., [2019](#bib.bib107)) DL segmentators, but the feature
    encoder is based on hand-crafted BSIF filters.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在尸检眼睛和死后样本的法医调查中，Czajka（Czajka, [2021](#bib.bib30)）还发布了一个结合分割和特征编码的完整包。模型基于作者及其合作者的前期工作，包括
    SegNet（Trokielewicz et al., [2020](#bib.bib173)）和 CCNet（Mishra et al., [2019](#bib.bib107)）DL
    分割器，但特征编码器基于手工制作的 BSIF 滤波器。
- en: Another complete segmentation and recognition package was released by Kuehlkamp
    et al. (Kuehlkamp et al., [2022](#bib.bib92)). The segmentator is based on a fine-tuned
    Mask-RCNN architecture, with the cropped iris region fed directly into a ResNet50
    pre-trained for face recognition on the very large VGGFace2 dataset, and fine-tuned
    for iris recognition using triplet loss. The paper is oriented towards postmortem
    iris analysis, so the methods use a mixture of live and postmortem images for
    training and evaluation.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Kuehlkamp 等人（Kuehlkamp 等人，[2022](#bib.bib92)）发布了另一个完整的分割和识别包。该分割器基于经过微调的 Mask-RCNN
    架构，裁剪后的虹膜区域直接输入到经过预训练的 ResNet50 中，该模型在非常大的 VGGFace2 数据集上进行了面部识别的预训练，并利用三元组损失进行了虹膜识别的微调。该论文针对尸检虹膜分析，因此方法使用了活体和尸体图像的混合进行训练和评估。
- en: Parzianello and Czajka (Parzianello and Czajka, [2022](#bib.bib126)) also released
    the models and annotated data for their textured contact lens aware iris recognition
    method. The foundation is that such lenses may be used normally for cosmetic purposes,
    without intention to fool the biometric system. Therefore, they proposed to detect
    and match portions of live iris tissue still visible in order to enable recognition
    even when a person wears textured contact lenses. To do so, they applied a Mask
    R-CNN as a segmentation backbone, trained to detect authentically-looking parts
    of the iris using manually segmented samples from NDIris3D dataset. Non-iris information
    is then removed from the training images by blurring it or replacing it with random
    noise to guide the subsequent recognition network (based on ResNet-18) to salient,
    non-occluded regions that should be used for matching.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: Parzianello 和 Czajka（Parzianello 和 Czajka，[2022](#bib.bib126)）还发布了他们的纹理隐形眼镜识别虹膜方法的模型和标注数据。基础是这些镜片可能正常用于美容目的，而无意欺骗生物识别系统。因此，他们提出检测和匹配仍然可见的活体虹膜组织部分，以便即使在佩戴纹理隐形眼镜时也能进行识别。为此，他们应用了
    Mask R-CNN 作为分割骨干网络，训练以检测虹膜中看起来真实的部分，使用来自 NDIris3D 数据集的手动分割样本。然后，通过模糊或用随机噪声替换训练图像中的非虹膜信息，以引导随后的识别网络（基于
    ResNet-18）关注应用于匹配的显著、未被遮挡的区域。
- en: 8.2.4\. Iris PAD
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.4\. 虹膜 PAD
- en: In the iris PAD arena, Gragnaniello et al. (Gragnaniello et al., [2016](#bib.bib62))
    proposed a CNN that incorporates domain-specific knowledge. Based on the assumption
    that PAD relies on residual artifacts left mostly in high-frequencies, a regularization
    term was added to the loss function which forces the first layer to behave as
    a high-pass filter. The method, which is available in the website of the first
    author, could be applied to PAD in multiple modalities, including iris and face.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在虹膜 PAD 领域，Gragnaniello 等人（Gragnaniello 等人，[2016](#bib.bib62)）提出了一种结合领域特定知识的
    CNN。基于 PAD 主要依赖于残余伪影的高频假设，向损失函数中添加了正则化项，这使得第一层表现得像一个高通滤波器。该方法可以应用于多种模式下的 PAD，包括虹膜和面部，方法可在第一作者的网站上获取。
- en: The code and model of the method of Sharma and Ross (Sharma and Ross, [2020](#bib.bib151))
    (D-NetPAD) is also available. It is based on DenseNet121 and trained for a variety
    of PAIs (printouts, artificial eye, cosmetic contacts, kindle replay, and transparent
    dome on print), with an script to retrain the method also available.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: Sharma 和 Ross（Sharma 和 Ross，[2020](#bib.bib151)）的方法（D-NetPAD）的代码和模型也可用。该方法基于
    DenseNet121，并针对各种 PAIs（打印件、人工眼、化妆用隐形眼镜、Kindle 回放和透明圆顶打印物）进行了训练，还提供了重新训练该方法的脚本。
- en: '8.3\. Other Tools: Iris Image Quality Assessment'
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 其他工具：虹膜图像质量评估
- en: 'Several image properties considered to potentially influence the accuracy of
    iris biometrics have been defined in support of the standard ISO/IEC 29794-6 (technology
    — Biometric sample quality — Part 6: Iris image data, [2015](#bib.bib165)). They
    include: grayscale spread (dynamic range), iris size (pixels across the iris radius
    when the boundaries are modeled by a circle), dilation (ratio of the pupil to
    iris radius), usable iris area (percentage of non-occluded iris, either by eyelashes,
    eyelids or reflections), contrast of pupil and sclera boundaries, shape (irregularity)
    of pupil and sclera boundaries, margin (distance between the iris boundary and
    the closest image edge), sharpness (absence of defocus blur), motion blur, signal
    to noise ratio, gaze (deviation of the optical axis of the eye from the optical
    axis of the camera), and interlace of the acquisition device.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在支持标准ISO/IEC 29794-6（技术 — 生物特征样本质量 — 第6部分：虹膜图像数据，[2015](#bib.bib165)）的过程中，已经定义了一些可能影响虹膜生物特征准确性的图像属性。它们包括：灰度分布（动态范围）、虹膜大小（边界由圆形建模时虹膜半径上的像素数）、瞳孔扩张（瞳孔与虹膜半径的比率）、可用虹膜区域（未被睫毛、眼睑或反射遮挡的虹膜百分比）、瞳孔和巩膜边界的对比度、瞳孔和巩膜边界的形状（不规则性）、边缘（虹膜边界与最接近图像边缘的距离）、清晰度（无失焦模糊）、运动模糊、信噪比、注视（眼睛光轴与相机光轴的偏离）以及采集设备的交错。
- en: Low quality iris images, which can potentially appear in uncontrolled or non-cooperative
    environments, are known to reduce the performance of iris location, segmentation
    and recognition. Thus, an accurate quality assessment can be a valuable tool in
    support of the overall pipeline, either by dropping low quality images, or invoking
    specialized processing (Alonso-Fernandez et al., [2012](#bib.bib6)). One possibility
    might be to quantify the properties mentioned above, and placing thresholds on
    each. A more elaborated alternative is to combine them according to some rule
    and produce an overall quality score. However, it is difficult to provide metrics
    that cover all types of quality distortions (Tabassi et al., [2011](#bib.bib158))
    and doing so for some indeed entails to segment the iris.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 低质量的虹膜图像，可能出现在非受控或不合作的环境中，已知会降低虹膜定位、分割和识别的性能。因此，准确的质量评估可以成为支持整体流程的宝贵工具，无论是通过丢弃低质量图像，还是调用专门的处理（Alonso-Fernandez等人，[2012](#bib.bib6)）。一种可能性是量化上述属性，并对每个属性设定阈值。更复杂的替代方案是根据某些规则将它们结合起来，产生一个总体质量评分。然而，很难提供涵盖所有类型质量失真的指标（Tabassi等人，[2011](#bib.bib158)），而且确实需要对虹膜进行分割。
- en: Broadly, a biometric sample is of good quality if it is suitable for recognition,
    so quality should correlate with recognition performance (Grother and Tabassi,
    [2007](#bib.bib63)). As such, quality assessment can be viewed as a regression
    problem. Wang et al. (Wang et al., [2020c](#bib.bib183)) considered that a non-ideal
    eye image will pivot in the feature space around the embedding of an ideal image.
    They defined quality as the distance to the embedding of such “ideal” image which,
    is regarded as a registration sample collected under a highly controlled environment.
    They used a model to learn the mapping between images and Distance in Feature
    Space (DFS) directly from a given dataset. Quality is computed via attention-based
    pooling that combines a heatmap that comes from a coarse segmentation based on
    U-Net and the feature map of an extraction network based on MobileNetv2 pre-trained
    on CASIA-Iris-V4 and NDIRIS-0405.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上，生物特征样本的质量良好意味着其适合于识别，因此质量应与识别性能相关（Grother和Tabassi，[2007](#bib.bib63)）。因此，质量评估可以被视为一个回归问题。Wang等人（Wang等人，[2020c](#bib.bib183)）认为非理想的眼睛图像将在特征空间中围绕理想图像的嵌入进行旋转。他们将质量定义为与这种“理想”图像的嵌入的距离，该理想图像被视为在高度控制的环境下收集的登记样本。他们使用模型直接从给定数据集中学习图像与特征空间距离（DFS）之间的映射。质量通过基于注意力的池化计算，该池化结合了基于U-Net的粗略分割生成的热图和基于MobileNetv2在CASIA-Iris-V4和NDIRIS-0405上预训练的提取网络的特征图。
- en: 9\. Emerging Research Directions
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9\. 新兴研究方向
- en: In this section, we discuss the most relevant open challenges and hypothesize
    about emerging research directions that could become *hot-topics* in biometrics
    literature in a close future.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了最相关的开放挑战，并对可能成为未来生物特征学文献中*热门话题*的新兴研究方向提出假设。
- en: 9.1\. Resource-aware designs of iris recognition networks
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1\. 面向资源的虹膜识别网络设计
- en: Application-wise, iris recognition can be performed on a wide range of hardware,
    ranging from high-end computers to low-end embedded devices, or from large computer
    clusters to personal devices such as mobile phones. Performing recognition on
    resource-limited hardware could pose new challenges for deep learning based iris
    networks, which usually contain hundreds of layers and millions of parameters.
    Therefore designing these deep learning networks necessarily need to be aware
    of the hardware platforms on which they will be run.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用方面，虹膜识别可以在各种硬件上进行，从高端计算机到低端嵌入式设备，或从大型计算机集群到个人设备如手机。在资源有限的硬件上执行识别可能对基于深度学习的虹膜网络提出新的挑战，这些网络通常包含数百层和数百万个参数。因此，设计这些深度学习网络时必须考虑它们将运行的硬件平台。
- en: 'Lightweight models: Lightweight CNNs employ advanced techniques to efficiently
    trade-off between resource and accuracy, minimising their model size and computations
    in term of the number of floating point operations (FLOPs), while retaining high
    accuracies. Specialized lightweight CNN architectures include MobileNets (Howard
    et al., [2019](#bib.bib74)) and U-Net (Ronneberger et al., [2015](#bib.bib140)).
    There are a few lightweight deep learning based models for both segmentation and
    feature extraction. Fang *et al.* (Fang and Czajka, [2020](#bib.bib51)) adapted
    the lightweight CC-Net (Mishra et al., [2019](#bib.bib107)) for iris segmentation.
    CC-Net has a U-Net structure (Ronneberger et al., [2015](#bib.bib140)), able to
    retain up to 95% accuracy using only 0.1% of the trainable parameters. Boutros
    *et al.* (Boutros et al., [2020](#bib.bib14)) benchmarked MobileNet-V3 against
    deeper networks for iris recognition and showed that the MobileNet based model
    can achieve similar EER with 85% less number of parameters and 80% less inference
    time.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 轻量级模型：轻量级CNN采用先进技术，在资源和准确性之间高效权衡，最小化其模型大小和计算量，即浮点运算数（FLOPs），同时保持高准确性。专门的轻量级CNN架构包括MobileNets（Howard
    et al., [2019](#bib.bib74)）和U-Net（Ronneberger et al., [2015](#bib.bib140)）。有一些轻量级的深度学习模型用于分割和特征提取。Fang
    *et al.* (Fang and Czajka, [2020](#bib.bib51)) 将轻量级的CC-Net（Mishra et al., [2019](#bib.bib107)）应用于虹膜分割。CC-Net具有U-Net结构（Ronneberger
    et al., [2015](#bib.bib140)），能够使用仅0.1%的可训练参数保持高达95%的准确率。Boutros *et al.* (Boutros
    et al., [2020](#bib.bib14)) 对MobileNet-V3进行了基准测试，与更深的网络进行对比，显示MobileNet基础模型能够在减少85%的参数和80%的推理时间的情况下实现类似的EER。
- en: 'Model compression: Studies have found that most of the large deep learning
    models tend to be overparameterized, leading to lots of redundant parameters and
    operations in the network. This becomes more severe considering iris texture images
    are different from generic object-based images. This has motivated a hot trend
    looking to remove these redundancies from the models, including pruning, quantization
    and low-rank factorization (Liang et al., [2021](#bib.bib96)). In our iris recognition
    literature, there a few lightweight deep learning based models for both segmentation
    and feature extraction. Tann *et al.* (Tann et al., [2019](#bib.bib162)) quantized
    64-bit floating points numbers of weights and activations of the full FCN-based
    iris segmentation model using an 8-bit dynamic fixed-point (DFP) format, which
    provide a 8$\times$ memory saving as well as speed enhancement due to reduced
    complexity of lower precision operations.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩：研究发现，大多数大型深度学习模型往往过度参数化，导致网络中存在大量冗余参数和操作。考虑到虹膜纹理图像与通用对象图像不同，这一问题变得更加严重。这促使了一个热门趋势，旨在从模型中去除这些冗余，包括剪枝、量化和低秩分解（Liang
    et al., [2021](#bib.bib96)）。在我们的虹膜识别文献中，有一些轻量级的深度学习模型用于分割和特征提取。Tann *et al.* (Tann
    et al., [2019](#bib.bib162)) 将全FCN基础虹膜分割模型的权重和激活的64位浮点数量化为8位动态定点（DFP）格式，这提供了8$\times$的内存节省，并由于降低了精度操作的复杂性而加快了速度。
- en: 'Neural Architecture Search: Neural Architecture Search (NAS) automates the
    process of architecture design of neural networks by iteratively sampling a population
    of child networks, evaluating the child models’ performance metrics as rewards
    and learning to generate high-performance architecture candidates (Elsken et al.,
    [2019](#bib.bib44)). In our iris recognition literature, Nguyen *et al.* (Nguyen
    et al., [2020](#bib.bib114)) showed that computation and memory can be incorporated
    into the NAS formulation to enable resource-constrained design of deep iris networks.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构搜索：神经架构搜索（NAS）通过迭代采样一组子网络，评估子模型的性能指标作为奖励，并学习生成高性能的架构候选，从而自动化神经网络的架构设计过程（Elsken
    et al., [2019](#bib.bib44)）。在我们的虹膜识别文献中，Nguyen *et al.*（Nguyen et al., [2020](#bib.bib114)）展示了可以将计算和内存整合到NAS公式中，以实现对资源受限的深度虹膜网络的设计。
- en: 9.2\. Human-interpretable methods and XAI
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2\. 人类可解释的方法与XAI
- en: With hundreds of layers and millions of parameters, deep learning networks are
    usually opaque or “blackbox” where humans struggle to understand why a deep network
    predict what it predicts. This necessitates approaches to make deep learning methods
    more interpretable and understandable to humans. Interestingly, the need for human-interpretable
    methods has been raised even from the handcrafted era. For example, Shen *et al.*
    published a series of work (Chen et al., [2016](#bib.bib21); Shen and Flynn, [2013](#bib.bib153))
    on using iris crypts for iris matching. Iris crypts are clearly visible to humans
    in a similar way as finger minutiae. Another example is the macro-features (Sunder
    and Ross, [2010](#bib.bib157)) which use SIFT to detect keypoints and perform
    iris matching based on these keypoints (Quinn, G. and Matey, J. and Grother, P.
    and Watters, E., [2022](#bib.bib137)). Another notable work is by Proença *et
    al.* (Proença and Neves, [2017](#bib.bib133)) where they proposed a deformation
    field to represent the correspondence between two iris images.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 由于拥有数百层和数百万个参数，深度学习网络通常是“黑箱”的，人们很难理解为什么深度网络会做出某种预测。这就需要采取方法使深度学习方法对人类更加可解释和易于理解。有趣的是，即使在手工设计时代，对人类可解释方法的需求也已经提出。例如，Shen
    *et al.* 发表了一系列工作（Chen et al., [2016](#bib.bib21); Shen and Flynn, [2013](#bib.bib153)）探讨了使用虹膜隐秘特征进行虹膜匹配。虹膜隐秘特征在人眼中与指纹细节类似，清晰可见。另一个例子是宏特征（Sunder
    and Ross, [2010](#bib.bib157)），它们使用SIFT检测关键点，并基于这些关键点进行虹膜匹配（Quinn, G. 和 Matey,
    J. 和 Grother, P. 和 Watters, E., [2022](#bib.bib137)）。另一个显著的工作是Proença *et al.*（Proença
    and Neves, [2017](#bib.bib133)）提出了一个变形场来表示两个虹膜图像之间的对应关系。
- en: From a deep learning perspective, researchers have also attempted to visualize
    the matching. Kuehlkamp *et al.* (Kuehlkamp et al., [2022](#bib.bib92)) argued
    that existing iris recognition methods offer limited and non-standard methods
    of visualization to let human examiners interpret the model output. They applied
    Class Activation Maps (CAM) (Zhou et al., [2016](#bib.bib213)) to visualize the
    level of contribution of each iris region to the overall matching score. Similarly,
    Nguyen *et al.* (Nguyen et al., [2022](#bib.bib116)) also decomposed the final
    matching score into pixel-level to visualize the level of contribution of each
    pixel to the overall matching score.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 从深度学习的角度来看，研究人员还尝试了匹配的可视化。Kuehlkamp *et al.*（Kuehlkamp et al., [2022](#bib.bib92)）认为现有的虹膜识别方法提供的可视化手段有限且非标准，难以让人工检查员解释模型输出。他们应用了类激活图（Class
    Activation Maps，CAM）（Zhou et al., [2016](#bib.bib213)）来可视化每个虹膜区域对整体匹配得分的贡献程度。类似地，Nguyen
    *et al.*（Nguyen et al., [2022](#bib.bib116)）也将最终匹配得分分解到像素级，以可视化每个像素对整体匹配得分的贡献程度。
- en: 9.3\. Deep learning-based synthetic iris generation
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3\. 基于深度学习的合成虹膜生成
- en: 'Data synthesis provides an alternative to time- and resource-consuming database
    collection. One could create as many images as desired, with new textures that
    even do not match any existing identity, which would avoid privacy problems too.
    On the other hand, fake irises that are indistinguishable from real ones can be
    used for identity concealment attacks (if the image does not match any identity)
    or impersonation attacks (if the image resembles an existing identity) (Czajka
    and Bowyer, [2018](#bib.bib31)). Indeed, synthetic irises are present in databases
    employed for iris PAD, such as CASIA-Iris-Fake (Table [5](#S8.T5 "Table 5 ‣ 8.1\.
    Data Sources ‣ 8\. Open-Source Deep Learning-Based Iris Recognition Tools ‣ Deep
    Learning for Iris Recognition: A Survey")).'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '数据合成提供了一种替代耗时且资源密集的数据库收集的方法。用户可以创建任意数量的图像，并使用新的纹理，这些纹理甚至不匹配任何现有的身份，这也有助于避免隐私问题。另一方面，伪造的虹膜如果与真实的难以区分，可以用于身份隐匿攻击（如果图像不匹配任何身份）或冒充攻击（如果图像与现有身份相似）（Czajka和Bowyer，[2018](#bib.bib31)）。实际上，合成虹膜出现在用于虹膜PAD的数据库中，如CASIA-Iris-Fake（表[5](#S8.T5
    "Table 5 ‣ 8.1\. Data Sources ‣ 8\. Open-Source Deep Learning-Based Iris Recognition
    Tools ‣ Deep Learning for Iris Recognition: A Survey")）。'
- en: Regardless of the purpose or ability to detect if an image is synthetic, Generative
    Adversarial Networks (GANs) (Goodfellow et al., [2014](#bib.bib61)) have shown
    impressive photo-realistic generating capabilities in many domains. GANs learn
    to model image distributions by an adversarial process, where a discriminator
    assesses the realism of images synthesized by a generator. At the end, the generator
    have learned the distribution of the training data, being able to synthesize new
    images with the same characteristics.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 无论目的是什么，或是否能够检测图像是否合成，生成对抗网络（GANs）（Goodfellow等人，[2014](#bib.bib61)）在许多领域展示了令人印象深刻的照片级真实生成能力。GANs通过对抗过程学习建模图像分布，其中一个鉴别器评估由生成器合成的图像的真实性。最终，生成器学习了训练数据的分布，能够合成具有相同特征的新图像。
- en: 'For iris generation, some methods by Yadav et al. (Yadav and Ross, [2021](#bib.bib194);
    Yadav et al., [2020](#bib.bib193), [2019a](#bib.bib192)) were mentioned in iris
    PAD contexts (Section [4.4](#S4.SS4 "4.4\. Adversarial Networks ‣ 4\. Deep Learning-Based
    Iris Presentation Attack Detection ‣ Deep Learning for Iris Recognition: A Survey")).
    RaSGAN (Yadav et al., [2020](#bib.bib193), [2019a](#bib.bib192)) followed the
    traditional approach of driving the generation/discrimination training by randomly
    sampling so-called latent vectors from a probabilistic distribution. As training
    progresses, the generator learns to associate features of the latent vectors with
    semantically meaningful attributes that naturally vary in the images. However,
    this does not impose any restriction in the relationship between features in latent
    space and factors of variation in the image domain, making difficult to decode
    what the latent vectors represent. As a result, the image characteristics (eye
    color, eyelids shape, eyelashes, gender, age…) are generated randomly. Kohli et
    al. (Kohli et al., [2017](#bib.bib91)) presented iDCGAN for iris PAD, which also
    followed the latent vector sampling concept. To counteract such issue, researchers
    have tried to incorporate constrains or mechanisms that guide the generation process
    to a desired characteristic. For example, CIT-GAN (Yadav and Ross, [2021](#bib.bib194))
    employed a Styling Network that learns style characteristics of each given domain,
    while taking as input a domain label that drives the network to embed a desired
    style into the generated data.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '对于虹膜生成，一些方法由Yadav等人（Yadav和Ross，[2021](#bib.bib194)；Yadav等人，[2020](#bib.bib193)，[2019a](#bib.bib192)）在虹膜PAD背景下被提及（第[4.4](#S4.SS4
    "4.4\. Adversarial Networks ‣ 4\. Deep Learning-Based Iris Presentation Attack
    Detection ‣ Deep Learning for Iris Recognition: A Survey")节）。RaSGAN（Yadav等人，[2020](#bib.bib193)，[2019a](#bib.bib192)）遵循传统的方法，通过从概率分布中随机抽取所谓的潜在向量来驱动生成/辨别训练。随着训练的进行，生成器学习将潜在向量的特征与图像中自然变化的语义属性相关联。然而，这并不限制潜在空间中的特征与图像域中的变化因素之间的关系，使得解码潜在向量所表示的内容变得困难。因此，图像特征（眼睛颜色、眼睑形状、睫毛、性别、年龄等）是随机生成的。Kohli等人（Kohli等人，[2017](#bib.bib91)）提出了用于虹膜PAD的iDCGAN，它也遵循潜在向量采样的概念。为了应对这一问题，研究人员尝试引入约束或机制，引导生成过程朝向所需的特征。例如，CIT-GAN（Yadav和Ross，[2021](#bib.bib194)）采用了一种样式网络，该网络学习每个给定领域的样式特征，同时以领域标签作为输入，推动网络将所需的样式嵌入生成的数据中。'
- en: In a similar direction, Kaur and Manduchi (Kaur and Manduchi, [2021](#bib.bib85),
    [2020](#bib.bib84)) proposed to synthesize eye images with a desired style (skin
    color, texture, iris color, identity) using an encoder-decoder ResNet. The method
    is aimed at manipulating gaze, so the generator receives a segmentation mask with
    the desired gaze, and an image with the style that will see its gaze modified.
    To achieve cross-spectral recognition, Hernandez-Diaz et al. (Hernandez-Diaz et al.,
    [2020](#bib.bib72)) used CGANs to convert ocular images between VW and NIR spectra
    while keeping identity, so comparisons are done in the same spectrum. This allows
    the use of existing feature methods, which are typically optimized to operate
    in a single spectrum.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似的方向上，Kaur和Manduchi（Kaur和Manduchi，[2021](#bib.bib85)，[2020](#bib.bib84)）提出了使用编码器-解码器ResNet合成具有所需风格（肤色、纹理、虹膜颜色、身份）的眼部图像。该方法旨在操控视线，因此生成器接收一个带有所需视线的分割掩码，并生成一个风格图像，该图像的视线会被修改。为了实现跨光谱识别，Hernandez-Diaz等（Hernandez-Diaz
    et al., [2020](#bib.bib72)）使用CGAN将眼部图像在VW和NIR光谱之间转换，同时保持身份，从而使比较在相同光谱中进行。这允许使用现有的特征方法，这些方法通常针对单一光谱进行优化。
- en: Despite great advances in DL-based synthetic image generation, one open problem
    is the possible identity leakage from the training set when creating data of non-existing
    identities, resulting in privacy issues. This has just been revealed recently
    in face generation (Tinsley et al., [2021](#bib.bib166)). Another issue in the
    opposite direction is the difficulty in preserving identity in the generation
    process when the target is precisely creating images of an existing identity with
    different properties. This is an issue being addressed in face generation methods
    [reference under review], but is lacking in iris synthesis research.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在基于深度学习的合成图像生成方面取得了巨大进展，但一个未解决的问题是，在创建不存在的身份数据时可能会泄露训练集中的身份信息，从而导致隐私问题。这一点在面部生成中刚刚被揭示（Tinsley
    et al., [2021](#bib.bib166)）。另一方面，当目标是精确生成具有不同属性的现有身份的图像时，保持身份的困难也是一个问题。这是面部生成方法正在解决的问题[参考文献待审]，但在虹膜合成研究中尚未得到解决。
- en: 9.4\. Deep learning-based iris super-resolution
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4\. 基于深度学习的虹膜超分辨率
- en: One of the main constraints for existing iris recognition systems is the short
    distance of image acquisition, which usually requires a subject to stay still
    less than 60 cm from iris cameras. This is due to the requirement of high-resolution
    iris region, *e.g.* 120 pixels across the iris diameter due to the European standard
    and NIST standard, despite the small physical size of an eye, *i.e.* $15\times
    15$ mm. The lack of resolution of imaging systems has critically adverse impacts
    on the recognition and performance of biometric systems, especially in less constrained
    conditions and long range surveillance applications (Nguyen et al., [2018](#bib.bib117)).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现有虹膜识别系统的主要限制之一是图像采集距离较短，这通常要求被摄者距离虹膜摄像头的距离少于60厘米。这是由于需要高分辨率的虹膜区域，例如，根据欧洲标准和NIST标准，虹膜直径需要120像素，尽管眼睛的物理尺寸很小，即$15\times
    15$毫米。成像系统分辨率的不足对生物识别系统的识别和性能产生了严重的不利影响，尤其是在较少限制的条件和远程监控应用中（Nguyen et al., [2018](#bib.bib117)）。
- en: 'Super-resolution, as one of the core innovations in computer vision, has been
    an attractive but challenging solution to address the low resolution problem in
    both general imaging systems and biometric systems. Deep learning based super-resolution
    approaches have been across multiple works in iris recognition. Ribeiro *et al.*
    (Ribeiro et al., [2017](#bib.bib139); Ribeiro et al., [2019](#bib.bib138)) experimented
    two deep learning single-image super-resolution approaches: Stacked Auto-Encoders
    (SAE) and Convolutional Neural Networks (CNN). Both approaches learn one encoder
    to map the high resolution iris images to the low resolution domain, and one decoder
    to learn to reconstruct the original high resolution images from the low resolution
    ones. Zhang *et al.* (Zhang et al., [2016a](#bib.bib202)) learned a single CNN
    to learn non-linear mapping function between LR images to HR images for mobile
    iris recognition. Wang *et al.* (Wang et al., [2019a](#bib.bib184)) extended the
    single CNN to two CNNs: one generator CNN and one discriminator CNN as in the
    GAN architecture. The generator functions similar to the single LR - HR mapping
    CNN. Adding the discriminator CNN allows them to control the generator to generate
    HR images not just visually higher resolution but also preserve the identity of
    the iris. Mostofa *et al.* (Mostofa et al., [2021](#bib.bib110)) incorporated
    a GAN-based photo-realistic super-resolution approach (Ledig et al., [2017](#bib.bib94))
    to improve the resolution of LR iris images from the NIR domain before cross-matching
    the HR outputs with the HR images from the RGB domain. While these approaches
    showed improved performance, dealing with noisy data in such cases as iris at
    a distance and on the move could require the quality of an input iris image to
    be included in the super-resolution process (Nguyen et al., [2011](#bib.bib115)).
    In addition, Nguyen *et al.* argued that a fundamental difference exists between
    conventional super-resolution motivations and those required for biometrics, hence
    proposing to perform super-resolution at the feature level targeting explicitly
    the representation used by recognition (Nguyen et al., [2012](#bib.bib118)).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 超分辨率作为计算机视觉领域的核心创新之一，一直是一个吸引人但充满挑战的解决方案，用于解决一般成像系统和生物特征识别系统中的低分辨率问题。基于深度学习的超分辨率方法已在虹膜识别中得到广泛应用。Ribeiro
    *et al.*（Ribeiro et al., [2017](#bib.bib139); Ribeiro et al., [2019](#bib.bib138)）实验了两种深度学习单图像超分辨率方法：堆叠自编码器（SAE）和卷积神经网络（CNN）。这两种方法都学习一个编码器将高分辨率虹膜图像映射到低分辨率领域，并且一个解码器学习从低分辨率图像中重建原始高分辨率图像。Zhang
    *et al.*（Zhang et al., [2016a](#bib.bib202)）学习了一个单一的CNN来学习低分辨率图像与高分辨率图像之间的非线性映射函数，用于移动虹膜识别。Wang
    *et al.*（Wang et al., [2019a](#bib.bib184)）将单一的CNN扩展为两个CNN：一个生成器CNN和一个判别器CNN，类似于GAN架构。生成器的功能类似于单一的LR
    - HR映射CNN。添加判别器CNN使他们能够控制生成器生成的高分辨率图像不仅在视觉上分辨率更高，还能保持虹膜的身份。Mostofa *et al.*（Mostofa
    et al., [2021](#bib.bib110)）结合了基于GAN的照片级真实超分辨率方法（Ledig et al., [2017](#bib.bib94)），以提高来自NIR领域的低分辨率虹膜图像的分辨率，然后将高分辨率输出与来自RGB领域的高分辨率图像进行交叉匹配。虽然这些方法展示了性能提升，但在处理如远距离和移动中的虹膜这类噪声数据时，可能需要在超分辨率过程中包含输入虹膜图像的质量（Nguyen
    et al., [2011](#bib.bib115)）。此外，Nguyen *et al.* 认为传统超分辨率的动机与生物特征识别所需的超分辨率存在根本性差异，因此建议在特征层面执行超分辨率，明确针对识别所用的表示（Nguyen
    et al., [2012](#bib.bib118)）。
- en: 9.5\. Privacy in deep learning-based iris recognition
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5\. 基于深度学习的虹膜识别中的隐私问题
- en: Privacy is becoming a key issue in computer vision and machine learning domains.
    In particular, it is accepted that the accuracy attained by deep learning models
    depends on the availability of large amounts of visual data, which stresses the
    need for privacy-preserving recognition solutions.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私问题正成为计算机视觉和机器学习领域的一个关键问题。特别是，深度学习模型所获得的准确性依赖于大量视觉数据的可用性，这强调了对隐私保护识别解决方案的需求。
- en: In short, the goal in privacy preserving deep-learning is to appropriately train
    models while preserving the privacy of the training datasets. While the utility
    of this kind of solutions is obvious, there are certain concerns about the training
    data that supported the model creation, as the collection of images from a large
    number of individuals comes with significant privacy risks. In particular, it
    should be considered that the subjects from whom the data were collected can neither
    delete nor control what actually will be learned from their data.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，隐私保护的深度学习目标是适当地训练模型，同时保护训练数据集的隐私。尽管这种解决方案的效用显而易见，但对于支持模型创建的训练数据存在一定的担忧，因为从大量个人那里收集图像带来了显著的隐私风险。特别是，应考虑到数据收集对象既不能删除也不能控制他们的数据实际将学到什么。
- en: As most of the existing biometric technologies, DL-based iris recognition pose
    challenges to privacy, which are even more concerning, considering the *data-driven*
    feature of such kind of systems. Particular attention should be paid to avoid
    function creep, guaranteeing that the system yielding from a set of data is not
    used for a different purpose than the originally communicated to the individual
    at the time of providing their information. Covert collection is another major
    concern, which is also particular important for the iris trait, according to the
    possibility of being imaged from large distances and in surreptitious way.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数现有的生物特征识别技术一样，基于深度学习的虹膜识别也面临隐私挑战，考虑到此类系统的*数据驱动*特性，这些挑战尤为令人担忧。应特别注意避免功能蔓延，确保系统在数据集的基础上生成后不会被用于与最初告知个人时的目的不同的用途。隐秘收集是另一个主要问题，这对于虹膜特征尤为重要，因为虹膜可以从远距离以及隐秘的方式进行成像。
- en: Particular attention has been paid to the development of fair recognition systems,
    in the sense that this kind of systems should attain similar effectiveness in
    different subgroups of the population, regarding different features such as *gender*,
    *age*, *race* or *ethnicity*. For data-driven systems, this might be a relevant
    challenge, considering that most of the existing datasets that support the learned
    systems have evident biases with regared tio the subjects’ characteristics above.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在公平识别系统的开发上已特别关注，这意味着这种系统应该在不同人群子组中达到类似的效果，涉及*性别*、*年龄*、*种族*或*民族*等不同特征。对于数据驱动的系统来说，这可能是一个相关的挑战，因为支持学习系统的大多数现有数据集在上述受试者特征方面具有明显的偏差。
- en: 'Lastly, in a more general machine learning perspective, potential attacks to
    the learned models have been concerning the research community and have been the
    scope of various recent works, attempting to provide defense mechanisms against:
    i) model inversion attacks, that aim to reconstruct the training data from the
    model parameters (e.g., (Khosravy et al., [2022](#bib.bib88)) and (He et al.,
    [2022](#bib.bib68))); ii) membership inference, that attempt to infer whether
    one individual was part of a training set (e.g., (Hu et al., [2022](#bib.bib76))
    and (Song et al., [2019](#bib.bib154))); and iii) training data extraction attacks,
    that aim to recover individual training samples by querying the models (e.g, (Khalid
    et al., [2019](#bib.bib87)) and (Ding et al., [2022](#bib.bib40))).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，从更广泛的机器学习角度来看，对学习模型的潜在攻击一直是研究社区关注的问题，也是近期各种工作的重点，旨在提供针对以下内容的防御机制：i) 模型反演攻击，旨在从模型参数重建训练数据（例如，（Khosravy
    et al., [2022](#bib.bib88)）和（He et al., [2022](#bib.bib68)））；ii) 成员推断，试图推断一个人是否曾是训练集的一部分（例如，（Hu
    et al., [2022](#bib.bib76)）和（Song et al., [2019](#bib.bib154)））；iii) 训练数据提取攻击，旨在通过查询模型恢复单个训练样本（例如，（Khalid
    et al., [2019](#bib.bib87)）和（Ding et al., [2022](#bib.bib40)））。
- en: 9.6\. Deep learning-based iris segmentation
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6\. 基于深度学习的虹膜分割
- en: Being one of the earliest phases of the recognition process, segmentation is
    known as one of the most challenging, as it is at the front line for facing the
    dynamics of the data acquisition environments. This is particularly true, in case
    of less constrained data acquisition protocols, where the resulting data have
    highly varying features and the particular conditions of each environment strongly
    determine the most likely data covariates.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 作为识别过程中的早期阶段之一，分割被认为是最具挑战性的任务之一，因为它处于应对数据采集环境动态变化的前线。这在数据采集协议不受约束的情况下尤为真实，此时结果数据具有高度变化的特征，每个环境的特定条件强烈决定了最可能的数据协变量。
- en: 'In the segmentation context, the main challenge remains as the development
    of methods robust to *cross-domain* settings, i.e., able to segment the iris region
    for a broad range of image features, e.g., in terms of: 1) illumination, 2) scale,
    3) gaze, 4) occlusions, 5) rotation and 6) pose, corresponding to the acquisition
    in very different environments. Over the past decades, many research groups have
    been devoting their attentions in improving the robustness of iris segmentation,
    which is known to be a primary factor for the final effectiveness of the recognition
    process. In this timeline, the proposed segmentation methods can be roughly grouped
    into three categories: 1) boundary-based methods (using the integro-differential
    operator or Hough transform); 2) based in handcrafted features (particularly suited
    for non-cooperative recognition, e.g., (Tan et al., [2010](#bib.bib161)) and (Tan
    and Kumar, [2012](#bib.bib160))) ; and 3) DL-based solutions.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在分割的背景下，主要挑战仍然是开发对*跨域*设置稳健的方法，即能够在广泛的图像特征范围内分割虹膜区域，例如：1）光照，2）尺度，3）视线，4）遮挡，5）旋转和6）姿势，这些对应于在非常不同的环境中进行采集。在过去几十年中，许多研究小组一直致力于提高虹膜分割的鲁棒性，这是识别过程最终效果的主要因素。在这段时间里，提出的分割方法大致可以分为三类：1）基于边界的方法（使用积分微分算子或霍夫变换）；2）基于手工特征的方法（特别适用于非合作性识别，例如（Tan
    et al., [2010](#bib.bib161)）和（Tan and Kumar, [2012](#bib.bib160)））；以及3）基于深度学习的解决方案。
- en: For the latter family of methods, the emerging trends are closely related to
    the *general* challenges of DL-based segmentation frameworks, namely to obtain
    interpretable models that allow us to perceive what exactly are these systems
    learning, or the minimal neural architecture that guarantees a predefined level
    of accuracy. Also, the development of weakly supervised or even unsupervised frameworks
    is another *grand-challenge*, as it is accepted that such systems will likely
    adapt better to previously unseen data acquisition conditions. Finally, the computational
    cost of segmentation (both in terms of space and time) is another concern, with
    special impact in the deployment of this kind of frameworks in mobile settings,
    and in the IoT setting (Saleh, [2018](#bib.bib141)).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 对于后者的这类方法，新兴趋势与基于深度学习的分割框架的*总体*挑战密切相关，即获得可解释的模型，使我们能够理解这些系统到底在学习什么，或者保证预定准确率的最小神经网络架构。此外，弱监督或甚至无监督框架的发展是另一个*重大挑战*，因为这些系统可能会更好地适应先前未见的数据采集条件。最后，分割的计算成本（无论是空间还是时间）也是一个关注点，特别是在将这类框架部署到移动环境和物联网环境（Saleh,
    [2018](#bib.bib141)）中的影响。
- en: 9.7\. Deep learning-based iris recognition in visible wavelengths
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.7. 基于深度学习的可见光波段虹膜识别
- en: Being a topic of study for over a decade (e.g. (Liu et al., [2019](#bib.bib100))
    and (Proença, [2013](#bib.bib130))), iris recognition in visible wavelengths remains
    essentially as an interesting possibility for delivering biometric recognition
    from large distances (in conditions that are typically associated to visual surveillance
    settings) and in handheld *commercial* devices, such as smartphones.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中（例如，（Liu et al., [2019](#bib.bib100)）和（Proença, [2013](#bib.bib130)）），可见光波段的虹膜识别仍然作为一种有趣的可能性，主要用于从远距离进行生物识别（在通常与视觉监控环境相关的条件下）以及在手持的*商业*设备中，如智能手机。
- en: The emerging trends in this scope regard the development of alternate ways to
    analyze the multi-spectral information available in visible light data (typically
    RGB), i.e., by developing deep learning architectures optimized for fusion, either
    at the data, feature, score or decision levels (Bigdeli et al., [2021](#bib.bib12)).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一领域，新兴趋势涉及开发分析可见光数据中多光谱信息的替代方法（通常是RGB），即通过开发优化用于融合的深度学习架构，无论是在数据、特征、得分还是决策层次（Bigdeli
    et al., [2021](#bib.bib12)）。
- en: In the visual surveillance setting, the main challenge regards the development
    of optimized data acquisition settings, profiting from the advances in remote
    sensing technologies, that should be able to augment the quality (e.g., resolution
    and sharpness) of the obtained irises. In this scope, the research on active data
    acquisition technologies (based in PTZ devices, or similar) might also be an interesting
    emerging possibility (Han, [2021](#bib.bib67)).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉监控环境中，主要挑战在于开发优化的数据采集设置，利用遥感技术的进步，这些技术应该能够提高获取虹膜的质量（例如，分辨率和清晰度）。在这方面，基于PTZ设备或类似设备的主动数据采集技术的研究也可能成为一个有趣的新兴可能性（Han，[2021](#bib.bib67)）。
- en: 10\. Conclusions
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10\. 结论
- en: Motivated by the tremendous success of DL-based solutions for many different
    solutions to everyday problems, machine learning is entering one of its golden
    era, attracting growing interests from the research, commercial and governmental
    communities. In short, deep learning uses multiple layers to represent the abstractions
    of data to build computational models that - even in a bit surprising way - typically
    surpass the previous generation of handcrafted-based automata. However, being
    extremely data-driven, the effectiveness of DL-based solutions is typically constrained
    by the existence of massive amounts of data, annotated in a consistent way.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 受到DL（深度学习）解决方案在许多日常问题中取得的巨大成功的激励，机器学习正进入其黄金时代，吸引了研究、商业和政府界的越来越多的关注。简而言之，深度学习使用多个层次来表示数据的抽象，从而构建计算模型，这些模型通常以略显惊讶的方式超越了之前一代的手工制作自动机。然而，由于极度依赖数据，基于DL的解决方案的有效性通常受到需要大量一致注释数据的限制。
- en: As in the generality of the computer-vision topics, a myriad of DL-based techniques
    has been proposed over the last years to perform biometric recognition, and -
    in particular - iris recognition. Nowadays, the existing methods cover the whole
    phases of the typical processing chain, from the preprocessing, segmentation,
    feature extraction up to the matching and recognition steps.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 与计算机视觉主题的一般情况一样，近年来已经提出了大量基于DL的技术来进行生物识别，特别是虹膜识别。目前，现有的方法涵盖了典型处理链的所有阶段，从预处理、分割、特征提取到匹配和识别步骤。
- en: Accordingly, this article provides the first comprehensive review of the historical
    and state-of-the-art approaches in DL-based techniques for iris recognition, followed
    by an in-depth analysis on pivoting and groundbreaking advances in each phase
    of the processing chain. We summarize and critically compare the most relevant
    methods for iris acquisition, segmentation, quality assessment, feature encoding,
    matching and recognition problems, also presenting the most relevant open-problems
    for each phase.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本文提供了对基于DL的虹膜识别技术的历史和最先进方法的首次全面回顾，随后对处理链每个阶段的关键性突破性进展进行了深入分析。我们总结并批判性地比较了与虹膜获取、分割、质量评估、特征编码、匹配和识别问题相关的最重要的方法，同时还呈现了每个阶段的最相关的开放问题。
- en: Finally, we review the typical issues faced in DL-based methods in this domain
    of expertize, such as unsupervised learning, black-box models, and online learning
    and to illustrate how these challenges can be important to open prolific future
    research paths and solutions.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们回顾了在这一领域的DL（深度学习）方法所面临的典型问题，如无监督学习、黑箱模型和在线学习，并展示了这些挑战如何为未来开辟丰富的研究路径和解决方案。
- en: Acknowledgements.
  id: totrans-340
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank Adam Czajka from the University of Notre Dame, USA for
    the contribution in the early version of this survey paper in Sections 1, 5 and
    6. The work due to Hugo Proença was funded by FCT/MEC through national funds and
    co-funded by FEDER - PT2020 partnership agreement under the projects UIDB/50008/2020,
    POCI-01-0247-FEDER- 033395. Author Alonso-Fernandez thanks the Swedish Innovation
    Agency VINNOVA (project MIDAS and DIFFUSE) and the Swedish Research Council (project
    2021-05110) for funding his research.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢美国圣母大学的Adam Czajka在本调查论文第1、5和6节的早期版本中的贡献。Hugo Proença的工作得到了FCT/MEC通过国家资金的资助，并通过FEDER
    - PT2020合作协议在项目UIDB/50008/2020、POCI-01-0247-FEDER-033395下共同资助。作者Alonso-Fernandez感谢瑞典创新署VINNOVA（MIDAS和DIFFUSE项目）以及瑞典研究委员会（项目2021-05110）对其研究的资助。
- en: References
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Agarwal et al. (2022a) A. Agarwal, A. Noore, M. Vatsa, and R. Singh. 2022a.
    Enhanced iris presentation attack detection via contraction-expansion CNN. *Pattern
    Recognition Letters* 159 (2022), 61–69.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等（2022a）A. Agarwal, A. Noore, M. Vatsa, 和 R. Singh. 2022a. 通过收缩-扩张CNN增强虹膜呈现攻击检测。*模式识别快报*
    159（2022），61–69。
- en: Agarwal et al. (2022b) A. Agarwal, A. Noore, M. Vatsa, and R. Singh. 2022b.
    Generalized Contact Lens Iris Presentation Attack Detection. *IEEE Transactions
    on Biometrics, Behavior, and Identity Science* (2022), 1–1.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等（2022b）A. Agarwal, A. Noore, M. Vatsa, 和 R. Singh. 2022b. 广义隐形眼镜虹膜呈现攻击检测。*IEEE
    生物识别、行为与身份科学汇刊*（2022），1–1。
- en: 'Ahmad and Fuller (2019) S. Ahmad and B. Fuller. 2019. ThirdEye: Triplet Based
    Iris Recognition without Normalization. In *IEEE Int. Conf. on Biometrics: Theory
    Applications and Systems (BTAS)*. 1–9.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad 和 Fuller（2019）S. Ahmad 和 B. Fuller. 2019. ThirdEye：基于三元组的虹膜识别，无需归一化。收录于*IEEE
    国际生物识别理论应用与系统会议 (BTAS)*。1–9。
- en: Alonso-Fernandez and Bigun (2016) F. Alonso-Fernandez and J. Bigun. 2016. A
    survey on periocular biometrics research. *Pattern Recognition Letters* 82 (2016),
    92–105.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alonso-Fernandez 和 Bigun（2016）F. Alonso-Fernandez 和 J. Bigun. 2016. 关于眼周生物特征研究的调查。*模式识别快报*
    82（2016），92–105。
- en: Alonso-Fernandez et al. (2012) F. Alonso-Fernandez, J. Fierrez, and J. Ortega-Garcia.
    2012. Quality Measures in Biometric Systems. *IEEE Security and Privacy* 10, 6
    (2012), 52–62.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alonso-Fernandez 等（2012）F. Alonso-Fernandez, J. Fierrez, 和 J. Ortega-Garcia.
    2012. 生物特征系统中的质量测量。*IEEE 安全与隐私* 10, 6（2012），52–62。
- en: 'Alonso-Fernandez et al. (2021) F. Alonso-Fernandez, K. Hernandez-Diaz, S. Ramis,
    F. J. Perales, and J. Bigun. 2021. Facial masks and soft-biometrics: Leveraging
    face recognition CNNs for age and gender prediction on mobile ocular images. *IET
    Biometrics* 10, 5 (2021).'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alonso-Fernandez 等（2021）F. Alonso-Fernandez, K. Hernandez-Diaz, S. Ramis, F.
    J. Perales, 和 J. Bigun. 2021. 面罩和软生物特征：利用人脸识别CNN进行移动眼部图像的年龄和性别预测。*IET 生物识别* 10,
    5（2021）。
- en: Anisetti et al. (2019) M. Anisetti, Y.-H. Li, P.-J. Huang, and Y. Juan. 2019.
    An Efficient and Robust Iris Segmentation Algorithm Using Deep Learning. *Mobile
    Information Systems* (2019), 4568929.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anisetti 等（2019）M. Anisetti, Y.-H. Li, P.-J. Huang, 和 Y. Juan. 2019. 一种高效且稳健的深度学习虹膜分割算法。*移动信息系统*（2019），4568929。
- en: Arora and Bhatia (2020) S. Arora and M.P.S. Bhatia. 2020. Presentation attack
    detection for iris recognition using deep learning. *Int J Syst Assur Eng Manag*
    11 (2020), 232–238.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora 和 Bhatia（2020）S. Arora 和 M.P.S. Bhatia. 2020. 基于深度学习的虹膜识别呈现攻击检测。*Int J
    Syst Assur Eng Manag* 11（2020），232–238。
- en: 'Badrinarayanan et al. (2017) V. Badrinarayanan, A. Kendall, and R. Cipolla.
    2017. SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.
    *IEEE Trans. Pattern Anal. Mach. Intell.* 39, 12 (2017), 2481–2495.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Badrinarayanan 等（2017）V. Badrinarayanan, A. Kendall, 和 R. Cipolla. 2017. SegNet：一种用于图像分割的深度卷积编码-解码架构。*IEEE
    模式分析与机器智能汇刊* 39, 12（2017），2481–2495。
- en: Banerjee et al. (2022) A. Banerjee, C. Ghosh, and S. N. Mandal. 2022. Analysis
    of V-Net Architecture for Iris Segmentation in Unconstrained Scenarios. *SN Computer
    Science* 3 (2022).
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banerjee 等（2022）A. Banerjee, C. Ghosh, 和 S. N. Mandal. 2022. 对V-Net架构在非约束场景中进行虹膜分割的分析。*SN
    计算机科学* 3（2022）。
- en: Bigdeli et al. (2021) B. Bigdeli, P. Pahlavani, and H. A. Amirkolaee. 2021.
    An ensemble deep learning method as data fusion system for remote sensing multisensor
    classification. *Applied Soft Computing* 110 (2021), 107563.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bigdeli 等（2021）B. Bigdeli, P. Pahlavani, 和 H. A. Amirkolaee. 2021. 作为数据融合系统的集成深度学习方法，用于遥感多传感器分类。*应用软计算*
    110（2021），107563。
- en: 'Bolme et al. (2016) D. S. Bolme, R. A. Tokola, C. B. Boehnen, T. B. Saul, K. A.
    Sauerwein, and D. W. Steadman. 2016. Impact of environmental factors on biometric
    matching during human decomposition. In *IEEE Int. Conf. on Biometrics: Theory
    Applications and Systems (BTAS)*. IEEE, USA, 1–8.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolme 等（2016）D. S. Bolme, R. A. Tokola, C. B. Boehnen, T. B. Saul, K. A. Sauerwein,
    和 D. W. Steadman. 2016. 环境因素对人体分解过程中生物特征匹配的影响。收录于*IEEE 国际生物识别理论应用与系统会议 (BTAS)*。IEEE，美国，1–8。
- en: Boutros et al. (2020) F. Boutros, N. Damer, K. Raja, R. Ramachandra, F. Kirchbuchner,
    and A. Kuijper. 2020. On Benchmarking Iris Recognition within a Head-mounted Display
    for AR/VR Applications. In *IEEE Int. Joint Conf. on Biometrics (IJCB)*. USA.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boutros 等（2020）F. Boutros, N. Damer, K. Raja, R. Ramachandra, F. Kirchbuchner,
    和 A. Kuijper. 2020. 在头戴式显示器中对虹膜识别进行基准测试，应用于AR/VR。收录于*IEEE 国际生物识别联合会议 (IJCB)*。美国。
- en: Boyd et al. (2022) A. Boyd, K. Bowyer, and A. Czajka. 2022. Human-Aided Saliency
    Maps Improve Generalization of Deep Learning. In *IEEE Winter Conference on Applications
    of Computer Vision (WACV)*.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boyd 等（2022）A. Boyd, K. Bowyer, 和 A. Czajka. 2022. 人工辅助显著性图改善深度学习的泛化能力。收录于*IEEE
    冬季计算机视觉应用会议 (WACV)*。
- en: 'Boyd et al. (2019) A. Boyd, A. Czajka, and K. Bowyer. 2019. DL-Based Feature
    Extraction in Iris Recognition: Use Existing Models, Fine-tune or Train From Scratch?.
    In *IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS)*. 1–9.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boyd 等 (2019) A. Boyd, A. Czajka, 和 K. Bowyer. 2019. 基于深度学习的虹膜识别特征提取：使用现有模型、微调还是从头训练？在
    *IEEE 生物特征学国际会议：理论应用与系统 (BTAS)*. 1–9.
- en: 'Boyd et al. (2020a) A. Boyd, Z. Fang, A. Czajka, and K. Bowyer. 2020a. Iris
    presentation attack detection: Where are we now? *Pattern Recognition Letters*
    138 (2020), 483–489.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boyd 等 (2020a) A. Boyd, Z. Fang, A. Czajka, 和 K. Bowyer. 2020a. 虹膜呈现攻击检测：我们现在处于何种阶段？*模式识别快报*
    138 (2020), 483–489.
- en: 'Boyd et al. (2021) A. Boyd, P. Tinsley, K. Bowyer, and A. Czajka. 2021. CYBORG:
    Blending Human Saliency Into the Loss Improves Deep Learning. arXiv:2112.00686 [cs.CV]'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boyd 等 (2021) A. Boyd, P. Tinsley, K. Bowyer, 和 A. Czajka. 2021. CYBORG：将人类显著性融入损失函数中改善深度学习。arXiv:2112.00686
    [cs.CV]
- en: Boyd et al. (2020b) A. Boyd, S. Yadav, T. Swearingen, A. Kuehlkamp, M. Trokielewicz,
    E. Benjamin, P. Maciejewicz, D. Chute, A. Ross, P. Flynn, K. Bowyer, and A. Czajka.
    2020b. Post-Mortem Iris Recognition—A Survey and Assessment of the State of the
    Art. *IEEE Access* 8 (2020), 136570–136593.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boyd 等 (2020b) A. Boyd, S. Yadav, T. Swearingen, A. Kuehlkamp, M. Trokielewicz,
    E. Benjamin, P. Maciejewicz, D. Chute, A. Ross, P. Flynn, K. Bowyer, 和 A. Czajka.
    2020b. 死后虹膜识别—技术现状调查与评估。*IEEE Access* 8 (2020), 136570–136593.
- en: Chen and Ross (2021) C. Chen and A. Ross. 2021. An Explainable Attention-Guided
    Iris Presentation Attack Detector. In *IEEE Workshop on Applications of Computer
    Vision (WACV)*. 97–106.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Ross (2021) C. Chen 和 A. Ross. 2021. 一种可解释的注意力引导虹膜呈现攻击检测器。在 *IEEE 计算机视觉应用研讨会
    (WACV)*. 97–106.
- en: Chen et al. (2016) J. Chen, F. Shen, D. Z. Chen, and P. Flynn. 2016. Iris Recognition
    Based on Human-Interpretable Features. *IEEE Transactions on Information Forensics
    and Security* 11, 7 (2016), 1476–1485.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2016) J. Chen, F. Shen, D. Z. Chen, 和 P. Flynn. 2016. 基于人类可解释特征的虹膜识别。*IEEE
    信息取证与安全汇刊* 11, 7 (2016), 1476–1485.
- en: 'Chen et al. (2017) L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. Yuille.
    2017. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous
    Convolution, and Fully Connected CRFs. *IEEE Trans. Pattern Anal. Mach. Intell.*
    4 (2017).'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2017) L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, 和 A. Yuille. 2017.
    DeepLab：基于深度卷积网络、扩张卷积和全连接条件随机场的语义图像分割。*IEEE 模式分析与机器智能汇刊* 4 (2017).
- en: Chen et al. (2019a) Y. Chen, W. Wang, Z. Zeng, and Y. Wang. 2019a. An Adaptive
    CNNs Technology for Robust Iris Segmentation. *IEEE Access* 7 (2019), 64517–64532.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2019a) Y. Chen, W. Wang, Z. Zeng, 和 Y. Wang. 2019a. 一种自适应 CNN 技术用于鲁棒虹膜分割。*IEEE
    Access* 7 (2019), 64517–64532.
- en: 'Chen et al. (2020) Y. Chen, C. Wu, and Y. Wang. 2020. T-Center: A Novel Feature
    Extraction Approach Towards Large-Scale Iris Recognition. *IEEE Access* 8 (2020),
    32365–32375.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2020) Y. Chen, C. Wu, 和 Y. Wang. 2020. T-Center：一种面向大规模虹膜识别的新型特征提取方法。*IEEE
    Access* 8 (2020), 32365–32375.
- en: Chen et al. (2019b) Y. Chen, Z. Zeng, and F. Hu. 2019b. End to End Robust Recognition
    Method for Iris Using a Dense Deep Convolutional Neural Network. In *Biometric
    Recognition*. Springer, Cham, 364–375.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2019b) Y. Chen, Z. Zeng, 和 F. Hu. 2019b. 使用密集深度卷积神经网络的端到端鲁棒识别方法。见 *生物识别*.
    Springer, Cham, 364–375.
- en: Choudhary et al. (2021) M. Choudhary, V. Tiwari, and Venkanna U. 2021. Ensuring
    Secured Iris Authentication for Mobile Devices. In *IEEE International Conference
    on Consumer Electronics (ICCE)*. 1–5.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choudhary 等 (2021) M. Choudhary, V. Tiwari, 和 Venkanna U. 2021. 确保移动设备的安全虹膜认证。在
    *IEEE 国际消费电子会议 (ICCE)*. 1–5.
- en: Choudhary et al. (2022a) M. Choudhary, V. Tiwari, and U. Venkanna. 2022a. Iris
    Liveness Detection Using Fusion of Domain-Specific Multiple BSIF and DenseNet
    Features. *IEEE Transactions on Cybernetics* 52, 4 (2022), 2370–2381.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choudhary 等 (2022a) M. Choudhary, V. Tiwari, 和 U. Venkanna. 2022a. 使用领域特定多种
    BSIF 和 DenseNet 特征的虹膜活体检测。*IEEE 控制论汇刊* 52, 4 (2022), 2370–2381.
- en: Choudhary et al. (2022b) M. Choudhary, Tiwari V., and Venkanna U. 2022b. Identifying
    discriminatory feature-vectors for fusion-based iris liveness detection. *J Ambient
    Intell Human Comput* (2022).
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choudhary 等 (2022b) M. Choudhary, Tiwari V., 和 Venkanna U. 2022b. 识别基于融合的虹膜活体检测的判别特征向量。*J
    Ambient Intell Human Comput* (2022).
- en: 'Czajka (2013) A. Czajka. 2013. Database of iris printouts and its application:
    Development of liveness detection method for iris recognition. In *International
    Conference on Methods and Models in Automation and Robotics (MMAR)*. 28–33.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Czajka (2013) A. Czajka. 2013. 虹膜打印数据库及其应用：虹膜识别活体检测方法的发展。见 *自动化与机器人方法与模型国际会议
    (MMAR)*. 28–33.
- en: Czajka (2021) A. Czajka. 2021. Iris recognition designed for post-mortem and
    diseased eyes. (2021). [https://github.com/aczajka/iris-recognition---pm-diseased-human-driven-bsif](https://github.com/aczajka/iris-recognition---pm-diseased-human-driven-bsif)
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Czajka（2021）A. Czajka. 2021. 针对死后和病态眼睛设计的虹膜识别。（2021年）。[https://github.com/aczajka/iris-recognition---pm-diseased-human-driven-bsif](https://github.com/aczajka/iris-recognition---pm-diseased-human-driven-bsif)
- en: 'Czajka and Bowyer (2018) A. Czajka and K. Bowyer. 2018. Presentation Attack
    Detection for Iris Recognition: An Assessment of the State-of-the-Art. *ACM Comput.
    Surv.* 51, 4, Article 86 (Jul 2018), 35 pages.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Czajka 和 Bowyer（2018）A. Czajka 和 K. Bowyer. 2018. 虹膜识别的伪造攻击检测：现状评估。*ACM 计算机调查*
    51, 4, 文章 86 (2018年7月), 35页。
- en: Czajka et al. (2019) A. Czajka, Z. Fang, and K. Bowyer. 2019. Iris Presentation
    Attack Detection Based on Photometric Stereo Features. In *IEEE Winter Conference
    on Applications of Computer Vision (WACV)*. 877–885.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Czajka 等（2019）A. Czajka, Z. Fang, 和 K. Bowyer. 2019. 基于光度立体特征的虹膜伪造攻击检测。见于 *IEEE
    冬季计算机视觉应用会议 (WACV)*。877–885。
- en: 'Damer et al. (2019) N. Damer, K. Dimitrov, A. Braun, and A. Kuijper. 2019.
    On Learning Joint Multi-biometric Representations by Deep Fusion. In *IEEE Int.
    Conf. on Biometrics: Theory Applications and Systems (BTAS)*. 1–8.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Damer 等（2019）N. Damer, K. Dimitrov, A. Braun, 和 A. Kuijper. 2019. 通过深度融合学习联合多生物特征表示。见于
    *IEEE 国际生物识别理论应用与系统会议 (BTAS)*。1–8。
- en: Das et al. (2020) P. Das, J. Mcfiratht, Z. Fang, A. Boyd, G. Jang, A. Mohammadi,
    S. Purnapatra, D. Yambay, S. Marcel, M. Trokielewicz, P. Maciejewicz, K. Bowyer,
    A. Czajka, S. Schuckers, J. Tapia, S. Gonzalez, M. Fang, N. Damer, F. Boutros,
    A. Kuijper, R. Sharma, C. Chen, and A. Ross. 2020. Iris Liveness Detection Competition
    (LivDet-Iris) - The 2020 Edition. In *IEEE Int. Joint Conf. on Biometrics (IJCB)*.
    1–9.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Das 等（2020）P. Das, J. Mcfiratht, Z. Fang, A. Boyd, G. Jang, A. Mohammadi, S.
    Purnapatra, D. Yambay, S. Marcel, M. Trokielewicz, P. Maciejewicz, K. Bowyer,
    A. Czajka, S. Schuckers, J. Tapia, S. Gonzalez, M. Fang, N. Damer, F. Boutros,
    A. Kuijper, R. Sharma, C. Chen, 和 A. Ross. 2020. 虹膜活体检测竞赛（LivDet-Iris）- 2020年版。见于
    *IEEE 国际联合生物识别会议 (IJCB)*。1–9。
- en: Daugman (1993) J. Daugman. 1993. High confidence visual recognition of persons
    by a test of statistical independence. *IEEE Trans. Pattern Anal. Mach. Intell.*
    15, 11 (1993), 1148–1161.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daugman（1993）J. Daugman. 1993. 通过统计独立性测试实现对人的高度可信视觉识别。*IEEE 事务. 模式分析与机器智能* 15,
    11 (1993), 1148–1161。
- en: Daugman (2007) J. Daugman. 2007. New methods in iris recognition. *IEEE Transactions
    on Systems, Man and Cybernetics* 37 (2007), 1167–1175.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daugman（2007）J. Daugman. 2007. 虹膜识别的新方法。*IEEE 系统、人类与控制论汇刊* 37 (2007), 1167–1175。
- en: Daugman (2016) J. Daugman. 2016. Information Theory and the IrisCode. *IEEE
    Transactions on Information Forensics and Security* 11 (2016), 400–409.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daugman（2016）J. Daugman. 2016. 信息理论与 IrisCode。*IEEE 信息取证与安全汇刊* 11 (2016), 400–409。
- en: 'Daugman (2021) J. Daugman. 2021. Collision Avoidance on National and Global
    Scales: Understanding and Using Big Biometric Entropy. *TechRxiv* (2021).'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daugman（2021）J. Daugman. 2021. 国家和全球尺度上的碰撞避免：理解和使用大规模生物特征熵。*TechRxiv* (2021)。
- en: De Marsico et al. (2015) M. De Marsico, M. Nappi, D. Riccio, and H. Wechsler.
    2015. Mobile Iris Challenge Evaluation (MICHE)-I, biometric iris dataset and protocols.
    *Pattern Recognition Letters* 57 (2015), 17–23. Mobile Iris CHallenge Evaluation
    part I (MICHE I).
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Marsico 等（2015）M. De Marsico, M. Nappi, D. Riccio, 和 H. Wechsler. 2015. 移动虹膜挑战评估（MICHE）-I，生物特征虹膜数据集和协议。*模式识别信函*
    57 (2015), 17–23. 移动虹膜挑战评估第一部分（MICHE I）。
- en: Ding et al. (2022) X. Ding, H. Fang, Z. Zhang, K.-K. R. Choo, and H. Jin. 2022.
    Privacy-Preserving Feature Extraction via Adversarial Training. *IEEE Transactions
    on Knowledge and Data Engineering* 34, 4 (2022), 1967–1979.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等（2022）X. Ding, H. Fang, Z. Zhang, K.-K. R. Choo, 和 H. Jin. 2022. 通过对抗训练进行隐私保护特征提取。*IEEE
    知识与数据工程汇刊* 34, 4 (2022), 1967–1979。
- en: Dong et al. (2009) W. Dong, Z. Sun, and T. Tan. 2009. A Design of Iris Recognition
    System at a Distance. In *Chinese Conference on Pattern Recognition*. 1–5.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2009）W. Dong, Z. Sun, 和 T. Tan. 2009. 远距离虹膜识别系统的设计。见于 *中国模式识别会议*。1–5。
- en: Doyle and Bowyer (2015) J. S. Doyle and K. Bowyer. 2015. Robust Detection of
    Textured Contact Lenses in Iris Recognition Using BSIF. *IEEE Access* 3 (2015),
    1672–1683.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doyle 和 Bowyer（2015）J. S. Doyle 和 K. Bowyer. 2015. 使用 BSIF 在虹膜识别中鲁棒检测纹理接触镜。*IEEE
    Access* 3 (2015), 1672–1683。
- en: 'Doyle et al. (2013) J. S. Doyle, K. Bowyer, and P. Flynn. 2013. Variation in
    accuracy of textured contact lens detection based on sensor and lens pattern.
    In *IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS)*. 1–7.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doyle 等（2013）J. S. Doyle, K. Bowyer, 和 P. Flynn. 2013. 基于传感器和镜片模式的纹理接触镜检测准确性的变化。见于
    *IEEE 国际生物识别理论应用与系统会议 (BTAS)*。1–7。
- en: 'Elsken et al. (2019) T. Elsken, J. H. Metzen, and F. Hutter. 2019. Neural architecture
    search: A survey. *J. Mach. Learn. Res.* 20, 55 (2019).'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Elsken et al. (2019) T. Elsken, J. H. Metzen, and F. Hutter. 2019. 神经架构搜索:
    一项调查。*J. Mach. Learn. Res.* 20, 55 (2019)。'
- en: Fang et al. (2020a) M. Fang, N. Damer, Fadi Boutros, F. Kirchbuchner, and A.
    Kuijper. 2020a. Deep Learning Multi-layer Fusion for an Accurate Iris Presentation
    Attack Detection. In *IEEE International Conference on Information Fusion (FUSION)*.
    1–8.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2020a) M. Fang, N. Damer, Fadi Boutros, F. Kirchbuchner, and A.
    Kuijper. 2020a. 深度学习多层融合用于准确的虹膜演示攻击检测。在*IEEE国际信息融合会议(FUSION)*上。1–8。
- en: Fang et al. (2021b) M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper.
    2021b. Cross-database and cross-attack Iris presentation attack detection using
    micro stripes analyses. *Image and Vision Computing* 105 (2021), 104057.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2021b) M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper.
    2021b. 通过微条纹分析进行跨数据库和跨攻击虹膜攻击检测。*图像与视觉计算* 105 (2021), 104057.
- en: Fang et al. (2021c) M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper.
    2021c. Iris Presentation Attack Detection by Attention-based and Deep Pixel-wise
    Binary Supervision Network. In *IEEE Int. Joint Conf. on Biometrics (IJCB)*. 1–8.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2021c) M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper.
    2021c. 基于注意力和深度像素二元监督网络的虹膜演示攻击检测。在*IEEE国际联合会议生物特征学(IJCB)*上。1–8。
- en: Fang et al. (2021d) M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper.
    2021d. The overlapping effect and fusion protocols of data augmentation techniques
    in iris PAD. *Machine Vision and Applications* 33 (2021).
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2021d) M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper.
    2021d. 数据扩增技术在虹膜PAD中的重叠效应和融合协议。*机器视觉与应用* 33 (2021)。
- en: Fang et al. (2020b) M. Fang, N. Damer, F. Kirchbuchner, and A. Kuijper. 2020b.
    Micro Stripes Analyses for Iris Presentation Attack Detection. In *IEEE Int. Joint
    Conf. on Biometrics (IJCB)*. 1–10.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2020b) M. Fang, N. Damer, F. Kirchbuchner, and A. Kuijper. 2020b.
    用于虹膜演示攻击检测的微条纹分析。在*IEEE国际联合会议生物特征学(IJCB)*上。1–10。
- en: Fang et al. (2021e) M. Fang, N. Damer, F. Kirchbuchner, and A. Kuijper. 2021e.
    Demographic Bias in Presentation Attack Detection of Iris Recognition Systems.
    In *28th European Signal Processing Conference (EUSIPCO)*. 835–839.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2021e) M. Fang, N. Damer, F. Kirchbuchner, and A. Kuijper. 2021e.
    虹膜识别系统中的人口统计偏见演示攻击检测。在*第28届欧洲信号处理会议(EUSIPCO)*上。835–839。
- en: Fang and Czajka (2020) Z. Fang and A. Czajka. 2020. Open Source Iris Recognition
    Hardware and Software with Presentation Attack Detection. In *IEEE Int. Joint
    Conf. on Biometrics (IJCB)*. 1–8.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang and Czajka (2020) Z. Fang and A. Czajka. 2020. 使用演示攻击检测的开源虹膜识别硬件和软件。在*IEEE国际联合会议生物特征学(IJCB)*上。1–8。
- en: Fang et al. (2021a) Z. Fang, A. Czajka, and K. Bowyer. 2021a. Robust Iris Presentation
    Attack Detection Fusing 2D and 3D Information. *IEEE Transactions on Information
    Forensics and Security* 16 (2021), 510–520.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2021a) Z. Fang, A. Czajka, and K. Bowyer. 2021a. 融合2D和3D信息的鲁棒虹膜演示攻击检测。*IEEE信息取证与安全事务*
    16 (2021), 510–520。
- en: FBI Criminal Justice Information Services (CJIS) Division (2021) FBI Criminal
    Justice Information Services (CJIS) Division. 2021. *Next Generation Identification
    (NGI)*. Retrieved 2021 from [https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi](https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi)
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FBI刑事司法信息服务(CJIS)部门(2021) FBI刑事司法信息服务(CJIS)部门。2021年。*下一代识别(NGI)*。取自2021年[https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi](https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi)
- en: 'Fierrez et al. (2007) J. Fierrez, J. Ortega-Garcia, D. Torre Toledano, and
    J. Gonzalez-Rodriguez. 2007. Biosec baseline corpus: A multimodal biometric database.
    *Pattern Recognition* 40, 4 (2007), 1389–1392.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fierrez et al. (2007) J. Fierrez, J. Ortega-Garcia, D. Torre Toledano, and
    J. Gonzalez-Rodriguez. 2007. 生物安全基线语料库: 一种多模态生物识别数据库。*Pattern Recognition* 40,
    4 (2007), 1389–1392。'
- en: Galbally and Gomez-Barrero (2016) J. Galbally and M. Gomez-Barrero. 2016. A
    review of iris anti-spoofing. In *4th International Conference on Biometrics and
    Forensics (IWBF)*. 1–6.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galbally and Gomez-Barrero (2016) J. Galbally and M. Gomez-Barrero. 2016. 虹膜反欺骗的综述。在*第四届国际生物特征学和法医学大会(IWBF)*上。1–6。
- en: Ganeeva and Myasnikov (2020) Y. Ganeeva and E. Myasnikov. 2020. Using Convolutional
    Neural Networks for Segmentation of Iris Images. In *International Multi-Conference
    on Industrial Engineering and Modern Technologies (FarEastCon)*. 1–4.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ganeeva and Myasnikov (2020) Y. Ganeeva and E. Myasnikov. 2020. 使用卷积神经网络对虹膜图像进行分割。在*工业工程和现代技术国际多会议(FarEastCon)*上。1–4。
- en: 'Gangwar and Joshi (2016) A. Gangwar and A. Joshi. 2016. DeepIrisNet: Deep iris
    representation with applications in iris recognition and cross-sensor iris recognition.
    In *IEEE International Conference on Image Processing*. 2301–2305.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gangwar 和 Joshi (2016) A. Gangwar 和 A. Joshi. 2016. DeepIrisNet: 深度虹膜表示及其在虹膜识别和跨传感器虹膜识别中的应用。发表于
    *IEEE 国际图像处理大会*。2301–2305。'
- en: 'Gangwar et al. (2019) A. Gangwar, A. Joshi, P. Joshi, and R. Raghavendra. 2019.
    DeepIrisNet2: Learning Deep-IrisCodes from Scratch for Segmentation-Robust Visible
    Wavelength and Near Infrared Iris Recognition. *CoRR* abs/1902.05390 (2019).'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gangwar 等 (2019) A. Gangwar, A. Joshi, P. Joshi, 和 R. Raghavendra. 2019. DeepIrisNet2:
    从零开始学习深度虹膜编码，用于分割稳健的可见光和近红外虹膜识别。*CoRR* abs/1902.05390 (2019)。'
- en: 'Garbin et al. (2019) S. J. Garbin, Y. Shen, I. Schuetz, R. Cavin, G. Hughes,
    and S. S. Talathi. 2019. OpenEDS: Open Eye Dataset. *CoRR* abs/1905.03702 (2019).
    arXiv:1905.03702'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Garbin 等 (2019) S. J. Garbin, Y. Shen, I. Schuetz, R. Cavin, G. Hughes, 和 S.
    S. Talathi. 2019. OpenEDS: 开放眼睛数据集。*CoRR* abs/1905.03702 (2019)。arXiv:1905.03702'
- en: Gautam et al. (2022) G. Gautam, A. Raj, and S. Mukhopadhyay. 2022. Deep supervised
    class encoding for iris presentation attack detection. *Digital Signal Processing*
    121 (2022), 103329.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gautam 等 (2022) G. Gautam, A. Raj, 和 S. Mukhopadhyay. 2022. 深度监督类编码用于虹膜展示攻击检测。*数字信号处理*
    121 (2022), 103329。
- en: Goodfellow et al. (2014) I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D.
    Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. 2014. Generative Adversarial
    Nets. In *International Conference on Neural Information Processing Systems (NIPS)*
    (Canada). USA, 9.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 (2014) I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, 和 Y. Bengio. 2014. 生成对抗网络。发表于 *国际神经信息处理系统大会 (NIPS)* (加拿大)。美国,
    9。
- en: Gragnaniello et al. (2016) D. Gragnaniello, C. Sansone, G. Poggi, and L. Verdoliva.
    2016. Biometric Spoofing Detection by a Domain-Aware Convolutional Neural Network.
    In *International Conference on Signal-Image Technology and Internet-Based Systems
    (SITIS)*.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gragnaniello 等 (2016) D. Gragnaniello, C. Sansone, G. Poggi, 和 L. Verdoliva.
    2016. 基于领域感知卷积神经网络的生物识别伪造检测。发表于 *国际信号图像技术与基于互联网的系统大会 (SITIS)*。
- en: Grother and Tabassi (2007) P. Grother and E. Tabassi. 2007. Performance of Biometric
    Quality Measures. *IEEE Trans. Pattern Anal. Mach. Intell.* 29 (2007), 531–543.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grother 和 Tabassi (2007) P. Grother 和 E. Tabassi. 2007. 生物识别质量测量的性能。*IEEE 模式分析与机器智能汇刊*
    29 (2007), 531–543。
- en: Gupta et al. (2021) M. Gupta, S. Singh, A. Agarwal, M. Vatsa, and R. Singh.
    2021. Generalized Iris Presentation Attack Detection Algorithm under Cross-Database
    Settings. In *Int. Conf. on Pattern Recognition (ICPR)*. 5318–5325.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等 (2021) M. Gupta, S. Singh, A. Agarwal, M. Vatsa, 和 R. Singh. 2021. 跨数据库环境下的广义虹膜展示攻击检测算法。发表于
    *国际模式识别大会 (ICPR)*. 5318–5325。
- en: Gupta et al. (2014) P. Gupta, S. Behera, M. Vatsa, and R. Singh. 2014. On Iris
    Spoofing Using Print Attack. In *Int. Conf. on Pattern Recognition (ICPR)*. 1681–1686.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等 (2014) P. Gupta, S. Behera, M. Vatsa, 和 R. Singh. 2014. 关于使用打印攻击的虹膜伪造。发表于
    *国际模式识别大会 (ICPR)*. 1681–1686。
- en: Hafner et al. (2021) A. Hafner, P. Peer, Ž. Emeršič, and M. Vitek. 2021. Deep
    Iris Feature Extraction. In *International Conference on Artificial Intelligence
    in Information and Communication (ICAIIC)*. 258–262.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner 等 (2021) A. Hafner, P. Peer, Ž. Emeršič, 和 M. Vitek. 2021. 深度虹膜特征提取。发表于
    *国际人工智能信息与通信大会 (ICAIIC)*. 258–262。
- en: Han (2021) Y. Han. 2021. Design of An Active Infrared Iris Recognition Device.
    In *IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers
    (IPEC)*. 435–437.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han (2021) Y. Han. 2021. 主动红外虹膜识别设备的设计。发表于 *IEEE 亚太图像处理、电子学与计算机会议 (IPEC)*. 435–437。
- en: 'He et al. (2022) Y. He, G. Meng, K. Chen, X. Hu, and J. He. 2022. Towards Security
    Threats of Deep Learning Systems: A Survey. *IEEE Transactions on Software Engineering*
    48, 5 (2022), 1743–1770.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2022) Y. He, G. Meng, K. Chen, X. Hu, 和 J. He. 2022. 面向深度学习系统的安全威胁：综述。*IEEE
    软件工程汇刊* 48, 5 (2022), 1743–1770。
- en: He et al. (2009) Z. He, T. Tan, Z. Sun, and X. Qiu. 2009. Toward Accurate and
    Fast Iris Segmentation for Iris Biometrics. *IEEE Trans. Pattern Anal. Mach. Intell.*
    31, 9 (2009), 1670–1684.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2009) Z. He, T. Tan, Z. Sun, 和 X. Qiu. 2009. 实现准确且快速的虹膜分割以用于虹膜生物识别。*IEEE
    模式分析与机器智能汇刊* 31, 9 (2009), 1670–1684。
- en: Hedman et al. (2021) P. Hedman, V. Skepetzis, K. Hernandez-Diaz, J. Bigün, and
    F. Alonso-Fernandez. 2021. On the Effect of Selfie Beautification Filters on Face
    Detection and Recognition. *CoRR* abs/2110.08934 (2021). arXiv:2110.08934
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hedman 等 (2021) P. Hedman, V. Skepetzis, K. Hernandez-Diaz, J. Bigün, 和 F. Alonso-Fernandez.
    2021. 自拍美容滤镜对面部检测和识别的影响。*CoRR* abs/2110.08934 (2021)。arXiv:2110.08934
- en: Hernandez-Diaz et al. (2018) K. Hernandez-Diaz, F. Alonso-Fernandez, and J.
    Bigun. 2018. Periocular Recognition Using CNN Features Off-the-Shelf. In *International
    Conference of the Biometrics Special Interest Group (BIOSIG)*. 1–5.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hernandez-Diaz et al. (2018) K. Hernandez-Diaz, F. Alonso-Fernandez, 和 J. Bigun.
    2018. 使用 CNN 特征的眼周识别。 在 *生物特征识别特别兴趣小组国际会议 (BIOSIG)* 上。1–5。
- en: Hernandez-Diaz et al. (2020) K. Hernandez-Diaz, F. Alonso-Fernandez, and J.
    Bigun. 2020. Cross-Spectral Periocular Recognition with Conditional Adversarial
    Networks. In *IEEE Int. Joint Conf. on Biometrics (IJCB)*. 1–9.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hernandez-Diaz et al. (2020) K. Hernandez-Diaz, F. Alonso-Fernandez, 和 J. Bigun.
    2020. 基于条件对抗网络的跨光谱眼周识别。在 *IEEE 国际生物特征识别联合会议 (IJCB)* 上。1–9。
- en: Hofbauer et al. (2019) H. Hofbauer, E. Jalilian, and A. Uhl. 2019. Exploiting
    superior CNN-based iris segmentation for better recognition accuracy. *Pattern
    Recognition Letters* 120 (2019), 17–23.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hofbauer et al. (2019) H. Hofbauer, E. Jalilian, 和 A. Uhl. 2019. 利用优越的 CNN 基础虹膜分割提高识别准确率。*模式识别快报*
    120 (2019), 17–23。
- en: Howard et al. (2019) A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M.
    Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le, and H. Adam. 2019. Searching
    for MobileNetV3\. In *IEEE Int. Conf. on Computer Vision (ICCV)*.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard et al. (2019) A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M.
    Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le, 和 H. Adam. 2019. 搜索 MobileNetV3。在
    *IEEE 国际计算机视觉会议 (ICCV)* 上。
- en: Hsiao and Fan (2021) C.-S. Hsiao and C.-P. Fan. 2021. EfficientNet Based Iris
    Biometric Recognition Methods with Pupil Positioning by U-Net. In *3rd International
    Conference on Computer Communication and the Internet (ICCCI)*. 1–5.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsiao 和 Fan (2021) C.-S. Hsiao 和 C.-P. Fan. 2021. 基于 EfficientNet 的虹膜生物识别方法及
    U-Net 定位瞳孔。在 *第三届国际计算机通信与互联网会议 (ICCCI)* 上。1–5。
- en: Hu et al. (2022) L. Hu, J. Li, G. Lin, S. Peng, Z. Zhang, Y. Zhang, and C. Dong.
    2022. Defending against Membership Inference Attacks with High Utility by GAN.
    *IEEE Transactions on Dependable and Secure Computing* (2022), 1–1.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2022) L. Hu, J. Li, G. Lin, S. Peng, Z. Zhang, Y. Zhang, 和 C. Dong.
    2022. 使用 GAN 防御高效性会员推断攻击。*IEEE 可靠性与安全计算汇刊* (2022), 1–1。
- en: Huynh et al. (2019) V. T. Huynh, S.-H. Kim, G.-S. Lee, and H.-J. Yang. 2019.
    Eye Semantic Segmentation with A Lightweight Model. In *IEEE/CVF International
    Conference on Computer Vision Workshop (ICCVW)*. 3694–3697.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huynh et al. (2019) V. T. Huynh, S.-H. Kim, G.-S. Lee, 和 H.-J. Yang. 2019. 使用轻量级模型的眼部语义分割。在
    *IEEE/CVF 国际计算机视觉会议研讨会 (ICCVW)* 上。3694–3697。
- en: Hwang and Lee (2020) H. Hwang and E. C. Lee. 2020. Near-Infrared Image-Based
    Periocular Biometric Method Using Convolutional Neural Network. *IEEE Access*
    8 (2020), 158612–158621.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hwang 和 Lee (2020) H. Hwang 和 E. C. Lee. 2020. 基于近红外图像的眼周生物特征方法，使用卷积神经网络。*IEEE
    Access* 8 (2020), 158612–158621。
- en: 'Jain et al. (2021) A. Jain, D. Deb, and J. Engelsma. 2021. Biometrics: Trust,
    but Verify. *IEEE Transactions on Biometrics, Behavior, and Identity Science*
    (2021), 1–1.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain et al. (2021) A. Jain, D. Deb, 和 J. Engelsma. 2021. 生物特征识别：信任，但要验证。*IEEE
    生物特征、行为与身份科学汇刊* (2021), 1–1。
- en: Jalilian et al. (2020) E. Jalilian, M. Karakaya, and A. Uhl. 2020. End-to-end
    Off-angle Iris Recognition Using CNN Based Iris Segmentation. In *International
    Conference of the Biometrics Special Interest Group (BIOSIG)*. 1–7.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jalilian et al. (2020) E. Jalilian, M. Karakaya, 和 A. Uhl. 2020. 基于 CNN 的虹膜分割的端到端偏角虹膜识别。在
    *生物特征识别特别兴趣小组国际会议 (BIOSIG)* 上。1–7。
- en: Jalilian et al. (2022) E. Jalilian, G. Wimmer, A. Uhl, and M. Karakaya. 2022.
    Deep Learning Based Off-Angle Iris Recognition. In *IEEE Int. Conf. on Acoustics,
    Speech, and Signal Processing (ICASSP)*. 4048–4052.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jalilian et al. (2022) E. Jalilian, G. Wimmer, A. Uhl, 和 M. Karakaya. 2022.
    基于深度学习的偏角虹膜识别。在 *IEEE 国际声学、语音与信号处理会议 (ICASSP)* 上。4048–4052。
- en: 'Johnson et al. (2010) P. A. Johnson, P. Lopez-Meyer, N. Sazonova, F. Hua, and
    S. Schuckers. 2010. Quality in face and iris research ensemble (Q-FIRE). In *IEEE
    Int. Conf. on Biometrics: Theory Applications and Systems (BTAS)*. 1–6.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson et al. (2010) P. A. Johnson, P. Lopez-Meyer, N. Sazonova, F. Hua, 和
    S. Schuckers. 2010. 面部和虹膜研究集合中的质量 (Q-FIRE)。在 *IEEE 生物特征识别理论应用与系统国际会议 (BTAS)* 上。1–6。
- en: Jung et al. (2020) Y. G. Jung, C. Y. Low, J. Park, and A. B. J. Teoh. 2020.
    Periocular Recognition in the Wild With Generalized Label Smoothing Regularization.
    *IEEE Signal Processing Letters* 27 (2020), 1455–1459.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jung et al. (2020) Y. G. Jung, C. Y. Low, J. Park, 和 A. B. J. Teoh. 2020. 在自然环境中的眼周识别与广义标签平滑正则化。*IEEE
    信号处理快报* 27 (2020), 1455–1459。
- en: 'Kaur and Manduchi (2020) H. Kaur and R. Manduchi. 2020. EyeGAN: Gaze–Preserving,
    Mask–Mediated Eye Image Synthesis. In *IEEE Winter Conference on Applications
    of Computer Vision (WACV)*. 299–308.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kaur 和 Manduchi (2020) H. Kaur 和 R. Manduchi. 2020. EyeGAN: 保持注视的、掩模介导的眼部图像合成。在
    *IEEE 冬季计算机视觉应用会议 (WACV)* 上。299–308。'
- en: Kaur and Manduchi (2021) H. Kaur and R. Manduchi. 2021. Subject Guided Eye Image
    Synthesis with Application to Gaze Redirection. In *IEEE Winter Conference on
    Applications of Computer Vision (WACV)*. 11–20.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaur 和 Manduchi (2021) H. Kaur 和 R. Manduchi. 2021. 基于主题引导的眼部图像合成及其在注视方向调整中的应用。见于
    *IEEE Winter Conference on Applications of Computer Vision (WACV)*. 11–20.
- en: Kerrigan et al. (2019) D. Kerrigan, M. Trokielewicz, A. Czajka, and K. Bowyer.
    2019. Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf
    Deep Neural Networks. In *IEEE Int. Conf. on Biometrics (ICB)*. 1–7.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kerrigan et al. (2019) D. Kerrigan, M. Trokielewicz, A. Czajka, 和 K. Bowyer.
    2019. 使用图像分割的虹膜识别与重新训练的现成深度神经网络。见于 *IEEE Int. Conf. on Biometrics (ICB)*. 1–7.
- en: 'Khalid et al. (2019) F. Khalid, M. A. Hanif, S. Rehman, R. Ahmed, and M. Shafique.
    2019. TrISec: Training Data-Unaware Imperceptible Security Attacks on Deep Neural
    Networks. In *IEEE 25th International Symposium on On-Line Testing and Robust
    System Design (IOLTS)*. 188–193.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Khalid et al. (2019) F. Khalid, M. A. Hanif, S. Rehman, R. Ahmed, 和 M. Shafique.
    2019. TrISec: 训练数据不可知的不可感知安全攻击对深度神经网络的影响。见于 *IEEE 25th International Symposium
    on On-Line Testing and Robust System Design (IOLTS)*. 188–193.'
- en: 'Khosravy et al. (2022) M. Khosravy, K. Nakamura, Y. Hirose, N. Nitta, and N.
    Babaguchi. 2022. Model Inversion Attack by Integration of Deep Generative Models:
    Privacy-Sensitive Face Generation From a Face Recognition System. *IEEE Transactions
    on Information Forensics and Security* 17 (2022), 357–372.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khosravy et al. (2022) M. Khosravy, K. Nakamura, Y. Hirose, N. Nitta, 和 N. Babaguchi.
    2022. 通过整合深度生成模型的模型反演攻击：从面部识别系统生成隐私敏感的面部图像。*IEEE Transactions on Information Forensics
    and Security* 17 (2022), 357–372.
- en: Kim et al. (2018) M. C. Kim, J. H. Koo, S. W. Cho, N. R. Baek, and K. R. Park.
    2018. Convolutional Neural Network-Based Periocular Recognition in Surveillance
    Environments. *IEEE Access* 6 (2018), 57291–57310.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2018) M. C. Kim, J. H. Koo, S. W. Cho, N. R. Baek, 和 K. R. Park.
    2018. 基于卷积神经网络的监控环境中的眼部区域识别。*IEEE Access* 6 (2018), 57291–57310.
- en: Kohli et al. (2013) N. Kohli, D. Yadav, M. Vatsa, and R. Singh. 2013. Revisiting
    iris recognition with color cosmetic contact lenses. In *IEEE Int. Conf. on Biometrics
    (ICB)*. 1–7.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kohli et al. (2013) N. Kohli, D. Yadav, M. Vatsa, 和 R. Singh. 2013. 重新审视使用彩色化妆隐形眼镜的虹膜识别。见于
    *IEEE Int. Conf. on Biometrics (ICB)*. 1–7.
- en: Kohli et al. (2017) N. Kohli, D. Yadav, M. Vatsa, R. Singh, and A. Noore. 2017.
    Synthetic iris presentation attack using iDCGAN. In *IEEE Int. Joint Conf. on
    Biometrics (IJCB)*. 674–680.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kohli et al. (2017) N. Kohli, D. Yadav, M. Vatsa, R. Singh, 和 A. Noore. 2017.
    使用iDCGAN的合成虹膜展示攻击。见于 *IEEE Int. Joint Conf. on Biometrics (IJCB)*. 674–680.
- en: Kuehlkamp et al. (2022) A. Kuehlkamp, A. Boyd, A. Czajka, K. Bowyer, P. Flynn,
    D. Chute, and E. Benjamin. 2022. Interpretable Deep Learning-Based Forensic Iris
    Segmentation and Recognition. In *2nd WACV Workshop on Explainable and Interpretable
    Artificial Intelligence for Biometrics (XAI4B)*. IEEE, USA, 1–8.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuehlkamp et al. (2022) A. Kuehlkamp, A. Boyd, A. Czajka, K. Bowyer, P. Flynn,
    D. Chute, 和 E. Benjamin. 2022. 可解释的深度学习基础的法医虹膜分割与识别。见于 *2nd WACV Workshop on Explainable
    and Interpretable Artificial Intelligence for Biometrics (XAI4B)*. IEEE, USA,
    1–8.
- en: Kumar and Passi (2010) A. Kumar and A. Passi. 2010. Comparison and combination
    of iris matchers for reliable personal authentication. *Pattern Recognition* 43,
    3 (2010), 1016–1026.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 和 Passi (2010) A. Kumar 和 A. Passi. 2010. 虹膜匹配器的比较与组合以实现可靠的个人身份认证。*Pattern
    Recognition* 43, 3 (2010), 1016–1026.
- en: Ledig et al. (2017) C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham,
    A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi. 2017. Photo-Realistic
    Single Image Super-Resolution Using a Generative Adversarial Network. In *IEEE
    Int. Conf. on Computer Vision and Pattern Recognition (CVPR)*. 105–114.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ledig et al. (2017) C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham,
    A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, 和 W. Shi. 2017. 使用生成对抗网络的照片级真实单图像超分辨率。见于
    *IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)*. 105–114.
- en: Li et al. (2021) Y.-H. Li, W. R. Putri, M. S. Aslam, and C.-C. Chang. 2021.
    Robust Iris Segmentation Algorithm in Non-Cooperative Environments Using Interleaved
    Residual U-Net. *Sensors* 21, 4 (2021).
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021) Y.-H. Li, W. R. Putri, M. S. Aslam, 和 C.-C. Chang. 2021. 使用交错残差U-Net的非合作环境中的稳健虹膜分割算法。*Sensors*
    21, 4 (2021).
- en: 'Liang et al. (2021) T. Liang, J. Glossner, L. Wang, S. Shi, and X. Zhang. 2021.
    Pruning and quantization for deep neural network acceleration: A survey. *Neurocomputing*
    461 (2021), 370–403.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2021) T. Liang, J. Glossner, L. Wang, S. Shi, 和 X. Zhang. 2021.
    深度神经网络加速的剪枝与量化：综述。*Neurocomputing* 461 (2021), 370–403.
- en: 'Lin et al. (2016) G. Lin, A. Milan, C. Shen, and I. Reid. 2016. RefineNet:
    Multi-Path Refinement Networks for High-Resolution Semantic Segmentation. arXiv:1611.06612 [cs.CV]'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2016) G. Lin, A. Milan, C. Shen, 和 I. Reid. 2016. RefineNet: 高分辨率语义分割的多路径精细化网络。arXiv:1611.06612
    [cs.CV]'
- en: 'Liu et al. (2016b) N. Liu, M. Zhang, H. Li, Z. Sun, and T. Tan. 2016b. DeepIris:
    Learning pairwise filter bank for heterogeneous iris verification. *Pattern Recognition
    Letters* 82 (2016), 154–161.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2016b）N. Liu, M. Zhang, H. Li, Z. Sun 和 T. Tan. 2016b. DeepIris：学习用于异构虹膜验证的成对滤波器组。*模式识别快报*
    82 (2016), 154–161。
- en: 'Liu et al. (2016a) W. Liu, A. Rabinovich, and A. Berg. 2016a. ParseNet: Looking
    Wider to See Better. In *International Conference on Learning Representations
    (ICLR)*.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2016a）W. Liu, A. Rabinovich 和 A. Berg. 2016a. ParseNet：拓宽视野以更好地观察。发表于
    *国际学习表征会议（ICLR）*。
- en: Liu et al. (2019) X. Liu, Y. Bai, Y. Luo, Z. Yang, and Y. Liu. 2019. Iris recognition
    in visible spectrum based on multi-layer analogous convolution and collaborative
    representation. *Pattern Recognition Letters* 117 (2019), 66–73.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2019）X. Liu, Y. Bai, Y. Luo, Z. Yang 和 Y. Liu. 2019. 基于多层类似卷积和协同表示的可见光谱虹膜识别。*模式识别快报*
    117 (2019), 66–73。
- en: Long et al. (2015) J. Long, E. Shelhamer, and T. Darrell. 2015. Fully convolutional
    networks for semantic segmentation. In *IEEE Int. Conf. on Computer Vision and
    Pattern Recognition (CVPR)*. 3431–3440.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long 等（2015）J. Long, E. Shelhamer 和 T. Darrell. 2015. 用于语义分割的全卷积网络。发表于 *IEEE
    计算机视觉与模式识别国际会议（CVPR）*。3431–3440。
- en: Lozej et al. (2018) J. Lozej, B. Meden, V. Struc, and P. Peer. 2018. End-to-End
    Iris Segmentation Using U-Net. In *IEEE International Work Conference on Bioinspired
    Intelligence (IWOBI)*. 1–6.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lozej 等（2018）J. Lozej, B. Meden, V. Struc 和 P. Peer. 2018. 使用 U-Net 的端到端虹膜分割。发表于
    *IEEE 国际生物启发智能工作会议（IWOBI）*。1–6。
- en: Luo et al. (2021) Z. Luo, J. Li, and Y. Zhu. 2021. A Deep Feature Fusion Network
    Based on Multiple Attention Mechanisms for Joint Iris-Periocular Biometric Recognition.
    *IEEE Signal Processing Letters* 28 (2021), 1060–1064.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等（2021）Z. Luo, J. Li 和 Y. Zhu. 2021. 基于多重注意力机制的深度特征融合网络用于联合虹膜-周边生物识别。*IEEE
    信号处理快报* 28 (2021), 1060–1064。
- en: Ma et al. (2003) L. Ma, T. Tan, Y. Wang, and D. Zhang. 2003. Personal identification
    based on iris texture analysis. *IEEE Trans. Pattern Anal. Mach. Intell.* 25,
    12 (2003), 1519–1533.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2003）L. Ma, T. Tan, Y. Wang 和 D. Zhang. 2003. 基于虹膜纹理分析的个人身份识别。*IEEE 计算机学会模式分析与机器智能期刊*
    25, 12 (2003), 1519–1533。
- en: Menotti et al. (2015) D. Menotti, G. Chiachia, A. Pinto, W. R. Schwartz, H.
    Pedrini, A. X. Falcão, and A. Rocha. 2015. Deep Representations for Iris, Face,
    and Fingerprint Spoofing Detection. *IEEE Transactions on Information Forensics
    and Security* 10, 4 (2015).
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menotti 等（2015）D. Menotti, G. Chiachia, A. Pinto, W. R. Schwartz, H. Pedrini,
    A. X. Falcão 和 A. Rocha. 2015. 用于虹膜、面部和指纹伪造检测的深度表示。*IEEE 信息取证与安全期刊* 10, 4 (2015)。
- en: Minaee et al. (2016) S. Minaee, A. Abdolrashidiy, and Y. Wang. 2016. An experimental
    study of deep convolutional features for iris recognition. *IEEE Signal Processing
    in Medicine and Biology Symposium (SPMB)*, 1–6.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee 等（2016）S. Minaee, A. Abdolrashidiy 和 Y. Wang. 2016. 深度卷积特征在虹膜识别中的实验研究。*IEEE
    医学和生物学信号处理研讨会（SPMB）*，1–6。
- en: 'Mishra et al. (2019) S. Mishra, P. Liang, A. Czajka, Danny Z. Chen, and X.
    Hu. 2019. CC-NET: Image Complexity Guided Network Compression for Biomedical Image
    Segmentation. In *IEEE 16th International Symposium on Biomedical Imaging (ISBI
    2019)*. 57–60.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 等（2019）S. Mishra, P. Liang, A. Czajka, Danny Z. Chen 和 X. Hu. 2019. CC-NET：图像复杂度引导的网络压缩用于生物医学图像分割。发表于
    *IEEE 第16届国际生物医学成像研讨会（ISBI 2019）*。57–60。
- en: Monro et al. (2007) D. Monro, S. Rakshit, and D. Zhang. 2007. DCT-Based Iris
    Recognition. *IEEE Trans. Pattern Anal. Mach. Intell.* 29, 4 (2007), 586–595.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Monro 等（2007）D. Monro, S. Rakshit 和 D. Zhang. 2007. 基于 DCT 的虹膜识别。*IEEE 计算机学会模式分析与机器智能期刊*
    29, 4 (2007), 586–595。
- en: 'Moreira et al. (2019) D. Moreira, M. Trokielewicz, A. Czajka, K. Bowyer, and
    P. Flynn. 2019. Performance of Humans in Iris Recognition: The Impact of Iris
    Condition and Annotation-Driven Verification. In *IEEE Winter Conference on Applications
    of Computer Vision (WACV)*. 941–949.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moreira 等（2019）D. Moreira, M. Trokielewicz, A. Czajka, K. Bowyer 和 P. Flynn.
    2019. 人类虹膜识别的表现：虹膜状态和注释驱动的验证影响。发表于 *IEEE 冬季计算机视觉应用会议（WACV）*。941–949。
- en: Mostofa et al. (2021) M. Mostofa, S. Mohamadi, J. Dawson, and N. Nasrabadi.
    2021. Deep GAN-Based Cross-Spectral Cross-Resolution Iris Recognition. *IEEE Transactions
    on Biometrics, Behavior, and Identity Science* 3, 4 (2021), 443–463.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mostofa 等（2021）M. Mostofa, S. Mohamadi, J. Dawson 和 N. Nasrabadi. 2021. 基于深度
    GAN 的跨光谱跨分辨率虹膜识别。*IEEE 生物识别、行为和身份科学期刊* 3, 4 (2021), 443–463。
- en: Muron and Pospisil (2000) A. Muron and J. Pospisil. 2000. The human iris structure
    and its usages. In *Acta Univ Plalcki Physica*. Vol. 39\. Acta Universitatis,
    87–95.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muron 和 Pospisil（2000）A. Muron 和 J. Pospisil. 2000. 人类虹膜结构及其应用。发表于 *Acta Univ
    Plalcki 物理学期刊*。第39卷。Acta Universitatis，87–95。
- en: 'Nguyen et al. (2017a) K. Nguyen, C. Fookes, R. Jillela, S. Sridharan, and A.
    Ross. 2017a. Long range iris recognition: A survey. *Pattern Recognition* 72 (2017),
    123–143.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. (2017a) K. Nguyen, C. Fookes, R. Jillela, S. Sridharan, 和 A. Ross.
    2017a. 长距离虹膜识别：综述。*模式识别* 72 (2017), 123–143。
- en: 'Nguyen et al. (2017b) K. Nguyen, C. Fookes, A. Ross, and S. Sridharan. 2017b.
    Iris recognition with Off-the-Shelf CNN Features: A deep learning perspective.
    *IEEE Access* 6 (2017), 18848–18855. Invited Paper.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. (2017b) K. Nguyen, C. Fookes, A. Ross, 和 S. Sridharan. 2017b.
    使用现成的 CNN 特征进行虹膜识别：深度学习视角。*IEEE Access* 6 (2017), 18848–18855。邀请论文。
- en: Nguyen et al. (2020) K. Nguyen, C. Fookes, and S. Sridharan. 2020. Constrained
    Design of Deep Iris Networks. *IEEE Transactions on Image Processing* 29 (2020),
    7166–7175.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. (2020) K. Nguyen, C. Fookes, 和 S. Sridharan. 2020. 深度虹膜网络的约束设计。*IEEE
    图像处理汇刊* 29 (2020), 7166–7175。
- en: Nguyen et al. (2011) K. Nguyen, C. Fookes, S. Sridharan, and S. Denman. 2011.
    Quality-Driven Super-Resolution for Less Constrained Iris Recognition at a Distance
    and on the Move. *IEEE Transactions on Information Forensics and Security* 6,
    4 (2011).
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. (2011) K. Nguyen, C. Fookes, S. Sridharan, 和 S. Denman. 2011.
    基于质量驱动的超分辨率技术在远距离和动态虹膜识别中的应用。*IEEE 信息取证与安全汇刊* 6, 4 (2011)。
- en: Nguyen et al. (2022) K. Nguyen, C. Fookes, S. Sridharan, and A. Ross. 2022.
    Complex-valued Iris Recognition Network. *IEEE Trans. Pattern Anal. Mach. Intell.*
    (2022).
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. (2022) K. Nguyen, C. Fookes, S. Sridharan, 和 A. Ross. 2022. 复数值虹膜识别网络。*IEEE
    模式分析与机器智能汇刊* (2022)。
- en: 'Nguyen et al. (2018) K. Nguyen, C. Fookes, S. Sridharan, M. Tistarelli, and
    M. Nixon. 2018. Super-resolution for biometrics: A comprehensive survey. *Pattern
    Recognition* 78 (2018), 23–42.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. (2018) K. Nguyen, C. Fookes, S. Sridharan, M. Tistarelli, 和 M.
    Nixon. 2018. 生物特征识别的超分辨率：全面综述。*模式识别* 78 (2018), 23–42。
- en: Nguyen et al. (2012) K. Nguyen, S. Sridharan, S. Denman, and C. Fookes. 2012.
    Feature-domain super-resolution framework for Gabor-based face and iris recognition.
    In *IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)*. 2642–2649.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. (2012) K. Nguyen, S. Sridharan, S. Denman, 和 C. Fookes. 2012.
    基于 Gabor 的人脸和虹膜识别的特征域超分辨率框架。发表于 *IEEE 计算机视觉与模式识别国际会议 (CVPR)*. 2642–2649。
- en: Nie et al. (2014) L. Nie, A. Kumar, and S. Zhan. 2014. Periocular Recognition
    Using Unsupervised Convolutional RBM Feature Learning. In *Int. Conf. on Pattern
    Recognition (ICPR)*. 399–404.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie et al. (2014) L. Nie, A. Kumar, 和 S. Zhan. 2014. 使用无监督卷积 RBM 特征学习的眼部识别。发表于
    *模式识别国际会议 (ICPR)*. 399–404。
- en: 'Nigam et al. (2015) I. Nigam, M. Vatsa, and R. Singh. 2015. Ocular biometrics:
    A survey of modalities and fusion approaches. *Information Fusion* 26 (2015),
    1–35.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nigam et al. (2015) I. Nigam, M. Vatsa, 和 R. Singh. 2015. 眼部生物特征识别：模态和融合方法的综述。*信息融合*
    26 (2015), 1–35。
- en: 'NIST (2021) NIST. 2021. IEG: Iris Examiner Training Discussion: https://www.nist.gov/itl/iad/image-group/ieg-iris-examiner-training-discussion.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'NIST (2021) NIST. 2021. IEG: 虹膜检查员培训讨论: [https://www.nist.gov/itl/iad/image-group/ieg-iris-examiner-training-discussion](https://www.nist.gov/itl/iad/image-group/ieg-iris-examiner-training-discussion)。'
- en: Odinokikh et al. (2019) G. Odinokikh, M. Korobkin, I. Solomatin, I. Efimov,
    and A. Fartukov. 2019. Iris Feature Extraction and Matching Method for Mobile
    Biometric Applications. In *IEEE Int. Conf. on Biometrics (ICB)*. 1–6.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Odinokikh et al. (2019) G. Odinokikh, M. Korobkin, I. Solomatin, I. Efimov,
    和 A. Fartukov. 2019. 移动生物识别应用中的虹膜特征提取与匹配方法。发表于 *IEEE 生物识别国际会议 (ICB)*. 1–6。
- en: Omelina et al. (2021) L. Omelina, J. Goga, J. Pavlovicova, M. Oravec, and B.
    Jansen. 2021. A survey of iris datasets. *Image and Vision Computing* 108 (2021),
    104109.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Omelina et al. (2021) L. Omelina, J. Goga, J. Pavlovicova, M. Oravec, 和 B. Jansen.
    2021. 虹膜数据集综述。*图像与视觉计算* 108 (2021), 104109。
- en: Ortega et al. (2010) J. Ortega, J. Fierrez, F. Alonso, J. Galbally, M. R. Freire,
    J. Gonzalez, C. Garcia, J. Alba, E. Gonzalez-Agulla, E. Otero, S. Garcia-Salicetti,
    L. Allano, B. Ly-Van, B. Dorizzi, J. Kittler, T. Bourlai, N. Poh, F. Deravi, Ming
    N. R. Ng, M. Fairhurst, J. Hennebert, A. Humm, M. Tistarelli, L. Brodo, J. Richiardi,
    A. Drygajlo, H. Ganster, F. M. Sukno, S. Pavani, A. Frangi, L. Akarun, and A.
    Savran. 2010. The Multiscenario Multienvironment BioSecure Multimodal Database
    (BMDB). *IEEE Trans. Pattern Anal. Mach. Intell.* 32, 6 (2010), 1097–1111.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ortega et al. (2010) J. Ortega, J. Fierrez, F. Alonso, J. Galbally, M. R. Freire,
    J. Gonzalez, C. Garcia, J. Alba, E. Gonzalez-Agulla, E. Otero, S. Garcia-Salicetti,
    L. Allano, B. Ly-Van, B. Dorizzi, J. Kittler, T. Bourlai, N. Poh, F. Deravi, Ming
    N. R. Ng, M. Fairhurst, J. Hennebert, A. Humm, M. Tistarelli, L. Brodo, J. Richiardi,
    A. Drygajlo, H. Ganster, F. M. Sukno, S. Pavani, A. Frangi, L. Akarun, 和 A. Savran.
    2010. 多场景多环境 BioSecure 多模态数据库 (BMDB)。*IEEE 模式分析与机器智能汇刊* 32, 6 (2010), 1097–1111。
- en: Park et al. (2011) U. Park, R. Jillela, A. Ross, and A. Jain. 2011. Periocular
    Biometrics in the Visible Spectrum. *IEEE Transactions on Information Forensics
    and Security* 6, 1 (2011), 96–106.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 (2011) U. Park, R. Jillela, A. Ross 和 A. Jain. 2011. 可见光谱中的周期眼生物特征识别。*IEEE
    信息取证与安全学报* 6, 1 (2011), 96–106。
- en: Parzianello and Czajka (2022) L. Parzianello and A. Czajka. 2022. Saliency-Guided
    Textured Contact Lens-Aware Iris Recognition. In *IEEE Workshop on Applications
    of Computer Vision (WACV)*. 330–337.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parzianello 和 Czajka (2022) L. Parzianello 和 A. Czajka. 2022. 关注度引导的纹理隐形眼镜感知虹膜识别。在
    *IEEE 计算机视觉应用研讨会 (WACV)*。330–337。
- en: Peng et al. (2020) H. Peng, B. Li, D. He, and J. Wang. 2020. End-to-End Anti-Attack
    Iris Location Based on Lightweight Network. In *IEEE International Conference
    on Advances in Electrical Engineering and Computer Applications (AEECA)*. 821–827.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等 (2020) H. Peng, B. Li, D. He 和 J. Wang. 2020. 基于轻量级网络的端到端抗攻击虹膜定位。在 *IEEE
    电气工程与计算机应用进展国际会议 (AEECA)*。821–827。
- en: Phillips et al. (2010) P. J. Phillips, W. T. Scruggs, A. J. O’Toole, P. Flynn,
    K. Bowyer, C. L. Schott, and M. Sharpe. 2010. FRVT 2006 and ICE 2006 Large-Scale
    Experimental Results. *IEEE Trans. Pattern Anal. Mach. Intell.* 32, 5 (2010),
    831–846.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phillips 等 (2010) P. J. Phillips, W. T. Scruggs, A. J. O’Toole, P. Flynn, K.
    Bowyer, C. L. Schott 和 M. Sharpe. 2010. FRVT 2006 和 ICE 2006 大规模实验结果。*IEEE 计算机视觉与模式识别学报*
    32, 5 (2010), 831–846。
- en: Planet Biometrics (2017) Planet Biometrics. 2017. Homeland Advanced Recognition
    Technology (HART). [http://www.planetbiometrics.com/article-details/i/5598/desc/dhs-launches-rfp-for-hart/](http://www.planetbiometrics.com/article-details/i/5598/desc/dhs-launches-rfp-for-hart/)
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Planet Biometrics (2017) Planet Biometrics. 2017. Homeland Advanced Recognition
    Technology (HART). [http://www.planetbiometrics.com/article-details/i/5598/desc/dhs-launches-rfp-for-hart/](http://www.planetbiometrics.com/article-details/i/5598/desc/dhs-launches-rfp-for-hart/)
- en: Proença (2013) H. Proença. 2013. *Iris Recognition in the Visible Wavelength*.
    Springer, UK, 151–169.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Proença (2013) H. Proença. 2013. *可见波长中的虹膜识别*。Springer, UK, 151–169。
- en: 'Proença and Alexandre (2005) H. Proença and L. Alexandre. 2005. UBIRIS: A Noisy
    Iris Image Database. In *Image Analysis and Processing – ICIAP 2005*. Springer,
    Germany, 970–977.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Proença 和 Alexandre (2005) H. Proença 和 L. Alexandre. 2005. UBIRIS: 一个嘈杂的虹膜图像数据库。在
    *图像分析与处理 – ICIAP 2005*。Springer, Germany, 970–977。'
- en: 'Proenca et al. (2010) H. Proenca, S. Filipe, R. Santos, J. Oliveira, and L.
    Alexandre. 2010. The UBIRIS.v2: A Database of Visible Wavelength Iris Images Captured
    On-the-Move and At-a-Distance. *IEEE Trans. Pattern Anal. Mach. Intell.* 32, 8
    (2010), 1529–1535.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Proenca 等 (2010) H. Proenca, S. Filipe, R. Santos, J. Oliveira 和 L. Alexandre.
    2010. UBIRIS.v2: 一个在运动中和远距离捕捉的可见波长虹膜图像数据库。*IEEE 计算机视觉与模式识别学报* 32, 8 (2010), 1529–1535。'
- en: 'Proença and Neves (2017) H. Proença and J. C. Neves. 2017. IRINA: Iris Recognition
    (Even) in Inaccurately Segmented Data. In *IEEE Int. Conf. on Computer Vision
    and Pattern Recognition (CVPR)*. 6747–6756.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Proença 和 Neves (2017) H. Proença 和 J. C. Neves. 2017. IRINA: 在不准确分割数据中的虹膜识别。在
    *IEEE 国际计算机视觉与模式识别会议 (CVPR)*。6747–6756。'
- en: 'Proença (2010) H. Proença. 2010. Iris Recognition: On the Segmentation of Degraded
    Images Acquired in the Visible Wavelength. *IEEE Trans. Pattern Anal. Mach. Intell.*
    32, 8 (2010), 1502–1516.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Proença (2010) H. Proença. 2010. 虹膜识别：关于在可见波长中获取的退化图像的分割。*IEEE 计算机视觉与模式识别学报*
    32, 8 (2010), 1502–1516。
- en: 'Proença and Neves (2018) H. Proença and J. Neves. 2018. Deep-PRWIS: Periocular
    Recognition Without the Iris and Sclera Using Deep Learning Frameworks. *IEEE
    Transactions on Information Forensics and Security* 13, 4 (2018), 888–896.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Proença 和 Neves (2018) H. Proença 和 J. Neves. 2018. Deep-PRWIS: 利用深度学习框架进行的周期眼识别，无需虹膜和巩膜。*IEEE
    信息取证与安全学报* 13, 4 (2018), 888–896。'
- en: Proença and Neves (2019) H. Proença and J. C. Neves. 2019. Segmentation-Less
    and Non-Holistic Deep-Learning Frameworks for Iris Recognition. In *IEEE Int.
    Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)*. 2296–2305.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Proença 和 Neves (2019) H. Proença 和 J. C. Neves. 2019. 无需分割和非整体深度学习框架用于虹膜识别。在
    *IEEE 国际计算机视觉与模式识别研讨会 (CVPRW)*。2296–2305。
- en: Quinn, G. and Matey, J. and Grother, P. and Watters, E. (2022) Quinn, G. and
    Matey, J. and Grother, P. and Watters, E. 2022. Statistics of Visual Features
    in the Human Iris.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quinn, G., Matey, J., Grother, P. 和 Watters, E. (2022) Quinn, G., Matey, J.,
    Grother, P. 和 Watters, E. 2022. 人眼虹膜视觉特征的统计数据。
- en: 'Ribeiro et al. (2019) E. Ribeiro, A. Uhl, and F. Alonso-Fernandez. 2019. Iris
    super-resolution using CNNs: is photo-realism important to iris recognition? *IET
    Biometrics* 8 (2019), 69–78.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro 等 (2019) E. Ribeiro, A. Uhl 和 F. Alonso-Fernandez. 2019. 使用 CNN 的虹膜超分辨率：照片现实主义对虹膜识别是否重要？*IET
    生物特征识别* 8 (2019), 69–78。
- en: Ribeiro et al. (2017) E. Ribeiro, A. Uhl, F. Alonso-Fernandez, and R. A. Farrugia.
    2017. Exploring deep learning image super-resolution for iris recognition. In
    *25th European Signal Processing Conference (EUSIPCO)*. 2176–2180.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro 等 (2017) E. Ribeiro, A. Uhl, F. Alonso-Fernandez 和 R. A. Farrugia. 2017.
    探索深度学习图像超分辨率用于虹膜识别。见 *第25届欧洲信号处理会议 (EUSIPCO)*。2176–2180。
- en: 'Ronneberger et al. (2015) O. Ronneberger, P.Fischer, and T. Brox. 2015. U-Net:
    Convolutional Networks for Biomedical Image Segmentation. In *Medical Image Computing
    and Computer-Assisted Intervention (MICCAI)* *(LNCS, Vol. 9351)*. Springer, 234–241.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ronneberger 等 (2015) O. Ronneberger, P. Fischer 和 T. Brox. 2015. U-Net: 用于生物医学图像分割的卷积网络。见
    *医学图像计算与计算机辅助干预 (MICCAI)* *(LNCS, 第 9351 卷)*。Springer, 234–241。'
- en: 'Saleh (2018) I. Saleh. 2018. *Internet of Things (IoT): Concepts, Issues, Challenges
    and Perspectives*. 1–26.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Saleh (2018) I. Saleh. 2018. *物联网 (IoT): 概念、问题、挑战和展望*。1–26。'
- en: Sansola (2015) A. Sansola. 2015. *Postmortem iris recognition and its application
    in human identification*. Master’s thesis. Boston University, USA.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sansola (2015) A. Sansola. 2015. *尸检虹膜识别及其在人类识别中的应用*。硕士论文。波士顿大学，美国。
- en: Sardar et al. (2020) M. Sardar, S. Banerjee, and S. Mitra. 2020. Iris Segmentation
    Using Interactive Deep Learning. *IEEE Access* 8 (2020), 219322–219330.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sardar 等 (2020) M. Sardar, S. Banerjee 和 S. Mitra. 2020. 使用交互式深度学习进行虹膜分割。*IEEE
    Access* 8 (2020), 219322–219330。
- en: Sauerwein et al. (2017) K. Sauerwein, T. B. Saul, D. W. Steadman, and C. B.
    Boehnen. 2017. The Effect of Decomposition on the Efficacy of Biometrics for Positive
    Identification. *Journal of Forensic Sciences* 62, 6 (2017), 1599–1602.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sauerwein 等 (2017) K. Sauerwein, T. B. Saul, D. W. Steadman 和 C. B. Boehnen.
    2017. 分解对生物识别技术正识别有效性的影响。*法医学杂志* 62, 6 (2017), 1599–1602。
- en: Schlett et al. (2018) T. Schlett, C. Rathgeb, and C. Busch. 2018. Multi-spectral
    Iris Segmentation in Visible Wavelengths. In *IEEE Int. Conf. on Biometrics (ICB)*.
    190–194.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schlett 等 (2018) T. Schlett, C. Rathgeb 和 C. Busch. 2018. 可见波长中的多光谱虹膜分割。见 *IEEE
    国际生物识别会议 (ICB)*。190–194。
- en: 'Schroff et al. (2015) F. Schroff, D. Kalenichenko, and J. Philbin. 2015. FaceNet:
    A unified embedding for face recognition and clustering. In *IEEE Int. Conf. on
    Computer Vision and Pattern Recognition (CVPR)*. 815–823.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schroff 等 (2015) F. Schroff, D. Kalenichenko 和 J. Philbin. 2015. FaceNet: 面部识别和聚类的统一嵌入。见
    *IEEE 计算机视觉与模式识别会议 (CVPR)*。815–823。'
- en: Sequeira et al. (2014) A. Sequeira, H. Oliveira, J. Monteiro, J. Monteiro, and
    J. Cardoso. 2014. MobILive 2014 - Mobile Iris Liveness Detection Competition.
    In *IEEE Int. Joint Conf. on Biometrics (IJCB)*. 1–6.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sequeira 等 (2014) A. Sequeira, H. Oliveira, J. Monteiro, J. Monteiro 和 J. Cardoso.
    2014. MobILive 2014 - 移动虹膜活性检测竞赛。见 *IEEE 国际联合生物识别会议 (IJCB)*。1–6。
- en: Shah and Ross (2009) S. Shah and A. Ross. 2009. Iris Segmentation Using Geodesic
    Active Contours. *IEEE Transactions on Information Forensics and Security* 4,
    4 (2009), 824–836.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shah 和 Ross (2009) S. Shah 和 A. Ross. 2009. 使用测地线活动轮廓进行虹膜分割。*IEEE 信息取证与安全学报*
    4, 4 (2009), 824–836。
- en: Sharma et al. (2014) A. Sharma, S. Verma, M. Vatsa, and R. Singh. 2014. On cross
    spectral periocular recognition. In *Int. IEEE Int. Conf. on Image Processing*.
    5007–5011.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等 (2014) A. Sharma, S. Verma, M. Vatsa 和 R. Singh. 2014. 跨光谱眼周识别研究。见
    *国际 IEEE 图像处理会议*。5007–5011。
- en: Sharma and Selwal (2021) D. Sharma and A. Selwal. 2021. On Data-Driven Approaches
    for Presentation Attack Detection in Iris Recognition Systems. In *Recent Innovations
    in Computing*. Springer, Singapore, 463–473.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 和 Selwal (2021) D. Sharma 和 A. Selwal. 2021. 关于数据驱动的方法在虹膜识别系统中检测呈现攻击。见
    *计算机领域的最新创新*。Springer, 新加坡, 463–473。
- en: 'Sharma and Ross (2020) R. Sharma and A. Ross. 2020. D-NetPAD: An Explainable
    and Interpretable Iris Presentation Attack Detector. In *IEEE Int. Joint Conf.
    on Biometrics (IJCB)*. 1–10.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sharma 和 Ross (2020) R. Sharma 和 A. Ross. 2020. D-NetPAD: 一种可解释和可解释的虹膜呈现攻击检测器。见
    *IEEE 国际联合生物识别会议 (IJCB)*. 1–10。'
- en: Sharma and Ross (2021) R. Sharma and A. Ross. 2021. Viability of Optical Coherence
    Tomography for Iris Presentation Attack Detection. In *Int. Conf. on Pattern Recognition
    (ICPR)*. 6165–6172.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 和 Ross (2021) R. Sharma 和 A. Ross. 2021. 光学相干断层扫描在虹膜呈现攻击检测中的可行性。见 *国际模式识别会议
    (ICPR)*。6165–6172。
- en: Shen and Flynn (2013) F. Shen and P. Flynn. 2013. Using crypts as iris minutiae.
    In *Biometric and Surveillance Technology for Human and Activity Identification
    X*, Vol. 8712. 87120B.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 和 Flynn (2013) F. Shen 和 P. Flynn. 2013. 使用隐窝作为虹膜特征点。见 *生物识别与监控技术用于人类与活动识别
    X*, 第 8712 卷。87120B。
- en: Song et al. (2019) L. Song, R. Shokri, and P. Mittal. 2019. Membership Inference
    Attacks Against Adversarially Robust Deep Learning Models. In *IEEE Security and
    Privacy Workshops (SPW)*. 50–56.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2019) L. Song, R. Shokri, and P. Mittal. 2019. 对抗性鲁棒深度学习模型的成员推断攻击。发表于*IEEE安全与隐私研讨会（SPW）*，50–56。
- en: 'Stark et al. (2010) L. Stark, K. Bowyer, and S. Siena. 2010. Human perceptual
    categorization of iris texture patterns. In *IEEE Int. Conf. on Biometrics: Theory
    Applications and Systems (BTAS)*. 1–7.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stark et al. (2010) L. Stark, K. Bowyer, and S. Siena. 2010. 人类对虹膜纹理模式的感知分类。发表于*IEEE国际生物特征识别：理论应用与系统会议（BTAS）*，1–7。
- en: Sun et al. (2014) Z. Sun, H. Zhang, T. Tan, and J. Wang. 2014. Iris Image Classification
    Based on Hierarchical Visual Codebook. *IEEE Trans. Pattern Anal. Mach. Intell.*
    36, 6 (2014), 1120–1133.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2014) Z. Sun, H. Zhang, T. Tan, and J. Wang. 2014. 基于层次视觉词汇的虹膜图像分类。*IEEE模式分析与机器智能汇刊*
    36, 6 (2014)，1120–1133。
- en: Sunder and Ross (2010) M. S. Sunder and A. Ross. 2010. Iris Image Retrieval
    Based on Macro-features. In *Int. Conf. on Pattern Recognition (ICPR)*. 1318–1321.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sunder and Ross (2010) M. S. Sunder and A. Ross. 2010. 基于宏特征的虹膜图像检索。发表于*国际模式识别大会（ICPR）*，1318–1321。
- en: Tabassi et al. (2011) E. Tabassi, P. Grother, and W. Salamon. 2011. IREX II
    - IQCE - Iris Quality Calibration and Evaluation. Performance of Iris Image Quality
    Assessment Algorithms. *NISTIR 7296 - http://iris.nist.gov/irex/* (2011).
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tabassi et al. (2011) E. Tabassi, P. Grother, and W. Salamon. 2011. IREX II
    - IQCE - 虹膜质量校准与评估。虹膜图像质量评估算法的性能。*NISTIR 7296 - http://iris.nist.gov/irex/* (2011)。
- en: 'Talreja et al. (2022) V. Talreja, N. M. Nasrabadi, and M. C. Valenti. 2022.
    Attribute-Based Deep Periocular Recognition: Leveraging Soft Biometrics to Improve
    Periocular Recognition. In *IEEE Winter Conference on Applications of Computer
    Vision (WACV)*.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talreja et al. (2022) V. Talreja, N. M. Nasrabadi, and M. C. Valenti. 2022.
    基于属性的深度眼周识别：利用软生物特征提升眼周识别。发表于*IEEE冬季计算机视觉应用会议（WACV）*。
- en: Tan and Kumar (2012) C.-W. Tan and A. Kumar. 2012. Unified Framework for Automated
    Iris Segmentation Using Distantly Acquired Face Images. *IEEE Transactions on
    Image Processing* 21, 9 (2012), 4068–4079.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan and Kumar (2012) C.-W. Tan and A. Kumar. 2012. 基于远程获取的人脸图像的自动虹膜分割统一框架。*IEEE图像处理汇刊*
    21, 9 (2012)，4068–4079。
- en: Tan et al. (2010) T. Tan, Zh. He, and Z. Sun. 2010. Efficient and robust segmentation
    of noisy iris images for non-cooperative iris recognition. *Image and Vision Computing*
    28, 2 (2010), 223–230.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan et al. (2010) T. Tan, Zh. He, and Z. Sun. 2010. 高效且鲁棒的噪声虹膜图像分割用于非合作虹膜识别。*图像与视觉计算*
    28, 2 (2010)，223–230。
- en: Tann et al. (2019) H. Tann, H. Zhao, and S. Reda. 2019. A Resource-Efficient
    Embedded Iris Recognition System Using Fully Convolutional Networks. *ACM Journal
    on Emerging Technologies in Computing Systems* 16, 1 (2019), 1–23.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tann et al. (2019) H. Tann, H. Zhao, and S. Reda. 2019. 使用全卷积网络的资源高效嵌入式虹膜识别系统。*ACM计算系统新兴技术期刊*
    16, 1 (2019)，1–23。
- en: Tapia et al. (2022) J. Tapia, S. Gonzalez, and C. Busch. 2022. Iris Liveness
    Detection Using a Cascade of Dedicated Deep Learning Networks. *IEEE Transactions
    on Information Forensics and Security* 17 (2022), 42–52.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tapia et al. (2022) J. Tapia, S. Gonzalez, and C. Busch. 2022. 使用专用深度学习网络级联的虹膜活体检测。*IEEE信息取证与安全汇刊*
    17 (2022)，42–52。
- en: technology — Biometric presentation attack detection — Part 1: Framework (2016)
    ISO/IEC 30107-1. Information technology — Biometric presentation attack detection
    — Part 1: Framework. 2016.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'technology — Biometric presentation attack detection — Part 1: Framework (2016)
    ISO/IEC 30107-1. 信息技术 — 生物特征呈现攻击检测 — 第1部分：框架。2016年。'
- en: 'technology — Biometric sample quality — Part 6: Iris image data (2015) ISO/IEC
    29794-6. Information technology — Biometric sample quality — Part 6: Iris image
    data. 2015.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'technology — Biometric sample quality — Part 6: Iris image data (2015) ISO/IEC
    29794-6. 信息技术 — 生物特征样本质量 — 第6部分：虹膜图像数据。2015年。'
- en: Tinsley et al. (2021) P. Tinsley, A. Czajka, and P. Flynn. 2021. This Face Does
    Not Exist… But It Might Be Yours! Identity Leakage in Generative Models. In *IEEE
    Winter Conference on Applications of Computer Vision (WACV)*. 1319–1327.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tinsley et al. (2021) P. Tinsley, A. Czajka, and P. Flynn. 2021. 这个面孔不存在……但它可能是你的！生成模型中的身份泄漏。发表于*IEEE冬季计算机视觉应用会议（WACV）*，1319–1327。
- en: Trokielewicz and Czajka (2018) M. Trokielewicz and A. Czajka. 2018. Data-driven
    segmentation of post-mortem iris images. In *International Workshop on Biometrics
    and Forensics (IWBF)*. IEEE, Italy, 1–7.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trokielewicz and Czajka (2018) M. Trokielewicz and A. Czajka. 2018. 基于数据驱动的尸检虹膜图像分割。发表于*国际生物特征识别与取证研讨会（IWBF）*，IEEE，意大利，1–7。
- en: 'Trokielewicz et al. (2016a) M. Trokielewicz, A. Czajka, and P. Maciejewicz.
    2016a. Human iris recognition in post-mortem subjects: Study and database. In
    *IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS)*. IEEE,
    USA, 1–6.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trokielewicz等（2016a）M. Trokielewicz，A. Czajka和P. Maciejewicz。2016a。死后受试者的人类虹膜识别：研究和数据库。在*IEEE生物特征识别国际会议（BTAS）*。IEEE，美国，1-6。
- en: Trokielewicz et al. (2016b) M. Trokielewicz, A. Czajka, and P. Maciejewicz.
    2016b. Post-mortem human iris recognition. In *IEEE Int. Conf. on Biometrics (ICB)*.
    IEEE, Sweden, 1–6.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trokielewicz等（2016b）M. Trokielewicz，A. Czajka和P. Maciejewicz。2016b。死后人类虹膜识别。在*IEEE生物特征识别国际会议（ICB）*。IEEE，瑞典，1-6。
- en: 'Trokielewicz et al. (2018) M. Trokielewicz, A. Czajka, and P. Maciejewicz.
    2018. Presentation Attack Detection for Cadaver Iris. In *IEEE Int. Conf. on Biometrics:
    Theory Applications and Systems (BTAS)*. 1–10.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trokielewicz等（2018）M. Trokielewicz，A. Czajka和P. Maciejewicz。2018。尸体虹膜的演示攻击检测。在*IEEE生物特征识别国际会议（BTAS）*。1-10。
- en: Trokielewicz et al. (2019) M. Trokielewicz, A. Czajka, and P. Maciejewicz. 2019.
    Iris Recognition After Death. *IEEE Transactions on Information Forensics and
    Security* 14, 6 (June 2019), 1501–1514.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trokielewicz等（2019）M. Trokielewicz，A. Czajka和P. Maciejewicz。2019。死后虹膜识别。*IEEE信息取证与安全杂志*
    14，6（2019年6月），1501-1514。
- en: Trokielewicz et al. (2020) M. Trokielewicz, A. Czajka, and P. Maciejewicz. 2020.
    Post-Mortem Iris Recognition Resistant to Biological Eye Decay Processes. In *IEEE
    Winter Conference on Applications of Computer Vision (WACV)*. IEEE, USA, 1–8.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trokielewicz等（2020）M. Trokielewicz，A. Czajka和P. Maciejewicz。2020。抵抗生物眼睛腐烂过程的死后虹膜识别。在*IEEE冬季计算机视觉应用会议（WACV）*。IEEE，美国，1-8。
- en: Trokielewicz et al. (2020) M. Trokielewicz, A. Czajka, and P. Maciejewicz. 2020.
    Post-mortem iris recognition with deep-learning-based image segmentation. *Image
    and Vision Computing* 94 (2020), 103866.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trokielewicz等（2020）M. Trokielewicz，A. Czajka和P. Maciejewicz。2020。基于深度学习的图像分割的死后虹膜识别。*图像与视觉计算*
    94（2020），103866。
- en: Tygert et al. (2016) M. Tygert, J. Bruna, S. Chintala, Y. LeCun, S. Piantino,
    and A. Szlam. 2016. A Mathematical Motivation for Complex-valued Convolutional
    Networks. *Neural Computation* 28 (2016), 815–825.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tygert等（2016）M. Tygert，J. Bruna，S. Chintala，Y. LeCun，S. Piantino和A. Szlam。2016。复值卷积网络的数学动机。*神经计算*
    28（2016），815-825。
- en: 'Unique Identification Authority of India (2021) Unique Identification Authority
    of India. 2021. AADHAAR: http://uidai.gov.in.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 印度唯一身份认证机构（2021）印度唯一身份认证机构。2021。Aadhaar：http://uidai.gov.in。
- en: Vatsa et al. (2008) M. Vatsa, R. Singh, and A. Noore. 2008. Improving Iris Recognition
    Performance Using Segmentation, Quality Enhancement, Match Score Fusion, and Indexing.
    *IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)* 38,
    4 (2008), 1021–1035.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vatsa等（2008）M. Vatsa，R. 辛格和A. 诺尔。2008。利用分割、质量增强、匹配分数融合和索引来提高虹膜识别性能。*IEEE系统、人和网络（网络）杂志*
    38，4（2008），1021-1035。
- en: Wang et al. (2020a) C. Wang, J. Muhammad, Y. Wang, Z. He, and Z. Sun. 2020a.
    Towards Complete and Accurate Iris Segmentation Using Deep Multi-Task Attention
    Network for Non-Cooperative Iris Recognition. *IEEE Transactions on Information
    Forensics and Security* 15 (2020), 2944–2959.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2020a）C. 王，J. 穆罕默德，Y. 王，Z. 吴和Z. 孙。2020a。利用深度多任务注意力网络实现完整准确的非合作虹膜识别。*IEEE信息取证与安全杂志*
    15 （2020年），2944-2959。
- en: Wang and Sun (2020) C. Wang and Z. Sun. 2020. A Benchmark for Iris Segmentation.
    *Journal of Computer Research and Development* 57, 2 (2020), 395–412.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王和孙（2020）C. 王和Z. 孙。2020。虹膜分割基准。*计算机研究与发展杂志* 57，2（2020），395-412。
- en: Wang et al. (2020b) C. Wang, Y. Wang, B. Xu, Y. He, Z. Dong, and Z. Sun. 2020b.
    A Lightweight Multi-Label Segmentation Network for Mobile Iris Biometrics. In
    *IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP)*. 1006–1010.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2020b）C. 王，Y. 王，B. 许，Y. 何，Z. 董和Z. 孙。2020b。移动虹膜生物识别轻量级多标签分割网络。在*IEEE声学、语音和信号处理国际会议（ICASSP）*。1006-1010。
- en: Wang et al. (2019b) C. Wang, Y. Zhu, Y. Liu, R. He, and Z. Sun. 2019b. Joint
    Iris Segmentation and Localization Using Deep Multi-task Learning Framework. arXiv:1901.11195 [cs.CV]
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2019b）C. 王，Y. 朱，Y. 刘，R. 何和Z. 孙。2019b。利用深度多任务学习框架进行联合虹膜分割和定位。arXiv:1901.11195
    [cs.CV]
- en: Wang and Kumar (2019) K. Wang and A. Kumar. 2019. Toward More Accurate Iris
    Recognition Using Dilated Residual Features. *IEEE Transactions on Information
    Forensics and Security* 14, 12 (2019), 3233–3245.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王和库马尔（2019）K. 王和A. 库马尔。2019。利用扩张残差特征实现更准确的虹膜识别。*IEEE信息取证与安全杂志* 14，12（2019年），3233-3245。
- en: Wang and Kumar (2021) K. Wang and A. Kumar. 2021. Periocular-Assisted Multi-Feature
    Collaboration for Dynamic Iris Recognition. *IEEE Transactions on Information
    Forensics and Security* 16 (2021), 866–879.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王和库马尔（2021）K. Wang 和 A. Kumar。2021。**周期性辅助的多特征协作用于动态虹膜识别**。*IEEE 信息取证与安全学报*
    16（2021），866–879。
- en: Wang et al. (2020c) L. Wang, K. Zhang, M. Ren, Y. Wang, and Z. Sun. 2020c. Recognition
    Oriented Iris Image Quality Assessment in the Feature Space. In *IEEE Int. Joint
    Conf. on Biometrics (IJCB)*. 1–9.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2020c）L. Wang、K. Zhang、M. Ren、Y. Wang 和 Z. Sun。2020c。**在特征空间中的识别导向虹膜图像质量评估**。在
    *IEEE 国际生物识别联合会议 (IJCB)*。1–9。
- en: Wang et al. (2019a) X. Wang, H. Zhang, J. Liu, L. Xiao, Z. He, L. Liu, and P.
    Duan. 2019a. Iris Image Super Resolution Based on GANs with Adversarial Triplets.
    In *Chinese Conference on Biometric Recognition (LNCS)*. Switzerland, 346 – 53.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2019a）X. Wang、H. Zhang、J. Liu、L. Xiao、Z. He、L. Liu 和 P. Duan。2019a。**基于 GAN
    的对抗三元组虹膜图像超分辨率**。在 *中国生物识别技术会议 (LNCS)*。瑞士，346–353。
- en: Wang et al. (2021) Z. Wang, J. Chai, and S. Xia. 2021. Realtime and Accurate
    3D Eye Gaze Capture with DCNN-Based Iris and Pupil Segmentation. *IEEE Transactions
    on Visualization and Computer Graphics* 27, 1 (2021), 190–203.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2021）Z. Wang、J. Chai 和 S. Xia。2021。**基于 DCNN 的虹膜和瞳孔分割实现实时准确的 3D 眼动捕捉**。*IEEE
    可视化与计算机图形学学报* 27，1（2021），190–203。
- en: Wei et al. (2007) Z. Wei, T. Tan, and Z. Sun. 2007. Nonlinear Iris Deformation
    Correction Based on Gaussian Model. In *Advances in Biometrics*. Springer, Germany,
    780–789.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等（2007）Z. Wei、T. Tan 和 Z. Sun。2007。**基于高斯模型的非线性虹膜变形修正**。在 *生物识别技术进展*。Springer，德国，780–789。
- en: Wu and Zhao (2019) X. Wu and L. Zhao. 2019. Study on Iris Segmentation Algorithm
    Based on Dense U-Net. *IEEE Access* 7 (2019), 123959–123968.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴和赵（2019）X. Wu 和 L. Zhao。2019。**基于密集 U-Net 的虹膜分割算法研究**。*IEEE Access* 7（2019），123959–123968。
- en: 'Xiao et al. (2013) L. Xiao, Z. Sun, R. He, and T. Tan. 2013. Coupled feature
    selection for cross-sensor iris recognition. In *IEEE Int. Conf. on Biometrics:
    Theory Applications and Systems (BTAS)*. 1–6.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肖等（2013）L. Xiao、Z. Sun、R. He 和 T. Tan。2013。**跨传感器虹膜识别的耦合特征选择**。在 *IEEE 生物识别理论应用与系统会议
    (BTAS)*。1–6。
- en: Y. and K. (2016) Fisher Y. and Vladlen K. 2016. Multi-Scale Context Aggregation
    by Dilated Convolutions. arXiv:1511.07122 [cs.CV]
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尼和卡（2016）Fisher Y. 和 Vladlen K. 2016。**通过膨胀卷积进行多尺度上下文聚合**。arXiv:1511.07122 [cs.CV]
- en: Yadav et al. (2019b) D. Yadav, N. Kohli, M. Vatsa, R. Singh, and A. Noore. 2019b.
    Detecting Textured Contact Lens in Uncontrolled Environment Using DensePAD. In
    *IEEE Int. Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)*.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 雅达夫等（2019b）D. Yadav、N. Kohli、M. Vatsa、R. Singh 和 A. Noore。2019b。**使用 DensePAD
    检测无控制环境中的纹理隐形眼镜**。在 *IEEE 计算机视觉与模式识别研讨会 (CVPRW)*。
- en: Yadav et al. (2018) D. Yadav, N. Kohli, S. Yadav, M. Vatsa, R. Singh, and A.
    Noore. 2018. Iris Presentation Attack via Textured Contact Lens in Unconstrained
    Environment. In *IEEE Winter Conference on Applications of Computer Vision (WACV)*.
    503–511.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 雅达夫等（2018）D. Yadav、N. Kohli、S. Yadav、M. Vatsa、R. Singh 和 A. Noore。2018。**在无约束环境中通过纹理隐形眼镜进行虹膜呈现攻击**。在
    *IEEE 冬季计算机视觉应用会议 (WACV)*。503–511。
- en: Yadav et al. (2019a) S. Yadav, C. Chen, and A. Ross. 2019a. Synthesizing Iris
    Images Using RaSGAN With Application in Presentation Attack Detection. In *IEEE
    Int. Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)*. 2422–2430.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 雅达夫等（2019a）S. Yadav、C. Chen 和 A. Ross。2019a。**使用 RaSGAN 合成虹膜图像及其在呈现攻击检测中的应用**。在
    *IEEE 计算机视觉与模式识别研讨会 (CVPRW)*。2422–2430。
- en: 'Yadav et al. (2020) S. Yadav, C. Chen, and A. Ross. 2020. Relativistic Discriminator:
    A One-Class Classifier for Generalized Iris Presentation Attack Detection. In
    *IEEE Winter Conference on Applications of Computer Vision (WACV)*. 2624–2633.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 雅达夫等（2020）S. Yadav、C. Chen 和 A. Ross。2020。**相对判别器：一种用于通用虹膜呈现攻击检测的单类分类器**。在 *IEEE
    冬季计算机视觉应用会议 (WACV)*。2624–2633。
- en: 'Yadav and Ross (2021) S. Yadav and A. Ross. 2021. CIT-GAN: Cyclic Image Translation
    Generative Adversarial Network With Application in Iris Presentation Attack Detection.
    In *IEEE Winter Conference on Applications of Computer Vision (WACV)*. 2411–2420.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 雅达夫和罗斯（2021）S. Yadav 和 A. Ross。2021。**CIT-GAN：具有虹膜呈现攻击检测应用的循环图像翻译生成对抗网络**。在
    *IEEE 冬季计算机视觉应用会议 (WACV)*。2411–2420。
- en: Yambay et al. (2017) D. Yambay, B. Becker, N. Kohli, D. Yadav, A. Czajka, K.
    Bowyer, S. Schuckers, R. Singh, M. Vatsa, A. Noore, D. Gragnaniello, C. Sansone,
    L. Verdoliva, L. He, Y. Ru, H. Li, N. Liu, Z. Sun, and T. Tan. 2017. LivDet iris
    2017 — Iris liveness detection competition 2017\. In *IEEE Int. Joint Conf. on
    Biometrics (IJCB)*. 733–741.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yambay 等人（2017）D. Yambay, B. Becker, N. Kohli, D. Yadav, A. Czajka, K. Bowyer,
    S. Schuckers, R. Singh, M. Vatsa, A. Noore, D. Gragnaniello, C. Sansone, L. Verdoliva,
    L. He, Y. Ru, H. Li, N. Liu, Z. Sun, 和 T. Tan. 2017. LivDet iris 2017 — 虹膜活体检测竞赛
    2017。在 *IEEE 国际生物识别联合会议 (IJCB)*。733–741。
- en: Yan et al. (2021) Z. Yan, L. He, Y. Wang, Z. Sun, and T. Tan. 2021. Flexible
    Iris Matching Based on Spatial Feature Reconstruction. *IEEE Transactions on Biometrics,
    Behavior, and Identity Science* (2021), 1–1.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等人（2021）Z. Yan, L. He, Y. Wang, Z. Sun, 和 T. Tan. 2021. 基于空间特征重构的灵活虹膜匹配。*IEEE
    生物识别、行为和身份科学汇刊* (2021), 1–1。
- en: 'Yang et al. (2021) K. Yang, Z. Xu, and J. Fei. 2021. DualSANet: Dual Spatial
    Attention Network for Iris Recognition. In *IEEE Winter Conference on Applications
    of Computer Vision (WACV)*. 888–896.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人（2021）K. Yang, Z. Xu, 和 J. Fei. 2021. DualSANet：用于虹膜识别的双重空间注意力网络。在 *IEEE
    冬季计算机视觉应用会议 (WACV)*。888–896。
- en: 'Yuan et al. (2020) Y. Yuan, W. Chen, Y. Yang, and Z. Wang. 2020. In Defense
    of the Triplet Loss Again: Learning Robust Person Re-Identification with Fast
    Approximated Triplet Loss and Label Distillation. In *IEEE Int. Conf. on Computer
    Vision and Pattern Recognition Workshops (CVPRW)*. 1454–1463.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人（2020）Y. Yuan, W. Chen, Y. Yang, 和 Z. Wang. 2020. 再次捍卫三元组损失：通过快速近似三元组损失和标签蒸馏学习鲁棒的行人重识别。在
    *IEEE 计算机视觉与模式识别会议研讨会 (CVPRW)*。1454–1463。
- en: 'Zanlorensi et al. (2020) L. A. Zanlorensi, H. Proença, and D. Menotti. 2020.
    Unconstrained Periocular Recognition: Using Generative Deep Learning Frameworks
    for Attribute Normalization. In *Int. IEEE Int. Conf. on Image Processing*. 1361–1365.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zanlorensi 等人（2020）L. A. Zanlorensi, H. Proença, 和 D. Menotti. 2020. 无约束的眼周识别：使用生成对抗深度学习框架进行属性归一化。在
    *国际 IEEE 图像处理会议*。1361–1365。
- en: Zhang et al. (2021) H. Zhang, Y. Bai, H. Zhang, J. Liu, X. Li, and Z. He. 2021.
    Local Attention and Global Representation Collaborating for Fine-grained Classification.
    In *Int. Conf. on Pattern Recognition (ICPR)*. 10658–10665.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2021）H. Zhang, Y. Bai, H. Zhang, J. Liu, X. Li, 和 Z. He. 2021. 局部注意力和全局表示的协同细粒度分类。在
    *国际模式识别会议 (ICPR)*。10658–10665。
- en: Zhang et al. (2010) H. Zhang, Z. Sun, and T. Tan. 2010. Contact Lens Detection
    Based on Weighted LBP. In *Int. Conf. on Pattern Recognition (ICPR)*. 4279–4282.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2010）H. Zhang, Z. Sun, 和 T. Tan. 2010. 基于加权 LBP 的隐形眼镜检测。在 *国际模式识别会议
    (ICPR)*。4279–4282。
- en: Zhang et al. (2016a) Q. Zhang, H. Li, Z. He, and Z. Sun. 2016a. Image Super-Resolution
    for Mobile Iris Recognition. In *Biometric Recognition*. Springer, Cham, 399–406.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2016a）Q. Zhang, H. Li, Z. He, 和 Z. Sun. 2016a. 移动虹膜识别的图像超分辨率。在 *生物识别识别*。Springer,
    Cham, 399–406。
- en: Zhang et al. (2016b) Q. Zhang, H. Li, Z. Sun, Z. He, and T. Tan. 2016b. Exploring
    complementary features for iris recognition on mobile devices. In *IEEE Int. Conf.
    on Biometrics (ICB)*. 1–8.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2016b）Q. Zhang, H. Li, Z. Sun, Z. He, 和 T. Tan. 2016b. 探索移动设备上虹膜识别的互补特征。在
    *IEEE 国际生物识别会议 (ICB)*。1–8。
- en: Zhang et al. (2018) Q. Zhang, H. Li, Z. Sun, and T. Tan. 2018. Deep Feature
    Fusion for Iris and Periocular Biometrics on Mobile Devices. *IEEE Transactions
    on Information Forensics and Security* 13, 11 (2018), 2897–2912.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2018）Q. Zhang, H. Li, Z. Sun, 和 T. Tan. 2018. 移动设备上的虹膜和眼周生物识别的深度特征融合。*IEEE
    信息取证与安全汇刊* 13, 11 (2018), 2897–2912。
- en: Zhang et al. (2015) Q. Zhang, H. Li, M. Zhang, Z. He, Z. Sun, and T. Tan. 2015.
    Fusion of Face and Iris Biometrics on Mobile Devices Using Near-infrared Images.
    In *Biometric Recognition*. Springer, Cham, 569–578.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2015）Q. Zhang, H. Li, M. Zhang, Z. He, Z. Sun, 和 T. Tan. 2015. 基于近红外图像的移动设备面部和虹膜生物识别融合。在
    *生物识别识别*。Springer, Cham, 569–578。
- en: Zhang et al. (2019) W. Zhang, X. Lu, Y. Gu, Y. Liu, X. Meng, and J. Li. 2019.
    A Robust Iris Segmentation Scheme Based on Improved U-Net. *IEEE Access* 7 (2019),
    85082–85089.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2019）W. Zhang, X. Lu, Y. Gu, Y. Liu, X. Meng, 和 J. Li. 2019. 基于改进 U-Net
    的鲁棒虹膜分割方案。*IEEE Access* 7 (2019), 85082–85089。
- en: Zhao et al. (2017) H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. 2017. Pyramid
    Scene Parsing Network. In *IEEE Int. Conf. on Computer Vision and Pattern Recognition
    (CVPR)*. 6230–6239.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2017）H. Zhao, J. Shi, X. Qi, X. Wang, 和 J. Jia. 2017. 金字塔场景解析网络。在 *IEEE
    计算机视觉与模式识别会议 (CVPR)*。6230–6239。
- en: Zhao et al. (2019) T. Zhao, Y. Liu, G. Huo, and X. Zhu. 2019. A Deep Learning
    Iris Recognition Method Based on Capsule Network Architecture. *IEEE Access* 7
    (2019), 49691–49701.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2019) T. Zhao, Y. Liu, G. Huo 和 X. Zhu. 2019. 基于胶囊网络架构的深度学习虹膜识别方法。*IEEE
    Access* 7 (2019), 49691–49701。
- en: Zhao and Kumar (2017a) Z. Zhao and A. Kumar. 2017a. Accurate Periocular Recognition
    Under Less Constrained Environment Using Semantics-Assisted CNN. *IEEE Transactions
    on Information Forensics and Security* 12, 5 (2017), 1017–1030.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 和 Kumar (2017a) Z. Zhao 和 A. Kumar. 2017a. 使用语义辅助 CNN 在较少约束环境下进行准确的周眼识别。*IEEE
    信息取证与安全学报* 12, 5 (2017), 1017–1030。
- en: Zhao and Kumar (2017b) Z. Zhao and A. Kumar. 2017b. Towards More Accurate Iris
    Recognition Using Deeply Learned Spatially Corresponding Features. In *IEEE Int.
    Conf. on Computer Vision (ICCV)*. 3829–3838.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 和 Kumar (2017b) Z. Zhao 和 A. Kumar. 2017b. 通过深度学习空间对应特征来实现更准确的虹膜识别。在 *IEEE
    计算机视觉国际会议 (ICCV)*。3829–3838。
- en: Zhao and Kumar (2018) Z. Zhao and A. Kumar. 2018. Improving Periocular Recognition
    by Explicit Attention to Critical Regions in Deep Neural Network. *IEEE Transactions
    on Information Forensics and Security* 13, 12 (2018), 2937–2952.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 和 Kumar (2018) Z. Zhao 和 A. Kumar. 2018. 通过深度神经网络中对关键区域的明确关注来改进周眼识别。*IEEE
    信息取证与安全学报* 13, 12 (2018), 2937–2952。
- en: Zhao and Kumar (2019) Z. Zhao and A. Kumar. 2019. A deep learning based unified
    framework to detect, segment and recognize irises using spatially corresponding
    features. *Pattern Recognition* 93 (2019), 546 – 557.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 和 Kumar (2019) Z. Zhao 和 A. Kumar. 2019. 基于深度学习的统一框架，用于检测、分割和识别虹膜，采用空间对应特征。*模式识别*
    93 (2019), 546 – 557。
- en: Zhou et al. (2016) B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba.
    2016. Learning Deep Features for Discriminative Localization. In *IEEE Int. Conf.
    on Computer Vision and Pattern Recognition (CVPR)*. 2921–2929.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等 (2016) B. Zhou, A. Khosla, A. Lapedriza, A. Oliva 和 A. Torralba. 2016.
    学习深度特征以实现判别性定位。在 *IEEE 计算机视觉与模式识别国际会议 (CVPR)*。2921–2929。
