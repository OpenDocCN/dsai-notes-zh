- en: 'Deep Learning 2: Part 1 Lesson 2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习2：第1部分第2课
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4](https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4](https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4)
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我从* [fast.ai 课程](http://www.fast.ai/) *中的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*
    [Jeremy](https://twitter.com/jeremyphoward) *和* [Rachel](https://twitter.com/math_rachel)
    *给了我这个学习的机会。*'
- en: '[Lesson 2](http://forums.fast.ai/t/wiki-lesson-2/9399/1)'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[第2课](http://forums.fast.ai/t/wiki-lesson-2/9399/1)'
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1.ipynb)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1.ipynb)'
- en: Review of last lesson [[01:02](https://youtu.be/JNxcznsrRb8?t=1m2s)]
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上一课的回顾[01:02]
- en: We used 3 lines of code to build an image classifier.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用3行代码构建了一个图像分类器。
- en: 'In order to train the model, data needs to be organized in a certain way under
    `PATH` (in this case `data/dogscats/`):'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了训练模型，数据需要以一定方式组织在`PATH`（在本例中为`data/dogscats/`）下：
- en: There should be `train` folder and `valid` folder, and under each of these,
    folders with classification labels (i.e. `cats` and `dogs` for this example) with
    corresponding images in them.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该有一个`train`文件夹和一个`valid`文件夹，每个文件夹下面都有带有分类标签的文件夹（例如本例中的`cats`和`dogs`），其中包含相应的图像。
- en: 'The training output: *[*`*epoch #*`*,* `*training loss*`*,* `*validation loss*`*,*
    `*accuracy*`*]*'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '训练输出：[*`*epoch #*`*,* `*training loss*`*,* `*validation loss*`*,* `*accuracy*`*]*'
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Learning Rate [**[**4:54**](https://youtu.be/JNxcznsrRb8?t=4m54s)**]**'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**学习率[4:54]**'
- en: The basic idea of learning rate is that it is going to decide how quickly we
    zoom/hone in on the solution.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率的基本思想是它将决定我们快速地聚焦在解决方案上。
- en: If the learning rate is too small, it will take very long time to get to the
    bottom
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果学习率太小，将需要很长时间才能到达底部
- en: If the learning rate is too big, it could get oscillate away from the bottom.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果学习率太大，它可能会从底部摆动。
- en: Learning rate finder (`learn.lr_find`) will increase the learning rate after
    each mini-batch. Eventually, the learning rate is too high that loss will get
    worse. We then look at the plot of learning rate against loss, and determine the
    lowest point and go back by one magnitude and choose that as a learning rate (`1e-2`
    in the example below).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率查找器（`learn.lr_find`）会在每个小批次后增加学习率。最终，学习率会变得太高，损失会变得更糟。然后，我们查看学习率与损失的图表，并确定最低点，然后后退一个数量级，并选择该学习率（在下面的示例中为`1e-2`）。
- en: Mini-batch is a set of few images we look at each time so that we are using
    the parallel processing power of the GPU effectively (generally 64 or 128 images
    at a time)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小批量是我们每次查看的几个图像，以便有效地利用GPU的并行处理能力（通常每次64或128个图像）。
- en: 'In Python:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中：
- en: By adjusting this one number, you should be able to get pretty good results.
    fast.ai library picks the rest of the hyper parameters for you. But as the course
    progresses, we will learn that there are some more things we can tweak to get
    slightly better results. But learning rate is the key number for us to set.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调整这个数字，您应该能够获得相当不错的结果。fast.ai库会为您选择其余的超参数。但随着课程的进行，我们将学习到一些更多的可以调整以获得稍微更好结果的东西。但学习率对我们来说是关键数字。
- en: Learning rate finder sits on top of other optimizers (e.g. momentum, Adam, etc)
    and help you choose the best learning rate given what other tweaks you are using
    (such as advanced optimizers but not limited to optimizers).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率查找器位于其他优化器（例如动量、Adam等）之上，并帮助您选择最佳学习率，考虑您正在使用的其他调整（例如高级优化器但不限于优化器）。
- en: 'Questions: what happens for optimizers that changes learning rate during the
    epoch? Is this finder choosing an initial learning rate?[[14:05](https://youtu.be/JNxcznsrRb8?t=14m5s)]
    We will learn about optimizers in details later, but the basic answer is no. Even
    Adam has a learning rate which gets divided by the average previous gradient and
    also the recent sum of squared gradients. Even those so-called “dynamic learning
    rate” methods have a learning rate.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：在epoch期间改变学习率的优化器会发生什么？这个查找器是否选择了初始学习率？[14:05] 我们稍后会详细了解优化器，但基本答案是否定的。即使是Adam也有一个学习率，该学习率会被平均先前梯度和最近平方梯度的总和除以。即使那些所谓的“动态学习率”方法也有学习率。
- en: The most important thing you can do to make the model better is to give it more
    data. Since these models have millions of parameters, if you train them for a
    while, they start to do what is called “overfitting”.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使模型更好的最重要的事情是提供更多数据。由于这些模型有数百万个参数，如果您训练它们一段时间，它们开始做所谓的“过拟合”。
- en: Overfitting — the model is starting to see the specific details of the images
    in the training set rather than learning something general that can be transferred
    across to the validation set.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合 - 模型开始看到训练集中图像的具体细节，而不是学习一些可以转移到验证集的通用内容。
- en: We can collect more data, but another easy way is data augmentation.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以收集更多数据，但另一种简单的方法是数据增强。
- en: Data Augmentation [[15:50](https://youtu.be/JNxcznsrRb8?t=15m50s)]
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强[15:50]
- en: Every epoch, we will randomly change the image a little bit. In other words,
    the model is going to see slightly different version of the image each epoch.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个epoch，我们会随机微调图像。换句话说，模型每个epoch都会看到图像的略微不同版本。
- en: You want to use different types of data augmentation for different types of
    image (flip horizontally, vertically, zoom in, zoom out, vary contrast and brightness,
    and many more).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您希望为不同类型的图像使用不同类型的数据增强（水平翻转、垂直翻转、放大、缩小、变化对比度和亮度等）。
- en: 'Learning Rate Finder Questions [[19:11](https://youtu.be/JNxcznsrRb8?t=19m11s)]:'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率查找问题：
- en: Why not pick the bottom? The point at which the loss was lowest is where the
    red circle is. But that learning rate was actually too large at that point and
    will not likely to converge. So the one before that would be a better choice (it
    is always better to pick a learning rate that is smaller than too big)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么不选择最低点？损失最低的点是红色圆圈所在的位置。但是在那一点学习率实际上太大了，不太可能收敛。因此，前一个点可能是更好的选择（总是选择比太大的学习率更小的学习率更好）
- en: When should we learn `lr_find`? [[23:02](https://youtu.be/JNxcznsrRb8?t=23m2s)]
    Run it once at the start, and maybe after unfreezing layers (we will learn it
    later). Also when I change the thing I am training or change the way I am training
    it. Never any harm in running it.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时学习`lr_find`？在开始时运行一次，也许在解冻层之后再运行（我们稍后会学习）。还有当我改变我正在训练的东西或改变我训练的方式时。运行它永远不会有害。
- en: Back to Data Augmentation [[24:10](https://youtu.be/JNxcznsrRb8?t=24m10s)]
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回到数据增强：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`transform_side_on` — a predefined set of transformations for side-on photos
    (there is also `transform_top_down`). Later we will learn how to create custom
    transform lists.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transform_side_on` - 用于侧面照片的预定义转换集（还有`transform_top_down`）。稍后我们将学习如何创建自定义转换列表。'
- en: It is not exactly creating new data, but allows the convolutional neural net
    to learn how to recognize cats or dogs from somewhat different angles.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这并不是在创建新数据，而是让卷积神经网络学习如何从略有不同的角度识别猫或狗。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we created a new `data` object that includes augmentation. Initially, the
    augmentations actually do nothing because of `precompute=True`.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们创建了一个包含增强的新`data`对象。最初，由于`precompute=True`，增强实际上什么也没做。
- en: Convolutional neural network have these things called “activations.” An activation
    is a number that says “this feature is in this place with this level of confidence
    (probability)”. We are using a pre-trained network which has already learned to
    recognize features (i.e. we do not want to change hyper parameters it learned),
    so what we can do is to pre-compute activations for hidden layers and just train
    the final linear portion.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络有这些称为“激活”的东西。激活是一个数字，表示“这个特征在这个位置以这个置信度（概率）”。我们正在使用一个已经学会识别特征的预训练网络（即我们不想改变它学到的超参数），所以我们可以预先计算隐藏层的激活，然后只训练最终的线性部分。
- en: This is why when you train your model for the first time, it takes longer —
    it is pre-computing these activations.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这就是为什么当你第一次训练模型时，需要更长时间 - 它正在预计算这些激活。
- en: Even though we are trying to show a different version of the cat each time,
    we had already pre-computed the activations for a particular version of the cat
    (i.e. we are not re-calculating the activations with the altered version).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管我们每次都试图展示猫的不同版本，但我们已经为特定版本的猫预先计算了激活（即我们没有使用改变后的版本重新计算激活）。
- en: 'To use data augmentation, we have to do `learn.precompute=False`:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用数据增强，我们必须执行`learn.precompute=False`：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Bad news is that accuracy is not improving. Training loss is decreasing but
    validation loss is not, but we are not overfitting. Overfitting when the training
    loss is much lower than the validation loss. In other words, when your model is
    doing much better job on the training set than it is on the validation set, that
    means your model is not generalizing.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坏消息是准确性没有提高。训练损失在减少，但验证损失没有，但我们没有过拟合。过拟合是指训练损失远低于验证损失。换句话说，当你的模型在训练集上表现比在验证集上好得多时，这意味着你的模型没有泛化。
- en: '`cycle_len=1` [[30:17](https://youtu.be/JNxcznsrRb8?t=30m17s)]: This enables
    **stochastic gradient descent with restarts (SGDR)**. The basic idea is as you
    get closer and closer to the spot with the minimal loss, you may want to start
    decrease the learning rate (taking smaller steps) in order to get to exactly the
    right spot.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cycle_len=1`：这使得**随机梯度下降重启（SGDR）**成为可能。基本思想是，当你越来越接近具有最小损失的位置时，你可能希望开始减小学习率（采取更小的步骤）以确切地到达正确的位置。'
- en: The idea of decreasing the learning rate as you train is called **learning rate
    annealing** which is very common. Most common and “hacky” way to do this is to
    train a model with a certain learning rate for a while, and when it stops improving,
    manually drop down the learning rate (stepwise annealing).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中降低学习率的想法被称为**学习率退火**，这是非常常见的。最常见和“hacky”方法是使用某个学习率训练模型一段时间，当它停止改进时，手动降低学习率（分阶段退火）。
- en: A better approach is simply to pick some kind of functional form — turns out
    the really good functional form is one half of the cosign curve which maintains
    the high learning rate for a while at the beginning, then drop quickly when you
    get closer.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的方法是简单地选择某种功能形式 - 结果表明，真正好的功能形式是余弦曲线的一半，它在开始时保持高学习率，然后在接近时迅速下降。
- en: 'However, we may find ourselves in a part of the weight space that isn’t very
    resilient — that is, small changes to the weights may result in big changes to
    the loss. We want to encourage our model to find parts of the weight space that
    are both accurate and stable. Therefore, from time to time we increase the learning
    rate (this is the ‘restarts’ in ‘SGDR’), which will force the model to jump to
    a different part of the weight space if the current area is “spiky”. Here’s a
    picture of how that might look if we reset the learning rates 3 times (in [this
    paper](https://arxiv.org/abs/1704.00109) they call it a “cyclic LR schedule”):'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，我们可能发现自己处于一个不太有弹性的权重空间中 - 也就是说，对权重进行微小的更改可能导致损失的巨大变化。我们希望鼓励我们的模型找到既准确又稳定的权重空间的部分。因此，我们不时增加学习率（这是“SGDR”中的“重启”），这将迫使模型跳到权重空间的不同部分，如果当前区域“尖锐”。如果我们三次重置学习率，它可能看起来像这样（在[这篇论文](https://arxiv.org/abs/1704.00109)中，他们称之为“循环LR计划”）：
- en: 'The number of epochs between resetting the learning rate is set by `cycle_len`,
    and the number of times this happens is referred to as the *number of cycles*,
    and is what we''re actually passing as the 2nd parameter to `fit()`. So here''s
    what our actual learning rates looked like:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重置学习率之间的周期数由`cycle_len`设置，这种情况下发生的次数被称为*周期数*，实际上是我们作为`fit()`的第二个参数传递的内容。这是我们实际学习率的样子：
- en: 'Question: Could we get the same effect by using random starting point? [[35:40](https://youtu.be/JNxcznsrRb8?t=35m40s)]
    Before SGDR was created, people used to create “ensembles” where they would relearn
    a whole new model ten times in the hope that one of them would end up being better.
    In SGDR, once we get close enough to the optimal and stable area, resetting will
    not actually “reset” but the weights keeps better. So SGDR will give you better
    results than just randomly try a few different starting points.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：我们可以通过使用随机起始点获得相同的效果吗？在创建SGDR之前，人们通常会创建“集成”，他们会重新学习一个全新的模型十次，希望其中一个会变得更好。在SGDR中，一旦我们接近最佳和稳定区域，重置实际上不会“重置”，而是权重保持更好。因此，SGDR将比随机尝试几个不同的起始点给出更好的结果。
- en: It is important to pick a learning rate (which is the highest learning rate
    SGDR uses) that is big enough to allow the reset to jump to a different part of
    the function. [[37:25](https://youtu.be/JNxcznsrRb8?t=37m25s)]
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个学习率（这是SGDR使用的最高学习率）很重要，它足够大，可以使重置跳转到函数的不同部分。
- en: SGDR reduces the learning rate every mini-batch, and reset occurs every `cycle_len`
    epoch (in this case it is set to 1).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SGDR会在每个小批次中降低学习率，并且重置每个`cycle_len`周期（在这种情况下设置为1）。
- en: 'Question: Our main goal is to generalize and not end up in the narrow optima.
    In this method, are we keeping track of the minima and averaging them and ensembling
    them? [[39:27](https://youtu.be/JNxcznsrRb8?t=39m27s)] That is another level of
    sophistication and you see “Snapshot Ensemble” in the diagram. We are not currently
    doing that but if you wanted it to generalize even better, you can save the weights
    right before the resets and take the average. But for now, we are just going to
    pick the last one.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：我们的主要目标是泛化，而不是陷入狭窄的最优解。在这种方法中，我们是否跟踪最小值并对其进行平均处理并集成它们？这是另一种复杂程度，您可以在图表中看到“快照集成”。我们目前没有这样做，但如果您希望泛化得更好，可以在重置之前保存权重并取平均值。但目前，我们只会选择最后一个。
- en: If you want to skip ahead, there is a parameter called `cycle_save_name` which
    you can add as well as `cycle_len`, which will save a set of weights at the end
    of every learning rate cycle and then you can ensemble them [[40:14](https://youtu.be/JNxcznsrRb8?t=40m14s)].
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想要跳过，还有一个名为`cycle_save_name`的参数，您可以添加它以及`cycle_len`，它将在每个学习率周期结束时保存一组权重，然后您可以将它们集成。
- en: Saving model [[40:31](https://youtu.be/JNxcznsrRb8?t=40m31s)]
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存模型
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When you precompute activations or create resized images (we will learn about
    it soon), various temporary files get created which you see under `data/dogcats/tmp`
    folder. If you are getting weird errors, it might be because of precomputed activations
    that are only half completed or are in some way incompatible with what you are
    doing. So you can always go ahead and delete this `/tmp` folder to see if it makes
    the error go away (fast.ai equivalent of turning it off and then on again).
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您预计算激活或创建调整大小的图像（我们将很快学习到），会创建各种临时文件，您可以在`data/dogcats/tmp`文件夹下看到。如果出现奇怪的错误，可能是因为预计算的激活只完成了一半，或者以某种方式与您正在进行的操作不兼容。因此，您可以随时继续并删除此`/tmp`文件夹，看看是否可以消除错误（相当于将其关闭然后重新打开）。
- en: You will also see there is a directory called `/models` that is where models
    get saved when you say `learn.save`
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还会看到一个名为`/models`的目录，这是当您说`learn.save`时保存模型的位置。
- en: Fine Tuning and Differential Learning Rate [[43:49](https://youtu.be/JNxcznsrRb8?t=43m49s)]
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调和差分学习率
- en: So far, we have not retrained any of pre-trained features — specifically, any
    of those weights in the convolutional kernels. All we have done is we added some
    new layers on top and learned how to mix and match pre-trained features.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有重新训练任何预训练的特征 - 具体来说，卷积核中的任何权重。我们所做的只是在顶部添加了一些新层，并学会了如何混合和匹配预训练的特征。
- en: Images like satellite images, CT scans, etc have totally different kinds of
    features all together (compare to ImageNet images), so you want to re-train many
    layers.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像卫星图像、CT扫描等图像具有完全不同类型的特征（与ImageNet图像相比），因此您需要重新训练许多层。
- en: For dogs and cats, images are similar to what the model was pre-trained with,
    but we still may find it is helpful to slightly tune some of the later layers.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于狗和猫，图像与模型预先训练的图像相似，但我们仍然可能发现微调一些后续层会有所帮助。
- en: 'Here is how you tell the learner that we want to start actually changing the
    convolutional filters themselves:'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是如何告诉学习者我们要开始实际更改卷积滤波器本身的方法：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: “frozen” layer is a layer which is not being trained/updated. `unfreeze` unfreezes
    all the layers.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “冻结”层是一个未被训练/更新的层。`unfreeze`会解冻所有层。
- en: Earlier layers like the first layer (which detects diagonal edges or gradient)
    or the second layer (which recognizes corners or curves) probably do not need
    to change by much, if at all.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像第一层（检测对角边缘或梯度）或第二层（识别角落或曲线）这样的早期层可能根本不需要或只需要很少的更改。
- en: 'Later layers are much more likely to need more learning. So we create an array
    of learning rates (differential learning rate):'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后续层更有可能需要更多的学习。因此，我们创建了一个学习率数组（差分学习率）：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`1e-4` : for the first few layers (basic geometric features)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1e-4`：用于前几层（基本几何特征）'
- en: '`1e-3` : for the middle layers (sophisticated convolutional features)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1e-3`：用于中间层（复杂的卷积特征）'
- en: '`1e-2` : for layers we added on top'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1e-2`：用于我们添加的顶部层'
- en: Why 3? Actually they are 3 ResNet blocks but for now, think of it as a group
    of layers.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么是3？实际上它们是3个ResNet块，但现在，可以将其视为一组层。
- en: '**Question**: What if I have a bigger images than the model is trained with?
    [[50:30](https://youtu.be/JNxcznsrRb8?t=50m30s)] The short answer is, with this
    library and modern architectures we are using, we can use any size we like.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：如果我的图片比模型训练的图片大怎么办？简短的答案是，使用这个库和我们正在使用的现代架构，我们可以使用任何大小的图片。
- en: '**Question**: Can we unfreeze just specific layers? [[51:03](https://youtu.be/JNxcznsrRb8?t=51m3s)]
    We are not doing it yet, but if you wanted, you can do `lean.unfreeze_to(n)` (which
    will unfreeze layers from layer `n` onwards). Jeremy almost never finds it helpful
    and he thinks it is because we are using differential learning rates, and the
    optimizer can learn just as much as it needs to. The one place he found it helpful
    is if he is using a really big memory intensive model and he is running out of
    GPU, the less layers you unfreeze, the less memory and time it takes.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：我们可以只解冻特定的层吗？我们还没有这样做，但如果你想的话，你可以使用`learn.unfreeze_to(n)`（这将从第`n`层开始解冻层）。Jeremy几乎从来没有发现这有帮助，他认为这是因为我们使用了不同的学习率，优化器可以学习到它需要的一样多。他发现有帮助的一个地方是，如果他使用一个真正大的内存密集型模型，而且他的GPU快要用完了，你解冻的层数越少，占用的内存和时间就越少。
- en: Using differential learning rate, we are up to 99.5%! [[52:28](https://youtu.be/JNxcznsrRb8?t=52m28s)]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的学习率，我们的准确率达到了99.5%！
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Earlier we said `3` is the number of epochs, but it is actually **cycles**.
    So if `cycle_len=2` , it will do 3 cycles where each cycle is 2 epochs (i.e. 6
    epochs). Then why did it 7? It is because of `cycle_mult`
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前我们说`3`是周期的数量，但实际上是**周期**。所以如果`cycle_len=2`，它将执行3个周期，每个周期为2个周期（即6个周期）。那为什么是7个？这是因为`cycle_mult`。
- en: '`cycle_mult=2` : this multiplies the length of the cycle after each cycle (1
    epoch + 2 epochs + 4 epochs = 7 epochs).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cycle_mult=2`：这会在每个周期后乘以周期的长度（1个周期+2个周期+4个周期=7个周期）。'
- en: Intuitively speaking [[53:57](https://youtu.be/JNxcznsrRb8?t=53m57s)], if the
    cycle length is too short, it starts going down to find a good spot, then pops
    out, and goes down trying to find a good spot and pops out, and never actually
    get to find a good spot. Earlier on, you want it to do that because it is trying
    to find a spot that is smoother, but later on, you want it to do more exploring.
    That is why `cycle_mult=2` seems to be a good approach.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，如果周期长度太短，它开始下降寻找一个好的位置，然后弹出，再次下降寻找一个好的位置，然后弹出，永远无法找到一个好的位置。在早期，你希望它这样做，因为它试图找到一个更平滑的位置，但后来，你希望它做更多的探索。这就是为什么`cycle_mult=2`似乎是一个好方法。
- en: 'We are introducing more and more hyper parameters having told you that there
    are not many. You can get away with just choosing a good learning rate, but then
    adding these extra tweaks helps get that extra level-up without any effort. In
    general, good starting points are:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在引入越来越多的超参数，告诉你没有很多。你可以只选择一个好的学习率，但添加这些额外的调整可以在不费力的情况下获得额外的提升。一般来说，好的起点是：
- en: '`n_cycle=3, cycle_len=1, cycle_mult=2`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_cycle=3，cycle_len=1，cycle_mult=2`'
- en: '`n_cycle=3, cycle_len=2` (no `cycle_mult`)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_cycle=3，cycle_len=2`（没有`cycle_mult`）'
- en: 'Question: why do smoother surfaces correlate to more generalized networks?
    [[55:28](https://youtu.be/JNxcznsrRb8?t=55m28s)]'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '问题：为什么更平滑的表面与更广义的网络相关？ '
- en: Say you have something spiky (blue line). X-axis is showing how good this is
    at recognizing dogs vs. cats as you change this particular parameter. Something
    to be generalizable means that we want it to work when we give it a slightly different
    dataset. Slightly different dataset may have a slightly different relationship
    between this parameter and how cat-like vs. dog-like it is. It may, instead look
    like the red line. In other words, if we end up at the blue pointy part, then
    it will not going to do a good job on this slightly different dataset. Or else,
    if we end up on the wider blue part, it will still do a good job on the red dataset.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个尖锐的东西（蓝线）。X轴显示了当你改变这个特定参数时，它在识别狗和猫方面的表现如何。可泛化意味着当我们给它一个略微不同的数据集时，我们希望它能够工作。略微不同的数据集可能在这个参数和猫狗之间的关系上有略微不同。它可能看起来像红线。换句话说，如果我们最终到达蓝色尖锐部分，那么它在这个略微不同的数据集上不会表现良好。或者，如果我们最终到达较宽的蓝色部分，它仍然会在红色数据集上表现良好。
- en: '[Here](http://forums.fast.ai/t/why-do-we-care-about-resilency-of-where-we-are-in-the-weight-space/7323)
    is some interesting discussion about spiky minima.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里有一些关于峰值最小值的有趣讨论。
- en: Test Time Augmentation (TTA) [[56:49](https://youtu.be/JNxcznsrRb8?t=56m49s)]
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试时间增强（TTA）
- en: 'Our model has achieved 99.5%. But can we make it better still? Let’s take a
    look at pictures we predicted incorrectly:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型已经达到了99.5%。但我们还能让它变得更好吗？让我们看看我们错误预测的图片：
- en: Here, Jeremy printed out the whole of these pictures. When we do the validation
    set, all of our inputs to our model must be square. The reason is kind of a minor
    technical detail, but GPU does not go very quickly if you have different dimensions
    for different images. It needs to be consistent so that every part of the GPU
    can do the same thing. This may probably be fixable but for now that is the state
    of the technology we have.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Jeremy打印出了所有这些图片。当我们进行验证集时，我们模型的所有输入必须是正方形的。原因有点小的技术细节，但如果不同的图片有不同的尺寸，GPU不会很快。它需要保持一致，以便GPU的每个部分都可以做同样的事情。这可能是可以解决的，但目前这是我们拥有的技术状态。
- en: 'To make it square, we just pick out the square in the middle — as you can see
    below, it is understandable why this picture was classified incorrectly:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使它成为正方形，我们只需挑选中间的正方形——正如你所看到的，可以理解为什么这张图片被错误分类：
- en: We are going to do what is called “**Test Time Augmentation**”. What this means
    is that we are going to take 4 data augmentations at random as well as the un-augmented
    original (center-cropped). We will then calculate predictions for all these images,
    take the average, and make that our final prediction. Note that this is only for
    validation set and/or test set.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行所谓的“**测试时间增强**”。这意味着我们将随机进行4次数据增强，以及未增强的原始图像（中心裁剪）。然后我们将为所有这些图像计算预测，取平均值，并将其作为我们的最终预测。请注意，这仅适用于验证集和/或测试集。
- en: To do this, all you have to do is `learn.TTA()` — which brings up the accuracy
    to 99.65%!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，您只需`learn.TTA()`——这将将准确性提高到99.65%！
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Questions on augmentation approach[**[**01:01:36**](https://youtu.be/JNxcznsrRb8?t=1h1m36s)**]:**
    Why not border or padding to make it square? Typically Jeremy does not do much
    padding, but instead he does a little bit of **zooming**. There is a thing called
    **reflection padding** that works well with satellite imagery. Generally speaking,
    using TTA plus data augmentation, the best thing to do is try to use as large
    image as possible. Also, having fixed crop locations plus random contrast, brightness,
    rotation changes might be better for TTA.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于增强方法的问题[**[**01:01:36**](https://youtu.be/JNxcznsrRb8?t=1h1m36s)**]**：为什么不使用边框或填充使其变成正方形？通常Jeremy不会做太多填充，而是会做一点**缩放**。有一种叫做**反射填充**的东西在卫星图像中效果很好。一般来说，使用TTA加数据增强，最好的做法是尽可能使用尽可能大的图像。此外，固定裁剪位置加上随机对比度、亮度、旋转变化可能对TTA更好。'
- en: '**Question:** Data augmentation for non-image dataset? [[01:03:35](https://youtu.be/JNxcznsrRb8?t=1h3m35s)]
    No one seems to know. It seems like it would be helpful, but there are very few
    number of examples. In natural language processing, people tried replacing synonyms
    for instance, but on the whole the area is under researched and under developed.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：非图像数据集的数据增强？[[01:03:35](https://youtu.be/JNxcznsrRb8?t=1h3m35s)] 没有人似乎知道。看起来会有帮助，但例子很少。在自然语言处理中，人们尝试替换同义词，但总体来说，这个领域研究不足，发展不足。'
- en: '**Question**: Is fast.ai library open source?[[01:05:34](https://youtu.be/JNxcznsrRb8?t=1h5m34s)]
    Yes. He then covered the reason [why Fast.ai switched from Keras + TensorFlow
    to PyTorch](http://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：fast.ai库是开源的吗？[[01:05:34](https://youtu.be/JNxcznsrRb8?t=1h5m34s)] 是的。然后他讲解了[Fast.ai从Keras
    + TensorFlow切换到PyTorch的原因](http://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/)'
- en: 'Random note: PyTorch is much more than just a deep learning library. It actually
    lets us write arbitrary GPU accelerated algorithms from scratch — Pyro is a great
    example of what people are now doing with PyTorch outside of deep learning.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 随机笔记：PyTorch不仅仅是一个深度学习库。它实际上让我们可以从头开始编写任意GPU加速的算法——Pyro是人们现在在PyTorch之外进行的一个很好的例子。
- en: Analyzing results [[01:11:50](https://youtu.be/JNxcznsrRb8?t=1h11m50s)]
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析结果[[01:11:50](https://youtu.be/JNxcznsrRb8?t=1h11m50s)]
- en: '**Confusion matrix**'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**混淆矩阵**'
- en: The simple way to look at the result of a classification is called confusion
    matrix — which is used not only for deep learning but in any kind of machine learning
    classifier. It is helpful particularly if there are four or five classes you are
    trying to predict to see which group you are having the most trouble with.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 分类结果的简单查看方式称为混淆矩阵——不仅用于深度学习，而且用于任何类型的机器学习分类器。如果你试图预测四五类，特别有帮助，可以看出你在哪个组别遇到了最大的困难。
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let’s look at the pictures again [[01:13:00](https://youtu.be/JNxcznsrRb8?t=1h13m)]
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们再次看看图片[[01:13:00](https://youtu.be/JNxcznsrRb8?t=1h13m)]
- en: 'Most incorrect cats (only the left two were incorrect — it displays 4 by default):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数错误的猫（只有左边两个是错误的——默认显示4个）：
- en: 'Most incorrect dots:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数错误的点：
- en: 'Review: easy steps to train a world-class image classifier [[01:14:09](https://youtu.be/JNxcznsrRb8?t=1h14m9s)]'
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾：训练世界一流的图像分类器的简单步骤[[01:14:09](https://youtu.be/JNxcznsrRb8?t=1h14m9s)]
- en: Enable data augmentation, and `precompute=True`
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用数据增强，`precompute=True`
- en: Use `lr_find()` to find highest learning rate where loss is still clearly improving
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`lr_find()`找到损失仍然明显改善的最高学习率
- en: Train last layer from precomputed activations for 1–2 epochs
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从预计算的激活中训练最后一层1-2个时期
- en: Train last layer with data augmentation (i.e. `precompute=False`) for 2–3 epochs
    with `cycle_len=1`
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据增强训练最后一层（即`precompute=False`）2-3个时期，`cycle_len=1`
- en: Unfreeze all layers
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解冻所有层
- en: 'Set earlier layers to 3x-10x lower learning rate than next higher layer. Rule
    of thumb: 10x for ImageNet like images, 3x for satellite or medical imaging'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前面的层设置为比下一层低3倍至10倍的学习率。经验法则：ImageNet类似的图像为10倍，卫星或医学成像为3倍
- en: 'Use `lr_find()` again (Note: if you call `lr_find` having set differential
    learning rates, what it prints out is the learning rate of the last layers.)'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次使用`lr_find()`（注意：如果您设置了不同的学习率并调用`lr_find`，它打印出的是最后几层的学习率。）
- en: Train full network with `cycle_mult=2` until over-fitting
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`cycle_mult=2`训练完整网络直到过拟合
- en: 'Let’s do it again: [Dog Breed Challenge](https://www.kaggle.com/c/dog-breed-identification)
    [[01:16:37](https://youtu.be/JNxcznsrRb8?t=1h16m37s)]'
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们再做一次：[狗品种挑战](https://www.kaggle.com/c/dog-breed-identification) [[01:16:37](https://youtu.be/JNxcznsrRb8?t=1h16m37s)]
- en: You can use [Kaggle CLI](https://github.com/floydwch/kaggle-cli) to download
    data for Kaggle competitions
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用[Kaggle CLI](https://github.com/floydwch/kaggle-cli)下载Kaggle竞赛的数据
- en: Notebook is not made public since it is an active competition
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 笔记本没有公开，因为它是一个活跃的竞赛
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is a little bit different to our previous dataset. Instead of `train`
    folder which has a separate folder for each breed of dog, it has a CSV file with
    the correct labels. We will read CSV file with Pandas. Pandas is what we use in
    Python to do structured data analysis like CSV and usually imported as `pd`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们以前的数据集有点不同。它没有一个包含每个狗品种的单独文件夹的`train`文件夹，而是有一个带有正确标签的CSV文件。我们将使用Pandas读取CSV文件。Pandas是我们在Python中用来进行结构化数据分析的工具，比如CSV，通常被导入为`pd`：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How many dog images per breed
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 每个品种有多少狗图像
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`max_zoom` — we will zoom in up to 1.1 times'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_zoom`——我们将放大1.1倍'
- en: '`ImageClassifierData.from_csv` — last time, we used `from_paths` but since
    the labels are in CSV file, we will call `from_csv` instead.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ImageClassifierData.from_csv` — 上次我们使用了`from_paths`，但由于标签在CSV文件中，我们将使用`from_csv`。'
- en: '`test_name` — we need to specify where the test set is if you want to submit
    to Kaggle competitions'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_name` — 如果要提交到Kaggle比赛，我们需要指定测试集的位置'
- en: '`val_idx` — there is no `validation` folder but we still want to track how
    good our performance is locally. So above you will see:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`val_idx` — 没有`validation`文件夹，但我们仍然想要跟踪我们的本地表现有多好。因此你会看到上面的：'
- en: '`n = len(list(open(label_csv)))-1` : Open CSV file, create a list of rows,
    then take the length. `-1` because the first row is a header. Hence `n` is the
    number of images we have.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`n = len(list(open(label_csv)))-1`：打开CSV文件，创建一个行列表，然后取长度。 `-1`是因为第一行是标题。因此`n`是我们拥有的图像数量。'
- en: '`val_idxs = **get_cv_idxs**(n)` : “get cross validation indexes” — this will
    return, by default, random 20% of the rows (indexes to be precise) to use as a
    validation set. You can also send `val_pct` to get different amount.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`val_idxs = **get_cv_idxs**(n)`： “获取交叉验证索引” — 默认情况下，这将返回随机20%的行（确切的索引）作为验证集。你也可以发送`val_pct`以获得不同的数量。'
- en: '`suffix=’.jpg’` — File names has `.jpg` at the end, but CSV file does not.
    So we will set `suffix` so it knows the full file names.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suffix=''.jpg''` — 文件名以`.jpg`结尾，但CSV文件没有。因此我们将设置`suffix`以便它知道完整的文件名。'
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can access to training dataset by saying `data.trn_ds` and `trn_ds` contains
    a lot of things including file names (`fnames`)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过说`data.trn_ds`来访问训练数据集，`trn_ds`包含很多东西，包括文件名（`fnames`）
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now we check image size. If they are huge, then you have to think really carefully
    about how to deal with them. If they are tiny, it is also challenging. Most of
    ImageNet models are trained on either 224 by 224 or 299 by 299 images
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们检查图像大小。如果它们很大，那么你必须非常仔细地考虑如何处理它们。如果它们很小，也是具有挑战性的。大多数ImageNet模型都是在224x224或299x299的图像上训练的
- en: '[PRE17]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Dictionary comprehension — `key: name of the file`, `value: size of the file`'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '字典推导 — `键: 文件名`，`值: 文件大小`'
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`*size_d.values()` will unpack a list. `zip` will pair up elements of tuples
    to create a list of tuples.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*size_d.values()`将解压缩一个列表。`zip`将元组的元素配对以创建一个元组列表。'
- en: '[PRE19]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Histogram of rows
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 行的直方图
- en: Matplotlib is something you want to be very familiar with if you do any kind
    of data science or machine learning in Python. Matplotlib is always referred to
    as `plt` .
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你在Python中进行任何数据科学或机器学习，Matplotlib是你想要非常熟悉的东西。Matplotlib总是被称为`plt`。
- en: '**Question**: How many images should we use as a validation set? [[01:26:28](https://youtu.be/JNxcznsrRb8?t=1h26m28s)]
    Using 20% is fine unless the dataset is small — then 20% is not enough. If you
    train the same model multiple times and you are getting very different validation
    set results, then your validation set is too small. If the validation set is smaller
    than a thousand, it is hard to interpret how well you are doing. If you care about
    the third decimal place of accuracy and you only have a thousand things in your
    validation set, a single image changes the accuracy. If you care about the difference
    between 0.01 and 0.02, you want that to represent 10 or 20 rows. Normally 20%
    seems to work fine.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我们应该使用多少图像作为验证集？[01:26:28] 使用20%是可以的，除非数据集很小 — 那么20%就不够了。如果你多次训练相同的模型并且得到非常不同的验证集结果，那么你的验证集太小了。如果验证集小于一千，很难解释你的表现如何。如果你关心准确度的第三位小数，并且验证集中只有一千个数据，一个图像的变化就会改变准确度。如果你关心0.01和0.02之间的差异，你希望这代表10或20行。通常20%似乎效果不错。'
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here is the regular two lines of code. When we start working with new dataset,
    we want everything to go super fast. So we made it possible to specify the size
    and start with something like 64 which will run fast. Later, we will use bigger
    images and bigger architectures at which point, you may run out of GPU memory.
    If you see CUDA out of memory error, the first thing you need to do is to restart
    kernel (you cannot recover from it), then make the batch size smaller.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是常规的两行代码。当我们开始使用新数据集时，我们希望一切都能快速进行。因此，我们可以指定大小并从64开始，这样会运行得更快。稍后，我们将使用更大的图像和更大的架构，到那时，你可能会耗尽GPU内存。如果你看到CUDA内存不足错误，你需要做的第一件事是重新启动内核（你无法从中恢复），然后减小批量大小。
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 83% for 120 classes is pretty good.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于120个类别来说，83%是相当不错的。
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Reminder: a `epoch` is one pass through the data, a `cycle` is how many epochs
    you said is in a cycle'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提醒：一个`epoch`是对数据的一次遍历，一个`cycle`是你说一个周期中有多少个epoch
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Increase image size [1:32:55]
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加图像大小 [1:32:55]
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If you trained a model on smaller size images, you can then call `learn.set_data`
    and pass in a larger size dataset. That is going to take your model, however it
    has been trained so far, and it is going to let you continue to train on larger
    images.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你在较小尺寸的图像上训练了一个模型，然后可以调用`learn.set_data`并传入一个更大尺寸的数据集。这将采用到目前为止已经训练过的模型，并让你继续在更大的图像上训练。
- en: Starting training on small images for a few epochs, then switching to bigger
    images, and continuing training is an amazingly effective way to avoid overfitting.
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从小图像开始训练几个时期，然后切换到更大的图像，并继续训练是一个非常有效的避免过拟合的方法。
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you see, validation set loss (0.2274) is much lower than training set loss
    (0.28341) — which means it is **under fitting**. When you are under fitting, it
    means `cycle_len=1` is too short (learning rate is getting reset before it had
    the chance to zoom in properly). So we will add `cycle_mult=2` (i.e. 1st cycle
    is 1 epoch, 2nd cycle is 2 epochs, and 3rd cycle is 4 epochs)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如你所见，验证集损失（0.2274）远低于训练集损失（0.28341） — 这意味着它是**欠拟合**。当你欠拟合时，意味着`cycle_len=1`太短了（学习率在适当缩小之前被重置）。所以我们将添加`cycle_mult=2`（即第一个周期是1个时期，第二个周期是2个时期，第三个周期是4个时期）
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now the validation loss and training loss are about the same — this is about
    the right track. Then we try `TTA` :'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在验证损失和训练损失大致相同 — 这是正确的轨道。然后我们尝试`TTA`：
- en: '[PRE27]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Other things to try:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 其他尝试：
- en: Try running one more cycle of 2 epochs
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试再运行一个2个时期的周期
- en: Unfreezing (in this case, training convolutional layers did not help in the
    slightest since the images actually came from ImageNet)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解冻（在这种情况下，训练卷积层根本没有帮助，因为图像实际上来自ImageNet）
- en: Remove validation set and just re-run the same steps, and submit that — which
    lets us use 100% of the data.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除验证集，只需重新运行相同的步骤，并提交 - 这样我们可以使用100%的数据。
- en: '**Question**: How do we deal with unbalanced dataset? [[01:38:46](https://youtu.be/JNxcznsrRb8?t=1h38m46s)]
    This dataset is not totally balanced (between 60 and 100) but it is not unbalanced
    enough that Jeremy would give it a second thought. A recent paper says the best
    way to deal with very unbalanced dataset is to make copies of the rare cases.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我们如何处理不平衡的数据集？[[01:38:46](https://youtu.be/JNxcznsrRb8?t=1h38m46s)]这个数据集不是完全平衡的（在60和100之间），但不够不平衡，以至于Jeremy不会再考虑。最近的一篇论文说，处理非常不平衡的数据集的最佳方法是复制罕见情况。'
- en: '**Question**: Difference between `precompute=True` and `unfreeze`?'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：`precompute=True`和`unfreeze`之间的区别？'
- en: We started with a pre-trained network
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从预训练网络开始
- en: We added a couple of layers on the end of it which start out random. With everything
    frozen and `precompute=True`, all we are learning is the layers we have added.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在其末尾添加了几层，这些层最初是随机的。当所有内容都被冻结且`precompute=True`时，我们学到的只是我们添加的层。
- en: With `precompute=True`, data augmentation does not do anything because we are
    showing exactly the same activations each time.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`precompute=True`，数据增强不起作用，因为每次显示的激活完全相同。
- en: We then set `precompute=False` which means we are still only training the layers
    we added because it is frozen but data augmentation is now working because it
    is actually going through and recalculating all of the activations from scratch.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将`precompute=False`设置为假，这意味着我们仍然只训练我们添加的层，因为它被冻结，但数据增强现在正在工作，因为它实际上正在重新计算所有激活。
- en: Then finally, we unfreeze which is saying “okay, now you can go ahead and change
    all of these earlier convolutional filters”.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们解冻，这意味着“好的，现在您可以继续更改所有这些早期卷积滤波器”。
- en: '**Question**: Why not just set `precompute=False` from the beginning? The only
    reason to have `precompute=True` is it is much faster (10 or more times). If you
    are working with quite a large dataset, it can save quite a bit of time. There
    is no accuracy reason ever to use `precompute=True` .'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：为什么不从一开始就将`precompute=False`设置为假？将`precompute=True`的唯一原因是它速度更快（快10倍或更多）。如果您正在处理相当大的数据集，它可以节省相当多的时间。从来没有理由使用`precompute=True`来提高准确性。'
- en: '**Minimal steps to get good results:**'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**获得良好结果的最小步骤：**'
- en: Use `lr_find()` to find highest learning rate where loss is still clearly improving
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`lr_find()`找到损失仍然明显改善的最高学习率
- en: Train last layer with data augmentation (i.e. `precompute=False`) for 2–3 epochs
    with `cycle_len=1`
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据增强（即`precompute=False`）训练最后一层2-3个周期，`cycle_len=1`
- en: Unfreeze all layers
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解冻所有层
- en: Set earlier layers to 3x-10x lower learning rate than next higher layer
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将较早的层设置为比下一层更高的层次低3倍至10倍的学习率
- en: Train full network with `cycle_mult=2` until over-fitting
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`cycle_mult=2`训练完整网络直到过拟合
- en: '**Question**: Does reducing the batch size only affect the speed of training?
    [[1:43:34](https://youtu.be/JNxcznsrRb8?t=1h43m34s)] Yes, pretty much. If you
    are showing it less images each time, then it is calculating the gradient with
    less images — hence less accurate. In other words, knowing which direction to
    go and how far to go in that direction is less accurate. So as you make the batch
    size smaller, you are making it more volatile. It impacts the optimal learning
    rate that you would need to use, but in practice, dividing the batch size by 2
    vs. 4 does not seem to change things very much. If you change the batch size by
    much, you can re-run learning rate finder to check.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：减少批量大小只影响训练速度吗？[[1:43:34](https://youtu.be/JNxcznsrRb8?t=1h43m34s)]是的，基本上是这样。如果每次显示的图像较少，则使用较少的图像计算梯度
    - 因此准确性较低。换句话说，知道要走哪个方向以及在该方向上走多远的准确性较低。因此，随着批量大小变小，它变得更加不稳定。它会影响您需要使用的最佳学习率，但实际上，将批量大小除以2与除以4似乎并没有太大变化。如果更改批量大小很大，可以重新运行学习率查找器进行检查。'
- en: '**Question:** What are the grey images vs. the ones on the right?'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：灰色图像与右侧图像之间有什么区别？'
- en: '[Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[可视化和理解卷积网络](https://arxiv.org/abs/1311.2901)'
- en: Layer 1, they are exactly what the filters look like. It is easy to visualize
    because input to it are pixels. Later on, it gets harder because inputs are themselves
    activations which is a combination of activations. Zeiler and Fergus came up with
    a clever technique to show what the filters tend to look like on average — called
    **deconvolution** (we will learn in Part 2).Ones on the right are the examples
    of patches of image which activated that filter highly.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层，它们确实是滤波器的样子。很容易可视化，因为输入是像素。后来，变得更难，因为输入本身是激活，是激活的组合。Zeiler和Fergus提出了一种聪明的技术，展示滤波器平均倾向于什么样子
    - 称为**反卷积**（我们将在第2部分学习）。右侧是激活该滤波器的图像块的示例。
- en: '**Question**: What would you have done if the dog was off to the corner or
    tiny (re: dog breed identification)? [[01:47:16](https://youtu.be/JNxcznsrRb8?t=1h47m16s)]
    We will learn about it in Part 2, but there is a technique that allows you to
    figure out roughly which parts of an image most likely have the interesting things
    in them. Then you can crop out that area.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：如果狗在角落或很小，你会怎么做（关于狗品种识别）？[[01:47:16](https://youtu.be/JNxcznsrRb8?t=1h47m16s)]我们将在第2部分学习，但有一种技术可以让您大致确定图像的哪些部分最有趣。然后您可以裁剪出该区域。'
- en: Further improvement [[01:48:16](https://youtu.be/JNxcznsrRb8?t=1h48m16s)]
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步改进[[01:48:16](https://youtu.be/JNxcznsrRb8?t=1h48m16s)]
- en: 'Two things we can do immediately to make it better:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 立即可以做两件事来使其更好：
- en: Assuming the size of images you were using is smaller than the average size
    of images you have been given, you can increase the size. As we have seen before,
    you can increase it during training.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设您使用的图像大小小于您所获得的图像的平均大小，您可以增加大小。正如我们之前所看到的，您可以在训练期间增加它。
- en: Use better architecture. There are different ways of putting together what size
    convolutional filters and how they are connected to each other, and different
    architectures have different number of layers, size of kernels, filters, etc.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用更好的架构。有不同的方法来组合卷积滤波器的大小以及它们如何连接在一起，不同的架构具有不同数量的层，内核大小，滤波器等。
- en: We have been using ResNet34 — a great starting point and often a good finishing
    point because it does not have too many parameters and works well with small dataset.
    There is another architecture called ResNext which was the second-place winner
    in last year’s ImageNet competition.ResNext50 takes twice as long and 2–4 times
    more memory than ResNet34.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在使用ResNet34 — 一个很好的起点，通常也是一个很好的终点，因为它没有太多参数，并且在小数据集上表现良好。还有另一种架构叫做ResNext，它是去年ImageNet比赛的第二名。ResNext50的训练时间是ResNet34的两倍，内存使用量是其2-4倍。
- en: '[Here](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1-rxt50.ipynb)
    is the notebook which is almost identical to the original dogs. vs. cats. which
    uses ResNext50 which achieved 99.75% accuracy.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[这里](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1-rxt50.ipynb)是几乎与原始狗和猫相同的笔记本。使用了ResNext50，实现了99.75%的准确率。'
- en: Satellite Imagery [01:53:01]
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卫星图像 [01:53:01]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson2-image_models.ipynb)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson2-image_models.ipynb)'
- en: 'Code is pretty much the same as what we had seen before. Here are some differences:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 代码基本与之前看到的相同。以下是一些不同之处：
- en: '`transforms_top_down` — Since they satellite imagery, they still make sense
    when they were flipped vertically.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transforms_top_down` — 由于它们是卫星图像，所以在垂直翻转时仍然有意义。'
- en: Much higher learning rate — something to do with this particular dataset
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率更高 — 与这个特定数据集有关
- en: '`lrs = np.array([lr/9,lr/3,lr])` — differential learning rate now change by
    3x because images are quite different from ImageNet images'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lrs = np.array([lr/9,lr/3,lr])` — 差异学习率现在变为3倍，因为图像与ImageNet图像非常不同'
- en: '`sz=64` — this helped to avoid over fitting for satellite images but he would
    not do that for dogs. vs. cats or dog breed (similar images to ImageNet) as 64
    by 64 is quite tiny and might destroy pre-trained weights.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sz=64` — 这有助于避免卫星图像的过拟合，但对于狗和猫或狗品种（与ImageNet相似的图像）他不会这样做，因为64x64相当小，可能会破坏预训练权重。'
- en: How to get your AWS setup [[01:58:54](https://youtu.be/JNxcznsrRb8?t=1h58m54s)]
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何设置您的AWS [[01:58:54](https://youtu.be/JNxcznsrRb8?t=1h58m54s)]
- en: You can follow along the video or [here](https://github.com/reshamas/fastai_deeplearn_part1/blob/master/tools/aws_ami_gpu_setup.md)
    is a great write up by one of the students.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以跟着视频或[这里](https://github.com/reshamas/fastai_deeplearn_part1/blob/master/tools/aws_ami_gpu_setup.md)是一位学生写的很好的文章。
