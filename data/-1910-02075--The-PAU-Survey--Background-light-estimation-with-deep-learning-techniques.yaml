- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 20:04:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 20:04:33'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1910.02075] The PAU Survey: Background light estimation with deep learning
    techniques'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1910.02075] PAU调查：使用深度学习技术进行背景光估计'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1910.02075](https://ar5iv.labs.arxiv.org/html/1910.02075)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1910.02075](https://ar5iv.labs.arxiv.org/html/1910.02075)
- en: 'The PAU Survey: Background light estimation with deep learning techniques'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PAU调查：使用深度学习技术进行背景光估计
- en: 'L. Cabayol-Garcia¹, M. Eriksen¹ , A. Alarcón^(2,3), A. Amara⁴, J. Carretero¹³³footnotemark:
    3, R. Casas^(2,3), F. J. Castander^(2,3), E. Fernández¹, J. García-Bellido⁵, E. Gaztanaga^(2,3),
    H. Hoekstra⁶, R. Miquel^(1,7), C. Neissner¹³³footnotemark: 3, C. Padilla¹ E. Sánchez⁸,
    S. Serrano², I. Sevilla-Noarbe², M. Siudek¹, P. Tallada⁸³³footnotemark: 3, L. Tortorelli⁹'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'L. Cabayol-Garcia¹, M. Eriksen¹, A. Alarcón^(2,3), A. Amara⁴, J. Carretero¹³³footnotemark:
    3, R. Casas^(2,3), F. J. Castander^(2,3), E. Fernández¹, J. García-Bellido⁵, E.
    Gaztanaga^(2,3), H. Hoekstra⁶, R. Miquel^(1,7), C. Neissner¹³³footnotemark: 3,
    C. Padilla¹ E. Sánchez⁸, S. Serrano², I. Sevilla-Noarbe², M. Siudek¹, P. Tallada⁸³³footnotemark:
    3, L. Tortorelli⁹'
- en: ¹Institut de Física d’Altes Energies (IFAE), The Barcelona Institute of Science
    and Technology, 08193 Bellaterra (Barcelona), Spain
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹阿尔特能源物理研究所（IFAE），巴萨罗那科学与技术研究所，08193 贝拉特拉（巴萨罗那），西班牙
- en: ²Institute of Space Sciences (ICE, CSIC), Campus UAB, Carrer de Can Magrans,
    s/n, 08193 Barcelona, Spain
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²空间科学研究所（ICE, CSIC），UAB 校区，卡雷尔·德·坎·马格兰斯，s/n，08193 巴萨罗那，西班牙
- en: ³Institut d’Estudis Espacials de Catalunya (IEEC), 08193 Barcelona, Spain
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³加泰罗尼亚空间研究所（IEEC），08193 巴萨罗那，西班牙
- en: ⁴Institute of Cosmology & Gravitation, University of Portsmouth, Dennis Sciama
    Building, Burnaby Road, Portsmouth PO1 3FX, UK
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴宇宙学与引力研究所，朴茨茅斯大学，丹尼斯·夏玛大楼，伯纳比路，朴茨茅斯 PO1 3FX，英国
- en: ⁵Instituto de Fisica Teorica UAM/CSIC, Universidad Autonoma de Madrid, 28049
    Madrid, Spain
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵马德里自治大学/西班牙国家科研委员会理论物理研究所，28049 马德里，西班牙
- en: ⁶Leiden Observatory, Leiden University, Leiden, The Netherlands
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ⁶莱顿大学天文台，莱顿大学，莱顿，荷兰
- en: ⁷Institució Catalana de Recerca i Estudis Avançats, E-08010 Barcelona, Spain
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ⁷加泰罗尼亚研究与高级研究机构，E-08010 巴萨罗那，西班牙
- en: ⁸Centro de Investigaciones Energéticas, Medioambientales y Tecnológicas (CIEMAT),
    Madrid, Spain
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ⁸能源、环境与技术研究中心（CIEMAT），马德里，西班牙
- en: '⁹Institute for Particle Physics and Astrophysics, ETH Zürich, Wolfgang-Pauli-Str.
    27, 8093 Zürich, Switzerland E-mail:lcabayol@ifae.esE-mail: eriksen@pic.esAlso
    at Port d’Informació Científica (PIC), Campus UAB, C. Albareda s/n, 08193 Bellaterra
    (Cerdanyola del Vallès), Spain(Accepted XXX. Received YYY; in original form ZZZ)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '⁹粒子物理与天体物理研究所，苏黎世联邦理工学院，沃尔夫冈-保利街 27，8093 苏黎世，瑞士 电子邮件: lcabayol@ifae.es 电子邮件:
    eriksen@pic.es 同时在科学信息港（PIC），UAB 校区，C. Albareda s/n，08193 贝拉特拉（塞尔达尼奥拉·德尔·瓦列斯），西班牙（接受
    XXX。收到 YYY；原始形式 ZZZ）'
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: \textcolor
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: 'blackIn any imaging survey, measuring accurately the astronomical background
    light is crucial to obtain good photometry. This paper introduces BKGnet, a deep
    neural network to predict the background and its associated error. BKGnet has
    been developed for data from the Physics of the Accelerating Universe Survey (PAUS),
    an imaging survey using a 40 narrow-band filter camera (PAUCam). Images obtained
    with PAUCam are affected by scattered light: an optical effect consisting of light
    multiply reflected that deposits energy in specific detector regions contaminating
    the science measurements. Fortunately, scattered light is not a random effect,
    but it can be predicted and corrected for. We have found that BKGnet background
    predictions are very robust to distorting effects, while still being statistically
    accurate. On average, the use of BKGnet improves the photometric flux measurements
    by $7\%$ and up to $20\%$ at the bright end. BKGnet also removes a systematic
    trend in the background error estimation with magnitude in the $i$-band that is
    present with the current PAU data management method. With BKGnet, we reduce the
    photometric redshift outlier rate by $35\%$ for the best $20\%$ galaxies selected
    with a photometric quality parameter.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何成像调查中，准确测量天文背景光对获得良好的光度测量至关重要。本文介绍了BKGnet，一个用于预测背景及其相关误差的深度神经网络。BKGnet是为加速宇宙物理调查（PAUS）的数据开发的，这是一项使用40个窄带滤光器相机（PAUCam）的成像调查。用PAUCam获得的图像受散射光的影响：这是一种光经过多次反射后，在特定探测器区域沉积能量的光学效应，从而污染科学测量。幸运的是，散射光不是随机现象，而是可以预测和校正的。我们发现BKGnet背景预测对畸变效应非常鲁棒，同时仍然统计上准确。平均而言，使用BKGnet可以将光度通量测量提高$7\%$，在亮度较高的端点提高最多$20\%$。BKGnet还消除了当前PAU数据管理方法在$i$-带光度中存在的背景误差估计的系统性趋势。通过BKGnet，我们将最佳$20\%$星系的光度红移异常值率降低了$35\%$。
- en: 'keywords:'
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'techniques: photometric – light pollution – instrumentation: photometers^†^†pubyear:
    2019^†^†pagerange: The PAU Survey: Background light estimation with deep learning
    techniques–[B](#A2 "Appendix B Variable annulus ‣ The PAU Survey: Background light
    estimation with deep learning techniques")'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 技术：光度测量 – 光污染 – 仪器：光度计^†^†出版年份：2019^†^†页码范围：PAU调查：使用深度学习技术进行背景光估计–[B](#A2 "附录B
    可变环 ‣ PAU调查：使用深度学习技术进行背景光估计")
- en: 1 Introduction
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: \textcolor
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackThe positions, fluxes and other properties of galaxies and stars can be
    determined by analysing images of the sky. Modern imaging surveys can cover large
    areas of sky efficiently, resulting in measurements for large numbers of galaxies
    to faint magnitudes (e.g. DES (Abbott et al., [2018](#bib.bib1))). Improving the
    accuracy of the measurements is crucial for future weak lensing surveys, e.g.
    LSST and Euclid (Ivezić et al., [2019](#bib.bib23); Laureijs et al., [2011](#bib.bib29)),
    to ensure that the results are not dominated by systematic errors.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析天空图像，可以确定星系和恒星的位置、通量及其他属性。现代成像调查能够高效地覆盖大面积的天空，从而为大量星系提供微弱的光度测量（例如，DES (Abbott
    et al., [2018](#bib.bib1)))。提高测量的准确性对于未来的弱引力透镜调查至关重要，例如LSST和Euclid (Ivezić et
    al., [2019](#bib.bib23); Laureijs et al., [2011](#bib.bib29))，以确保结果不会被系统误差主导。
- en: \textcolor
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: 'blackFor imaging surveys, accurate flux measurements are essential: they are
    used to select samples of galaxies, or to infer their physical properties. A key
    step towards a reliable flux estimate is the determination of the background,
    which needs to be subtracted. The main source of background is the brightness
    of the night sky, which may vary due to a range of effects, such as illumination
    by the Moon, airglow and light pollution. Instrumental effects can contribute
    as well, and in this paper we focus on scattered light, which is the result of
    light deflecting from the instrument optical path appearing at a different region
    of the detector (Romanishin, [2014](#bib.bib41)).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于成像调查，准确的通量测量至关重要：它们用于选择星系样本或推断它们的物理属性。可靠的通量估计的一个关键步骤是背景的确定，需要将其扣除。背景的主要来源是夜空的亮度，这可能由于月亮照射、空气辉光和光污染等一系列因素而变化。仪器效应也可能有所贡献，在这篇文章中，我们重点关注散射光，它是光从仪器光学路径偏折后出现在探测器的不同区域（Romanishin,
    [2014](#bib.bib41)）。
- en: \textcolor
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackDifferent approaches have been used to estimate the sky background (Bijaoui,
    [1980](#bib.bib4); Newell, [1983](#bib.bib34)), and example implementations include
    DAOPHOT (Stetson, [1987](#bib.bib43)) and SExtractor (Bertin & Arnouts, [1996](#bib.bib3)).
    DAOPHOT measures the background as the mode of the uniformly scattered pixels
    at a certain Full Width at Half Maximum (FWHM) of the given target source. On
    the other hand, SExtractor meshes the background and reconstructs a ’background
    map’ with the background estimated at each particular mesh location. Other methods
    aim to be more robust in the presence of nearby sources. In Teeninga et al. ([2015](#bib.bib45))
    the background is estimated at a location without nearby sources, while Popowicz
    & Smolka ([2015](#bib.bib40)), is based on the removal of small objects and an
    interpolation of missing pixels. In this paper we propose a new approach based
    on a deep neural network to predict the background and its associated error.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的方法已经被用来估计天空背景（Bijaoui，[1980](#bib.bib4)；Newell，[1983](#bib.bib34)），示例实现包括DAOPHOT（Stetson，[1987](#bib.bib43)）和SExtractor（Bertin
    & Arnouts，[1996](#bib.bib3)）。DAOPHOT将背景测量为给定目标源在某一半高宽（FWHM）的均匀散布像素的众数。另一方面，SExtractor则通过背景网格化，在每个特定网格位置估计背景，并重建“背景图”。其他方法则在存在附近源的情况下力求更为稳健。在Teeninga等（[2015](#bib.bib45)）中，背景是在没有附近源的位置估计的，而Popowicz
    & Smolka（[2015](#bib.bib40)）则基于小物体的去除和缺失像素的插值。在本文中，我们提出了一种基于深度神经网络的新方法来预测背景及其相关误差。
- en: \textcolor
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackOver the last few years, deep learning algorithms have resulted in revolutionary
    advances in machine learning and computer vision (Voulodimos et al., [2018](#bib.bib49)).
    Theoretical breakthroughs in training deep Neural Networks (NN) (Werbos, [1982](#bib.bib50)),
    or Convolutional Neural Networks (CNN) (LeCun et al., [1989](#bib.bib30); Lecun
    et al., [1998](#bib.bib31); Zeiler & Fergus, [2013a](#bib.bib52)) together with
    powerful and efficient parallel computing provided by Graphics Processing Units
    (GPUs) (Krizhevsky et al., [2012](#bib.bib27)) have lead to groundbreaking improvements
    across a variety of applications. The number of deep learning projects in cosmology
    is quickly increasing. This includes astronomical object classification (Carrasco-Davis
    et al., [2018](#bib.bib8); Cabayol et al., [2019](#bib.bib7)), (Cabayol et al.,
    [2019](#bib.bib7)), gravitational wave detection (George & Huerta, [2018](#bib.bib19))
    and directly constraining cosmological parameters from mass maps (Fluri et al.,
    [2018](#bib.bib17); Herbel et al., [2018a](#bib.bib20)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，深度学习算法在机器学习和计算机视觉领域取得了革命性的进展（Voulodimos 等，[2018](#bib.bib49)）。训练深度神经网络（NN）的理论突破（Werbos，[1982](#bib.bib50)），或卷积神经网络（CNN）（LeCun
    等，[1989](#bib.bib30)；Lecun 等，[1998](#bib.bib31)；Zeiler & Fergus，[2013a](#bib.bib52)），以及图形处理单元（GPU）提供的强大而高效的并行计算（Krizhevsky
    等，[2012](#bib.bib27)）都导致了各种应用领域的突破性改进。深度学习项目在宇宙学中的数量迅速增加。这包括天体分类（Carrasco-Davis
    等，[2018](#bib.bib8)；Cabayol 等，[2019](#bib.bib7)），引力波检测（George & Huerta，[2018](#bib.bib19)）和直接从质量图约束宇宙学参数（Fluri
    等，[2018](#bib.bib17)；Herbel 等，[2018a](#bib.bib20)）。
- en: \textcolor
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackExtracting the source photometry requires a significant amount of data
    engineering and parameter tweaking. This can be particularly challenging for noisy
    sources. Deep learning has already been successfully applied to different steps
    in source photometry extraction. Examples include point source detection (Vafaei
    Sadr et al., [2019](#bib.bib48)), cosmic ray detection (Zhang & Bloom, [2019](#bib.bib54))
    or Point Spread Function (PSF) modelling (Herbel et al., [2018b](#bib.bib21)).
    Deep learning has also been used to directly estimate photometric redshifts from
    images (D’Isanto & Polsterer, [2018](#bib.bib14); Pasquet et al., [2019](#bib.bib37)).
    These algorithms implicitly include steps for the background subtraction. Understanding
    these image processing steps can optimize the performance of e.g. redshift estimation
    and galaxy classification.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 提取源光度需要大量的数据工程和参数调整。这对于噪声源尤其具有挑战性。深度学习已经成功应用于源光度提取的不同步骤。例如，包括点源检测（Vafaei Sadr
    等，[2019](#bib.bib48)）、宇宙射线检测（Zhang & Bloom，[2019](#bib.bib54)）或点扩散函数（PSF）建模（Herbel
    等，[2018b](#bib.bib21)）。深度学习还被用于直接从图像中估计光度红移（D’Isanto & Polsterer，[2018](#bib.bib14)；Pasquet
    等，[2019](#bib.bib37)）。这些算法隐式地包括了背景减除步骤。了解这些图像处理步骤可以优化例如红移估计和星系分类的性能。
- en: \textcolor
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackOur goal is to develop and test a deep learning background subtraction
    method using data from the Physics of the Accelerating Universe Survey (PAUS).
    PAUS is an imaging survey that measures high precision photo-$z$s to faint magnitudes
    ($i_{\rm AB}<22.5$), while covering a large area of sky (Martí et al., [2014](#bib.bib33)).
    This is possible thanks to the PAUCam instrument (Castander et al., [2012](#bib.bib11);
    Padilla et al., [2016](#bib.bib35), [2019](#bib.bib36)), an optical camera equipped
    with 40 narrow bands (NB) covering a wavelength range from 450nm to 850nm (Casas
    et al., [2016](#bib.bib10)). PAUS reaches a photo-$z$ precision $\sigma(z)/(1+z)\sim
    0.0035$ for the best 50% of the sample, compared to typical precision of 0.05
    for broad band measurements (Eriksen et al., [2019](#bib.bib16)). The scientific
    goals of PAUS include the measurement of intrinsic alignments of galaxies out
    to $z\sim 0.75$, the study of their spectral energy distributions (SEDs), detailed
    studies of intermediate-scale cosmic structure (Stothert et al., [2018](#bib.bib44)),
    and improvements of image simulations (Tortorelli et al., [2018](#bib.bib47)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是使用加速宇宙调查（PAUS）数据开发和测试一种深度学习背景减除方法。PAUS 是一个成像调查，它测量高精度的光度红移（photo-$z$）至微弱的光度（$i_{\rm
    AB}<22.5$），同时覆盖了较大的天空区域（Martí et al., [2014](#bib.bib33)）。这是由于 PAUCam 仪器（Castander
    et al., [2012](#bib.bib11); Padilla et al., [2016](#bib.bib35), [2019](#bib.bib36)）的支持，该仪器是一台光学相机，配备有40个窄带（NB），覆盖了450nm到850nm的波长范围（Casas
    et al., [2016](#bib.bib10)）。PAUS 对于样本中最好的50%实现了光度红移精度 $\sigma(z)/(1+z)\sim 0.0035$，相比之下，宽带测量的典型精度为0.05（Eriksen
    et al., [2019](#bib.bib16)）。PAUS 的科学目标包括测量红移 $z\sim 0.75$ 的星系内在排列、研究它们的光谱能量分布（SEDs）、对中等规模的宇宙结构进行详细研究（Stothert
    et al., [2018](#bib.bib44)），以及改进图像模拟（Tortorelli et al., [2018](#bib.bib47)）。
- en: \textcolor
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackPAUS imaged the COSMOS field as a calibration area given the availability
    of spectroscopic redshifts. The PAUS photo-$z$ catalogue for the full COSMOS sample
    with $i_{\rm AB}<22.5$ contains outliers when compared to the spectroscopic redshifts.
    Some of these outliers simply arise from noisy photometry, but others are due
    to a strongly varying continuum produced by scattered light. The excess scattered
    light decreases the signal-to-noise ratio (SNR) of the photometric measurement
    and also alters the statistics of the values of the pixels from which the continuum
    is estimated. These effects can potentially bias the flux measurements.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PAUS 对 COSMOS 区域进行了成像，以作为校准区域，考虑到光谱红移的可用性。与光谱红移相比，PAUS 光度红移目录（$i_{\rm AB}<22.5$）包含一些异常值。这些异常值有些是由于光度测量噪声引起的，但其他一些则是由于散射光产生的强烈变化的连续谱造成的。多余的散射光降低了光度测量的信噪比（SNR），并且还改变了估计连续谱的像素值的统计数据。这些效应可能会对流量测量造成偏差。
- en: In this paper, we present BKGnet, a convolutional neural network capable of
    learning the underlying behaviour of scattered light and other distorting effects
    present in the PAUCam images. BKGnet predicts the background at the location of
    the target sources and the error associated with the background prediction. Although
    it is built to improve the PAUS photometry, BKGnet can be also be applied to other
    future imaging surveys such as LSST (Ivezić et al., [2019](#bib.bib23)) and Euclid
    (Laureijs et al., [2011](#bib.bib29)). The code is available at [https://gitlab.pic.es/pau/bkgnet](https://gitlab.pic.es/pau/bkgnet).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 BKGnet，这是一种卷积神经网络，能够学习 PAUCam 图像中散射光和其他扭曲效应的基本行为。BKGnet 预测目标源位置的背景以及与背景预测相关的误差。虽然它旨在改善
    PAUS 的光度测量，BKGnet 也可以应用于未来的其他成像调查，例如 LSST（Ivezić et al., [2019](#bib.bib23)）和
    Euclid（Laureijs et al., [2011](#bib.bib29)）。代码可以在 [https://gitlab.pic.es/pau/bkgnet](https://gitlab.pic.es/pau/bkgnet)
    上获得。
- en: The structure of this paper is as follows. In section 2, we describe the PAU
    Survey and the PAUCam camera and present the modelling of scattered light using
    scattered-light templates. In section 3, we introduce the specific network we
    have developed, as well as defining the training and testing process. Sections
    4 and 5 contain the results obtained for simulated and real PAUCam images, respectively.
    In section 6, we validate the network predictions on real target locations and
    we conclude and summarise in section 7.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的结构如下。在第2节中，我们描述了 PAU 调查和 PAUCam 相机，并介绍了使用散射光模板的散射光建模。在第3节中，我们介绍了我们开发的具体网络，并定义了训练和测试过程。第4节和第5节分别包含了对模拟和真实
    PAUCam 图像获得的结果。在第6节中，我们验证了网络对真实目标位置的预测，并在第7节中总结并得出结论。
- en: 2 Modelling scattered light
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 散射光建模
- en: PAUCam images contain substantial amounts of scattered light, which mostly affect
    the edge regions of some CCDs. Scattered light increases the amount of background
    in the affected regions and distorts the expected statistics of the pixel values
    used to estimate the background. Therefore, the scattered light present in PAUCam
    images can lead to an incorrect estimate of the background if not properly modelled,
    thus biasing the photometry. Moreover the elevated background lowers the SNR of
    the measurements. In 2016 the PAU camera was modified in order to mitigate the
    effect of scattered light by introducing baffles on all the edges of the NB filters
    of each filter tray. Although this reduced the amount of scattered light, residuals
    still remain. In the latest COSMOS data reduction, around 8% of exposures taken
    before the camera intervention are flagged as affected by scattered light, and
    therefore dismissed. After the intervention, this number reduced to 5% of the
    exposures, such that on average $7\%$ of data in the COSMOS field are lost due
    to scattered light. In this section, we present the PAUCam scattered light model
    we are using throughout the paper.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PAUCam图像包含大量散射光，这些散射光主要影响某些CCD的边缘区域。散射光增加了受影响区域的背景量，并扭曲了用于估计背景的像素值的预期统计数据。因此，PAUCam图像中存在的散射光如果没有得到正确建模，可能会导致背景估计不准确，从而偏倚光度测量。此外，背景的升高降低了测量的信噪比（SNR）。2016年，PAU相机被修改以减轻散射光的影响，通过在每个滤镜托盘的所有边缘引入挡板。虽然这减少了散射光的量，但仍然存在残留。在最新的COSMOS数据处理中，约8%的相机干预前的曝光被标记为受散射光影响，因此被排除。在干预后，这一数字减少到5%的曝光，因此COSMOS区域中平均有$7\%$的数据由于散射光而丢失。在这一部分，我们展示了在整篇论文中使用的PAUCam散射光模型。
- en: 2.1 The PAUS observations
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 PAUS观测
- en: PAUS has been observing since the 2015B semester and as of 2019A, PAUS has taken
    data for 160 nights. The current data covers 10 deg² of the CFHTLS fields¹¹1http://www.cfht.hawaii.edu/Science/CFHTLS_Y_WIRCam
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: PAUS自2015B学期开始观测，截至2019A，PAUS已采集数据160个夜晚。目前的数据覆盖了CFHTLS区域的10平方度¹¹1http://www.cfht.hawaii.edu/Science/CFHTLS_Y_WIRCam
- en: /cfhtlsdeepwidefields.html W1, W2; 20 deg² in W3 and 2 deg² of the COSMOS field²²2http://cosmos.astro.caltech.edu/.
    The PAUS data are stored at the Port d’Informació Científica (PIC), where the
    data are processed and distributed (Tonello et al., [2019](#bib.bib46)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: /cfhtlsdeepwidefields.html W1, W2; W3中20平方度，COSMOS区域中2平方度²²2http://cosmos.astro.caltech.edu/。PAUS数据存储在Port
    d’Informació Científica（PIC），数据在此处处理和分发（Tonello等，[2019](#bib.bib46)）。
- en: In this paper we focus only on the data from the COSMOS field, which were taken
    in the semesters 2015B, 2016A, 2016B and 2017B (the low efficiency was caused
    by bad weather). The COSMOS field observations comprise a total of 9749 images,
    343 images for each NB. From these images, 4928 were taken before the camera intervention
    and 4821 after. The basic exposure times in the COSMOS field are 70, 80, 90, 110
    and 130 seconds from the bluest to the reddest filter trays.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们仅关注COSMOS区域的数据，这些数据采集于2015B、2016A、2016B和2017B学期（低效率是由于天气不好造成的）。COSMOS区域的观测包含总共9749张图像，每个NB有343张图像。在这些图像中，4928张是在相机干预前拍摄的，4821张是在干预后拍摄的。COSMOS区域的基本曝光时间从最蓝的滤镜到最红的滤镜分别为70、80、90、110和130秒。
- en: 2.2 PAUS images processing
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 PAUS图像处理
- en: \textcolor
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackThe PAUCam instrument (Castander et al., [2012](#bib.bib11); Padilla et al.,
    [2016](#bib.bib35), [2019](#bib.bib36)) is an optical camera equipped with 40
    narrow bands (NB), covering a wavelength range from 450nm to 850nm (Casas et al.,
    [2016](#bib.bib10)). The NB filters have 13nm FWHM and a separation between consecutive
    bands of 10nm. The camera is also equipped with $ugrizY$ broad band filters that
    so far have been mainly used by external observers. The camera has 18 red-sensitive
    fully depleted Hamamatsu CCD detectors (Casas et al., [2012](#bib.bib9)), although
    only the 8 central CCDs are used for NB imaging. Each CCD has 4096x2048 pixels
    with a pixel scale of 0.26 arcsec/pix. The NB filter set effectively measures
    a high resolution photometric spectrum ($R\approx 50$).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: blackPAUCam仪器（Castander等，[2012](#bib.bib11)；Padilla等，[2016](#bib.bib35)，[2019](#bib.bib36)）是一种光学相机，配备了40个窄带滤镜（NB），覆盖从450nm到850nm的波长范围（Casas等，[2016](#bib.bib10)）。NB滤镜的全宽半高（FWHM）为13nm，相邻带的分隔为10nm。该相机还配备了$ugrizY$广带滤镜，这些滤镜目前主要由外部观测者使用。相机具有18个红敏感的完全耗尽的Hamamatsu
    CCD探测器（Casas等，[2012](#bib.bib9)），尽管仅使用8个中央CCD进行NB成像。每个CCD具有4096x2048像素，像素尺度为0.26弧秒/像素。NB滤镜组有效地测量高分辨率光度光谱（$R\approx
    50$）。
- en: \textcolor
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackIn this project we use images that have already been corrected for various
    instrumental effects (Serrano et al., [prep](#bib.bib42)) in the PAUS nightly
    pipeline. This pipeline performs basic instrumental de-trending processing. The
    electronic effects are corrected using a master bias, which is an observation
    with the shutter closed and zero exposure time. To correct pixel-to-pixel variations
    we use dome flats, which are obtained by imaging a uniformly illuminated screen.
    The astrometry of the narrow band images is calibrated comparing to the positions
    of GAIA DR2 stars (Gaia Collaboration et al., [2018](#bib.bib18)). The photometry
    calibration is done relative to SDSS stars (Castander et al., [prep](#bib.bib12)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们使用的图像已经过各种仪器效应的校正（Serrano 等，[prep](#bib.bib42)），这是 PAUS 每晚管道的一部分。这个管道执行基本的仪器去趋势处理。电子效应通过使用主偏差进行校正，主偏差是一个快门关闭且曝光时间为零的观测。为了校正像素到像素的变化，我们使用圆顶平场，这些平场是通过对均匀照明屏幕进行成像获得的。窄带图像的天文学测定通过与
    GAIA DR2 恒星的位置比较进行校准（Gaia 合作组等，[2018](#bib.bib18)）。光度校准是相对于 SDSS 恒星进行的（Castander
    等，[prep](#bib.bib12)）。
- en: Once the images have been corrected, we perform forced photometry to extract
    the galaxy flux. The current PAUdm pipeline, similarly to DAOPHOT, predicts the
    background noise as the median of the pixels within a ring placed around the target
    source. However, this algorithm requires a (fairly) flat background for an accurate
    estimate. This assumption breaks down when either the annulus or source extraction
    regions are affected by scattered light. In addition, other sources of errors
    in the background estimation are undetected sources, cosmic rays or cross-talk.
    In order to minimize the effect of any of these artifacts, the pixels inside the
    annulus are 3-$\sigma$ clipped before computing the median.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦图像经过校正，我们进行强制光度测量以提取星系光 flux。目前的 PAUdm 管道类似于 DAOPHOT，通过在目标源周围放置一个环形区域来预测背景噪声的中位值。然而，该算法需要（相当）平坦的背景才能进行准确的估计。当环形区域或源提取区域受散射光影响时，这一假设就会失效。此外，背景估计中的其他错误来源包括未检测到的源、宇宙射线或串扰。为了最小化这些伪影的影响，在计算中位数之前，环形区域内的像素会进行
    3-$\sigma$ 剪切。
- en: \textcolor
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: 'blackThe default PAUdm radii for the annulus region are $r_{\rm in}=30$ and
    $r_{\rm out}=45$ pixels (Serrano et al., [prep](#bib.bib42)). The annulus is selected
    to be sufficiently far away from the galaxy to avoid light leaking inside the
    ring and not too far so that the background is representative. Throughout this
    paper we use the default configuration to compare this commonly used approach
    to our deep learning algorithm. However, in Appendix [B](#A2 "Appendix B Variable
    annulus ‣ The PAU Survey: Background light estimation with deep learning techniques")
    we study the effect of a variable annulus.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 PAUdm 环形区域半径为 $r_{\rm in}=30$ 和 $r_{\rm out}=45$ 像素（Serrano 等，[prep](#bib.bib42)）。选择的环形区域距离星系足够远，以避免光线泄漏到环内，同时又不至于过远，以保证背景具有代表性。在本文中，我们使用默认配置将这种常用方法与我们的深度学习算法进行比较。然而，在附录
    [B](#A2 "附录 B 变量环形 ‣ PAU 调查：利用深度学习技术进行背景光估计") 中，我们研究了变量环形的影响。
- en: 2.3 Scattered-light templates
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 散射光模板
- en: '![Refer to caption](img/da5fe663a74a2aae12aa70660be88b5d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/da5fe663a74a2aae12aa70660be88b5d.png)'
- en: 'Figure 1: Images taken with the PAUCam, corresponding to the NB685 filter.
    *Left:* The first two images correspond to PAUCam images before the camera intervention.
    Notice that both exhibit the same scattered light pattern. *Right:* The two images
    on the right correspond to PAUCam images after the intervention. Again, both present
    the same scattered light pattern, but different to the first two images on the
    left. This shows the changes in scattered light patterns with the intervention.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：使用 PAUCam 拍摄的图像，对应于 NB685 滤光片。*左侧:* 前两个图像对应于相机干预之前的 PAUCam 图像。请注意，它们都显示了相同的散射光模式。*右侧:*
    右侧的两个图像对应于干预之后的 PAUCam 图像。同样，两者展示了相同的散射光模式，但与左侧的前两个图像不同。这显示了干预后散射光模式的变化。
- en: '![Refer to caption](img/698fa706544376b0009f25a7669a5720.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/698fa706544376b0009f25a7669a5720.png)'
- en: 'Figure 2: *Top*: Normalised background light content in each pixel as a function
    of the pixel position in the image for different images before (black dashed line)
    and after (orange solid line) the camera intervention. Each pixel value is divided
    by the mean background in the image. Regions without scattered light should fluctuate
    around unity. Regions affected by scattered light should be above unity. *Bottom*:
    Mean value of the normalised background curves considering all the images taken
    in that band, for the 40 narrow photometric bands.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：*上*：不同图像中每个像素的归一化背景光内容与图像中像素位置的函数关系，在相机干预前（黑色虚线）和干预后（橙色实线）。每个像素值除以图像中的平均背景。没有散射光的区域应该波动在单位附近。受散射光影响的区域应该高于单位。*下*：考虑到在该波段拍摄的所有图像的归一化背景曲线的均值，对于40个窄光度波段。
- en: 'Figure [1](#S2.F1 "Figure 1 ‣ 2.3 Scattered-light templates ‣ 2 Modelling scattered
    light ‣ The PAU Survey: Background light estimation with deep learning techniques")
    shows four PAUCam images in the NB filter NB685 before the camera intervention
    (first and second images on the left) and after the camera intervention (third
    and fourth images). They show scattered light near the edges of the CCD, displaying
    a spatially varying amount of scattered light. The scattered light patterns change
    from before the intervention (two images on the left) to after (two images on
    the right). The scattered light pattern is also filter dependent. The images taken
    in each filter show their own distinctive patterns, meaning that the pattern depends
    on the filter used.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](#S2.F1 "图 1 ‣ 2.3 散射光模板 ‣ 2 模型化散射光 ‣ PAU 调查：利用深度学习技术估计背景光")展示了在相机干预之前（左边的第一和第二张图像）和相机干预之后（第三和第四张图像）NB685滤镜下的四张PAUCam图像。它们展示了CCD边缘附近的散射光，显示出空间变化的散射光量。散射光模式在干预前（左边的两张图像）和干预后（右边的两张图像）有所变化。散射光模式也依赖于滤镜。每个滤镜拍摄的图像展示了各自独特的模式，这意味着模式依赖于使用的滤镜。
- en: 'One way to quantify and model the scattered light is to create background pixel
    maps per NB. This is done with the following steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 定量和建模散射光的一种方法是为每个NB创建背景像素图。这是通过以下步骤完成的：
- en: 'i. *Select images*:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: i. *选择图像*：
- en: Select a group of NB images from the same bands, since they have the same scattered
    light pattern.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从相同波段选择一组NB图像，因为它们具有相同的散射光模式。
- en: 'ii. *Compute median*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ii. *计算中位数*：
- en: For each of the images, compute the median background level in the central regions,
    $\mu_{\rm BKG}$, which are unaffected by scattered light.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每张图像，计算中央区域的中位背景水平，$\mu_{\rm BKG}$，这些区域不受散射光影响。
- en: 'iii. *Estimate ratios*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: iii. *估计比例*：
- en: Divide every image by its median to obtain a pixel ratio map.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将每张图像除以其中位数，以获得像素比例图。
- en: 'iv. *Mask sources*:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: iv. *掩蔽源*：
- en: Mask the images sources by masking all pixels above a given pixel ratio threshold.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过掩蔽所有超过给定像素比例阈值的像素来掩蔽图像源。
- en: 'v. *Combine images*:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: v. *合并图像*：
- en: Combine all individual pixels maps with a median to get a single scattered-light
    template (SLT) for all the selected images.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有单个像素图与中位数结合，以获得所有选定图像的单一散射光模板（SLT）。
- en: If the background were flat and followed Poisson statistics, all pixels in the
    ratio map should fluctuate along unity. However, if the image is affected by scattered
    light, the scattered-light templates in affected regions will have a value above
    unity. We can understand this ratio as approximately the percentage of extra light
    (scattered light) compared to the flat background. Notice that this model takes
    into account that scattered light depends on the amount of light falling on the
    CCD.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果背景是平坦的并且遵循泊松统计，比例图中的所有像素应该围绕单位波动。然而，如果图像受到散射光影响，受影响区域的散射光模板将具有高于单位的值。我们可以将这个比例理解为相对于平坦背景的额外光（散射光）的百分比。注意，这个模型考虑到散射光依赖于照射到CCD上的光量。
- en: The procedure in step (v) can be written as
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤（v）的过程可以写成
- en: '|  | ${\rm SLT}(x,y)=\text{median}_{j}\Big{[}\frac{I_{j}(x,y)}{\mu_{\rm BKG}}\Big{]},$
    |  | (1) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\rm SLT}(x,y)=\text{median}_{j}\Big{[}\frac{I_{j}(x,y)}{\mu_{\rm BKG}}\Big{]},$
    |  | (1) |'
- en: where $I_{j}$ is image $j$ and the median is over the selected images (step
    i). To determine the amount of scattered light we can follow the previous procedure
    to step [iii]. This way we obtain normalised background images that should fluctuate
    around unity if they contain a flat background, but would have values above one
    if they are affected by scattered light.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{j}$ 是图像 $j$，中位数是在选择的图像上计算的（步骤 i）。为了确定散射光的量，我们可以按照之前的步骤继续到步骤 [iii]。这样，我们可以获得归一化的背景图像，如果这些图像包含平坦的背景，它们的波动应该围绕单位值，但如果受到散射光的影响，它们的值会超过一。
- en: 'The Figure [2](#S2.F2 "Figure 2 ‣ 2.3 Scattered-light templates ‣ 2 Modelling
    scattered light ‣ The PAU Survey: Background light estimation with deep learning
    techniques") top panel shows some of these normalised images for the NB685 filter.
    It shows the background pixel value from side to side of the image before (black
    dashed line) and one after (orange solid line) the camera intervention. The plot
    shows an increasing background on the edges of the CCD before the camera intervention.
    After the intervention, the amount of scattered light is considerably reduced.
    Unfortunately it is still present and thus needs to be accounted for.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S2.F2 "图 2 ‣ 2.3 散射光模板 ‣ 2 散射光建模 ‣ PAU 调查：使用深度学习技术进行背景光估计") 的顶部面板显示了
    NB685 滤光器的一些归一化图像。它显示了图像边缘的背景像素值，在相机干预前（黑色虚线）和干预后（橙色实线）。该图表显示了在相机干预前，CCD 边缘背景的增加。干预后，散射光量显著减少。不幸的是，它仍然存在，因此需要考虑。
- en: 'We can use all the normalised background images in a given NB to create a general
    scattered-light template for that band (also splitting before/after the intervention).
    The bottom panel in Figure [2](#S2.F2 "Figure 2 ‣ 2.3 Scattered-light templates
    ‣ 2 Modelling scattered light ‣ The PAU Survey: Background light estimation with
    deep learning techniques") shows the resulting mean of each scattered-light templates
    (one per band) as a function of NB. The mean of the scattered-light templates
    gives information about the amount of scattered light in a given band. We can
    clearly see the effect of the intervention on the amount of scattered light, which
    is reduced.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用给定 NB 中的所有归一化背景图像来创建该波段的一般散射光模板（也可以在干预前后进行拆分）。图 [2](#S2.F2 "图 2 ‣ 2.3
    散射光模板 ‣ 2 散射光建模 ‣ PAU 调查：使用深度学习技术进行背景光估计") 的底部面板显示了每个散射光模板（每个波段一个）在 NB 上的平均值。散射光模板的平均值提供了关于给定波段散射光量的信息。我们可以清楚地看到干预对散射光量的影响，散射光量减少了。
- en: 2.4 Scattered-light templates as scattered light correcting method
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 散射光模板作为散射光校正方法
- en: If the scattered-light templates modelling is sufficiently accurate, it can
    be used to correct the scattered light on PAUCam images. Assuming that all the
    images from a given NB follow the same scattered light pattern scaled by the CCD
    sky background, a way of correcting scattered light would be
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果散射光模板建模足够准确，它可以用于校正 PAUCam 图像上的散射光。假设来自给定 NB 的所有图像遵循相同的散射光模式，并由 CCD 天空背景进行缩放，校正散射光的一种方法是
- en: '|  | $\tilde{I}(x,y)=I(x,y)-(\text{SLT}(x,y)-1)\mu_{\rm BKG},$ |  | (2) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{I}(x,y)=I(x,y)-(\text{SLT}(x,y)-1)\mu_{\rm BKG},$ |  | (2) |'
- en: where we subtract from a given target image ($I(x,y)$) the scattered-light templates
    scaled by the mean background of such image ($\mu_{\rm BKG}$). Notice that instead
    of subtracting the scattered-light templates, we subtract the scattered-light
    templates without the flat sky background. This way, the regions without scattered
    light are barely affected.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从给定的目标图像 ($I(x,y)$) 中减去按该图像的平均背景 ($\mu_{\rm BKG}$) 缩放的散射光模板。注意，我们不是减去散射光模板，而是减去没有平坦天空背景的散射光模板。这样，没有散射光的区域几乎不会受到影响。
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2.4 Scattered-light templates as scattered light
    correcting method ‣ 2 Modelling scattered light ‣ The PAU Survey: Background light
    estimation with deep learning techniques") shows the original CCD image (left),
    after correcting with the scattered-light templates (middle) and the scattered-light
    templates used for correction (right). Visually, the scattered light pattern in
    the original image (left) disappears after applying the scattered-light templates
    correction (middle). However, although the correction seems visually almost perfect,
    this method has a drawback. Even though scattered light follows approximately
    a pattern given a band, there might be fluctuations due to other external conditions.
    For example, the weather, Moon illumination and other observing conditions may
    induce variations between different observations in a NB. To be more precise estimating
    the correction, one should create a template per band and per night, such that
    the observing conditions are similar. However, for creating a scattered-light
    templates per night, there might be a insufficient number of images to obtain
    an accurate modelling of the correction pattern. Bright stars also contribute
    to scattered light and this cannot be corrected with the scattered-light templates.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S2.F3 "图3.2.4散射光模板作为散射光校正方法——2建模散射光——PAU调查：利用深度学习技术估计背景亮度") 显示了原始 CCD
    图像（左），经过散射光模板校正后的图像（中）以及用于校正的散射光模板（右）。从视觉上看，原始图像（左）中的散射光图案在应用散射光模板校正后（中）消失了。然而，尽管校正在视觉上几乎完美，但该方法有一个缺点。尽管散射光在给定频段下近似遵循一种模式，但由于其他外部条件的影响，可能会出现波动。例如，天气、月亮照明和其他观测条件可能导致不同观测之间的变化。为了更精确地估计校正，应该针对每个频带和每个夜晚创建一个模板，以确保观测条件相似。然而，对于每晚创建散射光模板，可能图像数量不足以获得准确的校正模式建模。亮星也会对散射光产生影响，而散射光模板无法对其进行校正。
- en: 'Figure [4](#S2.F4 "Figure 4 ‣ 2.4 Scattered-light templates as scattered light
    correcting method ‣ 2 Modelling scattered light ‣ The PAU Survey: Background light
    estimation with deep learning techniques") shows the background level for a specific
    image in the NB685 filter before and after the correction with the scattered-light
    templates. In this case, the image is corrected without considering any split
    on night to generate the scattered-light templates. This means that all images,
    despite being observed on different night and with different observing conditions
    are used to build the scattered-light templates. The image without correction
    displays large peaks at both edges and those are clearly corrected by the scattered-light
    templates. However, both sides of the CCD still show bumps that are caused by
    scattered light residuals.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S2.F4 "图4.2.4散射光模板作为散射光校正方法——2建模散射光——PAU调查：利用深度学习技术估计背景亮度")显示了在NB685滤波器下对特定图像进行散射光模板校正前后的背景水平。在这种情况下，图像是在没有考虑每晚分割以生成散射光模板的情况下进行校正的。这意味着尽管这些图像在不同的夜晚和不同的观测条件下进行观测，但它们都被用来构建散射光模板。未经校正的图像在两侧都显示出大的峰值，这些峰值显然被散射光模板校正了。然而，CCD的两侧仍然显示出由散射光残留引起的凸起。
- en: '![Refer to caption](img/8b03bfe0cf4619190c82e515250ca97b.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8b03bfe0cf4619190c82e515250ca97b.png)'
- en: 'Figure 3: *Left:* Image taken in the NB685 filter showing a scattered light
    pattern on the edges. *Middle:* Previous image corrected with the scattered light
    template. *Right:* The scattered-light template generated with equation [1](#S2.E1
    "In 2.3 Scattered-light templates ‣ 2 Modelling scattered light ‣ The PAU Survey:
    Background light estimation with deep learning techniques") considering all images
    taken the same observation night as the original image.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：*左：*在NB685滤波器下拍摄的显示出边缘散射光图案的图像。*中：*使用散射光模板校正的前一图像。*右：*使用公式 [1](#S2.E1 "在2.3散射光模板——2建模散射光——PAU调查：利用深度学习技术估计背景亮度")
    生成的散射光模板，考虑了与原始图像相同观测夜晚的所有图像。
- en: '![Refer to caption](img/c115331c6522e7cb05203b1f9d783423.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c115331c6522e7cb05203b1f9d783423.png)'
- en: 'Figure 4: Background pixel values across the image. The original image (orange
    solid line) displays high peaks on the edges caused by scattered light. After
    correcting with the scattered-light templates (dashed black line) the peaks are
    reduced, but some residuals remain. The images are in e/s.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：图像中的背景像素值。原始图像（橙色实线）显示了由于散射光而在边缘产生的高峰。通过使用散射光模板进行修正（黑色虚线），峰值有所减少，但仍有一些残余。图像单位为
    e/s。
- en: '3 BKGnet: A Deep Learning based method to predict the background'
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 BKGnet：一种基于深度学习的方法来预测背景
- en: 'In this section we start by describing the BKGnet architecture. We then describe
    our training and test samples and describe the training process. As a reference,
    in Appendix [A](#A1 "Appendix A Convolutional Neural Networks ‣ The PAU Survey:
    Background light estimation with deep learning techniques") we introduce the basics
    of deep learning and convolutional neural networks, together with some terminology
    definitions.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先描述 BKGnet 架构。然后描述我们的训练和测试样本，并说明训练过程。作为参考，在附录 [A](#A1 "附录 A 卷积神经网络 ‣
    PAU 调查：利用深度学习技术进行背景光估计") 中，我们介绍了深度学习和卷积神经网络的基础知识，以及一些术语定义。
- en: 3.1 Neural network architecture
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 神经网络架构
- en: 'BKGnet³³3https://gitlab.pic.es/pau/bkgnet is built using the PyTorch library
    (Paszke et al., [2017](#bib.bib38)). It has two main blocks: a convolutional neural
    network (CNN) and a linear neural network.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: BKGnet³³3https://gitlab.pic.es/pau/bkgnet 是使用 PyTorch 库（Paszke 等，[2017](#bib.bib38)）构建的。它有两个主要模块：卷积神经网络（CNN）和线性神经网络。
- en: 'Figure [5](#S3.F5 "Figure 5 ‣ 3.2 Data: training and test samples ‣ 3 BKGnet:
    A Deep Learning based method to predict the background ‣ The PAU Survey: Background
    light estimation with deep learning techniques") shows the BKGnet architecture.
    The CNN block handles the information coming from the image itself, as the background
    we want to recover is encoded in the pixel values. The inputs are 120x120 pixels
    stamps containing the target galaxy in the center. \textcolorblackThis choice
    for the stamp size is a compromise between having enough pixels whilst keeping
    the computing requirements (memory, GPU) within reasonable limits.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5](#S3.F5 "图 5 ‣ 3.2 数据：训练和测试样本 ‣ 3 BKGnet：一种基于深度学习的方法来预测背景 ‣ PAU 调查：利用深度学习技术进行背景光估计")
    显示了 BKGnet 架构。CNN 块处理来自图像本身的信息，因为我们想要恢复的背景编码在像素值中。输入是包含目标星系在中心的 120x120 像素图像块。*黑色*
    选择这个图像块的大小是为了在保持计算需求（内存、GPU）在合理范围内的同时拥有足够的像素。
- en: \textcolor
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: 'blackAs Figure [5](#S3.F5 "Figure 5 ‣ 3.2 Data: training and test samples ‣
    3 BKGnet: A Deep Learning based method to predict the background ‣ The PAU Survey:
    Background light estimation with deep learning techniques") shows, the CNN contains
    5 blocks of convolutional layer (red layer), pooling layer (yellow layer) and
    batch normalization layer (blue layer). In each convolutional layer, the network
    learns to gradually capture different features in the image. The first layers
    learn low-level features, like edge detection, while having more layers leads
    the network to learn high-level features (Zeiler & Fergus, [2013b](#bib.bib53)).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: black如图 [5](#S3.F5 "图 5 ‣ 3.2 数据：训练和测试样本 ‣ 3 BKGnet：一种基于深度学习的方法来预测背景 ‣ PAU 调查：利用深度学习技术进行背景光估计")
    所示，CNN 包含 5 个卷积层（红色层）、池化层（黄色层）和批量归一化层（蓝色层）。在每个卷积层中，网络逐渐学习捕捉图像中的不同特征。第一层学习低级特征，如边缘检测，而更多的层则使网络学习高级特征（Zeiler
    & Fergus，[2013b](#bib.bib53)）。
- en: \textcolor
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackThe scattered light model depends on parameters that are not encoded in
    the stamps. These are the position of the stamp in the original image, the NB
    used to observe the galaxy and a before/after intervention flag informing the
    network when the galaxy was observed. We also include the target galaxy magnitude
    from a reference catalogue, as it contains information about the number of pixels
    that are affected by the galaxy. To help the network learning the scattered light
    patterns, the previously mentioned parameters are provided to the linear neural
    network, together with the CNN’s output. The magnitude of the galaxy and the coordinates
    of the image are added as fixed parameters per stamp.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: black散射光模型依赖于未编码在图章中的参数。这些参数包括图章在原始图像中的位置、用于观察星系的NB和一个在星系被观察时通知网络的前后干预标志。我们还包括来自参考目录的目标星系亮度，因为它包含有关受星系影响的像素数量的信息。为了帮助网络学习散射光模式，前面提到的参数与CNN的输出一起提供给线性神经网络。星系的亮度和图像的坐标作为每个图章的固定参数添加。
- en: \textcolor
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackThe NB filter and the intervention flag are discrete variables with forty
    possible values for the band (1-40) and two for the intervention flag (0/1). The
    combination of the two is, however, directly related to the scattered light pattern,
    and we can effectively convert these two discrete variables into a single one
    with values from 1 to 80\. We add the band and intervention information using
    an embedding. The embedding replaces each combination with 10 trainable parameters.
    Before embedding the band and intervention information, BKGnet learns to encode
    each scattered light pattern using the ten numbers that should best characterise
    the pattern. Therefore, the linear network receives the output from the convolution
    layers, two galaxy coordinates, the magnitude and ten numbers from the embedding.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: blackNB滤镜和干预标志是离散变量，带有带宽（1-40）的四十个可能值和干预标志（0/1）的两个值。然而，这两者的组合与散射光模式直接相关，我们可以有效地将这两个离散变量转换为一个值范围从1到80的变量。我们通过嵌入添加带宽和干预信息。嵌入将每个组合替换为10个可训练参数。在嵌入带宽和干预信息之前，BKGnet学习使用十个最能表征模式的数字来编码每个散射光模式。因此，线性网络接收来自卷积层的输出、两个星系坐标、亮度和来自嵌入的十个数字。
- en: '3.2 Data: training and test samples'
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据：训练和测试样本
- en: BKGnet’s inputs are stamps with the target galaxy in the center. However, to
    train the network we use empty CCD positions, meaning regions where there are
    not target sources. This way, we can estimate the ground truth background value
    at the central CCD region (where there is supposed to be a target galaxy) and
    train the network to recover this value. The estimation of the true background
    values used as training sample labels is done by computing the mean background
    inside a circular aperture of a given fixed radius in the central region of the
    stamp. Therefore, these measurements have an associated uncertainty that directly
    depends on the aperture radius. Assuming that the background is purely Poissonian,
    then
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: BKGnet的输入是目标星系位于中心的图章。然而，为了训练网络，我们使用空的CCD位置，即没有目标源的区域。这样，我们可以估计中心CCD区域（应该有一个目标星系）处的真实背景值，并训练网络恢复这个值。用作训练样本标签的真实背景值的估计是通过计算图章中心区域中给定固定半径的圆形孔径内的平均背景来完成的。因此，这些测量具有一个直接依赖于孔径半径的相关不确定性。假设背景是纯粹的泊松分布，则
- en: '|  | $\sigma^{2}_{\rm label}=\frac{N_{\rm a}b}{t_{\rm exp}},$ |  | (3) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma^{2}_{\rm label}=\frac{N_{\rm a}b}{t_{\rm exp}},$ |  | (3) |'
- en: where $t_{\rm exp}$ is the exposure time, $b$ is the background estimated as
    the mean of the pixels inside the aperture, i.e. the background label, and $N_{\rm
    a}$ is the number of pixels inside the circular aperture, directly related with
    the choice of aperture radius. We have fixed it to 8 pixels as a balance between
    the error of the ground truth measurement and having a precise background measurement
    in the exact galaxy location. To select empty stamps for the training sample we
    identify sources by cross-correlating the sky coordinates of a given image location
    with the sky coordinates of the sources in the COSMOS catalogue (Laigle et al.,
    [2016](#bib.bib28)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t_{\rm exp}$ 是曝光时间，$b$ 是估计的背景，作为孔径内像素的均值，即背景标签，而 $N_{\rm a}$ 是圆形孔径内的像素数，直接与孔径半径的选择相关。我们将其固定为
    8 像素，以在真实测量误差和在精确星系位置获得准确背景测量之间取得平衡。为了选择用于训练样本的空白印章，我们通过将给定图像位置的天空坐标与 COSMOS 目录中的源天空坐标进行交叉相关来识别源（Laigle
    等人，[2016](#bib.bib28)）。
- en: 'In any deep learning algorithm, the training and test samples should be as
    similar as possible. In our case, our training sample does not contain target
    galaxies whereas the test sample does. We therefore add simulated galaxies at
    the center of the empty training stamps. The simulated galaxies are constructed
    with parameters based on PAUS data: the Sersic profile parameters, $r_{\rm 50}$,
    $I_{\rm 50}$ and the magnitude in the $i$-band. The Sersic profile describes the
    surface brightness profile ($I$) of a galaxy. The radius $r$ that contains 50%
    of the light intensity ($I_{\rm 50}$) is $r_{\rm 50}$. These simulated galaxies
    may differ from the real ones. \textcolorblackFor this reason, we mask the central
    16 x 16 pixels in both the training and test samples. Although the simulated galaxy
    is now masked, it is still important to include it, as for some profiles the galaxy
    light extends outside the masked region. Without the simulated galaxy, BKGnet
    fails on testing bright sources. As the label is estimated in a 8 pixels radius
    aperture, 16 x 16 pixels is the minimum area such that the network does not see
    the pixels used for the estimation of the ground truth background.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何深度学习算法中，训练样本和测试样本应尽可能相似。在我们的案例中，训练样本不包含目标星系，而测试样本包含。因此，我们在空白训练印章的中心添加模拟星系。模拟星系的构建基于
    PAUS 数据的参数：Sersic 型型曲线参数、$r_{\rm 50}$、$I_{\rm 50}$ 和 $i$ 波段的亮度。Sersic 型型曲线描述了星系的表面亮度分布（$I$）。包含
    50% 光强度（$I_{\rm 50}$）的半径是 $r_{\rm 50}$。这些模拟星系可能与真实星系有所不同。*因此，我们对训练样本和测试样本的中心 16
    x 16 像素进行掩码处理。虽然模拟星系现在被掩码处理，但仍然很重要，因为对于某些型曲线，星系光线会延伸到掩码区域之外。没有模拟星系时，BKGnet 在测试亮度源时失败。由于标签是在
    8 像素半径的孔径中估计的，因此 16 x 16 像素是网络不会看到用于真实背景估计的像素的最小区域。*
- en: We normalise the stamp before feeding the network. There are different ways
    of doing this. We apply a normalisation stamp by stamp, where we use the mean
    and the standard deviation of each stamp to normalise it. We have chosen this
    normalisation method as it performs better on our dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在将印章输入网络之前对其进行归一化。归一化的方法有多种。我们应用逐印章归一化方法，使用每个印章的均值和标准差来进行归一化。我们选择这种归一化方法，因为它在我们的数据集上表现更好。
- en: 'We use all the PAUCam images in the COSMOS field to train and validate the
    network. We have 4928 PAUCam images before the intervention and 4821 after (see
    sec. [3.2](#S3.SS2 "3.2 Data: training and test samples ‣ 3 BKGnet: A Deep Learning
    based method to predict the background ‣ The PAU Survey: Background light estimation
    with deep learning techniques") for details). For each of them, we sample around
    40 stamps per CCD image, giving a total of around 400,000 stamps. We use 90% of
    them for training and the remaining 10% for validation.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 COSMOS 场中的所有 PAUCam 图像来训练和验证网络。在干预之前我们有 4928 张 PAUCam 图像，干预之后有 4821 张（有关详细信息，请参见第
    [3.2](#S3.SS2 "3.2 数据：训练和测试样本 ‣ 3 BKGnet：一种基于深度学习的背景预测方法 ‣ PAU 调查：使用深度学习技术进行背景光估计")
    节）。对于每张图像，我们从每个 CCD 图像中采样约 40 个印章，总共约 400,000 个印章。我们将其中的 90% 用于训练，其余 10% 用于验证。
- en: '![Refer to caption](img/6141c8967606dd8034d09c38cf73445c.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6141c8967606dd8034d09c38cf73445c.png)'
- en: 'Figure 5: BKGnet scheme: The first set of layers corresponds to a Convolutional
    Neural Network to which one inputs the images. The CNN output, together with extra
    information are input to a linear neural network. The numbers on each of the convolutional
    layers represent the layer’s dimension. The first number corresponds to the number
    of channels. The second and third numbers are the dimension of the stamp in that
    layer.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：BKGnet 方案：第一组层对应于一个卷积神经网络，输入图像。CNN 输出与额外信息一起输入到一个线性神经网络。每个卷积层上的数字表示该层的维度。第一个数字对应于通道数。第二和第三个数字是该层中印章的维度。
- en: 3.3 Training process and loss function
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 训练过程和损失函数
- en: Supervised deep learning algorithms are trained comparing the true value with
    the algorithm’s prediction. The agreement between the prediction and the true
    value is evaluated with a loss function. The choice of loss function depends on
    the kind of problem one is facing, (e.g. classification, regression). A typical
    loss function for classification problems is the cross-entropy loss, whereas in
    regression problems the mean squared error is commonly used. With BKGnet we want
    the network to associate an uncertainty to each prediction. In supervised deep
    learning, there are some methods based on Bayesian statistics that deal with uncertainties
    associated with the predictions (e.g. Kendall & Gal, [2017](#bib.bib24); Kendall
    et al., [2017](#bib.bib25)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 监督深度学习算法通过比较真实值和算法的预测值来进行训练。预测值与真实值的一致性通过损失函数来评估。损失函数的选择取决于所面临的问题类型（例如分类、回归）。分类问题的典型损失函数是交叉熵损失，而回归问题中通常使用均方误差。使用
    BKGnet，我们希望网络将不确定性与每个预测值关联起来。在监督深度学习中，有一些基于贝叶斯统计的方法处理与预测相关的不确定性（例如 Kendall & Gal,
    [2017](#bib.bib24); Kendall et al., [2017](#bib.bib25)）。
- en: The method we use assumes that the distribution $p(\textbf{y}|f^{\textbf{w}}(\textbf{x}))$
    is Gaussian, where y are the background label values, x are the inputs and $f^{\textbf{w}}(\textbf{x})$
    are the network background predictions. Therefore, the loss function is defined
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的方法假设分布 $p(\textbf{y}|f^{\textbf{w}}(\textbf{x}))$ 是高斯分布，其中 y 是背景标签值，x 是输入，$f^{\textbf{w}}(\textbf{x})$
    是网络背景预测。因此，损失函数被定义为
- en: '|  | $Loss=\>-\log{p(f^{\textbf{w}}(\textbf{x}))}\>=\>\frac{(f^{\textbf{w}}(\textbf{x})-y)^{2}}{\sigma^{2}}+2\log\sigma.$
    |  | (4) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $Loss=\>-\log{p(f^{\textbf{w}}(\textbf{x}))}\>=\>\frac{(f^{\textbf{w}}(\textbf{x})-y)^{2}}{\sigma^{2}}+2\log\sigma.$
    |  | (4) |'
- en: In this way, we train the network to provide both, the background prediction
    $f^{\textbf{w}}(\textbf{x})$ and its associated error $\sigma$. Notice that the
    second term on the right hand side prevents the network from predicting a large
    error that minimizes the first term.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们训练网络提供背景预测 $f^{\textbf{w}}(\textbf{x})$ 及其相关的误差 $\sigma$。注意，右侧的第二项防止网络预测出一个大误差，从而最小化第一项。
- en: 'With the loss function in Equation [4](#S3.E4 "In 3.3 Training process and
    loss function ‣ 3 BKGnet: A Deep Learning based method to predict the background
    ‣ The PAU Survey: Background light estimation with deep learning techniques"),
    the network provides an error on the quantity $f^{\textbf{w}}(\textbf{x})-y$,
    which has an associated uncertainty $\sigma^{2}_{\rm pred}+\sigma^{2}_{\rm label}$.
    The error on the prediction is therefore'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 [4](#S3.E4 "在 3.3 训练过程和损失函数 ‣ 3 BKGnet：一种基于深度学习的背景预测方法 ‣ PAU 调查：基于深度学习技术的背景光估计")
    中的损失函数下，网络提供了关于 $f^{\textbf{w}}(\textbf{x})-y$ 的误差，该误差有一个相关的不确定性 $\sigma^{2}_{\rm
    pred}+\sigma^{2}_{\rm label}$。因此，预测的误差为
- en: '|  | $\sigma_{\rm pred}=\sqrt{\sigma^{2}_{\rm bkgnet}-\sigma^{2}_{\rm label}}\>,$
    |  | (5) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma_{\rm pred}=\sqrt{\sigma^{2}_{\rm bkgnet}-\sigma^{2}_{\rm label}}\>,$
    |  | (5) |'
- en: 'where $\sigma^{2}_{\rm bkgnet}$ is the error provided by the network and $\sigma^{2}_{\rm
    label}$ is the error of the background label. The error of the background label
    is defined in Equation ([3](#S3.E3 "In 3.2 Data: training and test samples ‣ 3
    BKGnet: A Deep Learning based method to predict the background ‣ The PAU Survey:
    Background light estimation with deep learning techniques")).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma^{2}_{\rm bkgnet}$ 是网络提供的误差，$\sigma^{2}_{\rm label}$ 是背景标签的误差。背景标签的误差在方程
    ([3](#S3.E3 "在 3.2 数据：训练和测试样本 ‣ 3 BKGnet：一种基于深度学习的背景预测方法 ‣ PAU 调查：基于深度学习技术的背景光估计"))
    中定义。
- en: \textcolor
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: 'blackBKGnet is trained in 60 epochs with a batch size of 100 stamps using the
    ADAM optimiser (Kingma & Ba, [2014](#bib.bib26)) and a learning rate of $10^{\rm-5}$
    (see Appendix [A](#A1 "Appendix A Convolutional Neural Networks ‣ The PAU Survey:
    Background light estimation with deep learning techniques") for terminology).
    The training takes about 2 hours using an NVIDIA TITAN V GPU.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: BKGnet 在 60 个周期内使用批量大小为 100 的邮票进行训练，采用 ADAM 优化器 (Kingma & Ba, [2014](#bib.bib26))
    和学习率 $10^{\rm-5}$ (见附录 [A](#A1 "附录 A 卷积神经网络 ‣ PAU 调查：利用深度学习技术进行背景光估计") 获取术语)。训练使用
    NVIDIA TITAN V GPU 约需 2 小时。
- en: 4 Testing BKGnet on simulations
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 测试 BKGnet 在模拟数据上的表现
- en: We test the performance of BKGnet with simulated data. We study how well we
    can predict the background with the network and explore what data are needed and
    how these data need to be treated before feeding the network. Throughout the rest
    of the paper we compare theBKGnet predictions to those obtained by calculating
    the background inside an annulus around the target source before and after correcting
    the image with the scattered-light templates.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用模拟数据测试 BKGnet 的性能。我们研究了使用该网络预测背景的效果，并探讨了在喂入网络之前需要什么数据以及如何处理这些数据。在本文的其余部分，我们将
    BKGnet 的预测与通过计算目标源周围圆环中的背景，并在用散射光模板校正图像前后的结果进行比较。
- en: 4.1 Simulated PAUCam background images
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 模拟的 PAUCam 背景图像
- en: \textcolor
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackThe final simulated image $I_{\rm sim}(x,y)$ can be expressed as
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的模拟图像 $I_{\rm sim}(x,y)$ 可以表示为
- en: '|  | ${\rm I_{\rm sim}}(x,y)=A\cdot\frac{t_{\rm exp}\cdot{\rm SLT}(x,y)+P(t_{\rm
    exp}\cdot{\rm SLT}(x,y))}{t_{\rm exp}},$ |  | (6) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\rm I_{\rm sim}}(x,y)=A\cdot\frac{t_{\rm exp}\cdot{\rm SLT}(x,y)+P(t_{\rm
    exp}\cdot{\rm SLT}(x,y))}{t_{\rm exp}},$ |  | (6) |'
- en: where SLT is the scattered light template used to generate the image. In this
    way, the simulated image shows the same scattered light pattern as the PAUCam
    images. To generate the Poisson noise, we first multiply with the exposure time
    $(t_{\rm exp})$ to convert the template from e/s to electrons. Additionally, the
    template is scaled with a factor $A$ to simulate a wide range of background levels.
    Finally, we generate a realization of Poisson sky noise $P(\cdot)$ that we add
    to the image and convert it back to e/s.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 SLT 是用于生成图像的散射光模板。通过这种方式，模拟图像显示了与 PAUCam 图像相同的散射光模式。为了生成泊松噪声，我们首先与曝光时间 $(t_{\rm
    exp})$ 相乘，将模板从 e/s 转换为电子。此外，模板还与一个因子 $A$ 进行缩放，以模拟广泛的背景水平。最后，我们生成一个泊松天噪声 $P(\cdot)$，将其添加到图像中，并将其转换回
    e/s。
- en: 'Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Simulated PAUCam background images ‣ 4 Testing
    BKGnet on simulations ‣ The PAU Survey: Background light estimation with deep
    learning techniques") illustrates the effects of scattered light in our simulated
    stamps. On the left, we show a simulated stamp image with a flat background pattern;
    and, on the right, with a gradient on the background. This gradient is caused
    by scattered light (see Fig. [1](#S2.F1 "Figure 1 ‣ 2.3 Scattered-light templates
    ‣ 2 Modelling scattered light ‣ The PAU Survey: Background light estimation with
    deep learning techniques") and [3](#S2.F3 "Figure 3 ‣ 2.4 Scattered-light templates
    as scattered light correcting method ‣ 2 Modelling scattered light ‣ The PAU Survey:
    Background light estimation with deep learning techniques")). Both stamps show
    a central 8x8 pixel masked region, blocking the light of the galaxy.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6](#S4.F6 "图 6 ‣ 4.1 模拟的 PAUCam 背景图像 ‣ 4 测试 BKGnet 在模拟数据上的表现 ‣ PAU 调查：利用深度学习技术进行背景光估计")
    展示了我们模拟邮票中的散射光效应。左侧展示了背景模式平坦的模拟邮票图像；右侧则是具有背景渐变的邮票。这个渐变是由散射光引起的（见图 [1](#S2.F1 "图
    1 ‣ 2.3 散射光模板 ‣ 2 模拟散射光 ‣ PAU 调查：利用深度学习技术进行背景光估计") 和 [3](#S2.F3 "图 3 ‣ 2.4 散射光模板作为散射光校正方法
    ‣ 2 模拟散射光 ‣ PAU 调查：利用深度学习技术进行背景光估计")）。两个邮票都显示了一个中央 8x8 像素的遮罩区域，阻挡了星系的光线。
- en: '![Refer to caption](img/db253c5eb2f4059b9e06d99f3d89c43b.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/db253c5eb2f4059b9e06d99f3d89c43b.png)'
- en: 'Figure 6: Simulated stamps. On the left, a stamp with a flat background. On
    the right a stamp with a background with a gradient.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：模拟的邮票。左侧是一个背景平坦的邮票。右侧是一个具有渐变背景的邮票。
- en: 4.2 BKGnet predictions on simulations
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 BKGnet 在模拟数据上的预测
- en: Throughout this section, we train and test on stamps without target galaxies
    (empty positions). This allows us to test whether it is possible to predict the
    background with this network’s assembly. We also fix the band we are testing to
    the NB685 filter after the camera intervention. This choice is a compromise between
    having a considerable amount of scattered light without being completely dominated
    by it. Before the intervention, the amount of scattered light in some of the CCD
    images is very large and might not be an adequate choice to test the network.
    On the other hand, after the intervention, some of the CCDs barely contain scattered
    light, and those would not be a good choice either. The NB685 filter contains
    a considerable amount of scattered light and therefore it is a representative
    example. We do not need to simulate all bands, as here we only want to test the
    viability of the the scattered light prediction with BKGnet and to have a better
    understanding of the network’s behaviour. To quantify the background prediction
    accuracy, we use
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们在没有目标星系的印刷图上进行训练和测试（空白位置）。这使我们能够测试是否可以使用该网络的组合来预测背景。我们还将测试的波段固定为相机干预后的NB685滤镜。这一选择是在拥有相当数量的散射光而又不完全被其主导之间的折中。在干预之前，一些CCD图像中的散射光量非常大，可能不适合测试网络。另一方面，干预后，一些CCD几乎不含散射光，这也不是一个好的选择。NB685滤镜包含了相当数量的散射光，因此是一个具有代表性的例子。我们不需要模拟所有波段，因为我们这里只是想测试BKGnet对散射光预测的可行性，并更好地了解网络的行为。为了量化背景预测的准确性，我们使用
- en: '|  | $\sigma_{\rm 68}\equiv 0.5\,(b_{\rm quant}^{84.1}-b_{\rm quant}^{15.9}),$
    |  | (7) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma_{\rm 68}\equiv 0.5\,(b_{\rm quant}^{84.1}-b_{\rm quant}^{15.9}),$
    |  | (7) |'
- en: with quantiles set to 84.1 and 15.9 percentage values. This quantity, $\sigma_{\rm
    68}$, is equivalent to a $1\sigma$ error for a normal distribution, but it is
    less affected by outliers.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中分位数设置为84.1和15.9百分比值。这个量，$\sigma_{\rm 68}$，等同于正态分布的$1\sigma$误差，但对异常值的影响较小。
- en: '![Refer to caption](img/fb1b966db11a6821845a37dd15cc3921.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fb1b966db11a6821845a37dd15cc3921.png)'
- en: 'Figure 7: Relative error distributions for the BKGnet (green without coordinate
    information and orange with coordinate information) and the annulus predictions.
    $b_{\rm 0}$ is the background label and $b_{\rm pred}$ is the background prediction,
    either for the annulus or for BKGnet.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：BKGnet的相对误差分布（绿色表示没有坐标信息，橙色表示有坐标信息）和环形预测。$b_{\rm 0}$是背景标签，$b_{\rm pred}$是背景预测值，适用于环形或BKGnet。
- en: 'Figure [7](#S4.F7 "Figure 7 ‣ 4.2 BKGnet predictions on simulations ‣ 4 Testing
    BKGnet on simulations ‣ The PAU Survey: Background light estimation with deep
    learning techniques") compares the accuracy with which BKGnet predicts the background
    to the PAUS default approach. As described before, PAUS estimates the background
    computing the median inside an annulus centered on the galaxy after the pixel
    values have been $\sigma$-clipped. The plot shows the relative error distribution
    of the predictions for both methods. We have tested BKGnet with and without embedding
    the image coordinates of the galaxy. The BKGnet performance improves significantly
    with the coordinate information. This is not surprising because the amount of
    scattered light depends on the CCD position (see Section [2](#S2 "2 Modelling
    scattered light ‣ The PAU Survey: Background light estimation with deep learning
    techniques")). Although scattered light is encoded in the image, the CCD position
    also includes essential information for the prediction. The network might need
    it to create something similar to the scattered light template. BKGnet achieves
    a $\sigma_{\rm 68}$ = 0.0038 with information coming only with the stamps. Including
    the coordinate information, this improves to $\sigma_{\rm 68}$ = 0.0022\. Therefore,
    the network improves by 70% with the coordinates embedding. The default background
    estimate shows tails on both sides of the distribution, and yields $\sigma_{\rm
    68}$ = 0.0033, which means BKGnet improves the estimate by 42%.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [7](#S4.F7 "图 7 ‣ 4.2 BKGnet 对模拟的预测 ‣ 4 测试 BKGnet 在模拟中的表现 ‣ PAU 调查：使用深度学习技术的背景光估计")
    比较了 BKGnet 预测背景的准确性与 PAUS 默认方法的准确性。如前所述，PAUS 通过在图像中心围绕星系的圆环内计算中位数来估计背景，像素值在计算之前已进行
    $\sigma$-裁剪。图中展示了两种方法预测结果的相对误差分布。我们测试了带有和不带有星系图像坐标的 BKGnet。BKGnet 的性能在包含坐标信息时显著提高。这并不令人惊讶，因为散射光的量取决于
    CCD 位置（参见第 [2](#S2 "2 建模散射光 ‣ PAU 调查：使用深度学习技术的背景光估计") 节）。尽管散射光被编码在图像中，但 CCD 位置也包含预测所需的重要信息。网络可能需要这些信息来创建类似于散射光模板的东西。BKGnet
    在仅使用印记信息时取得了 $\sigma_{\rm 68}$ = 0.0038 的结果。包括坐标信息后，改进至 $\sigma_{\rm 68}$ = 0.0022\.
    因此，网络在嵌入坐标信息后提高了 70%。默认背景估计在分布的两侧显示尾部，并产生了 $\sigma_{\rm 68}$ = 0.0033，这意味着 BKGnet
    将估计值提高了 42%。
- en: '![Refer to caption](img/449f5fb086576d048b51f660863699eb.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/449f5fb086576d048b51f660863699eb.png)'
- en: 'Figure 8: *Left*: CCD reconstruction with the true background values used to
    train the network. We sample these background values consecutively and we reconstruct
    the original image by placing each value in the position it was sampled from.
    *Second:* Accuracy on the background prediction with BKGnet in the different image
    positions. We can see there are not spatial patterns. *Third*: Accuracy on the
    background prediction with the annulus in the different image positions. We can
    see there are not spatial patterns. *Right*: Accuracy on the background prediction
    with a kNN in the different image positions.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: *左*: 使用真实背景值重建 CCD 图像，这些背景值用于训练网络。我们连续抽取这些背景值，并通过将每个值放置在其抽取位置来重建原始图像。
    *第二*: 在不同图像位置使用 BKGnet 进行背景预测的准确性。我们可以看到没有空间模式。 *第三*: 在不同图像位置使用圆环进行背景预测的准确性。我们可以看到没有空间模式。
    *右*: 在不同图像位置使用 kNN 进行背景预测的准确性。'
- en: 'Figure [8](#S4.F8 "Figure 8 ‣ 4.2 BKGnet predictions on simulations ‣ 4 Testing
    BKGnet on simulations ‣ The PAU Survey: Background light estimation with deep
    learning techniques") shows the spatial background map (left) and the relative
    error on the prediction of this map with the annulus background predictions (third
    panel) and the BKGnet background predictions (second panel). The precision is
    lower at the edges of the CCD for the annulus-based method, where scattered light
    is present. This indicates that the tails in Figure [7](#S4.F7 "Figure 7 ‣ 4.2
    BKGnet predictions on simulations ‣ 4 Testing BKGnet on simulations ‣ The PAU
    Survey: Background light estimation with deep learning techniques") are caused
    by scattered light. On the other hand, one can see that BKGnet is able to account
    for the presence of scattered light.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [8](#S4.F8 "Figure 8 ‣ 4.2 BKGnet predictions on simulations ‣ 4 Testing
    BKGnet on simulations ‣ The PAU Survey: Background light estimation with deep
    learning techniques") 展示了空间背景图 (左) 以及与环形背景预测 (第三面板) 和 BKGnet 背景预测 (第二面板) 的预测相对误差。对于基于环形的方法，CCD
    边缘的精度较低，那里存在散射光。这表明图 [7](#S4.F7 "Figure 7 ‣ 4.2 BKGnet predictions on simulations
    ‣ 4 Testing BKGnet on simulations ‣ The PAU Survey: Background light estimation
    with deep learning techniques") 中的尾部是由于散射光造成的。另一方面，可以看到 BKGnet 能够考虑散射光的存在。'
- en: \textcolor
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: 'blackThe right panel of Fig. [8](#S4.F8 "Figure 8 ‣ 4.2 BKGnet predictions
    on simulations ‣ 4 Testing BKGnet on simulations ‣ The PAU Survey: Background
    light estimation with deep learning techniques") shows the background reconstruction
    using a kNN algorithm. In addition to the CNN and annulus methods, we have also
    tested the k-Nearest Neighbors (kNN) (Cover & Hart, [2006](#bib.bib13)), Support
    Vector Regression (SVR) (Drucker et al., [1996](#bib.bib15)), Random Forest (RF)
    (Breiman, [2001](#bib.bib6)) and a Neural Network (NN) techniques. We used the
    scikit-learn implementations (Pedregosa et al., [2011](#bib.bib39)) to run the
    kNN, RF and SVR algorithms. For these tests, unlike the CNN, which can handle
    images, we input the embedded information and the median pixel value. As shown
    in Figure [8](#S4.F8 "Figure 8 ‣ 4.2 BKGnet predictions on simulations ‣ 4 Testing
    BKGnet on simulations ‣ The PAU Survey: Background light estimation with deep
    learning techniques"), the kNN measures the background with less accuracy than
    BKGnet and the annulus method in the flat background regions. The background measurements
    are biased by about 3% , which is 6 times larger than the relative errors (0.5%).
    The prediction also shows patterns on the edges with an error 6 times higher than
    those on flat regions. In contrast, for BKGnet the precision only degrades by
    a factor of 1.2 when we compare the center with the border positions. Concerning
    the other methods, the NN provides better predictions than the kNN, although it
    increases $\sigma_{68}$ by a factor of 2.5 with respect to BKGnet. It also shows
    patterns on the edges with 4 times higher errors than in flat background regions.
    Further, the errors of the RF and SVR algorithms are a factor of 6 and 4, respectively,
    higher than those of the BKGnet method, rendering these methods too imprecise.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [8](#S4.F8 "Figure 8 ‣ 4.2 BKGnet predictions on simulations ‣ 4 Testing
    BKGnet on simulations ‣ The PAU Survey: Background light estimation with deep
    learning techniques") 的右侧面板展示了使用 kNN 算法的背景重建。除了 CNN 和环形方法外，我们还测试了 k-最近邻 (kNN)
    (Cover & Hart, [2006](#bib.bib13))、支持向量回归 (SVR) (Drucker et al., [1996](#bib.bib15))、随机森林
    (RF) (Breiman, [2001](#bib.bib6)) 和神经网络 (NN) 技术。我们使用了 scikit-learn 实现 (Pedregosa
    et al., [2011](#bib.bib39)) 来运行 kNN、RF 和 SVR 算法。与可以处理图像的 CNN 不同，这些测试中我们输入了嵌入信息和中位像素值。如图
    [8](#S4.F8 "Figure 8 ‣ 4.2 BKGnet predictions on simulations ‣ 4 Testing BKGnet
    on simulations ‣ The PAU Survey: Background light estimation with deep learning
    techniques") 所示，kNN 在平坦背景区域的背景测量准确性低于 BKGnet 和环形方法。背景测量偏差约为 3%，是相对误差 (0.5%) 的
    6 倍。预测还显示出边缘的模式，其误差是平坦区域的 6 倍。相比之下，对于 BKGnet，当我们比较中心位置与边界位置时，精度仅降低了 1.2 倍。就其他方法而言，NN
    的预测优于 kNN，尽管相较于 BKGnet，它使 $\sigma_{68}$ 增加了 2.5 倍。它在边缘也显示出 4 倍高于平坦背景区域的误差。此外，RF
    和 SVR 算法的误差分别是 BKGnet 方法的 6 倍和 4 倍，使这些方法过于不精确。'
- en: \textcolor
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackIn the following sections we test the background estimation method on real
    PAUS images. Based on the performance on simulations, we will only consider the
    BKGnet and the annulus methods.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将在实际的 PAUS 图像上测试背景估计方法。基于模拟中的表现，我们将仅考虑 BKGnet 和环形方法。
- en: 5 BKGnet on PAUCam images
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 BKGnet 在 PAUCam 图像上的应用
- en: In the previous section, we have shown that BKGnet is able to accurately predict
    strongly scattered light backgrounds on simulated blank images including only
    the background. However, in real PAUCam data other complications, such as as cosmic
    rays, electronic cross-talk, read-out noise and dark current may affect the performance
    on the estimation of the background. Moreover, correlations between pixels might
    be introduced during the data reduction process. To examine the impact of these
    real-life effects we use actual PAUCam images. To assess the accuracy of our measurements,
    we test network on empty stamps, i.e. without target galaxies.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们展示了BKGnet能够准确预测模拟空白图像上强烈散射光背景的能力。然而，在真实的PAUCam数据中，其他问题如宇宙射线、电子串扰、读出噪声和暗电流可能会影响背景估计的性能。此外，数据处理过程中可能会引入像素间的相关性。为了检验这些现实影响的影响，我们使用实际的PAUCam图像。为了评估我们的测量精度，我们在空白印章上测试网络，即没有目标星系的印章。
- en: We will use all the images available in COSMOS, splitting the data into those
    obtained before the camera intervention (in 2016A) and after. This split yields
    4928 images and 4821 images for before and after the intervention, respectively.
    As these numbers are similar, we can easily balance the number of stamps before
    and after the intervention in our training sample. Although the training sample
    does not contain target galaxies in the center, sources might be placed in other
    stamp’s positions. To avoid outliers in the training set, e.g. a stamp with a
    bright star covering most of the background or a bright object too close to the
    center, we filter the training stamps based on the maximum pixel value. All stamps
    with a pixel containing more than 100,000 counts are excluded from the training
    sample.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用COSMOS中所有可用的图像，将数据分为相机干预前（2016A）和干预后获得的数据。这一分割产生了4928张干预前图像和4821张干预后图像。由于这些数字相似，我们可以在训练样本中轻松平衡干预前后的印章数量。尽管训练样本中不包含中心的目标星系，但源可能被放置在其他印章位置。为了避免训练集中出现异常值，例如一个明亮的星星覆盖了大部分背景的印章或一个离中心太近的明亮物体，我们根据最大像素值过滤训练印章。所有像素值超过100,000计数的印章都被排除在训练样本之外。
- en: We also exclude 40 images from each subsample before training the network. These
    80 images are not used to train the network, but are kept to test it. This is
    important, as we need to test the network on images it has never seen before.
    To generate the test set, instead of sampling randomly from the CCDs, we sample
    stamps consecutively in intervals of 60 pixels. This ensures that we test all
    CCD regions, including regions affected by scattered light.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练网络之前还排除了每个子样本中的40张图像。这80张图像未用于训练网络，而是保留用于测试。这很重要，因为我们需要在网络从未见过的图像上进行测试。为了生成测试集，我们不是从CCDs中随机采样，而是以60像素的间隔连续采样印章。这确保我们测试所有CCD区域，包括受到散射光影响的区域。
- en: '![Refer to caption](img/8a4a41be1ff2a747fd1a8ef71537e500.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8a4a41be1ff2a747fd1a8ef71537e500.png)'
- en: 'Figure 9: $\sigma_{\rm 68}$ of the relative error in the background prediction
    for the 40 NBs. *Left*: Before the intervention. *Right*: After the camera intervention.
    In almost all cases BKGnet performs better than the default approach that employs
    an annulus to estimate the background.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '图9：40个NB背景预测中相对误差的$\sigma_{\rm 68}$。*左*: 干预前。*右*: 相机干预后。在几乎所有情况下，BKGnet的表现优于默认的使用环形区域估计背景的方法。'
- en: 'Figure [9](#S5.F9 "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background
    light estimation with deep learning techniques") shows the results when we use
    BKGnet to predict the background on PAUCam images in empty regions. We also show
    results when the background is estimated using an annulus, and when we first correct
    the background variations using a scattered light template (’annulus + sky’).
    Figure [9](#S5.F9 "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background
    light estimation with deep learning techniques") shows the value of $\sigma_{\rm
    68}$ (Equation ([7](#S4.E7 "In 4.2 BKGnet predictions on simulations ‣ 4 Testing
    BKGnet on simulations ‣ The PAU Survey: Background light estimation with deep
    learning techniques"))) of the relative error distribution on the prediction for
    the 40 different bands. Because we are using the relative error, the comparison
    between the results before and after the intervention is not representative, as
    the background levels are different. For instance, in the first filter tray (NB455-NB515),
    the background before the intervention is between 3 and 5 times higher than after.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#S5.F9 "图 9 ‣ 5 BKGnet 在 PAUCam 图像上的结果 ‣ PAU 调查：利用深度学习技术进行背景光估计") 显示了我们在空区域使用
    BKGnet 预测 PAUCam 图像背景的结果。我们还展示了使用环形区域估计背景的结果，以及在使用散射光模板（‘环形区域 + 天空’）纠正背景变化后的结果。图[9](#S5.F9
    "图 9 ‣ 5 BKGnet 在 PAUCam 图像上的结果 ‣ PAU 调查：利用深度学习技术进行背景光估计") 显示了相对误差分布的 $\sigma_{\rm
    68}$ 值（方程式 ([7](#S4.E7 "在 4.2 BKGnet 在模拟中的预测 ‣ 4 测试 BKGnet 在模拟中的效果 ‣ PAU 调查：利用深度学习技术进行背景光估计"))），这是对于
    40 个不同波段预测的相对误差。由于我们使用的是相对误差，因此在干预前后的结果比较并不具代表性，因为背景水平不同。例如，在第一个滤光片托盘（NB455-NB515）中，干预前的背景比干预后的背景高出
    3 到 5 倍。
- en: 'We focus first on the results before the camera intervention (left panel in
    Figure [9](#S5.F9 "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background
    light estimation with deep learning techniques")). Images before the camera intervention
    contain more scattered light than those after (see Fig. [2](#S2.F2 "Figure 2 ‣
    2.3 Scattered-light templates ‣ 2 Modelling scattered light ‣ The PAU Survey:
    Background light estimation with deep learning techniques")). This makes the scattered-light
    template modelling more unstable than the modeling of images after the intervention.
    We find that correcting with the scattered-light template does not improve the
    annulus method result in every band. In the bluest NBs, i.e. those with the highest
    amount of scattered light, the scattered-light template seems to decrease the
    accuracy of the background prediction. On the other hand, BKGnet improves the
    accuracy compared to the other two methods, especially on the bluest filter tray.
    On average considering all bands, the network reduces the $\sigma_{\rm 68}$ by
    $37\%$ compared to the scattered-light template and up to 50% if we only consider
    the 8 bluest NBs.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先关注相机干预前的结果（图[9](#S5.F9 "图 9 ‣ 5 BKGnet 在 PAUCam 图像上的结果 ‣ PAU 调查：利用深度学习技术进行背景光估计")
    左侧面板）。相机干预前的图像包含的散射光比干预后的图像多（参见图[2](#S2.F2 "图 2 ‣ 2.3 散射光模板 ‣ 2 散射光建模 ‣ PAU 调查：利用深度学习技术进行背景光估计")）。这使得散射光模板建模比干预后图像的建模更不稳定。我们发现，使用散射光模板的纠正并未在每个波段上改善环形区域方法的结果。在最蓝的
    NB 波段中，即散射光量最多的波段，散射光模板似乎降低了背景预测的准确性。另一方面，BKGnet 在准确性上优于其他两种方法，特别是在最蓝的滤光片托盘上。综合考虑所有波段，网络将
    $\sigma_{\rm 68}$ 减少了 $37\%$，如果只考虑 8 个最蓝的 NB，则减少了高达 50%。
- en: 'If we consider the results after the camera intervention (right panel of Figure
    [9](#S5.F9 "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background
    light estimation with deep learning techniques")), we see that the scattered-light
    template improves the annulus method prediction in all the bands. This is expected
    from the top panel in Figure [2](#S2.F2 "Figure 2 ‣ 2.3 Scattered-light templates
    ‣ 2 Modelling scattered light ‣ The PAU Survey: Background light estimation with
    deep learning techniques"), which shows that scattered light trends are stable
    after the camera intervention. Before the intervention the scattered-light template
    fails in the bluer bands, which no longer happens after the camera intervention.
    Nevertheless, BKGnet performs even better: on average, after the intervention
    it achieves an 18% improvement compared to the scattered-light template correction.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们考虑摄像机干预后的结果（图 [9](#S5.F9 "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU
    Survey: Background light estimation with deep learning techniques")右侧面板），我们会发现散射光模板在所有频段中都改善了环形方法的预测。这一点从图
    [2](#S2.F2 "Figure 2 ‣ 2.3 Scattered-light templates ‣ 2 Modelling scattered light
    ‣ The PAU Survey: Background light estimation with deep learning techniques")的顶部面板可以预期，该面板显示了干预后散射光趋势的稳定性。在干预前，散射光模板在蓝色频段表现不佳，但在干预后这一情况不再发生。然而，BKGnet的表现更为出色：干预后，它在平均上实现了比散射光模板校正高18%的改进。'
- en: '|  | BEFORE | AFTER |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | 干预前 | 干预后 |'
- en: '| --- | --- | --- |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | filtered | sources | filtered | sources |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | 过滤后的 | 来源 | 过滤后的 | 来源 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Annulus | 0.011 | 0.011 | 0.014 | 0.014 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Annulus | 0.011 | 0.011 | 0.014 | 0.014 |'
- en: '| + SLT | 0.011 | 0.011 | 0.011 | 0.013 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| + SLT | 0.011 | 0.011 | 0.011 | 0.013 |'
- en: '| BKGnet | 0.008 | 0.008 | 0.011 | 0.011 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| BKGnet | 0.008 | 0.008 | 0.011 | 0.011 |'
- en: 'Table 1: Average $\sigma_{\rm 68}$ of the relative error in the background
    prediction across all the bands for BKGnet trained before and after the camera
    intervention. We list the results for the data sets without filtering out stamps
    affected by sources (‘sources’), and if we remove these (’filtered’).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在摄像机干预前后，BKGnet在所有频段的背景预测相对误差的平均值$\sigma_{\rm 68}$。我们列出了未过滤掉受源影响的印记数据集（‘sources’）和移除这些数据（‘filtered’）的结果。
- en: 'Table [1](#S5.T1 "Table 1 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background
    light estimation with deep learning techniques") lists the average value of $\sigma_{\rm
    68}$ of the relative error in the background prediction for the three methods:
    annulus, annulus + scattered-light template and BKGnet. When training and testing,
    we first exclude the stamps with a maximum pixel value above 100,000\. By doing
    this, we avoid stamps with very bright nearby sources that might bias the prediction.
    To examine the impact of this step, we also list the results when contaminated
    stamps are included (‘sources’) in Table [1](#S5.T1 "Table 1 ‣ 5 BKGnet on PAUCam
    images ‣ The PAU Survey: Background light estimation with deep learning techniques").
    These results show that the filtering does not make a difference before the intervention,
    but the performance improves somewhat for the correction that uses the scattered-light
    templates. The small difference suggests that scattered light is the main source
    of bias. For the images without bright sources taken after the intervention, BKGnet
    and the scattered-light template give the same $\sigma_{\rm 68}$. Therefore, it
    is possible that BKGnet learns the underlying behaviour of scattered light in
    a similar way as the scattered-light template. However, as the network also sees
    the stamp, the correction it infers is more flexible than applying a scattered-light
    template. This indicates that BKGnet is able to learn how to estimate the background
    in the presence of other artifacts (e.g. sources or cosmic rays).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](#S5.T1 "表格 1 ‣ 5 BKGnet 在 PAUCam 图像上的表现 ‣ PAU 调查：使用深度学习技术进行背景光估计") 列出了三种方法——圆环、圆环
    + 散射光模板和 BKGnet——在背景预测中的相对误差的 $\sigma_{\rm 68}$ 平均值。在训练和测试时，我们首先排除了最大像素值超过 100,000
    的印记。这样做是为了避免那些可能会偏倚预测的非常明亮的邻近源的印记。为了检查这一步骤的影响，我们还列出了在表格 [1](#S5.T1 "表格 1 ‣ 5 BKGnet
    在 PAUCam 图像上的表现 ‣ PAU 调查：使用深度学习技术进行背景光估计") 中包含污染印记（‘sources’）时的结果。这些结果显示，在干预之前，过滤并没有造成差异，但使用散射光模板的修正性能有所改善。这一小差异表明，散射光是主要的偏倚来源。对于干预后拍摄的没有亮源的图像，BKGnet
    和散射光模板给出了相同的 $\sigma_{\rm 68}$。因此，BKGnet 可能以类似于散射光模板的方式学习散射光的基本行为。然而，由于网络也能看到印记，它推断出的修正比应用散射光模板更加灵活。这表明
    BKGnet 能够学习在存在其他伪影（例如源或宇宙射线）时如何估计背景。
- en: BKGnet also provides an estimate for the uncertainty associated with the background
    prediction. To test the accuracy of this estimate we use the empty stamps and
    study the distribution of $(b_{\rm net}-b_{\rm true})/\sigma$, where $(b_{\rm
    net}$ and $\sigma$ are the network predictions. If the errors are correct, this
    distribution should be a Gaussian with zero mean and unit variance.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: BKGnet 还提供了背景预测的不确定性估计。为了测试这一估计的准确性，我们使用空印记并研究 $(b_{\rm net}-b_{\rm true})/\sigma$
    的分布，其中 $(b_{\rm net}$ 和 $\sigma$ 是网络的预测值。如果误差是正确的，这一分布应该是均值为零、方差为单位的高斯分布。
- en: 'Figure [10](#S5.F10 "Figure 10 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey:
    Background light estimation with deep learning techniques") shows the theoretical
    Gaussian we should recover and the measured distributions for the annulus and
    BKGnet predictions. The BKGnet results fit the theoretical Gaussian, which means
    that our errors are robust. In contrast, the annulus predictions underestimate
    the uncertainties by 47%. Therefore, BKGnet provides a more reliable estimate
    of the uncertainty in the background determination.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [10](#S5.F10 "图 10 ‣ 5 BKGnet 在 PAUCam 图像上的表现 ‣ PAU 调查：使用深度学习技术进行背景光估计") 显示了我们应该恢复的理论高斯分布和圆环及
    BKGnet 预测的测量分布。BKGnet 的结果符合理论高斯分布，这意味着我们的误差是稳健的。相比之下，圆环预测低估了 47% 的不确定性。因此，BKGnet
    提供了更可靠的背景确定不确定性的估计。
- en: '![Refer to caption](img/01e900b4f04986f02a1ff4a31a6e6bd4.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/01e900b4f04986f02a1ff4a31a6e6bd4.png)'
- en: 'Figure 10: The distribution of $(b_{\rm net}-b_{\rm 0})/\sigma$, where $b_{\rm
    net}$ is the background prediction and $b_{\rm 0}$ the true background. $\sigma$
    is the uncertainty in the prediction. We expect the distribution to be a Gaussian
    centered on zero with unit variance. We show the distribution for the annulus
    (orange) and BKGnet (blue) predictions.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：$(b_{\rm net}-b_{\rm 0})/\sigma$ 的分布，其中 $b_{\rm net}$ 是背景预测，$b_{\rm 0}$
    是真实背景。$\sigma$ 是预测中的不确定性。我们期望分布是以零为中心的高斯分布，方差为单位。我们展示了圆环（橙色）和 BKGnet（蓝色）预测的分布。
- en: 6 BKGnet validation
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 BKGnet 验证
- en: 'The results presented in section [5](#S5 "5 BKGnet on PAUCam images ‣ The PAU
    Survey: Background light estimation with deep learning techniques") show that,
    compared to the annulus-based methods, BKGnet yields better background estimates
    (see Fig [9](#S5.F9 "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background
    light estimation with deep learning techniques") and Tab [1](#S5.T1 "Table 1 ‣
    5 BKGnet on PAUCam images ‣ The PAU Survey: Background light estimation with deep
    learning techniques")). It also provides accurate estimates for the associated
    uncertainty. However, these tests were done on stamps without galaxies. Here we
    increase the realism of the problem and quantify the performance of BKGnet at
    galaxy positions.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](#S5 "5 BKGnet on PAUCam images ‣ The PAU Survey: Background light estimation
    with deep learning techniques") 节中呈现的结果表明，与基于圆环的方法相比，BKGnet 提供了更好的背景估计（见图 [9](#S5.F9
    "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background light estimation
    with deep learning techniques") 和表 [1](#S5.T1 "Table 1 ‣ 5 BKGnet on PAUCam images
    ‣ The PAU Survey: Background light estimation with deep learning techniques")）。它还提供了相关不确定性的准确估计。然而，这些测试是在没有星系的邮票上进行的。在这里，我们增加了问题的现实性，并量化了
    BKGnet 在星系位置的表现。'
- en: 6.1 Generating the PAUS catalogue with BKGnet predictions
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 使用 BKGnet 预测生成 PAUS 目录
- en: We use BKGnet to estimate the background for galaxies in the COSMOS field. We
    compare the results to those from the PAUdm catalogue, which uses an annulus to
    determine the background. These catalogues contain around 12 million flux measurements,
    approximately half of them done on images taken before the intervention and the
    other half on images taken after the intervention. The galaxy fluxes are obtained
    subtracting the background from the PAUS raw signal measurements,
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 BKGnet 来估算 COSMOS 区域中星系的背景。我们将结果与 PAUdm 目录中的结果进行比较，后者使用圆环来确定背景。这些目录包含约
    1200 万个通量测量值，约一半是在干预前拍摄的图像上进行的，另一半是在干预后拍摄的图像上进行的。星系通量是通过从 PAUS 原始信号测量中减去背景获得的，
- en: '|  | $F=S-N_{\rm a}b,$ |  | (8) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | $F=S-N_{\rm a}b,$ |  | (8) |'
- en: where $F$ is the net galaxy flux, $S$ is the total signal measured inside the
    aperture, $N_{\rm a}$ is the number of pixels inside the aperture and $b$ is the
    predicted background per pixel. When the background is estimated with an annulus,
    the error on the net flux is
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $F$ 是净星系通量，$S$ 是在孔径内测得的总信号，$N_{\rm a}$ 是孔径内的像素数量，$b$ 是每个像素的预测背景。当背景使用圆环进行估计时，净通量的误差为
- en: '|  | $\sigma^{2}=(S-b)+N_{\rm a}\sigma_{\rm b}^{2}+N^{2}_{\rm a}\Big{(}\frac{\pi}{2}\Big{)}\frac{\sigma_{\rm
    b}^{2}}{N_{\rm b}},$ |  | (9) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma^{2}=(S-b)+N_{\rm a}\sigma_{\rm b}^{2}+N^{2}_{\rm a}\Big{(}\frac{\pi}{2}\Big{)}\frac{\sigma_{\rm
    b}^{2}}{N_{\rm b}},$ |  | (9) |'
- en: where $b$ and $\sigma_{\rm b}$ are the background and the background error in
    that region and $N_{\rm b}$ is the number of pixels inside the annulus. The $\pi/2$
    factor arises from that fact that we use the median of the pixels inside the annulus
    instead of the mean ⁴⁴4http://wise2.ipac.caltech.edu/staff/fmasci/ApPhotUncert.pdf.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b$ 和 $\sigma_{\rm b}$ 是该区域的背景及其误差，$N_{\rm b}$ 是圆环内的像素数量。$\pi/2$ 因子来自于我们使用圆环内像素的中位数而不是均值的事实
    ⁴⁴4http://wise2.ipac.caltech.edu/staff/fmasci/ApPhotUncert.pdf。
- en: For BKGnet the error on the galaxy flux is
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 BKGnet，星系通量的误差为
- en: '|  | $\sigma^{2}=(S-b)+N_{\rm a}(b+RN^{2})+N_{\rm a}^{2}\sigma_{\rm b}^{2},$
    |  | (10) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma^{2}=(S-b)+N_{\rm a}(b+RN^{2})+N_{\rm a}^{2}\sigma_{\rm b}^{2},$
    |  | (10) |'
- en: where $RN$ is the read-out noise.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $RN$ 是读出噪声。
- en: 'Equations [9](#S6.E9 "In 6.1 Generating the PAUS catalogue with BKGnet predictions
    ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation with deep
    learning techniques") and [10](#S6.E10 "In 6.1 Generating the PAUS catalogue with
    BKGnet predictions ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation
    with deep learning techniques") reflect the differences in the flux uncertainty
    when the background is measured with an annulus or with BKGnet. In general, there
    are three main contributors to the flux uncertainty: the uncertainty in the net
    galaxy flux, the uncertainty in the background estimate, and the uncertainty introduced
    by the background subtraction. For both background estimation methods we assume
    that the uncertainty in the net galaxy flux is captured by shot noise. For BKGnet,
    the background uncertainty is also described by shot noise (Eq. [10](#S6.E10 "In
    6.1 Generating the PAUS catalogue with BKGnet predictions ‣ 6 BKGnet validation
    ‣ The PAU Survey: Background light estimation with deep learning techniques")),
    but we add a read-out noise contribution to the background error. For the PAUdm
    measurements, the background uncertainty is given by the mean variance per pixel
    (Eq. [9](#S6.E9 "In 6.1 Generating the PAUS catalogue with BKGnet predictions
    ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation with deep
    learning techniques")). Therefore, for PAUdm, this term should also account for
    other error contributions besides shot noise. The third terms in Eqs. [9](#S6.E9
    "In 6.1 Generating the PAUS catalogue with BKGnet predictions ‣ 6 BKGnet validation
    ‣ The PAU Survey: Background light estimation with deep learning techniques")
    and [10](#S6.E10 "In 6.1 Generating the PAUS catalogue with BKGnet predictions
    ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation with deep
    learning techniques") are the contributions from background subtraction uncertainties.
    In PAUdm, this is determined by the subtraction of a background measured in the
    annulus. In contrast, in Equation [10](#S6.E10 "In 6.1 Generating the PAUS catalogue
    with BKGnet predictions ‣ 6 BKGnet validation ‣ The PAU Survey: Background light
    estimation with deep learning techniques") we use the uncertainty provided by
    the network within the aperture where the flux is estimated.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 [9](#S6.E9 "在 6.1 使用 BKGnet 预测生成 PAUS 目录 ‣ 6 BKGnet 验证 ‣ PAU 调查：使用深度学习技术估算背景光")
    和 [10](#S6.E10 "在 6.1 使用 BKGnet 预测生成 PAUS 目录 ‣ 6 BKGnet 验证 ‣ PAU 调查：使用深度学习技术估算背景光")
    反映了当背景通过环形区域或 BKGnet 测量时，通量不确定性的差异。一般来说，通量不确定性的主要贡献者有三方面：净星系通量的不确定性、背景估计的不确定性，以及背景减法引入的不确定性。对于这两种背景估计方法，我们假设净星系通量的不确定性由噪声捕捉。对于
    BKGnet，背景不确定性也由噪声描述（方程 [10](#S6.E10 "在 6.1 使用 BKGnet 预测生成 PAUS 目录 ‣ 6 BKGnet 验证
    ‣ PAU 调查：使用深度学习技术估算背景光")），但我们在背景误差中加入了读出噪声的贡献。对于 PAUdm 测量，背景不确定性由每像素的平均方差给出（方程
    [9](#S6.E9 "在 6.1 使用 BKGnet 预测生成 PAUS 目录 ‣ 6 BKGnet 验证 ‣ PAU 调查：使用深度学习技术估算背景光")）。因此，对于
    PAUdm，此项也应考虑其他错误贡献，而不仅仅是噪声。方程 [9](#S6.E9 "在 6.1 使用 BKGnet 预测生成 PAUS 目录 ‣ 6 BKGnet
    验证 ‣ PAU 调查：使用深度学习技术估算背景光") 和 [10](#S6.E10 "在 6.1 使用 BKGnet 预测生成 PAUS 目录 ‣ 6 BKGnet
    验证 ‣ PAU 调查：使用深度学习技术估算背景光") 中的第三项是背景减法不确定性的贡献。在 PAUdm 中，这由在环形区域中测量的背景减去确定。相反，在方程
    [10](#S6.E10 "在 6.1 使用 BKGnet 预测生成 PAUS 目录 ‣ 6 BKGnet 验证 ‣ PAU 调查：使用深度学习技术估算背景光")
    中，我们使用网络在估算通量的孔径内提供的不确定性。
- en: 6.2 Validating the catalogues
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 验证目录
- en: 'The estimation of the background using an annulus is a viable method when the
    background is flat. In PAUS data, scattered light only affects objects near the
    edges of the images. Hence, for most of the galaxies in PAUS data the background
    should be (approximately) flat and we should not expect large differences between
    the BKGnet and the PAUdm catalogues. Comparing the fluxes estimated with Equation
    [8](#S6.E8 "In 6.1 Generating the PAUS catalogue with BKGnet predictions ‣ 6 BKGnet
    validation ‣ The PAU Survey: Background light estimation with deep learning techniques"),
    we find a 2% difference between the two approaches. On the other hand, the uncertainties
    estimated with BKGnet (Eq. [10](#S6.E10 "In 6.1 Generating the PAUS catalogue
    with BKGnet predictions ‣ 6 BKGnet validation ‣ The PAU Survey: Background light
    estimation with deep learning techniques")) are 4% lower than for PAUdm.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用环形区域来估算背景是一种可行的方法，特别是当背景是平坦的时候。在 PAUS 数据中，散射光只会影响图像边缘附近的物体。因此，对于 PAUS 数据中的大多数星系，背景应该是（大致上）平坦的，我们不应该期望
    BKGnet 和 PAUdm 目录之间存在大的差异。通过与方程 [8](#S6.E8 "在 6.1 生成 PAUS 目录与 BKGnet 预测 ‣ 6 BKGnet
    验证 ‣ PAU 调查：利用深度学习技术进行背景光估算") 计算得到的通量进行比较，我们发现这两种方法之间有 2% 的差异。另一方面，使用 BKGnet 估算的不确定性（方程
    [10](#S6.E10 "在 6.1 生成 PAUS 目录与 BKGnet 预测 ‣ 6 BKGnet 验证 ‣ PAU 调查：利用深度学习技术进行背景光估算")）比
    PAUdm 低 4%。
- en: We need to determine which catalogue provides better photometry estimates. To
    do so, we use the fact that PAUCam takes multiple observations of the same object
    in all NB filters. We can compare different exposures of the same object, which
    should be comparable once the background noise is subtracted. This is formulated
    as
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确定哪个目录提供了更好的光度估计。为此，我们利用 PAUCam 在所有 NB 过滤器中对同一物体进行多次观测的事实。我们可以比较同一物体的不同曝光，一旦背景噪声被去除，这些曝光应该是可比的。这可以表述为
- en: '|  | $D\equiv\frac{(e_{\rm 1}-e_{\rm 2})}{\sqrt{(\sigma^{2}_{\rm 1}+\sigma^{2}_{\rm
    2})}},$ |  | (11) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $D\equiv\frac{(e_{\rm 1}-e_{\rm 2})}{\sqrt{(\sigma^{2}_{\rm 1}+\sigma^{2}_{\rm
    2})}},$ |  | (11) |'
- en: where $e_{i}$ are different exposures of the same object and $\sigma_{i}$ the
    associated uncertainties. The distribution of $D$ should be a Gaussian with unit
    variance if the photometry is robust and the errors are properly accounted for.
    We call this the duplicates test.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $e_{i}$ 是同一物体的不同曝光，而 $\sigma_{i}$ 是相关的不确定性。如果光度测量是可靠的且误差得到了适当考虑，则 $D$ 的分布应该是方差为
    1 的高斯分布。我们称之为重复测试。
- en: '![Refer to caption](img/e302277c8e0d0f093a83a52be45eb1f4.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e302277c8e0d0f093a83a52be45eb1f4.png)'
- en: 'Figure 11: BKGnet validation with the duplicates distribution test. We plot
    the width of the distribution defined in Equation ([11](#S6.E11 "In 6.2 Validating
    the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation
    with deep learning techniques")) as a function of wavelength for the catalogue
    generated with BKGnet (black line) and the current PAUdm catalogue (orange line).
    The dashed line corresponds to the results excluding all objects flagged in PAUdm.
    The solid line includes all objects.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 使用重复分布测试进行 BKGnet 验证。我们将方程 ([11](#S6.E11 "在 6.2 验证目录 ‣ 6 BKGnet 验证 ‣
    PAU 调查：利用深度学习技术进行背景光估算")) 中定义的分布宽度作为波长的函数绘制，比较了使用 BKGnet 生成的目录（黑线）和当前 PAUdm 目录（橙线）。虚线对应于排除
    PAUdm 中所有标记物体的结果。实线包括所有物体。'
- en: 'Figure [11](#S6.F11 "Figure 11 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation
    ‣ The PAU Survey: Background light estimation with deep learning techniques")
    shows the results of the duplicates test as a function of wavelength. We estimate
    $\sigma_{\rm 68}[D]$ (Eq. [11](#S6.F11 "Figure 11 ‣ 6.2 Validating the catalogues
    ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation with deep
    learning techniques")) for each NB with the BKGnet (black line) and PAUdm (orange
    line) catalogues. It is possible to flag photometric outliers based on an ellipticity
    parameter to detect strongly varying backgrounds. The dashed lines in Figure [11](#S6.F11
    "Figure 11 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey:
    Background light estimation with deep learning techniques") show the results when
    we exclude such flagged objects. Dropping flagged objects does not significantly
    change the measurements for BKGnet, but we see a clear improvement for the PAUdm
    measurements. The improvement is particularly prominent for the NB755 filter (at
    7500Å), which is affected by telluric absorption of O[2] in the atmosphere. Interestingly
    BKGnet seems to know how to deal with these objects, showing that BKGnet is more
    robust towards various sources of bias, not only scattered light. When we consider
    all NBs, we find $\langle\sigma_{\rm 68}[D]\rangle=1.00$ for BKGnet, which is
    what we would expect for correct photometry. On the other hand, the current PAUdm
    catalogue yields $\langle\sigma_{\rm 68}[D]\rangle$ = 1.10, i.e. it overestimates
    the uncertainties.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图[11](#S6.F11 "图 11 ‣ 6.2 验证目录 ‣ 6 BKGnet 验证 ‣ PAU 调查：利用深度学习技术估计背景光")展示了重复测试结果随波长的变化。我们用BKGnet（黑线）和PAUdm（橙线）目录估算了每个NB的$\sigma_{\rm
    68}[D]$（公式[11](#S6.F11 "图 11 ‣ 6.2 验证目录 ‣ 6 BKGnet 验证 ‣ PAU 调查：利用深度学习技术估计背景光")）。可以根据椭圆度参数标记光度异常值，以检测背景强烈变化。图[11](#S6.F11
    "图 11 ‣ 6.2 验证目录 ‣ 6 BKGnet 验证 ‣ PAU 调查：利用深度学习技术估计背景光")中的虚线显示了排除这些标记对象后的结果。排除标记对象不会显著改变BKGnet的测量结果，但我们看到PAUdm测量有明显改善。对于受大气中O[2]的地球气体吸收影响的NB755滤光片（在7500Å），这种改善尤为明显。有趣的是，BKGnet似乎知道如何处理这些对象，表明BKGnet对各种偏差源（不仅仅是散射光）更具鲁棒性。当我们考虑所有NB时，BKGnet的$\langle\sigma_{\rm
    68}[D]\rangle=1.00$，这是我们对正确光度测量的期望。另一方面，当前的PAUdm目录得出的$\langle\sigma_{\rm 68}[D]\rangle$
    = 1.10，即它高估了不确定性。
- en: 'The measurement uncertainties should depend on the brightness of the source.
    To explore this we show $\sigma_{\rm 68}[D]$ as a function of the Subaru $i_{\rm
    Auto}$ magnitude in Figure [12](#S6.F12 "Figure 12 ‣ 6.2 Validating the catalogues
    ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation with deep
    learning techniques"). In the PAUdm catalogue there is a strong trend with magnitude.
    At the bright end, the fluxes differ by more than 20% compared to the expectation.
    This trend disappears when we predict the background and uncertainties with BKGnet.
    To explore the origin of the trend further we used the background prediction from
    BKGnet but the errors from the annulus method. As the blue dotted line in Figure [12](#S6.F12
    "Figure 12 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey:
    Background light estimation with deep learning techniques") shows, we find the
    same trend with magnitude. This implies that it is caused by the estimated uncertainties
    for the annulus method. Moreover, the blue dotted line lies below the PAUdm line.
    The only difference between these two curves is the background value prediction
    (not the error). Therefore, the predictions with PAUdm are more accurate than
    those with the annulus method.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 测量不确定性应取决于源的亮度。为探讨这一点，我们在图[12](#S6.F12 "图 12 ‣ 6.2 验证目录 ‣ 6 BKGnet 验证 ‣ PAU
    调查：利用深度学习技术估计背景光")中展示了$\sigma_{\rm 68}[D]$与Subaru $i_{\rm Auto}$星等的关系。在PAUdm目录中，星等与亮度之间存在强趋势。在亮度较高的部分，与预期相比，通量差异超过20%。当我们用BKGnet预测背景和不确定性时，这种趋势消失。为了进一步探讨这种趋势的起源，我们使用了BKGnet的背景预测，但使用了环形方法的误差。如图[12](#S6.F12
    "图 12 ‣ 6.2 验证目录 ‣ 6 BKGnet 验证 ‣ PAU 调查：利用深度学习技术估计背景光")中的蓝色虚线所示，我们发现了相同的星等趋势。这表明这是由环形方法估算的不确定性造成的。此外，蓝色虚线低于PAUdm线。这两条曲线之间的唯一区别是背景值预测（而非误差）。因此，PAUdm的预测比环形方法的预测更为准确。
- en: '![Refer to caption](img/f4c9bba78a3121461a830d85316dcae9.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f4c9bba78a3121461a830d85316dcae9.png)'
- en: 'Figure 12: BKGnet validation with the duplicates distribution test. We plot
    the width of the distribution defined in Equation ([11](#S6.E11 "In 6.2 Validating
    the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation
    with deep learning techniques")) as a function of $i_{\rm auto}$ in the Subaru
    $i$ band for the catalogue generated with BKGnet (black solid line), the current
    PAUdm catalogue (orange dashed line) and a mixed catalogue with the predictions
    from BKGnet and the errors from PAUdm (blue dotted line).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 使用重复分布测试验证 BKGnet。我们将根据 $i_{\rm auto}$ 在 Subaru $i$ 波段的分布宽度绘制为 BKGnet
    生成的目录 (黑色实线)、当前的 PAUdm 目录 (橙色虚线) 以及包含 BKGnet 预测和 PAUdm 错误的混合目录 (蓝色虚线)。'
- en: 'To further validate the BKGnet catalogue we run BCNz2 (Eriksen et al., [2019](#bib.bib16))
    using the fluxes determined using BKGnet. For this test, we exclude the objects
    flagged in the PAUdm catalogue, in order to use exactly the same objects as in
    (Eriksen et al., [2019](#bib.bib16)). However, as shown in Figures [11](#S6.F11
    "Figure 11 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey:
    Background light estimation with deep learning techniques") and [12](#S6.F12 "Figure
    12 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey: Background
    light estimation with deep learning techniques"), we do not need to exclude these
    objects. The photo-$z$s are compared to secure spectroscopic estimates from zCOSMOS
    DR3 (Lilly et al., [2007](#bib.bib32)) with $i_{\rm AB}<22.5$. We split the sample
    based on a quality parameter defined as:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步验证 BKGnet 目录，我们使用 BKGnet 确定的通量运行 BCNz2 (Eriksen 等人，[2019](#bib.bib16))。对于这个测试，我们排除了在
    PAUdm 目录中标记的对象，以便与 (Eriksen 等人，[2019](#bib.bib16)) 中使用的对象完全相同。然而，如图 [11](#S6.F11
    "Figure 11 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey:
    Background light estimation with deep learning techniques") 和 [12](#S6.F12 "Figure
    12 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey: Background
    light estimation with deep learning techniques") 所示，我们不需要排除这些对象。将 photo-$z$ 与
    zCOSMOS DR3 (Lilly 等人，[2007](#bib.bib32)) 中 $i_{\rm AB}<22.5$ 的安全光谱估计进行比较。我们根据定义的质量参数对样本进行划分：'
- en: '|  | ${\rm Qz}\equiv\frac{\chi^{2}}{N_{\rm f}-3}\Big{(}\frac{z_{\rm quant}^{\rm
    99}-z_{\rm quant}^{\rm 1}}{\rm ODDS(\Delta z=0.01)}\Big{)},$ |  | (12) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\rm Qz}\equiv\frac{\chi^{2}}{N_{\rm f}-3}\Big{(}\frac{z_{\rm quant}^{\rm
    99}-z_{\rm quant}^{\rm 1}}{\rm ODDS(\Delta z=0.01)}\Big{)},$ |  | (12) |'
- en: where $\chi^{2}/(n_{\rm f}-1)$ is the reduced chi-squared from the template
    fit and the $z_{\rm quant}$ are the percentiles of $(z_{\rm photo}-z_{\rm spec})/(1+z_{\rm
    spec}$). The ODDS is defined as
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\chi^{2}/(n_{\rm f}-1)$ 是来自模板拟合的归一化卡方，而 $z_{\rm quant}$ 是 $(z_{\rm photo}-z_{\rm
    spec})/(1+z_{\rm spec})$ 的百分位数。ODDS 被定义为
- en: '|  | ${\rm ODDS}\equiv\int_{z_{\rm b}-\Delta z}^{z_{\rm b}+\Delta z}{\rm dz\
    p(z)},$ |  | (13) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\rm ODDS}\equiv\int_{z_{\rm b}-\Delta z}^{z_{\rm b}+\Delta z}{\rm dz\
    p(z)},$ |  | (13) |'
- en: where $z_{\rm b}$ is the mode of the $p(z)$ and $\Delta z$ defines a redshift
    interval around the peak. In PAUS, a galaxy is considered an outlier if
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $z_{\rm b}$ 是 $p(z)$ 的模式，而 $\Delta z$ 定义了峰值周围的红移区间。在 PAUS 中，如果
- en: '|  | $&#124;z_{\rm photo}-z_{\rm spec}&#124;\ /\ (1+z_{\rm spec})>0.02.$ |  |
    (14) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | $|z_{\rm photo}-z_{\rm spec}|/(1+z_{\rm spec})>0.02.$ |  | (14) |'
- en: Notice that this outlier definition is very strict. In broad band photometry,
    a common outlier definition is $|z_{\rm photo}-z_{\rm spec}|>0.15\,(1+z_{\rm spec})$,
    e.g. Ilbert et al. ([2006](#bib.bib22)); Bilicki et al. ([2018](#bib.bib5)).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个离群值定义非常严格。在宽带光度测量中，常见的离群值定义是 $|z_{\rm photo}-z_{\rm spec}|>0.15\,(1+z_{\rm
    spec})$，例如 Ilbert 等人 ([2006](#bib.bib22)); Bilicki 等人 ([2018](#bib.bib5))。
- en: 'Table [2](#S6.T2 "Table 2 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation
    ‣ The PAU Survey: Background light estimation with deep learning techniques")
    lists the outlier rate and the photometric redshift precision obtained with BCNz2
    for the two catalogues. To quantify the redshift precision we use $\sigma_{\rm
    68}$ (Eq. [7](#S4.E7 "In 4.2 BKGnet predictions on simulations ‣ 4 Testing BKGnet
    on simulations ‣ The PAU Survey: Background light estimation with deep learning
    techniques")). The photometric redshift precision does not improve significantly
    between the two catalogues, but we find a reduction in the outlier rate. If we
    consider the complete sample (100%) this improvement is small. This might be because
    in the full sample the outliers are dominated by photo-$z$ outliers, rather than
    outliers on the photometry itself. However, if we cut using the $Qz$ parameter
    to get the best 20% and 50% of the sample, we notice that the outlier rate reduces
    significantly. These outliers should be dominated by photometric outliers. For
    the best 50% of the sample we reduce the number of outliers by 25%, whereas for
    the best 20% of objects this improvement rises to a 35%. This shows once more
    that BKGnet is a statistically accurate method that is also robust.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [2](#S6.T2 "Table 2 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation
    ‣ The PAU Survey: Background light estimation with deep learning techniques")
    列出了使用BCNz2获得的离群值率和光度红移精度。为了量化红移精度，我们使用$\sigma_{\rm 68}$（参见公式 [7](#S4.E7 "In 4.2
    BKGnet predictions on simulations ‣ 4 Testing BKGnet on simulations ‣ The PAU
    Survey: Background light estimation with deep learning techniques")）。两个目录之间的光度红移精度没有显著提高，但我们发现离群值率有所减少。如果我们考虑完整样本（100%），这种改进是微小的。这可能是因为在完整样本中，离群值主要由光度$z$离群值主导，而不是光度本身的离群值。然而，如果我们使用$Qz$参数来选取最佳的20%和50%的样本，我们会发现离群值率显著降低。这些离群值应主要由光度离群值主导。对于最佳的50%样本，我们将离群值的数量减少了25%，而对于最佳的20%对象，这一改进上升到35%。这再次表明BKGnet是一种统计上准确且稳健的方法。'
- en: '|  | Outlier percentage | $10^{3}\sigma_{\rm 68}$ |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | 离群值百分比 | $10^{3}\sigma_{\rm 68}$ |'
- en: '| --- | --- | --- |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Percentage | BKGnet | PAUdm | BKGnet | PAUdm |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 百分比 | BKGnet | PAUdm | BKGnet | PAUdm |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 20 | 3.5 | 5.4 | 2.0 | 2.1 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 3.5 | 5.4 | 2.0 | 2.1 |'
- en: '| 50 | 3.8 | 5.1 | 3.6 | 3.7 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 3.8 | 5.1 | 3.6 | 3.7 |'
- en: '| 80 | 10.4 | 11.3 | 5.8 | 6.0 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 80 | 10.4 | 11.3 | 5.8 | 6.0 |'
- en: '| 100 | 16.7 | 17.5 | 8.4 | 8.6 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 16.7 | 17.5 | 8.4 | 8.6 |'
- en: 'Table 2: Photo-$z$ outlier rate and accuracy obtained with BCNz2 for the BKGnet
    and the PAUdm catalogues. The percentages correspond to the samples selected by
    the photo-$z$ quality parameters $Qz$.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 使用BCNz2获得的光度$z$离群值率和精度，适用于BKGnet和PAUdm目录。百分比对应于通过光度$z$质量参数$Qz$选择的样本。'
- en: 7 Conclusions
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'Imaging surveys need accurate background subtraction methods to obtain precise
    source photometry. We have developed a deep learning method to predict the background
    for astronomical images that are affected by scattered light. The algorithm has
    been developed to predict the background on images taken with PAUCam. The edges
    of PAUCam images are affected by scattered light (see Fig. [1](#S2.F1 "Figure
    1 ‣ 2.3 Scattered-light templates ‣ 2 Modelling scattered light ‣ The PAU Survey:
    Background light estimation with deep learning techniques")), especially in the
    bluer bands. In 2016, the camera was modified to reduce the amount of scattered
    light. While the amount of scattered light decreased drastically, PAUcam images
    still contain a significant amount of scattered light (see Fig. [2](#S2.F2 "Figure
    2 ‣ 2.3 Scattered-light templates ‣ 2 Modelling scattered light ‣ The PAU Survey:
    Background light estimation with deep learning techniques")).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '成像调查需要准确的背景减除方法来获得精确的源光度。我们开发了一种深度学习方法来预测受散射光影响的天文图像的背景。该算法已被开发用于预测PAUCam拍摄的图像的背景。PAUCam图像的边缘受散射光影响（参见图
    [1](#S2.F1 "Figure 1 ‣ 2.3 Scattered-light templates ‣ 2 Modelling scattered light
    ‣ The PAU Survey: Background light estimation with deep learning techniques")），尤其是在蓝色波段。2016年，相机进行了改进以减少散射光的量。尽管散射光的量大幅减少，PAUCam图像仍包含显著的散射光（参见图
    [2](#S2.F2 "Figure 2 ‣ 2.3 Scattered-light templates ‣ 2 Modelling scattered light
    ‣ The PAU Survey: Background light estimation with deep learning techniques")）。'
- en: 'For each band, the scattered light follows the same spatial pattern within
    the CCD and scales approximately linear with the background level. We have constructed
    scattered-light templates and background pixel maps by combining images taken
    with the same NB and normalised by their background level. These scattered-light
    templates show the scattered light variation across the CCD and can be used to
    correct for scattered light (see Fig. [3](#S2.F3 "Figure 3 ‣ 2.4 Scattered-light
    templates as scattered light correcting method ‣ 2 Modelling scattered light ‣
    The PAU Survey: Background light estimation with deep learning techniques")).
    Nevertheless, background fluctuations due to external conditions (e.g. Moon, seeing,
    airmass) can trigger differences on scattered light from night to night. To accurately
    correct scattered light with scattered-light templates, we would need to generate
    a scattered-light templates per NB and night. However, even then, fluctuations
    during the night or a small number of available images in a given band can lead
    to inaccurate corrections.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个波段，散射光在 CCD 上遵循相同的空间模式，并且大致与背景水平线性缩放。我们通过结合使用相同 NB 拍摄的图像并根据其背景水平进行归一化，构建了散射光模板和背景像素图。这些散射光模板展示了
    CCD 上的散射光变化，并可用于纠正散射光（见图 [3](#S2.F3 "图 3 ‣ 2.4 散射光模板作为散射光校正方法 ‣ 2 散射光建模 ‣ PAU
    调查：使用深度学习技术的背景光估计")）。然而，由于外部条件（例如月亮、视场、气团）引起的背景波动可能导致夜间散射光的差异。为了准确地用散射光模板纠正散射光，我们需要生成每个
    NB 和每晚的散射光模板。然而，即使如此，夜间波动或某个波段可用图像数量较少也可能导致校正不准确。
- en: 'We therefore developed BKGnet, a deep learning based algorithm that predicts
    the background and its associated uncertainty behind target sources accounting
    for scattered light and other distorting effect. BKGnet consists of a Convolutional
    Neural Network followed by a linear neural network (see Fig. [5](#S3.F5 "Figure
    5 ‣ 3.2 Data: training and test samples ‣ 3 BKGnet: A Deep Learning based method
    to predict the background ‣ The PAU Survey: Background light estimation with deep
    learning techniques")). In the training set we use empty stamps, i.e. without
    a target galaxy, so that we can estimate the true background and use it for training.
    We need to simulate target galaxies in the training sample before masking the
    central region, otherwise the network fails when applied to bright and large sources.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们开发了 BKGnet，这是一种基于深度学习的算法，用于预测目标源后的背景及其相关不确定性，考虑到散射光和其他扭曲效应。BKGnet 包括一个卷积神经网络，后跟一个线性神经网络（见图
    [5](#S3.F5 "图 5 ‣ 3.2 数据：训练和测试样本 ‣ 3 BKGnet：一种基于深度学习的方法来预测背景 ‣ PAU 调查：使用深度学习技术的背景光估计")）。在训练集中，我们使用空白印章，即没有目标星系，以便我们可以估计真实背景并用于训练。在遮蔽中心区域之前，我们需要在训练样本中模拟目标星系，否则当应用于明亮且大的源时，网络会失败。
- en: 'We first tested the predictions on PAUCam empty stamps, i.e. without target
    galaxies. For data taken before the intervention, BKGnet improves over the scattered-light
    templates + annulus prediction by $37\%$. The scattered-light templates correction
    fails in many of the bands, specially on the bluer filter tray, which is affected
    the most by scattered light (left panel of Fig. [9](#S5.F9 "Figure 9 ‣ 5 BKGnet
    on PAUCam images ‣ The PAU Survey: Background light estimation with deep learning
    techniques")). For data taken after the intervention, BKGnet improves over the
    scattered-light templates + annulus prediction by $17\%$ (right panel of Fig. [9](#S5.F9
    "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background light estimation
    with deep learning techniques")).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在 PAUCam 空白印章上测试了预测，即没有目标星系。对于干预前拍摄的数据，BKGnet 在散射光模板 + 环形预测上的改进为 $37\%$。散射光模板的校正在许多波段中失败，特别是在蓝色滤镜托盘上，这个波段受散射光的影响最大（图
    [9](#S5.F9 "图 9 ‣ 5 BKGnet 在 PAUCam 图像上的应用 ‣ PAU 调查：使用深度学习技术的背景光估计") 的左侧面板）。对于干预后拍摄的数据，BKGnet
    在散射光模板 + 环形预测上的改进为 $17\%$（图 [9](#S5.F9 "图 9 ‣ 5 BKGnet 在 PAUCam 图像上的应用 ‣ PAU 调查：使用深度学习技术的背景光估计")
    的右侧面板）。
- en: 'BKGnet also predicts the uncertainty associated with the background prediction.
    For that, we use the log likelihood of a Gaussian centered at the background true
    value as loss function (Eq. [4](#S3.E4 "In 3.3 Training process and loss function
    ‣ 3 BKGnet: A Deep Learning based method to predict the background ‣ The PAU Survey:
    Background light estimation with deep learning techniques")). To validate BKGnet,
    we test on empty positions and estimate the difference between the prediction
    and the background level, divided by the estimated uncertainty. For the annulus
    method, we find that the errors are underestimated by 47% (Fig [10](#S5.F10 "Figure
    10 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background light estimation with
    deep learning techniques")). On the other hand, with BKGnet this quantity is normally
    distributed around zero with unit variance, showing that the uncertainties are
    correctly estimated.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: BKGnet还预测了与背景预测相关的不确定性。为此，我们使用以背景真实值为中心的高斯分布的对数似然作为损失函数（见公式 [4](#S3.E4 "在3.3训练过程和损失函数
    ‣ 3 BKGnet：一种基于深度学习的方法来预测背景 ‣ PAU调查：使用深度学习技术估计背景光")）。为了验证BKGnet，我们在空位置进行测试，估计预测值与背景水平之间的差异，并将其除以估计的不确定性。对于环形方法，我们发现误差被低估了47%（见图
    [10](#S5.F10 "图10 ‣ 5 BKGnet在PAUCam图像上 ‣ PAU调查：使用深度学习技术估计背景光")）。另一方面，使用BKGnet时，这一量通常分布在零附近，方差为1，显示出不确定性被正确估计。
- en: 'We generated a PAUS catalogue for the COSMOS field using BKGnet to predict
    the background. To validate the catalogue we took advantage of having multiple
    measurements of the same object. The resulting distribution of differences in
    flux measurements should be a Gaussian of unit variance (Eq. [11](#S6.E11 "In
    6.2 Validating the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey: Background
    light estimation with deep learning techniques")). The results demonstrate that
    BKGnet improves the photometry with respect to the current background subtraction
    algorithm. We test the performance for the full catalogue and a catalogue where
    we exclude all objects flagged in the current catalogue version. When excluding
    flagged objects, we find very similar results with the BKGnet catalogue and the
    current catalogue. However, when testing the full catalogue, we find a large improvement
    for BKGnet. It specially improves the results in a region with high atmospheric
    absorption, demonstrating that it is more robust against sources of bias while
    still being statistically accurate. It also removes a strong systematic trend
    with $i$-band magnitude, that disappears when the uncertainties are estimated
    with the network.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用BKGnet生成了COSMOS领域的PAUS目录以预测背景。为了验证该目录，我们利用了对同一对象的多个测量。流量测量差异的分布结果应该是方差为1的高斯分布（见公式
    [11](#S6.E11 "在6.2验证目录 ‣ 6 BKGnet验证 ‣ PAU调查：使用深度学习技术估计背景光")）。结果表明，BKGnet在光度测量方面相较于当前的背景减除算法有所改进。我们对完整目录和排除了当前目录版本中所有标记对象的目录进行了性能测试。排除标记对象时，BKGnet目录和当前目录的结果非常相似。然而，在测试完整目录时，我们发现BKGnet有显著改进。它特别在高大气吸收区域改善了结果，表明其对偏差源更加鲁棒，同时仍然保持统计准确性。它还消除了与$i$-带亮度相关的强系统性趋势，当使用网络估计不确定性时，这一趋势消失了。
- en: Finally, as the aim of PAUS is to provide accurate redshifts for large samples
    of galaxies, we have run the BCNz2 code using the BKGnet catalogue. BKGnet reduces
    the outlier rate by a 25% and 35% respectively for the best 50% and 20% photo-$z$
    samples, while the accuracy is not affected.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，由于PAUS的目标是为大量星系提供准确的红移，我们使用BKGnet目录运行了BCNz2代码。BKGnet分别将最佳50%和20% photo-$z$样本的离群率降低了25%和35%，而准确性没有受到影响。
- en: \textcolor
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackWith BKGnet we have optimised the background subtraction task, one of the
    image processing steps in photometric surveys that can improve the redshift estimation
    and classification of galaxies. Deep learning algorithms that predict these quantities
    directly from images have to subtract the background internally. Therefore, the
    understanding from BKGnet will also help to optimise such deep learning algorithms.
    Although the network has been tested with PAUCam images, the concept should also
    be applicable to future imaging surveys as Euclid and LSST.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用BKGnet，我们优化了背景减除任务，这是光度测量调查中的一个图像处理步骤，可以改善红移估计和星系分类。深度学习算法直接从图像中预测这些量时，需要在内部减去背景。因此，BKGnet的理解也有助于优化这种深度学习算法。尽管该网络已在PAUCam图像上进行了测试，但这一概念也应适用于未来的成像调查，如Euclid和LSST。
- en: Acknowledgement
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Funding for PAUS has been provided by Durham University (via the ERC StG DEGAS-259586),
    ETH Zurich, Leiden University (via ERC StG ADULT-279396 and Netherlands Organisation
    for Scientific Research (NWO) Vici grant 639.043.512) and University College London.
    The PAUS participants from Spanish institutions are partially supported by MINECO
    under grants CSD2007-00060, AYA2015-71825, ESP2015-88861, FPA2015-68048, SEV-2016-0588,
    SEV-2016-0597, and MDM-2015-0509, some of which include ERDF funds from the European
    Union. IEEC and IFAE are partially funded by the CERCA program of the Generalitat
    de Catalunya. The PAU data center is hosted by the Port d’Informació Científica
    (PIC), maintained through a collaboration of CIEMAT and IFAE, with additional
    support from Universitat Autònoma de Barcelona and ERDF. CosmoHub has been developed
    by PIC and was partially funded by the "Plan Estatal de Investigación Científica
    y Técnica y de Innovación" program of the Spanish government. We gratefully acknowledge
    the support of NVIDIA Corporation with the donation of the Titan V GPU used for
    this research. This project has received funding from the European Union’s Horizon
    2020 research and innovation programme under grant agreement No 776247\. Adam
    Amara is supported by a Royal Society Wolfson Fellowship. MS has been supported
    by the National Science Centre (grant UMO-2016/23/N/ST9/02963).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: PAUS的资金由杜伦大学（通过ERC StG DEGAS-259586）、苏黎世联邦理工学院、莱顿大学（通过ERC StG ADULT-279396和荷兰科学研究组织（NWO）Vici资助639.043.512）以及伦敦大学学院提供。来自西班牙机构的PAUS参与者部分由MINECO资助，资助项目包括CSD2007-00060、AYA2015-71825、ESP2015-88861、FPA2015-68048、SEV-2016-0588、SEV-2016-0597和MDM-2015-0509，其中一些项目包括来自欧盟的ERDF资金。IEEC和IFAE部分由加泰罗尼亚自治区政府的CERCA计划资助。PAU数据中心由Port
    d’Informació Científica（PIC）托管，通过CIEMAT和IFAE的合作进行维护，并得到巴塞罗那自治大学和ERDF的额外支持。CosmoHub由PIC开发，并部分由西班牙政府的“国家科学技术与创新计划”资助。我们感谢NVIDIA公司捐赠的Titan
    V GPU，用于本研究。该项目获得了欧盟Horizon 2020研究与创新计划的资助，资助协议编号为776247。Adam Amara获得了皇家学会Wolfson奖学金。MS获得了国家科学中心（资助编号UMO-2016/23/N/ST9/02963）的支持。
- en: References
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abbott et al. (2018) Abbott T. M. C., et al., 2018, [ApJS](http://dx.doi.org/10.3847/1538-4365/aae9f0),
    [239, 18](https://ui.adsabs.harvard.edu/abs/2018ApJS..239...18A)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbott等（2018）Abbott T. M. C.等，2018年，[ApJS](http://dx.doi.org/10.3847/1538-4365/aae9f0)，[239,
    18](https://ui.adsabs.harvard.edu/abs/2018ApJS..239...18A)
- en: Alexander et al. (2019) Alexander S., Gleyzer S., McDonough E., Toomey M. W.,
    Usai E., 2019, [p. arXiv:1909.07346](https://ui.adsabs.harvard.edu/abs/2019arXiv190907346A)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alexander等（2019）Alexander S.、Gleyzer S.、McDonough E.、Toomey M. W.、Usai E.，2019年，[p.
    arXiv:1909.07346](https://ui.adsabs.harvard.edu/abs/2019arXiv190907346A)
- en: Bertin & Arnouts (1996) Bertin E., Arnouts S., 1996, [Astronomy and Astrophysics
    Supplement Series](http://dx.doi.org/10.1051/aas:1996164), [117, 393](https://ui.adsabs.harvard.edu/#abs/1996A&AS..117..393B)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertin & Arnouts（1996）Bertin E.、Arnouts S.，1996年，[天文学与天体物理学补刊系列](http://dx.doi.org/10.1051/aas:1996164)，[117,
    393](https://ui.adsabs.harvard.edu/#abs/1996A&AS..117..393B)
- en: Bijaoui (1980) Bijaoui A., 1980, A&A, [84, 81](https://ui.adsabs.harvard.edu/#abs/1980A&A....84...81B)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bijaoui（1980）Bijaoui A.，1980年，A&A，[84, 81](https://ui.adsabs.harvard.edu/#abs/1980A&A....84...81B)
- en: Bilicki et al. (2018) Bilicki M., et al., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201731942),
    [616, A69](https://ui.adsabs.harvard.edu/abs/2018A&A...616A..69B)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bilicki等（2018）Bilicki M.等，2018年，[A&A](http://dx.doi.org/10.1051/0004-6361/201731942)，[616,
    A69](https://ui.adsabs.harvard.edu/abs/2018A&A...616A..69B)
- en: Breiman (2001) Breiman L., 2001, [Mach. Learn.](http://dx.doi.org/10.1023/A:1010933404324),
    45, 5
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Breiman（2001）Breiman L.，2001年，[Mach. Learn.](http://dx.doi.org/10.1023/A:1010933404324)，45，5
- en: Cabayol et al. (2019) Cabayol L., et al., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/sty3129),
    [483, 529](https://ui.adsabs.harvard.edu/#abs/2019MNRAS.483..529C)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cabayol等（2019）Cabayol L.等，2019年，[MNRAS](http://dx.doi.org/10.1093/mnras/sty3129)，[483,
    529](https://ui.adsabs.harvard.edu/#abs/2019MNRAS.483..529C)
- en: Carrasco-Davis et al. (2018) Carrasco-Davis R., et al., 2018, [p. arXiv:1807.03869](https://ui.adsabs.harvard.edu/abs/2018arXiv180703869C)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carrasco-Davis等（2018）Carrasco-Davis R.等，2018年，[p. arXiv:1807.03869](https://ui.adsabs.harvard.edu/abs/2018arXiv180703869C)
- en: Casas et al. (2012) Casas R., et al., 2012, in High Energy, Optical, and Infrared
    Detectors for Astronomy V. p. 845326, [doi:10.1117/12.924640](http://dx.doi.org/10.1117/12.924640)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Casas等（2012）Casas R.等，2012年，收录于《高能、光学和红外天文探测器V》中，页码845326，[doi:10.1117/12.924640](http://dx.doi.org/10.1117/12.924640)
- en: Casas et al. (2016) Casas R., et al., 2016, in Ground-based and Airborne Instrumentation
    for Astronomy VI. p. 99084K, [doi:10.1117/12.2232422](http://dx.doi.org/10.1117/12.2232422)
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Casas等（2016）Casas R.等，2016年，收录于《地基与空中天文仪器VI》中，页码99084K，[doi:10.1117/12.2232422](http://dx.doi.org/10.1117/12.2232422)
- en: Castander et al. (2012) Castander F. J., et al., 2012, in Ground-based and Airborne
    Instrumentation for Astronomy IV. p. 84466D, [doi:10.1117/12.926234](http://dx.doi.org/10.1117/12.926234)
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Castander et al. (2012) Castander F. J., et al., 2012, 在《地面与空中天文仪器 IV》中，第84466D页,
    [doi:10.1117/12.926234](http://dx.doi.org/10.1117/12.926234)
- en: Castander et al. (prep) Castander F., Eriksen M., Serrano S., et al. in prep.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Castander et al. (prep) Castander F., Eriksen M., Serrano S., 等，准备中。
- en: Cover & Hart (2006) Cover T., Hart P., 2006, [IEEE Trans. Inf. Theor.](http://dx.doi.org/10.1109/TIT.1967.1053964),
    13, 21
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cover & Hart (2006) Cover T., Hart P., 2006, [IEEE 信息理论学报](http://dx.doi.org/10.1109/TIT.1967.1053964),
    13, 21
- en: D’Isanto & Polsterer (2018) D’Isanto A., Polsterer K. L., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201731326),
    [609, A111](https://ui.adsabs.harvard.edu/abs/2018A&A...609A.111D)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D’Isanto & Polsterer (2018) D’Isanto A., Polsterer K. L., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201731326),
    [609, A111](https://ui.adsabs.harvard.edu/abs/2018A&A...609A.111D)
- en: Drucker et al. (1996) Drucker H., Burges C. J. C., Kaufman L., Smola A., Vapnik
    V., 1996, Support Vector Regression Machines
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Drucker et al. (1996) Drucker H., Burges C. J. C., Kaufman L., Smola A., Vapnik
    V., 1996, 支持向量回归机
- en: Eriksen et al. (2019) Eriksen M., et al., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz204),
    [484, 4200](https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.4200E)
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eriksen et al. (2019) Eriksen M., et al., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz204),
    [484, 4200](https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.4200E)
- en: Fluri et al. (2018) Fluri J., Kacprzak T., Refregier A., Amara A., Lucchi A.,
    Hofmann T., 2018, [Phys. Rev. D](http://dx.doi.org/10.1103/PhysRevD.98.123518),
    [98, 123518](https://ui.adsabs.harvard.edu/#abs/2018PhRvD..98l3518F)
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluri et al. (2018) Fluri J., Kacprzak T., Refregier A., Amara A., Lucchi A.,
    Hofmann T., 2018, [物理评论 D](http://dx.doi.org/10.1103/PhysRevD.98.123518), [98,
    123518](https://ui.adsabs.harvard.edu/#abs/2018PhRvD..98l3518F)
- en: Gaia Collaboration et al. (2018) Gaia Collaboration et al., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201833051),
    [616, A1](https://ui.adsabs.harvard.edu/abs/2018A&A...616A...1G)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gaia Collaboration et al. (2018) Gaia Collaboration et al., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201833051),
    [616, A1](https://ui.adsabs.harvard.edu/abs/2018A&A...616A...1G)
- en: George & Huerta (2018) George D., Huerta E. A., 2018, [Physics Letters B](http://dx.doi.org/10.1016/j.physletb.2017.12.053),
    [778, 64](https://ui.adsabs.harvard.edu/#abs/2018PhLB..778...64G)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: George & Huerta (2018) George D., Huerta E. A., 2018, [物理学快报 B](http://dx.doi.org/10.1016/j.physletb.2017.12.053),
    [778, 64](https://ui.adsabs.harvard.edu/#abs/2018PhLB..778...64G)
- en: Herbel et al. (2018a) Herbel J., Kacprzak T., Amara A., Refregier A., Lucchi
    A., 2018a, [J. Cosmology Astropart. Phys.](http://dx.doi.org/10.1088/1475-7516/2018/07/054),
    [2018, 054](https://ui.adsabs.harvard.edu/abs/2018JCAP...07..054H)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Herbel et al. (2018a) Herbel J., Kacprzak T., Amara A., Refregier A., Lucchi
    A., 2018a, [J. 宇宙学与天体物理学](http://dx.doi.org/10.1088/1475-7516/2018/07/054), [2018,
    054](https://ui.adsabs.harvard.edu/abs/2018JCAP...07..054H)
- en: Herbel et al. (2018b) Herbel J., Kacprzak T., Amara A., Refregier A., Lucchi
    A., 2018b, [J. Cosmology Astropart. Phys.](http://dx.doi.org/10.1088/1475-7516/2018/07/054),
    [2018, 054](https://ui.adsabs.harvard.edu/abs/2018JCAP...07..054H)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Herbel et al. (2018b) Herbel J., Kacprzak T., Amara A., Refregier A., Lucchi
    A., 2018b, [J. 宇宙学与天体物理学](http://dx.doi.org/10.1088/1475-7516/2018/07/054), [2018,
    054](https://ui.adsabs.harvard.edu/abs/2018JCAP...07..054H)
- en: Ilbert et al. (2006) Ilbert O., et al., 2006, [A&A](http://dx.doi.org/10.1051/0004-6361:20065138),
    [457, 841](https://ui.adsabs.harvard.edu/abs/2006A&A...457..841I)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilbert et al. (2006) Ilbert O., et al., 2006, [A&A](http://dx.doi.org/10.1051/0004-6361:20065138),
    [457, 841](https://ui.adsabs.harvard.edu/abs/2006A&A...457..841I)
- en: Ivezić et al. (2019) Ivezić Ž., et al., 2019, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c),
    [873, 111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivezić et al. (2019) Ivezić Ž., et al., 2019, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c),
    [873, 111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
- en: Kendall & Gal (2017) Kendall A., Gal Y., 2017, [p. arXiv:1703.04977](https://ui.adsabs.harvard.edu/abs/2017arXiv170304977K)
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall & Gal (2017) Kendall A., Gal Y., 2017, [p. arXiv:1703.04977](https://ui.adsabs.harvard.edu/abs/2017arXiv170304977K)
- en: Kendall et al. (2017) Kendall A., Gal Y., Cipolla R., 2017, [p. arXiv:1705.07115](https://ui.adsabs.harvard.edu/abs/2017arXiv170507115K)
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall et al. (2017) Kendall A., Gal Y., Cipolla R., 2017, [p. arXiv:1705.07115](https://ui.adsabs.harvard.edu/abs/2017arXiv170507115K)
- en: Kingma & Ba (2014) Kingma D. P., Ba J., 2014, arXiv e-prints, [p. arXiv:1412.6980](https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6980K)
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma & Ba (2014) Kingma D. P., Ba J., 2014, arXiv 电子预印本, [p. arXiv:1412.6980](https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6980K)
- en: Krizhevsky et al. (2012) Krizhevsky A., Sutskever I., Hinton G. E., 2012, in
    Proceedings of the 25th International Conference on Neural Information Processing
    Systems - Volume 1. NIPS’12. Curran Associates Inc., USA, pp 1097–1105, [http://dl.acm.org/citation.cfm?id=2999134.2999257](http://dl.acm.org/citation.cfm?id=2999134.2999257)
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 (2012) Krizhevsky A., Sutskever I., Hinton G. E., 2012，见《第 25 届国际神经信息处理系统会议论文集
    - 第 1 卷》。NIPS’12。Curran Associates Inc., USA，第 1097–1105 页，[http://dl.acm.org/citation.cfm?id=2999134.2999257](http://dl.acm.org/citation.cfm?id=2999134.2999257)
- en: Laigle et al. (2016) Laigle C., et al., 2016, [ApJS](http://dx.doi.org/10.3847/0067-0049/224/2/24),
    [224, 24](https://ui.adsabs.harvard.edu/abs/2016ApJS..224...24L)
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laigle 等 (2016) Laigle C. 等，2016，[ApJS](http://dx.doi.org/10.3847/0067-0049/224/2/24)，[224,
    24](https://ui.adsabs.harvard.edu/abs/2016ApJS..224...24L)
- en: Laureijs et al. (2011) Laureijs R., et al., 2011, [p. arXiv:1110.3193](https://ui.adsabs.harvard.edu/#abs/2011arXiv1110.3193L)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laureijs 等 (2011) Laureijs R. 等，2011，[第 arXiv:1110.3193 页](https://ui.adsabs.harvard.edu/#abs/2011arXiv1110.3193L)
- en: LeCun et al. (1989) LeCun Y., Boser B., Denker J. S., Henderson D., Howard R. E.,
    Hubbard W., Jackel L. D., 1989, [Neural Computation](http://dx.doi.org/10.1162/neco.1989.1.4.541),
    1, 541
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等 (1989) LeCun Y., Boser B., Denker J. S., Henderson D., Howard R. E.,
    Hubbard W., Jackel L. D., 1989，[Neural Computation](http://dx.doi.org/10.1162/neco.1989.1.4.541)，1，541
- en: Lecun et al. (1998) Lecun Y., Bottou L., Bengio Y., Haffner P., 1998, [Proceedings
    of the IEEE](http://dx.doi.org/10.1109/5.726791), 86, 2278
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lecun 等 (1998) Lecun Y., Bottou L., Bengio Y., Haffner P., 1998，[IEEE 会议论文集](http://dx.doi.org/10.1109/5.726791)，86，2278
- en: Lilly et al. (2007) Lilly S. J., et al., 2007, [ApJS](http://dx.doi.org/10.1086/516589),
    [172, 70](https://ui.adsabs.harvard.edu/abs/2007ApJS..172...70L)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lilly 等 (2007) Lilly S. J. 等，2007，[ApJS](http://dx.doi.org/10.1086/516589)，[172,
    70](https://ui.adsabs.harvard.edu/abs/2007ApJS..172...70L)
- en: Martí et al. (2014) Martí P., Miquel R., Castander F. J., Gaztañaga E., Eriksen
    M., Sánchez C., 2014, [MNRAS](http://dx.doi.org/10.1093/mnras/stu801), [442, 92](https://ui.adsabs.harvard.edu/#abs/2014MNRAS.442...92M)
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martí 等 (2014) Martí P., Miquel R., Castander F. J., Gaztañaga E., Eriksen M.,
    Sánchez C., 2014，[MNRAS](http://dx.doi.org/10.1093/mnras/stu801)，[442, 92](https://ui.adsabs.harvard.edu/#abs/2014MNRAS.442...92M)
- en: Newell (1983) Newell E. B., 1983, in Astronomical Measuring Machines Workshop.
    pp 15–40
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newell (1983) Newell E. B., 1983，《天文测量机器研讨会》。第 15–40 页
- en: Padilla et al. (2016) Padilla C., et al., 2016, in Ground-based and Airborne
    Instrumentation for Astronomy VI. p. 99080Z, [doi:10.1117/12.2231884](http://dx.doi.org/10.1117/12.2231884)
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Padilla 等 (2016) Padilla C. 等，2016，见《地面与空中天文仪器 VI》。第 99080Z 页，[doi:10.1117/12.2231884](http://dx.doi.org/10.1117/12.2231884)
- en: Padilla et al. (2019) Padilla C., et al., 2019, [AJ](http://dx.doi.org/10.3847/1538-3881/ab0412),
    [157, 246](https://ui.adsabs.harvard.edu/abs/2019AJ....157..246P)
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Padilla 等 (2019) Padilla C. 等，2019，[AJ](http://dx.doi.org/10.3847/1538-3881/ab0412)，[157,
    246](https://ui.adsabs.harvard.edu/abs/2019AJ....157..246P)
- en: Pasquet et al. (2019) Pasquet J., Bertin E., Treyer M., Arnouts S., Fouchez
    D., 2019, [A&A](http://dx.doi.org/10.1051/0004-6361/201833617), [621, A26](https://ui.adsabs.harvard.edu/abs/2019A&A...621A..26P)
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pasquet 等 (2019) Pasquet J., Bertin E., Treyer M., Arnouts S., Fouchez D., 2019，[A&A](http://dx.doi.org/10.1051/0004-6361/201833617)，[621,
    A26](https://ui.adsabs.harvard.edu/abs/2019A&A...621A..26P)
- en: Paszke et al. (2017) Paszke A., et al., 2017
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke 等 (2017) Paszke A. 等，2017
- en: Pedregosa et al. (2011) Pedregosa F., et al., 2011, Journal of Machine Learning
    Research, 12, 2825
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pedregosa 等 (2011) Pedregosa F. 等，2011，《机器学习研究杂志》，12，2825
- en: Popowicz & Smolka (2015) Popowicz A., Smolka B., 2015, [MNRAS](http://dx.doi.org/10.1093/mnras/stv1320),
    [452, 809](https://ui.adsabs.harvard.edu/#abs/2015MNRAS.452..809P)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Popowicz & Smolka (2015) Popowicz A., Smolka B., 2015，[MNRAS](http://dx.doi.org/10.1093/mnras/stv1320)，[452,
    809](https://ui.adsabs.harvard.edu/#abs/2015MNRAS.452..809P)
- en: Romanishin (2014) Romanishin W., 2014, An Introduction to Astronomical Photometry
    Using Ccds. Createspace Independent Pub, [https://books.google.es/books?id=0nbMoQEACAAJ](https://books.google.es/books?id=0nbMoQEACAAJ)
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Romanishin (2014) Romanishin W., 2014，《使用 CCD 的天文光度测量导论》。Createspace Independent
    Pub，[https://books.google.es/books?id=0nbMoQEACAAJ](https://books.google.es/books?id=0nbMoQEACAAJ)
- en: Serrano et al. (prep) Serrano S., Castander F., Fernandez E., et al. in prep.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serrano 等 (prep) Serrano S., Castander F., Fernandez E., 等，待发表
- en: Stetson (1987) Stetson P. B., 1987, [Publications of the Astronomical Society
    of the Pacific](http://dx.doi.org/10.1086/131977), [99, 191](https://ui.adsabs.harvard.edu/#abs/1987PASP...99..191S)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stetson (1987) Stetson P. B., 1987，[天文协会出版物](http://dx.doi.org/10.1086/131977)，[99,
    191](https://ui.adsabs.harvard.edu/#abs/1987PASP...99..191S)
- en: Stothert et al. (2018) Stothert L., et al., 2018, [MNRAS](http://dx.doi.org/10.1093/mnras/sty2491),
    [481, 4221](https://ui.adsabs.harvard.edu/#abs/2018MNRAS.481.4221S)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stothert 等 (2018) Stothert L. 等，2018，[MNRAS](http://dx.doi.org/10.1093/mnras/sty2491)，[481,
    4221](https://ui.adsabs.harvard.edu/#abs/2018MNRAS.481.4221S)
- en: Teeninga et al. (2015) Teeninga P., Moschini U., Trager S. C., Wilkinson M.
    H. F., 2015, in 2015 IEEE International Conference on Image Processing (ICIP).
    pp 1046–1050, [doi:10.1109/ICIP.2015.7350959](http://dx.doi.org/10.1109/ICIP.2015.7350959)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Teeninga 等人 (2015) Teeninga P., Moschini U., Trager S. C., Wilkinson M. H. F.,
    2015, 在 2015 IEEE 国际图像处理会议 (ICIP). pp 1046–1050, [doi:10.1109/ICIP.2015.7350959](http://dx.doi.org/10.1109/ICIP.2015.7350959)
- en: Tonello et al. (2019) Tonello N., et al., 2019, [Astronomy and Computing](http://dx.doi.org/10.1016/j.ascom.2019.04.002),
    [27, 171](https://ui.adsabs.harvard.edu/abs/2019A&C....27..171T)
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tonello 等人 (2019) Tonello N., 等人, 2019, [天文学与计算](http://dx.doi.org/10.1016/j.ascom.2019.04.002),
    [27, 171](https://ui.adsabs.harvard.edu/abs/2019A&C....27..171T)
- en: Tortorelli et al. (2018) Tortorelli L., et al., 2018, [p. arXiv:1805.05340](http://adsabs.harvard.edu/abs/2018arXiv180505340T)
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tortorelli 等人 (2018) Tortorelli L., 等人, 2018, [p. arXiv:1805.05340](http://adsabs.harvard.edu/abs/2018arXiv180505340T)
- en: Vafaei Sadr et al. (2019) Vafaei Sadr A., Vos E. E., Bassett B. A., Hosenie
    Z., Oozeer N., Lochner M., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz131),
    [484, 2793](https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.2793V)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vafaei Sadr 等人 (2019) Vafaei Sadr A., Vos E. E., Bassett B. A., Hosenie Z.,
    Oozeer N., Lochner M., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz131),
    [484, 2793](https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.2793V)
- en: Voulodimos et al. (2018) Voulodimos A., Doulamis N., Doulamis A., Protopapadakis
    E., 2018, [Computational Intelligence and Neuroscience](http://dx.doi.org/10.1155/2018/7068349),
    2018, 1
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voulodimos 等人 (2018) Voulodimos A., Doulamis N., Doulamis A., Protopapadakis
    E., 2018, [计算智能与神经科学](http://dx.doi.org/10.1155/2018/7068349), 2018, 1
- en: Werbos (1982) Werbos P. J., 1982, in Drenick R. F., Kozin F., eds, System Modeling
    and Optimization. Springer Berlin Heidelberg, Berlin, Heidelberg, pp 762–770
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Werbos (1982) Werbos P. J., 1982, 在 Drenick R. F., Kozin F., 编, 《系统建模与优化》。Springer
    Berlin Heidelberg, Berlin, Heidelberg, pp 762–770
- en: Xu et al. (2015) Xu B., Wang N., Chen T., Li M., 2015, [p. arXiv:1505.00853](https://ui.adsabs.harvard.edu/#abs/2015arXiv150500853X)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2015) Xu B., Wang N., Chen T., Li M., 2015, [p. arXiv:1505.00853](https://ui.adsabs.harvard.edu/#abs/2015arXiv150500853X)
- en: Zeiler & Fergus (2013a) Zeiler M. D., Fergus R., 2013a, [p. arXiv:1311.2901](https://ui.adsabs.harvard.edu/#abs/2013arXiv1311.2901Z)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler & Fergus (2013a) Zeiler M. D., Fergus R., 2013a, [p. arXiv:1311.2901](https://ui.adsabs.harvard.edu/#abs/2013arXiv1311.2901Z)
- en: Zeiler & Fergus (2013b) Zeiler M. D., Fergus R., 2013b, [p. arXiv:1311.2901](https://ui.adsabs.harvard.edu/abs/2013arXiv1311.2901Z)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler & Fergus (2013b) Zeiler M. D., Fergus R., 2013b, [p. arXiv:1311.2901](https://ui.adsabs.harvard.edu/abs/2013arXiv1311.2901Z)
- en: Zhang & Bloom (2019) Zhang K., Bloom J., 2019, [The Journal of Open Source Software](http://dx.doi.org/10.21105/joss.01651),
    [4, 1651](https://ui.adsabs.harvard.edu/abs/2019JOSS....4.1651Z)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang & Bloom (2019) Zhang K., Bloom J., 2019, [开源软件杂志](http://dx.doi.org/10.21105/joss.01651),
    [4, 1651](https://ui.adsabs.harvard.edu/abs/2019JOSS....4.1651Z)
- en: Appendix A Convolutional Neural Networks
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 卷积神经网络
- en: Machine learning methods are data analysis techniques where the algorithm learns
    from the data. In particular, one of the most popular class of algorithms are
    neural networks (Werbos, [1982](#bib.bib50)), which are designed to recognise
    patterns, usually learned from training data (supervised method). They are mainly
    used for regression and classification problems (Alexander et al., [2019](#bib.bib2)).
    Deep learning is a subset of machine learning that refers to a development of
    neural network technology, involving a large number of layers.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习方法是数据分析技术，其中算法从数据中学习。特别地，最受欢迎的算法类别之一是神经网络（Werbos, [1982](#bib.bib50)），这些网络旨在识别模式，通常从训练数据中学习（监督方法）。它们主要用于回归和分类问题（Alexander
    等人, [2019](#bib.bib2)）。深度学习是机器学习的一个子集，指的是神经网络技术的发展，涉及大量的层。
- en: 'Deep learning methods, and in general any supervised machine learning method,
    model a problem by optimising a set of trainable weights that fit the data. This
    is done in three stages: forward propagation, back propagation and weight optimisation.
    The network starts with the forward propagation. At this stage, the input data
    propagates through all the network layers and then, the network gives a prediction
    for each of the input samples. After that, by comparing with the known true value,
    which is technically called label, the network estimates a prediction error with
    a given loss function. After that, back propagation takes place. Back propagation
    consists of computing the contribution of each weight on the prediction error.
    Such contributions are calculated with the partial derivative of the loss with
    respect to each of the weights. The weight optimisation is the weights correction
    based on the quantities calculated in the back propagation to reduce the error
    in the next iteration.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法，以及一般的任何监督机器学习方法，通过优化一组可训练的权重来建模问题。这个过程分为三个阶段：前向传播、反向传播和权重优化。网络从前向传播开始。在这一阶段，输入数据通过所有网络层进行传播，然后，网络对每个输入样本进行预测。之后，通过与已知真实值（技术上称为标签）进行比较，网络使用给定的损失函数估计预测误差。接下来，进行反向传播。反向传播包括计算每个权重对预测误差的贡献。这些贡献通过损失函数对每个权重的偏导数来计算。权重优化是基于反向传播中计算的量对权重进行校正，以减少下一次迭代中的误差。
- en: 'In this work, we use a Convolutional Neural Network (CNN; Lecun et al., [1998](#bib.bib31);
    Zeiler & Fergus, [2013a](#bib.bib52)). Our network contains four differentiated
    types of layers:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们使用卷积神经网络（CNN；Lecun等，[1998](#bib.bib31)；Zeiler & Fergus，[2013a](#bib.bib52)）。我们的网络包含四种不同类型的层：
- en: '*Convolutional layer*:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积层*：'
- en: This layer makes the network powerful in image and pattern recognition tasks.
    It has a filter, technically named kernel and is usually 2-dimensional, which
    contains a set of trainable weights used to convolve the image. The outcome of
    this layer is the input image convolved with the kernel. In a given convolutional
    layer, one can convolve the input with as many kernels as desired. Each of these
    convolutions will generate a convolved image, which we refer to as channel. All
    of them together are the input of the next layer.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 该层使网络在图像和模式识别任务中变得强大。它有一个滤波器，技术上称为卷积核，通常是二维的，包含一组可训练的权重，用于对图像进行卷积。该层的结果是输入图像与卷积核进行卷积。在给定的卷积层中，可以使用任意数量的卷积核对输入进行卷积。每个卷积操作将生成一个卷积图像，我们称之为通道。所有这些通道一起作为下一层的输入。
- en: '*Pooling layers*:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '*池化层*：'
- en: This layer reduces the dimensionality of the set of convolved images. It applies
    some function (e.g. sum, mean, maximum) to a group of spatially connected pixels
    and reduces the dimensions of such group. For example, it takes 2 consecutive
    pixels and converts them to the mean of both. Although we use it to handle the
    amount of data generated after the convolutions, it also regularises the model
    to avoid learning from non-generalisable noise and details in the training data
    (also known as overfitting).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 该层减少了卷积图像集的维度。它对一组空间上连接的像素应用某些函数（例如和、均值、最大值），并减少该组的维度。例如，它将2个连续像素取平均。尽管我们使用它来处理卷积后生成的数据量，但它还对模型进行正则化，以避免从训练数据中的不可推广噪声和细节中学习（也称为过拟合）。
- en: '*Fully connected layer*:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '*全连接层*：'
- en: This layer is usually the last layer of the network. Its input is the linearised
    outcome of the previous ones (in our network, convolutions + poolings). It applies
    a linear transformation from the input to the output. The slope and bias of the
    linear transformation are the learning parameters.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 该层通常是网络的最后一层。它的输入是前面层的线性化结果（在我们的网络中，包括卷积 + 池化）。它对输入到输出进行线性变换。线性变换的斜率和偏差是学习参数。
- en: '*Batch normalisation layer*:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '*批量归一化层*：'
- en: In this layer the network normalises the output of a previous activation layer.
    It subtracts the mean and divides by the standard deviation. Batch normalisation
    helps to increase the stability of a neural network and avoids over-fitting problems.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在此层中，网络对先前激活层的输出进行归一化。它减去均值并除以标准差。批量归一化有助于提高神经网络的稳定性，并避免过拟合问题。
- en: After each convolution and fully connected layer there is an activation function
    that transforms the outcome. Activation functions are non-linear functions that
    map the outcome of a layer to the input of the following one. An example of an
    activation function is the Rectified Linear Unit (ReLu) (Krizhevsky et al., [2012](#bib.bib27)),
    although we use a variation of this function called LeakyReLu (Xu et al., [2015](#bib.bib51)),
    with which we find better results.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个卷积层和全连接层之后都有一个激活函数，它将输出结果进行转换。激活函数是将一个层的输出映射到下一个层输入的非线性函数。一个激活函数的例子是修正线性单元（ReLu）（Krizhevsky
    等，[2012](#bib.bib27)），尽管我们使用的是这个函数的一个变体叫做 LeakyReLu（Xu 等，[2015](#bib.bib51)），我们发现它能取得更好的结果。
- en: Other terms that one needs to be familiar with are epoch and batch. An epoch
    is an iteration over the complete training dataset. It is common practice to avoid
    feeding the network with all the training sample together. Instead, the training
    data is divided in groups of a certain size and each of these groups is called
    batch. Feeding the network in batches helps it learn faster as in every iteration
    over a batch, it back-propagates updating all the weights. Then, instead of updating
    once per epoch, it updates as many times as there are batches. The amount of variation
    allowed per iteration is regulated by the learning rate.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 其他需要了解的术语包括时期（epoch）和批次（batch）。时期是对完整训练数据集的迭代。通常避免将所有训练样本一次性输入网络。相反，训练数据被分成一定大小的组，每组称为批次。以批次喂入网络可以帮助它更快地学习，因为在每次对一个批次进行迭代时，它会反向传播并更新所有权重。然后，它会根据批次的数量进行更新，而不是每个时期更新一次。每次迭代允许的变化量由学习率调节。
- en: Appendix B Variable annulus
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 可变圆环
- en: \textcolor
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackCurrently, the PAUS photometry pipeline uses an annulus region to estimate
    the astronomical background behind a target source. The inner and outer radii
    are fixed at 30 and 45 pixels from the source, respectively. However, for each
    galaxy, one could adjust the annulus parameters to minimize the effect of the
    target galaxy flux falling inside the ring and background variations between the
    target and the annulus location.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，PAUS 光度学管道使用圆环区域来估算目标源后的天文背景。圆环的内半径和外半径分别固定在距离源 30 和 45 像素处。然而，对于每个银河系，可以调整圆环参数，以最小化目标银河系的光通量落在圆环内的影响以及目标与圆环位置之间的背景变化。
- en: \textcolor
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: blackWe can quantify the amount of extra light falling inside the annulus coming
    from the galaxy ($\Delta_{\rm F}$) and scattered light ($\Delta_{\rm B}$). For
    a flat background, $\Delta_{\rm B}$ should be independent of the annulus location.
    However, this term does depend on the annulus location when the background varies.
    On the other hand, $\Delta_{\rm F}$ is minimized by an annulus further away from
    the source and depends on the galaxy size and the PSF. We define
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以量化来自银河系的光量（$\Delta_{\rm F}$）和散射光（$\Delta_{\rm B}$）在圆环内部的额外光量。对于平坦背景，$\Delta_{\rm
    B}$ 应该与圆环位置无关。然而，当背景变化时，这个项会依赖于圆环位置。另一方面，$\Delta_{\rm F}$ 通过远离源的圆环来最小化，并且依赖于银河系的大小和点扩散函数（PSF）。我们定义
- en: '|  | $\Gamma\equiv\frac{&#124;\Delta_{\rm F}+\Delta_{\rm B}&#124;}{\sigma_{\rm
    b}},$ |  | (15) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Gamma\equiv\frac{&#124;\Delta_{\rm F}+\Delta_{\rm B}&#124;}{\sigma_{\rm
    b}},$ |  | (15) |'
- en: where $\sigma_{\rm b}$ is the error on the background subtraction. $\Gamma$
    measures the relative error on the background prediction due to scattered light
    and the source contribution. We use $\Gamma$ to study the effect of a variable
    annulus by minimising the quantity as a function of annulus radius.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma_{\rm b}$ 是背景减除的误差。$\Gamma$ 测量由于散射光和源贡献导致的背景预测的相对误差。我们使用 $\Gamma$
    来研究可变圆环的影响，通过将该量作为圆环半径的函数来最小化。
- en: \textcolor
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: 'blackWe have tested this on simulations. The background images are simulated
    as in section [3.2](#S3.SS2 "3.2 Data: training and test samples ‣ 3 BKGnet: A
    Deep Learning based method to predict the background ‣ The PAU Survey: Background
    light estimation with deep learning techniques"). The images also contain simulated
    galaxies with the same size (r[50]) and PSF distribution as observed PAUS galaxies.
    Using simulations allows us to evaluate $\Delta_{\rm B}$ on the background simulations
    and $\Delta_{\rm F}$ on the galaxy simulations. This method cannot directly be
    applied to observed images, since it requires distinguishing between the galaxy
    flux and the background. However, it shows under which conditions the annulus
    approach degrades. Our simulations only contain Poisson noise and scattered light.
    We know that real PAUCam images have a more complicated noise pattern and therefore,
    we would also expect the distributions of $\Gamma_{\rm opt}$ to shift towards
    higher values.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: black我们在模拟中测试了这一点。背景图像与第[3.2节](#S3.SS2 "3.2 数据：训练和测试样本 ‣ 3 BKGnet：一种基于深度学习的背景预测方法
    ‣ PAU调查：使用深度学习技术进行背景光估计")中模拟的一样。这些图像还包含具有与观测到的PAUS星系相同大小（r[50]）和PSF分布的模拟星系。使用模拟允许我们评估背景模拟中的$\Delta_{\rm
    B}$和星系模拟中的$\Delta_{\rm F}$。这种方法不能直接应用于观测图像，因为它需要区分星系光度和背景。然而，它展示了在何种条件下环形方法会降级。我们的模拟仅包含泊松噪声和散射光。我们知道真实的PAUCam图像具有更复杂的噪声模式，因此我们也会预期$\Gamma_{\rm
    opt}$的分布会向更高的值移动。
- en: \textcolor
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: 'blackFigure [13](#A2.F13 "Figure 13 ‣ Appendix B Variable annulus ‣ The PAU
    Survey: Background light estimation with deep learning techniques") shows the
    histogram of minimum $\Gamma$ measurements (Eq. [15](#A2.E15 "In Appendix B Variable
    annulus ‣ The PAU Survey: Background light estimation with deep learning techniques"))
    for a set of simulated galaxies. $\Gamma$ is evaluated for different annulus radii,
    moving the inner and outer radii between one and forty pixels from the source
    with fixed $r_{\rm out}-r_{\rm in}$ = 15 pixels. We split the results in galaxies
    in the center of the image (flat background, orange) and galaxies in the borders
    (scattered light, black). Both histograms show a large fraction of galaxies for
    which the annulus can predict the background very accurately (low gamma values).
    For galaxies in the center, we expect a flat background and therefore good results
    with the annulus method.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: black图[13](#A2.F13 "图 13 ‣ 附录 B 可变环形 ‣ PAU调查：使用深度学习技术进行背景光估计")显示了一组模拟星系的最小$\Gamma$测量值的直方图（公式[15](#A2.E15
    "在附录 B 可变环形 ‣ PAU调查：使用深度学习技术进行背景光估计")）。$\Gamma$在不同的环形半径下进行评估，将内半径和外半径在源点之间移动，一直移动到四十像素，固定$r_{\rm
    out}-r_{\rm in}$ = 15像素。我们将结果分为图像中心的星系（平坦背景，橙色）和边界的星系（散射光，黑色）。两个直方图都显示出大量星系，其中环形可以非常准确地预测背景（低gamma值）。对于位于中心的星系，我们预期背景平坦，因此环形方法会得到较好的结果。
- en: \textcolor
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: \textcolor
- en: 'blackThere are also many galaxies (Fig. [13](#A2.F13 "Figure 13 ‣ Appendix
    B Variable annulus ‣ The PAU Survey: Background light estimation with deep learning
    techniques")) on the border with accurate measurements. First of all, not all
    positions at the border are affected by scattered light. For those positions with
    scattered light, the annulus can be placed very close to the target galaxy if
    it is small ($r_{\rm 50}\approx$ 1 or 2 pixels). Then, the background variation
    from the target source to the annulus position would be small. Nevertheless, the
    distribution of border galaxies also shows a tail corresponding to galaxies for
    which the annulus estimation significantly degrades. In these cases, the optimal
    annulus is either too close to the source or capturing a strong background variation.
    If the background variation is very strong, the annulus will tend to get closer
    to the target source in order to minimize $\Delta_{\rm B}$. However, this is not
    possible for bright and large galaxies, since getting closer to the target increases
    $\Delta_{\rm F}$. The annulus method cannot be used to measure the background
    in large galaxies in varying background regions, since it strongly degrades.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在边界上也有许多星系（图 [13](#A2.F13 "图 13 ‣ 附录 B 可变圆环 ‣ PAU 调查：使用深度学习技术进行背景光估计")）有准确的测量数据。首先，并非所有边界位置都受到散射光的影响。对于那些受散射光影响的位置，如果圆环很小（$r_{\rm
    50}\approx$ 1 或 2 像素），圆环可以放置得非常靠近目标星系。这样，从目标源到圆环位置的背景变化将会很小。然而，边界星系的分布也显示出一个尾部，对应于圆环估计显著下降的星系。在这些情况下，最佳圆环要么太靠近源，要么捕捉到强背景变化。如果背景变化非常强，圆环将趋向于靠近目标源以最小化
    $\Delta_{\rm B}$。然而，对于明亮且大型的星系，这种做法是不可能的，因为靠近目标会增加 $\Delta_{\rm F}$。圆环方法不能用于测量背景在背景变化大的大型星系中，因为它会严重退化。
- en: '![Refer to caption](img/32332105b5537ff9df70bbdfb1b69203.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/32332105b5537ff9df70bbdfb1b69203.png)'
- en: 'Figure 13: The $\Gamma_{\rm min}$ distribution for galaxies in the border (scattered
    light affected, black) and in the center (flat background, filled orange) of the
    image.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：边界处（受散射光影响，黑色）和图像中心处（平坦背景，橙色填充）星系的 $\Gamma_{\rm min}$ 分布。
