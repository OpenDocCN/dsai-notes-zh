- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:34:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:34:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2402.04059] Deep Learning for Multivariate Time Series Imputation: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2402.04059] 多变量时间序列插补的深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.04059](https://ar5iv.labs.arxiv.org/html/2402.04059)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.04059](https://ar5iv.labs.arxiv.org/html/2402.04059)
- en: 'Deep Learning for Multivariate Time Series Imputation: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多变量时间序列插补的深度学习：综述
- en: Author Name Affiliation email@example.com    Jun Wang^(1,2,4) The first two
    authors contributed equally to this work.    Wenjie Du^(2∗)    Wei Cao²    Keli
    Zhang³    Wenjia Wang^(1,4)    Yuxuan Liang⁴    Qingsong Wen⁵ ¹Hong Kong University
    of Science and Technology  ²PyPOTS Research Team
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者姓名 所属机构 email@example.com    Jun Wang^(1,2,4) 前两位作者对这项工作贡献相等。    Wenjie Du^(2∗)
       Wei Cao²    Keli Zhang³    Wenjia Wang^(1,4)    Yuxuan Liang⁴    Qingsong Wen⁵
    ¹香港科技大学  ²PyPOTS 研究团队
- en: ³Huawei Noah’s Ark Lab ⁴Hong Kong University of Science and Technology (Guangzhou)
    ⁵Squirrel AI jwangfx@connect.ust.hk, wdu@pypots.com, weicaomsra@gmail.com, zhangkeli1@huawei.com, wenjiawang@ust.hk, yuxliang@outlook.com, qingsongedu@gmail.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³华为诺亚方舟实验室 ⁴香港科技大学（广州） ⁵Squirrel AI jwangfx@connect.ust.hk，wdu@pypots.com，weicaomsra@gmail.com，zhangkeli1@huawei.com，wenjiawang@ust.hk，yuxliang@outlook.com，qingsongedu@gmail.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The ubiquitous missing values cause the multivariate time series data to be
    partially observed, destroying the integrity of time series and hindering the
    effective time series data analysis. Recently deep learning imputation methods
    have demonstrated remarkable success in elevating the quality of corrupted time
    series data, subsequently enhancing performance in downstream tasks. In this paper,
    we conduct a comprehensive survey on the recently proposed deep learning imputation
    methods. First, we propose a taxonomy for the reviewed methods, and then provide
    a structured review of these methods by highlighting their strengths and limitations.
    We also conduct empirical experiments to study different methods and compare their
    enhancement for downstream tasks. Finally, the open issues for future research
    on multivariate time series imputation are pointed out. All code and configurations
    of this work, including a regularly maintained multivariate time series imputation
    paper list, can be found in the GitHub repository [https://github.com/WenjieDu/Awesome_Imputation](https://github.com/WenjieDu/Awesome_Imputation).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛存在的缺失值使得多变量时间序列数据部分观测，破坏了时间序列的完整性，阻碍了有效的时间序列数据分析。近年来，深度学习插补方法在提升损坏时间序列数据的质量方面表现出显著成功，进而提高了下游任务的性能。本文对最近提出的深度学习插补方法进行了全面的综述。首先，我们提出了所评审方法的分类法，然后通过突出这些方法的优点和局限性提供了结构化的评论。我们还进行了实证实验，以研究不同方法并比较它们对下游任务的提升。最后，指出了未来多变量时间序列插补研究的开放问题。所有的代码和配置，包括一个定期维护的多变量时间序列插补论文列表，可以在
    GitHub 仓库中找到 [https://github.com/WenjieDu/Awesome_Imputation](https://github.com/WenjieDu/Awesome_Imputation)。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The data collection process of multivariate time series in various fields, such
    as finance Bai and Ng ([2008](#bib.bib5)), medicine Esteban et al. ([2017](#bib.bib15)),
    and transportation Gong et al. ([2021](#bib.bib18)), is often fraught with difficulties
    and uncertainty, like sensor failures, instable system environment, privacy concerns,
    or other reasons. This leads to datasets usually containing a great number of
    missing values, and can significantly affect the accuracy and reliability of downstream
    analysis and decision-making. For example, the public real-world medical time
    series dataset PhysioNet2012 Silva et al. ([2012](#bib.bib47)) takes even above
    80$\%$ average missing rate, making it challenging to analyze. Consequently, exploring
    how to reasonably and effectively impute missing components in multivariate time
    series data is attractive and essential.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融 Bai 和 Ng ([2008](#bib.bib5))、医学 Esteban 等 ([2017](#bib.bib15)) 和交通运输 Gong
    等 ([2021](#bib.bib18)) 等领域，多变量时间序列的数据收集过程常常充满困难和不确定性，例如传感器故障、系统环境不稳定、隐私问题或其他原因。这导致数据集通常包含大量缺失值，可能严重影响下游分析和决策的准确性和可靠性。例如，公共的现实世界医疗时间序列数据集
    PhysioNet2012 Silva 等 ([2012](#bib.bib47)) 的缺失率甚至超过 80$\%$，使得分析变得具有挑战性。因此，探索如何合理有效地填补多变量时间序列数据中的缺失部分是非常有吸引力和必要的。
- en: The earlier statistical imputation methods have historically been widely used
    for handling missing data. Those methods substitute the missing values with the
    statistics (e.g., zero value, mean value, and last observed value Amiri and Jensen
    ([2016](#bib.bib4))) or simple statistical models, including ARIMA Bartholomew
    ([1971](#bib.bib7)), ARFIMA Hamzaçebi ([2008](#bib.bib20)), and SARIMA Hamzaçebi
    ([2008](#bib.bib20)). Furthermore, machine learning techniques like regression,
    K-nearest neighbor, matrix factorization, etc., have gained prominence in the
    literature for addressing missing values in multivariate time series. Key implementations
    of these approaches include KNNI Altman ([1992](#bib.bib3)), TIDER Liu et al.
    ([2022](#bib.bib31)), MICE Van Buuren and Groothuis-Oudshoorn ([2011](#bib.bib49)),
    etc. While statistical and machine learning imputation methods are simple and
    efficient, they fall short in capturing the intricate temporal relationships and
    complex variation patterns inherent in time series data, resulting in limited
    performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 较早的统计插补方法在处理缺失数据方面历史上被广泛使用。这些方法用统计量（例如，零值、均值和最后观测值 Amiri 和 Jensen（[2016](#bib.bib4)））或简单的统计模型进行缺失值替代，包括
    ARIMA Bartholomew（[1971](#bib.bib7)）、ARFIMA Hamzaçebi（[2008](#bib.bib20)）和 SARIMA
    Hamzaçebi（[2008](#bib.bib20)）。此外，回归、K 最近邻、矩阵分解等机器学习技术在文献中也获得了显著关注，用于处理多变量时间序列中的缺失值。这些方法的关键实现包括
    KNNI Altman（[1992](#bib.bib3)）、TIDER Liu 等（[2022](#bib.bib31)）、MICE Van Buuren
    和 Groothuis-Oudshoorn（[2011](#bib.bib49)）等。尽管统计和机器学习插补方法简单高效，但它们在捕捉时间序列数据中复杂的时间关系和变化模式方面有所不足，导致性能有限。
- en: More recently, deep learning imputation methods have shown great modeling ability
    in missing data imputation. These methods exploit powerful deep learning models
    like Transformers, Variational AutoEncoders (VAEs), Generative Adversarial Networks
    (GANs), and diffusion models to capture the intrinsic properties and potentially
    complex dynamics of time series. In this way, deep learning imputation methods
    can learn the true underlying data distribution from the observed data, so as
    to predict more reliable and reasonable values for the missing components. We
    note that there are several related surveys Khayati et al. ([2020](#bib.bib24));
    Fang and Wang ([2020](#bib.bib16)) that primarily focus on statistical and machine
    learning imputation methods, but they offer only limited consideration of deep
    learning imputation methods. Considering that multivariate time series imputation
    is a crucial data preprocessing step for subsequent time series analysis, a thorough
    and systematic survey on deep multivariate time series imputation methods would
    significantly contribute to the advancement of the time series community.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习插补方法在缺失数据插补方面表现出极强的建模能力。这些方法利用强大的深度学习模型，如 Transformers、变分自编码器（VAEs）、生成对抗网络（GANs）和扩散模型，以捕捉时间序列的内在属性和潜在复杂动态。通过这种方式，深度学习插补方法可以从观测数据中学习到真实的底层数据分布，从而为缺失部分预测出更可靠和合理的值。我们注意到，有几个相关的综述，如
    Khayati 等（[2020](#bib.bib24)）；Fang 和 Wang（[2020](#bib.bib16)），主要集中在统计和机器学习插补方法上，但对深度学习插补方法的考虑较少。考虑到多变量时间序列插补是后续时间序列分析的关键数据预处理步骤，对深度多变量时间序列插补方法进行全面系统的综述将显著促进时间序列领域的进步。
- en: 'In this paper, we endeavor to bridge the existing knowledge gap by providing
    a comprehensive summary of the latest developments in deep learning methods for
    multivariate time series imputation (MTSI). First, we present a succinct introduction
    to the topic, followed by the proposal of a novel taxonomy, categorizing approaches
    based on two perspectives: imputation uncertainty and neural network architecture.
    Imputation uncertainty reflects confidence in imputed values for missing data,
    and capturing this involves stochastically generating samples and conducting imputations
    based on these varied samples Little and Rubin ([2019](#bib.bib29)). Accordingly,
    we categorize imputation methods into predictive ones, offering fixed estimates,
    and generative ones, which provide a distribution of possible values to account
    for imputation uncertainty. For neural network architecture, we explore a range
    of deep learning models tailored for MTSI, including Recurrent Neural Network
    (RNN)-based ones, Graph Neural Network (GNN)-based ones, Convolutional Neural
    Network (CNN)-based ones, attention-based ones, Variational AutoEncoder (VAE)-based
    ones, Generative Adversarial Network (GAN)-based ones, and diffusion-based ones.
    To provide practical imputation guidelines in real scenarios, we conduct extensive
    empirical studies that examine multiple aspects of deep multivariate time series
    imputation models, including imputation performance and improvement on downstream
    tasks like classification. To the best of our knowledge, this is the first comprehensive
    and systematic review of deep learning algorithms in the realm of MSTI, aiming
    to stimulate further research in this field. A corresponding resource that has
    been continuously updated can be found in our GitHub repository¹¹1[https://github.com/WenjieDu/Awesome_Imputation](https://github.com/WenjieDu/Awesome_Imputation).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们力图通过提供多变量时间序列填充（MTSI）深度学习方法最新发展的全面总结，弥合现有的知识差距。首先，我们简要介绍了该主题，然后提出了一种新颖的分类方法，基于填充不确定性和神经网络架构这两个视角对方法进行分类。填充不确定性反映了对缺失数据填充值的信心，这涉及到随机生成样本并基于这些不同的样本进行填充 Little
    和 Rubin ([2019](#bib.bib29))。因此，我们将填充方法分为预测型方法，提供固定估计，以及生成型方法，提供可能值的分布以考虑填充不确定性。对于神经网络架构，我们探讨了多种针对MTSI的深度学习模型，包括基于循环神经网络（RNN）的模型、基于图神经网络（GNN）的模型、基于卷积神经网络（CNN）的模型、基于注意力机制的模型、基于变分自编码器（VAE）的模型、基于生成对抗网络（GAN）的模型以及基于扩散模型的模型。为了在实际场景中提供有效的填充指导，我们进行了广泛的实证研究，考察了深度多变量时间序列填充模型的多个方面，包括填充性能以及在分类等下游任务上的改进。据我们所知，这是第一个全面系统地审查MTSI领域深度学习算法的综述，旨在激发该领域的进一步研究。一个不断更新的相关资源可以在我们的GitHub仓库中找到¹¹1[https://github.com/WenjieDu/Awesome_Imputation](https://github.com/WenjieDu/Awesome_Imputation)。
- en: 'In summary, the contributions of this paper include: 1) A new taxonomy for
    deep multivariate time series imputation methods, considering imputation uncertainty
    and neural network architecture, with a comprehensive methodological review; 2)
    A thorough empirical evaluation of imputation algorithms via the PyPOTS toolkit
    we developed; 3) An exploration of future research opportunities for MTSI.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文的贡献包括：1) 针对深度多变量时间序列填充方法提出了一种新的分类方法，考虑了填充不确定性和神经网络架构，并进行了全面的方法综述；2) 通过我们开发的PyPOTS工具包对填充算法进行了彻底的实证评估；3)
    探索了多变量时间序列填充（MTSI）未来的研究机会。
- en: 2 Preliminary and Taxonomy
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步和分类
- en: 2.1 Background of MTSI
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 MTSI背景
- en: Problem Definition
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题定义
- en: A complete time series dataset on $[0,T]$ typically can be denoted as $\mathcal{D}=\{\mathbf{X}_{i},\mathbf{t}_{i}\}_{i=1}^{N}$.
    Hereby, $\mathbf{X}_{i}=\{x_{1:K,1:L}\}\in\mathcal{R}^{K\times L}$ and $\mathbf{t}_{i}=({t_{1},\cdots,t_{L}})\in[0,T]^{L}$,
    where $K$ is the number of features and $L$ is the length of time series. In the
    missing data context, each complete time series can be split into an observed
    and a missing part, i.e., $\mathbf{X}_{i}=\{\mathbf{X}^{o}_{i},\mathbf{X}^{m}_{i}\}$.
    For encoding the missingness, we also denote an observation matrix as $\mathbf{M}_{i}=\{m_{1:K,1:L}\}$,
    where $m_{k,l}=0$ if $x_{k,l}$ is missing at timestamp $t_{l}$, otherwise $m_{k,l}=1$.
    Furthermore, we can also calculate a time-lag matrix $\boldsymbol{\delta}_{i}=\{\delta_{1:K,1:L}\}$
    by the following rule,
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在$[0,T]$上的完整时间序列数据集通常可以表示为$\mathcal{D}=\{\mathbf{X}_{i},\mathbf{t}_{i}\}_{i=1}^{N}$。其中，$\mathbf{X}_{i}=\{x_{1:K,1:L}\}\in\mathcal{R}^{K\times
    L}$，而$\mathbf{t}_{i}=({t_{1},\cdots,t_{L}})\in[0,T]^{L}$，$K$是特征的数量，$L$是时间序列的长度。在缺失数据的背景下，每个完整的时间序列可以分为观察部分和缺失部分，即$\mathbf{X}_{i}=\{\mathbf{X}^{o}_{i},\mathbf{X}^{m}_{i}\}$。为了编码缺失情况，我们还用$\mathbf{M}_{i}=\{m_{1:K,1:L}\}$表示观察矩阵，其中，如果在时间戳$t_{l}$时$x_{k,l}$缺失，则$m_{k,l}=0$，否则$m_{k,l}=1$。此外，我们还可以通过以下规则计算时间滞后矩阵$\boldsymbol{\delta}_{i}=\{\delta_{1:K,1:L}\}$，
- en: '|  | <math   alttext="\delta_{k,l}=\left\{\begin{array}[]{ll}{0,}&amp;{\text{if
    }l=1}\\ {t_{l}-t_{l-1},}&amp;{\text{if }m_{k,l-1}=1\text{ and }l>1}\\'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\delta_{k,l}=\left\{\begin{array}[]{ll}{0,}&amp;{\text{if
    }l=1}\\ {t_{l}-t_{l-1},}&amp;{\text{if }m_{k,l-1}=1\text{ and }l>1}\\'
- en: '{\delta_{k,l-1}+t_{l}-t_{l-1},}&amp;{\text{if }m_{k,l-1}=0\text{ and }l>1}\end{array}\right."
    display="block"><semantics ><mrow  ><msub ><mi >δ</mi><mrow  ><mi >k</mi><mo >,</mo><mi  >l</mi></mrow></msub><mo
    >=</mo><mrow ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mn >0</mn><mo  >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext  >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >l</mi></mrow><mo >=</mo><mn >1</mn></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow  ><mrow ><msub ><mi  >t</mi><mi >l</mi></msub><mo >−</mo><msub ><mi >t</mi><mrow
    ><mi >l</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow  ><mtext >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >m</mi><mrow ><mi  >k</mi><mo >,</mo><mrow ><mi >l</mi><mo >−</mo><mn >1</mn></mrow></mrow></msub></mrow><mo
    >=</mo><mrow ><mn  >1</mn><mo lspace="0em" rspace="0em"  >​</mo><mtext > and </mtext><mo
    lspace="0em" rspace="0em"  >​</mo><mi >l</mi></mrow><mo >></mo><mn >1</mn></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow  ><mrow ><mrow ><msub  ><mi >δ</mi><mrow ><mi
    >k</mi><mo >,</mo><mrow ><mi >l</mi><mo >−</mo><mn >1</mn></mrow></mrow></msub><mo
    >+</mo><msub ><mi >t</mi><mi >l</mi></msub></mrow><mo >−</mo><msub ><mi  >t</mi><mrow
    ><mi >l</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow  ><mtext >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >m</mi><mrow ><mi  >k</mi><mo >,</mo><mrow ><mi >l</mi><mo >−</mo><mn >1</mn></mrow></mrow></msub></mrow><mo
    >=</mo><mrow ><mn  >0</mn><mo lspace="0em" rspace="0em"  >​</mo><mtext > and </mtext><mo
    lspace="0em" rspace="0em"  >​</mo><mi >l</mi></mrow><mo >></mo><mn >1</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝛿</ci><list ><ci  >𝑘</ci><ci >𝑙</ci></list></apply><apply ><csymbol cd="latexml"
    >cases</csymbol><matrix ><matrixrow ><cn type="integer"  >0</cn><apply ><apply
    ><ci  ><mtext >if </mtext></ci><ci >𝑙</ci></apply><cn type="integer"  >1</cn></apply></matrixrow><matrixrow
    ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑡</ci><ci
    >𝑙</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑡</ci><apply
    ><ci >𝑙</ci><cn type="integer" >1</cn></apply></apply></apply><apply ><apply  ><apply
    ><ci ><mtext  >if </mtext></ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑚</ci><list ><ci >𝑘</ci><apply ><ci >𝑙</ci><cn type="integer" >1</cn></apply></list></apply></apply><apply
    ><cn type="integer" >1</cn><ci ><mtext  > and </mtext></ci><ci >𝑙</ci></apply></apply><apply
    ><cn type="integer" >1</cn></apply></apply></matrixrow><matrixrow ><apply  ><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝛿</ci><list ><ci  >𝑘</ci><apply
    ><ci >𝑙</ci><cn type="integer" >1</cn></apply></list></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑡</ci><ci >𝑙</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑡</ci><apply ><ci >𝑙</ci><cn
    type="integer" >1</cn></apply></apply></apply><apply ><apply  ><apply ><ci ><mtext  >if </mtext></ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑚</ci><list ><ci >𝑘</ci><apply
    ><ci >𝑙</ci><cn type="integer" >1</cn></apply></list></apply></apply><apply ><cn
    type="integer" >0</cn><ci ><mtext > and </mtext></ci><ci >𝑙</ci></apply></apply><apply
    ><cn type="integer" >1</cn></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\delta_{k,l}=\left\{\begin{array}[]{ll}{0,}&{\text{if
    }l=1}\\ {t_{l}-t_{l-1},}&{\text{if }m_{k,l-1}=1\text{ and }l>1}\\ {\delta_{k,l-1}+t_{l}-t_{l-1},}&{\text{if
    }m_{k,l-1}=0\text{ and }l>1}\end{array}\right.</annotation></semantics></math>
    |  |'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: \(\delta_{k,l}=\left\{\begin{array}[]{ll}{0,}&{\text{如果 }l=1}\\ {t_{l}-t_{l-1},}&{\text{如果
    }m_{k,l-1}=1\text{ 且 }l>1}\\ {\delta_{k,l-1}+t_{l}-t_{l-1},}&{\text{如果 }m_{k,l-1}=0\text{
    且 }l>1}\end{array}\right.\)
- en: 'Hence, each incomplete time series is expressed as $\{\mathbf{X}_{i}^{o},\mathbf{M}_{i},\boldsymbol{\delta}_{i}\}$.
    The objective of MTSI is to construct an imputation model $\mathcal{M}_{\theta}$,
    parameterized by $\theta$, to accurately estimate missing values in $\mathbf{X}^{o}$.
    The *imputed* matrix is defined as:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个不完整的时间序列表示为$\{\mathbf{X}_{i}^{o},\mathbf{M}_{i},\boldsymbol{\delta}_{i}\}$。MTSI的目标是构建一个由$\theta$参数化的填补模型$\mathcal{M}_{\theta}$，以准确估计$\mathbf{X}^{o}$中的缺失值。*填补*矩阵定义为：
- en: '|  | $\mathbf{\hat{X}}=\mathbf{{M}}\odot\mathbf{{X}}^{o}+(1-\mathbf{M})\odot\mathbf{\bar{X}},$
    |  | (1) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\hat{X}}=\mathbf{{M}}\odot\mathbf{{X}}^{o}+(1-\mathbf{M})\odot\mathbf{\bar{X}},$
    |  | (1) |'
- en: 'where $\odot$ denotes element-wise multiplication, and $\mathbf{\bar{X}}=\mathcal{M}_{\theta}(\mathbf{{X}}^{o})$
    is the reconstructed matrix. The aim of $\mathcal{M}_{\theta}$ is twofold: (i)
    to make $\mathbf{\hat{X}}$ approximate the true *complete* data $\mathbf{X}$ as
    closely as possible, or (ii) to enhance the downstream task performance using
    $\mathbf{\hat{X}}$ compared to using the original $\mathbf{X}^{o}$.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\odot$表示逐元素乘法，而$\mathbf{\bar{X}}=\mathcal{M}_{\theta}(\mathbf{{X}}^{o})$是重建的矩阵。$\mathcal{M}_{\theta}$的目标有两个：（i）使$\mathbf{\hat{X}}$尽可能接近真实的*完整*数据$\mathbf{X}$，或者（ii）相比于使用原始的$\mathbf{X}^{o}$，利用$\mathbf{\hat{X}}$来提高下游任务的性能。
- en: Missing Mechanism
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缺失机制
- en: 'The missing mechanisms, i.e., the cause of missing data, represent the statistical
    relationship between observations and the probability of missing data Nakagawa
    ([2015](#bib.bib43)). In real-life scenarios, missing mechanisms are inherently
    complex, and the performance of an imputation model is significantly influenced
    by how closely the assumptions we make align with the actual missing data mechanisms.
    According to Robin’s theory Rubin ([1976](#bib.bib44)), the missing mechanisms
    fall into three categories: Missing Completely At Random (MCAR), Missing At Random
    (MAR), and Missing Not At Random (MNAR). MCAR implies that the probability of
    data being missing is independent of both the observed and missing data. Conversely,
    MAR indicates that the missing mechanism depends solely on the observed data.
    MNAR suggests that the missingness is related to the missing data itself and may
    also be influenced by the observed data. These three mechanisms can be formally
    defined as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失机制，即缺失数据的原因，表示观察值与缺失数据概率之间的统计关系 Nakagawa ([2015](#bib.bib43))。在现实生活中，缺失机制本质上是复杂的，填补模型的性能在很大程度上受到我们所做的假设与实际缺失数据机制的匹配程度的影响。根据Robin的理论
    Rubin ([1976](#bib.bib44))，缺失机制分为三类：完全随机缺失（MCAR）、随机缺失（MAR）和非随机缺失（MNAR）。MCAR意味着数据缺失的概率与观察到的数据和缺失数据都无关。相反，MAR表示缺失机制仅依赖于观察数据。MNAR则表明缺失情况与缺失数据本身相关，并且可能受到观察数据的影响。这三种机制可以正式定义如下：
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MCAR: $p(\mathbf{M}|\mathbf{X})=p(\mathbf{M})$,'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MCAR: $p(\mathbf{M}|\mathbf{X})=p(\mathbf{M})$，'
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MAR: $p(\mathbf{M}|\mathbf{X})=p(\mathbf{M}|\mathbf{X}^{o})$,'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MAR: $p(\mathbf{M}|\mathbf{X})=p(\mathbf{M}|\mathbf{X}^{o})$，'
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MNAR: $p(\mathbf{M}|\mathbf{X})=p(\mathbf{M}|\mathbf{X}^{o},\mathbf{X}^{m})$.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MNAR: $p(\mathbf{M}|\mathbf{X})=p(\mathbf{M}|\mathbf{X}^{o},\mathbf{X}^{m})$。'
- en: MCAR and MAR are stronger assumptions compared to MNAR and are considered “ignorable” Little
    and Rubin ([2019](#bib.bib29)). This means that the missing mechanism can be disregarded
    during imputation, focusing solely on learning the data distribution, i.e., $p(\mathbf{X}^{o})$.
    In contrast, MNAR, often more reflective of real-life scenarios, is “non-ignorable”,
    overlooking its missing mechanism can lead to biased parameter estimates. The
    objective here shifts to learning the joint distribution of the data and its missing
    mechanism, i.e., $p(\mathbf{X}^{o},\mathbf{M})$.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与MNAR相比，MCAR和MAR是更强的假设，并被认为是“可忽略的” Little和Rubin ([2019](#bib.bib29))。这意味着在填补过程中可以忽略缺失机制，专注于学习数据分布，即$p(\mathbf{X}^{o})$。相比之下，MNAR更能反映现实生活中的场景，它是“不可忽略的”，忽视其缺失机制可能导致参数估计偏差。在这种情况下，目标转向学习数据及其缺失机制的联合分布，即$p(\mathbf{X}^{o},\mathbf{M})$。
- en: 2.2 Taxonomy of Imputation Methods
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 填补方法的分类
- en: '![Refer to caption](img/fdd3bfda9eda9219f38e4865daab181c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/fdd3bfda9eda9219f38e4865daab181c.png)'
- en: 'Figure 1: The taxonomy of deep learning methods for multivariate time series
    imputation from the view of imputation uncertainty and neural network architecture.
    For each category, one representative model is picked to display.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：从填补不确定性和神经网络架构的角度来看，多变量时间序列填补方法的分类。每个类别中都选取了一个代表模型进行展示。
- en: '| Method | Venue | Category | Imputation Uncertainty | Neural Network Architecture
    | Missing Mechanism |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 会议 | 分类 | 插补不确定性 | 神经网络架构 | 缺失机制 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GRU-D Che et al. ([2018](#bib.bib10)) | Scientific Reports | predictive |
    \faTimes | RNN | MCAR |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| GRU-D Che 等 ([2018](#bib.bib10)) | Scientific Reports | 预测 | \faTimes | RNN
    | MCAR |'
- en: '| M-RNN Yoon et al. ([2019](#bib.bib56)) | TBME | predictive | \faTimes | RNN
    | MCAR |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| M-RNN Yoon 等 ([2019](#bib.bib56)) | TBME | 预测 | \faTimes | RNN | MCAR |'
- en: '| BRITS Cao et al. ([2018](#bib.bib9)) | NeurIPS | predictive | \faTimes |
    RNN | MCAR |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| BRITS Cao 等 ([2018](#bib.bib9)) | NeurIPS | 预测 | \faTimes | RNN | MCAR |'
- en: '| TimesNet Wu et al. ([2023a](#bib.bib53)) | ICLR | predictive | \faTimes |
    CNN | MCAR |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| TimesNet Wu 等 ([2023a](#bib.bib53)) | ICLR | 预测 | \faTimes | CNN | MCAR |'
- en: '| GRIN Cini et al. ([2022](#bib.bib12)) | ICLR | predictive | \faTimes | GNN
    | MCAR / MAR |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| GRIN Cini 等 ([2022](#bib.bib12)) | ICLR | 预测 | \faTimes | GNN | MCAR / MAR
    |'
- en: '| SPIN Marisca et al. ([2022](#bib.bib38)) | NeurIPS | predictive | \faTimes
    | GNN, Attention | MCAR / MAR |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| SPIN Marisca 等 ([2022](#bib.bib38)) | NeurIPS | 预测 | \faTimes | GNN, Attention
    | MCAR / MAR |'
- en: '| CDSA Ma et al. ([2019](#bib.bib37)) | arXiv | predictive | \faTimes | Attention
    | MCAR |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| CDSA Ma 等 ([2019](#bib.bib37)) | arXiv | 预测 | \faTimes | Attention | MCAR
    |'
- en: '| Transformer Vaswani et al. ([2017](#bib.bib50)) | NeurIPS | predictive |
    \faTimes | Attention | MCAR |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Transformer Vaswani 等 ([2017](#bib.bib50)) | NeurIPS | 预测 | \faTimes | Attention
    | MCAR |'
- en: '| SAITS Du et al. ([2023](#bib.bib13)) | ESWA | predictive | \faTimes | Attention
    | MCAR |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| SAITS Du 等 ([2023](#bib.bib13)) | ESWA | 预测 | \faTimes | Attention | MCAR
    |'
- en: '| DeepMVI Bansal et al. ([2021](#bib.bib6)) | VLDB | predictive | \faTimes
    | Attention, CNN | MCAR |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| DeepMVI Bansal 等 ([2021](#bib.bib6)) | VLDB | 预测 | \faTimes | Attention,
    CNN | MCAR |'
- en: '| NRTSI Shan et al. ([2023](#bib.bib46)) | ICASSP | predictive | \faTimes |
    Attention | MCAR |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| NRTSI Shan 等 ([2023](#bib.bib46)) | ICASSP | 预测 | \faTimes | Attention |
    MCAR |'
- en: '| GP-VAE Fortuin et al. ([2020](#bib.bib17)) | AISTATS | generative | \faCheckCircleO
    | VAE, CNN | MCAR / MAR |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| GP-VAE Fortuin 等 ([2020](#bib.bib17)) | AISTATS | 生成 | \faCheckCircleO |
    VAE, CNN | MCAR / MAR |'
- en: '| V-RIN Mulyadi et al. ([2021](#bib.bib42)) | Trans. Cybern. | generative |
    \faCheck | VAE, RNN | MCAR / MAR |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| V-RIN Mulyadi 等 ([2021](#bib.bib42)) | Trans. Cybern. | 生成 | \faCheck | VAE,
    RNN | MCAR / MAR |'
- en: '| supnotMIWAE Kim et al. ([2023](#bib.bib26)) | ICML | generative | \faCheckCircleO
    | VAE | MNAR |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| supnotMIWAE Kim 等 ([2023](#bib.bib26)) | ICML | 生成 | \faCheckCircleO | VAE
    | MNAR |'
- en: '| GRUI-GAN Luo et al. ([2018](#bib.bib35)) | NeurIPS | generative | \faCheckCircleO
    | GAN, RNN | MCAR |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| GRUI-GAN Luo 等 ([2018](#bib.bib35)) | NeurIPS | 生成 | \faCheckCircleO | GAN,
    RNN | MCAR |'
- en: '| E²GAN Luo et al. ([2019](#bib.bib36)) | IJCAI | generative | \faCheckCircleO
    | GAN, RNN | MCAR |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| E²GAN Luo 等 ([2019](#bib.bib36)) | IJCAI | 生成 | \faCheckCircleO | GAN, RNN
    | MCAR |'
- en: '| NAOMI Liu et al. ([2019](#bib.bib30)) | NeurIPS | generative | \faCheckCircleO
    | GAN, RNN | MCAR |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| NAOMI Liu 等 ([2019](#bib.bib30)) | NeurIPS | 生成 | \faCheckCircleO | GAN,
    RNN | MCAR |'
- en: '| SSGAN Miao et al. ([2021](#bib.bib39)) | AAAI | generative | \faCheckCircleO
    | GAN, RNN | MCAR |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| SSGAN Miao 等 ([2021](#bib.bib39)) | AAAI | 生成 | \faCheckCircleO | GAN, RNN
    | MCAR |'
- en: '| CSDI Tashiro et al. ([2021](#bib.bib48)) | NeurIPS | generative | \faCheckCircleO
    | Diffusion, Attention, CNN | MCAR |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| CSDI Tashiro 等 ([2021](#bib.bib48)) | NeurIPS | 生成 | \faCheckCircleO | Diffusion,
    Attention, CNN | MCAR |'
- en: '| SSSD Alcaraz and Strodthoff ([2023](#bib.bib1)) | TMLR | generative | \faCheckCircleO
    | Diffusion, Attention | MCAR |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| SSSD Alcaraz 和 Strodthoff ([2023](#bib.bib1)) | TMLR | 生成 | \faCheckCircleO
    | Diffusion, Attention | MCAR |'
- en: '| CSBI Chen et al. ([2023](#bib.bib11)) | ICML | generative | \faCheckCircleO
    | Diffusion, Attention | MCAR |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| CSBI Chen 等 ([2023](#bib.bib11)) | ICML | 生成 | \faCheckCircleO | Diffusion,
    Attention | MCAR |'
- en: '| MIDM Wang et al. ([2023](#bib.bib51)) | KDD | generative | \faCheckCircleO
    | Diffusion, Attention | MCAR |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| MIDM Wang 等 ([2023](#bib.bib51)) | KDD | 生成 | \faCheckCircleO | Diffusion,
    Attention | MCAR |'
- en: '| PriSTI Liu et al. ([2023](#bib.bib32)) | ICDE | generative | \faCheckCircleO
    | Diffusion, Attention, GNN, CNN | MCAR |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| PriSTI Liu 等 ([2023](#bib.bib32)) | ICDE | 生成 | \faCheckCircleO | Diffusion,
    Attention, GNN, CNN | MCAR |'
- en: '| DA-TASWDM Xu et al. ([2023](#bib.bib55)) | CIKM | generative | \faCheck |
    Diffusion, Attention | MCAR |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| DA-TASWDM Xu 等 ([2023](#bib.bib55)) | CIKM | 生成 | \faCheck | Diffusion, Attention
    | MCAR |'
- en: '| SPD Biloš et al. ([2023](#bib.bib8)) | ICML | generative | \faCheckCircleO
    | Diffusion, Attention | MCAR |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| SPD Biloš 等 ([2023](#bib.bib8)) | ICML | 生成 | \faCheckCircleO | Diffusion,
    Attention | MCAR |'
- en: 'Table 1: Summary of deep learning methods for multivariate time series imputation.
    \faCheck and \faCheckCircleO indicate methods capable of accounting for imputation
    uncertainty, whereas \faTimes denotes methods that do not. Furthermore, \faCheck denotes
    that the methods also define the fidelity score to explicitly measure the imputation
    uncertainty.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：多变量时间序列填充的深度学习方法总结。 \faCheck 和 \faCheckCircleO 表示能够考虑填充不确定性的方法，而 \faTimes
    表示不能够考虑的方法。此外，\faCheck 表示这些方法还定义了保真度评分，以明确测量填充不确定性。
- en: 'To summarize the existing deep multivariate time series imputation methods,
    we propose a taxonomy from the perspectives of imputation uncertainty and neural
    network architecture as illustrated in Figure [1](#S2.F1 "Figure 1 ‣ 2.2 Taxonomy
    of Imputation Methods ‣ 2 Preliminary and Taxonomy ‣ Deep Learning for Multivariate
    Time Series Imputation: A Survey"), and provide a more detailed summary of these
    methods in Table [1](#S2.T1 "Table 1 ‣ 2.2 Taxonomy of Imputation Methods ‣ 2
    Preliminary and Taxonomy ‣ Deep Learning for Multivariate Time Series Imputation:
    A Survey"). For imputation uncertainty, we categorize imputation methods into
    predictive and generative types, based on their ability to yield varied imputations
    that reflect the inherent uncertainty in the imputation process. In the context
    of the neural network architecture, we examine prominent deep learning models
    specifically designed for time series imputation. The discussed models encompass
    RNN-based ones, CNN-based ones, GNN-based ones, attention-based ones, VAE-based
    ones, GAN-based ones, and diffusion-based ones. In the following two sections,
    we will delve into and discuss the existing deep time series imputation methods
    from these two perspectives.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '为了总结现有的深度多变量时间序列填充方法，我们从填充不确定性和神经网络架构的角度提出了一个分类法，如图 [1](#S2.F1 "Figure 1 ‣
    2.2 Taxonomy of Imputation Methods ‣ 2 Preliminary and Taxonomy ‣ Deep Learning
    for Multivariate Time Series Imputation: A Survey") 所示，并在表 [1](#S2.T1 "Table 1
    ‣ 2.2 Taxonomy of Imputation Methods ‣ 2 Preliminary and Taxonomy ‣ Deep Learning
    for Multivariate Time Series Imputation: A Survey") 中提供了这些方法的更详细总结。对于填充不确定性，我们将填充方法分为预测型和生成型，根据它们在填充过程中产生反映固有不确定性的不同填充值的能力。在神经网络架构方面，我们考察了专门设计用于时间序列填充的显著深度学习模型。讨论的模型包括基于
    RNN 的、基于 CNN 的、基于 GNN 的、基于注意力的、基于 VAE 的、基于 GAN 的和基于扩散的模型。在接下来的两个部分中，我们将从这两个角度深入探讨并讨论现有的深度时间序列填充方法。'
- en: 3 Predictive Methods
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 预测方法
- en: 'This section delves into predictive imputation methods, and our discussion
    primarily focuses on four types: RNN-based, CNN-based, GNN-based, and attention-based
    models.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将深入探讨预测填充方法，我们的讨论主要集中在四种类型上：基于 RNN 的、基于 CNN 的、基于 GNN 的以及基于注意力的模型。
- en: 3.1 Learning Objective
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 学习目标
- en: Predictive imputation methods consistently predict deterministic values for
    the same missing components, thereby not accounting for the uncertainty in the
    imputed values. Typically, these methods employ a reconstruction-based learning
    manner with the learning objective being,
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 预测填充方法始终为相同的缺失组件预测确定性值，因此不考虑填充值中的不确定性。通常，这些方法采用基于重构的学习方式，学习目标为，
- en: '|  | $\mathcal{L}_{det}(\theta)=\sum_{i=1}^{N}\ell_{e}(\mathbf{M}_{i}\odot{\mathbf{\bar{X}}_{i}},\mathbf{M}_{i}\odot\mathbf{X}^{o}_{i}),$
    |  | (2) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{det}(\theta)=\sum_{i=1}^{N}\ell_{e}(\mathbf{M}_{i}\odot{\mathbf{\bar{X}}_{i}},\mathbf{M}_{i}\odot\mathbf{X}^{o}_{i}),$
    |  | (2) |'
- en: where $\ell_{e}$ is an absolute or squared error function.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\ell_{e}$ 是一个绝对值或平方误差函数。
- en: 3.2 RNN-based Models
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于 RNN 的模型
- en: As a natural way to model sequential data, Recurrent Neural Networks (RNNs)
    get developed early on the topic of advanced time-series analysis, and imputation
    is not an exception. GRU-D Che et al. ([2018](#bib.bib10)), a variant of GRU,
    is designed to process time series containing missing values. It is regulated
    by a temporal decay mechanism, which takes the time-lag matrix $\mathbf{\delta}_{i}$
    as input and models the temporal irregularity caused by missing values. Temporal
    belief memory Kim and Chi ([2018](#bib.bib25)), inspired by a biological neural
    model called the Hodgkin–Huxley model, is proposed to handle missing data by computing
    a belief of each feature’s last observation with a bidirectional RNN and imputing
    a missing value based on its according belief. M-RNN Yoon et al. ([2019](#bib.bib56))
    is an RNN variant that works in a multi-directional way. This model interpolates
    within data streams with a bidirectional RNN model and imputes across data streams
    with a fully connected network. BRITS Cao et al. ([2018](#bib.bib9)) models incomplete
    time series with a bidirectional RNN. It takes missing values as variables of
    the RNN graph and fills in missing data with the hidden states from the RNN. In
    addition to imputation, BRITS is capable of working on the time series classification
    task simultaneously. Both M-RNN and BRITS adopt the temporal decay function from
    GRU-D to capture the informative missingness for performance improvement. Subsequent
    works, such as Luo et al. ([2018](#bib.bib35), [2019](#bib.bib36)); Liu et al.
    ([2019](#bib.bib30)); Miao et al. ([2021](#bib.bib39)), combine RNNs with the
    GAN structure to output imputation with higher accuracy.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种自然的序列数据建模方式，递归神经网络（RNNs）在高级时间序列分析领域中早期得到了发展，数据插补也不例外。GRU-D Che et al. ([2018](#bib.bib10))
    是GRU的一种变体，旨在处理包含缺失值的时间序列。它通过一个时间衰减机制进行调节，该机制将时间滞后矩阵 $\mathbf{\delta}_{i}$ 作为输入，并建模由于缺失值引起的时间不规则性。Temporal
    belief memory Kim and Chi ([2018](#bib.bib25)) 受到一种称为霍奇金-赫克斯利模型的生物神经模型的启发，提出通过计算每个特征最后观测的信念并使用双向RNN对缺失值进行插补。M-RNN
    Yoon et al. ([2019](#bib.bib56)) 是一种在多方向上工作的RNN变体。该模型使用双向RNN模型在数据流中进行插值，并通过全连接网络在数据流之间进行插补。BRITS
    Cao et al. ([2018](#bib.bib9)) 使用双向RNN对不完整的时间序列进行建模。它将缺失值作为RNN图的变量，并利用RNN的隐藏状态填补缺失数据。除了插补，BRITS还能同时进行时间序列分类任务。M-RNN和BRITS都采用了GRU-D的时间衰减函数，以捕获信息性缺失以提升性能。后续工作，如Luo
    et al. ([2018](#bib.bib35), [2019](#bib.bib36)); Liu et al. ([2019](#bib.bib30));
    Miao et al. ([2021](#bib.bib39))，将RNN与GAN结构相结合，以更高的精度输出插补结果。
- en: 3.3 CNN-based Models
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于CNN的模型
- en: Convolutional Neural Networks (CNNs) represent a foundational deep learning
    architecture, extensively employed in sophisticated time series analysis. TimesNet Wu
    et al. ([2023a](#bib.bib53)) innovatively incorporates Fast Fourier Transform
    to restructure 1D time series into a 2D format, facilitating the utilization of
    CNNs for data processing. Also in GP-VAE Fortuin et al. ([2020](#bib.bib17)),
    CNNs play the role of the backbone in both the encoder and decoder. Furthermore,
    CNNs serve as pivotal feature extractors within attention-based models like DeepMVI Bansal
    et al. ([2021](#bib.bib6)), as well as in diffusion-based models such as CSDI Tashiro
    et al. ([2021](#bib.bib48)), by mapping input data into an embedding space for
    subsequent processing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）代表了深度学习中的一种基础架构，广泛应用于复杂的时间序列分析中。TimesNet Wu et al. ([2023a](#bib.bib53))
    创新性地结合了快速傅里叶变换，将一维时间序列重构为二维格式，从而便于利用CNN进行数据处理。在GP-VAE Fortuin et al. ([2020](#bib.bib17))中，CNN在编码器和解码器中都发挥了骨干的作用。此外，CNN还在基于注意力的模型如DeepMVI
    Bansal et al. ([2021](#bib.bib6))以及扩散模型如CSDI Tashiro et al. ([2021](#bib.bib48))中作为关键的特征提取器，通过将输入数据映射到嵌入空间以便后续处理。
- en: 3.4 GNN-based Models
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 基于GNN的模型
- en: GNN-based models, treating time series as graph sequences, reconstruct missing
    values using learned node representations. The authors in Cini et al. ([2022](#bib.bib12))
    introduce GRIN, the first graph-based recurrent architecture for MTSI. GRIN utilizes
    a bidirectional graph recurrent neural network to effectively harness both temporal
    dynamics and spatial similarities, thereby achieving significant improvements
    in imputation accuracy. Furthermore, SPIN Marisca et al. ([2022](#bib.bib38))
    is developed, integrating a unique sparse spatiotemporal attention mechanism into
    the GNN framework. This mechanism notably overcomes the error propagation issue
    of GRIN and bolsters robustness against the data sparsity presented by highly
    missing data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于GNN的模型将时间序列视为图序列，利用学习到的节点表示来重建缺失值。Cini et al. ([2022](#bib.bib12))中的作者介绍了GRIN，这是第一个用于MTSI的图基递归架构。GRIN利用双向图递归神经网络有效地利用了时间动态和空间相似性，从而显著提高了填补精度。此外，SPIN
    Marisca et al. ([2022](#bib.bib38))被开发出来，将独特的稀疏时空注意力机制整合到GNN框架中。该机制显著克服了GRIN的误差传播问题，并增强了对高度缺失数据所呈现的数据稀疏性的鲁棒性。
- en: 3.5 Attention-based Models
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 基于注意力的模型
- en: 'Since Transformer is proposed in Vaswani et al. ([2017](#bib.bib50)), the self-attention
    mechanism has been widely used to model sequence data including time series Wen
    et al. ([2023](#bib.bib52)). CDSA Ma et al. ([2019](#bib.bib37)) is proposed to
    impute geo-tagged spatiotemporal data by learning from time, location, and measurement
    jointly. DeepMVI Bansal et al. ([2021](#bib.bib6)) integrates transformers with
    convolutional techniques, tailoring key-query designs to effectively address missing
    value imputation. For each time series, DeepMVI harnesses attention mechanisms
    to concurrently distill long-term seasonal, granular local, and cross-dimensional
    embeddings, which are concatenated to predict the final output. NRTSI Shan et
    al. ([2023](#bib.bib46)) directly leverages a Transformer encoder for modeling
    and takes time series data as a set of timestamp and measurement tuples. As a
    permutation model, this model has to iterate over the time dimension to process
    time series. SAITS Du et al. ([2023](#bib.bib13)) employs a self-supervised training
    scheme to deal with missing data, which integrates dual joint learning tasks:
    a masked imputation task and an observed reconstruction task. This method, featuring
    two diagonal-masked self-attention blocks and a weighted-combination block, leverages
    attention weights and missingness indicators to enhance imputation precision.
    In addition to the above models, the attention mechanism is also widely adapted
    to build the denoising network in diffusion models like CSDI Tashiro et al. ([2021](#bib.bib48)),
    MIDM Wang et al. ([2023](#bib.bib51)), PriSTI Liu et al. ([2023](#bib.bib32)),
    etc.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 自从Transformer在Vaswani et al. ([2017](#bib.bib50))中提出以来，自注意力机制已被广泛应用于建模序列数据，包括时间序列 Wen
    et al. ([2023](#bib.bib52))。CDSA Ma et al. ([2019](#bib.bib37))旨在通过共同学习时间、位置和测量来填补地理标记的时空数据。DeepMVI Bansal
    et al. ([2021](#bib.bib6))将transformer与卷积技术相结合，调整关键-查询设计以有效解决缺失值填补问题。对于每个时间序列，DeepMVI利用注意力机制同时提取长期季节性、细粒度本地和跨维度的嵌入，这些嵌入被连接起来以预测最终输出。NRTSI Shan
    et al. ([2023](#bib.bib46))直接利用Transformer编码器进行建模，并将时间序列数据视为时间戳和测量值的集合。作为一个排列模型，该模型必须在时间维度上进行迭代以处理时间序列。SAITS Du
    et al. ([2023](#bib.bib13))采用自监督训练方案来处理缺失数据，结合了双重联合学习任务：掩蔽填补任务和观察重建任务。该方法具有两个对角掩蔽自注意力块和一个加权组合块，利用注意力权重和缺失指示符来提高填补精度。除了上述模型，注意力机制还被广泛应用于构建扩散模型中的去噪网络，如CSDI Tashiro
    et al. ([2021](#bib.bib48))、MIDM Wang et al. ([2023](#bib.bib51))、PriSTI Liu et
    al. ([2023](#bib.bib32))等。
- en: 3.6 Pros and Cons
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 优缺点
- en: This subsection synthesizes the strengths and challenges of the predictive imputation
    methods discussed. RNN-based models, while adept at capturing sequential information,
    are inherently limited by their sequential processing nature and memory constraints,
    which may lead to scalability issues with long sequences Khayati et al. ([2020](#bib.bib24)).
    Although CNNs have decades of development and are useful feature extractors to
    capture neighborhood information and local connectivity, their kernel size and
    working mechanism intrinsically limit their performance on time series data as
    the backbone. Due to the attention mechanism, attention-based models generally
    outperform RNN-based and CNN-based methods in imputation tasks due to their superior
    ability to handle long-range dependencies and parallel processing capabilities.
    GNN-based methods provide a deeper understanding of spatio-temporal dynamics,
    yet they often come with increased computational complexity, posing challenges
    for large-scale or high-dimensional data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节综合了讨论的预测插补方法的优点和挑战。虽然 RNN 基于模型能够捕捉序列信息，但其固有的顺序处理特性和记忆限制使其在处理长序列时可能出现扩展性问题 Khayati
    et al. ([2020](#bib.bib24))。尽管 CNNs 已有几十年的发展，并且是有效的特征提取器，可以捕捉邻域信息和局部连接，但其卷积核大小和工作机制固有地限制了其在时间序列数据中的性能。由于注意力机制，基于注意力的模型在插补任务中通常优于基于
    RNN 和 CNN 的方法，因为它们在处理长距离依赖和并行处理能力方面表现更佳。基于 GNN 的方法提供了对时空动态的更深理解，但通常伴随着更高的计算复杂度，这对大规模或高维数据提出了挑战。
- en: 4 Generative Methods
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 生成方法
- en: 'In this section, we examine generative imputation methods, including three
    primary types: VAE-based, GAN-based, and diffusion-based models.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨生成插补方法，包括三种主要类型：基于 VAE 的、基于 GAN 的和基于扩散的模型。
- en: 4.1 Learning Objective
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 学习目标
- en: Generative methods are essentially built upon generative models like VAEs, GANs,
    and diffusion models. They are characterized by their ability to generate varied
    outputs for missing observations, enabling the quantification of imputation uncertainty.
    Typically, these methods learn probability distributions from the observed data
    and subsequently generate slightly different values aligned with these learned
    distributions for the missing observation. The primary learning objective of generative
    methods is thus defined as,
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生成方法本质上建立在生成模型如 VAEs、GANs 和扩散模型之上。它们的特点是能够为缺失观测生成多样化的输出，从而实现插补不确定性的量化。通常，这些方法从观察到的数据中学习概率分布，并随后为缺失观测生成与这些学习到的分布一致的稍微不同的值。因此，生成方法的主要学习目标定义为：
- en: '|  | $\mathcal{L}_{pro}(\theta)=\sum_{i=1}^{N}\log p_{\theta}(\mathbf{{X}}^{o}_{i}).$
    |  | (3) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{pro}(\theta)=\sum_{i=1}^{N}\log p_{\theta}(\mathbf{{X}}^{o}_{i}).$
    |  | (3) |'
- en: where $\theta$ is the model parameters of the imputation model $\mathcal{M}$.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta$ 是插补模型 $\mathcal{M}$ 的模型参数。
- en: 4.2 VAE-based Models
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于 VAE 的模型
- en: VAEs employ an encoder-decoder structure to approximate the true data distribution
    by maximizing the Evidence Lower Bound (ELBO) on the marginal likelihood. This
    ELBO enforces a Gaussian-distributed latent space from which the decoder reconstructs
    diverse data points
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 使用编码器-解码器结构，通过最大化边际似然下界（ELBO）来近似真实数据分布。这个 ELBO 强制从中解码器重建多样的数据点的潜在空间服从高斯分布。
- en: The authors in Fortuin et al. ([2020](#bib.bib17)) propose the first VAE-based
    imputation method GP-VAE, where they utilized a Gaussian process prior in the
    latent space to capture temporal dynamics. Moreover, the ELBO in GP-VAE is only
    evaluated on the observed features of the data. Authors in Mulyadi et al. ([2021](#bib.bib42))
    design V-RIN to mitigate the risk of biased estimates in missing value imputation.
    V-RIN captures uncertainty by accommodating a Gaussian distribution over the model
    output, specifically interpreting the variance of the reconstructed data from
    a VAE model as an uncertainty measure. It then models temporal dynamics and seamlessly
    integrates this uncertainty into the imputed data through an uncertainty-aware
    GRU. More recently, authors in  Kim et al. ([2023](#bib.bib26)) propose supnotMIWAE
    and introduce an extra classifier, where they extend the ELBO in GP-VAE to model
    the joint distribution of the observed data, its mask matrix, and its label. In
    this way, their ELBO effectively models the imputation uncertainty, and the additional
    classifier encourages the VAE model to produce missing values that are more advantageous
    for the downstream classification task.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Fortuin 等人（[2020](#bib.bib17)）提出了首个基于 VAE 的插补方法 GP-VAE，其中他们在潜在空间中利用了高斯过程先验以捕捉时间动态。此外，GP-VAE
    中的 ELBO 仅在数据的观察特征上进行评估。Mulyadi 等人（[2021](#bib.bib42)）设计了 V-RIN 来减轻缺失值插补中的偏差估计风险。V-RIN
    通过对模型输出进行高斯分布建模来捕捉不确定性，具体来说，将 VAE 模型重建数据的方差解释为不确定性度量。然后，它建模时间动态，并通过不确定性感知 GRU
    无缝地将这种不确定性集成到插补数据中。最近，Kim 等人（[2023](#bib.bib26)）提出了 supnotMIWAE 并引入了额外的分类器，他们将
    GP-VAE 中的 ELBO 扩展到建模观察数据、其掩码矩阵和其标签的联合分布。通过这种方式，他们的 ELBO 有效地建模了插补不确定性，额外的分类器鼓励
    VAE 模型生成对下游分类任务更有利的缺失值。
- en: 4.3 GAN-based Models
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基于 GAN 的模型
- en: 'GANs facilitate adversarial training through a minimax game between two components:
    a generator aiming to mimic the real data distribution, and a discriminator tasked
    with distinguishing between the generated and real data. This dynamic fosters
    a progressive refinement of synthetic data that increasingly resembles real samples.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 通过生成器和判别器之间的极小极大博弈促进对抗训练：生成器旨在模拟真实数据分布，判别器负责区分生成数据和真实数据。这种动态促进了合成数据的逐步完善，使其越来越类似于真实样本。
- en: In Luo et al. ([2018](#bib.bib35)), authors propose a two-stage GAN imputation
    method (GRUI-GAN), which is the first GAN-based method for imputing time series
    data. GRUI-GAN first learns the distribution of the observed multivariate time
    series data by a standard adversarial training manner, and then optimizes the
    input noise of the generator to further maximize the similarity of the generated
    and observed multivariate time series data. However, the second stage in GRUI-GAN
    needs a lot of time to find the best matched input vector, and this vector is
    not always the best especially when the initial value of the “noise” is not properly
    set. Then, an end-to-end GAN imputation model $E^{2}$GAN Luo et al. ([2019](#bib.bib36))
    is further proposed, where the generator takes a denoising autoencoder module
    to avoid the “noise” optimization stage in GRUI-GAN. Meanwhile, authors in  Liu
    et al. ([2019](#bib.bib30)) propose a non-autoregressive multi-resolution GAN
    model (NAOMI), where the generator is assembled by a forward-backward encoder
    and a multiresolution decoder. The imputed data are recursively generated by the
    multiresolution decoder in a non-autoregressive manner, which mitigates error
    accumulation in scenarios involving high-missing and long sequence time series
    data. On the other hand, in Miao et al. ([2021](#bib.bib39)), authors propose
    USGAN, which generates high-quality imputed data by integrating a discriminator
    with a temporal reminder matrix. This matrix introduces added complexity to the
    training of the discriminator and subsequently leads to improvements in the generator’s
    performance. Furthermore, they extend USGAN to a semi-supervised model SSGAN,
    by introducing an extra classifier. In this way, SSGAN takes advantage of the
    label information, so that the generator can estimate the missing values, conditioned
    on observed components and data labels at the same time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Luo 等人 ([2018](#bib.bib35)) 的研究中，作者提出了一种两阶段的 GAN 填补方法（GRUI-GAN），这是第一种基于 GAN
    的时间序列数据填补方法。GRUI-GAN 首先通过标准对抗训练方式学习观察到的多变量时间序列数据的分布，然后优化生成器的输入噪声，以进一步最大化生成的和观察到的多变量时间序列数据之间的相似性。然而，GRUI-GAN
    的第二阶段需要大量时间来找到最佳匹配的输入向量，并且当“噪声”的初始值设置不当时，这个向量不一定总是最佳的。随后，提出了一种端到端的 GAN 填补模型 $E^{2}$GAN
    Luo 等人 ([2019](#bib.bib36))，其中生成器采用去噪自编码器模块，以避免 GRUI-GAN 中的“噪声”优化阶段。同时，Liu 等人
    ([2019](#bib.bib30)) 提出了一个非自回归多分辨率 GAN 模型（NAOMI），其中生成器由前向-反向编码器和多分辨率解码器组成。填补的数据由多分辨率解码器以非自回归方式递归生成，这在涉及高缺失和长序列时间序列数据的场景中减轻了误差累积。另一方面，在
    Miao 等人 ([2021](#bib.bib39)) 的研究中，作者提出了 USGAN，通过将判别器与时间提醒矩阵相结合来生成高质量的填补数据。该矩阵为判别器的训练引入了额外的复杂性，进而提高了生成器的性能。此外，他们将
    USGAN 扩展为半监督模型 SSGAN，通过引入额外的分类器。这样，SSGAN 利用标签信息，使生成器能够在观察到的组件和数据标签的条件下估计缺失值。
- en: 4.4 Diffusion-based Models
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 基于扩散的模型
- en: As an emerging and potent category of generative models, diffusion models are
    adept at capturing complex data distributions by progressively adding and then
    reversing noise through a Markov chain of diffusion steps. Distinct from VAE,
    these models utilize a fixed training procedure and operate with high-dimensional
    latent variables that retain the dimensionality of the input data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种新兴且强大的生成模型类别，扩散模型擅长通过逐步添加和反转噪声来捕捉复杂的数据分布，这一过程通过扩散步骤的马尔科夫链进行。与VAE不同，这些模型采用固定的训练程序，并且使用保留输入数据维度的高维潜变量。
- en: CSDI, introduced in Tashiro et al. ([2021](#bib.bib48)), stands out as the pioneering
    diffusion model specifically designed for MTSI. Different from conventional diffusion
    models, CSDI adopts a conditioned training approach, where a subset of observed
    data is utilized as conditional information to facilitate the generation of the
    remaining segment of observed data. However, the denoising network in CSDI relies
    on two transformers, exhibiting quadratic complexity concerning the number of
    variables and the time series length. This design limitation raises concerns about
    memory constraints, particularly when modeling extensive multivariate time series.
    In response to this challenge, a subsequent work by Alcaraz and Strodthoff ([2023](#bib.bib1))
    introduces SSSD, which addresses the quadratic complexity issue by replacing transformers
    with structured state space models Gu et al. ([2022](#bib.bib19)). This modification
    proves advantageous, especially when handling lengthy multivariate time series,
    as it mitigates the risk of memory overflow. Another approach CSBI, introduced
    in  Chen et al. ([2023](#bib.bib11)), improves the efficiency by modeling the
    diffusion process as a Schrodinger bridge problem, which could be transformed
    into computation-friendly stochastic differential equations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: CSDI，由Tashiro等人（[2021](#bib.bib48)）介绍，作为专门为MTSI设计的开创性扩散模型脱颖而出。与传统扩散模型不同，CSDI采用了条件训练方法，其中使用观察数据的子集作为条件信息，以便生成其余观察数据。然而，CSDI中的去噪网络依赖于两个transformers，在变量数量和时间序列长度方面呈现出二次复杂度。这一设计限制引发了对内存限制的担忧，特别是在建模广泛的多变量时间序列时。针对这一挑战，Alcaraz和Strodthoff（[2023](#bib.bib1)）的后续工作引入了SSSD，通过用结构化状态空间模型Gu等人（[2022](#bib.bib19)）替代transformers，从而解决了二次复杂度问题。这一修改尤其在处理较长的多变量时间序列时具有优势，因为它减轻了内存溢出的风险。另一种方法，CSBI，由Chen等人（[2023](#bib.bib11)）介绍，通过将扩散过程建模为Schrodinger桥问题，从而提高了效率，这可以转化为计算友好的随机微分方程。
- en: Moreover, the efficacy of diffusion models is notably influenced by the construction
    and utilization of conditional information. MIDM Wang et al. ([2023](#bib.bib51))
    proposes to sample noise from a distribution conditional on observed data’s representations
    in the denoising process, In this way, it can explicitly preserve the intrinsic
    correlations between observed and missing data. PriSTI Liu et al. ([2023](#bib.bib32))
    introduces the spatiotemporal dependencies as conditional information, i.e., provides
    the denoising network with spatiotemporal attention weights calculated by the
    conditional feature for spatiotemporal imputation. Additionally, DA-TASWDM Xu
    et al. ([2023](#bib.bib55)) suggests incorporating dynamic temporal relationships,
    i.e. the varying sampling densities, into the denoising network for medical time
    series imputation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，扩散模型的有效性显著受到条件信息构建和利用的影响。MIDM Wang等人（[2023](#bib.bib51)）建议从条件于观察数据表示的分布中采样噪声，在去噪过程中显式地保留观察数据和缺失数据之间的内在相关性。PriSTI
    Liu等人（[2023](#bib.bib32)）引入了时空依赖作为条件信息，即通过条件特征为去噪网络提供时空注意权重以进行时空插补。此外，DA-TASWDM
    Xu等人（[2023](#bib.bib55)）建议将动态时间关系，即变化的采样密度，纳入去噪网络以进行医疗时间序列插补。
- en: Contrasting with the above diffusion-based methods that treat time series as
    discrete time steps, SPD Biloš et al. ([2023](#bib.bib8)) views time series as
    discrete realizations of an underlying continuous function and generates data
    for imputation using stochastic process diffusion. In this way, SPD posits the
    continuous noise process as an inductive bias for the irregular time series, so
    as to better capture the true generative process, especially with the inherent
    stochasticity of the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述将时间序列视为离散时间步的方法相比，SPD Biloš等人（[2023](#bib.bib8)）将时间序列视为潜在连续函数的离散实现，并通过随机过程扩散生成插补数据。通过这种方式，SPD
    将连续噪声过程视为不规则时间序列的归纳偏差，以更好地捕捉真实的生成过程，特别是考虑到数据的固有随机性。
- en: 4.5 Pros and Cons
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 优缺点
- en: This subsection delineates the advantages and limitations of the aforementioned
    generative imputation models. VAE-based models are adept at modeling probabilities
    explicitly and offering a theoretical foundation for understanding data distributions.
    However, they are often constrained by their generative capacity, which can limit
    their performance in capturing complex data variability. GAN-based models, on
    the other hand, excel in data generation, providing high-quality imputations with
    impressive fidelity to the original data distributions. Yet, they are notoriously
    challenging to train due to issues like vanishing gradients Wu et al. ([2023b](#bib.bib54)),
    which can hamper model stability and convergence. Diffusion-based models emerge
    as powerful generative tools with a strong capacity for capturing intricate data
    patterns. Nevertheless, their computational complexity is considerable, and they
    also suffer from issues related to boundary coherence between missing and observed
    parts Lugmayr et al. ([2022](#bib.bib34)).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节阐述了上述生成填充模型的优缺点。基于 VAE 的模型擅长明确建模概率，并为理解数据分布提供了理论基础。然而，它们通常受到生成能力的限制，这可能限制了它们在捕捉复杂数据变异性方面的表现。另一方面，基于
    GAN 的模型在数据生成方面表现出色，提供了与原始数据分布高度一致的高质量填充。然而，由于诸如梯度消失 Wu et al. ([2023b](#bib.bib54))
    等问题，它们训练起来 notoriously 具有挑战性，这可能影响模型的稳定性和收敛性。基于扩散的模型作为强大的生成工具，具有捕捉复杂数据模式的强大能力。然而，它们的计算复杂度相当高，并且也面临缺失部分与观测部分之间的边界一致性问题
    Lugmayr et al. ([2022](#bib.bib34))。
- en: 5 Time Series Imputation Toolkits
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个时间序列填充工具包
- en: On the time series imputation task, there are existing libraries providing naive
    processing ways, statistical methods, machine learning imputation algorithms,
    and deep learning imputation neural networks for convenient usage.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列填充任务中，现有的库提供了简单处理方法、统计方法、机器学习填充算法以及深度学习填充神经网络以便于使用。
- en: imputeTS [Moritz and Bartz-Beielstein](#bib.bib41) , a library in R provides
    several naive approaches (e.g., mean values, last observation carried forward,
    etc.) and commonly-used imputation algorithms (e.g., linear interpolation, Kalman
    smoothing, and weighted moving average) but only for univariate time series. Another
    well-known R package, mice Van Buuren and Groothuis-Oudshoorn ([2011](#bib.bib49)),
    implements the method called multivariate imputation by chained equations to tackle
    missingness in data. Although it is not for time series specifically, it is widely
    used in practice for multivariate time-series imputation, especially in the field
    of statistics. Impyute²²2[https://github.com/eltonlaw/impyute](https://github.com/eltonlaw/impyute)
    and Autoimpute³³3[https://github.com/kearnz/autoimpute](https://github.com/kearnz/autoimpute)
    both offer naive imputation methods for cross-sectional data and time-series data.
    Impyute is only with simple approaches like the moving average window, and Autoimpute
    integrates parametric methods, for example, polynomial interpolation and spline
    interpolation. More recently, GluonTS Alexandrov et al. ([2020](#bib.bib2)), a
    generative machine-learning package for time series, provides some naive ways,
    such as dummy value imputation and casual mean value imputation, to handle missing
    values. In addition to simple and non-parametric methods, Sktime Löning et al.
    ([2019](#bib.bib33)) implements one more option that allows users to leverage
    integrated machine learning imputation algorithms to fit and predict missing values
    in the given data, though this works in a univariate way. When it comes to deep
    learning imputation, PyPOTS Du ([2023](#bib.bib14)) is a toolbox focusing on modeling
    partially-observed time series end-to-end. It contains more than a dozen deep-learning
    neural networks for tasks on incomplete time series, including eight imputation
    models so far.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: imputeTS [Moritz 和 Bartz-Beielstein](#bib.bib41)，这是一个 R 语言库，提供了几种简单的方法（如均值、最后观测值前推等）和常用的插补算法（如线性插值、卡尔曼平滑和加权移动平均），但仅适用于单变量时间序列。另一个著名的
    R 包，mice Van Buuren 和 Groothuis-Oudshoorn ([2011](#bib.bib49))，实现了一种称为链式方程的多变量插补方法，以处理数据中的缺失值。虽然它并不是专门针对时间序列的，但在实际中广泛用于多变量时间序列插补，尤其在统计领域。Impyute²²2[https://github.com/eltonlaw/impyute](https://github.com/eltonlaw/impyute)
    和 Autoimpute³³3[https://github.com/kearnz/autoimpute](https://github.com/kearnz/autoimpute)
    都为横截面数据和时间序列数据提供了简单的插补方法。Impyute 仅提供如移动平均窗口这样的简单方法，而 Autoimpute 集成了参数方法，例如多项式插值和样条插值。最近，GluonTS Alexandrov
    等人 ([2020](#bib.bib2))，一个用于时间序列的生成性机器学习包，提供了一些简单的方法，如虚拟值插补和随意均值插补，以处理缺失值。除了简单和非参数方法外，Sktime Löning
    等人 ([2019](#bib.bib33)) 实现了另一种选项，允许用户利用集成的机器学习插补算法来拟合和预测给定数据中的缺失值，尽管这仅在单变量方式下有效。至于深度学习插补，PyPOTS Du
    ([2023](#bib.bib14)) 是一个专注于端到端建模部分观测时间序列的工具包。它包含十多个用于处理不完整时间序列的深度学习神经网络，包括目前的八种插补模型。
- en: 6 Experimental Evaluation and Discussion
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验评估与讨论
- en: In this section, empirical experiments are conducted to evaluate and analyze
    deep multivariate time series imputation methods from different categories. The
    results are obtained with a machine with AMD EPYC 7543 32-Core CPU and an NVIDIA
    GeForce RTX 4090 GPU. All code, including the data preprocessing scripts, model
    configurations, and training scripts, are publicly available in the GitHub repository
    [https://github.com/WenjieDu/Awesome_Imputation](https://github.com/WenjieDu/Awesome_Imputation).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，进行实证实验以评估和分析来自不同类别的深度多变量时间序列插补方法。结果是在配备 AMD EPYC 7543 32 核 CPU 和 NVIDIA
    GeForce RTX 4090 GPU 的机器上获得的。所有代码，包括数据预处理脚本、模型配置和训练脚本，均公开在 GitHub 仓库 [https://github.com/WenjieDu/Awesome_Imputation](https://github.com/WenjieDu/Awesome_Imputation)
    上。
- en: 6.1 Datasets and Imputation Methods
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 数据集与插补方法
- en: 'Specifically, three naive imputation approaches and eight deep-learning neural
    networks are tested on three real-world datasets (Air Zhang et al. ([2017](#bib.bib57)),
    PhysioNet2012 Silva et al. ([2012](#bib.bib47)), and ETTm1 Zhou et al. ([2021](#bib.bib58))
    in Table [2](#S6.T2 "Table 2 ‣ 6.1 Datasets and Imputation Methods ‣ 6 Experimental
    Evaluation and Discussion ‣ Deep Learning for Multivariate Time Series Imputation:
    A Survey")) which are commonly used in the literature.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，对三种简单插补方法和八种深度学习神经网络进行了测试，使用了三个真实世界的数据集（Air Zhang 等人 ([2017](#bib.bib57))、PhysioNet2012 Silva
    等人 ([2012](#bib.bib47)) 和 ETTm1 Zhou 等人 ([2021](#bib.bib58))，见表 [2](#S6.T2 "表
    2 ‣ 6.1 数据集与插补方法 ‣ 6 实验评估与讨论 ‣ 多变量时间序列插补的深度学习：综述")），这些数据集在文献中常被使用。
- en: 'Regarding the imputation methods, apart from three naive ways Mean, Median,
    and LOCF (last observation carried forward) as baselines, the eight following
    representative deep-learning models are selected from different categories for
    experimental studies: M-RNN Yoon et al. ([2019](#bib.bib56)), GP-VAE Fortuin et
    al. ([2020](#bib.bib17)), BRITS Cao et al. ([2018](#bib.bib9)), USGAN Miao et
    al. ([2021](#bib.bib39)), CSDI Tashiro et al. ([2021](#bib.bib48)), TimesNet Wu
    et al. ([2023a](#bib.bib53)), Transformer Du et al. ([2023](#bib.bib13)), and
    SAITS Du et al. ([2023](#bib.bib13)). Experiments are performed with PyPOTS⁴⁴4[https://pypots.com](https://pypots.com) Du
    ([2023](#bib.bib14)) and all the above imputation methods are instantly available
    in the toolbox. Moreover, for a fair comparison, hyperparameters of all deep learning
    methods are optimized by the tuning functionality in PyPOTS.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 关于填补方法，除了三个简单的基线方法：均值（Mean）、中位数（Median）和前向填补（LOCF，最后观测值前向填补），选择了以下八种代表性深度学习模型进行实验研究：M-RNN
    Yoon et al. ([2019](#bib.bib56))，GP-VAE Fortuin et al. ([2020](#bib.bib17))，BRITS
    Cao et al. ([2018](#bib.bib9))，USGAN Miao et al. ([2021](#bib.bib39))，CSDI Tashiro
    et al. ([2021](#bib.bib48))，TimesNet Wu et al. ([2023a](#bib.bib53))，Transformer
    Du et al. ([2023](#bib.bib13))，以及 SAITS Du et al. ([2023](#bib.bib13))。实验使用了 PyPOTS⁴⁴4[https://pypots.com](https://pypots.com)
    Du ([2023](#bib.bib14))，所有上述填补方法在工具箱中均可立即使用。此外，为了公平比较，所有深度学习方法的超参数都通过 PyPOTS 中的调优功能进行了优化。
- en: '|  | Air | PhysioNet2012 | ETTm1 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | Air | PhysioNet2012 | ETTm1 |'
- en: '| Number of samples | 1,458 | 11,988 | 722 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 样本数量 | 1,458 | 11,988 | 722 |'
- en: '| Sequence length | 24 | 48 | 96 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 序列长度 | 24 | 48 | 96 |'
- en: '| Number of features | 132 | 37 | 7 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 特征数量 | 132 | 37 | 7 |'
- en: '| Original missing rate | 1.6% | 80.0% | 0% |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 原始缺失率 | 1.6% | 80.0% | 0% |'
- en: 'Table 2: The general information of the three preprocessed datasets. Note the
    detailed descriptions are available [in the code repository](https://github.com/WenjieDu/Awesome_Imputation/tree/main/time_series_imputation_survey_code).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：三个预处理数据集的一般信息。详细描述见[代码库](https://github.com/WenjieDu/Awesome_Imputation/tree/main/time_series_imputation_survey_code)。
- en: '| Method | Air | PhysioNet2012 | ETTm1 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Air | PhysioNet2012 | ETTm1 |'
- en: '| MAE $\downarrow$ | MSE $\downarrow$ | MAE $\downarrow$ | MSE $\downarrow$
    | MAE $\downarrow$ | MSE $\downarrow$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| MAE $\downarrow$ | MSE $\downarrow$ | MAE $\downarrow$ | MSE $\downarrow$
    | MAE $\downarrow$ | MSE $\downarrow$ |'
- en: '| Mean | 0.692$\pm$0.000 | 0.970$\pm$0.000 | 0.702$\pm$0.000 | 0.954$\pm$0.000
    | 0.663$\pm$0.000 | 0.809$\pm$0.000 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 均值 | 0.692$\pm$0.000 | 0.970$\pm$0.000 | 0.702$\pm$0.000 | 0.954$\pm$0.000
    | 0.663$\pm$0.000 | 0.809$\pm$0.000 |'
- en: '| Median | 0.660$\pm$0.000 | 1.027$\pm$0.000 | 0.685$\pm$0.000 | 0.991$\pm$0.000
    | 0.657$\pm$0.000 | 0.825$\pm$0.000 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 中位数 | 0.660$\pm$0.000 | 1.027$\pm$0.000 | 0.685$\pm$0.000 | 0.991$\pm$0.000
    | 0.657$\pm$0.000 | 0.825$\pm$0.000 |'
- en: '| LOCF | 0.206$\pm$0.000 | 0.279$\pm$0.000 | 0.411$\pm$0.000 | 0.569$\pm$0.000
    | 0.135$\pm$0.000 | 0.072$\pm$0.000 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| LOCF | 0.206$\pm$0.000 | 0.279$\pm$0.000 | 0.411$\pm$0.000 | 0.569$\pm$0.000
    | 0.135$\pm$0.000 | 0.072$\pm$0.000 |'
- en: '| M-RNN | 0.524$\pm$0.001 | 0.648$\pm$0.003 | 0.674$\pm$0.001 | 0.864$\pm$0.002
    | 0.651$\pm$0.060 | 1.074$\pm$0.120 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| M-RNN | 0.524$\pm$0.001 | 0.648$\pm$0.003 | 0.674$\pm$0.001 | 0.864$\pm$0.002
    | 0.651$\pm$0.060 | 1.074$\pm$0.120 |'
- en: '| GP-VAE | 0.280$\pm$0.003 | 0.266$\pm$0.009 | 0.400$\pm$0.007 | 0.433$\pm$0.011
    | 0.290$\pm$0.017 | 0.178$\pm$0.015 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GP-VAE | 0.280$\pm$0.003 | 0.266$\pm$0.009 | 0.400$\pm$0.007 | 0.433$\pm$0.011
    | 0.290$\pm$0.017 | 0.178$\pm$0.015 |'
- en: '| BRITS | 0.142$\pm$0.001 | 0.129$\pm$0.001 | 0.246$\pm$0.001 | 0.325$\pm$0.002
    | 0.124$\pm$0.002 | 0.046$\pm$0.002 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| BRITS | 0.142$\pm$0.001 | 0.129$\pm$0.001 | 0.246$\pm$0.001 | 0.325$\pm$0.002
    | 0.124$\pm$0.002 | 0.046$\pm$0.002 |'
- en: '| USGAN | 0.141$\pm$0.001 | 0.132$\pm$0.001 | 0.250$\pm$0.001 | 0.306$\pm$0.001
    | 0.127$\pm$0.005 | 0.048$\pm$0.003 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| USGAN | 0.141$\pm$0.001 | 0.132$\pm$0.001 | 0.250$\pm$0.001 | 0.306$\pm$0.001
    | 0.127$\pm$0.005 | 0.048$\pm$0.003 |'
- en: '| CSDI | 0.105$\pm$0.003 | 0.153$\pm$0.021 | 0.211$\pm$0.003 | 0.260$\pm$0.050
    | 0.157$\pm$0.052 | 0.292$\pm$0.456 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| CSDI | 0.105$\pm$0.003 | 0.153$\pm$0.021 | 0.211$\pm$0.003 | 0.260$\pm$0.050
    | 0.157$\pm$0.052 | 0.292$\pm$0.456 |'
- en: '| TimesNet | 0.159$\pm$0.002 | 0.172$\pm$0.003 | 0.266$\pm$0.007 | 0.272$\pm$0.006
    | 0.113$\pm$0.006 | 0.027$\pm$0.002 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| TimesNet | 0.159$\pm$0.002 | 0.172$\pm$0.003 | 0.266$\pm$0.007 | 0.272$\pm$0.006
    | 0.113$\pm$0.006 | 0.027$\pm$0.002 |'
- en: '| Transformer | 0.163$\pm$0.003 | 0.160$\pm$0.004 | 0.209$\pm$0.002 | 0.225$\pm$0.002
    | 0.133$\pm$0.009 | 0.035$\pm$0.004 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | 0.163$\pm$0.003 | 0.160$\pm$0.004 | 0.209$\pm$0.002 | 0.225$\pm$0.002
    | 0.133$\pm$0.009 | 0.035$\pm$0.004 |'
- en: '| SAITS | 0.133$\pm$0.002 | 0.128$\pm$0.001 | 0.202$\pm$0.002 | 0.218$\pm$0.002
    | 0.115$\pm$0.011 | 0.030$\pm$0.006 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| SAITS | 0.133$\pm$0.002 | 0.128$\pm$0.001 | 0.202$\pm$0.002 | 0.218$\pm$0.002
    | 0.115$\pm$0.011 | 0.030$\pm$0.006 |'
- en: 'Table 3: The MAE and MSE comparisons between imputation methods on the datasets
    Air,'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同填补方法在数据集 Air 上的 MAE 和 MSE 比较。
- en: PhysioNet2012, and ETTm1\. The reported values are means $\pm$ standard deviations
    of five runs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: PhysioNet2012 和 ETTm1。报告的值为五次运行的均值 $\pm$ 标准差。
- en: '| Method | PR-AUC $\uparrow$ | ROC-AUC $\uparrow$ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PR-AUC $\uparrow$ | ROC-AUC $\uparrow$ |'
- en: '| Mean | 0.434$\pm$0.016 | 0.813$\pm$0.009 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 0.434$\pm$0.016 | 0.813$\pm$0.009 |'
- en: '| Median | 0.434$\pm$0.018 | 0.808$\pm$0.014 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 中位数 | 0.434$\pm$0.018 | 0.808$\pm$0.014 |'
- en: '| LOCF | 0.425$\pm$0.015 | 0.804$\pm$0.007 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| LOCF | 0.425$\pm$0.015 | 0.804$\pm$0.007 |'
- en: '| M-RNN | 0.424$\pm$0.022 | 0.807$\pm$0.015 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| M-RNN | 0.424$\pm$0.022 | 0.807$\pm$0.015 |'
- en: '| GP-VAE | 0.384$\pm$0.018 | 0.788$\pm$0.008 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| GP-VAE | 0.384$\pm$0.018 | 0.788$\pm$0.008 |'
- en: '| BRITS | 0.428$\pm$0.017 | 0.821$\pm$0.008 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| BRITS | 0.428$\pm$0.017 | 0.821$\pm$0.008 |'
- en: '| USGAN | 0.431$\pm$0.017 | 0.814$\pm$0.010 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| USGAN | 0.431$\pm$0.017 | 0.814$\pm$0.010 |'
- en: '| CSDI | 0.433$\pm$0.017 | 0.811$\pm$0.005 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| CSDI | 0.433$\pm$0.017 | 0.811$\pm$0.005 |'
- en: '| TimesNet | 0.406$\pm$0.012 | 0.787$\pm$0.013 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| TimesNet | 0.406$\pm$0.012 | 0.787$\pm$0.013 |'
- en: '| Transformer | 0.446$\pm$0.016 | 0.807$\pm$0.018 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | 0.446$\pm$0.016 | 0.807$\pm$0.018 |'
- en: '| SAITS | 0.455$\pm$0.016 | 0.822$\pm$0.002 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| SAITS | 0.455$\pm$0.016 | 0.822$\pm$0.002 |'
- en: 'Table 4: The means and standard deviations of classification results in five
    runs.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：五次运行中的分类结果的均值和标准差。
- en: 6.2 Results and Analysis
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 结果与分析
- en: Imputation Accuracy Evaluation
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 插补准确性评估
- en: 'Imputation results in error metrics MAE (mean absolute error) and MSE (mean
    squared error) of twelve methods across three datasets are displayed in Table [4](#S6.T4
    "Table 4 ‣ 6.1 Datasets and Imputation Methods ‣ 6 Experimental Evaluation and
    Discussion ‣ Deep Learning for Multivariate Time Series Imputation: A Survey").
    The numbers tell that the performance of the methods varies on different datasets
    and there is no clear winner in this study. Further work needs to be done to deeply
    compare predictive and generative imputation methods. Notably, in cases like the
    Air and ETTm1 datasets, where data is continuously recorded by sensors and the
    proportion of missingness is relatively low, the non-parametric LOCF method shows
    commendable performance. Conversely, in the PhysioNet2012 dataset, which has a
    high missing rate, deep learning imputation methods markedly outperform statistical
    approaches. This observation corroborates the capability of deep learning methods
    to effectively capture complex temporal dynamics and accurately learn data distributions,
    especially in scenarios with highly sparse, discrete observations.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '插补结果中十二种方法在三个数据集上的误差指标MAE（平均绝对误差）和MSE（均方误差）如表[4](#S6.T4 "Table 4 ‣ 6.1 Datasets
    and Imputation Methods ‣ 6 Experimental Evaluation and Discussion ‣ Deep Learning
    for Multivariate Time Series Imputation: A Survey")所示。数据显示，不同方法在不同数据集上的表现存在差异，在本研究中没有明显的胜者。需要进一步工作以深入比较预测和生成插补方法。特别是在像Air和ETTm1数据集这样的情况中，其中数据由传感器连续记录且缺失比例相对较低时，非参数LOC方法表现出令人称赞的性能。相反，在缺失率较高的PhysioNet2012数据集中，深度学习插补方法明显优于统计方法。这一观察结果证实了深度学习方法有效捕捉复杂时间动态和准确学习数据分布的能力，尤其是在具有高度稀疏、离散观察的场景中。'
- en: Downstream Task Evaluation
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下游任务评估
- en: 'Generally, the better quality of imputed values represents the better overall
    dataset quality after imputation. Consequently, in addition to the imputation
    performance comparison, there is an experiment setting in the literature that
    evaluates the methods from the perspective of downstream task performance Du et
    al. ([2023](#bib.bib13)). Such a study is adopted in this work as well to help
    assess the selected methods. A simple LSTM model performs the binary classification
    task on the PhysioNet2012 dataset where each sample has a label indicating whether
    the patient in the ICU was deceased. The PhysioNet2012 dataset is processed by
    the imputation methods and the results are presented in Table [4](#S6.T4 "Table
    4 ‣ 6.1 Datasets and Imputation Methods ‣ 6 Experimental Evaluation and Discussion
    ‣ Deep Learning for Multivariate Time Series Imputation: A Survey"). PR-AUC (area
    under the precision-recall curve) and ROC-AUC (area under the receiver operating
    characteristic curve) are chosen to be the metrics, considering the dataset has
    imbalanced classes and 14.2% positive samples. Note that the only variable in
    this experiment is the imputed data.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，更好的插补值质量代表插补后的整体数据集质量更好。因此，除了插补性能比较外，文献中还有一种实验设置，从下游任务性能的角度评估方法Du et al.
    ([2023](#bib.bib13))。本研究也采用了这种研究方法来帮助评估所选方法。一个简单的LSTM模型在PhysioNet2012数据集上执行二分类任务，每个样本有一个标签指示ICU中的患者是否去世。PhysioNet2012数据集经过插补方法处理，结果如表格[4](#S6.T4
    "Table 4 ‣ 6.1 Datasets and Imputation Methods ‣ 6 Experimental Evaluation and
    Discussion ‣ Deep Learning for Multivariate Time Series Imputation: A Survey")所示。选择PR-AUC（精确度-召回曲线下面积）和ROC-AUC（接收器操作特征曲线下面积）作为指标，考虑到数据集有不平衡的类别和14.2%的正样本。请注意，这个实验中唯一的变量是插补数据。'
- en: 'As shown in Table [4](#S6.T4 "Table 4 ‣ 6.1 Datasets and Imputation Methods
    ‣ 6 Experimental Evaluation and Discussion ‣ Deep Learning for Multivariate Time
    Series Imputation: A Survey"), the classifier can benefit from better imputation
    on the downstream classification task. The best results from SAITS imputation
    obtain 5% and 1% gains than the best naive imputation Mean separately on the metrics
    PR-AUC and ROC-AUC. Please note that such improvements are achieved simply by
    better imputation, which can be seen as a data-preprocessing step in this experiment.
    Furthermore, this raises a research question about how to make deep learning imputation
    models learn from both the imputation task and downstream tasks to obtain a consistent
    and unified representation from the incomplete time series.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格[4](#S6.T4 "Table 4 ‣ 6.1 Datasets and Imputation Methods ‣ 6 Experimental
    Evaluation and Discussion ‣ Deep Learning for Multivariate Time Series Imputation:
    A Survey")所示，分类器可以通过更好的插补在下游分类任务中受益。SAITS插补的最佳结果在PR-AUC和ROC-AUC指标上分别比最佳的原始插补均值提高了5%和1%。请注意，这种改进仅通过更好的插补实现，可以视为本实验中的数据预处理步骤。此外，这引发了一个研究问题，即如何使深度学习插补模型同时从插补任务和下游任务中学习，以从不完整的时间序列中获得一致且统一的表示。'
- en: Complexity Analysis
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 复杂度分析
- en: 'We summarize the time and memory complexity of the deep learning imputation
    models in Table [5](#S6.T5 "Table 5 ‣ Complexity Analysis ‣ 6.2 Results and Analysis
    ‣ 6 Experimental Evaluation and Discussion ‣ Deep Learning for Multivariate Time
    Series Imputation: A Survey"). Additionally, their actual inference time on the
    test set of PhysioNet2012 is also listed for clear comparison.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[5](#S6.T5 "Table 5 ‣ Complexity Analysis ‣ 6.2 Results and Analysis ‣
    6 Experimental Evaluation and Discussion ‣ Deep Learning for Multivariate Time
    Series Imputation: A Survey")中总结了深度学习插补模型的时间和内存复杂度。此外，还列出了它们在PhysioNet2012测试集上的实际推理时间，以便进行清晰的比较。'
- en: '| Method | Computation | Memory | Running Time |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 计算复杂度 | 内存 | 运行时间 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| M-RNN | $\mathcal{O}(L*K)$ | $\mathcal{O}(1)$ | 5s |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| M-RNN | $\mathcal{O}(L*K)$ | $\mathcal{O}(1)$ | 5s |'
- en: '| GP-VAE | $\mathcal{O}(L*\log{}K)$ | $\mathcal{O}(1)$ | 1s |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| GP-VAE | $\mathcal{O}(L*\log{}K)$ | $\mathcal{O}(1)$ | 1s |'
- en: '| BRITS | $\mathcal{O}(L)$ | $\mathcal{O}(1)$ | 9s |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| BRITS | $\mathcal{O}(L)$ | $\mathcal{O}(1)$ | 9s |'
- en: '| USGAN | $\mathcal{O}(L)$ | $\mathcal{O}(1)$ | 9s |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| USGAN | $\mathcal{O}(L)$ | $\mathcal{O}(1)$ | 9s |'
- en: '| CSDI | $\mathcal{O}(L^{2})$ | $\mathcal{O}(L^{2})$ | 104s |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| CSDI | $\mathcal{O}(L^{2})$ | $\mathcal{O}(L^{2})$ | 104s |'
- en: '| TimesNet | $\mathcal{O}(L*\log{}K)$ | $\mathcal{O}(1)$ | 1s |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| TimesNet | $\mathcal{O}(L*\log{}K)$ | $\mathcal{O}(1)$ | 1s |'
- en: '| Transformer | $\mathcal{O}(L^{2})$ | $\mathcal{O}(L^{2})$ | 1s |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | $\mathcal{O}(L^{2})$ | $\mathcal{O}(L^{2})$ | 1s |'
- en: '| SAITS | $\mathcal{O}(L^{2})$ | $\mathcal{O}(L^{2})$ | 1s |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| SAITS | $\mathcal{O}(L^{2})$ | $\mathcal{O}(L^{2})$ | 1s |'
- en: 'Table 5: Computational and space complexity of imputation models, and their
    running time in seconds on the PhysioNet2012 test set.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：插补模型的计算和空间复杂度，以及在PhysioNet2012测试集上的运行时间（秒）。
- en: 7 Conclusion and Future Direction
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论和未来方向
- en: This paper presents a systematic review of deep learning models specifically
    tailored for multivariate time series imputation. We introduce a novel taxonomy
    to categorize the reviewed methods, providing a comprehensive introduction and
    an experimental comparison of each. To advance this field, the paper concludes
    by identifying and discussing the following potential avenues for future research.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种针对多变量时间序列插补的深度学习模型的系统回顾。我们引入了一种新颖的分类法来对所评估的方法进行分类，提供了每种方法的全面介绍和实验比较。为了推动这一领域的发展，本文最后确定并讨论了未来研究的潜在方向。
- en: Missingness Patterns
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缺失模式
- en: Existing imputation algorithms predominantly operate under the MCAR or MAR.
    However, real-world missing data mechanisms are often more complex, with the MNAR
    data being prevalent in diverse fields such as IoT devices Li et al. ([2023](#bib.bib28)),
    clinical studies Ibrahim et al. ([2012](#bib.bib21)), and meteorology Ruiz et
    al. ([2023](#bib.bib45)). The non-ignorable nature of MNAR indicates a distributional
    shift exists between observed and true data Kyono et al. ([2021](#bib.bib27)).
    For example, in airflow signal analysis Ruiz et al. ([2023](#bib.bib45)), the
    absence of high-value observations causes MNAR missing mechanism and leads to
    saturated peaks, visibly skewing the observed data distribution compared to the
    true underlying one. This scenario illustrates how imputation methods may incur
    inductive bias in model parameter estimation and underperform in the presence
    of MNAR. Addressing missing data in MNAR contexts, distinct from MCAR and MAR,
    calls for innovative methodologies to achieve better performance.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的插补算法主要是在MCAR或MAR下运行。然而，现实世界中的缺失数据机制通常更加复杂，MNAR数据在各个领域普遍存在，例如IoT设备中  Li 等人（[2023](#bib.bib28)）
    ，临床研究中  Ibrahim 等人（[2012](#bib.bib21)），和气象学中  Ruiz 等人（[2023](#bib.bib45)）。MNAR的不可忽视性表明观察到的数据与真实数据之间存在分布偏移  Kyono
    等人（[2021](#bib.bib27)）。例如，在气流信号分析中  Ruiz 等人（[2023](#bib.bib45)），缺乏高值观测导致了MNAR缺失机制，并导致饱和峰值，明显地扭曲了观察到的数据分布与真实基础数据分布之间的关系。这种情况说明了插补方法可能在模型参数估计中产生归纳偏差，并在存在MNAR时表现不佳。在处理MNAR背景下的缺失数据，与MCAR和MAR不同，需要创新方法以达到更好的性能。
- en: Downstream Performance
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下游性能
- en: The primary objective of imputing missing values lies in enhancing downstream
    data analytics, particularly in scenarios with incomplete information. The prevalent
    approach is the “impute and predict” two-stage paradigm, where missing value imputation
    is a part of data preprocessing and followed by task-specific downstream models
    (e.g. a classifier), either in tandem or sequentially. An alternative method is
    the “encode and predict” end-to-end paradigm, encoding the incomplete data into
    a proper representation for multitask learning, including imputation and other
    tasks (e.g. classification and forecasting, etc.). Despite the optimal paradigm
    for partially-observed time series data still remains an open area for future
    investigation, the latter end-to-end way turns out to be more promising especially
    when information embedded in the missing patterns is helpful to the downstream
    tasks Miyaguchi et al. ([2021](#bib.bib40)).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 插补缺失值的主要目标在于增强下游数据分析，特别是在存在不完整信息的情况下。流行的方法是“插补和预测”两阶段范式，其中缺失值插补是数据预处理的一部分，随后是专门任务下游模型（例如分类器），可以同时或顺序地运行。另一种方法是“编码和预测”端到端范式，将不完整数据编码为适当表示形式，用于多任务学习，包括插补和其他任务（例如分类和预测等）。尽管部分观测到的时间序列数据的最佳范式仍然是未来研究的一个开放领域，但后一种端到端方式尤其具有更大的前景，特别是当嵌入在缺失模式中的信息对下游任务有帮助时。
- en: Scalability
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可扩展性
- en: While deep learning imputation algorithms have shown impressive performance,
    their computational cost often exceeds that of statistical and machine learning
    based counterparts. In the era of burgeoning digital data, spurred by advancements
    in communication and IoT devices, we are witnessing an exponential increase in
    data generation. This surge, accompanied by the prevalence of incomplete datasets,
    poses significant challenges in training deep models effectively Wu et al. ([2023b](#bib.bib54)).
    Specifically, the high computational demands of existing deep imputation algorithms
    render them less feasible for large-scale datasets. Consequently, there is a growing
    need for scalable deep imputation solutions, leveraging parallel and distributed
    computing techniques, to effectively address the challenges of large-scale missing
    data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习插补算法表现出色，但其计算成本往往超过统计和机器学习方法。在通信和物联网设备的进步推动下，数字数据时代的到来带来了数据生成的指数增长。这一激增，加上不完整数据集的普遍存在，对有效训练深度模型提出了重大挑战 Wu
    等 ([2023b](#bib.bib54))。具体而言，现有深度插补算法的高计算需求使其在大规模数据集上的可行性降低。因此，迫切需要可扩展的深度插补解决方案，利用并行和分布式计算技术，以有效应对大规模缺失数据的挑战。
- en: Large Language Model for MTSI
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多变量时间序列插补的语言模型
- en: Large Language Models (LLMs) have catalyzed significant advancements in fields
    such as computer vision (CV) and natural language processing (NLP), and more recently
    in time series analysis Jin et al. ([2024](#bib.bib23)). LLMs, known for their
    exceptional generalization abilities, exhibit robust predictive performance, even
    when confronted with limited datasets. This characteristic is especially valuable
    in the context of MTSI. LLMs can adeptly mitigate these data gaps by leveraging
    multimodal knowledge, exemplified by their ability to incorporate additional textual
    information into analyses Jin et al. ([2023](#bib.bib22)), thus generating multimodal
    embeddings. Such a modeling paradigm not only enriches the imputation process
    by providing a more holistic understanding and representation of the data but
    also expands the horizons of MTSI. It enables the inclusion of varied data sources,
    thereby facilitating a more detailed and context-aware imputation. Exploring the
    integration of LLMs in MTSI represents a promising direction, with the potential
    to significantly enhance the efficacy and efficiency of handling missing data
    in multivariate time series data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已在计算机视觉（CV）、自然语言处理（NLP）等领域催生了重要进展，最近在时间序列分析中也取得了突破 Jin 等 ([2024](#bib.bib23))。LLMs
    以其卓越的泛化能力而闻名，即便在数据集有限的情况下，也能展现出强大的预测性能。这一特性在多变量时间序列插补（MTSI）的背景下尤为宝贵。LLMs 能通过利用多模态知识来巧妙地弥补这些数据空缺，如通过将额外的文本信息纳入分析中 Jin
    等 ([2023](#bib.bib22))，从而生成多模态嵌入。这种建模范式不仅通过提供更全面的理解和数据表示来丰富插补过程，还拓展了 MTSI 的视野。它使得包含多样的数据源成为可能，从而促进了更详细和上下文感知的插补。探索
    LLMs 在 MTSI 中的集成是一个有前途的方向，有潜力显著提高处理多变量时间序列数据中缺失数据的效能和效率。
- en: References
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Alcaraz and Strodthoff [2023] Juan Lopez Alcaraz and Nils Strodthoff. Diffusion-based
    time series imputation and forecasting with structured state space models. Transactions
    on Machine Learning Research, 2023.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alcaraz 和 Strodthoff [2023] Juan Lopez Alcaraz 和 Nils Strodthoff。基于扩散的时间序列插补和预测与结构化状态空间模型。《机器学习研究杂志》，2023年。
- en: 'Alexandrov et al. [2020] Alexander Alexandrov, Konstantinos Benidis, Michael
    Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, et al. GluonTS: Probabilistic
    and Neural Time Series Modeling in Python. Journal of Machine Learning Research,
    21(116):1–6, 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alexandrov 等 [2020] Alexander Alexandrov、Konstantinos Benidis、Michael Bohlke-Schneider、Valentin
    Flunkert、Jan Gasthaus 等。《GluonTS：Python中的概率和神经时间序列建模》。机器学习研究杂志，21(116):1–6，2020年。
- en: Altman [1992] Naomi S Altman. An introduction to kernel and nearest-neighbor
    nonparametric regression. The American Statistician, 46(3):175–185, 1992.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Altman [1992] Naomi S Altman。核函数和最近邻非参数回归导论。《美国统计学家》，46(3):175–185，1992年。
- en: Amiri and Jensen [2016] Mehran Amiri and Richard Jensen. Missing data imputation
    using fuzzy-rough methods. Neurocomputing, 205(1):152–164, 2016.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amiri 和 Jensen [2016] Mehran Amiri 和 Richard Jensen。使用模糊-粗糙方法进行缺失数据插补。《神经计算》，205(1):152–164，2016年。
- en: Bai and Ng [2008] Jushan Bai and Serena Ng. Forecasting economic time series
    using targeted predictors. Journal of Econometrics, 146(2):304–317, 2008.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 和 Ng [2008] Jushan Bai 和 Serena Ng。使用目标预测变量预测经济时间序列。《计量经济学杂志》，146(2):304–317，2008年。
- en: Bansal et al. [2021] Parikshit Bansal, Prathamesh Deshpande, and Sunita Sarawagi.
    Missing value imputation on multidimensional time series. In VLDB, 2021.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bansal et al. [2021] Parikshit Bansal、Prathamesh Deshpande 和 Sunita Sarawagi.
    多维时间序列的缺失值插补. 见 VLDB, 2021年.
- en: Bartholomew [1971] David J Bartholomew. Time series analysis forecasting and
    control. Journal of the Operational Research Society, 22(2):199–201, 1971.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bartholomew [1971] David J Bartholomew. 时间序列分析预测与控制. Journal of the Operational
    Research Society, 22(2):199–201, 1971年.
- en: Biloš et al. [2023] Marin Biloš, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka,
    and Stephan Günnemann. Modeling temporal data as continuous functions with stochastic
    process diffusion. In ICML, 2023.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biloš et al. [2023] Marin Biloš、Kashif Rasul、Anderson Schneider、Yuriy Nevmyvaka
    和 Stephan Günnemann. 将时间数据建模为具有随机过程扩散的连续函数. 见 ICML, 2023年.
- en: 'Cao et al. [2018] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan
    Li. Brits: Bidirectional recurrent imputation for time series. NeurIPS, 2018.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao et al. [2018] 曹伟、董望、李剑、郝舟、李磊 和 李艺谭. Brits: 双向递归插补时间序列. NeurIPS, 2018年.'
- en: Che et al. [2018] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag,
    and Yan Liu. Recurrent neural networks for multivariate time series with missing
    values. Scientific Reports, 8(1), Apr 2018.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Che et al. [2018] 郑平车、Sanjay Purushotham、Kyunghyun Cho、David Sontag 和 Yan Liu.
    针对缺失值的多变量时间序列的递归神经网络. Scientific Reports, 8(1), 2018年4月.
- en: Chen et al. [2023] Yu Chen, Wei Deng, Shikai Fang, Fengpei Li, Nicole Tianjiao
    Yang, Yikai Zhang, et al. Provably convergent schrödinger bridge with applications
    to probabilistic time series imputation. In ICML, 2023.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023] 陈宇、魏登、石开方、冯佩李、Nicole Tianjiao Yang、易凯张 等. 可证明收敛的薛定谔桥及其在概率时间序列插补中的应用.
    见 ICML, 2023年.
- en: 'Cini et al. [2022] Andrea Cini, Ivan Marisca, and Cesare Alippi. Filling the
    g_ap_s: Multivariate time series imputation by graph neural networks. In ICLR,
    2022.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cini et al. [2022] Andrea Cini、Ivan Marisca 和 Cesare Alippi. 填补缺口: 通过图神经网络进行多变量时间序列插补.
    见 ICLR, 2022年.'
- en: 'Du et al. [2023] Wenjie Du, David Cote, and Yan Liu. SAITS: Self-Attention-based
    Imputation for Time Series. Expert Systems with Applications, 219:119619, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du et al. [2023] 杜文杰、David Cote 和 Yan Liu. SAITS: 基于自注意力的时间序列插补. Expert Systems
    with Applications, 219:119619, 2023年.'
- en: 'Du [2023] Wenjie Du. PyPOTS: a Python toolbox for data mining on Partially-Observed
    Time Series. In SIGKDD workshop on Mining and Learning from Time Series, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du [2023] 杜文杰. PyPOTS: 一款用于部分观察时间序列数据挖掘的Python工具箱. 见 SIGKDD 时间序列挖掘与学习研讨会, 2023年.'
- en: Esteban et al. [2017] Cristóbal Esteban, Stephanie L Hyland, and Gunnar Rätsch.
    Real-valued (medical) time series generation with recurrent conditional gans.
    ArXiv Preprint ArXiv:1706.02633, 2017.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esteban et al. [2017] Cristóbal Esteban、Stephanie L Hyland 和 Gunnar Rätsch.
    使用递归条件GANs生成实值（医疗）时间序列. ArXiv 预印本 ArXiv:1706.02633, 2017年.
- en: 'Fang and Wang [2020] Chenguang Fang and Chen Wang. Time series data imputation:
    A survey on deep learning approaches. arXiv preprint arXiv:2011.11347, 2020.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fang and Wang [2020] 方程光 和 王晨. 时间序列数据插补: 深度学习方法综述. arXiv 预印本 arXiv:2011.11347,
    2020年.'
- en: 'Fortuin et al. [2020] Vincent Fortuin, Dmitry Baranchuk, Gunnar Raetsch, and
    Stephan Mandt. GP-VAE: Deep probabilistic time series imputation. In AISTATS,
    2020.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fortuin et al. [2020] Vincent Fortuin、Dmitry Baranchuk、Gunnar Raetsch 和 Stephan
    Mandt. GP-VAE: 深度概率时间序列插补. 见 AISTATS, 2020年.'
- en: Gong et al. [2021] Yongshun Gong, Zhibin Li, Jian Zhang, Wei Liu, Yilong Yin,
    and Yu Zheng. Missing value imputation for multi-view urban statistical data via
    spatial correlation learning. IEEE Transactions on Knowledge and Data Engineering,
    2021.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong et al. [2021] 钟永顺、Zhibin Li、Jian Zhang、Wei Liu、Yilong Yin 和 Yu Zheng. 通过空间相关学习对多视角城市统计数据进行缺失值插补.
    IEEE Transactions on Knowledge and Data Engineering, 2021年.
- en: Gu et al. [2022] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling
    long sequences with structured state spaces. In ICLR, 2022.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. [2022] Albert Gu、Karan Goel 和 Christopher Re. 使用结构状态空间高效建模长序列. 见 ICLR,
    2022年.
- en: Hamzaçebi [2008] Coşkun Hamzaçebi. Improving artificial neural networks’ performance
    in seasonal time series forecasting. Information Sciences, 178(23):4550–4559,
    2008.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamzaçebi [2008] Coşkun Hamzaçebi. 提升人工神经网络在季节性时间序列预测中的表现. Information Sciences,
    178(23):4550–4559, 2008年.
- en: 'Ibrahim et al. [2012] Joseph G Ibrahim, Haitao Chu, and Ming-Hui Chen. Missing
    data in clinical studies: issues and methods. Journal of clinical oncology, 2012.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ibrahim et al. [2012] Joseph G Ibrahim、Haitao Chu 和 Ming-Hui Chen. 临床研究中的缺失数据:
    问题与方法. Journal of clinical oncology, 2012年.'
- en: 'Jin et al. [2023] Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao
    Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, et al. Large models
    for time series and spatio-temporal data: A survey and outlook. arXiv preprint
    arXiv:2310.10196, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等人 [2023] 金铭、温青松、梁宇轩、张超力、薛思乔、王雪、张詹姆斯、王怡、陈海峰、李晓丽等。大模型用于时间序列和时空数据：综述与展望。arXiv
    预印本 arXiv:2310.10196，2023年。
- en: 'Jin et al. [2024] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang,
    Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong
    Wen. Time-LLM: Time series forecasting by reprogramming large language models.
    In ICLR, 2024.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等人 [2024] 金铭、王诗雨、马琳涛、朱志轩、詹姆斯·Y·张、石晓明、陈品宇、梁宇轩、李元芳、潘诗锐和温青松。Time-LLM：通过重新编程大语言模型进行时间序列预测。见
    ICLR，2024年。
- en: 'Khayati et al. [2020] Mourad Khayati, Alberto Lerner, Zakhar Tymchenko, and
    Philippe Cudré-Mauroux. Mind the gap: an experimental evaluation of imputation
    of missing values techniques in time series. In VLDB, 2020.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khayati 等人 [2020] 穆拉德·哈亚提、阿尔贝托·勒内尔、扎哈尔·季姆钦科和菲利普·古德雷-莫罗。留意差距：时间序列中缺失值插补技术的实验评估。见
    VLDB，2020年。
- en: 'Kim and Chi [2018] Yeo Jin Kim and Min Chi. Temporal Belief Memory: Imputing
    missing data during rnn training. In IJCAI, 2018.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 和 Chi [2018] 金汝珍和池敏。时间信念记忆：在 RNN 训练期间插补缺失数据。见 IJCAI，2018年。
- en: Kim et al. [2023] Seunghyun Kim, Hyunsu Kim, Eunggu Yun, Hwangrae Lee, Jaehun
    Lee, and Juho Lee. Probabilistic imputation for time-series classification with
    missing data. In ICML, 2023.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人 [2023] 金承炫、金贤秀、尹恩久、李辉瑞、李在勋和李周浩。针对缺失数据的时间序列分类的概率插补。见 ICML，2023年。
- en: 'Kyono et al. [2021] Trent Kyono, Yao Zhang, Alexis Bellot, and Mihaela van der
    Schaar. Miracle: Causally-aware imputation via learning missing data mechanisms.
    In NeurIPS, 2021.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kyono 等人 [2021] 特伦特·京野、张耀、亚历克西斯·贝洛特和米哈伊拉·范德·夏尔。奇迹：通过学习缺失数据机制的因果感知插补。见 NeurIPS，2021年。
- en: Li et al. [2023] Xiao Li, Huan Li, Harry Kai-Ho Chan, Hua Lu, and Christian S
    Jensen. Data imputation for sparse radio maps in indoor positioning. In ICDE,
    2023.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023] 李晓、李欢、哈里·凯-霍·陈、卢华和克里斯蒂安·S·詹森。室内定位中稀疏无线电地图的数据插补。见 ICDE，2023年。
- en: Little and Rubin [2019] Roderick JA Little and Donald B Rubin. Statistical analysis
    with missing data, volume 793. John Wiley & Sons, 2019.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Little 和 Rubin [2019] 罗德里克·JA·利特尔和唐纳德·B·鲁宾。缺失数据的统计分析，第793卷。约翰·威利父子公司，2019年。
- en: 'Liu et al. [2019] Yukai Liu, Rose Yu, Stephan Zheng, Eric Zhan, and Yisong
    Yue. Naomi: Non-autoregressive multiresolution sequence imputation. In NeurIPS,
    2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2019] 刘玉凯、于玫、郑斯坦、詹力克和岳一松。Naomi：非自回归多分辨率序列插补。见 NeurIPS，2019年。
- en: Liu et al. [2022] Shuai Liu, Xiucheng Li, Gao Cong, Yile Chen, and Yue Jiang.
    Multivariate time-series imputation with disentangled temporal representations.
    2022.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2022] 施帅·刘、李秀城、龚高、陈一乐和姜月。多变量时间序列插补与解耦的时间表示。2022年。
- en: 'Liu et al. [2023] Mingzhe Liu, Han Huang, Hao Feng, Leilei Sun, Bowen Du, and
    Yanjie Fu. Pristi: A conditional diffusion framework for spatiotemporal imputation.
    arXiv preprint arXiv:2302.09746, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023] 刘明哲、黄寒、冯浩、孙磊磊、杜博文和傅延杰。Pristi：用于时空插补的条件扩散框架。arXiv 预印本 arXiv:2302.09746，2023年。
- en: 'Löning et al. [2019] Markus Löning, Anthony Bagnall, Sajaysurya Ganesh, Viktor
    Kazakov, Jason Lines, et al. sktime: A unified interface for machine learning
    with time series. arXiv preprint arXiv:1909.07872, 2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Löning 等人 [2019] 马库斯·勒宁、安东尼·巴格纳尔、萨贾苏利亚·加内什、维克托·卡扎科夫、杰森·莱恩斯等。sktime：用于时间序列的统一机器学习接口。arXiv
    预印本 arXiv:1909.07872，2019年。
- en: 'Lugmayr et al. [2022] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
    Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion
    probabilistic models. In CVPR, 2022.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lugmayr 等人 [2022] 安德烈亚斯·卢格梅尔、马丁·达内尔詹、安德烈斯·罗梅罗、费舍尔·余、拉杜·蒂莫夫特和卢克·范·古尔。Repaint：使用去噪扩散概率模型进行图像修复。见
    CVPR，2022年。
- en: Luo et al. [2018] Yonghong Luo, Xiangrui Cai, Ying ZHANG, Jun Xu, and Yuan xiaojie.
    Multivariate time series imputation with generative adversarial networks. In NeurIPS,
    2018.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人 [2018] 罗永红、蔡向睿、张颖、徐军和袁小杰。基于生成对抗网络的多变量时间序列插补。见 NeurIPS，2018年。
- en: 'Luo et al. [2019] Yonghong Luo, Ying Zhang, Xiangrui Cai, and Xiaojie Yuan.
    E²GAN: End-to-end generative adversarial network for multivariate time series
    imputation. In IJCAI, 2019.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人 [2019] 罗永红、张颖、蔡向睿和袁小杰。E²GAN：端到端生成对抗网络用于多变量时间序列插补。见 IJCAI，2019年。
- en: 'Ma et al. [2019] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony
    Vetro, and Shih-Fu Chang. CDSA: cross-dimensional self-attention for multivariate,
    geo-tagged time series imputation. arXiv preprint arXiv:1905.09904, 2019.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2019] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony
    Vetro, 和 Shih-Fu Chang。CDSA：用于多变量、地理标签时间序列插补的跨维自注意力。arXiv 预印本 arXiv:1905.09904，2019年。
- en: Marisca et al. [2022] Ivan Marisca, Andrea Cini, and Cesare Alippi. Learning
    to reconstruct missing data from spatiotemporal graphs with sparse observations.
    NeurIPS, 2022.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marisca et al. [2022] Ivan Marisca, Andrea Cini, 和 Cesare Alippi。从稀疏观测的时空图中学习重建缺失数据。发表于
    NeurIPS，2022年。
- en: Miao et al. [2021] Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao,
    and Jianwei Yin. Generative semi-supervised learning for multivariate time series
    imputation. In AAAI, 2021.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao et al. [2021] Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao,
    和 Jianwei Yin。用于多变量时间序列插补的生成半监督学习。发表于 AAAI，2021年。
- en: Miyaguchi et al. [2021] Kohei Miyaguchi, Takayuki Katsuki, Akira Koseki, and
    Toshiya Iwamori. Variational inference for discriminative learning with generative
    modeling of feature incompletion. In ICLR, 2021.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miyaguchi et al. [2021] Kohei Miyaguchi, Takayuki Katsuki, Akira Koseki, 和 Toshiya
    Iwamori。特征缺失生成建模的判别学习变分推断。发表于 ICLR，2021年。
- en: '[41] Steffen Moritz and Thomas Bartz-Beielstein. imputeTS: Time Series Missing
    Value Imputation in R. The R Journal.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Steffen Moritz 和 Thomas Bartz-Beielstein。imputeTS：R语言中的时间序列缺失值插补。R Journal。'
- en: Mulyadi et al. [2021] Ahmad Wisnu Mulyadi, Eunji Jun, and Heung-Il Suk. Uncertainty-aware
    variational-recurrent imputation network for clinical time series. IEEE Transactions
    on Cybernetics, 52(9):9684–9694, 2021.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mulyadi et al. [2021] Ahmad Wisnu Mulyadi, Eunji Jun, 和 Heung-Il Suk。面向临床时间序列的具有不确定性意识的变分递归插补网络。IEEE
    网络科学与工程交易，52(9):9684–9694，2021年。
- en: 'Nakagawa [2015] Shinichi Nakagawa. Missing data: mechanisms, methods and messages.
    pages 81–105\. Oxford University Press Oxford, UK, 2015.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakagawa [2015] Shinichi Nakagawa。缺失数据：机制、方法与信息。第81–105页。牛津大学出版社，英国牛津，2015年。
- en: Rubin [1976] Donald B. Rubin. Inference and missing data. Biometrika, 63(3):581–592,
    12 1976.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubin [1976] Donald B. Rubin。推断与缺失数据。生物统计学，63(3):581–592，1976年12月。
- en: Ruiz et al. [2023] Joaquin Ruiz, Hau-tieng Wu, and Marcelo A Colominas. Enhancing
    missing data imputation of non-stationary signals with harmonic decomposition.
    arXiv preprint arXiv:2309.04630, 2023.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruiz et al. [2023] Joaquin Ruiz, Hau-tieng Wu, 和 Marcelo A Colominas。通过谐波分解增强非平稳信号的缺失数据插补。arXiv
    预印本 arXiv:2309.04630，2023年。
- en: 'Shan et al. [2023] Siyuan Shan, Yang Li, and Junier B. Oliva. Nrtsi: Non-recurrent
    time series imputation. In ICASSP, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shan et al. [2023] Siyuan Shan, Yang Li, 和 Junier B. Oliva。Nrtsi：非递归时间序列插补。发表于
    ICASSP，2023年。
- en: 'Silva et al. [2012] Ikaro Silva, George Moody, Daniel J Scott, Leo A Celi,
    and Roger G Mark. Predicting in-hospital mortality of icu patients: The physionet/computing
    in cardiology challenge 2012. Computing in cardiology, 39:245, 2012.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silva et al. [2012] Ikaro Silva, George Moody, Daniel J Scott, Leo A Celi, 和
    Roger G Mark。预测 ICU 患者的院内死亡率：PhysioNet/计算心脏病挑战2012。计算心脏病学，39:245，2012年。
- en: 'Tashiro et al. [2021] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano
    Ermon. CSDI: Conditional score-based diffusion models for probabilistic time series
    imputation. In NeurIPS, 2021.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tashiro et al. [2021] Yusuke Tashiro, Jiaming Song, Yang Song, 和 Stefano Ermon。CSDI：用于概率时间序列插补的条件得分基扩散模型。发表于
    NeurIPS，2021年。
- en: 'Van Buuren and Groothuis-Oudshoorn [2011] Stef Van Buuren and Karin Groothuis-Oudshoorn.
    mice: Multivariate imputation by chained equations in r. Journal of statistical
    software, 45:1–67, 2011.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Buuren 和 Groothuis-Oudshoorn [2011] Stef Van Buuren 和 Karin Groothuis-Oudshoorn。mice：R语言中的链式方程多重插补。统计软件杂志，45:1–67，2011年。
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, et al. Attention is all you need. In NeurIPS, 2017.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez 等。注意力即你所需。发表于 NeurIPS，2017年。
- en: Wang et al. [2023] Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu
    Wang, Zhengyang Zhou, and Yang Wang. An observed value consistent diffusion model
    for imputing missing values in multivariate time series. In SIGKDD, 2023.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023] Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu
    Wang, Zhengyang Zhou, 和 Yang Wang。用于插补多变量时间序列缺失值的观测值一致扩散模型。发表于 SIGKDD，2023年。
- en: 'Wen et al. [2023] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing
    Ma, Junchi Yan, and Liang Sun. Transformers in time series: A survey. In International
    Joint Conference on Artificial Intelligence(IJCAI), 2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen et al. [2023] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing
    Ma, Junchi Yan, 和 Liang Sun。时间序列中的变换器：综述。发表于国际人工智能联合会议(IJCAI)，2023年。
- en: 'Wu et al. [2023a] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and
    Mingsheng Long. TimesNet: Temporal 2D-Variation Modeling for General Time Series
    Analysis. In ICLR, 2023.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人 [2023a] 吴海旭、胡腾哥、刘永、周航、王建敏和龙名生。TimesNet：通用时间序列分析的时序2D变换建模。发表于ICLR，2023。
- en: Wu et al. [2023b] Yangyang Wu, Jun Wang, Xiaoye Miao, Wenjia Wang, and Jianwei
    Yin. Differentiable and scalable generative adversarial models for data imputation.
    IEEE Transactions on Knowledge and Data Engineering, 2023.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人 [2023b] 吴阳阳、王军、苗晓叶、王文佳和尹建伟。可微分和可扩展的生成对抗模型用于数据插补。IEEE知识与数据工程学报，2023。
- en: Xu et al. [2023] Jingwen Xu, Fei Lyu, and Pong C Yuen. Density-aware temporal
    attentive step-wise diffusion model for medical time series imputation. In CIKM,
    2023.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 徐等人 [2023] 朱经文、吕飞和袁鹏**C Y**。面向密度的时序注意力逐步扩散模型用于医学时间序列插补。发表于CIKM，2023。
- en: Yoon et al. [2019] Jinsung Yoon, William R. Zame, and Mihaela van der Schaar.
    Estimating missing data in temporal data streams using multi-directional recurrent
    neural networks. IEEE Trans. on Biomedical Engineering, 2019.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尹等人 [2019] 尹振生、威廉**R.** 扎梅和米哈伊拉·范德·沙尔。使用多方向递归神经网络估计时间数据流中的缺失数据。IEEE生物医学工程学报，2019。
- en: 'Zhang et al. [2017] Shuyi Zhang, Bin Guo, Anlan Dong, Jing He, Ziping Xu, and
    S. Chen. Cautionary tales on air-quality improvement in beijing. Proceedings of
    the Royal Society A: Mathematical, Physical and Engineering Sciences, 473, 2017.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2017] 张书宜、郭斌、董安兰、何静、徐自平和**S.** 陈。关于北京空气质量改善的警示故事。皇家学会A辑：数学、物理和工程科学，473，2017。
- en: 'Zhou et al. [2021] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin
    Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long
    sequence time-series forecasting. In AAAI, 2021.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等人 [2021] 周浩毅、张上航、彭杰齐、张帅、李建鑫、熊辉和张万才。Informer：超越高效Transformer的长序列时间序列预测。发表于AAAI，2021。
