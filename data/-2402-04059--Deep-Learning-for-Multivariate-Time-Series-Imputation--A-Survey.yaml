- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-06 19:34:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-06 19:34:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2402.04059] Deep Learning for Multivariate Time Series Imputation: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2402.04059] å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2402.04059](https://ar5iv.labs.arxiv.org/html/2402.04059)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2402.04059](https://ar5iv.labs.arxiv.org/html/2402.04059)
- en: 'Deep Learning for Multivariate Time Series Imputation: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°
- en: Author Name Affiliation email@example.com â€ƒâ€ƒ Jun Wang^(1,2,4) The first two
    authors contributed equally to this work. â€ƒâ€ƒ Wenjie Du^(2âˆ—) â€ƒâ€ƒ Wei CaoÂ² â€ƒâ€ƒ Keli
    ZhangÂ³ â€ƒâ€ƒ Wenjia Wang^(1,4) â€ƒâ€ƒ Yuxuan Liangâ´ â€ƒâ€ƒ Qingsong Wenâµ Â¹Hong Kong University
    of Science and Technology â€ƒÂ²PyPOTS Research Team
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…å§“å æ‰€å±æœºæ„ email@example.com â€ƒâ€ƒ Jun Wang^(1,2,4) å‰ä¸¤ä½ä½œè€…å¯¹è¿™é¡¹å·¥ä½œè´¡çŒ®ç›¸ç­‰ã€‚ â€ƒâ€ƒ Wenjie Du^(2âˆ—)
    â€ƒâ€ƒ Wei CaoÂ² â€ƒâ€ƒ Keli ZhangÂ³ â€ƒâ€ƒ Wenjia Wang^(1,4) â€ƒâ€ƒ Yuxuan Liangâ´ â€ƒâ€ƒ Qingsong Wenâµ
    Â¹é¦™æ¸¯ç§‘æŠ€å¤§å­¦ â€ƒÂ²PyPOTS ç ”ç©¶å›¢é˜Ÿ
- en: Â³Huawei Noahâ€™s Ark Lab â´Hong Kong University of Science and Technology (Guangzhou)
    âµSquirrel AI jwangfx@connect.ust.hk,â€ƒwdu@pypots.com,â€ƒweicaomsra@gmail.com,â€ƒzhangkeli1@huawei.com,â€ƒwenjiawang@ust.hk,â€ƒyuxliang@outlook.com,â€ƒqingsongedu@gmail.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Â³åä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤ â´é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰ âµSquirrel AI jwangfx@connect.ust.hkï¼Œwdu@pypots.comï¼Œweicaomsra@gmail.comï¼Œzhangkeli1@huawei.comï¼Œwenjiawang@ust.hkï¼Œyuxliang@outlook.comï¼Œqingsongedu@gmail.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: The ubiquitous missing values cause the multivariate time series data to be
    partially observed, destroying the integrity of time series and hindering the
    effective time series data analysis. Recently deep learning imputation methods
    have demonstrated remarkable success in elevating the quality of corrupted time
    series data, subsequently enhancing performance in downstream tasks. In this paper,
    we conduct a comprehensive survey on the recently proposed deep learning imputation
    methods. First, we propose a taxonomy for the reviewed methods, and then provide
    a structured review of these methods by highlighting their strengths and limitations.
    We also conduct empirical experiments to study different methods and compare their
    enhancement for downstream tasks. Finally, the open issues for future research
    on multivariate time series imputation are pointed out. All code and configurations
    of this work, including a regularly maintained multivariate time series imputation
    paper list, can be found in the GitHub repositoryÂ [https://github.com/WenjieDu/Awesome_Imputation](https://github.com/WenjieDu/Awesome_Imputation).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¿æ³›å­˜åœ¨çš„ç¼ºå¤±å€¼ä½¿å¾—å¤šå˜é‡æ—¶é—´åºåˆ—æ•°æ®éƒ¨åˆ†è§‚æµ‹ï¼Œç ´åäº†æ—¶é—´åºåˆ—çš„å®Œæ•´æ€§ï¼Œé˜»ç¢äº†æœ‰æ•ˆçš„æ—¶é—´åºåˆ—æ•°æ®åˆ†æã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ æ’è¡¥æ–¹æ³•åœ¨æå‡æŸåæ—¶é—´åºåˆ—æ•°æ®çš„è´¨é‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æˆåŠŸï¼Œè¿›è€Œæé«˜äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æœ¬æ–‡å¯¹æœ€è¿‘æå‡ºçš„æ·±åº¦å­¦ä¹ æ’è¡¥æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†æ‰€è¯„å®¡æ–¹æ³•çš„åˆ†ç±»æ³•ï¼Œç„¶åé€šè¿‡çªå‡ºè¿™äº›æ–¹æ³•çš„ä¼˜ç‚¹å’Œå±€é™æ€§æä¾›äº†ç»“æ„åŒ–çš„è¯„è®ºã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å®è¯å®éªŒï¼Œä»¥ç ”ç©¶ä¸åŒæ–¹æ³•å¹¶æ¯”è¾ƒå®ƒä»¬å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„æå‡ã€‚æœ€åï¼ŒæŒ‡å‡ºäº†æœªæ¥å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥ç ”ç©¶çš„å¼€æ”¾é—®é¢˜ã€‚æ‰€æœ‰çš„ä»£ç å’Œé…ç½®ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå®šæœŸç»´æŠ¤çš„å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥è®ºæ–‡åˆ—è¡¨ï¼Œå¯ä»¥åœ¨
    GitHub ä»“åº“ä¸­æ‰¾åˆ° [https://github.com/WenjieDu/Awesome_Imputation](https://github.com/WenjieDu/Awesome_Imputation)ã€‚
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: The data collection process of multivariate time series in various fields, such
    as financeÂ Bai and Ng ([2008](#bib.bib5)), medicineÂ Esteban et al. ([2017](#bib.bib15)),
    and transportationÂ Gong et al. ([2021](#bib.bib18)), is often fraught with difficulties
    and uncertainty, like sensor failures, instable system environment, privacy concerns,
    or other reasons. This leads to datasets usually containing a great number of
    missing values, and can significantly affect the accuracy and reliability of downstream
    analysis and decision-making. For example, the public real-world medical time
    series dataset PhysioNet2012 Silva et al. ([2012](#bib.bib47)) takes even above
    80$\%$ average missing rate, making it challenging to analyze. Consequently, exploring
    how to reasonably and effectively impute missing components in multivariate time
    series data is attractive and essential.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é‡‘èÂ Bai å’Œ Ng ([2008](#bib.bib5))ã€åŒ»å­¦Â Esteban ç­‰ ([2017](#bib.bib15)) å’Œäº¤é€šè¿è¾“Â Gong
    ç­‰ ([2021](#bib.bib18)) ç­‰é¢†åŸŸï¼Œå¤šå˜é‡æ—¶é—´åºåˆ—çš„æ•°æ®æ”¶é›†è¿‡ç¨‹å¸¸å¸¸å……æ»¡å›°éš¾å’Œä¸ç¡®å®šæ€§ï¼Œä¾‹å¦‚ä¼ æ„Ÿå™¨æ•…éšœã€ç³»ç»Ÿç¯å¢ƒä¸ç¨³å®šã€éšç§é—®é¢˜æˆ–å…¶ä»–åŸå› ã€‚è¿™å¯¼è‡´æ•°æ®é›†é€šå¸¸åŒ…å«å¤§é‡ç¼ºå¤±å€¼ï¼Œå¯èƒ½ä¸¥é‡å½±å“ä¸‹æ¸¸åˆ†æå’Œå†³ç­–çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ä¾‹å¦‚ï¼Œå…¬å…±çš„ç°å®ä¸–ç•ŒåŒ»ç–—æ—¶é—´åºåˆ—æ•°æ®é›†
    PhysioNet2012 Silva ç­‰ ([2012](#bib.bib47)) çš„ç¼ºå¤±ç‡ç”šè‡³è¶…è¿‡ 80$\%$ï¼Œä½¿å¾—åˆ†æå˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œæ¢ç´¢å¦‚ä½•åˆç†æœ‰æ•ˆåœ°å¡«è¡¥å¤šå˜é‡æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„ç¼ºå¤±éƒ¨åˆ†æ˜¯éå¸¸æœ‰å¸å¼•åŠ›å’Œå¿…è¦çš„ã€‚
- en: The earlier statistical imputation methods have historically been widely used
    for handling missing data. Those methods substitute the missing values with the
    statistics (e.g., zero value, mean value, and last observed valueÂ Amiri and Jensen
    ([2016](#bib.bib4))) or simple statistical models, including ARIMAÂ Bartholomew
    ([1971](#bib.bib7)), ARFIMAÂ HamzaÃ§ebi ([2008](#bib.bib20)), and SARIMAÂ HamzaÃ§ebi
    ([2008](#bib.bib20)). Furthermore, machine learning techniques like regression,
    K-nearest neighbor, matrix factorization, etc., have gained prominence in the
    literature for addressing missing values in multivariate time series. Key implementations
    of these approaches include KNNIÂ Altman ([1992](#bib.bib3)), TIDERÂ Liu et al.
    ([2022](#bib.bib31)), MICEÂ VanÂ Buuren and Groothuis-Oudshoorn ([2011](#bib.bib49)),
    etc. While statistical and machine learning imputation methods are simple and
    efficient, they fall short in capturing the intricate temporal relationships and
    complex variation patterns inherent in time series data, resulting in limited
    performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¾ƒæ—©çš„ç»Ÿè®¡æ’è¡¥æ–¹æ³•åœ¨å¤„ç†ç¼ºå¤±æ•°æ®æ–¹é¢å†å²ä¸Šè¢«å¹¿æ³›ä½¿ç”¨ã€‚è¿™äº›æ–¹æ³•ç”¨ç»Ÿè®¡é‡ï¼ˆä¾‹å¦‚ï¼Œé›¶å€¼ã€å‡å€¼å’Œæœ€åè§‚æµ‹å€¼ Amiri å’Œ Jensenï¼ˆ[2016](#bib.bib4)ï¼‰ï¼‰æˆ–ç®€å•çš„ç»Ÿè®¡æ¨¡å‹è¿›è¡Œç¼ºå¤±å€¼æ›¿ä»£ï¼ŒåŒ…æ‹¬
    ARIMA Bartholomewï¼ˆ[1971](#bib.bib7)ï¼‰ã€ARFIMA HamzaÃ§ebiï¼ˆ[2008](#bib.bib20)ï¼‰å’Œ SARIMA
    HamzaÃ§ebiï¼ˆ[2008](#bib.bib20)ï¼‰ã€‚æ­¤å¤–ï¼Œå›å½’ã€K æœ€è¿‘é‚»ã€çŸ©é˜µåˆ†è§£ç­‰æœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨æ–‡çŒ®ä¸­ä¹Ÿè·å¾—äº†æ˜¾è‘—å…³æ³¨ï¼Œç”¨äºå¤„ç†å¤šå˜é‡æ—¶é—´åºåˆ—ä¸­çš„ç¼ºå¤±å€¼ã€‚è¿™äº›æ–¹æ³•çš„å…³é”®å®ç°åŒ…æ‹¬
    KNNI Altmanï¼ˆ[1992](#bib.bib3)ï¼‰ã€TIDER Liu ç­‰ï¼ˆ[2022](#bib.bib31)ï¼‰ã€MICE Van Buuren
    å’Œ Groothuis-Oudshoornï¼ˆ[2011](#bib.bib49)ï¼‰ç­‰ã€‚å°½ç®¡ç»Ÿè®¡å’Œæœºå™¨å­¦ä¹ æ’è¡¥æ–¹æ³•ç®€å•é«˜æ•ˆï¼Œä½†å®ƒä»¬åœ¨æ•æ‰æ—¶é—´åºåˆ—æ•°æ®ä¸­å¤æ‚çš„æ—¶é—´å…³ç³»å’Œå˜åŒ–æ¨¡å¼æ–¹é¢æœ‰æ‰€ä¸è¶³ï¼Œå¯¼è‡´æ€§èƒ½æœ‰é™ã€‚
- en: More recently, deep learning imputation methods have shown great modeling ability
    in missing data imputation. These methods exploit powerful deep learning models
    like Transformers, Variational AutoEncoders (VAEs), Generative Adversarial Networks
    (GANs), and diffusion models to capture the intrinsic properties and potentially
    complex dynamics of time series. In this way, deep learning imputation methods
    can learn the true underlying data distribution from the observed data, so as
    to predict more reliable and reasonable values for the missing components. We
    note that there are several related surveys Khayati et al. ([2020](#bib.bib24));
    Fang and Wang ([2020](#bib.bib16)) that primarily focus on statistical and machine
    learning imputation methods, but they offer only limited consideration of deep
    learning imputation methods. Considering that multivariate time series imputation
    is a crucial data preprocessing step for subsequent time series analysis, a thorough
    and systematic survey on deep multivariate time series imputation methods would
    significantly contribute to the advancement of the time series community.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ æ’è¡¥æ–¹æ³•åœ¨ç¼ºå¤±æ•°æ®æ’è¡¥æ–¹é¢è¡¨ç°å‡ºæå¼ºçš„å»ºæ¨¡èƒ½åŠ›ã€‚è¿™äº›æ–¹æ³•åˆ©ç”¨å¼ºå¤§çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¦‚ Transformersã€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼Œä»¥æ•æ‰æ—¶é—´åºåˆ—çš„å†…åœ¨å±æ€§å’Œæ½œåœ¨å¤æ‚åŠ¨æ€ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ·±åº¦å­¦ä¹ æ’è¡¥æ–¹æ³•å¯ä»¥ä»è§‚æµ‹æ•°æ®ä¸­å­¦ä¹ åˆ°çœŸå®çš„åº•å±‚æ•°æ®åˆ†å¸ƒï¼Œä»è€Œä¸ºç¼ºå¤±éƒ¨åˆ†é¢„æµ‹å‡ºæ›´å¯é å’Œåˆç†çš„å€¼ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œæœ‰å‡ ä¸ªç›¸å…³çš„ç»¼è¿°ï¼Œå¦‚
    Khayati ç­‰ï¼ˆ[2020](#bib.bib24)ï¼‰ï¼›Fang å’Œ Wangï¼ˆ[2020](#bib.bib16)ï¼‰ï¼Œä¸»è¦é›†ä¸­åœ¨ç»Ÿè®¡å’Œæœºå™¨å­¦ä¹ æ’è¡¥æ–¹æ³•ä¸Šï¼Œä½†å¯¹æ·±åº¦å­¦ä¹ æ’è¡¥æ–¹æ³•çš„è€ƒè™‘è¾ƒå°‘ã€‚è€ƒè™‘åˆ°å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥æ˜¯åç»­æ—¶é—´åºåˆ—åˆ†æçš„å…³é”®æ•°æ®é¢„å¤„ç†æ­¥éª¤ï¼Œå¯¹æ·±åº¦å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥æ–¹æ³•è¿›è¡Œå…¨é¢ç³»ç»Ÿçš„ç»¼è¿°å°†æ˜¾è‘—ä¿ƒè¿›æ—¶é—´åºåˆ—é¢†åŸŸçš„è¿›æ­¥ã€‚
- en: 'In this paper, we endeavor to bridge the existing knowledge gap by providing
    a comprehensive summary of the latest developments in deep learning methods for
    multivariate time series imputation (MTSI). First, we present a succinct introduction
    to the topic, followed by the proposal of a novel taxonomy, categorizing approaches
    based on two perspectives: imputation uncertainty and neural network architecture.
    Imputation uncertainty reflects confidence in imputed values for missing data,
    and capturing this involves stochastically generating samples and conducting imputations
    based on these varied samplesÂ Little and Rubin ([2019](#bib.bib29)). Accordingly,
    we categorize imputation methods into predictive ones, offering fixed estimates,
    and generative ones, which provide a distribution of possible values to account
    for imputation uncertainty. For neural network architecture, we explore a range
    of deep learning models tailored for MTSI, including Recurrent Neural Network
    (RNN)-based ones, Graph Neural Network (GNN)-based ones, Convolutional Neural
    Network (CNN)-based ones, attention-based ones, Variational AutoEncoder (VAE)-based
    ones, Generative Adversarial Network (GAN)-based ones, and diffusion-based ones.
    To provide practical imputation guidelines in real scenarios, we conduct extensive
    empirical studies that examine multiple aspects of deep multivariate time series
    imputation models, including imputation performance and improvement on downstream
    tasks like classification. To the best of our knowledge, this is the first comprehensive
    and systematic review of deep learning algorithms in the realm of MSTI, aiming
    to stimulate further research in this field. A corresponding resource that has
    been continuously updated can be found in our GitHub repositoryÂ¹Â¹1[https://github.com/WenjieDu/Awesome_Imputation](https://github.com/WenjieDu/Awesome_Imputation).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åŠ›å›¾é€šè¿‡æä¾›å¤šå˜é‡æ—¶é—´åºåˆ—å¡«å……ï¼ˆMTSIï¼‰æ·±åº¦å­¦ä¹ æ–¹æ³•æœ€æ–°å‘å±•çš„å…¨é¢æ€»ç»“ï¼Œå¼¥åˆç°æœ‰çš„çŸ¥è¯†å·®è·ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç®€è¦ä»‹ç»äº†è¯¥ä¸»é¢˜ï¼Œç„¶åæå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†ç±»æ–¹æ³•ï¼ŒåŸºäºå¡«å……ä¸ç¡®å®šæ€§å’Œç¥ç»ç½‘ç»œæ¶æ„è¿™ä¸¤ä¸ªè§†è§’å¯¹æ–¹æ³•è¿›è¡Œåˆ†ç±»ã€‚å¡«å……ä¸ç¡®å®šæ€§åæ˜ äº†å¯¹ç¼ºå¤±æ•°æ®å¡«å……å€¼çš„ä¿¡å¿ƒï¼Œè¿™æ¶‰åŠåˆ°éšæœºç”Ÿæˆæ ·æœ¬å¹¶åŸºäºè¿™äº›ä¸åŒçš„æ ·æœ¬è¿›è¡Œå¡«å……Â Little
    å’Œ Rubin ([2019](#bib.bib29))ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å¡«å……æ–¹æ³•åˆ†ä¸ºé¢„æµ‹å‹æ–¹æ³•ï¼Œæä¾›å›ºå®šä¼°è®¡ï¼Œä»¥åŠç”Ÿæˆå‹æ–¹æ³•ï¼Œæä¾›å¯èƒ½å€¼çš„åˆ†å¸ƒä»¥è€ƒè™‘å¡«å……ä¸ç¡®å®šæ€§ã€‚å¯¹äºç¥ç»ç½‘ç»œæ¶æ„ï¼Œæˆ‘ä»¬æ¢è®¨äº†å¤šç§é’ˆå¯¹MTSIçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºäºå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„æ¨¡å‹ã€åŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„æ¨¡å‹ã€åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ¨¡å‹ã€åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹ã€åŸºäºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„æ¨¡å‹ã€åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ¨¡å‹ä»¥åŠåŸºäºæ‰©æ•£æ¨¡å‹çš„æ¨¡å‹ã€‚ä¸ºäº†åœ¨å®é™…åœºæ™¯ä¸­æä¾›æœ‰æ•ˆçš„å¡«å……æŒ‡å¯¼ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œè€ƒå¯Ÿäº†æ·±åº¦å¤šå˜é‡æ—¶é—´åºåˆ—å¡«å……æ¨¡å‹çš„å¤šä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬å¡«å……æ€§èƒ½ä»¥åŠåœ¨åˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ”¹è¿›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢ç³»ç»Ÿåœ°å®¡æŸ¥MTSIé¢†åŸŸæ·±åº¦å­¦ä¹ ç®—æ³•çš„ç»¼è¿°ï¼Œæ—¨åœ¨æ¿€å‘è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚ä¸€ä¸ªä¸æ–­æ›´æ–°çš„ç›¸å…³èµ„æºå¯ä»¥åœ¨æˆ‘ä»¬çš„GitHubä»“åº“ä¸­æ‰¾åˆ°Â¹Â¹1[https://github.com/WenjieDu/Awesome_Imputation](https://github.com/WenjieDu/Awesome_Imputation)ã€‚
- en: 'In summary, the contributions of this paper include: 1) A new taxonomy for
    deep multivariate time series imputation methods, considering imputation uncertainty
    and neural network architecture, with a comprehensive methodological review; 2)
    A thorough empirical evaluation of imputation algorithms via the PyPOTS toolkit
    we developed; 3) An exploration of future research opportunities for MTSI.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œæœ¬æ–‡çš„è´¡çŒ®åŒ…æ‹¬ï¼š1) é’ˆå¯¹æ·±åº¦å¤šå˜é‡æ—¶é—´åºåˆ—å¡«å……æ–¹æ³•æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»æ–¹æ³•ï¼Œè€ƒè™‘äº†å¡«å……ä¸ç¡®å®šæ€§å’Œç¥ç»ç½‘ç»œæ¶æ„ï¼Œå¹¶è¿›è¡Œäº†å…¨é¢çš„æ–¹æ³•ç»¼è¿°ï¼›2) é€šè¿‡æˆ‘ä»¬å¼€å‘çš„PyPOTSå·¥å…·åŒ…å¯¹å¡«å……ç®—æ³•è¿›è¡Œäº†å½»åº•çš„å®è¯è¯„ä¼°ï¼›3)
    æ¢ç´¢äº†å¤šå˜é‡æ—¶é—´åºåˆ—å¡«å……ï¼ˆMTSIï¼‰æœªæ¥çš„ç ”ç©¶æœºä¼šã€‚
- en: 2 Preliminary and Taxonomy
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 åˆæ­¥å’Œåˆ†ç±»
- en: 2.1 Background of MTSI
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 MTSIèƒŒæ™¯
- en: Problem Definition
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é—®é¢˜å®šä¹‰
- en: A complete time series dataset on $[0,T]$ typically can be denoted as $\mathcal{D}=\{\mathbf{X}_{i},\mathbf{t}_{i}\}_{i=1}^{N}$.
    Hereby, $\mathbf{X}_{i}=\{x_{1:K,1:L}\}\in\mathcal{R}^{K\times L}$ and $\mathbf{t}_{i}=({t_{1},\cdots,t_{L}})\in[0,T]^{L}$,
    where $K$ is the number of features and $L$ is the length of time series. In the
    missing data context, each complete time series can be split into an observed
    and a missing part, i.e., $\mathbf{X}_{i}=\{\mathbf{X}^{o}_{i},\mathbf{X}^{m}_{i}\}$.
    For encoding the missingness, we also denote an observation matrix as $\mathbf{M}_{i}=\{m_{1:K,1:L}\}$,
    where $m_{k,l}=0$ if $x_{k,l}$ is missing at timestamp $t_{l}$, otherwise $m_{k,l}=1$.
    Furthermore, we can also calculate a time-lag matrix $\boldsymbol{\delta}_{i}=\{\delta_{1:K,1:L}\}$
    by the following rule,
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨$[0,T]$ä¸Šçš„å®Œæ•´æ—¶é—´åºåˆ—æ•°æ®é›†é€šå¸¸å¯ä»¥è¡¨ç¤ºä¸º$\mathcal{D}=\{\mathbf{X}_{i},\mathbf{t}_{i}\}_{i=1}^{N}$ã€‚å…¶ä¸­ï¼Œ$\mathbf{X}_{i}=\{x_{1:K,1:L}\}\in\mathcal{R}^{K\times
    L}$ï¼Œè€Œ$\mathbf{t}_{i}=({t_{1},\cdots,t_{L}})\in[0,T]^{L}$ï¼Œ$K$æ˜¯ç‰¹å¾çš„æ•°é‡ï¼Œ$L$æ˜¯æ—¶é—´åºåˆ—çš„é•¿åº¦ã€‚åœ¨ç¼ºå¤±æ•°æ®çš„èƒŒæ™¯ä¸‹ï¼Œæ¯ä¸ªå®Œæ•´çš„æ—¶é—´åºåˆ—å¯ä»¥åˆ†ä¸ºè§‚å¯Ÿéƒ¨åˆ†å’Œç¼ºå¤±éƒ¨åˆ†ï¼Œå³$\mathbf{X}_{i}=\{\mathbf{X}^{o}_{i},\mathbf{X}^{m}_{i}\}$ã€‚ä¸ºäº†ç¼–ç ç¼ºå¤±æƒ…å†µï¼Œæˆ‘ä»¬è¿˜ç”¨$\mathbf{M}_{i}=\{m_{1:K,1:L}\}$è¡¨ç¤ºè§‚å¯ŸçŸ©é˜µï¼Œå…¶ä¸­ï¼Œå¦‚æœåœ¨æ—¶é—´æˆ³$t_{l}$æ—¶$x_{k,l}$ç¼ºå¤±ï¼Œåˆ™$m_{k,l}=0$ï¼Œå¦åˆ™$m_{k,l}=1$ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ä»¥ä¸‹è§„åˆ™è®¡ç®—æ—¶é—´æ»åçŸ©é˜µ$\boldsymbol{\delta}_{i}=\{\delta_{1:K,1:L}\}$ï¼Œ
- en: '|  | <math   alttext="\delta_{k,l}=\left\{\begin{array}[]{ll}{0,}&amp;{\text{if
    }l=1}\\ {t_{l}-t_{l-1},}&amp;{\text{if }m_{k,l-1}=1\text{ and }l>1}\\'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\delta_{k,l}=\left\{\begin{array}[]{ll}{0,}&amp;{\text{if
    }l=1}\\ {t_{l}-t_{l-1},}&amp;{\text{if }m_{k,l-1}=1\text{ and }l>1}\\'
- en: '{\delta_{k,l-1}+t_{l}-t_{l-1},}&amp;{\text{if }m_{k,l-1}=0\text{ and }l>1}\end{array}\right."
    display="block"><semantics ><mrow  ><msub ><mi >Î´</mi><mrow  ><mi >k</mi><mo >,</mo><mi  >l</mi></mrow></msub><mo
    >=</mo><mrow ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd columnalign="left"  ><mrow ><mn >0</mn><mo  >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext  >ifÂ </mtext><mo lspace="0em" rspace="0em"  >â€‹</mo><mi
    >l</mi></mrow><mo >=</mo><mn >1</mn></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow  ><mrow ><msub ><mi  >t</mi><mi >l</mi></msub><mo >âˆ’</mo><msub ><mi >t</mi><mrow
    ><mi >l</mi><mo >âˆ’</mo><mn >1</mn></mrow></msub></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow  ><mtext >ifÂ </mtext><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >m</mi><mrow ><mi  >k</mi><mo >,</mo><mrow ><mi >l</mi><mo >âˆ’</mo><mn >1</mn></mrow></mrow></msub></mrow><mo
    >=</mo><mrow ><mn  >1</mn><mo lspace="0em" rspace="0em"  >â€‹</mo><mtext >Â andÂ </mtext><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >l</mi></mrow><mo >></mo><mn >1</mn></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow  ><mrow ><mrow ><msub  ><mi >Î´</mi><mrow ><mi
    >k</mi><mo >,</mo><mrow ><mi >l</mi><mo >âˆ’</mo><mn >1</mn></mrow></mrow></msub><mo
    >+</mo><msub ><mi >t</mi><mi >l</mi></msub></mrow><mo >âˆ’</mo><msub ><mi  >t</mi><mrow
    ><mi >l</mi><mo >âˆ’</mo><mn >1</mn></mrow></msub></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow  ><mtext >ifÂ </mtext><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >m</mi><mrow ><mi  >k</mi><mo >,</mo><mrow ><mi >l</mi><mo >âˆ’</mo><mn >1</mn></mrow></mrow></msub></mrow><mo
    >=</mo><mrow ><mn  >0</mn><mo lspace="0em" rspace="0em"  >â€‹</mo><mtext >Â andÂ </mtext><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >l</mi></mrow><mo >></mo><mn >1</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ›¿</ci><list ><ci  >ğ‘˜</ci><ci >ğ‘™</ci></list></apply><apply ><csymbol cd="latexml"
    >cases</csymbol><matrix ><matrixrow ><cn type="integer"  >0</cn><apply ><apply
    ><ci  ><mtext >ifÂ </mtext></ci><ci >ğ‘™</ci></apply><cn type="integer"  >1</cn></apply></matrixrow><matrixrow
    ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘¡</ci><ci
    >ğ‘™</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘¡</ci><apply
    ><ci >ğ‘™</ci><cn type="integer" >1</cn></apply></apply></apply><apply ><apply  ><apply
    ><ci ><mtext  >ifÂ </mtext></ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘š</ci><list ><ci >ğ‘˜</ci><apply ><ci >ğ‘™</ci><cn type="integer" >1</cn></apply></list></apply></apply><apply
    ><cn type="integer" >1</cn><ci ><mtext  >Â andÂ </mtext></ci><ci >ğ‘™</ci></apply></apply><apply
    ><cn type="integer" >1</cn></apply></apply></matrixrow><matrixrow ><apply  ><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ›¿</ci><list ><ci  >ğ‘˜</ci><apply
    ><ci >ğ‘™</ci><cn type="integer" >1</cn></apply></list></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğ‘¡</ci><ci >ğ‘™</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘¡</ci><apply ><ci >ğ‘™</ci><cn
    type="integer" >1</cn></apply></apply></apply><apply ><apply  ><apply ><ci ><mtext  >ifÂ </mtext></ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘š</ci><list ><ci >ğ‘˜</ci><apply
    ><ci >ğ‘™</ci><cn type="integer" >1</cn></apply></list></apply></apply><apply ><cn
    type="integer" >0</cn><ci ><mtext >Â andÂ </mtext></ci><ci >ğ‘™</ci></apply></apply><apply
    ><cn type="integer" >1</cn></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\delta_{k,l}=\left\{\begin{array}[]{ll}{0,}&{\text{if
    }l=1}\\ {t_{l}-t_{l-1},}&{\text{if }m_{k,l-1}=1\text{ and }l>1}\\ {\delta_{k,l-1}+t_{l}-t_{l-1},}&{\text{if
    }m_{k,l-1}=0\text{ and }l>1}\end{array}\right.</annotation></semantics></math>
    |  |'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: \(\delta_{k,l}=\left\{\begin{array}[]{ll}{0,}&{\text{å¦‚æœ }l=1}\\ {t_{l}-t_{l-1},}&{\text{å¦‚æœ
    }m_{k,l-1}=1\text{ ä¸” }l>1}\\ {\delta_{k,l-1}+t_{l}-t_{l-1},}&{\text{å¦‚æœ }m_{k,l-1}=0\text{
    ä¸” }l>1}\end{array}\right.\)
- en: 'Hence, each incomplete time series is expressed as $\{\mathbf{X}_{i}^{o},\mathbf{M}_{i},\boldsymbol{\delta}_{i}\}$.
    The objective of MTSI is to construct an imputation model $\mathcal{M}_{\theta}$,
    parameterized by $\theta$, to accurately estimate missing values in $\mathbf{X}^{o}$.
    The *imputed* matrix is defined as:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ¯ä¸ªä¸å®Œæ•´çš„æ—¶é—´åºåˆ—è¡¨ç¤ºä¸º$\{\mathbf{X}_{i}^{o},\mathbf{M}_{i},\boldsymbol{\delta}_{i}\}$ã€‚MTSIçš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªç”±$\theta$å‚æ•°åŒ–çš„å¡«è¡¥æ¨¡å‹$\mathcal{M}_{\theta}$ï¼Œä»¥å‡†ç¡®ä¼°è®¡$\mathbf{X}^{o}$ä¸­çš„ç¼ºå¤±å€¼ã€‚*å¡«è¡¥*çŸ©é˜µå®šä¹‰ä¸ºï¼š
- en: '|  | $\mathbf{\hat{X}}=\mathbf{{M}}\odot\mathbf{{X}}^{o}+(1-\mathbf{M})\odot\mathbf{\bar{X}},$
    |  | (1) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\hat{X}}=\mathbf{{M}}\odot\mathbf{{X}}^{o}+(1-\mathbf{M})\odot\mathbf{\bar{X}},$
    |  | (1) |'
- en: 'where $\odot$ denotes element-wise multiplication, and $\mathbf{\bar{X}}=\mathcal{M}_{\theta}(\mathbf{{X}}^{o})$
    is the reconstructed matrix. The aim of $\mathcal{M}_{\theta}$ is twofold: (i)
    to make $\mathbf{\hat{X}}$ approximate the true *complete* data $\mathbf{X}$ as
    closely as possible, or (ii) to enhance the downstream task performance using
    $\mathbf{\hat{X}}$ compared to using the original $\mathbf{X}^{o}$.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\odot$è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ï¼Œè€Œ$\mathbf{\bar{X}}=\mathcal{M}_{\theta}(\mathbf{{X}}^{o})$æ˜¯é‡å»ºçš„çŸ©é˜µã€‚$\mathcal{M}_{\theta}$çš„ç›®æ ‡æœ‰ä¸¤ä¸ªï¼šï¼ˆiï¼‰ä½¿$\mathbf{\hat{X}}$å°½å¯èƒ½æ¥è¿‘çœŸå®çš„*å®Œæ•´*æ•°æ®$\mathbf{X}$ï¼Œæˆ–è€…ï¼ˆiiï¼‰ç›¸æ¯”äºä½¿ç”¨åŸå§‹çš„$\mathbf{X}^{o}$ï¼Œåˆ©ç”¨$\mathbf{\hat{X}}$æ¥æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚
- en: Missing Mechanism
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¼ºå¤±æœºåˆ¶
- en: 'The missing mechanisms, i.e., the cause of missing data, represent the statistical
    relationship between observations and the probability of missing dataÂ Nakagawa
    ([2015](#bib.bib43)). In real-life scenarios, missing mechanisms are inherently
    complex, and the performance of an imputation model is significantly influenced
    by how closely the assumptions we make align with the actual missing data mechanisms.
    According to Robinâ€™s theoryÂ Rubin ([1976](#bib.bib44)), the missing mechanisms
    fall into three categories: Missing Completely At Random (MCAR), Missing At Random
    (MAR), and Missing Not At Random (MNAR). MCAR implies that the probability of
    data being missing is independent of both the observed and missing data. Conversely,
    MAR indicates that the missing mechanism depends solely on the observed data.
    MNAR suggests that the missingness is related to the missing data itself and may
    also be influenced by the observed data. These three mechanisms can be formally
    defined as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼ºå¤±æœºåˆ¶ï¼Œå³ç¼ºå¤±æ•°æ®çš„åŸå› ï¼Œè¡¨ç¤ºè§‚å¯Ÿå€¼ä¸ç¼ºå¤±æ•°æ®æ¦‚ç‡ä¹‹é—´çš„ç»Ÿè®¡å…³ç³» Nakagawa ([2015](#bib.bib43))ã€‚åœ¨ç°å®ç”Ÿæ´»ä¸­ï¼Œç¼ºå¤±æœºåˆ¶æœ¬è´¨ä¸Šæ˜¯å¤æ‚çš„ï¼Œå¡«è¡¥æ¨¡å‹çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—åˆ°æˆ‘ä»¬æ‰€åšçš„å‡è®¾ä¸å®é™…ç¼ºå¤±æ•°æ®æœºåˆ¶çš„åŒ¹é…ç¨‹åº¦çš„å½±å“ã€‚æ ¹æ®Robinçš„ç†è®º
    Rubin ([1976](#bib.bib44))ï¼Œç¼ºå¤±æœºåˆ¶åˆ†ä¸ºä¸‰ç±»ï¼šå®Œå…¨éšæœºç¼ºå¤±ï¼ˆMCARï¼‰ã€éšæœºç¼ºå¤±ï¼ˆMARï¼‰å’Œééšæœºç¼ºå¤±ï¼ˆMNARï¼‰ã€‚MCARæ„å‘³ç€æ•°æ®ç¼ºå¤±çš„æ¦‚ç‡ä¸è§‚å¯Ÿåˆ°çš„æ•°æ®å’Œç¼ºå¤±æ•°æ®éƒ½æ— å…³ã€‚ç›¸åï¼ŒMARè¡¨ç¤ºç¼ºå¤±æœºåˆ¶ä»…ä¾èµ–äºè§‚å¯Ÿæ•°æ®ã€‚MNARåˆ™è¡¨æ˜ç¼ºå¤±æƒ…å†µä¸ç¼ºå¤±æ•°æ®æœ¬èº«ç›¸å…³ï¼Œå¹¶ä¸”å¯èƒ½å—åˆ°è§‚å¯Ÿæ•°æ®çš„å½±å“ã€‚è¿™ä¸‰ç§æœºåˆ¶å¯ä»¥æ­£å¼å®šä¹‰å¦‚ä¸‹ï¼š
- en: â€¢
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'MCAR: $p(\mathbf{M}|\mathbf{X})=p(\mathbf{M})$,'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MCAR: $p(\mathbf{M}|\mathbf{X})=p(\mathbf{M})$ï¼Œ'
- en: â€¢
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'MAR: $p(\mathbf{M}|\mathbf{X})=p(\mathbf{M}|\mathbf{X}^{o})$,'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MAR: $p(\mathbf{M}|\mathbf{X})=p(\mathbf{M}|\mathbf{X}^{o})$ï¼Œ'
- en: â€¢
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'MNAR: $p(\mathbf{M}|\mathbf{X})=p(\mathbf{M}|\mathbf{X}^{o},\mathbf{X}^{m})$.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MNAR: $p(\mathbf{M}|\mathbf{X})=p(\mathbf{M}|\mathbf{X}^{o},\mathbf{X}^{m})$ã€‚'
- en: MCAR and MAR are stronger assumptions compared to MNAR and are considered â€œignorableâ€Â Little
    and Rubin ([2019](#bib.bib29)). This means that the missing mechanism can be disregarded
    during imputation, focusing solely on learning the data distribution, i.e., $p(\mathbf{X}^{o})$.
    In contrast, MNAR, often more reflective of real-life scenarios, is â€œnon-ignorableâ€,
    overlooking its missing mechanism can lead to biased parameter estimates. The
    objective here shifts to learning the joint distribution of the data and its missing
    mechanism, i.e., $p(\mathbf{X}^{o},\mathbf{M})$.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸MNARç›¸æ¯”ï¼ŒMCARå’ŒMARæ˜¯æ›´å¼ºçš„å‡è®¾ï¼Œå¹¶è¢«è®¤ä¸ºæ˜¯â€œå¯å¿½ç•¥çš„â€ Littleå’ŒRubin ([2019](#bib.bib29))ã€‚è¿™æ„å‘³ç€åœ¨å¡«è¡¥è¿‡ç¨‹ä¸­å¯ä»¥å¿½ç•¥ç¼ºå¤±æœºåˆ¶ï¼Œä¸“æ³¨äºå­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œå³$p(\mathbf{X}^{o})$ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMNARæ›´èƒ½åæ˜ ç°å®ç”Ÿæ´»ä¸­çš„åœºæ™¯ï¼Œå®ƒæ˜¯â€œä¸å¯å¿½ç•¥çš„â€ï¼Œå¿½è§†å…¶ç¼ºå¤±æœºåˆ¶å¯èƒ½å¯¼è‡´å‚æ•°ä¼°è®¡åå·®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç›®æ ‡è½¬å‘å­¦ä¹ æ•°æ®åŠå…¶ç¼ºå¤±æœºåˆ¶çš„è”åˆåˆ†å¸ƒï¼Œå³$p(\mathbf{X}^{o},\mathbf{M})$ã€‚
- en: 2.2 Taxonomy of Imputation Methods
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 å¡«è¡¥æ–¹æ³•çš„åˆ†ç±»
- en: '![Refer to caption](img/fdd3bfda9eda9219f38e4865daab181c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§å›¾æ³¨](img/fdd3bfda9eda9219f38e4865daab181c.png)'
- en: 'Figure 1: The taxonomy of deep learning methods for multivariate time series
    imputation from the view of imputation uncertainty and neural network architecture.
    For each category, one representative model is picked to display.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šä»å¡«è¡¥ä¸ç¡®å®šæ€§å’Œç¥ç»ç½‘ç»œæ¶æ„çš„è§’åº¦æ¥çœ‹ï¼Œå¤šå˜é‡æ—¶é—´åºåˆ—å¡«è¡¥æ–¹æ³•çš„åˆ†ç±»ã€‚æ¯ä¸ªç±»åˆ«ä¸­éƒ½é€‰å–äº†ä¸€ä¸ªä»£è¡¨æ¨¡å‹è¿›è¡Œå±•ç¤ºã€‚
- en: '| Method | Venue | Category | Imputation Uncertainty | Neural Network Architecture
    | Missing Mechanism |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | ä¼šè®® | åˆ†ç±» | æ’è¡¥ä¸ç¡®å®šæ€§ | ç¥ç»ç½‘ç»œæ¶æ„ | ç¼ºå¤±æœºåˆ¶ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GRU-DÂ Che et al. ([2018](#bib.bib10)) | Scientific Reports | predictive |
    \faTimes | RNN | MCAR |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| GRU-D Che ç­‰ ([2018](#bib.bib10)) | Scientific Reports | é¢„æµ‹ | \faTimes | RNN
    | MCAR |'
- en: '| M-RNNÂ Yoon et al. ([2019](#bib.bib56)) | TBME | predictive | \faTimes | RNN
    | MCAR |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| M-RNN Yoon ç­‰ ([2019](#bib.bib56)) | TBME | é¢„æµ‹ | \faTimes | RNN | MCAR |'
- en: '| BRITSÂ Cao et al. ([2018](#bib.bib9)) | NeurIPS | predictive | \faTimes |
    RNN | MCAR |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| BRITS Cao ç­‰ ([2018](#bib.bib9)) | NeurIPS | é¢„æµ‹ | \faTimes | RNN | MCAR |'
- en: '| TimesNetÂ Wu et al. ([2023a](#bib.bib53)) | ICLR | predictive | \faTimes |
    CNN | MCAR |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| TimesNet Wu ç­‰ ([2023a](#bib.bib53)) | ICLR | é¢„æµ‹ | \faTimes | CNN | MCAR |'
- en: '| GRINÂ Cini et al. ([2022](#bib.bib12)) | ICLR | predictive | \faTimes | GNN
    | MCAR / MAR |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| GRIN Cini ç­‰ ([2022](#bib.bib12)) | ICLR | é¢„æµ‹ | \faTimes | GNN | MCAR / MAR
    |'
- en: '| SPINÂ Marisca et al. ([2022](#bib.bib38)) | NeurIPS | predictive | \faTimes
    | GNN, Attention | MCAR / MAR |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| SPIN Marisca ç­‰ ([2022](#bib.bib38)) | NeurIPS | é¢„æµ‹ | \faTimes | GNN, Attention
    | MCAR / MAR |'
- en: '| CDSAÂ Ma et al. ([2019](#bib.bib37)) | arXiv | predictive | \faTimes | Attention
    | MCAR |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| CDSA Ma ç­‰ ([2019](#bib.bib37)) | arXiv | é¢„æµ‹ | \faTimes | Attention | MCAR
    |'
- en: '| TransformerÂ Vaswani et al. ([2017](#bib.bib50)) | NeurIPS | predictive |
    \faTimes | Attention | MCAR |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Transformer Vaswani ç­‰ ([2017](#bib.bib50)) | NeurIPS | é¢„æµ‹ | \faTimes | Attention
    | MCAR |'
- en: '| SAITSÂ Du et al. ([2023](#bib.bib13)) | ESWA | predictive | \faTimes | Attention
    | MCAR |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| SAITS Du ç­‰ ([2023](#bib.bib13)) | ESWA | é¢„æµ‹ | \faTimes | Attention | MCAR
    |'
- en: '| DeepMVIÂ Bansal et al. ([2021](#bib.bib6)) | VLDB | predictive | \faTimes
    | Attention, CNN | MCAR |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| DeepMVI Bansal ç­‰ ([2021](#bib.bib6)) | VLDB | é¢„æµ‹ | \faTimes | Attention,
    CNN | MCAR |'
- en: '| NRTSIÂ Shan et al. ([2023](#bib.bib46)) | ICASSP | predictive | \faTimes |
    Attention | MCAR |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| NRTSI Shan ç­‰ ([2023](#bib.bib46)) | ICASSP | é¢„æµ‹ | \faTimes | Attention |
    MCAR |'
- en: '| GP-VAEÂ Fortuin et al. ([2020](#bib.bib17)) | AISTATS | generative | \faCheckCircleO
    | VAE, CNN | MCAR / MAR |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| GP-VAE Fortuin ç­‰ ([2020](#bib.bib17)) | AISTATS | ç”Ÿæˆ | \faCheckCircleO |
    VAE, CNN | MCAR / MAR |'
- en: '| V-RINÂ Mulyadi et al. ([2021](#bib.bib42)) | Trans. Cybern. | generative |
    \faCheck | VAE, RNN | MCAR / MAR |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| V-RIN Mulyadi ç­‰ ([2021](#bib.bib42)) | Trans. Cybern. | ç”Ÿæˆ | \faCheck | VAE,
    RNN | MCAR / MAR |'
- en: '| supnotMIWAEÂ Kim et al. ([2023](#bib.bib26)) | ICML | generative | \faCheckCircleO
    | VAE | MNAR |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| supnotMIWAE Kim ç­‰ ([2023](#bib.bib26)) | ICML | ç”Ÿæˆ | \faCheckCircleO | VAE
    | MNAR |'
- en: '| GRUI-GANÂ Luo et al. ([2018](#bib.bib35)) | NeurIPS | generative | \faCheckCircleO
    | GAN, RNN | MCAR |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| GRUI-GAN Luo ç­‰ ([2018](#bib.bib35)) | NeurIPS | ç”Ÿæˆ | \faCheckCircleO | GAN,
    RNN | MCAR |'
- en: '| EÂ²GANÂ Luo et al. ([2019](#bib.bib36)) | IJCAI | generative | \faCheckCircleO
    | GAN, RNN | MCAR |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| EÂ²GAN Luo ç­‰ ([2019](#bib.bib36)) | IJCAI | ç”Ÿæˆ | \faCheckCircleO | GAN, RNN
    | MCAR |'
- en: '| NAOMIÂ Liu et al. ([2019](#bib.bib30)) | NeurIPS | generative | \faCheckCircleO
    | GAN, RNN | MCAR |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| NAOMI Liu ç­‰ ([2019](#bib.bib30)) | NeurIPS | ç”Ÿæˆ | \faCheckCircleO | GAN,
    RNN | MCAR |'
- en: '| SSGANÂ Miao et al. ([2021](#bib.bib39)) | AAAI | generative | \faCheckCircleO
    | GAN, RNN | MCAR |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| SSGAN Miao ç­‰ ([2021](#bib.bib39)) | AAAI | ç”Ÿæˆ | \faCheckCircleO | GAN, RNN
    | MCAR |'
- en: '| CSDIÂ Tashiro et al. ([2021](#bib.bib48)) | NeurIPS | generative | \faCheckCircleO
    | Diffusion, Attention, CNN | MCAR |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| CSDI Tashiro ç­‰ ([2021](#bib.bib48)) | NeurIPS | ç”Ÿæˆ | \faCheckCircleO | Diffusion,
    Attention, CNN | MCAR |'
- en: '| SSSDÂ Alcaraz and Strodthoff ([2023](#bib.bib1)) | TMLR | generative | \faCheckCircleO
    | Diffusion, Attention | MCAR |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| SSSD Alcaraz å’Œ Strodthoff ([2023](#bib.bib1)) | TMLR | ç”Ÿæˆ | \faCheckCircleO
    | Diffusion, Attention | MCAR |'
- en: '| CSBIÂ Chen et al. ([2023](#bib.bib11)) | ICML | generative | \faCheckCircleO
    | Diffusion, Attention | MCAR |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| CSBI Chen ç­‰ ([2023](#bib.bib11)) | ICML | ç”Ÿæˆ | \faCheckCircleO | Diffusion,
    Attention | MCAR |'
- en: '| MIDMÂ Wang et al. ([2023](#bib.bib51)) | KDD | generative | \faCheckCircleO
    | Diffusion, Attention | MCAR |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| MIDM Wang ç­‰ ([2023](#bib.bib51)) | KDD | ç”Ÿæˆ | \faCheckCircleO | Diffusion,
    Attention | MCAR |'
- en: '| PriSTIÂ Liu et al. ([2023](#bib.bib32)) | ICDE | generative | \faCheckCircleO
    | Diffusion, Attention, GNN, CNN | MCAR |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| PriSTI Liu ç­‰ ([2023](#bib.bib32)) | ICDE | ç”Ÿæˆ | \faCheckCircleO | Diffusion,
    Attention, GNN, CNN | MCAR |'
- en: '| DA-TASWDMÂ Xu et al. ([2023](#bib.bib55)) | CIKM | generative | \faCheck |
    Diffusion, Attention | MCAR |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| DA-TASWDM Xu ç­‰ ([2023](#bib.bib55)) | CIKM | ç”Ÿæˆ | \faCheck | Diffusion, Attention
    | MCAR |'
- en: '| SPDÂ BiloÅ¡ et al. ([2023](#bib.bib8)) | ICML | generative | \faCheckCircleO
    | Diffusion, Attention | MCAR |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| SPD BiloÅ¡ ç­‰ ([2023](#bib.bib8)) | ICML | ç”Ÿæˆ | \faCheckCircleO | Diffusion,
    Attention | MCAR |'
- en: 'Table 1: Summary of deep learning methods for multivariate time series imputation.
    \faCheckÂ and \faCheckCircleOÂ indicate methods capable of accounting for imputation
    uncertainty, whereas \faTimesÂ denotes methods that do not. Furthermore, \faCheckÂ denotes
    that the methods also define the fidelity score to explicitly measure the imputation
    uncertainty.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 1ï¼šå¤šå˜é‡æ—¶é—´åºåˆ—å¡«å……çš„æ·±åº¦å­¦ä¹ æ–¹æ³•æ€»ç»“ã€‚ \faCheck å’Œ \faCheckCircleO è¡¨ç¤ºèƒ½å¤Ÿè€ƒè™‘å¡«å……ä¸ç¡®å®šæ€§çš„æ–¹æ³•ï¼Œè€Œ \faTimes
    è¡¨ç¤ºä¸èƒ½å¤Ÿè€ƒè™‘çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œ\faCheck è¡¨ç¤ºè¿™äº›æ–¹æ³•è¿˜å®šä¹‰äº†ä¿çœŸåº¦è¯„åˆ†ï¼Œä»¥æ˜ç¡®æµ‹é‡å¡«å……ä¸ç¡®å®šæ€§ã€‚
- en: 'To summarize the existing deep multivariate time series imputation methods,
    we propose a taxonomy from the perspectives of imputation uncertainty and neural
    network architecture as illustrated in FigureÂ [1](#S2.F1 "Figure 1 â€£ 2.2 Taxonomy
    of Imputation Methods â€£ 2 Preliminary and Taxonomy â€£ Deep Learning for Multivariate
    Time Series Imputation: A Survey"), and provide a more detailed summary of these
    methods in TableÂ [1](#S2.T1 "Table 1 â€£ 2.2 Taxonomy of Imputation Methods â€£ 2
    Preliminary and Taxonomy â€£ Deep Learning for Multivariate Time Series Imputation:
    A Survey"). For imputation uncertainty, we categorize imputation methods into
    predictive and generative types, based on their ability to yield varied imputations
    that reflect the inherent uncertainty in the imputation process. In the context
    of the neural network architecture, we examine prominent deep learning models
    specifically designed for time series imputation. The discussed models encompass
    RNN-based ones, CNN-based ones, GNN-based ones, attention-based ones, VAE-based
    ones, GAN-based ones, and diffusion-based ones. In the following two sections,
    we will delve into and discuss the existing deep time series imputation methods
    from these two perspectives.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†æ€»ç»“ç°æœ‰çš„æ·±åº¦å¤šå˜é‡æ—¶é—´åºåˆ—å¡«å……æ–¹æ³•ï¼Œæˆ‘ä»¬ä»å¡«å……ä¸ç¡®å®šæ€§å’Œç¥ç»ç½‘ç»œæ¶æ„çš„è§’åº¦æå‡ºäº†ä¸€ä¸ªåˆ†ç±»æ³•ï¼Œå¦‚å›¾Â [1](#S2.F1 "Figure 1 â€£
    2.2 Taxonomy of Imputation Methods â€£ 2 Preliminary and Taxonomy â€£ Deep Learning
    for Multivariate Time Series Imputation: A Survey") æ‰€ç¤ºï¼Œå¹¶åœ¨è¡¨Â [1](#S2.T1 "Table 1
    â€£ 2.2 Taxonomy of Imputation Methods â€£ 2 Preliminary and Taxonomy â€£ Deep Learning
    for Multivariate Time Series Imputation: A Survey") ä¸­æä¾›äº†è¿™äº›æ–¹æ³•çš„æ›´è¯¦ç»†æ€»ç»“ã€‚å¯¹äºå¡«å……ä¸ç¡®å®šæ€§ï¼Œæˆ‘ä»¬å°†å¡«å……æ–¹æ³•åˆ†ä¸ºé¢„æµ‹å‹å’Œç”Ÿæˆå‹ï¼Œæ ¹æ®å®ƒä»¬åœ¨å¡«å……è¿‡ç¨‹ä¸­äº§ç”Ÿåæ˜ å›ºæœ‰ä¸ç¡®å®šæ€§çš„ä¸åŒå¡«å……å€¼çš„èƒ½åŠ›ã€‚åœ¨ç¥ç»ç½‘ç»œæ¶æ„æ–¹é¢ï¼Œæˆ‘ä»¬è€ƒå¯Ÿäº†ä¸“é—¨è®¾è®¡ç”¨äºæ—¶é—´åºåˆ—å¡«å……çš„æ˜¾è‘—æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è®¨è®ºçš„æ¨¡å‹åŒ…æ‹¬åŸºäº
    RNN çš„ã€åŸºäº CNN çš„ã€åŸºäº GNN çš„ã€åŸºäºæ³¨æ„åŠ›çš„ã€åŸºäº VAE çš„ã€åŸºäº GAN çš„å’ŒåŸºäºæ‰©æ•£çš„æ¨¡å‹ã€‚åœ¨æ¥ä¸‹æ¥çš„ä¸¤ä¸ªéƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä»è¿™ä¸¤ä¸ªè§’åº¦æ·±å…¥æ¢è®¨å¹¶è®¨è®ºç°æœ‰çš„æ·±åº¦æ—¶é—´åºåˆ—å¡«å……æ–¹æ³•ã€‚'
- en: 3 Predictive Methods
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 é¢„æµ‹æ–¹æ³•
- en: 'This section delves into predictive imputation methods, and our discussion
    primarily focuses on four types: RNN-based, CNN-based, GNN-based, and attention-based
    models.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚å°†æ·±å…¥æ¢è®¨é¢„æµ‹å¡«å……æ–¹æ³•ï¼Œæˆ‘ä»¬çš„è®¨è®ºä¸»è¦é›†ä¸­åœ¨å››ç§ç±»å‹ä¸Šï¼šåŸºäº RNN çš„ã€åŸºäº CNN çš„ã€åŸºäº GNN çš„ä»¥åŠåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ã€‚
- en: 3.1 Learning Objective
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 å­¦ä¹ ç›®æ ‡
- en: Predictive imputation methods consistently predict deterministic values for
    the same missing components, thereby not accounting for the uncertainty in the
    imputed values. Typically, these methods employ a reconstruction-based learning
    manner with the learning objective being,
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹å¡«å……æ–¹æ³•å§‹ç»ˆä¸ºç›¸åŒçš„ç¼ºå¤±ç»„ä»¶é¢„æµ‹ç¡®å®šæ€§å€¼ï¼Œå› æ­¤ä¸è€ƒè™‘å¡«å……å€¼ä¸­çš„ä¸ç¡®å®šæ€§ã€‚é€šå¸¸ï¼Œè¿™äº›æ–¹æ³•é‡‡ç”¨åŸºäºé‡æ„çš„å­¦ä¹ æ–¹å¼ï¼Œå­¦ä¹ ç›®æ ‡ä¸ºï¼Œ
- en: '|  | $\mathcal{L}_{det}(\theta)=\sum_{i=1}^{N}\ell_{e}(\mathbf{M}_{i}\odot{\mathbf{\bar{X}}_{i}},\mathbf{M}_{i}\odot\mathbf{X}^{o}_{i}),$
    |  | (2) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{det}(\theta)=\sum_{i=1}^{N}\ell_{e}(\mathbf{M}_{i}\odot{\mathbf{\bar{X}}_{i}},\mathbf{M}_{i}\odot\mathbf{X}^{o}_{i}),$
    |  | (2) |'
- en: where $\ell_{e}$ is an absolute or squared error function.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\ell_{e}$ æ˜¯ä¸€ä¸ªç»å¯¹å€¼æˆ–å¹³æ–¹è¯¯å·®å‡½æ•°ã€‚
- en: 3.2 RNN-based Models
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 åŸºäº RNN çš„æ¨¡å‹
- en: As a natural way to model sequential data, Recurrent Neural Networks (RNNs)
    get developed early on the topic of advanced time-series analysis, and imputation
    is not an exception. GRU-DÂ Che et al. ([2018](#bib.bib10)), a variant of GRU,
    is designed to process time series containing missing values. It is regulated
    by a temporal decay mechanism, which takes the time-lag matrix $\mathbf{\delta}_{i}$
    as input and models the temporal irregularity caused by missing values. Temporal
    belief memoryÂ Kim and Chi ([2018](#bib.bib25)), inspired by a biological neural
    model called the Hodgkinâ€“Huxley model, is proposed to handle missing data by computing
    a belief of each featureâ€™s last observation with a bidirectional RNN and imputing
    a missing value based on its according belief. M-RNNÂ Yoon et al. ([2019](#bib.bib56))
    is an RNN variant that works in a multi-directional way. This model interpolates
    within data streams with a bidirectional RNN model and imputes across data streams
    with a fully connected network. BRITSÂ Cao et al. ([2018](#bib.bib9)) models incomplete
    time series with a bidirectional RNN. It takes missing values as variables of
    the RNN graph and fills in missing data with the hidden states from the RNN. In
    addition to imputation, BRITS is capable of working on the time series classification
    task simultaneously. Both M-RNN and BRITS adopt the temporal decay function from
    GRU-D to capture the informative missingness for performance improvement. Subsequent
    works, such asÂ Luo et al. ([2018](#bib.bib35), [2019](#bib.bib36)); Liu et al.
    ([2019](#bib.bib30)); Miao et al. ([2021](#bib.bib39)), combine RNNs with the
    GAN structure to output imputation with higher accuracy.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ç§è‡ªç„¶çš„åºåˆ—æ•°æ®å»ºæ¨¡æ–¹å¼ï¼Œé€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰åœ¨é«˜çº§æ—¶é—´åºåˆ—åˆ†æé¢†åŸŸä¸­æ—©æœŸå¾—åˆ°äº†å‘å±•ï¼Œæ•°æ®æ’è¡¥ä¹Ÿä¸ä¾‹å¤–ã€‚GRU-D Che et al. ([2018](#bib.bib10))
    æ˜¯GRUçš„ä¸€ç§å˜ä½“ï¼Œæ—¨åœ¨å¤„ç†åŒ…å«ç¼ºå¤±å€¼çš„æ—¶é—´åºåˆ—ã€‚å®ƒé€šè¿‡ä¸€ä¸ªæ—¶é—´è¡°å‡æœºåˆ¶è¿›è¡Œè°ƒèŠ‚ï¼Œè¯¥æœºåˆ¶å°†æ—¶é—´æ»åçŸ©é˜µ $\mathbf{\delta}_{i}$ ä½œä¸ºè¾“å…¥ï¼Œå¹¶å»ºæ¨¡ç”±äºç¼ºå¤±å€¼å¼•èµ·çš„æ—¶é—´ä¸è§„åˆ™æ€§ã€‚Temporal
    belief memory Kim and Chi ([2018](#bib.bib25)) å—åˆ°ä¸€ç§ç§°ä¸ºéœå¥‡é‡‘-èµ«å…‹æ–¯åˆ©æ¨¡å‹çš„ç”Ÿç‰©ç¥ç»æ¨¡å‹çš„å¯å‘ï¼Œæå‡ºé€šè¿‡è®¡ç®—æ¯ä¸ªç‰¹å¾æœ€åè§‚æµ‹çš„ä¿¡å¿µå¹¶ä½¿ç”¨åŒå‘RNNå¯¹ç¼ºå¤±å€¼è¿›è¡Œæ’è¡¥ã€‚M-RNN
    Yoon et al. ([2019](#bib.bib56)) æ˜¯ä¸€ç§åœ¨å¤šæ–¹å‘ä¸Šå·¥ä½œçš„RNNå˜ä½“ã€‚è¯¥æ¨¡å‹ä½¿ç”¨åŒå‘RNNæ¨¡å‹åœ¨æ•°æ®æµä¸­è¿›è¡Œæ’å€¼ï¼Œå¹¶é€šè¿‡å…¨è¿æ¥ç½‘ç»œåœ¨æ•°æ®æµä¹‹é—´è¿›è¡Œæ’è¡¥ã€‚BRITS
    Cao et al. ([2018](#bib.bib9)) ä½¿ç”¨åŒå‘RNNå¯¹ä¸å®Œæ•´çš„æ—¶é—´åºåˆ—è¿›è¡Œå»ºæ¨¡ã€‚å®ƒå°†ç¼ºå¤±å€¼ä½œä¸ºRNNå›¾çš„å˜é‡ï¼Œå¹¶åˆ©ç”¨RNNçš„éšè—çŠ¶æ€å¡«è¡¥ç¼ºå¤±æ•°æ®ã€‚é™¤äº†æ’è¡¥ï¼ŒBRITSè¿˜èƒ½åŒæ—¶è¿›è¡Œæ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ã€‚M-RNNå’ŒBRITSéƒ½é‡‡ç”¨äº†GRU-Dçš„æ—¶é—´è¡°å‡å‡½æ•°ï¼Œä»¥æ•è·ä¿¡æ¯æ€§ç¼ºå¤±ä»¥æå‡æ€§èƒ½ã€‚åç»­å·¥ä½œï¼Œå¦‚Luo
    et al. ([2018](#bib.bib35), [2019](#bib.bib36)); Liu et al. ([2019](#bib.bib30));
    Miao et al. ([2021](#bib.bib39))ï¼Œå°†RNNä¸GANç»“æ„ç›¸ç»“åˆï¼Œä»¥æ›´é«˜çš„ç²¾åº¦è¾“å‡ºæ’è¡¥ç»“æœã€‚
- en: 3.3 CNN-based Models
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 åŸºäºCNNçš„æ¨¡å‹
- en: Convolutional Neural Networks (CNNs) represent a foundational deep learning
    architecture, extensively employed in sophisticated time series analysis. TimesNetÂ Wu
    et al. ([2023a](#bib.bib53)) innovatively incorporates Fast Fourier Transform
    to restructure 1D time series into a 2D format, facilitating the utilization of
    CNNs for data processing. Also in GP-VAEÂ Fortuin et al. ([2020](#bib.bib17)),
    CNNs play the role of the backbone in both the encoder and decoder. Furthermore,
    CNNs serve as pivotal feature extractors within attention-based models like DeepMVIÂ Bansal
    et al. ([2021](#bib.bib6)), as well as in diffusion-based models such as CSDIÂ Tashiro
    et al. ([2021](#bib.bib48)), by mapping input data into an embedding space for
    subsequent processing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ä»£è¡¨äº†æ·±åº¦å­¦ä¹ ä¸­çš„ä¸€ç§åŸºç¡€æ¶æ„ï¼Œå¹¿æ³›åº”ç”¨äºå¤æ‚çš„æ—¶é—´åºåˆ—åˆ†æä¸­ã€‚TimesNet Wu et al. ([2023a](#bib.bib53))
    åˆ›æ–°æ€§åœ°ç»“åˆäº†å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼Œå°†ä¸€ç»´æ—¶é—´åºåˆ—é‡æ„ä¸ºäºŒç»´æ ¼å¼ï¼Œä»è€Œä¾¿äºåˆ©ç”¨CNNè¿›è¡Œæ•°æ®å¤„ç†ã€‚åœ¨GP-VAE Fortuin et al. ([2020](#bib.bib17))ä¸­ï¼ŒCNNåœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­éƒ½å‘æŒ¥äº†éª¨å¹²çš„ä½œç”¨ã€‚æ­¤å¤–ï¼ŒCNNè¿˜åœ¨åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹å¦‚DeepMVI
    Bansal et al. ([2021](#bib.bib6))ä»¥åŠæ‰©æ•£æ¨¡å‹å¦‚CSDI Tashiro et al. ([2021](#bib.bib48))ä¸­ä½œä¸ºå…³é”®çš„ç‰¹å¾æå–å™¨ï¼Œé€šè¿‡å°†è¾“å…¥æ•°æ®æ˜ å°„åˆ°åµŒå…¥ç©ºé—´ä»¥ä¾¿åç»­å¤„ç†ã€‚
- en: 3.4 GNN-based Models
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 åŸºäºGNNçš„æ¨¡å‹
- en: GNN-based models, treating time series as graph sequences, reconstruct missing
    values using learned node representations. The authors in Cini et al. ([2022](#bib.bib12))
    introduce GRIN, the first graph-based recurrent architecture for MTSI. GRIN utilizes
    a bidirectional graph recurrent neural network to effectively harness both temporal
    dynamics and spatial similarities, thereby achieving significant improvements
    in imputation accuracy. Furthermore, SPIN Marisca et al. ([2022](#bib.bib38))
    is developed, integrating a unique sparse spatiotemporal attention mechanism into
    the GNN framework. This mechanism notably overcomes the error propagation issue
    of GRIN and bolsters robustness against the data sparsity presented by highly
    missing data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºGNNçš„æ¨¡å‹å°†æ—¶é—´åºåˆ—è§†ä¸ºå›¾åºåˆ—ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„èŠ‚ç‚¹è¡¨ç¤ºæ¥é‡å»ºç¼ºå¤±å€¼ã€‚Cini et al. ([2022](#bib.bib12))ä¸­çš„ä½œè€…ä»‹ç»äº†GRINï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºMTSIçš„å›¾åŸºé€’å½’æ¶æ„ã€‚GRINåˆ©ç”¨åŒå‘å›¾é€’å½’ç¥ç»ç½‘ç»œæœ‰æ•ˆåœ°åˆ©ç”¨äº†æ—¶é—´åŠ¨æ€å’Œç©ºé—´ç›¸ä¼¼æ€§ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†å¡«è¡¥ç²¾åº¦ã€‚æ­¤å¤–ï¼ŒSPIN
    Marisca et al. ([2022](#bib.bib38))è¢«å¼€å‘å‡ºæ¥ï¼Œå°†ç‹¬ç‰¹çš„ç¨€ç–æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶æ•´åˆåˆ°GNNæ¡†æ¶ä¸­ã€‚è¯¥æœºåˆ¶æ˜¾è‘—å…‹æœäº†GRINçš„è¯¯å·®ä¼ æ’­é—®é¢˜ï¼Œå¹¶å¢å¼ºäº†å¯¹é«˜åº¦ç¼ºå¤±æ•°æ®æ‰€å‘ˆç°çš„æ•°æ®ç¨€ç–æ€§çš„é²æ£’æ€§ã€‚
- en: 3.5 Attention-based Models
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹
- en: 'Since Transformer is proposed inÂ Vaswani et al. ([2017](#bib.bib50)), the self-attention
    mechanism has been widely used to model sequence data including time seriesÂ Wen
    et al. ([2023](#bib.bib52)). CDSAÂ Ma et al. ([2019](#bib.bib37)) is proposed to
    impute geo-tagged spatiotemporal data by learning from time, location, and measurement
    jointly. DeepMVIÂ Bansal et al. ([2021](#bib.bib6)) integrates transformers with
    convolutional techniques, tailoring key-query designs to effectively address missing
    value imputation. For each time series, DeepMVI harnesses attention mechanisms
    to concurrently distill long-term seasonal, granular local, and cross-dimensional
    embeddings, which are concatenated to predict the final output. NRTSIÂ Shan et
    al. ([2023](#bib.bib46)) directly leverages a Transformer encoder for modeling
    and takes time series data as a set of timestamp and measurement tuples. As a
    permutation model, this model has to iterate over the time dimension to process
    time series. SAITSÂ Du et al. ([2023](#bib.bib13)) employs a self-supervised training
    scheme to deal with missing data, which integrates dual joint learning tasks:
    a masked imputation task and an observed reconstruction task. This method, featuring
    two diagonal-masked self-attention blocks and a weighted-combination block, leverages
    attention weights and missingness indicators to enhance imputation precision.
    In addition to the above models, the attention mechanism is also widely adapted
    to build the denoising network in diffusion models like CSDIÂ Tashiro et al. ([2021](#bib.bib48)),
    MIDMÂ Wang et al. ([2023](#bib.bib51)), PriSTIÂ Liu et al. ([2023](#bib.bib32)),
    etc.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä»Transformeråœ¨Vaswani et al. ([2017](#bib.bib50))ä¸­æå‡ºä»¥æ¥ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶å·²è¢«å¹¿æ³›åº”ç”¨äºå»ºæ¨¡åºåˆ—æ•°æ®ï¼ŒåŒ…æ‹¬æ—¶é—´åºåˆ—Â Wen
    et al. ([2023](#bib.bib52))ã€‚CDSAÂ Ma et al. ([2019](#bib.bib37))æ—¨åœ¨é€šè¿‡å…±åŒå­¦ä¹ æ—¶é—´ã€ä½ç½®å’Œæµ‹é‡æ¥å¡«è¡¥åœ°ç†æ ‡è®°çš„æ—¶ç©ºæ•°æ®ã€‚DeepMVIÂ Bansal
    et al. ([2021](#bib.bib6))å°†transformerä¸å·ç§¯æŠ€æœ¯ç›¸ç»“åˆï¼Œè°ƒæ•´å…³é”®-æŸ¥è¯¢è®¾è®¡ä»¥æœ‰æ•ˆè§£å†³ç¼ºå¤±å€¼å¡«è¡¥é—®é¢˜ã€‚å¯¹äºæ¯ä¸ªæ—¶é—´åºåˆ—ï¼ŒDeepMVIåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶åŒæ—¶æå–é•¿æœŸå­£èŠ‚æ€§ã€ç»†ç²’åº¦æœ¬åœ°å’Œè·¨ç»´åº¦çš„åµŒå…¥ï¼Œè¿™äº›åµŒå…¥è¢«è¿æ¥èµ·æ¥ä»¥é¢„æµ‹æœ€ç»ˆè¾“å‡ºã€‚NRTSIÂ Shan
    et al. ([2023](#bib.bib46))ç›´æ¥åˆ©ç”¨Transformerç¼–ç å™¨è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å°†æ—¶é—´åºåˆ—æ•°æ®è§†ä¸ºæ—¶é—´æˆ³å’Œæµ‹é‡å€¼çš„é›†åˆã€‚ä½œä¸ºä¸€ä¸ªæ’åˆ—æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¿…é¡»åœ¨æ—¶é—´ç»´åº¦ä¸Šè¿›è¡Œè¿­ä»£ä»¥å¤„ç†æ—¶é—´åºåˆ—ã€‚SAITSÂ Du
    et al. ([2023](#bib.bib13))é‡‡ç”¨è‡ªç›‘ç£è®­ç»ƒæ–¹æ¡ˆæ¥å¤„ç†ç¼ºå¤±æ•°æ®ï¼Œç»“åˆäº†åŒé‡è”åˆå­¦ä¹ ä»»åŠ¡ï¼šæ©è”½å¡«è¡¥ä»»åŠ¡å’Œè§‚å¯Ÿé‡å»ºä»»åŠ¡ã€‚è¯¥æ–¹æ³•å…·æœ‰ä¸¤ä¸ªå¯¹è§’æ©è”½è‡ªæ³¨æ„åŠ›å—å’Œä¸€ä¸ªåŠ æƒç»„åˆå—ï¼Œåˆ©ç”¨æ³¨æ„åŠ›æƒé‡å’Œç¼ºå¤±æŒ‡ç¤ºç¬¦æ¥æé«˜å¡«è¡¥ç²¾åº¦ã€‚é™¤äº†ä¸Šè¿°æ¨¡å‹ï¼Œæ³¨æ„åŠ›æœºåˆ¶è¿˜è¢«å¹¿æ³›åº”ç”¨äºæ„å»ºæ‰©æ•£æ¨¡å‹ä¸­çš„å»å™ªç½‘ç»œï¼Œå¦‚CSDIÂ Tashiro
    et al. ([2021](#bib.bib48))ã€MIDMÂ Wang et al. ([2023](#bib.bib51))ã€PriSTIÂ Liu et
    al. ([2023](#bib.bib32))ç­‰ã€‚
- en: 3.6 Pros and Cons
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 ä¼˜ç¼ºç‚¹
- en: This subsection synthesizes the strengths and challenges of the predictive imputation
    methods discussed. RNN-based models, while adept at capturing sequential information,
    are inherently limited by their sequential processing nature and memory constraints,
    which may lead to scalability issues with long sequencesÂ Khayati et al. ([2020](#bib.bib24)).
    Although CNNs have decades of development and are useful feature extractors to
    capture neighborhood information and local connectivity, their kernel size and
    working mechanism intrinsically limit their performance on time series data as
    the backbone. Due to the attention mechanism, attention-based models generally
    outperform RNN-based and CNN-based methods in imputation tasks due to their superior
    ability to handle long-range dependencies and parallel processing capabilities.
    GNN-based methods provide a deeper understanding of spatio-temporal dynamics,
    yet they often come with increased computational complexity, posing challenges
    for large-scale or high-dimensional data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬å°èŠ‚ç»¼åˆäº†è®¨è®ºçš„é¢„æµ‹æ’è¡¥æ–¹æ³•çš„ä¼˜ç‚¹å’ŒæŒ‘æˆ˜ã€‚è™½ç„¶ RNN åŸºäºæ¨¡å‹èƒ½å¤Ÿæ•æ‰åºåˆ—ä¿¡æ¯ï¼Œä½†å…¶å›ºæœ‰çš„é¡ºåºå¤„ç†ç‰¹æ€§å’Œè®°å¿†é™åˆ¶ä½¿å…¶åœ¨å¤„ç†é•¿åºåˆ—æ—¶å¯èƒ½å‡ºç°æ‰©å±•æ€§é—®é¢˜Â Khayati
    et al. ([2020](#bib.bib24))ã€‚å°½ç®¡ CNNs å·²æœ‰å‡ åå¹´çš„å‘å±•ï¼Œå¹¶ä¸”æ˜¯æœ‰æ•ˆçš„ç‰¹å¾æå–å™¨ï¼Œå¯ä»¥æ•æ‰é‚»åŸŸä¿¡æ¯å’Œå±€éƒ¨è¿æ¥ï¼Œä½†å…¶å·ç§¯æ ¸å¤§å°å’Œå·¥ä½œæœºåˆ¶å›ºæœ‰åœ°é™åˆ¶äº†å…¶åœ¨æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„æ€§èƒ½ã€‚ç”±äºæ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹åœ¨æ’è¡¥ä»»åŠ¡ä¸­é€šå¸¸ä¼˜äºåŸºäº
    RNN å’Œ CNN çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒä»¬åœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–å’Œå¹¶è¡Œå¤„ç†èƒ½åŠ›æ–¹é¢è¡¨ç°æ›´ä½³ã€‚åŸºäº GNN çš„æ–¹æ³•æä¾›äº†å¯¹æ—¶ç©ºåŠ¨æ€çš„æ›´æ·±ç†è§£ï¼Œä½†é€šå¸¸ä¼´éšç€æ›´é«˜çš„è®¡ç®—å¤æ‚åº¦ï¼Œè¿™å¯¹å¤§è§„æ¨¡æˆ–é«˜ç»´æ•°æ®æå‡ºäº†æŒ‘æˆ˜ã€‚
- en: 4 Generative Methods
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 ç”Ÿæˆæ–¹æ³•
- en: 'In this section, we examine generative imputation methods, including three
    primary types: VAE-based, GAN-based, and diffusion-based models.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨ç”Ÿæˆæ’è¡¥æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸‰ç§ä¸»è¦ç±»å‹ï¼šåŸºäº VAE çš„ã€åŸºäº GAN çš„å’ŒåŸºäºæ‰©æ•£çš„æ¨¡å‹ã€‚
- en: 4.1 Learning Objective
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 å­¦ä¹ ç›®æ ‡
- en: Generative methods are essentially built upon generative models like VAEs, GANs,
    and diffusion models. They are characterized by their ability to generate varied
    outputs for missing observations, enabling the quantification of imputation uncertainty.
    Typically, these methods learn probability distributions from the observed data
    and subsequently generate slightly different values aligned with these learned
    distributions for the missing observation. The primary learning objective of generative
    methods is thus defined as,
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ–¹æ³•æœ¬è´¨ä¸Šå»ºç«‹åœ¨ç”Ÿæˆæ¨¡å‹å¦‚ VAEsã€GANs å’Œæ‰©æ•£æ¨¡å‹ä¹‹ä¸Šã€‚å®ƒä»¬çš„ç‰¹ç‚¹æ˜¯èƒ½å¤Ÿä¸ºç¼ºå¤±è§‚æµ‹ç”Ÿæˆå¤šæ ·åŒ–çš„è¾“å‡ºï¼Œä»è€Œå®ç°æ’è¡¥ä¸ç¡®å®šæ€§çš„é‡åŒ–ã€‚é€šå¸¸ï¼Œè¿™äº›æ–¹æ³•ä»è§‚å¯Ÿåˆ°çš„æ•°æ®ä¸­å­¦ä¹ æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶éšåä¸ºç¼ºå¤±è§‚æµ‹ç”Ÿæˆä¸è¿™äº›å­¦ä¹ åˆ°çš„åˆ†å¸ƒä¸€è‡´çš„ç¨å¾®ä¸åŒçš„å€¼ã€‚å› æ­¤ï¼Œç”Ÿæˆæ–¹æ³•çš„ä¸»è¦å­¦ä¹ ç›®æ ‡å®šä¹‰ä¸ºï¼š
- en: '|  | $\mathcal{L}_{pro}(\theta)=\sum_{i=1}^{N}\log p_{\theta}(\mathbf{{X}}^{o}_{i}).$
    |  | (3) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{pro}(\theta)=\sum_{i=1}^{N}\log p_{\theta}(\mathbf{{X}}^{o}_{i}).$
    |  | (3) |'
- en: where $\theta$ is the model parameters of the imputation model $\mathcal{M}$.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\theta$ æ˜¯æ’è¡¥æ¨¡å‹ $\mathcal{M}$ çš„æ¨¡å‹å‚æ•°ã€‚
- en: 4.2 VAE-based Models
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 åŸºäº VAE çš„æ¨¡å‹
- en: VAEs employ an encoder-decoder structure to approximate the true data distribution
    by maximizing the Evidence Lower Bound (ELBO) on the marginal likelihood. This
    ELBO enforces a Gaussian-distributed latent space from which the decoder reconstructs
    diverse data points
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œé€šè¿‡æœ€å¤§åŒ–è¾¹é™…ä¼¼ç„¶ä¸‹ç•Œï¼ˆELBOï¼‰æ¥è¿‘ä¼¼çœŸå®æ•°æ®åˆ†å¸ƒã€‚è¿™ä¸ª ELBO å¼ºåˆ¶ä»ä¸­è§£ç å™¨é‡å»ºå¤šæ ·çš„æ•°æ®ç‚¹çš„æ½œåœ¨ç©ºé—´æœä»é«˜æ–¯åˆ†å¸ƒã€‚
- en: The authors inÂ Fortuin et al. ([2020](#bib.bib17)) propose the first VAE-based
    imputation method GP-VAE, where they utilized a Gaussian process prior in the
    latent space to capture temporal dynamics. Moreover, the ELBO in GP-VAE is only
    evaluated on the observed features of the data. Authors inÂ Mulyadi et al. ([2021](#bib.bib42))
    design V-RIN to mitigate the risk of biased estimates in missing value imputation.
    V-RIN captures uncertainty by accommodating a Gaussian distribution over the model
    output, specifically interpreting the variance of the reconstructed data from
    a VAE model as an uncertainty measure. It then models temporal dynamics and seamlessly
    integrates this uncertainty into the imputed data through an uncertainty-aware
    GRU. More recently, authors in Â Kim et al. ([2023](#bib.bib26)) propose supnotMIWAE
    and introduce an extra classifier, where they extend the ELBO in GP-VAE to model
    the joint distribution of the observed data, its mask matrix, and its label. In
    this way, their ELBO effectively models the imputation uncertainty, and the additional
    classifier encourages the VAE model to produce missing values that are more advantageous
    for the downstream classification task.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Fortuin ç­‰äººï¼ˆ[2020](#bib.bib17)ï¼‰æå‡ºäº†é¦–ä¸ªåŸºäº VAE çš„æ’è¡¥æ–¹æ³• GP-VAEï¼Œå…¶ä¸­ä»–ä»¬åœ¨æ½œåœ¨ç©ºé—´ä¸­åˆ©ç”¨äº†é«˜æ–¯è¿‡ç¨‹å…ˆéªŒä»¥æ•æ‰æ—¶é—´åŠ¨æ€ã€‚æ­¤å¤–ï¼ŒGP-VAE
    ä¸­çš„ ELBO ä»…åœ¨æ•°æ®çš„è§‚å¯Ÿç‰¹å¾ä¸Šè¿›è¡Œè¯„ä¼°ã€‚Mulyadi ç­‰äººï¼ˆ[2021](#bib.bib42)ï¼‰è®¾è®¡äº† V-RIN æ¥å‡è½»ç¼ºå¤±å€¼æ’è¡¥ä¸­çš„åå·®ä¼°è®¡é£é™©ã€‚V-RIN
    é€šè¿‡å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œé«˜æ–¯åˆ†å¸ƒå»ºæ¨¡æ¥æ•æ‰ä¸ç¡®å®šæ€§ï¼Œå…·ä½“æ¥è¯´ï¼Œå°† VAE æ¨¡å‹é‡å»ºæ•°æ®çš„æ–¹å·®è§£é‡Šä¸ºä¸ç¡®å®šæ€§åº¦é‡ã€‚ç„¶åï¼Œå®ƒå»ºæ¨¡æ—¶é—´åŠ¨æ€ï¼Œå¹¶é€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥ GRU
    æ— ç¼åœ°å°†è¿™ç§ä¸ç¡®å®šæ€§é›†æˆåˆ°æ’è¡¥æ•°æ®ä¸­ã€‚æœ€è¿‘ï¼ŒKim ç­‰äººï¼ˆ[2023](#bib.bib26)ï¼‰æå‡ºäº† supnotMIWAE å¹¶å¼•å…¥äº†é¢å¤–çš„åˆ†ç±»å™¨ï¼Œä»–ä»¬å°†
    GP-VAE ä¸­çš„ ELBO æ‰©å±•åˆ°å»ºæ¨¡è§‚å¯Ÿæ•°æ®ã€å…¶æ©ç çŸ©é˜µå’Œå…¶æ ‡ç­¾çš„è”åˆåˆ†å¸ƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä»–ä»¬çš„ ELBO æœ‰æ•ˆåœ°å»ºæ¨¡äº†æ’è¡¥ä¸ç¡®å®šæ€§ï¼Œé¢å¤–çš„åˆ†ç±»å™¨é¼“åŠ±
    VAE æ¨¡å‹ç”Ÿæˆå¯¹ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡æ›´æœ‰åˆ©çš„ç¼ºå¤±å€¼ã€‚
- en: 4.3 GAN-based Models
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 åŸºäº GAN çš„æ¨¡å‹
- en: 'GANs facilitate adversarial training through a minimax game between two components:
    a generator aiming to mimic the real data distribution, and a discriminator tasked
    with distinguishing between the generated and real data. This dynamic fosters
    a progressive refinement of synthetic data that increasingly resembles real samples.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: GAN é€šè¿‡ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¹‹é—´çš„æå°æå¤§åšå¼ˆä¿ƒè¿›å¯¹æŠ—è®­ç»ƒï¼šç”Ÿæˆå™¨æ—¨åœ¨æ¨¡æ‹ŸçœŸå®æ•°æ®åˆ†å¸ƒï¼Œåˆ¤åˆ«å™¨è´Ÿè´£åŒºåˆ†ç”Ÿæˆæ•°æ®å’ŒçœŸå®æ•°æ®ã€‚è¿™ç§åŠ¨æ€ä¿ƒè¿›äº†åˆæˆæ•°æ®çš„é€æ­¥å®Œå–„ï¼Œä½¿å…¶è¶Šæ¥è¶Šç±»ä¼¼äºçœŸå®æ ·æœ¬ã€‚
- en: InÂ Luo et al. ([2018](#bib.bib35)), authors propose a two-stage GAN imputation
    method (GRUI-GAN), which is the first GAN-based method for imputing time series
    data. GRUI-GAN first learns the distribution of the observed multivariate time
    series data by a standard adversarial training manner, and then optimizes the
    input noise of the generator to further maximize the similarity of the generated
    and observed multivariate time series data. However, the second stage in GRUI-GAN
    needs a lot of time to find the best matched input vector, and this vector is
    not always the best especially when the initial value of the â€œnoiseâ€ is not properly
    set. Then, an end-to-end GAN imputation model $E^{2}$GANÂ Luo et al. ([2019](#bib.bib36))
    is further proposed, where the generator takes a denoising autoencoder module
    to avoid the â€œnoiseâ€ optimization stage in GRUI-GAN. Meanwhile, authors in Â Liu
    et al. ([2019](#bib.bib30)) propose a non-autoregressive multi-resolution GAN
    model (NAOMI), where the generator is assembled by a forward-backward encoder
    and a multiresolution decoder. The imputed data are recursively generated by the
    multiresolution decoder in a non-autoregressive manner, which mitigates error
    accumulation in scenarios involving high-missing and long sequence time series
    data. On the other hand, inÂ Miao et al. ([2021](#bib.bib39)), authors propose
    USGAN, which generates high-quality imputed data by integrating a discriminator
    with a temporal reminder matrix. This matrix introduces added complexity to the
    training of the discriminator and subsequently leads to improvements in the generatorâ€™s
    performance. Furthermore, they extend USGAN to a semi-supervised model SSGAN,
    by introducing an extra classifier. In this way, SSGAN takes advantage of the
    label information, so that the generator can estimate the missing values, conditioned
    on observed components and data labels at the same time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Luo ç­‰äºº ([2018](#bib.bib35)) çš„ç ”ç©¶ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„ GAN å¡«è¡¥æ–¹æ³•ï¼ˆGRUI-GANï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ç§åŸºäº GAN
    çš„æ—¶é—´åºåˆ—æ•°æ®å¡«è¡¥æ–¹æ³•ã€‚GRUI-GAN é¦–å…ˆé€šè¿‡æ ‡å‡†å¯¹æŠ—è®­ç»ƒæ–¹å¼å­¦ä¹ è§‚å¯Ÿåˆ°çš„å¤šå˜é‡æ—¶é—´åºåˆ—æ•°æ®çš„åˆ†å¸ƒï¼Œç„¶åä¼˜åŒ–ç”Ÿæˆå™¨çš„è¾“å…¥å™ªå£°ï¼Œä»¥è¿›ä¸€æ­¥æœ€å¤§åŒ–ç”Ÿæˆçš„å’Œè§‚å¯Ÿåˆ°çš„å¤šå˜é‡æ—¶é—´åºåˆ—æ•°æ®ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ç„¶è€Œï¼ŒGRUI-GAN
    çš„ç¬¬äºŒé˜¶æ®µéœ€è¦å¤§é‡æ—¶é—´æ¥æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„è¾“å…¥å‘é‡ï¼Œå¹¶ä¸”å½“â€œå™ªå£°â€çš„åˆå§‹å€¼è®¾ç½®ä¸å½“æ—¶ï¼Œè¿™ä¸ªå‘é‡ä¸ä¸€å®šæ€»æ˜¯æœ€ä½³çš„ã€‚éšåï¼Œæå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„ GAN å¡«è¡¥æ¨¡å‹ $E^{2}$GAN
    Luo ç­‰äºº ([2019](#bib.bib36))ï¼Œå…¶ä¸­ç”Ÿæˆå™¨é‡‡ç”¨å»å™ªè‡ªç¼–ç å™¨æ¨¡å—ï¼Œä»¥é¿å… GRUI-GAN ä¸­çš„â€œå™ªå£°â€ä¼˜åŒ–é˜¶æ®µã€‚åŒæ—¶ï¼ŒLiu ç­‰äºº
    ([2019](#bib.bib30)) æå‡ºäº†ä¸€ä¸ªéè‡ªå›å½’å¤šåˆ†è¾¨ç‡ GAN æ¨¡å‹ï¼ˆNAOMIï¼‰ï¼Œå…¶ä¸­ç”Ÿæˆå™¨ç”±å‰å‘-åå‘ç¼–ç å™¨å’Œå¤šåˆ†è¾¨ç‡è§£ç å™¨ç»„æˆã€‚å¡«è¡¥çš„æ•°æ®ç”±å¤šåˆ†è¾¨ç‡è§£ç å™¨ä»¥éè‡ªå›å½’æ–¹å¼é€’å½’ç”Ÿæˆï¼Œè¿™åœ¨æ¶‰åŠé«˜ç¼ºå¤±å’Œé•¿åºåˆ—æ—¶é—´åºåˆ—æ•°æ®çš„åœºæ™¯ä¸­å‡è½»äº†è¯¯å·®ç´¯ç§¯ã€‚å¦ä¸€æ–¹é¢ï¼Œåœ¨
    Miao ç­‰äºº ([2021](#bib.bib39)) çš„ç ”ç©¶ä¸­ï¼Œä½œè€…æå‡ºäº† USGANï¼Œé€šè¿‡å°†åˆ¤åˆ«å™¨ä¸æ—¶é—´æé†’çŸ©é˜µç›¸ç»“åˆæ¥ç”Ÿæˆé«˜è´¨é‡çš„å¡«è¡¥æ•°æ®ã€‚è¯¥çŸ©é˜µä¸ºåˆ¤åˆ«å™¨çš„è®­ç»ƒå¼•å…¥äº†é¢å¤–çš„å¤æ‚æ€§ï¼Œè¿›è€Œæé«˜äº†ç”Ÿæˆå™¨çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä»–ä»¬å°†
    USGAN æ‰©å±•ä¸ºåŠç›‘ç£æ¨¡å‹ SSGANï¼Œé€šè¿‡å¼•å…¥é¢å¤–çš„åˆ†ç±»å™¨ã€‚è¿™æ ·ï¼ŒSSGAN åˆ©ç”¨æ ‡ç­¾ä¿¡æ¯ï¼Œä½¿ç”Ÿæˆå™¨èƒ½å¤Ÿåœ¨è§‚å¯Ÿåˆ°çš„ç»„ä»¶å’Œæ•°æ®æ ‡ç­¾çš„æ¡ä»¶ä¸‹ä¼°è®¡ç¼ºå¤±å€¼ã€‚
- en: 4.4 Diffusion-based Models
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 åŸºäºæ‰©æ•£çš„æ¨¡å‹
- en: As an emerging and potent category of generative models, diffusion models are
    adept at capturing complex data distributions by progressively adding and then
    reversing noise through a Markov chain of diffusion steps. Distinct from VAE,
    these models utilize a fixed training procedure and operate with high-dimensional
    latent variables that retain the dimensionality of the input data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ç§æ–°å…´ä¸”å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ç±»åˆ«ï¼Œæ‰©æ•£æ¨¡å‹æ“…é•¿é€šè¿‡é€æ­¥æ·»åŠ å’Œåè½¬å™ªå£°æ¥æ•æ‰å¤æ‚çš„æ•°æ®åˆ†å¸ƒï¼Œè¿™ä¸€è¿‡ç¨‹é€šè¿‡æ‰©æ•£æ­¥éª¤çš„é©¬å°”ç§‘å¤«é“¾è¿›è¡Œã€‚ä¸VAEä¸åŒï¼Œè¿™äº›æ¨¡å‹é‡‡ç”¨å›ºå®šçš„è®­ç»ƒç¨‹åºï¼Œå¹¶ä¸”ä½¿ç”¨ä¿ç•™è¾“å…¥æ•°æ®ç»´åº¦çš„é«˜ç»´æ½œå˜é‡ã€‚
- en: CSDI, introduced inÂ Tashiro et al. ([2021](#bib.bib48)), stands out as the pioneering
    diffusion model specifically designed for MTSI. Different from conventional diffusion
    models, CSDI adopts a conditioned training approach, where a subset of observed
    data is utilized as conditional information to facilitate the generation of the
    remaining segment of observed data. However, the denoising network in CSDI relies
    on two transformers, exhibiting quadratic complexity concerning the number of
    variables and the time series length. This design limitation raises concerns about
    memory constraints, particularly when modeling extensive multivariate time series.
    In response to this challenge, a subsequent work byÂ Alcaraz and Strodthoff ([2023](#bib.bib1))
    introduces SSSD, which addresses the quadratic complexity issue by replacing transformers
    with structured state space modelsÂ Gu et al. ([2022](#bib.bib19)). This modification
    proves advantageous, especially when handling lengthy multivariate time series,
    as it mitigates the risk of memory overflow. Another approach CSBI, introduced
    in Â Chen et al. ([2023](#bib.bib11)), improves the efficiency by modeling the
    diffusion process as a Schrodinger bridge problem, which could be transformed
    into computation-friendly stochastic differential equations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: CSDIï¼Œç”±Tashiroç­‰äººï¼ˆ[2021](#bib.bib48)ï¼‰ä»‹ç»ï¼Œä½œä¸ºä¸“é—¨ä¸ºMTSIè®¾è®¡çš„å¼€åˆ›æ€§æ‰©æ•£æ¨¡å‹è„±é¢–è€Œå‡ºã€‚ä¸ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä¸åŒï¼ŒCSDIé‡‡ç”¨äº†æ¡ä»¶è®­ç»ƒæ–¹æ³•ï¼Œå…¶ä¸­ä½¿ç”¨è§‚å¯Ÿæ•°æ®çš„å­é›†ä½œä¸ºæ¡ä»¶ä¿¡æ¯ï¼Œä»¥ä¾¿ç”Ÿæˆå…¶ä½™è§‚å¯Ÿæ•°æ®ã€‚ç„¶è€Œï¼ŒCSDIä¸­çš„å»å™ªç½‘ç»œä¾èµ–äºä¸¤ä¸ªtransformersï¼Œåœ¨å˜é‡æ•°é‡å’Œæ—¶é—´åºåˆ—é•¿åº¦æ–¹é¢å‘ˆç°å‡ºäºŒæ¬¡å¤æ‚åº¦ã€‚è¿™ä¸€è®¾è®¡é™åˆ¶å¼•å‘äº†å¯¹å†…å­˜é™åˆ¶çš„æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯åœ¨å»ºæ¨¡å¹¿æ³›çš„å¤šå˜é‡æ—¶é—´åºåˆ—æ—¶ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼ŒAlcarazå’ŒStrodthoffï¼ˆ[2023](#bib.bib1)ï¼‰çš„åç»­å·¥ä½œå¼•å…¥äº†SSSDï¼Œé€šè¿‡ç”¨ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹Guç­‰äººï¼ˆ[2022](#bib.bib19)ï¼‰æ›¿ä»£transformersï¼Œä»è€Œè§£å†³äº†äºŒæ¬¡å¤æ‚åº¦é—®é¢˜ã€‚è¿™ä¸€ä¿®æ”¹å°¤å…¶åœ¨å¤„ç†è¾ƒé•¿çš„å¤šå˜é‡æ—¶é—´åºåˆ—æ—¶å…·æœ‰ä¼˜åŠ¿ï¼Œå› ä¸ºå®ƒå‡è½»äº†å†…å­˜æº¢å‡ºçš„é£é™©ã€‚å¦ä¸€ç§æ–¹æ³•ï¼ŒCSBIï¼Œç”±Chenç­‰äººï¼ˆ[2023](#bib.bib11)ï¼‰ä»‹ç»ï¼Œé€šè¿‡å°†æ‰©æ•£è¿‡ç¨‹å»ºæ¨¡ä¸ºSchrodingeræ¡¥é—®é¢˜ï¼Œä»è€Œæé«˜äº†æ•ˆç‡ï¼Œè¿™å¯ä»¥è½¬åŒ–ä¸ºè®¡ç®—å‹å¥½çš„éšæœºå¾®åˆ†æ–¹ç¨‹ã€‚
- en: Moreover, the efficacy of diffusion models is notably influenced by the construction
    and utilization of conditional information. MIDMÂ Wang et al. ([2023](#bib.bib51))
    proposes to sample noise from a distribution conditional on observed dataâ€™s representations
    in the denoising process, In this way, it can explicitly preserve the intrinsic
    correlations between observed and missing data. PriSTIÂ Liu et al. ([2023](#bib.bib32))
    introduces the spatiotemporal dependencies as conditional information, i.e., provides
    the denoising network with spatiotemporal attention weights calculated by the
    conditional feature for spatiotemporal imputation. Additionally, DA-TASWDMÂ Xu
    et al. ([2023](#bib.bib55)) suggests incorporating dynamic temporal relationships,
    i.e. the varying sampling densities, into the denoising network for medical time
    series imputation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ€§æ˜¾è‘—å—åˆ°æ¡ä»¶ä¿¡æ¯æ„å»ºå’Œåˆ©ç”¨çš„å½±å“ã€‚MIDM Wangç­‰äººï¼ˆ[2023](#bib.bib51)ï¼‰å»ºè®®ä»æ¡ä»¶äºè§‚å¯Ÿæ•°æ®è¡¨ç¤ºçš„åˆ†å¸ƒä¸­é‡‡æ ·å™ªå£°ï¼Œåœ¨å»å™ªè¿‡ç¨‹ä¸­æ˜¾å¼åœ°ä¿ç•™è§‚å¯Ÿæ•°æ®å’Œç¼ºå¤±æ•°æ®ä¹‹é—´çš„å†…åœ¨ç›¸å…³æ€§ã€‚PriSTI
    Liuç­‰äººï¼ˆ[2023](#bib.bib32)ï¼‰å¼•å…¥äº†æ—¶ç©ºä¾èµ–ä½œä¸ºæ¡ä»¶ä¿¡æ¯ï¼Œå³é€šè¿‡æ¡ä»¶ç‰¹å¾ä¸ºå»å™ªç½‘ç»œæä¾›æ—¶ç©ºæ³¨æ„æƒé‡ä»¥è¿›è¡Œæ—¶ç©ºæ’è¡¥ã€‚æ­¤å¤–ï¼ŒDA-TASWDM
    Xuç­‰äººï¼ˆ[2023](#bib.bib55)ï¼‰å»ºè®®å°†åŠ¨æ€æ—¶é—´å…³ç³»ï¼Œå³å˜åŒ–çš„é‡‡æ ·å¯†åº¦ï¼Œçº³å…¥å»å™ªç½‘ç»œä»¥è¿›è¡ŒåŒ»ç–—æ—¶é—´åºåˆ—æ’è¡¥ã€‚
- en: Contrasting with the above diffusion-based methods that treat time series as
    discrete time steps, SPDÂ BiloÅ¡ et al. ([2023](#bib.bib8)) views time series as
    discrete realizations of an underlying continuous function and generates data
    for imputation using stochastic process diffusion. In this way, SPD posits the
    continuous noise process as an inductive bias for the irregular time series, so
    as to better capture the true generative process, especially with the inherent
    stochasticity of the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¸Šè¿°å°†æ—¶é—´åºåˆ—è§†ä¸ºç¦»æ•£æ—¶é—´æ­¥çš„æ–¹æ³•ç›¸æ¯”ï¼ŒSPD BiloÅ¡ç­‰äººï¼ˆ[2023](#bib.bib8)ï¼‰å°†æ—¶é—´åºåˆ—è§†ä¸ºæ½œåœ¨è¿ç»­å‡½æ•°çš„ç¦»æ•£å®ç°ï¼Œå¹¶é€šè¿‡éšæœºè¿‡ç¨‹æ‰©æ•£ç”Ÿæˆæ’è¡¥æ•°æ®ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSPD
    å°†è¿ç»­å™ªå£°è¿‡ç¨‹è§†ä¸ºä¸è§„åˆ™æ—¶é—´åºåˆ—çš„å½’çº³åå·®ï¼Œä»¥æ›´å¥½åœ°æ•æ‰çœŸå®çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯è€ƒè™‘åˆ°æ•°æ®çš„å›ºæœ‰éšæœºæ€§ã€‚
- en: 4.5 Pros and Cons
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 ä¼˜ç¼ºç‚¹
- en: This subsection delineates the advantages and limitations of the aforementioned
    generative imputation models. VAE-based models are adept at modeling probabilities
    explicitly and offering a theoretical foundation for understanding data distributions.
    However, they are often constrained by their generative capacity, which can limit
    their performance in capturing complex data variability. GAN-based models, on
    the other hand, excel in data generation, providing high-quality imputations with
    impressive fidelity to the original data distributions. Yet, they are notoriously
    challenging to train due to issues like vanishing gradients Wu et al. ([2023b](#bib.bib54)),
    which can hamper model stability and convergence. Diffusion-based models emerge
    as powerful generative tools with a strong capacity for capturing intricate data
    patterns. Nevertheless, their computational complexity is considerable, and they
    also suffer from issues related to boundary coherence between missing and observed
    partsÂ Lugmayr et al. ([2022](#bib.bib34)).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬å°èŠ‚é˜è¿°äº†ä¸Šè¿°ç”Ÿæˆå¡«å……æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚åŸºäº VAE çš„æ¨¡å‹æ“…é•¿æ˜ç¡®å»ºæ¨¡æ¦‚ç‡ï¼Œå¹¶ä¸ºç†è§£æ•°æ®åˆ†å¸ƒæä¾›äº†ç†è®ºåŸºç¡€ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸å—åˆ°ç”Ÿæˆèƒ½åŠ›çš„é™åˆ¶ï¼Œè¿™å¯èƒ½é™åˆ¶äº†å®ƒä»¬åœ¨æ•æ‰å¤æ‚æ•°æ®å˜å¼‚æ€§æ–¹é¢çš„è¡¨ç°ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŸºäº
    GAN çš„æ¨¡å‹åœ¨æ•°æ®ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†ä¸åŸå§‹æ•°æ®åˆ†å¸ƒé«˜åº¦ä¸€è‡´çš„é«˜è´¨é‡å¡«å……ã€‚ç„¶è€Œï¼Œç”±äºè¯¸å¦‚æ¢¯åº¦æ¶ˆå¤± Wu et al. ([2023b](#bib.bib54))
    ç­‰é—®é¢˜ï¼Œå®ƒä»¬è®­ç»ƒèµ·æ¥ notoriously å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™å¯èƒ½å½±å“æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚åŸºäºæ‰©æ•£çš„æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆå·¥å…·ï¼Œå…·æœ‰æ•æ‰å¤æ‚æ•°æ®æ¨¡å¼çš„å¼ºå¤§èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è®¡ç®—å¤æ‚åº¦ç›¸å½“é«˜ï¼Œå¹¶ä¸”ä¹Ÿé¢ä¸´ç¼ºå¤±éƒ¨åˆ†ä¸è§‚æµ‹éƒ¨åˆ†ä¹‹é—´çš„è¾¹ç•Œä¸€è‡´æ€§é—®é¢˜
    Lugmayr et al. ([2022](#bib.bib34))ã€‚
- en: 5 Time Series Imputation Toolkits
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 ä¸ªæ—¶é—´åºåˆ—å¡«å……å·¥å…·åŒ…
- en: On the time series imputation task, there are existing libraries providing naive
    processing ways, statistical methods, machine learning imputation algorithms,
    and deep learning imputation neural networks for convenient usage.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ—¶é—´åºåˆ—å¡«å……ä»»åŠ¡ä¸­ï¼Œç°æœ‰çš„åº“æä¾›äº†ç®€å•å¤„ç†æ–¹æ³•ã€ç»Ÿè®¡æ–¹æ³•ã€æœºå™¨å­¦ä¹ å¡«å……ç®—æ³•ä»¥åŠæ·±åº¦å­¦ä¹ å¡«å……ç¥ç»ç½‘ç»œä»¥ä¾¿äºä½¿ç”¨ã€‚
- en: imputeTSÂ [Moritz and Bartz-Beielstein](#bib.bib41) , a library in R provides
    several naive approaches (e.g., mean values, last observation carried forward,
    etc.) and commonly-used imputation algorithms (e.g., linear interpolation, Kalman
    smoothing, and weighted moving average) but only for univariate time series. Another
    well-known R package, miceÂ VanÂ Buuren and Groothuis-Oudshoorn ([2011](#bib.bib49)),
    implements the method called multivariate imputation by chained equations to tackle
    missingness in data. Although it is not for time series specifically, it is widely
    used in practice for multivariate time-series imputation, especially in the field
    of statistics. ImpyuteÂ²Â²2[https://github.com/eltonlaw/impyute](https://github.com/eltonlaw/impyute)
    and AutoimputeÂ³Â³3[https://github.com/kearnz/autoimpute](https://github.com/kearnz/autoimpute)
    both offer naive imputation methods for cross-sectional data and time-series data.
    Impyute is only with simple approaches like the moving average window, and Autoimpute
    integrates parametric methods, for example, polynomial interpolation and spline
    interpolation. More recently, GluonTSÂ Alexandrov et al. ([2020](#bib.bib2)), a
    generative machine-learning package for time series, provides some naive ways,
    such as dummy value imputation and casual mean value imputation, to handle missing
    values. In addition to simple and non-parametric methods, SktimeÂ LÃ¶ning et al.
    ([2019](#bib.bib33)) implements one more option that allows users to leverage
    integrated machine learning imputation algorithms to fit and predict missing values
    in the given data, though this works in a univariate way. When it comes to deep
    learning imputation, PyPOTSÂ Du ([2023](#bib.bib14)) is a toolbox focusing on modeling
    partially-observed time series end-to-end. It contains more than a dozen deep-learning
    neural networks for tasks on incomplete time series, including eight imputation
    models so far.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: imputeTSÂ [Moritz å’Œ Bartz-Beielstein](#bib.bib41)ï¼Œè¿™æ˜¯ä¸€ä¸ª R è¯­è¨€åº“ï¼Œæä¾›äº†å‡ ç§ç®€å•çš„æ–¹æ³•ï¼ˆå¦‚å‡å€¼ã€æœ€åè§‚æµ‹å€¼å‰æ¨ç­‰ï¼‰å’Œå¸¸ç”¨çš„æ’è¡¥ç®—æ³•ï¼ˆå¦‚çº¿æ€§æ’å€¼ã€å¡å°”æ›¼å¹³æ»‘å’ŒåŠ æƒç§»åŠ¨å¹³å‡ï¼‰ï¼Œä½†ä»…é€‚ç”¨äºå•å˜é‡æ—¶é—´åºåˆ—ã€‚å¦ä¸€ä¸ªè‘—åçš„
    R åŒ…ï¼ŒmiceÂ VanÂ Buuren å’Œ Groothuis-Oudshoorn ([2011](#bib.bib49))ï¼Œå®ç°äº†ä¸€ç§ç§°ä¸ºé“¾å¼æ–¹ç¨‹çš„å¤šå˜é‡æ’è¡¥æ–¹æ³•ï¼Œä»¥å¤„ç†æ•°æ®ä¸­çš„ç¼ºå¤±å€¼ã€‚è™½ç„¶å®ƒå¹¶ä¸æ˜¯ä¸“é—¨é’ˆå¯¹æ—¶é—´åºåˆ—çš„ï¼Œä½†åœ¨å®é™…ä¸­å¹¿æ³›ç”¨äºå¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥ï¼Œå°¤å…¶åœ¨ç»Ÿè®¡é¢†åŸŸã€‚ImpyuteÂ²Â²2[https://github.com/eltonlaw/impyute](https://github.com/eltonlaw/impyute)
    å’Œ AutoimputeÂ³Â³3[https://github.com/kearnz/autoimpute](https://github.com/kearnz/autoimpute)
    éƒ½ä¸ºæ¨ªæˆªé¢æ•°æ®å’Œæ—¶é—´åºåˆ—æ•°æ®æä¾›äº†ç®€å•çš„æ’è¡¥æ–¹æ³•ã€‚Impyute ä»…æä¾›å¦‚ç§»åŠ¨å¹³å‡çª—å£è¿™æ ·çš„ç®€å•æ–¹æ³•ï¼Œè€Œ Autoimpute é›†æˆäº†å‚æ•°æ–¹æ³•ï¼Œä¾‹å¦‚å¤šé¡¹å¼æ’å€¼å’Œæ ·æ¡æ’å€¼ã€‚æœ€è¿‘ï¼ŒGluonTSÂ Alexandrov
    ç­‰äºº ([2020](#bib.bib2))ï¼Œä¸€ä¸ªç”¨äºæ—¶é—´åºåˆ—çš„ç”Ÿæˆæ€§æœºå™¨å­¦ä¹ åŒ…ï¼Œæä¾›äº†ä¸€äº›ç®€å•çš„æ–¹æ³•ï¼Œå¦‚è™šæ‹Ÿå€¼æ’è¡¥å’Œéšæ„å‡å€¼æ’è¡¥ï¼Œä»¥å¤„ç†ç¼ºå¤±å€¼ã€‚é™¤äº†ç®€å•å’Œéå‚æ•°æ–¹æ³•å¤–ï¼ŒSktimeÂ LÃ¶ning
    ç­‰äºº ([2019](#bib.bib33)) å®ç°äº†å¦ä¸€ç§é€‰é¡¹ï¼Œå…è®¸ç”¨æˆ·åˆ©ç”¨é›†æˆçš„æœºå™¨å­¦ä¹ æ’è¡¥ç®—æ³•æ¥æ‹Ÿåˆå’Œé¢„æµ‹ç»™å®šæ•°æ®ä¸­çš„ç¼ºå¤±å€¼ï¼Œå°½ç®¡è¿™ä»…åœ¨å•å˜é‡æ–¹å¼ä¸‹æœ‰æ•ˆã€‚è‡³äºæ·±åº¦å­¦ä¹ æ’è¡¥ï¼ŒPyPOTSÂ Du
    ([2023](#bib.bib14)) æ˜¯ä¸€ä¸ªä¸“æ³¨äºç«¯åˆ°ç«¯å»ºæ¨¡éƒ¨åˆ†è§‚æµ‹æ—¶é—´åºåˆ—çš„å·¥å…·åŒ…ã€‚å®ƒåŒ…å«åå¤šä¸ªç”¨äºå¤„ç†ä¸å®Œæ•´æ—¶é—´åºåˆ—çš„æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬ç›®å‰çš„å…«ç§æ’è¡¥æ¨¡å‹ã€‚
- en: 6 Experimental Evaluation and Discussion
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 å®éªŒè¯„ä¼°ä¸è®¨è®º
- en: In this section, empirical experiments are conducted to evaluate and analyze
    deep multivariate time series imputation methods from different categories. The
    results are obtained with a machine with AMD EPYC 7543 32-Core CPU and an NVIDIA
    GeForce RTX 4090 GPU. All code, including the data preprocessing scripts, model
    configurations, and training scripts, are publicly available in the GitHub repository
    [https://github.com/WenjieDu/Awesome_Imputation](https://github.com/WenjieDu/Awesome_Imputation).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œè¿›è¡Œå®è¯å®éªŒä»¥è¯„ä¼°å’Œåˆ†ææ¥è‡ªä¸åŒç±»åˆ«çš„æ·±åº¦å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥æ–¹æ³•ã€‚ç»“æœæ˜¯åœ¨é…å¤‡ AMD EPYC 7543 32 æ ¸ CPU å’Œ NVIDIA
    GeForce RTX 4090 GPU çš„æœºå™¨ä¸Šè·å¾—çš„ã€‚æ‰€æœ‰ä»£ç ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†è„šæœ¬ã€æ¨¡å‹é…ç½®å’Œè®­ç»ƒè„šæœ¬ï¼Œå‡å…¬å¼€åœ¨ GitHub ä»“åº“ [https://github.com/WenjieDu/Awesome_Imputation](https://github.com/WenjieDu/Awesome_Imputation)
    ä¸Šã€‚
- en: 6.1 Datasets and Imputation Methods
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 æ•°æ®é›†ä¸æ’è¡¥æ–¹æ³•
- en: 'Specifically, three naive imputation approaches and eight deep-learning neural
    networks are tested on three real-world datasets (AirÂ Zhang et al. ([2017](#bib.bib57)),
    PhysioNet2012Â Silva et al. ([2012](#bib.bib47)), and ETTm1Â Zhou et al. ([2021](#bib.bib58))
    in TableÂ [2](#S6.T2 "Table 2 â€£ 6.1 Datasets and Imputation Methods â€£ 6 Experimental
    Evaluation and Discussion â€£ Deep Learning for Multivariate Time Series Imputation:
    A Survey")) which are commonly used in the literature.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œå¯¹ä¸‰ç§ç®€å•æ’è¡¥æ–¹æ³•å’Œå…«ç§æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œè¿›è¡Œäº†æµ‹è¯•ï¼Œä½¿ç”¨äº†ä¸‰ä¸ªçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ï¼ˆAirÂ Zhang ç­‰äºº ([2017](#bib.bib57))ã€PhysioNet2012Â Silva
    ç­‰äºº ([2012](#bib.bib47)) å’Œ ETTm1Â Zhou ç­‰äºº ([2021](#bib.bib58))ï¼Œè§è¡¨Â [2](#S6.T2 "è¡¨
    2 â€£ 6.1 æ•°æ®é›†ä¸æ’è¡¥æ–¹æ³• â€£ 6 å®éªŒè¯„ä¼°ä¸è®¨è®º â€£ å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥çš„æ·±åº¦å­¦ä¹ ï¼šç»¼è¿°")ï¼‰ï¼Œè¿™äº›æ•°æ®é›†åœ¨æ–‡çŒ®ä¸­å¸¸è¢«ä½¿ç”¨ã€‚
- en: 'Regarding the imputation methods, apart from three naive ways Mean, Median,
    and LOCF (last observation carried forward) as baselines, the eight following
    representative deep-learning models are selected from different categories for
    experimental studies: M-RNNÂ Yoon et al. ([2019](#bib.bib56)), GP-VAEÂ Fortuin et
    al. ([2020](#bib.bib17)), BRITSÂ Cao et al. ([2018](#bib.bib9)), USGANÂ Miao et
    al. ([2021](#bib.bib39)), CSDIÂ Tashiro et al. ([2021](#bib.bib48)), TimesNetÂ Wu
    et al. ([2023a](#bib.bib53)), TransformerÂ Du et al. ([2023](#bib.bib13)), and
    SAITSÂ Du et al. ([2023](#bib.bib13)). Experiments are performed with PyPOTSâ´â´4[https://pypots.com](https://pypots.com)Â Du
    ([2023](#bib.bib14)) and all the above imputation methods are instantly available
    in the toolbox. Moreover, for a fair comparison, hyperparameters of all deep learning
    methods are optimized by the tuning functionality in PyPOTS.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºå¡«è¡¥æ–¹æ³•ï¼Œé™¤äº†ä¸‰ä¸ªç®€å•çš„åŸºçº¿æ–¹æ³•ï¼šå‡å€¼ï¼ˆMeanï¼‰ã€ä¸­ä½æ•°ï¼ˆMedianï¼‰å’Œå‰å‘å¡«è¡¥ï¼ˆLOCFï¼Œæœ€åè§‚æµ‹å€¼å‰å‘å¡«è¡¥ï¼‰ï¼Œé€‰æ‹©äº†ä»¥ä¸‹å…«ç§ä»£è¡¨æ€§æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œå®éªŒç ”ç©¶ï¼šM-RNN
    Yoon et al. ([2019](#bib.bib56))ï¼ŒGP-VAE Fortuin et al. ([2020](#bib.bib17))ï¼ŒBRITS
    Cao et al. ([2018](#bib.bib9))ï¼ŒUSGAN Miao et al. ([2021](#bib.bib39))ï¼ŒCSDI Tashiro
    et al. ([2021](#bib.bib48))ï¼ŒTimesNet Wu et al. ([2023a](#bib.bib53))ï¼ŒTransformer
    Du et al. ([2023](#bib.bib13))ï¼Œä»¥åŠ SAITS Du et al. ([2023](#bib.bib13))ã€‚å®éªŒä½¿ç”¨äº† PyPOTSâ´â´4[https://pypots.com](https://pypots.com)
    Du ([2023](#bib.bib14))ï¼Œæ‰€æœ‰ä¸Šè¿°å¡«è¡¥æ–¹æ³•åœ¨å·¥å…·ç®±ä¸­å‡å¯ç«‹å³ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œä¸ºäº†å…¬å¹³æ¯”è¾ƒï¼Œæ‰€æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•çš„è¶…å‚æ•°éƒ½é€šè¿‡ PyPOTS ä¸­çš„è°ƒä¼˜åŠŸèƒ½è¿›è¡Œäº†ä¼˜åŒ–ã€‚
- en: '|  | Air | PhysioNet2012 | ETTm1 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | Air | PhysioNet2012 | ETTm1 |'
- en: '| Number of samples | 1,458 | 11,988 | 722 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| æ ·æœ¬æ•°é‡ | 1,458 | 11,988 | 722 |'
- en: '| Sequence length | 24 | 48 | 96 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| åºåˆ—é•¿åº¦ | 24 | 48 | 96 |'
- en: '| Number of features | 132 | 37 | 7 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ç‰¹å¾æ•°é‡ | 132 | 37 | 7 |'
- en: '| Original missing rate | 1.6% | 80.0% | 0% |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| åŸå§‹ç¼ºå¤±ç‡ | 1.6% | 80.0% | 0% |'
- en: 'Table 2: The general information of the three preprocessed datasets. Note the
    detailed descriptions are available [in the code repository](https://github.com/WenjieDu/Awesome_Imputation/tree/main/time_series_imputation_survey_code).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 2ï¼šä¸‰ä¸ªé¢„å¤„ç†æ•°æ®é›†çš„ä¸€èˆ¬ä¿¡æ¯ã€‚è¯¦ç»†æè¿°è§[ä»£ç åº“](https://github.com/WenjieDu/Awesome_Imputation/tree/main/time_series_imputation_survey_code)ã€‚
- en: '| Method | Air | PhysioNet2012 | ETTm1 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | Air | PhysioNet2012 | ETTm1 |'
- en: '| MAE $\downarrow$ | MSE $\downarrow$ | MAE $\downarrow$ | MSE $\downarrow$
    | MAE $\downarrow$ | MSE $\downarrow$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| MAE $\downarrow$ | MSE $\downarrow$ | MAE $\downarrow$ | MSE $\downarrow$
    | MAE $\downarrow$ | MSE $\downarrow$ |'
- en: '| Mean | 0.692$\pm$0.000 | 0.970$\pm$0.000 | 0.702$\pm$0.000 | 0.954$\pm$0.000
    | 0.663$\pm$0.000 | 0.809$\pm$0.000 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| å‡å€¼ | 0.692$\pm$0.000 | 0.970$\pm$0.000 | 0.702$\pm$0.000 | 0.954$\pm$0.000
    | 0.663$\pm$0.000 | 0.809$\pm$0.000 |'
- en: '| Median | 0.660$\pm$0.000 | 1.027$\pm$0.000 | 0.685$\pm$0.000 | 0.991$\pm$0.000
    | 0.657$\pm$0.000 | 0.825$\pm$0.000 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ä¸­ä½æ•° | 0.660$\pm$0.000 | 1.027$\pm$0.000 | 0.685$\pm$0.000 | 0.991$\pm$0.000
    | 0.657$\pm$0.000 | 0.825$\pm$0.000 |'
- en: '| LOCF | 0.206$\pm$0.000 | 0.279$\pm$0.000 | 0.411$\pm$0.000 | 0.569$\pm$0.000
    | 0.135$\pm$0.000 | 0.072$\pm$0.000 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| LOCF | 0.206$\pm$0.000 | 0.279$\pm$0.000 | 0.411$\pm$0.000 | 0.569$\pm$0.000
    | 0.135$\pm$0.000 | 0.072$\pm$0.000 |'
- en: '| M-RNN | 0.524$\pm$0.001 | 0.648$\pm$0.003 | 0.674$\pm$0.001 | 0.864$\pm$0.002
    | 0.651$\pm$0.060 | 1.074$\pm$0.120 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| M-RNN | 0.524$\pm$0.001 | 0.648$\pm$0.003 | 0.674$\pm$0.001 | 0.864$\pm$0.002
    | 0.651$\pm$0.060 | 1.074$\pm$0.120 |'
- en: '| GP-VAE | 0.280$\pm$0.003 | 0.266$\pm$0.009 | 0.400$\pm$0.007 | 0.433$\pm$0.011
    | 0.290$\pm$0.017 | 0.178$\pm$0.015 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GP-VAE | 0.280$\pm$0.003 | 0.266$\pm$0.009 | 0.400$\pm$0.007 | 0.433$\pm$0.011
    | 0.290$\pm$0.017 | 0.178$\pm$0.015 |'
- en: '| BRITS | 0.142$\pm$0.001 | 0.129$\pm$0.001 | 0.246$\pm$0.001 | 0.325$\pm$0.002
    | 0.124$\pm$0.002 | 0.046$\pm$0.002 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| BRITS | 0.142$\pm$0.001 | 0.129$\pm$0.001 | 0.246$\pm$0.001 | 0.325$\pm$0.002
    | 0.124$\pm$0.002 | 0.046$\pm$0.002 |'
- en: '| USGAN | 0.141$\pm$0.001 | 0.132$\pm$0.001 | 0.250$\pm$0.001 | 0.306$\pm$0.001
    | 0.127$\pm$0.005 | 0.048$\pm$0.003 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| USGAN | 0.141$\pm$0.001 | 0.132$\pm$0.001 | 0.250$\pm$0.001 | 0.306$\pm$0.001
    | 0.127$\pm$0.005 | 0.048$\pm$0.003 |'
- en: '| CSDI | 0.105$\pm$0.003 | 0.153$\pm$0.021 | 0.211$\pm$0.003 | 0.260$\pm$0.050
    | 0.157$\pm$0.052 | 0.292$\pm$0.456 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| CSDI | 0.105$\pm$0.003 | 0.153$\pm$0.021 | 0.211$\pm$0.003 | 0.260$\pm$0.050
    | 0.157$\pm$0.052 | 0.292$\pm$0.456 |'
- en: '| TimesNet | 0.159$\pm$0.002 | 0.172$\pm$0.003 | 0.266$\pm$0.007 | 0.272$\pm$0.006
    | 0.113$\pm$0.006 | 0.027$\pm$0.002 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| TimesNet | 0.159$\pm$0.002 | 0.172$\pm$0.003 | 0.266$\pm$0.007 | 0.272$\pm$0.006
    | 0.113$\pm$0.006 | 0.027$\pm$0.002 |'
- en: '| Transformer | 0.163$\pm$0.003 | 0.160$\pm$0.004 | 0.209$\pm$0.002 | 0.225$\pm$0.002
    | 0.133$\pm$0.009 | 0.035$\pm$0.004 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | 0.163$\pm$0.003 | 0.160$\pm$0.004 | 0.209$\pm$0.002 | 0.225$\pm$0.002
    | 0.133$\pm$0.009 | 0.035$\pm$0.004 |'
- en: '| SAITS | 0.133$\pm$0.002 | 0.128$\pm$0.001 | 0.202$\pm$0.002 | 0.218$\pm$0.002
    | 0.115$\pm$0.011 | 0.030$\pm$0.006 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| SAITS | 0.133$\pm$0.002 | 0.128$\pm$0.001 | 0.202$\pm$0.002 | 0.218$\pm$0.002
    | 0.115$\pm$0.011 | 0.030$\pm$0.006 |'
- en: 'Table 3: The MAE and MSE comparisons between imputation methods on the datasets
    Air,'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 3ï¼šä¸åŒå¡«è¡¥æ–¹æ³•åœ¨æ•°æ®é›† Air ä¸Šçš„ MAE å’Œ MSE æ¯”è¾ƒã€‚
- en: PhysioNet2012, and ETTm1\. The reported values are means $\pm$ standard deviations
    of five runs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: PhysioNet2012 å’Œ ETTm1ã€‚æŠ¥å‘Šçš„å€¼ä¸ºäº”æ¬¡è¿è¡Œçš„å‡å€¼ $\pm$ æ ‡å‡†å·®ã€‚
- en: '| Method | PR-AUC $\uparrow$ | ROC-AUC $\uparrow$ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | PR-AUC $\uparrow$ | ROC-AUC $\uparrow$ |'
- en: '| Mean | 0.434$\pm$0.016 | 0.813$\pm$0.009 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| å¹³å‡å€¼ | 0.434$\pm$0.016 | 0.813$\pm$0.009 |'
- en: '| Median | 0.434$\pm$0.018 | 0.808$\pm$0.014 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| ä¸­ä½æ•° | 0.434$\pm$0.018 | 0.808$\pm$0.014 |'
- en: '| LOCF | 0.425$\pm$0.015 | 0.804$\pm$0.007 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| LOCF | 0.425$\pm$0.015 | 0.804$\pm$0.007 |'
- en: '| M-RNN | 0.424$\pm$0.022 | 0.807$\pm$0.015 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| M-RNN | 0.424$\pm$0.022 | 0.807$\pm$0.015 |'
- en: '| GP-VAE | 0.384$\pm$0.018 | 0.788$\pm$0.008 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| GP-VAE | 0.384$\pm$0.018 | 0.788$\pm$0.008 |'
- en: '| BRITS | 0.428$\pm$0.017 | 0.821$\pm$0.008 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| BRITS | 0.428$\pm$0.017 | 0.821$\pm$0.008 |'
- en: '| USGAN | 0.431$\pm$0.017 | 0.814$\pm$0.010 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| USGAN | 0.431$\pm$0.017 | 0.814$\pm$0.010 |'
- en: '| CSDI | 0.433$\pm$0.017 | 0.811$\pm$0.005 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| CSDI | 0.433$\pm$0.017 | 0.811$\pm$0.005 |'
- en: '| TimesNet | 0.406$\pm$0.012 | 0.787$\pm$0.013 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| TimesNet | 0.406$\pm$0.012 | 0.787$\pm$0.013 |'
- en: '| Transformer | 0.446$\pm$0.016 | 0.807$\pm$0.018 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | 0.446$\pm$0.016 | 0.807$\pm$0.018 |'
- en: '| SAITS | 0.455$\pm$0.016 | 0.822$\pm$0.002 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| SAITS | 0.455$\pm$0.016 | 0.822$\pm$0.002 |'
- en: 'Table 4: The means and standard deviations of classification results in five
    runs.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4ï¼šäº”æ¬¡è¿è¡Œä¸­çš„åˆ†ç±»ç»“æœçš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚
- en: 6.2 Results and Analysis
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 ç»“æœä¸åˆ†æ
- en: Imputation Accuracy Evaluation
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ’è¡¥å‡†ç¡®æ€§è¯„ä¼°
- en: 'Imputation results in error metrics MAE (mean absolute error) and MSE (mean
    squared error) of twelve methods across three datasets are displayed in TableÂ [4](#S6.T4
    "Table 4 â€£ 6.1 Datasets and Imputation Methods â€£ 6 Experimental Evaluation and
    Discussion â€£ Deep Learning for Multivariate Time Series Imputation: A Survey").
    The numbers tell that the performance of the methods varies on different datasets
    and there is no clear winner in this study. Further work needs to be done to deeply
    compare predictive and generative imputation methods. Notably, in cases like the
    Air and ETTm1 datasets, where data is continuously recorded by sensors and the
    proportion of missingness is relatively low, the non-parametric LOCF method shows
    commendable performance. Conversely, in the PhysioNet2012 dataset, which has a
    high missing rate, deep learning imputation methods markedly outperform statistical
    approaches. This observation corroborates the capability of deep learning methods
    to effectively capture complex temporal dynamics and accurately learn data distributions,
    especially in scenarios with highly sparse, discrete observations.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ’è¡¥ç»“æœä¸­åäºŒç§æ–¹æ³•åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¯¯å·®æŒ‡æ ‡MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰å’ŒMSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰å¦‚è¡¨[4](#S6.T4 "Table 4 â€£ 6.1 Datasets
    and Imputation Methods â€£ 6 Experimental Evaluation and Discussion â€£ Deep Learning
    for Multivariate Time Series Imputation: A Survey")æ‰€ç¤ºã€‚æ•°æ®æ˜¾ç¤ºï¼Œä¸åŒæ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°å­˜åœ¨å·®å¼‚ï¼Œåœ¨æœ¬ç ”ç©¶ä¸­æ²¡æœ‰æ˜æ˜¾çš„èƒœè€…ã€‚éœ€è¦è¿›ä¸€æ­¥å·¥ä½œä»¥æ·±å…¥æ¯”è¾ƒé¢„æµ‹å’Œç”Ÿæˆæ’è¡¥æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨åƒAirå’ŒETTm1æ•°æ®é›†è¿™æ ·çš„æƒ…å†µä¸­ï¼Œå…¶ä¸­æ•°æ®ç”±ä¼ æ„Ÿå™¨è¿ç»­è®°å½•ä¸”ç¼ºå¤±æ¯”ä¾‹ç›¸å¯¹è¾ƒä½æ—¶ï¼Œéå‚æ•°LOCæ–¹æ³•è¡¨ç°å‡ºä»¤äººç§°èµçš„æ€§èƒ½ã€‚ç›¸åï¼Œåœ¨ç¼ºå¤±ç‡è¾ƒé«˜çš„PhysioNet2012æ•°æ®é›†ä¸­ï¼Œæ·±åº¦å­¦ä¹ æ’è¡¥æ–¹æ³•æ˜æ˜¾ä¼˜äºç»Ÿè®¡æ–¹æ³•ã€‚è¿™ä¸€è§‚å¯Ÿç»“æœè¯å®äº†æ·±åº¦å­¦ä¹ æ–¹æ³•æœ‰æ•ˆæ•æ‰å¤æ‚æ—¶é—´åŠ¨æ€å’Œå‡†ç¡®å­¦ä¹ æ•°æ®åˆ†å¸ƒçš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰é«˜åº¦ç¨€ç–ã€ç¦»æ•£è§‚å¯Ÿçš„åœºæ™¯ä¸­ã€‚'
- en: Downstream Task Evaluation
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°
- en: 'Generally, the better quality of imputed values represents the better overall
    dataset quality after imputation. Consequently, in addition to the imputation
    performance comparison, there is an experiment setting in the literature that
    evaluates the methods from the perspective of downstream task performanceÂ Du et
    al. ([2023](#bib.bib13)). Such a study is adopted in this work as well to help
    assess the selected methods. A simple LSTM model performs the binary classification
    task on the PhysioNet2012 dataset where each sample has a label indicating whether
    the patient in the ICU was deceased. The PhysioNet2012 dataset is processed by
    the imputation methods and the results are presented in TableÂ [4](#S6.T4 "Table
    4 â€£ 6.1 Datasets and Imputation Methods â€£ 6 Experimental Evaluation and Discussion
    â€£ Deep Learning for Multivariate Time Series Imputation: A Survey"). PR-AUC (area
    under the precision-recall curve) and ROC-AUC (area under the receiver operating
    characteristic curve) are chosen to be the metrics, considering the dataset has
    imbalanced classes and 14.2% positive samples. Note that the only variable in
    this experiment is the imputed data.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 'é€šå¸¸ï¼Œæ›´å¥½çš„æ’è¡¥å€¼è´¨é‡ä»£è¡¨æ’è¡¥åçš„æ•´ä½“æ•°æ®é›†è´¨é‡æ›´å¥½ã€‚å› æ­¤ï¼Œé™¤äº†æ’è¡¥æ€§èƒ½æ¯”è¾ƒå¤–ï¼Œæ–‡çŒ®ä¸­è¿˜æœ‰ä¸€ç§å®éªŒè®¾ç½®ï¼Œä»ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„è§’åº¦è¯„ä¼°æ–¹æ³•Du et al.
    ([2023](#bib.bib13))ã€‚æœ¬ç ”ç©¶ä¹Ÿé‡‡ç”¨äº†è¿™ç§ç ”ç©¶æ–¹æ³•æ¥å¸®åŠ©è¯„ä¼°æ‰€é€‰æ–¹æ³•ã€‚ä¸€ä¸ªç®€å•çš„LSTMæ¨¡å‹åœ¨PhysioNet2012æ•°æ®é›†ä¸Šæ‰§è¡ŒäºŒåˆ†ç±»ä»»åŠ¡ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ä¸€ä¸ªæ ‡ç­¾æŒ‡ç¤ºICUä¸­çš„æ‚£è€…æ˜¯å¦å»ä¸–ã€‚PhysioNet2012æ•°æ®é›†ç»è¿‡æ’è¡¥æ–¹æ³•å¤„ç†ï¼Œç»“æœå¦‚è¡¨æ ¼[4](#S6.T4
    "Table 4 â€£ 6.1 Datasets and Imputation Methods â€£ 6 Experimental Evaluation and
    Discussion â€£ Deep Learning for Multivariate Time Series Imputation: A Survey")æ‰€ç¤ºã€‚é€‰æ‹©PR-AUCï¼ˆç²¾ç¡®åº¦-å¬å›æ›²çº¿ä¸‹é¢ç§¯ï¼‰å’ŒROC-AUCï¼ˆæ¥æ”¶å™¨æ“ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯ï¼‰ä½œä¸ºæŒ‡æ ‡ï¼Œè€ƒè™‘åˆ°æ•°æ®é›†æœ‰ä¸å¹³è¡¡çš„ç±»åˆ«å’Œ14.2%çš„æ­£æ ·æœ¬ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸ªå®éªŒä¸­å”¯ä¸€çš„å˜é‡æ˜¯æ’è¡¥æ•°æ®ã€‚'
- en: 'As shown in TableÂ [4](#S6.T4 "Table 4 â€£ 6.1 Datasets and Imputation Methods
    â€£ 6 Experimental Evaluation and Discussion â€£ Deep Learning for Multivariate Time
    Series Imputation: A Survey"), the classifier can benefit from better imputation
    on the downstream classification task. The best results from SAITS imputation
    obtain 5% and 1% gains than the best naive imputation Mean separately on the metrics
    PR-AUC and ROC-AUC. Please note that such improvements are achieved simply by
    better imputation, which can be seen as a data-preprocessing step in this experiment.
    Furthermore, this raises a research question about how to make deep learning imputation
    models learn from both the imputation task and downstream tasks to obtain a consistent
    and unified representation from the incomplete time series.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦‚è¡¨æ ¼[4](#S6.T4 "Table 4 â€£ 6.1 Datasets and Imputation Methods â€£ 6 Experimental
    Evaluation and Discussion â€£ Deep Learning for Multivariate Time Series Imputation:
    A Survey")æ‰€ç¤ºï¼Œåˆ†ç±»å™¨å¯ä»¥é€šè¿‡æ›´å¥½çš„æ’è¡¥åœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­å—ç›Šã€‚SAITSæ’è¡¥çš„æœ€ä½³ç»“æœåœ¨PR-AUCå’ŒROC-AUCæŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”æœ€ä½³çš„åŸå§‹æ’è¡¥å‡å€¼æé«˜äº†5%å’Œ1%ã€‚è¯·æ³¨æ„ï¼Œè¿™ç§æ”¹è¿›ä»…é€šè¿‡æ›´å¥½çš„æ’è¡¥å®ç°ï¼Œå¯ä»¥è§†ä¸ºæœ¬å®éªŒä¸­çš„æ•°æ®é¢„å¤„ç†æ­¥éª¤ã€‚æ­¤å¤–ï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªç ”ç©¶é—®é¢˜ï¼Œå³å¦‚ä½•ä½¿æ·±åº¦å­¦ä¹ æ’è¡¥æ¨¡å‹åŒæ—¶ä»æ’è¡¥ä»»åŠ¡å’Œä¸‹æ¸¸ä»»åŠ¡ä¸­å­¦ä¹ ï¼Œä»¥ä»ä¸å®Œæ•´çš„æ—¶é—´åºåˆ—ä¸­è·å¾—ä¸€è‡´ä¸”ç»Ÿä¸€çš„è¡¨ç¤ºã€‚'
- en: Complexity Analysis
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¤æ‚åº¦åˆ†æ
- en: 'We summarize the time and memory complexity of the deep learning imputation
    models in TableÂ [5](#S6.T5 "Table 5 â€£ Complexity Analysis â€£ 6.2 Results and Analysis
    â€£ 6 Experimental Evaluation and Discussion â€£ Deep Learning for Multivariate Time
    Series Imputation: A Survey"). Additionally, their actual inference time on the
    test set of PhysioNet2012 is also listed for clear comparison.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬åœ¨è¡¨æ ¼[5](#S6.T5 "Table 5 â€£ Complexity Analysis â€£ 6.2 Results and Analysis â€£
    6 Experimental Evaluation and Discussion â€£ Deep Learning for Multivariate Time
    Series Imputation: A Survey")ä¸­æ€»ç»“äº†æ·±åº¦å­¦ä¹ æ’è¡¥æ¨¡å‹çš„æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œè¿˜åˆ—å‡ºäº†å®ƒä»¬åœ¨PhysioNet2012æµ‹è¯•é›†ä¸Šçš„å®é™…æ¨ç†æ—¶é—´ï¼Œä»¥ä¾¿è¿›è¡Œæ¸…æ™°çš„æ¯”è¾ƒã€‚'
- en: '| Method | Computation | Memory | Running Time |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | è®¡ç®—å¤æ‚åº¦ | å†…å­˜ | è¿è¡Œæ—¶é—´ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| M-RNN | $\mathcal{O}(L*K)$ | $\mathcal{O}(1)$ | 5s |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| M-RNN | $\mathcal{O}(L*K)$ | $\mathcal{O}(1)$ | 5s |'
- en: '| GP-VAE | $\mathcal{O}(L*\log{}K)$ | $\mathcal{O}(1)$ | 1s |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| GP-VAE | $\mathcal{O}(L*\log{}K)$ | $\mathcal{O}(1)$ | 1s |'
- en: '| BRITS | $\mathcal{O}(L)$ | $\mathcal{O}(1)$ | 9s |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| BRITS | $\mathcal{O}(L)$ | $\mathcal{O}(1)$ | 9s |'
- en: '| USGAN | $\mathcal{O}(L)$ | $\mathcal{O}(1)$ | 9s |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| USGAN | $\mathcal{O}(L)$ | $\mathcal{O}(1)$ | 9s |'
- en: '| CSDI | $\mathcal{O}(L^{2})$ | $\mathcal{O}(L^{2})$ | 104s |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| CSDI | $\mathcal{O}(L^{2})$ | $\mathcal{O}(L^{2})$ | 104s |'
- en: '| TimesNet | $\mathcal{O}(L*\log{}K)$ | $\mathcal{O}(1)$ | 1s |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| TimesNet | $\mathcal{O}(L*\log{}K)$ | $\mathcal{O}(1)$ | 1s |'
- en: '| Transformer | $\mathcal{O}(L^{2})$ | $\mathcal{O}(L^{2})$ | 1s |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | $\mathcal{O}(L^{2})$ | $\mathcal{O}(L^{2})$ | 1s |'
- en: '| SAITS | $\mathcal{O}(L^{2})$ | $\mathcal{O}(L^{2})$ | 1s |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| SAITS | $\mathcal{O}(L^{2})$ | $\mathcal{O}(L^{2})$ | 1s |'
- en: 'Table 5: Computational and space complexity of imputation models, and their
    running time in seconds on the PhysioNet2012 test set.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨5ï¼šæ’è¡¥æ¨¡å‹çš„è®¡ç®—å’Œç©ºé—´å¤æ‚åº¦ï¼Œä»¥åŠåœ¨PhysioNet2012æµ‹è¯•é›†ä¸Šçš„è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰ã€‚
- en: 7 Conclusion and Future Direction
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 ç»“è®ºå’Œæœªæ¥æ–¹å‘
- en: This paper presents a systematic review of deep learning models specifically
    tailored for multivariate time series imputation. We introduce a novel taxonomy
    to categorize the reviewed methods, providing a comprehensive introduction and
    an experimental comparison of each. To advance this field, the paper concludes
    by identifying and discussing the following potential avenues for future research.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥çš„æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç³»ç»Ÿå›é¡¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åˆ†ç±»æ³•æ¥å¯¹æ‰€è¯„ä¼°çš„æ–¹æ³•è¿›è¡Œåˆ†ç±»ï¼Œæä¾›äº†æ¯ç§æ–¹æ³•çš„å…¨é¢ä»‹ç»å’Œå®éªŒæ¯”è¾ƒã€‚ä¸ºäº†æ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„å‘å±•ï¼Œæœ¬æ–‡æœ€åç¡®å®šå¹¶è®¨è®ºäº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚
- en: Missingness Patterns
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¼ºå¤±æ¨¡å¼
- en: Existing imputation algorithms predominantly operate under the MCAR or MAR.
    However, real-world missing data mechanisms are often more complex, with the MNAR
    data being prevalent in diverse fields such as IoT devicesÂ Li et al. ([2023](#bib.bib28)),
    clinical studiesÂ Ibrahim et al. ([2012](#bib.bib21)), and meteorologyÂ Ruiz et
    al. ([2023](#bib.bib45)). The non-ignorable nature of MNAR indicates a distributional
    shift exists between observed and true dataÂ Kyono et al. ([2021](#bib.bib27)).
    For example, in airflow signal analysisÂ Ruiz et al. ([2023](#bib.bib45)), the
    absence of high-value observations causes MNAR missing mechanism and leads to
    saturated peaks, visibly skewing the observed data distribution compared to the
    true underlying one. This scenario illustrates how imputation methods may incur
    inductive bias in model parameter estimation and underperform in the presence
    of MNAR. Addressing missing data in MNAR contexts, distinct from MCAR and MAR,
    calls for innovative methodologies to achieve better performance.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ç°æœ‰çš„æ’è¡¥ç®—æ³•ä¸»è¦æ˜¯åœ¨MCARæˆ–MARä¸‹è¿è¡Œã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„ç¼ºå¤±æ•°æ®æœºåˆ¶é€šå¸¸æ›´åŠ å¤æ‚ï¼ŒMNARæ•°æ®åœ¨å„ä¸ªé¢†åŸŸæ™®éå­˜åœ¨ï¼Œä¾‹å¦‚IoTè®¾å¤‡ä¸­  Li ç­‰äººï¼ˆ[2023](#bib.bib28)ï¼‰
    ï¼Œä¸´åºŠç ”ç©¶ä¸­  Ibrahim ç­‰äººï¼ˆ[2012](#bib.bib21)ï¼‰ï¼Œå’Œæ°”è±¡å­¦ä¸­  Ruiz ç­‰äººï¼ˆ[2023](#bib.bib45)ï¼‰ã€‚MNARçš„ä¸å¯å¿½è§†æ€§è¡¨æ˜è§‚å¯Ÿåˆ°çš„æ•°æ®ä¸çœŸå®æ•°æ®ä¹‹é—´å­˜åœ¨åˆ†å¸ƒåç§»  Kyono
    ç­‰äººï¼ˆ[2021](#bib.bib27)ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨æ°”æµä¿¡å·åˆ†æä¸­  Ruiz ç­‰äººï¼ˆ[2023](#bib.bib45)ï¼‰ï¼Œç¼ºä¹é«˜å€¼è§‚æµ‹å¯¼è‡´äº†MNARç¼ºå¤±æœºåˆ¶ï¼Œå¹¶å¯¼è‡´é¥±å’Œå³°å€¼ï¼Œæ˜æ˜¾åœ°æ‰­æ›²äº†è§‚å¯Ÿåˆ°çš„æ•°æ®åˆ†å¸ƒä¸çœŸå®åŸºç¡€æ•°æ®åˆ†å¸ƒä¹‹é—´çš„å…³ç³»ã€‚è¿™ç§æƒ…å†µè¯´æ˜äº†æ’è¡¥æ–¹æ³•å¯èƒ½åœ¨æ¨¡å‹å‚æ•°ä¼°è®¡ä¸­äº§ç”Ÿå½’çº³åå·®ï¼Œå¹¶åœ¨å­˜åœ¨MNARæ—¶è¡¨ç°ä¸ä½³ã€‚åœ¨å¤„ç†MNARèƒŒæ™¯ä¸‹çš„ç¼ºå¤±æ•°æ®ï¼Œä¸MCARå’ŒMARä¸åŒï¼Œéœ€è¦åˆ›æ–°æ–¹æ³•ä»¥è¾¾åˆ°æ›´å¥½çš„æ€§èƒ½ã€‚
- en: Downstream Performance
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¸‹æ¸¸æ€§èƒ½
- en: The primary objective of imputing missing values lies in enhancing downstream
    data analytics, particularly in scenarios with incomplete information. The prevalent
    approach is the â€œimpute and predictâ€ two-stage paradigm, where missing value imputation
    is a part of data preprocessing and followed by task-specific downstream models
    (e.g. a classifier), either in tandem or sequentially. An alternative method is
    the â€œencode and predictâ€ end-to-end paradigm, encoding the incomplete data into
    a proper representation for multitask learning, including imputation and other
    tasks (e.g. classification and forecasting, etc.). Despite the optimal paradigm
    for partially-observed time series data still remains an open area for future
    investigation, the latter end-to-end way turns out to be more promising especially
    when information embedded in the missing patterns is helpful to the downstream
    tasksÂ Miyaguchi et al. ([2021](#bib.bib40)).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æ’è¡¥ç¼ºå¤±å€¼çš„ä¸»è¦ç›®æ ‡åœ¨äºå¢å¼ºä¸‹æ¸¸æ•°æ®åˆ†æï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨ä¸å®Œæ•´ä¿¡æ¯çš„æƒ…å†µä¸‹ã€‚æµè¡Œçš„æ–¹æ³•æ˜¯â€œæ’è¡¥å’Œé¢„æµ‹â€ä¸¤é˜¶æ®µèŒƒå¼ï¼Œå…¶ä¸­ç¼ºå¤±å€¼æ’è¡¥æ˜¯æ•°æ®é¢„å¤„ç†çš„ä¸€éƒ¨åˆ†ï¼Œéšåæ˜¯ä¸“é—¨ä»»åŠ¡ä¸‹æ¸¸æ¨¡å‹ï¼ˆä¾‹å¦‚åˆ†ç±»å™¨ï¼‰ï¼Œå¯ä»¥åŒæ—¶æˆ–é¡ºåºåœ°è¿è¡Œã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯â€œç¼–ç å’Œé¢„æµ‹â€ç«¯åˆ°ç«¯èŒƒå¼ï¼Œå°†ä¸å®Œæ•´æ•°æ®ç¼–ç ä¸ºé€‚å½“è¡¨ç¤ºå½¢å¼ï¼Œç”¨äºå¤šä»»åŠ¡å­¦ä¹ ï¼ŒåŒ…æ‹¬æ’è¡¥å’Œå…¶ä»–ä»»åŠ¡ï¼ˆä¾‹å¦‚åˆ†ç±»å’Œé¢„æµ‹ç­‰ï¼‰ã€‚å°½ç®¡éƒ¨åˆ†è§‚æµ‹åˆ°çš„æ—¶é—´åºåˆ—æ•°æ®çš„æœ€ä½³èŒƒå¼ä»ç„¶æ˜¯æœªæ¥ç ”ç©¶çš„ä¸€ä¸ªå¼€æ”¾é¢†åŸŸï¼Œä½†åä¸€ç§ç«¯åˆ°ç«¯æ–¹å¼å°¤å…¶å…·æœ‰æ›´å¤§çš„å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯å½“åµŒå…¥åœ¨ç¼ºå¤±æ¨¡å¼ä¸­çš„ä¿¡æ¯å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰å¸®åŠ©æ—¶ã€‚
- en: Scalability
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¯æ‰©å±•æ€§
- en: While deep learning imputation algorithms have shown impressive performance,
    their computational cost often exceeds that of statistical and machine learning
    based counterparts. In the era of burgeoning digital data, spurred by advancements
    in communication and IoT devices, we are witnessing an exponential increase in
    data generation. This surge, accompanied by the prevalence of incomplete datasets,
    poses significant challenges in training deep models effectivelyÂ Wu et al. ([2023b](#bib.bib54)).
    Specifically, the high computational demands of existing deep imputation algorithms
    render them less feasible for large-scale datasets. Consequently, there is a growing
    need for scalable deep imputation solutions, leveraging parallel and distributed
    computing techniques, to effectively address the challenges of large-scale missing
    data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æ·±åº¦å­¦ä¹ æ’è¡¥ç®—æ³•è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶è®¡ç®—æˆæœ¬å¾€å¾€è¶…è¿‡ç»Ÿè®¡å’Œæœºå™¨å­¦ä¹ æ–¹æ³•ã€‚åœ¨é€šä¿¡å’Œç‰©è”ç½‘è®¾å¤‡çš„è¿›æ­¥æ¨åŠ¨ä¸‹ï¼Œæ•°å­—æ•°æ®æ—¶ä»£çš„åˆ°æ¥å¸¦æ¥äº†æ•°æ®ç”Ÿæˆçš„æŒ‡æ•°å¢é•¿ã€‚è¿™ä¸€æ¿€å¢ï¼ŒåŠ ä¸Šä¸å®Œæ•´æ•°æ®é›†çš„æ™®éå­˜åœ¨ï¼Œå¯¹æœ‰æ•ˆè®­ç»ƒæ·±åº¦æ¨¡å‹æå‡ºäº†é‡å¤§æŒ‘æˆ˜Â Wu
    ç­‰ ([2023b](#bib.bib54))ã€‚å…·ä½“è€Œè¨€ï¼Œç°æœ‰æ·±åº¦æ’è¡¥ç®—æ³•çš„é«˜è®¡ç®—éœ€æ±‚ä½¿å…¶åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å¯è¡Œæ€§é™ä½ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦å¯æ‰©å±•çš„æ·±åº¦æ’è¡¥è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨å¹¶è¡Œå’Œåˆ†å¸ƒå¼è®¡ç®—æŠ€æœ¯ï¼Œä»¥æœ‰æ•ˆåº”å¯¹å¤§è§„æ¨¡ç¼ºå¤±æ•°æ®çš„æŒ‘æˆ˜ã€‚
- en: Large Language Model for MTSI
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥çš„è¯­è¨€æ¨¡å‹
- en: Large Language Models (LLMs) have catalyzed significant advancements in fields
    such as computer vision (CV) and natural language processing (NLP), and more recently
    in time series analysisÂ Jin et al. ([2024](#bib.bib23)). LLMs, known for their
    exceptional generalization abilities, exhibit robust predictive performance, even
    when confronted with limited datasets. This characteristic is especially valuable
    in the context of MTSI. LLMs can adeptly mitigate these data gaps by leveraging
    multimodal knowledge, exemplified by their ability to incorporate additional textual
    information into analysesÂ Jin et al. ([2023](#bib.bib22)), thus generating multimodal
    embeddings. Such a modeling paradigm not only enriches the imputation process
    by providing a more holistic understanding and representation of the data but
    also expands the horizons of MTSI. It enables the inclusion of varied data sources,
    thereby facilitating a more detailed and context-aware imputation. Exploring the
    integration of LLMs in MTSI represents a promising direction, with the potential
    to significantly enhance the efficacy and efficiency of handling missing data
    in multivariate time series data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²åœ¨è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç­‰é¢†åŸŸå‚¬ç”Ÿäº†é‡è¦è¿›å±•ï¼Œæœ€è¿‘åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­ä¹Ÿå–å¾—äº†çªç ´Â Jin ç­‰ ([2024](#bib.bib23))ã€‚LLMs
    ä»¥å…¶å“è¶Šçš„æ³›åŒ–èƒ½åŠ›è€Œé—»åï¼Œå³ä¾¿åœ¨æ•°æ®é›†æœ‰é™çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½å±•ç°å‡ºå¼ºå¤§çš„é¢„æµ‹æ€§èƒ½ã€‚è¿™ä¸€ç‰¹æ€§åœ¨å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥ï¼ˆMTSIï¼‰çš„èƒŒæ™¯ä¸‹å°¤ä¸ºå®è´µã€‚LLMs èƒ½é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€çŸ¥è¯†æ¥å·§å¦™åœ°å¼¥è¡¥è¿™äº›æ•°æ®ç©ºç¼ºï¼Œå¦‚é€šè¿‡å°†é¢å¤–çš„æ–‡æœ¬ä¿¡æ¯çº³å…¥åˆ†æä¸­Â Jin
    ç­‰ ([2023](#bib.bib22))ï¼Œä»è€Œç”Ÿæˆå¤šæ¨¡æ€åµŒå…¥ã€‚è¿™ç§å»ºæ¨¡èŒƒå¼ä¸ä»…é€šè¿‡æä¾›æ›´å…¨é¢çš„ç†è§£å’Œæ•°æ®è¡¨ç¤ºæ¥ä¸°å¯Œæ’è¡¥è¿‡ç¨‹ï¼Œè¿˜æ‹“å±•äº† MTSI çš„è§†é‡ã€‚å®ƒä½¿å¾—åŒ…å«å¤šæ ·çš„æ•°æ®æºæˆä¸ºå¯èƒ½ï¼Œä»è€Œä¿ƒè¿›äº†æ›´è¯¦ç»†å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ’è¡¥ã€‚æ¢ç´¢
    LLMs åœ¨ MTSI ä¸­çš„é›†æˆæ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ï¼Œæœ‰æ½œåŠ›æ˜¾è‘—æé«˜å¤„ç†å¤šå˜é‡æ—¶é—´åºåˆ—æ•°æ®ä¸­ç¼ºå¤±æ•°æ®çš„æ•ˆèƒ½å’Œæ•ˆç‡ã€‚
- en: References
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: Alcaraz and Strodthoff [2023] JuanÂ Lopez Alcaraz and Nils Strodthoff. Diffusion-based
    time series imputation and forecasting with structured state space models. Transactions
    on Machine Learning Research, 2023.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alcaraz å’Œ Strodthoff [2023] Juan Lopez Alcaraz å’Œ Nils Strodthoffã€‚åŸºäºæ‰©æ•£çš„æ—¶é—´åºåˆ—æ’è¡¥å’Œé¢„æµ‹ä¸ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚ã€Šæœºå™¨å­¦ä¹ ç ”ç©¶æ‚å¿—ã€‹ï¼Œ2023å¹´ã€‚
- en: 'Alexandrov et al. [2020] Alexander Alexandrov, Konstantinos Benidis, Michael
    Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, etÂ al. GluonTS: Probabilistic
    and Neural Time Series Modeling in Python. Journal of Machine Learning Research,
    21(116):1â€“6, 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alexandrov ç­‰ [2020] Alexander Alexandrovã€Konstantinos Benidisã€Michael Bohlke-Schneiderã€Valentin
    Flunkertã€Jan Gasthaus ç­‰ã€‚ã€ŠGluonTSï¼šPythonä¸­çš„æ¦‚ç‡å’Œç¥ç»æ—¶é—´åºåˆ—å»ºæ¨¡ã€‹ã€‚æœºå™¨å­¦ä¹ ç ”ç©¶æ‚å¿—ï¼Œ21(116):1â€“6ï¼Œ2020å¹´ã€‚
- en: Altman [1992] NaomiÂ S Altman. An introduction to kernel and nearest-neighbor
    nonparametric regression. The American Statistician, 46(3):175â€“185, 1992.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Altman [1992] Naomi S Altmanã€‚æ ¸å‡½æ•°å’Œæœ€è¿‘é‚»éå‚æ•°å›å½’å¯¼è®ºã€‚ã€Šç¾å›½ç»Ÿè®¡å­¦å®¶ã€‹ï¼Œ46(3):175â€“185ï¼Œ1992å¹´ã€‚
- en: Amiri and Jensen [2016] Mehran Amiri and Richard Jensen. Missing data imputation
    using fuzzy-rough methods. Neurocomputing, 205(1):152â€“164, 2016.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amiri å’Œ Jensen [2016] Mehran Amiri å’Œ Richard Jensenã€‚ä½¿ç”¨æ¨¡ç³Š-ç²—ç³™æ–¹æ³•è¿›è¡Œç¼ºå¤±æ•°æ®æ’è¡¥ã€‚ã€Šç¥ç»è®¡ç®—ã€‹ï¼Œ205(1):152â€“164ï¼Œ2016å¹´ã€‚
- en: Bai and Ng [2008] Jushan Bai and Serena Ng. Forecasting economic time series
    using targeted predictors. Journal of Econometrics, 146(2):304â€“317, 2008.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai å’Œ Ng [2008] Jushan Bai å’Œ Serena Ngã€‚ä½¿ç”¨ç›®æ ‡é¢„æµ‹å˜é‡é¢„æµ‹ç»æµæ—¶é—´åºåˆ—ã€‚ã€Šè®¡é‡ç»æµå­¦æ‚å¿—ã€‹ï¼Œ146(2):304â€“317ï¼Œ2008å¹´ã€‚
- en: Bansal et al. [2021] Parikshit Bansal, Prathamesh Deshpande, and Sunita Sarawagi.
    Missing value imputation on multidimensional time series. In VLDB, 2021.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bansal et al. [2021] Parikshit Bansalã€Prathamesh Deshpande å’Œ Sunita Sarawagi.
    å¤šç»´æ—¶é—´åºåˆ—çš„ç¼ºå¤±å€¼æ’è¡¥. è§ VLDB, 2021å¹´.
- en: Bartholomew [1971] DavidÂ J Bartholomew. Time series analysis forecasting and
    control. Journal of the Operational Research Society, 22(2):199â€“201, 1971.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bartholomew [1971] David J Bartholomew. æ—¶é—´åºåˆ—åˆ†æé¢„æµ‹ä¸æ§åˆ¶. Journal of the Operational
    Research Society, 22(2):199â€“201, 1971å¹´.
- en: BiloÅ¡ et al. [2023] Marin BiloÅ¡, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka,
    and Stephan GÃ¼nnemann. Modeling temporal data as continuous functions with stochastic
    process diffusion. In ICML, 2023.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BiloÅ¡ et al. [2023] Marin BiloÅ¡ã€Kashif Rasulã€Anderson Schneiderã€Yuriy Nevmyvaka
    å’Œ Stephan GÃ¼nnemann. å°†æ—¶é—´æ•°æ®å»ºæ¨¡ä¸ºå…·æœ‰éšæœºè¿‡ç¨‹æ‰©æ•£çš„è¿ç»­å‡½æ•°. è§ ICML, 2023å¹´.
- en: 'Cao et al. [2018] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan
    Li. Brits: Bidirectional recurrent imputation for time series. NeurIPS, 2018.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao et al. [2018] æ›¹ä¼Ÿã€è‘£æœ›ã€æå‰‘ã€éƒèˆŸã€æç£Š å’Œ æè‰ºè°­. Brits: åŒå‘é€’å½’æ’è¡¥æ—¶é—´åºåˆ—. NeurIPS, 2018å¹´.'
- en: Che et al. [2018] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag,
    and Yan Liu. Recurrent neural networks for multivariate time series with missing
    values. Scientific Reports, 8(1), Apr 2018.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Che et al. [2018] éƒ‘å¹³è½¦ã€Sanjay Purushothamã€Kyunghyun Choã€David Sontag å’Œ Yan Liu.
    é’ˆå¯¹ç¼ºå¤±å€¼çš„å¤šå˜é‡æ—¶é—´åºåˆ—çš„é€’å½’ç¥ç»ç½‘ç»œ. Scientific Reports, 8(1), 2018å¹´4æœˆ.
- en: Chen et al. [2023] YuÂ Chen, Wei Deng, Shikai Fang, Fengpei Li, NicoleÂ Tianjiao
    Yang, Yikai Zhang, etÂ al. Provably convergent schrÃ¶dinger bridge with applications
    to probabilistic time series imputation. In ICML, 2023.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023] é™ˆå®‡ã€é­ç™»ã€çŸ³å¼€æ–¹ã€å†¯ä½©æã€Nicole Tianjiao Yangã€æ˜“å‡¯å¼  ç­‰. å¯è¯æ˜æ”¶æ•›çš„è–›å®šè°”æ¡¥åŠå…¶åœ¨æ¦‚ç‡æ—¶é—´åºåˆ—æ’è¡¥ä¸­çš„åº”ç”¨.
    è§ ICML, 2023å¹´.
- en: 'Cini et al. [2022] Andrea Cini, Ivan Marisca, and Cesare Alippi. Filling the
    g_ap_s: Multivariate time series imputation by graph neural networks. In ICLR,
    2022.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cini et al. [2022] Andrea Ciniã€Ivan Marisca å’Œ Cesare Alippi. å¡«è¡¥ç¼ºå£: é€šè¿‡å›¾ç¥ç»ç½‘ç»œè¿›è¡Œå¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥.
    è§ ICLR, 2022å¹´.'
- en: 'Du et al. [2023] Wenjie Du, David Cote, and Yan Liu. SAITS: Self-Attention-based
    Imputation for Time Series. Expert Systems with Applications, 219:119619, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du et al. [2023] æœæ–‡æ°ã€David Cote å’Œ Yan Liu. SAITS: åŸºäºè‡ªæ³¨æ„åŠ›çš„æ—¶é—´åºåˆ—æ’è¡¥. Expert Systems
    with Applications, 219:119619, 2023å¹´.'
- en: 'Du [2023] Wenjie Du. PyPOTS: a Python toolbox for data mining on Partially-Observed
    Time Series. In SIGKDD workshop on Mining and Learning from Time Series, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du [2023] æœæ–‡æ°. PyPOTS: ä¸€æ¬¾ç”¨äºéƒ¨åˆ†è§‚å¯Ÿæ—¶é—´åºåˆ—æ•°æ®æŒ–æ˜çš„Pythonå·¥å…·ç®±. è§ SIGKDD æ—¶é—´åºåˆ—æŒ–æ˜ä¸å­¦ä¹ ç ”è®¨ä¼š, 2023å¹´.'
- en: Esteban et al. [2017] CristÃ³bal Esteban, StephanieÂ L Hyland, and Gunnar RÃ¤tsch.
    Real-valued (medical) time series generation with recurrent conditional gans.
    ArXiv Preprint ArXiv:1706.02633, 2017.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esteban et al. [2017] CristÃ³bal Estebanã€Stephanie L Hyland å’Œ Gunnar RÃ¤tsch.
    ä½¿ç”¨é€’å½’æ¡ä»¶GANsç”Ÿæˆå®å€¼ï¼ˆåŒ»ç–—ï¼‰æ—¶é—´åºåˆ—. ArXiv é¢„å°æœ¬ ArXiv:1706.02633, 2017å¹´.
- en: 'Fang and Wang [2020] Chenguang Fang and Chen Wang. Time series data imputation:
    A survey on deep learning approaches. arXiv preprint arXiv:2011.11347, 2020.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fang and Wang [2020] æ–¹ç¨‹å…‰ å’Œ ç‹æ™¨. æ—¶é—´åºåˆ—æ•°æ®æ’è¡¥: æ·±åº¦å­¦ä¹ æ–¹æ³•ç»¼è¿°. arXiv é¢„å°æœ¬ arXiv:2011.11347,
    2020å¹´.'
- en: 'Fortuin et al. [2020] Vincent Fortuin, Dmitry Baranchuk, Gunnar Raetsch, and
    Stephan Mandt. GP-VAE: Deep probabilistic time series imputation. In AISTATS,
    2020.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fortuin et al. [2020] Vincent Fortuinã€Dmitry Baranchukã€Gunnar Raetsch å’Œ Stephan
    Mandt. GP-VAE: æ·±åº¦æ¦‚ç‡æ—¶é—´åºåˆ—æ’è¡¥. è§ AISTATS, 2020å¹´.'
- en: Gong et al. [2021] Yongshun Gong, Zhibin Li, Jian Zhang, Wei Liu, Yilong Yin,
    and YuÂ Zheng. Missing value imputation for multi-view urban statistical data via
    spatial correlation learning. IEEE Transactions on Knowledge and Data Engineering,
    2021.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong et al. [2021] é’Ÿæ°¸é¡ºã€Zhibin Liã€Jian Zhangã€Wei Liuã€Yilong Yin å’Œ Yu Zheng. é€šè¿‡ç©ºé—´ç›¸å…³å­¦ä¹ å¯¹å¤šè§†è§’åŸå¸‚ç»Ÿè®¡æ•°æ®è¿›è¡Œç¼ºå¤±å€¼æ’è¡¥.
    IEEE Transactions on Knowledge and Data Engineering, 2021å¹´.
- en: Gu et al. [2022] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling
    long sequences with structured state spaces. In ICLR, 2022.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. [2022] Albert Guã€Karan Goel å’Œ Christopher Re. ä½¿ç”¨ç»“æ„çŠ¶æ€ç©ºé—´é«˜æ•ˆå»ºæ¨¡é•¿åºåˆ—. è§ ICLR,
    2022å¹´.
- en: HamzaÃ§ebi [2008] CoÅŸkun HamzaÃ§ebi. Improving artificial neural networksâ€™ performance
    in seasonal time series forecasting. Information Sciences, 178(23):4550â€“4559,
    2008.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HamzaÃ§ebi [2008] CoÅŸkun HamzaÃ§ebi. æå‡äººå·¥ç¥ç»ç½‘ç»œåœ¨å­£èŠ‚æ€§æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„è¡¨ç°. Information Sciences,
    178(23):4550â€“4559, 2008å¹´.
- en: 'Ibrahim et al. [2012] JosephÂ G Ibrahim, Haitao Chu, and Ming-Hui Chen. Missing
    data in clinical studies: issues and methods. Journal of clinical oncology, 2012.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ibrahim et al. [2012] Joseph G Ibrahimã€Haitao Chu å’Œ Ming-Hui Chen. ä¸´åºŠç ”ç©¶ä¸­çš„ç¼ºå¤±æ•°æ®:
    é—®é¢˜ä¸æ–¹æ³•. Journal of clinical oncology, 2012å¹´.'
- en: 'Jin et al. [2023] Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao
    Xue, Xue Wang, James Zhang, YiÂ Wang, Haifeng Chen, Xiaoli Li, etÂ al. Large models
    for time series and spatio-temporal data: A survey and outlook. arXiv preprint
    arXiv:2310.10196, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin ç­‰äºº [2023] é‡‘é“­ã€æ¸©é’æ¾ã€æ¢å®‡è½©ã€å¼ è¶…åŠ›ã€è–›æ€ä¹”ã€ç‹é›ªã€å¼ è©¹å§†æ–¯ã€ç‹æ€¡ã€é™ˆæµ·å³°ã€ææ™“ä¸½ç­‰ã€‚å¤§æ¨¡å‹ç”¨äºæ—¶é—´åºåˆ—å’Œæ—¶ç©ºæ•°æ®ï¼šç»¼è¿°ä¸å±•æœ›ã€‚arXiv
    é¢„å°æœ¬ arXiv:2310.10196ï¼Œ2023å¹´ã€‚
- en: 'Jin et al. [2024] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, JamesÂ Y Zhang,
    Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong
    Wen. Time-LLM: Time series forecasting by reprogramming large language models.
    In ICLR, 2024.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin ç­‰äºº [2024] é‡‘é“­ã€ç‹è¯—é›¨ã€é©¬ç³æ¶›ã€æœ±å¿—è½©ã€è©¹å§†æ–¯Â·YÂ·å¼ ã€çŸ³æ™“æ˜ã€é™ˆå“å®‡ã€æ¢å®‡è½©ã€æå…ƒèŠ³ã€æ½˜è¯—é”å’Œæ¸©é’æ¾ã€‚Time-LLMï¼šé€šè¿‡é‡æ–°ç¼–ç¨‹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚è§
    ICLRï¼Œ2024å¹´ã€‚
- en: 'Khayati et al. [2020] Mourad Khayati, Alberto Lerner, Zakhar Tymchenko, and
    Philippe CudrÃ©-Mauroux. Mind the gap: an experimental evaluation of imputation
    of missing values techniques in time series. In VLDB, 2020.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khayati ç­‰äºº [2020] ç©†æ‹‰å¾·Â·å“ˆäºšæã€é˜¿å°”è´æ‰˜Â·å‹’å†…å°”ã€æ‰å“ˆå°”Â·å­£å§†é’¦ç§‘å’Œè²åˆ©æ™®Â·å¤å¾·é›·-è«ç½—ã€‚ç•™æ„å·®è·ï¼šæ—¶é—´åºåˆ—ä¸­ç¼ºå¤±å€¼æ’è¡¥æŠ€æœ¯çš„å®éªŒè¯„ä¼°ã€‚è§
    VLDBï¼Œ2020å¹´ã€‚
- en: 'Kim and Chi [2018] YeoÂ Jin Kim and Min Chi. Temporal Belief Memory: Imputing
    missing data during rnn training. In IJCAI, 2018.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim å’Œ Chi [2018] é‡‘æ±çå’Œæ± æ•ã€‚æ—¶é—´ä¿¡å¿µè®°å¿†ï¼šåœ¨ RNN è®­ç»ƒæœŸé—´æ’è¡¥ç¼ºå¤±æ•°æ®ã€‚è§ IJCAIï¼Œ2018å¹´ã€‚
- en: Kim et al. [2023] Seunghyun Kim, Hyunsu Kim, Eunggu Yun, Hwangrae Lee, Jaehun
    Lee, and Juho Lee. Probabilistic imputation for time-series classification with
    missing data. In ICML, 2023.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim ç­‰äºº [2023] é‡‘æ‰¿ç‚«ã€é‡‘è´¤ç§€ã€å°¹æ©ä¹…ã€æè¾‰ç‘ã€æåœ¨å‹‹å’Œæå‘¨æµ©ã€‚é’ˆå¯¹ç¼ºå¤±æ•°æ®çš„æ—¶é—´åºåˆ—åˆ†ç±»çš„æ¦‚ç‡æ’è¡¥ã€‚è§ ICMLï¼Œ2023å¹´ã€‚
- en: 'Kyono et al. [2021] Trent Kyono, Yao Zhang, Alexis Bellot, and Mihaela vanÂ der
    Schaar. Miracle: Causally-aware imputation via learning missing data mechanisms.
    In NeurIPS, 2021.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kyono ç­‰äºº [2021] ç‰¹ä¼¦ç‰¹Â·äº¬é‡ã€å¼ è€€ã€äºšå†å…‹è¥¿æ–¯Â·è´æ´›ç‰¹å’Œç±³å“ˆä¼Šæ‹‰Â·èŒƒå¾·Â·å¤å°”ã€‚å¥‡è¿¹ï¼šé€šè¿‡å­¦ä¹ ç¼ºå¤±æ•°æ®æœºåˆ¶çš„å› æœæ„ŸçŸ¥æ’è¡¥ã€‚è§ NeurIPSï¼Œ2021å¹´ã€‚
- en: Li et al. [2023] Xiao Li, Huan Li, Harry Kai-Ho Chan, Hua Lu, and ChristianÂ S
    Jensen. Data imputation for sparse radio maps in indoor positioning. In ICDE,
    2023.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li ç­‰äºº [2023] ææ™“ã€ææ¬¢ã€å“ˆé‡ŒÂ·å‡¯-éœÂ·é™ˆã€å¢åå’Œå…‹é‡Œæ–¯è’‚å®‰Â·SÂ·è©¹æ£®ã€‚å®¤å†…å®šä½ä¸­ç¨€ç–æ— çº¿ç”µåœ°å›¾çš„æ•°æ®æ’è¡¥ã€‚è§ ICDEï¼Œ2023å¹´ã€‚
- en: Little and Rubin [2019] RoderickÂ JA Little and DonaldÂ B Rubin. Statistical analysis
    with missing data, volume 793. John Wiley & Sons, 2019.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Little å’Œ Rubin [2019] ç½—å¾·é‡Œå…‹Â·JAÂ·åˆ©ç‰¹å°”å’Œå”çº³å¾·Â·BÂ·é²å®¾ã€‚ç¼ºå¤±æ•°æ®çš„ç»Ÿè®¡åˆ†æï¼Œç¬¬793å·ã€‚çº¦ç¿°Â·å¨åˆ©çˆ¶å­å…¬å¸ï¼Œ2019å¹´ã€‚
- en: 'Liu et al. [2019] Yukai Liu, Rose Yu, Stephan Zheng, Eric Zhan, and Yisong
    Yue. Naomi: Non-autoregressive multiresolution sequence imputation. In NeurIPS,
    2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu ç­‰äºº [2019] åˆ˜ç‰å‡¯ã€äºç«ã€éƒ‘æ–¯å¦ã€è©¹åŠ›å…‹å’Œå²³ä¸€æ¾ã€‚Naomiï¼šéè‡ªå›å½’å¤šåˆ†è¾¨ç‡åºåˆ—æ’è¡¥ã€‚è§ NeurIPSï¼Œ2019å¹´ã€‚
- en: Liu et al. [2022] Shuai Liu, Xiucheng Li, Gao Cong, Yile Chen, and Yue Jiang.
    Multivariate time-series imputation with disentangled temporal representations.
    2022.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu ç­‰äºº [2022] æ–½å¸…Â·åˆ˜ã€æç§€åŸã€é¾šé«˜ã€é™ˆä¸€ä¹å’Œå§œæœˆã€‚å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥ä¸è§£è€¦çš„æ—¶é—´è¡¨ç¤ºã€‚2022å¹´ã€‚
- en: 'Liu et al. [2023] Mingzhe Liu, Han Huang, Hao Feng, Leilei Sun, Bowen Du, and
    Yanjie Fu. Pristi: A conditional diffusion framework for spatiotemporal imputation.
    arXiv preprint arXiv:2302.09746, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu ç­‰äºº [2023] åˆ˜æ˜å“²ã€é»„å¯’ã€å†¯æµ©ã€å­™ç£Šç£Šã€æœåšæ–‡å’Œå‚…å»¶æ°ã€‚Pristiï¼šç”¨äºæ—¶ç©ºæ’è¡¥çš„æ¡ä»¶æ‰©æ•£æ¡†æ¶ã€‚arXiv é¢„å°æœ¬ arXiv:2302.09746ï¼Œ2023å¹´ã€‚
- en: 'LÃ¶ning et al. [2019] Markus LÃ¶ning, Anthony Bagnall, Sajaysurya Ganesh, Viktor
    Kazakov, Jason Lines, etÂ al. sktime: A unified interface for machine learning
    with time series. arXiv preprint arXiv:1909.07872, 2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LÃ¶ning ç­‰äºº [2019] é©¬åº“æ–¯Â·å‹’å®ã€å®‰ä¸œå°¼Â·å·´æ ¼çº³å°”ã€è¨è´¾è‹åˆ©äºšÂ·åŠ å†…ä»€ã€ç»´å…‹æ‰˜Â·å¡æ‰ç§‘å¤«ã€æ°æ£®Â·è±æ©æ–¯ç­‰ã€‚sktimeï¼šç”¨äºæ—¶é—´åºåˆ—çš„ç»Ÿä¸€æœºå™¨å­¦ä¹ æ¥å£ã€‚arXiv
    é¢„å°æœ¬ arXiv:1909.07872ï¼Œ2019å¹´ã€‚
- en: 'Lugmayr et al. [2022] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
    Yu, Radu Timofte, and Luc VanÂ Gool. Repaint: Inpainting using denoising diffusion
    probabilistic models. In CVPR, 2022.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lugmayr ç­‰äºº [2022] å®‰å¾·çƒˆäºšæ–¯Â·å¢æ ¼æ¢…å°”ã€é©¬ä¸Â·è¾¾å†…å°”è©¹ã€å®‰å¾·çƒˆæ–¯Â·ç½—æ¢…ç½—ã€è´¹èˆå°”Â·ä½™ã€æ‹‰æœÂ·è’‚è«å¤«ç‰¹å’Œå¢å…‹Â·èŒƒÂ·å¤å°”ã€‚Repaintï¼šä½¿ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹è¿›è¡Œå›¾åƒä¿®å¤ã€‚è§
    CVPRï¼Œ2022å¹´ã€‚
- en: Luo et al. [2018] Yonghong Luo, Xiangrui Cai, Ying ZHANG, Jun Xu, and Yuan xiaojie.
    Multivariate time series imputation with generative adversarial networks. In NeurIPS,
    2018.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo ç­‰äºº [2018] ç½—æ°¸çº¢ã€è”¡å‘ç¿ã€å¼ é¢–ã€å¾å†›å’Œè¢å°æ°ã€‚åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„å¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥ã€‚è§ NeurIPSï¼Œ2018å¹´ã€‚
- en: 'Luo et al. [2019] Yonghong Luo, Ying Zhang, Xiangrui Cai, and Xiaojie Yuan.
    EÂ²GAN: End-to-end generative adversarial network for multivariate time series
    imputation. In IJCAI, 2019.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo ç­‰äºº [2019] ç½—æ°¸çº¢ã€å¼ é¢–ã€è”¡å‘ç¿å’Œè¢å°æ°ã€‚EÂ²GANï¼šç«¯åˆ°ç«¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç”¨äºå¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥ã€‚è§ IJCAIï¼Œ2019å¹´ã€‚
- en: 'Ma et al. [2019] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony
    Vetro, and Shih-Fu Chang. CDSA: cross-dimensional self-attention for multivariate,
    geo-tagged time series imputation. arXiv preprint arXiv:1905.09904, 2019.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2019] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony
    Vetro, å’Œ Shih-Fu Changã€‚CDSAï¼šç”¨äºå¤šå˜é‡ã€åœ°ç†æ ‡ç­¾æ—¶é—´åºåˆ—æ’è¡¥çš„è·¨ç»´è‡ªæ³¨æ„åŠ›ã€‚arXiv é¢„å°æœ¬ arXiv:1905.09904ï¼Œ2019å¹´ã€‚
- en: Marisca et al. [2022] Ivan Marisca, Andrea Cini, and Cesare Alippi. Learning
    to reconstruct missing data from spatiotemporal graphs with sparse observations.
    NeurIPS, 2022.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marisca et al. [2022] Ivan Marisca, Andrea Cini, å’Œ Cesare Alippiã€‚ä»ç¨€ç–è§‚æµ‹çš„æ—¶ç©ºå›¾ä¸­å­¦ä¹ é‡å»ºç¼ºå¤±æ•°æ®ã€‚å‘è¡¨äº
    NeurIPSï¼Œ2022å¹´ã€‚
- en: Miao et al. [2021] Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao,
    and Jianwei Yin. Generative semi-supervised learning for multivariate time series
    imputation. In AAAI, 2021.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao et al. [2021] Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao,
    å’Œ Jianwei Yinã€‚ç”¨äºå¤šå˜é‡æ—¶é—´åºåˆ—æ’è¡¥çš„ç”ŸæˆåŠç›‘ç£å­¦ä¹ ã€‚å‘è¡¨äº AAAIï¼Œ2021å¹´ã€‚
- en: Miyaguchi et al. [2021] Kohei Miyaguchi, Takayuki Katsuki, Akira Koseki, and
    Toshiya Iwamori. Variational inference for discriminative learning with generative
    modeling of feature incompletion. In ICLR, 2021.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miyaguchi et al. [2021] Kohei Miyaguchi, Takayuki Katsuki, Akira Koseki, å’Œ Toshiya
    Iwamoriã€‚ç‰¹å¾ç¼ºå¤±ç”Ÿæˆå»ºæ¨¡çš„åˆ¤åˆ«å­¦ä¹ å˜åˆ†æ¨æ–­ã€‚å‘è¡¨äº ICLRï¼Œ2021å¹´ã€‚
- en: '[41] Steffen Moritz and Thomas Bartz-Beielstein. imputeTS: Time Series Missing
    Value Imputation in R. The R Journal.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Steffen Moritz å’Œ Thomas Bartz-Beielsteinã€‚imputeTSï¼šRè¯­è¨€ä¸­çš„æ—¶é—´åºåˆ—ç¼ºå¤±å€¼æ’è¡¥ã€‚R Journalã€‚'
- en: Mulyadi et al. [2021] AhmadÂ Wisnu Mulyadi, Eunji Jun, and Heung-Il Suk. Uncertainty-aware
    variational-recurrent imputation network for clinical time series. IEEE Transactions
    on Cybernetics, 52(9):9684â€“9694, 2021.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mulyadi et al. [2021] Ahmad Wisnu Mulyadi, Eunji Jun, å’Œ Heung-Il Sukã€‚é¢å‘ä¸´åºŠæ—¶é—´åºåˆ—çš„å…·æœ‰ä¸ç¡®å®šæ€§æ„è¯†çš„å˜åˆ†é€’å½’æ’è¡¥ç½‘ç»œã€‚IEEE
    ç½‘ç»œç§‘å­¦ä¸å·¥ç¨‹äº¤æ˜“ï¼Œ52(9):9684â€“9694ï¼Œ2021å¹´ã€‚
- en: 'Nakagawa [2015] Shinichi Nakagawa. Missing data: mechanisms, methods and messages.
    pages 81â€“105\. Oxford University Press Oxford, UK, 2015.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakagawa [2015] Shinichi Nakagawaã€‚ç¼ºå¤±æ•°æ®ï¼šæœºåˆ¶ã€æ–¹æ³•ä¸ä¿¡æ¯ã€‚ç¬¬81â€“105é¡µã€‚ç‰›æ´¥å¤§å­¦å‡ºç‰ˆç¤¾ï¼Œè‹±å›½ç‰›æ´¥ï¼Œ2015å¹´ã€‚
- en: Rubin [1976] DonaldÂ B. Rubin. Inference and missing data. Biometrika, 63(3):581â€“592,
    12 1976.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubin [1976] Donald B. Rubinã€‚æ¨æ–­ä¸ç¼ºå¤±æ•°æ®ã€‚ç”Ÿç‰©ç»Ÿè®¡å­¦ï¼Œ63(3):581â€“592ï¼Œ1976å¹´12æœˆã€‚
- en: Ruiz et al. [2023] Joaquin Ruiz, Hau-tieng Wu, and MarceloÂ A Colominas. Enhancing
    missing data imputation of non-stationary signals with harmonic decomposition.
    arXiv preprint arXiv:2309.04630, 2023.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruiz et al. [2023] Joaquin Ruiz, Hau-tieng Wu, å’Œ Marcelo A Colominasã€‚é€šè¿‡è°æ³¢åˆ†è§£å¢å¼ºéå¹³ç¨³ä¿¡å·çš„ç¼ºå¤±æ•°æ®æ’è¡¥ã€‚arXiv
    é¢„å°æœ¬ arXiv:2309.04630ï¼Œ2023å¹´ã€‚
- en: 'Shan et al. [2023] Siyuan Shan, Yang Li, and JunierÂ B. Oliva. Nrtsi: Non-recurrent
    time series imputation. In ICASSP, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shan et al. [2023] Siyuan Shan, Yang Li, å’Œ Junier B. Olivaã€‚Nrtsiï¼šéé€’å½’æ—¶é—´åºåˆ—æ’è¡¥ã€‚å‘è¡¨äº
    ICASSPï¼Œ2023å¹´ã€‚
- en: 'Silva et al. [2012] Ikaro Silva, George Moody, DanielÂ J Scott, LeoÂ A Celi,
    and RogerÂ G Mark. Predicting in-hospital mortality of icu patients: The physionet/computing
    in cardiology challenge 2012. Computing in cardiology, 39:245, 2012.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silva et al. [2012] Ikaro Silva, George Moody, Daniel J Scott, Leo A Celi, å’Œ
    Roger G Markã€‚é¢„æµ‹ ICU æ‚£è€…çš„é™¢å†…æ­»äº¡ç‡ï¼šPhysioNet/è®¡ç®—å¿ƒè„ç—…æŒ‘æˆ˜2012ã€‚è®¡ç®—å¿ƒè„ç—…å­¦ï¼Œ39:245ï¼Œ2012å¹´ã€‚
- en: 'Tashiro et al. [2021] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano
    Ermon. CSDI: Conditional score-based diffusion models for probabilistic time series
    imputation. In NeurIPS, 2021.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tashiro et al. [2021] Yusuke Tashiro, Jiaming Song, Yang Song, å’Œ Stefano Ermonã€‚CSDIï¼šç”¨äºæ¦‚ç‡æ—¶é—´åºåˆ—æ’è¡¥çš„æ¡ä»¶å¾—åˆ†åŸºæ‰©æ•£æ¨¡å‹ã€‚å‘è¡¨äº
    NeurIPSï¼Œ2021å¹´ã€‚
- en: 'VanÂ Buuren and Groothuis-Oudshoorn [2011] Stef VanÂ Buuren and Karin Groothuis-Oudshoorn.
    mice: Multivariate imputation by chained equations in r. Journal of statistical
    software, 45:1â€“67, 2011.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Buuren å’Œ Groothuis-Oudshoorn [2011] Stef Van Buuren å’Œ Karin Groothuis-Oudshoornã€‚miceï¼šRè¯­è¨€ä¸­çš„é“¾å¼æ–¹ç¨‹å¤šé‡æ’è¡¥ã€‚ç»Ÿè®¡è½¯ä»¶æ‚å¿—ï¼Œ45:1â€“67ï¼Œ2011å¹´ã€‚
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, AidanÂ N Gomez, etÂ al. Attention is all you need. In NeurIPS, 2017.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez ç­‰ã€‚æ³¨æ„åŠ›å³ä½ æ‰€éœ€ã€‚å‘è¡¨äº NeurIPSï¼Œ2017å¹´ã€‚
- en: Wang et al. [2023] XuÂ Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu
    Wang, Zhengyang Zhou, and Yang Wang. An observed value consistent diffusion model
    for imputing missing values in multivariate time series. In SIGKDD, 2023.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023] Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu
    Wang, Zhengyang Zhou, å’Œ Yang Wangã€‚ç”¨äºæ’è¡¥å¤šå˜é‡æ—¶é—´åºåˆ—ç¼ºå¤±å€¼çš„è§‚æµ‹å€¼ä¸€è‡´æ‰©æ•£æ¨¡å‹ã€‚å‘è¡¨äº SIGKDDï¼Œ2023å¹´ã€‚
- en: 'Wen et al. [2023] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing
    Ma, Junchi Yan, and Liang Sun. Transformers in time series: A survey. In International
    Joint Conference on Artificial Intelligence(IJCAI), 2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen et al. [2023] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing
    Ma, Junchi Yan, å’Œ Liang Sunã€‚æ—¶é—´åºåˆ—ä¸­çš„å˜æ¢å™¨ï¼šç»¼è¿°ã€‚å‘è¡¨äºå›½é™…äººå·¥æ™ºèƒ½è”åˆä¼šè®®(IJCAI)ï¼Œ2023å¹´ã€‚
- en: 'Wu et al. [2023a] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and
    Mingsheng Long. TimesNet: Temporal 2D-Variation Modeling for General Time Series
    Analysis. In ICLR, 2023.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å´ç­‰äºº [2023a] å´æµ·æ—­ã€èƒ¡è…¾å“¥ã€åˆ˜æ°¸ã€å‘¨èˆªã€ç‹å»ºæ•å’Œé¾™åç”Ÿã€‚TimesNetï¼šé€šç”¨æ—¶é—´åºåˆ—åˆ†æçš„æ—¶åº2Då˜æ¢å»ºæ¨¡ã€‚å‘è¡¨äºICLRï¼Œ2023ã€‚
- en: Wu et al. [2023b] Yangyang Wu, Jun Wang, Xiaoye Miao, Wenjia Wang, and Jianwei
    Yin. Differentiable and scalable generative adversarial models for data imputation.
    IEEE Transactions on Knowledge and Data Engineering, 2023.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å´ç­‰äºº [2023b] å´é˜³é˜³ã€ç‹å†›ã€è‹—æ™“å¶ã€ç‹æ–‡ä½³å’Œå°¹å»ºä¼Ÿã€‚å¯å¾®åˆ†å’Œå¯æ‰©å±•çš„ç”Ÿæˆå¯¹æŠ—æ¨¡å‹ç”¨äºæ•°æ®æ’è¡¥ã€‚IEEEçŸ¥è¯†ä¸æ•°æ®å·¥ç¨‹å­¦æŠ¥ï¼Œ2023ã€‚
- en: Xu et al. [2023] Jingwen Xu, Fei Lyu, and PongÂ C Yuen. Density-aware temporal
    attentive step-wise diffusion model for medical time series imputation. In CIKM,
    2023.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¾ç­‰äºº [2023] æœ±ç»æ–‡ã€å•é£å’Œè¢é¹**C Y**ã€‚é¢å‘å¯†åº¦çš„æ—¶åºæ³¨æ„åŠ›é€æ­¥æ‰©æ•£æ¨¡å‹ç”¨äºåŒ»å­¦æ—¶é—´åºåˆ—æ’è¡¥ã€‚å‘è¡¨äºCIKMï¼Œ2023ã€‚
- en: Yoon et al. [2019] Jinsung Yoon, WilliamÂ R. Zame, and Mihaela vanÂ der Schaar.
    Estimating missing data in temporal data streams using multi-directional recurrent
    neural networks. IEEE Trans. on Biomedical Engineering, 2019.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°¹ç­‰äºº [2019] å°¹æŒ¯ç”Ÿã€å¨å»‰**R.** æ‰æ¢…å’Œç±³å“ˆä¼Šæ‹‰Â·èŒƒå¾·Â·æ²™å°”ã€‚ä½¿ç”¨å¤šæ–¹å‘é€’å½’ç¥ç»ç½‘ç»œä¼°è®¡æ—¶é—´æ•°æ®æµä¸­çš„ç¼ºå¤±æ•°æ®ã€‚IEEEç”Ÿç‰©åŒ»å­¦å·¥ç¨‹å­¦æŠ¥ï¼Œ2019ã€‚
- en: 'Zhang et al. [2017] Shuyi Zhang, Bin Guo, Anlan Dong, Jing He, Ziping Xu, and
    S.Â Chen. Cautionary tales on air-quality improvement in beijing. Proceedings of
    the Royal Society A: Mathematical, Physical and Engineering Sciences, 473, 2017.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼ ç­‰äºº [2017] å¼ ä¹¦å®œã€éƒ­æ–Œã€è‘£å®‰å…°ã€ä½•é™ã€å¾è‡ªå¹³å’Œ**S.** é™ˆã€‚å…³äºåŒ—äº¬ç©ºæ°”è´¨é‡æ”¹å–„çš„è­¦ç¤ºæ•…äº‹ã€‚çš‡å®¶å­¦ä¼šAè¾‘ï¼šæ•°å­¦ã€ç‰©ç†å’Œå·¥ç¨‹ç§‘å­¦ï¼Œ473ï¼Œ2017ã€‚
- en: 'Zhou et al. [2021] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin
    Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long
    sequence time-series forecasting. In AAAI, 2021.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‘¨ç­‰äºº [2021] å‘¨æµ©æ¯…ã€å¼ ä¸Šèˆªã€å½­æ°é½ã€å¼ å¸…ã€æå»ºé‘«ã€ç†Šè¾‰å’Œå¼ ä¸‡æ‰ã€‚Informerï¼šè¶…è¶Šé«˜æ•ˆTransformerçš„é•¿åºåˆ—æ—¶é—´åºåˆ—é¢„æµ‹ã€‚å‘è¡¨äºAAAIï¼Œ2021ã€‚
