- en: 'Machine Learning 1: Lesson 10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习1：第10课
- en: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-10-6ff502b2db45](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-10-6ff502b2db45)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-10-6ff502b2db45](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-10-6ff502b2db45)
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我个人从* [*机器学习课程*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*中的笔记。随着我继续审查课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*
    [*Jeremy*](https://twitter.com/jeremyphoward) *和* [*Rachel*](https://twitter.com/math_rachel)
    *给了我这个学习的机会。*'
- en: Fast AI on pip [[0:00](https://youtu.be/37sFIak42Sc)]
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: pip上的Fast AI [[0:00](https://youtu.be/37sFIak42Sc)]
- en: 'Welcome back to machine learning! Certainly the most exciting thing this week
    is that Fast AI is now on pip, so you can `pip install fastai`:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎回到机器学习！这周最令人兴奋的事情当然是Fast AI现在在pip上了，所以你可以`pip install fastai`：
- en: '[](https://pypi.org/project/fastai/?source=post_page-----6ff502b2db45--------------------------------)
    [## fastai'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pypi.org/project/fastai/?source=post_page-----6ff502b2db45--------------------------------)
    [## fastai'
- en: fastai makes deep learning with PyTorch faster, more accurate, and easier
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: fastai使得使用PyTorch进行深度学习更快、更准确、更容易
- en: pypi.org](https://pypi.org/project/fastai/?source=post_page-----6ff502b2db45--------------------------------)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: pypi.org](https://pypi.org/project/fastai/?source=post_page-----6ff502b2db45--------------------------------)
- en: It’s probably still easiest just to do the `conda env update` but a couple of
    places it would be handy instead to `pip install fastai` would be if you are working
    outside of the repo in the notebooks, then this gives you access to Fast AI everywhere.
    Also they submitted a pull request to Kaggle to try and get it added to the Kaggle
    kernels. So hopefully you’ll be able to use it on Kaggle kernels soon. You can
    use it at your work or whatever else, so that’s exciting. I’m not gonna say it’s
    officially released yet. It’s still very early, obviously and we are still adding
    (and you’re helping add) documentation and all that kind of stuff. But it’s great
    that that’s now there.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法可能仍然是执行`conda env update`，但有几个地方更方便的是执行`pip install fastai`，如果你在笔记本之外的地方工作，那么这将使你在任何地方都可以访问Fast
    AI。他们还向Kaggle提交了一个拉取请求，试图将其添加到Kaggle内核中。所以希望你很快就能在Kaggle内核上使用它。你可以在工作中或其他地方使用它，所以这很令人兴奋。我不会说它已经正式发布了。显然，现在还很早，我们仍在添加（你也在帮助添加）文档和所有这些东西。但很高兴现在有这个。
- en: Kaggle Kernels [[1:22](https://youtu.be/37sFIak42Sc?t=82)]
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kaggle内核 [[1:22](https://youtu.be/37sFIak42Sc?t=82)]
- en: A couple of cool kernels from USF students this week. I thought I’d highlight
    two that were both from the text normalization competition which was about trying
    to take text which was written out standard English text, there’s also one for
    Russian. You’re trying to identify things that could be “first, second, third”
    and say that’s a cardinal number or this is a phone number or whatever. I did
    a quick little bit of searching and I saw that there had been some attempts in
    academia to use deep learning for this, but they hadn’t managed to make much progress
    and actually noticed [Alvira’s kernel](https://www.kaggle.com/alvira12/class-wise-processing-lb-0-992-new-dataset)
    here which get’s 0.992 on the leader board which I think is top 20\. It’s entirely
    heuristic and it’s a great example of feature engineering. In this case the whole
    thing is basically entirely feature engineering. It’s basically looking through
    and using lots of regular expressions to figure out for each token, what is it.
    And I think she’s done a great job here laying it all out clearly as to what all
    the different pieces are and how they all fit together. And she mentioned that
    she’s maybe hoping to turn this into a library which I think would be great. You
    could use this to grab a piece of text and pull out what are all the pieces in
    it. It’s the kind of thing that the natural language processing community hopes
    to be able to do without lots of hand written code like this. But for now, I’ll
    be interested to see what the winners turn out to have done, but I haven’t seen
    machine learning being used really to do this particularly well. Perhaps the best
    approach is the one which combine this kind of feature engineering along with
    some machine learning. But I think this is a great example of effective feature
    engineering.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这周有几个来自USF学生的很酷的内核。我想强调两个都来自文本规范化竞赛的内核，该竞赛旨在尝试将标准英语文本转换为文本，还有一个俄语的。你要尝试识别可能是“第一，第二，第三”之类的东西，并说这是一个基数，或者这是一个电话号码或其他什么。我快速搜索了一下，发现学术界曾尝试使用深度学习来做这个，但他们没有取得太多进展，实际上我注意到[Alvira的内核](https://www.kaggle.com/alvira12/class-wise-processing-lb-0-992-new-dataset)在这里得到了0.992的排名，我认为是前20名。这完全是启发式的，是特征工程的一个很好的例子。在这种情况下，整个事情基本上完全是特征工程。基本上是通过查看和使用大量正则表达式来弄清楚每个标记是什么。我认为她在这里做得很好，清楚地列出了所有不同的部分以及它们如何相互配合。她提到她也许希望将这个变成一个库，我认为这将是很好的。你可以使用它来提取文本中的所有部分。这是自然语言处理社区希望能够做到的事情，而不需要像这样大量手写代码。但目前，我很感兴趣看看获胜者到底做了什么，但我还没有看到机器学习被用来做这个特别好。也许最好的方法是将这种特征工程与一些机器学习结合起来。但我认为这是一个有效特征工程的很好例子。
- en: '[This](https://www.kaggle.com/neerjad/class-wise-regex-functions-l-b-0-995)
    is another USF student who has done much the same thing, got a similar score,
    but used her own different set of rules. Again, it would get you a good leader
    board position with these as well. So I thought that was interesting to see examples
    of some of our students entering a competition and getting top 20ish results by
    basically just handwritten heuristics. This is where, for example, computer vision
    was six years ago still. Basically all the best approach was whole a lot of carefully
    handwritten heuristics often combined with some simple machine learning. So I
    think over time the field is definitely trying to move towards automating much
    more of this.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[这位](https://www.kaggle.com/neerjad/class-wise-regex-functions-l-b-0-995)是另一位USF的学生，她做了类似的事情，得到了类似的分数，但使用了自己不同的规则。同样，这也会让你在排行榜上获得一个不错的位置。所以我觉得看到我们的一些学生参加比赛并通过基本的手写启发式方法获得前20名结果的例子很有趣。这就是，例如，六年前的计算机视觉仍然是这样。基本上最好的方法是大量仔细手写的启发式方法，通常结合一些简单的机器学习。所以我认为随着时间的推移，这个领域肯定在努力向更多自动化方向发展。'
- en: '[Porto Seguro’s Safe Driver Prediction Winner](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629)
    [[4:21](https://youtu.be/37sFIak42Sc?t=261)]'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[波尔图塞古罗的安全驾驶员预测获奖者](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629)
    [[4:21](https://youtu.be/37sFIak42Sc?t=261)]'
- en: And actually interestingly, in the safe driver prediction competition was just
    finished. One of the Netflix prize winners won this competition and he invented
    a new algorithm for dealing with structured data which basically doesn’t require
    any feature engineering at all. So he came first place using nothing but five
    deep learning models and one gradient boosting machine. His basic approach was
    very similar to what we’ve been learning in this class so far and what we’ll be
    learning also tomorrow which is using fully connected neural networks and one
    hot encoding and specifically embedding which we will learn about. But he had
    a very clever technique which was there was a lot of data in this competition
    which was unlabeled. So in other words where they didn’t know whether that driver
    would file a claim or not. So when you got some labeled and some unlabeled data,
    we call that semi-supervised learning. In real life, most learning is semi-supervised
    learning. In real life, normally you have some things that are labeled and some
    things that are unlabeled. So this is kind of the most practically useful kind
    of learning. And then structured data is the most common kind of data that companies
    deal with day to day. So the fact that this competition was a semi-supervised,
    structure data competition made it incredibly practically useful.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上有趣的是，在刚刚结束的安全驾驶员预测比赛中，一个Netflix奖获得者赢得了这个比赛，他发明了一种处理结构化数据的新算法，基本上不需要任何特征工程。所以他使用了五个深度学习模型和一个梯度提升机获得了第一名。他的基本方法与我们迄今为止在这门课程中学到的以及明天将要学习的内容非常相似，即使用全连接的神经网络和独热编码，特别是我们将要学习的嵌入。但他有一个非常聪明的技巧，就是在这个比赛中有很多未标记的数据。换句话说，他们不知道那个司机是否会提出索赔。所以当你有一些标记和一些未标记的数据时，我们称之为半监督学习。在现实生活中，大多数学习都是半监督学习。在现实生活中，通常有一些被标记的东西和一些未被标记的东西。所以这是最实用的学习方式。而结构化数据是公司日常处理的最常见的数据类型。所以这个比赛是一个半监督的、结构化数据比赛，使其非常实用。
- en: So what his technique for winning this was to do data augmentation which those
    of you doing the deep learning course have learned about which is basically the
    idea like if you had pictures you would flip them horizontally or rotate them
    a bit. Data augmentation means creating new data examples which are slightly different
    versions of ones you already have. And the way he did it was for each row from
    the data, he would at random replaced 15% of the variables with a different row.
    So each row now would represent like a mix of 85% of the original row, the 15%
    randomly selected from a different row. So this was a way of randomly changing
    the data a little bit and then he used something called an autoencoder which we
    will probably won’t study until a part 2 of a deep learning course but the basic
    idea of an autoencoder is your dependent variable is the same as your independent
    variable. So in other words, you try to predict your input which obviously is
    trivial if you are allowed to identity transform, for example, trivially predicts
    the input, but the trick with an autoencoder is to have less activations in at
    least one of your layers than your input. So if your input was a hundred dimensional
    vector and you put it through a 100 by 10 matrix, create ten activations, and
    then have to create the original 100 long vector from that. Then you basically
    had to have compressed it effectively. So it turns out that that kind of neural
    network is forced to find correlations and features and interesting relationships
    in the data even when it’s not labeled. So he used that rather than doing any
    hand engineering. He just used an autoencoder. So these are some interesting kind
    of directions that if you keep going with your machine learning studies, particularly
    if you do a part two of a deep learning course next year, you’ll learn about.
    And you can kind of see how feature engineering is going away and this was just
    an hour ago. So this is very recent indeed. But this is one of the most important
    breakthroughs I’ve seen in a long time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以他赢得比赛的技巧是进行数据增强，你们在深度学习课程中学到的，基本上就是这样的想法，比如如果你有图片，你会水平翻转它们或者稍微旋转一下。数据增强意味着创建新的数据示例，这些示例是你已经拥有的数据的略微不同的版本。他的做法是对于数据中的每一行，他会随机地用另一行替换15%的变量。所以现在每一行代表的是原始行的85%混合，15%是随机选择的另一行。这是一种随机改变数据的方法，然后他使用了一种叫做自动编码器的东西，我们可能要等到深度学习课程的第二部分才会学习，但自动编码器的基本思想是你的因变量与自变量相同。换句话说，你试图预测你的输入，如果你被允许进行恒等变换，例如，显然可以轻松地预测输入，但自动编码器的技巧是至少在一个层中具有比输入更少的激活。所以如果你的输入是一个百维向量，你通过一个100乘以10的矩阵，创建十个激活，然后必须从中创建原始的100维向量。那么你基本上已经有效地对其进行了压缩。事实证明，这种类型的神经网络被迫在数据中找到相关性、特征和有趣的关系，即使数据没有标记。所以他使用了这个而不是进行任何手工工程。这些是一些有趣的方向，如果你继续进行机器学习研究，特别是如果你明年参加深度学习课程的第二部分，你会学到的。你可以看到特征工程正在消失，这就是刚刚一个小时前。所以这确实是非常近期的。但这是我长时间以来看到的最重要的突破之一。
- en: MNIST continued [[8:32](https://youtu.be/37sFIak42Sc?t=8m32s)]
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST继续[[8:32](https://youtu.be/37sFIak42Sc?t=8m32s)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson4-mnist_sgd.ipynb)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson4-mnist_sgd.ipynb)'
- en: This (`LogReg`) was that little handwritten `nn.Module` class we created [[9:15](https://youtu.be/37sFIak42Sc?t=555)].
    We defined our loss. We defined our learning rate, and we defined our optimizer.
    And `optim.SGD` is the thing we are going to try and write by hand in a moment.
    So `nn.NLLLoss` and `optim.SGD`, we are stealing from PyTorch, but we’ve written
    the module `LogReg` and the training loop ourselves.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这（`LogReg`）是我们创建的那个小手写的`nn.Module`类[[9:15](https://youtu.be/37sFIak42Sc?t=555)]。我们定义了损失。我们定义了学习率，我们定义了优化器。`optim.SGD`是我们接下来要尝试手写的东西。所以`nn.NLLLoss`和`optim.SGD`，我们是从PyTorch中借鉴的，但我们自己编写了模块`LogReg`和训练循环。
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: So the basic idea was we are going to go through some number of epochs [[9:39](https://youtu.be/37sFIak42Sc?t=579)],
    so let’s go through one epoch. And we are going to keep track of for each mini
    batch, what the loss was so that we can report it at the end. We are going to
    turn our training data loader into an iterator so that we can loop through it
    — loop through every mini batch. So now we can go ahead and say `for` tensor `in`
    the length of the data loader and then we can call `next` to grab the next independent
    variable and the dependent variables from that iterator.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本思想是我们将经历一些时期[[9:39](https://youtu.be/37sFIak42Sc?t=579)]，所以让我们经历一个时期。我们将跟踪每个小批次的损失，以便在最后报告。我们将把我们的训练数据加载器转换为迭代器，以便我们可以循环遍历它
    - 遍历每个小批次。现在我们可以继续说`for`张量`in`数据加载器的长度，然后我们可以调用`next`来从该迭代器中获取下一个自变量和因变量。
- en: So then remember, we can then pass the x tensor (`xt`) into our model by calling
    the model as if it was a function. But first of all, we have to turn it into a
    variable. Last week, we were typing `Variable(blah).cuda()` to turn it into a
    variable, a shorthand for that is just the capital `V`. So capital `T` for a tensor,
    capital `V` for a variable. That’s just a shortcut in Fast AI. So that returns
    our predictions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后记住，我们可以通过调用模型来将x张量（`xt`）传递给我们的模型，就好像它是一个函数一样。但首先，我们必须将其转换为一个变量。上周，我们在输入`Variable(blah).cuda()`来将其转换为一个变量，Fast
    AI中的一个简写是大写`V`。所以张量的大写`T`，变量的大写`V`。这只是Fast AI中的一个快捷方式。这将返回我们的预测。
- en: The next thing we needed was to calculate our loss because we can’t calculate
    the derivative of the loss if we haven’t calculated the loss [[10:43](https://youtu.be/37sFIak42Sc?t=643)].
    So the loss takes the predictions and the actuals. The actuals, again, are the
    y tensor and we have to turn that into a variable. A variable keeps track of all
    of the steps to get computed. There’s actually a fantastic tutorial on the PyTorch
    website
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要做的是计算我们的损失，因为如果我们没有计算损失，就无法计算损失的导数。所以损失接受预测值和实际值。实际值再次是y张量，我们必须将其转换为一个变量。变量跟踪所有计算步骤。实际上，在PyTorch网站上有一个很棒的教程。
- en: The next thing we needed was to calculate our loss because we can’t calculate
    the derivative of the loss if we haven’t calculated the loss [[10:43](https://youtu.be/37sFIak42Sc?t=643)].
    So the loss takes the predictions and the actuals. The actuals, again, are the
    y tensor and we have to turn that into a variable. A variable keeps track of all
    of the steps to get computed. There’s actually a fantastic tutorial on the PyTorch
    website. On the PyTorch website, there is a tutorial section and there’s a tutorial
    there about [Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html).
    Autograd is the name of the automatic differentiation package that comes with
    PyTorch, and it’s an implementation of automatic differentiation. So the Variable
    class is really the key class here, because that’s the thing that turns a tensor
    into something where we can keep track of its gradients. So basically here they
    show how to create a variable, do an operation to a variable, and then you can
    go back and actually look at the grad function (`grad_fn`) which is the function
    it’s keeping track of to calculate the gradient. So as we do more and more operations
    to this variable and the variable calculated from that variable, it keeps keeping
    track of it. So later on, we can go `.backward()` and then print `.grad` and find
    out the gradient. So you notice we never defined the gradient, we just defined
    it as being `(x + 2)² * 3` whatever, and it can calculate the gradient.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要做的是计算我们的损失，因为如果我们没有计算损失，就无法计算损失的导数。所以损失接受预测值和实际值。实际值再次是y张量，我们必须将其转换为一个变量。变量跟踪所有计算步骤。实际上，在PyTorch网站上有一个很棒的教程。在PyTorch网站上，有一个教程部分，其中有一个关于Autograd的教程。Autograd是随PyTorch一起提供的自动微分包的名称，它是自动微分的实现。因此，Variable类是真正的关键类，因为它可以将张量转换为我们可以跟踪其梯度的东西。基本上在这里他们展示了如何创建一个变量，对变量进行操作，然后可以回头查看grad函数（`grad_fn`），这是它跟踪以计算梯度的函数。因此，当我们对这个变量和从该变量计算出的变量进行更多操作时，它会继续跟踪。稍后，我们可以进行`.backward()`，然后打印`.grad`并找出梯度。所以你会注意到我们从未定义过梯度，我们只是定义它为`(x
    + 2)² * 3`之类的表达式，它可以计算梯度。
- en: So that’s why we need to turn that into a variable [[13:12](https://youtu.be/37sFIak42Sc?t=792)].
    So `l` is now a variable containing the loss. So it contains a single number for
    this mini batch which is the loss for this mini batch. But it’s not just a number.
    It’s a number as a variable, so it’s a number that knows how it was calculated.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们需要将其转换为一个变量。所以`l`现在是一个包含损失的变量。它包含了这个小批量的损失的单个数字。但它不仅仅是一个数字。它是一个作为变量的数字，所以它是一个知道如何计算的数字。
- en: We are going to append that loss to our array just so we can get the average
    of it later. And now we are going to calculate the gradient. So `l.backward()`
    is a thing that says calculate the gradient. So remember when we call the network,
    it’s actually calling our forward function. So that’s like go through it forward.
    And then backward is like using the chain rule to calculate the gradients backwards.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个损失添加到我们的数组中，这样我们以后可以得到它的平均值。现在我们要计算梯度。所以`l.backward()`是一个指令，表示计算梯度。所以记住当我们调用网络时，实际上是调用我们的前向函数。这就像是向前走一遍。然后向后就像是使用链式法则向后计算梯度。
- en: So `optimizer.step()` is the thing we are about to write which is update the
    weights based on the gradients and the learning rate. `zero_grad()`, we will explain
    when we write this out by hand.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`optimizer.step()`是我们即将编写的内容，即根据梯度和学习率更新权重。`zero_grad()`，我们将在手动编写时解释。'
- en: '![](../Images/f2ed3582160de8a493fd76c0b8f198b9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2ed3582160de8a493fd76c0b8f198b9.png)'
- en: 'So then at the end, we can turn our validation data loader into an iterator
    [[14:16](https://youtu.be/37sFIak42Sc?t=856)]. And we can then to through its
    length grabbing each x and y out of that, and asking for the score which we defined
    to be equal to which thing did you predict, which thing was actual, and check
    whether they are equal. Then the mean of that is going to be our accuracy:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后最后，我们可以将验证数据加载器转换为迭代器。然后我们可以遍历它的长度，每次取出一个x和y，并询问我们定义的分数，即你预测了哪个，实际上是哪个，并检查它们是否相等。然后这些的平均值将是我们的准确率。
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Question**: What’s the advantage that you converted into an iterator rather
    than use normal Python loop[[14:53](https://youtu.be/37sFIak42Sc?t=893)]? We are
    using a normal Python loop, so the question really is like compare to what. So
    the alternative, perhaps you are thinking of, could be like we could use something
    like a list with an indexer. So the problem there is that we want each time we
    grab a new mini batch, we want it to be random. We want a different shuffled thing.
    So this `for t in range(len(dl))`, you can actually iterate from forever. You
    can loop through it as many times as you like. So this is kind of idea it’s called
    different things in different languages, but a lot of languages call it a stream
    processing and it’s this basic idea that rather than saying I want the third thing
    or ninth thing, it’s just like I want the next thing. It’s great for network programming
    — like grab the next thing from the network. It’s great for UI programming — grab
    the next event where somebody clicked a button. It also turns out to be great
    for this kind of numeric programming — it’s like I just want the next batch of
    data. It means that the data can be kind of arbitrarily long because we are just
    grabbing one piece at a time. And I guess the short answer is because it’s how
    PyTorch works. PyTorch’s data loaders are designed to be called in this way. So
    Python has this concept of a generator which is a way that you can create a function
    that behaves like an iterator. So Python has recognized that this stream processing
    approach to programming is super handy and helpful, and supports it everywhere.
    So basically anywhere that you use `for ... in` loop, anywhere you use a list
    comprehension, those things can always be generators or iterators. So by programming
    this way, we get a lot of flexibility. Does that sound about right, Terrence?
    You’re the programming language expert.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**: 为什么你要将其转换为迭代器而不是使用普通的Python循环[[14:53](https://youtu.be/37sFIak42Sc?t=893)]？我们正在使用普通的Python循环，所以问题实际上是与什么进行比较。所以，也许你在考虑的替代方案可能是，我们可以使用类似带有索引器的列表。问题在于我们每次获取一个新的小批次时，我们希望它是随机的。我们希望有一个不同的洗牌过的东西。所以这个`for
    t in range(len(dl))`，你实际上可以无限迭代。你可以循环遍历它任意次数。所以这种想法在不同的语言中被称为不同的东西，但很多语言称之为流处理，这是一种基本的想法，而不是说我想要第三个或第九个东西，而是说我想要下一个东西。这对网络编程非常有用——从网络中获取下一个东西。对于UI编程也非常有用——获取下一个事件，比如有人点击了一个按钮。事实证明，这对于这种数值编程也非常有用——就像我只想要下一个数据批次。这意味着数据可以是任意长的，因为我们一次只获取一部分。我想简短的回答是因为这是PyTorch的工作方式。PyTorch的数据加载器被设计为以这种方式调用。所以Python有这个生成器的概念，它是一种可以创建行为像迭代器的函数的方式。Python已经认识到这种流处理编程方法非常方便和有用，并且在各处支持它。所以基本上任何你使用`for
    ... in`循环的地方，任何你使用列表推导的地方，这些东西都可以是生成器或迭代器。通过这种方式编程，我们获得了很大的灵活性。听起来对吗，Terrence？你是编程语言专家。'
- en: '**Terrence**: Yeah, I mean the short answer is what you said. You might say
    something about space, but in this case all that data has to be in memory anyway
    because we’ve got…'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**Terrence**: 是的，我的意思是你说的很对。你可能会提到空间的问题，但在这种情况下，所有这些数据都必须在内存中，因为我们有...'
- en: '**Jeremy**: No doesn’t have to be in memory. In fact, most of the time with
    PyTorch, the mini batch will be read from separate images spread over your disk
    on demand, so most of the time it’s not in memory.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jeremy**: 不需要在内存中。事实上，大多数情况下，使用PyTorch，小批次将根据需要从分布在磁盘上的单独图像中读取，所以大多数情况下它不在内存中。'
- en: '**Terrence**: But in general, you want to keep as little in memory as possible
    at a time. And so the idea of stream processing also is great because you can
    do compositions. You can pipe the data to a different machine.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**Terrence**: 但一般来说，你希望尽可能少地一次性保存在内存中。所以流处理的想法也很棒，因为你可以进行组合。你可以将数据传送到另一台机器。'
- en: '**Jeremy**: Yeah, the composition is great. You can grab the next thing from
    here, and then send it off to the next stream, which you can then grab it and
    do something else.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jeremy**: 是的，组合很棒。你可以从这里获取下一个东西，然后将其发送到下一个流中，然后你可以获取它并做其他事情。'
- en: '**Terrence**: which you guys all recognize, of course, in the command-line
    pipes and I/O redirection.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**Terrence**: 你们都认识到，当然，在命令行管道和I/O重定向中。'
- en: '**Jeremy**: Thanks, Terrence. It’s a benefit of working with people that actually
    know what they are talking about.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jeremy**: 谢谢，Terrence。与真正知道自己在谈论什么的人一起工作的好处。'
- en: Implementing Stochastic Gradient Descent [[18:24](https://youtu.be/37sFIak42Sc?t=1104)]
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现随机梯度下降[[18:24](https://youtu.be/37sFIak42Sc?t=1104)]
- en: So let’s now take that and get rid of the optimizer. The only thing that we
    are going to be left with is the negative log likelihood loss function which we
    could also replace actually. We have an implementation of that from scratch that
    Yannet wrote in the notebooks. It’s only one line of code as we learned earlier.
    You can do it with a single if statement. So I don’t know why I was so lazy as
    to include this.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们拿掉优化器。我们唯一剩下的是负对数似然损失函数，实际上我们也可以替换。我们在笔记本中有Yannet从头开始编写的实现。正如我们之前学到的那样，只需要一行代码。你可以用一个if语句来做到。所以我不知道为什么我如此懒惰，要包括这个。
- en: 'So what we are going to do is, we are going to, again, grab this module that
    we’ve written ourselves (the logistic regression module). We’re going to have
    one epoch again. We are going to loop through each thing in an iterator. We are
    going to grab our independent and dependent variable for the mini batch, pass
    it into our network, calculate the loss. So this is all the same as before, but
    now we’re going to get rid of `optimizer.step()`, and we are going to do it by
    hand. So the basic trick is, as I mentioned, we are not going to do the calculus
    by hand. So we call `l.backward()` to calculate the gradients automatically, and
    that’s going to fill in our matrix. Here is that module we built:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们要做的是，再次获取我们自己编写的模块（逻辑回归模块）。我们将再次进行一个epoch。我们将循环遍历迭代器中的每个元素。我们将获取我们的独立和依赖变量用于小批量，将其传递给我们的网络，计算损失。所以这一切和以前一样，但现在我们要摆脱`optimizer.step()`，我们要手动做。所以基本的技巧是，正如我提到的，我们不会手动进行微积分。我们调用`l.backward()`来自动计算梯度，这将填充我们的矩阵。这就是我们构建的模块：
- en: '![](../Images/46b2245273d42fbdcf3b03261414ef9f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46b2245273d42fbdcf3b03261414ef9f.png)'
- en: 'So the weight matrix for the linear layer weights, we call `l1_w` and the biases
    we call `l1_b`. They were the attributes we created. So I’ve just put them into
    things called `w` and `b` just to save some typing basically. So w is our weights,
    b is our biases. So the weights, remember the weights are a variable and to get
    the tensor out of the variable, we have to use `.data`. So we want to update the
    actual tensor that’s in this variable so we say:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所以线性层权重的权重矩阵，我们称为`l1_w`，偏差我们称为`l1_b`。它们是我们创建的属性。所以我只是把它们放到了叫做`w`和`b`的东西里，基本上是为了节省一些输入。所以w是我们的权重，b是我们的偏差。所以权重，记住权重是一个变量，要从变量中获取张量，我们必须使用`.data`。所以我们想要更新这个变量中的实际张量，所以我们说：
- en: '`w.data -= w.grad.data * lr`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`w.data -= w.grad.data * lr`'
- en: '`-=` we want to go in the opposite direction to the gradient. The gradient
    tell us which way is up. We want to go down.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-=`我们想要朝着梯度的相反方向前进。梯度告诉我们哪个方向是向上的。我们想要向下。'
- en: '`w.grad.data * lr` whatever is currently in the gradients times the learning
    rate.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`w.grad.data * lr`当前梯度乘以学习率。'
- en: So that is the formula for gradient descent. As you can see, it’s as easier
    thing as you can possibly imagine. It’s like update the weights to be equal to
    whatever they are now minus the gradients times the learning rate. And do the
    same thing for the bias.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是梯度下降的公式。正如你所看到的，这是你可能想象到的最简单的事情。就像更新权重等于它们现在的值减去梯度乘以学习率一样。对偏差也是同样的操作。
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Question**: When we do the `next` on top, when it’s the end of the loop,
    how do we grab the next element [[21:08](https://youtu.be/37sFIak42Sc?t=1268)]?
    So this (`**for** t **in** range(len(dl)):`) is going through each index in range
    of length, so this is going 0, 1, 2… At the end of this loop, it’s going to print
    out the mean of the validation set, go back to the start of the epoch, at which
    point, it’s going to create a new iterator. So basically behind the scenes in
    Python when you call `iter(md.trn_dl)`, it basically tells it to reset its state
    to create a new iterator. And if you are interested in how that works, the code
    is all available for you to look at. `md.trn_dl` is `fastai.dataset.ModelDataLoader`
    so we could take a look at the code of that and see exactly how it’s being built.
    So you can see here, the `__next__` function which is keeping track of how many
    times it’s been through in this `self.i`, and here is the `__iter__` function
    which is the thing that gets called when you create a new iterator. And you can
    see it’s passing it off to something else which is of type DataLoader, and then
    you can check out DataLoader if you’re interested to see how that’s implemented
    as well.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：当我们在顶部执行`next`时，当循环结束时，我们如何获取下一个元素？所以这个(`**for** t **in** range(len(dl)):`)是在长度范围内的每个索引进行循环，所以这是0、1、2...在这个循环结束时，它将打印出验证集的平均值，然后回到epoch的开始，在这一点上，它将创建一个新的迭代器。所以基本上在Python的后台当你调用`iter(md.trn_dl)`时，它基本上告诉它重置其状态以创建一个新的迭代器。如果你对它是如何工作感兴趣，所有的代码都可以供你查看。`md.trn_dl`是`fastai.dataset.ModelDataLoader`，所以我们可以看一下它的代码，看看它是如何构建的。所以你可以在这里看到，`__next__`函数跟踪了它在`self.i`中经历了多少次，这里是`__iter__`函数，当你创建一个新的迭代器时会调用这个函数。你可以看到它将其传递给另一个类型为DataLoader的东西，然后如果你对它是如何实现的感兴趣，你可以查看DataLoader。
- en: '![](../Images/7e85975843cfe77ce0f3736b045634cb.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e85975843cfe77ce0f3736b045634cb.png)'
- en: So the DataLoader that we wrote basically uses multi-threading to allow it to
    have multiple of these going on at the same time. It’s really simple. It’s only
    about a screen full of code so if you are interested in simple multi-threaded
    programming, it’s a good thing to look at.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们编写的DataLoader基本上使用多线程，允许同时进行多个操作。这非常简单。只有大约一屏的代码，所以如果你对简单的多线程编程感兴趣，这是一个值得一看的好东西。
- en: '**Question**: Why have you wrapped this in `for epoch in range(1)` since that’ll
    only run once [[23:10](https://youtu.be/37sFIak42Sc?t=1390)]? Because in real
    life, we would normally be running multiple epochs. Like in this case, because
    it’s a linear model, it actually trains to as good as it’s going to get in one
    epoch, so if I type 3 here, it actually won’t improve after the first epoch much
    at all as you can see. But when we go back up to the top, we’re going to look
    at some slightly deeper and more interesting versions which will take more epochs.
    So if I was turning this into a function, I’d be going like `def train_mdl` and
    one of the things you would pass in is the number of epochs kind of thing.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：为什么你把这个包装在`for epoch in range(1)`中，因为这只会运行一次？因为在现实生活中，我们通常会运行多个epochs。就像在这种情况下，因为这是一个线性模型，它实际上在一个epoch内训练到了它能达到的最好状态，所以如果我在这里输入3，你会看到在第一个epoch之后它实际上不会有太大的改进。但当我们回到顶部时，我们将看一些稍微更深入和更有趣的版本，这将需要更多的epochs。所以如果我要把这个转换成一个函数，我会像这样写`def
    train_mdl`，你会传入一些epochs的数量之类的东西。
- en: '![](../Images/21c83f079cd9958857b6612123329796.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21c83f079cd9958857b6612123329796.png)'
- en: One thing to remember is that when you are creating these neural network layers,
    and remember this (`LogReg()`) is, as far as PyTorch is concerned, just a nn.Module
    —we could be using it as a layer, we could be using as a function, we could be
    using it as a neural net [[24:10](https://youtu.be/37sFIak42Sc?t=1450)]. PyTorch
    doesn’t think of those as different things. So this could be a layer inside some
    other network. So how do gradients work? So if you’ve got a layer which we can
    think of as activations or some activations that get computed through some other
    nonlinear/linear activation function. And from that layer, it’s very likely that
    we’re then putting it through a matrix product to create some new layer. So each
    one of these, so if we were to grab like one of these activations, is actually
    going to be used to calculate every one of those outputs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要记住的一件事是，当你创建这些神经网络层时，记住这个（`LogReg()`）在PyTorch看来只是一个nn.Module ——我们可以将其用作一个层，我们可以将其用作一个函数，我们可以将其用作一个神经网络。PyTorch不认为这些是不同的东西。因此，这可能是另一个网络中的一层。那么梯度是如何工作的呢？所以如果你有一个层，我们可以将其看作是激活或通过某些其他非线性/线性激活函数计算出的一些激活。然后从该层，很可能我们会将其通过矩阵乘积来创建一些新层。因此，如果我们抓取像这样的一个激活，实际上会用来计算每一个输出。
- en: '![](../Images/de146454ddefdb2e5ce3a93e33df246a.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de146454ddefdb2e5ce3a93e33df246a.png)'
- en: 'So if you want to calculate the derivative, you have to know how this weight
    matrix impacts each output and you have to add all of those together to find the
    total impact of the one activation across all of its outputs. So that’s why in
    PyTorch you have to tell it when to set the gradients to zero. Because the idea
    is that you could be having lots of different loss functions or lots of different
    outputs in your next set of activations or whatever, all adding up increasing
    or decreasing your gradients. So you basically have to say, okay this is a new
    calculation — reset. So here is where we do that:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果要计算导数，你必须知道这个权重矩阵如何影响每个输出，并将所有这些加在一起以找到一个激活在所有输出上的总影响。这就是为什么在PyTorch中你必须告诉它何时将梯度设置为零。因为想法是你可能有很多不同的损失函数或者下一组激活中的很多不同的输出，所有这些都会增加或减少你的梯度。所以你基本上必须说，好的，这是一个新的计算
    — 重置。所以这就是我们这样做的地方：
- en: '![](../Images/40c4e80e057b3ce8324feb07a3bca0ef.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40c4e80e057b3ce8324feb07a3bca0ef.png)'
- en: Before we do `l.backward()`, we say reset. So let’s take our weights, let’s
    take the gradients, let’s take the tensor that they point to and then `zero_`.
    Underscore as a suffix in PyTorch means “in place” so it sounds like a minor technicality
    but it’s super useful to remember. Every function pretty much has an underscore
    version suffix which does it in place. So normally zero returns a tensor of zeros
    of a particular size, so zero_ means replace the contents of this with a bunch
    of zeros.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们执行`l.backward()`之前，我们说重置。所以让我们拿出我们的权重，拿出梯度，拿出它们指向的张量，然后`zero_`。在PyTorch中，下划线作为后缀意味着“原地”，听起来像一个小技术细节，但记住这一点非常有用。几乎每个函数都有一个下划线版本的后缀，它会原地执行操作。所以通常zero返回一个特定大小的零张量，所以zero_意味着用一堆零替换这个内容。
- en: Alright so that’s it. That’s SGD from scratch. And if I get rid of my menu bar,
    we can officially say it fits within a screen. Of course we haven’t gotten our
    definition of our logistic regression here, that’s another half the screen, but
    basically there’s not much to it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，就是这样。这就是从头开始的SGD。如果我去掉我的菜单栏，我们可以正式说它适合在一个屏幕内。当然，我们还没有得到我们的逻辑回归的定义，那是另外半个屏幕，但基本上没有太多内容。
- en: '**Question**: Why do we need multiple epochs [[27:39](https://youtu.be/37sFIak42Sc?t=1659)]?
    The simple way to answer that would be, let’s say our learning rate was tiny.
    Then it’s just not going to get very far. There is nothing that says going through
    one epoch is enough to get you all the way there. So then it would be like okay,
    let’s increase our learning rate. Sure, we can increase the learning rate, but
    who is to say that the highest learning rate that learned stably is enough to
    learn this as well as it can be learnt. For most datasets for most architectures,
    one epoch is very rarely enough to get you to the best result you can get to.
    Linear models are very nicely behaved. So you can often use higher learning rates
    and learn more quickly. Also you can’t generally get as good an accuracy so there’s
    not as far to take them either. So doing one epoch is going to be the rarity.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：为什么我们需要多个epochs？简单回答就是，假设我们的学习率很小。那么它就不会走得很远。没有什么规定说通过一个epoch就足以让你达到目标。所以这就像是，好吧，让我们增加学习率。当然，我们可以增加学习率，但谁能说最高的学习率是稳定学习的足够多呢。对于大多数数据集和大多数架构来说，一个epoch很少能让你达到最好的结果。线性模型的行为非常好。所以你通常可以使用更高的学习率并更快地学习。此外，通常无法获得如此好的准确性，因此也没有太多的提升空间。因此，进行一个epoch将是罕见的。'
- en: Going backwards [[28:54](https://youtu.be/37sFIak42Sc?t=1734)]
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向后走
- en: Let’s go backwards. So going backwards, we are basically going to say let’s
    not write these lines over and over again (on the left). Let’s have somebody do
    that for us.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们往回走。所以往回走，我们基本上会说让我们不要一遍又一遍地写这些代码（在左侧）。让别人替我们做。
- en: '![](../Images/b4106f13f7923879ed11a7b263c18953.png)![](../Images/01fbfbcb74744b81c26b249826661172.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4106f13f7923879ed11a7b263c18953.png)![](../Images/01fbfbcb74744b81c26b249826661172.png)'
- en: So that’s the only difference between these versions. Rather than saying `.zero_`
    or `-= gradient * lr` ourselves, these are wrapped up for us (on the right).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些版本之间唯一的区别就是，不是我们自己说`.zero_`或`-= gradient * lr`，而是这些操作已经为我们封装好了（在右侧）。
- en: There is another wrinkle here which is the left approach to updating the weights
    is actually pretty inefficient. It doesn’t take advantage of momentum and and
    curvature. So in the DL course, we learned about how to do momentum from scratch
    as well. So if we actually just used plain old SGD instead of Adam, they are doing
    exactly the same and you’ll see that the left version learns is much slower.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一个问题，即左侧更新权重的方法实际上效率很低。它没有利用动量和曲率。因此，在深度学习课程中，我们也学习了如何从头开始实现动量。因此，如果我们实际上只是使用普通的SGD而不是Adam，它们实际上是完全相同的，你会看到左侧版本学习得更慢。
- en: 'Let’s do a little bit more stuff automatically [[30:25](https://youtu.be/37sFIak42Sc?t=1825)].
    Given that every time we train something, we have to loop through epoch, batch,
    do forward, get the loss, zero the gradient, do backward, do a step of the optimizer,
    let’s put all that in a function. And that function is called `fit`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们自动做更多的事情。考虑到每次训练时，我们必须循环遍历epoch、batch，进行前向传播，计算损失，梯度清零，反向传播，优化器进行一步操作，让我们把所有这些放在一个函数中。这个函数被称为`fit`：
- en: '![](../Images/3c18200c4e88b5628722a7caf4048f67.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c18200c4e88b5628722a7caf4048f67.png)'
- en: 'There it is. So let’s take a look at fit:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。让我们看看fit：
- en: '![](../Images/f0b1ada09feacf43b531ee9515253df1.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0b1ada09feacf43b531ee9515253df1.png)'
- en: 'Then here is step [[31:41](https://youtu.be/37sFIak42Sc?t=1901)]:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这是步骤：
- en: '![](../Images/4dfdbfbebb92064c0084972a30810850.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dfdbfbebb92064c0084972a30810850.png)'
- en: Zero out the gradients, calculate the loss (remember, PyTorch tends to call
    it criterion rather than loss), do backward. And then, there is something else
    we haven’t learned here, but we do learn in the deep learning course which is
    “gradient clipping” so you can ignore that. So you can see, all the stuff that
    we’ve learnt, when you look inside the actual framework, that’s the code you see.
    So that’s what fit does.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度清零，计算损失（记住，PyTorch倾向于称之为准则而不是损失），进行反向传播。然后，还有一些我们在这里没有学到的东西，但我们在深度学习课程中学到的，那就是“梯度裁剪”，所以你可以忽略它。所以你可以看到，我们学到的所有东西，当你查看实际框架内部时，那就是你看到的代码。这就是fit的作用。
- en: 'Then the next step would be this idea of having some weights and a bias and
    doing a matrix product and addition, let’s put that in a function [[32:14](https://youtu.be/37sFIak42Sc?t=1934)].
    This thing of doing the log softmax, let’s put that in a function. Then the very
    idea of first doing this and then doing that, the idea of chaining functions together,
    let’s put that into a function. And that finally gets us to:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后下一步就是有一些权重和偏差，进行矩阵乘法和加法，让我们把它放在一个函数中。进行对数softmax的操作，让我们把它放在一个函数中。然后首先进行这个操作，然后进行那个操作，将函数链接在一起的想法，让我们把它放在一个函数中。最终我们得到了：
- en: '![](../Images/7972d12ef22f7f25d818e1d016b4bd4b.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7972d12ef22f7f25d818e1d016b4bd4b.png)'
- en: So Sequential simply means through this do this function, take the result, send
    it to this function, etc. And Linear means create the weight matrix, create the
    biases. That’s it.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以Sequential简单地意味着通过这个函数执行这个操作，将结果传递给这个函数，依此类推。而Linear意味着创建权重矩阵，创建偏差。就是这样。
- en: We can then, as we started to talk about, turns this into a deep neural network
    by saying rather than sending this straight off into 10 activations, let’s put
    it into, say, 100, activations. We could pick whatever number we like. Put it
    through a ReLU to make it nonlinear, put it through another linear layer, another
    ReLu, and then our final output with our final activation function.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，正如我们开始讨论的那样，通过将其放入100个激活中，而不是直接将其发送到10个激活中，我们可以将其转换为一个深度神经网络。我们可以选择任何我们喜欢的数字。通过ReLU使其非线性化，通过另一个线性层，再通过一个ReLU，然后通过我们的最终输出和最终激活函数。
- en: '![](../Images/f42f3688824f5573ad7bef86b933f040.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f42f3688824f5573ad7bef86b933f040.png)'
- en: 'This is now a deep network. So we could fit that. And this time now, because
    it’s deeper, I’m actually going to run a few more epochs. And you can see the
    accuracy increasing:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这是一个深度网络。所以我们可以拟合它。这一次，因为它更深了，我实际上要运行更多的epochs。你可以看到准确性在增加：
- en: '![](../Images/ca95301e53b23d9e318385159f4686b1.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca95301e53b23d9e318385159f4686b1.png)'
- en: If you try and increase the learning rate from 0.1 further, it actually starts
    to become unstable.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试将学习率从0.1进一步增加，它实际上开始变得不稳定。
- en: Learning rate annealing [[34:12](https://youtu.be/37sFIak42Sc?t=2052)]
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率退火
- en: I’ll show you a trick. This is called learning rate annealing and the trick
    is this. When you are trying to fit to a function, you’ve been taking a few steps.
    As you get closer to the bottom, your steps probably want to become smaller. Otherwise
    what tends to happen is you start finding yourself going back and forth the same
    spots (oscillating).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我会告诉你一个技巧。这被称为学习率退火，技巧就是这样。当你试图拟合一个函数时，你已经走了几步。当你接近底部时，你的步伐可能会变得更小。否则，通常会发生的情况是你发现自己在同样的地方来回摆动。
- en: '![](../Images/ce98d3174562f0e45bc4502989874cbf.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce98d3174562f0e45bc4502989874cbf.png)'
- en: You can actually see it in accuracies above that it’s starting to flatten out.
    That could be because it has done as well as it can, or it could be that it’s
    going backwards and forwards. So it’s a good idea to decrease your learning rate
    later on in training, and take smaller steps. That’s called learning rate annealing.
    There is a function in Fast AI called `set_lrs` (set learning rates), you can
    pass in your optimizer and your new learning rate, and see if that helps. Very
    often it does. You should reduce by about an order of magnitude. In the deep learning
    course, we learn a much better technique than this to do learning rate annealing
    automatically and at more granular level. But if you are doing it by hand, an
    order of magnitude at a time is what people generally do.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在上面的准确性中实际看到它开始变得平坦。这可能是因为它已经做得尽可能好了，或者可能是因为它在前进和后退。所以在训练后期降低学习率并采取更小的步骤是个好主意。这就是所谓的学习率退火。在Fast
    AI中有一个名为`set_lrs`（设置学习率）的函数，你可以传入你的优化器和新的学习率，看看是否有帮助。很多时候确实有帮助。你应该减少大约一个数量级。在深度学习课程中，我们学习了一种比这更好的技术，可以自动进行学习率退火并在更细粒度的级别上进行。但如果你手动操作，一次减少一个数量级是人们通常做的事情。
- en: '![](../Images/c1530034befe6d3368c88b48bff8b52f.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1530034befe6d3368c88b48bff8b52f.png)'
- en: You’ll see people in papers talk about learning rate schedules, this is like
    a learning rate schedule. So this schedule has got us to 97%. And I tried going
    further and we don’t seem to be able to get much better than that. So here we’ve
    got something where we can get 97% accuracy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到论文中谈论学习率调度，这就像一个学习率调度。这个调度让我们达到了97%。我尝试继续下去，但似乎我们无法比这更好。所以我们得到了一个可以达到97%准确性的东西。
- en: '**Question**: I had a question about the data loading. I know it’s a Fast AI
    function, but could you go into a little detail of how it’s creating batches,
    how it’s done, and how it’s making those decisions [[36:47](https://youtu.be/37sFIak42Sc?t=2207)]?
    Sure. Basically there’s really nice design in PyTorch where they basically say
    let’s create a thing called a dataset. A dataset is basically something that looks
    like a list. It has a length (e.g. how many images are in the dataset). And it
    has the ability to index into it like a list. So if you had Dataset `d`, you can
    do:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我有一个关于数据加载的问题。我知道这是一个Fast AI函数，但你能详细介绍一下它是如何创建批次的，如何完成的，以及如何做出这些决定吗？当然。基本上，PyTorch中有一个非常好的设计，他们基本上说让我们创建一个叫做数据集的东西。数据集基本上看起来像一个列表。它有一个长度（例如数据集中有多少图像），并且可以像列表一样进行索引。所以如果你有数据集`d`，你可以这样做：'
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That’s basically all the dataset is as far as PyTorch is concerned. So you start
    with the dataset, so it’s like `d[3]` gives you the third image, etc. So you take
    a dataset and you can pass that into a constructor for a data loader `dl = DataLoader(d)`.
    That gives you something which is now iterable. So you can now say `iter(dl)`
    and that’s something you can call next on (i.e. `next(iter(dl))`). And what that
    now is going to do is when you call data loader’s constructor you can choose to
    have shuffle on or shuffle off. Shuffle on means give me random mini batch, shuffle
    off means go through it sequentially. So what the data loader does when you call
    `next` is it basically, assuming you said `shuffle=True` and batch size is 64,
    it’s going to grab 64 random integers between 0 and length, and call this (`d[i]`)
    64 times to get 64 different items and jam them together. So Fast AI uses the
    exact same terminology and the exact same API. We just do some of the details
    differently. Specifically, particularly with computer vision, you often want to
    do a lot of data augmentation like flipping, changing the colors a little bit,
    rotating, those turn out to be computationally expensive. Even just reading the
    JPEGS turns out to be computationally expensive. So PyTorch uses an approach where
    it fires off multiple processors to do that in parallel, where else the Fast AI
    library, instead, does something called multi-threading which can be a much faster
    way of doing it.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就是PyTorch关心的数据集。所以你从数据集开始，就像`d[3]`给你第三张图像，等等。所以你拿一个数据集，你可以把它传递给一个数据加载器的构造函数`dl
    = DataLoader(d)`。这会给你一个现在可迭代的东西。所以你现在可以说`iter(dl)`，这是你可以调用next的东西（即`next(iter(dl))`）。当你调用数据加载器的构造函数时，你可以选择打开或关闭洗牌。打开洗牌意味着给我随机的小批量，关闭洗牌意味着按顺序进行。所以当你调用`next`时，假设你说`shuffle=True`并且批量大小是64，它会在0到长度之间抓取64个随机整数，并调用这个（`d[i]`）64次以获取64个不同的项目并将它们组合在一起。所以Fast
    AI使用完全相同的术语和完全相同的API。我们只是以不同的方式处理一些细节。特别是在计算机视觉中，你经常想要做很多数据增强，比如翻转、稍微改变颜色、旋转，这些都是计算密集型的。甚至只是读取JPEG文件也是计算密集型的。所以PyTorch使用一种方法，即启动多个处理器并行进行处理，而Fast
    AI库则使用一种称为多线程的方法，这可能是更快的方法。
- en: '**Question**: Is an “epoch” a real epoch in the sense that all of the elements
    get returned once? Is it a shuffle at the beginning of the epoch [[39:47](https://youtu.be/37sFIak42Sc?t=2387)]?
    Yeah, not all libraries work the same way. Some do sampling with replacement,
    some don’t. The Fast AI library actually hands off the shuffling off to the actual
    PyTorch version and I believe the PyTorch version actually shuffles and an epoch
    covers everything once, I believe.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：在“epoch”中，所有元素都会被返回一次吗？是在epoch开始时进行洗牌吗？是的，并非所有库都以相同的方式工作。有些会进行有放回抽样，有些则不会。Fast
    AI库实际上将洗牌交给了实际的PyTorch版本，我相信PyTorch版本实际上会进行洗牌，一个epoch会覆盖所有元素，我相信。'
- en: Now the thing is when you start to get these bigger networks, potentially you’re
    getting quite a few parameters [[40:1](https://youtu.be/37sFIak42Sc?t=2416)7].
    I want to ask you to calculate how many parameters there are, but let’s remember
    here we’ve got 28 by 28 input into 100 output, and 100 into 10\. Then for each
    of those, we’ve got weights and biases.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在问题是，当你开始使用这些更大的网络时，潜在地你会得到相当多的参数。我想要求你计算一下有多少参数，但让我们记住这里我们有28乘以28的输入进入100个输出，100进入10。然后对于每一个，我们有权重和偏置。
- en: '![](../Images/1b496e638d891e310a9054eb0a75453d.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b496e638d891e310a9054eb0a75453d.png)'
- en: So we can actually do this.`net.parameters` returns a list where each element
    of the list is a tensor of the parameters for not just that layer but if it’s
    a layer with both weights and biases, that would be two parameters. So basically
    returns us a list of all of the tensors containing the parameters. `numel` in
    PyTorch tells you how big that is.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们实际上可以这样做。`net.parameters`返回一个列表，列表中的每个元素都是一个包含参数的张量，不仅仅是那一层，如果是一个既有权重又有偏置的层，那就是两个参数。所以基本上给我们返回了一个包含所有参数的张量的列表。PyTorch中的`numel`告诉你它有多大。
- en: '![](../Images/e02fb0a9a364dd46f6bd16a682c8c68a.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e02fb0a9a364dd46f6bd16a682c8c68a.png)'
- en: So if I run this, here is the number of parameters in each layer. So I’ve got
    784 inputs and the first layer has a hundred outputs, therefore the first weight
    matrix is of size 78,400\. And the first bias vector is of size 100\. Then the
    next one is a hundred by hundred, and there’s 100\. And then the next one is 100
    by 10 and 10 is the bias. So there’s the number of elements in each layer. I add
    them all up and it’s nearly a hundred thousand. So I’m possibly at a risk of overfitting
    here. So we might want to think about using regularization.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我运行这个，这里是每一层中的参数数量。所以我有784个输入，第一层有100个输出，因此第一个权重矩阵的大小是78400。第一个偏置向量的大小是100。然后下一个是100乘以100，有100。然后下一个是100乘以10，10是偏置。所以每一层中的元素数量就是这样。我把它们加起来，差不多有十万个。所以我可能有过拟合的风险。所以我们可能需要考虑使用正则化。
- en: Regularization [[42:05](https://youtu.be/37sFIak42Sc?t=2525)]
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: A really simple common approach to regularization in all of machine learning
    is something called L2 regularization. It’s super important, super handy, you
    can use it with just about anything. The basic idea is this. Normally we’d say
    our loss is equal to (let’s do RMSE to keep things simple) our predictions minus
    our actuals squared and we sum them up, take the average, take the square root.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有机器学习中，一种非常简单常见的正则化方法叫做L2正则化。这非常重要，非常方便，你可以将它用于几乎任何东西。基本思想是这样的。通常我们会说我们的损失等于（让我们用RMSE来保持简单）我们的预测减去我们的实际值的平方，然后求和，取平均值，再开平方。
- en: '![](../Images/5c714a9ebf074a54f4ccfe3e296d8426.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c714a9ebf074a54f4ccfe3e296d8426.png)'
- en: 'What if we then want to say, if I’ve got lots and lots of parameters, don’t
    use them unless they are really helping enough. If you’ve got a million parameters
    and you only really needed 10 parameters to be useful, just use 10\. So how could
    we tell the loss function to do that? Basically what we want to say is hey, if
    a parameter is zero, that’s no problem. It’s like it doesn’t exist at all. So
    let’s penalize a parameter for not being zero. What would be a way we could measure
    that? How can we calculate how un-zero our parameters are? L1 is the absolute
    value of the weights average. L2 is the squares of the weights themselves. Then
    we want to be able to say okay how much do we want to penalize not being zero?
    Because if we actually don’t have that many parameters, we don’t want to regularize
    much at all. If we’ve got heaps, we do want to regularize a lot. So then we put
    a parameter a:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想说，如果我有很多参数，除非它们真的足够有用，否则不要使用它们。如果你有一百万个参数，而你只真正需要10个参数来有用，那就只用10个。那么我们如何告诉损失函数做到这一点呢？基本上我们想说的是，嘿，如果一个参数是零，那没问题。就好像它根本不存在一样。所以让我们惩罚一个参数不为零。我们如何衡量这一点呢？我们如何计算我们的参数有多不为零？L1是权重平均值的绝对值。L2是权重本身的平方。然后我们想要说好，我们想要惩罚不为零的程度有多大？因为如果我们实际上没有那么多参数，我们根本不想要进行正则化。如果我们有很多参数，我们确实想要进行大量正则化。所以我们加入一个参数a：
- en: '![](../Images/9ebf05d44c382eba30f00f16a8f5f74b.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ebf05d44c382eba30f00f16a8f5f74b.png)'
- en: 'Except I have a rule in my classes which is never to use Greek letters, so
    normally people use alpha, I’m going to use a. So this is some number which you
    often see around 1e–6 to 1e-4 ish. Now we actually don’t care about the loss other
    than maybe to print it out. What we actually care about is the gradient of the
    loss. So the gradient of *aw²* is *2aw*. So there are two ways to do this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可能打印出来之外，我们实际上并不关心损失。我们真正关心的是损失的梯度。*aw²*的梯度是*2aw*。所以有两种方法可以做到这一点：
- en: We can actually modify our loss function to add in this square penalty.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实际上可以修改我们的损失函数来添加这个平方惩罚。
- en: We can modify that thing where we said weights equals weights minus gradient
    times learning rate to add *2aw* as well.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以修改我们说的权重等于权重减去梯度乘以学习率的那个东西，也加上*2aw*。
- en: These are basically equivalent but they have different names. The first one
    is called L2 regularization and the second one is called weight decay. So the
    first version was how it was first posed in the neural network literature, where
    else, the second version is how it was posed in the statistics literature, and
    they are equivalent.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基本上是等价的，但它们有不同的名称。第一个称为L2正则化，第二个称为权重衰减。第一个版本是最初在神经网络文献中提出的，而第二个版本是在统计文献中提出的，它们是等价的。
- en: As we talked about in the deep learning class, it turns out they are not exactly
    equivalent because when you have things like momentum and Adam, it can behave
    differently. And two weeks ago, a researcher figured out a way to actually do
    proper weight decay in modern optimizers and one of our Fast AI students implemented
    that in the Fast AI library, so Fast AI is now the first library to actually support
    this.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在深度学习课程中讨论的那样，事实证明它们并不完全等价，因为当你有动量和Adam等因素时，它们的行为可能会有所不同。两周前，一位研究人员找到了一种方法来在现代优化器中实现正确的权重衰减，我们Fast
    AI的一位学生在Fast AI库中实现了这一点，因此Fast AI现在是第一个实际支持这一功能的库。
- en: Anyways, for now, let’s do the version which PyTorch calls weight decay, but
    actually it turns out, based on this paper two weeks ago, is actually L2 regularization.
    It’s not quite correct, but it’s close enough. So here, we can say weight decay
    is 1e-3.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，现在让我们使用PyTorch称为权重衰减的版本，但实际上根据两周前的这篇论文，它实际上是L2正则化。这并不完全正确，但足够接近。所以在这里，我们可以说权重衰减是1e-3。
- en: '![](../Images/a68bddadf01d20a8d590fbfb690f0799.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a68bddadf01d20a8d590fbfb690f0799.png)'
- en: So this is going to set our penalty multiplier a to 1e-3 and it’s going to add
    that to the loss function. Let’s make a copy of these cells so we can compare
    how they train. So you might notice something kind of counterintuitive here [[48:54](https://youtu.be/37sFIak42Sc?t=2934)].
    0.23547 is our training error. You would expect our training error with regularization
    to be worse because we are penalizing parameters that specifically can make it
    better. And yet actually, it started out better not worse (previously it was 0.29756).
    Why could that be?
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把我们的惩罚乘数a设置为1e-3，并将其添加到损失函数中。让我们复制这些单元格，以便我们可以比较它们的训练方式。你可能会注意到这里有一些反直觉的地方[48:54]。0.23547是我们的训练误差。你可能会期望我们的带有正则化的训练误差会更糟，因为我们正在惩罚那些可以使其更好的参数。然而实际上，它开始时并不是更糟（之前是0.29756）。这可能是为什么呢？
- en: '![](../Images/d49b15a4a472ef6be94b4ef2df56349e.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d49b15a4a472ef6be94b4ef2df56349e.png)'
- en: 'The reason that can happen is that if you have a function that looks like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的原因是，如果你有一个看起来像这样的函数：
- en: '![](../Images/02b6b59f34a1bc4321c158d7294affd9.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02b6b59f34a1bc4321c158d7294affd9.png)'
- en: 'It takes potentially a really long time to train, where else, if you have a
    function that kind of looks more like this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可能需要很长时间，否则，如果你有一个看起来更像这样的函数：
- en: '![](../Images/dbaab92ab46e0974dbac84f4041e118b.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbaab92ab46e0974dbac84f4041e118b.png)'
- en: It’s going to train a lot more quickly. And there are certain things that you
    can do which sometime just can take a function that’s horrible and make it less
    horrible. And sometimes weight decay can actually make your functions a little
    more nicely behaved and that’s actually happened here. So I just mentioned that
    to say don’t let that confuse you. Weight decay really does penalize the training
    set and strictly speaking, the final number we get to for the training set shouldn’t
    end up being better, but it can train sometimes more quickly.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 它会训练得更快。有些事情你可以做，有时只是可以让一个可怕的函数变得不那么可怕。有时候权重衰减实际上可以使你的函数行为更好，这实际上在这里发生了。所以我只是提到这一点是为了说不要让这使你困惑。权重衰减确实对训练集进行惩罚，严格来说，我们最终得到的训练集的数字不应该更好，但有时候它可以更快地训练。
- en: '**Question**: I don’t get it. Why making it faster? The training time matters
    [[50:26](https://youtu.be/37sFIak42Sc?t=3026)]? No, this is after one epoch. The
    bottom is our training without weight decay, and the top is with weight decay.
    This is not related to time, this is related to just one epoch. After one epoch,
    my claim was that you would expect the training set, all other things being equal,
    to have a worse loss with weight decay because we are penalizing it. And I’m saying
    “oh, it’s not. That’s weird.”'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：我不明白。为什么会使它更快？训练时间重要吗？不，这是在一个时代之后。底部是我们没有使用权重衰减的训练，顶部是使用了权重衰减的训练。这与时间无关，只与一个时代有关。在一个时代之后，我的观点是，所有其他条件相同，你会预期使用权重衰减的训练集会有更糟的损失，因为我们在惩罚它。我说“哦，不是这样。这很奇怪。”
- en: '![](../Images/e5cc767768501c7098c2027aed39c1d3.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5cc767768501c7098c2027aed39c1d3.png)'
- en: The reason it’s not is because in a single epoch, it matters a lot as to whether
    you are trying to optimize something that’s very bumpy or whether you are trying
    to optimize something that’s nice and smooth. If you are trying to optimize something
    that’s really bumpy, imagine in some high dimensional space, you end up rolling
    around through all these different tubes and tunnels and stuff. Where else, if
    it’s just smooth, you just go boom. Imagine a marble rolling down a hill where
    one of them you’ve got Lombard street in San Francisco — it’s like backwards,
    forwards, backwards, forwards, it takes a long time to drive down the road. Where
    else, if you kind of took a motorbike and just went straight over the top, it’s
    much faster. So the shape of the loss function surface defines how easy it is
    to optimize. Therefore, how far it can get in a single epoch, based on these results,
    it would appear that weight decay here has made this function easier to optimize.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 原因在于，在一个单独的时代中，重要的是你是在尝试优化一个非常崎岖的东西，还是在尝试优化一个平滑的东西。如果你试图优化一个非常崎岖的东西，想象一下在某个高维空间中，你最终会在所有这些不同的管道和隧道中滚动。而如果它只是平滑的，你只是一下就到了。想象一颗大理石滚下山坡，其中一个是旧金山的隆巴德街
    - 前进，后退，前进，后退，需要很长时间才能开到尽头。而如果你骑摩托车直接越过山顶，速度就快得多。因此，损失函数表面的形状定义了优化的难易程度。因此，根据这些结果，似乎在这里使用权重衰减使得这个函数更容易优化。
- en: '**Question**: So just to make sure, penalizing is making the optimizer more
    than likely to reach the global minimum [[52:45](https://youtu.be/37sFIak42Sc?t=3165)]?
    No, I wouldn’t say that. My claim actually is that at the end, it’s probably going
    to be less good on the training set, indeed this does look to be the case. At
    the end, after five epochs, our training set is now worse with weight decay. That’s
    what I would expect. Like I never use a term global optimum because it’s just
    not something we have any guarantees about. We don’t really care about. We just
    care where do we get to after a certain number of epochs. We hope that we found
    somewhere that’s a good solution. So by the time we get to a good solution, the
    training set with weight decay, the loss is worse because it’s penalty. But on
    the validation set, the loss is better because we penalized the training set in
    order to try and create something that generalizes better. So parameters that
    are pointless is now zero and it generalizes better. So all we are saying is that
    it just got to a good point after one epoch.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：所以只是为了确保，惩罚会使优化器更有可能达到全局最小值吗？不，我不会这么说。我的观点实际上是，最终，它可能在训练集上表现得不太好，确实看起来是这样。最终，在五个时代之后，我们的训练集现在比使用权重衰减时更糟糕。这是我所期望的。就像我从不使用全局最优这个术语，因为我们对此没有任何保证。我们并不真正关心。我们只关心在经过一定数量的时代之后我们到达了哪里。我们希望我们找到了一个好的解决方案。因此，当我们达到一个好的解决方案时，使用权重衰减的训练集，损失更糟糕，因为它是惩罚。但在验证集上，损失更好，因为我们对训练集进行了惩罚，以便尝试创建一个更好泛化的东西。因此，无用的参数现在为零，泛化更好。所以我们所说的只是在一个时代之后它达到了一个好的点。
- en: '**Question**: Is it always true [[54:04](https://youtu.be/37sFIak42Sc?t=3244)]?
    No. By “it” if you mean weight decay always make the function surface smoother,
    no it’s not always true. But it’s worth remembering that if you are having trouble
    training a function, adding a little bit of weight decay may help.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：这总是真的吗？如果你的意思是权重衰减总是使函数表面更加平滑，那么不，这并不总是真的。但值得记住，如果你在训练一个函数时遇到困难，添加一点点权重衰减可能会有所帮助。
- en: '**Question**: So by regularizing the parameters, what it does is it smoothen
    out the loss function surface [[54:29](https://youtu.be/37sFIak42Sc?t=3269)]?
    I mean, it’s not why we do it. The reason why we do it is because we want to penalize
    things that aren’t zero to say don’t make this parameter a high number unless
    its really helping the loss a lot. Set it to zero if you can because setting as
    many parameters to zero as possible means that it’s going to generalize better.
    It’s like same as having a smaller network. That’s why we do it. But it can change
    how it learns as well.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：通过对参数进行正则化，它的作用是使损失函数表面变得更加平滑？我的意思是，这不是我们这样做的原因。我们这样做的原因是因为我们想惩罚那些不为零的东西，告诉它不要让这个参数变得很大，除非它真的对损失有很大帮助。如果可以的话，将其设置为零，因为将尽可能多的参数设置为零意味着它会更好地泛化。这就像拥有一个更小的网络一样。这就是我们这样做的原因。但它也可以改变学习方式。'
- en: I wanted to check how we actually went here [[55:11](https://youtu.be/37sFIak42Sc?t=3311)].
    So after the second epoch, you can see here it really has helped. After the second
    epoch, before we got to 97% accuracy, now we are nearly up to about 98% accuracy.
    And you can see that the loss was 0.08 vs. 0.13\. So adding regularization has
    allowed us to find 50% better solution (3% versus 2 %).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我想检查一下我们实际上在这里是怎么做的。所以在第二个时代之后，你可以看到这里确实有帮助。在第二个时代之后，我们之前达到了97%的准确率，现在我们几乎达到了98%的准确率。你可以看到损失是0.08对0.13。所以添加正则化使我们能够找到更好的解决方案（3%对2%）。
- en: '![](../Images/db0499dae78e6b84adb56f2936d8ea28.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db0499dae78e6b84adb56f2936d8ea28.png)'
- en: '**Question**: So there are two pieces to this — one is L2 regularization and
    weight decay [[55:42](https://youtu.be/37sFIak42Sc?t=3342)]? No, so my claim was
    they are the same thing. So weight decay is the version if you just take the derivative
    of L2 regularization you get weight decay. So you can implement it either by changing
    the loss function with a squared loss penalty or you can implement it by adding
    the weights themselves as part of the gradient.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：所以这有两个部分——一个是L2正则化和权重衰减？不，我的观点是它们是同一件事。所以权重衰减是L2正则化的版本，如果你只是对L2正则化求导，你会得到权重衰减。所以你可以通过改变损失函数来实现它，加入平方损失惩罚，或者你可以通过将权重本身添加到梯度中来实现它。'
- en: '**Question**: Can we use regularizations for convolution layer as well [[56:19](https://youtu.be/37sFIak42Sc?t=3379)]?
    Absolutely. A convolution layer is just weights.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我们可以在卷积层中使用正则化吗？当然可以。卷积层只是权重。'
- en: '**Question**: Can you explain why you thought you needed weight decay in this
    particular problem [[56:29](https://youtu.be/37sFIak42Sc?t=3389)]? Not easily.
    I mean, other than to say it’s something that I would always try. **Question continued:**
    Overfitting? So if my training loss was higher than my validation loss, then I’m
    underfitting. So there’s definitely no point regularizing. That would always be
    a bad thing. That would always mean you need more parameters in your model. In
    this case, I’m overfitting. That doesn’t necessarily mean regularization will
    help but it’s certainly worth trying.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：你能解释一下为什么你认为在这个特定问题中需要权重衰减吗？不容易。我是说，除了说这是我总是会尝试的事情之外。**继续提问**：过拟合？所以如果我的训练损失高于验证损失，那么我就是欠拟合。所以肯定没有必要正则化。那总是不好的。那总是意味着你的模型需要更多的参数。在这种情况下，我是过拟合的。这并不一定意味着正则化会有帮助，但值得一试。'
- en: '![](../Images/19fb64c2ca587b333b9c03680917b504.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19fb64c2ca587b333b9c03680917b504.png)'
- en: '**Question**: How do you choose the optimal number of epoch [[57:27](https://youtu.be/37sFIak42Sc?t=3447)]?
    You do my deep learning course 😆 That’s a long story. We don’t have time to cover
    best practices in this class. We are going to learn the fundamentals.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：你如何选择最佳的时代数？你参加我的深度学习课程😆 这是一个很长的故事。我们没有时间在这堂课上讨论最佳实践。我们将学习基础知识。'
- en: The secret to modern machine learning techniques [[58:14](https://youtu.be/37sFIak42Sc?t=3494)]
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现代机器学习技术的秘密
- en: Something that we cover in great detail in the deep learning course, but it’s
    really important mention here is the secret, in my opinion, to modern machine
    learning techniques is to massively over parameterize the solution to your problem
    as we just did. We’ve got like 100,000 weights when we only had a small number
    of 28 by 28 images, and then use regularization. It’s like the direct opposite
    of how nearly all statistics and learning was done for decades before, and still
    most senior lecturers at most universities in most areas have this background
    where they’ve learned the correct way to build a model is to have as few parameters
    as possible. So hopefully we’ve learnt two things so far. One is we can build
    very accurate models even when they have lots and lots of parameters. Random forest
    has a lot of parameters, and this here, deep network has a lot of parameters,
    and they can be accurate. And we can do that by either using bagging or by using
    regularization. And regularization in neural nets means either weight decay (also
    known as “kind of” L2 regularization) or dropout which we won’t worry too much
    about here. It’s a very different way of thinking about building useful models.
    And I just wanted to warn you that once you leave this classroom, even possibly
    when you go to the next faculty members talk, there’ll be people at USF as well
    who are entirely trained in the world of models with small number of parameters.
    Your next boss is likely to have been trained in the world of models with small
    number of parameters. The idea that they are somehow more pure or easier or better
    or more interpretable or whatever. I am convinced that is not true — probably
    not ever true. Certainly very rarely true. And that actually models with lots
    of parameters can be extremely interpretable as we learnt from our whole lesson
    of random forest interpretation. You can use most of the same technique with neural
    nets, but with neural nets are even easier. Remember how we did feature importance
    by randomizing a column to see how changes in that column would impact the output?
    Well, that’s just like a kind of dumb way of calculating its gradient. How much
    does varying this input change the output? With a neural net, we can actually
    calculate its gradient. So with PyTorch, you can actually say what’s the gradient
    of the output with respect to this column? You can do the same kind of things
    to do partial dependence plot with a neural net. And I’ll mention for those of
    you interested in making a real impact, nobody’s written basically any of these
    things for neural nets. So that whole area needs libraries to be written, blog
    posts to be written. Some papers have been written, but only in very narrow domains
    like computer vision. As far as I know, nobody’s written the paper saying here’s
    how to do structured data neural networks interpretation methods. So it’s a really
    exciting big area.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习课程中我们详细讨论的一点是，我认为现代机器学习技术的秘密是对问题的解决方案进行大规模的参数化，就像我们刚刚做的那样。当我们只有少量的28x28图像时，我们有大约10万个权重，然后使用正则化。这与几十年前几乎所有统计和学习的做法完全相反，大多数大学的大多数领域的高级讲师仍然具有这种背景，他们学习到构建模型的正确方式是尽可能少地使用参数。因此，希望我们迄今已经学到了两件事。一是即使模型有很多参数，我们也可以构建非常准确的模型。随机森林有很多参数，这里的深度网络也有很多参数，它们可以很准确。我们可以通过使用装袋或使用正则化来实现。在神经网络中，正则化意味着权重衰减（也称为“某种程度的”
    L2 正则化）或者我们在这里不会过多担心的 dropout。这是一种构建有用模型的非常不同的思考方式。我只是想警告你，一旦你离开这个教室，甚至可能当你去听下一个教员的讲座时，甚至在美国旧金山大学也会有完全受过小参数模型训练的人。你的下一个老板可能是在小参数模型的世界中接受过培训的。他们认为这些模型在某种程度上更纯净、更容易、更好、更可解释或者其他什么。我相信这不是真的
    - 可能永远不是真的。当然很少是真的。实际上，正如我们从随机森林解释的整个课程中学到的那样，具有大量参数的模型可以非常可解释。你可以使用大部分相同的技术来处理神经网络，但是神经网络更容易。记得我们是如何通过随机化一列来计算特征重要性的吗，以查看该列的变化如何影响输出？嗯，这就像一种愚蠢的计算梯度的方式。改变这个输入会如何改变输出？对于神经网络，我们实际上可以计算其梯度。因此，使用
    PyTorch，你实际上可以说输出相对于这一列的梯度是多少？你可以使用相同的方法来使用神经网络进行偏依赖图。对于那些对产生真正影响感兴趣的人，基本上没有人为神经网络编写这些东西。因此，整个领域需要编写库、撰写博客文章。一些论文已经写了，但只在非常狭窄的领域，比如计算机视觉。据我所知，没有人写过关于如何进行结构化数据神经网络解释方法的论文。因此，这是一个非常令人兴奋的大领域。
- en: NLP [[1:02:04](https://youtu.be/37sFIak42Sc?t=3724)]
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP [[1:02:04](https://youtu.be/37sFIak42Sc?t=3724)]
- en: So what we are going to do, though, is we are going to start with applying this
    with a simple linear model. This is mildly terrifying for me because we are going
    to do NLP and our NLP faculty expert is in the room. So David, just yell at me
    if I screw this up too badly. NLP refers to any kind of modeling where we are
    working with natural language text. Interestingly enough, we are going to look
    at a situation where a linear model is pretty close to the state of the art for
    solving a particular problem. It’s actually something where I actually surpassed
    state of the art in this using a recurrent neural network a few weeks ago, but
    this is actually going to show you pretty close to the state of the art with a
    linear model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们将从应用简单的线性模型开始。对我来说有点可怕，因为我们将进行 NLP，而我们的 NLP 专家就在房间里。因此，如果我搞砸了，David，请大声告诉我。NLP
    指的是我们处理自然语言文本的任何建模。有趣的是，我们将看一个情况，线性模型在解决特定问题时非常接近最先进技术。几周前，我实际上使用递归神经网络超越了最先进技术，但这实际上将向你展示线性模型非常接近最先进技术。
- en: IMDb
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IMDb
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson5-nlp.ipynb)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson5-nlp.ipynb)'
- en: 'We are going to be working with the IMDb dataset. So this is a dataset of movie
    reviews. You can download it by following these steps:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 IMDb 数据集。这是一个电影评论数据集。您可以按照以下步骤下载它：
- en: '[PRE4]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And once you download it, you’ll see that you’ve got a train and a test directory
    and in your train directory, you’ll see there is a negative and a positive directory.
    And in your positive directory, you’ll see there is a bunch of text files.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你下载了它，你会看到你有一个训练和一个测试目录，在你的训练目录中，你会看到有一个负面和一个正面目录。在你的正面目录中，你会看到有一堆文本文件。
- en: '[PRE5]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And here is an example of a text file:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个文本文件的例子：
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: So somehow we’ve managed to pick out a story of a man who has unnatural feelings
    for a pig as our first choice. That wasn’t intentional but it’ll be fine.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们不知何故选出了一个男人对猪有不自然感情的故事作为我们的第一个选择。这并不是故意的，但没关系。
- en: We are going to look at these movie reviews and for each one, we are going to
    look to see whether they were positive or negative. So they’ve been put into one
    of these folders. They were downloaded from IMDb (the movie database and review
    site). The ones that were strongly positive went in `/pos` and strongly negative
    went in `/neg`, and the rest they didn’t label at all (`/unsup`). So there are
    only highly polarized reviews.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看这些电影评论，对于每一个，我们将查看它们是积极的还是消极的。所以它们被放入了其中一个文件夹。它们是从IMDb（电影数据库和评论网站）下载的。那些非常积极的评论放在`/pos`中，非常消极的评论放在`/neg`中，而其余的则没有标签（`/unsup`）。所以只有高度极化的评论。
- en: So in the above example, we have an insane violent mob which unfortunately is
    too absurd, too off-putting, those from the era should be turned off. So the label
    for this was a zero which is negative, so this is a negative review
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在上面的例子中，我们有一个疯狂的暴力暴民，不幸的是太荒谬了，太令人反感了，那些来自那个时代的人应该被关掉。所以这个的标签是零，即负面的，所以这是一个负面的评论。
- en: '[PRE7]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the Fast AI library, there’s lots of functions and classes to help with most
    kinds of domains that you do machine learning on. For NLP, one of the simple things
    we have is texts from folders.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在Fast AI库中，有很多函数和类可以帮助你处理大多数机器学习的领域。对于自然语言处理，我们有一个简单的东西，就是来自文件夹的文本。
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That will go through and find all of the folders in here (the first argument
    `f'{PATH}train'`) with these names (the second argument `names`) and create a
    labeled dataset. Don’t let these things ever stop you from understanding what’s
    going on behind the scenes. We can grab its source code and as you can see it’s
    tiny, like 5 lines.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这将遍历并找到这里的所有文件夹（第一个参数`f'{PATH}train'`）与这些名称（第二个参数`names`）并创建一个带标签的数据集。不要让这些事情阻止你理解幕后发生的事情。我们可以获取它的源代码，你会看到它很小，就像5行。
- en: '![](../Images/4507d68fe1668c9a65d505b8f9b08394.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4507d68fe1668c9a65d505b8f9b08394.png)'
- en: I don’t like to write these things out in full, but hide them behind little
    functions so you can reuse them. But basically, it’s going to go through each
    directory, and go through each file in that directory, then stick that into an
    array of texts, figure out what folder it’s in, and stick that into an array of
    labels. So that’s how we end up with something where we have an array of the reviews
    and an array of the labels.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我不喜欢把这些东西写得很详细，而是把它们隐藏在一些小函数后面，这样你就可以重复使用它们。但基本上，它将遍历每个目录，遍历该目录中的每个文件，然后将其放入一个文本数组中，找出它在哪个文件夹中，并将其放入一个标签数组中。这就是我们最终得到的东西，我们有一个评论数组和一个标签数组。
- en: That’s our data. So our job will be to take a movie review and to predict the
    label. The way we are going to do is, we are going to throw away all of the interesting
    stuff about language which is the order in which the words are in. This is very
    often not a good idea, but in this particular case, it’s going to turn out to
    work not too badly. So let me show you what I mean by throwing away the order
    of the words. Normally, the order of the words matters a lot. If you’ve got a
    “not” before something, then that “not” refers to that thing. But in this case,
    we are trying to predict whether something is positive or negative. If you see
    the word “absurd” or “cryptic” appear a lot then maybe that’s a sign that this
    isn’t very good. So the idea is that we are going to turn it into something called
    a term document matrix where for each document (i.e. each review), we are just
    going to create a list of what words are in it, rather than what order they are
    in.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的数据。所以我们的工作将是接受一部电影评论并预测标签。我们将要做的是，我们将丢弃关于语言的所有有趣的东西，即单词的顺序。这通常不是一个好主意，但在这种情况下，它将不会太糟糕。所以让我告诉你我所说的丢弃单词顺序是什么意思。通常，单词的顺序非常重要。如果你在某个单词前面有一个“not”，那么这个“not”就指的是那个东西。但在这种情况下，我们试图预测某物是积极的还是消极的。如果你看到“荒谬”或“神秘”这样的词经常出现，也许这是一个迹象表明这不是很好。所以我们的想法是将其转换为一个称为术语文档矩阵的东西，对于每个文档（即每个评论），我们只是创建一个包含其中的单词列表，而不是它们的顺序。
- en: 'Term document matrix example:'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 术语文档矩阵示例：
- en: '[naivebayes.xlsx](https://github.com/fastai/fastai/blob/master/courses/ml1/excel/naivebayes.xlsx)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[朴素贝叶斯.xlsx](https://github.com/fastai/fastai/blob/master/courses/ml1/excel/naivebayes.xlsx)'
- en: '![](../Images/a36e17ad6cf82b55b0a0920b5adaae09.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a36e17ad6cf82b55b0a0920b5adaae09.png)'
- en: 'Here are four movie reviews that I made up. So I’m going to turn this into
    a term document matrix. The first thing I need to do is create something called
    vocabulary. A vocabulary is a list of all the unique words that appear. Here are
    my vocabulary: this, movie, is, good, the bad. That’s all the words. Now I’m going
    to take each of my movie reviews and turn it into a vector of which words appear
    and how often they appear. In this case, none of my words appear twice. So this
    is called a term document matrix:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有四个我编造的电影评论。所以我将把这些转换成一个术语文档矩阵。我需要做的第一件事是创建一个称为词汇表的东西。词汇表是出现的所有唯一单词的列表。这是我的词汇表：this,
    movie, is, good, the bad。这就是所有的单词。现在我将把我的每个电影评论转换成一个向量，显示哪些单词出现以及它们出现的频率。在这种情况下，我的单词没有出现两次。所以这被称为术语文档矩阵：
- en: '![](../Images/51358fce5caadc78cacf2078c39f9886.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51358fce5caadc78cacf2078c39f9886.png)'
- en: And this representation, we call **a bag of words** representation. So this
    here is a bag of words representation of the review.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示法，我们称之为**词袋**表示法。所以这里是评论的一个词袋表示。
- en: '![](../Images/125e5bdc87893a8c9aa2298284baeeec.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/125e5bdc87893a8c9aa2298284baeeec.png)'
- en: It doesn’t contain the order of the text anymore. It’s just a bag of the words
    (i.e. what words are in it). It contains “bad”, “is”, movie”, “this”. So the first
    thing we are going to do is we are going to turn it into a bag of words representation.
    The reason that this is convenient for linear models is that this is a nice rectangular
    matrix that we can do math on. Specifically, we can do a logistic regression and
    that’s what we are going to do. We are going to get to a point we do a logistic
    regression. Before we get there, though, we are going to do something else which
    is called Naive Bayes. sklearn has something which will create a term document
    matrix for us which is called [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html),
    so we’ll just use it.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 它不再包含文本的顺序。它只是一袋词（即其中包含哪些词）。它包含“bad”，“is”，“movie”，“this”。所以我们要做的第一件事是将其转换为一种词袋表示。这对于线性模型来说很方便的原因是，这是一个我们可以进行数学运算的漂亮矩阵。具体来说，我们可以进行逻辑回归，这就是我们要做的。我们将达到一个进行逻辑回归的点。不过，在那之前，我们将做另一件事，那就是朴素贝叶斯。sklearn有一个可以为我们创建术语文档矩阵的东西，叫做[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)，所以我们将使用它。
- en: Tokenization [[1:09:01](https://youtu.be/37sFIak42Sc?t=4141)]
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记化 [[1:09:01](https://youtu.be/37sFIak42Sc?t=4141)]
- en: 'Now in NLP, you have to turn your text into a list of words, and that’s called
    tokenization. That’s actually non-trivial because if this was actually `This movie
    is good.` or `This “movie” is good.`, how do you deal with that punctuation? More
    interestingly, what if this was `This "movie" isn’t good.` How you turn a piece
    of text into a list of tokens is called tokenization. A good tokenizer would turn
    this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在自然语言处理中，你必须将文本转换为单词列表，这就是所谓的标记化。这实际上并不是微不足道的，因为如果这实际上是`This movie is good.`或`This
    “movie” is good.`，你如何处理标点符号呢？更有趣的是，如果这是`This "movie" isn’t good.`，你如何将一段文本转换为标记列表呢？一个好的标记器会将这个转换为：
- en: 'Before: `This "movie" isn’t good.`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 之前：`This "movie" isn’t good.`
- en: 'After: `This " movie " is n’t good .`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 之后：`This " movie " is n’t good .`
- en: 'So you can see in this version here, if I now split this on spaces, every token
    is either a single piece of punctuation or this suffix `n''t` and is considered
    like a word. That’s how we would probably want to tokenize that piece of text
    because you wouldn’t want `good.` to be an object. There is no concept of `good.`
    or `"movie"` is not an object. So tokenization is something we hand off to a tokenizer.
    Fast AI has a tokenizer in it that we can use, so this is how we create our term
    document matrix with a tokenizer:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到，在这个版本中，如果我现在按空格分割这个文本，每个标记要么是一个单独的标点符号，要么是这个后缀`n't`，被视为一个单词。这就是我们可能想要对这段文本进行标记化的方式，因为你不希望`good.`成为一个对象。没有`good.`或`"movie"`是一个对象的概念。所以标记化是我们交给标记器的事情。Fast
    AI中有一个我们可以使用的标记器，这就是我们如何使用标记器创建我们的术语文档矩阵：
- en: '[PRE9]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: sklearn has a pretty standard API which is nice. I’m sure you’ve seen it a few
    times before. Once we’ve built some kind of “model”, we can kind of think of `CountVectorizer`
    as a model-ish, this is just defining what it’s going to do. We can call `fit_transform`
    to do that.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn有一个相当标准的API，这很好。我相信你以前见过几次。一旦我们建立了某种“模型”，我们可以把`CountVectorizer`看作是一种模型，这只是定义它将要做什么。我们可以调用`fit_transform`来执行这个操作。
- en: '[PRE10]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So in this case `fit_transform` is going to create the vocabulary and create
    the term document matrix based on the training set. `transform` is a little bit
    different. That says use the previously fitted model which in this case means
    use the previously created vocabulary. We wouldn’t want the validation set and
    the training set to have the words in different orders in the matrices. Because
    then they would have different meanings. So this is here saying use the same vocabulary
    to create a bag of words for the validation set.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`fit_transform`将创建词汇表，并基于训练集创建术语文档矩阵。`transform`有点不同。它表示使用先前拟合的模型，这在这种情况下意味着使用先前创建的词汇表。我们不希望验证集和训练集在矩阵中有不同顺序的单词。因为那样它们会有不同的含义。所以这里说的是使用相同的词汇表为验证集创建一个词袋。
- en: '**Question**: What if the validation set has different set of words other than
    training set [[1:11:40](https://youtu.be/37sFIak42Sc?t=4300)]? That’s a great
    question. So generally, most of these vocab creating approaches will have a special
    token for unknown. Sometimes you can also say like hey if a word appears less
    than three times, call it unknown. But otherwise, if you see something you haven’t
    seen before, call it unknown. So that (i.e. “unknown”) would just become a column
    in the bag of words.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：如果验证集中有不同于训练集的单词集合怎么办[[1:11:40](https://youtu.be/37sFIak42Sc?t=4300)]？这是一个很好的问题。通常，大多数这些词汇创建方法会为未知单词设定一个特殊标记。有时你也可以说，如果一个单词出现少于三次，就称之为未知。但是，如果你看到了以前没有见过的东西，就称之为未知。所以（即“未知”）只会成为词袋中的一列。'
- en: When we create this term document matrix, the training set we have 25,000 rows
    because there are 25,000 movie reviews and there are 75,132 columns which is the
    number of unique words.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建这个术语文档矩阵时，训练集有25,000行，因为有25,000条电影评论，有75,132列，这是唯一单词的数量。
- en: '[PRE11]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, most of the documents don’t have most of these 75,132 words. So we don’t
    want to actually store that as a normal array in memory. Because it’s going to
    be very wasteful. So instead, we store it as a sparse matrix. What a sparse matrix
    does is it just stores it as something that says whereabouts of the non-zeros.
    So it says okay, document number 1, word number 4 appears and it has 4 of them.
    Document number 1, term number 123 appears once, and so forth.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，大多数文档并不包含这75,132个单词中的大部分。所以我们不想把它实际存储为内存中的普通数组。因为这样会非常浪费。所以，我们将其存储为稀疏矩阵。稀疏矩阵的作用是将其存储为一种只指示非零值位置的东西。所以它会说，文档编号1，单词编号4出现了4次。文档编号1，术语编号123出现了一次，依此类推。
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: That’s basically how it’s stored. There’s actually a number of different ways
    of storing and if you do Rachel’s computational linear algebra course, you’ll
    learn about the different types and why you choose them, and how to convert and
    so forth. But they’re all something like this and you don’t really , on the whole,
    have to worry about the details. The important thing is it’s efficient.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就是它的存储方式。实际上有许多不同的存储方式，如果你学习Rachel的计算线性代数课程，你将了解不同类型的存储方式以及为什么选择它们，如何转换等等。但它们都类似于这样，你不需要太担心细节。重要的是它是高效的。
- en: So we could grab the first review and that gives us 75,000 long sparse one row
    long matrix with 93 stored elements [[1:14:02](https://youtu.be/37sFIak42Sc?t=4442)].
    So in other words, 93 of those words are actually used in the first document.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以拿到第一条评论，这给了我们一个75,000个长稀疏的一行长矩阵，其中有93个存储元素。换句话说，这些单词中有93个实际上在第一个文档中使用。
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can have a look at the vocabulary by saying `veczr.get_feature_names` that
    gives us the vocab. And so here is an example of a few of the elements of feature
    names:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过说`veczr.get_feature_names`来查看词汇表，这给我们提供了词汇表。这里是一些特征名称的元素的示例：
- en: '[PRE14]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: I didn’t intentionally pick the one that had aussie but that’s the important
    words, obviously 😄 I haven’t used the tokenizer here. I’m just splitting on space,
    so this isn’t quite the same as what the vectorizer did. But to simplify things,
    let’s grab a set of all the lowercased words. By making it a set, we make them
    unique. So this is roughly the list of words that would appear.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我并没有故意选择那个有澳大利亚的，但那是重要的单词，显然😄我这里没有使用分词器。我只是按空格分割，所以这与矢量化器所做的不完全相同。但为了简化事情，让我们拿到所有小写单词的集合。通过将其设置为集合，我们使它们成为唯一的。所以这大致是可能出现的单词列表。
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: And that length is 91 which is pretty similar to 93, and just the difference
    will be that I didn’t use a real tokenizer. So that’s basically all that has been
    done there. Created this unique list of words and mapped them. We could check
    by calling `veczr.vocabulary_` to find the ID of a particular word. So this is
    like the reverse map of `veczr.get_feature_names` which maps integer to word,
    `veczr.vocabulary_` maps word to integer.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这个长度是91，与93相似，唯一的区别是我没有使用真正的分词器。所以基本上就是这样。创建了这个唯一的单词列表并将它们映射。我们可以通过调用`veczr.vocabulary_`来查找特定单词的ID。所以这就像`veczr.get_feature_names`的反向映射，它将整数映射到单词，`veczr.vocabulary_`将单词映射到整数。
- en: '[PRE16]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'So we saw “absurd” appear twice in the first document, so let’s check:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们看到“荒谬”在第一个文档中出现了两次，所以让我们检查一下：
- en: '[PRE17]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'There it is, this is 2\. Or else, unfortunately aussie did not appear in the
    unnatural relationship with a pig movie, so this is zero:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是，这是2。否则，不幸的是，澳大利亚人没有出现在与猪有不自然关系的电影中，所以这是零：
- en: '[PRE18]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: So that’s our term document matrix.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的术语文档矩阵。
- en: '**Question**: Does it care about the relationship between the words as in the
    ordering of the words [[1:16:02](https://youtu.be/37sFIak42Sc?t=4562)]? No, we’ve
    thrown away the orderings. That’s why it’s a bag of words. And I’m not claiming
    that this is necessarily a good idea. What I will say is that the vast majority
    of NLP work that’s been done over the last few decades generally uses this representation
    because we didn’t really know much better. Nowadays, increasingly we are using
    recurrent neural networks instead which we will learn about in our last deep learning
    lesson of part 1\. But sometimes this representation works pretty well, and it’s
    actually going to work pretty well in this case.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：它是否关心单词之间的关系，比如单词的顺序？不，我们抛弃了顺序。这就是为什么它是一个词袋。我并不是在声称这一定是一个好主意。我要说的是，在过去几十年里进行的绝大多数自然语言处理工作通常使用这种表示，因为我们并不真正知道更好的方法。如今，我们越来越多地使用递归神经网络，我们将在第1部分的最后一个深度学习课程中学习。但有时这种表示方法效果还不错，实际上在这种情况下也会效果不错。
- en: In fact, back when I was at FastMail, my email company, a lot of the spam filtering
    we did used this next technique Naive Bayes which is a bag of words approach [[1:16:49](https://youtu.be/37sFIak42Sc?t=4609)].
    If you are getting a lot of email containing the word Viagra and it’s always been
    a spam and you never get email from your friends talking about Viagra, then it’s
    very likely something that says Viagra regardless of the detail of the language
    is probably from a spammer. So that’s the basic theory about classification using
    a term document matrix.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，当我在我的电子邮件公司FastMail时，我们做的很多垃圾邮件过滤都使用了下一个技术朴素贝叶斯，这是一种词袋方法。如果你收到很多包含“伟哥”一词的电子邮件，而且它们一直是垃圾邮件，你从来没有收到朋友谈论“伟哥”的电子邮件，那么很可能说“伟哥”的东西无论语言的细节如何，都可能来自垃圾邮件发送者。所以这就是使用术语文档矩阵进行分类的基本理论。
- en: Naive Bayes [[1:17:26](https://youtu.be/37sFIak42Sc?t=4646)]
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: 'Let’s talk about Naive Bayes. And here is the basic idea. We are going to start
    with our term document matrix. And the first two is our corpus of positive reviews.
    The next two is our corpus of negative reviews. And so here is our whole corpus
    of all reviews:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来谈谈朴素贝叶斯。这里是基本思想。我们将从我们的术语文档矩阵开始。前两个是我们的正面评论语料库。接下来两个是我们的负面评论语料库。所以这里是我们所有评论的整个语料库：
- en: '![](../Images/345a0096c2757e1113a1f1770cec0194.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/345a0096c2757e1113a1f1770cec0194.png)'
- en: We tend to call these columns more generically “features” rather than “words”.
    `this` is a feature, `movie` is a feature, etc. So it’s more now like machine
    learning language. A column is a feature. We call those *f* in Naive Bayes. So
    we can basically say the probability that you would see the word `this` given
    that the class is 1 (i.e. positive review) is just the average of how often do
    you see `this` in the positive reviews. Now we’ve got to be a bit careful though,
    because if you never ever see a particular word in a particular class, so if I’ve
    never received an email from a friend that said “Viagra”, that doesn’t actually
    mean the probability of a friend sending me an email about Viagra is zero. It’s
    not really zero. I hope I don’t get an email from Terrence tomorrow saying like
    “Jeremy you probably could use this advertisement for Viagra” but it could happen.
    I’m sure it would be in my best interest 🤣 So what we do is we say actually what
    we’ve seen so far is not the full sample of everything that could happen. It’s
    like a sample of what’s happened so far. So let’s assume that the next email you
    get actually does mention Viagra and every other possible word. So basically we
    are going to add a row of 1’s.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们倾向于更通用地称这些列为“特征”而不是“单词”。`this`是一个特征，`movie`是一个特征，等等。所以现在更像是机器学习语言。一列是一个特征。在朴素贝叶斯中我们称这些为*f*。所以我们基本上可以说，给定类别为1时你会看到单词`this`的概率（即积极评论）就是你在积极评论中看到`this`的频率的平均值。现在我们必须要小心一点，因为如果你在某个类别中从未看到某个单词，所以如果我从未收到过朋友发来的提到“伟哥”的电子邮件，这并不意味着朋友给我发送有关伟哥的电子邮件的概率是零。实际上并不是零。我希望明天不会收到Terrence发来的电子邮件，说“Jeremy，你可能需要这个关于伟哥的广告”，但这种情况可能发生。我相信这对我有利🤣所以我们说实际上到目前为止我们看到的并不是所有可能发生的一切的完整样本。这更像是迄今为止发生的事情的样本。所以让我们假设你接下来收到的电子邮件实际上提到了伟哥和其他所有可能的单词。所以基本上我们要添加一行1。
- en: '![](../Images/af14aab0cab209b7419aee938ab0549e.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af14aab0cab209b7419aee938ab0549e.png)'
- en: 'That’s like the email that contains every possible word. That way, nothing
    is ever infinitely unlikely. So I take the average of all of the times that `this`
    appears in my positive corpus plus the 1''s:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 就像包含每个可能单词的电子邮件一样。这样，没有什么是无限不可能的。所以我取我积极语料库中所有`this`出现的平均次数再加上1：
- en: '![](../Images/4eb27c8051d0d6625447192ce0c79cbe.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4eb27c8051d0d6625447192ce0c79cbe.png)'
- en: So that’s like the probability that `feature = 'this’` appears in a document
    given that `class = 1` (i.e. *p(f|1)* for `this`).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就像是在文档中`feature = 'this'`出现的概率，假设`class = 1`（即`this`的*p(f|1)*）。
- en: 'Not surprisingly, here is the same thing for probability that the feature `this`
    appears given `class = 0`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，这里是相同的概率，即给定`class = 0`时出现`this`的概率：
- en: '![](../Images/e29f33692af0e2321013a217c282d435.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e29f33692af0e2321013a217c282d435.png)'
- en: Same calculation except for the zero rows. Obviously these are the same because
    `this` appears once in the positives, and once in the negatives.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的计算，只是对于零行。显然这些是相同的，因为`this`在积极评论中出现一次，在消极评论中也出现一次。
- en: So we can do that for every feature for every class [[1:20:40](https://youtu.be/37sFIak42Sc?t=4840)]
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为每个特征的每个类别做同样的计算[[1:20:40](https://youtu.be/37sFIak42Sc?t=4840)]
- en: '![](../Images/ae326d59b7b9ffe50d8ae287cec4862f.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae326d59b7b9ffe50d8ae287cec4862f.png)'
- en: So our trick now is to basically use Bayes rule to fill this in. So what we
    want is the probability that given this particular document (so somebody sent
    me this particular email, or I have this particular IMDb review), what is the
    probability that its class is equal to positive. So for this particular movie
    review, what’s the probability that its class is positive. So we can say, well,
    that’s equal to the probability that we got this particular movie review given
    that its class is positive multiplied by the probability that any movie review’s
    class is positive divided by the probability of getting this particular movie
    review.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们现在的技巧基本上是使用贝叶斯规则来填充这个。所以我们想要的是在给定这个特定文档的情况下（所以有人给我发送了这封特定的电子邮件，或者我有这个特定的IMDb评论），其类别等于积极的概率是多少。所以对于这个特定的电影评论，它的类别是积极的概率是多少。所以我们可以说，嗯，这等于我们得到这个特定电影评论的概率，假设它的类别是积极的，乘以任何电影评论的类别是积极的概率除以得到这个特定电影评论的概率。
- en: '![](../Images/de2db7bf513d704a0f182adea43dfeed.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de2db7bf513d704a0f182adea43dfeed.png)'
- en: That’s just Bayes’ rule. So we can calculate all of those things but actually
    what we really want to know is is it more likely that this is class 0 or class
    1\. Wo what if we actually took probability that’s class 1 and divided by probability
    that’s class 0\. What if we did that?
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是贝叶斯规则。所以我们可以计算所有这些东西，但实际上我们真正想知道的是这是类别0还是类别1更有可能。如果我们实际上计算了类别1的概率并除以类别0的概率会怎样呢？
- en: '![](../Images/748dbde38221a70b5d5a64dcdc06372a.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/748dbde38221a70b5d5a64dcdc06372a.png)'
- en: Okay, so if this number is bigger than 1, then it’s more likely to be class
    1, if it’s smaller than 1, it’s more likely to be class 0\. So in that case, we
    could just divide this whole thing by the same version for class 0 which is the
    same as multiplying it by the reciprocal. So the nice thing is now *p(d)* gets
    cancelled and probability of getting the data given class 0 down here, probability
    of getting class 0 here.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，如果这个数字大于1，那么更有可能是类别1，如果小于1，更有可能是类别0。所以在这种情况下，我们可以将整个事情除以类别0的相同版本，这等同于乘以倒数。所以好处是现在*p(d)*被取消了，下面是给定类别0时得到数据的概率，这里是得到类别0的概率。
- en: '![](../Images/6254953e85c80ea37af2c7a14ea24475.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6254953e85c80ea37af2c7a14ea24475.png)'
- en: 'Basically what that means is we want to calculate the probability that we would
    get this particular document given that the class is 1 times the probability that
    the class is 1 divided by the probability of getting this particular document
    given the class is 0 times the probability that the class is 0:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上这意味着我们想要计算的是在类别为1的情况下得到这个特定文档的概率乘以类别为1的概率除以在类别为0的情况下得到这个特定文档的概率乘以类别为0的概率：
- en: '![](../Images/a3ae5c9f5c4f4fa3195160f8d6bd73eb.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3ae5c9f5c4f4fa3195160f8d6bd73eb.png)'
- en: 'So probability that the class is 1 is just equal to the average of the labels
    [[1:23:20](https://youtu.be/37sFIak42Sc?t=5000)]. Probability the class is 0 is
    1 minus that. So there are those two numbers:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 所以类别为1的概率就等于标签的平均值[[1:23:20](https://youtu.be/37sFIak42Sc?t=5000)]。类别为0的概率是1减去那个值。所以有这两个数字：
- en: '![](../Images/89ff7ecc068a644fca1c5d2aea188155.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89ff7ecc068a644fca1c5d2aea188155.png)'
- en: I’ve got an equal amount of both, so it’s both 0.5.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我有相同数量的两者，所以都是0.5。
- en: 'What is the probability of getting this document given that the class is 1?Student:
    Look at all the documents that have class equal to 1 and 1 divided by that would
    give you …[[1:24:02](https://youtu.be/37sFIak42Sc?t=5042)] Jeremy: So remember
    it’s going to be for a particular document. For example would be saying like what’s
    the probability that this review is positive. So you are on the right track, but
    what we are going to have to do is say let’s just look at the words it has, and
    then multiply the probabilities together for class equals 1\. So the probability
    that a class 1 review has `this` is 2/3, the probability it has `movie` is 1,
    `is` is 1, and `good` is 1\. So the probability it has all of them is all of those
    multiplied together. Kinda. Tyler, why is it not really? So glad you look horrified
    and skeptical 😄'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '如果这个文档的类别是1，获得这个文档的概率是多少？学生：看看所有类别等于1的文档，1除以那个会给你……[1:24:02] Jeremy: 所以记住这将是针对特定文档的。例如，会说这个评论是积极的概率是多少。所以你走在正确的轨道上，但我们要做的是看看它包含的单词，然后将类别等于1的概率相乘在一起。所以类别1的评论包含`this`的概率是2/3，包含`movie`的概率是1，`is`是1，`good`是1。所以它包含所有这些的概率是所有这些相乘在一起。有点。Tyler，为什么不是真的？所以很高兴你看起来震惊和怀疑
    😄'
- en: '**Tyler:** Were choices not independent?'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tyler:** 选择不是独立的吗？'
- en: '**Jeremy:** Thank you. So nobody can call Tyler naive because the reason this
    is Naive Bayes is because this is what happens if you take Bayes’ theorem in a
    naive way. And Tyler is not naive. Anything but. So Naive Bayes says let’s assume
    that if you have “this movie is bloody stupid I hate it” but the probability of
    `hate` is independent of the probability of `bloody` is independent of the probability
    of `stupid` which is definitely not true. So Naive Bayes aren’t actually very
    good but I’m teaching it to you because it’s going to turn out to be a convenient
    piece for something we are about to learn later. **Background:** And it often
    works pretty well. **Jeremy:** It’s okay. I mean I would never choose it. I don’t
    think it’s better than any other technique that’s equally fast and equally easy.
    But it’s a thing you can do and it’s certainly going to be useful foundation.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jeremy:** 谢谢。所以没有人能说Tyler天真，因为这是朴素贝叶斯的原因，因为这是如果你以朴素的方式使用贝叶斯定理会发生的事情。而Tyler并不天真。完全不是。所以朴素贝叶斯说让我们假设如果你有“这部电影太愚蠢了，我讨厌它”但`讨厌`的概率独立于`愚蠢`的概率独立于`愚蠢`的概率，这显然是不正确的。所以朴素贝叶斯实际上并不是很好，但我教给你们是因为它将成为我们即将学习的某些东西的便利工具。**背景:**
    而且它通常效果还不错。**Jeremy:** 还好。我的意思是我永远不会选择它。我不认为它比任何其他同样快速和同样简单的技术更好。但这是你可以做的事情，肯定会成为有用的基础。'
- en: 'So here is now calculation of the probability that we get this particular document
    assuming it’s a positive review [[1:26:08](https://youtu.be/37sFIak42Sc?t=5168)]:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在这是计算我们得到这个特定文档的概率，假设它是一个积极的评论[1:26:08]：
- en: '![](../Images/fea3b2ef20e9fadb34f746d07074a06f.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fea3b2ef20e9fadb34f746d07074a06f.png)'
- en: Here is the probability given it’s negative
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这是给定它是负面的概率
- en: '![](../Images/15f1d89de194e433078ca0c79d4aff62.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15f1d89de194e433078ca0c79d4aff62.png)'
- en: And here is the ratio. And the ratio is above 1, so we are going to say we think
    that this is probably a positive review.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是比率。比率大于1，所以我们将说我们认为这可能是一个积极的评论。
- en: '![](../Images/731f32df9f718a94e0b3aba84e849e2e.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/731f32df9f718a94e0b3aba84e849e2e.png)'
- en: So that’s the Excel version. So you can tell I let Yannet touch this because
    it’s got LaTeX in it. We got actual math. So here is the same thing; the log-count
    ratio for each feature *f*.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Excel版本。所以你可以看出我让Yannet来处理这个，因为里面有LaTeX。我们有实际的数学。所以这里是相同的东西；每个特征*f*的对数计数比。
- en: '![](../Images/6d2ea394db692eab3cce0c6d1643b28a.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d2ea394db692eab3cce0c6d1643b28a.png)'
- en: So here it is written out as Python. Our independent variable is our term document
    matrix, our dependent variable is just the labels for the `y`. So using numpy,
    this `x[y==1]` is going to grab the rows where the dependent variable is 1\. Them
    we can sum them over the rows to get the total word count for that feature across
    all the documents, plus 1 — Terrence is totally going to send me something about
    Viagra today, I can tell. That’s that. Then do the same thing for the negative
    reviews. Then of course it’s nicer to take the log because if we take the log,
    then we can add things together rather than multiply them together. And once you
    multiply enough of these things together, it’s going to get so close to zero that
    you’ll probably run out of the floating-point. So we take the log of the ratios.
    Then, as I say, we then multiply that, or with log, we add that to the ratio of
    the whole class probabilities.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是用Python写出来的。我们的自变量是我们的术语文档矩阵，我们的因变量只是`y`的标签。所以使用numpy，这个`x[y==1]`将抓取因变量为1的行。然后我们可以对行求和，以获得该特征在所有文档中的总词数，再加1
    — Terrence今天肯定会给我发关于伟哥的东西，我能感觉到。就是这样。然后对负面评论做同样的事情。然后当然最好取对数，因为如果我们取对数，那么我们可以将事物相加而不是相乘。一旦你将足够多的这些东西相乘在一起，它将接近零，你可能会用完浮点数。所以我们取这些比率的对数。然后，正如我所说的，我们将将其相乘，或者用对数，将其加到整个类别概率的比率上。
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: So in order to say for each document, multiply the Bayes’ probabilities by the
    counts, we can just use matrix multiply. Then to add on the log of the class ratios,
    you can just use `+ b`. So we end up something that looks a lot like a logistic
    regression. But we are not learning anything. Not in kind of SGD point of view.
    We are just calculating it using this theoretical model. As I said, we can then
    compare that as to whether it’s bigger or smaller than zero — not one anymore
    because we are now in log space. Then we can compare that to the mean. And that’s
    ~81% accurate. So Naive Bayes is not nothing. It gave us something.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对每个文档说，将贝叶斯概率乘以计数，我们可以使用矩阵乘法。然后添加类别比率的对数，你可以使用`+ b`。所以我们最终得到的东西看起来很像逻辑回归。但我们并没有学到任何东西。不是从SGD的角度来看。我们只是使用这个理论模型进行计算。正如我所说，我们可以将其与零进行比较，看看它是更大还是更小
    — 不再是1，因为我们现在处于对数空间。然后我们可以将其与均值进行比较。这样的准确率约为81%。所以朴素贝叶斯并不是没有用的。它给了我们一些东西。
- en: '[PRE20]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It turns out that this version where we are actually looking at how often a
    appears, like “absurd” appeared twice, it turns out at least for this problem
    and quite often it doesn’t matter whether “absurd” appeared twice or once [[1:29:03](https://youtu.be/37sFIak42Sc?t=5343)].
    All that matter is that it appeared. So what people tend to try doing is to say
    take the term matrix document and go `.sign()` which replaces anything positive
    as `1`, and anything negative with `-1` (we don’t have negative counts obviously).
    So this binarizes it. It says I don’t care that you saw “absurd” twice, I just
    care that you saw it. So if we do exactly the same thing with the binarized version,
    then you get a better result.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这个版本实际上是在看a出现的频率，比如“荒谬”出现了两次，至少对于这个问题来说，通常无论“荒谬”出现了两次还是一次都无关紧要[[1:29:03](https://youtu.be/37sFIak42Sc?t=5343)]。重要的是它出现了。所以人们倾向于尝试的是取术语矩阵文档并使用`.sign()`，它会将任何正数替换为`1`，将任何负数替换为`-1`（显然我们没有负计数）。这样就实现了二值化。它表示我不在乎你看到“荒谬”两次，我只在乎你看到它。所以如果我们对二值化版本做完全相同的事情，那么结果会更好。
- en: '[PRE21]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Logistic regression [[1:30:01](https://youtu.be/37sFIak42Sc?t=5401)]
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归[[1:30:01](https://youtu.be/37sFIak42Sc?t=5401)]
- en: Now this is the difference between theory and practice. In theory, Naive Bayes
    sounds okay but it’s naive, unlike Tyler, it’s naive. So what Tyler would probably
    do would instead say rather than assuming that I should use these coefficients
    `r`, why don’t we learn them? So let’s learn them. We can totally learn them.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这就是理论和实践之间的区别。理论上，朴素贝叶斯听起来还可以，但是它是朴素的，不像泰勒，它是朴素的。所以泰勒可能会说，与其假设我应该使用这些系数`r`，为什么不让我们学习它们呢？所以让我们学习它们。我们完全可以学习它们。
- en: So let’s create a logistic regression, and let’s fit some coefficients. And
    that’s going to literally give us something with exactly the same functional form
    that we had before but now rather than using a theoretical `r` and theoretical
    `b`, we are going to calculate the two things based on logistic regression. And
    that’s better.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们创建一个逻辑回归，并拟合一些系数。这实际上会给我们提供与之前完全相同的功能形式，但现在我们不再使用理论上的`r`和理论上的`b`，而是根据逻辑回归计算这两个值。这样更好。
- en: '[PRE22]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: So it’s kind of like yeah, why do something based on some theoretical model?
    Because theoretical models are never going to be as accurate, pretty much, as
    a data driven model. Because theoretical models, unless you are dealing with some
    physics thing or something where you’re like okay this is actually how the world
    works, there really is no … I don’t know, we are working in a vacuum and there
    is the exact gravity, etc. But most of the real world, this is how things are
    — it’s better to learn your coefficients and calculate them.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这有点像是，为什么要基于某种理论模型进行某些操作呢？因为理论模型几乎永远不会像数据驱动模型那样准确。因为理论模型，除非你在处理某种物理问题或者某种你认为这实际上是世界如何运作的东西，否则没有……我不知道，我们是在真空中工作，有确切的重力等等。但是在现实世界中，事情是这样的——更好的方法是学习你的系数并计算它们。
- en: '**Yannet**: What’s this dual=True [[1:31:30](https://youtu.be/37sFIak42Sc?t=5490)]?
    I was hoping you’d ignore, not notice, but you saw it. Basically in this case,
    our term document matrix is much wider than it is tall. There is an almost mathematically
    equivalent reformulation of logistic regression that happens to be a lot faster
    when it’s wider than it is tall. So the short answer is anytime it’s wider than
    it’s tall, put `dual=True`, it’ll run fast. This runs in like 2 seconds. If you
    don’t have it here, it’ll take a few minutes. So in math, there is a concept of
    dual versions of problems which are kind of like equivalent versions that sometimes
    work better for certain situations.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**Yannet**：这个`dual=True`是什么[[1:31:30](https://youtu.be/37sFIak42Sc?t=5490)]？我希望你会忽略，不会注意到，但你看到了。基本上，在这种情况下，我们的术语文档矩阵比高度更宽。逻辑回归有一个几乎在数学上等价的重新表述，当它比高度更宽时，速度会更快。简短的答案是，每当它比高度更宽时，加上`dual=True`，它会运行得更快。这只需要2秒。如果你不在这里加上它，那么需要几分钟。因此，在数学中，有一种问题的双重版本的概念，这些版本在某些情况下可能更适合。'
- en: Here is the binarized version [[1:32:20](https://youtu.be/37sFIak42Sc?t=5540)].
    It’s about the same. So you can see I’ve fitted it with the sign of term doc matrix,
    and predicted with `val_term_doc.sign()`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这是二值化版本[[1:32:20](https://youtu.be/37sFIak42Sc?t=5540)]。差不多一样。所以你可以看到我用术语文档矩阵的符号进行了拟合，并用`val_term_doc.sign()`进行了预测。
- en: '[PRE23]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now the thing is that this is going to be a coefficient for every term where
    there was about 75,000 terms in our vocabulary and that seems like a lot of coefficients
    given that we’ve only got 25,000 reviews [[1:32:38](https://youtu.be/37sFIak42Sc?t=5558)].
    So maybe we should try regularizing this.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在问题是，对于我们词汇表中大约有75,000个术语的每个术语都会有一个系数，考虑到我们只有25,000条评论，这似乎是很多系数[[1:32:38](https://youtu.be/37sFIak42Sc?t=5558)]。所以也许我们应该尝试对此进行正则化。
- en: So we can use regularization built into sklearn’s LogisticRegression class which
    is `C` is the parameter that they use. This is slightly weird, a smaller parameter
    is more regularization. What’s why I used `1e8` to basically turn off regularization.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以使用内置在sklearn的LogisticRegression类中的正则化，它使用的参数是`C`。这有点奇怪，较小的参数表示更多的正则化。这就是为什么我使用`1e8`基本上关闭了正则化。
- en: '![](../Images/30ffd979a7eb492e752bc97a7b8a76e0.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30ffd979a7eb492e752bc97a7b8a76e0.png)'
- en: 'So if I turn on regularization, set it to 0.1, then now it’s 88%:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我打开正则化，将其设置为0.1，那么现在是88%：
- en: '[PRE24]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Which makes sense. You would think 75,000 parameters for 25,000 documents, it’s
    likely to overfit. Indeed, it did overfit. So this is adding L2 regularization
    to avoid overfitting.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有道理的。你会认为对于25,000个文档的75,000个参数，很可能会过拟合。事实上，它确实过拟合了。因此，这是添加L2正则化以避免过拟合。
- en: I mentioned earlier that as well as L2 which is looking at the weight squared,
    there’s also L1 which is looking at just the absolute value of the weights [[1:33:37](https://youtu.be/37sFIak42Sc?t=5617)].
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到过，除了L2（查看权重的平方）之外，还有L1（仅查看权重的绝对值）[[1:33:37](https://youtu.be/37sFIak42Sc?t=5617)]。
- en: '![](../Images/a7921d0b1ab6d15d730468e03b149186.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7921d0b1ab6d15d730468e03b149186.png)'
- en: I was kind of pretty sloppy in my wording before I said that L2 tries to make
    things zero. That’s kind of true but if you’ve got two things that are highly
    correlated, then L2 regularization will move them both down together. It won’t
    make one of them zero and one of them nonzero. So L1 regularization actually has
    the property that it will try to make as many things zero as possible where else
    L2 regularization has a property that it tends to try to make everything smaller.
    We actually don’t care about that difference in really any modern machine learning
    because we very rarely try to directly interpret the coefficients. We try to understand
    our models through interrogation using the kind of techniques that we’ve learned.
    The reason we would care about L1 versus L2 is simply which one ends up with a
    better error on the validation set. And you can try both. With sklearn’s LogisticRegression,
    L2 actually turns out to be a lot faster because you can’t use `dual=True` unless
    you have L2 and L2 is the default. So I didn’t really worry too much about the
    difference yet.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前在措辞上有点粗心，我说L2试图将事物变为零。这在某种程度上是正确的，但如果你有两个高度相关的事物，那么L2正则化会将它们一起降低。它不会使其中一个变为零，另一个变为非零。因此，L1正则化实际上具有这样的特性，它会尽可能使尽可能多的事物变为零，而L2正则化具有这样的特性，它倾向于使一切变得更小。实际上，在任何现代机器学习中，我们并不关心这种差异，因为我们很少直接尝试解释系数。我们尝试通过我们学到的技术来审查我们的模型。我们关心L1与L2的原因仅仅是哪一个在验证集上的错误更小。你可以尝试两种方法。使用sklearn的LogisticRegression，L2实际上会更快，因为你不能使用`dual=True`，除非你使用L2，而L2是默认的。所以我并没有太担心这种差异。
- en: 'So you can see here if we use regularization and binarized, we actually do
    pretty well [[1:35:04](https://youtu.be/37sFIak42Sc?t=5704)]:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到，如果我们使用正则化和二值化，我们实际上做得相当不错：
- en: '[PRE25]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Question**: Before we learned about Elastic-net like combining L1 and L2\.
    Can we do that [[1:35:23](https://youtu.be/37sFIak42Sc?t=5723)]? Yeah, you can
    do that, but with deeper models. And I’ve never seen anybody find that useful.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：在我们学习关于类似于组合L1和L2的Elastic-net之前。我们可以这样做吗？是的，你可以这样做，但需要更深层次的模型。我从来没有见过有人发现这有用。'
- en: So the last thing I’ll mention is that when you do your CountVectorizer, you
    can also ask for n-grams. By default, we get unigrams that is single words. But
    if we say `ngram_range=(1,3)`, that’s also going to give us bigrams and trigrams.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后我要提到的是，当你做CountVectorizer时，你也可以要求n-gram。默认情况下，我们得到的是单字，也就是单个单词。但是如果我们说`ngram_range=(1,3)`，那也会给我们二元组和三元组。
- en: '[PRE26]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'By which I mean, if I now say okay, let’s go ahead and do the CountVectorizer,
    and get_feature_names, now my vocabulary includes bigram: `''by vast''` , `''by
    vengeance''` and trigram: `''by vengeance .''` `''by vera miles''` . So this now
    doing the same thing but after tokenizing, it’s not just grabbing each word and
    saying that’s part of your vocabulary, but each two words next to each other,
    and each three words next to each other. And this turns out to be super helpful
    in taking advantage of bag of word approaches because we now can see the difference
    between `not good` versus `not bad` versus `not terrible`. Or even like `"good"`
    which is probably going to be sarcastic. So using trigram features actually is
    going to turn out to make both Naive Bayes and logistic regression quite a lot
    better. It really takes us quite a lot further and makes them quite useful.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，如果我现在说好的，让我们继续使用CountVectorizer，并获取特征名称，现在我的词汇表包括二元组：`'by vast'`，`'by vengeance'`和三元组：`'by
    vengeance .'`，`'by vera miles'`。所以现在做的事情与之前相同，但在分词之后，它不仅仅是抓取每个单词并说这是你的词汇表的一部分，而是抓取相邻的每两个单词和每三个单词。这实际上对利用词袋方法非常有帮助，因为我们现在可以看到`not
    good`与`not bad`与`not terrible`之间的区别。甚至像`"good"`这样的词可能是讽刺的。因此，实际上使用三元组特征将使朴素贝叶斯和逻辑回归变得更好。这确实让我们走得更远，使它们变得更有用。
- en: '**Question**: I have a question about tokenizers. You are saying `max_features`,
    so how are these bigrams and trigrams selected [[1:37:17](https://youtu.be/37sFIak42Sc?t=5837)]?
    Since I’m using a linear model, I didn’t want to create too many features. It
    actually worked fine even without `max_features`. I think I had something like
    70 million coefficients. It still worked. But there’s no need to have 70 million
    coefficients. So if you say `max_features=800,000`, the CountVectorizer will sort
    the vocabulary by how often everything appears whether it be unigram, bigram,
    trigram, and it will cut it off after the first 800,000 most common ngrams. N-gram
    is just a generic word for unigram, bigram, and trigram.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我有一个关于分词器的问题。你说`max_features`，那么这些二元组和三元组是如何选择的？由于我使用的是线性模型，我不想创建太多特征。即使没有`max_features`，它实际上也可以正常工作。我想我有大约7000万个系数。它仍然有效。但没有必要有7000万个系数。所以如果你说`max_features=800,000`，CountVectorizer将按照所有内容出现的频率对词汇表进行排序，无论是单字、二元组还是三元组，然后在前800,000个最常见的n元组之后截断。N-gram只是单字、二元组和三元组的通用词。'
- en: '![](../Images/c736d56bd03763b4bdf8891e71651dc6.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c736d56bd03763b4bdf8891e71651dc6.png)'
- en: So that’s why the `train_term_doc.shape` is now 25,000 by 800,000\. If you are
    not sure what number the max should be, I just picked something that was really
    big and didn’t worry about it too much and it seemed to be fine. It’s not terribly
    sensitive.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是为什么`train_term_doc.shape`现在是25,000乘以800,000。如果你不确定最大值应该是多少，我只是选择了一个非常大的数字，不太担心，似乎也没问题。这并不是非常敏感的。
- en: 'Okay, we are out of time so what we are going to see tomorrow… By the way,
    we could have replaced this LogisticRegression with our PyTorch version:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，我们时间到了，明天我们将看到什么...顺便说一句，我们可以用我们的PyTorch版本替换这个LogisticRegression：
- en: '![](../Images/6778286629f183885b16c861a1337d52.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6778286629f183885b16c861a1337d52.png)'
- en: And tomorrow, we’ll actually see something in the Fast AI library that does
    exactly that but also what we will see tomorrow is how to combine logistic regression
    and Naive Bayes together to get something better than either. Then we’ll learn
    how to move from there to create a deeper neural network to get pretty much state-of-the-art
    result for structured learning. All right. We’ll see you then.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 明天，我们实际上会在Fast AI库中看到一个可以做到这一点的东西，但明天我们还将看到如何将逻辑回归和朴素贝叶斯结合在一起，以获得比任何一个都更好的结果。然后我们将学习如何从那里开始创建一个更深层的神经网络，以获得几乎是结构化学习的最新结果。好的，到时候见。
