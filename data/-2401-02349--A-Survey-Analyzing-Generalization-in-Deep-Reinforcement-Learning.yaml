- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:35:20'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:35:20'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2401.02349] A Survey Analyzing Generalization in Deep Reinforcement Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2401.02349] 关于深度强化学习泛化的调查分析'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.02349](https://ar5iv.labs.arxiv.org/html/2401.02349)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.02349](https://ar5iv.labs.arxiv.org/html/2401.02349)
- en: A Survey Analyzing Generalization in
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于泛化的调查分析
- en: Deep Reinforcement Learning
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: Ezgi Korkmaz
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Ezgi Korkmaz
- en: University College London
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦大学学院
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Reinforcement learning research obtained significant success and attention with
    the utilization of deep neural networks to solve problems in high dimensional
    state or action spaces. While deep reinforcement learning policies are currently
    being deployed in many different fields from medical applications to self driving
    vehicles, there are still ongoing questions the field is trying to answer on the
    generalization capabilities of deep reinforcement learning policies. In this paper,
    we will outline the fundamental reasons why deep reinforcement learning policies
    encounter overfitting problems that limit their robustness and generalization
    capabilities. Furthermore, we will formalize and unify the diverse solution approaches
    to increase generalization, and overcome overfitting in state-action value functions.
    We believe our study can provide a compact systematic unified analysis for the
    current advancements in deep reinforcement learning, and help to construct robust
    deep neural policies with improved generalization abilities.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习研究通过利用深度神经网络在高维状态或动作空间中解决问题，取得了显著的成功和关注。虽然深度强化学习策略目前已经在从医疗应用到自动驾驶车辆等多个领域得到应用，但仍有许多未解的问题在等待解决，特别是关于深度强化学习策略的泛化能力。在本文中，我们将概述深度强化学习策略遇到过拟合问题的基本原因，这些问题限制了其鲁棒性和泛化能力。此外，我们将对各种提高泛化能力并克服状态-动作值函数中过拟合的解决方法进行形式化和统一。我们相信我们的研究可以为当前深度强化学习的进展提供紧凑的系统统一分析，并帮助构建具有更好泛化能力的鲁棒深度神经策略。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: The performance of reinforcement learning algorithms has been boosted with the
    utilization of deep neural networks as function approximators (Mnih et al., [2015](#bib.bib53)).
    Currently, it is possible to learn deep reinforcement learning policies that can
    operate in large state and/or action space MDPs Silver et al. ([2017](#bib.bib62));
    Vinyals et al. ([2019](#bib.bib71)). This progress consequently resulted in building
    reasonable deep reinforcement learning policies that can play computer games with
    high dimensional state representations (e.g. Atari, StarCraft), solve complex
    robotics control tasks, design algorithms (Mankowitz et al., [2023](#bib.bib52);
    Fawzi et al., [2022](#bib.bib14)), and play some of the most complicated board
    games (e.g. Chess, Go) (Schrittwieser et al., [2020](#bib.bib60)). However, deep
    reinforcement learning algorithms also experience several problems caused by their
    overall limited generalization capabilities. Some studies demonstrated these problems
    via adversarial perturbations introduced to the state observations of the policy
    (Huang et al., [2017](#bib.bib26); Kos & Song, [2017](#bib.bib42); Korkmaz, [2022](#bib.bib39)),
    several focused on exploring the fundamental issues with function approximation,
    estimation biases in the state-action value function (Hasselt et al., [2016](#bib.bib23)),
    or with new architectural design ideas (Wang et al., [2016](#bib.bib73)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法的性能通过将深度神经网络作为函数近似器（Mnih et al., [2015](#bib.bib53)）得到了提升。目前，能够学习到可以在大状态和/或动作空间MDP中运行的深度强化学习策略（Silver
    et al., [2017](#bib.bib62)；Vinyals et al., [2019](#bib.bib71)）。这一进展使得构建出能够在高维状态表示（如Atari,
    StarCraft）下进行计算机游戏、解决复杂机器人控制任务、设计算法（Mankowitz et al., [2023](#bib.bib52)；Fawzi
    et al., [2022](#bib.bib14)）以及玩一些最复杂的棋盘游戏（如国际象棋、围棋）（Schrittwieser et al., [2020](#bib.bib60)）的合理深度强化学习策略成为可能。然而，深度强化学习算法也遇到了一些由于其总体泛化能力有限而引发的问题。一些研究通过引入对策略状态观测的对抗性扰动（Huang
    et al., [2017](#bib.bib26)；Kos & Song, [2017](#bib.bib42)；Korkmaz, [2022](#bib.bib39)）来展示这些问题，另一些则集中在探索函数近似、状态-动作值函数中的估计偏差（Hasselt
    et al., [2016](#bib.bib23)）或新架构设计思想（Wang et al., [2016](#bib.bib73)）的基本问题。
- en: The fact that we are not able to completely explore the entire MDP for high
    dimensional state representation MDPs, even with deep neural networks as function
    approximators, is one of the root problems that limits generalization. On top
    of this, some portion of the problems are directly caused by the utilization of
    deep neural networks and thereby the intrinsic problems inherited from their utilization
    (Goodfellow et al., [2015](#bib.bib21); Szegedy et al., [2014](#bib.bib63); Korkmaz,
    [2022](#bib.bib39)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用深度神经网络作为函数逼近器，我们也无法完全探索高维状态表示的MDP，这一点是限制泛化能力的根本问题之一。除此之外，一些问题直接由深度神经网络的使用引发，从而继承了其固有的问题（Goodfellow
    et al., [2015](#bib.bib21); Szegedy et al., [2014](#bib.bib63); Korkmaz, [2022](#bib.bib39)）。
- en: 'In this paper we will focus on generalization in deep reinforcement learning
    and the underlying causes of the limitations deep reinforcement learning research
    currently faces. In particular, we will try to answer the following questions:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将关注深度强化学习中的泛化及其当前面临的限制的根本原因。特别地，我们将尝试回答以下问题：
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What is the role of exploration in overfitting for deep reinforcement learning?
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索在深度强化学习中的过拟合中扮演什么角色？
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What are the causes of overestimation bias observed in state-action value functions?
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在状态-动作值函数中观察到的高估偏差的原因是什么？
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What has been done to overcome the overfitting problems that deep reinforcement
    learning algorithms have encountered so far?
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到目前为止，为克服深度强化学习算法遇到的过拟合问题，已经做了哪些工作？
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What future directions are there for reinforcement learning research to obtain
    higher level generalization abilities for deep neural policies?
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 强化学习研究未来的方向有哪些，以获得更高水平的深度神经策略泛化能力？
- en: To answer these questions we will go through research connecting several subfields
    in reinforcement learning on the problems and corresponding proposed solutions
    regarding generalization. In this paper we introduce a categorization of the different
    methods used to both achieve and test generalization, and use it to systematically
    summarize and consolidate the current body of research. We further describe the
    issue of value function overestimation, and the role of exploration in overfitting
    in reinforcement learning. Furthermore, we explain new emerging research areas
    that can potentially target these questions in the long run including meta-reinforcement
    learning and lifelong learning. We hope that our paper can provide a compact overview
    and unification of the current advancements and limitations in the field.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为回答这些问题，我们将探讨连接强化学习多个子领域的研究，关注泛化相关的问题及其相应提出的解决方案。本文介绍了用于实现和测试泛化的不同方法的分类，并用其系统地总结和整合当前的研究成果。我们进一步描述了价值函数高估的问题以及探索在强化学习中过拟合的作用。此外，我们解释了可能在长期内解决这些问题的新兴研究领域，包括元强化学习和终身学习。我们希望我们的论文能够提供对当前领域进展和局限性的简明概述和统一。
- en: 2 Preliminaries on Deep Reinforcement Learning
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度强化学习的基础
- en: The aim in deep reinforcement learning is to learn a policy via interacting
    with an environment in a Markov Decision Process (MDP) that maximize expected
    cumulative discounted rewards. An MDP is represented by a tuple $\mathcal{M}=(S,A,P,r,\rho_{0},\gamma)$,
    where $S$ represents the state space, $A$ represents the action space, $r:S\times
    A\to\mathbb{R}$ is a reward function, $\mathcal{P}:S\times A\to\Delta(S)$ is a
    transition probability kernel, $\rho_{0}$ represents the initial state distribution,
    and $\gamma$ represents the discount factor. The objective in reinforcement learning
    is to learn a policy $\pi:S\to\Delta(A)$ which maps states to probability distributions
    on actions in order to maximize the expected cumulative reward $R=\mathbb{E}\sum_{t=0}^{T-1}\gamma^{t}r(s_{t},a_{t})$
    where $a_{t}\sim\pi(s_{t}),s_{t+1}\sim\mathcal{P}(s_{t},a_{t})$. In $Q$-learning
    the goal is to learn the optimal state-action value function (Watkins, [1989](#bib.bib74))
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习的目标是通过与环境在马尔可夫决策过程（MDP）中交互来学习一种策略，以最大化期望的累积折扣奖励。MDP由一个五元组 $\mathcal{M}=(S,A,P,r,\rho_{0},\gamma)$
    表示，其中 $S$ 代表状态空间，$A$ 代表动作空间，$r:S\times A\to\mathbb{R}$ 是奖励函数，$\mathcal{P}:S\times
    A\to\Delta(S)$ 是转移概率核，$\rho_{0}$ 代表初始状态分布，$\gamma$ 代表折扣因子。强化学习的目标是学习一个策略 $\pi:S\to\Delta(A)$，该策略将状态映射到动作的概率分布上，以最大化期望的累积奖励
    $R=\mathbb{E}\sum_{t=0}^{T-1}\gamma^{t}r(s_{t},a_{t})$，其中 $a_{t}\sim\pi(s_{t}),s_{t+1}\sim\mathcal{P}(s_{t},a_{t})$。在
    $Q$-学习中，目标是学习最优的状态-动作价值函数（Watkins, [1989](#bib.bib74)）
- en: '|  | $Q^{*}(s,a)=R(s,a)+\sum_{s^{\prime}\in S}P(s^{\prime}&#124;s,a)\max_{a^{\prime}\in
    A}Q^{*}(s^{\prime},a^{\prime}).$ |  | (1) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{*}(s,a)=R(s,a)+\sum_{s^{\prime}\in S}P(s^{\prime}\mid s,a)\max_{a^{\prime}\in
    A}Q^{*}(s^{\prime},a^{\prime}).$ |  | (1) |'
- en: This is achieved via iterative Bellman update which updates $Q(s_{t},a_{t})$
    by
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过迭代的贝尔曼更新实现的，该更新通过
- en: '|  | $Q(s_{t},a_{t})+\alpha[\mathcal{R}_{t+1}+\gamma\max_{a}Q(s_{t+1},a)-Q(s_{t},a_{t})].$
    |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(s_{t},a_{t})+\alpha[\mathcal{R}_{t+1}+\gamma\max_{a}Q(s_{t+1},a)-Q(s_{t},a_{t})].$
    |  |'
- en: Thus, the optimal policy is determined by choosing the action $a^{*}(s)=\operatorname*{arg\,max}_{a}Q(s,a)$
    in state $s$. In high dimensional state space or action space MDPs the optimal
    policy is decided via a function-approximated state-action value function represented
    by a deep neural network. In a parallel line of algorithm families the policy
    itself is directly parametrized by $\pi_{\theta}$, and the gradient estimator
    used in learning is
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最优策略是通过在状态 $s$ 中选择动作 $a^{*}(s)=\operatorname*{arg\,max}_{a}Q(s,a)$ 来确定的。在高维状态空间或动作空间的
    MDP 中，最优策略通过由深度神经网络表示的函数逼近的状态-动作价值函数来决定。在一个平行的算法家族中，策略本身由 $\pi_{\theta}$ 直接参数化，学习中使用的梯度估计器是
- en: '|  | $g=\mathbb{E}_{t}\big{[}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})(Q(s_{t},a_{t})-\max_{a}Q(s_{t},a))\big{]}$
    |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $g=\mathbb{E}_{t}\big{[}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})(Q(s_{t},a_{t})-\max_{a}Q(s_{t},a))\big{]}$
    |  |'
- en: where $Q(s_{t},a_{t})$ refers to the state-action value function at timestep
    $t$.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q(s_{t},a_{t})$ 指的是时间步 $t$ 的状态-动作价值函数。
- en: 3 How to Achieve Generalization?
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 如何实现泛化？
- en: To be able to categorize different paths to achieve generalization first we
    will provide a definition meant to capture the behavior of a generic reinforcement
    learning algorithm.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够对不同的路径进行分类以实现泛化，我们首先提供一个定义，旨在捕捉通用强化学习算法的行为。
- en: Definition 3.1.
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.1.
- en: A reinforcement learning training algorithm $\mathcal{A}$ learns a policy $\pi$
    by interacting with an MDP $\mathcal{M}$. We divide up the execution of $\mathcal{A}$
    into discrete time steps as follows. At each time $t$, the algorithm chooses a
    state $s_{t}$, takes an action $a_{t}$, observes a transition to state $s^{\prime}_{t}$
    with corresponding reward $r_{t}=r(s_{t},a_{t},s^{\prime}_{t})$. We define the
    history of algorithm $\mathcal{A}$ in MDP $\mathcal{M}$ to be the sequence $H_{t}=(s_{0},a_{0},s^{\prime}_{0},r_{0}),\dots(s_{t},a_{t},s^{\prime}_{t},r_{t})$
    of all the transitions observed by the algorithm so far. We require that state
    and action $(s_{t},a_{t})$ chosen at time $t$ are a function only of $H_{t-1}$,
    i.e the transitions observed so far by $\mathcal{A}$. At time $t=T$, the algorithm
    stops and outputs a policy $\pi$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个强化学习训练算法$\mathcal{A}$通过与MDP $\mathcal{M}$交互来学习一个策略$\pi$。我们将$\mathcal{A}$的执行分为离散时间步骤，如下所示。在每个时间$t$，算法选择一个状态$s_{t}$，采取一个动作$a_{t}$，观察到状态$s^{\prime}_{t}$的转移及其对应的奖励$r_{t}=r(s_{t},a_{t},s^{\prime}_{t})$。我们将算法$\mathcal{A}$在MDP
    $\mathcal{M}$中的历史定义为序列$H_{t}=(s_{0},a_{0},s^{\prime}_{0},r_{0}),\dots(s_{t},a_{t},s^{\prime}_{t},r_{t})$，即算法迄今为止观察到的所有转移。我们要求在时间$t$选择的状态和动作$(s_{t},a_{t})$仅是$H_{t-1}$的函数，即算法迄今为止观察到的转移。在时间$t=T$时，算法停止并输出一个策略$\pi$。
- en: Intuitively, a reinforcement learning algorithm performs a sequence of queries
    $(s_{t},a_{t})$ to the MDP, and observes the resulting state transitions and rewards.
    In order to be as generic as possible, the definition makes no assumptions about
    how the algorithm chooses the sequence of queries. Notably, if taking action $a_{t}$
    in state $s_{t}$ leads to a transition to state $s^{\prime}_{t}$, there is no
    requirement that $s_{t+1}=s^{\prime}_{t}$. Indeed, the only assumption is that
    $(s_{t+1},a_{t+1})$ depends only on $H_{t}$, the history of transitions observed
    so far. This allows the definition to capture deep reinforcement learning algorithms,
    which may choose to query states and actions in a complex way based on previously
    observed state transitions. Based on this definition of generic reinforcement
    learning algorithm, we will now further define the different techniques proposed
    to achieve generalization.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，强化学习算法对MDP执行一系列查询$(s_{t},a_{t})$，并观察结果状态转换和奖励。为了尽可能地通用，定义没有对算法如何选择查询序列做出假设。特别地，如果在状态$s_{t}$下采取动作$a_{t}$导致转移到状态$s^{\prime}_{t}$，并不要求$s_{t+1}=s^{\prime}_{t}$。实际上，唯一的假设是$(s_{t+1},a_{t+1})$仅依赖于$H_{t}$，即迄今为止观察到的转移历史。这使得定义能够涵盖深度强化学习算法，这些算法可能基于先前观察到的状态转换以复杂的方式选择查询状态和动作。基于这种通用强化学习算法的定义，我们将进一步定义为实现泛化而提出的不同技术。
- en: Definition 3.2  (*Rewards transforming generalization*).
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.2  (*奖励转化泛化*)。
- en: Let $\mathcal{A}$ be a training algorithm that takes as input an MDP and outputs
    a policy. Given an MDP $\mathcal{M}=(S,A,P,r,\rho_{0},\gamma)$, a *rewards transforming*
    generalization method $\mathcal{G}_{R}$ is given by a sequence of functions $F_{t}:(S\times
    A\times S\times\mathbb{R})^{t}\to\mathbb{R}$. The method attempts to achieve generalization
    by running $\mathcal{A}$ on MDP $\mathcal{M}$, but modifying the rewards at each
    time $t$ to be $\tilde{r}_{t}(s_{t},a_{t},s^{\prime}_{t})=F_{t-1}(H_{t-1})$, where
    $H_{t-1}$ is the history of algorithm $\mathcal{A}$ when running with the perturbed
    rewards.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 设$\mathcal{A}$是一个训练算法，输入为MDP并输出一个策略。给定一个MDP $\mathcal{M}=(S,A,P,r,\rho_{0},\gamma)$，*奖励转化*
    泛化方法$\mathcal{G}_{R}$由一系列函数$F_{t}:(S\times A\times S\times\mathbb{R})^{t}\to\mathbb{R}$给出。该方法试图通过在MDP
    $\mathcal{M}$上运行$\mathcal{A}$来实现泛化，但在每个时间$t$修改奖励为$\tilde{r}_{t}(s_{t},a_{t},s^{\prime}_{t})=F_{t-1}(H_{t-1})$，其中$H_{t-1}$是算法$\mathcal{A}$在使用扰动奖励运行时的历史。
- en: In summary, a rewards transforming generalization methods simply runs the original
    algorithm, but modifies the observed rewards. Similarly, we define two additional
    generalization methods which run the original algorithm while modifying states
    and transition probabilities respectively.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，奖励转化泛化方法只是运行原始算法，但修改观察到的奖励。类似地，我们定义了另外两种泛化方法，它们在运行原始算法的同时，分别修改状态和转移概率。
- en: Definition 3.3  (*State transforming generalization*).
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.3  (*状态转化泛化*)。
- en: Let $\mathcal{A}$ be a training algorithm that takes as input an MDP and outputs
    a policy. Given an MDP $\mathcal{M}=(S,A,P,r,\rho_{0},\gamma)$, a *state transforming*
    generalization method $\mathcal{G}_{S}$ is given by a sequence of functions $F_{t}:(S\times
    A\times S\times\mathbb{R})^{t}\times S\to S$. The method attempts to achieve generalization
    by running $\mathcal{A}$ on MDP $\mathcal{M}$, but modifying the state chosen
    at time $t$ to be $\tilde{s}_{t}=F_{t-1}(H_{t-1},s_{t})$, where $H_{t-1}$ is the
    history of algorithm $\mathcal{A}$ when running with the perturbed states.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 设$\mathcal{A}$是一个训练算法，输入一个MDP并输出一个策略。给定一个MDP $\mathcal{M}=(S,A,P,r,\rho_{0},\gamma)$，一个*状态变换*泛化方法$\mathcal{G}_{S}$由一系列函数$F_{t}:(S\times
    A\times S\times\mathbb{R})^{t}\times S\to S$给出。该方法试图通过在MDP $\mathcal{M}$上运行$\mathcal{A}$来实现泛化，但通过将时间$t$选择的状态修改为$\tilde{s}_{t}=F_{t-1}(H_{t-1},s_{t})$，其中$H_{t-1}$是算法$\mathcal{A}$在使用扰动状态时的历史记录。
- en: Definition 3.4  (*Transition probability transforming generalization*).
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.4  (*转移概率变换泛化*)。
- en: Let $\mathcal{A}$ be a training algorithm that takes as input an MDP and outputs
    a policy. Given an MDP $\mathcal{M}=(S,A,P,r,\rho_{0},\gamma)$, a *transition
    probability transforming* generalization method $\mathcal{G}_{\mathcal{P}}$ is
    given by a sequence of functions $F_{t}:(S\times A\times S\times\mathbb{R})^{t}\times(S\times
    A\times S)\to\mathbb{R}$. The method attempts to achieve generalization by running
    $\mathcal{A}$ on MDP $\mathcal{M}$, but modifying the transition probabilities
    at time $t$ to be $\tilde{P}(s_{t},a_{t},s^{\prime}_{t})=F_{t-1}(H_{t-1},s_{t},a_{t},s^{\prime}_{t})$,
    where $H_{t-1}$ is the history of algorithm $\mathcal{A}$ when running with the
    perturbed transition probabilities.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 设$\mathcal{A}$是一个训练算法，输入一个MDP并输出一个策略。给定一个MDP $\mathcal{M}=(S,A,P,r,\rho_{0},\gamma)$，一个*转移概率变换*泛化方法$\mathcal{G}_{\mathcal{P}}$由一系列函数$F_{t}:(S\times
    A\times S\times\mathbb{R})^{t}\times(S\times A\times S)\to\mathbb{R}$给出。该方法试图通过在MDP
    $\mathcal{M}$上运行$\mathcal{A}$来实现泛化，但通过将时间$t$的转移概率修改为$\tilde{P}(s_{t},a_{t},s^{\prime}_{t})=F_{t-1}(H_{t-1},s_{t},a_{t},s^{\prime}_{t})$，其中$H_{t-1}$是算法$\mathcal{A}$在使用扰动转移概率时的历史记录。
- en: The last type of generalization method we define is based on directly modifying
    the way in which the training algorithm chooses the state and action pair for
    the next time step. While this definition is broad enough to capture very complex
    changes to the training algorithm, in practice the choice of modification generally
    has a simple description.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的最后一种泛化方法基于直接修改训练算法选择下一时间步状态和动作对的方式。虽然这个定义足够宽泛，可以捕捉到训练算法的非常复杂的变化，但在实践中，修改的选择通常有一个简单的描述。
- en: Definition 3.5  (*Policy transforming generalization*).
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.5  (*策略变换泛化*)。
- en: Let $\mathcal{A}$ be a training algorithm that takes as input an MDP and outputs
    a policy. Given an MDP $\mathcal{M}=(S,A,P,r,\rho_{0},\gamma)$, a *policy transforming*
    generalization method $\mathcal{G}_{\pi}$ is given by a sequence of functions
    $F_{t}:(S\times A\times S\times\mathbb{R})^{t}\to S\times A$. The method attempts
    to achieve generalization by running $\mathcal{A}$ on MDP $\mathcal{M}$, but modifying
    the policy by which $\mathcal{A}$ chooses the next state and action to be $(\tilde{s_{t}},\tilde{a_{t}})=F_{t-1}(H_{t-1})$,
    where $H_{t-1}$ is the history of algorithm $\mathcal{A}$ when running with the
    perturbed policy.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 设$\mathcal{A}$是一个训练算法，输入一个MDP并输出一个策略。给定一个MDP $\mathcal{M}=(S,A,P,r,\rho_{0},\gamma)$，一个*策略变换*泛化方法$\mathcal{G}_{\pi}$由一系列函数$F_{t}:(S\times
    A\times S\times\mathbb{R})^{t}\to S\times A$给出。该方法试图通过在MDP $\mathcal{M}$上运行$\mathcal{A}$来实现泛化，但通过将$\mathcal{A}$选择下一个状态和动作的策略修改为$(\tilde{s_{t}},\tilde{a_{t}})=F_{t-1}(H_{t-1})$，其中$H_{t-1}$是算法$\mathcal{A}$在使用扰动策略时的历史记录。
- en: All the definitions so far categorize methods to modify training algorithms
    in order to achieve generalization. However, many such methods for modifying training
    algorithms have a corresponding method which can be used to test the generalization
    capabilities of a trained policy. Our final definition captures this correspondence.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有定义都将方法分类为修改训练算法以实现泛化。然而，许多修改训练算法的方法都有一个对应的方法，可以用来测试训练策略的泛化能力。我们的最终定义捕捉了这种对应关系。
- en: Definition 3.6  (*Generalization testing*).
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.6  (*泛化测试*)。
- en: Let $\hat{\pi}$ be a trained policy for an MDP $\mathcal{M}$. Let $F_{t}$ be
    a sequence of functions corresponding to a generalization method from one of the
    previous definitions. The *generalization testing* method of $F_{t}$ is given
    by executing the policy $\hat{\pi}$ in $\mathcal{M}$, but in each time step applying
    the modification $F_{t}$ where the history $H_{t}$ is given by the transitions
    executed by $\hat{\pi}$ so far. When both a generalization method and a generalization
    testing method are used concurrently, we will use subscripts to denote the generalization
    method and superscripts to denote the testing method. For instance, $\mathcal{G}_{S}^{\pi}$
    corresponds to training with a state transforming method, and testing with a policy
    transforming method.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 设$\hat{\pi}$为MDP $\mathcal{M}$的训练策略。设$F_{t}$为对应于之前定义之一的泛化方法的函数序列。$F_{t}$的*泛化测试*方法通过在$\mathcal{M}$中执行策略$\hat{\pi}$来给出，但在每一步中应用修改$F_{t}$，其中历史$H_{t}$由迄今为止由$\hat{\pi}$执行的过渡给出。当同时使用泛化方法和泛化测试方法时，我们将使用下标表示泛化方法，并使用上标表示测试方法。例如，$\mathcal{G}_{S}^{\pi}$对应于使用状态转换方法进行训练，使用策略转换方法进行测试。
- en: 4 Roots of Overestimation in Deep Reinforcement Learning
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度强化学习中的4种高估根源
- en: Many reinforcement learning algorithms compute estimates for the state-action
    values in an MDP. Because these estimates are usually based on a stochastic interaction
    with the MDP, computing accurate estimates that correctly generalize to further
    interactions is one of the most fundamental tasks in reinforcement learning. A
    major challenge in this area has been the tendency of many classes of reinforcement
    learning algorithms to consistently overestimate state-action values. Initially
    the overestimation bias for $Q$-learning is discussed and theoretically justified
    by Thrun & Schwartz ([1993](#bib.bib65)) as a biproduct of using function approximators
    for state-action value estimates. Following this initial discussion it has been
    shown that several parts of the deep reinforcement learning process can cause
    overestimation bias. Learning overestimated state-action values can be caused
    by statistical bias of utilizing a single max operator (van Hasselt, [2010](#bib.bib67)),
    coupling between value function and the optimal policy (Raileanu & Fergus, [2021](#bib.bib59);
    Cobbe et al., [2021](#bib.bib13)), or caused by the accumulated function approximation
    error (Boyan & Moore, [1994](#bib.bib9)).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 许多强化学习算法计算MDP中状态-行动值的估计。因为这些估计通常基于与MDP的随机交互，所以计算出准确的估计，并且这些估计能正确地推广到进一步的交互，是强化学习中最基本的任务之一。在这一领域的主要挑战是许多类别的强化学习算法倾向于一致性地高估状态-行动值。最初，$Q$-学习的高估偏差被Thrun和Schwartz
    ([1993](#bib.bib65)) 讨论并理论上证明，作为使用函数逼近器进行状态-行动值估计的副产品。随后已证明，深度强化学习过程中的几个部分可能会导致高估偏差。学习高估的状态-行动值可能是由于使用单个最大操作符的统计偏差（van
    Hasselt, [2010](#bib.bib67)）、值函数与最优策略之间的耦合（Raileanu & Fergus, [2021](#bib.bib59)；Cobbe等人,
    [2021](#bib.bib13)）或由累积的函数逼近误差引起（Boyan & Moore, [1994](#bib.bib9)）。
- en: Several methods have been proposed to target overestimation bias for value iteration
    algorithms. In particular, to solve this overestimation bias introduced by the
    max operator (van Hasselt, [2010](#bib.bib67)) proposed to utilize a double estimator
    for the state-action value estimates. Later, the authors also created a version
    of this algorithm that can solve high dimensional state space problems (Hasselt
    et al., [2016](#bib.bib23)). Some of the work on this line of research targeting
    overestimation bias for value iteration algorithms is based on simply averaging
    the state-action values with previously learned state-action value estimates during
    training time (Anschel et al., [2017](#bib.bib4)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出几种方法来针对价值迭代算法的高估偏差。特别是，为了解决由最大操作符引入的这种高估偏差（van Hasselt, [2010](#bib.bib67)）建议使用双重估计器来进行状态-行动值估计。后来，作者还创建了一个可以解决高维状态空间问题的算法版本（Hasselt等人,
    [2016](#bib.bib23)）。一些针对价值迭代算法的研究工作基于在训练期间简单地平均状态-行动值与之前学习的状态-行动值估计（Anschel等人,
    [2017](#bib.bib4)）。
- en: While overestimation bias was demonstrated to be a problem and discussed over
    a long period of time (Thrun & Schwartz, [1993](#bib.bib65); van Hasselt, [2010](#bib.bib67)),
    recent studies also further demonstrated that actor critic algorithms also suffer
    from this issue (Fujimoto et al., [2018](#bib.bib16)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管高估偏差被证明是一个问题，并且经过长时间讨论（Thrun & Schwartz, [1993](#bib.bib65); van Hasselt,
    [2010](#bib.bib67)），最近的研究还进一步表明，演员-评论家算法也面临这个问题（Fujimoto et al., [2018](#bib.bib16)）。
- en: 'Table 1: Environment and algorithm details for different exploration strategies
    for generalization.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同探索策略在泛化中的环境和算法细节。
- en: '| Citation | Proposed Method | Environment | Reinforcement Learning Algorithm
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 引用 | 提议的方法 | 环境 | 强化学习算法 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Mnih et al. ([2015](#bib.bib53)) | $\epsilon$-greedy | ALE | DQN |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Mnih et al. ([2015](#bib.bib53)) | $\epsilon$-贪婪 | ALE | DQN |'
- en: '| Bellemare et al. ([2016](#bib.bib8)) | Count-based | ALE | A3C and DQN |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Bellemare et al. ([2016](#bib.bib8)) | 基于计数 | ALE | A3C 和 DQN |'
- en: '| Osband et al. ([2016b](#bib.bib57)) | RLSVI | Tetris | Tabular $Q$-learning
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Osband et al. ([2016b](#bib.bib57)) | RLSVI | Tetris | 表格 $Q$-学习 |'
- en: '| Osband et al. ([2016a](#bib.bib56)) | Bootstrapped DQN | ALE | DQN |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Osband et al. ([2016a](#bib.bib56)) | 启动 DQN | ALE | DQN |'
- en: '| Houthooft et al. ([2017](#bib.bib24)) | VIME | DeepMind Control Suite | TRPO
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Houthooft et al. ([2017](#bib.bib24)) | VIME | DeepMind Control Suite | TRPO
    |'
- en: '| Fortunato et al. ([2018](#bib.bib15)) | NoisyNet | ALE | A3C and DQN |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Fortunato et al. ([2018](#bib.bib15)) | NoisyNet | ALE | A3C 和 DQN |'
- en: '| Lee et al. ([2021](#bib.bib48)) | SUNRISE | DCS¹¹1DeepMind Control Suiteand
    Atari | Soft Actor-Critic and Rainbow DQN |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Lee et al. ([2021](#bib.bib48)) | SUNRISE | DCS¹¹1DeepMind Control Suite和Atari
    | Soft Actor-Critic 和 Rainbow DQN |'
- en: 5 The Role of Exploration in Overfitting
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 探索在过拟合中的作用
- en: The fundamental trade-off of exploration vs exploitation is the dilemma that
    the agent can try to take actions to move towards more unexplored states by sacrificing
    the current immediate rewards. While there is a significant body of studies on
    provably efficient exploration strategies the results from these studies do not
    necessarily directly transfer to the high dimensional state or action MDPs. The
    most prominent indication of this is that, even though it is possible to use deep
    neural networks as function approximators for large state spaces, the agent will
    simply not be able to explore the full state space. The fact that the agent is
    able to only explore a portion of the state space simply creates a bias in the
    learnt value function (Baird, [1995](#bib.bib5)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与利用的根本权衡是代理可以通过牺牲当前的即时奖励来尝试采取行动以接近更多未探索的状态。虽然有大量关于可证明有效的探索策略的研究，但这些研究的结果并不一定直接转移到高维状态或动作
    MDPs。最显著的迹象是，即使可以使用深度神经网络作为大状态空间的函数逼近器，代理仍然无法完全探索整个状态空间。代理只能探索状态空间的一部分，这就会在学习的价值函数中产生偏差（Baird,
    [1995](#bib.bib5)）。
- en: In this section, we will go through several exploration strategies in deep reinforcement
    learning and how they affect policy overfitting. A quite simple version of this
    is based on adding noise in action selection during training e.g. $\epsilon$-greedy
    exploration. Note that this is an example of a policy transforming generalization
    method $\mathcal{G}_{\pi}$ in Definition [3.5](#S3.Thmtheorem5 "Definition 3.5
    (Policy transforming generalization). ‣ 3 How to Achieve Generalization? ‣ A Survey
    Analyzing Generalization in Deep Reinforcement Learning") in Section [3](#S3 "3
    How to Achieve Generalization? ‣ A Survey Analyzing Generalization in Deep Reinforcement
    Learning"). While $\epsilon$-greedy exploration is widely used in deep reinforcement
    learning (Wang et al., [2016](#bib.bib73); Hamrick et al., [2020](#bib.bib22);
    Kapturowski et al., [2023](#bib.bib31)), it has also been proven that to explore
    the state space these algorithms may take exponentially long (Kakade, [2003](#bib.bib30)).
    Several others focused on randomizing different components of the reinforcement
    learning training algorithms. In particular, (Osband et al., [2016b](#bib.bib57))
    proposes the randomized least squared value iteration algorithm to explore more
    efficiently in order to increase generalization in reinforcement learning for
    linearly parametrized value functions. This is achieved by simply adding Gaussian
    noise as a function of state visitation frequencies to the training dataset. Later,
    the authors also propose the bootstrapped DQN algorithm (i.e. adding temporally
    correlated noise) to increase generalization with non-linear function approximation
    (Osband et al., [2016a](#bib.bib56)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨深度强化学习中的几种探索策略及其对策略过拟合的影响。其中一个相对简单的版本是通过在训练期间的动作选择中添加噪声，例如 $\epsilon$-贪婪探索。请注意，这是定义[3.5](#S3.Thmtheorem5
    "Definition 3.5 (Policy transforming generalization). ‣ 3 How to Achieve Generalization?
    ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning")中 $\mathcal{G}_{\pi}$
    策略变换泛化方法的一个例子，见第[3](#S3 "3 How to Achieve Generalization? ‣ A Survey Analyzing
    Generalization in Deep Reinforcement Learning")节。虽然 $\epsilon$-贪婪探索在深度强化学习中被广泛使用（Wang
    et al., [2016](#bib.bib73); Hamrick et al., [2020](#bib.bib22); Kapturowski et
    al., [2023](#bib.bib31)），但也已证明，这些算法在探索状态空间时可能需要指数级的时间（Kakade, [2003](#bib.bib30)）。还有一些研究关注于随机化强化学习训练算法的不同组件。特别地，（Osband
    et al., [2016b](#bib.bib57)）提出了随机最小二乘值迭代算法，以便更有效地探索，从而提高线性参数化值函数的泛化。这是通过简单地将高斯噪声作为状态访问频率的函数添加到训练数据集中来实现的。后来，作者们还提出了自助式DQN算法（即添加时间相关噪声）以提高非线性函数逼近的泛化（Osband
    et al., [2016a](#bib.bib56)）。
- en: Houthooft et al. ([2017](#bib.bib24)) proposed an exploration technique centered
    around maximizing the information gain on the agent’s belief of the environment
    dynamics. In practice, the authors use Bayesian neural networks for effectively
    exploring high dimensional action space MDPs. Following this line of work on increasing
    efficiency during exploration Fortunato et al. ([2018](#bib.bib15)) proposes to
    add parametric noise to the deep reinforcement learning policy weights in high
    dimensional state MDPs. While several methods focused on ensemble state-action
    value function learning (Osband et al., [2016a](#bib.bib56)), Lee et al. ([2021](#bib.bib48))
    proposed reweighting target Q-values from an ensemble of policies (i.e. weighted
    Bellman backups) combined with highest upper-confidence bound action selection.
    Another line of research in exploration strategies focused on count-based methods
    that use the direct count of state visitations. In this line of work, Bellemare
    et al. ([2016](#bib.bib8)) tried to lay out the relationship between count based
    methods and intrinsic motivation, and used count-based methods for high dimensional
    state MDPs (i.e. Arcade Learning Environment). Yet it is worthwhile to note that
    most of the current deep reinforcement learning algorithms use very simple exploration
    techniques such as $\epsilon$-greedy which is based on taking the action maximizing
    the state-action value function with probability $1-\epsilon$ and taking a random
    action with probability $\epsilon$ (Mnih et al., [2015](#bib.bib53); Hasselt et al.,
    [2016](#bib.bib23); Wang et al., [2016](#bib.bib73); Hamrick et al., [2020](#bib.bib22);
    Kapturowski et al., [2023](#bib.bib31)).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Houthooft 等人 ([2017](#bib.bib24)) 提出了一个探索技术，集中于最大化代理对环境动态的信念的信息增益。实际上，作者使用贝叶斯神经网络有效地探索高维动作空间的
    MDP。继这一提高探索效率的工作之后，Fortunato 等人 ([2018](#bib.bib15)) 提出了在高维状态 MDP 中将参数噪声添加到深度强化学习策略权重中。虽然有几种方法专注于集成状态-动作价值函数学习（Osband
    等人，[2016a](#bib.bib56)），Lee 等人 ([2021](#bib.bib48)) 提出了从一组策略的目标 Q 值中进行重新加权（即加权贝尔曼备份），并结合最高上置信界行动选择。探索策略的另一条研究方向集中于基于计数的方法，这些方法使用状态访问的直接计数。在这一研究方向中，Bellemare
    等人 ([2016](#bib.bib8)) 尝试阐明基于计数的方法与内在动机之间的关系，并将基于计数的方法用于高维状态 MDP（即 Arcade 学习环境）。然而，值得注意的是，目前大多数深度强化学习算法使用非常简单的探索技术，例如
    $\epsilon$-贪婪策略，该策略基于以概率 $1-\epsilon$ 采取最大化状态-动作价值函数的行动，并以概率 $\epsilon$ 采取随机行动（Mnih
    等人，[2015](#bib.bib53)；Hasselt 等人，[2016](#bib.bib23)；Wang 等人，[2016](#bib.bib73)；Hamrick
    等人，[2020](#bib.bib22)；Kapturowski 等人，[2023](#bib.bib31)）。
- en: It is possible to argue that the fact that the deep reinforcement learning policy
    obtained a higher score with the same number of samples by a particular type of
    training method $\mathcal{A}$ compared to method $\mathcal{B}$ is by itself evidence
    that the technique $\mathcal{A}$ leads to more generalized policies. Even though
    the agent is trained and tested in the same environment, the explored states during
    training time are not exactly the same states visited during test time. The fact
    that the policy trained with technique $\mathcal{A}$ obtains a higher score at
    the end of an episode is sole evidence that the agent trained with $\mathcal{A}$
    was able to visit further states in the MDP and thus succeed in them. Yet, throughout
    the paper we will discuss different notions of generalization investigated in
    different subfields of reinforcement learning research. While exploration vs exploitation
    stands out as one of the main problems in reinforcement learning policy performance
    most of the work conducted in this section focuses on achieving higher score in
    hard-exploration games (i.e. Montezuma’s Revenge) rather than aiming for a generally
    higher score for each game overall across a given benchmark. Thus, it is possible
    that the majority of work focusing on exploration so far might not be able to
    obtain policies that perform as well as those in the studies described in Section
    [6](#S6 "6 Regularization ‣ A Survey Analyzing Generalization in Deep Reinforcement
    Learning") across a given benchmark.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可以争辩说，通过特定类型的训练方法 $\mathcal{A}$ 相比于方法 $\mathcal{B}$，深度强化学习策略在相同样本数量下获得了更高分数，这本身就是技术
    $\mathcal{A}$ 导致更泛化策略的证据。尽管代理在相同环境中进行训练和测试，但训练期间探索的状态并不完全是测试期间访问的状态。通过技术 $\mathcal{A}$
    训练出的策略在一个回合结束时获得更高分数，唯一的证据是代理使用 $\mathcal{A}$ 训练能够在 MDP 中访问更远的状态并取得成功。然而，整篇论文将讨论在强化学习研究的不同子领域中探讨的不同泛化概念。虽然探索与利用是强化学习策略性能中的主要问题之一，但本节的大部分工作集中于在难以探索的游戏（即蒙特祖玛的复仇）中获得更高分，而不是在给定基准下对每个游戏总体目标设定更高分。因此，到目前为止，专注于探索的大多数工作可能无法在给定基准下获得与第
    [6](#S6 "6 正则化 ‣ 一项分析深度强化学习泛化的调查") 节中描述的研究一样表现良好的策略。
- en: 6 Regularization
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 正则化
- en: In this section we will focus on different regularization techniques employed
    to increase generalization in deep reinforcement learning policies. We will go
    through these works by categorizing each of them under data augmentation, adversarial
    training, and direct function regularization. Under each category we will connect
    these different line of approaches to increase generalization in deep reinforcement
    learning to the settings we defined in Section [3](#S3 "3 How to Achieve Generalization?
    ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning").
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点讨论用于提高深度强化学习策略泛化的不同正则化技术。我们将通过将每个技术分类为数据增强、对抗训练和直接函数正则化来进行讨论。在每个类别下，我们将这些不同的泛化方法与我们在第
    [3](#S3 "3 如何实现泛化？ ‣ 一项分析深度强化学习泛化的调查") 节中定义的设置相联系。
- en: 'Table 2: Environment and algorithm details for data augmentation techniques
    for state observation generalization. All of the studies in this section focus
    on state transformation methods $\mathcal{G}_{S}$ defined in Section [3](#S3 "3
    How to Achieve Generalization? ‣ A Survey Analyzing Generalization in Deep Reinforcement
    Learning").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：用于状态观察泛化的数据增强技术的环境和算法细节。本节中的所有研究都集中在第 [3](#S3 "3 如何实现泛化？ ‣ 一项分析深度强化学习泛化的调查")
    节中定义的状态变换方法 $\mathcal{G}_{S}$ 上。
- en: '| Citation | Proposed Method | Environment | Reinforcement Learning Algorithm
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 引用 | 提议方法 | 环境 | 强化学习算法 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Yarats et al. ([2021](#bib.bib77)) | DrQ | DeepMind Control Suite, ALE |
    DQN |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Yarats 等 ([2021](#bib.bib77)) | DrQ | DeepMind Control Suite, ALE | DQN |'
- en: '| Laskin et al. ([2020b](#bib.bib45)) | CuRL | DeepMind Control Suite, ALE
    | Soft Actor Critic and DQN |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Laskin 等 ([2020b](#bib.bib45)) | CuRL | DeepMind Control Suite, ALE | Soft
    Actor Critic 和 DQN |'
- en: '| Laskin et al. ([2020a](#bib.bib44)) | RAD | DeepMind Control Suite, ProcGen
    | Soft Actor Critic and PPO |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Laskin 等 ([2020a](#bib.bib44)) | RAD | DeepMind Control Suite, ProcGen |
    Soft Actor Critic 和 PPO |'
- en: '| Wang et al. ([2020](#bib.bib72)) | Mixreg | ProcGen | DQN and PPO |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等 ([2020](#bib.bib72)) | Mixreg | ProcGen | DQN 和 PPO |'
- en: 6.1 Data Augmentation
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 数据增强
- en: Several studies focus on diversifying the observations of the deep reinforcement
    learning policy to increase generalization capabilities. A line of research in
    this regard focused on simply employing versions of data augmentation techniques
    (Laskin et al., [2020a](#bib.bib44); [b](#bib.bib45); Yarats et al., [2021](#bib.bib77))
    for high dimensional state representation environments. In particular, these studies
    involve simple techniques such as cropping, rotating or shifting the state observations
    during training time. While this line of work got considerable attention, a quite
    recent study (Agarwal et al., [2021b](#bib.bib2)) demonstrated that when the number
    of random seeds is increased to one hundred the relative performance achieved
    and reported in the original papers of (Laskin et al., [2020b](#bib.bib45); Yarats
    et al., [2021](#bib.bib77)) on data augmentation training in deep reinforcement
    learning decreases to a level that might be significant to mention.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 几项研究专注于通过多样化深度强化学习策略的观察来提高泛化能力。在这方面的研究线集中在简单使用数据增强技术的不同版本（Laskin et al., [2020a](#bib.bib44);
    [b](#bib.bib45); Yarats et al., [2021](#bib.bib77)）用于高维状态表示环境。特别是，这些研究涉及在训练期间简单技术，如裁剪、旋转或移动状态观察。尽管这一工作得到了相当大的关注，但一项较新的研究（Agarwal
    et al., [2021b](#bib.bib2)）表明，当随机种子的数量增加到一百时，相对性能在（Laskin et al., [2020b](#bib.bib45);
    Yarats et al., [2021](#bib.bib77)）的数据增强训练中减少到一个可能需要提及的重要水平。
- en: While some of the work on this line of research simply focuses on using a set
    of data augmentation methods (Laskin et al., [2020a](#bib.bib44); [b](#bib.bib45);
    Yarats et al., [2021](#bib.bib77)), other work focuses on proposing new environments
    to train in (Cobbe et al., [2020](#bib.bib12)). The studies on designing new environments
    to train deep reinforcement learning policies basically aim to provide high variation
    in the observed environment such as changing background colors and changing object
    shapes in ways that are meaningful in the game, in order to increase test time
    generalization. In the line of robustness and test time performance, a more recent
    work that is also mentioned in Section [6.3](#S6.SS3 "6.3 The Adversarial Perspective
    for Deep Neural Policy Generalization ‣ 6 Regularization ‣ A Survey Analyzing
    Generalization in Deep Reinforcement Learning") demonstrated that imperceptible
    data augmentations can cause significant damage on the policy performance and
    certified robust deep reinforcement learning policies are more vulnerable to these
    imperceptible augmentations (Korkmaz, [2023](#bib.bib40)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这一研究线中的一些工作简单地专注于使用一组数据增强方法（Laskin et al., [2020a](#bib.bib44); [b](#bib.bib45);
    Yarats et al., [2021](#bib.bib77)），其他工作则专注于提出新的训练环境（Cobbe et al., [2020](#bib.bib12)）。关于设计新环境以训练深度强化学习策略的研究基本上旨在提供高变化的观察环境，例如在游戏中以有意义的方式更改背景颜色和物体形状，以提高测试时间泛化能力。在鲁棒性和测试时间表现方面，最近的一项工作也在第[6.3](#S6.SS3
    "6.3 The Adversarial Perspective for Deep Neural Policy Generalization ‣ 6 Regularization
    ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning")节中提到，表明不可察觉的数据增强可能对策略性能造成重大损害，而经过认证的鲁棒深度强化学习策略对这些不可察觉的增强更为脆弱（Korkmaz,
    [2023](#bib.bib40)）。
- en: Within this category some work focuses on producing more observations by simply
    blending in (e.g. creating a mixture state from multiple different observations)
    several observations to increase generalization (Wang et al., [2020](#bib.bib72)).
    While most of the studies trying to increase generalization by data augmentation
    techniques are primarily conducted in the DeepMind Control Suite or the Arcade
    Learning Environment (ALE) (Bellemare et al., [2013](#bib.bib7)), some small fraction
    of these studies (Wang et al., [2020](#bib.bib72)) are conducted in relatively
    recently designed training environments like ProcGen (Cobbe et al., [2020](#bib.bib12)).
    Cobbe et al. ([2019](#bib.bib11)) focuses on decoupling the training and testing
    set for reinforcement learning via simply proposing a new game environment CoinRun.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一类别中，有些工作专注于通过简单地混合多个观察（例如，从多个不同观察中创建混合状态）来产生更多观察以增加泛化能力（Wang et al., [2020](#bib.bib72)）。虽然大多数试图通过数据增强技术提高泛化能力的研究主要在DeepMind
    Control Suite或Arcade Learning Environment (ALE)（Bellemare et al., [2013](#bib.bib7)）中进行，但这些研究中的一小部分（Wang
    et al., [2020](#bib.bib72)）是在相对较新设计的训练环境如ProcGen（Cobbe et al., [2020](#bib.bib12)）中进行的。Cobbe
    et al. ([2019](#bib.bib11))专注于通过提出新的游戏环境CoinRun来解耦强化学习的训练和测试集。
- en: 'Table 3: Environment and algorithm details for different direct function regularization
    strategies for trying to overcome overfitting problems in reinforcement learning.
    Note that most of the methods based on direct function regularization are a form
    of policy perturbation method $\mathcal{G}_{\pi}$ to overcome overfitting as described
    in Section [3](#S3 "3 How to Achieve Generalization? ‣ A Survey Analyzing Generalization
    in Deep Reinforcement Learning").'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同直接函数正则化策略的环境和算法详细信息，旨在克服强化学习中的过拟合问题。请注意，大多数基于直接函数正则化的方法都是一种策略扰动方法 $\mathcal{G}_{\pi}$，以克服过拟合，如第[3](#S3
    "3 如何实现泛化？ ‣ 深度强化学习泛化分析调查")节中所述。
- en: '| Citation | Proposed Method | Environment | Reinforcement Learning Algorithm
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 引用 | 提议的方法 | 环境 | 强化学习算法 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Igl et al. ([2019](#bib.bib27)) | SNI and IBAC | GridWorld and CoinRun |
    Proximal Policy Optimization |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Igl 等 ([2019](#bib.bib27)) | SNI 和 IBAC | GridWorld 和 CoinRun | Proximal
    Policy Optimization |'
- en: '| Vieillard et al. ([2020b](#bib.bib70)) | Munchausen RL | Atari | DQN and
    IQN |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Vieillard 等 ([2020b](#bib.bib70)) | Munchausen RL | Atari | DQN 和 IQN |'
- en: '| Lee et al. ([2020](#bib.bib47)) | Network Randomization | 2D CoinRun and
    3D DeepMind Lab | Proximal Policy Optimization |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Lee 等 ([2020](#bib.bib47)) | 网络随机化 | 2D CoinRun 和 3D DeepMind Lab | Proximal
    Policy Optimization |'
- en: '| Amit et al. ([2020](#bib.bib3)) | Discount Regularization | GridWorld and
    Mujoco²²2Low dimensional setting of Mujoco is used for this study. | Twin Delayed
    DDPG (TD3) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Amit 等 ([2020](#bib.bib3)) | 折扣正则化 | GridWorld 和 Mujoco²²2Mujoco 的低维设置用于这项研究。
    | Twin Delayed DDPG (TD3) |'
- en: '| Agarwal et al. ([2021a](#bib.bib1)) | PSM | DDMC and Rectangle Game³³3Rectangle
    game is a simple video game with only two actions, ”Right” and ”Jump”. The game
    has black background and two rectangles where the goal of the game is to avoid
    white obstacles and reach to the right side of the screen. Agarwal et al. ([2021a](#bib.bib1))
    is the only paper we encountered experimenting with this particular game. | DrQ
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Agarwal 等 ([2021a](#bib.bib1)) | PSM | DDMC 和 Rectangle Game³³3Rectangle
    游戏是一个简单的视频游戏，只有两个动作，“右”和“跳”。游戏有黑色背景和两个矩形，游戏目标是避免白色障碍物并到达屏幕右侧。Agarwal 等 ([2021a](#bib.bib1))
    是我们遇到的唯一一个实验这个特定游戏的论文。 | DrQ |'
- en: '| Liu et al. ([2021](#bib.bib50)) | BN and dropout and $L_{2}/L_{1}$ | Mujoco
    | PPO, TRPO, SAC, A2C |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等 ([2021](#bib.bib50)) | BN 和 dropout 以及 $L_{2}/L_{1}$ | Mujoco | PPO,
    TRPO, SAC, A2C |'
- en: 6.2 Direct Function Regularization
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 直接函数正则化
- en: While some of the work we have discussed so far focuses on regularizing the
    data (i.e. state observations) as in Section [6.1](#S6.SS1 "6.1 Data Augmentation
    ‣ 6 Regularization ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning"),
    some focuses on directly regularizing the function learned with the intention
    of simulating techniques from deep neural network regularization like batch normalization
    and dropout (Igl et al., [2019](#bib.bib27)). While some studies have attempted
    to simulate these known techniques in reinforcement learning, some focus on directly
    applying them to overcome overfitting. In this line of research, Liu et al. ([2021](#bib.bib50))
    proposes to use known techniques from deep neural network regularization to apply
    in continous control deep reinforcement learning training. In particular, these
    techniques are batch normalization (BN) (Ioffe & Szegedy, [2015](#bib.bib28)),
    weight clipping, dropout, entropy and $L_{2}/L_{1}$ weight regularization.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们迄今讨论的一些工作侧重于对数据（即状态观察）进行正则化，如第[6.1](#S6.SS1 "6.1 数据增强 ‣ 6 正则化 ‣ 深度强化学习泛化分析调查")节所述，但有些则侧重于直接对所学函数进行正则化，旨在模拟来自深度神经网络正则化的技术，如批量归一化和
    dropout（Igl 等，[2019](#bib.bib27)）。尽管一些研究尝试在强化学习中模拟这些已知技术，但有些则专注于直接应用这些技术以克服过拟合。在这方面的研究中，Liu
    等 ([2021](#bib.bib50)) 提出使用深度神经网络正则化中的已知技术应用于连续控制深度强化学习训练。特别是，这些技术包括批量归一化（BN）（Ioffe
    & Szegedy，[2015](#bib.bib28)）、权重裁剪、dropout、熵和 $L_{2}/L_{1}$ 权重正则化。
- en: Lee et al. ([2020](#bib.bib47)) proposes to utilize a random network to randomize
    the input observations to increase generalization skills of deep reinforcement
    learning policies, and tests the proposal in the 2D CoinRun game proposed by Cobbe
    et al. ([2019](#bib.bib11)) and 3D DeepMind Lab. In particular, the authors essentially
    introduce a random convolutional layer to perturb the state observations. Hence,
    this study is also a clear example of a state transformation generalization method
    $\mathcal{G}_{S}$ described in Definition [3.3](#S3.Thmtheorem3 "Definition 3.3
    (State transforming generalization). ‣ 3 How to Achieve Generalization? ‣ A Survey
    Analyzing Generalization in Deep Reinforcement Learning"). While this is another
    example of random state perturbation methods we will further explain in Section
    [6.3](#S6.SS3 "6.3 The Adversarial Perspective for Deep Neural Policy Generalization
    ‣ 6 Regularization ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning")
    the worst-case perturbation methods to target generalization in reinforcement
    learning policies.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Lee 等人 ([2020](#bib.bib47)) 提出了利用随机网络来随机化输入观察，以提高深度强化学习策略的泛化能力，并在 Cobbe 等人 ([2019](#bib.bib11))
    提出的 2D CoinRun 游戏和 3D DeepMind Lab 中测试了该提议。特别地，作者实际上引入了一个随机卷积层来扰动状态观察。因此，这项研究也是定义
    [3.3](#S3.Thmtheorem3 "Definition 3.3 (State transforming generalization). ‣ 3
    How to Achieve Generalization? ‣ A Survey Analyzing Generalization in Deep Reinforcement
    Learning") 中描述的状态变换泛化方法 $\mathcal{G}_{S}$ 的一个明确示例。虽然这又是一个随机状态扰动方法的例子，我们将在第 [6.3](#S6.SS3
    "6.3 The Adversarial Perspective for Deep Neural Policy Generalization ‣ 6 Regularization
    ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning") 节进一步解释最坏情况下的扰动方法，以针对强化学习策略中的泛化。
- en: Some work employs contrastive representation learning to learn deep reinforcement
    learning policies from state observations that are close to each other (Agarwal
    et al., [2021a](#bib.bib1)). The authors of this study leverage the temporal aspect
    of reinforcement learning and propose a policy similarity metric. The main goal
    of the paper is to lay out the sequential structure and utilize representation
    learning to learn generalizable abstractions from state representations. One drawback
    of this study is that most of the experimental study is conducted in a non-baseline
    environment (Rectangle game). Even though the authors show surprising results
    for this particular game, it is not directly indicated that the proposed method
    would work for high dimensional state representation MDPs such as the Arcade Learning
    Environment. Malik et al. ([2021](#bib.bib51)) studies query complexity of reinforcement
    learning policies that can generalize to multiple environments. The authors of
    this study focus on an example of the transition probability transformation setting
    $\mathcal{G}_{\mathcal{P}}$ in Definition [3.4](#S3.Thmtheorem4 "Definition 3.4
    (Transition probability transforming generalization). ‣ 3 How to Achieve Generalization?
    ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning"), and the
    reward function transformation setting $\mathcal{G}_{R}$ in Definition [3.2](#S3.Thmtheorem2
    "Definition 3.2 (Rewards transforming generalization). ‣ 3 How to Achieve Generalization?
    ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning").
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究利用对比表示学习从相互接近的状态观察中学习深度强化学习策略（Agarwal 等人，[2021a](#bib.bib1)）。该研究的作者利用了强化学习的时间特性，并提出了一个策略相似性度量。本文的主要目标是阐明序列结构，并利用表示学习从状态表示中学习可泛化的抽象。该研究的一个缺点是，大部分实验研究是在非基准环境（矩形游戏）中进行的。尽管作者在这个特定游戏中展示了令人惊讶的结果，但并未直接说明所提出的方法是否适用于高维状态表示的
    MDP，例如 Arcade Learning Environment。Malik 等人 ([2021](#bib.bib51)) 研究了能够泛化到多个环境的强化学习策略的查询复杂性。该研究的作者关注于定义
    [3.4](#S3.Thmtheorem4 "Definition 3.4 (Transition probability transforming generalization).
    ‣ 3 How to Achieve Generalization? ‣ A Survey Analyzing Generalization in Deep
    Reinforcement Learning") 中的过渡概率变换设置 $\mathcal{G}_{\mathcal{P}}$，以及定义 [3.2](#S3.Thmtheorem2
    "Definition 3.2 (Rewards transforming generalization). ‣ 3 How to Achieve Generalization?
    ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning") 中的奖励函数变换设置
    $\mathcal{G}_{R}$。
- en: Another line of study in direct function generalization investigates the relationship
    between reduced discount factor and adding an $\ell_{2}$-regularization term to
    the loss function (i.e. weight decay) (Amit et al., [2020](#bib.bib3)). The authors
    in this work demonstrate the explicit connection between reducing the discount
    factor and adding an $\ell_{2}$-regularizer to the value function for temporal
    difference learning. In particular, this study demonstrates that adding an $\ell_{2}$-regularization
    term to the loss function is equal to training with a lower discount term, which
    the authors refer to as discount regularization. The results of this study however
    are based on experiments from tabular reinforcement learning, and the low dimensional
    setting of the Mujoco environment.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 直接函数泛化的另一个研究方向探讨了减少折扣因子与在损失函数中加入$\ell_{2}$-正则化项（即权重衰减）之间的关系（Amit等人，[2020](#bib.bib3)）。这项工作的作者展示了减少折扣因子与在时间差分学习的价值函数中加入$\ell_{2}$-正则化器之间的明确联系。特别是，这项研究表明，在损失函数中加入$\ell_{2}$-正则化项等同于使用较低折扣因子进行训练，作者称之为折扣正则化。然而，这项研究的结果基于表格型强化学习的实验以及Mujoco环境的低维设置。
- en: On the reward transformation for generalization setting $\mathcal{G}_{R}$ defined
    in Definition [3.2](#S3.Thmtheorem2 "Definition 3.2 (Rewards transforming generalization).
    ‣ 3 How to Achieve Generalization? ‣ A Survey Analyzing Generalization in Deep
    Reinforcement Learning"), Vieillard et al. ([2020b](#bib.bib70)) adds the scaled
    log policy to the current rewards. To overcome overfitting some work tries to
    learn explicit or implicit similarity between the states to obtain a reasonable
    policy (Lan et al., [2021](#bib.bib43)). In particular, the authors in this work
    try to unify the state space representations by providing a taxonomy of metrics
    in reinforcement learning. Several studies proposed different ways to include
    Kullback-Leibler (KL) divergence between the current policy and the pre-updated
    policy to add as a regularization term in the reinforcement learning objective
    (Schulman et al., [2015](#bib.bib61)). Recently, some studies argued that utilizing
    Kullback-Leibler regularization implicitly averages the state-action value estimates
    (Vieillard et al., [2020a](#bib.bib69)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定义中设定的奖励变换$\mathcal{G}_{R}$（见定义[3.2](#S3.Thmtheorem2 "定义 3.2（奖励变换泛化） ‣ 3 如何实现泛化？
    ‣ 深度强化学习泛化分析综述")），Vieillard等人（[2020b](#bib.bib70)）将缩放后的对数策略添加到当前奖励中。为了克服过拟合，一些工作尝试学习状态之间的显式或隐式相似性，以获得合理的策略（Lan等人，[2021](#bib.bib43)）。特别是，这项工作中的作者试图通过提供强化学习中的度量分类法来统一状态空间表示。几项研究提出了将当前策略和更新前策略之间的Kullback-Leibler（KL）散度作为正则化项添加到强化学习目标中的不同方法（Schulman等人，[2015](#bib.bib61)）。最近，一些研究认为，利用Kullback-Leibler正则化隐式地平均了状态-动作值估计（Vieillard等人，[2020a](#bib.bib69)）。
- en: 6.3 The Adversarial Perspective for Deep Neural Policy Generalization
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 深度神经策略泛化的对抗视角
- en: One of the ways to regularize the state observations is based on considering
    worst-case perturbations added to state observations (i.e. adversarial perturbations).
    This line of work starts with introducing perturbations produced by the fast gradient
    sign method proposed by Goodfellow et al. ([2015](#bib.bib21)) into deep reinforcement
    learning observations at test time Huang et al. ([2017](#bib.bib26)) Kos & Song
    ([2017](#bib.bib42)), and compares the generalization capabilities of the trained
    deep reinforcement learning policies in the presence worst-case perturbations
    and Gaussian noise. These gradient based adversarial methods are based on taking
    the gradient of the cost function used to train the policy with respect to the
    state observation. Several other techniques have been proposed on the optimization
    line of the adversarial alteration of state observations. In this line of work,
    Korkmaz ([2020](#bib.bib33)) suggested a Nesterov momentum-based method to produce
    adversarial perturbations for deep reinforcement learning policies. Korkmaz ([2022](#bib.bib39))
    further showed that deep reinforcement learning policies learn shared adversarial
    features across MDPs. In this work the authors investigate the root causes of
    this problem, and demonstrate that policy high-sensitivity directions and the
    perceptual similarity of the state observations are uncorrelated. Furthermore,
    the study demonstrates that the current state-of-the-art adversarial training
    techniques also learn similar high-sensitivity directions as the vanilla trained
    deep reinforcement learning policies.⁴⁴4From the security point of view, this
    adversarial framework is under the category of black-box adversarial attacks for
    which this is the first study that demonstrated that deep reinforcement learning
    policies are vulnerable to black-box adversarial attacks (Korkmaz, [2022](#bib.bib39)).
    Furthermore, note that black-box adversarial perturbations are more generalizable
    global perturbations that can effect many different policies. While some studies
    focused on state observation alterations to assess policy resilience with respect
    to these changes, some studies focused on interpretability and explainability
    of these changes in these state observation alterations and how these alterations
    have different effects on standard deep reinforcement learning training algorithms
    and certified robust (i.e. adversarial) training algorithms Korkmaz ([2021e](#bib.bib38))⁵⁵5See
    an initial and preliminary version of this paper here Korkmaz ([2021a](#bib.bib34);
    [d](#bib.bib37)).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化状态观察的一种方法是基于将最坏情况下的扰动添加到状态观察中（即对抗扰动）。这项工作始于将Goodfellow等人提出的快速梯度符号法（[2015](#bib.bib21)）产生的扰动引入深度强化学习观察中（Huang等人，[2017](#bib.bib26)；Kos
    & Song，[2017](#bib.bib42)），并比较在最坏情况下的扰动和高斯噪声存在下，训练的深度强化学习策略的泛化能力。这些基于梯度的对抗方法基于对训练策略的成本函数关于状态观察的梯度。还有几种其他技术被提出用于对状态观察的对抗性改变的优化方向。在这方面的工作中，Korkmaz（[2020](#bib.bib33)）提出了一种基于Nesterov动量的方法，用于生成深度强化学习策略的对抗扰动。Korkmaz（[2022](#bib.bib39)）进一步表明，深度强化学习策略在不同MDP中学习到共享的对抗特征。在这项工作中，作者探讨了这一问题的根本原因，并展示了策略高灵敏度方向与状态观察的感知相似性之间的不相关性。此外，研究表明，目前最先进的对抗训练技术也学习到与原始训练的深度强化学习策略类似的高灵敏度方向。从安全角度来看，这种对抗框架属于黑箱对抗攻击范畴，这是首个研究表明深度强化学习策略容易受到黑箱对抗攻击的研究（Korkmaz，[2022](#bib.bib39)）。此外，需要注意的是，黑箱对抗扰动是更具通用性的全局扰动，能够影响许多不同的策略。虽然一些研究专注于状态观察的改变，以评估策略对这些变化的鲁棒性，一些研究则专注于这些状态观察变化的可解释性和解释性，以及这些变化如何对标准深度强化学习训练算法和认证鲁棒（即对抗性）训练算法产生不同的影响（Korkmaz，[2021e](#bib.bib38)）。有关此论文的初步版本，请参见Korkmaz（[2021a](#bib.bib34)；[d](#bib.bib37)）。
- en: 'Table 4: Environment and algorithm details for adversarial policy regularization
    and attack techniques in deep reinforcement learning. Note that most of the methods
    based on adversarial policy regularization are a form of state observation perturbation
    method $\mathcal{G}_{S}^{S}$ as described in Definition [3.6](#S3.Thmtheorem6
    "Definition 3.6 (Generalization testing). ‣ 3 How to Achieve Generalization? ‣
    A Survey Analyzing Generalization in Deep Reinforcement Learning").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：用于深度强化学习中对抗策略正则化和攻击技术的环境及算法细节。请注意，大多数基于对抗策略正则化的方法都是一种状态观察扰动方法 $\mathcal{G}_{S}^{S}$，如定义[3.6](#S3.Thmtheorem6
    "Definition 3.6 (Generalization testing). ‣ 3 How to Achieve Generalization? ‣
    A Survey Analyzing Generalization in Deep Reinforcement Learning") 所述。
- en: '| Citation | Proposed Method | Environment | Reinforcement Learning Algorithm
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 引用 | 提出的方法 | 环境 | 强化学习算法 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Huang et al. ([2017](#bib.bib26)) | FGSM | ALE | DQN, TRPO, A3C |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Huang 等人 ([2017](#bib.bib26)) | FGSM | ALE | DQN, TRPO, A3C |'
- en: '| Kos & Song ([2017](#bib.bib42)) | FGSM | ALE | DQN and IQN |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Kos & Song ([2017](#bib.bib42)) | FGSM | ALE | DQN 和 IQN |'
- en: '| Lin et al. ([2017](#bib.bib49)) | Strategically-Timed Attack | ALE | A3C
    and DQN |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Lin 等人 ([2017](#bib.bib49)) | 战略性时机攻击 | ALE | A3C 和 DQN |'
- en: '| Gleave et al. ([2020](#bib.bib20)) | Adversarial Policies | Mujoco | Proximal
    Policy Optimization |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Gleave 等人 ([2020](#bib.bib20)) | 对抗策略 | Mujoco | 近端策略优化 |'
- en: '| Huan et al. ([2020](#bib.bib25)) | SA-DQN | ALE and $L_{\textrm{Mujoco}}$⁶⁶6Low
    dimensional state Mujoco refers to the setting of Mujoco where the state dimensions
    are not represented by pixels and dimensions of the state observations range from
    11 to 117. | DDQN and PPO |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Huan 等人 ([2020](#bib.bib25)) | SA-DQN | ALE 和 $L_{\textrm{Mujoco}}$⁶⁶6 低维状态
    Mujoco 指的是 Mujoco 的设置，其中状态维度不是由像素表示，状态观察的维度范围从 11 到 117。 | DDQN 和 PPO |'
- en: '| Korkmaz ([2022](#bib.bib39)) | Adversarial Framework | ALE | DDQN and A3C
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Korkmaz ([2022](#bib.bib39)) | 对抗框架 | ALE | DDQN 和 A3C |'
- en: '| Korkmaz ([2023](#bib.bib40)) | Natural Attacks | ALE | DDQN and A3C |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Korkmaz ([2023](#bib.bib40)) | 自然攻击 | ALE | DDQN 和 A3C |'
- en: '| Korkmaz & Brown-Cohen ([2023](#bib.bib41)) | Adversarial Detection | ALE
    | DDQN |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Korkmaz & Brown-Cohen ([2023](#bib.bib41)) | 对抗检测 | ALE | DDQN |'
- en: Note that this line of work falls under the state observation generalization
    testing category $\mathcal{G}_{S}^{S}$ provided in Definition [3.6](#S3.Thmtheorem6
    "Definition 3.6 (Generalization testing). ‣ 3 How to Achieve Generalization? ‣
    A Survey Analyzing Generalization in Deep Reinforcement Learning").
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这一工作属于状态观察泛化测试类别 $\mathcal{G}_{S}^{S}$，如定义[3.6](#S3.Thmtheorem6 "Definition
    3.6 (Generalization testing). ‣ 3 How to Achieve Generalization? ‣ A Survey Analyzing
    Generalization in Deep Reinforcement Learning") 所述。
- en: While several studies focused on improving optimization techniques to compute
    optimal perturbations, a line of research focused on making deep neural policies
    resilient to these perturbations. Pinto et al. ([2017](#bib.bib58)) proposed to
    model the dynamics between the adversary and the deep neural policy as a zero-sum
    game where the goal of the adversary is to minimize expected cumulative rewards
    of the deep reinforcement learning policy. This study is a clear example of transition
    probability perturbation to achieve generalization $\mathcal{G}_{\mathcal{P}}$
    in Definition [3.4](#S3.Thmtheorem4 "Definition 3.4 (Transition probability transforming
    generalization). ‣ 3 How to Achieve Generalization? ‣ A Survey Analyzing Generalization
    in Deep Reinforcement Learning") of Section [3](#S3 "3 How to Achieve Generalization?
    ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning"). Gleave et al.
    ([2020](#bib.bib20)) approached this problem with an adversary model which is
    restricted to take natural actions in the MDP instead of modifying the observations
    with $\ell_{p}$-norm bounded perturbations. The authors model this dynamic as
    a zero-sum Markov game and solve it via self play Proximal Policy Optimization
    (PPO). Some recent studies, proposed to model the interaction between the adversary
    and the deep reinforcement learning policy as a state-adversarial MDP, and claimed
    that their proposed algorithm State Adversarial Double Deep Q-Network (SA-DDQN)
    learns theoretically certified robust policies against natural noise and perturbations.
    In particular, these certified adversarial training techniques aim to add a regularizer
    term to the temporal difference loss in deep $Q$-learning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些研究专注于改进优化技术以计算最优扰动，但有一系列研究致力于使深度神经策略对这些扰动具有鲁棒性。Pinto等人 ([2017](#bib.bib58))
    提出了将对手与深度神经策略之间的动态建模为零和游戏，其中对手的目标是最小化深度强化学习策略的预期累计奖励。这项研究清楚地展示了通过转移概率扰动实现通用性$\mathcal{G}_{\mathcal{P}}$，如第[3.4](#S3.Thmtheorem4
    "Definition 3.4 (Transition probability transforming generalization). ‣ 3 How
    to Achieve Generalization? ‣ A Survey Analyzing Generalization in Deep Reinforcement
    Learning")节中的定义所述。Gleave等人 ([2020](#bib.bib20)) 通过限制对手在MDP中采取自然动作而不是用$\ell_{p}$-范数有界的扰动修改观测来处理这个问题。作者将这一动态建模为零和马尔可夫游戏，并通过自我博弈的近端策略优化（PPO）来解决它。一些近期研究提出将对手与深度强化学习策略之间的互动建模为状态对抗MDP，并声称他们提出的算法状态对抗双深度Q网络（SA-DDQN）能够学习理论上认证的鲁棒策略，对抗自然噪声和扰动。特别是，这些认证对抗训练技术旨在在深度$Q$-学习中的时间差损失中添加一个正则化项。
- en: '|  | $\mathcal{H}(r_{i}+\gamma\max_{a}\hat{Q}_{\hat{\theta}}(s_{i},a;\theta)-Q_{\theta}(s_{i},a_{i};\theta))+\kappa\mathcal{R}(\theta)$
    |  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{H}(r_{i}+\gamma\max_{a}\hat{Q}_{\hat{\theta}}(s_{i},a;\theta)-Q_{\theta}(s_{i},a_{i};\theta))+\kappa\mathcal{R}(\theta)$
    |  |'
- en: where $\mathcal{H}$ is the Huber loss, $\hat{Q}$ refers to the target network
    and $\kappa$ is to adjust the level of regularization for convergence. The regularizer
    term can vary for different certified adversarial training techniques yet the
    baseline technique uses
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{H}$是Huber损失，$\hat{Q}$指的是目标网络，$\kappa$用于调整收敛的正则化水平。不同的认证对抗训练技术可能使用不同的正则化项，而基线技术使用的是
- en: '|  | $\displaystyle\mathcal{R}(\theta)=\max\{\max_{\hat{s}\in B(s)}\max_{a\neq\operatorname*{arg\,max}_{a^{\prime}}Q(s,a^{\prime})}Q_{\theta}(\hat{s},a)-Q_{\theta}(\hat{s},\operatorname*{arg\,max}_{a^{\prime}}Q(s,a^{\prime}),-c\}.$
    |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{R}(\theta)=\max\{\max_{\hat{s}\in B(s)}\max_{a\neq\operatorname*{arg\,max}_{a^{\prime}}Q(s,a^{\prime})}Q_{\theta}(\hat{s},a)-Q_{\theta}(\hat{s},\operatorname*{arg\,max}_{a^{\prime}}Q(s,a^{\prime}),-c\}.$
    |  |'
- en: where $B(s)$ is an $\ell_{p}$-norm ball of radius $\epsilon$. While these certified
    adversarial training techniques drew some attention from the community, more recently
    manifold concerns have been raised on the robustness of theoretically certified
    adversarially trained deep reinforcement learning policies (Korkmaz, [2021e](#bib.bib38);
    [2022](#bib.bib39)). In these studies, the authors argue that adversarially trained
    (i.e. certified robust) deep reinforcement learning policies learn inaccurate
    state-action value functions and non-robust features from the environment. In
    particular, in Korkmaz ([2021c](#bib.bib36)) the authors use action manipulation
    to investigate worst-case perturbation training. This study is also a clear example
    of a policy perturbation generalization testing method $\mathcal{G}_{S}^{\pi}$
    in Definition [3.6](#S3.Thmtheorem6 "Definition 3.6 (Generalization testing).
    ‣ 3 How to Achieve Generalization? ‣ A Survey Analyzing Generalization in Deep
    Reinforcement Learning"). More importantly, recently it has been shown that adversarially
    trained deep reinforcement learning policies have worse generalization capabilities
    compared to vanilla trained reinforcement learning policies in high dimensional
    state space MDPs (Korkmaz, [2023](#bib.bib40))⁷⁷7A short and preliminary version
    of the paper (Korkmaz, [2023](#bib.bib40)) can also be found here (Korkmaz, [2021b](#bib.bib35)).
    While this study provides a contradistinction between adversarial directions and
    natural directions that are intrinsic to the MDP, it further demonstrates that
    the certified adversarial training techniques block generalization capabilities
    of standard deep reinforcement learning policies. Furthermore note that this study
    is also a clear example of a state observation perturbation generalization testing
    method $\mathcal{G}_{S}^{S}$ in Definition [3.6](#S3.Thmtheorem6 "Definition 3.6
    (Generalization testing). ‣ 3 How to Achieve Generalization? ‣ A Survey Analyzing
    Generalization in Deep Reinforcement Learning") in Section [3](#S3 "3 How to Achieve
    Generalization? ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning").
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $B(s)$ 是半径为 $\epsilon$ 的 $\ell_{p}$-范数球体。尽管这些经过认证的对抗训练技术引起了社区的一些关注，但最近对理论上经过认证的对抗性训练的深度强化学习策略的鲁棒性提出了更多关注（Korkmaz,
    [2021e](#bib.bib38); [2022](#bib.bib39)）。在这些研究中，作者认为对抗训练（即经过认证的鲁棒性）深度强化学习策略从环境中学习到了不准确的状态-动作值函数和非鲁棒特征。特别是在
    Korkmaz ([2021c](#bib.bib36)) 中，作者使用动作操控来研究最坏情况扰动训练。这项研究也是定义 [3.6](#S3.Thmtheorem6
    "Definition 3.6 (Generalization testing). ‣ 3 How to Achieve Generalization? ‣
    A Survey Analyzing Generalization in Deep Reinforcement Learning") 中政策扰动泛化测试方法
    $\mathcal{G}_{S}^{\pi}$ 的一个清晰示例。更重要的是，最近已显示对抗性训练的深度强化学习策略在高维状态空间 MDPs 中的泛化能力比普通训练的强化学习策略更差（Korkmaz,
    [2023](#bib.bib40)）⁷⁷7A short and preliminary version of the paper (Korkmaz, [2023](#bib.bib40))
    can also be found here (Korkmaz, [2021b](#bib.bib35))。虽然这项研究提供了对抗方向与 MDP 固有的自然方向之间的对比，但它进一步展示了经过认证的对抗训练技术阻碍了标准深度强化学习策略的泛化能力。此外，请注意这项研究也是定义
    [3.6](#S3.Thmtheorem6 "Definition 3.6 (Generalization testing). ‣ 3 How to Achieve
    Generalization? ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning")
    中状态观察扰动泛化测试方法 $\mathcal{G}_{S}^{S}$ 的一个清晰示例。
- en: 7 Meta-Reinforcement Learning and Meta Gradients
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 元强化学习与元梯度
- en: A quite recent line of research directs its research efforts to discovering
    reinforcement learning algorithms automatically, without explicitly designing
    them, via meta-gradients (Oh et al., [2020](#bib.bib55); Xu et al., [2020](#bib.bib76)).
    This line of study targets learning the ”learning algorithm” by only interacting
    with a set of environments as a meta-learning problem. In particular,
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一项相当新的研究方向将其研究精力集中在自动发现强化学习算法上，而无需明确设计这些算法，通过元梯度（Oh et al., [2020](#bib.bib55);
    Xu et al., [2020](#bib.bib76)）。这项研究的目标是通过仅与一组环境进行交互，将“学习算法”作为元学习问题进行学习。特别是，
- en: '|  | $\eta^{*}=\operatorname*{arg\,max}_{\eta}\mathbb{E}_{\varepsilon\sim\rho(\varepsilon)}\mathbb{E}_{\theta_{0}\sim\rho(\theta_{0})}[\mathbb{E}_{\theta_{N}}[\sum_{t=0}^{\infty}\gamma^{t}r_{t}]]$
    |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $\eta^{*}=\operatorname*{arg\,max}_{\eta}\mathbb{E}_{\varepsilon\sim\rho(\varepsilon)}\mathbb{E}_{\theta_{0}\sim\rho(\theta_{0})}[\mathbb{E}_{\theta_{N}}[\sum_{t=0}^{\infty}\gamma^{t}r_{t}]]$
    |  |'
- en: here the optimal update rule is parametrized by $\eta$, for a distribution on
    environments $\rho(\varepsilon)$ and initial policy parameters $\rho(\theta_{0})$
    where $\mathbb{E}_{\theta_{N}}[\sum_{t=0}^{\infty}\gamma^{t}r_{t}]$ is the expected
    return for the end of the lifetime of the agent.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的最优更新规则由 $\eta$ 参数化，对于环境的分布 $\rho(\varepsilon)$ 和初始策略参数 $\rho(\theta_{0})$，其中
    $\mathbb{E}_{\theta_{N}}[\sum_{t=0}^{\infty}\gamma^{t}r_{t}]$ 是智能体生命周期结束时的期望回报。
- en: The objective of meta-reinforcement learning is to be able to build agents that
    can learn how to learn over time, thus allowing these policies to adapt to a changing
    environment or even any other changing conditions of the MDP. Quite recently,
    a significant line of research has been conducted to achieve this objective, particularly
    Oh et al. ([2020](#bib.bib55)) proposes to discover update rules for reinforcement
    learning. This line of work also falls under the policy transformation generalization
    $\mathcal{G}_{\pi}$ in Definition [3.5](#S3.Thmtheorem5 "Definition 3.5 (Policy
    transforming generalization). ‣ 3 How to Achieve Generalization? ‣ A Survey Analyzing
    Generalization in Deep Reinforcement Learning") defined in Section [3](#S3 "3
    How to Achieve Generalization? ‣ A Survey Analyzing Generalization in Deep Reinforcement
    Learning"). Following this work Xu et al. ([2020](#bib.bib76)) proposed a joint
    meta-learning framework to learn what the policy should predict and how these
    predictions should be used in updating the policy. Recently, Kirsch et al. ([2022](#bib.bib32))
    proposes to use symmetry information in discovering reinforcement learning algorithms
    and discusses meta-generalization.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 元强化学习的目标是能够构建能够随着时间学习如何学习的智能体，从而使这些策略能够适应变化的环境或MDP的任何其他变化条件。最近，为实现这一目标进行了大量研究，特别是Oh
    et al. ([2020](#bib.bib55)) 提出了发现强化学习更新规则的方案。这一工作也属于定义在第[3.5](#S3.Thmtheorem5
    "Definition 3.5 (Policy transforming generalization). ‣ 3 How to Achieve Generalization?
    ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning")节中的政策转化泛化
    $\mathcal{G}_{\pi}$。随后，Xu et al. ([2020](#bib.bib76)) 提出了一个联合元学习框架，以学习策略应该预测什么以及如何利用这些预测来更新策略。最近，Kirsch
    et al. ([2022](#bib.bib32)) 提出了在发现强化学习算法时使用对称信息，并讨论了元泛化。
- en: There is also some work on enabling reinforcement learning algorithms to discover
    temporal abstractions (Veeriah et al., [2021](#bib.bib68)). In particular, temporal
    abstraction refers to the ability of the policy to abstract a sequence of actions
    to achieve certain sub-tasks. As it is promised within this subfield, meta-reinforcement
    learning is considered to be a research direction that could enable us to build
    deep reinforcement learning policies that can generalize to different environments,
    to changing environments over time, or even to different tasks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些工作致力于使强化学习算法发现时间抽象（Veeriah et al., [2021](#bib.bib68)）。特别地，时间抽象指的是策略能够将一系列动作抽象为完成某些子任务的能力。正如该子领域所承诺的，元强化学习被认为是一个研究方向，它可以使我们构建出能够适应不同环境、随时间变化的环境甚至不同任务的深度强化学习策略。
- en: 8 Transfer in Reinforcement Learning
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 强化学习中的迁移
- en: Transfer in reinforcement learning is a subfield heavily discussed in certain
    applications of reinforcement learning algorithms e.g. robotics. In current robotics
    research there is not a safe way of training a reinforcement learning agent by
    letting the robot explore in real life. Hence, the way to overcome this is to
    train policies in a simulated environment, and install the trained policies in
    the actual application setting. The fact that the simulation environment and the
    installation environment are not identical is one of the main problems for reinforcement
    learning application research. This is referred to as the sim-to-real gap.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的迁移是一个在某些强化学习算法应用中（如机器人技术）被广泛讨论的子领域。在当前的机器人研究中，没有安全的方法通过让机器人在现实生活中探索来训练强化学习智能体。因此，克服这一问题的方法是先在模拟环境中训练策略，然后将训练好的策略应用到实际应用环境中。模拟环境和安装环境的不完全相同是强化学习应用研究的主要问题之一。这被称为模拟到现实的差距。
- en: Another subfield in reinforcement learning research focusing on obtaining generalizable
    policies investigates this concept through transfer in reinforcement learning.
    The consideration in this line of research is to build policies that are trained
    for a particular task with limited data and to try to make these policies perform
    well on slightly different tasks. An initial discussion on this starts with (Taylor
    & Stone, [2007](#bib.bib64)) to obtain policies initially trained in a source
    task and transferred to a target task in a more sample efficient way. Later, Tirinzoni
    et al. ([2018](#bib.bib66)) proposes to transfer value functions that are based
    on learning a prior distribution over optimal value functions from a source task.
    However, this study is conducted in simple environments with low dimensional state
    spaces. (Barreto et al., [2017](#bib.bib6)) considers the reward transformation
    setting $\mathcal{G}_{R}$ in Definition [3.2](#S3.Thmtheorem2 "Definition 3.2
    (Rewards transforming generalization). ‣ 3 How to Achieve Generalization? ‣ A
    Survey Analyzing Generalization in Deep Reinforcement Learning") from Section
    [3](#S3 "3 How to Achieve Generalization? ‣ A Survey Analyzing Generalization
    in Deep Reinforcement Learning"). In particular, the authors consider a policy
    transfer between a specific task with a reward function $r(s,a)$ and a different
    task with reward function $r^{\prime}(s,a)$. The goal of the study is to decouple
    the state representations from the task. In the setting of state transformation
    for generalization $\mathcal{G}_{S}$ in Definition [3.3](#S3.Thmtheorem3 "Definition
    3.3 (State transforming generalization). ‣ 3 How to Achieve Generalization? ‣
    A Survey Analyzing Generalization in Deep Reinforcement Learning") Gamrian & Goldberg
    ([2019](#bib.bib17)) focuses on state-wise differences between source and target
    task. In particular, the authors use unaligned generative adversarial networks
    to create target task states from source task states. In the setting of policy
    transformation for generalization $\mathcal{G}_{\pi}$ in Definition [3.5](#S3.Thmtheorem5
    "Definition 3.5 (Policy transforming generalization). ‣ 3 How to Achieve Generalization?
    ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning") Jain et al.
    ([2020](#bib.bib29)) focuses on zero-shot generalization to a newly introduced
    action set to increase adaptability.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习研究中的另一个子领域专注于获取具有普遍适应性的策略，通过强化学习中的迁移来探讨这一概念。该研究领域的考虑是建立针对特定任务在有限数据下训练的策略，并尝试使这些策略在略微不同的任务上表现良好。关于这方面的初步讨论开始于
    (Taylor & Stone, [2007](#bib.bib64))，旨在以更高效的样本方式将初步在源任务中训练的策略迁移到目标任务。随后，Tirinzoni
    等人 ([2018](#bib.bib66)) 提出了迁移基于从源任务中学习的最优价值函数的先验分布的价值函数。然而，该研究是在具有低维状态空间的简单环境中进行的。
    (Barreto 等人, [2017](#bib.bib6)) 考虑了在第 [3.2](#S3.Thmtheorem2 "定义 3.2 (奖励变换泛化)。
    ‣ 3 如何实现泛化？ ‣ 一项分析深度强化学习中泛化的调查") 节中定义的奖励变换设置 $\mathcal{G}_{R}$。特别是，作者考虑了在奖励函数
    $r(s,a)$ 的特定任务与奖励函数 $r^{\prime}(s,a)$ 的不同任务之间的策略迁移。该研究的目标是将状态表示与任务解耦。在第 [3.3](#S3.Thmtheorem3
    "定义 3.3 (状态变换泛化)。 ‣ 3 如何实现泛化？ ‣ 一项分析深度强化学习中泛化的调查") 节中定义的状态变换泛化 $\mathcal{G}_{S}$
    设置中，Gamrian & Goldberg ([2019](#bib.bib17)) 专注于源任务和目标任务之间的状态差异。特别是，作者使用未对齐的生成对抗网络从源任务状态生成目标任务状态。在第
    [3.5](#S3.Thmtheorem5 "定义 3.5 (策略变换泛化)。 ‣ 3 如何实现泛化？ ‣ 一项分析深度强化学习中泛化的调查") 节中定义的策略变换泛化
    $\mathcal{G}_{\pi}$ 设置中，Jain 等人 ([2020](#bib.bib29)) 专注于对新引入的动作集进行零样本泛化以提高适应性。
- en: While transfer learning is a promising research direction for reinforcement
    learning, the studies in this subfield still remain oriented only towards reinforcement
    learning applications, and it is possible to consider the research centered on
    this subfield as not at the same level of maturity as the previously discussed
    line of research in Section [6](#S6 "6 Regularization ‣ A Survey Analyzing Generalization
    in Deep Reinforcement Learning") in terms of being able to test the claims or
    propositions in complex established baselines.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管迁移学习是强化学习的一个有前景的研究方向，但该子领域的研究仍然仅仅面向强化学习应用，与第 [6](#S6 "6 正则化 ‣ 一项分析深度强化学习中泛化的调查")
    节中讨论的研究线相比，可能存在研究成熟度的差异，无法在复杂的已建立基准中测试这些主张或提议。
- en: 9 Lifelong Reinforcement Learning
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 终身强化学习
- en: Lifelong learning is a subfield closely related to transfer learning that has
    recently drawn attention from the reinforcement learning community. Lifelong learning
    aims to build policies that can sequentially solve different tasks by being able
    to transfer knowledge between tasks. On this line of research, Lecarpentier et al.
    ([2021](#bib.bib46)) provide an algorithm for value-based transfer in the Lipschitz
    continuous task space with theoretical contributions for lifelong learning goals.
    In the setting of action transformation for generalization $\mathcal{G}_{\pi}$
    in Definition [3.5](#S3.Thmtheorem5 "Definition 3.5 (Policy transforming generalization).
    ‣ 3 How to Achieve Generalization? ‣ A Survey Analyzing Generalization in Deep
    Reinforcement Learning") Chandak et al. ([2020](#bib.bib10)) focuses on temporally
    varying (e.g. variations between source task and target task) the action set in
    lifelong learning. In lifelong reinforcement learning some studies focus on different
    exploration strategies. In particular, Garcia & Thomas ([2019](#bib.bib18)) models
    the exploration strategy problem for lifelong learning as another MDP, and the
    study uses a separate reinforcement learning agent to find an optimal exploration
    method for the initial lifelong learning agent. The lack of benchmarks limits
    the progress of lifelong reinforcement learning research by restricting the direct
    comparison between proposed algorithms or methods. However, quite recent work
    proposed a new training environment benchmark based on robotics applications for
    lifelong learning to overcome this issue (Wolczyk et al., [2021](#bib.bib75))⁸⁸8The
    state dimension for this benchmark is 12\. Hence, the state space is low dimensional..
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 终身学习是一个与迁移学习紧密相关的子领域，最近引起了强化学习社区的关注。终身学习旨在构建能够通过任务间知识转移来顺序解决不同任务的策略。在这方面的研究中，Lecarpentier
    等人 ([2021](#bib.bib46)) 提出了一个在Lipschitz连续任务空间中基于价值的转移算法，并为终身学习目标提供了理论贡献。在定义为第[3.5](#S3.Thmtheorem5
    "定义 3.5 (策略转换的泛化). ‣ 3 如何实现泛化？ ‣ 深度强化学习中泛化分析的综述")节的动作转换泛化设置$\mathcal{G}_{\pi}$中，Chandak
    等人 ([2020](#bib.bib10)) 关注于终身学习中动作集的时间变化（例如，源任务和目标任务之间的变化）。在终身强化学习中，一些研究关注不同的探索策略。特别是，Garcia
    & Thomas ([2019](#bib.bib18)) 将终身学习的探索策略问题建模为另一个MDP，并使用一个独立的强化学习代理来为初始终身学习代理寻找最佳探索方法。基准测试的缺乏限制了终身强化学习研究的进展，因为它限制了提出的算法或方法之间的直接比较。然而，最近有工作提出了一个基于机器人应用的新的训练环境基准，以克服这一问题（Wolczyk
    等人，[2021](#bib.bib75))⁸⁸8该基准的状态维度为12\. 因此，状态空间是低维的。
- en: 10 Inverse Reinforcement Learning
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 逆向强化学习
- en: Inverse reinforcement learning focuses on learning a functioning policy in the
    absence of a reward function. Since the real reward function is inaccessible in
    this setting and the reward function needs to be learnt from observing an expert
    completing the given task, the inverse reinforcement learning setting falls under
    the reward transformation for generalization setting $\mathcal{G}_{R}$ defined
    in Definition [3.2](#S3.Thmtheorem2 "Definition 3.2 (Rewards transforming generalization).
    ‣ 3 How to Achieve Generalization? ‣ A Survey Analyzing Generalization in Deep
    Reinforcement Learning") in Section [3](#S3 "3 How to Achieve Generalization?
    ‣ A Survey Analyzing Generalization in Deep Reinforcement Learning"). The initial
    work that introduced inverse reinforcement learning was proposed by Ng & Russell
    ([2000](#bib.bib54)) demonstrating that multiple different reward functions can
    be constructed for an observed optimal policy. The authors of this initial study
    achieve this objective via linear programming,
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 逆向强化学习关注于在缺乏奖励函数的情况下学习一个有效的策略。由于在这种设置下真实的奖励函数无法访问，而奖励函数需要通过观察专家完成给定任务来学习，因此逆向强化学习的设置属于定义在第[3.2](#S3.Thmtheorem2
    "定义 3.2 (奖励转换的泛化). ‣ 3 如何实现泛化？ ‣ 深度强化学习中泛化分析的综述")节[3](#S3 "3 如何实现泛化？ ‣ 深度强化学习中泛化分析的综述")中的奖励转换泛化设置$\mathcal{G}_{R}$。首次引入逆向强化学习的工作由Ng
    & Russell ([2000](#bib.bib54))提出，展示了对于一个观察到的最优策略，可以构造出多个不同的奖励函数。该初步研究的作者通过线性规划实现了这一目标。
- en: '|  | $\displaystyle\textrm{maximize}\sum_{s\in S_{\rho}}$ | $\displaystyle\min_{a\in
    A}\{p(\mathbb{E}_{s^{\prime}\sim\mathcal{P}(s,a_{1}&#124;\cdot)}\mathcal{V}^{\pi}(s^{\prime})-\mathbb{E}_{s^{\prime}\sim\mathcal{P}(s,a&#124;\cdot)}\mathcal{V}^{\pi}(s^{\prime}))\}$
    |  | (2) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textrm{maximize}\sum_{s\in S_{\rho}}$ | $\displaystyle\min_{a\in
    A}\{p(\mathbb{E}_{s^{\prime}\sim\mathcal{P}(s,a_{1}&#124;\cdot)}\mathcal{V}^{\pi}(s^{\prime})-\mathbb{E}_{s^{\prime}\sim\mathcal{P}(s,a&#124;\cdot)}\mathcal{V}^{\pi}(s^{\prime}))\}$
    |  | (2) |'
- en: '|  |  | $\displaystyle\textrm{s.t.}\>\>&#124;\alpha_{i}&#124;\leq 1\>,\>i=1,2,\dots,d$
    |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\textrm{s.t.}\>\>&#124;\alpha_{i}&#124;\leq 1\>,\>i=1,2,\dots,d$
    |  |'
- en: where $p(x)=x$ if $x\geq 0$, $p(x)=2x$ otherwise and $\mathcal{V}^{\pi}=\alpha_{1}\mathcal{V}^{\pi}_{1}+\alpha_{2}\mathcal{V}^{\pi}_{2}+\dots+\alpha_{d}\mathcal{V}^{\pi}_{d}$.
    In this line of work, there has been recent progress that achieved learning functioning
    policies in high-dimensional state observation MDPs (Garg et al., [2021](#bib.bib19)).
    The study achieves this by learning a soft $Q$-function from observing expert
    demonstrations, and the study further argues that it is possible to recover rewards
    from the learnt soft state-action value function.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p(x)=x$ 如果 $x\geq 0$，否则 $p(x)=2x$，且 $\mathcal{V}^{\pi}=\alpha_{1}\mathcal{V}^{\pi}_{1}+\alpha_{2}\mathcal{V}^{\pi}_{2}+\dots+\alpha_{d}\mathcal{V}^{\pi}_{d}$。在这方面的研究中，最近取得了一些进展，实现了在高维状态观测MDP中学习有效的策略（Garg
    等，[2021](#bib.bib19)）。该研究通过观察专家演示来学习软 $Q$-函数，并进一步论证了从学习到的软状态-动作值函数中恢复奖励是可能的。
- en: 11 Conclusion
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11 结论
- en: 'In this paper we tried to answer the following questions: (i) What are the
    explicit problems limiting reinforcement learning algorithms from obtaining high-performing
    policies that can generalize? (ii) How can we categorize the different techniques
    proposed so far to achieve generalization in reinforcement learning? (iii) What
    are the similarities and differences of these different techniques proposed by
    different subfields of reinforcement learning research to build reinforcement
    learning policies that generalize? To answer these questions first we explain
    the importance of exploration strategies in overfitting, and explain the manifold
    causes of overestimation bias in reinforcement learning. In the second part of
    the paper we propose a framework to unify and categorize the various techniques
    to achieve generalization in reinforcement learning. Starting from explaining
    all the different regularization techniques in either state representations or
    in learnt value functions from worst-case to average-case, we provide a current
    layout of the wide range of reinforcement learning subfields that are essentially
    working towards the same objective, i.e. generalizable deep reinforcement learning
    policies. Finally, we provided a discussion for each category on the drawbacks
    and advantages of these algorithms. We believe our study can provide a compact
    unifying formalization on recent reinforcement learning generalization research.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们尝试回答以下问题：（i）哪些显性问题限制了强化学习算法获得能够泛化的高性能策略？（ii）我们如何对迄今为止提出的不同技术进行分类，以实现强化学习中的泛化？（iii）这些由不同强化学习研究子领域提出的不同技术在构建能够泛化的强化学习策略方面有哪些相似性和差异？为了回答这些问题，我们首先解释了探索策略在过拟合中的重要性，并解释了强化学习中过估计偏差的多种原因。在论文的第二部分，我们提出了一个框架，用于统一和分类各种实现强化学习泛化的技术。从解释状态表示或学习的价值函数中的不同正则化技术，从最坏情况到平均情况，我们提供了一个关于强化学习子领域的广泛布局，这些子领域本质上都在朝着相同的目标努力，即可泛化的深度强化学习策略。最后，我们对每个类别的算法的缺点和优点进行了讨论。我们相信我们的研究可以为近期强化学习泛化研究提供一个紧凑的统一形式化。
- en: References
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Agarwal et al. (2021a) Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro,
    and Marc G. Bellemare. Contrastive behavioral similarity embeddings for generalization
    in reinforcement learning. In *International Conference on Learning Representations
    (ICLR)*, 2021a.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等（2021a）Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, 和 Marc
    G. Bellemare。强化学习中的对比行为相似性嵌入。在 *国际学习表征会议（ICLR）*，2021a。
- en: Agarwal et al. (2021b) Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro,
    Aaron C. Courville, and Marc G. Bellemare. Deep reinforcement learning at the
    edge of the statistical precipice. *Conference on Neural Information Processing
    Systems (NeurIPS)*, 2021b.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等（2021b）Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C.
    Courville, 和 Marc G. Bellemare。统计悬崖边缘的深度强化学习。*神经信息处理系统会议（NeurIPS）*，2021b。
- en: Amit et al. (2020) Ron Amit, Ron Meir, and Kamil Ciosek. Discount factor as
    a regularizer in RL. In *International Conference on Machine Learning (ICML)*,
    2020.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amit et al. (2020) Ron Amit, Ron Meir 和 Kamil Ciosek. 强化学习中的折扣因子作为正则化项。发表于*国际机器学习会议（ICML）*，2020年。
- en: 'Anschel et al. (2017) Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn:
    Variance reduction and stabilization for deep reinforcement learning. In *International
    Conference on Machine Learning (ICML)*, 2017.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anschel et al. (2017) Oron Anschel, Nir Baram 和 Nahum Shimkin. 平均DQN：深度强化学习中的方差减少和稳定性。发表于*国际机器学习会议（ICML）*，2017年。
- en: 'Baird (1995) Leemon Baird. Residual algorithms: RL with function approximation.
    In *International Conference on Machine Learning (ICML)*, 1995.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baird (1995) Leemon Baird. 残差算法：具有函数逼近的强化学习。发表于*国际机器学习会议（ICML）*，1995年。
- en: Barreto et al. (2017) André Barreto, Will Dabney, Rémi Munos, Jonathan J. Hunt,
    Tom Schaul, David Silver, and Hado van Hasselt. Successor features for transfer
    in reinforcement learning. In *Conference on Neural Information Processing Systems
    (NeurIPS)*, 2017.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barreto et al. (2017) André Barreto, Will Dabney, Rémi Munos, Jonathan J. Hunt,
    Tom Schaul, David Silver 和 Hado van Hasselt. 强化学习中的后继特征用于迁移。发表于*神经信息处理系统会议（NeurIPS）*，2017年。
- en: 'Bellemare et al. (2013) Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael.
    Bowling. The arcade learning environment: An evaluation platform for general agents.
    *JAIR*, 2013.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare et al. (2013) Marc G Bellemare, Yavar Naddaf, Joel Veness 和 Michael
    Bowling. 游戏机学习环境：通用智能体的评估平台。发表于*JAIR*，2013年。
- en: Bellemare et al. (2016) Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski,
    Tom Schaul, David Saxton, and Rémi Munos. Unifying count-based exploration and
    intrinsic motivation. *Conference on Neural Information Processing Systems (NeurIPS)*,
    2016.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare et al. (2016) Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski,
    Tom Schaul, David Saxton 和 Rémi Munos. 统一基于计数的探索和内在动机。发表于*神经信息处理系统会议（NeurIPS）*，2016年。
- en: 'Boyan & Moore (1994) Justin A. Boyan and Andrew W. Moore. Generalization in
    rl: Safely approximating the value function. In *Conference on Neural Information
    Processing Systems (NeurIPS)*, 1994.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boyan & Moore (1994) Justin A. Boyan 和 Andrew W. Moore. 强化学习中的泛化：安全地逼近价值函数。发表于*神经信息处理系统会议（NeurIPS）*，1994年。
- en: Chandak et al. (2020) Yash Chandak, Georgios Theocharous, Chris Nota, and Philip S.
    Thomas. Lifelong learning with a changing action set. In *AAAI Conference on Artificial
    Intelligence, AAAI* , 2020.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chandak et al. (2020) Yash Chandak, Georgios Theocharous, Chris Nota 和 Philip
    S. Thomas. 具有变化动作集的终身学习。发表于*AAAI 人工智能会议（AAAI）*，2020年。
- en: Cobbe et al. (2019) Karl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim,
    and John Schulman. Quantifying generalization in reinforcement learning. In *International
    Conference on Machine Learning (ICML)*, 2019.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe et al. (2019) Karl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim
    和 John Schulman. 量化强化学习中的泛化。发表于*国际机器学习会议（ICML）*，2019年。
- en: Cobbe et al. (2020) Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman.
    Leveraging procedural generation to benchmark reinforcement learning. *International
    Conference on MAchine Learning (ICML)*, 2020.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe et al. (2020) Karl Cobbe, Christopher Hesse, Jacob Hilton 和 John Schulman.
    利用过程生成来基准测试强化学习。发表于*国际机器学习会议（ICML）*，2020年。
- en: Cobbe et al. (2021) Karl Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman.
    Phasic policy gradient. In *International Conference on Machine Learning (ICML)*,
    2021.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe et al. (2021) Karl Cobbe, Jacob Hilton, Oleg Klimov 和 John Schulman. 相位策略梯度。发表于*国际机器学习会议（ICML）*，2021年。
- en: Fawzi et al. (2022) Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert,
    Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco
    J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis,
    and Pushmeet Kohli. Discovering faster matrix multiplication algorithms with reinforcement
    learning. *Nature*, 610(7930):47–53, 2022.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fawzi et al. (2022) Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert,
    Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco
    J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis
    和 Pushmeet Kohli. 通过强化学习发现更快的矩阵乘法算法。发表于*自然*，610(7930):47–53，2022年。
- en: Fortunato et al. (2018) Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot,
    Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Rémi Munos, Demis Hassabis,
    Olivier Pietquin, Charles Blundell, and Shane Legg. Noisy networks for exploration.
    *International Conference on Learning Representations (ICLR)*, 2018.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fortunato et al. (2018) Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot,
    Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Rémi Munos, Demis Hassabis,
    Olivier Pietquin, Charles Blundell 和 Shane Legg. 用于探索的噪声网络。发表于*国际学习表征会议（ICLR）*，2018年。
- en: Fujimoto et al. (2018) Scott Fujimoto, Herke van Hoof, and David Meger. Addressing
    function approximation error in actor-critic methods. In *International Conference
    on Machine Learning (ICML)*, 2018.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fujimoto 等 (2018) Scott Fujimoto, Herke van Hoof 和 David Meger。解决演员-评论家方法中的函数逼近误差。在
    *国际机器学习大会 (ICML)*，2018 年。
- en: Gamrian & Goldberg (2019) Shani Gamrian and Yoav Goldberg. Transfer learning
    for related RL tasks via image-to-image translation. In *International Conference
    on Machine Learning (ICML)*, 2019.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamrian & Goldberg (2019) Shani Gamrian 和 Yoav Goldberg。通过图像到图像转换的相关 RL 任务的迁移学习。在
    *国际机器学习大会 (ICML)*，2019 年。
- en: Garcia & Thomas (2019) Francisco M. Garcia and Philip S. Thomas. A meta-mdp
    approach to exploration for lifelong reinforcement learning. In *Conference on
    Neural Information Processing Systems (NeurIPS)*, 2019.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garcia & Thomas (2019) Francisco M. Garcia 和 Philip S. Thomas。用于终身强化学习的探索的元
    MDP 方法。在 *神经信息处理系统会议 (NeurIPS)*，2019 年。
- en: 'Garg et al. (2021) Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming
    Song, and Stefano Ermon. Iq-learn: Inverse soft-q learning for imitation. *Neural
    Information Processing Systems (NeurIPS) [Spotlight Presentation]*, 2021.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garg 等 (2021) Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song 和
    Stefano Ermon。Iq-learn：用于模仿的逆软 Q 学习。*神经信息处理系统 (NeurIPS) [专题报告]*，2021 年。
- en: 'Gleave et al. (2020) Adam Gleave, Michael Dennis, Cody Wild, Kant Neel, Sergey
    Levine, and Stuart Russell. Adversarial policies: Attacking deep RL. *International
    Conference on Learning Representations (ICLR)*, 2020.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gleave 等 (2020) Adam Gleave, Michael Dennis, Cody Wild, Kant Neel, Sergey Levine
    和 Stuart Russell。对抗性策略：攻击深度 RL。*国际学习表征大会 (ICLR)*，2020 年。
- en: Goodfellow et al. (2015) Ian Goodfellow, Jonathan Shelens, and Christian Szegedy.
    Explaning and harnessing adversarial examples. *International Conference on Learning
    Representations (ICLR)*, 2015.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 (2015) Ian Goodfellow, Jonathan Shelens 和 Christian Szegedy。解释和利用对抗样本。*国际学习表征大会
    (ICLR)*，2015 年。
- en: Hamrick et al. (2020) Jessica Hamrick, Victor Bapst, Alvaro SanchezGonzalez,
    Tobias Pfaff, Theophane Weber, Lars Buesing, and Peter Battaglia. Combining q-learning
    and search with amortized value estimates. In *8th International Conference on
    Learning Representations, ICLR*, 2020.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamrick 等 (2020) Jessica Hamrick, Victor Bapst, Alvaro SanchezGonzalez, Tobias
    Pfaff, Theophane Weber, Lars Buesing 和 Peter Battaglia。结合 Q 学习和搜索的摊销价值估计。在 *第八届国际学习表征大会,
    ICLR*，2020 年。
- en: Hasselt et al. (2016) Hado van Hasselt, Arthur Guez, and David Silver. Deep
    reinforcement learning with double q-learning. *AAAI Conference on Artificial
    Intelligence, AAAI*, 2016.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasselt 等 (2016) Hado van Hasselt, Arthur Guez 和 David Silver。使用双重 Q 学习的深度强化学习。*AAAI
    人工智能会议, AAAI*，2016 年。
- en: 'Houthooft et al. (2017) Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De
    Turck, and Pieter Abbeel. VIME: variational information maximizing exploration.
    In *Conference on Neural Information Processing Systems (NeurIPS)*, 2017.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houthooft 等 (2017) Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De
    Turck 和 Pieter Abbeel。VIME：变分信息最大化探索。在 *神经信息处理系统会议 (NeurIPS)*，2017 年。
- en: Huan et al. (2020) Zhang Huan, Chen Hongge, Xiao Chaowei, Bo Li, Mingyan Boning,
    Duane Liu, and ChoJui Hsiesh. Robust deep reinforcement learning against adversarial
    perturbations on state observatons. *Conference on Neural Information Processing
    Systems (NeurIPS)*, 2020.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huan 等 (2020) Zhang Huan, Chen Hongge, Xiao Chaowei, Bo Li, Mingyan Boning,
    Duane Liu 和 ChoJui Hsiesh。针对状态观测的对抗性扰动的鲁棒深度强化学习。*神经信息处理系统会议 (NeurIPS)*，2020 年。
- en: Huang et al. (2017) Sandy Huang, Nicholas Papernot, Yan Goodfellow, Ian an Duan,
    and Pieter Abbeel. Adversarial attacks on neural network policies. *International
    Conference on Learning Representations (ICLR)*, 2017.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2017) Sandy Huang, Nicholas Papernot, Yan Goodfellow, Ian an Duan 和
    Pieter Abbeel。对神经网络策略的对抗性攻击。*国际学习表征大会 (ICLR)*，2017 年。
- en: Igl et al. (2019) Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek,
    Cheng Zhang, Sam Devlin, and Katja Hofmann. Generalization in reinforcement learning
    with selective noise injection and information bottleneck. *Conference on Neural
    Information Processing Systems (NeurIPS)*, 2019.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Igl 等 (2019) Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek,
    Cheng Zhang, Sam Devlin 和 Katja Hofmann。具有选择性噪声注入和信息瓶颈的强化学习泛化。*神经信息处理系统会议 (NeurIPS)*，2019
    年。
- en: 'Ioffe & Szegedy (2015) Sergey Ioffe and Christian Szegedy. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. In *International
    Conference on Machine Learning (ICML)*, 2015.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe & Szegedy (2015) Sergey Ioffe 和 Christian Szegedy。批量归一化：通过减少内部协变量偏移加速深度网络训练。在
    *国际机器学习大会 (ICML)*，2015 年。
- en: Jain et al. (2020) Ayush Jain, Andrew Szot, and Joseph J. Lim. Generalization
    to new actions in RL. In *International Conference on Machine Learning (ICML)*,
    2020.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain et al. (2020) Ayush Jain, Andrew Szot 和 Joseph J. Lim. 强化学习中对新动作的泛化。见于
    *国际机器学习会议（ICML）*，2020。
- en: Kakade (2003) Sham Kakade. On the sample complexity of reinforcement learning.
    In *PhD Thesis*, 2003.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kakade (2003) Sham Kakade. 强化学习的样本复杂度。见于 *博士论文*，2003。
- en: Kapturowski et al. (2023) Steven Kapturowski, Victor Campos, Ray Jiang, Nemanja
    Rakicevic, Hado van Hasselt, Charles Blundell, and Adrià Puigdomènech Badia. Human-level
    atari 200x faster. In *The Eleventh International Conference on Learning Representations,
    ICLR 2023*, 2023.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapturowski et al. (2023) Steven Kapturowski, Victor Campos, Ray Jiang, Nemanja
    Rakicevic, Hado van Hasselt, Charles Blundell 和 Adrià Puigdomènech Badia. 人类水平的Atari游戏速度提升200倍。见于
    *第十一届国际学习表征会议，ICLR 2023*，2023。
- en: Kirsch et al. (2022) Louis Kirsch, Sebastian Flennerhag, Hado van Hasselt, Abram L.
    Friesen, Junhyuk Oh, and Yutian Chen. Introducing symmetries to black box meta
    reinforcement learning. In *AAAI Conference on Artificial Intelligence, AAAI*,
    2022.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirsch et al. (2022) Louis Kirsch, Sebastian Flennerhag, Hado van Hasselt, Abram
    L. Friesen, Junhyuk Oh 和 Yutian Chen. 在黑箱元强化学习中引入对称性。见于 *AAAI人工智能会议，AAAI*，2022。
- en: Korkmaz (2020) Ezgi Korkmaz. Nesterov momentum adversarial perturbations in
    the deep reinforcement learning domain. *International Conference on Machine Learning
    (ICML) Workshop.*, 2020.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korkmaz (2020) Ezgi Korkmaz. 深度强化学习领域中的Nesterov动量对抗扰动。见于 *国际机器学习会议（ICML）研讨会*，2020。
- en: Korkmaz (2021a) Ezgi Korkmaz. Adversarially trained neural policies in fourier
    domain. *International Conference on Learning Representation (ICLR) Robust and
    Reliable Machine Learning in the Real World Workshop*, 2021a.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korkmaz (2021a) Ezgi Korkmaz. 四维域中的对抗训练神经策略。见于 *国际学习表征会议（ICLR）现实世界中的稳健与可靠机器学习研讨会*，2021a。
- en: Korkmaz (2021b) Ezgi Korkmaz. Adversarial training blocks generalization in
    neural policies. *International Conference on Learning Representation (ICLR) Robust
    and Reliable Machine Learning in the Real World Workshop*, 2021b.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korkmaz (2021b) Ezgi Korkmaz. 对抗训练阻碍神经策略的泛化。见于 *国际学习表征会议（ICLR）现实世界中的稳健与可靠机器学习研讨会*，2021b。
- en: Korkmaz (2021c) Ezgi Korkmaz. Inaccuracy of state-action value function for
    non-optimal actions in adversarially trained deep neural policies. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops*,
    2021c.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korkmaz (2021c) Ezgi Korkmaz. 对抗训练深度神经策略中非最优动作的状态-动作值函数的不准确性。见于 *IEEE/CVF计算机视觉与模式识别会议（CVPR）研讨会*，2021c。
- en: Korkmaz (2021d) Ezgi Korkmaz. Non-robust feature mapping in deep reinforcement
    learning. *International Conference on Machine Learning (ICML) Adversarial Machine
    Learning Workshop*, 2021d.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korkmaz (2021d) Ezgi Korkmaz. 深度强化学习中的非鲁棒特征映射。见于 *国际机器学习会议（ICML）对抗机器学习研讨会*，2021d。
- en: Korkmaz (2021e) Ezgi Korkmaz. Investigating vulnerabilities of deep neural policies.
    *Conference on Uncertainty in Artificial Intelligence (UAI)*, 2021e.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korkmaz (2021e) Ezgi Korkmaz. 探讨深度神经策略的脆弱性。见于 *人工智能不确定性会议（UAI）*，2021e。
- en: Korkmaz (2022) Ezgi Korkmaz. Deep reinforcement learning policies learn shared
    adversarial features across mdps. *AAAI Conference on Artificial Intelligence,
    AAAI*, 2022.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korkmaz (2022) Ezgi Korkmaz. 深度强化学习策略在多智能体决策问题中学习共享对抗特征。见于 *AAAI人工智能会议，AAAI*，2022。
- en: Korkmaz (2023) Ezgi Korkmaz. Adversarial robust deep reinforcement learning
    requires redefining robustness. *AAAI Conference on Artificial Intelligence, AAAI*,
    2023.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korkmaz (2023) Ezgi Korkmaz. 对抗鲁棒深度强化学习需要重新定义鲁棒性。见于 *AAAI人工智能会议，AAAI*，2023。
- en: Korkmaz & Brown-Cohen (2023) Ezgi Korkmaz and Jonah Brown-Cohen. Detecting adversarial
    directions in deep reinforcement learning to make robust decisions. In *International
    Conference on Machine Learning, ICML 2023*, volume 202 of *Proceedings of Machine
    Learning Research*, pp. 17534–17543\. PMLR, 2023.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korkmaz & Brown-Cohen (2023) Ezgi Korkmaz 和 Jonah Brown-Cohen. 检测深度强化学习中的对抗方向以做出稳健决策。见于
    *国际机器学习会议，ICML 2023*，第202卷 *机器学习研究论文集*，第17534–17543页。PMLR，2023。
- en: Kos & Song (2017) Jernej Kos and Dawn Song. Delving into adversarial attacks
    on deep policies. *International Conference on Learning Representations (ICLR)*,
    2017.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kos & Song (2017) Jernej Kos 和 Dawn Song. 深入探讨对深度策略的对抗攻击。见于 *国际学习表征会议（ICLR）*，2017。
- en: Lan et al. (2021) Charline Le Lan, Marc G. Bellemare, and Pablo Samuel Castro.
    Metrics and continuity in reinforcement learning. In *AAAI Conference on Artificial
    Intelligence, AAAI*, 2021.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan等（2021）Charline Le Lan、Marc G. Bellemare和Pablo Samuel Castro。强化学习中的度量与连续性。发表于*AAAI人工智能会议，AAAI*，2021年。
- en: Laskin et al. (2020a) Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto,
    Pieter Abbeel, and Aravind Srinivas. Rl with augmented data. In *Conference on
    Neural Information Processing Systems (NeurIPS)*, 2020a.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laskin等（2020a）Michael Laskin、Kimin Lee、Adam Stooke、Lerrel Pinto、Pieter Abbeel和Aravind
    Srinivas。增强数据下的强化学习。发表于*神经信息处理系统会议（NeurIPS）*，2020年。
- en: 'Laskin et al. (2020b) Michael Laskin, Aravind Srinivas, and Pieter Abbeel.
    CURL: contrastive unsupervised representations for reinforcement learning. In
    *International Conference on Machine Learning (ICML)*, 2020b.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laskin等（2020b）Michael Laskin、Aravind Srinivas和Pieter Abbeel。CURL：用于强化学习的对比无监督表征。发表于*国际机器学习会议（ICML）*，2020年。
- en: Lecarpentier et al. (2021) Erwan Lecarpentier, David Abel, Kavosh Asadi, Yuu
    Jinnai, Emmanuel Rachelson, and Michael L. Littman. Lipschitz lifelong RL. *AAAI
    Conference on Artificial Intelligence, AAAI*, 2021.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lecarpentier等（2021）Erwan Lecarpentier、David Abel、Kavosh Asadi、Yuu Jinnai、Emmanuel
    Rachelson和Michael L. Littman。Lipschitz终身强化学习。*AAAI人工智能会议，AAAI*，2021年。
- en: 'Lee et al. (2020) Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network
    randomization: A simple technique for generalization in deep reinforcement learning.
    In *International Conference on Learning Representations (ICLR)*, 2020.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee等（2020）Kimin Lee、Kibok Lee、Jinwoo Shin和Honglak Lee。网络随机化：深度强化学习中的一种通用化简单技术。发表于*国际学习表征会议（ICLR）*，2020年。
- en: 'Lee et al. (2021) Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel.
    SUNRISE: A simple unified framework for ensemble learning in deep reinforcement
    learning. In *International Conference on Machine Learning (ICML)*, 2021.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee等（2021）Kimin Lee、Michael Laskin、Aravind Srinivas和Pieter Abbeel。SUNRISE：深度强化学习中的简单统一集成学习框架。发表于*国际机器学习会议（ICML）*，2021年。
- en: Lin et al. (2017) Yen-Chen Lin, Hong Zhang-Wei, Yuan-Hong Liao, Meng-Li Shih,
    ing-Yu Liu, and Min Sun. Tactics of adversarial attack on DRL agents. *IJCAI*,
    2017.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2017）Yen-Chen Lin、Hong Zhang-Wei、Yuan-Hong Liao、Meng-Li Shih、Ing-Yu Liu和Min
    Sun。对抗攻击DRL代理的战术。*IJCAI*，2017年。
- en: Liu et al. (2021) Zhuang Liu, Xuanlin Li, and Trevor Darrell. Regularization
    matters in policy optimization - an empirical study on continuous control. In
    *International Conference on Learning Representations (ICLR)*, 2021.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2021）Zhuang Liu、Xuanlin Li和Trevor Darrell。政策优化中的正则化问题 - 对连续控制的实证研究。发表于*国际学习表征会议（ICLR）*，2021年。
- en: Malik et al. (2021) Dhruv Malik, Yuanzhi Li, and Pradeep Ravikumar. When is
    generalizable reinforcement learning tractable? In *Conference on Neural Information
    Processing Systems (NeurIPS)*, 2021.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malik等（2021）Dhruv Malik、Yuanzhi Li和Pradeep Ravikumar。何时强化学习是可泛化的？发表于*神经信息处理系统会议（NeurIPS）*，2021年。
- en: Mankowitz et al. (2023) Daniel J. Mankowitz, Andrea Michi, Anton Zhernov, Marco
    Gelmi, Marco Selvi, Cosmin Paduraru, Edouard Leurent, Shariq Iqbal, Jean-Baptiste
    Lespiau, Alex Ahern, Thomas Köppe, Kevin Millikin, Stephen Gaffney, Sophie Elster,
    Jackson Broshear, Chris Gamble, Kieran Milan, Robert Tung, Minjae Hwang, Taylan
    Cemgil, Mohammadamin Barekatain, Yujia Li, Amol Mandhane, Thomas Hubert, Julian
    Schrittwieser, Demis Hassabis, Pushmeet Kohli, Martin A. Riedmiller, Oriol Vinyals,
    and David Silver. Faster sorting algorithms discovered using deep reinforcement
    learning. *Nature*, 618(7964):257–263, 2023.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mankowitz等（2023）Daniel J. Mankowitz、Andrea Michi、Anton Zhernov、Marco Gelmi、Marco
    Selvi、Cosmin Paduraru、Edouard Leurent、Shariq Iqbal、Jean-Baptiste Lespiau、Alex
    Ahern、Thomas Köppe、Kevin Millikin、Stephen Gaffney、Sophie Elster、Jackson Broshear、Chris
    Gamble、Kieran Milan、Robert Tung、Minjae Hwang、Taylan Cemgil、Mohammadamin Barekatain、Yujia
    Li、Amol Mandhane、Thomas Hubert、Julian Schrittwieser、Demis Hassabis、Pushmeet Kohli、Martin
    A. Riedmiller、Oriol Vinyals和David Silver。利用深度强化学习发现的更快排序算法。*自然*，618(7964)：257–263，2023年。
- en: Mnih et al. (2015) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A
    Rusu, Joel Veness, arc G Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland,
    Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Antonoglou, Helen
    King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level
    control through deep reinforcement learning. *Nature*, 518:529–533, 2015.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih等（2015）Volodymyr Mnih、Koray Kavukcuoglu、David Silver、Andrei A Rusu、Joel
    Veness、Marc G Bellemare、Alex Graves、Martin Riedmiller、Andreas Fidjeland、Georg
    Ostrovski、Stig Petersen、Charles Beattie、Amir Sadik、Antonoglou、Helen King、Dharshan
    Kumaran、Daan Wierstra、Shane Legg和Demis Hassabis。通过深度强化学习实现人类水平控制。*自然*，518：529–533，2015年。
- en: Ng & Russell (2000) Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse
    reinforcement learning. In Pat Langley (ed.), *Proceedings of the Seventeenth
    International Conference on Machine Learning (ICML 2000), Stanford University,
    Stanford, CA, USA, June 29 - July 2, 2000*, pp.  663–670, 2000.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng & Russell (2000) Andrew Y. Ng 和 Stuart J. Russell。逆向强化学习的算法。见 Pat Langley
    (编), *第十七届国际机器学习会议（ICML 2000）论文集，斯坦福大学，斯坦福，加州，美国，2000年6月29日 - 7月2日*，第663–670页，2000年。
- en: Oh et al. (2020) Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen
    Xu, Hado van Hasselt, Satinder Singh, and David Silver. Discovering reinforcement
    learning algorithms. In *Conference on Neural Information Processing Systems (NeurIPS)*,
    2020.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh et al. (2020) Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen
    Xu, Hado van Hasselt, Satinder Singh 和 David Silver。发现强化学习算法。见 *神经信息处理系统会议（NeurIPS）*，2020年。
- en: Osband et al. (2016a) Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van
    Roy. Deep exploration via bootstrapped DQN. *Conference on Neural Information
    Processing Systems (NeurIPS)*, 2016a.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osband et al. (2016a) Ian Osband, Charles Blundell, Alexander Pritzel 和 Benjamin
    Van Roy。通过自助DQN进行深度探索。*神经信息处理系统会议（NeurIPS）*，2016a年。
- en: Osband et al. (2016b) Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization
    and exploration via randomized value functions. In *International Conference on
    Machine Learning (ICML)*, 2016b.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osband et al. (2016b) Ian Osband, Benjamin Van Roy 和 Zheng Wen。通过随机化价值函数实现泛化与探索。见
    *国际机器学习会议（ICML）*，2016b年。
- en: Pinto et al. (2017) Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav
    Gupta. Robust adversarial reinforcement learning. *International Conference on
    Machine Learning (ICML)*, 2017.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinto et al. (2017) Lerrel Pinto, James Davidson, Rahul Sukthankar 和 Abhinav
    Gupta。鲁棒的对抗性强化学习。*国际机器学习会议（ICML）*，2017年。
- en: Raileanu & Fergus (2021) Roberta Raileanu and Rob Fergus. Decoupling value and
    policy for generalization in reinforcement learning. In *International Conference
    on Machine Learning (ICML)*, 2021.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raileanu & Fergus (2021) Roberta Raileanu 和 Rob Fergus。强化学习中的价值与策略解耦以实现泛化。见
    *国际机器学习会议（ICML）*，2021年。
- en: Schrittwieser et al. (2020) Julian Schrittwieser, Ioannis Antonoglou, Thomas
    Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart,
    Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. Mastering
    atari, go, chess and shogi by planning with a learned model. *Nat.*, 588(7839):604–609,
    2020.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schrittwieser et al. (2020) Julian Schrittwieser, Ioannis Antonoglou, Thomas
    Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart,
    Demis Hassabis, Thore Graepel, Timothy P. Lillicrap 和 David Silver。通过规划和学习的模型掌握Atari、围棋、国际象棋和将棋。*自然*，588(7839):604–609，2020年。
- en: Schulman et al. (2015) John Schulman, Sergey Levine, Philipp Moritz, Michael I.
    Jordan, and Pieter Abbeel. Trust region policy optimization. *CoRR*, 2015.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al. (2015) John Schulman, Sergey Levine, Philipp Moritz, Michael
    I. Jordan 和 Pieter Abbeel。信任区域策略优化。*计算机研究报告*，2015年。
- en: Silver et al. (2017) David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis
    Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, atthew Lai, Adrian
    Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, van den George
    Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without
    human knowledge. *Nature*, 500:354–359, 2017.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver et al. (2017) David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis
    Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian
    Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George Driessche,
    Thore Graepel 和 Demis Hassabis。无须人类知识的围棋游戏掌握。*自然*，500:354–359，2017年。
- en: Szegedy et al. (2014) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dimutru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of
    neural networks. *International Conference on Learning Representations (ICLR)*,
    2014.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy et al. (2014) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dimutru Erhan, Ian Goodfellow 和 Rob Fergus。神经网络的有趣特性。*学习表征国际会议（ICLR）*，2014年。
- en: Taylor & Stone (2007) Matthew E. Taylor and Peter Stone. Cross-domain transfer
    for RL. In *International Conference on Machine Learning (ICML)*, 2007.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taylor & Stone (2007) Matthew E. Taylor 和 Peter Stone。强化学习的跨领域迁移。见 *国际机器学习会议（ICML）*，2007年。
- en: Thrun & Schwartz (1993) Sebastian Thrun and Anton Schwartz. Issues in using
    function approximation for reinforcement learning. *In Fourth Connectionist Models
    Summer School*, 1993.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thrun & Schwartz (1993) Sebastian Thrun 和 Anton Schwartz。使用函数逼近进行强化学习的问题。*第四届连接主义模型暑期学校*，1993年。
- en: Tirinzoni et al. (2018) Andrea Tirinzoni, Rafael Rodriguez Sanchez, and Marcello
    Restelli. Transfer of value functions via variational methods. *Conference on
    Neural Information Processing Systems (NeurIPS)*, 2018.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tirinzoni et al. (2018) Andrea Tirinzoni, Rafael Rodriguez Sanchez 和 Marcello
    Restelli。通过变分方法进行价值函数的迁移。*神经信息处理系统会议（NeurIPS）*，2018年。
- en: van Hasselt (2010) Hado van Hasselt. Double q-learning. In *Conference on Neural
    Information Processing Systems (NeurIPS)*, 2010.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Hasselt（2010）Hado van Hasselt。双重 Q 学习。在 *神经信息处理系统大会（NeurIPS）*，2010。
- en: Veeriah et al. (2021) Vivek Veeriah, Tom Zahavy, Matteo Hessel, Zhongwen Xu,
    Junhyuk Oh, Iurii Kemaev, Hado van Hasselt, David Silver, and Satinder Singh.
    Discovery of options via meta-learned subgoals. In *Conference on Neural Information
    Processing Systems (NeurIPS)*, 2021.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veeriah 等（2021）Vivek Veeriah、Tom Zahavy、Matteo Hessel、Zhongwen Xu、Junhyuk Oh、Iurii
    Kemaev、Hado van Hasselt、David Silver 和 Satinder Singh。通过元学习子目标发现选项。在 *神经信息处理系统大会（NeurIPS）*，2021。
- en: 'Vieillard et al. (2020a) Nino Vieillard, Tadashi Kozuno, Olivier Pietquin,
    Rémi Munos, and Matthieu Geist. Leverage the average: an analysis of KL regularization
    in reinforcement learning. In *Conference on Neural Information Processing Systems
    (NeurIPS)*, 2020a.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vieillard 等（2020a）Nino Vieillard、Tadashi Kozuno、Olivier Pietquin、Rémi Munos
    和 Matthieu Geist。利用平均值：对强化学习中 KL 正则化的分析。在 *神经信息处理系统大会（NeurIPS）*，2020a。
- en: Vieillard et al. (2020b) Nino Vieillard, Olivier Pietquin, and Matthieu Geist.
    Munchausen RL. In *Conference on Neural Information Processing Systems (NeurIPS)*,
    2020b.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vieillard 等（2020b）Nino Vieillard、Olivier Pietquin 和 Matthieu Geist。Munchausen
    强化学习。在 *神经信息处理系统大会（NeurIPS）*，2020b。
- en: Vinyals et al. (2019) Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki,
    Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell,
    Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka,
    Aja Huang, L. Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha
    Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury
    Sulsky, James Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff,
    Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith,
    Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps,
    and David Silver. Grandmaster level in starcraft II using multi-agent reinforcement
    learning. *Nature*, pp.  1–5, 2019.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 等（2019）Oriol Vinyals、Igor Babuschkin、Wojciech M. Czarnecki、Michaël Mathieu、Andrew
    Dudzik、Junyoung Chung、David H. Choi、Richard Powell、Timo Ewalds、Petko Georgiev、Junhyuk
    Oh、Dan Horgan、Manuel Kroiss、Ivo Danihelka、Aja Huang、L. Sifre、Trevor Cai、John P.
    Agapiou、Max Jaderberg、Alexander Sasha Vezhnevets、Rémi Leblond、Tobias Pohlen、Valentin
    Dalibard、David Budden、Yury Sulsky、James Molloy、Tom Le Paine、Caglar Gulcehre、Ziyun
    Wang、Tobias Pfaff、Yuhuai Wu、Roman Ring、Dani Yogatama、Dario Wünsch、Katrina McKinney、Oliver
    Smith、Tom Schaul、Timothy P. Lillicrap、Koray Kavukcuoglu、Demis Hassabis、Chris Apps
    和 David Silver。利用多智能体强化学习在《星际争霸 II》中达到大宗师级别。*自然*，第 1–5 页，2019。
- en: Wang et al. (2020) Kaixin Wang, Bingyi Kang, Jie Shao, and Jiashi Feng. Improving
    generalization in RL with mixture regularization. In *Conference on Neural Information
    Processing Systems (NeurIPS)*, 2020.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020）Kaixin Wang、Bingyi Kang、Jie Shao 和 Jiashi Feng。在 RL 中通过混合正则化改善泛化。在
    *神经信息处理系统大会（NeurIPS）*，2020。
- en: Wang et al. (2016) Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc
    Lanctot, and Nando. De Freitas. Dueling network architectures for deep reinforcement
    learning. *International Conference on Machine Learning (ICML)*, pp. 1995–2003,
    2016.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2016）Ziyu Wang、Tom Schaul、Matteo Hessel、Hado Van Hasselt、Marc Lanctot
    和 Nando De Freitas。用于深度强化学习的对抗网络架构。*国际机器学习大会（ICML）*，第 1995–2003 页，2016。
- en: Watkins (1989) Chris Watkins. Learning from delayed rewards. In *PhD thesis,
    Cambridge*, 1989.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watkins（1989）Chris Watkins。学习延迟奖励。在 *剑桥大学博士论文*，1989。
- en: 'Wolczyk et al. (2021) Maciej Wolczyk, Michal Zajac, Razvan Pascanu, Lukasz
    Kucinski, and Piotr Milos. Continual world: A robotic benchmark for continual
    reinforcement learning. *Conference on Neural Information Processing Systems (NeurIPS)*,
    2021.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolczyk 等（2021）Maciej Wolczyk、Michal Zajac、Razvan Pascanu、Lukasz Kucinski 和
    Piotr Milos。持续世界：一个用于持续强化学习的机器人基准。*神经信息处理系统大会（NeurIPS）*，2021。
- en: Xu et al. (2020) Zhongwen Xu, Hado Philip van Hasselt, Matteo Hessel, Junhyuk
    Oh, Satinder Singh, and David Silver. Meta-gradient reinforcement learning with
    an objective discovered online. In *Conference on Neural Information Processing
    Systems (NeurIPS)*, 2020.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2020）中英文 Xu、Hado Philip van Hasselt、Matteo Hessel、Junhyuk Oh、Satinder Singh
    和 David Silver。通过在线发现目标的元梯度强化学习。在 *神经信息处理系统大会（NeurIPS）*，2020。
- en: 'Yarats et al. (2021) Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation
    is all you need: Regularizing deep reinforcement learning from pixels. In *International
    Conference on Learning Representations (ICLR)*, 2021.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yarats 等（2021）Denis Yarats、Ilya Kostrikov 和 Rob Fergus。图像增强即是你所需：从像素中正则化深度强化学习。在
    *学习表示国际会议（ICLR）*，2021。
