- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:41:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:41:53'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2302.08210] A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2302.08210] 深度学习的几何优化调查：从欧几里得空间到黎曼流形'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.08210](https://ar5iv.labs.arxiv.org/html/2302.08210)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2302.08210](https://ar5iv.labs.arxiv.org/html/2302.08210)
- en: 'A Survey of Geometric Optimization for Deep Learning: From Euclidean Space
    to Riemannian Manifold'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的几何优化调查：从欧几里得空间到黎曼流形
- en: Yanhong Fei, Xian Wei, Yingjie Liu, Zhengyu Li, Mingsong Chen
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 颜洪飞、魏贤、刘英杰、李正宇、陈铭松
- en: Software Engineering Institute, East China Normal University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程研究所，华东师范大学
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Although Deep Learning (DL) has achieved success in complex Artificial Intelligence
    (AI) tasks, it suffers from various notorious problems (e.g., feature redundancy,
    and vanishing or exploding gradients), since updating parameters in Euclidean
    space cannot fully exploit the geometric structure of the solution space. As a
    promising alternative solution, Riemannian-based DL uses geometric optimization
    to update parameters on Riemannian manifolds and can leverage the underlying geometric
    information. Accordingly, this article presents a comprehensive survey of applying
    geometric optimization in DL. At first, this article introduces the basic procedure
    of the geometric optimization, including various geometric optimizers and some
    concepts of Riemannian manifold. Subsequently, this article investigates the application
    of geometric optimization in different DL networks in various AI tasks, e.g.,
    convolution neural network, recurrent neural network, transfer learning, and optimal
    transport. Additionally, typical public toolboxes that implement optimization
    on manifold are also discussed. Finally, this article makes a performance comparison
    between different deep geometric optimization methods under image recognition
    scenarios.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习（DL）在复杂的人工智能（AI）任务中取得了成功，但由于在欧几里得空间中更新参数无法充分利用解空间的几何结构，它仍然面临各种恶名昭彰的问题（例如特征冗余以及梯度消失或爆炸）。作为一种有前景的替代解决方案，基于黎曼几何的深度学习利用几何优化在黎曼流形上更新参数，并能够利用底层几何信息。因此，本文提供了对在深度学习中应用几何优化的全面调查。首先，本文介绍了几何优化的基本过程，包括各种几何优化器和一些黎曼流形的概念。随后，本文调查了几何优化在各种人工智能任务中的不同深度学习网络中的应用，例如卷积神经网络、递归神经网络、迁移学习和最优传输。此外，本文还讨论了实现流形优化的典型公共工具箱。最后，本文在图像识别场景下对不同的深度几何优化方法进行了性能比较。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: With increasing computing power, deep neural networks optimized in Euclidean
    space have achieved remarkable success from computer vision to natural language
    processing (e.g., autonomous driving and protein structure prediction) [[1](#bib.bib1),
    [2](#bib.bib2)]. However, to fully exploit the valuable information hidden in
    the data, most deep learning models tend to increase the capacity of their networks,
    either by widening the existing layers or by adding more layers [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]. For example, models often contain hundreds of
    convolution and pooling layers with various activation functions and multiple
    fully connected layers, producing millions or billions of parameters during training.
    These massive parameters associated with complex model architectures challenge
    the optimization of deep learning networks. As an alternative paradigm, optimization
    on the Riemannian manifold exploits hidden valuable information by utilizing geometric
    properties of parameters, rather than increasing the network capacity. Therefore,
    geometric optimization can alleviate over-parameterization and feature redundancy
    problems. For example, deep learning models trained on the orthogonal manifold
    have less correlated parameters, making features much less redundant [[6](#bib.bib6)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算能力的提升，在欧几里得空间中优化的深度神经网络在从计算机视觉到自然语言处理（例如，自主驾驶和蛋白质结构预测）方面取得了显著成功[[1](#bib.bib1),
    [2](#bib.bib2)]。然而，为了充分利用数据中隐藏的有价值信息，大多数深度学习模型倾向于增加网络的容量，通常是通过扩展现有层或添加更多层[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]。例如，模型通常包含数百个卷积层和池化层，具有各种激活函数以及多个全连接层，在训练过程中产生数百万或数十亿的参数。这些与复杂模型架构相关的大量参数对深度学习网络的优化提出了挑战。作为一种替代范式，黎曼流形上的优化通过利用参数的几何特性来发掘隐藏的有价值信息，而不是增加网络容量。因此，几何优化可以缓解过度参数化和特征冗余问题。例如，在正交流形上训练的深度学习模型具有较少相关的参数，使得特征冗余大大减少[[6](#bib.bib6)]。
- en: The optimization objective in most deep learning methods can be formulated as
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习方法中的优化目标可以被表述为
- en: '|  |  | $\displaystyle\operatorname*{argmin}_{\boldsymbol{\theta}\in\mathcal{D}}\
    f_{\boldsymbol{\theta}}(\mathbf{x}),\ s.t.\ C(\boldsymbol{\theta}),$ |  | (1)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\operatorname*{argmin}_{\boldsymbol{\theta}\in\mathcal{D}}\
    f_{\boldsymbol{\theta}}(\mathbf{x}),\ s.t.\ C(\boldsymbol{\theta}),$ |  | (1)
    |'
- en: 'where $\mathcal{D}$ denotes the predefined admissible search space, $f$ denotes
    a real-value optimization function (e.g., loss function) to be minimized by trainable
    parameters $\boldsymbol{\theta}$, and $C(\boldsymbol{\theta})$ represents constraints
    (e.g., orthogonality [[6](#bib.bib6)] and unit row sums [[7](#bib.bib7)]) that
    $\boldsymbol{\theta}$ is subject to. Most deep learning methods define the search
    space $\mathcal{D}$ as the Euclidean space. However, parameters satisfying constraints
    are on the manifold, which is a low dimensional subspace and only occupies a small
    part of Euclidean space. Therefore, to eliminate constraints and reduce parameters,
    geometric optimization [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]
    narrows the search space from Euclidean space to a smooth manifold. Hence, Equation ([1](#S1.E1
    "In 1 Introduction ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) is transformed into a differentiable
    optimization function $f:\mathcal{M}\rightarrow\mathcal{R}$ on a Riemannian manifold,
    i.e.,'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$\mathcal{D}$ 表示预定义的可接受搜索空间，$f$ 表示一个需要通过可训练参数 $\boldsymbol{\theta}$ 最小化的实值优化函数（例如，损失函数），$C(\boldsymbol{\theta})$
    表示 $\boldsymbol{\theta}$ 所满足的约束（例如，正交性[[6](#bib.bib6)]和单位行和[[7](#bib.bib7)]）。大多数深度学习方法将搜索空间
    $\mathcal{D}$ 定义为欧几里得空间。然而，满足约束的参数在流形上，流形是一个低维子空间，仅占据欧几里得空间的一小部分。因此，为了消除约束并减少参数，几何优化[[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)] 将搜索空间从欧几里得空间缩小到一个光滑的流形。因此，方程式（[1](#S1.E1
    "In 1 Introduction ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")）被转化为在黎曼流形上的可微优化函数 $f:\mathcal{M}\rightarrow\mathcal{R}$，即，'
- en: '|  | $\operatorname*{argmin}_{\theta\in\mathcal{M},M=\{\theta&#124;C(\theta)\}}\
    f_{\boldsymbol{\theta}}(\mathbf{x}).$ |  | (2) |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname*{argmin}_{\theta\in\mathcal{M},M=\{\theta\mid C(\theta)\}}\
    f_{\boldsymbol{\theta}}(\mathbf{x}).$ |  | (2) |'
- en: 'As shown in Equation ([2](#S1.E2 "In 1 Introduction ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")),
    selecting a manifold composed of points that meet constraints $C(\boldsymbol{\theta})$
    in Equation ([1](#S1.E1 "In 1 Introduction ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")), a large class
    of constrained deep learning problems in Euclidean space can be optimized as unconstrained
    and convex ones on the Riemannian manifold [[10](#bib.bib10)], which helps ensure
    the convergence. For example, a typical dimension reduction problem can be defined
    as follows'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如方程 ([2](#S1.E2 "在 1 介绍 ‣ 深度学习几何优化的综述：从欧几里得空间到黎曼流形")) 所示，选择一个由满足方程 ([1](#S1.E1
    "在 1 介绍 ‣ 深度学习几何优化的综述：从欧几里得空间到黎曼流形")) 中约束 $C(\boldsymbol{\theta})$ 的点组成的流形，可以将欧几里得空间中大量受限的深度学习问题优化为黎曼流形上的无约束且凸的问题
    [[10](#bib.bib10)]，这有助于确保收敛性。例如，一个典型的降维问题可以定义如下
- en: '|  |  | $\displaystyle\operatorname*{argmin}_{\boldsymbol{\theta}\in{E}}f_{\boldsymbol{\theta}}(\mathbf{x})=-tr(\boldsymbol{\theta}^{T}x^{T}x\boldsymbol{\theta}),\
    s.t.\ \ \boldsymbol{\theta}^{T}\boldsymbol{\theta}=I,$ |  | (3) |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\operatorname*{argmin}_{\boldsymbol{\theta}\in{E}}f_{\boldsymbol{\theta}}(\mathbf{x})=-tr(\boldsymbol{\theta}^{T}x^{T}x\boldsymbol{\theta}),\
    s.t.\ \ \boldsymbol{\theta}^{T}\boldsymbol{\theta}=I,$ |  | (3) |'
- en: 'where $E$ represents the Euclidean space, $I$ represents the identity matrix
    and parameters $\boldsymbol{\theta}$ are constrained to be orthogonal. Since all
    matrices that satisfy orthogonality compose of the Stiefel manifold , Equation ([3](#S1.E3
    "In 1 Introduction ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) can be treated as an unconstrained problem
    on the Stiefel manifold, which is a kind of Riemannian manifold.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $E$ 代表欧几里得空间，$I$ 代表单位矩阵，参数 $\boldsymbol{\theta}$ 被约束为正交。由于所有满足正交性的矩阵组成 Stiefel
    流形，方程 ([3](#S1.E3 "在 1 介绍 ‣ 深度学习几何优化的综述：从欧几里得空间到黎曼流形")) 可以视作 Stiefel 流形上的无约束问题，这是一种黎曼流形。
- en: '![Refer to caption](img/80fcad216bea951de0934902f9a7c3c0.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/80fcad216bea951de0934902f9a7c3c0.png)'
- en: 'Figure 1: Comparison between geometric and Euclidean optimization path. The
    blue center point is the global optimum. The red curve describes the Riemannian
    optimization path converging upon the global optimal goal, always along a curve
    on manifolds. In contrast, the green dotted line indicates the Euclidean gradient
    descent path towards the optimal goal, taking the risk of moving off the manifold.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：几何优化与欧几里得优化路径的比较。蓝色中心点是全局最优点。红色曲线描述了黎曼优化路径收敛到全局最优目标，总是沿着流形上的曲线。相比之下，绿色虚线表示欧几里得梯度下降路径朝向最优目标，存在偏离流形的风险。
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") depicts the intuitive
    paradigm for optimization processes in arbitrary Euclidean space and on Riemannian
    manifolds. Traditional optimization methods in Euclidean space may ignore the
    advantages of applying geometric optimization strategies. For example, the latter
    can obtain richer geometric information from different unique manifold structures
    and convert constrained optimization problems into unconstrained problems. Moreover,
    geometric optimization can achieve faster convergence speed and mitigate gradient
    explosion and disappearance problems in deep learning, which will be detailed
    in Section [4](#S4 "4 Applications in Deep Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold"). Due to the above
    potential, geometric optimization has been applied to various deep neural networks
    in recent years, such as convolution neural network (CNN) [[12](#bib.bib12), [6](#bib.bib6),
    [13](#bib.bib13)], recurrent neural network (RNN) [[14](#bib.bib14)] and vision
    transformer (ViT) [[15](#bib.bib15)]. For instance, orthogonal parameterization
    is used in CNN to reduce filter similarities, make spectra uniform [[16](#bib.bib16)],
    and stabilize the activation distribution in different network layers [[17](#bib.bib17)].
    However, there is a lack of comprehensive surveys focused on deep learning methods
    applying geometric optimization. To explore benefits of geometric optimization,
    this article aims to give an overall review of recent advances on applying geometric
    optimization in deep learning.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") 描绘了任意欧几里得空间和黎曼流形上优化过程的直观范式。传统的欧几里得空间优化方法可能忽视了应用几何优化策略的优势。例如，后者可以从不同的独特流形结构中获得更丰富的几何信息，并将约束优化问题转化为无约束问题。此外，几何优化可以实现更快的收敛速度，减轻深度学习中的梯度爆炸和消失问题，详细内容将在第[4](#S4
    "4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")节中介绍。由于上述潜力，近年来几何优化已被应用于各种深度神经网络，如卷积神经网络（CNN）[[12](#bib.bib12),
    [6](#bib.bib6), [13](#bib.bib13)]，递归神经网络（RNN）[[14](#bib.bib14)]和视觉变换器（ViT）[[15](#bib.bib15)]。例如，正交参数化被用于CNN中以减少滤波器相似性，使光谱均匀[[16](#bib.bib16)]，并在不同网络层中稳定激活分布[[17](#bib.bib17)]。然而，缺乏专注于应用几何优化的深度学习方法的综合调查。为了探讨几何优化的好处，本文旨在对近期在深度学习中应用几何优化的进展进行全面回顾。'
- en: '![Refer to caption](img/e1f28b6c67d65b8163e033c412022ca7.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e1f28b6c67d65b8163e033c412022ca7.png)'
- en: 'Figure 2: An overview of the central idea of this article.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 本文中心思想的概述。'
- en: ¹¹1*Notations* In this work, vectors and matrices are denoted by bold lower
    case letters and upper case ones, respectively. Let $\mathbb{R}$ be the set of
    real numbers, $\mathbb{C}$ be the set of complex numbers and $\nabla f$ denotes
    the Euclidean gradient.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹1*符号* 在这项工作中，向量和矩阵分别用粗体小写字母和大写字母表示。设$\mathbb{R}$为实数集，$\mathbb{C}$为复数集，$\nabla
    f$表示欧几里得梯度。
- en: 'Overview and article organization. In this article, a survey of geometric optimization
    techniques for deep learning is presented, including the theory and applications
    of geometric optimization. Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold") displays an overview of the central idea of this article. Since the
    optimization theory is unified and model-independent, this article illustrates
    the theory first, including various geometric gradient descent optimizers (Section [2](#S2
    "2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")). The motivation and technique
    of applying geometric optimization in classical machine learning is different
    from that of deep learning. Therefore, this article reviews how to apply geometric
    optimization to shallow learning (Section [3](#S3 "3 Applications in Classical
    Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) and deep learning (Section [4](#S4 "4
    Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) separately. In particular, this
    article investigates representative manifold optimization toolboxes (Section [5](#S5
    "5 Toolbox ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")), followed by performance comparisons of different
    geometric deep learning methods on image recognition tasks (Section [6](#S6 "6
    Performance Evaluation ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")). Finally, we conclude the article
    and highlight future challenges and research trend (Section [7](#S7 "7 Conclusions
    and Future Work ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '概述与文章组织。在本文中，介绍了深度学习几何优化技术的综述，包括几何优化的理论和应用。图[2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold")展示了本文的核心思想。由于优化理论是统一的且与模型无关，本文首先阐述了理论，包括各种几何梯度下降优化器（第[2](#S2
    "2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")节）。几何优化在经典机器学习中的动机和技术与在深度学习中的有所不同。因此，本文分别回顾了如何将几何优化应用于浅层学习（第[3](#S3
    "3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")节）和深度学习（第[4](#S4
    "4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")节）。特别地，本文调查了具有代表性的流形优化工具箱（第[5](#S5
    "5 Toolbox ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")节），随后比较了不同几何深度学习方法在图像识别任务上的性能（第[6](#S6 "6 Performance
    Evaluation ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")节）。最后，我们总结了本文并突出未来的挑战和研究趋势（第[7](#S7 "7 Conclusions
    and Future Work ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")节）。'
- en: 2 Geometric Optimization Theory
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 几何优化理论
- en: The essence of an optimization problem is to find the maximum or minimum value
    of a cost function. An unconstrained optimization problem can use conventional
    optimization methods (e.g., steepest descent method, conjugate gradient method,
    and Newton method) to find an optimal solution [[18](#bib.bib18)]. However, a
    broad range of optimization problems that occur in computer vision tasks are known
    as constrained optimization problems. In such a case, finding a closed form for
    the cost function is difficult. To use the aforementioned conventional optimization
    techniques, the constrained problem can be transformed into an unconstrained form
    by using the method of Lagrange multipliers or using a barrier penalty function
    [[18](#bib.bib18)]. However, the above methods hardly take advantage of underlying
    manifold structures. They merely treat the constrained problem as a “black box”
    and solve them by using algebraic manipulation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 优化问题的本质是寻找成本函数的最大值或最小值。无约束优化问题可以使用传统的优化方法（例如，最速下降法、共轭梯度法和牛顿法）来找到最优解[[18](#bib.bib18)]。然而，在计算机视觉任务中发生的广泛优化问题被称为有约束优化问题。在这种情况下，找到成本函数的封闭形式是困难的。为了使用上述传统的优化技术，可以通过使用拉格朗日乘子法或障碍惩罚函数将有约束问题转化为无约束形式[[18](#bib.bib18)]。然而，上述方法几乎无法利用潜在的流形结构。它们仅仅将有约束问题视为“黑箱”，并通过代数操作进行求解。
- en: As an alternative solution, geometric optimization methods are developed to
    exploit intrinsic geometric structures of objective function parameters. By utilizing
    the underlying geometry of a cost function, geometric optimization methods can
    narrow the search space of constrained optimization problems from Euclidean space
    to smooth Riemannian manifolds. Riemannian manifold has a differentiable structure
    and is equipped with smooth inner product and Riemannian gradients, which are
    different from Euclidean space and lay the foundation for geometric optimization.
    Based on the Riemannian inner product and Riemannian gradients, a broad spectrum
    of conventional optimization techniques in Euclidean space can have their counterparts
    on smooth manifolds [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [8](#bib.bib8)],
    including the steepest descent method [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)],
    conjugate gradient descent method [[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)],
    trust-region method [[8](#bib.bib8), [25](#bib.bib25)] and Newton’s method [[26](#bib.bib26),
    [8](#bib.bib8)]. Therefore, geometric optimization methods can use Riemannian
    optimizers to find an optimal solution for objective functions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种替代方案，几何优化方法被开发出来以利用目标函数参数的内在几何结构。通过利用成本函数的基础几何结构，几何优化方法可以将约束优化问题的搜索空间从欧几里得空间缩小到平滑的黎曼流形。黎曼流形具有可微分的结构，并配备了平滑的内积和黎曼梯度，这些都不同于欧几里得空间，并为几何优化奠定了基础。基于黎曼内积和黎曼梯度，广泛的欧几里得空间中的传统优化技术可以在平滑的流形上找到其对应的技术[[19](#bib.bib19)、[20](#bib.bib20)、[21](#bib.bib21)、[8](#bib.bib8)]，包括最速下降法[[19](#bib.bib19)、[20](#bib.bib20)、[21](#bib.bib21)]、共轭梯度下降法[[22](#bib.bib22)、[23](#bib.bib23)、[24](#bib.bib24)]、信赖域方法[[8](#bib.bib8)、[25](#bib.bib25)]和牛顿法[[26](#bib.bib26)、[8](#bib.bib8)]。因此，几何优化方法可以使用黎曼优化器来寻找目标函数的最优解。
- en: 'In the following subsections, this article first illustrates the model-independent
    optimization process on the Riemannian manifold, covering basic concepts related
    to geometric optimization (Section [2.1](#S2.SS1 "2.1 Geometric Optimization Process
    on Manifolds ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")). Next, this
    article briefly introduces various Riemannian gradient descent optimizers implementing
    geometric optimization, which is a counterpart of optimizers in Euclidean space
    (Section [2.2](#S2.SS2 "2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")). Finally, this article presents a series of manifold
    structures that are commonly used in deep geometric learning methods (Section [2.3](#S2.SS3
    "2.3 Manifold Examples ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '在接下来的各节中，本文首先阐述了黎曼流形上的模型无关优化过程，涵盖与几何优化相关的基本概念（第[2.1节](#S2.SS1 "2.1 Geometric
    Optimization Process on Manifolds ‣ 2 Geometric Optimization Theory ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")）。接着，本文简要介绍了实现几何优化的各种黎曼梯度下降优化器，这些优化器是欧几里得空间中优化器的对应物（第[2.2节](#S2.SS2
    "2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization Theory ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")）。最后，本文介绍了一系列在深度几何学习方法中常用的流形结构（第[2.3节](#S2.SS3 "2.3 Manifold Examples
    ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")）。'
- en: '<svg   height="181.84" overflow="visible" version="1.1" width="300.69"><g transform="translate(0,181.84)
    matrix(1 0 0 -1 0 0) translate(112.31,0) translate(0,50.3)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 18.46 -35.73)" fill="#000000"
    stroke="#000000"><foreignobject width="12.68" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\mathcal{M}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -20.13 8.7)" fill="#000000" stroke="#000000"><foreignobject width="20.42"
    height="12.36" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\Theta}^{(j)}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -4.72 -9.19)" fill="#000000" stroke="#000000"><foreignobject
    width="34.24" height="12.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$T_{\bm{\Theta}^{(j)}}\mathcal{M}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.45 58.23)" fill="#000000" stroke="#000000"><foreignobject
    width="8.3" height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{H}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -2.82 119.18)" fill="#000000" stroke="#000000"><foreignobject
    width="104.84" height="15.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Update:
    $\bm{\Theta}^{(j)}+\gamma\mathbf{H}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 65.85 98.24)" fill="#000000" stroke="#000000"><foreignobject width="122.53"
    height="13.86" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Retraction:
    $\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -78.01 -37.67)" fill="#000000" stroke="#000000"><foreignobject width="56.8"
    height="12.66" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\operatorname{grad}{f}(\bm{\Theta}^{(j)})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -96.65 60.31)" fill="#000000" stroke="#000000"><foreignobject
    width="44.47" height="12.66" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\nabla{f}(\bm{\Theta}^{(j)})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 44.48 41.89)" fill="#000000" stroke="#000000"><foreignobject
    width="42.49" height="11.08" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 115.06 31.02)" fill="#000000" stroke="#000000"><foreignobject
    width="30.32" height="12.36" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\Theta}^{(j+1)}$</foreignobject></g></g></g></svg>'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg height="181.84" overflow="visible" version="1.1" width="300.69"><g transform="translate(0,181.84)
    matrix(1 0 0 -1 0 0) translate(112.31,0) translate(0,50.3)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 18.46 -35.73)" fill="#000000"
    stroke="#000000"><foreignobject width="12.68" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\mathcal{M}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -20.13 8.7)" fill="#000000" stroke="#000000"><foreignobject width="20.42"
    height="12.36" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\Theta}^{(j)}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -4.72 -9.19)" fill="#000000" stroke="#000000"><foreignobject
    width="34.24" height="12.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$T_{\bm{\Theta}^{(j)}}\mathcal{M}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.45 58.23)" fill="#000000" stroke="#000000"><foreignobject
    width="8.3" height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{H}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -2.82 119.18)" fill="#000000" stroke="#000000"><foreignobject
    width="104.84" height="15.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Update:
    $\bm{\Theta}^{(j)}+\gamma\mathbf{H}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 65.85 98.24)" fill="#000000" stroke="#000000"><foreignobject width="122.53"
    height="13.86" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Retraction:
    $\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -78.01 -37.67)" fill="#000000" stroke="#000000"><foreignobject width="56.8"
    height="12.66" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\operatorname{grad}{f}(\bm{\Theta}^{(j)})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -96.65 60.31)" fill="#000000" stroke="#000000"><foreignobject
    width="44.47" height="12.66" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\nabla{f}(\bm{\Theta}^{(j)})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 44.48 41.89)" fill="#000000" stroke="#000000"><foreignobject
    width="42.49" height="11.08" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 115.06 31.02)" fill="#000000" stroke="#000000"><foreignobject
    width="30.32" height="12.36" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\Theta}^{(j+1)}$</foreignobject></g></g></g></svg>'
- en: 'Figure 3: The update process in geometric gradient descent algorithm. It shows
    an update from the point $\bm{\Theta}^{(j)}$ to the point $\bm{\Theta}^{(j+1)}$
    in a search direction $\mathbf{H}\in T_{\bm{\Theta}^{(j)}}\mathcal{M}$ along the
    geodesic curve $\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$. Moreover, it describes
    how to approximate the geodesic $\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$
    by using the retraction $\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：几何梯度下降算法中的更新过程。它展示了从点 $\bm{\Theta}^{(j)}$ 到点 $\bm{\Theta}^{(j+1)}$ 的更新过程，在搜索方向
    $\mathbf{H}\in T_{\bm{\Theta}^{(j)}}\mathcal{M}$ 上沿测地线曲线 $\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$
    进行。此外，它描述了如何通过使用回缩 $\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$ 来近似测地线
    $\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$。
- en: 2.1 Geometric Optimization Process on Manifolds
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 流形上的几何优化过程
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2 Geometric Optimization Theory ‣ A Survey of
    Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    depicts the update process in geometric optimization[[27](#bib.bib27)] through
    the gradient descent example. There are two nearby points $\Theta^{(j)}$ and $\Theta^{(j+1)}$
    on a manifold $\mathcal{M}$ together with the tangent space at $\Theta^{(j)}$
    (refer to the green area in Figure [3](#S2.F3 "Figure 3 ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")). Each point $\Theta$ on the manifold has its corresponding
    tangent space $T_{\Theta}\mathcal{M}$, which is a generalization of the tangent
    plane in Euclidean space and consists of all tangent vectors passing through $\Theta$
    [[28](#bib.bib28)]. Each tangent space has an inner product, which is vital for
    vector metrics such as length and angles. Inner product space further helps induce
    the concept of orthogonality, an extension of vertical in higher dimensions. A
    Riemannian gradient $grad\,f(\Theta)$ for geometric optimization is a tangent
    vector on the tangent space $T_{\Theta}\mathcal{M}$ and points to the direction
    where the cost function on the manifold ascends steepest [[28](#bib.bib28)]. Figure [3](#S2.F3
    "Figure 3 ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") shows that gradient
    $\nabla f(\Theta)$ is computed in the ambient Euclidean space. Since the manifold
    is locally homomorphic to the Euclidean space, $grad\,f(\Theta)$ can be achieved
    by projecting Euclidean gradient $\nabla f(\Theta)$ to the appropriate tangent
    space $T_{\Theta}\mathcal{M}$, i.e.,'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S2.F3 "图 3 ‣ 2 几何优化理论 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形") 通过梯度下降示例描绘了几何优化中的更新过程[[27](#bib.bib27)]。图中展示了流形
    $\mathcal{M}$ 上的两个相邻点 $\Theta^{(j)}$ 和 $\Theta^{(j+1)}` 以及在 $\Theta^{(j)}$ 处的切空间（参见图
    [3](#S2.F3 "图 3 ‣ 2 几何优化理论 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形") 中绿色区域）。流形上的每个点 $\Theta$
    都有其对应的切空间 $T_{\Theta}\mathcal{M}$，它是欧几里得空间中切平面的推广，并且包含了通过 $\Theta$ 的所有切向量 [[28](#bib.bib28)]。每个切空间都有一个内积，这对向量的度量（如长度和角度）至关重要。内积空间进一步帮助引入正交性的概念，这是在更高维度中垂直性的扩展。黎曼梯度
    $grad\,f(\Theta)$ 对于几何优化是切空间 $T_{\Theta}\mathcal{M}$ 上的一个切向量，指向流形上代价函数上升最陡的方向
    [[28](#bib.bib28)]。图 [3](#S2.F3 "图 3 ‣ 2 几何优化理论 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形") 显示了梯度
    $\nabla f(\Theta)$ 是在环境欧几里得空间中计算的。由于流形在局部上同构于欧几里得空间，因此 $grad\,f(\Theta)$ 可以通过将欧几里得梯度
    $\nabla f(\Theta)$ 投影到适当的切空间 $T_{\Theta}\mathcal{M}$ 来实现，即，
- en: '|  | $grad\ f(\Theta)=\Pi_{T_{\Theta}\mathcal{M}}(\nabla f(\Theta)),$ |  |
    (4) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $grad\ f(\Theta)=\Pi_{T_{\Theta}\mathcal{M}}(\nabla f(\Theta)),$ |  |
    (4) |'
- en: where $\Pi$ means the orthogonal projection.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Pi$ 表示正交投影。
- en: 'As a counterpart of Euclidean straight lines, a geodesic is a locally shortest
    path between two points on the manifold. Therefore, reaching the optimal goal
    along a correct geodesic is shortest. Formally, a geodesic $\Gamma_{\bm{\Theta}}(\gamma\mathbf{H})$
    is a smooth curve on the manifold, proceeding from $\Theta$ in the direction of
    tangent vector $\mathbf{H}\in T_{\Theta}\textit{M}$ with a step size of $\gamma\in\mathbb{R^{+}}$
    [[23](#bib.bib23)]. Since each tangent vector is the direction vector of a specific
    geodesic curve, it can uniquely determine a geodesic curve. In particular, the
    geodesic defined by the negative Riemannian gradient reveals the next point in
    the optimization direction. A point can be mapped from the tangent space to the
    manifold through exponential mapping. In practice, to alleviate the high computational
    cost of exponential mapping, retraction operation $\mathfrak{R}_{\bm{\Theta}}(\gamma\mathbf{H})$
    is often used as an approximation [[29](#bib.bib29)]:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作为欧几里得直线的对应物，测地线是在流形上两个点之间的局部最短路径。因此，沿着正确的测地线达到最优目标是最短的。形式上，测地线 $\Gamma_{\bm{\Theta}}(\gamma\mathbf{H})$
    是流形上的光滑曲线，从 $\Theta$ 沿切向量 $\mathbf{H}\in T_{\Theta}\textit{M}$ 的方向前进，步长为 $\gamma\in\mathbb{R^{+}}$
    [[23](#bib.bib23)]。由于每个切向量是特定测地线曲线的方向向量，它可以唯一确定一条测地线曲线。特别是，由负黎曼梯度定义的测地线揭示了优化方向上的下一点。一个点可以通过指数映射从切空间映射到流形上。在实践中，为了减轻指数映射的高计算成本，通常使用回缩操作
    $\mathfrak{R}_{\bm{\Theta}}(\gamma\mathbf{H})$ 作为近似 [[29](#bib.bib29)]：
- en: '|  | $\mathfrak{R}_{\bm{\Theta}}(\gamma\mathbf{H}):T_{p}\mathcal{M}\rightarrow\mathcal{M},\
    \gamma\mathbf{H}\rightarrow\Gamma_{\bm{\Theta}}(\gamma\mathbf{H}),$ |  | (5) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathfrak{R}_{\bm{\Theta}}(\gamma\mathbf{H}):T_{p}\mathcal{M}\rightarrow\mathcal{M},\
    \gamma\mathbf{H}\rightarrow\Gamma_{\bm{\Theta}}(\gamma\mathbf{H}),$ |  | (5) |'
- en: where $\mathbf{H}$ denotes an opposite vector of the Riemannian gradient. Therefore,
    $\mathbf{H}$ points in the direction of the steepest descent of the optimization
    function. As a result, the optimization function will be minimized if parameter
    $\Theta$ is updated along a geodesic curve in the direction of $\mathbf{H}$. In
    summary, with a step size of $\gamma$, the optimizing process from the current
    parameter $\Theta^{(j)}$ to the next parameter $\Theta^{(j+1)}$ can be formulated
    as
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{H}$ 表示黎曼梯度的相反向量。因此，$\mathbf{H}$ 指向优化函数的最陡下降方向。因此，如果参数 $\Theta$ 沿着
    $\mathbf{H}$ 的方向在测地线曲线上更新，优化函数将会最小化。总之，以步长 $\gamma$，从当前参数 $\Theta^{(j)}$ 到下一个参数
    $\Theta^{(j+1)}$ 的优化过程可以表示为
- en: '|  | $\Theta^{(j+1)}=\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})\approx\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})=\mathfrak{R}_{\bm{\Theta}^{(j)}}(-\gamma
    grad\,f(\Theta^{(j)})).$ |  | (6) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Theta^{(j+1)}=\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})\approx\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})=\mathfrak{R}_{\bm{\Theta}^{(j)}}(-\gamma
    grad\,f(\Theta^{(j)})).$ |  | (6) |'
- en: 2.2 Gradient Descent Optimizers
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 梯度下降优化器
- en: Optimization problems defined in Euclidean space can be abstracted as
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在欧几里得空间中定义的优化问题可以抽象化为
- en: '|  | ${\min}\{f_{\boldsymbol{\theta}}(\mathbf{x}):\boldsymbol{\theta}\in\mathbb{E}\},$
    |  | (7) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\min}\{f_{\boldsymbol{\theta}}(\mathbf{x}):\boldsymbol{\theta}\in\mathbb{E}\},$
    |  | (7) |'
- en: 'where $\boldsymbol{\theta}$ are trainable parameters and $E$ means the Euclidean
    space. There are a variety of standard optimizers for Equation ([7](#S2.E7 "In
    2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization Theory ‣ A Survey of
    Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")).
    The gradient descent method is a most basic optimization strategy. It can be improved
    by stochastic gradient descent (SGD), which can accelerate convergence. The other
    two typical variants of the gradient descent method are stochastic gradient descent-momentum
    (SGD-M) and root mean square prop (RMSProp). To solve valley oscillation and saddle
    point stagnation problems that SGD suffers from, SGD-M is developed to maintain
    the inertia of the previous step. According to empirical judgments of different
    parameters, RMSProp can adaptively determine the learning rate of parameters,
    i.e., parameters with low update frequency can have a larger learning rate, while
    parameters with high update frequency can reduce the step size. Let $\boldsymbol{\theta}^{(k)}$
    represent parameters at iteration $k$ and $\boldsymbol{\theta}^{(k+1)}$ represent
    parameters at iteration $k+1$, this section first explains the above Euclidean
    gradient descent optimizers and then shows how to generalize them to the Riemannian
    manifold for geometric optimization ²²2For simplicity, this paper uses ${\nabla}f$
    to denote $\frac{\partial f_{\boldsymbol{\theta}}(x)}{\partial\boldsymbol{\theta}}$
    in Section [2.2](#S2.SS2 "2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")..'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{\theta}$ 是可训练参数，$E$ 表示欧几里得空间。对于方程 ([7](#S2.E7 "在 2.2 梯度下降优化器
    ‣ 2 几何优化理论 ‣ 深度学习几何优化综述：从欧几里得空间到黎曼流形")) 有多种标准优化器。梯度下降法是一种最基本的优化策略。它可以通过随机梯度下降（SGD）进行改进，从而加速收敛。梯度下降法的其他两个典型变体是随机梯度下降-动量（SGD-M）和均方根传播（RMSProp）。为了解决
    SGD 遇到的谷底振荡和鞍点停滞问题，SGD-M 被开发出来以保持前一步的惯性。根据不同参数的经验判断，RMSProp 可以自适应地确定参数的学习率，即更新频率低的参数可以具有较大的学习率，而更新频率高的参数则可以减小步长。让
    $\boldsymbol{\theta}^{(k)}$ 代表迭代 $k$ 时的参数，$\boldsymbol{\theta}^{(k+1)}$ 代表迭代 $k+1$
    时的参数，本节首先解释上述欧几里得梯度下降优化器，然后展示如何将其推广到黎曼流形以进行几何优化。²²2为简便起见，本文在 [2.2](#S2.SS2 "2.2
    梯度下降优化器 ‣ 2 几何优化理论 ‣ 深度学习几何优化综述：从欧几里得空间到黎曼流形") 节中使用 ${\nabla}f$ 来表示 $\frac{\partial
    f_{\boldsymbol{\theta}}(x)}{\partial\boldsymbol{\theta}}$。
- en: Gradient Descent. The gradient descent method takes the following form
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法。梯度下降法的形式如下
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-{\lambda}{\nabla}f(\boldsymbol{\theta}^{(k)}),$
    |  | (8) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-{\lambda}{\nabla}f(\boldsymbol{\theta}^{(k)}),$
    |  | (8) |'
- en: where $\lambda$ is a hyper-parameter representing the step size. The negative
    direction of the gradient ${\nabla}f(\boldsymbol{\theta}^{(k)})$ has a vital property,
    i.e., it is a descent direction of the optimization problem. Therefore, the optimization
    process is to iteratively update trainable parameters along the negative direction
    of gradient until convergence.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是表示步长的超参数。梯度 ${\nabla}f(\boldsymbol{\theta}^{(k)})$ 的负方向具有一个重要的属性，即它是优化问题的下降方向。因此，优化过程是沿梯度的负方向迭代更新可训练参数，直到收敛。
- en: Stochastic Gradient Descent (SGD). The main idea behind SGD is to use random
    mini-batches of training data to update parameters of the optimization problem,
    which inherently reduces the calculation workload. Although the parameters may
    not be updated in the direction of the steepest descent every time, the overall
    update is in the steepest descent direction through multiple rounds of updates.
    As a result, SGD can greatly speed up the optimization process.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）。SGD 的主要思想是使用随机的小批量训练数据来更新优化问题的参数，这本质上减少了计算工作量。尽管参数可能不会每次都沿着最陡下降方向更新，但通过多轮更新，总体更新方向仍然是最陡下降方向。因此，SGD
    可以大大加快优化过程。
- en: Stochastic Gradient Descent-Momentum (SGD-M). Inspired by the concept of momentum
    in physics, SGD-M exerts the influence of the last update on the current update
    to damp oscillation and accelerate convergence. Let $m^{(k)}$ denote the update
    imposed on $\boldsymbol{\theta}^{(k-1)}$ and $\nabla f$ denote the gradient at
    time $k$, the update $m^{(k+1)}$ to be imposed on $\boldsymbol{\theta}^{(k)}$
    can be achieved as
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降-动量（SGD-M）。受到物理中动量概念的启发，SGD-M 在当前更新中施加上一次更新的影响，以减缓振荡并加速收敛。令 $m^{(k)}$
    表示施加在 $\boldsymbol{\theta}^{(k-1)}$ 上的更新，$\nabla f$ 表示时间 $k$ 时的梯度，施加在 $\boldsymbol{\theta}^{(k)}$
    上的更新 $m^{(k+1)}$ 可以通过以下方式实现
- en: '|  | $m^{(k+1)}=\lambda_{0}\ m^{(k)}+\lambda_{1}\nabla f,$ |  | (9) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $m^{(k+1)}=\lambda_{0}\ m^{(k)}+\lambda_{1}\nabla f,$ |  | (9) |'
- en: where $\lambda_{0}$ and $\lambda_{1}$ are hyper-parameters. Sequentially, the
    parameter $\boldsymbol{\theta}^{(k)}$ is updated to $\boldsymbol{\theta}^{(k+1)}$
    by $m^{(k+1)}$ as follows
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{0}$ 和 $\lambda_{1}$ 是超参数。参数 $\boldsymbol{\theta}^{(k)}$ 通过 $m^{(k+1)}$
    被更新为 $\boldsymbol{\theta}^{(k+1)}$，具体如下
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-m^{(k+1)}.$ |  |
    (10) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-m^{(k+1)}.$ |  |
    (10) |'
- en: Root Mean Square Prop (RMSProp). Similar to SGD-M, RMSProp considers the influence
    of the last update when calculating the upcoming update. Let $m^{(k)}$ be the
    update on the previous occasion and $\nabla f$ be the current gradient, RMSProp
    designs upcoming update $m^{(k+1)}$ as follows
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 均方根传播（RMSProp）。类似于 SGD-M，RMSProp 在计算即将到来的更新时考虑上一次更新的影响。令 $m^{(k)}$ 为上一次的更新，$\nabla
    f$ 为当前的梯度，RMSProp 将即将到来的更新 $m^{(k+1)}$ 设计如下
- en: '|  | $m^{(k+1)}=\lambda\ m^{(k)}+(1-\lambda)(\nabla f\odot\nabla f),$ |  |
    (11) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $m^{(k+1)}=\lambda\ m^{(k)}+(1-\lambda)(\nabla f\odot\nabla f),$ |  |
    (11) |'
- en: where $\lambda$ is a hyper-parameter and $\odot$ denotes the $Hadamard$ product
    [[29](#bib.bib29)] which is element-wise. RMSProp updates $\boldsymbol{\theta}^{(k)}$
    to $\boldsymbol{\theta}^{(k+1)}$ in the following way, i.e.,
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是一个超参数，$\odot$ 表示 $Hadamard$ 乘积 [[29](#bib.bib29)]，即逐元素乘积。RMSProp
    以以下方式将 $\boldsymbol{\theta}^{(k)}$ 更新为 $\boldsymbol{\theta}^{(k+1)}$，即：
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-\eta\frac{\nabla
    f}{\sqrt{m^{(k+1)}+\epsilon}},$ |  | (12) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-\eta\frac{\nabla
    f}{\sqrt{m^{(k+1)}+\epsilon}},$ |  | (12) |'
- en: where $\eta$ is a hyper-parameter and $\epsilon$ is positive to prevent the
    denominator from being zero. Using element-wise square root and division operation,
    RMSProp guarantees that different elements in gradient $\nabla f$ have different
    coefficients, which represent learning rates in deep learning. Therefore, RMSProp
    enables parameters to have different learning rates [[29](#bib.bib29)], which
    makes the optimization process more flexible.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\eta$ 是一个超参数，$\epsilon$ 是正数，以防止分母为零。通过逐元素的平方根和除法操作，RMSProp 确保梯度 $\nabla
    f$ 中的不同元素具有不同的系数，这些系数表示深度学习中的学习率。因此，RMSProp 使得参数能够具有不同的学习率 [[29](#bib.bib29)]，从而使优化过程更加灵活。
- en: 'Based on the aforementioned optimization process on manifolds (Section [2.1](#S2.SS1
    "2.1 Geometric Optimization Process on Manifolds ‣ 2 Geometric Optimization Theory
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold")), the Euclidean gradient descent algorithm in Equation ([8](#S2.E8
    "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization Theory ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")) can be transferred to Riemannian manifolds as'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述流形上的优化过程（第[2.1](#S2.SS1 "2.1 流形上的几何优化过程 ‣ 2 几何优化理论 ‣ 深度学习中的几何优化调查：从欧几里得空间到黎曼流形")节），方程([8](#S2.E8
    "在 2.2 梯度下降优化器 ‣ 2 几何优化理论 ‣ 深度学习中的几何优化调查：从欧几里得空间到黎曼流形")中的欧几里得梯度下降算法可以转移到黎曼流形中，如下所示
- en: '|  | $\theta^{(k+1)}=\mathfrak{R}_{{\theta}^{(k)}}(-\lambda grad\,f(\theta^{(k)})),$
    |  | (13) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta^{(k+1)}=\mathfrak{R}_{{\theta}^{(k)}}(-\lambda grad\,f(\theta^{(k)})),$
    |  | (13) |'
- en: where $\mathfrak{R}_{{\theta}^{(k)}}$ means the retraction operation at point
    ${\theta}^{(k)}$ and $grad\,f$ means the Riemannian gradient. For better understanding,
    this article takes constraint SGD-M and constraint RMSProp as an instance to explain
    how to generalize gradient descent optimizers from Euclidean space to manifolds.
    By performing orthogonal projection and retraction, other Euclidean gradient descent
    optimizers can be similarly converted to Riemannian optimizers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathfrak{R}_{{\theta}^{(k)}}$ 代表在点 ${\theta}^{(k)}$ 的回撤操作，而 $grad\,f$ 代表黎曼梯度。为了更好地理解，本文以约束
    SGD-M 和约束 RMSProp 为例，解释如何将梯度下降优化器从欧几里得空间推广到流形。通过执行正交投影和回撤，其他欧几里得梯度下降优化器也可以类似地转化为黎曼优化器。
- en: Constraint SGD-M [[29](#bib.bib29)]. Constraint SGD-M is a generalization of
    SGD-M optimizer on manifolds. In the $k$-th iteration, $m^{(k)}$ denotes a tangent
    vector on the tangent space $T_{\boldsymbol{\theta}^{(k-1)}}M$ and $m^{(k+1)}$
    denotes another vector on the tangent space $T_{\boldsymbol{\theta}^{(k)}}M$.
    Since $\nabla f$ is in the surrounding Euclidean space, it needs to be orthogonally
    projected to tangent space $T_{\boldsymbol{\theta}^{(k)}}M$, i.e., the current
    Riemannian gradient $grad\,f$ is achieved as follows
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 约束 SGD-M [[29](#bib.bib29)]。约束 SGD-M 是对流形上的 SGD-M 优化器的推广。在第 $k$ 次迭代中，$m^{(k)}$
    表示在切空间 $T_{\boldsymbol{\theta}^{(k-1)}}M$ 上的一个切向量，而 $m^{(k+1)}$ 表示在切空间 $T_{\boldsymbol{\theta}^{(k)}}M$
    上的另一个向量。由于 $\nabla f$ 在周围的欧几里得空间中，它需要正交投影到切空间 $T_{\boldsymbol{\theta}^{(k)}}M$，即当前黎曼梯度
    $grad\,f$ 通过以下方式获得
- en: '|  | $grad\,f=\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla f).$ |  | (14) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $grad\,f=\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla f).$ |  | (14) |'
- en: 'The transportation from a tangent space associated with point $p$ to another
    tangent space associated with point $q$ is called parallel transportation, i.e.,
    $\Gamma_{p\rightarrow q}:\ T_{p}M\rightarrow T_{q}M$. After projecting the Euclidean
    gradient $\nabla f$ to the tangent space $T_{\boldsymbol{\theta}^{(k)}}M$ and
    transporting $m^{(k)}$ from $T_{\boldsymbol{\theta}^{(k-1)}}M$ to $T_{\boldsymbol{\theta}^{(k)}}M$,
    Equation ([9](#S2.E9 "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")) is transformed to:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '从与点 $p$ 相关的切空间到与点 $q$ 相关的另一个切空间的传输称为平行传输，即 $\Gamma_{p\rightarrow q}:\ T_{p}M\rightarrow
    T_{q}M$。在将欧几里得梯度 $\nabla f$ 投影到切空间 $T_{\boldsymbol{\theta}^{(k)}}M$ 并将 $m^{(k)}$
    从 $T_{\boldsymbol{\theta}^{(k-1)}}M$ 传输到 $T_{\boldsymbol{\theta}^{(k)}}M$ 后，方程 ([9](#S2.E9
    "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization Theory ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")) 被转化为：'
- en: '|  | $m^{(k+1)}=\lambda_{0}\,\Gamma_{\boldsymbol{\theta}^{(k-1)}\rightarrow\boldsymbol{\theta}^{(k)}}(m^{(k)})+\lambda_{1}\,\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f).$ |  | (15) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $m^{(k+1)}=\lambda_{0}\,\Gamma_{\boldsymbol{\theta}^{(k-1)}\rightarrow\boldsymbol{\theta}^{(k)}}(m^{(k)})+\lambda_{1}\,\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f).$ |  | (15) |'
- en: Based on the retraction operation, the optimization parameter $\boldsymbol{\theta}^{(k+1)}$
    can be updated from $\boldsymbol{\theta}^{(k)}$ by searching along the geodesic
    in the negative direction of $m^{(k+1)}$, i.e., the iterate optimization can be
    expressed as
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于回撤操作，优化参数 $\boldsymbol{\theta}^{(k+1)}$ 可以通过沿着 $m^{(k+1)}$ 的负方向在测地线上搜索来从 $\boldsymbol{\theta}^{(k)}$
    更新，即迭代优化可以表示为
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\mathfrak{R}_{\boldsymbol{\theta}^{(k)}}(-m^{(k+1)}).$
    |  | (16) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}^{(k+1)}=\mathfrak{R}_{\boldsymbol{\theta}^{(k)}}(-m^{(k+1)}).$
    |  | (16) |'
- en: 'Constraint RMSProp [[29](#bib.bib29)]. Similar to constraint SGD-M, after transporting
    $m^{(k)}$ from tangent space $T_{\boldsymbol{\theta}^{(k-1)}}M$ to $T_{\boldsymbol{\theta}^{(k)}}M$
    and orthogonally projecting $\nabla f\odot\nabla f$ to corresponding tangent space,
    Equation ([11](#S2.E11 "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")) can be transformed into:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '约束 RMSProp [[29](#bib.bib29)]。类似于约束 SGD-M，在将 $m^{(k)}$ 从切空间 $T_{\boldsymbol{\theta}^{(k-1)}}M$
    传送到 $T_{\boldsymbol{\theta}^{(k)}}M$ 并将 $\nabla f\odot\nabla f$ 正交投影到相应的切空间后，方程 ([11](#S2.E11
    "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization Theory ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")) 可以转化为：'
- en: '|  | $m^{(k+1)}=\lambda\Gamma_{\boldsymbol{\theta}^{(k-1)}\rightarrow\boldsymbol{\theta}^{(k)}}(m^{(k)})+(1-\lambda)\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f\odot\nabla f).$ |  | (17) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $m^{(k+1)}=\lambda\Gamma_{\boldsymbol{\theta}^{(k-1)}\rightarrow\boldsymbol{\theta}^{(k)}}(m^{(k)})+(1-\lambda)\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f\odot\nabla f).$ |  | (17) |'
- en: The parameter $\boldsymbol{\theta}^{(k+1)}$ of the optimization goal can be
    iteratively searched on the manifold with a determined direction $-\eta\,\frac{\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f)}{\sqrt{m^{(k+1)}+\epsilon}}$, that is,
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 优化目标的参数$\boldsymbol{\theta}^{(k+1)}$可以在流形上以确定的方向$-\eta\,\frac{\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f)}{\sqrt{m^{(k+1)}+\epsilon}}$进行迭代搜索，即，
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\mathfrak{R}_{\boldsymbol{\theta}^{(k)}}(-\eta\,\frac{\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f)}{\sqrt{m^{(k+1)}+\epsilon}}).$ |  | (18) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}^{(k+1)}=\mathfrak{R}_{\boldsymbol{\theta}^{(k)}}(-\eta\,\frac{\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f)}{\sqrt{m^{(k+1)}+\epsilon}}).$ |  | (18) |'
- en: 2.3 Manifold Examples
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 流形示例
- en: 'Different kinds of matrix manifolds have different geometry structures and
    satisfy different constraints, bringing different advantages when applying geometric
    optimization to deep learning. For example, the oblique manifold plays a significant
    role in dictionary learning due to its property of unit-norm columns, while the
    Stiefel manifold has a positive effect on optimizing RNNs since matrices on the
    Stiefel manifold have orthogonal and uncorrelated columns, which helps alleviate
    feature abundancy problems in RNNs. Since space is limited, this section only
    presents common manifold structures³³3For more introduction on matrix manifolds,
    we refer interested readers to the website https://www.Pymanopt.org. such as Stiefel
    manifold, oblique manifold, and Graßmann manifold, all of which are widely used
    in existing geometric optimization techniques that are discussed in Section [3](#S3
    "3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") and Section [4](#S4
    "4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '不同种类的矩阵流形具有不同的几何结构和约束条件，在将几何优化应用于深度学习时带来不同的优势。例如，倾斜流形在字典学习中发挥了重要作用，因为它的单位范数列的特性，而Stiefel流形对优化RNN有积极影响，因为Stiefel流形上的矩阵具有正交且不相关的列，这有助于缓解RNN中的特征冗余问题。由于空间有限，本节仅介绍常见的流形结构³³3有关矩阵流形的更多介绍，请感兴趣的读者访问网站
    https://www.Pymanopt.org。例如Stiefel流形、倾斜流形和Grassmann流形，它们都广泛应用于现有的几何优化技术，这些技术在第[3](#S3
    "3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")节和第[4](#S4 "4
    Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")节中讨论。'
- en: Product Manifold and Quotient Manifold. Let $\mathcal{A}$ and $\mathcal{B}$
    be two manifolds of dimension $d_{A}$ and $d_{B}$, for any pair of charts $(U,\phi)$
    and $(V,\varphi)$ of $\mathcal{A}$ and $\mathcal{B}$, the map $\Phi$ is defined
    on $U\times V$ by $\Phi(x,y)=(\phi(x),\varphi(y))$. It specifies a smooth product
    manifold structure on the product space $\mathcal{A}\times\mathcal{B}$. Quotient
    manifold is an abstract space with similar subsets in the same manifold. These
    subsets can be described with equivalence relationship. $\mathcal{A}$ represents
    a manifold equipped with an equivalence relation $\sim$, which satisfies three
    properties, i.e., reflexivity, symmetry and transitivity [[8](#bib.bib8)]. The
    equivalence class of one point $x$ consists of all elements that are equivalent
    to it, i.e.,
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 乘积流形和商流形。设$\mathcal{A}$和$\mathcal{B}$是两个维度分别为$d_{A}$和$d_{B}$的流形，对于$\mathcal{A}$和$\mathcal{B}$的任意一对图$(U,\phi)$和$(V,\varphi)$，映射$\Phi$在$U\times
    V$上定义为$\Phi(x,y)=(\phi(x),\varphi(y))$。它在乘积空间$\mathcal{A}\times\mathcal{B}$上指定了一个光滑的乘积流形结构。商流形是一个抽象空间，具有在同一流形中相似的子集。这些子集可以用等价关系描述。$\mathcal{A}$表示一个配备有等价关系$\sim$的流形，满足三种属性，即自反性、对称性和传递性[[8](#bib.bib8)]。一个点$x$的等价类由所有与之等价的元素组成，即，
- en: '|  | $[x]:=\{y\in\mathcal{A}:y\sim x\},$ |  | (19) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $[x]:=\{y\in\mathcal{A}:y\sim x\},$ |  | (19) |'
- en: where $[x]$ indicates the equivalence class of $x$. The quotient of manifold
    $\mathcal{A}$ by relation $\sim$ is defined as follows
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$[x]$表示$x$的等价类。流形$\mathcal{A}$通过关系$\sim$的商定义如下
- en: '|  | $\mathcal{A}/\sim\ :=\{[x]:x\in\mathcal{A}\},$ |  | (20) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{A}/\sim\ :=\{[x]:x\in\mathcal{A}\},$ |  | (20) |'
- en: with the projection $\pi:\mathcal{A}\rightarrow\mathcal{A}/\sim$, indicated
    by $x\rightarrow[x]$. When $\pi$ is a submersion projection, and $\mathcal{A}$
    is a smooth manifold [[8](#bib.bib8), [30](#bib.bib30)], $\mathcal{A}/\sim$ admits
    a unique smooth manifold structure $B$, which is the quotient manifold of $\mathcal{A}$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过投影 $\pi:\mathcal{A}\rightarrow\mathcal{A}/\sim$，用 $x\rightarrow[x]$ 表示。当 $\pi$
    是一个沉浸投影，且 $\mathcal{A}$ 是一个光滑流形 [[8](#bib.bib8), [30](#bib.bib30)] 时，$\mathcal{A}/\sim$
    具有一个唯一的光滑流形结构 $B$，即 $\mathcal{A}$ 的商流形。
- en: Symmetric Positive-Definite Manifold [[29](#bib.bib29)]. It consists of Symmetric
    Positive-Definite (SPD) matrices $M\in\mathbb{R}^{p\times p}$ equipped with the
    Affine Invariant Riemannian Metric (*AIRM*) as follows
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对称正定流形 [[29](#bib.bib29)]。它由对称正定（SPD）矩阵 $M\in\mathbb{R}^{p\times p}$ 组成，配备有如下的仿射不变黎曼度量
    (*AIRM*)。
- en: '|  | $S_{++}^{p}\triangleq\{M\in\mathbb{R}^{p\times p}:v^{T}Mv>0,\forall v\in\mathbb{R}^{p}-\{0_{p}\}\}.$
    |  | (21) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{++}^{p}\triangleq\{M\in\mathbb{R}^{p\times p}:v^{T}Mv>0,\forall v\in\mathbb{R}^{p}-\{0_{p}\}\}.$
    |  | (21) |'
- en: SPD manifold achieves great success in computer vision due to its powerful statistical
    representations for images and videos. For example, SPD matrices are used to construct
    region covariance matrices for pedestrian detection [[31](#bib.bib31)], joint
    covariance descriptors for action recognition [[32](#bib.bib32)], and image set
    covariance matrices for face recognition [[33](#bib.bib33)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: SPD 流形在计算机视觉中取得了巨大成功，因为它对图像和视频具有强大的统计表示。例如，SPD 矩阵用于构建行人检测的区域协方差矩阵 [[31](#bib.bib31)]，动作识别的联合协方差描述符
    [[32](#bib.bib32)]，以及面部识别的图像集协方差矩阵 [[33](#bib.bib33)]。
- en: Stiefel Manifold [[29](#bib.bib29)]. The Stiefel manifold $St(p,n)$ is composed
    of orthogonal matrices $W\in\mathbb{R}^{n\times p}(p\leq n)$ endowed with the
    Frobenius inner product as follows
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Stiefel 流形 [[29](#bib.bib29)]。Stiefel 流形 $St(p,n)$ 由正交矩阵 $W\in\mathbb{R}^{n\times
    p}(p\leq n)$ 组成，配备有如下的 Frobenius 内积。
- en: '|  | $St(p,n)\triangleq\{W\in\mathbb{R}^{n\times p}:W^{T}W=I_{p}\},$ |  | (22)
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $St(p,n)\triangleq\{W\in\mathbb{R}^{n\times p}:W^{T}W=I_{p}\},$ |  | (22)
    |'
- en: where $I_{p}$ denotes $\mathbb{R}^{p\times p}$ identity matrix. The optimization
    function over the compact Stiefel manifolds has an upper bound, which allows it
    to achieve an optimal solution.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{p}$ 表示 $\mathbb{R}^{p\times p}$ 单位矩阵。紧致 Stiefel 流形上的优化函数具有上界，这使其能够实现最优解。
- en: Sphere Manifold and Oblique Manifold. The set of unit Frobenius norm matrices
    of size $n\times m$ is denoted by the sphere $\mathbb{S}^{nm-1}$. It can be treated
    as a Riemannian submanifold embedded in Euclidean space $\mathbb{R}^{n\times m}$
    endowed with the usual inner product $\langle H_{1},H_{2}\rangle=\operatorname{trace}(H_{1}^{T}H_{2})$.
    The oblique manifold $\mathcal{OB}(n,m)$ is the set of matrices of size $n\times
    m$ with unit-norm columns. It has the same geometry as that of the product manifold
    of spheres $\prod_{i=0}^{m}\mathbb{S}^{n-1}$.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 球面流形和斜面流形。单位 Frobenius 范数的 $n\times m$ 矩阵集合用球面 $\mathbb{S}^{nm-1}$ 表示。它可以被视为嵌入在欧几里得空间
    $\mathbb{R}^{n\times m}$ 中的黎曼子流形，配备有通常的内积 $\langle H_{1},H_{2}\rangle=\operatorname{trace}(H_{1}^{T}H_{2})$。斜面流形
    $\mathcal{OB}(n,m)$ 是单位范数列的 $n\times m$ 矩阵集合。它具有与球面乘积流形 $\prod_{i=0}^{m}\mathbb{S}^{n-1}$
    相同的几何结构。
- en: Graßmann Manifold [[29](#bib.bib29)]. The Graßmann manifold $\mathcal{G}(n,p)$
    embraces the set of subspaces spanned by the orthogonal matrices $X\in\mathbb{R}^{n\times
    p}(p\leq n)$ as
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Graßmann 流形 [[29](#bib.bib29)]。Graßmann 流形 $\mathcal{G}(n,p)$ 包含由正交矩阵 $X\in\mathbb{R}^{n\times
    p}(p\leq n)$ 张成的子空间集合，如下所示。
- en: '|  | $\mathcal{G}(n,p)\triangleq\{Span(X):X\in\mathbb{R}^{n\times p},X^{T}X=I_{p}\}.$
    |  | (23) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{G}(n,p)\triangleq\{Span(X):X\in\mathbb{R}^{n\times p},X^{T}X=I_{p}\}.$
    |  | (23) |'
- en: Note that a Graßmann manifold is different from a Stiefel manifold, i.e., a
    point on the Stiefel manifold represents a basis for a subspace, whereas a point
    on the Graßmann manifold represents an entire subspace. Moreover, Graßmann manifolds
    are of linear subspaces and can be used to perform a geometry-aware dimension
    reduction.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 Graßmann 流形与 Stiefel 流形不同，即 Stiefel 流形上的一个点代表一个子空间的基，而 Graßmann 流形上的一个点代表一个整个子空间。此外，Graßmann
    流形是线性子空间的集合，可用于执行几何感知的维度减少。
- en: Unitary Manifold. Unitary matrices are the extension of orthogonal matrices
    to the complex domain, i.e.,
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 单位流形。单位矩阵是正交矩阵在复数域的扩展，即，
- en: '|  | $U(n)\triangleq\{U\in\mathbb{C}^{n\times n}:U^{\ast}U=I_{n}\},$ |  | (24)
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $U(n)\triangleq\{U\in\mathbb{C}^{n\times n}:U^{\ast}U=I_{n}\},$ |  | (24)
    |'
- en: 'where $U^{\ast}$ denotes the conjugate transpose matrix and $I_{n}$ represents
    the identity matrix of size $n\times n$. Orthogonal or unitary matrices can preserve
    norm of vectors, i.e., $\|Wh\|_{2}$ = $\|h\|_{2}$ when $W$ is an orthogonal or
    unitary matrix. Therefore, exploding and vanishing gradient problems in deep temporary
    networks can be alleviated when parameters are optimized on the orthogonal or
    unitary manifold, which will de detailed in Section [4.2](#S4.SS2 "4.2 Geometric
    RNN ‣ 4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for
    Deep Learning: From Euclidean Space to Riemannian Manifold").'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $U^{\ast}$ 表示共轭转置矩阵，$I_{n}$ 代表大小为 $n\times n$ 的单位矩阵。正交或单位ary矩阵可以保持向量的范数，即，当
    $W$ 是一个正交或单位ary矩阵时，$\|Wh\|_{2}$ = $\|h\|_{2}$。因此，当参数在正交或单位ary流形上优化时，可以缓解深度临时网络中的梯度爆炸和消失问题，这将在第
    [4.2](#S4.SS2 "4.2 Geometric RNN ‣ 4 Applications in Deep Learning ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold") 节中详细讨论。'
- en: Lie Group [[13](#bib.bib13)]. Lie groups are real or complex manifolds with
    group structure. There are two compact and connected Lie groups, i.e., the special
    orthogonal group formulated as
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 李群 [[13](#bib.bib13)]。李群是具有群结构的实数或复数流形。存在两个紧致且连通的李群，即特殊正交群，公式如下
- en: '|  | $SO(n)=\{B\in\mathbb{R}^{n\times n}&#124;B^{T}B=I,det(B)=1\},$ |  | (25)
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $SO(n)=\{B\in\mathbb{R}^{n\times n} \mid B^{T}B=I, \det(B)=1\},$ |  |
    (25) |'
- en: and the unitary group formulated as
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 和单位ary群的公式如下
- en: '|  | $U(n)=\{B\in\mathbb{C}^{n\times n}&#124;B^{\ast}B=I\}.$ |  | (26) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $U(n)=\{B\in\mathbb{C}^{n\times n} \mid B^{\ast}B=I\}.$ |  | (26) |'
- en: The tangent space at the identity element of the Lie group is called the $Lie\
    algebra$ of it. For the special orthogonal group and the unitary group, their
    Lie algebras are given by
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 李群在单位元素处的切空间被称为它的 $Lie\ algebra$。对于特殊正交群和单位ary群，它们的李代数分别由以下公式给出
- en: '|  | $\displaystyle\mathfrak{so}(n)$ | $\displaystyle=\{A\in\mathbb{R}^{n\times
    n}&#124;A\ +A^{T}=0\},$ |  | (27) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathfrak{so}(n)$ | $\displaystyle=\{A\in\mathbb{R}^{n\times
    n} \mid A + A^{T}=0\},$ |  | (27) |'
- en: '|  | $\displaystyle\mathfrak{u}(n)$ | $\displaystyle=\{A\in\mathbb{C}^{n\times
    n}&#124;A\ +A^{\ast}=0\}.$ |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathfrak{u}(n)$ | $\displaystyle=\{A\in\mathbb{C}^{n\times
    n} \mid A + A^{\ast}=0\}.$ |  |'
- en: $\mathfrak{so}(n)$ is known as skew-symmetric matrix, while $\mathfrak{u}(n)$
    is skew-Hermitian matrix. The Lie exponential map ($exp:\mathfrak{g}\rightarrow
    G$ where $G$ denotes the Lie Group and $\mathfrak{g}$ denotes its Lie algebra)
    on a connected, compact Lie group is surjective. Therefore, the optimization problem
    on a Lie group can be converted to the optimization problem in Euclidean space
    where Euclidean gradient descent optimizers can be directly used.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathfrak{so}(n)$ 被称为反对称矩阵，而 $\mathfrak{u}(n)$ 是反厄米矩阵。对一个连通的、紧致的李群，李指数映射（$exp:\mathfrak{g}\rightarrow
    G$，其中 $G$ 表示李群，$\mathfrak{g}$ 表示其李代数）是满射的。因此，李群上的优化问题可以转换为欧几里得空间中的优化问题，在那里可以直接使用欧几里得梯度下降优化器。
- en: 3 Applications in Classical Machine Learning
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 经典机器学习中的应用
- en: Classical machine learning methods gained achievements in solving artificial
    intelligence problems (e.g., dimension reduction, inverse problem, sparse representation,
    analysis operator learning, and temporal models). Despite the increasing computing
    power of modern computer facilities, it is still difficult to solve a large category
    of constrained classical machine learning problems in Euclidean space. To decrease
    the solving difficulty, geometric optimization focuses on the special structure
    of constrained problems and regards them as unconstrained ones on Riemannian manifolds
    [[10](#bib.bib10)].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的机器学习方法在解决人工智能问题（例如，维度约简、逆问题、稀疏表示、分析算子学习和时间模型）方面取得了成就。尽管现代计算设施的计算能力不断提高，但在欧几里得空间中解决大量约束的经典机器学习问题仍然困难。为了降低解决难度，几何优化关注约束问题的特殊结构，将其视为黎曼流形上的无约束问题
    [[10](#bib.bib10)]。
- en: 3.1 Dimension Reduction
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 维度约简
- en: 'By using a mapping $\mu\colon\mathbb{R}^{m}\to\mathbb{R}^{l}$ with $l<m$, dimension
    reduction (DR) aims to find a lower-dimensional representation $y_{i}\in\mathbb{R}^{l}$
    of given data samples $x_{i}\in\mathbb{R}^{m}$. The most popular DR paradigm uses
    a linear projection while others employ a nonlinear transformation to constrain
    locality properties between data. Table [1](#S3.T1 "Table 1 ‣ 3.1 Dimension Reduction
    ‣ 3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") summarizes main
    properties of mainstream DR approaches (e.g., linear discriminant analysis (LDA)
    [[34](#bib.bib34)], principal component analysis (PCA) [[35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37)], multi-dimensional scaling (MDS) [[38](#bib.bib38), [39](#bib.bib39)],
    isometric feature mapping (ISOMAP) [[40](#bib.bib40)], local linear embedding
    (LLE) [[41](#bib.bib41)], laplace eigenmaps (LE) [[42](#bib.bib42)], and locality
    preserving projections (LPP) [[43](#bib.bib43)]).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '通过使用映射 $\mu\colon\mathbb{R}^{m}\to\mathbb{R}^{l}$ 其中 $l<m$，降维（DR）旨在找到给定数据样本
    $x_{i}\in\mathbb{R}^{m}$ 的低维表示 $y_{i}\in\mathbb{R}^{l}$。最流行的 DR 范式使用线性投影，而其他方法则采用非线性变换以约束数据之间的局部性质。表 [1](#S3.T1
    "Table 1 ‣ 3.1 Dimension Reduction ‣ 3 Applications in Classical Machine Learning
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold") 总结了主流 DR 方法的主要特性（例如，线性判别分析（LDA） [[34](#bib.bib34)]，主成分分析（PCA）
    [[35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)]，多维尺度分析（MDS） [[38](#bib.bib38),
    [39](#bib.bib39)]，等距特征映射（ISOMAP） [[40](#bib.bib40)]，局部线性嵌入（LLE） [[41](#bib.bib41)]，拉普拉斯特征映射（LE）
    [[42](#bib.bib42)]，和保持局部性的投影（LPP） [[43](#bib.bib43)])。'
- en: The mapping $\mu\colon\mathbb{R}^{m}\to\mathbb{R}^{l}$ used in DR methods is
    often restricted to be an orthogonal projection, i.e.,
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DR 方法中使用的映射 $\mu\colon\mathbb{R}^{m}\to\mathbb{R}^{l}$ 通常被限制为正交投影，即，
- en: '|  | $\mu(\mathbf{x}):=\mathbf{V}^{\top}\mathbf{x},$ |  | (28) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu(\mathbf{x}):=\mathbf{V}^{\top}\mathbf{x},$ |  | (28) |'
- en: where the orthogonal matrix $\mathbf{V}\in\mathbb{R}^{m\times l}$ belongs to
    the *Stiefel* manifold ${St}(l,m):=\big{\{}\mathbf{V}\in\mathbb{R}^{m\times l}|\mathbf{V}^{\top}\mathbf{V}=\mathbf{I}_{l}\big{\}}$.
    One generic algorithmic framework to find an optimal $\mathbf{V}\in St(l,m)$ can
    be formulated as a maximization problem, i.e.,
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，正交矩阵 $\mathbf{V}\in\mathbb{R}^{m\times l}$ 属于 *Stiefel* 流形 ${St}(l,m):=\big{\{}\mathbf{V}\in\mathbb{R}^{m\times
    l}|\mathbf{V}^{\top}\mathbf{V}=\mathbf{I}_{l}\big{\}}$。寻找一个最优的 $\mathbf{V}\in
    St(l,m)$ 的一种通用算法框架可以被表述为一个最大化问题，即，
- en: '|  | $\operatorname*{argmax}_{\mathbf{V}\in St(l,m)}\,\frac{\operatorname{tr}(\mathbf{V}^{\top}\mathbf{A}\mathbf{V})}{\operatorname{tr}(\mathbf{V}^{\top}\mathbf{B}\mathbf{V})+\sigma},$
    |  | (29) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname*{argmax}_{\mathbf{V}\in St(l,m)}\,\frac{\operatorname{tr}(\mathbf{V}^{\top}\mathbf{A}\mathbf{V})}{\operatorname{tr}(\mathbf{V}^{\top}\mathbf{B}\mathbf{V})+\sigma},$
    |  | (29) |'
- en: 'where matrices $A,B\in\mathbb{R}^{m\times m}$ are often symmetric or positive
    definite matrices. Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) is called *trace quotient* or *trace
    ratio*. Note that constant $\sigma>0$ can prevent the denominator from being zero.
    Matrices $A$ and $B$ are constructed to measure the similarity between data points
    according to specific problems. $V$ is not unique and closely related to selected
    eigenvalues. Solutions of Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣
    3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")) are rotation
    invariant, i.e., let $\mathbf{V}^{*}\in St(l,m)$ be a solution of the problem,
    then $\mathbf{V}^{*}\boldsymbol{\Theta}$ for any orthogonal $\boldsymbol{\Theta}\in\mathbb{R}^{l\times
    l}$ is also a solution of Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣
    3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")). In other words,
    the solution set of Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) is the set of all $l$-dimensional
    linear subspaces in $\mathbb{R}^{m}$, which can be represented by Graßmann manifold
    , i.e.,'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中矩阵 $A,B\in\mathbb{R}^{m\times m}$ 通常是对称矩阵或正定矩阵。公式 ([29](#S3.E29 "在 3.1 降维
    ‣ 3 经典机器学习中的应用 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形")) 被称为*迹商*或*迹比*。注意常数 $\sigma>0$ 可以防止分母为零。矩阵
    $A$ 和 $B$ 被构建用来根据特定问题测量数据点之间的相似性。$V$ 并不是唯一的，且与选择的特征值密切相关。公式 ([29](#S3.E29 "在 3.1
    降维 ‣ 3 经典机器学习中的应用 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形")) 的解是旋转不变的，即，设 $\mathbf{V}^{*}\in
    St(l,m)$ 为问题的一个解，则对于任何正交矩阵 $\boldsymbol{\Theta}\in\mathbb{R}^{l\times l}$，$\mathbf{V}^{*}\boldsymbol{\Theta}$
    也是公式 ([29](#S3.E29 "在 3.1 降维 ‣ 3 经典机器学习中的应用 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形")) 的解。换句话说，公式 ([29](#S3.E29
    "在 3.1 降维 ‣ 3 经典机器学习中的应用 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形")) 的解集是 $\mathbb{R}^{m}$ 中所有
    $l$ 维线性子空间的集合，可以用 Graßmann 流形表示，即，
- en: '|  | $\mathfrak{Gr}(l,m):=\left\{\mathbf{V}\mathbf{V}^{\top}&#124;\mathbf{V}\in
    St(l,m)\right\}.$ |  | (30) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathfrak{Gr}(l,m):=\left\{\mathbf{V}\mathbf{V}^{\top} \mid \mathbf{V}\in
    St(l,m)\right\}.$ |  | (30) |'
- en: 'As shown above, most linear DR methods begin with solving $tr(V^{T}AV)$ while
    nonlinear DR methods construct a graph by connecting nearby points, which captures
    information on the local neighborhood structure of data and forms a similar optimization
    problem. Taking the non-linear DR method LE as an example, the Laplace matrix
    associated with the neighborhood graph [[44](#bib.bib44)] can be regarded as the
    symmetric matrix $A$ in Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣ 3
    Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，大多数线性降维方法从解决 $tr(V^{T}AV)$ 开始，而非线性降维方法通过连接邻近点构建图，这捕捉了数据局部邻域结构的信息，并形成类似的优化问题。以非线性降维方法
    LE 为例，与邻域图相关的拉普拉斯矩阵 [[44](#bib.bib44)] 可以视为公式 ([29](#S3.E29 "在 3.1 降维 ‣ 3 经典机器学习中的应用
    ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形")) 中的对称矩阵 $A$。
- en: '[b]'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[b]'
- en: 'Table 1: Summary of Dimension Reduction Algorithms'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 降维算法总结'
- en: '| Methods | $Linear/Non-Linear^{1}$ | $Global/Local^{2}$ | Properties |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | $线性/非线性^{1}$ | $全局/局部^{2}$ | 属性 |'
- en: '| LDA | Linear | Global | is supervised, uses prior knowledge of categories,
    is limited to Gaussian distribution samples |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| LDA | 线性 | 全局 | 是有监督的，使用类别的先验知识，限制于高斯分布样本 |'
- en: '| PCA | Linear | Global | is unsupervised, uses orthogonal principal components
    to eliminate interactions between each components |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| PCA | 线性 | 全局 | 是无监督的，使用正交主成分消除各成分之间的相互作用 |'
- en: '| MDS | Non-Linear | Global | has simple calculation, preserves the data relationship
    in original space, is visualization-friendly, mistakenly assumes that each dimension has
    a same contribution |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| MDS | 非线性 | 全局 | 计算简单，保留了原始空间中的数据关系，适合可视化，错误地假设每个维度有相同的贡献 |'
- en: '| ISOMAP | Non-Linear | Global | suits low dimensional manifolds with a flat interior
    rather than that with large internal curvature, has high computation cost |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ISOMAP | 非线性 | 全局 | 适用于具有平坦内部的低维流形，而不是具有大内曲率的流形，计算成本高 |'
- en: '| LLE | Non-Linear | Local | suits non-closed locally linear low dimensional
    manifolds, has small computational complexity, is limited to dense uniform dataset,
    is sensitive to the number of nearest neighbor samples |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LLE | 非线性 | 局部 | 适用于非封闭的局部线性低维流形，计算复杂度小，限于稠密均匀数据集，对最近邻样本的数量敏感 |'
- en: '| LE | Non-Linear | Local | preserves local features, is less sensitive to
    outliers and noise, has a stable embedding |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| LE | 非线性 | 局部 | 保留局部特征，对离群点和噪声的敏感性较小，嵌入稳定 |'
- en: '| LPP | Linear | Local | is defined at any point in space, i.e., can be generalized
    to the testing set and not limited to the training set |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| LPP | 线性 | 局部 | 在空间中的任何点定义，即可以推广到测试集而不仅限于训练集 |'
- en: '1'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: Linear represents linear projection mapping, while non-linear represents non-linear
    projection mapping.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性表示线性投影映射，而非线性表示非线性投影映射。
- en: '2'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2'
- en: The global/local represents the geometric relationship of the input data.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全局/局部代表输入数据的几何关系。
- en: 3.2 Inverse Problem
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 逆问题
- en: 'Aiming to explore internal patterns from phenomena [[45](#bib.bib45)], an inverse
    problem has a significant impact on practical applications. For example, the following
    practical problems can be modeled as inverse problems: i) deducing structural
    information in human body from the X-ray; and ii) infering interior appearance
    of stratigraphy from seismic wave. An inverse problem can be viewed as reconstructing
    inputs from outputs as follows,'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在从现象中探索内部模式 [[45](#bib.bib45)]，逆问题对实际应用具有重要影响。例如，以下实际问题可以被建模为逆问题：i) 从 X 光中推导人体结构信息；ii)
    从地震波推测地层的内部外观。逆问题可以被视为从输出重建输入，如下所示，
- en: '|  | $\mathbf{y}=\bm{{W}}{\mathbf{x}},$ |  | (31) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{y}=\bm{{W}}{\mathbf{x}},$ |  | (31) |'
- en: 'where $\mathbf{y}\in\mathbb{R}^{l}$ is the given output and $\bm{{W}}$ is a
    matrix that maps input data $\mathbf{x}$ to output data $\mathbf{y}$. The goal
    of the inverse problem in Equation ([31](#S3.E31 "In 3.2 Inverse Problem ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) is to recover $\mathbf{x}$ on the
    premise that $\mathbf{y}$ is a priori. It is challenging to get a precise solution,
    however, an approximate solution can be achieved by confining the parameter matrix
    $\bm{{W}}$ to reside on a smooth Riemannian manifold. Let the sum of elements
    in the same row of matrix $\bm{{W}}$ be exact 1, Equation ([31](#S3.E31 "In 3.2
    Inverse Problem ‣ 3 Applications in Classical Machine Learning ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold"))
    can be solved by optimization on the oblique manifold $\mathcal{M}$ where matrices
    all have unit row sums [[7](#bib.bib7)], i.e.,'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{y}\in\mathbb{R}^{l}$ 是给定的输出，而 $\bm{{W}}$ 是一个将输入数据 $\mathbf{x}$ 映射到输出数据
    $\mathbf{y}$ 的矩阵。方程中的逆问题（[31](#S3.E31 "在 3.2 逆问题 ‣ 3 经典机器学习中的应用 ‣ 深度学习的几何优化调查：从欧几里得空间到黎曼流形")）的目标是在
    $\mathbf{y}$ 是先验已知的前提下恢复 $\mathbf{x}$。虽然获取精确解是具有挑战性的，但可以通过将参数矩阵 $\bm{{W}}$ 限制在光滑的黎曼流形上来获得近似解。让矩阵
    $\bm{{W}}$ 中同一行的元素之和恰好为 1，方程（[31](#S3.E31 "在 3.2 逆问题 ‣ 3 经典机器学习中的应用 ‣ 深度学习的几何优化调查：从欧几里得空间到黎曼流形")）可以通过在所有具有单位行和的斜截流形
    $\mathcal{M}$ 上进行优化来解决 [[7](#bib.bib7)]，即，
- en: '|  | $\min_{\bm{{W}}\in\mathcal{M}}\left\&#124;\mathbf{y}-\bm{{W}}\mathbf{x}\right\&#124;_{2}^{2}.$
    |  | (32) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\bm{{W}}\in\mathcal{M}}\left\&#124;\mathbf{y}-\bm{{W}}\mathbf{x}\right\&#124;_{2}^{2}.$
    |  | (32) |'
- en: 3.3 Dictionary Learning
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 字典学习
- en: As a specific inverse problem, dictionary learning has been widely used to obtain
    the most essential features of input data [[23](#bib.bib23)]. Let $X\in R_{n\times
    k}$ denote the input sample, in dictionary learning, $X$ is expanded into a linear
    combination as
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个具体的逆问题，字典学习已被广泛用于获取输入数据中最重要的特征 [[23](#bib.bib23)]。设 $X\in R_{n\times k}$
    表示输入样本，在字典学习中，$X$ 被扩展为线性组合如下
- en: '|  | $X=D_{1}\phi_{1}+\cdots+D_{n}\phi_{n},$ |  | (33) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $X=D_{1}\phi_{1}+\cdots+D_{n}\phi_{n},$ |  | (33) |'
- en: 'where $D_{1},\cdots,D_{n}$ represent the most essential features to be learned
    from the input, while $\phi_{1},\cdots,\phi_{n}$ indicate combination coefficients
    of features $D_{1},\cdots,D_{n}$. Let $D\in R_{k\times n}$ indicate the dictionary
    set $\{D_{1},\cdots,D_{n}\}$ and $\Phi\in R_{n\times r}$ indicate the set $\{\phi_{1},\cdots,\phi_{n}\}$,
    Equation ([33](#S3.E33 "In 3.3 Dictionary Learning ‣ 3 Applications in Classical
    Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) can be simplified as follows,'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D_{1},\cdots,D_{n}$ 代表从输入中学习到的最重要特征，而 $\phi_{1},\cdots,\phi_{n}$ 表示特征 $D_{1},\cdots,D_{n}$
    的组合系数。设 $D\in R_{k\times n}$ 表示字典集 $\{D_{1},\cdots,D_{n}\}$，$\Phi\in R_{n\times
    r}$ 表示集合 $\{\phi_{1},\cdots,\phi_{n}\}$，方程 ([33](#S3.E33 "在 3.3 字典学习 ‣ 3 在经典机器学习中的应用
    ‣ 深度学习的几何优化综述：从欧几里得空间到黎曼流形")) 可以简化为如下，
- en: '|  | $X=D\Phi,$ |  | (34) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $X=D\Phi,$ |  | (34) |'
- en: 'where $D$ and $\Phi$ can have various kinds of combinations. Dictionary learning
    aims to learn a $D$ that makes the coefficients $\Phi$ be zero or close to zero,
    i.e., a sparse representation of samples $X$. The dictionary $D$ and the sparse
    coefficients $\Phi$ are calculated alternately. When $\Phi$ is fixed, the dictionary
    learning part is the same as the form of Equation ([31](#S3.E31 "In 3.2 Inverse
    Problem ‣ 3 Applications in Classical Machine Learning ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")),
    which is an inverse problem of reconstructing $D$. Let $\left\|\phi\right\|_{0}$
    denote the number of entries in $\Phi$ that are different from zero, the dictionary
    $D$ is subject to $\left\|D_{1}\right\|=...=\left\|D_{n}\right\|=1$. Therefore,
    the above dictionary learning problem can be transformed to the following minimization
    problem on the oblique manifold:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D$ 和 $\Phi$ 可以有多种组合方式。字典学习旨在学习一个 $D$ 使得系数 $\Phi$ 为零或接近零，即样本 $X$ 的稀疏表示。字典
    $D$ 和稀疏系数 $\Phi$ 是交替计算的。当 $\Phi$ 固定时，字典学习部分与方程 ([31](#S3.E31 "在 3.2 反问题 ‣ 3 在经典机器学习中的应用
    ‣ 深度学习的几何优化综述：从欧几里得空间到黎曼流形")) 的形式相同，这是重建 $D$ 的反问题。设 $\left\|\phi\right\|_{0}$
    表示 $\Phi$ 中非零条目的数量，字典 $D$ 满足 $\left\|D_{1}\right\|=...=\left\|D_{n}\right\|=1$。因此，上述字典学习问题可以转化为在斜流形上的以下最小化问题：
- en: '|  | $\operatorname*{argmin}_{D\in\textit{OB}(k,n)}\left\&#124;X-D\Phi\right\&#124;_{2}^{2}+\lambda\left\&#124;\Phi\right\&#124;_{0}.$
    |  | (35) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname*{argmin}_{D\in\textit{OB}(k,n)}\left\|X-D\Phi\right\|_{2}^{2}+\lambda\left\|\Phi\right\|_{0}.$
    |  | (35) |'
- en: 3.4 Analysis Operator Learning
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 分析算子学习
- en: Analysis operator learning assumes that a few operators are sufficient to represent
    observed high-dimensional variables [[46](#bib.bib46)]. However, these operators
    are implicit and unobserved, for instance, store environment and service quality
    are latent operators hidden behind the observed variable “price”. The goal of
    analysis operator learning is to find out these invisible operators, since low-dimensional
    operators can simplify original high-dimensional variables.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 分析算子学习假设几个算子足以表示观察到的高维变量[[46](#bib.bib46)]。然而，这些算子是隐含的且不可观察的，例如，存储环境和服务质量是隐藏在观察变量“价格”背后的潜在算子。分析算子学习的目标是找出这些隐形的算子，因为低维算子可以简化原始的高维变量。
- en: Let $X$ be original high-dimensional variables and $F$ be latent operators with
    lower dimensions, the analysis operator learning can be generally formulated as
    follows
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $X$ 为原始高维变量，$F$ 为低维潜在算子，分析算子学习通常可以表述为如下
- en: '|  | $X=AF,$ |  | (36) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $X=AF,$ |  | (36) |'
- en: where $A$ denotes the operator loading matrix, in which the element $A_{ij}$
    represents the load of variable $x_{i}$ on factor $f_{j}$. It is proved that the
    parameter $A$ can be positive [[47](#bib.bib47)], the analysis operator learning
    can therefore be converted to an optimization problem on the positive manifold
    $\mathcal{M}$ as follows
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$ 表示算子载荷矩阵，其中元素 $A_{ij}$ 表示变量 $x_{i}$ 在因子 $f_{j}$ 上的载荷。已经证明参数 $A$ 可以是正的[[47](#bib.bib47)]，因此分析算子学习可以转化为在正流形
    $\mathcal{M}$ 上的优化问题，如下所示
- en: '|  | $\min_{A\in\mathcal{M}}\left\&#124;X-AF\right\&#124;_{2}^{2}.$ |  | (37)
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{A\in\mathcal{M}}\left\|X-AF\right\|_{2}^{2}.$ |  | (37) |'
- en: 3.5 Temporal Model
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 时间模型
- en: The temporal probability model is composed of a transition model describing
    the state evolution over time and a sensor model describing the observation process
    [[27](#bib.bib27)]. A temporal model is helpful to cope with filtering, prediction
    and smoothing. In the transition model, next state $z_{t+1}$ is transited from
    the current state $z_{t}$, independent from previous states. Given the time-relevant
    transition probability $A(t)$, the transition process of states can be modeled
    as
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 时间概率模型由描述状态随时间演变的转移模型和描述观测过程的传感器模型组成[[27](#bib.bib27)]。时间模型有助于处理滤波、预测和光滑。在转移模型中，下一状态
    $z_{t+1}$ 是从当前状态 $z_{t}$ 转移过来的，与之前的状态无关。给定时间相关的转移概率 $A(t)$，状态的转移过程可以被建模为
- en: '|  | $z_{t+1}=A(t)\cdot z_{t}+\epsilon(t),$ |  | (38) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | $z_{t+1}=A(t)\cdot z_{t}+\epsilon(t),$ |  | (38) |'
- en: where the noise $\epsilon(t)$ follows the Gaussian distribution.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其中噪声 $\epsilon(t)$ 遵循高斯分布。
- en: States are invisible and a hidden state can manifest as a specific observation
    with the help of an emission probability. The current observation $x_{t}$ is only
    defined by the current state $z_{t}$, having nothing to do with previous states
    and observations. Given a time-varying emission probability $C(t)$, the observation
    process can be modeled as
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 状态是不可见的，隐藏状态可以通过发射概率表现为特定的观测。当前观测 $x_{t}$ 仅由当前状态 $z_{t}$ 定义，与之前的状态和观测无关。给定时间变化的发射概率
    $C(t)$，观测过程可以被建模为
- en: '|  | $x_{t}=C(t)\cdot z_{t}+\delta(t),$ |  | (39) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t}=C(t)\cdot z_{t}+\delta(t),$ |  | (39) |'
- en: where the noise $\delta(t)$ follows the Gaussian distribution.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中噪声 $\delta(t)$ 遵循高斯分布。
- en: 'As a mixture of Equation ([38](#S3.E38 "In 3.5 Temporal Model ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) and Equation ([39](#S3.E39 "In
    3.5 Temporal Model ‣ 3 Applications in Classical Machine Learning ‣ A Survey of
    Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")),
    temporal models can be divided into hidden Markov models and linear dynamic systems.
    A hidden Markov model has discrete hidden state variables while the hidden state
    and observed variables of a linear dynamic system obey Gaussian distribution.
    Let $n$ represent the size of the temporal sequence, the expectation of observation
    sequences $E[x_{0},x_{1},x_{2}\cdots]$ can be deduced as:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '作为方程 ([38](#S3.E38 "In 3.5 Temporal Model ‣ 3 Applications in Classical Machine
    Learning ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")) 和方程 ([39](#S3.E39 "In 3.5 Temporal Model ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) 的混合，时间模型可以分为隐马尔可夫模型和线性动态系统。隐马尔可夫模型具有离散的隐藏状态变量，而线性动态系统的隐藏状态和观测变量遵循高斯分布。设
    $n$ 代表时间序列的大小，观测序列 $E[x_{0},x_{1},x_{2}\cdots]$ 的期望可以推导为：'
- en: '|  | $[C(t),C(t)A(t),C(t)A(t)^{2}\cdots C(t)A(t)^{n-1}]\,z_{0},$ |  | (40)
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $[C(t),C(t)A(t),C(t)A(t)^{2}\cdots C(t)A(t)^{n-1}]\,z_{0},$ |  | (40)
    |'
- en: 'where $z_{0}$ is the initial hidden state. It can be considered as a sequence
    of subspaces spanned by the emission and transition matrix columns at the corresponding
    time [[48](#bib.bib48)]. As is mentioned in Section [2.3](#S2.SS3 "2.3 Manifold
    Examples ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold"), a point on the
    Graßmann manifold is a subspace. Therefore, the temporal model can be mathematically
    optimized on the Graßmann manifold.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $z_{0}$ 是初始隐藏状态。它可以被视为由对应时间的发射和转移矩阵列所张成的子空间序列[[48](#bib.bib48)]。如[2.3](#S2.SS3
    "2.3 Manifold Examples ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")节中提到的，Graßmann
    流形上的一点是一个子空间。因此，时间模型可以在 Graßmann 流形上进行数学优化。'
- en: 4 Applications in Deep Learning
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度学习中的应用
- en: 'With the increasing attention to geometric optimization, more and more deep
    learning methods have developed to combine with it. Geometric optimization techniques
    vary with different deep learning backbones (e.g., CNN, RNN and GNN). Therefore,
    this section classifies applications in deep learning into the following categories,
    i.e., i) geometric CNN; ii) geometric RNN; iii) geometric GNN and iv) geometric
    optimization for other deep learning methods, such as transfer learning and optimal
    transport. Orthogonal manifold is widely employed in geometric CNNs to reduce
    feature redundancy. Examples include utilizing kernel orthogonality in Orthogonal
    CNNs [[6](#bib.bib6)], optimization on Submanifolds of Convolution Kernels in
    CNNs [[49](#bib.bib49)], and regularizing the convolution kernel with orthogonality
    when training deep CNNs [[12](#bib.bib12)]. In addition, geometric CNNs can leverage
    the unique structure of Stiefel manifold [[50](#bib.bib50)] and Graßmann manifold
    [[51](#bib.bib51)]. Geometric RNNs take advantage of the norm-keeping property
    of orthogonal and unitary manifolds to alleviate gradient explosion and vanishing
    problems. Examples include complex unitary matrices in Unitary Evolution Recurrent
    Neural Networks [[14](#bib.bib14)], and the special orthogonal group and unitary
    group in Cheap Orthogonal Constraints: A Simple Parameterization of the Orthogonal
    and Unitary Group [[13](#bib.bib13)]. Geometric GNNs pay much attention to hyperbolic
    manifold and extensively use it for structure capturing. Examples include the
    hyperbolic GNN [[52](#bib.bib52)] and a geometric neural network which incorporates
    Euclidean space with hyperbolic geometry [[53](#bib.bib53)].'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对几何优化的关注日益增加，越来越多的深度学习方法开始与之结合。几何优化技术在不同的深度学习骨干网络（如CNN、RNN和GNN）中有所不同。因此，本节将深度学习中的应用分类为以下几类：即i)
    几何CNN；ii) 几何RNN；iii) 几何GNN以及iv) 针对其他深度学习方法（如迁移学习和最优传输）的几何优化。正交流形在几何CNN中广泛应用以减少特征冗余。示例包括在正交CNN中利用核正交性[[6](#bib.bib6)]，在CNN中对卷积核的子流形进行优化[[49](#bib.bib49)]，以及在训练深度CNN时通过正交性正则化卷积核[[12](#bib.bib12)]。此外，几何CNN可以利用Stiefel流形[[50](#bib.bib50)]和Graßmann流形[[51](#bib.bib51)]的独特结构。几何RNN利用正交和单位流形的范数保持特性，以缓解梯度爆炸和消失问题。示例包括单位进化递归神经网络中的复单位矩阵[[14](#bib.bib14)]，以及在《便宜的正交约束：正交和单位群的简单参数化》中使用的特殊正交群和单位群[[13](#bib.bib13)]。几何GNN特别关注双曲流形，并广泛用于结构捕获。示例包括双曲GNN[[52](#bib.bib52)]和一个将欧几里得空间与双曲几何结合的几何神经网络[[53](#bib.bib53)]。
- en: 4.1 Geometric CNN
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 几何CNN
- en: Deep CNN has achieved great success in various computer vision tasks, such as
    image recognition [[54](#bib.bib54)] and segmentation tasks [[55](#bib.bib55)].
    CNN can automatically learn features from large-scale data by benefiting from
    three essential structures, i.e., convolution, activation, and pooling structures
    [[10](#bib.bib10)]. Although CNNs have worked efficiently, using the entire Euclidean
    space to search optimal solutions cause problems (e.g., training instability and
    feature redundancy) that hinder the further development. To alleviate these problems,
    geometric optimization approaches optimize CNNs on the suitable Riemannian manifold
    via kernel space, geometric regularization, and quasi-CNN architectures with parameters
    on the manifold.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 深度CNN在各种计算机视觉任务中取得了巨大成功，如图像识别[[54](#bib.bib54)]和分割任务[[55](#bib.bib55)]。CNN通过利用卷积、激活和池化这三种基本结构，从大规模数据中自动学习特征[[10](#bib.bib10)]。尽管CNN运作高效，但在整个欧几里得空间中搜索最优解会导致一些问题（例如，训练不稳定和特征冗余），这些问题阻碍了进一步的发展。为了解决这些问题，几何优化方法通过在合适的黎曼流形上进行核空间、几何正则化和带有流形参数的准CNN架构来优化CNN。
- en: Kernel Space. A low-dimensional manifold is often embedded in the high-dimensional
    Euclidean space. Kernel functions can map original features to a higher dimensional
    space. Therefore, with the help of kernel functions, computationally cheap operations
    on manifolds can represent complex operations in Euclidean space. Kernel spaces
    can be utilized and described by topological smooth manifolds. For example, positive-definite
    kernels, which are known as Graßmann kernels on the Graßmann manifold, can be
    used to map the manifold into a Hilbert space [[56](#bib.bib56)]. Zhang et al.
    [[57](#bib.bib57)] designed a new kind of Graßmann kernel based on canonical correlations
    to distinguish one class from others more accurately. Liu et al. [[58](#bib.bib58)]
    designed RBF kernels for linear subspace, covariance matrix, and Gaussian distribution
    to optimize emotion video recognition on the Riemannian manifolds. Hariri et al.
    [[59](#bib.bib59)] defined a kernel based on the SPD covariance matrix to indicate
    the similarity of two face images for face matching.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 核空间。低维流形通常嵌入在高维欧几里得空间中。核函数可以将原始特征映射到更高维的空间。因此，借助核函数，流形上的计算成本低的操作可以表示欧几里得空间中的复杂操作。核空间可以利用拓扑光滑流形进行描述。例如，正定核，也被称为Graßmann流形上的Graßmann核，可以用来将流形映射到希尔伯特空间[[56](#bib.bib56)]。Zhang等人[[57](#bib.bib57)]
    设计了一种基于典型相关性的新的Graßmann核，以更准确地区分不同类别。Liu等人[[58](#bib.bib58)] 设计了用于线性子空间、协方差矩阵和高斯分布的RBF核，以优化在黎曼流形上的情感视频识别。Hariri等人[[59](#bib.bib59)]
    定义了一种基于SPD协方差矩阵的核，用于指示两个面部图像的相似性，以进行人脸匹配。
- en: Kernel space constructed on nonlinear data helps learn the inherent manifold
    structure. Yuan et al. [[60](#bib.bib60)] combined manifold kernel space with
    deep learning architecture for scene recognition. To preserve the geometric structure
    of input scene images and achieve a greater representational ability, [[60](#bib.bib60)]
    defines a low-level feature layer $X$ and a hidden manifold kernel space $Y$ as
    a base unit. Moreover, the deep architecture is unit-by-unit and $Y_{k}$ serves
    as the input of another base unit to generate the next hidden space $Y_{k+1}$.
    Comparative experiments evaluate the performance of the manifold regularized deep
    network on the large-scale scene data set.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 基于非线性数据构建的核空间有助于学习固有的流形结构。Yuan等人[[60](#bib.bib60)] 将流形核空间与深度学习架构结合用于场景识别。为了保持输入场景图像的几何结构并实现更强的表示能力，[[60](#bib.bib60)]
    定义了低级特征层 $X$ 和一个隐藏的流形核空间 $Y$ 作为基础单元。此外，深度架构是逐单元构建的，$Y_{k}$ 作为另一个基础单元的输入，用于生成下一个隐藏空间
    $Y_{k+1}$。比较实验评估了流形正则化深度网络在大规模场景数据集上的性能。
- en: Ozay et al. [[49](#bib.bib49)] considered the kernel estimation problem in CNNs
    as an optimization on embedded or immersed submanifolds of kernels. [[49](#bib.bib49)]
    explores geometric properties of convolution kernel space in CNNs and reveals
    that different kernel normalization methods induce different geometric properties.
    For example, the orthonormal normalization manner implies Stiefel manifold, while
    kernels normalized with the unit-norm reside on the sphere manifold. Furthermore,
    [[49](#bib.bib49)] proposes an SGD algorithm for optimization on kernel submanifolds.
    Experiments carried out on three kernel submanifolds confirm that the above approach
    can boost the performance of traditional CNN training.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Ozay等人[[49](#bib.bib49)] 将CNN中的核估计问题视为对嵌入或浸入的核子流形的优化。[[49](#bib.bib49)] 探索了CNN中卷积核空间的几何性质，并揭示了不同的核归一化方法会引入不同的几何特性。例如，正交归一化方式意味着Stiefel流形，而归一化为单位范数的核则位于球面流形上。此外，[[49](#bib.bib49)]
    提出了一个针对核子流形的SGD优化算法。对三个核子流形进行的实验确认了上述方法可以提升传统CNN训练的性能。
- en: 'Geometric Regularization. Regularization acts as the penalty term of the optimization
    function. It is used to impose restrictions on the parameters of the optimization
    function. The commonly used geometric regularization is the orthogonal constraint,
    aiming to restrict parameters to be on the orthogonal manifold. Recall orthogonal
    matrices $W^{T}W=I$ introduced in Section [2.3](#S2.SS3 "2.3 Manifold Examples
    ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold"), orthogonal regularization
    methods are roughly divided into hard orthogonality as'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '几何正则化。正则化作为优化函数的惩罚项。它用于对优化函数的参数施加限制。常用的几何正则化是正交约束，旨在将参数限制在正交流形上。回顾在第[2.3](#S2.SS3
    "2.3 Manifold Examples ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")节中介绍的正交矩阵
    $W^{T}W=I$，正交正则化方法大致分为硬正交性，如下所示'
- en: '|  | $\&#124;W^{T}W-I\&#124;_{F}^{2}$ |  | (41) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | $\|W^{T}W-I\|_{F}^{2}$ |  | (41) |'
- en: and soft orthogonality as
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 和软正交性作为
- en: '|  | $\lambda\&#124;W^{T}W-I\&#124;_{F}^{2}$ |  | (42) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lambda\|W^{T}W-I\|_{F}^{2}$ |  | (42) |'
- en: where $\|\cdot\|_{F}$ indicates the Frobenius norm and $\lambda$ represents
    a relaxation coefficient. Based on the soft orthogonality, we can achieve double
    soft orthogonality as
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\|\cdot\|_{F}$ 表示 Frobenius 范数，$\lambda$ 表示一个松弛系数。基于软正交性，我们可以实现双重软正交性作为
- en: '|  | $\lambda(\&#124;W^{T}W-I\&#124;_{F}^{2}+\&#124;WW^{T}-I\&#124;_{F}^{2}).$
    |  | (43) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lambda(\|W^{T}W-I\|_{F}^{2}+\|WW^{T}-I\|_{F}^{2}).$ |  | (43) |'
- en: Based on the observation that the kernel orthogonality is necessary but insufficient
    for the orthogonal convolution, Wang et al. [[6](#bib.bib6)] proposed an approach
    where orthogonality constraints directly regularize a convolution layer. During
    training, the convolution filter $K$ is transformed into a Doubly Block-Toeplitz
    (DBT) matrix and the spectrum is regularized to be uniform, which requires row
    or column orthogonality. The orthogonality constraint on the DBT matrix helps
    relieve exploding and vanishing gradient problems, making the training more stable.
    Moreover, a number of experiments show that it can achieve amazing performance
    such as stronger robustness and better generalization.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 基于观察到核正交性对于正交卷积是必要但不充分的，Wang等人[[6](#bib.bib6)] 提出了一种方法，通过直接对卷积层进行正交性约束进行正则化。在训练过程中，卷积滤波器$K$被转换为双重块Toeplitz（DBT）矩阵，并且谱被正则化为均匀，这需要行或列的正交性。对DBT矩阵的正交性约束有助于缓解梯度爆炸和消失的问题，使训练更加稳定。此外，许多实验表明，它能够实现惊人的性能，例如更强的鲁棒性和更好的泛化能力。
- en: Bansal et al. [[12](#bib.bib12)] observed that orthogonality can stabilize the
    energy distribution of activations within CNNs and enhance the efficiency of training.
    [[12](#bib.bib12)] compares different orthogonality regularizers, e.g. soft orthogonality,
    double soft orthogonality and mutual coherence regularization that lowers the
    column correlation as much as possible to enforce orthogonality. Meanwhile, [[12](#bib.bib12)]
    designs a novel orthogonality regularizer named Spectral Restricted Isometry Property
    Regularization, which focuses on minimizing the spectral norm of $W^{T}W-I$. Remarkable
    experimental results suggest that regarding orthogonality regularizations as standard
    tools for training deep CNNs offers better accuracy and stablity.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Bansal等人[[12](#bib.bib12)] 观察到正交性可以稳定CNN中激活值的能量分布，并提高训练效率。[[12](#bib.bib12)]
    比较了不同的正交性正则化方法，例如软正交性、双重软正交性以及通过降低列相关性来强制正交性的互相关正则化。同时，[[12](#bib.bib12)] 设计了一种名为谱受限等距特性正则化的新型正交性正则化器，该正则化器专注于最小化
    $W^{T}W-I$ 的谱范数。显著的实验结果表明，将正交性正则化作为训练深度CNN的标准工具能够提供更好的准确性和稳定性。
- en: In order to estimate human face poses under challenging circumtances such as
    complex background or various orientations, Hong et al. [[61](#bib.bib61)] proposed
    manifold regularized convolutional layers (MRCL) to enhance the nonlinear locality
    constraints of CNN parameters. With MRCL being on top of traditional CNN’s pooling
    and activation operations, a low-rank manifold structure of latent data can be
    recovered for better optimization. By employing multitask learning with low-rank
    learning, multimodal of different data representations can be combined to predicate
    face postures. Comparative experiments validate the benefit of imposing manifold
    regularization to traditional convolutional layers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在复杂背景或各种方向等挑战性情况下估计人脸姿势，洪等人[[61](#bib.bib61)]提出了流形正则化卷积层（MRCL），以增强CNN参数的非线性局部约束。MRCL位于传统CNN的池化和激活操作之上，可以恢复潜在数据的低秩流形结构以实现更好的优化。通过采用低秩学习的多任务学习，可以将不同数据表示的多模态信息结合起来预测面部姿态。对比实验验证了对传统卷积层施加流形正则化的好处。
- en: Roufosse et al. [[62](#bib.bib62)] proposed a spectral unsupervised functional
    map network (SURFMNet) where the matching network from one shape to another is
    constrained to the orthogonal manifold. SURFMNet computes correspondences across
    3D shapes using unsupervised learning, i.e., building shape correspondences without
    ground truth. Solid experimental results support the consistent superiority of
    SURFMNet compared to state-of-the-art unsupervised shape matching methods. Experimental
    results also show that SURFMNet is comparable to supervised ones.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Roufosse等人[[62](#bib.bib62)]提出了一种谱无监督功能图网络（SURFMNet），其中形状匹配网络从一个形状到另一个形状的匹配被约束到正交流形。SURFMNet使用无监督学习计算3D形状之间的对应关系，即在没有真实数据的情况下建立形状对应关系。实验证据支持SURFMNet在与最先进的无监督形状匹配方法相比具有一致的优势。实验结果还显示SURFMNet与监督方法相当。
- en: Different from existing methods that shallowly learn Lie group features, Huang
    et al. [[63](#bib.bib63)] incorporated a Lie group structure to parameter matrices
    in the deep human action recognition network. The proposed skeleton-based human
    model $(V,E)$ is a binary relation, where $V$ represents a set of vertexes that
    consists of body joints $(v_{1},\dots,v_{N})$ and $E$ represents a set of edges
    that consists of body bones $(e_{1},\dots,e_{M})$. The rotation matrix is represented
    by the axis-angle model based on the skeleton and forms the special orthogonal
    group. To preserve the Lie group structure of the input rotation matrix, the above
    human action recognition network is optimized on the Lie group manifold and mapped
    to a tangent space for the final classification.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于现有的浅层学习Lie群特征的方法，黄等人[[63](#bib.bib63)]在深度人体动作识别网络中引入了Lie群结构到参数矩阵。所提议的基于骨架的人体模型$(V,E)$是一个二元关系，其中$V$代表由身体关节$(v_{1},\dots,v_{N})$组成的顶点集合，而$E$代表由身体骨骼$(e_{1},\dots,e_{M})$组成的边集合。旋转矩阵通过基于骨架的轴角模型表示，形成了特殊的正交群。为了保留输入旋转矩阵的Lie群结构，上述人体动作识别网络在Lie群流形上进行优化，并映射到切空间进行最终分类。
- en: Similar to the above action recognition network [[63](#bib.bib63)], Chen et
    al. [[64](#bib.bib64)] put forward a deep manifold learning (DML) framework to
    learn manifold information and deep representations of action videos. [[64](#bib.bib64)]
    studies that leveraging geometry information in deep learning contributes to high
    accuracy and efficiency for action recognition. To extract more expressing features,
    the DML framework applies a manifold regularizer on the previous layer, label
    information and manifold parameters. Furthermore, adapting the DML framework to
    restricted Boltzmann machine can relieve the overfitting problem and improve the
    recognition accuracy.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于上述动作识别网络[[63](#bib.bib63)]，陈等人[[64](#bib.bib64)]提出了一种深度流形学习（DML）框架，以学习动作视频的流形信息和深度表示。[[64](#bib.bib64)]研究表明，利用深度学习中的几何信息有助于提高动作识别的准确性和效率。为了提取更多表达特征，DML框架在前一层、标签信息和流形参数上应用了流形正则化器。此外，将DML框架适应于限制玻尔兹曼机可以缓解过拟合问题并提高识别准确性。
- en: Quasi-CNN Architecture. Kernel methods and orthogonal regularization do not
    change fundamental CNN components (e.g., convolution and pooling operations).
    Another method of applying geometric optimization to deep learning is to mimic
    traditional CNN architecture and establish a new architecture suitable for the
    manifold structure. In this article, the above architecture is named as quasi-CNN
    architecture. Convolution and activation layers are rebuilt to induce geometric
    optimization in the quasi-CNN architecture. To achieve this goal, parameters in
    the quasi-CNN architecture are designed to reside on the compact Stiefel manifold.
    For a more intuitive explanation, this article takes the deep SPD matrix network
    (SPDNet) [[50](#bib.bib50)] and deep Graßmann neural network architecture (GrNet)
    [[51](#bib.bib51)] as examples.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '拟CNN架构。核方法和正交正则化并不改变基本的CNN组件（例如，卷积和池化操作）。将几何优化应用于深度学习的另一种方法是模仿传统的CNN架构并建立一种适合流形结构的新架构。在本文中，上述架构被称为拟CNN架构。卷积和激活层被重建以在拟CNN架构中引入几何优化。为了实现这个目标，拟CNN架构中的参数被设计为位于紧致斯蒂弗尔流形上。为了更直观地解释，本文以深度SPD矩阵网络（SPDNet）[[50](#bib.bib50)]和深Graßmann神经网络架构（GrNet）[[51](#bib.bib51)]为例。 '
- en: Let $X$ be the input, and $W$ be the transformation parameter on the compact
    Stiefel manifold. First, SPDNet is designed for optimization on the SPD manifold.
    Bilinear mapping (BiMap) layer $f_{b}=WXW^{T}$ plays the role of convolution layers
    in traditional CNNs. Based on the eigenvalue decomposition $X$ = $U\Sigma U$,
    eigenvalue rectification (ReEig) layers $f_{r}=Umax(\epsilon I,\Sigma)U^{T}$ are
    designed to replace nonlinear activation layers and $\epsilon$ is the activation
    threshold. SPDNet designs the eigenvalue logarithm (LogEig) layer to flatten the
    Riemannian manifold to a flat space where classical Euclidean computations can
    be applied. GrNet is designed for optimization along the orthonormal manifold.
    Full rank mapping (FRMap) layers $f_{fr}=WX$ in GrNet replace the convolution
    layer in traditional CNNs. Inspired by the QR decomposition $X=QR$ where $Q$ is
    orthonormal, GrNet designs re-orthonormalization (ReOrth) layer $f_{fo}=XR^{-1}=Q$
    to achieve an orthonormal output. Unlike the LogEig layer in SPDnet, GrNet uses
    inner product $XX^{T}$ to reduce the manifold to a flat Euclidean space. After
    pooling operations on the resulting Euclidean data, GrNet designs orthonormal
    mapping (OrthMap) layer $f_{om}=U_{1:q}$ to transform the output matrix back to
    the orthonormal manifold, where $U_{1:q}$ denotes the first $q$ largest eigenvectors
    achieved by the eigenvalue decomposition.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让$X$作为输入，$W$作为紧致斯蒂弗尔流形上的转换参数。首先，SPDNet是为在SPD流形上进行优化而设计的。双线性映射（BiMap）层$f_{b}=WXW^{T}$扮演着传统CNN中卷积层的角色。基于特征值分解$X$
    = $U\Sigma U$，特征值修正（ReEig）层$f_{r}=Umax(\epsilon I,\Sigma)U^{T}$被设计来替换非线性激活层，其中$\epsilon$是激活阈值。SPDNet设计了特征值对数（LogEig）层以将黎曼流形展平到一个平坦空间，从而可以应用经典的欧氏计算。GrNet是为了在正交流形上进行优化而设计的。全秩映射（FRMap）层$f_{fr}=WX$在GrNet中取代了传统CNN中的卷积层。受QR分解$X=QR$的启发，其中$Q$是正交矩阵，GrNet设计了重正交化（ReOrth）层$f_{fo}=XR^{-1}=Q$以实现正交输出。与SPDnet中的LogEig层不同，GrNet使用内积$XX^{T}$将流形降低到一个平坦的欧氏空间。在对得到的欧氏数据进行池化操作后，GrNet设计了正交映射（OrthMap）层$f_{om}=U_{1:q}$以将输出矩阵转换回正交流形，其中$U_{1:q}$表示特征值分解获得的前$q$个最大特征向量。
- en: 4.2 Geometric RNN
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 几何循环神经网络
- en: 'RNNs are designed to process sequential data since they can capture spatial
    and temporal dependencies between the sequential input. Therefore, RNN can be
    applied in tasks such as speech recognition, text prediction, and machine translation.
    Given an input sequence $X_{\tau}={x_{1},x_{2},\cdots,x_{\tau}}$ ($x_{i}\in\mathbb{R}^{n}$)
    with length $\tau$, a basic RNN framework is aimed to generate the output sequence
    $Y_{\tau}={y_{1},y_{2},\cdots,y_{\tau}}$ ($y_{i}\in\mathbb{R}^{p}$). With hidden
    state $h$ passed recurrently into the model at each time step, output predictions
    $o_{i}\in\mathbb{R}^{p}$ of the RNN are computed as follows [[65](#bib.bib65)]:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是为了处理序列数据而设计的，因为它们可以捕捉顺序输入之间的空间和时间依赖关系。因此，RNN可以应用于诸如语音识别、文本预测和机器翻译等任务。给定长度为$\tau$的输入序列$X_{\tau}={x_{1},x_{2},\cdots,x_{\tau}}$
    ($x_{i}\in\mathbb{R}^{n}$)，基本RNN框架旨在生成长度为$\tau$的输出序列$Y_{\tau}={y_{1},y_{2},\cdots,y_{\tau}}$
    ($y_{i}\in\mathbb{R}^{p}$)。通过隐藏状态$h$在每个时间步长递归地传递到模型中，RNN的输出预测$o_{i}\in\mathbb{R}^{p}$如下计算[[65](#bib.bib65)]：
- en: '|  | $\displaystyle h_{i}$ | $\displaystyle=\sigma(Ux_{i}+Wh_{i-1}+b),$ |  |
    (44) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| | $\displaystyle h_{i}$ | $\displaystyle=\sigma(Ux_{i}+Wh_{i-1}+b),$ | |
    (44) |'
- en: '|  | $\displaystyle o_{i}$ | $\displaystyle=Vh_{i}+c,$ |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle o_{i}$ | $\displaystyle=Vh_{i}+c,$ |  |'
- en: where $U\in\mathbb{R}^{m\times n}$ is the input weight matrix, $W\in\mathbb{R}^{m\times
    m}$ is the recurrent weight matrix, $h_{i-1}\in\mathbb{R}^{m}$ is the previous
    hidden state, $b\in\mathbb{R}^{m}$ is the input bias, $\sigma(\cdot)$ is a pointwise
    nonlinearity function, $h_{i}\in\mathbb{R}^{m}$ is the current hidden state, $V\in\mathbb{R}^{p\times
    m}$ is the output weight matrix, and $c\in\mathbb{R}^{p}$ is the output bias.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $U\in\mathbb{R}^{m\times n}$ 是输入权重矩阵，$W\in\mathbb{R}^{m\times m}$ 是递归权重矩阵，$h_{i-1}\in\mathbb{R}^{m}$
    是前一个隐藏状态，$b\in\mathbb{R}^{m}$ 是输入偏置，$\sigma(\cdot)$ 是一个点对点的非线性函数，$h_{i}\in\mathbb{R}^{m}$
    是当前隐藏状态，$V\in\mathbb{R}^{p\times m}$ 是输出权重矩阵，$c\in\mathbb{R}^{p}$ 是输出偏置。
- en: 4.2.1 Orthogonal RNN (ORNN)
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 正交 RNN (ORNN)
- en: 'Denote $\mathcal{L}$ as the objective function to be minimized, the gradient
    of the loss function for the hidden state is computed as:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\mathcal{L}$ 为需要最小化的目标函数，隐藏状态的损失函数的梯度计算为：
- en: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial h_{i}}$ | $\displaystyle=\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\cdot\frac{\partial h_{\tau}}{\partial h_{i}}=\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\cdot\prod_{j=i}^{\tau-1}\frac{\partial h_{j+1}}{\partial h_{j}}=\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}(\prod_{j=i}^{\tau-1}D_{j+1}W^{T}),$ |  | (45) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial h_{i}}$ | $\displaystyle=\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\cdot\frac{\partial h_{\tau}}{\partial h_{i}}=\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\cdot\prod_{j=i}^{\tau-1}\frac{\partial h_{j+1}}{\partial h_{j}}=\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}(\prod_{j=i}^{\tau-1}D_{j+1}W^{T}),$ |  | (45) |'
- en: 'where $D_{j+1}\in\mathbb{R}^{m\times m}$ is a diagonal matrix, whose entries
    consist of the derivate of the activation function. The pointwise non-linearity
    function $\sigma(\cdot)$ in Equation ([44](#S4.E44 "In 4.2 Geometric RNN ‣ 4 Applications
    in Deep Learning ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) is suggested to be a rectified linear
    unit (ReLU) function [[66](#bib.bib66), [67](#bib.bib67)], whose output has a
    minimum value of $0$. The input $D_{j+1}$ has at least one non-zero entry of the
    derivative value for all $j$. Taking the Euclidean $l_{2}-norm$ to both sides
    of Equation ([45](#S4.E45 "In 4.2.1 Orthogonal RNN (ORNN) ‣ 4.2 Geometric RNN
    ‣ 4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")), we have:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D_{j+1}\in\mathbb{R}^{m\times m}$ 是一个对角矩阵，其元素由激活函数的导数组成。公式中点对点的非线性函数 $\sigma(\cdot)$
    ([44](#S4.E44 "在 4.2 几何 RNN ‣ 4 深度学习应用 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形")) 被建议使用修正线性单元（ReLU）函数
    [[66](#bib.bib66), [67](#bib.bib67)]，其输出的最小值为 $0$。输入 $D_{j+1}$ 在所有 $j$ 中至少有一个非零的导数值。对公式
    ([45](#S4.E45 "在 4.2.1 正交 RNN (ORNN) ‣ 4.2 几何 RNN ‣ 4 深度学习应用 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形"))
    两边进行欧几里得 $l_{2}-norm$ 操作，我们得到：
- en: '|  | $\displaystyle\left\&#124;\frac{\partial\mathcal{L}}{\partial h_{i}}\right\&#124;_{2}$
    | $\displaystyle\leqslant(\prod_{j=i}^{\tau-1}\left\&#124;D_{j+1}W^{T}\right\&#124;_{2})\left\&#124;\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\right\&#124;_{2}=(\prod_{j=i}^{\tau-1}\left\&#124;W\right\&#124;_{2})\left\&#124;\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\right\&#124;_{2}$ |  | (46) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left\&#124;\frac{\partial\mathcal{L}}{\partial h_{i}}\right\&#124;_{2}$
    | $\displaystyle\leqslant(\prod_{j=i}^{\tau-1}\left\&#124;D_{j+1}W^{T}\right\&#124;_{2})\left\&#124;\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\right\&#124;_{2}=(\prod_{j=i}^{\tau-1}\left\&#124;W\right\&#124;_{2})\left\&#124;\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\right\&#124;_{2}$ |  | (46) |'
- en: 'If $\|W\|_{2}$ is greater than one, $\left\|\frac{\partial\mathcal{L}}{\partial
    h_{i}}\right\|_{2}$ grows exponentially as the increase of $\tau$. As a result,
    the norm of the gradient in Equation ([46](#S4.E46 "In 4.2.1 Orthogonal RNN (ORNN)
    ‣ 4.2 Geometric RNN ‣ 4 Applications in Deep Learning ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold"))
    disclosures the well-known gradient exploding problem that hinders the RNN from
    training [[68](#bib.bib68)]. If $\|W\|_{2}$ is smaller than one, $\left\|\frac{\partial\mathcal{L}}{\partial
    h_{i}}\right\|_{2}$ declines exponentially as the increase of $\tau$, which leads
    to gradient vanishing problems [[68](#bib.bib68)].'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $\|W\|_{2}$ 大于一，$\left\|\frac{\partial\mathcal{L}}{\partial h_{i}}\right\|_{2}$
    会随着 $\tau$ 的增加而指数级增长。因此，方程中 $\left\|\frac{\partial\mathcal{L}}{\partial h_{i}}\right\|_{2}$
    的范数揭示了著名的梯度爆炸问题，这会阻碍 RNN 的训练 [[68](#bib.bib68)]。如果 $\|W\|_{2}$ 小于一，$\left\|\frac{\partial\mathcal{L}}{\partial
    h_{i}}\right\|_{2}$ 会随着 $\tau$ 的增加而指数级下降，这导致梯度消失问题 [[68](#bib.bib68)]。
- en: 'A recent line of ORNNs imposes the orthogonal constraint on the hidden-to-hidden
    transformation of RNN. The recurrent weight transformation matrix $W$ is restricted
    to be on the orthogonal manifold. Let $A$ be an orthogonal matrix, for each vector
    $X$, its norm after orthogonal transformation is:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最近一系列的 ORNN 对 RNN 的隐藏到隐藏变换施加了正交约束。递归权重变换矩阵 $W$ 被限制在正交流形上。设 $A$ 为一个正交矩阵，对于每个向量
    $X$，其经过正交变换后的范数为：
- en: '|  | $(AX)^{T}(AX)=X^{T}A^{T}AX=X^{T}X,$ |  | (47) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | $(AX)^{T}(AX)=X^{T}A^{T}AX=X^{T}X,$ |  | (47) |'
- en: 'which means that orthogonal transformations do not change the norm of the original
    vector. As a result, $\left\|\frac{\partial\mathcal{L}}{\partial h_{i}}\right\|_{2}$
    can remain invariant in ORNN when the transformation matrix $W$ in Equation ([46](#S4.E46
    "In 4.2.1 Orthogonal RNN (ORNN) ‣ 4.2 Geometric RNN ‣ 4 Applications in Deep Learning
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold")) is orthogonal. Therefore, the exploding and vanishing gradient
    problem of RNN can be alleviated. Moreover, orthogonal constraints can be generalized
    to unitary constraints in the complex domain.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着正交变换不会改变原始向量的范数。因此，当方程中的变换矩阵 $W$ ([46](#S4.E46 "在 4.2.1 正交 RNN (ORNN) ‣
    4.2 几何 RNN ‣ 4 深度学习中的应用 ‣ 深度学习几何优化的调查：从欧几里得空间到黎曼流形")) 是正交的时，$\left\|\frac{\partial\mathcal{L}}{\partial
    h_{i}}\right\|_{2}$ 可以在 ORNN 中保持不变。因此，RNN 的梯度爆炸和消失问题可以得到缓解。此外，正交约束可以在复数域中推广到单位ary
    约束。
- en: 4.2.2 Recent Advances of ORNN
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 ORNN 的最新进展
- en: uRNN [[14](#bib.bib14)] constructs a large unitary matrix by simple parametric
    unitary matrices, i.e., the unitary hidden-to-hidden matrix $W$ is composed as
    follows,
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: uRNN [[14](#bib.bib14)] 通过简单的参数单位ary 矩阵构造了一个大型单位ary 矩阵，即，单位ary 隐藏到隐藏矩阵 $W$ 由以下部分组成，
- en: '|  | $\displaystyle W=D_{3}R_{2}F^{-1}D_{2}\Pi R_{1}FD_{1},$ |  | (48) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle W=D_{3}R_{2}F^{-1}D_{2}\Pi R_{1}FD_{1},$ |  | (48) |'
- en: 'where $D$ is a diagonal matrix whose diagonal element $D_{j,j}=e^{iw_{j}}$
    is defined by the imaginary unit $i$ and parameters $w_{j}\in\mathbb{R}$, $R=I-\
    2\frac{vv^{\ast}}{\|v\|^{2}}$ is a reflection matrix with the complex vector $v\in\mathbb{C}^{n}$,
    $\Pi$ is a fixed random index permutation matrix, and $F$ and $F^{-1}$ are the
    Fourier and inverse Fourier transforms. In the matrix construction strategy like
    Equation ([48](#S4.E48 "In 4.2.2 Recent Advances of ORNN ‣ 4.2 Geometric RNN ‣
    4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")), the number of parameters,
    memory, and computational overhead increase slowly at approximately linear speeds.
    Therefore, the training cost of large hidden layers can be reduced. In uRNN, a
    variation of the nonlinear activation ReLU named modReLU has been proposed to
    maintain the phase of complex-valued hidden states:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D$ 是一个对角矩阵，其对角元素 $D_{j,j}=e^{iw_{j}}$ 由虚数单位 $i$ 和参数 $w_{j}\in\mathbb{R}$
    定义，$R=I-\ 2\frac{vv^{\ast}}{\|v\|^{2}}$ 是一个反射矩阵，$v\in\mathbb{C}^{n}$ 是复数向量，$\Pi$
    是一个固定的随机索引置换矩阵，$F$ 和 $F^{-1}$ 分别是傅里叶变换和反傅里叶变换。在类似于方程中的矩阵构造策略中 ([48](#S4.E48 "在
    4.2.2 ORNN 的最新进展 ‣ 4.2 几何 RNN ‣ 4 深度学习中的应用 ‣ 深度学习几何优化的调查：从欧几里得空间到黎曼流形"))，参数、内存和计算开销的增加速度大致为线性。因此，可以降低大隐藏层的训练成本。在
    uRNN 中，提出了一种名为 modReLU 的非线性激活函数 ReLU 的变体，以保持复数值隐藏状态的相位：
- en: '|  | $\sigma_{modReLU}(z)=\left\{\begin{aligned} &amp;(&#124;z&#124;+b)\frac{z}{&#124;z&#124;},\quad&amp;if\
    &#124;z&#124;+b\geq 0\\ &amp;0,\quad&amp;if\ &#124;z&#124;+b\leq 0\end{aligned}\right.$
    |  | (49) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma_{modReLU}(z)=\left\{\begin{aligned} &amp;(&#124;z&#124;+b)\frac{z}{&#124;z&#124;},\quad&amp;如果\
    &#124;z&#124;+b\geq 0\\ &amp;0,\quad&amp;如果\ &#124;z&#124;+b\leq 0\end{aligned}\right.$
    |  | (49) |'
- en: where $b$ $\in\mathbb{R}$ is a bias parameter. uRNN defines a matrix $U$ to
    map complex-valued hidden state $h_{t}$ to real-valued output for prediction.
    The corresponding loss function is calculated as follows
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b$ $\in\mathbb{R}$ 是一个偏置参数。uRNN 定义一个矩阵 $U$ 将复值隐藏状态 $h_{t}$ 映射到实值输出以进行预测。相应的损失函数计算如下：
- en: '|  | $o_{t}=U\left(\begin{aligned} Re(h_{t})\\ Im(h_{t})\end{aligned}\right)+b_{o},$
    |  | (50) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $o_{t}=U\left(\begin{aligned} Re(h_{t})\\ Im(h_{t})\end{aligned}\right)+b_{o},$
    |  | (50) |'
- en: where $b_{o}$ is the output bias, $Re(h_{t})$ and $Im(h_{t})$ represent the
    real and imaginary part of $h_{t}$ respectively.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b_{o}$ 是输出偏置，$Re(h_{t})$ 和 $Im(h_{t})$ 分别表示 $h_{t}$ 的实部和虚部。
- en: 'However, Wisdom et al. [[69](#bib.bib69)] noticed that the unitary parameter
    construction of Equation ([48](#S4.E48 "In 4.2.2 Recent Advances of ORNN ‣ 4.2
    Geometric RNN ‣ 4 Applications in Deep Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")) cannot cover
    all $N\times N$ unitary matrices for $N>7$, i.e., at least one $N\times N$ unitary
    matrix cannot be represented in the form of Equation ([48](#S4.E48 "In 4.2.2 Recent
    Advances of ORNN ‣ 4.2 Geometric RNN ‣ 4 Applications in Deep Learning ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")). To address this problem, [[69](#bib.bib69)] designs a method to measure
    the representation capacity of the structured $N\times N$ unitary matrix. [[69](#bib.bib69)]
    comes up with a perspective that the unitary matrices parameterized by $P$ real-valued
    parameters for $P\geq N^{2}$ is full-capacity, which means that it can cover all
    $N\times N$ unitary matrices.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Wisdom 等人 [[69](#bib.bib69)] 注意到方程 ([48](#S4.E48 "在 4.2.2 近期进展的 ORNN ‣ 4.2
    几何 RNN ‣ 4 深度学习中的应用 ‣ 从欧几里得空间到黎曼流形的几何优化调查")) 的单位参数构造不能覆盖所有 $N\times N$ 单位矩阵，对于
    $N>7$ 的情况，即至少存在一个 $N\times N$ 单位矩阵不能以方程 ([48](#S4.E48 "在 4.2.2 近期进展的 ORNN ‣ 4.2
    几何 RNN ‣ 4 深度学习中的应用 ‣ 从欧几里得空间到黎曼流形的几何优化调查")) 的形式表示。为了解决这个问题，[[69](#bib.bib69)]
    设计了一种方法来测量结构化 $N\times N$ 单位矩阵的表示能力。[[69](#bib.bib69)] 提出了一个观点，即由 $P$ 个实值参数参数化的单位矩阵对于
    $P\geq N^{2}$ 是全容量的，这意味着它可以覆盖所有 $N\times N$ 单位矩阵。
- en: Unlike generating compound orthogonal matrices with simple ones, the Lie exponential
    map can achieve orthogonal constraint on the hidden-to-hidden transformation [[13](#bib.bib13)].
    The connected subjective exponential mapping $exp:\mathfrak{g}$ $\rightarrow$
    G on the special orthogonal group is defined as
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 与生成复合正交矩阵不同，李代数指数映射可以实现隐藏到隐藏变换的正交约束 [[13](#bib.bib13)]。特殊正交群上的连通主观指数映射 $exp:\mathfrak{g}$
    $\rightarrow$ G 定义为：
- en: '|  | $\displaystyle exp(A):=I\ +\ A+\ \frac{1}{2}A^{2}+\ \dots.$ |  | (51)
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle exp(A):=I\ +\ A+\ \frac{1}{2}A^{2}+\ \dots.$ |  | (51)
    |'
- en: Since it is a subjection, for each hidden-to-hidden transformation matrix $W$
    belonging to the special orthogonal group or unitary group, there must exist a
    skew-symmetric (or skew-Hermitian matrix) $A$ that satisfies $exp(A)=W$. Therefore,
    the hidden-to-hidden transformation $h_{t+1}=\sigma(Wh_{t}\ +Tx_{t+1})$ is equivalent
    to $h_{t+1}=\sigma(exp(A)h_{t}\ +Tx_{t+1})$. That is, the optimization on the
    orthogonal or unitary manifold can be transformed to the optimization in Euclidean
    space. Consequently, classic gradient descent optimizers such as Adam can be applied
    to minimize the loss function as well as satisfying orthogonal constraints. As
    a result, the Lie exponential map can achieve both cheap computation overhead
    and the mitigation of gradient exploding and vanishing problems.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个主观问题，对于每一个属于特殊正交群或单位群的隐藏到隐藏的变换矩阵 $W$，必须存在一个满足 $exp(A)=W$ 的反对称（或反厄米矩阵）$A$。因此，隐藏到隐藏的变换
    $h_{t+1}=\sigma(Wh_{t}\ +Tx_{t+1})$ 等价于 $h_{t+1}=\sigma(exp(A)h_{t}\ +Tx_{t+1})$。也就是说，正交或单位流形上的优化可以转化为欧几里得空间中的优化。因此，经典的梯度下降优化器如
    Adam 可以应用于最小化损失函数，同时满足正交约束。因此，李代数指数映射可以实现低计算开销并缓解梯度爆炸和消失问题。
- en: Another method [[70](#bib.bib70)], which is based on the Lie group, defines
    a basis $\{T_{j}\}_{j=\{1,\cdots,n^{2}\}}$ and coefficients $\{\lambda_{j}\}_{j=\{1,\cdots,n^{2}\}}$
    to construct the element $L\in\mathfrak{u}(n)$ as follows,
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法 [[70](#bib.bib70)] 基于李群，定义了一个基 $\{T_{j}\}_{j=\{1,\cdots,n^{2}\}}$ 和系数
    $\{\lambda_{j}\}_{j=\{1,\cdots,n^{2}\}}$ 来构造元素 $L\in\mathfrak{u}(n)$，如下所示
- en: '|  | $L=\Sigma_{j=1}^{n^{2}}\lambda_{j}T_{j}.$ |  | (52) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $L=\Sigma_{j=1}^{n^{2}}\lambda_{j}T_{j}.$ |  | (52) |'
- en: 'By using exponential mapping, the element $U$ of corresponding unitary Lie
    group $U(n)$ can be represented as:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用指数映射，对应单位李群 $U(n)$ 的元素 $U$ 可以表示为：
- en: '|  | $U=exp(L)=exp(\Sigma_{j=1}^{n^{2}}\lambda_{j}T_{j}).$ |  | (53) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $U=exp(L)=exp(\Sigma_{j=1}^{n^{2}}\lambda_{j}T_{j}).$ |  | (53) |'
- en: Furthermore, Hyland et al. [[70](#bib.bib70)] offered an argument that the above
    parameterization helps generalize unitary RNN to arbitrary unitary matrices and
    figure out long-memory tasks.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Hyland 等人 [[70](#bib.bib70)] 提出了一个论点，认为上述参数化有助于将单位RNN推广到任意单位矩阵，并解决长时记忆任务。
- en: 'Learning orthogonal filters in deep neural networks (DNN) can be formulated
    as an optimization problem over multiple dependent Stiefel manifolds (OMDSM) [[71](#bib.bib71)].
    The orthogonal linear module can substitute standard linear module in DNNs to
    stabilize the distributions of activation and regularize networks. Let $W_{k}$
    and $b_{k}$ be learnable weight matrix and bias, parameter $\theta$ be $\{W_{k},b_{k}|k=1,2,\dots
    K\}$, the deep neural network can be represented as $f(x,\theta):x\rightarrow\hat{y}$,
    where $x$ is the input feature, and $\hat{y}$ is the output prediction of DNN.
    The loss function is often designed as the discrepancy between label $y$ and prediction
    values: $\mathcal{L}(y,f(x,\theta))$. Finally, the optimization problem is formulated
    as'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络（DNN）中学习正交滤波器可以被表述为一个优化问题，涉及多个相关的 Stiefel 流形（OMDSM） [[71](#bib.bib71)]。正交线性模块可以替代
    DNN 中的标准线性模块，以稳定激活分布并规范化网络。令 $W_{k}$ 和 $b_{k}$ 为可学习的权重矩阵和偏置，参数 $\theta$ 为 $\{W_{k},b_{k}|k=1,2,\dots
    K\}$，深度神经网络可以表示为 $f(x,\theta):x\rightarrow\hat{y}$，其中 $x$ 是输入特征，$\hat{y}$ 是 DNN
    的输出预测。损失函数通常设计为标签 $y$ 和预测值之间的差异：$\mathcal{L}(y,f(x,\theta))$。最终，优化问题被表述为
- en: '|  | $\displaystyle\theta^{\ast}=argmin_{\theta}\mathbb{E}_{(x,y)\in D}[\mathcal{L}(y,f(x,\theta))].$
    |  | (54) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{\ast}=argmin_{\theta}\mathbb{E}_{(x,y)\in D}[\mathcal{L}(y,f(x,\theta))].$
    |  | (54) |'
- en: OMDSM trains DNN with orthogonal weight matrix $W_{k}$ in each layer. Thus,
    the optimization problem is reformulated as
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: OMDSM 在每一层使用正交权重矩阵 $W_{k}$ 训练深度神经网络（DNN）。因此，优化问题被重新表述为
- en: '|  | $\displaystyle\theta^{\ast}=argmin_{\theta}\mathbb{E}_{(x,y)\in D}[\mathcal{L}(y,f(x,\theta))]$
    |  | (55) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{\ast}=argmin_{\theta}\mathbb{E}_{(x,y)\in D}[\mathcal{L}(y,f(x,\theta))]$
    |  | (55) |'
- en: '|  | $\displaystyle s.t.\ W_{k}\in\mathbb{O}_{k}^{n_{k}\times d_{k}},k=1,2,\dots
    K,$ |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s.t.\ W_{k}\in\mathbb{O}_{k}^{n_{k}\times d_{k}},k=1,2,\dots
    K,$ |  |'
- en: where the matrix family $\mathbb{O}_{k}^{n_{k}\times d_{k}}=\{W_{k}\in\mathbb{R}^{n_{k}\times
    d_{k}}|W_{k}W_{k}^{T}=I\}$ is composed of multiple real Stiefel manifolds, which
    is an embedded sub-manifold of $\mathbb{R}^{n_{k}\times d_{k}}$. Each independent
    orthogonal filter $W\in\mathbb{R}^{n\times d}$ is given by the proxy parameter
    $V\in\mathbb{R}^{n\times d}$ as
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 其中矩阵族 $\mathbb{O}_{k}^{n_{k}\times d_{k}}=\{W_{k}\in\mathbb{R}^{n_{k}\times
    d_{k}}|W_{k}W_{k}^{T}=I\}$ 由多个真实的 Stiefel 流形组成，这些流形是 $\mathbb{R}^{n_{k}\times
    d_{k}}$ 的一个嵌入子流形。每个独立的正交滤波器 $W\in\mathbb{R}^{n\times d}$ 由代理参数 $V\in\mathbb{R}^{n\times
    d}$ 给出，如下所示
- en: '|  | $W=PV,$ |  | (56) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $W=PV,$ |  | (56) |'
- en: 'where $n$ is the number of output channels, $d$ is the number of input channels,
    and $P\in\mathbb{R}^{n\times n}$ is the coefficient of the linear transformation.
    Firstly, $V$ is centered by $V_{C}$:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n$ 是输出通道的数量，$d$ 是输入通道的数量，$P\in\mathbb{R}^{n\times n}$ 是线性变换的系数。首先，通过 $V_{C}$
    对 $V$ 进行中心化：
- en: '|  | $V_{C}=V-c{1_{d}^{T}},$ |  | (57) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | $V_{C}=V-c{1_{d}^{T}},$ |  | (57) |'
- en: 'where $c=\frac{1}{d}V1_{d}$ and $1_{d}$ is the $d$-dimension vector with all
    ones. Moreover, the eigenvalues $\wedge$ and eigenvectors $D$ of the covariance
    matrix $V_{C}{V_{C}}^{T}$ are used to construct $P$:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c=\frac{1}{d}V1_{d}$，$1_{d}$ 是全为 1 的 $d$ 维向量。此外，协方差矩阵 $V_{C}{V_{C}}^{T}$
    的特征值 $\wedge$ 和特征向量 $D$ 用于构造 $P$：
- en: '|  | $P=D\wedge^{-1/2}D^{T}.$ |  | (58) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $P=D\wedge^{-1/2}D^{T}.$ |  | (58) |'
- en: Finally, $W$ is formulated as
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，$W$ 被表述为
- en: '|  | $W=D\wedge^{-1/2}D^{T}V_{C}.$ |  | (59) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $W=D\wedge^{-1/2}D^{T}V_{C}.$ |  | (59) |'
- en: Research has been conducted on exploring the influence of soft orthogonal constraints
    [[72](#bib.bib72)]. By allowing the diagonal elements of $S$ to float around 1,
    the orthogonal transformation matrix $W$ is relaxed as
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 已有研究探讨了软正交约束的影响 [[72](#bib.bib72)]。通过允许 $S$ 的对角元素在 1 附近浮动，正交变换矩阵 $W$ 被放宽为
- en: '|  | $W=USV^{T},$ |  | (60) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | $W=USV^{T},$ |  | (60) |'
- en: where $U$ and $V$ are strict orthogonal matrices.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $U$ 和 $V$ 是严格的正交矩阵。
- en: The above methods are mainly subject to $O(n^{3})$ time complexity or dependent
    on complex matrices [[73](#bib.bib73)]. It is discovered that orthogonal matrices
    $W\subseteq O(2n)$ with doubled hidden size, can substitute complex or unitary
    matrices in $\mathbb{C}^{n\times n}$. Inspired by the above discovery, [[73](#bib.bib73)]
    proposed to utilize Householder matrices to achieve parametrization of orthogonal
    transition matrices. As a result, complex matrices are unneeded and time complexity
    is reduced, while the effect is similar to the unitary constraint.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法主要受到 $O(n^{3})$ 时间复杂度的限制，或者依赖于复杂矩阵 [[73](#bib.bib73)]。发现正交矩阵 $W\subseteq
    O(2n)$ 具有双倍的隐藏尺寸，可以替代 $\mathbb{C}^{n\times n}$ 中的复杂或单位矩阵。受上述发现的启发，[[73](#bib.bib73)]
    提出了利用 Householder 矩阵实现正交过渡矩阵的参数化。因此，复杂矩阵不再需要，时间复杂度降低，而效果类似于单位约束。
- en: The norm-keeping property of orthogonal matrices may make ORNN have difficulty
    paying little attention to extraneous information [[74](#bib.bib74)]. To relieve
    this problem, Jing et al. [[74](#bib.bib74)] put forward the gated orthogonal
    recurrent unit (GORU) to be unconcerned with irrelevant or noise information while
    learning long-term dependencies. By adding the gating mechanism, experiment results
    demonstrate that GORU outperforms the unitary RNN on natural language processing
    tasks such as question answering tasks, together with long-term dependency tasks
    such as denoising and copying tasks.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 正交矩阵的保持范数特性可能使 ORNN 难以对无关信息保持较低关注 [[74](#bib.bib74)]。为缓解这个问题，Jing 等人 [[74](#bib.bib74)]
    提出了门控正交递归单元（GORU），以在学习长期依赖性时忽略无关或噪声信息。通过添加门控机制，实验结果表明 GORU 在自然语言处理任务（如问答任务）以及长期依赖任务（如去噪和复制任务）中优于单位
    RNN。
- en: In summary, uRNN [[14](#bib.bib14)] parameterizes the unitary hidden-to-hidden
    matrix by composing simple unitary matrices. However, the above parameterization
    cannot cover all $N\times N$ unitary matrices. To make up for that, full-capacity
    uRNN [[69](#bib.bib69)] is put forward. Unlike uRNN, expRNN [[13](#bib.bib13)]
    exploits the exponential map to achieve orthogonal constraints more easily. Furthermore,
    OMDSM innovatively uses re-parameterization to optimize DNN over multiple dependent
    Stiefel manifolds instead of one manifold [[71](#bib.bib71)]. Moreover, research
    has explored whether and how the hard orthogonal constraints on RNN can be relaxed
    [[72](#bib.bib72)]. By creatively introducing the householder matrix, the considerable
    time complexity of parameterizing unitary matrices can be mitigated [[73](#bib.bib73)].
    Last but not least, GORU [[74](#bib.bib74)] designs a forget gate, so that ORNN
    can pay little attention to extraneous information.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，uRNN [[14](#bib.bib14)] 通过组合简单的单位矩阵来参数化单位隐藏到隐藏矩阵。然而，上述参数化无法涵盖所有 $N\times
    N$ 单位矩阵。为弥补这一点，提出了全容量 uRNN [[69](#bib.bib69)]。与 uRNN 不同，expRNN [[13](#bib.bib13)]
    利用指数映射更容易实现正交约束。此外，OMDSM 创新性地使用重新参数化来优化 DNN 在多个相关 Stiefel 流形上，而不是一个流形 [[71](#bib.bib71)]。此外，研究探讨了如何放宽
    RNN 上的硬正交约束 [[72](#bib.bib72)]。通过创造性地引入 Householder 矩阵，可以减轻参数化单位矩阵的巨大时间复杂度 [[73](#bib.bib73)]。最后但同样重要的是，GORU
    [[74](#bib.bib74)] 设计了一个遗忘门，使 ORNN 可以减少对无关信息的关注。
- en: 4.3 Geometric GNN
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 几何 GNN
- en: GNN can be used to construct a learning network based on irregular graphs. Each
    graph is represented by vertexes and edges, which describes the relationship between
    vertexes. GNN encodes vertexes as feature vectors and models edges as a relationship
    matrix between vertexes. In GNN, graph convolution is performed between the relationship
    matrix and the feature matrix. Therefore, GNN can take advantage of the graph
    structure and update the feature information of each vertex iteratively. Endowing
    Eucludiean GNN with hyperbolic geometry can make it superior in capturing graph
    structure [[52](#bib.bib52)]. Recently, plenty of geometric GNN research has investigated
    how to incorporate GNN with hyperbolic manifold to benefit from a neighborhood
    with a highly organized structure.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: GNN可以用于构建基于不规则图的学习网络。每个图由顶点和边表示，这描述了顶点之间的关系。GNN将顶点编码为特征向量，将边建模为顶点之间的关系矩阵。在GNN中，图卷积在关系矩阵和特征矩阵之间进行。因此，GNN可以利用图结构并迭代更新每个顶点的特征信息。将欧几里得GNN赋予超曲面几何可以使其在捕捉图结构方面更具优势[[52](#bib.bib52)]。最近，大量几何GNN研究探讨了如何将GNN与超曲面流形结合，以从具有高度组织结构的邻域中受益。
- en: To make full use of the rich geometric information in the graph, geometry interaction
    learning (GIL) [[53](#bib.bib53)] incorporates Euclidean space with hyperbolic
    geometry by exponential and logarithmic transformations. Moreover, learnable message
    passing parameters are optimized on the $M\ddot{o}bius$ manifold. To allow each
    node to determinate the importance of each geometry space freely, the GIL framework
    employs a flexible dual-space to model both low-dimensional regular data and complex
    hierarchical structures. A broad spectrum of experiments show that the GIL method
    is adaptative to node classification and link prediction tasks.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用图中的丰富几何信息，几何交互学习（GIL）[[53](#bib.bib53)]通过指数和对数变换将欧几里得空间与超曲面几何结合起来。此外，学习型消息传递参数在$M\ddot{o}bius$流形上进行优化。为了让每个节点自由地确定每个几何空间的重要性，GIL框架采用了灵活的双空间来建模低维规则数据和复杂的层次结构。广泛的实验表明，GIL方法适用于节点分类和链接预测任务。
- en: Observing that GCN cannot cope with changes in static structure information,
    Liu et al. [[75](#bib.bib75)] put forward a manifold regularized dynamic graph
    convolutional network (MRDGCN), which integrated manifold regularization into
    GCN to model dynamic structure information. MRDGCN automatically updates the structure
    information before convengence, which makes up for GCN’s inability to remain optimal
    in pace with the learning process. Considerable comparative experiments on human
    activity datasets and citation network datasets evaluate that MRDGCN outperforms
    GCN and other semi-supervised learning methods.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到GCN无法应对静态结构信息的变化，Liu等人[[75](#bib.bib75)]提出了一种流形正则化动态图卷积网络（MRDGCN），该网络将流形正则化集成到GCN中以建模动态结构信息。MRDGCN在收敛之前自动更新结构信息，这弥补了GCN在学习过程中无法保持最佳的不足。大量关于人类活动数据集和引文网络数据集的比较实验评估了MRDGCN在性能上优于GCN和其他半监督学习方法。
- en: 4.4 Geometric Optimization for Other Deep Learning Methods
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 其他深度学习方法的几何优化
- en: Robust Time Series Prediction. Considering that noises and outliers are inevitable
    and important for system modeling, Feng et al. [[76](#bib.bib76)] put forward
    a robust manifold broad learning system (RM-BLS) for time series prediction with
    large-scale noisy disturbations. RM-BLS applies low-rank constraint so that features
    spoiled by perturbations can be abandoned by feature selection. Furthermore, RM-BLS
    can also abandon features that are not satisfied to low-dimensional manifold embedding.
    In addition to the low-rank manifold, [[76](#bib.bib76)] also considers Stiefel
    manifold optimization and satisfies orthogonal constraints by Cayley transformation
    and curvilinear search algorithm.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒时间序列预测。考虑到噪声和异常值在系统建模中不可避免且重要，Feng等人[[76](#bib.bib76)]提出了一种用于大规模噪声干扰时间序列预测的鲁棒流形广义学习系统（RM-BLS）。RM-BLS应用低秩约束，使得被扰动损坏的特征可以通过特征选择被舍弃。此外，RM-BLS还可以舍弃不满足低维流形嵌入的特征。除了低秩流形之外，[[76](#bib.bib76)]还考虑了Stiefel流形优化，并通过Cayley变换和曲线搜索算法满足正交约束。
- en: Medical Reconstruction. Geometric optimization have played an essential role
    in the medical field, such as magnetic resonance imaging (MRI) for cardiac diagnosis.
    Dynamic MR can be optimized on a low-rank tensor manifold [[77](#bib.bib77)] to
    seize the powerful temporal connection between dynamic signals. Moreover, the
    iterative reconstruction process is flattened to a neural network for acceleration,
    called dubbed Manifold-Net. To recover free breathing and ungated cardiac MRI
    data, Biswas et al. [[78](#bib.bib78)] creatively combined CNN with smoothness
    regularization on manifolds (SToRM) prior. The Laplacian matrix $L$ in SToRM $tr(X^{T}LX)$
    is defined on the manifold to model similarities between data beyond the ambient
    space. To utilize the manifold structure and patient-specific information, data
    denoizing based on CNN and SToRM together with conjugate gradients (CG) step take
    place alternatively. Experiments confirm that combining CNN with SToRM leads to
    a fast and high quality reconstruction of MRI data even when the down sampling
    frequency is less than 8.2s of acquisition time per slice.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 医学重建。几何优化在医学领域发挥了重要作用，例如用于心脏诊断的磁共振成像（MRI）。动态 MR 可以在低秩张量流形上进行优化 [[77](#bib.bib77)]，以把握动态信号之间强大的时间关联。此外，迭代重建过程被转化为一个神经网络进行加速，称为
    Manifold-Net。为了恢复自由呼吸和未门控的心脏 MRI 数据，Biswas 等人 [[78](#bib.bib78)] 创造性地将 CNN 与流形上的光滑性正则化（SToRM）先验结合起来。SToRM
    中的拉普拉斯矩阵 $L$ 在流形上定义 $tr(X^{T}LX)$ 以建模数据之间超越环境空间的相似性。为了利用流形结构和患者特定信息，基于 CNN 和 SToRM
    的数据去噪以及共轭梯度（CG）步骤交替进行。实验表明，将 CNN 与 SToRM 结合能够实现快速和高质量的 MRI 数据重建，即使下采样频率小于每片 8.2
    秒的采集时间。
- en: Transfer Learning. To maximize the utilization of finite computing resources,
    transfer learning aims to reuse the neural network, which is trained for task
    $A$, to address a similar task $B$. Knowledge distillation (KD) is intended to
    transfer model knowledge from a well-trained model (teacher) to a compact model
    (student) with soft labels. Zhang et al. [[79](#bib.bib79)] devised an end-to-end
    deep manifold-to-manifold transforming network (DMT-Net) for discriminative feature
    learning. However, reconstructing a more discriminative SPD manifold from the
    original one is challenging. DMT-Net designs a local SPD convolutional layer and
    the non-linear SPD activation layer to deal with it. Huang et al. [[80](#bib.bib80)]
    designed a manifold-to-manifold transformation matrix $W$ and constrained the
    optimization to reside on the SPD manifold. Moreover, the intra-class and inter-class
    dissimilarity graphs are built under $W$. Hence, they can represent local geometry
    structures and learn the discriminative feature of SPD data.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习。为了最大化有限计算资源的利用，迁移学习旨在重用为任务 $A$ 训练的神经网络来处理类似的任务 $B$。知识蒸馏（KD）旨在将模型知识从一个经过良好训练的模型（教师）传递到一个紧凑模型（学生），并使用软标签。Zhang
    等人 [[79](#bib.bib79)] 设计了一个端到端的深度流形到流形变换网络（DMT-Net）用于判别特征学习。然而，从原始流形重建出一个更具判别性的
    SPD 流形具有挑战性。DMT-Net 设计了一个局部 SPD 卷积层和非线性 SPD 激活层来应对这一挑战。Huang 等人 [[80](#bib.bib80)]
    设计了一个流形到流形的变换矩阵 $W$ 并将优化约束在 SPD 流形上。此外，在 $W$ 下建立了类内和类间相似性图。因此，它们可以表示局部几何结构并学习
    SPD 数据的判别特征。
- en: Optimal Transport. Optimal transport aims to measure the distance between two
    probability distributions by using transport plan $\Gamma$ and cost matrix $C$
    , i.e.,
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最优传输。最优传输旨在通过使用传输计划 $\Gamma$ 和成本矩阵 $C$ 来测量两个概率分布之间的距离，即，
- en: '|  | $\min_{\Gamma\in\Pi(\mu_{1},\mu_{2})}trace(\Gamma^{T}C),$ |  | (61) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\Gamma\in\Pi(\mu_{1},\mu_{2})}trace(\Gamma^{T}C),$ |  | (61) |'
- en: where $\Pi(\mu_{1},\mu_{2})$ consists of joint distributions with marginals
    $\mu_{1}$ and $\mu_{2}$. Supposing $\mu_{1}$ has $m$ points and $\mu_{2}$ has
    $n$ points, the size of both $\Gamma$ and $C$ is $m\times n$. There are works
    that have explored the application of geometric optimization in optimal transport
    problems [[81](#bib.bib81)]. By using the Riemannian gradient descent (RGD) algorithm,
    [[82](#bib.bib82)] explored how to convert optimal transport problems with different
    regularizations to the optimization problem on the coupling matrix manifold (CMM).
    To clarify the geometry optimization process, [[82](#bib.bib82)] took classic
    optimal transport problems (e.g., the entropy-regularized [[83](#bib.bib83)] and
    power-regularized optimal transport problems [[84](#bib.bib84)]) as an example.
    Observing that the constrained set $\Pi(\mu_{1},\mu_{2})$ has a differentiable
    manifold structure, [[85](#bib.bib85)] and [[86](#bib.bib86)] solved the optimal
    transport problem on a generalized doubly stochastic manifold, broadening the
    application of manifold geometry in non-linear optimal transport problems. In
    addition to general problems, [[86](#bib.bib86)] discusses how to adapt the above
    geometric optimization framework to particular ones, such as problems with sparse
    optimal transport map and problems of how to learn multiple transport plans simultaneously.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Pi(\mu_{1},\mu_{2})$ 由边际分布 $\mu_{1}$ 和 $\mu_{2}$ 组成。假设 $\mu_{1}$ 有 $m$
    个点，$\mu_{2}$ 有 $n$ 个点，则 $\Gamma$ 和 $C$ 的大小均为 $m\times n$。已有研究探讨了几何优化在最优传输问题中的应用
    [[81](#bib.bib81)]。通过使用黎曼梯度下降 (RGD) 算法，[[82](#bib.bib82)] 探讨了如何将具有不同正则化的最优传输问题转化为耦合矩阵流形
    (CMM) 上的优化问题。为明确几何优化过程，[[82](#bib.bib82)] 以经典的最优传输问题（例如，熵正则化 [[83](#bib.bib83)]
    和幂正则化最优传输问题 [[84](#bib.bib84)]）作为示例。观察到受约束的集合 $\Pi(\mu_{1},\mu_{2})$ 具有可微流形结构，[[85](#bib.bib85)]
    和 [[86](#bib.bib86)] 在广义双重随机流形上解决了最优传输问题，拓宽了流形几何在非线性最优传输问题中的应用。除了普通问题外，[[86](#bib.bib86)]
    还讨论了如何将上述几何优化框架适应于特定问题，如稀疏最优传输映射问题和如何同时学习多个传输计划的问题。
- en: Robots. Bayesian optimization is an important technology for robots since it
    is effective in solving optimization problems such as controller tuning, policy
    adaptation, and robot design. Bayesian optimization is based on the Gaussian Process
    that relies on domain knowledge exploration. Therefore, geometry-aware Bayesian
    optimization emerges as a promising paradigm that can incorporate domain geometry
    into the optimization algorithm. There are many commonly used kernels in Gaussian
    Process, among which Matérn kernel is used to study geometry-aware Gaussian process
    and Bayesian optimization. Euclidean Matérn kernel is defined as follows,
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人。贝叶斯优化是一项对机器人非常重要的技术，因为它在解决优化问题方面非常有效，如控制器调优、策略适应和机器人设计。贝叶斯优化基于高斯过程，这一过程依赖于领域知识的探索。因此，几何感知的贝叶斯优化作为一种有前景的范式出现，它可以将领域几何信息纳入优化算法中。在高斯过程中，有许多常用的核函数，其中
    Matérn 核用于研究几何感知高斯过程和贝叶斯优化。欧几里得 Matérn 核定义如下，
- en: '|  | $K(x,x^{\prime})=exp(-\frac{\&#124;x-x^{\prime}\&#124;^{2}}{2\sigma^{2}}),$
    |  | (62) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | $K(x,x^{\prime})=exp(-\frac{\&#124;x-x^{\prime}\&#124;^{2}}{2\sigma^{2}}),$
    |  | (62) |'
- en: where $\sigma$ is a free parameter. Matérn kernel is a commonly used kernel
    function when constructing stationary Gaussian process. Borovitskiy et al. [[87](#bib.bib87)]
    pointed out that generalizing the Matérn kernel to the Riemannian manifold merely
    by replacing Euclidean norms $\|x-x^{\prime}\|^{2}$ with geodesic distances $d_{g}(x-x^{\prime})$
    could not produce a well-defined kernel function. To construct the Riemannian
    Matérn kernel defined by stochastic partial differential equations, Borovitskiy
    et al. proposed to obtain Laplace–Beltrami eigenpairs for the specific manifold
    and approximate the infinite sum, which forms the basis for geometry-aware Bayesian
    optimization on robotics. However, the above method suffers from two problems
    [[88](#bib.bib88)], i.e., i) the amount of computation increases exponentially
    with the manifold dimension; and ii) such method is inapplicable to non-compact
    manifolds. To address these problems, Jaquier et al. [[88](#bib.bib88)] observed
    a general expression of Matérn kernels, which is helpful to generalize them to
    the torus and sphere manifold. More importantly, Matérn kernels can be generalized
    to non-compact manifolds (e.g., SPD matrix manifold and the Hyperbolic space)
    by using the general expression.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma$ 是一个自由参数。Matérn 核函数是在构建平稳高斯过程时常用的核函数。Borovitskiy 等人 [[87](#bib.bib87)]
    指出，仅通过用测地距离 $d_{g}(x-x^{\prime})$ 替换欧几里得范数 $\|x-x^{\prime}\|^{2}$ 来将 Matérn 核推广到黎曼流形，无法产生一个明确定义的核函数。为了构造由随机偏微分方程定义的黎曼
    Matérn 核，Borovitskiy 等人提出了获取特定流形的 Laplace–Beltrami 特征对并近似无限和的方法，这为机器人上的几何感知贝叶斯优化奠定了基础。然而，上述方法存在两个问题
    [[88](#bib.bib88)]，即 i) 计算量随着流形维度的增加而呈指数增长； ii) 该方法不适用于非紧流形。为了解决这些问题，Jaquier 等人
    [[88](#bib.bib88)] 观察到了 Matérn 核的通用表达式，这有助于将其推广到圆环和球面流形。更重要的是，Matérn 核可以通过使用通用表达式推广到非紧流形（如
    SPD 矩阵流形和双曲空间）。
- en: Continual learning. Continual learning aims to remember and use the experience
    of previous tasks to learn new tasks, which raises requirements for the memory
    ability of neural networks. Chaudhry et al. [[89](#bib.bib89)] proposed to achieve
    the purpose of continual learning on the low-rank orthogonal manifold. The core
    idea of this method is to project the gradient into disjoint low-rank orthogonal
    subspace by introducing task-specific projection matrix in the last second layer,
    which can make the gradient between different tasks orthogonal and alleviate catastrophic
    forgetting. The concept of gradient orthogonality was first proposed in [[90](#bib.bib90)].
    The essential reason for catastrophic forgetting is that learning new tasks will
    affect the parameters learned on the old tasks. Updating parameters in the direction
    orthogonal to the gradient of the old tasks can not only learn new tasks but also
    keep the loss of the old tasks, which alleviates catastrophic forgetting. In the
    deep neural network, the chain derivation process can be approximately regarded
    as the linear transformation of the gradient, which will destroy the orthogonality
    of the gradient of the earlier layers and lead to catastrophic forgetting. To
    ensure the orthogonality of the gradient between different tasks, [[89](#bib.bib89)]
    constrains parameters on the Stiefel manifold, making this linear transformation
    an orthogonal transformation.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 持续学习。持续学习旨在记住和利用以前任务的经验来学习新任务，这对神经网络的记忆能力提出了更高的要求。Chaudhry 等人 [[89](#bib.bib89)]
    提出了在低秩正交流形上实现持续学习的目标。该方法的核心思想是通过在最后一层引入任务特定的投影矩阵，将梯度投影到不相交的低秩正交子空间，从而使不同任务之间的梯度正交，并缓解灾难性遗忘。梯度正交性的概念最早在
    [[90](#bib.bib90)] 中提出。灾难性遗忘的根本原因是学习新任务会影响在旧任务上学到的参数。在与旧任务梯度正交的方向上更新参数，不仅可以学习新任务，还能保持旧任务的损失，从而减轻灾难性遗忘。在深度神经网络中，链式导数过程可以近似看作梯度的线性变换，这会破坏早期层梯度的正交性，并导致灾难性遗忘。为了确保不同任务之间梯度的正交性，[[89](#bib.bib89)]
    将参数限制在 Stiefel 流形上，使得这一线性变换成为正交变换。
- en: 5 Toolbox
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 工具箱
- en: 'The success of the Tensorflow platform and PyTorch framework in deep learning
    shows that toolboxes can conveniently help build neural networks. There are valuable
    toolboxes designed for quickly setting up manifolds optimization. Manopt [[91](#bib.bib91)],
    Pymanopt [[92](#bib.bib92)], McTorch [[93](#bib.bib93)], and Geomstats [[94](#bib.bib94)]
    are classic toolboxes that implement manifold geometries and optimization algorithms.
    Moreover, they are user-friendly and time-saving. Table [2](#S5.T2 "Table 2 ‣
    5 Toolbox ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold") compares these toolboxes from the aspect of applicable
    manifolds and geometry operations.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 'Tensorflow 平台和 PyTorch 框架在深度学习中的成功表明，工具箱可以方便地帮助构建神经网络。有价值的工具箱被设计用于快速设置流形优化。Manopt
    [[91](#bib.bib91)]、Pymanopt [[92](#bib.bib92)]、McTorch [[93](#bib.bib93)] 和 Geomstats
    [[94](#bib.bib94)] 是实现流形几何和优化算法的经典工具箱。此外，它们用户友好且节省时间。表 [2](#S5.T2 "Table 2 ‣ 5
    Toolbox ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold") 从适用流形和几何操作的角度比较了这些工具箱。'
- en: 'Manopt, which is built on Matlab, is a helpful tool to handle a variety of
    geometry constraints (e.g., different manifold structures introduced in  [2.3](#S2.SS3
    "2.3 Manifold Examples ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")).
    A Riemannian optimization in Manopt [[91](#bib.bib91)] is designed as a problem
    including manifold structures that the search space is confined to. The cost function,
    or optimization object, is included in the above optimization problem as well.
    If needed, a problem structure can also cover derivatives of the objective function.
    In Manopt, solvers are functions that give a general implementation to Riemannian
    optimization algorithms, including steepest-descent, conjugate-gradient, and Riemannian
    trust-regions algorithms. Since solvers in Manopt is designed to minimize the
    cost function, the cost function should be multiplied by a negative one if it
    is a maximization problem.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'Manopt 是建立在 Matlab 上的，是处理各种几何约束（例如，[2.3](#S2.SS3 "2.3 Manifold Examples ‣ 2
    Geometric Optimization Theory ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold") 中介绍的不同流形结构）的有用工具。Manopt [[91](#bib.bib91)]
    中的黎曼优化被设计为一个包含流形结构的问题，搜索空间被限制在这些结构内。成本函数或优化目标也包含在上述优化问题中。如果需要，问题结构还可以涵盖目标函数的导数。在
    Manopt 中，求解器是给黎曼优化算法提供通用实现的函数，包括最速下降、共轭梯度和黎曼信任域算法。由于 Manopt 中的求解器旨在最小化成本函数，因此如果是最大化问题，则应将成本函数乘以负数。'
- en: Pymanopt [[92](#bib.bib92)] extends Manopt to python. Similar to the usage of
    Manopt in Matlab, a Riemannian optimization in Pymanopt should be initialized
    with a predefined manifold and cost function. Equipped with different solvers,
    the optimization process and result can be diverse. Pymanopt covers all sorts
    of smooth manifolds such as the oblique manifold, sphere manifold, and Graßmann
    manifold. Numerable optimization algorithms are included as solvers, for instance,
    trust-regions, conjugate-gradient, and steepest-descent algorithms are contained
    by Pymanopt.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Pymanopt [[92](#bib.bib92)] 将 Manopt 扩展到了 Python。与 Matlab 中使用 Manopt 类似，Pymanopt
    中的黎曼优化应当以预定义的流形和成本函数初始化。配备了不同的求解器，优化过程和结果可能有所不同。Pymanopt 涵盖了各种平滑流形，如斜流形、球面流形和格拉斯曼流形。包括许多优化算法作为求解器，例如，Pymanopt
    包含信任域、共轭梯度和最速下降算法。
- en: Manopt and Pymanopt are limited to shallow learning optimizations and are not
    applicable to deep learning optimizations. To fill the deficiency of Manopt and
    Pymanopt, McTorch has been implemented by extending Pytorch [[93](#bib.bib93)],
    a handy framework for deep learning. As a result, it implements a general solution
    for deep learning optimizations on the manifold. Unlike Manopt and Pymanopt, Riemannian
    optimization in McTorch does not need to define problems, manifolds, and solvers.
    Similar to Pytorch, Riemannian optimization in McTorch only needs to define modules
    and optimizers such as Adam. Network modules inherited from $torch.nn.module$
    initialize layers with manifolds and forward functions.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Manopt 和 Pymanopt 仅限于浅层学习优化，不适用于深度学习优化。为了弥补 Manopt 和 Pymanopt 的不足，McTorch 被通过扩展
    Pytorch [[93](#bib.bib93)] 实现了，这是一种适用于深度学习的方便框架。因此，它为流形上的深度学习优化实现了一般解决方案。与 Manopt
    和 Pymanopt 不同，McTorch 中的黎曼优化不需要定义问题、流形和求解器。类似于 Pytorch，McTorch 中的黎曼优化只需要定义模块和优化器，例如
    Adam。继承自 $torch.nn.module$ 的网络模块用流形和前向函数初始化层。
- en: Geoopt [[95](#bib.bib95)], which is implemented on top of Pytorch, has a cheaper
    infrastructure cost than McTorch. Extended from $torch.nn.Module.parameters$,
    Geoopt supports tensors and parameters on the manifold. Moreover, Geoopt provides
    Riemannian optimizers, for instance, $RiemannianSGD$ and $RiemannianAdam$ are
    available and inherited from $torch.optim.SGD$ and $torch.optim.Adam$, respectively
    [[95](#bib.bib95)].
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Geoopt [[95](#bib.bib95)]，它是在 Pytorch 上实现的，基础设施成本低于 McTorch。从 $torch.nn.Module.parameters$
    扩展，Geoopt 支持流形上的张量和参数。此外，Geoopt 提供了黎曼优化器，例如，$RiemannianSGD$ 和 $RiemannianAdam$，它们分别继承自
    $torch.optim.SGD$ 和 $torch.optim.Adam$ [[95](#bib.bib95)]。
- en: Another toolbox, Geomstats, is composed of two core modules, i.e., geometry
    and learning [[94](#bib.bib94)]. The former implements Riemannian metrics, including
    geodesic distance. The latter implements statistics and learning algorithms inherited
    from Scikit-Learn classes such as *K-Means* and PCA. Compared with Geomstats,
    other toolboxes mentioned are less modular and lack statistical learning algorithms.
    Taking clustering, one of the classic statistical learning problems, as an example,
    Geomstats encapsulates the class *Online K-Means* with the parameter *metric*.
    To perform clustering operation, users only need to initialize the Riemannian
    metric and call *fit* function of class *Online K-Means* as they do in Scikit-Learn,
    which is easy and convenient.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个工具箱 Geomstats 由两个核心模块组成，即几何和学习 [[94](#bib.bib94)]。前者实现了黎曼度量，包括测地距离。后者实现了从
    Scikit-Learn 类中继承的统计和学习算法，如 *K-Means* 和 PCA。与 Geomstats 相比，提到的其他工具箱模块化程度较低，且缺乏统计学习算法。以聚类为例，Geomstats
    封装了带有 *metric* 参数的 *Online K-Means* 类。为了执行聚类操作，用户只需初始化黎曼度量，并像在 Scikit-Learn 中一样调用
    *Online K-Means* 类的 *fit* 函数，这非常简单方便。
- en: TheanoGeometry [[96](#bib.bib96)] uses Theano, a python-based and research-oriented
    framework, to implement differential geometry and non-linear statistics problems.
    TheanoGeometry outperforms other manifold toolboxes since it can handle symbolic
    calculations. Thus, Theano code can be generated from symbolic expression directly,
    where non-linear symbolic statistics can be optimized with a trivial amount of
    code. TheanoGeometry goes further beyond efficient symbolic computation. It implements
    Riemannian geometry such as geodesic equations, parallel transport, and curvature
    with automatic differentiation features [[97](#bib.bib97)].
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: TheanoGeometry [[96](#bib.bib96)] 使用 Theano，这是一个基于 Python 的研究导向框架，用于实现微分几何和非线性统计问题。由于
    TheanoGeometry 可以处理符号计算，因此优于其他流形工具箱。因此，可以直接从符号表达式生成 Theano 代码，其中非线性符号统计可以通过极少量的代码进行优化。TheanoGeometry
    进一步超越了高效的符号计算。它实现了包括测地线方程、平行运输和曲率在内的黎曼几何，并具有自动微分功能 [[97](#bib.bib97)]。
- en: 'Table 2: Toolboxes Comparison in Terms of Manifolds and Geometry'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：按流形和几何进行的工具箱比较
- en: '| Toolboxes | Manifolds | Geometry |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 工具箱 | 流形 | 几何 |'
- en: '| Manopt [[91](#bib.bib91)] | Euclidean manifold, symmetric matrices, sphere,
    complex circle, SO (n), Stiefel, Graßmannian, oblique manifold, SPD (n), fixed-rank
    PSD matrices | Exponential and logarithmic maps, tangent space projector, retraction,
    vector transport, egrad2rgrad, ehess2rhess, vector, metric, distance, norm |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Manopt [[91](#bib.bib91)] | 欧几里得流形，对称矩阵，球面，复圆，SO (n)，Stiefel，Graßmannian，斜流形，SPD
    (n)，固定秩 PSD 矩阵 | 指数和对数映射，切空间投影，回缩，向量运输，egrad2rgrad，ehess2rhess，向量，度量，距离，范数 |'
- en: '| Pymanopt [[92](#bib.bib92)] | Same as Manopt | Same as Manopt |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Pymanopt [[92](#bib.bib92)] | 与 Manopt 相同 | 与 Manopt 相同 |'
- en: '| McTorch [[93](#bib.bib93)] | Stiefel, SPD (n) | Same as Manopt |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| McTorch [[93](#bib.bib93)] | Stiefel，SPD (n) | 与 Manopt 相同 |'
- en: '| Geoopt [[95](#bib.bib95)] | Euclidean manifold, sphere, Stiefel, Poincaré
    ball | Same as Manopt |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Geoopt [[95](#bib.bib95)] | 欧几里得流形，球面，Stiefel，庞加莱球 | 与 Manopt 相同 |'
- en: '| Geomstats [[94](#bib.bib94)] | Euclidean manifold, Minkowski and hyperbolic
    space, sphere, SO (n), SE (n), GL (n), Stiefel, Graßmannian, SPD (n), discretized
    curves, Landmarks | Exponential and logarithmic maps, parallel transport, inner
    product, distance, norm, Levi-Civita conne- ction, geodesics, invariant metrics
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Geomstats [[94](#bib.bib94)] | 欧几里得流形，闵可夫斯基和双曲空间，球面，SO (n)，SE (n)，GL (n)，Stiefel，Graßmannian，SPD
    (n)，离散曲线，地标 | 指数和对数映射，平行运输，内积，距离，范数，Levi-Civita 连接，测地线，不变度量 |'
- en: '| TheanoGeometry [[96](#bib.bib96)] | Sphere, ellipsoid, SPD (n), Landmarks,
    GL (n), SO (n), SE (n) | Inner product, exponential and logarithmic maps, parallel
    transport, Christoffel symbols, Riemann, Ricci and scalar curvature, geodesics,
    Fréchet mean |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| TheanoGeometry [[96](#bib.bib96)] | 球面、椭球面、SPD (n)、标志点、GL (n)、SO (n)、SE (n)
    | 内积、指数和对数映射、平行运输、Christoffel 符号、黎曼、里奇和标量曲率、测地线、Fréchet 均值 |'
- en: 6 Performance Evaluation
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 性能评估
- en: 'Table [4](#S6.T4 "Table 4 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold"),
     [5](#S6.T5 "Table 5 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold"), [6](#S6.T6 "Table
    6 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold"), [7](#S6.T7 "Table 7 ‣ 6 Performance
    Evaluation ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold"), [8](#S6.T8 "Table 8 ‣ 6 Performance Evaluation
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold") compare the performance of aforementioned geometric optimization
    methods on various visual tasks (e.g., character recognition, emotion recognition,
    act recognition, and scene recognition tasks). Each image dataset used in different
    visual tasks is summarized in Table [3](#S6.T3 "Table 3 ‣ 6 Performance Evaluation
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold").'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S6.T4 "Table 4 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")、[5](#S6.T5 "Table
    5 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")、[6](#S6.T6 "Table 6 ‣ 6 Performance
    Evaluation ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")、[7](#S6.T7 "Table 7 ‣ 6 Performance Evaluation
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold")、[8](#S6.T8 "Table 8 ‣ 6 Performance Evaluation ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold") 比较了前述几何优化方法在各种视觉任务（例如，字符识别、情感识别、动作识别和场景识别任务）上的性能。不同视觉任务中使用的每个图像数据集汇总在表 [3](#S6.T3
    "Table 3 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold") 中。'
- en: 'Table 3: Datasets for Different Visual Tasks'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 不同视觉任务的数据集'
- en: '| Vision Task | Dataset | Total Samples | Categories | Image Size |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 视觉任务 | 数据集 | 总样本数 | 类别 | 图像尺寸 |'
- en: '| Character Recognition | MNIST[[98](#bib.bib98)] | 70000 | 10 | 32 $\times$
    32 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 字符识别 | MNIST [[98](#bib.bib98)] | 70000 | 10 | 32 $\times$ 32 |'
- en: '| Emotion Recognition | AFEW [[99](#bib.bib99)] | 1345 | 7 | 400 $\times$ 400
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 情感识别 | AFEW [[99](#bib.bib99)] | 1345 | 7 | 400 $\times$ 400 |'
- en: '| NABU3DFE [[100](#bib.bib100)] | 2500 | 6 | NA |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| NABU3DFE [[100](#bib.bib100)] | 2500 | 6 | NA |'
- en: '| Bosphorus dataset [[101](#bib.bib101)] | 4666 | 6 | NA |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Bosphorus 数据集 [[101](#bib.bib101)] | 4666 | 6 | NA |'
- en: '| Action Recognition | HDM05 [[98](#bib.bib98)] | 18000 | 130 | 93 $\times$93
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 动作识别 | HDM05 [[98](#bib.bib98)] | 18000 | 130 | 93 $\times$93 |'
- en: '| Face Verification | PaSC [[102](#bib.bib102)] | 12529 | NA | $401\times 401$
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 人脸验证 | PaSC [[102](#bib.bib102)] | 12529 | NA | $401\times 401$ |'
- en: '| Scene Recognition | Scene15 [[103](#bib.bib103)] | NA | 15 | $300\times 250$
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 场景识别 | Scene15 [[103](#bib.bib103)] | NA | 15 | $300\times 250$ |'
- en: '| Eight sports event categories[[104](#bib.bib104)] | NA | 8 | NA |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 八个运动事件类别 [[104](#bib.bib104)] | NA | 8 | NA |'
- en: '| SUN [[105](#bib.bib105), [106](#bib.bib106)] | 899 | NA | NA |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| SUN [[105](#bib.bib105), [106](#bib.bib106)] | 899 | NA | NA |'
- en: 'Table [4](#S6.T4 "Table 4 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    shows that GORU [[74](#bib.bib74)] outperforms other ORNNs on the MNIST dataset.
    GORU adds a forget gate, which enables ORNN to filter out irrelevant information.
    Taking advantage of the surjective exponential map, expRNN [[13](#bib.bib13)]
    realizes orthogonal parameterization with a more straightforward way. Unlike expRNN,
    uRNN [[14](#bib.bib14)] uses simple unitary matrices to construct the unitary
    hidden-to-hidden matrix. However, such matrix construction method fails to represent
    all $N\times N$ unitary matrices. Therefore, Scott Wisdom et al. [[69](#bib.bib69)]
    proposed full-capacity uRNN to overcome that bottleneck of uRNN. Using regularization
    terms to realize orthogonal parameterization, soRNN [[72](#bib.bib72)] explores
    the effect of soft orthogonal constraints on RNN. ORNN [[73](#bib.bib73)] exploits
    the householder matrix to enforce an orthogonal constraint on RNN, which mitigates
    the considerable time complexity of unitary matrices. Table [4](#S6.T4 "Table
    4 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold") shows that combining the forget
    gate, or noise filter, with ORNN improves the performance of ORNN.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S6.T4 "Table 4 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")显示，GORU [[74](#bib.bib74)]
    在 MNIST 数据集上表现优于其他 ORNN。GORU 添加了一个遗忘门，使 ORNN 能够过滤掉无关信息。利用射影指数映射，expRNN [[13](#bib.bib13)]
    实现了更简单的正交参数化。与 expRNN 不同，uRNN [[14](#bib.bib14)] 使用简单的单位矩阵构造单位隐藏到隐藏的矩阵。然而，这种矩阵构造方法无法表示所有
    $N\times N$ 单位矩阵。因此，Scott Wisdom 等人 [[69](#bib.bib69)] 提出了全容量 uRNN 以克服 uRNN 的瓶颈。通过正则化项实现正交参数化，soRNN
    [[72](#bib.bib72)] 探索了软正交约束对 RNN 的影响。ORNN [[73](#bib.bib73)] 利用 Householder 矩阵对
    RNN 强制实施正交约束，从而减轻了单位矩阵的巨大时间复杂度。表 [4](#S6.T4 "Table 4 ‣ 6 Performance Evaluation
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold")显示，将遗忘门或噪声滤波器与 ORNN 结合可以提高 ORNN 的性能。'
- en: 'Table 4: Comparison Results of Character Recognition'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 字符识别的比较结果'
- en: '| Dataset | Method | Accuracy |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | 准确率 |'
- en: '| MNIST [[98](#bib.bib98)] | uRNN [[14](#bib.bib14)] | 97.6% |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| MNIST [[98](#bib.bib98)] | uRNN [[14](#bib.bib14)] | 97.6% |'
- en: '| full-capacity uRNN [[69](#bib.bib69)] | 96.9% |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 全容量 uRNN [[69](#bib.bib69)] | 96.9% |'
- en: '| expRNN [[13](#bib.bib13)] | 98.7% |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| expRNN [[13](#bib.bib13)] | 98.7% |'
- en: '| soRNN [[72](#bib.bib72)] | 97.3% |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| soRNN [[72](#bib.bib72)] | 97.3% |'
- en: '| ORNN [[73](#bib.bib73)] | 97.2% |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| ORNN [[73](#bib.bib73)] | 97.2% |'
- en: '| GORU [[74](#bib.bib74)] | 98.9% |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| GORU [[74](#bib.bib74)] | 98.9% |'
- en: 'Table 5: Comparison Results of Emotion Recognition'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 情感识别的比较结果'
- en: '| Dataset | Method | Accuracy |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | 准确率 |'
- en: '| AFEW [[99](#bib.bib99)] | STM-ExpLet [[107](#bib.bib107)] | 31.73% |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| AFEW [[99](#bib.bib99)] | STM-ExpLet [[107](#bib.bib107)] | 31.73% |'
- en: '| RSR-SPDML [[32](#bib.bib32)] | 30.12% |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| RSR-SPDML [[32](#bib.bib32)] | 30.12% |'
- en: '| DeepO2P [[108](#bib.bib108)] | 28.54% |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| DeepO2P [[108](#bib.bib108)] | 28.54% |'
- en: '| DCC [[109](#bib.bib109)] | 25.78% |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| DCC [[109](#bib.bib109)] | 25.78% |'
- en: '| GDA [[56](#bib.bib56)] | 29.11% |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| GDA [[56](#bib.bib56)] | 29.11% |'
- en: '| GGDA [[56](#bib.bib56)] | 29.45% |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| GGDA [[56](#bib.bib56)] | 29.45% |'
- en: '| PML [[110](#bib.bib110)] | 28.98% |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| PML [[110](#bib.bib110)] | 28.98% |'
- en: '| SPDNet [[50](#bib.bib50)] | 34.23% |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| SPDNet [[50](#bib.bib50)] | 34.23% |'
- en: '| GrNet [[51](#bib.bib51)] | 34.23% |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| GrNet [[51](#bib.bib51)] | 34.23% |'
- en: '| BU-3DFE [[100](#bib.bib100)] | Tree-PNN [[111](#bib.bib111)] | 93.23% |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| BU-3DFE [[100](#bib.bib100)] | Tree-PNN [[111](#bib.bib111)] | 93.23% |'
- en: '| Berretti et al. [[112](#bib.bib112)] | 77.53% |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Berretti 等人 [[112](#bib.bib112)] | 77.53% |'
- en: '| Huynh et al. [[113](#bib.bib113)] | 92.73% |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Huynh 等人 [[113](#bib.bib113)] | 92.73% |'
- en: '| Azazi et al. [[114](#bib.bib114)] | 85.71% |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Azazi 等人 [[114](#bib.bib114)] | 85.71% |'
- en: '| Hariri et al. [[59](#bib.bib59)] | 93.50% |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Hariri 等人 [[59](#bib.bib59)] | 93.50% |'
- en: '|  | CSLBP [[115](#bib.bib115)] | 76.98 % |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | CSLBP [[115](#bib.bib115)] | 76.98% |'
- en: '| CLBP [[116](#bib.bib116)] | 76.56% |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| CLBP [[116](#bib.bib116)] | 76.56% |'
- en: '| Bosphorus | ZernikeMoments [[117](#bib.bib117)] | 60.53% |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Bosphorus | ZernikeMoments [[117](#bib.bib117)] | 60.53% |'
- en: '|   [[101](#bib.bib101)] | Azazi et al. [[114](#bib.bib114)] | 84.10% |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|   [[101](#bib.bib101)] | Azazi 等人 [[114](#bib.bib114)] | 84.10% |'
- en: '|  | Hariri et al. [[59](#bib.bib59)] | 90.01% |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | Hariri 等人 [[59](#bib.bib59)] | 90.01% |'
- en: 'Table [5](#S6.T5 "Table 5 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    shows that SPDNet [[50](#bib.bib50)] and GrNet [[51](#bib.bib51)] can achieve
    better classification results than state-of-the-art methods on AFEW dataset [[99](#bib.bib99)].
    The following methods for comparison are shallow learning methods applying manifold
    structure: Expressionlets on Spatio-Temporal Manifold (STM-ExpLet) [[107](#bib.bib107)],
    Riemannian Sparse Representation combining with Manifold Learning on the manifold
    of SPD matrices (RSR-SPDML) [[32](#bib.bib32)], Discriminative Canonical Correlations
    (DCC) [[109](#bib.bib109)], Graßmann Discriminant Analysis (GDA) [[56](#bib.bib56)],
    Grassmannian Graph-Embedding Discriminant Analysis (GGDA) [[118](#bib.bib118)],
    and Projection Metric Learning (PML) [[110](#bib.bib110)]. Deep Second-order Pooling
    (DeepO2P) [[108](#bib.bib108)] is a traditional CNN model using the standard optimization
    method. SPDNet exploits the Stiefel manifold parameterization by BiMap layers
    and introduces non-linearity into the network by ReEig layers. Experiments prove
    that using the manifold geometry in deep learning optimization can improve network
    performance. The LogEig layer is crucial to Riemannian computing and contributes
    to the emotion classification success of SPDNet. The success of GrNet shows that
    optimizing on the Graßmann manifold and building a geometry-aware deep learning
    network is significant for learning representative features and classifying emotions
    with a relatively high level of accuracy.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](#S6.T5 "Table 5 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") 显示，SPDNet [[50](#bib.bib50)]
    和 GrNet [[51](#bib.bib51)] 在 AFEW 数据集 [[99](#bib.bib99)] 上可以比最先进的方法实现更好的分类结果。以下方法用于比较，它们是应用流形结构的浅层学习方法：Spatio-Temporal
    Manifold 上的 Expressionlets (STM-ExpLet) [[107](#bib.bib107)]、结合流形学习的流形上 Riemannian
    稀疏表示 (RSR-SPDML) [[32](#bib.bib32)]、判别典型相关性 (DCC) [[109](#bib.bib109)]、Graßmann
    判别分析 (GDA) [[56](#bib.bib56)]、Grassmannian 图嵌入判别分析 (GGDA) [[118](#bib.bib118)]
    和投影度量学习 (PML) [[110](#bib.bib110)]。Deep Second-order Pooling (DeepO2P) [[108](#bib.bib108)]
    是一个使用标准优化方法的传统 CNN 模型。SPDNet 通过 BiMap 层利用 Stiefel 流形参数化，并通过 ReEig 层将非线性引入网络。实验证明，使用流形几何在深度学习优化中可以提高网络性能。LogEig
    层对黎曼计算至关重要，并有助于 SPDNet 在情感分类中的成功。GrNet 的成功表明，在 Graßmann 流形上进行优化和构建一个具有几何感知的深度学习网络，对于学习具有代表性的特征和以较高精度分类情感具有重要意义。'
- en: 'Table [5](#S6.T5 "Table 5 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    presents that the manifold-based classification method proposed by Hariri et al.
    [[59](#bib.bib59)] achieves the highest precision on BU-3DFE and Bosphorus datasets.
    Hariri et al. used a Graph-Matching kernel and classified facial expression data
    with SPD covariance descriptors. It outperforms Tree-PNN [[111](#bib.bib111)]
    and XP Huynh [[113](#bib.bib113)] on the BU-3DFE dataset by a narrow margin, and
    the latter two methods use traditional CNN. The manifold-based method proposed
    by Hariri et al. greatly exceeds the methods proposed by Stefano Berretti [[112](#bib.bib112)]
    and Amal Azazi [[114](#bib.bib114)] by approximately 15% and 8% on BU-3DFE dataset.
    In particular, the latter two methods apply SIFT and Speed Up Robust Features
    descriptors. On the Bosphorus dataset, the classification accuracy of Hariri et
    al.’s method [[59](#bib.bib59)] is almost far higher than all state-of-the-art
    methods. For example, it is even 30% better than the ZernikeMoments [[117](#bib.bib117)].
    The low-accuracy methods use local features rather than SPD covariance matrices.
    Overall, these results indicate that using geometry constraints is vital for feature
    representation and emotion recognition.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](#S6.T5 "Table 5 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") 显示，Hariri 等人提出的基于流形的分类方法在
    BU-3DFE 和 Bosphorus 数据集上达到了最高的精度。Hariri 等人使用了 Graph-Matching 核，并用 SPD 协方差描述符对面部表情数据进行了分类。该方法在
    BU-3DFE 数据集上的表现优于 Tree-PNN [[111](#bib.bib111)] 和 XP Huynh [[113](#bib.bib113)]，而后两者使用传统的
    CNN。Hariri 等人提出的基于流形的方法在 BU-3DFE 数据集上的表现远远超过了 Stefano Berretti [[112](#bib.bib112)]
    和 Amal Azazi [[114](#bib.bib114)] 提出的的方法，分别高出约 15% 和 8%。特别地，后两种方法应用了 SIFT 和 Speed
    Up Robust Features 描述符。在 Bosphorus 数据集上，Hariri 等人方法 [[59](#bib.bib59)] 的分类准确率几乎远高于所有最先进的方法。例如，它比
    ZernikeMoments [[117](#bib.bib117)] 高出 30%。低精度的方法使用局部特征而不是 SPD 协方差矩阵。总体而言，这些结果表明，使用几何约束对于特征表示和情感识别至关重要。'
- en: 'Table [6](#S6.T6 "Table 6 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    shows that SPDNet achieves the highest accuracy on the action recognition task,
    followed by GrNet. As Table [7](#S6.T7 "Table 7 ‣ 6 Performance Evaluation ‣ A
    Survey of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold") shows, SPDNet and GrNet outperform state-of-the-art methods on the
    face recognition task. The eigenvalue decomposition in SPDNet introduces non-linearity
    and the QR decomposition in GrNet performs re-orthonormalization, both of which
    contribute to the classification accuracy. Therefore, using matrix decomposition
    is vital for exploring manifold constrained parameters. The success of the deep
    manifold network on the action recognition and face recognition task shows that
    optimizing deep learning on the manifold helps learn favorable features and classify
    human actions better.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [6](#S6.T6 "Table 6 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") 显示，SPDNet 在动作识别任务中达到了最高的准确率，其次是
    GrNet。正如表 [7](#S6.T7 "Table 7 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    所示，SPDNet 和 GrNet 在面部识别任务中优于最先进的方法。SPDNet 中的特征值分解引入了非线性，而 GrNet 中的 QR 分解执行了重新正交化，这两者都促进了分类准确性。因此，使用矩阵分解对于探索流形约束参数至关重要。深度流形网络在动作识别和面部识别任务上的成功表明，优化流形上的深度学习有助于学习有利的特征并更好地分类人类动作。'
- en: 'Table 6: Comparison Results of Action Recognition'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 动作识别的比较结果'
- en: '| Dataset | Method | Accuracy |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | 准确率 |'
- en: '| HDM05 [[98](#bib.bib98)] | RSR-SPDML [[32](#bib.bib32)] | 48.01% |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| HDM05 [[98](#bib.bib98)] | RSR-SPDML [[32](#bib.bib32)] | 48.01% |'
- en: '| DCC [[109](#bib.bib109)] | 41.74% |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| DCC [[109](#bib.bib109)] | 41.74% |'
- en: '| GDA [[56](#bib.bib56)] | 46.25% |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| GDA [[56](#bib.bib56)] | 46.25% |'
- en: '| GGDA [[118](#bib.bib118)] | 46.87% |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| GGDA [[118](#bib.bib118)] | 46.87% |'
- en: '| PML [[110](#bib.bib110)] | 47.25% |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| PML [[110](#bib.bib110)] | 47.25% |'
- en: '| SPDNet [[50](#bib.bib50)] | 61.45% |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| SPDNet [[50](#bib.bib50)] | 61.45% |'
- en: '| GrNet [[51](#bib.bib51)] | 59.23% |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| GrNet [[51](#bib.bib51)] | 59.23% |'
- en: 'Table 7: Comparison Results of Face Recognition'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 面部识别的比较结果'
- en: '|  | Accuracy |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  | 准确率 |'
- en: '| Method | PaSC1 [[102](#bib.bib102)] | PaSC2 [[102](#bib.bib102)] |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PaSC1 [[102](#bib.bib102)] | PaSC2 [[102](#bib.bib102)] |'
- en: '| VGGDeepFace [[119](#bib.bib119)] | 78.82% | 68.24% |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| VGGDeepFace [[119](#bib.bib119)] | 78.82% | 68.24% |'
- en: '| DeepO2P [[108](#bib.bib108)] | 68.76% | 60.14% |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| DeepO2P [[108](#bib.bib108)] | 68.76% | 60.14% |'
- en: '| DCC [[109](#bib.bib109)] | 75.83% | 67.04% |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| DCC [[109](#bib.bib109)] | 75.83% | 67.04% |'
- en: '| GDA [[56](#bib.bib56)] | 71.38% | 67.49% |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| GDA [[56](#bib.bib56)] | 71.38% | 67.49% |'
- en: '| GGDA [[118](#bib.bib118)] | 66.71% | 68.41% |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| GGDA [[118](#bib.bib118)] | 66.71% | 68.41% |'
- en: '| PML [[110](#bib.bib110)] | 73.45% | 68.32% |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| PML [[110](#bib.bib110)] | 73.45% | 68.32% |'
- en: '| SPDNet [[50](#bib.bib50)] | 80.12% | 72.83% |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| SPDNet [[50](#bib.bib50)] | 80.12% | 72.83% |'
- en: '| GrNet [[51](#bib.bib51)] | 80.52% | 72.76% |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| GrNet [[51](#bib.bib51)] | 80.52% | 72.76% |'
- en: 'As shown in Table [8](#S6.T8 "Table 8 ‣ 6 Performance Evaluation ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold"), Scene Recognition by Manifold Regularized Deep Learning Architecture
    (SRMR) [[60](#bib.bib60)] outperforms state-of-the-art non-manifold methods on
    all three scene recognition datasets. Lazebnik et al. [[120](#bib.bib120)] partitioned
    images into fine subregions for image matching. Dixit et al. [[121](#bib.bib121)]
    formulated Bayesian adaptation for scene image classification. Kwitt et al. [[122](#bib.bib122)]
    recognized scene images on the statistical (semantic) manifold. From the perspective
    of information geometry, they can consider the parameter vectors as Riemannian
    manifolds. Goh et al. [[123](#bib.bib123)] used SIFT descriptors and represented
    vectorially for image recognition. Li et al. [[104](#bib.bib104)] interpreted
    the semantic components of images. Wu and Rehg [[124](#bib.bib124)] used the Histogram
    Intersection Kernel (HIK) for sports game classification. Donahue et al. [[124](#bib.bib124)]
    used extracted features for novel generic tasks. SRMR’s incredible success on
    scene recognition tasks shows that manifold regularizations are significant for
    improving the classification accuracy of deep learning.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[8](#S6.T8 "Table 8 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")所示，流形正则化深度学习架构
    (SRMR) [[60](#bib.bib60)] 在所有三个场景识别数据集上的表现优于最先进的非流形方法。 Lazebnik 等 [[120](#bib.bib120)]
    将图像划分为细小子区域进行图像匹配。 Dixit 等 [[121](#bib.bib121)] 为场景图像分类制定了贝叶斯适应。 Kwitt 等 [[122](#bib.bib122)]
    在统计（语义）流形上识别场景图像。从信息几何的角度来看，他们可以将参数向量视为黎曼流形。 Goh 等 [[123](#bib.bib123)] 使用 SIFT
    描述符并以向量形式表示图像识别。 Li 等 [[104](#bib.bib104)] 解释了图像的语义组件。 Wu 和 Rehg [[124](#bib.bib124)]
    使用直方图交集核 (HIK) 进行体育比赛分类。 Donahue 等 [[124](#bib.bib124)] 使用提取的特征进行新颖的通用任务。 SRMR
    在场景识别任务中的巨大成功表明，流形正则化对提高深度学习的分类准确性至关重要。'
- en: 'Table 8: Comparison Results of Scene Recognition'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：场景识别的比较结果
- en: '| Dataset | Method | Accuracy |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | 准确率 |'
- en: '| Scene15 [[103](#bib.bib103)] | Lazebnik et al. [[120](#bib.bib120)] | 81.2%
    |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| Scene15 [[103](#bib.bib103)] | Lazebnik 等 [[120](#bib.bib120)] | 81.2% |'
- en: '| Dixit et al. [[121](#bib.bib121)] | 82.3% |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Dixit 等 [[121](#bib.bib121)] | 82.3% |'
- en: '| Kwitt et al. [[122](#bib.bib122)] | 85.4% |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| Kwitt 等 [[122](#bib.bib122)] | 85.4% |'
- en: '| Goh et al. [[123](#bib.bib123)] | 85.4% |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Goh 等 [[123](#bib.bib123)] | 85.4% |'
- en: '| SRMR [[60](#bib.bib60)] | 86.9% |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| SRMR [[60](#bib.bib60)] | 86.9% |'
- en: '| Eight sports event categories[[104](#bib.bib104)] | Li et al. [[104](#bib.bib104)]
    | 73.4% |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 八个体育赛事类别 [[104](#bib.bib104)] | Li 等 [[104](#bib.bib104)] | 73.4% |'
- en: '| Kwitt et al. [[122](#bib.bib122)] | 83.0% |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Kwitt 等 [[122](#bib.bib122)] | 83.0% |'
- en: '| Wu and Rehg [[124](#bib.bib124)] | 84.3% |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Wu 和 Rehg [[124](#bib.bib124)] | 84.3% |'
- en: '| SRMR [[60](#bib.bib60)] | 86.1% |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| SRMR [[60](#bib.bib60)] | 86.1% |'
- en: '|  | Xiao et al. [[105](#bib.bib105)] | 27.2% |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  | Xiao 等 [[105](#bib.bib105)] | 27.2% |'
- en: '| SUN | Kwitt et al. [[122](#bib.bib122)] | 28.9% |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| SUN | Kwitt 等 [[122](#bib.bib122)] | 28.9% |'
- en: '| [[105](#bib.bib105)] | Donahue et al. [[125](#bib.bib125)] | 30.14% |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| [[105](#bib.bib105)] | Donahue 等 [[125](#bib.bib125)] | 30.14% |'
- en: '|  | SRMR [[60](#bib.bib60)] | 30.3% |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  | SRMR [[60](#bib.bib60)] | 30.3% |'
- en: 'Experimental results vary with different network architecture settings for
    the same manifold constrained method. For example, SPDNet [[50](#bib.bib50)] has
    four different architecture configurations: i) SPDNet-0BiRe without using blocks
    of BiMap/ReEig, ii) SPDNet-1BiRe using $1$ block of BiMap/ReEig, iii) SPDNet-2BiRe
    using $2$ blocks of BiMap/ReEig, and iv) SPDNet-3BiRe using $3$ blocks of BiMap/ReEig.
    GrNet [[51](#bib.bib51)] has three different configurations: i) GrNet-0Block without
    using blocks of Projection-Pooling, ii) GrNet-1Block using $1$ block of Projection-Pooling,
    and iii) GrNet-2Block using $2$ blocks of Projection-Pooling. These methods studied
    how different architecture settings affected classification accuracy. Note that
    our article follows the raw settings reported from corresponding articles. On
    that account, this article did not present classification accuracy under different
    architecture configurations.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果因相同流形约束方法下不同的网络架构设置而异。例如，SPDNet [[50](#bib.bib50)] 有四种不同的架构配置：i) SPDNet-0BiRe
    不使用 BiMap/ReEig 块，ii) SPDNet-1BiRe 使用 $1$ 个 BiMap/ReEig 块，iii) SPDNet-2BiRe 使用
    $2$ 个 BiMap/ReEig 块，iv) SPDNet-3BiRe 使用 $3$ 个 BiMap/ReEig 块。GrNet [[51](#bib.bib51)]
    有三种不同的配置：i) GrNet-0Block 不使用 Projection-Pooling 块，ii) GrNet-1Block 使用 $1$ 个 Projection-Pooling
    块，iii) GrNet-2Block 使用 $2$ 个 Projection-Pooling 块。这些方法研究了不同架构设置如何影响分类准确性。请注意，我们的文章遵循了来自相关文章的原始设置。因此，本文未展示不同架构配置下的分类准确性。
- en: 7 Conclusions and Future Work
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与未来工作
- en: In this article, a survey on recent advances in applying geometric optimization
    to deep learning is presented. This article reviewed progress of optimizing deep
    learning networks on manifolds according to the classification of deep learning
    backbones (e.g., CNN, RNN, and GNN). In particular, this article discussed the
    theory and toolboxes for geometric optimization. Although geometric optimization
    brings various advantages to deep learning methods, it still suffers from the
    following challenges.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对应用几何优化于深度学习的最新进展进行了综述。本文回顾了根据深度学习骨干网络的分类（例如，CNN、RNN 和 GNN）在流形上优化深度学习网络的进展。特别地，本文讨论了几何优化的理论和工具箱。尽管几何优化为深度学习方法带来了各种优势，但仍面临以下挑战。
- en: '-'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '-'
- en: Dataset-Oriented Geometric Optimization. Various methods (e.g., uRNN [[14](#bib.bib14)]
    and Cheap Orthogonal Constraints in Neural Networks [[13](#bib.bib13)]) utilize
    small image datasets such as MNIST handwritten digits to validate the effectiveness
    of geometric optimization. Whether geometric optimization can achieve good performance
    on enormous and complicated datasets such as Penn Tree Bank (PTB) needs further
    research. This prompts researchers to use more challenging datasets to verify
    the performance of deep learning techniques after applying geometric optimization.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 面向数据集的几何优化。各种方法（例如，uRNN [[14](#bib.bib14)] 和神经网络中的便宜正交约束 [[13](#bib.bib13)]）利用小型图像数据集如
    MNIST 手写数字来验证几何优化的有效性。几何优化是否能在如 Penn Tree Bank (PTB) 等庞大而复杂的数据集上取得良好性能仍需进一步研究。这促使研究人员使用更多具有挑战性的数据集来验证应用几何优化后的深度学习技术的性能。
- en: '-'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '-'
- en: Model-Oriented Geometric Optimization. Although optimizing deep learning networks
    such as CNN and RNN on the Riemannian manifold has been proven successful, geometric
    optimization has not been applied to all deep learning methods. For example, there
    is a lack of research in optimizing reinforcement learning and federated learning
    on manifolds, which is crucial in automatic control and privacy protection. This
    forces researchers to further explore the potential and benefit of optimizing
    more deep learning networks from a geometric perspective.
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 面向模型的几何优化。尽管在黎曼流形上优化深度学习网络如 CNN 和 RNN 已被证明成功，但几何优化尚未应用于所有深度学习方法。例如，关于在流形上优化强化学习和联邦学习的研究仍然不足，这在自动控制和隐私保护中至关重要。这迫使研究人员进一步**挖掘**从几何角度优化更多深度学习网络的潜力和好处。
- en: '-'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '-'
- en: Manifold-Oriented Geometric Optimization. Manifold geometry plays an important
    role in geometric optimization and different manifolds have different applications.
    For instance, the orthogonal manifold can be used to alleviate feature redundancy
    and oblique manifold can be utilized for optimizing dictionary learning. However,
    applications of certain manifolds such as centered matrix manifold remain blank
    in the literature. This motivates researchers to exploit and use manifold structures
    for geometric optimization applications as much as possible.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 流形导向的几何优化。流形几何在几何优化中发挥着重要作用，不同的流形具有不同的应用。例如，正交流形可以用来缓解特征冗余，而倾斜流形可以用于优化字典学习。然而，某些流形如中心矩阵流形在文献中的应用仍然空白。这激励研究人员尽可能多地利用流形结构进行几何优化应用。
- en: This article demonstrated that geometric optimization can grasp advantage of
    the geometry information of search space, speed up the optimization process, and
    mitigate gradient explosion and vanishing problems. However, considering unexplored
    deep learning methods such as reinforcement learning, together with unused manifold
    structures such as centered matrix manifold, it is still a huge challenge to push
    the boundaries of geometric optimization in deep learning.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了几何优化可以利用搜索空间的几何信息，加快优化过程，并缓解梯度爆炸和消失问题。然而，考虑到未探索的深度学习方法如强化学习，以及未使用的流形结构如中心矩阵流形，推动几何优化在深度学习中的边界仍然是一个巨大的挑战。
- en: References
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep
    learning. 2016.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Ian Goodfellow、Yoshua Bengio、Aaron Courville 和 Yoshua Bengio。深度学习。2016。'
- en: '[2] Raphael JL Townshend, Stephan Eismann, Andrew M Watkins, Ramya Rangan,
    Maria Karelina, Rhiju Das, and Ron O Dror. Geometric deep learning of rna structure.
    Science, 373(6558):1047–1051, 2021.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Raphael JL Townshend、Stephan Eismann、Andrew M Watkins、Ramya Rangan、Maria
    Karelina、Rhiju Das 和 Ron O Dror。RNA结构的几何深度学习。《科学》，373(6558)：1047–1051，2021。'
- en: '[3] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional
    neural networks. In International Conference on Machine Learning (ICML), pages
    6105–6114, 2019.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Mingxing Tan 和 Quoc Le。Efficientnet：重新思考卷积神经网络的模型扩展。在国际机器学习会议（ICML），页码6105–6114，2019。'
- en: '[4] Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, and David Barber.
    Wider and deeper, cheaper and faster: Tensorized lstms for sequence learning.
    In Advances in Neural Information Processing Systems, pages 1–11, 2017.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Zhen He、Shaobing Gao、Liang Xiao、Daxue Liu、Hangen He 和 David Barber。更宽更深，更便宜更快：用于序列学习的张量化LSTM。在神经信息处理系统进展，页码1–11，2017。'
- en: '[5] David Rolnick and Max Tegmark. The power of deeper networks for expressing
    natural functions. In International Conference on Learning Representations (ICLR),
    2018.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] David Rolnick 和 Max Tegmark。深度网络在表达自然函数方面的力量。在国际学习表征会议（ICLR），2018。'
- en: '[6] Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, and Stella X Yu. Orthogonal
    convolutional neural networks. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 11505–11515, 2020.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Jiayun Wang、Yubei Chen、Rudrasis Chakraborty 和 Stella X Yu。正交卷积神经网络。在IEEE计算机视觉与模式识别会议（CVPR）论文集中，页码11505–11515，2020。'
- en: '[7] M. Storath and A. Weinmann. Variational regularization of inverse problems
    for manifold-valued data. Information and Inference: A Journal of the IMA, 10(1):195–230,
    2021.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. Storath 和 A. Weinmann。流形值数据的变分正则化。信息与推断：IMA期刊，10(1)：195–230，2021。'
- en: '[8] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms
    on matrix manifolds. 2008.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] P-A Absil、Robert Mahony 和 Rodolphe Sepulchre。矩阵流形上的优化算法。2008。'
- en: '[9] Nicolas Boumal. An introduction to optimization on smooth manifolds. Available
    online, Aug 2020.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Nicolas Boumal。光滑流形上的优化简介。在线提供，2020年8月。'
- en: '[10] Jiang Hu, Xin Liu, Zai-Wen Wen, and Ya-Xiang Yuan. A brief introduction
    to manifold optimization. Journal of the Operations Research Society of China,
    8(2):199–248, 2020.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Jiang Hu、Xin Liu、Zai-Wen Wen 和 Ya-Xiang Yuan。流形优化简要介绍。《中国运筹学会期刊》，8(2)：199–248，2020。'
- en: '[11] Xian Wei. Learning Image and Video Representations Based on Sparsity Priors.
    2017.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Xian Wei。基于稀疏先验的图像和视频表示学习。2017。'
- en: '[12] Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from
    orthogonality regularizations in training deep cnns. In Proceedings of the International
    Conference on Neural Information Processing Systems, pages 4266–4276, 2018.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Nitin Bansal, Xiaohan Chen, 和 Zhangyang Wang. 我们能从训练深度卷积神经网络中的正交正则化中获得更多收益吗？发表于国际神经信息处理系统会议论文集，页面4266–4276，2018年。'
- en: '[13] Mario Lezcano-Casado and David Martınez-Rubio. Cheap orthogonal constraints
    in neural networks: A simple parametrization of the orthogonal and unitary group.
    In International Conference on Machine Learning (ICML), pages 3794–3803, 2019.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Mario Lezcano-Casado 和 David Martınez-Rubio. 神经网络中的便宜正交约束：正交和单位群的简单参数化。发表于国际机器学习会议（ICML），页面3794–3803，2019年。'
- en: '[14] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent
    neural networks. In International Conference on Machine Learning (ICML), pages
    1120–1128, 2016.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Martin Arjovsky, Amar Shah, 和 Yoshua Bengio. 单位演化递归神经网络。发表于国际机器学习会议（ICML），页面1120–1128，2016年。'
- en: '[15] Yanhong Fei, Yingjie Liu, Xian Wei, and Mingsong Chen. O-vit: Orthogonal
    vision transformer. arXiv preprint arXiv:2201.12133, 2022.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Yanhong Fei, Yingjie Liu, Xian Wei, 和 Mingsong Chen. O-vit: 正交视觉变换器。arXiv预印本
    arXiv:2201.12133，2022年。'
- en: '[16] J. Zhou, M. N. Do, and J Kovačević. Ieee transactions on image processing
    1 special paraunitary matrices, cayley transform, and multidimensional orthogonal
    filter banks. IEEE Transactions on Image Processing, 14(6):760, 2008.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Zhou, M. N. Do, 和 J Kovačević. IEEE图像处理学报 1 专辑：伪单位矩阵、凯利变换和多维正交滤波器组。IEEE图像处理学报，14(6):760，2008年。'
- en: '[17] P Rodríguez, J Gonzàlez, G. Cucurull, J. M. Gonfaus, and X. Roca. Regularizing
    cnns with locally constrained decorrelations. 2017.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] P Rodríguez, J Gonzàlez, G. Cucurull, J. M. Gonfaus, 和 X. Roca. 使用局部约束去相关的卷积神经网络正则化方法。2017年。'
- en: '[18] Jorge Nocedal and Stephen Wright. Numerical optimization. 2006.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Jorge Nocedal 和 Stephen Wright. 数值优化。2006年。'
- en: '[19] David G Luenberger, Yinyu Ye, et al. Linear and nonlinear programming,
    volume 2. 1984.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] David G Luenberger, Yinyu Ye 等. 线性与非线性规划，第2卷。1984年。'
- en: '[20] Daniel Gabay. Minimizing a differentiable function over a differential
    manifold. Journal of Optimization Theory and Applications, 37(2):177–219, 1982.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Daniel Gabay. 在微分流形上最小化可微函数。优化理论与应用期刊，37(2):177–219，1982年。'
- en: '[21] Roger W Brockett. Differential geometry and the design of gradient algorithms.
    In Proc. Symp. Pure Math., AMS, pages 69–92, 1993.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Roger W Brockett. 微分几何与梯度算法的设计。发表于美国数学学会纯数学研讨会论文集，页面69–92，1993年。'
- en: '[22] Alan Edelman, Tomás A Arias, and Steven T Smith. The geometry of algorithms
    with orthogonality constraints. SIAM journal on Matrix Analysis and Applications,
    20(2):303–353, 1998.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Alan Edelman, Tomás A Arias, 和 Steven T Smith. 带正交约束的算法几何。SIAM矩阵分析与应用期刊，20(2):303–353，1998年。'
- en: '[23] Simon Hawe, Matthias Seibert, and Martin Kleinsteuber. Separable dictionary
    learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), pages 438–445, June 2013.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Simon Hawe, Matthias Seibert, 和 Martin Kleinsteuber. 可分字典学习。发表于IEEE计算机视觉与模式识别会议论文集（CVPR），页面438–445，2013年6月。'
- en: '[24] Xian Wei, Hao Shen, and Martin Kleinsteuber. Trace quotient meets sparsity:
    A method for learning low dimensional image representations. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5268–5277,
    2016.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Xian Wei, Hao Shen, 和 Martin Kleinsteuber. 跟踪商遇到稀疏性：一种学习低维图像表示的方法。发表于IEEE计算机视觉与模式识别会议论文集（CVPR），页面5268–5277，2016年。'
- en: '[25] P-A Absil, Christopher G Baker, and Kyle A Gallivan. Trust-region methods
    on riemannian manifolds. Foundations of Computational Mathematics, 7(3):303–330,
    2007.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] P-A Absil, Christopher G Baker, 和 Kyle A Gallivan. 在黎曼流形上的信任域方法。计算数学基础，7(3):303–330，2007年。'
- en: '[26] Jean-Pierre Dedieu, Pierre Priouret, and Gregorio Malajovich. Newton’s
    method on riemannian manifolds: covariant alpha theory. IMA Journal of Numerical
    Analysis, 23(3):395–419, 2003.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Jean-Pierre Dedieu, Pierre Priouret, 和 Gregorio Malajovich. 在黎曼流形上的牛顿法：协变α理论。IMA数值分析期刊，23(3):395–419，2003年。'
- en: '[27] Xian Wei, Yuanxiang Li, Hao Shen, Fang Chen, Martin Kleinsteuber, and
    Zhongfeng Wang. Dynamical textures modeling via joint video dictionary learning.
    IEEE Transactions on Image Processing, 26(6):2929–2943, 2017.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Xian Wei, Yuanxiang Li, Hao Shen, Fang Chen, Martin Kleinsteuber, 和 Zhongfeng
    Wang. 通过联合视频字典学习进行动态纹理建模。IEEE图像处理学报，26(6):2929–2943，2017年。'
- en: '[28] Simon Alois Hawe. Learning sparse data models via geometric optimization
    with applications to image processing. PhD thesis, Technische Universität München,
    2013.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Simon Alois Hawe. 通过几何优化学习稀疏数据模型及其在图像处理中的应用。博士论文，慕尼黑工业大学，2013年。'
- en: '[29] S. Kumar, Z. Mhammedi, and M. Harandi. Geometry aware constrained optimization
    techniques for deep learning. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 4460–4469, 2018.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. Kumar, Z. Mhammedi, 和 M. Harandi. 面向几何的约束优化技术用于深度学习。在 IEEE 计算机视觉与模式识别会议（CVPR）论文集中,
    页码 4460–4469, 2018。'
- en: '[30] John M Lee. Smooth manifolds. In Introduction to Smooth Manifolds, pages
    1–31\. Springer, 2013.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] John M Lee. 光滑流形。在《光滑流形导论》中, 页码 1–31\. Springer, 2013。'
- en: '[31] Diego Tosato, Michela Farenzena, Mauro Spera, Vittorio Murino, and Marco
    Cristani. Multi-class classification on riemannian manifolds for video surveillance.
    In ECCV, pages 378–391, 2010.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Diego Tosato, Michela Farenzena, Mauro Spera, Vittorio Murino, 和 Marco
    Cristani. 利用黎曼流形进行多类分类的视频监控。在 ECCV, 页码 378–391, 2010。'
- en: '[32] Mehrtash T Harandi, Mathieu Salzmann, and Richard Hartley. From manifold
    to manifold: Geometry-aware dimensionality reduction for spd matrices. In European
    Conference on Computer Vision (ECCV), pages 17–32, 2014.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Mehrtash T Harandi, Mathieu Salzmann, 和 Richard Hartley. 从流形到流形：几何感知的
    SPD 矩阵降维。在欧洲计算机视觉会议（ECCV）, 页码 17–32, 2014。'
- en: '[33] Zhiwu Huang, Ruiping Wang, Shiguang Shan, Xianqiu Li, and Xilin Chen.
    Log-euclidean metric learning on symmetric positive definite manifold with application
    to image set classification. In Proceedings of International Conference on Machine
    Learning (ICMR), volume 37, pages 720–729, 2015.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Zhiwu Huang, Ruiping Wang, Shiguang Shan, Xianqiu Li, 和 Xilin Chen. 对称正定流形上的对数欧几里得度量学习及其在图像集分类中的应用。在国际机器学习会议（ICMR）论文集中,
    第 37 卷, 页码 720–729, 2015。'
- en: '[34] Jieping Ye and Qi Li. Lda/qr: an efficient and effective dimension reduction
    algorithm and its theoretical foundation. Pattern Recognition, 37(4):851–854,
    2004.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Jieping Ye 和 Qi Li. Lda/qr: 一种高效且有效的降维算法及其理论基础。模式识别, 37(4):851–854, 2004。'
- en: '[35] John NR Jeffers. Two case studies in the application of principal component
    analysis. Journal of the Royal Statistical Society: Series C (Applied Statistics),
    16(3):225–236, 1967.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] John NR Jeffers. 主成分分析应用的两个案例研究。皇家统计学会杂志：C 系列（应用统计），16(3):225–236, 1967。'
- en: '[36] Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis.
    Chemometrics and Intelligent Laboratory Systems, 2(1-3):37–52, 1987.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Svante Wold, Kim Esbensen, 和 Paul Geladi. 主成分分析。化学计量学与智能实验室系统, 2(1-3):37–52,
    1987。'
- en: '[37] Hui Zou and Lingzhou Xue. A selective overview of sparse principal component
    analysis. Proceedings of the IEEE, 106(8):1311–1320, 2018.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Hui Zou 和 Lingzhou Xue. 稀疏主成分分析的选择性概述。IEEE 汇刊, 106(8):1311–1320, 2018。'
- en: '[38] Frank Rehm, Frank Klawonn, and Rudolf Kruse. Mds polar: A new approach
    for dimension reduction to visualize high dimensional data. In International Symposium
    on Intelligent Data Analysis, pages 316–327, 2005.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Frank Rehm, Frank Klawonn, 和 Rudolf Kruse. Mds polar: 一种用于高维数据可视化的新方法。在国际智能数据分析研讨会,
    页码 316–327, 2005。'
- en: '[39] Andreas Buja, Deborah F Swayne, Michael L Littman, Nathaniel Dean, Heike
    Hofmann, and Lisha Chen. Data visualization with multidimensional scaling. Journal
    of Computational and Graphical Statistics, 17(2):444–472, 2008.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Andreas Buja, Deborah F Swayne, Michael L Littman, Nathaniel Dean, Heike
    Hofmann, 和 Lisha Chen. 使用多维尺度进行数据可视化。计算与图形统计学杂志, 17(2):444–472, 2008。'
- en: '[40] Mingyu Fan, Hong Qiao, Bo Zhang, and Xiaoqin Zhang. Isometric multi-manifold
    learning for feature extraction. In International Conference on Data Mining, pages
    241–250. IEEE, 2012.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Mingyu Fan, Hong Qiao, Bo Zhang, 和 Xiaoqin Zhang. 等距多流形学习用于特征提取。在数据挖掘国际会议上,
    页码 241–250. IEEE, 2012。'
- en: '[41] Yepeng Ni, Jianping Chai, Yan Wang, and Weidong Fang. A fast radio map
    construction method merging self-adaptive local linear embedding (lle) and graph-based
    label propagation in wlan fingerprint localization systems. Sensors, 20(3):767,
    2020.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Yepeng Ni, Jianping Chai, Yan Wang, 和 Weidong Fang. 一种快速的无线局域网指纹定位系统中的自适应局部线性嵌入
    (LLE) 和基于图的标签传播合并的方法。传感器, 20(3):767, 2020。'
- en: '[42] Bo Li, Yan-Rui Li, and Xiao-Long Zhang. A survey on laplacian eigenmaps
    based manifold learning methods. Neurocomputing, 335:336–351, 2019.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Bo Li, Yan-Rui Li, 和 Xiao-Long Zhang. 基于拉普拉斯特征映射的流形学习方法综述。神经计算, 335:336–351,
    2019。'
- en: '[43] Rong Wang, Feiping Nie, Richang Hong, Xiaojun Chang, Xiaojun Yang, and
    Weizhong Yu. Fast and orthogonal locality preserving projections for dimensionality
    reduction. IEEE Transactions on Image Processing, 26(10):5019–5030, 2017.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Rong Wang, Feiping Nie, Richang Hong, Xiaojun Chang, Xiaojun Yang, 和 Weizhong
    Yu. 快速且正交的局部保持投影用于降维。IEEE 图像处理汇刊, 26(10):5019–5030, 2017。'
- en: '[44] Effrosini Kokiopoulou, Jie Chen, and Yousef Saad. Trace optimization and
    eigenproblems in dimension reduction methods. Numerical Linear Algebra with Applications,
    18(3):565–602, 2011.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Effrosini Kokiopoulou, Jie Chen 和 Yousef Saad. 迹优化和维度减少方法中的特征值问题。数值线性代数及其应用,
    18(3):565–602, 2011。'
- en: '[45] Albert Tarantola. Inverse problem theory and methods for model parameter
    estimation. 2005.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Albert Tarantola. 逆问题理论与模型参数估计方法。2005。'
- en: '[46] Simon Hawe, Martin Kleinsteuber, and Klaus Diepold. Analysis operator
    learning and its application to image reconstruction. IEEE Transactions on Image
    Processing, 22(6):2138–2150, 2013.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Simon Hawe, Martin Kleinsteuber 和 Klaus Diepold. 分析算子学习及其在图像重建中的应用。IEEE
    图像处理学报, 22(6):2138–2150, 2013。'
- en: '[47] W. P. Krijnen. Positive loadings and factor correlations from positive
    covariance matrices. Psychometrika, 69(4):655–660, 2004.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] W. P. Krijnen. 从正协方差矩阵中提取的正加载和因子相关性。心理计量学, 69(4):655–660, 2004。'
- en: '[48] Pavan Turaga and Rama Chellappa. Locally time-invariant models of human
    activities using trajectories on the grassmannian. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR), pages 2435–2441,
    2009.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Pavan Turaga 和 Rama Chellappa. 使用Grassmann流形上的轨迹的人类活动局部时间不变模型。在IEEE计算机视觉与模式识别会议
    (CVPR) 论文集中, 页 2435–2441, 2009。'
- en: '[49] M. Ozay and Takayuki Okatani. Optimization on submanifolds of convolution
    kernels in cnns. ArXiv, abs/1610.07008:arXiv–1610, 2016.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] M. Ozay 和 Takayuki Okatani. 卷积核在卷积神经网络中的子流形优化。ArXiv, abs/1610.07008:arXiv–1610,
    2016。'
- en: '[50] Z. Huang and L. Gool. A riemannian network for spd matrix learning. In
    Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI), pages
    2036–2042, 2017.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Z. Huang 和 L. Gool. 一种用于SPD矩阵学习的黎曼网络。在第31届AAAI人工智能会议 (AAAI) 论文集中, 页 2036–2042,
    2017。'
- en: '[51] Z. Huang, Jiqing Wu, and L. Gool. Building deep networks on grassmann
    manifolds. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
    pages 3279–3286, 2018.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Z. Huang, Jiqing Wu 和 L. Gool. 在Grassmann流形上构建深度网络。在AAAI人工智能会议 (AAAI)
    论文集中, 页 3279–3286, 2018。'
- en: '[52] Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks.
    Advances in Neural Information Processing Systems, 32:8230–8241, 2019.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Qi Liu, Maximilian Nickel 和 Douwe Kiela. 双曲图神经网络。神经信息处理系统进展, 32:8230–8241,
    2019。'
- en: '[53] Shichao Zhu, Shirui Pan, Chuan Zhou, Jia Wu, Yanan Cao, and Bin Wang.
    Graph geometry interaction learning. In Advances in Neural Information Processing
    Systems, volume 33, pages 7548–7558, 2020.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Shichao Zhu, Shirui Pan, Chuan Zhou, Jia Wu, Yanan Cao 和 Bin Wang. 图几何交互学习。在神经信息处理系统进展,
    第33卷, 页 7548–7558, 2020。'
- en: '[54] Rahul Chauhan, Kamal Kumar Ghanshala, and RC Joshi. Convolutional neural
    network (cnn) for image detection and recognition. In International Conference
    on Secure Cyber Computing and Communication (ICSCCC), pages 278–282\. IEEE, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Rahul Chauhan, Kamal Kumar Ghanshala 和 RC Joshi. 用于图像检测和识别的卷积神经网络 (cnn)。在国际安全网络计算与通信会议
    (ICSCCC) 论文集中, 页 278–282. IEEE, 2018。'
- en: '[55] Aliasghar Mortazi and Ulas Bagci. Automatically designing cnn architectures
    for medical image segmentation. In International Workshop on Machine Learning
    in Medical Imaging, pages 98–106, 2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Aliasghar Mortazi 和 Ulas Bagci. 自动设计医学图像分割的cnn架构。在医学成像中的机器学习国际研讨会论文集中,
    页 98–106, 2018。'
- en: '[56] Jihun Hamm and Daniel D Lee. Grassmann discriminant analysis: a unifying
    view on subspace-based learning. In Proceedings of the 25th International Conference
    on Machine Learning (ICML), pages 376–383, 2008.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Jihun Hamm 和 Daniel D Lee. Grassmann判别分析：基于子空间学习的统一视角。在第25届国际机器学习会议 (ICML)
    论文集中, 页 376–383, 2008。'
- en: '[57] Lei Zhang, Xiantong Zhen, Ling Shao, and Jingkuan Song. Learning match
    kernels on grassmann manifolds for action recognition. IEEE Transactions on Image
    Processing, 28(1):205–215, 2019.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Lei Zhang, Xiantong Zhen, Ling Shao 和 Jingkuan Song. 在Grassmann流形上学习匹配核用于动作识别。IEEE
    图像处理学报, 28(1):205–215, 2019。'
- en: '[58] Mengyi Liu, Ruiping Wang, Shaoxin Li, Shiguang Shan, Zhiwu Huang, and
    Xilin Chen. Combining multiple kernel methods on riemannian manifold for emotion
    recognition in the wild. In Proceedings of International Conference on multimodal
    interaction, pages 494–501, 2014.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Mengyi Liu, Ruiping Wang, Shaoxin Li, Shiguang Shan, Zhiwu Huang 和 Xilin
    Chen. 在黎曼流形上结合多核方法进行野外情感识别。在国际多模态交互会议论文集中, 页 494–501, 2014。'
- en: '[59] Walid Hariri and Nadir Farah. Efficient graph-based kernel using covariance
    descriptors for 3d facial expression classification. In Proceedings of International
    Conference on Intelligent Systems and Pattern Recognition, pages 7–11, 2020.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Walid Hariri 和 Nadir Farah. 使用协方差描述符的高效图形核用于3D面部表情分类. 载于《国际智能系统与模式识别会议论文集》，第7–11页，2020年。'
- en: '[60] Yuan Yuan, Lichao Mou, and Xiaoqiang Lu. Scene recognition by manifold
    regularized deep learning architecture. IEEE Transactions on Neural Networks and
    Learning Systems, 26(10):2222–2233, 2015.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Yuan Yuan, Lichao Mou, 和 Xiaoqiang Lu. 通过流形正则化深度学习架构进行场景识别. 《IEEE神经网络与学习系统汇刊》，26(10):2222–2233，2015年。'
- en: '[61] Chaoqun Hong, Jun Yu, Jian Zhang, Xiongnan Jin, and Kyong-Ho Lee. Multimodal
    face-pose estimation with multitask manifold deep learning. IEEE Transactions
    on Industrial Informatics, 15(7):3952–3961, 2018.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Chaoqun Hong, Jun Yu, Jian Zhang, Xiongnan Jin, 和 Kyong-Ho Lee. 多模态人脸姿势估计与多任务流形深度学习.
    《IEEE工业信息学汇刊》，15(7):3952–3961，2018年。'
- en: '[62] Jean-Michel Roufosse, Abhishek Sharma, and Maks Ovsjanikov. Unsupervised
    deep learning for structured shape matching. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV), pages 1617–1627, 2019.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Jean-Michel Roufosse, Abhishek Sharma, 和 Maks Ovsjanikov. 用于结构化形状匹配的无监督深度学习.
    载于《IEEE/CVF国际计算机视觉会议论文集》，第1617–1627页，2019年。'
- en: '[63] Zhiwu Huang, Chengde Wan, Thomas Probst, and Luc Van Gool. Deep learning
    on lie groups for skeleton-based action recognition. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR), pages 6099–6108,
    2017.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Zhiwu Huang, Chengde Wan, Thomas Probst, 和 Luc Van Gool. 在Lie群上进行深度学习以实现基于骨架的动作识别.
    载于《IEEE计算机视觉与模式识别会议论文集》，第6099–6108页，2017年。'
- en: '[64] Xin Chen, Jian Weng, Wei Lu, Jiaming Xu, and Jiasi Weng. Deep manifold
    learning combined with convolutional neural networks for action recognition. IEEE
    Transactions on Neural Networks and Learning Systems, 29(9):3938–3952, 2017.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Xin Chen, Jian Weng, Wei Lu, Jiaming Xu, 和 Jiasi Weng. 结合卷积神经网络的深度流形学习用于动作识别.
    《IEEE神经网络与学习系统汇刊》，29(9):3938–3952，2017年。'
- en: '[65] Razvan Pascanu, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. How
    to construct deep recurrent neural networks. ArXiv, abs/1312.6026:arXiv–1312,
    2014.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Razvan Pascanu, Çaglar Gülçehre, Kyunghyun Cho, 和 Yoshua Bengio. 如何构建深度递归神经网络.
    ArXiv, abs/1312.6026:arXiv–1312，2014年。'
- en: '[66] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier
    neural networks. In Proceedings of International Conference on Artificial Intelligence
    and Statistics, pages 315–323, 2011.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Xavier Glorot, Antoine Bordes, 和 Yoshua Bengio. 深度稀疏整流神经网络. 载于《国际人工智能与统计学会议论文集》，第315–323页，2011年。'
- en: '[67] Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted
    boltzmann machines. In Proceedings of the International Conference on International
    Conference on Machine Learning, page 807–814, 2010.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Vinod Nair 和 Geoffrey E. Hinton. 整流线性单元改善限制玻尔兹曼机. 载于《国际机器学习会议论文集》，第807–814页，2010年。'
- en: '[68] Sekitoshi Kanai, Yasuhiro Fujiwara, and Sotetsu Iwamura. Preventing gradient
    explosions in gated recurrent units. In Proceedings of the 31st International
    Conference on Neural Information Processing Systems, pages 435–444, 2017.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Sekitoshi Kanai, Yasuhiro Fujiwara, 和 Sotetsu Iwamura. 防止门控递归单元中的梯度爆炸.
    载于《第31届国际神经信息处理系统会议论文集》，第435–444页，2017年。'
- en: '[69] Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas.
    Full-capacity unitary recurrent neural networks. 10 2016.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, 和 Les Atlas.
    全容量单位递归神经网络. 2016年10月。'
- en: '[70] Stephanie L Hyland and Gunnar Rätsch. Learning unitary operators with
    help from u(n). In Proceedings of the Thirty-First AAAI Conference on Artificial
    Intelligence, pages 2050–2058, 2017.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Stephanie L Hyland 和 Gunnar Rätsch. 在U(n)的帮助下学习单位算符. 载于《第31届AAAI人工智能会议论文集》，第2050–2058页，2017年。'
- en: '[71] Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, Yongliang Wang, and Bo Li.
    Orthogonal weight normalization: Solution to optimization over multiple dependent
    stiefel manifolds in deep neural networks. In Proceedings of the AAAI Conference
    on Artificial Intelligence (AAAI), pages 3271–3278, 2018.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, Yongliang Wang, 和 Bo
    Li. 正交权重归一化：解决深度神经网络中多个相关斯蒂费尔流形上的优化问题. 载于《AAAI人工智能会议论文集》，第3271–3278页，2018年。'
- en: '[72] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality
    and learning recurrent networks with long term dependencies. In International
    Conference on Machine Learning (ICML), pages 3570–3578, 2017.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Eugene Vorontsov、Chiheb Trabelsi、Samuel Kadoury 和 Chris Pal。关于正交性和学习具有长期依赖的递归网络。国际机器学习大会（ICML），页面3570–3578,
    2017年。'
- en: '[73] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey.
    Efficient orthogonal parametrisation of recurrent neural networks using householder
    reflections. In International Conference on Machine Learning (ICML), pages 2401–2409\.
    PMLR, 2017.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Zakaria Mhammedi、Andrew Hellicar、Ashfaqur Rahman 和 James Bailey。利用Householder反射的递归神经网络的高效正交参数化。国际机器学习大会（ICML），页面2401–2409。PMLR，2017年。'
- en: '[74] Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin
    Soljacic, and Yoshua Bengio. Gated orthogonal recurrent units: On learning to
    forget. Neural Computation, 31(4):765–783, 2019.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] 李静、Caglar Gulcehre、John Peurifoy、Yichen Shen、Max Tegmark、Marin Soljacic
    和 Yoshua Bengio。门控正交递归单元：关于学习遗忘。Neural Computation, 31(4):765–783, 2019。'
- en: '[75] Weifeng Liu, Sichao Fu, Yicong Zhou, Zheng-Jun Zha, and Liqiang Nie. Human
    activity recognition by manifold regularization based dynamic graph convolutional
    networks. Neurocomputing, 444:217–225, 2021.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] 刘伟锋、傅思超、周逸聪、查政军 和 聂立强。通过流形正则化的动态图卷积网络进行人类活动识别。Neurocomputing, 444:217–225,
    2021。'
- en: '[76] Shoubo Feng, Weijie Ren, Min Han, and Yen Wei Chen. Robust manifold broad
    learning system for large-scale noisy chaotic time series prediction: A perturbation
    perspective. Neural Networks, 117:179–190, 2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] 冯寿博、任伟杰、韩敏 和 陈燕伟。用于大规模噪声混沌时间序列预测的鲁棒流形广泛学习系统：一种扰动视角。神经网络，117:179–190, 2019年。'
- en: '[77] Ziwen Ke, Zhuoxu Cui, Wenqi Huang, Jing Cheng, Seng Jia, Haifeng Wang,
    Xin Liu, Hairong Zheng, Leslie Ying, Yanjie Zhu, and Dong Liang. Deep manifold
    learning for dynamic mr imaging. ArXiv, abs/2104.01102:arXiv–2104, 2021.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] 纪文、崔卓旭、黄文琪、程晶、贾生、王海峰、刘欣、郑海容、Leslie Ying、朱艳洁 和 梁栋。动态MRI成像的深度流形学习。ArXiv,
    abs/2104.01102:arXiv–2104, 2021年。'
- en: '[78] Sampurna Biswas, Hemant K Aggarwal, and Mathews Jacob. Dynamic mri using
    model-based deep learning and storm priors: Modl-storm. Magnetic Resonance in
    Medicine, 82(1):485–494, 2019.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Sampurna Biswas、Hemant K Aggarwal 和 Mathews Jacob。使用基于模型的深度学习和风暴先验的动态MRI：MODL-STORM。医学磁共振，82(1):485–494,
    2019年。'
- en: '[79] Tong Zhang, Wenming Zheng, Zhen Cui, and Chaolong Li. Deep manifold-to-manifold
    transforming network. In 2018 25th IEEE International Conference on Image Processing
    (ICIP), pages 4098–4102, 2018.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] 张彤、郑文名、崔震 和 李超龙。深度流形到流形的变换网络。2018年第25届IEEE国际图像处理大会（ICIP），页面4098–4102,
    2018。'
- en: '[80] Zhiwu Huang, Ruiping Wang, Xianqiu Li, Wenxian Liu, Shiguang Shan, Luc
    Van Gool, and Xilin Chen. Geometry-aware similarity learning on spd manifolds
    for visual recognition. IEEE Transactions on Circuits and Systems for Video Technology,
    28(10):2513–2523, 2018.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] 黄志武、王瑞平、李显秋、刘文贤、单世光、Luc Van Gool 和 陈曦林。基于SPD流形的几何感知相似性学习用于视觉识别。IEEE Transactions
    on Circuits and Systems for Video Technology, 28(10):2513–2523, 2018。'
- en: '[81] L. Ambrosio, Italien, and Scuola Normale Superiore). A survey on monge’s
    optimal transport problem. 2009.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] L. Ambrosio、意大利，和 Scuola Normale Superiore）。关于蒙日最优传输问题的调查。2009年。'
- en: '[82] Dai Shi, Junbin Gao, Xia Hong, S. T. Boris Choy, and Zhiyong Wang. Coupling
    matrix manifolds assisted optimization for optimal transport problems. Machine
    Learning, 110(3):533–558, 2021.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Dai Shi、Junbin Gao、Xia Hong、S. T. Boris Choy 和 Zhiyong Wang。辅以矩阵流形优化的最优传输问题优化。机器学习,
    110(3):533–558, 2021年。'
- en: '[83] B. K. Abid and R. M. Gower. Greedy stochastic algorithms for entropy-regularized
    optimal transport problems. 2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] B. K. Abid 和 R. M. Gower。用于熵正则化最优传输问题的贪婪随机算法。2018年。'
- en: '[84] Arnaud Dessein, Nicolas Papadakis, and Jean Luc Rouas. Regularized optimal
    transport and the rot mover’s distance. Journal of Machine Learning Research,
    19, 2016.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Arnaud Dessein、Nicolas Papadakis 和 Jean Luc Rouas。正则化最优传输和ROT移动者的距离。机器学习研究杂志，19,
    2016年。'
- en: '[85] B. Mishra, Ntvs Dev, H. Kasai, and P. Jawanpuria. Manifold optimization
    for optimal transport. 2021.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] B. Mishra、Ntvs Dev、H. Kasai 和 P. Jawanpuria。最优传输的流形优化。2021年。'
- en: '[86] Bamdev Mishra, N T V Satyadev, Hiroyuki Kasai, and Pratik Jawanpuria.
    Manifold optimization for non-linear optimal transport problems. 2021.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Bamdev Mishra、N T V Satyadev、Kasai Hiroyuki 和 Pratik Jawanpuria。非线性最优传输问题的流形优化。2021年。'
- en: '[87] Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc Deisenroth (he/him).
    Matérn gaussian processes on riemannian manifolds. In Advances in Neural Information
    Processing Systems, volume 33, pages 12426–12437, 2020.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky 和 Marc Deisenroth
    (他/他). 在黎曼流形上的马特恩高斯过程. 见神经信息处理系统进展, 第 33 卷, 页 12426–12437, 2020.'
- en: '[88] Noémie Jaquier, Viacheslav Borovitskiy, Andrei Smolensky, Alexander Terenin,
    Tamim Asfour, and Leonel Dario Rozo. Geometry-aware bayesian optimization in robotics
    using riemannian matérn kernels. In CoRL, 2021.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Noémie Jaquier, Viacheslav Borovitskiy, Andrei Smolensky, Alexander Terenin,
    Tamim Asfour 和 Leonel Dario Rozo. 使用黎曼马特恩核的机器人几何感知贝叶斯优化. 见 CoRL, 2021.'
- en: '[89] Arslan Chaudhry, Naeemullah Khan, Puneet Dokania, and Philip Torr. Continual
    learning in low-rank orthogonal subspaces. Advances in Neural Information Processing
    Systems, 33:9900–9911, 2020.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Arslan Chaudhry, Naeemullah Khan, Puneet Dokania 和 Philip Torr. 低秩正交子空间中的持续学习.
    神经信息处理系统进展, 33:9900–9911, 2020.'
- en: '[90] Guanxiong Zeng, Yang Chen, Bo Cui, and Shan Yu. Continual learning of
    context-dependent processing in neural networks. Nature Machine Intelligence,
    1(8):364–372, 2019.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Guanxiong Zeng, Yang Chen, Bo Cui 和 Shan Yu. 神经网络中上下文相关处理的持续学习. 自然机器智能,
    1(8):364–372, 2019.'
- en: '[91] Nicolas Boumal, Bamdev Mishra, Pierre Antoine Absil, and Rodolphe J Sepulchre.
    Manopt, a matlab toolbox for optimization on manifolds. The Journal of Machine
    Learning Research, 15(1):1455–1459, 2014.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Nicolas Boumal, Bamdev Mishra, Pierre Antoine Absil 和 Rodolphe J Sepulchre.
    Manopt，一个用于流形上优化的 MATLAB 工具箱. 机器学习研究期刊, 15(1):1455–1459, 2014.'
- en: '[92] Niklas Koep and Sebastian Weichwald. Pymanopt: A python toolbox for optimization
    on manifolds using automatic differentiation. Journal of Machine Learning Research,
    17:1–5, 2016.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Niklas Koep 和 Sebastian Weichwald. Pymanopt: 一个用于在流形上进行优化的 Python 工具箱，使用自动微分.
    机器学习研究期刊, 17:1–5, 2016.'
- en: '[93] Mayank Meghwanshi, Pratik Jawanpuria, Anoop Kunchukuttan, Hiroyuki Kasai,
    and Bamdev Mishra. Mctorch, a manifold optimization library for deep learning.
    Technical report, arXiv preprint arXiv:1810.01811, 2018.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Mayank Meghwanshi, Pratik Jawanpuria, Anoop Kunchukuttan, Hiroyuki Kasai
    和 Bamdev Mishra. Mctorch，一个用于深度学习的流形优化库. 技术报告, arXiv 预印本 arXiv:1810.01811, 2018.'
- en: '[94] Nina Miolane, Nicolas Guigui, Alice Le Brigant, Johan Mathe, Benjamin
    Hou, Yann Thanwerdas, Stefan Heyder, Olivier Peltre, Niklas Koep, Hadi Zaatiti,
    et al. Geomstats: A python package for riemannian geometry in machine learning.
    Journal of Machine Learning Research, 21(223):1–9, 2020.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Nina Miolane, Nicolas Guigui, Alice Le Brigant, Johan Mathe, Benjamin
    Hou, Yann Thanwerdas, Stefan Heyder, Olivier Peltre, Niklas Koep, Hadi Zaatiti
    等. Geomstats: 用于机器学习中的黎曼几何的 Python 包. 机器学习研究期刊, 21(223):1–9, 2020.'
- en: '[95] Max Kochurov, Rasul Karimov, and Sergei Kozlukov. Geoopt: Riemannian optimization
    in pytorch. ArXiv, abs/2005.02819:arXiv–2005, 2020.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Max Kochurov, Rasul Karimov 和 Sergei Kozlukov. Geoopt: PyTorch 中的黎曼优化.
    ArXiv, abs/2005.02819:arXiv–2005, 2020.'
- en: '[96] Kühnel and Stefan Sommer. Computational anatomy in theano. In Graphs in
    Biomedical Image Analysis, Computational Anatomy and Imaging Genetics, pages 4098–4102,
    2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Kühnel 和 Stefan Sommer. Theano 中的计算解剖学. 见《生物医学图像分析中的图形，计算解剖学和成像遗传学》，页
    4098–4102, 2017.'
- en: '[97] Line Kühnel, Stefan Sommer, and Alexis Arnaudon. Differential geometry
    and stochastic dynamics with deep learning numerics. Applied Mathematics and Computation,
    356:411–437, 2019.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Line Kühnel, Stefan Sommer 和 Alexis Arnaudon. 利用深度学习数值计算的微分几何和随机动力学. 应用数学与计算,
    356:411–437, 2019.'
- en: '[98] M. Müller, T. Röder, M. Clausen, B. Eberhardt, B. Krüger, and A. Weber.
    Documentation mocap database hdm05. Technical Report CG-2007-2, Universität Bonn,
    June 2007.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. Müller, T. Röder, M. Clausen, B. Eberhardt, B. Krüger 和 A. Weber. 文档
    mocap 数据库 hdm05. 技术报告 CG-2007-2, 波恩大学, 2007年6月.'
- en: '[99] Abhinav Dhall, Roland Goecke, Jyoti Joshi, Karan Sikka, and Tom Gedeon.
    Emotion recognition in the wild challenge 2014: Baseline, data and protocol. In
    Proceedings of the 16th International Conference on Multimodal Interaction, pages
    461–466, 2014.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Abhinav Dhall, Roland Goecke, Jyoti Joshi, Karan Sikka 和 Tom Gedeon. 2014
    年“野外情感识别挑战”：基线、数据和协议. 见第16届多模态交互国际会议论文集, 页 461–466, 2014.'
- en: '[100] Lijun Yin, Xiaozhou Wei, Yi Sun, Jun Wang, and M.J. Rosato. A 3d facial
    expression database for facial behavior research. In 7th International Conference
    on Automatic Face and Gesture Recognition (FGR06), pages 211–216, 2006.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Lijun Yin, Xiaozhou Wei, Yi Sun, Jun Wang 和 M.J. Rosato. 用于面部行为研究的 3D
    面部表情数据库. 见第七届自动面部与手势识别国际会议 (FGR06), 页 211–216, 2006.'
- en: '[101] Arman Savran, Neşe Alyüz, Hamdi Dibeklioğlu, Oya Çeliktutan, Berk Gökberk,
    Bülent Sankur, and Lale Akarun. Bosphorus database for 3d face analysis. In European
    Workshop on Biometrics and Identity Management, pages 47–56, 2008.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Arman Savran, Neşe Alyüz, Hamdi Dibeklioğlu, Oya Çeliktutan, Berk Gökberk,
    Bülent Sankur, 和 Lale Akarun. Bosphorus 数据库用于 3D 面部分析。发表于欧洲生物特征识别与身份管理研讨会，页码 47–56，2008
    年。'
- en: '[102] J Ross Beveridge, P Jonathon Phillips, David S Bolme, Bruce A Draper,
    Geof H Givens, Yui Man Lui, Mohammad Nayeem Teli, Hao Zhang, W Todd Scruggs, Kevin W
    Bowyer, et al. The challenge of face recognition from digital point-and-shoot
    cameras. In International Conference on Biometrics: Theory, Applications and Systems
    (BTAS), pages 1–8\. IEEE, 2013.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J Ross Beveridge, P Jonathon Phillips, David S Bolme, Bruce A Draper,
    Geof H Givens, Yui Man Lui, Mohammad Nayeem Teli, Hao Zhang, W Todd Scruggs, Kevin
    W Bowyer 等. 数码快拍相机面部识别的挑战。发表于生物特征识别国际会议：理论、应用与系统 (BTAS)，页码 1–8\. IEEE，2013 年。'
- en: '[103] Li Fei-Fei and Pietro Perona. A bayesian hierarchical model for learning
    natural scene categories. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), volume 2, pages 524–531, 2005.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Li Fei-Fei 和 Pietro Perona. 学习自然场景类别的贝叶斯层次模型。发表于 IEEE 计算机视觉与模式识别会议 (CVPR)
    论文集，卷 2，页码 524–531，2005 年。'
- en: '[104] Li-Jia Li and Li Fei-Fei. What, where and who? classifying events by
    scene and object recognition. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 1–8\. IEEE, 2007.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Li-Jia Li 和 Li Fei-Fei. 什么、哪里和谁？通过场景和对象识别对事件进行分类。发表于 IEEE 计算机视觉与模式识别会议
    (CVPR) 论文集，页码 1–8\. IEEE，2007 年。'
- en: '[105] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio
    Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010
    IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages
    3485–3492, 2010.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, 和 Antonio Torralba.
    Sun 数据库：从修道院到动物园的大规模场景识别。发表于 2010 年 IEEE 计算机学会计算机视觉与模式识别会议，页码 3485–3492，2010 年。'
- en: '[106] Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude
    Oliva. Sun database: Exploring a large collection of scene categories. International
    Journal of Computer Vision, 119(1):3–22, 2016.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, 和 Aude
    Oliva. Sun 数据库：探索大规模场景类别集合。计算机视觉国际期刊，119(1):3–22，2016 年。'
- en: '[107] Mengyi Liu, Shiguang Shan, Ruiping Wang, and Xilin Chen. Learning expressionlets
    on spatio-temporal manifold for dynamic facial expression recognition. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
    1749–1756, 2014.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Mengyi Liu, Shiguang Shan, Ruiping Wang, 和 Xilin Chen. 在时空流形上学习表达式以进行动态面部表情识别。发表于
    IEEE 计算机视觉与模式识别会议 (CVPR) 论文集，页码 1749–1756，2014 年。'
- en: '[108] Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Matrix backpropagation
    for deep networks with structured layers. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 2965–2973, 2015.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Catalin Ionescu, Orestis Vantzos, 和 Cristian Sminchisescu. 用于具有结构层的深度网络的矩阵反向传播。发表于
    IEEE 国际计算机视觉会议论文集，页码 2965–2973，2015 年。'
- en: '[109] Tae-Kyun Kim, Josef Kittler, and Roberto Cipolla. Discriminative learning
    and recognition of image set classes using canonical correlations. IEEE Transactions
    on Pattern Analysis and Machine Intelligence, 29(6):1005–1018, 2007.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Tae-Kyun Kim, Josef Kittler, 和 Roberto Cipolla. 使用典型相关性进行图像集类别的辨别学习和识别。IEEE
    模式分析与机器智能学报，29(6):1005–1018，2007 年。'
- en: '[110] Zhiwu Huang, Ruiping Wang, Shiguang Shan, and Xilin Chen. Projection
    metric learning on grassmann manifold with application to video based face recognition.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pages 140–149, 2015.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Zhiwu Huang, Ruiping Wang, Shiguang Shan, 和 Xilin Chen. 在 Grassmann 流形上进行投影度量学习并应用于基于视频的面部识别。发表于
    IEEE 计算机视觉与模式识别会议 (CVPR) 论文集，页码 140–149，2015 年。'
- en: '[111] Hamit Soyel and Hasan Demirel. Optimal feature selection for 3d facial
    expression recognition using coarse-to-fine classification. Turkish Journal of
    Electrical Engineering and Computer Sciences, 18(6):1031–1040, 2010.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Hamit Soyel 和 Hasan Demirel. 使用粗到细分类的 3D 面部表情识别的最优特征选择。土耳其电气工程与计算机科学学报，18(6):1031–1040，2010
    年。'
- en: '[112] Stefano Berretti, Alberto Del Bimbo, Pietro Pala, Boulbaba Ben Amor,
    and Mohamed Daoudi. A set of selected sift features for 3d facial expression recognition.
    In International Conference on Pattern Recognition, pages 4125–4128\. IEEE, 2010.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Stefano Berretti, Alberto Del Bimbo, Pietro Pala, Boulbaba Ben Amor,
    和 Mohamed Daoudi. 用于 3D 面部表情识别的选定 SIFT 特征集合。发表于模式识别国际会议，页码 4125–4128\. IEEE，2010
    年。'
- en: '[113] Xuan-Phung Huynh, Tien-Duc Tran, and Yong-Guk Kim. Convolutional neural
    network models for facial expression recognition using bu-3dfe database. In Information
    Science and Applications (ICISA) 2016, pages 441–450\. 2016.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Xuan-Phung Huynh, Tien-Duc Tran, 和 Yong-Guk Kim. 使用BU-3DFE数据库的面部表情识别的卷积神经网络模型。发表于信息科学与应用会议（ICISA）2016，第441–450页，2016。'
- en: '[114] Amal Azazi, Syaheerah Lebai Lutfi, Ibrahim Venkat, and Fernando Fernández-Martínez.
    Towards a robust affect recognition: Automatic facial expression recognition in
    3d faces. Expert Systems with Applications, 42(6):3056–3066, 2015.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Amal Azazi, Syaheerah Lebai Lutfi, Ibrahim Venkat, 和 Fernando Fernández-Martínez.
    面向鲁棒情感识别：3D 面部的自动面部表情识别。《专家系统与应用》，42(6)：3056–3066，2015。'
- en: '[115] Soon-Yong Chun, Chan-Su Lee, and Sang-Heon Lee. Facial expression recognition
    using extended local binary patterns of 3d curvature. In Multimedia and Ubiquitous
    Engineering, pages 1005–1012. 2013.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Soon-Yong Chun, Chan-Su Lee, 和 Sang-Heon Lee. 使用扩展的3D曲率局部二值模式进行面部表情识别。发表于多媒体与普适工程会议，第1005–1012页，2013。'
- en: '[116] Yiding Wang, Meng Meng, and Qingkai Zhen. Learning encoded facial curvature
    information for 3d facial emotion recognition. In 2013 7th International Conference
    on Image and Graphics, pages 529–532, 2013.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Yiding Wang, Meng Meng, 和 Qingkai Zhen. 学习编码面部曲率信息以进行3D面部情感识别。发表于2013年第7届国际图像与图形会议，第529–532页，2013。'
- en: '[117] Nicholas Vretos, Nikos Nikolaidis, and Ioannis Pitas. 3d facial expression
    recognition using zernike moments on depth images. In 2011 18th IEEE International
    Conference on Image Processing, pages 773–776, 2011.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Nicholas Vretos, Nikos Nikolaidis, 和 Ioannis Pitas. 使用Zernike矩在深度图像上进行3D面部表情识别。发表于2011年第18届IEEE国际图像处理会议，第773–776页，2011。'
- en: '[118] Jihun Hamm and Daniel D Lee. Extended grassmann kernels for subspace-based
    learning. In Advances in Neural Information Processing Systems (NIPS), pages 601–608,
    2009.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Jihun Hamm 和 Daniel D Lee. 用于子空间学习的扩展Grassmann核。发表于神经信息处理系统（NIPS）进展，第601–608页，2009。'
- en: '[119] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep face recognition.
    2015.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Omkar M Parkhi, Andrea Vedaldi, 和 Andrew Zisserman. 深度人脸识别。2015。'
- en: '[120] Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. Beyond bags of features:
    Spatial pyramid matching for recognizing natural scene categories. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2,
    pages 2169–2178\. IEEE, 2006.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Svetlana Lazebnik, Cordelia Schmid, 和 Jean Ponce. 超越特征袋：用于识别自然场景类别的空间金字塔匹配。发表于IEEE计算机视觉与模式识别会议（CVPR），第2卷，第2169–2178页，IEEE，2006。'
- en: '[121] Mandar Dixit, Nikhil Rasiwasia, and Nuno Vasconcelos. Adapted gaussian
    models for image classification. In CVPR, pages 937–943\. IEEE, 2011.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Mandar Dixit, Nikhil Rasiwasia, 和 Nuno Vasconcelos. 图像分类的自适应高斯模型。发表于计算机视觉与模式识别大会（CVPR），第937–943页，IEEE，2011。'
- en: '[122] Roland Kwitt, Nuno Vasconcelos, and Nikhil Rasiwasia. Scene recognition
    on the semantic manifold. In European Conference on Computer Vision (ECCV), pages
    359–372, 2012.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Roland Kwitt, Nuno Vasconcelos, 和 Nikhil Rasiwasia. 基于语义流形的场景识别。发表于欧洲计算机视觉大会（ECCV），第359–372页，2012。'
- en: '[123] Hanlin Goh, Nicolas Thome, Matthieu Cord, and Joo-Hwee Lim. Learning
    deep hierarchical visual feature coding. IEEE Transactions on Neural Networks
    and Learning Systems, 25(12):2212–2225, 2014.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Hanlin Goh, Nicolas Thome, Matthieu Cord, 和 Joo-Hwee Lim. 学习深度层次视觉特征编码。《IEEE神经网络与学习系统汇刊》，25(12)：2212–2225，2014。'
- en: '[124] Jianxin Wu and James M Rehg. Beyond the euclidean distance: Creating
    effective visual codebooks using the histogram intersection kernel. In IEEE International
    Conference on Computer Vision, pages 630–637, 2009.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Jianxin Wu 和 James M Rehg. 超越欧几里得距离：使用直方图交集核创建有效的视觉词典。发表于IEEE国际计算机视觉会议，第630–637页，2009。'
- en: '[125] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang,
    Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature
    for generic visual recognition. In International Conference on Machine Learning
    (ICML), pages 647–655, 2014.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang,
    Eric Tzeng, 和 Trevor Darrell. Decaf：一种用于通用视觉识别的深度卷积激活特征。发表于国际机器学习大会（ICML），第647–655页，2014。'
