- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:34:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:34:00'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2403.05314] Advances of Deep Learning in Protein Science: A Comprehensive
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2403.05314] 深度学习在蛋白质科学中的进展：一项综合调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.05314](https://ar5iv.labs.arxiv.org/html/2403.05314)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.05314](https://ar5iv.labs.arxiv.org/html/2403.05314)
- en: \useunder
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \useunder
- en: \ul \ArticleTypeREVIEW \Year2022 \Month \Vol \No \DOI \ArtNo \ReceiveDate \ReviseDate
    \AcceptDate \OnlineDate
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \ul \ArticleTypeREVIEW \Year2022 \Month \Vol \No \DOI \ArtNo \ReceiveDate \ReviseDate
    \AcceptDate \OnlineDate
- en: Title keyword 5 for citation Title for citation Title for citation
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Title keyword 5 for citation Title for citation Title for citation
- en: Stan.ZQ.Li@westlake.edu.cn
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Stan.ZQ.Li@westlake.edu.cn
- en: \AuthorMark
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \AuthorMark
- en: Hu B Z, et al
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Hu B Z, et al
- en: \AuthorCitation
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \AuthorCitation
- en: Hu B Z, et al
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Hu B Z, et al
- en: 'Advances of Deep Learning in Protein Science: A Comprehensive Survey'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在蛋白质科学中的进展：一项综合调查
- en: Bozhen HU    Cheng TAN    Lirong WU    Jiangbin ZHENG    Jun XIA    Zhangyang
    GAO    Zicheng LIU    Fandi WU    Guijun ZHANG    Stan Z. LI Zhejiang University,
    Hangzhou 310058, China AI Division, School of Engineering, Westlake University,
    Hangzhou 310030, China Tencent AI Lab, Shenzhen 518054, China Zhejiang University
    of Technology, Hangzhou 310014, China
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Bozhen HU    Cheng TAN    Lirong WU    Jiangbin ZHENG    Jun XIA    Zhangyang
    GAO    Zicheng LIU    Fandi WU    Guijun ZHANG    Stan Z. LI 浙江大学，杭州 310058，中国
    西湖大学工程学院人工智能部，杭州 310030，中国 腾讯AI实验室，深圳 518054，中国 浙江工业大学，杭州 310014，中国
- en: Abstract
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Protein representation learning plays a crucial role in understanding the structure
    and function of proteins, which are essential biomolecules involved in various
    biological processes. In recent years, deep learning has emerged as a powerful
    tool for protein modeling due to its ability to learn complex patterns and representations
    from large-scale protein data. This comprehensive survey aims to provide an overview
    of the recent advances in deep learning techniques applied to protein science.
    The survey begins by introducing the developments of deep learning based protein
    models and emphasizes the importance of protein representation learning in drug
    discovery, protein engineering, and function annotation. It then delves into the
    fundamentals of deep learning, including convolutional neural networks, recurrent
    neural networks, attention models, and graph neural networks in modeling protein
    sequences, structures, and functions, and explores how these techniques can be
    used to extract meaningful features and capture intricate relationships within
    protein data. Next, the survey presents various applications of deep learning
    in the field of proteins, including protein structure prediction, protein-protein
    interaction prediction, protein function prediction, etc. Furthermore, it highlights
    the challenges and limitations of these deep learning techniques and also discusses
    potential solutions and future directions for overcoming these challenges. This
    comprehensive survey provides a valuable resource for researchers and practitioners
    in the field of proteins who are interested in harnessing the power of deep learning
    techniques. It is a hands-on guide for researchers to understand protein science,
    develop powerful protein models, and tackle challenging problems for practical
    purposes. By consolidating the latest advancements and discussing potential avenues
    for improvement, this review contributes to the ongoing progress in protein research
    and paves the way for future breakthroughs in the field.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质表示学习在理解蛋白质的结构和功能中扮演着关键角色，这些蛋白质是涉及各种生物过程的重要生物分子。近年来，深度学习因其从大规模蛋白质数据中学习复杂模式和表示的能力而成为蛋白质建模的强大工具。本综合调查旨在概述最近在蛋白质科学中应用的深度学习技术的进展。调查从介绍基于深度学习的蛋白质模型的发展开始，并强调蛋白质表示学习在药物发现、蛋白质工程和功能注释中的重要性。接着，调查深入探讨了深度学习的基础知识，包括卷积神经网络、递归神经网络、注意力模型和图神经网络在建模蛋白质序列、结构和功能中的应用，并探索了这些技术如何用于提取有意义的特征和捕捉蛋白质数据中的复杂关系。随后，调查介绍了深度学习在蛋白质领域的各种应用，包括蛋白质结构预测、蛋白质-蛋白质相互作用预测、蛋白质功能预测等。此外，它还突出了这些深度学习技术的挑战和局限性，并讨论了克服这些挑战的潜在解决方案和未来方向。这一综合调查为感兴趣于利用深度学习技术的蛋白质领域研究人员和从业者提供了宝贵的资源。它是研究人员理解蛋白质科学、开发强大蛋白质模型并解决实际问题的实用指南。通过整合最新的进展并讨论潜在的改进途径，这篇综述为蛋白质研究的持续进展做出了贡献，并为该领域未来的突破铺平了道路。
- en: 'keywords:'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 'keywords:'
- en: protein representation learning, structure prediction, sequence and structure,
    function, graph neural network
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质表示学习、结构预测、序列与结构、功能、图神经网络
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Proteins are the workhorses of life, playing an essential role in a broad range
    of applications ranging from therapeutics to materials. They are built from twenty
    different basic chemical building blocks (called amino acids), which fold into
    complex ensembles of three-dimensional (3D) structures that determine their functions
    and orchestrate the biological processes of cells [[1](#bib.bib1)]. Protein modeling
    is a vital field in bioinformatics and computational biology, aimed at understanding
    the structure, function, and interactions of proteins. With the rapid advancement
    of deep learning techniques, there has been a significant impact on the field
    of protein [[2](#bib.bib2)], enabling more accurate predictions and facilitating
    breakthroughs in various areas of biological research.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质是生命的主力军，在从治疗到材料的广泛应用中发挥着至关重要的作用。它们由二十种不同的基本化学构件（称为氨基酸）构建，这些氨基酸折叠成复杂的三维（3D）结构，这些结构决定了它们的功能并协调细胞的生物过程[[1](#bib.bib1)]。蛋白质建模是生物信息学和计算生物学中的一个重要领域，旨在理解蛋白质的结构、功能和相互作用。随着深度学习技术的快速进步，该领域的影响显著，提高了预测的准确性，并促进了生物研究各个领域的突破[[2](#bib.bib2)]。
- en: Protein structures determine their interactions with other molecules and their
    ability to perform specific tasks. However, predicting protein structure from
    amino acid sequence is challenging because small perturbations in the sequence
    of a protein can drastically change the protein’s shape and even render it useless,
    and the polypeptide is flexible and can fold into a staggering number of different
    shapes [[3](#bib.bib3), [4](#bib.bib4)]. One way to find out the structure of
    a protein is to use an experimental approach, including X-ray crystallography,
    Nuclear Magnetic Resonance (NMR) Spectroscopy [[5](#bib.bib5)], and cryo-electron
    microscopy (cryo-EM) [[6](#bib.bib6)]. Unfortunately, laboratory approaches for
    structure determination are expensive and cannot be used on all proteins. Therefore,
    protein sequences vastly outnumber available structures and annotations [[7](#bib.bib7)].
    For example, there are about 190K (thousand) structures in the Protein Data Bank
    (PDB) [[8](#bib.bib8)] versus over 500M (million) sequences in UniParc [[9](#bib.bib9)]
    and only approximately 5M Gene Ontology (GO) term triplets in ProteinKG25 [[10](#bib.bib10)],
    including about 600K protein, 50K attribute terms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质结构决定了它们与其他分子的相互作用及其执行特定任务的能力。然而，从氨基酸序列预测蛋白质结构是具有挑战性的，因为蛋白质序列中的微小扰动可以极大地改变蛋白质的形状，甚至使其失去功能，并且多肽是灵活的，可以折叠成大量不同的形状[[3](#bib.bib3),
    [4](#bib.bib4)]。确定蛋白质结构的一种方法是使用实验性方法，包括X射线晶体学、核磁共振（NMR）光谱[[5](#bib.bib5)]和冷冻电镜（cryo-EM）[[6](#bib.bib6)]。不幸的是，实验室结构确定方法成本高昂，不能用于所有蛋白质。因此，蛋白质序列的数量远远超过了可用的结构和注释[[7](#bib.bib7)]。例如，蛋白质数据银行（PDB）中大约有190K（千）个结构[[8](#bib.bib8)]，而UniParc中则有超过500M（百万）个序列[[9](#bib.bib9)]，并且在ProteinKG25中只有大约5M
    Gene Ontology（GO）术语三元组[[10](#bib.bib10)]，其中包括约600K蛋白质和50K属性术语。
- en: '![Refer to caption](img/f35eddf7350057b88d226e9ca646028c.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f35eddf7350057b88d226e9ca646028c.png)'
- en: 'Figure 1: A general framework for deep learning models applied in protein,
    learning protein representations for various applications.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：应用于蛋白质的深度学习模型的通用框架，用于各种应用的蛋白质表示学习。
- en: 'In recent years, there has been a growing interest in applying deep learning
    techniques to proteins. Researchers have recognized the potential of deep learning
    models to learn complex patterns and extract meaningful features from large-scale
    protein data, which includes information from protein sequences, structures, functions,
    and interactions. One particular area of active research is protein representation
    learning (PRL), which draws inspiration from approaches used in natural language
    processing (NLP) and aims to learn representations that can be utilized for various
    downstream tasks [[11](#bib.bib11)]. However, a major challenge in protein research
    is the scarcity of labeled data. Labeling proteins often requires time-consuming
    and resource-intensive laboratory experiments, making it difficult to obtain sufficient
    labeled data for training deep learning models. To address this issue, researchers
    have adopted a pre-train and fine-tune paradigm, similar to what has been performed
    in NLP. This approach involves pre-training a model on a pre-training task, where
    knowledge about the protein data is gained, and then fine-tuning the model on
    a downstream task with a smaller amount of labeled data. Self-supervised learning
    methods are commonly employed during the pre-training phase to learn protein representations.
    One popular pretext task is predicting masked tokens, where the model is trained
    to reconstruct corrupted tokens given the surrounding sequence. Several well-known
    pre-trained protein encoders have been developed, including ProtTrans [[12](#bib.bib12)],
    ESM models [[13](#bib.bib13), [14](#bib.bib14)] and GearNet [[15](#bib.bib15)].
    These pre-trained models have demonstrated their effectiveness in various protein
    tasks and have contributed to advancements in protein research. Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey") illustrates the comprehensive pipeline of deep learning based protein
    models utilized for various tasks.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，越来越多的人开始关注将深度学习技术应用于蛋白质研究。研究人员已经认识到深度学习模型在学习复杂模式和从大规模蛋白质数据中提取有意义特征方面的潜力，这些数据包括蛋白质序列、结构、功能和相互作用的信息。一个特别活跃的研究领域是蛋白质表示学习（PRL），该领域从自然语言处理（NLP）中采用的方法中汲取灵感，旨在学习可以用于各种下游任务的表示[[11](#bib.bib11)]。然而，蛋白质研究中的一个主要挑战是标记数据的稀缺。标记蛋白质通常需要耗时且资源密集的实验室实验，这使得获得足够的标记数据以训练深度学习模型变得困难。为了解决这个问题，研究人员采用了类似于NLP中采用的预训练和微调范式。这种方法包括在预训练任务上对模型进行预训练，从中获取关于蛋白质数据的知识，然后在具有较少标记数据的下游任务上对模型进行微调。在预训练阶段，通常采用自监督学习方法来学习蛋白质表示。一种流行的预训练任务是预测掩码标记，其中模型被训练以根据周围的序列重建损坏的标记。一些著名的预训练蛋白质编码器已经被开发出来，包括ProtTrans[[12](#bib.bib12)]、ESM模型[[13](#bib.bib13),
    [14](#bib.bib14)]和GearNet[[15](#bib.bib15)]。这些预训练模型在各种蛋白质任务中表现出了其有效性，并且推动了蛋白质研究的发展。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")展示了用于各种任务的深度学习蛋白质模型的全面流程。'
- en: 'Deep learning models for proteins are widely used in various applications such
    as protein structure prediction (PSP), property prediction, and protein design.
    One of the key challenges is predicting the 3D structure of proteins from their
    sequences. Computational methods have traditionally taken two approaches: focusing
    on (a) physical interactions or (b) evolutionary principles [[16](#bib.bib16)].
    (a) The physics-based approach simulates the folding process of the amino acid
    chain using molecular dynamics or fragment assembly based on the potential energy
    of the force field. This approach emphasizes physical interactions to form a stable
    3D structure with the lowest free energy state. However, it is highly challenging
    to apply this approach to moderately sized proteins due to the computational complexity
    of molecular simulation, the limited accuracy of fragment assembly, and the difficulty
    in accurately modeling protein physics [[17](#bib.bib17), [18](#bib.bib18)]. (b)
    On the other hand, recent advancements in protein sequencing have resulted in
    a large number of available protein sequences [[19](#bib.bib19), [20](#bib.bib20)],
    enabling the generation of multiple sequence alignments (MSAs) for homologous
    proteins. With the availability of these large-scale datasets and the development
    of deep learning models, evolutionary-based models such as AlphaFold2 (AF2) [[16](#bib.bib16)]
    and recent works [[21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)]
    have achieved remarkable success in PSP. As researchers continue to explore the
    potential of these models, they are now focusing on developing even deeper models
    to address more challenging problems that have yet to be solved.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型在各种应用中被广泛使用，如蛋白质结构预测（PSP）、性质预测和蛋白质设计。其中一个关键挑战是从蛋白质序列中预测其三维结构。传统的计算方法主要采用两种方法：（a）关注物理相互作用或（b）进化原则[[16](#bib.bib16)]。
    (a) 基于物理的 Approach 模拟氨基酸链的折叠过程，使用分子动力学或基于力场势能的片段组装。这种方法强调物理相互作用，以形成具有最低自由能状态的稳定三维结构。然而，由于分子模拟的计算复杂性、片段组装的有限准确性以及准确建模蛋白质物理的难度，将这种方法应用于中等大小的蛋白质是非常具有挑战性的[[17](#bib.bib17),
    [18](#bib.bib18)]。 (b) 另一方面，最近在蛋白质测序方面的进展导致了大量可用的蛋白质序列[[19](#bib.bib19), [20](#bib.bib20)]，使得生成同源蛋白质的多序列比对（MSAs）成为可能。随着这些大规模数据集的可用性和深度学习模型的发展，基于进化的模型，如
    AlphaFold2（AF2）[[16](#bib.bib16)] 和最近的研究[[21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24)] 在 PSP 中取得了显著成功。随着研究人员继续探索这些模型的潜力，他们现在正专注于开发更深层的模型，以解决尚未解决的更具挑战性的问题。
- en: 'In the following sections, we provide definitions, commonly used terms, and
    explanations of various deep learning architectures that have been employed in
    protein research. These architectures include convolutional neural networks (CNNs),
    recurrent neural networks (RNNs), transformer models, and graph neural networks
    (GNNs). Although deep learning models have been increasingly applied in the field
    of protein research, there is still a need for a systematic summary of this fast-growing
    field. Existing surveys related to protein research focus mainly on biological
    applications [[25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)], without delving
    deeper into other important aspects, such as comparing different pre-trained protein
    models. We explore how these architectures have been adapted to be used as protein
    models, summarize and contrast the model architectures used for learning protein
    sequences, structures, and functions. Besides, the models optimized for protein-related
    tasks are discussed, such as PSP, protein-protein interaction (PPI) prediction,
    and protein property prediction, with their innovations and differences being
    highlighted. Furthermore, a collection of resources is also provided, including
    deep protein methods, pre-training databases, and paper lists¹¹1[https://github.com/bozhenhhu/A-Review-of-pLMs-and-Methods-for-Protein-Structure-Prediction](https://github.com/bozhenhhu/A-Review-of-pLMs-and-Methods-for-Protein-Structure-Prediction)²²2[https://github.com/LirongWu/awesome-protein-representation-learning](https://github.com/LirongWu/awesome-protein-representation-learning).
    Finally, this survey presents the limitations and unsolved problems of existing
    methods and proposes possible future research directions. An overview of this
    paper’s organization is shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '在接下来的章节中，我们提供了定义、常用术语以及在蛋白质研究中应用的各种深度学习架构的解释。这些架构包括卷积神经网络（CNNs）、递归神经网络（RNNs）、变换器模型和图神经网络（GNNs）。尽管深度学习模型在蛋白质研究领域的应用越来越广泛，但仍然需要对这一快速发展的领域进行系统的总结。现有的蛋白质研究相关调查主要集中在生物学应用上[[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)]，而未深入探讨其他重要方面，如比较不同的预训练蛋白质模型。我们探讨了这些架构如何被调整用于蛋白质模型，总结和对比了用于学习蛋白质序列、结构和功能的模型架构。此外，还讨论了优化用于蛋白质相关任务的模型，如PSP、蛋白质-蛋白质相互作用（PPI）预测和蛋白质属性预测，并突出了其创新和差异。此外，还提供了一系列资源，包括深度蛋白质方法、预训练数据库和论文列表¹¹1[https://github.com/bozhenhhu/A-Review-of-pLMs-and-Methods-for-Protein-Structure-Prediction](https://github.com/bozhenhhu/A-Review-of-pLMs-and-Methods-for-Protein-Structure-Prediction)²²2[https://github.com/LirongWu/awesome-protein-representation-learning](https://github.com/LirongWu/awesome-protein-representation-learning)。最后，本调查介绍了现有方法的局限性和未解决的问题，并提出了可能的未来研究方向。图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")展示了本文的组织结构概述。'
- en: '![Refer to caption](img/e540c56a3431098abfae495d33e18b0f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e540c56a3431098abfae495d33e18b0f.png)'
- en: 'Figure 2: A general diagram of the organization of this paper.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：本文组织结构的总体示意图。
- en: To the best of our knowledge, this is the first comprehensive survey for proteins,
    specifically focusing on large-scale pre-training models and their connections,
    contrasts, and developments. Our goal is to assist researchers in the field of
    protein and artificial intelligence (AI) in developing more suitable algorithms
    and addressing essential, challenging, and urgent problems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，这是首次针对蛋白质的大规模预训练模型及其关联、对比和发展的全面调查。我们的目标是帮助蛋白质和人工智能（AI）领域的研究人员开发更合适的算法，并解决重要、具有挑战性和紧迫的问题。
- en: 2 Definitions, Notations, and Terms
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 定义、符号和术语
- en: 2.1 Mathematical Definitions
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 数学定义
- en: The sequence of amino acids can be folded into a stable 3D structure, which
    can be represented by a 3D graph as $G=(\mathcal{V},\mathcal{E},X,E)$, where $\mathcal{V}=\{v_{i}\}_{i=1,\ldots,n}$
    and $\mathcal{E}=\left\{\varepsilon_{ij}\right\}_{i,j=1,\ldots,n}$ denote the
    vertex and edge sets with $n$ residues, respectively, and $\mathcal{P}=\{P_{i}\}_{i=1,\ldots,n}$
    is the set of position matrices, where $P_{i}\in\mathbb{R}^{k_{i}\times 3}$ represents
    the position matrix for node $v_{i}$. We treat each amino acid as a graph node
    for a protein, then $k_{i}$ depends on the number of atoms in the $i$-th amino
    acid. The node and edge feature matrices are $X=[\bm{x}_{i}]_{i=1,\ldots,n}$ and
    $E=[\bm{e}_{ij}]_{i,j=1,\ldots,n}$, the feature vectors of node and edge are $\bm{x}_{i}\in\mathbb{R}^{d_{1}}$
    and $\bm{e}_{ij}\in\mathbb{R}^{d_{2}}$, $d_{1}$ and $d_{2}$ are the initial feature
    dimensions. The goal of protein graph representation learning is to form a set
    of low-dimensional embeddings $\bm{z}$ for each protein, which is then applied
    in various downstream tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 氨基酸序列可以折叠成一个稳定的三维结构，这可以用三维图表示为 $G=(\mathcal{V},\mathcal{E},X,E)$，其中 $\mathcal{V}=\{v_{i}\}_{i=1,\ldots,n}$
    和 $\mathcal{E}=\left\{\varepsilon_{ij}\right\}_{i,j=1,\ldots,n}$ 分别表示具有 $n$ 个残基的顶点集和边集，而
    $\mathcal{P}=\{P_{i}\}_{i=1,\ldots,n}$ 是位置矩阵的集合，其中 $P_{i}\in\mathbb{R}^{k_{i}\times
    3}$ 表示节点 $v_{i}$ 的位置矩阵。我们将每个氨基酸视为蛋白质的图节点，那么 $k_{i}$ 取决于第 $i$ 个氨基酸中的原子数量。节点和边的特征矩阵分别是
    $X=[\bm{x}_{i}]_{i=1,\ldots,n}$ 和 $E=[\bm{e}_{ij}]_{i,j=1,\ldots,n}$，节点和边的特征向量分别是
    $\bm{x}_{i}\in\mathbb{R}^{d_{1}}$ 和 $\bm{e}_{ij}\in\mathbb{R}^{d_{2}}$，$d_{1}$
    和 $d_{2}$ 是初始特征维度。蛋白质图表示学习的目标是为每个蛋白质形成一组低维嵌入 $\bm{z}$，然后将其应用于各种下游任务。
- en: '![Refer to caption](img/64c19db01475cebf1623bacac98bbf31.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/64c19db01475cebf1623bacac98bbf31.png)'
- en: 'Figure 3: Four different levels of protein structures [[28](#bib.bib28), [29](#bib.bib29)].'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：四种不同水平的蛋白质结构 [[28](#bib.bib28), [29](#bib.bib29)]。
- en: 2.2 Notations and Terms
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 符号和术语
- en: '• Sequence/primary structure: The linear sequence of amino acids in a peptide
    or protein [[30](#bib.bib30)]. Any sequence of polypeptides is reported starting
    from the single amine (N-terminus) end to carboxylic acid (C-terminus) [[31](#bib.bib31)]
    (refer to Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Mathematical Definitions ‣ 2 Definitions,
    Notations, and Terms ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")). • Secondary structure (SS): The 3D form of local segments of proteins.
    The two most common secondary structural elements are $\alpha$-helix (H) and $\beta$-strand
    (E); 3-state SS includes H, E, C (coil region); 8 fine-grained states include
    three types for helix (G for $3_{10}$-helix, H for $\alpha$-helix, and I for $\pi$-helix),
    two types for strand (E for $\beta$-strand and B for $\beta$-bridge), and three
    types for coil (T for $\beta$-turn, S for high curvature loop, and L for irregular) [[32](#bib.bib32)].
    • Tertiary structure: The 3D arrangement of its polypeptide chains and their component
    atoms. • Quaternary structure: The 3D arrangement of the subunits in a multisubunit
    protein [[33](#bib.bib33)]. • Multiple sequence alignment (MSA): The result of
    the alignment of three or more biological sequences (protein or nucleic acid).
    • Sequence homology: The biological homology between sequences (proteins or nucleic
    acids) [[34](#bib.bib34)]. MSA assumes all the sequences to be aligned may share
    recognizable evolutionary homology [[35](#bib.bib35)] and is used to indicate
    which regions of each sequence are homologous. • Coevolution: The interdependence
    between the evolutionary changes of two entities [[36](#bib.bib36)] plays an important
    role at all biological levels, which is evident between protein residues (see
    Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Protein and Language ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")(a)).
    • Templates: The homologous 3D structures of proteins. • Contact map: A two-dimensional
    binary matrix represents the residue-residue contacts of a protein within a distance
    threshold [[37](#bib.bib37)]. • Protein structure prediction (PSP): The prediction
    of the 3D structure of a protein from its amino acid sequence. • Orphan proteins:
    Proteins without any detectable homology [[38](#bib.bib38)] (MSAs of homologous
    proteins are not available). • Antibody: A Y-shaped protein is produced by the
    immune system to detect and neutralize harmful substances, such as viruses and
    pathogenic bacteria. • Ribonucleic acid (RNA): A polymeric molecule essential
    in various biological roles, including transcription, translation, and gene regulation,
    most often single-stranded. • Protein complex: A form of quaternary structure
    associated with two or more polypeptide chains. • Protein conformation: The spatial
    arrangement of its constituent atoms that determines the overall shape [[39](#bib.bib39)].
    • Protein energy function: Proteins fold into 3D structures in a way that leads
    to a low-energy state. Protein-energy functions are used to guide PSP by minimizing
    the energy value. • Gene Ontology (GO): GO is a widely used bioinformatics resource
    that provides a standardized vocabulary to describe the functions, processes,
    and cellular locations of genes and gene products, organizing biological knowledge
    into three main domains: molecular function (MF), cellular component (CC), and
    biological process (BP). • Monte Carlo methods: A class of computational mathematical
    algorithms that use repeated random sampling to estimate the possible outcomes
    of an uncertain event. • Supervised learning: The use of labeled input-output
    pairs to learn a function that can classify data or predict outcomes accurately.
    • Unsupervised learning: Models are trained without a labeled dataset and encouraged
    to discover hidden patterns and insights from the given data. • Natural language
    processing (NLP): The ability of computer programs to process, analyze, and understand
    the text and spoken words in much the same way humans can. • Language model (LM):
    A LM is a statistical model that is trained to predict the probability of a sequence
    of words in a given language. • Embedding: An embedding is a low-dimensional,
    learned continuous vector representation of discrete variables into which you
    can translate high-dimensional and real-valued vectors (words or sentences) [[40](#bib.bib40)].
    • Convolution neural networks (CNNs): A class of neural networks that consist
    of convolutional operations to capture the local information. • Recurrent neural
    networks (RNNs): A class of neural networks where connections between nodes form
    a directed or undirected graph along a temporal sequence. • Attention models:
    A class of neural network architectures that are able to focus their computation
    on specific parts of their input or memory [[41](#bib.bib41)]. • Graph neural
    networks (GNNs): A type of deep learning model specifically designed to operate
    on graph-structured data. • Tansfer learning: A machine learning method where
    a model developed for one task is reused for a model to solve a different but
    related task [[42](#bib.bib42), [43](#bib.bib43)], which has two major activities,
    i.e., pre-training and fine-tuning. • Pre-training: A strategy in AI refers to
    training a model with one task to help it form parameters that can be used in
    other tasks. • Fine-tuning: A method that takes the weights of a pre-trained neural
    network, which are used to initialize a new model being trained on the same domain.
    • Autoregressive language model: A feed-forward model predicts the future word
    from a set of words given a context [[44](#bib.bib44)]. • Masked language model:
    A LM masks some of the words in a sentence and predicts which words should replace
    those masks. • Bidirectional language model: A LM learns to predict the probability
    of the next token in the past and future directions [[45](#bib.bib45)]. • Multi-task
    learning: A machine learning paradigm in which multiple tasks are solved simultaneously
    while exploiting commonalities and differences across tasks [[46](#bib.bib46)].
    • Sequence-to-Sequence (Seq2Seq): A family of machine learning approaches train
    models to convert sequences from one domain to sequences in another domain. •
    Knowledge distillation: The process of transferring the knowledge from a large
    model or set of models to a single smaller model [[47](#bib.bib47)]. • Multi-modal
    learning: Training models by combining information obtained from more than one
    modality [[48](#bib.bib48), [49](#bib.bib49)]. • Residual neural network: A neural
    network in which skip connections or shortcuts are used to jump over some layers,
    e.g., the deep residual network, ResNet [[50](#bib.bib50)].'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: • 序列/初级结构：肽链或蛋白质中氨基酸的线性序列[[30](#bib.bib30)]。任何多肽的序列都从单胺（N-末端）到羧酸（C-末端）[[31](#bib.bib31)]（参见图[3](#S2.F3
    "图 3 ‣ 2.1 数学定义 ‣ 2 定义、符号和术语 ‣ 深度学习在蛋白质科学中的进展：综合调查")）。 • 次级结构（SS）：蛋白质局部片段的3D形态。两种最常见的次级结构元素是$\alpha$-螺旋（H）和$\beta$-链（E）；3态SS包括H、E、C（卷曲区）；8种精细状态包括螺旋的三种类型（G表示$3_{10}$-螺旋，H表示$\alpha$-螺旋，I表示$\pi$-螺旋），链的两种类型（E表示$\beta$-链和B表示$\beta$-桥），以及卷曲的三种类型（T表示$\beta$-转角，S表示高曲率环，L表示不规则）[[32](#bib.bib32)]。
    • 第三级结构：其多肽链及其组成原子的3D排列。 • 四级结构：多亚单位蛋白质中亚单位的3D排列[[33](#bib.bib33)]。 • 多重序列比对（MSA）：三条或更多生物序列（蛋白质或核酸）比对的结果。
    • 序列同源性：序列（蛋白质或核酸）之间的生物学同源性[[34](#bib.bib34)]。MSA假设所有对齐的序列可能共享可识别的进化同源性[[35](#bib.bib35)]，用于指示每个序列中哪些区域是同源的。
    • 共进化：两个实体的进化变化之间的相互依赖[[36](#bib.bib36)]在所有生物学层面上都发挥着重要作用，这在蛋白质残基之间显而易见（见图[7](#S4.F7
    "图 7 ‣ 4.1 蛋白质和语言 ‣ 4 蛋白质基础 ‣ 深度学习在蛋白质科学中的进展：综合调查")(a)）。 • 模板：蛋白质的同源3D结构。 • 联系图：一个二维二进制矩阵，表示蛋白质在距离阈值内的残基-残基接触[[37](#bib.bib37)]。
    • 蛋白质结构预测（PSP）：从氨基酸序列预测蛋白质的3D结构。 • 孤儿蛋白质：没有任何可检测同源性的蛋白质[[38](#bib.bib38)]（同源蛋白质的MSA不可用）。
    • 抗体：由免疫系统产生的Y形蛋白质，用于检测和中和有害物质，如病毒和致病菌。 • 核糖核酸（RNA）：一种在转录、翻译和基因调控等各种生物学角色中必不可少的聚合物，通常是单链的。
    • 蛋白质复合体：与两个或更多多肽链相关的四级结构形式。 • 蛋白质构象：其组成原子的空间排列决定了整体形状[[39](#bib.bib39)]。 • 蛋白质能量函数：蛋白质折叠成3D结构的方式使其达到低能量状态。蛋白质-能量函数用于通过最小化能量值来指导PSP。
    • 基因本体（GO）：GO是一个广泛使用的生物信息学资源，提供标准化的词汇来描述基因和基因产物的功能、过程和细胞位置，将生物学知识组织为三个主要领域：分子功能（MF）、细胞组成（CC）和生物过程（BP）。
    • 蒙特卡罗方法：一种计算数学算法，通过重复随机采样来估计不确定事件的可能结果。 • 监督学习：利用标记的输入-输出对来学习一种能够准确分类数据或预测结果的函数。
    • 无监督学习：模型在没有标记数据集的情况下进行训练，并鼓励从给定数据中发现隐藏的模式和见解。 • 自然语言处理（NLP）：计算机程序处理、分析和理解文本和口头语言的能力，类似于人类的处理方式。
    • 语言模型（LM）：一种统计模型，用于预测给定语言中一系列单词的概率。 • 嵌入：嵌入是一种低维、学习的连续向量表示离散变量，你可以将高维和实值向量（单词或句子）转换为此表示[[40](#bib.bib40)]。
    • 卷积神经网络（CNNs）：一种神经网络类别，通过卷积操作捕捉局部信息。 • 循环神经网络（RNNs）：一种神经网络类别，其中节点之间的连接形成一个沿时间序列的有向或无向图。
    • 注意力模型：一种神经网络架构，能够将计算集中在输入或记忆的特定部分[[41](#bib.bib41)]。 • 图神经网络（GNNs）：一种深度学习模型，专门设计用于处理图结构数据。
    • 迁移学习：一种机器学习方法，其中为一个任务开发的模型被重用于解决不同但相关的任务[[42](#bib.bib42), [43](#bib.bib43)]，主要包括预训练和微调两个主要活动。
    • 预训练：一种AI策略，指使用一个任务训练模型，以帮助其形成可以用于其他任务的参数。 • 微调：一种方法，利用预训练神经网络的权重来初始化在相同领域中训练的新模型。
    • 自回归语言模型：一种前馈模型，根据上下文从一组词预测未来的词[[44](#bib.bib44)]。 • 掩码语言模型：一种语言模型掩盖句子中的一些单词，并预测应该用哪些单词来替代这些掩盖词。
    • 双向语言模型：一种语言模型，学习预测过去和未来方向上的下一个标记的概率[[45](#bib.bib45)]。 • 多任务学习：一种机器学习范式，同时解决多个任务，同时利用任务之间的共性和差异[[46](#bib.bib46)]。
    • 序列到序列（Seq2Seq）：一种机器学习方法，将一个领域的序列转换为另一个领域的序列。 • 知识蒸馏：将知识从大型模型或模型集合转移到单一小型模型的过程[[47](#bib.bib47)]。
    • 多模态学习：通过结合来自多种模态的信息来训练模型[[48](#bib.bib48), [49](#bib.bib49)]。 • 残差神经网络：一种神经网络，使用跳过连接或快捷方式跳过一些层，例如深度残差网络ResNet[[50](#bib.bib50)]。
- en: 3 Basic Neural Networks in Protein Modeling
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 蛋白建模中的基础神经网络
- en: 'This section initiates by introducing fundamental deep learning architectures,
    delineated into four primary categories: CNNs, RNNs, attention mechanisms, and
    GNNs. Notably, attention models like transformers [[51](#bib.bib51)] and message
    passing mechanisms receive particular emphasis. Subsequently, two frequently employed
    LMs, Bidirectional encoder representations from transformers (BERT) [[52](#bib.bib52)]
    and Generative pre-trained transformer (GPT) [[53](#bib.bib53), [54](#bib.bib54),
    [55](#bib.bib55)], are outlined. These foundational neural networks typically
    serve as building blocks for constructing intricate models in the realm of protein
    research.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先介绍了基本的深度学习架构，分为四个主要类别：CNNs, RNNs, attention mechanisms, 和 GNNs。特别强调了像transformers
    [[51](#bib.bib51)] 和message passing mechanisms 这样的attention模型。随后，概述了两种经常使用的LMs，Bidirectional
    encoder representations from transformers (BERT) [[52](#bib.bib52)] 和 Generative
    pre-trained transformer (GPT) [[53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55)]。这些基础神经网络通常被用来构建蛋白质研究领域中的复杂模型的构件。
- en: 3.1 Convolution Neural Networks
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 卷积神经网络
- en: CNNs are a type of deep learning algorithm that has revolutionized the field
    of computer vision [[56](#bib.bib56)]. Inspired by the visual cortex of the human
    brain [[57](#bib.bib57)], CNNs are particularly effective at analyzing and extracting
    features from images and other grid-like data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs是一种深度学习算法，彻底改变了计算机视觉领域 [[56](#bib.bib56)]。受到人脑视觉皮层的启发 [[57](#bib.bib57)]，CNNs特别擅长分析和提取图像和其他类似网格的数据中的特征。
- en: 'The key idea behind CNNs is the use of convolutional layers, which apply filters
    or kernels to input data to extract local patterns. These filters are small matrices
    that slide over the input data, performing element-wise multiplications and summations
    to produce feature maps [[58](#bib.bib58)]. Convolution is a mathematical operation
    that involves the integration of the product of two functions, where one function
    is reversed and shifted. In the context of deep learning, the convolution of two
    functions, denoted as $f$ and $g$, can be expressed as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs背后的关键思想是使用卷积层，它们对输入数据应用滤波器或核以提取局部模式。这些滤波器是小矩阵，它们在输入数据上滑动，进行逐元素乘法和求和以产生特征图
    [[58](#bib.bib58)]。卷积是一个数学运算，涉及两个函数的乘积的积分，其中一个函数是反转并移位的。在深度学习的背景下，两个函数的卷积可以表示为$f$和$g$，如下所示：
- en: '|  | $\text{ Convolution: }(f*g)(t)=\int_{\tau\in\Omega}g(\tau)f(t+\tau)d\tau\text{,
    }$ |  | (1) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{ 卷积: }(f*g)(t)=\int_{\tau\in\Omega}g(\tau)f(t+\tau)d\tau\text{,
    }$ |  | (1) |'
- en: here, $\Omega$ represents a neighborhood in a given space. In deep learning
    applications, $f(t)$ typically represents the feature at position $t$, denoted
    as $f(t)=f_{t}\in\mathbb{R}^{d_{1}\times 1}$, where $d_{1}$ refers to the number
    of input feature channels. On the other hand, $g(\tau)\in\mathbb{R}^{d\times d_{1}}$
    is commonly implemented as a parametric kernel function, with $d$ representing
    the number of output feature channels [[59](#bib.bib59)].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\Omega$表示给定空间中的一个邻域。在深度学习应用中，$f(t)$通常表示位置$t$处的特征，表示为$f(t)=f_{t}\in\mathbb{R}^{d_{1}\times
    1}$，其中$d_{1}$是输入特征通道的数量。另一方面，$g(\tau)\in\mathbb{R}^{d\times d_{1}}$通常被实现为参数化的核函数，其中$d$表示输出特征通道的数量
    [[59](#bib.bib59)]。
- en: 'By stacking multiple convolutional layers, CNNs can learn increasingly complex
    and abstract features from the input data. In addition to convolutional layers,
    CNNs typically include pooling layers, which reduce the spatial dimensions of
    the feature maps while preserving the most important information. Pooling helps
    to make the network more robust to variations in the input data and reduces the
    computational complexity. CNNs also incorporate fully connected layers at the
    end of the network, which perform classification or regression tasks based on
    the extracted features. Figure [4](#S3.F4 "Figure 4 ‣ 3.1 Convolution Neural Networks
    ‣ 3 Basic Neural Networks in Protein Modeling ‣ Advances of Deep Learning in Protein
    Science: A Comprehensive Survey") shows a framework of deep CNNs. The versatility
    and effectiveness of CNNs have made them a fundamental tool in the field of deep
    learning and have contributed to significant advancements in various domains.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过堆叠多个卷积层，CNN可以从输入数据中学习越来越复杂和抽象的特征。除了卷积层外，CNN通常还包括池化层，这些层减少特征图的空间维度，同时保留最重要的信息。池化有助于使网络对输入数据的变化更加稳健，并减少计算复杂性。CNN还在网络的末端包含全连接层，这些层基于提取的特征执行分类或回归任务。图[4](#S3.F4
    "图4 ‣ 3.1 卷积神经网络 ‣ 3 蛋白质建模中的基本神经网络 ‣ 深度学习在蛋白质科学中的进展：全面调查")展示了深度CNN的框架。CNN的多功能性和有效性使其成为深度学习领域的基本工具，并在各个领域的重大进展中做出了贡献。
- en: '![Refer to caption](img/5a9a32368a7ecdc73c64a1bd49bb1d51.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5a9a32368a7ecdc73c64a1bd49bb1d51.png)'
- en: 'Figure 4: An illustration of CNN.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：CNN的插图。
- en: 3.2 Recurrent Neural Networks and Long Short-term Memory
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 循环神经网络与长短期记忆
- en: The first example of LM was studied by Andrey Markov, who proposed the Markov
    chain in 1913 [[60](#bib.bib60), [61](#bib.bib61)]. After that, some machine learning
    methods, particularly hidden Markov models and their variants, have been described
    and applied as fundamental tools in many fields, including biological sequences [[62](#bib.bib62)].
    The goal is to recover a data sequence that is not immediately observable [[63](#bib.bib63),
    [64](#bib.bib64), [65](#bib.bib65)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的第一个例子由安德烈·马尔可夫研究，他在1913年提出了马尔可夫链[[60](#bib.bib60), [61](#bib.bib61)]。此后，一些机器学习方法，特别是隐马尔可夫模型及其变体，已被描述并应用为许多领域的基本工具，包括生物序列[[62](#bib.bib62)]。目标是恢复一个不立即可观察到的数据序列[[63](#bib.bib63),
    [64](#bib.bib64), [65](#bib.bib65)]。
- en: Since the 2010s, neural networks have started to produce superior results in
    various NLP tasks [[66](#bib.bib66)]. RNNs allow previous outputs to be used as
    inputs while having hidden states to exhibit temporal dynamic behaviors. Therefore,
    RNNs can use their internal states to process variable-length sequences of inputs,
    which are useful and applicable in NLP tasks [[67](#bib.bib67)]. In a recent development,
    Google DeepMind introduced Hawk, an RNN featuring gated linear recurrences, alongside
    Griffin, a hybrid model blending gated linear recurrences with local attention
    mechanisms [[68](#bib.bib68)], which match the hardware efficiency of transformers [[51](#bib.bib51)]
    during training.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 自2010年代以来，神经网络在各种自然语言处理任务中开始产生卓越的结果[[66](#bib.bib66)]。RNN允许将先前的输出作为输入，同时具有隐藏状态以展示时间动态行为。因此，RNN可以利用其内部状态处理可变长度的输入序列，这在自然语言处理任务中非常有用和适用[[67](#bib.bib67)]。最近，谷歌DeepMind推出了Hawk，一种具有门控线性递归的RNN，以及Griffin，一种将门控线性递归与局部注意力机制混合的混合模型[[68](#bib.bib68)]，这些模型在训练过程中具有与变压器相匹配的硬件效率[[51](#bib.bib51)]。
- en: '![Refer to caption](img/231c7d6d91285f3e2423d24216c5e0a5.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/231c7d6d91285f3e2423d24216c5e0a5.png)'
- en: (a) RNNs
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: (a) RNNs
- en: '![Refer to caption](img/76ae6d8097e0de81bcc78974164967e4.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/76ae6d8097e0de81bcc78974164967e4.png)'
- en: (b) LSTM cell
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LSTM单元
- en: 'Figure 5: Graphical explanation of RNNs and LSTM.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：RNN和LSTM的图解说明。
- en: 'RNNs are typically shown in Figure [5a](#S3.F5.sf1 "In Figure 5 ‣ 3.2 Recurrent
    Neural Networks and Long Short-term Memory ‣ 3 Basic Neural Networks in Protein
    Modeling ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey").
    In each timestep $t$, the input $x_{t}\in\mathbb{R}^{d_{1}}$, hidden $h_{t}\in\mathbb{R}^{d}$
    and output state vectors $o_{t}\in\mathbb{R}^{d}$, where the superscripts $d_{1}$
    and $d$ refer to the number of input features and the number of hidden units,
    respectively, are formulated as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 通常如图 [5a](#S3.F5.sf1 "在图 5 ‣ 3.2 循环神经网络与长短期记忆 ‣ 3 蛋白质建模中的基础神经网络 ‣ 蛋白质科学深度学习的进展：全面调查")所示。在每个时间步
    $t$，输入 $x_{t}\in\mathbb{R}^{d_{1}}$、隐藏状态 $h_{t}\in\mathbb{R}^{d}$ 和输出状态向量 $o_{t}\in\mathbb{R}^{d}$，其中上标
    $d_{1}$ 和 $d$ 分别表示输入特征的数量和隐藏单元的数量，按如下公式构造：
- en: '|  | $\displaystyle h_{t}$ | $\displaystyle=g\left(W_{x}x_{t}+W_{h}h_{t-1}+b_{h}\right)$
    |  | (2) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h_{t}$ | $\displaystyle=g\left(W_{x}x_{t}+W_{h}h_{t-1}+b_{h}\right)$
    |  | (2) |'
- en: '|  | $\displaystyle o_{t}$ | $\displaystyle=g\left(W_{y}h_{t}+b_{y}\right)$
    |  | (3) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle o_{t}$ | $\displaystyle=g\left(W_{y}h_{t}+b_{y}\right)$
    |  | (3) |'
- en: where $W_{x}\in\mathbb{R}^{d\times d_{1}}$, $W_{h}\in\mathbb{R}^{d\times d}$
    and $W_{y}\in\mathbb{R}^{d\times d}$ are the weights associated with the input,
    hidden and output vectors in the recurrent layer, and $b_{h}\in\mathbb{R}^{d}$,
    $b_{y}\in\mathbb{R}^{d}$ are the bias, which are shared temporally, $g(\cdot)$
    is the activation function.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{x}\in\mathbb{R}^{d\times d_{1}}$、$W_{h}\in\mathbb{R}^{d\times d}$ 和 $W_{y}\in\mathbb{R}^{d\times
    d}$ 是与递归层中输入、隐藏和输出向量相关的权重，$b_{h}\in\mathbb{R}^{d}$、$b_{y}\in\mathbb{R}^{d}$ 是共享的偏置，$g(\cdot)$
    是激活函数。
- en: In order to deal with the vanishing gradient problem [[69](#bib.bib69)] that
    can be encountered when training traditional RNNs, LSTM networks are developed
    to process sequences of data. They present superior capabilities in learning long-term
    dependencies [[70](#bib.bib70)] with various applications such as time series
    prediction [[71](#bib.bib71)], protein homology detection [[72](#bib.bib72)],
    drug design [[73](#bib.bib73)], etc. Unlike standard LSTM, bidirectional LSTM
    (BiLSTM) adds one more LSTM layer, reversing the information flow direction. This
    means it is capable of utilizing information from both sides and is also a powerful
    tool for modeling the sequential dependencies between words and phrases in a sequence [[74](#bib.bib74)].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决训练传统 RNN 时可能遇到的梯度消失问题 [[69](#bib.bib69)]，开发了 LSTM 网络来处理数据序列。它们在学习长期依赖性方面表现出卓越的能力 [[70](#bib.bib70)]，并有多种应用，如时间序列预测 [[71](#bib.bib71)]、蛋白质同源性检测 [[72](#bib.bib72)]、药物设计 [[73](#bib.bib73)]
    等。与标准 LSTM 不同，双向 LSTM（BiLSTM）添加了另一层 LSTM，反转信息流的方向。这意味着它能够利用来自两侧的信息，并且也是建模序列中单词和短语之间顺序依赖关系的强大工具 [[74](#bib.bib74)]。
- en: 'The LSTM architecture aims to provide a short-term memory that can last more
    timesteps, shown in Figure [5b](#S3.F5.sf2 "In Figure 5 ‣ 3.2 Recurrent Neural
    Networks and Long Short-term Memory ‣ 3 Basic Neural Networks in Protein Modeling
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey"), $\mathrm{Sigmoid}(\cdot)$
    and $\mathrm{tanh}(\cdot)$ represent the sigmoid and tanh layer. Forget gate layer
    in the LSTM is to decide what information is going to be thrown away from the
    cell state at timestep $t$, $x_{t}\in\mathbb{R}^{d_{1}}$, $h_{t}\in(-1,1)^{d}$
    and $f_{t}\in(0,1)^{d}$ are the input, hidden state vectors and forget gate’s
    activation vector.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 架构旨在提供一个能够持续更多时间步的短期记忆，如图 [5b](#S3.F5.sf2 "在图 5 ‣ 3.2 循环神经网络与长短期记忆 ‣ 3
    蛋白质建模中的基础神经网络 ‣ 蛋白质科学深度学习的进展：全面调查")所示，$\mathrm{Sigmoid}(\cdot)$ 和 $\mathrm{tanh}(\cdot)$
    分别表示 sigmoid 层和 tanh 层。LSTM 中的遗忘门层决定了在时间步 $t$ 时要从单元状态中丢弃哪些信息，$x_{t}\in\mathbb{R}^{d_{1}}$、$h_{t}\in(-1,1)^{d}$
    和 $f_{t}\in(0,1)^{d}$ 是输入、隐藏状态向量和遗忘门的激活向量。
- en: '|  | $f_{t}=\mathrm{Sigmoid}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})$ |  | (4) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{t}=\mathrm{Sigmoid}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})$ |  | (4) |'
- en: Then, the input gate layer decides which values should be updated, and a tanh
    layer creates a vector of new candidate values, $\tilde{C}_{t}\in(-1,1)^{d}$ that
    could be added to the state, $i_{t}\in(0,1)^{d}$ is the input gate’s activation
    vector.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，输入门层决定哪些值应该被更新，tanh 层创建一个新的候选值向量，$\tilde{C}_{t}\in(-1,1)^{d}$，可以被添加到状态中，$i_{t}\in(0,1)^{d}$
    是输入门的激活向量。
- en: '|  | $\displaystyle i_{t}$ | $\displaystyle=\mathrm{Sigmoid}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})$
    |  | (5) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle i_{t}$ | $\displaystyle=\mathrm{Sigmoid}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})$
    |  | (5) |'
- en: '|  | $\displaystyle\tilde{C}_{t}$ | $\displaystyle=\tanh(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})$
    |  | (6) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{C}_{t}$ | $\displaystyle=\tanh(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})$
    |  | (6) |'
- en: Next, we combine old state $C_{t-1}\in\mathbb{R}^{d}$ and new candidate values
    $\tilde{C}_{t}\in(-1,1)^{d}$ to create an update to the new state $C_{t}\in\mathbb{R}^{d}$.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将旧状态 $C_{t-1}\in\mathbb{R}^{d}$ 和新候选值 $\tilde{C}_{t}\in(-1,1)^{d}$ 结合起来，以创建对新状态
    $C_{t}\in\mathbb{R}^{d}$ 的更新。
- en: '|  | $C_{t}=f_{t}\odot C_{t-1}+i_{t}\odot\tilde{C}_{t}$ |  | (7) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $C_{t}=f_{t}\odot C_{t-1}+i_{t}\odot\tilde{C}_{t}$ |  | (7) |'
- en: Finally, the output gate layer decides what parts of the cell state to be outputted,
    $o_{t}\in(0,1)^{d}$.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，输出门层决定要输出细胞状态的哪些部分，$o_{t}\in(0,1)^{d}$。
- en: '|  | $\displaystyle o_{t}$ | $\displaystyle=\mathrm{Sigmoid}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})$
    |  | (8) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle o_{t}$ | $\displaystyle=\mathrm{Sigmoid}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})$
    |  | (8) |'
- en: '|  | $\displaystyle h_{t}$ | $\displaystyle=o_{t}\odot\tanh\left(C_{t}\right)$
    |  | (9) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h_{t}$ | $\displaystyle=o_{t}\odot\tanh\left(C_{t}\right)$
    |  | (9) |'
- en: where $\{W_{f},W_{i},W_{c},W_{o}\}\in\mathbb{R}^{d\times d_{1}},\{U_{f},U_{i},U_{c},U_{o}\}\in\mathbb{R}^{d\times
    d}$ and $\{b_{f},b_{i},b_{c},b_{o}\}\in\mathbb{R}^{d}$ are weight matrices and
    bias vector parameters in the LSTM cell, $\odot$ means the pointwise multiplication.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\{W_{f},W_{i},W_{c},W_{o}\}\in\mathbb{R}^{d\times d_{1}},\{U_{f},U_{i},U_{c},U_{o}\}\in\mathbb{R}^{d\times
    d}$ 和 $\{b_{f},b_{i},b_{c},b_{o}\}\in\mathbb{R}^{d}$ 是 LSTM 单元中的权重矩阵和偏置向量参数，$\odot$
    表示逐点乘法。
- en: 3.3 Attention Mechanism and Transformer
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 注意力机制与 Transformer
- en: 'Traditional Sequence-to-Sequence (Seq2Seq) models typically use RNNs or LSTMs
    as encoders and decoders [[75](#bib.bib75)] to process sequences and extract features
    for various tasks. However, these models have limitations, such as the final state
    of the RNNs or LSTMs needing to hold information for the entire input sequence,
    which can lead to information loss. To overcome these limitations, attention mechanisms [[76](#bib.bib76),
    [77](#bib.bib77)] have been introduced, which can be divided into two categories,
    local attention, and global attention (refer to Figure [6a](#S3.F6.sf1 "In Figure
    6 ‣ 3.3 Attention Mechanism and Transformer ‣ 3 Basic Neural Networks in Protein
    Modeling ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")).
    They allow models to focus on specific parts of the input sequence that are relevant
    to the task at hand. The basic idea behind attention is to assign weights to different
    elements of the input sequence based on their relevance to the current step of
    the output sequence generation. Attention mechanisms are first applied in machine
    translation [[76](#bib.bib76)] and have gradually replaced traditional RNNs and
    LSTMs.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的序列到序列（Seq2Seq）模型通常使用 RNNs 或 LSTMs 作为编码器和解码器 [[75](#bib.bib75)] 来处理序列并提取特征用于各种任务。然而，这些模型存在一些限制，比如
    RNNs 或 LSTMs 的最终状态需要保持整个输入序列的信息，这可能导致信息丢失。为了克服这些限制，引入了注意力机制 [[76](#bib.bib76),
    [77](#bib.bib77)]，这些机制可以分为两类，本地注意力和全局注意力（参见图 [6a](#S3.F6.sf1 "在图 6 ‣ 3.3 注意力机制与
    Transformer ‣ 3 基本神经网络在蛋白质建模中的应用 ‣ 深度学习在蛋白质科学中的进展：全面调查")）。它们允许模型专注于与当前任务相关的输入序列的特定部分。注意力机制的基本思想是根据输入序列的不同元素与当前输出序列生成步骤的相关性分配权重。注意力机制最初应用于机器翻译 [[76](#bib.bib76)]
    并逐渐取代了传统的 RNNs 和 LSTMs。
- en: '![Refer to caption](img/f67b0e311757b80e0a78adffb6e9870d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f67b0e311757b80e0a78adffb6e9870d.png)'
- en: (a) Attention mechanism
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注意力机制
- en: '![Refer to caption](img/e82d6a3d3451e51879c3b01bae19b843.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e82d6a3d3451e51879c3b01bae19b843.png)'
- en: (b) The Transformer
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Transformer
- en: 'Figure 6: Graphical explanation of attention mechanism and the architecture
    of Transformer.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：注意力机制和 Transformer 架构的图示解释。
- en: 'The attention layer in a model can access all previous states and learn their
    importance by assigning weights to them. In 2017, Google Brain introduced the
    Transformer architecture [[51](#bib.bib51)], which completely eliminates recurrence
    and convolutions. This breakthrough leads to the development of pre-trained models
    such as BERT and GPT, which are trained on large language datasets. Unlike RNNs,
    as shown in Figure [6b](#S3.F6.sf2 "In Figure 6 ‣ 3.3 Attention Mechanism and
    Transformer ‣ 3 Basic Neural Networks in Protein Modeling ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey"), the Transformer processes the entire
    input simultaneously using $N$ stacked self-attention layers for both the encoder
    and decoder. Each layer consists of a multi-head attention module followed by
    a feed-forward module with a residual connection and normalization. The basic
    attention mechanism used in Transformer is called ”Scaled Dot-Product Attention”
    and operates as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中的注意力层可以访问所有先前的状态，并通过为这些状态分配权重来学习它们的重要性。2017年，Google Brain 引入了 Transformer
    架构 [[51](#bib.bib51)]，该架构完全消除了递归和卷积。这一突破导致了诸如 BERT 和 GPT 的预训练模型的开发，这些模型在大规模语言数据集上进行训练。与
    RNNs 不同，如图 [6b](#S3.F6.sf2 "图 6 ‣ 3.3 注意力机制与 Transformer ‣ 3 蛋白质建模中的基本神经网络 ‣ 深度学习在蛋白质科学中的进展：全面调查")
    所示，Transformer 使用 $N$ 层堆叠的自注意力层同时处理整个输入，既用于编码器也用于解码器。每一层由一个多头注意力模块组成，后跟一个具有残差连接和归一化的前馈模块。Transformer
    中使用的基本注意力机制称为“缩放点积注意力”，其操作如下：
- en: '|  | $\operatorname{Att}(Q,K,V)=\operatorname{Softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right)V$
    |  | (10) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{Att}(Q,K,V)=\operatorname{Softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right)V$
    |  | (10) |'
- en: 'where $Q,K,V\in\mathbb{R}^{l\times d}$ are $d$-dimensional vector representations
    of $l$ words in sequences of queries, keys and values, respectively. The multi-head
    attention mechanism allows the model to attend to different representation subspaces
    in parallel. The multi-head attention module can be defined as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q,K,V\in\mathbb{R}^{l\times d}$ 是查询、键和值序列中 $l$ 个单词的 $d$ 维向量表示。多头注意力机制使模型能够并行关注不同的表示子空间。多头注意力模块可以定义如下：
- en: '|  | $\displaystyle\operatorname{MultiHead}\left(Q,K,V\right)$ | $\displaystyle=\mathrm{Concat}\left(\mathrm{head}_{1},\ldots,\mathrm{head}_{h}\right)W^{O}$
    |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname{MultiHead}\left(Q,K,V\right)$ | $\displaystyle=\mathrm{Concat}\left(\mathrm{head}_{1},\ldots,\mathrm{head}_{h}\right)W^{O}$
    |  |'
- en: '|  | $\displaystyle\mathrm{where}\ \mathrm{head}_{i}$ | $\displaystyle=\mathrm{Att}\left(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}\right)$
    |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{where}\ \mathrm{head}_{i}$ | $\displaystyle=\mathrm{Att}\left(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}\right)$
    |  |'
- en: where the projections are parameter matrices $W_{i}^{Q}\in\mathbb{R}^{d\times
    d_{i}},W_{i}^{K}\in\mathbb{R}^{d\times d_{i}},W_{i}^{V}\in\mathbb{R}^{d\times
    d_{i}}$ and $W^{O}\in\mathbb{R}^{hd_{i}\times d},d_{i}=d/h$, there are $h$ parallel
    attention layers or heads. Additionally, the Transformer architecture includes
    position-wise feed-forward networks, which consist of two linear transformations
    with a ReLU activation in between. Positional encoding is also added to the embedding
    at the bottom of the encoder and decoder stacks to incorporate the order of the
    sequence.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，投影是参数矩阵 $W_{i}^{Q}\in\mathbb{R}^{d\times d_{i}},W_{i}^{K}\in\mathbb{R}^{d\times
    d_{i}},W_{i}^{V}\in\mathbb{R}^{d\times d_{i}}$ 和 $W^{O}\in\mathbb{R}^{hd_{i}\times
    d},d_{i}=d/h$，有 $h$ 个并行的注意力层或头。此外，Transformer 架构还包括位置-wise 前馈网络，这些网络由两个线性变换和一个中间的
    ReLU 激活组成。位置编码也被添加到编码器和解码器堆栈底部的嵌入中，以纳入序列的顺序。
- en: 3.4 Graph Neural Networks
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 图神经网络
- en: Unlike traditional neural networks, which operate on grid-like data structures
    such as images or sequences, GNNs are specifically designed to handle data represented
    as graphs. In a graph, data entities are represented as nodes, and the relationships
    between these entities are captured by edges. This flexible and expressive representation
    makes GNNs well-suited for a wide range of applications, including social network
    analysis [[78](#bib.bib78)], recommendation systems [[79](#bib.bib79)], drug discovery [[80](#bib.bib80)],
    and knowledge graph reasoning [[81](#bib.bib81)].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 与操作在网格状数据结构（如图像或序列）上的传统神经网络不同，GNNs 专门设计用于处理表示为图的数据。在图中，数据实体被表示为节点，而这些实体之间的关系通过边来捕捉。这种灵活而富有表现力的表示使
    GNNs 非常适合广泛的应用，包括社交网络分析 [[78](#bib.bib78)]、推荐系统 [[79](#bib.bib79)]、药物发现 [[80](#bib.bib80)]
    和知识图谱推理 [[81](#bib.bib81)]。
- en: 'The key idea behind GNNs is to learn node representations by aggregating information
    from their neighboring nodes. This is achieved through a series of message passing
    steps, where each node updates its representation by incorporating information
    from its neighbors. By iteratively propagating and updating information across
    the graph, GNNs can capture complex dependencies and patterns in the data. Given
    a protein 3D graph as $G=(\mathcal{V},\mathcal{E},X,E)$ as presented in Subsection [2.1](#S2.SS1
    "2.1 Mathematical Definitions ‣ 2 Definitions, Notations, and Terms ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey"), a message passing
    layer can be expressed as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 'GNN 的关键思想是通过从邻近节点聚合信息来学习节点表示。这通过一系列消息传递步骤来实现，其中每个节点通过结合来自其邻居的信息来更新其表示。通过在图中迭代地传播和更新信息，GNN
    能够捕捉数据中的复杂依赖关系和模式。给定一个如 Subsection [2.1](#S2.SS1 "2.1 Mathematical Definitions
    ‣ 2 Definitions, Notations, and Terms ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey") 中所示的蛋白质 3D 图 $G=(\mathcal{V},\mathcal{E},X,E)$，一个消息传递层可以表示为：'
- en: '|  | $\bm{h}_{i}=\phi\left(\bm{x}_{i},\bigoplus_{v_{j}\in\mathcal{N}(v_{i})}\psi\left(\bm{x}_{i},\bm{x}_{j},\bm{e}_{ij}\right)\right)$
    |  | (11) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{h}_{i}=\phi\left(\bm{x}_{i},\bigoplus_{v_{j}\in\mathcal{N}(v_{i})}\psi\left(\bm{x}_{i},\bm{x}_{j},\bm{e}_{ij}\right)\right)$
    |  | (11) |'
- en: where $\bigoplus$ is a permutation invariant aggregation operator (e.g., element-wise
    sum), $\phi(\cdot)$ and $\psi(\cdot)$ are denoted as update and message functions,
    and $\mathcal{N}(v_{i})$ means the neighbors of node $v_{i}$.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bigoplus$ 是一个排列不变的聚合操作符（例如，逐元素求和），$\phi(\cdot)$ 和 $\psi(\cdot)$ 分别表示更新函数和消息函数，而
    $\mathcal{N}(v_{i})$ 表示节点 $v_{i}$ 的邻居。
- en: The variation of GNN models, including GCN [[82](#bib.bib82)], GAT [[83](#bib.bib83)],
    and GraphSAGE [[84](#bib.bib84)], differ in aggregation strategies, attention
    mechanisms, and propagation rules, but they all share the fundamental idea of
    learning node representations through the message passing mechanism. GNNs have
    the ability to capture both local and global information, providing a powerful
    framework for understanding graph-structured data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: GNN 模型的变体，包括 **GCN** [[82](#bib.bib82)], **GAT** [[83](#bib.bib83)] 和 **GraphSAGE**
    [[84](#bib.bib84)]，在聚合策略、注意机制和传播规则上有所不同，但它们都共享通过消息传递机制学习节点表示的基本思想。GNN 具有捕捉局部和全局信息的能力，为理解图结构数据提供了一个强大的框架。
- en: 3.5 Language Models
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 语言模型
- en: 'In order to effectively train deep neural models that can store knowledge for
    specific tasks using limited human-annotated data, the approach of transfer learning
    has been widely adopted. This involves a two-step process: pre-training and fine-tuning [[77](#bib.bib77),
    [85](#bib.bib85), [43](#bib.bib43)].'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效训练可以利用有限的人类标注数据存储特定任务知识的深度神经模型，迁移学习的方法被广泛采用。这涉及到一个两步过程：预训练和微调 [[77](#bib.bib77),
    [85](#bib.bib85), [43](#bib.bib43)]。
- en: Over the past few years, there has been remarkable progress in the development
    of pre-trained LMs, which have found extensive applications in various domains
    such as NLP and computer vision. Among these models, the transformer architecture
    has emerged as a standard neural architecture for both natural language understanding
    and generation. Notably, BERT and GPT are two landmark models that have opened
    the doors to large-scale pre-training LMs. GPT [[53](#bib.bib53)] is designed
    to optimize autoregressive language modeling during pre-training. It utilizes
    a transformer to model the conditional probability of each word, making it proficient
    in predicting the next token in a sequence. On the other hand, BERT [[52](#bib.bib52)]
    employs a multi-layer bidirectional transformer encoder as its architecture. During
    the pre-training phase, BERT utilizes next-sentence prediction and masked language
    modeling strategies to understand sentence relationships and capture contextual
    information.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，预训练语言模型（LMs）的发展取得了显著进展，这些模型在自然语言处理（NLP）和计算机视觉等多个领域得到了广泛应用。在这些模型中，变压器架构已经成为自然语言理解和生成的标准神经架构。特别是，**BERT**和**GPT**是两个具有里程碑意义的模型，为大规模预训练语言模型开辟了新天地。**GPT**
    [[53](#bib.bib53)] 旨在优化预训练过程中的自回归语言建模。它利用变压器来建模每个单词的条件概率，使其在预测序列中的下一个标记方面表现出色。另一方面，**BERT**
    [[52](#bib.bib52)] 采用多层双向变压器编码器作为其架构。在预训练阶段，BERT 利用下一个句子预测和掩蔽语言建模策略来理解句子关系并捕获上下文信息。
- en: Following the introduction of GPT and BERT, numerous improvements and variants
    have been proposed by researchers. One notable trend has been the increase in
    model size and dataset size [[86](#bib.bib86), [87](#bib.bib87)]. Large transformer
    models have become the de facto standard in NLP, driven by scaling laws that govern
    the relationship between overfitting, model size, and dataset size within a given
    computational budget [[88](#bib.bib88), [89](#bib.bib89)]. In November 2022, OpenAI
    released ChatGPT [[90](#bib.bib90)], which garnered significant attention for
    its ability to understand human language, answer questions, write code, and even
    generate novels. It stands as one of the most successful applications of large-scale
    pre-trained LMs. Moreover, as pre-trained LMs have demonstrated their effectiveness
    and efficiency in various domains, they have gradually expanded beyond NLP into
    fields such as finance, computer vision, and biomedicine [[91](#bib.bib91), [92](#bib.bib92),
    [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随着GPT和BERT的引入，研究人员提出了众多改进和变体。其中一个显著的趋势是模型规模和数据集规模的增加[[86](#bib.bib86), [87](#bib.bib87)]。大型变压器模型已经成为自然语言处理（NLP）的事实标准，由于缩放法则的驱动，控制过拟合、模型规模和数据集规模之间的关系[[88](#bib.bib88),
    [89](#bib.bib89)]。2022年11月，OpenAI发布了ChatGPT[[90](#bib.bib90)]，因其理解人类语言、回答问题、编写代码甚至生成小说的能力而引起了广泛关注。它是大型预训练语言模型最成功的应用之一。此外，随着预训练语言模型在各个领域表现出其有效性和效率，它们逐渐扩展到金融、计算机视觉和生物医学等领域[[91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)]。
- en: 4 Protein Foundations
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 蛋白质基础
- en: This section delves into the fundamental aspects of proteins, exploring their
    intricate connections with human languages, physicochemical properties, structural
    geometries, and biological insights. The discussion aims to unveil the foundational
    elements that contribute to a comprehensive understanding of proteins and their
    roles in various contexts.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本节*深入探讨*了蛋白质的基本方面，探索它们与人类语言、物理化学属性、结构几何以及生物学见解的复杂联系。讨论旨在揭示那些有助于全面理解蛋白质及其在各种背景下作用的**基础元素**。
- en: 4.1 Protein and Language
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 蛋白质与语言
- en: 'LMs are increasingly being utilized in the analysis of large-scale protein
    sequence databases to acquire embeddings [[96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98)]. One significant reason for this trend is the shared characteristics
    between human languages and proteins. For instance, both exhibit a hierarchical
    organization [[66](#bib.bib66), [99](#bib.bib99)], where the four distinct levels
    of protein structures (as depicted in Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Mathematical
    Definitions ‣ 2 Definitions, Notations, and Terms ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey")) can be analogized to letters, words,
    sentences, and texts in human languages. This analogy illustrates that proteins
    and languages consist of modular elements that can be reused and rearranged. Additionally,
    principles governing protein folding, such as the hydrophilicity and hydrophobicity
    of amino acids, the principle of minimal frustration [[100](#bib.bib100)], and
    the folding funnel landscapes of proteins [[101](#bib.bib101)], bear a resemblance
    to language grammars in linguistics.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '语言模型（LMs）越来越多地被用于分析大规模蛋白质序列数据库，以获取嵌入[[96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98)]。这一趋势的一个重要原因是人类语言和蛋白质之间的共同特征。例如，两者都表现出层次化的组织[[66](#bib.bib66),
    [99](#bib.bib99)]，其中蛋白质结构的四个不同层次（如图[3](#S2.F3 "Figure 3 ‣ 2.1 Mathematical Definitions
    ‣ 2 Definitions, Notations, and Terms ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey")所示）可以类比为人类语言中的字母、单词、句子和文本。这种类比说明了蛋白质和语言由可以重用和重新排列的模块化元素组成。此外，蛋白质折叠的原理，如氨基酸的亲水性和疏水性、最小挫折原则[[100](#bib.bib100)]，以及蛋白质的折叠漏斗景观[[101](#bib.bib101)]，与语言学中的语言语法有一定的相似性。'
- en: '![Refer to caption](img/37d4bd2cac294433265957ebba7c8005.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/37d4bd2cac294433265957ebba7c8005.png)'
- en: 'Figure 7: Comparisons of protein and language. (a) Relationship between a MSA
    and the residue contact of one protein in the alignment. The positions that coevolved
    are highlighted in red and light blue. Residues within these positions where changes
    occurred are shown in blue. Given such a MSA, one can infer correlations statistically
    found between two residues that these sequence positions are spatially adjacent,
    i.e., they are contacts [[102](#bib.bib102), [36](#bib.bib36), [103](#bib.bib103)].
    (b) One grammatically complex sentence contains long-distance dependencies (shown
    in bold).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：蛋白质与语言的比较。(a) 对齐中的一个蛋白质的 MSA 和残基接触之间的关系。共演变的位置以红色和浅蓝色突出显示。这些位置中发生变化的残基以蓝色显示。给定这样的
    MSA，可以推断出统计上发现的两个残基之间的相关性，即这些序列位置在空间上相邻，即它们是接触的[[102](#bib.bib102), [36](#bib.bib36),
    [103](#bib.bib103)]。(b) 一个语法复杂的句子包含远距离依赖关系（以**粗体**显示）。
- en: 'Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Protein and Language ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")(a) demonstrates
    the statistical inference of residue contacts in a protein based on a MSA. It
    highlights the existence of long-range dependencies between two residues, where
    they may be distant in the sequence but spatially close, indicating coevolution.
    A similar phenomenon of long-distance dependencies is observed in human languages
    as well. Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Protein and Language ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")(b) provides
    an example of language grammar rules that require agreement between words that
    are far apart [[104](#bib.bib104)]. These similarities suggest that successful
    methods from NLP can be applied to analyze protein data. However, it is important
    to note that proteins are distinct from human languages, despite these shared
    characteristics. For instance, training LMs often necessitates a vast corpus,
    which requires tokenization, i.e., breaking down the text into individual tokens
    or using words directly as tokens. This serves computational purposes and ideally
    aligns with linguistic goals in NLP [[105](#bib.bib105), [106](#bib.bib106), [97](#bib.bib97),
    [98](#bib.bib98), [107](#bib.bib107)]. In contrast, protein tokenization methods
    are still at a rudimentary stage without a well-defined and biologically meaningful
    algorithm.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [7](#S4.F7 "图 7 ‣ 4.1 蛋白质与语言 ‣ 4 蛋白质基础 ‣ 深度学习在蛋白质科学中的进展：综合调查")(a) 展示了基于 MSA
    的蛋白质中残基接触的统计推断。它突出了两个残基之间的远程依赖关系，它们在序列中可能距离很远，但在空间上很近，表明共演变。人类语言中也观察到类似的远距离依赖现象。图
    [7](#S4.F7 "图 7 ‣ 4.1 蛋白质与语言 ‣ 4 蛋白质基础 ‣ 深度学习在蛋白质科学中的进展：综合调查")(b) 提供了一个语言语法规则的示例，该规则要求远离的单词之间的一致性[[104](#bib.bib104)]。这些相似性表明，自然语言处理（NLP）中的成功方法可以应用于分析蛋白质数据。然而，重要的是要注意，尽管存在这些共享特征，但蛋白质与人类语言是不同的。例如，训练语言模型通常需要大量的语料库，这需要标记化，即将文本拆分为单独的标记或直接使用单词作为标记。这有助于计算目的，并理想地与
    NLP 的语言学目标一致[[105](#bib.bib105), [106](#bib.bib106), [97](#bib.bib97), [98](#bib.bib98),
    [107](#bib.bib107)]。相比之下，蛋白质标记化方法仍处于初步阶段，尚无明确且具有生物学意义的算法。
- en: 4.2 Protein Physicochemical Properties
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 蛋白质的物理化学性质
- en: Physicochemical properties of proteins refer to the characteristics and behaviors
    of proteins that are determined by their chemical and physical properties, playing
    a crucial role in protein structure, stability, function, and interactions. Only
    when the environment is suitable for the physicochemical properties of a protein
    can it remain alive and play its role [[108](#bib.bib108)]. Understanding the
    physicochemical properties of proteins is crucial for developing new protein drugs.
    Certain physicochemical characteristics of proteins resemble those of amino acids,
    including amphoteric ionization, isoelectric point, color reaction, salt reaction,
    and more. However, there are also differences between proteins and amino acids
    in terms of properties such as high molecular weight, colloid behavior, denaturation,
    and others. Several studies have utilized amino acid related physicochemical properties
    to gain insights into the biochemical nature of each amino acid [[109](#bib.bib109),
    [110](#bib.bib110)]. These properties include steric parameter, hydrophobicity,
    volume, polarizability, isoelectric point, helix probability, and sheet probability.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质的物理化学性质指的是由其化学和物理性质决定的蛋白质特性和行为，这些特性在蛋白质的结构、稳定性、功能和相互作用中发挥着至关重要的作用。只有当环境适合蛋白质的物理化学性质时，它才能保持活性并发挥其作用 [[108](#bib.bib108)]。理解蛋白质的物理化学性质对于开发新型蛋白质药物至关重要。蛋白质的一些物理化学特征类似于氨基酸，包括两性电离点、等电点、颜色反应、盐反应等。然而，在分子量、高分子行为、变性等性质方面，蛋白质和氨基酸也存在差异。一些研究利用与氨基酸相关的物理化学性质，深入了解每种氨基酸的生化特性 [[109](#bib.bib109),
    [110](#bib.bib110)]。这些特性包括立体参数、疏水性、体积、极化性、等电点、螺旋概率和片层概率。
- en: The importance of these features at the residue level has been calculated and
    visualized by HIGH-PPI [[111](#bib.bib111)]. They have found that features such
    as isoelectric point, polarity, and hydrogen bond acceptor function in protein-protein
    interaction interfaces. By analyzing the changes in evaluation scores before and
    after dropping each individual feature dimension from the model, they have identified
    topological polar surface area and octanol-water partition coefficient as dominant
    features for PPI interface characterization.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过HIGH-PPI [[111](#bib.bib111)]，已经计算并可视化了这些特性在残基水平上的重要性。他们发现，等电点、极性和氢键受体功能等特性在蛋白质-蛋白质相互作用界面中发挥作用。通过分析在从模型中删除每个单独特征维度前后的评估分数变化，他们确定了拓扑极性表面积和辛醇-水分配系数是PPI界面特征表征的主要特征。
- en: 4.3 Motifs, Regions, and Domains
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 模体、区域和结构域
- en: 'Motifs, regions, and domains are commonly used as additional information for
    training deep learning models. A motif refers to a short, conserved sequence pattern
    or structural feature that is found in multiple proteins or nucleic acids. It
    represents a functional or structural unit that is often associated with a specific
    biological activity [[112](#bib.bib112)]. Motifs can be used as signatures to
    identify potential binding sites for ligands, substrates, or other interacting
    molecules. On the other hand, the term “region” denotes a specific region of interest
    within a sequence. In contrast, a domain is an independent unit within a protein
    or nucleic acid sequence, both structurally and functionally [[113](#bib.bib113)].
    Domains can fold into stable 3D structures and often perform specific functions.
    Figure [8](#S4.F8 "Figure 8 ‣ 4.3 Motifs, Regions, and Domains ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey") illustrates
    various data categories in the protein field, providing an example of motifs,
    regions, and domains. Hu et al. [[114](#bib.bib114)] have collected multiple datasets
    that include 1364 categories of motifs, 3383 categories of domains, and 10628
    categories of regions. It is important to note that these categories exhibit long-tailed
    distributions. Therefore, when working with protein-related tasks, it is crucial
    to consider the multimodal property and long-tail effects of protein data.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 模式、区域和结构域通常作为额外信息用于训练深度学习模型。模式是指在多个蛋白质或核酸中发现的短的、保守的序列模式或结构特征。它代表了一个功能性或结构性单元，通常与特定的生物学活动相关[[112](#bib.bib112)]。模式可以作为标志来识别潜在的配体、底物或其他相互作用分子的结合位点。另一方面，“区域”一词指的是序列中的特定兴趣区域。相比之下，结构域是指蛋白质或核酸序列中的独立单元，无论在结构上还是功能上[[113](#bib.bib113)]。结构域可以折叠成稳定的三维结构，并且通常执行特定的功能。图[8](#S4.F8
    "图 8 ‣ 4.3 模式、区域和结构域 ‣ 4 蛋白质基础 ‣ 深度学习在蛋白质科学中的进展：全面综述")展示了蛋白质领域中的各种数据类别，并提供了模式、区域和结构域的示例。Hu
    等人[[114](#bib.bib114)]收集了多个数据集，包括1364类模式、3383类结构域和10628类区域。需要注意的是，这些类别呈长尾分布。因此，在处理与蛋白质相关的任务时，必须考虑蛋白质数据的多模态特性和长尾效应。
- en: '![Refer to caption](img/e49bb3987bb3e78cd151726bb6a69e8f.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e49bb3987bb3e78cd151726bb6a69e8f.png)'
- en: 'Figure 8: An overview of the multimodal dataset of proteins [[114](#bib.bib114)],
    including sequences, structures, GO terms, regions, domains, and motifs.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：蛋白质的多模态数据集概述[[114](#bib.bib114)]，包括序列、结构、GO 术语、区域、结构域和模式。
- en: 4.4 Protein Structure Geometries
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 蛋白质结构几何
- en: 'Wang et al. [[115](#bib.bib115)] emphasize the importance of effectively utilizing
    multi-level structural information for accurate protein function prediction, where
    the four distinct levels are shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Mathematical
    Definitions ‣ 2 Definitions, Notations, and Terms ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey"). They propose incorporating the PPI
    task during the pre-training phase to capture quaternary structure information.
    In addition to considering multiple levels of protein structures, deep learning
    models can leverage hierarchical relationships to process tertiary structure information.
    For instance, ProNet [[116](#bib.bib116)] focuses on representation learning for
    proteins with 3D structures at various levels, such as the amino acid, backbone,
    or all-atom levels. At the amino acid level, ProNet considers the $\mathrm{C}_{\alpha}$
    positions of the structures. At the backbone level, it incorporates the information
    of all backbone atoms ($\mathrm{C}_{\alpha},\mathrm{C},\mathrm{N},\mathrm{O}$).
    Finally, at the all-atom level, ProNet processes the coordinates of both backbone
    and side chain atoms.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人[[115](#bib.bib115)]强调了有效利用多层次结构信息对准确预测蛋白质功能的重要性，四个不同的层次在图[3](#S2.F3
    "图 3 ‣ 2.1 数学定义 ‣ 2 定义、符号和术语 ‣ 深度学习在蛋白质科学中的进展：全面综述")中展示。他们建议在预训练阶段结合 PPI 任务，以捕捉四级结构信息。除了考虑蛋白质结构的多个层次外，深度学习模型还可以利用层级关系来处理三级结构信息。例如，ProNet[[116](#bib.bib116)]专注于对具有三维结构的蛋白质进行不同层次的表征学习，如氨基酸、骨架或全原子层次。在氨基酸层次，ProNet
    考虑了结构的 $\mathrm{C}_{\alpha}$ 位置。在骨架层次，它包含了所有骨架原子（$\mathrm{C}_{\alpha},\mathrm{C},\mathrm{N},\mathrm{O}$）的信息。最后，在全原子层次，ProNet
    处理骨架和侧链原子的坐标。
- en: '![Refer to caption](img/649b6526a3e92a7c0b24b5338b690bc4.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/649b6526a3e92a7c0b24b5338b690bc4.png)'
- en: (a)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/345bb4e19766667a2921b6830bd8593a.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/345bb4e19766667a2921b6830bd8593a.png)'
- en: (b)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 9: Protein structure geometries [[119](#bib.bib119)]. (a) The local
    coordinate system, $P_{i,\mathrm{C}_{\alpha}}$ is the coordinate of $\mathrm{C}_{\alpha}$
    in residue $i$. (b) Interresidue geometries, including the distance ($d_{ij,\mathrm{C}_{\beta}}$),
    three dihedral angles ($\omega_{ij},\theta_{ij},\theta_{ji}$) and two planar angles
    ($\varphi_{ij},\varphi_{ji}$).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：蛋白质结构几何 [[119](#bib.bib119)]。(a) 局部坐标系统，$P_{i,\mathrm{C}_{\alpha}}$ 是残基
    $i$ 中 $\mathrm{C}_{\alpha}$ 的坐标。(b) 残基间几何，包括距离 ($d_{ij,\mathrm{C}_{\beta}}$)、三个二面角
    ($\omega_{ij},\theta_{ij},\theta_{ji}$) 和两个平面角 ($\varphi_{ij},\varphi_{ji}$)。
- en: Local Coordinate System
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 局部坐标系统
- en: 'The locally informative features are developed from the local coordinate system
    (LCS) [[117](#bib.bib117)], shown in Figure [9a](#S4.F9.sf1 "In Figure 9 ‣ 4.4
    Protein Structure Geometries ‣ 4 Protein Foundations ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey"), which is defined as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 局部信息特征源自局部坐标系统 (LCS) [[117](#bib.bib117)]，如图 [9a](#S4.F9.sf1 "在图 9 ‣ 4.4 蛋白质结构几何
    ‣ 4 蛋白质基础 ‣ 深度学习在蛋白质科学中的进展：综合调查") 所示，定义为：
- en: '|  | $\bm{Q}_{i}=[\bm{b_{i}}\quad\bm{n_{i}}\quad\bm{b_{i}}\times\bm{n_{i}}]$
    |  | (12) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{Q}_{i}=[\bm{b_{i}}\quad\bm{n_{i}}\quad\bm{b_{i}}\times\bm{n_{i}}]$
    |  | (12) |'
- en: where $\bm{u}_{i}=\frac{{P}_{i,\mathrm{C}\alpha}-{P}_{i-1,\mathrm{C}\alpha}}{\left\|{P}_{i,\mathrm{C}\alpha}-{P}_{i-1,\mathrm{C}\alpha}\right\|},\bm{b_{i}}=\frac{\bm{u}_{i}-\bm{u}_{i+1}}{\left\|\bm{u}_{i}-\bm{u}_{i+1}\right\|},\bm{n}_{i}=\frac{\bm{u}_{i}\times\bm{u}_{i+1}}{\left\|\bm{u}_{i}\times\bm{u}_{i+1}\right\|}$,
    $\bm{b_{i}}$ is the negative bisector of the angle between the rays ($P_{i-1,\mathrm{C}_{\alpha}}-P_{i,\mathrm{C}_{\alpha}}$)
    and ($P_{i+1,\mathrm{C}_{\alpha}}-P_{i,\mathrm{C}_{\alpha}}$), ${P}_{i,\mathrm{C}\alpha}$
    represent the coordinate of atom $\mathrm{C}_{\alpha}$ in node $v_{i}$, and $\left\|\cdot\right\|$
    denotes the $l^{2}$-norm. It is clear to see that LCS is defined at the amino
    acid level. The spatial edge features $\bm{e}_{ij}^{(1)}$ can be obtained considering
    the distance, direction, and orientation by LCS,
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{u}_{i}=\frac{{P}_{i,\mathrm{C}\alpha}-{P}_{i-1,\mathrm{C}\alpha}}{\left\|{P}_{i,\mathrm{C}\alpha}-{P}_{i-1,\mathrm{C}\alpha}\right\|},\bm{b_{i}}=\frac{\bm{u}_{i}-\bm{u}_{i+1}}{\left\|\bm{u}_{i}-\bm{u}_{i+1}\right\|},\bm{n}_{i}=\frac{\bm{u}_{i}\times\bm{u}_{i+1}}{\left\|\bm{u}_{i}\times\bm{u}_{i+1}\right\|}$，$\bm{b_{i}}$
    是角度的负平分线，角度由射线 ($P_{i-1,\mathrm{C}_{\alpha}}-P_{i,\mathrm{C}_{\alpha}}$) 和 ($P_{i+1,\mathrm{C}_{\alpha}}-P_{i,\mathrm{C}_{\alpha}}$)
    形成，${P}_{i,\mathrm{C}\alpha}$ 代表节点 $v_{i}$ 中原子 $\mathrm{C}_{\alpha}$ 的坐标，$\left\|\cdot\right\|$
    表示 $l^{2}$-范数。可以清楚地看到，LCS 在氨基酸级别定义。空间边特征 $\bm{e}_{ij}^{(1)}$ 可以通过 LCS 考虑距离、方向和取向来获得，
- en: '|  | $\bm{e}_{ij}^{(1)}=\mathrm{Concat}(\left\&#124;d_{ij,\mathrm{C}\alpha}\right\&#124;,\bm{Q}_{i}^{T}\cdot\frac{d_{ij,\mathrm{C}\alpha}}{\left\&#124;d_{ij,\mathrm{C}\alpha}\right\&#124;},\bm{Q}_{i}^{T}\cdot\bm{Q}_{j})$
    |  | (13) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{e}_{ij}^{(1)}=\mathrm{Concat}(\left\&#124;d_{ij,\mathrm{C}\alpha}\right\&#124;,\bm{Q}_{i}^{T}\cdot\frac{d_{ij,\mathrm{C}\alpha}}{\left\&#124;d_{ij,\mathrm{C}\alpha}\right\&#124;},\bm{Q}_{i}^{T}\cdot\bm{Q}_{j})$
    |  | (13) |'
- en: where $\cdot$ is the matrix multiplication, and $d_{ij,\mathrm{C}\alpha}={P}_{i,\mathrm{C}\alpha}-{P}_{j,\mathrm{C}\alpha}$.
    This implementation obtains complete representations at the amino acid level;
    as if we have $\bm{Q}_{i}$, the LCS $\bm{Q}_{j}$ can be easily obtained by $\bm{e}_{ij}^{(1)}$.
    Thus, LCS is widely utilized in protein design, antibody design, and PRL [[117](#bib.bib117),
    [118](#bib.bib118), [119](#bib.bib119)].
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\cdot$ 是矩阵乘法，$d_{ij,\mathrm{C}\alpha}={P}_{i,\mathrm{C}\alpha}-{P}_{j,\mathrm{C}\alpha}$。该实现能够在氨基酸级别获得完整的表示；例如我们有
    $\bm{Q}_{i}$，LCS $\bm{Q}_{j}$ 可以通过 $\bm{e}_{ij}^{(1)}$ 容易获得。因此，LCS 被广泛应用于蛋白质设计、抗体设计以及
    PRL [[117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119)]。
- en: trRosetta Interresidue Geometries
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: trRosetta 残基间几何
- en: 'We introduce the relative rotations and distances in trRosetta [[120](#bib.bib120)],
    including the distance ($d_{ij,\mathrm{C}_{\beta}}$), three dihedral angles ($\omega_{ij},\theta_{ij},\theta_{ji}$)
    and two planar angles ($\varphi_{ij},\varphi_{ji}$), as shown in Figure [9b](#S4.F9.sf2
    "In Figure 9 ‣ 4.4 Protein Structure Geometries ‣ 4 Protein Foundations ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey"), where $d_{ij,\mathrm{C}_{\beta}}=d_{ji,\mathrm{C}_{\beta}},\omega_{ij}=\omega_{ji}$,
    but $\theta$ and $\varphi$ values depend on the order of residues. These interresidue
    geometries define the relative locations of the backbone atoms of two residues
    in all their details [[120](#bib.bib120)], because the torsion angles of $\mathrm{N}_{i}-\mathrm{C}_{\alpha
    i}$ and $\mathrm{C}_{\alpha i}-\mathrm{C}_{i}$ do not influence their positions.
    Therefore, these six geometries are complete for amino acids at the backbone level
    for the radius graph, which are commonly used in PSP and protein model quality
    assessment [[120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122), [123](#bib.bib123),
    [124](#bib.bib124)]. The edge features $\bm{e}_{ij}^{(2)}$ can be obtained by
    these interresidue geometries, which contain the relative spatial information
    between any two neighboring amino acids.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 trRosetta 中的相对旋转和距离[[120](#bib.bib120)]，包括距离（$d_{ij,\mathrm{C}_{\beta}}$），三个二面角（$\omega_{ij},\theta_{ij},\theta_{ji}$）和两个平面角（$\varphi_{ij},\varphi_{ji}$），如图
    [9b](#S4.F9.sf2 "图 9 ‣ 4.4 蛋白质结构几何 ‣ 4 蛋白质基础 ‣ 深度学习在蛋白质科学中的进展：综合调查")所示，其中 $d_{ij,\mathrm{C}_{\beta}}=d_{ji,\mathrm{C}_{\beta}},\omega_{ij}=\omega_{ji}$，但
    $\theta$ 和 $\varphi$ 的值取决于残基的顺序。这些残基间的几何结构定义了两个残基的骨架原子的相对位置的所有细节[[120](#bib.bib120)]，因为
    $\mathrm{N}_{i}-\mathrm{C}_{\alpha i}$ 和 $\mathrm{C}_{\alpha i}-\mathrm{C}_{i}$
    的扭转角度不会影响它们的位置。因此，这六种几何结构对于骨架级别的氨基酸在半径图中是完整的，这些图在 PSP 和蛋白质模型质量评估中常被使用[[120](#bib.bib120),
    [121](#bib.bib121), [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124)]。这些残基间几何结构可以得到边缘特征
    $\bm{e}_{ij}^{(2)}$，它包含了任何两个相邻氨基酸之间的相对空间信息。
- en: '|  | $\bm{e}_{ij}^{(2)}=\mathrm{Concat}(d_{ij,\mathrm{C}_{\beta}},(\sin\wedge\cos)(\omega_{ij},\theta_{ij},\varphi_{ij}))$
    |  | (14) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{e}_{ij}^{(2)}=\mathrm{Concat}(d_{ij,\mathrm{C}_{\beta}},(\sin\wedge\cos)(\omega_{ij},\theta_{ij},\varphi_{ij}))$
    |  | (14) |'
- en: '![Refer to caption](img/c54774d6c039255f3d677e046dd0f4a9.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c54774d6c039255f3d677e046dd0f4a9.png)'
- en: 'Figure 10: The polypeptide chain depicting the characteristic backbone bond
    lengths, angles, and torsion angles ($\Psi_{i},\Phi_{i},\Omega_{i}$). The planar
    peptide groups are denoted as shaded gray regions, indicating that the peptide
    plane differs from the geometric plane calculated from 3D positions [[119](#bib.bib119)].'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：显示了特征性骨架键长、角度和扭转角度（$\Psi_{i},\Phi_{i},\Omega_{i}$）的多肽链。平面肽基团以灰色阴影区域表示，表明肽平面与从
    3D 位置计算的几何平面不同[[119](#bib.bib119)]。
- en: Backbone Torsion Angles
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 骨架扭转角
- en: 'The peptide bond exhibits partial double-bond character due to resonance [[125](#bib.bib125)],
    indicating that the three non-hydrogen atoms comprising the bond are coplanar,
    as shown in Figure [10](#S4.F10 "Figure 10 ‣ trRosetta Interresidue Geometries
    ‣ 4.4 Protein Structure Geometries ‣ 4 Protein Foundations ‣ Advances of Deep
    Learning in Protein Science: A Comprehensive Survey"), the free rotation about
    the bond is limited due to the coplanar property. The $\mathrm{N}_{i}-\mathrm{C}_{\alpha
    i}$ and $\mathrm{C}_{\alpha i}-\mathrm{C}_{i}$ bonds, are the two bonds in the
    basic repeating unit of the polypeptide backbone. These single bonds allow unrestricted
    rotation until sterically restricted by side chains [[126](#bib.bib126), [127](#bib.bib127)].
    The coordinates of backbone atoms based on these rigid bond lengths and angles
    are able to be determined with the remaining degree of the backbone torsion angles
    $\Phi_{i},\Psi_{i},\Omega_{i}$. The omega torsion angle around the $\mathrm{C}-\mathrm{N}$
    peptide bond is typically restricted to nearly $180^{\circ}$ (trans) but can approach
    $0^{\circ}$ (cis) in rare instances. Other than the bond lengths and angles presented
    in Figure [10](#S4.F10 "Figure 10 ‣ trRosetta Interresidue Geometries ‣ 4.4 Protein
    Structure Geometries ‣ 4 Protein Foundations ‣ Advances of Deep Learning in Protein
    Science: A Comprehensive Survey"), all the H bond lengths measure approximately
    1 Å.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '肽键由于共振效应表现出部分双键特性[[125](#bib.bib125)]，这表明组成该键的三个非氢原子是共面的，如图[10](#S4.F10 "Figure
    10 ‣ trRosetta Interresidue Geometries ‣ 4.4 Protein Structure Geometries ‣ 4
    Protein Foundations ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")所示，由于共面特性，键的自由旋转受到限制。$\mathrm{N}_{i}-\mathrm{C}_{\alpha i}$ 和 $\mathrm{C}_{\alpha
    i}-\mathrm{C}_{i}$ 键是多肽骨架基本重复单元中的两个键。这些单键允许不受限制的旋转，直到被侧链立体限制[[126](#bib.bib126),
    [127](#bib.bib127)]。基于这些刚性键长和角度，可以确定骨架原子的坐标，剩余的骨架扭转角度为$\Phi_{i},\Psi_{i},\Omega_{i}$。围绕$\mathrm{C}-\mathrm{N}$肽键的omega扭转角通常限制在接近$180^{\circ}$（反式），但在少数情况下可以接近$0^{\circ}$（顺式）。除了图[10](#S4.F10
    "Figure 10 ‣ trRosetta Interresidue Geometries ‣ 4.4 Protein Structure Geometries
    ‣ 4 Protein Foundations ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")中展示的键长和角度外，所有氢键长测量约为1 Å。'
- en: Euler Angles
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 欧拉角
- en: 'Different from the trRosetta interresidue geometries, Wang et al. [[116](#bib.bib116)]
    propose to use Euler angles to capture the rotation between two backbone planes.
    Unlike this protein design method [[117](#bib.bib117)], ProNet [[116](#bib.bib116)]
    defines its local coordinate system for an amino acid $i$ as $\bm{y}_{i}=\bm{r}_{i}^{\mathrm{N}}-\bm{r}_{i}^{\mathrm{C}_{\alpha}},\bm{t}_{i}=\bm{r}_{i}^{\mathrm{C}}-\bm{r}_{i}^{\mathrm{C}_{\alpha}}$,
    and $\bm{z}_{i}=\bm{t}_{i}\times\bm{y}_{i},\bm{x}_{i}=\bm{y}_{i}\times\bm{z}_{i}$,
    where the $\bm{r}_{i}$ represents the position vector of the $i$-th amino acid
    in a protein, this coordinate system is shown in Figure [11](#S4.F11 "Figure 11
    ‣ Euler Angles ‣ 4.4 Protein Structure Geometries ‣ 4 Protein Foundations ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey")(a). The three Euler
    angles $\tau_{ij}^{1},\tau_{ij}^{2}$ and $\tau_{ij}^{3}$ between two backbone
    coordinate systems can be computed as shown in Figure [11](#S4.F11 "Figure 11
    ‣ Euler Angles ‣ 4.4 Protein Structure Geometries ‣ 4 Protein Foundations ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey")(b), where $\bm{n}=\bm{z}_{i}\times\bm{z}_{j}$,
    is the intersection of two planes. $\tau_{ij}^{1}$ is the signed angle between
    $\bm{n}$ and $\bm{x}_{i}$, $\tau_{ij}^{2}$ is the angle between $\bm{z}_{i}$ and
    $\bm{z}_{j}$, and $\tau_{ij}^{3}$ is the angle from $\bm{n}$ to $\bm{x}_{j}$.
    The relative rotations for any two amino acids $i$ and $j$ can be determined by
    these three Euler angles [[116](#bib.bib116)].'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与 trRosetta 的残基几何结构不同，Wang 等人[[116](#bib.bib116)] 提出了使用欧拉角来捕捉两个主链平面之间的旋转。与这种蛋白质设计方法[[117](#bib.bib117)]不同，ProNet[[116](#bib.bib116)]
    将氨基酸 $i$ 的局部坐标系定义为 $\bm{y}_{i}=\bm{r}_{i}^{\mathrm{N}}-\bm{r}_{i}^{\mathrm{C}_{\alpha}},\bm{t}_{i}=\bm{r}_{i}^{\mathrm{C}}-\bm{r}_{i}^{\mathrm{C}_{\alpha}}$
    和 $\bm{z}_{i}=\bm{t}_{i}\times\bm{y}_{i},\bm{x}_{i}=\bm{y}_{i}\times\bm{z}_{i}$，其中
    $\bm{r}_{i}$ 代表蛋白质中第 $i$ 个氨基酸的位置矢量，该坐标系如图 [11](#S4.F11 "图 11 ‣ 欧拉角 ‣ 4.4 蛋白质结构几何
    ‣ 4 蛋白质基础 ‣ 深度学习在蛋白质科学中的进展：综合调查")(a) 所示。两个主链坐标系之间的三个欧拉角 $\tau_{ij}^{1},\tau_{ij}^{2}$
    和 $\tau_{ij}^{3}$ 可以如图 [11](#S4.F11 "图 11 ‣ 欧拉角 ‣ 4.4 蛋白质结构几何 ‣ 4 蛋白质基础 ‣ 深度学习在蛋白质科学中的进展：综合调查")(b)
    所示进行计算，其中 $\bm{n}=\bm{z}_{i}\times\bm{z}_{j}$ 是两个平面的交线。$\tau_{ij}^{1}$ 是 $\bm{n}$
    和 $\bm{x}_{i}$ 之间的有向角，$\tau_{ij}^{2}$ 是 $\bm{z}_{i}$ 和 $\bm{z}_{j}$ 之间的角度，而 $\tau_{ij}^{3}$
    是从 $\bm{n}$ 到 $\bm{x}_{j}$ 的角度。这两个氨基酸 $i$ 和 $j$ 之间的相对旋转可以通过这三个欧拉角来确定[[116](#bib.bib116)]。
- en: '![Refer to caption](img/7b6e8e727d441aec800fe214dcf6c51f.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7b6e8e727d441aec800fe214dcf6c51f.png)'
- en: 'Figure 11: Illustration of Euler angles [[116](#bib.bib116)]. (a) The backbone
    coordinate system for an amino acid. (b) The three Euler angles between the backbone
    coordinate system for amino acids $i$ and $j$.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：欧拉角的示意图[[116](#bib.bib116)]。(a) 氨基酸的主链坐标系。(b) 氨基酸 $i$ 和 $j$ 之间主链坐标系的三个欧拉角。
- en: In summary, the backbone torsion angles $\Phi_{i},\Psi_{i},\Omega_{i}$, trRosetta
    interresidue geometries and Euler angles are defined at the backbone level, while
    the LCS is defined at the amino acid level. When considering the positions of
    all atoms, following the implementations in AF2 [[16](#bib.bib16)], the first
    four torsion angles $\chi_{1},\chi_{2},\chi_{3},\chi_{4}$, are usually considered
    for side chain atoms, as only the amino acid arginine has five side chain torsion
    angles, and the fifth angle is close to 0 [[116](#bib.bib116)].
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，主链的扭转角 $\Phi_{i},\Psi_{i},\Omega_{i}$，trRosetta 的残基几何结构和欧拉角是在主链层面定义的，而
    LCS 是在氨基酸层面定义的。在考虑所有原子的位置时，参照 AF2 的实现[[16](#bib.bib16)]，前四个扭转角 $\chi_{1},\chi_{2},\chi_{3},\chi_{4}$
    通常被考虑用于侧链原子，因为只有氨基酸精氨酸有五个侧链扭转角，第五个角接近 0 [[116](#bib.bib116)]。
- en: 4.5 Structure Properties
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 结构性质
- en: Proteins can move in the 3D space through translations and rotations. These
    properties, such as translation and rotation invariance, improve the accuracy,
    reliability, and usefulness of models in different protein-related tasks. One
    example is GVP-GNN [[128](#bib.bib128)], which can process both scalar features
    and vectors, enabling the inclusion of detailed geometric information at nodes
    and edges without oversimplifying it into scalar values that may not fully represent
    complex geometry.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质可以通过平移和旋转在 3D 空间中移动。这些性质，例如平移和旋转不变性，提升了模型在不同蛋白质相关任务中的准确性、可靠性和实用性。一个例子是 GVP-GNN[[128](#bib.bib128)]，它可以处理标量特征和向量，从而能够在节点和边上包含详细的几何信息，而不会将其过度简化为可能无法完全代表复杂几何的标量值。
- en: Invariance and Equivariance
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不变性与等变性
- en: We examine affine transformations that maintain the distance between any two
    points, known as the isometric group SE(3) in Euclidean space. This group, denoted
    as the symmetry group, encompasses 3D translations and the 3D rotation group SO(3) [[129](#bib.bib129),
    [130](#bib.bib130)].
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了保持任意两点之间距离不变的仿射变换，这在欧几里得空间中被称为等距群 SE(3)。这个群被称为对称群，包括 3D 平移和 3D 旋转群 SO(3)
    [[129](#bib.bib129), [130](#bib.bib130)]。
- en: 'The collection of $4\times 4$ real matrices of the SE(3) is shown as:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: SE(3) 的 $4\times 4$ 实矩阵的集合如下所示：
- en: '|  | <math   alttext="\left[\begin{array}[]{cc}R&amp;\mathbf{t}\\ 0&amp;1\end{array}\right]=\left[\begin{array}[]{cccc}r_{11}&amp;r_{12}&amp;r_{13}&amp;t_{1}\\'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\left[\begin{array}[]{cc}R&amp;\mathbf{t}\\ 0&amp;1\end{array}\right]=\left[\begin{array}[]{cccc}r_{11}&amp;r_{12}&amp;r_{13}&amp;t_{1}\\'
- en: r_{21}&amp;r_{22}&amp;r_{23}&amp;t_{2}\\
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: r_{21}&amp;r_{22}&amp;r_{23}&amp;t_{2}\\
- en: r_{31}&amp;r_{32}&amp;r_{33}&amp;t_{3}\\
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: r_{31}&amp;r_{32}&amp;r_{33}&amp;t_{3}\\
- en: 0&amp;0&amp;0&amp;1\end{array}\right]," display="block"><semantics ><mrow ><mrow
    ><mrow  ><mo >[</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  ><mi >R</mi></mtd><mtd ><mi  >𝐭</mi></mtd></mtr><mtr ><mtd  ><mn >0</mn></mtd><mtd
    ><mn  >1</mn></mtd></mtr></mtable><mo >]</mo></mrow><mo >=</mo><mrow ><mo  >[</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  ><msub
    ><mi >r</mi><mn  >11</mn></msub></mtd><mtd ><msub  ><mi >r</mi><mn >12</mn></msub></mtd><mtd
    ><msub  ><mi >r</mi><mn >13</mn></msub></mtd><mtd ><msub  ><mi >t</mi><mn >1</mn></msub></mtd></mtr><mtr
    ><mtd  ><msub ><mi >r</mi><mn  >21</mn></msub></mtd><mtd ><msub  ><mi >r</mi><mn
    >22</mn></msub></mtd><mtd ><msub  ><mi >r</mi><mn >23</mn></msub></mtd><mtd ><msub  ><mi
    >t</mi><mn >2</mn></msub></mtd></mtr><mtr ><mtd  ><msub ><mi >r</mi><mn  >31</mn></msub></mtd><mtd
    ><msub  ><mi >r</mi><mn >32</mn></msub></mtd><mtd ><msub  ><mi >r</mi><mn >33</mn></msub></mtd><mtd
    ><msub  ><mi >t</mi><mn >3</mn></msub></mtd></mtr><mtr ><mtd  ><mn >0</mn></mtd><mtd
    ><mn  >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd ><mn  >1</mn></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow><mo >,</mo></mrow><annotation-xml encoding="MathML-Content"
    ><apply ><apply  ><csymbol cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow
    ><ci  >𝑅</ci><ci >𝐭</ci></matrixrow><matrixrow ><cn type="integer" >0</cn><cn
    type="integer" >1</cn></matrixrow></matrix></apply><apply ><csymbol cd="latexml"
    >delimited-[]</csymbol><matrix ><matrixrow  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑟</ci><cn type="integer" >11</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑟</ci><cn type="integer" >12</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑟</ci><cn type="integer" >13</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑡</ci><cn type="integer" >1</cn></apply></matrixrow><matrixrow ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑟</ci><cn type="integer" >21</cn></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑟</ci><cn type="integer" >22</cn></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑟</ci><cn type="integer" >23</cn></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑡</ci><cn type="integer" >2</cn></apply></matrixrow><matrixrow
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑟</ci><cn type="integer"
    >31</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑟</ci><cn
    type="integer" >32</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑟</ci><cn type="integer" >33</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑡</ci><cn type="integer" >3</cn></apply></matrixrow><matrixrow ><cn type="integer"
    >0</cn><cn type="integer" >0</cn><cn type="integer" >0</cn><cn type="integer"
    >1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >\left[\begin{array}[]{cc}R&\mathbf{t}\\ 0&1\end{array}\right]=\left[\begin{array}[]{cccc}r_{11}&r_{12}&r_{13}&t_{1}\\
    r_{21}&r_{22}&r_{23}&t_{2}\\ r_{31}&r_{32}&r_{33}&t_{3}\\ 0&0&0&1\end{array}\right],</annotation></semantics></math>
    |  | (15) |
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: \left[\begin{array}[]{cc}R&\mathbf{t}\\ 0&1\end{array}\right]=\left[\begin{array}[]{cccc}r_{11}&r_{12}&r_{13}&t_{1}\\
    r_{21}&r_{22}&r_{23}&t_{2}\\ r_{31}&r_{32}&r_{33}&t_{3}\\ 0&0&0&1\end{array}\right]。
- en: where $R\in\mathrm{SO(3)}$ and $\mathbf{t}\in\mathbb{R}^{3}$, SO(3) is the 3D
    rotation group. $R$ satisfying $R^{T}R=I$ and $\mathrm{det}(R)=1$.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$R\in\mathrm{SO(3)}$和$\mathbf{t}\in\mathbb{R}^{3}$，SO(3)是三维旋转群。$R$满足$R^{T}R=I$和$\mathrm{det}(R)=1$。
- en: 'Given the function $f:\mathbb{R}^{d}\to\mathbb{R}^{d^{\prime}}$, assuming the
    given symmetry group $G$ acts on $\mathbb{R}^{d}$ and $\mathbb{R}^{d^{\prime}}$,
    $f$ is considered G-equivariant if it satisfies the following condition:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 给定函数$f:\mathbb{R}^{d}\to\mathbb{R}^{d^{\prime}}$，假设给定对称群$G$作用于$\mathbb{R}^{d}$和$\mathbb{R}^{d^{\prime}}$，如果$f$满足以下条件，则称$f$是G-等变的：
- en: '|  | $f(T_{g}\bm{x})=S_{g}f(\bm{x}),\ \forall\bm{x}\in\mathbb{R}^{d},g\in G$
    |  | (16) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(T_{g}\bm{x})=S_{g}f(\bm{x}),\ \forall\bm{x}\in\mathbb{R}^{d},g\in G$
    |  | (16) |'
- en: here, $T_{g}$ and $S_{g}$ represent the transformations. For the SE(3) group,
    when $d^{{}^{\prime}}=1$ and the output of $f$ is a scalar, we have
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$T_{g}$和$S_{g}$表示变换。对于SE(3)群，当$d^{{}^{\prime}}=1$且$f$的输出是标量时，我们有
- en: '|  | $f(T_{g}\bm{x})=f(\bm{x}),\ \forall\bm{x}\in\mathbb{R}^{d},g\in G$ |  |
    (17) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(T_{g}\bm{x})=f(\bm{x}),\ \forall\bm{x}\in\mathbb{R}^{d},g\in G$ |  |
    (17) |'
- en: thus $f$ is SE(3)-invariant [[131](#bib.bib131)].
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此$f$是SE(3)-不变的 [[131](#bib.bib131)]。
- en: By maintaining SE(3)-equivariance in structural geometries, protein models demonstrate
    the ability to recognize and interpret protein structures irrespective of transformations
    in 3D space. This enables them to learn from a diverse array of protein structures
    and seamlessly apply that knowledge to predict the functions of novel proteins.
    The pivotal capability of generalization plays a critical role in PSP and protein
    design [[132](#bib.bib132)]. Notably, a considerable number of proteins exhibit
    symmetrical properties, such as recurring motifs or symmetric domains. SE(3)-equivariant
    models can effectively capture and leverage these symmetrical properties, thereby
    enhancing their comprehension of protein structures and functions [[133](#bib.bib133)].
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 通过保持结构几何中的SE(3)-等变性，蛋白质模型展现了识别和解释蛋白质结构的能力，无论在三维空间中的变换如何。这使得它们能够从各种蛋白质结构中学习，并将这些知识无缝应用于预测新蛋白质的功能。泛化的关键能力在PSP和蛋白质设计中扮演着重要角色 [[132](#bib.bib132)]。值得注意的是，许多蛋白质具有对称特性，如重复的模体或对称域。SE(3)-等变模型能够有效捕捉和利用这些对称特性，从而增强对蛋白质结构和功能的理解 [[133](#bib.bib133)]。
- en: Complete Geometries
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 完整几何
- en: A geometric transformation $\mathcal{F}(\cdot)$ is complete if for two 3D graphs
    $G^{1}=(\mathcal{V},\mathcal{E},\mathcal{P}^{1})$ and $G^{2}=(\mathcal{V},\mathcal{E},\mathcal{P}^{2})$,
    there exists $T_{g}\in\mathrm{SE(3)}$ such that the representations,
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对于两个三维图形$G^{1}=(\mathcal{V},\mathcal{E},\mathcal{P}^{1})$和$G^{2}=(\mathcal{V},\mathcal{E},\mathcal{P}^{2})$，存在$T_{g}\in\mathrm{SE(3)}$使得表示，
- en: '|  | $\mathcal{F}(G^{1})=\mathcal{F}(G^{2})\Longleftrightarrow P_{i}^{1}=T_{g}(P_{i}^{2}),\
    \mathrm{for}\ i=1,\dots n$ |  | (18) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{F}(G^{1})=\mathcal{F}(G^{2})\Longleftrightarrow P_{i}^{1}=T_{g}(P_{i}^{2}),\
    \mathrm{for}\ i=1,\dots n$ |  | (18) |'
- en: the operation $T_{g}$ would not change the 3D conformation of a 3D graph [[134](#bib.bib134),
    [135](#bib.bib135), [116](#bib.bib116)]. $\mathcal{P}=\{P_{i}\}_{i=1,\ldots,n}$
    is the set of position matrices, $\mathcal{F}(G)\Longleftrightarrow\mathcal{P}$,
    means positions can generate geometric representations, which can also be recovered
    from them.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 操作$T_{g}$不会改变三维图形的三维构型 [[134](#bib.bib134), [135](#bib.bib135), [116](#bib.bib116)]。$\mathcal{P}=\{P_{i}\}_{i=1,\ldots,n}$是位置矩阵的集合，$\mathcal{F}(G)\Longleftrightarrow\mathcal{P}$，意味着位置可以生成几何表示，也可以从中恢复。
- en: Global completeness enhances the robustness of statistical analyses applied
    to protein structure data. Proteins with SE(3) equivalence share identical 3D
    conformations but may differ in orientations and positions. To discern diverse
    conformers, it is essential to comprehensively model entire protein structures.
    Solely focusing on local regions would overlook substantial long-range effects
    arising from subtle conformational changes occurring at a distance [[119](#bib.bib119)].
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 全局完备性增强了应用于蛋白质结构数据的统计分析的鲁棒性。具有SE(3)等价的蛋白质共享相同的三维构型，但可能在方向和位置上有所不同。为了辨别不同的构象，必须全面建模整个蛋白质结构。仅关注局部区域会忽略由于远处微小构象变化而产生的重大远程效应 [[119](#bib.bib119)]。
- en: '![Refer to caption](img/286328a0e6140514dd3ac544410118ff.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/286328a0e6140514dd3ac544410118ff.png)'
- en: 'Figure 12: Examples of proteins with biology knowledge. (a) A sub-graph in
    ProteinKG25 with proteins, GO terms, and relations [[10](#bib.bib10)]. Yellow
    nodes are proteins and blue nodes are GO entities with biological descriptions.
    (b) An example of protein property descriptions, including protein name, function
    texts, subcellular, and similarity [[136](#bib.bib136)].'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：具有生物学知识的蛋白质示例。 (a) ProteinKG25 中的子图，包括蛋白质、GO 术语和关系 [[10](#bib.bib10)]。黄色节点为蛋白质，蓝色节点为带有生物学描述的
    GO 实体。 (b) 蛋白质属性描述示例，包括蛋白质名称、功能文本、亚细胞位置和相似性 [[136](#bib.bib136)]。
- en: 4.6 Biology Knowledge
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 生物学知识
- en: 'The biology knowledge can enhance deep learning based protein models to understand
    the protein structure-function relationships and enable various applications in
    bioinformatics. Zhang et al. [[10](#bib.bib10)] have constructed ProteinKG25,
    which provides large-scale biology knowledge facts aligned with protein sequences,
    as shown in Figure [12](#S4.F12 "Figure 12 ‣ Complete Geometries ‣ 4.5 Structure
    Properties ‣ 4 Protein Foundations ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey")(a). The attributes and relation terms are described using
    natural languages, which are extracted from, GO³³3[https://www.uniprot.org/uniprotkb](https://www.uniprot.org/uniprotkb),
    the world’s largest source of information on the functions of genes and gene products
    (e.g., protein). In order to enhance protein sequences with text descriptions
    of their functions, Xu et al. [[136](#bib.bib136)] describe protein in four fields:
    protein name, functions, the location in a cell, and protein families that a protein
    belongs to (refer to Figure [12](#S4.F12 "Figure 12 ‣ Complete Geometries ‣ 4.5
    Structure Properties ‣ 4 Protein Foundations ‣ Advances of Deep Learning in Protein
    Science: A Comprehensive Survey")(b)).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '生物学知识可以增强基于深度学习的蛋白质模型，以理解蛋白质的结构-功能关系，并在生物信息学中启用各种应用。张等人 [[10](#bib.bib10)]
    构建了 ProteinKG25，它提供了与蛋白质序列对齐的大规模生物学知识事实，如图 [12](#S4.F12 "Figure 12 ‣ Complete
    Geometries ‣ 4.5 Structure Properties ‣ 4 Protein Foundations ‣ Advances of Deep
    Learning in Protein Science: A Comprehensive Survey")(a) 所示。这些属性和关系术语使用自然语言描述，提取自
    GO³³3[https://www.uniprot.org/uniprotkb](https://www.uniprot.org/uniprotkb)，这是全球最大的基因及基因产物（例如蛋白质）功能信息来源。为了增强蛋白质序列的功能文本描述，徐等人
    [[136](#bib.bib136)] 从四个领域描述蛋白质：蛋白质名称、功能、细胞内位置和蛋白质所属的家族（参见图 [12](#S4.F12 "Figure
    12 ‣ Complete Geometries ‣ 4.5 Structure Properties ‣ 4 Protein Foundations ‣
    Advances of Deep Learning in Protein Science: A Comprehensive Survey")(b)）。'
- en: 5 Deep Learning based Protein Models
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个基于深度学习的蛋白质模型
- en: In this section, we summarize some commonly used deep learning models for processing
    protein data, including sequences, structures, functions or hybrid of them.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了一些常用的深度学习模型，用于处理蛋白质数据，包括序列、结构、功能或它们的混合。
- en: 5.1 Protein Language Models
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 蛋白质语言模型
- en: Due to the inherent similarities between proteins and human languages, protein
    sequences, which are represented as strings of amino acid letters, naturally lend
    themselves to LMs. LMs are capable of capturing complex dependencies among these
    amino acids [[99](#bib.bib99)]. Consequently, protein LMs have emerged as promising
    approaches for learning protein sequences, with the ability to handle both single
    sequences and MSAs as input.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 由于蛋白质和人类语言之间固有的相似性，蛋白质序列（以氨基酸字母字符串表示）自然适合于语言模型（LMs）。语言模型能够捕捉这些氨基酸之间的复杂依赖关系 [[99](#bib.bib99)]。因此，蛋白质语言模型已成为学习蛋白质序列的有前途的方法，能够处理单序列和多序列比对（MSAs）作为输入。
- en: Single Sequences
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单序列
- en: 'To begin, we introduce methods that mainly take a single sequence as input.
    Early deep learning methods often utilized CNNs, LSTM, or their combinations [[137](#bib.bib137),
    [138](#bib.bib138), [139](#bib.bib139)], to predict protein structural features
    and properties. Examples of such methods include DeepPrime2Sec [[140](#bib.bib140)],
    SPOT-1D-Single [[141](#bib.bib141)]. Additionally, the Variational Auto-Encoder
    (VAE) [[142](#bib.bib142), [143](#bib.bib143)] has been employed to learn interactions
    between positions within a protein. Given protein sequential data $X$ and latent
    variables $Z$, the protein VAE model aims to learn the joint probability $p(X,Z)=p(Z)p(X|Z)$.
    However, computing $p(Z|X)$ from observed data necessitates evaluating the evidence
    term for each data point:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们介绍主要以单一序列作为输入的方法。早期的深度学习方法通常使用 CNN、LSTM 或其组合[[137](#bib.bib137), [138](#bib.bib138),
    [139](#bib.bib139)]，来预测蛋白质的结构特征和属性。这些方法的例子包括 DeepPrime2Sec [[140](#bib.bib140)]、SPOT-1D-Single
    [[141](#bib.bib141)]。此外，变分自编码器（VAE）[[142](#bib.bib142), [143](#bib.bib143)]也被用于学习蛋白质内部位置之间的交互。给定蛋白质序列数据$X$和潜在变量$Z$，蛋白质
    VAE 模型的目标是学习联合概率$p(X,Z)=p(Z)p(X|Z)$。然而，从观测数据计算$p(Z|X)$需要评估每个数据点的证据项：
- en: '|  | $p(X)=\int p(X&#124;Z)p(Z)d_{Z}$ |  | (19) |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(X)=\int p(X\mid Z)p(Z)d_{Z}$ |  | (19) |'
- en: direct computation of this integral is intractable. Consequently, VAE models
    typically adopt an Evidence Lower BOund (ELBO) [[142](#bib.bib142)] to approximate
    $p(X)$.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 直接计算这个积分是不可行的。因此，VAE 模型通常采用证据下界（ELBO）[[142](#bib.bib142)]来近似$p(X)$。
- en: 'Table 1: LMs used in ProtTrans [[12](#bib.bib12)]'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：ProtTrans中使用的语言模型[[12](#bib.bib12)]
- en: Model Network Pretext Task $\#$Params. Comments BERT [[52](#bib.bib52)] Transformer
    Masked Language Modeling 340M a commonly-used LM for predicting masked tokens
    ALBERT [[146](#bib.bib146)] BERT Masked Language Modeling 223M a lite version
    of BERT Transformer-XL [[147](#bib.bib147)] Transformer Autoregressive Language
    Modeling 257M enabling learn dependencies beyond a fixed length XLNet [[87](#bib.bib87)]
    BERT Autoregressive Language Modeling 340M more training data, integrates ideas
    from Transformer-XL ELECTRA [[148](#bib.bib148)] BERT Replaced Token Detection
    335M for token detection T5 [[149](#bib.bib149)] Transformer Masked Language Modeling
    11B a text-to-text transfer learning framework • All examples report the largest
    model of their public series. Network displays high-level backbone models preferentially
    if they are used to initialize parameters. $\#$Param. means the number of parameters;
    M, millions; B, billions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 网络 预训练任务 $\#$参数 备注 BERT [[52](#bib.bib52)] Transformer 掩码语言建模 340M 一种常用的语言模型，用于预测掩码标记
    ALBERT [[146](#bib.bib146)] BERT 掩码语言建模 223M BERT 的轻量版 Transformer-XL [[147](#bib.bib147)]
    Transformer 自回归语言建模 257M 能够学习超过固定长度的依赖关系 XLNet [[87](#bib.bib87)] BERT 自回归语言建模
    340M 更多训练数据，结合了 Transformer-XL 的思想 ELECTRA [[148](#bib.bib148)] BERT 替换标记检测 335M
    用于标记检测 T5 [[149](#bib.bib149)] Transformer 掩码语言建模 11B 一种文本到文本的迁移学习框架 • 所有示例均报告其公开系列中的最大模型。网络优先显示高层骨干模型，如果它们用于初始化参数。$\#$参数表示参数数量；M，百万；B，十亿。
- en: 'Secondly, the pre-training of protein LMs in the absence of structural or evolutionary
    data has been explored. Alley et al. [[105](#bib.bib105)] employ multiplicative
    long-/short-term memory (mLSTM) [[144](#bib.bib144)] to condense arbitrary protein
    sequences into fixed-length vectors. Notably, TAPE [[145](#bib.bib145)] has introduced
    a benchmark for protein models, including LSTM, Transformer, ResNet, among others,
    by means of self-supervised pre-training and subsequent evaluation on a set of
    five biologically relevant tasks. Elnaggar et al. [[12](#bib.bib12)] successfully
    trained six LMs (BERT [[52](#bib.bib52)], ALBERT [[146](#bib.bib146)], Transformer-XL [[147](#bib.bib147)],
    XLNet [[87](#bib.bib87)], ELECTRA [[148](#bib.bib148)] and T5 [[149](#bib.bib149)],
    show in Table [1](#S5.T1 "Table 1 ‣ Single Sequences ‣ 5.1 Protein Language Models
    ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning in Protein
    Science: A Comprehensive Survey")) on protein sequences encompassing a staggering
    393B amino acids, leveraging extensive computational resources (5616 GPUs and
    one TPU Pod). ESM-1b [[14](#bib.bib14)], on the other hand, consists of a deep
    Transformer architecture (illustrated in Figure [13](#S5.F13 "Figure 13 ‣ Single
    Sequences ‣ 5.1 Protein Language Models ‣ 5 Deep Learning based Protein Models
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")(a)) and
    a masking strategy to construct intricate representations that incorporate contextual
    information from across the entire sequence. The outcomes of ProtTrans [[12](#bib.bib12)]
    and ESM-1b suggest that large-scale protein LMs possess the ability to learn the
    underlying grammar of proteins, even without explicit utilization of apparent
    evolutionary information. With the support of large-scale databases, training
    resources, researchers have started exploring the boundaries of protein LMs by
    constructing billion-level models [[166](#bib.bib166), [169](#bib.bib169), [13](#bib.bib13)].
    For example, ProGen2 [[166](#bib.bib166)] demonstrates that large protein LMs
    can generate libraries of viable sequences, expanding the sequence and structural
    space of natural proteins, obtaining the results suggest the scale of the model
    size can be continued. Moreover, a large-scale protein LM, ESM-2 [[13](#bib.bib13)]
    with trainable parameters up to 15B (billion), has achieved impressive results
    on the PSP task, surpassing smaller ESM models in terms of validation perplexity
    and TM-score [[175](#bib.bib175)]. Chen et al. have proposed a unified protein
    LM, xTrimoPGLM [[170](#bib.bib170)], to handle protein understanding and generation
    tasks concurrently, which involves 100B parameters and 1 trillion training tokens.
    These pre-trained large-scale protein LMs showcase their effectiveness on various
    protein-related tasks. A summary of the pre-trained protein LMs and structure
    models is listed in Table [2](#S5.T2 "Table 2 ‣ Single Sequences ‣ 5.1 Protein
    Language Models ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey").'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '其次，已经探讨了在缺乏结构性或进化数据的情况下对蛋白质语言模型进行预训练的方法。Alley等人[[105](#bib.bib105)]采用乘性长短期记忆（mLSTM）[[144](#bib.bib144)]将任意蛋白质序列浓缩为固定长度的向量。值得注意的是，TAPE[[145](#bib.bib145)]通过自监督预训练及在五个生物学相关任务上的后续评估，推出了蛋白质模型的基准测试，包括LSTM、Transformer、ResNet等。Elnaggar等人[[12](#bib.bib12)]成功训练了六个语言模型（BERT[[52](#bib.bib52)]、ALBERT[[146](#bib.bib146)]、Transformer-XL[[147](#bib.bib147)]、XLNet[[87](#bib.bib87)]、ELECTRA[[148](#bib.bib148)]和T5[[149](#bib.bib149)]，如表[1](#S5.T1
    "Table 1 ‣ Single Sequences ‣ 5.1 Protein Language Models ‣ 5 Deep Learning based
    Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")所示，这些模型在涵盖了惊人的3930亿氨基酸的蛋白质序列上进行了训练，利用了大量计算资源（5616个GPU和一个TPU Pod）。另一方面，ESM-1b[[14](#bib.bib14)]则由一个深度Transformer架构（如图[13](#S5.F13
    "Figure 13 ‣ Single Sequences ‣ 5.1 Protein Language Models ‣ 5 Deep Learning
    based Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")(a)所示）和掩码策略组成，以构建包含整个序列上下文信息的复杂表示。ProtTrans[[12](#bib.bib12)]和ESM-1b的结果表明，大规模的蛋白质语言模型即使在没有明确利用显著的进化信息的情况下，也具备学习蛋白质基本语法的能力。在大型数据库和训练资源的支持下，研究人员开始通过构建十亿级模型来探索蛋白质语言模型的边界[[166](#bib.bib166)、[169](#bib.bib169)、[13](#bib.bib13)]。例如，ProGen2[[166](#bib.bib166)]展示了大型蛋白质语言模型能够生成有效序列库，扩展了自然蛋白质的序列和结构空间，结果表明模型的规模可以继续扩大。此外，具有高达150亿（billion）可训练参数的大规模蛋白质语言模型ESM-2[[13](#bib.bib13)]在PSP任务上取得了令人印象深刻的结果，在验证困惑度和TM-score[[175](#bib.bib175)]方面超过了较小的ESM模型。Chen等人提出了一种统一的蛋白质语言模型xTrimoPGLM[[170](#bib.bib170)]，用于同时处理蛋白质理解和生成任务，该模型涉及1000亿参数和1万亿训练标记。这些预训练的大规模蛋白质语言模型展示了它们在各种与蛋白质相关任务上的有效性。预训练蛋白质语言模型和结构模型的总结列在表[2](#S5.T2
    "Table 2 ‣ Single Sequences ‣ 5.1 Protein Language Models ‣ 5 Deep Learning based
    Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")中。'
- en: '![Refer to caption](img/020fdbf7283e22cd187b0d861789f837.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/020fdbf7283e22cd187b0d861789f837.png)'
- en: 'Figure 13: Core modules of ESM-1b and MSA Transformer.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：ESM-1b 和 MSA Transformer 的核心模块。
- en: 'Table 2: List of representative pre-trained protein LMs and structure models'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：代表性预训练蛋白质语言模型和结构模型列表
- en: 'Model and Repository Input Network $\#$Embedding $\#$Param. Pretext Task Pre-training
    Dataset Year [UniRep](https://github.com/churchlab/Unirep) [[105](#bib.bib105)]
    Seq mLSTM [[144](#bib.bib144)] 1900 18.2M Next Amino Acid Prediction UniRef50
    2019 [TAPE](https://github.com/songlab-cal/tape) [[145](#bib.bib145)] Seq LSTM,
    Transformer, ResNet - 38M Masked Language Modeling Pfam 2019 Next Amino Acid Prediction
    [SeqVec](https://github.com/Rostlab/SeqVec) [[150](#bib.bib150)] Seq ELMo (LSTM) [[151](#bib.bib151)]
    1024 93.6M Next Amino Acid Prediction UniRef50 2019 [UDSMProt](https://github.com/nstrodt/UDSMProt) [[152](#bib.bib152)]
    Seq LSTM 400 24M Next Amino Acid Prediction Swiss-Prot 2020 [CPCProt](https://github.com/amyxlu/CPCProt) [[153](#bib.bib153)]
    Seq GRU [[154](#bib.bib154)], LSTM 1024, 2048 1.7M Contrastive Predictive Coding
    Pfam 2020 [MuPIPR](https://github.com/guangyu-zhou/MuPIPR) [[155](#bib.bib155)]
    Seq GRU, LSTM 64 - Next Amino Acid Prediction STRING [[156](#bib.bib156)] 2020
    Profile Prediction [[157](#bib.bib157)] MSA Transformer - - Alignment Profiles
    Prediction Pfam 2020 PRoBERTa [[158](#bib.bib158)] Seq Transformer 768 44M Masked
    Language Modeling Swiss-Prot 2020 [ESM-1b](https://github.com/facebookresearch/esm) [[14](#bib.bib14)]
    Seq Transformer 1280 650M Masked Language Modeling UniParc 2021 [ProtTXL](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)]
    Seq Transformer-XL 1024 562M Masked Language Modeling BFD100, UniRef100 2021 [ProtBert](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)]
    Seq BERT 1024 420M Masked Language Modeling BFD100, UniRef100 2021 [ProtXLNet](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)]
    Seq XLNet 1024 409M Masked Language Modeling UniRef100 2021 [ProtAlbert](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)]
    Seq ALBERT 4096 224M Masked Language Modeling UniRef100 2021 [ProtElectra](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)]
    Seq ELECTRA 1024 420M Masked Language Modeling UniRef100 2021 [ProtT5](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)]
    Seq T5 1024 11B Masked Language Modeling UniRef50, BFD100 2021 PMLM [[159](#bib.bib159)]
    Seq Transformer 1280 715M Masked Language Modeling UniRef50 2021 [MSA Transformer](https://github.com/facebookresearch/esm) [[160](#bib.bib160)]
    MSA Transformer 768 100M Masked Language Modeling UniRef50, UniClust30 2021 [ProteinLM](https://github.com/THUDM/ProteinLM) [[161](#bib.bib161)]
    Seq BERT - 3B Masked Language Modeling Pfam 2021 [PLUS-RNN](https://github.com/seonwoo-min/PLUS) [[162](#bib.bib162)]
    Seq RNN 2024 59M Masked Language Modeling Pfam 2021 Same-Family Prediction [CARP](https://github.com/microsoft/protein-sequence-models) [[163](#bib.bib163)]
    Seq CNN 1280 640M Masked Language Modeling UniRef50 2022 [AminoBERT](https://github.com/aqlaboratory/rgn2) [[22](#bib.bib22)]
    Seq Transformer 3072 - Masked Language Modeling UniParc 2022 [OmegaPLM](https://github.com/HeliXonProtein/OmegaFold) [[164](#bib.bib164)]
    Seq GAU [[165](#bib.bib165)] 1280 670M Masked Language Modeling UniRef50 2022
    Span and Sequential Masking [ProGen2](https://github.com/salesforce/progen) [[166](#bib.bib166)]
    Seq Transformer 4096 6.4B Masked Language Modeling UniRef90, BFD30, BFD90 2022
    [ProtGPT2](https://huggingface.co/nferruz/ProtGPT2) [[167](#bib.bib167)] Seq GPT-2 [[168](#bib.bib168)]
    1280 738M Next Amino Acid Prediction UniRef50 2022 [RITA](https://github.com/lightonai/RITA) [[169](#bib.bib169)]
    Seq GPT-3 [[55](#bib.bib55)] 2048 1.2B Next Amino Acid Prediction UniRef100 2022
    [ESM-2](https://github.com/facebookresearch/esm) [[13](#bib.bib13)] Seq Transformer
    5120 15B Masked Language Modeling UniRef50 2022 xTrimoPGLM [[170](#bib.bib170)]
    Seq Transformer 10240 100B Masked Language Modeling Uniref90 2023 Span Tokens
    Prediction ColAbFoldDB [ReprogBERT](https://github.com/IBM/ReprogBERT) [[171](#bib.bib171)]
    Seq BERT 768 110M Masked Language Modeling English Wikipedia 2023 BookCorpus [PoET](https://github.com/OpenProteinAI/PoET) [[172](#bib.bib172)]
    MSA Transformer 1024 201M Next Amino Acid Prediction UniRef50 2023 [CELL-E2](https://bohuanglab.github.io/CELL-E_2/) [[173](#bib.bib173)]
    Seq Transformer 480 35M Masked Language Modeling Human Protein Atlas [[174](#bib.bib174)]
    2023 GraphMS [[187](#bib.bib187)] Struct GCN - - Multiview Contrast NeoDTI[[188](#bib.bib188)]
    2021 CRL [[189](#bib.bib189)] Struct IEConv [[186](#bib.bib186)] 2048 36.6M Multiview
    Contrast PDB 2022 STEPS [[190](#bib.bib190)] Struct GNN 1280 - Distance and Dihedral
    Prediction PDB 2022 • Examples report the largest model of their public series.
    Seq: sequence, Struct: Structure. $\#$Embedding means the dimension of embeddings;
    $\#$Param., the number of parameters of network; M, millions; B, billions. Some
    models are linked with the GitHub repositories.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和仓库 输入网络 $\#$嵌入 $\#$参数 预训练任务 预训练数据集 年份 [UniRep](https://github.com/churchlab/Unirep)
    [[105](#bib.bib105)] Seq mLSTM [[144](#bib.bib144)] 1900 18.2M 下一氨基酸预测 UniRef50
    2019 [TAPE](https://github.com/songlab-cal/tape) [[145](#bib.bib145)] Seq LSTM,
    Transformer, ResNet - 38M 掩码语言建模 Pfam 2019 下一氨基酸预测 [SeqVec](https://github.com/Rostlab/SeqVec)
    [[150](#bib.bib150)] Seq ELMo (LSTM) [[151](#bib.bib151)] 1024 93.6M 下一氨基酸预测 UniRef50
    2019 [UDSMProt](https://github.com/nstrodt/UDSMProt) [[152](#bib.bib152)] Seq
    LSTM 400 24M 下一氨基酸预测 Swiss-Prot 2020 [CPCProt](https://github.com/amyxlu/CPCProt)
    [[153](#bib.bib153)] Seq GRU [[154](#bib.bib154)], LSTM 1024, 2048 1.7M 对比预测编码
    Pfam 2020 [MuPIPR](https://github.com/guangyu-zhou/MuPIPR) [[155](#bib.bib155)]
    Seq GRU, LSTM 64 - 下一氨基酸预测 STRING [[156](#bib.bib156)] 2020 配体预测 [[157](#bib.bib157)]
    MSA Transformer - - 对齐概况预测 Pfam 2020 PRoBERTa [[158](#bib.bib158)] Seq Transformer
    768 44M 掩码语言建模 Swiss-Prot 2020 [ESM-1b](https://github.com/facebookresearch/esm)
    [[14](#bib.bib14)] Seq Transformer 1280 650M 掩码语言建模 UniParc 2021 [ProtTXL](https://github.com/agemagician/ProtTrans)
    [[12](#bib.bib12)] Seq Transformer-XL 1024 562M 掩码语言建模 BFD100, UniRef100 2021
    [ProtBert](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)] Seq BERT
    1024 420M 掩码语言建模 BFD100, UniRef100 2021 [ProtXLNet](https://github.com/agemagician/ProtTrans)
    [[12](#bib.bib12)] Seq XLNet 1024 409M 掩码语言建模 UniRef100 2021 [ProtAlbert](https://github.com/agemagician/ProtTrans)
    [[12](#bib.bib12)] Seq ALBERT 4096 224M 掩码语言建模 UniRef100 2021 [ProtElectra](https://github.com/agemagician/ProtTrans)
    [[12](#bib.bib12)] Seq ELECTRA 1024 420M 掩码语言建模 UniRef100 2021 [ProtT5](https://github.com/agemagician/ProtTrans)
    [[12](#bib.bib12)] Seq T5 1024 11B 掩码语言建模 UniRef50, BFD100 2021 PMLM [[159](#bib.bib159)]
    Seq Transformer 1280 715M 掩码语言建模 UniRef50 2021 [MSA Transformer](https://github.com/facebookresearch/esm)
    [[160](#bib.bib160)] MSA Transformer 768 100M 掩码语言建模 UniRef50, UniClust30 2021
    [ProteinLM](https://github.com/THUDM/ProteinLM) [[161](#bib.bib161)] Seq BERT
    - 3B 掩码语言建模 Pfam 2021 [PLUS-RNN](https://github.com/seonwoo-min/PLUS) [[162](#bib.bib162)]
    Seq RNN 2024 59M 掩码语言建模 Pfam 2021 同家族预测 [CARP](https://github.com/microsoft/protein-sequence-models)
    [[163](#bib.bib163)] Seq CNN 1280 640M 掩码语言建模 UniRef50 2022 [AminoBERT](https://github.com/aqlaboratory/rgn2)
    [[22](#bib.bib22)] Seq Transformer 3072 - 掩码语言建模 UniParc 2022 [OmegaPLM](https://github.com/HeliXonProtein/OmegaFold)
    [[164](#bib.bib164)] Seq GAU [[165](#bib.bib165)] 1280 670M 掩码语言建模 UniRef50 2022
    跨越和序列掩码 [ProGen2](https://github.com/salesforce/progen) [[166](#bib.bib166)] Seq
    Transformer 4096 6.4B 掩码语言建模 UniRef90, BFD30, BFD90 2022 [ProtGPT2](https://huggingface.co/nferruz/ProtGPT2)
    [[167](#bib.bib167)] Seq GPT-2 [[168](#bib.bib168)] 1280 738M 下一氨基酸预测 UniRef50
    2022 [RITA](https://github.com/lightonai/RITA) [[169](#bib.bib169)] Seq GPT-3
    [[55](#bib.bib55)] 2048 1.2B 下一氨基酸预测 UniRef100 2022 [ESM-2](https://github.com/facebookresearch/esm)
    [[13](#bib.bib13)] Seq Transformer 5120 15B 掩码语言建模 UniRef50 2022 xTrimoPGLM [[170](#bib.bib170)]
    Seq Transformer 10240 100B 掩码语言建模 Uniref90 2023 跨越标记预测 ColAbFoldDB [ReprogBERT](https://github.com/IBM/ReprogBERT)
    [[171](#bib.bib171)] Seq BERT 768 110M 掩码语言建模 英文维基百科 2023 BookCorpus [PoET](https://github.com/OpenProteinAI/PoET)
    [[172](#bib.bib172)] MSA Transformer 1024 201M 下一氨基酸预测 UniRef50 2023 [CELL-E2](https://bohuanglab.github.io/CELL-E_2/)
    [[173](#bib.bib173)] Seq Transformer 480 35M 掩码语言建模 人类蛋白质图谱 [[174](#bib.bib174)]
    2023 GraphMS [[187](#bib.bib187)] 结构 GCN - - 多视图对比 NeoDTI [[188](#bib.bib188)]
    2021 CRL [[189](#bib.bib189)] 结构 IEConv [[186](#bib.bib186)] 2048 36.6M 多视图对比
    PDB 2022 STEPS [[190](#bib.bib190)] 结构 GNN 1280 - 距离和二面角预测 PDB 2022 • 示例报告了其公共系列中最大的模型。Seq：序列，Struct：结构。$\#$嵌入表示嵌入维度；$\#$参数，网络的参数数量；M，百万；B，十亿。一些模型链接到GitHub仓库。
- en: MSA Sequences
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MSA序列
- en: 'As depicted in Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Protein and Language ‣ 4 Protein
    Foundations ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")(a),
    the inference of residue contact maps from MSA sequences has been a long-standing
    practice in computational biology [[176](#bib.bib176)]. This approach has been
    relied upon in the early stages, as large protein LMs were not developed to extract
    implicit coevolutionary information from individual sequences. It is evident that
    additional information, such as MSAs, can enhance protein embeddings. MSA Transformer [[160](#bib.bib160)]
    extends transformer-based LMs to handle sets of sequences as inputs by employing
    alternating attention over rows and columns, as illustrated in Figure [13](#S5.F13
    "Figure 13 ‣ Single Sequences ‣ 5.1 Protein Language Models ‣ 5 Deep Learning
    based Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")(b). The internal representations of MSA Transformer enable high-quality
    unsupervised structure learning with significantly fewer parameters compared to
    contemporary protein LMs. Additionally, AF2 [[16](#bib.bib16)] has leveraged row-wise
    and column-wise self-attentions to capture rich information within MSA representations.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [7](#S4.F7 "Figure 7 ‣ 4.1 Protein and Language ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey") 所示，从MSA序列推断残基接触图一直是计算生物学中长期实践的方法 [[176](#bib.bib176)]。在早期阶段，由于没有开发大型蛋白LM从单个序列中提取隐含的共进化信息，这种方法一直被依赖。可以明显看出，额外信息，如MSA，可以增强蛋白质嵌入。MSA
    Transformer [[160](#bib.bib160)] 将基于transformer的LM扩展为处理一组序列作为输入，通过交替关注行和列进行，如图 [13](#S5.F13
    "Figure 13 ‣ Single Sequences ‣ 5.1 Protein Language Models ‣ 5 Deep Learning
    based Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")(b) 所示。与当代蛋白质LM相比，MSA Transformer的内部表示使得高质量的无监督结构学习只需较少的参数。此外，AF2 [[16](#bib.bib16)]
    已利用行向和列向的自注意力机制捕捉MSA表示中丰富的信息。'
- en: 5.2 Protein Structure Models
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 蛋白质结构模型
- en: Protein structures contain extremely valuable information that can be used for
    understanding biological processes and facilitating important interventions like
    the development of drugs based on structural characteristics or targeted genetic
    modifications [[16](#bib.bib16)]. In addition to sequence-based encoders, structure-based
    encoders have been developed to leverage the 3D structural information of proteins.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质结构包含极其宝贵的信息，可用于理解生物过程，并促进重要干预措施，例如基于结构特征开发的药物或针对基因的有针对性修饰 [[16](#bib.bib16)]。除了基于序列的编码器外，还开发了基于结构的编码器，以利用蛋白质的3D结构信息。
- en: '![Refer to caption](img/4588ddbe8fdb49342b0a126f550776b7.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4588ddbe8fdb49342b0a126f550776b7.png)'
- en: (a) Distance map
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 距离图
- en: '![Refer to caption](img/6d0135025320e3775c3e8aacc93a0f20.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6d0135025320e3775c3e8aacc93a0f20.png)'
- en: (b) Contact map
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 接触图
- en: 'Figure 14: An example of protein distance and contact maps, visualized by MapPred [[178](#bib.bib178)].'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '图14: 由MapPred [[178](#bib.bib178)] 可视化的蛋白质距离和接触图示例。'
- en: Invariant Geometries
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不变几何
- en: 'Firstly, we introduce the modeling methods of distance and contact maps. A
    distance map of proteins, also referred to as a residue-residue distance map,
    is a graphical representation that shows the distances between pairs of amino
    acid residues in a protein structure. It provides valuable information about the
    spatial proximity between residues within the protein. Typically, the distance
    map is calculated based on the positions of the $\mathrm{C}_{\alpha}$ atoms, denoted
    as $d_{ij,\mathrm{C}\alpha}={P}_{i,\mathrm{C}\alpha}-{P}_{j,\mathrm{C}\alpha}$,
    where ${P}_{i,\mathrm{C}\alpha}$ denotes the 3D position of $\mathrm{C}_{\alpha}$
    in the $i$-th residue. From a distance map, a contact map can be derived by assigning
    a value to each element representing the distance between two atoms. In practice,
    two residues (or amino acids) $i$ and $j$ of the same protein are considered in
    contact if their Euclidean distance is below a specific threshold value, often
    set at 8 Å [[177](#bib.bib177)]. An example of a distance map and contact map
    can be seen in Figure [14](#S5.F14 "Figure 14 ‣ 5.2 Protein Structure Models ‣
    5 Deep Learning based Protein Models ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey"). CNNs, such as ResNet, are commonly employed to process
    these feature maps [[179](#bib.bib179)], generating more accurate maps [[180](#bib.bib180)]
    or protein embeddings. Additionally, 3D CNNs have been utilized to identify interaction
    patterns on protein surfaces [[181](#bib.bib181)]. It is worth noting that these
    feature maps are often used in conjunction with protein sequences to provide supplementary
    information. For instance, ProSE [[46](#bib.bib46)] incorporates structural supervision
    through residue-residue contact loss, along with sequence masking loss, to better
    capture the semantic organization of proteins, leading to improved protein function
    prediction. Furthermore, the recently proposed Struct2GO model [[182](#bib.bib182)]
    transforms the protein’s 3D structure into a protein contact graph and utilizes
    amino acid-level embeddings as node representations, enhancing the accuracy of
    protein function prediction.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，我们介绍距离图和接触图的建模方法。蛋白质的距离图，也称为残基-残基距离图，是一种图形表示，显示了蛋白质结构中一对氨基酸残基之间的距离。它提供了有关蛋白质内残基空间接近度的宝贵信息。通常，距离图是基于$\mathrm{C}_{\alpha}$原子的位置信息计算的，表示为$d_{ij,\mathrm{C}\alpha}={P}_{i,\mathrm{C}\alpha}-{P}_{j,\mathrm{C}\alpha}$，其中${P}_{i,\mathrm{C}\alpha}$表示第$i$个残基中$\mathrm{C}_{\alpha}$的三维位置。从距离图中，可以通过为每个元素分配一个值来表示两个原子之间的距离，从而推导出接触图。在实际操作中，如果同一蛋白质的两个残基（或氨基酸）$i$和$j$的欧几里得距离低于特定的阈值，通常设置为8
    Å [[177](#bib.bib177)]，则认为它们在接触中。距离图和接触图的示例可以在图 [14](#S5.F14 "Figure 14 ‣ 5.2
    Protein Structure Models ‣ 5 Deep Learning based Protein Models ‣ Advances of
    Deep Learning in Protein Science: A Comprehensive Survey")中看到。卷积神经网络（CNN），如ResNet，通常用于处理这些特征图 [[179](#bib.bib179)]，生成更准确的图 [[180](#bib.bib180)]或蛋白质嵌入。此外，3D
    CNN已被用于识别蛋白质表面的交互模式 [[181](#bib.bib181)]。值得注意的是，这些特征图通常与蛋白质序列结合使用，以提供补充信息。例如，ProSE [[46](#bib.bib46)]通过残基-残基接触损失以及序列掩码损失来整合结构监督，从而更好地捕捉蛋白质的语义组织，提升蛋白质功能预测的准确性。此外，最近提出的Struct2GO模型 [[182](#bib.bib182)]将蛋白质的三维结构转换为蛋白质接触图，并利用氨基酸级别的嵌入作为节点表示，从而提高了蛋白质功能预测的准确性。'
- en: 'In addition to distance and contact maps, there are other invariant features
    that can be used to capture the geometric properties of proteins. These include
    backbone torsion angles, trRosetta interresidue geometries, and Euler angles,
    etc., which are described in Subsection [4.4](#S4.SS4 "4.4 Protein Structure Geometries
    ‣ 4 Protein Foundations ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey"). The simple approach to achieve SE(3) group symmetry in molecule geometry
    is through invariant modeling. Invariant modeling focuses on capturing only the
    invariant features or type-0 features [[131](#bib.bib131)]. These type-0 features
    remain unchanged regardless of rotation and translation. To model the geometric
    relationships between amino acids, GNNs are commonly employed, including methods
    such as GCN [[82](#bib.bib82), [183](#bib.bib183), [184](#bib.bib184)], GAT [[83](#bib.bib83)],
    and GraphSAGE [[84](#bib.bib84)]. To represent the protein structures as graphs,
    the 1D and 2D features are used as node and edge features, such as the edge features
    $\bm{e}_{ij}^{(1)}$ and $\bm{e}_{ij}^{(2)}$.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 除了距离和接触图，还有其他不变特征可以用于捕捉蛋白质的几何属性。这些特征包括主链扭转角、trRosetta残基间几何以及欧拉角等，详细描述见第[4.4](#S4.SS4
    "4.4 蛋白质结构几何 ‣ 4 蛋白质基础 ‣ 深度学习在蛋白质科学中的进展：综合调查")节。实现分子几何中的SE(3)群对称性的简单方法是通过不变建模。不变建模专注于捕捉仅不变特征或类型0特征[[131](#bib.bib131)]。这些类型0特征在旋转和平移下保持不变。为了建模氨基酸之间的几何关系，通常采用GNN，包括GCN
    [[82](#bib.bib82), [183](#bib.bib183), [184](#bib.bib184)]、GAT [[83](#bib.bib83)]和GraphSAGE
    [[84](#bib.bib84)]等方法。为了将蛋白质结构表示为图，1D和2D特征被用作节点和边特征，如边特征$\bm{e}_{ij}^{(1)}$和$\bm{e}_{ij}^{(2)}$。
- en: 'In the field of protein research, there are three common graph construction
    methods: sequential graph, radius graph, and $k$-Nearest Neighbors ($k$NN) graph [[15](#bib.bib15)].
    Here, given a graph $G=(\mathcal{V},\mathcal{E},X,E)$, with vertex and edge sets
    $\mathcal{V}=\{v_{i}\}_{i=1,\ldots,n}$ and $\mathcal{E}=\left\{\varepsilon_{ij}\right\}_{i,j=1,\ldots,n}$,
    the sequential graph is defined based on the sequence. If $\left\|i-j\right\|<l_{seq}$,
    an edge $\varepsilon_{ij}$ exists, where $l_{seq}$ is a hyperparameter. For the
    radius graph, an edge exists between nodes $v_{i}$ and $v_{j}$ if $\left\|d_{ij,\mathrm{C}\alpha}\right\|<r$,
    where $r$ is a pre-defined radius. The $k$NN graph connects each node to its $k$
    closest neighbors, with $k$ commonly set to 30 [[119](#bib.bib119), [185](#bib.bib185)].
    An illustration of these graph construction methods is shown in Figure [15](#S5.F15
    "Figure 15 ‣ Invariant Geometries ‣ 5.2 Protein Structure Models ‣ 5 Deep Learning
    based Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey").'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在蛋白质研究领域，有三种常见的图构建方法：顺序图、半径图和$k$-最近邻（$k$NN）图[[15](#bib.bib15)]。这里，给定图$G=(\mathcal{V},\mathcal{E},X,E)$，其中顶点和边集分别为$\mathcal{V}=\{v_{i}\}_{i=1,\ldots,n}$和$\mathcal{E}=\left\{\varepsilon_{ij}\right\}_{i,j=1,\ldots,n}$，顺序图是基于序列定义的。如果$\left\|i-j\right\|<l_{seq}$，则存在一条边$\varepsilon_{ij}$，其中$l_{seq}$是一个超参数。对于半径图，如果$\left\|d_{ij,\mathrm{C}\alpha}\right\|<r$，则节点$v_{i}$和$v_{j}$之间存在一条边，其中$r$是预定义的半径。$k$NN图将每个节点连接到其$k$个最近邻，$k$通常设置为30[[119](#bib.bib119),
    [185](#bib.bib185)]。这些图构建方法的示意图见图[15](#S5.F15 "图15 ‣ 不变几何 ‣ 5.2 蛋白质结构模型 ‣ 5 基于深度学习的蛋白质模型
    ‣ 深度学习在蛋白质科学中的进展：综合调查")。
- en: '![Refer to caption](img/ceda8d30f91b137a44fa47a3bf838b1c.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ceda8d30f91b137a44fa47a3bf838b1c.png)'
- en: 'Figure 15: Relational protein residue graphs with sequential edges, radius
    edges, and $k$NN edges for residue $i$.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：具有顺序边、半径边和$k$NN边的关系蛋白质残基图。
- en: 'To leverage these invariant geometric features effectively, Hermosilla et al. [[186](#bib.bib186)]
    introduced protein convolution and pooling techniques to capture the primary,
    secondary, and tertiary structures of proteins by incorporating intrinsic and
    extrinsic distances between nodes and atoms. Furthermore, Wang et al. [[116](#bib.bib116)]
    introduced complete geometric representations and a complete message passing scheme
    that covers protein geometries at the amino acid, backbone, and all-atom levels.
    In Subsection [4.5](#S4.SS5 "4.5 Structure Properties ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey"), we provide
    definitions for complete geometries, which generally refer to 3D positions that
    can generate geometric representations and can be recovered from them. Such representations
    are considered complete. By incorporating complete geometric representations into
    the commonly-used message passing framework (Eq. [11](#S3.E11 "In 3.4 Graph Neural
    Networks ‣ 3 Basic Neural Networks in Protein Modeling ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey")), a complete message passing scheme
    can be achieved [[134](#bib.bib134), [135](#bib.bib135), [116](#bib.bib116)].'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效利用这些不变几何特征，Hermosilla 等人[[186](#bib.bib186)] 引入了蛋白质卷积和池化技术，通过结合节点和原子之间的内在和外在距离来捕捉蛋白质的一级、二级和三级结构。此外，Wang
    等人[[116](#bib.bib116)] 引入了完整的几何表示和完整的信息传递方案，覆盖了氨基酸、主链和全原子级别的蛋白质几何形状。在子节[4.5](#S4.SS5
    "4.5 结构属性 ‣ 4 蛋白质基础 ‣ 深度学习在蛋白质科学中的进展：全面调查")中，我们提供了完整几何形状的定义，这通常指可以生成几何表示并可以从中恢复的
    3D 位置。这些表示被认为是完整的。通过将完整的几何表示纳入常用的信息传递框架（方程[11](#S3.E11 "在 3.4 图神经网络 ‣ 3 蛋白质建模中的基本神经网络
    ‣ 深度学习在蛋白质科学中的进展：全面调查")），可以实现完整的信息传递方案[[134](#bib.bib134), [135](#bib.bib135),
    [116](#bib.bib116)]。
- en: Equivariant Geometries
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 等变几何
- en: 'Invariant modeling only captures the type-0 features, although protein can
    be represented as a graph naturally, it remains under-explored mainly due to the
    significant challenges. For example, it is challenging to extract and preserve
    multi-level rotation and translation equivariant information and effectively leverage
    the input spatial representations to capture complex geometries across the spatial
    dimension. Thus higher-order particles include type-1 features like coordinates
    and forces in molecular conformation are important to be considered [[131](#bib.bib131)].
    An equivariant message passing paradigm on proteins embeded into existing GNNs
    has been developed [[191](#bib.bib191)], like GBPNet [[192](#bib.bib192)], showing
    superior versatility in maintaining equivariance. AtomRefine [[193](#bib.bib193)]
    uses a SE(3)-equivariant graph transformer network to refine protein structures,
    where each block in the SE(3) transformer network consists of one equivariant
    GCN attention block. Specifically, jing et al. [[128](#bib.bib128)] introduce
    geometric vector perceptrons (GVP) and operate directly on both scalar and vector
    features under a global coordinate system (refer to Figure [16](#S5.F16 "Figure
    16 ‣ Equivariant Geometries ‣ 5.2 Protein Structure Models ‣ 5 Deep Learning based
    Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 不变建模仅捕捉类型 0 特征，尽管蛋白质可以自然地表示为图形，但由于面临显著挑战，它仍未被充分探索。例如，提取和保存多级旋转和位移等变信息以及有效利用输入空间表示以捕捉空间维度上的复杂几何形状是具有挑战性的。因此，高阶粒子包括像坐标和分子构象中的力等类型
    1 特征是重要的[[131](#bib.bib131)]。一种将等变信息传递范式嵌入现有 GNNs 的方法已经被开发[[191](#bib.bib191)]，如
    GBPNet [[192](#bib.bib192)]，展现了在保持等变性方面的卓越多功能性。AtomRefine[[193](#bib.bib193)]
    使用 SE(3) 等变图变换网络来细化蛋白质结构，其中 SE(3) 变换网络中的每个块都由一个等变的 GCN 注意力块组成。具体来说，jing 等人[[128](#bib.bib128)]
    引入了几何向量感知机（GVP），并在全局坐标系统下直接操作标量和向量特征（参见图[16](#S5.F16 "图 16 ‣ 等变几何 ‣ 5.2 蛋白质结构模型
    ‣ 5 基于深度学习的蛋白质模型 ‣ 深度学习在蛋白质科学中的进展：全面调查")）。
- en: 'We have introduced the definitions of invariance and equivariance in Subsection [4.5](#S4.SS5
    "4.5 Structure Properties ‣ 4 Protein Foundations ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey"). Here, we introduce the mechanisms
    of equivariant GNNs (EGNNs [[194](#bib.bib194)]) applied in protein. We consider
    a 3D graph as $G=(\mathcal{V},\mathcal{E},X,E)$, $P_{i,\mathrm{C}_{\alpha}}$ is
    the position of $\mathrm{C}_{\alpha}$ in node $v_{i}$, coordinate embeddings $\mathcal{P}_{\mathrm{C}_{\alpha}}^{(l)}=\{P_{i,\mathrm{C}_{\alpha}}^{(l)}\}_{i=1,\ldots,n}$.
    The node and edge embeddings are $H^{(l)}=[\bm{h}_{i}^{(l)}]_{i=1,\ldots,n}$ and
    $E=[\bm{e}_{ij}]_{i,j=1,\ldots,n}$, $H^{(0)}=X$, the following equations can define
    the $l$-th equivariant message passing layer:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在小节[4.5](#S4.SS5 "4.5 结构性质 ‣ 4 蛋白质基础 ‣ 蛋白质科学中的深度学习进展：综合调查")中介绍了不变性和等变性的定义。在这里，我们介绍了应用于蛋白质的等变GNN（EGNNs [[194](#bib.bib194)])的机制。我们将3D图表示为
    $G=(\mathcal{V},\mathcal{E},X,E)$，$P_{i,\mathrm{C}_{\alpha}}$ 是节点 $v_{i}$ 中 $\mathrm{C}_{\alpha}$
    的位置，坐标嵌入 $\mathcal{P}_{\mathrm{C}_{\alpha}}^{(l)}=\{P_{i,\mathrm{C}_{\alpha}}^{(l)}\}_{i=1,\ldots,n}$。节点和边的嵌入分别为
    $H^{(l)}=[\bm{h}_{i}^{(l)}]_{i=1,\ldots,n}$ 和 $E=[\bm{e}_{ij}]_{i,j=1,\ldots,n}$，$H^{(0)}=X$，以下方程可以定义第
    $l$ 层等变信息传递层：
- en: '|  | $\displaystyle\bm{m}_{ij}$ | $\displaystyle=\psi_{e}\left(\bm{h}_{i}^{l},\bm{h}_{j}^{l},\left\&#124;P_{i,\mathrm{C}_{\alpha}}^{l}-P_{j,\mathrm{C}_{\alpha}}^{l}\right\&#124;,e_{ij}\right)$
    |  | (20) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{m}_{ij}$ | $\displaystyle=\psi_{e}\left(\bm{h}_{i}^{l},\bm{h}_{j}^{l},\left\|P_{i,\mathrm{C}_{\alpha}}^{l}-P_{j,\mathrm{C}_{\alpha}}^{l}\right\|,e_{ij}\right)$
    |  | (20) |'
- en: '|  | $\displaystyle P_{i,\mathrm{C}_{\alpha}}^{l+1}$ | $\displaystyle=P_{i,\mathrm{C}_{\alpha}}^{l}+C\sum_{v_{j}\in\mathcal{N}(v_{i})}\left(P_{i,\mathrm{C}_{\alpha}}^{l}-P_{j,\mathrm{C}_{\alpha}}^{l}\right)\psi_{x}\left(\bm{m}_{ij}\right)$
    |  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P_{i,\mathrm{C}_{\alpha}}^{l+1}$ | $\displaystyle=P_{i,\mathrm{C}_{\alpha}}^{l}+C\sum_{v_{j}\in\mathcal{N}(v_{i})}\left(P_{i,\mathrm{C}_{\alpha}}^{l}-P_{j,\mathrm{C}_{\alpha}}^{l}\right)\psi_{x}\left(\bm{m}_{ij}\right)$
    |  |'
- en: '|  | $\displaystyle\bm{m}_{i}$ | $\displaystyle=\sum_{v_{j}\in\mathcal{N}(v_{i})}\bm{m}_{ij}$
    |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{m}_{i}$ | $\displaystyle=\sum_{v_{j}\in\mathcal{N}(v_{i})}\bm{m}_{ij}$
    |  |'
- en: '|  | $\displaystyle\bm{h}_{i}^{l+1}$ | $\displaystyle=\phi\left(\bm{h}_{i}^{l},\bm{m}_{i}\right)$
    |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{h}_{i}^{l+1}$ | $\displaystyle=\phi\left(\bm{h}_{i}^{l},\bm{m}_{i}\right)$
    |  |'
- en: here, $C$ is a constant number, $\psi_{e}$ and $\psi_{x}$ are the message functions,
    and $\phi$ is the update function. The coordinate embeddings $\mathcal{P}_{\mathrm{C}_{\alpha}}^{(l)}$
    are updated by the weighted sum of all relative neighbors’ differences $(P_{i,\mathrm{C}_{\alpha}}^{l}-P_{j,\mathrm{C}_{\alpha}}^{l})$.
    The coordinate embeddings are also used to update the invariant node embeddings
    by the $l^{2}$-norm ($\left\|\cdot\right\|$). These operations maintain the equivariance
    in GNNs.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$C$ 是一个常数，$\psi_{e}$ 和 $\psi_{x}$ 是消息函数，$\phi$ 是更新函数。坐标嵌入 $\mathcal{P}_{\mathrm{C}_{\alpha}}^{(l)}$
    通过所有相对邻居差异 $(P_{i,\mathrm{C}_{\alpha}}^{l}-P_{j,\mathrm{C}_{\alpha}}^{l})$ 的加权和进行更新。坐标嵌入也用于通过
    $l^{2}$-范数 ($\left\|\cdot\right\|$) 更新不变节点嵌入。这些操作保持了GNN中的等变性。
- en: '![Refer to caption](img/4edb28d24345292a17d5b226ed46e034.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4edb28d24345292a17d5b226ed46e034.png)'
- en: 'Figure 16: Schematic of the geometric vector perceptron in GVP-GNN [[128](#bib.bib128)].
    FC: The linear weight in a fully connected layer.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：GVP-GNN 中几何向量感知器的示意图 [[128](#bib.bib128)]。FC：全连接层中的线性权重。
- en: 'Table 3: List of representative pre-trained protein multimodal models'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：代表性预训练蛋白质多模态模型列表
- en: 'Model and Repository Input Network $\#$Embedding $\#$Param. Pretext Task Pre-training
    Dataset Year [SSA](https://github.com/tbepler/protein-sequence-embedding-iclr2019) [[195](#bib.bib195)]
    Seq, Struct BiLSTM 100, 512 - Contact and Similarity Prediction Pfam, SCOP 2019
    [LM-GVP](https://github.com/aws-samples/lm-gvp) [[196](#bib.bib196)] Seq, Struct
    ProtBert, GVP 1024 420M - UniRef100 2021 [DeepFRI](https://github.com/flatironinstitute/DeepFRI) [[197](#bib.bib197)]
    Seq, Struct LSTM, GCN 512 6.2M GO Term Prediction Pfam 2021 HJRSS [[198](#bib.bib198)]
    Seq, Struct SE(3) Transformer 128 16M Masked Language and Graph Modeling trRosetta2 [[199](#bib.bib199)]
    2021 [GraSR](https://github.com/chunqiux/GraSR) [[200](#bib.bib200)] Seq, Struct
    LSTM, GCN 32 - Momentum Contrast[[202](#bib.bib202)] SCOPe 2022 [CPAC](https://github.com/Shen-Lab/CPAC) [[203](#bib.bib203)]
    Seq, Struct RNN, GAT 128 - Masked Language and Graph Modeling Pfam 2022 [MIF-ST](https://github.com/microsoft/protein-sequence-models) [[204](#bib.bib204)]
    Seq, Struct CNN, GNN 256 640M Masked Language Modeling UniRef50 2022 PeTriBERT [[293](#bib.bib293)]
    Seq, Struct BERT 3072 40M Next Amino Acid Prediction AlphaFoldDB 2022 [GearNet](https://github.com/DeepGraphLearning/GearNet) [[15](#bib.bib15)]
    Seq, Struct GNN 512 42M Distance, Angle and Residue Type Prediction AlphaFoldDB
    2022 Multiview Contrast [ESM-GearNet](https://github.com/DeepGraphLearning/SiamDiff) [[205](#bib.bib205)]
    Seq, Struct ESM, GearNet 512 692M Distance, Angle and Residue Type Prediction
    AlphaFoldDB 2023 Multiview Contrast, SiamDiff [[206](#bib.bib206)] [SaProt](https://github.com/SaProt/SaProt) [[207](#bib.bib207)]
    Seq, Struct ESM-2 1280 650M Masked Language Modeling AlphaFoldDB, PDB 2023 [ProGen](https://github.com/lucidrains/progen) [[208](#bib.bib208)]
    Seq, Func Transformer 1028 1.2B Next Amino Acid Prediction Uniparc, UniProtKB,
    Swiss-Prot 2020 Pfam, TrEMBL, NCBI [[209](#bib.bib209)] [ProteinBERT](https://github.com/nadavbra/protein_bert) [[210](#bib.bib210)]
    Seq, Func Transformer 512 16M Masked Language Modeling UniRef90 2021 [OntoProtein](https://github.com/zjunlp/OntoProtein) [[10](#bib.bib10)]
    Seq, Func ProtBert, BERT 1024 - Masked Language Modeling ProteinKG25 [[10](#bib.bib10)]
    2022 Embedding Contrast [KeAP](https://github.com/RL4M/KeAP) [[211](#bib.bib211)]
    Seq, Func BERT, PubMedBERT [[213](#bib.bib213)] 1024 520M Masked Language Modeling
    ProteinKG25 2023 [ProtST](https://github.com/DeepGraphLearning/ProtST) [[136](#bib.bib136)]
    Seq, Func ProtBert, ESM, PubMedBERT 1024 750M Masked Language Modeling ProtDescribe [[136](#bib.bib136)]
    2023 [MASSA](https://github.com/SIAT-code/MASSA) [[114](#bib.bib114)] Seq, Struct
    ESM-MSA, GVP-GNN - - Masked Language Modeling UniProtKB, Swiss-Prot 2023 Func
    Transformer, GraphGO AlphaFoldDB, RCSB PDB [[214](#bib.bib214)] ProteinINR [[212](#bib.bib212)]
    Seq, Struct ESM-1b, GearNet - - Multiview Contrast 20 Species, 2024 Surface Transformer
    Swiss-Prot • Examples report the largest model of their public series. Seq: sequence,
    Struct: Structure, Func: Function. $\#$Embedding means the dimension of embeddings;
    $\#$Param., the number of parameters of network; M, millions; B, billions. Some
    models are linked with the GitHub repositories.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和库输入网络 $\#$嵌入 $\#$参数 预训练任务 预训练数据集 年份 [SSA](https://github.com/tbepler/protein-sequence-embedding-iclr2019)
    [[195](#bib.bib195)] 序列, 结构 BiLSTM 100, 512 - 接触和相似性预测 Pfam, SCOP 2019 [LM-GVP](https://github.com/aws-samples/lm-gvp)
    [[196](#bib.bib196)] 序列, 结构 ProtBert, GVP 1024 420M - UniRef100 2021 [DeepFRI](https://github.com/flatironinstitute/DeepFRI)
    [[197](#bib.bib197)] 序列, 结构 LSTM, GCN 512 6.2M GO 术语预测 Pfam 2021 HJRSS [[198](#bib.bib198)]
    序列, 结构 SE(3) Transformer 128 16M 掩蔽语言和图建模 trRosetta2 [[199](#bib.bib199)] 2021
    [GraSR](https://github.com/chunqiux/GraSR) [[200](#bib.bib200)] 序列, 结构 LSTM, GCN
    32 - 动量对比 [[202](#bib.bib202)] SCOPe 2022 [CPAC](https://github.com/Shen-Lab/CPAC)
    [[203](#bib.bib203)] 序列, 结构 RNN, GAT 128 - 掩蔽语言和图建模 Pfam 2022 [MIF-ST](https://github.com/microsoft/protein-sequence-models)
    [[204](#bib.bib204)] 序列, 结构 CNN, GNN 256 640M 掩蔽语言建模 UniRef50 2022 PeTriBERT [[293](#bib.bib293)]
    序列, 结构 BERT 3072 40M 下一氨基酸预测 AlphaFoldDB 2022 [GearNet](https://github.com/DeepGraphLearning/GearNet)
    [[15](#bib.bib15)] 序列, 结构 GNN 512 42M 距离、角度和残基类型预测 AlphaFoldDB 2022 多视角对比 [ESM-GearNet](https://github.com/DeepGraphLearning/SiamDiff)
    [[205](#bib.bib205)] 序列, 结构 ESM, GearNet 512 692M 距离、角度和残基类型预测 AlphaFoldDB 2023
    多视角对比, SiamDiff [[206](#bib.bib206)] [SaProt](https://github.com/SaProt/SaProt)
    [[207](#bib.bib207)] 序列, 结构 ESM-2 1280 650M 掩蔽语言建模 AlphaFoldDB, PDB 2023 [ProGen](https://github.com/lucidrains/progen)
    [[208](#bib.bib208)] 序列, 功能 Transformer 1028 1.2B 下一氨基酸预测 Uniparc, UniProtKB,
    Swiss-Prot 2020 Pfam, TrEMBL, NCBI [[209](#bib.bib209)] [ProteinBERT](https://github.com/nadavbra/protein_bert)
    [[210](#bib.bib210)] 序列, 功能 Transformer 512 16M 掩蔽语言建模 UniRef90 2021 [OntoProtein](https://github.com/zjunlp/OntoProtein)
    [[10](#bib.bib10)] 序列, 功能 ProtBert, BERT 1024 - 掩蔽语言建模 ProteinKG25 [[10](#bib.bib10)]
    2022 嵌入对比 [KeAP](https://github.com/RL4M/KeAP) [[211](#bib.bib211)] 序列, 功能 BERT,
    PubMedBERT [[213](#bib.bib213)] 1024 520M 掩蔽语言建模 ProteinKG25 2023 [ProtST](https://github.com/DeepGraphLearning/ProtST)
    [[136](#bib.bib136)] 序列, 功能 ProtBert, ESM, PubMedBERT 1024 750M 掩蔽语言建模 ProtDescribe
    [[136](#bib.bib136)] 2023 [MASSA](https://github.com/SIAT-code/MASSA) [[114](#bib.bib114)]
    序列, 结构 ESM-MSA, GVP-GNN - - 掩蔽语言建模 UniProtKB, Swiss-Prot 2023 功能 Transformer,
    GraphGO AlphaFoldDB, RCSB PDB [[214](#bib.bib214)] ProteinINR [[212](#bib.bib212)]
    序列, 结构 ESM-1b, GearNet - - 多视角对比 20 物种, 2024 表面 Transformer Swiss-Prot • 示例报告了其公开系列中最大的模型。
    序列：序列，结构：结构，功能：功能。 $\#$嵌入指的是嵌入的维度；$\#$参数，网络的参数数量；M，百万；B，十亿。一些模型与 GitHub 仓库链接。
- en: 'Table 4: Information of Protein Databases'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：蛋白质数据库信息
- en: Dataset $\#$Proteins Disk Space Description Link UniProtKB/Swiss-Prot [[9](#bib.bib9)]
    500K 0.59GB knowledgebase [https://www.uniprot.org/uniprotkb?query=*](https://www.uniprot.org/uniprotkb?query=*)
    UniProtKB/TrEMBL [[9](#bib.bib9)] 229M 146GB knowledgebase [https://www.uniprot.org/uniprotkb?query=*](https://www.uniprot.org/uniprotkb?query=*)
    UniRef100 [[215](#bib.bib215)] 314M 76.9GB clustered sets of sequences [https://www.uniprot.org/uniref?query=*](https://www.uniprot.org/uniref?query=*)
    UniRef90 [[215](#bib.bib215)] 150M 34GB 90$\%$ identity [https://www.uniprot.org/uniref?query=*](https://www.uniprot.org/uniref?query=*)
    UniRef50 [[215](#bib.bib215)] 53M 10.3GB 50$\%$ identity [https://www.uniprot.org/uniref?query=*](https://www.uniprot.org/uniref?query=*)
    UniParc [[9](#bib.bib9)] 528M 106GB sequence [https://www.uniprot.org/uniparc?query=*](https://www.uniprot.org/uniparc?query=*)
    PDB [[8](#bib.bib8)] 180K 50GB 3D structure [https://www.wwpdb.org/ftp/pdb-ftp-sites](https://www.wwpdb.org/ftp/pdb-ftp-sites)
    CATH4.3 [[216](#bib.bib216)] - 1073MB hierarchical classification [https://www.cathdb.info/](https://www.cathdb.info/)
    BFD [[217](#bib.bib217)] 2500M 272GB sequence profile [https://bfd.mmseqs.com/](https://bfd.mmseqs.com/)
    Pfam [[218](#bib.bib218)] 47M 14.1GB protein families [https://www.ebi.ac.uk/interpro/entry/pfam/](https://www.ebi.ac.uk/interpro/entry/pfam/)
    AlphaFoldDB [[219](#bib.bib219)] 214M 23 TB predicted 3D structures [https://alphafold.ebi.ac.uk/](https://alphafold.ebi.ac.uk/)
    ESM Metagenomic Atlas [[13](#bib.bib13)] 772M - predicted metagenomic protein
    structures [https://esmatlas.com/](https://esmatlas.com/) ColAbFoldDB [[170](#bib.bib170)]
    950M - an amalgamation of various metagenomic databases [https://colabfold.mmseqs.com/](https://colabfold.mmseqs.com/)
    ProteinKG25 [[10](#bib.bib10)] 5.6M 147MB a knowledge graph dataset with GO [https://drive.google.com/file/d/1iTC2-zbvYZCDhWM_wxRufCvV6vvPk8HR](https://drive.google.com/file/d/1iTC2-zbvYZCDhWM_wxRufCvV6vvPk8HR)
    Uniclust30 [[220](#bib.bib220)] - 6.6GB clustered protein sequences [https://uniclust.mmseqs.com/](https://uniclust.mmseqs.com/)
    SCOP [[221](#bib.bib221)] - - structural classification [http://scop.mrc-lmb.cam.ac.uk/](http://scop.mrc-lmb.cam.ac.uk/)
    SCOPe [[222](#bib.bib222)] - 86MB an extended version of SCOP [http://scop.berkeley.edu](http://scop.berkeley.edu)
    OpenProteinSet [[223](#bib.bib223)] 16M - MSAs [https://dagshub.com/datasets/openproteinset/](https://dagshub.com/datasets/openproteinset/)
    • K, thousand; M, million, disk space is in GB or TB (compressed storage as text),
    which is estimated data influenced by the compressed format.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 $\#$蛋白质 磁盘空间 描述 链接 UniProtKB/Swiss-Prot [[9](#bib.bib9)] 500K 0.59GB 知识库
    [https://www.uniprot.org/uniprotkb?query=*](https://www.uniprot.org/uniprotkb?query=*)
    UniProtKB/TrEMBL [[9](#bib.bib9)] 229M 146GB 知识库 [https://www.uniprot.org/uniprotkb?query=*](https://www.uniprot.org/uniprotkb?query=*)
    UniRef100 [[215](#bib.bib215)] 314M 76.9GB 序列簇 [https://www.uniprot.org/uniref?query=*](https://www.uniprot.org/uniref?query=*)
    UniRef90 [[215](#bib.bib215)] 150M 34GB 90$\%$ 同源 [https://www.uniprot.org/uniref?query=*](https://www.uniprot.org/uniref?query=*)
    UniRef50 [[215](#bib.bib215)] 53M 10.3GB 50$\%$ 同源 [https://www.uniprot.org/uniref?query=*](https://www.uniprot.org/uniref?query=*)
    UniParc [[9](#bib.bib9)] 528M 106GB 序列 [https://www.uniprot.org/uniparc?query=*](https://www.uniprot.org/uniparc?query=*)
    PDB [[8](#bib.bib8)] 180K 50GB 3D 结构 [https://www.wwpdb.org/ftp/pdb-ftp-sites](https://www.wwpdb.org/ftp/pdb-ftp-sites)
    CATH4.3 [[216](#bib.bib216)] - 1073MB 层次分类 [https://www.cathdb.info/](https://www.cathdb.info/)
    BFD [[217](#bib.bib217)] 2500M 272GB 序列轮廓 [https://bfd.mmseqs.com/](https://bfd.mmseqs.com/)
    Pfam [[218](#bib.bib218)] 47M 14.1GB 蛋白质家族 [https://www.ebi.ac.uk/interpro/entry/pfam/](https://www.ebi.ac.uk/interpro/entry/pfam/)
    AlphaFoldDB [[219](#bib.bib219)] 214M 23 TB 预测 3D 结构 [https://alphafold.ebi.ac.uk/](https://alphafold.ebi.ac.uk/)
    ESM Metagenomic Atlas [[13](#bib.bib13)] 772M - 预测宏基因组蛋白质结构 [https://esmatlas.com/](https://esmatlas.com/)
    ColAbFoldDB [[170](#bib.bib170)] 950M - 各种宏基因组数据库的融合 [https://colabfold.mmseqs.com/](https://colabfold.mmseqs.com/)
    ProteinKG25 [[10](#bib.bib10)] 5.6M 147MB 带有 GO 的知识图谱数据集 [https://drive.google.com/file/d/1iTC2-zbvYZCDhWM_wxRufCvV6vvPk8HR](https://drive.google.com/file/d/1iTC2-zbvYZCDhWM_wxRufCvV6vvPk8HR)
    Uniclust30 [[220](#bib.bib220)] - 6.6GB 聚类蛋白质序列 [https://uniclust.mmseqs.com/](https://uniclust.mmseqs.com/)
    SCOP [[221](#bib.bib221)] - - 结构分类 [http://scop.mrc-lmb.cam.ac.uk/](http://scop.mrc-lmb.cam.ac.uk/)
    SCOPe [[222](#bib.bib222)] - 86MB SCOP 的扩展版本 [http://scop.berkeley.edu](http://scop.berkeley.edu)
    OpenProteinSet [[223](#bib.bib223)] 16M - 多序列比对 [https://dagshub.com/datasets/openproteinset/](https://dagshub.com/datasets/openproteinset/)
    • K，千；M，百万，磁盘空间以 GB 或 TB 计（以文本形式压缩存储），数据估算受压缩格式的影响。
- en: 5.3 Protein Multimodal Methods
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 蛋白质多模态方法
- en: 'As mentioned in Subsection [4.3](#S4.SS3 "4.3 Motifs, Regions, and Domains
    ‣ 4 Protein Foundations ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey"), protein data encompasses various types of information, such as sequences,
    structures, GO annotations, motifs, regions, domains, and more. To gain a comprehensive
    understanding of proteins, it is crucial to consider and integrate these multimodal
    sources of information.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '如小节 [4.3](#S4.SS3 "4.3 Motifs, Regions, and Domains ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey") 所述，蛋白质数据包含各种类型的信息，如序列、结构、GO
    注释、基序、区域、结构域等。为了全面了解蛋白质，考虑和整合这些多模态信息源至关重要。'
- en: Sequence-structure Modeling
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 序列-结构建模
- en: In recent years, there has been a growing focus on sequence-structure co-modeling
    methods, which aim to capture the intricate relationships between protein sequences
    and structures. Rather than treating protein sequences and structures as separate
    entities, these methods leverage the complementary information from both domains
    to improve modeling performance.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，对序列-结构共建模方法的关注不断增加，这些方法旨在捕捉蛋白质序列和结构之间的复杂关系。这些方法不是将蛋白质序列和结构视为独立的实体，而是利用来自这两个领域的互补信息来提高建模性能。
- en: One prevailing approach involves using pre-trained protein LMs, such as ESM-1b
    and ProtTrans, to obtain embeddings for sequences. For example, SPOT-1D-LM [[224](#bib.bib224),
    [225](#bib.bib225)] and BIB [[226](#bib.bib226)] utilize pre-trained LMs to generate
    embeddings for contact map prediction and biological sequence design. To incorporate
    structural information into protein LMs, LM-GVP [[196](#bib.bib196)] proposes
    a novel fine-tuning procedure that explicitly injects inductive bias from complex
    structure information using GVP. This method enhances the representation capability
    of protein LMs by considering both sequence and structure information, leading
    to improved performance in downstream tasks. Another approach, GearNet [[15](#bib.bib15)],
    simultaneously encodes sequential and spatial information by incorporating different
    types of sequential or structural edges. It performs node-level and edge-level
    message passing, enabling it to capture both local and global dependencies in
    protein sequences and structures. This comprehensive modeling approach has shown
    promising results in various applications. Foldseek [[227](#bib.bib227)] employs
    a VQ-VAE [[228](#bib.bib228)] model to encode protein structures into informative
    tokens. These tokens combine residue and 3D geometric features, effectively representing
    both primary and tertiary structures. By representing protein structures as a
    sequence of these novel tokens, Foldseek seamlessly integrates the foundational
    models like BERT and GPT to process protein sequences and structures simultaneously,
    SaProt [[207](#bib.bib207)] is also an example of such integration, where it combines
    these tokens using general-purpose protein LMs. Without using the structure information
    as input, Bepler and Berger [[46](#bib.bib46)] carry out multi-task with structural
    supervision, leading to an even better-organized embedding space compared with
    a single task.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的方法是使用预训练的蛋白质语言模型（LMs），例如 ESM-1b 和 ProtTrans，来获取序列的嵌入。例如，SPOT-1D-LM [[224](#bib.bib224),
    [225](#bib.bib225)] 和 BIB [[226](#bib.bib226)] 使用预训练的 LMs 生成用于接触图预测和生物序列设计的嵌入。为了将结构信息纳入蛋白质
    LMs，LM-GVP [[196](#bib.bib196)] 提出了一个新颖的微调程序，该程序通过 GVP 显式地注入来自复杂结构信息的归纳偏差。这种方法通过同时考虑序列和结构信息，增强了蛋白质
    LMs 的表征能力，从而在下游任务中提升了性能。另一种方法，GearNet [[15](#bib.bib15)]，通过结合不同类型的序列或结构边缘，来同时编码顺序和空间信息。它执行节点级和边缘级的消息传递，使其能够捕捉蛋白质序列和结构中的局部和全局依赖关系。这种全面建模方法在各种应用中显示出良好的结果。Foldseek [[227](#bib.bib227)]
    使用 VQ-VAE [[228](#bib.bib228)] 模型将蛋白质结构编码为信息性标记。这些标记结合了残基和三维几何特征，有效地表示了主链和三级结构。通过将蛋白质结构表示为这些新型标记的序列，Foldseek
    无缝地将像 BERT 和 GPT 这样的基础模型集成在一起，以同时处理蛋白质序列和结构，SaProt [[207](#bib.bib207)] 也是这种集成的一个例子，它结合了这些标记，使用通用的蛋白质
    LMs。在没有使用结构信息作为输入的情况下，Bepler 和 Berger [[46](#bib.bib46)] 进行了结构监督的多任务学习，与单任务相比，获得了更为优化的嵌入空间。
- en: Sequence-function Modeling
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 序列-功能建模
- en: 'Protein Sequence-function Modeling is a research field dedicated to comprehending
    the intricate relationships between protein sequences and their functional properties.
    GO annotations provide valuable structured information about protein functions,
    enabling researchers to systematically analyze and compare protein functions across
    different species and experimental studies [[229](#bib.bib229)]. The function
    information is typically derived from prior biological knowledge, which can be
    easily incorporated into sentences, as depicted in Figure [12](#S4.F12 "Figure
    12 ‣ Complete Geometries ‣ 4.5 Structure Properties ‣ 4 Protein Foundations ‣
    Advances of Deep Learning in Protein Science: A Comprehensive Survey"). Consequently,
    the study of protein sequences and functions often goes hand in hand, with LMs
    being commonly employed. One notable model in this domain is ProteinBERT [[210](#bib.bib210)],
    which undergoes pre-training on protein sequences and GO annotations using two
    interconnected BERT-like encoders. By leveraging large-scale biology knowledge
    datasets, ProteinKG25 [[10](#bib.bib10)], OntoProtein [[10](#bib.bib10)], on the
    other hand, focuses on reconstructing masked amino acids while simultaneously
    minimizing the embedding distance between contextual representations of proteins
    and associated knowledge terms. In comparison, KeAP [[211](#bib.bib211)] aims
    to explore the relationships between proteins and knowledge at a more granular
    level than OntoProtein.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '蛋白质序列-功能建模是一个致力于理解蛋白质序列与其功能属性之间复杂关系的研究领域。GO 注释提供了关于蛋白质功能的宝贵结构化信息，使研究人员能够系统地分析和比较不同物种和实验研究中的蛋白质功能[[229](#bib.bib229)]。功能信息通常源自先前的生物学知识，这些知识可以轻松地融入到句子中，如图[12](#S4.F12
    "Figure 12 ‣ Complete Geometries ‣ 4.5 Structure Properties ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")所示。因此，蛋白质序列和功能的研究通常是并行进行的，其中常常使用语言模型（LMs）。该领域一个显著的模型是
    ProteinBERT [[210](#bib.bib210)]，它通过两个相互连接的 BERT-like 编码器对蛋白质序列和 GO 注释进行预训练。通过利用大规模生物知识数据集，ProteinKG25
    [[10](#bib.bib10)]、OntoProtein [[10](#bib.bib10)] 则专注于重建掩码氨基酸，同时最小化蛋白质的上下文表示和相关知识术语之间的嵌入距离。相比之下，KeAP
    [[211](#bib.bib211)] 旨在比 OntoProtein 更加细致地探索蛋白质与知识之间的关系。'
- en: '![Refer to caption](img/2be587b725eb4e0ad386eb98b5113c0c.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2be587b725eb4e0ad386eb98b5113c0c.png)'
- en: (a)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/2ffe72de1b91d582b12e3efb60be891f.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ffe72de1b91d582b12e3efb60be891f.png)'
- en: (b)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 17: Illustration of diverse protein modalities. (a) The method of calculating
    the conditional probability of a protein that contains domain $i$ having the GO
    $j$ function in DomainPFP [[230](#bib.bib230)]. (b) The protein surface at different
    time steps in a molecular dynamics trajectory [[232](#bib.bib232)].'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：不同蛋白质模式的示意图。 (a) 计算包含域 $i$ 的蛋白质具有 GO $j$ 功能的条件概率的方法，见 DomainPFP [[230](#bib.bib230)]。
    (b) 分子动力学轨迹中不同时间步的蛋白质表面，见[[232](#bib.bib232)]。
- en: Diverse Modalities Modeling
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多样化模式建模
- en: 'There are models that leverage deep learning techniques to incorporate diverse
    modalities, enabling a more comprehensive understanding of protein structure,
    function, and dynamics. For instance, MASSA [[114](#bib.bib114)] is an advanced
    multimodal deep learning framework designed to incorporate protein sequence, structure,
    and functional annotation. It employs five specific pre-training objectives to
    enhance its performance, including masked amino acid and GO, placement capture
    of motifs, domains, and regions. Domain-PFP [[230](#bib.bib230)], on the other
    hand, utilizes a self-supervised protocol to derive functionally consistent representations
    for protein domains. It achieves this by learning domain-GO co-occurrences and
    associations by calculating the probabilities of domains and GO terms as illustrated
    in Figure [17a](#S5.F17.sf1 "In Figure 17 ‣ Sequence-function Modeling ‣ 5.3 Protein
    Multimodal Methods ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey"), resulting in an improved understanding
    of domain functionality. BiDock [[231](#bib.bib231)] is a robust rigid docking
    model that effectively integrates MSAs and structural information. By employing
    a cross-modal transformer through bi-level optimization, BiDock enhances the accuracy
    of protein docking predictions. To capture protein dynamics, Sun et al. [[232](#bib.bib232)]
    focus on representing protein surfaces using implicit neural networks. This approach
    is particularly well-suited for modeling the dynamic shapes of proteins, which
    often exhibit complex and varied conformations, as shown in Figure [17b](#S5.F17.sf2
    "In Figure 17 ‣ Sequence-function Modeling ‣ 5.3 Protein Multimodal Methods ‣
    5 Deep Learning based Protein Models ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey"). Representative pre-trained protein multimodal models
    are listed in Table [3](#S5.T3 "Table 3 ‣ Equivariant Geometries ‣ 5.2 Protein
    Structure Models ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey"), and relative common datasets are
    presented in Table [4](#S5.T4 "Table 4 ‣ Equivariant Geometries ‣ 5.2 Protein
    Structure Models ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey").'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '有一些模型利用深度学习技术来结合多种模态，从而实现对蛋白质结构、功能和动态的更全面理解。例如，MASSA [[114](#bib.bib114)] 是一个先进的多模态深度学习框架，旨在结合蛋白质序列、结构和功能注释。它使用五个特定的预训练目标来提升其性能，包括掩蔽氨基酸和GO，基序、结构域和区域的定位捕获。另一方面，Domain-PFP [[230](#bib.bib230)]
    利用自监督协议来推导蛋白质结构域的功能一致表示。它通过学习域-GO的共现和关联来实现这一点，计算域和GO术语的概率，如图 [17a](#S5.F17.sf1
    "In Figure 17 ‣ Sequence-function Modeling ‣ 5.3 Protein Multimodal Methods ‣
    5 Deep Learning based Protein Models ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey")所示，从而改善了对结构域功能的理解。BiDock [[231](#bib.bib231)] 是一个强大的刚性对接模型，有效地整合了MSA和结构信息。通过使用跨模态变换器进行双层优化，BiDock
    提高了蛋白质对接预测的准确性。为了捕捉蛋白质的动态，Sun 等人 [[232](#bib.bib232)] 专注于使用隐式神经网络来表示蛋白质表面。这种方法特别适用于建模蛋白质的动态形状，蛋白质通常表现出复杂和多样的构象，如图 [17b](#S5.F17.sf2
    "In Figure 17 ‣ Sequence-function Modeling ‣ 5.3 Protein Multimodal Methods ‣
    5 Deep Learning based Protein Models ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey")所示。代表性的预训练蛋白质多模态模型列在表 [3](#S5.T3 "Table 3 ‣ Equivariant
    Geometries ‣ 5.2 Protein Structure Models ‣ 5 Deep Learning based Protein Models
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")中，相应的常用数据集在表 [4](#S5.T4
    "Table 4 ‣ Equivariant Geometries ‣ 5.2 Protein Structure Models ‣ 5 Deep Learning
    based Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")中呈现。'
- en: 5.4 Assessment of Pre-training Methods for Protein Representation Learning
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 蛋白质表示学习的预训练方法评估
- en: 'In this section, we enumerate various types of deep protein models, with a
    particular focus on PRL methods commonly employed in diverse scenarios. The evaluation
    encompasses several widely used pre-trained PRL methods applied to four distinct
    downstream tasks: (a) protein fold classification, (b) enzyme reaction classification,
    (c) GO term prediction, and (d) enzyme commission (EC) number prediction.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们列举了各种类型的深度蛋白质模型，特别关注在不同场景中常用的PRL方法。评估包括对四种不同下游任务应用的几种广泛使用的预训练PRL方法：（a）蛋白质折叠分类，（b）酶反应分类，（c）GO术语预测，以及（d）酶委员会（EC）编号预测。
- en: 'For protein fold classification (a), we adhere to the methodology outlined
    by Hermosilla et al. [[186](#bib.bib186)]. This dataset [[222](#bib.bib222)] comprises
    16,712 proteins distributed across 1,195 fold classes, featuring three provided
    test sets: Fold (excluding proteins from the same superfamily during training),
    SuperFamily (omitting proteins from the same family during training), and Family
    (including proteins from the same family in the training set). Enzyme reaction
    classification (b) is treated as a protein function prediction task based on enzyme-catalyzed
    reactions defined by the four levels of enzyme commission numbers [[233](#bib.bib233)].
    The dataset, compiled by Hermosilla et al. [[186](#bib.bib186)], encompasses 29,215
    training proteins, 2,562 validation proteins, and 5,651 test proteins. For GO
    term prediction (c), the objective is to predict whether a given protein should
    be annotated with a specific GO term. The GO term prediction dataset includes
    29,898/3,322/3,415 proteins for training/validation/test, respectively. In EC
    number prediction (d), the goal is to predict 538 EC numbers at the third and
    fourth levels of hierarchies for different proteins, following the methodology
    of DeepFRI [[197](#bib.bib197)]. The training/validation/test datasets comprise
    a total of 15,550/1,729/1,919 proteins. For both GO term and EC number prediction,
    the test sets only include PDB chains with a sequence identity no greater than
    95% to the training set [[59](#bib.bib59)]. Similar settings are also employed
    in LM-GVP [[196](#bib.bib196)], GearNet [[15](#bib.bib15)], ProtST [[136](#bib.bib136)],
    etc. These results are presented in Table [5](#S5.T5 "Table 5 ‣ 5.4 Assessment
    of Pre-training Methods for Protein Representation Learning ‣ 5 Deep Learning
    based Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey") and Table [6](#S5.T6 "Table 6 ‣ 5.4 Assessment of Pre-training Methods
    for Protein Representation Learning ‣ 5 Deep Learning based Protein Models ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey").'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '对于蛋白质折叠分类（a），我们遵循了Hermosilla等人提出的方法论[[186](#bib.bib186)]。该数据集[[222](#bib.bib222)]包含16,712个蛋白质，分布在1,195个折叠类别中，提供了三个测试集：Fold（训练过程中排除来自同一超家族的蛋白质）、SuperFamily（训练过程中排除来自同一家族的蛋白质）和Family（训练集中包括来自同一家族的蛋白质）。酶反应分类（b）被视为基于酶催化反应的蛋白质功能预测任务，这些反应由四级酶委员会编号[[233](#bib.bib233)]定义。该数据集由Hermosilla等人汇编[[186](#bib.bib186)]，包括29,215个训练蛋白质、2,562个验证蛋白质和5,651个测试蛋白质。对于GO术语预测（c），目标是预测给定蛋白质是否应标注特定的GO术语。GO术语预测数据集包括29,898/3,322/3,415个用于训练/验证/测试的蛋白质。在EC编号预测（d）中，目标是预测538个第三和第四层级的EC编号，遵循DeepFRI的方法[[197](#bib.bib197)]。训练/验证/测试数据集总共包含15,550/1,729/1,919个蛋白质。对于GO术语和EC编号预测，测试集仅包括序列相似度不超过95%的PDB链[[59](#bib.bib59)]。类似的设置也在LM-GVP[[196](#bib.bib196)]、GearNet[[15](#bib.bib15)]、ProtST[[136](#bib.bib136)]等中使用。这些结果分别呈现在表[5](#S5.T5
    "Table 5 ‣ 5.4 Assessment of Pre-training Methods for Protein Representation Learning
    ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning in Protein
    Science: A Comprehensive Survey")和表[6](#S5.T6 "Table 6 ‣ 5.4 Assessment of Pre-training
    Methods for Protein Representation Learning ‣ 5 Deep Learning based Protein Models
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")中。'
- en: We can see that the protein multimodal modeling methods, such as GearNet [[15](#bib.bib15)],
    SaProt [[207](#bib.bib207)], and ESM-GearNet-INR-MC [[212](#bib.bib212)], consistently
    deliver superior results across various tasks, showcasing the effectiveness of
    leveraging both sequence and structure information. This highlights the effectiveness
    of integrating both sequence and structure information in protein modeling. Notably,
    methods with a higher number of trainable parameters, such as ESM-1b and ProtBERT-BFD,
    exhibit competitive performance, emphasizing the significance of model complexity
    in specific scenarios. These observations underscore the pivotal role played by
    the pre-training dataset and model architecture choices in attaining superior
    performance across various facets of protein modeling.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，蛋白质多模态建模方法，如GearNet[[15](#bib.bib15)]、SaProt[[207](#bib.bib207)]和ESM-GearNet-INR-MC[[212](#bib.bib212)]，在各种任务中始终表现出优越的结果，展示了利用序列和结构信息的有效性。这突显了在蛋白质建模中整合序列和结构信息的有效性。值得注意的是，具有更多可训练参数的方法，如ESM-1b和ProtBERT-BFD，表现出具有竞争力的性能，强调了模型复杂性在特定场景中的重要性。这些观察结果突显了预训练数据集和模型架构选择在各个方面取得优越性能中的关键作用。
- en: 'Table 5: Accuracy ($\%$) of fold classification and enzyme reaction classification.
    The best results are shown in bold.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：折叠分类和酶反应分类的准确率（$\%$）。最佳结果以**粗体**显示。
- en: 'Method Input Param. Pre-training Fold Classification Enzyme Dataset (Used)
    Fold SuperFamily Family Reaction ESM-1b [[14](#bib.bib14)] Seq 650M UniRef50 (24M)
    26.8 60.1 97.8 83.1 ProtBert-BFD [[12](#bib.bib12)] Seq 420M BFD (2.1B) 26.6 55.8
    97.6 72.2 IEConv [[186](#bib.bib186)] Struct 36.6M PDB (180K) 50.3 80.6 99.7 88.1
    DeepFRI [[197](#bib.bib197)] Seq, Struct 6.2M Pfam (10M) 15.3 20.6 73.2 63.3 GearNet
    (Multiview Contras) [[15](#bib.bib15)] Seq, Struct 42M AlphaFoldDB (805K) 54.1
    80.5 99.9 87.5 GearNet (Residue Type Prediction) [[15](#bib.bib15)] Seq, Struct
    42M AlphaFoldDB (805K) 48.8 71.0 99.4 86.6 GearNet (Distance Prediction) [[15](#bib.bib15)]
    Seq, Struct 42M AlphaFoldDB (805K) 50.9 73.5 99.4 87.5 GearNet (Angle Prediction) [[15](#bib.bib15)]
    Seq, Struct 42M AlphaFoldDB (805K) 56.5 76.3 99.6 86.8 GearNet (Dihedral Prediction) [[15](#bib.bib15)]
    Seq, Struct 42M AlphaFoldDB (805K) 51.8 77.8 99.6 87.0 • Seq: sequence, Struct:
    Structure, Func: Function. Param., means the number of trainable parameters (B:
    billion; M: million; K: thousand). The dataset used here does not exceed the reported
    size as shown in Table [4](#S5.T4 "Table 4 ‣ Equivariant Geometries ‣ 5.2 Protein
    Structure Models ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey").'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '方法 输入 参数 预训练 折叠分类 酶数据集（使用） 折叠 超家族 家族 反应 ESM-1b [[14](#bib.bib14)] 序列 650M UniRef50
    (24M) 26.8 60.1 97.8 83.1 ProtBert-BFD [[12](#bib.bib12)] 序列 420M BFD (2.1B) 26.6
    55.8 97.6 72.2 IEConv [[186](#bib.bib186)] 结构 36.6M PDB (180K) 50.3 80.6 99.7
    88.1 DeepFRI [[197](#bib.bib197)] 序列, 结构 6.2M Pfam (10M) 15.3 20.6 73.2 63.3 GearNet
    (Multiview Contras) [[15](#bib.bib15)] 序列, 结构 42M AlphaFoldDB (805K) 54.1 80.5
    99.9 87.5 GearNet (Residue Type Prediction) [[15](#bib.bib15)] 序列, 结构 42M AlphaFoldDB
    (805K) 48.8 71.0 99.4 86.6 GearNet (Distance Prediction) [[15](#bib.bib15)] 序列,
    结构 42M AlphaFoldDB (805K) 50.9 73.5 99.4 87.5 GearNet (Angle Prediction) [[15](#bib.bib15)]
    序列, 结构 42M AlphaFoldDB (805K) 56.5 76.3 99.6 86.8 GearNet (Dihedral Prediction)
    [[15](#bib.bib15)] 序列, 结构 42M AlphaFoldDB (805K) 51.8 77.8 99.6 87.0 • 序列：序列，结构：结构，功能：功能。参数，表示可训练参数的数量（B：十亿；M：百万；K：千）。这里使用的数据集不超过表
    [4](#S5.T4 "Table 4 ‣ Equivariant Geometries ‣ 5.2 Protein Structure Models ‣
    5 Deep Learning based Protein Models ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey") 中报告的大小。'
- en: 'Table 6: $\mathrm{F}_{\mathrm{max}}$ [[15](#bib.bib15)] of GO term prediction
    and EC number prediction. The best results are shown in bold.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：$\mathrm{F}_{\mathrm{max}}$ [[15](#bib.bib15)] 在 GO 术语预测和 EC 编号预测中的表现。最佳结果以**粗体**显示。
- en: 'Method Input Param. Pre-training GO EC Dataset (Used) BP MF CC ESM-1b [[14](#bib.bib14)]
    Seq 650M UniRef50 (24M) 0.470 0.657 0.488 0.864 ProtBERT-BFD [[12](#bib.bib12)]
    Seq 420M BFD (2.1B) 0.279 0.456 0.408 0.838 ESM-2 [[13](#bib.bib13)] Seq 650M
    UniRef50 (24M) 0.472 0.662 0.472 0.874 IEConv [[186](#bib.bib186)] Struct 36.6M
    PDB (180K) 0.468 0.661 0.516 - DeepFRI [[197](#bib.bib197)] Seq, Struct 6.2M Pfam
    (10M) 0.399 0.465 0.460 0.631 LM-GVP [[196](#bib.bib196)] Seq, Struct 420M UniRef100
    (216M) 0.417 0.545 0.527 0.664 GearNet (Multiview Contrast) [[15](#bib.bib15)]
    Seq, Struct 42M AlphaFoldDB (805K) 0.490 0.654 0.488 0.874 GearNet (Residue Type
    Prediction) [[15](#bib.bib15)] Seq, Struct 42M AlphaFoldDB (805K) 0.430 0.604
    0.465 0.843 GearNet (Distance Prediction) [[15](#bib.bib15)] Seq, Struct 42M AlphaFoldDB
    (805K) 0.448 0.616 0.464 0.839 GearNet (Angle Prediction) [[15](#bib.bib15)] Seq,
    Struct 42M AlphaFoldDB (805K) 0.458 0.625 0.473 0.853 GearNet (Dihedral Prediction) [[15](#bib.bib15)]
    Seq, Struct 42M AlphaFoldDB (805K) 0.458 0.626 0.465 0.859 ESM-GearNet [[205](#bib.bib205)]
    Seq, Struct 692M AlphaFoldDB (805K) 0.488 0.681 0.464 0.890 SaProt [[207](#bib.bib207)]
    Seq, Struct 650M AlphaFoldDB (805K) 0.356 0.678 0.414 0.884 KeAP [[211](#bib.bib211)]
    Seq, Func 520M ProteinKG25 (5M) 0.466 0.659 0.470 0.845 ProtST-ESM-1b [[136](#bib.bib136)]
    Seq, Func 759M ProtDescribe (553K) 0.480 0.661 0.488 0.878 ProtST-ESM-2 [[136](#bib.bib136)]
    Seq, Func 759M ProtDescribe (553K) 0.482 0.668 0.487 0.878 ESM-GearNet-INR-MC [[212](#bib.bib212)]
    Seq, Struct, Surface - Swiss-Prot 0.518 0.683 0.504 0.896 • Seq: sequence, Struct:
    Structure, Func: Function. Param., means the number of trainable parameters (B:
    billion; M: million; K: thousand). The dataset used here does not exceed the reported
    size as shown in Table [4](#S5.T4 "Table 4 ‣ Equivariant Geometries ‣ 5.2 Protein
    Structure Models ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey").'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '方法 输入 参数 预训练 GO EC 数据集（使用） BP MF CC ESM-1b [[14](#bib.bib14)] 序列 650M UniRef50
    (24M) 0.470 0.657 0.488 0.864 ProtBERT-BFD [[12](#bib.bib12)] 序列 420M BFD (2.1B)
    0.279 0.456 0.408 0.838 ESM-2 [[13](#bib.bib13)] 序列 650M UniRef50 (24M) 0.472
    0.662 0.472 0.874 IEConv [[186](#bib.bib186)] 结构 36.6M PDB (180K) 0.468 0.661
    0.516 - DeepFRI [[197](#bib.bib197)] 序列, 结构 6.2M Pfam (10M) 0.399 0.465 0.460
    0.631 LM-GVP [[196](#bib.bib196)] 序列, 结构 420M UniRef100 (216M) 0.417 0.545 0.527
    0.664 GearNet (Multiview Contrast) [[15](#bib.bib15)] 序列, 结构 42M AlphaFoldDB (805K)
    0.490 0.654 0.488 0.874 GearNet (Residue Type Prediction) [[15](#bib.bib15)] 序列,
    结构 42M AlphaFoldDB (805K) 0.430 0.604 0.465 0.843 GearNet (Distance Prediction) [[15](#bib.bib15)]
    序列, 结构 42M AlphaFoldDB (805K) 0.448 0.616 0.464 0.839 GearNet (Angle Prediction) [[15](#bib.bib15)]
    序列, 结构 42M AlphaFoldDB (805K) 0.458 0.625 0.473 0.853 GearNet (Dihedral Prediction) [[15](#bib.bib15)]
    序列, 结构 42M AlphaFoldDB (805K) 0.458 0.626 0.465 0.859 ESM-GearNet [[205](#bib.bib205)]
    序列, 结构 692M AlphaFoldDB (805K) 0.488 0.681 0.464 0.890 SaProt [[207](#bib.bib207)]
    序列, 结构 650M AlphaFoldDB (805K) 0.356 0.678 0.414 0.884 KeAP [[211](#bib.bib211)]
    序列, 功能 520M ProteinKG25 (5M) 0.466 0.659 0.470 0.845 ProtST-ESM-1b [[136](#bib.bib136)]
    序列, 功能 759M ProtDescribe (553K) 0.480 0.661 0.488 0.878 ProtST-ESM-2 [[136](#bib.bib136)]
    序列, 功能 759M ProtDescribe (553K) 0.482 0.668 0.487 0.878 ESM-GearNet-INR-MC [[212](#bib.bib212)]
    序列, 结构, 表面 - Swiss-Prot 0.518 0.683 0.504 0.896 • 序列: sequence, 结构: Structure,
    功能: Function。参数表示可训练参数的数量（B: 亿; M: 百万; K: 千）。这里使用的数据集不超过表格 [4](#S5.T4 "表格 4 ‣
    等变几何 ‣ 5.2 蛋白质结构模型 ‣ 5 深度学习基于蛋白质模型 ‣ 深度学习在蛋白质科学中的进展：综合调查") 中报告的大小。'
- en: 6 Pretext Task
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 预文本任务
- en: 'In pre-training, models are exposed to a large amount of unlabeled data to
    learn general representations before being fine-tuned on specific downstream tasks.
    The pretext task is typically designed to encourage the model to learn useful
    features or patterns from the input data. Thus, the pretext task refers to a specific
    objective or task that a machine learning model is trained on as part of a pre-training
    phase. We have listed the pretext tasks in the Table [1](#S5.T1 "Table 1 ‣ Single
    Sequences ‣ 5.1 Protein Language Models ‣ 5 Deep Learning based Protein Models
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")-Table [3](#S5.T3
    "Table 3 ‣ Equivariant Geometries ‣ 5.2 Protein Structure Models ‣ 5 Deep Learning
    based Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey").'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练过程中，模型会接触大量未标记的数据，以便在对特定下游任务进行微调之前学习一般性表征。预文本任务通常被设计用来促使模型从输入数据中学习有用的特征或模式。因此，预文本任务指的是机器学习模型在预训练阶段作为一部分进行训练的特定目标或任务。我们在表格 [1](#S5.T1
    "表格 1 ‣ 单序列 ‣ 5.1 蛋白质语言模型 ‣ 5 深度学习基于蛋白质模型 ‣ 深度学习在蛋白质科学中的进展：综合调查")-表格 [3](#S5.T3
    "表格 3 ‣ 等变几何 ‣ 5.2 蛋白质结构模型 ‣ 5 深度学习基于蛋白质模型 ‣ 深度学习在蛋白质科学中的进展：综合调查") 中列出了预文本任务。
- en: 6.1 Self-supervised Pretext Task
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 自监督预文本任务
- en: The self-supervised pretext tasks leverage the available training data as supervision
    signals, eliminating the requirement for additional annotations [[234](#bib.bib234)].
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督的预训练任务利用现有的训练数据作为监督信号，从而消除了对额外标注的需求 [[234](#bib.bib234)]。
- en: Predictive Pretext Task
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预测预训练任务
- en: The goal of predictive methods is to generate informative labels directly from
    the data itself, which are then utilized as supervision to establish and manage
    the relationships between data and their corresponding labels.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 预测方法的目标是直接从数据中生成信息标签，然后利用这些标签作为监督，以建立和管理数据与其对应标签之间的关系。
- en: 'As we have stated in Subsection [3.5](#S3.SS5 "3.5 Language Models ‣ 3 Basic
    Neural Networks in Protein Modeling ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey"), GPT is developed to enhance the performance of autoregressive
    language modeling in the pre-training phase. Formally, given an unsupervised corpus
    of tokens $\mathcal{X}=\left\{x_{0},x_{1},\ldots,x_{n},x_{n+1}\right\}$, GPT employs
    a standard language modeling objective to maximize the likelihood:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们在小节 [3.5](#S3.SS5 "3.5 Language Models ‣ 3 Basic Neural Networks in Protein
    Modeling ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")中所述，GPT
    的开发旨在提升自回归语言建模在预训练阶段的表现。正式地，给定一个无监督的符号语料库 $\mathcal{X}=\left\{x_{0},x_{1},\ldots,x_{n},x_{n+1}\right\}$，GPT
    使用标准语言建模目标来最大化似然：'
- en: '|  | $\mathcal{L}_{next}(\mathcal{X})=\sum_{i=1}^{n+1}\log P\left(x_{i}\mid
    x_{i-k},\ldots,x_{i-1};\Theta_{P}\right)$ |  | (21) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{next}(\mathcal{X})=\sum_{i=1}^{n+1}\log P\left(x_{i}\mid
    x_{i-k},\ldots,x_{i-1};\Theta_{P}\right)$ |  | (21) |'
- en: here, $k$ represents the size of the context window, and the conditional probability
    $P$ is modeled using a network decoder with parameters $\Theta_{P}$. The next
    token prediction is a fundamental aspect of autoregressive language modeling.
    The model learns to generate coherent and meaningful sequences by estimating the
    probability distribution over the vocabulary and selecting the most likely next
    token.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$k$ 代表上下文窗口的大小，条件概率 $P$ 是通过具有参数 $\Theta_{P}$ 的网络解码器建模的。下一个符号预测是自回归语言建模的一个基本方面。该模型通过估计词汇表上的概率分布并选择最可能的下一个符号来学习生成连贯且有意义的序列。
- en: 'For the masked language modeling in BERT, formally, given a corpus of tokens
    $\mathcal{X}=\left\{x_{0},x_{1},\ldots,x_{n},x_{n+1}\right\}$, BERT maximizes
    the likelihood as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 BERT 中的掩码语言建模，正式地，给定一个符号语料库 $\mathcal{X}=\left\{x_{0},x_{1},\ldots,x_{n},x_{n+1}\right\}$，BERT
    通过以下方式最大化似然：
- en: '|  | $\mathcal{L}_{masked}(\mathcal{X})=\textstyle\sum_{x\in\text{mask}(\mathcal{X})}\log
    P(x\mid\tilde{\mathcal{X}};\Theta_{P})$ |  | (22) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{masked}(\mathcal{X})=\textstyle\sum_{x\in\text{mask}(\mathcal{X})}\log
    P(x\mid\tilde{\mathcal{X}};\Theta_{P})$ |  | (22) |'
- en: $\text{mask}(\mathcal{X})$ represents the masked tokens, $\tilde{\mathcal{X}}$
    is the result obtained after masking certain tokens in $\mathcal{X}$, and the
    probability $P$ is modeled by the transformer encoder with parameters $\Theta_{P}$.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{mask}(\mathcal{X})$ 表示被掩码的符号，$\tilde{\mathcal{X}}$ 是在 $\mathcal{X}$ 中掩码某些符号后的结果，概率
    $P$ 是由具有参数 $\Theta_{P}$ 的 transformer 编码器建模的。
- en: 'Bidirectional language modeling is to model the probability of a token based
    on both the preceding and following tokens. Formally, given a corpus of tokens
    $\mathcal{X}=\left\{x_{0},x_{1},\ldots,x_{n},x_{n+1}\right\}$, $k$ is the size
    of the context window, and $x_{i}$ denotes the $i$-th token in the sequence:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 双向语言建模是基于前面和后面的符号来建模一个符号的概率。正式地，给定一个符号语料库 $\mathcal{X}=\left\{x_{0},x_{1},\ldots,x_{n},x_{n+1}\right\}$，$k$
    是上下文窗口的大小，$x_{i}$ 表示序列中的第 $i$ 个符号：
- en: '|  | $\mathcal{L}_{bi}(\mathcal{X})=\sum_{i=1}^{n+1}\left[\log P(x_{i}\mid
    x_{i-k},\ldots,x_{i-1};\Theta_{P})+\log P(x_{i}\mid x_{i+1},\ldots,x_{i+k};\Theta_{P})\right]$
    |  | (23) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{bi}(\mathcal{X})=\sum_{i=1}^{n+1}\left[\log P(x_{i}\mid
    x_{i-k},\ldots,x_{i-1};\Theta_{P})+\log P(x_{i}\mid x_{i+1},\ldots,x_{i+k};\Theta_{P})\right]$
    |  | (23) |'
- en: '![Refer to caption](img/52973e64d763130ef64fff080ffc676c.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/52973e64d763130ef64fff080ffc676c.png)'
- en: 'Figure 18: Diagram of language modeling approaches, including the autoregressive,
    masked and bidirectional language modeling strategies [[46](#bib.bib46)].'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：语言建模方法的示意图，包括自回归、掩码和双向语言建模策略 [[46](#bib.bib46)]。
- en: Instead of predicting the likelihood of an individual amino acid, PMLM [[159](#bib.bib159)]
    suggests modeling the likelihood of a pair of masked amino acids. Besides, there
    are span masking and sequential masking strategies used in the training of OmegaPLM [[164](#bib.bib164)].
    For the span masking, the span length is sampled from Poisson distribution and
    clipped at 5 and 8, the protein sequence is masked consecutively according to
    the span length. For the sequential masking, the protein sequence is masked in
    either the first half or the second half. Moreover, xTrimoPGLM [[170](#bib.bib170)]
    employs two types of pre-training objectives, each with its specific indicator
    tokens, to ensure both understanding and generative capacities. One is predicting
    randomly masked tokens, and the other is to predict span tokens, i.e., recovering
    short spans in an autoregressive manner.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: PMLM [[159](#bib.bib159)] 建议建模一对掩蔽氨基酸的可能性，而不是预测单个氨基酸的可能性。此外，在 OmegaPLM [[164](#bib.bib164)]
    的训练中使用了跨度掩蔽和顺序掩蔽策略。对于跨度掩蔽，跨度长度从 Poisson 分布中采样，并限制在 5 和 8 之间，蛋白质序列根据跨度长度进行连续掩蔽。对于顺序掩蔽，蛋白质序列被掩蔽在前半部分或后半部分。此外，xTrimoPGLM
    [[170](#bib.bib170)] 采用了两种类型的预训练目标，每种目标都有其特定的指示标记，以确保理解能力和生成能力。一个是预测随机掩蔽的标记，另一个是预测跨度标记，即以自回归方式恢复短跨度。
- en: 'Similar to the masked language modeling methods, which involve masking and
    predicting tokens, there are also predictive pretext tasks designed for GNNs in
    the context of protein analysis. In these tasks, the pseudo labels are derived
    from protein features. For example, Chen et al. [[190](#bib.bib190)] employ a
    GNN model that takes the masked protein structure as input and aims to reconstruct
    the pairwise residue distances and dihedral angles. These masked protein features
    commonly include $\mathrm{C}_{\alpha}$ distances, backbone dihedral angles, and
    residue types, as indicated in Table [3](#S5.T3 "Table 3 ‣ Equivariant Geometries
    ‣ 5.2 Protein Structure Models ‣ 5 Deep Learning based Protein Models ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey") [[15](#bib.bib15),
    [190](#bib.bib190)].'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于掩蔽语言建模方法，这些方法涉及掩蔽和预测标记，也有针对 GNN 的预测前置任务，特别是在蛋白质分析的背景下。在这些任务中，伪标签来源于蛋白质特征。例如，Chen
    等人[[190](#bib.bib190)] 使用 GNN 模型，以掩蔽蛋白质结构作为输入，并旨在重建成对残基距离和二面角。这些掩蔽的蛋白质特征通常包括 $\mathrm{C}_{\alpha}$
    距离、主链二面角和残基类型，如表 [3](#S5.T3 "Table 3 ‣ Equivariant Geometries ‣ 5.2 Protein Structure
    Models ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning in Protein
    Science: A Comprehensive Survey") [[15](#bib.bib15), [190](#bib.bib190)] 所示。'
- en: Contrastive Pretext Task
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对比前置任务
- en: 'The contrastive pretext task involves training a model to learn meaningful
    representations by contrasting similar and dissimilar examples. Hermosilla et
    al. [[189](#bib.bib189)] propose contrastive learning for protein structures,
    addressing the challenge of limited annotated datasets. Multiview contrastive
    learning aims to learn meaningful representations from multiple views of the same
    data. For instance, GearNet [[15](#bib.bib15)] aligns representations from different
    views of the same protein while minimizing similarity with representations from
    different proteins (Multiview Contrast). Diverse views of a protein are generated
    using various data augmentation strategies, such as sequence cropping and random
    edge masking. In terms of momentum contrast in GraSR [[200](#bib.bib200)], there
    are two GNN-based encoders, denoted as $f_{q}$ and $f_{k}$, which take the query
    protein and key protein as inputs, respectively. These proteins have similar structures
    and are considered positive pairs. $f_{q}$ and $f_{k}$ share the same architecture
    but have different parameter sets ($\theta_{q}$ and $\theta_{k}$). $\theta_{q}$
    is updated through back-propagation, while $\theta_{k}$ is updated using the following
    equation:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对比前置任务涉及通过对比相似和不同的例子来训练模型以学习有意义的表示。Hermosilla 等人[[189](#bib.bib189)] 提出了用于蛋白质结构的对比学习，解决了注释数据集有限的挑战。多视角对比学习旨在从同一数据的多个视角中学习有意义的表示。例如，GearNet
    [[15](#bib.bib15)] 对齐了来自同一蛋白质不同视角的表示，同时最小化了与不同蛋白质表示之间的相似性（多视角对比）。蛋白质的多样视角是通过各种数据增强策略生成的，例如序列裁剪和随机边缘掩蔽。在
    GraSR [[200](#bib.bib200)] 的动量对比方面，有两个基于 GNN 的编码器，分别表示为 $f_{q}$ 和 $f_{k}$，它们分别以查询蛋白质和关键蛋白质作为输入。这些蛋白质具有相似的结构，并被视为正对。$f_{q}$
    和 $f_{k}$ 共享相同的架构，但具有不同的参数集（$\theta_{q}$ 和 $\theta_{k}$）。$\theta_{q}$ 通过反向传播进行更新，而
    $\theta_{k}$ 则使用以下公式进行更新：
- en: '|  | $\theta_{k}\leftarrow m\theta_{k}+(1-m)\theta_{q}$ |  | (24) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{k}\leftarrow m\theta_{k}+(1-m)\theta_{q}$ |  | (24) |'
- en: here, $m\in(0,1]$ is a momentum coefficient. The ProteinINR [[212](#bib.bib212)]
    employs a continual pre-training approach [[201](#bib.bib201)] to effectively
    model the embeddings across various modalities. Specifically, sequence data undergo
    initial pre-training using a sequence encoder, with the resultant encodings serving
    as inputs for the subsequent structure encoder phase. This structure encoder is
    then pre-trained on surface data, utilizing the weights from this phase as the
    foundational weights for further pre-training focused on structural details. The
    process culminates in the structure encoder undergoing additional pre-training
    on the architectural aspects through a multi-view contrastive learning method.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$m\in(0,1]$ 是一个动量系数。ProteinINR [[212](#bib.bib212)] 采用持续预训练方法 [[201](#bib.bib201)]
    来有效建模各种模态下的嵌入。具体而言，序列数据通过序列编码器进行初步预训练，生成的编码作为后续结构编码器阶段的输入。这个结构编码器然后在表面数据上进行预训练，利用这一阶段的权重作为进一步关注结构细节的预训练的基础权重。该过程的最后阶段是通过多视角对比学习方法对结构编码器进行额外的预训练。
- en: 6.2 Supervised Pretext Task
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 监督预文本任务
- en: During the pre-training phase, a supervised pretext task is commonly employed
    to provide auxiliary information and guide the model in learning enhanced representations.
    Min et al. [[162](#bib.bib162)] introduces a protein-specific pretext task called
    Same-Family Prediction, where the model is trained to predict whether a given
    pair of proteins belongs to the same protein family. This task helps the model
    learn meaningful protein representations. In addition, Bepler and Berger [[46](#bib.bib46)]
    utilize a masked language modeling objective, denoted as $\mathcal{L}_{masked}$,
    for training on sequences. They predict intra-residue contacts by employing a
    bilinear projection of the sequence embeddings, which is measured by the negative
    log-likelihood of the true contacts. They also introduce a structural similarity
    prediction loss to incorporate structural information into the LM. However, Wu
    et al. [[235](#bib.bib235)] argue that not all external knowledge is beneficial
    for downstream tasks. It is crucial to carefully select appropriate tasks to avoid
    task interference, especially when dealing with diverse tasks. Task interference
    is a common problem that needs to be considered [[115](#bib.bib115)].
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练阶段，通常会使用监督式预文本任务来提供辅助信息，并引导模型学习增强的表示。Min 等人 [[162](#bib.bib162)] 引入了一种特定于蛋白质的预文本任务，称为同家族预测，其中模型被训练以预测给定的一对蛋白质是否属于同一蛋白质家族。这个任务帮助模型学习有意义的蛋白质表示。此外，Bepler
    和 Berger [[46](#bib.bib46)] 利用一个掩码语言建模目标，表示为 $\mathcal{L}_{masked}$，用于对序列进行训练。他们通过采用序列嵌入的双线性投影来预测内部残基接触，该过程通过真实接触的负对数似然来测量。他们还引入了一个结构相似性预测损失，以将结构信息纳入语言模型。然而，Wu
    等人 [[235](#bib.bib235)] 认为并非所有外部知识对下游任务都有益。仔细选择适当的任务以避免任务干扰是至关重要的，特别是在处理多样化任务时。任务干扰是一个常见问题，需要加以考虑
    [[115](#bib.bib115)]。
- en: 7 Downstream Tasks
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 个下游任务
- en: In the field of protein analysis, there are several downstream tasks that aim
    to extract valuable information and insights from protein data. These tasks play
    a crucial role in understanding protein structure, function, interactions, and
    their implications in various biological processes, including PSP, protein property
    prediction, and protein engineering and design, etc.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在蛋白质分析领域，有几个下游任务旨在从蛋白质数据中提取有价值的信息和见解。这些任务在理解蛋白质的结构、功能、相互作用及其在各种生物过程中的意义方面发挥着关键作用，包括
    PSP、蛋白质属性预测和蛋白质工程与设计等。
- en: 7.1 Protein Structure Prediction
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 蛋白质结构预测
- en: Structural features can be categorized into 1D and 2D representations. The 1D
    features encompass various aspects such as secondary structure, solvent accessibility,
    torsion angles, contact density, and more. On the other hand, the 2D features
    include contact and distance maps. For example, RaptorX-Contact [[236](#bib.bib236)]
    integrates sequence and evolutionary coupling information using two deep residual
    neural networks to predict contact maps. This approach significantly enhances
    contact prediction performance. Other than the contact and distance maps, Yang
    et al. [[120](#bib.bib120)], Li and Xu [[237](#bib.bib237)] focus on studying
    inter-atom distance and inter-residue orientation using a ResNet network. They
    utilize the predicted means and deviations to construct 3D structure models, leveraging
    the constraints provided by PyRosetta.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 结构特征可以分为1D和2D表示。1D特征包括各种方面，如二级结构、溶剂可及性、扭转角、接触密度等。另一方面，2D特征包括接触图和距离图。例如，RaptorX-Contact[[236](#bib.bib236)]
    利用两个深度残差神经网络整合序列和进化耦合信息，以预测接触图。这种方法显著提升了接触预测性能。除了接触图和距离图，Yang等人[[120](#bib.bib120)]、Li和Xu[[237](#bib.bib237)]
    还专注于使用ResNet网络研究原子间距离和残基间取向。他们利用预测的均值和偏差来构建3D结构模型，借助PyRosetta提供的约束条件。
- en: While it is true that the prediction of structural features may not have significant
    practical value in the presence of highly accurate 3D structure data from AF2\.
    It still holds relevance and utility in several aspects. From one perspective,
    predicting structural features can serve as a reference for evaluating and comparing
    the results of various proposed methods [[238](#bib.bib238)]. Furthermore, these
    predictions can contribute to the exploration of relationships between protein
    sequence, structure, and function by uncovering additional protein grammars and
    patterns. For instance, DeepHomo [[239](#bib.bib239)] focuses on predicting inter-protein
    residue-residue contacts across homo-oligomeric protein interfaces. By integrating
    multiple sources of information and removing potential noise from intra-protein
    contacts, DeepHomo aims to provide insights into the complex interactions within
    protein complexes. Another example is Geoformer [[164](#bib.bib164)], which refines
    contact prediction to address the issue of triangular inconsistency.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在拥有来自AF2的高度准确的3D结构数据的情况下，预测结构特征可能没有显著的实际价值，但它在多个方面仍然具有相关性和实用性。从一个角度来看，预测结构特征可以作为评估和比较各种提议方法结果的参考[[238](#bib.bib238)]。此外，这些预测可以通过揭示额外的蛋白质语法和模式，促进对蛋白质序列、结构和功能之间关系的探索。例如，DeepHomo[[239](#bib.bib239)]
    专注于预测跨同源寡聚体蛋白质界面的蛋白质残基-残基接触。通过整合多个信息源并去除潜在的噪声，DeepHomo旨在深入了解蛋白质复合物中的复杂相互作用。另一个例子是Geoformer[[164](#bib.bib164)]，它通过改进接触预测来解决三角不一致的问题。
- en: 'The introduction of the highly accurate model, AF2 [[16](#bib.bib16)] has significantly
    influenced the development of end-to-end models for PSP. As depicted in Figure [19](#S7.F19
    "Figure 19 ‣ 7.1 Protein Structure Prediction ‣ 7 Downstream Tasks ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey"), AF2 consists of
    an encoder and decoder. The core module of AF2 is Evoformer (encoder), which utilizes
    a variant of axial attention, including row-wise gated self-attention and column-wise
    gated self-attention, to process the MSA representation. To ensure consistency
    in the embedding space, triangle multiplicative update, and triangle self-attention
    blocks are designed. The former combines information within each triangle of graph
    edges, while the latter operates self-attention around graph nodes. The structure
    module (decoder) consists of eight layers with shared weights. It takes the pair
    and first row MSA representations from the Evoformer as input. Each layer updates
    the single representation and the backbone frames using Euclidean transforms.
    The structure module includes the Invariant Point Attention (IPA) module, a form
    of attention that acts on a set of frames and is invariant under global Euclidean
    transformation. Another notable PSP method, RoseTTAFold [[240](#bib.bib240)] is
    a three-track model with attention layers that facilitate information flow at
    the 1D, 2D, and 3D levels. It comprises seven modules. The MSA features are processed
    by attention over rows and columns, and the resulting features are aggregated
    using the outer product to update pair features, which are further refined via
    axial attention. The MSA features are also updated based on attention maps derived
    from pair features, which exhibit good agreement with the true contact maps. A
    fully connected graph, built using the learned MSA and pair features as node and
    edge embeddings, is employed with a graph transformer to estimate the initial
    3D structure. New attention maps derived from the current structure are used to
    update MSA features. Finally, the 3D coordinates are refined by SE(3)-Transformer [[129](#bib.bib129)]
    based on the updated MSA and pair features.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 高精度模型AF2 [[16](#bib.bib16)]的引入显著影响了端到端模型在PSP中的发展。如图[19](#S7.F19 "图 19 ‣ 7.1
    蛋白质结构预测 ‣ 7 下游任务 ‣ 深度学习在蛋白质科学中的进展：综合调查")所示，AF2由一个编码器和一个解码器组成。AF2的核心模块是Evoformer（编码器），它利用了一种轴向注意力的变体，包括行-wise门控自注意力和列-wise门控自注意力，以处理MSA表示。为了确保嵌入空间的一致性，设计了三角乘法更新和三角自注意力块。前者结合了图边缘中每个三角形的信息，而后者在图节点周围进行自注意力。结构模块（解码器）由八层共享权重的层组成。它以Evoformer的对接和第一行MSA表示作为输入。每一层使用欧几里得变换更新单一表示和骨架框架。结构模块包括不变点注意力（IPA）模块，这是一种作用于一组框架的注意力形式，并且在全局欧几里得变换下保持不变。另一个显著的PSP方法，RoseTTAFold
    [[240](#bib.bib240)]是一个三轨模型，具有促进1D、2D和3D层级信息流动的注意力层。它由七个模块组成。MSA特征通过行和列的注意力进行处理，结果特征通过外积进行聚合，以更新对接特征，这些特征通过轴向注意力进一步细化。MSA特征还根据对接特征派生的注意力图进行更新，这些图与真实接触图具有良好的一致性。使用学习到的MSA和对接特征作为节点和边缘嵌入构建的全连接图，通过图变换器来估计初始3D结构。根据当前结构派生的新注意力图被用来更新MSA特征。最后，基于更新后的MSA和对接特征，3D坐标通过SE(3)-Transformer
    [[129](#bib.bib129)]进行细化。
- en: '![Refer to caption](img/0ca4da1e63c9deeee599bf6904be562f.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/0ca4da1e63c9deeee599bf6904be562f.png)'
- en: 'Figure 19: Comparison of schematic architectures of AF2 and single sequence
    structure prediction methods.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：AF2 和单序列结构预测方法的示意架构比较。
- en: 'However, AF2 and RosettaFold may face challenges when predicting structures
    for proteins with low or no evolutionary information, such as orphan proteins [[241](#bib.bib241)].
    In such cases, OmegaFold [[164](#bib.bib164)] utilizes a LM to obtain the residue
    and pairwise embeddings from a single sequence. These embeddings are then fed
    into Geoformer for accurate predictions on orphan proteins. ESMFold [[13](#bib.bib13)]
    tackles this issue by training large-scale protein LMs (ESM-2) to learn more structural
    and functional properties, replacing the need for explicit MSAs, as shown in Figure [19](#S7.F19
    "Figure 19 ‣ 7.1 Protein Structure Prediction ‣ 7 Downstream Tasks ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey"), which can reduce
    training resources and time. Thus, ESMFold achieves inference speeds 60 times
    faster than AF2.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，AF2和RosettaFold在预测具有低或没有进化信息的蛋白质结构时可能面临挑战，例如孤立蛋白质[[241](#bib.bib241)]。在这种情况下，OmegaFold[[164](#bib.bib164)]利用语言模型从单一序列中获取残基和配对嵌入。这些嵌入随后被输入Geoformer，以对孤立蛋白质进行准确预测。ESMFold[[13](#bib.bib13)]通过训练大规模蛋白质语言模型（ESM-2）来解决这一问题，以学习更多的结构和功能属性，从而替代显式的MSA，如图[19](#S7.F19
    "Figure 19 ‣ 7.1 Protein Structure Prediction ‣ 7 Downstream Tasks ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey")所示，这可以减少训练资源和时间。因此，ESMFold实现了比AF2快60倍的推断速度。'
- en: 'The remarkable success in PSP has led to research in various related areas,
    including protein-peptide binders identification [[242](#bib.bib242)], antibody
    structure prediction [[243](#bib.bib243)], protein complex structure prediction [[244](#bib.bib244),
    [245](#bib.bib245)], RNA structure prediction [[246](#bib.bib246)], and protein
    conformation prediction [[247](#bib.bib247)], etc. Representative methods for
    PSP are summarized in Figure [20](#S7.F20 "Figure 20 ‣ 7.1 Protein Structure Prediction
    ‣ 7 Downstream Tasks ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey").'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '在蛋白质结构预测（PSP）领域的显著成功引发了对各种相关领域的研究，包括蛋白质-肽结合体识别[[242](#bib.bib242)]，抗体结构预测[[243](#bib.bib243)]，蛋白质复合物结构预测[[244](#bib.bib244)]，[245](#bib.bib245)]，RNA结构预测[[246](#bib.bib246)]，以及蛋白质构象预测[[247](#bib.bib247)]等。PSP的代表性方法在图[20](#S7.F20
    "Figure 20 ‣ 7.1 Protein Structure Prediction ‣ 7 Downstream Tasks ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey")中进行了总结。'
- en: '{forest}'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '{森林}'
- en: for tree= grow=east, reversed=true, anchor=base west, parent anchor=east, child
    anchor=west, base=middle, font=, rectangle, draw=black, rounded corners,align=left,
    minimum width=2.5em, minimum height=1.2em, s sep=6pt, inner xsep=3pt, inner ysep=1pt,
    , where level=1text width=4.5em, where level=2text width=6em,font=, where level=3font=,
    where level=4font=, where level=5font=, [Application, edge, [Structure Prediction,text
    width=7em, edge, [Protein, text width=5.8em, edge, [ [RGN](https://github.com/aqlaboratory/rgn) [[248](#bib.bib248)],
    [AF2](https://github.com/deepmind/alphafold) [[16](#bib.bib16)], [RoseTTAFold](https://github.com/RosettaCommons/RoseTTAFold) [[240](#bib.bib240)],
    [EigenFold](https://github.com/bjing2016/EigenFold) [[262](#bib.bib262)],
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 对于树=生长=东，反向=true，锚点=基部西，父锚点=东，子锚点=西，基部=中，字体=，矩形，绘制=黑色，圆角，左对齐，最小宽度=2.5em，最小高度=1.2em，s
    sep=6pt，内xsep=3pt，内ysep=1pt，，其中级别=1文本宽度=4.5em，其中级别=2文本宽度=6em，字体=，其中级别=3字体=，其中级别=4字体=，其中级别=5字体=，[应用，边缘，[结构预测，文本宽度=7em，边缘，[蛋白质，文本宽度=5.8em，边缘，[
    [RGN](https://github.com/aqlaboratory/rgn) [[248](#bib.bib248)]，[AF2](https://github.com/deepmind/alphafold)
    [[16](#bib.bib16)]，[RoseTTAFold](https://github.com/RosettaCommons/RoseTTAFold)
    [[240](#bib.bib240)]，[EigenFold](https://github.com/bjing2016/EigenFold) [[262](#bib.bib262)]，
- en: '[ESMFold](https://github.com/facebookresearch/esm) [[13](#bib.bib13)], [OmegaFold](https://github.com/HeliXonProtein/OmegaFold) [[164](#bib.bib164)],
    DMPfold2 [[263](#bib.bib263)], RFAA [[264](#bib.bib264)] ,text width=22.2em, edge]
    ] [Antibody, text width=5.8em, edge [ EquiFold [[255](#bib.bib255)], [IgFold](https://github.com/Graylab/IgFold) [[243](#bib.bib243)]
    ,text width=22.2em, edge] ] [Complex, text width=5.8em, edge [[AlphaFold-Multimer](https://github.com/deepmind/alphafold) [[244](#bib.bib244)],
    ColAttn [[257](#bib.bib257)], GAPN [[258](#bib.bib258)],'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[ESMFold](https://github.com/facebookresearch/esm) [[13](#bib.bib13)]，[OmegaFold](https://github.com/HeliXonProtein/OmegaFold)
    [[164](#bib.bib164)]，DMPfold2[[263](#bib.bib263)]，RFAA[[264](#bib.bib264)]，文本宽度=22.2em，边缘]
    [抗体，文本宽度=5.8em，边缘 [ EquiFold [[255](#bib.bib255)]，[IgFold](https://github.com/Graylab/IgFold)
    [[243](#bib.bib243)]，文本宽度=22.2em，边缘] [复杂，文本宽度=5.8em，边缘 [[AlphaFold-Multimer](https://github.com/deepmind/alphafold)
    [[244](#bib.bib244)]，ColAttn[[257](#bib.bib257)]，GAPN[[258](#bib.bib258)]，'
- en: '[RoseTTAFoldNA](https://github.com/uw-ipd/RoseTTAFold2NA) [[259](#bib.bib259)],
    BiDock [[286](#bib.bib286)], PromptMSP [[287](#bib.bib287)] ,text width=22.2em,
    edge] ] [RNA, text width=5.8em, edge [ [E2Efold-3D](https://github.com/RFOLD/RhoFold) [[246](#bib.bib246)]
    ,text width=22.2em, edge] ] [Conformation, text width=5.8em, edge [ [SPEACH_AF](https://github.com/RSvan/SPEACH_AF) [[247](#bib.bib247)],
    [Atom Transformer](https://github.com/facebookresearch/protein-ebm) [[249](#bib.bib249)],'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[RoseTTAFoldNA](https://github.com/uw-ipd/RoseTTAFold2NA) [[259](#bib.bib259)]、BiDock
    [[286](#bib.bib286)]、PromptMSP [[287](#bib.bib287)]，文本宽度=22.2em，边缘] [RNA，文本宽度=5.8em，边缘
    [ [E2Efold-3D](https://github.com/RFOLD/RhoFold) [[246](#bib.bib246)]，文本宽度=22.2em，边缘]
    ] [构象，文本宽度=5.8em，边缘 [ [SPEACH_AF](https://github.com/RSvan/SPEACH_AF) [[247](#bib.bib247)]、[Atom
    Transformer](https://github.com/facebookresearch/protein-ebm) [[249](#bib.bib249)]，'
- en: MultiSFold [[250](#bib.bib250)], DiffMD [[251](#bib.bib251)], Str2Str [[252](#bib.bib252)]
    ,text width=22.2em, edge] ] [Folding Path, text width=5.8em, edge [ PAthreader [[253](#bib.bib253)],
    Pathfinder [[254](#bib.bib254)] ,text width=22.2em, edge] ] [Refinement, text
    width=5.8em, edge [ EquiFold [[255](#bib.bib255)], DeepACCNet [[256](#bib.bib256)],
    GNNRefine [[185](#bib.bib185)],
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: MultiSFold [[250](#bib.bib250)]、DiffMD [[251](#bib.bib251)]、Str2Str [[252](#bib.bib252)]，文本宽度=22.2em，边缘]
    ] [折叠路径，文本宽度=5.8em，边缘 [ PAthreader [[253](#bib.bib253)]、Pathfinder [[254](#bib.bib254)]，文本宽度=22.2em，边缘]
    ] [精细化，文本宽度=5.8em，边缘 [ EquiFold [[255](#bib.bib255)]、DeepACCNet [[256](#bib.bib256)]、GNNRefine
    [[185](#bib.bib185)]、
- en: ATOMRefine [[193](#bib.bib193)] ,text width=22.2em, edge] ] [Assessment, text
    width=5.8em, edge [ QDistance [[122](#bib.bib122)], DeepUMQA [[124](#bib.bib124)]
    ,text width=22.2em, edge] ] ] [Design,text width=7em, edge, [MSA Generation, text
    width=5.8em, edge [ EvoGen [[260](#bib.bib260)], [MSA-Augmenter](https://github.com/lezhang7/MSA-Augmentor) [[261](#bib.bib261)],
    [PoET](https://github.com/OpenProteinAI/PoET) [[281](#bib.bib281)] ,text width=22.2em,
    edge] ] [Protein Design, text width=5.8em, edge, [ [PiFold](https://github.com/A4Bio/PiFold) [[118](#bib.bib118)],
    [PROTSEED](https://github.com/sokrypton/ColabDesign) [[270](#bib.bib270)], LM-DESIGN [[271](#bib.bib271)],
    [Genie](https://github.com/aqlaboratory/genie) [[272](#bib.bib272)],
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ATOMRefine [[193](#bib.bib193)]，文本宽度=22.2em，边缘] ] [评估，文本宽度=5.8em，边缘 [ QDistance
    [[122](#bib.bib122)]、DeepUMQA [[124](#bib.bib124)]，文本宽度=22.2em，边缘] ] ] [设计，文本宽度=7em，边缘，[MSA
    生成，文本宽度=5.8em，边缘 [ EvoGen [[260](#bib.bib260)]、[MSA-Augmenter](https://github.com/lezhang7/MSA-Augmentor)
    [[261](#bib.bib261)]、[PoET](https://github.com/OpenProteinAI/PoET) [[281](#bib.bib281)]，文本宽度=22.2em，边缘]
    ] [蛋白质设计，文本宽度=5.8em，边缘，[ [PiFold](https://github.com/A4Bio/PiFold) [[118](#bib.bib118)]、[PROTSEED](https://github.com/sokrypton/ColabDesign)
    [[270](#bib.bib270)]、LM-DESIGN [[271](#bib.bib271)]、[Genie](https://github.com/aqlaboratory/genie)
    [[272](#bib.bib272)]
- en: '[IsEM-Pro](https://github.com/JocelynSong/IsEM-Pro) [[273](#bib.bib273)], [FrameDiff](https://github.com/jasonkyuyim/se3_diffusion) [[274](#bib.bib274)],
    [GraDe_IF](https://github.com/ykiiiiii/GraDe_IF) [[282](#bib.bib282)], NOS [[289](#bib.bib289)]
    ,text width=22.2em, edge] ] [Antibody Design, text width=5.8em, edge, [ [dyMEAN](https://github.com/THUNLP-MT/dyMEAN) [[275](#bib.bib275)],
    [AbODE](https://github.com/yogeshverma1998/AbODE_) [[276](#bib.bib276)], AntiDesigner [[277](#bib.bib277)],
    HTP [[278](#bib.bib278)] ,text width=22.2em, edge] ] [DNA/RNA, text width=5.8em,
    edge, [ [BIB](https://github.com/GGchen1997/BIB-ICML2023-Submission) [[279](#bib.bib279)],
    RDesign [[280](#bib.bib280)] ,text width=22.2em, edge] ] [Protein Pocket, text
    width=5.8em, edge, [ [FAIR](https://github.com/zaixizhang/FAIR?tab=readme-ov-file) [[288](#bib.bib288)],
    RFdiffusionAA [[264](#bib.bib264)] ,text width=22.2em, edge] ] ] [Property Prediction,text
    width=7em, edge, [Substructure, text width=5.8em, edge, [ RaptorX-Contact [[236](#bib.bib236)],
    DeepDist [[266](#bib.bib266)] ,text width=22.2em, edge] ] [Function, text width=5.8em,
    edge, [ RaptorX-Property [[265](#bib.bib265)], Domain-PFP [[230](#bib.bib230)],
    Struct2GO [[182](#bib.bib182)] ,text width=22.2em, edge] ] [Mutation Effect, text
    width=5.8em, edge, [ [HotProtein](https://github.com/VITA-Group/HotProtein) [[267](#bib.bib267)],
    SidechainDiff [[268](#bib.bib268)], [RDE](https://github.com/luost26/RDE-PPI) [[269](#bib.bib269)],'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[IsEM-Pro](https://github.com/JocelynSong/IsEM-Pro) [[273](#bib.bib273)]，[FrameDiff](https://github.com/jasonkyuyim/se3_diffusion)
    [[274](#bib.bib274)]，[GraDe_IF](https://github.com/ykiiiiii/GraDe_IF) [[282](#bib.bib282)]，NOS
    [[289](#bib.bib289)]，text width=22.2em，edge] ] [抗体设计，text width=5.8em，edge，[ [dyMEAN](https://github.com/THUNLP-MT/dyMEAN)
    [[275](#bib.bib275)]，[AbODE](https://github.com/yogeshverma1998/AbODE_) [[276](#bib.bib276)]，AntiDesigner
    [[277](#bib.bib277)]，HTP [[278](#bib.bib278)]，text width=22.2em，edge] ] [DNA/RNA，text
    width=5.8em，edge，[ [BIB](https://github.com/GGchen1997/BIB-ICML2023-Submission)
    [[279](#bib.bib279)]，RDesign [[280](#bib.bib280)]，text width=22.2em，edge] ] [蛋白质口袋，text
    width=5.8em，edge，[ [FAIR](https://github.com/zaixizhang/FAIR?tab=readme-ov-file)
    [[288](#bib.bib288)]，RFdiffusionAA [[264](#bib.bib264)]，text width=22.2em，edge]
    ] ] [属性预测，text width=7em，edge，[子结构，text width=5.8em，edge，[ RaptorX-Contact [[236](#bib.bib236)]，DeepDist
    [[266](#bib.bib266)]，text width=22.2em，edge] ] [功能，text width=5.8em，edge，[ RaptorX-Property
    [[265](#bib.bib265)]，Domain-PFP [[230](#bib.bib230)]，Struct2GO [[182](#bib.bib182)]，text
    width=22.2em，edge] ] [突变效应，text width=5.8em，edge，[ [HotProtein](https://github.com/VITA-Group/HotProtein)
    [[267](#bib.bib267)]，SidechainDiff [[268](#bib.bib268)]，[RDE](https://github.com/luost26/RDE-PPI)
    [[269](#bib.bib269)]，'
- en: '[Mutate Everything](https://github.com/jozhang97/MutateEverything) [[290](#bib.bib290)],
    Tranception [[299](#bib.bib299)], PPIRef [[300](#bib.bib300)] ,text width=22.2em,
    edge] ] [Interaction, text width=5.8em, edge, [ [FABind](https://github.com/qizhipei/fabind) [[283](#bib.bib283)],
    NERE DSM [[284](#bib.bib284)], MAPE-PPI [[285](#bib.bib285)] ,text width=22.2em,
    edge] ] ] ]'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[Mutate Everything](https://github.com/jozhang97/MutateEverything) [[290](#bib.bib290)]，Tranception
    [[299](#bib.bib299)]，PPIRef [[300](#bib.bib300)]，text width=22.2em，edge] ] [Interaction，text
    width=5.8em，edge，[ [FABind](https://github.com/qizhipei/fabind) [[283](#bib.bib283)]，NERE
    DSM [[284](#bib.bib284)]，MAPE-PPI [[285](#bib.bib285)]，text width=22.2em，edge]
    ] ] ]'
- en: 'Figure 20: Taxonomy of representative methods for different protein applications.
    The model name is linked with the official GitHub or server page.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：不同蛋白质应用的代表性方法分类。模型名称链接到官方 GitHub 或服务器页面。
- en: 7.2 Protein Design
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 蛋白质设计
- en: Protein design is a field of research that focuses on engineering and creating
    novel proteins with specific structures and functions. It involves modifying existing
    proteins or designing entirely new ones to perform desired tasks, such as enzyme
    catalysis, drug binding, molecular sensing, or protein-based materials. In recent
    years, significant advancements have been made in the field of protein design
    through the application of deep learning techniques. We divide the major works
    into three catogories.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质设计是一个研究领域，专注于工程化和创建具有特定结构和功能的新型蛋白质。它涉及修改现有的蛋白质或设计全新的蛋白质以执行所需的任务，如酶催化、药物结合、分子传感或基于蛋白质的材料。近年来，通过应用深度学习技术，在蛋白质设计领域取得了重大进展。我们将主要工作分为三类。
- en: The first approach involves pre-training a model on a large dataset of sequences.
    The pre-trained model can be utilized to generate novel homologous sequences sharing
    sequence features with that protein family, denoted as MSA generation, which is
    used to enhance the model’s ability to predict structures [[260](#bib.bib260)].
    ProGen [[208](#bib.bib208)] is trained on sequences conditioned on a set of protein
    properties, like function or affiliation with a particular organism, to generate
    desired sequences.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法涉及在大规模序列数据集上对模型进行预训练。预训练的模型可以用于生成与该蛋白质家族共享序列特征的新型同源序列，这被称为MSA生成，用于提高模型预测结构的能力[[260](#bib.bib260)]。ProGen
    [[208](#bib.bib208)]则在一组蛋白质特性（如功能或与特定生物体的关联）条件下进行序列训练，以生成所需的序列。
- en: 'Another crucial problem is to design protein sequences that fold into desired
    structures, namely structure-based protein design [[118](#bib.bib118), [117](#bib.bib117),
    [128](#bib.bib128), [271](#bib.bib271), [272](#bib.bib272), [282](#bib.bib282),
    [291](#bib.bib291), [292](#bib.bib292), [293](#bib.bib293), [294](#bib.bib294),
    [295](#bib.bib295), [296](#bib.bib296)]. Formally, it is to find the amino acid
    sequence $\mathcal{S}=\{s_{i}\}_{i=1,\ldots,n}$ can fold into the desired structure
    $\mathcal{P}=\{P_{i}\}_{i=1,\ldots,n}$, where $P_{i}\in\mathbb{R}^{1\times 3}$,
    $n$ is the number of residues and the natural proteins are composed by 20 types
    of amino acids. It is to learn a deep network having a function $f_{\theta}$:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键问题是设计能够折叠成期望结构的蛋白质序列，即基于结构的蛋白质设计[[118](#bib.bib118), [117](#bib.bib117),
    [128](#bib.bib128), [271](#bib.bib271), [272](#bib.bib272), [282](#bib.bib282),
    [291](#bib.bib291), [292](#bib.bib292), [293](#bib.bib293), [294](#bib.bib294),
    [295](#bib.bib295), [296](#bib.bib296)]。形式上，就是要找到氨基酸序列 $\mathcal{S}=\{s_{i}\}_{i=1,\ldots,n}$
    能够折叠成期望的结构 $\mathcal{P}=\{P_{i}\}_{i=1,\ldots,n}$，其中 $P_{i}\in\mathbb{R}^{1\times
    3}$，$n$ 是残基数量，而自然蛋白质由20种氨基酸组成。这是要学习一个具有函数 $f_{\theta}$ 的深度网络：
- en: '|  | $\mathcal{F}_{\theta}:\mathcal{P}\mapsto\mathcal{S}$ |  | (25) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{F}_{\theta}:\mathcal{P}\mapsto\mathcal{S}$ |  | (25) |'
- en: 'There are also some works [[291](#bib.bib291)] combining the 3D structural
    encoder and 1D sequence decoder, where the protein sequences are generated in
    an autoregressive way:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些工作[[291](#bib.bib291)]结合了3D结构编码器和1D序列解码器，其中蛋白质序列是以自回归方式生成的：
- en: '|  | $p(S\mid\mathcal{P};\theta)=\prod_{i=1}^{n}p\left(s_{i}\mid s_{<i},\mathcal{P};\theta\right)$
    |  | (26) |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(S\mid\mathcal{P};\theta)=\prod_{i=1}^{n}p\left(s_{i}\mid s_{<i},\mathcal{P};\theta\right)$
    |  | (26) |'
- en: Thirdly, generating a novel protein satisfying specified structural or functional
    properties is the task of de novo protein design [[273](#bib.bib273), [274](#bib.bib274),
    [297](#bib.bib297), [298](#bib.bib298)]. For example, in order to design protein
    sequences with desired biological function, such as high fitness, Song et al. [[273](#bib.bib273)]
    develop a Monte Carlo Expectation-Maximization method to learn a latent generative
    model, augmented by combinatorial structure features from a separate learned Markov
    random fields. PROTSEED [[270](#bib.bib270)] learns the joint generation of residue
    types and 3D conformations of a protein with $n$ residues based on context features
    as input to encourage designed proteins to have desired structural properties.
    These context features can be secondary structure annotations, binary contact
    features between residues, etc.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，生成满足指定结构或功能特性的全新蛋白质是de novo蛋白质设计的任务[[273](#bib.bib273), [274](#bib.bib274),
    [297](#bib.bib297), [298](#bib.bib298)]。例如，为了设计具有所需生物功能的蛋白质序列，如高适应性，Song等人[[273](#bib.bib273)]开发了一种蒙特卡洛期望最大化方法，以学习一个潜在的生成模型，并通过来自单独学习的马尔可夫随机场的组合结构特征进行增强。PROTSEED
    [[270](#bib.bib270)] 基于上下文特征作为输入，学习氨基酸类型和具有$n$个残基的蛋白质的3D构象的联合生成，以鼓励设计的蛋白质具有所需的结构特性。这些上下文特征可以是二级结构注释、残基间的二进制接触特征等。
- en: In addition to protein sequence and structure design, some research focuses
    on designing antibody and DNA sequences [[275](#bib.bib275), [279](#bib.bib279)].
    A critical yet challenging task in this domain is the design of the protein pocket,
    the cavity region of the protein where the ligand binds [[288](#bib.bib288)].
    The pocket is essential for molecular recognition and plays a key role in protein
    function. Effective pocket design can enable control over selectivity and affinity
    towards desired ligands.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 除了蛋白质序列和结构设计之外，一些研究还专注于抗体和DNA序列的设计[[275](#bib.bib275), [279](#bib.bib279)]。该领域一个关键而具有挑战性的任务是设计蛋白质口袋，即蛋白质与配体结合的腔体区域[[288](#bib.bib288)]。口袋对分子识别至关重要，并在蛋白质功能中发挥关键作用。有效的口袋设计可以实现对所需配体的选择性和亲和力的控制。
- en: 7.3 Protein Property Prediction
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 蛋白质属性预测
- en: Protein property prediction aims to predict various properties and characteristics
    of proteins, such as solvent accessibility, functions, subcellular localization,
    fitness, etc. The structural properties, like secondary structure and contact
    maps, are useful in other tasks [[237](#bib.bib237)]. For the function prediction,
    there is a group of PRL methods enabling inference about biochemical, cellular,
    systemic or phenotypic functions [[15](#bib.bib15), [59](#bib.bib59)]. Many proteins
    contain intrinsic fluorescent amino acids like tryptophan and tyrosine. Mutations
    can alter the microenvironment of these residues and change the emitted fluorescence
    upon excitation [[145](#bib.bib145)]. Stability landscape prediction estimates
    the impact of mutations on the overall thermodynamic stability of protein structures.
    Stability is quantified by $\Delta\mathrm{G}$, the Gibbs free energy change between
    folded and unfolded states. Mutations disrupting key interactions can undesirably
    destabilize the native state [[145](#bib.bib145), [302](#bib.bib302), [299](#bib.bib299),
    [301](#bib.bib301)], leading to changes in protein properties. Thus, there are
    models are developed to evaluate the mutational effects on PPI identifications [[268](#bib.bib268),
    [269](#bib.bib269), [303](#bib.bib303)]. The protein fitness landscape refers
    to the mapping between genotype (e.g., the amino acid sequence) and phenotype
    (e.g., protein function), which is a fairly broad concept. Models that learn the
    protein fitness landscape are expected to be effective at predicting the effects
    of mutations [[303](#bib.bib303)].
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质属性预测旨在预测蛋白质的各种属性和特征，如溶剂可及性、功能、亚细胞定位、适应性等。结构属性，如二级结构和接触图，在其他任务中也很有用[[237](#bib.bib237)]。对于功能预测，有一组PRL方法可以推断生化、细胞、系统或表型功能[[15](#bib.bib15),
    [59](#bib.bib59)]。许多蛋白质含有内源性荧光氨基酸，如色氨酸和酪氨酸。突变可以改变这些残基的微环境，并改变激发时的发射荧光[[145](#bib.bib145)]。稳定性景观预测估计突变对蛋白质结构总体热力学稳定性的影响。稳定性通过$\Delta\mathrm{G}$来量化，即折叠态和未折叠态之间的吉布斯自由能变化。破坏关键相互作用的突变可能会不利地破坏天然状态[[145](#bib.bib145),
    [302](#bib.bib302), [299](#bib.bib299), [301](#bib.bib301)]，导致蛋白质属性的变化。因此，已经开发了模型来评估突变对PPI识别的影响[[268](#bib.bib268),
    [269](#bib.bib269), [303](#bib.bib303)]。蛋白质适应性景观指的是基因型（例如，氨基酸序列）和表型（例如，蛋白质功能）之间的映射，这是一种相当广泛的概念。学习蛋白质适应性景观的模型预计在预测突变效果方面会有效[[303](#bib.bib303)]。
- en: '8 Discussion: Insights and Future Outlooks'
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 讨论：见解与未来展望
- en: Based on a comprehensive review of fundamental deep learning techniques, protein
    fundamentals, protein model architectures, pretext tasks, and downstream applications,
    we aim to provide deeper perspectives into protein models.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对基础深度学习技术、蛋白质基础、蛋白质模型架构、预任务和下游应用的全面回顾，我们旨在提供对蛋白质模型的更深入视角。
- en: Towards a Generalizable Large-Scale Biological Model
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 朝着可推广的大规模生物模型迈进
- en: While breakthroughs like ChatGPT [[90](#bib.bib90)] have demonstrated remarkable
    success across various domains, there is still a need to develop large-scale models
    tailored to biological data encompassing proteins, DNA, RNA, antibodies, enzymes,
    and more in order to address existing challenges. Roney and Ovchinnikov [[304](#bib.bib304)]
    find that AlphaFold has learned an accurate biophysical energy function and can
    identify low-energy conformations using co-evolutionary information [[305](#bib.bib305)].
    However, some studies have indicated AlphaFold may not reliably predict mutation
    impacts on proteins [[306](#bib.bib306), [307](#bib.bib307), [308](#bib.bib308)].
    Performance on structure-based tasks doesn’t directly transfer to other predictive
    problems[[302](#bib.bib302), [166](#bib.bib166)]. Data-driven methods have attracted
    more attention, but there is no singular optimal model architecture or pretext
    task that generalizes across all data types and downstream applications. Continued
    efforts are imperative to develop versatile, scalable, and interpretable models
    integrating both physical and data sciences for comprehensively tackling biomolecular
    modeling and design across contexts [[92](#bib.bib92)].
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像ChatGPT [[90](#bib.bib90)]这样的突破在多个领域取得了显著成功，但仍需开发针对生物数据的大规模模型，包括蛋白质、DNA、RNA、抗体、酶等，以解决现有的挑战。Roney和Ovchinnikov [[304](#bib.bib304)]发现，AlphaFold已学会了准确的生物物理能量函数，并且可以利用共进化信息识别低能量构象 [[305](#bib.bib305)]。然而，一些研究表明，AlphaFold可能无法可靠地预测突变对蛋白质的影响 [[306](#bib.bib306),
    [307](#bib.bib307), [308](#bib.bib308)]。在结构基础任务上的表现并不能直接转移到其他预测问题上[[302](#bib.bib302),
    [166](#bib.bib166)]。数据驱动的方法吸引了更多关注，但尚未出现一种通用的最优模型架构或前置任务来适用于所有数据类型和下游应用。持续的努力是必要的，以开发出兼具物理和数据科学的多功能、可扩展且可解释的模型，以全面应对不同背景下的生物分子建模和设计 [[92](#bib.bib92)]。
- en: Case by Case
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 个案分析
- en: With ever-expanding protein sequence databases now containing millions of entries,
    protein models have witnessed a commensurate growth in scale, with parameter counts
    in the billions (e.g., ESM-2 [[13](#bib.bib13)] and xTrimoPGLM [[170](#bib.bib170)]).
    However, training such enormous deep learning models currently remains accessible
    only to large corporations with vast computational resources. For instance, DeepMind
    utilized 128 TPU v3 cores over one week to train AF2\. The requirements pose a
    challenge for academic research groups to learn protein representations from scratch
    and also raise environmental sustainability concerns. Given these constraints,
    increased focus on targeted, problem-centric formulations may be prudent. The
    choice of appropriate model architecture and self-supervised learning scheme should
    match dataset attributes and application objectives. This demands careful scrutiny
    of design choices dependent on available inputs and intended predictive utility.
    Furthermore, the vast potential of large pre-trained models remains underexplored
    from the lens of effectively utilizing them under specific problem contexts with
    the integration of prior knowledge.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 随着蛋白质序列数据库的不断扩展，目前已包含数百万条记录，蛋白质模型的规模也相应增长，参数数量达到数十亿（例如，ESM-2 [[13](#bib.bib13)]和xTrimoPGLM [[170](#bib.bib170)]）。然而，训练如此庞大的深度学习模型目前仅限于拥有大量计算资源的大型公司。例如，DeepMind利用128个TPU
    v3核心训练AF2一个星期。这些要求对学术研究团队从头学习蛋白质表示提出了挑战，同时也引发了环境可持续性的关注。鉴于这些限制，增加对有针对性的问题中心公式的关注可能是明智的。适当的模型架构和自监督学习方案应与数据集特性和应用目标匹配。这需要仔细审视设计选择，取决于可用输入和预期的预测效用。此外，大型预训练模型的巨大潜力在特定问题背景下有效利用的角度仍未得到充分探索。
- en: Expanding Multimodal Representation Learning
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩展多模态表示学习
- en: 'ESM-2 [[13](#bib.bib13)] analysis indicates that model performance saturates
    quickly with increasing size for high evolutionary depth, while gains continue
    at low depths with larger models. This exemplifies that appropriately incorporating
    additional modalities like biological and physical priors, MSAs, etc., can reduce
    model scale and improve metrics. As evident in Table [6](#S5.T6 "Table 6 ‣ 5.4
    Assessment of Pre-training Methods for Protein Representation Learning ‣ 5 Deep
    Learning based Protein Models ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey"), combining sequence with structure or functional data
    into models like LM-GVP, GearNet and ProtST-ESM-2 confers improvements over ESM-2
    alone. More extensive exploration into multimodal representation learning is imperative [[46](#bib.bib46)].'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 'ESM-2 [[13](#bib.bib13)] 分析表明，对于高进化深度的模型，性能很快达到饱和，而在较低深度下，大型模型的收益持续增加。这表明，适当地结合生物和物理先验信息、MSAs等附加模态可以减少模型规模并改善指标。如表 [6](#S5.T6
    "Table 6 ‣ 5.4 Assessment of Pre-training Methods for Protein Representation Learning
    ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning in Protein
    Science: A Comprehensive Survey")所示，将序列与结构或功能数据结合到像LM-GVP、GearNet和ProtST-ESM-2这样的模型中，相较于仅使用ESM-2有所改善。对多模态表示学习的更广泛探索是必要的 [[46](#bib.bib46)]。'
- en: Interpretability
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**可解释性**'
- en: Proteins and languages share similarities but also key differences, as discussed
    previously. Most deep learning models currently lack interpretability, obstructing
    insights into underlying protein folding mechanisms. Models like AlphaFold cannot
    furnish detailed characterizations of molecular interactions and chemical principles
    imperative for mechanistic studies and structure-based drug design. Interpretable
    models that reveal grammar-like rules governing proteins would inform impactful
    biomedical applications. Hence, conceptualizing methodologies tailored to the
    nuances of protein data is an urgent priority. Visualization tools that capture
    folding dynamics and functional conformational transitions can powerfully address
    these needs, which would grant researchers the ability to visually traverse the
    atomic trajectories of proteins. Such molecular recordings would waveguide principles
    discovered to be harnessed towards materials and therapeutic innovation.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质和语言有相似之处，但也存在关键差异，如前所述。大多数深度学习模型目前缺乏可解释性，阻碍了对蛋白质折叠机制的深入了解。像AlphaFold这样的模型无法提供分子相互作用和化学原理的详细表征，而这些对于机制研究和基于结构的药物设计至关重要。揭示蛋白质的类似语法规则的可解释模型将有助于影响力巨大的生物医学应用。因此，构思针对蛋白质数据细微差别的方法学是一个紧迫的优先事项。能够捕捉折叠动态和功能性构象变化的可视化工具可以强有力地满足这些需求，使研究人员能够直观地遍历蛋白质的原子轨迹。这种分子记录将指导发现的原理在材料和治疗创新中的应用。
- en: Practical Utility Across Domains
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*跨领域的实用性*'
- en: 'As depicted in Figure [20](#S7.F20 "Figure 20 ‣ 7.1 Protein Structure Prediction
    ‣ 7 Downstream Tasks ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey"), researchers have progressed towards tackling more complex challenges
    such as predicting structures from single sequences, modeling complex assemblies,
    elucidating folding mechanisms, and characterizing protein-ligand interactions [[164](#bib.bib164),
    [13](#bib.bib13), [259](#bib.bib259), [283](#bib.bib283)]. Since mutations can
    precipitate genetic diseases, modeling their functional effects provides insights
    into how sequence constraints structure. Thus, accurately predicting robust structures
    and mutation impacts is imperative [[268](#bib.bib268), [269](#bib.bib269)]. Drug
    design represents a promising avenue for expeditious and economical compound discovery.
    Recent years have witnessed innovations in AI-driven methodologies for identifying
    candidate molecules from huge libraries to bind specific pockets [[309](#bib.bib309),
    [310](#bib.bib310), [311](#bib.bib311)]. Going further, generating enhanced out-of-distribution
    sequences with desirably tuned attributes (like stability) remains an engaging
    prospect [[312](#bib.bib312), [313](#bib.bib313)]. For groups equipped with wet-lab
    capabilities, synergistic combinations of computational predictions and experiments
    offer traction for multifarious problems at the interface of deep learning and
    biology.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[20](#S7.F20 "Figure 20 ‣ 7.1 Protein Structure Prediction ‣ 7 Downstream
    Tasks ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")所示，研究人员在解决更复杂的挑战方面取得了进展，例如从单个序列预测结构、建模复杂组装、阐明折叠机制和表征蛋白质-配体相互作用[[164](#bib.bib164),
    [13](#bib.bib13), [259](#bib.bib259), [283](#bib.bib283)]。由于突变可能引发遗传性疾病，建模其功能效应能够提供序列约束如何影响结构的见解。因此，准确预测稳健的结构和突变影响至关重要[[268](#bib.bib268),
    [269](#bib.bib269)]。药物设计代表了一个有前景的领域，有助于快速且经济地发现化合物。近年来，AI驱动的方法在从大量库中识别候选分子以结合特定口袋方面取得了创新[[309](#bib.bib309),
    [310](#bib.bib310), [311](#bib.bib311)]。更进一步，生成具有理想属性（如稳定性）的增强型分布外序列仍然是一个引人入胜的前景[[312](#bib.bib312),
    [313](#bib.bib313)]。对于具备湿实验室能力的团队来说，计算预测与实验的协同组合为深度学习与生物学交界处的各种问题提供了有力支持。'
- en: Unified Benchmarks and Evaluation Protocols
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 统一基准和评估协议
- en: Amidst an influx of emerging work, comparative assessments are often impeded
    by inconsistencies in datasets, architectures, metrics, and other evaluation factors.
    To enable healthy progress, there is a pressing need to establish standardized
    benchmarking protocols across tasks. Unified frameworks for fair performance analysis
    will consolidate disjoint efforts and clarify model capabilities to distill collective
    progress [[234](#bib.bib234)]. Considering factors like data leakage, bias and
    ethics, some groups develop new datasets tailored to their needs [[314](#bib.bib314)];
    On the other hand, constructing rigorous, reliable and equitable benchmarks remains
    essential for evaluating models and promoting impactful methods. Initiatives like
    TAPE [[145](#bib.bib145)], ProteinGym [[303](#bib.bib303)], ProteinShake [[315](#bib.bib315)],
    PEER [[316](#bib.bib316)], ProteinInvBench [[317](#bib.bib317)], ProteinWorkshop [[318](#bib.bib318)],
    exemplify the critical role of comprehensive benchmarking in furthering innovation.
    Moreover, the Critical Assessment of Protein Structure Prediction (CASP) experiments
    act as a crucial platform for evaluating the most recent advancements in PSP within
    the field, which has been conducted 15 times by mid-December 2022\. Research groups
    from all over the world participate in CASP to objectively test their structure
    prediction methods. By categorizing different themes, like quality assessment,
    domain boundary prediction, and protein complex structure prediction, CASPers
    can identify what progress has been made and highlight the future efforts that
    may be most productively focused on.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在新兴工作的涌入中，比较评估常常受到数据集、架构、指标和其他评估因素不一致的阻碍。为了实现健康的进展，迫切需要在任务之间建立标准化的基准测试协议。统一的框架可以实现公平的性能分析，整合零散的努力，明确模型能力，从而提炼集体进展[[234](#bib.bib234)]。考虑到数据泄露、偏见和伦理等因素，一些研究组开发了适合其需求的新数据集[[314](#bib.bib314)]；另一方面，构建严格、可靠和公平的基准仍然是评估模型和推动有影响力的方法的关键。像
    TAPE [[145](#bib.bib145)]、ProteinGym [[303](#bib.bib303)]、ProteinShake [[315](#bib.bib315)]、PEER
    [[316](#bib.bib316)]、ProteinInvBench [[317](#bib.bib317)] 和 ProteinWorkshop [[318](#bib.bib318)]
    等倡议，展示了全面基准测试在推动创新中的重要作用。此外，蛋白质结构预测的关键评估（CASP）实验作为评估领域内最新进展的关键平台，自2022年12月中旬已进行了15次。来自世界各地的研究组参与
    CASP，以客观测试他们的结构预测方法。通过分类不同的主题，如质量评估、领域边界预测和蛋白质复合物结构预测，CASP 参与者可以识别已取得的进展，并突出未来最值得集中精力的方向。
- en: Protein Structure Prediction in Post-AF2 Era
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 蛋白质结构预测在后AF2时代
- en: 'AF2 marked a significant advancement in protein structure prediction, yet opportunities
    for enhancement persist. Several key strategies can be employed to refine the
    conventional AF2 model. These include diversifying approaches or expanding the
    database to generate more comprehensive MSAs, optimizing template utilization,
    and integrating distances and constraints derived from the AF2 model with alternative
    methods [[27](#bib.bib27)]. Notably, spatial constraints like contact, distance,
    and hydrogen-bond networks have been incorporated into I-TASSER [[319](#bib.bib319)]
    to predict full-length protein structures, achieving superior performance. In
    CASP15, the performance of the top models from server groups closely approached
    and, in some cases, even surpassed that of the human groups. This indicates that
    AI models have reached a stage where they can effectively assimilate and apply
    human knowledge in the field. However, when benchmarked on the human proteome,
    only 36% of residues fall within its highest accuracy tier [[320](#bib.bib320)].
    While approximately 35% of AF2’s predictions rival experimental structures, challenges
    remain in enhancing coverage, and at least 40 teams surpassed AF2 in accuracy
    in CASP15\. Although models completely replacing AF2 have not yet emerged in the
    past two or three years, it has revealed some limitations. For instance, AF2’s
    prediction accuracy on multidomain proteins is not as robust as its accuracy for
    individual domains [[27](#bib.bib27)]. Addressing challenges posed by multidomain
    proteins, including protein complexes, multiple conformational states, and folding
    pathways, may be crucial research directions in the field of PSP, though there
    have appeared works attempting to tackle these problems, as shown in Figure [20](#S7.F20
    "Figure 20 ‣ 7.1 Protein Structure Prediction ‣ 7 Downstream Tasks ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey").'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 'AF2标志着蛋白质结构预测的重大进展，但仍然有改进的机会。可以采用几种关键策略来完善传统的AF2模型。这些策略包括多样化方法或扩展数据库以生成更全面的MSA，优化模板的使用，以及将从AF2模型中得出的距离和约束与其他方法进行整合 [[27](#bib.bib27)]。值得注意的是，像接触、距离和氢键网络这样的空间约束已被纳入I-TASSER [[319](#bib.bib319)]中，以预测全长蛋白质结构，取得了优异的表现。在CASP15中，服务器组的顶尖模型的表现接近甚至在某些情况下超越了人类组。这表明，AI模型已经达到了能够有效吸收和应用领域内人类知识的阶段。然而，当对人类蛋白组进行基准测试时，仅有36%的残基落在其最高准确度层级 [[320](#bib.bib320)]。尽管约35%的AF2预测与实验结构相媲美，但在提升覆盖范围方面仍面临挑战，在CASP15中至少有40个团队在准确度上超越了AF2。虽然过去两三年中尚未出现完全取代AF2的模型，但其揭示了一些局限性。例如，AF2在多域蛋白质上的预测准确度不如在单域蛋白质上的准确度 [[27](#bib.bib27)]。解决多域蛋白质所带来的挑战，包括蛋白质复合体、多种构象状态和折叠途径，可能是蛋白质结构预测领域的重要研究方向，尽管已有一些研究尝试解决这些问题，如图 [20](#S7.F20
    "Figure 20 ‣ 7.1 Protein Structure Prediction ‣ 7 Downstream Tasks ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey")所示。'
- en: Boundary of Large Language Models in Proteins
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**大型语言模型在蛋白质研究中的边界**'
- en: Large language models (LLMs), e.g., ChatGPT [[90](#bib.bib90)], have prevailed
    in NLP [[55](#bib.bib55), [52](#bib.bib52)]. Demis Hassabis, CEO and co-founder
    of DeepMind, has stated that biology can be thought of as an information processing
    system, albeit an extraordinarily complex and dynamic one, just as mathematics
    turned out to be the right description language for physics, biology may turn
    out to be the perfect type of regime for the application of AI. Like the rules
    of grammar would emerge from training an LLM on language samples, the limitations
    dictated by evolution would arise from training the system on samples of proteins.
    There are essentially two families in LLMs, BERT and GPT, which have different
    training objectives and processing methods, as we have stated above [[168](#bib.bib168)].
    Based on the two base models, different LLMs are proposed in protein, like ProtGPT2 [[167](#bib.bib167)],
    ESM-2 [[13](#bib.bib13)], and ProtChatGPT [[321](#bib.bib321)], etc. Researchers
    have tried a range of LLM sizes and found some intriguing facts, for example,
    the ESM-2 [[13](#bib.bib13)] model can get better results when increasing the
    resources, but it is still not clear when it would max out [[322](#bib.bib322)].
    It would be interesting to explore the boundaries of LLMs in proteins.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如ChatGPT [[90](#bib.bib90)]，在自然语言处理（NLP）中占主导地位 [[55](#bib.bib55),
    [52](#bib.bib52)]。DeepMind的首席执行官兼联合创始人Demis Hassabis曾表示，生物学可以被看作是一个信息处理系统，尽管是一个非常复杂和动态的系统，就像数学成为物理的正确描述语言一样，生物学可能会成为应用于人工智能的完美类型体制。就像语法规则会出现在训练LLM处理语言样本上一样，进化所决定的限制将会出现在系统对蛋白质样本的训练中。LLMs基本上分为两大家族，BERT和GPT，它们有不同的训练目标和处理方法，正如我们上述所述 [[168](#bib.bib168)]。基于这两个基础模型，在蛋白质中提出了不同的LLMs，如ProtGPT2 [[167](#bib.bib167)]，ESM-2 [[13](#bib.bib13)]和ProtChatGPT [[321](#bib.bib321)]等。研究人员尝试了一系列LLM的规模，并发现了一些有趣的事实，例如，当资源增加时，ESM-2 [[13](#bib.bib13)]模型能够获得更好的结果，但目前尚不清楚它何时会达到最大值 [[322](#bib.bib322)]。探索LLMs在蛋白质中的边界将是有趣的。
- en: 9 Conclusion
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: This paper presents a systematic overview of pertinent terminologies, notations,
    network architectures, and protein fundamentals, spanning CNNs, LSTMs, GNNs, LMs,
    physicochemical properties, sequence motifs and structural regions, etc. Connections
    between language and protein domains are elucidated, exemplifying model utility
    across applications. Through a comprehensive literature survey, current protein
    models are reviewed, analyzing architectural designs, self-supervised objectives,
    training data and downstream use cases. Limitations and promising directions are
    discussed, covering aspects like multimodal learning, model interpretability,
    and knowledge integration. Overall, this survey aims to orient machine learning
    and biology researchers towards open challenges for enabling the next generation
    of innovations. By condensing progress and perspectives, we hope to crystallize
    collective headway while charting fruitful trails for advancing protein modeling
    and design, elucidating molecular mechanisms, and translating insights into functional
    applications for the benefit of science and society.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 本文系统地概述了相关术语、符号、网络体系结构和蛋白质基础知识，涵盖了CNNs，LSTMs，GNNs，LMs，物理化学性质，序列模式和结构区域等。阐明了语言和蛋白质领域之间的联系，例证了模型跨应用的效用。通过全面的文献调查，审查了当前蛋白质模型，分析了体系结构设计、自监督目标、训练数据和下游使用情况。讨论了局限性和有前途的方向，涵盖了多模态学习、模型可解释性和知识整合等方面。总的来说，这项调查旨在引导机器学习和生物学研究人员面对培育下一代创新的开放挑战。通过总结进展和展望，我们希望在确定有益进展的同时，开辟有成效的道路，推动蛋白质建模和设计的进展，阐明分子机制，并将见解转化为科学和社会的功能应用。
- en: References
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Pleiss J, Fischer M, Peiker M, et al. Lipase engineering database: understanding
    and exploiting sequence–structure–function relationships. Journal of Molecular
    Catalysis B: Enzymatic, 2000, 10: 491-508'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Pleiss J, Fischer M, Peiker M, 等. 脂肪酶工程数据库: 理解和利用序列-结构-功能关系. 分子催化B: 酶类,
    2000, 10: 491-508'
- en: '[2] Gromiha M M. Protein bioinformatics: from sequence to function. academic
    press, 2010'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Gromiha M M. 蛋白质生物信息学: 从序列到功能. 学术出版社, 2010'
- en: '[3] Kryshtafovych A, Schwede B, Topf M, et al. Critical assessment of methods
    of protein structure prediction (CASP)—Round XIII. Proteins: Structure, Function,
    and Bioinformatics, 2019, 87: 1011–1020'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Kryshtafovych A, Schwede B, Topf M, 等. 蛋白质结构预测方法的临界评估（CASP）-第13轮. 蛋白质:
    结构，功能和生物信息学, 2019, 87: 1011–1020'
- en: '[4] Senior A W, Evans R, Jumper J, et al. Improved protein structure prediction
    using potentials from deep learning. Nature, 2020, 577: 706–710'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Senior A W，Evans R，Jumper J，等。使用深度学习的势能改进蛋白质结构预测。Nature，2020，577: 706–710'
- en: '[5] Ikeya T, Güntert P, Ito Y. Protein structure determination in living cells.
    International Journal of Molecular Sciences, 2019, 20: 2442'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ikeya T，Güntert P，Ito Y。活细胞中的蛋白质结构确定。International Journal of Molecular
    Sciences，2019，20: 2442'
- en: '[6] Gauto D F, Estrozi L F, Schwieters C D, et al. Integrated NMR and cryo-EM
    atomic-resolution structure determination of a half-megadalton enzyme complex.
    Nature, 2019, 10: 1-12'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Gauto D F，Estrozi L F，Schwieters C D，等。集成NMR和冷冻电子显微镜的原子分辨率结构确定半兆道尔酶复合物。Nature，2019，10:
    1-12'
- en: '[7] Ashburner M, Ball C A, Blake J A, et al. Gene ontology: tool for the unification
    of biology. Nature genetics, 2000, 25: 25-9'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Ashburner M，Ball C A，Blake J A，等。基因本体：生物学统一工具。Nature genetics，2000，25:
    25-9'
- en: '[8] Protein Data Bank: the single global archive for 3D macromolecular structure
    data. Nucleic Acids Research, 2019, 47: D520–D528'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 蛋白质数据银行：3D大分子结构数据的全球唯一档案。Nucleic Acids Research，2019，47: D520–D528'
- en: '[9] UniProt Consortium. Update on activities at the Universal Protein Resource
    (UniProt) in 2013\. Nucleic Acids Research, 2013, 41: D43-D47'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] UniProt Consortium。关于2013年Universal Protein Resource (UniProt)活动的更新。Nucleic
    Acids Research，2013，41: D43-D47'
- en: '[10] Zhang N, Bi Z, Liang X, et al. Ontoprotein: Protein pretraining with gene
    ontology embedding. arXiv, January 2022'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 张宁，毕智，梁晓，等。Ontoprotein：利用基因本体嵌入进行蛋白质预训练。arXiv，2022年1月'
- en: '[11] Unsal S, Atas H, Albayrak M, et al. Learning functional properties of
    proteins with language models. Nature Machine Intelligence, 2022, 4: 227-45'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Unsal S，Atas H，Albayrak M，等。通过语言模型学习蛋白质的功能特性。Nature Machine Intelligence，2022，4:
    227-45'
- en: '[12] Elnaggar A, Heinzinger M, Dallago C, et al. Prottrans: Towards cracking
    the language of lifes code through self-supervised deep learning and high performance
    computing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Elnaggar A，Heinzinger M，Dallago C，等。Prottrans：通过自监督深度学习和高性能计算破解生命密码。IEEE
    Transactions on Pattern Analysis and Machine Intelligence，2021'
- en: '[13] Lin Z, Akin H, Rao R, et al. Language models of protein sequences at the
    scale of evolution enable accurate structure prediction. bioRxiv, 2022'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Lin Z，Akin H，Rao R，等。进化规模的蛋白质序列语言模型实现准确的结构预测。bioRxiv，2022'
- en: '[14] Rives A, Meier J, Sercu T, et al. Biological structure and function emerge
    from scaling unsupervised learning to 250 million protein sequences. In: Proceedings
    of the National Academy of Sciences, 2021\. 118'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Rives A，Meier J，Sercu T，等。通过将无监督学习扩展到2.5亿个蛋白质序列，生物学结构和功能逐渐显现。见：美国国家科学院学报，2021年，118'
- en: '[15] Zhang Z, Xu M, Jamasb A, et al. Protein representation learning by geometric
    structure pretraining. arXiv, March 2022'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 张志，徐明，贾马斯，等。通过几何结构预训练进行蛋白质表示学习。arXiv，2022年3月'
- en: '[16] Jumper J M, Evans R O, Pritzel A, et al. Highly accurate protein structure
    prediction with AlphaFold. Nature, 2021, 596: 583-589'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Jumper J M，Evans R O，Pritzel A，等。使用AlphaFold进行高精度蛋白质结构预测。Nature，2021，596:
    583-589'
- en: '[17] AlQuraishi M. End-to-end differentiable learning of protein structure.
    Cell systems, 2019, 292–301'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] AlQuraishi M。端到端可微分的蛋白质结构学习。Cell systems，2019，292–301'
- en: '[18] Susanty M, Rajab T E, Hertadi R. A review of protein structure prediction
    using deep learning. In: BIO Web of Conferences, 2021\. 41'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Susanty M，Rajab T E，Hertadi R。使用深度学习的蛋白质结构预测综述。见：BIO Web of Conferences，2021年，41'
- en: '[19] Ma B, Johnson R. De novo sequencing and homology searching. Molecular
    & Cellular Proteomics, 2012, 11: 2'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Ma B，Johnson R。de novo测序和同源搜索。Molecular & Cellular Proteomics，2012，11:
    2'
- en: '[20] Ma B. Novor: real-time peptide de novo sequencing software. Journal of
    the American Society for Mass Spectrometry, 2015, 26: 1885-1894'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Ma B。Novor：实时肽de novo测序软件。Journal of the American Society for Mass Spectrometry，2015，26:
    1885-1894'
- en: '[21] Zheng W, Li Y, Zhang C, et al. Protein structure prediction using deep
    learning distance and hydrogen‐bonding restraints in CASP14\. Proteins: Structure,
    Function, and Bioinformatics, 2021, 89: 1734-1751'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Zheng W，Li Y，Zhang C，等。使用深度学习的距离和氢键约束进行蛋白质结构预测。Proteins: Structure, Function,
    and Bioinformatics，2021，89: 1734-1751'
- en: '[22] Chowdhury R, Bouatta N, Biswas S, et al. Single-sequence protein structure
    prediction using a language model and deep learning. Nature Biotechnology, 2022,
    40: 1617-1623'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Chowdhury R，Bouatta N，Biswas S，等。使用语言模型和深度学习进行单序列蛋白质结构预测。Nature Biotechnology，2022，40:
    1617-1623'
- en: '[23] Wu F, Xu J. Deep template-based protein structure prediction. PLoS computational
    biology, 2021, 17(5): e1008954'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Wu F，Xu J。基于模板的深度蛋白质结构预测。PLoS computational biology，2021，17(5): e1008954'
- en: '[24] Wu F, Jing X, Luo X. et al. Improving protein structure prediction using
    templates and sequence embedding. Bioinformatics, 2023, 39: btac723'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Wu F, Jing X, Luo X. 等. 使用模板和序列嵌入提高蛋白质结构预测。生物信息学，2023，39: btac723'
- en: '[25] Iuchi H, Matsutani T, Yamada K, et al. Representation learning applications
    in biological sequence analysis. Computational and Structural Biotechnology Journal,
    2021, 19: 3198-3208'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Iuchi H, Matsutani T, Yamada K, 等. 生物序列分析中的表示学习应用。计算与结构生物技术杂志，2021，19:
    3198-3208'
- en: '[26] Hu L, Wang X, Huang Y A, et al. A survey on computational models for predicting
    protein–protein interactions. Briefings in bioinformatics, 2021, 22: bbab036'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Hu L, Wang X, Huang Y A, 等. 预测蛋白质–蛋白质相互作用的计算模型综述。生物信息学简报，2021，22: bbab036'
- en: '[27] Peng C X, Liang F, Xia Y H, et al. Recent Advances and Challenges in Protein
    Structure Prediction. Journal of Chemical Information and Modeling, 2023'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Peng C X, Liang F, Xia Y H, 等. 蛋白质结构预测的近期进展与挑战。化学信息与建模杂志，2023'
- en: '[28] Ihm Y. A threading approach to protein structure prediction: studies on
    TNF-like molecules, Rev proteins, and protein kinases. Iowa State University,
    2004'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Ihm Y. 蛋白质结构预测的线程方法：对TNF样分子、Rev蛋白和蛋白质激酶的研究。爱荷华州立大学，2004'
- en: '[29] Patel M, Shah H. Protein secondary structure prediction using support
    vector machines (svms). In: International Conference on Machine Intelligence and
    Research Advancement, IEEE, 2013\. 594-598'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Patel M, Shah H. 使用支持向量机（svms）预测蛋白质二级结构。载：国际机器智能与研究进展会议，IEEE，2013\. 594-598'
- en: '[30] Sanger F. The arrangement of amino acids in proteins. Advances in protein
    chemistry, 1952, 7: 1-67'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Sanger F. 蛋白质中氨基酸的排列。蛋白质化学进展，1952，7: 1-67'
- en: '[31] Hao X H, Zhang G J, Zhou X G. Conformational space sampling method using
    multi-subpopulation differential evolution for de novo protein structure prediction.
    IEEE Transactions on NanoBioscience, 2017, 16: 618-33'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Hao X H, Zhang G J, Zhou X G. 使用多亚群体差分进化的构象空间采样方法进行新蛋白质结构预测。IEEE纳米生物科学学报，2017，16:
    618-33'
- en: '[32] Wang S, Peng J, Ma J, et al. Protein secondary structure prediction using
    deep convolutional neural fields. Scientific reports, 2016, 6: 1-11'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Wang S, Peng J, Ma J, 等. 使用深度卷积神经场预测蛋白质二级结构。科学报告，2016，6: 1-11'
- en: '[33] Chou K C, Cai Y D. Predicting protein quaternary structure by pseudo amino
    acid composition. Proteins: Structure, Function, and Bioinformatics, 2003, 53:
    282–289'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Chou K C, Cai Y D. 通过伪氨基酸组成预测蛋白质四级结构。蛋白质：结构、功能与生物信息学，2003，53: 282–289'
- en: '[34] Koonin E V. Orthologs, paralogs, and evolutionary genomics. Annual review
    of genetics, 2005, 39: 309–338'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Koonin E V. 正交基因、旁系基因与进化基因组学。遗传学年鉴，2005，39: 309–338'
- en: '[35] Wang Y, Wu H, Cai Y. A benchmark study of sequence alignment methods for
    protein clustering. BMC bioinformatics, 2018, 19: 95–104'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Wang Y, Wu H, Cai Y. 蛋白质聚类的序列比对方法基准研究。BMC生物信息学，2018，19: 95–104'
- en: '[36] Ochoa D, Pazos F. Practical aspects of protein co-evolution. Frontiers
    in cell and developmental biology, 2014, 2: 14'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Ochoa D, Pazos F. 蛋白质共同进化的实际方面。细胞与发育生物学前沿，2014，2: 14'
- en: '[37] Emerson I A, Amala A. Protein contact maps: a binary depiction of protein
    3D structures. Physica A: Statistical Mechanics and its Applications, 2017, 465:
    782-91'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Emerson I A, Amala A. 蛋白质接触图：蛋白质三维结构的二值描述。物理学A：统计力学及其应用，2017，465: 782-91'
- en: '[38] Basile W, Sachenkova O, Light S, et al. High gc content causes orphan
    proteins to be intrinsically disordered. PLoS computational biology, 2017, 13:
    3'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Basile W, Sachenkova O, Light S, 等. 高GC含量导致孤儿蛋白具有内在的无序性。PLoS计算生物学，2017，13:
    3'
- en: '[39] Blackstock J C. Guide to Biochemistry. Butterworth-Heinemann, 2014'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Blackstock J C. 生物化学指南。Butterworth-Heinemann，2014'
- en: '[40] Li S, Chua T S, Zhu J, et al. Generative topic embedding: a continuous
    representation of documents. In: Proceedings of the 54th Annual Meeting of the
    Association for Computational Linguistics, 2016\. 666–675'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Li S, Chua T S, Zhu J, 等. 生成主题嵌入：文档的连续表示。载：第54届计算语言学协会年会论文集，2016\. 666–675'
- en: '[41] Lin Z, Feng M, Santos C N, et al. A structured self-attentive sentence
    embedding. arXiv, March 2017'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Lin Z, Feng M, Santos C N, 等. 结构化自注意句子嵌入。arXiv，2017年3月'
- en: '[42] Weiss K, Khoshgoftaar T M, Wang D. A survey of transfer learning. Journal
    of Big data, 2016, 3: 1-40'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Weiss K, Khoshgoftaar T M, Wang D. 转移学习综述。大数据杂志，2016，3: 1-40'
- en: '[43] Pan S J, Yang Q. A survey on transfer learning. IEEE Transactions on knowledge
    and data engineering, 2010, 22: 1345-1359'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Pan S J, Yang Q. 转移学习综述。IEEE知识与数据工程学报，2010，22: 1345-1359'
- en: '[44] Bond-Taylor S, Leach A, Long Y, et al. Deep generative modelling: A comparative
    review of VAEs, GANs, normalizing flows, energy-based and autoregressive models.
    arXiv, March 2021'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Bond-Taylor S, Leach A, Long Y, 等. 深度生成建模：变分自编码器、生成对抗网络、归一化流、基于能量的模型和自回归模型的比较综述.
    arXiv, 2021年3月'
- en: '[45] Jahan M S, Khan H U, Akbar S, et al. Bidirectional Language Modeling:
    A Systematic Literature Review. Scientific Programming, 2021'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Jahan M S, Khan H U, Akbar S, 等. 双向语言建模：系统文献综述. 科学编程, 2021'
- en: '[46] Bepler T, Berger B. Learning the protein language: Evolution, structure,
    and function. Cell systems, 2021, 12: 654-669'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Bepler T, Berger B. 学习蛋白质语言：进化、结构与功能. 细胞系统, 2021, 12: 654-669'
- en: '[47] Gou J, Yu B, Maybank SJ, et al. Knowledge distillation: A survey. International
    Journal of Computer Vision, 2021, 129: 1789-1819'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Gou J, Yu B, Maybank SJ, 等. 知识蒸馏：综述. 国际计算机视觉杂志, 2021, 129: 1789-1819'
- en: '[48] Skocaj D, Leonardis A, Kruijff GJ. Cross-modal learning. Encyclopedia
    of the Sciences of Learning, 2012, 861-864'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Skocaj D, Leonardis A, Kruijff GJ. 跨模态学习. 学习科学百科全书, 2012, 861-864'
- en: '[49] Wang H, Zhang J, Chen Y, et al. Uncertainty-aware multi-modal learning
    via cross-modal random network prediction. arxiv, July 2022'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Wang H, Zhang J, Chen Y, 等. 通过跨模态随机网络预测的带不确定性感知的多模态学习. arxiv, 2022年7月'
- en: '[50] He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks.
    In: European conference on computer vision, Springer, Cham, 2016\. 630-645'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] He K, Zhang X, Ren S, 等. 深度残差网络中的身份映射. 见：计算机视觉欧洲会议, Springer, Cham, 2016.
    630-645'
- en: '[51] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. Advances
    in neural information processing systems. 2017, 30'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Vaswani A, Shazeer N, Parmar N, 等. 注意力机制是你所需要的一切. 神经信息处理系统进展. 2017, 30'
- en: '[52] Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional
    transformers for language understanding. arxiv, October 2018'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Devlin J, Chang M W, Lee K, 等. Bert：深度双向变换器的预训练用于语言理解. arxiv, 2018年10月'
- en: '[53] Radford A, Narasimhan K, Salimans T, et al. Improving language understanding
    by generative pre-training. 2018'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Radford A, Narasimhan K, Salimans T, 等. 通过生成预训练改善语言理解. 2018'
- en: '[54] Radford A, Wu J, Child R, et al. Language models are unsupervised multitask
    learners. OpenAI blog. 2019, 1: 9'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Radford A, Wu J, Child R, 等. 语言模型是无监督的多任务学习者. OpenAI博客. 2019, 1: 9'
- en: '[55] Brown T, Mann B, Ryder N, et al. Language models are few-shot learners.
    Advances in neural information processing systems, 2020, 33: 1877-1901'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Brown T, Mann B, Ryder N, 等. 语言模型是少样本学习者. 神经信息处理系统进展, 2020, 33: 1877-1901'
- en: '[56] Lindsay, G W. Convolutional neural networks as a model of the visual system:
    Past, present, and future. Journal of cognitive neuroscience, 2021, 33: 2017-2031'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Lindsay, G W. 卷积神经网络作为视觉系统的模型：过去、现在和未来. 认知神经科学杂志, 2021, 33: 2017-2031'
- en: '[57] Kriegeskorte N. Deep neural networks: a new framework for modeling biological
    vision and brain information processing. Annual review of vision science, 2015,
    1: 417-446'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Kriegeskorte N. 深度神经网络：用于建模生物视觉和大脑信息处理的新框架. 视觉科学年鉴, 2015, 1: 417-446'
- en: '[58] Chartrand G, Cheng P M, Vorontsov E, et al. Deep learning: a primer for
    radiologists. Radiographics, 2017, 37: 2113-2131'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Chartrand G, Cheng P M, Vorontsov E, 等. 深度学习：放射科医生的入门指南. 放射学, 2017, 37:
    2113-2131'
- en: '[59] Fan H, Wang Z, Yang Y, et al. Continuous-Discrete Convolution for Geometry-Sequence
    Modeling in Proteins. In: The Eleventh International Conference on Learning Representations,
    2022'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Fan H, Wang Z, Yang Y, 等. 用于蛋白质几何序列建模的连续-离散卷积. 见：第十一届国际学习表征会议, 2022'
- en: '[60] Hayes B. First links in the Markov chain. American Scientist, 2013, 101:
    252'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Hayes B. 马尔可夫链中的首个链接. 美国科学家, 2013, 101: 252'
- en: '[61] Li H. Language models: past, present, and future. Communications of the
    ACM, 2022, 65: 56-63'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Li H. 语言模型：过去、现在与未来. ACM通讯, 2022, 65: 56-63'
- en: '[62] Bishop M, Thompson E A. Maximum likelihood alignment of dna sequences.
    Journal of molecular biology, 1986, 190: 159–165'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Bishop M, Thompson E A. DNA序列的最大似然比对. 分子生物学杂志, 1986, 190: 159–165'
- en: '[63] Chiu J T, Rush A M. Scaling hidden markov language models. arxiv, November
    2020'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Chiu J T, Rush A M. 扩展隐藏马尔可夫语言模型. arxiv, 2020年11月'
- en: '[64] Stigler J, Ziegler F, Gieseke A, et al. The complex folding network of
    single calmodulin molecules. Science, 2011, 334: 512–516'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Stigler J, Ziegler F, Gieseke A, 等. 单一钙调蛋白分子的复杂折叠网络. 科学, 2011, 334: 512–516'
- en: '[65] Wong K C, Chan T M, Peng C, et al. DNA motif elucidation using belief
    propagation. Nucleic Acids Research, 2013, 41: e153–e153'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Wong K C, Chan T M, Peng C, 等. 使用置信传播的DNA基序阐明. 核酸研究, 2013, 41: e153–e153'
- en: '[66] Ferruz N, Höcker B, Controllable protein design with language models.
    Nature Machine Intelligence, 2022, 1-12'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Ferruz N, Höcker B, 使用语言模型的可控蛋白质设计。Nature Machine Intelligence, 2022,
    1-12'
- en: '[67] AGMLS F, Bunke R, Schmiduber J. A novel connectionist system for improved
    unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis Machine
    Intelligence, 2009, 31: 5'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] AGMLS F, Bunke R, Schmiduber J。一种新颖的连接主义系统，用于改进无约束手写识别。IEEE Transactions
    on Pattern Analysis Machine Intelligence, 2009, 31: 5'
- en: '[68] De S, Smith S L, Fernando A, et al. Griffin: Mixing Gated Linear Recurrences
    with Local Attention for Efficient Language Models. arxiv, February 2024'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] De S, Smith S L, Fernando A 等人。Griffin: 结合门控线性递归和局部注意力以提高语言模型效率。arxiv,
    2024年2月'
- en: '[69] Hochreiter S. Untersuchungen zu dynamischen neuronalen netzen. Diploma,
    Technische Universität München, 1991, 91: 1'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Hochreiter S。动态神经网络的研究。Diploma, 慕尼黑工业大学, 1991, 91: 1'
- en: '[70] Lample G, Ballesteros M, Subramanian S, et al. Neural architectures for
    named entity recognition. arxiv, March 2016'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Lample G, Ballesteros M, Subramanian S 等人。用于命名实体识别的神经网络架构。arxiv, 2016年3月'
- en: '[71] Schmidhuber J, Wierstra D, Gomez F J. Evolino: Hybrid neuroevolution/optimal
    linear search for sequence prediction. In: Proceedings of the 19th International
    Joint Conferenceon Artificial Intelligence (IJCAI), 2005'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Schmidhuber J, Wierstra D, Gomez F J。Evolino：用于序列预测的混合神经进化/最优线性搜索。在：第19届国际联合人工智能会议（IJCAI）论文集，2005年'
- en: '[72] Hochreiter S, Heusel M, Obermayer K. Fast model-based protein homology
    detection without alignment. Bioinformatics, 2007, 23: 1728–1736'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Hochreiter S, Heusel M, Obermayer K。快速基于模型的蛋白质同源检测，无需对齐。Bioinformatics,
    2007, 23: 1728–1736'
- en: '[73] Gupta A, Müller A T, Huisman B J, et al. Generative recurrent networks
    for de novo drug design. Molecular informatics, 2018, 37: 1700111'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Gupta A, Müller A T, Huisman B J 等人。用于新药设计的生成递归网络。Molecular informatics,
    2018, 37: 1700111'
- en: '[74] Ma C, Dai G, Zhou J. Short-term traffic flow prediction for urban road
    sections based on time series analysis and LSTM_BILSTM method. IEEE Transactions
    on Intelligent Transportation Systems, 2021'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Ma C, Dai G, Zhou J。基于时间序列分析和LSTM_BILSTM方法的城市道路段短期交通流预测。IEEE Transactions
    on Intelligent Transportation Systems, 2021'
- en: '[75] Radford A, Jozefowicz R, Sutskever I. Learning to generate reviews and
    discovering sentiment. arxiv, April 2017'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Radford A, Jozefowicz R, Sutskever I。学习生成评论和发现情感。arxiv, 2017年4月'
- en: '[76] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning
    to align and translate. arxiv, September 2014'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Bahdanau D, Cho K, Bengio Y。通过联合学习对齐和翻译进行神经机器翻译。arxiv, 2014年9月'
- en: '[77] Han X, Zhang Z, Ding N, et al. Pre-trained models: Past, present and future.
    AI Open, 2021, 2: 225-50.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Han X, Zhang Z, Ding N 等人。预训练模型：过去、现在和未来。AI Open, 2021, 2: 225-50'
- en: '[78] Tan Q, Liu N, Hu X. Deep representation learning for social network analysis.
    Frontiers in big Data, 2019, 2: 2'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Tan Q, Liu N, Hu X。社交网络分析的深度表示学习。Frontiers in Big Data, 2019, 2: 2'
- en: '[79] Wu S, Sun F, Zhang W, et al. Graph neural networks in recommender systems:
    a survey. ACM Computing Surveys, 2022, 55: 1-37'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Wu S, Sun F, Zhang W 等人。推荐系统中的图神经网络：综述。ACM Computing Surveys, 2022, 55:
    1-37'
- en: '[80] Deng J, Yang Z, Ojima I, et al. Artificial intelligence in drug discovery:
    applications and techniques. Briefings in Bioinformatics, 2022, 23: bbab430'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Deng J, Yang Z, Ojima I 等人。人工智能在药物发现中的应用与技术。Briefings in Bioinformatics,
    2022, 23: bbab430'
- en: '[81] Fensel D, Şimşek U, Angele K, et al. Introduction: what is a knowledge
    graph?. Knowledge Graphs, Springer, Cham, 2020, 1-10'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Fensel D, Şimşek U, Angele K 等人。简介：什么是知识图谱？。知识图谱，Springer, Cham, 2020,
    1-10'
- en: '[82] Kipf T N, Welling M. Semi-supervised classification with graph convolutional
    networks. arxiv, September 2016'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Kipf T N, Welling M。图卷积网络的半监督分类。arxiv, 2016年9月'
- en: '[83] Veličković P, Cucurull G, Casanova A, et al. Graph attention networks.
    arxiv, October 2017'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Veličković P, Cucurull G, Casanova A 等人。图注意力网络。arxiv, 2017年10月'
- en: '[84] Oh J, Cho K, Bruna J. Advancing graphsage with a data-driven node sampling.
    arxiv, April 2019'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Oh J, Cho K, Bruna J。通过数据驱动的节点采样提升GraphSAGE。arxiv, 2019年4月'
- en: '[85] Thrun S, Pratt L. Learning to learn: Introduction and overview. In: Learning
    to learn, 1998\. 3-17'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Thrun S, Pratt L。学习学习：简介和概述。在：《学习学习》，1998年。3-17'
- en: '[86] Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining
    approach. arxiv, July 2019'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Liu Y, Ott M, Goyal N 等人。RoBERTa：一种强健优化的BERT预训练方法。arxiv, 2019年7月'
- en: '[87] Yang Z, Dai Z, Yang Y, et al. Xlnet: Generalized autoregressive pretraining
    for language understanding. Advances in neural information processing systems.
    2019, 32'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Yang Z, Dai Z, Yang Y 等人。XLNet：用于语言理解的广义自回归预训练。神经信息处理系统进展。2019, 32'
- en: '[88] Kaplan J, McCandlish S, Henighan T, et al. Scaling laws for neural language
    models. arxiv, January 2020'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Kaplan J, McCandlish S, Henighan T, 等. 神经语言模型的尺度定律。arxiv，2020年1月'
- en: '[89] Hoffmann J, Borgeaud S, Mensch A, et al. Training Compute-Optimal Large
    Language Models. arxiv, March 2022'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Hoffmann J, Borgeaud S, Mensch A, 等. 训练计算优化的大型语言模型。arxiv，2022年3月'
- en: '[90] Perlman AM. The Implications of OpenAI’s Assistant for Legal Services
    and Society. Available at SSRN. 2022'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Perlman AM. OpenAI助手对法律服务和社会的影响。可在SSRN获取。2022年'
- en: '[91] Araci D. Finbert: Financial sentiment analysis with pre-trained language
    models. arxiv, August 2019'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Araci D. Finbert：使用预训练语言模型的金融情感分析。arxiv，2019年8月'
- en: '[92] Wang B, Xie Q, Pei J, et al. Pre-trained language models in biomedical
    domain: A systematic survey. arxiv, October 2021'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Wang B, Xie Q, Pei J, 等. 生物医学领域中的预训练语言模型：系统综述。arxiv，2021年10月'
- en: '[93] Bengio Y, Ducharme R, Vincent P. A neural probabilistic language model.
    Advances in neural information processing systems, 2000, 13'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Bengio Y, Ducharme R, Vincent P. 一种神经概率语言模型。神经信息处理系统进展，2000年，13'
- en: '[94] Mikolov T, Chen K, Corrado G S, et al. Efficient estimation of word representations
    in vector space. In: International conference on learning representations, 2013'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Mikolov T, Chen K, Corrado G S, 等. 向量空间中词表示的高效估计。在：国际学习表征会议，2013年'
- en: '[95] Lee J, Yoon W, Kim S, et al. Biobert: a pre-trained biomedical language
    representation model for biomedical text mining. Bioinformatics, 2020, 36: 1234–1240'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Lee J, Yoon W, Kim S, 等. Biobert：用于生物医学文本挖掘的预训练生物医学语言表示模型。生物信息学，2020年，36:
    1234–1240'
- en: '[96] Young T, Hazarika D, Poria S, et al. Recent trends in deep learning based
    natural language processing. IEEE Computational intelligenCe magazine, 2018, 13:
    55-75'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Young T, Hazarika D, Poria S, 等. 基于深度学习的自然语言处理的最新趋势。IEEE计算智能杂志，2018年，13:
    55-75'
- en: '[97] Yang K K, Wu Z, Bedbrook C N, et al. Learned protein embeddings for machine
    learning. Bioinformatics, 2018, 34: 2642-2648'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Yang K K, Wu Z, Bedbrook C N, 等. 用于机器学习的学习型蛋白质嵌入。生物信息学，2018年，34: 2642-2648'
- en: '[98] Asgari E, Mofrad M R. Continuous distributed representation of biological
    sequences for deep proteomics and genomics. PloS one, 2015, 10: 11'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Asgari E, Mofrad M R. 生物序列的连续分布表示用于深度蛋白组学和基因组学。PloS one，2015年，10: 11'
- en: '[99] Ofer D, Brandes N, Linial M. The language of proteins: NLP, machine learning
    & protein sequences. Computational and Structural Biotechnology Journal, 2021,
    19:1750-1758'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Ofer D, Brandes N, Linial M. 蛋白质的语言：自然语言处理、机器学习与蛋白质序列。计算与结构生物技术杂志，2021年，19:1750-1758'
- en: '[100] Bryngelson J D, Onuchic J N, Socci N D, et al. Funnels, pathways, and
    the energy landscape of protein folding: a synthesis. Proteins: Structure, Function,
    and Bioinformatics, 1995, 21: 167-95'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Bryngelson J D, Onuchic J N, Socci N D, 等. 蛋白质折叠的漏斗、路径和能量景观：综合分析。蛋白质：结构、功能与生物信息学，1995年，21:
    167-95'
- en: '[101] Leopold P E, Montal M, Onuchic J N. Protein folding funnels: a kinetic
    approach to the sequence-structure relationship. In: Proceedings of the National
    Academy of Sciences, 1992, 8721–8725'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Leopold P E, Montal M, Onuchic J N. 蛋白质折叠漏斗：一种序列-结构关系的动力学方法。在：美国国家科学院院刊，1992年，8721–8725'
- en: '[102] Zerihun MB, Schug A. Biomolecular Structure Prediction via Coevolutionary
    Analysis: A Guide to the Statistical Framework. In: NIC Symposium, 2018\. FZJ-2018-02966'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Zerihun MB, Schug A. 通过共进化分析进行生物分子结构预测：统计框架指南。在：NIC研讨会，2018年。FZJ-2018-02966'
- en: '[103] Vorberg S. Bayesian statistical approach for protein residue-residue
    contact prediction. Doctoral dissertation, lmu, 2017'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Vorberg S. 用于蛋白质残基-残基接触预测的贝叶斯统计方法。博士论文，lmu，2017年'
- en: '[104] Choshen L, Abend O. Automatically Extracting Challenge Sets for Non local
    Phenomena in Neural Machine Translation. arxiv, September 2019'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Choshen L, Abend O. 自动提取神经机器翻译中非局部现象的挑战集。arxiv，2019年9月'
- en: '[105] Alley EC, Khimulya G, Biswas S, et al. Unified rational protein engineering
    with sequence-based deep representation learning. Nature methods. 2019, 16: 1315-1322'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Alley EC, Khimulya G, Biswas S, 等. 基于序列的深度表示学习的统一理性蛋白质工程。自然方法，2019年，16:
    1315-1322'
- en: '[106] Madani A, Krause B, Greene ER, et al. Deep neural language modeling enables
    functional protein generation across families. bioRxiv, 2021'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Madani A, Krause B, Greene ER, 等. 深度神经语言建模实现跨家族的功能性蛋白质生成。bioRxiv，2021年'
- en: '[107] Ofer D, Brandes N, Linial M. The language of proteins: NLP, machine learning
    & protein sequences. Computational and Structural Biotechnology Journal, 2021,
    19: 1750-1758'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Ofer D, Brandes N, Linial M. 蛋白质的语言：自然语言处理、机器学习与蛋白质序列。计算与结构生物技术杂志，2021年，19:
    1750-1758'
- en: '[108] Jia J, Liu Z, Xiao X, et al. Identification of protein-protein binding
    sites by incorporating the physicochemical properties and stationary wavelet transforms
    into pseudo amino acid composition. Journal of Biomolecular Structure and Dynamics,
    2016, 34: 1946-1961'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Jia J, Liu Z, Xiao X 等. 通过将物理化学性质和静态小波变换纳入伪氨基酸组成来识别蛋白质-蛋白质结合位点。生物分子结构与动态杂志，2016，34：1946-1961'
- en: '[109] Xu G, Wang Q, Ma J. OPUS-Rota4: a gradient-based protein side-chain modeling
    framework assisted by deep learning-based predictors. Briefings in Bioinformatics,
    23: bbab529'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Xu G, Wang Q, Ma J. OPUS-Rota4：一种基于梯度的蛋白质侧链建模框架，辅以深度学习预测器。生物信息学简报，23：bbab529'
- en: '[110] Hanson J, Paliwal K, Litfin T, et al. Improving prediction of protein
    secondary structure, backbone angles, solvent accessibility and contact numbers
    by using predicted contact maps and an ensemble of recurrent and residual convolutional
    neural networks. Bioinformatics, 2019, 35: 2403–2410'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Hanson J, Paliwal K, Litfin T 等. 通过使用预测接触图和递归及残差卷积神经网络集成来改进蛋白质二级结构、骨架角度、溶剂可接触性和接触数的预测。生物信息学，2019，35：2403–2410'
- en: '[111] Gao Z, Jiang C, Zhang J, et al. Hierarchical graph learning for protein–protein
    interaction. Nature Communications, 2023, 14: 1093'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Gao Z, Jiang C, Zhang J 等. 针对蛋白质–蛋白质相互作用的分层图学习。自然通讯，2023，14：1093'
- en: '[112] Johansson M U. Defining and searching for structural motifs using DeepView/Swiss-PdbViewer.
    BMC Bioinformatics, 2012, 13: 173'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Johansson M U. 使用DeepView/Swiss-PdbViewer定义和搜索结构基序。BMC生物信息学，2012，13：173'
- en: '[113] Xu D, Nussinov R. Favorable domain size in proteins. Folding & Design,
    1998, 3: 11–7'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Xu D, Nussinov R. 蛋白质中的有利结构域大小。折叠与设计，1998，3：11–7'
- en: '[114] Hu F, Hu Y, Zhang W, et al. A Multimodal Protein Representation Framework
    for Quantifying Transferability Across Biochemical Downstream Tasks. Advanced
    Science, 2023, 2301223'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Hu F, Hu Y, Zhang W 等. 用于量化生化下游任务可转移性的多模态蛋白质表示框架。先进科学，2023，2301223'
- en: '[115] Wang Z, Zhang Q, Shuang-Wei H U, et al. Multi-level Protein Structure
    Pre-training via Prompt Learning. In: The Eleventh International Conference on
    Learning Representations, 2022'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Wang Z, Zhang Q, Shuang-Wei H U 等. 通过提示学习进行多级蛋白质结构预训练。第十一届国际学习表征会议，2022'
- en: '[116] Wang L, Liu H, Liu Y, et al. Learning hierarchical protein representations
    via complete 3d graph networks. In: The Eleventh International Conference on Learning
    Representations, 2022'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Wang L, Liu H, Liu Y 等. 通过完整的3D图网络学习层次化蛋白质表示。第十一届国际学习表征会议，2022'
- en: '[117] Ingraham J, Garg V, Barzilay R, et al. Generative models for graph-based
    protein design. Advances in neural information processing systems, 2019, 32'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Ingraham J, Garg V, Barzilay R 等. 基于图的蛋白质设计的生成模型。神经信息处理系统进展，2019，32'
- en: '[118] Gao Z, Tan C, Li S Z. PiFold: Toward effective and efficient protein
    inverse folding. In: ICLR, 2023'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Gao Z, Tan C, Li S Z. PiFold：朝向有效和高效的蛋白质逆折叠。ICLR，2023'
- en: '[119] Hu B, Tan C, Xia J, et al. Learning Complete Protein Representation by
    Deep Coupling of Sequence and Structure. bioRxiv, 2023'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Hu B, Tan C, Xia J 等. 通过深度耦合序列和结构学习完整的蛋白质表示。bioRxiv，2023'
- en: '[120] Yang J, Anishchenko I, Park H, et al. Improved protein structure prediction
    using predicted inter-residue orientations. In: Proceedings of the National Academy
    of Sciences of the United States of America, 2019'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Yang J, Anishchenko I, Park H 等. 使用预测的残基间方向改进蛋白质结构预测。美国国家科学院院刊会议录，2019'
- en: '[121] Du Z, Su H, Wang W, et al. The trRosetta server for fast and accurate
    protein structure prediction. Nature protocols, 2021, 16: 5634-5651'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Du Z, Su H, Wang W 等. trRosetta 服务器用于快速准确的蛋白质结构预测。自然协议，2021，16：5634-5651'
- en: '[122] Ye L, Wu P, Peng Z, et al. Improved estimation of model quality using
    predicted inter-residue distance. Bioinformatics, 2021, 37: 3752-3759'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Ye L, Wu P, Peng Z 等. 利用预测的残基间距离改进模型质量估计。生物信息学，2021，37：3752-3759'
- en: '[123] Tischer D, Lisanza S, Wang J, et al. Design of proteins presenting discontinuous
    functional sites using deep learning. bioRxiv, 2020'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Tischer D, Lisanza S, Wang J 等. 使用深度学习设计呈现不连续功能位点的蛋白质。bioRxiv，2020'
- en: '[124] Guo S S, Liu J, Zhou X G, et al. DeepUMQA: ultrafast shape recognition-based
    protein model quality assessment using deep learning. Bioinformatics, 2022, 38:
    1895-1903'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Guo S S, Liu J, Zhou X G 等. DeepUMQA：基于超快形状识别的蛋白质模型质量评估，采用深度学习。生物信息学，2022，38：1895-1903'
- en: '[125] Gross E H, Meienhofer J. The Peptide Bond. Major Methods of Peptide Bond
    Formation: The Peptides Analysis, Synthesis, Biology, 2014, 1: 1'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Gross E H, Meienhofer J. 肽键. 肽键形成的主要方法: 肽的分析、合成、生物学, 2014年, 1: 1'
- en: '[126] Nelson D L, Lehninger A L, Cox M M. Lehninger principles of biochemistry.
    Macmillan. 2008'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Nelson D L, Lehninger A L, Cox M M. Lehninger生物化学原理. Macmillan. 2008年'
- en: '[127] Vollhardt K P C, Schore N E. Organic chemistry: structure and function.
    Macmillan. 2003'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Vollhardt K P C, Schore N E. 有机化学: 结构与功能. Macmillan. 2003年'
- en: '[128] Jing B, Eismann S, Suriana P, et al. Learning from protein structure
    with geometric vector perceptrons. arxiv, September 2020'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Jing B, Eismann S, Suriana P 等人. 通过几何向量感知器学习蛋白质结构. arxiv, 2020年9月'
- en: '[129] Fuchs F, Worrall D, Fischer V, et al. Se (3)-transformers: 3d roto-translation
    equivariant attention networks. Advances in Neural Information Processing Systems,
    2020, 33: 1970-1981'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Fuchs F, Worrall D, Fischer V 等人. Se (3)-变换器: 3D旋转平移等变注意力网络. 神经信息处理系统进展,
    2020年, 33: 1970-1981'
- en: '[130] Du W, Zhang H, Du Y. et al. SE (3) Equivariant Graph Neural Networks
    with Complete Local Frames. In: International Conference on Machine Learning,
    2022\. 5583–5608'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Du W, Zhang H, Du Y 等人. SE (3) 等变图神经网络与完整局部框架. 在: 国际机器学习会议, 2022年. 5583–5608'
- en: '[131] Liu S, Du W, Li Y, et al. Symmetry-Informed Geometric Representation
    for Molecules, Proteins, and Crystalline Materials. arxiv, June 2023'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Liu S, Du W, Li Y 等人. 面向分子、蛋白质和晶体材料的对称性信息几何表示. arxiv, 2023年6月'
- en: '[132] Liu D, Chen S, Zheng S, et al. SE (3) Equivalent Graph Attention Network
    as an Energy-Based Model for Protein Side Chain Conformation. bioRxiv, 2022'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Liu D, Chen S, Zheng S 等人. SE (3) 等价图注意力网络作为蛋白质侧链构象的能量模型. bioRxiv, 2022年'
- en: '[133] Krapp L F, Abriata L A, Cortés Rodriguez F, et al. PeSTo: parameter-free
    geometric deep learning for accurate prediction of protein binding interfaces.
    Nature Communications, 2023, 14: 2175'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Krapp L F, Abriata L A, Cortés Rodriguez F 等人. PeSTo: 无参数几何深度学习用于准确预测蛋白质结合界面.
    自然通讯, 2023年, 14: 2175'
- en: '[134] Liu Y, Wang L, Liu M, et al. Spherical message passing for 3d molecular
    graphs. In: International Conference on Learning Representations, 2021'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Liu Y, Wang L, Liu M 等人. 用于3D分子图的球面消息传递. 在: 国际学习表征会议, 2021年'
- en: '[135] Wang L, Liu Y, Lin Y, et al. ComENet: Towards Complete and Efficient
    Message Passing for 3D Molecular Graphs. arxiv, June 2022'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Wang L, Liu Y, Lin Y 等人. ComENet: 面向3D分子图的完整且高效的消息传递. arxiv, 2022年6月'
- en: '[136] Xu M, Yuan X, Miret S, et al. Protst: Multi-modality learning of protein
    sequences and biomedical texts. arxiv, January 2023'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Xu M, Yuan X, Miret S 等人. Protst: 蛋白质序列和生物医学文本的多模态学习. arxiv, 2023年1月'
- en: '[137] Klausen M S, Jespersen M C, Nielsen H, et al. Netsurfp-2.0: improved
    prediction of protein structural features by integrated deep learning. Proteins,
    2018'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Klausen M S, Jespersen M C, Nielsen H 等人. Netsurfp-2.0: 通过集成深度学习提高蛋白质结构特征预测.
    蛋白质, 2018年'
- en: '[138] Heffernan R, Paliwal K, Lyons J, et al. Single‐sequence‐based prediction
    of protein secondary structures and solvent accessibility by deep whole‐sequence
    learning. Journal of computational chemistry, 2018, 39: 2210-2216'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Heffernan R, Paliwal K, Lyons J 等人. 基于单序列的蛋白质二级结构和溶剂可及性的深度学习预测. 计算化学杂志,
    2018年, 39: 2210-2216'
- en: '[139] Almagro Armenteros J J, Johansen A R, Winther O, et al. Language modelling
    for biological sequences–curated datasets and baselines. bioRxiv, 2020'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Almagro Armenteros J J, Johansen A R, Winther O 等人. 生物序列的语言建模–策划的数据集和基线.
    bioRxiv, 2020年'
- en: '[140] Asgari E, Poerner N, McHardy A C, et al. Deepprime2sec: Deep learning
    for protein secondary structure prediction from the primary sequences. bioRxiv,
    2019'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Asgari E, Poerner N, McHardy A C 等人. Deepprime2sec: 基于深度学习的蛋白质二级结构预测.
    bioRxiv, 2019年'
- en: '[141] Singh J, Litfin T, Paliwal K, et al. SPOT-1D-Single: improving the single-sequence-based
    prediction of protein secondary structure, backbone angles, solvent accessibility
    and half-sphere exposures using a large training set and ensembled deep learning.
    Bioinformatics, 2021, 37:3464-3472'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Singh J, Litfin T, Paliwal K 等人. SPOT-1D-Single: 使用大型训练集和集成深度学习提高基于单序列的蛋白质二级结构、主链角度、溶剂可及性和半球暴露预测.
    生物信息学, 2021年, 37:3464-3472'
- en: '[142] Sinai S, Kelsic E, Church G M, et al. Variational auto-encoding of protein
    sequences. arxiv, December 2017'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Sinai S, Kelsic E, Church G M 等人. 蛋白质序列的变分自编码. arxiv, 2017年12月'
- en: '[143] Ding X, Zou Z, Brooks III C L. Deciphering protein evolution and fitness
    landscapes with latent space models. Nature communications, 2019, 10: 5644'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Ding X, Zou Z, Brooks III C L. 使用潜在空间模型解码蛋白质进化和适应性景观. 自然通讯, 2019年, 10:
    5644'
- en: '[144] Krause B, Lu L, Murray I, et al. Multiplicative LSTM for sequence modelling.
    arxiv, September 2016'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Krause B, Lu L, Murray I, 等. 用于序列建模的乘法LSTM. arxiv, 2016年9月'
- en: '[145] Rao R, Bhattacharya N, Thomas N, et al. Evaluating protein transfer learning
    with TAPE. Advances in neural information processing systems, 2019, 32'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Rao R, Bhattacharya N, Thomas N, 等. 使用TAPE评估蛋白质迁移学习. 神经信息处理系统进展, 2019年,
    32'
- en: '[146] Lan Z, Chen M, Goodman S, et al. Albert: A lite bert for self-supervised
    learning of language representations. arxiv, September 2019'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Lan Z, Chen M, Goodman S, 等. Albert: 一种轻量级BERT用于自监督语言表示学习. arxiv, 2019年9月'
- en: '[147] Dai Z, Yang Z, Yang Y, et al. Transformer-xl: Attentive language models
    beyond a fixed-length context. arxiv, January 2019'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Dai Z, Yang Z, Yang Y, 等. Transformer-xl: 超越固定长度上下文的注意力语言模型. arxiv, 2019年1月'
- en: '[148] Clark K, Luong M T, Le Q V, et al. Electra: Pre-training text encoders
    as discriminators rather than generators. arxiv, March 2020'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Clark K, Luong M T, Le Q V, 等. Electra: 作为鉴别器而非生成器的文本编码器预训练. arxiv, 2020年3月'
- en: '[149] Raffel C, Shazeer N, Roberts A, et al. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of Machine Learning
    Research, 2020, 21: 1–67'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Raffel C, Shazeer N, Roberts A, 等. 探索使用统一文本到文本变换器的迁移学习极限. 机器学习研究期刊, 2020年,
    21: 1–67'
- en: '[150] Heinzinger M, Elnaggar A, Wang Y, et al. Modeling the language of life-deep
    learning protein sequences. bioRxiv, 2019'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Heinzinger M, Elnaggar A, Wang Y, 等. 模拟生命语言-深度学习蛋白质序列. bioRxiv, 2019年'
- en: '[151] Peters ME, Neumann M, Iyyer M, et al. Deep contextualized word representations.
    North American chapter of the association for computational linguistics, 2018'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Peters ME, Neumann M, Iyyer M, 等. 深度上下文化词表示. 北美计算语言学协会年会, 2018年'
- en: '[152] Strodthoff N, Wagner P, Wenzel M, et al. UDSMProt: universal deep sequence
    models for protein classification. Bioinformatics, 2020, 36: 2401–2409'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Strodthoff N, Wagner P, Wenzel M, 等. UDSMProt: 用于蛋白质分类的通用深度序列模型. 生物信息学,
    2020年, 36: 2401–2409'
- en: '[153] Lu A X, Zhang H, Ghassemi M, et al. Self-supervised contrastive learning
    of protein representations by mutual information maximization. bioRxiv, 2020'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Lu A X, Zhang H, Ghassemi M, 等. 通过互信息最大化的自监督对比学习蛋白质表示. bioRxiv, 2020年'
- en: '[154] Dey R, Salem F M. Gate-variants of gated recurrent unit (GRU) neural
    networks. In: 2017 IEEE 60th international midwest symposium on circuits and systems
    (MWSCAS), IEEE, 2017\. 1597-1600'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Dey R, Salem F M. 门控递归单元(GRU)神经网络的门控变体. 在：2017年IEEE第60届国际中西部电路与系统研讨会(MWSCAS),
    IEEE, 2017年. 1597-1600'
- en: '[155] Zhou G, Chen M, Ju C J, et al. Mutation effect estimation on protein–protein
    interactions using deep contextualized representation learning. NAR genomics and
    bioinformatics, 2020, 2: lqaa015'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Zhou G, Chen M, Ju C J, 等. 使用深度上下文化表示学习对蛋白质-蛋白质相互作用的突变效应估计. NAR基因组学与生物信息学,
    2020年, 2: lqaa015'
- en: '[156] Szklarczyk D, Gable A L, Lyon D, et al. STRING v11: protein–protein association
    networks with increased coverage, supporting functional discovery in genome-wide
    experimental datasets. Nucleic acids research, 2019, 47: D607-D613'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Szklarczyk D, Gable A L, Lyon D, 等. STRING v11: 增加覆盖的蛋白质-蛋白质关联网络，支持基因组范围实验数据集中的功能发现.
    核酸研究, 2019年, 47: D607-D613'
- en: '[157] Sturmfels P, Vig J, Madani A, et al. Profile prediction: An alignment-based
    pre-training task for protein sequence models. arxiv, December 2020'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Sturmfels P, Vig J, Madani A, 等. 侧标预测：一种基于对齐的预训练任务用于蛋白质序列模型. arxiv, 2020年12月'
- en: '[158] Nambiar A, Heflin M, Liu S, et al. Transforming the language of life:
    transformer neural networks for protein prediction tasks. In: Proceedings of the
    11th ACM international conference on bioinformatics, computational biology and
    health informatics, 2020\. 1-8'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Nambiar A, Heflin M, Liu S, 等. 转变生命语言：用于蛋白质预测任务的变换器神经网络. 在：第11届ACM国际生物信息学、计算生物学与健康信息学会议论文集,
    2020年. 1-8'
- en: '[159] He L, Zhang S, Wu L, et al. Pre-training co-evolutionary protein representation
    via a pairwise masked language model. arxiv, October 2021'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] He L, Zhang S, Wu L, 等. 通过配对掩码语言模型进行预训练的共进化蛋白质表示. arxiv, 2021年10月'
- en: '[160] Rao R M, Liu J, Verkuil R, et al. Msa transformer. In: International
    Conference on Machine Learning, 2021\. 8844-8856'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Rao R M, Liu J, Verkuil R, 等. Msa transformer. 在：国际机器学习会议, 2021年. 8844-8856'
- en: '[161] Xiao Y, Qiu J, Li Z, et al. Modeling protein using large-scale pretrain
    language model. arxiv, August 2021'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Xiao Y, Qiu J, Li Z, 等. 使用大规模预训练语言模型建模蛋白质. arxiv, 2021年8月'
- en: '[162] Min S, Park S, Kim S, et al. Pre-training of deep bidirectional protein
    sequence representations with structural information. IEEE Access, 2021, 9: 123912-123926'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Min S，Park S，Kim S，等。利用结构信息对深度双向蛋白质序列表示进行预训练。IEEE Access，2021，9：123912-123926'
- en: '[163] Yang K K, Fusi N, Lu A X. Convolutions are competitive with transformers
    for protein sequence pretraining. bioRxiv, 2022'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Yang K K，Fusi N，Lu A X。卷积在蛋白质序列预训练中与 Transformer 相竞争。bioRxiv，2022'
- en: '[164] Wu R, Ding F, Wang R, et al. High-resolution de novo structure prediction
    from primary sequence. bioRxiv, 2022'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] 吴睿，丁飞，王睿，等。基于原始序列的高分辨率 de novo 结构预测。bioRxiv，2022'
- en: '[165] Hua W, Dai Z, Liu H, et al. Transformer quality in linear time. In: International
    Conference on Machine Learning, PMLR, 2022\. 9099-9117'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] 华伟，戴子，刘辉，等。线性时间下的 Transformer 质量。在：国际机器学习会议，PMLR，2022\. 9099-9117'
- en: '[166] Nijkamp E, Ruffolo J, Weinstein E N, et al. ProGen2: exploring the boundaries
    of protein language models. arxiv, June 2022'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Nijkamp E，Ruffolo J，Weinstein E N，等。ProGen2：探索蛋白质语言模型的边界。arxiv，2022年6月'
- en: '[167] Ferruz N, Schmidt S, Höcker B. A deep unsupervised language model for
    protein design. bioRxiv, 2022'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Ferruz N，Schmidt S，Höcker B。一种用于蛋白质设计的深度无监督语言模型。bioRxiv，2022'
- en: '[168] Radford A, Wu J, Child R, et al. Language models are unsupervised multitask
    learners. OpenAI blog, 2019, 1: 9'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Radford A，吴静，Child R，等。语言模型是无监督的多任务学习者。OpenAI 博客，2019，1：9'
- en: '[169] Hesslow D, Zanichelli N, Notin P, et al. Rita: a study on scaling up
    generative protein sequence models. arxiv, May 2022'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Hesslow D，Zanichelli N，Notin P，等。Rita：关于扩展生成蛋白质序列模型的研究。arxiv，2022年5月'
- en: '[170] Chen B, Cheng X, Geng Y A, et al. xTrimoPGLM: unified 100B-scale pre-trained
    transformer for deciphering the language of protein. bioRxiv, 2023'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] 陈博，程晓，耿雅，等。xTrimoPGLM：解读蛋白质语言的统一100B规模预训练 Transformer。bioRxiv，2023'
- en: '[171] Melnyk I, Chenthamarakshan V, Chen P Y, et al. Reprogramming pretrained
    language models for antibody sequence infilling. In: International Conference
    on Machine Learning, PMLR, 2023\. 24398-24419'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Melnyk I，Chenthamarakshan V，陈培玉，等。重新编程预训练语言模型以填充抗体序列。在：国际机器学习会议，PMLR，2023\.
    24398-24419'
- en: '[172] Truong, T F, Bepler T. PoET: A generative model of protein families as
    sequences-of-sequences. arxiv, June 2023'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Truong T F，Bepler T。PoET：作为序列的序列的蛋白质家族生成模型。arxiv，2023年6月'
- en: '[173] Emaad K, Yun S S, Aaron A, et al. CELL-E2: Translating Proteins to Pictures
    and Back with a Bidirectional Text-to-Image Transformer. In: Thirty-seventh Conference
    on Neural Information Processing Systems, 2023'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Emaad K，尹胜石，Aaron A，等。CELL-E2：用双向文本到图像 Transformer 将蛋白质翻译为图像再翻译回来。在：第37届神经信息处理系统会议，2023'
- en: '[174] Andreas D, Cecilia L. The Human Protein AtlasSpatial localization of
    the human proteome in health and disease. Protein Science, 2021, 30: 218–233'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Andreas D，Cecilia L。人类蛋白质图谱：健康与疾病中人类蛋白质组的空间定位。蛋白质科学，2021，30：218–233'
- en: '[175] Zhang Y, Skolnick J. Tm-align: a protein structure alignment algorithm
    based on the tm-score. Nucleic Acids Research, 2005'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] 张宇，Skolnick J。Tm-align：一种基于 tm-score 的蛋白质结构比对算法。核酸研究，2005'
- en: '[176] Thomas J, Ramakrishnan N, Bailey-Kellogg, C. Graphical models of residue
    coupling in protein families. In: Proceedings of the 5th international workshop
    on Bioinformatics, 2005\. 12-20'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Thomas J，Ramakrishnan N，Bailey-Kellogg C。蛋白质家族中残基耦合的图模型。在：第5届国际生物信息学研讨会论文集，2005\.
    12-20'
- en: '[177] Vassura M, Margara L, Di Lena P, et al. Reconstruction of 3D Structures
    From Protein Contact Maps. IEEE/ACM Transactions on Computational Biology and
    Bioinformatics, 2008, 5: 357–367'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] Vassura M，Margara L，Di Lena P，等。从蛋白质接触图重建三维结构。IEEE/ACM 计算生物学与生物信息学交易，2008，5：357–367'
- en: '[178] Wu Q, Peng Z, Anishchenko I, et al. Protein contact prediction using
    metagenome sequence data and residual neural networks. Bioinformatics, 2020, 36:
    41-48'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] 吴琦，彭志，Anishchenko I，等。利用宏基因组序列数据和残差神经网络进行蛋白质接触预测。生物信息学，2020，36：41-48'
- en: '[179] Derevyanko G, Grudinin S, Bengio Y, et al. Deep convolutional networks
    for quality assessment of protein folds. Bioinformatics, 2018, 34: 4046-4053'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Derevyanko G，Grudinin S，Bengio Y，等。用于蛋白质折叠质量评估的深度卷积网络。生物信息学，2018，34：4046-4053'
- en: '[180] Maddhuri Venkata Subramaniya S R, Terashi G, Jain A, et al. Protein contact
    map refinement for improving structure prediction using generative adversarial
    networks. Bioinformatics, 2021, 37: 3168-3174'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Maddhuri Venkata Subramaniya S R，Terashi G，Jain A，等。利用生成对抗网络改进结构预测的蛋白质接触图精炼。生物信息学，2021，37：3168-3174'
- en: '[181] Sverrisson F, Feydy J, Correia B E, et al. Fast end-to-end learning on
    protein surfaces. In: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2021\. 15272-15281'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Sverrisson F, Feydy J, Correia B E 等. 蛋白质表面的快速端到端学习. 见: IEEE/CVF计算机视觉与模式识别大会论文集,
    2021\. 15272-15281'
- en: '[182] Jiao P, Wang B, Wang X, et al. Struct2GO: protein function prediction
    based on graph pooling algorithm and AlphaFold2 structure information. Bioinformatics,
    2023, 39: btad637'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Jiao P, Wang B, Wang X 等. Struct2GO: 基于图池化算法和AlphaFold2结构信息的蛋白质功能预测.
    Bioinformatics, 2023, 39: btad637'
- en: '[183] Gelman S, Fahlberg S A, Heinzelman P, et al. Neural networks to learn
    protein sequence–function relationships from deep mutational scanning data. Proceedings
    of the National Academy of Sciences, 2021, 118: e2104878118'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Gelman S, Fahlberg S A, Heinzelman P 等. 利用深度突变扫描数据学习蛋白质序列-功能关系的神经网络.
    Proceedings of the National Academy of Sciences, 2021, 118: e2104878118'
- en: '[184] Xia T, Ku W S. Geometric Graph Representation Learning on Protein Structure
    Prediction. In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
    & amp; Data Mining, 2021'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Xia T, Ku W S. 蛋白质结构预测中的几何图表示学习. 见: 第27届ACM SIGKDD知识发现与数据挖掘大会论文集, 2021'
- en: '[185] Jing X, Xu J. Fast and effective protein model refinement using deep
    graph neural networks. Nature computational science, 2021, 1: 462-469'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Jing X, Xu J. 使用深度图神经网络的快速有效蛋白质模型细化. Nature computational science, 2021,
    1: 462-469'
- en: '[186] Hermosilla P, Schäfer M, Lang M, et al. Intrinsic-extrinsic convolution
    and pooling for learning on 3d protein structures. In: ICLR, 2021'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Hermosilla P, Schäfer M, Lang M 等. 用于3D蛋白质结构学习的内在-外在卷积和池化. 见: ICLR, 2021'
- en: '[187] Cheng S, Zhang L, Jin B, et al. GraphMS: Drug Target Prediction Using
    Graph Representation Learning with Substructures. Appl. Sci. 2021, 11: 3239'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Cheng S, Zhang L, Jin B 等. GraphMS: 使用图表示学习和子结构的药物靶点预测. Appl. Sci. 2021,
    11: 3239'
- en: '[188] Wan F, Hong L, Xiao A, et al. NeoDTI: neural integration of neighbor
    information from a heterogeneous network for discovering new drug–target interactions.
    Bioinformatics, 2019, 35: 104-111'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Wan F, Hong L, Xiao A 等. NeoDTI: 神经集成来自异质网络的邻居信息以发现新的药物-靶点相互作用. Bioinformatics,
    2019, 35: 104-111'
- en: '[189] Hermosilla P, Ropinski T. Contrastive representation learning for 3d
    protein structures. arxiv, May 2022'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Hermosilla P, Ropinski T. 用于3D蛋白质结构的对比表示学习. arxiv, 2022年5月'
- en: '[190] Chen C, Zhou J, Wang F, et al. Structure-aware protein self-supervised
    learning. arxiv, April 2022'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Chen C, Zhou J, Wang F 等. 结构感知的蛋白质自监督学习. arxiv, 2022年4月'
- en: '[191] Li J, Luo S, Deng C. Directed weight neural networks for protein structure
    representation learning. arxiv, January 2022'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] Li J, Luo S, Deng C. 用于蛋白质结构表示学习的定向权重神经网络. arxiv, 2022年1月'
- en: '[192] Aykent S, Xia T. Gbpnet: Universal geometric representation learning
    on protein structures. In: Proceedings of the 28th ACM SIGKDD Conference on Knowledge
    Discovery and Data Mining, 2022\. 4-14'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Aykent S, Xia T. Gbpnet: 蛋白质结构上的通用几何表示学习. 见: 第28届ACM SIGKDD知识发现与数据挖掘大会论文集,
    2022\. 4-14'
- en: '[193] Wu T, Cheng J. Atomic protein structure refinement using all-atom graph
    representations and SE (3)-equivariant graph neural networks. bioRxiv, 2022'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Wu T, Cheng J. 使用全原子图表示和SE (3)-等变图神经网络的原子蛋白质结构细化. bioRxiv, 2022'
- en: '[194] Satorras V G, Hoogeboom E, Welling M. E (n) equivariant graph neural
    networks. In: International conference on machine learning. PMLR, 2021\. 9323-9332'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Satorras V G, Hoogeboom E, Welling M. E (n) 等变图神经网络. 见: 国际机器学习大会. PMLR,
    2021\. 9323-9332'
- en: '[195] Bepler T, Berger B. Learning protein sequence embeddings using information
    from structure. arxiv, February 2019'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Bepler T, Berger B. 使用结构信息学习蛋白质序列嵌入. arxiv, 2019年2月'
- en: '[196] Wang Z, Combs S A, Brand R, et al. LM-GVP: A Generalizable Deep Learning
    Framework for Protein Property Prediction from Sequence and Structure. bioRxiv,
    2021'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Wang Z, Combs S A, Brand R 等. LM-GVP: 一种可泛化的深度学习框架，用于从序列和结构中预测蛋白质属性.
    bioRxiv, 2021'
- en: '[197] Gligorijević V, Renfrew P D, Kosciolek T, et al. Structure-based protein
    function prediction using graph convolutional networks. Nature communications,
    2021, 12: 3168'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Gligorijević V, Renfrew P D, Kosciolek T 等. 基于结构的蛋白质功能预测使用图卷积网络. Nature
    communications, 2021, 12: 3168'
- en: '[198] Mansoor S, Baek M, Madan U, et al. Toward more general embeddings for
    protein design: Harnessing joint representations of sequence and structure. bioRxiv,
    2021'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] Mansoor S, Baek M, Madan U 等. 朝着更通用的蛋白质设计嵌入: 利用序列和结构的联合表示. bioRxiv, 2021'
- en: '[199] Anishchenko I, Baek M, Park H, et al. Protein tertiary structure prediction
    and refinement using deep learning and Rosetta in CASP14\. Proteins: Structure,
    Function, and Bioinformatics, 2021, 89: 1722-1733'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] Anishchenko I, Baek M, Park H 等。使用深度学习和Rosetta在CASP14中预测和优化蛋白质三级结构。蛋白质：结构、功能与生物信息学，2021，89：1722-1733'
- en: '[200] Xia C, Feng S H, Xia Y, et al. Fast protein structure comparison through
    effective representation learning with contrastive graph neural networks. PLoS
    computational biology, 2022, 18: e1009986'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] Xia C, Feng S H, Xia Y 等。通过有效的对比图神经网络进行快速蛋白质结构比较。PLoS计算生物学，2022，18：e1009986'
- en: '[201] Ke Z, Shao Y, Lin H, et al. Continual Pre-training of Language Models.
    In: ICLR, 2023'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Ke Z, Shao Y, Lin H 等。语言模型的持续预训练。发表于：ICLR，2023'
- en: '[202] He K, Fan H, Wu Y, et al. Momentum contrast for unsupervised visual representation
    learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2020'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] He K, Fan H, Wu Y 等。动量对比用于无监督视觉表示学习。发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2020'
- en: '[203] You Y, Shen Y. Cross-modality and self-supervised protein embedding for
    compound–protein affinity and contact prediction. Bioinformatics, 2022, 38: ii68-ii74'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] You Y, Shen Y。跨模态和自监督蛋白质嵌入用于化合物-蛋白质亲和力和接触预测。生物信息学，2022，38：ii68-ii74'
- en: '[204] Yang K K, Zanichelli N, Yeh H. Masked inverse folding with sequence transfer
    for protein representation learning. bioRxiv, 2022'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Yang K K, Zanichelli N, Yeh H。通过序列迁移进行掩蔽反折叠的蛋白质表示学习。bioRxiv，2022'
- en: '[205] Zhang, Z, Wang C, Xu, M, et al. A Systematic Study of Joint Representation
    Learning on Protein Sequences and Structures. arxiv, March 2023'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Zhang, Z, Wang C, Xu, M 等。蛋白质序列和结构的联合表示学习的系统研究。arxiv，2023年3月'
- en: '[206] Zhang Z, Xu M, Lozano A, et al. Pre-Training Protein Encoder via Siamese
    Sequence-Structure Diffusion Trajectory Prediction. In: Annual Conference on Neural
    Information Processing Systems, 2023'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] Zhang Z, Xu M, Lozano A 等。通过孪生序列-结构扩散轨迹预测的蛋白质编码器预训练。发表于：神经信息处理系统年会，2023'
- en: '[207] Su J, Han C, Zhou Y, et al. SaProt: Protein Language Modeling with Structure-aware
    Vocabulary. bioRxiv, 2023'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Su J, Han C, Zhou Y 等。SaProt：具有结构感知词汇的蛋白质语言建模。bioRxiv，2023'
- en: '[208] Madani A, McCann B, Naik N, et al. Progen: Language modeling for protein
    generation. arxiv, April 2020'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Madani A, McCann B, Naik N 等。Progen：用于蛋白质生成的语言建模。arxiv，2020年4月'
- en: '[209] Federhen, S. The NCBI Taxonomy database. Nucleic Acids Research, 2012,
    40: D136–D143'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Federhen, S. NCBI分类数据库。核酸研究，2012，40：D136–D143'
- en: '[210] Brandes N, Ofer D, Peleg Y, et al. ProteinBERT: A universal deep-learning
    model of protein sequence and function. Bioinformatics, 2022, 38: 2102-10'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] Brandes N, Ofer D, Peleg Y 等。ProteinBERT：一种通用的蛋白质序列和功能深度学习模型。生物信息学，2022，38：2102-10'
- en: '[211] Zhou H Y, Fu Y, Zhang Z, et al. Protein Representation Learning via Knowledge
    Enhanced Primary Structure Modeling. bioRxiv, 2023'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Zhou H Y, Fu Y, Zhang Z 等。通过知识增强的主结构建模进行蛋白质表示学习。bioRxiv，2023'
- en: '[212] Lee Y, Yu H, Lee J, et al. Pre-training Sequence, Structure, and Surface
    Features for Comprehensive Protein Representation Learning. In: ICLR, 2024'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] Lee Y, Yu H, Lee J 等。预训练序列、结构和表面特征用于全面的蛋白质表示学习。发表于：ICLR，2024'
- en: '[213] Gu Y, Tinn R, Cheng H, et al. Domain-Specific Language Model Pretraining
    for Biomedical Natural Language Processing. ACM Transactions on Computing for
    Healthcare, 2022: 1–23'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] Gu Y, Tinn R, Cheng H 等。用于生物医学自然语言处理的领域特定语言模型预训练。ACM计算医疗事务，2022：1–23'
- en: '[214] Rose P W, Prlić A, Altunkaya A, et al. The RCSB protein data bank: integrative
    view of protein, gene and 3D structural information. Nucleic acids research, 2016:
    gkw1000'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] Rose P W, Prlić A, Altunkaya A 等。RCSB蛋白质数据银行：蛋白质、基因和3D结构信息的综合视图。核酸研究，2016：gkw1000'
- en: '[215] Suzek B E, Wang Y, Huang H, et al. UniRef clusters: a comprehensive and
    scalable alternative for improving sequence similarity searches. Bioinformatics,
    2015, 31: 926-932'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] Suzek B E, Wang Y, Huang H 等。UniRef簇：改进序列相似性搜索的全面且可扩展的替代方案。生物信息学，2015，31：926-932'
- en: '[216] Orengo C A, Michie A D, Jones S, et al. CATH–a hierarchic classification
    of protein domain structures. Structure, 1997, 5: 1093-1109'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] Orengo C A, Michie A D, Jones S 等。CATH——蛋白质结构域的分级分类。结构，1997，5：1093-1109'
- en: '[217] Steinegger M, Söding J. Clustering huge protein sequence sets in linear
    time. Nature communications, 2018, 9: 1-8'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Steinegger M, Söding J。线性时间内对庞大蛋白质序列集进行聚类。自然通讯，2018，9：1-8'
- en: '[218] El-Gebali S, Mistry J, Bateman A, et al. The Pfam protein families database
    in 2019\. Nucleic Acids Research, 2019, 47: D427-D432'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] El-Gebali S, Mistry J, Bateman A 等. 2019年 Pfam 蛋白质家族数据库。核酸研究，2019年，47：D427-D432'
- en: '[219] Varadi M, Anyango S, Deshpande M, et al. AlphaFold Protein Structure
    Database: massively expanding the structural coverage of protein-sequence space
    with high-accuracy models. Nucleic acids research, 2022, 50: D439-D444'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Varadi M, Anyango S, Deshpande M 等. AlphaFold 蛋白质结构数据库：用高精度模型大规模扩展蛋白质序列空间的结构覆盖。核酸研究，2022年，50：D439-D444'
- en: '[220] Mirdita M, Von Den Driesch L, Galiez C, et al. Uniclust databases of
    clustered and deeply annotated protein sequences and alignments. Nucleic Acids
    Research, 2017, 45: D170-D176'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] Mirdita M, Von Den Driesch L, Galiez C 等. Uniclust 数据库：聚类和深入注释的蛋白质序列与比对。核酸研究，2017年，45：D170-D176'
- en: '[221] Lo Conte L, Ailey B, Hubbard T J, et al. SCOP: a structural classification
    of proteins database. Nucleic Acids Research. 2000, 28: 257-259'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] Lo Conte L, Ailey B, Hubbard T J 等. SCOP：蛋白质结构分类数据库。核酸研究，2000年，28：257-259'
- en: '[222] Chandonia J M, Fox N K, Brenner S E. SCOPe: manual curation and artifact
    removal in the structural classification of proteins–extended database. Journal
    of molecular biology, 2017, 429: 348-355'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] Chandonia J M, Fox N K, Brenner S E. SCOPe：蛋白质结构分类中的手动编目和伪影去除—扩展数据库。分子生物学杂志，2017年，429：348-355'
- en: '[223] Ahdritz G, Bouatta N, Kadyan S, et al. OpenProteinSet: Training data
    for structural biology at scale. arxiv, August 2023'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] Ahdritz G, Bouatta N, Kadyan S 等. OpenProteinSet: 大规模结构生物学的训练数据。arxiv，2023年8月'
- en: '[224] Singh J, Paliwal K, Litfin T,et al. Reaching alignment-profile-based
    accuracy in predicting protein secondary and tertiary structural properties without
    alignment. Scientific reports, 2022, 12: 1-9'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] Singh J, Paliwal K, Litfin T 等. 在不进行对齐的情况下实现基于对齐-谱的蛋白质二级和三级结构属性预测准确性。科学报告，2022年，12：1-9'
- en: '[225] Singh J, Litfin T, Singh J, et al. SPOT-Contact-Single: Improving Single-Sequence-Based
    Prediction of Protein Contact Map using a Transformer Language Model. bioRxiv,
    2021'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] Singh J, Litfin T, Singh J 等. SPOT-Contact-Single: 使用 Transformer 语言模型改进基于单序列的蛋白质接触图预测。bioRxiv，2021年'
- en: '[226] Chen C, Zhang Y, Liu X, et al. Bidirectional learning for offline model-based
    biological sequence design. OpenProteinSet: Training data for structural biology
    at scale, January 2023'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] Chen C, Zhang Y, Liu X 等. 面向离线模型的双向学习生物序列设计。OpenProteinSet：大规模结构生物学的训练数据，2023年1月'
- en: '[227] van Kempen M, Kim S S, Tumescheit C, et al. Fast and accurate protein
    structure search with Foldseek. Nature Biotechnology, 2023, 1-4'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] van Kempen M, Kim S S, Tumescheit C 等. 使用 Foldseek 快速准确地搜索蛋白质结构。自然生物技术，2023年，1-4'
- en: '[228] Oord A, Vinyals O, et al. Neural Discrete Representation Learning. Advances
    in neural information processing systems, 2017, 30'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] Oord A, Vinyals O 等. 神经离散表示学习。神经信息处理系统进展，2017年，30'
- en: '[229] Hu B, Tan C, Wu L R, et al. Multimodal Distillation of Protein Sequence,
    Structure, and Function. arxiv, 2023'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] Hu B, Tan C, Wu L R 等. 蛋白质序列、结构和功能的多模态蒸馏。arxiv，2023年'
- en: '[230] Ibtehaz N, Kagaya Y, et al. Domain-PFP allows protein function prediction
    using function-aware domain embedding representations. Communications Biology,
    2023, 6, 1103'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] Ibtehaz N, Kagaya Y 等. Domain-PFP 通过功能感知的领域嵌入表示实现蛋白质功能预测。通讯生物学，2023年，6，1103'
- en: '[231] Wang R, Sun Y, Luo Y, et al. Injecting Multimodal Information into Rigid
    Protein Docking via Bi-level Optimization. In: Thirty-seventh Conference on Neural
    Information Processing Systems, 2023'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] Wang R, Sun Y, Luo Y 等. 通过双层优化将多模态信息注入刚性蛋白质对接中。在：第37届神经信息处理系统会议，2023年'
- en: '[232] Sun D, Huang H, Li Y, et al. DSR: Dynamical Surface Representation as
    Implicit Neural Networks for Protein. In: Thirty-seventh Conference on Neural
    Information Processing Systems, 2023'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] Sun D, Huang H, Li Y 等. DSR：作为隐式神经网络的动态表面表示用于蛋白质。在：第37届神经信息处理系统会议，2023年'
- en: '[233] Omelchenko M V, Galperin M Y, Wolf Y I, et al. Non-homologous isofunctional
    enzymes: a systematic analysis of alternative solutions in enzyme evolution. Biology
    direct, 2010, 5: 1-20'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] Omelchenko M V, Galperin M Y, Wolf Y I 等. 非同源等功能酶：酶进化中替代解决方案的系统分析。生物学直报，2010年，5：1-20'
- en: '[234] Wu L, Huang Y, Lin H, et al. A survey on protein representation learning:
    Retrospect and prospect. arxiv, January 2022'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] Wu L, Huang Y, Lin H 等. 蛋白质表示学习调查：回顾与展望。arxiv，2022年1月'
- en: '[235] Liu W, Zhou P, Zhao Z, et al. K-bert: Enabling language representation
    with knowledge graph. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    2020\. 2901-2908'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] Liu W, Zhou P, Zhao Z, 等. K-bert：通过知识图谱实现语言表示。见：AAAI人工智能会议论文集，2020，2901-2908'
- en: '[236] Wang S, Sun S, Li Z, et al. Accurate de novo prediction of protein contact
    map by ultra-deep learning model. PLoS computational biology, 2017, 13: e1005324'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] Wang S, Sun S, Li Z, 等. 通过超深学习模型准确的de novo蛋白质接触图预测。PLoS计算生物学，2017，13:
    e1005324'
- en: '[237] Li J, Xu J. Study of real-valued distance prediction for protein structure
    prediction with deep learning. Bioinformatics, 2021, 37:3197-3203'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] Li J, Xu J. 基于深度学习的蛋白质结构预测的实值距离预测研究。生物信息学，2021，37:3197-3203'
- en: '[238] Hu B, Zang Z, Tan C, etc. Deep Manifold Transformation for Protein Representation
    Learning. ICASSP, 2024'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] Hu B, Zang Z, Tan C, 等. 蛋白质表示学习的深度流形变换。ICASSP，2024'
- en: '[239] Yan Y, Huang S Y. Accurate prediction of inter-protein residue–residue
    contacts for homo-oligomeric protein complexes. Briefings in bioinformatics, 2021,
    22: bbab038'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] Yan Y, Huang S Y. 对于同源寡聚蛋白质复合体，准确预测蛋白质间残基-残基接触。生物信息学简报，2021，22: bbab038'
- en: '[240] Baek M, DiMaio F, Anishchenko I, et al. Accurate prediction of protein
    structures and interactions using a three-track neural network. Science, 2021,
    373: 871-876'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Baek M, DiMaio F, Anishchenko I, 等. 使用三轨神经网络准确预测蛋白质结构和相互作用。科学，2021，373:
    871-876'
- en: '[241] Jing X, Wu F, Luo X, et al. RaptorX-Single: single-sequence protein structure
    prediction by integrating protein language models. bioRxiv, 2023'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] Jing X, Wu F, Luo X, 等. RaptorX-Single：通过整合蛋白质语言模型进行单序列蛋白质结构预测。bioRxiv，2023'
- en: '[242] Chang L, Perez A. AlphaFold encodes the principles to identify high affinity
    peptide binders. bioRxiv, 2022'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] Chang L, Perez A. AlphaFold编码了识别高亲和力肽结合体的原则。bioRxiv，2022'
- en: '[243] Ruffolo J A, Gray J J. Fast, accurate antibody structure prediction from
    deep learning on massive set of natural antibodies. Biophysical Journal, 2022'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] Ruffolo J A, Gray J J. 基于深度学习的大规模自然抗体集的快速、准确抗体结构预测。生物物理学杂志，2022'
- en: '[244] Evans R, O’Neill M, Pritzel A, et al. Protein complex prediction with
    AlphaFold-Multimer. bioRxiv, 2022'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] Evans R, O’Neill M, Pritzel A, 等. 使用AlphaFold-Multimer进行蛋白质复合体预测。bioRxiv，2022'
- en: '[245] Bryant P, Pozzati G, Elofsson A. Improved prediction of protein-protein
    interactions using alphafold2 and extended multiple-sequence alignments. bioRxiv,
    2021'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] Bryant P, Pozzati G, Elofsson A. 使用alphafold2和扩展的多序列比对改进蛋白质-蛋白质相互作用预测。bioRxiv，2021'
- en: '[246] Shen T, Hu Z, Peng Z, et al. E2Efold-3D: End-to-End Deep Learning Method
    for accurate de novo RNA 3D Structure Prediction. arxiv, July 2022'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] Shen T, Hu Z, Peng Z, 等. E2Efold-3D：用于准确的de novo RNA 3D结构预测的端到端深度学习方法。arxiv，2022年7月'
- en: '[247] Stein RA, Mchaourab HS. SPEACH_AF: Sampling protein ensembles and conformational
    heterogeneity with Alphafold2\. PLOS Computational Biology, 2022, 18: e1010483'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] Stein RA, Mchaourab HS. SPEACH_AF：使用Alphafold2对蛋白质集合和构象异质性进行采样。PLOS计算生物学，2022，18:
    e1010483'
- en: '[248] AlQuraishi M. End-to-end differentiable learning of protein structure.
    Cell systems, 2019, 8: 292-301'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] AlQuraishi M. 蛋白质结构的端到端可微分学习。细胞系统，2019，8: 292-301'
- en: '[249] Du Y, Meier J, Ma J, et al. Energy-based models for atomic-resolution
    protein conformations. arxiv, April 2020'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] Du Y, Meier J, Ma J, 等. 基于能量的原子分辨率蛋白质构象模型。arxiv，2020年4月'
- en: '[250] Hou M, Jin S, Cui X, et al. Protein multiple conformations prediction
    using multi-objective evolution algorithm. bioRxiv, 2023'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] Hou M, Jin S, Cui X, 等. 使用多目标进化算法预测蛋白质的多种构象。bioRxiv，2023'
- en: '[251] Wu F, Li S Z. DIFFMD: a geometric diffusion model for molecular dynamics
    simulations. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    2023\. 37: 5321-5329'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] Wu F, Li S Z. DIFFMD：一种用于分子动力学模拟的几何扩散模型。见：AAAI人工智能会议论文集，2023，37: 5321-5329'
- en: '[252] Lu J, Zhong B, Zhang Z, et al. Str2Str: A Score-based Framework for Zero-shot
    Protein Conformation Sampling. In: ICLR, 2024'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] Lu J, Zhong B, Zhang Z, 等. Str2Str：一种基于评分的零样本蛋白质构象采样框架。见：ICLR，2024'
- en: '[253] Zhao K, Xia Y, Zhang F, et al. Protein structure and folding pathway
    prediction based on remote homologs recognition using PAthreader. Communications
    Biololy, 2023, 6: 243'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] Zhao K, Xia Y, Zhang F, 等. 基于远程同源体识别的蛋白质结构和折叠途径预测，使用PAthreader。通讯生物学，2023，6:
    243'
- en: '[254] Huang Z, Cui X, Xia Y, et al. Pathfinder: protein folding pathway prediction
    based on conformational sampling. bioRxiv, 2023'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] Huang Z, Cui X, Xia Y, 等. Pathfinder：基于构象采样的蛋白质折叠途径预测。bioRxiv，2023'
- en: '[255] Lee J H, Yadollahpour P, Watkins A, et al. Equifold: Protein structure
    prediction with a novel coarse-grained structure representation. bioRxiv, 2022'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] 李静赫，Yadollahpour P，Watkins A，等。Equifold：一种新型粗粒度结构表示的蛋白质结构预测。bioRxiv，2022'
- en: '[256] Hiranuma N, Park H, Baek M, et al. Improved protein structure refinement
    guided by deep learning based accuracy estimation. Nature communications, 2021,
    12: 1-11'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] 平沼直，朴豪，白孟，等。通过深度学习指导的精确性估计改进蛋白质结构优化。自然通讯，2021，12：1-11'
- en: '[257] Chen B, Xie Z, Xu J, Qiu J, Ye Z, Tang J. Improve the Protein Complex
    Prediction with Protein Language Models. bioRxiv, 2022'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] 陈博，谢泽，徐杰，邱静，叶泽，唐军。通过蛋白质语言模型改进蛋白质复合物预测。bioRxiv，2022'
- en: '[258] Feng T, Gao Z, You J, et al. Deep Reinforcement Learning for Modelling
    Protein Complexes. In: ICLR, 2024'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] 冯涛，高震，游俊，等。用于建模蛋白质复合物的深度强化学习。在：ICLR，2024'
- en: '[259] Baek M, McHugh R, Anishchenko I, et al. Accurate prediction of nucleic
    acid and protein-nucleic acid complexes using RoseTTAFoldNA. bioRxiv, 2022'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] 白孟，McHugh R，Anishchenko I，等。使用 RoseTTAFoldNA 准确预测核酸和蛋白质-核酸复合物。bioRxiv，2022'
- en: '[260] Zhang J, Liu S, Chen M, et al. Few-shot learning of accurate folding
    landscape for protein structure prediction. arxiv, August 2022'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] 张杰，刘松，陈明，等。准确折叠景观的少样本学习用于蛋白质结构预测。arxiv，2022年8月'
- en: '[261] Zhang L, Chen J, Shen T, et al. Enhancing the Protein Tertiary Structure
    Prediction by Multiple Sequence Alignment Generation. arxiv, June 2023'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] 张磊，陈佳，沈涛，等。通过多序列比对生成提升蛋白质三级结构预测。arxiv，2023年6月'
- en: '[262] Jing B, Erives E, Pao-Huang P, et al. EigenFold: Generative Protein Structure
    Prediction with Diffusion Models. arxiv, April 2023'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] 荆博，Erives E，Pao-Huang P，等。EigenFold：基于扩散模型的生成蛋白质结构预测。arxiv，2023年4月'
- en: '[263] Kandathil S M, Greener J G, Lau A M. et al. Ultrafast end-to-end protein
    structure prediction enables high-throughput exploration of uncharacterized proteins.
    Proceedings of the National Academy of Sciences, 2022, 119: e2113348119'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] Kandathil S M，Greener J G，Lau A M，等。超快的端到端蛋白质结构预测实现了对未表征蛋白质的高通量探索。国家科学院院刊，2022，119：e2113348119'
- en: '[264] Krishna R, Wang J, Ahern W, et al. Generalized biomolecular modeling
    and design with RoseTTAFold All-Atom. bioRxiv, 2023'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] 克里希纳 R，王杰，Ahern W，等。使用 RoseTTAFold All-Atom 的广义生物分子建模和设计。bioRxiv，2023'
- en: '[265] Wang S, Li W, Liu S, et al. RaptorX-Property: a web server for protein
    structure property prediction. Nucleic acids research, 2016, 44: W430-W435'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] 王思，李伟，刘松，等。RaptorX-Property：一种用于蛋白质结构属性预测的网页服务器。核酸研究，2016，44：W430-W435'
- en: '[266] Wu T, Guo Z, Hou J, et al. DeepDist: real-value inter-residue distance
    prediction with deep residual convolutional network. BMC bioinformatics, 2021,
    22: 1-17'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] 吴天，郭卓，侯军，等。DeepDist：基于深度残差卷积网络的真实值残基间距预测。BMC生物信息学，2021，22：1-17'
- en: '[267] Chen T, Gong C, Diaz D J, et al. HotProtein: A novel framework for protein
    thermostability prediction and editing. In: ICLR, 2023'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] 陈涛，龚晨，Diaz D J，等。HotProtein：用于蛋白质热稳定性预测和编辑的新框架。在：ICLR，2023'
- en: '[268] Liu S, Zhu T, Ren M, et al. Predicting mutational effects on protein-protein
    binding via a side-chain diffusion probabilistic model. In: NeurIPS, 2023'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] 刘松，朱婷，任敏，等。通过侧链扩散概率模型预测突变对蛋白质-蛋白质结合的影响。在：NeurIPS，2023'
- en: '[269] Luo S, Su Y, Wu Z, et al. Rotamer Density Estimator is an Unsupervised
    Learner of the Effect of Mutations on Protein-Protein Interaction. In: ICLR, 2023'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] 罗思，苏颖，吴泽，等。Rotamer Density Estimator 是一种无监督学习器，用于研究突变对蛋白质-蛋白质相互作用的影响。在：ICLR，2023'
- en: '[270] Shi C, Wang C, Lu J, et al. Protein Sequence and Structure Co-Design
    with Equivariant Translation. In: ICLR, 2023'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] 石川，王超，陆骏，等。蛋白质序列和结构的协同设计与等变转换。在：ICLR，2023'
- en: '[271] Zheng Z, Deng Y, Xue D, et al. Structure-informed Language Models Are
    Protein Designers. In: ICML, 2023'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] 郑志，邓勇，薛栋，等。结构信息语言模型是蛋白质设计师。在：ICML，2023'
- en: '[272] Lin Y, AlQuraishi M. Generating novel, designable, and diverse protein
    structures by equivariantly diffusing oriented residue clouds. In: ICML, 2023'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] 林永，AlQuraishi M。通过等变扩散定向残基云生成新颖的、可设计的、多样化的蛋白质结构。在：ICML，2023'
- en: '[273] Song Z, Li L. Importance Weighted Expectation-Maximization for Protein
    Sequence Design. In: ICML, 2023'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] 宋志，李雷。用于蛋白质序列设计的重要性加权期望最大化。在：ICML，2023'
- en: '[274] Yim J, Tripple B L, Bortoli V D, et al. SE(3) diffusion model with application
    to protein backbone generation. In: ICML, 2023'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] 尹杰，Tripple B L，Bortoli V D，等。SE(3)扩散模型及其在蛋白质骨架生成中的应用。在：ICML，2023'
- en: '[275] Kong X, Huang W, Liu Y, et al. End-to-End Full-Atom Antibody Design.
    In: ICML, 2023'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] 孔晓，黄伟，刘洋，等。端到端全原子抗体设计。在：ICML，2023'
- en: '[276] Verma Y, Heinonen M, Garg V, et al. AbODE: Ab Initio Antibody Design
    using Conjoined ODEs. In: ICML, 2023'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] Verma Y, Heinonen M, Garg V, 等. AbODE：使用结合ODE的从头抗体设计。发表于：ICML, 2023'
- en: '[277] Tan C, Gao Z, Li S Z. Cross-Gate MLP with Protein Complex Invariant Embedding
    is A One-Shot Antibody Designer. arxiv, May 2023'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] Tan C, Gao Z, Li S Z. Cross-Gate MLP与蛋白质复合体不变嵌入是一个单次抗体设计器。arxiv, 2023年5月'
- en: '[278] Wu F, Li S Z. A Hierarchical Training Paradigm for Antibody Structure-sequence
    Co-design. arxiv, November 2023'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] Wu F, Li S Z. 一种用于抗体结构-序列共设计的分层训练范式。arxiv, 2023年11月'
- en: '[279] Chen C, Zhang Y, Liu X, et al. Bidirectional Learning for Offline Model-based
    Biological Sequence Design. In: ICML, 2023'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] Chen C, Zhang Y, Liu X, 等. 离线模型基础生物序列设计的双向学习。发表于：ICML, 2023'
- en: '[280] Tan, Zhang Y, Gao Z, et al. RDesign: Hierarchical Data-efficient Representation
    Learning for Tertiary Structure-based RNA Design. In: ICLR, 2024'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] Tan, Zhang Y, Gao Z, 等. RDesign：基于三级结构的RNA设计的分层数据高效表示学习。发表于：ICLR, 2024'
- en: '[281] Truong Jr T F, Bepler T. PoET: A generative model of protein families
    as sequences-of-sequences. In: NeurIPS, 2023'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] Truong Jr T F, Bepler T. PoET：一种作为序列-序列生成模型的蛋白质家族。发表于：NeurIPS, 2023'
- en: '[282] Yi K, Zhou B, Shen Y, et al. Graph Denoising Diffusion for Inverse Protein
    Folding. In: NeurIPS, 2023'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] Yi K, Zhou B, Shen Y, 等. 逆蛋白质折叠的图去噪扩散。发表于：NeurIPS, 2023'
- en: '[283] Pei Q, Gao K, Wu L, et al. FABind: Fast and Accurate Protein-Ligand Binding.
    In: NeurIPS, 2023'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] Pei Q, Gao K, Wu L, 等. FABind：快速准确的蛋白质-配体结合。发表于：NeurIPS, 2023'
- en: '[284] Jin W, Sarzikova S, Chen X, et al. Unsupervised Protein-Ligand Binding
    Energy Prediction via Neural Euler’s Rotation Equation. In: NeurIPS, 2023'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] Jin W, Sarzikova S, Chen X, 等. 通过神经欧拉旋转方程进行无监督蛋白质-配体结合能预测。发表于：NeurIPS,
    2023'
- en: '[285] Wu L, Tian Y, Huang Y, et al. MAPE-PPI: Towards Effective and Efficient
    Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding.
    In: ICLR, 2024'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] Wu L, Tian Y, Huang Y, 等. MAPE-PPI：通过微环境感知蛋白质嵌入实现有效且高效的蛋白质-蛋白质相互作用预测。发表于：ICLR,
    2024'
- en: '[286] Wang R, Sun Y, Luo Y, et al. Injecting Multimodal Information into Rigid
    Protein Docking via Bi-level Optimization. In: NeurIPS, 2023'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] Wang R, Sun Y, Luo Y, 等. 通过双层优化将多模态信息注入刚性蛋白质对接。发表于：NeurIPS, 2023'
- en: '[287] Gao Z, Sun X, Liu Z, et al. Protein Multimer Structure Prediction via
    Prompt Learning. In: ICLR, 2024'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] Gao Z, Sun X, Liu Z, 等. 通过提示学习预测蛋白质多聚体结构。发表于：ICLR, 2024'
- en: '[288] Zhang Z, Lu Z, Hao Z, et al. Full-Atom Protein Pocket Design via Iterative
    Refinement. In: NeurIPS, 2023'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] Zhang Z, Lu Z, Hao Z, 等. 通过迭代优化进行全原子蛋白质口袋设计。发表于：NeurIPS, 2023'
- en: '[289] Gruver N, Stanton S, Frey N, et al. Protein Design with Guided Discrete
    Diffusion. In: NeurIPS, 2023'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] Gruver N, Stanton S, Frey N, 等. 具有引导离散扩散的蛋白质设计。发表于：NeurIPS, 2023'
- en: '[290] Zhang J O, Diaz D J, Klivans A R, et al. Predicting a Protein ’ s Stability
    under a Million Mutations. In: NeurIPS, 2023'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] Zhang J O, Diaz D J, Klivans A R, 等. 预测蛋白质在百万个突变下的稳定性。发表于：NeurIPS, 2023'
- en: '[291] Hsu C, Verkuil R, Liu J, et al. Learning inverse folding from millions
    of predicted structures. In: International Conference on Machine Learning, 2022\.
    8946-8970'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] Hsu C, Verkuil R, Liu J, 等. 从数百万个预测结构中学习逆折叠。发表于：国际机器学习会议, 2022。8946-8970'
- en: '[292] Cao Y, Das P, Chenthamarakshan V, et al. Fold2seq: A joint sequence (1d)-fold
    (3d) embedding-based generative model for protein design. In: International Conference
    on Machine Learning, 2021\. 1261-1271'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] Cao Y, Das P, Chenthamarakshan V, 等. Fold2seq：一种基于序列（1d）-折叠（3d）嵌入的蛋白质设计生成模型。发表于：国际机器学习会议,
    2021。1261-1271'
- en: '[293] Dumortier B, Liutkus A, Carré C,et al. PeTriBERT: Augmenting BERT with
    tridimensional encoding for inverse protein folding and design. bioRxiv, 2022'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] Dumortier B, Liutkus A, Carré C, 等. PeTriBERT：通过三维编码增强BERT，用于逆蛋白质折叠和设计。bioRxiv,
    2022'
- en: '[294] Dauparas J, Anishchenko I, Bennett N, et al. Robust deep learning–based
    protein sequence design using ProteinMPNN. Science, 2022, 378: 49-56'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] Dauparas J, Anishchenko I, Bennett N, 等. 使用ProteinMPNN的鲁棒深度学习蛋白质序列设计。科学,
    2022, 378: 49-56'
- en: '[295] Gao Z, Tan C, Li S Z. AlphaDesign: A graph protein design method and
    benchmark on AlphaFoldDB. arxiv, February 2022'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] Gao Z, Tan C, Li S Z. AlphaDesign：一种图形蛋白质设计方法及在AlphaFoldDB上的基准测试。arxiv,
    2022年2月'
- en: '[296] Tan C, Gao Z Y, Xia J, et al. Generative de novo protein design with
    global context. arxiv, April 2022'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] Tan C, Gao Z Y, Xia J, 等. 全球背景下的生成式新蛋白质设计。arxiv, 2022年4月'
- en: '[297] Korendovych I V, DeGrado W F. De novo protein design, a retrospective.
    Quarterly reviews of biophysics, 2020, 53, e3'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] Korendovych I V, DeGrado W F. 从头蛋白质设计的回顾。生物物理学季刊, 2020, 53, e3'
- en: '[298] Mao W, Sun Z, Zhu M, et al. De novo Protein Design Using Geometric Vector
    Field Networks. In: ICLR, 2024'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] 毛伟, 孙志, 朱敏, 等. 使用几何向量场网络进行de novo蛋白质设计. 载于：ICLR, 2024'
- en: '[299] Notin P, Dias M, Frazer J, et al. Tranception: protein fitness prediction
    with autoregressive transformers and inference-time retrieval. In: International
    Conference on Machine Learning, 2022\. 16990-17017'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] 诺廷 P, 迪亚斯 M, 弗雷泽 J, 等. Tranception: 使用自回归变换器和推理时检索进行蛋白质适应性预测. 载于：国际机器学习会议,
    2022. 16990-17017'
- en: '[300] Bushuiev A, Bushuiev R, Kouba P, et al. Learning to design protein-protein
    interactions with enhanced generalization. In: ICLR, 2024'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] 布舒耶夫 A, 布舒耶夫 R, 库巴 P, 等. 学习设计具有增强泛化能力的蛋白质-蛋白质相互作用. 载于：ICLR, 2024'
- en: '[301] Meier J, Rao R, Verkuil R, et al. Language models enable zero-shot prediction
    of the effects of mutations on protein function. Advances in Neural Information
    Processing Systems, 2021, 34:29287-303'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] 梅耶尔 J, 饶睿 R, 维尔基尔 R, 等. 语言模型使零样本预测突变对蛋白质功能的影响成为可能. 神经信息处理系统进展, 2021,
    34:29287-303'
- en: '[302] Hu M, Yuan F, Yang K K, et al. Exploring evolution-based &-free protein
    language models as protein function predictors. arxiv, June 2022'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] 胡敏, 袁丰, 杨克凯, 等. 探索基于进化的&-free蛋白质语言模型作为蛋白质功能预测器. arxiv, 2022年6月'
- en: '[303] Notin P, Kollasch A W, Ritter D, et al. ProteinGym: Large-Scale Benchmarks
    for Protein Design and Fitness Prediction. In: NeurIPS. 2023'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] 诺廷 P, 科拉施 A W, 里特 D, 等. ProteinGym: 蛋白质设计和适应性预测的大规模基准测试. 载于：NeurIPS,
    2023'
- en: '[304] Roney JP, Ovchinnikov S. State-of-the-Art estimation of protein model
    accuracy using AlphaFold. bioRxiv, 2022'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] 罗尼 JP, 奥夫奇尼科夫 S. 使用AlphaFold进行蛋白质模型准确性的最先进估计. bioRxiv, 2022'
- en: '[305] Roney J. Evidence for and Applications of Physics-Based Reasoning in
    AlphaFold. Doctoral dissertation, 2022'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] 罗尼 J. AlphaFold中基于物理的推理的证据及应用. 博士论文, 2022'
- en: '[306] Akdel M, Pires D E, Pardo E P, et al. A structural biology community
    assessment of alphafold2 applications. Nature Structural & Molecular Biology,
    2022'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] 阿克德尔 M, 皮雷斯 D E, 帕尔多 E P, 等. 结构生物学社区对alphafold2应用的评估. 自然结构与分子生物学, 2022'
- en: '[307] Pak M A, Markhieva K A, Novikova M S, et al. Using alphafold to predict
    the impact of single mutations on protein stability and function. bioRxiv, 2021'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] 帕克 M A, 马尔基耶娃 K A, 诺维科娃 M S, 等. 使用alphafold预测单突变对蛋白质稳定性和功能的影响. bioRxiv,
    2021'
- en: '[308] Buel G R, Walters K J. Can AlphaFold2 predict the impact of missense
    mutations on structure?. Nature Structural & Molecular Biology, 2022, 29: 1-2'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] 布埃尔 G R, 沃尔特斯 K J. AlphaFold2能否预测错义突变对结构的影响？. 自然结构与分子生物学, 2022, 29: 1-2'
- en: '[309] Zheng S, Li Y, Chen S, et al. Predicting drug–protein interaction using
    quasi-visual question answering system. Nature Machine Intelligence, 2023, 2:
    134–140'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] 郑磊, 李燕, 陈硕, 等. 使用准视觉问答系统预测药物-蛋白质相互作用. 自然机器智能, 2023, 2: 134–140'
- en: '[310] Zhang Z, Liu Q. Learning Subpocket Prototypes for Generalizable Structure-based
    Drug Design. In: ICML, 2023'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] 张志, 刘琦. 学习亚口袋原型以进行可推广的基于结构的药物设计. 载于：ICML, 2023'
- en: '[311] Gao B, Qiang B, Tan H, et al. DrugCLIP: Contrastive Protein-Molecule
    Representation Learning for Virtual Screening. In: NeurIPS, 2023'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] 高斌, 强博, 谭辉, 等. DrugCLIP: 用于虚拟筛选的对比蛋白-分子表示学习. 载于：NeurIPS, 2023'
- en: '[312] Padmakumar V, Pang R Y, He H, et al. Extrapolative Controlled Sequence
    Generation via Iterative Refinement. In: ICML, 2023'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] 帕德马库马 V, 庞荣义, 何浩, 等. 通过迭代细化的外推控制序列生成. 载于：ICML, 2023'
- en: '[313] Lee S, Jo J, Hwang S J. Exploring Chemical Space with Score-based Out-of-distribution
    Generation. In: ICML, 2023'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] 李晟, 乔俊, 黄盛杰. 利用基于评分的分布外生成探索化学空间. 载于：ICML, 2023'
- en: '[314] Ahdritz G, Bouatta N, Kadyan S, et al. OpenProteinSet: Training data
    for structural biology at scale. In: NeurIPS, 2023'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] 阿赫德里茨 G, 布阿塔 N, 卡迪安 S, 等. OpenProteinSet: 大规模结构生物学训练数据. 载于：NeurIPS, 2023'
- en: '[315] Kucera T, Oliver C, Chen D, et al. ProteinShake: Building datasets and
    benchmarks for deep learning on protein structures. In: NeurIPS, 2023'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] 库切拉 T, 奥利弗 C, 陈东, 等. ProteinShake: 为蛋白质结构的深度学习构建数据集和基准. 载于：NeurIPS, 2023'
- en: '[316] Xu M, Zhang Z, Lu J, et al. PEER: A Comprehensive and Multi-Task Benchmark
    for Protein Sequence Understanding. arxiv, June 2022'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] 徐明, 张志, 陆杰, 等. PEER: 一个全面且多任务的蛋白质序列理解基准. arxiv, 2022年6月'
- en: '[317] Gao Z, Tan C, Zhang Y, et al. ProteinInvBench: Benchmarking Protein Inverse
    Folding on Diverse Tasks, Models, and Metrics. In: NeurIPS, 2023'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] 高志, 谭聪, 张艳, 等. ProteinInvBench: 在多样任务、模型和度量上的蛋白质逆折叠基准测试. 载于：NeurIPS,
    2023'
- en: '[318] Jamasb A R, Morehead A, Zhang Z, et al. Evaluating Representation Learning
    on the Protein Structure Universe. In: ICLR, 2024'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[318] 贾马斯布 A R, 莫赫德 A, 张志, 等. 评估蛋白质结构宇宙上的表示学习. 载于：ICLR, 2024'
- en: '[319] Roy A, Kucukural A, Zhang Y. I-TASSER: a unified platform for automated
    protein structure and function prediction. Nature protocols, 2010, 5: 725-738'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[319] Roy A, Kucukural A, Zhang Y。I-TASSER：一个用于自动蛋白质结构和功能预测的统一平台。《自然协议》，2010，5:
    725-738'
- en: '[320] Tunyasuvunakool K, Adler J, Wu Z, et al. Highly accurate protein structure
    prediction for the human proteome. Nature, 2021, 596: 590-596'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[320] Tunyasuvunakool K, Adler J, Wu Z 等。人类蛋白质组的高度准确蛋白质结构预测。《自然》，2021，596:
    590-596'
- en: '[321] Under review as a conference paper at ICLR 2024\. ProtChatGPT: Towards
    Understanding Proteins with Large Language Models, 2023'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[321] 正在作为会议论文审稿中，ICLR 2024\. ProtChatGPT：利用大语言模型理解蛋白质，2023'
- en: '[322] Lin Z, Akin H, Rao R, et al. Evolutionary-scale prediction of atomic-level
    protein structure with a language model. Science, 2023, 379: 1123-1130'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[322] Lin Z, Akin H, Rao R 等。利用语言模型进行原子级蛋白质结构的进化尺度预测。《科学》，2023，379: 1123-1130'
