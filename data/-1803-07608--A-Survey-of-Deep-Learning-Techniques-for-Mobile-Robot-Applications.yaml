- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:07:56'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:07:56
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1803.07608] A Survey of Deep Learning Techniques for Mobile Robot Applications'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1803.07608] 移动机器人应用的深度学习技术综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1803.07608](https://ar5iv.labs.arxiv.org/html/1803.07608)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1803.07608](https://ar5iv.labs.arxiv.org/html/1803.07608)
- en: A Survey of Deep Learning Techniques for
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习技术综述
- en: Mobile Robot Applications
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 移动机器人应用
- en: Jahanzaib Shabbir, and Tarique Anwer
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Jahanzaib Shabbir 和 Tarique Anwer
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Advancements in deep learning over the years have attracted research into how
    deep artificial neural networks can be used in robotic systems. It is on this
    basis that the following research survey will present a discussion of the applications,
    gains, and obstacles to deep learning in comparison to physical robotic systems
    while using modern research as examples. The research survey will present a summarization
    of the current research with specific focus on the gains and obstacles in comparison
    to robotics. This will be followed by a primer on discussing how notable deep
    learning structures can be used in robotics with relevant examples. The next section
    will show the practical considerations robotics researchers desire to use in regard
    to deep learning neural networks. Finally, the research survey will show the shortcomings
    and solutions to mitigate them in addition to discussion of the future trends.
    The intention of this research is to show how recent advancements in the broader
    robotics field can inspire additional research in applying deep learning in robotics.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习的进展引起了对深度人工神经网络在机器人系统中应用的研究。基于此，以下研究综述将讨论深度学习在与物理机器人系统的比较中的应用、收益和障碍，同时使用现代研究作为例子。研究综述将总结当前的研究，特别关注相较于机器人技术的收益和障碍。接下来将介绍如何将显著的深度学习结构应用于机器人领域，并提供相关例子。下一部分将展示机器人研究人员在深度学习神经网络方面希望使用的实际考虑因素。最后，研究综述将展示缺陷及其解决方案，并讨论未来趋势。此研究旨在展示最近在更广泛机器人领域的进展如何激发进一步在机器人中应用深度学习的研究。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep learning, robotic vision, navigation, autonomous driving, deep reinforcement
    learning, algorithms for robotic perception, Semi-supervised and self-supervised
    learning, Deep learning architectures, multimodal, decision making and control.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，机器人视觉，导航，自动驾驶，深度强化学习，机器人感知算法，半监督和自监督学习，深度学习架构，多模态，决策制定与控制。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 1.1 Defining Deep Learning in the Context of Robotic Systems
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 在机器人系统背景下定义深度学习
- en: Deep learning is defined as the field of science that involves training extensive
    artificial neural networks using complex functions, for example, nonlinear dynamics
    to change data from a raw, high-dimension, multimodal state to that which can
    be understood by a robotic system [[1](#bib.bib1)]. However, deep learning entails
    certain shortcomings which affect physical robotic systems whereby generation
    of training data in overall is costly and therefore sub-optimal performance in
    the course of training poses a risk in certain applications. Yet, even with such
    difficulties, robotics researchers are searching for creative options, for instance,
    leveraging training data through digital manipulation, automated training and
    using multiple deep neural networks to improve the performance and lower the time
    for training [[2](#bib.bib2)]. The idea of using machine learning to control robots
    needs humans to show the willingness to lose a certain measure of control. This
    is seemingly counterintuitive in the beginning although the gain for doing this
    is to allow the system to begin learning on its own [[3](#bib.bib3)]. This makes
    the systems capable of adaptation such that the potential of ultimately improving
    their direction is that originating from human control. This makes deep neural
    networks well suited to be used with robots since they are flexible and can be
    used in frameworks that cannot be supported by other machine learning models [[4](#bib.bib4)].
    For a long time, the most notable method for optimization in neural networks is
    known as the stochastic gradient descent. However, improved techniques, for instance,
    RMSProp, as well as Adam of recent, have gained widespread use. Each of the many
    types of deep learning models is made through the stacking of several layers of
    regression models [[5](#bib.bib5)]. Within these models, distinct types of layers
    have undergone evolution for many aims.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习被定义为一个科学领域，它涉及使用复杂函数（例如，非线性动态）训练广泛的人工神经网络，将数据从原始、高维、多模态状态转变为机器人系统能够理解的形式[[1](#bib.bib1)]。然而，深度学习存在一些缺陷，这些缺陷影响物理机器人系统，其中训练数据的生成总体上是昂贵的，因此在训练过程中表现不佳在某些应用中构成风险。尽管存在这些困难，机器人研究人员仍在寻找创造性的解决方案，例如，通过数字处理、自动化训练和使用多个深度神经网络来提升性能并缩短训练时间[[2](#bib.bib2)]。使用机器学习来控制机器人需要人类愿意放弃一定程度的控制。尽管这在开始时似乎违反直觉，但这样做的收益是让系统开始自主学习[[3](#bib.bib3)]。这使得系统具备适应能力，其最终提升方向的潜力源自于人类控制。这使得深度神经网络非常适合与机器人一起使用，因为它们灵活，并且可以在其他机器学习模型无法支持的框架中使用[[4](#bib.bib4)]。长期以来，优化神经网络的最著名方法被称为随机梯度下降。然而，改进的技术，例如最近的RMSProp和Adam，已经得到了广泛使用。每种深度学习模型都是通过堆叠多个回归模型层来构建的[[5](#bib.bib5)]。在这些模型中，针对多种目标的不同类型的层已经经历了演变。
- en: 1.2 Forms of Deep Learning Applied to Mobile Robotic Systems
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 应用于移动机器人系统的深度学习形式
- en: One type of layer that demands specific mention is convolutional layers. Unlike
    traditional layers that are fully connected, convolutional layers apply the same
    weights in order to operate in all the input space. This brings about a significant
    reduction of the overall number of weights in the neural network which is specifically
    vital with images that normally compose of hundreds of thousands and millions
    of pixels that require processing [[6](#bib.bib6)]. It should be noted that processing
    these kinds of images which have fully connected layers would need over 100K2
    to 1M2 weights which connect to each layer which makes it entirely impractical.
    The inspiration of convolutional layers came from cortical neurons within the
    visual cortex which only respond to stimuli in a receptive environment. Since
    convolution estimates such behavior, convolutional layers can be expected to excel
    at image processing assignments [[7](#bib.bib7)]. The pioneering research in neural
    networks using convolutional layers uses image recognition tasks which we built
    on the advancements of ImageNet recognition competitions around 2012\. The lessons
    learned in this period gained widespread interest in convolutional layers being
    able to gain super-human recognition of images [[8](#bib.bib8)]. Currently, convolutional
    neural networks have been come well known and highly effective as a deep learning
    model for many image-based applications. These applications comprise of semantic
    image segmentation, scaling images using super-resolution, scene recognition,
    object localization with images, human gesture recognition and facial recognition
    [[9](#bib.bib9)] [[10](#bib.bib10)]. Images are not the only form of a signal
    which illustrates the excellence of convolutional neural networks. Their capability
    is also effective in any form of a signal which demonstrates spatiotemporal proximity
    for example speech recognition as well as speech and audio synthesis [[11](#bib.bib11)].
    Naturally, these have also started to be dominative in the domain of signal processing
    and heavily used in robotics, for instance, pedestrian detection with the use
    of LIDAR, mico-Doppler signatures, as well as depth-map, estimating [[12](#bib.bib12)]
    [[13](#bib.bib13)]. Recent projects have even started to integrate signals from
    several modalities and combine them for unified recognition and perception [[14](#bib.bib14)].
    Ultimately, the philosophy that underlies and prevails in the deep learning community
    is that every component of a complex system can be taught to ”learn.” Therefore,
    the actual power of deep learning does not come from applying just one of the
    described structures in the previous section as a part in robotics systems by
    in connecting components of all these structures to form a complete system that
    learns entirely [[15](#bib.bib15)]. This is the point where deep learning starts
    to make its impact such that each component of the system is capable of learning
    as a whole and is capable of adapting to sophisticated methods. For example, neuroscientists
    have even started recognizing the many patterns evolving in the deep learning
    community and in all artificial intelligence are starting to mirror patterns previously
    evolved in the human brain [[16](#bib.bib16)]. In the process of learning complex,
    high-dimensional as well as novel dynamics, the analysis of derivatives within
    these complex dynamics needs human expertise. However, this process normally consumes
    a lot of time and can bring about a trade-off between the dimensionality and tractability
    of states [[17](#bib.bib17)]. Therefore making these models robust to unforeseen
    impact is challenging and in most cases, full state information is normally unknown.
    In this case, systems that are able to rapidly and autonomously adapt to modern
    dynamics are required to solve problems for instance moving over services with
    unknown or uncertain attributes, managing interaction in a new environment or
    adapting or degrading robot subsystems [[18](#bib.bib18)]. Therefore, we need
    methods that are able to accomplish possession of hundreds or thousands of degrees
    of freedom and demonstrate high measures of uncertainty which are only available
    in a state of partial information. On the other hand, the process of learning
    control policies in dynamic environments and dynamic control systems is able to
    accommodate high measures of freedom for applications such as swarm robotics,
    anthropomorphic hands, robot vision, autonomous robot driving and robotic arm
    manipulation [[19](#bib.bib19)]. However, despite the advancements gained over
    the years in active research, robust and overall solutions for tasks, for instance,
    moving in deformed surfaces or navigating complex geometries with the use of tools
    and actuator systems have remained elusive more so in novel scenarios. This shortcoming
    is inclusive of kinematic and path planning tasks which are inherent in advanced
    movement [[20](#bib.bib20)]. On the other hand, in terms of advanced object recognition,
    deep neural networks have proved to be increasingly adapt to the recognition and
    classification of objects. Examples of advanced application include recognition
    of deformed objects and estimation of their state and pose for movement, semantic
    task, and path specification, for example, moving around the table [[20](#bib.bib20)].
    In addition, it includes recognizing the attributes of an object and surface whereby
    for instance a sharp object could present a danger to human collaborators in certain
    environments such as rough terrain. In the face of such difficulties, deep learning
    models can be used in the approximation of functions from sample input-output
    pairs. These can be the most general purpose deep learning structures since there
    are several distinct functions in robotics which researchers can use in approximating
    from sample observations [[21](#bib.bib21)]. Certain examples of these observations
    entail mapping from actions to corresponding changes in the stage, mapping these
    changes in state to actions that can cause it or mapping from force to motion.
    While in certain cases particular physical equations for such functions may already
    be defined, there are several situations where the environment is highly complex
    for such equations to generate acceptable accuracy [[22](#bib.bib22)]. However,
    in such scenarios learning approximation of functions from sample observations
    can yield accuracy that is significantly better. In other words, approximated
    functions do not need to be continuous. However, function approximation models
    are also excellent at classification tasks, for instance, determining the type
    of obstacles before a robot, the overall path planning strategy well suited for
    present environments or the state of a certain complex object which the object
    is interacting with [[23](#bib.bib23)]. Furthermore, function approximation deep
    learning architecture using rectifiers can model the high coupled dynamics of
    an autonomous mobile robot to solve the analytic derivatives and challenging system
    identification problems. Deep neural networks have superseded other models in
    detection and perception since they are capable of engaging in direct operation
    with highly-dimensional input rather than needing feature vectors based on hand-engineered
    designs by humans [[24](#bib.bib24)]. This lowers the dependence on humans such
    that additional training time can be partially offset by lowering initial engineering
    efforts [[25](#bib.bib25)]. Extraction of meaning from video or still imagery
    is another application where deep learning has gained impressive progression.
    This process demands simultaneously addressing using the four independent factors
    of object detection and a single deep neural network. These factors include feature
    extraction, motion handling, classification, articulation and occlusion handling
    [[26](#bib.bib26)]. Unified systems limit suboptimal interactions between normally
    separate systems by predicting the physical results of dynamic scenes using vision
    alone. This is based on the actions of humans to be able to predict the results
    of a dynamic scene from visual information, for example, a rock falling down and
    impacting another rock [[27](#bib.bib27)]. It is therefore on this premise that
    deep learning has been identified as being effective in managing multimodal data
    generation in robotic sensor applications. These applications include integration
    of vision and haptic sensor data, incorporating depth data and image information
    from RGB-D camera data. Due to the extensive number of meta-parameters, deep neural
    networks have evolved somewhat a reputation of being challenging for non-experts
    to be used effectively [[28](#bib.bib28)]. However, such parameters also avail
    significant flexibility which is a vital factor in their general success. Therefore,
    training deep neural networks needs the user to be able to develop at least an
    elementary level of familiarization with many concepts. Specifically, applying
    these techniques will help in tacking advanced object recognition challenges and
    reduced the extent of the entire changes as well [[29](#bib.bib29)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特别值得提及的层是卷积层。与传统的完全连接层不同，卷积层在所有输入空间中应用相同的权重。这大大减少了神经网络中的总体权重数量，这在通常由数十万甚至数百万像素组成的图像处理时尤其重要[[6](#bib.bib6)]。值得注意的是，处理这些具有完全连接层的图像将需要超过100K2到1M2的权重来连接每一层，这使得它完全不切实际。卷积层的灵感来源于视觉皮层中的皮质神经元，这些神经元只对特定环境中的刺激做出反应。由于卷积层可以估计这种行为，因此可以预期卷积层在图像处理任务中表现出色[[7](#bib.bib7)]。在使用卷积层的神经网络的开创性研究中，我们在2012年左右的ImageNet图像识别竞赛的进展上进行了构建。在这一时期获得的经验引起了广泛的关注，卷积层能够实现超人级别的图像识别[[8](#bib.bib8)]。目前，卷积神经网络已广为人知，并在许多基于图像的应用中表现出高度的有效性。这些应用包括语义图像分割、使用超分辨率对图像进行缩放、场景识别、图像中的目标定位、人类手势识别和面部识别[[9](#bib.bib9)]
    [[10](#bib.bib10)]。图像并不是唯一能够体现卷积神经网络卓越性能的信号形式。它们在任何形式的信号中也表现出有效性，例如语音识别以及语音和音频合成[[11](#bib.bib11)]。自然，这些技术也开始在信号处理领域占据主导地位，并在机器人技术中得到广泛应用，例如利用LIDAR、微多普勒签名以及深度图估计行人检测[[12](#bib.bib12)]
    [[13](#bib.bib13)]。最近的项目甚至开始整合来自多种模态的信号，并将其结合以进行统一的识别和感知[[14](#bib.bib14)]。最终，深度学习社区的基本理念是复杂系统的每个组件都可以被教会“学习”。因此，深度学习的真正力量并不来自于在机器人系统中应用前面描述的某一种结构，而是将所有这些结构的组件连接起来形成一个完整的系统，从而实现全面学习[[15](#bib.bib15)]。这是深度学习开始发挥其影响力的地方，使得系统的每个组件都能够作为一个整体进行学习，并能够适应复杂的方法。例如，神经科学家们甚至开始认识到深度学习社区及人工智能中的许多模式开始镜像人脑中已演化的模式[[16](#bib.bib16)]。在学习复杂、高维和新颖动态的过程中，这些复杂动态的导数分析需要人类的专业知识。然而，这一过程通常消耗大量时间，并可能在维度和可处理性之间带来权衡[[17](#bib.bib17)]。因此，使这些模型对不可预见的影响具有鲁棒性是具有挑战性的，并且在大多数情况下，完整的状态信息通常是未知的。在这种情况下，需要能够快速、自主适应现代动态的系统来解决问题，例如在具有未知或不确定属性的服务上移动、在新环境中管理互动或适应或降级机器人子系统[[18](#bib.bib18)]。因此，我们需要能够实现数百或数千个自由度并表现出高度不确定性的技术，这些不确定性仅在部分信息状态下才能得到解决。另一方面，学习动态环境中的控制策略和动态控制系统的过程能够容纳高度自由度，用于诸如群体机器人、类人手、机器人视觉、自动驾驶和机器人手臂操作等应用[[19](#bib.bib19)]。然而，尽管多年来在积极研究中取得了进展，但针对任务的稳健和全面解决方案，例如在变形表面上移动或使用工具和执行器系统在复杂几何体中导航，仍然难以捉摸，尤其是在新颖场景中。这一缺陷包括先进运动中固有的运动学和路径规划任务[[20](#bib.bib20)]。另一方面，在高级物体识别方面，深度神经网络已被证明在物体识别和分类方面越来越具有适应性。高级应用的例子包括对变形物体的识别及其状态和姿态的估计、语义任务和路径规范，例如在桌子周围移动[[20](#bib.bib20)]。此外，还包括识别物体和表面的属性，例如在某些环境下，如粗糙地形，尖锐物体可能对人类协作者构成危险。面对这些困难，深度学习模型可以用于从样本输入输出对中近似函数。这些可以是最通用的深度学习结构，因为在机器人学中有多个不同的函数，研究人员可以利用样本观察进行近似[[21](#bib.bib21)]。这些观察的一些例子包括从动作到阶段变化的映射，将这些状态变化映射到可以引起这些变化的动作，或从力到运动的映射。虽然在某些情况下，这些函数的具体物理方程可能已经定义，但在环境高度复杂的情况下，这些方程生成的精度可能不够理想[[22](#bib.bib22)]。然而，在这种情况下，从样本观察中学习函数近似可以获得显著更好的精度。换句话说，近似函数不需要是连续的。然而，函数近似模型在分类任务中也表现出色，例如，确定机器人面前的障碍物类型、为当前环境制定的整体路径规划策略或某些复杂对象的状态[[23](#bib.bib23)]。此外，使用整流器的函数近似深度学习架构可以建模自主移动机器人高度耦合的动态，以解决解析导数和具有挑战性的系统识别问题。深度神经网络在检测和感知方面已经超越了其他模型，因为它们能够直接处理高维输入，而无需基于人类设计的特征向量[[24](#bib.bib24)]。这降低了对人类的依赖，使得额外的训练时间可以通过降低初始工程努力来部分抵消[[25](#bib.bib25)]。从视频或静态图像中提取意义是深度学习取得显著进展的另一个应用领域。这个过程要求使用四个独立因素同时解决对象检测和单个深度神经网络。这些因素包括特征提取、运动处理、分类、关节处理和遮挡处理[[26](#bib.bib26)]。统一系统通过仅使用视觉预测动态场景的物理结果，从而限制了通常分开的系统之间的次优交互。这是基于人类的动作，能够从视觉信息预测动态场景的结果，例如，一块石头落下并撞击另一块石头[[27](#bib.bib27)]。因此，深度学习已被确定为在机器人传感器应用中管理多模态数据生成的有效工具。这些应用包括集成视觉和触觉传感器数据、结合深度数据和来自RGB-D相机的数据。由于大量的元参数，深度神经网络在非专家有效使用上形成了相当的挑战[[28](#bib.bib28)]。然而，这些参数也提供了显著的灵活性，这是其成功的一个关键因素。因此，训练深度神经网络需要用户能够至少对许多概念有基本了解。具体而言，应用这些技术将有助于解决高级物体识别挑战，并减少整体变化的程度[[29](#bib.bib29)]。
- en: 2 Deep Learning for Robotic Perception
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度学习用于机器人感知
- en: 2.1 Current Robotic Perception Trends
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 当前机器人感知趋势
- en: Although current trends are more leaning to deep and big models, a simplified
    neural network with just a single hidden layer and a basic sigmoid shaped activation
    function will train faster and provide a baseline that is used to give meaning
    to any deeper model improvements. When we use deeper models, Leaky Rectifiers
    are able to normally promote faster training by lowering the impact of the diminishing
    gradient challenge and improving accuracy through using simplified monotonic derivatives
    [[30](#bib.bib30)]. Furthermore, since models with additional weights have increased
    flexibility to over fitting training data, regularization is a vital technique
    in training the best model. In addition, an elastic net is a combination of well-established
    regularization methods used in promoting robustness against weight saturation
    and also promotion sparsity in weights [[31](#bib.bib31)]. However, newer regularization
    methods inclusive of drop-out and drop-connect has attained even better empirical
    outcomes. Furthermore, many regularization methods are also in existence in specifically
    improving the robustness of autoencoders. In this case, special-purpose layers
    can also make a significant distinction with deep neural networks [[32](#bib.bib32)].
    It is a common method to alternate between convolutional and max pool layers.
    These pool layers can lower the general number of weights in the network and also
    allow the model to be able to recognize objects independent of where they are
    placed in the visual field [[33](#bib.bib33)]. On the other hand, batch normalization
    can provide us with significant improvements in rating the convergence by ensuring
    the gradient in range affects the weights of all neurons. In addition, residual
    layers can allow a deeper and consequently more flexible model to be trained [[34](#bib.bib34)].
    To make effective use of deep learning models, it is vital to train one or many
    General Purpose Graphical Processing Units since the other methods of parallelization
    of deep neural networks have been tried but none of them have yet provided gains
    in beneficial performance of General Purpose Graphical Processing Units [[35](#bib.bib35)].
    For a long time until in recent years, robots have long been used in industrial
    environments. In industrial environments, robotic systems are pre-programmed with
    repetitive assignments which lack the capability of autonomy and as such operate
    on the basis of a structured approach. Such an environment cannot be adaptive
    for a mobile robot since it eliminates the need for autonomy. On the other hand,
    mobile robots need less structured environments such that they can be able to
    make their own decisions such as navigate paths, determine whether objects are
    obstacles, recognize images and audio as well as map their environments. As such,
    surviving and adapting in the real world is more complex for any robotic system
    in comparison to the industrial setting since the risk of failure, system error,
    external factors, obstacles, corrupt data, human error and unrecognizable environments
    is more prevalent [[15](#bib.bib15)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管当前趋势更倾向于深度和大型模型，但一个仅有单个隐藏层和基本 S 型激活函数的简化神经网络将会训练更快，并提供一个基准，用于赋予任何更深模型改进的含义。当我们使用更深的模型时，Leaky
    Rectifiers 能够通过降低梯度消失挑战的影响，从而通常促进更快的训练，并通过使用简化的单调导数来提高准确性 [[30](#bib.bib30)]。此外，由于具有额外权重的模型具有增强拟合训练数据的灵活性，正则化是训练最佳模型的关键技术。此外，弹性网络是促进对抗权重饱和和促进权重稀疏性的多种成熟正则化方法的组合
    [[31](#bib.bib31)]。然而，包括 drop-out 和 drop-connect 在内的新正则化方法已经取得了更好的经验性结果。此外，许多正则化方法也专门用于改善自编码器的鲁棒性。在这种情况下，专用层也可以在深度神经网络中产生显著区别
    [[32](#bib.bib32)]。在卷积和最大池化层之间交替是一种常见方法。这些池化层可以降低网络中的权重总数，并且使模型能够独立于它们在视觉场景中的放置位置识别对象
    [[33](#bib.bib33)]。另一方面，批归一化可以通过确保梯度范围对所有神经元的权重产生影响，显著改善收敛评级。此外，残差层可以允许训练更深和因此更灵活的模型
    [[34](#bib.bib34)]。为了有效利用深度学习模型，至关重要的是训练一个或多个通用图形处理单元，因为其他深度神经网络并行化方法已经尝试过，但尚未提供通用图形处理单元在性能上的收益。长期以来直到近年来，机器人一直被用于工业环境。在工业环境中，机器人系统预先编程了重复的任务，缺乏自主性的能力，因此基于结构化方法操作。这样的环境无法适应移动机器人，因为它消除了自主性的需求。另一方面，移动机器人需要较少结构化的环境，以便能够做出自己的决策，例如导航路径，确定物体是否为障碍物，识别图像和音频以及映射其环境。因此，与工业设置相比，生存和适应现实世界对任何机器人系统来说更为复杂，因为失败风险、系统错误、外部因素、障碍物、数据损坏、人为错误和不可识别的环境更为普遍
    [[15](#bib.bib15)]。
- en: 2.2 Machine Learning Usage in Training Robotic System Perception
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 机器学习在训练机器人系统感知中的应用
- en: The difference between deep learning and machine learning is that deep learning
    place emphasis on the subset of machine learning resources and method and uses
    them to solve any difficulties that need ”thought” whether human or artificial.
    Deep learning is also introduced as a means of making sense of data with the use
    of multiple abstraction layers. In the course of the training process, deep neural
    networks are able to learn the means of discovering useful patterns to digitally
    represent data such as sounds and images. This is specifically why we observe
    more advances in the areas of image recognition and natural language processing
    originating from deep learning [[36](#bib.bib36)]. It is on this backdrop that
    deep learning has taken the forefront position in helping researchers develop
    breakthrough methods to the perception capabilities of robotics systems [[16](#bib.bib16)].
    In more simplified language, perception refers to the functionality of robots
    being able to detect its surroundings. It is therefore heavily reliant on multiple
    sources of sensory information. However, with traditional robot technology extracting
    data from raw sensor by using rudimentary constructed sensors theseold methods
    were limited by constraints of adapting to generic settings [[17](#bib.bib17)].
    In situations where these robotic systems faced dynamic environments, they operated
    in an unstructured manner by combining hybrid and autonomous functionality to
    process information about their surroundings [[18](#bib.bib18)]. As such, with
    deep learning came the introduction of new methods of processing data from robotic
    sensorsof a robotic system’s surroundings using a feature known as perception.
    These methods comprise of robot motion, perception, human-interaction, manipulation
    and grasping, automation, self-supervision, self training and learning as well
    as robot vision [[19](#bib.bib19)]. Deep learning models utilize automated actions
    technology at only half the cost by using supervised learning to attain their
    goals [[20](#bib.bib20)]. For example, in order to perfect an image recognition
    application, a neural net will be required to be trained with a collection of
    labeled data. On the other hand, unsupervised learning is how deep learning operates
    and allows for the discovery of new patterns and insights by tackling problems
    with little or no insight on what the results should be perceived by a robot [[21](#bib.bib21)].
    The method by which a mobile robot is able to detect it’s environment with the
    use perception is by using definitive decision-making policies [[20](#bib.bib20)].
    For instance, mobile robots using deep learning are able to navigate with rationality
    by using motion and track precision sensors which are driven machine learning
    algorithms [[21](#bib.bib21)]. However, in difficult environments such as a congested
    room, the level of accurately perceiving their environment is limited. Therefore,
    deep learning based solutions can tackle this challenge by using artificial intelligence,
    high computational hardware and processing layers known as deep convolution neural
    networks to solve this dilemma by successfully deciphering intricate environment
    perception difficulties [[22](#bib.bib22)]. The various obstacles that exist in
    a robot’s environment are an indication of the immense high-dimensional data processing
    capabilitiesrequiredby a robotic system to be able to perceive its surroundings
    [[23](#bib.bib23)]. By using self-supervised, semi-supervised and full supervised
    training coupled with learning, robotic systems are able to utilize their machine
    learning and pattern recognition capabilities to process raw data such as images,
    objects, and semantics, audio as well as process natural language in the real
    time. This segment of deep learning is the best since it uses feed-forward artificial
    neural networks to successfully analyze visual imagery. It also uses multiple
    multilayer perceptrons based on designs that need low preprocessing [[22](#bib.bib22)].
    In comparison to other image classification algorithms, it uses minimal pre-processing
    which means that the network is able to learn filters using automated procedures
    unlike traditional algorithms that are manually engineered. Therefore, by not
    relying on past knowledge and human efforts in the design of features makes it
    a major advantage [[24](#bib.bib24)]. As such in general, this makes deep learning
    capable of the extraction of multi-level attributes from raw sensor data in a
    direct manner with no need for human assisted robotic control [[24](#bib.bib24)].
    This presents researchers with the implication that deep learning programming
    librariessuch as TensorflowTheano of Python, Caffe of C++, darch in R, CNTK, Convent.js
    of Javascript, and Deeplearning4j derived from C++ and Java among others are extremely
    of use in providing robotic systems with a platform to develop their sensory data
    analysis and environmentlearningby using deep learning algorithms [[25](#bib.bib25)].
    Robotic system perception concerns auxiliary functions within mobile robotic are
    vital in interactingwith a robot’s environment.Sensing and intelligent perception
    are some of the applications which are vital since they determine the performance
    of a robotic system. These performances are largely dependent on how robot sensors
    perform. Modern sensors and their functionality can provide impressive robot perception
    which is the foundation of self-adaptive as well as robotic artificial intelligence
    [[26](#bib.bib26)]. The process of changing from sensory input to control output
    using sensory-motor control structures presentsa big difficultyin robotic perception
    [[27](#bib.bib27)]. Some of the vital mobile robot components include the manipulator
    comprising of many joints and connections, a locomotion device, sensors, a controller
    and an endeffector [[37](#bib.bib37)]. Mobile robots are automated systems capable
    of moving. They also have the ability to move around their surroundings and are
    not fixed to a single physical location. With these features mobile robots are
    able to perceive their environments usingsensory data andautonomous control commands
    [[28](#bib.bib28)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习与机器学习之间的区别在于，深度学习强调机器学习资源和方法的子集，并使用它们来解决需要“思考”的任何困难，无论是人类还是人工的。深度学习也被引入为利用多个抽象层次理解数据的手段。在训练过程中，深度神经网络能够学习发现有用模式以数字化表示数据，如声音和图像。这正是为什么我们从深度学习中观察到更多的在图像识别和自然语言处理领域的进展
    [[36](#bib.bib36)]。正是在这个背景下，深度学习在帮助研究人员开发感知能力突破性方法方面占据了前沿位置 [[16](#bib.bib16)]。更简化的说法是，感知指的是机器人能够检测其周围环境的功能。因此，它在很大程度上依赖于多源感官信息。然而，传统的机器人技术通过使用简陋构造的传感器从原始传感器中提取数据，这些老方法受到适应通用设置的约束
    [[17](#bib.bib17)]。在这些机器人系统面对动态环境的情况下，它们通过结合混合和自主功能以无结构方式操作来处理其周围环境的信息 [[18](#bib.bib18)]。因此，随着深度学习的引入，从机器人系统感知环境中处理数据的新方法被称为感知特征。这些方法包括机器人运动、感知、人机交互、操作和抓取、自动化、自我监督、自我训练和学习以及机器人视觉
    [[19](#bib.bib19)]。深度学习模型利用自动化动作技术只需使用监督学习即可实现他们的目标 [[20](#bib.bib20)]。例如，为了完善图像识别应用程序，神经网络将需要用标记数据集进行训练。另一方面，无监督学习是深度学习的操作方式，允许通过解决问题发现新的模式和见解，没有对机器人应该如何感知结果的洞察
    [[21](#bib.bib21)]。移动机器人能够使用感知技术检测其环境的方法是通过使用明确的决策制定政策 [[20](#bib.bib20)]。例如，使用深度学习的移动机器人能够通过使用驱动机器学习算法的运动和跟踪精度传感器来理性导航。然而，在拥挤的房间等困难环境中，准确感知其环境的能力受到限制。因此，基于深度学习的解决方案可以通过使用人工智能、高计算硬件和深度卷积神经网络的处理层来成功解决这一难题，成功解析复杂环境感知困难
    [[22](#bib.bib22)]。机器人系统环境中存在的各种障碍表明机器人系统需要具备的巨大高维数据处理能力来感知其环境 [[23](#bib.bib23)]。通过使用自我监督、半监督和全监督训练以及学习，机器人系统能够利用其机器学习和模式识别能力实时处理原始数据，如图像、对象和语义、音频以及处理自然语言。这一深度学习领域最好使用前馈人工神经网络成功分析视觉图像。它还使用基于低预处理设计的多个多层感知器
    [[22](#bib.bib22)]。与其他图像分类算法相比，它使用最少的预处理，这意味着网络能够使用自动化过程学习过滤器，而不像传统算法那样需要手动设计。因此，不依赖于过去的知识和人类在特征设计上的努力使其具有重大优势
    [[24](#bib.bib24)]。因此，总的来说，这使得深度学习能够直接从原始传感器数据中提取多层次属性，无需人类辅助机器人控制 [[24](#bib.bib24)]。这使得深度学习编程库，如Python的TensorflowTheano、C++的Caffe、R的darch、CNTK、Javascript的Convent.js以及源自C++和Java的Deeplearning4j等在提供机器人系统开发其感知数据分析和环境学习方面极为有用
    [[25](#bib.bib25)]。机器人系统感知是移动机器人辅助功能中的关键部分，对与机器人环境进行交互至关重要。感知和智能感知是一些关键应用，因为它们决定了机器人系统的性能。这些性能在很大程度上取决于机器人传感器的表现。现代传感器及其功能可以提供令人印象深刻的机器人感知，这是自适应和机器人人工智能的基础
    [[26](#bib.bib26)]。从感官输入到控制输出的过程利用感觉-运动控制结构在机器人感知中面临巨大困难 [[27](#bib.bib27)]。一些重要的移动机器人组件包括由多个关节和连接组成的操作器，一个运动装置，传感器，控制器和一个端效应器
    [[37](#bib.bib37)]。移动机器人是能够移动的自动化系统。它们还能够在其周围环境中移动，并且不限于单个物理位置。有了这些特征，移动机器人能够使用感觉数据和自主控制命令感知其环境
    [[28](#bib.bib28)]。
- en: 2.3 Shortcomings of Deep Learning in Robotic Perception
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 深度学习在机器人感知中的不足之处
- en: However, certain challenges remain unresolved in these robotic systems particularly
    the areas of perception and intelligent control. Some of these challenges are
    reflected in the process needing a lot of data to be able to train and teach algorithms
    progressively. Large datasets are required to ensure machines deliver the desired
    outcomes. In the same way as the human brain needs rich experience to learn and
    deduce information, artificial neural networks also need abundant amounts of data.
    This means for more powerful abstractions, more parameters are required and hence
    more data. Another challenge is the tendency of over fitting in neural networks
    whereby in certain cases, there is a sharp distinction between an error within
    a training set and that encountered into new untrained datasets. This problem
    arises when many models make the relative number of parameters fail to reliably
    perform. Therefore, the model only memorizes training examples and fails to learn
    generalization of new situations and new datasets.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些机器人系统中某些挑战仍未解决，尤其是在感知和智能控制方面。其中一些挑战体现在需要大量数据以逐步训练和教导算法。需要大规模的数据集来确保机器能够产生期望的结果。就像人脑需要丰富的经验来学习和推导信息一样，人工神经网络也需要大量的数据。这意味着，为了实现更强大的抽象，所需的参数更多，因此数据也更多。另一个挑战是神经网络的过拟合倾向，在某些情况下，训练集中的错误与新未训练数据集中的错误之间存在明显的区别。当许多模型使得参数的相对数量无法可靠地执行时，这个问题就会出现。因此，模型仅仅记住训练示例，而无法学会对新情况和新数据集进行泛化。
- en: 2.4 How Mobile Robotic Perception Models Can Be Used to Attain Complete Situational
    Awareness
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 如何利用移动机器人感知模型实现全面的情境意识
- en: These capabilities will be surveyed within the research survey through deep
    learning perception model algorithms that are ableto determine how a robot responds
    to the dynamic changes within an environment [[29](#bib.bib29)]. The basis of
    these models revolves around control theory affiliated paradigms for instance
    system stability, control as well as observation [[30](#bib.bib30)]. This theory
    states that a robotic system is able to perceive its environment by using hierarchical
    extensions or enhancements of learning by maximizing the range of its sensor capabilities
    while using path planning algorithms to maneuver around obstacles or paths [[31](#bib.bib31)].
    Most real-time map algorithms are concerned with the acquisition of compact 3D
    mapping within indoor settings with the use of range as well as imaging sensory
    capabilities [[32](#bib.bib32)]. The process of developing models of a robot’s
    environment is a vital problem to deal with more so in regard to managing its
    workspace especially when it is shared with other machinery [[33](#bib.bib33)].
    A mobile robot interacts with the environment by using control systems which define
    structures or obstacles as geometrical areas so as to be able to cover all the
    likely configurations on the robot. Objects or structures are defined according
    to parallelepipeds, spheres, planes, and cylinders. With such a simplified model,
    the mobile robot system is able to define many geometrical areas of this nature
    to cover nearly all objects within its surrounding, for example, moving objects,
    stationary items such as furniture and machinery. Therefore, we propose elementary
    geometrical volumes as a means of modeling a mobile robot’s perception capabilities
    of its environment [[34](#bib.bib34)]. This method will allow the robot to be
    able to move within an environment with the certainty of not colliding with regions
    that are forbidden; these regions must already be defined, declared and activated
    so as to be able to correctly work [[35](#bib.bib35)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些能力将在研究调查中通过深度学习感知模型算法进行调查，这些算法能够确定机器人如何应对环境中的动态变化[[29](#bib.bib29)]。这些模型的基础围绕着控制理论相关的范式，例如系统稳定性、控制以及观察[[30](#bib.bib30)]。该理论指出，机器人系统能够通过利用学习的层次扩展或增强来感知其环境，最大化其传感器能力的范围，同时使用路径规划算法来绕过障碍物或路径[[31](#bib.bib31)]。大多数实时地图算法关注于在室内环境中获取紧凑的3D映射，利用距离和成像感测能力[[32](#bib.bib32)]。开发机器人环境模型的过程是一个重要的问题，特别是在管理其工作空间时，尤其是当它与其他机械设备共享时[[33](#bib.bib33)]。移动机器人通过使用控制系统与环境互动，将结构或障碍物定义为几何区域，以便覆盖机器人上所有可能的配置。对象或结构根据平行六面体、球体、平面和圆柱体来定义。通过这样的简化模型，移动机器人系统能够定义许多这种性质的几何区域，以覆盖其周围几乎所有的物体，例如移动物体、固定物体如家具和机械。因此，我们提出基本几何体积作为建模移动机器人环境感知能力的一种手段[[34](#bib.bib34)]。这种方法将允许机器人在环境中移动时有一定的确定性，以避免与已定义、声明和激活的禁区碰撞，从而正确工作[[35](#bib.bib35)]。
- en: The geometrical perception algorithm will check if the robot end effector is
    within the controlled area or warning zone. This checking is done through the
    use of already stored geometrical areas already defined by the user [[38](#bib.bib38)].
    Similarly, in this context, the position of the dynamic object avails the perception
    system with the likelihood of connecting the geometrical regions with arbitrary
    moving points [[39](#bib.bib39)]. These points can be read from external sensors
    such as encoders. Control of speed is undertaken with the user of geometrical
    area blocks able to detect the shape typology and choose the correct movement
    law to be used so as to modify the robot override and avoid collisions with user-defined
    zones [[40](#bib.bib40)]. The speed override is transformed smoothly when the
    robot end effector collides with a spherical zone in accordance with the perception
    law in ([1](#S2.E1 "In 2.4 How Mobile Robotic Perception Models Can Be Used to
    Attain Complete Situational Awareness ‣ 2 Deep Learning for Robotic Perception
    ‣ A Survey of Deep Learning Techniques for Mobile Robot Applications")) [[3](#bib.bib3)].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 几何感知算法将检查机器人末端执行器是否在受控区域或警告区内。这项检查是通过使用用户已经定义的几何区域来完成的[[38](#bib.bib38)]。类似地，在这种情况下，动态物体的位置为感知系统提供了将几何区域与任意移动点连接的可能性[[39](#bib.bib39)]。这些点可以从外部传感器如编码器读取。速度控制是通过几何区域块来实现的，这些块能够检测形状类型并选择正确的运动规律，以修改机器人覆盖并避免与用户定义的区域发生碰撞[[40](#bib.bib40)]。当机器人末端执行器与球形区域发生碰撞时，速度覆盖会平滑地转变，符合感知法则([1](#S2.E1
    "In 2.4 How Mobile Robotic Perception Models Can Be Used to Attain Complete Situational
    Awareness ‣ 2 Deep Learning for Robotic Perception ‣ A Survey of Deep Learning
    Techniques for Mobile Robot Applications")) [[3](#bib.bib3)]。
- en: '|  | <math   alttext="\begin{split}V=v_{0}.\frac{d-r}{\delta},r\leq
    d\leq r+\delta,\\ V=v_{0},d>r+\delta,and\\'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}V=v_{0}.\frac{d-r}{\delta},r\leq
    d\leq r+\delta,\\ V=v_{0},d>r+\delta,and\\'
- en: V=0,d<r\end{split}" display="block"><semantics ><mtable displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd 
    columnalign="right" ><mrow ><mrow
    ><mrow ><mi
     >V</mi><mo 
    >=</mo><msub ><mi
     >v</mi><mn 
    >0</mn></msub></mrow><mo lspace="0em" rspace="0.167em"
     >.</mo><mrow ><mrow
    ><mfrac  ><mrow
     ><mi 
    >d</mi><mo  >−</mo><mi
     >r</mi></mrow><mi
     >δ</mi></mfrac><mo
     >,</mo><mi 
    >r</mi></mrow><mo  >≤</mo><mi
     >d</mi><mo
     >≤</mo><mrow
    ><mi 
    >r</mi><mo 
    >+</mo><mi 
    >δ</mi></mrow></mrow></mrow><mo 
    >,</mo></mrow></mtd></mtr><mtr ><mtd
     columnalign="right" ><mrow ><mrow
    ><mi  >V</mi><mo
     >=</mo><msub ><mi
     >v</mi><mn 
    >0</mn></msub></mrow><mo 
    >,</mo><mrow ><mi
     >d</mi><mo 
    >></mo><mrow ><mrow
    ><mi  >r</mi><mo
     >+</mo><mi 
    >δ</mi></mrow><mo 
    >,</mo><mrow ><mi
     >a</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >n</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >d</mi></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow
    ><mrow ><mi 
    >V</mi><mo  >=</mo><mn
     >0</mn></mrow><mo
     >,</mo><mrow ><mi
     >d</mi><mo 
    ><</mo><mi  >r</mi></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><csymbol cd="ambiguous" 
    >formulae-sequence</csymbol><apply 
    ><ci  >𝑉</ci><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑣</ci><cn type="integer"
     >0</cn></apply></apply><apply
     ><apply 
    ><list  ><apply
     ><apply 
    ><ci  >𝑑</ci><ci
     >𝑟</ci></apply><ci
     >𝛿</ci></apply><ci
     >𝑟</ci></list><ci 
    >𝑑</ci></apply><apply 
    ><apply  ><ci
     >𝑟</ci><ci
     >𝛿</ci></apply></apply></apply><apply
     ><csymbol cd="ambiguous"
     >formulae-sequence</csymbol><apply
     ><ci 
    >𝑉</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑣</ci><cn type="integer" 
    >0</cn></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >formulae-sequence</csymbol><apply 
    ><ci  >𝑑</ci><apply
     ><ci 
    >𝑟</ci><ci 
    >𝛿</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >formulae-sequence</csymbol><apply 
    ><apply 
    ><ci  >𝑎</ci><ci
     >𝑛</ci><ci
     >𝑑</ci><ci
     >𝑉</ci></apply><cn
    type="integer"  >0</cn></apply><apply
     ><ci 
    >𝑑</ci><ci  >𝑟</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}V=v_{0}.\frac{d-r}{\delta},r\leq
    d\leq r+\delta,\\ V=v_{0},d>r+\delta,and\\ V=0,d<r\end{split}</annotation></semantics></math>
    |  | (1) |
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`V=v_{0}.\frac{d-r}{\delta},r\leq d\leq r+\delta,` 和 `V=v_{0},d>r+\delta,and`
    和 `V=0,d<r`'
- en: In this case, $v$ is represented as the robot end effector actual speed override,
    $v_{0}$ as the old override, $d$ as the distance between the robot end-effector
    and the main spherical area, the thickness of the warning zone is represented
    as $\delta$ while $r$ is the sphere radius [[41](#bib.bib41)]. At thestage when
    the robot meets a cylindrical zone, the speed override will be subjected to the
    perception law in ([2](#S2.E2 "In 2.4 How Mobile Robotic Perception Models Can
    Be Used to Attain Complete Situational Awareness ‣ 2 Deep Learning for Robotic
    Perception ‣ A Survey of Deep Learning Techniques for Mobile Robot Applications"))
    [[3](#bib.bib3)].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，$v$ 表示为机器人末端执行器的实际速度覆盖值，$v_{0}$ 为旧的覆盖值，$d$ 表示机器人末端执行器与主要球形区域之间的距离，警告区域的厚度表示为
    $\delta$，而 $r$ 是球体半径 [[41](#bib.bib41)]。当机器人遇到圆柱形区域时，速度覆盖值将受到在 ([2](#S2.E2 "In
    2.4 How Mobile Robotic Perception Models Can Be Used to Attain Complete Situational
    Awareness ‣ 2 Deep Learning for Robotic Perception ‣ A Survey of Deep Learning
    Techniques for Mobile Robot Applications")) [[3](#bib.bib3)] 的感知法则的影响。
- en: '|  | <math   alttext="\begin{split}V=v_{0}.\frac{d_{1}-r}{R-r},0\leq
    z\leq h,\\ V=v_{0}.\frac{d_{2}}{\delta},h\leq z\leq h+\delta,-\delta\leq z<0,p\in
    cyl,and\\'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}V=v_{0}.\frac{d_{1}-r}{R-r},0\leq
    z\leq h,\\ V=v_{0}.\frac{d_{2}}{\delta},h\leq z\leq h+\delta,-\delta\leq z<0,p\in
    cyl,and\\'
- en: V=v_{0}.\frac{d_{3}}{\delta},h\leq z\leq h+\delta,-\delta\leq z<0,p\in cyl\end{split}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd 
    columnalign="right" ><mrow ><mrow
    ><mrow ><mi
     >V</mi><mo 
    >=</mo><msub ><mi
     >v</mi><mn 
    >0</mn></msub></mrow><mo lspace="0em" rspace="0.167em"
     >.</mo><mrow ><mrow
    ><mfrac  ><mrow
     ><msub 
    ><mi  >d</mi><mn
     >1</mn></msub><mo
     >−</mo><mi 
    >r</mi></mrow><mrow 
    ><mi  >R</mi><mo
     >−</mo><mi 
    >r</mi></mrow></mfrac><mo 
    >,</mo><mn  >0</mn></mrow><mo
     >≤</mo><mi 
    >z</mi><mo 
    >≤</mo><mi 
    >h</mi></mrow></mrow><mo 
    >,</mo></mrow></mtd></mtr><mtr ><mtd
     columnalign="right" ><mrow ><mrow
    ><mi  >V</mi><mo
     >=</mo><msub ><mi
     >v</mi><mn 
    >0</mn></msub></mrow><mo lspace="0em" rspace="0.167em"
     >.</mo><mrow ><mrow
    ><mfrac  ><msub
     ><mi 
    >d</mi><mn 
    >2</mn></msub><mi 
    >δ</mi></mfrac><mo 
    >,</mo><mi  >h</mi></mrow><mo
     >≤</mo><mi 
    >z</mi><mo 
    >≤</mo><mrow ><mi
     >h</mi><mo
     >+</mo><mi
     >δ</mi></mrow></mrow><mo
     >,</mo><mrow ><mrow
    ><mo 
    >−</mo><mi  >δ</mi></mrow><mo
     >≤</mo><mi
     >z</mi><mo
     ><</mo><mn
     >0</mn></mrow><mo
     >,</mo><mrow ><mi
     >p</mi><mo
     >∈</mo><mrow
    ><mrow ><mi
     >c</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >y</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >l</mi></mrow><mo
     >,</mo><mrow ><mi
     >a</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >n</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >d</mi></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow
    ><mrow ><mi
     >V</mi><mo 
    >=</mo><msub ><mi
     >v</mi><mn 
    >0</mn></msub></mrow><mo lspace="0em" rspace="0.167em"
     >.</mo><mrow ><mrow
    ><mfrac  ><msub
     ><mi 
    >d</mi><mn 
    >3</mn></msub><mi 
    >δ</mi></mfrac><mo 
    >,</mo><mi  >h</mi></mrow><mo
     >≤</mo><mi 
    >z</mi><mo 
    >≤</mo><mrow ><mi
     >h</mi><mo
     >+</mo><mi
     >δ</mi></mrow></mrow><mo
     >,</mo><mrow ><mrow
    ><mo 
    >−</mo><mi  >δ</mi></mrow><mo
     >≤</mo><mi
     >z</mi><mo
     ><</mo><mn
     >0</mn></mrow><mo
     >,</mo><mrow ><mi
     >p</mi><mo
     >∈</mo><mrow
    ><mi  >c</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >y</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >l</mi></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><csymbol cd="ambiguous" 
    >formulae-sequence</csymbol><apply 
    ><ci  >𝑉</ci><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑣</ci><cn type="integer"
     >0</cn></apply></apply><apply
     ><apply 
    ><list  ><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑑</ci><cn
    type="integer"  >1</cn></apply><ci
     >𝑟</ci></apply><apply
     ><ci 
    >𝑅</ci><ci  >𝑟</ci></apply></apply><cn
    type="integer"  >0</cn></list><ci
     >𝑧</ci></apply><apply
     ><ci 
    >ℎ</ci></apply></apply><apply 
    ><ci  >𝑉</ci><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑣</ci><cn type="integer"
     >0</cn></apply></apply><apply
     ><apply 
    ><list  ><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑑</ci><cn type="integer" 
    >2</cn></apply><ci 
    >𝛿</ci></apply><ci 
    >ℎ</ci></list><ci 
    >𝑧</ci></apply><apply 
    ><apply  ><ci
     >ℎ</ci><ci
     >𝛿</ci></apply></apply></apply><apply
     ><apply 
    ><apply  ><ci
     >𝛿</ci></apply><ci
     >𝑧</ci></apply><apply
     ><cn type="integer"
     >0</cn></apply></apply><apply
     ><csymbol cd="ambiguous"
     >formulae-sequence</csymbol><apply
     ><ci 
    >𝑝</ci><apply 
    ><ci  >𝑐</ci><ci
     >𝑦</ci><ci
     >𝑙</ci></apply></apply><apply
     ><apply 
    ><ci  >𝑎</ci><ci
     >𝑛</ci><ci
     >𝑑</ci><ci
     >𝑉</ci></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑣</ci><cn type="integer"
     >0</cn></apply></apply></apply><apply
     ><apply 
    ><list  ><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑑</ci><cn type="integer" 
    >3</cn></apply><ci 
    >𝛿</ci></apply><ci 
    >ℎ</ci></list><ci 
    >𝑧</ci></apply><apply 
    ><apply  ><ci
     >ℎ</ci><ci
     >𝛿</ci></apply></apply></apply><apply
     ><apply 
    ><apply  ><ci
     >𝛿</ci></apply><ci
     >𝑧</ci></apply><apply
     ><cn type="integer"
     >0</cn></apply></apply><apply
     ><ci 
    >𝑝</ci><apply 
    ><ci  >𝑐</ci><ci
     >𝑦</ci><ci
     >𝑙</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}V=v_{0}.\frac{d_{1}-r}{R-r},0\leq
    z\leq h,\\ V=v_{0}.\frac{d_{2}}{\delta},h\leq z\leq h+\delta,-\delta\leq z<0,p\in
    cyl,and\\ V=v_{0}.\frac{d_{3}}{\delta},h\leq z\leq h+\delta,-\delta\leq z<0,p\in
    cyl\end{split}</annotation></semantics></math> |  | (2) |
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: V=`v_{0}`.\frac{d_{3}}{\delta}, h\leq z\leq h+\delta, -\delta\leq z<0, p\in
    cyl\end{split}" display="block"><semantics ><mtable displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd 
    columnalign="right" ><mrow ><mrow
    ><mrow ><mi
     >V</mi><mo 
    >=</mo><msub ><mi
     >v</mi><mn 
    >0</mn></msub></mrow><mo lspace="0em" rspace="0.167em"
     >.</mo><mrow ><mrow
    ><mfrac  ><mrow
     ><msub 
    ><mi  >d</mi><mn
     >1</mn></msub><mo
     >−</mo><mi 
    >r</mi></mrow><mrow 
    ><mi  >R</mi><mo
     >−</mo><mi 
    >r</mi></mrow></mfrac><mo 
    >,</mo><mn  >0</mn></mrow><mo
     >≤</mo><mi 
    >z</mi><mo 
    >≤</mo><mi 
    >h</mi></mrow></mrow><mo 
    >,</mo></mrow></mtd></mtr><mtr ><mtd
     columnalign="right" ><mrow ><mrow
    ><mi  >V</mi><mo
     >=</mo><msub ><mi
     >v</mi><mn 
    >0</mn></msub></mrow><mo lspace="0em" rspace="0.167em"
     >.</mo><mrow ><mrow
    ><mfrac id="S2.E2.m1.
- en: The perception law above represents $h$ as the cylinder height, the position
    of the robot end effector is denoted as $p$, the distance between the center of
    the cylinder and the position of the robot is denoted as $d_{1}$ while the $d_{2}$
    represents the distance between the top or bottom base of the cylinder as well
    as the position of the robot [[42](#bib.bib42)].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述感知法则将 $h$ 表示为圆柱体的高度，机器人的末端执行器位置记作 $p$，圆柱体中心与机器人位置之间的距离记作 $d_{1}$，而 $d_{2}$
    表示圆柱体顶部或底部基座与机器人位置之间的距离 [[42](#bib.bib42)]。
- en: In addition, the least position between the position of the robot and the top/bottom
    circumference points of the cylinder base is denoted as $d_{3}$. Furthermore,
    the robot speed override coincides with the past speed override at the stage when
    the robot end effector is outside the warning zone [[43](#bib.bib43)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，机器人位置与圆柱体顶部/底部周长点之间的最小距离记作 $d_{3}$。此外，当机器人末端执行器位于警告区域之外时，机器人的速度超控与过去的速度超控相一致
    [[43](#bib.bib43)]。
- en: 3 Deep Learning for Robotic Control and Exploration
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习在机器人控制和探索中的应用
- en: 3.1 How Autonomous Robotic Systems Use Deep Learning to Control and Explore
    Their Enivornments
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 自主机器人系统如何使用深度学习来控制和探索其环境
- en: Realizing the benefits of autonomous robot exploration presents robotics researchers
    with many applications of considerable community and financial impact [[44](#bib.bib44)].
    Robotics research relies on perfect knowledge and control of the environment [[45](#bib.bib45)].
    The problems related to unstructured environments are an outcome of the high-dimensional
    state space as well as the inherent likelihood in mapping sensory perceptions
    on particular states. It should be noted that the high dimensionality of the state
    space is representative of the most basic difficulty since robots leave highly
    controlled environments of a laboratory and enter into unstructured surroundings.
    For example autonomous unmanned aerial vehicles used deep learning to classify
    terrain and solve any exploration shortcomings by generating control commands
    for its human operator so as to adapt to a certain trade-off [[46](#bib.bib46)].
    The major hypothesis of this approach is therefore for mobile robots to succeed
    in unstructured surroundings such that they can carefully choose assignment specific
    attributes and identify the relevant real-time structures to lower their state
    space without impacting the performance of their exploration objectives [[47](#bib.bib47)].
    Robots perform assignments by exploring their surroundings. As such, given our
    focus on autonomous mobile exploration, we shall direct most attention to exploration
    in service of movement, that is to say collision-free movement for end-effector
    placement [[48](#bib.bib48)]. The challenge of generating such movement is an
    example of the problem faced in motion planning. Motion planning for robotic systems
    with many levels of freedom is computationally challenging even in environments
    that are highly structured due to the increased-dimensional configuration space
    [[49](#bib.bib49)]. In addition, unstructured environments are associated with
    the imposition of added difficulties in motion generation in comparison to the
    traditional motion planning process. Furthermore, in environments that are unstructured,
    a robot is only able to possess limited knowledge of its environment, objects
    are able to change their unknown to a known state for the robot by manipulating
    assignments may using the end effector [[50](#bib.bib50)]. However, this capability
    can be challenged by to a constrained trajectory such that the mobile robot is
    unable to reach a particular location with ease [[51](#bib.bib51)]. Each of these
    problems makes the challenge of motion generation more complex. The explicit coordination
    of planning and sensing necessary to manage dynamic environments increases the
    dimension of the state space. In addition, robotic assignment requirements impose
    stringent terms in form of high-frequency feedback [[52](#bib.bib52)]. Therefore,
    existing motion planners tend to make assumptions that are highly restrictive
    for environments that are unstructured since they are highly computationally difficult
    to satisfy in terms of gaining operator-to-robot feedback [[53](#bib.bib53)].
    These assumptions, as well as the computational difficulty, are an outcome of
    the foundation of motion planning with its high-dimensional configuration spacing
    which makes it highly unsuited in solving space problems. Planners instead can
    be able to solve this paradigm by solely using workspace information for collision
    avoidance. Nearly all real-world surroundings, however, comprised of a considerable
    measure of structure [[54](#bib.bib54)]. For instance, buildings are segmented
    into hallways, doors, and rooms; outdoor environments comprise of paths, streets,
    and intersections while objects for example tables, shelves and chairs have more
    favorable approach directions. This information, however, is neglected when robot
    exploration planners exclusively operate on the basis of a configuration space
    [[55](#bib.bib55)]. Therefore, the outcome for most robotic exploration planners
    is to operate by the assumption that any environment is perfectly defined and
    sustained as static in the course of planning [[56](#bib.bib56)].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 实现自主机器人探索的好处为机器人研究人员提供了许多具有重大社区和财务影响的应用[[44](#bib.bib44)]。机器人研究依赖于对环境的完美知识和控制[[45](#bib.bib45)]。与非结构化环境相关的问题是由于高维状态空间以及将传感器感知映射到特定状态的固有可能性所产生的。需要注意的是，状态空间的高维度是最基本的困难，因为机器人离开高度控制的实验室环境，进入非结构化的周围环境。例如，使用深度学习的自主无人机对地形进行分类，并通过生成控制命令来解决任何探索上的不足，以便对某种折衷进行适应[[46](#bib.bib46)]。因此，这种方法的主要假设是移动机器人在非结构化环境中成功，以便它们可以仔细选择任务特定属性，并识别相关的实时结构，以降低其状态空间，而不影响其探索目标的性能[[47](#bib.bib47)]。机器人通过探索其周围环境来执行任务。因此，鉴于我们对自主移动探索的关注，我们将主要注意力集中在服务于运动的探索上，即无碰撞运动以进行末端执行器定位[[48](#bib.bib48)]。产生这种运动的挑战是运动规划中面临的一个例子。由于维度配置空间的增加，具有许多自由度的机器人系统的运动规划在高度结构化的环境中甚至也具有计算挑战[[49](#bib.bib49)]。此外，非结构化环境相较于传统运动规划过程，还会增加运动生成的难度。此外，在非结构化环境中，机器人只能对其环境拥有有限的知识，通过操作分配来使用末端执行器可以将对象从未知状态转变为已知状态[[50](#bib.bib50)]。然而，这种能力可能受到受限轨迹的挑战，以至于移动机器人无法轻松到达特定位置[[51](#bib.bib51)]。这些问题使得运动生成的挑战变得更加复杂。管理动态环境所需的规划和传感的明确协调增加了状态空间的维度。此外，机器人任务要求以高频反馈的形式施加严格的条件[[52](#bib.bib52)]。因此，现有的运动规划者往往对非结构化环境做出高度限制性的假设，因为这些假设在获得操作员与机器人反馈方面非常难以满足[[53](#bib.bib53)]。这些假设以及计算难度，都是运动规划基础的结果，其高维配置空间使得解决空间问题变得非常不适合。规划者可以通过仅使用工作空间信息来避免碰撞来解决这一范式。然而，几乎所有现实世界的环境都包含了相当程度的结构[[54](#bib.bib54)]。例如，建筑物被划分为走廊、门和房间；户外环境包括路径、街道和交叉路口，而例如桌子、架子和椅子等物体有更有利的接近方向。然而，当机器人探索规划者仅基于配置空间操作时，这些信息却被忽视了[[55](#bib.bib55)]。因此，大多数机器人探索规划者的结果是基于假设任何环境在规划过程中是完全定义并保持静态的[[56](#bib.bib56)]。
- en: 4 Deep Learning in Robotic Robotic Navigation and Autonomous Driving
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度学习在机器人导航和自动驾驶中的应用
- en: Using deep learning to attain autonomous driving assignment is not a perfectly
    controlled and modeled task as most people think. Instead, it needs optimal perceptual
    capabilities [[57](#bib.bib57)]. The process of perception of a robot’s environment
    and interpreting the information it acquires allows it to understand the condition
    of its surroundings, devise plans to change the state and observe how its actions
    impact its environment. In unstructured environments, recognition of objects has
    been proven to be highly challenging. With immense volumes of sensor data and
    increased variation of objects within similar object categories, for example,
    a paved and unpaved road as well as recognizing objects [[58](#bib.bib58)]. Deep
    learning uses machine learning motion capturing abilities as well as optimal perceptual
    functionalities. This is a prerequisite of several vital applications for robots
    for instance flexible manufacturing, planetary exploration, collaboration with
    human experts and elder care [[59](#bib.bib59)]. The challenge of driving in an
    environment includes problems of movement of the robot in navigating varied obstacles
    by pushing and pulling. Even in structured environments, automated driving is
    difficult due to the complexities of the related state space [[60](#bib.bib60)].
    This state space comprises of appearance, dimension, position as well as the weight
    of objects within the scene. It also comprises of several other relevant attributes
    which provide indications of where to pull, push or grasp as well as the level
    of force to apply [[61](#bib.bib61)]. Deep learning and associated machine learning
    collective actions improved the performance of robots in making the decision by
    embedding the capability within these systems to choose between possible actions
    and determine the required parameters for their controllers.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习来实现自主驾驶任务并不像大多数人认为的那样是一个完全受控和建模的任务。相反，它需要最优的感知能力[[57](#bib.bib57)]。机器人感知环境和解读其获取的信息的过程使其能够理解周围环境的状况，制定改变状态的计划，并观察其行为对环境的影响。在非结构化环境中，物体识别已被证明极具挑战性。例如，区分铺砌和非铺砌的道路以及识别物体[[58](#bib.bib58)]。深度学习利用机器学习运动捕捉能力以及最优的感知功能。这是机器人若干关键应用的前提，例如灵活制造、行星探索、与人类专家的合作以及老人护理[[59](#bib.bib59)]。在环境中驾驶的挑战包括机器人在导航不同障碍物时的移动问题，比如推和拉。即使在结构化环境中，由于相关状态空间的复杂性，自动驾驶仍然困难[[60](#bib.bib60)]。这个状态空间包括场景中物体的外观、尺寸、位置以及重量。它还包括其他若干相关属性，这些属性提供了拉、推或抓取的位置以及施加力量的程度的指示[[61](#bib.bib61)]。深度学习及其相关的机器学习集体行动提高了机器人决策的性能，通过将选择可能行动和确定其控制器所需参数的能力嵌入这些系统中。
- en: 4.1 How Autonomous Robotic Systems Use Deep Learning to Navigate Their Enivornments
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 自主机器人系统如何利用深度学习来导航其环境
- en: Autonomous driving in unstructured environments faces many challenges which
    do not exist in structured environments. In unstructured environments, object
    attributes needed for driving cannot be defined as priori. Information concerning
    objects has to be gained through sensors even though these are normally ambiguous
    and therefore introduce uncertainty and avail information that is redundant. Furthermore,
    autonomous driving in unstructured and dynamic surroundings normally needs responding
    in a fashion that is timely to a rapidly transforming environment [[62](#bib.bib62)].
    Problems of autonomous driving can be made simple through the exploitation of
    the structure which is inherent to human surroundings. Most objects in the real
    world are based on designs to perform certain functions with the intention of
    being utilized by humans [[44](#bib.bib44)]. As an outcome, several real-world
    objects can share common attributes which allude to their intended usage. By placing
    emphasis on these assignment-related object attributes, the complexity of autonomous
    driving is lowered. For example, visual data can be analyzed for the identification
    of few points which correspond to positive locations at which a robot can maneuver.
    Furthermore, since movement features are similar across many objects, robots could
    be trained to identify them. As a consequence, the state space that requires exploration
    in order to move is significantly lowered [[63](#bib.bib63)]. In this case, researchers
    normally make assumptions to lower the complexity of autonomous driving in unstructured
    environments. For instance, it is normally assumed that full models of objects
    in the environment can be availed a priori or it can be gained through sensors
    while the environment remains the same in the process of interaction [[64](#bib.bib64)].
    However, in practical terms, it is impossible to avail autonomous driving with
    full priori models in the actual world. However, models that are perfect are not
    a prerequisite for successful autonomous driving [[65](#bib.bib65)]. Robot mobile
    driving can be guided using existing structures in the world and in most cases
    those which are easy to perceive. As such, by leveraging this structure, the complexity
    of autonomous driving in unstructured environments is lowered significantly. Similarly,
    understanding the intrinsic measures of freedom of objects in an environment is
    also able to lower the complexity of autonomous driving in unstructured surroundings
    [[45](#bib.bib45)].
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在非结构化环境中，自主驾驶面临许多在结构化环境中不存在的挑战。在非结构化环境中，驾驶所需的物体属性无法事先定义。有关物体的信息必须通过传感器获得，尽管这些传感器通常是模糊的，因此引入了不确定性，并提供冗余的信息。此外，在非结构化和动态环境中的自主驾驶通常需要及时响应迅速变化的环境[[62](#bib.bib62)]。通过利用人类环境中固有的结构，可以简化自主驾驶的问题。现实世界中的大多数物体都是基于执行某些功能的设计，目的是被人类使用[[44](#bib.bib44)]。因此，一些现实世界的物体可以共享指向其预期用途的共同属性。通过强调这些与任务相关的物体属性，可以降低自主驾驶的复杂性。例如，视觉数据可以被分析以识别几个点，这些点对应于机器人可以操控的积极位置。此外，由于许多物体的运动特征相似，机器人可以被训练以识别这些物体。因此，需要探索的状态空间显著减少[[63](#bib.bib63)]。在这种情况下，研究人员通常会做出假设，以降低在非结构化环境中自主驾驶的复杂性。例如，通常假设环境中物体的完整模型可以事先获得，或者可以通过传感器获得，同时环境在交互过程中保持不变[[64](#bib.bib64)]。然而，实际情况是，在现实世界中，无法提供具有完整先验模型的自主驾驶。然而，完美的模型并不是成功自主驾驶的前提条件[[65](#bib.bib65)]。机器人移动驾驶可以利用世界上现有的结构，并且在大多数情况下，这些结构容易被感知。因此，通过利用这些结构，可以显著降低在非结构化环境中的自主驾驶复杂性。同样，理解环境中物体的内在自由度也能够降低在非结构化环境中的自主驾驶复杂性[[45](#bib.bib45)]。
- en: 5 Semi-Supervised and Self-Supervised Learning for Robotics
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 半监督和自监督学习在机器人中的应用
- en: Imitation based learning is a promising approach to tackling the difficult robotic
    assignments, for instance, autonomous navigation. However, it needs human supervision
    to oversee the process of training and sending correct control commands to robots
    without feedback. It is this form of procedure that is prone to failure and high
    cost [[66](#bib.bib66)]. Therefore, in order to lower human involvement and limit
    manual data labeling of autonomous robotic navigation using imitation learning,
    the techniques of semi-supervised and self-supervised learning can be introduced.
    It should be noted however that these techniques need to operate according to
    a multi-sensory design approach. The solution should comprise of a suboptimal
    sensor policy founded on sensor fusion and automatic labeling of states the robot
    could encounter [[67](#bib.bib67)]. This is also aimed at eliminating human supervision
    in the course of the learning process [[68](#bib.bib68)] [[69](#bib.bib69)]. Furthermore,
    a recording policy needs to be developed to provide throttling of the adversarial
    impact of too much data being learned from the suboptimal sensor policy. As such,
    this solution will equip the robot with the capability of achieving near-human
    performance to a large extent in most of its assignments [[70](#bib.bib70)]. It
    is also capable of surpassing human performance in situations of unexpected outcomes
    for instance hardware failure or human operator error. Furthermore, the semi-supervised
    method can be considered as a solution to the problem of track classification
    in congested environments such as a room. This problem entails object classification
    undergoing segmenting and tracking without using class models [[71](#bib.bib71)].
    Therefore, we introduce semi-supervised learning as a technique capable of solving
    this problem by iteratively training a classifier and extracting vital training
    examples from the data in its unlabeled state. This is achieved by exploiting
    the tracking data. In addition, the process also involves evaluating large multiclass
    difficulties presented by data sourced from congested artificial natural environments
    such as a street [[72](#bib.bib72)]. As such, when provided with manually labeled
    training tracks of individual object classes, then semi-supervised learning performance
    in comparison to self-supervised learning is able to use thousands of training
    tracks [[73](#bib.bib73)]. In addition, when also provided with augmented unlabeled
    data, semi-supervised learning has demonstrated the capability of outperforming
    the self-supervised learning method. In this case, semi-supervised learning presents
    itself as the most simplified algorithmic approach to speeding up incremental
    updating of booster classifiers by lowering the learning time factor by three
    [[74](#bib.bib74)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模仿的学习是一种有前景的方法，用于解决困难的机器人任务，例如自主导航。然而，它需要人工监督来监管训练过程，并在没有反馈的情况下向机器人发送正确的控制指令。正是这种过程形式容易导致失败和高成本[[66](#bib.bib66)]。因此，为了减少人工干预并限制模仿学习中自主机器人导航的手动数据标注，可以引入半监督学习和自监督学习技术。然而，需要注意的是，这些技术需要按照多感知设计方法来操作。解决方案应包括基于传感器融合和自动标注机器人可能遇到的状态的次优传感器策略[[67](#bib.bib67)]。这也是为了在学习过程中消除人工监督[[68](#bib.bib68)]
    [[69](#bib.bib69)]。此外，需要制定记录策略，以缓解从次优传感器策略中学习过多数据的对抗性影响。因此，该解决方案将使机器人具备在大多数任务中达到接近人类表现的能力[[70](#bib.bib70)]。在面对意外结果，例如硬件故障或操作员错误的情况下，它也能够超越人类表现。此外，半监督方法可以作为解决拥挤环境（例如房间）中轨迹分类问题的解决方案。这个问题涉及到物体分类、分割和跟踪，而不使用类模型[[71](#bib.bib71)]。因此，我们引入半监督学习作为一种能够通过迭代训练分类器并从未标记数据中提取重要训练样本的技术。这是通过利用跟踪数据来实现的。此外，该过程还涉及评估来自拥挤的人工自然环境（例如街道）的数据所呈现的大规模多类难题[[72](#bib.bib72)]。因此，当提供手动标记的单个对象类别训练轨迹时，半监督学习的性能相较于自监督学习能够使用数千个训练轨迹[[73](#bib.bib73)]。此外，当提供增强的未标记数据时，半监督学习显示出超越自监督学习方法的能力。在这种情况下，半监督学习呈现为加速增量更新助推分类器的最简化算法方法，通过将学习时间因素降低三倍[[74](#bib.bib74)]。
- en: 6 Multimodal Deep Learning Methods for Robotic Vision and Control
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 种用于机器人视觉与控制的多模态深度学习方法
- en: Performing tasks in an imperfect controlled and modeled surrounding means robots
    need to have optimal multimodal capabilities. The procedure of perceiving the
    environment and interpreting the gained information allows robots to perceive
    the state of the environment, devise methods to alter the state and observe the
    impacts of their actions on the environment
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在不完美的受控和建模环境中执行任务意味着机器人需要具备最佳的多模态能力。感知环境并解释获得的信息的过程使得机器人能够感知环境状态，制定改变状态的方法，并观察其行动对环境的影响。
- en: 6.1 How Autonomous Robotic Systems Use Semi-Supervised and Self-Supervised Learning
    to Learn Their Enivornments
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 自主机器人系统如何使用半监督和自监督学习来学习其环境
- en: The environment of a robot can be controlled too many levels. In principle,
    less constrained environments are more difficult to perceive. In the real-world
    and its unstructured and dynamic surroundings such as vegetation landscape and
    terrain, the perception of a mobile robot needs to be capable of navigating this
    unknown environment by using sensor modalities. More so, even without the introduction
    of uncertainty, sensors in themselves are ambiguous [[75](#bib.bib75)]. For example,
    a lemon and a soccer ball can look similar from a certain perspective. In addition,
    a cup could be invisible in case the cupboard is shut and it can be challenging
    to tell the difference between a remote control and cell phone is they are both
    facing down. These factors are all contributive to the challenges of perceiving
    the state of the environment. Furthermore, for example, advances in face recognition
    normally operate under the assumption concerning the position and orientation
    of the individual in the image. The outcomes of object segmentation are normally
    founded on the capability of telling the difference between an object and background
    on the basis of differences in color [[76](#bib.bib76)]. In addition, object recognition
    is normally reduced to similarities in computing to a limited collection of given
    objects. On the other hand, in an unstructured environment, the position and orientation
    are uncontrollable since assumptions concerning color and shades and problematic
    to justify. Furthermore, the range and likely objects the robot could encounter
    are intractable [[77](#bib.bib77)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人的环境可以在多个层次上进行控制。原则上，约束较少的环境更难以感知。在现实世界及其如植被景观和地形这样的非结构化和动态环境中，移动机器人需要能够利用传感器模态在未知环境中导航。更重要的是，即使不考虑不确定性，传感器本身也是模糊的[[75](#bib.bib75)]。例如，从某个角度来看，柠檬和足球可能看起来很相似。此外，如果橱柜关闭，杯子可能会变得不可见，而在遥控器和手机都朝下的情况下，很难区分它们。这些因素都增加了感知环境状态的挑战。此外，例如，人脸识别的进展通常在假设图像中个体的位置和方向的基础上进行。对象分割的结果通常基于通过颜色差异区分物体和背景的能力[[76](#bib.bib76)]。另外，对象识别通常被简化为对有限集合中的给定对象的相似性进行计算。另一方面，在非结构化环境中，位置和方向是不可控的，因为对颜色和阴影的假设难以证明。此外，机器人可能遇到的范围和可能对象是不可处理的[[77](#bib.bib77)]。
- en: 6.2 How Autonomous Robotic Systems Use Multimodal and Deep Learning Methods
    to Percieve Their Enivornments
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 自主机器人系统如何使用多模态和深度学习方法来感知其环境
- en: Therefore, in order to tackle perception in unstructured surroundings, robots
    need to be able to lower the state space that requires being analyzed. to provide
    facilitation of certain perceptual assignments by limiting uncertainty and as
    such lowering the dimensionality of the state space. For instance, in order to
    compute the distance of objects in an environment, robots need to relate depth
    to visual information [[78](#bib.bib78)]. This is normally done with the use of
    a stereo vision system and solving the correspondence challenge between two static
    2D images. In addressing the correspondence problem, however, it is complicated
    due to noise, many likely matches and the uncertainty in calibrating the camera
    [[79](#bib.bib79)]. On the other hand, in a system capable of the capture of at
    least three view angles in one image, this lowers the state space by reducing
    a multi-sensor system to a single sensor. Furthermore, in an unstructured environment,
    recognition of objects has proven to be highly challenging [[57](#bib.bib57)].
    This is due to large volumes of sensor data and an increased variation within
    objects of a similar category. In this case, object recognition is an increased
    dimensional challenge. However, even in the face of these challenges, objects
    in the similar category do share similar attributes. As such, by applying this
    insight, robots are able to place emphasis to only a minimal subset of the state
    space that comprises of the most relevant characteristics for classification [[58](#bib.bib58)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了应对不结构化环境中的感知，机器人需要能够降低需要分析的状态空间。通过限制不确定性并因此降低状态空间的维度来提供某些感知任务的便利。例如，为了计算环境中物体的距离，机器人需要将深度与视觉信息相关联[[78](#bib.bib78)]。这通常通过使用立体视觉系统并解决两个静态2D图像之间的对应问题来完成。然而，在解决对应问题时，由于噪声、许多可能的匹配以及相机标定的不确定性，问题变得复杂[[79](#bib.bib79)]。另一方面，在一个能够在一张图像中捕获至少三个视角的系统中，这通过将多传感器系统减少为单传感器来降低状态空间。此外，在不结构化环境中，物体识别被证明是非常具有挑战性的[[57](#bib.bib57)]。这是由于大量的传感器数据以及同一类别物体之间的变化增加。在这种情况下，物体识别是一个增加维度的挑战。然而，即使面对这些挑战，同一类别的物体确实具有相似的属性。因此，通过应用这一见解，机器人能够将重点放在仅由最相关特征组成的状态空间的最小子集上，以进行分类[[58](#bib.bib58)]。
- en: 7 Application of Deep Models to Problems in Vision and Robotics
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 深度模型在视觉和机器人问题中的应用
- en: The preceding overview of machine learning applications in robotics will highlight
    five major areas where considerable impacts have been made by robotic technologies
    currently and in the development levels for long-term use. However, by no means
    inclusive, the aim of this summarization is to provide the reader with a preview
    of the form of machine learning applications in existence within robotics and
    motivate the desire for extended research in such and other fields [[80](#bib.bib80)].
    The growth of big data which is to day visual information provided on the internet
    with the inclusion of annotated images and video has pushed forward advancements
    in computer vision which has in turn assisted in extending machine-based learning
    systems to prediction learning methods such as those presented by research at
    Carnegie Mellon [[81](#bib.bib81)]. This presentation involved unveiling offshoot
    examples such as the anomaly detection using supervised learning have been applied
    in building structures with the capability of searching and assessing damages
    in silicon wafers with the use of convolutional neural networks [[81](#bib.bib81)].
    In addition, extrasensory technologies for instance lidar, ultrasound, and radar
    such as those developed by Nvidia as also propelling the creation of 360-degree
    vision-based systems for autonomous vehicles and UAVs [[82](#bib.bib82)]. Imitation
    learning which is closely associated with observational learning is also a field
    categorized by reinforcement learning or the difficulties of gaining an agent
    to act towards maximizing rewards. One example is Bayesian or probabilistic models
    which stand out as a common machine learning method used an integral component
    of field robotics where attributes of mobility in fields such as construction,
    rescue, and inverse optimal control methods have been utilized in humanoid robotics,
    off-road terrain navigation, and legged locomotion [[83](#bib.bib83)]. Self-supervised
    learning is another method that allows robots to generate personalized training
    instances so as to refine performance. This has been integrated robots and optical
    devices for instance in detection and rejection of objects such as dust and snow,
    identification of obstacles, vehicle dynamics modeling, and 3D-scene analysis
    [[84](#bib.bib84)]. Assistive and medical technologies are another application
    where assistive robots entail devices capable of sensing, processing sensory data
    and undertaking actions that gain individuals with disabilities. Even as smart
    assistive technologies are existent for the overall population, for instance,
    driver assistance resources, movement therapy robots avail diagnostic or therapeutic
    gains. Furthermore, multi-agent learning concerning coordination and negotiation
    are vital components involving machine learning based robots also known as agents.
    This method is broadly utilized in games that are capable of adapting to a transforming
    landscape of other robots or agents and searching for equilibrium strategies [[76](#bib.bib76)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 之前对机器人领域机器学习应用的概述将突出五个主要领域，这些领域在当前的机器人技术中和长期使用的发展阶段中产生了显著影响。然而，这并不是全面的总结，旨在为读者提供对机器人领域现有机器学习应用形式的预览，并激发对这些领域及其他领域进一步研究的兴趣[[80](#bib.bib80)]。大
- en: The interdisciplinary arena of computer vision concerned with how computers
    are able to be developed for attaining an increased measure of perception from
    the digital imagery of video. Computer vision assignments comprise of techniques
    for acquisition, processing, analysis, perceiving digital imagery as well as extracting
    high dimensional information from the actual world so as to yield numerical or
    symbolic data for example in the format of decisions [[85](#bib.bib85)]. Artificial
    intelligence areas concerned with autonomous planning or deliberated robotic systems
    navigation demand a thorough perception of such settings since information concerning
    the environment can be availed by a computer vision system in action as a vision
    sensor [[57](#bib.bib57)]. Therefore, artificial intelligence, as well as computer
    vision, share other fields, for instance, pattern recognition and learning methods.
    The consequence is that in certain cases, computer vision is viewed as a component
    of artificial intelligence. Another application of computer vision is solid state
    physics since a large portion of computer vision systems are reliant on imagery
    sensors that provide detection of electromagnetic radiation which normally takes
    a form that is either visible or infra-red lighting [[81](#bib.bib81)] [[86](#bib.bib86)].
    These sensors operate according to quantum physics designs with the procedure
    by which light interacts with the surface better explained by optics behaviors.
    Such intricate inner working demonstrates how even complex image sensors even
    need quantum mechanics to avail a full understanding of the process of image formation.
    Furthermore, another application of computer vision is the multiple measurement
    challenges in physics which can be tackled by utilizing computer vision for instance
    fluids motion [[39](#bib.bib39)].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的跨学科领域关注于如何开发计算机以提高对视频数字图像的感知。计算机视觉任务包括获取、处理、分析数字图像以及从实际世界中提取高维信息，以便生成数字或符号数据，例如决策的格式[[85](#bib.bib85)]。涉及自主规划或深思熟虑的机器人系统导航的人工智能领域，需要对这些设置有透彻的理解，因为计算机视觉系统作为视觉传感器可以提供环境信息[[57](#bib.bib57)]。因此，人工智能和计算机视觉共享其他领域，例如模式识别和学习方法。因此，在某些情况下，计算机视觉被视为人工智能的一部分。计算机视觉的另一个应用是固态物理，因为许多计算机视觉系统依赖于图像传感器，这些传感器检测电磁辐射，通常以可见光或红外光的形式出现[[81](#bib.bib81)]
    [[86](#bib.bib86)]。这些传感器根据量子物理设计操作，光与表面的相互作用过程更好地通过光学行为解释。这些复杂的内部工作机制表明，即使是复杂的图像传感器也需要量子力学来全面理解图像形成过程。此外，计算机视觉的另一个应用是物理中的多重测量挑战，例如流体运动[[39](#bib.bib39)]。
- en: 8 Benefits and Drawbacks of Deep Learning to be Applied in Mobile Robots
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 深度学习在移动机器人中的优缺点
- en: 8.1 Benefits of Deep Learning in the Context of Mobile Robots
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 深度学习在移动机器人中的好处
- en: The gains of deep learning as a component of the wider family of machine learning
    techniques founded on representations of learning data in opposition to assignment-particular
    algorithms through supervised, unsupervised and semi-supervised learning allows
    for structured on the interpretation of information processing and patterns of
    communications which can be viewed as trials at defining a relation between multiple
    stimuli and related neuronal responses [[87](#bib.bib87)]. Deep learning architectures
    for instance deep neural, deep beliefs as well as recurrent neural networks have
    been utilized in arenas inclusive of computer vision, natural language processing,
    social network filtering, speech recognition, bioinformatics and audio recognition.
    In these mentioned fields, deep learning architecture has produced outcomes in
    comparison to an in certain case more advanced to human expertise. Furthermore,
    deep learning algorithms utilize a cascade of many nonlinear processing unit layers
    for extraction of features and transformation with particular layers applying
    the output from the past layer as input [[88](#bib.bib88)].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习作为更广泛的机器学习技术家族的一部分，其优势在于基于对学习数据的表征，而不是通过监督、无监督和半监督学习的任务特定算法，这允许对信息处理和通信模式的结构化解读，可以视为定义多个刺激与相关神经反应之间关系的尝试[[87](#bib.bib87)]。例如，深度神经网络、深度信念网络以及递归神经网络等深度学习架构已经在计算机视觉、自然语言处理、社交网络过滤、语音识别、生物信息学和音频识别等领域得到了应用。在这些领域中，深度学习架构的成果与某些情况下甚至超越了人类专业知识。此外，深度学习算法利用多层非线性处理单元进行特征提取和变换，特定层将前一层的输出作为输入[[88](#bib.bib88)]。
- en: Deep reinforcement learning proposes a simplified conceptual light framework
    that utilizes asynchronous gradient descent to cater for deep neural network controller
    optimization. The presented asynchronous variants in standard reinforcement learning
    algorithms reveal that parallel actor learner holds a stabilizing influence on
    training which allows for the successful training of the neural network controller
    [[19](#bib.bib19)]. It is on this premise that asynchronous variants are presented
    as the most appealing Deep reinforcement learning approach.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习提出了一个简化的概念框架，利用异步梯度下降来优化深度神经网络控制器。标准强化学习算法中的异步变体显示，平行的演员学习者对训练具有稳定影响，从而允许成功训练神经网络控制器[[19](#bib.bib19)]。在这一前提下，异步变体被提出作为最具吸引力的深度强化学习方法。
- en: 8.2 Drawbacks of Deep Learning in the Context of Mobile Robots
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 移动机器人中深度学习的缺点
- en: The drawbacks of deep learning in applied robotics is that the storage of the
    agent data using replay memory does not allow for re-batching or sampling at randomly
    from varied time-stages. As such, memory aggregation in this approach lowers non-stationary
    and eroded updates while in simultaneously limiting the techniques to off-policy
    reinforcement learning algorithms [[23](#bib.bib23)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在应用机器人学中的缺点是，使用回放记忆存储代理数据不允许从不同时间阶段随机重批次或采样。因此，这种方法中的记忆聚合降低了非平稳和侵蚀更新，同时限制了技术仅用于脱离策略强化学习算法[[23](#bib.bib23)]。
- en: 9 Conclusion
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: Deep learning is set to transform the arena of artificial intelligence as well
    as represent a measure in the direction of developing autonomous systems with
    an increased scope of perceiving the visual world. Presently, deep learning is
    allowing scaling of challenges that were traditionally intractable for instance
    learning to directly play video games for pixels. Furthermore, deep learning algorithms
    are also utilized in robotics to foster the capability of control functionality
    for robots indirectly learning from cameral inputs in the actual world. It is
    on this premise that the survey above illustrates the major advances and approaches
    of reinforcement learning in regard to the main streams of value and policy-driven
    methods as well as associated coverage of central algorithms in deep learning
    for instance deep network, asynchronous advantage actor-critic as well as trust
    region policy optimization. Furthermore, the research survey has highlighted the
    gains of deep neural networking with emphasis on visual perception through deep
    learning. One of the core objectives of the discipline of artificial intelligence
    it the production of completely autonomous agents that are able to interact with
    their settings to learn optimal behavior and demonstrate improvements with time
    by trial and error regiments. It is therefore on this premise that creating artificial
    intelligence systems that are responsive and with the capability of learning has
    long been an elusive challenge. However, hope is found in the principled mathematical
    framework of deep learning with utilizes experience driven autonomous learning
    to apply a functional approximation to represent learning attributes of deep neural
    networks to overcome these challenges.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习有望改变人工智能领域，并代表了朝着开发具有更广泛视觉感知能力的自主系统迈进的一步。目前，深度学习使得处理传统上难以解决的挑战成为可能，例如通过像素直接玩视频游戏。此外，深度学习算法还被应用于机器人技术中，以增强机器人从实际世界中的摄像头输入间接学习控制功能的能力。正是在这种基础上，上述调查展示了强化学习在价值驱动和政策驱动方法主要流派中的主要进展和方法，并涵盖了深度学习中的核心算法，例如深度网络、异步优势演员-评论员以及信任区域策略优化。此外，研究调查还突出了深度神经网络在通过深度学习进行视觉感知方面的收益。人工智能学科的核心目标之一是生产完全自主的智能体，使其能够与环境互动，以学习最佳行为，并通过试错法随着时间的推移展示改进。因此，创建响应性强且具有学习能力的人工智能系统长期以来一直是一个难以攻克的挑战。然而，在深度学习的原则性数学框架中找到希望，该框架利用经验驱动的自主学习来应用功能逼近，从而表示深度神经网络的学习属性，以克服这些挑战。
- en: References
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, p. 436, 2015.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. LeCun, Y. Bengio, 和 G. Hinton, “深度学习，” *自然*，第 521 卷，第 7553 期，第 436 页，2015
    年。'
- en: '[2] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver,
    and D. Wierstra, “Continuous control with deep reinforcement learning,” *arXiv
    preprint arXiv:1509.02971*, 2015.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D.
    Silver, 和 D. Wierstra, “使用深度强化学习进行连续控制，” *arXiv 预印本 arXiv:1509.02971*，2015 年。'
- en: '[3] J. S. Esteves, A. Carvalho, and C. Couto, “Generalized geometric triangulation
    algorithm for mobile robot absolute self-localization,” in *Industrial Electronics,
    2003\. ISIE’03\. 2003 IEEE International Symposium on*, vol. 1.   IEEE, 2003,
    pp. 346–351.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] J. S. Esteves, A. Carvalho, 和 C. Couto, “用于移动机器人绝对自我定位的广义几何三角测量算法，” 发表在
    *工业电子学，2003 年 ISIE’03 第 2003 年 IEEE 国际研讨会*，第 1 卷。IEEE，2003 年，第 346–351 页。'
- en: '[4] A. Vedaldi and K. Lenc, “Matconvnet: Convolutional neural networks for
    matlab,” in *Proceedings of the 23rd ACM international conference on Multimedia*.   ACM,
    2015, pp. 689–692.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] A. Vedaldi 和 K. Lenc, “Matconvnet：用于 Matlab 的卷积神经网络，” 发表在 *第 23 届 ACM 国际多媒体会议论文集*。ACM，2015
    年，第 689–692 页。'
- en: '[5] A. Eitel, J. T. Springenberg, L. Spinello, M. Riedmiller, and W. Burgard,
    “Multimodal deep learning for robust rgb-d object recognition,” in *Intelligent
    Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on*.   IEEE,
    2015, pp. 681–687.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. Eitel, J. T. Springenberg, L. Spinello, M. Riedmiller, 和 W. Burgard,
    “多模态深度学习用于鲁棒的 RGB-D 物体识别，” 发表在 *智能机器人与系统（IROS），2015 年 IEEE/RSJ 国际会议上*。IEEE，2015
    年，第 681–687 页。'
- en: '[6] Y. Yang and Y. Li, “Robot learning manipulation action plans by” watching”
    unconstrained videos from the world wide web.” 2015.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Y. Yang 和 Y. Li, “机器人通过观看来自互联网的无约束视频来学习操作计划。” 2015 年。'
- en: '[7] D. C. Cireşan, U. Meier, L. M. Gambardella, and J. Schmidhuber, “Deep,
    big, simple neural nets for handwritten digit recognition,” *Neural computation*,
    vol. 22, no. 12, pp. 3207–3220, 2010.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] D. C. Cireşan, U. Meier, L. M. Gambardella, 和 J. Schmidhuber， “用于手写数字识别的深度、大规模、简单神经网络，”
    *神经计算*，第 22 卷，第 12 期，第 3207–3220 页，2010 年。'
- en: '[8] J. Lu, V. Behbood, P. Hao, H. Zuo, S. Xue, and G. Zhang, “Transfer learning
    using computational intelligence: a survey,” *Knowledge-Based Systems*, vol. 80,
    pp. 14–23, 2015.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Lu, V. Behbood, P. Hao, H. Zuo, S. Xue, 和 G. Zhang， “使用计算智能的迁移学习：综述，”
    *基于知识的系统*，第 80 卷，第 14–23 页，2015 年。'
- en: '[9] M. Turan, J. Shabbir, H. Araujo, E. Konukoglu, and M. Sitti, “A deep learning
    based fusion of rgb camera information and magnetic localization information for
    endoscopic capsule robots,” *International journal of intelligent robotics and
    applications*, vol. 1, no. 4, pp. 442–450, 2017.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] M. Turan, J. Shabbir, H. Araujo, E. Konukoglu, 和 M. Sitti， “基于深度学习的 RGB
    相机信息与磁性定位信息融合，用于内窥镜胶囊机器人，” *国际智能机器人与应用杂志*，第 1 卷，第 4 期，第 442–450 页，2017 年。'
- en: '[10] M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, perspectives,
    and prospects,” *Science*, vol. 349, no. 6245, pp. 255–260, 2015.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. I. Jordan 和 T. M. Mitchell， “机器学习：趋势、观点与前景，” *科学*，第 349 卷，第 6245 期，第
    255–260 页，2015 年。'
- en: '[11] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, and M. Sitti, “Deep
    endovo: A recurrent convolutional neural network (rcnn) based visual odometry
    approach for endoscopic capsule robots,” *Neurocomputing*, vol. 275, pp. 1861–1870,
    2018.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, 和 M. Sitti， “深度 Endovo：一种基于递归卷积神经网络（RCNN）的视觉里程计方法，用于内窥镜胶囊机器人，”
    *神经计算*，第 275 卷，第 1861–1870 页，2018 年。'
- en: '[12] N. Sünderhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell, B. Upcroft,
    and M. Milford, “Place recognition with convnet landmarks: Viewpoint-robust, condition-robust,
    training-free,” *Proceedings of Robotics: Science and Systems XII*, 2015.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] N. Sünderhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell, B. Upcroft,
    和 M. Milford， “使用卷积网络地标进行地点识别：视角鲁棒、条件鲁棒、无需训练，” *机器人学：科学与系统 XII 会议录*，2015 年。'
- en: '[13] M. Turan, Y. Y. Pilavci, R. Jamiruddin, H. Araujo, E. Konukoglu, and M. Sitti,
    “A fully dense and globally consistent 3d map reconstruction approach for gi tract
    to enhance therapeutic relevance of the endoscopic capsule robot,” *arXiv preprint
    arXiv:1705.06524*, 2017.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] M. Turan, Y. Y. Pilavci, R. Jamiruddin, H. Araujo, E. Konukoglu, 和 M.
    Sitti， “一种完全密集且全球一致的 3D 地图重建方法，用于胃肠道，以增强内窥镜胶囊机器人治疗的相关性，” *arXiv 预印本 arXiv:1705.06524*，2017
    年。'
- en: '[14] M. Turan, Y. Y. Pilavci, I. Ganiyusufoglu, H. Araujo, E. Konukoglu, and
    M. Sitti, “Sparse-then-dense alignment-based 3d map reconstruction method for
    endoscopic capsule robots,” *Machine Vision and Applications*, vol. 29, no. 2,
    pp. 345–359, 2018.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. Turan, Y. Y. Pilavci, I. Ganiyusufoglu, H. Araujo, E. Konukoglu, 和
    M. Sitti， “基于稀疏到密集对齐的 3D 地图重建方法，用于内窥镜胶囊机器人，” *机器视觉与应用*，第 29 卷，第 2 期，第 345–359
    页，2018 年。'
- en: '[15] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, “Human-level concept
    learning through probabilistic program induction,” *Science*, vol. 350, no. 6266,
    pp. 1332–1338, 2015.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] B. M. Lake, R. Salakhutdinov, 和 J. B. Tenenbaum， “通过概率程序归纳进行人类水平的概念学习，”
    *科学*，第 350 卷，第 6266 期，第 1332–1338 页，2015 年。'
- en: '[16] S. Marsland, “Machine learning, an algorithmic perspective, chapman &
    hall/crc machine learning & pattern recognition,” *CRC, Boca Raton, Fla*, 2009.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. Marsland， “机器学习，算法视角，Chapman & Hall/CRC 机器学习与模式识别，” *CRC，佛罗里达州博卡拉顿*，2009
    年。'
- en: '[17] I. Lenz, H. Lee, and A. Saxena, “Deep learning for detecting robotic grasps,”
    *The International Journal of Robotics Research*, vol. 34, no. 4-5, pp. 705–724,
    2015.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] I. Lenz, H. Lee, 和 A. Saxena， “用于检测机器人抓取的深度学习，” *国际机器人研究杂志*，第 34 卷，第 4-5
    期，第 705–724 页，2015 年。'
- en: '[18] Z. Ghahramani, “Probabilistic machine learning and artificial intelligence,”
    *Nature*, vol. 521, no. 7553, p. 452, 2015.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Z. Ghahramani， “概率机器学习与人工智能，” *自然*，第 521 卷，第 7553 期，第 452 页，2015 年。'
- en: '[19] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, “Benchmarking
    deep reinforcement learning for continuous control,” in *International Conference
    on Machine Learning*, 2016, pp. 1329–1338.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Y. Duan, X. Chen, R. Houthooft, J. Schulman, 和 P. Abbeel， “用于连续控制的深度强化学习基准测试，”
    在 *国际机器学习大会* 中，2016 年，第 1329–1338 页。'
- en: '[20] S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen, “Learning hand-eye
    coordination for robotic grasping with large-scale data collection,” in *International
    Symposium on Experimental Robotics*.   Springer, 2016, pp. 173–184.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] S. Levine, P. Pastor, A. Krizhevsky, 和 D. Quillen， “利用大规模数据收集学习手眼协调以进行机器人抓取，”
    在 *国际实验机器人学研讨会* 中。 Springer，2016 年，第 173–184 页。'
- en: '[21] Y. Yang, C. Fermuller, Y. Li, and Y. Aloimonos, “Grasp type revisited:
    A modern perspective on a classical feature for vision,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, 2015, pp. 400–408.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. Yang, C. Fermuller, Y. Li 和 Y. Aloimonos，“抓取类型重访：对视觉经典特征的现代视角”，载于 *IEEE计算机视觉与模式识别会议论文集*，2015年，第400–408页。'
- en: '[22] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution using
    deep convolutional networks,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 38, no. 2, pp. 295–307, 2016.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] C. Dong, C. C. Loy, K. He 和 X. Tang，“使用深度卷积网络的图像超分辨率”，*IEEE模式分析与机器智能学报*，第38卷，第2期，第295–307页，2016年。'
- en: '[23] A. Gongal, S. Amatya, M. Karkee, Q. Zhang, and K. Lewis, “Sensors and
    systems for fruit detection and localization: A review,” *Computers and Electronics
    in Agriculture*, vol. 116, pp. 8–19, 2015.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. Gongal, S. Amatya, M. Karkee, Q. Zhang 和 K. Lewis，“用于果实检测和定位的传感器与系统：综述”，*农业中的计算机与电子*，第116卷，第8–19页，2015年。'
- en: '[24] A. M. Nguyen, J. Yosinski, and J. Clune, “Innovation engines: Automated
    creativity and improved stochastic optimization via deep learning,” in *Proceedings
    of the 2015 Annual Conference on Genetic and Evolutionary Computation*.   ACM,
    2015, pp. 959–966.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. M. Nguyen, J. Yosinski 和 J. Clune，“创新引擎：通过深度学习实现自动化创造力和改进的随机优化”，载于
    *2015年年度遗传与进化计算会议论文集*。 ACM，2015年，第959–966页。'
- en: '[25] Y. Du, W. Wang, and L. Wang, “Hierarchical recurrent neural network for
    skeleton based action recognition,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2015, pp. 1110–1118.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Du, W. Wang 和 L. Wang，“基于骨架的层次递归神经网络用于动作识别”，载于 *IEEE计算机视觉与模式识别会议论文集*，2015年，第1110–1118页。'
- en: '[26] Y. Tang, “Deep learning using linear support vector machines,” *arXiv
    preprint arXiv:1306.0239*, 2013.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Y. Tang，“使用线性支持向量机的深度学习”，*arXiv预印本arXiv:1306.0239*，2013年。'
- en: '[27] N. Sünderhauf, S. Shirazi, F. Dayoub, B. Upcroft, and M. Milford, “On
    the performance of convnet features for place recognition,” in *Intelligent Robots
    and Systems (IROS), 2015 IEEE/RSJ International Conference on*.   IEEE, 2015,
    pp. 4297–4304.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] N. Sünderhauf, S. Shirazi, F. Dayoub, B. Upcroft 和 M. Milford，“关于用于地点识别的卷积网络特征的性能”，载于
    *智能机器人与系统（IROS），2015年IEEE/RSJ国际会议*。 IEEE，2015年，第4297–4304页。'
- en: '[28] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, “Deepdriving: Learning affordance
    for direct perception in autonomous driving,” in *Computer Vision (ICCV), 2015
    IEEE International Conference on*.   IEEE, 2015, pp. 2722–2730.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] C. Chen, A. Seff, A. Kornhauser 和 J. Xiao，“Deepdriving：为自动驾驶中的直接感知学习可用性”，载于
    *计算机视觉（ICCV），2015年IEEE国际会议*。 IEEE，2015年，第2722–2730页。'
- en: '[29] J. Schmidhuber, “Deep learning in neural networks: An overview,” *Neural
    networks*, vol. 61, pp. 85–117, 2015.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Schmidhuber，“神经网络中的深度学习：概述”，*神经网络*，第61卷，第85–117页，2015年。'
- en: '[30] H. Brighton and H. Selina, *Introducing Artificial Intelligence: A Graphic
    Guide*, ser. Introducing…   Icon Books Limited, 2015\. [Online]. Available: https://books.google.com.pk/books?id=4GxGCgAAQBAJ'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] H. Brighton 和 H. Selina，*《人工智能简介：图解指南》*，系列：Introducing… Icon Books Limited，2015年。[在线].
    可用： https://books.google.com.pk/books?id=4GxGCgAAQBAJ'
- en: '[31] J. Bai, Y. Wu, J. Zhang, and F. Chen, “Subset based deep learning for
    rgb-d object recognition,” *Neurocomputing*, vol. 165, pp. 280–292, 2015.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Bai, Y. Wu, J. Zhang 和 F. Chen，“基于子集的深度学习用于RGB-D对象识别”，*神经计算*，第165卷，第280–292页，2015年。'
- en: '[32] V. Veeriah, N. Zhuang, and G.-J. Qi, “Differential recurrent neural networks
    for action recognition,” in *Computer Vision (ICCV), 2015 IEEE International Conference
    on*.   IEEE, 2015, pp. 4041–4049.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] V. Veeriah, N. Zhuang 和 G.-J. Qi，“用于动作识别的差分递归神经网络”，载于 *计算机视觉（ICCV），2015年IEEE国际会议*。
    IEEE，2015年，第4041–4049页。'
- en: '[33] R. Xu, C. Xiong, W. Chen, and J. J. Corso, “Jointly modeling deep video
    and compositional text to bridge vision and language in a unified framework.”
    2015.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] R. Xu, C. Xiong, W. Chen 和 J. J. Corso，“联合建模深度视频与组成文本以在统一框架中弥合视觉与语言”，2015年。'
- en: '[34] K. Narasimhan, T. Kulkarni, and R. Barzilay, “Language understanding for
    text-based games using deep reinforcement learning,” *arXiv preprint arXiv:1506.08941*,
    2015.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] K. Narasimhan, T. Kulkarni 和 R. Barzilay，“利用深度强化学习进行文本游戏的语言理解”，*arXiv预印本arXiv:1506.08941*，2015年。'
- en: '[35] J. Wu, I. Yildirim, J. J. Lim, B. Freeman, and J. Tenenbaum, “Galileo:
    Perceiving physical object properties by integrating a physics engine with deep
    learning,” in *Advances in neural information processing systems*, 2015, pp. 127–135.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J. Wu, I. Yildirim, J. J. Lim, B. Freeman 和 J. Tenenbaum，“Galileo：通过将物理引擎与深度学习结合来感知物理对象属性”，载于
    *神经信息处理系统进展*，2015年，第127–135页。'
- en: '[36] M. Turan, E. P. Ornek, N. Ibrahimli, C. Giracoglu, Y. Almalioglu, M. F.
    Yanik, and M. Sitti, “Unsupervised odometry and depth learning for endoscopic
    capsule robots,” *arXiv preprint arXiv:1803.01047*, 2018.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] M. Turan, E. P. Ornek, N. Ibrahimli, C. Giracoglu, Y. Almalioglu, M. F.
    Yanik 和 M. Sitti，“用于内镜胶囊机器人的无监督里程计和深度学习，” *arXiv 预印本 arXiv:1803.01047*，2018年。'
- en: '[37] M. Turan, Y. Almalioglu, H. Araujo, T. Cemgil, and M. Sitti, “Endosensorfusion:
    Particle filtering-based multi-sensory data fusion with switching state-space
    model for endoscopic capsule robots using recurrent neural network kinematics,”
    *arXiv preprint arXiv:1709.03401*, 2017.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] M. Turan, Y. Almalioglu, H. Araujo, T. Cemgil 和 M. Sitti，“内镜传感器融合：基于粒子滤波的多传感器数据融合及切换状态空间模型，用于使用递归神经网络运动学的内镜胶囊机器人，”
    *arXiv 预印本 arXiv:1709.03401*，2017年。'
- en: '[38] R. Lun and W. Zhao, “A survey of applications and human motion recognition
    with microsoft kinect,” *International Journal of Pattern Recognition and Artificial
    Intelligence*, vol. 29, no. 05, p. 1555008, 2015.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] R. Lun 和 W. Zhao，“微软 Kinect 的应用和人体运动识别综述，” *国际模式识别与人工智能期刊*，第29卷，第05期，页码1555008，2015年。'
- en: '[39] J. Kuen, K. M. Lim, and C. P. Lee, “Self-taught learning of a deep invariant
    representation for visual tracking via temporal slowness principle,” *Pattern
    recognition*, vol. 48, no. 10, pp. 2964–2982, 2015.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. Kuen, K. M. Lim 和 C. P. Lee，“通过时间缓慢原则自学深度不变表征用于视觉跟踪，” *模式识别*，第48卷，第10期，页码2964–2982，2015年。'
- en: '[40] Y. Hou, H. Zhang, and S. Zhou, “Convolutional neural network-based image
    representation for visual loop closure detection,” in *Information and Automation,
    2015 IEEE International Conference on*.   IEEE, 2015, pp. 2238–2245.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Y. Hou, H. Zhang 和 S. Zhou，“基于卷积神经网络的图像表征用于视觉回环检测，”在 *信息与自动化，2015 IEEE
    国际会议* 中。IEEE，2015年，页码2238–2245。'
- en: '[41] Y. Qian, J. Dong, W. Wang, and T. Tan, “Deep learning for steganalysis
    via convolutional neural networks,” in *Media Watermarking, Security, and Forensics
    2015*, vol. 9409.   International Society for Optics and Photonics, 2015, p. 94090J.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Y. Qian, J. Dong, W. Wang 和 T. Tan，“通过卷积神经网络进行深度学习隐写分析，”在 *2015 媒体水印、安全与取证*
    中，第9409卷。国际光学与光子学学会，2015年，页码94090J。'
- en: '[42] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko, “Simultaneous deep transfer
    across domains and tasks,” in *Computer Vision (ICCV), 2015 IEEE International
    Conference on*.   IEEE, 2015, pp. 4068–4076.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] E. Tzeng, J. Hoffman, T. Darrell 和 K. Saenko，“跨领域和任务的深度转移学习，”在 *计算机视觉（ICCV），2015
    IEEE 国际会议* 中。IEEE，2015年，页码4068–4076。'
- en: '[43] L. Pinto, D. Gandhi, Y. Han, Y.-L. Park, and A. Gupta, “The curious robot:
    Learning visual representations via physical interactions,” in *European Conference
    on Computer Vision*.   Springer, 2016, pp. 3–18.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] L. Pinto, D. Gandhi, Y. Han, Y.-L. Park 和 A. Gupta，“好奇的机器人：通过物理交互学习视觉表征，”在
    *欧洲计算机视觉会议* 中。Springer，2016年，页码3–18。'
- en: '[44] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman, “Building
    machines that learn and think like people,” *Behavioral and Brain Sciences*, vol. 40,
    2017.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] B. M. Lake, T. D. Ullman, J. B. Tenenbaum 和 S. J. Gershman，“构建像人类一样学习和思考的机器，”
    *行为与脑科学*，第40卷，2017年。'
- en: '[45] G. Chen, D. Clarke, M. Giuliani, A. Gaschler, and A. Knoll, “Combining
    unsupervised learning and discrimination for 3d action recognition,” *Signal Processing*,
    vol. 110, pp. 67–81, 2015.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] G. Chen, D. Clarke, M. Giuliani, A. Gaschler 和 A. Knoll，“结合无监督学习和区分以进行3D动作识别，”
    *信号处理*，第110卷，页码67–81，2015年。'
- en: '[46] J. Wulff and M. J. Black, “Efficient sparse-to-dense optical flow estimation
    using a learned basis and layers,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2015, pp. 120–130.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Wulff 和 M. J. Black，“使用学习基和层进行高效稀疏到密集光流估计，”在 *IEEE 计算机视觉与模式识别会议论文集*
    中，2015年，页码120–130。'
- en: '[47] J.-R. Ruiz-Sarmiento, C. Galindo, and J. Gonzalez-Jimenez, “Scene object
    recognition for mobile robots through semantic knowledge and probabilistic graphical
    models,” *Expert Systems with Applications*, vol. 42, no. 22, pp. 8805–8816, 2015.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J.-R. Ruiz-Sarmiento, C. Galindo 和 J. Gonzalez-Jimenez，“通过语义知识和概率图模型进行移动机器人场景物体识别，”
    *专家系统与应用*，第42卷，第22期，页码8805–8816，2015年。'
- en: '[48] N. Das, E. Ohn-Bar, and M. M. Trivedi, “On performance evaluation of driver
    hand detection algorithms: Challenges, dataset, and metrics,” in *Intelligent
    Transportation Systems (ITSC), 2015 IEEE 18th International Conference on*.   IEEE,
    2015, pp. 2953–2958.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] N. Das, E. Ohn-Bar 和 M. M. Trivedi，“驾驶员手部检测算法性能评估：挑战、数据集和指标，”在 *智能交通系统（ITSC），2015
    IEEE 第18届国际会议* 中。IEEE，2015年，页码2953–2958。'
- en: '[49] R. Salakhutdinov, “Learning deep generative models,” *Annual Review of
    Statistics and Its Application*, vol. 2, pp. 361–385, 2015.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] R. Salakhutdinov，“学习深度生成模型，” *Annual Review of Statistics and Its Application*，第2卷，页码361–385，2015年。'
- en: '[50] J. Schmidhuber, “On learning to think: Algorithmic information theory
    for novel combinations of reinforcement learning controllers and recurrent neural
    world models,” *arXiv preprint arXiv:1511.09249*, 2015.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. Schmidhuber，“关于学习思考：针对新型强化学习控制器和递归神经世界模型的算法信息理论，” *arXiv preprint arXiv:1511.09249*，2015年。'
- en: '[51] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, and M. Sitti, “A non-rigid
    map fusion-based direct slam method for endoscopic capsule robots,” *International
    journal of intelligent robotics and applications*, vol. 1, no. 4, pp. 399–409,
    2017.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu 和 M. Sitti，“一种基于非刚性地图融合的内窥镜胶囊机器人直接SLAM方法，”
    *International journal of intelligent robotics and applications*，第1卷，第4期，页码399–409，2017年。'
- en: '[52] T. Chen, Z. Chen, Q. Shi, and X. Huang, “Road marking detection and classification
    using machine learning algorithms,” in *Intelligent Vehicles Symposium (IV), 2015
    IEEE*.   IEEE, 2015, pp. 617–621.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] T. Chen, Z. Chen, Q. Shi 和 X. Huang，“使用机器学习算法进行道路标线检测与分类，” 载于*Intelligent
    Vehicles Symposium (IV), 2015 IEEE*。IEEE，2015，页码617–621。'
- en: '[53] M. Vrigkas, C. Nikou, and I. A. Kakadiaris, “A review of human activity
    recognition methods,” *Frontiers in Robotics and AI*, vol. 2, p. 28, 2015.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. Vrigkas, C. Nikou 和 I. A. Kakadiaris，“人类活动识别方法的综述，” *Frontiers in Robotics
    and AI*，第2卷，页码28，2015年。'
- en: '[54] O. K. Oyedotun and A. Khashman, “Deep learning in vision-based static
    hand gesture recognition,” *Neural Computing and Applications*, vol. 28, no. 12,
    pp. 3941–3951, 2017.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] O. K. Oyedotun 和 A. Khashman，“基于视觉的静态手势识别中的深度学习，” *Neural Computing and
    Applications*，第28卷，第12期，页码3941–3951，2017年。'
- en: '[55] R. K. Moore, “From talking and listening robots to intelligent communicative
    machines.”'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] R. K. Moore，“从谈话和倾听机器人到智能交互机器。”'
- en: '[56] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm for
    deep belief nets,” *Neural computation*, vol. 18, no. 7, pp. 1527–1554, 2006.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] G. E. Hinton, S. Osindero 和 Y.-W. Teh，“一种快速学习深度信念网络的算法，” *Neural computation*，第18卷，第7期，页码1527–1554，2006年。'
- en: '[57] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi,
    “Target-driven visual navigation in indoor scenes using deep reinforcement learning,”
    in *Robotics and Automation (ICRA), 2017 IEEE International Conference on*.   IEEE,
    2017, pp. 3357–3364.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei 和 A. Farhadi，“使用深度强化学习进行室内场景的目标驱动视觉导航，”
    载于*Robotics and Automation (ICRA), 2017 IEEE International Conference on*。IEEE，2017，页码3357–3364。'
- en: '[58] F. Cruz, J. Twiefel, S. Magg, C. Weber, and S. Wermter, “Interactive reinforcement
    learning through speech guidance in a domestic scenario,” in *Neural Networks
    (IJCNN), 2015 International Joint Conference on*.   IEEE, 2015, pp. 1–8.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] F. Cruz, J. Twiefel, S. Magg, C. Weber, 和 S. Wermter，“在家庭场景中的语音指导交互式强化学习，”
    载于*Neural Networks (IJCNN), 2015 International Joint Conference on*。IEEE，2015，页码1–8。'
- en: '[59] A. Vinciarelli, A. Esposito, E. André, F. Bonin, M. Chetouani, J. F. Cohn,
    M. Cristani, F. Fuhrmann, E. Gilmartin, Z. Hammal *et al.*, “Open challenges in
    modelling, analysis and synthesis of human behaviour in human–human and human–machine
    interactions,” *Cognitive Computation*, vol. 7, no. 4, pp. 397–413, 2015.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] A. Vinciarelli, A. Esposito, E. André, F. Bonin, M. Chetouani, J. F. Cohn,
    M. Cristani, F. Fuhrmann, E. Gilmartin, Z. Hammal *等*，“在人类-人类和人类-机器交互中的人类行为建模、分析和合成的开放挑战，”
    *Cognitive Computation*，第7卷，第4期，页码397–413，2015年。'
- en: '[60] J. Doshi, Z. Kira, and A. Wagner, “From deep learning to episodic memories:
    Creating categories of visual experiences,” in *Proceedings of the third annual
    conference on advances in cognitive systems ACS*, 2015, p. 15.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J. Doshi, Z. Kira 和 A. Wagner，“从深度学习到情节记忆：创建视觉体验的类别，” 载于*Proceedings of
    the third annual conference on advances in cognitive systems ACS*，2015年，页码15。'
- en: '[61] X. Wang, D. Fouhey, and A. Gupta, “Designing deep networks for surface
    normal estimation,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2015, pp. 539–547.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] X. Wang, D. Fouhey 和 A. Gupta，“为表面法线估计设计深度网络，” 载于*Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*，2015年，页码539–547。'
- en: '[62] H. Cuayáhuitl, S. Keizer, and O. Lemon, “Strategic dialogue management
    via deep reinforcement learning,” *arXiv preprint arXiv:1511.08099*, 2015.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] H. Cuayáhuitl, S. Keizer 和 O. Lemon，“通过深度强化学习进行战略对话管理，” *arXiv preprint
    arXiv:1511.08099*，2015年。'
- en: '[63] E. Ohn-Bar and M. M. Trivedi, “Looking at humans in the age of self-driving
    and highly automated vehicles,” *IEEE Transactions on Intelligent Vehicles*, vol. 1,
    no. 1, pp. 90–104, 2016.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] E. Ohn-Bar 和 M. M. Trivedi，“在自动驾驶和高度自动化车辆时代观察人类，” *IEEE Transactions on
    Intelligent Vehicles*，第1卷，第1期，页码90–104，2016年。'
- en: '[64] J. Wei, H. Liu, G. Yan, and F. Sun, “Robotic grasping recognition using
    multi-modal deep extreme learning machine,” *Multidimensional Systems and Signal
    Processing*, vol. 28, no. 3, pp. 817–833, 2017.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] J. Wei, H. Liu, G. Yan, 和 F. Sun, “使用多模态深度极端学习机的机器人抓取识别，” *多维系统与信号处理*，第
    28 卷，第 3 期，页码 817–833，2017年。'
- en: '[65] M. Mathieu, C. Couprie, and Y. LeCun, “Deep multi-scale video prediction
    beyond mean square error,” *arXiv preprint arXiv:1511.05440*, 2015.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] M. Mathieu, C. Couprie, 和 Y. LeCun, “超越均方误差的深度多尺度视频预测，” *arXiv 预印本 arXiv:1511.05440*，2015年。'
- en: '[66] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016, pp. 2818–2826.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, 和 Z. Wojna, “重新思考计算机视觉中的
    inception 架构，” 见于 *IEEE 计算机视觉与模式识别会议论文集*，2016年，页码 2818–2826。'
- en: '[67] M. Turan, Y. Almalioglu, E. Konukoglu, and M. Sitti, “A deep learning
    based 6 degree-of-freedom localization method for endoscopic capsule robots,”
    *arXiv preprint arXiv:1705.05435*, 2017.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] M. Turan, Y. Almalioglu, E. Konukoglu, 和 M. Sitti, “一种基于深度学习的 6 自由度定位方法用于内窥镜胶囊机器人，”
    *arXiv 预印本 arXiv:1705.05435*，2017年。'
- en: '[68] J. Tang, C. Deng, and G.-B. Huang, “Extreme learning machine for multilayer
    perceptron,” *IEEE transactions on neural networks and learning systems*, vol. 27,
    no. 4, pp. 809–821, 2016.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] J. Tang, C. Deng, 和 G.-B. Huang, “多层感知器的极端学习机，” *IEEE 神经网络与学习系统交易*，第 27
    卷，第 4 期，页码 809–821，2016年。'
- en: '[69] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, and M. Sitti, “A non-rigid
    map fusion-based rgb-depth slam method for endoscopic capsule robots,” *arXiv
    preprint arXiv:1705.05444*, 2017.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, 和 M. Sitti, “基于非刚性地图融合的
    RGB-深度 SLAM 方法用于内窥镜胶囊机器人，” *arXiv 预印本 arXiv:1705.05444*，2017年。'
- en: '[70] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *arXiv preprint arXiv:1606.00915*, 2016.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, 和 A. L. Yuille, “Deeplab:
    使用深度卷积网络、空洞卷积和全连接 CRFs 的语义图像分割，” *arXiv 预印本 arXiv:1606.00915*，2016年。'
- en: '[71] J.-C. Chen, V. M. Patel, and R. Chellappa, “Unconstrained face verification
    using deep cnn features,” in *Applications of Computer Vision (WACV), 2016 IEEE
    Winter Conference on*.   IEEE, 2016, pp. 1–9.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] J.-C. Chen, V. M. Patel, 和 R. Chellappa, “使用深度 CNN 特征进行无约束的人脸验证，” 见于 *计算机视觉应用（WACV），2016
    IEEE 冬季会议*。IEEE，2016年，页码 1–9。'
- en: '[72] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami,
    “The limitations of deep learning in adversarial settings,” in *Security and Privacy
    (EuroS&P), 2016 IEEE European Symposium on*.   IEEE, 2016, pp. 372–387.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, 和 A. Swami,
    “对抗设置中深度学习的局限性，” 见于 *安全与隐私（EuroS&P），2016 IEEE 欧洲研讨会*。IEEE，2016年，页码 372–387。'
- en: '[73] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of
    deep visuomotor policies,” *The Journal of Machine Learning Research*, vol. 17,
    no. 1, pp. 1334–1373, 2016.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] S. Levine, C. Finn, T. Darrell, 和 P. Abbeel, “端到端深度视觉运动策略的训练，” *机器学习研究杂志*，第
    17 卷，第 1 期，页码 1334–1373，2016年。'
- en: '[74] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, “Deep networks
    with stochastic depth,” in *European Conference on Computer Vision*.   Springer,
    2016, pp. 646–661.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] G. Huang, Y. Sun, Z. Liu, D. Sedra, 和 K. Q. Weinberger, “具有随机深度的深度网络，”
    见于 *欧洲计算机视觉会议*。Springer，2016年，页码 646–661。'
- en: '[75] S. Niekum, S. Osentoski, G. Konidaris, S. Chitta, B. Marthi, and A. G.
    Barto, “Learning grounded finite-state representations from unstructured demonstrations,”
    *The International Journal of Robotics Research*, vol. 34, no. 2, pp. 131–157,
    2015.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] S. Niekum, S. Osentoski, G. Konidaris, S. Chitta, B. Marthi, 和 A. G. Barto,
    “从非结构化演示中学习基于状态的有限表示，” *国际机器人研究杂志*，第 34 卷，第 2 期，页码 131–157，2015年。'
- en: '[76] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine, “Learning modular
    neural network policies for multi-task and multi-robot transfer,” in *Robotics
    and Automation (ICRA), 2017 IEEE International Conference on*.   IEEE, 2017, pp.
    2169–2176.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] C. Devin, A. Gupta, T. Darrell, P. Abbeel, 和 S. Levine, “学习用于多任务和多机器人迁移的模块化神经网络策略，”
    见于 *机器人与自动化（ICRA），2017 IEEE 国际会议*。IEEE，2017年，页码 2169–2176。'
- en: '[77] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, and P. Abbeel, “Deep
    spatial autoencoders for visuomotor learning,” in *Robotics and Automation (ICRA),
    2016 IEEE International Conference on*.   IEEE, 2016, pp. 512–519.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, 和 P. Abbeel, “用于视觉运动学习的深度空间自编码器，”
    见于 *机器人与自动化（ICRA），2016 IEEE 国际会议*。IEEE，2016年，页码 512–519。'
- en: '[78] A. A. Rusu, M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell,
    “Sim-to-real robot learning from pixels with progressive nets,” *arXiv preprint
    arXiv:1610.04286*, 2016.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] A. A. Rusu, M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, 和 R. Hadsell,
    “从像素到现实的机器人学习与渐进网络，” *arXiv 预印本 arXiv:1610.04286*，2016年。'
- en: '[79] S. Mohamed and D. J. Rezende, “Variational information maximisation for
    intrinsically motivated reinforcement learning,” in *Advances in neural information
    processing systems*, 2015, pp. 2125–2133.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] S. Mohamed 和 D. J. Rezende, “用于内在动机强化学习的变分信息最大化，” 在 *神经信息处理系统进展*，2015年，第2125–2133页。'
- en: '[80] D. Maturana and S. Scherer, “Voxnet: A 3d convolutional neural network
    for real-time object recognition,” in *Intelligent Robots and Systems (IROS),
    2015 IEEE/RSJ International Conference on*.   IEEE, 2015, pp. 922–928.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] D. Maturana 和 S. Scherer, “Voxnet：用于实时物体识别的3D卷积神经网络，” 在 *智能机器人与系统（IROS），2015
    IEEE/RSJ国际会议*。IEEE, 2015年，第922–928页。'
- en: '[81] Y. Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, “Improving object
    detection with deep convolutional networks via bayesian optimization and structured
    prediction,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2015, pp. 249–258.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Y. Zhang, K. Sohn, R. Villegas, G. Pan, 和 H. Lee, “通过贝叶斯优化和结构化预测改进深度卷积网络的目标检测，”
    在 *IEEE计算机视觉与模式识别会议论文集*，2015年，第249–258页。'
- en: '[82] M. Turan, Y. Almalioglu, E. P. Ornek, H. Araujo, M. F. Yanik, and M. Sitti,
    “Magnetic-visual sensor fusion-based dense 3d reconstruction and localization
    for endoscopic capsule robots,” *arXiv preprint arXiv:1803.01048*, 2018.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] M. Turan, Y. Almalioglu, E. P. Ornek, H. Araujo, M. F. Yanik, 和 M. Sitti,
    “基于磁视觉传感器融合的内窥镜胶囊机器人稠密三维重建与定位，” *arXiv 预印本 arXiv:1803.01048*，2018年。'
- en: '[83] C. Finn and S. Levine, “Deep visual foresight for planning robot motion,”
    in *Robotics and Automation (ICRA), 2017 IEEE International Conference on*.   IEEE,
    2017, pp. 2786–2793.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] C. Finn 和 S. Levine, “用于规划机器人运动的深度视觉预测，” 在 *机器人与自动化（ICRA），2017 IEEE国际会议*。IEEE,
    2017年，第2786–2793页。'
- en: '[84] V. Campos, A. Salvador, X. Giro-i Nieto, and B. Jou, “Diving deep into
    sentiment: Understanding fine-tuned cnns for visual sentiment prediction,” in
    *Proceedings of the 1st International Workshop on Affect & Sentiment in Multimedia*.   ACM,
    2015, pp. 57–62.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] V. Campos, A. Salvador, X. Giro-i Nieto, 和 B. Jou, “深入探讨情感：理解用于视觉情感预测的精细调整卷积神经网络，”
    在 *第一届国际多媒体情感与情感研讨会论文集*。ACM, 2015年，第57–62页。'
- en: '[85] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement learning
    for robotic manipulation with asynchronous off-policy updates,” in *Robotics and
    Automation (ICRA), 2017 IEEE International Conference on*.   IEEE, 2017, pp. 3389–3396.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. Gu, E. Holly, T. Lillicrap, 和 S. Levine, “用于机器人操作的深度强化学习与异步脱离策略更新，”
    在 *机器人与自动化（ICRA），2017 IEEE国际会议*。IEEE, 2017年，第3389–3396页。'
- en: '[86] M. Turan, Y. Almalioglu, H. Gilbert, A. E. Sari, U. Soylu, and M. Sitti,
    “Endo-vmfusenet: deep visual-magnetic sensor fusion approach for uncalibrated,
    unsynchronized and asymmetric endoscopic capsule robot localization data,” *arXiv
    preprint arXiv:1709.06041*, 2017.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] M. Turan, Y. Almalioglu, H. Gilbert, A. E. Sari, U. Soylu, 和 M. Sitti,
    “Endo-vmfusenet：用于非校准、不同步和不对称内窥镜胶囊机器人定位数据的深度视觉-磁传感器融合方法，” *arXiv 预印本 arXiv:1709.06041*，2017年。'
- en: '[87] J. Sanchez-Riera, K.-L. Hua, Y.-S. Hsiao, T. Lim, S. C. Hidayati, and
    W.-H. Cheng, “A comparative study of data fusion for rgb-d based visual recognition,”
    *Pattern Recognition Letters*, vol. 73, pp. 1–6, 2016.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] J. Sanchez-Riera, K.-L. Hua, Y.-S. Hsiao, T. Lim, S. C. Hidayati, 和 W.-H.
    Cheng, “基于RGB-D的视觉识别数据融合的比较研究，” *模式识别快报*，第73卷，第1–6页，2016年。'
- en: '[88] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado,
    A. Davis, J. Dean, M. Devin *et al.*, “Tensorflow: Large-scale machine learning
    on heterogeneous distributed systems,” *arXiv preprint arXiv:1603.04467*, 2016.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado,
    A. Davis, J. Dean, M. Devin *等*，“Tensorflow：异构分布式系统上的大规模机器学习，” *arXiv 预印本 arXiv:1603.04467*，2016年。'
