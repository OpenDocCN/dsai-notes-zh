- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:34:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:34:06
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2403.01255] Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2403.01255] 使用先进深度学习方法的自动语音识别：一项调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01255](https://ar5iv.labs.arxiv.org/html/2403.01255)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01255](https://ar5iv.labs.arxiv.org/html/2403.01255)
- en: '[orcid=0000-0002-9532-2453] \cormark[1] \creditConceptualization; Methodology;
    Data Curation; Resources; Investigation; Visualization; Writing original draft;
    Writing, review, and editing'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[orcid=0000-0002-9532-2453] \cormark[1] \credit概念化；方法论；数据整理；资源；调查；可视化；撰写初稿；撰写、审阅和编辑'
- en: '[orcid=0000-0002-6353-0215] \creditConceptualization; Methodology; Resources;
    Investigation; Writing original draft; Writing, review, and editing'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[orcid=0000-0002-6353-0215] \credit概念化；方法论；资源；调查；撰写初稿；撰写、审阅和编辑'
- en: '[orcid=0000-0001-8904-5587] \creditConceptualization; Methodology; Resources;
    Investigation; Writing original draft; Writing, review, and editing'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[orcid=0000-0001-8904-5587] \credit概念化；方法论；资源；调查；撰写初稿；撰写、审阅和编辑'
- en: 'Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用先进深度学习方法的自动语音识别：一项调查
- en: Hamza Kheddar kheddar.hamza@univ-medea.dz    Mustapha Hemis hemismustapha@yahoo.fr
       Yassine Himeur yhimeur@ud.ac.ae LSEA Laboratory, Department of Electrical Engineering,
    University of Medea, 26000, Algeria LCPTS Laboratory, University of Sciences and
    Technology Houari Boumediene (USTHB), P.O. Box 32, El-Alia, Bab-Ezzouar, Algiers
    16111, Algeria. College of Engineering and Information Technology, University
    of Dubai, Dubai, UAE
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Hamza Kheddar kheddar.hamza@univ-medea.dz    Mustapha Hemis hemismustapha@yahoo.fr
       Yassine Himeur yhimeur@ud.ac.ae LSEA实验室，电气工程系，梅迪亚大学，26000，阿尔及利亚 LCPTS实验室，霍阿里·布梅迪恩科技大学（USTHB），P.O.
    Box 32, El-Alia, Bab-Ezzouar, Algiers 16111, 阿尔及利亚。 迪拜大学工程与信息技术学院，迪拜，阿联酋
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent advancements in deep learning (DL) have posed a significant challenge
    for automatic speech recognition (ASR). ASR relies on extensive training datasets,
    including confidential ones, and demands substantial computational and storage
    resources. Enabling adaptive systems improves ASR performance in dynamic environments.
    DL techniques assume training and testing data originate from the same domain,
    which is not always true. Advanced DL techniques like deep transfer learning (DTL),
    federated learning (FL), and reinforcement learning (RL) address these issues.
    DTL allows high-performance models using small yet related datasets, FL enables
    training on confidential data without dataset possession, and RL optimizes decision-making
    in dynamic environments, reducing computation costs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）的最新进展给自动语音识别（ASR）带来了重大挑战。ASR依赖于大量的训练数据集，包括机密数据，并且需要大量的计算和存储资源。启用自适应系统可以提高ASR在动态环境中的性能。DL技术假设训练和测试数据来源于相同的领域，这并不总是正确的。先进的DL技术，如深度迁移学习（DTL）、联邦学习（FL）和强化学习（RL），解决了这些问题。DTL允许使用小而相关的数据集来实现高性能模型，FL允许在不拥有数据集的情况下对机密数据进行训练，RL优化动态环境中的决策制定，从而降低计算成本。
- en: This survey offers a comprehensive review of DTL, FL, and RL-based ASR frameworks,
    aiming to provide insights into the latest developments and aid researchers and
    professionals in understanding the current challenges. Additionally, transformers,
    which are advanced DL techniques heavily used in proposed ASR frameworks, are
    considered in this survey for their ability to capture extensive dependencies
    in the input ASR sequence. The paper starts by presenting the background of DTL,
    FL, RL, and Transformers and then adopts a well-designed taxonomy to outline the
    [state-of-the-art](#Sx1.44.44.44) ([SOTA](#Sx1.44.44.44)) approaches. Subsequently,
    a critical analysis is conducted to identify the strengths and weaknesses of each
    framework. Additionally, a comparative study is presented to highlight the existing
    challenges, paving the way for future research opportunities.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查提供了关于基于DTL、FL和RL的ASR框架的全面回顾，旨在提供对最新发展的见解，并帮助研究人员和专业人士了解当前的挑战。此外，变换器作为一种先进的深度学习技术，在提出的ASR框架中被广泛使用，本调查考虑了它们在捕捉输入ASR序列中广泛依赖关系的能力。本文首先介绍了DTL、FL、RL和变换器的背景，然后采用精心设计的分类法概述了[前沿技术](#Sx1.44.44.44)（[SOTA](#Sx1.44.44.44)）方法。随后，进行了关键分析，以识别每个框架的优缺点。此外，呈现了一项比较研究，以突出现有的挑战，为未来的研究机会铺平道路。
- en: 'keywords:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Automatic speech recognition \sepDeep transfer learning \sepTransformers \sepFederated
    learning \sepReinforcement learning
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自动语音识别 \sep 深度迁移学习 \sep 变换器 \sep 联邦学习 \sep 强化学习
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 1.1 Preliminary
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 初步
- en: Advancements in [artificial intelligence](#Sx1.1.1.1) ([AI](#Sx1.1.1.1)) have
    significantly improved [human-machine interaction](#Sx1.22.22.22) ([HMI](#Sx1.22.22.22)),
    especially with technologies that convert speech into executable actions. [automatic
    speech recognition](#Sx1.4.4.4) ([ASR](#Sx1.4.4.4)) emerges as a leading communication
    technology in [HMI](#Sx1.22.22.22), extensively utilized by corporations and service
    providers for facilitating interactions through AI platforms like chatbots and
    digital assistants. Spoken language forms the core of these interactions, emphasizing
    the necessity for sophisticated speech processing in [AI](#Sx1.1.1.1) systems
    tailored for [ASR](#Sx1.4.4.4). [ASR](#Sx1.4.4.4) technology encompasses the analysis
    of (i) acoustic, lexical, and syntactic aspects; and (ii) semantic understanding.
    The [acoustic model](#Sx1.2.2.2) ([AM](#Sx1.2.2.2)) processing includes speech
    coding [[1](#bib.bib1)], enhancement [[2](#bib.bib2)], and source separation [[3](#bib.bib3)],
    alongside securing speech via steganography [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]
    and watermarking [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]. These components
    are integral to audio analysis. On the other hand, the [semantic model](#Sx1.42.42.42)
    ([SM](#Sx1.42.42.42)), often identified as [language model](#Sx1.24.24.24) ([LM](#Sx1.24.24.24))
    processing in literature, involves all [natural language processing](#Sx1.31.31.31)
    ([NLP](#Sx1.31.31.31)) techniques. This AI branch aims at teaching computers to
    understand and interpret human language, serving as the basis for applications
    like music information retrieval [[10](#bib.bib10)], sound file organization [[11](#bib.bib11)],
    audio tagging, and [event detection](#Sx1.17.17.17) ([ED](#Sx1.17.17.17)) [[12](#bib.bib12)],
    as well as converting speech to text and vice versa [[13](#bib.bib13)], detecting
    hate speech [[14](#bib.bib14)], and cyberbullying [[15](#bib.bib15)]. Employing
    [NLP](#Sx1.31.31.31) across various domains enables AI models to effectively comprehend
    and respond to human inputs, unveiling extensive research prospects in diverse
    sectors.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[人工智能](#Sx1.1.1.1) ([AI](#Sx1.1.1.1))的进步显著改善了[人机交互](#Sx1.22.22.22) ([HMI](#Sx1.22.22.22))，特别是将语音转换为可执行动作的技术。[自动语音识别](#Sx1.4.4.4)
    ([ASR](#Sx1.4.4.4))成为[HMI](#Sx1.22.22.22)的领先通信技术，被公司和服务提供商广泛用于通过聊天机器人和数字助理等AI平台促进互动。口语是这些互动的核心，这强调了在为[ASR](#Sx1.4.4.4)量身定制的[AI](#Sx1.1.1.1)系统中需要复杂的语音处理。[ASR](#Sx1.4.4.4)技术包括对（i）声学、词汇和句法方面的分析；以及（ii）语义理解的处理。[声学模型](#Sx1.2.2.2)
    ([AM](#Sx1.2.2.2))处理包括语音编码 [[1](#bib.bib1)]、增强 [[2](#bib.bib2)] 和源分离 [[3](#bib.bib3)]，以及通过隐写术
    [[4](#bib.bib4)、[5](#bib.bib5)、[6](#bib.bib6)] 和水印 [[7](#bib.bib7)、[8](#bib.bib8)、[9](#bib.bib9)]
    保护语音。这些组件对音频分析至关重要。另一方面，[语义模型](#Sx1.42.42.42) ([SM](#Sx1.42.42.42))，通常在文献中称为[语言模型](#Sx1.24.24.24)
    ([LM](#Sx1.24.24.24))处理，涉及所有[自然语言处理](#Sx1.31.31.31) ([NLP](#Sx1.31.31.31))技术。这一AI分支旨在教会计算机理解和解释人类语言，为音乐信息检索
    [[10](#bib.bib10)]、声音文件组织 [[11](#bib.bib11)]、音频标记以及[事件检测](#Sx1.17.17.17) ([ED](#Sx1.17.17.17))
    [[12](#bib.bib12)]、将语音转换为文本及其反向 [[13](#bib.bib13)]、检测仇恨言论 [[14](#bib.bib14)] 和网络欺凌
    [[15](#bib.bib15)] 等应用提供基础。跨领域应用[NLP](#Sx1.31.31.31)使AI模型能够有效理解和回应人类输入，揭示了各个领域广泛的研究前景。'
- en: Abbreviations
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩写
- en: AI
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: AI
- en: artificial intelligence
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能
- en: AM
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: AM
- en: acoustic model
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 声学模型
- en: APT
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: APT
- en: audio pyramid transformer
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 音频金字塔变换器
- en: ASR
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ASR
- en: automatic speech recognition
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自动语音识别
- en: AST
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: AST
- en: audio spectrogram transformer
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 音频频谱图变换器
- en: AT
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: AT
- en: audio tagging
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 音频标记
- en: CAFT
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: CAFT
- en: client adaptive federated training
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端自适应联邦训练
- en: CER
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CER
- en: character error rate
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 字符错误率
- en: CNN
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: CNN
- en: convolutional neural network
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: CS
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: CS
- en: code-switching
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 代码切换
- en: CTC
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: CTC
- en: connectionist temporal classification
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 连接时序分类
- en: CV
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: CV
- en: computer vision
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: DA
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: DA
- en: domain adaptation
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应
- en: DL
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: DL
- en: deep learning
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习
- en: DRL
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: DRL
- en: deep reinforcement learning
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: DTL
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: DTL
- en: deep transfer learning
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 深度迁移学习
- en: ED
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ED
- en: event detection
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 事件检测
- en: FCF
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: FCF
- en: feature correlation-based fusion
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 特征相关性融合
- en: FedNST
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: FedNST
- en: federated noisy student training
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦嘈杂学生训练
- en: FL
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: FL
- en: federated learning
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习
- en: FR
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: FR
- en: form recognition
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 形式识别
- en: HMI
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 人机交互
- en: human-machine interaction
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 人机交互
- en: HMM
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: HMM
- en: hidden Markov models
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型
- en: LM
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: LM
- en: language model
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型
- en: LPC
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: LPC
- en: linear predictive coding
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 线性预测编码
- en: LSTM
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM
- en: long short term memory
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆
- en: mAP
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: mAP
- en: mean average precision
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 平均精度
- en: MFCC
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: MFCC
- en: Mel-frequency cepstral coefficient
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Mel频率倒谱系数
- en: MHSA
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: MHSA
- en: multi-head self-attention
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力
- en: ML
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ML
- en: machine learning
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习
- en: NLP
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: NLP
- en: natural language processing
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: NT
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: NT
- en: negative transfer
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 负迁移
- en: PESQ
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: PESQ
- en: perceptual evaluation of speech quality
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 语音质量感知评估
- en: RER
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: RER
- en: relative error rate
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 相对误差率
- en: RL
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: RL
- en: reinforcement learning
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习
- en: RNN
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: RNN
- en: recurrent neural network
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: RTF
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: RTF
- en: real-time factor
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 实时因子
- en: S2S
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: S2S
- en: sequence-to-sequence
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列
- en: S2S
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: S2S
- en: sequence-to-sequence
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列
- en: SD
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: SD
- en: source domain
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 源领域
- en: SER
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: SER
- en: speech emotion recognition
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 语音情感识别
- en: SM
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: SM
- en: semantic model
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 语义模型
- en: SNR
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: SNR
- en: signal-to-noise ratio
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 信噪比
- en: SOTA
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: SOTA
- en: state-of-the-art
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的
- en: SS
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: SS
- en: speech security
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 语音安全
- en: SSAST
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: SSAST
- en: self-supervised audio spectrogram transformer
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督音频谱图 transformer
- en: SWBD
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: SWBD
- en: switchboard
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: switchboard
- en: TD
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: TD
- en: target domain
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 目标领域
- en: TNR
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: TNR
- en: true negative rate
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 真负率
- en: TPR
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: TPR
- en: true positive rate
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 真正率
- en: TRUNet
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: TRUNet
- en: transformer-recurrent-U network
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: transformer-recurrent-U 网络
- en: WER
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: WER
- en: word error rate
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 词错误率
- en: WSJ
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: WSJ
- en: wall street journal
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 华尔街日报
- en: GAN
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: GAN
- en: generative adversarial network
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: MTL
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: MTL
- en: multitask learning
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习
- en: FMTL
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: FMTL
- en: federated multi-task learning
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦多任务学习
- en: DSLM
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: DSLM
- en: domain-specific language modeling
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 领域特定语言建模
- en: LLM
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: LLM
- en: large language model
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: DDQN
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: DDQN
- en: double deep Q-network
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 双深度 Q 网络
- en: AC
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: AC
- en: actor-critic
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: actor-critic
- en: SARSA
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA
- en: State–action–reward–state–action
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 状态–动作–奖励–状态–动作
- en: DDPG
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG
- en: deep deterministic policy gradien
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度
- en: Recent advancements in [ASR](#Sx1.4.4.4) have been significantly propelled by
    the evolution of [deep learning](#Sx1.14.14.14) ([DL](#Sx1.14.14.14)) methodologies.
    An extensive range of [DL](#Sx1.14.14.14) models has been developed, demonstrating
    remarkable improvements and surpassing former [SOTA](#Sx1.44.44.44) achievements
    [[16](#bib.bib16), [17](#bib.bib17)]. Transformers, a notable innovation within
    these [DL](#Sx1.14.14.14) approaches, have become a cornerstone in advancing various
    [NLP](#Sx1.31.31.31) tasks, including [ASR](#Sx1.4.4.4). Initially conceptualized
    for [sequence-to-sequence](#Sx1.39.39.39) ([S2S](#Sx1.39.39.39)) applications
    in [NLP](#Sx1.31.31.31), their success is largely attributed to their adeptness
    at discerning long-range dependencies and complex patterns within sequential data.
    A hallmark of transformer models is their utilization of an attention mechanism,
    which precisely focuses on specific portions of the input sequence during prediction
    tasks. This mechanism is particularly effective in [ASR](#Sx1.4.4.4), facilitating
    the detailed modeling of contextual nuances and the interconnections among acoustic
    signals, essential for accurate transcription. Models such as the Transformer
    Transducer, Conformer, and ESPnet, leveraging self-attention and parallel processing,
    have achieved leading performance in [ASR](#Sx1.4.4.4) tasks. Their robustness
    across diverse languages further underscores their capability to adapt to a wide
    range of linguistic features and acoustic variations, making transformers an exceptionally
    promising option for enhancing [ASR](#Sx1.4.4.4) systems, surpassing the constraints
    of conventional models.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在[ASR](#Sx1.4.4.4)领域的进展，主要得益于[深度学习](#Sx1.14.14.14)（[DL](#Sx1.14.14.14)）方法的演变。已开发出广泛的[DL](#Sx1.14.14.14)模型，表现出显著改进，超越了以往的[SOTA](#Sx1.44.44.44)成就[[16](#bib.bib16),
    [17](#bib.bib17)]。变换器作为这些[DL](#Sx1.14.14.14)方法中的一项重要创新，已成为推进各种[NLP](#Sx1.31.31.31)任务的基石，包括[ASR](#Sx1.4.4.4)。最初为[NLP](#Sx1.31.31.31)中的[序列到序列](#Sx1.39.39.39)（[S2S](#Sx1.39.39.39)）应用而构思，其成功主要归因于其在识别长距离依赖关系和复杂模式方面的高效性。变换器模型的一个标志性特点是其使用注意力机制，该机制在预测任务中精确地关注输入序列的特定部分。该机制在[ASR](#Sx1.4.4.4)中尤其有效，促进了上下文细节和声学信号之间关系的详细建模，对于准确的转录至关重要。利用自注意力和并行处理的模型，如Transformer
    Transducer、Conformer和ESPnet，在[ASR](#Sx1.4.4.4)任务中取得了领先的表现。它们在各种语言中的鲁棒性进一步强调了它们适应广泛语言特征和声学变异的能力，使变换器成为增强[ASR](#Sx1.4.4.4)系统的极具潜力的选择，超越了传统模型的局限。
- en: The integration of [DL](#Sx1.14.14.14) with its variants in [ASR](#Sx1.4.4.4)
    introduces substantial challenges, especially concerning its application in natural
    [HMI](#Sx1.22.22.22). Despite [DL](#Sx1.14.14.14)’s numerous advantages, it encounters
    various obstacles. The inherent complexity of [DL](#Sx1.14.14.14) models, which
    stems from their need for extensive training data to attain high performance,
    demands significant computational and storage resources [[18](#bib.bib18)]. Moreover,
    the issue of data scarcity in [ASR](#Sx1.4.4.4) reflects the inadequate quantities
    of training data available for exploiting complex [DL](#Sx1.14.14.14) algorithms
    effectively [[19](#bib.bib19)]. The paucity of annotated data further complicates
    the development of supervised [DL](#Sx1.14.14.14)-based [ASR](#Sx1.4.4.4) models.
    Additionally, the presumption that training and testing datasets originate from
    the same domain, possessing identical feature spaces and distribution characteristics,
    is often misguided. This mismatch challenges the practical deployment of [DL](#Sx1.14.14.14)
    models in real-world settings [[20](#bib.bib20)]. Thus, the performance of [DL](#Sx1.14.14.14)
    models may be compromised when faced with limited training datasets or discrepancies
    in data distribution between training and testing environments [[21](#bib.bib21)].
    These challenges highlight the critical need for adaptive methodologies and improved
    data management approaches to fully harness the capabilities of [DL](#Sx1.14.14.14)
    in [ASR](#Sx1.4.4.4) systems.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[深度学习](#Sx1.14.14.14)及其变体在[自动语音识别](#Sx1.4.4.4)中的整合带来了实质性的挑战，特别是在其自然[人机交互](#Sx1.22.22.22)应用方面。尽管[深度学习](#Sx1.14.14.14)有众多优势，但也面临各种障碍。[深度学习](#Sx1.14.14.14)模型的固有复杂性，源于它们需要大量训练数据以达到高性能，这需要大量的计算和存储资源[[18](#bib.bib18)]。此外，[自动语音识别](#Sx1.4.4.4)中数据稀缺的问题反映了有效利用复杂[深度学习](#Sx1.14.14.14)算法所需的训练数据量不足[[19](#bib.bib19)]。注释数据的不足进一步使基于[深度学习](#Sx1.14.14.14)的[自动语音识别](#Sx1.4.4.4)模型的开发变得更加复杂。此外，假设训练和测试数据集来自相同领域，具有相同的特征空间和分布特性，通常是不切实际的。这种不匹配挑战了[深度学习](#Sx1.14.14.14)模型在实际环境中的部署[[20](#bib.bib20)]。因此，当面临有限的训练数据集或训练和测试环境之间的数据分布不一致时，[深度学习](#Sx1.14.14.14)模型的性能可能会受到影响[[21](#bib.bib21)]。这些挑战突显了在[自动语音识别](#Sx1.4.4.4)系统中充分发挥[深度学习](#Sx1.14.14.14)能力的关键需要适应性方法和改进的数据管理方法。'
- en: In an effort to address existing challenges and increase the robustness and
    flexibility of [ASR](#Sx1.4.4.4) systems, novel [DL](#Sx1.14.14.14) methodologies
    have been introduced. These include [deep transfer learning](#Sx1.16.16.16) ([DTL](#Sx1.16.16.16)),
    [deep reinforcement learning](#Sx1.15.15.15) ([DRL](#Sx1.15.15.15)), and [federated
    learning](#Sx1.20.20.20) ([FL](#Sx1.20.20.20)), which collectively aim at overcoming
    difficulties related to the transfer of knowledge, enhancing the generalization
    capabilities of models, and optimizing training processes. These innovative approaches
    significantly broaden the operational scope of conventional [DL](#Sx1.14.14.14)
    frameworks within the [ASR](#Sx1.4.4.4) field.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决现有挑战，提高[自动语音识别](#Sx1.4.4.4)系统的鲁棒性和灵活性，已经引入了新型[深度学习](#Sx1.14.14.14)方法。这些方法包括[深度迁移学习](#Sx1.16.16.16)
    ([DTL](#Sx1.16.16.16))、[深度强化学习](#Sx1.15.15.15) ([DRL](#Sx1.15.15.15))和[联邦学习](#Sx1.20.20.20)
    ([FL](#Sx1.20.20.20))，这些方法共同旨在克服知识转移相关的困难，提高模型的泛化能力，并优化训练过程。这些创新方法显著扩展了传统[深度学习](#Sx1.14.14.14)框架在[自动语音识别](#Sx1.4.4.4)领域的操作范围。
- en: 'Figure [1](#Sx1.F1 "Figure 1 ‣ 1.1 Preliminary ‣ 1 Introduction ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey") highlights
    critical areas in speech processing where [DTL](#Sx1.16.16.16), [DRL](#Sx1.15.15.15),
    and [FL](#Sx1.20.20.20) can be applied. Consequently, domains such as [ASR](#Sx1.4.4.4),
    speech enhancement (SE), hate speech detection (HSD), and [speech security](#Sx1.45.45.45)
    ([SS](#Sx1.45.45.45)) are closely interconnected. [ASR](#Sx1.4.4.4) provides acoustic
    parameters to [NLP](#Sx1.31.31.31) for HSD task, which in turn provides semantic
    details to [ASR](#Sx1.4.4.4). Additionally, [ASR](#Sx1.4.4.4) can be employed
    in the SS domain as a steganalytic process to verify the integrity of speech [[4](#bib.bib4),
    [22](#bib.bib22)]. Furthermore, [ASR](#Sx1.4.4.4) and SE can mutually offer performance
    feedback.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](#Sx1.F1 "图 1 ‣ 1.1 初步 ‣ 1 引言 ‣ 使用先进深度学习方法的自动语音识别：综述")突出显示了语音处理中的关键领域，说明了[DTL](#Sx1.16.16.16)、[DRL](#Sx1.15.15.15)和[FL](#Sx1.20.20.20)的应用。因此，[ASR](#Sx1.4.4.4)、语音增强（SE）、仇恨言论检测（HSD）和[语音安全](#Sx1.45.45.45)（[SS](#Sx1.45.45.45)）领域密切相关。[ASR](#Sx1.4.4.4)为HSD任务提供声学参数，而HSD任务则为[ASR](#Sx1.4.4.4)提供语义细节。此外，[ASR](#Sx1.4.4.4)还可以在SS领域作为隐写分析过程来验证语音的完整性[[4](#bib.bib4),
    [22](#bib.bib22)]。此外，[ASR](#Sx1.4.4.4)和SE可以互相提供性能反馈。
- en: '![Refer to caption](img/831c165f8b6ed8f5a79c8c80547a59aa.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/831c165f8b6ed8f5a79c8c80547a59aa.png)'
- en: 'Figure 1: Summary of critical areas in speech processing where [DTL](#Sx1.16.16.16),
    DRL, FL, and transformers can be applied.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 语音处理中的关键领域汇总，其中[DTL](#Sx1.16.16.16)、DRL、FL和transformers可以应用。'
- en: 1.2 Contribution of the paper
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 论文贡献
- en: This article offers an extensive examination of contemporary frameworks within
    advanced deep learning approaches, spanning the period from 2016 to 2023\. These
    approaches include [DTL](#Sx1.16.16.16), [DRL](#Sx1.15.15.15), [FL](#Sx1.20.20.20),
    and Transformers, all within the context of [ASR](#Sx1.4.4.4). To the best of
    the authors’ knowledge, there has been no prior research paper that has intricately
    explored and critically evaluated contributions in the aforementioned advanced
    DL-based [ASR](#Sx1.4.4.4) until now.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对2016年至2023年间的先进深度学习方法中的现代框架进行了广泛的探讨。这些方法包括[DTL](#Sx1.16.16.16)、[DRL](#Sx1.15.15.15)、[FL](#Sx1.20.20.20)和Transformers，均在[ASR](#Sx1.4.4.4)的背景下进行讨论。据作者所知，迄今为止尚未有研究论文深入探讨并批判性评估上述先进深度学习基础的[ASR](#Sx1.4.4.4)的贡献。
- en: 'In recent years, numerous survey papers have been published to assess various
    aspects of [ASR](#Sx1.4.4.4) models. Some of these surveys concentrate on specific
    languages, such as Portuguese [[23](#bib.bib23)], Indian [[24](#bib.bib24)], Turkish
    [[25](#bib.bib25)], Arabic [[26](#bib.bib26)] and tonal languages (including Asian,
    Indo-European and African) [[27](#bib.bib27)]. Additionally, Abushariah et al.’s
    review emphasizes bilingual ASR [[28](#bib.bib28)]. On the non-specific language
    review front, specific areas within ASR have been targeted, including ASR using
    limited vocabulary [[29](#bib.bib29)], ASR for children [[30](#bib.bib30)], error
    detection and correction [[31](#bib.bib31)], and unsupervised ASR [[32](#bib.bib32)].
    Systematic reviews with a focus on neural networks [[33](#bib.bib33)] and deep
    neural networks [[34](#bib.bib34)] have also been proposed. In another comprehensive
    review, Malik et al. [[35](#bib.bib35)] discussed diverse feature extraction methods,
    [SOTA](#Sx1.44.44.44) classification models, and some deep learning approaches.
    Recently, the authors presented an ASR review focused on DTL for ASR [[36](#bib.bib36)].
    Table [1](#Sx1.T1 "Table 1 ‣ 1.2 Contribution of the paper ‣ 1 Introduction ‣
    Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    presents a summary of the main contributions of the proposed ASR review compared
    to other existing ASR reviews/surveys.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '最近几年，已经发表了大量的综述论文，以评估[ASR](#Sx1.4.4.4)模型的各个方面。这些综述中有些集中于特定语言，如葡萄牙语[[23](#bib.bib23)]、印度语[[24](#bib.bib24)]、土耳其语[[25](#bib.bib25)]、阿拉伯语[[26](#bib.bib26)]和声调语言（包括亚洲、印欧和非洲语言）[[27](#bib.bib27)]。此外，Abushariah等人的综述强调了双语ASR[[28](#bib.bib28)]。在非特定语言的综述方面，ASR的具体领域也被关注，包括使用有限词汇的ASR[[29](#bib.bib29)]、儿童ASR[[30](#bib.bib30)]、错误检测与纠正[[31](#bib.bib31)]以及无监督ASR[[32](#bib.bib32)]。还提出了集中于神经网络[[33](#bib.bib33)]和深度神经网络[[34](#bib.bib34)]的系统评审。在另一项综合综述中，Malik等人[[35](#bib.bib35)]讨论了多种特征提取方法，[SOTA](#Sx1.44.44.44)分类模型以及一些深度学习方法。最近，作者们提出了一项以DTL为ASR的综述[[36](#bib.bib36)]。表[1](#Sx1.T1
    "Table 1 ‣ 1.2 Contribution of the paper ‣ 1 Introduction ‣ Automatic Speech Recognition
    using Advanced Deep Learning Approaches: A survey")展示了与其他现有ASR综述/调查相比，所提出ASR综述的主要贡献总结。'
- en: 'This survey article offers several significant enhancements and additions compared
    to previous ASR surveys. Firstly, it consolidates works that utilize both ASR
    and advanced DL approaches, providing a comprehensive overview of their intersection.
    Secondly, it provides performance evaluation results of all considered approaches.
    Thirdly, it includes metrics and dataset reviews used in ASR models. Furthermore,
    it tackles ongoing challenges and consequently proposes future directions. The
    main contributions of this article can be summarized as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述文章相比于之前的ASR综述提供了若干重要的改进和补充。首先，它整合了同时使用ASR和先进DL方法的研究，提供了它们交汇处的全面概述。其次，它提供了所有考虑的方法的性能评估结果。第三，它包含了ASR模型中使用的指标和数据集的评审。此外，它解决了当前面临的挑战，并因此提出了未来的方向。本文的主要贡献可以总结如下：
- en: •
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Presenting the background of advanced DL techniques including DTL, DRL, FL and
    transformers. Describing the evaluation metrics and datasets employed for validating
    [ASR](#Sx1.4.4.4) approaches.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 介绍先进DL技术的背景，包括DTL、DRL、FL和transformers。描述用于验证[ASR](#Sx1.4.4.4)方法的评估指标和数据集。
- en: •
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Introducing a well-defined taxonomy categorizing [ASR](#Sx1.4.4.4) methodologies
    based on the domains of [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24).
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 引入一个明确的分类法，将[ASR](#Sx1.4.4.4)方法按照[AM](#Sx1.2.2.2)和[LM](#Sx1.24.24.24)的领域进行分类。
- en: •
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Identifying challenges and gaps in advanced DL-based [ASR](#Sx1.4.4.4).
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确定基于先进DL的[ASR](#Sx1.4.4.4)中的挑战和不足之处。
- en: •
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Proposing future directions to enhance the performance of advanced DL-based
    [ASR](#Sx1.4.4.4) solutions and predicting the potential advancements in the field.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出未来方向，以提升基于先进DL的[ASR](#Sx1.4.4.4)解决方案的性能，并预测该领域的潜在进展。
- en: 'Table 1: Contribution comparison of the proposed contribution against other
    hand ASR review. The tick mark (✓) indicates that the specific field has been
    addressed, whereas the cross mark (✗) means addressing the specific fields has
    been missed.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：所提出的贡献与其他ASR综述的比较。勾选标记（✓）表示特定领域已被覆盖，而叉号（✗）表示特定领域未被覆盖。
- en: '| Refs | Year | Description of the survey/review | Advanced DL methods | Performances
    | Metrics | Dataset | Current | Future |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 年份 | 调查/综述描述 | 先进的深度学习方法 | 性能 | 指标 | 数据集 | 当前 | 未来 |'
- en: '|  |  |  | DRL | FL | TL | Transf. | evaluation |  | review | challe/Gaps |
    directions |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | DRL | FL | TL | 迁移 | 评估 |  | 综述 | 挑战/差距 | 方向 |'
- en: '| [[31](#bib.bib31)] | 2018 | ASR review for error errors detection and correction
    | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ | ✗ |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| [[31](#bib.bib31)] | 2018 | 错误检测和纠正的 ASR 综述 | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ |
    ✗ | ✗ |'
- en: '| [[34](#bib.bib34)] | 2019 | Systematic review on DL-based speech recognition
    | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| [[34](#bib.bib34)] | 2019 | 基于深度学习的语音识别系统性综述 | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗
    | ✗ | ✗ |'
- en: '| [[24](#bib.bib24)] | 2020 | ASR survey for Indian languages | ✗ | ✗ | ✗ |
    ✗ | ✗ | ✗ | ✓ | ✗ | ✓ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| [[24](#bib.bib24)] | 2020 | 印地语 ASR 调查 | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ |
    ✓ |'
- en: '| [[23](#bib.bib23)] | 2020 | ASR survey for Portuguese language | ✗ | ✗ |
    ✗ | ✗ | ✗ | ✗ | ✓ | ✓ | ✓ |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| [[23](#bib.bib23)] | 2020 | 葡萄牙语 ASR 调查 | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ | ✓ |
    ✓ |'
- en: '| [[25](#bib.bib25)] | 2020 | ASR survey for Turkich language | ✗ | ✗ | ✗ |
    ✗ | ✓ | ✗ | ✓ | ✗ | ✗ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| [[25](#bib.bib25)] | 2020 | 土耳其语言 ASR 调查 | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✓ | ✗
    | ✗ |'
- en: '| [[35](#bib.bib35)] | 2021 | ASR survey | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ |
    ✓ |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| [[35](#bib.bib35)] | 2021 | ASR 调查 | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ | ✓ |'
- en: '| [[27](#bib.bib27)] | 2021 | ASR survey for tonal languages | ✗ | ✗ | ✗ |
    ✗ | ✓ | ✗ | ✓ | ✓ | ✓ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| [[27](#bib.bib27)] | 2021 | 语调语言的 ASR 调查 | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✓ | ✓
    | ✓ |'
- en: '| [[32](#bib.bib32)] | 2022 | Unsupervised ASR review | ✗ | ✗ | ✗ | ✗ | ✓ |
    ✗ | ✗ | ✓ | ✗ |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| [[32](#bib.bib32)] | 2022 | 无监督 ASR 综述 | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ | ✓ |
    ✗ |'
- en: '| [[30](#bib.bib30)] | 2022 | ASR Systematic review for children | ✗ | ✗ |
    ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| [[30](#bib.bib30)] | 2022 | 儿童 ASR 系统性综述 | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ | ✗
    | ✗ |'
- en: '| [[29](#bib.bib29)] | 2022 | ASR survey for limited vocabulary | ✓ | ✗ | ✗
    | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] | 2022 | 有限词汇的 ASR 调查 | ✓ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✗
    | ✓ |'
- en: '| [[26](#bib.bib26)] | 2022 | ASR Systematic review for Arabic language | ✗
    | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ | ✓ | ✓ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| [[26](#bib.bib26)] | 2022 | 阿拉伯语 ASR 系统性综述 | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ |
    ✓ | ✓ |'
- en: '| [[28](#bib.bib28)] | 2022 | Bilingual ASR review | ✗ | ✗ | ✗ | ✗ | ✓ | ✗
    | ✓ | ✓ | ✗ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| [[28](#bib.bib28)] | 2022 | 双语 ASR 综述 | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✓ | ✓ | ✗
    |'
- en: '| [[33](#bib.bib33)] | 2023 | ASR survey on neural network techniques | ✗ |
    ✗ | ✓ | ✗ | ✓ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| [[33](#bib.bib33)] | 2023 | 神经网络技术的 ASR 调查 | ✗ | ✗ | ✓ | ✗ | ✓ | ✓ | ✓ |
    ✓ | ✗ |'
- en: '| [[36](#bib.bib36)] | 2023 | ASR based on [DTL](#Sx1.16.16.16) review | ✗
    | ✗ | ✓ | ✗ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| [[36](#bib.bib36)] | 2023 | 基于 [DTL](#Sx1.16.16.16) 的 ASR 综述 | ✗ | ✗ | ✓
    | ✗ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: '| Our | 2024 | ASR review on advanced DL techniques | ✓ | ✓ | ✓ | ✓ | ✓ | ✓
    | ✓ | ✓ | ✓ |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 我们 | 2024 | 关于先进深度学习技术的 ASR 综述 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: 1.3 Review methodology
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 综述方法论
- en: 'The methodology for the review is delineated in this segment, encompassing
    the search strategy and study selection. Inclusion criteria, comprising keyword
    alignment, creativity and impact, and uniqueness, are explicated, collectively
    influencing the formulation of the paper’s quality assessment protocol. To locate
    and compile extant advanced DL-based [ASR](#Sx1.4.4.4) studies, a thorough search
    was executed on renowned publication databases recognized for hosting top-tier
    scientific research articles. The exploration encompassed Scopus and Web of Science.
    Keywords were extracted and organized from the initial set of references through
    manual analysis. Employing "theme clustering," these publications were sorted
    based on keywords found in the "Abstract," "Title," and "Authors keywords." The
    outcome of this process yielded the formulation of the following query:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分阐述了综述的方法论，包括搜索策略和研究选择。包括标准，如关键词对齐、创新性和影响力以及独特性，都被详细说明，这些共同影响了论文质量评估协议的制定。为了定位和汇编现有的先进深度学习
    [ASR](#Sx1.4.4.4) 研究，在知名的出版数据库中进行了全面的搜索，这些数据库以托管顶级科学研究文章而著称。探索涵盖了 Scopus 和 Web
    of Science。关键词从初步参考文献集中提取并组织。通过“主题聚类”，这些出版物根据“摘要”、“标题”和“作者关键词”中的关键词进行排序。此过程的结果形成了以下查询：
- en: References=FROM "Abstract" || "Title"|| "Authors keywords" SELECT( Papers WHERE
    keywords= ([ASR](#Sx1.4.4.4) || [NLP](#Sx1.31.31.31)) & ( [DTL](#Sx1.16.16.16)
    || [DRL](#Sx1.15.15.15) || [FL](#Sx1.20.20.20) || Transformers)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献=从“摘要” || “标题”|| “作者关键词”选择（论文，其中关键词= ([ASR](#Sx1.4.4.4) || [NLP](#Sx1.31.31.31))
    & ([DTL](#Sx1.16.16.16) || [DRL](#Sx1.15.15.15) || [FL](#Sx1.20.20.20) || Transformers)）。
- en: The symbols || and & denote OR and AND logical operations, respectively. The
    evaluation of publications considered the innovation level in [ASR](#Sx1.4.4.4),
    the study’s quality, and the contributions and findings presented. This review
    exclusively encompassed research contributions that were published within the
    timeframe of 2016 to 2023.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 符号 || 和 & 分别表示逻辑运算 OR 和 AND。对出版物的评估考虑了[ASR](#Sx1.4.4.4)中的创新水平、研究质量以及所呈现的贡献和发现。本综述仅涵盖了2016年至2023年间发表的研究贡献。
- en: 1.4 Structure of the paper
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 论文结构
- en: 'This paper is structured into six sections. The current section provides an
    introduction to the paper. Section [1.3](#Sx1.SS3 "1.3 Review methodology ‣ 1
    Introduction ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey") providing background on [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24), and
    reviewing evaluation metrics and datasets utilized in [ASR](#Sx1.4.4.4). Moving
    forward, Section [4](#S4 "4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey") delves
    into a comprehensive review of recent advancements in [ASR](#Sx1.4.4.4) utilizing
    advanced [DL](#Sx1.14.14.14) approaches, including Transformers, [DTL](#Sx1.16.16.16),
    [FL](#Sx1.20.20.20) and [DRL](#Sx1.15.15.15). Sections [5](#S5 "5 Open Issues
    and of Key challenges ‣ Automatic Speech Recognition using Advanced Deep Learning
    Approaches: A survey") and [6](#S6 "6 Future directions ‣ Automatic Speech Recognition
    using Advanced Deep Learning Approaches: A survey") respectively address the existing
    challenges and future directions concerning advanced DL-based [ASR](#Sx1.4.4.4).
    Finally, Section [7](#S7 "7 Conclusion ‣ Automatic Speech Recognition using Advanced
    Deep Learning Approaches: A survey") presents concluding remarks summarizing the
    key findings of the paper.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '本论文分为六个部分。当前部分介绍了论文。第[1.3](#Sx1.SS3 "1.3 Review methodology ‣ 1 Introduction
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")节提供了关于[AM](#Sx1.2.2.2)和[LM](#Sx1.24.24.24)的背景信息，并回顾了在[ASR](#Sx1.4.4.4)中使用的评价指标和数据集。接下来，第[4](#S4
    "4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey")节深入探讨了利用先进[DL](#Sx1.14.14.14)方法（包括Transformers）、[DTL](#Sx1.16.16.16)、[FL](#Sx1.20.20.20)和[DRL](#Sx1.15.15.15)的最新[ASR](#Sx1.4.4.4)进展。第[5](#S5
    "5 Open Issues and of Key challenges ‣ Automatic Speech Recognition using Advanced
    Deep Learning Approaches: A survey")节和第[6](#S6 "6 Future directions ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey")节分别讨论了关于先进DL基础的[ASR](#Sx1.4.4.4)的现有挑战和未来方向。最后，第[7](#S7
    "7 Conclusion ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey")节总结了论文的关键发现。'
- en: '![Refer to caption](img/58ddfc5ff4acf16ee636d0f80a2461db.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/58ddfc5ff4acf16ee636d0f80a2461db.png)'
- en: 'Figure 2: Survey roadmap: A guide for navigating paper sections and subsections.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：调查路线图：导航论文部分和子部分的指南。
- en: table[t!] Literature acquisition databases. to be modified Database Research
    Articles Conference Papers Book Chapter Total ACM 5 1 – 6 Elsevier 26 – – 26 Springer
    23 – 10 33 IEEE 39 28 – 67 Others 44 29 – 70
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 表[t!] 文献获取数据库。待修改 数据库 研究文章 会议论文 书籍章节 总计 ACM 5 1 – 6 Elsevier 26 – – 26 Springer
    23 – 10 33 IEEE 39 28 – 67 其他 44 29 – 70
- en: 2 Overview of DTL techniques for speech recognition
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 语音识别中DTL技术的概述
- en: 2.1 Taxonomy of existing DTL techniques
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 现有DTL技术的分类
- en: 'To date, there is no standardized and comprehensive technique for classifying
    DTL into categories. However, DTL algorithms could be classified into several
    types depending on what, when, and how knowledge is transferred:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 至今，还没有标准化和全面的技术来将DTL分类为不同类别。然而，DTL算法可以根据知识的传递方式、时间和方式分为几种类型：
- en: (a) What knowledge is transferred? Enquires about which characteristics of knowledge
    are transferable across domains or tasks. Some information is particular to certain
    domains or tasks, while other knowledge is shared across domains and can aid increase
    performance in the target task or domain. Based on this definition, DTL could
    be feature-based, instance-based, relation-based, or model-based.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 传递了什么知识？询问哪些知识特征可以跨领域或任务进行传递。有些信息特定于某些领域或任务，而其他知识则在不同领域之间共享，并可以帮助提高目标任务或领域的表现。根据这个定义，DTL可以是基于特征、基于实例、基于关系或基于模型的。
- en: (b) How is knowledge transferred? Enquires about which learning algorithms must
    be implemented to transfer the knowledge.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 知识是如何传递的？询问需要实施哪些学习算法来传递知识。
- en: (c) When is knowledge transferred? Inquires as to when and under what circumstances
    knowledge should or should not be transferred.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 知识何时转移？探讨在何时以及在什么情况下应或不应转移知识。
- en: 'Furthermore, researchers have proposed taxonomies to categorize DTL-based ASR
    techniques. For instance, Niu et al. [[21](#bib.bib21)] present a taxonomy with
    two levels. The first level consists of four sub-groups based on the availability
    of labeled data and the data modality in the source and target domains. These
    sub-groups include inductive DTL, transductive DTL, cross-modality DTL, and unsupervised
    DTL [[80](#bib.bib80)]. Table [2](#S2.T2 "Table 2 ‣ 2.1 Taxonomy of existing DTL
    techniques ‣ 2 Overview of DTL techniques for speech recognition ‣ Automatic Speech
    Recognition using Advanced Deep Learning Approaches: A survey") provides a summary
    of these possibilities. Moreover, each sub-group at the first level can be further
    divided into four distinct learning types: learning on instances, learning on
    features, learning on parameters, and learning on relations.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，研究人员提出了分类法来对基于DTL的ASR技术进行分类。例如，Niu等人[[21](#bib.bib21)]提出了一个包含两个层次的分类法。第一个层次包括基于源领域和目标领域的标注数据的可用性和数据模态的四个子组。这些子组包括归纳式DTL、转导式DTL、跨模态DTL和无监督DTL[[80](#bib.bib80)]。表[2](#S2.T2
    "Table 2 ‣ 2.1 Taxonomy of existing DTL techniques ‣ 2 Overview of DTL techniques
    for speech recognition ‣ Automatic Speech Recognition using Advanced Deep Learning
    Approaches: A survey")提供了这些可能性的总结。此外，第一个层次的每个子组还可以进一步细分为四种不同的学习类型：实例学习、特征学习、参数学习和关系学习。'
- en: 'Table 2: DTL possibilities. whereas the mark ($\varsubsetneq$) indicates that
    the domains/tasks are different but related, ($\exists!$) indicates that there
    exists one and only one domain/task, and ($\cong$) indicates that domains, tasks,
    or spaces are not always equals.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：DTL的可能性。其中标记（$\varsubsetneq$）表示领域/任务不同但相关，($\exists!$)表示存在唯一的领域/任务，而($\cong$)表示领域、任务或空间不总是相等的。
- en: '|  | Domains | Tasks | Math. propriety | Sub-categories / Usage |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | 领域 | 任务 | 数学性质 | 子类别/用途 |'
- en: '| Traditional ML/DL | $\mathbb{D}_{S}=\mathbb{D}_{T}$ | $\mathbb{T}_{S}=\mathbb{T}_{T}$
    | $X_{S}\neq X_{T}$, $Y_{S}=Y_{T}$ | ASR model trained with $X_{S}$ database and
    used to recognise $X_{T}$ database. |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 传统的机器学习/深度学习 | $\mathbb{D}_{S}=\mathbb{D}_{T}$ | $\mathbb{T}_{S}=\mathbb{T}_{T}$
    | $X_{S}\neq X_{T}$, $Y_{S}=Y_{T}$ | 使用$X_{S}$数据库训练的ASR模型用于识别$X_{T}$数据库。 |'
- en: '| Inductive DTL | $\mathbb{D}_{S}\cong\mathbb{D}_{T}$ | $\mathbb{T}_{S}\neq\mathbb{T}_{T}$
    | $X_{S}\neq X_{T}$, $Y_{S}\exists,Y_{T}\exists$ | If $Y_{S}\exists$, DTL is multitask
    learning. If $Y_{S}\nexists$, DTL is self-taught learning, thus $\chi_{S}\cong\chi_{T}$.
    |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 归纳式DTL | $\mathbb{D}_{S}\cong\mathbb{D}_{T}$ | $\mathbb{T}_{S}\neq\mathbb{T}_{T}$
    | $X_{S}\neq X_{T}$, $Y_{S}\exists,Y_{T}\exists$ | 如果$Y_{S}\exists$，则DTL是多任务学习。如果$Y_{S}\nexists$，则DTL是自学学习，因此$\chi_{S}\cong\chi_{T}$。
    |'
- en: '| Transductive DTL | $\mathbb{D}_{S}\neq\mathbb{D}_{T}$ | $\mathbb{T}_{S}=\mathbb{T}_{T}$
    | $P(X_{S})\neq P(X_{T})$, $Y_{S}\exists,Y_{T}\nexists$ ,'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '| 转导式DTL | $\mathbb{D}_{S}\neq\mathbb{D}_{T}$ | $\mathbb{T}_{S}=\mathbb{T}_{T}$
    | $P(X_{S})\neq P(X_{T})$, $Y_{S}\exists,Y_{T}\nexists$ |'
- en: $\chi_{S}=\chi_{T}$ | When $\chi_{S}=\chi_{T}$, DTL is is related to DA. If
    $\mathbb{D}_{T}\exists!$ and $\mathbb{T}_{T}\exists!$, DTL is used for sample
    selection bias or covariate shift. |
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: $\chi_{S}=\chi_{T}$ | 当$\chi_{S}=\chi_{T}$时，DTL与DA有关。如果$\mathbb{D}_{T}\exists!$且$\mathbb{T}_{T}\exists!$，DTL用于样本选择偏差或协变量偏移。
    |
- en: '| Cross-modality DTL | $\mathbb{D}_{S}\neq\mathbb{D}_{T}$ | $\mathbb{T}_{S}\neq\mathbb{T}_{T}$
    | $P(Y_{S}/X_{S})\neq P(Y_{T}/X_{T})$, $Y_{S}\neq Y_{T}$, $\chi_{S}\neq\chi_{T}$
    | i,e. the dataset $X_{S}$ of $\mathbb{D}_{S}$ is speech data, and the dataset
    $X_{T}$ of $\mathbb{D}_{T}$ is text data. |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 跨模态DTL | $\mathbb{D}_{S}\neq\mathbb{D}_{T}$ | $\mathbb{T}_{S}\neq\mathbb{T}_{T}$
    | $P(Y_{S}/X_{S})\neq P(Y_{T}/X_{T})$, $Y_{S}\neq Y_{T}$, $\chi_{S}\neq\chi_{T}$
    | 例如，$\mathbb{D}_{S}$的$X_{S}$数据集是语音数据，而$\mathbb{D}_{T}$的$X_{T}$数据集是文本数据。 |'
- en: '| Unsupervised DTL | $\mathbb{D}_{S}\varsubsetneq\mathbb{D}_{T}$ | $\mathbb{T}_{S}\varsubsetneq\mathbb{T}_{T}$
    | $Y_{S}\nexists,Y_{T}\nexists$ | DTL used for clustering, dimensionality reduction,
    and density estimation, etc. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 无监督DTL | $\mathbb{D}_{S}\varsubsetneq\mathbb{D}_{T}$ | $\mathbb{T}_{S}\varsubsetneq\mathbb{T}_{T}$
    | $Y_{S}\nexists,Y_{T}\nexists$ | 用于聚类、降维和密度估计等的DTL。 |'
- en: 'Inductive DTL In comparison to classical ML, which may be used as a reference
    for DTL comparison, and given that the target tasks $\mathbb{T}_{T}$ are distinct
    from the source tasks $\mathbb{T}_{S}$, the goal of inductive DTL is to enhance
    the target prediction function $\mathbb{F}_{T}$ in the TD, mentioned above in
    subsection LABEL:sub12. However, the SD $\mathbb{D}_{S}$ and TD $\mathbb{D}_{T}$
    may not always be the same (Table [2](#S2.T2 "Table 2 ‣ 2.1 Taxonomy of existing
    DTL techniques ‣ 2 Overview of DTL techniques for speech recognition ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey")). The inductive
    DTL can be stated similarly to the following two cases, depending on whether labeled
    or unlabeled data is available:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳 DTL 与经典 ML 相比，经典 ML 可以作为 DTL 比较的参考，并且鉴于目标任务 $\mathbb{T}_{T}$ 与源任务 $\mathbb{T}_{S}$
    不同，归纳 DTL 的目标是增强目标预测函数 $\mathbb{F}_{T}$ 在 TD 中，如前述于子节 LABEL:sub12。然而，SD $\mathbb{D}_{S}$
    和 TD $\mathbb{D}_{T}$ 可能不总是相同的（表 [2](#S2.T2 "表 2 ‣ 2.1 现有 DTL 技术的分类 ‣ 2 语音识别的
    DTL 技术概述 ‣ 使用先进深度学习方法的自动语音识别：综述")）。归纳 DTL 可以根据是否有标记或未标记的数据，类似于以下两种情况进行说明：
- en: '(a) Multi-task DTL: The SD has a huge labeled database ($X_{S}$ labeled with
    $Y_{S}$), which is a distinctive form of multi-task learning. However, with multi-task
    approaches, many tasks $(T_{1},T_{2},\dots,T_{n})$ are learned at the same time
    (in parallel), including both source and target activities (tasks).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 多任务 DTL：SD 拥有一个庞大的标记数据库（$X_{S}$ 标记 $Y_{S}$），这是一种多任务学习的独特形式。然而，通过多任务方法，许多任务
    $(T_{1},T_{2},\dots,T_{n})$ 会同时（并行）学习，包括源活动和目标活动（任务）。
- en: '(b) Sequential DTL: ( Commonly known as self-taught learning) Dataset is not
    labeled in the SD ($X_{S}$ is not labeled with $Y_{S}$) but the labels are available
    in the destination domain ($X_{T}$ is labeled with $Y_{T}$). Sequential learning
    is a DL system that can be realized in two steps for classification purposes.
    The first step is the feature representation transfer, which is learned from a
    large collection of the unlabeled datasets, and the second stage is when this
    learned representation is applied to labeled data to accomplish classification
    tasks. Hence, sequential DTL is a method of sequentially learning a number of
    activities (Tasks). The spaces between the source and destination domains may
    differ. For example, let suppose we have a pre-trained model $M$ and consider
    applying DTL to a number of tasks $(T_{1},T_{2},\dots,T_{n})$. We learn a specific
    task $\mathbb{T}_{T}$ at each time step $t$, which is slower than multi-task learning.
    However, when not all the tasks are present during training time, it might be
    beneficial. Sequential DTL is additionally classified into several types [alyafeai2020survey]:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 顺序 DTL：（通常称为自学学习）数据集在 SD 中未标记（$X_{S}$ 未标记 $Y_{S}$），但在目标领域中标签可用（$X_{T}$ 标记
    $Y_{T}$）。顺序学习是一种 DL 系统，可以在两个步骤中实现分类目的。第一步是特征表示迁移，从大量未标记数据集中学习，第二阶段是将此学到的表示应用于标记数据以完成分类任务。因此，顺序
    DTL 是一种顺序学习多个活动（任务）的方法。源领域和目标领域之间的间隔可能不同。例如，假设我们有一个预训练模型 $M$，并考虑将 DTL 应用到多个任务
    $(T_{1},T_{2},\dots,T_{n})$。我们在每个时间步骤 $t$ 学习一个特定的任务 $\mathbb{T}_{T}$，这比多任务学习要慢。然而，当不是所有任务在训练期间都存在时，这可能是有益的。顺序
    DTL 进一步分为几种类型 [alyafeai2020survey]：
- en: 1-
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1-
- en: 'Fine-tuning: The principle is to learn a new function $\mathbb{F}_{T}$ that
    translates the parameters $\mathbb{F}_{T}(W_{S})=W_{T}$ by using $M$, given a
    pre-trained model $M_{S}$ having $W_{S}$ as weights and target task $\mathbb{T}_{T}$
    having $W_{T}$ as weights. The settings can be adjusted across all layers or just
    some of (Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Taxonomy of existing DTL techniques ‣
    2 Overview of DTL techniques for speech recognition ‣ Automatic Speech Recognition
    using Advanced Deep Learning Approaches: A survey") (a)). The learning rate for
    each layer could be distinct (discriminative fine tuning). A new set of parameters
    $K$ could be added to most of the tasks so that $\mathbb{F}_{T}(W_{T},K)=W_{S}\circ
    K$.'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调：原理是学习一个新的函数 $\mathbb{F}_{T}$，该函数通过使用 $M$ 将参数 $\mathbb{F}_{T}(W_{S})=W_{T}$
    转换，给定一个预训练模型 $M_{S}$，其权重为 $W_{S}$，以及目标任务 $\mathbb{T}_{T}$，其权重为 $W_{T}$。设置可以在所有层中调整，或者仅在某些层中（图
    [3](#S2.F3 "图 3 ‣ 2.1 现有 DTL 技术的分类 ‣ 2 语音识别的 DTL 技术概述 ‣ 使用先进深度学习方法的自动语音识别：综述")
    (a)）。每层的学习率可以是不同的（区分微调）。可以向大多数任务添加一组新的参数 $K$，使得 $\mathbb{F}_{T}(W_{T},K)=W_{S}\circ
    K$。
- en: 2-
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2-
- en: 'Adapter modules: Given an $M_{S}$ model that has been pre-trained and output
    $W_{S}$, for a target task $\mathbb{T}_{T}$. The adapter module aims to lunch
    a different set of parameters $K$ that is too much less than $W_{S}$, i.e, $K\ll
    W_{S}$. $K$ and $W_{S}$ must have the ability to be decomposed into more compact
    modules such that, $W_{S}=\{w\}_{n}$ and $K=\{k\}_{n}$. The adapter module permit
    learning the following new function $\mathbb{F}_{T}$:'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '适配器模块: 给定一个预先训练的模型$M_{S}$并输出$W_{S}$，对于目标任务$\mathbb{T}_{T}$。适配器模块旨在启动不同的参数集$K$，这个集合比$W_{S}$要少得多，即，$K\ll
    W_{S}$。$K$和$W_{S}$必须能够分解成更紧凑的模块，即$W_{S}=\{w\}_{n}$和$K=\{k\}_{n}$。适配器模块允许学习以下新函数$\mathbb{F}_{T}$:'
- en: '|  | $\mathbb{F}_{T}(K,W_{S})=k_{1}^{\prime}\circ w_{1}\circ\dots k_{n}^{\prime}\circ
    w_{n}.$ |  | (1) |'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathbb{F}_{T}(K,W_{S})=k_{1}^{\prime}\circ w_{1}\circ\dots k_{n}^{\prime}\circ
    w_{n}.$ |  | (1) |'
- en: 'According to equation [1](#S2.E1 "In item 2- ‣ 2.1 Taxonomy of existing DTL
    techniques ‣ 2 Overview of DTL techniques for speech recognition ‣ Automatic Speech
    Recognition using Advanced Deep Learning Approaches: A survey"), during the adaptation
    procedure, the set of original weights $W_{S}=\{w\}_{n}$ is left unaltered, but
    the set of weights K is changed to $K^{\prime}=\{k^{\prime}\}_{n}$. The principle
    of the adaptation domain is illustrated in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Taxonomy
    of existing DTL techniques ‣ 2 Overview of DTL techniques for speech recognition
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    (b).'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据方程[1](#S2.E1 "在项目2- ‣ 2.1现有DTL技术的分类 ‣ 2深度学习技术的概述 ‣使用先进的深度学习方法进行语音识别：一项调查")，在适应过程中，原始权重集合$W_{S}=\{w\}_{n}$保持不变，但权重集合$K$变为$K^{\prime}=\{k^{\prime}\}_{n}$，适应域的原理如图[3](#S2.F3
    "图3 ‣ 2.1现有的DTL技术分类 ‣ 2深度学习技术概述 ‣使用先进的深度学习方法进行语音识别：一项调查") (b)所示。
- en: 3-
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3-
- en: 'Feature based: Interested only in learning concepts and representations, at
    various levels, such as, word, character, phrase, or paragraph embedding $E$.
    The collection of $E$ based on a model $M$ remains unaltered, i.e., $\mathbb{F}_{T}(W_{S},E)=E\circ
    W^{\prime}$, in the way that $W^{\prime}$ is fine-tuned. For example, researchers
    have applied the generative adversarial network (GAN) principle to DTL where,
    the generators send features from the SD and the TD to a discriminator, which
    determines the source of the features and feeds the result back to the generators
    until they can no longer be distinguished. In this procedure, GAN obtains the
    common properties of two domains, as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1
    Taxonomy of existing DTL techniques ‣ 2 Overview of DTL techniques for speech
    recognition ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey") (c).'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '基于特征: 仅对学习概念和表示感兴趣，例如，单词，字符，短语或段落嵌入$E$。基于模型$M$的$E$的集合保持不变，即，$\mathbb{F}_{T}(W_{S},E)=E\circ
    W^{\prime}$，其中$W^{\prime}$被微调。例如，研究人员已经将生成对抗网络(GAN)的原理应用到DTL中，生成器将特征从SD和TD发送到鉴别器，鉴别器确定特征的来源并将结果反馈给生成器，直到它们无法再被区分。在这个过程中，GAN获得了两个域的共同属性，如图[3](#S2.F3
    "图3 ‣ 2.1现有的DTL技术分类 ‣ 2深度学习技术概述 ‣使用先进的深度学习方法进行语音识别：一项调查") (c)所示。'
- en: 4-
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4-
- en: 'Zero-shot: Is the easiest method among all of the others. Making the assumption
    that the parameters $W_{S}$ can’t be modified or add $K$ as a new parameters to
    a pre-trained model $M_{S}$ using $W_{S}$. To put this into context, in zero-shot
    there is no training technique to optimize or learn new parameters.'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '零-shot: 是所有方法中最简单的。假设参数$W_{S}$无法修改或向预先训练的模型$M_{S}$添加$K$作为新参数使用$W_{S}。举个例子，在零-shot中没有训练技术来优化或学习新参数。'
- en: '![Refer to caption](img/0b511658b064674ac061c891b2172afe.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/0b511658b064674ac061c891b2172afe.png)'
- en: 'Figure 3: Structures of: (a) Fine-tuning, (b) DA, and (c) DTL-based GAN.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '图3: 结构：(a) 微调 、(b) DA 和 (c) 基于DTL的GAN。'
- en: 2.1.1 Transductive DTL
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 迁移DTL
- en: 'Compared to the traditional ML, which can be considered as a reference for
    DTL comparison, and given that the TDs $\mathbb{D}_{T}$ are distinct from the
    SDs $\mathbb{D}_{S}$. The SD has a labeled dataset ($X_{S}$ labeled with $Y_{S}$),
    whereas the TD has no labeled dataset, the source and target tasks are equal (
    Table [2](#S2.T2 "Table 2 ‣ 2.1 Taxonomy of existing DTL techniques ‣ 2 Overview
    of DTL techniques for speech recognition ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey")). The goal of transductive DTL is
    to build the target prediction function $\mathbb{F}_{T}$ in the $\mathbb{D}_{T}$
    by knowledge of the $\mathbb{D}_{S}$ and $\mathbb{T}_{T}$. Furthermore, the transductive
    DTL environment may be further classified into two categories depending on different
    conditions between the source and destination domains:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统 ML 相比（可作为 DTL 比较的参考），并且考虑到TDs $\mathbb{D}_{T}$与SDs $\mathbb{D}_{S}$不同。SD具有一个标记数据集（$X_{S}$带有$Y_{S}$的标记），而TD没有标记数据集，源任务和目标任务相同（表
    [2](#S2.T2 "表 2 ‣ 2.1 现有 DTL 技术的分类 ‣ 2 语音识别的 DTL 技术概述 ‣ 使用先进深度学习方法的自动语音识别：综述")）。归纳
    DTL 的目标是通过$\mathbb{D}_{S}$和$\mathbb{T}_{T}$中的知识构建目标预测函数$\mathbb{F}_{T}$。此外，归纳
    DTL 环境可以根据源领域和目标领域之间的不同条件进一步分类为两类：
- en: '(a) Domain adaptation (DA): The feature spaces across domains, $\chi_{S}$ and
    $\chi_{T}$, are the identical, but the marginal probability distributions of the
    input dataset are not, $P(Y_{S}/X_{S})\neq P(Y_{T}/X_{T})$. For example, an assessment
    may be done on the topic of the resort in the $\mathbb{D}_{S}$ and it will be
    used to train a model for restaurants in the $\mathbb{D}_{T}$. DA is mostly effective
    when the $\mathbb{T}_{T}$ has a distinct distribution or there is a scarcity of
    labeled data.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 领域适应（DA）：不同领域间的特征空间$\chi_{S}$和$\chi_{T}$是相同的，但输入数据集的边际概率分布不同，即$P(Y_{S}/X_{S})\neq
    P(Y_{T}/X_{T})$。例如，可能在$\mathbb{D}_{S}$中对度假村的主题进行评估，并用于训练$\mathbb{D}_{T}$中餐馆的模型。当$\mathbb{T}_{T}$具有明显不同的分布或标记数据稀缺时，DA通常最为有效。
- en: '(b) Cross-modality DTL: Also known as cross-lingual DTL in the spoken language
    field, most DTL methods, more or less, a connection in feature spaces or label
    spaces is required between $\mathbb{D}_{S}$ and $\mathbb{D}_{T}$. In other words,
    DTL can only occur when the source and destination data are both in the same modality,
    like video, speech, or text. cross-lingual DTL, in contrast to all other DTL approaches,
    is one of the most complicated issues in DTL. It is assumed that the feature spaces
    of the source and destination domains are completely distinct ($\chi_{S}\neq\chi_{T}$),
    as in speech-to-image, image-to-text, and text-to-speech. Furthermore, the label
    spaces of source $Y_{S}$ and destination $Y_{S}$ domains might differ ($Y_{S}\neq
    Y_{T}$).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 跨模态 DTL：也称为口语领域的跨语言 DTL，大多数 DTL 方法或多或少需要在$\mathbb{D}_{S}$和$\mathbb{D}_{T}$之间建立特征空间或标签空间的连接。换句话说，DTL
    仅在源数据和目标数据处于相同模态时发生，例如视频、语音或文本。与所有其他 DTL 方法相比，跨语言 DTL 是 DTL 中最复杂的问题之一。假设源和目标领域的特征空间完全不同（$\chi_{S}\neq\chi_{T}$），如语音到图像、图像到文本和文本到语音。此外，源领域$Y_{S}$和目标领域$Y_{T}$的标签空间可能不同（$Y_{S}\neq
    Y_{T}$）。
- en: '(c) Unsupervised DTL: Intends to enhance the learning of the target predictive
    function $\mathbb{F}_{T}$ in $\mathbb{D}_{T}$ using the knowledge in $\mathbb{D}_{S}$
    and $\mathbb{T}_{S}$, where $\mathbb{T}_{S}$ different from $\mathbb{T}_{T}$ but
    related, and $Y_{S}$ and $Y_{T}$ are not visible, given a SD $\mathbb{D}_{S}$
    with a learning task $\mathbb{T}_{S}$, a TD $\mathbb{D}_{T}$ and a matching learning
    task $\mathbb{T}_{T}$ ($\mathbb{D}_{S}$ different from $\mathbb{D}_{T}$, but related).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 无监督 DTL：旨在利用$\mathbb{D}_{S}$和$\mathbb{T}_{S}$中的知识来增强目标预测函数$\mathbb{F}_{T}$在$\mathbb{D}_{T}$中的学习，其中$\mathbb{T}_{S}$与$\mathbb{T}_{T}$不同但相关，且$Y_{S}$和$Y_{T}$不可见，给定一个包含学习任务$\mathbb{T}_{S}$的源数据集$\mathbb{D}_{S}$、一个目标数据集$\mathbb{D}_{T}$和一个匹配的学习任务$\mathbb{T}_{T}$（$\mathbb{D}_{S}$与$\mathbb{D}_{T}$不同但相关）。
- en: 2.1.2 Adversarial DTL
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 对抗性 DTL
- en: In contrast to the methods described above for DTL, adversarial learning [wang2020transfer]
    aids in the learning of more transferable and discriminative representations.
    The work in [ganin2016domain], was the first that introduced the domain-adversarial
    neural network (DANN). Instead of using a predefined distance function like maximum
    mean discrepancy (MMD), the core idea is to use a domain-adversarial loss in the
    network. This has greatly aided the network’s ability to learn more discriminative
    data. Many studies have used domain-adversarial training as a result of DANN’s
    idea [bousmalis2016domain, chen2019joint, long2017deep, zhang2018collaborative].
    All of the previous work ignores the different effects of marginal and conditional
    distributions in adversarial TL, whereas in [wang2020transfer], the proposed scheme,
    named dynamic distribution alignment (DDA), can dynamically evaluate the importance
    of each distribution.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述DTL方法相比，对抗学习[wang2020transfer]有助于学习更具可转移性和区分性的表示。在[ganin2016domain]的工作中，首次引入了领域对抗神经网络（DANN）。核心思想是在网络中使用领域对抗损失，而不是使用预定义的距离函数，如最大均值差异（MMD）。这极大地帮助网络学习更有区分性的数据。许多研究都使用了领域对抗训练，导致了DANN思想的出现[bousmalis2016domain,
    chen2019joint, long2017deep, zhang2018collaborative]。之前的所有工作都忽略了对抗TL中边际和条件分布的不同影响，而在[wang2020transfer]中，提出的方案，名为动态分布对齐（DDA），可以动态评估每个分布的重要性。
- en: 3 Background
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 背景
- en: 3.1 Acoustic and language models
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 声学模型和语言模型
- en: 'The [AM](#Sx1.2.2.2) is in charge of capturing the sound characteristics of
    different phonetic units. This involves generating statistical measures for characteristic
    vector sequences from the audio waveform. Various techniques, such as [linear
    predictive coding](#Sx1.25.25.25) ([LPC](#Sx1.25.25.25)), Cepstral analysis, filter-bank
    analysis, [Mel-frequency cepstral coefficients](#Sx1.28.28.28), wavelet analysis,
    and others, can be used to extract these features [[37](#bib.bib37)]. In the processing
    stage, a decoder (search algorithm) uses the acoustic lexicon and [LM](#Sx1.24.24.24)
    to create the hypothesized word or phoneme. You can see the overall process illustrated
    in Figure [4](#S3.F4 "Figure 4 ‣ 3.1 Acoustic and language models ‣ 3 Background
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey").'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[AM](#Sx1.2.2.2)负责捕捉不同语音单元的声音特征。这涉及从音频波形中生成特征向量序列的统计量。可以使用各种技术，例如[线性预测编码](#Sx1.25.25.25)
    ([LPC](#Sx1.25.25.25))，倒谱分析，滤波器组分析，[梅尔频率倒谱系数](#Sx1.28.28.28)，小波分析和其他技术来提取这些特征[[37](#bib.bib37)]。在处理阶段，一个解码器（搜索算法）使用声学词典和[LM](#Sx1.24.24.24)来创建假设的单词或音素。可以在图[4](#S3.F4
    "图4 ‣ 3.1声学和语言模型 ‣ 3背景 ‣ 使用高级深度学习方法的自动语音识别：一项调查")中看到整个过程的图解。'
- en: \Acp
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: \Acp
- en: 'LM provide probabilities of sequences of words, crucial for [ASR](#Sx1.4.4.4)
    systems to predict the likelihood of subsequent words in a sentence [[38](#bib.bib38)].
    A domain-specific LM is trained on text data from the target domain to capture
    its unique vocabulary and grammatical structures [[39](#bib.bib39)]. For n-gram
    models, this involves calculating the conditional probability of a word given
    the previous $n-1$ words [[40](#bib.bib40)]:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: LM提供单词序列的概率，这对[ASR](#Sx1.4.4.4)系统来预测句子中后续单词的可能性至关重要[[38](#bib.bib38)]。针对特定领域的LM是在目标领域的文本数据上训练的，以捕捉其独特的词汇和语法结构[[39](#bib.bib39)]。对于n-gram模型，这涉及计算给定前$n-1$个单词的条件概率[[40](#bib.bib40)]：
- en: '|  | $P(w_{n}&#124;w_{n-1},w_{n-2},\ldots,w_{n-(n-1)})=\frac{C(w_{n-(n-1)},\ldots,w_{n})}{C(w_{n-(n-1)},\ldots,w_{n-1})}$
    |  | (2) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(w_{n}&#124;w_{n-1},w_{n-2},\ldots,w_{n-(n-1)})=\frac{C(w_{n-(n-1)},\ldots,w_{n})}{C(w_{n-(n-1)},\ldots,w_{n-1})}$
    |  | (2) |'
- en: In the context of [ASR](#Sx1.4.4.4), the [LM](#Sx1.24.24.24) complements the
    [AM](#Sx1.2.2.2) by providing linguistic context. The combined probability from
    the [AM](#Sx1.2.2.2) and the [LM](#Sx1.24.24.24) helps in determining the most
    likely transcription for a given audio input during the decoding process. The
    frequently utilized [LM](#Sx1.24.24.24) in [ASR](#Sx1.4.4.4) systems is the backoff
    n-gram model.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ASR](#Sx1.4.4.4)的背景下，[LM](#Sx1.24.24.24)通过提供语言环境来补充[AM](#Sx1.2.2.2)。[AM](#Sx1.2.2.2)和[LM](#Sx1.24.24.24)的综合概率有助于在解码过程中确定给定音频输入的最可能转录。在[ASR](#Sx1.4.4.4)系统中经常使用的[LM](#Sx1.24.24.24)是回退n-gram模型。
- en: '![Refer to caption](img/c7b78cedf3451c18a4ac47c0bc1b5439.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c7b78cedf3451c18a4ac47c0bc1b5439.png)'
- en: 'Figure 4: Diagram illustrating the end-to-end framework for [ASR](#Sx1.4.4.4).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：说明端到端的[ASR](#Sx1.4.4.4)框架。
- en: 3.2 Evaluation criteria in ASR
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 ASR中的评估标准
- en: 'To assess the effectiveness and suitability of [ASR](#Sx1.4.4.4) techniques,
    researchers have employed diverse methods. Some of these encompass well-established
    [DL](#Sx1.14.14.14) metrics, including accuracy, F1-score, recall (sensitivity
    or [true positive rate](#Sx1.50.50.50) ([TPR](#Sx1.50.50.50))), precision (also
    known as positive predictive value), and specificity (commonly referred to as
    [true negative rate](#Sx1.49.49.49) ([TNR](#Sx1.49.49.49))) [[41](#bib.bib41)].
    These metrics serve as crucial evaluation criteria for experimental outcomes,
    as evidenced in studies such as [[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)].
    Additionally, there are [ASR](#Sx1.4.4.4)-specific metrics, which are detailed
    in Table [3](#S3.T3 "Table 3 ‣ 3.2 Evaluation criteria in ASR ‣ 3 Background ‣
    Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey").'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估[ASR](#Sx1.4.4.4)技术的有效性和适用性，研究人员采用了多种方法。其中一些包括成熟的[DL](#Sx1.14.14.14)指标，如准确率、F1-score、召回率（灵敏度或[真正率](#Sx1.50.50.50)
    ([TPR](#Sx1.50.50.50)))、精确度（也称为阳性预测值）和特异性（通常称为[真阴性率](#Sx1.49.49.49) ([TNR](#Sx1.49.49.49)))
    [[41](#bib.bib41)]。这些指标作为实验结果的关键评估标准，在[[42](#bib.bib42)、[43](#bib.bib43)、[44](#bib.bib44)]等研究中得到了证明。此外，还有[ASR](#Sx1.4.4.4)特定的指标，这些指标在表[3](#S3.T3
    "Table 3 ‣ 3.2 Evaluation criteria in ASR ‣ 3 Background ‣ Automatic Speech Recognition
    using Advanced Deep Learning Approaches: A survey")中有详细说明。'
- en: '|'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 3: An overview of the metrics employed for evaluating [ASR](#Sx1.4.4.4)
    methods.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：评估[ASR](#Sx1.4.4.4)方法所使用指标的概览。
- en: '|  |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '| Metric | Formula | Description |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 公式 | 描述 |'
- en: '| [WER](#Sx1.52.52.52) | $\displaystyle\mathrm{\frac{S+D+I}{N}=\frac{S+D+I}{H+S+D}}.$
    | The [word error rate](#Sx1.52.52.52) ([WER](#Sx1.52.52.52)) serves as a frequently
    utilized metric to assess the performance of Automatic Speech Recognition ([ASR](#Sx1.4.4.4)).
    It is computed by determining the ratio of incorrectly recognized words to the
    overall number of processed words [[45](#bib.bib45), [17](#bib.bib17), [46](#bib.bib46)].
    In the given context, $\mathrm{I,D,S,H}$, and $\mathrm{N}$ denote the quantities
    of insertions, deletions, substitutions, hits, and input words, respectively.
    Instead of [WER](#Sx1.52.52.52), the [character error rate](#Sx1.8.8.8) ([CER](#Sx1.8.8.8))
    has been employed, while adhering to the same evaluation principle. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| [WER](#Sx1.52.52.52) | $\displaystyle\mathrm{\frac{S+D+I}{N}=\frac{S+D+I}{H+S+D}}.$
    | [词错误率](#Sx1.52.52.52) ([WER](#Sx1.52.52.52))作为一种常用的指标来评估自动语音识别([ASR](#Sx1.4.4.4))的性能。它通过确定错误识别的词与处理的总词数之间的比例来计算[[45](#bib.bib45)、[17](#bib.bib17)、[46](#bib.bib46)]。在给定的背景下，$\mathrm{I,D,S,H}$
    和 $\mathrm{N}$ 表示插入、删除、替换、命中和输入词的数量。与[WER](#Sx1.52.52.52)相反，[字符错误率](#Sx1.8.8.8)
    ([CER](#Sx1.8.8.8))被采用，同时遵循相同的评估原则。 |'
- en: '| PESQ and MOS-LQO | $\displaystyle\mathrm{MOS-LQO}=0.999+\frac{4.999-0.999}{1+e^{-1.4945.PESQ+4.6607}}$
    | [PESQ](#Sx1.33.33.33) serves as an objective technique for evaluating the perceived
    quality of speech [[47](#bib.bib47)]. The assessment involves assigning numerical
    scores within the range of -0.5 to 4.5\. Additionally, a correlation can be established
    between MOS and PESQ scores, giving rise to a novel evaluation metric termed the
    mean opinion score-listening quality objective (MOS-LQO), also identified as PESQ
    Rec.862.1\. [[22](#bib.bib22)] |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| PESQ 和 MOS-LQO | $\displaystyle\mathrm{MOS-LQO}=0.999+\frac{4.999-0.999}{1+e^{-1.4945.PESQ+4.6607}}$
    | [PESQ](#Sx1.33.33.33)作为评估语音感知质量的客观技术[[47](#bib.bib47)]。评估过程中涉及将数值评分分配在-0.5到4.5的范围内。此外，可以在MOS和PESQ评分之间建立相关性，从而产生一种新的评估指标，称为均值意见评分-听觉质量客观（MOS-LQO），也被称为PESQ
    Rec.862.1。[[22](#bib.bib22)] |'
- en: '| [RTF](#Sx1.37.37.37) | $\displaystyle\mathrm{RTF=\frac{\text{Total Processing
    Time}}{\text{Total Duration}}}$ | \Ac RTF serves as a standard metric to assess
    the processing time cost of an [ASR](#Sx1.4.4.4) system. It represents the average
    processing time required for one second of speech |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| [RTF](#Sx1.37.37.37) | $\displaystyle\mathrm{RTF=\frac{\text{Total Processing
    Time}}{\text{Total Duration}}}$ | \Ac RTF作为评估[ASR](#Sx1.4.4.4)系统处理时间成本的标准指标。它表示处理一秒语音所需的平均处理时间。
    |'
- en: '| [RER](#Sx1.34.34.34) | $\displaystyle\frac{(E_{\text{baseline}}-E_{\text{proposed}})}{E_{\text{baseline}}}\times
    100\%$ | The [relative error rate](#Sx1.34.34.34) ([RER](#Sx1.34.34.34)) expresses
    the percentage error rate achieved by the proposed DL model compared to the baseline.
    $E_{\text{baseline}}$ is the error rate of the baseline model. $E_{\text{proposed}}$
    is the error rate of the proposed model or method. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| [RER](#Sx1.34.34.34) | $\displaystyle\frac{(E_{\text{baseline}}-E_{\text{proposed}})}{E_{\text{baseline}}}\times
    100\%$ | [相对误差率](#Sx1.34.34.34) ([RER](#Sx1.34.34.34)) 表达了提议的深度学习模型相较于基线的百分比误差率。
    $E_{\text{baseline}}$ 是基线模型的误差率。 $E_{\text{proposed}}$ 是提议模型或方法的误差率。 |'
- en: '| D | $\displaystyle\frac{\sum_{i=1}^{n}\Big{(}1-\frac{\sum_{j=1}^{n}a_{ij}\cdot&#124;i-j&#124;}{\max(&#124;i-1&#124;,&#124;i-2&#124;,\ldots,&#124;i-n&#124;)}\Big{)}}{n}\hskip
    28.45274pt$ | Diagonal centrality of an attention matrix (D) is defined as the
    mean value across the centrality of all its rows. where $j$ represents the index
    of each column, $n$ signifies the length of the input sequence, $a_{ij}$ denotes
    the attention weight between the $i$-th and $j$-th elements of the input sequence,
    and $&#124;i-j&#124;$ signifies the distance between the $i$-th and $j$-th elements
    of the input sequence [[48](#bib.bib48)]. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| D | $\displaystyle\frac{\sum_{i=1}^{n}\Big{(}1-\frac{\sum_{j=1}^{n}a_{ij}\cdot&#124;i-j&#124;}{\max(&#124;i-1&#124;,&#124;i-2&#124;,\ldots,&#124;i-n&#124;)}\Big{)}}{n}\hskip
    28.45274pt$ | 注意力矩阵的对角中心性（D）定义为所有行中心性的均值。 其中 $j$ 代表每列的索引，$n$ 表示输入序列的长度，$a_{ij}$
    表示输入序列中第 $i$ 个和第 $j$ 个元素之间的注意力权重，$&#124;i-j&#124;$ 表示输入序列中第 $i$ 个和第 $j$ 个元素之间的距离
    [[48](#bib.bib48)]。 |'
- en: 3.3 ASR datasets
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 ASR 数据集
- en: 'Various datasets have been employed in the literature for diverse [ASR](#Sx1.4.4.4)
    tasks. Table [4](#S3.T4 "Table 4 ‣ 3.3 ASR datasets ‣ 3 Background ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey") presents
    a selection of datasets utilized for DTL-based [ASR](#Sx1.4.4.4) applications,
    along with their respective characteristics. It is important to note that the
    table primarily includes publicly accessible repositories. Furthermore, it is
    worth mentioning that certain datasets have undergone multiple updates and improvements
    over time, leading to their enhanced development.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中已使用各种数据集来进行不同的[ASR](#Sx1.4.4.4)任务。 表[4](#S3.T4 "表 4 ‣ 3.3 ASR 数据集 ‣ 3 背景
    ‣ 使用先进深度学习方法的自动语音识别：调查") 展示了用于DTL基础的[ASR](#Sx1.4.4.4)应用的数据集选择及其各自特征。 需要注意的是，该表主要包括公开访问的存储库。此外，还值得提及的是，某些数据集随着时间的推移经历了多次更新和改进，从而提升了其发展水平。
- en: 'Table 4: List of publicly available datasets used for advanced DL-based [ASR](#Sx1.4.4.4)
    applications'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：用于高级深度学习（DL）基础的[自动语音识别（ASR）](#Sx1.4.4.4)应用的公开数据集列表
- en: . Dataset Used by Default ASR task Characteristics LibriSpeech [[49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51)] Train and assess systems for recognizing speech.
    The collection consists of 1000 hours of speech recorded at a 16 kHz sampling
    rate, sourced from audiobooks included in the LibriVox project. DCASE [[52](#bib.bib52),
    [53](#bib.bib53)] Identifying acoustic environments and detecting sound occurrences.
    Comprise 8 coarse-level and 23 fine-level urban sound categories, collected in
    New York City in 2020 using 50 acoustic sensors. WSJ [[48](#bib.bib48)] Acoustic
    scene and sound event corpus Comprises an extensive 81 hours of meticulously curated
    read speech training data. SWBD [[48](#bib.bib48)] Conversational telephone speech
    corpus Is a comprehensive collection, boasting a substantial 260 hours of conversational
    telephone speech training data. AISHELL [[54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56)]
    Chinese Mandarin speech corpus 400 participants from diverse Chinese accent regions
    recorded in a quiet indoor space using high-fidelity microphones, later downsampled
    to 16kHz. CHIME3 [[57](#bib.bib57)] SR for distant microphone in real-world settings.
    Includes around 342 hours of English speech with noisy transcripts and 50 hours
    of noisy environment recordings. Google-SC [[57](#bib.bib57)] Speech commands
    with a restricted range of words. The dataset contains 105,829 one-second utterances
    of 35 words categorized by frequency. Each utterance is stored as a one-second
    WAVE format file with 16-bit single-channel at 16KHz rate. It involves 2,618 speakers
    AURORA4 [[57](#bib.bib57)] Compare front-ends for large vocabulary recognition
    performance. Aurora-4 is a speech recognition dataset derived from the WSJ corpus,
    offering four conditions (Clean, channel, noisy, channel+noisy) with two microphone
    types and six noise types, totaling 4,620 utterances per set. Car-env [[57](#bib.bib57)]
    Vehicle environment sound Is a dataset from Korea that spans 100 hours of recordings
    in a vehicle. It comprises brief commands, with an average of 1.6 words per command.
    HKUST [[50](#bib.bib50), [56](#bib.bib56)] Classify Mandarin speech into standard
    and accented types. Comprises roughly 149 hours of telephone conversations in
    Mandarin. AudioSet [[51](#bib.bib51)] Audio event recognition Includes 1,789,621
    segments of 10 seconds each (equivalent to 4,971 hours). It consists of at least
    100 instances clustered into 632 audio classes, with only 485 audio event categories
    clearly identified. AWIC-19 [[58](#bib.bib58)] Arabic words recognition It comprises
    770 recordings featuring isolated Arabic words. TED2 [[59](#bib.bib59)] English
    corpus for ASR The dataset was first made available in May 2012, for training,
    it comprising 118 hours, 4 minutes, and 48 seconds of training data from 666 speakers,
    containing approximately 1.7 million words.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 默认使用的数据集 ASR 任务 特点 LibriSpeech [[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51)]
    训练和评估语音识别系统。该集合包括 1000 小时的语音录音，采样率为 16 kHz，来源于 LibriVox 项目中的有声读物。DCASE [[52](#bib.bib52),
    [53](#bib.bib53)] 识别声学环境和检测声音出现。包括 8 个粗略级别和 23 个精细级别的城市声音类别，数据收集于 2020 年的纽约市，使用
    50 个声学传感器。WSJ [[48](#bib.bib48)] 声学场景和声音事件语料库 包括 81 小时精心整理的朗读语音训练数据。SWBD [[48](#bib.bib48)]
    对话电话语音语料库 是一个综合性的集合，包含 260 小时的对话电话语音训练数据。AISHELL [[54](#bib.bib54), [55](#bib.bib55),
    [56](#bib.bib56)] 中文普通话语音语料库 400 名来自不同中文口音地区的参与者，在安静的室内空间使用高保真麦克风录制，后期降采样至 16kHz。CHIME3
    [[57](#bib.bib57)] 用于现实世界环境中的远程麦克风的语音识别。包括约 342 小时的英语语音带有噪声的转录和 50 小时的噪声环境录音。Google-SC
    [[57](#bib.bib57)] 具有有限词汇范围的语音命令。该数据集包含 105,829 条 35 个词汇的 1 秒语音记录，按频率分类。每条语音记录以
    1 秒的 WAVE 格式文件存储，具有 16 位单声道和 16KHz 采样率。涉及 2,618 名说话者。AURORA4 [[57](#bib.bib57)]
    比较前端在大词汇量识别性能上的表现。Aurora-4 是从 WSJ 语料库衍生的语音识别数据集，提供四种条件（清晰、通道、噪声、通道+噪声），有两种麦克风类型和六种噪声类型，每套数据总计
    4,620 条语音记录。Car-env [[57](#bib.bib57)] 车辆环境声音 是来自韩国的数据集，涵盖 100 小时的车辆录音。包括简短的命令，每条命令平均
    1.6 个词。HKUST [[50](#bib.bib50), [56](#bib.bib56)] 将普通话语音分类为标准和口音类型。包括大约 149 小时的普通话电话对话。AudioSet
    [[51](#bib.bib51)] 音频事件识别 包括 1,789,621 段每段 10 秒的音频（总计 4,971 小时）。包括至少 100 个实例，分为
    632 个音频类别，但仅有 485 个音频事件类别被明确识别。AWIC-19 [[58](#bib.bib58)] 阿拉伯语词汇识别 包括 770 条录音，包含孤立的阿拉伯语单词。TED2
    [[59](#bib.bib59)] 英语 ASR 语料库 该数据集首次提供于 2012 年 5 月，用于训练，包括 118 小时 4 分钟 48 秒的训练数据，来自
    666 名说话者，包含约 170 万个单词。
- en: 4 Advanced ASR methods and applications
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 高级 ASR 方法和应用
- en: Traditional statistical [LMs](#Sx1.24.24.24), such as backoff n-gram [LMs](#Sx1.24.24.24),
    have been widely used due to their simplicity and reliability. However, bidirectional
    encoder representations from transformers (BERT), which utilize attention models,
    have shown better contextual understanding compared to single-direction LMs, as
    demonstrated in the work of Devlin et al. [[60](#bib.bib60)].
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的统计[语言模型](#Sx1.24.24.24)，如回退n-gram[语言模型](#Sx1.24.24.24)，因其简单性和可靠性被广泛使用。然而，利用注意力模型的双向编码器表示（BERT）相比于单向语言模型表现出了更好的上下文理解，如Devlin等人的研究所示[[60](#bib.bib60)]。
- en: In terms of [AM](#Sx1.2.2.2), deep learning-based models like the deep neural
    network-hidden Markov model (DNN-HMM) and the [connectionist temporal classification](#Sx1.11.11.11)
    ([CTC](#Sx1.11.11.11)) have made significant advancements. DNN-HMM models have
    been extensively studied in [ASR](#Sx1.4.4.4) research, while [CTC](#Sx1.11.11.11)
    is an end-to-end training method that does not require pre-alignment and only
    needs input and output sequences. The [S2S](#Sx1.39.39.39) model has also been
    successful in solving [ASR](#Sx1.4.4.4) tasks without using an [LM](#Sx1.24.24.24)
    or pronunciation dictionary, as described in Chiu et al. [[61](#bib.bib61)].
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在[声学模型](#Sx1.2.2.2)方面，基于深度学习的模型如深度神经网络-隐马尔可夫模型（DNN-HMM）和[连接时序分类](#Sx1.11.11.11)（[CTC](#Sx1.11.11.11)）取得了显著进展。DNN-HMM模型在[自动语音识别](#Sx1.4.4.4)研究中得到了广泛研究，而[CTC](#Sx1.11.11.11)是一种端到端的训练方法，不需要预对齐，只需输入和输出序列。[S2S](#Sx1.39.39.39)模型在解决[自动语音识别](#Sx1.4.4.4)任务时也取得了成功，且不使用[语言模型](#Sx1.24.24.24)或发音词典，如Chiu等人所述[[61](#bib.bib61)]。
- en: '[ASR](#Sx1.4.4.4) systems often face performance degradation in certain situations
    due to the "one-model-fits-all" approach. Additionally, the lack of diverse and
    sufficient training data affects [AM](#Sx1.2.2.2) performance. To overcome these
    constraints and improve the resilience and flexibility of [ASR](#Sx1.4.4.4) systems,
    advanced DL methodologies such as [DTL](#Sx1.16.16.16) and it sub-field [domain
    adaptation](#Sx1.13.13.13) ([DA](#Sx1.13.13.13)), [DRL](#Sx1.15.15.15), and [FL](#Sx1.20.20.20)
    have surfaced. These innovative methodologies collectively address issues concerning
    knowledge transfer, model generalization, and training effectiveness, offering
    remedies that expand upon the capabilities of traditional DL models within the
    [ASR](#Sx1.4.4.4) sphere. Thus, many research studies have focused on enhancing
    existing [ASR](#Sx1.4.4.4) systems by applying the aforementioned algorithms.
    Figure [5](#S4.F5 "Figure 5 ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey") provides
    an overview of the current [SOTA](#Sx1.44.44.44) advanced DL-based [ASR](#Sx1.4.4.4)
    and its most useful related schemes in both [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[自动语音识别](#Sx1.4.4.4)系统在某些情况下常面临性能下降，这是由于“一种模型适用所有”方法所致。此外，缺乏多样化和充足的训练数据会影响[声学模型](#Sx1.2.2.2)的表现。为了克服这些限制，提高[自动语音识别](#Sx1.4.4.4)系统的弹性和灵活性，先进的深度学习方法如[深度迁移学习](#Sx1.16.16.16)及其子领域[领域适应](#Sx1.13.13.13)（[DA](#Sx1.13.13.13)）、[深度强化学习](#Sx1.15.15.15)和[联邦学习](#Sx1.20.20.20)已出现。这些创新方法集体解决了关于知识转移、模型泛化和训练效果的问题，提供了扩展传统深度学习模型能力的补救措施。因此，许多研究集中在通过应用上述算法来增强现有的[自动语音识别](#Sx1.4.4.4)系统。图[5](#S4.F5
    "图5 ‣ 4 高级自动语音识别方法及应用 ‣ 基于先进深度学习方法的自动语音识别：综述")提供了当前[最先进技术](#Sx1.44.44.44)深度学习驱动的[自动语音识别](#Sx1.4.4.4)及其在[声学模型](#Sx1.2.2.2)和[语言模型](#Sx1.24.24.24)中的最有用的相关方案的概述。'
- en: '![Refer to caption](img/715294b829e105e4ec94b5782bd80b8f.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/715294b829e105e4ec94b5782bd80b8f.png)'
- en: 'Figure 5: Overview of advanced DL-driven [ASR](#Sx1.4.4.4) algorithms and their
    commonly utilized models.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：先进的深度学习驱动的[自动语音识别](#Sx1.4.4.4)算法及其常用模型概述。
- en: 4.1 Transformer-based ASR
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于Transformer的自动语音识别
- en: 'The Transformer stands as a prominent deep learning model extensively employed
    across diverse domains, including [NLP](#Sx1.31.31.31), [computer vision](#Sx1.12.12.12)
    ([CV](#Sx1.12.12.12)), and speech processing. Originally conceived for machine
    translation as a [S2S](#Sx1.39.39.39) model, it has evolved to find applications
    in various fields. The Transformer heavily relies on the self-attention mechanism,
    enabling it to capture extensive dependencies in input sequences. The standard
    Transformer model incorporates the query–key–value (QKV) attention mechanism.
    In this setup, given matrix representations of queries $\mathbf{Q}\in\mathbb{R}^{N\times
    D_{k}}$, keys $\mathbf{K}\in\mathbb{R}^{M\times D_{k}}$, and values $\mathbf{V}\in\mathbb{R}^{M\times
    D_{v}}$, the scaled dot-product attention is defined as:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer作为一个杰出的深度学习模型，被广泛应用在各个领域，包括[NLP](#Sx1.31.31.31)、[计算机视觉](#Sx1.12.12.12)（CV）、语音处理。最初是作为[S2S](#Sx1.39.39.39)模型用于机器翻译，它已经发展到在各个领域都有应用。Transformer极大依赖于自注意力机制，使其能够捕捉输入序列中的广泛依赖。标准的Transformer模型包含了查询-键-值（QKV）注意力机制。在这种设置下，给定查询的矩阵表示$\mathbf{Q}\in\mathbb{R}^{N\times
    D_{k}}$，键的$\mathbf{K}\in\mathbb{R}^{M\times D_{k}}$，以及值的$\mathbf{V}\in\mathbb{R}^{M\times
    D_{v}}$，缩放的点积注意力定义如下：
- en: '|  | $\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{D_{k}}}\right)\mathbf{V}$
    |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{注意力}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{D_{k}}}\right)\mathbf{V}$
    |  |'
- en: 'Here, $N$ and $M$ represent the lengths of queries and keys (or values), and
    $D_{k}$ and $D_{v}$ denote the dimensions of keys (or queries) and values. The
    softmax operation is applied row-wise to the matrix $\mathbf{A}$. Within the Transformer
    architecture, three attention mechanisms exist based on the source of queries
    and key–value pairs:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$N$和$M$表示查询和键（或值）的长度，而$D_{k}$和$D_{v}$表示键（或查询）和值的维度。softmax操作逐行应用于矩阵$\mathbf{A}$。在Transformer架构中，基于查询来源和键-值对的三种注意力机制存在：
- en: •
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Self-attention: In the Transformer encoder, queries $\mathbf{Q}$, keys $\mathbf{K}$,
    and values $\mathbf{V}$ all equal the outputs of the previous layer, denoted as
    $\mathbf{X}$ in Eq. (2).'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自注意力：在Transformer编码器中，查询$\mathbf{Q}$、键$\mathbf{K}$和值$\mathbf{V}$都等于前一层的输出，用Eq.
    (2)表示为$\mathbf{X}$。
- en: •
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Masked Self-attention: In the Transformer decoder, self-attention is constrained,
    allowing queries at each position to attend only to key–value pairs up to and
    including that position. This is accomplished by implementing a mask function,
    before normalization, on the attention matrix $\hat{\mathbf{A}}=\exp\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{D_{k}}}\right)$,
    where illegal positions are masked out by setting $\hat{A}_{ij}=-\infty$ if $i<j$.
    This type of self-attention is often referred to as autoregressive or causal attention.'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 掩码自注意力：在Transformer解码器中，自注意力受到限制，允许每个位置的查询只能关注到该位置及之前的键-值对。这是通过在注意力矩阵$\hat{\mathbf{A}}=\exp\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{D_{k}}}\right)$进行规范化之前，实现一个掩码函数来实现的，其中非法位置通过设定$\hat{A}_{ij}=-\infty$（如果$i<j$）来屏蔽掉。这种自注意力通常称为自回归或因果关注。
- en: •
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cross-attention: In cross-attention, queries originate from the results of
    the preceding (decoder) layer, while keys and values stem from the outputs of
    the encoder.'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 交叉注意力：在交叉关注中，查询源自前一层（解码器）的结果，而键和值源自编码器的输出。
- en: Numerous studies in the [ASR](#Sx1.4.4.4) field have introduced transformer-based
    approaches, encompassing both the acoustic and language domains. In the subsequent
    subsections, we delve into a comprehensive review and detailed analysis of several
    cutting-edge techniques within each of these categories.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ASR](#Sx1.4.4.4)领域的许多研究都引入了基于Transformer的方法，包括声学和语言领域。在接下来的小节中，我们将深入探讨每个类别中几种尖端技术的全面审查和详细分析。
- en: 'Table 5: Summary of some proposed work in transformer-based ASR. The symbol
    ($\uparrow$) denotes result increase, whereas ($\downarrow$) signifies result
    decrease. In cases where multiple scenarios are examined, only the top-performing
    outcome is mentioned.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：基于变压器的ASR中一些提出的工作总结。符号($\uparrow$)表示结果增加，而($\downarrow$)表示结果减少。在多种情景下进行了研究的情况下，只提及了最佳表现的结果。
- en: '| Ref. | Based on | Speech recognition task | Transformer | AM/LM | Result
    with metric |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 基于 | 语音识别任务 | 变压器 | AM/LM | 度量标准结果 |'
- en: '| [[55](#bib.bib55)] | CNN | Solve the problem of code-switching | Multi-head
    attention | LM | RER= 10.2% |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| [[55](#bib.bib55)] | CNN | 解决了混合代码问题 | 多头注意力 | LM | RER= 10.2% |'
- en: '| [[56](#bib.bib56)] | VGGnet | Compress ASR parameters and speeds up the inference
    time | Low-rank multi-head attention | AM | CER= 13.09% |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| [[56](#bib.bib56)] | VGGnet | 压缩ASR参数并加快推理时间 | 低秩多头注意力 | AM | CER= 13.09%
    |'
- en: '| [[57](#bib.bib57)] | DNN-HMM | Improve ASR | Attention | AM | RER= 4.7%$\downarrow$
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| [[57](#bib.bib57)] | DNN-HMM | 改进ASR | 注意力 | AM | RER= 4.7%$\downarrow$ |'
- en: '| [[62](#bib.bib62)] | Emformer | Large scale [ASR](#Sx1.4.4.4) | Attention
    | AM | RERR= 26% |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| [[62](#bib.bib62)] | Emformer | 大规模[ASR](#Sx1.4.4.4) | 注意力 | AM | RERR= 26%
    |'
- en: '| [[63](#bib.bib63)] | TRUNet | Sound source separation | TNet | AM | PESQ=
    0.22$\uparrow$ |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| [[63](#bib.bib63)] | TRUNet | 声音源分离 | TNet | AM | PESQ= 0.22$\uparrow$ |'
- en: '| [[64](#bib.bib64)] | [MHSA](#Sx1.29.29.29) | Improve speech/ASR | D²Net |
    AM | PESQ= 0.96$\uparrow$ |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| [[64](#bib.bib64)] | [MHSA](#Sx1.29.29.29) | 提高语音/ASR | D²Net | AM | PESQ=
    0.96$\uparrow$ |'
- en: '| [[58](#bib.bib58)] | HMM | Improve ASR | Acoustic Encoder | AM | Acc= 96%
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| [[58](#bib.bib58)] | HMM | 改进ASR | 声学编码器 | AM | Acc= 96% |'
- en: '| [[65](#bib.bib65)] | RNN-T | Acoustic re-scoring scenario | Transformer-
    Transducer | AM | Acc= 8%$\uparrow$ |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| [[65](#bib.bib65)] | RNN-T | 声学重新评分场景 | Transformer-Transducer | AM | Acc=
    8%$\uparrow$ |'
- en: '| [[66](#bib.bib66)] | CTC | ASR, ST, Acoustic [ED](#Sx1.17.17.17) | All-in-one
    | AM | WER=0.3%$\uparrow$ |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| [[66](#bib.bib66)] | CTC | ASR、ST、声学[ED](#Sx1.17.17.17) | 一体化 | AM | WER=0.3%$\uparrow$
    |'
- en: '| [[67](#bib.bib67)] | CNN | Speech recognition with low latency, reduced frame
    rate, and streamability. | Transformer-Transducer | AM | WER= 3.6% |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| [[67](#bib.bib67)] | CNN | 低延迟、减少帧率且支持流式传输的语音识别 | Transformer-Transducer
    | AM | WER= 3.6% |'
- en: '| [[68](#bib.bib68)] | CTC alignment | Retrieve the acoustic embedding at the
    token level for better ASR | Attention | AM | 51.2x RTF$\uparrow$ WER= 2.3% |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| [[68](#bib.bib68)] | CTC对齐 | 在标记级别检索声学嵌入以改善ASR | 注意力 | AM | 51.2x RTF$\uparrow$
    WER= 2.3% |'
- en: '| [[69](#bib.bib69)] | RNN-LSTM | Improve the efficiency of end-to-end ASR
    | Attention | LM | CER=1.98% |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| [[69](#bib.bib69)] | RNN-LSTM | 提高端到端ASR的效率 | 注意力 | LM | CER=1.98% |'
- en: '| [[59](#bib.bib59)] | CTC Alignment | Enhance the performance of end-to-end
    [ASR](#Sx1.4.4.4) | Autoregressive Transformer | AM | RTF= 0.0134 WER= 2.7% |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| [[59](#bib.bib59)] | CTC对齐 | 提升端到端[ASR](#Sx1.4.4.4)的性能 | 自回归Transformer |
    AM | RTF= 0.0134 WER= 2.7% |'
- en: 4.1.1 Acoustic domain
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 声学领域
- en: The study [[57](#bib.bib57)] reveals the Transformer model’s increased susceptibility
    to input sparsity compared to the [convolutional neural network](#Sx1.9.9.9) ([CNN](#Sx1.9.9.9)).
    The authors analyze the performance decline, attributing it to the Transformer’s
    structural characteristics. Additionally, they introduce a novel regularization
    method to enhance the Transformer’s resilience to input sparsity. This method
    directly regulates attention weights through silence label information in forced-alignment,
    offering the advantage of not requiring extra module training and excessive computation.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 研究[[57](#bib.bib57)]揭示了Transformer模型在输入稀疏性方面比[卷积神经网络](#Sx1.9.9.9)（[CNN](#Sx1.9.9.9)）更易受影响。作者分析了性能下降，归因于Transformer的结构特性。此外，他们引入了一种新颖的正则化方法，以增强Transformer对输入稀疏性的韧性。这种方法通过强制对齐中的静音标签信息直接调节注意力权重，具有不需要额外模块训练和过度计算的优点。
- en: The paper [[50](#bib.bib50)] addresses a limitation in Transformer-based end-to-end
    modeling for [ASR](#Sx1.4.4.4) tasks, where intermediate features from multiple
    input streams may lack diversity. The proposed solution introduces a multi-level
    acoustic feature extraction framework, incorporating shallow and deep streams
    to capture both traditional features for classification and speaker-invariant
    deep features for diversity. A [feature correlation-based fusion](#Sx1.18.18.18)
    ([FCF](#Sx1.18.18.18)) strategy, employed to combine intermediate features across
    both the frequency and time domains, correlates and combines these features before
    feeding them into the Transformer encoder-decoder module.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[[50](#bib.bib50)]解决了Transformer基础的端到端建模在[ASR](#Sx1.4.4.4)任务中的一个限制，即来自多个输入流的中间特征可能缺乏多样性。提出的解决方案引入了一个多级声学特征提取框架，结合浅层和深层流，以捕获分类的传统特征和具有多样性的说话人不变深层特征。一个[基于特征相关性的融合](#Sx1.18.18.18)（[FCF](#Sx1.18.18.18)）策略被用来结合频率和时间域的中间特征，在将这些特征输入到Transformer编码解码模块之前，关联并合并这些特征。
- en: The proposed masked autoencoding audio spectrogram Transformer (MAE-AST) operates
    solely on unmasked tokens [[51](#bib.bib51)], utilizing a large encoder. It concatenates
    mask tokens with encoder output embeddings, feeding them into a shallow decoder.
    Fine-tuning for downstream tasks involves using only the encoder, eliminating
    the decoder’s reconstruction layers. MAE-AST represents a significant improvement
    over the [self-supervised audio spectrogram transformer](#Sx1.46.46.46) ([SSAST](#Sx1.46.46.46))
    model for speech and audio classification. Addressing the high masking ratio issue,
    the method achieves a 3$\times$ speedup and 2$\times$ memory usage reduction.
    During downstream tasks, the approach consistently outperforms [SSAST](#Sx1.46.46.46).
    To identify varities of sounds types, Bai et al. [[52](#bib.bib52)] introduce
    SE-Trans, a cross-task model for environmental sound recognition, encompassing
    acoustic scene classification, urban sound tagging, and anomalous sound detection.
    Utilizing attention mechanisms and Transformer encoder modules, SE-Trans learns
    channel-wise relationships and temporal dependencies in acoustic features. The
    model incorporates FMix data augmentation, involving the creation of a binary
    mask from a randomly sampled complex matrix with a low-pass filter. SE-Trans achieves
    outstanding performance in ESR tasks, proven through evaluations on DCASE challenge
    databases, underscoring its robustness and versatility in environmental sound
    recognition.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的掩码自编码音频谱图变换器（MAE-AST）仅在未掩码的标记上操作[[51](#bib.bib51)]，并使用一个大型编码器。它将掩码标记与编码器输出嵌入连接在一起，并将其输入到一个浅层解码器中。下游任务的微调仅涉及使用编码器，去除了解码器的重建层。MAE-AST相比于[自监督音频谱图变换器](#Sx1.46.46.46)（[SSAST](#Sx1.46.46.46)）模型，在语音和音频分类方面表现出显著的改进。针对高掩码比率的问题，该方法实现了3$\times$的加速和2$\times$的内存使用减少。在下游任务中，该方法始终优于[SSAST](#Sx1.46.46.46)。为了识别各种声音类型，Bai等人[[52](#bib.bib52)]引入了SE-Trans，这是一种用于环境声音识别的跨任务模型，涵盖了声学场景分类、城市声音标记和异常声音检测。SE-Trans利用注意力机制和变换器编码器模块，学习声学特征中的通道级关系和时间依赖性。该模型包含FMix数据增强模块，通过从随机采样的复杂矩阵中创建二进制掩码，并应用低通滤波器。SE-Trans在ESR任务中表现出色，通过对DCASE挑战数据库的评估证明了其在环境声音识别中的鲁棒性和多样性。
- en: 'Automated audio captioning (AAC) involves generating textual descriptions for
    audio recordings, covering sound events, acoustic scenes, and event relationships.
    Current AAC systems typically employ an encoder-decoder architecture, with the
    decoder crafting captions based on extracted audio features. Chen et al. in their
    paper [[53](#bib.bib53)] introduces a novel approach that enhances caption generation
    by leveraging multi-level information extracted from the audio clip. The proposed
    method consists of a CNN encoder with multi-level feature extraction (channel
    attention, spatial attention), A module specialized in predicting keywords to
    generate guidance information at the word level and Transformer decoder. Figure
    [6](#S4.F6 "Figure 6 ‣ 4.1.1 Acoustic domain ‣ 4.1 Transformer-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") depicts the overall architecture incorporating
    the three mentioned modules. Results demonstrate significant improvements in various
    metrics, achieving [SOTA](#Sx1.44.44.44) performance during the cross-entropy
    training stage.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 自动音频字幕生成（AAC）涉及为音频录音生成文本描述，涵盖声音事件、声学场景和事件关系。目前的AAC系统通常采用编码器-解码器架构，解码器根据提取的音频特征制作字幕。Chen等人在其论文[[53](#bib.bib53)]中介绍了一种新方法，通过利用从音频片段中提取的多层次信息来增强字幕生成。所提方法包括一个具有多层次特征提取（通道注意力、空间注意力）的CNN编码器，一个专门用于预测关键词以生成单词级指导信息的模块，以及变换器解码器。图[6](#S4.F6
    "图6 ‣ 4.1.1 声学领域 ‣ 4.1 基于变换器的ASR ‣ 4 高级ASR方法和应用 ‣ 使用先进深度学习方法的自动语音识别：综述")展示了包含这三种模块的总体架构。结果显示，在各种指标上取得了显著改善，在交叉熵训练阶段达到了[SOTA](#Sx1.44.44.44)性能。
- en: '![Refer to caption](img/0fa6f421045cf2a9b6f459effbd6fa0b.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0fa6f421045cf2a9b6f459effbd6fa0b.png)'
- en: 'Figure 6: An example of CNN-based transformer for automated audio captioning
    [[53](#bib.bib53)].'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：基于CNN的变换器用于自动音频字幕生成[[53](#bib.bib53)]。
- en: Adversarial audio involves manipulating sound to deceive or compromise [machine
    learning](#Sx1.30.30.30) ([ML](#Sx1.30.30.30)) systems, exploiting vulnerabilities
    in audio recognition models. Both [[70](#bib.bib70), [71](#bib.bib71)] work are
    built to combat adversarial noise using transformers. The authors in [[70](#bib.bib70)]
    employed a vision transformer customized for audio signals to identify speech
    regions amidst challenging acoustic conditions. To enhance adaptability, they
    incorporated an augmentation module as an additional head in the transformer,
    integrating low-pass and band-pass filters. Experimental results reveal that the
    augmented vision architecture achieves an F1-score of up to 85.2% when using a
    low-pass filter, surpassing the baseline vision transformer, which attains an
    F1-score of up to 81.2%, in speech detection. However, in [[71](#bib.bib71)] the
    authors present an adversarial detection framework using an attention-based transformer
    mechanism to identify adversarial audio. Spectrogram features are segmented and
    integrated with positional information before input into the transformer encoder,
    achieving 96.5% accuracy under diverse conditions such as noisy environments,
    black-box attacks, and white-box attacks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗音频涉及操控声音以欺骗或危害[机器学习](#Sx1.30.30.30) ([ML](#Sx1.30.30.30))系统，利用音频识别模型中的漏洞。[[70](#bib.bib70),
    [71](#bib.bib71)]中的工作旨在使用变换器对抗对抗噪声。[[70](#bib.bib70)]中的作者使用了定制的视觉变换器来识别具有挑战性的声学条件下的语音区域。为了增强适应性，他们在变换器中加入了一个增广模块作为额外的头，集成了低通和带通滤波器。实验结果表明，使用低通滤波器的增广视觉架构在语音检测中达到了高达85.2%的F1分数，超越了基线视觉变换器的81.2%
    F1分数。然而，在[[71](#bib.bib71)]中，作者提出了一种使用基于注意力的变换器机制来识别对抗音频的对抗检测框架。谱图特征在输入到变换器编码器之前被分段并与位置性信息集成，在嘈杂环境、黑盒攻击和白盒攻击等多种条件下实现了96.5%的准确率。
- en: The paper [[72](#bib.bib72)] introduces a parallel-path transformer model to
    address computation cost challenges for speech separation tasks. Using improved
    feed-forward networks and transformer modules, it employs a parallel processing
    strategy with intra-chunk and inter-chunk transformers. This enables parallel
    local and global modeling of speech signals, enhancing overall system performance
    by capturing short and long-term dependencies.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[[72](#bib.bib72)]介绍了一种并行路径变换器模型，以解决语音分离任务中的计算成本挑战。该模型使用改进的前馈网络和变换器模块，通过在块内和块间的变换器采用并行处理策略。这使得语音信号的局部和全局建模可以并行进行，从而通过捕捉短期和长期依赖关系来提高整体系统性能。
- en: A Hybrid [ASR](#Sx1.4.4.4) approach outlines the conceptualization and execution
    of a technique that integrates neural network methodologies into advanced continuous
    speech recognition systems. This integration is built upon [hidden Markov modelss](#Sx1.23.23.23)
    with the aim of enhancing their overall performance. Wang et al. [[73](#bib.bib73)]
    introduce and assess transformer-based [AMs](#Sx1.2.2.2) for hybrid speech recognition.
    The approach incorporates various positional embedding methods and an iterated
    loss for training deep transformers. Demonstrating superior performance on the
    Librispeech benchmark, the suggested transformer-based [AM](#Sx1.2.2.2) outperforms
    the best hybrid result by 19% to 26% relative with a standard n-gram [LM](#Sx1.24.24.24).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 混合[ASR](#Sx1.4.4.4)方法概述了将神经网络方法整合到先进的连续语音识别系统中的技术的构思和执行。这一整合基于[隐马尔可夫模型](#Sx1.23.23.23)，旨在提升其整体性能。王等人[[73](#bib.bib73)]引入并评估了基于变换器的[AMs](#Sx1.2.2.2)用于混合语音识别。这种方法结合了多种位置嵌入方法和迭代损失以训练深度变换器。该方法在Librispeech基准测试中表现优越，建议的基于变换器的[AM](#Sx1.2.2.2)比最佳混合结果高出19%到26%，相较于标准n-gram
    [LM](#Sx1.24.24.24)。
- en: \Ac
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: 'TRUNet proposed in [[63](#bib.bib63)] represents an innovative approach to
    end-to-end multi-channel reverberant sound source separation. The model incorporates
    a recurrent-U network that directly estimates multi-channel filters from input
    spectra, enabling the exploitation of spatial and spectro-temporal diversity in
    sound source separation. In Figure [7](#S4.F7 "Figure 7 ‣ 4.1.1 Acoustic domain
    ‣ 4.1 Transformer-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey"), the block
    diagram illustrates the architecture of TRUNet’s transformer. The transformer
    network (TNet) encompasses three variations: (i) TNet–Cat, which concatenates
    multi-channel spectra, treating them as a single input. This approach allows for
    the direct utilization of spatial information between channels. (ii) TNet–RealImag,
    utilizing two separate transformer stacks for real and imaginary parts, respectively.
    Queries and keys are computed from the multi-channel spectra. Despite this, the
    method may not fully exploit spatial information, such as phase differences, directly.
    (iii) TNet–MagPhase, analogous to TNet–RealImag, but employing spectral magnitude
    and spectral phase instead of real and imaginary parts. This variation proves
    superior in extracting spatial information from complex-valued spectra, resulting
    in maximum enhanced performance in sound source separation when employing TRUNet-MagPhase
    architecture.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[63](#bib.bib63)]中提出的 TRUNet 代表了一种创新的端到端多通道混响声音源分离方法。该模型结合了递归-U 网络，直接从输入光谱估计多通道滤波器，从而利用声音源分离中的空间和光谱-时间多样性。在图
    [7](#S4.F7 "Figure 7 ‣ 4.1.1 Acoustic domain ‣ 4.1 Transformer-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") 中，框图展示了 TRUNet 的 transformer 结构。transformer 网络（TNet）包括三种变体：（i）TNet–Cat，将多通道光谱连接起来，作为单一输入。这种方法允许直接利用通道间的空间信息。（ii）TNet–RealImag，分别利用两个独立的
    transformer 堆栈处理实部和虚部。查询和键从多通道光谱中计算出来。然而，这种方法可能无法完全利用空间信息，如相位差异。（iii）TNet–MagPhase，类似于
    TNet–RealImag，但使用光谱幅度和光谱相位而不是实部和虚部。这种变体在从复值光谱中提取空间信息方面表现更优，使用 TRUNet-MagPhase
    结构时能获得最大程度的声音源分离性能提升。'
- en: '![Refer to caption](img/c9d1a546ad646a23b07c65baa27d073e.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c9d1a546ad646a23b07c65baa27d073e.png)'
- en: 'Figure 7: An example of source separation scheme based on Transformer [[63](#bib.bib63)].'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：基于 Transformer 的源分离方案示例 [[63](#bib.bib63)]。
- en: In recent times, dual-path networks have demonstrated effective results in many
    speech processing tasks such as speech separation and speech enhancement. In light
    of this, Wang Ke and colleagues incorporated the transformer into the structure
    of dual-path networks, presenting a time-domain speech enhancement model called
    the Two-stage Transformer-based Neural Network (TSTNN). This model significantly
    enhances the performance of speech enhancement [[74](#bib.bib74)]. Some research
    findings suggest that the dot-product self-attention may not be essential for
    transformer models. Similarly, The paper [[64](#bib.bib64)] introduces D²Net,
    a denoising and dereverberation network for challenging single-channel mixture
    speech in complex acoustic environments. D²Net incorporates a two-branch encoder
    (TBE) for feature extraction and fusion, along with a global-local dual-path transformer
    (GLDPT) featuring local dense synthesizer attention (LDSA) to enhance local information
    perception.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，双路径网络在许多语音处理任务中，如语音分离和语音增强，表现出了有效的结果。鉴于此，王科及其同事将 transformer 融入双路径网络结构中，提出了一种名为双阶段
    Transformer 基于神经网络（TSTNN）的时域语音增强模型。该模型显著提升了语音增强的性能 [[74](#bib.bib74)]。一些研究发现表明，点积自注意力可能对
    transformer 模型并非必不可少。类似地，论文 [[64](#bib.bib64)] 介绍了 D²Net，一种用于复杂声学环境中挑战性单通道混合语音的去噪和去混响网络。D²Net
    结合了用于特征提取和融合的双分支编码器（TBE），以及具有局部密集合成注意力（LDSA）的全球-局部双路径 transformer（GLDPT），以增强局部信息感知。
- en: The study proposed by gong et al. [[75](#bib.bib75)] explores self-attention-based
    neural networks like the [audio spectrogram transformer](#Sx1.5.5.5) ([AST](#Sx1.5.5.5))
    for audio tasks. It introduces a self-supervised framework, improving [AST](#Sx1.5.5.5)
    performance by 60.9% on various speech classification tasks such as [ASR](#Sx1.4.4.4),
    speaker recognition, and more, reducing reliance on labeled data. The approach
    marks a pioneering effort in audio self-supervised learning. Likewise, a novel
    augmented memory self-attention addresses limitations of transformer-based acoustic
    modeling in streaming applications has been proposed in [[76](#bib.bib76)], outperforming
    existing streamable methods by over 15% in relative error reduction on benchmark
    datasets.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: gong等人[[75](#bib.bib75)]提出的研究探讨了基于自注意力的神经网络，如[音频谱图Transformer](#Sx1.5.5.5)（[AST](#Sx1.5.5.5)），用于音频任务。它引入了一种自监督框架，提高了[AST](#Sx1.5.5.5)在[ASR](#Sx1.4.4.4)、说话人识别等各种语音分类任务上的性能，提升幅度达60.9%，减少了对标注数据的依赖。这一方法标志着音频自监督学习的开创性努力。同样，[[76](#bib.bib76)]中提出了一种新颖的增强记忆自注意力方法，解决了Transformer基于声学建模在流媒体应用中的局限性，在基准数据集上相对误差减少超过15%。
- en: Shareef et al. in [[58](#bib.bib58)] propose a collaborative training method
    for acoustic encoders in Arabic ASR systems for speech-impaired children, achieving
    a 10% relative accuracy improvement on phoneme alignment in the output sequence.
    Pioneering in recognizing impaired children’s Arabic speech. Similarly in [[77](#bib.bib77)],
    collaboratively training acoustic encoders of various sizes for on-device [ASR](#Sx1.4.4.4)
    improves efficiency and reduces redundancy. Using co-distillation with an auxiliary
    task, collaborative training achieves up to 11% relative [WER](#Sx1.52.52.52)
    improvement on LibriSpeech corpus.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Shareef等人[[58](#bib.bib58)]提出了一种用于阿拉伯ASR系统的协作训练方法，针对言语障碍儿童的声学编码器，在输出序列中的音素对齐上实现了10%的相对准确性提升。开创性地识别言语障碍儿童的阿拉伯语。类似地，在[[77](#bib.bib77)]中，针对设备上的[ASR](#Sx1.4.4.4)协作训练不同规模的声学编码器提高了效率并减少了冗余。通过使用带有辅助任务的共蒸馏，协作训练在LibriSpeech语料库上实现了高达11%的相对[WER](#Sx1.52.52.52)提升。
- en: Transducer models, in the context of [ASR](#Sx1.4.4.4), map input sequences
    (acoustic features) to output sequences (transcriptions). Unlike traditional [S2S](#Sx1.39.39.39)
    models, transducers can handle variable-length input and output sequences more
    efficiently. The study [[65](#bib.bib65)] explores attention masking in transformer-transducer-based
    [ASR](#Sx1.4.4.4), comparing fixed masking with chunked masking in terms of accuracy
    and latency. The authors claim that variable masking is the viable choice in acoustic
    rescoring scenarios. Similarly, to adapt the Transformer for streaming ASR, the
    authors in [[67](#bib.bib67)] employ the Transducer framework for streamable alignments.
    Using a unidirectional Transformer with interleaved convolution layers for audio
    encoding, they model future context and gradually downsample input to reduce computation
    cost, while limiting history context length.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ASR](#Sx1.4.4.4)的背景下，换能器模型将输入序列（声学特征）映射到输出序列（转录）。与传统的[S2S](#Sx1.39.39.39)模型不同，换能器可以更高效地处理可变长度的输入和输出序列。研究[[65](#bib.bib65)]探讨了Transformer换能器基于[ASR](#Sx1.4.4.4)中的注意力掩蔽，比较了固定掩蔽与分块掩蔽在准确性和延迟方面的表现。作者声称，在声学重新评分场景中，可变掩蔽是可行的选择。同样，为了使Transformer适应流媒体ASR，[[67](#bib.bib67)]中的作者采用了换能器框架用于可流式对齐。使用带有交错卷积层的单向Transformer进行音频编码，他们对未来上下文进行建模，并逐渐下采样输入以减少计算成本，同时限制历史上下文长度。
- en: Moving on, The work [[66](#bib.bib66)] introduces an all-in-one [AM](#Sx1.2.2.2)
    based on the Transformer architecture, combined with the [CTC](#Sx1.11.11.11)
    to ensure a sequential arrangement and utilize timing details. It addresses [ASR](#Sx1.4.4.4),
    [audio tagging](#Sx1.6.6.6) ([AT](#Sx1.6.6.6)), and acoustic [ED](#Sx1.17.17.17)
    simultaneously. The model demonstrates superior performance, showcasing its suitability
    for comprehensive acoustic scene transcription.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，文献[[66](#bib.bib66)]介绍了一种基于Transformer架构的全功能[AM](#Sx1.2.2.2)，结合了[CTC](#Sx1.11.11.11)以确保顺序排列并利用时间细节。它同时解决了[ASR](#Sx1.4.4.4)、[音频标记](#Sx1.6.6.6)（[AT](#Sx1.6.6.6)）和声学[ED](#Sx1.17.17.17)的问题。该模型表现出卓越的性能，展示了其在综合声学场景转录中的适用性。
- en: Winata et al. [[56](#bib.bib56)] propose a memory-efficient Transformer architecture
    for end-to-end speech recognition. It significantly reduces parameters, boosting
    training speed by over 50% and inference time by 1.35$\times$ compared to baseline.
    Experiments show better generalization, lower error rates, and outperformance
    existing schemes without external language or acoustic models. Growing demand
    for on-device [ASR](#Sx1.4.4.4) systems prompts interest in model compression.compression.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: Winata 等人 [[56](#bib.bib56)] 提出了一个内存高效的 Transformer 架构用于端到端语音识别。与基线相比，它显著减少了参数，提升了训练速度超过
    50%，推理时间提高了 1.35$\times$。实验表明其具有更好的泛化能力、更低的错误率，并超越了现有方案，无需外部语言或声学模型。对设备上的 [ASR](#Sx1.4.4.4)
    系统的需求不断增长，引发了对模型压缩的兴趣。
- en: 4.1.2 Language domain
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 语言领域
- en: Self-attention models, such as Transformers, excel in speech recognition and
    reveal an important pattern. As upper self-attention layers are replaced with
    feed-forward layers, resembling CLDNN architecture in [[48](#bib.bib48)], experiments
    on [wall street journal](#Sx1.53.53.53) ([WSJ](#Sx1.53.53.53)) and [switchboard](#Sx1.47.47.47)
    ([SWBD](#Sx1.47.47.47)) datasets show no performance drop and minor gains. The
    novel proposed metric of attention matrix diagonality indicates increased diagonality
    in lower to upper encoder self-attention layers. The authors conclude that a global
    view appears unnecessary for training upper encoder layers in speech recognition
    Transformers when lower layers capture sufficient contextual information. The
    study conducted by Hrinchuk et al. [[49](#bib.bib49)] presents a proficient postprocessing
    model for [ASR](#Sx1.4.4.4) with a Transformer-based encoder-decoder architecture,
    initialized with the weights of pre-trained BERT model [[36](#bib.bib36)]. The
    model effectively refines [ASR](#Sx1.4.4.4) output, demonstrating substantial
    performance gains through strategies like extensive data augmentation and pretrained
    weight initialization. On the LibriSpeech benchmark dataset, significant reductions
    in [WERs](#Sx1.52.52.52) are observed, particularly on noisier evaluation dataset
    portions, outperforming baseline models and approaching the performance of Transformer-XL
    neural language model re-scoring with 6-gram.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力模型，如 Transformers，在语音识别中表现优异，并揭示了一个重要的模式。由于上层自注意力层被前馈层替代，类似于 [[48](#bib.bib48)]
    中的 CLDNN 架构，对 [wall street journal](#Sx1.53.53.53) ([WSJ](#Sx1.53.53.53)) 和 [switchboard](#Sx1.47.47.47)
    ([SWBD](#Sx1.47.47.47)) 数据集的实验显示没有性能下降，并有小幅提升。提出的新指标——注意力矩阵对角性——表明从下到上的编码器自注意力层中对角性增加。作者得出结论：当下层捕获了足够的上下文信息时，训练上层编码器层时不再需要全球视图。Hrinchuk
    等人 [[49](#bib.bib49)] 的研究提出了一种高效的后处理模型，用于基于 Transformer 的编码器-解码器架构，初始化为预训练的 BERT
    模型 [[36](#bib.bib36)] 的权重。该模型有效地优化了 [ASR](#Sx1.4.4.4) 输出，通过广泛的数据增强和预训练权重初始化等策略显示了显著的性能提升。在
    LibriSpeech 基准数据集上，观察到 [WERs](#Sx1.52.52.52) 的显著减少，特别是在更嘈杂的评估数据集部分，超越了基线模型，并接近
    Transformer-XL 神经语言模型的 6-gram 再评分性能。
- en: '![Refer to caption](img/9ea04dd52913074b01f1d7e0b8d76a12.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9ea04dd52913074b01f1d7e0b8d76a12.png)'
- en: 'Figure 8: Some end-to-end models: Basic CTC , RNN-Transducer, and attention
    architectures [[78](#bib.bib78)].'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 一些端到端模型：基本 CTC、RNN-Transducer 和注意力架构 [[78](#bib.bib78)]。'
- en: \Ac
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: 'CTC is an architecture and principle commonly used in [S2S](#Sx1.39.39.39)
    tasks (Figure [8](#S4.F8 "Figure 8 ‣ 4.1.2 Language domain ‣ 4.1 Transformer-based
    ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey")), such as [ASR](#Sx1.4.4.4). It
    enables alignment-free training by introducing a blank symbol and allowing variable-length
    alignments between input and output sequences. During training, the model learns
    to align the input sequence with the target sequence, and the blank symbol accounts
    for multiple possible alignments. [CTC](#Sx1.11.11.11) is particularly effective
    in tasks with variable-length outputs, making it well-suited for applications
    like speech recognition where the duration of spoken words may vary. Figure Figure
    [8](#S4.F8 "Figure 8 ‣ 4.1.2 Language domain ‣ 4.1 Transformer-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") (a) illustrates the operational principles and
    components of [CTC](#Sx1.11.11.11). This latter has been used in many [ASR](#Sx1.4.4.4)
    schemes, for example, Deng et al. [[54](#bib.bib54)] presents the innovative pretrained
    Transformer [S2S](#Sx1.39.39.39) [ASR](#Sx1.4.4.4) architecture, which integrates
    self-supervised pretraining techniques for comprehensive end-to-end [ASR](#Sx1.4.4.4).
    Employing a hybrid [CTC](#Sx1.11.11.11)/attention model, it maximizes the potential
    of pretrained [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24). The inclusion of a [CTC](#Sx1.11.11.11)
    branch aids in the encoder’s convergence during training and considers all potential
    time boundaries in beam searching. The encoder is initiated with wav2vec2.0, and
    the introduction of a one-cross decoder (OCD) mitigates reliance on acoustic representations,
    enabling initialization with pretrained DistilGPT2 and overcoming the constraint
    of conditioning on acoustic features.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 'CTC 是一种常用于[S2S](#Sx1.39.39.39)任务的架构和原理（图[8](#S4.F8 "Figure 8 ‣ 4.1.2 Language
    domain ‣ 4.1 Transformer-based ASR ‣ 4 Advanced ASR methods and applications ‣
    Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")），例如[ASR](#Sx1.4.4.4)。它通过引入空白符号并允许输入和输出序列之间的可变长度对齐，从而实现无对齐训练。在训练过程中，模型学习将输入序列与目标序列对齐，空白符号则考虑了多个可能的对齐。
    [CTC](#Sx1.11.11.11)在具有可变长度输出的任务中特别有效，使其非常适合像语音识别这样的应用，因为口语的持续时间可能有所不同。图[8](#S4.F8
    "Figure 8 ‣ 4.1.2 Language domain ‣ 4.1 Transformer-based ASR ‣ 4 Advanced ASR
    methods and applications ‣ Automatic Speech Recognition using Advanced Deep Learning
    Approaches: A survey")（a）展示了[CTC](#Sx1.11.11.11)的操作原理和组件。后者已在许多[ASR](#Sx1.4.4.4)方案中使用，例如，Deng等人[[54](#bib.bib54)]提出了创新的预训练Transformer
    [S2S](#Sx1.39.39.39) [ASR](#Sx1.4.4.4)架构，该架构结合了自监督预训练技术，实现了全面的端到端[ASR](#Sx1.4.4.4)。通过采用混合[CTC](#Sx1.11.11.11)/注意力模型，它最大化了预训练[AM](#Sx1.2.2.2)和[LM](#Sx1.24.24.24)的潜力。引入[CTC](#Sx1.11.11.11)分支有助于训练过程中编码器的收敛，并在光束搜索中考虑所有潜在的时间边界。编码器以wav2vec2.0为起始，并且引入了一种one-cross解码器（OCD）减少对声学表征的依赖，实现了与预训练DistilGPT2的初始化，并克服了对声学特征条件的限制。'
- en: \Ac
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: CS takes place when a speaker switches between words of two or more languages
    within a single sentence or across sentences. Zhou et al. [[55](#bib.bib55)] introduces
    a multi-encoder-decoder Transformer, for [code-switching](#Sx1.10.10.10) ([CS](#Sx1.10.10.10))
    problem. It employs language-specific encoders and attention mechanisms to enhance
    acoustic representations, pre-trained on monolingual data to address limited [CS](#Sx1.10.10.10)
    training data. Hadwan et al. research [[69](#bib.bib69)] employ an attention-based
    encoder-decoder transformer, to enhance end-to-end [ASR](#Sx1.4.4.4) for the Arabic
    language, focusing on Qur’an recitation. The proposed model incorporates a multi-head
    attention mechanism and Mel filter bank for feature extraction. For constructing
    a [LM](#Sx1.24.24.24), [recurrent neural network](#Sx1.36.36.36) ([RNN](#Sx1.36.36.36))
    and [long short term memory](#Sx1.26.26.26) ([LSTM](#Sx1.26.26.26)) techniques
    were employed to train an n-gram word-based [LM](#Sx1.24.24.24). The study introduces
    a new dataset, yielding [SOTA](#Sx1.44.44.44) results with a low character error
    rate.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 代码切换（CS）发生在说话者在单个句子或跨句子之间切换两种或多种语言的单词时。Zhou 等人 [[55](#bib.bib55)] 介绍了一种多编码器-解码器
    Transformer，用于[代码切换](#Sx1.10.10.10)（[CS](#Sx1.10.10.10)）问题。该方法采用了特定语言的编码器和注意力机制，以增强声学表示，这些编码器和机制在单语数据上进行预训练，以解决有限的[CS](#Sx1.10.10.10)训练数据问题。Hadwan
    等人 [[69](#bib.bib69)] 的研究采用了基于注意力的编码器-解码器 Transformer，以增强阿拉伯语的端到端[ASR](#Sx1.4.4.4)，重点关注古兰经朗读。所提模型结合了多头注意力机制和
    Mel 频谱滤波器进行特征提取。为了构建[语言模型（LM）](#Sx1.24.24.24)，使用了[递归神经网络（RNN）](#Sx1.36.36.36)和[长短期记忆（LSTM）](#Sx1.26.26.26)技术，训练了基于
    n-gram 单词的[LM](#Sx1.24.24.24)。该研究引入了一个新数据集，并取得了具有低字符错误率的[最新技术水平（SOTA）](#Sx1.44.44.44)结果。
- en: 4.2 TL-based ASR
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于迁移学习（TL）的 ASR
- en: Overall, DTL consists of training a DL model on a specific domain (or task)
    and then transferring the acquired knowledge to a new, similar domain (or task).
    In what follows, we present some of the definitions that are essential to understand
    the principle of DTL for [ASR](#Sx1.4.4.4) applications.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，DTL 包括在特定领域（或任务）上训练一个深度学习（DL）模型，然后将获得的知识转移到新的、类似的领域（或任务）。接下来，我们将介绍一些理解
    DTL 原则在[自动语音识别（ASR）](#Sx1.4.4.4)应用中的关键定义。
- en: \Ac
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: 'DTL refers to a [DL](#Sx1.14.14.14) paradigm where knowledge gained from pre-training
    a model (Source model) on one domain or task is leveraged to enhance performance
    of target model, on a different but related domain or task. In this context, a
    "domain" refers to a specific data distribution, while a "task" represents a learning
    objective. \AcDA in [DTL](#Sx1.16.16.16) involves adapting a model trained on
    a $D_{S}$ to perform well on a target domain. This is crucial when there are differences
    in data distributions between the two domains. Fine-tuning is a technique where
    a pre-trained model is further trained on task-specific data to improve its performance
    on a related task. Cross-domain learning extends transfer learning to scenarios
    where the source and target domains are distinct. Zero-shot learning involves
    training a model to recognize classes not present in the training data. Transductive
    [DTL](#Sx1.16.16.16) focuses on adapting a model based on a specific set of target
    instances. Inductive [DTL](#Sx1.16.16.16) aims to generalize knowledge across
    domains by training a model to handle diverse tasks and domains simultaneously
    [[36](#bib.bib36), [79](#bib.bib79), [80](#bib.bib80)]. These techniques contribute
    to the versatility and adaptability of [DL](#Sx1.14.14.14) models in various applications.
    Fig. [9](#S4.F9 "Figure 9 ‣ 4.2 TL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    depicted the principle of [DTL](#Sx1.16.16.16) techniques. Table [6](#S4.T6 "Table
    6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    summarises the most recent DTL-based [ASR](#Sx1.4.4.4) techniques used in [AM](#Sx1.2.2.2)
    and [LM](#Sx1.24.24.24) domains.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: DTL 指的是一种[DL](#Sx1.14.14.14)范式，在这种范式中，从在一个领域或任务上预训练的模型（源模型）获得的知识被用来提升目标模型在不同但相关领域或任务上的表现。在这个上下文中，“领域”指的是特定的数据分布，而“任务”代表一个学习目标。\AcDA
    在[DTL](#Sx1.16.16.16)中涉及将一个在$D_{S}$上训练的模型适应到目标领域。当两个领域之间的数据分布存在差异时，这一点尤为重要。微调是一种将预训练模型在任务特定数据上进一步训练以改善其在相关任务上表现的技术。跨领域学习将迁移学习扩展到源领域和目标领域不同的场景。零样本学习涉及训练一个模型识别训练数据中不存在的类别。归纳[DTL](#Sx1.16.16.16)旨在通过训练一个模型来同时处理多种任务和领域，从而在领域间推广知识[[36](#bib.bib36),
    [79](#bib.bib79), [80](#bib.bib80)]。这些技术提升了[DL](#Sx1.14.14.14)模型在各种应用中的通用性和适应性。图[9](#S4.F9
    "图 9 ‣ 4.2 基于 TL 的 ASR ‣ 4 高级 ASR 方法与应用 ‣ 使用先进深度学习方法的自动语音识别：一项综述")展示了[DTL](#Sx1.16.16.16)技术的原理。表[6](#S4.T6
    "表 6 ‣ 4.2.2 语言领域 ‣ 4.2 基于 TL 的 ASR ‣ 4 高级 ASR 方法与应用 ‣ 使用先进深度学习方法的自动语音识别：一项综述")总结了在[AM](#Sx1.2.2.2)和[LM](#Sx1.24.24.24)领域中使用的最新
    DTL 基于的[ASR](#Sx1.4.4.4)技术。
- en: '![Refer to caption](img/62e0398eeaf05a91872fe5b5b52f9571.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/62e0398eeaf05a91872fe5b5b52f9571.png)'
- en: 'Figure 9: Deep transfer learning principle.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：深度迁移学习原理。
- en: 4.2.1 Acoustic domain
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 声学领域
- en: Schneider et al. [[81](#bib.bib81)] explored unsupervised pre-training for speech
    recognition using wav2vec model on large unlabeled audio data. The learned representations
    enhanced [AM](#Sx1.2.2.2) training with a simple [CNN](#Sx1.9.9.9) optimized through
    noise contrastive binary classification. In [[82](#bib.bib82)], a source filter
    warping data augmentation strategy is proposed to enhance the robustness of children’s
    speech [ASR](#Sx1.4.4.4). The authors constructed an end-to-end acoustic model
    using the XLS-R wav2vec 2.0 model, pre-trained in a self-supervised manner on
    extensive cross-lingual corpora of adult speech. The work proposed in [[83](#bib.bib83)]
    introduces a multi-dialect acoustic model employing soft-parameter-sharing multi-task
    learning, a transductive DTL subcategory, within the Transformer architecture.
    Auxiliary cross-attentions aid dialect ID recognition, providing dialect information.
    Adaptive cross-entropy loss automatically balances multi-task learning. Experimental
    results demonstrate a significant reduction in error rates compared to various
    single- and multi-task models on multi-dialect speech recognition and dialect
    ID recognition tasks. Similarly, in the realm of computer vision, [CNNs](#Sx1.9.9.9)
    models like ConvNeXt have outperformed cutting-edge transformers, partly due to
    the integration of depthwise separable convolutions (DSC). DSC, which approximates
    regular convolutions, enhances the efficiency of [CNNs](#Sx1.9.9.9) in terms of
    time and memory usage without compromising accuracy—in some cases, even enhancing
    it. The study [[84](#bib.bib84)] introduces DSC into the pre-trained audio model
    family for audio classification on AudioSet (target task), demonstrating its advantages
    in balancing accuracy and model size. Xin et al. [[85](#bib.bib85)] introduce
    an audio pyramid Transformer with an attention tree structure, with four branches,
    to reduce computational complexity in fine-grained audio spectrogram processing.
    It proposes a [DA](#Sx1.13.13.13) transfer learning approach for weakly supervised
    sound event detection, a sub-field of [ASR](#Sx1.4.4.4), enhancing localization
    performance by aligning feature distributions between frame and clip domains with
    a [DA](#Sx1.13.13.13) detection loss.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: Schneider 等人 [[81](#bib.bib81)] 探索了使用 wav2vec 模型对大规模无标注音频数据进行无监督预训练的语音识别方法。所学习的表示通过噪声对比二元分类优化的简单
    [CNN](#Sx1.9.9.9) 增强了 [AM](#Sx1.2.2.2) 训练。在 [[82](#bib.bib82)] 中，提出了一种源过滤器扭曲数据增强策略，以提高儿童语音
    [ASR](#Sx1.4.4.4) 的鲁棒性。作者使用 XLS-R wav2vec 2.0 模型构建了一个端到端的声学模型，该模型在大量跨语言的成人语音语料库上以自监督方式进行预训练。[[83](#bib.bib83)]
    中提出的工作引入了一个多方言声学模型，采用软参数共享的多任务学习，这是一个归纳 DTL 子类别，嵌入在 Transformer 架构中。辅助的交叉注意力有助于方言
    ID 识别，提供方言信息。自适应交叉熵损失自动平衡了多任务学习。实验结果表明，与各种单任务和多任务模型相比，在多方言语音识别和方言 ID 识别任务上显著降低了错误率。同样，在计算机视觉领域，像
    ConvNeXt 这样的 [CNNs](#Sx1.9.9.9) 模型已超越了前沿的 Transformer，部分原因是深度可分离卷积（DSC）的集成。DSC
    近似于常规卷积，提升了 [CNNs](#Sx1.9.9.9) 在时间和内存使用上的效率而不影响准确性——在某些情况下，甚至提高了准确性。研究 [[84](#bib.bib84)]
    将 DSC 引入到预训练音频模型家族中，用于在 AudioSet 上进行音频分类（目标任务），展示了在平衡准确性和模型大小方面的优势。Xin 等人 [[85](#bib.bib85)]
    引入了一种带有四个分支的注意力树结构的音频金字塔 Transformer，以减少细粒度音频频谱处理中的计算复杂性。它提出了一种 [DA](#Sx1.13.13.13)
    迁移学习方法，用于弱监督声音事件检测，这是 [ASR](#Sx1.4.4.4) 的一个子领域，通过对齐帧和片段域之间的特征分布，利用 [DA](#Sx1.13.13.13)
    检测损失来提高定位性能。
- en: 4.2.2 Language domain
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 语言领域
- en: 'The methodology is founded on the utilization of the BERT model [[86](#bib.bib86)],
    which involves pretraining language models and demonstrates improved performance
    across various downstream tasks. DTL approaches for language models, specifically
    employed in the domain of voice recognition, are referred to as [LM](#Sx1.24.24.24)
    adaptation. These approaches aim to bridge the gap between the source distribution
    $\mathbb{D}_{S}$ and the target distribution $\mathbb{D}_{T}$. Song et al. [[87](#bib.bib87)]
    present a novel learning-to-rescore (L2RS) approach, which relies on two main
    components: (i) utilizing diverse textual data from [SOTA](#Sx1.44.44.44) NLP
    models, such as BERT, and (ii) automatically determining their weights to rescore
    the N-best lists for [ASR](#Sx1.4.4.4) systems.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法论基于 BERT 模型的使用 [[86](#bib.bib86)]，涉及预训练语言模型，并在各种下游任务中展示了改进的性能。针对语言模型的 DTL
    方法，特别是在语音识别领域中，称为 [LM](#Sx1.24.24.24) 适应。这些方法旨在弥合源分布 $\mathbb{D}_{S}$ 和目标分布 $\mathbb{D}_{T}$
    之间的差距。Song 等人 [[87](#bib.bib87)] 提出了一个新颖的学习重评分（L2RS）方法，该方法依赖于两个主要组件：（i）利用来自 [SOTA](#Sx1.44.44.44)
    NLP 模型（如 BERT）的多样化文本数据，以及（ii）自动确定其权重以重新评分 [ASR](#Sx1.4.4.4) 系统的 N-best 列表。
- en: 'Recent advancements in [S2S](#Sx1.39.39.39) models have shown promising results
    for training monolingual [ASR](#Sx1.4.4.4) systems. The [CTC](#Sx1.11.11.11) and
    encoder-decoder models are two popular architectures for end-to-end [ASR](#Sx1.4.4.4).
    Additionally, joint training of these architectures in a multi-task hybrid approach
    has been explored, demonstrating improved overall performance. For instance, the
    architecture illustrated in Fig. [8](#S4.F8 "Figure 8 ‣ 4.1.2 Language domain
    ‣ 4.1 Transformer-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey") (a) comprises
    [S2S](#Sx1.39.39.39) layers. The encoder network of [S2S](#Sx1.39.39.39) consists
    of a series of [RNNs](#Sx1.36.36.36) that generate embedding vectors, while the
    [RNN](#Sx1.36.36.36) decoder utilizes these vectors to produce final results.
    The [RNN](#Sx1.36.36.36) also benefits from prior predictions ($P_{i},i=0,\dots,n$),
    enhancing the accuracy of subsequent predictions. Moving on, a novel TL-based
    approach that enhances end-to-end speech recognition has been proposed in [[88](#bib.bib88)].
    The novelty lies in applying [DTL](#Sx1.16.16.16) through multilingual training
    and multi-task learning at two levels. The initial stage utilizes nonnegative
    matrix factorization (NMF), instead of a bottleneck layer, and multilingual training
    for high-level feature extraction. The subsequent stage employs joint CTC-attention
    models on these features, where the CTC was transferred to the target attention-based
    model. The scheme demonstrated superior performance on TIMIT but requires testing
    on high-resource data. Further optimization is needed for standard end-to-end
    training. In addition, Integrating both [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24)
    methodologies has the potential to enhance or construct an effective DTL-based
    ASR model, as demonstrated in [[46](#bib.bib46), [89](#bib.bib89), [90](#bib.bib90),
    [91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)].'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，[S2S](#Sx1.39.39.39) 模型的进展显示了在训练单语 [ASR](#Sx1.4.4.4) 系统方面的良好前景。[CTC](#Sx1.11.11.11)
    和编码器-解码器模型是端到端 [ASR](#Sx1.4.4.4) 的两种流行架构。此外，这些架构在多任务混合方法中的联合训练也得到了探索，展示了整体性能的提升。例如，图
    [8](#S4.F8 "Figure 8 ‣ 4.1.2 Language domain ‣ 4.1 Transformer-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") (a) 所示的架构包括 [S2S](#Sx1.39.39.39) 层。[S2S](#Sx1.39.39.39)
    的编码器网络由一系列生成嵌入向量的 [RNNs](#Sx1.36.36.36) 组成，而 [RNN](#Sx1.36.36.36) 解码器利用这些向量生成最终结果。[RNN](#Sx1.36.36.36)
    还受益于之前的预测（$P_{i},i=0,\dots,n$），提高了后续预测的准确性。接下来，一种新颖的基于 TL 的方法已被提出以增强端到端语音识别，其创新在于通过多语言训练和多任务学习在两个层面上应用
    [DTL](#Sx1.16.16.16)。初始阶段利用非负矩阵分解（NMF），而非瓶颈层，以及多语言训练进行高级特征提取。随后的阶段在这些特征上应用联合 CTC-注意力模型，其中
    CTC 被转移到目标注意力模型。该方案在 TIMIT 上表现出色，但需要在高资源数据上进行测试。标准端到端训练仍需进一步优化。此外，结合 [AM](#Sx1.2.2.2)
    和 [LM](#Sx1.24.24.24) 方法具有增强或构建有效 DTL 基于 ASR 模型的潜力，如 [[46](#bib.bib46), [89](#bib.bib89),
    [90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)] 所示。'
- en: 'Table 6: In contemporary cutting-edge frameworks, diverse pre-trained models
    are utilized for distinct tasks within the field. These frameworks employ different
    DTL approaches and assess their efficacy using specific metrics. The symbol ($\uparrow$)
    result increase, whereas ($\downarrow$) signifies result decrease. In cases where
    multiple scenarios are examined, only the top-performing outcome is mentioned.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：在当代前沿框架中，为不同任务采用了各种预训练模型。这些框架使用不同的 DTL 方法，并通过特定指标评估其效果。符号 ($\uparrow$) 表示结果增加，而
    ($\downarrow$) 表示结果减少。在检查多个场景时，仅提及表现最佳的结果。
- en: '| Scheme | Based on | Speech recognition task ($\mathbb{T}_{T}$) | AM/LM |
    Adaptation | Result with metric |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 方案 | 基于 | 语音识别任务 ($\mathbb{T}_{T}$) | AM/LM | 适应 | 结果及指标 |'
- en: '| [[84](#bib.bib84)] | ConvNeXt-Tiny | Audio classification | AM | DA | [mAP](#Sx1.27.27.27)=
    0.471 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| [[84](#bib.bib84)] | ConvNeXt-Tiny | 音频分类 | AM | DA | [mAP](#Sx1.27.27.27)=
    0.471 |'
- en: '| [[85](#bib.bib85)] | [APT](#Sx1.3.3.3) | Sound event detection | AM | DA
    | F1= 79.6% |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| [[85](#bib.bib85)] | [APT](#Sx1.3.3.3) | 声音事件检测 | AM | DA | F1= 79.6% |'
- en: '| [[49](#bib.bib49)] | BERT (Jasper) | Speech-to-text | LM | TL | WER= 14%
    |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| [[49](#bib.bib49)] | BERT (Jasper) | 语音转文本 | LM | TL | WER= 14% |'
- en: '| [[54](#bib.bib54)] | DistilGPT2 | Improve ASR | Both | Fine-tuning | CER=
    4.6% |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| [[54](#bib.bib54)] | DistilGPT2 | 改进 ASR | 两者 | 微调 | CER= 4.6% |'
- en: '| [[93](#bib.bib93)] | XLRS Wave2vec | Improve ASR in low resource language
    | Both | Fine-tuning | 5.6% WER $\downarrow$ |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| [[93](#bib.bib93)] | XLRS Wave2vec | 提升低资源语言的 ASR | 两者 | 微调 | 5.6% WER $\downarrow$
    |'
- en: '| [[82](#bib.bib82)] | XLRS Wave2vec | Improve ASR for children’s speech |
    AM | TL | WER = 4.86% |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| [[82](#bib.bib82)] | XLRS Wave2vec | 改进儿童语音的 ASR | AM | TL | WER = 4.86%
    |'
- en: '| [[90](#bib.bib90)] | [S2S](#Sx1.39.39.39) | Speaker adaptation | Both | Features
    norm. | 25.0% WER$\downarrow$ |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| [[90](#bib.bib90)] | [S2S](#Sx1.39.39.39) | 说话人适应 | 两者 | 特征归一化。 | 25.0% WER$\downarrow$
    |'
- en: '| [[83](#bib.bib83)] | Transformer | Multi-dialect model aids recognizing diverse
    speech dialects effectively | AM | Multi-task learning | Acc= 100% |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| [[83](#bib.bib83)] | Transformer | 多方言模型帮助有效识别多种语音方言 | AM | 多任务学习 | 准确率=
    100% |'
- en: '| [[94](#bib.bib94)] | [S2S](#Sx1.39.39.39) | Enhancing the existing multilingual
    [S2S](#Sx1.39.39.39) model. | LM | DTL | 4%CER $\downarrow$ 6% WER |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| [[94](#bib.bib94)] | [S2S](#Sx1.39.39.39) | 提升现有多语言[S2S](#Sx1.39.39.39)模型。
    | LM | DTL | 4% CER $\downarrow$ 6% WER |'
- en: '| [[95](#bib.bib95)] | PaSST | Audio tagging and event detection | AM | Fine-tuning
    | F1= 64.85% |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| [[95](#bib.bib95)] | PaSST | 音频标记和事件检测 | AM | 微调 | F1= 64.85% |'
- en: '| [[81](#bib.bib81)] | Wav2vec | WSJ data speech | AM | affine transform. |
    36% WER$\downarrow$ |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| [[81](#bib.bib81)] | Wav2vec | WSJ 数据语音 | AM | 仿射变换。 | 36% WER$\downarrow$
    |'
- en: '| [[96](#bib.bib96)] | ARoBERT | Spoken language understanding | LM | Fine-tuning
    | F1-score=92.56% |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] | ARoBERT | 口语理解 | LM | 微调 | F1-score=92.56% |'
- en: 'Abbreviations: Transformer (T)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 缩写词：Transformer (T)
- en: 4.3 FL-based ASR
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基于 FL 的 ASR
- en: \Ac
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: 'FL revolutionizes AI model training by enabling collaboration without the need
    to share sensitive training data. Traditional centralized approaches are evolving
    towards decentralized models, where [ML](#Sx1.30.30.30) algorithms are trained
    collaboratively on edge devices like mobile phones, laptops, or private servers
    [[97](#bib.bib97)]. The mathematical formulation of FL focuses on training a single
    global model across multiple devices or nodes (clients) while keeping the data
    localized. The objective is to minimize a global loss function that is typically
    the weighted sum of the local loss functions on all clients. The standard [FL](#Sx1.20.20.20)
    problem can be formulated as:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: FL 革新了 AI 模型训练，通过实现协作而无需共享敏感训练数据。传统的集中式方法正在向去中心化模型演变，其中[ML](#Sx1.30.30.30)算法在边缘设备上（如手机、笔记本电脑或私人服务器）进行协作训练
    [[97](#bib.bib97)]。FL 的数学公式侧重于在多个设备或节点（客户端）上训练一个单一的全局模型，同时保持数据本地化。其目标是最小化一个全局损失函数，该函数通常是所有客户端上的本地损失函数的加权和。标准的[FL](#Sx1.20.20.20)问题可以表述为：
- en: '|  | $\min_{\theta}F(\theta)=\min_{\theta}\sum_{k=1}^{K}\frac{n_{k}}{N}F_{k}(\theta)$
    |  | (3) |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}F(\theta)=\min_{\theta}\sum_{k=1}^{K}\frac{n_{k}}{N}F_{k}(\theta)$
    |  | (3) |'
- en: In this context, $\theta$ denotes the parameters of the global model to be learned,
    $K$ represents the total number of clients, $n_{k}$ signifies the number of data
    samples at client $k$, $N=\sum_{k=1}^{K}n_{k}$ stands for the total number of
    data samples across all clients, and $F_{k}(\theta)$ indicates the local loss
    function computed on the data of client $k$. In [FL](#Sx1.20.20.20), the goal
    is to find the global model parameters $\theta$ that minimize the global loss
    function $F(\theta)$, which is an aggregate of the local loss functions from all
    participating clients. This process typically involves iterative updates to the
    model parameters using algorithms like federated averaging (FedAvg), where clients
    compute gradients or updates based on their local data and send these updates
    to a central server. The server then aggregates these updates to improve the global
    model.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，$\theta$表示待学习的全局模型参数，$K$代表客户端的总数，$n_{k}$表示客户端$k$的数据样本数量，$N=\sum_{k=1}^{K}n_{k}$代表所有客户端的数据样本总数，$F_{k}(\theta)$表示在客户端$k$数据上计算的本地损失函数。在[FL](#Sx1.20.20.20)中，目标是找到使全局损失函数$F(\theta)$最小化的全局模型参数$\theta$，这是所有参与客户端本地损失函数的聚合。这个过程通常涉及使用如联邦平均（FedAvg）等算法对模型参数进行迭代更新，其中客户端根据本地数据计算梯度或更新，并将这些更新发送到中央服务器。然后，服务器聚合这些更新以改善全局模型。
- en: •
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Horizontal federated fearning (HFL): In HFL, clients train a shared global
    model using their respective datasets, characterized by the same feature space
    but different sample spaces. Each client utilizes a local AI model, and their
    updates are aggregated by a central server without exposing raw data. The HFL
    training process involves: (1) initialization, (2) local training, (3) encryption
    of gradients, (4) secure aggregation, and (5) global model parameter updates.
    The objective function minimizes a global loss across all parties’ datasets [[97](#bib.bib97)].'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 横向联邦学习（HFL）：在HFL中，客户端使用各自的数据集训练一个共享的全局模型，这些数据集具有相同的特征空间但不同的样本空间。每个客户端利用本地AI模型，其更新由中央服务器聚合，而不暴露原始数据。HFL训练过程包括：（1）初始化，（2）本地训练，（3）梯度加密，（4）安全聚合，以及（5）全局模型参数更新。目标函数在所有参与方的数据集上最小化全局损失[[97](#bib.bib97)]。
- en: •
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Vertical federated learning (VFL): VFL trains models on datasets sharing the
    same sample space but having different feature spaces. Through entity data alignment
    (EDA) and encrypted model trained (EMT), VFL allows clients to cooperatively train
    models without sharing raw data. The training process involves the same steps
    as HFL [[97](#bib.bib97)].'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 纵向联邦学习（VFL）：VFL在具有相同样本空间但不同特征空间的数据集上训练模型。通过实体数据对齐（EDA）和加密模型训练（EMT），VFL允许客户端在不共享原始数据的情况下协同训练模型。训练过程与HFL相同[[97](#bib.bib97)]。
- en: '![Refer to caption](img/4d782211be5f99014cc3b67a0f5c17de.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4d782211be5f99014cc3b67a0f5c17de.png)'
- en: 'Figure 10: Federated learning principle: (a) Horizontal FL, (b) Vertical FL.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：联邦学习原理：（a）横向FL，（b）纵向FL。
- en: 'FL presents a paradigm shift in AI training, promoting collaboration while
    respecting data privacy. The working principle of HFL, and VFL is depicted on
    Figure [10](#S4.F10 "Figure 10 ‣ 4.3 FL-based ASR ‣ 4 Advanced ASR methods and
    applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey"), they cater to various data distribution scenarios, offering flexible
    solutions for decentralized and secure [ML](#Sx1.30.30.30). The application of
    these FL frameworks extends across diverse domains, promising improved model accuracy
    and privacy preservation. The first work introducing [FL](#Sx1.20.20.20) in [ASR](#Sx1.4.4.4)
    is presented in [[98](#bib.bib98)]. The authors introduced a FL platform that
    is easily generalizable, incorporating hierarchical optimization and a gradient
    selection algorithm to enhance training time and SR performance. Guliani et al.
    [[99](#bib.bib99)] proposed a strategy to compensate non-independent and identically
    distributed (non-IID) data in federated training of [ASR](#Sx1.4.4.4) systems.
    The proposed strategy involved random client data sampling, which resulted in
    a cost-quality trade-off. Zhu et al. [[100](#bib.bib100)] addressed also FL-based
    [ASR](#Sx1.4.4.4) in non-IID scenarios with personalized [FL](#Sx1.20.20.20).
    They introduced two approaches: adapting personalization layer-based FL for [ASR](#Sx1.4.4.4),
    involving local layers for personalized models, and proposing decoupled federated
    learning (DecoupleFL). DecoupleFL reduces computation on clients by shifting the
    computation burden to the server. Additionally, it communicates secure high-level
    features instead of model parameters, reducing communication costs, particularly
    for large models. In [[101](#bib.bib101)], the authors proposed a client-adaptive
    federated training scheme to mitigate data heterogeneity when training [ASR](#Sx1.4.4.4)
    models. Nguyen et al. [[102](#bib.bib102)] used FL to train an [ASR](#Sx1.4.4.4)
    model based on a wav2vec 2.0 model pre-trained by self supervision. Yang et al.
    [[103](#bib.bib103)] proposed a decentralized feature extraction approach in federated
    learning. This approach is built upon a quantum CNN (QCNN) composed of a quantum
    circuit encoder for feature extraction, and an RNN based end-to-end [AM](#Sx1.2.2.2).
    This framework takes advantage of the quantum learning progress to secure models
    and to avoid privacy leakage attacks. Gao et al. [[104](#bib.bib104)] tackled
    a challenging and realistic [ASR](#Sx1.4.4.4) federated experimental setup with
    clients having heterogeneous data distributions, featuring thousands of different
    speakers, acoustic environments, and noises. Their empirical study focused on
    attention-based [S2S](#Sx1.39.39.39) End-to-End (E2E) [ASR](#Sx1.4.4.4) models,
    evaluating three aggregation weighting strategies: standard FedAvg, loss-based
    aggregation, and a novel WER-based aggregation. Table [7](#S4.T7 "Table 7 ‣ 4.3
    FL-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition
    using Advanced Deep Learning Approaches: A survey") summarizes the most recent
    FL-based [ASR](#Sx1.4.4.4) techniques.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 'FL提出了人工智能培训的范式转变，推动协作并尊重数据隐私。 HFL和VFL的工作原理如图 [10](#S4.F10 "图10 ‣ 4.3 FL基础的ASR
    ‣ 4高级ASR方法和应用 ‣ 使用先进深度学习方法进行自动语音识别: 一项调查 ")所示，它们适用于各种数据分布场景，为分散和安全的[ML](#Sx1.30.30.30)提供了灵活的解决方案。这些FL框架的应用涵盖了不同的领域，承诺提高模型的准确性和隐私保护。在[[98](#bib.bib98)]中介绍了第一个引入[FL](#Sx1.20.20.20)在[ASR](#Sx1.4.4.4)中的工作。作者介绍了一个易于泛化的FL平台，该平台结合了分层优化和梯度选择算法，以增强训练时间和SR性能。Guliani等人[[99](#bib.bib99)]提出了一种策略，以补偿非独立和同分布（non-IID）的数据在联邦式训练的[ASR](#Sx1.4.4.4)系统中。提出的策略涉及随机客户数据抽样，导致成本-质量的权衡。Zhu等人[[100](#bib.bib100)]还在个性化的[FL](#Sx1.20.20.20)中针对非IID场景中的基于[ASR](#Sx1.4.4.4)的FL进行了介绍。他们提出了两种方法：调整基于FL的个性化层以用于[ASR](#Sx1.4.4.4)，包括用于个性化模型的本地层，并提出了分离的联邦学习（DecoupleFL）。DecoupleFL通过将计算负担转移到服务器来减少客户端上的计算。此外，它传输安全的高级特征，而不是模型参数，特别适用于大型模型的通信成本。在[[101](#bib.bib101)]中，作者提出了一种客户端自适应的联邦训练方案，以减轻在训练[ASR](#Sx1.4.4.4)模型时数据的异质性。Nguyen等人[[102](#bib.bib102)]使用FL训练了一个基于自我监督预训练的wav2vec
    2.0模型的[ASR](#Sx1.4.4.4)模型。Yang等人[[103](#bib.bib103)]提出了一种分散式特征提取方法在联邦学习中。此方法建立在量子CNN（QCNN）之上，由用于特征提取的量子电路编码器和基于RNN的端对端[AM](#Sx1.2.2.2)组成。该框架利用量子学习进步来保护模型，避免隐私泄露攻击。Gao等人[[104](#bib.bib104)]在具有异构数据分布的客户端端的具有挑战性和现实性的[ASR](#Sx1.4.4.4)联合实验设置中进行了研究。他们的实证研究集中在基于注意力的[S2S](#Sx1.39.39.39)端到端（E2E）[ASR](#Sx1.4.4.4)模型上，评估了三种聚合加权策略：标准FedAvg，基于损失的聚合以及一种新颖的基于WER的聚合。表
    [7](#S4.T7 "表7 ‣ 4.3 FL基础的ASR ‣ 4高级ASR方法和应用 ‣ 使用先进深度学习方法进行自动语音识别: 一项调查 ")总结了最新的FL基础[ASR](#Sx1.4.4.4)技术。'
- en: 'Table 7: Summary of recent proposed work in FL-based [ASR](#Sx1.4.4.4). All
    the schemes are suggested for [AM](#Sx1.2.2.2). The symbol ($\uparrow$) result
    increase, whereas ($\downarrow$) signifies result decrease. In cases where multiple
    scenarios are examined, only the top-performing outcome is mentioned.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：近期提出的基于FL的[ASR](#Sx1.4.4.4)工作总结。所有方案均建议用于[AM](#Sx1.2.2.2)。符号($\uparrow$)表示结果增加，而($\downarrow$)表示结果减少。在多个场景被考察的情况下，仅提及表现最佳的结果。
- en: '| Scheme | Based on | Speech recognition task | FL technique | Metric and result
    |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 方案 | 基于 | 语音识别任务 | FL技术 | 指标和结果 |'
- en: '| [[98](#bib.bib98)] | [S2S](#Sx1.39.39.39) | Improve ASR | FedAvg | WER =
    6% |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| [[98](#bib.bib98)] | [S2S](#Sx1.39.39.39) | 提升ASR | FedAvg | WER = 6% |'
- en: '| [[99](#bib.bib99)] | End-to-end RNN-T | ASR on non-IDD data | FedAvg | WER=
    |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| [[99](#bib.bib99)] | 端到端RNN-T | 在非IDD数据上的ASR | FedAvg | WER= |'
- en: '| [[100](#bib.bib100)] | CNN+Transformer extractor | ASR on non-IDD data |
    DecoupleFL | 2.3- 3.4% WER $\downarrow$ compared with FedAvg |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| [[100](#bib.bib100)] | CNN+Transformer提取器 | 在非IDD数据上的ASR | DecoupleFL | 与FedAvg相比WER下降2.3-3.4%$\downarrow$
    |'
- en: '| [[101](#bib.bib101)] | LSTM | ASR on non-IDD data | [CAFT](#Sx1.7.7.7) |
    WER = 15.13% |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| [[101](#bib.bib101)] | LSTM | 在非IDD数据上的ASR | [CAFT](#Sx1.7.7.7) | WER = 15.13%
    |'
- en: '| [[102](#bib.bib102)] | wav2vec 2.0 | Improve ASR | FedAvg | WER= 10.92% EER=
    5-20% |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| [[102](#bib.bib102)] | wav2vec 2.0 | 提升ASR | FedAvg | WER= 10.92% EER= 5-20%
    |'
- en: '| [[103](#bib.bib103)] | QCNN and RNN | Improve privacy-preservation in ASR
    | VFL | Accuracy = 95.12% |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| [[103](#bib.bib103)] | QCNN和RNN | 提升ASR中的隐私保护 | VFL | 准确率 = 95.12% |'
- en: '| [[104](#bib.bib104)] | [S2S](#Sx1.39.39.39) | ASR on heterogeneous data distributions
    | FedAvg | WER= 19.98-23.86% |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| [[104](#bib.bib104)] | [S2S](#Sx1.39.39.39) | 在异质数据分布上的ASR | FedAvg | WER=
    19.98-23.86% |'
- en: '| [[105](#bib.bib105)] | [S2S](#Sx1.39.39.39) | ASR on private and unlabelled
    user data. | [FedNST](#Sx1.19.19.19) | WER= 22.5% |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| [[105](#bib.bib105)] | [S2S](#Sx1.39.39.39) | 在私有和未标记用户数据上的ASR | [FedNST](#Sx1.19.19.19)
    | WER= 22.5% |'
- en: '| [[106](#bib.bib106)] | Wav2vec 2.0 and Whisper | ASR & KWS for child exploitation
    settings | FedAvg | WER = 11-25% |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| [[106](#bib.bib106)] | Wav2vec 2.0和Whisper | 儿童剥削环境中的ASR和KWS | FedAvg | WER
    = 11-25% |'
- en: '| [[107](#bib.bib107)] | Kaldi and backoff n-gram | Improve privacy-preservation
    in ASR | Merging models | WER= 17.7% |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| [[107](#bib.bib107)] | Kaldi和回退n-gram | 提升ASR中的隐私保护 | 合并模型 | WER= 17.7% |'
- en: '| [[108](#bib.bib108)] | TDNN | Improve privacy-preservation in ASR | Aggregation
    | EER = 1-2%. |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| [[108](#bib.bib108)] | TDNN | 提升ASR中的隐私保护 | 聚合 | EER = 1-2%。 |'
- en: '| [[109](#bib.bib109)] | Non-Streaming & Streaming Conformer | Reduce client
    ASR model size | Federated Dropout | 6-22% $\downarrow$ Client size model; 34-3%
    WER $\downarrow$ |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| [[109](#bib.bib109)] | 非流式和流式Conformer | 减少客户端ASR模型大小 | Federated Dropout
    | 客户端模型大小下降6-22%$\downarrow$；WER下降34-3%$\downarrow$ |'
- en: 4.4 RL-based ASR
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 基于RL的ASR
- en: \Ac
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: 'RL is a [ML](#Sx1.30.30.30) paradigm where an agent learns optimal decision-making
    by interacting with an environment. The agent receives feedback in the form of
    rewards or penalties, adapting its behavior to maximize cumulative reward over
    time through a trial-and-error process. [reinforcement learning](#Sx1.35.35.35)
    ([RL](#Sx1.35.35.35)) involves several key concepts, as defined in the following
    terms: Environment model, serving as a representation of contextual dynamics;
    State (s), denoting the current situation perceived by the agent; Observation
    (o), a subset of the state directly perceived by the agent; Action (a), the decision
    made by the agent in response to the environment; Policy ($\pi$), describing how
    the agent converts environmental conditions into actions; Agent, the entity making
    decisions based on current states and past experiences; Reward, numerical values
    assigned by the environment to the agent based on state-action interactions. Figure
    [11](#S4.F11 "Figure 11 ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    illustrate the principle of [RL](#Sx1.35.35.35).'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: RL是一个[ML](#Sx1.30.30.30)范式，其中一个智能体通过与环境交互来学习最佳决策。智能体以奖励或惩罚的形式获得反馈，调整其行为以通过试错过程最大化累积奖励。[强化学习](#Sx1.35.35.35)（[RL](#Sx1.35.35.35)）涉及几个关键概念，如下所述：环境模型，作为上下文动态的表示；状态(s)，表示智能体感知的当前情况；观察(o)，智能体直接感知的状态的子集；行动(a)，智能体对环境做出的决策；策略($\pi$)，描述智能体如何将环境条件转化为行动；智能体，根据当前状态和过去经验做出决策的实体；奖励，环境根据状态-行动交互分配给智能体的数值。图[11](#S4.F11
    "图11 ‣ 4.4 基于RL的ASR ‣ 4 高级ASR方法和应用 ‣ 使用先进深度学习方法的自动语音识别：综述")展示了[RL](#Sx1.35.35.35)的原理。
- en: '![Refer to caption](img/d950f54e56c9c73a22f77d9dbedbac92.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d950f54e56c9c73a22f77d9dbedbac92.png)'
- en: 'Figure 11: RL principle.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：RL 原理。
- en: Markov Decision Process (MDP) is a fundamental framework for dynamic and stochastic
    decision-making, characterized by state space $S$, action space $\mathbb{A}$,
    transition probabilities $\mathbb{P}$, and a reward function $R$. The primary
    objective in an MDP is to identify an optimal policy $\pi^{*}$ maximizing the
    expected discounted total reward over time, expressed as $\pi^{*}=\max_{\pi}\mathbb{E}_{\pi}[\sum_{t=0}^{T}\gamma^{t}r_{t}(s_{t},\pi(s_{t}))]$,
    where $\gamma$ is the discount factor. MDPs find extensive applications in addressing
    uncertainties in intelligent systems within dynamic wireless environments, including
    spectrum management, cognitive radios, and wireless security.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDP）是动态和随机决策的基本框架，其特点包括状态空间 $S$、动作空间 $\mathbb{A}$、转移概率 $\mathbb{P}$
    和奖励函数 $R$。MDP 的主要目标是识别一个最优策略 $\pi^{*}$，以最大化预期的折现总奖励，表达为 $\pi^{*}=\max_{\pi}\mathbb{E}_{\pi}[\sum_{t=0}^{T}\gamma^{t}r_{t}(s_{t},\pi(s_{t}))]$，其中
    $\gamma$ 是折扣因子。MDP 广泛应用于解决动态无线环境中智能系统的不确定性问题，包括频谱管理、认知无线电和无线安全。
- en: 'In the field of [ASR](#Sx1.4.4.4), [RL](#Sx1.35.35.35) has primarily been proposed
    to tackle discrepancies between training and testing phases. Two main discrepancies
    leading to potential performance deterioration have been identified: 1) The conventional
    use of the cross-entropy criterion maximizes log-likelihood during training, while
    performance is assessed by [WER](#Sx1.52.52.52), not log-likelihood; 2) The teacher-forcing
    method, which relies on ground truth during training, implies that the model has
    never encountered its own predictions before testing. [RL](#Sx1.35.35.35) addresses
    these discrepancies by bridging the gap between the training and testing phases.
    Several [RL](#Sx1.35.35.35)-based approaches for [ASR](#Sx1.4.4.4) have been proposed
    [[110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113),
    [114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117),
    [118](#bib.bib118), [119](#bib.bib119)]. For example, in [[110](#bib.bib110)],
    the authors introduced a [RL](#Sx1.35.35.35)-based optimization method for the
    [S2S](#Sx1.39.39.39)  [ASR](#Sx1.4.4.4) task called Self-critical sequence training
    (SCST). This method can be conceptualized as a sequential decision model, depicted
    in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods
    and applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey"). The entire encoder-decoder neural network is treated as an agent.
    At each time step $t$, the current state $s_{t}$ is formed by concatenating the
    acoustic feature $x_{t}$ and the previous prediction $Y_{t-1}$. The output token
    serves as the action, updating the generated hypotheses sequence. SCST associates
    training loss and [WER](#Sx1.52.52.52) using a WER-related reward function, calculating
    the reward $r_{t}$ at each token generation step by comparing it with the ground
    truth sequence $Y^{*}$. SCST uses the test-time beam search algorithm to sample
    hypotheses for reward normalization, assigning positive weights to high-reward
    hypotheses that outperform the current test-time system and negative weights to
    low-reward hypotheses. The framework is illustrated in Figure [12](#S4.F12 "Figure
    12 ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech
    Recognition using Advanced Deep Learning Approaches: A survey").'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ASR](#Sx1.4.4.4)领域，[RL](#Sx1.35.35.35)主要被提出来解决训练和测试阶段之间的不一致。已经确定了导致潜在性能下降的两个主要不一致：1）传统使用的交叉熵标准在训练期间最大化对数似然，而性能是通过[WER](#Sx1.52.52.52)而不是对数似然来评估的；2）教师强迫法在训练期间依赖于地面真相，意味着模型在测试之前从未遇到自己的预测。[RL](#Sx1.35.35.35)通过弥合训练和测试阶段之间的差距来解决这些不一致。已经提出了几种基于[RL](#Sx1.35.35.35)的[ASR](#Sx1.4.4.4)方法[[110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118),
    [119](#bib.bib119)]。例如，在[[110](#bib.bib110)]中，作者介绍了一种基于[RL](#Sx1.35.35.35)的优化方法，用于[S2S](#Sx1.39.39.39)[ASR](#Sx1.4.4.4)任务，称为自临界序列训练（SCST）。这种方法可以被概念化为一个序贯决策模型，如图[12](#S4.F12
    "Figure 12 ‣ 4.4基于RL的ASR ‣ 4高级ASR方法和应用 ‣ 使用先进深度学习方法的自动语音识别：一项调查")所示。整个编码器-解码器神经网络被视为代理。在每个时间步$t$，当前状态$s_{t}$由声学特征$x_{t}$和上一次预测$Y_{t-1}$连接而成。输出标记作为行动，更新生成的假设序列。SCST使用与WER相关的奖励函数关联训练损失和[WER](#Sx1.52.52.52)，通过将每个标记生成步骤的奖励$r_{t}$与地面真相序列$Y^{*}$进行比较来计算奖励。SCST使用测试时间波束搜索算法对假设进行抽样以进行奖励标准化，对表现优于当前测试时间系统的高奖励假设分配正权重，对低奖励假设分配负权重。该框架在图[12](#S4.F12
    "Figure 12 ‣ 4.4基于RL的ASR ‣ 4高级ASR方法和应用 ‣ 使用先进深度学习方法的自动语音识别：一项调查")中有所说明。
- en: 'In [ASR](#Sx1.4.4.4), RL has been primarily proposed to address mismatches
    between training and testing phases. Two main mismatches leading to potential
    performance degradation are identified: 1) The commonly used cross-entropy criterion
    maximizes log-likelihood during training, while performance is evaluated by WER,
    not log-likelihood; 2) The teacher-forcing method, relying on ground truth during
    training, implies that the model has never encountered its own predictions before
    testing. RL bridges the gap between the training and testing phases, addressing
    these mismatches.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ASR](#Sx1.4.4.4)中，RL主要被提出来解决训练和测试阶段之间的不匹配。已经确定了导致潜在性能下降的两个主要不匹配：1）通常使用的交叉熵标准在训练期间最大化对数似然，而性能是通过WER而不是对数似然来评估的；2）教师强迫法在训练期间依赖于地面真相，意味着模型在测试之前从未遇到自己的预测。RL弥合了训练和测试阶段之间的差距，解决了这些不匹配。
- en: 'Several RL-based [ASR](#Sx1.4.4.4) approaches have proposed [[110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118),
    [119](#bib.bib119)]. For instance, In [[110](#bib.bib110)], the authors presented
    a RL-based optimization method for [S2S](#Sx1.39.39.39)  [ASR](#Sx1.4.4.4) task
    called self-critical sequence training (SCST). This can be viewed as a sequential
    decision model as shown in Fig. [12](#S4.F12 "Figure 12 ‣ 4.4 RL-based ASR ‣ 4
    Advanced ASR methods and applications ‣ Automatic Speech Recognition using Advanced
    Deep Learning Approaches: A survey"). The whole encoder-decoder neural network
    can be viewed as an agent. In each time step $t$, acoustic feature $x_{t}$ and
    previous $Y_{t-1}$ prediction are concatenated as current state $s_{t}$. The output
    token is the action at that will update the generated hypotheses sequence. After
    comparing it with ground truth sequence $Y^{*}$, a reward $r_{t}$ of this time
    step is calculated. SCST associates the training loss and WER using WER-related
    reward function, which considers the intermediate reward at each token generation
    step. Furthermore, SCST utilizes the test-time beam search algorithm to sample
    a set of hypotheses for reward normalization. As a result, the high-reward hypotheses
    that outperform the current test-time system are given positive weights, while
    the low-reward hypotheses are given negative weights.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '一些基于强化学习的[ASR](#Sx1.4.4.4)方法已被提出[[110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112),
    [113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119)]。例如，在[[110](#bib.bib110)]中，作者提出了一种基于强化学习的优化方法，称为自我批判序列训练（SCST），用于[S2S](#Sx1.39.39.39)
    [ASR](#Sx1.4.4.4)任务。这可以被视为一个序列决策模型，如图[12](#S4.F12 "Figure 12 ‣ 4.4 RL-based ASR
    ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey")所示。整个编码器-解码器神经网络可以被视为一个智能体。在每个时间步$t$，声学特征$x_{t}$和之前的$Y_{t-1}$预测被连接成当前状态$s_{t}$。输出的令牌是更新生成的假设序列的动作。通过与真实序列$Y^{*}$进行比较，计算出该时间步的奖励$r_{t}$。SCST通过与WER相关的奖励函数将训练损失和WER关联起来，这考虑了每个令牌生成步骤的中间奖励。此外，SCST利用测试时间的束搜索算法采样一组假设进行奖励归一化。因此，超越当前测试系统的高奖励假设会被赋予正权重，而低奖励假设会被赋予负权重。'
- en: 'The entire encoder-decoder neural network is treated as an agent. At each time
    step $t$, the current state $s_{t}$ is formed by concatenating the acoustic feature
    $x_{t}$ and the previous prediction $Y_{t-1}$. The output token serves as the
    action, updating the generated hypotheses sequence. SCST associates training loss
    and WER using a WER-related reward function, calculating the reward $r_{t}$ at
    each token generation step by comparing it with the ground truth sequence $Y^{*}$.
    SCST uses the test-time beam search algorithm to sample hypotheses for reward
    normalization, assigning positive weights to high-reward hypotheses that outperform
    the current test-time system and negative weights to low-reward hypotheses. The
    framework is illustrated in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 RL-based ASR
    ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey").'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '整个编码器-解码器神经网络被视为一个智能体。在每个时间步$t$，当前状态$s_{t}$通过将声学特征$x_{t}$和之前的预测$Y_{t-1}$连接而成。输出的令牌作为动作，更新生成的假设序列。SCST通过与真实序列$Y^{*}$进行比较，在每个令牌生成步骤中计算奖励$r_{t}$，将训练损失和WER通过与WER相关的奖励函数关联起来。SCST使用测试时间的束搜索算法来采样假设进行奖励归一化，为超越当前测试系统的高奖励假设分配正权重，为低奖励假设分配负权重。该框架在图[12](#S4.F12
    "Figure 12 ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey")中进行了说明。'
- en: '![Refer to caption](img/b4af5b569eede20f04fbced21f432978.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b4af5b569eede20f04fbced21f432978.png)'
- en: 'Figure 12: SCST sequential decision model of [ASR](#Sx1.4.4.4) [[110](#bib.bib110)].'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：SCST 序列决策模型的[ASR](#Sx1.4.4.4) [[110](#bib.bib110)]。
- en: 'In [[111](#bib.bib111)], the authors developed a [RL](#Sx1.35.35.35) framework
    for speech recognition systems using the policy gradient method. They introduced
    a [RL](#Sx1.35.35.35) method within this framework, incorporating user feedback
    through hypothesis selection. Tjandra et al. [[112](#bib.bib112), [113](#bib.bib113)]
    also employed policy gradient [RL](#Sx1.35.35.35) to train a [S2S](#Sx1.39.39.39)  [ASR](#Sx1.4.4.4)
    model. In [[114](#bib.bib114)], the authors constructed a generic [RL](#Sx1.35.35.35)-based
    AutoML system. This system automatically optimizes per-layer compression ratios
    for a [SOTA](#Sx1.44.44.44) attention-based end-to-end [ASR](#Sx1.4.4.4) model,
    which consists of multiple [LSTM](#Sx1.26.26.26) layers. The compression method
    employed in this work is singular value decomposition (SVD) low-rank matrix factorization.
    The authors improved this approach by combining iterative compression with AutoML-based
    rank searching, achieving over 5 x [ASR](#Sx1.4.4.4) compression without degrading
    the WER [[115](#bib.bib115)]. Shen et al. [[116](#bib.bib116)] suggested employing
    [RL](#Sx1.35.35.35) to optimize a speech enhancement model based on recognition
    results, aiming to directly enhance [ASR](#Sx1.4.4.4) performance. AutoML-based
    low-rank factorization (LRF) achieves up to 3.7× speedup. In the shade of this,
    Mehrotra et al. in their work [[115](#bib.bib115)] propose an iterative AutoML-based
    LRF that employs [RL](#Sx1.35.35.35) for the iterative search, surpassing 5× compression
    without degrading [WERs](#Sx1.52.52.52), advancing [ASR](#Sx1.4.4.4). Table [8](#S4.T8
    "Table 8 ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey") summarizes
    the recent RL-based [ASR](#Sx1.4.4.4) techniques.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [[111](#bib.bib111)] 中，作者开发了一个用于语音识别系统的[RL](#Sx1.35.35.35)框架，采用策略梯度方法。他们在这个框架中引入了一种[RL](#Sx1.35.35.35)方法，通过假设选择整合用户反馈。Tjandra
    等人 [[112](#bib.bib112), [113](#bib.bib113)] 也利用策略梯度[RL](#Sx1.35.35.35)来训练一个[S2S](#Sx1.39.39.39)[ASR](#Sx1.4.4.4)模型。在
    [[114](#bib.bib114)] 中，作者构建了一个通用的基于[RL](#Sx1.35.35.35)的AutoML系统。该系统自动优化每层压缩比，以适应由多个[LSTM](#Sx1.26.26.26)层组成的[SOTA](#Sx1.44.44.44)基于注意力的端到端[ASR](#Sx1.4.4.4)模型。该工作的压缩方法是奇异值分解（SVD）低秩矩阵分解。作者通过结合迭代压缩和基于AutoML的秩搜索改进了这种方法，实现了超过5倍的[ASR](#Sx1.4.4.4)压缩，而没有降低WER
    [[115](#bib.bib115)]。Shen 等人 [[116](#bib.bib116)] 建议利用[RL](#Sx1.35.35.35)来优化基于识别结果的语音增强模型，旨在直接提升[ASR](#Sx1.4.4.4)性能。基于AutoML的低秩分解（LRF）实现了最高3.7×的加速。在此基础上，Mehrotra
    等人在他们的工作 [[115](#bib.bib115)] 中提出了一种迭代的基于AutoML的LRF，采用[RL](#Sx1.35.35.35)进行迭代搜索，实现了超过5×的压缩而不降低[WERs](#Sx1.52.52.52)，推动了[ASR](#Sx1.4.4.4)。表[8](#S4.T8
    "Table 8 ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey")总结了最近基于RL的[ASR](#Sx1.4.4.4)技术。'
- en: 'Table 8: Summary of recent proposed works in RL-based ASR. The symbol ($\uparrow$)
    result increase, whereas ($\downarrow$) signifies result decrease. In cases where
    multiple scenarios are examined, only the top-performing outcome is mentioned.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：总结了最近提出的基于RL的ASR工作。符号 ($\uparrow$) 表示结果增加，而 ($\downarrow$) 表示结果减少。在多种情境被考察的情况下，仅提及表现最佳的结果。
- en: '| Scheme | Model-based | ASR Tasks | RL technique | Metric and result |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 方案 | 基于模型的 | ASR 任务 | RL 技术 | 指标和结果 |'
- en: '| [[110](#bib.bib110)] | [S2S](#Sx1.39.39.39) Conformer | Improve ASR | Policy
    gradient | 8.7%-7.8% WER $\downarrow$ over Baseline model |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110)] | [S2S](#Sx1.39.39.39) Conformer | 改善 ASR | 策略梯度 | 8.7%-7.8%
    WER $\downarrow$ 相较于基线模型 |'
- en: '| [[111](#bib.bib111)] | DNN-HMM | Improve ASR for AM | Policy gradient | WER=23.82-25.43%
    |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| [[111](#bib.bib111)] | DNN-HMM | 改善 AM 的 ASR | 策略梯度 | WER=23.82-25.43% |'
- en: '| [[112](#bib.bib112)] | [S2S](#Sx1.39.39.39) | Improve ASR FOR am | Policy
    gradient | CER=6.10% |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| [[112](#bib.bib112)] | [S2S](#Sx1.39.39.39) | 改善 AM 的 ASR | 策略梯度 | CER=6.10%
    |'
- en: '| [[113](#bib.bib113)] | [S2S](#Sx1.39.39.39) | Improve ASR for AM | Policy
    gradient | CER=6.10% |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| [[113](#bib.bib113)] | [S2S](#Sx1.39.39.39) | 改善 AM 的 ASR | 策略梯度 | CER=6.10%
    |'
- en: '| [[114](#bib.bib114)] | End-to-end encoder-attention- decoder | Improve ASR
    Model compression for AM | Policy gradient | Up to $\sim$3x compression; WER=8.06%
    |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| [[114](#bib.bib114)] | 端到端编码器-注意力-解码器 | 改善 AM 的 ASR 模型压缩 | 策略梯度 | 最高 $\sim$3x
    压缩；WER=8.06% |'
- en: '| [[115](#bib.bib115)] | End-to-end encoder-attention- decoder | Improve ASR
    Model compression for AM | Policy gradient | Up to $\sim$5x compression; WER=8.19%
    |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| [[115](#bib.bib115)] | 端到端编码器-注意力-解码器 | 改善 AM 的 ASR 模型压缩 | 策略梯度 | 最高 $\sim$5x
    压缩；WER=8.19% |'
- en: '| [[116](#bib.bib116)] | CD-DNN-HMM AM & SRI LM | Speech enhancement for ASR
    for AM and LM | Q-learning | 12.40% and 19.23% CER $\downarrow$ at 5 and 0 dB
    SNR conditions. |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| [[116](#bib.bib116)] | CD-DNN-HMM AM & SRI LM | 用于 AM 和 LM 的 ASR 语音增强 | Q-learning
    | 5 和 0 dB SNR 条件下 CER 降低 12.40% 和 19.23% |'
- en: '| [[117](#bib.bib117)] | LSTM | Improve ASR for dialogue state tracking | DQN
    | Acc= 3.1%$\uparrow$ |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| [[117](#bib.bib117)] | LSTM | 改善对话状态跟踪的 ASR | DQN | 准确率 3.1% 提升 |'
- en: '| [[118](#bib.bib118)] | [S2S](#Sx1.39.39.39) | Improve ASR for AM | Policy
    gradient | CER=8.7% |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| [[118](#bib.bib118)] | [S2S](#Sx1.39.39.39) | 改善 AM 的 ASR | 策略梯度 | CER=8.7%
    |'
- en: '| [[119](#bib.bib119)] | Wav2vec 2.0 | Improve ASR for AM | Policy gradient
    | 4% WER $\downarrow$ |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| [[119](#bib.bib119)] | Wav2vec 2.0 | 改善 AM 的 ASR | 策略梯度 | WER 降低 4% |'
- en: DTL for acoustic models (AMs) Two types of deep learning-based AMs commonly
    used in ASR are the end-to-end model and the layered deep neural network-hidden
    Markov model (DNN-HMM) model. The DNN component is responsible for extracting
    high-level features, such as MFCCs and HMM lexical sequences, from acoustic signals.
    These features are then decoded into transcripts. The DNN takes acoustic characteristics
    as input and produces context-dependent lexical units (tri-phonemes) that are
    used as input for the downstream HMM component.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 对于声学模型（AMs）的 DTL，ASR 中常用的两种深度学习基 AM 是端到端模型和层叠深度神经网络-隐马尔可夫模型（DNN-HMM）模型。DNN 组件负责从声学信号中提取高层特征，如
    MFCC 和 HMM 词汇序列。这些特征随后被解码成转录本。DNN 将声学特征作为输入，并产生依赖上下文的词汇单元（三音素），这些词汇单元用作下游 HMM
    组件的输入。
- en: 'In contrast, the end-to-end model is a DNN-based technique that directly takes
    acoustic characteristics (features) as input and outputs the recognition rate
    without the need for separate components like the DNN and HMM. A typical end-to-end
    framework for voice recognition is illustrated in Figure [13](#S4.F13 "Figure
    13 ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech
    Recognition using Advanced Deep Learning Approaches: A survey"). The neural network
    in this framework generates embeddings from the input features, which are then
    passed through a stack of recurrent layers. These recurrent layers analyze patterns
    based on prior and current input information to produce the final output. The
    network is trained using back-propagation with the CTC loss [mridha2021study].
    Regarding domain transfer learning (DTL), three main strategies have been commonly
    employed with AMs.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，端到端模型是一种基于 DNN 的技术，直接将声学特征（特征）作为输入，并输出识别率，无需像 DNN 和 HMM 这样的单独组件。图 [13](#S4.F13
    "图 13 ‣ 4.4 RL-based ASR ‣ 4 高级 ASR 方法和应用 ‣ 基于先进深度学习方法的自动语音识别：综述") 中展示了一个典型的语音识别端到端框架。该框架中的神经网络从输入特征中生成嵌入，然后通过一系列递归层。这些递归层根据先前和当前的输入信息分析模式，以产生最终输出。网络使用
    CTC 损失 [mridha2021study] 通过反向传播进行训练。关于领域迁移学习（DTL），AMs 常用三种主要策略。
- en: '![Refer to caption](img/b3f26de3a4f0aeb8fb3b23f3112a2872.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b3f26de3a4f0aeb8fb3b23f3112a2872.png)'
- en: 'Figure 13: An example of end-to-end source model for DTL-based ASR [mridha2021study].'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：基于 DTL 的 ASR 端到端源模型示例 [mridha2021study]。
- en: 4.4.1 Feature normalisation based-DTL
  id: totrans-391
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 基于特征归一化的 DTL
- en: 'The concept behind linear transformation is to normalize speech characteristics
    through a linear mapping process. This involves adding a transformation network
    or layer to an existing network. The transformation network is an effective method
    for adapting neural networks. Typically, the last hidden layer is designed to
    act as a bottleneck, reducing the number of parameters to be adjusted by using
    fewer neurons. Figure [14](#S4.F14 "Figure 14 ‣ 4.4.1 Feature normalisation based-DTL
    ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech
    Recognition using Advanced Deep Learning Approaches: A survey") illustrates this
    architecture. The transformation network can be either a linear input network
    or a linear output network.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 线性变换的概念是通过线性映射过程对语音特征进行归一化。这涉及向现有网络中添加转换网络或层。转换网络是适应神经网络的有效方法。通常，最后一层隐藏层被设计为瓶颈，通过使用较少的神经元来减少需要调整的参数数量。图
    [14](#S4.F14 "图 14 ‣ 4.4.1 基于特征归一化的 DTL ‣ 4.4 RL-based ASR ‣ 4 高级 ASR 方法和应用 ‣
    基于先进深度学习方法的自动语音识别：综述") 说明了这种架构。转换网络可以是线性输入网络或线性输出网络。
- en: '![Refer to caption](img/e80ab26db2766ad3b7256f598ae2db8f.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e80ab26db2766ad3b7256f598ae2db8f.png)'
- en: 'Figure 14: The basics of transformation in DTL involve estimating the weights
    connected to the links within the dashed rectangles, while leaving the remaining
    weights unchanged. In the case of mono-task DTL, feature normalization is applied
    as shown in Figure (a). For multi-task DTL, feature normalization is also performed,
    as illustrated in Figure (b).'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：在DTL中的变换基础涉及估计连接到虚线矩形内的权重，同时保持其他权重不变。在单任务DTL的情况下，如图（a）所示应用特征归一化。对于多任务DTL，特征归一化也会执行，如图（b）所示。
- en: 'Assuming that the last hidden network (LHN) functions as a feature extractor
    and the output layer serves as a discriminative source model ($M_{S}$), the weights
    of the linear transformation matrix $W_{L}$ in the output layer correspond to
    the parameters of the target model, $M_{T}$. The representation of $M_{T}$ can
    be expressed as follows [huang2015maximum]:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 假设最后的隐藏网络（LHN）作为特征提取器，输出层作为判别源模型（$M_{S}$），那么输出层中线性变换矩阵$W_{L}$的权重对应于目标模型$M_{T}$的参数。$M_{T}$的表示可以表示为如下[huang2015maximum]：
- en: '|  | $\mathrm{M_{T}=softmax(W_{L}M_{S})}.$ |  | (4) |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{M_{T}=softmax(W_{L}M_{S})}.$ |  | (4) |'
- en: 'The activation of the last hidden layer in $M_{S}$ can serve as the extracted
    feature representation for the hidden layers in $M_{T}$. Transforming the model
    parameters using a transformation matrix $W_{LHN}$ to generate an adapted set
    of model parameters is equivalent to incorporating an extended last hidden layer
    after the existing last hidden layer [huang2015maximum]:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: $M_{S}$中最后一个隐藏层的激活可以作为$M_{T}$中隐藏层的提取特征表示。使用变换矩阵$W_{LHN}$变换模型参数以生成一组适应后的模型参数，相当于在现有最后隐藏层之后加入扩展的最后隐藏层[huang2015maximum]：
- en: '|  | $\mathrm{M_{T}=softmax(W_{LHN}W_{L}M_{S})}.$ |  | (5) |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{M_{T}=softmax(W_{LHN}W_{L}M_{S})}.$ |  | (5) |'
- en: 'Several ASR research studies have utilized linear transformation strategies
    to improve performance. In one example [elaraby2016deep], the authors aimed to
    enhance a computer-aided language learner (CAPL) system for teaching Arabic pronunciation
    for Quran recitation regulations. They implemented various improvements, including
    speaker adaptive training (SAT), integrating a hybrid DNN-HMM model, combining
    the hybrid DNN with minimum phone error (MPE), and using a grammar-based decoding
    graph during testing. Another study [mimura2016joint] employed a multi-target
    learning approach to optimize the output of a denoising auto-encoder (DAE) and
    the input of a DNN. The DAE was trained in the first stage to reduce input-related
    errors propagated to the DNN. Then, a unified network of DAE and DNN was fine-tuned
    for phone state ASR, with an additional target of input voice augmentation applied
    to the DAE. Additionally, in [ma2017approaches], an adaptation layer was introduced
    for fine-tuning, and non-linearity was incorporated to learn a more complex function
    than a linear transformation in the softmax layer. During fine-tuning, adjustments
    were made to the cluster softmax and NNadapt layers while keeping the other layers
    unchanged. Table [6](#S4.T6 "Table 6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based ASR
    ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey") provides a summary of additional
    schemes that employ linear transformation techniques, along with their performance
    details.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '一些ASR研究利用了线性变换策略以提高性能。例如，[elaraby2016deep]中，作者旨在增强一个计算机辅助语言学习者（CAPL）系统，用于教授阿拉伯语发音规则。他们实施了各种改进，包括说话者自适应训练（SAT）、集成混合DNN-HMM模型、将混合DNN与最小语音错误（MPE）相结合，并在测试期间使用基于语法的解码图。另一项研究[mimura2016joint]采用了多目标学习方法来优化去噪自动编码器（DAE）的输出和DNN的输入。DAE在第一阶段进行了训练，以减少传递到DNN的输入相关错误。然后，DAE和DNN的统一网络被微调用于电话状态ASR，并在DAE上应用了输入语音增强的附加目标。此外，在[ma2017approaches]中，引入了一个适应层用于微调，并将非线性融入，以学习比softmax层中的线性变换更复杂的函数。在微调过程中，调整了cluster
    softmax和NNadapt层，而其他层保持不变。表[6](#S4.T6 "Table 6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based
    ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey")提供了使用线性变换技术的其他方案的总结，以及其性能细节。'
- en: 4.4.2 Conservative training for DTL
  id: totrans-400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 DTL的保守训练
- en: 'The conservative training approach is commonly used for accent adaptation in
    ASR. It is an efficient method that requires only a limited amount of spoken data
    to achieve satisfactory results. However, it can lead to an excessive number of
    parameters, which may disrupt the model’s structure. To address this, KL-Divergence
    (KLD) is widely employed as a DNN-based DTL algorithm for ASR. KLD regularization
    provides a mathematical framework for DL training, aiming to minimize the loss
    and make the output distributions of the source model $M_{S}$ and target model
    $M_{T}$ more similar. The KL-divergence prevents overfitting and helps to keep
    the adapted $M_{T}$ close to the domain of $M_{S}$  [[89](#bib.bib89), [90](#bib.bib90)].
    In terms of model-based DTL with KLD-regularization, assuming the loss functions
    for training DNNs in the source and target domains are $\mathbb{D}_{S}$ and $\mathbb{D}_{T}$,
    respectively, the conservative approach can be summarized mathematically as follows
    [[89](#bib.bib89), [90](#bib.bib90)]:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 保守训练方法通常用于 ASR 中的口音适配。这是一种高效的方法，仅需要有限的语音数据即可实现令人满意的结果。然而，它可能会导致参数过多，从而扰乱模型的结构。为了解决这个问题，KL
    散度（KLD）被广泛应用于 DNN 基于的 DTL 算法中。KLD 正则化为深度学习训练提供了数学框架，旨在最小化损失，并使源模型 $M_{S}$ 和目标模型
    $M_{T}$ 的输出分布更加相似。KL 散度防止了过拟合，并有助于保持适应后的 $M_{T}$ 靠近 $M_{S}$ 的领域 [[89](#bib.bib89),
    [90](#bib.bib90)]。在具有 KLD 正则化的基于模型的 DTL 中，假设在源领域和目标领域训练 DNN 的损失函数分别是 $\mathbb{D}_{S}$
    和 $\mathbb{D}_{T}$，则保守方法可以数学上总结如下 [[89](#bib.bib89), [90](#bib.bib90)]：
- en: '|  | $\mathbb{D}_{T}^{KLD}=(1-\rho)\mathbb{D}_{S}+\frac{\rho}{N}\sum P_{S}(x_{T}/x_{T})logP_{T}(y_{T}/x_{T}),$
    |  | (6) |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{D}_{T}^{KLD}=(1-\rho)\mathbb{D}_{S}+\frac{\rho}{N}\sum P_{S}(x_{T}/x_{T})logP_{T}(y_{T}/x_{T}),$
    |  | (6) |'
- en: 'In the context of DTL-based ASR, the conservative training approach with KLD-regularization
    is utilized to build a target AM. This approach involves training DNNs on speech
    samples collected from the target domain $\mathbb{D}_{T}$, denoted as ($x_{T},y_{T}$),
    where $N$ represents the number of speech samples in $\mathbb{D}_{T}$. The hyper-parameter
    $\rho$ controls the transfer ratio from the source domain $\mathbb{D}_{S}$. An
    example of applying this DTL-based ASR approach with conservative training and
    KLD-regularization can be found in [[89](#bib.bib89)] for building a target AM.
    Additionally, in [[90](#bib.bib90)], both KLD and LHN techniques were employed
    for speaker adaptation using a pre-trained seq2seq ASR model. Table [6](#S4.T6
    "Table 6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based ASR ‣ 4 Advanced ASR methods and
    applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey") summarizes other techniques that employ conservative training along
    with their corresponding performance details.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '在基于 DTL 的 ASR 的背景下，使用带有 KLD 正则化的保守训练方法来构建目标 AM。这种方法涉及在来自目标领域 $\mathbb{D}_{T}$
    的语音样本上训练 DNN，表示为 ($x_{T},y_{T}$)，其中 $N$ 表示 $\mathbb{D}_{T}$ 中的语音样本数量。超参数 $\rho$
    控制从源领域 $\mathbb{D}_{S}$ 的转移比例。在 [[89](#bib.bib89)] 中可以找到一个应用此 DTL 基于 ASR 的保守训练和
    KLD 正则化的方法的示例，用于构建目标 AM。此外，在 [[90](#bib.bib90)] 中，KLD 和 LHN 技术都被用于利用预训练的 seq2seq
    ASR 模型进行说话人适配。表 [6](#S4.T6 "Table 6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based ASR
    ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey") 总结了其他采用保守训练的技术及其对应的性能细节。'
- en: 4.4.3 Subspace-based DTL
  id: totrans-404
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3 基于子空间的 DTL
- en: 'Subspace-based DTL approaches in ASR utilize unsupervised techniques such as
    PCA, SVD, and NMF for data dimensionality reduction. These techniques aim to identify
    a subspace of model parameters or transformations that capture essential information.
    PCA maps high-dimensional data to lower-dimensional subspaces while preserving
    correlation and maximizing variance. SVD, similar to PCA, condenses networks by
    selecting a specific number of singular values. However, SVD introduces nonlinearity
    in the weight matrix, which can lead to information loss when constructing a linear
    projection layer. NMF algorithms require at least one nonnegative matrix and express
    the target matrix as a weighted sum of base matrix columns, making it more interpretable
    than SVD and PCA. In the study [[88](#bib.bib88)], Convex Nonnegative Matrix Factorization
    (CNMF), a variant of NMF, is employed to extract high-level features. Subsequently,
    DTL is applied to both high and low-level features through multilingual training
    and multi-task learning. These techniques yield significant performance improvements
    compared to state-of-the-art ASR studies. Other studies utilizing subspace techniques
    and their respective performance details can be found in Table [6](#S4.T6 "Table
    6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey").'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '基于子空间的DTL方法在ASR（自动语音识别）中利用无监督技术，如PCA、SVD和NMF进行数据维度减少。这些技术旨在识别能够捕获关键信息的模型参数或变换的子空间。PCA将高维数据映射到低维子空间，同时保持相关性并最大化方差。SVD与PCA类似，通过选择特定数量的奇异值来压缩网络。然而，SVD在权重矩阵中引入了非线性，这可能在构建线性投影层时导致信息丢失。NMF算法需要至少一个非负矩阵，并将目标矩阵表示为基础矩阵列的加权和，这使得NMF比SVD和PCA更具可解释性。在研究[[88](#bib.bib88)]中，采用了NMF的一个变体——凸非负矩阵分解（CNMF）来提取高层次特征。随后，通过多语言训练和多任务学习，将DTL应用于高层次和低层次特征。这些技术相比于最先进的ASR研究，显著提高了性能。使用子空间技术及其各自性能细节的其他研究可以在表[6](#S4.T6
    "Table 6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based ASR ‣ 4 Advanced ASR methods and
    applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey")中找到。'
- en: LDA-based DTL In the case of discrete data collection, generative probabilistic
    models, such as the latent Dirichlet allocation (LDA), are employed. Typically,
    LDA is a hierarchical Bayesian model with three levels, where each item in a collection
    is represented as a finite mixture over a set of underlying topics. Each topic,
    in turn, is modeled as an infinite mixture of topic probabilities. To capture
    the relationship between words and create language models for specific documents,
    topic model-based techniques, such as LDA, have been utilized as described in
    [song2019topic]. In the work presented in [hentschel2018feature], LDA features
    are transformed using a linear layer consisting of a weight matrix and a bias
    vector. These transformed features are then utilized as inputs in the LHN (Local
    Hidden Node) during the training and evaluation of the network.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在离散数据收集的情况下，使用生成概率模型，如潜在狄利克雷分配（LDA）。通常，LDA是一个具有三层的层次贝叶斯模型，其中集合中的每个项目被表示为一组基础主题的有限混合。每个主题又被建模为主题概率的无限混合。为了捕获词语之间的关系并为特定文档创建语言模型，已利用基于主题模型的技术，如LDA，如[歌2019主题]中所述。在[hentschel2018feature]中展示的工作中，LDA特征通过包含权重矩阵和偏置向量的线性层进行变换。这些变换后的特征随后被用作LHN（局部隐藏节点）在网络的训练和评估中的输入。
- en: 4.4.4 NNLM-based DTL
  id: totrans-407
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.4 基于NNLM的DTL
- en: 'Neural network language models (NN-LMs) generally outperform count-based LM
    models in automatic speech recognition (ASR) across various tasks. Specifically,
    when applied to N-best rescoring, NN-LMs achieve WER as mentioned in [hentschel2018feature].
    Adapting NN-LMs to new domains poses a research challenge, and current approaches
    can be categorized as either model-based or feature-based adaptations. In feature-based
    adaptation, auxiliary features are incorporated into the input of an NN-LM, while
    model-based adaptation involves fine-tuning and adapting network layers. The authors
    of [[91](#bib.bib91)] propose a recurrent neural network-based LM (RNN-LM) approach
    where both types of adaptation are explored. As an illustrative example, Figures
    [15](#S4.F15 "Figure 15 ‣ 4.4.4 NNLM-based DTL ‣ 4.4 RL-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") (a) and (b) provide detailed explanations of the
    adopted RNN-based DTL in the study. Furthermore, the authors in [[92](#bib.bib92)]
    suggest a DNN-based model for LM modification in ASR, employing a factorized time-delay
    neural network (TDNN-F). Specifically, the TDNN-F model is trained using a combination
    of cross-entropy and lattice-free maximum mutual information objective functions
    (LF-MMI). The effectiveness of TDNN-F is demonstrated in the recognition of English
    child speech [[92](#bib.bib92)].'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '神经网络语言模型（NN-LMs）通常在各种任务的自动语音识别（ASR）中优于基于计数的语言模型（LM）。具体来说，当应用于N-best重评分时，NN-LMs实现了WER，如在[hentschel2018feature](https://example.org)中提到的。将NN-LMs适应于新领域是一个研究挑战，目前的方法可以归类为模型基础的或特征基础的适应。在特征基础的适应中，辅助特征被纳入NN-LM的输入，而模型基础的适应涉及微调和调整网络层。[[91](#bib.bib91)]的作者提出了一种基于递归神经网络的语言模型（RNN-LM）方法，探索了这两种适应类型。作为一个说明性的例子，图[15](#S4.F15
    "Figure 15 ‣ 4.4.4 NNLM-based DTL ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods
    and applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey")（a）和（b）详细说明了该研究中采用的基于RNN的DTL。此外，[[92](#bib.bib92)]的作者建议了一种用于ASR中LM修改的基于DNN的模型，采用了分解时间延迟神经网络（TDNN-F）。具体而言，TDNN-F模型使用交叉熵和无晶格最大互信息目标函数（LF-MMI）的组合进行训练。TDNN-F在英语儿童语音识别中的有效性在[[92](#bib.bib92)]中得到了证明。'
- en: '![Refer to caption](img/2c2171b391d75043b013d096ab808962.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2c2171b391d75043b013d096ab808962.png)'
- en: 'Figure 15: DTL-based RNNLM with different adaptation techniques, where the
    out-of-vocabulary (OOV) node represents an input word that does not belong to
    the specified vocabulary but can be included in the input. Similarly, out-of-shortlist
    (OOS) nodes can also be included in the output: (a) RNNLM with LHN adaptation
    layer. (b) RNNLM with feature-based adaptation layer.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：不同适应技术的基于DTL的RNNLM，其中词汇外（OOV）节点表示不属于指定词汇表的输入词，但可以包含在输入中。同样，词汇短缺（OOS）节点也可以包含在输出中：（a）具有LHN适应层的RNNLM。（b）具有特征基础适应层的RNNLM。
- en: LSTM-based DTL Generally, the NNLM models used in ASR are still trained on a
    sentence-level corpus, despite the attempts to train them at the document level.
    This is due to various factors; for example, a more extended context may not be
    relevant for enhancing next-word prediction in conventional ASR systems. It is
    also challenging to gather training data representing extended session contexts
    in many conversational circumstances. Long-span models are becoming more common
    in scenarios where they are beneficial. Long-span models will likely help scenarios,
    such as transcriptions of conversations and meetings, human-to-human communication,
    and document production by voice [parthasarathy2019long]. LSTM models are widely
    employed, and their architectures are well-suited to variable-length sequences.
    Therefore, they can exploit extreme long-range dependencies without using n-gram
    approximation. For instance, by employing equal context, the authors in [tuske2018investigation]
    demonstrate that the deep 4-gram LSTM outperforms big interpolated count models
    by performing considerably better backing off and smoothing. In another example,
    the central part of a shared encoder is constructed using BLSTM [[94](#bib.bib94)].
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 LSTM 的 DTL 通常，尽管有尝试在文档级别进行训练，但 ASR 中使用的 NNLM 模型仍然是在句子级语料库上训练的。这是由于多种因素，例如，更长的上下文可能与传统
    ASR 系统中增强下一词预测无关。在许多对话情况下，收集代表扩展会话上下文的训练数据也很困难。在有利的场景中，长跨度模型变得越来越常见。长跨度模型可能有助于诸如对话和会议的转录、人际沟通以及通过语音生成文档等场景
    [parthasarathy2019long]。LSTM 模型被广泛使用，其架构非常适合变长序列。因此，它们可以在不使用 n-gram 近似的情况下利用极端的长距离依赖。例如，通过使用相同的上下文，文献
    [tuske2018investigation] 中的作者展示了深度 4-gram LSTM 比大 interpolated count 模型表现更好，回退和光滑效果显著提高。在另一个例子中，共享编码器的核心部分使用了
    BLSTM [[94](#bib.bib94)]。
- en: Cross-domain ASR
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 跨领域 ASR
- en: 4.4.5 Cross-language DTL
  id: totrans-413
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.5 跨语言 DTL
- en: 'Cross-language DTL is a particular application of cross-modality DTL. It serves
    as a commonly employed method for constructing ASR models for low resource languages
    by leveraging models trained on other languages. This approach is based on the
    assumption that phoneme features can be shared across languages. Additionally,
    a generic ASR model can be adapted to a specific narrow domain using DTL techniques.
    To address the issue of data sparsity, various knowledge transfer methods are
    explored in [liu2019investigation] with the assistance of high-resource languages.
    These methods include DTL and fine-tuning, where a well-trained neural network
    initializes the parameters of the LHN. Progressive neural networks (Prognets)
    are also examined, as they are resistant to the forgetting effect and excel at
    knowledge transfer due to the presence of lateral connections in the network architecture.
    Furthermore, the utilization of cross-lingual DNNs is explored, where bottleneck
    features are extracted to enhance the effectiveness of the ASR system. Recent
    approaches related to ASR using cross-language DTL and their respective performances
    are summarized in Tables [6](#S4.T6 "Table 6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based
    ASR ‣ 4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using
    Advanced Deep Learning Approaches: A survey") and [9](#S4.T9 "Table 9 ‣ 4.4.5
    Cross-language DTL ‣ 4.4 RL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey").'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '跨语言 DTL 是跨模态 DTL 的一个特定应用。它作为一种常用的方法，通过利用在其他语言上训练的模型来构建低资源语言的 ASR 模型。这种方法基于一个假设，即音素特征可以跨语言共享。此外，通用
    ASR 模型可以使用 DTL 技术适应特定的狭域。为了应对数据稀疏问题，文献 [liu2019investigation] 探索了多种知识迁移方法，借助高资源语言。这些方法包括
    DTL 和微调，其中一个训练良好的神经网络初始化 LHN 的参数。渐进神经网络（Prognets）也被研究，因为它们对遗忘效应具有抗性，并且由于网络架构中的横向连接而在知识迁移方面表现出色。此外，还探索了跨语言
    DNN 的应用，其中提取瓶颈特征以增强 ASR 系统的有效性。最近涉及跨语言 DTL 的 ASR 方法及其各自性能总结在表 [6](#S4.T6 "Table
    6 ‣ 4.2.2 Language domain ‣ 4.2 TL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    和 [9](#S4.T9 "Table 9 ‣ 4.4.5 Cross-language DTL ‣ 4.4 RL-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") 中。'
- en: 'Table 9: A summary of the recent ASR-based cross-language DTL technique. Whereas
    the marks ($\uparrow$) and ($\downarrow$) indicate improvement and reduction,
    respectively. If many scenarios has been conducted in one metric, only the best
    result is mentioned.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：近期基于ASR的跨语言DTL技术的总结。其中标记（$\uparrow$）和（$\downarrow$）分别表示改进和减少。如果一个指标有多个场景，只有最佳结果被提及。
- en: '| Scheme | Model-based | ASR Tasks ($\mathbb{T}_{T}$) | Characteristic | Performance
    |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 方案 | 基于模型 | ASR任务（$\mathbb{T}_{T}$） | 特点 | 性能 |'
- en: '| [yusuf2019low] | EDML | Framework of performing the DTL that reduces the
    impact of the prevalence of out-of vocabulary terms. | query-by-example task |
    74% TWV$\uparrow$ |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| [yusuf2019low] | EDML | 执行DTL的框架，减少词汇外术语的影响。 | 例句查询任务 | 74% TWV$\uparrow$
    |'
- en: '| [liu2019investigation] | Prognets | Improving ASR scheme quality by overcoming
    the data sparsity problems by means of high-resource languages. | Fine-tuning
    LHN adapt. | 38.6% WER$\downarrow$ |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| [liu2019investigation] | Prognets | 通过利用高资源语言克服数据稀疏问题，提高ASR方案质量。 | 微调LHN适应
    | 38.6% WER$\downarrow$ |'
- en: '| [feng2019low] | FNN and CNN architectures | Indo-European speech samples
    used to improve the identification of African languages. | Using PLP coeff. Fine-tuning
    | 2.1% EER$\downarrow$ |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| [feng2019low] | FNN和CNN架构 | 使用印欧语系语音样本来提高对非洲语言的识别。 | 使用PLP系数进行微调 | 2.1% EER$\downarrow$
    |'
- en: '| [sahraeian2018cross] | DNN | Weighted averaging schemes are used to combine
    the ensemble’s constituents, with the combination weights being trained to minimize
    the cross-entropy objective function. | Weights interpolation | 7.7% WER$\downarrow$
    |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| [sahraeian2018cross] | DNN | 使用加权平均方案来组合集成的组成部分，组合权重经过训练以最小化交叉熵目标函数。 | 权重插值
    | 7.7% WER$\downarrow$ |'
- en: '| [wilkinson2020semi] | CNN-GMM-HMM | Fully-automatic segmentation, semi-supervised
    training of ASR systems for five-lingual code-switched speech | Semi-supervised
    | 1.1 % WER$\downarrow$ |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| [wilkinson2020semi] | CNN-GMM-HMM | 全自动分割，五语言代码切换语音的半监督ASR系统训练 | 半监督 | 1.1%
    WER$\downarrow$ |'
- en: '4.4.6 DTL-based ASR for emotion recognition:'
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.6 基于DTL的情感识别ASR：
- en: 'Combining language information with acoustic data has been shown to enhance
    the accuracy of speech emotion recognition (SER). Therefore, integrating both
    systems can be advantageous to enhance the capability of ASR systems to handle
    emotional speech and provide linguistic input to SER systems. Figure [16](#S4.F16
    "Figure 16 ‣ 4.4.6 DTL-based ASR for emotion recognition: ‣ 4.4 RL-based ASR ‣
    4 Advanced ASR methods and applications ‣ Automatic Speech Recognition using Advanced
    Deep Learning Approaches: A survey") illustrates a hybrid ASR-SER system [fayek2016deep],
    where a spectrogram is inputted into shared convolutional layers, followed by
    specialized layers that have shared levels to facilitate interaction between the
    two systems. This integration allows for improved performance in handling emotional
    speech and leveraging linguistic features.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '结合语言信息和声学数据已被证明能够提高语音情感识别（SER）的准确性。因此，将这两种系统结合起来可以提升ASR系统处理情感语音的能力，并为SER系统提供语言输入。图
    [16](#S4.F16 "图16 ‣ 4.4.6 基于DTL的情感识别ASR: ‣ 4.4 RL-based ASR ‣ 4 高级ASR方法与应用 ‣ 使用高级深度学习方法的自动语音识别:
    综述") 展示了一个混合ASR-SER系统 [fayek2016deep]，其中一个声谱图被输入到共享卷积层中，然后经过专门的层，这些层具有共享级别以促进两个系统之间的交互。这种整合允许在处理情感语音和利用语言特征方面表现得更好。'
- en: '![Refer to caption](img/be0a4f56dcdad101d578426c2822fbf3.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/be0a4f56dcdad101d578426c2822fbf3.png)'
- en: 'Figure 16: A linguistic-paralinguistic hybrid system [fayek2016deep]. (a) ASR-SER
    Hybrid system. (b) An example of DTL-based ASR-SER hybrid system.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：语言-副语言混合系统 [fayek2016deep]。（a）ASR-SER混合系统。（b）基于DTL的ASR-SER混合系统示例。
- en: The approach presented in [tits2018asr] utilizes the internal representation
    of a speech-to-text system to explore the connection between valence/arousal and
    different modalities through the use of DTL. A speech-to-text or ASR system learns
    to map audio speech signals to their corresponding transcriptions. By employing
    DTL, the proposed method can estimate valence and arousal using features learned
    from an ASR task. This approach offers the advantage of combining large datasets
    of speech with transcriptions with smaller datasets annotated with emotional dimensions.
    In a similar vein, the work described in [ananthram2020multi] fine-tunes a TDNN-based
    speaker recognition model for the task of emotion detection using the crema-D
    multi-modal emotion dataset and canonical label clustering. By adapting the model
    using fine-tuning, the authors aim to improve its performance on emotion detection.
    The study presented in [boateng2020speech] focuses on extracting features from
    audio segments with extreme positive and negative ratings, as well as the ending
    of the audio. They employ the peak-end rule and a DTL approach to extract acoustic
    features. The authors utilize a pre-trained CNN speech model called YAMNet and
    a linear SVM for binary classification of partner valence.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在[tits2018asr](https://example.org)中提出的方法利用了语音到文本系统的内部表示，通过使用DTL探索情感价值/唤醒度与不同模态之间的关系。语音到文本或ASR系统学习将音频语音信号映射到其对应的转录文本。通过采用DTL，所提出的方法可以利用从ASR任务中学习到的特征来估计情感价值和唤醒度。这种方法的优势在于将大规模的语音数据集与转录文本结合起来，同时还可以使用带有情感维度的小数据集。类似地，[ananthram2020multi](https://example.org)的工作通过使用crema-D多模态情感数据集和标准标签聚类，对基于TDNN的说话人识别模型进行微调，以进行情感检测。通过微调模型，作者旨在提高其情感检测性能。[boateng2020speech](https://example.org)的研究集中于从具有极端正面和负面评分的音频片段以及音频结束部分提取特征。他们采用了峰值-结束规则和DTL方法来提取声学特征。作者利用了一个名为YAMNet的预训练CNN语音模型和一个线性SVM来对合作伙伴的情感价值进行二分类。
- en: 4.4.7 Cross-corpus SER (CC-SER)
  id: totrans-427
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.7 跨语料库情感识别（CC-SER）
- en: In the context of Speech Emotion Recognition (SER), it is typically assumed
    that speech utterances in the training and testing domains are recorded under
    the same conditions. However, in real-world scenarios, speech data is often collected
    from diverse environments or devices, resulting in a domain discrepancy that adversely
    affects recognition performance. As a solution, researchers have recently investigated
    the problem of cross-corpus SER (CC-SER) and explored various DTL models.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在语音情感识别（SER）的背景下，通常假设训练和测试领域中的语音样本是在相同条件下录制的。然而，在现实场景中，语音数据常常来自不同的环境或设备，从而导致领域间的不一致，进而影响识别性能。为了解决这个问题，研究人员最近研究了跨语料库情感识别（CC-SER）的问题，并探索了各种DTL模型。
- en: For example, [song2019transfer] proposes a transfer linear subspace learning
    (TLSL) scheme to develop a CC-SER framework. This approach facilitates the learning
    of shared feature space for both the source and target domains. The similarity
    between different corpora is estimated using a nearest-neighbor graph algorithm.
    Additionally, a feature grouping method is devised to partition emotional features
    into highly transferable parts (HTP) and low transferable parts (LTP).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[song2019transfer](https://example.org)提出了一种转移线性子空间学习（TLSL）方案，以开发一个CC-SER框架。这种方法有助于为源领域和目标领域学习共享特征空间。不同语料库之间的相似性使用最近邻图算法来估计。此外，还设计了一种特征分组方法，将情感特征划分为高可转移部分（HTP）和低可转移部分（LTP）。
- en: In the case of unsupervised CC-SER examined in [liu2018unsupervised], only the
    training data is annotated. The authors introduce a domain-adaptive subspace learning
    (DoSL) technique to learn a projection matrix that transforms the source and target
    speech data from the initial domain to the labeled domain. This allows the classifier
    trained on the labeled source domain data to effectively predict the emotional
    states of the unlabeled target domain data. Furthermore, [liu2021transfer] presents
    an improvement to the DoSL-based CC-SER method by introducing transfer subspace
    learning (TRaSL). In another study by [luo2019cross], a semi-supervised CC-SER
    approach is proposed using non-negative matrix factorization (NMF). This approach
    incorporates training corpus labels into NMF and seeks a latent low-rank feature
    space where the differences in conditional and marginal distributions between
    the two corpora can be simultaneously minimized.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在[liu2018unsupervised]中考察的无监督CC-SER情况下，只有训练数据被标注。作者引入了一种领域自适应子空间学习（DoSL）技术，以学习一个投影矩阵，将源领域和目标领域的语音数据从初始领域转换到标记领域。这使得在标记源领域数据上训练的分类器能够有效地预测未标记目标领域数据的情感状态。此外，[liu2021transfer]通过引入转移子空间学习（TRaSL）对基于DoSL的CC-SER方法进行了改进。在[luo2019cross]的另一项研究中，提出了一种使用非负矩阵分解（NMF）的半监督CC-SER方法。这种方法将训练语料库标签纳入NMF，并寻求一个潜在的低秩特征空间，在该空间中，可以同时最小化两个语料库之间的条件和边际分布的差异。
- en: Advancing further, [zhang2019transfer] proposes a transfer sparse discriminant
    subspace learning (TSDSL) method to discover a shared feature subspace among multiple
    corpora. This is achieved by incorporating the $\ell_{2,1}$-norm penalty and discriminative
    learning, facilitating the identification of the most discriminative characteristics
    across the corpora. Similarly, [luo2020nonnegative] introduces a non-negative
    matrix factorization-based transfer subspace learning (NMFTSL) scheme. The goal
    is to minimize the distances between the marginal and conditional distributions
    in the common subspace. The distances are estimated using the maximum mean discrepancy
    (MMD) criterion. In [zhang2021cross], a joint transfer subspace learning and regression
    (JTSLR) technique is employed. It learns a latent subspace using discriminative
    MMD as the discrepancy metric, followed by modeling the relationships between
    features and annotations using a regression function in the latent subspace. A
    label graph is utilized to enhance knowledge transfer from the source domain (SD)
    data to the target domain (TD) data. Similarly, [chen2019target] presents a target-adapted
    subspace learning (TaSL) approach for CC-SER. It aims to find a projection subspace
    that enables more accurate regression of labels from the features. This effectively
    bridges the gap in feature distributions between the TD and SD. The projection
    matrix is optimized by combining $\ell_{1}$-norm and $\ell_{2,1}$-norm penalty
    terms with other regularization terms. Furthermore, [zhao2021cross] employs sparse
    subspace transfer learning (SSTL) to develop a CC-SER technique. It learns a robust
    common subspace projection using discriminative subspace learning and transfers
    knowledge from the source corpus to the target corpus through sparse reconstruction
    based on $\ell_{2,1}$-norm. The target samples are suitably represented as linear
    combinations of the SD data. On a different note, [braunschweiler2021study] investigates
    the impact of cross-corpus data complementation and data augmentation on the performance
    of SER models. The study focuses on six emotional speech corpora, considering
    factors such as single and multiple speakers, as well as variations in emotion
    style (natural, elicited, and acted).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的，[zhang2019transfer] 提出了一个转移稀疏判别子空间学习（TSDSL）方法，以发现多个语料库之间的共享特征子空间。这通过结合$\ell_{2,1}$-范数惩罚和判别学习实现，促进了在各语料库中识别最具判别性的特征。同样，[luo2020nonnegative]
    介绍了一种基于非负矩阵分解的转移子空间学习（NMFTSL）方案。其目标是最小化共同子空间中边际分布和条件分布之间的距离。这些距离通过最大均值差异（MMD）准则进行估计。在[zhang2021cross]中，采用了联合转移子空间学习和回归（JTSLR）技术。它使用判别MMD作为差异度量来学习潜在子空间，然后在潜在子空间中使用回归函数建模特征和注释之间的关系。通过利用标签图来增强从源领域（SD）数据到目标领域（TD）数据的知识转移。同样，[chen2019target]
    提出了一个针对CC-SER的目标适应子空间学习（TaSL）方法。其目标是找到一个投影子空间，以实现从特征中更准确地回归标签。这有效地弥合了TD和SD之间的特征分布差距。投影矩阵通过将$\ell_{1}$-范数和$\ell_{2,1}$-范数惩罚项与其他正则化项结合来优化。此外，[zhao2021cross]
    使用稀疏子空间转移学习（SSTL）来开发CC-SER技术。它通过判别子空间学习学习一个稳健的共同子空间投影，并通过基于$\ell_{2,1}$-范数的稀疏重建将知识从源语料库转移到目标语料库。目标样本被适当地表示为SD数据的线性组合。另一方面，[braunschweiler2021study]
    研究了跨语料库数据补全和数据增强对SER模型性能的影响。研究集中在六个情感语音语料库上，考虑了单一和多个说话者以及情感风格（自然、诱发和表演）的变化。
- en: 4.5 Adversarial TL-based ASR
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 基于对抗学习的自动语音识别
- en: 'In many cases, the source model is trained using multilingual training, where
    abundant speech data is available in multiple languages [[88](#bib.bib88), [94](#bib.bib94)].
    Multilingual training involves shared hidden layers (SHL) and language-specific
    layers or classifier layers for different languages. The SHL of the source model
    acts as a feature converter, transforming language-specific features into a common
    feature space [liu2019investigation]. However, there might be language-dependent
    features present in the common feature space, which hinders effective cross-lingual
    knowledge transfer. To address this issue, language-adversarial training is employed.
    Adversarial training helps in creating a language-invariant feature space. Once
    the source model is prepared, the first $n$ SHL can be transferred to the target
    model of an unknown language. In [yi2018language], the authors propose language-adversarial
    transfer learning as a solution to mitigate the performance degradation of the
    target model caused by shared features that may contain unnecessary language-dependent
    information. Fig. [17](#S4.F17 "Figure 17 ‣ 4.5 Adversarial TL-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") illustrates the architecture of the suggested
    language-adversarial transfer learning method [yi2018language]. The source model,
    also known as the adversarial SHL model, is shown on the left, while the target
    model is depicted on the right. The presence of an additional language discriminator
    in the SHL model is denoted. The fully connected layer is represented as FC. The
    gradient reversal layer (GRL) ensures that the feature distributions across all
    languages are made as similar as possible for the language discriminator. The
    output labels of the language discriminator indicate the languages.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '在许多情况下，源模型使用多语言训练进行训练，在多种语言中都有丰富的语音数据[[88](#bib.bib88), [94](#bib.bib94)]。多语言训练包括共享隐藏层（SHL）和不同语言的语言特定层或分类器层。源模型的SHL作为特征转换器，将语言特定的特征转换为共同特征空间[liu2019investigation]。然而，常见特征空间中可能存在语言依赖特征，这阻碍了有效的跨语言知识转移。为了解决这个问题，采用了语言对抗训练。对抗训练有助于创建一个语言不变的特征空间。一旦源模型准备好，前$n$个SHL可以转移到未知语言的目标模型中。在[yi2018language]中，作者提出了语言对抗迁移学习作为解决因共享特征可能包含不必要的语言依赖信息而导致目标模型性能下降的方案。图[17](#S4.F17
    "Figure 17 ‣ 4.5 Adversarial TL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")展示了建议的语言对抗迁移学习方法[yi2018language]的架构。源模型，也称为对抗SHL模型，显示在左侧，而目标模型则显示在右侧。SHL模型中存在额外的语言判别器。全连接层表示为FC。梯度反转层（GRL）确保语言判别器的特征分布在所有语言中尽可能相似。语言判别器的输出标签指示语言。'
- en: '![Refer to caption](img/f77e3e1bf8deb2575dfea1a07de292ff.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f77e3e1bf8deb2575dfea1a07de292ff.png)'
- en: 'Figure 17: An example of proposed model architecture language-adversarial TL
    for limited ASR resource [yi2018language]. Senones refers to feature cluster’s
    name, representing similar acoustic states/events.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：提出的模型架构语言对抗迁移学习的示例，用于有限ASR资源[yi2018language]。Senones指的是特征簇的名称，代表相似的声学状态/事件。
- en: 'To enhance the effectiveness of ASR in low-resource scenarios, a combination
    of semi-supervised training and language adversarial TL is explored in [kumar2021exploration].
    The research presented in [yi2020adversarial] suggests utilizing adversarial transfer
    learning to improve punctuation prediction performance. Specifically, a pre-trained
    BERT model is employed to transfer bidirectional representations to punctuation
    prediction models. The proposed approach is applied to the ASR task as the target
    task. Table [10](#S4.T10 "Table 10 ‣ 4.5 Adversarial TL-based ASR ‣ 4 Advanced
    ASR methods and applications ‣ Automatic Speech Recognition using Advanced Deep
    Learning Approaches: A survey") provides a summary of the performance achieved
    in recent studies involving adversarial TL for ASR.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提高低资源场景中自动语音识别（ASR）的效果，[kumar2021exploration] 探索了半监督训练和语言对抗迁移学习的结合。研究[yi2020adversarial]建议利用对抗迁移学习来改善标点预测性能。具体而言，采用预训练的BERT模型将双向表示转移到标点预测模型中。所提出的方法应用于ASR任务作为目标任务。表[10](#S4.T10
    "Table 10 ‣ 4.5 Adversarial TL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")总结了近期涉及ASR对抗迁移学习的研究成果。'
- en: 'Table 10: A summary of the recent ASR-based adversarial-language TL technique.
    Whereas the marks ($\uparrow$) and ($\downarrow$) indicate improvement and reduction,
    respectively. If many scenarios has been conducted in one metric, only the best
    result is mentioned.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：近期基于ASR的对抗性语言迁移技术的总结。标记（$\uparrow$）和（$\downarrow$）分别表示改进和减少。如果在一个指标上进行了多个场景的测试，仅提及最佳结果。
- en: '| Scheme | Model-based | ASR Tasks ($\mathbb{T}_{T}$) | Characteristic | Performance
    |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 方案 | 基于模型 | ASR任务（$\mathbb{T}_{T}$） | 特征 | 性能 |'
- en: '| [kumar2021exploration] | Adversarial SHL-Mode ( SincNet-CNN-LiGRU ) | Use
    three Indian languages ( Hindi, Marathi, and Bengali ) cross-lingual to improve
    Hindi ASR | Semi-supervised | WER=5.5% (25.65% WER $\downarrow$) |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| [kumar2021exploration] | 对抗性SHL模式（SincNet-CNN-LiGRU） | 使用三种印度语言（印地语、马拉地语和孟加拉语）跨语言提高印地语ASR
    | 半监督 | WER=5.5%（25.65% WER $\downarrow$） |'
- en: '| [yi2018language] | SHL Model | Improve the performance of low-resource ASR
    | Cross-lingual | 10.1 % WER $\downarrow$ |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| [yi2018language] | SHL模型 | 改善低资源ASR的性能 | 跨语言 | 10.1% WER $\downarrow$ |'
- en: '| [yi2020adversarial] | BERT | Improve the performance of punctuation predicting
    | Multi-task | 9.4% F1-score $\uparrow$ |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| [yi2020adversarial] | BERT | 改善标点预测的性能 | 多任务 | 9.4% F1-score $\uparrow$ |'
- en: 4.6 DTL-based ASR for medical diagnosis
  id: totrans-442
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 基于DTL的医学诊断ASR
- en: 'DTL-based ASR has made significant advancements in the field of medicine, particularly
    in the early detection of diseases. These advancements have been observed in various
    medical domains, as outlined in Table [11](#S4.T11 "Table 11 ‣ 4.6.3 Other medical
    diagnosis ‣ 4.6 DTL-based ASR for medical diagnosis ‣ 4 Advanced ASR methods and
    applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey"), which includes:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 基于DTL的ASR在医学领域取得了重大进展，特别是在疾病的早期检测方面。这些进展已在各种医学领域得到观察，如表[11](#S4.T11 "表 11 ‣
    4.6.3 其他医学诊断 ‣ 4.6 基于DTL的医学诊断ASR ‣ 4 高级ASR方法和应用 ‣ 使用先进深度学习方法的自动语音识别：综述")所列。
- en: 4.6.1 Heart sound classification
  id: totrans-444
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.1 心音分类
- en: The components of the heart sound encompass various elements. The first (S1)
    and second (S2) heart sounds are considered normal, whereas the third (S3) and
    fourth (S4) heart sounds are often associated with murmurs, and ejection clicks
    typically indicate certain illnesses or abnormalities. Koike et al. [koike2020audio]
    introduced a novel DTL approach based on Probabilistic Audio Neural Networks (PANNs),
    which involves pre-training a model on a large-scale audio dataset for the purpose
    of classifying heart sounds. Another approach for heart sound classification was
    proposed by Boulares et al. [boulares2020transfer]. Their method utilizes DTL
    on the Pascal public dataset, without any denoising or cleaning procedures, to
    establish an experimental benchmark. The primary objective is to provide a foundation
    of experimental results that can serve as a starting point for future research
    on cardiovascular disease (CVD) recognition using phonocardiogram (PCG)-based
    cardiac cycle vibration sounds. This proposed scheme addresses the absence of
    a CVD recognition benchmark and the lack of objective comparability among classification
    results, which tend to vary significantly.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 心音的组成包括各种元素。第一个（S1）和第二个（S2）心音被认为是正常的，而第三个（S3）和第四个（S4）心音通常与杂音有关，射血点击声通常指示某些疾病或异常。Koike等人[koike2020audio]提出了一种基于概率音频神经网络（PANNs）的新颖DTL方法，该方法通过在大规模音频数据集上预训练模型来对心音进行分类。另一种心音分类的方法由Boulares等人[boulares2020transfer]提出。他们的方法利用DTL在Pascal公共数据集上进行，没有任何去噪或清理程序，以建立实验基准。主要目标是提供一组实验结果，以作为未来基于心音图（PCG）的心血管疾病（CVD）识别研究的起点。该方案解决了缺乏CVD识别基准和分类结果客观可比性不足的问题，这些结果往往差异显著。
- en: 4.6.2 Parkinson disease detection
  id: totrans-446
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.2 帕金森病检测
- en: Parkinson’s disease (PD) is a progressive neurodegenerative condition with a
    global impact. Accurate diagnosis of PD is crucial for improving daily activities
    and extending patients’ lives. However, predicting symptom changes and their impact
    on patients’ lives is challenging due to the variability in symptoms and disease
    progression. Traditional PD detection methods are often manual and require specialized
    expertise. Language impairment and atrophy are common early symptoms in over 90%
    of PD patients, characterized by reduced voice volume, monotonous and rapid speech,
    and eventual loss of audibility. Karaman et al. [[43](#bib.bib43)] proposed a
    robust automated PD detection approach using DTL-based ASR, where pre-trained
    models like SqueezeNet1_1, ResNet101, and DenseNet161 were fine-tuned and retrained.
    This scheme showed promising results in PD detection. To address the scarcity
    of speech data for PD and the distribution inconsistency among subjects, Li et
    al. [[42](#bib.bib42)] introduced a two-step unsupervised DTL algorithm called
    two-step sparse transfer learning (TSTL). This algorithm helps extract useful
    information from large amounts of unlabeled speech data, align the distribution
    of training and test sets, and preserve the original sample structure simultaneously.
    Another strategy proposed by Qing et al. [[45](#bib.bib45)] involves enhancing
    ASR for Parkinson’s patients using a pre-trained long short-term memory (LSTM)
    neural network model. To mitigate overfitting and reduce WER, the scheme employs
    frequency spectrogram masking data augmentation.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 帕金森病（PD）是一种逐渐恶化的神经退行性疾病，具有全球影响。准确诊断 PD 对改善日常活动和延长患者生命至关重要。然而，由于症状和疾病进展的变化，预测症状变化及其对患者生活的影响是具有挑战性的。传统的
    PD 检测方法通常是手动的，需要专业知识。语言障碍和萎缩是超过 90% PD 患者的常见早期症状，表现为声音音量减少、单调而快速的语速，以及最终的听不清。Karaman
    等人 [[43](#bib.bib43)] 提出了一个使用基于 DTL 的 ASR 的鲁棒自动化 PD 检测方法，其中如 SqueezeNet1_1、ResNet101
    和 DenseNet161 等预训练模型经过微调和重新训练。该方案在 PD 检测中显示了有希望的结果。为了解决 PD 语音数据稀缺和受试者之间分布不一致的问题，Li
    等人 [[42](#bib.bib42)] 引入了一种称为两步稀疏迁移学习（TSTL）的两步无监督 DTL 算法。该算法有助于从大量未标记的语音数据中提取有用信息，调整训练集和测试集的分布，同时保留原始样本结构。Qing
    等人 [[45](#bib.bib45)] 提出的另一种策略涉及使用预训练的长短期记忆（LSTM）神经网络模型增强 Parkinson’s 患者的 ASR。为减少过拟合和降低
    WER，该方案采用了频谱图掩蔽数据增强技术。
- en: 4.6.3 Other medical diagnosis
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.3 其他医学诊断
- en: 'In the field of speech-based depression prediction, Harati et al. [harati2021speech]
    proposed a DTL method that utilizes a lightweight encoder and transfers only the
    encoder weights. This approach simplifies the runtime model for depression prediction.
    For individuals with dysarthria, Xiong et al. [xiong2020source] developed an improved
    DTL framework for robust personalized speech recognition models. They adapted
    the CNN-TDNN-F ASR AM onto the target dysarthric speakers using neural network
    weight adaptation. In the context of dysarthria speaking identification, Takashima
    et al. [takashima2019knowledge] proposed a method that transfers two types of
    knowledge from different datasets: the language-dependent characteristics of unimpaired
    speech and the language-independent characteristics of dysarthric speech. They
    focused on Japanese people with articulation disorders. Additionally, Sertolli
    et al. [sertolli2021representation] presented a novel feature representation for
    health states identification using an end-to-end DTL-based ASR framework. They
    utilized ASR DNNs as feature extractors, combined multiple feature representations
    using compact bilinear pooling (CBP), and employed an optimized RNN classifier
    for inference.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于语音的抑郁症预测领域，Harati 等人 [harati2021speech] 提出了一个利用轻量级编码器并仅传递编码器权重的 DTL 方法。这种方法简化了抑郁症预测的运行时模型。对于患有言语障碍的个体，Xiong
    等人 [xiong2020source] 开发了一种改进的 DTL 框架，用于鲁棒的个性化语音识别模型。他们通过神经网络权重适配将 CNN-TDNN-F ASR
    AM 调整到目标言语障碍发言者上。在言语障碍识别的背景下，Takashima 等人 [takashima2019knowledge] 提出了一个方法，从不同数据集中转移两种类型的知识：正常语音的语言依赖特征和言语障碍语音的语言独立特征。他们集中在患有发音障碍的日本人身上。此外，Sertolli
    等人 [sertolli2021representation] 提出了一个新颖的特征表示方法，用于健康状态识别，使用了端到端的基于 DTL 的 ASR 框架。他们利用
    ASR DNN 作为特征提取器，结合多种特征表示方法，使用紧凑双线性池化（CBP），并采用优化的 RNN 分类器进行推断。
- en: 'Table 11: A summary of a DTL-based ASR technique in medical diagnosis, whereas
    the marks ($\uparrow$) and ($\downarrow$) indicate the improvement and reduction,
    respectively. If many scenarios have been conducted in one metric, only the best
    result is mentioned.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 表11：基于DTL的ASR技术在医疗诊断中的总结，其中标记（$\uparrow$）和（$\downarrow$）分别表示提升和减少。如果在一个度量中进行了多个场景，只提及最佳结果。
- en: '| Scheme | Model-based | ASR Tasks ($\mathbb{T}_{T}$) $\uparrow$ | DTL Type
    | Performance |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 方案 | 基于模型的 | ASR任务（$\mathbb{T}_{T}$）$\uparrow$ | DTL类型 | 性能 |'
- en: '| [[43](#bib.bib43)] | DenseNet-161 | PD detection | Fine-tuning | Accuracy=
    91.17% |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| [[43](#bib.bib43)] | DenseNet-161 | PD检测 | 微调 | 准确率= 91.17% |'
- en: '| [[42](#bib.bib42)] | TSTL-based CSC&SF | PD speech diagnosis | Unsupervised
    | Accuracy= 97.50% |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| [[42](#bib.bib42)] | 基于TSTL的CSC&SF | PD语音诊断 | 无监督 | 准确率= 97.50% |'
- en: '| [[45](#bib.bib45)] | Proposed four layers | PD speech | Fine-tuning | 13.5%
    WER$\downarrow$ |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| [[45](#bib.bib45)] | 提出的四层 | PD语音 | 微调 | 13.5% WER$\downarrow$ |'
- en: '| [koike2020audio] | PANN CNN14 | Heart sound classification | Fine-tuning
    | UAR= 89.7% |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| [koike2020audio] | PANN CNN14 | 心音分类 | 微调 | UAR= 89.7% |'
- en: '| [boulares2020transfer] | InceptionResNet-v2 | PCG-based CVD classification
    | Fine-tuning | Accuracy= 0.89% |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| [boulares2020transfer] | InceptionResNet-v2 | 基于PCG的CVD分类 | 微调 | 准确率= 0.89%
    |'
- en: '| [harati2021speech] | EH-AC | Depression prediction | LHN (encoder weights)
    | 27% AUC $\uparrow$ |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| [harati2021speech] | EH-AC | 抑郁预测 | LHN（编码器权重） | 27% AUC $\uparrow$'
- en: '| [xiong2020source] | CNN-TDNN-F | Dysarthric speech | Neural weight adapter
    | 11.6% WER$\downarrow$ |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| [xiong2020source] | CNN-TDNN-F | 构音障碍语音 | 神经权重适配器 | 11.6% WER$\downarrow$
    |'
- en: '| [takashima2019knowledge] | LAS | Dysarthric Speech | Multilingual | 45.9%
    PER$\downarrow$ |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| [takashima2019knowledge] | LAS | 构音障碍语音 | 多语言 | 45.9% PER$\downarrow$ |'
- en: '| [sertolli2021representation] | Wav2Letter and DeepSpeech | Health states
    classification | Transductive | UAR= 73.0% (8.6% $\uparrow$) |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| [sertolli2021representation] | Wav2Letter 和 DeepSpeech | 健康状态分类 | 传导性 | UAR=
    73.0% (提升8.6%) |'
- en: 4.7 DTL-based ASR attacks and security
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 基于DTL的ASR攻击与安全
- en: 'Adversarial examples are created by introducing small perturbations or noise
    to valid audio files or speech characteristics in order to manipulate or deceive
    ASR systems. These perturbations, although often imperceptible to the human auditory
    system and may be perceived as background noise by the ASR model, can lead to
    correct or incorrect classification of the inputs. For instance, by introducing
    a slight disturbance to the speech "At the still point, there the dance is," an
    ASR system may produce the incorrect transcription "At the tail point, there the
    tense is" [hu2019adversarial]. The concept behind attacking ASR systems lies in
    the vulnerability of these models to adversarial examples, which has prompted
    speech researchers to explore the creation of such examples. By generating adversarial
    examples for different representations of speech in the time or frequency domain,
    which capture various speech features and can be used as inputs to neural networks,
    researchers can manipulate ASR performance, either improving it or decreasing
    it. DTL plays a key role in achieving transferability, whereby adversarial examples
    crafted to attack a source model can also be effective in attacking target models
    that classify the same type of data. In this context, adversarial attacks on DTL-based
    ASR models can be categorized into two groups, as illustrated in Fig. [18](#S4.F18
    "Figure 18 ‣ 4.7 DTL-based ASR attacks and security ‣ 4 Advanced ASR methods and
    applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey").'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '对抗样本是通过对有效音频文件或语音特征引入小的扰动或噪声来创建的，以操控或欺骗ASR系统。这些扰动虽然通常对人耳系统不可察觉，可能被ASR模型视为背景噪声，但仍会导致输入的正确或错误分类。例如，通过对语音“在静止点，那里的舞蹈”引入轻微扰动，ASR系统可能会产生错误的转录“在尾点，那里的紧张感”[hu2019adversarial]。攻击ASR系统的概念在于这些模型对对抗样本的脆弱性，这促使语音研究人员探索这些样本的创建。通过生成用于时间或频率域不同语音表示的对抗样本，这些样本捕捉了各种语音特征，并可以作为神经网络的输入，研究人员可以操控ASR性能，无论是提升还是降低。DTL在实现可转移性方面发挥了关键作用，即为攻击源模型而制作的对抗样本也可以有效攻击分类相同类型数据的目标模型。在这种情况下，对基于DTL的ASR模型的对抗攻击可以分为两组，如图[18](#S4.F18
    "Figure 18 ‣ 4.7 DTL-based ASR attacks and security ‣ 4 Advanced ASR methods and
    applications ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches:
    A survey")所示。'
- en: '![Refer to caption](img/4b2528a932381c3f998b4b99c917d263.png)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4b2528a932381c3f998b4b99c917d263.png)'
- en: 'Figure 18: Possible adversarial attacks in DTL-based ASR schemes.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：基于 DTL 的 ASR 方案中的可能对抗攻击。
- en: 4.7.1 Positive adversarial attacks
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.1 积极对抗攻击
- en: Encompassing various techniques that aim to enhance or ensure the efficacy of
    current ASR systems, these methods strive to improve the robustness and performance
    of acoustic models. In [sun2018training], the authors introduced a methodology
    that combines natural data with adversarial data to train a resilient acoustic
    model. Specifically, they focused on utilizing MFCC features and employed a gradient-based
    approach to generate adversarial MFCC features, taking into account the network
    model and input parameters for each mini-batch. By applying the teacher/student
    training concept, the neural network was trained using a combination of natural
    data and the generated adversarial data. The effectiveness of the proposed approach
    was validated through experiments conducted on the CHiME-4 and Aurora-4 tasks,
    utilizing a customized convolutional neural network (CNN).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及各种旨在增强或确保当前 ASR 系统有效性的技术，这些方法努力提高声学模型的鲁棒性和性能。在 [sun2018training] 中，作者提出了一种将自然数据与对抗数据结合以训练具有鲁棒性的声学模型的方法。具体来说，他们专注于利用
    MFCC 特征，并采用基于梯度的方法生成对抗 MFCC 特征，考虑到网络模型和每个小批量的输入参数。通过应用师生训练概念，神经网络使用自然数据和生成的对抗数据进行训练。通过在
    CHiME-4 和 Aurora-4 任务上进行实验，利用定制的卷积神经网络（CNN）验证了所提出方法的有效性。
- en: 4.7.2 Negative adversarial attacks
  id: totrans-467
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.2 消极对抗攻击
- en: 'Encompassing various techniques that aim to diminish or undermine the effectiveness
    of existing ASR systems, these methods pose potential threats to the performance
    and robustness of acoustic models. In [abdullah2021hear], the authors propose
    a framework that utilizes the Google (Phone) model as the source model and investigates
    the impact of adversarial attacks on the target model (DeepSpeech 1). The adversarial
    attacks are applied to the audio waveform, specifically after the signal decomposition
    and thresholding processes, and the resulting manipulated input is fed into the
    ASR source model. According to [hu2019adversarial], the adversarial attack models
    can be categorized into two types based on the adversary’s objectives, knowledge,
    and background, as illustrated in Figure [18](#S4.F18 "Figure 18 ‣ 4.7 DTL-based
    ASR attacks and security ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey"):'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '涉及各种旨在削弱或破坏现有 ASR 系统有效性的技术，这些方法对声学模型的性能和鲁棒性构成潜在威胁。在 [abdullah2021hear] 中，作者提出了一种利用
    Google (Phone) 模型作为源模型的框架，并研究了对抗攻击对目标模型（DeepSpeech 1）的影响。这些对抗攻击被应用于音频波形，特别是在信号分解和阈值处理之后，处理后的输入被输入到
    ASR 源模型中。根据 [hu2019adversarial]，对抗攻击模型可以根据对手的目标、知识和背景分为两种类型，如图 [18](#S4.F18 "Figure
    18 ‣ 4.7 DTL-based ASR attacks and security ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    所示：'
- en: •
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adversary knowledge: It is divided into two types: white-box attack, which
    assumes the adversary has complete knowledge of $M_{T}$ including its architecture,
    training weights, and parameters; and black-box attack, which assumes the adversary
    has no access to $M_{T}$ and only knows its output like a regular user.'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对手知识：分为两种类型：白盒攻击，假设对手对 $M_{T}$ 有完全的知识，包括其架构、训练权重和参数；和黑盒攻击，假设对手无法访问 $M_{T}$，仅知道其输出，如同普通用户。
- en: •
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adversarial specificity: It is divided into two types: non-targeted attack,
    which aims to make the adversarial example’s $M_{T}$ predict any incorrect class
    with the sole goal of compromising the ASR algorithm, and targeted attack, which
    aims to deceive $M_{T}$ into assigning the adversarial example to a specific class
    selected by the attacker, imposing specific instructions on the ASR scheme.'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗特性：分为两种类型：非定向攻击，旨在使对抗样本的 $M_{T}$ 预测任何不正确的类别，唯一目标是破坏 ASR 算法；和定向攻击，旨在欺骗 $M_{T}$
    将对抗样本分配给攻击者选择的特定类别，对 ASR 方案施加特定指令。
- en: 'Moving on, Table [12](#S4.T12 "Table 12 ‣ 4.7.2 Negative adversarial attacks
    ‣ 4.7 DTL-based ASR attacks and security ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")
    shows a summary of DTL-based adversarial models for existing works.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，表[12](#S4.T12 "Table 12 ‣ 4.7.2 Negative adversarial attacks ‣ 4.7 DTL-based
    ASR attacks and security ‣ 4 Advanced ASR methods and applications ‣ Automatic
    Speech Recognition using Advanced Deep Learning Approaches: A survey")展示了基于DTL的对抗模型在现有工作中的总结。'
- en: 'Table 12: A summary of the recent DTL-based ASR for adversarial attacks. Whereas
    the marks ($\uparrow$)and ($\downarrow$), indicate improvement and reduction respectively.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 表12：近期基于DTL的ASR对抗攻击的总结。标记（$\uparrow$）和（$\downarrow$）分别表示改进和减少。
- en: '| Scheme | Model-based | ASR Tasks ($\mathbb{T}_{T}$) | Target object | Adversary
    knowledge | Adversarial specificity | Performance |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 方案 | 基于模型 | ASR任务 ($\mathbb{T}_{T}$) | 目标对象 | 对手知识 | 对抗特异性 | 性能 |'
- en: '| [sun2018training] | Aurora-4 | Boost | MFCC | White-box | Targeted | 23%
    WER $\downarrow$ |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| [sun2018training] | Aurora-4 | 提升 | MFCC | 白盒 | 定向 | 23% WER $\downarrow$
    |'
- en: '| [schonherr2018adversarial] | DNN-HMM (Kaldi) | Boost | Waveform | White-box
    | Targeted | Accuracy= 98 % |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| [schonherr2018adversarial] | DNN-HMM (Kaldi) | 提升 | 波形 | 白盒 | 定向 | 准确率=98%
    |'
- en: '| [zelasko2021adversarial] | DeepSpeech | Fool | Waveform | White-box | Targeted
    | 4-5% WER$\uparrow$ |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| [zelasko2021adversarial] | DeepSpeech | 欺骗 | 波形 | 白盒 | 定向 | 4-5% WER$\uparrow$
    |'
- en: '| [subramanian2020study] | VGG13 | Fool (Dense_mel) | Mel-spectrogram | White-box
    | Non-targeted | SNR=29.06 dB |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| [subramanian2020study] | VGG13 | 欺骗（密集Mel） | Mel谱图 | 白盒 | 非定向 | SNR=29.06
    dB |'
- en: '| [carlini2018audio] | DeepSpeech | Fool (Speech-to-Text) | Waveform | White-box
    | Targeted | Attack success rate= 100% |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| [carlini2018audio] | DeepSpeech | 欺骗（语音到文本） | 波形 | 白盒 | 定向 | 攻击成功率=100% |'
- en: '| [abdullah2021hear] | Google (Phone) | Fool (Deep-Speech 1) | Waveform | Black-box
    | Non-targeted | Attack success rate=87% |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| [abdullah2021hear] | Google (电话) | 欺骗（Deep-Speech 1） | 波形 | 黑盒 | 非定向 | 攻击成功率=87%
    |'
- en: '| [kwon2019selective] | DeepSpeech | Fool (victime DeepSpeech) | Mel-frequency
    cepstrum | White-box | Targeted | Attack success rate=91.67%) |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| [kwon2019selective] | DeepSpeech | 欺骗（DeepSpeech受害者） | Mel频率倒谱系数 | 白盒 | 定向
    | 攻击成功率=91.67% |'
- en: 4.8 DTL-based ASR for other applications
  id: totrans-483
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8 基于DTL的ASR在其他应用中的使用
- en: DTL-based ASR has been applied in various fields. In the work by Boes et al.
    [[12](#bib.bib12)], DTL is explored for audio tagging and sound event detection
    tasks, where pre-trained auditory and visual features are incorporated into a
    baseline system using feature fusion. Arora et al. [arora2017study] use DTL to
    address the lack of annotated databases for audio event detection. Wang et al.
    [wang2020cross] propose an environment adaptation technique using DTL for deep
    speech enhancement models. Chen et al. [chen2018transfer] employ DTL for wearable
    devices to evaluate social speech in natural daily situations. Wu et al. [wu2020self]
    investigate self-supervised pre-trained speech for speech translation, while Zhu
    et al. [zhu2021conwst] propose a self-supervised bidirectional distillation system
    for low-resource speech translation. DTL is used in the speaker verification field
    by Hong et al. [hong2017transfer] for discriminative learning, and in the detection
    of marine mammal sounds by Lu et al. [lu2021detection] to classify different species
    using DTL with the AlexNet pre-trained model. Lastly, Azizah et al. [azizah2020hierarchical]
    employ hierarchical DTL for multilingual text-to-speech synthesis using DNNs for
    low-resource languages.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 基于DTL的ASR已应用于多个领域。在Boes等人的工作中[[12](#bib.bib12)]，DTL被探索用于音频标记和声音事件检测任务，其中将预训练的听觉和视觉特征结合到使用特征融合的基准系统中。Arora等人[arora2017study]利用DTL解决音频事件检测的标注数据库不足问题。Wang等人[wang2020cross]提出了一种使用DTL的环境适应技术，用于深度语音增强模型。Chen等人[chen2018transfer]使用DTL在可穿戴设备上评估自然日常情况下的社交语音。Wu等人[wu2020self]研究了自监督预训练语音用于语音翻译，而Zhu等人[zhu2021conwst]提出了一种自监督双向蒸馏系统，用于低资源语音翻译。DTL在说话人验证领域被Hong等人[hong2017transfer]用于辨别学习，并在Lu等人[lu2021detection]的工作中用于分类不同物种的海洋哺乳动物声音，使用DTL和预训练的AlexNet模型。最后，Azizah等人[azizah2020hierarchical]使用层次化DTL进行多语言文本到语音合成，利用DNNs处理低资源语言。
- en: 5 Open Issues and of Key challenges
  id: totrans-485
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 开放问题和关键挑战
- en: Integrating advanced techniques like [DTL](#Sx1.16.16.16), [FL](#Sx1.20.20.20),
    and [RL](#Sx1.35.35.35) into [ASR](#Sx1.4.4.4) systems presents exciting opportunities
    but comes with its set of challenges. This section delves into the distinct challenges
    associated with each approach, emphasizing the critical areas that demand attention
    and innovation.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 将先进技术如[DTL](#Sx1.16.16.16)、[FL](#Sx1.20.20.20)和[RL](#Sx1.35.35.35)集成到[ASR](#Sx1.4.4.4)系统中带来了激动人心的机会，但也面临着一系列挑战。本节将深入探讨每种方法相关的独特挑战，强调需要关注和创新的关键领域。
- en: 5.1 DTL and domain adaptation
  id: totrans-487
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 DTL和领域适应
- en: This section discusses challenges and concepts related to [DTL](#Sx1.16.16.16)
    and [DA](#Sx1.13.13.13) in speech recognition, including distribution shift, feature
    space adaptation, label distribution shift, catastrophic forgetting, domain-invariant
    feature learning, sample selection bias, and hyperparameter optimization.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了与[DTL](#Sx1.16.16.16)和[DA](#Sx1.13.13.13)在语音识别中相关的挑战和概念，包括分布转移、特征空间适应、标签分布转移、灾难性遗忘、领域不变特征学习、样本选择偏差和超参数优化。
- en: When applying a model trained on one domain (source) to another (target), a
    distribution shift often occurs, referred to as domain shift. Formally, if $P_{S}(X,Y)$
    and $P_{T}(X,Y)$ represent the joint distributions of features $X$ and labels
    $Y$ in the source and target domains, respectively, the challenge arises when
    $P_{S}(X,Y)\neq P_{T}(X,Y)$. To address this, techniques focus on learning a transformation
    of the feature space to minimize the difference between the source and target
    distributions. This involves finding a mapping function $f:X\rightarrow Z$, where
    $Z$ is a latent space in which the distributions of transformed features $f(X_{S})$
    and $f(X_{T})$ are more similar, quantified using measures such as the maximum
    mean discrepancy (MMD). Label distribution shift occurs when the distributions
    of labels ($P_{S}(Y)$ vs. $P_{T}(Y)$) differ, even if the feature distributions
    align. This poses challenges, especially with underrepresented classes in the
    target domain. Addressing this mathematically involves adjusting the model or
    learning process, possibly by re-weighting the loss function based on class distribution
    estimates. Catastrophic Forgetting is a risk during fine-tuning on a new domain,
    where the model may lose its performance on the original task. Balancing loss
    functions ($L_{S}$ for the source and $L_{T}$ for the target) is crucial, often
    weighted by a hyperparameter $\lambda$ to control their importance. Domain-invariant
    feature learning aims to learn features invariant across domains while remaining
    predictive. It involves optimizing a feature extractor $f$ and predictor $g$ to
    minimize the $D_{S}$ loss $L_{S}$ and the domain discrepancy (e.g., MMD). The
    problem of sample selection bias occurs in selecting samples for [DA](#Sx1.13.13.13),
    affecting the effectiveness of adaptation strategies. Mathematically, addressing
    this bias involves weighting or selecting samples to minimize it, often using
    importance sampling or re-weighting techniques. Hyperparameter optimization is
    critical in [DA](#Sx1.13.13.13), where the choice of hyperparameters (e.g., $\lambda$)
    significantly impacts performance. Finding the optimal hyperparameters typically
    involves solving complex optimization problems using techniques like grid search,
    random search, or Bayesian optimization on a validation set.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 当将一个在某个领域（源领域）训练的模型应用到另一个领域（目标领域）时，通常会发生分布偏移，这被称为领域偏移。形式上，如果 $P_{S}(X,Y)$ 和
    $P_{T}(X,Y)$ 分别表示源领域和目标领域中特征 $X$ 和标签 $Y$ 的联合分布，当 $P_{S}(X,Y)\neq P_{T}(X,Y)$ 时，就会出现挑战。为了解决这个问题，技术重点在于学习特征空间的转换，以最小化源领域和目标领域分布之间的差异。这涉及到寻找一个映射函数
    $f:X\rightarrow Z$，其中 $Z$ 是一个潜在空间，在这个空间中，变换后的特征 $f(X_{S})$ 和 $f(X_{T})$ 的分布更加相似，这种相似性通过最大均值差异（MMD）等度量进行量化。标签分布偏移发生在标签的分布
    ($P_{S}(Y)$ 与 $P_{T}(Y)$) 不同，即使特征分布对齐。这会带来挑战，特别是在目标领域中类的表示不足时。数学上解决这一问题涉及调整模型或学习过程，可能通过根据类分布估计重新加权损失函数来实现。灾难性遗忘是在对新领域进行微调时的一个风险，此时模型可能会丧失对原始任务的性能。平衡损失函数（源领域的
    $L_{S}$ 和目标领域的 $L_{T}$）至关重要，通常由超参数 $\lambda$ 加权以控制其重要性。领域不变特征学习旨在学习跨领域不变的特征，同时保持预测能力。它涉及优化特征提取器
    $f$ 和预测器 $g$ 以最小化 $D_{S}$ 损失 $L_{S}$ 和领域差异（例如，MMD）。样本选择偏差问题发生在选择 [DA](#Sx1.13.13.13)
    样本时，影响适应策略的有效性。数学上，解决这一偏差涉及加权或选择样本以最小化偏差，通常使用重要性采样或重新加权技术。超参数优化在 [DA](#Sx1.13.13.13)
    中至关重要，其中超参数（例如 $\lambda$）的选择对性能有显著影响。找到最佳超参数通常涉及使用网格搜索、随机搜索或贝叶斯优化等技术，在验证集上解决复杂的优化问题。
- en: MMD^2[f(X_S), f(X_T)] = ∥1nS∑_i=1^n_Sϕ(x_S_i) - 1nT∑_j=1^n_Tϕ(x_T_j)∥^2
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: MMD^2[f(X_S), f(X_T)] = ∥1nS∑_i=1^n_Sϕ(x_S_i) - 1nT∑_j=1^n_Tϕ(x_T_j)∥^2
- en: where $\phi(\cdot)$ represents the mapping to a reproducing kernel Hilbert space
    (RKHS), and $n_{S}$, $n_{T}$ are the numbers of samples in the source and target
    domains, respectively.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi(\cdot)$ 表示映射到再生核希尔伯特空间（RKHS），$n_{S}$ 和 $n_{T}$ 分别是源领域和目标领域的样本数量。
- en: Moreover, the unification of [DTL](#Sx1.16.16.16) in [ASR](#Sx1.4.4.4) poses
    a challenge due to the varied mathematical formulations used in different studies.
    While efforts have been made to unify definitions and formulations, further work
    is needed for a consistent understanding of [DTL](#Sx1.16.16.16). Speech-based
    [DTL](#Sx1.16.16.16) processing faces challenges compared to image-based processing
    due to potential mismatches between source and target databases arising from factors
    like language, speakers, age groups, ethnicity, and acoustic environments. The
    [CTC](#Sx1.11.11.11) approach, while promising, is limited by the assumption of
    frame independence. Cross-lingual [DTL](#Sx1.16.16.16) challenges include incorporating
    linguistic characteristics from multiple sources and integrating knowledge at
    different hierarchical levels, considering linguistic differences. Finally, computational
    burden remains a significant challenge in [DTL](#Sx1.16.16.16) and [DA](#Sx1.13.13.13)
    processes. Knowledge transfer between domains can incur additional computational
    costs, especially considering the extensive computational resources required for
    deep architectures inherent in [DTL](#Sx1.16.16.16) techniques.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[DTL](#Sx1.16.16.16)在[ASR](#Sx1.4.4.4)中的统一性因不同研究中使用的数学公式多样而面临挑战。尽管已经采取措施统一定义和公式，但仍需进一步工作以实现对[DTL](#Sx1.16.16.16)的一致理解。与基于图像的处理相比，基于语音的[DTL](#Sx1.16.16.16)处理面临挑战，因为源数据库和目标数据库之间可能由于语言、说话者、年龄组、民族和声学环境等因素而存在潜在的不匹配。[CTC](#Sx1.11.11.11)方法虽然有前景，但由于假设帧独立性而受到限制。跨语言[DTL](#Sx1.16.16.16)的挑战包括从多个来源中融入语言特征和整合不同层次的知识，同时考虑语言差异。最后，计算负担在[DTL](#Sx1.16.16.16)和[DA](#Sx1.13.13.13)过程中仍然是一个重大挑战。领域间的知识转移可能会产生额外的计算成本，特别是考虑到[DTL](#Sx1.16.16.16)技术中深度架构所需的大量计算资源。
- en: 5.2 FL-based
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 基于FL的方法
- en: FL has significant potential for ASR systems, particularly in enhancing privacy
    and personalization. However, deploying this technology in ASR also comes with
    a set of challenges. Typically, in FL, data is inherently decentralized and can
    vary significantly across devices. This heterogeneity in speech data—due to differences
    in accents, dialects, languages, and background noise—can make it challenging
    to train a model that performs well across all nodes. Ensuring robustness and
    generalization of the ASR model under these conditions is a complex task. Moving
    on, FL requires periodic communication between the central server and the devices
    to update the model. For ASR systems, where models can be quite large, this can
    result in substantial communication overhead. Optimizing the efficiency of these
    updates, in terms of both bandwidth usage and energy consumption, especially on
    mobile devices, is a significant challenge. Besides, although FL is designed to
    enhance privacy by not sharing raw data, there are still privacy challenges. For
    instance, it’s possible to infer sensitive information from model updates. Ensuring
    that these updates do not leak private information about the users’ speech data
    is a critical concern that requires sophisticated privacy-preserving techniques
    like differential privacy or secure multi-party computation.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习（FL）在自动语音识别（ASR）系统中具有显著的潜力，特别是在增强隐私和个性化方面。然而，将这种技术应用于ASR也带来了一系列挑战。通常，在FL中，数据本质上是去中心化的，并且可能在设备之间有显著的差异。这种语音数据的异质性——由于口音、方言、语言和背景噪音的差异——可能使得训练一个在所有节点上表现良好的模型变得具有挑战性。在这些条件下，确保ASR模型的鲁棒性和泛化能力是一项复杂的任务。此外，FL需要中央服务器与设备之间定期通信以更新模型。对于ASR系统而言，模型可能非常庞大，这可能导致大量的通信开销。优化这些更新的效率，尤其是在移动设备上，包括带宽使用和能源消耗，是一个重大挑战。此外，尽管FL旨在通过不共享原始数据来增强隐私，但仍然存在隐私挑战。例如，从模型更新中推断敏感信息是可能的。确保这些更新不会泄露用户语音数据的私人信息是一个关键问题，需要采用诸如差分隐私或安全多方计算等复杂的隐私保护技术。
- en: Additionally, one of the advantages of FL is the ability to personalize models
    based on local data. However, balancing personalization with the need for a generally
    effective model—especially in a diverse ecosystem with varying speech patterns—is
    challenging. Achieving this balance without compromising the model’s overall performance
    or the personalization benefits is a key challenge. Moving forward, FL systems
    need to manage potentially thousands or millions of devices participating in the
    training process. Scalability issues, including managing updates from such a large
    and potentially unreliable network of devices, ensuring consistent model improvements,
    and handling devices joining or leaving the network, are significant technical
    hurdles. Lastly, in FL, the distribution of data across devices is often non-identically
    distributed (non-IID). This means that the speech data on one device might be
    very different from that on another, leading to challenges in training a model
    that generalizes well across all devices. Overcoming the bias introduced by non-IID
    data is a major challenge in [FL](#Sx1.20.20.20) for [ASR](#Sx1.4.4.4).
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，FL 的一个优势是能够根据本地数据个性化模型。然而，在具有不同语音模式的多样化生态系统中，平衡个性化与通用有效模型的需求是一项挑战。实现这一平衡而不损害模型的整体性能或个性化收益是一个关键挑战。未来，FL
    系统需要管理参与训练过程的数千或数百万台设备。可扩展性问题，包括管理如此大且可能不可靠的设备网络中的更新，确保一致的模型改进，以及处理设备加入或离开网络，是重大技术难题。最后，在
    FL 中，数据在设备上的分布通常是非独立同分布（non-IID）的。这意味着一个设备上的语音数据可能与另一个设备上的数据差异很大，这导致在训练一个在所有设备上都能很好地泛化的模型时面临挑战。克服由非
    IID 数据引入的偏差是[FL](#Sx1.20.20.20)在[ASR](#Sx1.4.4.4)中的一个主要挑战。
- en: 5.3 RL-based
  id: totrans-496
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 基于 RL
- en: Using RL in [ASR](#Sx1.4.4.4) systems offers promising avenues for improvement
    but also presents several challenges. Specifically, one of the primary challenges
    in applying [RL](#Sx1.35.35.35) to [ASR](#Sx1.4.4.4) is the issue of sparse and
    delayed rewards. In many [ASR](#Sx1.4.4.4) tasks, the system only receives feedback
    (rewards or penalties) after processing lengthy sequences of speech, making it
    difficult to attribute the reward to specific actions or decisions. This delay
    complicates the learning process, as the model struggles to identify which actions
    led to successful outcomes. Moreover, balancing exploration, trying new actions
    to discover their effects, with exploitation, using known actions that yield the
    best results, is a critical challenge in RL. In the context of [ASR](#Sx1.4.4.4),
    this means the system must balance between adhering to known speech patterns and
    exploring new patterns or interpretations. Overemphasis on exploration can lead
    to inaccurate transcriptions, while excessive exploitation may prevent the model
    from adapting to new speakers or accents. Additionally, RL models typically require
    a significant amount of interaction data to learn effectively. In [ASR](#Sx1.4.4.4),
    obtaining large volumes of labeled speech data, especially with user feedback,
    can be challenging and expensive. Additionally, RL algorithms can be sample-inefficient,
    meaning they need a lot of data before they start performing well, which can be
    a bottleneck in practical applications.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ASR](#Sx1.4.4.4)系统中使用 RL 提供了有前景的改进途径，但也带来了若干挑战。特别是，将[RL](#Sx1.35.35.35)应用于[ASR](#Sx1.4.4.4)的主要挑战之一是稀疏和延迟奖励的问题。在许多[ASR](#Sx1.4.4.4)任务中，系统在处理冗长的语音序列后才收到反馈（奖励或惩罚），这使得难以将奖励归因于特定的动作或决策。这种延迟使得学习过程复杂化，因为模型很难识别哪些动作导致了成功的结果。此外，平衡探索，即尝试新动作以发现其效果，与利用，即使用已知动作以获得最佳结果，是
    RL 中的一个关键挑战。在[ASR](#Sx1.4.4.4)的背景下，这意味着系统必须在遵循已知语音模式和探索新模式或解释之间取得平衡。过度强调探索可能导致不准确的转录，而过度利用则可能阻止模型适应新的发言者或口音。此外，RL
    模型通常需要大量的交互数据才能有效学习。在[ASR](#Sx1.4.4.4)中，获取大量标记的语音数据，尤其是带有用户反馈的数据，可能既具有挑战性又昂贵。此外，RL
    算法可能样本效率低，意味着它们需要大量数据才能开始表现良好，这可能是实际应用中的瓶颈。
- en: Moving forward, most [ASR](#Sx1.4.4.4) systems are built using supervised learning
    techniques that rely on vast amounts of annotated data. Integrating RL into these
    systems poses technical challenges, as it requires a different training paradigm
    that focuses on learning from user interactions and feedback rather than static
    datasets. Besides, using RL in [ASR](#Sx1.4.4.4) often involves collecting and
    analyzing user feedback and interactions to improve the model. This raises concerns
    about user privacy and data security, as sensitive information might be inadvertently
    captured and used for training. Ensuring that data is handled securely and in
    compliance with privacy regulations is a significant challenge.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，大多数[自动语音识别（ASR）](#Sx1.4.4.4)系统都是基于监督学习技术构建的，这些技术依赖于大量标注数据。将强化学习（RL）集成到这些系统中会面临技术挑战，因为这需要一种不同的训练范式，重点在于从用户交互和反馈中学习，而不是从静态数据集中学习。此外，在[ASR](#Sx1.4.4.4)中使用RL通常涉及收集和分析用户反馈和交互，以改进模型。这引发了关于用户隐私和数据安全的担忧，因为敏感信息可能会被无意中捕获并用于训练。确保数据的安全处理并遵守隐私法规是一个重要的挑战。
- en: Designing an appropriate reward function that accurately reflects the desired
    outcomes in [ASR](#Sx1.4.4.4) is challenging. The reward function must capture
    the nuances of speech recognition, such as accuracy, naturalness, and user satisfaction,
    which can be difficult to quantify. Poorly designed reward functions can lead
    to suboptimal learning outcomes or unintended behaviors. Lestly, [ASR](#Sx1.4.4.4)
    systems are used in a wide range of environments, from quiet offices to noisy
    streets. RL models need to adapt to these varying conditions, but training them
    to handle such diversity can be complex. The environment’s variability requires
    models that can generalize well across different acoustic conditions, which remains
    a challenge for RL-based [ASR](#Sx1.4.4.4) systems.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个适当的奖励函数以准确反映[ASR](#Sx1.4.4.4)中的期望结果是一项挑战。奖励函数必须捕捉语音识别的细微差别，例如准确性、自然性和用户满意度，这些往往难以量化。设计不良的奖励函数可能导致次优的学习结果或意外行为。此外，[ASR](#Sx1.4.4.4)系统被应用于各种环境中，从安静的办公室到嘈杂的街道。RL模型需要适应这些不同的条件，但训练它们以处理如此多样化的环境可能很复杂。环境的变异性要求模型能够在不同的声学条件下良好地进行泛化，这仍然是RL基础的[ASR](#Sx1.4.4.4)系统面临的挑战。
- en: 6 Future directions
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来方向
- en: 6.1 Personalized data augmentation for dysarthric and older people
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 针对言语障碍患者和老年人的个性化数据增强
- en: While ASR technologies have advanced significantly, especially in recognizing
    typical speech patterns, they still struggle to accurately identify speech from
    individuals with dysarthria or older adults [[120](#bib.bib120)]. Gathering extensive
    datasets from these groups is challenging due to mobility limitations often associated
    with these populations. In this context, personalized data augmentation plays
    a crucial role [[121](#bib.bib121), [122](#bib.bib122)]. Personalized data augmentation
    tailores the training process to accommodate the unique speech patterns and challenges
    associated with these groups. Dysarthria, a motor speech disorder, and the natural
    aging process can lead to speech that deviates from the normative models typically
    used to train ASR systems, making accurate recognition difficult [[123](#bib.bib123)].
    Personalized data augmentation introduces a wider range of speech variations into
    the training dataset, including those specific to dysarthric speakers or older
    adults. This can include variations in speech rate, pitch, articulation, and clarity.
    By training on this augmented dataset, the ASR system learns to recognize and
    accurately transcribe speech that exhibits these characteristics [[124](#bib.bib124)].
    Moreover, this helps the ASR models generalize better to unseen examples of speech
    from dysarthric speakers or older adults. This enhanced generalization is crucial
    for real-world applications where the system encounters a wide range of speech
    variations. Moving forward, personalized data augmentation can employ specific
    techniques tailored to the needs of dysarthric speakers or older adults, such
    as simulating the slurring of words, varying speech tempo, or introducing background
    noise [[125](#bib.bib125)], commonly challenging for these groups [[126](#bib.bib126)].
    Techniques like pitch perturbation, temporal stretching, and adding noise can
    simulate real-world conditions more accurately for these users. A typical example
    is presented in [[127](#bib.bib127)], where a unique approach utilizes speaker-dependent
    [generative adversarial networks](#Sx1.54.54.54) has been proposed.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ASR技术已取得显著进展，特别是在识别典型的语音模式方面，但它们仍然难以准确识别来自言语障碍患者或老年人的语音[[120](#bib.bib120)]。由于这些群体常常伴随行动能力限制，收集来自这些群体的大规模数据集非常具有挑战性。在这种背景下，个性化数据增强起着至关重要的作用[[121](#bib.bib121),
    [122](#bib.bib122)]。个性化数据增强根据这些群体特有的语音模式和挑战调整训练过程。言语障碍，一种运动性言语障碍，以及自然衰老过程，可能导致与通常用于训练ASR系统的规范模型偏离的语音，从而使得准确识别变得困难[[123](#bib.bib123)]。个性化数据增强将更广泛的语音变化引入训练数据集中，包括特定于言语障碍患者或老年人的变化。这可以包括语速、音高、发音和清晰度的变化。通过在这些增强数据集上训练，ASR系统可以学习识别和准确转录具有这些特征的语音[[124](#bib.bib124)]。此外，这有助于ASR模型更好地推广到来自言语障碍患者或老年人的未见语音示例。这种增强的泛化能力对于系统在现实世界中遇到各种语音变化至关重要。未来，个性化数据增强可以采用针对言语障碍患者或老年人需求的特定技术，例如模拟词语模糊、变化语速或引入背景噪音[[125](#bib.bib125)]，这些通常对这些群体具有挑战性[[126](#bib.bib126)]。像音高扰动、时间拉伸和添加噪音这样的技术可以更准确地模拟这些用户的现实条件。一个典型的例子在[[127](#bib.bib127)]中提出，其中提出了一种利用依赖说话者的[生成对抗网络](#Sx1.54.54.54)的独特方法。
- en: 6.2 Multitask learning for ASR
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 ASR的多任务学习
- en: \Ac
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: MTL enhances the performance of ASR systems by leveraging the inherent relatedness
    of multiple learning tasks to improve the generalization of the primary [ASR](#Sx1.4.4.4)
    task. This approach allows the [ASR](#Sx1.4.4.4) model to learn shared representations
    that capture underlying patterns across different but related tasks, leading to
    several key benefits [[128](#bib.bib128)]. Typically, MTL encourages the [ASR](#Sx1.4.4.4)
    system to learn representations that are beneficial across multiple tasks. This
    can lead to more robust feature extraction, as the model is not optimized solely
    for transcribing speech but also for other related tasks, such as speaker identification
    or emotion recognition. This shared learning process helps in capturing a broader
    range of speech characteristics, which can improve the [ASR](#Sx1.4.4.4) system’s
    ability to handle varied speech inputs. Moving one, By simultaneously learning
    related tasks, MTL acts as a form of regularization, reducing the risk of overfitting
    on the primary [ASR](#Sx1.4.4.4) task. This is because the model must find a solution
    that performs well across all tasks, preventing it from relying too heavily on
    noise or idiosyncrasies specific to the training data of the main task. Besides,
    learning auxiliary tasks alongside the main [ASR](#Sx1.4.4.4) task can improve
    the model’s generalization capabilities. For example, learning to identify the
    speaker or the language can provide additional contextual clues that help the
    [ASR](#Sx1.4.4.4) system better understand and transcribe ambiguous audio signals.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: MTL 通过利用多个学习任务之间的固有相关性来提高[ASR](#Sx1.4.4.4)系统的性能，从而改善主要[ASR](#Sx1.4.4.4)任务的泛化能力。这种方法允许[ASR](#Sx1.4.4.4)模型学习共享表示，捕捉不同但相关任务中的潜在模式，从而带来几个关键好处[[128](#bib.bib128)]。通常，MTL
    鼓励[ASR](#Sx1.4.4.4)系统学习在多个任务中都有效的表示。这可以导致更强健的特征提取，因为模型不仅仅为语音转录而优化，还为其他相关任务，如说话人识别或情感识别优化。这种共享学习过程有助于捕捉更广泛的语音特征，从而提高[ASR](#Sx1.4.4.4)系统处理多样化语音输入的能力。进一步说，通过同时学习相关任务，MTL
    充当了一种正则化形式，减少了对主要[ASR](#Sx1.4.4.4)任务过拟合的风险。这是因为模型必须找到一个在所有任务上表现良好的解决方案，防止它过度依赖于主要任务训练数据中的噪声或特定性。此外，学习与主要[ASR](#Sx1.4.4.4)任务并行的辅助任务可以提高模型的泛化能力。例如，学习识别说话人或语言可以提供额外的上下文线索，帮助[ASR](#Sx1.4.4.4)系统更好地理解和转录模糊的音频信号。
- en: Additionally, MTL can make more efficient use of available data by leveraging
    auxiliary tasks for which more data might be available. In scenarios where annotated
    data for [ASR](#Sx1.4.4.4) is limited, incorporating additional tasks with more
    abundant data can help improve the learning process and performance of the [ASR](#Sx1.4.4.4)
    system. Moreover, MTL allows [ASR](#Sx1.4.4.4) systems to better handle acoustic
    variability in speech, such as accents, dialects, or noisy environments, by incorporating
    tasks that directly or indirectly encourage the model to learn features that are
    invariant to these variations. Last but not least, modern [ASR](#Sx1.4.4.4) systems
    often employ [DL](#Sx1.14.14.14) architectures that can benefit from end-to-end
    learning strategies. MTL fits naturally into this paradigm, allowing for the joint
    optimization of multiple objectives within a single model architecture. This can
    simplify the training process and reduce the need for separately trained models
    or handcrafted features.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MTL 可以通过利用更多数据可能可用的辅助任务，更有效地使用现有数据。在[ASR](#Sx1.4.4.4)标注数据有限的情况下，结合更多数据丰富的额外任务可以帮助改善学习过程和[ASR](#Sx1.4.4.4)系统的性能。此外，MTL
    允许[ASR](#Sx1.4.4.4)系统更好地处理语音中的声学变异性，例如口音、方言或嘈杂环境，通过结合直接或间接促使模型学习对这些变化不变特征的任务。最后但同样重要的是，现代的[ASR](#Sx1.4.4.4)系统通常采用可以从端到端学习策略中受益的[DL](#Sx1.14.14.14)架构。MTL
    自然融入这一范式，允许在单一模型架构中共同优化多个目标。这可以简化训练过程，并减少对单独训练模型或手工特征的需求。
- en: 6.3 Federated multi-task learning and distillation for ASR
  id: totrans-507
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 联邦多任务学习与[ASR](#Sx1.4.4.4)的蒸馏
- en: \Ac
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: 'FMTL extends the concept of FL by allowing each client to learn a personalized
    model that addresses its specific task, while still benefiting from collaboration
    with other clients.This approach recognizes the heterogeneity in clients’ data
    distributions and tasks. Mathematically and compared with [FL](#Sx1.20.20.20)
    (Equation [3](#S4.E3 "In 4.3 FL-based ASR ‣ 4 Advanced ASR methods and applications
    ‣ Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey")),
    [federated multi-task learning](#Sx1.56.56.56) ([FMTL](#Sx1.56.56.56)) can be
    formulated as:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: FMTL 通过允许每个客户端学习一个针对其特定任务的个性化模型，同时仍从与其他客户端的协作中受益，扩展了 FL 的概念。这种方法认识到客户端数据分布和任务的异质性。从数学上讲，与[FL](#Sx1.20.20.20)（公式
    [3](#S4.E3 "在 4.3 基于 FL 的 ASR ‣ 4 高级 ASR 方法与应用 ‣ 使用先进深度学习方法的自动语音识别：一项调查")）相比，[联邦多任务学习](#Sx1.56.56.56)（[FMTL](#Sx1.56.56.56)）可以表示为：
- en: '|  | $\min_{\theta_{1},\theta_{2},...,\theta_{K}}\sum_{k=1}^{K}F_{k}(\theta_{k})+\lambda
    R(\theta_{1},\theta_{2},...,\theta_{K})$ |  | (7) |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta_{1},\theta_{2},...,\theta_{K}}\sum_{k=1}^{K}F_{k}(\theta_{k})+\lambda
    R(\theta_{1},\theta_{2},...,\theta_{K})$ |  | (7) |'
- en: Different from [FL](#Sx1.20.20.20), $R(\theta_{1},\theta_{2},...,\theta_{K})$
    is a regularization term that encourages some form of similarity or sharing among
    the model parameters of different tasks, promoting collaboration among clients.
    $\lambda$ is a regularization coefficient that balances the trade-off between
    fitting the local data well and collaborating with other clients. FMTL has task-specific
    model parameters $\theta_{k}$ for each client, where only a single global model
    parameter $\theta$ in FL.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 与[FL](#Sx1.20.20.20)不同，$R(\theta_{1},\theta_{2},...,\theta_{K})$ 是一个正则化项，它鼓励不同任务的模型参数之间存在某种形式的相似性或共享，促进客户端之间的协作。$\lambda$
    是一个正则化系数，用于平衡拟合本地数据和与其他客户端协作之间的权衡。FMTL 为每个客户端具有任务特定的模型参数 $\theta_{k}$，而 FL 中只有一个全局模型参数
    $\theta$。
- en: In this regard, FMTL offers a promising approach to improving [ASR](#Sx1.4.4.4)
    systems while also enhancing privacy and security measures. This learning paradigm
    extends the traditional [FL](#Sx1.20.20.20) model by enabling the simultaneous
    training of multiple tasks across distributed devices or nodes, without the need
    to share raw data [[129](#bib.bib129)]. [FMTL](#Sx1.56.56.56) leverages data from
    a wide range of devices and users, each potentially offering unique speech data,
    accents, dialects, and noise conditions. This diversity helps in training more
    robust [ASR](#Sx1.4.4.4) models that can perform well across various speech patterns
    and environments [[130](#bib.bib130)]. By learning from many tasks simultaneously,
    FMTL can personalize [ASR](#Sx1.4.4.4) models to individual users or specific
    groups without compromising the model’s general performance [[131](#bib.bib131)].
    This is particularly beneficial for users with unique speech patterns, such as
    those with accents or speech impairments. Moreover, FMTL encourages the development
    of compact models that can handle multiple tasks efficiently. For [ASR](#Sx1.4.4.4)
    systems, this means that a single model can potentially perform speech recognition,
    speaker identification, and even emotion detection, reducing the computational
    overhead on client devices [[132](#bib.bib132)].
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，FMTL 提供了一种有前景的方法来改进[ASR](#Sx1.4.4.4)系统，同时提高隐私和安全措施。这个学习范式通过允许在分布式设备或节点上同时训练多个任务而无需共享原始数据[[129](#bib.bib129)]，扩展了传统的[FL](#Sx1.20.20.20)模型。[FMTL](#Sx1.56.56.56)利用来自广泛设备和用户的数据，每个用户可能提供独特的语音数据、口音、方言和噪声条件。这种多样性有助于训练更强大的[ASR](#Sx1.4.4.4)模型，这些模型可以在各种语音模式和环境中表现良好[[130](#bib.bib130)]。通过同时学习多个任务，FMTL
    可以将[ASR](#Sx1.4.4.4)模型个性化到个人用户或特定群体，而不会影响模型的总体性能[[131](#bib.bib131)]。这对于具有独特语音模式的用户，诸如有口音或言语障碍的人，尤其有益。此外，FMTL
    鼓励开发能够高效处理多任务的紧凑模型。对于[ASR](#Sx1.4.4.4)系统来说，这意味着一个模型可能同时进行语音识别、说话人识别甚至情感检测，从而减少对客户端设备的计算开销[[132](#bib.bib132)]。
- en: On the other hand, in FMTL, raw data remains on the user’s device and does not
    need to be shared or transferred to a central server. This inherently reduces
    the risk of data breaches and unauthorized access, as sensitive speech data is
    not centralized [[133](#bib.bib133)]. Additionally, FMTL can be combined with
    differential privacy techniques to further anonymize the model updates sent from
    devices to the central server. This ensures that the shared information does not
    reveal sensitive details about the data or the user, enhancing privacy protection
    [[134](#bib.bib134)]. Moving on, the aggregation process in FMTL can be secured
    using cryptographic techniques, ensuring that the aggregated model updates cannot
    be traced back to individual users. This secure aggregation process protects user
    privacy while allowing the benefits of collective learning [[135](#bib.bib135)].
    Lastly, by aggregating model updates from a wide range of tasks and users, FMTL
    can improve the system’s robustness to malicious attempts at data poisoning. The
    diversity of inputs helps in diluting the impact of any adversarial data introduced
    to compromise the model.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在 FMTL 中，原始数据保留在用户的设备上，不需要共享或转移到中央服务器。这本质上减少了数据泄露和未经授权访问的风险，因为敏感的语音数据不会集中
    [[133](#bib.bib133)]。此外，FMTL 可以与差分隐私技术结合使用，进一步匿名化从设备发送到中央服务器的模型更新。这确保了共享的信息不会泄露关于数据或用户的敏感细节，从而增强隐私保护
    [[134](#bib.bib134)]。接下来，FMTL 中的聚合过程可以使用加密技术来保护，确保聚合的模型更新不能追溯到单个用户。这个安全的聚合过程在允许集体学习的同时保护用户隐私
    [[135](#bib.bib135)]。最后，通过从广泛的任务和用户中聚合模型更新，FMTL 可以提高系统对恶意数据中毒攻击的鲁棒性。输入的多样性有助于稀释任何旨在破坏模型的对抗性数据的影响。
- en: End-to-End ASR models Continuing to develop and refine end-to-end deep learning
    models that directly map speech inputs to text outputs without the need for intermediate
    representations (like phonetic transcriptions) [mamyrbayev2023hybrid]. Advances
    in models such as Transformer and Conformer architectures, which can capture long-range
    dependencies in speech, are promising for improving [ASR](#Sx1.4.4.4) accuracy
    and efficiency [liu2023sfa]. Specifically, end-to-end models in [ASR](#Sx1.4.4.4)
    aim to directly convert speech input into text output using a single neural network
    architecture. This simplifies the [ASR](#Sx1.4.4.4) pipeline by bypassing traditional
    stages such as acoustic, pronunciation, and language modeling [yang2023attention].
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端 ASR 模型 继续开发和优化端到端深度学习模型，这些模型直接将语音输入映射到文本输出，而无需中间表示（如语音转录）[mamyrbayev2023hybrid]。模型如
    Transformer 和 Conformer 架构能够捕捉语音中的长程依赖，对于提高 [ASR](#Sx1.4.4.4) 的准确性和效率具有很大前景 [liu2023sfa]。具体来说，[ASR](#Sx1.4.4.4)
    中的端到端模型旨在使用单一神经网络架构将语音输入直接转换为文本输出。这通过绕过传统的声学、发音和语言建模阶段简化了 [ASR](#Sx1.4.4.4) 流水线
    [yang2023attention]。
- en: 'An end-to-end [ASR](#Sx1.4.4.4) model maps an input sequence of speech features
    $X=(x_{1},x_{2},\ldots,x_{T})$ to an output sequence of tokens $Y=(y_{1},y_{2},\ldots,y_{N})$.
    The model function $f$ with parameters $\theta$ aims to minimize the difference
    between the predicted sequence and the ground truth. Two common loss functions
    are used [almadhor2023e2e]:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 一个端到端的 [ASR](#Sx1.4.4.4) 模型将语音特征的输入序列 $X=(x_{1},x_{2},\ldots,x_{T})$ 映射到标记的输出序列
    $Y=(y_{1},y_{2},\ldots,y_{N})$。模型函数 $f$ 带有参数 $\theta$，旨在最小化预测序列与真实值之间的差异。常用的两个损失函数是
    [almadhor2023e2e]：
- en: •
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CTC Loss: Connectionist Temporal Classification (CTC) introduces a ’blank’
    token for aligning sequences. The CTC loss is given by:'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CTC 损失：连接时序分类（CTC）引入了一个“空白”标记来对齐序列。CTC 损失定义为：
- en: '|  | $\mathcal{L}_{\text{CTC}}(\theta)=-\sum_{(X,Y)\in\mathcal{D}}\log P(Y&#124;X;\theta)$
    |  |'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{CTC}}(\theta)=-\sum_{(X,Y)\in\mathcal{D}}\log P(Y\mid
    X;\theta)$ |  |'
- en: where $\mathcal{D}$ is the dataset, and $P(Y|X;\theta)$ is the sequence probability.
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{D}$ 是数据集，而 $P(Y|X;\theta)$ 是序列概率。
- en: •
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Attention-based Seq2Seq Loss: This uses an encoder-decoder architecture with
    attention to predict each token. The loss is the negative log-likelihood:'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于注意力的 Seq2Seq 损失：这使用了带有注意力机制的编码器-解码器架构来预测每个标记。损失是负对数似然：
- en: '|  | $\mathcal{L}_{\text{Seq2Seq}}(\theta)=-\sum_{(X,Y)\in\mathcal{D}}\log
    P(Y&#124;X;\theta)$ |  |'
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{Seq2Seq}}(\theta)=-\sum_{(X,Y)\in\mathcal{D}}\log
    P(Y\mid X;\theta)$ |  |'
- en: 'Besides, model architectures can be described as follows:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模型架构可以描述如下：
- en: •
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recurrent Neural Networks (RNNs) including Long Short-Term Memory (LSTM) networks
    for temporal dependencies [vander2023using].
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs），包括用于时间依赖性的长短期记忆（LSTM）网络 [vander2023using]。
- en: •
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Convolutional Neural Networks (CNNs) for capturing local patterns in speech
    features.
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）用于捕捉语音特征中的局部模式。
- en: •
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transformer Models utilize self-attention mechanisms for parallel processing
    and superior performance [wang2022optimizing].
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Transformer 模型利用自注意力机制进行并行处理和优越性能 [wang2022optimizing]。
- en: Gradient-based methods like stochastic gradient descent (SGD) or Adam are used
    to minimize the loss function, optimizing the model parameters $\theta$ to improve
    the speech-to-text mapping. However, end-to-end models face challenges such as
    data scarcity for low-resource languages, computational resource demands, and
    integrating external language models for linguistic improvements [qu2023emphasizing].
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的方法，如随机梯度下降（SGD）或 Adam 被用来最小化损失函数，优化模型参数 $\theta$ 以改善语音到文本的映射。然而，端到端模型面临数据稀缺、计算资源需求以及整合外部语言模型以改进语言学表现等挑战
    [qu2023emphasizing]。
- en: 6.4 Domain-specific language modeling
  id: totrans-531
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 领域特定语言建模
- en: \Ac
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: 'DSLM represents a significant advancement in the field of [ASR](#Sx1.4.4.4),
    especially as systems increasingly cater to specialized fields like healthcare,
    legal, or customer service [jia2023deep, li2023prompting]. \AcpLM provide probabilities
    of sequences of words, crucial for [ASR](#Sx1.4.4.4) systems to predict the likelihood
    of subsequent words in a sentence [[38](#bib.bib38)]. A domain-specific LM is
    trained on text data from the target domain to capture its unique vocabulary and
    grammatical structures [[39](#bib.bib39)]. For n-gram models, this involves calculating
    the conditional probability of a word given the previous $n-1$ words [[40](#bib.bib40)]:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: DSLM 代表了在 [ASR](#Sx1.4.4.4) 领域的重要进展，特别是当系统越来越多地服务于医疗、法律或客户服务等专业领域时 [jia2023deep,
    li2023prompting]。 \AcpLM 提供词序列的概率，这对于 [ASR](#Sx1.4.4.4) 系统预测句子中后续词的可能性至关重要 [[38](#bib.bib38)]。领域特定的
    LM 通过在目标领域的文本数据上进行训练，以捕捉其独特的词汇和语法结构 [[39](#bib.bib39)]。对于 n-gram 模型，这涉及计算给定前 $n-1$
    个词的条件概率 [[40](#bib.bib40)]：
- en: '|  | $P(w_{n}&#124;w_{n-1},w_{n-2},\ldots,w_{n-(n-1)})=\frac{C(w_{n-(n-1)},\ldots,w_{n})}{C(w_{n-(n-1)},\ldots,w_{n-1})}$
    |  | (8) |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(w_{n}&#124;w_{n-1},w_{n-2},\ldots,w_{n-(n-1)})=\frac{C(w_{n-(n-1)},\ldots,w_{n})}{C(w_{n-(n-1)},\ldots,w_{n-1})}$
    |  | (8) |'
- en: 'Additionally, data augmentation plays a crucial role in ASR-based [DA](#Sx1.13.13.13).
    Typically, data augmentation in domain-specific ASR involves generating synthetic
    training examples by altering existing recordings or using text-to-speech (TTS)
    systems to create new audio samples from domain-specific texts. Mathematically,
    augmentation techniques can include time stretching, pitch shifting, adding noise,
    or simulating room acoustics, each represented as transformations $T$ applied
    to the original audio signal $x(t)$:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据增强在基于 ASR 的 [DA](#Sx1.13.13.13) 中扮演着关键角色。通常，领域特定的 ASR 数据增强涉及通过改变现有录音或使用文本到语音（TTS）系统从领域特定文本生成新的音频样本来生成合成训练样本。在数学上，增强技术可以包括时间拉伸、音高变化、添加噪声或模拟房间声学，每种技术都表示为对原始音频信号
    $x(t)$ 应用的变换 $T$：
- en: '|  | $x^{\prime}(t)=T(x(t);\phi)$ |  | (9) |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '|  | $x^{\prime}(t)=T(x(t);\phi)$ |  | (9) |'
- en: 'where $\phi$ represents the parameters of the transformation. Moving on, regularization
    techniques are applied to prevent overfitting to the domain-specific data, ensuring
    the model maintains generalization capabilities. Techniques such as dropout, L1/L2
    regularization, or elastic net involve adding terms to the loss function or modifying
    the optimization process to penalize large weights or complex models. For instance,
    L2 regularization adds a penalty equal to the square of the magnitude of coefficients:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi$ 表示变换的参数。接下来，应用正则化技术以防止对领域特定数据的过拟合，确保模型保持泛化能力。正则化技术如 dropout、L1/L2
    正则化或弹性网涉及在损失函数中添加项或修改优化过程以惩罚大权重或复杂模型。例如，L2 正则化添加的惩罚等于系数的平方：
- en: '|  | $L^{\prime}(D;\theta)=L(D;\theta)+\lambda\&#124;\theta\&#124;^{2}$ |  |
    (10) |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '|  | $L^{\prime}(D;\theta)=L(D;\theta)+\lambda\&#124;\theta\&#124;^{2}$ |  |
    (10) |'
- en: where $\lambda$ is a regularization parameter.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是正则化参数。
- en: While DSLM offers substantial benefits, challenges remain, such as data scarcity
    in niche domains, the need for continual model updates to keep pace with evolving
    language use, and ensuring privacy and security in sensitive domains like healthcare
    and finance. Addressing these challenges through innovative approaches in model
    training, data augmentation, and privacy-preserving technologies will be crucial
    for the advancement of domain-specific ASR systems.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DSLM 提供了显著的好处，但仍面临挑战，如小众领域的数据稀缺、需要持续更新模型以跟上语言使用的变化，以及在医疗和金融等敏感领域确保隐私和安全。通过创新的模型训练、数据增强和隐私保护技术来应对这些挑战，对于领域特定的
    ASR 系统的发展至关重要。
- en: Privacy preservation With the advancements in [ML](#Sx1.30.30.30), DL, and [DTL](#Sx1.16.16.16),
    [ASR](#Sx1.4.4.4) systems have become more practical and scalable. However, these
    systems also introduce serious privacy concerns due to the abundance of sensitive
    acoustic and textual information in speech data.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私保护 随着 [ML](#Sx1.30.30.30)、DL 和 [DTL](#Sx1.16.16.16) 的进步，[ASR](#Sx1.4.4.4)
    系统变得更加实用和可扩展。然而，这些系统也引入了严重的隐私问题，因为语音数据中包含大量敏感的声学和文本信息。
- en: While open-source and offline [ASR](#Sx1.4.4.4) systems can mitigate privacy
    risks, online [DTL](#Sx1.16.16.16)-based systems can amplify these threats. Additionally,
    the transcription performance of offline and open-source [ASR](#Sx1.4.4.4) systems
    is typically inferior to cloud-based [ASR](#Sx1.4.4.4) systems, especially in
    real-world scenarios [ahmed2020preech]. In this context, the $D_{S}$ data may
    contain sensitive information that needs to be protected. Thus, preserving users’
    privacy during the knowledge transfer from the [source domain](#Sx1.40.40.40)
    ([SD](#Sx1.40.40.40)) to the [target domain](#Sx1.48.48.48) ([TD](#Sx1.48.48.48))
    becomes a critical issue.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管开源和离线 [ASR](#Sx1.4.4.4) 系统可以减轻隐私风险，但在线 [DTL](#Sx1.16.16.16)-based 系统可能会加剧这些威胁。此外，离线和开源
    [ASR](#Sx1.4.4.4) 系统的转录性能通常低于基于云的 [ASR](#Sx1.4.4.4) 系统，特别是在实际场景中 [ahmed2020preech]。在这种情况下，$D_{S}$
    数据可能包含需要保护的敏感信息。因此，在知识从 [源领域](#Sx1.40.40.40) ([SD](#Sx1.40.40.40)) 转移到 [目标领域](#Sx1.48.48.48)
    ([TD](#Sx1.48.48.48)) 的过程中，保护用户隐私成为一个关键问题。
- en: To address this challenge, future research efforts should focus on integrating
    effective security and privacy protection strategies. Examples include decentralized
    [DTL](#Sx1.16.16.16) approaches utilizing blockchain technology [ul2020decentralized,
    wang2021enabling] and federated [DTL](#Sx1.16.16.16) methods [zhang2021federated,
    maurya2021federated]. These approaches aim to ensure privacy-preserving knowledge
    transfer in [DTL](#Sx1.16.16.16)-based [ASR](#Sx1.4.4.4) systems.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 为应对这一挑战，未来的研究应重点关注整合有效的安全和隐私保护策略。例如，利用区块链技术的去中心化 [DTL](#Sx1.16.16.16) 方法 [ul2020decentralized,
    wang2021enabling] 和联邦 [DTL](#Sx1.16.16.16) 方法 [zhang2021federated, maurya2021federated]。这些方法旨在确保
    [DTL](#Sx1.16.16.16)-based [ASR](#Sx1.4.4.4) 系统中的隐私保护知识转移。
- en: 6.5 Interpretation of [DTL](#Sx1.16.16.16) models
  id: totrans-544
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 [DTL](#Sx1.16.16.16) 模型的解释
- en: '[DTL](#Sx1.16.16.16)-based [ASR](#Sx1.4.4.4) models, despite their success,
    are often considered as "black box" systems lacking interpretability. This lack
    of interpretability raises doubts about the credibility and repeatability of their
    decisions, making it crucial to explain the reasoning behind their predictions.
    The field of explainable and interpretable [ML](#Sx1.30.30.30)/DL has gained increasing
    interest in various applications, including speech processing.'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 [DTL](#Sx1.16.16.16)-based [ASR](#Sx1.4.4.4) 模型取得了成功，但通常被认为是缺乏可解释性的“黑箱”系统。这种缺乏可解释性的问题引发了对其决策可信度和重复性的怀疑，因此解释其预测背后的推理显得尤为重要。可解释和可理解的
    [ML](#Sx1.30.30.30)/DL 领域在包括语音处理在内的各种应用中越来越受到关注。
- en: Several studies have explored the interpretability of [DTL](#Sx1.16.16.16) models.
    In one study [ramakrishnan2016towards], an agent was designed to explain how it
    learns a new task using prior common knowledge, aiming to enhance users’ trust
    and acceptance of the system results. Another study [kim2019the] defined interpretable
    features in a [DTL](#Sx1.16.16.16) algorithm and examined the relationship between
    the [SD](#Sx1.40.40.40) and TD in the task, focusing on the interpretability of
    the pretrained [DTL](#Sx1.16.16.16) model.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究探讨了 [DTL](#Sx1.16.16.16) 模型的可解释性。在一项研究 [ramakrishnan2016towards] 中，设计了一个代理来解释其如何利用先前的常识学习新任务，旨在增强用户对系统结果的信任和接受度。另一项研究
    [kim2019the] 在 [DTL](#Sx1.16.16.16) 算法中定义了可解释特征，并研究了任务中 [SD](#Sx1.40.40.40) 和
    TD 之间的关系，重点关注预训练 [DTL](#Sx1.16.16.16) 模型的可解释性。
- en: The work by Lee et al. [lee2021interpretable] introduced a knowledge distillation
    approach that generated interpretable embedding procedure (IEP) knowledge based
    on PCA and transferred it to the student network using a message passing neural
    network. This approach enhanced interpretability while maintaining accuracy through
    multi-task learning. Additionally, Carr et al. proposed an interpretable staged
    TL (iSTL) scheme for accurate and explainable classification of optical coherence
    tomography (OCT) scans with a small sample size [carr2021interpretable]. iSTL
    outperformed [DTL](#Sx1.16.16.16) techniques on unseen data, utilizing clinical
    features for predictions with interpretable attention maps.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: Lee等人的研究[lee2021interpretable]引入了一种知识蒸馏方法，该方法基于PCA生成了可解释的嵌入过程（IEP）知识，并使用消息传递神经网络将其转移到学生网络中。该方法通过多任务学习增强了可解释性，同时保持了准确性。此外，Carr等人提出了一种可解释的分阶段TL（iSTL）方案，用于小样本量的光学相干断层扫描（OCT）图像的准确和可解释的分类[carr2021interpretable]。iSTL在未见数据上优于[DTL](#Sx1.16.16.16)技术，利用临床特征进行预测，并提供了可解释的注意力图。
- en: Delving deeper into techniques for FL distillation (optimizing model size) within
    [FL](#Sx1.20.20.20) frameworks is essential. This exploration involves researching
    methods to compress neural [ASR](#Sx1.4.4.4) models effectively while ensuring
    their performance remains intact, especially tailored for edge devices with storage
    and computational constraints. It is imperative to investigate the trade-offs
    associated with reducing model size while maintaining performance metrics like
    [WER](#Sx1.52.52.52). Developing strategies to strike a balance between downsizing
    models and preserving satisfactory performance levels within federated learning
    environments is crucial.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 深入研究[FL](#Sx1.20.20.20)框架中的FL蒸馏技术（优化模型大小）是必要的。这项探索涉及研究在确保性能不受影响的情况下有效压缩神经[ASR](#Sx1.4.4.4)模型的方法，特别是针对具有存储和计算限制的边缘设备。必须调查在减少模型大小的同时保持性能指标如[WER](#Sx1.52.52.52)的权衡。制定在联邦学习环境中缩小模型并保持满意性能水平的策略至关重要。
- en: 6.6 Recent RL techniques for [ASR](#Sx1.4.4.4)
  id: totrans-549
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 最近的RL技术用于[ASR](#Sx1.4.4.4)
- en: Exploring incremental [DRL](#Sx1.15.15.15) approaches [[136](#bib.bib136), [137](#bib.bib137),
    [138](#bib.bib138)] in DRL-based [ASR](#Sx1.4.4.4) systems, could be very interesting.
    This approach involves the model continuously learning from newly acquired data
    and dynamically adjusting its ASR functionalities over time. By incrementally
    updating its knowledge base, the model can enhance its performance without necessitating
    full retraining, thus enabling continual enhancement of [ASR](#Sx1.4.4.4) systems.
    This capability not only fosters greater resilience and adaptability in speech
    recognition capabilities but also offers potential applications in scenarios where
    real-time adaptation to changing conditions is crucial, such as in noisy environments
    or with varying speaker accents. Moreover, incremental RL can potentially lead
    to more efficient use of computational resources, as the model only needs to focus
    on learning from new data, rather than reprocessing the entire dataset. Further
    research in this area could unlock new possibilities for [ASR](#Sx1.4.4.4) systems
    to evolve and improve over time, ultimately enhancing their usability and effectiveness
    in diverse real-world settings.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 探索增量[DRL](#Sx1.15.15.15)方法[[136](#bib.bib136), [137](#bib.bib137), [138](#bib.bib138)]在基于DRL的[ASR](#Sx1.4.4.4)系统中的应用，可能会非常有趣。这种方法涉及模型不断从新获得的数据中学习，并随着时间的推移动态调整其ASR功能。通过增量更新知识库，模型可以提高性能而无需完全重新训练，从而实现[ASR](#Sx1.4.4.4)系统的持续增强。这一能力不仅促进了语音识别能力的更大弹性和适应性，还为实时适应变化条件的场景（如噪声环境或变化的说话口音）提供了潜在的应用。此外，增量RL可能会更有效地利用计算资源，因为模型只需专注于从新数据中学习，而不是重新处理整个数据集。在这一领域的进一步研究可能会解锁[ASR](#Sx1.4.4.4)系统进化和改进的新可能性，*最终*提高其在各种现实世界环境中的可用性和有效性。
- en: Although some ASR schemes based on RL have been proposed, there remains a notable
    scarcity in the application of RL techniques to enhance ASR methods. While policy
    gradient and Q-learning are commonly employed, the realm of RL encompasses various
    subcategories such as [double deep Q-network](#Sx1.59.59.59) ([DDQN](#Sx1.59.59.59)),
    [actor-critic](#Sx1.60.60.60) ([AC](#Sx1.60.60.60)), [deep deterministic policy
    gradien](#Sx1.62.62.62) ([DDPG](#Sx1.62.62.62)), and more [[139](#bib.bib139)],
    which hold promise for advancing ASR with innovative approaches. Researchers are
    encouraged to delve into these diverse DRL-based methods to further enrich the
    field of ASR for both [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24) fields.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经提出了一些基于**RL**的**ASR**方案，但将**RL**技术应用于提升**ASR**方法的情况仍然相当稀缺。虽然策略梯度和Q学习被广泛应用，但**RL**领域包括各种子类别，如
    [双重深度Q网络](#Sx1.59.59.59) ([DDQN](#Sx1.59.59.59))、[演员-评论员](#Sx1.60.60.60) ([AC](#Sx1.60.60.60))、[深度确定性策略梯度](#Sx1.62.62.62)
    ([DDPG](#Sx1.62.62.62)) 等 [[139](#bib.bib139)]，这些子类别有望通过创新方法推动**ASR**的发展。鼓励研究人员深入探讨这些多样化的基于DRL的方法，以进一步丰富**ASR**领域的**AM**和**LM**。
- en: 6.7 Online [DTL](#Sx1.16.16.16)
  id: totrans-552
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 在线 [DTL](#Sx1.16.16.16)
- en: 'Online DTL combines the principles of TL and online learning with deep neural
    networks, enabling models to adapt in real-time to new tasks or data distributions.
    This approach is beneficial in dynamic environments where data arrives sequentially.
    [DL](#Sx1.14.14.14) models, specifically Deep Neural Networks (DNNs), learn through
    optimizing the weights $\theta$ to minimize a loss function $L$, which measures
    the discrepancy between predicted outputs $\hat{y}$ and true outputs $y$: $\theta^{*}=\arg\min_{\theta}L(D;\theta)$.
    On the other hand, TL improves learning in a new target task through the transfer
    of knowledge from a related source task, adapting a pre-trained model $\theta_{S}$
    on $D_{S}$ to a $D_{T}$: $\theta_{T}^{*}=\arg\min_{\theta}L(D_{T};\theta_{T})$.
    In this regard, online learning updates the model incrementally as new data $(x_{t},y_{t})$
    arrives:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在线**DTL**结合了**TL**和在线学习的原理以及深度神经网络，使模型能够实时适应新的任务或数据分布。这种方法在数据顺序到达的动态环境中非常有用。
    [DL](#Sx1.14.14.14)模型，特别是深度神经网络（DNNs），通过优化权重$\theta$来最小化损失函数$L$，该函数测量预测输出$\hat{y}$与真实输出$y$之间的差异：$\theta^{*}=\arg\min_{\theta}L(D;\theta)$。另一方面，**TL**通过将知识从相关源任务转移到新的目标任务来提高学习，将在$D_{S}$上预训练的模型$\theta_{S}$适应到$D_{T}$上：$\theta_{T}^{*}=\arg\min_{\theta}L(D_{T};\theta_{T})$。在这方面，在线学习在新数据$(x_{t},y_{t})$到达时逐步更新模型：
- en: '|  | $\theta_{t+1}=\theta_{t}-\alpha_{t}\nabla_{\theta}L(y_{t},f(x_{t};\theta_{t}))$
    |  | (11) |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{t+1}=\theta_{t}-\alpha_{t}\nabla_{\theta}L(y_{t},f(x_{t};\theta_{t}))$
    |  | (11) |'
- en: Where $\alpha_{t}$ is the learning rate, and $\nabla_{\theta}L$ is the gradient
    of the loss with respect to $\theta$. Moving on, online TL integrates these concepts
    to continuously adapt a deep learning model to new tasks or data streams, often
    involving techniques such as feature extraction, fine-tuning, model adaptation,
    and continual learning. The adaptation process at each time step $t$ can be viewed
    as $\theta_{t+1}^{*}=\arg\min_{\theta}L(D_{T_{t}};\theta_{T_{t}})$. Where $D_{T_{t}}$
    represents the data available at time $t$, including new target domain data.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha_{t}$是学习率，$\nabla_{\theta}L$是损失函数相对于$\theta$的梯度。接下来，在线**TL**将这些概念整合在一起，以持续适应新的任务或数据流，通常涉及特征提取、微调、模型适应和持续学习等技术。在每个时间步骤$t$的适应过程可以视为$\theta_{t+1}^{*}=\arg\min_{\theta}L(D_{T_{t}};\theta_{T_{t}})$。其中$D_{T_{t}}$表示时间$t$时可用的数据，包括新的目标领域数据。
- en: The approach of online DTL offers a forward-looking solution to this issue.
    Particularly, discrepancies in class distributions and the representation of features
    between the $D_{S}$ and $D_{T}$ amplify the complexity of online DTL [[140](#bib.bib140)].
    To navigate the complexities mentioned, online DTL has been dissected into two
    primary methodologies. The first, known as homogeneous online DTL, operates on
    the premise of a unified feature space across both domains. Conversely, heterogeneous
    online DTL acknowledges the distinct feature spaces intrinsic to each domain [[141](#bib.bib141)].
    An exemplary solution to the challenges of heterogeneous online DTL includes leveraging
    unlabeled instances of co-occurrence to forge a connective bridge between the
    $D_{S}$ and $D_{T}$, facilitating the precursor to knowledge transfer [[142](#bib.bib142)].
    Furthering the discourse, online DTL augmented with extreme learning machines
    introduces a novel framework [[143](#bib.bib143)]. Addressing the challenge of
    limited data in the $D_{T}$, the technique of transfer learning with lag (TLL),
    rooted in shallow neural network embeddings, has been applied. This method ensures
    the continuity of knowledge transfer, notwithstanding fluctuations in the feature
    set.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 在线DTL的方法为此问题提供了前瞻性的解决方案。特别是，$D_{S}$和$D_{T}$之间的类别分布和特征表示的差异加剧了在线DTL的复杂性[[140](#bib.bib140)]。为了应对上述复杂性，在线DTL被细分为两种主要方法。第一种，被称为同质在线DTL，基于两个领域之间的统一特征空间。相反，异质在线DTL承认每个领域固有的不同特征空间[[141](#bib.bib141)]。对异质在线DTL挑战的一个典型解决方案包括利用未标记的共现实例在$D_{S}$和$D_{T}$之间建立连接桥梁，促进知识转移的前奏[[142](#bib.bib142)]。进一步讨论，增强极端学习机的在线DTL引入了一种新框架[[143](#bib.bib143)]。为了解决$D_{T}$中数据有限的问题，应用了基于浅层神经网络嵌入的传输学习滞后（TLL）技术。这种方法确保了知识转移的连续性，尽管特征集会发生波动。
- en: 6.8 Transformers and LLMs-based ASR
  id: totrans-557
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8 基于变压器和LLM的ASR
- en: \Acp
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: \Acp
- en: LLM and transformers represent the forefront of [AI](#Sx1.1.1.1), trained on
    vast datasets spanning various domains, including text, speech, images, and multi-modal
    inputs. Despite extensive research on [ASR](#Sx1.4.4.4), existing [SOTA](#Sx1.44.44.44)
    approaches often lack integration of advanced [AI](#Sx1.1.1.1) techniques like
    [DRL](#Sx1.15.15.15) and [FL](#Sx1.20.20.20) into both [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24)
    domains. For example, \AcLLM based on [DTL](#Sx1.16.16.16) has demonstrated significant
    potential for [ASR](#Sx1.4.4.4) tasks, particularly for both [LM](#Sx1.24.24.24)
    and [AM](#Sx1.2.2.2) components. The incorporation of TL techniques into [large
    language model](#Sx1.58.58.58) ([LLM](#Sx1.58.58.58)) facilitates the transfer
    of knowledge from extensive pre-training tasks to enhance [ASR](#Sx1.4.4.4) effectiveness.
    In terms of [AM](#Sx1.2.2.2), fine-tuning [LLM](#Sx1.58.58.58) can leverage insights
    gained from pre-trained models exposed to sample acoustic data. This enables the
    [AM](#Sx1.2.2.2) component to grasp acoustic features like spectrograms or Mel-frequency
    cepstral coefficients (MFCCs) and utilize pre-trained knowledge to improve speech
    recognition accuracy. Through fine-tuning, the model can adjust and specialize
    in specific datasets or acoustic domains, leading to enhanced [ASR](#Sx1.4.4.4)
    performance.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 和变压器代表了[AI](#Sx1.1.1.1)的前沿，训练于跨越文本、语音、图像和多模态输入等各种领域的大型数据集。尽管对[ASR](#Sx1.4.4.4)进行了广泛的研究，现有的[SOTA](#Sx1.44.44.44)方法往往缺乏将先进的[AI](#Sx1.1.1.1)技术如[DRL](#Sx1.15.15.15)和[FL](#Sx1.20.20.20)整合到[AM](#Sx1.2.2.2)和[LM](#Sx1.24.24.24)领域中的能力。例如，基于[DTL](#Sx1.16.16.16)的\AcLLM在[ASR](#Sx1.4.4.4)任务中展现出显著潜力，特别是在[LM](#Sx1.24.24.24)和[AM](#Sx1.2.2.2)组件方面。将TL技术纳入[大型语言模型](#Sx1.58.58.58)（[LLM](#Sx1.58.58.58)）有助于将来自广泛预训练任务的知识转移，以提升[ASR](#Sx1.4.4.4)的有效性。在[AM](#Sx1.2.2.2)方面，微调[LLM](#Sx1.58.58.58)可以利用从预训练模型中获得的对样本声学数据的洞察。这使得[AM](#Sx1.2.2.2)组件能够理解声学特征如声谱图或梅尔频率倒谱系数（MFCCs），并利用预训练知识来提高语音识别的准确性。通过微调，模型可以调整和专门化于特定数据集或声学领域，从而提升[ASR](#Sx1.4.4.4)性能。
- en: Similarly, the [LM](#Sx1.24.24.24) aspect of [LLM](#Sx1.58.58.58) based on [DTL](#Sx1.16.16.16)
    can enhance [ASR](#Sx1.4.4.4) by leveraging TL. Pre-training [LLM](#Sx1.58.58.58)
    on vast text corpora equips it with extensive language representations, aiding
    in addressing diverse language challenges. Fine-tuning enables adaptation to specific
    language characteristics, improving transcription accuracy and contextual appropriateness.
    Both [AM](#Sx1.2.2.2) and [LM](#Sx1.24.24.24) fine-tuning can benefit from [DA](#Sx1.13.13.13),
    incorporating target domain data to tailor models, reducing domain mismatch, and
    enhancing effectiveness and generalization. Utilizing [LLM](#Sx1.58.58.58) for
    objective [ASR](#Sx1.4.4.4) testing and MOS evaluation involves compiling diverse
    datasets, fine-tuning [LLM](#Sx1.58.58.58), and integrating it into the [ASR](#Sx1.4.4.4)
    system. Evaluation metrics like [CER](#Sx1.8.8.8) and Pearson’s correlation gauge
    system performance, guiding further fine-tuning iterations for improved results.
    This iterative process ensures the [ASR](#Sx1.4.4.4) system’s continual enhancement
    and accurate MOS scale generation. Researchers are invited to explore these gaps
    and advance the integration of more advanced [AI](#Sx1.1.1.1) techniques such
    as [DRL](#Sx1.15.15.15) and [FL](#Sx1.20.20.20) into both [AM](#Sx1.2.2.2) and
    [LM](#Sx1.24.24.24) domains within transformer-based models. Additionally, there
    is a need for further investigation into Transformers and DTL-based ASR schemes
    specifically tailored for the [LM](#Sx1.24.24.24) domain. Closing these gaps will
    contribute to the development of more robust and effective language models across
    various applications.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，基于[DTL](#Sx1.16.16.16)的[LLM](#Sx1.58.58.58)的[LM](#Sx1.24.24.24)方面可以通过利用TL来增强[ASR](#Sx1.4.4.4)。在大量文本语料上进行预训练的[LLM](#Sx1.58.58.58)赋予其广泛的语言表示，帮助解决各种语言挑战。微调使其能够适应特定语言特征，提高转录准确性和语境适切性。[AM](#Sx1.2.2.2)和[LM](#Sx1.24.24.24)的微调都可以从[DA](#Sx1.13.13.13)中受益，通过整合目标领域数据来调整模型，减少领域不匹配，提高效果和泛化能力。利用[LLM](#Sx1.58.58.58)进行客观的[ASR](#Sx1.4.4.4)测试和MOS评估涉及编制多样化的数据集，微调[LLM](#Sx1.58.58.58)，并将其集成到[ASR](#Sx1.4.4.4)系统中。像[CER](#Sx1.8.8.8)和皮尔逊相关系数这样的评估指标衡量系统性能，指导进一步的微调迭代，以获得更好的结果。这一迭代过程确保了[ASR](#Sx1.4.4.4)系统的持续改进和准确的MOS评分生成。邀请研究人员探索这些空白，推动将更先进的[AI](#Sx1.1.1.1)技术如[DRL](#Sx1.15.15.15)和[FL](#Sx1.20.20.20)融入基于变换器的[AM](#Sx1.2.2.2)和[LM](#Sx1.24.24.24)领域。此外，还需要进一步研究专门针对[LM](#Sx1.24.24.24)领域的变换器和基于DTL的ASR方案。填补这些空白将有助于在各种应用中开发更强大、更有效的语言模型。
- en: 7 Conclusion
  id: totrans-561
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In conclusion, recent advancements in deep learning have presented both challenges
    and opportunities for[ASR](#Sx1.4.4.4). Traditional [ASR](#Sx1.4.4.4) systems
    require extensive training datasets, often including confidential information,
    and consume significant computational resources. However, the demand for adaptive
    systems capable of performing well in dynamic environments has spurred the development
    of advanced deep learning techniques such as [DTL](#Sx1.16.16.16), [FL](#Sx1.20.20.20),
    and [DRL](#Sx1.15.15.15), with all their variant techniques. These advanced techniques
    address issues related to [DA](#Sx1.13.13.13), privacy preservation, and dynamic
    decision-making, thereby enhancing [ASR](#Sx1.4.4.4) performance and reducing
    computational costs.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，深度学习的最新进展带来了[ASR](#Sx1.4.4.4)的挑战和机遇。传统的[ASR](#Sx1.4.4.4)系统需要大量训练数据集，通常包含机密信息，并消耗大量计算资源。然而，对在动态环境中表现良好的自适应系统的需求推动了先进深度学习技术的发展，如[DTL](#Sx1.16.16.16)、[FL](#Sx1.20.20.20)和[DRL](#Sx1.15.15.15)，以及所有变体技术。这些先进技术解决了与[DA](#Sx1.13.13.13)相关的问题、隐私保护和动态决策，从而提高了[ASR](#Sx1.4.4.4)性能并降低了计算成本。
- en: This survey has provided a comprehensive review of [DTL](#Sx1.16.16.16), [FL](#Sx1.20.20.20),
    and [RL](#Sx1.35.35.35)-based ASR frameworks, offering insights into the latest
    developments and helping researchers and professionals understand current challenges.
    Additionally, the integration of transformers, powerful [DL](#Sx1.14.14.14) models,
    has been explored for their ability to capture complex dependencies in ASR sequences.
    By presenting a structured taxonomy and conducting critical analyses, this paper
    has shed light on the strengths and weaknesses of existing frameworks, as well
    as highlighted ongoing challenges. Moving forward, further research is needed
    to overcome these challenges and unlock the full potential of advanced DL techniques
    in ASR. Future work should focus on refining existing approaches, addressing privacy
    concerns in FL, improving RL algorithms for ASR optimization, and exploring innovative
    ways to leverage transformers for more efficient and accurate speech recognition.
    By continuing to innovate and collaborate across disciplines, we can push the
    boundaries of [ASR](#Sx1.4.4.4) technology and realize its transformative impact
    on various fields, including healthcare, communication, and accessibility.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查提供了对基于[DTL](#Sx1.16.16.16)、[FL](#Sx1.20.20.20)和[RL](#Sx1.35.35.35)的ASR框架的全面回顾，提供了对最新发展的见解，并帮助研究人员和专业人士理解当前的挑战。此外，还探讨了变压器的集成，这些强大的[DL](#Sx1.14.14.14)模型能够捕捉ASR序列中的复杂依赖关系。通过呈现结构化分类法和进行关键分析，本文阐明了现有框架的优势和劣势，并突出了持续存在的挑战。未来，仍需进一步研究以克服这些挑战，发挥先进DL技术在ASR中的全部潜力。未来的工作应集中在改进现有方法、解决FL中的隐私问题、改进RL算法以优化ASR，并探索利用变压器实现更高效准确的语音识别的创新方法。通过继续在各学科之间进行创新和合作，我们可以推动[ASR](#Sx1.4.4.4)技术的边界，并实现其在医疗、通信和无障碍等领域的变革性影响。
- en: References
  id: totrans-564
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] H. Haneche, A. Ouahabi, B. Boudraa, Compressed sensing-speech coding scheme
    for mobile communications, Circuits, Systems, and Signal Processing (2021) 1–21.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] H. Haneche, A. Ouahabi, B. Boudraa, 用于移动通信的压缩感知语音编码方案，电路、系统与信号处理（2021）1–21。'
- en: '[2] D. Michelsanti, Z.-H. Tan, S.-X. Zhang, Y. Xu, M. Yu, D. Yu, J. Jensen,
    An overview of deep-learning-based audio-visual speech enhancement and separation,
    IEEE/ACM Transactions on Audio, Speech, and Language Processing (2021).'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. Michelsanti, Z.-H. Tan, S.-X. Zhang, Y. Xu, M. Yu, D. Yu, J. Jensen,
    基于深度学习的音频-视觉语音增强与分离综述，IEEE/ACM音频、语音与语言处理汇刊（2021）。'
- en: '[3] Y. Luo, C. Han, N. Mesgarani, Group communication with context codec for
    lightweight source separation, IEEE/ACM Transactions on Audio, Speech, and Language
    Processing 29 (2021) 1752–1761.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Luo, C. Han, N. Mesgarani, 基于上下文编解码器的轻量级源分离组通信，IEEE/ACM音频、语音与语言处理汇刊
    29 (2021) 1752–1761。'
- en: '[4] H. Kheddar, M. Bouzid, D. Megías, Pitch and fourier magnitude based steganography
    for hiding 2.4 kbps melp bitstream, IET Signal Processing 13 (3) (2019) 396–407.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] H. Kheddar, M. Bouzid, D. Megías, 基于音高和傅里叶幅度的隐写术用于隐藏2.4 kbps MELP比特流，IET信号处理
    13 (3) (2019) 396–407。'
- en: '[5] H. Kheddar, A. C. Mazari, G. H. Ilk, Speech steganography based on double
    approximation of lsfs parameters in amr coding, in: 2022 7th International Conference
    on Image and Signal Processing and their Applications (ISPA), IEEE, 2022, pp.
    1–8.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] H. Kheddar, A. C. Mazari, G. H. Ilk, 基于双重近似的AMR编码中的LSFS参数语音隐写术，见：2022年第七届国际图像与信号处理及其应用会议（ISPA），IEEE，2022，第1–8页。'
- en: '[6] H. Kheddar, D. Megias, M. Bouzid, Fourier magnitude-based steganography
    for hiding 2.4 kbpsmelp secret speech, in: 2018 International Conference on Applied
    Smart Systems (ICASS), IEEE, 2018, pp. 1–5.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] H. Kheddar, D. Megias, M. Bouzid, 基于傅里叶幅度的隐写术用于隐藏2.4 kbps MELP秘密语音，见：2018年应用智能系统国际会议（ICASS），IEEE，2018，第1–5页。'
- en: '[7] H. Yassine, B. Bachir, K. Aziz, A secure and high robust audio watermarking
    system for copyright protection, International Journal of Computer Applications
    53 (17) (2012) 33–39.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] H. Yassine, B. Bachir, K. Aziz, 一种安全且高鲁棒性的音频水印系统用于版权保护，国际计算机应用杂志 53 (17)
    (2012) 33–39。'
- en: '[8] M. Yamni, H. Karmouni, M. Sayyouri, H. Qjidaa, Efficient watermarking algorithm
    for digital audio/speech signal, Digital Signal Processing 120 (2022) 103251.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] M. Yamni, H. Karmouni, M. Sayyouri, H. Qjidaa, 高效的数字音频/语音信号水印算法，数字信号处理
    120 (2022) 103251。'
- en: '[9] H. Chen, B. D. Rouhani, F. Koushanfar, Specmark: A spectral watermarking
    framework for ip protection of speech recognition systems., in: INTERSPEECH, 2020,
    pp. 2312–2316.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] H. Chen, B. D. Rouhani, F. Koushanfar, Specmark: 用于语音识别系统IP保护的光谱水印框架，见：INTERSPEECH，2020，第2312–2316页。'
- en: '[10] M. Olivieri, R. Malvermi, M. Pezzoli, M. Zanoni, S. Gonzalez, F. Antonacci,
    A. Sarti, Audio information retrieval and musical acoustics, IEEE Instrumentation
    & Measurement Magazine 24 (7) (2021) 10–20.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. Olivieri, R. Malvermi, M. Pezzoli, M. Zanoni, S. Gonzalez, F. Antonacci,
    A. Sarti, 音频信息检索与音乐声学，IEEE 仪器与测量杂志 24 (7) (2021) 10–20。'
- en: '[11] E. Wold, T. Blum, D. Keislar, J. Wheaten, Content-based classification,
    search, and retrieval of audio, IEEE multimedia 3 (3) (1996) 27–36.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] E. Wold, T. Blum, D. Keislar, J. Wheaten, 基于内容的音频分类、搜索和检索，IEEE 多媒体 3 (3)
    (1996) 27–36。'
- en: '[12] W. Boes, et al., Audiovisual transfer learning for audio tagging and sound
    event detection, Proceedings Interspeech 2021 (2021).'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] W. Boes 等，音视频转移学习用于音频标记和声音事件检测，国际语音通信会议论文集 2021 (2021)。'
- en: '[13] Y. Tang, J. Pino, C. Wang, X. Ma, D. Genzel, A general multi-task learning
    framework to leverage text data for speech to text tasks, in: ICASSP 2021-2021
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    IEEE, 2021, pp. 6209–6213.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Tang, J. Pino, C. Wang, X. Ma, D. Genzel, 一种通用的多任务学习框架，用于利用文本数据进行语音到文本任务，载于：ICASSP
    2021-2021 IEEE 国际声学、语音与信号处理会议（ICASSP），IEEE，2021年，第 6209–6213 页。'
- en: '[14] F. M. Plaza-del Arco, M. D. Molina-González, L. A. Ureña-López, M. T.
    Martín-Valdivia, Comparing pre-trained language models for spanish hate speech
    detection, Expert Systems with Applications 166 (2021) 114120.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] F. M. Plaza-del Arco, M. D. Molina-González, L. A. Ureña-López, M. T.
    Martín-Valdivia, 比较用于西班牙语仇恨言论检测的预训练语言模型，专家系统与应用 166 (2021) 114120。'
- en: '[15] A. C. Mazari, H. Kheddar, Deep learning-based analysis of algerian dialect
    dataset targeted hate speech, offensive language and cyberbullying, International
    Journal of Computing and Digital Systems (2023).'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] A. C. Mazari, H. Kheddar, 基于深度学习的阿尔及利亚方言数据集分析：针对仇恨言论、攻击性语言和网络欺凌，计算与数字系统国际杂志
    (2023)。'
- en: '[16] D. Meghraoui, B. Boudraa, T. Merazi, P. G. Vilda, A novel pre-processing
    technique in pathologic voice detection: Application to parkinson’s disease phonation,
    Biomedical Signal Processing and Control 68 (2021) 102604.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] D. Meghraoui, B. Boudraa, T. Merazi, P. G. Vilda, 一种新颖的病理语音检测预处理技术：应用于帕金森病发音，生物医学信号处理与控制
    68 (2021) 102604。'
- en: '[17] Y.-Y. Lin, W.-Z. Zheng, W. C. Chu, J.-Y. Han, Y.-H. Hung, G.-M. Ho, C.-Y.
    Chang, Y.-H. Lai, A speech command control-based recognition system for dysarthric
    patients based on deep learning technology, Applied Sciences 11 (6) (2021) 2477.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y.-Y. Lin, W.-Z. Zheng, W. C. Chu, J.-Y. Han, Y.-H. Hung, G.-M. Ho, C.-Y.
    Chang, Y.-H. Lai, 基于深度学习技术的言语命令控制系统，用于失语症患者的识别，应用科学 11 (6) (2021) 2477。'
- en: '[18] Y. Kumar, S. Gupta, W. Singh, A novel deep transfer learning models for
    recognition of birds sounds in different environment, Soft Computing 26 (3) (2022)
    1003–1023.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. Kumar, S. Gupta, W. Singh, 一种新颖的深度迁移学习模型用于不同环境中的鸟鸣声识别，软计算 26 (3) (2022)
    1003–1023。'
- en: '[19] S. Padi, S. O. Sadjadi, R. D. Sriram, D. Manocha, Improved speech emotion
    recognition using transfer learning and spectrogram augmentation, in: Proceedings
    of the 2021 International Conference on Multimodal Interaction, 2021, pp. 645–652.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. Padi, S. O. Sadjadi, R. D. Sriram, D. Manocha, 通过迁移学习和谱图增强提高语音情感识别，载于：2021年国际多模态交互会议论文集，2021年，第
    645–652 页。'
- en: '[20] Y. Himeur, M. Elnour, F. Fadli, N. Meskin, I. Petri, Y. Rezgui, F. Bensaali,
    A. Amra, Next-generation energy systems for sustainable smart cities: Roles of
    transfer learning, Sustainable Cities and Society (2022) 1–35.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Himeur, M. Elnour, F. Fadli, N. Meskin, I. Petri, Y. Rezgui, F. Bensaali,
    A. Amra, 面向可持续智能城市的下一代能源系统：迁移学习的作用，可持续城市与社会 (2022) 1–35。'
- en: '[21] S. Niu, Y. Liu, J. Wang, H. Song, A decade survey of transfer learning
    (2010–2020), IEEE Transactions on Artificial Intelligence 1 (2) (2020) 151–166.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. Niu, Y. Liu, J. Wang, H. Song, 迁移学习十年回顾 (2010–2020)，IEEE 人工智能学报 1 (2)
    (2020) 151–166。'
- en: '[22] H. Kheddar, D. Megías, High capacity speech steganography for the g723\.
    1 coder based on quantised line spectral pairs interpolation and cnn auto-encoding,
    Applied Intelligence (2022) 1–19.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Kheddar, D. Megías, 高容量语音隐写术用于 g723.1 编码器，基于量化线谱对插值和 CNN 自动编码，应用智能
    (2022) 1–19。'
- en: '[23] T. A. de Lima, M. Da Costa-Abreu, A survey on automatic speech recognition
    systems for portuguese language and its variations, Computer Speech & Language
    62 (2020) 101055.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] T. A. de Lima, M. Da Costa-Abreu, 关于葡萄牙语及其变体的自动语音识别系统的调查，计算机语音与语言 62 (2020)
    101055。'
- en: '[24] A. Singh, V. Kadyan, M. Kumar, N. Bassan, Asroil: a comprehensive survey
    for automatic speech recognition of indian languages, Artificial Intelligence
    Review 53 (2020) 3673–3704.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. Singh, V. Kadyan, M. Kumar, N. Bassan, Asroil: 对印度语言自动语音识别的全面调查，人工智能评论
    53 (2020) 3673–3704。'
- en: '[25] R. S. Arslan, N. BARIŞÇI, A detailed survey of turkish automatic speech
    recognition, Turkish journal of electrical engineering and computer sciences 28 (6)
    (2020) 3253–3269.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] R. S. Arslan, N. BARIŞÇI, 土耳其自动语音识别的详细调查，土耳其电气工程与计算机科学杂志 28（6）（2020）3253–3269。'
- en: '[26] A. Dhouib, A. Othman, O. El Ghoul, M. K. Khribi, A. Al Sinani, Arabic
    automatic speech recognition: a systematic literature review, Applied Sciences
    12 (17) (2022) 8898.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Dhouib, A. Othman, O. El Ghoul, M. K. Khribi, A. Al Sinani, 阿拉伯语自动语音识别：系统文献综述，应用科学
    12（17）（2022）8898。'
- en: '[27] J. Kaur, A. Singh, V. Kadyan, Automatic speech recognition system for
    tonal languages: State-of-the-art survey, Archives of Computational Methods in
    Engineering 28 (2021) 1039–1068.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] J. Kaur, A. Singh, V. Kadyan, 音调语言的自动语音识别系统：最新进展综述，计算方法工程档案 28（2021）1039–1068。'
- en: '[28] A. A. Abushariah, H.-N. Ting, M. B. P. Mustafa, A. S. M. Khairuddin, M. A.
    Abushariah, T.-P. Tan, Bilingual automatic speech recognition: A review, taxonomy
    and open challenges, IEEE Access (2022).'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] A. A. Abushariah, H.-N. Ting, M. B. P. Mustafa, A. S. M. Khairuddin, M.
    A. Abushariah, T.-P. Tan, 双语自动语音识别：综述、分类和开放挑战，IEEE Access（2022）。'
- en: '[29] J. L. K. E. Fendji, D. C. Tala, B. O. Yenke, M. Atemkeng, Automatic speech
    recognition using limited vocabulary: A survey, Applied Artificial Intelligence
    36 (1) (2022) 2095039.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. L. K. E. Fendji, D. C. Tala, B. O. Yenke, M. Atemkeng, 使用有限词汇的自动语音识别：综述，应用人工智能
    36（1）（2022）2095039。'
- en: '[30] V. Bhardwaj, M. T. Ben Othman, V. Kukreja, Y. Belkhier, M. Bajaj, B. S.
    Goud, A. U. Rehman, M. Shafiq, H. Hamam, Automatic speech recognition (asr) systems
    for children: A systematic literature review, Applied Sciences 12 (9) (2022) 4419.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] V. Bhardwaj, M. T. Ben Othman, V. Kukreja, Y. Belkhier, M. Bajaj, B. S.
    Goud, A. U. Rehman, M. Shafiq, H. Hamam, 面向儿童的自动语音识别（ASR）系统：系统文献综述，应用科学 12（9）（2022）4419。'
- en: '[31] R. Errattahi, A. El Hannani, H. Ouahmane, Automatic speech recognition
    errors detection and correction: A review, Procedia Computer Science 128 (2018)
    32–37.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] R. Errattahi, A. El Hannani, H. Ouahmane, 自动语音识别错误检测与修正：综述，Procedia计算机科学
    128（2018）32–37。'
- en: '[32] H. Aldarmaki, A. Ullah, S. Ram, N. Zaki, Unsupervised automatic speech
    recognition: A review, Speech Communication 139 (2022) 76–91.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] H. Aldarmaki, A. Ullah, S. Ram, N. Zaki, 无监督自动语音识别：综述，语音通信 139（2022）76–91。'
- en: '[33] A. S. Dhanjal, W. Singh, A comprehensive survey on automatic speech recognition
    using neural networks, Multimedia Tools and Applications (2023) 1–46.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] A. S. Dhanjal, W. Singh, 基于神经网络的自动语音识别全面综述，多媒体工具与应用（2023）1–46。'
- en: '[34] A. B. Nassif, I. Shahin, I. Attili, M. Azzeh, K. Shaalan, Speech recognition
    using deep neural networks: A systematic review, IEEE access 7 (2019) 19143–19165.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. B. Nassif, I. Shahin, I. Attili, M. Azzeh, K. Shaalan, 使用深度神经网络的语音识别：系统综述，IEEE
    Access 7（2019）19143–19165。'
- en: '[35] M. Malik, M. K. Malik, K. Mehmood, I. Makhdoom, Automatic speech recognition:
    a survey, Multimedia Tools and Applications 80 (6) (2021) 9411–9457.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] M. Malik, M. K. Malik, K. Mehmood, I. Makhdoom, 自动语音识别：综述，多媒体工具与应用 80（6）（2021）9411–9457。'
- en: '[36] H. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, F. Bensaali, Deep transfer
    learning for automatic speech recognition: Towards better generalization, Knowledge-Based
    Systems 277 (2023) 110851.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] H. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, F. Bensaali, 深度迁移学习在自动语音识别中的应用：迈向更好的泛化，知识基础系统
    277（2023）110851。'
- en: '[37] F. Filippidou, L. Moussiades, A benchmarking of ibm, google and wit automatic
    speech recognition systems, in: IFIP International Conference on Artificial Intelligence
    Applications and Innovations, Springer, 2020, pp. 73–82.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] F. Filippidou, L. Moussiades, IBM、谷歌和WIT自动语音识别系统的基准测试，见：IFIP国际人工智能应用与创新会议，Springer，2020，第73–82页。'
- en: '[38] M. Suzuki, H. Sakaji, M. Hirano, K. Izumi, Constructing and analyzing
    domain-specific language model for financial text mining, Information Processing
    & Management 60 (2) (2023) 103194.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] M. Suzuki, H. Sakaji, M. Hirano, K. Izumi, 构建和分析用于金融文本挖掘的领域特定语言模型，信息处理与管理
    60（2）（2023）103194。'
- en: '[39] C.-H. H. Yang, Y. Gu, Y.-C. Liu, S. Ghosh, I. Bulyko, A. Stolcke, Generative
    speech recognition error correction with large language models and task-activating
    prompting, in: 2023 IEEE Automatic Speech Recognition and Understanding Workshop
    (ASRU), IEEE, 2023, pp. 1–8.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] C.-H. H. Yang, Y. Gu, Y.-C. Liu, S. Ghosh, I. Bulyko, A. Stolcke, 使用大语言模型和任务激活提示进行生成式语音识别错误修正，见：2023
    IEEE自动语音识别与理解研讨会（ASRU），IEEE，2023，第1–8页。'
- en: '[40] Z. Dong, Q. Ding, W. Zhai, M. Zhou, A speech recognition method based
    on domain-specific datasets and confidence decision networks, Sensors 23 (13)
    (2023) 6036.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Z. Dong, Q. Ding, W. Zhai, M. Zhou, 基于领域特定数据集和置信决策网络的语音识别方法，传感器 23（13）（2023）6036。'
- en: '[41] H. Kheddar, Y. Himeur, A. I. Awad, Deep transfer learning for intrusion
    detection in industrial control networks: A comprehensive review, Journal of Network
    and Computer Applications 220 (2023) 103760.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H. Kheddar, Y. Himeur, A. I. Awad, 用于工业控制网络入侵检测的深度迁移学习：全面综述，《网络与计算机应用杂志》220（2023）103760。'
- en: '[42] Y. Li, X. Zhang, P. Wang, X. Zhang, Y. Liu, Insight into an unsupervised
    two-step sparse transfer learning algorithm for speech diagnosis of parkinson’s
    disease, Neural Computing and Applications (2021) 1–18.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Li, X. Zhang, P. Wang, X. Zhang, Y. Liu, 对用于帕金森病语音诊断的无监督两步稀疏迁移学习算法的见解，《神经计算与应用》（2021）1–18。'
- en: '[43] O. Karaman, H. Çakın, A. Alhudhaif, K. Polat, Robust automated parkinson
    disease detection based on voice signals with transfer learning, Expert Systems
    with Applications 178 (2021) 115013.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] O. Karaman, H. Çakın, A. Alhudhaif, K. Polat, 基于语音信号的鲁棒自动化帕金森病检测与迁移学习，《专家系统应用》178（2021）115013。'
- en: '[44] R. A. Ramadan, Detecting adversarial attacks on audio-visual speech recognition
    using deep learning method, International Journal of Speech Technology (2021)
    1–7.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] R. A. Ramadan, 使用深度学习方法检测音视频语音识别中的对抗攻击，《语音技术国际期刊》（2021）1–7。'
- en: '[45] Q. Yu, Y. Ma, Y. Li, Enhancing speech recognition for parkinson’s disease
    patient using transfer learning technique, Journal of Shanghai Jiaotong University
    (Science) (2021) 1–9.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Q. Yu, Y. Ma, Y. Li, 利用迁移学习技术增强帕金森病患者的语音识别，《上海交通大学学报（理工版）》 （2021）1–9。'
- en: '[46] Y. Bai, J. Yi, J. Tao, Z. Tian, Z. Wen, S. Zhang, Fast end-to-end speech
    recognition via non-autoregressive models and cross-modal knowledge transferring
    from bert, IEEE/ACM Transactions on Audio, Speech, and Language Processing 29
    (2021) 1897–1911.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Y. Bai, J. Yi, J. Tao, Z. Tian, Z. Wen, S. Zhang, 通过非自回归模型和BERT的跨模态知识迁移实现快速端到端语音识别，《IEEE/ACM音频、语音与语言处理汇刊》29（2021）1897–1911。'
- en: '[47] I.-T. Recommendation, Perceptual evaluation of speech quality (pesq):
    An objective method for end-to-end speech quality assessment of narrow-band telephone
    networks and speech codecs, Rec. ITU-T P. 862 (2001).'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] I.-T. Recommendation, 语音质量的感知评估（pesq）：一种用于窄带电话网络和语音编解码器的端到端语音质量评估的客观方法，Rec.
    ITU-T P. 862（2001）。'
- en: '[48] S. Zhang, E. Loweimi, P. Bell, S. Renals, On the usefulness of self-attention
    for automatic speech recognition with transformers, in: 2021 IEEE Spoken Language
    Technology Workshop (SLT), IEEE, 2021, pp. 89–96.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] S. Zhang, E. Loweimi, P. Bell, S. Renals, 关于自注意力在变换器自动语音识别中的有效性，载于：2021
    IEEE语音语言技术研讨会（SLT），IEEE，2021年，页码89–96。'
- en: '[49] O. Hrinchuk, M. Popova, B. Ginsburg, Correction of automatic speech recognition
    with transformer sequence-to-sequence model, in: Icassp 2020-2020 ieee international
    conference on acoustics, speech and signal processing (icassp), IEEE, 2020, pp.
    7074–7078.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] O. Hrinchuk, M. Popova, B. Ginsburg, 使用变换器序列到序列模型纠正自动语音识别，在：Icassp 2020-2020
    IEEE国际声学、语音与信号处理会议（icassp），IEEE，2020年，页码7074–7078。'
- en: '[50] J. Li, R. Su, X. Xie, N. Yan, L. Wang, A multi-level acoustic feature
    extraction framework for transformer based end-to-end speech recognition, arXiv
    preprint arXiv:2108.07980 (2021).'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. Li, R. Su, X. Xie, N. Yan, L. Wang, 基于变换器的端到端语音识别的多级声学特征提取框架，arXiv预印本
    arXiv:2108.07980（2021）。'
- en: '[51] A. Baade, P. Peng, D. Harwath, Mae-ast: Masked autoencoding audio spectrogram
    transformer, arXiv preprint arXiv:2203.16691 (2022).'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] A. Baade, P. Peng, D. Harwath, Mae-ast：掩蔽自编码音频谱图变换器，arXiv预印本 arXiv:2203.16691（2022）。'
- en: '[52] J. Bai, J. Chen, M. Wang, M. S. Ayub, Q. Yan, A squeeze-and-excitation
    and transformer based cross-task model for environmental sound recognition, IEEE
    Transactions on Cognitive and Developmental Systems (2022).'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Bai, J. Chen, M. Wang, M. S. Ayub, Q. Yan, 基于挤压与激励以及变换器的跨任务模型用于环境声音识别，《IEEE认知与发展系统汇刊》（2022）。'
- en: '[53] K. Chen, J. Wang, F. Deng, X. Wang, icnn-transformer: An improved cnn-transformer
    with channel-spatial attention and keyword prediction for automated audio captioning,
    in: INTERSPEECH, 2022, pp. 4167–4171.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] K. Chen, J. Wang, F. Deng, X. Wang, icnn-transformer：一种改进的CNN-变换器，具有通道-空间注意力和关键字预测，用于自动音频描述，载于：INTERSPEECH，2022年，页码4167–4171。'
- en: '[54] K. Deng, S. Cao, Y. Zhang, L. Ma, Improving hybrid ctc/attention end-to-end
    speech recognition with pretrained acoustic and language models, in: 2021 IEEE
    Automatic Speech Recognition and Understanding Workshop (ASRU), IEEE, 2021, pp.
    76–82.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] K. Deng, S. Cao, Y. Zhang, L. Ma, 通过预训练的声学和语言模型改进混合CTC/注意力端到端语音识别，载于：2021
    IEEE自动语音识别与理解研讨会（ASRU），IEEE，2021年，页码76–82。'
- en: '[55] X. Zhou, E. Yılmaz, Y. Long, Y. Li, H. Li, Multi-encoder-decoder transformer
    for code-switching speech recognition, arXiv preprint arXiv:2006.10414 (2020).'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] X. Zhou, E. Yılmaz, Y. Long, Y. Li, H. Li, 《用于代码切换语音识别的多编码器-解码器变压器》，arXiv预印本
    arXiv:2006.10414（2020）。'
- en: '[56] G. I. Winata, S. Cahyawijaya, Z. Lin, Z. Liu, P. Fung, Lightweight and
    efficient end-to-end speech recognition using low-rank transformer, in: ICASSP
    2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP), IEEE, 2020, pp. 6144–6148.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] G. I. Winata, S. Cahyawijaya, Z. Lin, Z. Liu, P. Fung, 《使用低秩变压器的轻量级高效端到端语音识别》，收录于：ICASSP
    2020-2020 IEEE国际声学、语音与信号处理会议（ICASSP），IEEE，2020，第6144–6148页。'
- en: '[57] M.-H. Lee, S.-E. Lee, J.-S. Seong, J.-H. Chang, H. Kwon, C. Park, Regularizing
    transformer-based acoustic models by penalizing attention weights for robust speech
    recognition, in: Proceedings of the Annual Conference of the International Speech
    Communication Association, INTERSPEECH, Vol. 2022, International Speech Communication
    Association, 2022, pp. 56–60.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] M.-H. Lee, S.-E. Lee, J.-S. Seong, J.-H. Chang, H. Kwon, C. Park, 《通过惩罚注意力权重对变压器基础声学模型进行正则化以实现鲁棒的语音识别》，收录于：国际语音通信协会年会论文集，INTERSPEECH，卷2022，国际语音通信协会，2022，第56–60页。'
- en: '[58] S. R. Shareef, Y. F. Mohammed, Collaborative training of acoustic encoder
    for recognizing the impaired children speech, in: 2022 Fifth College of Science
    International Conference of Recent Trends in Information Technology (CSCTIT),
    IEEE, 2022, pp. 79–85.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] S. R. Shareef, Y. F. Mohammed, 《用于识别受损儿童语音的声学编码器的协作训练》，收录于：2022年第五届理学院国际信息技术趋势大会（CSCTIT），IEEE，2022，第79–85页。'
- en: '[59] R. Fan, W. Chu, P. Chang, A. Alwan, A ctc alignment-based non-autoregressive
    transformer for end-to-end automatic speech recognition, IEEE/ACM Transactions
    on Audio, Speech, and Language Processing 31 (2023) 1436–1448.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] R. Fan, W. Chu, P. Chang, A. Alwan, 《基于CTC对齐的非自回归变压器用于端到端自动语音识别》，IEEE/ACM音频、语音与语言处理交易，31（2023）1436–1448。'
- en: '[60] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep
    bidirectional transformers for language understanding (2019). [arXiv:1810.04805](http://arxiv.org/abs/1810.04805).'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, 《Bert：用于语言理解的深度双向变压器的预训练》（2019）。[arXiv:1810.04805](http://arxiv.org/abs/1810.04805)。'
- en: '[61] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen,
    A. Kannan, R. J. Weiss, K. Rao, E. Gonina, et al., State-of-the-art speech recognition
    with sequence-to-sequence models, in: 2018 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), IEEE, 2018, pp. 4774–4778.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen,
    A. Kannan, R. J. Weiss, K. Rao, E. Gonina, 等， 《基于序列到序列模型的最先进语音识别》，收录于：2018 IEEE国际声学、语音与信号处理会议（ICASSP），IEEE，2018，第4774–4778页。'
- en: '[62] Y. Wang, Y. Shi, F. Zhang, C. Wu, J. Chan, C.-F. Yeh, A. Xiao, Transformer
    in action: a comparative study of transformer-based acoustic models for large
    scale speech recognition applications, in: ICASSP 2021-2021 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2021, pp.
    6778–6782.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Y. Wang, Y. Shi, F. Zhang, C. Wu, J. Chan, C.-F. Yeh, A. Xiao, 《变压器在行动：基于变压器的声学模型在大规模语音识别应用中的比较研究》，收录于：ICASSP
    2021-2021 IEEE国际声学、语音与信号处理会议（ICASSP），IEEE，2021，第6778–6782页。'
- en: '[63] A. Aroudi, S. Uhlich, M. F. Font, Trunet: Transformer-recurrent-u network
    for multi-channel reverberant sound source separation, arXiv preprint arXiv:2110.04047
    (2021).'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] A. Aroudi, S. Uhlich, M. F. Font, 《Trunet：用于多通道混响声音源分离的变压器-递归-U网络》，arXiv预印本
    arXiv:2110.04047（2021）。'
- en: '[64] L. Wang, W. Wei, Y. Chen, Y. Hu, D 2 net: A denoising and dereverberation
    network based on two-branch encoder and dual-path transformer, in: 2022 Asia-Pacific
    Signal and Information Processing Association Annual Summit and Conference (APSIPA
    ASC), IEEE, 2022, pp. 1649–1654.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] L. Wang, W. Wei, Y. Chen, Y. Hu, 《D 2 net：一种基于双分支编码器和双路径变压器的去噪和去混响网络》，收录于：2022亚太信号与信息处理协会年会与会议（APSIPA
    ASC），IEEE，2022，第1649–1654页。'
- en: '[65] P. Swietojanski, S. Braun, D. Can, T. F. Da Silva, A. Ghoshal, T. Hori,
    R. Hsiao, H. Mason, E. McDermott, H. Silovsky, et al., Variable attention masking
    for configurable transformer transducer speech recognition, in: ICASSP 2023-2023
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    IEEE, 2023, pp. 1–5.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] P. Swietojanski, S. Braun, D. Can, T. F. Da Silva, A. Ghoshal, T. Hori,
    R. Hsiao, H. Mason, E. McDermott, H. Silovsky, 等， 《可配置变压器转换器语音识别的可变注意力掩蔽》，收录于：ICASSP
    2023-2023 IEEE国际声学、语音与信号处理会议（ICASSP），IEEE，2023，第1–5页。'
- en: '[66] N. Moritz, G. Wichern, T. Hori, J. Le Roux, All-in-one transformer: Unifying
    speech recognition, audio tagging, and event detection., in: INTERSPEECH, 2020,
    pp. 3112–3116.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] N. Moritz, G. Wichern, T. Hori, J. Le Roux, 《全能变换器：统一语音识别、音频标记和事件检测》，收录于：INTERSPEECH，2020，第3112–3116页。'
- en: '[67] W. Huang, W. Hu, Y. T. Yeung, X. Chen, Conv-transformer transducer: Low
    latency, low frame rate, streamable end-to-end speech recognition, arXiv preprint
    arXiv:2008.05750 (2020).'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] W. Huang, W. Hu, Y. T. Yeung, X. Chen, 《Conv-transformer转导器：低延迟、低帧率、可流式处理的端到端语音识别》，arXiv预印本
    arXiv:2008.05750 (2020)。'
- en: '[68] R. Fan, W. Chu, P. Chang, J. Xiao, Cass-nat: Ctc alignment-based single
    step non-autoregressive transformer for speech recognition, in: ICASSP 2021-2021
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    IEEE, 2021, pp. 5889–5893.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] R. Fan, W. Chu, P. Chang, J. Xiao, 《Cass-nat: 基于CTC对齐的单步非自回归变换器用于语音识别》，收录于：ICASSP
    2021-2021 IEEE国际声学、语音和信号处理会议（ICASSP），IEEE，2021，第5889–5893页。'
- en: '[69] M. Hadwan, H. A. Alsayadi, S. AL-Hagree, An end-to-end transformer-based
    automatic speech recognition for qur’an reciters., Computers, Materials & Continua
    74 (2) (2023).'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] M. Hadwan, H. A. Alsayadi, S. AL-Hagree, 《一种用于古兰经朗读者的端到端基于变换器的自动语音识别》，Computers,
    Materials & Continua 74 (2) (2023)。'
- en: '[70] L. Smietanka, T. Maka, Augmented transformer for speech detection in adverse
    acoustical conditions, in: 2023 Signal Processing: Algorithms, Architectures,
    Arrangements, and Applications (SPA), IEEE, 2023, pp. 14–18.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] L. Smietanka, T. Maka, 《用于恶劣声学条件下语音检测的增强变换器》，收录于：2023年信号处理：算法、架构、安排与应用会议（SPA），IEEE，2023，第14–18页。'
- en: '[71] Y. Li, D. Luo, Adversarial audio detection method based on transformer,
    in: 2022 International Conference on Machine Learning and Intelligent Systems
    Engineering (MLISE), IEEE, 2022, pp. 77–82.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Y. Li, D. Luo, 《基于变换器的对抗音频检测方法》，收录于：2022年国际机器学习与智能系统工程会议（MLISE），IEEE，2022，第77–82页。'
- en: '[72] C. Wang, M. Jia, Y. Zhang, L. Li, Parallel-path transformer network for
    time-domain monaural speech separation, in: 2023 International Conference on Cyber-Physical
    Social Intelligence (ICCSI), IEEE, 2023, pp. 509–514.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] C. Wang, M. Jia, Y. Zhang, L. Li, 《用于时间域单通道语音分离的并行路径变换器网络》，收录于：2023年国际网络物理社会智能会议（ICCSI），IEEE，2023，第509–514页。'
- en: '[73] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar, H. Huang,
    A. Tjandra, X. Zhang, F. Zhang, et al., Transformer-based acoustic modeling for
    hybrid speech recognition, in: ICASSP 2020-2020 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2020, pp. 6874–6878.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar, H. Huang,
    A. Tjandra, X. Zhang, F. Zhang, 等，《基于变换器的混合语音识别声学建模》，收录于：ICASSP 2020-2020 IEEE国际声学、语音和信号处理会议（ICASSP），IEEE，2020，第6874–6878页。'
- en: '[74] K. Wang, B. He, W.-P. Zhu, Tstnn: Two-stage transformer based neural network
    for speech enhancement in the time domain, in: ICASSP 2021-2021 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2021, pp.
    7098–7102.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] K. Wang, B. He, W.-P. Zhu, 《Tstnn: 基于两阶段变换器的神经网络用于时间域语音增强》，收录于：ICASSP
    2021-2021 IEEE国际声学、语音和信号处理会议（ICASSP），IEEE，2021，第7098–7102页。'
- en: '[75] Y. Gong, C.-I. Lai, Y.-A. Chung, J. Glass, Ssast: Self-supervised audio
    spectrogram transformer, in: Proceedings of the AAAI Conference on Artificial
    Intelligence, Vol. 36, 2022, pp. 10699–10709.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Y. Gong, C.-I. Lai, Y.-A. Chung, J. Glass, 《Ssast: 自监督音频谱图变换器》，收录于：AAAI人工智能会议论文集，第36卷，2022，第10699–10709页。'
- en: '[76] C. Wu, Y. Wang, Y. Shi, C.-F. Yeh, F. Zhang, Streaming transformer-based
    acoustic models using self-attention with augmented memory, arXiv preprint arXiv:2005.08042
    (2020).'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] C. Wu, Y. Wang, Y. Shi, C.-F. Yeh, F. Zhang, 《基于自注意力和增强记忆的流式变换器声学模型》，arXiv预印本
    arXiv:2005.08042 (2020)。'
- en: '[77] V. Nagaraja, Y. Shi, G. Venkatesh, O. Kalinli, M. L. Seltzer, V. Chandra,
    Collaborative training of acoustic encoders for speech recognition, arXiv preprint
    arXiv:2106.08960 (2021).'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] V. Nagaraja, Y. Shi, G. Venkatesh, O. Kalinli, M. L. Seltzer, V. Chandra,
    《声学编码器的协同训练用于语音识别》，arXiv预印本 arXiv:2106.08960 (2021)。'
- en: '[78] G. Ahmed, A. A. Lawaye, T. A. Mir, P. Rana, Toward developing attention-based
    end-to-end automatic speech recognition, in: International Conference On Innovative
    Computing And Communication, Springer, 2023, pp. 147–161.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] G. Ahmed, A. A. Lawaye, T. A. Mir, P. Rana, 《朝着开发基于注意力的端到端自动语音识别》，收录于：国际创新计算与通信会议，Springer，2023，第147–161页。'
- en: '[79] Y. Himeur, S. Al-Maadeed, H. Kheddar, N. Al-Maadeed, K. Abualsaud, A. Mohamed,
    T. Khattab, Video surveillance using deep transfer learning and deep domain adaptation:
    Towards better generalization, Engineering Applications of Artificial Intelligence
    119 (2023) 105698.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Y. Himeur, S. Al-Maadeed, H. Kheddar, N. Al-Maadeed, K. Abualsaud, A.
    Mohamed, T. Khattab, 使用深度迁移学习和深度领域适应的视频监控：迈向更好的泛化，工程人工智能应用 119 (2023) 105698。'
- en: '[80] H. Kheddar, M. Hemis, Y. Himeur, D. Megías, A. Amira, Deep learning for
    diverse data types steganalysis: A review, arXiv preprint arXiv:2308.04522 (2023).'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] H. Kheddar, M. Hemis, Y. Himeur, D. Megías, A. Amira, 针对多样数据类型的深度学习隐写分析：综述，arXiv
    预印本 arXiv:2308.04522 (2023)。'
- en: '[81] S. Schneider, A. Baevski, R. Collobert, M. Auli, wav2vec: Unsupervised
    pre-training for speech recognition, arXiv preprint arXiv:1904.05862 (2019).'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] S. Schneider, A. Baevski, R. Collobert, M. Auli, wav2vec：用于语音识别的无监督预训练，arXiv
    预印本 arXiv:1904.05862 (2019)。'
- en: '[82] J. Thienpondt, K. Demuynck, Transfer learning for robust low-resource
    children’s speech asr with transformers and source-filter warping, arXiv preprint
    arXiv:2206.09396 (2022).'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] J. Thienpondt, K. Demuynck, 具有变换器和源滤波器变形的稳健低资源儿童语音 ASR 的迁移学习，arXiv 预印本
    arXiv:2206.09396 (2022)。'
- en: '[83] Z. Dan, Y. Zhao, X. Bi, L. Wu, Q. Ji, Multi-task transformer with adaptive
    cross-entropy loss for multi-dialect speech recognition, Entropy 24 (10) (2022)
    1429.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Z. Dan, Y. Zhao, X. Bi, L. Wu, Q. Ji, 具有自适应交叉熵损失的多任务变换器用于多方言语音识别，熵 24
    (10) (2022) 1429。'
- en: '[84] T. Pellegrini, I. Khalfaoui-Hassani, E. Labbé, T. Masquelier, Adapting
    a convnext model to audio classification on audioset, arXiv preprint arXiv:2306.00830
    (2023).'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] T. Pellegrini, I. Khalfaoui-Hassani, E. Labbé, T. Masquelier, 将 convnext
    模型适应于 AudioSet 上的音频分类，arXiv 预印本 arXiv:2306.00830 (2023)。'
- en: '[85] Y. Xin, D. Yang, Y. Zou, Audio pyramid transformer with domain adaption
    for weakly supervised sound event detection and audio classification, in: Proc.
    Interspeech 2022, 2022, pp. 1546–1550.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Y. Xin, D. Yang, Y. Zou, 具有领域适应的音频金字塔变换器用于弱监督声音事件检测和音频分类，见：Proc. Interspeech
    2022, 2022，第 1546–1550 页。'
- en: '[86] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep
    bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805
    (2018).'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert：用于语言理解的深度双向变换器预训练，arXiv
    预印本 arXiv:1810.04805 (2018)。'
- en: '[87] Y. Song, D. Jiang, X. Zhao, Q. Xu, R. C.-W. Wong, L. Fan, Q. Yang, L2rs:
    a learning-to-rescore mechanism for automatic speech recognition, arXiv preprint
    arXiv:1910.11496 (2019).'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Y. Song, D. Jiang, X. Zhao, Q. Xu, R. C.-W. Wong, L. Fan, Q. Yang, L2rs：一种用于自动语音识别的学习重评分机制，arXiv
    预印本 arXiv:1910.11496 (2019)。'
- en: '[88] C.-X. Qin, D. Qu, L.-H. Zhang, Towards end-to-end speech recognition with
    transfer learning, EURASIP Journal on Audio, Speech, and Music Processing 2018 (1)
    (2018) 1–9.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C.-X. Qin, D. Qu, L.-H. Zhang, 借助迁移学习实现端到端语音识别，EURASIP 音频、语音与音乐处理期刊 2018
    (1) (2018) 1–9。'
- en: '[89] D. Jiang, C. Tan, J. Peng, C. Chen, X. Wu, W. Zhao, Y. Song, Y. Tong,
    C. Liu, Q. Xu, et al., A gdpr-compliant ecosystem for speech recognition with
    transfer, federated, and evolutionary learning, ACM Transactions on Intelligent
    Systems and Technology (TIST) 12 (3) (2021) 1–19.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] D. Jiang, C. Tan, J. Peng, C. Chen, X. Wu, W. Zhao, Y. Song, Y. Tong,
    C. Liu, Q. Xu, 等人，符合 GDPR 的语音识别生态系统，采用迁移学习、联邦学习和进化学习，ACM 智能系统与技术事务 (TIST) 12 (3)
    (2021) 1–19。'
- en: '[90] F. Weninger, J. Andrés-Ferrer, X. Li, P. Zhan, Listen, attend, spell and
    adapt: Speaker adapted sequence-to-sequence asr, arXiv preprint arXiv:1907.04916
    (2019).'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] F. Weninger, J. Andrés-Ferrer, X. Li, P. Zhan, 听、关注、拼写和适应：说话人适应的序列到序列
    ASR，arXiv 预印本 arXiv:1907.04916 (2019)。'
- en: '[91] S. Deena, M. Hasan, M. Doulaty, O. Saz, T. Hain, Recurrent neural network
    language model adaptation for multi-genre broadcast speech recognition and alignment,
    IEEE/ACM Transactions on Audio, Speech, and Language Processing 27 (3) (2018)
    572–582.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] S. Deena, M. Hasan, M. Doulaty, O. Saz, T. Hain, 多体裁广播语音识别和对齐的递归神经网络语言模型适应，IEEE/ACM
    音频、语音与语言处理事务 27 (3) (2018) 572–582。'
- en: '[92] S.-I. Ng, W. Liu, Z. Peng, S. Feng, H.-P. Huang, O. Scharenborg, T. Lee,
    The cuhk-tudelft system for the slt 2021 children speech recognition challenge,
    arXiv preprint arXiv:2011.06239 (2020).'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] S.-I. Ng, W. Liu, Z. Peng, S. Feng, H.-P. Huang, O. Scharenborg, T. Lee,
    cuhk-tudelft 系统在 SLT 2021 儿童语音识别挑战中的应用，arXiv 预印本 arXiv:2011.06239 (2020)。'
- en: '[93] K. Manohar, G. G. Menon, A. Abraham, R. Rajan, A. Jayan, Automatic recognition
    of continuous malayalam speech using pretrained multilingual transformers, in:
    2023 International Conference on Intelligent Systems for Communication, IoT and
    Security (ICISCoIS), IEEE, 2023, pp. 671–675.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] K. Manohar, G. G. Menon, A. Abraham, R. Rajan, A. Jayan, 使用预训练的多语言变换器自动识别连续的马拉雅拉姆语语音，见：2023
    国际智能系统通信、物联网与安全会议（ICISCoIS），IEEE，2023，第 671–675 页。'
- en: '[94] J. Cho, M. K. Baskar, R. Li, M. Wiesner, S. H. Mallidi, N. Yalta, M. Karafiat,
    S. Watanabe, T. Hori, Multilingual sequence-to-sequence speech recognition: architecture,
    transfer learning, and language modeling, in: 2018 IEEE Spoken Language Technology
    Workshop (SLT), IEEE, 2018, pp. 521–527.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] J. Cho, M. K. Baskar, R. Li, M. Wiesner, S. H. Mallidi, N. Yalta, M. Karafiat,
    S. Watanabe, T. Hori, 多语言序列到序列语音识别：架构、迁移学习和语言建模，见：2018 IEEE 语音语言技术研讨会（SLT），IEEE，2018，第
    521–527 页。'
- en: '[95] K. Li, Y. Song, I. McLoughlin, L. Liu, J. Li, L.-R. Dai, Fine-tuning audio
    spectrogram transformer with task-aware adapters for sound event detection.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] K. Li, Y. Song, I. McLoughlin, L. Liu, J. Li, L.-R. Dai, 通过任务感知适配器微调音频谱图变换器用于声音事件检测。'
- en: '[96] C. Wang, S. Dai, Y. Wang, F. Yang, M. Qiu, K. Chen, W. Zhou, J. Huang,
    Arobert: An asr robust pre-trained language model for spoken language understanding,
    IEEE/ACM Transactions on Audio, Speech, and Language Processing (2022).'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] C. Wang, S. Dai, Y. Wang, F. Yang, M. Qiu, K. Chen, W. Zhou, J. Huang,
    Arobert：用于口语理解的自动语音识别鲁棒预训练语言模型，IEEE/ACM 音频、语音与语言处理交易（2022）。'
- en: '[97] Y. Himeur, I. Varlamis, H. Kheddar, A. Amira, S. Atalla, Y. Singh, F. Bensaali,
    W. Mansoor, Federated learning for computer vision, arXiv preprint arXiv:2308.13558
    (2023).'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Y. Himeur, I. Varlamis, H. Kheddar, A. Amira, S. Atalla, Y. Singh, F.
    Bensaali, W. Mansoor, 联邦学习用于计算机视觉，arXiv 预印本 arXiv:2308.13558（2023）。'
- en: '[98] D. Dimitriadis, R. G. Ken’ichi Kumatani, R. Gmyr, Y. Gaur, S. E. Eskimez,
    A federated approach in training acoustic models., in: Interspeech, 2020, pp.
    981–985.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] D. Dimitriadis, R. G. Ken’ichi Kumatani, R. Gmyr, Y. Gaur, S. E. Eskimez,
    用于训练声学模型的联邦方法，见：Interspeech，2020，第 981–985 页。'
- en: '[99] D. Guliani, F. Beaufays, G. Motta, Training speech recognition models
    with federated learning: A quality/cost framework, in: ICASSP 2021-2021 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2021, pp.
    3080–3084.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] D. Guliani, F. Beaufays, G. Motta, 使用联邦学习训练语音识别模型：一个质量/成本框架，见：ICASSP 2021-2021
    IEEE 国际声学、语音与信号处理会议（ICASSP），IEEE，2021，第 3080–3084 页。'
- en: '[100] H. Zhu, J. Wang, G. Cheng, P. Zhang, Y. Yan, Decoupled federated learning
    for asr with non-iid data, arXiv preprint arXiv:2206.09102 (2022).'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] H. Zhu, J. Wang, G. Cheng, P. Zhang, Y. Yan, 针对非独立同分布数据的解耦联邦学习用于自动语音识别，arXiv
    预印本 arXiv:2206.09102（2022）。'
- en: '[101] X. Cui, S. Lu, B. Kingsbury, Federated acoustic modeling for automatic
    speech recognition, in: ICASSP 2021-2021 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), IEEE, 2021, pp. 6748–6752.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] X. Cui, S. Lu, B. Kingsbury, 联邦声学建模用于自动语音识别，见：ICASSP 2021-2021 IEEE 国际声学、语音与信号处理会议（ICASSP），IEEE，2021，第
    6748–6752 页。'
- en: '[102] T. Nguyen, S. Mdhaffar, N. Tomashenko, J.-F. Bonastre, Y. Estève, Federated
    learning for asr based on wav2vec 2.0, in: ICASSP 2023-2023 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2023, pp.
    1–5.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] T. Nguyen, S. Mdhaffar, N. Tomashenko, J.-F. Bonastre, Y. Estève, 基于
    wav2vec 2.0 的联邦学习，用于自动语音识别，见：ICASSP 2023-2023 IEEE 国际声学、语音与信号处理会议（ICASSP），IEEE，2023，第
    1–5 页。'
- en: '[103] C.-H. H. Yang, J. Qi, S. Y.-C. Chen, P.-Y. Chen, S. M. Siniscalchi, X. Ma,
    C.-H. Lee, Decentralizing feature extraction with quantum convolutional neural
    network for automatic speech recognition, in: ICASSP 2021-2021 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2021, pp.
    6523–6527.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] C.-H. H. Yang, J. Qi, S. Y.-C. Chen, P.-Y. Chen, S. M. Siniscalchi, X.
    Ma, C.-H. Lee, 使用量子卷积神经网络去中心化特征提取用于自动语音识别，见：ICASSP 2021-2021 IEEE 国际声学、语音与信号处理会议（ICASSP），IEEE，2021，第
    6523–6527 页。'
- en: '[104] Y. Gao, T. Parcollet, S. Zaiem, J. Fernandez-Marques, P. P. de Gusmao,
    D. J. Beutel, N. D. Lane, End-to-end speech recognition from federated acoustic
    models, in: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP), IEEE, 2022, pp. 7227–7231.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Y. Gao, T. Parcollet, S. Zaiem, J. Fernandez-Marques, P. P. de Gusmao,
    D. J. Beutel, N. D. Lane, 基于联邦声学模型的端到端语音识别，见：ICASSP 2022-2022 IEEE 国际声学、语音与信号处理会议（ICASSP），IEEE，2022，第
    7227–7231 页。'
- en: '[105] H. Mehmood, A. Dobrowolska, K. Saravanan, M. Ozay, Fednst: Federated
    noisy student training for automatic speech recognition, arXiv preprint arXiv:2206.02797
    (2022).'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] H. Mehmood, A. Dobrowolska, K. Saravanan, M. Ozay, Fednst：用于自动语音识别的联邦噪声学生训练，arXiv
    预印本 arXiv:2206.02797（2022）。'
- en: '[106] J. C. Vásquez-Correa, A. Álvarez Muniain, Novel speech recognition systems
    applied to forensics within child exploitation: Wav2vec2\. 0 vs. whisper, Sensors
    23 (4) (2023) 1843.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] J. C. Vásquez-Correa, A. Álvarez Muniain, 应用于儿童剥削的法医学的新型语音识别系统：Wav2vec2.0
    与 whisper，Sensors 23 (4) (2023) 1843。'
- en: '[107] C. Tan, D. Jiang, H. Mo, J. Peng, Y. Tong, W. Zhao, C. Chen, R. Lian,
    Y. Song, Q. Xu, Federated acoustic model optimization for automatic speech recognition,
    in: Database Systems for Advanced Applications: 25th International Conference,
    DASFAA 2020, Jeju, South Korea, September 24–27, 2020, Proceedings, Part III 25,
    Springer, 2020, pp. 771–774.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] C. Tan, D. Jiang, H. Mo, J. Peng, Y. Tong, W. Zhao, C. Chen, R. Lian,
    Y. Song, Q. Xu, 联邦声学模型优化用于自动语音识别，在：高级应用数据库系统：第25届国际会议，DASFAA 2020，韩国济州，2020年9月24–27日，论文集，第三部分
    25，Springer，2020，页码 771–774。'
- en: '[108] N. Tomashenko, S. Mdhaffar, M. Tommasi, Y. Estève, J.-F. Bonastre, Privacy
    attacks for automatic speech recognition acoustic models in a federated learning
    framework, in: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP), IEEE, 2022, pp. 6972–6976.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] N. Tomashenko, S. Mdhaffar, M. Tommasi, Y. Estève, J.-F. Bonastre, 在联邦学习框架下的自动语音识别声学模型的隐私攻击，在：ICASSP
    2022-2022 IEEE国际声学、语音和信号处理会议（ICASSP），IEEE，2022，页码 6972–6976。'
- en: '[109] D. Guliani, L. Zhou, C. Ryu, T.-J. Yang, H. Zhang, Y. Xiao, F. Beaufays,
    G. Motta, Enabling on-device training of speech recognition models with federated
    dropout, in: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech
    and Signal Processing (ICASSP), IEEE, 2022, pp. 8757–8761.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] D. Guliani, L. Zhou, C. Ryu, T.-J. Yang, H. Zhang, Y. Xiao, F. Beaufays,
    G. Motta, 通过联邦 dropout 实现设备上的语音识别模型训练，在：ICASSP 2022-2022 IEEE国际声学、语音和信号处理会议（ICASSP），IEEE，2022，页码
    8757–8761。'
- en: '[110] C. Chen, Y. Hu, N. Hou, X. Qi, H. Zou, E. S. Chng, Self-critical sequence
    training for automatic speech recognition, in: ICASSP 2022-2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2022, pp.
    3688–3692.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] C. Chen, Y. Hu, N. Hou, X. Qi, H. Zou, E. S. Chng, 自动语音识别的自我批判序列训练，在：ICASSP
    2022-2022 IEEE国际声学、语音和信号处理会议（ICASSP），IEEE，2022，页码 3688–3692。'
- en: '[111] T. Kala, T. Shinozaki, Reinforcement learning of speech recognition system
    based on policy gradient and hypothesis selection, in: 2018 IEEE international
    conference on acoustics, speech and signal processing (ICASSP), IEEE, 2018, pp.
    5759–5763.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] T. Kala, T. Shinozaki, 基于策略梯度和假设选择的语音识别系统强化学习，在：2018 IEEE国际声学、语音和信号处理会议（ICASSP），IEEE，2018，页码
    5759–5763。'
- en: '[112] A. Tjandra, S. Sakti, S. Nakamura, Sequence-to-sequence asr optimization
    via reinforcement learning, in: 2018 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), IEEE, 2018, pp. 5829–5833.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] A. Tjandra, S. Sakti, S. Nakamura, 通过强化学习优化序列到序列的自动语音识别，在：2018 IEEE国际声学、语音和信号处理会议（ICASSP），IEEE，2018，页码
    5829–5833。'
- en: '[113] A. Tjandra, S. Sakti, S. Nakamura, End-to-end speech recognition sequence
    training with reinforcement learning, IEEE Access 7 (2019) 79758–79769.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Tjandra, S. Sakti, S. Nakamura, 基于强化学习的端到端语音识别序列训练，IEEE Access 7 (2019)
    79758–79769。'
- en: '[114] Ł. Dudziak, M. S. Abdelfattah, R. Vipperla, S. Laskaridis, N. D. Lane,
    Shrinkml: End-to-end asr model compression using reinforcement learning, arXiv
    preprint arXiv:1907.03540 (2019).'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Ł. Dudziak, M. S. Abdelfattah, R. Vipperla, S. Laskaridis, N. D. Lane,
    Shrinkml：使用强化学习的端到端自动语音识别模型压缩，arXiv 预印本 arXiv:1907.03540 (2019)。'
- en: '[115] A. Mehrotra, Ł. Dudziak, J. Yeo, Y.-y. Lee, R. Vipperla, M. S. Abdelfattah,
    S. Bhattacharya, S. Ishtiaq, A. G. C. Ramos, S. Lee, et al., Iterative compression
    of end-to-end asr model using automl, arXiv preprint arXiv:2008.02897 (2020).'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] A. Mehrotra, Ł. Dudziak, J. Yeo, Y.-y. Lee, R. Vipperla, M. S. Abdelfattah,
    S. Bhattacharya, S. Ishtiaq, A. G. C. Ramos, S. Lee 等，使用自动机器学习的端到端自动语音识别模型的迭代压缩，arXiv
    预印本 arXiv:2008.02897 (2020)。'
- en: '[116] Y.-L. Shen, C.-Y. Huang, S.-S. Wang, Y. Tsao, H.-M. Wang, T.-S. Chi,
    Reinforcement learning based speech enhancement for robust speech recognition,
    in: ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP), IEEE, 2019, pp. 6750–6754.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Y.-L. Shen, C.-Y. Huang, S.-S. Wang, Y. Tsao, H.-M. Wang, T.-S. Chi,
    基于强化学习的语音增强以实现稳健的语音识别，在：ICASSP 2019-2019 IEEE国际声学、语音和信号处理会议（ICASSP），IEEE，2019，页码
    6750–6754。'
- en: '[117] R. T.-H. Tsai, C.-H. Chen, C.-K. Wu, Y.-C. Hsiao, H.-y. Lee, Using deep-q
    network to select candidates from n-best speech recognition hypotheses for enhancing
    dialogue state tracking, in: ICASSP 2019-2019 IEEE International Conference on
    Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2019, pp. 7375–7379.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] R. T.-H. Tsai, C.-H. Chen, C.-K. Wu, Y.-C. Hsiao, H.-y. Lee, 使用深度Q网络从n-best语音识别假设中选择候选者以增强对话状态跟踪，见：ICASSP
    2019-2019 IEEE国际声学、语音与信号处理会议（ICASSP），IEEE，2019，第7375–7379页。'
- en: '[118] H. Chung, H.-B. Jeon, J. G. Park, Semi-supervised training for sequence-to-sequence
    speech recognition using reinforcement learning, in: 2020 international joint
    conference on neural networks (IJCNN), IEEE, 2020, pp. 1–6.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] H. Chung, H.-B. Jeon, J. G. Park, 使用强化学习的序列到序列语音识别的半监督训练，见：2020年国际联合神经网络会议（IJCNN），IEEE，2020，第1–6页。'
- en: '[119] Z. Chen, W. Zhang, End-to-end speech recognition with reinforcement learning,
    in: Eighth International Conference on Electronic Technology and Information Science
    (ICETIS 2023), Vol. 12715, SPIE, 2023, pp. 392–398.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Z. Chen, W. Zhang, 使用强化学习的端到端语音识别，见：第八届电子技术与信息科学国际会议（ICETIS 2023），第12715卷，SPIE，2023，第392–398页。'
- en: '[120] A. Hamza, D. Addou, H. Kheddar, Machine learning approaches for automated
    detection and classification of dysarthria severity, in: 2023 2nd International
    Conference on Electronics, Energy and Measurement (IC2EM), Vol. 1, IEEE, 2023,
    pp. 1–6.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] A. Hamza, D. Addou, H. Kheddar, 自动检测和分类构音障碍严重程度的机器学习方法，见：2023年第2届国际电子、能源与测量会议（IC2EM），第1卷，IEEE，2023，第1–6页。'
- en: '[121] S. Feng, B. M. Halpern, O. Kudina, O. Scharenborg, Towards inclusive
    automatic speech recognition, Computer Speech & Language 84 (2024) 101567.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] S. Feng, B. M. Halpern, O. Kudina, O. Scharenborg, 迈向包容性的自动语音识别，《计算机语音与语言》84（2024）101567。'
- en: '[122] Z. Zhou, J. Chen, N. Wang, L. Li, D. Wang, Adversarial data augmentation
    for robust speaker verification, arXiv preprint arXiv:2402.02699 (2024).'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Z. Zhou, J. Chen, N. Wang, L. Li, D. Wang, 用于鲁棒说话人验证的对抗数据增强，arXiv预印本
    arXiv:2402.02699（2024）。'
- en: '[123] Adastreamlite: Environment-adaptive streaming speech recognition on mobile
    devices, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous
    Technologies 7 (4) (2024) 1–29.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Adastreamlite：移动设备上的环境自适应流媒体语音识别，《ACM互动、移动、可穿戴与普适技术会议论文集》7（4）（2024）1–29。'
- en: '[124] J. H. Yeo, M. Kim, J. Choi, D. H. Kim, Y. M. Ro, Akvsr: Audio knowledge
    empowered visual speech recognition by compressing audio knowledge of a pretrained
    model, IEEE Transactions on Multimedia (2024).'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] J. H. Yeo, M. Kim, J. Choi, D. H. Kim, Y. M. Ro, AKVSR：通过压缩预训练模型的音频知识实现音频知识增强的视觉语音识别，《IEEE多媒体学报》（2024）。'
- en: '[125] N. Djeffal, D. Addou, H. Kheddar, S. A. Selouani, Noise-robust speech
    recognition: A comparative analysis of lstm and cnn approaches, in: 2023 2nd International
    Conference on Electronics, Energy and Measurement (IC2EM), Vol. 1, IEEE, 2023,
    pp. 1–6.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] N. Djeffal, D. Addou, H. Kheddar, S. A. Selouani, 噪声鲁棒语音识别：LSTM和CNN方法的比较分析，见：2023年第2届国际电子、能源与测量会议（IC2EM），第1卷，IEEE，2023，第1–6页。'
- en: '[126] Q. Zhao, L. Yang, N. Lyu, A driver stress detection model via data augmentation
    based on deep convolutional recurrent neural network, Expert Systems with Applications
    238 (2024) 122056.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Q. Zhao, L. Yang, N. Lyu, 基于深度卷积递归神经网络的数据增强驱动压力检测模型，《专家系统应用》238（2024）122056。'
- en: '[127] Z. Jin, M. Geng, J. Deng, T. Wang, S. Hu, G. Li, X. Liu, Personalized
    adversarial data augmentation for dysarthric and elderly speech recognition, IEEE/ACM
    Transactions on Audio, Speech, and Language Processing (2023).'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Z. Jin, M. Geng, J. Deng, T. Wang, S. Hu, G. Li, X. Liu, 针对构音障碍和老年语音识别的个性化对抗数据增强，《IEEE/ACM音频、语音与语言处理汇刊》（2023）。'
- en: '[128] A. Brack, E. Entrup, M. Stamatakis, P. Buschermöhle, A. Hoppe, R. Ewerth,
    Sequential sentence classification in research papers using cross-domain multi-task
    learning, International Journal on Digital Libraries (2024) 1–24.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. Brack, E. Entrup, M. Stamatakis, P. Buschermöhle, A. Hoppe, R. Ewerth,
    使用跨领域多任务学习进行研究论文中的顺序句子分类，《国际数字图书馆期刊》（2024）1–24。'
- en: '[129] H. Zhang, M. Tao, Y. Shi, X. Bi, K. B. Letaief, Federated multi-task
    learning with non-stationary and heterogeneous data in wireless networks, IEEE
    Transactions on Wireless Communications (2023).'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] H. Zhang, M. Tao, Y. Shi, X. Bi, K. B. Letaief, 在无线网络中使用非平稳和异质数据的联邦多任务学习，《IEEE无线通信学报》（2023）。'
- en: '[130] A. Singh, S. Chandrasekar, T. Sen, S. Saha, Federated multi-task learning
    for complaint identification using graph attention network, IEEE Transactions
    on Artificial Intelligence (2023).'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] A. Singh, S. Chandrasekar, T. Sen, S. Saha, 使用图注意力网络的联邦多任务学习用于投诉识别，《IEEE人工智能汇刊》(2023)。'
- en: '[131] X. Jiang, J. Zhang, L. Zhang, Fedradar: Federated multi-task transfer
    learning for radar-based internet of medical things, IEEE Transactions on Network
    and Service Management (2023).'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] X. Jiang, J. Zhang, L. Zhang, Fedradar: 基于雷达的医疗物联网的联邦多任务迁移学习，《IEEE网络与服务管理汇刊》(2023)。'
- en: '[132] B. Azadi, M. Haslgrübler, B. Anzengruber-Tanase, G. Sopidis, A. Ferscha,
    Robust feature representation using multi-task learning for human activity recognition,
    Sensors 24 (2) (2024) 681.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] B. Azadi, M. Haslgrübler, B. Anzengruber-Tanase, G. Sopidis, A. Ferscha,
    使用多任务学习的鲁棒特征表示用于人类活动识别，《传感器》24(2) (2024) 681。'
- en: '[133] J. Ji, Z. Shu, H. Li, K. X. Lai, M. Lu, G. Jiang, W. Wang, Y. Zheng,
    X. Jiang, Edge-computing based knowledge distillation and multi-task learning
    for partial discharge recognition, IEEE Transactions on Instrumentation and Measurement
    (2024).'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] J. Ji, Z. Shu, H. Li, K. X. Lai, M. Lu, G. Jiang, W. Wang, Y. Zheng,
    X. Jiang, 基于边缘计算的知识蒸馏和多任务学习用于局部放电识别，《IEEE仪器与测量汇刊》(2024)。'
- en: '[134] R. Šajina, N. Tanković, I. Ipšić, Multi-task peer-to-peer learning using
    an encoder-only transformer model, Future Generation Computer Systems 152 (2024)
    170–178.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] R. Šajina, N. Tanković, I. Ipšić, 使用仅编码器的变换器模型的多任务对等学习，《未来计算机系统》152 (2024)
    170–178。'
- en: '[135] C. Ye, H. Zheng, Z. Hu, M. Zheng, Pfedsa: Personalized federated multi-task
    learning via similarity awareness, in: 2023 IEEE International Parallel and Distributed
    Processing Symposium (IPDPS), IEEE, 2023, pp. 480–488.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] C. Ye, H. Zheng, Z. Hu, M. Zheng, Pfedsa: 基于相似性意识的个性化联邦多任务学习，发表于：2023年IEEE国际并行与分布式处理研讨会
    (IPDPS)，IEEE，2023，第480–488页。'
- en: '[136] Z. Wang, C. Chen, D. Dong, Lifelong incremental reinforcement learning
    with online bayesian inference, IEEE Transactions on Neural Networks and Learning
    Systems 33 (8) (2021) 4003–4016.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Z. Wang, C. Chen, D. Dong, 基于在线贝叶斯推断的终身增量强化学习，《IEEE神经网络与学习系统汇刊》33(8)
    (2021) 4003–4016。'
- en: '[137] J. Wang, J. Cao, S. Wang, Z. Yao, W. Li, Irda: Incremental reinforcement
    learning for dynamic resource allocation, IEEE Transactions on Big Data 8 (3)
    (2020) 770–783.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] J. Wang, J. Cao, S. Wang, Z. Yao, W. Li, Irda: 动态资源分配的增量强化学习，《IEEE大数据汇刊》8(3)
    (2020) 770–783。'
- en: '[138] Z. Wang, C. Chen, H.-X. Li, D. Dong, T.-J. Tarn, Incremental reinforcement
    learning with prioritized sweeping for dynamic environments, IEEE/ASME Transactions
    on Mechatronics 24 (2) (2019) 621–632.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Z. Wang, C. Chen, H.-X. Li, D. Dong, T.-J. Tarn, 使用优先化扫掠的增量强化学习用于动态环境，《IEEE/ASME机电一体化汇刊》24(2)
    (2019) 621–632。'
- en: '[139] A. Gueriani, H. Kheddar, A. C. Mazari, Deep reinforcement learning for
    intrusion detection in iot: A survey, in: 2023 2nd International Conference on
    Electronics, Energy and Measurement (IC2EM), Vol. 1, IEEE, 2023, pp. 1–7.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] A. Gueriani, H. Kheddar, A. C. Mazari, 面向物联网的入侵检测的深度强化学习：综述，发表于：2023年第2届国际电子、能源与测量会议
    (IC2EM)，第1卷，IEEE，2023，第1–7页。'
- en: '[140] P. Zhao, S. C. Hoi, J. Wang, B. Li, Online transfer learning, Artificial
    intelligence 216 (2014) 76–102.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] P. Zhao, S. C. Hoi, J. Wang, B. Li, 在线迁移学习，《人工智能》216 (2014) 76–102。'
- en: '[141] Q. Wu, H. Wu, X. Zhou, M. Tan, Y. Xu, Y. Yan, T. Hao, Online transfer
    learning with multiple homogeneous or heterogeneous sources, IEEE Transactions
    on Knowledge and Data Engineering 29 (7) (2017) 1494–1507.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Q. Wu, H. Wu, X. Zhou, M. Tan, Y. Xu, Y. Yan, T. Hao, 基于多个同质或异质来源的在线迁移学习，《IEEE知识与数据工程汇刊》29(7)
    (2017) 1494–1507。'
- en: '[142] H. Wu, Y. Yan, Y. Ye, H. Min, M. K. Ng, Q. Wu, Online heterogeneous transfer
    learning by knowledge transition, ACM Transactions on Intelligent Systems and
    Technology (TIST) 10 (3) (2019) 1–19.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] H. Wu, Y. Yan, Y. Ye, H. Min, M. K. Ng, Q. Wu, 基于知识转移的在线异质迁移学习，《ACM智能系统与技术汇刊》（TIST）10(3)
    (2019) 1–19。'
- en: '[143] R. Alasbahi, X. Zheng, An online transfer learning framework with extreme
    learning machine for automated credit scoring, IEEE Access 10 (2022) 46697–46716.
    [doi:10.1109/ACCESS.2022.3171569](https://doi.org/10.1109/ACCESS.2022.3171569).'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] R. Alasbahi, X. Zheng, 一种基于极限学习机的在线迁移学习框架用于自动化信用评分，《IEEE Access》10 (2022)
    46697–46716。 [doi:10.1109/ACCESS.2022.3171569](https://doi.org/10.1109/ACCESS.2022.3171569)。'
